{
    "hands_on_practices": [
        {
            "introduction": "Understanding a numerical method begins with mastering its fundamental mechanics. This first practice focuses on the core principles of weighted least-squares (WLS) gradient reconstruction in a controlled two-dimensional setting. By deriving the normal equations from the objective function and solving them for different weighting strategies, you will gain a concrete intuition for how the positions and contributions of neighboring points influence the final gradient estimate and its statistical uncertainty .",
            "id": "3339283",
            "problem": "A scalar field $u(\\boldsymbol{x})$ is sampled on an unstructured two-dimensional stencil around a central control volume centroid located at $\\boldsymbol{x}_{0}=(0,0)$. The neighbor offsets are $\\boldsymbol{r}_{1}=(1,0)$, $\\boldsymbol{r}_{2}=(0,1)$, $\\boldsymbol{r}_{3}=(-1,0)$, $\\boldsymbol{r}_{4}=(0,-1)$, and $\\boldsymbol{r}_{5}=(1,1)$. The measured increments relative to the center are $\\Delta u_{1}=1.25$, $\\Delta u_{2}=-0.72$, $\\Delta u_{3}=-1.17$, $\\Delta u_{4}=0.69$, and $\\Delta u_{5}=0.54$, all dimensionless. Use the linearized reconstruction model $\\Delta u_{i}\\approx \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}$, where $\\boldsymbol{g}=(g_{x},g_{y})^{\\mathsf{T}}$ is the gradient at the centroid, and form the weighted least-squares objective $J(\\boldsymbol{g})=\\sum_{i=1}^{5}w_{i}\\left(\\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}-\\Delta u_{i}\\right)^{2}$.\n\nStarting from this model and the definition of weighted least squares, derive the normal equations from first principles and solve them to obtain the reconstructed gradient $\\boldsymbol{g}$ under two choices of weights:\n- Uniform weights $W_{1}=\\mathrm{diag}(1,1,1,1,1)$.\n- Distance-based weights $W_{2}=\\mathrm{diag}(1,1,1,1,1/2)$.\n\nAssume the measurement errors are independent with a common variance, so that $\\Delta u_{i}= \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}_{\\mathrm{true}}+\\varepsilon_{i}$ with $\\mathbb{E}[\\varepsilon_{i}]=0$ and $\\mathrm{Cov}(\\varepsilon)=\\sigma^{2}I$. Under this assumption, use the general expression for the covariance of a weighted least-squares estimator derived from the objective above to compute the ratio of determinants of the covariance matrices of $\\boldsymbol{g}$ obtained with $W_{2}$ versus $W_{1}$.\n\nExpress your final answer as a single row matrix containing five entries in the following order: $g_{x}$ with $W_{1}$, $g_{y}$ with $W_{1}$, $g_{x}$ with $W_{2}$, $g_{y}$ with $W_{2}$, and the ratio of determinants $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$. Round your numerical entries to four significant figures. No physical units are required.",
            "solution": "The user wants to solve a weighted least-squares problem to estimate a gradient vector.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A scalar field is denoted by $u(\\boldsymbol{x})$.\n- The central point is at $\\boldsymbol{x}_{0}=(0,0)$.\n- Neighbor offsets are $\\boldsymbol{r}_{1}=(1,0)$, $\\boldsymbol{r}_{2}=(0,1)$, $\\boldsymbol{r}_{3}=(-1,0)$, $\\boldsymbol{r}_{4}=(0,-1)$, and $\\boldsymbol{r}_{5}=(1,1)$.\n- Measured increments are $\\Delta u_{1}=1.25$, $\\Delta u_{2}=-0.72$, $\\Delta u_{3}=-1.17$, $\\Delta u_{4}=0.69$, and $\\Delta u_{5}=0.54$.\n- The linearized model is $\\Delta u_{i}\\approx \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}$, where $\\boldsymbol{g}=(g_{x},g_{y})^{\\mathsf{T}}$ is the gradient.\n- The weighted least-squares objective function is $J(\\boldsymbol{g})=\\sum_{i=1}^{5}w_{i}\\left(\\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}-\\Delta u_{i}\\right)^{2}$.\n- Two weight matrices are specified:\n    1. Uniform weights: $W_{1}=\\mathrm{diag}(1,1,1,1,1)$.\n    2. Distance-based weights: $W_{2}=\\mathrm{diag}(1,1,1,1,1/2)$.\n- The error model is $\\Delta u_{i}= \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}_{\\mathrm{true}}+\\varepsilon_{i}$ with $\\mathbb{E}[\\varepsilon_{i}]=0$ and $\\mathrm{Cov}(\\varepsilon)=\\sigma^{2}I$.\n- The task is to derive the normal equations, solve for $\\boldsymbol{g}$ for both weight sets, and compute the ratio of determinants of the covariance matrices of the estimators, $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$.\n- Numerical results should be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes the weighted least-squares method for gradient reconstruction from scattered data. This is a standard and widely used technique in computational fluid dynamics, finite element analysis, and other areas of computational science. The underlying principles are based on Taylor series expansions and statistical estimation, which are well-established.\n- **Well-Posed:** The problem is structured as a linear least-squares problem. The design matrix $\\boldsymbol{A}$ (whose rows are $\\boldsymbol{r}_i^{\\mathsf{T}}$) has full column rank, and the weight matrices $\\boldsymbol{W}_1$ and $\\boldsymbol{W}_2$ are positive definite. This ensures that the matrix $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}$ is invertible, guaranteeing a unique solution for the gradient $\\boldsymbol{g}$.\n- **Objective:** The problem is stated using precise mathematical language and definitions. All quantities are unambiguously defined. No subjective or opinion-based statements are present.\n- The problem is self-contained, with all necessary data and models provided. It is not contradictory, unrealistic, ill-posed, or trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation and Solution\n\nThe problem is to find the gradient vector $\\boldsymbol{g} = (g_x, g_y)^{\\mathsf{T}}$ that minimizes the weighted least-squares objective function:\n$$ J(\\boldsymbol{g}) = \\sum_{i=1}^{5} w_i (\\boldsymbol{r}_i \\cdot \\boldsymbol{g} - \\Delta u_i)^2 $$\nThis can be expressed in matrix form. Let the design matrix $\\boldsymbol{A}$ be a $5 \\times 2$ matrix whose rows are the vectors $\\boldsymbol{r}_i^{\\mathsf{T}}$, and let $\\boldsymbol{b}$ be a $5 \\times 1$ vector of the measurements $\\Delta u_i$.\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix}, \\quad \\boldsymbol{b} = \\begin{pmatrix} 1.25 \\\\ -0.72 \\\\ -1.17 \\\\ 0.69 \\\\ 0.54 \\end{pmatrix}\n$$\nThe system of equations is $\\boldsymbol{A}\\boldsymbol{g} \\approx \\boldsymbol{b}$. The objective function in matrix form is:\n$$ J(\\boldsymbol{g}) = (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b})^{\\mathsf{T}} \\boldsymbol{W} (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b}) $$\nwhere $\\boldsymbol{W}$ is the diagonal matrix of weights $w_i$. To minimize $J(\\boldsymbol{g})$, we set its gradient with respect to $\\boldsymbol{g}$ to zero. First, expand the objective function:\n$$ J(\\boldsymbol{g}) = (\\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}} - \\boldsymbol{b}^{\\mathsf{T}}) \\boldsymbol{W} (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b}) = \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} - \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g} + \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\nSince $\\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g}$ is a scalar, it equals its transpose $(\\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g})^{\\mathsf{T}} = \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}^{\\mathsf{T}}\\boldsymbol{b}$. Since $\\boldsymbol{W}$ is diagonal, $\\boldsymbol{W}^{\\mathsf{T}} = \\boldsymbol{W}$. Thus, the two middle terms are identical.\n$$ J(\\boldsymbol{g}) = \\boldsymbol{g}^{\\mathsf{T}}(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} - 2\\boldsymbol{g}^{\\mathsf{T}}(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) + \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\nDifferentiating with respect to $\\boldsymbol{g}$ and setting the result to zero yields:\n$$ \\frac{\\partial J}{\\partial \\boldsymbol{g}} = 2(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} - 2(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{0} $$\nThis gives the **normal equations**:\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} = \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\nThe solution for the gradient estimate $\\boldsymbol{g}$ is:\n$$ \\boldsymbol{g} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) $$\n\n**Case 1: Uniform Weights** $\\boldsymbol{W}_1 = \\mathrm{diag}(1,1,1,1,1) = \\boldsymbol{I}$\nThe normal equations simplify to $(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})\\boldsymbol{g}_1 = \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b}$.\nFirst, we compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A} = \\begin{pmatrix} 1 & 0 & -1 & 0 & 1 \\\\ 0 & 1 & 0 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1^2+(-1)^2+1^2 & 1(1) \\\\ 1(1) & 1^2+(-1)^2+1^2 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} $$\nNext, we compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b} = \\begin{pmatrix} 1 & 0 & -1 & 0 & 1 \\\\ 0 & 1 & 0 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1.25 \\\\ -0.72 \\\\ -1.17 \\\\ 0.69 \\\\ 0.54 \\end{pmatrix} = \\begin{pmatrix} 1.25 - (-1.17) + 0.54 \\\\ -0.72 - 0.69 + 0.54 \\end{pmatrix} = \\begin{pmatrix} 2.96 \\\\ -0.87 \\end{pmatrix} $$\nThe inverse of $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}$ is:\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} = \\frac{1}{3(3)-1(1)} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} $$\nNow we solve for $\\boldsymbol{g}_1$:\n$$ \\boldsymbol{g}_1 = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 2.96 \\\\ -0.87 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3(2.96) - 1(-0.87) \\\\ -1(2.96) + 3(-0.87) \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 8.88 + 0.87 \\\\ -2.96 - 2.61 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 9.75 \\\\ -5.57 \\end{pmatrix} = \\begin{pmatrix} 1.21875 \\\\ -0.69625 \\end{pmatrix} $$\nRounding to four significant figures, $g_{x,1} = 1.219$ and $g_{y,1} = -0.6963$.\n\n**Case 2: Distance-based Weights** $\\boldsymbol{W}_2 = \\mathrm{diag}(1,1,1,1,1/2)$\nWe compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A} = \\sum_{i=1}^5 w_i \\boldsymbol{r}_i \\boldsymbol{r}_i^{\\mathsf{T}} = 1\\begin{pmatrix}1\\\\0\\end{pmatrix}\\begin{pmatrix}1&0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\1\\end{pmatrix}\\begin{pmatrix}0&1\\end{pmatrix} + 1\\begin{pmatrix}-1\\\\0\\end{pmatrix}\\begin{pmatrix}-1&0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\-1\\end{pmatrix}\\begin{pmatrix}0&-1\\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix}\\begin{pmatrix}1&1\\end{pmatrix} $$\n$$ = \\begin{pmatrix}1&0\\\\0&0\\end{pmatrix} + \\begin{pmatrix}0&0\\\\0&1\\end{pmatrix} + \\begin{pmatrix}1&0\\\\0&0\\end{pmatrix} + \\begin{pmatrix}0&0\\\\0&1\\end{pmatrix} + \\begin{pmatrix}0.5&0.5\\\\0.5&0.5\\end{pmatrix} = \\begin{pmatrix} 2.5 & 0.5 \\\\ 0.5 & 2.5 \\end{pmatrix} $$\nNext, we compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{b}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{b} = \\sum_{i=1}^5 w_i \\boldsymbol{r}_i \\Delta u_i = 1\\begin{pmatrix}1\\\\0\\end{pmatrix}(1.25) + 1\\begin{pmatrix}0\\\\1\\end{pmatrix}(-0.72) + 1\\begin{pmatrix}-1\\\\0\\end{pmatrix}(-1.17) + 1\\begin{pmatrix}0\\\\-1\\end{pmatrix}(0.69) + \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix}(0.54) $$\n$$ = \\begin{pmatrix}1.25\\\\-0.72\\end{pmatrix} + \\begin{pmatrix}1.17\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\-0.69\\end{pmatrix} + \\begin{pmatrix}0.27\\\\0.27\\end{pmatrix} = \\begin{pmatrix} 1.25+1.17+0.27 \\\\-0.72-0.69+0.27 \\end{pmatrix} = \\begin{pmatrix} 2.69 \\\\ -1.14 \\end{pmatrix} $$\nThe inverse of $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}$ is:\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} = \\frac{1}{2.5^2-0.5^2} \\begin{pmatrix} 2.5 & -0.5 \\\\ -0.5 & 2.5 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2.5 & -0.5 \\\\ -0.5 & 2.5 \\end{pmatrix} $$\nNow we solve for $\\boldsymbol{g}_2$:\n$$ \\boldsymbol{g}_2 = \\frac{1}{6} \\begin{pmatrix} 2.5 & -0.5 \\\\ -0.5 & 2.5 \\end{pmatrix} \\begin{pmatrix} 2.69 \\\\ -1.14 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2.5(2.69) - 0.5(-1.14) \\\\ -0.5(2.69) + 2.5(-1.14) \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 6.725 + 0.57 \\\\ -1.345 - 2.85 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 7.295 \\\\ -4.195 \\end{pmatrix} = \\begin{pmatrix} 1.2158\\bar{3} \\\\ -0.6991\\bar{6} \\end{pmatrix} $$\nRounding to four significant figures, $g_{x,2} = 1.216$ and $g_{y,2} = -0.6992$.\n\n**Covariance Matrix Ratio**\nThe estimated gradient is $\\hat{\\boldsymbol{g}} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}$. The measurement vector $\\boldsymbol{b}$ is related to the true gradient $\\boldsymbol{g}_{\\mathrm{true}}$ by $\\boldsymbol{b} = \\boldsymbol{A}\\boldsymbol{g}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon}$ is the error vector with $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$ and $\\mathrm{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] = \\sigma^2 \\boldsymbol{I}$.\nThe estimation error is $\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon}$.\nThe covariance matrix of the estimator $\\hat{\\boldsymbol{g}}$ is:\n$$ \\mathrm{Cov}(\\hat{\\boldsymbol{g}}) = \\mathbb{E}[(\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}})(\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}})^{\\mathsf{T}}] = \\mathbb{E}[((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon}) ((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon})^{\\mathsf{T}}] $$\n$$ \\mathrm{Cov}(\\hat{\\boldsymbol{g}}) = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] \\boldsymbol{W}^{\\mathsf{T}}\\boldsymbol{A} ((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1})^{\\mathsf{T}} $$\nSince $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] = \\sigma^2\\boldsymbol{I}$, $\\boldsymbol{W}$ is diagonal, and $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}$ is symmetric:\n$$ \\mathrm{Cov}_W(\\boldsymbol{g}) = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}^2\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} $$\nFor $\\boldsymbol{W}_1 = \\boldsymbol{I}$, we have $\\boldsymbol{W}_1^2 = \\boldsymbol{I}$. The formula simplifies to the OLS case:\n$$ \\mathrm{Cov}_{W_1}(\\boldsymbol{g}) = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} $$\nThe determinant is:\n$$ \\det(\\mathrm{Cov}_{W_1}(\\boldsymbol{g})) = \\det(\\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1}) = (\\sigma^2)^2 \\det((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1}) = \\frac{\\sigma^4}{\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})} = \\frac{\\sigma^4}{8} $$\nFor $\\boldsymbol{W}_2 = \\mathrm{diag}(1,1,1,1,1/2)$, we have $\\boldsymbol{W}_2^2 = \\mathrm{diag}(1,1,1,1,1/4)$.\n$$ \\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g})) = \\det\\left( \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} \\right) $$\n$$ = (\\sigma^2)^2 \\frac{\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A})}{(\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}))^2} $$\nWe already found $\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}) = \\det \\begin{pmatrix} 2.5 & 0.5 \\\\ 0.5 & 2.5 \\end{pmatrix} = 6$.\nWe need to compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A} = \\sum_{i=1}^5 w_i^2 \\boldsymbol{r}_i \\boldsymbol{r}_i^{\\mathsf{T}} = 1^2\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix} + 1^2\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix} + (\\frac{1}{2})^2\\begin{pmatrix}1&1\\\\1&1\\end{pmatrix} = \\begin{pmatrix}2&0\\\\0&2\\end{pmatrix} + \\frac{1}{4}\\begin{pmatrix}1&1\\\\1&1\\end{pmatrix} = \\begin{pmatrix} 2.25 & 0.25 \\\\ 0.25 & 2.25 \\end{pmatrix} $$\nThe determinant is $\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}) = 2.25^2 - 0.25^2 = 5.0625 - 0.0625 = 5$.\nSo, $\\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g})) = \\sigma^4 \\frac{5}{6^2} = \\frac{5\\sigma^4}{36}$.\nThe ratio is:\n$$ \\frac{\\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g}))}{\\det(\\mathrm{Cov}_{W_1}(\\boldsymbol{g}))} = \\frac{5\\sigma^4/36}{\\sigma^4/8} = \\frac{5}{36} \\times 8 = \\frac{40}{36} = \\frac{10}{9} = 1.111\\bar{1} $$\nRounding to four significant figures, the ratio is $1.111$.\n\nThe final results, rounded to four significant figures, are:\n- $g_{x}$ with $W_1$: $1.219$\n- $g_{y}$ with $W_1$: $-0.6963$\n- $g_{x}$ with $W_2$: $1.216$\n- $g_{y}$ with $W_2$: $-0.6992$\n- Ratio of determinants: $1.111$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.219 & -0.6963 & 1.216 & -0.6992 & 1.111\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theoretical correctness does not always guarantee practical success, especially in numerical computing where finite precision arithmetic is a reality. This exercise transitions from pure calculation to robust implementation, tackling the critical issue of ill-conditioning that arises from poor-quality meshes in three dimensions. By implementing and comparing a naive normal-equations solver against a rank-revealing QR factorization, you will diagnose and remedy geometric pathologies, learning why robust linear algebra techniques are indispensable tools for a computational scientist .",
            "id": "3339293",
            "problem": "You are asked to implement and analyze least-squares gradient reconstruction for a single control volume in three-dimensional space under increasingly pathological geometric stencils. The goal is to show how near-coplanar neighbor configurations drive the linear least-squares system toward singularity and to design rank-revealing remedies based on pivoting and stencil expansion. The task must be solved by writing a complete, runnable program that constructs the specified stencils, computes gradients with multiple methods, and reports quantitative diagnostics.\n\nConsider a scalar field $\\,\\phi(\\mathbf{x})\\,$ that is linear in space, $\\;\\phi(\\mathbf{x}) = a\\,x + b\\,y + c\\,z\\,$, where $\\,\\mathbf{x} = (x,y,z)\\,$ and the constants $\\,a,b,c\\,$ are given. For a central point $\\,\\mathbf{x}_0\\,$ with neighboring points $\\,\\mathbf{x}_i\\,$, define the least-squares gradient reconstruction of $\\,\\nabla \\phi\\,$ at $\\,\\mathbf{x}_0\\,$ by minimizing\n$$\nJ(\\mathbf{g}) \\,=\\, \\sum_{i=1}^{N} w_i \\,\\Big(\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0) - \\mathbf{g}\\cdot(\\mathbf{x}_i - \\mathbf{x}_0)\\Big)^2,\n$$\nwith weights $\\,w_i = 1\\,$ for all neighbors. This leads to the linear system\n$$\n\\mathbf{A}\\,\\mathbf{g} \\,\\approx\\, \\mathbf{b},\n$$\nwhere the $\\,i$-th row of $\\,\\mathbf{A}\\in\\mathbb{R}^{N\\times 3}\\,$ is $\\,(\\mathbf{x}_i - \\mathbf{x}_0)^\\top\\,$, and $\\,\\mathbf{b}\\in\\mathbb{R}^{N}\\,$ has entries $\\,\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)\\,$. In the absence of noise and with full column rank of $\\,\\mathbf{A}\\,$, the exact solution $\\,\\mathbf{g}^\\star = \\nabla\\phi(\\mathbf{x}_0)\\,$ is recovered. However, if the neighbor vectors are nearly coplanar, $\\,\\mathbf{A}\\,$ becomes ill-conditioned or rank-deficient.\n\nYour program must implement and compare the following three methods:\n- A naive normal-equations solver using the matrix $\\,\\mathbf{N} = \\mathbf{A}^\\top \\mathbf{A}\\,$ and right-hand side $\\,\\mathbf{c} = \\mathbf{A}^\\top \\mathbf{b}\\,$ solved for $\\,\\mathbf{g}\\,$ by direct linear solve. If $\\,\\mathbf{N}\\,$ is singular or numerically too ill-conditioned, this method may fail or produce unstable results.\n- A rank-revealing solver based on a column-pivoted $\\,\\mathbf{Q}\\mathbf{R}\\,$ factorization of $\\,\\mathbf{A}\\,$, with a numerical rank determined from the diagonal of $\\,\\mathbf{R}\\,$ using a threshold $\\,\\tau = \\mathrm{rcond}\\cdot |R_{11}|\\,$ with $\\,\\mathrm{rcond} = 10^{-12}\\,$. The solution should set components associated with negligible columns to zero to obtain a minimal-norm least-squares solution consistent with the estimated numerical rank.\n- A stencil expansion remedy: augment the initial neighbor set with additional candidate points (provided below) until the estimated numerical rank reaches $\\,3\\,$, and then recompute the pivoted $\\,\\mathbf{Q}\\mathbf{R}\\,$ solution on the expanded stencil.\n\nTo quantify near-singularity, use the singular value decomposition to estimate the $\\,2$-norm condition number of $\\,\\mathbf{A}\\,$ as $\\,\\kappa_2(\\mathbf{A}) = \\sigma_{\\max}/\\sigma_{\\min}\\,$. If $\\,\\sigma_{\\min} = 0\\,$, report $\\,\\kappa_2(\\mathbf{A})\\,$ as the sentinel value $\\,10^{18}\\,$. For the naive normal-equations method, if the linear solve fails due to singularity or if the condition number of $\\,\\mathbf{N}\\,$ exceeds $\\,10^{15}\\,$, report the error for this method as the sentinel value $\\,10^{9}\\,$.\n\nImplement the following test suite with three cases. In all cases, take the central point $\\,\\mathbf{x}_0 = (0,0,0)\\,$ and unit weights $\\,w_i = 1\\,$. Round all floating-point outputs to six decimal places.\n\n- Case 1 (well-conditioned): use coefficients $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.9\\,$ and neighbors\n  $$\n  (1,0,0),\\;(-1,0,0),\\;(0,1,0),\\;(0,-1,0),\\;(0,0,1),\\;(0,0,-1).\n  $$\n- Case 2 (nearly coplanar): use coefficients $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.9\\,$. Let $\\,\\epsilon = 10^{-8}\\,$ and neighbors\n  $$\n  (1,0,\\epsilon),\\;(-1,0,\\epsilon),\\;(0,1,\\epsilon),\\;(0,-1,\\epsilon),\\;(1,1,\\epsilon),\\;(-1,-1,\\epsilon).\n  $$\n  Provide candidate expansion points in the following order and add them one at a time until the estimated numerical rank reaches $\\,3\\,$:\n  $$\n  (0,0,0.2),\\;(0.2,0,0.2),\\;(0,0.2,0.2).\n  $$\n- Case 3 (exactly coplanar with compatible field): use coefficients $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.0\\,$, and neighbors\n  $$\n  (1,0,0),\\;(-1,0,0),\\;(0,1,0),\\;(0,-1,0),\\;(1,1,0),\\;(-1,-1,0).\n  $$\n  Use the same candidate expansion points as in Case 2, in the same order.\n\nFor each case, construct $\\,\\mathbf{A}\\,$ and $\\,\\mathbf{b}\\,$ exactly from the definitions above. Denote the true gradient by $\\,\\mathbf{g}_{\\mathrm{true}} = (a,b,c)^\\top\\,$. For each method, compute the reconstructed gradient $\\,\\widehat{\\mathbf{g}}\\,$ and the Euclidean error $\\,\\|\\widehat{\\mathbf{g}}-\\mathbf{g}_{\\mathrm{true}}\\|_2\\,$. Also estimate the numerical rank before and after expansion using the pivoted $\\,\\mathbf{Q}\\mathbf{R}\\,$ with the threshold described above.\n\nYour program must produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets. Each case must be a list of the form\n$$\n[\\kappa_2(\\mathbf{A}),\\; r_{\\mathrm{init}},\\; e_{\\mathrm{naive}},\\; e_{\\mathrm{pivot}},\\; e_{\\mathrm{expand}},\\; r_{\\mathrm{expand}}],\n$$\nwhere $\\,\\kappa_2(\\mathbf{A})\\,$ is the capped condition number of $\\,\\mathbf{A}\\,$ (with $\\,10^{18}\\,$ sentinel if singular), $\\,r_{\\mathrm{init}}\\,$ is the estimated numerical rank for the initial stencil, $\\,e_{\\mathrm{naive}}\\,$ is the naive normal-equations error (with $\\,10^{9}\\,$ sentinel if the solve fails or is too ill-conditioned), $\\,e_{\\mathrm{pivot}}\\,$ is the error from the rank-revealing pivoted solution on the initial stencil, $\\,e_{\\mathrm{expand}}\\,$ is the error from the pivoted solution on the expanded stencil, and $\\,r_{\\mathrm{expand}}\\,$ is the estimated numerical rank after expansion. Round all floating-point entries to six decimal places. The final printed line must look like\n$$\n[[\\cdots],[\\cdots],[\\cdots]],\n$$\nwith no additional text.\n\nAll angles, if any, are not applicable here. No physical units are involved in this problem. All requested numerical outputs are pure real numbers without units and must be rounded as specified.",
            "solution": "The present task requires the implementation and comparative analysis of three numerical methods for least-squares gradient reconstruction of a linear scalar field in three dimensions. The analysis focuses on the numerical stability and accuracy of these methods when applied to geometric stencils of varying quality, ranging from well-conditioned to rank-deficient.\n\n### Problem Formulation\n\nWe are given a linear scalar field $\\phi(\\mathbf{x}) = a\\,x + b\\,y + c\\,z$. The gradient of this field is constant everywhere, given by the vector of coefficients $\\mathbf{g}_{\\text{true}} = \\nabla\\phi = (a, b, c)^\\top$.\n\nThe objective is to reconstruct this gradient at a central point $\\mathbf{x}_0$ using a set of $N$ neighboring points $\\{\\mathbf{x}_i\\}_{i=1}^N$. The least-squares formulation seeks a gradient vector $\\mathbf{g}$ that minimizes the sum of squared errors:\n$$\nJ(\\mathbf{g}) = \\sum_{i=1}^{N} w_i \\left( (\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)) - \\mathbf{g} \\cdot (\\mathbf{x}_i - \\mathbf{x}_0) \\right)^2\n$$\nGiven that weights are unity ($w_i=1$) and the central point is the origin ($\\mathbf{x}_0 = (0,0,0)$), we have $\\Delta\\mathbf{x}_i = \\mathbf{x}_i$ and $\\phi(\\mathbf{x}_0) = 0$. The functional simplifies to:\n$$\nJ(\\mathbf{g}) = \\sum_{i=1}^{N} \\left( \\phi(\\mathbf{x}_i) - \\mathbf{g} \\cdot \\mathbf{x}_i \\right)^2\n$$\nThis is a standard linear least-squares problem, which can be expressed in matrix form as finding $\\mathbf{g}$ that minimizes $\\|\\mathbf{A}\\mathbf{g} - \\mathbf{b}\\|_2^2$. The system matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times 3}$ and the right-hand-side vector $\\mathbf{b} \\in \\mathbb{R}^{N}$ are constructed as follows:\n- The $i$-th row of $\\mathbf{A}$ is the transposed displacement vector, $(\\mathbf{x}_i - \\mathbf{x}_0)^\\top = \\mathbf{x}_i^\\top$.\n- The $i$-th entry of $\\mathbf{b}$ is the scalar difference, $\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0) = \\phi(\\mathbf{x}_i)$.\n\nSince $\\phi(\\mathbf{x}_i) = a x_i + b y_i + c z_i = \\mathbf{x}_i^\\top \\mathbf{g}_{\\text{true}}$, it follows that $\\mathbf{b} = \\mathbf{A}\\mathbf{g}_{\\text{true}}$. This means the linear system is consistent. If the matrix $\\mathbf{A}$ has full column rank (i.e., rank $3$), the unique least-squares solution is $\\mathbf{g} = \\mathbf{g}_{\\text{true}}$. However, if the neighbor points $\\{\\mathbf{x}_i\\}$ are coplanar or nearly coplanar, the rows of $\\mathbf{A}$ become linearly dependent or nearly so, causing $\\mathbf{A}$ to be rank-deficient or ill-conditioned.\n\n### Numerical Methods and Diagnostics\n\nWe will implement and compare three methods to solve for $\\mathbf{g}$:\n\n1.  **Naive Normal Equations**: This method transforms the overdetermined system into a square $3 \\times 3$ system by pre-multiplying by $\\mathbf{A}^\\top$:\n    $$\n    (\\mathbf{A}^\\top \\mathbf{A}) \\mathbf{g} = \\mathbf{A}^\\top \\mathbf{b}\n    $$\n    The solution is $\\widehat{\\mathbf{g}}_{\\text{naive}} = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}$. While simple, this approach is numerically unstable if $\\mathbf{A}$ is ill-conditioned, because the condition number of the normal matrix $\\mathbf{N} = \\mathbf{A}^\\top \\mathbf{A}$ is the square of the condition number of $\\mathbf{A}$, i.e., $\\kappa_2(\\mathbf{N}) = \\kappa_2(\\mathbf{A})^2$. A large condition number for $\\mathbf{A}$ can lead to a prohibitively large one for $\\mathbf{N}$, resulting in significant numerical error or failure of the linear solver. We will monitor $\\kappa_2(\\mathbf{N})$ and use a sentinel error value of $10^9$ if it exceeds $10^{15}$ or if the solver fails.\n\n2.  **Rank-Revealing Pivoted QR Factorization**: This method provides a more robust solution by directly addressing the rank of $\\mathbf{A}$. We compute the column-pivoted QR factorization $\\mathbf{A}\\mathbf{P} = \\mathbf{Q}\\mathbf{R}$, where $\\mathbf{P}$ is a permutation matrix, $\\mathbf{Q}$ has orthonormal columns, and $\\mathbf{R}$ is upper triangular with diagonal entries of decreasing magnitude. The numerical rank $r$ is estimated by counting the number of diagonal elements $|R_{kk}|$ that are larger than a threshold $\\tau = \\text{rcond} \\cdot |R_{11}|$, with $\\text{rcond} = 10^{-12}$. The least-squares problem is solved by finding the minimal-norm solution. This is achieved by solving the well-conditioned part of the system corresponding to the estimated rank $r$ and setting the remaining $3-r$ components of the permuted solution vector to zero. This procedure effectively regularizes the problem by projecting it onto the numerically stable subspace spanned by the \"strong\" columns of $\\mathbf{A}$.\n\n3.  **Stencil Expansion**: When the initial stencil of neighbors is geometrically deficient (rank $< 3$), this method remedies the issue by augmenting the stencil. Candidate points are added one by one to the set of neighbors until the numerical rank, estimated via pivoted QR, reaches the full rank of $3$. Once a full-rank stencil is achieved, the gradient $\\widehat{\\mathbf{g}}_{\\text{expand}}$ is computed using the robust pivoted QR method on this expanded system. This approach aims to restore the well-posedness of the problem by incorporating additional geometric information.\n\nThe quality of the initial stencil is quantified by the $2$-norm condition number of $\\mathbf{A}$, $\\kappa_2(\\mathbf{A}) = \\sigma_{\\max}/\\sigma_{\\min}$, computed from its singular values. If $\\mathbf{A}$ is singular ($\\sigma_{\\min}=0$), a sentinel value of $10^{18}$ is reported. The performance of each method is measured by the Euclidean error $\\|\\widehat{\\mathbf{g}} - \\mathbf{g}_{\\text{true}}\\|_2$.\n\n### Analysis of Test Cases\n\n-   **Case 1 (Well-conditioned)**: The neighbors form an orthogonal basis. The matrix $\\mathbf{A}$ is perfectly well-conditioned with $\\kappa_2(\\mathbf{A}) = 1$. The numerical rank is robustly identified as $3$. All three methods are expected to work flawlessly, yielding errors close to machine precision ($0$).\n\n-   **Case 2 (Nearly Coplanar)**: The neighbors lie on the plane $z = \\epsilon = 10^{-8}$. The displacement vectors are nearly coplanar, making the matrix $\\mathbf{A}$ severely ill-conditioned with a very large $\\kappa_2(\\mathbf{A})$. The numerical rank will be estimated as $2$. The normal equations method is expected to produce a large error due to the squared condition number. The pivoted QR method will correctly identify the rank deficiency and find a solution within the dominant $2$D subspace, resulting in a significantly smaller error. Stencil expansion will add a point with a non-trivial $z$-component, restoring the rank to $3$ and enabling an accurate reconstruction with very low error.\n\n-   **Case 3 (Exactly Coplanar)**: The neighbors lie on the plane $z=0$. The matrix $\\mathbf{A}$ is exactly rank-deficient (rank $2$), and its smallest singular value is zero, yielding an infinite condition number. The matrix $\\mathbf{A}^\\top \\mathbf{A}$ is singular, so the normal equations method will fail, triggering the sentinel error. The true gradient's $z$-component is $c=0$, meaning $\\mathbf{g}_{\\text{true}}$ lies within the subspace spanned by the columns of $\\mathbf{A}$. The pivoted QR method, by finding the minimal-norm solution, will correctly identify $\\mathbf{g}_{\\text{true}}$ and yield zero error. Stencil expansion will restore rank to $3$ and also find the exact solution.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\ndef solve_pivoted_qr(A, b, rcond):\n    \"\"\"\n    Solves the least-squares problem Ax = b using rank-revealing QR\n    factorization with column pivoting.\n    \"\"\"\n    n_cols = A.shape[1]\n    if A.shape[0] == 0:\n        return np.zeros(n_cols), 0\n        \n    Q, R, P = qr(A, pivoting=True)\n    \n    # Estimate numerical rank\n    if np.abs(R[0, 0]) < np.finfo(float).eps:\n        rank = 0\n    else:\n        tau = rcond * np.abs(R[0, 0])\n        rank = np.sum(np.abs(np.diag(R)) > tau)\n\n    # Solve for the permuted gradient vector y = P^T * g\n    y = np.zeros(n_cols)\n    if rank > 0:\n        d = Q.T @ b\n        try:\n            y[:rank] = solve_triangular(R[:rank, :rank], d[:rank], check_finite=False)\n        except np.linalg.LinAlgError:\n            # This should not happen with a proper rank check, but as a safeguard.\n            pass\n\n    # Un-permute the solution to get g\n    g_hat = np.zeros(n_cols)\n    g_hat[P] = y\n    \n    return g_hat, rank\n\ndef calc_error(g_hat, g_true):\n    \"\"\"Computes the Euclidean norm of the error vector.\"\"\"\n    return np.linalg.norm(g_hat - g_true)\n\ndef process_case(coeffs, neighbors, expansion_pts):\n    \"\"\"\n    Processes a single test case, computes gradients with three methods,\n    and returns all specified diagnostic values.\n    \"\"\"\n    # Constants and Sentinels\n    RCOND_QR = 1e-12\n    COND_N_THRESH = 1e15\n    ERROR_SENTINEL = 1e9\n    KAPPA_SENTINEL = 1e18\n\n    # --- Initial Stencil Setup ---\n    g_true = np.array(coeffs, dtype=float)\n    x0 = np.array([0.0, 0.0, 0.0])\n    \n    A = neighbors - x0\n    b = A @ g_true\n\n    # --- Diagnostics for Initial Stencil ---\n    # Condition number of A using SVD\n    try:\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        if singular_values[-1] < np.finfo(float).eps * singular_values[0]:\n            kappa_A = KAPPA_SENTINEL\n        else:\n            kappa_A = singular_values[0] / singular_values[-1]\n    except np.linalg.LinAlgError:\n        kappa_A = KAPPA_SENTINEL\n\n    # --- Method 2: Rank-Revealing Pivoted QR (Initial Stencil) ---\n    g_pivot, r_init = solve_pivoted_qr(A, b, RCOND_QR)\n    e_pivot = calc_error(g_pivot, g_true)\n\n    # --- Method 1: Naive Normal Equations ---\n    e_naive = ERROR_SENTINEL\n    N = A.T @ A\n    c = A.T @ b\n    if np.linalg.cond(N) < COND_N_THRESH:\n        try:\n            g_naive = np.linalg.solve(N, c)\n            e_naive = calc_error(g_naive, g_true)\n        except np.linalg.LinAlgError:\n            e_naive = ERROR_SENTINEL\n    \n    # --- Method 3: Stencil Expansion ---\n    if r_init == 3:\n        # If rank is already full, expansion is not needed.\n        A_exp, b_exp = A, b\n        r_expand = r_init\n        g_expand = g_pivot\n        e_expand = e_pivot\n    else:\n        # Augment stencil until rank is 3\n        current_neighbors = neighbors.copy()\n        current_rank = r_init\n        \n        for pt in expansion_pts:\n            if current_rank == 3:\n                break\n            \n            # Add point and rebuild system\n            current_neighbors = np.vstack([current_neighbors, pt])\n            A_exp_loop = current_neighbors - x0\n            \n            # Re-evaluate rank\n            _, R_loop, _ = qr(A_exp_loop, pivoting=True)\n            tau_loop = RCOND_QR * np.abs(R_loop[0, 0])\n            current_rank = np.sum(np.abs(np.diag(R_loop)) > tau_loop)\n\n        # Final expanded system\n        A_exp = current_neighbors - x0\n        b_exp = A_exp @ g_true\n        r_expand = current_rank\n        \n        # Solve with expanded stencil\n        g_expand, _ = solve_pivoted_qr(A_exp, b_exp, RCOND_QR)\n        e_expand = calc_error(g_expand, g_true)\n        \n    return [kappa_A, float(r_init), e_naive, e_pivot, e_expand, float(r_expand)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Case 1: Well-conditioned\n    case1 = {\n        \"coeffs\": (2.3, -1.7, 0.9),\n        \"neighbors\": np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [0, 0, 1], [0, 0, -1]\n        ]),\n        \"expansion_pts\": []\n    }\n\n    # Case 2: Nearly coplanar\n    epsilon = 1e-8\n    case2 = {\n        \"coeffs\": (2.3, -1.7, 0.9),\n        \"neighbors\": np.array([\n            [1, 0, epsilon], [-1, 0, epsilon], [0, 1, epsilon],\n            [0, -1, epsilon], [1, 1, epsilon], [-1, -1, epsilon]\n        ]),\n        \"expansion_pts\": np.array([\n            [0, 0, 0.2], [0.2, 0, 0.2], [0, 0.2, 0.2]\n        ])\n    }\n\n    # Case 3: Exactly coplanar\n    case3 = {\n        \"coeffs\": (2.3, -1.7, 0.0),\n        \"neighbors\": np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [1, 1, 0], [-1, -1, 0]\n        ]),\n        \"expansion_pts\": np.array([\n            [0, 0, 0.2], [0.2, 0, 0.2], [0, 0.2, 0.2]\n        ])\n    }\n    \n    test_cases = [case1, case2, case3]\n    all_results = [process_case(**params) for params in test_cases]\n\n    # Format output string\n    output_parts = []\n    for result in all_results:\n        formatted_result = [f\"{val:.6f}\" for val in result]\n        output_parts.append(f\"[{','.join(formatted_result)}]\")\n    \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Modern computational science increasingly involves fusing simulation data with external measurements, where noise is not only present but often correlated. This final practice elevates the least-squares concept to a more general statistical framework, introducing the Best Linear Unbiased Estimator (BLUE) for scenarios with known error covariance. You will derive and implement this powerful estimator, comparing its performance to ordinary least-squares and appreciating its optimality when handling complex, correlated uncertainty structures as described by the Gauss–Markov theorem .",
            "id": "3339272",
            "problem": "Consider a two-dimensional cell-centered reconstruction of the gradient of a scalar field in Computational Fluid Dynamics (CFD). Let the scalar field be denoted by $\\phi(\\mathbf{x})$, with a cell center at $\\mathbf{x}_0 \\in \\mathbb{R}^2$. Assume a first-order Taylor expansion holds in a neighborhood of $\\mathbf{x}_0$, such that for each neighbor $i$ with position vector $\\mathbf{x}_i$ and displacement $\\mathbf{r}_i = \\mathbf{x}_i - \\mathbf{x}_0$, the relationship $\\phi(\\mathbf{x}_i) \\approx \\phi(\\mathbf{x}_0) + \\nabla \\phi(\\mathbf{x}_0) \\cdot \\mathbf{r}_i$ is valid. Define the measurement differences $y_i = \\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)$ and stack these into the vector $\\mathbf{y} \\in \\mathbb{R}^m$, where $m$ is the number of neighbors. Form the geometry matrix $\\mathbf{R} \\in \\mathbb{R}^{m \\times 2}$ whose $i$-th row is $\\mathbf{r}_i^\\top$. The measurement noise is modeled as zero-mean Gaussian with covariance $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times m}$, so that $\\mathbf{y} = \\mathbf{R}\\mathbf{g} + \\boldsymbol{\\eta}$, where $\\mathbf{g} = \\nabla \\phi(\\mathbf{x}_0) \\in \\mathbb{R}^2$ is unknown, and $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})$ captures uncertainty due to sensor fusion (e.g., multiple sensors with correlated noise). All quantities are dimensionless for this problem.\n\nYour tasks are:\n- Starting from the linearization by the Taylor expansion and the Gaussian noise model, derive from first principles the Best Linear Unbiased Estimator (BLUE) of the gradient $\\mathbf{g}$ that accounts for the covariance $\\mathbf{\\Sigma}$ of the neighbor differences. Establish the unbiasedness of the estimator and derive its estimator covariance.\n- In parallel, consider the ordinary least squares estimator that ignores $\\mathbf{\\Sigma}$ (assumes identity covariance). Derive its estimator covariance under the true noise covariance $\\mathbf{\\Sigma}$.\n- Implement these estimators in a program and, for each test case below, compute the norm of the gradient estimation error for the BLUE estimator and verify theoretically that the BLUE estimator has less than or equal total variance compared with ordinary least squares, quantified by the trace of the estimator covariance. The program must implement matrix solves without relying on external data and must produce a single-line output aggregating all test results as specified.\n\nTest suite (use exactly these values):\n- Case A (happy path, correlated and heteroscedastic sensor fusion):\n  - Geometry matrix $\\mathbf{R}_A$ with neighbor displacements $\\left[ (1, 0), (0, 1), (-1, 0), (0, -1) \\right]$.\n  - True gradient $\\mathbf{g}_{\\text{true},A} = [2, -1]$.\n  - Noise-free differences $\\mathbf{y}_{0,A} = \\mathbf{R}_A \\mathbf{g}_{\\text{true},A} = [2, -1, -2, 1]$.\n  - Deterministic noise sample $\\mathbf{n}_A = [0.3, -0.1, 0.05, 0.2]$, so $\\mathbf{y}_A = \\mathbf{y}_{0,A} + \\mathbf{n}_A = [2.3, -1.1, -1.95, 1.2]$.\n  - Covariance modeled by independent neighbor noise with variances $[0.25, 0.10, 0.04, 0.16]$ and a shared central measurement variance $0.09$, i.e., $\\mathbf{\\Sigma}_A = \\operatorname{diag}([0.25, 0.10, 0.04, 0.16]) + 0.09\\,\\mathbf{1}\\mathbf{1}^\\top$, where $\\mathbf{1} \\in \\mathbb{R}^4$ is the vector of ones.\n- Case B (boundary condition where the BLUE and ordinary least squares are equivalent):\n  - Geometry matrix $\\mathbf{R}_B = \\mathbf{R}_A$.\n  - True gradient $\\mathbf{g}_{\\text{true},B} = [1.5, -0.5]$.\n  - Noise-free differences $\\mathbf{y}_{0,B} = \\mathbf{R}_B \\mathbf{g}_{\\text{true},B}$ and deterministic noise sample $\\mathbf{n}_B = [0, 0, 0, 0]$, so $\\mathbf{y}_B = \\mathbf{y}_{0,B}$.\n  - Covariance $\\mathbf{\\Sigma}_B = \\sigma^2 \\mathbf{I}$ with $\\sigma^2 = 0.04$ and $\\mathbf{I}$ the identity matrix of size $4$.\n- Case C (edge case with near-collinear geometry and correlated fusion noise):\n  - Geometry matrix $\\mathbf{R}_C$ with neighbor displacements $\\left[ (1, 0), (2, 0), (0.01, 1), (-0.01, -1) \\right]$.\n  - True gradient $\\mathbf{g}_{\\text{true},C} = [0.5, 3.0]$.\n  - Noise-free differences $\\mathbf{y}_{0,C} = \\mathbf{R}_C \\mathbf{g}_{\\text{true},C} = [0.5, 1.0, 3.005, -3.005]$.\n  - Deterministic noise sample $\\mathbf{n}_C = [0.05, -0.02, 0.01, -0.01]$, so $\\mathbf{y}_C = [0.55, 0.98, 3.015, -3.015]$.\n  - Covariance $\\mathbf{\\Sigma}_C = \\operatorname{diag}([0.09, 0.09, 0.01, 0.01]) + 0.04\\,\\mathbf{1}\\mathbf{1}^\\top$.\n\nFor each case, compute:\n- The Euclidean norm $\\|\\mathbf{g}_{\\text{BLUE}} - \\mathbf{g}_{\\text{true}}\\|_2$ as a float.\n- A boolean that is true if and only if $\\operatorname{trace}(\\operatorname{Cov}[\\mathbf{g}_{\\text{BLUE}}]) \\le \\operatorname{trace}(\\operatorname{Cov}[\\mathbf{g}_{\\text{OLS}}])$, where OLS denotes ordinary least squares.\n\nFinal output format:\n- Your program should produce a single line of output containing the six results for Cases A, B, and C in order, as a comma-separated list enclosed in square brackets, specifically $[\\text{err}_A,\\text{valid}_A,\\text{err}_B,\\text{valid}_B,\\text{err}_C,\\text{valid}_C]$, where each $\\text{err}$ is a float and each $\\text{valid}$ is a boolean.",
            "solution": "We begin from the first-order Taylor expansion of the scalar field $\\phi(\\mathbf{x})$ around the cell center $\\mathbf{x}_0 \\in \\mathbb{R}^2$, which provides the approximation $\\phi(\\mathbf{x}_i) \\approx \\phi(\\mathbf{x}_0) + \\nabla \\phi(\\mathbf{x}_0) \\cdot \\mathbf{r}_i$, where $\\mathbf{r}_i = \\mathbf{x}_i - \\mathbf{x}_0$. Constructing the differences $y_i = \\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)$ yields the linear model\n$$\n\\mathbf{y} = \\mathbf{R}\\mathbf{g} + \\boldsymbol{\\eta},\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^m$ stacks the $y_i$, the matrix $\\mathbf{R} \\in \\mathbb{R}^{m \\times 2}$ stacks the displacement vectors $\\mathbf{r}_i^\\top$ as rows, the unknown gradient is $\\mathbf{g} = \\nabla \\phi(\\mathbf{x}_0) \\in \\mathbb{R}^2$, and the noise $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma})$ is zero-mean Gaussian with covariance $\\mathbf{\\Sigma} \\in \\mathbb{R}^{m \\times m}$ representing sensor-fusion uncertainty.\n\nUnder the Gauss–Markov framework and the Gaussian noise assumption, the Best Linear Unbiased Estimator (BLUE) minimizes the generalized least-squares quadratic form\n$$\nJ(\\mathbf{g}) = (\\mathbf{y} - \\mathbf{R}\\mathbf{g})^\\top \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{R}\\mathbf{g}),\n$$\nsubject to unbiasedness in the class of linear estimators. Taking the gradient of $J(\\mathbf{g})$ with respect to $\\mathbf{g}$ and setting it to zero yields the normal equations\n$$\n\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\,\\mathbf{g} = \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{y}.\n$$\nAssuming $\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}$ is invertible, the BLUE is\n$$\n\\mathbf{g}_{\\text{BLUE}} = \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{y}.\n$$\nTo establish unbiasedness, note that $\\mathbb{E}[\\mathbf{y}] = \\mathbf{R}\\mathbf{g}$, hence\n$$\n\\mathbb{E}[\\mathbf{g}_{\\text{BLUE}}] = \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbb{E}[\\mathbf{y}] = \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\mathbf{g} = \\mathbf{g}.\n$$\nThe estimator covariance follows from linearity: with $\\mathbf{A} = \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1}$,\n$$\n\\operatorname{Cov}[\\mathbf{g}_{\\text{BLUE}}] = \\mathbf{A}\\,\\operatorname{Cov}[\\mathbf{y}]\\,\\mathbf{A}^\\top = \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{\\Sigma} \\left(\\mathbf{\\Sigma}^{-1}\\right)^\\top \\mathbf{R} \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1} = \\left(\\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}\\right)^{-1}.\n$$\nThis last simplification uses $\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma} = \\mathbf{I}$ and symmetry of $\\mathbf{\\Sigma}$.\n\nFor ordinary least squares (OLS), the estimator ignoring $\\mathbf{\\Sigma}$ minimizes $\\|\\mathbf{y} - \\mathbf{R}\\mathbf{g}\\|_2^2$, yielding\n$$\n\\mathbf{g}_{\\text{OLS}} = \\left(\\mathbf{R}^\\top \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{y}.\n$$\nUnder the true noise covariance $\\mathbf{\\Sigma}$, the covariance of $\\mathbf{g}_{\\text{OLS}}$ is\n$$\n\\operatorname{Cov}[\\mathbf{g}_{\\text{OLS}}] = \\left(\\mathbf{R}^\\top \\mathbf{R}\\right)^{-1} \\mathbf{R}^\\top \\mathbf{\\Sigma} \\mathbf{R} \\left(\\mathbf{R}^\\top \\mathbf{R}\\right)^{-1}.\n$$\nBy the Gauss–Markov theorem, among all linear unbiased estimators, the BLUE achieves minimal estimator covariance in the positive semidefinite order. A scalar summary such as the trace satisfies $\\operatorname{trace}(\\operatorname{Cov}[\\mathbf{g}_{\\text{BLUE}}]) \\le \\operatorname{trace}(\\operatorname{Cov}[\\mathbf{g}_{\\text{OLS}}])$ for any positive definite $\\mathbf{\\Sigma}$ when $\\mathbf{R}$ has full column rank, with equality when $\\mathbf{\\Sigma}$ is proportional to the identity (isotropic noise).\n\nAlgorithmic design for robust computation:\n- For the BLUE, avoid explicit inversion of $\\mathbf{\\Sigma}$ by solving linear systems: compute $\\mathbf{A} = \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{R}$ and $\\mathbf{b} = \\mathbf{R}^\\top \\mathbf{\\Sigma}^{-1} \\mathbf{y}$ using solves of the form $\\mathbf{\\Sigma}\\mathbf{z} = \\mathbf{v}$, then solve $\\mathbf{A}\\mathbf{g} = \\mathbf{b}$.\n- For OLS, compute $\\mathbf{g}_{\\text{OLS}}$ via normal equations, and its covariance via the stated formula.\n- For each test case, compute the Euclidean norm $\\|\\mathbf{g}_{\\text{BLUE}} - \\mathbf{g}_{\\text{true}}\\|_2$ and the boolean validating the trace inequality. Aggregate results in the specified output format.\n\nApplying to the test suite:\n- Case A uses $\\mathbf{R}_A$ with four orthogonal neighbors, a nontrivial true gradient $\\mathbf{g}_{\\text{true},A}$, deterministic noise $\\mathbf{n}_A$, and a covariance $\\mathbf{\\Sigma}_A$ reflecting heteroscedastic independent neighbor variances plus a shared central measurement variance via a rank-one term $0.09\\,\\mathbf{1}\\mathbf{1}^\\top$.\n- Case B uses isotropic noise $\\mathbf{\\Sigma}_B = 0.04\\,\\mathbf{I}$, under which BLUE and OLS covariances are equal, so the trace inequality holds with equality.\n- Case C uses near-collinear geometry in $\\mathbf{R}_C$ and correlated fusion noise $\\mathbf{\\Sigma}_C$, challenging numerical conditioning while still maintaining full column rank, thus demonstrating the advantage of incorporating $\\mathbf{\\Sigma}$.\n\nThe program implements these computations exactly and prints a single line with the six results in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef blue_estimate(R, Sigma, y):\n    \"\"\"\n    Compute the BLUE (Best Linear Unbiased Estimator) gradient estimate and its covariance.\n    R: (m x 2) geometry matrix\n    Sigma: (m x m) covariance matrix (positive definite)\n    y: (m,) measurement differences\n    Returns: g_hat (2,), Cov_g (2 x 2)\n    \"\"\"\n    # Solve Sigma * Z = R for Z = Sigma^{-1} R without explicit inversion\n    Z = np.linalg.solve(Sigma, R)\n    # Solve Sigma * z_y = y for z_y = Sigma^{-1} y\n    z_y = np.linalg.solve(Sigma, y)\n    # Form normal equations\n    A = R.T @ Z           # R^T Sigma^{-1} R\n    b = R.T @ z_y         # R^T Sigma^{-1} y\n    # Solve for g_hat\n    g_hat = np.linalg.solve(A, b)\n    # Estimator covariance is (R^T Sigma^{-1} R)^{-1}\n    Cov_g = np.linalg.inv(A)\n    return g_hat, Cov_g\n\ndef ols_estimate(R, y, Sigma):\n    \"\"\"\n    Compute the ordinary least squares gradient estimate and its covariance under true Sigma.\n    R: (m x 2) geometry matrix\n    y: (m,) measurement differences\n    Sigma: (m x m) true covariance matrix\n    Returns: g_hat (2,), Cov_g (2 x 2)\n    \"\"\"\n    RtR = R.T @ R\n    RtR_inv = np.linalg.inv(RtR)\n    g_hat = RtR_inv @ (R.T @ y)\n    Cov_g = RtR_inv @ (R.T @ Sigma @ R) @ RtR_inv\n    return g_hat, Cov_g\n\ndef solve():\n    # Define test cases from the problem statement.\n\n    # Case A\n    R_A = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [-1.0, 0.0],\n        [0.0, -1.0]\n    ])\n    g_true_A = np.array([2.0, -1.0])\n    y0_A = R_A @ g_true_A  # [2, -1, -2, 1]\n    n_A = np.array([0.3, -0.1, 0.05, 0.2])\n    y_A = y0_A + n_A\n    diag_vars_A = np.diag([0.25, 0.10, 0.04, 0.16])\n    sigma0_sq_A = 0.09\n    Sigma_A = diag_vars_A + sigma0_sq_A * np.ones((4, 4))\n\n    # Case B\n    R_B = R_A.copy()\n    g_true_B = np.array([1.5, -0.5])\n    y0_B = R_B @ g_true_B\n    n_B = np.array([0.0, 0.0, 0.0, 0.0])\n    y_B = y0_B + n_B\n    sigma_sq_B = 0.04\n    Sigma_B = sigma_sq_B * np.eye(4)\n\n    # Case C\n    R_C = np.array([\n        [1.0, 0.0],\n        [2.0, 0.0],\n        [0.01, 1.0],\n        [-0.01, -1.0]\n    ])\n    g_true_C = np.array([0.5, 3.0])\n    y0_C = R_C @ g_true_C  # [0.5, 1.0, 3.005, -3.005]\n    n_C = np.array([0.05, -0.02, 0.01, -0.01])\n    y_C = y0_C + n_C       # [0.55, 0.98, 3.015, -3.015]\n    diag_vars_C = np.diag([0.09, 0.09, 0.01, 0.01])\n    sigma0_sq_C = 0.04\n    Sigma_C = diag_vars_C + sigma0_sq_C * np.ones((4, 4))\n\n    test_cases = [\n        (R_A, Sigma_A, y_A, g_true_A),\n        (R_B, Sigma_B, y_B, g_true_B),\n        (R_C, Sigma_C, y_C, g_true_C),\n    ]\n\n    results = []\n    for R, Sigma, y, g_true in test_cases:\n        # BLUE\n        g_blue, Cov_blue = blue_estimate(R, Sigma, y)\n        # OLS\n        g_ols, Cov_ols = ols_estimate(R, y, Sigma)\n        # Error norm for BLUE\n        err_blue = float(np.linalg.norm(g_blue - g_true))\n        # Trace comparison\n        trace_blue = float(np.trace(Cov_blue))\n        trace_ols = float(np.trace(Cov_ols))\n        valid = trace_blue <= trace_ols\n        results.extend([err_blue, valid])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}