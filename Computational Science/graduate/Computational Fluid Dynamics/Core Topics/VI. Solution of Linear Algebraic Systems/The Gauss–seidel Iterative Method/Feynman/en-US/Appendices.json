{
    "hands_on_practices": [
        {
            "introduction": "A powerful feature of numerical methods derived from physical conservation laws is that the resulting mathematical structures often inherit desirable properties. This practice explores the fundamental link between the physics of diffusion, the choice of boundary conditions, and the mathematical guarantee of convergence for the Gauss-Seidel method. By discretizing a one-dimensional diffusion problem with a Robin boundary condition, you will analyze how physical parameters directly influence the matrix properties—specifically, symmetry and positive definiteness—that ensure the iterative solution will converge .",
            "id": "3374017",
            "problem": "Consider steady one-dimensional diffusion of a scalar field $u(x)$ on the interval $x \\in [0,L]$ with constant diffusivity $k0$, governed by the conservation law $\\frac{d}{dx}\\left(k \\frac{du}{dx}\\right)=0$. Discretize this equation using the finite volume method (FVM) on a uniform mesh of $N$ control volumes with cell centers at $x_i = \\left(i-\\frac{1}{2}\\right)h$ for $i=1,2,\\dots,N$ and uniform spacing $h = \\frac{L}{N}$. At the left boundary $x=0$, impose a Robin (convective) boundary condition $-k \\frac{du}{dx}\\big|_{x=0} = h_c \\left(u(0) - u_{\\infty}\\right)$, with convective coefficient $h_c$ and ambient value $u_{\\infty}$. At the right boundary $x=L$, impose a Dirichlet boundary condition $u(L)=0$. Assemble the linear system $\\boldsymbol{A}\\boldsymbol{u}=\\boldsymbol{b}$ for the unknown vector of cell-centered values $\\boldsymbol{u}=(u_1,\\dots,u_N)^{\\top}$ using second-order face fluxes consistent with FVM.\n\nFrom fundamental conservation and face flux balance, derive the discrete operator coefficients at interior faces and boundary faces, and analyze the diagonal dominance of the resulting tridiagonal matrix $\\boldsymbol{A}$. Then, using the discrete energy form associated with diffusion and the Robin boundary contribution, determine the parameter ranges of the Robin coefficient $h_c$ for which the Gauss–Seidel iterative method (GS) without relaxation is guaranteed to converge, by ensuring that the coefficient matrix is symmetric positive definite (SPD).\n\nFinally, extract the two critical values of the Robin coefficient $h_c$ (as functions of $k$ and $h$) that delineate the boundary of the convergence region you identified. Express your final answer as a pair of symbolic expressions in a single row matrix. No numerical rounding is required.",
            "solution": "The finite volume method is applied to the governing equation $\\frac{d}{dx}\\left(k \\frac{du}{dx}\\right)=0$.\n\n**Interior Cells ($i=2, \\dots, N-1$):**\nIntegrating over control volume $i$ gives $J_{i+1/2} - J_{i-1/2} = 0$, where $J$ is the flux $k\\frac{du}{dx}$. Using second-order central differences for fluxes at faces $x_{i\\pm1/2}$:\n$$ k\\frac{u_{i+1}-u_i}{h} - k\\frac{u_i-u_{i-1}}{h} = 0 \\implies -\\frac{k}{h}u_{i-1} + \\frac{2k}{h}u_i - \\frac{k}{h}u_{i+1} = 0 $$\n\n**Right Boundary (Cell $N$):**\nThe Dirichlet condition $u(L)=0$ is applied. The flux balance for cell $N$ is $J_{N+1/2} - J_{N-1/2} = 0$. The flux at the boundary face $x=L$ is $J_{N+1/2} = k\\frac{u_N-u(L)}{h/2} = k\\frac{u_N-0}{h/2} = \\frac{2k}{h}u_N$.\n$$ \\frac{2k}{h}u_N + k\\frac{u_N-u_{N-1}}{h} = 0 \\implies -\\frac{k}{h}u_{N-1} + \\frac{3k}{h}u_N = 0 $$\nNote: The equation for fluxes out of cell $N$ is used here, so $J_{N+1/2} + J_{N-1/2, out} = 0$.\n\n**Left Boundary (Cell 1):**\nThe flux balance is $J_{3/2, out} + J_{1/2, out} = 0$. The flux out of the Robin boundary face $x=0$ is $J_{1/2, out} = -k\\frac{du}{dx}|_{x=0} = h_c(u(0)-u_\\infty)$. To discretize this, a symmetric formulation is used where the flux is related to the cell-center value $u_1$ and the ambient value $u_\\infty$ via a total resistance $R_{tot} = R_{cond} + R_{conv} = \\frac{h/2}{k} + \\frac{1}{h_c} = \\frac{hh_c+2k}{2kh_c}$. The flux is $J_{1/2, out} = \\frac{u_1-u_\\infty}{R_{tot}} = \\frac{2kh_c}{hh_c+2k}(u_1-u_\\infty)$. The balance equation is:\n$$ k\\frac{u_1-u_2}{h} + \\frac{2kh_c}{hh_c+2k}(u_1-u_\\infty) = 0 $$\nRearranging gives the first row of the system:\n$$ \\left(\\frac{k}{h} + \\frac{2kh_c}{hh_c+2k}\\right)u_1 - \\frac{k}{h}u_2 = \\frac{2kh_c u_\\infty}{hh_c+2k} $$\n\n**Matrix Analysis:**\nThe resulting matrix $\\boldsymbol{A}$ is symmetric and tridiagonal. The Gauss-Seidel method is guaranteed to converge if $\\boldsymbol{A}$ is symmetric positive definite (SPD). We check the condition for positive definiteness by analyzing the quadratic form $\\boldsymbol{u}^\\top \\boldsymbol{A} \\boldsymbol{u}$. This form can be expressed as a sum of squares representing physical dissipation:\n$$ \\boldsymbol{u}^\\top \\boldsymbol{A} \\boldsymbol{u} = \\frac{k}{h} \\sum_{i=1}^{N-1}(u_i-u_{i+1})^2 + \\frac{2k}{h} u_N^2 + \\left(\\frac{2kh_c}{hh_c+2k}\\right) u_1^2 $$\nFor this form to be positive for any non-zero vector $\\boldsymbol{u}$, all terms must be non-negative, and they must not all be zero unless $\\boldsymbol{u}=\\boldsymbol{0}$. The first two terms are always non-negative. For the entire expression to be positive definite, we require the coefficient of $u_1^2$ to be non-negative:\n$$ \\frac{2kh_c}{hh_c+2k} \\ge 0 $$\nSince $k,h0$, this inequality holds if either (1) $h_c \\ge 0$ (which makes the denominator positive) or (2) $h_c  0$ and $hh_c+2k  0$, which implies $h_c  -2k/h$.\nIf this condition holds, $\\boldsymbol{u}^\\top \\boldsymbol{A} \\boldsymbol{u} = 0$ implies $u_N=0$, $u_i=u_{i+1}$ for all $i$, and either $h_c=0$ or $u_1=0$. This chain of implications leads to $\\boldsymbol{u}=\\boldsymbol{0}$, confirming that $\\boldsymbol{A}$ is SPD.\nThe region of guaranteed convergence for Gauss-Seidel is for $h_c \\in (-\\infty, -2k/h) \\cup [0, \\infty)$. The boundary points that delineate this region are $-2k/h$ and $0$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{2k}{h}  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In the idealized world of exact arithmetic, iterative methods converge predictably based on matrix properties. However, on a real computer, finite-precision floating-point arithmetic can introduce subtle and sometimes confusing artifacts. This thought experiment investigates a common practical observation: the stagnation of the residual in a Gauss-Seidel solver on very fine grids. You will diagnose how catastrophic cancellation during the residual calculation can create a \"noise floor\" that masks true convergence, and explore defect correction as a robust strategy to overcome this numerical limitation .",
            "id": "3374015",
            "problem": "You are solving the one-dimensional Poisson problem $-u'' = f$ on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$. The domain is discretized with $N$ interior points and uniform mesh spacing $h = 1/(N+1)$, yielding the standard second-order central-difference linear system $A \\boldsymbol{u} = \\boldsymbol{b}$, where for interior index $i$,\n$$(A \\boldsymbol{u})_i = -\\frac{1}{h^2} u_{i-1} + \\frac{2}{h^2} u_i - \\frac{1}{h^2} u_{i+1}, \\quad b_i = f(x_i), \\quad x_i = i h.$$\nYou apply pointwise Gauss–Seidel iteration in finite precision arithmetic to approximate the discrete solution. Let the unit roundoff be $\\epsilon_{\\text{mach}}$ (for IEEE double, $\\epsilon_{\\text{mach}} \\approx 1.11\\times 10^{-16}$). The residual at iterate $\\boldsymbol{u}^{(k)}$ is defined as $\\boldsymbol{r}^{(k)} = \\boldsymbol{b} - A \\boldsymbol{u}^{(k)}$.\n\nOn very fine grids (small $h$), practitioners often observe that the norm of the computed residual $\\|\\boldsymbol{r}^{(k)}\\|_2$ stops decreasing after many Gauss–Seidel sweeps, even though the true algebraic error $\\boldsymbol{e}^{(k)} = \\boldsymbol{u}^\\star - \\boldsymbol{u}^{(k)}$ (with $A \\boldsymbol{u}^\\star = \\boldsymbol{b}$) could still be reduced in exact arithmetic. The phenomenon is attributed to cancellation and loss of significance in the formation of Gauss–Seidel updates and residuals.\n\nFrom first principles, and without invoking any specialized pre-derived stabilization formulas, reason about:\n- how the magnitudes of the discrete operator entries scale with $h$,\n- how floating-point rounding in the Gauss–Seidel update and in residual formation $\\boldsymbol{r} = \\boldsymbol{b} - A \\boldsymbol{u}$ leads to a stagnation floor for the computed residual that depends on $h$ and $\\epsilon_{\\text{mach}}$,\n- and what computational strategy can mitigate this apparent stagnation by separating the effect of roundoff in residual evaluation from the subsequent correction step.\n\nWhich option best captures both the correct $h$-scaling mechanism of the stagnation floor and a sound residual-correction (defect-correction) strategy suitable for computational fluid dynamics workflows?\n\nA. Because $a_{ii} = 2/h^2$ and $a_{i,i\\pm 1} = -1/h^2$, the intermediate sums in each Gauss–Seidel update and in residual computation involve cancellation among terms of magnitude $\\mathcal{O}(h^{-2})$. In floating point, the absolute error in each such sum is $\\mathcal{O}(\\epsilon_{\\text{mach}} h^{-2})$, so the update error in $u_i$ is $\\mathcal{O}(\\epsilon_{\\text{mach}})$ and the computed residual floor is $\\|\\boldsymbol{r}\\|_2 \\sim \\mathcal{O}(\\epsilon_{\\text{mach}} \\|A\\|_2 \\|\\boldsymbol{u}\\|_2) = \\mathcal{O}(\\epsilon_{\\text{mach}} h^{-2})$. A robust mitigation is to employ defect correction: periodically recompute $\\boldsymbol{r} = \\boldsymbol{b} - A \\boldsymbol{u}$ using compensated dot products or extended precision to suppress cancellation, then approximately solve $A \\boldsymbol{e} = \\boldsymbol{r}$ with a few Gauss–Seidel sweeps starting from $\\boldsymbol{e}=\\boldsymbol{0}$, and update $\\boldsymbol{u} \\leftarrow \\boldsymbol{u} + \\boldsymbol{e}$. Additionally, diagonally scale the system so that $a_{ii} \\approx 1$ (e.g., multiply both sides by $h^2$ to work with entries of order unity), which further reduces cancellation in subsequent iterations.\n\nB. On finer grids the computed residual floor decreases like $\\mathcal{O}(\\epsilon_{\\text{mach}} h^{2})$ because the discretization is second order; therefore apparent stagnation can be removed by increasing the over-relaxation factor $\\omega  1$ in Gauss–Seidel to push updates past roundoff, without any special residual recomputation.\n\nC. Switching from Gauss–Seidel to Jacobi removes cancellation because Jacobi uses only old iterates; thus the computed residual can always be driven down to machine precision independent of $h$, and no defect-correction step is necessary.\n\nD. The stagnation is an artifact of the stopping criterion; tightening the tolerance by a factor of $h^{2}$ suffices. Computing the residual as $A \\boldsymbol{u} - \\boldsymbol{b}$ instead of $\\boldsymbol{b} - A \\boldsymbol{u}$ also avoids cancellation, so no correction equation needs to be solved.",
            "solution": "The problem is to explain the observed stagnation of the residual norm in the Gauss-Seidel method for the discretized Poisson equation on fine grids and to identify a correct mitigation strategy. This requires analyzing the effects of finite-precision arithmetic.\n\n**1. Scaling of Operator Entries**\nThe matrix $A$ arises from the second-order central difference approximation of the negative second derivative, scaled by $1/h^2$. The entries $a_{ii} = 2/h^2$ and $a_{i,i\\pm 1} = -1/h^2$ scale as $\\mathcal{O}(h^{-2})$. As the grid is refined, $h \\to 0$, and the matrix entries become very large. The condition number of this matrix also scales as $\\mathcal{O}(h^{-2})$, meaning the system becomes increasingly ill-conditioned.\n\n**2. Residual Stagnation and Floating-Point Error**\nThe residual is $\\boldsymbol{r} = \\boldsymbol{b} - A \\boldsymbol{u}$. As the iteration converges, the discrete solution $\\boldsymbol{u}$ approaches the true discrete solution $\\boldsymbol{u}^\\star$. By construction, $(A \\boldsymbol{u})_i \\approx b_i$. The computation of $(A \\boldsymbol{u})_i = \\frac{1}{h^2} (-u_{i-1} + 2u_i - u_{i+1})$ involves adding terms of magnitude $\\mathcal{O}(h^{-2}\\|\\boldsymbol{u}\\|_{\\infty})$ which sum to a result of magnitude $\\mathcal{O}(\\|\\boldsymbol{b}\\|_{\\infty})$. This subtraction of nearly equal large numbers is a classic case of **catastrophic cancellation**.\n\nStandard floating-point error analysis shows that the absolute error in computing $(A \\boldsymbol{u})_i$ is proportional to $\\epsilon_{\\text{mach}}$ times the sum of the magnitudes of the terms involved. This error is of order $\\mathcal{O}(\\epsilon_{\\text{mach}} h^{-2})$. When the true residual $r_i$ becomes comparable in magnitude to this computational error, the computed residual $\\hat{r}_i$ is dominated by this floating-point noise. Further iteration cannot reduce the true error because the computed residual, which drives the iterative correction, no longer contains meaningful information about the true residual. The norm of the computed residual, $\\|\\hat{\\boldsymbol{r}}\\|$, stagnates at a \"noise floor\" that scales as $\\mathcal{O}(\\epsilon_{\\text{mach}} h^{-2})$.\n\n**3. Mitigation Strategy**\nThe core problem is the inaccurate computation of the residual $\\boldsymbol{r} = \\boldsymbol{b} - A \\boldsymbol{u}$ due to catastrophic cancellation. A robust strategy must address this. Two common and effective strategies are:\n\n*   **Iterative Refinement (Defect Correction):** This technique involves computing the residual $\\boldsymbol{r} = \\boldsymbol{b} - A \\boldsymbol{u}$ using **extended precision** or a compensated summation algorithm (like Kahan summation) to obtain an accurate residual despite the cancellation. Then, the correction equation $A \\boldsymbol{e} = \\boldsymbol{r}$ is solved approximately for the error vector $\\boldsymbol{e}$ (e.g., with a few Gauss-Seidel sweeps). Finally, the solution is updated: $\\boldsymbol{u} \\leftarrow \\boldsymbol{u} + \\boldsymbol{e}$. This separates the high-accuracy requirement (for the residual) from the main solver.\n\n*   **System Scaling:** An alternative is to rescale the linear system to improve its conditioning. Multiplying the entire equation by $h^2$ gives $(h^2 A) \\boldsymbol{u} = h^2 \\boldsymbol{b}$. The entries of the new matrix $A' = h^2 A$ are of order $\\mathcal{O}(1)$. This avoids the large intermediate terms and the associated cancellation, drastically lowering the residual noise floor.\n\n**Evaluation of Options:**\n\n*   **A:** This option correctly identifies the $\\mathcal{O}(h^{-2})$ scaling, the catastrophic cancellation, and the resulting residual floor scaling of $\\mathcal{O}(\\epsilon_{\\text{mach}} h^{-2})$. It proposes both defect correction (with extended precision) and system scaling as valid mitigation strategies. This is a comprehensive and correct answer.\n*   **B:** This option incorrectly claims the residual floor *decreases* with $h$, confusing roundoff error with truncation error. Successive over-relaxation (SOR) accelerates convergence but does not fix the underlying cancellation problem.\n*   **C:** This option falsely claims that the Jacobi method eliminates cancellation. The matrix-vector product $A \\boldsymbol{u}$ still needs to be computed, and it suffers from the same numerical issue.\n*   **D:** This option incorrectly dismisses the stagnation as an artifact of the stopping criterion and nonsensically suggests that computing $A \\boldsymbol{u} - \\boldsymbol{b}$ instead of $\\boldsymbol{b} - A \\boldsymbol{u}$ avoids cancellation.\n\nTherefore, option A provides the correct diagnosis and the standard, robust solutions to the problem.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Modern computational fluid dynamics often employs complex geometric techniques like cut-cell methods to accurately represent boundaries. These methods can, however, lead to numerical challenges, particularly when cells have very small volume fractions. This exercise provides a hands-on, code-based investigation into this issue using a simplified model matrix that captures the essence of the problem. You will derive the Gauss-Seidel iteration matrix, use its spectral radius to rigorously analyze convergence, and implement a diagonal dominance stabilization technique to ensure the solver remains robust even in these challenging, ill-conditioned scenarios .",
            "id": "3374003",
            "problem": "Consider the steady scalar diffusion (Poisson) operator in one spatial dimension discretized by a conservative finite-volume method on a uniform mesh, as used in Computational Fluid Dynamics (CFD). Let a single interior control volume be a cut-cell with small volume fraction $\\phi \\in (0,1]$, representing the portion of the full cell volume occupied by fluid. A naive volume-weighted assembly of the discrete operator for $-\\frac{d^2 u}{dx^2} = f$ can produce an asymmetric linear system that degrades as $\\phi \\to 0^+$. We study a minimal three-cell model that captures this degradation and its effect on the Gauss–Seidel (GS) iterative method.\n\nDefine the parametric $3 \\times 3$ matrix $A(\\phi)$ and the right-hand side $b$ as follows. Let the left and right boundaries enforce homogeneous Dirichlet conditions through ghost elimination, and place the cut-cell at the middle index. The naive cut-cell assembly is modeled by\n$$\nA(\\phi) \\equiv\n\\begin{bmatrix}\n2  -1  0 \\\\\n-\\phi  2\\phi  -\\phi \\\\\n0  -1  2\n\\end{bmatrix}, \\qquad b \\equiv \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nAll entries are dimensionless. The off-diagonal entries are nonpositive and the diagonal entries are positive for all $\\phi \\in (0,1]$. However, for very small $\\phi \\ll 1$, the middle row becomes weakly weighted and the lower-triangular part of the matrix, used by Gauss–Seidel, can approach singularity.\n\nYour tasks are:\n\n1) From first principles, starting with the standard matrix splitting $A = D + L + U$ into the diagonal part $D$, the strict lower-triangular part $L$, and the strict upper-triangular part $U$, derive the Gauss–Seidel iteration on the error for the linear system $A x = b$ and the associated iteration matrix. Your derivation must rely only on the definitions of matrix splitting, linear iterations, and the notion of spectral radius, and not assume any shortcut formulas. Use the following fundamental facts as the base:\n- The error $\\boldsymbol{e}^{(k)} \\equiv \\boldsymbol{x}^{(k)} - \\boldsymbol{x}^\\star$ evolves linearly under stationary iterations.\n- A linear stationary iteration converges if and only if the spectral radius of its iteration matrix is strictly less than $1$.\n\n2) Use the derived framework to determine, as a function of the volume fraction $\\phi$, when the Gauss–Seidel iteration diverges for the system $A(\\phi) x = b$ (that is, when the iteration matrix has spectral radius greater than or equal to $1$). Use the matrix $A(\\phi)$ given above and do not simplify it further.\n\n3) Propose and justify a stabilization that restores strict diagonal dominance in the cut-cell row without altering the sign pattern of the off-diagonal entries. Implement the stabilization as a local diagonal augmentation applied row-wise:\n$$\n\\tilde{a}_{ii} \\leftarrow \\max\\Big(a_{ii}, \\, (1+\\sigma)\\sum_{j\\ne i} |a_{ij}|\\Big),\n$$\nwith a chosen small stabilization parameter $\\sigma  0$. Explain why this modification recovers strict diagonal dominance and how that re-establishes convergence of Gauss–Seidel.\n\n4) Implement the complete analysis as a program that, for a set of prescribed volume fractions $\\phi$, constructs $A(\\phi)$, computes the Gauss–Seidel iteration matrix using your derived process, evaluates the spectral radius, and classifies convergence (boolean) both before and after stabilization. Use $\\sigma = 0.05$.\n\n5) Test Suite. Use the following set of volume fractions that cover a nominal case, moderately small cut-cell, and extreme sliver cells:\n- $\\phi = 1.0$,\n- $\\phi = 0.2$,\n- $\\phi = 0.05$,\n- $\\phi = 0.01$,\n- $\\phi = 10^{-4}$,\n- $\\phi = 10^{-8}$.\n\n6) Output specification. Your program should produce a single line of output containing the results as a comma-separated list of four lists enclosed in square brackets:\n- First list: spectral radii for the unstabilized matrices $A(\\phi)$ in the order of the test suite, each rounded to six decimal places as a decimal number.\n- Second list: spectral radii for the stabilized matrices $\\tilde{A}(\\phi)$ in the same order, each rounded to six decimal places as a decimal number.\n- Third list: boolean convergence classifications for the unstabilized cases, where $true$ indicates convergence (spectral radius strictly less than $1$) and $false$ indicates divergence (spectral radius greater than or equal to $1$).\n- Fourth list: boolean convergence classifications for the stabilized cases, same convention.\n\nFor example, your program must print a single line in the form\n$[ [r_1,\\dots,r_6], [s_1,\\dots,s_6], [c_1,\\dots,c_6], [d_1,\\dots,d_6] ]$\nwith no extra whitespace or explanatory text, where $r_i, s_i$ are floats and $c_i, d_i$ are booleans.\n\nAll computations are unitless. Angles are not used. The final output must strictly adhere to the specified single-line format. The only allowed operations are those implementable with real arithmetic and standard linear algebra routines.",
            "solution": "### Part 1: Derivation of the Gauss–Seidel Iteration Matrix\n\nLet the linear system be $A \\boldsymbol{x} = \\boldsymbol{b}$. Decompose the matrix $A$ into its diagonal part $D$, strict lower-triangular part $L$, and strict upper-triangular part $U$, such that $A = D + L + U$. The Gauss–Seidel iterative method updates the solution vector $\\boldsymbol{x}^{(k+1)}$ from $\\boldsymbol{x}^{(k)}$ using the equation:\n$$\n(D+L) \\boldsymbol{x}^{(k+1)} = -U \\boldsymbol{x}^{(k)} + \\boldsymbol{b}\n$$\nAssuming $D+L$ is invertible, the iteration is $\\boldsymbol{x}^{(k+1)} = -(D+L)^{-1} U \\boldsymbol{x}^{(k)} + (D+L)^{-1} \\boldsymbol{b}$. This is a stationary linear iteration with the Gauss–Seidel iteration matrix $T_{GS} = -(D+L)^{-1} U$.\n\nTo analyze convergence, we examine the evolution of the error, $\\boldsymbol{e}^{(k)} = \\boldsymbol{x}^{(k)} - \\boldsymbol{x}^\\star$, where $\\boldsymbol{x}^\\star$ is the exact solution satisfying $A \\boldsymbol{x}^\\star = \\boldsymbol{b}$.\nSubtracting $\\boldsymbol{x}^\\star = -(D+L)^{-1} U \\boldsymbol{x}^\\star + (D+L)^{-1} \\boldsymbol{b}$ from the iteration equation gives:\n$$\n\\boldsymbol{x}^{(k+1)} - \\boldsymbol{x}^\\star = -(D+L)^{-1} U (\\boldsymbol{x}^{(k)} - \\boldsymbol{x}^\\star)\n$$\nThus, the error evolves as $\\boldsymbol{e}^{(k+1)} = T_{GS} \\boldsymbol{e}^{(k)}$. The iteration converges for any initial error if and only if the spectral radius of the iteration matrix, $\\rho(T_{GS})$, is strictly less than $1$.\n\n### Part 2: Convergence Analysis for the Unstabilized System\n\nGiven the matrix $A(\\phi)$, we decompose it into $D$, $L$, and $U$:\n$$\nD = \\begin{bmatrix} 2  0  0 \\\\ 0  2\\phi  0 \\\\ 0  0  2 \\end{bmatrix}, \\quad\nL = \\begin{bmatrix} 0  0  0 \\\\ -\\phi  0  0 \\\\ 0  -1  0 \\end{bmatrix}, \\quad\nU = \\begin{bmatrix} 0  -1  0 \\\\ 0  0  -\\phi \\\\ 0  0  0 \\end{bmatrix}\n$$\nThe matrix $(D+L)$ and its inverse are:\n$$\nD+L = \\begin{bmatrix} 2  0  0 \\\\ -\\phi  2\\phi  0 \\\\ 0  -1  2 \\end{bmatrix}, \\quad\n(D+L)^{-1} = \\begin{bmatrix} 1/2  0  0 \\\\ 1/4  1/(2\\phi)  0 \\\\ 1/8  1/(4\\phi)  1/2 \\end{bmatrix}\n$$\nThe iteration matrix is $T_{GS} = -(D+L)^{-1}U$:\n$$\nT_{GS} = - \\begin{bmatrix} 1/2  0  0 \\\\ 1/4  1/(2\\phi)  0 \\\\ 1/8  1/(4\\phi)  1/2 \\end{bmatrix} \\begin{bmatrix} 0  -1  0 \\\\ 0  0  -\\phi \\\\ 0  0  0 \\end{bmatrix}\n= \\begin{bmatrix}\n0  1/2  0 \\\\\n0  1/4  1/2 \\\\\n0  1/8  1/4\n\\end{bmatrix}\n$$\nSurprisingly, the parameter $\\phi$ cancels out, and the iteration matrix $T_{GS}$ is constant for all $\\phi \\in (0, 1]$. To determine convergence, we find its spectral radius. The eigenvalues $\\lambda$ are the roots of $\\det(T_{GS} - \\lambda I) = 0$. Since the matrix is block upper triangular, one eigenvalue is $\\lambda_1 = 0$. The others are eigenvalues of the lower-right $2 \\times 2$ submatrix, given by $(\\frac{1}{4}-\\lambda)^2 - (\\frac{1}{2})(\\frac{1}{8}) = 0$. This simplifies to $(\\frac{1}{4}-\\lambda)^2 = \\frac{1}{16}$, which yields $\\lambda = 0$ and $\\lambda = 1/2$. The set of eigenvalues is $\\{0, 0, 1/2\\}$.\nThe spectral radius is $\\rho(T_{GS}) = \\max\\{|0|, |0|, |1/2|\\} = 1/2$.\n\nSince $\\rho(T_{GS}) = 1/2  1$, the Gauss–Seidel iteration for this system converges for all $\\phi \\in (0,1]$. The set of $\\phi$ for which the iteration diverges is the empty set.\n\n### Part 3: Stabilization\n\nThe goal is to restore strict diagonal dominance using the rule:\n$$\n\\tilde{a}_{ii} \\leftarrow \\max\\Big(a_{ii}, \\, (1+\\sigma)\\sum_{j\\ne i} |a_{ij}|\\Big)\n$$\nwith $\\sigma = 0.05$. We apply this to each row of $A(\\phi)$. Only the second row is modified:\n$a_{22}=2\\phi$. $\\sum_{j\\ne 2}|a_{2j}| = |-\\phi| + |-\\phi| = 2\\phi$. The new value is $\\tilde{a}_{22} = \\max(2\\phi, (1+0.05) \\times 2\\phi) = 2.1\\phi$.\nThe stabilized matrix, $\\tilde{A}(\\phi)$, is:\n$$\n\\tilde{A}(\\phi) =\n\\begin{bmatrix}\n2  -1  0 \\\\\n-\\phi  2.1\\phi  -\\phi \\\\\n0  -1  2\n\\end{bmatrix}\n$$\n**Justification:** By construction, for every row $i$, the new diagonal entry satisfies $|\\tilde{a}_{ii}| \\ge (1+\\sigma)\\sum_{j\\ne i}|a_{ij}|$. Since $\\sigma  0$, this implies $|\\tilde{a}_{ii}|  \\sum_{j\\ne i}|a_{ij}|$. This is the definition of strict diagonal dominance. For a strictly diagonally dominant matrix, the Gauss-Seidel method is guaranteed to converge.\n\n### Part 4: Implementation and Analysis of Stabilized System\n\nWe compute the spectral radius for the stabilized system. The new iteration matrix $\\tilde{T}_{GS}$ is computed similarly:\n$$\n\\tilde{T}_{GS} = \\begin{bmatrix}\n0  1/2  0 \\\\\n0  1/4.2  1/2.1 \\\\\n0  1/8.4  1/4.2\n\\end{bmatrix}\n$$\nAgain, the matrix is independent of $\\phi$. Its eigenvalues are $\\{0, 0, 2/4.2\\}$.\nThe spectral radius is $\\rho(\\tilde{T}_{GS}) = 1/2.1 \\approx 0.47619$. This is less than the original spectral radius of $0.5$, indicating a slightly faster convergence rate. Both the original and stabilized systems converge for all tested $\\phi$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the convergence of the Gauss-Seidel method for a parametric\n    cut-cell matrix model before and after stabilization.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    phi_values = [\n        1.0,\n        0.2,\n        0.05,\n        0.01,\n        1e-4,\n        1e-8,\n    ]\n\n    sigma = 0.05\n\n    radii_unstable = []\n    radii_stable = []\n    conv_unstable = []\n    conv_stable = []\n\n    for phi in phi_values:\n        # 1. Unstabilized Case\n        A_unstable = np.array([\n            [2.0, -1.0, 0.0],\n            [-phi, 2.0 * phi, -phi],\n            [0.0, -1.0, 2.0]\n        ])\n\n        # Decompose A = D + L + U\n        D_unstable = np.diag(np.diag(A_unstable))\n        L_unstable = np.tril(A_unstable, k=-1)\n        U_unstable = np.triu(A_unstable, k=1)\n\n        # Gauss-Seidel iteration matrix T_GS = -(D+L)^-1 * U\n        DL_inv_unstable = np.linalg.inv(D_unstable + L_unstable)\n        T_gs_unstable = -DL_inv_unstable @ U_unstable\n        \n        # Spectral radius is max absolute eigenvalue\n        eigvals_unstable = np.linalg.eigvals(T_gs_unstable)\n        rho_unstable = np.max(np.abs(eigvals_unstable))\n        \n        radii_unstable.append(f\"{rho_unstable:.6f}\")\n        conv_unstable.append(rho_unstable  1.0)\n\n        # 2. Stabilized Case\n        A_stable = A_unstable.copy()\n        \n        # Apply row-wise stabilization\n        for i in range(A_stable.shape[0]):\n            a_ii = A_stable[i, i]\n            # Sum of absolute values of off-diagonal entries\n            sum_abs_off_diag = np.sum(np.abs(A_stable[i, :])) - np.abs(a_ii)\n            \n            # Stabilization rule\n            stabilized_diag = (1 + sigma) * sum_abs_off_diag\n            \n            A_stable[i, i] = max(a_ii, stabilized_diag)\n\n        # Decompose stabilized A\n        D_stable = np.diag(np.diag(A_stable))\n        L_stable = np.tril(A_stable, k=-1)\n        U_stable = np.triu(A_stable, k=1)\n\n        # Gauss-Seidel iteration matrix for stabilized system\n        DL_inv_stable = np.linalg.inv(D_stable + L_stable)\n        T_gs_stable = -DL_inv_stable @ U_stable\n\n        # Spectral radius for stabilized system\n        eigvals_stable = np.linalg.eigvals(T_gs_stable)\n        rho_stable = np.max(np.abs(eigvals_stable))\n\n        radii_stable.append(f\"{rho_stable:.6f}\")\n        conv_stable.append(rho_stable  1.0)\n\n    # Convert boolean lists to lowercase strings as required\n    conv_unstable_str = [str(c).lower() for c in conv_unstable]\n    conv_stable_str = [str(c).lower() for c in conv_stable]\n    \n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(radii_unstable)}],[{','.join(radii_stable)}],[{','.join(conv_unstable_str)}],[{','.join(conv_stable_str)}]]\")\n\nsolve()\n```"
        }
    ]
}