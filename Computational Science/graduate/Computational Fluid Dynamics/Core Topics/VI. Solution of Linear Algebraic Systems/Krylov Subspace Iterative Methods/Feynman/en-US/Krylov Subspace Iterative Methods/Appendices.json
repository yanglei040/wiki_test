{
    "hands_on_practices": [
        {
            "introduction": "The Conjugate Gradient (CG) method is a cornerstone of iterative solvers, celebrated for its efficiency on symmetric positive-definite (SPD) systems common in CFD, such as the discrete Poisson equation for pressure correction. This exercise invites you to manually perform the first two iterations of CG on a model 1D Poisson problem . By deriving each step from first principles, you will gain a concrete understanding of how the method constructs $A$-orthogonal search directions to minimize the energy norm of the error with each step.",
            "id": "3338514",
            "problem": "In the projection methods for incompressible flow in computational fluid dynamics, the pressure field at each timestep is obtained by solving a discrete Poisson equation with homogeneous Dirichlet boundary conditions. Consider the one-dimensional model problem on the unit interval with zero Dirichlet boundary conditions discretized on a uniform grid of $n=5$ interior points, leading to a sparse symmetric positive definite linear system $A x = b$ with the standard three-point stencil. Work with the nondimensionalized discrete operator that absorbs the grid spacing factor into the right-hand side, so that the coefficient matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals, that is,\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{pmatrix}.\n$$\nAssume a localized forcing at the first interior node, so that the right-hand side is $b = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix}^{T}$, and choose the initial guess $x_{0} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\end{pmatrix}^{T}$.\n\nStarting from first principles for Krylov subspace iterative methods and the Conjugate Gradient (CG) method—namely, that $A$ is symmetric positive definite, the residual is $r_{k} = b - A x_{k}$, search directions are $A$-conjugate, and each step minimizes the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{T} A x - b^{T} x$ along the affine subspace—derive the first two iterations of CG applied to this system. Specifically, construct $p_{0}$, compute the step length $\\alpha_{0}$ by enforcing the line-minimization along $p_{0}$, update $x_{1}$ and $r_{1}$, derive the coefficient $\\beta_{0}$ by enforcing $A$-conjugacy of $p_{1}$ with $p_{0}$, and then compute $\\alpha_{1}$, $x_{2}$, and $r_{2}$.\n\nReport the final result as follows:\n- Provide the five components of $x_{2}$ followed by the value of $\\|r_{2}\\|_{2}$, all as exact rational values.\n- Express your final answer as a single row using parentheses, listing the entries of $x_{2}$ in order and then $\\|r_{2}\\|_{2}$ as the last entry, with no units.",
            "solution": "The task is to compute the first two iterations of the Conjugate Gradient (CG) method for the linear system $A x = b$, where the matrix $A$, the right-hand side vector $b$, and the initial guess $x_0$ are given by:\n$$ A = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe matrix $A$ is symmetric and positive definite, which is a prerequisite for the CG method. The CG algorithm iteratively constructs a sequence of approximate solutions $x_k$ that minimize the quadratic functional $\\phi(x) = \\frac{1}{2} x^T A x - b^T x$.\n\n**Initialization ($k=0$)**\n\nFirst, we compute the initial residual, $r_0$. The residual is defined as $r_k = b - A x_k$.\nFor $k=0$, we have:\n$$ r_0 = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe initial search direction, $p_0$, is set equal to the initial residual:\n$$ p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\n**First Iteration ($k=0$)**\n\nThe first step is to compute the optimal step length $\\alpha_0$ that minimizes $\\phi(x_0 + \\alpha_0 p_0)$. This condition is equivalent to making the new residual $r_1$ orthogonal to the search direction $p_0$, i.e., $r_1^T p_0 = 0$. Since $r_1 = r_0 - \\alpha_0 A p_0$, we have $(r_0 - \\alpha_0 A p_0)^T p_0 = 0$, which gives the formula for $\\alpha_0$:\n$$ \\alpha_0 = \\frac{r_0^T p_0}{p_0^T A p_0} $$\nSince $p_0 = r_0$, this simplifies to $\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0}$. First, we compute the numerator:\n$$ r_0^T r_0 = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1 $$\nNext, we compute the matrix-vector product $A p_0$:\n$$ A p_0 = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNow, we compute the denominator for $\\alpha_0$:\n$$ p_0^T A p_0 = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 2 $$\nThus, the step length $\\alpha_0$ is:\n$$ \\alpha_0 = \\frac{1}{2} $$\nWe can now update the solution vector $x_1$:\n$$ x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nAnd the residual vector $r_1$:\n$$ r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ 0 - (-\\frac{1}{2}) \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\n**Second Iteration ($k=1$)**\n\nFirst, we determine the new search direction $p_1 = r_1 + \\beta_0 p_0$. The coefficient $\\beta_0$ is chosen to enforce $A$-conjugacy between $p_1$ and $p_0$, i.e., $p_1^T A p_0 = 0$.\n$$ (r_1 + \\beta_0 p_0)^T A p_0 = 0 \\implies r_1^T A p_0 + \\beta_0 p_0^T A p_0 = 0 $$\nSolving for $\\beta_0$:\n$$ \\beta_0 = - \\frac{r_1^T A p_0}{p_0^T A p_0} $$\nA common alternative formula is $\\beta_0 = (r_1^T r_1) / (r_0^T r_0)$. Let's use this simpler form. $r_1^T r_1 = (1/2)^2 = 1/4$ and $r_0^T r_0 = 1$, yielding $\\beta_0 = 1/4$.\n\nNow we construct the new search direction $p_1$:\n$$ p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNext, we compute the step length $\\alpha_1$:\n$$ \\alpha_1 = \\frac{r_1^T r_1}{p_1^T A p_1} $$\nThe numerator is $r_1^T r_1 = 1/4$. For the denominator, we first need $A p_1$:\n$$ A p_1 = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(\\frac{1}{4}) - 1(\\frac{1}{2}) \\\\ -1(\\frac{1}{4}) + 2(\\frac{1}{2}) \\\\ 0 - 1(\\frac{1}{2}) \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThen, the denominator is:\n$$ p_1^T A p_1 = \\begin{pmatrix} 1/4 & 1/2 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = (\\frac{1}{4})(0) + (\\frac{1}{2})(\\frac{3}{4}) = \\frac{3}{8} $$\nSo the step length $\\alpha_1$ is:\n$$ \\alpha_1 = \\frac{1/4}{3/8} = \\frac{1}{4} \\cdot \\frac{8}{3} = \\frac{2}{3} $$\nWe update the solution to get $x_2$:\n$$ x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{2}{3} \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/6 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3/6 + 1/6 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nFinally, we compute the new residual $r_2$:\n$$ r_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{2}{3} \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1/2 \\\\ -1/3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe problem requires the five components of $x_2$ and the Euclidean norm of $r_2$, denoted $\\|r_2\\|_2$.\nThe components of $x_2$ are $\\frac{2}{3}$, $\\frac{1}{3}$, $0$, $0$, $0$.\nThe norm of $r_2$ is:\n$$ \\|r_2\\|_2 = \\sqrt{0^2 + 0^2 + (1/3)^2 + 0^2 + 0^2} = \\sqrt{\\frac{1}{9}} = \\frac{1}{3} $$\nThe final result consists of the five components of $x_2$ and the value of $\\|r_2\\|_2$.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} & 0 & 0 & 0 & \\frac{1}{3} \\end{pmatrix}} $$"
        },
        {
            "introduction": "While the Conjugate Gradient method is powerful, many CFD problems involving advection result in non-symmetric linear systems where it is inapplicable. The Generalized Minimal Residual (GMRES) method addresses this by minimizing the Euclidean norm of the residual over the Krylov subspace, using an orthonormal basis generated by the Arnoldi process. This practice problem tasks you with executing a single GMRES iteration for a simple non-symmetric system , revealing the core mechanism of solving a small least-squares problem to find the optimal update at each step.",
            "id": "3338495",
            "problem": "In computational fluid dynamics (CFD), implicit time integration and linearization of transport-diffusion operators lead to linear systems of the form $A x = b$ that must be solved efficiently. Krylov subspace iterative methods construct approximations by minimizing the residual in spaces generated by repeated applications of the system matrix. Consider the linear system defined by the $3 \\times 3$ matrix\n$$\nA = \\begin{pmatrix}\n4 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n$$\nand the right-hand side\n$$\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix},\n$$\nwith the initial approximation $x_{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$. Using the Generalized Minimal Residual (GMRES) method, perform exactly one iteration (that is, construct the $1$-dimensional Krylov subspace and compute the corresponding GMRES update based on the Arnoldi process with the Euclidean inner product) to obtain $x_{1}$. Then, compute the Euclidean norm of the residual $r_{1} = b - A x_{1}$.\n\nExpress the final residual norm in exact form with no rounding. No physical units are required.",
            "solution": "The GMRES method generates an approximate solution $x_m$ from the affine space $x_0 + \\mathcal{K}_m(A, r_0)$, where $\\mathcal{K}_m(A, r_0)$ is the $m$-th Krylov subspace. The solution $x_m$ is chosen to minimize the Euclidean norm of the residual, $\\|b - A x_m\\|_2$. We perform the steps for one iteration, where $m=1$.\n\nFirst, we compute the initial residual, $r_0$.\n$$\nr_0 = b - A x_0\n$$\nGiven $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, the term $A x_0$ is the zero vector. Therefore, the initial residual is equal to $b$:\n$$\nr_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nNext, we compute the Euclidean norm of $r_0$:\n$$\n\\|r_0\\|_2 = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\n$$\nThe GMRES method constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$ using the Arnoldi process. For $m=1$, this basis consists of a single vector, $v_1$, which is the normalized initial residual.\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe updated approximation, $x_1$, is sought in the form $x_1 = x_0 + z_1$, where $z_1 \\in \\mathcal{K}_1(A, r_0)$. This means $z_1$ can be written as a scalar multiple of $v_1$, i.e., $z_1 = y_1 v_1$ for some scalar $y_1 \\in \\mathbb{R}$. The GMRES method finds the optimal $y_1$ by solving a least-squares problem.\nThe approximation is $x_1 = x_0 + y_1 v_1$.\nThe corresponding residual is $r_1 = b - A x_1 = b - A(x_0 + y_1 v_1) = (b - A x_0) - y_1 A v_1 = r_0 - y_1 A v_1$.\nThe coefficient $y_1$ is chosen to minimize $\\|r_1\\|_2 = \\|r_0 - y_1 A v_1\\|_2$. This is a classic linear least-squares problem, where we are finding the best approximation of the vector $r_0$ in the subspace spanned by $A v_1$. The solution for $y_1$ is the coefficient of the orthogonal projection of $r_0$ onto $A v_1$:\n$$\ny_1 = \\frac{(A v_1)^T r_0}{(A v_1)^T (A v_1)} = \\frac{(A v_1)^T r_0}{\\|A v_1\\|_2^2}\n$$\nWe compute the necessary components. First, the vector $A v_1$:\n$$\nA v_1 = \\begin{pmatrix} 4 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 4(1)+1(1)+0(1) \\\\ 0(1)+3(1)+1(1) \\\\ 0(1)+0(1)+2(1) \\end{pmatrix} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nNow, we calculate the numerator of the expression for $y_1$:\n$$\n(A v_1)^T r_0 = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 & 4 & 2 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{3}}(5 \\cdot 1 + 4 \\cdot 1 + 2 \\cdot 1) = \\frac{11}{\\sqrt{3}}\n$$\nNext, we calculate the denominator:\n$$\n\\|A v_1\\|_2^2 = \\left\\| \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} \\right\\|_2^2 = \\frac{1}{(\\sqrt{3})^2} (5^2 + 4^2 + 2^2) = \\frac{1}{3}(25 + 16 + 4) = \\frac{45}{3} = 15\n$$\nNow we can compute $y_1$:\n$$\ny_1 = \\frac{11/\\sqrt{3}}{15} = \\frac{11}{15\\sqrt{3}}\n$$\nWe can now find the new approximation $x_1$:\n$$\nx_1 = x_0 + y_1 v_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{11}{15\\sqrt{3}} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{11}{15 \\cdot 3} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe problem asks for the Euclidean norm of the residual $r_1 = b - A x_1$. Let's compute $r_1$:\n$$\nA x_1 = \\begin{pmatrix} 4 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix} \\left( \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{11}{45} \\begin{pmatrix} 4(1)+1(1)+0(1) \\\\ 0(1)+3(1)+1(1) \\\\ 0(1)+0(1)+2(1) \\end{pmatrix} = \\frac{11}{45} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\n$$\nr_1 = b - A x_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{11}{45} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{55}{45} \\\\ 1 - \\frac{44}{45} \\\\ 1 - \\frac{22}{45} \\end{pmatrix} = \\begin{pmatrix} \\frac{45-55}{45} \\\\ \\frac{45-44}{45} \\\\ \\frac{45-22}{45} \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix}\n$$\nFinally, we compute the Euclidean norm of $r_1$:\n$$\n\\|r_1\\|_2 = \\left\\| \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix} \\right\\|_2 = \\frac{1}{45} \\sqrt{(-10)^2 + 1^2 + 23^2}\n$$\nThe terms inside the square root are:\n$(-10)^2 = 100$\n$1^2 = 1$\n$23^2 = 529$\nSumming these gives $100 + 1 + 529 = 630$. So,\n$$\n\\|r_1\\|_2 = \\frac{\\sqrt{630}}{45}\n$$\nTo simplify the expression, we factor the number $630$:\n$$\n630 = 63 \\times 10 = (9 \\times 7) \\times (2 \\times 5) = 3^2 \\times 70\n$$\nTherefore, $\\sqrt{630} = \\sqrt{3^2 \\times 70} = 3\\sqrt{70}$.\nSubstituting this back into the expression for the norm:\n$$\n\\|r_1\\|_2 = \\frac{3\\sqrt{70}}{45} = \\frac{\\sqrt{70}}{15}\n$$\nThis is the final exact value for the Euclidean norm of the residual after one iteration of GMRES.",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{70}}{15}}\n$$"
        },
        {
            "introduction": "For large-scale CFD simulations, the convergence of Krylov methods can be prohibitively slow due to the ill-conditioning of the system matrix $A$. Preconditioning transforms the system into an equivalent one, such as $M^{-1}Ax = M^{-1}b$, with better spectral properties. This exercise guides you through constructing an Incomplete LU factorization with zero fill-in (ILU($0$)), a widely used and effective preconditioner for matrices arising from discretized PDEs . Understanding how to build this approximate factorization is a crucial practical skill for accelerating iterative solvers.",
            "id": "3338499",
            "problem": "Consider the standard two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the unit square, discretized by the second-order central difference method on a uniform grid with $3 \\times 3$ interior points. Use lexicographic ordering by rows (increasing $x$ within each fixed $y$) to enumerate the $9$ unknowns. The resulting linear system has coefficient matrix $A \\in \\mathbb{R}^{9 \\times 9}$, which is the $5$-point discrete Laplacian with diagonal entries $4$ and off-diagonal entries $-1$ corresponding to immediate neighbors in the grid graph (left, right, up, down), and zeros elsewhere. The uniform grid spacing factor is common to all entries and may be omitted.\n\nIn order to design a preconditioner for a Krylov subspace method within computational fluid dynamics, construct the Incomplete Lower-Upper factorization with zero fill (ILU($0$)) of $A$ with a unit lower triangular factor $L$ and an upper triangular factor $U$. By definition of ILU($0$), the factorization mimics Gaussian elimination while discarding any fill-in outside the original sparsity pattern of $A$, and $L$ is constrained to have unit diagonal.\n\nYour tasks are:\n- Starting from first principles (the discrete conservation balance leading to the $5$-point Laplacian and the definition of ILU($0$) as a drop-tolerance-free incomplete factorization that preserves the sparsity pattern of $A$), derive the elimination relations that determine the entries of $L$ and $U$ under ILU($0$), and apply them to this $9 \\times 9$ problem under the specified ordering.\n- From this derivation, explicitly characterize the sparsity patterns of $L$ and $U$ in terms of grid-neighbor couplings (which neighbor directions are present in $L$ and which in $U$, along with the diagonals).\n- Finally, compute the total number of nonzero entries in the incomplete factors $L$ and $U$ combined, that is, compute $\\operatorname{nnz}(L) + \\operatorname{nnz}(U)$ as a single integer.\n\nState the final answer as a single integer. No rounding is necessary and no physical units are required.",
            "solution": "The problem asks for the total number of nonzero entries in the ILU($0$) factors of the matrix representation of the $5$-point discrete Laplacian on a $3 \\times 3$ grid of interior points. Let the number of interior grid points in the $x$ and $y$ directions be $n_x=3$ and $n_y=3$, respectively. The total number of unknowns is $N = n_x n_y = 3 \\times 3 = 9$.\n\nThe unknowns $u_{ij}$, representing the solution at interior grid point $(x_i, y_j)$ for $i,j \\in \\{1, 2, 3\\}$, are ordered using lexicographic ordering by rows. This maps the two-dimensional grid index $(i,j)$ to a single one-dimensional vector index $k$ as $k = (j-1)n_x + i$.\nThe grid points and their corresponding indices $k$ are:\n$k=7: (i=1, j=3)$, $k=8: (i=2, j=3)$, $k=9: (i=3, j=3)$\n$k=4: (i=1, j=2)$, $k=5: (i=2, j=2)$, $k=6: (i=3, j=2)$\n$k=1: (i=1, j=1)$, $k=2: (i=2, j=1)$, $k=3: (i=3, j=1)$\n\nThe second-order central difference discretization of the Poisson equation, $-\\nabla^2 u = f$, at an interior point $(i,j)$ yields the 5-point stencil equation (omitting the grid spacing factor $h^2$ as instructed):\n$$4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = f_{i,j}$$\nThis structure defines the coefficient matrix $A \\in \\mathbb{R}^{9 \\times 9}$. For a given unknown $k$ corresponding to grid point $(i,j)$, the matrix row $k$ will have:\n- A diagonal entry $A_{k,k} = 4$.\n- Off-diagonal entries $A_{k,l} = -1$ if unknown $l$ corresponds to a direct grid neighbor of $(i,j)$. The neighbors are:\n  - West: $(i-1, j)$, with index $l=k-1$ (if $i>1$).\n  - East: $(i+1, j)$, with index $l=k+1$ (if $i<n_x$).\n  - South: $(i, j-1)$, with index $l=k-n_x$ (if $j>1$).\n  - North: $(i, j+1)$, with index $l=k+n_x$ (if $j<n_y$).\n- All other entries $A_{k,l}$ are $0$.\n\nThe resulting matrix $A$ is symmetric and has five non-zero diagonals. The number of non-zero entries, $\\operatorname{nnz}(A)$ is $33$.\n\nThe Incomplete LU factorization with zero fill-in, ILU($0$), seeks factors $L$ and $U$ such that $A \\approx LU$. $L$ is a unit lower triangular matrix ($L_{kk}=1$) and $U$ is an upper triangular matrix. The defining constraint of ILU($0$) is that the sparsity pattern of the factors is a subset of the sparsity pattern of $A$. Specifically, if $A_{ij}=0$, then we enforce $L_{ij}=0$ for $i>j$ and $U_{ij}=0$ for $i > j$. In practice, for entries $(i,j)$ where $A_{ij} \\neq 0$, we compute $L_{ij}$ and $U_{ij}$ by equating $(LU)_{ij} = A_{ij}$.\n\nThe sparsity patterns of $L$ and $U$ are as follows:\n- The non-zero entries of $L$ are located on its main diagonal ($L_{ii}=1$) and in the strictly lower triangular part where $A$ also has non-zero entries. These correspond to the West and South neighbor connections.\n- The non-zero entries of $U$ are located on its main diagonal and in the strictly upper triangular part where $A$ also has non-zero entries. These correspond to the East and North neighbor connections. The diagonal entries $U_{ii}$ are generally different from $A_{ii}$ but are non-zero.\n\nFinally, we compute the total number of non-zero entries, $\\operatorname{nnz}(L) + \\operatorname{nnz}(U)$.\nThe set of non-zero positions in the strictly lower triangular part of $L$ is identical to that of $A$. The number of such positions is the sum of West and South connections: $n_y(n_x-1) + (n_y-1)n_x = 3(2) + (2)3 = 6+6=12$.\nSince $L$ is unit lower triangular, its diagonal has $N=9$ non-zero entries.\nThus, $\\operatorname{nnz}(L) = 9 (\\text{diagonal}) + 12 (\\text{strictly lower}) = 21$.\n\nThe set of non-zero positions in the strictly upper triangular part of $U$ is identical to that of $A$. The number of such positions is the sum of East and North connections, which is also $12$ due to symmetry of the grid.\nThe diagonal entries $U_{ii}$ are all non-zero. There are $N=9$ such entries.\nThus, $\\operatorname{nnz}(U) = 9 (\\text{diagonal}) + 12 (\\text{strictly upper}) = 21$.\n\nThe total number of non-zero entries is the sum of the counts for each matrix:\n$$\\operatorname{nnz}(L) + \\operatorname{nnz}(U) = 21 + 21 = 42$$",
            "answer": "$$\\boxed{42}$$"
        }
    ]
}