{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp an iterative algorithm, it's often best to trace its execution by hand. This first exercise invites you to compute the initial step of the Conjugate Gradient method for a small, representative linear system . By explicitly calculating the step size $\\alpha_0$, the updated solution $x_1$, the new residual $r_1$, and the parameter $\\beta_0$ for the next search direction, you will build a concrete understanding of the algorithm's core mechanics.",
            "id": "3371621",
            "problem": "Consider a linear system arising from a symmetric positive definite discretization of an elliptic operator in computational fluid dynamics, such as the pressure Poisson equation on a minimal control-volume stencil. Let the matrix be $A=\\begin{pmatrix}4&1\\\\1&3\\end{pmatrix}$, the right-hand side be $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$, and the initial iterate be $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The Conjugate Gradient method (CG) is derived by minimizing the quadratic functional $\\phi(x)=\\frac{1}{2}x^{\\top}Ax-b^{\\top}x$ over Krylov subspaces, with residuals $r_{k}=b-Ax_{k}$, mutually $A$-conjugate search directions, and step sizes chosen so that the new residual is orthogonal to the current search direction in the Euclidean inner product. Starting from these principles, and without assuming any shortcut formulas, compute the first-step quantities $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$ explicitly for the given $A$, $b$, and $x_{0}$. Provide exact values with no rounding. For reporting, express the final answer as the row $\\left(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0}\\right)$, where $x_{1,i}$ and $r_{1,i}$ denote the components of $x_{1}$ and $r_{1}$, respectively.",
            "solution": "The problem is well-posed and scientifically sound. It requires the computation of the first iteration of the Conjugate Gradient (CG) method for a given linear system, starting from the fundamental principles of the algorithm rather than relying on a pre-packaged algorithm summary.\n\nThe system to be solved is $Ax=b$, where the matrix $A$ is symmetric and positive definite (SPD). The CG method iteratively constructs a solution by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2}x^{\\top}Ax - b^{\\top}x$. The gradient of this functional is $\\nabla\\phi(x) = Ax - b$, which is the negative of the residual, $r(x) = b - Ax$. Thus, minimizing $\\phi(x)$ is equivalent to finding $x$ such that $\\nabla\\phi(x) = 0$, which is the solution to $Ax=b$.\n\nThe givens are:\nThe matrix $A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\nThe right-hand side vector $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial guess for the solution $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm proceeds as follows for iteration $k=0, 1, 2, ...$:\n1. Update the solution: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n2. Update the residual: $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n3. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\nThe parameters $\\alpha_k$ and $\\beta_k$ are derived from core principles.\n\n**Step 0: Initialization**\n\nFirst, we compute the initial residual $r_0$ based on the initial guess $x_0$.\n$$r_{0} = b - Ax_{0}$$\nWith $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the initial residual is simply $b$:\n$$r_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe first search direction $p_0$ is chosen to be the direction of steepest descent, which is the initial residual:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n\n**Step 1: First Iteration ($k=0$)**\n\nWe need to compute $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$.\n\n**Computing the step size $\\alpha_{0}$**\nThe step size $\\alpha_{0}$ is chosen to minimize $\\phi(x_{1}) = \\phi(x_{0} + \\alpha_{0} p_{0})$ along the search direction $p_{0}$. This minimum is achieved when the new residual $r_{1}$ is orthogonal to the current search direction $p_{0}$, i.e., $p_{0}^{\\top}r_{1} = 0$.\nThe new residual is given by $r_{1} = b - Ax_{1} = b - A(x_{0} + \\alpha_{0} p_{0}) = (b - Ax_{0}) - \\alpha_{0}Ap_{0} = r_0 - \\alpha_0 A p_0$.\nSubstituting this into the orthogonality condition:\n$$p_{0}^{\\top}(r_{0} - \\alpha_{0} A p_{0}) = 0$$\n$$p_{0}^{\\top}r_{0} - \\alpha_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\alpha_0$ yields:\n$$\\alpha_{0} = \\frac{p_{0}^{\\top}r_{0}}{p_{0}^{\\top}A p_{0}}$$\nSince $p_0 = r_0$, this becomes:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top}r_{0}}{r_{0}^{\\top}A r_{0}}$$\nWe calculate the necessary quantities:\n$r_{0}^{\\top}r_{0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1)(1) + (2)(2) = 1 + 4 = 5$.\n$A p_{0} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\n$p_{0}^{\\top}A p_{0} = r_{0}^{\\top}A p_{0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (1)(6) + (2)(7) = 6 + 14 = 20$.\nSubstituting these values:\n$$\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$$\n\n**Computing the new iterate $x_{1}$**\nThe new solution estimate $x_1$ is found by moving from $x_0$ along the direction $p_0$ by the step size $\\alpha_0$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nSo, $x_{1,1} = \\frac{1}{4}$ and $x_{1,2} = \\frac{1}{2}$.\n\n**Computing the new residual $r_{1}$**\nThe new residual $r_1$ can be computed using the update formula:\n$$r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{6}{4} \\\\ 2 - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{4} - \\frac{6}{4} \\\\ \\frac{8}{4} - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$$\nSo, $r_{1,1} = -\\frac{1}{2}$ and $r_{1,2} = \\frac{1}{4}$.\n\n**Computing the coefficient $\\beta_{0}$**\nThe coefficient $\\beta_0$ is used to construct the next search direction, $p_1 = r_1 + \\beta_0 p_0$. The fundamental principle is that the new search direction $p_1$ must be $A$-conjugate to the previous direction $p_0$, meaning $p_{1}^{\\top}A p_{0} = 0$.\n$$(r_{1} + \\beta_{0} p_{0})^{\\top}A p_{0} = 0$$\n$$r_{1}^{\\top}A p_{0} + \\beta_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = -\\frac{r_{1}^{\\top}A p_{0}}{p_{0}^{\\top}A p_{0}}$$\nWe have the terms from the previous calculations: $A p_0 = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$ and $p_{0}^{\\top}A p_{0} = 20$.\nWe need to calculate the numerator:\n$r_{1}^{\\top}A p_{0} = \\begin{pmatrix} -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (-\\frac{1}{2})(6) + (\\frac{1}{4})(7) = -3 + \\frac{7}{4} = -\\frac{12}{4} + \\frac{7}{4} = -\\frac{5}{4}$.\nNow we can compute $\\beta_0$:\n$$\\beta_{0} = - \\frac{-\\frac{5}{4}}{20} = \\frac{5}{4 \\cdot 20} = \\frac{5}{80} = \\frac{1}{16}$$\n\nThe requested quantities are $\\alpha_{0} = \\frac{1}{4}$, $x_{1} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$, $r_{1} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$, and $\\beta_{0} = \\frac{1}{16}$.\nThe final answer is assembled into the specified row vector format $(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0})$.\nThis gives the row vector $(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{16})$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{4} & \\frac{1}{16} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The efficiency of the Conjugate Gradient method hinges on a special property known as $A$-conjugacy, which ensures that progress made in one search direction is not undone by the next. This practice focuses directly on this \"conjugate\" aspect of the algorithm's name . You will verify by direct calculation that the first two search directions, $p_0$ and $p_1$, are indeed $A$-orthogonal, reinforcing the geometric principle that underpins the method's power.",
            "id": "3371638",
            "problem": "Consider a linear system arising in Computational Fluid Dynamics (CFD), where a local diagonal block of a preconditioned diffusion operator is modeled as a symmetric positive definite (SPD) matrix. Let the quadratic functional be $f(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$ with $A = \\mathrm{diag}(1, 2, 3)$, $b = (1, 1, 1)^{\\top}$, and the initial guess $x_{0} = (0, 0, 0)^{\\top}$. Starting from first principles—namely, minimizing $f(x)$ through steepest descent for the initial direction and enforcing $A$-conjugacy for subsequent directions—perform the following:\n\n1. Compute the initial residual $r_{0}$ and the first search direction $p_{0}$ using the steepest descent principle applied to $f(x)$.\n2. Determine the step length $\\alpha_{0}$ by exact line search along $p_{0}$, update $x_{1}$ and compute the new residual $r_{1}$.\n3. Determine the scalar $\\beta_{0}$ by enforcing the $A$-conjugacy condition between the search directions, and form $p_{1}$ accordingly.\n4. Verify explicitly that $p_{0}^{\\top} A p_{1} = 0$.\n\nReport the value of $p_{0}^{\\top} A p_{1}$ as your final answer. No rounding is necessary. Express the final answer as a pure number without units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It is a standard application of the initial steps of the conjugate gradient algorithm, which is a cornerstone of numerical methods for solving linear systems, particularly in contexts like Computational Fluid Dynamics (CFD). The provided matrix $A$ is symmetric and its eigenvalues are the diagonal entries $1$, $2$, and $3$, all of which are positive. Thus, $A$ is symmetric positive definite (SPD) as stated, ensuring the problem is well-posed. We may proceed with the solution.\n\nThe problem asks to perform the first iteration of the conjugate gradient method, starting from first principles. The goal is to find the minimum of the quadratic functional $f(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$. The minimizer of $f(x)$ is the solution to the linear system $Ax = b$. The gradient of the functional is $\\nabla f(x) = Ax - b$. The residual of the system is defined as $r(x) = b - Ax$. It follows that the residual is the negative gradient, $r(x) = -\\nabla f(x)$, and thus points in the direction of steepest descent of $f(x)$.\n\nThe given data are:\nThe matrix $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}$.\nThe vector $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\nThe initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nWe follow the requested steps.\n\n1.  Compute the initial residual $r_{0}$ and the first search direction $p_{0}$.\n\nThe initial residual $r_0$ is calculated at the initial guess $x_0$:\n$$r_{0} = b - A x_{0}$$\nSubstituting the given values:\n$$r_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nThe first search direction $p_0$ in the conjugate gradient method is taken as the direction of steepest descent, which is the initial residual $r_0$:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\n\n2.  Determine the step length $\\alpha_{0}$, update $x_{1}$, and compute the new residual $r_{1}$.\n\nThe next iterate is given by $x_{1} = x_{0} + \\alpha_{0} p_{0}$. The optimal step length $\\alpha_0$ is found by minimizing $f(x_1)$ with respect to $\\alpha_0$. This is an exact line search. We set the derivative of $f(x_0 + \\alpha p_0)$ with respect to $\\alpha$ to zero.\n$$\\frac{d}{d\\alpha} f(x_{0} + \\alpha p_{0}) = \\nabla f(x_{0} + \\alpha p_{0})^{\\top} p_{0} = 0$$\nUsing $\\nabla f(x) = Ax - b$, we get:\n$$(A(x_{0} + \\alpha_{0} p_{0}) - b)^{\\top} p_{0} = 0$$\nSince $r_0 = b - Ax_0$ and $p_0 = r_0$:\n$$(-r_{0} + \\alpha_{0} A p_{0})^{\\top} p_{0} = 0$$\n$$-r_{0}^{\\top} p_{0} + \\alpha_{0} p_{0}^{\\top} A p_{0} = 0$$\nSolving for $\\alpha_0$:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top} p_{0}}{p_{0}^{\\top} A p_{0}}$$\nSince $p_0=r_0$, this simplifies to:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top} r_{0}}{r_{0}^{\\top} A r_{0}}$$\nWe compute the necessary dot products:\n$$r_{0}^{\\top} r_{0} = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 3$$\n$$A r_{0} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\n$$r_{0}^{\\top} A r_{0} = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 3 = 6$$\nThe step length is:\n$$\\alpha_{0} = \\frac{3}{6} = \\frac{1}{2}$$\nNow we update the solution vector $x_1$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nAnd compute the new residual $r_1$:\n$$r_{1} = b - A x_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$$\n\n3.  Determine the scalar $\\beta_{0}$ and form $p_{1}$.\n\nThe next search direction $p_1$ is a linear combination of the new residual $r_1$ and the previous search direction $p_0$:\n$$p_{1} = r_{1} + \\beta_{0} p_{0}$$\nThe scalar $\\beta_0$ is chosen to enforce $A$-conjugacy (also called $A$-orthogonality) between $p_1$ and $p_0$, which is the condition $p_{0}^{\\top} A p_{1} = 0$:\n$$p_{0}^{\\top} A (r_{1} + \\beta_{0} p_{0}) = 0$$\n$$p_{0}^{\\top} A r_{1} + \\beta_{0} p_{0}^{\\top} A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = - \\frac{p_{0}^{\\top} A r_{1}}{p_{0}^{\\top} A p_{0}}$$\nSince A is symmetric, $p_{0}^{\\top} A r_{1} = (A p_{0})^{\\top} r_{1}$.\nThe denominator was previously calculated: $p_{0}^{\\top} A p_{0} = r_{0}^{\\top} A r_{0} = 6$.\nWe calculate the numerator:\n$$A p_{0} = A r_0 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\n$$(A p_{0})^{\\top} r_{1} = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = 1 \\cdot \\frac{1}{2} + 2 \\cdot 0 + 3 \\cdot \\left(-\\frac{1}{2}\\right) = \\frac{1}{2} - \\frac{3}{2} = -1$$\nThus, we find $\\beta_0$:\n$$\\beta_{0} = - \\frac{-1}{6} = \\frac{1}{6}$$\nNow we form the new search direction $p_1$:\n$$p_{1} = r_{1} + \\beta_{0} p_{0} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{6} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} + \\frac{1}{6} \\\\ 0 + \\frac{1}{6} \\\\ -\\frac{1}{2} + \\frac{1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{3+1}{6} \\\\ \\frac{1}{6} \\\\ \\frac{-3+1}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{6} \\\\ \\frac{1}{6} \\\\ -\\frac{2}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{6} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\n\n4.  Verify explicitly that $p_{0}^{\\top} A p_{1} = 0$.\n\nThis final step confirms that our calculated value of $\\beta_0$ correctly enforces $A$-conjugacy. We perform the calculation directly with the vectors we have found.\n$$p_{0} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\quad \\text{and} \\quad p_{1} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{6} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\nFirst, compute $A p_1$:\n$$A p_{1} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{6} \\\\ -\\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\frac{2}{3} \\\\ 2 \\cdot \\frac{1}{6} \\\\ 3 \\cdot (-\\frac{1}{3}) \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{2}{6} \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{3} \\\\ -1 \\end{pmatrix}$$\nNow, compute the dot product $p_0^{\\top} (A p_1)$:\n$$p_{0}^{\\top} A p_{1} = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{3} \\\\ -1 \\end{pmatrix} = 1 \\cdot \\frac{2}{3} + 1 \\cdot \\frac{1}{3} + 1 \\cdot (-1) = \\frac{2}{3} + \\frac{1}{3} - 1 = \\frac{3}{3} - 1 = 1 - 1 = 0$$\nThe calculation confirms that $p_0$ and $p_1$ are $A$-conjugate. The value of the expression $p_{0}^{\\top} A p_{1}$ is exactly $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "After understanding the mechanics, a crucial practical question arises: how many iterations will it take to converge? This exercise connects the algorithm to its theoretical performance bounds, demonstrating how the matrix's spectral condition number, $\\kappa(A)$, governs the worst-case convergence rate . Applying the standard error bound provides a powerful tool for estimating computational cost and quantitatively illustrates why preconditioning is essential for large-scale CFD problems.",
            "id": "3371645",
            "problem": "Consider the symmetric positive definite linear system $A x = b$ that arises from a cell-centered finite volume discretization of the steady pressure Poisson equation in incompressible flow on a uniform grid, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and its spectral condition number $\\kappa(A)$ is defined as $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. The Conjugate Gradient method (CG) is applied to this system with exact arithmetic and no preconditioning. Assume the spectral condition number is known to be $\\kappa(A) = 10^{4}$. Using the standard worst-case error bound for the Conjugate Gradient method in the $A$-norm in terms of $\\kappa(A)$, determine the smallest integer iteration count $k$ such that the bound guarantees an $A$-norm error reduction factor of at most $10^{-6}$ from an arbitrary initial error. Express your final iteration count as an integer with no units.",
            "solution": "The problem requires the determination of the smallest integer iteration count, denoted by $k$, for the Conjugate Gradient (CG) method to guarantee a specified error reduction for a linear system $A x = b$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is given to be symmetric positive definite. The analysis relies on the standard worst-case convergence bound of the CG method.\n\nThe error at iteration $k$ is defined as $e_k = x_k - x$, where $x_k$ is the approximate solution at iteration $k$ and $x$ is the exact solution. The error is measured in the $A$-norm, which is defined as $\\|v\\|_A = \\sqrt{v^{\\top} A v}$ for any vector $v \\in \\mathbb{R}^n$. The classical upper bound for the relative error reduction in the $A$-norm after $k$ iterations of the CG method is given by the inequality:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\leq 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k $$\nwhere $e_0$ is the initial error and $\\kappa(A)$ is the spectral condition number of the matrix $A$, defined as the ratio of its largest to smallest eigenvalue, $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$.\n\nThe problem provides the following data:\n1. The spectral condition number is $\\kappa(A) = 10^4$.\n2. The desired error reduction factor is at most $10^{-6}$. This implies we seek the smallest $k$ such that the bound on the error ratio is less than or equal to this value: $\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\leq 10^{-6}$.\n\nWe must find the smallest integer $k$ that satisfies the inequality when the right-hand side of the theoretical bound is constrained by the required reduction factor:\n$$ 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\leq 10^{-6} $$\nWe substitute the given value of $\\kappa(A) = 10^4$. First, we compute the square root of the condition number:\n$$ \\sqrt{\\kappa(A)} = \\sqrt{10^4} = 10^2 = 100 $$\nSubstituting this into the inequality gives:\n$$ 2 \\left( \\frac{100 - 1}{100 + 1} \\right)^k \\leq 10^{-6} $$\n$$ 2 \\left( \\frac{99}{101} \\right)^k \\leq 10^{-6} $$\nTo solve for the integer $k$, we first isolate the term raised to the power of $k$:\n$$ \\left( \\frac{99}{101} \\right)^k \\leq \\frac{1}{2} \\times 10^{-6} $$\nNext, we apply the natural logarithm to both sides of the inequality. As the natural logarithm, $\\ln(x)$, is a monotonically increasing function for $x > 0$, the direction of the inequality is preserved:\n$$ \\ln\\left[ \\left( \\frac{99}{101} \\right)^k \\right] \\leq \\ln\\left( \\frac{1}{2} \\times 10^{-6} \\right) $$\nUsing the logarithmic property $\\ln(a^b) = b \\ln(a)$, we get:\n$$ k \\ln\\left( \\frac{99}{101} \\right) \\leq \\ln\\left( \\frac{1}{2} \\right) + \\ln(10^{-6}) $$\n$$ k \\ln\\left( \\frac{99}{101} \\right) \\leq -\\ln(2) - 6 \\ln(10) $$\nThe term $\\ln(99/101)$ is negative because its argument, $99/101$, is less than $1$. Therefore, when we divide by $\\ln(99/101)$ to isolate $k$, we must reverse the direction of the inequality:\n$$ k \\geq \\frac{-\\ln(2) - 6 \\ln(10)}{\\ln(99/101)} $$\nThis expression can be made more convenient for calculation by multiplying the numerator and denominator by $-1$ and using the property $-\\ln(x/y) = \\ln(y/x)$:\n$$ k \\geq \\frac{\\ln(2) + 6 \\ln(10)}{\\ln(101/99)} $$\nTo find the numerical value for this lower bound on $k$, we use the standard values for the natural logarithms: $\\ln(2) \\approx 0.693147$ and $\\ln(10) \\approx 2.302585$.\nThe numerator is:\n$$ \\ln(2) + 6 \\ln(10) \\approx 0.693147 + 6 \\times (2.302585) = 0.693147 + 13.81551 = 14.508657 $$\nThe denominator is:\n$$ \\ln\\left(\\frac{101}{99}\\right) = \\ln(101) - \\ln(99) \\approx 4.6151205 - 4.5951198 = 0.0200007 $$\nNow, we compute the ratio:\n$$ k \\geq \\frac{14.508657}{0.0200007} \\approx 725.409 $$\nThe number of iterations $k$ must be an integer. The inequality $k \\geq 725.409$ requires that we find the smallest integer satisfying this condition. This is obtained by taking the ceiling of the numerical value:\n$$ k = \\lceil 725.409 \\rceil = 726 $$\nTherefore, a minimum of $726$ iterations are required for the worst-case bound to guarantee an error reduction of at most $10^{-6}$.",
            "answer": "$$\\boxed{726}$$"
        }
    ]
}