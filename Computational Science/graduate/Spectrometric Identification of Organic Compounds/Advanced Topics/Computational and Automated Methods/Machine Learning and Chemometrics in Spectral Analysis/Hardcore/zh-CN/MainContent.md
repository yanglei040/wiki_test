## 引言
[光谱分析](@entry_id:275514)是现代科学研究与工业生产中不可或缺的分析工具，它能快速、无损地提供物质的化学成分与结构信息。然而，原始[光谱](@entry_id:185632)数据往往十分复杂，充满了噪声、[仪器伪影](@entry_id:185069)、基线漂移以及谱峰重叠等问题，这使得直接从中提取精确的定量和定性信息成为一项巨大挑战。传统的分析方法在面对高维、共线且存在[非线性](@entry_id:637147)效应的[光谱](@entry_id:185632)数据时常常力不从心。机器学习与[化学计量学](@entry_id:140916)的结合为解决这一难题提供了强大的理论框架和方法论，它使我们能够从看似杂乱无章的数据中发掘出潜在的化学规律。

本文旨在系统性地介绍如何应用[化学计量学](@entry_id:140916)与机器学习方法来驾驭复杂的[光谱](@entry_id:185632)数据。我们将带领读者开启一段从理论到实践的旅程。在 **「原理与机制」** 章节中，我们将奠定坚实的理论基础，深入剖析从[比尔-朗伯定律](@entry_id:192870)到[潜变量模型](@entry_id:174856)的核心原理，并重点探讨[数据预处理](@entry_id:197920)与[模型验证](@entry_id:141140)的关键技术。接着，在 **「应用与交叉学科联系」** 章节中，我们将展示这些原理如何在化学、制药、生物等多个领域解决真实世界的复杂问题，探讨从[光谱](@entry_id:185632)拆分到高级分类，再到模型解释性的前沿应用。最后，通过 **「动手实践」** 部分，您将有机会亲手实现和验证文中所学的关键算法与工作流程。通过这一系列的学习，您将掌握一套完整的、能够解决实际[光谱分析](@entry_id:275514)问题的技能。

## 原理与机制

本章深入探讨了化学计量学和机器学习在[光谱分析](@entry_id:275514)中应用的核心科学原理和基本机制。我们将从[光谱](@entry_id:185632)数据[线性响应](@entry_id:146180)的物理基础出发，逐步探讨信号处理、[数据预处理](@entry_id:197920)、线性校正模型构建以及[模型验证](@entry_id:141140)等关键环节。本章的目标是为后续章节介绍更高级的算法和应用奠定坚实的理论基础。

### 线性[光谱](@entry_id:185632)模型：基础与局限性

许多[光谱](@entry_id:185632)定量分析技术，尤其是在紫外-可见光（UV-Vis）和近红外（NIR）区域，都建立在一个基本假设之上：测得的信号与待测[分析物](@entry_id:199209)的浓度之间存在线性关系。这一假设的物理基础是 **比尔-朗伯定律**（Beer-Lambert law）。对于单一溶质在透明溶剂中的情况，该定律指出，在特定波长 $\lambda$ 处的吸光度 $A(\lambda)$ 与溶质的[摩尔浓度](@entry_id:139283) $c$ 和[光程](@entry_id:178906)长度 $\ell$ 成正比：

$A(\lambda) = \epsilon(\lambda) c \ell$

其中，比例常数 $\epsilon(\lambda)$ 是 **[摩尔吸光系数](@entry_id:148758)**，它是物质在特定波长下吸收[光子](@entry_id:145192)能力的内在度量。

当溶液中含有多种互不相互作用的组分时，总[吸光度](@entry_id:176309)是各组分吸光度的线性叠加。对于一个包含 $N$ 种组分的混合物，总[吸光度](@entry_id:176309)可以表示为：

$A(\lambda) = \ell \sum_{i=1}^{N} \epsilon_i(\lambda) c_i$

这个 **[线性叠加原理](@entry_id:196987)** 是许多经典[化学计量学](@entry_id:140916)方法，如经典最小二乘法（Classical Least Squares, CLS）和[多元线性回归](@entry_id:141458)（Multiple Linear Regression, MLR）的基石。在[矩阵表示法](@entry_id:190318)中，如果我们在一系列波长下测量一组样品，整个数据集可以被模型化为：

$\mathbf{A} = \ell \mathbf{C} \mathbf{E}^T + \mathbf{N}$

其中，$\mathbf{A}$ 是 $m \times p$ 的[吸光度](@entry_id:176309)矩阵（$m$ 个样品，$p$ 个波长），$\mathbf{C}$ 是 $m \times N$ 的浓度矩阵，$\mathbf{E}$ 是 $p \times N$ 的[摩尔吸光系数](@entry_id:148758)矩阵（每列代表一种纯物质的[光谱](@entry_id:185632)），$\mathbf{N}$ 是代表[测量噪声](@entry_id:275238)的 $m \times p$ 矩阵。

然而，在实际应用中，尤其是在处理高浓度的有机样品时，这种理想的[线性关系](@entry_id:267880)常常会失效。[线性模型](@entry_id:178302)的有效性取决于几个前提条件，包括溶液的[均匀性](@entry_id:152612)、散射可忽略不计以及组分之间不存在化学相互作用。当这些前提被违反时，就会出现[非线性](@entry_id:637147)。

一个典型的例子是溶液中组分间的化学缔合。假设我们正在分析两种有机溶质 $X$ 和 $Y$。在高浓度下，它们可能会通过非共价作用形成一个 1:1 的复合物 $XY$，其[平衡常数](@entry_id:141040)为 $K_a$：

$X + Y \rightleftharpoons XY$

这种化学平衡的存在意味着溶质 $X$ 和 $Y$ 的自由浓度（$[X]$ 和 $[Y]$）不等于它们的总分析浓度（$C_X$ 和 $C_Y$）。由于复合物 $XY$ 本身也具有不同于 $X$ 或 $Y$ 的[摩尔吸光系数](@entry_id:148758) $\epsilon_{XY}(\lambda)$，因此溶液的真实[吸光度](@entry_id:176309) $A_{\text{true}}$ 应为：

$A_{\text{true}}(\lambda) = \ell (\epsilon_X(\lambda)[X] + \epsilon_Y(\lambda)[Y] + \epsilon_{XY}(\lambda)[XY])$

而忽略这种相互作用的理想[线性模型](@entry_id:178302)预测的[吸光度](@entry_id:176309) $A_{\text{lin}}$ 则是：

$A_{\text{lin}}(\lambda) = \ell (\epsilon_X(\lambda)C_X + \epsilon_Y(\lambda)C_Y)$

这两个值之间的差异导致了[非线性](@entry_id:637147)。我们可以定义相对[非线性](@entry_id:637147)度 $\delta$ 来量化这种偏离，如  中所示。分析表明，$\delta$ 的大小与结合在复合物中的[分析物](@entry_id:199209)的摩尔分数以及各物种[摩尔吸光系数](@entry_id:148758)的差异直接相关。当[缔合常数](@entry_id:273525) $K_a$ 较大或总浓度 $C$较高时，复合物的浓度 $[XY]$ 会显著增加，导致线性近似失效。这提醒我们，在构建线性校正模型（如[偏最小二乘法](@entry_id:194701)，PLS）之前，必须仔细评估样品体系中可能存在的化学相互作用。

### 信号、噪声与仪器效应

[光谱仪](@entry_id:193181)记录的信号并不仅仅是样品真实的吸收光谱，它还包含了仪器本身特性以及随机噪声的影响。一个更精确的模型将测量的[光谱](@entry_id:185632) $y(\lambda)$ 描述为真实[光谱](@entry_id:185632) $A(\lambda)$ 与 **仪器线型函数**（instrument line shape function）$h(\lambda)$ 的卷积，再加上[加性噪声](@entry_id:194447) $\eta(\lambda)$：

$y(\lambda) = \int_{-\infty}^{\infty} A(\lambda') h(\lambda - \lambda') d\lambda' + \eta(\lambda) = (A * h)(\lambda) + \eta(\lambda)$

这里的 $h(\lambda)$ 描述了仪器如何将一个理想的、无限窄的[谱线](@entry_id:193408)展宽成一个有限宽度的峰。这个过程在数学上是 **卷积**，它等效于对真实[光谱](@entry_id:185632)进行模糊处理。[傅里叶变换](@entry_id:142120)为我们提供了分析这种效应的有力工具。根据卷积定理，上述关系在频率域（$\omega$ 是与波长 $\lambda$ 共轭的[角频率](@entry_id:261565)）中变为一个简单的乘积：

$Y(\omega) = H(\omega)A(\omega) + N(\omega)$

其中 $Y(\omega)$, $H(\omega)$, $A(\omega)$ 和 $N(\omega)$ 分别是 $y(\lambda)$, $h(\lambda)$, $A(\lambda)$ 和 $\eta(\lambda)$ 的[傅里叶变换](@entry_id:142120)。$H(\omega)$ 被称为 **[光学传递函数](@entry_id:172898)**（Optical Transfer Function）。

从这个表达式可以看出，要从测量值中恢复真实[光谱](@entry_id:185632)（即 **反褶积**），理论上需要计算 $A(\omega) = (Y(\omega) - N(\omega)) / H(\omega)$。这就引出了 **可辨识性**（identifiability）的关键问题。如  中所探讨的，如果仪器[传递函数](@entry_id:273897) $H(\omega)$ 在某些频率上为零，那么真实[光谱](@entry_id:185632) $A(\omega)$在这些频率上的所有信息都会在测量过程中被永久抹去。在这种情况下，有无限多个可能的真实[光谱](@entry_id:185632) $A(\lambda)$ 可以产生完全相同的测量[光谱](@entry_id:185632) $y(\lambda)$，使得真实[光谱](@entry_id:185632)无法唯一确定。然而，如果我们拥有关于 $A(\lambda)$ 的先验知识，例如知道它是由一个已知[光谱](@entry_id:185632)库中有限数量纯组分[光谱](@entry_id:185632)的线性组合构成，那么即使 $H(\omega)$ 存在零点，我们仍有可能唯一地确定这些组合系数，从而恢复 $A(\lambda)$ 。

### [数据预处理](@entry_id:197920)：校正与增强[光谱](@entry_id:185632)

原始[光谱](@entry_id:185632)数据往往包含与目标信息无关的变异，如基线漂移、[光散射](@entry_id:269379)效应和噪声。[数据预处理](@entry_id:197920)的目的就是消除这些干扰，并凸显出与化学成分相关的有用信息。

#### 基线校正

[光谱](@entry_id:185632)基线漂移可能源于仪器不稳定、样品浑浊或（在拉曼[光谱](@entry_id:185632)中）荧光背景。有效的基线校正依赖于一个关键假设：基线信号与[分析信号](@entry_id:190094)在尺度上是分离的。例如，在拉曼[光谱](@entry_id:185632)中，尖锐的拉曼峰（由[振动跃迁](@entry_id:167069)产生，典型宽度为几个波数 $\text{cm}^{-1}$）叠加在一个宽阔、平滑的荧光背景上（由[电子跃迁](@entry_id:152949)产生，宽度可达数百 $\text{cm}^{-1}$）。这种 **[尺度分离](@entry_id:270204)** 使得我们可以将基线 $B(\tilde{\nu})$ 建模为一个光滑函数。

其光滑性的合理解释来自两个方面：首先，荧光发射的物理过程本身就产生宽谱带；其次，任何信号通过仪器时都会与仪器线型函数 $g(\tilde{\nu})$ 发生卷积，而卷积本身就是一个平滑操作。只要 $g(\tilde{\nu})$ 是一个光滑的函数（例如近似高斯函数），卷积后的基线 $B(\tilde{\nu}) = (B_0 * g)(\tilde{\nu})$ 将会比原始荧光发射 $B_0(\tilde{\nu})$ 更加光滑 。因此，我们可以采用惩罚高阶导数的方法（如 Whittaker 平滑器）或设置了较大长度尺度的[高斯过程](@entry_id:182192)来拟合基线，同时保留尖锐的分析峰。

#### 散射校正

在近红外（NIR）等[漫反射](@entry_id:173213)或透射[光谱](@entry_id:185632)技术中，样品[颗粒大小](@entry_id:161460)、[压实](@entry_id:161543)度等物理差异会导致严重的[光散射](@entry_id:269379)效应。这种效应通常表现为[光谱](@entry_id:185632)整体的平移（加性效应）和倾斜（乘性效应）。**乘法散射校正**（Multiplicative Scatter Correction, MSC）是一种广泛用于校正此类效应的方法。

MSC 的原理是将每个样品的[光谱](@entry_id:185632) $\mathbf{s}_j$ 与一个理想的 **参考[光谱](@entry_id:185632)** $\mathbf{r}$ (通常是所有样品的平均[光谱](@entry_id:185632)) 进行线性回归 ：

$\mathbf{s}_j \approx a_j \mathbf{1} + b_j \mathbf{r}$

其中 $\mathbf{1}$ 是全1向量。通过[最小二乘法](@entry_id:137100)估计出偏移系数 $\hat{a}_j$ 和斜率系数 $\hat{b}_j$：

$\hat{b}_{j} = \frac{\operatorname{Cov}(\mathbf{r},\mathbf{s}_{j})}{\operatorname{Var}(\mathbf{r})}, \quad \hat{a}_{j} = \bar{s}_{j} - \hat{b}_{j}\bar{r}$

然后，利用这两个系数对原始[光谱](@entry_id:185632)进行校正：

$\mathbf{s}^{\text{MSC}}_{j} = \frac{\mathbf{s}_{j} - \hat{a}_{j}\mathbf{1}}{\hat{b}_{j}}$

这个过程旨在消除样品间的光程差异，使得校正后的[光谱](@entry_id:185632)主要反映化学吸收的差异。然而，需要注意的是，MSC 的结果对参考[光谱](@entry_id:185632)的选择敏感。分析表明 ，估计出的系数 $\hat{a}_j$ 和 $\hat{b}_j$ 的[期望值](@entry_id:153208)依赖于参考[光谱](@entry_id:185632)的基线偏移、信号强度和噪声水平，这强调了选择一个高质量、有[代表性](@entry_id:204613)的参考[光谱](@entry_id:185632)的重要性。

#### 分辨率增强与导数[光谱](@entry_id:185632)

当[光谱](@entry_id:185632)中包含严重重叠的谱峰时，直接解译变得困难。**导数[光谱](@entry_id:185632)**（Derivative Spectroscopy）技术通过计算[光谱](@entry_id:185632)的一阶或更高阶导数来增强分辨率。[一阶导数](@entry_id:749425)可以定位谱峰的精确位置（在峰顶处过零），而[二阶导数](@entry_id:144508)则可以显著“锐化”谱峰，将肩峰转变为可辨识的独立峰。

导数操作在数学上是一个高通滤波器，它放大了信号的快速变化部分（如谱峰边缘），同时抑制了缓慢变化的背景（基线）。然而，这也意味着它会不可避免地放大高频噪声。这构成了分辨率增强中的一个经典 **权衡**：增强了谱峰分辨率，但降低了信噪比。

我们可以通过分析一个由两个重叠高斯峰组成的模型来量化这一效应 。[二阶导数](@entry_id:144508)谱在两峰之间的中点处的响应符号，可以指示两个峰是否被解析（当曲率变为正时，形成一个局部最小值）。同时，通过[傅里叶分析](@entry_id:137640)可以计算出导数滤波器对[白噪声](@entry_id:145248)的响应，从而得到滤波后噪声的[标准差](@entry_id:153618)。将增强的信号响应与增加的噪声水平相结合，可以得到一个[信噪比](@entry_id:185071)（SNR）表达式，它明确地展示了分辨率增强（与谱[峰间距](@entry_id:271130) $\Delta$ 和宽度 $\sigma$ 相关）与噪声放大（与导数阶数和平滑参数 $\sigma_s$ 相关）之间的定量关系。

#### 中心化与缩放

在应用主成分分析（PCA）等多变量方法之前，通常需要对数据进行中心化和缩放。

**均值中心化**（Mean-centering）是将数据矩阵的每一列（即每个波长变量）减去其均值。在[光谱分析](@entry_id:275514)中，这是至关重要的一步，因为它使得后续分析关注于样品之间的 **差异**，而不是所有样品共有的平均[光谱](@entry_id:185632)形态。若不进行中心化，PCA 的第一个主成分几乎总是被平均[光谱](@entry_id:185632)所主导，掩盖了更有化学意义的变异。

中心化之后，我们还需考虑是否对变量进行 **缩放**（scaling）。由于不同谱带的强度差异很大（例如，宽的 O-H 伸缩[振动](@entry_id:267781)峰可能比尖锐的 C=O 伸缩[振动](@entry_id:267781)峰强度高几个[数量级](@entry_id:264888)），如果不进行缩放，[方差](@entry_id:200758)最大的变量（即最强的谱峰）将主导 PCA 分析，而那些强度较弱但可能包含关键信息的窄峰则被忽略。

- **自适应缩放**（Autoscaling）：将每个变量除以其[标准差](@entry_id:153618)，使得所有变量具有单位[方差](@entry_id:200758)。这给予了所有变量平等的权重。但如  所述，当噪声水平在不同波长上不均匀时（[异方差性](@entry_id:136378)），自适应缩放会过度放大那些信号弱、噪声相对强的变量，从而将噪声引入到模型中。
- **帕累托缩放**（Pareto scaling）：将每个变量除以其标准差的平方根。这是一种折衷方案，它减弱了高[方差](@entry_id:200758)变量的主导地位，但又不像自适应缩放那样极端地放大低[方差](@entry_id:200758)变量。因此，对于同时包含宽峰、窄峰且噪声不均匀的典型[光谱](@entry_id:185632)数据，帕累托缩放通常能在保留窄峰信息和控制噪声放大之间取得良好平衡  。

### 线性校正与[降维](@entry_id:142982)模型

当拥有了经过良好[预处理](@entry_id:141204)的数据后，下一步就是建立浓度与[光谱](@entry_id:185632)之间的定量关系模型。

#### 经典[最小二乘法](@entry_id:137100) (CLS)

当混合物中所有纯组分的[光谱](@entry_id:185632)都已知时，我们可以使用 **经典[最小二乘法](@entry_id:137100)**（Classical Least Squares, CLS）或称 K-matrix 方法。模型为 $\mathbf{y} = \mathbf{S}\mathbf{c} + \mathbf{e}$，其中 $\mathbf{y}$ 是混合物的[光谱](@entry_id:185632)向量，$\mathbf{S}$ 是一个矩阵，其列为纯组分的[光谱](@entry_id:185632)，$\mathbf{c}$ 是待求的浓度向量。

通过最小化[残差平方和](@entry_id:174395) $||\mathbf{y} - \mathbf{S}\mathbf{c}||_2^2$，可以推导出浓度向量的估计值 ：

$\widehat{\mathbf{c}} = (\mathbf{S}^T\mathbf{S})^{-1} \mathbf{S}^T \mathbf{y}$

这个解的存在性和唯一性要求矩阵 $\mathbf{S}^T\mathbf{S}$ 是可逆的，这等价于要求纯组分[光谱](@entry_id:185632)（$\mathbf{S}$ 的列）是 **[线性无关](@entry_id:148207)** 的。如果某些组分的[光谱](@entry_id:185632)形状非常相似（即高度 **共線性**），$\mathbf{S}^T\mathbf{S}$ 会变得接近奇异（病态），导致浓度估计值对测量噪声非常敏感，解不稳定。

#### [潜变量模型](@entry_id:174856)：PCR 与 PLS

在更常见的情况下，我们并不知道所有纯组分的[光谱](@entry_id:185632)，或者体系过于复杂。此外，[光谱](@entry_id:185632)数据通常是高度[共线性](@entry_id:270224)的，且变量数（波长点 $p$）远大于样品数 ($n$)。在这种 $p \gg n$ 的情况下，直接应用[最小二乘法](@entry_id:137100)是不可行的。**潜变量**（Latent Variables, LVs）方法通过将高维的[光谱](@entry_id:185632)数据投影到低维的[潜变量](@entry_id:143771)空间来解决这个问题。

**主成分回归**（Principal Component Regression, PCR）和 **[偏最小二乘法](@entry_id:194701)**（Partial Least Squares, PLS）是两种最常用的潜变量方法。

- **PCR** 首先对[光谱](@entry_id:185632)矩阵 $\mathbf{X}$ 进行主成分分析（PCA），找到解释 $\mathbf{X}$ 中最大[方差](@entry_id:200758)的方向（主成分），然后用样品在这些主成分上的得分作为预测变量，对浓度 $\mathbf{y}$ 进行回归。
- **PLS** 则寻找一系列潜变量方向，这些方向不仅能很好地解释 $\mathbf{X}$ 的[方差](@entry_id:200758)，而且与浓度 $\mathbf{y}$ 的协[方差](@entry_id:200758)也最大化。

这两种方法的关键区别在于它们选择潜变量的准则，这也导致了它们在特定问题上的 **偏倚-[方差](@entry_id:200758)权衡**（bias-variance trade-off）差异。如  中的情景所述，当我们需要定量一个痕量组分，而其[光谱](@entry_id:185632)信号非常微弱，仅在 $\mathbf{X}$ 的低[方差](@entry_id:200758)方向上有所体现时：
- PCR 会优先选择那些解释了主要变异（如溶剂或主要组分）的高[方差](@entry_id:200758)主成分，很可能需要非常多的主成分才能捕获到与痕量组分相关的低[方差](@entry_id:200758)信息。在只使用少量主成分时，PCR 模型会因为忽略了关键的信号方向而产生 **高偏倚**。
- PLS 由于其算法包含了与 $\mathbf{y}$ 的协[方差](@entry_id:200758)信息，能够“感知”到哪怕是低[方差](@entry_id:200758)但与浓度高度相关的方向。因此，PLS 只需要很少的[潜变量](@entry_id:143771)就能建立一个低偏倚的模型。

在这种情况下，PLS 通常能以更少的潜变量（更简约的模型）达到比 PCR 更低的预测误差，展现出更优的偏倚-[方差](@entry_id:200758)权衡。

### [模型验证](@entry_id:141140)与性能评估

建立模型后，必须对其性能进行严格、客观的评估。这包括选择合适的性能指标和设计可靠的验证策略。

#### 性能指标

对于定量模型，常用的性能指标包括 ：
- **预测[均方根误差](@entry_id:170440)** (Root Mean Squared Error of Prediction, RMSEP): $RMSEP = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_{i} - \hat{y}_{i})^{2}}$。它量化了预测误差的平均大小，单位与浓度相同。
- **平均[绝对误差](@entry_id:139354)** (Mean Absolute Error, MAE): $MAE = \frac{1}{N} \sum_{i=1}^{N} |y_{i} - \hat{y}_{i}|$。与 RMSEP 相比，MAE 对异常值的敏感度较低。
- **[决定系数](@entry_id:142674)** ($R^2$) 和 **预测[相关系数](@entry_id:147037)** ($Q^2$): $R^{2} = 1 - \frac{\sum (y_{i} - \hat{y}_{i})^{2}}{\sum (y_{i} - \bar{y})^{2}}$。$R^2$衡量模型对校正集数据的[拟合优度](@entry_id:637026)，而$Q^2$（通过[交叉验证](@entry_id:164650)计算）衡量模型对未知数据的预测能力。

深入分析这些指标的理论极限是很有启发性的。在一个理想的、无偏的模型中，当样品数量足够大时，[预测误差](@entry_id:753692) $(y_i - \hat{y}_i)$ 的来源主要是参考方法本身的[测量误差](@entry_id:270998) $\varepsilon_i$。假设参考值[测量误差](@entry_id:270998)服从[正态分布](@entry_id:154414) $\mathcal{N}(0, \sigma^2)$，那么在大样本极限下，可以推导出 ：
- $RMSEP \to \sigma$
- $MAE \to \sigma \sqrt{2/\pi}$
- $R^2, Q^2 \to \frac{\sigma_c^2}{\sigma_c^2 + \sigma^2}$

其中 $\sigma_c^2$ 是真实浓度的[方差](@entry_id:200758)。这些关系告诉我们，模型的预测精度有一个由参考方法精度决定的理论上限。$R^2$ 和 $Q^2$ 的极限值实际上是数据的 **信噪比** 的一种度量。

#### [交叉验证](@entry_id:164650)与信息泄漏

为了获得对[模型泛化](@entry_id:174365)性能的[无偏估计](@entry_id:756289)，必须使用 **交叉验证**（Cross-Validation, CV）。然而，交叉验证的实施细节至关重要，不当的操作会导致称为 **信息泄漏**（information leakage）的严重问题，从而得到过于乐观的性能评估。

信息泄漏的根本原因在于，在模型构建的任何阶段，用于测试的数据以任何方式“接触”到了训练过程。最常见的错误是在进行[交叉验证](@entry_id:164650) **之前**，对整个数据集进行[预处理](@entry_id:141204)或特征选择。

一个健壮的、避免信息泄漏的验证流程必须遵循以下原则 ：**整个模型构建流程，包括所有数据驱动的[预处理](@entry_id:141204)步骤（如中心化、缩放、基线校正、MSC、PCA降维等）和[超参数调整](@entry_id:143653)，都必须在交叉验证的每一个折叠（fold）内独立重复**。

这意味着，对于一个 K-折[交叉验证](@entry_id:164650)：
1. 将数据分为 K 个折叠。
2. 对于第 $k$ 折（作为[测试集](@entry_id:637546)）：
   a. 将其余 K-1 折作为训练集。
   b. **仅使用这个[训练集](@entry_id:636396)** 来计算中心化、缩放的参数，拟合基线模型，确定MSC的参考[光谱](@entry_id:185632)，或计算PCA/PLS的载量。
   c. 将从训练集学到的这些变换应用到[训练集](@entry_id:636396)自身和 **[测试集](@entry_id:637546)** 上。
   d. 使用变换后的[训练集](@entry_id:636396)来训练分类器或[回归模型](@entry_id:163386)。
   e. 在变换后的测试集上评估模型性能。
3. 重复以上步骤 K 次，平均 K 次的性能指标作为最终的[泛化误差](@entry_id:637724)估计。

如果需要调整超参数（如PLS的潜变量个数 $k$），则需要使用 **[嵌套交叉验证](@entry_id:176273)**（nested cross-validation） 。外层循环用于性能评估，内层循环（仅在每步外层循环的训练集上进行）用于选择最佳超参数。

此外，当数据具有特定结构时，如存在来自同一批次的多个样品（**[批次效应](@entry_id:265859)**）或同一成分的多个重复测量（**技术重复**），标准的随机 K-折[交叉验证](@entry_id:164650)也是不合适的。这会导致相关性极高的样品被分到训练集和测试集中，同样造成信息泄漏和过于乐观的结果。在这种情况下，必须采用 **[分组交叉验证](@entry_id:634144)**（Group K-fold CV），确保来自同一组（如同一个批次或同一个样品）的所有数据点要么都在训练集中，要么都在测试集中 。

通过遵循这些严谨的验证原则，我们才能建立不仅性能优越，而且其性能评估结果真实可靠的化学计量学模型。