{
    "hands_on_practices": [
        {
            "introduction": "The foundation of free energy perturbation (FEP) lies in the Zwanzig equation, which provides a direct link between the free energy difference, $\\Delta F$, and an ensemble average. This first practice challenges you to implement this cornerstone equation, translating statistical mechanics theory into a functional program. By calculating $\\Delta F$ and its associated statistical error from a set of pre-computed energy differences, you will gain a concrete understanding of how macroscopic thermodynamic properties are extracted from microscopic simulation data .",
            "id": "3453627",
            "problem": "You are given a set of independent configurations $\\{\\mathbf{x}_i\\}_{i=1}^{N}$ sampled from a canonical ensemble defined by a reference potential $U_0(\\mathbf{x})$, with corresponding energy differences $\\Delta U_i = U_1(\\mathbf{x}_i) - U_0(\\mathbf{x}_i)$ reported in kilojoules per mole (kJ mol$^{-1}$) at absolute temperature $T$ in Kelvin. Using the framework of Free Energy Perturbation (FEP), the change in Helmholtz free energy $\\Delta F$ between $U_0$ and $U_1$ can be linked to the molar canonical partition functions through the statistical expectation under $U_0$. The molar inverse temperature is $\\beta = (R T)^{-1}$ where $R$ is the molar gas constant. Your task is to implement a program that, for each test case:\n- Computes the weights $w_i = e^{-\\beta \\Delta U_i}$.\n- Computes the sample mean $\\bar{w}$ and the unbiased sample variance $s_w^2$ of $\\{w_i\\}$.\n- Transforms $\\bar{w}$ into the molar free energy change $\\Delta F$ in kJ mol$^{-1}$ using the canonical definition of free energy and the logarithm implied by the ratio of partition functions.\n- Estimates the standard error of $\\Delta F$ in kJ mol$^{-1}$ by linearly propagating the uncertainty of the mean via the first-order Taylor expansion (delta method) of the logarithmic transformation $f(m)$, where $m$ denotes the sample mean of $\\{w_i\\}$.\n\nUse the molar gas constant $R = 8.314462618 \\times 10^{-3}$ kJ mol$^{-1}$ K$^{-1}$. For any case with $N=1$, define $s_w^2=0$ and return a standard error of $0$ in kJ mol$^{-1}$. Express all final free energy changes and their standard errors in kJ mol$^{-1}$, rounded to six decimal places.\n\nImplement the program to process the following test suite of parameter values:\n- Test case $1$: $T = 300$ K, $\\{\\Delta U_i\\} = \\{-1.2, 0.5, 0.0, 1.3, -0.7, 0.2\\}$ kJ mol$^{-1}$.\n- Test case $2$: $T = 298.15$ K, $\\{\\Delta U_i\\} = \\{0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1\\}$ kJ mol$^{-1}$.\n- Test case $3$: $T = 300$ K, $\\{\\Delta U_i\\} = \\{12.0, 0.0, 5.0, -1.0, 3.0, 8.0, -2.0\\}$ kJ mol$^{-1}$.\n- Test case $4$: $T = 100$ K, $\\{\\Delta U_i\\} = \\{0.5, 1.0, -0.2, 0.3, -0.1, 0.0\\}$ kJ mol$^{-1}$.\n- Test case $5$: $T = 1000$ K, $\\{\\Delta U_i\\} = \\{5.0, -5.0, 2.0, -2.0, 0.0\\}$ kJ mol$^{-1}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $\\left[\\Delta F, \\text{SE}\\right]$ for the corresponding test case, both in kJ mol$^{-1}$ and rounded to six decimal places. For example: $\\left[[a,b],[c,d],\\dots\\right]$ where $a$, $b$, $c$, $d$ are floats.",
            "solution": "The problem requires the implementation of a calculation based on the principles of Free Energy Perturbation (FEP) in statistical mechanics. The core of the task is to estimate the change in Helmholtz free energy, $\\Delta F$, between two states defined by potential energies $U_0$ and $U_1$, and to quantify the statistical uncertainty of this estimate.\n\nThe fundamental relationship connecting free energy to the microscopic states of a system in the canonical ensemble (constant number of particles $N$, volume $V$, and temperature $T$) is given by:\n$$ F = -k_B T \\ln Z $$\nwhere $k_B$ is the Boltzmann constant and $Z$ is the canonical partition function. For molar quantities, as specified in the problem, this is expressed as:\n$$ F = -RT \\ln Q $$\nwhere $R$ is the molar gas constant and $Q$ is the molar partition function. The change in free energy, $\\Delta F = F_1 - F_0$, is then:\n$$ \\Delta F = -RT \\ln \\left( \\frac{Q_1}{Q_0} \\right) $$\nThe ratio of the partition functions for two systems with potential energy functions $U_1(\\mathbf{x})$ and $U_0(\\mathbf{x})$ can be written as an ensemble average over the reference state (system $0$):\n$$ \\frac{Q_1}{Q_0} = \\frac{\\int e^{-\\beta U_1(\\mathbf{x})} d\\mathbf{x}}{\\int e^{-\\beta U_0(\\mathbf{x})} d\\mathbf{x}} = \\frac{\\int e^{-\\beta (U_1(\\mathbf{x}) - U_0(\\mathbf{x}))} e^{-\\beta U_0(\\mathbf{x})} d\\mathbf{x}}{\\int e^{-\\beta U_0(\\mathbf{x})} d\\mathbf{x}} = \\langle e^{-\\beta \\Delta U} \\rangle_0 $$\nHere, $\\beta = (RT)^{-1}$ is the molar inverse temperature and $\\Delta U = U_1 - U_0$ is the difference in potential energy. Substituting this into the expression for $\\Delta F$ yields the Zwanzig equation, which is the cornerstone of FEP:\n$$ \\Delta F = -RT \\ln \\langle e^{-\\beta \\Delta U} \\rangle_0 $$\nIn a computational setting, we do not have access to the full ensemble. Instead, we are given a finite number of $N$ configurations, $\\{\\mathbf{x}_i\\}_{i=1}^{N}$, sampled from the reference ensemble (defined by $U_0$). The ensemble average $\\langle \\dots \\rangle_0$ is therefore estimated by a sample mean. We first compute the weights $w_i$ for each configuration:\n$$ w_i = e^{-\\beta \\Delta U_i} $$\nwhere $\\Delta U_i = U_1(\\mathbf{x}_i) - U_0(\\mathbf{x}_i)$ are the provided energy differences. The sample mean of these weights, $\\bar{w}$, serves as the estimator for the ensemble average:\n$$ \\bar{w} = \\frac{1}{N} \\sum_{i=1}^{N} w_i $$\nThis leads to the computational formula for the estimated free energy change:\n$$ \\Delta F = -RT \\ln \\bar{w} $$\nThe next step is to estimate the statistical error in $\\Delta F$. Since $\\Delta F$ is a function of the sample mean $\\bar{w}$, we can propagate the uncertainty from $\\bar{w}$ to $\\Delta F$. The standard error of the mean, $\\text{SE}(\\bar{w})$, is given by:\n$$ \\text{SE}(\\bar{w}) = \\frac{s_w}{\\sqrt{N}} $$\nwhere $s_w$ is the sample standard deviation of the weights. The sample variance, $s_w^2$, is calculated using the unbiased estimator for $N > 1$:\n$$ s_w^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (w_i - \\bar{w})^2 $$\nFor the special case where $N=1$, the problem specifies that $s_w^2=0$, which implies $s_w=0$ and thus $\\text{SE}(\\bar{w})=0$.\n\nTo find the standard error of $\\Delta F$, denoted $\\text{SE}(\\Delta F)$, we use the delta method, which applies a first-order Taylor expansion to propagate the variance. For a generic function $f(m)$, the variance of $f(\\bar{w})$ is approximately:\n$$ \\text{Var}(f(\\bar{w})) \\approx [f'(\\mathbb{E}[\\bar{w}])]^2 \\text{Var}(\\bar{w}) $$\nIn our case, the function is $f(m) = -RT \\ln m$. Its derivative with respect to $m$ is:\n$$ f'(m) = \\frac{d}{dm}(-RT \\ln m) = -\\frac{RT}{m} $$\nWe evaluate this derivative at the sample mean $\\bar{w}$ and use the estimated variance of the mean, $\\widehat{\\text{Var}}(\\bar{w}) = (\\text{SE}(\\bar{w}))^2 = s_w^2/N$. The variance of $\\Delta F$ is then:\n$$ \\text{Var}(\\Delta F) \\approx \\left( -\\frac{RT}{\\bar{w}} \\right)^2 \\frac{s_w^2}{N} = \\left( \\frac{RT}{\\bar{w}} \\right)^2 \\frac{s_w^2}{N} $$\nThe standard error of $\\Delta F$ is the square root of its variance:\n$$ \\text{SE}(\\Delta F) = \\sqrt{\\text{Var}(\\Delta F)} = \\left| \\frac{RT}{\\bar{w}} \\right| \\frac{s_w}{\\sqrt{N}} $$\nSince $R$, $T$, and $\\bar{w}$ (as a mean of positive numbers, $w_i > 0$) are all positive, the absolute value is unnecessary.\n$$ \\text{SE}(\\Delta F) = \\frac{RT}{\\bar{w}} \\frac{s_w}{\\sqrt{N}} $$\nThis formula is used for cases where $N > 1$. For $N=1$, since $s_w=0$, the standard error $\\text{SE}(\\Delta F)$ is also $0$.\n\nThe algorithm for each test case is as follows:\n1.  Given the temperature $T$ and a set of energy differences $\\{\\Delta U_i\\}$, determine the sample size $N$.\n2.  Use the provided constant $R = 8.314462618 \\times 10^{-3}$ kJ mol$^{-1}$ K$^{-1}$ to compute the thermal energy $RT$ and the molar inverse temperature $\\beta = (RT)^{-1}$.\n3.  Calculate the set of weights $\\{w_i\\}$ using $w_i = e^{-\\beta \\Delta U_i}$.\n4.  Compute the sample mean $\\bar{w}$ of the weights.\n5.  Compute the free energy change $\\Delta F = -RT \\ln \\bar{w}$.\n6.  If $N > 1$, compute the unbiased sample variance $s_w^2 = \\frac{1}{N-1}\\sum(w_i - \\bar{w})^2$. Then, calculate the standard error $\\text{SE}(\\Delta F) = \\frac{RT}{\\bar{w}} \\sqrt{\\frac{s_w^2}{N}}$.\n7.  If $N \\le 1$, set $\\text{SE}(\\Delta F) = 0$.\n8.  Round both $\\Delta F$ and $\\text{SE}(\\Delta F)$ to six decimal places and format the output as a two-element list.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the free energy change and its standard error using the Free\n    Energy Perturbation (FEP) method for a suite of test cases.\n    \"\"\"\n    # Molar gas constant in kJ mol^-1 K^-1\n    R = 8.314462618e-3\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (300.0, np.array([-1.2, 0.5, 0.0, 1.3, -0.7, 0.2])),\n        (298.15, np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])),\n        (300.0, np.array([12.0, 0.0, 5.0, -1.0, 3.0, 8.0, -2.0])),\n        (100.0, np.array([0.5, 1.0, -0.2, 0.3, -0.1, 0.0])),\n        (1000.0, np.array([5.0, -5.0, 2.0, -2.0, 0.0])),\n    ]\n\n    results = []\n    for T, delta_U_values in test_cases:\n        # Number of samples\n        N = len(delta_U_values)\n\n        # Thermal energy in kJ mol^-1\n        RT = R * T\n        \n        # Molar inverse temperature in mol kJ^-1\n        if RT == 0:\n            # Avoid division by zero, although not expected for T > 0\n            # If RT is zero, FEP is ill-defined. We can treat this as an edge case.\n            # However, all given T are positive. This is for robustness.\n            # A very large beta would lead to weights being 0 or inf.\n            # For simplicity, we proceed, but np.exp will handle large args.\n            beta = np.inf\n        else:\n            beta = 1.0 / RT\n\n        # Calculate weights w_i = exp(-beta * deltaU_i)\n        weights = np.exp(-beta * delta_U_values)\n\n        # Calculate the sample mean of the weights\n        w_bar = np.mean(weights)\n\n        # Calculate free energy change in kJ mol^-1\n        # np.log is the natural logarithm\n        if w_bar <= 0:\n            # Logarithm of non-positive number is undefined.\n            # Weights are always positive, so w_bar should be positive.\n            # This is a safeguard.\n            delta_F = np.nan\n        else:\n            delta_F = -RT * np.log(w_bar)\n\n        # Estimate the standard error of the free energy change\n        if N > 1:\n            # Unbiased sample variance of weights (ddof=1)\n            s_w_sq = np.var(weights, ddof=1)\n            \n            # Standard error of Delta F using the delta method\n            # SE(ΔF) = (RT/w_bar) * SE(w_bar)\n            # SE(w_bar) = s_w / sqrt(N) = sqrt(s_w_sq / N)\n            if w_bar > 0:\n                se_delta_F = (RT / w_bar) * np.sqrt(s_w_sq / N)\n            else:\n                 se_delta_F = np.nan\n        else:\n            # As per problem statement for N=1 (or N<=1)\n            se_delta_F = 0.0\n\n        # Round to six decimal places\n        delta_F_rounded = round(delta_F, 6)\n        se_delta_F_rounded = round(se_delta_F, 6)\n\n        results.append(f\"[{delta_F_rounded},{se_delta_F_rounded}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the Zwanzig equation is fundamental, its efficiency drops when the two states being compared are very different. The Multistate Bennett Acceptance Ratio (MBAR) method provides a powerful solution by optimally combining data from multiple intermediate states. In this practice, you will implement the iterative MBAR equations from first principles, a task that requires careful attention to numerical stability, and estimate the uncertainty using bootstrap resampling . Mastering MBAR is a key step toward performing state-of-the-art free energy calculations.",
            "id": "3453638",
            "problem": "You are provided a scenario in which three thermodynamic states, indexed by $k \\in \\{0,1,2\\}$, are sampled under the canonical ensemble at temperature $T$. For each state $k$, you have $N_k$ independent samples of molecular configurations. For each configuration across all states, the reduced potentials $u_k(x)$ are known for all three states. The reduced potential is defined as $u_k(x) = \\beta U_k(x)$, where $U_k(x)$ is the potential energy and $\\beta = 1/(k_B T)$ with $k_B$ the Boltzmann constant. The goal is to estimate the dimensionless free energy offsets $f_k$ (which satisfy $Z_k = \\exp(-f_k)$ where $Z_k$ is the partition function of state $k$) by solving the Multistate Bennett Acceptance Ratio (MBAR) self-consistent equations iteratively. From these $f_k$, compute the free energy difference $\\Delta F_{0\\to 2} = k_B T (f_2 - f_0)$ and report it in kilojoules per mole, along with a $95\\%$ confidence interval obtained by nonparametric bootstrap resampling of configurations within each state block.\n\nStarting only from core principles of equilibrium statistical mechanics and maximum likelihood reasoning in a mixture of canonical ensembles, implement a numerically stable iterative solver for the MBAR equations without using any pre-derived shortcut formulas. Use a stable computation strategy (e.g., a log-sum-exp transformation) to avoid numerical underflow or overflow. Convergence should be declared when the maximum absolute change in $f_k$ across iterations falls below $10^{-12}$, or after a maximum of $10^{4}$ iterations, whichever comes first. Fix the gauge by reporting free energies relative to state $k=0$ (i.e., report $f_k - f_0$ values, though internally, any consistent gauge is acceptable since only differences matter).\n\nConfidence intervals must be computed by bootstrap resampling with replacement within each state’s block of configurations. For each bootstrap replicate, resample $N_k$ rows from the rows originally associated with state $k$, concatenate the resampled blocks in the original order to form the replicate dataset, re-solve MBAR to obtain the replicate $\\Delta F_{0\\to 2}$, and collect the empirical distribution of these replicate free energy differences. Use $300$ bootstrap replicates and a fixed random seed of $123$ to ensure reproducibility. The $95\\%$ confidence interval should be reported as the empirical $2.5$th and $97.5$th percentiles of the bootstrap distribution. Express $\\Delta F_{0\\to 2}$ in $\\mathrm{kJ/mol}$ using $R = N_A k_B$ with $R = 8.314462618 \\times 10^{-3} \\; \\mathrm{kJ/(mol\\cdot K)}$ and the provided temperature $T$ for each test case, and round all reported floating-point outputs to six decimal places.\n\nConstruct and solve the following test suite of three cases. In every case, the matrix $U$ is of shape $N_{\\mathrm{tot}} \\times 3$ with rows ordered by state blocks: the first $N_0$ rows are samples originating from state $0$, the next $N_1$ rows from state $1$, and the final $N_2$ rows from state $2$. Each row contains the reduced potentials $[u_0(x), u_1(x), u_2(x)]$ of that configuration under all three states.\n\n- Case A (balanced sampling, moderate separation):\n  - Temperature: $T = 300.0$ $\\mathrm{K}$.\n  - Sample counts: $(N_0, N_1, N_2) = (12,12,12)$.\n  - Reduced energy matrix $U \\in \\mathbb{R}^{36 \\times 3}$ is defined blockwise by the deterministic sequences:\n    - For state $0$ block ($j = 0,1,\\dots,11$): $[u_0, u_1, u_2] = [0.20 + 0.05 j,\\; 1.10 + 0.04 j,\\; 2.00 + 0.03 j]$.\n    - For state $1$ block ($j = 0,1,\\dots,11$): $[u_0, u_1, u_2] = [0.30 + 0.03 j,\\; 0.90 + 0.05 j,\\; 1.80 + 0.06 j]$.\n    - For state $2$ block ($j = 0,1,\\dots,11$): $[u_0, u_1, u_2] = [0.40 + 0.06 j,\\; 0.80 + 0.04 j,\\; 1.60 + 0.05 j]$.\n\n- Case B (highly unbalanced sampling):\n  - Temperature: $T = 310.0$ $\\mathrm{K}$.\n  - Sample counts: $(N_0, N_1, N_2) = (5,20,5)$.\n  - Reduced energy matrix $U \\in \\mathbb{R}^{30 \\times 3}$ is defined blockwise by the deterministic sequences:\n    - For state $0$ block ($j = 0,1,\\dots,4$): $[u_0, u_1, u_2] = [0.50 + 0.02 j,\\; 0.70 + 0.03 j,\\; 1.20 + 0.04 j]$.\n    - For state $1$ block ($j = 0,1,\\dots,19$): $[u_0, u_1, u_2] = [0.60 + 0.04 j,\\; 0.65 + 0.02 j,\\; 1.00 + 0.03 j]$.\n    - For state $2$ block ($j = 0,1,\\dots,4$): $[u_0, u_1, u_2] = [0.55 + 0.03 j,\\; 0.75 + 0.02 j,\\; 1.15 + 0.03 j]$.\n\n- Case C (degenerate identical energies across states):\n  - Temperature: $T = 298.0$ $\\mathrm{K}$.\n  - Sample counts: $(N_0, N_1, N_2) = (10,10,10)$.\n  - Reduced energy matrix $U \\in \\mathbb{R}^{30 \\times 3}$ has every row equal to $[1.50, 1.50, 1.50]$.\n\nYour program must:\n- Implement an iterative MBAR solver using a numerically stable log-sum-exp procedure.\n- Compute $\\Delta F_{0\\to 2}$ in $\\mathrm{kJ/mol}$ for each case and its $95\\%$ confidence interval by bootstrap resampling as specified.\n- Produce a single line of output containing the results as a comma-separated list of lists, each inner list being $[\\Delta F_{0\\to 2}, \\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}]$ for the three cases, with all values rounded to six decimal places. For example, the output format must be exactly of the form: \"[[x1,y1,z1],[x2,y2,z2],[x3,y3,z3]]\" (with numeric values in place of symbols).\n\nAll answers must be expressed in $\\mathrm{kJ/mol}$ and rounded to six decimal places. Angles are not involved. Percentages must not be used; quantiles should be given as decimal fractions via numeric values. Your implementation must be fully self-contained and runnable without user input or external files.",
            "solution": "The problem requires the implementation of the Multistate Bennett Acceptance Ratio (MBAR) method to calculate free energy differences from simulation data, including a robust estimation of statistical uncertainty using a bootstrap procedure. The implementation must be derived from foundational principles of statistical mechanics and maximum likelihood theory, and must be numerically stable.\n\n**1. Theoretical Foundation of MBAR**\n\nThe core of the problem lies in estimating the dimensionless free energy $f_k = -\\ln Z_k$ for a set of $K$ thermodynamic states, where $Z_k$ is the canonical partition function of state $k$. The probability density of observing a microscopic configuration $x$ in state $k$ is given by the Boltzmann distribution:\n$$ p_k(x) = \\frac{\\exp(-u_k(x))}{Z_k} = \\exp(f_k - u_k(x)) $$\nwhere $u_k(x) = \\beta U_k(x)$ is the reduced potential energy at inverse temperature $\\beta = 1/(k_B T)$.\n\nThe input data consists of $N_{\\mathrm{tot}} = \\sum_{k=0}^{K-1} N_k$ configurations, $\\{x_n\\}_{n=1}^{N_{\\mathrm{tot}}}$. For each configuration $x_n$, we know the state $s_n \\in \\{0, \\ldots, K-1\\}$ from which it was sampled, and we are given the matrix of reduced potentials $u_j(x_n)$ for all states $j=0, \\ldots, K-1$.\n\nThe MBAR method provides an optimal estimate of the free energies by combining all data. It is derived by maximizing the logarithm of the likelihood of observing the complete dataset as a function of the unknown free energies $\\{f_k\\}$. The log-likelihood function, treating the samples as being drawn from a mixture of the canonical ensembles, is given by:\n$$ \\ln \\mathcal{L}(\\mathbf{f}) = \\sum_{n=1}^{N_{\\mathrm{tot}}} \\ln \\left( \\frac{\\exp(f_{s_n} - u_{s_n}(x_n))}{\\sum_{j=0}^{K-1} N_j \\exp(f_j)} \\right) $$\nMaximizing this log-likelihood with respect to the set of free energies $\\mathbf{f} = \\{f_0, \\dots, f_{K-1}\\}$ yields a set of coupled, self-consistent equations. After some algebraic manipulation, these equations can be written in a form that is convenient for iterative solution:\n$$ \\exp(-f_k) = \\sum_{n=1}^{N_{\\mathrm{tot}}} \\frac{\\exp(-u_k(x_n))}{\\sum_{j=0}^{K-1} N_j \\exp(f_j - u_j(x_n))} \\quad \\text{for } k=0, \\ldots, K-1 $$\nThis is the central system of equations for MBAR.\n\n**2. Algorithmic Implementation**\n\nThe MBAR equations are of the form $\\mathbf{f} = G(\\mathbf{f})$ and can be solved using a fixed-point iteration scheme, $\\mathbf{f}^{(m+1)} = G(\\mathbf{f}^{(m)})$, starting from an initial guess such as $\\mathbf{f}^{(0)} = \\mathbf{0}$.\n\n**Numerical Stability:**\nDirectly computing the exponentials in the MBAR equations is numerically hazardous due to the large range of values typical for potential energies, which can lead to floating-point overflow or underflow. To ensure stability, we reformulate the calculation using logarithms and the log-sum-exp (LSE) function, defined as $\\mathrm{LSE}(\\mathbf{v}) = \\ln(\\sum_i e^{v_i})$. This function can be computed robustly as:\n$$ \\mathrm{LSE}(\\mathbf{v}) = v_{\\max} + \\ln\\left(\\sum_i e^{v_i - v_{\\max}}\\right) $$\nwhere $v_{\\max} = \\max_i v_i$.\n\nWe can rewrite the iterative update for $f_k$ in a numerically stable form. First, let's define a log-space denominator term for each sample $n$:\n$$ \\log S_n = \\ln\\left(\\sum_{j=0}^{K-1} N_j \\exp(f_j - u_j(x_n))\\right) = \\mathrm{LSE}_j(\\ln N_j + f_j - u_j(x_n)) $$\nThe subscript on $\\mathrm{LSE}_j$ indicates that the operation is performed over the state indices $j$. With this, the equation for $\\exp(-f_k)$ becomes:\n$$ \\exp(-f_k) = \\sum_{n=1}^{N_{\\mathrm{tot}}} \\exp(-u_k(x_n) - \\log S_n) $$\nTaking the negative logarithm of both sides gives the update rule for $f_k$:\n$$ f_k^{(m+1)} = -\\ln\\left(\\sum_{n=1}^{N_{\\mathrm{tot}}} \\exp(-u_k(x_n) - \\log S_n^{(m)})\\right) = -\\mathrm{LSE}_n(-u_k(x_n) - \\log S_n^{(m)}) $$\nwhere $\\log S_n^{(m)}$ is computed using the free energies from the previous iteration, $\\mathbf{f}^{(m)}$.\n\n**Gauge Fixing:**\nThe MBAR equations only determine the free energies up to an arbitrary additive constant. This is because adding a constant $C$ to all $f_k$ values leaves the equations unchanged. To obtain a unique solution, we must fix this \"gauge\". A common choice is to set the free energy of one state (e.g., state $0$) to zero. In the iterative solver, this is enforced after each update step by shifting all free energies: $f_k \\leftarrow f_k - f_0$.\n\n**Convergence:**\nThe iteration proceeds until the solution converges. Convergence is declared when the maximum absolute change in any of the free energy estimates between two successive iterations falls below a specified tolerance, $\\tau = 10^{-12}$:\n$$ \\max_k |f_k^{(m+1)} - f_k^{(m)}| < \\tau $$\nA maximum number of iterations, $10^4$, is also set to prevent infinite loops in non-convergent cases.\n\n**3. Uncertainty Quantification via Bootstrap**\n\nTo estimate the statistical uncertainty in the computed free energy difference, $\\Delta F_{0\\to 2}$, we use a nonparametric bootstrap method.\n\n**Procedure:**\n1.  **Resampling**: For a specified number of replicates ($300$ in this problem), we generate a new \"bootstrap\" dataset. This dataset is constructed by sampling *with replacement* from the original configurations. The resampling is stratified by state: for each state $k$, we randomly draw $N_k$ configurations from the original block of $N_k$ configurations belonging to that state.\n2.  **Re-solving**: For each of the $300$ bootstrap datasets, we solve the MBAR equations to obtain a replicate set of free energies $\\{f_{k, \\text{boot}}\\}$.\n3.  **Distribution**: From each replicate, we calculate the target quantity, $\\Delta F_{0\\to 2, \\text{boot}} = R T (f_{2, \\text{boot}} - f_{0, \\text{boot}})$, where $R = 8.314462618 \\times 10^{-3} \\; \\mathrm{kJ/(mol \\cdot K)}$. This process yields an empirical probability distribution for $\\Delta F_{0\\to 2}$.\n4.  **Confidence Interval**: The $95\\%$ confidence interval is constructed by finding the $2.5$th and $97.5$th percentiles of the sorted bootstrap distribution of $\\Delta F_{0\\to 2}$ values. A fixed random seed ($123$) is used for the resampling process to ensure reproducibility.\n\nThis comprehensive approach provides not only a point estimate for the free energy difference but also a robust measure of its statistical precision, grounded in the variability of the underlying sampled data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Main function to solve the MBAR problem for the three specified test cases.\n    \"\"\"\n    R_KJ_MOL_K = 8.314462618e-3\n    N_BOOTSTRAP = 300\n    RANDOM_SEED = 123\n\n    def generate_test_cases():\n        \"\"\"Generates the data for the three test cases as specified in the problem.\"\"\"\n        # Case A\n        T_A = 300.0\n        N_A = np.array([12, 12, 12])\n        u_A_blocks = []\n        j_vals_A = np.arange(12)\n        u_A_blocks.append(np.array([[0.20 + 0.05*j, 1.10 + 0.04*j, 2.00 + 0.03*j] for j in j_vals_A]))\n        u_A_blocks.append(np.array([[0.30 + 0.03*j, 0.90 + 0.05*j, 1.80 + 0.06*j] for j in j_vals_A]))\n        u_A_blocks.append(np.array([[0.40 + 0.06*j, 0.80 + 0.04*j, 1.60 + 0.05*j] for j in j_vals_A]))\n        U_A = np.vstack(u_A_blocks)\n\n        # Case B\n        T_B = 310.0\n        N_B = np.array([5, 20, 5])\n        u_B_blocks = []\n        u_B_blocks.append(np.array([[0.50 + 0.02*j, 0.70 + 0.03*j, 1.20 + 0.04*j] for j in np.arange(5)]))\n        u_B_blocks.append(np.array([[0.60 + 0.04*j, 0.65 + 0.02*j, 1.00 + 0.03*j] for j in np.arange(20)]))\n        u_B_blocks.append(np.array([[0.55 + 0.03*j, 0.75 + 0.02*j, 1.15 + 0.03*j] for j in np.arange(5)]))\n        U_B = np.vstack(u_B_blocks)\n\n        # Case C\n        T_C = 298.0\n        N_C = np.array([10, 10, 10])\n        U_C = np.full((sum(N_C), 3), 1.50)\n        \n        return [\n            (U_A, N_A, T_A),\n            (U_B, N_B, T_B),\n            (U_C, N_C, T_C)\n        ]\n\n    def compute_f_k(u_nk, N_k, tol=1e-12, max_iter=10000):\n        \"\"\"\n        Solves the MBAR equations iteratively to find the dimensionless free energies.\n        \n        Args:\n            u_nk (np.ndarray): The reduced potential energy matrix of shape (N_tot, K).\n            N_k (np.ndarray): An array of sample counts for each state.\n            tol (float): Convergence tolerance.\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            np.ndarray: The converged dimensionless free energies, with f_0 fixed to 0.\n        \"\"\"\n        K = len(N_k)\n        log_N_k = np.log(N_k)\n        f_k = np.zeros(K)\n\n        for _ in range(max_iter):\n            f_old = np.copy(f_k)\n            \n            # log_S_n[n] = log(sum_j N_j exp(f_j - u_jn)) via LSE\n            # The argument to logsumexp for each sample n is an array over states j:\n            # [log(N_0)+f_0-u_0n, log(N_1)+f_1-u_1n, ...]\n            log_S_n_arg = log_N_k + f_k - u_nk\n            log_S_n = logsumexp(log_S_n_arg, axis=1)\n\n            # Update f_k using LSE over samples n\n            # f_k_new[k] = -log(sum_n exp(-u_kn - log_S_n))\n            for k in range(K):\n                arg = -u_nk[:, k] - log_S_n\n                f_k[k] = -logsumexp(arg)\n            \n            # Apply gauge f_0 = 0 for stability\n            f_k -= f_k[0]\n\n            if np.max(np.abs(f_k - f_old)) < tol:\n                break\n        \n        return f_k\n\n    test_cases = generate_test_cases()\n    all_results = []\n    \n    for u_nk, N_k, T in test_cases:\n        # 1. Calculate the point estimate\n        f_k_point = compute_f_k(u_nk, N_k)\n        delta_F_point = (f_k_point[2] - f_k_point[0]) * R_KJ_MOL_K * T\n\n        # 2. Perform bootstrap analysis for confidence interval\n        rng = np.random.default_rng(RANDOM_SEED)\n        delta_F_boots = np.zeros(N_BOOTSTRAP)\n        \n        # Prepare indices for stratified resampling\n        N_tot = sum(N_k)\n        state_indices_blocks = []\n        start_idx = 0\n        for count in N_k:\n            state_indices_blocks.append(np.arange(start_idx, start_idx + count))\n            start_idx += count\n\n        for i in range(N_BOOTSTRAP):\n            # Create bootstrap sample indices\n            bootstrap_indices = []\n            for k in range(len(N_k)):\n                indices_for_state_k = rng.choice(\n                    state_indices_blocks[k], size=N_k[k], replace=True\n                )\n                bootstrap_indices.append(indices_for_state_k)\n            \n            final_boot_indices = np.concatenate(bootstrap_indices)\n            u_nk_boot = u_nk[final_boot_indices, :]\n            \n            # Solve MBAR for the bootstrap sample\n            f_k_boot = compute_f_k(u_nk_boot, N_k)\n            delta_F_boots[i] = (f_k_boot[2] - f_k_boot[0]) * R_KJ_MOL_K * T\n\n        # 3. Calculate 95% CI from bootstrap distribution\n        ci_low = np.percentile(delta_F_boots, 2.5)\n        ci_high = np.percentile(delta_F_boots, 97.5)\n\n        # 4. Store results rounded to 6 decimal places\n        all_results.append([\n            round(delta_F_point, 6),\n            round(ci_low, 6),\n            round(ci_high, 6)\n        ])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "A calculated free energy difference is only meaningful if the underlying simulations have adequately sampled the relevant configurations. For multi-stage methods, this requires sufficient phase space overlap between adjacent states. This practice introduces the overlap matrix, a critical diagnostic tool used to assess the quality of sampling and automatically flag potentially unreliable segments of an alchemical path . By working through a model system, you will develop an intuition for how factors like the energetic separation of states and temperature influence the reliability of your free energy calculations.",
            "id": "3453645",
            "problem": "You are given an alchemical path discretized into $\\lambda$ windows indexed by $i \\in \\{0,1,\\dots,L-1\\}$. For each pair of windows $(i,j)$, define the diagnostic overlap matrix entry as the ensemble average under window $i$,\n$$\nW_{ij} \\equiv \\left\\langle \\exp\\left(-\\beta \\left(U_j - U_i\\right)\\right) \\right\\rangle_i,\n$$\nwhere $U_i$ and $U_j$ are the potential energies of the same configuration evaluated under windows $i$ and $j$, respectively, and $\\beta = \\frac{1}{k_{\\mathrm{B}} T}$ with temperature $T$ (in $\\mathrm{K}$) and Boltzmann constant $k_{\\mathrm{B}}$ (in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$). This setup is used in Free Energy Perturbation (FEP) diagnostics to assess configuration-space overlap.\n\nAssume the following idealized, yet scientifically plausible model for the energy differences along the alchemical path:\n- The path consists of $L$ windows with successive increments in $\\lambda$ that are independent and identically distributed when viewed as energy differences.\n- For a single step between adjacent windows, the energy difference $\\Delta U_{i \\to i+1} \\equiv U_{i+1} - U_i$ sampled under ensemble $i$ is modeled as a normal random variable with mean $a$ (in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$) and variance $s^2$ (in $(\\mathrm{kJ}\\,\\mathrm{mol}^{-1})^2$), denoted $\\mathcal{N}(a, s^2)$.\n- By independence of increments, the $n$-step difference obeys $\\Delta U_{i \\to j} \\sim \\mathcal{N}\\!\\left((j-i)\\,a, |j-i|\\,s^2\\right)$ when sampled under ensemble $i$.\n\nTasks:\n1. Starting from the definition of $W_{ij}$ and the given Gaussian model for $\\Delta U_{i \\to j}$, derive an explicit expression for $W_{ij}$ in terms of $a$, $s$, $i$, $j$, and $\\beta$, without using any unproven shortcut formulas. Clearly state each mathematical step and principle you use.\n2. Using your derived expression, construct the full matrix $W \\in \\mathbb{R}^{L \\times L}$ for given $L$, $a$, $s$, and $T$.\n3. Define an automatic flagging rule for insufficient overlap between adjacent windows: a segment $(i,i+1)$ is considered sufficiently overlapped if and only if both directional overlaps satisfy $\\min\\{W_{i,i+1}, W_{i+1,i}\\} \\ge \\tau$, where $\\tau$ is a given threshold (dimensionless). Otherwise, the segment is flagged as insufficient. Produce, for each parameter set, the list of all indices $i \\in \\{0,1,\\dots,L-2\\}$ for which the segment $(i,i+1)$ is flagged.\n4. Physical units: use $k_{\\mathrm{B}} = 0.00831446261815324$ in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}\\,\\mathrm{K}^{-1}$ and $T$ in $\\mathrm{K}$. All energies $a$ and $s$ must be in $\\mathrm{kJ}\\,\\mathrm{mol}^{-1}$. The diagnostic overlaps $W_{ij}$ are dimensionless.\n5. Implement a complete, runnable program that computes the required outputs for the following test suite. Your program should not read input; it must internally use these parameter sets.\n\nTest suite (each case is $(L, T, a, s, \\tau)$):\n- Case A (typical good overlap): $(L=\\;5,\\; T=\\;300,\\; a=\\;0.5,\\; s=\\;1.0,\\; \\tau=\\;0.03)$.\n- Case B (poor overlap due to large mean mismatch): $(L=\\;6,\\; T=\\;300,\\; a=\\;9.0,\\; s=\\;1.0,\\; \\tau=\\;0.03)$.\n- Case C (boundary condition chosen so that the adjacent forward overlap equals the threshold at one step): $(L=\\;4,\\; T=\\;300,\\; a=\\;\\text{chosen so that the one-step forward overlap equals }\\tau,\\; s=\\;1.0,\\; \\tau=\\;0.03)$. You must determine the corresponding value of $a$ under this condition using your derived expression, with the given $T$ and $s$.\n- Case D (higher temperature improves effective overlap): $(L=\\;3,\\; T=\\;600,\\; a=\\;9.0,\\; s=\\;1.0,\\; \\tau=\\;0.03)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of Python lists, where each inner list corresponds to one test case and contains the indices $i$ (integers) of flagged adjacent segments $(i,i+1)$ for that case. For example, an output with four cases might look like $[[],[0,1,2],[],[1]]$.\n- No additional text should be printed; only the single line with the list should be produced.",
            "solution": "The problem requires the derivation of a diagnostic overlap matrix for an alchemical path, its use in assessing sampling quality between adjacent simulation windows, and the implementation of this logic in a program. The validation of the problem statement confirms that it is scientifically sound, well-posed, and complete. We can therefore proceed with a full solution.\n\n### Part 1: Derivation of the Overlap Matrix Entry $W_{ij}$\n\nThe diagnostic overlap matrix entry $W_{ij}$ is defined by the ensemble average:\n$$\nW_{ij} \\equiv \\left\\langle \\exp\\left(-\\beta \\left(U_j - U_i\\right)\\right) \\right\\rangle_i\n$$\nwhere the average is taken over the statistical ensemble of window $i$. The term $\\Delta U_{i \\to j} \\equiv U_j - U_i$ is a random variable when evaluated over the configurations sampled from ensemble $i$. The problem states that $\\Delta U_{i \\to j}$ follows a normal (Gaussian) distribution:\n$$\n\\Delta U_{i \\to j} \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\nwith mean $\\mu = (j-i)a$ and variance $\\sigma^2 = |j-i|s^2$.\n\nTo compute $W_{ij}$, we need to find the expectation of $\\exp(-\\beta X)$ where $X$ is a normally distributed random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. This is a direct application of the moment-generating function (MGF) of the normal distribution. The MGF of a random variable $X$ is defined as $M_X(t) = E[\\exp(tX)]$. For a normally distributed variable, the MGF is a standard result:\n$$\nM_X(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nWe can derive this by integrating the definition of the expectation:\n$$\nE[\\exp(tX)] = \\int_{-\\infty}^{\\infty} \\exp(tx) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx\n$$\nThe argument of the combined exponential is:\n$$\ntx - \\frac{(x-\\mu)^2}{2\\sigma^2} = -\\frac{1}{2\\sigma^2} [ (x^2 - 2x\\mu + \\mu^2) - 2\\sigma^2tx ] = -\\frac{1}{2\\sigma^2} [ x^2 - 2x(\\mu + t\\sigma^2) + \\mu^2 ]\n$$\nBy completing the square for the terms involving $x$:\n$$\nx^2 - 2x(\\mu + t\\sigma^2) = (x - (\\mu+t\\sigma^2))^2 - (\\mu+t\\sigma^2)^2\n$$\nThe argument of the exponential becomes:\n$$\n-\\frac{1}{2\\sigma^2} [ (x - (\\mu+t\\sigma^2))^2 - (\\mu+t\\sigma^2)^2 + \\mu^2 ] = -\\frac{(x - (\\mu+t\\sigma^2))^2}{2\\sigma^2} + \\frac{(\\mu+t\\sigma^2)^2 - \\mu^2}{2\\sigma^2}\n$$\nThe second term simplifies to:\n$$\n\\frac{\\mu^2 + 2\\mu t\\sigma^2 + t^2\\sigma^4 - \\mu^2}{2\\sigma^2} = \\mu t + \\frac{1}{2}\\sigma^2 t^2\n$$\nThe integral for the MGF thus separates into:\n$$\nM_X(t) = \\exp\\left(\\mu t + \\frac{1}{2}\\sigma^2 t^2\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - (\\mu+t\\sigma^2))^2}{2\\sigma^2}\\right) dx\n$$\nThe integral is over the PDF of a normal distribution with mean $\\mu' = \\mu + t\\sigma^2$ and variance $\\sigma^2$. The integral of any PDF over its domain is $1$. Therefore, the MGF is proven to be $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$.\n\nTo find $W_{ij}$, we set $X = \\Delta U_{i \\to j}$ and evaluate its MGF at $t = -\\beta$:\n$$\nW_{ij} = E[\\exp(-\\beta \\Delta U_{i \\to j})] = M_{\\Delta U_{i \\to j}}(-\\beta)\n$$\nSubstituting $\\mu = (j-i)a$, $\\sigma^2 = |j-i|s^2$, and $t = -\\beta$ into the MGF formula, we obtain the final expression for $W_{ij}$:\n$$\nW_{ij} = \\exp\\left( (j-i)a(-\\beta) + \\frac{1}{2}|j-i|s^2(-\\beta)^2 \\right) = \\exp\\left( -(j-i)a\\beta + \\frac{1}{2}|j-i|s^2\\beta^2 \\right)\n$$\n\n### Part 2 & 3: Flagging Rule for Adjacent Windows\n\nThe problem defines a flagging rule for insufficient overlap between adjacent windows $(i, i+1)$. A segment is flagged if $\\min\\{W_{i,i+1}, W_{i+1,i}\\} < \\tau$. To apply this rule, we need expressions for the forward overlap $W_{i,i+1}$ and the backward overlap $W_{i+1,i}$.\n\nFor the forward overlap, $W_{i,i+1}$, we set $j=i+1$ in our derived formula. Here, $j-i=1$ and $|j-i|=1$:\n$$\nW_{i,i+1} = \\exp\\left( -a\\beta + \\frac{1}{2}s^2\\beta^2 \\right)\n$$\nFor the backward overlap, $W_{i+1,i}$, the roles of $i$ and $j$ are swapped. We are calculating $\\langle \\exp(-\\beta(U_i - U_{i+1})) \\rangle_{i+1}$. The random variable is now $\\Delta U_{i+1 \\to i}$, sampled from ensemble $i+1$. According to the model, its distribution is $\\mathcal{N}((i-(i+1))a, |i-(i+1)|s^2) = \\mathcal{N}(-a, s^2)$. We use the MGF result with $\\mu = -a$ and $\\sigma^2 = s^2$, evaluated at $t=-\\beta$:\n$$\nW_{i+1,i} = \\exp\\left( (-a)(-\\beta) + \\frac{1}{2}s^2(-\\beta)^2 \\right) = \\exp\\left( a\\beta + \\frac{1}{2}s^2\\beta^2 \\right)\n$$\nBoth $W_{i,i+1}$ and $W_{i+1,i}$ are independent of the index $i$, which is a consequence of the assumption that the energy increments are identically distributed along the path.\n\nThe flagging condition is based on the minimum of these two values. The term $\\exp(\\frac{1}{2}s^2\\beta^2)$ is a common factor. The exponents differ by the sign of the $a\\beta$ term. Assuming $\\beta > 0$, the minimum is determined by the term with the negative coefficient of $|a|$:\n$$\n\\min\\{W_{i,i+1}, W_{i+1,i}\\} = \\exp\\left( \\frac{1}{2}s^2\\beta^2 - |a|\\beta \\right)\n$$\nA segment $(i, i+1)$ is flagged if this quantity is less than $\\tau$. Since this check is independent of $i$, for any given set of parameters $(T, a, s, \\tau)$, either all adjacent segments are flagged, or none are.\n\nFor Case C, we must determine the value of $a$ that makes the one-step forward overlap equal to the threshold $\\tau$. Assuming $a \\ge 0$, this is $W_{i,i+1} = \\tau$:\n$$\n\\exp\\left( -a\\beta + \\frac{1}{2}s^2\\beta^2 \\right) = \\tau\n$$\nTaking the natural logarithm and solving for $a$:\n$$\n-a\\beta + \\frac{1}{2}s^2\\beta^2 = \\ln(\\tau) \\implies a\\beta = \\frac{1}{2}s^2\\beta^2 - \\ln(\\tau) \\implies a = \\frac{1}{2}s^2\\beta - \\frac{\\ln(\\tau)}{\\beta}\n$$\nWith this value of $a$, the flagging condition is $\\min\\{W_{i,i+1}, W_{i+1,i}\\} < \\tau$. Since we have $a \\ge 0$, the minimum is $W_{i,i+1}$, which we set to be exactly $\\tau$. The condition becomes $\\tau < \\tau$, which is false. Therefore, no segments will be flagged in Case C.\n\n### Part 4 & 5: Implementation\n\nThe logic above is implemented in Python. For each test case, we compute $\\beta = 1/(k_B T)$ and then evaluate the minimum overlap $\\min\\{W_{i,i+1}, W_{i+1,i}\\}$. If this value is less than $\\tau$, we generate a list of all possible indices $i$ from $0$ to $L-2$. Otherwise, we return an empty list. For Case C, the value of $a$ is calculated first as derived above.\n\nAll calculations are performed using the provided physical constants and parameters. The final output is a list containing the lists of flagged indices for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the FEP overlap problem for a suite of test cases.\n    \"\"\"\n    KB_KJ_PER_MOL_K = 0.00831446261815324\n\n    # Test suite: (L, T, a, s, tau)\n    # For Case C, 'a' is a placeholder string that signals its calculation.\n    test_cases = [\n        (5, 300.0, 0.5, 1.0, 0.03),   # Case A\n        (6, 300.0, 9.0, 1.0, 0.03),   # Case B\n        (4, 300.0, \"calculate\", 1.0, 0.03), # Case C\n        (3, 600.0, 9.0, 1.0, 0.03),   # Case D\n    ]\n\n    results = []\n    for L, T, a_val, s, tau in test_cases:\n        # Calculate inverse temperature beta\n        beta = 1.0 / (KB_KJ_PER_MOL_K * T)\n\n        a = a_val\n        # Special handling for Case C to determine 'a'\n        if a_val == \"calculate\":\n            # We solve for 'a' from the condition W_forward = tau, assuming a >= 0.\n            # exp(-a*beta + 0.5*s^2*beta^2) = tau\n            # -a*beta + 0.5*s^2*beta^2 = log(tau)\n            # a = (0.5*s^2*beta^2 - log(tau)) / beta\n            a = (0.5 * s**2 * beta**2 - np.log(tau)) / beta\n\n        # The flagging rule is based on min(W_i,i+1, W_i+1,i) < tau.\n        # This minimum is given by exp(0.5*s^2*beta^2 - |a|*beta).\n        # Since 'a' is non-negative in all test cases, |a| = a.\n        min_overlap = np.exp(0.5 * s**2 * beta**2 - np.abs(a) * beta)\n\n        flagged_indices = []\n        # The condition min_overlap < tau is independent of the index i.\n        # If it's true, all segments are flagged. Otherwise, none are.\n        if min_overlap < tau:\n            flagged_indices = list(range(L - 1))\n        \n        results.append(flagged_indices)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}