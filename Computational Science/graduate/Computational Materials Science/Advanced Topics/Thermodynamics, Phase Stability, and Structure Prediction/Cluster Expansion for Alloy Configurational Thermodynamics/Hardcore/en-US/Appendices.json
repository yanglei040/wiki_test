{
    "hands_on_practices": [
        {
            "introduction": "The central task in constructing a cluster expansion is determining the effective cluster interactions (ECIs) that link configurational geometry to energy. This is an inverse problem solved by fitting to a dataset of known structures and their energies. This exercise  provides a foundational, hands-on demonstration of this process by asking you to explicitly calculate correlation functions and solve for the ECIs in a minimal model system.",
            "id": "3437949",
            "problem": "Consider a binary alloy modeled on a one-dimensional periodic ring with $N=6$ lattice sites and Ising-like occupation variables $\\sigma_{i}\\in\\{+1,-1\\}$, where $\\sigma_{i}=+1$ denotes species $A$ and $\\sigma_{i}=-1$ denotes species $B$. The configurational energy per lattice site is represented by a truncated cluster expansion that retains only pair clusters up to second-nearest neighbors. For a given configuration, define the pair correlation $\\Pi_{\\alpha}$ for shell $\\alpha$ as\n$$\n\\Pi_{\\alpha} \\equiv \\frac{1}{M_{\\alpha}}\\sum_{\\langle i,j\\rangle_{\\alpha}} \\sigma_{i}\\sigma_{j},\n$$\nwhere $M_{\\alpha}$ is the number of distinct pairs in shell $\\alpha$, and the sum runs over all symmetry-equivalent pairs separated by the $\\alpha$-th neighbor distance. On the $N=6$ ring, each site has $z_{1}=2$ first-nearest neighbors and $z_{2}=2$ second-nearest neighbors, so $M_{1}=M_{2}=6$. Assume equiatomic composition and formation energies such that the empty-cluster and point contributions are constants that are absorbed into the reference energy (set to zero), so the energy per site $E$ depends linearly on the pair correlations with two unknown effective cluster interactions (ECIs) $J_{1}$ and $J_{2}$:\n$$\nE = J_{1}\\,\\Pi_{1} + J_{2}\\,\\Pi_{2}.\n$$\nConsider the following three distinct equiatomic configurations on the ring, specified by the sequence of $\\sigma_{i}$ along the ring:\n- Configuration $\\mathcal{A}$: $+1,-1,+1,-1,+1,-1$ (alternating $A$ and $B$),\n- Configuration $\\mathcal{B}$: $+1,+1,+1,-1,-1,-1$ (three $A$ followed by three $B$),\n- Configuration $\\mathcal{C}$: $+1,-1,-1,+1,+1,-1$.\n\nFor these three configurations, suppose the measured formation energies per site (from a hypothetical first-principles dataset) are\n$$\nE_{\\mathcal{A}}=1\\ \\text{eV},\\quad E_{\\mathcal{B}}=-\\frac{1}{3}\\ \\text{eV},\\quad E_{\\mathcal{C}}=-1\\ \\text{eV}.\n$$\nStarting from the definitions above and without invoking any unintroduced shortcut formulas, compute the pair correlations $\\Pi_{1}$ and $\\Pi_{2}$ for each configuration explicitly, assemble the linear system relating $\\{E_{\\mathcal{A}},E_{\\mathcal{B}},E_{\\mathcal{C}}\\}$ to $\\{J_{1},J_{2}\\}$, and solve for the two pair effective cluster interactions $J_{1}$ and $J_{2}$ exactly. Express the final values of $J_{1}$ and $J_{2}$ in electronvolts. No rounding is required; provide exact values.",
            "solution": "The problem requires the calculation of two effective cluster interactions (ECIs), $J_{1}$ and $J_{2}$, for a binary alloy on a one-dimensional periodic ring of $N=6$ lattice sites. The configurational energy per site, $E$, is modeled by a truncated cluster expansion:\n$$\nE = J_{1}\\,\\Pi_{1} + J_{2}\\,\\Pi_{2}\n$$\nwhere $\\Pi_{\\alpha}$ is the pair correlation for the $\\alpha$-th neighbor shell. The occupation of site $i$ is described by a variable $\\sigma_{i} \\in \\{+1, -1\\}$. The pair correlation is defined as:\n$$\n\\Pi_{\\alpha} \\equiv \\frac{1}{M_{\\alpha}}\\sum_{\\langle i,j\\rangle_{\\alpha}} \\sigma_{i}\\sigma_{j}\n$$\nwhere the sum is over the $M_{\\alpha}$ distinct pairs of $\\alpha$-th neighbors. For a one-dimensional periodic ring of $N=6$ sites, the first-nearest neighbors of site $i$ are sites $i+1$ and $i-1$, and the second-nearest neighbors are $i+2$ and $i-2$ (indices are taken modulo $6$). The total number of distinct first-neighbor pairs is $M_1=6$, and the total number of distinct second-neighbor pairs is $M_2=6$. The summation over all distinct pairs $\\langle i,j\\rangle_{\\alpha}$ can be computed for a periodic ring by summing $\\sigma_i \\sigma_{i+\\delta}$ over all sites $i$ from $1$ to $N$, where $\\delta$ is the neighbor distance ($\\delta=1$ for first neighbors, $\\delta=2$ for second neighbors) and periodic boundary conditions ($\\sigma_{N+k} = \\sigma_k$) are applied.\n\nThe procedure is to first compute the pair correlations $\\Pi_{1}$ and $\\Pi_{2}$ for each of the three given configurations, then use the provided energies $\\{E_{\\mathcal{A}}, E_{\\mathcal{B}}, E_{\\mathcal{C}}\\}$ to establish a system of linear equations for $J_{1}$ and $J_{2}$, and finally solve this system.\n\n**1. Calculation of Pair Correlations**\n\n**Configuration $\\mathcal{A}$: $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4}, \\sigma_{5}, \\sigma_{6}) = (+1, -1, +1, -1, +1, -1)$**\nFor first-nearest neighbors ($\\delta=1$):\nThe sum of products is $\\sum_{i=1}^{6} \\sigma_{i}\\sigma_{i+1} = \\sigma_1\\sigma_2 + \\sigma_2\\sigma_3 + \\sigma_3\\sigma_4 + \\sigma_4\\sigma_5 + \\sigma_5\\sigma_6 + \\sigma_6\\sigma_1$.\nSubstituting the values: $(+1)(-1) + (-1)(+1) + (+1)(-1) + (-1)(+1) + (+1)(-1) + (-1)(+1) = -1 -1 -1 -1 -1 -1 = -6$.\nThe first-neighbor correlation is $\\Pi_{1,\\mathcal{A}} = \\frac{1}{M_1}\\sum \\sigma_i\\sigma_{i+1} = \\frac{1}{6}(-6) = -1$.\n\nFor second-nearest neighbors ($\\delta=2$):\nThe sum of products is $\\sum_{i=1}^{6} \\sigma_{i}\\sigma_{i+2} = \\sigma_1\\sigma_3 + \\sigma_2\\sigma_4 + \\sigma_3\\sigma_5 + \\sigma_4\\sigma_6 + \\sigma_5\\sigma_1 + \\sigma_6\\sigma_2$.\nSubstituting the values: $(+1)(+1) + (-1)(-1) + (+1)(+1) + (-1)(-1) + (+1)(+1) + (-1)(-1) = 1 + 1 + 1 + 1 + 1 + 1 = 6$.\nThe second-neighbor correlation is $\\Pi_{2,\\mathcal{A}} = \\frac{1}{M_2}\\sum \\sigma_i\\sigma_{i+2} = \\frac{1}{6}(6) = 1$.\n\n**Configuration $\\mathcal{B}$: $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4}, \\sigma_{5}, \\sigma_{6}) = (+1, +1, +1, -1, -1, -1)$**\nFor first-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+1} = (+1)(+1) + (+1)(+1) + (+1)(-1) + (-1)(-1) + (-1)(-1) + (-1)(+1) = 1 + 1 - 1 + 1 + 1 - 1 = 2$.\n$\\Pi_{1,\\mathcal{B}} = \\frac{1}{6}(2) = \\frac{1}{3}$.\n\nFor second-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+2} = (+1)(+1) + (+1)(-1) + (+1)(-1) + (-1)(-1) + (-1)(+1) + (-1)(+1) = 1 - 1 - 1 + 1 - 1 - 1 = -2$.\n$\\Pi_{2,\\mathcal{B}} = \\frac{1}{6}(-2) = -\\frac{1}{3}$.\n\n**Configuration $\\mathcal{C}$: $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4}, \\sigma_{5}, \\sigma_{6}) = (+1, -1, -1, +1, +1, -1)$**\nFor first-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+1} = (+1)(-1) + (-1)(-1) + (-1)(+1) + (+1)(+1) + (+1)(-1) + (-1)(+1) = -1 + 1 - 1 + 1 - 1 - 1 = -2$.\n$\\Pi_{1,\\mathcal{C}} = \\frac{1}{6}(-2) = -\\frac{1}{3}$.\n\nFor second-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+2} = (+1)(-1) + (-1)(+1) + (-1)(+1) + (+1)(-1) + (+1)(+1) + (-1)(-1) = -1 - 1 - 1 - 1 + 1 + 1 = -2$.\n$\\Pi_{2,\\mathcal{C}} = \\frac{1}{6}(-2) = -\\frac{1}{3}$.\n\n**2. Assembling and Solving the Linear System**\n\nWe now use the energy expression $E = J_{1}\\Pi_{1} + J_{2}\\Pi_{2}$ for each configuration along with their given energies: $E_{\\mathcal{A}}=1$ eV, $E_{\\mathcal{B}}=-\\frac{1}{3}$ eV, and $E_{\\mathcal{C}}=-1$ eV.\n\nFor $\\mathcal{A}$: $1 = J_{1}(-1) + J_{2}(1) \\implies -J_{1} + J_{2} = 1$. (Eq. 1)\nFor $\\mathcal{B}$: $-\\frac{1}{3} = J_{1}(\\frac{1}{3}) + J_{2}(-\\frac{1}{3}) \\implies J_{1} - J_{2} = -1$. (Eq. 2)\nFor $\\mathcal{C}$: $-1 = J_{1}(-\\frac{1}{3}) + J_{2}(-\\frac{1}{3}) \\implies J_{1} + J_{2} = 3$. (Eq. 3)\n\nThe resulting system of linear equations is:\n$$\n\\begin{cases}\n-J_{1} + J_{2} = 1 \\\\\nJ_{1} - J_{2} = -1 \\\\\nJ_{1} + J_{2} = 3\n\\end{cases}\n$$\nEquation (2) is mathematically identical to Equation (1), since multiplying Eq. (2) by $-1$ yields Eq. (1). This confirms that the data for configurations $\\mathcal{A}$ and $\\mathcal{B}$ are consistent and co-linear in the correlation space, but it means we must use the independent information from configuration $\\mathcal{C}$ to find a unique solution. We solve the system formed by Eq. (1) and Eq. (3):\n$$\n\\begin{cases}\n-J_{1} + J_{2} = 1 \\\\\nJ_{1} + J_{2} = 3\n\\end{cases}\n$$\nAdding the two equations together:\n$(-J_{1} + J_{2}) + (J_{1} + J_{2}) = 1 + 3$\n$2J_{2} = 4$\n$J_{2} = 2$.\n\nSubstituting the value of $J_{2}$ into Eq. (3):\n$J_{1} + 2 = 3$\n$J_{1} = 1$.\n\nThe solution is $J_{1} = 1$ eV and $J_{2} = 2$ eV. We can verify this solution using Eq. (2) (from configuration $\\mathcal{B}$): $(1) - (2) = -1$, which is correct. The values are consistent with all three data points.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A fitted cluster expansion model is only reliable within the configurational space spanned by its training structures. Predictions for configurations outside this domain represent risky extrapolations. This practice  introduces leverage, a powerful statistical diagnostic that quantifies how much a new configuration deviates from the training set, enabling you to assess the model's predictive reliability.",
            "id": "3437939",
            "problem": "Consider a linear Cluster Expansion (CE) for alloy configurational thermodynamics, where the configurational energy is modeled as a linear functional of cluster correlation functions. Let the training design matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$, where each row corresponds to a configurational structure and each column corresponds to a basis correlation function value (including an intercept column of ones). The CE fitting via linear least squares may be regularized by Tikhonov regularization (ridge), with parameter $\\lambda \\ge 0$. Define the regularized normal matrix $A = X^{\\top} X + \\lambda I_p$, where $I_p$ is the $p \\times p$ identity matrix. The projection (hat) matrix for training data generalizes to $H_{\\lambda} = X A^{-1} X^{\\top}$, and the leverage for a prospective new structure with correlation vector $x \\in \\mathbb{R}^{p}$ is given by the scalar\n$$\n\\ell(x) = x^{\\top} A^{-1} x.\n$$\nIn practice, high $\\ell(x)$ indicates extrapolation risk. A commonly used decision rule compares $\\ell(x)$ to a scaled average leverage threshold,\n$$\nh_{\\mathrm{thr}} = \\alpha \\frac{p}{n},\n$$\nwith user-specified $\\alpha  0$. If any prospective structure’s leverage exceeds $h_{\\mathrm{thr}}$, one should deem extrapolation risk high and conclude that additional training data are required in that region of configuration space.\n\nTask: For each test case below, compute $\\ell(x)$ for each provided prospective structure $x$ using the corresponding $X$, $\\alpha$, and $\\lambda$, and output a boolean decision per test case defined as “true” if any $\\ell(x)  h_{\\mathrm{thr}}$ and “false” otherwise.\n\nFundamental bases you may assume:\n- The CE energy model is linear in the correlation basis and leads to linear least squares with design matrix $X$.\n- For ridge-regularized least squares, the regularized normal matrix is $A = X^{\\top} X + \\lambda I_p$.\n- The leverage of a point $x$ in linear models equals $x^{\\top} A^{-1} x$ for ridge with parameter $\\lambda \\ge 0$, which reduces to the classical ordinary least squares case when $\\lambda = 0$.\n\nNo physical units are involved. Angles do not appear. All numerical outputs must be computed as real numbers internally; the final decision per test case must be boolean.\n\nTest suite:\n- Test case $1$:\n  - $n = 10$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.0$.\n  - $X$ has rows $(x_i^{\\top})$ defined with an intercept column of ones and four correlation columns:\n    - Row $1$: $(\\;1.0,\\; 0.10,\\; 0.05,\\; 0.02,\\; 0.00\\;)$\n    - Row $2$: $(\\;1.0,\\; 0.20,\\; 0.10,\\; 0.04,\\; 0.00\\;)$\n    - Row $3$: $(\\;1.0,\\; -0.10,\\; -0.05,\\; -0.01,\\; 0.00\\;)$\n    - Row $4$: $(\\;1.0,\\; 0.00,\\; 0.00,\\; 0.00,\\; 0.00\\;)$\n    - Row $5$: $(\\;1.0,\\; 0.15,\\; 0.07,\\; 0.03,\\; 0.02\\;)$\n    - Row $6$: $(\\;1.0,\\; -0.05,\\; -0.02,\\; -0.01,\\; -0.02\\;)$\n    - Row $7$: $(\\;1.0,\\; 0.05,\\; 0.03,\\; 0.015,\\; 0.00\\;)$\n    - Row $8$: $(\\;1.0,\\; -0.20,\\; -0.10,\\; -0.05,\\; 0.00\\;)$\n    - Row $9$: $(\\;1.0,\\; 0.10,\\; 0.06,\\; 0.025,\\; 0.01\\;)$\n    - Row $10$: $(\\;1.0,\\; -0.10,\\; -0.04,\\; -0.02,\\; -0.01\\;)$\n  - Prospective structures to evaluate:\n    - $(\\;1.0,\\; 0.12,\\; 0.06,\\; 0.02,\\; 0.00\\;)$\n    - $(\\;1.0,\\; 0.00,\\; 0.00,\\; 0.00,\\; 1.00\\;)$\n    - $(\\;1.0,\\; -0.30,\\; -0.15,\\; -0.06,\\; -0.02\\;)$\n  - Decision rule: return “true” if any $\\ell(x)  h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\n- Test case $2$:\n  - $n = 16$, $p = 6$, $\\alpha = 2.0$, $\\lambda = 0.0$.\n  - $X$ rows are $(1.0, c_2, c_3, c_4, c_5, c_6)$, where the tuples $(c_2,\\ldots,c_6)$ are:\n    - $(-0.8,\\; -0.8,\\; -0.8,\\; -0.8,\\; -0.8)$\n    - $(-0.8,\\; -0.4,\\; -0.4,\\; -0.4,\\; -0.4)$\n    - $(-0.8,\\; 0.4,\\; 0.0,\\; -0.4,\\; 0.4)$\n    - $(-0.8,\\; 0.8,\\; 0.8,\\; 0.0,\\; -0.4)$\n    - $(-0.4,\\; -0.8,\\; 0.4,\\; 0.8,\\; 0.0)$\n    - $(-0.4,\\; -0.4,\\; -0.8,\\; 0.4,\\; 0.8)$\n    - $(-0.4,\\; 0.4,\\; -0.4,\\; -0.8,\\; 0.4)$\n    - $(-0.4,\\; 0.8,\\; 0.8,\\; -0.4,\\; -0.8)$\n    - $(0.4,\\; -0.8,\\; 0.8,\\; 0.4,\\; -0.4)$\n    - $(0.4,\\; -0.4,\\; 0.4,\\; -0.8,\\; 0.8)$\n    - $(0.4,\\; 0.4,\\; -0.8,\\; 0.8,\\; 0.4)$\n    - $(0.4,\\; 0.8,\\; -0.4,\\; 0.4,\\; -0.8)$\n    - $(0.8,\\; -0.8,\\; -0.4,\\; 0.8,\\; 0.4)$\n    - $(0.8,\\; -0.4,\\; 0.8,\\; -0.4,\\; -0.8)$\n    - $(0.8,\\; 0.4,\\; 0.4,\\; -0.8,\\; 0.0)$\n    - $(0.8,\\; 0.8,\\; -0.8,\\; 0.4,\\; -0.4)$\n  - Prospective structures to evaluate:\n    - $(\\;1.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0\\;)$\n    - $(\\;1.0,\\; 0.9,\\; 0.9,\\; 0.9,\\; 0.9,\\; 0.9\\;)$\n  - Decision rule: return “true” if any $\\ell(x)  h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\n- Test case $3$:\n  - $n = 8$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.05$.\n  - $X$ rows are $(1.0, c_2, c_3, c_4, c_5)$ with nearly collinear features:\n    - $(-0.5,\\; -1.0,\\; 0.5,\\; 0.01)$\n    - $(-0.4,\\; -0.8,\\; 0.4,\\; 0.00)$\n    - $(-0.3,\\; -0.6,\\; 0.3,\\; 0.00)$\n    - $(-0.2,\\; -0.4,\\; 0.2,\\; -0.01)$\n    - $(0.2,\\; 0.4,\\; -0.2,\\; 0.01)$\n    - $(0.3,\\; 0.6,\\; -0.3,\\; 0.00)$\n    - $(0.4,\\; 0.8,\\; -0.4,\\; 0.00)$\n    - $(0.5,\\; 1.0,\\; -0.5,\\; -0.01)$\n  - Prospective structures to evaluate:\n    - $(\\;1.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 1.0\\;)$\n    - $(\\;1.0,\\; 0.25,\\; 0.5,\\; -0.25,\\; 0.0\\;)$\n  - Decision rule: return “true” if any $\\ell(x)  h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\n- Test case $4$:\n  - $n = 16$, $p = 6$, $\\alpha = 0.5$, $\\lambda = 0.0$.\n  - Use the same $X$ as in Test case $2$.\n  - Prospective structures to evaluate:\n    - Exactly equal to the first training row in Test case $2$: $(\\;1.0,\\; -0.8,\\; -0.8,\\; -0.8,\\; -0.8,\\; -0.8\\;)$\n    - $(\\;1.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0\\;)$\n  - Decision rule: return “true” if any $\\ell(x)  h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\nYour program must:\n- Implement the computation of $A^{-1}$ for each test case, evaluate $\\ell(x)$ for each listed prospective structure, compare to $h_{\\mathrm{thr}}$, and return a boolean per test case indicating whether additional training data are required.\n- Use no external inputs; all data are embedded.\n- Produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, for the four test cases in order, for example: “[true,false,true,true]” but using Python boolean literals and exact comma placement, like “[True,False,True,True]”.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It is based on standard principles of linear modeling and regression diagnostics within the context of computational materials science. All necessary parameters and data are provided for each test case, and there are no internal contradictions. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe core task is to determine, for several test cases, whether a prospective new structure represents an extrapolation with respect to a given training set. This determination is based on the concept of leverage in a regularized linear regression model.\n\nThe model for the configurational energy in a Cluster Expansion (CE) is linear, which allows the use of linear least squares. The training data are represented by a design matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of training structures and $p$ is the number of basis functions (correlation functions, including an intercept).\n\nThe regression is stabilized using Tikhonov regularization (ridge regression) with a parameter $\\lambda \\ge 0$. This leads to the regularized normal matrix $A$, a $p \\times p$ matrix defined as:\n$$\nA = X^{\\top} X + \\lambda I_p\n$$\nwhere $X^{\\top}$ is the transpose of $X$ and $I_p$ is the $p \\times p$ identity matrix. The matrix $A$ is guaranteed to be invertible for $\\lambda  0$. If $\\lambda=0$, $A$ is invertible if and only if the columns of $X$ are linearly independent (i.e., $X$ has full column rank).\n\nThe leverage of a prospective new structure, characterized by its correlation vector $x \\in \\mathbb{R}^{p}$, is a scalar quantity $\\ell(x)$ that measures the influence of this point on a potential model fit. It is given by:\n$$\n\\ell(x) = x^{\\top} A^{-1} x\n$$\nA high leverage value indicates that the point $x$ is far from the \"center\" of the training data in the space of correlation functions, suggesting that a prediction at this point would be an extrapolation.\n\nA decision rule is established to flag such extrapolations. The leverage $\\ell(x)$ is compared to a threshold $h_{\\mathrm{thr}}$, which is defined as a multiple of the average leverage of the training points:\n$$\nh_{\\mathrm{thr}} = \\alpha \\frac{p}{n}\n$$\nHere, $\\alpha  0$ is a user-specified sensitivity parameter. The task requires determining if any of the prospective structures in a test case have a leverage exceeding this threshold. If for any prospective structure $x$, $\\ell(x)  h_{\\mathrm{thr}}$, the result for that test case is `true`; otherwise, it is `false`.\n\nThe procedure for each test case is as follows:\n1.  Identify the parameters $n$, $p$, $\\alpha$, and $\\lambda$.\n2.  Construct the training matrix $X$ and the set of prospective vectors $\\{x_i\\}$.\n3.  Calculate the threshold $h_{\\mathrm{thr}} = \\alpha \\frac{p}{n}$.\n4.  Compute the matrix $A = X^{\\top} X + \\lambda I_p$.\n5.  Compute the inverse matrix $A^{-1}$.\n6.  For each prospective vector $x_i$, calculate its leverage $\\ell(x_i) = x_i^{\\top} A^{-1} x_i$.\n7.  If any calculated leverage $\\ell(x_i)$ is greater than $h_{\\mathrm{thr}}$, the test case evaluates to `true`. If all $\\ell(x_i) \\le h_{\\mathrm{thr}}$, it evaluates to `false`.\n\nWe now apply this procedure to each test case.\n\n**Test Case 1**\n- Parameters: $n = 10$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.0$.\n- Threshold: $h_{\\mathrm{thr}} = 1.0 \\times \\frac{5}{10} = 0.5$.\n- Matrix $A = X^{\\top} X$.\n- Prospective structures $x_1, x_2, x_3$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, 0.12, 0.06, 0.02, 0.00)^{\\top}$: $\\ell(x_1) \\approx 0.491$. This point is an interpolation and its leverage is below the threshold.\n  - For $x_2 = (1.0, 0.00, 0.00, 0.00, 1.00)^{\\top}$: $\\ell(x_2) \\approx 2503.74$. This point is a significant extrapolation, as the last feature's value ($1.0$) is far outside its range in the training data (maximum of $0.02$). Its leverage is vastly greater than $h_{\\mathrm{thr}}$.\n  - For $x_3 = (1.0, -0.30, -0.15, -0.06, -0.02)^{\\top}$: $\\ell(x_3) \\approx 4.391$. This is also an extrapolation, with feature values outside the training range.\nSince $\\ell(x_2)  0.5$, the condition is met. The decision for this case is **true**.\n\n**Test Case 2**\n- Parameters: $n = 16$, $p = 6$, $\\alpha = 2.0$, $\\lambda = 0.0$.\n- Threshold: $h_{\\mathrm{thr}} = 2.0 \\times \\frac{6}{16} = 0.75$.\n- Matrix $A = X^{\\top} X$.\n- Prospective structures $x_1, x_2$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, 0.0, 0.0, 0.0, 0.0, 0.0)^{\\top}$: $\\ell(x_1) = 0.0625$. This point is the center of the design space and has the minimum possible leverage. $0.0625 \\le 0.75$.\n  - For $x_2 = (1.0, 0.9, 0.9, 0.9, 0.9, 0.9)^{\\top}$: $\\ell(x_2) \\approx 2.766$. The features in the training data lie in the range $[-0.8, 0.8]$, so this point is an extrapolation. Its leverage exceeds the threshold.\nSince $\\ell(x_2)  0.75$, the decision for this case is **true**.\n\n**Test Case 3**\n- Parameters: $n = 8$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.05$.\n- Threshold: $h_{\\mathrm{thr}} = 1.0 \\times \\frac{5}{8} = 0.625$.\n- The training matrix $X$ has nearly collinear features. The regularization term $\\lambda I_p$ is crucial for ensuring the numerical stability of the inversion of $A = X^{\\top} X + \\lambda I_p$.\n- Prospective structures $x_1, x_2$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, 0.0, 0.0, 0.0, 1.0)^{\\top}$: $\\ell(x_1) \\approx 20.01$. The last feature of this point is $1.0$, while it is close to $0.0$ for all training points. This represents an extrapolation in a direction poorly represented in the training data, leading to a high leverage value. $20.01  0.625$.\n  - For $x_2 = (1.0, 0.25, 0.5, -0.25, 0.0)^{\\top}$: $\\ell(x_2) \\approx 0.355$. This point lies on the manifold defined by the collinear training points and is an interpolation. Its leverage is low.\nSince $\\ell(x_1)  0.625$, the decision for this case is **true**.\n\n**Test Case 4**\n- Parameters: $n = 16$, $p = 6$, $\\alpha = 0.5$, $\\lambda = 0.0$.\n- Threshold: $h_{\\mathrm{thr}} = 0.5 \\times \\frac{6}{16} = 0.1875$. This is a strict threshold, being half the average leverage ($p/n = 0.375$).\n- The training matrix $X$ is the same as in Test Case $2$.\n- Prospective structures $x_1, x_2$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, -0.8, -0.8, -0.8, -0.8, -0.8)^{\\top}$: This is identical to the first row of the training matrix $X$. The leverage of a training point is the corresponding diagonal element of the hat matrix $H = X(X^{\\top}X)^{-1}X^{\\top}$. The calculation yields $\\ell(x_1) \\approx 0.590$. This value represents the leverage of a \"corner\" point of the experimental design. $0.590  0.1875$.\n  - For $x_2 = (1.0, 0.0, 0.0, 0.0, 0.0, 0.0)^{\\top}$: As in Test Case $2$, $\\ell(x_2) = 0.0625$. This is below the threshold.\nSince $\\ell(x_1)  0.1875$, the decision for this case is **true**.\n\nIn summary, at least one prospective structure in each test case exhibits a leverage greater than the respective threshold, leading to a decision of 'true' for all four cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the leverage calculation problem for all test cases.\n    \"\"\"\n    \n    # Test cases data defined as a list of dictionaries.\n    test_cases_data = [\n        # Test case 1\n        {\n            \"n\": 10, \"p\": 5, \"alpha\": 1.0, \"lambda_val\": 0.0,\n            \"X\": np.array([\n                [1.0,  0.10,  0.05,  0.020,  0.00],\n                [1.0,  0.20,  0.10,  0.040,  0.00],\n                [1.0, -0.10, -0.05, -0.010,  0.00],\n                [1.0,  0.00,  0.00,  0.000,  0.00],\n                [1.0,  0.15,  0.07,  0.030,  0.02],\n                [1.0, -0.05, -0.02, -0.010, -0.02],\n                [1.0,  0.05,  0.03,  0.015,  0.00],\n                [1.0, -0.20, -0.10, -0.050,  0.00],\n                [1.0,  0.10,  0.06,  0.025,  0.01],\n                [1.0, -0.10, -0.04, -0.020, -0.01]\n            ]),\n            \"prospective_x\": [\n                np.array([1.0,  0.12,  0.06,  0.02,  0.00]),\n                np.array([1.0,  0.00,  0.00,  0.00,  1.00]),\n                np.array([1.0, -0.30, -0.15, -0.06, -0.02])\n            ]\n        },\n        # Test case 2\n        {\n            \"n\": 16, \"p\": 6, \"alpha\": 2.0, \"lambda_val\": 0.0,\n            \"X\": np.hstack([np.ones((16, 1)), np.array([\n                [-0.8, -0.8, -0.8, -0.8, -0.8], [-0.8, -0.4, -0.4, -0.4, -0.4],\n                [-0.8,  0.4,  0.0, -0.4,  0.4], [-0.8,  0.8,  0.8,  0.0, -0.4],\n                [-0.4, -0.8,  0.4,  0.8,  0.0], [-0.4, -0.4, -0.8,  0.4,  0.8],\n                [-0.4,  0.4, -0.4, -0.8,  0.4], [-0.4,  0.8,  0.8, -0.4, -0.8],\n                [ 0.4, -0.8,  0.8,  0.4, -0.4], [ 0.4, -0.4,  0.4, -0.8,  0.8],\n                [ 0.4,  0.4, -0.8,  0.8,  0.4], [ 0.4,  0.8, -0.4,  0.4, -0.8],\n                [ 0.8, -0.8, -0.4,  0.8,  0.4], [ 0.8, -0.4,  0.8, -0.4, -0.8],\n                [ 0.8,  0.4,  0.4, -0.8,  0.0], [ 0.8,  0.8, -0.8,  0.4, -0.4]\n            ])]),\n            \"prospective_x\": [\n                np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n                np.array([1.0, 0.9, 0.9, 0.9, 0.9, 0.9])\n            ]\n        },\n        # Test case 3\n        {\n            \"n\": 8, \"p\": 5, \"alpha\": 1.0, \"lambda_val\": 0.05,\n            \"X\": np.hstack([np.ones((8, 1)), np.array([\n                [-0.5, -1.0,  0.5,  0.01], [-0.4, -0.8,  0.4,  0.00],\n                [-0.3, -0.6,  0.3,  0.00], [-0.2, -0.4,  0.2, -0.01],\n                [ 0.2,  0.4, -0.2,  0.01], [ 0.3,  0.6, -0.3,  0.00],\n                [ 0.4,  0.8, -0.4,  0.00], [ 0.5,  1.0, -0.5, -0.01]\n            ])]),\n            \"prospective_x\": [\n                np.array([1.0,  0.00,  0.0, -0.00,  1.0]),\n                np.array([1.0,  0.25,  0.5, -0.25,  0.0])\n            ]\n        },\n        # Test case 4\n        {\n            \"n\": 16, \"p\": 6, \"alpha\": 0.5, \"lambda_val\": 0.0,\n            \"X\": np.hstack([np.ones((16, 1)), np.array([\n                [-0.8, -0.8, -0.8, -0.8, -0.8], [-0.8, -0.4, -0.4, -0.4, -0.4],\n                [-0.8,  0.4,  0.0, -0.4,  0.4], [-0.8,  0.8,  0.8,  0.0, -0.4],\n                [-0.4, -0.8,  0.4,  0.8,  0.0], [-0.4, -0.4, -0.8,  0.4,  0.8],\n                [-0.4,  0.4, -0.4, -0.8,  0.4], [-0.4,  0.8,  0.8, -0.4, -0.8],\n                [ 0.4, -0.8,  0.8,  0.4, -0.4], [ 0.4, -0.4,  0.4, -0.8,  0.8],\n                [ 0.4,  0.4, -0.8,  0.8,  0.4], [ 0.4,  0.8, -0.4,  0.4, -0.8],\n                [ 0.8, -0.8, -0.4,  0.8,  0.4], [ 0.8, -0.4,  0.8, -0.4, -0.8],\n                [ 0.8,  0.4,  0.4, -0.8,  0.0], [ 0.8,  0.8, -0.8,  0.4, -0.4]\n            ])]),\n            \"prospective_x\": [\n                np.array([1.0, -0.8, -0.8, -0.8, -0.8, -0.8]),\n                np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases_data:\n        n, p, alpha, lambda_val = case[\"n\"], case[\"p\"], case[\"alpha\"], case[\"lambda_val\"]\n        X, prospective_x_list = case[\"X\"], case[\"prospective_x\"]\n\n        # Calculate the leverage threshold\n        h_thr = alpha * p / n\n\n        # Form the regularized normal matrix A\n        A = X.T @ X + lambda_val * np.eye(p)\n\n        # Compute the inverse of A\n        A_inv = np.linalg.inv(A)\n\n        # Initialize the decision for this test case\n        decision = False\n\n        # Evaluate leverage for each prospective structure\n        for x in prospective_x_list:\n            leverage = x.T @ A_inv @ x\n            if leverage  h_thr:\n                decision = True\n                break  # Found one, no need to check others\n\n        results.append(decision)\n\n    # Format the final output as a string representing a list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A primary application of a fitted cluster expansion is the prediction of stable ordered phases, or ground states, which correspond to the global minimum of the energy model. This search is a computationally challenging optimization problem. This exercise  demonstrates how to transform this task into a mixed-integer linear program (MILP), a standard and powerful framework for finding the exact ground state configuration.",
            "id": "3437877",
            "problem": "Consider a binary alloy on a two-dimensional square Bravais lattice with periodic boundary conditions, modeled by a minimal Cluster Expansion (CE) that retains only pair interactions. Let the square supercell contain $L_x \\times L_y$ lattice sites, index sites by $i = 1, \\dots, N$ with $N = L_x L_y$, and represent the occupancy of species $A$ at site $i$ by a binary variable $x_i \\in \\{0,1\\}$. Define an Ising spin variable $s_i = 2 x_i - 1 \\in \\{-1, +1\\}$. The pair-only CE energy in dimensionless reduced units is given by\n$$\nE = \\sum_{\\langle i,j \\rangle_{1}} J_1 \\, s_i s_j + \\sum_{\\langle i,j \\rangle_{2}} J_2 \\, s_i s_j + h \\sum_{i=1}^{N} s_i,\n$$\nwhere $\\langle i,j \\rangle_{1}$ and $\\langle i,j \\rangle_{2}$ denote first-neighbor and second-neighbor (diagonal) pairs under periodic boundary conditions, $J_1$ and $J_2$ are pair Effective Cluster Interaction coefficients, and $h$ is a uniform on-site field that biases the occupancy. The physical unit is set so that energy is dimensionless. First neighbors are defined by offsets $(\\Delta x, \\Delta y) \\in \\{(1,0), (0,1), (-1,0), (0,-1)\\}$ and second neighbors (diagonal) by offsets $(\\Delta x, \\Delta y) \\in \\{(1,1), (1,-1), (-1,1), (-1,-1)\\}$, with all displacements taken modulo $(L_x, L_y)$ to enforce periodicity.\n\nStarting from the definitions above and using only general facts about the Ising representation of a binary alloy and the structure of pair-only CE on a periodic lattice, derive a Mixed Integer Linear Programming (MILP) formulation that exactly represents the ground-state search for this CE. Your formulation must:\n- Express the energy in terms of $x_i$ and auxiliary variables, using only linear terms and linear constraints.\n- Enforce $x_i \\in \\{0,1\\}$.\n- Accurately represent periodic first- and second-neighbor interactions on the given finite supercell.\n- Allow the optional imposition of a fixed composition constraint $\\sum_{i=1}^{N} x_i = f N$ for a prescribed fraction $f \\in [0,1]$ such that $f N$ is an integer.\n\nImplement a branch-and-bound solver that uses Linear Programming (LP) relaxations to solve the MILP to global optimality for the given small supercells. The LP relaxations must be solved by a standard LP solver. The branch-and-bound must:\n- Use the LP-relaxed solution as a lower bound.\n- Branch on fractional $x_i$ variables until integrality is reached.\n- Prune nodes whose LP lower bounds exceed the current best integral energy.\n\nYour program must build the lattice neighbor pairs under periodic boundary conditions, construct the MILP, and compute the ground-state energy for each test case below. Energies must be reported in dimensionless units, rounded to six decimal places.\n\nTest Suite:\n- Case 1 (happy path): $L_x = 2$, $L_y = 2$, $J_1 = -1.0$, $J_2 = 0.0$, $h = 0.0$, no composition constraint ($f$ is absent).\n- Case 2 (antiferromagnetic with fixed composition): $L_x = 2$, $L_y = 2$, $J_1 = +1.0$, $J_2 = 0.0$, $h = 0.0$, fixed composition with $f = 0.5$.\n- Case 3 (frustration edge case): $L_x = 2$, $L_y = 2$, $J_1 = +1.0$, $J_2 = -0.4$, $h = 0.0$, no composition constraint.\n- Case 4 (boundary composition): $L_x = 2$, $L_y = 2$, $J_1 = +1.0$, $J_2 = 0.0$, $h = 0.0$, fixed composition with $f = 0.0$.\n\nOutput specification:\n- Your program should produce a single line of output containing the four ground-state energies, in the order of the test cases above, as a comma-separated list enclosed in square brackets, for example, $[e_1,e_2,e_3,e_4]$, where each $e_k$ is a float rounded to six decimal places, expressed in dimensionless units.\n\nNo other output is permitted.",
            "solution": "The user wants to find the ground-state energy of a binary alloy on a two-dimensional square lattice using a Cluster Expansion (CE) model. The energy expression is equivalent to an Ising Hamiltonian with first and second neighbor interactions, subject to periodic boundary conditions. The task is to formulate this as a Mixed-Integer Linear Programming (MILP) problem and solve it using a custom branch-and-bound (BB) algorithm that leverages a standard Linear Programming (LP) solver.\n\n### Step 1: Derivation of the MILP Formulation\n\nThe energy of the system is given in terms of Ising spin variables $s_i \\in \\{-1, +1\\}$:\n$$\nE = \\sum_{\\langle i,j \\rangle_{1}} J_1 \\, s_i s_j + \\sum_{\\langle i,j \\rangle_{2}} J_2 \\, s_i s_j + h \\sum_{i=1}^{N} s_i\n$$\nThe problem requires a formulation in terms of binary occupancy variables $x_i \\in \\{0,1\\}$, which are related to the spin variables by $s_i = 2x_i - 1$.\n\nFirst, substitute $s_i = 2x_i - 1$ into the energy equation. The on-site term becomes:\n$$\nh \\sum_{i=1}^{N} s_i = h \\sum_{i=1}^{N} (2x_i - 1) = 2h \\sum_{i=1}^{N} x_i - hN\n$$\nThis term is linear in $x_i$, with a constant offset $-hN$.\n\nThe pair interaction term $s_i s_j$ is quadratic:\n$$\ns_i s_j = (2x_i - 1)(2x_j - 1) = 4x_i x_j - 2x_i - 2x_j + 1\n$$\nThis expression contains a quadratic term $x_i x_j$, which must be linearized for an MILP formulation. We introduce an auxiliary continuous variable $z_{ij}$ for each interacting pair $\\langle i,j \\rangle$ and define it to be equal to the product $z_{ij} = x_i x_j$. Since $x_i$ and $x_j$ are binary, their product $z_{ij}$ is also binary. This equality can be replaced by a set of linear inequalities that are exact for binary variables (known as McCormick linearization):\n1. $z_{ij} \\le x_i$\n2. $z_{ij} \\le x_j$\n3. $z_{ij} \\ge x_i + x_j - 1$\n\nWith this substitution, the energy contribution from a single pair interaction becomes linear in $x_i, x_j,$ and $z_{ij}$:\n$$\nJ_{\\langle i,j \\rangle} s_i s_j = J_{\\langle i,j \\rangle} (4z_{ij} - 2x_i - 2x_j + 1)\n$$\nSumming over all pairs and including the on-site term, the total energy is:\n$$\nE = \\sum_{\\langle i,j \\rangle} J_{\\langle i,j \\rangle} \\left(4z_{ij} - 2x_i - 2x_j + 1\\right) + 2h \\sum_i x_i - hN\n$$\nwhere $J_{\\langle i,j \\rangle}$ is either $J_1$ or $J_2$. To obtain the final linear objective function for the MILP, we rearrange the terms by variable:\n$$\nE = \\sum_{\\langle i,j \\rangle} (4 J_{\\langle i,j \\rangle}) z_{ij} + \\sum_i \\left( 2h - 2 \\sum_{j \\in \\text{N}(i)} J_{\\langle i,j \\rangle} \\right) x_i + \\left( \\sum_{\\langle i,j \\rangle} J_{\\langle i,j \\rangle} - hN \\right)\n$$\nHere, $\\text{N}(i)$ denotes the set of all neighbors of site $i$. The final term is a constant offset, $E_{\\text{const}}$, that can be added to the objective value found by the MILP solver.\n\nThe full MILP formulation is:\n**Minimize:**\n$$\nE_{\\text{MILP}} = \\sum_{\\langle i,j \\rangle} c_{ij} z_{ij} + \\sum_i d_i x_i\n$$\nwhere $c_{ij} = 4 J_{\\langle i,j \\rangle}$ and $d_i = 2h - 2 \\sum_{j \\in \\text{N}(i)} J_{\\langle i,j \\rangle}$. The ground state energy is $E_{GS} = E_{\\text{MILP}}^* + E_{\\text{const}}$.\n\n**Subject to:**\n- For each interacting pair $\\langle i,j \\rangle$:\n    - $z_{ij} - x_i \\le 0$\n    - $z_{ij} - x_j \\le 0$\n    - $x_i + x_j - z_{ij} \\le 1$\n- For each site $i$:\n    - $x_i \\in \\{0, 1\\}$ (enforced by the branch-and-bound process)\n- Optional composition constraint:\n    - $\\sum_{i=1}^N x_i = fN$ (for a given fraction $f$)\n\n### Step 2: Algorithmic Design for the Branch-and-Bound Solver\n\nThe MILP will be solved using a branch-and-bound (BB) algorithm.\n1.  **Lattice and MILP Construction**: First, a function constructs the list of first- and second-neighbor pairs for the given $L_x \\times L_y$ lattice with periodic boundary conditions. Then, it assembles the MILP matrices: the objective vector $c$, the inequality constraint matrix $A_{ub}$ and vector $b_{ub}$, the equality constraint matrix $A_{eq}$ and vector $b_{eq}$ (if composition is fixed), and the constant energy offset $E_{\\text{const}}$.\n\n2.  **Branch-and-Bound Core Logic**:\n    - **Node Representation**: A node in the BB tree represents an LP subproblem, defined by the original MILP relaxation plus additional constraints that fix certain $x_i$ variables to $0$ or $1$. These constraints are managed by adjusting the `bounds` of the variables.\n    - **Initialization**: The algorithm starts with a single node (the root) representing the LP relaxation of the original problem (all $x_i \\in [0, 1]$). An upper bound on the minimum energy is initialized to infinity. A stack is used to manage the nodes to be explored (depth-first search).\n    - **Iteration**: The algorithm proceeds as follows:\n        a. Pop a node (subproblem) from the stack.\n        b. Solve the corresponding LP relaxation using `scipy.optimize.linprog`.\n        c. **Pruning**:\n            i. If the LP is infeasible, prune this branch.\n            ii. If the LP's objective value (a lower bound for this branch) is greater than the current best global upper bound, prune this branch.\n        d. **Solution Analysis**:\n            i. If the LP solution for all $x_i$ variables is integral (within a small tolerance), a valid integer solution has been found. Its total energy is calculated and used to update the global upper bound if it is better. This branch is pruned as it is a leaf of the integer solution tree.\n            ii. If at least one $x_k$ is fractional, this is a **branching** point. Two new child nodes are created and pushed to the stack: one with the added constraint $x_k=0$ and another with $x_k=1$.\n\n3.  **Termination**: The algorithm terminates when the stack of nodes is empty. The final stored upper bound is the globally optimal ground-state energy.\n\nThis procedure guarantees finding the exact ground-state energy of the specified CE model.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    def _build_milp(Lx, Ly, J1, J2, h):\n        \"\"\"\n        Builds the MILP formulation for the given cluster expansion problem.\n        Returns the components needed for the LP solver and BB algorithm.\n        \"\"\"\n        N = Lx * Ly\n        \n        # 1. Generate neighbor lists and counts\n        sites_coords = {i: (i % Lx, i // Ly) for i in range(N)}\n        coords_sites = {(c, r): i for i, (c, r) in sites_coords.items()}\n\n        first_neighbor_offsets = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n        second_neighbor_offsets = [(1, 1), (1, -1), (-1, 1), (-1, -1)]\n\n        adj = [set() for _ in range(N)]\n        adj_type = {} # To store pair type (1 or 2)\n\n        for i in range(N):\n            c, r = sites_coords[i]\n            # First neighbors\n            for dc, dr in first_neighbor_offsets:\n                nc, nr = (c + dc) % Lx, (r + dr) % Ly\n                j = coords_sites[(nc, nr)]\n                if i != j:\n                    adj[i].add(j)\n                    pair = tuple(sorted((i, j)))\n                    if pair not in adj_type:\n                        adj_type[pair] = 1\n            # Second neighbors\n            for dc, dr in second_neighbor_offsets:\n                nc, nr = (c + dc) % Lx, (r + dr) % Ly\n                j = coords_sites[(nc, nr)]\n                if i != j:\n                    adj[i].add(j)\n                    pair = tuple(sorted((i, j)))\n                    # J1 dominates if a pair is both 1st and 2nd nbr\n                    if pair not in adj_type:\n                        adj_type[pair] = 2\n\n        all_pairs = sorted(list(adj_type.keys()))\n        pairs1 = [p for p in all_pairs if adj_type[p] == 1]\n        pairs2 = [p for p in all_pairs if adj_type[p] == 2]\n        \n        # 2. Construct MILP matrices\n        num_x_vars = N\n        num_z_vars = len(all_pairs)\n        num_total_vars = num_x_vars + num_z_vars\n        \n        pair_to_z_idx = {pair: i + num_x_vars for i, pair in enumerate(all_pairs)}\n\n        # Objective function vector `c`\n        c = np.zeros(num_total_vars)\n        for i in range(N):\n            c_i = 2 * h\n            for j in adj[i]:\n                pair = tuple(sorted((i, j)))\n                pair_type = adj_type[pair]\n                J = J1 if pair_type == 1 else J2\n                c_i -= 2 * J\n            c[i] = c_i\n        \n        for pair in all_pairs:\n            z_idx = pair_to_z_idx[pair]\n            pair_type = adj_type[pair]\n            J = J1 if pair_type == 1 else J2\n            c[z_idx] = 4 * J\n            \n        # Constant energy term\n        E_const = len(pairs1) * J1 + len(pairs2) * J2 - h * N\n        \n        # Inequality constraints A_ub * x = b_ub\n        num_ineq_constraints = 3 * num_z_vars\n        A_ub = np.zeros((num_ineq_constraints, num_total_vars))\n        b_ub = np.zeros(num_ineq_constraints)\n\n        row_idx = 0\n        for i, j in all_pairs:\n            z_idx = pair_to_z_idx[(i, j)]\n            # z_ij - x_i = 0\n            A_ub[row_idx, z_idx] = 1\n            A_ub[row_idx, i] = -1\n            row_idx += 1\n            # z_ij - x_j = 0\n            A_ub[row_idx, z_idx] = 1\n            A_ub[row_idx, j] = -1\n            row_idx += 1\n            # x_i + x_j - z_ij = 1\n            A_ub[row_idx, i] = 1\n            A_ub[row_idx, j] = 1\n            A_ub[row_idx, z_idx] = -1\n            b_ub[row_idx] = 1\n            row_idx += 1\n            \n        return c, A_ub, b_ub, E_const, N, num_total_vars\n\n    def _solve_milp_with_bb(c, A_ub, b_ub, A_eq, b_eq, E_const, N, num_total_vars):\n        \"\"\"\n        Solves the MILP using a branch-and-bound algorithm.\n        \"\"\"\n        initial_bounds = [(0, 1)] * num_total_vars\n        stack = [initial_bounds]\n        \n        upper_bound = float('inf')\n\n        while stack:\n            current_bounds = stack.pop()\n            \n            res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=current_bounds, method='highs-ds')\n            \n            if not res.success:\n                continue\n                \n            lp_min_val = res.fun\n            \n            if lp_min_val + E_const = upper_bound:\n                continue\n                \n            x_vars = res.x[:N]\n            \n            # Find a fractional variable to branch on\n            frac_idx = -1\n            for i in range(N):\n                if not np.isclose(x_vars[i], round(x_vars[i]), atol=1e-8):\n                    frac_idx = i\n                    break\n                    \n            if frac_idx == -1: # All integer solution\n                current_energy = lp_min_val + E_const\n                if current_energy  upper_bound:\n                    upper_bound = current_energy\n            else: # Branch\n                # Create two new subproblems by copying bounds and adding constraints\n                bounds_0 = list(current_bounds)\n                bounds_0[frac_idx] = (0, 0)\n                stack.append(bounds_0)\n\n                bounds_1 = list(current_bounds)\n                bounds_1[frac_idx] = (1, 1)\n                stack.append(bounds_1)\n\n        return upper_bound\n\n    def run_case(Lx, Ly, J1, J2, h, f=None):\n        \"\"\"\n        Sets up and solves one test case configuration.\n        \"\"\"\n        c, A_ub, b_ub, E_const, N, num_total_vars = _build_milp(Lx, Ly, J1, J2, h)\n        \n        A_eq = None\n        b_eq = None\n        \n        if f is not None:\n            target_sum = f * N\n            A_eq = np.zeros((1, num_total_vars))\n            A_eq[0, :N] = 1\n            b_eq = np.array([target_sum])\n\n        min_energy = _solve_milp_with_bb(c, A_ub, b_ub, A_eq, b_eq, E_const, N, num_total_vars)\n        \n        if min_energy == float('inf'):\n            return np.nan # Should not happen for these test cases\n            \n        return min_energy\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'Lx': 2, 'Ly': 2, 'J1': -1.0, 'J2': 0.0, 'h': 0.0, 'f': None},\n        {'Lx': 2, 'Ly': 2, 'J1': +1.0, 'J2': 0.0, 'h': 0.0, 'f': 0.5},\n        {'Lx': 2, 'Ly': 2, 'J1': +1.0, 'J2': -0.4, 'h': 0.0, 'f': None},\n        {'Lx': 2, 'Ly': 2, 'J1': +1.0, 'J2': 0.0, 'h': 0.0, 'f': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        energy = run_case(**case)\n        results.append(f\"{energy:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}