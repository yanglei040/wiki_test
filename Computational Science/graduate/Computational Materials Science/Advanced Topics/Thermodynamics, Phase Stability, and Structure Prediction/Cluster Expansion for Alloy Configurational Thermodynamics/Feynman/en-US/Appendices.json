{
    "hands_on_practices": [
        {
            "introduction": "The foundation of the cluster expansion method is fitting a model that links configurational geometry to energy. This exercise demystifies this process by reducing it to its essentials: solving a system of linear equations. By manually calculating pair correlation functions for a few simple, hypothetical alloy configurations and using their given energies, you will directly determine the Effective Cluster Interactions (ECIs), gaining a concrete understanding of how these crucial parameters are derived .",
            "id": "3437949",
            "problem": "Consider a binary alloy modeled on a one-dimensional periodic ring with $N=6$ lattice sites and Ising-like occupation variables $\\sigma_{i}\\in\\{+1,-1\\}$, where $\\sigma_{i}=+1$ denotes species $A$ and $\\sigma_{i}=-1$ denotes species $B$. The configurational energy per lattice site is represented by a truncated cluster expansion that retains only pair clusters up to second-nearest neighbors. For a given configuration, define the pair correlation $\\Pi_{\\alpha}$ for shell $\\alpha$ as\n$$\n\\Pi_{\\alpha} \\equiv \\frac{1}{M_{\\alpha}}\\sum_{\\langle i,j\\rangle_{\\alpha}} \\sigma_{i}\\sigma_{j},\n$$\nwhere $M_{\\alpha}$ is the number of distinct pairs in shell $\\alpha$, and the sum runs over all symmetry-equivalent pairs separated by the $\\alpha$-th neighbor distance. On the $N=6$ ring, each site has $z_1=2$ first-nearest neighbors and $z_2=2$ second-nearest neighbors, so $M_1=M_2=6$. Assume equiatomic composition and formation energies such that the empty-cluster and point contributions are constants that are absorbed into the reference energy (set to zero), so the energy per site $E$ depends linearly on the pair correlations with two unknown effective cluster interactions (ECIs) $J_1$ and $J_2$:\n$$\nE = J_{1}\\,\\Pi_{1} + J_{2}\\,\\Pi_{2}.\n$$\nConsider the following three distinct equiatomic configurations on the ring, specified by the sequence of $\\sigma_{i}$ along the ring:\n- Configuration $\\mathcal{A}$: $+1,-1,+1,-1,+1,-1$ (alternating $A$ and $B$),\n- Configuration $\\mathcal{B}$: $+1,+1,+1,-1,-1,-1$ (three $A$ followed by three $B$),\n- Configuration $\\mathcal{C}$: $+1,-1,-1,+1,+1,-1$.\n\nFor these three configurations, suppose the measured formation energies per site (from a hypothetical first-principles dataset) are\n$$\nE_{\\mathcal{A}}=1\\ \\text{eV},\\quad E_{\\mathcal{B}}=-\\frac{1}{3}\\ \\text{eV},\\quad E_{\\mathcal{C}}=-1\\ \\text{eV}.\n$$\nStarting from the definitions above and without invoking any unintroduced shortcut formulas, compute the pair correlations $\\Pi_{1}$ and $\\Pi_{2}$ for each configuration explicitly, assemble the linear system relating $\\{E_{\\mathcal{A}},E_{\\mathcal{B}},E_{\\mathcal{C}}\\}$ to $\\{J_1,J_2\\}$, and solve for the two pair effective cluster interactions $J_1$ and $J_2$ exactly. Express the final values of $J_1$ and $J_2$ in electronvolts. No rounding is required; provide exact values.",
            "solution": "The problem requires the calculation of two effective cluster interactions (ECIs), $J_{1}$ and $J_{2}$, for a binary alloy on a one-dimensional periodic ring of $N=6$ lattice sites. The configurational energy per site, $E$, is modeled by a truncated cluster expansion:\n$$\nE = J_{1}\\,\\Pi_{1} + J_{2}\\,\\Pi_{2}\n$$\nwhere $\\Pi_{\\alpha}$ is the pair correlation for the $\\alpha$-th neighbor shell. The occupation of site $i$ is described by a variable $\\sigma_{i} \\in \\{+1, -1\\}$. The pair correlation is defined as:\n$$\n\\Pi_{\\alpha} \\equiv \\frac{1}{M_{\\alpha}}\\sum_{\\langle i,j\\rangle_{\\alpha}} \\sigma_{i}\\sigma_{j}\n$$\nwhere the sum is over the $M_{\\alpha}$ distinct pairs of $\\alpha$-th neighbors. For a one-dimensional periodic ring of $N=6$ sites, the first-nearest neighbors of site $i$ are sites $i+1$ and $i-1$, and the second-nearest neighbors are $i+2$ and $i-2$ (indices are taken modulo $6$). The total number of distinct first-neighbor pairs is $M_1=6$, and the total number of distinct second-neighbor pairs is $M_2=6$. The summation over all distinct pairs $\\langle i,j\\rangle_{\\alpha}$ can be computed for a periodic ring by summing $\\sigma_i \\sigma_{i+\\delta}$ over all sites $i$ from $1$ to $N$, where $\\delta$ is the neighbor distance ($\\delta=1$ for first neighbors, $\\delta=2$ for second neighbors) and periodic boundary conditions ($\\sigma_{N+k} = \\sigma_k$) are applied.\n\nThe procedure is to first compute the pair correlations $\\Pi_{1}$ and $\\Pi_{2}$ for each of the three given configurations, then use the provided energies $\\{E_{\\mathcal{A}}, E_{\\mathcal{B}}, E_{\\mathcal{C}}\\}$ to establish a system of linear equations for $J_{1}$ and $J_{2}$, and finally solve this system.\n\n**1. Calculation of Pair Correlations**\n\n**Configuration $\\mathcal{A}$: $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4}, \\sigma_{5}, \\sigma_{6}) = (+1, -1, +1, -1, +1, -1)$**\nFor first-nearest neighbors ($\\delta=1$):\nThe sum of products is $\\sum_{i=1}^{6} \\sigma_{i}\\sigma_{i+1} = \\sigma_1\\sigma_2 + \\sigma_2\\sigma_3 + \\sigma_3\\sigma_4 + \\sigma_4\\sigma_5 + \\sigma_5\\sigma_6 + \\sigma_6\\sigma_1$.\nSubstituting the values: $(+1)(-1) + (-1)(+1) + (+1)(-1) + (-1)(+1) + (+1)(-1) + (-1)(+1) = -1 -1 -1 -1 -1 -1 = -6$.\nThe first-neighbor correlation is $\\Pi_{1,\\mathcal{A}} = \\frac{1}{M_1}\\sum \\sigma_i\\sigma_{i+1} = \\frac{1}{6}(-6) = -1$.\n\nFor second-nearest neighbors ($\\delta=2$):\nThe sum of products is $\\sum_{i=1}^{6} \\sigma_{i}\\sigma_{i+2} = \\sigma_1\\sigma_3 + \\sigma_2\\sigma_4 + \\sigma_3\\sigma_5 + \\sigma_4\\sigma_6 + \\sigma_5\\sigma_1 + \\sigma_6\\sigma_2$.\nSubstituting the values: $(+1)(+1) + (-1)(-1) + (+1)(+1) + (-1)(-1) + (+1)(+1) + (-1)(-1) = 1 + 1 + 1 + 1 + 1 + 1 = 6$.\nThe second-neighbor correlation is $\\Pi_{2,\\mathcal{A}} = \\frac{1}{M_2}\\sum \\sigma_i\\sigma_{i+2} = \\frac{1}{6}(6) = 1$.\n\n**Configuration $\\mathcal{B}$: $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4}, \\sigma_{5}, \\sigma_{6}) = (+1, +1, +1, -1, -1, -1)$**\nFor first-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+1} = (+1)(+1) + (+1)(+1) + (+1)(-1) + (-1)(-1) + (-1)(-1) + (-1)(+1) = 1 + 1 - 1 + 1 + 1 - 1 = 2$.\n$\\Pi_{1,\\mathcal{B}} = \\frac{1}{6}(2) = \\frac{1}{3}$.\n\nFor second-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+2} = (+1)(+1) + (+1)(-1) + (+1)(-1) + (-1)(-1) + (-1)(+1) + (-1)(+1) = 1 - 1 - 1 + 1 - 1 - 1 = -2$.\n$\\Pi_{2,\\mathcal{B}} = \\frac{1}{6}(-2) = -\\frac{1}{3}$.\n\n**Configuration $\\mathcal{C}$: $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}, \\sigma_{4}, \\sigma_{5}, \\sigma_{6}) = (+1, -1, -1, +1, +1, -1)$**\nFor first-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+1} = (+1)(-1) + (-1)(-1) + (-1)(+1) + (+1)(+1) + (+1)(-1) + (-1)(+1) = -1 + 1 - 1 + 1 - 1 - 1 = -2$.\n$\\Pi_{1,\\mathcal{C}} = \\frac{1}{6}(-2) = -\\frac{1}{3}$.\n\nFor second-nearest neighbors:\n$\\sum \\sigma_i\\sigma_{i+2} = (+1)(-1) + (-1)(+1) + (-1)(+1) + (+1)(-1) + (+1)(+1) + (-1)(-1) = -1 - 1 - 1 - 1 + 1 + 1 = -2$.\n$\\Pi_{2,\\mathcal{C}} = \\frac{1}{6}(-2) = -\\frac{1}{3}$.\n\n**2. Assembling and Solving the Linear System**\n\nWe now use the energy expression $E = J_{1}\\Pi_{1} + J_{2}\\Pi_{2}$ for each configuration along with their given energies: $E_{\\mathcal{A}}=1$ eV, $E_{\\mathcal{B}}=-\\frac{1}{3}$ eV, and $E_{\\mathcal{C}}=-1$ eV.\n\nFor $\\mathcal{A}$: $1 = J_{1}(-1) + J_{2}(1) \\implies -J_{1} + J_{2} = 1$. (Eq. 1)\nFor $\\mathcal{B}$: $-\\frac{1}{3} = J_{1}(\\frac{1}{3}) + J_{2}(-\\frac{1}{3}) \\implies J_{1} - J_{2} = -1$. (Eq. 2)\nFor $\\mathcal{C}$: $-1 = J_{1}(-\\frac{1}{3}) + J_{2}(-\\frac{1}{3}) \\implies J_{1} + J_{2} = 3$. (Eq. 3)\n\nThe resulting system of linear equations is:\n$$\n\\begin{cases}\n-J_{1} + J_{2} = 1 \\\\\nJ_{1} - J_{2} = -1 \\\\\nJ_{1} + J_{2} = 3\n\\end{cases}\n$$\nEquation (2) is mathematically identical to Equation (1), since multiplying Eq. (2) by $-1$ yields Eq. (1). This confirms that the data for configurations $\\mathcal{A}$ and $\\mathcal{B}$ are consistent and co-linear in the correlation space, but it means we must use the independent information from configuration $\\mathcal{C}$ to find a unique solution. We solve the system formed by Eq. (1) and Eq. (3):\n$$\n\\begin{cases}\n-J_{1} + J_{2} = 1 \\\\\nJ_{1} + J_{2} = 3\n\\end{cases}\n$$\nAdding the two equations together:\n$(-J_{1} + J_{2}) + (J_{1} + J_{2}) = 1 + 3$\n$2J_{2} = 4$\n$J_{2} = 2$.\n\nSubstituting the value of $J_{2}$ into Eq. (3):\n$J_{1} + 2 = 3$\n$J_{1} = 1$.\n\nThe solution is $J_{1} = 1$ eV and $J_{2} = 2$ eV. We can verify this solution using Eq. (2) (from configuration $\\mathcal{B}$): $(1) - (2) = -1$, which is correct. The values are consistent with all three data points.",
            "answer": "$$\n\\boxed{J_1 = 1 \\, \\text{eV}, \\quad J_2 = 2 \\, \\text{eV}}\n$$"
        },
        {
            "introduction": "A fitted model is only useful if it can accurately predict the energy of new, unseen configurations. This practice introduces K-fold cross-validation, a cornerstone of statistical learning, to rigorously assess the predictive power of your cluster expansion model. You will write code to implement this technique and use it to investigate a critical practical issue: how redundant data in a training set can affect the apparent accuracy of the model, providing insight into building robust and efficient training datasets .",
            "id": "3437890",
            "problem": "You are given a minimal linear Cluster Expansion (CE) setting for a binary alloy in which the configurational energy of each structure is modeled as a linear combination of precomputed cluster correlation functions. Specifically, for a dataset of structures indexed by $i \\in \\{0,1,\\dots,N-1\\}$, each structure has a correlation vector $\\boldsymbol{\\phi}_i \\in \\mathbb{R}^p$, collected as rows of a design matrix $X \\in \\mathbb{R}^{N \\times p}$, and an energy $y_i \\in \\mathbb{R}$ collected into a vector $\\mathbf{y} \\in \\mathbb{R}^N$. The model postulates\n$$\nE_i \\equiv y_i \\approx \\sum_{\\alpha=0}^{p-1} J_{\\alpha} \\, \\phi_{i,\\alpha} \\quad \\text{or in matrix form} \\quad \\mathbf{y} \\approx X \\mathbf{J},\n$$\nwhere $\\mathbf{J} \\in \\mathbb{R}^p$ are the unknown Effective Cluster Interactions (ECIs). The ECIs are obtained via Ordinary Least Squares (OLS), defined as the minimizer of the sum of squared residuals. When $X$ is rank-deficient or ill-conditioned due to collinear rows, the solution is taken to be the minimum-norm OLS solution computed by the Moore–Penrose Pseudoinverse (MPP), denoted $X^{+}$, so that\n$$\n\\widehat{\\mathbf{J}} = X^{+} \\mathbf{y}.\n$$\n\nTo assess predictive performance, you will implement $K$-fold Cross-Validation (K-FCV). In $K$-fold Cross-Validation (K-FCV), the index set $\\{0,1,\\dots,N-1\\}$ is partitioned deterministically into $K$ folds according to the rule: index $i$ is assigned to fold $f$ if $i \\bmod K = f$, for $f \\in \\{0,1,\\dots,K-1\\}$. For each fold $f$, the model is trained on the complement of fold $f$ and evaluated on fold $f$. The Cross-Validation Score (CVS) is defined as the root-mean-square error (RMSE) aggregated over all held-out predictions across all folds:\n$$\n\\mathrm{CVS}_K(X,\\mathbf{y}) = \\sqrt{ \\frac{1}{N} \\sum_{f=0}^{K-1} \\ \\sum_{i \\in \\mathcal{I}_f} \\left( y_i - \\widehat{y}_i^{(f)} \\right)^2 },\n$$\nwhere $\\mathcal{I}_f$ is the set of indices in fold $f$, and $\\widehat{y}_i^{(f)} = \\boldsymbol{\\phi}_i^{\\mathsf{T}} \\, \\widehat{\\mathbf{J}}^{(f)}$ with\n$$\n\\widehat{\\mathbf{J}}^{(f)} = X_{-f}^{+} \\, \\mathbf{y}_{-f},\n$$\nand $(X_{-f}, \\mathbf{y}_{-f})$ denote the training data excluding fold $f$.\n\nYour task is to numerically demonstrate how the $K$-fold Cross-Validation Score (CVS) changes when a collinear training structure (an exact duplicate of an existing correlation vector) is added to the dataset. You will compute the CVS before and after the addition, and report the change. All calculations must use the OLS estimator computed via the Moore–Penrose Pseudoinverse.\n\nUse the following base dataset with $N = 6$ structures and $p = 4$ features (the first feature is an intercept equal to $1$). The design matrix $X_0 \\in \\mathbb{R}^{6 \\times 4}$ and energy vector $\\mathbf{y}_0 \\in \\mathbb{R}^6$ are:\n$$\nX_0 = \\begin{bmatrix}\n1 & -1.0 & -0.2 & 0.5 \\\\\n1 & -0.5 & \\phantom{-}0.1 & -0.3 \\\\\n1 & \\phantom{-}0.0 & -0.4 & \\phantom{-}0.2 \\\\\n1 & \\phantom{-}0.5 & \\phantom{-}0.2 & -0.1 \\\\\n1 & -0.3 & \\phantom{-}0.4 & -0.5 \\\\\n1 & \\phantom{-}0.8 & -0.1 & \\phantom{-}0.4\n\\end{bmatrix},\n\\quad\n\\mathbf{y}_0 = \\begin{bmatrix}\n1.05 \\\\ 0.56 \\\\ 0.06 \\\\ -0.13 \\\\ 0.49 \\\\ -0.37\n\\end{bmatrix}.\n$$\n\nIn each test case below, you will:\n- Compute $\\mathrm{CVS}_K(X_0,\\mathbf{y}_0)$ for the specified $K$.\n- Construct an augmented dataset $(X', \\mathbf{y}')$ by appending one additional row that is collinear with row index $d = 1$ (zero-based indexing) of $X_0$; that is, append the identical feature vector $\\boldsymbol{\\phi}_{1}$ as a new $(N+1)$-th row. The appended energy is a specified scalar $y_{\\mathrm{dup}}$ for that test case. Explicitly, $X' = \\begin{bmatrix} X_0 \\\\ \\boldsymbol{\\phi}_1^{\\mathsf{T}} \\end{bmatrix}$ and $\\mathbf{y}' = \\begin{bmatrix} \\mathbf{y}_0 \\\\ y_{\\mathrm{dup}} \\end{bmatrix}$.\n- Compute $\\mathrm{CVS}_K(X',\\mathbf{y}')$ for the same $K$.\n- Report both values and their difference $\\Delta \\mathrm{CVS} = \\mathrm{CVS}_K(X',\\mathbf{y}') - \\mathrm{CVS}_K(X_0,\\mathbf{y}_0)$.\n\nImplement the above using only deterministic operations with no randomness, and use zero-based indexing for fold assignment and for the duplicate index $d = 1$.\n\nTest suite:\n- Test case $1$: $K = 3$, $y_{\\mathrm{dup}} = 0.56$ (consistent duplicate).\n- Test case $2$: $K = 3$, $y_{\\mathrm{dup}} = 0.62$ (slightly inconsistent duplicate).\n- Test case $3$: $K = 6$, $y_{\\mathrm{dup}} = 0.86$ (strongly inconsistent duplicate).\n- Test case $4$: $K = 2$, $y_{\\mathrm{dup}} = 0.56$ (boundary fold count with consistent duplicate).\n\nOutput specification:\n- For each test case $t \\in \\{1,2,3,4\\}$, compute the triple\n$$\n\\big( \\mathrm{CVS}^{\\mathrm{before}}_t, \\ \\mathrm{CVS}^{\\mathrm{after}}_t, \\ \\Delta \\mathrm{CVS}_t \\big).\n$$\n- Round each of the $12$ reported scalar values to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$$\n\\big[ \\mathrm{CVS}^{\\mathrm{before}}_1, \\ \\mathrm{CVS}^{\\mathrm{after}}_1, \\ \\Delta \\mathrm{CVS}_1, \\ \\mathrm{CVS}^{\\mathrm{before}}_2, \\ \\mathrm{CVS}^{\\mathrm{after}}_2, \\ \\Delta \\mathrm{CVS}_2, \\ \\mathrm{CVS}^{\\mathrm{before}}_3, \\ \\mathrm{CVS}^{\\mathrm{after}}_3, \\ \\Delta \\mathrm{CVS}_3, \\ \\mathrm{CVS}^{\\mathrm{before}}_4, \\ \\mathrm{CVS}^{\\mathrm{after}}_4, \\ \\Delta \\mathrm{CVS}_4 \\big].\n$$\n\nNotes:\n- Clearly implement $K$-fold assignment by $i \\bmod K$.\n- Use the Moore–Penrose Pseudoinverse for OLS to ensure numerical stability under collinearity.\n- No physical units are involved in this computation.",
            "solution": "The task is to compute the $K$-fold Cross-Validation Score (CVS) for a linear model, both before and after adding a collinear data point to the dataset. The model's parameters, called Effective Cluster Interactions (ECIs), are determined using the Ordinary Least Squares (OLS) estimator computed via the Moore-Penrose Pseudoinverse (MPP).\n\nThe overall algorithmic design is based on the direct implementation of the $K$-fold cross-validation procedure as described. The CVS is defined as the root-mean-square error (RMSE) aggregated over all hold-out predictions:\n$$\n\\mathrm{CVS}_K(X,\\mathbf{y}) = \\sqrt{ \\frac{1}{N} \\sum_{f=0}^{K-1} \\ \\sum_{i \\in \\mathcal{I}_f} \\left( y_i - \\widehat{y}_i^{(f)} \\right)^2 }\n$$\nwhere $N$ is the total number of structures, $\\mathcal{I}_f$ is the set of indices in fold $f$, and $\\widehat{y}_i^{(f)}$ is the predicted energy for structure $i$ when it is part of the hold-out set for fold $f$.\n\nThe procedure can be broken down into the following steps:\n\n1.  **Data Partitioning**: The set of data indices $\\{0, 1, \\dots, N-1\\}$ is deterministically partitioned into $K$ folds. Following the problem's rule, a structure with index $i$ is assigned to fold $f$ where $f = i \\bmod K$.\n\n2.  **Iterative Training and Testing**: The process iterates through each fold $f$ from $0$ to $K-1$. In each iteration, the data corresponding to fold $f$ serves as the test set, while the data from all other $K-1$ folds constitute the training set.\n    - Let $(X_{-f}, \\mathbf{y}_{-f})$ be the training data (design matrix and energy vector) and $(X_f, \\mathbf{y}_f)$ be the test data for fold $f$.\n    - The ECI vector $\\widehat{\\mathbf{J}}^{(f)}$ for this fold is computed from the training data using the MPP-based OLS estimator:\n    $$\n    \\widehat{\\mathbf{J}}^{(f)} = X_{-f}^{+} \\, \\mathbf{y}_{-f}\n    $$\n    The Moore-Penrose Pseudoinverse, $X_{-f}^{+}$, is essential. When the augmented dataset is used, certain folds will have training sets $X_{-f}$ that contain two identical rows, making the matrix rank-deficient. The standard matrix inverse $(X_{-f}^{\\mathsf{T}}X_{-f})^{-1}$ would be undefined in such cases. The MPP provides a unique, minimum-norm solution, ensuring numerical stability. This is implemented using `numpy.linalg.pinv`.\n\n3.  **Prediction and Error Accumulation**: The fitted model $\\widehat{\\mathbf{J}}^{(f)}$ is used to make predictions on the hold-out test set $X_f$:\n    $$\n    \\widehat{\\mathbf{y}}_f = X_f \\, \\widehat{\\mathbf{J}}^{(f)}\n    $$\n    The predicted energies $\\widehat{\\mathbf{y}}_f$ (containing elements $\\widehat{y}_i^{(f)}$ for $i \\in \\mathcal{I}_f$) are compared against the true energies $\\mathbf{y}_f$ (containing elements $y_i$). The sum of squared errors (SSE) for the current fold, $\\sum_{i \\in \\mathcal{I}_f} (y_i - \\widehat{y}_i^{(f)})^2$, is calculated and added to a running total.\n\n4.  **CVS Calculation**: After the loop over all $K$ folds is complete, every data point has been used for testing exactly once. The total SSE is divided by the total number of data points $N$, and the square root of this value yields the final CVS.\n\nThis entire procedure is encapsulated within a single function. For each test case, this function is first called with the initial dataset $(X_0, \\mathbf{y}_0)$ and the specified $K$ to find $\\mathrm{CVS}^{\\mathrm{before}}$. An augmented dataset $(X', \\mathbf{y}')$ is then created by appending a duplicate of the row with index $d=1$ from $X_0$ and its corresponding energy $y_{\\mathrm{dup}}$, increasing the total number of samples to $N'=N+1$. The CVS function is called again with $(X', \\mathbf{y}')$ to find $\\mathrm{CVS}^{\\mathrm{after}}$. The difference $\\Delta \\mathrm{CVS} = \\mathrm{CVS}^{\\mathrm{after}} - \\mathrm{CVS}^{\\mathrm{before}}$ is then computed. The final output collates these three values for all test cases, with each value rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_cvs(X: np.ndarray, y: np.ndarray, K: int) -> float:\n    \"\"\"\n    Computes the K-fold Cross-Validation Score (CVS) for a linear model.\n    The model is trained using the Moore-Penrose Pseudoinverse.\n    \"\"\"\n    N = X.shape[0]\n    total_sse = 0.0\n\n    for f in range(K):\n        # Create boolean masks for the train/test split for the current fold.\n        # An index i belongs to fold f if i mod K == f.\n        test_mask = np.array([i % K == f for i in range(N)])\n        \n        # Guard against empty test folds, though not expected with the problem's parameters.\n        if not np.any(test_mask):\n            continue\n            \n        train_mask = ~test_mask\n\n        # Guard against empty training folds.\n        if not np.any(train_mask):\n            # If the training set is empty, no model can be fit.\n            # Predictions are undefined, so we can't calculate error.\n            # For this problem's constraints (K <= N), this case will not occur.\n            continue\n            \n        X_train, y_train = X[train_mask], y[train_mask]\n        X_test, y_test = X[test_mask], y[test_mask]\n\n        # Compute the Effective Cluster Interactions (ECIs) using the Moore-Penrose Pseudoinverse.\n        # This is robust to collinearity/rank-deficiency in X_train.\n        X_train_pinv = np.linalg.pinv(X_train)\n        J_hat_f = X_train_pinv @ y_train\n\n        # Predict energies for the held-out test set.\n        y_pred_f = X_test @ J_hat_f\n\n        # Calculate and accumulate the sum of squared errors for this fold.\n        fold_sse = np.sum((y_test - y_pred_f)**2)\n        total_sse += fold_sse\n    \n    # The CVS is the root mean square error over all N out-of-sample predictions.\n    cvs = np.sqrt(total_sse / N)\n    return cvs\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating CVS before and after adding a collinear row.\n    \"\"\"\n    # Define the base dataset from the problem statement.\n    X0 = np.array([\n        [1.0, -1.0, -0.2,  0.5],\n        [1.0, -0.5,  0.1, -0.3],\n        [1.0,  0.0, -0.4,  0.2],\n        [1.0,  0.5,  0.2, -0.1],\n        [1.0, -0.3,  0.4, -0.5],\n        [1.0,  0.8, -0.1,  0.4]\n    ])\n    y0 = np.array([1.05, 0.56, 0.06, -0.13, 0.49, -0.37])\n\n    # Define the test cases.\n    test_cases = [\n        # (K, y_dup)\n        (3, 0.56),\n        (3, 0.62),\n        (6, 0.86),\n        (2, 0.56)\n    ]\n\n    # The row to be duplicated is at index d=1 (zero-based).\n    row_to_dup = X0[1:2, :]  # Slicing keeps it as a 2D array for vstack.\n    \n    results = []\n    for K, y_dup in test_cases:\n        # 1. Compute CVS for the original base dataset.\n        cvs_before = calculate_cvs(X0, y0, K)\n\n        # 2. Construct the augmented dataset by appending the duplicate row.\n        X_prime = np.vstack([X0, row_to_dup])\n        y_prime = np.append(y0, y_dup)\n        \n        # 3. Compute CVS for the augmented dataset.\n        cvs_after = calculate_cvs(X_prime, y_prime, K)\n        \n        # 4. Calculate the change in CVS.\n        delta_cvs = cvs_after - cvs_before\n        \n        # 5. Store the triplet of results.\n        results.extend([cvs_before, cvs_after, delta_cvs])\n    \n    # Format the final list of results for printing.\n    # Each value is rounded to 6 decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n\n    # Print the final output in the required single-line format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Every statistical model has a domain of applicability; making predictions far outside the space of the training data is risky. This exercise introduces the concept of leverage, a powerful diagnostic tool used to quantify whether a new configuration represents an interpolation or an extrapolation. By calculating leverage scores, you will learn to build a 'warning system' that flags predictions with high uncertainty, ensuring that your cluster expansion model is applied responsibly and its predictions can be trusted .",
            "id": "3437939",
            "problem": "Consider a linear Cluster Expansion (CE) for alloy configurational thermodynamics, where the configurational energy is modeled as a linear functional of cluster correlation functions. Let the training design matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$, where each row corresponds to a configurational structure and each column corresponds to a basis correlation function value (including an intercept column of ones). The CE fitting via linear least squares may be regularized by Tikhonov regularization (ridge), with parameter $\\lambda \\ge 0$. Define the regularized normal matrix $A = X^{\\top} X + \\lambda I_p$, where $I_p$ is the $p \\times p$ identity matrix. The projection (hat) matrix for training data generalizes to $H_{\\lambda} = X A^{-1} X^{\\top}$, and the leverage for a prospective new structure with correlation vector $x \\in \\mathbb{R}^{p}$ is given by the scalar\n$$\n\\ell(x) = x^{\\top} A^{-1} x.\n$$\nIn practice, high $\\ell(x)$ indicates extrapolation risk. A commonly used decision rule compares $\\ell(x)$ to a scaled average leverage threshold,\n$$\nh_{\\mathrm{thr}} = \\alpha \\frac{p}{n},\n$$\nwith user-specified $\\alpha > 0$. If any prospective structure’s leverage exceeds $h_{\\mathrm{thr}}$, one should deem extrapolation risk high and conclude that additional training data are required in that region of configuration space.\n\nTask: For each test case below, compute $\\ell(x)$ for each provided prospective structure $x$ using the corresponding $X$, $\\alpha$, and $\\lambda$, and output a boolean decision per test case defined as “true” if any $\\ell(x) > h_{\\mathrm{thr}}$ and “false” otherwise.\n\nFundamental bases you may assume:\n- The CE energy model is linear in the correlation basis and leads to linear least squares with design matrix $X$.\n- For ridge-regularized least squares, the regularized normal matrix is $A = X^{\\top} X + \\lambda I_p$.\n- The leverage of a point $x$ in linear models equals $x^{\\top} A^{-1} x$ for ridge with parameter $\\lambda \\ge 0$, which reduces to the classical ordinary least squares case when $\\lambda = 0$.\n\nNo physical units are involved. Angles do not appear. All numerical outputs must be computed as real numbers internally; the final decision per test case must be boolean.\n\nTest suite:\n- Test case $1$:\n  - $n = 10$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.0$.\n  - $X$ has rows $(x_i^{\\top})$ defined with an intercept column of ones and four correlation columns:\n    - Row $1$: $(\\;1.0,\\; 0.10,\\; 0.05,\\; 0.02,\\; 0.00\\;)$\n    - Row $2$: $(\\;1.0,\\; 0.20,\\; 0.10,\\; 0.04,\\; 0.00\\;)$\n    - Row $3$: $(\\;1.0,\\; -0.10,\\; -0.05,\\; -0.01,\\; 0.00\\;)$\n    - Row $4$: $(\\;1.0,\\; 0.00,\\; 0.00,\\; 0.00,\\; 0.00\\;)$\n    - Row $5$: $(\\;1.0,\\; 0.15,\\; 0.07,\\; 0.03,\\; 0.02\\;)$\n    - Row $6$: $(\\;1.0,\\; -0.05,\\; -0.02,\\; -0.01,\\; -0.02\\;)$\n    - Row $7$: $(\\;1.0,\\; 0.05,\\; 0.03,\\; 0.015,\\; 0.00\\;)$\n    - Row $8$: $(\\;1.0,\\; -0.20,\\; -0.10,\\; -0.05,\\; 0.00\\;)$\n    - Row $9$: $(\\;1.0,\\; 0.10,\\; 0.06,\\; 0.025,\\; 0.01\\;)$\n    - Row $10$: $(\\;1.0,\\; -0.10,\\; -0.04,\\; -0.02,\\; -0.01\\;)$\n  - Prospective structures to evaluate:\n    - $(\\;1.0,\\; 0.12,\\; 0.06,\\; 0.02,\\; 0.00\\;)$\n    - $(\\;1.0,\\; 0.00,\\; 0.00,\\; 0.00,\\; 1.00\\;)$\n    - $(\\;1.0,\\; -0.30,\\; -0.15,\\; -0.06,\\; -0.02\\;)$\n  - Decision rule: return “true” if any $\\ell(x) > h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\n- Test case $2$:\n  - $n = 16$, $p = 6$, $\\alpha = 2.0$, $\\lambda = 0.0$.\n  - $X$ rows are $(1.0, c_2, c_3, c_4, c_5, c_6)$, where the tuples $(c_2,\\ldots,c_6)$ are:\n    - $(-0.8,\\; -0.8,\\; -0.8,\\; -0.8,\\; -0.8)$\n    - $(-0.8,\\; -0.4,\\; -0.4,\\; -0.4,\\; -0.4)$\n    - $(-0.8,\\; 0.4,\\; 0.0,\\; -0.4,\\; 0.4)$\n    - $(-0.8,\\; 0.8,\\; 0.8,\\; 0.0,\\; -0.4)$\n    - $(-0.4,\\; -0.8,\\; 0.4,\\; 0.8,\\; 0.0)$\n    - $(-0.4,\\; -0.4,\\; -0.8,\\; 0.4,\\; 0.8)$\n    - $(-0.4,\\; 0.4,\\; -0.4,\\; -0.8,\\; 0.4)$\n    - $(-0.4,\\; 0.8,\\; 0.8,\\; -0.4,\\; -0.8)$\n    - $(0.4,\\; -0.8,\\; 0.8,\\; 0.4,\\; -0.4)$\n    - $(0.4,\\; -0.4,\\; 0.4,\\; -0.8,\\; 0.8)$\n    - $(0.4,\\; 0.4,\\; -0.8,\\; 0.8,\\; 0.4)$\n    - $(0.4,\\; 0.8,\\; -0.4,\\; 0.4,\\; -0.8)$\n    - $(0.8,\\; -0.8,\\; -0.4,\\; 0.8,\\; 0.4)$\n    - $(0.8,\\; -0.4,\\; 0.8,\\; -0.4,\\; -0.8)$\n    - $(0.8,\\; 0.4,\\; 0.4,\\; -0.8,\\; 0.0)$\n    - $(0.8,\\; 0.8,\\; -0.8,\\; 0.4,\\; -0.4)$\n  - Prospective structures to evaluate:\n    - $(\\;1.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0\\;)$\n    - $(\\;1.0,\\; 0.9,\\; 0.9,\\; 0.9,\\; 0.9,\\; 0.9\\;)$\n  - Decision rule: return “true” if any $\\ell(x) > h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\n- Test case $3$:\n  - $n = 8$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.05$.\n  - $X$ rows are $(1.0, c_2, c_3, c_4, c_5)$ with nearly collinear features:\n    - $(-0.5,\\; -1.0,\\; 0.5,\\; 0.01)$\n    - $(-0.4,\\; -0.8,\\; 0.4,\\; 0.00)$\n    - $(-0.3,\\; -0.6,\\; 0.3,\\; 0.00)$\n    - $(-0.2,\\; -0.4,\\; 0.2,\\; -0.01)$\n    - $(0.2,\\; 0.4,\\; -0.2,\\; 0.01)$\n    - $(0.3,\\; 0.6,\\; -0.3,\\; 0.00)$\n    - $(0.4,\\; 0.8,\\; -0.4,\\; 0.00)$\n    - $(0.5,\\; 1.0,\\; -0.5,\\; -0.01)$\n  - Prospective structures to evaluate:\n    - $(\\;1.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 1.0\\;)$\n    - $(\\;1.0,\\; 0.25,\\; 0.5,\\; -0.25,\\; 0.0\\;)$\n  - Decision rule: return “true” if any $\\ell(x) > h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\n- Test case $4$:\n  - $n = 16$, $p = 6$, $\\alpha = 0.5$, $\\lambda = 0.0$.\n  - Use the same $X$ as in Test case $2$.\n  - Prospective structures to evaluate:\n    - Exactly equal to the first training row in Test case $2$: $(\\;1.0,\\; -0.8,\\; -0.8,\\; -0.8,\\; -0.8,\\; -0.8\\;)$\n    - $(\\;1.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0\\;)$\n  - Decision rule: return “true” if any $\\ell(x) > h_{\\mathrm{thr}}$ with $h_{\\mathrm{thr}} = \\alpha p / n$.\n\nYour program must:\n- Implement the computation of $A^{-1}$ for each test case, evaluate $\\ell(x)$ for each listed prospective structure, compare to $h_{\\mathrm{thr}}$, and return a boolean per test case indicating whether additional training data are required.\n- Use no external inputs; all data are embedded.\n- Produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, for the four test cases in order, for example: “[true,false,true,true]” but using Python boolean literals and exact comma placement, like “[True,False,True,True]”.",
            "solution": "The core task is to determine, for several test cases, whether a prospective new structure represents an extrapolation with respect to a given training set. This determination is based on the concept of leverage in a regularized linear regression model.\n\nThe model for the configurational energy in a Cluster Expansion (CE) is linear, which allows the use of linear least squares. The training data are represented by a design matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of training structures and $p$ is the number of basis functions (correlation functions, including an intercept).\n\nThe regression is stabilized using Tikhonov regularization (ridge regression) with a parameter $\\lambda \\ge 0$. This leads to the regularized normal matrix $A$, a $p \\times p$ matrix defined as:\n$$\nA = X^{\\top} X + \\lambda I_p\n$$\nwhere $X^{\\top}$ is the transpose of $X$ and $I_p$ is the $p \\times p$ identity matrix. The matrix $A$ is guaranteed to be invertible for $\\lambda > 0$. If $\\lambda=0$, $A$ is invertible if and only if the columns of $X$ are linearly independent (i.e., $X$ has full column rank).\n\nThe leverage of a prospective new structure, characterized by its correlation vector $x \\in \\mathbb{R}^{p}$, is a scalar quantity $\\ell(x)$ that measures the influence of this point on a potential model fit. It is given by:\n$$\n\\ell(x) = x^{\\top} A^{-1} x\n$$\nA high leverage value indicates that the point $x$ is far from the \"center\" of the training data in the space of correlation functions, suggesting that a prediction at this point would be an extrapolation.\n\nA decision rule is established to flag such extrapolations. The leverage $\\ell(x)$ is compared to a threshold $h_{\\mathrm{thr}}$, which is defined as a multiple of the average leverage of the training points:\n$$\nh_{\\mathrm{thr}} = \\alpha \\frac{p}{n}\n$$\nHere, $\\alpha > 0$ is a user-specified sensitivity parameter. The task requires determining if any of the prospective structures in a test case have a leverage exceeding this threshold. If for any prospective structure $x$, $\\ell(x) > h_{\\mathrm{thr}}$, the result for that test case is `true`; otherwise, it is `false`.\n\nThe procedure for each test case is as follows:\n1.  Identify the parameters $n$, $p$, $\\alpha$, and $\\lambda$.\n2.  Construct the training matrix $X$ and the set of prospective vectors $\\{x_i\\}$.\n3.  Calculate the threshold $h_{\\mathrm{thr}} = \\alpha \\frac{p}{n}$.\n4.  Compute the matrix $A = X^{\\top} X + \\lambda I_p$.\n5.  Compute the inverse matrix $A^{-1}$.\n6.  For each prospective vector $x_i$, calculate its leverage $\\ell(x_i) = x_i^{\\top} A^{-1} x_i$.\n7.  If any calculated leverage $\\ell(x_i)$ is greater than $h_{\\mathrm{thr}}$, the test case evaluates to `true`. If all $\\ell(x_i) \\le h_{\\mathrm{thr}}$, it evaluates to `false`.\n\nWe now apply this procedure to each test case.\n\n**Test Case 1**\n- Parameters: $n = 10$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.0$.\n- Threshold: $h_{\\mathrm{thr}} = 1.0 \\times \\frac{5}{10} = 0.5$.\n- Matrix $A = X^{\\top} X$.\n- Prospective structures $x_1, x_2, x_3$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, 0.12, 0.06, 0.02, 0.00)^{\\top}$: $\\ell(x_1) \\approx 0.491$. This point is an interpolation and its leverage is below the threshold.\n  - For $x_2 = (1.0, 0.00, 0.00, 0.00, 1.00)^{\\top}$: $\\ell(x_2) \\approx 2503.74$. This point is a significant extrapolation, as the last feature's value ($1.0$) is far outside its range in the training data (maximum of $0.02$). Its leverage is vastly greater than $h_{\\mathrm{thr}}$.\n  - For $x_3 = (1.0, -0.30, -0.15, -0.06, -0.02)^{\\top}$: $\\ell(x_3) \\approx 4.391$. This is also an extrapolation, with feature values outside the training range.\nSince $\\ell(x_2) > 0.5$, the condition is met. The decision for this case is **true**.\n\n**Test Case 2**\n- Parameters: $n = 16$, $p = 6$, $\\alpha = 2.0$, $\\lambda = 0.0$.\n- Threshold: $h_{\\mathrm{thr}} = 2.0 \\times \\frac{6}{16} = 0.75$.\n- Matrix $A = X^{\\top} X$.\n- Prospective structures $x_1, x_2$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, 0.0, 0.0, 0.0, 0.0, 0.0)^{\\top}$: $\\ell(x_1) = 0.0625$. This point is the center of the design space and has the minimum possible leverage. $0.0625 \\le 0.75$.\n  - For $x_2 = (1.0, 0.9, 0.9, 0.9, 0.9, 0.9)^{\\top}$: $\\ell(x_2) \\approx 2.766$. The features in the training data lie in the range $[-0.8, 0.8]$, so this point is an extrapolation. Its leverage exceeds the threshold.\nSince $\\ell(x_2) > 0.75$, the decision for this case is **true**.\n\n**Test Case 3**\n- Parameters: $n = 8$, $p = 5$, $\\alpha = 1.0$, $\\lambda = 0.05$.\n- Threshold: $h_{\\mathrm{thr}} = 1.0 \\times \\frac{5}{8} = 0.625$.\n- The training matrix $X$ has nearly collinear features. The regularization term $\\lambda I_p$ is crucial for ensuring the numerical stability of the inversion of $A = X^{\\top} X + \\lambda I_p$.\n- Prospective structures $x_1, x_2$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, 0.0, 0.0, 0.0, 1.0)^{\\top}$: $\\ell(x_1) \\approx 20.01$. The last feature of this point is $1.0$, while it is close to $0.0$ for all training points. This represents an extrapolation in a direction poorly represented in the training data, leading to a high leverage value. $20.01 > 0.625$.\n  - For $x_2 = (1.0, 0.25, 0.5, -0.25, 0.0)^{\\top}$: $\\ell(x_2) \\approx 0.355$. This point lies on the manifold defined by the collinear training points and is an interpolation. Its leverage is low.\nSince $\\ell(x_1) > 0.625$, the decision for this case is **true**.\n\n**Test Case 4**\n- Parameters: $n = 16$, $p = 6$, $\\alpha = 0.5$, $\\lambda = 0.0$.\n- Threshold: $h_{\\mathrm{thr}} = 0.5 \\times \\frac{6}{16} = 0.1875$. This is a strict threshold, being half the average leverage ($p/n = 0.375$).\n- The training matrix $X$ is the same as in Test Case $2$.\n- Prospective structures $x_1, x_2$ are provided.\n- Calculations:\n  - For $x_1 = (1.0, -0.8, -0.8, -0.8, -0.8, -0.8)^{\\top}$: This is identical to the first row of the training matrix $X$. The leverage of a training point is the corresponding diagonal element of the hat matrix $H = X(X^{\\top}X)^{-1}X^{\\top}$. The calculation yields $\\ell(x_1) \\approx 0.590$. This value represents the leverage of a \"corner\" point of the experimental design. $0.590 > 0.1875$.\n  - For $x_2 = (1.0, 0.0, 0.0, 0.0, 0.0, 0.0)^{\\top}$: As in Test Case $2$, $\\ell(x_2) = 0.0625$. This is below the threshold.\nSince $\\ell(x_1) > 0.1875$, the decision for this case is **true**.\n\nIn summary, at least one prospective structure in each test case exhibits a leverage greater than the respective threshold, leading to a decision of 'true' for all four cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the leverage calculation problem for all test cases.\n    \"\"\"\n    \n    # Test cases data defined as a list of dictionaries.\n    test_cases_data = [\n        # Test case 1\n        {\n            \"n\": 10, \"p\": 5, \"alpha\": 1.0, \"lambda_val\": 0.0,\n            \"X\": np.array([\n                [1.0,  0.10,  0.05,  0.020,  0.00],\n                [1.0,  0.20,  0.10,  0.040,  0.00],\n                [1.0, -0.10, -0.05, -0.010,  0.00],\n                [1.0,  0.00,  0.00,  0.000,  0.00],\n                [1.0,  0.15,  0.07,  0.030,  0.02],\n                [1.0, -0.05, -0.02, -0.010, -0.02],\n                [1.0,  0.05,  0.03,  0.015,  0.00],\n                [1.0, -0.20, -0.10, -0.050,  0.00],\n                [1.0,  0.10,  0.06,  0.025,  0.01],\n                [1.0, -0.10, -0.04, -0.020, -0.01]\n            ]),\n            \"prospective_x\": [\n                np.array([1.0,  0.12,  0.06,  0.02,  0.00]),\n                np.array([1.0,  0.00,  0.00,  0.00,  1.00]),\n                np.array([1.0, -0.30, -0.15, -0.06, -0.02])\n            ]\n        },\n        # Test case 2\n        {\n            \"n\": 16, \"p\": 6, \"alpha\": 2.0, \"lambda_val\": 0.0,\n            \"X\": np.hstack([np.ones((16, 1)), np.array([\n                [-0.8, -0.8, -0.8, -0.8, -0.8], [-0.8, -0.4, -0.4, -0.4, -0.4],\n                [-0.8,  0.4,  0.0, -0.4,  0.4], [-0.8,  0.8,  0.8,  0.0, -0.4],\n                [-0.4, -0.8,  0.4,  0.8,  0.0], [-0.4, -0.4, -0.8,  0.4,  0.8],\n                [-0.4,  0.4, -0.4, -0.8,  0.4], [-0.4,  0.8,  0.8, -0.4, -0.8],\n                [ 0.4, -0.8,  0.8,  0.4, -0.4], [ 0.4, -0.4,  0.4, -0.8,  0.8],\n                [ 0.4,  0.4, -0.8,  0.8,  0.4], [ 0.4,  0.8, -0.4,  0.4, -0.8],\n                [ 0.8, -0.8, -0.4,  0.8,  0.4], [ 0.8, -0.4,  0.8, -0.4, -0.8],\n                [ 0.8,  0.4,  0.4, -0.8,  0.0], [ 0.8,  0.8, -0.8,  0.4, -0.4]\n            ])]),\n            \"prospective_x\": [\n                np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n                np.array([1.0, 0.9, 0.9, 0.9, 0.9, 0.9])\n            ]\n        },\n        # Test case 3\n        {\n            \"n\": 8, \"p\": 5, \"alpha\": 1.0, \"lambda_val\": 0.05,\n            \"X\": np.hstack([np.ones((8, 1)), np.array([\n                [-0.5, -1.0,  0.5,  0.01], [-0.4, -0.8,  0.4,  0.00],\n                [-0.3, -0.6,  0.3,  0.00], [-0.2, -0.4,  0.2, -0.01],\n                [ 0.2,  0.4, -0.2,  0.01], [ 0.3,  0.6, -0.3,  0.00],\n                [ 0.4,  0.8, -0.4,  0.00], [ 0.5,  1.0, -0.5, -0.01]\n            ])]),\n            \"prospective_x\": [\n                np.array([1.0,  0.00,  0.0, -0.00,  1.0]),\n                np.array([1.0,  0.25,  0.5, -0.25,  0.0])\n            ]\n        },\n        # Test case 4\n        {\n            \"n\": 16, \"p\": 6, \"alpha\": 0.5, \"lambda_val\": 0.0,\n            \"X\": np.hstack([np.ones((16, 1)), np.array([\n                [-0.8, -0.8, -0.8, -0.8, -0.8], [-0.8, -0.4, -0.4, -0.4, -0.4],\n                [-0.8,  0.4,  0.0, -0.4,  0.4], [-0.8,  0.8,  0.8,  0.0, -0.4],\n                [-0.4, -0.8,  0.4,  0.8,  0.0], [-0.4, -0.4, -0.8,  0.4,  0.8],\n                [-0.4,  0.4, -0.4, -0.8,  0.4], [-0.4,  0.8,  0.8, -0.4, -0.8],\n                [ 0.4, -0.8,  0.8,  0.4, -0.4], [ 0.4, -0.4,  0.4, -0.8,  0.8],\n                [ 0.4,  0.4, -0.8,  0.8,  0.4], [ 0.4,  0.8, -0.4,  0.4, -0.8],\n                [ 0.8, -0.8, -0.4,  0.8,  0.4], [ 0.8, -0.4,  0.8, -0.4, -0.8],\n                [ 0.8,  0.4,  0.4, -0.8,  0.0], [ 0.8,  0.8, -0.8,  0.4, -0.4]\n            ])]),\n            \"prospective_x\": [\n                np.array([1.0, -0.8, -0.8, -0.8, -0.8, -0.8]),\n                np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases_data:\n        n, p, alpha, lambda_val = case[\"n\"], case[\"p\"], case[\"alpha\"], case[\"lambda_val\"]\n        X, prospective_x_list = case[\"X\"], case[\"prospective_x\"]\n\n        # Calculate the leverage threshold\n        h_thr = alpha * p / n\n\n        # Form the regularized normal matrix A\n        A = X.T @ X + lambda_val * np.eye(p)\n\n        # Compute the inverse of A\n        A_inv = np.linalg.inv(A)\n\n        # Initialize the decision for this test case\n        decision = False\n\n        # Evaluate leverage for each prospective structure\n        for x in prospective_x_list:\n            leverage = x.T @ A_inv @ x\n            if leverage > h_thr:\n                decision = True\n                break  # Found one, no need to check others\n\n        results.append(decision)\n\n    # Format the final output as a string representing a list of booleans\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}