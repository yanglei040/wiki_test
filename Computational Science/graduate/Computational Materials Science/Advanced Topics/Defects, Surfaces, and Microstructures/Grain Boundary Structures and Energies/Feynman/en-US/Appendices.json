{
    "hands_on_practices": [
        {
            "introduction": "The proliferation of interatomic potentials, from the classical Embedded Atom Method (EAM) to modern Machine Learning (ML) models, necessitates robust methods for validation. This practice introduces two key metrics: the Root-Mean-Square Error (RMSE) for accuracy and the Akaike Information Criterion (AIC) for model selection, which judiciously penalizes model complexity to favor generalizability . By applying these metrics, you will learn to quantitatively compare different models and make informed decisions about their suitability for specific tasks.",
            "id": "3455449",
            "problem": "You are tasked with benchmarking three interatomic potential models—Embedded Atom Method (EAM), Modified Embedded Atom Method (MEAM), and Machine Learning (ML)—against Density Functional Theory (DFT) reference grain boundary energies for multiple grain boundaries. In each test case, you are given arrays of DFT reference grain boundary energies per unit area in joules per square meter and corresponding predicted energies from each model for the same set of grain boundaries. Your program must compute, per test case and per model, the root-mean-square error (RMSE) as a measure of predictive accuracy and the Akaike Information Criterion (AIC) as a measure of generalization that penalizes model complexity. The final goal is to report, for each test case, the RMSEs, the AICs, and the indices of the best models by RMSE and by AIC. Assume that all residuals are independent and identically distributed with a normal distribution having unknown variance, and use the natural logarithm in all likelihood expressions.\n\nFundamental base and definitions to be used:\n- The DFT energy for grain boundary $i$ is denoted $E^{\\mathrm{DFT}}_{i}$, and a model prediction is $E^{\\mathrm{model}}_{i}$, both expressed in $\\mathrm{J}/\\mathrm{m}^{2}$.\n- The residuals are $r_{i} = E^{\\mathrm{model}}_{i} - E^{\\mathrm{DFT}}_{i}$.\n- The sum of squared errors is $\\mathrm{SSE} = \\sum_{i=1}^{n} r_{i}^{2}$, and the root-mean-square error is $\\mathrm{RMSE} = \\sqrt{\\mathrm{SSE} / n}$, expressed in $\\mathrm{J}/\\mathrm{m}^{2}$.\n- The Akaike Information Criterion (AIC) is defined by $\\mathrm{AIC} = 2k - 2 \\ln \\hat{L}$, where $k$ is the number of fitted parameters (or effective degrees of freedom) of the model and $\\hat{L}$ is the maximized likelihood under the normal error model with unknown variance. You must derive and use the explicit expression that results when $\\hat{L}$ is computed under the assumption that the residuals $r_{i}$ are independent, identically distributed Gaussian random variables with zero mean and unknown variance.\n\nModel complexities:\n- Use the following parameter counts for all test cases: $k_{\\mathrm{EAM}} = 12$, $k_{\\mathrm{MEAM}} = 22$, $k_{\\mathrm{ML}} = 80$.\n\nTest suite:\n- Case $1$ ($n=8$):\n  - DFT reference $E^{\\mathrm{DFT}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.45,\\, 0.62,\\, 0.78,\\, 1.05,\\, 0.71,\\, 0.88,\\, 0.95,\\, 0.66]$.\n  - EAM predictions $E^{\\mathrm{EAM}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.50,\\, 0.59,\\, 0.81,\\, 1.00,\\, 0.77,\\, 0.84,\\, 0.90,\\, 0.70]$.\n  - MEAM predictions $E^{\\mathrm{MEAM}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.47,\\, 0.61,\\, 0.79,\\, 1.03,\\, 0.72,\\, 0.89,\\, 0.93,\\, 0.67]$.\n  - ML predictions $E^{\\mathrm{ML}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.452,\\, 0.621,\\, 0.779,\\, 1.049,\\, 0.712,\\, 0.879,\\, 0.951,\\, 0.659]$.\n- Case $2$ ($n=4$):\n  - DFT reference $E^{\\mathrm{DFT}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.55,\\, 0.80,\\, 0.95,\\, 0.60]$.\n  - EAM predictions $E^{\\mathrm{EAM}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.58,\\, 0.78,\\, 0.98,\\, 0.62]$.\n  - MEAM predictions $E^{\\mathrm{MEAM}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.56,\\, 0.81,\\, 0.96,\\, 0.61]$.\n  - ML predictions $E^{\\mathrm{ML}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.5505,\\, 0.8005,\\, 0.9495,\\, 0.6005]$.\n- Case $3$ ($n=12$):\n  - DFT reference $E^{\\mathrm{DFT}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.52,\\, 0.60,\\, 0.68,\\, 0.75,\\, 0.83,\\, 0.90,\\, 0.98,\\, 1.05,\\, 1.12,\\, 0.70,\\, 0.79,\\, 0.88]$.\n  - EAM predictions $E^{\\mathrm{EAM}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.56,\\, 0.58,\\, 0.74,\\, 0.71,\\, 0.86,\\, 0.93,\\, 1.02,\\, 1.00,\\, 1.08,\\, 0.74,\\, 0.84,\\, 0.85]$.\n  - MEAM predictions $E^{\\mathrm{MEAM}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.70,\\, 0.78,\\, 0.86,\\, 0.94,\\, 1.02,\\, 1.10,\\, 1.18,\\, 1.26,\\, 1.34,\\, 0.88,\\, 0.97,\\, 1.06]$.\n  - ML predictions $E^{\\mathrm{ML}}$ in $\\mathrm{J}/\\mathrm{m}^{2}$: $[0.5201,\\, 0.6001,\\, 0.6801,\\, 0.7499,\\, 0.8299,\\, 0.8999,\\, 0.9799,\\, 1.0501,\\, 1.1201,\\, 0.7001,\\, 0.7901,\\, 0.8799]$.\n\nComputational requirements:\n- For each test case and each model, compute $\\mathrm{RMSE}$ in $\\mathrm{J}/\\mathrm{m}^{2}$ and $\\mathrm{AIC}$ (dimensionless) using the above definitions and assumptions.\n- Round each $\\mathrm{RMSE}$ to $6$ decimal places and each $\\mathrm{AIC}$ to $3$ decimal places.\n- Determine the index of the best model by RMSE (smallest $\\mathrm{RMSE}$) and by AIC (smallest $\\mathrm{AIC}$). Use the index mapping EAM $\\rightarrow 0$, MEAM $\\rightarrow 1$, ML $\\rightarrow 2$.\n\nFinal output format:\n- For each test case, output the $8$ values in this exact order:\n  - $\\mathrm{RMSE}_{\\mathrm{EAM}}$, $\\mathrm{RMSE}_{\\mathrm{MEAM}}$, $\\mathrm{RMSE}_{\\mathrm{ML}}$, $\\mathrm{AIC}_{\\mathrm{EAM}}$, $\\mathrm{AIC}_{\\mathrm{MEAM}}$, $\\mathrm{AIC}_{\\mathrm{ML}}$, best-by-RMSE index, best-by-AIC index.\n- Aggregate the results of the $3$ test cases into a single list in the same order and print a single line containing the results as a comma-separated list enclosed in square brackets, for example $[x_{1}, x_{2}, \\dots, x_{24}]$.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. All necessary data, definitions, and constraints are provided, and no contradictions are present. The task is to benchmark three interatomic potential models by computing the Root-Mean-Square Error (RMSE) and the Akaike Information Criterion (AIC), and then identifying the best model according to each metric for several test cases. I will now proceed with a full solution.\n\nThe solution requires two primary calculations for each model in each test case: the RMSE and the AIC. The RMSE is a measure of predictive accuracy, while the AIC is a tool for model selection that balances goodness of fit with model complexity.\n\nFirst, we must derive the specific formula for the AIC based on the problem's premises. The general definition is given as:\n$$\n\\mathrm{AIC} = 2k - 2 \\ln \\hat{L}\n$$\nwhere $k$ is the number of fitted parameters of the model, and $\\hat{L}$ is the maximized value of the likelihood function for the model.\n\nThe problem states to assume that the residuals, $r_i = E^{\\mathrm{model}}_{i} - E^{\\mathrm{DFT}}_{i}$, are independent and identically distributed (i.i.d.) random variables from a Gaussian (normal) distribution with a mean of zero and an unknown variance, $\\sigma^2$. That is, $r_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nThe probability density function (PDF) for a single residual $r_i$ under this assumption is:\n$$\nf(r_i | \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)\n$$\nSince the residuals are i.i.d., the likelihood function $L$ for observing the set of $n$ residuals, $\\{r_1, r_2, \\dots, r_n\\}$, given the variance $\\sigma^2$, is the product of their individual probabilities:\n$$\nL(\\sigma^2) = \\prod_{i=1}^{n} f(r_i | \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} r_i^2\\right)\n$$\nIt is more convenient to work with the natural log-likelihood, $\\ln L$:\n$$\n\\ln L(\\sigma^2) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} r_i^2\n$$\nTo find the maximized likelihood $\\hat{L}$, we must find the value of $\\sigma^2$ that maximizes this function. This is the Maximum Likelihood Estimate (MLE) of the variance, denoted $\\hat{\\sigma}^2$. We find it by taking the derivative of $\\ln L(\\sigma^2)$ with respect to $\\sigma^2$ and setting it to zero.\n$$\n\\frac{\\partial}{\\partial(\\sigma^2)} \\ln L(\\sigma^2) = -\\frac{n}{2} \\frac{1}{\\sigma^2} + \\frac{1}{2(\\sigma^2)^2} \\sum_{i=1}^{n} r_i^2 = 0\n$$\nSolving for $\\sigma^2$ yields the MLE:\n$$\n\\frac{n}{2\\sigma^2} = \\frac{\\sum r_i^2}{2(\\sigma^2)^2} \\implies \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2\n$$\nThe sum of squared residuals, $\\sum_{i=1}^{n} r_i^2$, is defined as the Sum of Squared Errors (SSE). Thus, the MLE for the variance is:\n$$\n\\hat{\\sigma}^2 = \\frac{\\mathrm{SSE}}{n}\n$$\nNow, we substitute this estimate $\\hat{\\sigma}^2$ back into the log-likelihood function to obtain the maximized log-likelihood, $\\ln \\hat{L}$:\n$$\n\\ln \\hat{L} = \\ln L(\\hat{\\sigma}^2) = -\\frac{n}{2} \\ln(2\\pi \\hat{\\sigma}^2) - \\frac{1}{2\\hat{\\sigma}^2} \\sum_{i=1}^{n} r_i^2\n$$\n$$\n\\ln \\hat{L} = -\\frac{n}{2} \\ln\\left(2\\pi \\frac{\\mathrm{SSE}}{n}\\right) - \\frac{1}{2\\left(\\frac{\\mathrm{SSE}}{n}\\right)} (\\mathrm{SSE}) = -\\frac{n}{2} \\ln\\left(2\\pi \\frac{\\mathrm{SSE}}{n}\\right) - \\frac{n}{2}\n$$\nFinally, we substitute this expression for $\\ln \\hat{L}$ into the AIC formula:\n$$\n\\mathrm{AIC} = 2k - 2\\left[ -\\frac{n}{2} \\ln\\left(2\\pi \\frac{\\mathrm{SSE}}{n}\\right) - \\frac{n}{2} \\right]\n$$\n$$\n\\mathrm{AIC} = 2k + n \\ln\\left(2\\pi \\frac{\\mathrm{SSE}}{n}\\right) + n\n$$\nThis is the explicit formula for the AIC that will be used in the calculations.\n\nThe computational procedure for each test case and each model is as follows:\n1.  Given the arrays of DFT reference energies $E^{\\mathrm{DFT}}$ and model predictions $E^{\\mathrm{model}}$, calculate the residuals $r_i = E^{\\mathrm{model}}_{i} - E^{\\mathrm{DFT}}_{i}$ for $i = 1, \\dots, n$.\n2.  Calculate the Sum of Squared Errors: $\\mathrm{SSE} = \\sum_{i=1}^{n} r_{i}^{2}$.\n3.  Calculate the Root-Mean-Square Error: $\\mathrm{RMSE} = \\sqrt{\\mathrm{SSE} / n}$.\n4.  Using the calculated $\\mathrm{SSE}$, the number of data points $n$, and the given model parameter count $k$ ($k_{\\mathrm{EAM}} = 12$, $k_{\\mathrm{MEAM}} = 22$, $k_{\\mathrm{ML}} = 80$), calculate the AIC using the derived formula: $\\mathrm{AIC} = 2k + n \\ln(2\\pi \\, \\mathrm{SSE}/n) + n$.\n5.  Round the calculated RMSE values to $6$ decimal places and AIC values to $3$ decimal places.\n6.  Compare the RMSE values for the three models and identify the index of the model with the minimum RMSE.\n7.  Compare the AIC values for the three models and identify the index of the model with the minimum AIC. A lower AIC value indicates a better model.\n8.  Assemble the eight resulting values for each case in the prescribed order and combine them into a single list for the final output. The index mapping is EAM $\\rightarrow 0$, MEAM $\\rightarrow 1$, ML $\\rightarrow 2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are permitted according to problem spec.\n\ndef solve():\n    \"\"\"\n    Solves the model benchmarking problem by calculating RMSE and AIC for three models\n    across three test cases and identifying the best model by each metric.\n    \"\"\"\n\n    # Model complexities (number of parameters k)\n    k_vals = {\n        \"EAM\": 12,\n        \"MEAM\": 22,\n        \"ML\": 80\n    }\n\n    # Test suite data\n    test_cases = [\n        {\n            \"n\": 8,\n            \"dft\": np.array([0.45, 0.62, 0.78, 1.05, 0.71, 0.88, 0.95, 0.66]),\n            \"models\": {\n                \"EAM\": np.array([0.50, 0.59, 0.81, 1.00, 0.77, 0.84, 0.90, 0.70]),\n                \"MEAM\": np.array([0.47, 0.61, 0.79, 1.03, 0.72, 0.89, 0.93, 0.67]),\n                \"ML\": np.array([0.452, 0.621, 0.779, 1.049, 0.712, 0.879, 0.951, 0.659])\n            }\n        },\n        {\n            \"n\": 4,\n            \"dft\": np.array([0.55, 0.80, 0.95, 0.60]),\n            \"models\": {\n                \"EAM\": np.array([0.58, 0.78, 0.98, 0.62]),\n                \"MEAM\": np.array([0.56, 0.81, 0.96, 0.61]),\n                \"ML\": np.array([0.5505, 0.8005, 0.9495, 0.6005])\n            }\n        },\n        {\n            \"n\": 12,\n            \"dft\": np.array([0.52, 0.60, 0.68, 0.75, 0.83, 0.90, 0.98, 1.05, 1.12, 0.70, 0.79, 0.88]),\n            \"models\": {\n                \"EAM\": np.array([0.56, 0.58, 0.74, 0.71, 0.86, 0.93, 1.02, 1.00, 1.08, 0.74, 0.84, 0.85]),\n                \"MEAM\": np.array([0.70, 0.78, 0.86, 0.94, 1.02, 1.10, 1.18, 1.26, 1.34, 0.88, 0.97, 1.06]),\n                \"ML\": np.array([0.5201, 0.6001, 0.6801, 0.7499, 0.8299, 0.8999, 0.9799, 1.0501, 1.1201, 0.7001, 0.7901, 0.8799])\n            }\n        }\n    ]\n\n    def calculate_metrics(dft_energies, model_energies, k):\n        \"\"\"\n        Calculates RMSE and AIC for a given model.\n\n        Args:\n            dft_energies (np.ndarray): Array of reference DFT energies.\n            model_energies (np.ndarray): Array of predicted model energies.\n            k (int): Number of parameters for the model.\n\n        Returns:\n            tuple: A tuple containing the calculated RMSE and AIC.\n        \"\"\"\n        n = len(dft_energies)\n        residuals = model_energies - dft_energies\n        sse = np.sum(residuals**2)\n\n        # Calculate RMSE\n        rmse = np.sqrt(sse / n)\n\n        # Calculate AIC\n        # AIC = 2k - 2*ln(L_hat)\n        # L_hat is the maximized likelihood, which for a Gaussian error model is\n        # derived from the MLE of variance, sigma_hat^2 = SSE/n.\n        # This results in: AIC = 2k + n*ln(2*pi*SSE/n) + n\n        # Handle case where SSE is zero to avoid log(0)\n        if sse == 0:\n            # If SSE is zero, the likelihood is infinite, and ln(L_hat) is infinite.\n            # Thus, AIC approaches -infinity. In practice, this signals a perfect fit.\n            # For this problem's context, machine precision might yield a very small\n            # positive SSE, so this case is unlikely but handled defensively.\n            aic = -np.inf\n        else:\n            aic = 2 * k + n * np.log(2 * np.pi * sse / n) + n\n\n        return rmse, aic\n\n    all_results = []\n    \n    model_order = [\"EAM\", \"MEAM\", \"ML\"]\n\n    for case in test_cases:\n        dft = case[\"dft\"]\n        n = case[\"n\"]\n        \n        case_rmses = []\n        case_aics = []\n\n        for model_name in model_order:\n            model_energies = case[\"models\"][model_name]\n            k = k_vals[model_name]\n            \n            rmse, aic = calculate_metrics(dft, model_energies, k)\n            \n            # Round as per specifications\n            rounded_rmse = round(rmse, 6)\n            rounded_aic = round(aic, 3)\n            \n            case_rmses.append(rounded_rmse)\n            case_aics.append(rounded_aic)\n\n        # Determine best model indices\n        best_rmse_idx = np.argmin(case_rmses)\n        best_aic_idx = np.argmin(case_aics)\n        \n        # Append results for the current case to the master list\n        all_results.extend(case_rmses)\n        all_results.extend(case_aics)\n        all_results.append(best_rmse_idx)\n        all_results.append(best_aic_idx)\n\n    # Format the final output string as a comma-separated list in brackets\n    # Using a custom mapping to handle float formatting without trailing zeros for integers\n    def format_number(num):\n        if isinstance(num, int):\n            return str(num)\n        # Check if float is whole number, e.g., 107.039 vs 110.540\n        if isinstance(num, float) and num.is_integer():\n             return f\"{num:.1f}\" # Avoids scientific notation in some cases\n        return str(num)\n\n    # Apply specific formatting for floats as needed, stringify integers\n    formatted_results = []\n    for val in all_results:\n        if isinstance(val, float):\n             # Ensure correct number of decimal places for RMSE and AIC\n            if abs(val) > 1e-4: # Heuristic to differentiate RMSE and AIC\n                # This could be AIC or a large RMSE\n                formatted_results.append(f\"{val:.3f}\")\n            else:\n                 # This is likely a small RMSE\n                formatted_results.append(f\"{val:.6f}\")\n        else:\n            formatted_results.append(str(val))\n\n\n    # This logic is a bit tricky. Let's make it more robust.\n    # 8 items per case: 3 RMSE, 3 AIC, 2 indices.\n    final_output_list = []\n    for i in range(len(test_cases)):\n        base_idx = i * 8\n        # RMSEs (3 items)\n        final_output_list.extend([f\"{val:.6f}\" for val in all_results[base_idx:base_idx+3]])\n        # AICs (3 items)\n        final_output_list.extend([f\"{val:.3f}\" for val in all_results[base_idx+3:base_idx+6]])\n        # Indices (2 items)\n        final_output_list.extend([str(val) for val in all_results[base_idx+6:base_idx+8]])\n        \n\n    print(f\"[{','.join(final_output_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond evaluating existing potentials, a common task is to develop custom surrogate models for specific material systems. This exercise guides you through calibrating a simplified model for grain boundary energy against high-fidelity DFT data using weighted ridge regression . Critically, it introduces the Mahalanobis distance as a powerful statistical tool to assess the model's extrapolation risk, helping you to quantify the domain of your model's applicability.",
            "id": "3455300",
            "problem": "You are given a calibration task for a surrogate interatomic-potential model that predicts grain boundary energy for a boundary characterized by five degrees of freedom. The five degrees of freedom are encoded as angles in radians: misorientation angles and boundary plane orientations. The model predicts the grain boundary energy using a linear-in-parameters basis on trigonometric functions of the angles. All energies are in $\\mathrm{J}/\\mathrm{m}^2$, and all angles are in radians. The loss function to be minimized is the weighted sum of squared differences between the model predictions and Density Functional Theory (DFT) computed energies. Your goal is to implement the following, entirely in code, and solve the specified test suite.\n\nFundamental setup: A grain boundary configuration is parameterized by a tuple of five angles $(\\phi_k,\\psi_k,\\omega_k,\\alpha_k,\\beta_k)$, all in radians. Define a feature vector of dimension $5$ for each configuration $k$ as\n$$\n\\mathbf{f}_k = \\begin{bmatrix}\n1 \\\\\n\\cos(\\omega_k) \\\\\n\\sin(\\omega_k)\\cos(\\phi_k) \\\\\n\\cos(\\alpha_k - \\phi_k) \\\\\n\\cos(\\beta_k - \\psi_k)\n\\end{bmatrix}.\n$$\nLet the model energy be\n$$\n\\gamma_k^{\\mathrm{model}} = \\mathbf{f}_k^\\top \\mathbf{a},\n$$\nwhere $\\mathbf{a}\\in\\mathbb{R}^5$ is the parameter vector to calibrate. The DFT energies $\\gamma_k^{\\mathrm{DFT}}$ and weights $w_k$ are given. The weighted loss to minimize is\n$$\nL(\\mathbf{a}) = \\sum_{k=1}^{K} w_k\\left(\\gamma_k^{\\mathrm{model}} - \\gamma_k^{\\mathrm{DFT}}\\right)^2,\n$$\nwith $K$ training points. Optionally, a Tikhonov regularization parameter $\\lambda \\ge 0$ may be introduced to stabilize the calibration by augmenting the loss with $\\lambda\\|\\mathbf{a}\\|_2^2$.\n\nExtrapolation assessment: After calibrating $\\mathbf{a}$, quantify extrapolation risk for query configurations by the Mahalanobis distance in feature space using the weighted training feature distribution. Compute the weighted mean\n$$\n\\boldsymbol{\\mu} = \\left(\\sum_{k=1}^{K} w_k\\right)^{-1}\\sum_{k=1}^{K}w_k\\,\\mathbf{f}_k,\n$$\nand the weighted covariance\n$$\n\\mathbf{S} = \\left(\\sum_{k=1}^{K} w_k\\right)^{-1}\\sum_{k=1}^{K}w_k\\,(\\mathbf{f}_k - \\boldsymbol{\\mu})(\\mathbf{f}_k - \\boldsymbol{\\mu})^\\top,\n$$\nwhich should be regularized as needed for numerical stability. For a query feature $\\mathbf{f}$, define the Mahalanobis distance\n$$\nd_M(\\mathbf{f}) = \\sqrt{(\\mathbf{f} - \\boldsymbol{\\mu})^\\top \\mathbf{S}^{-1} (\\mathbf{f} - \\boldsymbol{\\mu})}.\n$$\nGiven a threshold $\\tau > 0$, classify extrapolation as true if $d_M(\\mathbf{f}) > \\tau$, and false otherwise.\n\nTraining dataset: Use $K=10$ training boundaries with tuples $(\\phi_k,\\psi_k,\\omega_k,\\alpha_k,\\beta_k)$, weights $w_k$, and DFT energies $\\gamma_k^{\\mathrm{DFT}}$ listed below. Every angle is in radians, given by simple rational multiples of $\\pi$, and every energy is in $\\mathrm{J}/\\mathrm{m}^2$.\n\n- $k=1$: $(\\phi_1,\\psi_1,\\omega_1,\\alpha_1,\\beta_1) = \\left(0,\\frac{\\pi}{6},\\frac{\\pi}{3},\\frac{\\pi}{4},\\frac{\\pi}{2}\\right)$, $w_1 = 1.0$, $\\gamma_1^{\\mathrm{DFT}} = 0.747785391$.\n- $k=2$: $(\\phi_2,\\psi_2,\\omega_2,\\alpha_2,\\beta_2) = \\left(\\frac{\\pi}{6},\\frac{\\pi}{3},\\frac{\\pi}{2},\\frac{\\pi}{3},\\frac{\\pi}{6}\\right)$, $w_2 = 2.0$, $\\gamma_2^{\\mathrm{DFT}} = 0.700000000$.\n- $k=3$: $(\\phi_3,\\psi_3,\\omega_3,\\alpha_3,\\beta_3) = \\left(\\frac{\\pi}{4},\\frac{\\pi}{4},\\frac{\\pi}{4},\\frac{\\pi}{2},\\frac{\\pi}{4}\\right)$, $w_3 = 1.5$, $\\gamma_3^{\\mathrm{DFT}} = 0.773137085$.\n- $k=4$: $(\\phi_4,\\psi_4,\\omega_4,\\alpha_4,\\beta_4) = \\left(\\frac{\\pi}{3},\\frac{\\pi}{2},\\frac{\\pi}{6},0,\\frac{2\\pi}{3}\\right)$, $w_4 = 1.0$, $\\gamma_4^{\\mathrm{DFT}} = 0.789282032$.\n- $k=5$: $(\\phi_5,\\psi_5,\\omega_5,\\alpha_5,\\beta_5) = \\left(0,\\frac{\\pi}{2},\\frac{2\\pi}{3},\\frac{5\\pi}{6},\\frac{\\pi}{3}\\right)$, $w_5 = 0.7$, $\\gamma_5^{\\mathrm{DFT}} = 0.546076952$.\n- $k=6$: $(\\phi_6,\\psi_6,\\omega_6,\\alpha_6,\\beta_6) = \\left(\\frac{\\pi}{2},0,\\pi,\\frac{\\pi}{2},\\pi\\right)$, $w_6 = 2.0$, $\\gamma_6^{\\mathrm{DFT}} = 0.680000000$.\n- $k=7$: $(\\phi_7,\\psi_7,\\omega_7,\\alpha_7,\\beta_7) = \\left(\\frac{3\\pi}{4},\\frac{\\pi}{4},\\frac{\\pi}{3},\\frac{\\pi}{3},\\frac{\\pi}{4}\\right)$, $w_7 = 1.2$, $\\gamma_7^{\\mathrm{DFT}} = 0.770024040$.\n- $k=8$: $(\\phi_8,\\psi_8,\\omega_8,\\alpha_8,\\beta_8) = \\left(\\frac{\\pi}{6},\\frac{5\\pi}{6},\\frac{\\pi}{6},\\frac{\\pi}{6},\\frac{5\\pi}{6}\\right)$, $w_8 = 1.0$, $\\gamma_8^{\\mathrm{DFT}} = 0.809282032$.\n- $k=9$: $(\\phi_9,\\psi_9,\\omega_9,\\alpha_9,\\beta_9) = \\left(\\frac{\\pi}{2},\\frac{\\pi}{2},\\frac{\\pi}{4},\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)$, $w_9 = 0.8$, $\\gamma_9^{\\mathrm{DFT}} = 0.810710678$.\n- $k=10$: $(\\phi_{10},\\psi_{10},\\omega_{10},\\alpha_{10},\\beta_{10}) = \\left(\\frac{\\pi}{3},\\frac{2\\pi}{3},\\frac{5\\pi}{6},\\frac{\\pi}{2},\\frac{\\pi}{6}\\right)$, $w_{10} = 1.3$, $\\gamma_{10}^{\\mathrm{DFT}} = 0.655358984$.\n\nCalibration and extrapolation tasks: Implement weighted ridge regression to estimate $\\mathbf{a}$ by minimizing $L(\\mathbf{a}) + \\lambda\\|\\mathbf{a}\\|_2^2$, where $\\lambda$ is specified per test case. Then compute the training loss $L(\\hat{\\mathbf{a}})$, with $\\hat{\\mathbf{a}}$ the minimizer. Use the training features to compute $\\boldsymbol{\\mu}$ and $\\mathbf{S}$, regularizing $\\mathbf{S}$ by adding a small diagonal term if needed to ensure invertibility. For each query configuration, compute the feature vector, the Mahalanobis distance $d_M$, and the extrapolation flag relative to a threshold $\\tau$.\n\nTest suite: Two test cases are to be executed.\n\n- Test case $1$:\n  - Regularization $\\lambda = 0.0$.\n  - Threshold $\\tau = 2.0$.\n  - Queries:\n    - $Q_{1}$: $(\\phi,\\psi,\\omega,\\alpha,\\beta) = \\left(\\frac{\\pi}{4},\\frac{\\pi}{4},\\frac{\\pi}{3},\\frac{\\pi}{6},\\frac{\\pi}{6}\\right)$.\n    - $Q_{2}$: $(\\phi,\\psi,\\omega,\\alpha,\\beta) = \\left(0,0,0,\\pi,\\pi\\right)$.\n- Test case $2$:\n  - Regularization $\\lambda = 0.000001$.\n  - Threshold $\\tau = 1.0$.\n  - Queries:\n    - $Q_{3}$: $(\\phi,\\psi,\\omega,\\alpha,\\beta) = \\left(\\frac{\\pi}{2},\\frac{\\pi}{2},\\frac{\\pi}{2},\\frac{\\pi}{2},\\frac{\\pi}{2}\\right)$.\n    - $Q_{4}$: $(\\phi,\\psi,\\omega,\\alpha,\\beta) = \\left(\\pi,\\pi,\\pi,0,0\\right)$.\n\nOutput specification: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be\n$$\n\\left[ L_1, d_{M}(Q_1), \\mathrm{flag}(Q_1), d_{M}(Q_2), \\mathrm{flag}(Q_2), L_2, d_{M}(Q_3), \\mathrm{flag}(Q_3), d_{M}(Q_4), \\mathrm{flag}(Q_4) \\right],\n$$\nwhere $L_1$ and $L_2$ are the training losses in $(\\mathrm{J}/\\mathrm{m}^2)^2$, each float rounded to six decimal places; $d_M(\\cdot)$ are Mahalanobis distances, each float rounded to six decimal places; and $\\mathrm{flag}(\\cdot)$ are booleans computed as $d_M(\\cdot)>\\tau$ for the corresponding test case. For example, the output format must look like $[0.123456,1.234567,True,2.345678,False,0.234567,1.111111,False,3.222222,True]$.",
            "solution": "The problem requires the implementation of a computational workflow for calibrating a surrogate model for grain boundary energy, followed by an assessment of the model's extrapolation risk for new configurations. The process involves two main parts: first, a parameter-fitting step using weighted ridge regression; second, the calculation of the Mahalanobis distance in a feature space to quantify extrapolation.\n\nThe problem is determined to be valid as it is scientifically grounded in standard practices of computational materials science and machine learning, is mathematically well-posed, and provides a complete and consistent set of data and instructions.\n\nThe core of the problem lies in linear algebra and statistical analysis. We will formalize the procedure step-by-step.\n\nA grain boundary is defined by a set of five angles $(\\phi_k, \\psi_k, \\omega_k, \\alpha_k, \\beta_k)$. For each configuration $k$, we construct a $5$-dimensional feature vector $\\mathbf{f}_k$:\n$$\n\\mathbf{f}_k = \\begin{bmatrix}\n1 \\\\\n\\cos(\\omega_k) \\\\\n\\sin(\\omega_k)\\cos(\\phi_k) \\\\\n\\cos(\\alpha_k - \\phi_k) \\\\\n\\cos(\\beta_k - \\psi_k)\n\\end{bmatrix}\n$$\nThe model for the grain boundary energy $\\gamma_k^{\\mathrm{model}}$ is linear in a parameter vector $\\mathbf{a} \\in \\mathbb{R}^5$:\n$$\n\\gamma_k^{\\mathrm{model}} = \\mathbf{f}_k^\\top \\mathbf{a} = \\sum_{j=1}^{5} f_{kj} a_j\n$$\n\nThe first task is to find the optimal parameter vector $\\hat{\\mathbf{a}}$ by minimizing a weighted and regularized loss function. The loss function is the sum of squared errors between model predictions $\\gamma_k^{\\mathrm{model}}$ and reference Density Functional Theory (DFT) energies $\\gamma_k^{\\mathrm{DFT}}$, weighted by $w_k$, with an added Tikhonov regularization term:\n$$\nL_{reg}(\\mathbf{a}) = \\sum_{k=1}^{K} w_k \\left( \\gamma_k^{\\mathrm{model}} - \\gamma_k^{\\mathrm{DFT}} \\right)^2 + \\lambda \\|\\mathbf{a}\\|_2^2\n$$\nwhere $K=10$ is the number of training points and $\\lambda \\ge 0$ is the regularization parameter.\n\nTo solve this minimization problem, we express it in matrix form. Let $F$ be the $K \\times 5$ design matrix where each row is a feature vector $\\mathbf{f}_k^\\top$. Let $\\mathbf{y}$ be the $K \\times 1$ column vector of DFT energies $\\gamma_k^{\\mathrm{DFT}}$. Let $W$ be the $K \\times K$ diagonal matrix with the weights $w_k$ on its diagonal. The loss function becomes:\n$$\nL_{reg}(\\mathbf{a}) = (\\mathbf{y} - F\\mathbf{a})^\\top W (\\mathbf{y} - F\\mathbf{a}) + \\lambda \\mathbf{a}^\\top \\mathbf{a}\n$$\nThis is a quadratic function of $\\mathbf{a}$. To find the minimum, we take the gradient with respect to $\\mathbf{a}$ and set it to zero:\n$$\n\\nabla_{\\mathbf{a}} L_{reg}(\\mathbf{a}) = -2 F^\\top W (\\mathbf{y} - F\\mathbf{a}) + 2 \\lambda \\mathbf{a} = 0\n$$\nRearranging the terms, we get a linear system of equations for the optimal parameter vector $\\hat{\\mathbf{a}}$:\n$$\n(F^\\top W F + \\lambda I) \\hat{\\mathbf{a}} = F^\\top W \\mathbf{y}\n$$\nwhere $I$ is the $5 \\times 5$ identity matrix. The solution is:\n$$\n\\hat{\\mathbf{a}} = (F^\\top W F + \\lambda I)^{-1} F^\\top W \\mathbf{y}\n$$\nThis system can be solved efficiently using standard linear algebra libraries. The matrix $(F^\\top W F + \\lambda I)$ is guaranteed to be invertible if $\\lambda > 0$, as $F^\\top W F$ is positive semi-definite. For $\\lambda = 0$, invertibility requires the columns of $F$ to be linearly independent.\n\nOnce $\\hat{\\mathbf{a}}$ is determined, the training loss $L(\\hat{\\mathbf{a}})$ is calculated using the unregularized loss formula:\n$$\nL(\\hat{\\mathbf{a}}) = \\sum_{k=1}^{K} w_k \\left( \\mathbf{f}_k^\\top \\hat{\\mathbf{a}} - \\gamma_k^{\\mathrm{DFT}} \\right)^2 = (\\mathbf{y} - F\\hat{\\mathbf{a}})^\\top W (\\mathbf{y} - F\\hat{\\mathbf{a}})\n$$\n\nThe second part of the problem is to assess extrapolation risk using the Mahalanobis distance. This requires characterizing the distribution of the training data in the feature space. We compute the weighted mean vector $\\boldsymbol{\\mu}$ and the weighted covariance matrix $\\mathbf{S}$ of the training feature vectors $\\mathbf{f}_k$:\n$$\nW_{sum} = \\sum_{k=1}^{K} w_k\n$$\n$$\n\\boldsymbol{\\mu} = \\frac{1}{W_{sum}} \\sum_{k=1}^{K} w_k \\mathbf{f}_k\n$$\n$$\n\\mathbf{S} = \\frac{1}{W_{sum}} \\sum_{k=1}^{K} w_k (\\mathbf{f}_k - \\boldsymbol{\\mu})(\\mathbf{f}_k - \\boldsymbol{\\mu})^\\top\n$$\nThe covariance matrix $\\mathbf{S}$ describes the shape and orientation of the training data cloud in the $5$-dimensional feature space. To ensure numerical stability during inversion, $\\mathbf{S}$ may be regularized by adding a small multiple of the identity matrix, $\\epsilon I$.\n\nFor a new query configuration with feature vector $\\mathbf{f}_{query}$, the Mahalanobis distance $d_M$ from the center of the training data distribution is:\n$$\nd_M(\\mathbf{f}_{query}) = \\sqrt{(\\mathbf{f}_{query} - \\boldsymbol{\\mu})^\\top \\mathbf{S}^{-1} (\\mathbf{f}_{query} - \\boldsymbol{\\mu})}\n$$\nThis distance measures how many standard deviations away the query point is from the mean of the training data, accounting for the covariance structure. A large $d_M$ suggests that the query point lies in a region of the feature space that was sparsely sampled by the training data, indicating a high risk of extrapolation. The query is flagged as an extrapolation if $d_M$ exceeds a given threshold $\\tau$.\n\nThe implementation will proceed as follows:\n1.  Define the training data (angles, energies, weights).\n2.  For each test case (defined by $\\lambda$ and $\\tau$):\n    a. Construct the design matrix $F$, the target vector $\\mathbf{y}$, and the weight vector (for efficient computation in place of the matrix $W$).\n    b. Solve the weighted ridge regression system for $\\hat{\\mathbf{a}}$.\n    c. Calculate the training loss $L(\\hat{\\mathbf{a}})$.\n    d. Compute the weighted mean $\\boldsymbol{\\mu}$ and weighted covariance matrix $\\mathbf{S}$ of the training features. Regularize and invert $\\mathbf{S}$ to get $\\mathbf{S}^{-1}$. Note that $\\boldsymbol{\\mu}$ and $\\mathbf{S}$ are independent of $\\lambda$ and only need to be computed once.\n    e. For each query configuration in the test case:\n        i. Compute its feature vector $\\mathbf{f}_{query}$.\n        ii. Calculate the Mahalanobis distance $d_M(\\mathbf{f}_{query})$.\n        iii. Compare $d_M$ to $\\tau$ to set the boolean extrapolation flag.\n3.  Collect and format all results as specified in the problem statement.\nAll calculations will be performed using the `numpy` library for numerical and linear algebra operations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the grain boundary energy model calibration and extrapolation assessment problem.\n    \"\"\"\n    pi = np.pi\n    \n    # Training dataset (K=10)\n    # Each entry: (phi, psi, omega, alpha, beta), weight, dft_energy\n    training_data = [\n        ((0, pi/6, pi/3, pi/4, pi/2), 1.0, 0.747785391),\n        ((pi/6, pi/3, pi/2, pi/3, pi/6), 2.0, 0.700000000),\n        ((pi/4, pi/4, pi/4, pi/2, pi/4), 1.5, 0.773137085),\n        ((pi/3, pi/2, pi/6, 0, 2*pi/3), 1.0, 0.789282032),\n        ((0, pi/2, 2*pi/3, 5*pi/6, pi/3), 0.7, 0.546076952),\n        ((pi/2, 0, pi, pi/2, pi), 2.0, 0.680000000),\n        ((3*pi/4, pi/4, pi/3, pi/3, pi/4), 1.2, 0.770024040),\n        ((pi/6, 5*pi/6, pi/6, pi/6, 5*pi/6), 1.0, 0.809282032),\n        ((pi/2, pi/2, pi/4, pi/2, pi/2), 0.8, 0.810710678),\n        ((pi/3, 2*pi/3, 5*pi/6, pi/2, pi/6), 1.3, 0.655358984),\n    ]\n\n    # Test suite\n    test_cases = [\n        {\n            \"lambda\": 0.0,\n            \"tau\": 2.0,\n            \"queries\": [\n                (pi/4, pi/4, pi/3, pi/6, pi/6),\n                (0, 0, 0, pi, pi)\n            ]\n        },\n        {\n            \"lambda\": 0.000001,\n            \"tau\": 1.0,\n            \"queries\": [\n                (pi/2, pi/2, pi/2, pi/2, pi/2),\n                (pi, pi, pi, 0, 0)\n            ]\n        }\n    ]\n\n    def get_feature_vector(angles):\n        \"\"\"Computes the feature vector for a given set of angles.\"\"\"\n        phi, psi, omega, alpha, beta = angles\n        return np.array([\n            1,\n            np.cos(omega),\n            np.sin(omega) * np.cos(phi),\n            np.cos(alpha - phi),\n            np.cos(beta - psi)\n        ])\n\n    # Prepare training data matrices\n    angles_train = [d[0] for d in training_data]\n    weights_train = np.array([d[1] for d in training_data])\n    y_train = np.array([d[2] for d in training_data])\n    \n    F_train = np.array([get_feature_vector(angs) for angs in angles_train])\n    \n    # --- Pre-calculate stats for Mahalanobis distance (independent of test cases) ---\n    \n    # Weighted mean feature vector mu\n    W_sum = np.sum(weights_train)\n    mu = np.sum(weights_train[:, np.newaxis] * F_train, axis=0) / W_sum\n    \n    # Weighted covariance matrix S\n    F_centered = F_train - mu\n    S = (F_centered.T @ (weights_train[:, np.newaxis] * F_centered)) / W_sum\n    \n    # Regularize and invert S\n    S_reg = S + 1e-10 * np.identity(S.shape[0])\n    try:\n        S_inv = np.linalg.inv(S_reg)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse if still singular, though regularization should prevent this.\n        S_inv = np.linalg.pinv(S_reg)\n\n    all_results = []\n    \n    for case in test_cases:\n        lambda_reg = case[\"lambda\"]\n        tau = case[\"tau\"]\n        \n        # --- 1. Weighted Ridge Regression ---\n        F_T_W = F_train.T * weights_train # Efficient F.T @ diag(W)\n        A = F_T_W @ F_train + lambda_reg * np.identity(F_train.shape[1])\n        b = F_T_W @ y_train\n        \n        a_hat = np.linalg.solve(A, b)\n        \n        # --- 2. Training Loss Calculation ---\n        y_pred = F_train @ a_hat\n        errors = y_train - y_pred\n        loss = np.sum(weights_train * (errors ** 2))\n        all_results.append(round(loss, 6))\n\n        # --- 3. Extrapolation Assessment for Queries ---\n        for query_angles in case[\"queries\"]:\n            f_query = get_feature_vector(query_angles)\n            f_centered_query = f_query - mu\n            \n            # Mahalanobis distance\n            dM_sq = f_centered_query.T @ S_inv @ f_centered_query\n            dM = np.sqrt(dM_sq)\n            \n            # Extrapolation flag\n            extrap_flag = dM > tau\n            \n            all_results.append(round(dM, 6))\n            all_results.append(extrap_flag)\n            \n    # Format and print the final output\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A central challenge in materials science is to synthesize information from disparate sources—such as first-principles calculations, classical simulations, and experimental measurements—into a single, robust conclusion. This exercise tackles this head-on by having you reconcile grain boundary energy estimates from DFT, classical potentials, and thermal grooving experiments . You will apply a random-effects meta-analysis, a sophisticated statistical technique that accounts for both within-method and between-method uncertainties to produce a consolidated value and a credible confidence interval.",
            "id": "3455409",
            "problem": "You are tasked with developing a complete, runnable program that estimates the grain boundary energy for a face-centered cubic Nickel (Ni) $\\,\\Sigma 3\\,$ coherent twin boundary using two Density Functional Theory (DFT) functionals and two classical interatomic potentials, quantifies uncertainties from the underlying energetic inputs, and reconciles these calculations with thermal-grooving measurements via a principled model-averaging procedure. The program must start from fundamental definitions and well-tested physical relationships to derive the necessary expressions. The final outputs must be expressed in millijoules per square meter (mJ/m$^2$).\n\nThe computational scenario is as follows:\n- The grain boundary energy is defined as an interfacial excess free energy per unit area. In a periodic bicrystal supercell that contains two identical grain boundaries, the interface excess energy is obtained from the difference between the total energy of the bicrystal and the energy of the equivalent number of atoms in the bulk reference, divided by the total interfacial area and the number of identical interfaces.\n- The thermal grooving measurement at a free surface that intersects a grain boundary yields a geometric equilibrium condition that relates the grain boundary energy to the surface energy of Nickel and the dihedral angle at the groove root. The angle unit is specified in degrees.\n\nYour program must:\n- Derive the expressions needed to compute the grain boundary energy for each method and the propagation of uncertainties from the given energetic inputs.\n- Derive the relation needed to compute the grain boundary energy from thermal grooving data and propagate its uncertainties with respect to both the surface energy and the measured angle.\n- Reconcile all estimates using a statistically principled weighted average that accounts for both within-method uncertainty and potential between-method dispersion.\n\nAll inputs are provided as physically plausible, internally consistent values. For each test case, you are given the bicrystal area in square angstroms, the number of atoms in the bicrystal, and the energy data for two Density Functional Theory (DFT) functionals—Perdew–Burke–Ernzerhof (PBE) and Strongly Constrained and Appropriately Normed (SCAN)—and two classical potentials—Embedded Atom Method (EAM) and Modified Embedded Atom Method (MEAM). Each method includes total bicrystal energy, bulk per-atom energy (to define the bulk reference), and uncertainties for these energies. The thermal grooving dataset consists of the Nickel surface energy, its uncertainty, the measured dihedral angle at the groove intersection in degrees, and its uncertainty in degrees. The unit for the final grain boundary energies must be millijoules per square meter (mJ/m$^2$).\n\nAngle units must be treated in degrees. Your program must compute and output only the requested values in the specified format.\n\nUse the following test suite of three parameter sets:\n\n- Test case $1$:\n  - Bicrystal geometry: area $A = 800$ $\\text{\\AA}^2$, atoms $N = 480$.\n  - DFT (PBE): $E_{\\text{bulk,atom}} = -4.30$ $\\text{eV}$, $E_{\\text{cell}} = -2060.8064$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.015$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0001$ $\\text{eV}$.\n  - DFT (SCAN): $E_{\\text{bulk,atom}} = -4.45$ $\\text{eV}$, $E_{\\text{cell}} = -2132.5072$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.020$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0001$ $\\text{eV}$.\n  - EAM: $E_{\\text{bulk,atom}} = -4.44$ $\\text{eV}$, $E_{\\text{cell}} = -2128.4080$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.050$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0010$ $\\text{eV}$.\n  - MEAM: $E_{\\text{bulk,atom}} = -4.42$ $\\text{eV}$, $E_{\\text{cell}} = -2118.3072$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.050$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0010$ $\\text{eV}$.\n  - Thermal grooving: $\\,\\gamma_s = 1800$ $\\text{mJ/m}^2$, $\\sigma_{\\gamma_s} = 50$ $\\text{mJ/m}^2$, $\\alpha = 178.9$ degrees, $\\sigma_{\\alpha} = 0.1$ degrees.\n\n- Test case $2$:\n  - Bicrystal geometry: area $A = 600$ $\\text{\\AA}^2$, atoms $N = 360$.\n  - DFT (PBE): $E_{\\text{bulk,atom}} = -4.30$ $\\text{eV}$, $E_{\\text{cell}} = -1546.6524$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.015$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0001$ $\\text{eV}$.\n  - DFT (SCAN): $E_{\\text{bulk,atom}} = -4.45$ $\\text{eV}$, $E_{\\text{cell}} = -1600.5024$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.020$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0001$ $\\text{eV}$.\n  - EAM: $E_{\\text{bulk,atom}} = -4.44$ $\\text{eV}$, $E_{\\text{cell}} = -1597.2768$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.050$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0010$ $\\text{eV}$.\n  - MEAM: $E_{\\text{bulk,atom}} = -4.42$ $\\text{eV}$, $E_{\\text{cell}} = -1589.5524$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.050$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0010$ $\\text{eV}$.\n  - Thermal grooving: $\\,\\gamma_s = 1800$ $\\text{mJ/m}^2$, $\\sigma_{\\gamma_s} = 50$ $\\text{mJ/m}^2$, $\\alpha = 179.5$ degrees, $\\sigma_{\\alpha} = 0.2$ degrees.\n\n- Test case $3$:\n  - Bicrystal geometry: area $A = 900$ $\\text{\\AA}^2$, atoms $N = 520$.\n  - DFT (PBE): $E_{\\text{bulk,atom}} = -4.30$ $\\text{eV}$, $E_{\\text{cell}} = -2232.2956$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.015$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0001$ $\\text{eV}$.\n  - DFT (SCAN): $E_{\\text{bulk,atom}} = -4.45$ $\\text{eV}$, $E_{\\text{cell}} = -2309.9570$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.020$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0001$ $\\text{eV}$.\n  - EAM: $E_{\\text{bulk,atom}} = -4.44$ $\\text{eV}$, $E_{\\text{cell}} = -2305.5438$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.050$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0010$ $\\text{eV}$.\n  - MEAM: $E_{\\text{bulk,atom}} = -4.42$ $\\text{eV}$, $E_{\\text{cell}} = -2294.9188$ $\\text{eV}$, $\\sigma_{E_{\\text{cell}}} = 0.050$ $\\text{eV}$, $\\sigma_{E_{\\text{bulk}}} = 0.0010$ $\\text{eV}$.\n  - Thermal grooving: $\\,\\gamma_s = 1800$ $\\text{mJ/m}^2$, $\\sigma_{\\gamma_s} = 50$ $\\text{mJ/m}^2$, $\\alpha = 178.0$ degrees, $\\sigma_{\\alpha} = 0.1$ degrees.\n\nYour program must implement the following computational steps for each test case:\n- Compute the grain boundary energy for each method from the bicrystal total energy and the bulk reference, acknowledging that the bicrystal cell contains two identical grain boundaries.\n- Propagate uncertainties from the provided energy uncertainties to the grain boundary energy uncertainty for each method.\n- Compute the grain boundary energy from thermal grooving and propagate its uncertainty with respect to both the surface energy and the angle (the angle must be treated in degrees).\n- Perform a random-effects model averaging across all five estimates (the two DFT functionals, the two classical potentials, and thermal grooving) to yield a reconciled mean and uncertainty that account for both within-method variance and between-method dispersion.\n- Express all grain boundary energies and uncertainties in mJ/m$^2$.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist of three floats in mJ/m$^2$, ordered as $[$consolidated mean$, $consolidated standard deviation$, $thermal grooving energy$]$.\n- Use degree units for all angles.\n- Round each float in the final output to one decimal place.\n- Example of the required output format with three test cases: $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3]]$ where all $x_i$, $y_i$, $z_i$ are floats in mJ/m$^2$.\n\nAll computations must be performed without reading external files and without any user input. The program must be deterministic and self-contained.",
            "solution": "The problem requires the calculation of the grain boundary energy for a $\\Sigma 3$ coherent twin boundary in Nickel (Ni) using multiple theoretical and experimental methods, and then reconciling these estimates using a statistically principled model. The solution is presented in four parts: derivation of the grain boundary energy from atomistic simulations, derivation from thermal grooving experiments, the statistical model for data reconciliation, and the necessary unit conversions.\n\nThe following physical constants and conversion factors will be used:\n- Elementary charge, $e = 1.602176634 \\times 10^{-19}$ C.\n- $1$ eV $= 1.602176634 \\times 10^{-19}$ J.\n- $1$ Angstrom ($\\text{\\AA}$) = $10^{-10}$ m.\n\n### 1. Grain Boundary Energy from Atomistic Simulations\n\nThe grain boundary energy, $\\gamma_{gb}$, is defined as the excess free energy per unit area of the interface. For a periodic bicrystal supercell of cross-sectional area $A$ containing $N$ atoms and two identical grain boundaries, $\\gamma_{gb}$ is computed by subtracting the energy of an equivalent number of atoms in a perfect bulk crystal ($N E_{\\text{bulk,atom}}$) from the total energy of the supercell ($E_{\\text{cell}}$), and dividing by the total area of the two interfaces ($2A$).\n\n$$ \\gamma_{gb} = \\frac{E_{\\text{cell}} - N E_{\\text{bulk,atom}}}{2A} $$\n\nHere, $E_{\\text{bulk,atom}}$ is the energy per atom in the bulk reference state.\n\nThe uncertainty in $\\gamma_{gb}$ is determined by propagating the uncertainties of the input energies, $\\sigma_{E_{\\text{cell}}}$ and $\\sigma_{E_{\\text{bulk,atom}}}$. Assuming these uncertainties are independent, the variance of $\\gamma_{gb}$ is given by the standard error propagation formula:\n\n$$ \\sigma_{\\gamma_{gb}}^2 = \\left(\\frac{\\partial \\gamma_{gb}}{\\partial E_{\\text{cell}}}\\right)^2 \\sigma_{E_{\\text{cell}}}^2 + \\left(\\frac{\\partial \\gamma_{gb}}{\\partial E_{\\text{bulk,atom}}}\\right)^2 \\sigma_{E_{\\text{bulk,atom}}}^2 $$\n\nThe partial derivatives are:\n$$ \\frac{\\partial \\gamma_{gb}}{\\partial E_{\\text{cell}}} = \\frac{1}{2A} $$\n$$ \\frac{\\partial \\gamma_{gb}}{\\partial (N E_{\\text{bulk,atom}})} = -\\frac{1}{2A} \\implies \\frac{\\partial \\gamma_{gb}}{\\partial E_{\\text{bulk,atom}}} = -\\frac{N}{2A} $$\n\nSubstituting these into the variance equation:\n$$ \\sigma_{\\gamma_{gb}}^2 = \\left(\\frac{1}{2A}\\right)^2 \\sigma_{E_{\\text{cell}}}^2 + \\left(-\\frac{N}{2A}\\right)^2 \\sigma_{E_{\\text{bulk,atom}}}^2 $$\n$$ \\sigma_{\\gamma_{gb}}^2 = \\frac{\\sigma_{E_{\\text{cell}}}^2 + N^2 \\sigma_{E_{\\text{bulk,atom}}}^2}{4A^2} $$\n\nThe standard deviation is the square root of the variance:\n$$ \\sigma_{\\gamma_{gb}} = \\frac{\\sqrt{\\sigma_{E_{\\text{cell}}}^2 + N^2 \\sigma_{E_{\\text{bulk,atom}}}^2}}{2A} $$\n\nThese formulas are applied to the data from the two Density Functional Theory (DFT) functionals (PBE, SCAN) and the two classical interatomic potentials (EAM, MEAM).\n\n### 2. Grain Boundary Energy from Thermal Grooving\n\nThe thermal grooving method relates the grain boundary energy $\\gamma_{gb}$ to the surface energy $\\gamma_s$ and the equilibrium dihedral angle $\\alpha$ at the root of the groove where the grain boundary intersects the free surface. The relationship is derived from the balance of interfacial tension forces at the triple line, known as the Young-Dupré equation for a symmetric boundary:\n\n$$ \\gamma_{gb} = 2 \\gamma_s \\cos\\left(\\frac{\\alpha}{2}\\right) $$\n\nThe angle $\\alpha$ is given in degrees and must be converted to radians for use in trigonometric functions.\n\nThe uncertainty in $\\gamma_{gb}$ from this method is found by propagating the uncertainties in $\\gamma_s$ and $\\alpha$, $\\sigma_{\\gamma_s}$ and $\\sigma_{\\alpha}$, respectively. Assuming these are independent, the variance $\\sigma_{\\gamma_{gb}}^2$ is:\n\n$$ \\sigma_{\\gamma_{gb}}^2 = \\left(\\frac{\\partial \\gamma_{gb}}{\\partial \\gamma_s}\\right)^2 \\sigma_{\\gamma_s}^2 + \\left(\\frac{\\partial \\gamma_{gb}}{\\partial \\alpha}\\right)^2 \\sigma_{\\alpha}^2 $$\n\nThe partial derivatives are:\n$$ \\frac{\\partial \\gamma_{gb}}{\\partial \\gamma_s} = 2 \\cos\\left(\\frac{\\alpha_{rad}}{2}\\right) $$\n$$ \\frac{\\partial \\gamma_{gb}}{\\partial \\alpha} = 2 \\gamma_s \\left(-\\sin\\left(\\frac{\\alpha_{rad}}{2}\\right) \\cdot \\frac{1}{2}\\right) = -\\gamma_s \\sin\\left(\\frac{\\alpha_{rad}}{2}\\right) $$\n\nFor the derivative with respect to $\\alpha$, the angle and its uncertainty must be in radians. Let $\\alpha_{rad}$ and $\\sigma_{\\alpha,rad}$ be the angle and its uncertainty in radians. The conversion is $\\sigma_{\\alpha,rad} = \\sigma_{\\alpha,deg} \\cdot \\frac{\\pi}{180}$. The variance is thus:\n\n$$ \\sigma_{\\gamma_{gb}}^2 = \\left[2 \\cos\\left(\\frac{\\alpha_{rad}}{2}\\right)\\right]^2 \\sigma_{\\gamma_s}^2 + \\left[-\\gamma_s \\sin\\left(\\frac{\\alpha_{rad}}{2}\\right)\\right]^2 \\sigma_{\\alpha,rad}^2 $$\n\n### 3. Statistical Reconciliation via Random-Effects Model\n\nWe have $k=5$ estimates of $\\gamma_{gb}$, denoted $y_i$, each with a corresponding variance $v_i = \\sigma_i^2$, for $i=1, \\dots, k$. These estimates are derived from different physical models and an experiment, so systematic differences between them (heterogeneity) are expected in addition to the statistical uncertainty within each estimate. A random-effects meta-analysis model is appropriate for combining these results. This model assumes that each study estimates a slightly different true value $\\mu_i$, where the $\\mu_i$ are drawn from a common distribution with mean $\\mu$ and variance $\\tau^2$. The parameter $\\tau^2$ represents the between-method variance (or dispersion).\n\nThe procedure is as follows:\n1.  **Estimate Between-Method Variance ($\\tau^2$)**: The DerSimonian-Laird method is used. First, we compute Cochran's $Q$ statistic, which measures the total weighted variation:\n    $$ Q = \\sum_{i=1}^{k} w_i^{FE}(y_i - \\hat{\\mu}_{FE})^2 $$\n    where $w_i^{FE} = 1/v_i$ are the fixed-effect weights and $\\hat{\\mu}_{FE} = \\frac{\\sum w_i^{FE}y_i}{\\sum w_i^{FE}}$ is the fixed-effect mean.\n    The estimator for $\\tau^2$ is:\n    $$ \\hat{\\tau}^2 = \\max\\left(0, \\frac{Q - (k-1)}{C}\\right) \\quad \\text{where} \\quad C = \\sum w_i^{FE} - \\frac{\\sum (w_i^{FE})^2}{\\sum w_i^{FE}} $$\n\n2.  **Compute Consolidated Mean and Uncertainty**: New weights (random-effects weights) are calculated that incorporate both within-method variance ($v_i$) and between-method variance ($\\hat{\\tau}^2$):\n    $$ w_i^{RE} = \\frac{1}{v_i + \\hat{\\tau}^2} $$\n    The consolidated mean (random-effects mean) is the weighted average using these new weights:\n    $$ \\hat{\\mu}_{RE} = \\frac{\\sum_{i=1}^{k} w_i^{RE} y_i}{\\sum_{i=1}^{k} w_i^{RE}} $$\n    The variance of this consolidated mean is:\n    $$ v_{\\hat{\\mu}_{RE}} = \\frac{1}{\\sum_{i=1}^{k} w_i^{RE}} $$\n    The consolidated standard deviation is $\\sigma_{\\hat{\\mu}_{RE}} = \\sqrt{v_{\\hat{\\mu}_{RE}}}$. The values $\\hat{\\mu}_{RE}$ and $\\sigma_{\\hat{\\mu}_{RE}}$ are the final reconciled estimate and its uncertainty.\n\n### 4. Unit Conversion\n\nThe atomistic calculations yield $\\gamma_{gb}$ in units of eV/$\\text{\\AA}^2$. The final results must be in millijoules per square meter (mJ/m$^2$). The conversion factor is derived as follows:\n$$ 1 \\frac{\\text{eV}}{\\text{\\AA}^2} = \\frac{1.602176634 \\times 10^{-19} \\text{ J}}{(10^{-10} \\text{ m})^2} = \\frac{1.602176634 \\times 10^{-19} \\text{ J}}{10^{-20} \\text{ m}^2} = 16.02176634 \\frac{\\text{J}}{\\text{m}^2} $$\nTo convert J/m$^2$ to mJ/m$^2$, we multiply by $1000$:\n$$ 1 \\frac{\\text{eV}}{\\text{\\AA}^2} = 16021.76634 \\frac{\\text{mJ}}{\\text{m}^2} $$\nThis conversion factor is applied to both the grain boundary energy values and their standard deviations obtained from the atomistic simulations. The thermal grooving calculation is performed using $\\gamma_s$ in mJ/m$^2$, so its results are already in the correct final unit. All five estimates are in mJ/m$^2$ before being passed to the random-effects model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the grain boundary energy problem for all test cases.\n    \"\"\"\n    \n    # Fundamental constants and conversion factors\n    EV_PER_A2_TO_MJ_PER_M2 = 16021.76634\n\n    # Test cases as provided in the problem description\n    test_cases = [\n        # Test case 1\n        {\n            \"geometry\": {\"A\": 800.0, \"N\": 480},\n            \"methods\": {\n                \"PBE\": {\"E_bulk_atom\": -4.30, \"E_cell\": -2060.8064, \"sigma_E_cell\": 0.015, \"sigma_E_bulk\": 0.0001},\n                \"SCAN\": {\"E_bulk_atom\": -4.45, \"E_cell\": -2132.5072, \"sigma_E_cell\": 0.020, \"sigma_E_bulk\": 0.0001},\n                \"EAM\": {\"E_bulk_atom\": -4.44, \"E_cell\": -2128.4080, \"sigma_E_cell\": 0.050, \"sigma_E_bulk\": 0.0010},\n                \"MEAM\": {\"E_bulk_atom\": -4.42, \"E_cell\": -2118.3072, \"sigma_E_cell\": 0.050, \"sigma_E_bulk\": 0.0010},\n            },\n            \"grooving\": {\"gamma_s\": 1800.0, \"sigma_gamma_s\": 50.0, \"alpha_deg\": 178.9, \"sigma_alpha_deg\": 0.1},\n        },\n        # Test case 2\n        {\n            \"geometry\": {\"A\": 600.0, \"N\": 360},\n            \"methods\": {\n                \"PBE\": {\"E_bulk_atom\": -4.30, \"E_cell\": -1546.6524, \"sigma_E_cell\": 0.015, \"sigma_E_bulk\": 0.0001},\n                \"SCAN\": {\"E_bulk_atom\": -4.45, \"E_cell\": -1600.5024, \"sigma_E_cell\": 0.020, \"sigma_E_bulk\": 0.0001},\n                \"EAM\": {\"E_bulk_atom\": -4.44, \"E_cell\": -1597.2768, \"sigma_E_cell\": 0.050, \"sigma_E_bulk\": 0.0010},\n                \"MEAM\": {\"E_bulk_atom\": -4.42, \"E_cell\": -1589.5524, \"sigma_E_cell\": 0.050, \"sigma_E_bulk\": 0.0010},\n            },\n            \"grooving\": {\"gamma_s\": 1800.0, \"sigma_gamma_s\": 50.0, \"alpha_deg\": 179.5, \"sigma_alpha_deg\": 0.2},\n        },\n        # Test case 3\n        {\n            \"geometry\": {\"A\": 900.0, \"N\": 520},\n            \"methods\": {\n                \"PBE\": {\"E_bulk_atom\": -4.30, \"E_cell\": -2232.2956, \"sigma_E_cell\": 0.015, \"sigma_E_bulk\": 0.0001},\n                \"SCAN\": {\"E_bulk_atom\": -4.45, \"E_cell\": -2309.9570, \"sigma_E_cell\": 0.020, \"sigma_E_bulk\": 0.0001},\n                \"EAM\": {\"E_bulk_atom\": -4.44, \"E_cell\": -2305.5438, \"sigma_E_cell\": 0.050, \"sigma_E_bulk\": 0.0010},\n                \"MEAM\": {\"E_bulk_atom\": -4.42, \"E_cell\": -2294.9188, \"sigma_E_cell\": 0.050, \"sigma_E_bulk\": 0.0010},\n            },\n            \"grooving\": {\"gamma_s\": 1800.0, \"sigma_gamma_s\": 50.0, \"alpha_deg\": 178.0, \"sigma_alpha_deg\": 0.1},\n        },\n    ]\n    \n    all_results = []\n\n    for case in test_cases:\n        estimates = []\n        uncertainties = []\n\n        # Part 1: Atomistic calculations\n        A = case[\"geometry\"][\"A\"]\n        N = case[\"geometry\"][\"N\"]\n        \n        for method_name, params in case[\"methods\"].items():\n            E_cell = params[\"E_cell\"]\n            E_bulk_atom = params[\"E_bulk_atom\"]\n            sigma_E_cell = params[\"sigma_E_cell\"]\n            sigma_E_bulk = params[\"sigma_E_bulk\"]\n\n            # Calculate gamma_gb in eV/A^2\n            gamma_gb_raw = (E_cell - N * E_bulk_atom) / (2 * A)\n            \n            # Propagate uncertainty for gamma_gb in eV/A^2\n            sigma_gamma_gb_raw = np.sqrt(sigma_E_cell**2 + (N * sigma_E_bulk)**2) / (2 * A)\n            \n            # Convert to mJ/m^2\n            estimates.append(gamma_gb_raw * EV_PER_A2_TO_MJ_PER_M2)\n            uncertainties.append(sigma_gamma_gb_raw * EV_PER_A2_TO_MJ_PER_M2)\n\n        # Part 2: Thermal grooving calculation\n        g = case[\"grooving\"]\n        gamma_s, sigma_gamma_s = g[\"gamma_s\"], g[\"sigma_gamma_s\"]\n        alpha_deg, sigma_alpha_deg = g[\"alpha_deg\"], g[\"sigma_alpha_deg\"]\n        \n        alpha_rad = np.deg2rad(alpha_deg)\n        sigma_alpha_rad = np.deg2rad(sigma_alpha_deg)\n        \n        # Calculate gamma_gb\n        gamma_gb_tg = 2 * gamma_s * np.cos(alpha_rad / 2)\n        \n        # Propagate uncertainty\n        d_gamma_s = 2 * np.cos(alpha_rad / 2)\n        d_alpha = -gamma_s * np.sin(alpha_rad / 2)\n        sigma_gamma_gb_tg = np.sqrt((d_gamma_s * sigma_gamma_s)**2 + (d_alpha * sigma_alpha_rad)**2)\n        \n        estimates.append(gamma_gb_tg)\n        uncertainties.append(sigma_gamma_gb_tg)\n        \n        # Part 3: Random-effects model averaging\n        y = np.array(estimates)\n        sigma = np.array(uncertainties)\n        v = sigma**2\n        k = len(y)\n        \n        # Fixed-effect calculations\n        w_fe = 1 / v\n        mu_fe = np.sum(w_fe * y) / np.sum(w_fe)\n        \n        # Cochran's Q and tau^2\n        q_stat = np.sum(w_fe * (y - mu_fe)**2)\n        \n        c_denom = np.sum(w_fe)\n        c_val = c_denom - (np.sum(w_fe**2) / c_denom)\n\n        if c_val > 0:\n            tau2 = max(0, (q_stat - (k - 1)) / c_val)\n        else:\n            tau2 = 0\n\n        # Random-effects calculations\n        w_re = 1 / (v + tau2)\n        mu_re = np.sum(w_re * y) / np.sum(w_re)\n        v_mu_re = 1 / np.sum(w_re)\n        sigma_mu_re = np.sqrt(v_mu_re)\n        \n        all_results.append([mu_re, sigma_mu_re, gamma_gb_tg])\n\n    # Final print statement in the exact required format.\n    output_str = \",\".join(f\"[{v[0]:.1f},{v[1]:.1f},{v[2]:.1f}]\" for v in all_results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        }
    ]
}