{
    "hands_on_practices": [
        {
            "introduction": "The first step in any high-throughput materials screening is to define the search space. This involves enumerating all plausible crystal structures that can be formed from a set of prototype lattices and elemental substitutions. This exercise introduces the fundamental combinatorial problem of decorating a crystal structure, where we treat symmetry-equivalent sites as indistinguishable. By applying a method equivalent to the classic \"stars and bars\" approach, you will develop an algorithm to count the vast number of potential compounds, a crucial skill for designing and scoping a screening project. ",
            "id": "3456708",
            "problem": "You are given prototype crystal structures and allowed element substitution sets intended for high-throughput materials screening in computational materials science. The goal is to algorithmically estimate the number of distinct decorated structures modulo space-group symmetries, assuming an automated workflow where symmetry-equivalent positions are treated as indistinguishable within a Wyckoff orbit. The derivation must start from the mathematical notion of a group action and the definition of an orbit, without using canned enumeration formulas.\n\nFundamental base to use:\n- A space group acts on the set of Wyckoff positions by permuting symmetry-equivalent sites.\n- An assignment of elements to sites can be viewed as a labeling under this action.\n- Distinct decorated structures modulo symmetry correspond to orbits of the labeling under the group action.\n\nScenario:\n- Consider the prototype set $P=\\{\\text{perovskite},\\text{spinel}\\}$.\n- For perovskite of type $ABO_3$ in space group $\\mathrm{Pm}\\bar{3}\\mathrm{m}$, assume Wyckoff orbit multiplicities $n_A=1$, $n_B=1$, $n_O=3$ for the $A$, $B$, and $O$ sublattices, respectively.\n- For spinel of type $AB_2O_4$ in space group $\\mathrm{Fd}\\bar{3}\\mathrm{m}$, assume Wyckoff orbit multiplicities $n_A=8$, $n_B=16$, $n_O=32$ for the $A$, $B$, and $O$ sublattices, respectively.\n- For each sublattice $X\\in\\{A,B,O\\}$, you are given an allowed element substitution set $E_X$ with cardinality $m_X=|E_X|$, meaning there are $m_X$ distinct element types that can occupy sites on sublattice $X$.\n\nAssumptions:\n- Within each Wyckoff orbit, space-group symmetries render positions indistinguishable, so labelings that differ only by permutations within the orbit are considered equivalent.\n- Substitutions across different sublattices are treated independently, and there are no global composition constraints beyond the orbit multiplicities $n_X$.\n\nTask:\n- Starting from the provided fundamental base, construct a principled algorithm that, for a given prototype and $m_A$, $m_B$, $m_O$, estimates the cardinality of the set of distinct decorated structures modulo space-group symmetries by counting inequivalent labelings on each orbit and aggregating over sublattices.\n- Implement this as a complete, runnable program that applies the algorithm to the test suite below and outputs the results in the specified format.\n\nTest suite:\n- Case $1$: prototype $=$ perovskite, $m_A=2$, $m_B=3$, $m_O=2$.\n- Case $2$: prototype $=$ perovskite, $m_A=1$, $m_B=1$, $m_O=1$.\n- Case $3$: prototype $=$ perovskite, $m_A=5$, $m_B=4$, $m_O=3$.\n- Case $4$: prototype $=$ spinel, $m_A=2$, $m_B=2$, $m_O=1$.\n- Case $5$: prototype $=$ spinel, $m_A=2$, $m_B=3$, $m_O=2$.\n\nAnswer specification:\n- For each case, the answer is an integer giving the estimated number of distinct decorated structures modulo space-group symmetries.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\ldots]$.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of computational materials science, specifically relating to crystallographic symmetry and high-throughput screening. It is well-posed, with all necessary parameters and assumptions clearly defined, leading to a unique and meaningful solution. The problem is objective and free of ambiguities.\n\nThe task is to derive and implement an algorithm to count the number of distinct decorated crystal structures, given a prototype structure, its Wyckoff orbit multiplicities, and a set of allowed elemental substitutions for each sublattice. The derivation must be rooted in fundamental principles.\n\nThe total number of distinct decorated structures, $N_{total}$, is the product of the number of possible unique decorations for each independent sublattice. This follows from the assumption that substitutions across different sublattices ($A$, $B$, and $O$) are treated independently.\n$$N_{total} = N_A \\cdot N_B \\cdot N_O$$\nwhere $N_X$ is the number of distinct decorations for sublattice $X$.\n\nWe must now derive a formula for $N_X$. A sublattice $X$ is defined by a single Wyckoff orbit containing $n_X$ sites. We are given a set of $m_X$ distinct element types that can be placed on these sites. The crucial assumption is that \"Within each Wyckoff orbit, space-group symmetries render positions indistinguishable, so labelings that differ only by permutations within the orbit are considered equivalent.\"\n\nThis assumption simplifies the problem significantly. Instead of considering the specific action of the space group on the set of sites, the assumption implies that any permutation of elements among the $n_X$ sites of the orbit results in the same, indistinguishable structure. This reduces the problem from a complex group-theoretical enumeration (like one requiring Burnside's Lemma or the Pólya Enumeration Theorem for the specific space group) to a fundamental combinatorial problem.\n\nWe are essentially trying to determine the number of ways to choose $n_X$ elements from a set of $m_X$ element types, with replacement, where the order of selection is irrelevant. This is a problem of counting multisets of size $n_X$ from a set of $m_X$ items.\n\nLet the $m_X$ element types be $E_1, E_2, \\dots, E_{m_X}$. A specific decoration of the sublattice $X$ is defined by the number of sites occupied by each element type. Let $k_i$ be the number of sites occupied by element type $E_i$. Since there are $n_X$ sites in total, the set of non-negative integers $\\{k_1, k_2, \\dots, k_{m_X}\\}$ must satisfy the condition:\n$$k_1 + k_2 + \\dots + k_{m_X} = n_X, \\quad k_i \\ge 0$$\nThe number of distinct decorations $N_X$ is therefore equal to the number of non-negative integer solutions to this equation.\n\nThis is a classic combinatorial problem that can be solved using the \"stars and bars\" method. Imagine we have $n_X$ indistinguishable items (stars, $*$) that we wish to place into $m_X$ distinguishable bins (the element types). We can represent this arrangement by lining up the $n_X$ stars and separating them with $m_X - 1$ dividers (bars, $|$). For example, if $n_X=5$ and $m_X=4$, the arrangement `**|*||**` corresponds to the solution $k_1=2$, $k_2=1$, $k_3=0$, and $k_4=2$.\n\nThe total number of symbols (stars and bars) is $n_X + (m_X - 1)$. Any sequence of these symbols represents a unique solution. The number of unique sequences is equivalent to choosing the positions for the $n_X$ stars from the total of $n_X + m_X - 1$ available positions. This is given by the binomial coefficient:\n$$N_X = \\binom{n_X + m_X - 1}{n_X}$$\nThis can also be written as choosing the positions for the $m_X - 1$ bars:\n$$N_X = \\binom{n_X + m_X - 1}{m_X - 1}$$\nThese two expressions are equivalent, as $\\binom{n}{k} = \\binom{n}{n-k}$.\n\nCombining this result for each sublattice, the total number of distinct decorated structures is given by the formula:\n$$N_{total} = \\binom{n_A + m_A - 1}{n_A} \\cdot \\binom{n_B + m_B - 1}{n_B} \\cdot \\binom{n_O + m_O - 1}{n_O}$$\n\nThis formula will be implemented to solve for the given test cases.\n\nThe specific parameters for the prototypes are:\n-   Perovskite ($ABO_3$): $n_A=1$, $n_B=1$, $n_O=3$.\n-   Spinel ($AB_2O_4$): $n_A=8$, $n_B=16$, $n_O=32$.\n\nFor any sublattice where the multiplicity $n_X=1$, the formula simplifies to:\n$$N_X = \\binom{1 + m_X - 1}{1} = \\binom{m_X}{1} = m_X$$\nThis is intuitive: there is only one site, so there are $m_X$ choices for what element to place there.\n\nFor any sublattice where the number of allowed element types $m_X=1$, the formula simplifies to:\n$$N_X = \\binom{n_X + 1 - 1}{n_X} = \\binom{n_X}{n_X} = 1$$\nThis is also intuitive: if only one element type is allowed, there is only one way to decorate the sublattice (by filling all sites with that element).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport math\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the number of distinct decorated crystal structures based on\n    combinatorial principles for high-throughput materials screening.\n    \"\"\"\n\n    # Define the Wyckoff orbit multiplicities for each prototype structure.\n    # n_X is the number of symmetry-equivalent sites for sublattice X.\n    prototype_params = {\n        'perovskite': {'n_A': 1, 'n_B': 1, 'n_O': 3},\n        'spinel': {'n_A': 8, 'n_B': 16, 'n_O': 32},\n    }\n\n    # Test suite provided in the problem statement.\n    # Each case is a tuple: (prototype_name, m_A, m_B, m_O)\n    # m_X is the number of allowed element types for sublattice X.\n    test_cases = [\n        ('perovskite', 2, 3, 2),\n        ('perovskite', 1, 1, 1),\n        ('perovskite', 5, 4, 3),\n        ('spinel', 2, 2, 1),\n        ('spinel', 2, 3, 2),\n    ]\n\n    results = []\n\n    def count_decorations(n, m):\n        \"\"\"\n        Calculates the number of ways to decorate a sublattice with n sites\n        using m element types.\n\n        This is a multiset combination problem, also known as \"stars and bars\".\n        The formula is C(n + m - 1, n).\n\n        Args:\n            n (int): The number of indistinguishable sites (orbit multiplicity).\n            m (int): The number of distinct element types to choose from.\n\n        Returns:\n            int: The number of distinct decorations for the sublattice.\n        \"\"\"\n        # Ensure n and m are non-negative integers.\n        if n < 0 or m < 0:\n            raise ValueError(\"Number of sites and element types must be non-negative.\")\n        if m == 0 and n > 0:\n            return 0  # Cannot fill sites if no elements are available.\n        if n == 0:\n            return 1 # One way to have an empty decoration.\n        \n        # Using math.comb for robust calculation of binomial coefficients\n        # C(n, k) = n! / (k! * (n-k)!)\n        return math.comb(n + m - 1, n)\n\n    for case in test_cases:\n        prototype_name, m_A, m_B, m_O = case\n        params = prototype_params[prototype_name]\n        n_A, n_B, n_O = params['n_A'], params['n_B'], params['n_O']\n\n        # Calculate the number of distinct decorations for each sublattice.\n        N_A = count_decorations(n_A, m_A)\n        N_B = count_decorations(n_B, m_B)\n        N_O = count_decorations(n_O, m_O)\n\n        # The total number of distinct structures is the product of the\n        # possibilities for each independent sublattice.\n        total_structures = N_A * N_B * N_O\n        results.append(total_structures)\n\n    # Format the final output as a comma-separated list in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a vast search space of candidate materials, the next challenge is to execute the computational screening campaign efficiently. This practice tackles the real-world problem of scheduling a large number of independent calculations on a High-Performance Computing (HPC) cluster with a limited walltime budget $T$. You will formulate this as a convex optimization problem to derive an optimal resource allocation strategy that maximizes the total number of expected successful calculations, a core task in managing automated workflows. ",
            "id": "3456767",
            "problem": "You are designing an automated workflow scheduler for High-Performance Computing (HPC) in the context of high-throughput materials screening. There are $n$ independent jobs, each corresponding to a materials property calculation. For job $i$, you have a predicted runtime distribution conditioned on its features $x_i$, denoted by $p_i(t\\mid x_i)$, and an independent failure probability $p_{f,i}(x_i)$ that models the likelihood of a non-numerical failure (for example, electronic convergence failure). The total walltime budget is $T>0$, and the cluster provides $M\\ge 0$ identical cores of compute capacity, which can be time-shared across jobs.\n\nAssume the following scientifically grounded model:\n- Each job $i$ can be time-shared over the interval $[0,T]$ to receive a constant fractional allocation $\\alpha_i\\in[0,1]$ of one core, where $\\sum_{i=1}^n \\alpha_i \\le M$. This models automated workflow orchestration using fair time-slicing across $M$ cores, where each job is single-threaded and cannot effectively use more than one core concurrently.\n- The runtime distribution for each job is memoryless and scales linearly with its compute share: specifically, for features $x_i$, the predicted runtime distribution is exponential with rate parameter $\\lambda_i>0$ so that $p_i(t\\mid x_i) = \\lambda_i e^{-\\lambda_i t}$ for $t\\ge 0$. Under fractional allocation $\\alpha_i$, the effective rate becomes $\\lambda_i \\alpha_i$.\n- The failure event for job $i$ is independent of its runtime and occurs with probability $p_{f,i}(x_i)$; a job that fails yields no completed calculation.\n- A job $i$ is counted as completed within the walltime if it finishes by time $T$ and does not fail. Under the above assumptions, the probability that job $i$ completes by time $T$ is $(1-p_{f,i}(x_i))\\left(1-e^{-\\lambda_i \\alpha_i T}\\right)$.\n\nYour objective is to determine the fractional allocations $\\alpha_1,\\ldots,\\alpha_n$ that maximize the expected number of completed calculations $\\mathbb{E}[N]$ within the walltime $T$, subject to the constraints stated above. You must derive the optimization approach from first principles, using valid foundational bases such as the law of total expectation, properties of the exponential distribution (including memorylessness and hazard rates), and convex optimization optimality conditions when applicable. You must not rely on shortcut formulas without justification. The output of your program should be the maximum expected number of completed calculations $\\mathbb{E}[N]$ for each test case.\n\nImplement a complete, runnable program that:\n- Accepts no input and uses the following fixed test suite.\n- For each test case, computes the optimal allocation $\\alpha_i$ for all jobs $i$ under the stated constraints and assumptions, and outputs the corresponding maximal $\\mathbb{E}[N]$ as a float.\n\nTest suite:\n1. Happy path mix of rates and failures: $n=5$, $\\lambda=[0.2,0.5,0.1,1.0,0.3]$, $p_f=[0.05,0.10,0.00,0.20,0.05]$, $M=3$, $T=10$.\n2. Zero walltime boundary: $n=5$, same $\\lambda$ and $p_f$ as case $1$, $M=3$, $T=0$.\n3. One certain failure: $n=3$, $\\lambda=[0.2,1.2,0.4]$, $p_f=[0.00,1.00,0.00]$, $M=2$, $T=5$.\n4. Abundant cores relative to jobs (saturation at $\\alpha_i=1$): $n=3$, $\\lambda=[0.5,0.2,1.5]$, $p_f=[0.00,0.30,0.10]$, $M=10$, $T=2$.\n5. Zero cores boundary: $n=2$, $\\lambda=[0.5,0.8]$, $p_f=[0.00,0.00]$, $M=0$, $T=10$.\n\nYour program should produce a single line of output containing the results for the above five test cases as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places (for example, $[a_1,a_2,a_3,a_4,a_5]$). No physical units are required in the output because $\\mathbb{E}[N]$ is a dimensionless expectation of a count. Angles are not involved. Percentages must be interpreted as decimals, not with a percentage sign; for example, a failure probability of $0.10$ represents one-tenth.\n\nThe final output format is strictly:\n- A single line: $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5]$,\n- Where each $\\text{result}_i$ is a float rounded to six decimal places.",
            "solution": "The user-provided problem is valid. It is scientifically grounded in established models of computational job scheduling and performance, well-posed as a convex optimization problem, and objectively stated with no a priori invalidating flaws. We may proceed with a full solution.\n\nThe problem asks us to find the fractional core allocations $\\alpha_1, \\alpha_2, \\ldots, \\alpha_n$ that maximize the expected number of successfully completed calculations within a total walltime $T$ on a cluster with $M$ cores.\n\n**1. Mathematical Formulation of the Optimization Problem**\n\nLet $C_i$ be an indicator random variable for the successful completion of job $i$. $C_i=1$ if job $i$ finishes by time $T$ and does not suffer a non-numerical failure; otherwise, $C_i=0$. The total number of completed jobs is $N = \\sum_{i=1}^n C_i$. Our objective is to maximize the expectation of $N$. By the linearity of expectation, this is:\n$$\n\\mathbb{E}[N] = \\mathbb{E}\\left[\\sum_{i=1}^n C_i\\right] = \\sum_{i=1}^n \\mathbb{E}[C_i]\n$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, $\\mathbb{E}[C_i] = P(C_i=1)$.\nThe problem states that for job $i$, failure is independent of runtime and occurs with probability $p_{f,i}$. The time-to-completion, assuming no failure, follows an exponential distribution with an effective rate of $\\lambda_i \\alpha_i$ under fractional allocation $\\alpha_i$. The probability of this job finishing before time $T$ is given by the cumulative distribution function (CDF) of the exponential distribution, which is $1 - e^{-(\\lambda_i \\alpha_i) T}$.\nThe probability of successful completion is therefore the product of the probabilities of two independent events: (1) no failure, and (2) finishing by time $T$.\n$$\nP(C_i=1) = (1 - p_{f,i}) \\left(1 - e^{-\\lambda_i \\alpha_i T}\\right)\n$$\nLet's define $c_i = 1 - p_{f,i}$ as the probability of job $i$ not failing. The objective function to maximize is:\n$$\nf(\\boldsymbol{\\alpha}) = \\sum_{i=1}^n c_i \\left(1 - e^{-\\lambda_i T \\alpha_i}\\right)\n$$\nwhere $\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_n)$.\n\nThe allocations are subject to the following constraints:\n1. Total core allocation cannot exceed capacity: $\\sum_{i=1}^n \\alpha_i \\le M$\n2. Each job's allocation is a fraction of a single core: $0 \\le \\alpha_i \\le 1$ for all $i=1, \\ldots, n$.\n\n**2. Convexity Analysis**\n\nTo determine the nature of this optimization problem, we analyze the objective function and the constraints.\nThe constraint set defined by $\\sum \\alpha_i \\le M$ and $0 \\le \\alpha_i \\le 1$ is a convex polytope, which is a convex set.\nNow, consider the objective function. It is a sum of individual functions $f_i(\\alpha_i) = c_i (1 - e^{-\\lambda_i T \\alpha_i})$. We check the concavity of each $f_i(\\alpha_i)$ by examining its second derivative with respect to $\\alpha_i$.\nThe first derivative is:\n$$\n\\frac{df_i}{d\\alpha_i} = c_i \\frac{d}{d\\alpha_i} (1 - e^{-\\lambda_i T \\alpha_i}) = c_i (-\\lambda_i T)(-e^{-\\lambda_i T \\alpha_i}) = c_i \\lambda_i T e^{-\\lambda_i T \\alpha_i}\n$$\nThe second derivative is:\n$$\n\\frac{d^2f_i}{d\\alpha_i^2} = c_i \\lambda_i T \\frac{d}{d\\alpha_i} (e^{-\\lambda_i T \\alpha_i}) = c_i \\lambda_i T (-\\lambda_i T) e^{-\\lambda_i T \\alpha_i} = -c_i (\\lambda_i T)^2 e^{-\\lambda_i T \\alpha_i}\n$$\nGiven that $c_i = 1 - p_{f,i} \\ge 0$, $\\lambda_i > 0$, and $T > 0$, the exponential term is always positive. Therefore, $\\frac{d^2f_i}{d\\alpha_i^2} \\le 0$, which means each function $f_i(\\alpha_i)$ is concave. The sum of concave functions is also concave, so the total objective function $f(\\boldsymbol{\\alpha})$ is concave.\n\nMaximizing a concave function over a convex set is a convex optimization problem, which guarantees that any local maximum is also a global maximum. This structure allows us to use the Karush-Kuhn-Tucker (KKT) conditions to find the optimal solution.\n\n**3. Karush-Kuhn-Tucker (KKT) Conditions**\n\nWe form the Lagrangian for the problem:\n$$\nL(\\boldsymbol{\\alpha}, \\mu, \\boldsymbol{\\beta}, \\boldsymbol{\\gamma}) = \\sum_{i=1}^n c_i (1 - e^{-\\lambda_i T \\alpha_i}) - \\mu \\left(\\sum_{i=1}^n \\alpha_i - M\\right) - \\sum_{i=1}^n \\beta_i (\\alpha_i - 1) - \\sum_{i=1}^n \\gamma_i (-\\alpha_i)\n$$\nHere, $\\mu \\ge 0$, $\\beta_i \\ge 0$, and $\\gamma_i \\ge 0$ are the Lagrange multipliers for the constraints $\\sum \\alpha_i \\le M$, $\\alpha_i \\le 1$, and $\\alpha_i \\ge 0$, respectively.\n\nThe KKT stationarity condition requires $\\frac{\\partial L}{\\partial \\alpha_i} = 0$ for all $i$:\n$$\n\\frac{\\partial L}{\\partial \\alpha_i} = c_i \\lambda_i T e^{-\\lambda_i T \\alpha_i} - \\mu - \\beta_i + \\gamma_i = 0\n$$\n$$\n\\implies c_i \\lambda_i T e^{-\\lambda_i T \\alpha_i} = \\mu + \\beta_i - \\gamma_i\n$$\nThe term $c_i \\lambda_i T e^{-\\lambda_i T \\alpha_i}$ represents the marginal gain in the objective function from an infinitesimal increase in $\\alpha_i$. The KKT conditions state that at the optimum, this marginal gain must be balanced by the \"prices\" (Lagrange multipliers) of the active constraints.\n\nThe complementary slackness conditions are:\n1. $\\mu (\\sum_{i=1}^n \\alpha_i - M) = 0$\n2. $\\beta_i (\\alpha_i - 1) = 0$ for all $i$\n3. $\\gamma_i \\alpha_i = 0$ for all $i$\n\n**4. Structure of the Optimal Solution**\n\nWe analyze the KKT conditions for different cases of $\\alpha_i$:\n- Case 1: $0 < \\alpha_i < 1$. Complementary slackness implies $\\beta_i=0$ and $\\gamma_i=0$. The stationarity condition simplifies to $c_i \\lambda_i T e^{-\\lambda_i T \\alpha_i} = \\mu$. Solving for $\\alpha_i$ (assuming $\\mu > 0$ and $c_i \\lambda_i T > 0$):\n  $$\n  e^{-\\lambda_i T \\alpha_i} = \\frac{\\mu}{c_i \\lambda_i T} \\implies \\alpha_i = \\frac{1}{\\lambda_i T} \\ln\\left(\\frac{c_i \\lambda_i T}{\\mu}\\right)\n  $$\n- Case 2: $\\alpha_i = 1$. Complementary slackness implies $\\gamma_i=0$, so $c_i \\lambda_i T e^{-\\lambda_i T} = \\mu + \\beta_i$. Since $\\beta_i \\ge 0$, this means $\\mu \\le c_i \\lambda_i T e^{-\\lambda_i T}$.\n- Case 3: $\\alpha_i = 0$. Complementary slackness implies $\\beta_i=0$, so $c_i \\lambda_i T = \\mu - \\gamma_i$. Since $\\gamma_i \\ge 0$, this means $\\mu \\ge c_i \\lambda_i T$.\n\nCombining these cases, we can express the optimal $\\alpha_i$ as a function of the single Lagrange multiplier $\\mu$:\n$$\n\\alpha_i(\\mu) = \\min\\left(1, \\max\\left(0, \\frac{1}{\\lambda_i T} \\ln\\left(\\frac{c_i \\lambda_i T}{\\mu}\\right)\\right)\\right)\n$$\nThis expression holds by defining $\\ln(\\cdot)$ appropriately for non-positive arguments or, more practically, by using conditional logic. If $c_i\\lambda_i T = 0$, the marginal gain is always zero, so $\\alpha_i=0$. If $c_i \\lambda_i T > 0$ and $\\mu \\ge c_i \\lambda_i T$, then $\\ln(\\cdot) \\le 0$, resulting in $\\alpha_i=0$.\n\n**5. Algorithmic Solution**\n\nThe total requested allocation, $A(\\mu) = \\sum_{i=1}^n \\alpha_i(\\mu)$, is a monotonically non-increasing function of the shadow price $\\mu$. We must find the optimal $\\mu^*$ that satisfies all KKT conditions.\n\n- **Boundary Cases**: If $T=0$ or $M=0$, no calculation can be completed or no resources can be allocated. Thus, all $\\alpha_i=0$ and $\\mathbb{E}[N]=0$.\n\n- **Abundant Resources Case**: If we can set $\\alpha_i=1$ for all \"effective\" jobs (where $c_i \\lambda_i T > 0$) without violating the total core limit. Let $n_{eff}$ be the number of such jobs. If $n_{eff} \\le M$, we can set $\\alpha_i=1$ for all effective jobs and $\\alpha_i=0$ for others. The total allocation is $n_{eff} \\le M$, satisfying the constraint. This corresponds to an effective shadow price $\\mu^*=0$ (or, more precisely, a value low enough that $\\alpha_i(\\mu^*)=1$ for all effective jobs). This is optimal because each $f_i$ is non-decreasing in $\\alpha_i$.\n\n- **Constrained Resources Case**: If $n_{eff} > M$, the resources are scarce, and the constraint $\\sum \\alpha_i \\le M$ will be active, meaning $\\sum \\alpha_i = M$ and $\\mu^* > 0$. We must find the unique value of $\\mu^* > 0$ that solves the equation:\n  $$\n  A(\\mu) = \\sum_{i=1}^n \\min\\left(1, \\max\\left(0, \\frac{1}{\\lambda_i T} \\ln\\left(\\frac{c_i \\lambda_i T}{\\mu}\\right)\\right)\\right) = M\n  $$\n  Since $A(\\mu)$ is monotonic, this root-finding problem is efficiently solved using the bisection method. We search for $\\mu$ in an interval $[\\mu_{low}, \\mu_{high}]$. A suitable initial interval is $[0, \\max_i(c_i \\lambda_i T)]$. As $\\mu \\to 0^+$, $A(\\mu) \\to n_{eff} > M$. For $\\mu > \\max_i(c_i \\lambda_i T)$, $A(\\mu) = 0 < M$. A root is guaranteed to exist within this interval.\n\n**6. Final Calculation**\n\nThe algorithm is as follows:\n1. For a given test case, identify parameters $n, \\boldsymbol{\\lambda}, \\boldsymbol{p_f}, M, T$.\n2. Handle the boundary cases $T=0$ or $M=0$.\n3. Determine if the problem is resource-abundant ($n_{eff} \\le M$). If so, set $\\alpha_i=1$ for effective jobs.\n4. If resources are constrained, use bisection to find the root $\\mu^*$ of $A(\\mu) - M = 0$.\n5. With the optimal $\\mu^*$, calculate the final allocations $\\alpha_i^* = \\alpha_i(\\mu^*)$.\n6. Compute the maximum expected completions using these allocations: $\\mathbb{E}[N]^* = \\sum_{i=1}^n (1-p_{f,i}) (1 - e^{-\\lambda_i \\alpha_i^* T})$.\n\nThis procedure provides a unique and optimal solution for each valid set of inputs.",
            "answer": "```python\nimport numpy as np\n\ndef compute_for_case(params):\n    \"\"\"\n    Computes the maximum expected number of completed calculations for a single test case.\n    \"\"\"\n    n_jobs, lambdas, p_fs, M, T = params\n    lambdas = np.array(lambdas)\n    p_fs = np.array(p_fs)\n    \n    # Handle boundary cases where no progress is possible\n    if T == 0 or M == 0:\n        return 0.0\n\n    cs = 1.0 - p_fs\n    \n    # Identify effective jobs (those that can possibly complete)\n    effective_mask = (cs > 0) & (lambdas > 0)\n    n_eff = np.sum(effective_mask)\n\n    # If no effective jobs, nothing can be completed\n    if n_eff == 0:\n        return 0.0\n\n    # Case 1: Abundant resources. We can allocate a full core to each effective job.\n    if n_eff <= M:\n        alphas = np.zeros(n_jobs)\n        alphas[effective_mask] = 1.0\n        \n    # Case 2: Constrained resources. We need to find the optimal shadow price mu.\n    else:\n        v = cs * lambdas * T\n        lambda_T = lambdas * T\n\n        # Define the total allocation function based on shadow price 'mu'\n        def get_total_alpha(mu):\n            if mu <= 1e-12: # Effectively zero mu\n                return float('inf')\n                \n            total_alpha = 0.0\n            for i in range(n_jobs):\n                if v[i] > mu:\n                    # min(1, ...) part of the formula\n                    # The value is capped at 1 if mu is very small\n                    alloc = (1.0 / lambda_T[i]) * np.log(v[i] / mu)\n                    total_alpha += min(1.0, alloc)\n            return total_alpha\n\n        # Bisection search for the optimal mu\n        mu_low = 0.0\n        mu_high = np.max(v) * 1.0001 # Search up to the max possible initial marginal gain\n        \n        # 100 iterations is more than enough for double precision convergence\n        for _ in range(100):\n            mu_mid = (mu_low + mu_high) / 2.0\n            if mu_mid == mu_low or mu_mid == mu_high: # Reached precision limit\n                break\n            \n            if get_total_alpha(mu_mid) > M:\n                mu_low = mu_mid\n            else:\n                mu_high = mu_mid\n        \n        optimal_mu = (mu_low + mu_high) / 2.0\n        \n        # Compute optimal allocations from the found mu\n        alphas = np.zeros(n_jobs)\n        for i in range(n_jobs):\n            if effective_mask[i] and v[i] > optimal_mu:\n                 alloc = (1.0 / lambda_T[i]) * np.log(v[i] / optimal_mu)\n                 alphas[i] = min(1.0, alloc)\n\n    # Calculate final expected number of completions with optimal alphas\n    completion_probs = cs * (1.0 - np.exp(-lambdas * alphas * T))\n    max_expected_N = np.sum(completion_probs)\n    \n    return max_expected_N\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # 1. Happy path\n        (5, [0.2, 0.5, 0.1, 1.0, 0.3], [0.05, 0.10, 0.00, 0.20, 0.05], 3, 10),\n        # 2. Zero walltime\n        (5, [0.2, 0.5, 0.1, 1.0, 0.3], [0.05, 0.10, 0.00, 0.20, 0.05], 3, 0),\n        # 3. One certain failure\n        (3, [0.2, 1.2, 0.4], [0.00, 1.00, 0.00], 2, 5),\n        # 4. Abundant cores\n        (3, [0.5, 0.2, 1.5], [0.00, 0.30, 0.10], 10, 2),\n        # 5. Zero cores\n        (2, [0.5, 0.8], [0.00, 0.00], 0, 10),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_for_case(case)\n        results.append(result)\n    \n    # Format output as required, rounding to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The output of a high-throughput screening campaign is not a set of perfect numbers, but rather predictions with inherent uncertainties. To make reliable scientific discoveries, we must quantify the confidence in our results, such as the predicted stability of a new phase, often measured by its energy above the convex hull, $E_{\\mathrm{hull}}$. This final practice guides you through a powerful statistical technique, the Monte Carlo bootstrap, to propagate the uncertainty from individual DFT energy calculations $\\mathcal{N}(H_{f,i}, \\sigma_i^2)$ to the final phase diagram, allowing you to compute confidence intervals for a material's stability. ",
            "id": "3456702",
            "problem": "You are designing an automated workflow to propagate uncertainty in Density Functional Theory (DFT) calculated formation enthalpies to phase stability estimates for a binary system, suitable for high-throughput materials screening. For a binary system composed of elements A and B, consider a set of intermediate compounds indexed by $i$ with composition $x_i \\in (0,1)$ representing the atomic fraction of B, and formation enthalpy per atom $H_{f,i}$ in electronvolts per atom. The pure elements at $x=0$ and $x=1$ have $H_f = 0$ by definition. Each intermediate compound has an independently estimated uncertainty represented by a standard deviation $\\sigma_i$, and uncertainties are modeled as independent Gaussian distributions.\n\nYour task is to implement a Monte Carlo bootstrap to propagate the uncertainties in $H_{f,i}$ to the energy above the convex hull $E_{\\mathrm{hull},i}$ for a specified target compound in each test case. The required steps and definitions are as follows:\n\n- Fundamental base: The formation enthalpy per atom of a mixture along a tie-line between two phases at compositions $x_a$ and $x_b$ with formation enthalpies $H_{f,a}$ and $H_{f,b}$ is given by the convex combination $H_{f,\\mathrm{mix}}(x) = \\lambda H_{f,a} + (1-\\lambda) H_{f,b}$ where $\\lambda = \\dfrac{x_b - x}{x_b - x_a}$ for $x \\in [x_a, x_b]$. The lower convex envelope (the convex hull in one dimension) of the set of points $\\{(x,H_f)\\}$ represents the set of energetically stable mixtures under the lever rule. The energy above hull at composition $x_i$ is defined as $E_{\\mathrm{hull},i} = H_{f,i} - H_{\\mathrm{hull}}(x_i)$, where $H_{\\mathrm{hull}}(x)$ is the hull energy evaluated at $x$ by linear interpolation between adjacent hull vertices that bracket $x$.\n\n- Bootstrap procedure to propagate uncertainty:\n  1. For a given case, form the full set of points that includes the pure elements $(0,0)$ and $(1,0)$ and all intermediate compounds $(x_i, H_{f,i})$. The pure elements have zero uncertainty.\n  2. For each bootstrap replicate $b \\in \\{1,\\dots,B\\}$, draw one sample of $H'_{f,i} \\sim \\mathcal{N}(H_{f,i}, \\sigma_i^2)$ independently for each intermediate compound; keep $(0,0)$ and $(1,0)$ unchanged.\n  3. Compute the lower convex hull of the sampled points $\\{(x_i, H'_{f,i})\\}$ plus $(0,0)$ and $(1,0)$ using a correct convexity condition: for three consecutive candidate hull points $(x_1,y_1)$, $(x_2,y_2)$, $(x_3,y_3)$ with $x_1 < x_2 < x_3$, the slopes must satisfy $\\dfrac{y_2 - y_1}{x_2 - x_1} \\le \\dfrac{y_3 - y_2}{x_3 - x_2}$; otherwise, remove the middle point and recheck, until convexity holds.\n  4. Evaluate $H_{\\mathrm{hull}}(x_t)$ at the target composition $x_t$ by linear interpolation between the two adjacent hull vertices bracketing $x_t$.\n  5. Compute $E^{(b)}_{\\mathrm{hull}} = \\max\\{0, H'_{f,t} - H_{\\mathrm{hull}}(x_t)\\}$ for that bootstrap replicate.\n  6. After $B$ replicates, estimate the sample mean and the two-sided $95\\%$ confidence interval as the empirical quantiles at probabilities $0.025$ and $0.975$ of the set $\\{E^{(b)}_{\\mathrm{hull}}\\}_{b=1}^B$.\n\n- Your program must implement the above with deterministic pseudorandomness by using the provided seeds.\n\n- All energies must be expressed in electronvolts per atom, and all outputs must be reported in electronvolts per atom, rounded to $6$ decimal places.\n\n- For numerical stability, treat any negative value of $E_{\\mathrm{hull}}$ arising from floating-point round-off as $0$.\n\nMathematical and algorithmic specifications:\n\n- You must compute the one-dimensional lower convex hull over the set of points $(x,H_f)$ augmented with $(0,0)$ and $(1,0)$. The hull is the unique minimal convex function that lies below or equal to all points. A practical construction in one dimension is to sort points by $x$ and iteratively enforce the convexity inequality on consecutive triples as given above.\n\n- You must produce results for the specified target compound only for each test case. For a target with composition exactly equal to a hull vertex in a bootstrap replicate, $E_{\\mathrm{hull}}$ is $0$ in that replicate.\n\n- Confidence interval definition: If the sorted $E_{\\mathrm{hull}}$ samples are $e_{(1)} \\le \\dots \\le e_{(B)}$, the lower endpoint is the element at position $\\lceil 0.025 \\times B \\rceil$ and the upper endpoint is the element at position $\\lceil 0.975 \\times B \\rceil$, using standard empirical quantiles with linear interpolation permitted; using a numerical library’s quantile at probabilities $0.025$ and $0.975$ is acceptable.\n\nTest suite to cover general, boundary, and edge cases. In every case below, include the pure elements $(0,0)$ and $(1,0)$ with zero uncertainty in the hull construction; only the listed intermediate compounds are uncertain.\n\n- Case $1$ (general stable interior phase):\n  - Intermediate compositions and statistics: $\\{(x_i, H_{f,i}, \\sigma_i)\\} = \\{(0.25, -0.090, 0.020), (0.50, -0.150, 0.015), (0.75, -0.085, 0.020)\\}$.\n  - Target index: $1$ (that is, the compound at $x = 0.50$).\n  - Bootstrap replicates: $B = 400$.\n  - Seed: $7$.\n\n- Case $2$ (metastable boundary-leaning phase with uncertainty that can cross the hull):\n  - Intermediate compositions and statistics: $\\{(x_i, H_{f,i}, \\sigma_i)\\} = \\{(0.20, -0.030, 0.010), (0.33, -0.070, 0.025), (0.60, -0.040, 0.010), (0.80, -0.010, 0.010)\\}$.\n  - Target index: $0$ (that is, the compound at $x = 0.20$).\n  - Bootstrap replicates: $B = 400$.\n  - Seed: $11$.\n\n- Case $3$ (near-degenerate interior phases testing hull flips):\n  - Intermediate compositions and statistics: $\\{(x_i, H_{f,i}, \\sigma_i)\\} = \\{(0.10, -0.005, 0.004), (0.40, -0.085, 0.012), (0.55, -0.083, 0.012), (0.90, -0.004, 0.004)\\}$.\n  - Target index: $2$ (that is, the compound at $x = 0.55$).\n  - Bootstrap replicates: $B = 600$.\n  - Seed: $13$.\n\nFinal output specification:\n\n- For each case, compute three values for the target compound: the bootstrap sample mean of $E_{\\mathrm{hull}}$, the empirical lower $0.025$ quantile, and the empirical upper $0.975$ quantile.\n- Round each of the three values to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of lists in the order of the cases, each inner list structured as $[ \\text{mean}, \\text{lower}, \\text{upper} ]$. For example, the printed line should look like $[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],[m_3,\\ell_3,u_3]]$, where every entry is a float in electronvolts per atom, rounded to $6$ decimal places.",
            "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational materials science. It provides a complete and consistent set of definitions, data, and algorithmic requirements to perform uncertainty quantification for phase stability using a Monte Carlo bootstrap method. All specified parameters are physically reasonable and the methodology aligns with standard practices in the field.\n\nThe task is to propagate uncertainties from Density Functional Theory (DFT) calculated formation enthalpies, $H_{f,i}$, to the derived quantity of energy above the convex hull, $E_{\\mathrm{hull},i}$, for a binary material system. The uncertainty in each $H_{f,i}$ is given by a standard deviation $\\sigma_i$ and modeled by an independent Gaussian distribution, $\\mathcal{N}(H_{f,i}, \\sigma_i^2)$. The solution requires implementing the specified bootstrap procedure.\n\nThe core of the methodology is a Monte Carlo simulation with $B$ replicates for each test case. In each replicate, we perform the following steps:\n\n1.  **Stochastic Sampling**: For each intermediate compound with composition $x_i$ and mean formation enthalpy $H_{f,i}$, a new formation enthalpy sample, $H'_{f,i}$, is drawn from its Gaussian probability distribution. The pure elements at $x=0$ and $x=1$ have their formation enthalpies fixed at $H_f=0$ with zero uncertainty. The reproducibility of this stochastic process is ensured by seeding a pseudorandom number generator as specified for each case.\n\n2.  **Convex Hull Construction**: The set of points $\\{(x_i, H'_{f,i})\\}$ for all intermediate compounds, augmented with the fixed points $(0,0)$ and $(1,0)$ for the pure elements, is used to construct the one-dimensional lower convex hull. This is achieved using a variant of Andrew's monotone chain algorithm, which is well-suited for this one-dimensional problem. The algorithm proceeds as follows:\n    a. All points are sorted by their composition (x-coordinate).\n    b. The points are iterated through, and a candidate hull is built. For each new point $p_3$ being considered, it is checked against the last two points, $p_1$ and $p_2$, on the candidate hull. The three points must form a \"right turn\" or be collinear to maintain the convexity of the lower hull. This corresponds to the sequence of slopes of the segments being non-decreasing: $\\text{slope}(p_1, p_2) \\le \\text{slope}(p_2, p_3)$.\n    c. If the points form a \"left turn\" (i.e., $\\text{slope}(p_1, p_2) > \\text{slope}(p_2, p_3)$), the middle point $p_2$ introduces a concavity and is removed from the candidate hull. This check is repeated until the convexity condition is satisfied, after which the new point $p_3$ is added.\n    d. This process results in a list of vertices that define the lower convex hull, sorted by composition. For numerical stability, the slope comparison is implemented using a cross-product, which avoids division. The condition to remove point $p_2 = (x_2, y_2)$ when considering point $p_3 = (x_3, y_3)$, given the prior hull point $p_1 = (x_1, y_1)$, is when $(y_2 - y_1)(x_3 - x_2) - (x_2 - x_1)(y_3 - y_2) > 0$.\n\n3.  **Energy Above Hull Calculation**: For a specified target compound with composition $x_t$, its energy above the hull, $E_{\\mathrm{hull}}$, must be computed.\n    a. First, the energy of the hull at the target composition, $H_{\\mathrm{hull}}(x_t)$, is determined. This involves finding the two adjacent vertices on the computed convex hull, $(x_a, H_a)$ and $(x_b, H_b)$, that bracket the target composition, i.e., $x_a \\le x_t \\le x_b$.\n    b. $H_{\\mathrm{hull}}(x_t)$ is then calculated by linear interpolation between these two vertices: $H_{\\mathrm{hull}}(x_t) = H_a + (H_b - H_a) \\frac{x_t - x_a}{x_b - x_a}$.\n    c. The energy above the hull for the current bootstrap replicate, $E^{(b)}_{\\mathrm{hull}}$, is the difference between the target compound's sampled enthalpy and the hull energy at its composition: $E^{(b)}_{\\mathrm{hull}} = H'_{f,t} - H_{\\mathrm{hull}}(x_t)$. As specified, any resulting negative values, which can occur due to floating-point imprecision when a point is on the hull, are set to $0$.\n\n4.  **Statistical Analysis**: After completing all $B$ bootstrap replicates, a distribution of $\\{E^{(b)}_{\\mathrm{hull}}\\}_{b=1}^B$ values is obtained. From this distribution, the sample mean is calculated, representing the best estimate for the energy above the hull. The uncertainty in this estimate is quantified by a two-sided $95\\%$ confidence interval, which is determined by the empirical quantiles of the sample distribution at probabilities $0.025$ and $0.975$.\n\nThe implementation will be encapsulated within a single Python script. The `numpy` library is utilized for efficient numerical operations, specifically for generating Gaussian random numbers, performing array manipulations, and calculating the final statistical measures (mean and quantiles). The final results for each case, consisting of the mean, lower quantile, and upper quantile, are rounded to $6$ decimal places and formatted into a single output line as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the uncertainty propagation problem for DFT formation enthalpies\n    using a Monte Carlo bootstrap method as specified.\n    \"\"\"\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.25, -0.090, 0.020],\n                [0.50, -0.150, 0.015],\n                [0.75, -0.085, 0.020]\n            ]),\n            \"target_idx\": 1,\n            \"B\": 400,\n            \"seed\": 7\n        },\n        {\n            \"data\": np.array([\n                [0.20, -0.030, 0.010],\n                [0.33, -0.070, 0.025],\n                [0.60, -0.040, 0.010],\n                [0.80, -0.010, 0.010]\n            ]),\n            \"target_idx\": 0,\n            \"B\": 400,\n            \"seed\": 11\n        },\n        {\n            \"data\": np.array([\n                [0.10, -0.005, 0.004],\n                [0.40, -0.085, 0.012],\n                [0.55, -0.083, 0.012],\n                [0.90, -0.004, 0.004]\n            ]),\n            \"target_idx\": 2,\n            \"B\": 600,\n            \"seed\": 13\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        data = case[\"data\"]\n        target_idx = case[\"target_idx\"]\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n\n        x_coords = data[:, 0]\n        h_means = data[:, 1]\n        h_stds = data[:, 2]\n        target_x = x_coords[target_idx]\n\n        rng = np.random.default_rng(seed)\n\n        e_hull_samples = []\n        for _ in range(B):\n            # 1. Sample H_f values for intermediate compounds\n            h_samples = rng.normal(loc=h_means, scale=h_stds)\n            \n            # 2. Form the full set of points including endpoints (0,0) and (1,0)\n            points = [(0.0, 0.0)]\n            for i in range(len(x_coords)):\n                points.append((x_coords[i], h_samples[i]))\n            points.append((1.0, 0.0))\n            \n            # Points are already sorted by x-coordinate based on problem input structure\n            # points.sort(key=lambda p: p[0]) is implicitly handled.\n\n            # 3. Compute the lower convex hull using the monotone chain algorithm\n            lower_hull = []\n            for p3 in points:\n                # p1, p2, p3 must form a \"right turn\" for a lower hull. Pop on \"left turn\".\n                while len(lower_hull) >= 2:\n                    p1 = lower_hull[-2]\n                    p2 = lower_hull[-1]\n                    # Cross product check: (y2-y1)(x3-x2) - (x2-x1)(y3-y2) > 0 -> \"left turn\"\n                    # We pop if the turn is counter-clockwise (left turn), which corresponds\n                    # to a concave corner in the lower hull.\n                    if (p2[1] - p1[1]) * (p3[0] - p2[0]) - (p2[0] - p1[0]) * (p3[1] - p2[1]) > 0:\n                        lower_hull.pop()\n                    else:\n                        break\n                lower_hull.append(p3)\n\n            # 4. Evaluate H_hull(x_t) by interpolation on the computed hull\n            h_hull_at_target = 0.0\n            for i in range(len(lower_hull) - 1):\n                p_a = lower_hull[i]\n                p_b = lower_hull[i+1]\n                if p_a[0] <= target_x <= p_b[0]:\n                    xa, ha = p_a\n                    xb, hb = p_b\n                    # Handle cases where target_x is exactly a hull vertex\n                    if abs(target_x - xa) < 1e-12:\n                        h_hull_at_target = ha\n                        break\n                    if abs(target_x - xb) < 1e-12:\n                        h_hull_at_target = hb\n                        break\n                    # Linear interpolation for x_t between two hull vertices\n                    h_hull_at_target = ha + (hb - ha) * (target_x - xa) / (xb - xa)\n                    break\n            \n            # 5. Compute E_hull for the replicate\n            h_prime_t = h_samples[target_idx]\n            e_hull = h_prime_t - h_hull_at_target\n            e_hull = max(0.0, e_hull)\n            e_hull_samples.append(e_hull)\n\n        # 6. Compute statistics (mean, 95% CI)\n        e_hull_samples = np.array(e_hull_samples)\n        mean_e_hull = np.mean(e_hull_samples)\n        lower_ci = np.quantile(e_hull_samples, 0.025)\n        upper_ci = np.quantile(e_hull_samples, 0.975)\n        \n        all_results.append([\n            round(mean_e_hull, 6),\n            round(lower_ci, 6),\n            round(upper_ci, 6)\n        ])\n\n    result_str = \"[\" + \",\".join([f\"[{m},{l},{u}]\" for m, l, u in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```"
        }
    ]
}