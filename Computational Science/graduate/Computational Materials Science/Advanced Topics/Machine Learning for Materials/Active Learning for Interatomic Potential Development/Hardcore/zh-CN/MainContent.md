## 引言
[原子间势](@entry_id:177673)是连接原子尺度结构与宏观材料性质的关键桥梁，在计算材料科学中扮演着至关重要的角色。然而，构建能够媲美量子力学计算精度的[机器学习势](@entry_id:183033)，往往需要海量的训练数据，其高昂的计算成本构成了该领域的一大瓶颈。为了解决这一挑战，[主动学习](@entry_id:157812)（Active Learning）应运而生，它提供了一种智能的数据驱动策略，旨在以最小的计算投入，最高效地构建出稳健而精确的[原子间势](@entry_id:177673)，从而颠覆了传统的[势函数](@entry_id:176105)开发模式。

本文将系统性地引导读者深入了解这一前沿领域。我们首先将在“原理与机制”一章中，揭示[主动学习](@entry_id:157812)的理论核心，从[势函数](@entry_id:176105)的概率性描述、不确定性的量化与分解，到指导[数据采集](@entry_id:273490)的各类策略。接着，在“应用与跨学科连接”一章中，我们将展示该方法如何在探索新材料、优化特定物理性质以及应对复杂模拟场景中大显身手。最后，通过“动手实践”部分，读者将有机会将理论知识应用于解决具体的物理问题，加深对主动学习工作流的理解。通过这一系列的学习，您将掌握构建下一代智能[原子间势](@entry_id:177673)的关键技术。

## 原理与机制

发展[原子间势](@entry_id:177673)的核心挑战在于以最小的计算成本（通常是昂贵的量子力学计算）获得一个在广阔[构型空间](@entry_id:149531)内都准确的模型。主动学习通过一种数据驱动的智能[采样策略](@entry_id:188482)来应对这一挑战，它旨在迭代地识别并标记那些能够最大程度提升模型性能的原子构型。本章将深入探讨支撑这一过程的核心原理与机制，从势函数的概率表征到[量化不确定性](@entry_id:272064)，再到指导[数据采集](@entry_id:273490)的策略和模型部署中的实际保障措施。

### 势函数的概率表征

传统上，[原子间势](@entry_id:177673)被视为一个确定的函数，将原子构型映射到一个唯一的能量值。然而，在[主动学习](@entry_id:157812)的框架下，我们采用一种更强大的**概率视角**：将[势函数](@entry_id:176105)本身视为一个[随机变量](@entry_id:195330)。我们不是寻找单一的“最佳”函数，而是旨在推断一个符合已有数据且能体现我们知识局限性的函数**[分布](@entry_id:182848)**。这种方法的核心是贝叶斯推断。

#### 贝叶斯线性模型

为了使讨论具体化，我们首先考虑一个在参数上为线性的模型，这是许多[现代机器学习](@entry_id:637169)势的基础。假设一个原子局域环境可以通过一个[特征向量](@entry_id:151813)或**描述符** $\boldsymbol{\phi} \in \mathbb{R}^D$ 来表示，该描述符捕捉了邻近原子的几何信息。能量 $E$ 则被建模为这些特征的线性组合：

$E(\boldsymbol{\phi}) = \boldsymbol{\phi}^{\top}\mathbf{w}$

其中 $\mathbf{w} \in \mathbb{R}^D$ 是模型的权重参数。贝叶斯方法的精髓在于将这些权重 $\mathbf{w}$ 视为[随机变量](@entry_id:195330)，并通过数据来约束其可能取值。

1.  **先验分布 $p(\mathbf{w})$**：在观测任何数据之前，我们对参数 $\mathbf{w}$ 的信念由先验分布描述。一个常见且计算上便利的选择是零均值[高斯先验](@entry_id:749752) ：
    $p(\mathbf{w}) = \mathcal{N}(\mathbf{w} | \mathbf{0}, \alpha^{-1}\mathbf{I})$
    这里，$\mathbf{I}$ 是[单位矩阵](@entry_id:156724)，$\alpha$ 是先验**精度**（[方差](@entry_id:200758)的倒数）。这个先验表达了一种偏好，即较小的权重值更为可能，这在功能上起到了**正则化**的作用，防止模型对训练数据过拟合。

2.  **[似然函数](@entry_id:141927) $p(\text{data}|\mathbf{w})$**：[似然函数](@entry_id:141927)描述了在给定一组特定参数 $\mathbf{w}$ 的情况下，观测到当前训练数据的概率。我们通常假设观测值（如能量或力）包含[高斯噪声](@entry_id:260752)。例如，一个能量的观测值 $y_E$ 可以被建模为：
    $y_E = \boldsymbol{\phi}^{\top}\mathbf{w} + \varepsilon_E, \quad \text{其中} \quad \varepsilon_E \sim \mathcal{N}(0, \sigma_n^2)$
    这里，$\sigma_n^2$ 是观测噪声的[方差](@entry_id:200758)，其精度为 $\beta = \sigma_n^{-2}$ 。对于包含 $N$ 个数据点的[训练集](@entry_id:636396) $\{(\boldsymbol{\phi}_i, y_i)\}_{i=1}^N$，其[似然函数](@entry_id:141927)是所有数据点概率的乘积。

3.  **后验分布 $p(\mathbf{w}|\text{data})$**：[贝叶斯推断](@entry_id:146958)的核心是通过贝叶斯定理将先验信念与数据证据（[似然](@entry_id:167119)）结合起来，得到更新后的[后验分布](@entry_id:145605)：
    $p(\mathbf{w}|\text{data}) \propto p(\text{data}|\mathbf{w}) p(\mathbf{w})$
    由于[高斯先验](@entry_id:749752)和高斯[似然](@entry_id:167119)的共轭性质，[后验分布](@entry_id:145605)也将是一个高斯分布, $p(\mathbf{w}|\text{data}) = \mathcal{N}(\mathbf{w}|\mathbf{m}_N, \mathbf{S}_N)$。其均值 $\mathbf{m}_N$ 和协[方差](@entry_id:200758) $\mathbf{S}_N$ 可以解析地求出。具体来说，后验协[方差](@entry_id:200758) $\mathbf{S}_N$ 由下式给出：
    $\mathbf{S}_N = (\alpha \mathbf{I} + \beta \mathbf{X}^{\top}\mathbf{X})^{-1}$
    其中 $\mathbf{X}$ 是一个 $N \times D$ 的**[设计矩阵](@entry_id:165826)**，其行是训练数据点的描述符 $\boldsymbol{\phi}_i^{\top}$。[后验协方差矩阵](@entry_id:753631) $\mathbf{S}_N$ 至关重要，因为它量化了在观测到数据后，我们对模型参数 $\mathbf{w}$ 仍然存在的不确定性。矩阵越“大”，不确定性就越高。

### 量化与分解预测不确定性

拥有了参数的[后验分布](@entry_id:145605)，我们便可以对新构型的性质进行预测，并量化这些预测的[置信度](@entry_id:267904)。对于一个新的描述符 $\boldsymbol{\phi}_*$, 模型的预测不再是一个单一的数值，而是一个完整的**[后验预测分布](@entry_id:167931)**。

该[分布](@entry_id:182848)的[方差](@entry_id:200758)，即**总预测[方差](@entry_id:200758)**，可以被精确地分解为两个有深刻物理意义的组成部分 ：

$\sigma^2(\boldsymbol{\phi}_*) = \underbrace{\sigma_n^2}_{\text{任意不确定性}} + \underbrace{\boldsymbol{\phi}_*^{\top}\mathbf{S}_N \boldsymbol{\phi}_*}_{\text{认知不确定性}}$

1.  **任意不确定性 (Aleatoric Uncertainty)**：$\sigma_n^2$ 项源于数据本身固有的随机性或噪声。例如，DFT计算中的收敛误差，或在有限温度下原子位置的随机[振动](@entry_id:267781)。这种不确定性是系统内在的，原则上无法通过收集更多相同类型的数据来减少。

2.  **认知不确定性 (Epistemic Uncertainty)**：$\boldsymbol{\phi}_*^{\top}\mathbf{S}_N \boldsymbol{\phi}_*$ 项源于我们对模型参数 $\mathbf{w}$ 的不确定性（由后验协[方差](@entry_id:200758) $\mathbf{S}_N$ 捕获）。它反映了模型的“知识欠缺”。在训练数据稀疏或缺失的[构型空间](@entry_id:149531)区域，$\mathbf{S}_N$ 较大，导致[认知不确定性](@entry_id:149866)也较大。这正是主动学习旨在通过收集新数据来减小的不确定性。

除了上述解析方法，一种在实践中非常流行且灵活的技术是使用**模型集成 (ensemble)** 。通过训练多个（例如5到10个）具有不同随机初始化或数据[子集](@entry_id:261956)的模型，我们可以得到一组不同的预测值。这些预测值的[方差](@entry_id:200758)可以作为[认知不确定性](@entry_id:149866)的一个良好近似。这种方法不局限于[线性模型](@entry_id:178302)，可广泛应用于深度神经网络等复杂模型。

### [主动学习](@entry_id:157812)循环：[采集函数](@entry_id:168889)

[主动学习](@entry_id:157812)的核心循环在于一个**[采集函数](@entry_id:168889) (acquisition function)**，它评估每个未标记的候选构型，并为其分配一个“[信息量](@entry_id:272315)”分数。分数最高的构型将被送去进行昂贵的标记（例如，DFT计算），然后加入训练集以更新模型。

#### 基于不确定性的采集

最直观的策略是选择模型最不确定的构型进行查询。

**最大[方差](@entry_id:200758) (Maximum Variance, MV)**：这种[采集函数](@entry_id:168889)直接选择具有最大[认知不确定性](@entry_id:149866)的构型 。其准则为：
$\text{argmax}_{\boldsymbol{\phi}_*} \quad \boldsymbol{\phi}_*^{\top}\mathbf{S}_N \boldsymbol{\phi}_*$
这种策略的直觉是“在最无知的地方提问”，通过在模型不确定的区域增加数据，可以最快地填补知识空白。

#### 基于信息论的采集

一种更具理论依据的方法是将采集问题框架化为最大化有关模型参数的[信息增益](@entry_id:262008)。

**贝叶斯[主动学习](@entry_id:157812)分歧 (Bayesian Active Learning by Disagreement, BALD)**：该方法旨在最大化模型参数 $\mathbf{w}$ 和新观测值 $y_*$ 之间的**[互信息](@entry_id:138718)** $I(y_*; \mathbf{w})$ [@problem_id:3431842, @problem_id:3431872]。互信息可以表示为：
$I(y_*; \mathbf{w}) = H(y_*) - \mathbb{E}_{\mathbf{w}}[H(y_*|\mathbf{w})]$
这可以被直观地解释为“预测的总不确定性”减去“预期的噪声不确定性”。换句话说，我们寻找的是那些模型预测虽然整体上不确定（$H(y_*)$ 大），但对于任何一组固定的参数，其预测都是相当确定的（$H(y_*|\mathbf{w})$ 小）的构型。这种[分歧](@entry_id:193119)正是[信息增益](@entry_id:262008)的来源。对于高斯模型，该[互信息](@entry_id:138718)可以简化为 ：
$I(y_*; \mathbf{w}) = \frac{1}{2} \ln \left(1 + \frac{\sigma_e^2}{\sigma_a^2}\right)$
其中 $\sigma_e^2$ 是认知（epistemic）[方差](@entry_id:200758)，$\sigma_a^2$ 是任意（aleatoric）[方差](@entry_id:200758)。

**[D-最优性](@entry_id:748151) (D-Optimality)**：在贝叶斯[线性模型](@entry_id:178302)的特定情况下，最大化互信息等价于一个被称为[D-最优性](@entry_id:748151)设计的准则 [@problem_id:3431856, @problem_id:3431842]。该准则旨在最大化**[费雪信息矩阵](@entry_id:750640) (Fisher Information Matrix)** 的[行列式](@entry_id:142978)。对于我们的模型，这相当于最大化后验[精度矩阵](@entry_id:264481) $\mathbf{S}_N^{-1}$ 的[行列式](@entry_id:142978)。每次添加一个新数据点 $\boldsymbol{\phi}_*$ 就是要最大化 $\log\det(\mathbf{S}_{\text{new}}^{-1})$。这个目标函数具有一个重要的数学性质——**[子模性](@entry_id:270750) (submodularity)** 。这意味着它表现出“收益递减”的特性：一个数据点对已有知识的边际贡献会随着知识的增加而减少。[子模性](@entry_id:270750)保证了使用简单的**贪心算法**（即一次选择一个最优的数据点）可以达到接近理论最优解（通常是[NP难问题](@entry_id:146946)）的效果，其性能下界为 $(1-1/e)$。

### [训练集](@entry_id:636396)的质量：覆盖度与多样性

仅仅依赖不确定性可能会导致采集策略选择一堆彼此非常相似但模型都不确定的点。一个高质量的[训练集](@entry_id:636396)不仅要包含[信息量](@entry_id:272315)大的点，还应该具有良好的**覆盖度 (coverage)** 和**多样性 (diversity)**。

-   **覆盖度**衡量训练集对整个目标[构型空间](@entry_id:149531)的代表性。可以用诸如**定向[豪斯多夫距离](@entry_id:152367) (directed Hausdorff distance)** 这样的度量来评估，它表示候选池中距离训练集最远的点有多远 。
-   **多样性**确保训练集中的点不会过于冗余。一个简单的度量是**最小成对距离 (minimum pairwise distance)**。更复杂的度量，如基于**核函数矩阵的[对数行列式](@entry_id:751430)**，可以量化点集在某个高维[特征空间](@entry_id:638014)中张成的“体积”，[行列式](@entry_id:142978)越大，多样性越好 。

在实践中，[采集函数](@entry_id:168889)通常是不确定性与多样性度量的某种结合，以确保高效探索和避免冗余采样。

### 实际应用与保障措施

将[主动学习](@entry_id:157812)应用于实际的[材料模拟](@entry_id:176516)（如分子动力学）需要考虑模型的可靠性和物理一致性。

#### 力匹配与能量一致性

物理上，力是[势能梯度](@entry_id:167095)的负值，$\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E$。一个物理上可靠的势函数必须同时准确地预测能量和力。因此，训练过程通常是**力匹配 (force matching)** 和**能量一致性 (energy consistency)** 的结合 [@problem_id:3431856, @problem_id:3431833]。这意味着线性系统 $A\mathbf{w} \approx \mathbf{y}$ 的行不仅包含能量的描述符，还包含由能量描述符对原子位置求导得到的力的描述符。通过联合拟合能量和力，可以构建出更稳健、物理性质更优的[势函数](@entry_id:176105)。

#### 外推检测与信任区域

[机器学习模型](@entry_id:262335)在其训练数据[分布](@entry_id:182848)范围内的预测最为可靠。当模型被要求对远离训练数据的构型（即**外推**）进行预测时，其结果可能非常不可靠。因此，在模拟过程中建立“护栏”或**保障措施 (safeguards)** 至关重要。

1.  **基于输出的检测**：最直接的方法是利用模型的预测不确定性。我们可以定义一个**信任区域 (trust region)**，如果一个构型的[认知不确定性](@entry_id:149866)超过了预设阈值，就认为模型在该点的预测是不可信的 [@problem_id:3431887, @problem_id:3431898]。

2.  **基于输入的检测**：另一种方法是直接在描述符空间中判断一个[新构型](@entry_id:199611)是否是“离群点”。**[马氏距离](@entry_id:269828) (Mahalanobis distance)** 和**统计杠杆值 (statistical leverage)** 是两种有效的度量，它们能量化一个查询点相对于训练数据[分布](@entry_id:182848)的“远近”程度，同时考虑了数据[分布](@entry_id:182848)的协[方差](@entry_id:200758)结构 [@problem_id:3431887, @problem_id:3431898]。

在分子动力学模拟中，这些保障措施可以用于实际决策。例如，当检测到模型即将进入不可信的外推区域时，可以触发一次新的DFT计算来增强模型，或者动态地减小模拟的**时间步长**以保证轨迹的稳定性 。

### 评估与[终止准则](@entry_id:136282)

主动学习过程不能无限进行下去。我们需要一个明确的**[终止准则](@entry_id:136282) (stopping criterion)** 来判断模型何时“足够好”。

一个好的[终止准则](@entry_id:136282)不应只看单一的准确性指标，如能量的**平均[绝对误差](@entry_id:139354) (MAE)** 或力的**[均方根误差](@entry_id:170440) (RMSE)**。它还应包括对[模型不确定性](@entry_id:265539)质量的评估 。

-   **不确定性校准 (Uncertainty Calibration)**：一个好的不确定性模型应该是“校准的”。这意味着其预测的置信区间应该与经验观察到的频率相符。例如，对于所有被预测为95%置信区间的预测，我们期望大约95%的真实值确实落在这个区间内。**期望校准误差 (Expected Calibration Error, ECE)** 是衡量这种偏差的常用指标 。
-   **[离群点检测](@entry_id:175858)**：通过检查**校准后的[标准化残差](@entry_id:634169)**，可以识别出模型预测与真实值差异极大的个别“离群点”。

一个稳健的[终止准则](@entry_id:136282)通常会结合多个条件：当能量和力的误差、校准误差以及最大离群点误差同时低于预设的容忍阈值时，主动学习循环便可以终止 。

### 高级主题：[多保真度学习](@entry_id:752239)

为了进一步提高效率，[主动学习](@entry_id:157812)可以扩展到**多保真度 (multi-fidelity)** 框架 。其核心思想是结合少量昂贵但精确的高保真度计算（如DFT）和大量廉价但不够精确的低保真度计算（如[经典力场](@entry_id:747367)或[半经验方法](@entry_id:176276)）。

通过构建一个[自回归模型](@entry_id:140558)，例如 $f_H(r) = \rho f_L(r) + \delta(r)$，其中高保真函数 $f_H$ 被建模为经过缩放的低保真函数 $f_L$ 加上一个差值（或称“缺陷”）函数 $\delta(r)$。贝叶斯框架可以自然地将来自不同保真度来源的数据统一起来，共同学习所有模型参数。此时，[采集函数](@entry_id:168889)也需要考虑成本因素，其目标变为最大化**单位成本的[信息增益](@entry_id:262008)** 。这种方法允许我们利用廉价计算来探索广阔的构型空间，同时用少量昂贵的计算来精确校准关键区域，从而以极高的效率构建出高质量的[势函数](@entry_id:176105)。