{
    "hands_on_practices": [
        {
            "introduction": "在材料数据库中，同一个晶体结构可以有无穷多种晶胞表示方式，这给结构比较和数据去重带来了挑战。为了解决这个问题，我们需要一个规范化的、唯一的表示方法，而Niggli约化晶胞就是这样一种广泛应用的标准。这个实践将指导你从基本定义出发，通过算法找到一个晶格的Niggli约化原胞，这是处理和标准化晶体学数据的一项基本功。",
            "id": "3463942",
            "problem": "您会收到一组案例，每个案例由一个实空间晶格基矢（一个由三个线性无关的笛卡尔坐标表示的晶格矢量组成的矩阵）和一组分数坐标表示的原子位置指定。您的任务是编写一个完整且可运行的程序，仅从关于布拉维晶格和整数基变换操作的核心定义出发，构建一个 Niggli 约化原胞，并将所有原子分数坐标一致地转换到该约化表示中。\n\n使用的基本原理和定义：\n- 三维布拉维晶格是由基底矩阵 $\\mathbf{L} \\in \\mathbb{R}^{3 \\times 3}$ 生成的矢量集合 $\\{\\mathbf{r} = \\mathbf{L}\\mathbf{f} \\mid \\mathbf{f} \\in \\mathbb{Z}^3\\}$，该矩阵的列是晶格矢量 $\\mathbf{a}$、$\\mathbf{b}$、$\\mathbf{c}$。度规张量为 $\\mathbf{G} = \\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$，其元素为 $G_{ij} = \\mathbf{v}_i \\cdot \\mathbf{v}_j$，其中 $\\mathbf{v}_1=\\mathbf{a}$、$\\mathbf{v}_2=\\mathbf{b}$、$\\mathbf{v}_3=\\mathbf{c}$。\n- 两个基底 $\\mathbf{L}$ 和 $\\mathbf{L}'$ 生成同一个晶格，当且仅当存在一个整数矩阵 $\\mathbf{S} \\in \\mathrm{GL}(3,\\mathbb{Z})$（整数上的一般线性群 (GL)，其行列式 $\\det(\\mathbf{S}) = \\pm 1$），使得 $\\mathbf{L}' = \\mathbf{L}\\mathbf{S}$。在此基变换下，分数坐标作逆变转换，即 $\\mathbf{f}' = \\mathbf{S}^{-1}\\mathbf{f}$。\n- 晶胞的六维 度规表示 (G6) 是由 $A = \\mathbf{a}\\cdot\\mathbf{a}$、$B = \\mathbf{b}\\cdot\\mathbf{b}$、$C = \\mathbf{c}\\cdot\\mathbf{c}$、$D = 2\\,\\mathbf{b}\\cdot\\mathbf{c}$、$E = 2\\,\\mathbf{a}\\cdot\\mathbf{c}$、$F = 2\\,\\mathbf{a}\\cdot\\mathbf{b}$ 定义的 6-元组 $(A,B,C,D,E,F)$。\n- 就本问题而言，一个标准的 Niggli 约化原胞可以被描述为任何可通过整数幺模变换 $\\mathbf{S}\\in\\mathrm{GL}(3,\\mathbb{Z})$ 到达的基底，且该基底满足：(i) 边长平方非递减，即 $A \\leq B \\leq C$；(ii) 轴间角为非锐角，因此 $D \\leq 0$, $E \\leq 0$, $F \\leq 0$（角度 $\\geq 90$ 度）；以及 (iii) 在所有此类变换中，6-元组 $(A,B,C,D,E,F)$ 是字典序最小的。您必须仅使用整数幺模变换来完成此任务。您可以假设，对于所提供的测试套件，在元素为 $\\{-1,0,1\\}$ 的整数幺模变换中进行搜索，加上基矢量的列置换和独立的符号反转，足以达到这样一个标准的代表。\n\n推导目标：\n- 从上述定义出发，推导出一个算法，该算法搜索 $\\mathrm{GL}(3,\\mathbb{Z})$ 的一个有限子集，将诱导的基变换应用于候选晶格，并通过强制排序 $A \\leq B \\leq C$、强制轴间角为非锐角（等效于 $D \\leq 0$, $E \\leq 0, F \\leq 0$）以及在满足这些条件的晶格中选择字典序最小的 $(A,B,C,D,E,F)$ 来选择标准的约化晶胞。然后，通过 $\\mathbf{f}'=\\mathbf{S}^{-1}\\mathbf{f}$ 变换所有原子分数坐标，并通过应用模 1 的分数部分映射将它们按分量约化到半开区间 $[0,1)$ 内。\n\n单位与数值约定：\n- 晶格矢量以埃（ångströms）为单位。在所有涉及长度的内部计算中，请一致使用埃。轴间角必须以度（degrees）为单位进行解释，并在需要时进行计算。您的程序的最终报告输出仅为布尔值和整数，因此输出中不需要单位注释。\n\n每个案例需要计算的验证标准：\n- 边长排序检查：返回一个布尔值，指示约化晶胞是否在数值容差内满足 $A \\leq B \\leq C$。\n- 非锐角检查：返回一个布尔值，指示所有三个轴间角是否都至少为 $90$ 度（等效于在容差范围内 $D \\leq 0$, $E \\leq 0, F \\leq 0$）。\n- 体积保持检查：返回一个布尔值，指示 $|\\det(\\mathbf{L})|$ 是否在容差范围内等于 $|\\det(\\mathbf{L}')|$。\n- 原子计数：返回变换后原子位置的整数数量（当仅使用幺模变换时，此数量必须等于输入数量）。\n- 单位立方体包含检查：返回一个布尔值，指示所有变换后的分数坐标是否在容差范围内按分量位于 $[0,1)$ 区间内。\n\n程序要求：\n- 仅使用此处陈述的定义和事实来实现上述算法。\n- 使用一个固定的较小数值容差（例如 $10^{-8}$）进行比较。\n- 对每个案例，生成一个形式为 $[\\text{sorted\\_edge},\\ \\text{non\\_acute\\_angles},\\ \\text{volume\\_preserved},\\ \\text{atom\\_count},\\ \\text{coords\\_in\\_unit}]$ 的结果列表。\n- 下面的测试套件需要硬编码到您的程序中。您的程序应该生成单行输出，其中包含所有案例的结果，格式为一个由逗号分隔的列表，并用方括号括起来（例如 `[[...],[...],...]`），每个案例的结果列表按顺序排列。\n\n测试套件（每个晶格由其三个列矢量给出；原子位置为分数坐标）：\n- 案例 1（简单立方，已约化）：\n  - $\\mathbf{L}_1$ 的列矢量为 $(3,0,0)$、$(0,3,0)$、$(0,0,3)$，单位均为埃。\n  - 分数坐标：$(0,0,0)$ 和 $(0.5,0.5,0.5)$。\n- 案例 2（正交晶胞，经过列置换和符号翻转）：\n  - 从 $(3,0,0)$、$(0,4,0)$、$(0,0,5)$ 开始，将列重新排列为 $(0,4,0)$、$(0,0,-5)$、$(3,0,0)$，以形成 $\\mathbf{L}_2$，单位为埃。\n  - 分数坐标：$(0.25,0.4,0.6)$ 和 $(0.9,0.1,0.3)$。\n- 案例 3（$\\gamma$ 角为锐角，必须转为非锐角）：\n  - $\\mathbf{L}_3$ 的列矢量为 $(3,0,0)$、$(1.5,2.598076211,0)$、$(0,0,4)$，单位为埃。\n  - 分数坐标：$(0,0,0)$ 和 $(1.2,-0.1,0.5)$。\n- 案例 4（边长相等 $a=b$ 且夹角为直角；包含超出范围的分数坐标分量以测试环绕处理）：\n  - $\\mathbf{L}_4$ 的列矢量为 $(3,0,0)$、$(0,3,0)$、$(0,0,4)$，单位为埃。\n  - 分数坐标：$(-0.25,1.75,0.0)$、$(0.5,0.5,0.5)$ 和 $(1.000000001, -1\\times 10^{-9}, 0.999999999)$。\n\n最终输出格式：\n- 您的程序应精确生成一行输出：一个外部列表，其中包含每个案例的一个内部列表，顺序与上述相同。每个内部列表的形式必须是 $[\\text{布尔值},\\ \\text{布尔值},\\ \\text{布尔值},\\ \\text{整数},\\ \\text{布尔值}]$，并且外部列表必须以标准的 Python 列表语法打印，元素之间用逗号分隔，无额外文本。",
            "solution": "用户提供的问题被评估为 **有效**。它在科学上基于晶体学和线性代数的原理，问题设定良好，目标和约束明确，并且表述客观。该问题要求实现一个算法，为给定的晶格找到一个 Niggli 约化原胞，并将原子坐标转换到这个新基底中，同时遵循一套特定的简化约化规则。\n\n该算法方法是基于布拉维晶格中的基变换原理和字典序下的优化而设计的。\n\n布拉维晶格由一组点 $\\mathbf{r} = n_1\\mathbf{a} + n_2\\mathbf{b} + n_3\\mathbf{c}$ 定义，其中 $n_i \\in \\mathbb{Z}$，而 $\\{\\mathbf{a}, \\mathbf{b}, \\mathbf{c}\\}$ 是三个线性无关的晶格矢量。这些矢量构成了基底矩阵 $\\mathbf{L} = [\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c}] \\in \\mathbb{R}^{3 \\times 3}$ 的列。晶格中的任何点都由 $\\mathbf{r} = \\mathbf{L}\\mathbf{f}$ 给出，其中 $\\mathbf{f} \\in \\mathbb{Z}^3$。晶胞的几何结构完全由度规张量 $\\mathbf{G} = \\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$ 描述，其元素是基矢量的点积，$G_{ij} = \\mathbf{v}_i \\cdot \\mathbf{v}_j$。\n\n有无限多个基底矩阵 $\\mathbf{L}$ 可以生成同一个晶格。两个基底 $\\mathbf{L}$ 和 $\\mathbf{L}'$ 描述同一个晶格，如果它们通过一个幺模整数变换矩阵 $\\mathbf{S} \\in \\mathrm{GL}(3,\\mathbb{Z})$ 相关联，使得 $\\mathbf{L}' = \\mathbf{L}\\mathbf{S}$。矩阵 $\\mathbf{S}$ 必须具有整数元素，且行列式为 $\\det(\\mathbf{S}) = \\pm 1$。由 $|\\det(\\mathbf{L})|$ 给出的单胞体积在这种变换下是不变的，因为 $|\\det(\\mathbf{L}')| = |\\det(\\mathbf{L}\\mathbf{S})| = |\\det(\\mathbf{L})||\\det(\\mathbf{S})| = |\\det(\\mathbf{L})|$。当基底通过 $\\mathbf{S}$ 变换时，任何分数坐标矢量 $\\mathbf{f}$ 都必须作逆变转换，即 $\\mathbf{f}' = \\mathbf{S}^{-1}\\mathbf{f}$，以确保实空间位置 $\\mathbf{r} = \\mathbf{L}\\mathbf{f} = \\mathbf{L}'\\mathbf{f}'$ 保持不变。\n\n该问题要求根据一套简化的规则找到一个“标准”的 Niggli 约化晶胞。这是通过搜索一个最优变换矩阵 $\\mathbf{S}$ 来实现的，该矩阵将给定的初始基底 $\\mathbf{L}$ 映射到一个满足特定标准的约化基底 $\\mathbf{L}'$。问题提供了一个关键的简化：对 $\\mathbf{S}$ 的搜索可以限制在一个有限集合内，该集合包含所有元素在 $\\{-1, 0, 1\\}$ 中且行列式为 $\\pm 1$ 的 $3 \\times 3$ 矩阵。\n\n算法流程如下：\n首先，生成所有候选变换矩阵的静态集合 $\\mathcal{S}$。这是通过遍历所有 $3^9 = 19,683$ 个可能的元素在 $\\{-1, 0, 1\\}$ 中的 $3 \\times 3$ 矩阵，并仅保留那些行列式在小的数值容差内接近 $\\pm 1$ 的矩阵来完成的。这个集合 $\\mathcal{S}$ 是预先计算的，并用于所有测试案例。\n\n对于每个由初始晶格 $\\mathbf{L}_{\\text{initial}}$ 和一组初始分数坐标 $\\{\\mathbf{f}_{\\text{initial}}\\}$ 指定的测试案例：\n1.  初始化一个变量 `best_g6`，用于保存迄今为止找到的字典序最小的合格 G6 向量，以及相应的 `best_L` 和 `best_S`。\n2.  遍历 $\\mathcal{S}$ 中的每个变换矩阵 $\\mathbf{S}$。\n3.  对于每个 $\\mathbf{S}$，计算候选的变换后基底 $\\mathbf{L}_{\\text{cand}} = \\mathbf{L}_{\\text{initial}}\\mathbf{S}$。\n4.  从 $\\mathbf{L}_{\\text{cand}}$ 计算度规张量 $\\mathbf{G}_{\\text{cand}} = \\mathbf{L}_{\\text{cand}}^{\\mathsf{T}}\\mathbf{L}_{\\text{cand}}$ 和相应的 G6 向量 $(A,B,C,D,E,F)$，其中 $A=\\mathbf{a}'\\cdot\\mathbf{a}'$, $B=\\mathbf{b}'\\cdot\\mathbf{b}'$, $C=\\mathbf{c}'\\cdot\\mathbf{c}'$, $D=2\\,\\mathbf{b}'\\cdot\\mathbf{c}'$, $E=2\\,\\mathbf{a}'\\cdot\\mathbf{c}'$, and $F=2\\,\\mathbf{a}'\\cdot\\mathbf{b}'$。\n5.  将问题陈述中的约化标准应用于候选 G6 向量：\n    a.  边长平方排序：$A \\le B \\le C$。\n    b.  非锐角轴间角：$D \\le 0, E \\le 0, F \\le 0$。\n    数值比较使用容差 $\\epsilon = 10^{-8}$ 进行。\n6.  如果一个候选者满足这些条件，则将其 G6 向量与 `best_g6` 进行字典序比较。如果 `best_g6` 尚未设置或候选 G6 严格更小，则用当前候选者的属性更新 `best_g6`、`best_L` 和 `best_S`。\n7.  在搜索完 $\\mathcal{S}$ 中所有的 $\\mathbf{S}$ 之后，最终的 `best_L` 和 `best_S` 代表了 Niggli 约化晶胞以及产生它的变换。\n\n一旦找到最优变换 $\\mathbf{S}_{\\text{final}}$，就执行验证检查：\n1.  **边长排序检查**：验证最终晶胞的 G6 参数是否在容差内满足 $A \\le B \\le C$。\n2.  **非锐角检查**：验证是否在容差内满足 $D \\le 0, E \\le 0, F \\le 0$。\n3.  **体积保持检查**：确认 $|\\det(\\mathbf{L}_{\\text{initial}})|$ 是否在容差内等于 $|\\det(\\mathbf{L}_{\\text{final}})|$。\n4.  **原子计数**：原子数就是初始分数坐标向量的数量。\n5.  **单位立方体包含检查**：初始分数坐标通过 $\\mathbf{f}_{\\text{final}} = \\mathbf{S}_{\\text{final}}^{-1}\\mathbf{f}_{\\text{initial}}$进行变换。然后，$\\mathbf{f}_{\\text{final}}$ 的每个分量使用模运算符 $f'' = f' \\pmod 1$ 映射到区间 $[0, 1)$。一个布尔检查确认所有结果坐标向量的所有分量都在 $[0, 1)$ 内，并具有适当的数值容差，具体检查每个环绕处理后的坐标 $c$ 是否满足 $c \\ge -\\epsilon$ 和 $c  1.0$。\n\n这个系统化的过程保证了能找到问题所定义的标准晶胞，并正确地变换所有相关属性以进行验证。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\n# Define a global tolerance for floating point comparisons.\nTOL = 1e-8\n\ndef get_s_matrices():\n    \"\"\"\n    Generates and returns the set of all 3x3 matrices with entries in {-1, 0, 1}\n    and determinant of +/-1. This set is pre-calculated once.\n    \"\"\"\n    s_matrices = []\n    for p in product([-1, 0, 1], repeat=9):\n        s = np.array(p, dtype=np.int32).reshape(3, 3)\n        det_s = np.linalg.det(s)\n        if np.isclose(det_s, 1.0) or np.isclose(det_s, -1.0):\n            s_matrices.append(s)\n    return s_matrices\n\n# Pre-compute the set of valid transformation matrices.\nS_MATRICES = get_s_matrices()\n\ndef get_g6_from_L(L):\n    \"\"\"\n    Calculates the 6-dimensional metric representation (G6) from a basis matrix L.\n    G6 = (a*a, b*b, c*c, 2*b*c, 2*a*c, 2*a*b)\n    \"\"\"\n    G = L.T @ L\n    A = G[0, 0]\n    B = G[1, 1]\n    C = G[2, 2]\n    D = 2 * G[1, 2]\n    E = 2 * G[0, 2]\n    F = 2 * G[0, 1]\n    return (A, B, C, D, E, F)\n\ndef solve_case(L_initial, f_initial):\n    \"\"\"\n    Finds the Niggli-reduced cell for a given lattice and performs validation checks.\n    \"\"\"\n    best_g6 = None\n    best_L = None\n    best_S = None\n\n    for s_matrix in S_MATRICES:\n        L_candidate = L_initial @ s_matrix\n        g6_candidate = get_g6_from_L(L_candidate)\n        A, B, C, D, E, F = g6_candidate\n\n        # Condition (i): Sorted edge-length squares (A = B = C)\n        if not (A = B + TOL and B = C + TOL):\n            continue\n        \n        # Condition (ii): Non-acute interaxial angles (D, E, F = 0)\n        if not (D = TOL and E = TOL and F = TOL):\n            continue\n\n        # Condition (iii): Lexicographically minimal G6 vector\n        if best_g6 is None or g6_candidate  best_g6:\n            best_g6 = g6_candidate\n            best_L = L_candidate\n            best_S = s_matrix\n    \n    # After checking all S, we have the best reduced cell\n    L_final = best_L\n    S_final = best_S\n    g6_final = best_g6\n\n    # --- Perform Validation Checks ---\n    \n    # 1. Sorted-edge check\n    A_f, B_f, C_f, _, _, _ = g6_final\n    sorted_edge = (A_f = B_f + TOL) and (B_f = C_f + TOL)\n\n    # 2. Non-acute-angles check\n    _, _, _, D_f, E_f, F_f = g6_final\n    non_acute_angles = (D_f = TOL) and (E_f = TOL) and (F_f = TOL)\n\n    # 3. Volume preservation check\n    vol_initial = np.abs(np.linalg.det(L_initial))\n    vol_final = np.abs(np.linalg.det(L_final))\n    volume_preserved = np.isclose(vol_initial, vol_final, atol=TOL, rtol=TOL)\n\n    # 4. Atom count\n    f_initial_arr = np.array(f_initial)\n    if f_initial_arr.ndim == 1:\n        atom_count = 1 if f_initial_arr.size  0 else 0\n    else:\n        atom_count = f_initial_arr.shape[0]\n\n    # 5. Unit-cube containment check\n    coords_in_unit = True # Default to True for case with no atoms\n    if atom_count  0:\n        S_inv = np.linalg.inv(S_final)\n        # Transform coords: S_inv is 3x3, f_initial_arr.T is 3xN_atoms\n        f_transformed = (S_inv @ f_initial_arr.T).T\n        # Reduce components to [0, 1) interval\n        f_wrapped = np.mod(f_transformed, 1.0)\n        # Check if all components are in [0, 1) within tolerance\n        # Check >= -TOL for lower bound robustness, and  1.0 for upper bound\n        coords_in_unit = bool(np.all((f_wrapped = -TOL)  (f_wrapped  1.0)))\n\n    return [sorted_edge, non_acute_angles, volume_preserved, atom_count, coords_in_unit]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": np.array([[3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype=float).T,\n            \"f\": [[0, 0, 0], [0.5, 0.5, 0.5]]\n        },\n        {\n            \"L\": np.array([[0, 4, 0], [0, 0, -5], [3, 0, 0]], dtype=float).T,\n            \"f\": [[0.25, 0.4, 0.6], [0.9, 0.1, 0.3]]\n        },\n        {\n            \"L\": np.array([[3, 0, 0], [1.5, 2.598076211, 0], [0, 0, 4]], dtype=float).T,\n            \"f\": [[0, 0, 0], [1.2, -0.1, 0.5]]\n        },\n        {\n            \"L\": np.array([[3, 0, 0], [0, 3, 0], [0, 0, 4]], dtype=float).T,\n            \"f\": [[-0.25, 1.75, 0.0], [0.5, 0.5, 0.5], [1.000000001, -1e-9, 0.999999999]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        L_initial = case[\"L\"]\n        f_initial = case[\"f\"]\n        result_list = solve_case(L_initial, f_initial)\n        results.append(result_list)\n\n    # Final print statement in the exact required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "当我们利用机器学习预测材料性质时，一个常见的陷阱是数据泄露，尤其是在处理包含相同化学成分的多种晶型（polymorphs）的数据集时。如果不加注意，模型可能在训练时“窥探”到测试集中的信息，从而导致评估结果过于乐观。这个实践将引导你实现分组交叉验证（grouped cross-validation），这是一种确保模型泛化能力得到公正评估的关键技术，要求同一组分的所有多晶型物必须被划分到同一数据折中。",
            "id": "3463892",
            "problem": "给定一个合成材料数据集，该数据集旨在模拟一个材料数据库，其中每个化学成分包含多个多晶型物（多晶型物是相同化学成分但具有不同晶体结构的物质）。任务目标是实现分组交叉验证，以确保同一组分的所有多晶型物都保留在同一个折叠（fold）中，训练一个用于预测生成能的回归模型，并报告带有 $95\\%$ 置信区间的平均绝对误差（MAE）。此任务旨在通过确保共享组分级别描述符的多晶型物不会被分割到不同的训练集和测试集折叠中，从而防止信息泄露。\n\n数据集构建与物理真实性：\n- 共有 $12$ 个不同的组分（组），由整数 $0,1,\\dots,11$ 索引。\n- 每个组分恰好有 $6$ 个多晶型物，因此总共有 $72$ 个样本。\n- 每个组分 $g$ 都有一个组分级别描述符 $\\mathbf{c}_g \\in \\mathbb{R}^3$，该描述符在其所有多晶型物中都是固定的，并使用固定的随机种子从 $[0,1]^3$ 的均匀分布中生成，以确保可复现性。\n- 组分 $g$ 内的每个多晶型物都有一个结构级别描述符 $\\mathbf{s}_{g,j} \\in \\mathbb{R}^2$，该描述符是使用相同的固定随机种子从 $[0,1]^2$ 的均匀分布中独立生成的，以确保可复现性。\n- 每个原子的真实生成能由一个带有截距和加性噪声的线性模型生成：\n$$\nE_f(g,j) = b + \\mathbf{c}_g \\cdot \\mathbf{w}_c + \\mathbf{s}_{g,j} \\cdot \\mathbf{w}_s + \\varepsilon_{g,j},\n$$\n其中 $b=-2.0$（单位为 $\\mathrm{eV/atom}$），$\\mathbf{w}_c = [0.8,-0.6,0.4]$（单位为 $\\mathrm{eV}$/描述符单位），$\\mathbf{w}_s = [0.5,-0.3]$（单位为 $\\mathrm{eV}$/描述符单位），而 $\\varepsilon_{g,j} \\sim \\mathcal{N}(0,\\sigma^2)$ 是加性高斯噪声，其标准差 $\\sigma$ 按每个测试用例指定。所有能量单位均为每个原子的电子伏特（$\\mathrm{eV/atom}$）。\n\n建模与评估要求：\n- 实现分组 $K$ 折交叉验证（CV），其中组是组分索引。属于同一组分的所有多晶型物必须被分配到同一个折叠中。\n- 对每个折叠，训练一个用于预测生成能的线性回归模型，该模型最小化残差平方和，并在权重上施加强度为 $\\lambda$ 的 $\\ell_2$（欧几里得）惩罚项，但不惩罚截距。学习到的模型必须能在留出的折叠上预测生成能。使用标准线性代数来获得解；截距必须被正确处理并从正则化中排除。\n- 计算每折的平均绝对误差（MAE），即测试折叠中所有样本的 $|E_f^{\\mathrm{pred}} - E_f^{\\mathrm{true}}|$ 的平均值。通过对各折叠的 MAE 求平均，将它们汇总为总体 MAE。\n- 通过将每折的 MAE 视为 $K$ 个样本，并使用具有 $K-1$ 个自由度的学生 $t$ 分布，来计算总体 MAE 的 $95\\%$ 置信区间。具体来说，如果 $m_k$ 是 $k=1,\\dots,K$ 时每折的 MAE，样本均值为 $\\bar{m}$，样本标准差为 $s$，则报告区间 $\\left[\\bar{m} - t^\\star \\frac{s}{\\sqrt{K}}, \\bar{m} + t^\\star \\frac{s}{\\sqrt{K}}\\right]$，其中 $t^\\star$ 是具有 $K-1$ 个自由度的学生 $t$ 分布的 $0.975$ 分位数。\n- 所有误差指标必须以 $\\mathrm{eV/atom}$ 为单位报告。不使用角度；如果您的实现中出现任何三角函数，其角度必须以弧度为单位。\n\n实现细节与可复现性：\n- 使用固定的种子 $0$ 来生成所有组分和结构描述符，以使特征在所有测试用例中都相同。\n- 为每个测试用例的加性高斯噪声使用不同的噪声种子，以在改变噪声水平的同时保证目标值的可复现性。\n- 使用固定的种子 $99$ 来将组打乱并划分为 $K$ 个折叠。\n\n测试套件：\n实现您的程序以运行以下三个测试用例，每个用例由元组 $(K,\\lambda,\\sigma,\\text{noise\\_seed})$ 指定：\n1. $K=3$, $\\lambda=0.1$, $\\sigma=0.05$, $\\text{noise\\_seed}=11$。\n2. $K=2$, $\\lambda=0.0$, $\\sigma=0.10$, $\\text{noise\\_seed}=22$。\n3. $K=4$, $\\lambda=1.0$, $\\sigma=0.20$, $\\text{noise\\_seed}=33$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，每个测试用例贡献一个三元素列表 $[\\bar{m}, \\mathrm{lower}, \\mathrm{upper}]$，其中的浮点数单位为 $\\mathrm{eV/atom}$。例如，输出必须类似于\n$$\n[[\\bar{m}_1,\\mathrm{lower}_1,\\mathrm{upper}_1],[\\bar{m}_2,\\mathrm{lower}_2,\\mathrm{upper}_2],[\\bar{m}_3,\\mathrm{lower}_3,\\mathrm{upper}_3]]\n$$\n打印在单行上，不含任何额外文本。",
            "solution": "核心的科学原理是通过确保共享相同组分级别描述符的多晶型物不被分割到训练集和测试集中，从而避免信息泄露，因为这样做会使模型能通过其他多晶型物间接获取留出组分的描述符。分组 $K$ 折交叉验证（CV）通过根据组标签（此处为组分）对数据进行划分，直接强制执行了这一点。\n\n我们从经验风险最小化开始。设 $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ 表示样本 $i$ 的描述符向量，它是由组分描述符 $\\mathbf{c}_g \\in \\mathbb{R}^{3}$ 和结构描述符 $\\mathbf{s}_{g,j} \\in \\mathbb{R}^{2}$ 拼接而成的，因此 $d=5$。设 $y_{i} \\in \\mathbb{R}$ 为生成能，单位为 $\\mathrm{eV/atom}$。带有截距 $b$ 和权重 $\\mathbf{w} \\in \\mathbb{R}^{d}$ 的岭回归目标函数为\n$$\nJ(b,\\mathbf{w}) = \\sum_{i=1}^{n} \\left(y_i - b - \\mathbf{x}_i^\\top \\mathbf{w}\\right)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是正则化强度，截距 $b$ 不受惩罚。这是一个凸二次问题，其最小化器满足正规方程。定义增广设计矩阵\n$$\n\\mathbf{X}_{\\mathrm{aug}} = \\begin{bmatrix}\n\\mathbf{1}  \\mathbf{X}\n\\end{bmatrix},\n$$\n其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 堆叠了 $\\mathbf{x}_i^\\top$，$\\mathbf{1} \\in \\mathbb{R}^{n}$ 是一个全为 1 的列向量，并定义增广参数向量\n$$\n\\boldsymbol{\\beta} = \\begin{bmatrix}\nb \\\\ \\mathbf{w}\n\\end{bmatrix} \\in \\mathbb{R}^{d+1},\n$$\n一阶最优性条件得出\n$$\n\\left(\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}} + \\mathbf{L}\\right)\\boldsymbol{\\beta} = \\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{y},\n$$\n其中 $\\mathbf{L} \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ 是一个对角矩阵，其截距对应的条目为 0，其余 $d$ 个对角线上的条目为 $\\lambda$。当 $\\lambda>0$ 时，解这个线性系统可以得到唯一的最小化器；当 $\\lambda=0$ 且 $\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}}$ 可逆时，得到普通最小二乘（OLS）解。对测试集的预测通过 $\\hat{\\mathbf{y}} = \\mathbf{X}_{\\mathrm{aug,test}} \\boldsymbol{\\beta}$ 获得。\n\n对于分组 $K$ 折交叉验证，设数据集由 $i=1,\\dots,n$ 索引，并设 $g(i)$ 为样本 $i$ 的组标签（组分索引）。我们使用一个固定的种子随机排列 $G$ 个唯一的组，并将它们划分为 $K$ 个大小几乎相等的不相交折叠 $\\mathcal{G}_1,\\dots,\\mathcal{G}_K$。对于每个折叠 $k$，训练集是组标签不在 $\\mathcal{G}_k$ 中的所有样本，测试集是组标签在 $\\mathcal{G}_k$ 中的所有样本。这确保了对于任何组分 $g$，其所有多晶型物在给定折叠中要么全部在训练集中，要么全部在测试集中。\n\n第 $k$ 折上的平均绝对误差（MAE）定义为\n$$\nm_k = \\frac{1}{|\\mathcal{I}_k|} \\sum_{i \\in \\mathcal{I}_k} \\left| \\hat{y}_i - y_i \\right|,\n$$\n其中 $\\mathcal{I}_k$ 是测试集折叠 $k$ 中样本的索引集。总体 MAE 是各折叠的平均值，\n$$\n\\bar{m} = \\frac{1}{K} \\sum_{k=1}^{K} m_k.\n$$\n为了获得 $\\bar{m}$ 的 $95\\%$ 置信区间，我们将 $\\{m_k\\}_{k=1}^K$ 视为 $K$ 个样本，并依赖于学生 $t$ 分布。计算样本标准差\n$$\ns = \\sqrt{\\frac{1}{K-1}\\sum_{k=1}^{K} \\left(m_k - \\bar{m}\\right)^2},\n$$\n并设 $t^\\star$ 为具有 $K-1$ 个自由度的学生 $t$ 分布的 $0.975$ 分位数。则 $95\\%$ 置信区间为\n$$\n\\left[\\bar{m} - t^\\star \\frac{s}{\\sqrt{K}}, \\bar{m} + t^\\star \\frac{s}{\\sqrt{K}}\\right].\n$$\n所有误差值的单位均为 $\\mathrm{eV/atom}$。\n\n与上述原则一致的算法步骤：\n1.  对于 $g=0,\\dots,11$，使用种子 0 从 $[0,1]^3$ 上的均匀分布中生成组分描述符 $\\mathbf{c}_g$。对于 $j=1,\\dots,6$，使用相同的种子从 $[0,1]^2$ 上的均匀分布中生成结构描述符 $\\mathbf{s}_{g,j}$，以确保特征是固定的。形成 $\\mathbf{x}_{g,j} = [\\mathbf{c}_g; \\mathbf{s}_{g,j}] \\in \\mathbb{R}^{5}$。\n2.  对于每个测试用例，使用 $E_f(g,j) = b + \\mathbf{c}_g \\cdot \\mathbf{w}_c + \\mathbf{s}_{g,j} \\cdot \\mathbf{w}_s + \\varepsilon_{g,j}$ 和指定的 $\\sigma$ 及噪声种子生成目标值，得到单位为 $\\mathrm{eV/atom}$ 的 $y_{g,j}$。\n3.  使用种子 99 将组划分为 $K$ 个折叠。对于每个折叠，通过求解 $\\left(\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}} + \\mathbf{L}\\right)\\boldsymbol{\\beta} = \\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{y}$ 在训练集上拟合岭回归，其中 $\\mathbf{L}=\\mathrm{diag}(0,\\lambda,\\dots,\\lambda)$ 以将截距从正则化中排除。\n4.  计算每折的 MAE $m_k$，然后计算 $\\bar{m}$、$s$ 和使用具有 $K-1$ 个自由度的学生 $t$ 分布的 $95\\%$ 置信区间。\n5.  对所有测试用例重复此过程，并在单行上打印由三个用例的三元组 $[\\bar{m},\\mathrm{lower},\\mathrm{upper}]$ 组成的列表。\n\n此设计基于带有 $\\ell_2$ 正则化的经验风险最小化、对截距的正确处理、通过学生 $t$ 分布进行的统计推断，以及为防止共享组分描述符的多晶型物之间的信息泄露而进行的分组感知验证。以 $\\mathrm{eV/atom}$ 为单位报告 MAE 及其置信区间，符合计算材料科学中有物理意义的单位。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef generate_features(num_groups=12, polymorphs_per_group=6, comp_dim=3, struct_dim=2, seed=0):\n    \"\"\"\n    Generate fixed composition and structure descriptors for reproducibility.\n    Returns:\n        X: (N, d) feature matrix where d = comp_dim + struct_dim\n        groups: (N,) array of group labels (composition indices)\n        comp_desc: (num_groups, comp_dim) array of composition descriptors\n        struct_desc: (N, struct_dim) array of structure descriptors\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    comp_desc = rng.uniform(0.0, 1.0, size=(num_groups, comp_dim))\n    N = num_groups * polymorphs_per_group\n    struct_desc = rng.uniform(0.0, 1.0, size=(N, struct_dim))\n\n    # Build full features by concatenating composition and structure descriptors\n    d = comp_dim + struct_dim\n    X = np.zeros((N, d))\n    groups = np.zeros(N, dtype=int)\n\n    idx = 0\n    for g in range(num_groups):\n        c = comp_desc[g]\n        for j in range(polymorphs_per_group):\n            s = struct_desc[idx]\n            X[idx, :] = np.concatenate([c, s])\n            groups[idx] = g\n            idx += 1\n    return X, groups, comp_desc, struct_desc\n\ndef generate_targets(comp_desc, struct_desc, num_groups=12, polymorphs_per_group=6,\n                     w_c=np.array([0.8, -0.6, 0.4]),\n                     w_s=np.array([0.5, -0.3]),\n                     b=-2.0, sigma=0.05, noise_seed=11):\n    \"\"\"\n    Generate formation energy targets based on ground-truth linear model and Gaussian noise.\n    Returns:\n        y: (N,) target vector in eV/atom\n    \"\"\"\n    N = num_groups * polymorphs_per_group\n    y = np.zeros(N)\n    rng = np.random.RandomState(noise_seed)\n    idx = 0\n    for g in range(num_groups):\n        c = comp_desc[g]\n        for j in range(polymorphs_per_group):\n            s = struct_desc[idx]\n            mean_val = b + np.dot(c, w_c) + np.dot(s, w_s)\n            noise = rng.normal(loc=0.0, scale=sigma)\n            y[idx] = mean_val + noise\n            idx += 1\n    return y\n\ndef grouped_kfold(groups, K, seed=99):\n    \"\"\"\n    Create grouped K-fold splits: returns list of (train_indices, test_indices).\n    Ensures all samples of the same group are in the same fold.\n    \"\"\"\n    unique_groups = np.unique(groups)\n    rng = np.random.RandomState(seed)\n    shuffled = unique_groups.copy()\n    rng.shuffle(shuffled)\n    n_groups = len(shuffled)\n    # Partition groups into K folds as evenly as possible\n    fold_sizes = np.full(K, n_groups // K, dtype=int)\n    fold_sizes[: (n_groups % K)] += 1\n    folds = []\n    start = 0\n    for fs in fold_sizes:\n        folds.append(shuffled[start:start+fs])\n        start += fs\n\n    # Map groups to sample indices\n    group_to_indices = {g: np.where(groups == g)[0] for g in unique_groups}\n    splits = []\n    all_indices = np.arange(len(groups))\n    for test_groups in folds:\n        test_idx_list = []\n        for g in test_groups:\n            test_idx_list.append(group_to_indices[g])\n        test_indices = np.concatenate(test_idx_list)\n        train_mask = np.ones(len(groups), dtype=bool)\n        train_mask[test_indices] = False\n        train_indices = all_indices[train_mask]\n        splits.append((train_indices, test_indices))\n    return splits\n\ndef ridge_fit(X, y, lam):\n    \"\"\"\n    Fit ridge regression with intercept unpenalized.\n    Solve (X_aug^T X_aug + L) beta = X_aug^T y, where L has 0 for intercept and lam for weights.\n    Returns:\n        beta: (d+1,) parameter vector [intercept, weights...]\n    \"\"\"\n    n, d = X.shape\n    X_aug = np.hstack([np.ones((n, 1)), X])\n    # Regularization matrix L: 0 for intercept, lam for other coefficients\n    L = np.diag(np.concatenate(([0.0], np.full(d, lam))))\n    A = X_aug.T @ X_aug + L\n    b = X_aug.T @ y\n    beta = np.linalg.solve(A, b)\n    return beta\n\ndef evaluate_case(K, lam, sigma, noise_seed,\n                  num_groups=12, polymorphs_per_group=6,\n                  comp_dim=3, struct_dim=2):\n    \"\"\"\n    Evaluate grouped K-fold CV for one case.\n    Returns:\n        (mae_mean, ci_lower, ci_upper)\n    \"\"\"\n    # Fixed features across cases\n    X, groups, comp_desc, struct_desc = generate_features(\n        num_groups=num_groups,\n        polymorphs_per_group=polymorphs_per_group,\n        comp_dim=comp_dim,\n        struct_dim=struct_dim,\n        seed=0\n    )\n    # Targets for this case\n    y = generate_targets(\n        comp_desc=comp_desc,\n        struct_desc=struct_desc,\n        num_groups=num_groups,\n        polymorphs_per_group=polymorphs_per_group,\n        b=-2.0,\n        w_c=np.array([0.8, -0.6, 0.4]),\n        w_s=np.array([0.5, -0.3]),\n        sigma=sigma,\n        noise_seed=noise_seed\n    )\n\n    splits = grouped_kfold(groups, K=K, seed=99)\n    fold_maes = []\n    for train_idx, test_idx in splits:\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_test, y_test = X[test_idx], y[test_idx]\n        beta = ridge_fit(X_train, y_train, lam=lam)\n        # Predict\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        y_pred = X_test_aug @ beta\n        mae = float(np.mean(np.abs(y_pred - y_test)))\n        fold_maes.append(mae)\n\n    # Aggregate MAE and compute 95% CI via Student's t\n    K_float = float(K)\n    mae_mean = float(np.mean(fold_maes))\n    if K  1:\n        s = float(np.std(fold_maes, ddof=1))\n        t_star = float(t.ppf(0.975, df=K - 1))\n        margin = t_star * s / np.sqrt(K_float)\n    else:\n        # Degenerate case; no CI with a single fold, margin set to 0\n        margin = 0.0\n\n    ci_lower = mae_mean - margin\n    ci_upper = mae_mean + margin\n    return [mae_mean, ci_lower, ci_upper]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (K, lambda, sigma, noise_seed)\n    test_cases = [\n        (3, 0.1, 0.05, 11),\n        (2, 0.0, 0.10, 22),\n        (4, 1.0, 0.20, 33),\n    ]\n\n    results = []\n    for case in test_cases:\n        K, lam, sigma, noise_seed = case\n        result = evaluate_case(K=K, lam=lam, sigma=sigma, noise_seed=noise_seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "图神经网络（GNNs）为原子尺度的材料建模提供了一个强大的框架，因为它能自然地表示原子间的连接关系。为了构建更精确、更具泛化能力的模型，我们可以将基本的物理定律（如电荷守恒）作为归纳偏置直接编码到网络架构中。这个高级实践将带你从零开始构建一个GNN，并设计一个特殊的“消息传递”机制来强制执行电荷守恒约束，从而探索物理知识如何指导和提升机器学习模型的性能。",
            "id": "3463928",
            "problem": "你的任务是根据第一性原理，实现一个最小化的图神经网络（GNN），用于预测以图形式表示的小型无机化合物的氧化态。该设计必须通过约束有向边消息函数来显式地编码电荷守恒，以使得对于每个源节点 $j$，其发出的消息满足 $\\sum_{i} m_{ji} = 0$。你必须实现并比较两种模型：（i）一个无约束的基线模型，以及（ii）一个在消息层面强制实施电荷守恒的约束模型。目标是测试此约束对氧化态预测准确性的影响。\n\n从以下基础开始：\n\n- 材料结构可以表示为一个无向图 $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$，其中 $\\mathcal{V}$ 是节点集，$\\mathcal{E}$ 是边集。每个节点 $i \\in \\mathcal{V}$ 带有一个特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^{d_{\\text{in}}}$，该向量包括原子序数和泡林电负性。\n- 图消息传递在边上使用有向消息。对于每条无向边 $(j,i) \\in \\mathcal{E}$，构造两条有向边 $j \\to i$ 和 $i \\to j$。一个标量消息 $m_{ji}$ 由一个可学习的函数 $f_{\\theta}$ 产生，该函数应用于源节点和目标节点的嵌入。\n- 消息层面的电荷守恒要求对于每个源节点 $j$，其发出的消息遵循 $\\sum_{i \\in \\mathcal{N}(j)} m_{ji} = 0$，其中 $\\mathcal{N}(j)$ 是 $j$ 的邻居节点集合。\n\n你必须根据以下架构和约束，推导、实现并训练这两种模型：\n\n- 节点嵌入：$\\mathbf{h}_i = W_{\\text{node}} \\mathbf{x}_i$，其中 $W_{\\text{node}} \\in \\mathbb{R}^{d_h \\times d_{\\text{in}}}$。\n- 边消息预激活：$\\mathbf{u}_{ji} = W_{\\phi} [\\mathbf{h}_j ; \\mathbf{h}_i] + \\mathbf{b}_{\\phi}$，其中 $W_{\\phi} \\in \\mathbb{R}^{d_m \\times 2d_h}$ 且 $\\mathbf{b}_{\\phi} \\in \\mathbb{R}^{d_m}$，并且 $[\\cdot ; \\cdot]$ 表示拼接。\n- 边隐藏层：$\\mathbf{z}_{ji} = \\tanh(\\mathbf{u}_{ji})$。\n- 原始标量消息：$r_{ji} = \\mathbf{v}^{\\top} \\mathbf{z}_{ji} + b_m$，其中 $\\mathbf{v} \\in \\mathbb{R}^{d_m}$ 且 $b_m \\in \\mathbb{R}$。\n- 约束消息：$m_{ji} = r_{ji} - \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk}$，其中 $\\deg(j)$ 是节点 $j$ 的出度。这确保了对每个 $j$ 都有 $\\sum_{i} m_{ji} = 0$。无约束基线模型使用 $m_{ji} = r_{ji}$。\n- 预测电荷累积：$q_i = \\sum_{j \\in \\mathcal{N}(i)} m_{ji}$（仅传入消息）。\n- 预测氧化态：$y_i = \\alpha q_i$，其中 $\\alpha \\in \\mathbb{R}$ 是一个可学习的标量。\n- 在具有节点标签 $o_i$ 的图上的训练目标是均方误差（MSE）：$L = \\frac{1}{|\\mathcal{V}|} \\sum_{i \\in \\mathcal{V}} (y_i - o_i)^2$。\n\n你必须使用链式法则，通过反向传播推导出所有参数的梯度。特别是，你必须展示守恒约束如何影响相对于原始边消息 $r_{ji}$ 的梯度，并进而影响上游参数。\n\n通过对一个小数据集进行批量梯度下降来实现训练，该数据集由以下化合物和氧化态（假设为典型的形式氧化态）构建的图组成：\n\n- 氯化钠 (NaCl)：节点 $\\{ \\text{Na}, \\text{Cl} \\}$，氧化态 $[+1,-1]$。\n- 氧化镁 (MgO)：节点 $\\{ \\text{Mg}, \\text{O} \\}$，氧化态 $[+2,-2]$。\n- 氧化铝 (Al$_2$O$_3$)：节点 $\\{ \\text{Al}, \\text{Al}, \\text{O}, \\text{O}, \\text{O} \\}$，氧化态 $[+3,+3,-2,-2,-2]$。\n- 氧化亚铁 (FeO)：节点 $\\{ \\text{Fe}, \\text{O} \\}$，氧化态 $[+2,-2]$。\n- 氧化铁 (Fe$_2$O$_3$)：节点 $\\{ \\text{Fe}, \\text{Fe}, \\text{O}, \\text{O}, \\text{O} \\}$，氧化态 $[+3,+3,-2,-2,-2]$。\n- 氧化钠 (Na$_2$O)：节点 $\\{ \\text{Na}, \\text{Na}, \\text{O} \\}$，氧化态 $[+1,+1,-2]$。\n- 氮化铝 (AlN)：节点 $\\{ \\text{Al}, \\text{N} \\}$，氧化态 $[+3,-3]$。\n\n在化合物中，边应连接每个阳离子与每个阴离子（对于双原子化合物，是一条单一的无向边；对于具有多个阳离子和阴离子的化合物，连接所有阳离子-阴离子对）。\n\n使用维度为 $d_{\\text{in}} = 2$ 的节点特征：原子序数和泡林电负性。你的程序必须为元素 $\\{ \\text{Na}, \\text{Cl}, \\text{Mg}, \\text{O}, \\text{Al}, \\text{Fe}, \\text{N} \\}$ 内部提供这些特征的合理值。\n\n训练和模型超参数：\n\n- 隐藏层维度：$d_h = 4$, $d_m = 4$。\n- 学习率：$\\eta = 0.02$。\n- 训练轮次：$T = 200$。\n- 初始化：使用固定种子的小随机值。\n\n评估协议和指标：\n\n- 将数据集划分为训练集和一个留出的测试图。在训练图上训练无约束和约束模型，并在测试图上进行评估。\n- 预测准确率是测试图上节点的一个分数，其预测的氧化态（四舍五入到最近的整数）等于目标氧化态。将准确率表示为小数。\n\n测试套件：\n\n- 案例 $1$：将 NaCl 作为测试图留出。\n- 案例 $2$：将 Fe$_2$O$_3$ 作为测试图留出。\n- 案例 $3$：将 AlN 作为测试图留出。\n\n你的程序应生成单行输出，其中包含三种情况下无约束模型和约束模型的准确率，按顺序排列，形式为用方括号括起来的逗号分隔列表。例如，你的输出必须是 $[a_1,a_2,a_3,a_4,a_5,a_6]$ 的格式，其中 $a_{2k-1}$ 是案例 $k$ 的无约束模型准确率，$a_{2k}$ 是约束模型的准确率。",
            "solution": "该问题被验证为科学上合理、定义明确且客观。它提出了一个计算材料科学中明确定义的任务，要求实现并比较两种用于氧化态预测的图神经网络（GNN）模型。所有必需的架构组件、超参数、数据集和评估协议都已指定，构成了一个自洽且易于处理的问题。\n\n问题的核心在于将物理原理——电荷守恒——编码为 GNN 架构中的归纳偏置，并分析其对模型性能的影响。我们将继续对模型的前向和后向传播进行完整推导，然后进行实现。\n\n**1. 理论阐述**\n\n我们的任务是开发一个 GNN，用于在代表小型无机化合物的图上预测节点属性（氧化态）。解决方案涉及定义信息在网络中的前向传播，推导用于学习的梯度反向传播，以及比较一个无约束模型和一个在消息传递层面显式强制执行电荷守恒约束的模型。\n\n**1.1. 图表示与特征**\n\n化合物表示为一个图 $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$，其中节点 $i \\in \\mathcal{V}$ 是原子，边 $(j,i) \\in \\mathcal{E}$ 表示相互作用。该模型使用有向消息，因此每个无向化学键（定义为阳离子-阴离子连接）对应于两条有向边 $j \\to i$ 和 $i \\to j$。\n\n每个节点 $i$ 初始化时带有一个特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^{d_{\\text{in}}}$，其中 $d_{\\text{in}} = 2$。特征是原子序数（$Z$）和泡林电负性（$\\chi$）。我们使用以下标准值：\n- 钠 (Na): $\\mathbf{x}_{\\text{Na}} = [11, 0.93]^{\\top}$\n- 氯 (Cl): $\\mathbf{x}_{\\text{Cl}} = [17, 3.16]^{\\top}$\n- 镁 (Mg): $\\mathbf{x}_{\\text{Mg}} = [12, 1.31]^{\\top}$\n- 氧 (O): $\\mathbf{x}_{\\text{O}} = [8, 3.44]^{\\top}$\n- 铝 (Al): $\\mathbf{x}_{\\text{Al}} = [13, 1.61]^{\\top}$\n- 铁 (Fe): $\\mathbf{x}_{\\text{Fe}} = [26, 1.83]^{\\top}$\n- 氮 (N): $\\mathbf{x}_{\\text{N}} = [7, 3.04]^{\\top}$\n\n**1.2. 模型架构：前向传播**\n\n前向传播为每个节点 $i$ 计算一个预测的氧化态 $y_i$。需要学习的参数是权重矩阵 $W_{\\text{node}} \\in \\mathbb{R}^{d_h \\times d_{\\text{in}}}$、$W_{\\phi} \\in \\mathbb{R}^{d_m \\times 2d_h}$，偏置向量 $\\mathbf{b}_{\\phi} \\in \\mathbb{R}^{d_m}$、$b_m \\in \\mathbb{R}$，向量 $\\mathbf{v} \\in \\mathbb{R}^{d_m}$，以及标量 $\\alpha \\in \\mathbb{R}$。给定的隐藏维度为 $d_h=4$ 和 $d_m=4$。\n\n计算过程如下：\n\n1.  **节点嵌入**：每个节点的原始特征 $\\mathbf{x}_i$ 被投影到一个隐藏的嵌入空间中：\n    $$ \\mathbf{h}_i = W_{\\text{node}} \\mathbf{x}_i $$\n\n2.  **边消息构建**：对于每条有向边 $j \\to i$，通过拼接源节点和目标节点的嵌入，然后进行仿射变换，形成一个多维的消息前体：\n    $$ \\mathbf{u}_{ji} = W_{\\phi} [\\mathbf{h}_j ; \\mathbf{h}_i] + \\mathbf{b}_{\\phi} $$\n    其中 $[\\cdot ; \\cdot]$ 表示向量拼接。\n\n3.  **非线性激活**：前体通过一个双曲正切激活函数，以创建一个隐藏的边表示：\n    $$ \\mathbf{z}_{ji} = \\tanh(\\mathbf{u}_{ji}) $$\n\n4.  **原始标量消息**：通过对隐藏边表示进行线性变换，计算出原始标量消息 $r_{ji}$：\n    $$ r_{ji} = \\mathbf{v}^{\\top} \\mathbf{z}_{ji} + b_m $$\n    该值可以解释为从节点 $j$ 转移到节点 $i$ 的潜在“电荷”。\n\n5.  **消息归一化（模型相关）**：\n    - **无约束模型**：最终消息 $m_{ji}$ 就是原始消息：\n      $$ m_{ji}^{\\text{unconstrained}} = r_{ji} $$\n    - **约束模型**：为了在源节点上强制执行电荷守恒，发出的消息被归一化，使其总和为零。对于出度为 $\\deg(j) = |\\mathcal{N}(j)|$ 的源节点 $j$，约束消息为：\n      $$ m_{ji}^{\\text{constrained}} = r_{ji} - \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk} $$\n      这个约束确保了 $\\sum_{i \\in \\mathcal{N}(j)} m_{ji}^{\\text{constrained}} = \\sum_{i \\in \\mathcal{N}(j)} r_{ji} - \\sum_{i \\in \\mathcal{N}(j)} \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk} = \\sum_{i} r_{ji} - \\deg(j) \\frac{1}{\\deg(j)} \\sum_{k} r_{jk} = 0$。这模拟了电荷是从节点 $j$ *转移走* 的，而不是在 $j$ 处产生或销毁的。\n\n6.  **节点更新（电荷累积）**：节点 $i$ 上的预测电荷累积 $q_i$ 是所有传入消息的总和：\n    $$ q_i = \\sum_{j \\in \\mathcal{N}(i)} m_{ji} $$\n    注意索引：总和是针对传入边 $j \\to i$ 的源节点 $j$。\n\n7.  **最终预测**：预测的氧化态 $y_i$ 是累积电荷的缩放版本：\n    $$ y_i = \\alpha q_i $$\n\n**1.3. 训练目标与梯度下降**\n\n模型通过最小化图中所有节点的预测氧化态 $y_i$ 与真实氧化态 $o_i$ 之间的均方误差（MSE）损失 $L$ 进行训练：\n$$ L = \\frac{1}{|\\mathcal{V}|} \\sum_{i \\in \\mathcal{V}} (y_i - o_i)^2 $$\n我们使用全批量梯度下降，在参数更新之前，梯度会在训练集中的所有图上累积。参数更新使用学习率 $\\eta = 0.02$。任何参数 $\\theta$ 的更新规则是 $\\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}$。\n\n**2. 通过反向传播推导梯度**\n\n我们使用链式法则推导损失 $L$ 相对于所有可学习参数的梯度。对于给定的图 $g$，令 $N_g = |\\mathcal{V}_g|$。对于批量训练，总梯度 $\\frac{\\partial L_{\\text{total}}}{\\partial \\theta}$ 是批次中每个图的梯度之和，即 $\\sum_g \\frac{\\partial L_g}{\\partial \\theta}$。\n\n- **关于预测 $y_i$ 的梯度**：\n  $$ \\frac{\\partial L}{\\partial y_i} = \\frac{2}{N_g} (y_i - o_i) $$\n\n- **关于 $\\alpha$ 的梯度**：\n  $$ \\frac{\\partial L}{\\partial \\alpha} = \\sum_{i \\in \\mathcal{V}} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\alpha} = \\sum_{i \\in \\mathcal{V}} \\frac{2}{N_g}(y_i - o_i) q_i $$\n\n- **关于电荷累积 $q_i$ 的梯度**：\n  $$ \\frac{\\partial L}{\\partial q_i} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial q_i} = \\frac{2}{N_g}(y_i - o_i) \\alpha $$\n\n- **关于消息 $m_{ji}$ 的梯度**：消息 $m_{ji}$ 只影响目标节点 $i$ 的电荷累积。\n  $$ \\frac{\\partial L}{\\partial m_{ji}} = \\frac{\\partial L}{\\partial q_i} \\frac{\\partial q_i}{\\partial m_{ji}} = \\frac{\\partial L}{\\partial q_i} = \\frac{2\\alpha}{N_g}(y_i - o_i) $$\n\n- **关于原始消息 $r_{ab}$ 的梯度**：这是两个模型产生分歧的关键步骤。我们使用多元链式法则：$\\frac{\\partial L}{\\partial r_{ab}} = \\sum_{j,i} \\frac{\\partial L}{\\partial m_{ji}} \\frac{\\partial m_{ji}}{\\partial r_{ab}}$。令 $\\delta_{m_{ji}} = \\frac{\\partial L}{\\partial m_{ji}}$。\n\n  - **无约束模型**：此处，$m_{ji} = r_{ji}$，因此 $\\frac{\\partial m_{ji}}{\\partial r_{ab}} = \\delta_{ja}\\delta_{ib}$（克罗内克 δ）。总和塌缩为一项：\n    $$ \\frac{\\partial L}{\\partial r_{ab}}^{\\text{unconstrained}} = \\delta_{m_{ab}} = \\frac{2\\alpha}{N_g}(y_b - o_b) $$\n\n  - **约束模型**：此处，$m_{ji} = r_{ji} - \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk}$。如果 $j=a$，则导数 $\\frac{\\partial m_{ji}}{\\partial r_{ab}}$ 非零。\n    $$ \\frac{\\partial m_{ji}}{\\partial r_{ab}} = \\begin{cases} 1 - 1/\\deg(a)  \\text{if } (j,i) = (a,b) \\\\ -1/\\deg(a)  \\text{if } j=a, i \\ne b \\\\ 0  \\text{otherwise} \\end{cases} $$\n    代入链式法则求和：\n    $$ \\frac{\\partial L}{\\partial r_{ab}}^{\\text{constrained}} = \\delta_{m_{ab}}\\left(1 - \\frac{1}{\\deg(a)}\\right) + \\sum_{i \\in \\mathcal{N}(a), i \\ne b} \\delta_{m_{ai}}\\left(-\\frac{1}{\\deg(a)}\\right) $$\n    $$ = \\delta_{m_{ab}} - \\frac{1}{\\deg(a)} \\sum_{i \\in \\mathcal{N}(a)} \\delta_{m_{ai}} $$\n    这个结果很直观：原始消息 $r_{ab}$ 的梯度是它对损失的直接贡献（通过 $m_{ab}$），并减去从节点 $a$ 发出的所有消息的平均梯度进行修正。这种修正分散了误差信号，反映了学习过程中的守恒约束。\n\n其余梯度由 $\\frac{\\partial L}{\\partial r_{ji}}$（记为 $\\delta_{r_{ji}}$）推导得出。\n- **$b_m$ 和 $\\mathbf{v}$ 的梯度**：\n  $$ \\frac{\\partial L}{\\partial b_m} = \\sum_{j,i} \\delta_{r_{ji}} \\quad , \\quad \\frac{\\partial L}{\\partial \\mathbf{v}} = \\sum_{j,i} \\delta_{r_{ji}} \\mathbf{z}_{ji} $$\n\n- **通过 $\\tanh$ 的梯度流**：令 $\\delta_{\\mathbf{z}_{ji}} = \\frac{\\partial L}{\\partial \\mathbf{z}_{ji}} = \\delta_{r_{ji}} \\mathbf{v}$。那么关于预激活 $\\mathbf{u}_{ji}$ 的梯度是：\n  $$ \\delta_{\\mathbf{u}_{ji}} = \\frac{\\partial L}{\\partial \\mathbf{u}_{ji}} = \\delta_{\\mathbf{z}_{ji}} \\odot (1 - \\mathbf{z}_{ji}^2) $$\n  其中 $\\odot$ 是哈达玛（逐元素）积。\n\n- **$W_{\\phi}$ 和 $\\mathbf{b}_{\\phi}$ 的梯度**：\n  $$ \\frac{\\partial L}{\\partial \\mathbf{b}_{\\phi}} = \\sum_{j,i} \\delta_{\\mathbf{u}_{ji}} \\quad , \\quad \\frac{\\partial L}{\\partial W_{\\phi}} = \\sum_{j,i} \\delta_{\\mathbf{u}_{ji}} [\\mathbf{h}_j ; \\mathbf{h}_i]^{\\top} $$\n\n- **关于节点嵌入 $\\mathbf{h}_k$ 的梯度**：节点嵌入 $\\mathbf{h}_k$ 对所有发出的消息（作为源节点，$j=k$）和所有传入的消息（作为目标节点，$i=k$）都有贡献。设 $W_\\phi$ 被划分为 $[W_{\\phi, \\text{src}} | W_{\\phi, \\text{tgt}}]$，其中 $W_{\\phi, \\text{src}}, W_{\\phi, \\text{tgt}} \\in \\mathbb{R}^{d_m \\times d_h}$。\n  $$ \\delta_{\\mathbf{h}_k} = \\frac{\\partial L}{\\partial \\mathbf{h}_k} = \\sum_{i \\in \\mathcal{N}(k)} W_{\\phi, \\text{src}}^{\\top} \\delta_{\\mathbf{u}_{ki}} + \\sum_{j \\in \\mathcal{N}(k)} W_{\\phi, \\text{tgt}}^{\\top} \\delta_{\\mathbf{u}_{jk}} $$\n\n- **关于 $W_{\\text{node}}$ 的梯度**：\n  $$ \\frac{\\partial L}{\\partial W_{\\text{node}}} = \\sum_{i \\in \\mathcal{V}} \\delta_{\\mathbf{h}_i} \\mathbf{x}_i^{\\top} $$\n\n这就完成了两种模型所有必要梯度的推导。实现将遵循此数学框架，在所提供的数据集上执行全批量梯度下降。",
            "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\n\n# Global constants for configuration and data\nRANDOM_SEED = 42\nELEMENT_FEATURES = {\n    'Na': np.array([11, 0.93]), 'Cl': np.array([17, 3.16]),\n    'Mg': np.array([12, 1.31]), 'O': np.array([8, 3.44]),\n    'Al': np.array([13, 1.61]), 'Fe': np.array([26, 1.83]),\n    'N': np.array([7, 3.04])\n}\nCOMPOUNDS = {\n    'NaCl': {'cations': ['Na'], 'anions': ['Cl'], 'states': [1, -1]},\n    'MgO': {'cations': ['Mg'], 'anions': ['O'], 'states': [2, -2]},\n    'Al2O3': {'cations': ['Al']*2, 'anions': ['O']*3, 'states': [3, 3, -2, -2, -2]},\n    'FeO': {'cations': ['Fe'], 'anions': ['O'], 'states': [2, -2]},\n    'Fe2O3': {'cations': ['Fe']*2, 'anions': ['O']*3, 'states': [3, 3, -2, -2, -2]},\n    'Na2O': {'cations': ['Na']*2, 'anions': ['O']*1, 'states': [1, 1, -2]},\n    'AlN': {'cations': ['Al'], 'anions': ['N'], 'states': [3, -3]}\n}\nHYPERPARAMS = {\n    'd_in': 2, 'd_h': 4, 'd_m': 4, 'lr': 0.02, 'epochs': 200\n}\n\ndef create_graph_data(compounds_dict):\n    \"\"\"Constructs graph representations from the compound dictionary.\"\"\"\n    graph_data = {}\n    for name, data in compounds_dict.items():\n        atom_list = data['cations'] + data['anions']\n        num_atoms = len(atom_list)\n        num_cations = len(data['cations'])\n        \n        node_features = np.array([ELEMENT_FEATURES[atom] for atom in atom_list])\n        targets = np.array(data['states'])\n        \n        adj = []\n        out_degree = np.zeros(num_atoms)\n        outgoing_edges = defaultdict(list)\n        \n        cation_indices = list(range(num_cations))\n        anion_indices = list(range(num_cations, num_atoms))\n\n        for j in cation_indices:\n            for i in anion_indices:\n                adj.extend([(j, i), (i, j)])\n                out_degree[j] += 1\n                out_degree[i] += 1\n                outgoing_edges[j].append((j,i))\n                outgoing_edges[i].append((i,j))\n        \n        graph_data[name] = {\n            'name': name,\n            'node_features': node_features,\n            'targets': targets,\n            'adj': adj,\n            'out_degree': out_degree,\n            'outgoing_edges': outgoing_edges,\n            'num_nodes': num_atoms\n        }\n    return graph_data\n\nclass GNN:\n    \"\"\"A minimal GNN for oxidation state prediction.\"\"\"\n    def __init__(self, d_in, d_h, d_m, constrained, learning_rate, seed):\n        self.d_in, self.d_h, self.d_m = d_in, d_h, d_m\n        self.constrained = constrained\n        self.lr = learning_rate\n        self.rng = np.random.default_rng(seed)\n\n        # Initialize parameters\n        self.W_node = self.rng.standard_normal((d_h, d_in)) * 0.1\n        self.W_phi = self.rng.standard_normal((d_m, 2 * d_h)) * 0.1\n        self.b_phi = np.zeros(d_m)\n        self.v = self.rng.standard_normal(d_m) * 0.1\n        self.b_m = 0.0\n        self.alpha = self.rng.standard_normal(1) * 0.1\n\n    def forward(self, graphs):\n        \"\"\"Performs the forward pass for a batch of graphs.\"\"\"\n        total_loss = 0.0\n        batch_activations = []\n\n        for graph in graphs:\n            x, targets = graph['node_features'], graph['targets']\n            num_nodes = graph['num_nodes']\n            adj = graph['adj']\n\n            # 1. Node embedding\n            h = x @ self.W_node.T\n\n            # 2-4. Edge message computation\n            u, z, r = {}, {}, {}\n            for j, i in adj:\n                h_concat = np.concatenate([h[j], h[i]])\n                u_ji = self.W_phi @ h_concat + self.b_phi\n                z_ji = np.tanh(u_ji)\n                r_ji = self.v @ z_ji + self.b_m\n                u[j,i], z[j,i], r[j,i] = u_ji, z_ji, r_ji\n            \n            # 5. Message normalization\n            m = {}\n            if self.constrained:\n                for j in range(num_nodes):\n                    r_sum = sum(r[edge] for edge in graph['outgoing_edges'][j])\n                    for edge in graph['outgoing_edges'][j]:\n                        m[edge] = r[edge] - r_sum / graph['out_degree'][j]\n            else:\n                m = r\n\n            # 6. Charge accumulation\n            q = np.zeros(num_nodes)\n            for j, i in adj:\n                q[i] += m[j,i]\n            \n            # 7. Final prediction\n            y = self.alpha * q\n            \n            # Loss calculation\n            loss = np.mean((y - targets)**2)\n            total_loss += loss\n            \n            batch_activations.append({'h': h, 'u': u, 'z': z, 'r': r, 'm': m, 'q': q, 'y': y})\n        \n        return total_loss / len(graphs), batch_activations\n\n    def backward(self, graphs, activations):\n        \"\"\"Performs the backward pass and computes gradients.\"\"\"\n        # Initialize aggregate gradients\n        grad_W_node = np.zeros_like(self.W_node)\n        grad_W_phi = np.zeros_like(self.W_phi)\n        grad_b_phi = np.zeros_like(self.b_phi)\n        grad_v = np.zeros_like(self.v)\n        grad_b_m = 0.0\n        grad_alpha = 0.0\n\n        for i, graph in enumerate(graphs):\n            act = activations[i]\n            x, targets, adj, n_nodes = graph['node_features'], graph['targets'], graph['adj'], graph['num_nodes']\n            y, q, m, r, z, h = act['y'], act['q'], act['m'], act['r'], act['z'], act['h']\n\n            # Gradient of Loss w.r.t y\n            dL_dy = 2.0 / n_nodes * (y - targets)\n            \n            # Gradients for alpha and q\n            grad_alpha += np.sum(dL_dy * q)\n            dL_dq = dL_dy * self.alpha\n\n            # Gradient w.r.t messages m\n            dL_dm = {}\n            for j, i_node in adj:\n                dL_dm[j, i_node] = dL_dq[i_node]\n            \n            # Gradient w.r.t raw messages r\n            dL_dr = {}\n            if self.constrained:\n                for j_node in range(n_nodes):\n                    avg_dL_dm_sum = sum(dL_dm[edge] for edge in graph['outgoing_edges'][j_node]) / graph['out_degree'][j_node]\n                    for edge in graph['outgoing_edges'][j_node]:\n                        dL_dr[edge] = dL_dm[edge] - avg_dL_dm_sum\n            else:\n                dL_dr = dL_dm\n\n            # Backprop through r = v.z + b_m\n            dL_dh = np.zeros_like(h)\n            for j, i_node in adj:\n                delta_r_ji = dL_dr[j, i_node]\n                grad_v += delta_r_ji * z[j,i_node]\n                grad_b_m += delta_r_ji\n\n                # Backprop through z = tanh(u)\n                dL_dz_ji = delta_r_ji * self.v\n                dL_du_ji = dL_dz_ji * (1 - z[j,i_node]**2)\n                \n                # Gradients for W_phi, b_phi\n                grad_b_phi += dL_du_ji\n                h_concat = np.concatenate([h[j], h[i_node]])\n                grad_W_phi += np.outer(dL_du_ji, h_concat)\n\n                # Backprop to h\n                W_phi_src = self.W_phi[:, :self.d_h]\n                W_phi_tgt = self.W_phi[:, self.d_h:]\n                dL_dh[j] += W_phi_src.T @ dL_du_ji\n                dL_dh[i_node] += W_phi_tgt.T @ dL_du_ji\n            \n            # Backprop to W_node\n            grad_W_node += dL_dh.T @ x\n\n        num_graphs = len(graphs)\n        return {\n            'W_node': grad_W_node / num_graphs, 'W_phi': grad_W_phi / num_graphs,\n            'b_phi': grad_b_phi / num_graphs, 'v': grad_v / num_graphs,\n            'b_m': grad_b_m / num_graphs, 'alpha': grad_alpha / num_graphs\n        }\n\n    def update(self, grads):\n        \"\"\"Updates model parameters using gradient descent.\"\"\"\n        self.W_node -= self.lr * grads['W_node']\n        self.W_phi -= self.lr * grads['W_phi']\n        self.b_phi -= self.lr * grads['b_phi']\n        self.v -= self.lr * grads['v']\n        self.b_m -= self.lr * grads['b_m']\n        self.alpha -= self.lr * grads['alpha']\n\ndef run_experiment(train_graphs, test_graph, constrained, hyperparams, seed):\n    \"\"\"Trains and evaluates a GNN model.\"\"\"\n    model = GNN(\n        d_in=hyperparams['d_in'], d_h=hyperparams['d_h'], d_m=hyperparams['d_m'],\n        constrained=constrained, learning_rate=hyperparams['lr'], seed=seed\n    )\n    \n    # Training Loop\n    for _ in range(hyperparams['epochs']):\n        loss, activations = model.forward(train_graphs)\n        grads = model.backward(train_graphs, activations)\n        model.update(grads)\n    \n    # Evaluation\n    _, test_activations = model.forward([test_graph])\n    predictions = test_activations[0]['y']\n    targets = test_graph['targets']\n    \n    rounded_preds = np.round(predictions)\n    accuracy = np.mean(rounded_preds == targets)\n    \n    return accuracy\n\ndef solve():\n    \"\"\"Main function to run the experiments and print results.\"\"\"\n    all_graph_data = create_graph_data(COMPOUNDS)\n    test_cases = ['NaCl', 'Fe2O3', 'AlN']\n    results = []\n\n    for test_compound_name in test_cases:\n        train_graphs = [g for name, g in all_graph_data.items() if name != test_compound_name]\n        test_graph = all_graph_data[test_compound_name]\n        \n        # Run unconstrained model\n        acc_unconstrained = run_experiment(train_graphs, test_graph, constrained=False, hyperparams=HYPERPARAMS, seed=RANDOM_SEED)\n        results.append(acc_unconstrained)\n        \n        # Run constrained model\n        acc_constrained = run_experiment(train_graphs, test_graph, constrained=True, hyperparams=HYPERPARAMS, seed=RANDOM_SEED)\n        results.append(acc_constrained)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{acc:.2f}' for acc in results)}]\")\n\nsolve()\n```"
        }
    ]
}