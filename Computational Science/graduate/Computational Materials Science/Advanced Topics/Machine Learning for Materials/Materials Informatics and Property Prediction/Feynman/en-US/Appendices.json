{
    "hands_on_practices": [
        {
            "introduction": "Modern materials informatics heavily relies on machine learning models that can directly interpret atomic structures. This practice guides you through the fundamental task of converting crystallographic data into a periodic graph representation, a crucial preprocessing step for applying powerful graph neural networks. By implementing the minimum-image convention, you will learn how to handle the periodic nature of crystals to build a meaningful graph of atomic neighbors, forming the bedrock of structure-based property prediction. ",
            "id": "3464241",
            "problem": "You are asked to implement an algorithm that converts data equivalent to a Crystallographic Information File (CIF), consisting of lattice vectors and fractional atomic coordinates, into a periodic neighbor graph under the minimum-image convention within a cutoff radius $r_c$. The goal is to produce a self-contained program that, for a provided test suite, constructs a directed neighbor graph where nodes are atoms and edges represent periodic nearest neighbors. Each edge must be annotated with two attributes: the interatomic distance and the direction unit vector from atom $i$ to the minimum-image of atom $j$. All distances must be expressed in Angstroms and rounded as specified.\n\nFundamental basis: use crystallographic definitions and Euclidean geometry. Let the lattice be defined by three lattice vectors $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{c}$ in Cartesian coordinates, arranged as columns of a $3 \\times 3$ matrix $L = [\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c}]$. Fractional coordinates $\\mathbf{f}_i \\in [0,1)^3$ map to Cartesian coordinates via $\\mathbf{r}_i = L \\mathbf{f}_i$. The periodic minimum-image distance between atoms $i$ and $j$ is defined as $d_{ij} = \\min_{\\mathbf{n} \\in \\mathbb{Z}^3} \\left\\| L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right) \\right\\|_2$, where $\\mathbf{n}$ is an integer translation vector enumerating periodic images. The corresponding direction unit vector is $\\hat{\\mathbf{u}}_{ij} = \\frac{L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}^*\\right)}{\\left\\| L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}^*\\right) \\right\\|_2}$, where $\\mathbf{n}^*$ attains the minimum.\n\nThe program must:\n- Construct a directed edge from atom $i$ to atom $j$ if $d_{ij} \\leq r_c$, excluding self-edges when $i=j$.\n- Compute and store for each directed edge the attributes $d_{ij}$ and $\\hat{\\mathbf{u}}_{ij}$, with $d_{ij}$ in Angstroms.\n- Use an exhaustive search over integer translations $\\mathbf{n}$ in a bounded cube $[-N,N]^3$ to locate $\\mathbf{n}^*$, where $N = \\left\\lceil \\frac{r_c}{\\min\\{\\|\\mathbf{a}\\|_2,\\|\\mathbf{b}\\|_2,\\|\\mathbf{c}\\|_2\\}} \\right\\rceil + 1$.\n- Round reported floating-point quantities in the final aggregated output to $6$ decimals.\n\nAngles in the test suite are specified in degrees; trigonometric functions must use radians internally. The program must not read external input or files and must compute results for the following test suite of parameter values:\n\nTest case $1$ (orthorhombic cubic cell, general case):\n- Lattice matrix $L_1$ in Angstroms: columns $\\mathbf{a}_1 = (4,0,0)$, $\\mathbf{b}_1 = (0,4,0)$, $\\mathbf{c}_1 = (0,0,4)$.\n- Fractional coordinates $\\{\\mathbf{f}_i\\}$: $\\mathbf{f}_1 = (0,0,0)$, $\\mathbf{f}_2 = (0.5,0.5,0.5)$.\n- Cutoff $r_c = 3.5$ Angstroms.\n\nTest case $2$ (boundary condition at exact cutoff):\n- Same $L_2 = L_1$ and fractional coordinates as in test case $1$.\n- Cutoff $r_c = \\sqrt{12}$ Angstroms.\n\nTest case $3$ (triclinic lattice to test non-orthogonality):\n- Lattice parameters $a=5$ Angstroms, $b=6$ Angstroms, $c=7$ Angstroms, $\\alpha=75$ degrees, $\\beta=80$ degrees, $\\gamma=70$ degrees. Construct the lattice matrix $L_3$ with columns $(\\mathbf{a}_3,\\mathbf{b}_3,\\mathbf{c}_3)$ using the standard crystallographic relations:\n  - $\\mathbf{a}_3 = (a,0,0)$,\n  - $\\mathbf{b}_3 = (b\\cos\\gamma,b\\sin\\gamma,0)$,\n  - $\\mathbf{c}_3 = \\left(c\\cos\\beta,\\ c\\frac{\\cos\\alpha - \\cos\\beta\\cos\\gamma}{\\sin\\gamma},\\ c\\sqrt{1 - \\cos^2\\beta - \\left(\\frac{\\cos\\alpha - \\cos\\beta\\cos\\gamma}{\\sin\\gamma}\\right)^2}\\right)$.\n- Fractional coordinates $\\{\\mathbf{f}_i\\}$: $\\mathbf{f}_1 = (0.1,0.1,0.1)$, $\\mathbf{f}_2 = (0.3,0.2,0.15)$, $\\mathbf{f}_3 = (0.9,0.85,0.8)$.\n- Cutoff $r_c = 2.5$ Angstroms.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list of three numbers $[n_{\\text{edges}}, S, M]$ with $n_{\\text{edges}}$ the integer count of directed edges, $S$ the sum of all directed edge distances in Angstroms rounded to $6$ decimals, and $M$ the maximum directed edge distance in Angstroms rounded to $6$ decimals. For example, output must be of the form $[[n_1,S_1,M_1],[n_2,S_2,M_2],[n_3,S_3,M_3]]$ with no additional text.",
            "solution": "The construction of a periodic neighbor graph under the minimum-image convention is grounded in fundamental crystallography and Euclidean geometry. At the base, a Bravais lattice is defined by three lattice vectors $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{c}$ in Cartesian space. The $3 \\times 3$ lattice matrix $L = [\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c}]$ maps fractional coordinates $\\mathbf{f}_i$ to Cartesian coordinates via $\\mathbf{r}_i = L \\mathbf{f}_i$. The Euclidean distance in Cartesian space is derived from the squared norm: for any vector $\\mathbf{v} \\in \\mathbb{R}^3$, $\\|\\mathbf{v}\\|_2^2 = \\mathbf{v}^\\top \\mathbf{v}$.\n\nUnder periodic boundary conditions, the true nearest-image distance between atoms $i$ and $j$ is defined as the minimum over all lattice translations by integer multiples of $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{c}$. Let $\\mathbf{n} \\in \\mathbb{Z}^3$ denote an integer translation vector. The Cartesian displacement between atom $i$ and a periodic image of atom $j$ is $\\Delta\\mathbf{r}_{ij}(\\mathbf{n}) = L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right)$. The squared distance is then\n$$\nd_{ij}^2(\\mathbf{n}) = \\left\\| \\Delta\\mathbf{r}_{ij}(\\mathbf{n}) \\right\\|_2^2 = \\left(L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right)\\right)^\\top \\left(L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right)\\right).\n$$\nThis can also be expressed via the metric tensor $G = L^\\top L$ as\n$$\nd_{ij}^2(\\mathbf{n}) = \\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right)^\\top G \\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right),\n$$\nwhere $G$ is symmetric positive definite because $L$ has full rank in a valid lattice. The minimum-image convention chooses $\\mathbf{n}^* \\in \\mathbb{Z}^3$ that minimizes $d_{ij}^2(\\mathbf{n})$, and the corresponding nearest-image distance and direction are\n$$\nd_{ij} = \\min_{\\mathbf{n} \\in \\mathbb{Z}^3} d_{ij}(\\mathbf{n}), \\quad \\hat{\\mathbf{u}}_{ij} = \\frac{\\Delta\\mathbf{r}_{ij}(\\mathbf{n}^*)}{\\left\\|\\Delta\\mathbf{r}_{ij}(\\mathbf{n}^*)\\right\\|_2}.\n$$\n\nAlgorithmic approach. Direct minimization over $\\mathbf{n} \\in \\mathbb{Z}^3$ is infinite. However, a finite bounded search suffices for any finite cutoff $r_c$ due to the following observation: if $\\left\\|L\\left(\\mathbf{f}_j - \\mathbf{f}_i - \\mathbf{n}\\right)\\right\\|_2 \\leq r_c$, then the magnitude of each component of $\\mathbf{n}$ cannot grow without bound. A safe and widely used practical bound leverages the shortest lattice vector length, $m = \\min\\{\\|\\mathbf{a}\\|_2,\\|\\mathbf{b}\\|_2,\\|\\mathbf{c}\\|_2\\}$. Any translation by one unit along the shortest lattice vector changes the image by at least $m$ in Euclidean norm if considered alone. To ensure coverage even in skewed lattices where vector components can partially cancel, we search all integer translations within a cube\n$$\nn_k \\in \\{-N,-N+1,\\dots,N\\} \\text{ for } k \\in \\{1,2,3\\}, \\quad N = \\left\\lceil \\frac{r_c}{m} \\right\\rceil + 1.\n$$\nThis bound is conservative: the additional $+1$ ensures that even if partial cancellations occur among $\\mathbf{a}$, $\\mathbf{b}$, and $\\mathbf{c}$, any translation that could yield a displacement length within $r_c$ is still considered.\n\nFor each pair $(i,j)$ with $i \\neq j$:\n- Compute the fractional difference $\\Delta\\mathbf{f}_{ij} = \\mathbf{f}_j - \\mathbf{f}_i$.\n- Enumerate all $\\mathbf{n} = (n_1,n_2,n_3)$ with $n_k \\in [-N,N]$ and compute $\\Delta\\mathbf{r}_{ij}(\\mathbf{n}) = L\\left(\\Delta\\mathbf{f}_{ij} - \\mathbf{n}\\right)$.\n- Find $\\mathbf{n}^*$ minimizing $\\left\\|\\Delta\\mathbf{r}_{ij}(\\mathbf{n})\\right\\|_2$. Let $d_{ij} = \\left\\|\\Delta\\mathbf{r}_{ij}(\\mathbf{n}^*)\\right\\|_2$.\n- If $d_{ij} \\leq r_c$ (inclusive), then add a directed edge $(i \\to j)$ with attributes $d_{ij}$ and $\\hat{\\mathbf{u}}_{ij} = \\Delta\\mathbf{r}_{ij}(\\mathbf{n}^*) / d_{ij}$ if $d_{ij} > 0$, otherwise define the direction vector as the zero vector to avoid division by zero.\n\nGraph design. Nodes are atoms indexed by $i \\in \\{1,\\dots,N_{\\text{atoms}}\\}$. Edges are directed to capture orientation-dependent attributes (direction unit vector) and to facilitate usage in machine learning models where edge directions matter. For $N_{\\text{atoms}}$ atoms, a naive all-pairs enumeration yields $N_{\\text{atoms}}(N_{\\text{atoms}}-1)$ candidate directed edges, each minimized over $(2N+1)^3$ translations, leading to a computational cost on the order of $\\mathcal{O}\\left(N_{\\text{atoms}}^2 (2N+1)^3\\right)$. In the given test suite, atom counts are small and cutoffs are moderate, making this exhaustive approach computationally feasible.\n\nUnits and rounding. All distances $d_{ij}$ are computed in Angstroms. The final output for each test case reports three quantities: $n_{\\text{edges}}$ as an integer, the sum of distances $S$ rounded to $6$ decimals, and the maximum distance $M$ rounded to $6$ decimals. Angles supplied in degrees are converted to radians internally via the relation $\\theta_{\\text{rad}} = \\theta_{\\text{deg}} \\cdot \\frac{\\pi}{180}$.\n\nTest suite application:\n- In test case $1$, with $L$ orthorhombic and two atoms separated by fractional $(0.5,0.5,0.5)$, the minimum-image displacement $\\Delta\\mathbf{r}$ evaluates to $(2,2,2)$ Angstroms, yielding $d_{12} = \\sqrt{12}$ Angstroms, which is less than $3.5$ Angstroms; thus both directed edges $(1\\to2)$ and $(2\\to1)$ are included.\n- In test case $2$, the cutoff $r_c$ equals the exact distance $\\sqrt{12}$ Angstroms, thus the edges at this boundary are included by the inclusive rule $d_{ij} \\leq r_c$.\n- In test case $3$, the triclinic lattice vectors are constructed using the provided formulae with $\\alpha$, $\\beta$, $\\gamma$ in degrees converted to radians. The algorithm proceeds identically; exhaustive translation search ensures correct minimum images despite non-orthogonality.\n\nThe program will report for each test case the triple $[n_{\\text{edges}},S,M]$ and aggregate all three results into a single line, formatted as $[[n_1,S_1,M_1],[n_2,S_2,M_2],[n_3,S_3,M_3]]$ without any additional text.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lattice_from_lengths_angles(a, b, c, alpha_deg, beta_deg, gamma_deg):\n    \"\"\"\n    Construct lattice matrix L (3x3) with columns [a_vec, b_vec, c_vec]\n    given lengths a,b,c (Angstrom) and angles alpha (between b and c),\n    beta (between a and c), gamma (between a and b) in degrees.\n    \"\"\"\n    alpha = np.deg2rad(alpha_deg)\n    beta = np.deg2rad(beta_deg)\n    gamma = np.deg2rad(gamma_deg)\n\n    # Precompute cos and sin\n    cos_alpha = np.cos(alpha)\n    cos_beta = np.cos(beta)\n    cos_gamma = np.cos(gamma)\n    sin_gamma = np.sin(gamma)\n\n    # Vectors in Cartesian coordinates\n    a_vec = np.array([a, 0.0, 0.0], dtype=float)\n    b_vec = np.array([b * cos_gamma, b * sin_gamma, 0.0], dtype=float)\n    c_x = c * cos_beta\n    c_y = c * (cos_alpha - cos_beta * cos_gamma) / (sin_gamma if sin_gamma != 0 else 1e-12)\n    # Ensure numerical stability inside sqrt\n    c_z_sq = c**2 - c_x**2 - c_y**2\n    # Small negative due to numerical error should be clamped to 0\n    if c_z_sq < 0 and c_z_sq > -1e-12:\n        c_z_sq = 0.0\n    c_z = np.sqrt(c_z_sq)\n    c_vec = np.array([c_x, c_y, c_z], dtype=float)\n\n    # Assemble columns\n    L = np.column_stack([a_vec, b_vec, c_vec])\n    return L\n\ndef minimum_image_vector(L, df, rc):\n    \"\"\"\n    Compute the minimum-image Cartesian displacement vector for fractional difference df\n    using an exhaustive search over integer translations n in [-N, N]^3, where\n    N = ceil(rc / min(|a|,|b|,|c|)) + 1.\n    Returns the displacement vector dr_min and its norm d_min.\n    \"\"\"\n    # Norms of lattice vectors (columns)\n    a_vec, b_vec, c_vec = L[:, 0], L[:, 1], L[:, 2]\n    min_len = min(np.linalg.norm(a_vec), np.linalg.norm(b_vec), np.linalg.norm(c_vec))\n    N = int(np.ceil(rc / (min_len if min_len > 0 else 1e-12))) + 1\n\n    d_min_sq = np.inf\n    dr_min = None\n\n    # Enumerate translations\n    for n1 in range(-N, N + 1):\n        for n2 in range(-N, N + 1):\n            for n3 in range(-N, N + 1):\n                n = np.array([n1, n2, n3], dtype=float)\n                dr = L @ (df - n)\n                dsq = np.dot(dr, dr)\n                if dsq < d_min_sq:\n                    d_min_sq = dsq\n                    dr_min = dr\n\n    d_min = float(np.sqrt(d_min_sq))\n    return dr_min, d_min\n\ndef build_neighbor_graph(L, frac_coords, rc):\n    \"\"\"\n    Build directed neighbor graph under periodic boundary conditions.\n    Returns a list of edges, each edge is a dict with keys:\n    'i', 'j', 'distance', 'direction'.\n    \"\"\"\n    n_atoms = len(frac_coords)\n    edges = []\n    for i in range(n_atoms):\n        fi = np.array(frac_coords[i], dtype=float)\n        for j in range(n_atoms):\n            if i == j:\n                continue\n            fj = np.array(frac_coords[j], dtype=float)\n            df = fj - fi\n            dr_min, d_min = minimum_image_vector(L, df, rc)\n            if d_min <= rc + 1e-12:\n                # Direction unit vector; handle zero distance\n                if d_min > 0:\n                    direction = dr_min / d_min\n                else:\n                    direction = np.array([0.0, 0.0, 0.0], dtype=float)\n                edges.append({\n                    'i': i,\n                    'j': j,\n                    'distance': d_min,\n                    'direction': direction\n                })\n    return edges\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Test case 1: Orthorhombic cubic cell\n    L1 = np.column_stack([\n        np.array([4.0, 0.0, 0.0]),\n        np.array([0.0, 4.0, 0.0]),\n        np.array([0.0, 0.0, 4.0])\n    ])\n    frac1 = [\n        [0.0, 0.0, 0.0],\n        [0.5, 0.5, 0.5]\n    ]\n    rc1 = 3.5\n\n    # Test case 2: Boundary at exact cutoff\n    L2 = L1.copy()\n    frac2 = frac1.copy()\n    rc2 = float(np.sqrt(12.0))  # sqrt(12) Angstroms\n\n    # Test case 3: Triclinic lattice\n    a3, b3, c3 = 5.0, 6.0, 7.0\n    alpha3, beta3, gamma3 = 75.0, 80.0, 70.0  # degrees\n    L3 = lattice_from_lengths_angles(a3, b3, c3, alpha3, beta3, gamma3)\n    frac3 = [\n        [0.1, 0.1, 0.1],\n        [0.3, 0.2, 0.15],\n        [0.9, 0.85, 0.8]\n    ]\n    rc3 = 2.5\n\n    test_cases = [\n        (L1, frac1, rc1),\n        (L2, frac2, rc2),\n        (L3, frac3, rc3)\n    ]\n\n    results = []\n    for L, frac_coords, rc in test_cases:\n        edges = build_neighbor_graph(L, frac_coords, rc)\n        n_edges = len(edges)\n        distances = np.array([e['distance'] for e in edges], dtype=float)\n        sum_dist = float(np.sum(distances)) if n_edges > 0 else 0.0\n        max_dist = float(np.max(distances)) if n_edges > 0 else 0.0\n        # Round to 6 decimals as required\n        results.append([n_edges, round(sum_dist, 6), round(max_dist, 6)])\n\n    # Final print statement in the exact required format.\n    # Format: [[n1,S1,M1],[n2,S2,M2],[n3,S3,M3]]\n    formatted = \"[\" + \",\".join(\n        [\"[\" + \",\".join([str(item) for item in res]) + \"]\" for res in results]\n    ) + \"]\"\n    print(formatted)\n\nsolve()\n```"
        },
        {
            "introduction": "A key advantage of modern predictive models, such as Gaussian processes, is their ability to quantify uncertainty. This exercise demonstrates how to translate the probabilistic output of a model into a concrete, physically meaningful metric: the probability of thermodynamic stability. You will practice applying basic probability theory to a model's predicted formation energy distribution to assess whether a hypothetical new compound is likely to be stable, a critical question in materials discovery. ",
            "id": "3464202",
            "problem": "In data-driven phase stability screening within materials informatics, a candidate compound at composition $x$ is considered thermodynamically stable with respect to decomposition if its energy above the convex hull, defined as $d(x) = E_{f}(x) - E_{\\mathrm{hull}}(x)$, satisfies $d(x) < 0$. Here, $E_{f}(x)$ is the formation energy per atom of the candidate and $E_{\\mathrm{hull}}(x)$ is the convex hull line value determined from competing phases at the same composition. In a predictive setting, $E_{f}(x)$ is treated as a random variable due to model uncertainty, while $E_{\\mathrm{hull}}(x)$ is treated as deterministic, having been computed from a set of known phases.\n\nAssume that at $x = 0.4$, the predictive distribution for the formation energy is Gaussian, $E_{f}(0.4) \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ with $\\mu = -0.12$ and $\\sigma = 0.03$, and that the hull line value is $\\hat{E}_{\\mathrm{hull}}(0.4) = -0.10$. Using the definitions of the probability density function (PDF) and cumulative distribution function (CDF) of a normal random variable, and the definition of $d(x)$ above, compute the probability of stability $P(d(0.4) < 0)$.\n\nRound your final answer to four significant figures and express it as a unitless decimal between $0$ and $1$.",
            "solution": "The problem is found to be valid as it is scientifically grounded in the principles of computational thermodynamics, well-posed with all necessary information provided, and stated objectively.\n\nThe objective is to compute the probability of thermodynamic stability for a candidate compound at composition $x=0.4$. The condition for stability is given by the energy above the convex hull, $d(x)$, being less than zero.\n$$\nd(x) = E_{f}(x) - E_{\\mathrm{hull}}(x) < 0\n$$\nWe are asked to find the probability $P(d(0.4) < 0)$. Substituting the definition of $d(x)$ into the probability statement, we get:\n$$\nP(E_{f}(0.4) - E_{\\mathrm{hull}}(0.4) < 0)\n$$\nThis inequality can be rearranged to isolate the random variable, $E_{f}(0.4)$:\n$$\nP(E_{f}(0.4) < E_{\\mathrm{hull}}(0.4))\n$$\nThe problem states that the formation energy $E_{f}(0.4)$ is a normally distributed random variable, $E_{f}(0.4) \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, with mean $\\mu = -0.12$ and standard deviation $\\sigma = 0.03$. The energy of the convex hull is given as a deterministic value, $\\hat{E}_{\\mathrm{hull}}(0.4) = -0.10$.\n\nLet the random variable for the formation energy be denoted by $Y = E_{f}(0.4)$. We are therefore tasked with calculating $P(Y < -0.10)$, where $Y \\sim \\mathcal{N}(-0.12, (0.03)^{2})$.\n\nTo compute this probability, we standardize the random variable $Y$ by transforming it into a standard normal random variable, $Z$, which has a mean of $0$ and a standard deviation of $1$. The transformation is given by:\n$$\nZ = \\frac{Y - \\mu}{\\sigma}\n$$\nApplying this transformation to our inequality:\n$$\nP(Y < -0.10) = P\\left(\\frac{Y - \\mu}{\\sigma} < \\frac{-0.10 - \\mu}{\\sigma}\\right)\n$$\nThis is equivalent to finding the probability that the standard normal variable $Z$ is less than a specific value, which is the z-score of the threshold $-0.10$:\n$$\nP\\left(Z < \\frac{-0.10 - \\mu}{\\sigma}\\right)\n$$\nWe now substitute the given numerical values for $\\mu$ and $\\sigma$:\n$$\nP\\left(Z < \\frac{-0.10 - (-0.12)}{0.03}\\right) = P\\left(Z < \\frac{0.02}{0.03}\\right) = P\\left(Z < \\frac{2}{3}\\right)\n$$\nThe probability $P(Z < z)$ for a standard normal variable $Z$ is given by its cumulative distribution function (CDF), typically denoted as $\\Phi(z)$. Therefore, the required probability is:\n$$\nP(\\text{stability}) = \\Phi\\left(\\frac{2}{3}\\right)\n$$\nTo obtain a numerical value, we evaluate the CDF at $z = \\frac{2}{3} \\approx 0.6666...$. Using standard statistical tables or computational software, we find:\n$$\n\\Phi\\left(\\frac{2}{3}\\right) \\approx 0.74750746\n$$\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $7, 4, 7, 5$. The digit following the fifth place is $0$, so we round down.\n$$\nP(\\text{stability}) \\approx 0.7475\n$$\nThus, the probability that the candidate compound is stable at composition $x=0.4$ is approximately $0.7475$.",
            "answer": "$$\\boxed{0.7475}$$"
        },
        {
            "introduction": "Beyond single predictions, materials informatics aims to accelerate discovery by intelligently guiding the search for new materials. This practice introduces Bayesian optimization, a powerful framework for this task, by focusing on its core component: the acquisition function. You will derive and apply the Expected Improvement (EI) criterion to decide which candidate material to investigate next, learning how to strategically balance exploring uncertain options with exploiting promising ones. ",
            "id": "3464220",
            "problem": "You are running Bayesian optimization to maximize a costly-to-evaluate materials property (for example, the room-temperature bulk modulus) over a discrete set of alloy compositions. A Gaussian process (GP) surrogate model provides, for each candidate composition, a posterior predictive distribution that is Gaussian with mean $\\mu$ and standard deviation $\\sigma$. The current best observed property value across all completed experiments is $y^{*}$.\n\nFor three new candidate compositions, the GP posterior summaries are:\n- Means: $\\boldsymbol{\\mu} = [0.5, 0.7, 0.6]$,\n- Standard deviations: $\\boldsymbol{\\sigma} = [0.1, 0.2, 0.15]$,\n- Current best: $y^{*} = 0.65$.\n\nAssume that at any candidate, the predictive distribution of the unknown property $Y$ is $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ and define the improvement over the current best for maximization as $I = \\max(0, Y - y^{*})$. Starting from the definition of $I$ and the Gaussian probability density function and cumulative distribution function, derive an analytical expression for the expected improvement $\\operatorname{EI} = \\mathbb{E}[I]$ at a single candidate in terms of $\\mu$, $\\sigma$, and $y^{*}$, without using any pre-memorized formula. Then evaluate $\\operatorname{EI}$ numerically for each of the three candidates using the given $\\boldsymbol{\\mu}$, $\\boldsymbol{\\sigma}$, and $y^{*}$.\n\nSelect the next experiment as the candidate with the largest expected improvement. Report only the index $i \\in \\{1,2,3\\}$ (using $1$-based indexing) of the selected candidate as your final answer. The final answer is unitless and does not require rounding.",
            "solution": "The user has provided a problem that is scientifically grounded, well-posed, and objective. It is a standard application of Bayesian optimization principles within the field of computational materials science. All necessary data and definitions are provided, and no inconsistencies or ambiguities are present. The problem is valid.\n\nThe problem requires the derivation of the analytical expression for the Expected Improvement (EI) acquisition function and its subsequent application to select the next experimental point.\n\nLet the unknown materials property at a candidate composition be a random variable $Y$, which follows a Gaussian (normal) distribution given by the Gaussian process posterior: $Y \\sim \\mathcal{N}(\\mu, \\sigma^{2})$. The probability density function (PDF) of $Y$ is given by:\n$$\nf(y; \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{y-\\mu}{\\sigma}\\right)^2\\right)\n$$\nThe current best observed value is denoted by $y^{*}$. The improvement $I$ for a maximization problem is defined as the amount by which $Y$ exceeds $y^{*}$, and is zero if $Y$ is not better than $y^{*}$. This is mathematically expressed as:\n$$\nI = \\max(0, Y - y^{*})\n$$\nThe Expected Improvement, $\\operatorname{EI}$, is the expectation of $I$ with respect to the posterior distribution of $Y$.\n$$\n\\operatorname{EI} = \\mathbb{E}[I] = \\mathbb{E}[\\max(0, Y - y^{*})]\n$$\nBy the definition of expectation for a continuous random variable, this is calculated by integrating the improvement over its distribution:\n$$\n\\operatorname{EI} = \\int_{-\\infty}^{\\infty} \\max(0, y - y^{*}) f(y; \\mu, \\sigma) \\, dy\n$$\nThe term $\\max(0, y - y^{*})$ is non-zero only for $y > y^{*}$. Therefore, the integral's lower limit can be changed to $y^{*}$, and the integrand becomes $(y - y^{*})$:\n$$\n\\operatorname{EI} = \\int_{y^{*}}^{\\infty} (y - y^{*}) f(y; \\mu, \\sigma) \\, dy\n$$\nWe can split this integral into two parts:\n$$\n\\operatorname{EI} = \\int_{y^{*}}^{\\infty} y f(y; \\mu, \\sigma) \\, dy - y^{*} \\int_{y^{*}}^{\\infty} f(y; \\mu, \\sigma) \\, dy\n$$\nTo evaluate these integrals, we perform a change of variables to standardize the distribution. Let $z = \\frac{y - \\mu}{\\sigma}$. This implies $y = \\sigma z + \\mu$ and $dy = \\sigma \\, dz$. The PDF of the standard normal distribution $\\mathcal{N}(0, 1)$ is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{z^2}{2})$, and its cumulative distribution function (CDF) is $\\Phi(z) = \\int_{-\\infty}^{z} \\phi(t) \\, dt$. The original PDF can be written in terms of the standard normal PDF as $f(y; \\mu, \\sigma) = \\frac{1}{\\sigma}\\phi\\left(\\frac{y-\\mu}{\\sigma}\\right)$. Therefore, $f(y; \\mu, \\sigma) \\, dy = \\phi(z) \\, dz$.\n\nThe lower integration limit $y = y^{*}$ transforms to $z = \\frac{y^{*} - \\mu}{\\sigma}$. Let us denote this standardized value as $z_{y^{*}} = \\frac{y^{*} - \\mu}{\\sigma}$. The integrals become:\n$$\n\\operatorname{EI} = \\int_{z_{y^{*}}}^{\\infty} (\\sigma z + \\mu) \\phi(z) \\, dz - y^{*} \\int_{z_{y^{*}}}^{\\infty} \\phi(z) \\, dz\n$$\nLet's expand the first term:\n$$\n\\operatorname{EI} = \\sigma \\int_{z_{y^{*}}}^{\\infty} z \\phi(z) \\, dz + \\mu \\int_{z_{y^{*}}}^{\\infty} \\phi(z) \\, dz - y^{*} \\int_{z_{y^{*}}}^{\\infty} \\phi(z) \\, dz\n$$\n$$\n\\operatorname{EI} = (\\mu - y^{*}) \\int_{z_{y^{*}}}^{\\infty} \\phi(z) \\, dz + \\sigma \\int_{z_{y^{*}}}^{\\infty} z \\phi(z) \\, dz\n$$\nThe first integral is the probability $P(Z > z_{y^{*}})$ for a standard normal variable $Z$, which is $1 - \\Phi(z_{y^{*}})$.\nThe second integral can be solved directly:\n$$\n\\int z \\phi(z) \\, dz = \\int z \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right) \\, dz\n$$\nUsing the substitution $u = -z^2/2$, we have $du = -z \\, dz$.\n$$\n\\int -\\frac{1}{\\sqrt{2\\pi}} \\exp(u) \\, du = -\\frac{1}{\\sqrt{2\\pi}} \\exp(u) = -\\phi(z)\n$$\nEvaluating the definite integral:\n$$\n\\int_{z_{y^{*}}}^{\\infty} z \\phi(z) \\, dz = [-\\phi(z)]_{z_{y^{*}}}^{\\infty} = -\\lim_{z \\to \\infty}\\phi(z) - (-\\phi(z_{y^{*}})) = 0 + \\phi(z_{y^{*}}) = \\phi(z_{y^{*}})\n$$\nSubstituting these results back into the expression for $\\operatorname{EI}$:\n$$\n\\operatorname{EI} = (\\mu - y^{*}) [1 - \\Phi(z_{y^{*}})] + \\sigma \\phi(z_{y^{*}})\n$$\nwhere $z_{y^{*}} = \\frac{y^{*} - \\mu}{\\sigma}$.\nFor notational convenience, let's define a new variable $Z = \\frac{\\mu - y^{*}}{\\sigma} = -z_{y^{*}}$. Using the properties of the standard normal distribution, $\\phi(-z) = \\phi(z)$ and $1 - \\Phi(-z) = \\Phi(z)$, the expression becomes:\n$$\n\\operatorname{EI} = (\\mu - y^{*}) \\Phi(Z) + \\sigma \\phi(Z)\n$$\nThis is the final analytical expression for Expected Improvement.\n\nNow, we evaluate $\\operatorname{EI}$ for the three given candidates.\nThe data are:\n- Means: $\\mu_1 = 0.5$, $\\mu_2 = 0.7$, $\\mu_3 = 0.6$.\n- Standard deviations: $\\sigma_1 = 0.1$, $\\sigma_2 = 0.2$, $\\sigma_3 = 0.15$.\n- Current best: $y^{*} = 0.65$.\n\nFor Candidate 1 ($i=1$):\n$\\mu_1 = 0.5$, $\\sigma_1 = 0.1$.\n$Z_1 = \\frac{\\mu_1 - y^{*}}{\\sigma_1} = \\frac{0.5 - 0.65}{0.1} = -1.5$.\n$\\operatorname{EI}_1 = (0.5 - 0.65) \\Phi(-1.5) + 0.1 \\phi(-1.5)$.\nUsing standard tables or a calculator, $\\Phi(-1.5) \\approx 0.066807$ and $\\phi(-1.5) = \\phi(1.5) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-1.5^2/2) \\approx 0.129518$.\n$\\operatorname{EI}_1 \\approx (-0.15)(0.066807) + (0.1)(0.129518) \\approx -0.010021 + 0.0129518 \\approx 0.00293$.\n\nFor Candidate 2 ($i=2$):\n$\\mu_2 = 0.7$, $\\sigma_2 = 0.2$.\n$Z_2 = \\frac{\\mu_2 - y^{*}}{\\sigma_2} = \\frac{0.7 - 0.65}{0.2} = \\frac{0.05}{0.2} = 0.25$.\n$\\operatorname{EI}_2 = (0.7 - 0.65) \\Phi(0.25) + 0.2 \\phi(0.25)$.\nUsing standard tables or a calculator, $\\Phi(0.25) \\approx 0.598706$ and $\\phi(0.25) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-0.25^2/2) \\approx 0.386668$.\n$\\operatorname{EI}_2 \\approx (0.05)(0.598706) + (0.2)(0.386668) \\approx 0.029935 + 0.077334 \\approx 0.10727$.\n\nFor Candidate 3 ($i=3$):\n$\\mu_3 = 0.6$, $\\sigma_3 = 0.15$.\n$Z_3 = \\frac{\\mu_3 - y^{*}}{\\sigma_3} = \\frac{0.6 - 0.65}{0.15} = \\frac{-0.05}{0.15} = -\\frac{1}{3}$.\n$\\operatorname{EI}_3 = (0.6 - 0.65) \\Phi(-1/3) + 0.15 \\phi(-1/3)$.\nUsing a calculator, $\\Phi(-1/3) \\approx 0.369441$ and $\\phi(-1/3) = \\phi(1/3) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-(1/3)^2/2) \\approx 0.377383$.\n$\\operatorname{EI}_3 \\approx (-0.05)(0.369441) + (0.15)(0.377383) \\approx -0.018472 + 0.056607 \\approx 0.038135$.\n\nComparing the computed Expected Improvement values:\n$\\operatorname{EI}_1 \\approx 0.00293$\n$\\operatorname{EI}_2 \\approx 0.10727$\n$\\operatorname{EI}_3 \\approx 0.03814$\n\nThe largest value is $\\operatorname{EI}_2$. Therefore, the next experiment should be performed on Candidate 2. The problem asks for the $1$-based index of this candidate. The index is $2$.",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}