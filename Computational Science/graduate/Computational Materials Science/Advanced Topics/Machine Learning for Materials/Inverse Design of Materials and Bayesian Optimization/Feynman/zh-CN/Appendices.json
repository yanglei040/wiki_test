{
    "hands_on_practices": [
        {
            "introduction": "物理仿真模型常常存在系统性偏差，这给材料逆向设计带来了挑战。本练习将指导您使用高斯过程（GP）对这种模型偏差进行贝叶斯校准，从而构建一个融合了物理知识和观测数据的概率代理模型。通过这个实践，您将掌握从数据处理到不确定性量化，再到最终优化决策的完整逆向设计工作流程，这对于在现实世界中利用不完美模型进行材料发现至关重要。",
            "id": "3459016",
            "problem": "给定一个材料属性的随机模型，其中确定性过程变量 $x \\in [0,1]$ 通过一个基于物理的确定性模拟器 $f(x)$、一个未知的模型差异 $\\delta(x)$ 和一个随机测量误差 $\\epsilon$ 映射到一个测量属性 $y$。数据生成关系为 $y = f(x) + \\delta(x) + \\epsilon$，其中 $\\epsilon$ 被建模为具有已知方差的零均值高斯噪声。目标是使用高斯过程 (GP) 回归对差异 $\\delta(x)$ 进行贝叶斯校准，然后通过优化 $x$ 来利用校准后的后验分布实现目标属性值，从而执行逆向设计。\n\n使用的基本建模假设如下：\n- 以贝叶斯定理作为概率推断和基于观测值进行条件化的基础。\n- 差异 $\\delta(x)$ 的先验为一个零均值高斯过程 (GP)，其核函数为平方指数核，由振幅（方差）和长度尺度参数化，并附加具有已知方差的独立高斯噪声。\n- 确定性模拟器 $f(x)$ 是已知且固定的。\n\n设确定性模拟器为 $f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$，$x \\in [0,1]$。将差异 $\\delta(x)$ 视为一个零均值高斯过程，其平方指数协方差为 $k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$，其中 $\\sigma_f^2$ 是过程方差，$\\ell$ 是特征长度尺度。假设每种情况下的观测噪声方差 $\\sigma_n^2$ 均已知，并且给定潜函数时，观测值是条件独立的。\n\n你的任务是：\n1. 使用 GP 回归，通过对残差 $r_i = y_i - f(x_i)$ 进行操作，从训练数据中校准差异 $\\delta(x)$。在已知 $\\sigma_n^2$ 的情况下，通过最大化 GP 边际对数似然来估计超参数 $\\sigma_f^2$ 和 $\\ell$。将超参数限制在 $\\sigma_f^2 \\in [10^{-4}, 1]$ 和 $\\ell \\in [0.05, 1]$ 范围内。为保证数值稳定性，在对数参数空间中执行优化，并在必要时使用数值稳定的线性代数方法，并添加一个微小的正对角抖动。\n2. 对于任意 $x \\in [0,1]$，通过适当地组合确定性函数 $f(x)$ 和 $\\delta(x)$ 的 GP 后验，计算校准模型下 $y(x)$ 的后验均值和方差。\n3. 对于给定的目标属性值 $y^\\star$，将逆向设计目标定义为后验分布下与 $y^\\star$ 的期望平方偏差，即 $J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\text{data}]$。在 $[0,1]$ 区间内一个包含 $N=2001$ 个点的均匀网格（即网格间距为 $0.0005$）上，找到使 $J(x)$ 最小化的设计 $x^\\star \\in [0,1]$。如果存在多个最小值点，选择最小的 $x$。\n\n从第一性原理出发，利用贝叶斯决策理论和 $\\delta(x)$ 的 GP 后验的正态性，推导出一个用 $y(x)$ 的后验均值和方差表示的 $J(x)$ 的可计算表达式，并实现它。\n\n对于以下每个独立的测试用例，你必须：\n- 使用给定的训练输入和输出来校准 $\\delta(x)$ 的 GP 模型。\n- 在网格上计算 $y(x)$ 的后验均值 $\\mu_y(x)$ 和方差 $\\sigma_y^2(x)$。\n- 在网格上计算 $J(x)$，并返回其最小值点 $x^\\star$ 以及对应的后验均值 $\\mu_y(x^\\star)$。\n\n超参数估计必须使用最大似然估计 (MLE)，通过最大化关于 $\\sigma_f^2$ 和 $\\ell$ 的 GP 边际似然函数，并遵循上述边界约束。对矩阵分解使用数值稳定的方法。\n\n测试套件：\n- 案例 A：\n  - 训练输入 $X_{\\text{train}} = [\\,0.1,\\,0.4,\\,0.6,\\,0.8\\,]$。\n  - 训练输出 $Y_{\\text{train}} = [\\,0.57541019,\\,-0.3653731671,\\,-0.19004635764,\\,0.50077402636\\,]$。\n  - 已知噪声方差 $\\sigma_n^2 = 0.0009$。\n  - 目标属性 $y^\\star = 0.0$。\n- 案例 B：\n  - 训练输入 $X_{\\text{train}} = [\\,0.15,\\,0.35,\\,0.55,\\,0.75,\\,0.95\\,]$。\n  - 训练输出 $Y_{\\text{train}} = [\\,0.4626711514,\\,-0.2526697054,\\,-0.3690141424,\\,0.45364023493,\\,0.80121304604\\,]$。\n  - 已知噪声方差 $\\sigma_n^2 = 0.0025$。\n  - 目标属性 $y^\\star = 0.2$。\n- 案例 C：\n  - 训练输入 $X_{\\text{train}} = [\\,0.2,\\,0.5,\\,0.9\\,]$。\n  - 训练输出 $Y_{\\text{train}} = [\\,0.285410194,\\,-0.4345053083334,\\,0.7309048856666\\,]$。\n  - 已知噪声方差 $\\sigma_n^2 = 10^{-12}$。\n  - 目标属性 $y^\\star = 0.6$。\n\n角度单位不适用。除了所示的无量纲量外，不需要其他物理单位。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含三个案例的结果，格式为逗号分隔的列表的列表，不含空格。每个内部列表包含最优的 $x^\\star$ 和相应的后验均值 $\\mu_y(x^\\star)$，均四舍五入到六位小数。例如，你的输出必须类似于 $[[x_1^\\star,\\mu_1],[x_2^\\star,\\mu_2],[x_3^\\star,\\mu_3]]$，其中每个值都四舍五入到六位小数，并且整行中不含任何空格。\n\n你的程序必须是自包含的，不读取任何外部输入，并仅使用指定的数值库来实现上述校准和优化步骤。超参数的优化必须按照规定通过最大化边际似然来进行，而对 $x$ 的优化必须在规定的 $N=2001$ 个点的均匀网格上进行。",
            "solution": "该问题要求为材料属性的贝叶斯校准和逆向设计开发一个计算框架。解决方案包括三个主要阶段：(1) 构建概率模型和逆向设计的目标，(2) 通过从数据中估计超参数来校准模型，以及 (3) 优化设计变量以实现目标属性。\n\n**1. 概率模型构建**\n\n过程变量 $x \\in [0,1]$ 与测量属性 $y$ 之间的关系由以下随机模型给出：\n$$y = f(x) + \\delta(x) + \\epsilon$$\n此处，$f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$ 是已知的基于物理的确定性模拟器。项 $\\delta(x)$ 代表系统性模型差异，而 $\\epsilon$ 是一个随机测量误差。我们将 $\\epsilon$ 建模为独立同分布的高斯噪声，$\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$，其方差 $\\sigma_n^2$ 已知。\n\n校准任务的核心是从数据中推断未知的差异函数 $\\delta(x)$。我们采用贝叶斯方法，为 $\\delta(x)$ 设置一个高斯过程 (GP) 先验。GP 是函数上的一个分布，我们假设其先验为零均值：\n$$\\delta(x) \\sim \\mathcal{GP}(0, k(x, x'))$$\n协方差函数（或称核函数）$k(x, x')$ 定义了从 GP 中抽取的函数的性质。我们使用平方指数核：\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$$\n该核由振幅（过程方差）$\\sigma_f^2$ 和特征长度尺度 $\\ell$ 参数化。这些是我们 GP 模型的超参数。\n\n**2. 通过最大似然估计进行贝叶斯校准**\n\n给定一组训练数据 $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n$，我们首先计算残差 $r_i = y_i - f(x_i)$。根据我们的模型，这些残差是差异加上噪声的样本：$r_i = \\delta(x_i) + \\epsilon_i$。设 $\\mathbf{r} = [r_1, \\dots, r_n]^T$ 为残差向量，$\\mathbf{X}_{\\text{train}} = [x_1, \\dots, x_n]^T$ 为训练输入向量。向量 $\\mathbf{r}$ 服从一个多元高斯分布：\n$$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K} + \\sigma_n^2 \\mathbf{I})$$\n其中 $\\mathbf{K}$ 是 $n \\times n$ 的核矩阵，其元素为 $K_{ij} = k(x_i, x_j)$，$\\mathbf{I}$ 是单位矩阵。\n\n超参数 $\\theta = \\{\\sigma_f^2, \\ell\\}$ 是未知的。我们通过最大化观测残差的边际对数似然来估计它们。对数似然函数为：\n$$\\log p(\\mathbf{r} \\mid \\mathbf{X}_{\\text{train}}, \\theta) = -\\frac{1}{2} \\mathbf{r}^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r} - \\frac{1}{2} \\log |\\mathbf{K} + \\sigma_n^2 \\mathbf{I}| - \\frac{n}{2} \\log(2\\pi)$$\n为找到最优超参数 $\\theta^\\star$，我们在约束条件 $\\sigma_f^2 \\in [10^{-4}, 1]$ 和 $\\ell \\in [0.05, 1]$ 下最大化此函数。为保证数值稳定性，此优化在参数的对数上进行，即在 $\\log(\\sigma_f^2)$ 和 $\\log(\\ell)$ 上寻找 $\\arg\\max [\\log p]$。逆矩阵和行列式的计算通过对协方差矩阵 $\\mathbf{K}_{yy} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$ 进行 Cholesky 分解来高效且稳定地执行。在 $\\mathbf{K}_{yy}$ 的对角线上添加一个小的抖动项以确保其为正定矩阵。\n\n**3. 后验预测与逆向设计**\n\n有了优化后的超参数 $\\theta^\\star$，我们可以在任何新的测试点 $x_*$ 计算差异 $\\delta(x_*)$ 的后验分布。后验分布 $p(\\delta(x_*) \\mid \\mathcal{D})$ 也是高斯分布，其均值 $\\mu_\\delta(x_*)$ 和方差 $\\sigma_\\delta^2(x_*)$ 由以下公式给出：\n$$\\mu_\\delta(x_*) = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r}$$\n$$\\sigma_\\delta^2(x_*) = k(x_*, x_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*$$\n其中 $\\mathbf{k}_*$ 是一个向量，其元素为 $k(x_i, x_*)$，$i=1, \\dots, n$。\n\n逆向设计中我们感兴趣的属性是潜函数 $y(x) = f(x) + \\delta(x)$。由于 $f(x)$ 是确定性的， $y(x_*)$ 的后验分布是 $\\delta(x_*)$ 后验分布的一个平移版本：\n$$p(y(x_*) \\mid \\mathcal{D}) \\sim \\mathcal{N}(\\mu_y(x_*), \\sigma_y^2(x_*))$$\n其中后验均值和方差为：\n$$\\mu_y(x_*) = f(x_*) + \\mu_\\delta(x_*)$$\n$$\\sigma_y^2(x_*) = \\sigma_\\delta^2(x_*)$$\n\n逆向设计的目标是找到一个设计 $x^\\star$，它能最小化与目标值 $y^\\star$ 的期望平方偏差，其中期望是基于 $y(x)$ 的后验分布计算的：\n$$J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\mathcal{D}]$$\n对于一个均值为 $\\mu$、方差为 $\\sigma^2$ 的随机变量 $Z$，我们有 $\\mathbb{E}[(Z-c)^2] = \\text{Var}(Z) + (\\mathbb{E}[Z]-c)^2$。将此性质应用于我们的 $y(x)$ 的后验分布，我们得到目标函数的一个可计算表达式：\n$$J(x) = \\sigma_y^2(x) + (\\mu_y(x) - y^\\star)^2$$\n这个目标函数明智地平衡了两个目标：将后验均值 $\\mu_y(x)$ 推向目标 $y^\\star$（利用），以及通过在确定性高的区域进行设计来减小后验方差 $\\sigma_y^2(x)$（探索）。\n\n**4. 计算流程**\n\n对于每个测试用例，整体算法按以下步骤进行：\n1.  计算训练残差 $\\mathbf{r} = \\mathbf{Y}_{\\text{train}} - f(\\mathbf{X}_{\\text{train}})$。\n2.  将负边际对数似然定义为超参数优化的目标函数。\n3.  使用数值优化器（`L-BFGS-B`）在指定边界内找到最小化此目标函数的最优 $\\log(\\sigma_f^2)$ 和 $\\log(\\ell)$。\n4.  在 $[0,1]$ 区间内建立一个包含 $N=2001$ 个测试点 $x_j$ 的均匀网格。\n5.  使用优化后的超参数，计算网格上所有点的后验均值 $\\mu_y(x_j)$ 和方差 $\\sigma_y^2(x_j)$。\n6.  在整个网格上评估逆向设计目标 $J(x_j) = \\sigma_y^2(x_j) + (\\mu_y(x_j) - y^\\star)^2$。\n7.  找出对应于 $J(x)$ 最小值的网格点 $x^\\star$。如果多个点产生相同的最小值，根据问题说明选择 $x$ 最小的那个点。\n8.  该案例的最终结果是最优设计 $x^\\star$ 及其对应的后验均值属性值 $\\mu_y(x^\\star)$。\n对所有提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cho_solve, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian calibration and inverse design problem\n    for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"X_train\": np.array([0.1, 0.4, 0.6, 0.8]),\n            \"Y_train\": np.array([0.57541019, -0.3653731671, -0.19004635764, 0.50077402636]),\n            \"sigma_n_sq\": 0.0009,\n            \"y_star\": 0.0,\n        },\n        {\n            \"X_train\": np.array([0.15, 0.35, 0.55, 0.75, 0.95]),\n            \"Y_train\": np.array([0.4626711514, -0.2526697054, -0.3690141424, 0.45364023493, 0.80121304604]),\n            \"sigma_n_sq\": 0.0025,\n            \"y_star\": 0.2,\n        },\n        {\n            \"X_train\": np.array([0.2, 0.5, 0.9]),\n            \"Y_train\": np.array([0.285410194, -0.4345053083334, 0.7309048856666]),\n            \"sigma_n_sq\": 1e-12,\n            \"y_star\": 0.6,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_opt, mu_y_opt = process_case(\n            case[\"X_train\"],\n            case[\"Y_train\"],\n            case[\"sigma_n_sq\"],\n            case[\"y_star\"]\n        )\n        results.append([x_opt, mu_y_opt])\n    \n    # Format the final output string\n    result_str = \",\".join([f\"[{val[0]:.6f},{val[1]:.6f}]\" for val in results])\n    print(f\"[{result_str}]\")\n\ndef f(x):\n    \"\"\"The deterministic simulator f(x)\"\"\"\n    return 0.6 * np.cos(2 * np.pi * x) + 0.4 * x\n\ndef kernel(x1, x2, sigma_f_sq, l):\n    \"\"\"Squared-exponential kernel\"\"\"\n    # Using broadcasting to compute squared Euclidean distances\n    sq_dist = (x1.reshape(-1, 1) - x2.reshape(1, -1))**2\n    return sigma_f_sq * np.exp(-sq_dist / (2 * l**2))\n\ndef neg_log_likelihood(log_params, X, r, sigma_n_sq):\n    \"\"\"Negative marginal log-likelihood of the GP.\"\"\"\n    log_sigma_f_sq, log_l = log_params\n    sigma_f_sq = np.exp(log_sigma_f_sq)\n    l = np.exp(log_l)\n    \n    n = len(X)\n    jitter = 1e-8\n\n    K = kernel(X, X, sigma_f_sq, l)\n    K_yy = K + np.eye(n) * (sigma_n_sq + jitter)\n\n    try:\n        L = np.linalg.cholesky(K_yy)\n    except np.linalg.LinAlgError:\n        return np.inf\n\n    alpha = cho_solve((L, True), r)\n    log_det_K_yy = 2 * np.sum(np.log(np.diag(L)))\n    \n    nll = 0.5 * r.T @ alpha + 0.5 * log_det_K_yy + 0.5 * n * np.log(2 * np.pi)\n    return nll\n\ndef process_case(X_train, Y_train, sigma_n_sq, y_star):\n    \"\"\"\n    Processes a single test case: calibrates GP and performs inverse design.\n    \"\"\"\n    # 1. Compute residuals\n    r_train = Y_train - f(X_train)\n\n    # 2. Hyperparameter optimization\n    log_sigma_f_sq_bounds = (np.log(1e-4), np.log(1.0))\n    log_l_bounds = (np.log(0.05), np.log(1.0))\n    bounds = [log_sigma_f_sq_bounds, log_l_bounds]\n    \n    # Initial guess: center of the log-space hyperparameter box\n    x0 = [np.mean(b) for b in bounds]\n    \n    opt_result = minimize(\n        neg_log_likelihood,\n        x0=x0,\n        args=(X_train, r_train, sigma_n_sq),\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n    \n    opt_log_sigma_f_sq, opt_log_l = opt_result.x\n    opt_sigma_f_sq = np.exp(opt_log_sigma_f_sq)\n    opt_l = np.exp(opt_log_l)\n\n    # 3. Posterior Prediction on the grid\n    N_grid = 2001\n    x_grid = np.linspace(0, 1, N_grid)\n\n    # Pre-compute matrices for prediction\n    jitter = 1e-8\n    K_train = kernel(X_train, X_train, opt_sigma_f_sq, opt_l)\n    K_yy = K_train + np.eye(len(X_train)) * (sigma_n_sq + jitter)\n    L = np.linalg.cholesky(K_yy)\n    alpha = cho_solve((L, True), r_train)\n\n    # Predict at grid points\n    k_star = kernel(X_train, x_grid, opt_sigma_f_sq, opt_l)\n    \n    # Posterior mean for delta(x)\n    mu_delta_grid = k_star.T @ alpha\n    \n    # Posterior variance for delta(x)\n    v = solve_triangular(L, k_star, lower=True)\n    var_delta_grid = opt_sigma_f_sq - np.sum(v**2, axis=0)\n    \n    # Posterior for y(x) = f(x) + delta(x)\n    mu_y_grid = f(x_grid) + mu_delta_grid\n    var_y_grid = var_delta_grid\n\n    # 4. Inverse Design Optimization\n    # Objective function J(x) = E[(y(x) - y_star)^2]\n    J_grid = var_y_grid + (mu_y_grid - y_star)**2\n    \n    # Find minimizer\n    idx_min = np.argmin(J_grid)\n    x_star = x_grid[idx_min]\n    mu_y_at_x_star = mu_y_grid[idx_min]\n    \n    return x_star, mu_y_at_x_star\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "在序贯的材料发现流程中，关键在于决定下一步应进行哪个实验以获得最大收益，尤其是在面对成本与精度各异的多种仿真方法时。本练习聚焦于此决策问题，指导您从第一性原理出发，实现一种强大的采集函数——知识梯度（Knowledge Gradient, KG），用于量化和比较不同信息源（如MD和DFT）的单位成本预期价值。 掌握这项技能将使您能够设计出经济高效的计算实验方案，通过智能权衡来最大化信息获取效率。",
            "id": "3459019",
            "problem": "您正在为有限候选成分集合上的高斯后验模型下的逆向设计，设计一个信息高效的单步决策规则。您有两个信息源：分子动力学 (MD) 和密度泛函理论 (DFT)，它们在观测噪声和计算成本上有所不同。对于单个指定的候选索引，您必须计算知识梯度，该梯度定义为所有候选对象中最大后验均值目标值的预期单步增量，然后通过所选源的成本进行归一化，以获得每单位成本的期望价值。然后，您必须选择能够最大化每单位成本期望价值的源。\n\n从贝叶斯线性高斯推断和有限视界决策中的以下基本基础开始：\n- 在有限候选集合上的高斯过程的后验由一个均值向量 $\\,\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\,$ 和一个协方差矩阵 $\\,\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\,$ 描述。\n- 来自源 $\\,s \\in \\{\\text{MD}, \\text{DFT}\\}\\,$、在候选索引 $\\,q\\,$ 处的带噪观测被建模为 $\\,y = f_q + \\varepsilon_s\\,$，其中 $\\,\\varepsilon_s \\sim \\mathcal{N}(0,\\tau_s^2)\\,$，而 $\\,\\tau_s^2\\,$ 是源 $\\,s\\,$ 的观测噪声方差。\n- 在索引 $\\,q\\,$ 处观测到 $\\,y\\,$ 后，高斯条件化给出更新后的后验均值：\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr),\n$$\n且下面的知识梯度计算不需要更新后的协方差。\n- 使用源 $\\,s\\,$ 在索引 $\\,q\\,$ 进行单步测量的知识梯度，是在更新后的后验均值下最佳设计值的预期增量，减去当前最佳值：\n$$\n\\mathrm{KG}(q,s) \\equiv \\mathbb{E}\\Bigl[\\max_{i \\in \\{1,\\dots,n\\}} \\mu_i^+ \\Bigr] - \\max_{i \\in \\{1,\\dots,n\\}} \\mu_i.\n$$\n- 每单位成本的期望价值则为 $\\,\\mathrm{KG}(q,s) / c_s\\,$，其中 $\\,c_s\\,$ 是源 $\\,s\\,$ 的成本。\n\n您的任务是实现一个确定性算法，该算法对下面提供的每个测试用例，计算 $\\,\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}\\,$ 和 $\\,\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}\\,$，然后选择具有较大会单位成本期望价值的源。如果两个值在 $\\,10^{-12}\\,$ 的容差范围内相等，您必须选择分子动力学 (MD)。\n\n您必须使用第一性原理来实现知识梯度。即，使用上述事实将更新后的均值 $\\,\\boldsymbol{\\mu}^+\\,$ 表示为标量观测的仿射函数，然后通过在逐点最大值发生变化的断点上对标准正态密度进行积分，以闭式形式计算标准正态随机变量的仿射函数的最大值的期望。您的实现必须是完全确定性的，并且不得使用随机抽样。\n\n测试套件：\n- 案例 A：\n  - 候选数量 $\\,n = 3\\,$.\n  - 后验均值\n    $$\n    \\boldsymbol{\\mu}^{(A)} = \\begin{bmatrix} 0.5  0.6  0.4 \\end{bmatrix}.\n    $$\n  - 后验协方差\n    $$\n    \\boldsymbol{\\Sigma}^{(A)} = \\begin{bmatrix}\n    0.16  0.04  0.08 \\\\\n    0.04  0.26  0.07 \\\\\n    0.08  0.07  0.41\n    \\end{bmatrix}.\n    $$\n  - 测量索引 $\\,q^{(A)} = 2\\,$（使用基于 1 的人类索引，这对应于第二个条目；您的代码应在内部使用基于 0 的索引）。\n  - MD 噪声方差 $\\,\\tau_{\\text{MD}}^{2\\,(A)} = 0.2^2\\,$, DFT 噪声方差 $\\,\\tau_{\\text{DFT}}^{2\\,(A)} = 0.05^2\\,$.\n  - MD 成本 $\\,c_{\\text{MD}}^{(A)} = 1.0\\,$, DFT 成本 $\\,c_{\\text{DFT}}^{(A)} = 12.0\\,$.\n\n- 案例 B：\n  - 候选数量 $\\,n = 3\\,$.\n  - 后验均值\n    $$\n    \\boldsymbol{\\mu}^{(B)} = \\begin{bmatrix} 0.7  0.65  0.66 \\end{bmatrix}.\n    $$\n  - 后验协方差\n    $$\n    \\boldsymbol{\\Sigma}^{(B)} = \\begin{bmatrix}\n    0.0  0.0  0.0 \\\\\n    0.0  0.2  0.0 \\\\\n    0.0  0.0  0.1\n    \\end{bmatrix}.\n    $$\n  - 测量索引 $\\,q^{(B)} = 1\\,$（基于 1 的索引中的第一个条目）。\n  - MD 噪声方差 $\\,\\tau_{\\text{MD}}^{2\\,(B)} = 0.3^2\\,$, DFT 噪声方差 $\\,\\tau_{\\text{DFT}}^{2\\,(B)} = 0.05^2\\,$.\n  - MD 成本 $\\,c_{\\text{MD}}^{(B)} = 1.0\\,$, DFT 成本 $\\,c_{\\text{DFT}}^{(B)} = 10.0\\,$.\n\n- 案例 C：\n  - 候选数量 $\\,n = 4\\,$.\n  - 后验均值\n    $$\n    \\boldsymbol{\\mu}^{(C)} = \\begin{bmatrix} 1.0  0.9  0.95  0.8 \\end{bmatrix}.\n    $$\n  - 后验协方差（对称正定）\n    $$\n    \\boldsymbol{\\Sigma}^{(C)} = \\begin{bmatrix}\n    0.36  -0.12  0.06  0.00 \\\\\n    -0.12  0.53  -0.23  -0.14 \\\\\n    0.06  -0.23  0.35  0.11 \\\\\n    0.00  -0.14  0.11  0.21\n    \\end{bmatrix}.\n    $$\n  - 测量索引 $\\,q^{(C)} = 3\\,$（基于 1 的索引中的第三个条目）。\n  - MD 噪声方差 $\\,\\tau_{\\text{MD}}^{2\\,(C)} = 0.3^2\\,$, DFT 噪声方差 $\\,\\tau_{\\text{DFT}}^{2\\,(C)} = 0.1^2\\,$.\n  - MD 成本 $\\,c_{\\text{MD}}^{(C)} = 1.0\\,$, DFT 成本 $\\,c_{\\text{DFT}}^{(C)} = 3.0\\,$.\n\n数值和输出要求：\n- 对于每个测试用例，完全按照上面的定义计算 $\\,\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}\\,$ 和 $\\,\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}\\,$。\n- 所有中间计算都是无单位的，并使用实数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按顺序附加三个值：MD 每单位成本的期望价值，DFT 每单位成本的期望价值，以及所选源的整数表示（MD 为 $0$，DFT 为 $1$）。所有浮点值必须四舍五入到六位小数。例如，整体输出格式必须类似于 $[\\text{md}_A,\\text{dft}_A,\\text{choice}_A,\\text{md}_B,\\text{dft}_B,\\text{choice}_B,\\text{md}_C,\\text{dft}_C,\\text{choice}_C]$，并带有指定的四舍五入。",
            "solution": "用户提供的问题已经过验证，并被确定为是合理的。\n\n### 1. 问题验证\n\n**步骤 1：提取已知条件**\n\n-   **模型**：在 $n$ 个候选的有限集合上，目标函数的后验信念是一个多元高斯分布，其均值为 $\\boldsymbol{\\mu} \\in \\mathbb{R}^n$，协方差为 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}$。\n-   **观测模型**：来自源 $s \\in \\{\\text{MD}, \\text{DFT}\\}$、在候选索引 $q$ 处的带噪观测 $y$ 为 $y = f_q + \\varepsilon_s$，其中 $\\varepsilon_s \\sim \\mathcal{N}(0, \\tau_s^2)$。\n-   **后验均值更新规则**：$\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr)$。\n-   **知识梯度 (KG)**：$\\mathrm{KG}(q,s) \\equiv \\mathbb{E}\\Bigl[\\max_{i \\in \\{1,\\dots,n\\}} \\mu_i^+ \\Bigr] - \\max_{i \\in \\{1,\\dots,n\\}} \\mu_i$。\n-   **决策度量**：最大化每单位成本的期望价值，$\\mathrm{KG}(q,s) / c_s$。\n-   **平局决胜规则**：如果值在 $10^{-12}$ 的容差范围内相等，则选择 MD（源 $0$）。\n-   **测试用例**：\n    -   **案例 A**：$n = 3$, $\\boldsymbol{\\mu}^{(A)} = [0.5, 0.6, 0.4]$, $\\boldsymbol{\\Sigma}^{(A)} = \\begin{bmatrix} 0.16  0.04  0.08 \\\\ 0.04  0.26  0.07 \\\\ 0.08  0.07  0.41 \\end{bmatrix}$, $q^{(A)}=2$, $\\tau_{\\text{MD}}^{2\\,(A)} = 0.2^2$, $\\tau_{\\text{DFT}}^{2\\,(A)} = 0.05^2$, $c_{\\text{MD}}^{(A)} = 1.0$, $c_{\\text{DFT}}^{(A)} = 12.0$。\n    -   **案例 B**：$n = 3$, $\\boldsymbol{\\mu}^{(B)} = [0.7, 0.65, 0.66]$, $\\boldsymbol{\\Sigma}^{(B)} = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.0  0.2  0.0 \\\\ 0.0  0.0  0.1 \\end{bmatrix}$, $q^{(B)}=1$, $\\tau_{\\text{MD}}^{2\\,(B)} = 0.3^2$, $\\tau_{\\text{DFT}}^{2\\,(B)} = 0.05^2$, $c_{\\text{MD}}^{(B)} = 1.0$, $c_{\\text{DFT}}^{(B)} = 10.0$。\n    -   **案例 C**：$n = 4$, $\\boldsymbol{\\mu}^{(C)} = [1.0, 0.9, 0.95, 0.8]$, $\\boldsymbol{\\Sigma}^{(C)} = \\begin{bmatrix} 0.36  -0.12  0.06  0.00 \\\\ -0.12  0.53  -0.23  -0.14 \\\\ 0.06  -0.23  0.35  0.11 \\\\ 0.00  -0.14  0.11  0.21 \\end{bmatrix}$, $q^{(C)}=3$, $\\tau_{\\text{MD}}^{2\\,(C)} = 0.3^2$, $\\tau_{\\text{DFT}}^{2\\,(C)} = 0.1^2$, $c_{\\text{MD}}^{(C)} = 1.0$, $c_{\\text{DFT}}^{(C)} = 3.0$。\n-   **索引**：测量索引 $q$ 是以基于 1 的“人类”索引方式给出的。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n-   **科学依据**：该问题基于贝叶斯优化、高斯过程回归和决策理论（知识梯度）的标准原理，这些原理在计算材料科学和机器学习领域已经非常成熟。公式是正确的。\n-   **适定性**：问题定义清晰，为每个测试用例提供了所有必需的参数。任务是为一个闭式计算实现一个确定性算法，这保证了唯一解。\n-   **客观性**：语言精确且量化。没有主观或含糊的陈述。\n-   **完整性和一致性**：为每个测试用例提供的数据是完整的。协方差矩阵是对称的。它们的半正定性是有效协方差矩阵的要求。验证证实，所提供的 $\\boldsymbol{\\Sigma}$ 矩阵的所有特征值都是非负的，满足此属性。对于案例 B，当 $q=1$（索引 0）时，$\\boldsymbol{\\Sigma}^{(B)}_{0,0}=0$，这代表了一个物理上有效但简并的情景，其中一个属性是确定已知的，并且与其他属性不相关，导致预期知识梯度为零。\n\n**步骤 3：结论与行动**\n\n该问题在科学上是合理的，适定的，并且包罗万象。验证成功。将提供一个解决方案。\n\n### 2. 解法推导\n\n目标是在候选索引 $q$ 处选择测量源 $s \\in \\{\\text{MD}, \\text{DFT}\\}$，以最大化每单位成本的知识梯度 $\\mathrm{KG}(q,s)/c_s$。\n\n**步骤 2.1：表示更新后的后验均值**\n\n知识梯度的计算需要更新后后验均值最大值的期望，即 $\\mathbb{E}[\\max_i \\mu_i^+]$。更新规则是：\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr)\n$$\n观测值 $y$ 是一个随机变量。在当前后验下，索引 $q$ 处观测的预测分布是高斯分布：$y \\sim \\mathcal{N}(\\mu_q, \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2)$。因此，项 $(y - \\mu_q)$ 是一个零均值的高斯随机变量，其方差为 $\\sigma_p^2 = \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2$。我们可以使用一个标准正态变量 $Z \\sim \\mathcal{N}(0,1)$ 来表示这个随机变量：\n$$\ny - \\mu_q = Z \\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2} = Z \\sigma_p\n$$\n将此代入 $\\boldsymbol{\\mu}^+$ 的更新规则中：\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p^2} (Z \\sigma_p) = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p} Z\n$$\n让我们定义一个常数向量 $\\boldsymbol{b} = \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p} = \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}}$。每个候选 $i$ 的更新均值是 $Z$ 的一个仿射函数：\n$$\n\\mu_i^+(Z) = \\mu_i + b_i Z\n$$\n这里，$\\mu_i$ 是截距，而 $b_i$ 是斜率。\n\n**步骤 2.2：计算最大值的期望**\n\n知识梯度是 $\\mathrm{KG}(q,s) = \\mathbb{E}_Z\\left[\\max_i \\mu_i^+(Z)\\right] - \\max_i \\mu_i$。问题的核心是计算期望项：\n$$\n\\mathbb{E}_Z\\left[\\max_i(\\mu_i + b_i Z)\\right] = \\int_{-\\infty}^{\\infty} \\max_i(\\mu_i + b_i Z) \\phi(Z) dZ\n$$\n其中 $\\phi(Z)$ 是标准正态概率密度函数 (PDF)。函数 $f(Z) = \\max_i(\\mu_i + b_i Z)$ 是一个分段线性凸函数。最大化线发生变化的点是“断点”。两条线 $i$ 和 $j$ 之间的断点 $z_{ij}$ 可通过设置 $\\mu_i + b_i Z = \\mu_j + b_j Z$ 找到，得到：\n$$\nz_{ij} = \\frac{\\mu_j - \\mu_i}{b_i - b_j}\n$$\n这对任何斜率不相等的线对 ($b_i \\neq b_j$) 都有定义。\n\n所有唯一的、排序后的断点集合 $\\{ \\zeta_k \\}_{k=1}^m$ 将实线划分为 $m+1$ 个区间。在每个区间 $(\\zeta_k, \\zeta_{k+1})$ 内（其中 $\\zeta_0 = -\\infty$ 和 $\\zeta_{m+1} = \\infty$），只有一条线，比如 $\\mu_{i_k^*} + b_{i_k^*}Z$，是最大的。因此，期望积分可以分解为在这些区间上的积分之和：\n$$\n\\mathbb{E}_Z\\left[\\max_i \\mu_i^+(Z)\\right] = \\sum_{k=0}^{m} \\int_{\\zeta_k}^{\\zeta_{k+1}} (\\mu_{i_k^*} + b_{i_k^*} Z) \\phi(Z) dZ\n$$\n\n**步骤 2.3：解析积分**\n\n和中的每个积分都可以解析求解。我们需要 $\\phi(Z)$ 和 $Z\\phi(Z)$ 的积分：\n1.  $\\int_L^U \\phi(Z) dZ = \\Phi(U) - \\Phi(L)$，其中 $\\Phi(Z)$ 是标准正态累积分布函数 (CDF)。\n2.  $\\int_L^U Z \\phi(Z) dZ = \\int_L^U Z \\frac{1}{\\sqrt{2\\pi}} e^{-Z^2/2} dZ = [-\\phi(Z)]_L^U = \\phi(L) - \\phi(U)$。\n\n结合这些，单个区间 $(\\zeta_k, \\zeta_{k+1})$ 上的积分为：\n$$\n\\int_{\\zeta_k}^{\\zeta_{k+1}} (\\mu_{i_k^*} + b_{i_k^*} Z) \\phi(Z) dZ = \\mu_{i_k^*}[\\Phi(\\zeta_{k+1}) - \\Phi(\\zeta_k)] + b_{i_k^*}[\\phi(\\zeta_k) - \\phi(\\zeta_{k+1})]\n$$\n\n**步骤 2.4：确定性算法**\n\n对于每个源 $s$ 及其对应参数 $(\\tau_s^2, c_s)$：\n1.  计算斜率向量 $\\boldsymbol{b} = \\frac{\\boldsymbol{\\Sigma}_{:,q-1}}{\\sqrt{\\boldsymbol{\\Sigma}_{q-1,q-1} + \\tau_s^2}}$，其中 $q$ 是基于 1 的索引。截距为 $\\boldsymbol{a} = \\boldsymbol{\\mu}$。\n2.  在简并情况下，如果测量不提供信息（例如 $\\boldsymbol{\\Sigma}_{:,q-1}$ 是零向量），则所有 $b_i=0$，因此 $\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu}$。KG 为 0。\n3.  对所有 $i, j \\in \\{1,\\dots,n\\}$ 且 $b_i \\neq b_j$ 的情况，计算所有成对的断点 $z_{ij}$。收集唯一值并对其进行排序以得到 $\\{\\zeta_1, \\dots, \\zeta_m\\}$。\n4.  形成一个区间边界序列：$[-\\infty, \\zeta_1, \\dots, \\zeta_m, \\infty]$。\n5.  初始化总期望 $E = 0$。\n6.  遍历每个区间 $(\\zeta_k, \\zeta_{k+1})$：\n    a.  在区间内选择一个测试点（例如，中点）。\n    b.  在此测试点，找到最大化线的索引 $i_k^*$：$i_k^* = \\arg\\max_i (\\mu_i + b_i Z_{test})$。\n    c.  使用上面的解析公式计算此区间上的积分，并将其加到 $E$ 中。\n7.  知识梯度为 $\\mathrm{KG}(q,s) = E - \\max_i \\mu_i$。\n8.  价值为 $V_s = \\mathrm{KG}(q,s) / c_s$。\n\n计算出 $V_{\\text{MD}}$ 和 $V_{\\text{DFT}}$ 后，根据哪个值更大来选择源。如果 $|V_{\\text{MD}} - V_{\\text{DFT}}| \\le 10^{-12}$，则根据平局决胜规则选择 MD。实现将使用 `scipy.stats.norm` 来获取 $\\Phi$ (CDF) 和 $\\phi$ (PDF) 函数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_kg_per_cost(mu, Sigma, q_one_based, tau2, cost):\n    \"\"\"\n    Computes the knowledge gradient per unit cost for a measurement.\n    \"\"\"\n    if cost == 0:\n        return -np.inf\n\n    n = len(mu)\n    q_idx = q_one_based - 1\n\n    # Define intercepts `a` and slopes `b` for the affine functions of Z.\n    a = mu\n    \n    # The effective variance of the observation's predictive distribution.\n    var_predictive = Sigma[q_idx, q_idx] + tau2\n\n    # If the measurement is completely uninformative or the variance is zero.\n    sigma_q_col = Sigma[:, q_idx]\n    if var_predictive = 1e-15 or np.all(np.abs(sigma_q_col) = 1e-15):\n        return 0.0\n\n    b = sigma_q_col / np.sqrt(var_predictive)\n\n    # Compute all unique pairwise breakpoints.\n    breakpoints = set()\n    for i in range(n):\n        for j in range(i + 1, n):\n            if np.abs(b[i] - b[j]) > 1e-15:\n                z = (a[j] - a[i]) / (b[i] - b[j])\n                breakpoints.add(z)\n    \n    sorted_breakpoints = sorted(list(breakpoints))\n    \n    # Define the integration intervals using the breakpoints.\n    zeta_points = [-np.inf] + sorted_breakpoints + [np.inf]\n    \n    # Calculate the expected maximum of the updated mean by integrating.\n    expected_max_mu_plus = 0.0\n    for k in range(len(zeta_points) - 1):\n        z_low = zeta_points[k]\n        z_high = zeta_points[k+1]\n        \n        # Select a test point to find the maximizing line in the interval.\n        if np.isneginf(z_low):\n            test_z = z_high - 1.0\n        elif np.isposinf(z_high):\n            test_z = z_low + 1.0\n        else:\n            test_z = (z_low + z_high) / 2.0\n            \n        # Determine a_star and b_star for the maximizing line.\n        line_values = a + b * test_z\n        i_star = np.argmax(line_values)\n        a_star, b_star = a[i_star], b[i_star]\n        \n        # Analytically compute the integral of (a* + b*Z)phi(Z) over [z_low, z_high].\n        cdf_high = norm.cdf(z_high)\n        cdf_low = norm.cdf(z_low)\n        pdf_high = norm.pdf(z_high)\n        pdf_low = norm.pdf(z_low)\n        \n        # Handle -inf and +inf boundaries.\n        if np.isneginf(z_low): cdf_low, pdf_low = 0.0, 0.0\n        if np.isposinf(z_high): cdf_high, pdf_high = 1.0, 0.0\n            \n        term_a = a_star * (cdf_high - cdf_low)\n        term_b = b_star * (pdf_low - pdf_high)\n        \n        expected_max_mu_plus += term_a + term_b\n\n    # Compute the knowledge gradient and normalize by cost.\n    kg = expected_max_mu_plus - np.max(mu)\n    \n    return kg / cost\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Case A\n            \"mu\": np.array([0.5, 0.6, 0.4]),\n            \"Sigma\": np.array([\n                [0.16, 0.04, 0.08],\n                [0.04, 0.26, 0.07],\n                [0.08, 0.07, 0.41]\n            ]),\n            \"q\": 2,\n            \"md\": {\"tau2\": 0.2**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 12.0}\n        },\n        {\n            # Case B\n            \"mu\": np.array([0.7, 0.65, 0.66]),\n            \"Sigma\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.2, 0.0],\n                [0.0, 0.0, 0.1]\n            ]),\n            \"q\": 1,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 10.0}\n        },\n        {\n            # Case C\n            \"mu\": np.array([1.0, 0.9, 0.95, 0.8]),\n            \"Sigma\": np.array([\n                [0.36, -0.12, 0.06, 0.00],\n                [-0.12, 0.53, -0.23, -0.14],\n                [0.06, -0.23, 0.35, 0.11],\n                [0.00, -0.14, 0.11, 0.21]\n            ]),\n            \"q\": 3,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.1**2, \"cost\": 3.0}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case[\"mu\"]\n        Sigma = case[\"Sigma\"]\n        q = case[\"q\"]\n\n        # Calculate KG per unit cost for MD\n        val_md = calculate_kg_per_cost(mu, Sigma, q, case[\"md\"][\"tau2\"], case[\"md\"][\"cost\"])\n        \n        # Calculate KG per unit cost for DFT\n        val_dft = calculate_kg_per_cost(mu, Sigma, q, case[\"dft\"][\"tau2\"], case[\"dft\"][\"cost\"])\n\n        # Decide which source to use based on the specified tolerance and tie-breaking rule\n        # If val_md >= val_dft - 1e-12, choose MD.\n        if val_dft - val_md = 1e-12:\n            choice = 0  # MD\n        else:\n            choice = 1  # DFT\n        \n        results.append(f\"{val_md:.6f}\")\n        results.append(f\"{val_dft:.6f}\")\n        results.append(str(choice))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}