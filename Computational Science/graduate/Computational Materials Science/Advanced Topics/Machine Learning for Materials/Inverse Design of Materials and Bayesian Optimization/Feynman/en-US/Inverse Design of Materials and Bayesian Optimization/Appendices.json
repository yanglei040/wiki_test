{
    "hands_on_practices": [
        {
            "introduction": "Real-world scientific models are often imperfect. We might have a physics-based simulator, $f(x)$, that captures the main trends but systematically deviates from experimental reality. This exercise  tackles this common scenario head-on, providing a powerful framework for integrating imperfect physical models with sparse observational data. You will move beyond treating simulators as absolute truth and instead learn to quantify and correct for their deficiencies.\n\nThis practice challenges you to use Gaussian Processes not just as a black-box regressor, but as a principled tool for Bayesian calibration. The core task is to learn a probabilistic model for the *discrepancy* between a given simulator and a set of observations, creating a hybrid model that is both more accurate and provides a full account of its uncertainty. By completing this exercise, you will master the end-to-end workflow of calibrating a computational model and then leveraging the resulting probabilistic understanding to perform inverse design, finding the optimal input parameters to achieve a desired material property.",
            "id": "3459016",
            "problem": "You are given a stochastic model for a materials property, where a deterministic process variable $x \\in [0,1]$ maps to a measured property $y$ through a deterministic physics-based simulator $f(x)$, an unknown model discrepancy $\\delta(x)$, and a stochastic measurement error $\\epsilon$. The data-generating relationship is $y = f(x) + \\delta(x) + \\epsilon$, where $\\epsilon$ is modeled as zero-mean Gaussian noise with known variance. The goal is to perform Bayesian calibration of the discrepancy $\\delta(x)$ using Gaussian Process (GP) regression and then perform inverse design by optimizing $x$ to achieve a target property value using the calibrated posterior.\n\nFundamental modeling assumptions to use are:\n- Bayes' theorem as the foundation for probabilistic inference and conditioning on observations.\n- A zero-mean Gaussian Process (GP) prior for the discrepancy $\\delta(x)$ with a squared-exponential kernel parameterized by an amplitude (variance) and a length-scale, and additive independent Gaussian noise with known variance.\n- The deterministic simulator $f(x)$ is known and fixed.\n\nLet the deterministic simulator be $f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$ for $x \\in [0,1]$. Treat the discrepancy $\\delta(x)$ as a zero-mean GP with squared-exponential covariance $k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$, where $\\sigma_f^2$ is the process variance and $\\ell$ is the characteristic length-scale. Assume the observation noise variance $\\sigma_n^2$ is known for each case, and the observations are conditionally independent given the latent function.\n\nYour task is to:\n1. Calibrate the discrepancy $\\delta(x)$ from training data by operating on the residuals $r_i = y_i - f(x_i)$ using GP regression. Estimate the hyperparameters $\\sigma_f^2$ and $\\ell$ by maximizing the GP marginal log-likelihood under known $\\sigma_n^2$. Restrict the hyperparameters to $\\sigma_f^2 \\in [10^{-4}, 1]$ and $\\ell \\in [0.05, 1]$. Perform optimization in log-parameter space for numerical stability, and use a numerically stable linear algebra method with a small positive diagonal jitter if necessary.\n2. For any $x \\in [0,1]$, compute the posterior mean and variance of $y(x)$ under the calibrated model by appropriately combining the deterministic $f(x)$ and the GP posterior for $\\delta(x)$.\n3. For a given target property value $y^\\star$, define the inverse design objective as the expected squared deviation from $y^\\star$ under the posterior, that is, $J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\text{data}]$. Find the design $x^\\star \\in [0,1]$ minimizing $J(x)$ over a uniform grid of $N=2001$ points in $[0,1]$ (i.e., grid spacing $0.0005$). If there are ties, select the smallest $x$.\n\nFrom first principles, use Bayesian decision theory and the normality of the GP posterior for $\\delta(x)$ to derive a computable expression for $J(x)$ in terms of the posterior mean and variance of $y(x)$, and implement it.\n\nFor each independent test case below, you must:\n- Use the given training inputs and outputs to calibrate the GP for $\\delta(x)$.\n- Compute the posterior mean $\\mu_y(x)$ and variance $\\sigma_y^2(x)$ of $y(x)$ on the grid.\n- Compute $J(x)$ on the grid and return the minimizer $x^\\star$ and the corresponding posterior mean $\\mu_y(x^\\star)$.\n\nHyperparameter estimation must use Maximum Likelihood Estimation (MLE) by maximizing the GP marginal likelihood with respect to $\\sigma_f^2$ and $\\ell$, subject to the bounds specified above. Use a numerically stable method for matrix factorizations.\n\nTest suite:\n- Case A:\n  - Training inputs $X_{\\text{train}} = [0.1, 0.4, 0.6, 0.8]$.\n  - Training outputs $Y_{\\text{train}} = [0.57541019, -0.3653731671, -0.19004635764, 0.50077402636]$.\n  - Known noise variance $\\sigma_n^2 = 0.0009$.\n  - Target property $y^\\star = 0.0$.\n- Case B:\n  - Training inputs $X_{\\text{train}} = [0.15, 0.35, 0.55, 0.75, 0.95]$.\n  - Training outputs $Y_{\\text{train}} = [0.4626711514, -0.2526697054, -0.3690141424, 0.45364023493, 0.80121304604]$.\n  - Known noise variance $\\sigma_n^2 = 0.0025$.\n  - Target property $y^\\star = 0.2$.\n- Case C:\n  - Training inputs $X_{\\text{train}} = [0.2, 0.5, 0.9]$.\n  - Training outputs $Y_{\\text{train}} = [0.285410194, -0.4345053083334, 0.7309048856666]$.\n  - Known noise variance $\\sigma_n^2 = 10^{-12}$.\n  - Target property $y^\\star = 0.6$.\n\nAngle units are not applicable. No additional physical units are required beyond the dimensionless quantities shown.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of lists, with no spaces, where each inner list contains the optimal $x^\\star$ and the corresponding posterior mean $\\mu_y(x^\\star)$ rounded to six decimal places. For example, your output must look like $[[x_1^\\star,\\mu_1],[x_2^\\star,\\mu_2],[x_3^\\star,\\mu_3]]$, where each value is rounded to six decimal places and there are no spaces anywhere in the line.\n\nYour program must be self-contained, not read any external input, and implement the calibration and optimization steps described above using only the specified numerical libraries. The optimization over the hyperparameters must be performed by maximizing the marginal likelihood as specified, and the optimization over $x$ must be performed over the uniform grid of $N=2001$ points as specified.",
            "solution": "The problem requires the development of a computational framework for Bayesian calibration and inverse design of a materials property. The solution involves three primary stages: (1) formulating the probabilistic model and the objective for inverse design, (2) calibrating the model by estimating hyperparameters from data, and (3) optimizing the design variable to achieve a target property.\n\n**1. Probabilistic Model Formulation**\n\nThe relationship between the process variable $x \\in [0,1]$ and the measured property $y$ is given by the stochastic model:\n$$y = f(x) + \\delta(x) + \\epsilon$$\nHere, $f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$ is the known deterministic physics-based simulator. The term $\\delta(x)$ represents a systematic model discrepancy, and $\\epsilon$ is a random measurement error. We model $\\epsilon$ as independent and identically distributed Gaussian noise, $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$, with a known variance $\\sigma_n^2$.\n\nThe core of the calibration task is to infer the unknown discrepancy function, $\\delta(x)$, from data. We adopt a Bayesian approach by placing a Gaussian Process (GP) prior on $\\delta(x)$. A GP is a distribution over functions, and we assume a zero-mean prior:\n$$\\delta(x) \\sim \\mathcal{GP}(0, k(x, x'))$$\nThe covariance function, or kernel, $k(x, x')$, defines the properties of the functions drawn from the GP. We use the squared-exponential kernel:\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$$\nThis kernel is parameterized by the amplitude (process variance) $\\sigma_f^2$ and the characteristic length-scale $\\ell$. These are the hyperparameters of our GP model.\n\n**2. Bayesian Calibration via Maximum Likelihood Estimation**\n\nGiven a set of training data $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n$, we first compute the residuals $r_i = y_i - f(x_i)$. According to our model, these residuals are samples of the discrepancy plus noise: $r_i = \\delta(x_i) + \\epsilon_i$. Let $\\mathbf{r} = [r_1, \\dots, r_n]^T$ be the vector of residuals and $\\mathbf{X}_{\\text{train}} = [x_1, \\dots, x_n]^T$ be the vector of training inputs. The vector $\\mathbf{r}$ is drawn from a multivariate Gaussian distribution:\n$$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K} + \\sigma_n^2 \\mathbf{I})$$\nwhere $\\mathbf{K}$ is the $n \\times n$ kernel matrix with entries $K_{ij} = k(x_i, x_j)$ and $\\mathbf{I}$ is the identity matrix.\n\nThe hyperparameters $\\theta = \\{\\sigma_f^2, \\ell\\}$ are unknown. We estimate them by maximizing the marginal log-likelihood of the observed residuals. The log-likelihood function is:\n$$\\log p(\\mathbf{r} \\mid \\mathbf{X}_{\\text{train}}, \\theta) = -\\frac{1}{2} \\mathbf{r}^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r} - \\frac{1}{2} \\log |\\mathbf{K} + \\sigma_n^2 \\mathbf{I}| - \\frac{n}{2} \\log(2\\pi)$$\nTo find the optimal hyperparameters $\\theta^\\star$, we maximize this function subject to the constraints $\\sigma_f^2 \\in [10^{-4}, 1]$ and $\\ell \\in [0.05, 1]$. For numerical stability, this optimization is performed on the logarithm of the parameters, i.e., finding $\\arg\\max [\\log p]$ over $\\log(\\sigma_f^2)$ and $\\log(\\ell)$. The computation of the inverse and determinant is performed efficiently and stably using the Cholesky decomposition of the covariance matrix $\\mathbf{K}_{yy} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$. A small jitter term is added to the diagonal of $\\mathbf{K}_{yy}$ to ensure it is positive definite.\n\n**3. Posterior Prediction and Inverse Design**\n\nWith the optimized hyperparameters $\\theta^\\star$, we can compute the posterior distribution of the discrepancy $\\delta(x_*)$ at any new test point $x_*$. The posterior $p(\\delta(x_*) \\mid \\mathcal{D})$ is also Gaussian, with mean $\\mu_\\delta(x_*)$ and variance $\\sigma_\\delta^2(x_*)$ given by:\n$$\\mu_\\delta(x_*) = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r}$$\n$$\\sigma_\\delta^2(x_*) = k(x_*, x_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*$$\nwhere $\\mathbf{k}_*$ is a vector with elements $k(x_i, x_*)$ for $i=1, \\dots, n$.\n\nThe property of interest for inverse design is the latent function $y(x) = f(x) + \\delta(x)$. Since $f(x)$ is deterministic, the posterior distribution for $y(x_*)$ is a shifted version of the posterior for $\\delta(x_*)$:\n$$p(y(x_*) \\mid \\mathcal{D}) \\sim \\mathcal{N}(\\mu_y(x_*), \\sigma_y^2(x_*))$$\nwhere the posterior mean and variance are:\n$$\\mu_y(x_*) = f(x_*) + \\mu_\\delta(x_*)$$\n$$\\sigma_y^2(x_*) = \\sigma_\\delta^2(x_*)$$\n\nThe inverse design objective is to find the design $x^\\star$ that minimizes the expected squared deviation from a target value $y^\\star$, where the expectation is taken over the posterior distribution of $y(x)$:\n$$J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\mathcal{D}]$$\nUsing the law of total expectation, for a random variable $Z$ with mean $\\mu$ and variance $\\sigma^2$, we have $\\mathbb{E}[(Z-c)^2] = \\text{Var}(Z) + (\\mathbb{E}[Z]-c)^2$. Applying this to our posterior for $y(x)$, we obtain a computable expression for the objective function:\n$$J(x) = \\sigma_y^2(x) + (\\mu_y(x) - y^\\star)^2$$\nThis objective function judiciously balances two goals: driving the posterior mean $\\mu_y(x)$ towards the target $y^\\star$ (exploitation) and reducing the posterior variance $\\sigma_y^2(x)$ by designing in regions of high certainty (exploration).\n\n**4. Computational Procedure**\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Compute the training residuals $\\mathbf{r} = \\mathbf{Y}_{\\text{train}} - f(\\mathbf{X}_{\\text{train}})$.\n2.  Define the negative marginal log-likelihood as the objective function for hyperparameter optimization.\n3.  Use a numerical optimizer (`L-BFGS-B`) to find the optimal $\\log(\\sigma_f^2)$ and $\\log(\\ell)$ that minimize this objective, within the specified bounds.\n4.  Establish a uniform grid of $N=2001$ test points $x_j$ in $[0,1]$.\n5.  Using the optimized hyperparameters, calculate the posterior mean $\\mu_y(x_j)$ and variance $\\sigma_y^2(x_j)$ for all points on the grid.\n6.  Evaluate the inverse design objective $J(x_j) = \\sigma_y^2(x_j) + (\\mu_y(x_j) - y^\\star)^2$ across the grid.\n7.  Identify the grid point $x^\\star$ that corresponds to the minimum value of $J(x)$. If multiple points yield the same minimum value, the one with the smallest $x$ is chosen as per the problem specification.\n8.  The final result for the case is the optimal design $x^\\star$ and its corresponding posterior mean property value $\\mu_y(x^\\star)$.\nThis procedure is repeated for all provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cho_solve, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian calibration and inverse design problem\n    for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"X_train\": np.array([0.1, 0.4, 0.6, 0.8]),\n            \"Y_train\": np.array([0.57541019, -0.3653731671, -0.19004635764, 0.50077402636]),\n            \"sigma_n_sq\": 0.0009,\n            \"y_star\": 0.0,\n        },\n        {\n            \"X_train\": np.array([0.15, 0.35, 0.55, 0.75, 0.95]),\n            \"Y_train\": np.array([0.4626711514, -0.2526697054, -0.3690141424, 0.45364023493, 0.80121304604]),\n            \"sigma_n_sq\": 0.0025,\n            \"y_star\": 0.2,\n        },\n        {\n            \"X_train\": np.array([0.2, 0.5, 0.9]),\n            \"Y_train\": np.array([0.285410194, -0.4345053083334, 0.7309048856666]),\n            \"sigma_n_sq\": 1e-12,\n            \"y_star\": 0.6,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_opt, mu_y_opt = process_case(\n            case[\"X_train\"],\n            case[\"Y_train\"],\n            case[\"sigma_n_sq\"],\n            case[\"y_star\"]\n        )\n        results.append([x_opt, mu_y_opt])\n    \n    # Format the final output string\n    result_str = \",\".join([f\"[{val[0]:.6f},{val[1]:.6f}]\" for val in results])\n    print(f\"[{result_str}]\")\n\ndef f(x):\n    \"\"\"The deterministic simulator f(x)\"\"\"\n    return 0.6 * np.cos(2 * np.pi * x) + 0.4 * x\n\ndef kernel(x1, x2, sigma_f_sq, l):\n    \"\"\"Squared-exponential kernel\"\"\"\n    # Using broadcasting to compute squared Euclidean distances\n    sq_dist = (x1.reshape(-1, 1) - x2.reshape(1, -1))**2\n    return sigma_f_sq * np.exp(-sq_dist / (2 * l**2))\n\ndef neg_log_likelihood(log_params, X, r, sigma_n_sq):\n    \"\"\"Negative marginal log-likelihood of the GP.\"\"\"\n    log_sigma_f_sq, log_l = log_params\n    sigma_f_sq = np.exp(log_sigma_f_sq)\n    l = np.exp(log_l)\n    \n    n = len(X)\n    jitter = 1e-8\n\n    K = kernel(X, X, sigma_f_sq, l)\n    K_yy = K + np.eye(n) * (sigma_n_sq + jitter)\n\n    try:\n        L = np.linalg.cholesky(K_yy)\n    except np.linalg.LinAlgError:\n        return np.inf\n\n    alpha = cho_solve((L, True), r)\n    log_det_K_yy = 2 * np.sum(np.log(np.diag(L)))\n    \n    nll = 0.5 * r.T @ alpha + 0.5 * log_det_K_yy + 0.5 * n * np.log(2 * np.pi)\n    return nll\n\ndef process_case(X_train, Y_train, sigma_n_sq, y_star):\n    \"\"\"\n    Processes a single test case: calibrates GP and performs inverse design.\n    \"\"\"\n    # 1. Compute residuals\n    r_train = Y_train - f(X_train)\n\n    # 2. Hyperparameter optimization\n    log_sigma_f_sq_bounds = (np.log(1e-4), np.log(1.0))\n    log_l_bounds = (np.log(0.05), np.log(1.0))\n    bounds = [log_sigma_f_sq_bounds, log_l_bounds]\n    \n    # Initial guess: center of the log-space hyperparameter box\n    x0 = [np.mean(b) for b in bounds]\n    \n    opt_result = minimize(\n        neg_log_likelihood,\n        x0=x0,\n        args=(X_train, r_train, sigma_n_sq),\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n    \n    opt_log_sigma_f_sq, opt_log_l = opt_result.x\n    opt_sigma_f_sq = np.exp(opt_log_sigma_f_sq)\n    opt_l = np.exp(opt_log_l)\n\n    # 3. Posterior Prediction on the grid\n    N_grid = 2001\n    x_grid = np.linspace(0, 1, N_grid)\n\n    # Pre-compute matrices for prediction\n    jitter = 1e-8\n    K_train = kernel(X_train, X_train, opt_sigma_f_sq, opt_l)\n    K_yy = K_train + np.eye(len(X_train)) * (sigma_n_sq + jitter)\n    L = np.linalg.cholesky(K_yy)\n    alpha = cho_solve((L, True), r_train)\n\n    # Predict at grid points\n    k_star = kernel(X_train, x_grid, opt_sigma_f_sq, opt_l)\n    \n    # Posterior mean for delta(x)\n    mu_delta_grid = k_star.T @ alpha\n    \n    # Posterior variance for delta(x)\n    v = solve_triangular(L, k_star, lower=True)\n    var_delta_grid = opt_sigma_f_sq - np.sum(v**2, axis=0)\n    \n    # Posterior for y(x) = f(x) + delta(x)\n    mu_y_grid = f(x_grid) + mu_delta_grid\n    var_y_grid = var_delta_grid\n\n    # 4. Inverse Design Optimization\n    # Objective function J(x) = E[(y(x) - y_star)^2]\n    J_grid = var_y_grid + (mu_y_grid - y_star)**2\n    \n    # Find minimizer\n    idx_min = np.argmin(J_grid)\n    x_star = x_grid[idx_min]\n    mu_y_at_x_star = mu_y_grid[idx_min]\n    \n    return x_star, mu_y_at_x_star\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "In materials discovery, computational experiments like Density Functional Theory (DFT) and Molecular Dynamics (MD) are powerful but expensive. This makes the choice of which material composition to simulate next a critical, high-stakes decision. This problem  places you in the role of a research strategist who must allocate a limited computational budget wisely to maximize the rate of discovery.\n\nHere, you will implement the Knowledge Gradient (KG), a sophisticated acquisition function from Bayesian optimization that formally quantifies the expected value of running a new simulation. You will extend this concept to a realistic multi-fidelity setting, learning to mathematically balance the high cost of accurate DFT calculations against the lower cost but noisier information from MD simulations. This practice moves beyond simply building a static model to actively using it for sequential decision-making, providing a deep, quantitative understanding of how to measure the \"value of information\" to design efficient, adaptive experimental campaigns.",
            "id": "3459019",
            "problem": "You are designing an information-efficient one-step decision rule for inverse design under a Gaussian posterior model on a finite set of candidate compositions. You have two information sources: Molecular Dynamics (MD) and Density Functional Theory (DFT), which differ by their observational noise and computational cost. At a single specified candidate index, you must compute the knowledge gradient, defined as the expected single-step increase in the maximum posterior mean objective value across all candidates, and then normalize by the cost of the selected source to obtain the expected value per unit cost. You must then choose the source that maximizes expected value per unit cost.\n\nStart from the following fundamental base in Bayesian linear-Gaussian inference and finite-horizon decision making:\n- The posterior of a Gaussian process on a finite set of candidates is described by a mean vector $\\boldsymbol{\\mu} \\in \\mathbb{R}^n$ and a covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}$.\n- A noisy observation at candidate index $q$ from source $s \\in \\{\\text{MD}, \\text{DFT}\\}$ is modeled as $y = f_q + \\varepsilon_s$ with $\\varepsilon_s \\sim \\mathcal{N}(0,\\tau_s^2)$, where $\\tau_s^2$ is the observation noise variance for source $s$.\n- Gaussian conditioning gives an updated posterior mean after observing $y$ at index $q$:\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr),\n$$\nand the updated covariance is not required for the knowledge gradient calculation below.\n- The knowledge gradient for a one-step measurement at index $q$ using source $s$ is the expected increase in the value of the best design under the updated posterior mean, minus the current best value:\n$$\n\\mathrm{KG}(q,s) \\equiv \\mathbb{E}\\Bigl[\\max_{i \\in \\{1,\\dots,n\\}} \\mu_i^+ \\Bigr] - \\max_{i \\in \\{1,\\dots,n\\}} \\mu_i.\n$$\n- The expected value per unit cost is then $\\mathrm{KG}(q,s) / c_s$, where $c_s$ is the cost of source $s$.\n\nYour task is to implement a deterministic algorithm that, for each test case provided below, computes $\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}$ and $\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}$ and then selects the source with the larger expected value per unit cost. If the two values are equal within a tolerance of $10^{-12}$, you must choose Molecular Dynamics (MD).\n\nYou must implement the knowledge gradient using first principles. Namely, use the facts above to express the updated mean $\\boldsymbol{\\mu}^+$ as an affine function of the scalar observation, and then compute the expectation of the maximum of affine functions of a standard normal random variable in closed form via integration against the standard normal density over the breakpoints where the pointwise maximum changes identity. Your implementation must be fully deterministic and must not use random sampling.\n\nTest suite:\n- Case A:\n  - Number of candidates $n = 3$.\n  - Posterior mean\n    $$ \\boldsymbol{\\mu}^{(A)} = \\begin{bmatrix} 0.5  0.6  0.4 \\end{bmatrix} $$\n  - Posterior covariance\n    $$ \\boldsymbol{\\Sigma}^{(A)} = \\begin{bmatrix}\n    0.16  0.04  0.08 \\\\\n    0.04  0.26  0.07 \\\\\n    0.08  0.07  0.41\n    \\end{bmatrix} $$\n  - Measurement index $q^{(A)} = 2$ (using one-based human indexing, this corresponds to the second entry; your code should use zero-based indexing internally).\n  - MD noise variance $\\tau_{\\text{MD}}^{2\\,(A)} = 0.2^2$, DFT noise variance $\\tau_{\\text{DFT}}^{2\\,(A)} = 0.05^2$.\n  - MD cost $c_{\\text{MD}}^{(A)} = 1.0$, DFT cost $c_{\\text{DFT}}^{(A)} = 12.0$.\n\n- Case B:\n  - Number of candidates $n = 3$.\n  - Posterior mean\n    $$ \\boldsymbol{\\mu}^{(B)} = \\begin{bmatrix} 0.7  0.65  0.66 \\end{bmatrix} $$\n  - Posterior covariance\n    $$ \\boldsymbol{\\Sigma}^{(B)} = \\begin{bmatrix}\n    0.0  0.0  0.0 \\\\\n    0.0  0.2  0.0 \\\\\n    0.0  0.0  0.1\n    \\end{bmatrix} $$\n  - Measurement index $q^{(B)} = 1$ (first entry in one-based indexing).\n  - MD noise variance $\\tau_{\\text{MD}}^{2\\,(B)} = 0.3^2$, DFT noise variance $\\tau_{\\text{DFT}}^{2\\,(B)} = 0.05^2$.\n  - MD cost $c_{\\text{MD}}^{(B)} = 1.0$, DFT cost $c_{\\text{DFT}}^{(B)} = 10.0$.\n\n- Case C:\n  - Number of candidates $n = 4$.\n  - Posterior mean\n    $$ \\boldsymbol{\\mu}^{(C)} = \\begin{bmatrix} 1.0  0.9  0.95  0.8 \\end{bmatrix} $$\n  - Posterior covariance (symmetric positive definite)\n    $$ \\boldsymbol{\\Sigma}^{(C)} = \\begin{bmatrix}\n    0.36  -0.12  0.06  0.00 \\\\\n    -0.12  0.53  -0.23  -0.14 \\\\\n    0.06  -0.23  0.35  0.11 \\\\\n    0.00  -0.14  0.11  0.21\n    \\end{bmatrix} $$\n  - Measurement index $q^{(C)} = 3$ (third entry in one-based indexing).\n  - MD noise variance $\\tau_{\\text{MD}}^{2\\,(C)} = 0.3^2$, DFT noise variance $\\tau_{\\text{DFT}}^{2\\,(C)} = 0.1^2$.\n  - MD cost $c_{\\text{MD}}^{(C)} = 1.0$, DFT cost $c_{\\text{DFT}}^{(C)} = 3.0$.\n\nNumerical and output requirements:\n- For each test case, compute $\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}$ and $\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}$ exactly as defined above.\n- All intermediate calculations are unitless and use real numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, append three values in order: the MD expected value per unit cost, the DFT expected value per unit cost, and the chosen source as an integer ($0$ for MD and $1$ for DFT). All floating-point values must be rounded to six decimal places. For example, the overall output format must be like $[\\text{md}_A,\\text{dft}_A,\\text{choice}_A,\\text{md}_B,\\text{dft}_B,\\text{choice}_B,\\text{md}_C,\\text{dft}_C,\\text{choice}_C]$ with the specified rounding.",
            "solution": "The user-provided problem has been validated and is determined to be sound.\n\n### 1. Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Model**: The posterior belief over the objective function on a finite set of $n$ candidates is a multivariate Gaussian with mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^n$ and covariance $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}$.\n-   **Observation Model**: A noisy observation $y$ at candidate index $q$ from source $s \\in \\{\\text{MD}, \\text{DFT}\\}$ is $y = f_q + \\varepsilon_s$, where $\\varepsilon_s \\sim \\mathcal{N}(0, \\tau_s^2)$.\n-   **Posterior Mean Update Rule**: $\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr)$.\n-   **Knowledge Gradient (KG)**: $\\mathrm{KG}(q,s) \\equiv \\mathbb{E}\\Bigl[\\max_{i \\in \\{1,\\dots,n\\}} \\mu_i^+ \\Bigr] - \\max_{i \\in \\{1,\\dots,n\\}} \\mu_i$.\n-   **Decision Metric**: Maximize the expected value per unit cost, $\\mathrm{KG}(q,s) / c_s$.\n-   **Tie-Breaking Rule**: If values are equal within a tolerance of $10^{-12}$, choose MD (source $0$).\n-   **Test Cases**:\n    -   **Case A**: $n = 3$, $\\boldsymbol{\\mu}^{(A)} = [0.5, 0.6, 0.4]$, $\\boldsymbol{\\Sigma}^{(A)} = \\begin{bmatrix} 0.16  0.04  0.08 \\\\ 0.04  0.26  0.07 \\\\ 0.08  0.07  0.41 \\end{bmatrix}$, $q^{(A)}=2$, $\\tau_{\\text{MD}}^{2\\,(A)} = 0.2^2$, $\\tau_{\\text{DFT}}^{2\\,(A)} = 0.05^2$, $c_{\\text{MD}}^{(A)} = 1.0$, $c_{\\text{DFT}}^{(A)} = 12.0$.\n    -   **Case B**: $n = 3$, $\\boldsymbol{\\mu}^{(B)} = [0.7, 0.65, 0.66]$, $\\boldsymbol{\\Sigma}^{(B)} = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.0  0.2  0.0 \\\\ 0.0  0.0  0.1 \\end{bmatrix}$, $q^{(B)}=1$, $\\tau_{\\text{MD}}^{2\\,(B)} = 0.3^2$, $\\tau_{\\text{DFT}}^{2\\,(B)} = 0.05^2$, $c_{\\text{MD}}^{(B)} = 1.0$, $c_{\\text{DFT}}^{(B)} = 10.0$.\n    -   **Case C**: $n = 4$, $\\boldsymbol{\\mu}^{(C)} = [1.0, 0.9, 0.95, 0.8]$, $\\boldsymbol{\\Sigma}^{(C)} = \\begin{bmatrix} 0.36  -0.12  0.06  0.00 \\\\ -0.12  0.53  -0.23  -0.14 \\\\ 0.06  -0.23  0.35  0.11 \\\\ 0.00  -0.14  0.11  0.21 \\end{bmatrix}$, $q^{(C)}=3$, $\\tau_{\\text{MD}}^{2\\,(C)} = 0.3^2$, $\\tau_{\\text{DFT}}^{2\\,(C)} = 0.1^2$, $c_{\\text{MD}}^{(C)} = 1.0$, $c_{\\text{DFT}}^{(C)} = 3.0$.\n-   **Indexing**: Measurement index $q$ is given in one-based \"human\" indexing.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is based on standard principles of Bayesian optimization, Gaussian process regression, and decision theory (knowledge gradient), which are well-established in computational materials science and machine learning. The formulas are correct.\n-   **Well-Posed**: The problem is clearly defined, with all necessary parameters provided for each test case. The task is to implement a deterministic algorithm for a closed-form calculation, which guarantees a unique solution.\n-   **Objective**: The language is precise and quantitative. There are no subjective or ambiguous statements.\n-   **Completeness and Consistency**: The provided data for each test case is complete. The covariance matrices are symmetric. Their positive semi-definiteness is a requirement for a valid covariance matrix. Validation confirms that all eigenvalues of the provided $\\boldsymbol{\\Sigma}$ matrices are non-negative, satisfying this property. For Case B, $\\boldsymbol{\\Sigma}^{(B)}_{0,0}=0$ with $q=1$ (index $0$), which represents a physically valid, albeit degenerate, scenario where a property is known with certainty and is uncorrelated with others, leading to an expected knowledge gradient of zero.\n\n**Step 3: Verdict and Action**\n\nThe problem is scientifically sound, well-posed, and all-encompassing. The validation is successful. A solution will be provided.\n\n### 2. Solution Derivation\n\nThe objective is to select the measurement source $s \\in \\{\\text{MD}, \\text{DFT}\\}$ at a candidate index $q$ that maximizes the knowledge gradient per unit cost, $\\mathrm{KG}(q,s)/c_s$.\n\n**Step 2.1: Expressing the Updated Posterior Mean**\n\nThe knowledge gradient calculation requires the expectation of the maximum of the updated posterior mean, $\\mathbb{E}[\\max_i \\mu_i^+]$. The update rule is:\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr)\n$$\nThe observation $y$ is a random variable. Under the current posterior, the predictive distribution for an observation at index $q$ is Gaussian: $y \\sim \\mathcal{N}(\\mu_q, \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2)$. The term $(y - \\mu_q)$ is therefore a zero-mean Gaussian random variable with variance $\\sigma_p^2 = \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2$. We can express this random variable using a standard normal variable $Z \\sim \\mathcal{N}(0,1)$:\n$$\ny - \\mu_q = Z \\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2} = Z \\sigma_p\n$$\nSubstituting this into the update rule for $\\boldsymbol{\\mu}^+$:\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p^2} (Z \\sigma_p) = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p} Z\n$$\nLet's define a constant vector $\\boldsymbol{b} = \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p} = \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}}$. The updated mean for each candidate $i$ is an affine function of $Z$:\n$$\n\\mu_i^+(Z) = \\mu_i + b_i Z\n$$\nHere, $\\mu_i$ is the intercept and $b_i$ is the slope.\n\n**Step 2.2: Computing the Expectation of the Maximum**\n\nThe knowledge gradient is $\\mathrm{KG}(q,s) = \\mathbb{E}_Z\\left[\\max_i \\mu_i^+(Z)\\right] - \\max_i \\mu_i$. The core of the problem is to compute the expectation term:\n$$\n\\mathbb{E}_Z\\left[\\max_i(\\mu_i + b_i Z)\\right] = \\int_{-\\infty}^{\\infty} \\max_i(\\mu_i + b_i Z) \\phi(Z) dZ\n$$\nwhere $\\phi(Z)$ is the standard normal probability density function (PDF). The function $f(Z) = \\max_i(\\mu_i + b_i Z)$ is a piecewise linear, convex function. The points where the maximizing line changes are the \"breakpoints\". A breakpoint $z_{ij}$ between two lines $i$ and $j$ is found by setting $\\mu_i + b_i Z = \\mu_j + b_j Z$, which yields:\n$$\nz_{ij} = \\frac{\\mu_j - \\mu_i}{b_i - b_j}\n$$\nThis is defined for any pair of lines with non-equal slopes ($b_i \\neq b_j$).\n\nThe set of all unique, sorted breakpoints $\\{ \\zeta_k \\}_{k=1}^m$ partitions the real line into $m+1$ intervals. Within each interval $(\\zeta_k, \\zeta_{k+1})$ (with $\\zeta_0 = -\\infty$ and $\\zeta_{m+1} = \\infty$), a single line, say $\\mu_{i_k^*} + b_{i_k^*}Z$, is maximal. The expectation integral can thus be split into a sum of integrals over these intervals:\n$$\n\\mathbb{E}_Z\\left[\\max_i \\mu_i^+(Z)\\right] = \\sum_{k=0}^{m} \\int_{\\zeta_k}^{\\zeta_{k+1}} (\\mu_{i_k^*} + b_{i_k^*} Z) \\phi(Z) dZ\n$$\n\n**Step 2.3: Analytical Integration**\n\nEach integral in the sum can be solved analytically. We require integrals of $\\phi(Z)$ and $Z\\phi(Z)$:\n1.  $\\int_L^U \\phi(Z) dZ = \\Phi(U) - \\Phi(L)$, where $\\Phi(Z)$ is the standard normal cumulative distribution function (CDF).\n2.  $\\int_L^U Z \\phi(Z) dZ = \\int_L^U Z \\frac{1}{\\sqrt{2\\pi}} e^{-Z^2/2} dZ = [-\\phi(Z)]_L^U = \\phi(L) - \\phi(U)$.\n\nCombining these, the integral over a single interval $(\\zeta_k, \\zeta_{k+1})$ is:\n$$\n\\int_{\\zeta_k}^{\\zeta_{k+1}} (\\mu_{i_k^*} + b_{i_k^*} Z) \\phi(Z) dZ = \\mu_{i_k^*}[\\Phi(\\zeta_{k+1}) - \\Phi(\\zeta_k)] + b_{i_k^*}[\\phi(\\zeta_k) - \\phi(\\zeta_{k+1})]\n$$\n\n**Step 2.4: Deterministic Algorithm**\n\nFor each source $s$ and its corresponding parameters $(\\tau_s^2, c_s)$:\n1.  Calculate the vector of slopes $\\boldsymbol{b} = \\frac{\\boldsymbol{\\Sigma}_{:,q-1}}{\\sqrt{\\boldsymbol{\\Sigma}_{q-1,q-1} + \\tau_s^2}}$, where $q$ is the one-based index. The intercepts are $\\boldsymbol{a} = \\boldsymbol{\\mu}$.\n2.  In the degenerate case where the measurement is uninformative (e.g., $\\boldsymbol{\\Sigma}_{:,q-1}$ is a zero vector), all $b_i=0$, so $\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu}$. The KG is $0$.\n3.  Compute all pairwise breakpoints $z_{ij}$ for $i, j \\in \\{1,\\dots,n\\}$ where $b_i \\neq b_j$. Collect the unique values and sort them to get $\\{\\zeta_1, \\dots, \\zeta_m\\}$.\n4.  Form a sequence of interval boundaries: $[-\\infty, \\zeta_1, \\dots, \\zeta_m, \\infty]$.\n5.  Initialize total expectation $E = 0$.\n6.  Iterate through each interval $(\\zeta_k, \\zeta_{k+1})$:\n    a.  Select a test point within the interval (e.g., the midpoint).\n    b.  At this test point, find the index $i_k^*$ of the maximizing line: $i_k^* = \\arg\\max_i (\\mu_i + b_i Z_{test})$.\n    c.  Calculate the integral over this interval using the analytical formula above and add it to $E$.\n7.  The knowledge gradient is $\\mathrm{KG}(q,s) = E - \\max_i \\mu_i$.\n8.  The value is $V_s = \\mathrm{KG}(q,s) / c_s$.\n\nAfter computing $V_{\\text{MD}}$ and $V_{\\text{DFT}}$, the source is chosen based on which value is larger. If $|V_{\\text{MD}} - V_{\\text{DFT}}| \\le 10^{-12}$, MD is chosen as per the tie-breaking rule. The implementation will use `scipy.stats.norm` for the $\\Phi$ (CDF) and $\\phi$ (PDF) functions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_kg_per_cost(mu, Sigma, q_one_based, tau2, cost):\n    \"\"\"\n    Computes the knowledge gradient per unit cost for a measurement.\n    \"\"\"\n    if cost = 0:\n        return -np.inf\n\n    n = len(mu)\n    q_idx = q_one_based - 1\n\n    # Define intercepts `a` and slopes `b` for the affine functions of Z.\n    a = mu\n    \n    # The effective variance of the observation's predictive distribution.\n    var_predictive = Sigma[q_idx, q_idx] + tau2\n\n    # If the measurement is completely uninformative or the variance is zero.\n    sigma_q_col = Sigma[:, q_idx]\n    if var_predictive = 1e-15 or np.all(np.abs(sigma_q_col)  1e-15):\n        return 0.0\n\n    b = sigma_q_col / np.sqrt(var_predictive)\n\n    # Compute all unique pairwise breakpoints.\n    breakpoints = set()\n    for i in range(n):\n        for j in range(i + 1, n):\n            if np.abs(b[i] - b[j]) > 1e-15:\n                z = (a[j] - a[i]) / (b[i] - b[j])\n                breakpoints.add(z)\n    \n    sorted_breakpoints = sorted(list(breakpoints))\n    \n    # Define the integration intervals using the breakpoints.\n    zeta_points = [-np.inf] + sorted_breakpoints + [np.inf]\n    \n    # Calculate the expected maximum of the updated mean by integrating.\n    expected_max_mu_plus = 0.0\n    for k in range(len(zeta_points) - 1):\n        z_low = zeta_points[k]\n        z_high = zeta_points[k+1]\n        \n        # Select a test point to find the maximizing line in the interval.\n        if np.isneginf(z_low):\n            test_z = z_high - 1.0\n        elif np.isposinf(z_high):\n            test_z = z_low + 1.0\n        else:\n            test_z = (z_low + z_high) / 2.0\n            \n        # Determine a_star and b_star for the maximizing line.\n        line_values = a + b * test_z\n        i_star = np.argmax(line_values)\n        a_star, b_star = a[i_star], b[i_star]\n        \n        # Analytically compute the integral of (a* + b*Z)phi(Z) over [z_low, z_high].\n        cdf_high = norm.cdf(z_high)\n        cdf_low = norm.cdf(z_low)\n        pdf_high = norm.pdf(z_high)\n        pdf_low = norm.pdf(z_low)\n        \n        # Handle -inf and +inf boundaries.\n        if np.isneginf(z_low): cdf_low, pdf_low = 0.0, 0.0\n        if np.isposinf(z_high): cdf_high, pdf_high = 1.0, 0.0\n            \n        term_a = a_star * (cdf_high - cdf_low)\n        term_b = b_star * (pdf_low - pdf_high)\n        \n        expected_max_mu_plus += term_a + term_b\n\n    # Compute the knowledge gradient and normalize by cost.\n    kg = expected_max_mu_plus - np.max(mu)\n    \n    return kg / cost\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Case A\n            \"mu\": np.array([0.5, 0.6, 0.4]),\n            \"Sigma\": np.array([\n                [0.16, 0.04, 0.08],\n                [0.04, 0.26, 0.07],\n                [0.08, 0.07, 0.41]\n            ]),\n            \"q\": 2,\n            \"md\": {\"tau2\": 0.2**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 12.0}\n        },\n        {\n            # Case B\n            \"mu\": np.array([0.7, 0.65, 0.66]),\n            \"Sigma\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.2, 0.0],\n                [0.0, 0.0, 0.1]\n            ]),\n            \"q\": 1,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 10.0}\n        },\n        {\n            # Case C\n            \"mu\": np.array([1.0, 0.9, 0.95, 0.8]),\n            \"Sigma\": np.array([\n                [0.36, -0.12, 0.06, 0.00],\n                [-0.12, 0.53, -0.23, -0.14],\n                [0.06, -0.23, 0.35, 0.11],\n                [0.00, -0.14, 0.11, 0.21]\n            ]),\n            \"q\": 3,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.1**2, \"cost\": 3.0}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case[\"mu\"]\n        Sigma = case[\"Sigma\"]\n        q = case[\"q\"]\n\n        # Calculate KG per unit cost for MD\n        val_md = calculate_kg_per_cost(mu, Sigma, q, case[\"md\"][\"tau2\"], case[\"md\"][\"cost\"])\n        \n        # Calculate KG per unit cost for DFT\n        val_dft = calculate_kg_per_cost(mu, Sigma, q, case[\"dft\"][\"tau2\"], case[\"dft\"][\"cost\"])\n\n        # Decide which source to use based on the specified tolerance and tie-breaking rule\n        # If val_md >= val_dft - 1e-12, choose MD.\n        if val_dft - val_md = 1e-12:\n            choice = 0  # MD\n        else:\n            choice = 1  # DFT\n        \n        results.append(f\"{val_md:.6f}\")\n        results.append(f\"{val_dft:.6f}\")\n        results.append(str(choice))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}