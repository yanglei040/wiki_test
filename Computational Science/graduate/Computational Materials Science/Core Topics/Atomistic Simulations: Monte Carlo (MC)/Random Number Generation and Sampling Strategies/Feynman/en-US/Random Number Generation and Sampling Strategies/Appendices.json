{
    "hands_on_practices": [
        {
            "introduction": "In computational materials science, simulating systems at a constant temperature often involves methods like Langevin dynamics. This approach models the interaction of atoms with a surrounding heat bath through two complementary forces: a dissipative frictional drag and a fluctuating stochastic force. This exercise () bridges the crucial gap between the abstract, continuous-time theory of thermal noise and its concrete, discrete-time implementation in a molecular dynamics code. By starting from the formal definition of Gaussian white noise, you will derive the precise statistical properties—specifically the variance—that the random force must have in a numerical integrator with a finite timestep $\\Delta t$, a foundational step for correctly thermostatting any simulation.",
            "id": "3484301",
            "problem": "In Langevin molecular dynamics for a single Cartesian degree of freedom of an atom of mass $m$, thermostatting at temperature $T$ with linear friction coefficient $\\gamma$ is modeled by the fluctuation-dissipation theorem. Let the stochastic force $\\eta(t)$ be a stationary zero-mean Gaussian white noise satisfying\n$$\n\\langle \\eta(t) \\rangle = 0, \\qquad \\langle \\eta(t)\\,\\eta(t') \\rangle = 2\\,\\gamma\\,k_{B}\\,T\\,\\delta(t-t'),\n$$\nwhere $k_{B}$ is the Boltzmann constant and $\\delta(\\cdot)$ is the Dirac delta distribution. Consider a time-discrete integrator that, over each time step of width $\\Delta t>0$ from $t_{n}$ to $t_{n+1}=t_{n}+\\Delta t$, approximates the stochastic force by a constant value equal to the time average of $\\eta(t)$ over that interval,\n$$\n\\eta_{n} \\equiv \\frac{1}{\\Delta t}\\int_{t_{n}}^{t_{n+1}} \\eta(t)\\,dt.\n$$\nWorking from the definitions above and standard properties of Gaussian processes and the Dirac delta distribution, do the following:\n\n- Derive the mean and covariance of the discrete-time sequence $\\{\\eta_{n}\\}$ under this averaging rule, and justify whether $\\eta_{n}$ is Gaussian.\n- Show that each $\\eta_{n}$ can be sampled in practice by drawing a standard normal variate and rescaling by a deterministic standard deviation $\\sigma$ that depends on $\\gamma$, $k_{B}$, $T$, and $\\Delta t$.\n- Provide the closed-form symbolic expression for this standard deviation $\\sigma$.\n\nAssume ideal pseudorandom numbers with no correlations beyond those implied by the model. Express your final answer as a single closed-form analytic expression for $\\sigma$ in terms of $\\gamma$, $k_{B}$, $T$, and $\\Delta t$. Do not include units in your final answer. If any approximations are made, state and justify them in your derivation; the final answer must be exact in the white-noise model stated above.",
            "solution": "The problem is well-posed and scientifically grounded in the principles of statistical mechanics, specifically the theory of Brownian motion and Langevin dynamics. All provided definitions and conditions are standard and self-consistent. We may proceed with the derivation.\n\nThe objective is to determine the statistical properties of the discrete-time stochastic force $\\eta_n$, which is defined as the time-average of the continuous-time stochastic force $\\eta(t)$ over a time interval of width $\\Delta t$. The continuous-time force $\\eta(t)$ is a stationary Gaussian white noise process with the following properties:\n$$\n\\langle \\eta(t) \\rangle = 0\n$$\n$$\n\\langle \\eta(t)\\,\\eta(t') \\rangle = 2\\,\\gamma\\,k_{B}\\,T\\,\\delta(t-t')\n$$\nThe discrete-time force $\\eta_n$ for the interval $[t_n, t_{n+1}]$ where $t_{n+1} = t_n + \\Delta t$ is defined as:\n$$\n\\eta_{n} \\equiv \\frac{1}{\\Delta t}\\int_{t_{n}}^{t_{n+1}} \\eta(t)\\,dt\n$$\n\nFirst, we derive the mean of the random variable $\\eta_n$. By linearity of the expectation operator and the integral, we can write:\n$$\n\\langle \\eta_n \\rangle = \\left\\langle \\frac{1}{\\Delta t}\\int_{t_{n}}^{t_{n+1}} \\eta(t)\\,dt \\right\\rangle = \\frac{1}{\\Delta t}\\int_{t_{n}}^{t_{n+1}} \\langle \\eta(t) \\rangle\\,dt\n$$\nUsing the given property that $\\langle \\eta(t) \\rangle = 0$ for all $t$, the integral evaluates to zero:\n$$\n\\langle \\eta_n \\rangle = \\frac{1}{\\Delta t}\\int_{t_{n}}^{t_{n+1}} 0\\,dt = 0\n$$\nThus, the sequence of discrete-time forces $\\{\\eta_n\\}$ is a zero-mean process.\n\nNext, we establish that $\\eta_n$ is a Gaussian random variable. The continuous-time process $\\eta(t)$ is stipulated to be a Gaussian process. The discrete-time variable $\\eta_n$ is obtained by a linear operation (integration) on $\\eta(t)$. A fundamental property of Gaussian processes is that any linear functional of the process yields a Gaussian-distributed random variable. Therefore, each $\\eta_n$ in the sequence is a Gaussian random variable.\n\nNow, we derive the covariance of the sequence, $\\langle \\eta_n \\eta_m \\rangle$. Using the definition of $\\eta_n$:\n$$\n\\langle \\eta_n \\eta_m \\rangle = \\left\\langle \\left( \\frac{1}{\\Delta t}\\int_{t_{n}}^{t_{n+1}} \\eta(t)\\,dt \\right) \\left( \\frac{1}{\\Delta t}\\int_{t_{m}}^{t_{m+1}} \\eta(t')\\,dt' \\right) \\right\\rangle\n$$\nWe can rearrange the terms and interchange the expectation and integration operators:\n$$\n\\langle \\eta_n \\eta_m \\rangle = \\frac{1}{(\\Delta t)^2} \\int_{t_{n}}^{t_{n+1}} dt \\int_{t_{m}}^{t_{m+1}} dt' \\langle \\eta(t)\\,\\eta(t') \\rangle\n$$\nSubstituting the given autocorrelation for $\\eta(t)$:\n$$\n\\langle \\eta_n \\eta_m \\rangle = \\frac{1}{(\\Delta t)^2} \\int_{t_{n}}^{t_{n+1}} dt \\int_{t_{m}}^{t_{m+1}} dt' \\, (2\\,\\gamma\\,k_{B}\\,T\\,\\delta(t-t'))\n$$\nThe constant prefactor can be moved outside the integrals:\n$$\n\\langle \\eta_n \\eta_m \\rangle = \\frac{2\\,\\gamma\\,k_{B}\\,T}{(\\Delta t)^2} \\int_{t_{n}}^{t_{n+1}} dt \\int_{t_{m}}^{t_{m+1}} dt' \\,\\delta(t-t')\n$$\nWe evaluate the inner integral with respect to $t'$ first. The sifting property of the Dirac delta distribution states that $\\int f(x)\\delta(x-a)dx = f(a)$. Here, the integral of $\\delta(t-t')$ over the interval $[t_m, t_{m+1}]$ yields $1$ if $t$ is within this interval, and $0$ otherwise. So,\n$$\n\\int_{t_{m}}^{t_{m+1}} \\delta(t-t')\\,dt' = \n\\begin{cases}\n1 & \\text{if } t \\in [t_m, t_{m+1}] \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThe expression for the covariance becomes:\n$$\n\\langle \\eta_n \\eta_m \\rangle = \\frac{2\\,\\gamma\\,k_{B}\\,T}{(\\Delta t)^2} \\int_{t_{n}}^{t_{n+1}} dt \\, \\left( \\int_{t_{m}}^{t_{m+1}} \\delta(t-t')\\,dt' \\right)\n$$\nWe must now consider two cases for the remaining integral over $t$.\nCase 1: $n \\neq m$. The integration intervals $[t_n, t_{n+1}]$ and $[t_m, t_{m+1}]$ are disjoint (they only touch at a boundary point, which has measure zero). Therefore, for any $t$ in $[t_n, t_{n+1}]$, $t$ is not in $[t_m, t_{m+1}]$. The inner integral is zero for all $t$ in the domain of the outer integral. Thus:\n$$\n\\langle \\eta_n \\eta_m \\rangle = 0 \\quad \\text{for } n \\neq m\n$$\nCase 2: $n = m$. The integration intervals are identical, i.e., $[t_n, t_{n+1}]$. For any $t$ in this interval, the result of the inner integral is $1$. The expression becomes:\n$$\n\\langle \\eta_n^2 \\rangle = \\frac{2\\,\\gamma\\,k_{B}\\,T}{(\\Delta t)^2} \\int_{t_{n}}^{t_{n+1}} 1\\,dt = \\frac{2\\,\\gamma\\,k_{B}\\,T}{(\\Delta t)^2} (t_{n+1} - t_n)\n$$\nSince $t_{n+1} - t_n = \\Delta t$, we have:\n$$\n\\langle \\eta_n^2 \\rangle = \\frac{2\\,\\gamma\\,k_{B}\\,T}{(\\Delta t)^2} \\Delta t = \\frac{2\\,\\gamma\\,k_{B}\\,T}{\\Delta t}\n$$\nCombining both cases using the Kronecker delta $\\delta_{nm}$, the covariance of the sequence is:\n$$\n\\langle \\eta_n \\eta_m \\rangle = \\frac{2\\,\\gamma\\,k_{B}\\,T}{\\Delta t} \\delta_{nm}\n$$\nThis shows that the discrete-time forces are uncorrelated between different time steps. Since they are also Gaussian, they form a sequence of independent and identically distributed (i.i.d.) random variables.\n\nThe variance of $\\eta_n$ is given by $\\text{Var}(\\eta_n) = \\langle \\eta_n^2 \\rangle - \\langle \\eta_n \\rangle^2$. Since $\\langle \\eta_n \\rangle = 0$, the variance is simply:\n$$\n\\text{Var}(\\eta_n) = \\langle \\eta_n^2 \\rangle = \\frac{2\\,\\gamma\\,k_{B}\\,T}{\\Delta t}\n$$\nA Gaussian random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, denoted $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, can be generated from a standard normal variate $Z \\sim \\mathcal{N}(0, 1)$ using the transformation $X = \\mu + \\sigma Z$.\nIn our case, $\\eta_n$ is a Gaussian random variable with mean $\\mu = 0$ and variance given above. Therefore, $\\eta_n$ can be sampled by drawing a standard normal variate $Z_n$ and rescaling it by the standard deviation $\\sigma = \\sqrt{\\text{Var}(\\eta_n)}$.\n$$\n\\eta_n = \\sigma Z_n, \\quad \\text{where } Z_n \\sim \\mathcal{N}(0, 1)\n$$\nThe required deterministic standard deviation $\\sigma$ is therefore:\n$$\n\\sigma = \\sqrt{\\frac{2\\,\\gamma\\,k_{B}\\,T}{\\Delta t}}\n$$\nThis expression depends only on the physical parameters $\\gamma$, $k_B$, $T$, and the integrator time step $\\Delta t$, as requested. No approximations were made beyond the initial model of white noise.",
            "answer": "$$\n\\boxed{\\sqrt{\\frac{2 \\gamma k_{B} T}{\\Delta t}}}\n$$"
        },
        {
            "introduction": "Having established the statistical requirements for the stochastic force in a Langevin thermostat, the next practical challenge is to generate random numbers that follow this distribution. While computers typically provide generators for the uniform distribution, physical models often demand more complex distributions, such as the Gaussian (or normal) distribution. This practice () focuses on this transformation, guiding you through the implementation of the classic Box-Muller transform to convert uniform variates into the needed standard normal variates. Furthermore, you will analyze its computational cost and compare it to a model of more advanced rejection-sampling methods, providing valuable insight into the algorithmic trade-offs that are critical in high-throughput simulations.",
            "id": "3484341",
            "problem": "You are tasked with implementing a transformation-based Gaussian random number generator and performing an analytic comparison of computational costs for two different strategies commonly discussed in high-throughput simulations in computational materials science. Specifically, you will implement a generator based on the Box–Muller transform and compare its operation counts to those of a Ziggurat-style acceptance-rejection scheme under a parametric model. All questions must be answered purely in mathematical and algorithmic terms, and all results must be produced by a single, complete program.\n\nStarting from the fundamental base of the transformation method for random variate generation and basic probability definitions, you must:\n\n1. Implement a generator that maps two independent uniforms on the open interval $(0,1)$, denoted $U_1$ and $U_2$, to two independent standard normal random variables $Z_1$ and $Z_2$. Your generator must produce $n$ independent samples from the standard normal distribution $\\mathcal{N}(0,1)$ using i.i.d. uniforms and the transformation method. Use a pseudorandom number generator with a provided integer seed to ensure determinism. When computing statistics, use the unbiased sample variance with divisor $n-1$. There are no physical units.\n\n2. From first principles of expected values and independence, and without relying on any timing measurements, derive expressions for the expected per-sample operation counts for the following categories of operations: uniform draws, logarithms, square roots, and trigonometric evaluations. Treat the Box–Muller transform as using two uniforms to produce two normals, along with one logarithm, one square root, and two trigonometric evaluations per application of the transform. For the Ziggurat scheme, assume a parametric model in which each “attempt” to generate a sample succeeds with probability $p \\in (0,1)$ independently, and that:\n   - Each attempt uses one uniform draw.\n   - Conditional on rejection (which occurs with probability $1-p$), a tail path is taken that incurs $c_{\\log}$ logarithms, $c_{\\sqrt}$ square roots, and $c_{\\mathrm{trig}}$ trigonometric evaluations. You may assume no transcendental operations are used on accepted attempts outside of this tail path.\n   Using only the above definitions and linearity of expectation, compute the expected total counts of each operation to generate exactly $M$ samples for both strategies.\n\n3. For each test case, your program must produce deterministic numerical outputs as specified below. All answers must be dimensionless. If rounding is required, apply it as stated for each test case.\n\nTest suite:\n- Test case A (happy path): Generate $n = 100000$ samples with seed $2025$ using the Box–Muller transform. Output the sample mean $\\bar{Z}$ and the unbiased sample variance $s^2$ each rounded to $6$ decimal places.\n- Test case B (boundary condition): Generate $n = 2$ samples with seed $1$ using the same generator. Output the two generated values, each rounded to $6$ decimal places, in the order they are produced by your generator.\n- Test case C (tradeoff, high acceptance): For $M = 1000000$, $p = 0.9973$, $c_{\\log} = 1.0$, $c_{\\sqrt} = 1.0$, $c_{\\mathrm{trig}} = 0.0$, compute the expected total operation counts for both strategies. For the Box–Muller transform, report counts as $[N_{\\mathrm{uni}}, N_{\\log}, N_{\\sqrt}, N_{\\mathrm{trig}}]$. For the Ziggurat model, report counts in the same order. Concatenate these into a single list of eight values in the order Box–Muller then Ziggurat. Round each count to $1$ decimal place.\n- Test case D (edge acceptance): For $M = 1000000$, $p = 0.9$, $c_{\\log} = 1.0$, $c_{\\sqrt} = 1.0$, $c_{\\mathrm{trig}} = 0.0$, repeat the same computation and output as in Test case C, again rounding each count to $1$ decimal place.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the four test case outputs in order. The elements should be:\n- For Test case A: a two-element list $[\\bar{Z}, s^2]$ (each rounded to $6$ decimals).\n- For Test case B: a two-element list with the two generated values (each rounded to $6$ decimals).\n- For Test case C: an eight-element list of rounded counts as specified.\n- For Test case D: an eight-element list of rounded counts as specified.\nFor example, the shape should be $[[\\cdot,\\cdot],[\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$. The output must be a single line and contain only this list representation.",
            "solution": "The problem statement is assessed to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information for a unique solution. The task involves implementing the Box–Muller transform for generating standard normal random variates and analytically comparing its computational cost against a parametric model for a Ziggurat-style rejection sampling method. I will proceed with a full solution.\n\nThe solution is divided into two primary sections: the derivation of the sampling algorithm and the analytical cost models, followed by the specific calculations for the provided test cases.\n\n**Part 1: The Box–Muller Transform Generator**\n\nThe objective is to generate random variates from the standard normal distribution, $\\mathcal{N}(0,1)$, which has the probability density function (PDF) $f(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$. The transformation method leverages the fact that if we can express a target random variable as a function of one or more \"base\" random variables (for which we have generators), we can generate samples from the target distribution.\n\nThe Box–Muller transform maps two independent random variables, $U_1$ and $U_2$, drawn from the uniform distribution on the interval $(0,1)$, to two independent standard normal random variables, $Z_1$ and $Z_2$. The transformation is defined by the following equations:\n$$Z_1 = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n$$Z_2 = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\nThis is derived by a change of variables from a two-dimensional Cartesian system $(Z_1, Z_2)$ to a polar coordinate system. The joint PDF of two independent standard normal variables is $f(z_1, z_2) = f(z_1)f(z_2) = \\frac{1}{2\\pi} e^{-(z_1^2+z_2^2)/2}$. In polar coordinates, where $z_1 = r \\cos\\theta$ and $z_2 = r \\sin\\theta$, we have $r^2 = z_1^2 + z_2^2$ and $\\theta = \\arctan(z_2/z_1)$. The joint PDF becomes $g(r, \\theta) = \\frac{1}{2\\pi} e^{-r^2/2} r$, where $r \\in [0, \\infty)$ and $\\theta \\in [0, 2\\pi)$. This PDF is separable into $g(r, \\theta) = \\left(r e^{-r^2/2}\\right) \\left(\\frac{1}{2\\pi}\\right)$, which shows that the radial component $R$ and the angular component $\\Theta$ are independent. The distribution of $\\Theta$ is Uniform$(0, 2\\pi)$, and the distribution of $R$ has PDF $h(r) = r e^{-r^2/2}$ (a Rayleigh distribution).\n\nWe can generate $\\Theta$ from $U_2$ via $\\Theta = 2\\pi U_2$. We can generate $R$ from $U_1$ by setting the cumulative distribution function (CDF) of $R^2$ equal to $U_1$. The CDF of $R^2$ is $P(R^2 \\le x) = P(R \\le \\sqrt{x}) = \\int_0^{\\sqrt{x}} r' e^{-(r')^2/2} dr' = 1 - e^{-x/2}$. Setting $U_1 = 1 - e^{-R^2/2}$ and solving for $R^2$ yields $R^2 = -2 \\ln(1-U_1)$. Since $U_1$ and $1-U_1$ are identically distributed, we can simplify this to $R^2 = -2 \\ln U_1$, which gives $R = \\sqrt{-2 \\ln U_1}$. Substituting these expressions for $R$ and $\\Theta$ back into the Cartesian conversion formulas $Z_1 = R\\cos\\Theta$ and $Z_2 = R\\sin\\Theta$ yields the Box–Muller equations.\n\nTo generate $n$ samples, we perform $\\lceil n/2 \\rceil$ applications of this transform. Each application consumes two uniform variates and produces two normal variates. If $n$ is odd, the final transform will produce one more sample than needed, which is then discarded.\n\n**Part 2: Analytical Operation Count Comparison**\n\nWe will derive the expected total number of operations for generating $M$ samples using two strategies. The operations of interest are uniform random number draws ($N_{\\mathrm{uni}}$), logarithms ($N_{\\log}$), square roots ($N_{\\sqrt}$), and trigonometric function evaluations ($N_{\\mathrm{trig}}$).\n\n**Strategy 1: Box–Muller Transform**\nThe cost model is explicitly given: one transform produces $2$ normal samples and costs $2$ uniform draws, $1$ logarithm, $1$ square root, and $2$ trigonometric evaluations. To generate exactly $M$ samples, we need to perform $\\lceil M/2 \\rceil$ transforms. For the even value $M=1000000$ in the test cases, this simplifies to $M/2$.\nThe total operation counts are deterministic:\n-   $N_{\\mathrm{uni}} = 2 \\times (M/2) = M$\n-   $N_{\\log} = 1 \\times (M/2) = M/2$\n-   $N_{\\sqrt} = 1 \\times (M/2) = M/2$\n-   $N_{\\mathrm{trig}} = 2 \\times (M/2) = M$\n\n**Strategy 2: Parametric Ziggurat Model**\nThis model describes an acceptance-rejection scheme where the generation of a single sample is a sequence of Bernoulli trials. Each attempt to generate a sample succeeds with probability $p$. Let $K$ be the random variable representing the number of attempts needed to generate one successful sample. $K$ follows a geometric distribution with success probability $p$, so its probability mass function is $P(K=k) = (1-p)^{k-1}p$ for $k=1, 2, \\dots$.\nThe expected number of attempts to generate one sample is $E[K] = 1/p$.\n\nTo generate $M$ samples, we require $M$ independent successful generation processes. By linearity of expectation, the total expected number of attempts is:\n$$E[N_{\\mathrm{attempts}}] = M \\cdot E[K] = \\frac{M}{p}$$\nAccording to the model, each attempt uses one uniform draw. Therefore, the expected total count of uniform draws is:\n$$E[N_{\\mathrm{uni}}] = E[N_{\\mathrm{attempts}}] = \\frac{M}{p}$$\nTranscendental operations (logarithms, square roots, trigonometric functions) are incurred only on rejected attempts. A single attempt is rejected with probability $1-p$. The number of rejections to obtain one successful sample is $K-1$. The expected number of rejections per sample is $E[K-1] = E[K]-1 = (1/p) - 1 = (1-p)/p$.\nThe total expected number of rejections to generate $M$ samples is $M \\cdot (1-p)/p$.\nGiven the per-rejection costs $c_{\\log}$, $c_{\\sqrt}$, and $c_{\\mathrm{trig}}$, the total expected counts for these operations are:\n-   $E[N_{\\log}] = M \\frac{1-p}{p} c_{\\log}$\n-   $E[N_{\\sqrt}] = M \\frac{1-p}{p} c_{\\sqrt}$\n-   $E[N_{\\mathrm{trig}}] = M \\frac{1-p}{p} c_{\\mathrm{trig}}$\n\n**Part 3: Numerical Evaluation for Test Cases**\n\n**Test Case A & B**: These require direct implementation of the Box–Muller generator and calculation of sample statistics. The implementation will follow the equations in Part 1. For Test Case A, $n=100000$ samples are generated with seed $2025$. The sample mean $\\bar{Z} = \\frac{1}{n}\\sum_{i=1}^n Z_i$ and the unbiased sample variance $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (Z_i-\\bar{Z})^2$ are computed. For Test Case B, $n=2$ samples are generated with seed $1$.\n\n**Test Case C**: $M = 1000000$, $p = 0.9973$, $c_{\\log} = 1.0$, $c_{\\sqrt} = 1.0$, $c_{\\mathrm{trig}} = 0.0$.\n-   **Box–Muller Counts**:\n    -   $N_{\\mathrm{uni}} = 1000000$\n    -   $N_{\\log} = 1000000 / 2 = 500000$\n    -   $N_{\\sqrt} = 1000000 / 2 = 500000$\n    -   $N_{\\mathrm{trig}} = 1000000$\n-   **Ziggurat Expected Counts**:\n    -   $E[N_{\\mathrm{uni}}] = 1000000 / 0.9973 \\approx 1002707.31$\n    -   $E[N_{\\log}] = 1000000 \\times \\frac{1-0.9973}{0.9973} \\times 1.0 \\approx 2707.31$\n    -   $E[N_{\\sqrt}] = 1000000 \\times \\frac{1-0.9973}{0.9973} \\times 1.0 \\approx 2707.31$\n    -   $E[N_{\\mathrm{trig}}] = 1000000 \\times \\frac{1-0.9973}{0.9973} \\times 0.0 = 0.0$\n-   After rounding to $1$ decimal place, the final list is $[1000000.0, 500000.0, 500000.0, 1000000.0, 1002707.3, 2707.3, 2707.3, 0.0]$.\n\n**Test Case D**: $M = 1000000$, $p = 0.9$, $c_{\\log} = 1.0$, $c_{\\sqrt} = 1.0$, $c_{\\mathrm{trig}} = 0.0$.\n-   **Box–Muller Counts**: Unchanged from Test Case C.\n    -   $N_{\\mathrm{uni}} = 1000000$\n    -   $N_{\\log} = 500000$\n    -   $N_{\\sqrt} = 500000$\n    -   $N_{\\mathrm{trig}} = 1000000$\n-   **Ziggurat Expected Counts**:\n    -   $E[N_{\\mathrm{uni}}] = 1000000 / 0.9 \\approx 1111111.11$\n    -   $E[N_{\\log}] = 1000000 \\times \\frac{1-0.9}{0.9} \\times 1.0 \\approx 111111.11$\n    -   $E[N_{\\sqrt}] = 1000000 \\times \\frac{1-0.9}{0.9} \\times 1.0 \\approx 111111.11$\n    -   $E[N_{\\mathrm{trig}}] = 1000000 \\times \\frac{1-0.9}{0.9} \\times 0.0 = 0.0$\n-   After rounding to $1$ decimal place, the final list is $[1000000.0, 500000.0, 500000.0, 1000000.0, 1111111.1, 111111.1, 111111.1, 0.0]$.\nThis completes the analytical solution. The implementation will produce the final numerical results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef box_muller_generator(n, seed):\n    \"\"\"\n    Generates n samples from the standard normal distribution using the Box-Muller transform.\n\n    Args:\n        n (int): The number of samples to generate.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        np.ndarray: An array of n standard normal random variates.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # We generate pairs of normals, so we need ceil(n/2) pairs.\n    num_pairs = (n + 1) // 2\n    \n    # Generate all uniform random numbers at once for efficiency.\n    # U1 and U2 must be in (0, 1). rng.random gives [0, 1). The probability of\n    # getting 0 is negligible and can be ignored for this problem's scope.\n    u1 = rng.random(size=num_pairs)\n    u2 = rng.random(size=num_pairs)\n    \n    R = np.sqrt(-2.0 * np.log(u1))\n    theta = 2.0 * np.pi * u2\n    \n    z1 = R * np.cos(theta)\n    z2 = R * np.sin(theta)\n    \n    # Interleave z1 and z2 to form the full sample array\n    samples = np.empty(num_pairs * 2, dtype=np.float64)\n    samples[0::2] = z1\n    samples[1::2] = z2\n    \n    # If n is odd, we will have generated n+1 samples, so truncate.\n    return samples[:n]\n\ndef calculate_operation_counts(M, p, c_log, c_sqrt, c_trig):\n    \"\"\"\n    Calculates expected operation counts for Box-Muller and Ziggurat model.\n\n    Args:\n        M (int): Number of samples to generate.\n        p (float): Success probability for Ziggurat model.\n        c_log (float): Logarithm cost per rejection in Ziggurat.\n        c_sqrt (float): Square root cost per rejection in Ziggurat.\n        c_trig (float): Trig cost per rejection in Ziggurat.\n    \n    Returns:\n        list: A list of 8 floats representing the operation counts for\n              [BM_uni, BM_log, BM_sqrt, BM_trig, ZIG_uni, ZIG_log, ZIG_sqrt, ZIG_trig]\n    \"\"\"\n    # Box-Muller counts\n    # M is even, so ceil(M/2) is M/2.\n    bm_uni = float(M)\n    bm_log = float(M / 2)\n    bm_sqrt = float(M / 2)\n    bm_trig = float(M)\n    \n    # Ziggurat expected counts\n    exp_rej_factor = M * (1.0 - p) / p\n    \n    zig_uni = M / p\n    zig_log = exp_rej_factor * c_log\n    zig_sqrt = exp_rej_factor * c_sqrt\n    zig_trig = exp_rej_factor * c_trig\n    \n    counts = [\n        bm_uni, bm_log, bm_sqrt, bm_trig,\n        zig_uni, zig_log, zig_sqrt, zig_trig\n    ]\n    \n    return [round(c, 1) for c in counts]\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases and print the final result.\n    \"\"\"\n    # --- Test Case A ---\n    n_A = 100000\n    seed_A = 2025\n    samples_A = box_muller_generator(n_A, seed_A)\n    mean_A = round(np.mean(samples_A), 6)\n    var_A = round(np.var(samples_A, ddof=1), 6)\n    result_A = [mean_A, var_A]\n\n    # --- Test Case B ---\n    n_B = 2\n    seed_B = 1\n    samples_B = box_muller_generator(n_B, seed_B)\n    result_B = [round(s, 6) for s in samples_B]\n    \n    # --- Test Case C ---\n    M_C = 1000000\n    p_C = 0.9973\n    clog_C, csqrt_C, ctri_C = 1.0, 1.0, 0.0\n    result_C = calculate_operation_counts(M_C, p_C, clog_C, csqrt_C, ctri_C)\n\n    # --- Test Case D ---\n    M_D = 1000000\n    p_D = 0.9\n    clog_D, csqrt_D, ctri_D = 1.0, 1.0, 0.0\n    result_D = calculate_operation_counts(M_D, p_D, clog_D, csqrt_D, ctri_D)\n\n    # Aggregate all results\n    final_results = [result_A, result_B, result_C, result_D]\n\n    # Format the final output string exactly as required, without spaces.\n    # Using str() and replace() is a reliable way to get the format.\n    output_str = str(final_results).replace(' ', '')\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Modern large-scale simulations are almost invariably run in parallel, distributing the computational work across many processors. This paradigm introduces a subtle but critical challenge for stochastic algorithms: ensuring the random number streams used in different parallel tasks are statistically independent. This hands-on exercise () tackles this advanced topic directly, showing you how to construct independent streams for a domain-decomposed simulation using a modern counter-based generator and a 'skip-ahead' strategy. You will then learn to perform a vital quality control check by empirically measuring the cross-correlation between streams, gaining a practical skill for diagnosing and preventing one of the most pernicious sources of error in parallel scientific computing.",
            "id": "3484374",
            "problem": "Consider independent random force generation for domain-decomposed Molecular Dynamics (MD) where the computational domain is split into $P$ non-overlapping subdomains, each advanced by a separate task. Suppose the stochastic thermostat requires a discrete-time, zero-mean, unit-variance white-noise force sequence $\\{\\eta_i(t)\\}_{t=0}^{N-1}$ per subdomain $i \\in \\{0,1,\\dots,P-1\\}$. Independence across subdomains is critical to avoid spurious correlations.\n\nYou are tasked with designing a procedure to construct $P$ independent random streams using skip-ahead on a counter-based Pseudorandom Number Generator (PRNG), and quantifying inter-stream correlation by computing cross-covariances $\\langle \\eta_i(t)\\eta_j(t+\\Delta t)\\rangle$ for $i \\neq j$. The construction and quantification must be implemented as a complete, runnable program.\n\nFundamental base:\n- A counter-based PRNG outputs a value that is a deterministic function of an integer counter and a fixed seed. Independence between separate streams is achieved by ensuring counters from different streams do not overlap.\n- For ideal white noise, for all $i \\neq j$ and all integer lags $\\Delta t \\geq 0$, $\\mathbb{E}[\\eta_i(t)] = 0$, $\\mathrm{Var}[\\eta_i(t)]=1$, and $\\mathbb{E}[\\eta_i(t)\\eta_j(t+\\Delta t)] = 0$. Empirically, cross-covariance is estimated from finite samples.\n- The sample mean of a sequence $x(t)$ is $\\bar{x} = \\frac{1}{N} \\sum_{t=0}^{N-1} x(t)$. The sample cross-covariance at lag $\\Delta t$ between sequences $x(t)$ and $y(t)$ is defined by\n$$\n\\hat{C}_{xy}(\\Delta t) = \\frac{1}{N-\\Delta t} \\sum_{t=0}^{N-\\Delta t - 1} \\big(x(t) - \\bar{x}\\big)\\big(y(t+\\Delta t)-\\bar{y}\\big),\n$$\nfor integer $0 \\leq \\Delta t < N$.\n\nRequirements:\n1. Implement a fixed counter-based PRNG using the SplitMix64 mixing function, with a $64$-bit unsigned integer counter $c$ and a $64$-bit unsigned integer seed $s$, defined by\n$$\n\\begin{aligned}\nx &= (c + s) \\bmod 2^{64}, \\\\\nx &= \\left(x \\oplus (x \\gg 30)\\right) \\times 0x\\mathrm{BF58476D1CE4E5B9} \\bmod 2^{64}, \\\\\nx &= \\left(x \\oplus (x \\gg 27)\\right) \\times 0x\\mathrm{94D049BB133111EB} \\bmod 2^{64}, \\\\\nx &= x \\oplus (x \\gg 31),\n\\end{aligned}\n$$\nwhere $\\oplus$ denotes bitwise exclusive-or and $\\gg$ denotes bitwise right shift. Convert the $64$-bit output to a floating-point uniform variate on $[0,1)$ by extracting the high $53$ bits and dividing by $2^{53}$.\n\n2. Use skip-ahead to construct $P$ streams. For stream index $i$, define a base counter offset $C_i = i \\times M$, where $M$ is a positive integer stride. For time index $t$, the counter is $c = C_i + t$. This ensures non-overlapping counters across streams provided $M$ is sufficiently large relative to $N$.\n\n3. Map the uniform variates to standard normal variates to produce $\\eta_i(t)$ using the Box–Muller transform. For each pair of uniforms $u_1,u_2 \\in (0,1)$, compute\n$$\nr = \\sqrt{-2 \\ln(u_1)}, \\quad \\theta = 2\\pi u_2, \\quad z_0 = r \\cos(\\theta), \\quad z_1 = r \\sin(\\theta),\n$$\nand use $\\{z_0,z_1\\}$ as successive samples. The angle must be in radians.\n\n4. For each test case, compute the cross-covariances between all distinct stream pairs $(i,j)$, $i \\neq j$, for each specified lag $\\Delta t$. Use the sample cross-covariance definition with the full-sequence sample means $\\bar{\\eta}_i$ and $\\bar{\\eta}_j$ for each stream.\n\n5. For each test case and each specified $\\Delta t$, report the single float\n$$\nm(\\Delta t) = \\max_{i \\neq j} \\left| \\hat{C}_{\\eta_i,\\eta_j}(\\Delta t) \\right|.\n$$\n\n6. If $P < 2$ (i.e., there are no distinct pairs), define $m(\\Delta t) = 0.0$ for all specified $\\Delta t$.\n\n7. Numerical values are dimensionless. Angles are in radians. No physical units are required.\n\nTest suite:\n- Case A (happy path, non-overlapping streams): $P = 4$, $N = 200000$, $M = 2^{40}$, $s = 1469598103934665603$, lags $\\Delta t \\in \\{0,1,2\\}$.\n- Case B (boundary condition, single stream): $P = 1$, $N = 100000$, $M = 2^{20}$, $s = 1099511628211$, lags $\\Delta t \\in \\{0,1,2\\}$.\n- Case C (edge case, deliberately overlapping streams): $P = 3$, $N = 150000$, $M = 10000$, $s = 16045690984833335023$, lags $\\Delta t \\in \\{0, 10000, 20000\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to one test case and containing the floats $[m(\\Delta t_1), m(\\Delta t_2), \\dots]$ in the order of the specified lags for that case, with no spaces. For example, a valid output format is $[[a_1,a_2,a_3],[b_1,b_2,b_3],[c_1,c_2,c_3]]$.",
            "solution": "The problem requires the design and implementation of a procedure for generating multiple independent streams of random numbers for use in parallel Molecular Dynamics simulations. The core of the task is to use a counter-based pseudorandom number generator (PRNG) with a skip-ahead strategy to ensure stream independence, and then to quantify any residual inter-stream correlation using a sample cross-covariance metric. The solution involves several distinct algorithmic steps: implementing the PRNG, transforming its output into standard normal variates, generating the parallel streams, and finally, computing the statistical measure of their independence.\n\n**1. Counter-Based PRNG and Uniform Variate Generation**\n\nThe foundation of the method is a deterministic, counter-based PRNG. The problem specifies the SplitMix64 algorithm. Its output is a function of a $64$-bit integer state, which is formed by the sum of a $64$-bit seed $s$ and a $64$-bit counter $c$. The mixing function is defined as:\n$$\n\\begin{aligned}\nx &= (c + s) \\pmod{2^{64}} \\\\\nx &= \\left(x \\oplus (x \\gg 30)\\right) \\times C_1 \\pmod{2^{64}} \\\\\nx &= \\left(x \\oplus (x \\gg 27)\\right) \\times C_2 \\pmod{2^{64}} \\\\\nx &= x \\oplus (x \\gg 31)\n\\end{aligned}\n$$\nwhere $C_1 = 0x\\mathrm{BF58476D1CE4E5B9}$ and $C_2 = 0x\\mathrm{94D049BB133111EB}$ are $64$-bit constants. The operations are bitwise right-shift ($\\gg$), bitwise exclusive-or ($\\oplus$), and unsigned $64$-bit multiplication. To simulate unsigned $64$-bit arithmetic in Python, where integers have arbitrary precision, bitwise AND operations with a mask of $2^{64}-1$ are applied after each step that could result in a value exceeding this limit.\n\nThe resulting $64$-bit integer $x$ is then converted to a double-precision floating-point number uniformly distributed in $[0,1)$. This is achieved by taking the $53$ most significant bits of $x$ (the mantissa precision of a standard double) and scaling the result. Specifically, `(x >> 11) / 2**53`. This maps the integer range $[0, 2^{53}-1]$ to the floating-point range $[0, 1 - 2^{-53}]$.\n\n**2. Parallel Stream Generation: Skip-Ahead and Box-Muller Transform**\n\nTo generate $P$ independent streams, a skip-ahead strategy is employed. Each stream $i \\in \\{0, \\dots, P-1\\}$ is assigned a unique, non-overlapping block of the PRNG's counter sequence. This is done by defining a base counter offset $C_i = i \\times M$, where $M$ is a large integer stride. For a given stream $i$ at time step $t$, the random variates are generated using counters starting from $C_i$.\n\nThe uniform variates are transformed into standard normal variates, $\\eta_i(t)$, using the Box-Muller transform. This method takes a pair of independent uniform variates, $u_1, u_2 \\in (0,1)$, and produces a pair of independent standard normal variates, $z_0, z_1$:\n$$\nr = \\sqrt{-2 \\ln(u_1)}, \\quad \\theta = 2\\pi u_2, \\quad z_0 = r \\cos(\\theta), \\quad z_1 = r \\sin(\\theta)\n$$\nA crucial implementation detail is that $\\ln(u_1)$ is undefined if $u_1=0$. Although the probability of generating $u_1=0$ is very low, a robust implementation must handle it. If $u_1=0$, it is replaced by a small positive value, such as $2^{-53}$, which is the smallest non-zero value this generator can produce.\n\nTo generate a full sequence $\\{\\eta_i(t)\\}_{t=0}^{N-1}$ for stream $i$, we repeatedly generate pairs of uniform variates using consecutive counters, starting from $C_i$. The $k$-th pair of normal variates, $(\\eta_i(2k), \\eta_i(2k+1))$, is generated from uniform variates derived from counters $C_i+2k$ and $C_i+2k+1$. This process is repeated $\\lceil N/2 \\rceil$ times to fill the array of $N$ normal variates. If $N$ is odd, the last generated normal variate is discarded.\n\n**3. Inter-Stream Correlation Quantification**\n\nThe independence of the generated streams is assessed by computing the sample cross-covariance. For two streams, $\\eta_i(t)$ and $\\eta_j(t)$, the cross-covariance at a time lag $\\Delta t$ is estimated by:\n$$\n\\hat{C}_{\\eta_i,\\eta_j}(\\Delta t) = \\frac{1}{N-\\Delta t} \\sum_{t=0}^{N-\\Delta t - 1} \\big(\\eta_i(t) - \\bar{\\eta}_i\\big)\\big(\\eta_j(t+\\Delta t)-\\bar{\\eta}_j\\big)\n$$\nwhere $\\bar{\\eta}_i$ and $\\bar{\\eta}_j$ are the sample means of the entire sequences. For truly independent streams, this value should be close to zero for all $i \\neq j$ and all lags $\\Delta t$. This calculation is performed for all distinct ordered pairs of streams $(i,j)$ where $i \\neq j$.\n\nThe final reported metric for each lag $\\Delta t$ is the maximum absolute cross-covariance found among all these pairs:\n$$\nm(\\Delta t) = \\max_{i \\neq j} \\left| \\hat{C}_{\\eta_i,\\eta_j}(\\Delta t) \\right|\n$$\nThis metric provides a single number summarizing the worst-case spurious correlation at a given lag. If $P < 2$, no distinct pairs exist, and $m(\\Delta t)$ is defined as $0.0$.\n\n**4. Analysis of Test Cases**\n\nThe test suite is designed to validate the implementation and demonstrate the principles.\n- **Case A ($M \\gg N$)**: The stride $M$ is very large, ensuring the counter ranges for different streams are widely separated. This represents the correct usage of the skip-ahead method, and we expect the resulting cross-covariances to be very small, on the order of statistical noise ($1/\\sqrt{N}$).\n- **Case B ($P=1$)**: This is a boundary case. With only one stream, the concept of inter-stream correlation is not applicable, and the result is correctly defined as $0.0$.\n- **Case C ($M < N$)**: The stride is smaller than the stream length, causing the counter ranges to overlap. This is a deliberate misconfiguration. Specifically, the uniform sequence for stream $j$ is a time-shifted version of the sequence for stream $i$, i.e., $u_j(t) = u_i(t + (j-i)M)$. Consequently, the normal variate sequences are also shifted, $\\eta_j(t) \\approx \\eta_i(t + (j-i)M)$. This introduced dependency will result in a large cross-covariance, close to $1$, when the lag $\\Delta t$ compensates for the shift, i.e., when $\\Delta t = (j-i)M$. For the lags $\\Delta t = 10000 = M$ and $\\Delta t = 20000 = 2M$, we expect to observe a correlation peak of approximately $1.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# --- PRNG Implementation ---\n\n# 64-bit mask for unsigned integer arithmetic\nMASK64 = (1 << 64) - 1\n\n# Constants for SplitMix64\nC1 = 0xBF58476D1CE4E5B9\nC2 = 0x94D049BB133111EB\n\ndef splitmix64_func(state):\n    \"\"\"The mixing function of the SplitMix64 PRNG.\"\"\"\n    x = state\n    x = (x ^ (x >> 30)) * C1\n    x &= MASK64\n    x = (x ^ (x >> 27)) * C2\n    x &= MASK64\n    x = x ^ (x >> 31)\n    return x & MASK64\n\ndef to_uniform_float(x):\n    \"\"\"Converts a 64-bit integer to a float in [0,1) using the upper 53 bits.\"\"\"\n    return (x >> 11) * (1.0 / (1 << 53))\n\n# --- Stream Generation ---\n\ndef generate_stream(stream_idx, N, M, seed):\n    \"\"\"Generates a stream of N standard normal variates.\"\"\"\n    normals = np.empty(N, dtype=np.float64)\n    base_counter = stream_idx * M\n    \n    num_pairs = (N + 1) // 2\n    \n    for i in range(num_pairs):\n        # Generate two uniform variates from consecutive counters\n        counter1 = base_counter + 2 * i\n        state1 = (counter1 + seed) & MASK64\n        u_raw1 = splitmix64_func(state1)\n        u1 = to_uniform_float(u_raw1)\n        \n        counter2 = base_counter + 2 * i + 1\n        state2 = (counter2 + seed) & MASK64\n        u_raw2 = splitmix64_func(state2)\n        u2 = to_uniform_float(u_raw2)\n        \n        # Box-Muller transform\n        # Handle u1 == 0.0 to avoid log(0)\n        if u1 == 0.0:\n            # Use the smallest representable positive value from this generator\n            u1 = 1.0 / (1 << 53)\n            \n        r = math.sqrt(-2.0 * math.log(u1))\n        theta = 2.0 * math.pi * u2\n        \n        z0 = r * math.cos(theta)\n        z1 = r * math.sin(theta)\n        \n        # Store the generated pair, handling odd N\n        if 2 * i < N:\n            normals[2 * i] = z0\n        if 2 * i + 1 < N:\n            normals[2 * i + 1] = z1\n            \n    return normals\n\n# --- Correlation Calculation ---\n\ndef calculate_cross_covariance(s1, s2, lag):\n    \"\"\"Computes the sample cross-covariance between two streams at a given lag.\"\"\"\n    N = len(s1)\n    \n    if lag >= N:\n        return 0.0\n\n    num_terms = N - lag\n    if num_terms <= 0:\n        return 0.0\n        \n    s1_mean = np.mean(s1)\n    s2_mean = np.mean(s2)\n    \n    s1_demeaned = s1 - s1_mean\n    s2_demeaned = s2 - s2_mean\n    \n    # Sum of products: (s1[t]-mean1) * (s2[t+lag]-mean2) for t from 0 to N-lag-1\n    covariance_sum = np.dot(s1_demeaned[:num_terms], s2_demeaned[lag:N])\n    \n    return covariance_sum / num_terms\n\n# --- Main Solver ---\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    test_cases = [\n        # Case A: happy path, non-overlapping streams\n        {'P': 4, 'N': 200000, 'M': 2**40, 's': 1469598103934665603, 'lags': [0, 1, 2]},\n        # Case B: boundary condition, single stream\n        {'P': 1, 'N': 100000, 'M': 2**20, 's': 1099511628211, 'lags': [0, 1, 2]},\n        # Case C: edge case, deliberately overlapping streams\n        {'P': 3, 'N': 150000, 'M': 10000, 's': 16045690984833335023, 'lags': [0, 10000, 20000]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        P, N, M, s, lags = case['P'], case['N'], case['M'], case['s'], case['lags']\n        \n        case_results = []\n        \n        # Per requirement 6, if P < 2, result is 0.0\n        if P < 2:\n            case_results = [0.0] * len(lags)\n            all_results.append(case_results)\n            continue\n            \n        # Generate all P streams\n        streams = [generate_stream(i, N, M, s) for i in range(P)]\n        \n        for lag in lags:\n            max_abs_cov = 0.0\n            \n            # Iterate over all distinct ordered pairs (i, j)\n            for i in range(P):\n                for j in range(P):\n                    if i == j:\n                        continue\n                    \n                    cov = calculate_cross_covariance(streams[i], streams[j], lag)\n                    max_abs_cov = max(max_abs_cov, abs(cov))\n            \n            case_results.append(max_abs_cov)\n            \n        all_results.append(case_results)\n\n    # Format the final output string as specified\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}