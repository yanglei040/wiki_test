{
    "hands_on_practices": [
        {
            "introduction": "标度分析是检验有限尺寸标度理论和确定临界指数的核心工具。本练习  提供了一个具体的数值任务，旨在通过一个引导性的编程问题来掌握数据坍缩 (data collapse) 这一强大技术。通过最小化“失塌”误差函数，你将亲手验证不同微观哈密顿量在同一普适类中如何共享一个普适标度函数，同时体会非普适度规因子 (non-universal metric factors) 在实现完美数据坍缩中的关键作用。",
            "id": "3495505",
            "problem": "考虑一个线性尺寸为 $L$ 的有限系统的序参量分布，记为 $P_L(m)$，其中 $m$ 是强度序参量（例如，每个自旋的磁化强度）。假设在临界温度 $T_c$ 下，于正则系综中进行平衡抽样，因此 $P_L(m)$ 编码了从玻尔兹曼分布中抽取的微观构象的热力学平均。在 $T_c$ 附近，有限尺寸标度假设断言，存在正的临界指数 $\\beta$ 和 $\\nu$，以及每个微观哈密顿量 $H$ 对应的依赖于模型的非普适度规因子 $a_H$，使得当用标度化变量 $x = m L^{\\beta/\\nu} / a_H$ 表示时，分布族 $\\{P_L(m)\\}$ 服从一种标度形式。特别地，如果使用了正确的指数比 $\\beta/\\nu$ 并恰当地考虑了每个哈密顿量的度规因子 $a_H$，那么重标度后的分布将塌缩到一个普适标度函数上，该函数仅依赖于 $x$ 而不依赖于 $L$ 或 $H$ 的微观细节。\n\n从第一性原理出发，定义 $P_L(m)$ 的热力学平均是一个平衡概率分布，通过对具有磁化强度 $m$ 的构象进行计数并由玻尔兹曼因子 $\\exp(-\\mathcal{H}/k_{\\mathrm{B}}T)$ 加权得到；除上述标度假设外，不应使用任何其他未经检验的假设。目标是通过最小化一个定量的“非塌缩”度量，来数值上验证在 $T_c$ 时的标度塌缩，并估计指数比 $\\beta/\\nu$ 以及两个被认为处于同一普适类中的不同微观哈密顿量之间的相对度规因子。\n\n给定一个序参量分布的合成但科学上一致的构造，该构造编码了一个带有有限尺寸修正的普适标度函数。对于给定的哈密顿量标签 $H \\in \\{A,B\\}$、系统尺寸 $L$ 和真实的指数比 $(\\beta/\\nu)_{\\mathrm{true}}$，定义\n$$\nk_H(L) = \\frac{(1 + b_H/L)\\, L^{(\\beta/\\nu)_{\\mathrm{true}}}}{a_H},\n$$\n其中 $a_H0$ 是非普适度规因子，$b_H$ 控制着一个次领头的解析标度修正项。设普适形状由偶函数\n$$\nf(x) \\propto \\exp\\!\\big(-u x^4 - v x^2\\big),\n$$\n生成，其中 $u$ 和 $v$ 是固定的正参数。在间距为 $\\Delta m$ 的均匀磁化网格 $m \\in [-m_{\\max}, m_{\\max}]$ 上，该合成的离散分布定义为\n$$\nP_L^{(H)}(m) \\propto f\\!\\big(k_H(L)\\, m\\big),\n$$\n然后进行归一化，使得 $\\sum_m P_L^{(H)}(m)\\,\\Delta m = 1$。这种构造在不同的 $H$ 之间强制使用一个共同的普适形状 $f(x)$，同时允许通过 $b_H$ 引入不同的 $a_H$ 和有限尺寸修正。\n\n为了对候选指数比 $\\theta$ 和哈密顿量 $B$ 的候选度规因子 $\\tilde{a}_B$（其中 $\\tilde{a}_A$ 固定为 $1$ 以设定单位）执行标度塌缩，定义标度化变量 $x = m L^{\\theta}/\\tilde{a}_H$ 并通过保持概率守恒的变量变换，在公共 $x$-网格上定义适当变换后的概率密度，\n$$\n\\widetilde{P}^{(H)}_{L,\\theta}(x) = \\frac{\\tilde{a}_H}{L^{\\theta}}\\, P_L^{(H)}\\!\\left(\\frac{\\tilde{a}_H}{L^{\\theta}}\\,x\\right).\n$$\n给定一组在不同尺寸 $L$ 和两种哈密顿量 $H \\in \\{A,B\\}$ 下的分布，将给定参数对 $(\\theta,\\tilde{a}_B)$ 的塌缩误差定义为与系综平均曲线的平均积分平方偏差，\n$$\n\\mathcal{E}(\\theta,\\tilde{a}_B) = \\frac{1}{N_{\\mathrm{sets}}}\\sum_{j=1}^{N_{\\mathrm{sets}}}\\int_{x_{\\min}}^{x_{\\max}} \\left[\\widetilde{P}_j(x) - \\overline{P}(x)\\right]^2\\,dx,\n$$\n其中 $j$ 遍历测试中包含的所有哈密顿量-尺寸对，$\\overline{P}(x)$ 是每个 $x$ 处 $\\widetilde{P}_j(x)$ 的算术平均值，积分域 $[x_{\\min},x_{\\max}]$ 是测试中所有重标度网格共有的最大对称区间。最佳拟合参数是在指定搜索域内使 $\\mathcal{E}(\\theta,\\tilde{a}_B)$ 最小化的参数。\n\n您的程序必须：\n- 使用上述构造在均匀的 $m$-网格上确定性地生成 $P_L^{(H)}(m)$，对其进行归一化，然后执行标度变换，在测试中所有数据集共享的公共均匀 $x$-网格上评估 $\\mathcal{E}(\\theta,\\tilde{a}_B)$。\n- 在试验参数的矩形网格上搜索 $\\mathcal{E}(\\theta,\\tilde{a}_B)$ 的最小值。\n- 对于每个独立的测试用例，输出三个实数：最小化参数 $\\theta^\\star$、最小化参数 $\\tilde{a}_B^\\star$ 以及最小化误差 $\\mathcal{E}^\\star = \\mathcal{E}(\\theta^\\star,\\tilde{a}_B^\\star)$。\n\n在此构造中，物理单位是无量纲的。不使用角度。不使用百分比。\n\n对所有测试共享的普适形状和磁化网格，使用以下固定的数值设置：\n- $u = 0.5$, $v = 0.25$。\n- $m_{\\max} = 1.6$, $\\Delta m = 0.002$。\n\n将用于参数估计的试验搜索域和抽样定义为：\n- $\\theta \\in [0.05, 0.70]$，以步长 $0.02$ 均匀抽样。\n- $\\tilde{a}_B \\in [0.60, 1.60]$，以步长 $0.02$ 均匀抽样。\n- 对于在给定 $(\\theta,\\tilde{a}_B)$ 下用于评估 $\\mathcal{E}$ 的重标度 $x$-网格，选择包含在每个数据集在该 $(\\theta,\\tilde{a}_B)$ 下的重标度域中的最大对称区间 $[x_{\\min},x_{\\max}]$，并进一步裁剪至 $[-x_{\\mathrm{clip}},x_{\\mathrm{clip}}]$（其中 $x_{\\mathrm{clip}}=5.0$），然后以均匀间距 $\\Delta x = 0.01$ 进行抽样。\n\n测试套件。运行以下三个测试；每个测试包含两个哈密顿量 $H\\in\\{A,B\\}$ 和几个尺寸 $L$，其参数 $(\\beta/\\nu)_{\\mathrm{true}}$、$a_H$ 和 $b_H$ 均已指定。在所有测试中，估计时固定 $\\tilde{a}_A \\equiv 1$，因此估计的相对标度为 $\\tilde{a}_B$。\n\n- 测试 1（二维类伊辛指数，中等尺寸和温和修正）：\n  - $(\\beta/\\nu)_{\\mathrm{true}} = 0.125$。\n  - 尺寸 $L \\in \\{16, 32, 64\\}$。\n  - 哈密顿量 A: $a_A = 1.0$, $b_A = 0.7$。\n  - 哈密顿量 B: $a_B = 1.3$, $b_B = -0.5$。\n\n- 测试 2（三维类伊辛指数，中等尺寸和温和修正）：\n  - $(\\beta/\\nu)_{\\mathrm{true}} = 0.518$。\n  - 尺寸 $L \\in \\{14, 20, 28\\}$。\n  - 哈密顿量 A: $a_A = 1.0$, $b_A = 0.5$。\n  - 哈密顿量 B: $a_B = 0.85$, $b_B = -0.6$。\n\n- 测试 3（二维类伊辛指数，小尺寸和较强修正）：\n  - $(\\beta/\\nu)_{\\mathrm{true}} = 0.125$。\n  - 尺寸 $L \\in \\{8, 12\\}$。\n  - 哈密顿量 A: $a_A = 1.0$, $b_A = 1.5$。\n  - 哈密顿量 B: $a_B = 1.5$, $b_B = -1.2$。\n\n您的程序应生成单行输出，其中包含所有测试结果，形式为一个单一的实数平坦列表，按测试排序，每个测试内部按 $[\\theta^\\star,\\tilde{a}_B^\\star,\\mathcal{E}^\\star]$ 排序。例如，如果有三个测试，则要求的格式是单行字符串，表示一个长度为 $9$ 的 Python 列表，即 $[\\theta^\\star_{(1)},\\tilde{a}_B^\\star_{(1)},\\mathcal{E}^\\star_{(1)},\\theta^\\star_{(2)},\\tilde{a}_B^\\star_{(2)},\\mathcal{E}^\\star_{(2)},\\theta^\\star_{(3)},\\tilde{a}_B^\\star_{(3)},\\mathcal{E}^\\star_{(3)}]$。",
            "solution": "该问题要求一个数值程序，通过对一组序参量分布最小化一个“非塌缩”误差函数，来验证有限尺寸标度假设并估计临界参数。解决方案的核心是实现指定的误差函数 $\\mathcal{E}(\\theta, \\tilde{a}_B)$，并在试验参数 $(\\theta, \\tilde{a}_B)$ 的离散网格上找到其最小值。这个过程分为三个主要阶段：数据生成、带误差计算的标度变换以及参数优化。\n\n首先，我们生成合成的序参量分布 $\\{P_L^{(H)}(m)\\}$。每个分布都与一个哈密顿量标签 $H \\in \\{A, B\\}$ 和一个系统尺寸 $L$ 相关联。生成过程遵循所提供的模型：\n$$\nP_L^{(H)}(m) \\propto f\\big(k_H(L)\\, m\\big),\n$$\n其中 $f(x) \\propto \\exp(-u x^4 - v x^2)$ 是普适标度函数，$k_H(L) = \\frac{(1 + b_H/L)\\, L^{(\\beta/\\nu)_{\\mathrm{true}}}}{a_H}$ 是一个标度因子，包含了真实的临界指数比 $(\\beta/\\nu)_{\\mathrm{true}}$、一个非普适度规因子 $a_H$ 和一个由 $b_H$ 控制的领头阶标度修正。\n在数值上，这包括以下步骤：\n1.  在区间 $[-m_{\\max}, m_{\\max}]$ 上定义一个间距为 $\\Delta m$ 的均匀序参量 $m$ 网格。问题指定 $m_{\\max}=1.6$ 和 $\\Delta m=0.002$。普适形状的参数固定为 $u=0.5$ 和 $v=0.25$。\n2.  对于测试用例中指定的每一对 $(H, L)$，我们在离散的 $m$-网格上计算未归一化分布 $f(k_H(L)\\, m)$ 的值。\n3.  然后对分布进行数值归一化。连续归一化条件 $\\int P_L^{(H)}(m)\\, dm = 1$ 由离散求和 $\\sum_m P_L^{(H)}(m)\\, \\Delta m = 1$ 近似。因此，归一化常数通过网格上所有未归一化值之和乘以网格间距 $\\Delta m$ 来计算。\n\n其次，我们实现标度变换和误差评估。这构成了主要的计算循环，该循环在指定的试验参数域上执行网格搜索：$\\theta \\in [0.05, 0.70]$，步长为 $0.02$；以及 $\\tilde{a}_B \\in [0.60, 1.60]$，步长为 $0.02$。对于每个试验对 $(\\theta, \\tilde{a}_B)$，执行以下过程来计算塌缩误差 $\\mathcal{E}(\\theta, \\tilde{a}_B)$：\n1.  构建一个用于积分的公共标度化网格。目标是在标度化变量 $x = m L^{\\theta} / \\tilde{a}_H$ 的公共坐标轴上比较所有分布，其中 $\\tilde{a}_A$ 固定为 $1$，而 $\\tilde{a}_B$ 是试验参数。对于每个数据集 $j$（对应于特定的 $H$ 和 $L$），原始的 $m$-网格 $[-m_{\\max}, m_{\\max}]$ 映射到一个标度化区间 $[-m_{\\max} L^{\\theta}/\\tilde{a}_H, m_{\\max} L^{\\theta}/\\tilde{a}_H]$。用于比较的公共域是包含在所有这些独立标度化区间内的最大对称区间 $[x_{\\min}, x_{\\max}]$。该域由最严格的范围确定，即 $x_{\\max} = \\min_{j}(m_{\\max} L_j^{\\theta}/\\tilde{a}_{H_j})$。然后将此公共区间裁剪至范围 $[-x_{\\mathrm{clip}}, x_{\\mathrm{clip}}]$，其中 $x_{\\mathrm{clip}}=5.0$。最终得到的区间以均匀间距 $\\Delta x = 0.01$ 进行抽样，以创建共享的积分网格。\n2.  每个生成的分布 $P_L^{(H)}(m)$，表示为离散 $m$-网格上的一组值，被变换为这个公共 $x$-网格上的一个函数。概率密度的变换由概率守恒定律给出：\n    $$\n    \\widetilde{P}^{(H)}_{L,\\theta}(x) = \\frac{\\tilde{a}_H}{L^{\\theta}}\\, P_L^{(H)}\\!\\left(\\frac{\\tilde{a}_H}{L^{\\theta}}\\,x\\right).\n    $$\n    为了进行数值评估，对于公共 $x$-网格上的每个点 $x_i$，计算相应的序参量值 $m_i = x_i \\tilde{a}_H / L^{\\theta}$。由于 $m_i$ 通常不会与原始 $m$-网格上的点重合，我们对 $P_L^{(H)}(m)$ 的离散数据使用线性插值来找到其在 $m_i$ 处的值。此过程产生一组重标度分布 $\\{\\widetilde{P}_j(x)\\}$，它们都在同一个最终的 $x$-网格上定义和评估。\n3.  塌缩误差 $\\mathcal{E}$ 是从这组重标度曲线中计算出来的。首先，在 $x$-网格的每个点上计算算术平均曲线 $\\overline{P}(x) = (1/N_{\\mathrm{sets}}) \\sum_j \\widetilde{P}_j(x)$。然后，对于每个分布 $\\widetilde{P}_j(x)$，通过对网格点进行数值求和，将其与平均值的积分平方偏差 $\\int [\\widetilde{P}_j(x) - \\overline{P}(x)]^2\\,dx$ 计算出来，其中积分近似为宽度为 $\\Delta x$ 的黎曼和。最终误差 $\\mathcal{E}(\\theta, \\tilde{a}_B)$ 是测试中包含的所有 $N_{\\mathrm{sets}}$ 个数据集的这些积分平方偏差的平均值。\n\n第三，确定最优参数 $(\\theta^\\star, \\tilde{a}_B^\\star)$。这些参数是使误差函数 $\\mathcal{E}(\\theta, \\tilde{a}_B)$ 最小化的参数。执行一个直接的网格搜索：在指定的二维搜索网格上为每一对 $(\\theta, \\tilde{a}_B)$ 计算误差 $\\mathcal{E}$。产生最小误差的参数对被选为最佳拟合估计 $(\\theta^\\star, \\tilde{a}_B^\\star)$，并且相应的最小化误差被记录为 $\\mathcal{E}^\\star$。\n\n整个过程在一个 Python 程序中实现，该程序利用 `numpy` 库进行高效的数组操作、数值计算和插值。该程序按顺序处理三个测试用例中的每一个，为每个用例执行数据生成和优化例程。所有测试的最终结果 $(\\theta^\\star, \\tilde{a}_B^\\star, \\mathcal{E}^\\star)$ 被收集并格式化为一个单一列表进行输出，如问题所指定。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from the problem statement\nU_PARAM = 0.5\nV_PARAM = 0.25\nM_MAX = 1.6\nDM = 0.002\nTHETA_RANGE = (0.05, 0.70, 0.02)\nAB_RANGE = (0.60, 1.60, 0.02)\nX_CLIP = 5.0\nDX = 0.01\n\ndef universal_f(x):\n    \"\"\"\n    Computes the universal shape function f(x) prop. to exp(-u*x^4 - v*x^2).\n    \"\"\"\n    return np.exp(-U_PARAM * x**4 - V_PARAM * x**2)\n\ndef generate_p_dist(L, beta_nu_true, a_H, b_H):\n    \"\"\"\n    Generates a single normalized synthetic distribution P_L(m).\n    \"\"\"\n    m_grid = np.arange(-M_MAX, M_MAX + DM / 2, DM)\n\n    k_H_L = (1 + b_H / L) * L**beta_nu_true / a_H\n    \n    p_unnormalized = universal_f(k_H_L * m_grid)\n    \n    # Normalize with numerical integration: sum(P_i) * dm = 1\n    norm_factor = np.sum(p_unnormalized) * DM\n    p_normalized = p_unnormalized / norm_factor\n    \n    return m_grid, p_normalized\n\ndef calculate_collapse_error(datasets, theta, aB_try):\n    \"\"\"\n    Calculates the collapse error E(theta, aB_try) for a set of distributions.\n    \"\"\"\n    aA_try = 1.0\n\n    # 1. Determine the common rescaled x-grid\n    x_max_individual_bounds = []\n    for _, _, L, H_label in datasets:\n        aH_try = aA_try if H_label == 'A' else aB_try\n        # The m-range [-M_MAX, M_MAX] maps to x-range [-x_max, x_max]\n        x_max = M_MAX * L**theta / aH_try\n        x_max_individual_bounds.append(x_max)\n\n    # The common symmetric interval is bounded by the minimum of the individual max values.\n    common_x_max_bound = np.min(x_max_individual_bounds)\n    \n    # Clip the range\n    final_x_bound = min(common_x_max_bound, X_CLIP)\n\n    # If the common interval is too small for integration, return a large error\n    if final_x_bound  DX:\n        return np.inf\n    \n    x_integrate_grid = np.arange(-final_x_bound, final_x_bound + DX / 2, DX)\n\n    # 2. Transform each distribution to the common x-grid\n    rescaled_p_curves = []\n    for m_grid, p_dist, L, H_label in datasets:\n        aH_try = aA_try if H_label == 'A' else aB_try\n\n        # For each point x_i on the integration grid, find the corresponding m_i\n        # m = x * aH_try / L**theta\n        m_values_to_interp = x_integrate_grid * aH_try / L**theta\n        \n        # Interpolate P(m) at these new m values\n        p_interp_values = np.interp(m_values_to_interp, m_grid, p_dist)\n        \n        # Apply the probability conservation scaling factor (Jacobian of m w.r.t x)\n        # P_tilde(x) = P(m(x)) * dm/dx\n        prob_scale_factor = aH_try / L**theta\n        p_tilde_curve = prob_scale_factor * p_interp_values\n        rescaled_p_curves.append(p_tilde_curve)\n\n    # 3. Compute the mean integrated square deviation\n    all_rescaled_p = np.array(rescaled_p_curves)\n    if all_rescaled_p.shape[0]  2:\n        return 0.0 # Error is zero if there's only one curve to compare\n\n    p_bar = np.mean(all_rescaled_p, axis=0)\n    deviations = (all_rescaled_p - p_bar)**2\n    \n    # Integrate squared deviations for each curve (sum * dx)\n    integrated_sq_devs = np.sum(deviations, axis=1) * DX\n    \n    # Error is the mean of these integrated deviations\n    error = np.mean(integrated_sq_devs)\n\n    return error\n\ndef solve():\n    test_cases = [\n        {\n            \"beta_nu_true\": 0.125,\n            \"sizes\": [16, 32, 64],\n            \"params\": {'A': {'aH': 1.0, 'bH': 0.7}, 'B': {'aH': 1.3, 'bH': -0.5}}\n        },\n        {\n            \"beta_nu_true\": 0.518,\n            \"sizes\": [14, 20, 28],\n            \"params\": {'A': {'aH': 1.0, 'bH': 0.5}, 'B': {'aH': 0.85, 'bH': -0.6}}\n        },\n        {\n            \"beta_nu_true\": 0.125,\n            \"sizes\": [8, 12],\n            \"params\": {'A': {'aH': 1.0, 'bH': 1.5}, 'B': {'aH': 1.5, 'bH': -1.2}}\n        }\n    ]\n\n    theta_grid = np.arange(THETA_RANGE[0], THETA_RANGE[1] + THETA_RANGE[2]/2, THETA_RANGE[2])\n    aB_grid = np.arange(AB_RANGE[0], AB_RANGE[1] + AB_RANGE[2]/2, AB_RANGE[2])\n\n    all_results = []\n    for case in test_cases:\n        # 1. Generate all required P_L(m) distributions for this test case\n        datasets = []\n        for L in case[\"sizes\"]:\n            for H_label in ['A', 'B']:\n                params = case[\"params\"][H_label]\n                m_grid, p_dist = generate_p_dist(L, case[\"beta_nu_true\"], params['aH'], params['bH'])\n                datasets.append((m_grid, p_dist, L, H_label))\n\n        # 2. Perform grid search for (theta*, aB*) that minimizes the error\n        min_error = np.inf\n        best_params = (np.nan, np.nan, np.nan)\n\n        for theta_try in theta_grid:\n            for aB_try in aB_grid:\n                error = calculate_collapse_error(datasets, theta_try, aB_try)\n                if error  min_error:\n                    min_error = error\n                    best_params = (theta_try, aB_try, error)\n        \n        all_results.extend(best_params)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了序参量的标度分析之后，我们转向另一个关键的物理量：关联长度 $\\xi$。此实践  将指导你从可测量的模拟数据——静态结构因子 $S(\\mathbf{k})$——中推导出 $\\xi$ 的一个估计量。进而，你将利用这个估计量来检验有限尺寸标度理论的另一个核心预测，即在临界点处，比值 $\\xi/L$ 会趋于一个普适常数。这个过程不仅加深了对不同物理量如何通过标度理论相互关联的理解，也展示了如何从一级模拟数据中提取二级物理量。",
            "id": "3495541",
            "problem": "考虑一个二维方格子上周期性的各向同性自旋系统，其线性尺寸为 $L$，晶格间距设为单位1，因此所有长度都是无量纲的。定义自旋场 $s_{\\mathbf{r}}$ 的离散傅里叶变换为 $m(\\mathbf{k})$，静态结构因子 $S(\\mathbf{k})$ 为热力学平均值 $S(\\mathbf{k}) = L^2 \\langle |m(\\mathbf{k})|^2 \\rangle$。假设系统具有平移不变性，且两点关联函数衰减得足够快，使得其二阶矩对于所有有限的 $L$ 都是有限的。使用离散最小波矢 $\\mathbf{k}_{\\min} = (2\\pi/L, 0)$ 和晶格动量 $\\hat{k}^2 = 4 \\sum_{\\alpha=1}^{2} \\sin^2(k_{\\alpha}/2)$ 作为合适的小 $\\mathbf{k}$ 变量。你将在平衡统计力学和正则系综、蒙特卡洛（MC）热力学平均以及有限尺寸标度（FSS）的基本框架内进行研究。\n\n任务1（从第一性原理推导）：从上述定义和关联函数二阶矩有限所蕴含的小 $\\mathbf{k}$ 解析展开出发，推导一个有限尺寸二阶矩关联长度 $\\xi(L,T)$ 的估计量，该估计量仅使用 $S(\\mathbf{0})$、$S(\\mathbf{k}_{\\min})$ 和晶格动量。你的推导必须仅依赖于核心定义以及与各向同性和周期性边界条件一致的 $S(\\mathbf{k})$ 的领头小 $\\mathbf{k}$ 展开，不得引用任何现成的公式。\n\n任务2（算法设计）：使用你推导出的估计量，设计一个算法，在给定多个数据集的 $L$、$S(\\mathbf{0})$ 和 $S(\\mathbf{k}_{\\min})$ 的情况下，计算 $\\xi(L,T)$ 和无量纲比率 $\\xi(L,T)/L$。三角函数必须使用弧度。\n\n任务3（临界点的普适比率检验）：有限尺寸标度理论预测，在连续相变温度 $T_c$ 下，比率 $\\xi(L,T_c)/L$ 会趋近于一个普适常数 $R_\\xi$，该常数仅依赖于普适类和边界条件，而与微观细节无关。使用提供的测试套件，对三个不同模型在它们各自的 $T_c$ 下的 $\\xi(L,T)/L$ 进行数值提取，估计每个模型的尺寸平均值 $\\overline{R_\\xi}$，并量化这些平均值之间的最大绝对偏差以检验普适性。\n\n任务4（非临界标度趋势）：对于一个来自 $T \\neq T_c$ 的非临界数据集（其本征关联长度有限且与 $L$ 无关），使用你的估计量计算几个不同 $L$ 值的 $\\xi(L,T)/L$，并报告 $\\xi(L,T)/L$ 作为 $L$ 的函数的最小二乘斜率，以展示在偏离临界点时随 $L$ 预期的衰减行为。\n\n单位和输出：所有量在晶格单位下都是无量纲的。角度必须是弧度。你的程序必须打印一行，包含一个由方括号括起来的逗号分隔列表，其中按顺序包含以下五个浮点数，并精确到小数点后六位：$[\\overline{R_\\xi}^{(A)}, \\overline{R_\\xi}^{(B)}, \\overline{R_\\xi}^{(C)}, \\Delta_{\\max}, b_{\\text{nc}}]$，其中 $\\overline{R_\\xi}^{(X)}$ 是模型 $X \\in \\{A,B,C\\}$ 在 $T_c$ 下 $\\xi/L$ 的模型平均值，$\\Delta_{\\max}$ 是这三个平均值之间的最大绝对偏差，而 $b_{\\text{nc}}$ 是非临界数据集的 $\\xi/L$ 相对于 $L$ 的最小二乘斜率。\n\n测试套件：使用以下数据集。每个条目是一个元组 $(\\text{模型}, \\text{是否临界}, L, S(\\mathbf{0}), S(\\mathbf{k}_{\\min}))$。字段 $\\text{is\\_critical} \\in \\{\\text{True}, \\text{False}\\}$ 指示数据是否在 $T_c$。\n\n- 临界，模型 $A$：$('A', \\text{True}, 2, 25.0, 5.0)$、$('A', \\text{True}, 4, 63.0, 7.0)$、$('A', \\text{True}, 6, 30.0, 3.0)$。\n- 临界，模型 $B$：$('B', \\text{True}, 2, 30.0, 6.0)$、$('B', \\text{True}, 4, 99.0, 11.0)$、$('B', \\text{True}, 6, 20.0, 2.0)$。\n- 临界，模型 $C$：$('C', \\text{True}, 2, 40.0, 8.0)$、$('C', \\text{True}, 4, 45.0, 5.0)$、$('C', \\text{True}, 6, 90.0, 9.0)$。\n- 非临界，模型 $A$ 在 $T \\neq T_c$：$('A', \\text{False}, 2, 40.0, 4.0)$、$('A', \\text{False}, 4, 22.0, 4.0)$、$('A', \\text{False}, 6, 13.0, 4.0)$。\n\n约束和覆盖范围：数据包括非常小的 $L$（边界条件）和多个模型，用于测试 $\\xi/L$ 在 $T_c$ 的普适性。你的实现必须严格遵循任务1中推导的估计量。你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表（例如 $[r_A,r_B,r_C,\\delta,b]$），其中每个条目都四舍五入到小数点后六位。",
            "solution": "该问题要求推导一个用于有限尺寸系统的二阶矩关联长度 $\\xi(L,T)$ 的估计量，然后将其应用于多个数据集以检验有限尺寸标度原理，即在临界点 $\\xi/L$ 比率的普适性及其在偏离临界点时的行为。\n\n**任务1：关联长度 $\\xi(L,T)$ 估计量的推导**\n\n我们的推导从 $d$ 维各向同性系统中二阶矩关联长度 $\\xi$ 的基本定义开始。它是根据两点空间关联函数 $G(\\mathbf{r}) = \\langle s_{\\mathbf{0}} s_{\\mathbf{r}} \\rangle$ 定义的，其中 $s_{\\mathbf{r}}$ 是位点 $\\mathbf{r}$ 处的自旋。对于离散晶格，定义为：\n$$\n\\xi^2 = \\frac{1}{2d} \\frac{\\sum_{\\mathbf{r}} |\\mathbf{r}|^2 G(\\mathbf{r})}{\\sum_{\\mathbf{r}} G(\\mathbf{r})}\n$$\n其中求和遍及 $L \\times L$ 网格上的所有晶格矢量 $\\mathbf{r}$，维度为 $d=2$。\n\n分母是关联函数在整个空间上的总和，它与零波矢处的静态结构因子 $S(\\mathbf{k})$直接相关。结构因子是关联函数的离散傅里叶变换：\n$$\nS(\\mathbf{k}) = \\sum_{\\mathbf{r}} G(\\mathbf{r}) e^{-i \\mathbf{k} \\cdot \\mathbf{r}}\n$$\n在 $\\mathbf{k}=\\mathbf{0}$ 处求值，我们发现分母就是 $S(\\mathbf{0})$：\n$$\n\\sum_{\\mathbf{r}} G(\\mathbf{r}) = S(\\mathbf{k}=\\mathbf{0}) \\equiv S(\\mathbf{0})\n$$\n分子 $\\sum_{\\mathbf{r}} |\\mathbf{r}|^2 G(\\mathbf{r})$ 可以与 $S(\\mathbf{k})$ 在 $\\mathbf{k}=\\mathbf{0}$ 处的曲率联系起来。我们计算 $S(\\mathbf{k})$ 关于 $\\mathbf{k}$ 的拉普拉斯算子：\n$$\n\\nabla_{\\mathbf{k}}^2 S(\\mathbf{k}) = \\sum_{\\alpha=1}^{d} \\frac{\\partial^2 S(\\mathbf{k})}{\\partial k_{\\alpha}^2} = \\sum_{\\alpha=1}^{d} \\sum_{\\mathbf{r}} G(\\mathbf{r}) (-i r_{\\alpha})^2 e^{-i \\mathbf{k} \\cdot \\mathbf{r}} = - \\sum_{\\mathbf{r}} G(\\mathbf{r}) \\left(\\sum_{\\alpha=1}^{d} r_{\\alpha}^2\\right) e^{-i \\mathbf{k} \\cdot \\mathbf{r}} = - \\sum_{\\mathbf{r}} G(\\mathbf{r}) |\\mathbf{r}|^2 e^{-i \\mathbf{k} \\cdot \\mathbf{r}}\n$$\n在 $\\mathbf{k}=\\mathbf{0}$ 处求值：\n$$\n\\nabla_{\\mathbf{k}}^2 S(\\mathbf{k}) \\Big|_{\\mathbf{k}=\\mathbf{0}} = - \\sum_{\\mathbf{r}} |\\mathbf{r}|^2 G(\\mathbf{r})\n$$\n将分子和分母的这些表达式代回 $\\xi^2$ 的定义，得到傅里叶空间中的一个关系式：\n$$\n\\xi^2 = - \\frac{\\nabla_{\\mathbf{k}}^2 S(\\mathbf{k}) \\Big|_{\\mathbf{k}=\\mathbf{0}}}{2d S(\\mathbf{0})}\n$$\n这个精确关系启发了我们对 $S(\\mathbf{k})$ 进行小 $\\mathbf{k}$ 展开。对于一个各向同性系统，$S(\\mathbf{k})$ 在 $\\mathbf{k}=\\mathbf{0}$ 附近的泰勒展开为：\n$$\nS(\\mathbf{k}) \\approx S(\\mathbf{0}) + \\frac{1}{2} \\sum_{\\alpha,\\beta} k_{\\alpha} k_{\\beta} \\frac{\\partial^2 S}{\\partial k_{\\alpha} \\partial k_{\\beta}}\\Big|_{\\mathbf{0}} \\approx S(\\mathbf{0}) + \\frac{1}{2} k^2 \\frac{\\nabla_{\\mathbf{k}}^2 S}{d}\\Big|_{\\mathbf{0}} = S(\\mathbf{0}) \\left(1 + \\frac{k^2}{2d} \\frac{\\nabla_{\\mathbf{k}}^2 S|_{\\mathbf{0}}}{S(\\mathbf{0})}\\right) = S(\\mathbf{0})(1 - k^2 \\xi^2)\n$$\n一个更稳健的 $S(\\mathbf{k})$ 近似形式，即著名的 Ornstein-Zernike 形式，是一个与泰勒展开领头项一致的 Padé 近似。对于离散晶格，连续波矢模长的平方 $k^2$ 被晶格动量 $\\hat{k}^2$ 所取代，后者是离散晶格拉普拉斯算子的本征值。问题中已给出其形式为 $\\hat{k}^2 = 4 \\sum_{\\alpha=1}^{2} \\sin^2(k_{\\alpha}/2)$。因此，晶格上的 Ornstein-Zernike 形式为：\n$$\nS(\\mathbf{k}) \\approx \\frac{S(\\mathbf{0})}{1 + \\xi^2 \\hat{k}^2}\n$$\n这种形式更优，因为它保证为正，并且通常在更宽的 $\\mathbf{k}$ 范围内成立。通过重新整理这个方程，我们可以推导出 $\\xi^2$ 的一个估计量：\n$$\n1 + \\xi^2 \\hat{k}^2 = \\frac{S(\\mathbf{0})}{S(\\mathbf{k})} \\implies \\xi^2 = \\frac{1}{\\hat{k}^2} \\left( \\frac{S(\\mathbf{0})}{S(\\mathbf{k})} - 1 \\right)\n$$\n为了获得有限尺寸关联长度 $\\xi(L,T)$ 的估计量，我们在周期性晶格上可用的最小非零波矢 $\\mathbf{k}_{\\min}$ 处计算此表达式。问题指定使用 $\\mathbf{k}_{\\min} = (2\\pi/L, 0)$。对于此波矢，晶格动量为：\n$$\n\\hat{k}_{\\min}^2 = 4 \\left( \\sin^2\\left(\\frac{2\\pi/L}{2}\\right) + \\sin^2\\left(\\frac{0}{2}\\right) \\right) = 4 \\sin^2(\\pi/L)\n$$\n将此代入我们关于 $\\xi^2$ 的表达式，得到最终所需的估计量：\n$$\n\\xi^2(L,T) = \\frac{1}{4 \\sin^2(\\pi/L)} \\left( \\frac{S(\\mathbf{0})}{S(\\mathbf{k}_{\\min})} - 1 \\right)\n$$\n\n**任务2：算法设计**\n\n推导出的公式可以直接转化为一个算法。对于一个给定的数据集，包含线性尺寸 $L$、零波矢处的结构因子 $S(\\mathbf{0})$ 以及最小波矢处的结构因子 $S(\\mathbf{k}_{\\min})$，算法如下：\n1.  计算最小波矢的晶格动量：$\\hat{k}_{\\min}^2 = 4 \\sin^2(\\pi/L)$，确保角度以弧度为单位。\n2.  计算结构因子之比 $R_S = S(\\mathbf{0}) / S(\\mathbf{k}_{\\min})$。\n3.  计算关联长度的平方：$\\xi^2 = (R_S - 1) / \\hat{k}_{\\min}^2$。\n4.  取平方根求得关联长度 $\\xi(L,T) = \\sqrt{\\xi^2}$。\n5.  计算无量纲比率 $\\xi(L,T)/L$。\n\n**任务3和4：数据应用**\n\n对于任务3，此算法应用于标记为临界（$T=T_c$）的数据集。对于每个模型（$A、B、C$），计算所有可用尺寸 $L$ 的比率 $\\xi(L,T_c)/L$。然后为每个模型计算这些比率的平均值 $\\overline{R_\\xi}$。通过计算三个特定模型平均值之间的最大绝对差 $\\Delta_{\\max}$ 来检验该比率的普适性。\n\n对于任务4，该算法应用于非临界数据集（$T \\neq T_c$）。这提供了比率 $\\xi(L,T)/L$ 作为 $L$ 的函数的值。问题假设，在远离临界点时，真实关联长度是有限的且很大程度上与 $L$ 无关，此时该比率应随系统尺寸衰减。通过计算 $\\xi(L,T)/L$ 对 $L$ 的最小二乘线性拟合的斜率 $b_{\\text{nc}}$ 来量化这一趋势。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Derives and applies an estimator for the second-moment correlation length \n    to test finite-size scaling predictions.\n    \"\"\"\n\n    def calculate_xi_over_L(L, S0, Skmin):\n        \"\"\"\n        Computes the ratio xi/L based on the derived estimator.\n\n        Args:\n            L (int): Linear size of the lattice.\n            S0 (float): Static structure factor at k=0.\n            Skmin (float): Static structure factor at k_min.\n\n        Returns:\n            float: The dimensionless ratio xi/L.\n        \"\"\"\n        if L = 1:\n            # For L=1, k_min is ill-defined in this context and its denominator would be 0.\n            # The test data is for L>=2.\n            return np.nan\n\n        # Calculate the lattice momentum for k_min = (2*pi/L, 0)\n        # Angles for Python's sin function are in radians as required.\n        k_min_sq = 4 * np.sin(np.pi / L)**2\n        \n        # Ensure physical and numerical validity\n        if k_min_sq == 0 or Skmin == 0:\n            return np.nan\n        \n        ratio_S = S0 / Skmin\n        if ratio_S  1:\n            # An unphysical condition implying a complex correlation length.\n            return np.nan\n            \n        # Apply the derived formula for the squared correlation length\n        xi_sq = (ratio_S - 1) / k_min_sq\n        xi = np.sqrt(xi_sq)\n        \n        return xi / L\n\n    # The test suite provided in the problem statement.\n    # Format: (model, is_critical, L, S(0), S(k_min))\n    test_cases = [\n        ('A', True, 2, 25.0, 5.0), ('A', True, 4, 63.0, 7.0), ('A', True, 6, 30.0, 3.0),\n        ('B', True, 2, 30.0, 6.0), ('B', True, 4, 99.0, 11.0), ('B', True, 6, 20.0, 2.0),\n        ('C', True, 2, 40.0, 8.0), ('C', True, 4, 45.0, 5.0), ('C', True, 6, 90.0, 9.0),\n        ('A', False, 2, 40.0, 4.0), ('A', False, 4, 22.0, 4.0), ('A', False, 6, 13.0, 4.0)\n    ]\n\n    # Data structures to store results for each task\n    critical_ratios = {'A': [], 'B': [], 'C': []}\n    noncritical_data = {'L': [], 'ratio': []}\n\n    # Process all test cases\n    for model, is_critical, L, S0, Skmin in test_cases:\n        ratio = calculate_xi_over_L(L, S0, Skmin)\n        if is_critical:\n            critical_ratios[model].append(ratio)\n        else:\n            # Only model 'A' has non-critical data in this suite\n            noncritical_data['L'].append(L)\n            noncritical_data['ratio'].append(ratio)\n\n    # Task 3: Universal ratio test at criticality\n    r_A_mean = np.mean(critical_ratios['A'])\n    r_B_mean = np.mean(critical_ratios['B'])\n    r_C_mean = np.mean(critical_ratios['C'])\n\n    # Calculate the maximum absolute deviation among the mean ratios\n    delta_max = max(\n        abs(r_A_mean - r_B_mean),\n        abs(r_A_mean - r_C_mean),\n        abs(r_B_mean - r_C_mean)\n    )\n\n    # Task 4: Noncritical scaling trend\n    # Perform a linear regression of xi/L vs L for the noncritical data.\n    # linregress returns: slope, intercept, r-value, p-value, std-err\n    slope, _, _, _, _ = linregress(noncritical_data['L'], noncritical_data['ratio'])\n    b_nc = slope\n\n    # Prepare the final list of results for printing.\n    final_results = [r_A_mean, r_B_mean, r_C_mean, delta_max, b_nc]\n    \n    # Print the final answer in the exact specified format.\n    print(f\"[{','.join(f'{x:.6f}' for x in final_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "高效地生成用于有限尺寸标度分析的数据本身就是一个挑战，其核心障碍是“临界慢化” (critical slowing down) 现象。本练习  将我们的关注点从数据分析转向数据生成。通过从第一性原理出发，量化对比两种常用蒙特卡洛算法——局域 Metropolis 算法和团簇 (cluster) 算法——的计算效率，你将深刻理解动力学临界指数 $z$ 如何决定计算成本，以及为何先进的算法对于精确研究临界现象至关重要。",
            "id": "3495526",
            "problem": "在线性尺寸为 $L$ 的方格格子上，一个二维类伊辛模型在临界温度下通过两种不同的马尔可夫链蒙特卡洛（MC）算法进行模拟：一种是局域Metropolis算法（记为 $\\mathrm{M}$），另一种是Wolff类型的单团簇算法（记为 $\\mathrm{C}$）。我们关注的物理可观测量是零场磁化率 $\\chi(L)$，它是根据磁化强度平方的MC时间序列估算得出的。请将以下经过充分检验的事实作为基本依据：\n\n- 在临界点附近，任何可观测量 $O$ 的积分自相关时间（IAT）$\\tau_{\\mathrm{int},O}(L)$ 表现出动态临界标度行为 $\\tau_{\\mathrm{int},O}(L) \\propto L^{z}$，这定义了所选算法和可观测量对应的动态临界指数 $z$。\n- 在临界点，磁化率的有限尺寸标度行为是 $\\chi(L) \\propto L^{\\gamma/\\nu}$，对于二维伊辛普适类，$\\gamma/\\nu = 7/4$。\n- 对于局域Metropolis更新，一次蒙特卡洛扫描会访问 $L^{d}$ 个自旋，其中 $d=2$。对于临界点下的单团簇Wolff更新，平均团簇大小与磁化率成正比，因此每次团簇更新访问的平均自旋数与 $L^{\\gamma/\\nu}$ 成标度关系。\n- 以总自旋访问次数为单位来衡量，生成长度为 $t$（MC时间单位）的轨迹所需的成本与执行的自旋级操作次数成正比。对于局域算法，一个扫描步长的时间成本与 $L^{d}$ 次自旋访问成正比；对于团簇算法，一个团簇更新步长的时间成本与 $L^{\\gamma/\\nu}$ 次自旋访问成正比。\n\n要求您从相关MC数据的热力学平均的基本原理以及上述标度事实出发，确定为了在 $\\chi(L)$ 的MC估计中达到固定的相对标准误差 $\\epsilon$，总计算功 $W$（以总自旋访问次数衡量）如何随 $L$ 变化。假设，为在 $\\chi(L)$ 中达到固定的相对精度 $\\epsilon$ 所需的有效独立样本数量与 $\\epsilon^{-2}$ 成正比（直到一个与 $L$ 无关的系数），这反映了相关序列的中心极限定理以及临界点处序参量分布的标度不变性。\n\n请按以下步骤进行：\n\n1. 从积分自相关时间的定义和相关数据的有效样本容量概念出发，推导为达到固定 $\\epsilon$ 时，两种算法的总功 $W_{\\mathrm{M}}(L,\\epsilon)$ 和 $W_{\\mathrm{C}}(L,\\epsilon)$ 如何随 $L$ 变化，将它们分别表示为 $L$ 的幂律形式（不计与 $L$ 无关的乘法常数）。\n2. 定义比率 $R(L) = W_{\\mathrm{M}}(L,\\epsilon)/W_{\\mathrm{C}}(L,\\epsilon)$ 并将其简化为 $L$ 的单一幂次。\n3. 使用以下在临界点处磁化强度平方可观测量的动态临界指数期望值：局域Metropolis算法为 $z_{\\mathrm{M}} = 2.17$，Wolff单团簇算法为 $z_{\\mathrm{C}} = 0$。取 $d=2$ 和 $\\gamma/\\nu = 7/4$。将 $R(L)$ 的最终答案表示为单一幂律形式 $L^{\\alpha}$。\n\n您的最终答案必须是关于 $L$ 的单一解析表达式，不带单位。除了使用给定的数值外，不需要进行任何取整。.",
            "solution": "该问题经检验具有科学依据、提法明确且客观。这是统计物理学中关于蒙特卡洛算法分析和有限尺寸标度的一个标准练习。所有必要的数据和物理原理均已给出，并与既有理论一致。因此，我们可以开始求解。\n\n目标是确定在达到磁化率 $\\chi(L)$ 估计的固定相对标准误差 $\\epsilon$ 的约束下，两种不同蒙特卡洛算法的总计算功 $W$ 如何随系统尺寸 $L$ 变化。\n\n令 $\\hat{\\chi}$ 为真实磁化率 $\\langle \\chi \\rangle$ 的蒙特卡洛估计量。对于总长度为 $t$（以MC步为单位）的相关时间序列，估计量的方差由下式给出：\n$$\n\\mathrm{Var}(\\hat{\\chi}) = \\frac{\\mathrm{Var}(\\chi)}{N_{\\mathrm{eff}}}\n$$\n其中 $\\mathrm{Var}(\\chi)$ 是可观测量的内禀方差，$N_{\\mathrm{eff}}$ 是有效独立样本的数量。$N_{\\mathrm{eff}}$ 与总步数 $t$ 和积分自相关时间 $\\tau_{\\mathrm{int},\\chi}$ 的关系为：\n$$\nN_{\\mathrm{eff}} = \\frac{t}{2\\tau_{\\mathrm{int},\\chi}}\n$$\n对于标度分析，前置因子 $2$ 可以并入比例常数中，因此我们用 $N_{\\mathrm{eff}} \\propto t / \\tau_{\\mathrm{int},\\chi}$ 继续分析。\n\n相对标准误差 $\\epsilon$ 定义为估计量的标准差除以可观测量（的期望）的均值：\n$$\n\\epsilon = \\frac{\\sqrt{\\mathrm{Var}(\\hat{\\chi})}}{\\langle \\chi \\rangle}\n$$\n将此表达式平方并代入 $\\mathrm{Var}(\\hat{\\chi})$ 可得：\n$$\n\\epsilon^2 = \\frac{\\mathrm{Var}(\\chi)}{N_{\\mathrm{eff}} \\langle \\chi \\rangle^2}\n$$\n解出 $N_{\\mathrm{eff}}$：\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\epsilon^2} \\frac{\\mathrm{Var}(\\chi)}{\\langle \\chi \\rangle^2}\n$$\n问题指出，对于固定的 $\\epsilon$，所需的有效样本数量与 $L$ 无关。这意味着对于大的 $L$，在临界点比率 $\\mathrm{Var}(\\chi)/\\langle \\chi \\rangle^2$ 与 $L$ 无关。这是临界现象理论中的一个已知结果。磁化率与磁化强度 $M$ 的二阶矩有关，$\\chi \\propto \\langle M^2 \\rangle$，其方差与二阶和四阶矩有关，$\\mathrm{Var}(\\chi) \\propto \\langle M^4 \\rangle - \\langle M^2 \\rangle^2$。比率 $\\mathrm{Var}(\\chi)/\\langle \\chi \\rangle^2 \\propto (\\langle M^4 \\rangle / \\langle M^2 \\rangle^2) - 1$。在临界点，量 $\\langle M^4 \\rangle / \\langle M^2 \\rangle^2$ 趋近于一个与宾德累积量相关的普适常数，这证实了问题的假设。\n\n由于对于固定的 $\\epsilon$，$N_{\\mathrm{eff}}$ 与 $L$ 无关，所需的总MC步数 $t$ 必须与积分自相关时间成标度关系：\n$$\nt(L) \\propto \\tau_{\\mathrm{int},\\chi}(L)\n$$\n问题给出了IAT的动态临界标度：\n$$\n\\tau_{\\mathrm{int},\\chi}(L) \\propto L^z\n$$\n其中 $z$ 是动态临界指数，它依赖于算法。因此，$t(L) \\propto L^z$。\n\n总计算功 $W$ 以总自旋访问次数来衡量。它是MC步数 $t$ 与每一步的计算成本 $C(L)$ 的乘积：\n$$\nW(L) = t(L) \\times C(L) \\propto L^z \\times C(L)\n$$\n\n现在我们将这个通用框架应用于指定的两种算法。\n\n第一部分：推导 $W_{\\mathrm{M}}(L,\\epsilon)$ 和 $W_{\\mathrm{C}}(L,\\epsilon)$。\n\n对于局域Metropolis算法（M）：\nIAT指数为 $z_{\\mathrm{M}}$。每一步（一次MC扫描）的成本是晶格中的自旋数，即 $L^d$。\n所以，$C_{\\mathrm{M}}(L) \\propto L^d$。\nMetropolis算法的总功 $W_{\\mathrm{M}}(L)$ 的标度关系为：\n$$\nW_{\\mathrm{M}}(L) \\propto L^{z_{\\mathrm{M}}} \\times L^d = L^{d+z_{\\mathrm{M}}}\n$$\n\n对于单团簇Wolff算法（C）：\nIAT指数为 $z_{\\mathrm{C}}$。每一步（一次团簇更新）的成本与团簇中的平均自旋数成正比。在临界点，平均团簇大小与磁化率 $\\chi(L)$ 成标度关系。\n问题给出了磁化率的有限尺寸标度：$\\chi(L) \\propto L^{\\gamma/\\nu}$。\n所以，$C_{\\mathrm{C}}(L) \\propto L^{\\gamma/\\nu}$。\n团簇算法的总功 $W_{\\mathrm{C}}(L)$ 的标度关系为：\n$$\nW_{\\mathrm{C}}(L) \\propto L^{z_{\\mathrm{C}}} \\times L^{\\gamma/\\nu} = L^{\\gamma/\\nu + z_{\\mathrm{C}}}\n$$\n\n第二部分：比率 $R(L) = W_{\\mathrm{M}}(L,\\epsilon)/W_{\\mathrm{C}}(L,\\epsilon)$。\n\n两种算法的总功之比为：\n$$\nR(L) = \\frac{W_{\\mathrm{M}}(L)}{W_{\\mathrm{C}}(L)} \\propto \\frac{L^{d+z_{\\mathrm{M}}}}{L^{\\gamma/\\nu + z_{\\mathrm{C}}}} = L^{d+z_{\\mathrm{M}} - (\\gamma/\\nu + z_{\\mathrm{C}})}\n$$\n对 $\\epsilon$ 和其他与 $L$ 无关的前置因子的依赖性在该比率中被消除了。\n\n第三部分：代入数值。\n\n问题提供了以下数值：\n维度 $d=2$。\n磁化率指数比 $\\gamma/\\nu = 7/4 = 1.75$。\nMetropolis动态指数 $z_{\\mathrm{M}} = 2.17$。\n团簇动态指数 $z_{\\mathrm{C}} = 0$。\n\n我们将这些值代入 $R(L)$ 的 $L$ 的指数中：\n$$\n\\alpha = d+z_{\\mathrm{M}} - (\\gamma/\\nu + z_{\\mathrm{C}}) = 2 + 2.17 - \\left(\\frac{7}{4} + 0\\right)\n$$\n$$\n\\alpha = 2 + 2.17 - 1.75 = 4.17 - 1.75 = 2.42\n$$\n因此，计算功的比率标度关系为：\n$$\nR(L) \\propto L^{2.42}\n$$\n问题要求一个形如 $L^{\\alpha}$ 的表达式。",
            "answer": "$$\n\\boxed{L^{2.42}}\n$$"
        }
    ]
}