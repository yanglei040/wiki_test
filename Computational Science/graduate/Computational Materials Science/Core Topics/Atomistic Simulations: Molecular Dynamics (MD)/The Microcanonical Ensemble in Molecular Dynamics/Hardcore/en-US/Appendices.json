{
    "hands_on_practices": [
        {
            "introduction": "The microcanonical ensemble provides a fundamental definition of temperature through the derivative of entropy with respect to energy, $1/T = \\partial S/\\partial E$. This exercise bridges the gap between this abstract statistical concept and the practical world of molecular dynamics, where temperature is typically measured via the average kinetic energy. By using a system of classical harmonic oscillators, for which the phase space volume can be calculated analytically, you will verify the profound equivalence of these two perspectives on temperature .",
            "id": "3494648",
            "problem": "Consider a system of $N$ independent one-dimensional classical harmonic oscillators evolving under deterministic molecular dynamics in the microcanonical ensemble. The Hamiltonian is $H(\\mathbf{x},\\mathbf{p}) = \\sum_{i=1}^{N} \\left( \\frac{p_i^2}{2 m} + \\frac{1}{2} m \\omega^2 x_i^2 \\right)$, where $m$ is the mass (in kilograms), $\\omega$ is the angular frequency (in radians per second), $x_i$ are positions (in meters), and $p_i$ are momenta (in kilogram meters per second). The microcanonical entropy is defined as $S(E) = k_{\\mathrm{B}} \\ln \\Omega(E)$, with Boltzmann constant $k_{\\mathrm{B}}$ (in joules per kelvin), and $\\Omega(E)$ is the phase space volume of microstates with energy less than or equal to $E$. The density of states is $g(E) = \\partial \\Omega(E)/\\partial E$. The microcanonical inverse temperature is defined by $1/T(E) = \\partial S(E) / \\partial E$.\n\nStarting from the definitions above and fundamental principles, derive a procedure to reconstruct the density of states $g(E)$ by sampling energy shells in phase space for this Hamiltonian. From the reconstructed $g(E)$, compute $S(E)$ and then evaluate $1/T(E) = \\partial S(E)/\\partial E$. Independently, perform microcanonical molecular dynamics (constant-energy dynamics governed by Newton’s laws and integrated using the velocity Verlet algorithm) at the same total energy $E$ and estimate the kinetic temperature from the time series of the kinetic energy $K(t)$ via the classical definition of kinetic temperature. Compare the two inverse temperatures, $1/T(E)$ obtained from the entropy derivative and $1/T_{\\mathrm{kin}}$ obtained from $K(t)$.\n\nYour program must implement the following:\n\n- A velocity Verlet integrator for $N$ independent harmonic oscillators with the specified $m$ and $\\omega$, using a time step $\\Delta t$ (in seconds) and a given number of steps. Initialize positions and velocities randomly, then rescale them to match an exact target total energy $E$ (in joules).\n- A reconstruction of $g(E)$ for this Hamiltonian by energy shell sampling in phase space, starting from first principles. Use this reconstruction to compute $S(E)$ and then $1/T(E) = \\partial S/\\partial E$.\n- A computation of the kinetic temperature $T_{\\mathrm{kin}}$ from the time-average of $K(t)$ as obtained from the molecular dynamics trajectory. Use the relation appropriate for classical kinetic energy with $N$ quadratic velocity degrees of freedom.\n\nFor each test case, report the relative difference between the two inverse temperatures as a unitless decimal defined by\n$$\n\\delta = \\frac{\\left| \\frac{1}{T(E)} - \\frac{1}{T_{\\mathrm{kin}}} \\right|}{\\frac{1}{T_{\\mathrm{kin}}}}.\n$$\n\nUse International System of Units (SI) throughout: mass in kilograms, angular frequency in radians per second, time in seconds, energy in joules, temperature in kelvin. Angles are not required beyond the angular frequency specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n\nTest suite (each case is a tuple $(N, m, \\omega, E, \\Delta t, \\text{steps})$):\n\n1. $(4,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;$4 k_{\\mathrm{B}} \\cdot 300$,\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;80000)$\n2. $(4,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;$4 k_{\\mathrm{B}} \\cdot 1$,\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;80000)$\n3. $(1,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;$1 k_{\\mathrm{B}} \\cdot 270$,\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;120000)$\n4. $(8,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;$8 k_{\\mathrm{B}} \\cdot 1200$,\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;60000)$\n\nYour program should produce a single line of output containing the four $\\delta$ values as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").",
            "solution": "The problem requires a comparison between two definitions of temperature for a system of $N$ independent one-dimensional classical harmonic oscillators in the microcanonical ensemble. The first, $T(E)$, is derived from the statistical mechanical definition of entropy. The second, $T_{\\mathrm{kin}}$, is derived from the time-averaged kinetic energy obtained via a molecular dynamics simulation. This solution presents the theoretical derivations for both, outlines the numerical procedure, and establishes their theoretical equivalence, which the accompanying program will verify numerically.\n\n### 1. Statistical Temperature from the Density of States\n\nThe first step is to derive the statistical temperature, $T(E)$, from the microcanonical entropy $S(E) = k_{\\mathrm{B}} \\ln \\Omega(E)$, where $\\Omega(E)$ is the phase space volume for states with energy less than or equal to $E$. The inverse temperature is then given by $1/T(E) = \\partial S(E)/\\partial E$.\n\nThe Hamiltonian for the system is:\n$$\nH(\\mathbf{x},\\mathbf{p}) = \\sum_{i=1}^{N} \\left( \\frac{p_i^2}{2 m} + \\frac{1}{2} m \\omega^2 x_i^2 \\right)\n$$\nThe phase space volume $\\Omega(E)$ is the integral over all positions $x_i$ and momenta $p_i$ such that $H(\\mathbf{x},\\mathbf{p}) \\leq E$:\n$$\n\\Omega(E) = \\int_{H(\\mathbf{x},\\mathbf{p}) \\leq E} \\prod_{i=1}^{N} dx_i dp_i\n$$\nTo evaluate this integral, we perform a change of variables to transform the inequality into the equation of a hypersphere. Let us define new coordinates $q_j$:\n$$\nq_{2i-1} = \\sqrt{\\frac{m \\omega^2}{2}} x_i \\quad \\text{and} \\quad q_{2i} = \\frac{1}{\\sqrt{2m}} p_i\n$$\nIn these new coordinates, the Hamiltonian becomes:\n$$\nH = \\sum_{i=1}^{N} (q_{2i-1}^2 + q_{2i}^2) = \\sum_{j=1}^{2N} q_j^2\n$$\nThe condition $H \\leq E$ is now $\\sum_{j=1}^{2N} q_j^2 \\leq E$, which describes the interior of a $2N$-dimensional hypersphere of radius $R = \\sqrt{E}$.\n\nThe differential volume element transforms as:\n$$\ndx_i = \\sqrt{\\frac{2}{m \\omega^2}} dq_{2i-1} \\quad \\text{and} \\quad dp_i = \\sqrt{2m} dq_{2i}\n$$\nThe Jacobian for each oscillator is $J_i = \\sqrt{\\frac{2}{m \\omega^2}} \\sqrt{2m} = \\frac{2}{\\omega}$. For the entire $N$-oscillator system, the phase space volume element is:\n$$\n\\prod_{i=1}^{N} dx_i dp_i = \\left(\\frac{2}{\\omega}\\right)^N \\prod_{j=1}^{2N} dq_j\n$$\nThe phase space volume integral becomes:\n$$\n\\Omega(E) = \\left(\\frac{2}{\\omega}\\right)^N \\int_{\\sum_{j=1}^{2N} q_j^2 \\leq E} \\prod_{j=1}^{2N} dq_j\n$$\nThe integral is the volume of a $d$-dimensional hypersphere of radius $R$, given by the formula $V_d(R) = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)} R^d$. With $d=2N$ and $R=\\sqrt{E}$, the volume is:\n$$\nV_{2N}(\\sqrt{E}) = \\frac{\\pi^N}{\\Gamma(N+1)} (\\sqrt{E})^{2N} = \\frac{(\\pi E)^N}{N!}\n$$\nSubstituting this back, we obtain the phase space volume:\n$$\n\\Omega(E) = \\left(\\frac{2}{\\omega}\\right)^N \\frac{(\\pi E)^N}{N!} = \\frac{1}{N!} \\left(\\frac{2\\pi E}{\\omega}\\right)^N\n$$\nThe problem asks for the density of states $g(E)$, which is the derivative of $\\Omega(E)$:\n$$\ng(E) = \\frac{\\partial \\Omega(E)}{\\partial E} = \\frac{1}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N \\frac{\\partial (E^N)}{\\partial E} = \\frac{N}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N E^{N-1} = \\frac{1}{(N-1)!} \\left(\\frac{2\\pi}{\\omega}\\right)^N E^{N-1}\n$$\nThis analytical derivation constitutes the required \"reconstruction\" of $g(E)$ from first principles for this specific Hamiltonian. The entropy is then:\n$$\nS(E) = k_{\\mathrm{B}} \\ln \\Omega(E) = k_{\\mathrm{B}} \\ln \\left[ \\frac{1}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N E^N \\right] = k_{\\mathrm{B}} \\left( \\ln\\left[\\frac{1}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N\\right] + N \\ln E \\right)\n$$\nThe microcanonical inverse temperature $1/T(E)$ is the derivative of entropy with respect to energy:\n$$\n\\frac{1}{T(E)} = \\frac{\\partial S(E)}{\\partial E} = k_{\\mathrm{B}} \\frac{\\partial}{\\partial E} (N \\ln E) = \\frac{N k_{\\mathrm{B}}}{E}\n$$\nThis provides the first, purely theoretical, value for the inverse temperature.\n\n### 2. Kinetic Temperature from Molecular Dynamics\n\nThe second approach is to compute the kinetic temperature $T_{\\mathrm{kin}}$ from a molecular dynamics (MD) simulation. The classical definition of kinetic temperature arises from the equipartition theorem, which states that each quadratic degree of freedom in the Hamiltonian contributes an average energy of $\\frac{1}{2} k_{\\mathrm{B}} T$ to the system.\n\nThe total kinetic energy of the system is $K = \\sum_{i=1}^{N} \\frac{p_i^2}{2m}$. This is a sum of $N$ quadratic terms (one for each momentum $p_i$). According to the equipartition theorem, the time-averaged total kinetic energy $\\langle K \\rangle$ is related to the kinetic temperature $T_{\\mathrm{kin}}$ by:\n$$\n\\langle K \\rangle = N \\cdot \\frac{1}{2} k_{\\mathrm{B}} T_{\\mathrm{kin}}\n$$\nFrom this, we can express the inverse kinetic temperature as:\n$$\n\\frac{1}{T_{\\mathrm{kin}}} = \\frac{N k_{\\mathrm{B}}}{2 \\langle K \\rangle}\n$$\nThe value of $\\langle K \\rangle$ is obtained by performing a constant-energy MD simulation and averaging the instantaneous kinetic energy over a long trajectory.\n\nThe simulation proceeds as follows:\n1.  **Initialization**: The $N$ positions $x_i$ and $N$ velocities $v_i$ are initialized with random values drawn from a standard normal distribution. The initial momenta are $p_i = m v_i$. The total energy $E_{\\mathrm{init}}$ is calculated from these initial conditions. The positions and momenta are then rescaled by a factor $\\lambda = \\sqrt{E / E_{\\mathrm{init}}}$, where $E$ is the target total energy. This ensures the simulation starts with the precise energy required for the microcanonical ensemble.\n2.  **Integration**: The trajectory is propagated using the velocity Verlet algorithm, a time-reversible and symplectic integrator that provides excellent energy conservation for long simulations. For an oscillator $i$, the force is $F_i = -m \\omega^2 x_i$, so the acceleration is $a_i = -\\omega^2 x_i$. The update steps for positions $x_i$ and velocities $v_i$ over a time step $\\Delta t$ are:\n    $$\n    v_i(t + \\Delta t/2) = v_i(t) + \\frac{1}{2} a_i(t) \\Delta t\n    $$\n    $$\n    x_i(t + \\Delta t) = x_i(t) + v_i(t + \\Delta t/2) \\Delta t\n    $$\n    $$\n    a_i(t + \\Delta t) = -\\omega^2 x_i(t + \\Delta t)\n    $$\n    $$\n    v_i(t + \\Delta t) = v_i(t + \\Delta t/2) + \\frac{1}{2} a_i(t + \\Delta t) \\Delta t\n    $$\n3.  **Averaging**: During the simulation, the instantaneous kinetic energy $K(t) = \\sum_{i=1}^N \\frac{(m v_i(t))^2}{2m}$ is calculated at each step and accumulated. The time average $\\langle K \\rangle$ is computed at the end of the simulation.\n\n### 3. Comparison and Theoretical Equivalence\n\nFor a system of classical harmonic oscillators, the Virial theorem states that the time average of the total kinetic energy is equal to the time average of the total potential energy, $\\langle K \\rangle = \\langle V \\rangle$. Since the total energy $E = K + V$ is conserved in the microcanonical ensemble, we have $E = \\langle K + V \\rangle = \\langle K \\rangle + \\langle V \\rangle$. This implies that $\\langle K \\rangle = E/2$.\n\nSubstituting this expected result into the expression for the kinetic temperature gives:\n$$\n\\frac{1}{T_{\\mathrm{kin}}} = \\frac{N k_{\\mathrm{B}}}{2 \\langle K \\rangle} = \\frac{N k_{\\mathrm{B}}}{2 (E/2)} = \\frac{N k_{\\mathrm{B}}}{E}\n$$\nThis is identical to the expression for $1/T(E)$ derived from statistical mechanics. Thus, the two definitions of temperature are theoretically equivalent for this system. The purpose of the calculation is to numerically verify this equivalence. The relative difference $\\delta$ will quantify any deviation arising from the finite time step and finite duration of the numerical simulation. A small value of $\\delta$ confirms the consistency of the statistical and kinetic pictures of temperature as well as the accuracy of the MD simulation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.constants import k as k_B\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite from the problem statement.\n    # Each case is a tuple (N, m, omega, E_factor, T_factor, dt, steps)\n    # where E = N * k_B * T_factor for cases 1,3,4 and 4*k_B*T_factor for case 2\n    test_cases_raw = [\n        (4, 6.6335209e-26, 1.0e13, 4, 300, 1.0e-16, 80000),\n        (4, 6.6335209e-26, 1.0e13, 4, 1, 1.0e-16, 80000),\n        (1, 6.6335209e-26, 1.0e13, 1, 270, 1.0e-16, 120000),\n        (8, 6.6335209e-26, 1.0e13, 8, 1200, 1.0e-16, 60000),\n    ]\n\n    test_cases = []\n    for i, case in enumerate(test_cases_raw):\n        N, m, omega, E_factor, T_factor, dt, steps = case\n        # Energy E is defined as N * k_B * T for all cases in the problem logic\n        # For N=4, E=4*kB*T. For N=1, E=1*kB*T. For N=8, E=8*kB*T.\n        E = E_factor * k_B * T_factor\n        test_cases.append((N, m, omega, E, dt, steps))\n\n    results = []\n    for case in test_cases:\n        delta = compute_relative_difference(case)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_relative_difference(case_params):\n    \"\"\"\n    Computes the relative difference delta for a single test case.\n    \"\"\"\n    N, m, omega, E_target, dt, n_steps = case_params\n\n    # ======== 1. Statistical Temperature T(E) ========\n    # From the derivation, 1/T(E) = N * k_B / E\n    inv_T_statistical = (N * k_B) / E_target\n\n    # ======== 2. Kinetic Temperature T_kin from MD ========\n    # Perform MD simulation to get the time-averaged kinetic energy K\n    \n    # Set a seed for reproducibility\n    rng = np.random.default_rng(seed=0)\n\n    # Initialize positions and velocities randomly\n    x = rng.standard_normal(N)\n    v = rng.standard_normal(N)\n    p = m * v\n\n    # Calculate initial energy\n    K_init = 0.5 * np.sum(p**2) / m\n    V_init = 0.5 * m * omega**2 * np.sum(x**2)\n    E_init = K_init + V_init\n\n    # Rescale to target energy E_target\n    if E_init  0:\n        scale_factor = np.sqrt(E_target / E_init)\n        x *= scale_factor\n        p *= scale_factor\n    \n    # Re-calculate v from scaled p\n    v = p / m\n\n    # Velocity Verlet Integration\n    kinetic_energies = []\n    \n    # Initial acceleration\n    a = -omega**2 * x\n\n    for _ in range(n_steps):\n        # First half-step for velocity\n        v_half = v + 0.5 * a * dt\n        \n        # Full step for position\n        x = x + v_half * dt\n        \n        # Update acceleration using new positions\n        a = -omega**2 * x\n        \n        # Second half-step for velocity\n        v = v_half + 0.5 * a * dt\n        \n        # Update momentum\n        p = m * v\n        \n        # Calculate and store instantaneous kinetic energy\n        K_inst = 0.5 * np.sum(p**2) / m\n        kinetic_energies.append(K_inst)\n\n    # Calculate time-averaged kinetic energy\n    avg_K = np.mean(kinetic_energies)\n\n    # Calculate inverse kinetic temperature 1/T_kin = N * k_B / (2 * K)\n    inv_T_kinetic = (N * k_B) / (2 * avg_K)\n\n    # ======== 3. Comparison ========\n    # Calculate the relative difference delta\n    delta = np.abs(inv_T_statistical - inv_T_kinetic) / inv_T_kinetic\n    \n    return delta\n\nsolve()\n```"
        },
        {
            "introduction": "The validity of using time averages from a single molecular dynamics trajectory to represent microcanonical ensemble averages rests on the ergodic hypothesis—the assumption that the system explores all accessible microstates on the energy shell. This practice introduces powerful diagnostics, including Poincaré sections and time-autocorrelation functions, to critically assess whether a system's dynamics are truly mixing or are confined to a smaller region of phase space. You will see firsthand how non-ergodic behavior in classic model systems can lead to significant bias in calculated properties, underscoring the importance of verifying this core assumption in any $NVE$ simulation .",
            "id": "3494707",
            "problem": "Write a complete, runnable program that implements and applies a principled diagnostic of mixing on a constant-energy manifold for Hamiltonian molecular dynamics in the microcanonical ensemble. Your diagnostic must combine two independent criteria: (i) decay of the time autocorrelation of an observable and (ii) coverage of a Poincaré surface of section on the energy shell. Then quantify the finite-time microcanonical bias for a symmetry-selected observable. The design must start from fundamental definitions and laws without shortcut formulas.\n\nYou must base your derivation and algorithm on the following fundamental principles:\n- Newtonian mechanics for Hamiltonian systems, with position $q$ and momentum $p$ obeying Hamilton's equations $\\dot{q} = \\partial H / \\partial p$ and $\\dot{p} = - \\partial H / \\partial q$, where $H(q,p)$ is the Hamiltonian.\n- The microcanonical ensemble is defined by the uniform invariant density on the energy shell $H(q,p)=E$ for total energy $E$. For a phase function (observable) $A(q,p)$, the microcanonical expectation $\\langle A \\rangle_{\\mu E}$ is the average of $A$ over the energy shell with respect to the invariant measure.\n- Mixing on the energy shell implies that time correlations of centered observables decay to zero at long times, and that trajectories explore the available manifold in a way that fills Poincaré sections in area-like regions (rather than lying on smooth curves characteristic of integrable dynamics).\n- For a stationary time series $a(t)$ constructed from a bounded observable along a Hamiltonian trajectory, define the fluctuation $\\delta a(t) = a(t) - \\overline{a}$, where $\\overline{a}$ is the time average over a finite interval. Define the normalized autocorrelation $\\rho(\\tau) = C(\\tau)/C(0)$, where $C(\\tau) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\delta a(t)\\,\\delta a(t+\\tau)\\,dt$. In computation with finite $T$, approximate $C(\\tau)$ by the discrete-time estimator using a long trajectory.\n- A Poincaré section is defined by a codimension-one surface, here taken as $y=0$ with $p_ygt;0$ in two-dimensional systems. The set of section points $(x,p_x)$ produced by a long trajectory at energy $E$ lies within the energy-allowed domain of the section. For an integrable system, section points lie on smooth invariant curves; for a mixing (chaotic) system, section points fill positive-area regions.\n\nAlgorithmic requirements and observable selection:\n- You must use the symplectic velocity-Verlet scheme to integrate each Hamiltonian system with a constant time step $dt$ to minimize energy drift.\n- For each system, define a scalar observable $A(q,p)$ that has a known microcanonical expectation by symmetry at the specified energy:\n  - Isotropic two-dimensional harmonic oscillator: $A(t) = x(t)^2 - y(t)^2$, whose microcanonical expectation is $\\langle A \\rangle_{\\mu E} = 0$ by symmetry on the energy shell.\n  - Henon–Heiles system: $A(t) = x(t)$, whose microcanonical expectation is $\\langle A \\rangle_{\\mu E} = 0$ by the $x \\mapsto -x$ symmetry of the Hamiltonian.\n  - Symmetric one-dimensional double-well: $A(t) = x(t)$, whose microcanonical expectation is $\\langle A \\rangle_{\\mu E} = 0$ by $x \\mapsto -x$ symmetry.\n- Define a finite-time estimate of bias as $b_T = \\left|\\overline{A}_T - \\langle A \\rangle_{\\mu E}\\right|$, where $\\overline{A}_T$ is the time average of $A$ over the simulated time $T$.\n- Define an integrated absolute correlation time over a fixed lag window as\n$$\n\\tau_{\\mathrm{abs}} = dt \\sum_{k=1}^{K} \\left|\\rho(k\\,dt)\\right|,\n$$\nwith $K$ chosen to correspond to a window $T_{\\mathrm{lag}} = K\\,dt$ that is small compared to the total simulation time but large compared to the microscopic time scales.\n- Define a Poincaré coverage fraction for two-dimensional systems as follows: using the crossings of $y=0$ with $p_ygt;0$, record $(x,p_x)$ at the crossing times. Partition the fixed energy-allowed domain on the section into a uniform $G \\times G$ grid and compute the fraction of grid cells visited at least once. For the chosen section $y=0$ in the two-dimensional isotropic harmonic oscillator and Henon–Heiles systems at energy $E$, the allowed domain is the disk $x^2 + p_x^2 \\le 2E$. For one-dimensional systems, report a coverage fraction of $0$ because the chosen section is not applicable.\n\nMixing decision rule:\n- Declare that a trajectory is mixing if and only if both conditions hold: $\\tau_{\\mathrm{abs}} lt; \\tau_{\\mathrm{thr}}$ and the coverage fraction exceeds a threshold $c_{\\mathrm{thr}}$, with thresholds specified below.\n\nNumerical details, units, and test suite:\n- Use dimensionless units throughout.\n- Use the velocity-Verlet integrator with time step $dt = 10^{-2}$ and total number of steps $N = 60000$ for each system, giving a total simulated time $T = N \\, dt = 600$.\n- Compute the normalized autocorrelation using a fast convolution method and take $T_{\\mathrm{lag}} = 50$ so that $K = T_{\\mathrm{lag}}/dt = 5000$. Compute $\\tau_{\\mathrm{abs}}$ as defined above.\n- For Poincaré coverage, use a uniform $G \\times G$ grid with $G = 64$ over the square $[-R,R] \\times [-R,R]$ where $R = \\sqrt{2E}$, and normalize the coverage by the number of grid cells whose centers lie inside the disk $x^2 + p_x^2 \\le 2E$.\n- Thresholds for the mixing decision: $\\tau_{\\mathrm{thr}} = 10$ and $c_{\\mathrm{thr}} = 0.05$.\n\nSimulate the following three systems (this set of parameter values forms the required test suite):\n- Case $1$ (two-dimensional isotropic harmonic oscillator): Hamiltonian $H = \\frac{1}{2}(p_x^2 + p_y^2) + \\frac{1}{2}(x^2 + y^2)$, total energy $E = 1$. Initial condition $(x_0,y_0,p_{x,0},p_{y,0}) = (1,0,1,0)$. Observable $A(t)=x(t)^2 - y(t)^2$. For this case, the microcanonical value is $\\langle A \\rangle_{\\mu E} = 0$.\n- Case $2$ (Henon–Heiles): Hamiltonian $H = \\frac{1}{2}(p_x^2 + p_y^2) + \\frac{1}{2}(x^2 + y^2) + x^2 y - \\frac{1}{3} y^3$, total energy $E = 0.12$. Initial condition $(x_0,y_0,p_{x,0}) = (0,0.1,0)$ and choose $p_{y,0} gt; 0$ to satisfy $H=E$. Observable $A(t)=x(t)$. For this case, the microcanonical value is $\\langle A \\rangle_{\\mu E} = 0$.\n- Case $3$ (one-dimensional symmetric double well): Hamiltonian $H = \\frac{1}{2}p^2 + \\frac{1}{4}(x^2 - 1)^2$, total energy $E = 0.1$. Initial condition $x_0 = -1$, choose $p_0 gt; 0$ to satisfy $H=E$. Observable $A(t)=x(t)$. For this case, the microcanonical value is $\\langle A \\rangle_{\\mu E} = 0$.\n\nProgram output specification:\n- For each case, compute the boolean mixing decision and the absolute bias $b_T$. Your program should produce a single line of output containing a list of three two-element lists in the order of the cases, where each inner list is of the form $[\\text{mixing\\_boolean}, \\text{bias}]$. The mixing boolean must be a programming-language boolean. The bias must be a floating-point number rounded to six decimal places. For example, an output line could look like `[[True,0.012345],[False,0.678901],[False,0.234567]]`.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in classical and statistical mechanics, well-posed with all necessary parameters and definitions provided, and objective in its formulation. The problem requires the implementation of a sophisticated diagnostic for mixing in Hamiltonian systems, a canonical task in computational physics. It combines temporal analysis (autocorrelation) and phase-space geometric analysis (Poincaré section) to classify the dynamics of three well-understood model systems.\n\nThe solution proceeds by first constructing the necessary components from fundamental principles, and then applying them to the three specified test cases.\n\n### 1. Theoretical Foundation and Numerical Integration\n\nThe dynamics of each system are governed by a Hamiltonian $H(q,p)$, where $q$ are the generalized coordinates and $p$ are the conjugate momenta. The time evolution follows Hamilton's equations of motion:\n$$\n\\dot{q} = \\frac{\\partial H}{\\partial p}, \\quad \\dot{p} = - \\frac{\\partial H}{\\partial q}\n$$\nFor the Hamiltonians specified, which take the form $H(q,p) = \\frac{1}{2m}p^2 + V(q)$, and assuming unit mass ($m=1$), these equations become $\\dot{q} = p$ and $\\dot{p} = -\\nabla V(q) = F(q)$, where $F(q)$ is the force derived from the potential energy $V(q)$.\n\nTo numerically integrate these equations of motion while preserving the geometric structure of Hamiltonian flow and ensuring good energy conservation, we employ the **symplectic velocity-Verlet algorithm**. For a time step $dt$, the algorithm updates positions and momenta from time $t$ to $t+dt$ as follows:\n1.  Update momentum by a half-step: $p(t + dt/2) = p(t) + F(q(t)) \\cdot (dt/2)$.\n2.  Update position by a full step using the half-step momentum: $q(t + dt) = q(t) + p(t + dt/2) \\cdot dt$.\n3.  Compute the new force $F(q(t + dt))$ at the new position.\n4.  Complete the momentum update: $p(t + dt) = p(t + dt/2) + F(q(t + dt)) \\cdot (dt/2)$.\n\nThis process is repeated for $N=60000$ steps with $dt=10^{-2}$ to generate a trajectory of total duration $T=600$.\n\n### 2. Diagnostic Metrics for Mixing\n\nThe microcanonical ensemble describes an isolated system at constant total energy $E$. The ergodic hypothesis posits that for a chaotic system, time averages along a single long trajectory are equivalent to ensemble averages over the constant-energy surface. A system is **mixing** if it is ergodic and correlations between observables decay over time. We use two metrics to test for mixing behavior.\n\n#### 2.1. Autocorrelation Time, $\\tau_{\\mathrm{abs}}$\n\nFor a stationary time series of an observable $A(t)$, we first compute its time average over the trajectory, $\\overline{A}_T$. The fluctuation is $\\delta A(t) = A(t) - \\overline{A}_T$. The normalized time autocorrelation function is:\n$$\n\\rho(\\tau) = \\frac{C(\\tau)}{C(0)}, \\quad \\text{where} \\quad C(\\tau) = \\langle \\delta A(t) \\delta A(t+\\tau) \\rangle_t\n$$\nIn a mixing system, $\\rho(\\tau) \\to 0$ as $\\tau \\to \\infty$. In a non-ergodic or integrable system, $\\rho(\\tau)$ may exhibit persistent oscillations. We quantify the decay rate by computing the integrated absolute correlation time over a lag window $T_{\\mathrm{lag}} = K\\,dt = 50$:\n$$\n\\tau_{\\mathrm{abs}} = dt \\sum_{k=1}^{K} \\left|\\rho(k\\,dt)\\right|\n$$\nA small $\\tau_{\\mathrm{abs}}$ indicates rapid decay and is a signature of mixing. The autocorrelation is computed efficiently using a fast Fourier transform (FFT) based convolution method (the Wiener-Khinchin theorem).\n\n#### 2.2. Poincaré Section Coverage\n\nA Poincaré section provides a geometric view of the phase space dynamics. For our two-dimensional systems, we define the section by the plane $y=0$ and record the state $(x, p_x)$ whenever the trajectory crosses this plane with positive momentum in the $y$-direction ($p_y0$).\n-   For an **integrable** system, these points lie on smooth, one-dimensional curves (invariant tori).\n-   For a **mixing** system, the points appear to fill a two-dimensional area on the section, corresponding to chaotic wandering on the energy manifold.\n\nTo quantify this, we discretize the accessible region of the section, $x^2 + p_x^2 \\le 2E$, with a uniform $G \\times G$ grid (where $G=64$). The coverage fraction is the number of grid cells visited by the trajectory at least once, normalized by the total number of grid cell centers that lie within the accessible disk. A high coverage fraction indicates chaotic, area-filling behavior consistent with mixing. For one-dimensional systems, this specific section is not applicable, and the coverage is defined as $0$.\n\n### 3. Finite-Time Bias and Mixing Decision\n\nThe **finite-time bias**, $b_T$, measures the deviation of the computed time average of an observable, $\\overline{A}_T$, from its true microcanonical expectation, $\\langle A \\rangle_{\\mu E}$. The observables are chosen such that symmetry dictates $\\langle A \\rangle_{\\mu E} = 0$. Thus, the bias is simply:\n$$\nb_T = \\left|\\overline{A}_T\\right|\n$$\nA large bias suggests that the trajectory has not explored the phase space ergodically.\n\nThe final **mixing decision** combines our two criteria. A trajectory is declared `mixing` if and only if both conditions are met:\n1.  The integrated correlation time is small: $\\tau_{\\mathrm{abs}}  \\tau_{\\mathrm{thr}}$ (with $\\tau_{\\mathrm{thr}} = 10$).\n2.  The Poincaré coverage is significant: `coverage` $ c_{\\mathrm{thr}}$ (with $c_{\\mathrm{thr}} = 0.05$).\n\n### 4. Application to Test Systems\n\nThis framework is applied to three systems:\n\n1.  **Isotropic Harmonic Oscillator ($E=1$)**: This system is fully integrable. The chosen initial condition, $(1,0,1,0)$, leads to a trivial trajectory along the $x$-axis where $y(t) \\equiv 0$ and $p_y(t) \\equiv 0$. It will never cross the Poincaré section with $p_y0$, resulting in zero coverage. The observable $A(t) = x(t)^2 - y(t)^2 = x(t)^2$ is periodic, leading to a non-decaying correlation function and a large $\\tau_{\\mathrm{abs}}$. The system is correctly identified as **non-mixing**.\n\n2.  **Henon-Heiles System ($E=0.12$)**: At this energy, the system is known to be predominantly chaotic. We expect the trajectory to explore a large volume of the energy shell. The autocorrelation of $A(t)=x(t)$ should decay quickly, and the Poincaré section should be densely filled. The system should be identified as **mixing**, and the bias should be small due to the symmetric exploration of phase space.\n\n3.  **Symmetric Double Well ($E=0.1$)**: The potential barrier height is $V(0) = 0.25$. Since the total energy $E=0.1$ is less than the barrier height, a trajectory starting in one well (at $x_0=-1$) remains trapped there. It cannot explore the full energy surface, violating ergodicity. The time average of $A(t)=x(t)$ will be negative, not the symmetric expectation of $0$, leading to a large bias. The motion is periodic, so $\\tau_{\\mathrm{abs}}$ will be large. The Poincaré coverage is defined as $0$. The system is correctly identified as **non-mixing**.\n\nThe implementation details follow the numerical parameters specified in the problem, including the calculation of initial momenta and the specific definitions of the analysis metrics.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import fftconvolve\n\n# This program must be run with:\n# python 3.12\n# numpy 1.23.5\n# scipy 1.11.4\n\ndef solve():\n    \"\"\"\n    Main function to run the mixing diagnostics on three Hamiltonian systems.\n    \"\"\"\n    # Global numerical parameters from the problem statement\n    DT = 1e-2\n    N_STEPS = 60000\n    T_LAG = 50.0\n    K_STEPS = int(T_LAG / DT)\n    GRID_SIZE = 64\n    TAU_THR = 10.0\n    C_THR = 0.05\n\n    # --- System Definitions ---\n\n    # Case 1: Isotropic Harmonic Oscillator\n    def ho_force(q):\n        return -q\n    def ho_observable(q, p):\n        return q[0]**2 - q[1]**2\n\n    # Case 2: Henon-Heiles\n    def hh_force(q):\n        x, y = q\n        fx = -x - 2 * x * y\n        fy = -y - x**2 + y**2\n        return np.array([fx, fy])\n    def hh_observable(q, p):\n        return q[0]\n\n    # Case 3: 1D Symmetric Double Well\n    def dw_force(q):\n        x = q[0]\n        fx = x - x**3\n        return np.array([fx])\n    def dw_observable(q, p):\n        return q[0]\n\n    # --- Core Simulation and Analysis Functions ---\n\n    def velocity_verlet_integrator(z0, dim, force_func, dt, n_steps):\n        \"\"\"\n        Integrates Hamiltonian dynamics using the velocity-Verlet algorithm.\n        Also detects and records Poincaré section crossings for 2D systems.\n        \"\"\"\n        z_history = np.zeros((n_steps + 1, 2 * dim))\n        z = z0.copy()\n        z_history[0] = z\n        \n        poincare_points = []\n\n        q_old, p_old = None, None\n\n        for i in range(n_steps):\n            q = z[:dim]\n            p = z[dim:]\n\n            if dim == 2:\n                q_old = q.copy()\n            \n            # Velocity-Verlet update\n            p_half = p + force_func(q) * (dt / 2.0)\n            q_new = q + p_half * dt\n            p_new = p_half + force_func(q_new) * (dt / 2.0)\n            \n            z[:dim], z[dim:] = q_new, p_new\n            z_history[i + 1] = z\n\n            # Poincaré section detection for 2D systems (y=0, py0)\n            if dim == 2:\n                y_old, y_new = q_old[1], q_new[1]\n                if y_old * y_new  0 and p_new[1]  0:\n                    # Linear interpolation to find the crossing point\n                    alpha = -y_old / (y_new - y_old)\n                    x_cross = q_old[0] + alpha * (q_new[0] - q_old[0])\n                    px_cross = z_history[i, 2] + alpha * (p_new[0] - z_history[i, 2])\n                    poincare_points.append((x_cross, px_cross))\n\n        return z_history, poincare_points\n\n    def calculate_autocorrelation(A_series, k_max, dt):\n        \"\"\"\n        Calculates the normalized autocorrelation and the integrated absolute correlation time.\n        \"\"\"\n        signal = A_series - np.mean(A_series)\n        n = len(signal)\n        # Using FFT for fast convolution\n        autocov = fftconvolve(signal, signal[::-1], mode='full')[n - 1:]\n        \n        # Biased estimator for autocovariance\n        autocov /= n\n        \n        # Avoid division by zero if variance is nil\n        if autocov[0] == 0:\n            return 0.0\n\n        rho = autocov / autocov[0]\n        \n        if len(rho) = k_max:\n             k_max = len(rho) -1\n        \n        tau_abs = dt * np.sum(np.abs(rho[1:k_max + 1]))\n        return tau_abs\n\n    def calculate_poincare_coverage(points, E, G):\n        \"\"\"\n        Calculates the coverage fraction of a Poincaré section.\n        \"\"\"\n        if not points:\n            return 0.0\n\n        R = np.sqrt(2 * E)\n        \n        # Calculate the total number of grid cells within the allowed domain\n        grid_coords = np.linspace(-R, R, G, endpoint=False) + R/G\n        centers_x, centers_y = np.meshgrid(grid_coords, grid_coords)\n        in_disk = centers_x**2 + centers_y**2 = 2 * E\n        total_allowed_cells = np.sum(in_disk)\n\n        if total_allowed_cells == 0:\n            return 0.0\n\n        visited_cells = set()\n        for x, px in points:\n            if -R = x  R and -R = px  R:\n                ix = int((x + R) / (2 * R) * G)\n                ipx = int((px + R) / (2 * R) * G)\n                visited_cells.add((ix, ipx))\n        \n        coverage = len(visited_cells) / total_allowed_cells\n        return coverage\n\n    def process_case(case, dt, n_steps, k_steps, grid_size, tau_thr, c_thr):\n        \"\"\"\n        Processes a single test case: runs simulation and performs analysis.\n        \"\"\"\n        # Run integrator\n        state_history, poincare_points = velocity_verlet_integrator(\n            case[\"ic\"], case[\"dim\"], case[\"force_func\"], dt, n_steps\n        )\n\n        # Calculate observable time series\n        q_history = state_history[:, :case[\"dim\"]]\n        p_history = state_history[:, case[\"dim\"]:]\n        A_series = np.array([case[\"observable_func\"](q, p) for q, p in zip(q_history, p_history)])\n        \n        # Calculate bias\n        A_mean = np.mean(A_series)\n        bias = np.abs(A_mean - case[\"mu_A\"])\n\n        # Calculate correlation time\n        tau_abs = calculate_autocorrelation(A_series, k_steps, dt)\n        \n        # Calculate Poincaré coverage\n        if case[\"dim\"] == 2:\n            coverage = calculate_poincare_coverage(poincare_points, case[\"E\"], grid_size)\n        else:\n            coverage = 0.0\n            \n        # Apply mixing decision rule\n        is_mixing = (tau_abs  tau_thr) and (coverage  c_thr)\n        \n        return is_mixing, bias\n\n    # --- Test Suite Setup and Execution ---\n\n    test_cases = [\n        # Case 1: Harmonic Oscillator\n        {\n            \"name\": \"HO\", \"dim\": 2, \"force_func\": ho_force,\n            \"observable_func\": ho_observable, \"E\": 1.0,\n            \"ic\": np.array([1.0, 0.0, 1.0, 0.0]), \"mu_A\": 0.0\n        },\n        # Case 2: Henon-Heiles\n        {\n            \"name\": \"HH\", \"dim\": 2, \"force_func\": hh_force,\n            \"observable_func\": hh_observable, \"E\": 0.12,\n            \"ic\": np.array([0.0, 0.1, 0.0, 0.0]), \"mu_A\": 0.0\n        },\n        # Case 3: Double Well\n        {\n            \"name\": \"DW\", \"dim\": 1, \"force_func\": dw_force,\n            \"observable_func\": dw_observable, \"E\": 0.1,\n            \"ic\": np.array([-1.0, 0.0]), \"mu_A\": 0.0\n        }\n    ]\n\n    # Complete initial conditions using energy constraint\n    # Case 2: Henon-Heiles\n    E2, q2, p2x = test_cases[1][\"E\"], test_cases[1][\"ic\"][:2], test_cases[1][\"ic\"][2]\n    V2 = 0.5 * (q2[0]**2 + q2[1]**2) + q2[0]**2 * q2[1] - (q2[1]**3) / 3.0\n    p2y_sq = 2 * (E2 - V2) - p2x**2\n    test_cases[1][\"ic\"][3] = np.sqrt(p2y_sq)\n\n    # Case 3: Double Well\n    E3, q3 = test_cases[2][\"E\"], test_cases[2][\"ic\"][0]\n    V3 = 0.25 * (q3**2 - 1.0)**2\n    p3_sq = 2 * (E3 - V3)\n    test_cases[2][\"ic\"][1] = np.sqrt(p3_sq)\n\n    # Process all cases and collect results\n    results = []\n    for case in test_cases:\n        is_mixing, bias = process_case(\n            case, DT, N_STEPS, K_STEPS, GRID_SIZE, TAU_THR, C_THR\n        )\n        results.append([is_mixing, round(bias, 6)])\n\n    # Final print statement in the exact required format\n    # The default str() representation of lists has spaces, which we remove\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Realistic molecular simulations frequently employ holonomic constraints, such as fixed bond lengths, to eliminate high-frequency motions and allow for larger integration time steps. These constraints, however, reduce the dimensionality of the accessible phase space, requiring a careful re-evaluation of statistical mechanical principles like the equipartition theorem. This exercise provides a rigorous, hands-on procedure for determining the effective number of kinetic degrees of freedom, $f_{\\mathrm{eff}}$, and deriving the correct relationship between kinetic energy and temperature for constrained systems .",
            "id": "3494645",
            "problem": "You are asked to investigate energy equipartition in the microcanonical ensemble with constraints, as realized in the constant Number of particles, Volume, and Energy (NVE) ensemble for constrained Molecular Dynamics using the SHAKE constraint algorithm (SHAKE). Starting from first principles, derive the expression for the microcanonical temperature under holonomic constraints and implement a program that constructs constrained velocity fields, computes mode-resolved kinetic energies, and evaluates equipartition quantitatively.\n\nBegin from the following foundations:\n- Newton’s Second Law and the definition of kinetic energy: for a system with generalized Cartesian velocities $\\mathbf{v} \\in \\mathbb{R}^{n}$ and diagonal mass matrix $\\mathbf{M} = \\mathrm{diag}(m_1,\\dots,m_n)$, the kinetic energy is $K = \\tfrac{1}{2}\\,\\mathbf{v}^{\\mathsf{T}}\\mathbf{M}\\mathbf{v}$.\n- Holonomic constraints of the form $\\boldsymbol{\\Phi}(\\mathbf{q}) = \\mathbf{0}$, with Jacobian $\\mathbf{J}(\\mathbf{q}) = \\partial \\boldsymbol{\\Phi}/\\partial \\mathbf{q}$, imply the instantaneous velocity constraints $\\mathbf{J}(\\mathbf{q})\\,\\mathbf{v} = \\mathbf{0}$ when the constraints are maintained exactly.\n- The Boltzmann constant $k_{\\mathrm{B}}$ is a positive constant relating energy and temperature and has value $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$.\n\nYour tasks:\n1) Derive, from the above definitions only, the constrained-mode representation of the kinetic energy by introducing mass-weighted velocities $\\mathbf{w} = \\mathbf{M}^{1/2}\\mathbf{v}$, and the mass-weighted constraint operator $\\mathbf{A} = \\mathbf{J}\\,\\mathbf{M}^{-1/2}$. Show that admissible $\\mathbf{w}$ satisfy $\\mathbf{A}\\,\\mathbf{w} = \\mathbf{0}$ and that any admissible $\\mathbf{w}$ can be written as $\\mathbf{w} = \\mathbf{N}\\,\\mathbf{a}$ where columns of $\\mathbf{N}$ form an orthonormal basis of $\\mathrm{null}(\\mathbf{A})$ and $\\mathbf{a} \\in \\mathbb{R}^{f_{\\mathrm{eff}}}$ are modal amplitudes, where $f_{\\mathrm{eff}} = \\dim \\mathrm{null}(\\mathbf{A})$ is the effective number of unconstrained quadratic degrees of freedom.\n2) Using the microcanonical definition of temperature for a quadratic kinetic energy on the constraint manifold, and the geometric structure of constant-energy surfaces in the space of $\\mathbf{a}$, derive the formula that relates the total kinetic energy $K$ to temperature $T$ using $f_{\\mathrm{eff}}$.\n3) Implement a program that, for each test case below, does the following:\n   - Constructs $\\mathbf{N}$ numerically via a singular value decomposition of $\\mathbf{A}$ and computes $f_{\\mathrm{eff}}$ as the nullity of $\\mathbf{A}$.\n   - Generates a constrained velocity field with exactly specified total kinetic energy $K_{\\text{target}}$ by choosing equal modal energies in the coordinates $\\mathbf{a}$.\n   - Computes the mode-resolved kinetic energies $K_i = \\tfrac{1}{2} a_i^2$ and the maximum relative deviation from equal partition, defined as\n     $$\\delta_{\\max} = \\begin{cases}\n     \\max_i \\left|\\dfrac{K_i - K_{\\text{target}}/f_{\\mathrm{eff}}}{K_{\\text{target}}/f_{\\mathrm{eff}}}\\right|,  \\text{if } K_{\\text{target}}  0,\\\\\n     0,  \\text{if } K_{\\text{target}} = 0,\n     \\end{cases}$$\n     where $i = 1,\\dots,f_{\\mathrm{eff}}$.\n   - Uses your derived temperature formula to compute the constrained microcanonical temperature $T$ in Kelvin.\n   - Returns, for each test case, a list of the form $[T, f_{\\mathrm{eff}}, \\delta_{\\max}]$, where $T$ is a float in Kelvin, $f_{\\mathrm{eff}}$ is an integer, and $\\delta_{\\max}$ is a float.\n\nPhysical units and conventions:\n- All masses must be interpreted in kilograms, all energies in Joules, and the output temperature must be expressed in Kelvin. The Boltzmann constant must be $k_{\\mathrm{B}} = 1.380649\\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$.\n- Angles do not appear; no angle unit is required.\n\nTest suite:\nProvide the following four cases to your program. Each case is one-dimensional (so $\\mathbf{v} \\in \\mathbb{R}^{N}$ for $N$ particles), with linear holonomic constraints that enforce fixed pairwise distances at a given configuration. The constraint Jacobian acting on velocities can be represented at that configuration by a constant matrix $\\mathbf{J}$ whose rows encode equal-velocity constraints along constrained bonds. Specifically, use:\n\n- Case $1$ (happy path with fully constrained chain):\n  - Masses $\\mathbf{m} = [\\,1.66\\times 10^{-27},\\, 3.32\\times 10^{-27},\\, 4.98\\times 10^{-27}\\,]$\n  - Constraints $\\mathbf{J} = \\begin{bmatrix} -1  1  0 \\\\ 0  -1  1 \\end{bmatrix}$\n  - Target kinetic energy $K_{\\text{target}} = 1.0\\times 10^{-20}$\n\n- Case $2$ (partial constraint with remaining internal motion):\n  - Masses $\\mathbf{m} = [\\,1.66\\times 10^{-27},\\, 3.32\\times 10^{-27},\\, 4.98\\times 10^{-27},\\, 6.64\\times 10^{-27}\\,]$\n  - Constraints $\\mathbf{J} = \\begin{bmatrix} -1  1  0  0 \\end{bmatrix}$\n  - Target kinetic energy $K_{\\text{target}} = 3.0\\times 10^{-21}$\n\n- Case $3$ (redundant constraints testing numerical rank detection):\n  - Masses $\\mathbf{m} = [\\,1.66\\times 10^{-27},\\, 3.32\\times 10^{-27},\\, 4.98\\times 10^{-27}\\,]$\n  - Constraints $\\mathbf{J} = \\begin{bmatrix} -1  1  0 \\\\ -2  2  0 \\end{bmatrix}$\n  - Target kinetic energy $K_{\\text{target}} = 2.5\\times 10^{-21}$\n\n- Case $4$ (zero-energy boundary case):\n  - Masses $\\mathbf{m} = [\\,1.00\\times 10^{-26},\\, 2.00\\times 10^{-26}\\,]$\n  - Constraints $\\mathbf{J} = \\begin{bmatrix} -1  1 \\end{bmatrix}$\n  - Target kinetic energy $K_{\\text{target}} = 0.0$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list $[T, f_{\\mathrm{eff}}, \\delta_{\\max}]$ in that order, where $T$ is a float in Kelvin, $f_{\\mathrm{eff}}$ is an integer, and $\\delta_{\\max}$ is a float. For example, the output must look like\n$[[T_1, f_{\\mathrm{eff},1}, \\delta_{\\max,1}], [T_2, f_{\\mathrm{eff},2}, \\delta_{\\max,2}], \\dots]$.",
            "solution": "The objective is to derive the statistical-mechanical description of temperature for a classical system subject to holonomic constraints and to implement an algorithm to construct constrained velocity fields that satisfy energy equipartition.\n\n**Part 1: Constrained-Mode Representation of Kinetic Energy**\n\nWe begin with the foundational definition of kinetic energy for a system of $n$ generalized Cartesian coordinates with corresponding velocities $\\mathbf{v} \\in \\mathbb{R}^{n}$ and a diagonal mass matrix $\\mathbf{M} = \\mathrm{diag}(m_1, \\dots, m_n)$.\nThe kinetic energy $K$ is given by:\n$$K = \\frac{1}{2}\\mathbf{v}^{\\mathsf{T}}\\mathbf{M}\\mathbf{v}$$\nTo simplify this expression, we introduce mass-weighted velocities $\\mathbf{w} \\in \\mathbb{R}^{n}$ defined as:\n$$\\mathbf{w} = \\mathbf{M}^{1/2}\\mathbf{v}$$\nwhere $\\mathbf{M}^{1/2} = \\mathrm{diag}(\\sqrt{m_1}, \\dots, \\sqrt{m_n})$. Consequently, the Cartesian velocities can be expressed as $\\mathbf{v} = \\mathbf{M}^{-1/2}\\mathbf{w}$. Substituting this into the kinetic energy expression yields:\n$$K = \\frac{1}{2}(\\mathbf{M}^{-1/2}\\mathbf{w})^{\\mathsf{T}}\\mathbf{M}(\\mathbf{M}^{-1/2}\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^{\\mathsf{T}}(\\mathbf{M}^{-1/2})^{\\mathsf{T}}\\mathbf{M}\\mathbf{M}^{-1/2}\\mathbf{w}$$\nSince $\\mathbf{M}$ is diagonal, its square root and inverse square root are also diagonal, and thus symmetric. Therefore, $(\\mathbf{M}^{-1/2})^{\\mathsf{T}} = \\mathbf{M}^{-1/2}$. The expression simplifies to:\n$$K = \\frac{1}{2}\\mathbf{w}^{\\mathsf{T}}\\mathbf{M}^{-1/2}\\mathbf{M}\\mathbf{M}^{-1/2}\\mathbf{w} = \\frac{1}{2}\\mathbf{w}^{\\mathsf{T}}\\mathbf{I}\\mathbf{w} = \\frac{1}{2}\\mathbf{w}^{\\mathsf{T}}\\mathbf{w} = \\frac{1}{2}\\sum_{i=1}^{n} w_i^2$$\nThis transformation diagonalizes the kinetic energy expression, making each coordinate's contribution have a uniform weight of unity.\n\nThe system is subject to $k$ holonomic constraints $\\boldsymbol{\\Phi}(\\mathbf{q}) = \\mathbf{0}$, which, upon differentiation with respect to time, impose linear constraints on the velocities:\n$$\\mathbf{J}(\\mathbf{q})\\mathbf{v} = \\mathbf{0}$$\nwhere $\\mathbf{J}(\\mathbf{q}) = \\partial \\boldsymbol{\\Phi}/\\partial \\mathbf{q}$ is the $k \\times n$ constraint Jacobian matrix. Substituting $\\mathbf{v} = \\mathbf{M}^{-1/2}\\mathbf{w}$ into the velocity constraint equation gives:\n$$\\mathbf{J}\\mathbf{M}^{-1/2}\\mathbf{w} = \\mathbf{0}$$\nWe define the mass-weighted constraint operator $\\mathbf{A} = \\mathbf{J}\\mathbf{M}^{-1/2}$. The constraint equation for the mass-weighted velocities becomes:\n$$\\mathbf{A}\\mathbf{w} = \\mathbf{0}$$\nThis demonstrates that any physically admissible mass-weighted velocity vector $\\mathbf{w}$ must reside in the null space of the matrix $\\mathbf{A}$, denoted $\\mathrm{null}(\\mathbf{A})$. The dimension of this null space, $f_{\\mathrm{eff}} = \\dim(\\mathrm{null}(\\mathbf{A})) = n - \\mathrm{rank}(\\mathbf{A})$, represents the number of effective, unconstrained kinetic degrees of freedom of the system.\n\nLet $\\{\\mathbf{n}_1, \\dots, \\mathbf{n}_{f_{\\mathrm{eff}}}\\}$ be an orthonormal basis for $\\mathrm{null}(\\mathbf{A})$. We can form a matrix $\\mathbf{N} = [\\mathbf{n}_1|\\dots|\\mathbf{n}_{f_{\\mathrm{eff}}}]$ of size $n \\times f_{\\mathrm{eff}}$, whose columns are these basis vectors. By construction, $\\mathbf{A}\\mathbf{N} = \\mathbf{0}$ and $\\mathbf{N}^{\\mathsf{T}}\\mathbf{N} = \\mathbf{I}_{f_{\\mathrm{eff}}}$, where $\\mathbf{I}_{f_{\\mathrm{eff}}}$ is the $f_{\\mathrm{eff}} \\times f_{\\mathrm{eff}}}$ identity matrix. Any admissible vector $\\mathbf{w}$ can be uniquely expressed as a linear combination of these basis vectors:\n$$\\mathbf{w} = \\sum_{i=1}^{f_{\\mathrm{eff}}} a_i \\mathbf{n}_i = \\mathbf{N}\\mathbf{a}$$\nwhere $\\mathbf{a} = [a_1, \\dots, a_{f_{\\mathrm{eff}}}]^{\\mathsf{T}}$ is a vector of modal amplitudes.\n\nSubstituting this representation of $\\mathbf{w}$ back into the kinetic energy expression, we find:\n$$K = \\frac{1}{2}(\\mathbf{N}\\mathbf{a})^{\\mathsf{T}}(\\mathbf{N}\\mathbf{a}) = \\frac{1}{2}\\mathbf{a}^{\\mathsf{T}}\\mathbf{N}^{\\mathsf{T}}\\mathbf{N}\\mathbf{a} = \\frac{1}{2}\\mathbf{a}^{\\mathsf{T}}\\mathbf{I}_{f_{\\mathrm{eff}}}\\mathbf{a} = \\frac{1}{2}\\mathbf{a}^{\\mathsf{T}}\\mathbf{a}$$\nThis final result shows that the total kinetic energy is a sum of $f_{\\mathrm{eff}}$ independent quadratic terms:\n$$K = \\sum_{i=1}^{f_{\\mathrm{eff}}} \\frac{1}{2} a_i^2$$\nEach term $K_i = \\frac{1}{2}a_i^2$ can be interpreted as the kinetic energy of an independent dynamical mode.\n\n**Part 2: Microcanonical Temperature under Constraints**\n\nIn the microcanonical ensemble, for a system at a fixed configuration $\\mathbf{q}$, the total kinetic energy $K$ is constant. The accessible states of the system in the mass-weighted velocity space are confined to the intersection of the hypersphere defined by $\\mathbf{w}^{\\mathsf{T}}\\mathbf{w} = 2K$ and the linear subspace defined by $\\mathbf{A}\\mathbf{w} = \\mathbf{0}$. In the modal amplitude space, this corresponds to the surface of a hypersphere of dimension $f_{\\mathrm{eff}}$ defined by $\\mathbf{a}^{\\mathsf{T}}\\mathbf{a} = \\sum_{i=1}^{f_{\\mathrm{eff}}} a_i^2 = 2K$.\n\nThe equipartition theorem states that for a classical system in thermal equilibrium, the average energy associated with each independent quadratic degree of freedom in the Hamiltonian is $\\frac{1}{2}k_{\\mathrm{B}}T$. In our formulation, the kinetic energy is composed of $f_{\\mathrm{eff}}$ such quadratic terms, $\\frac{1}{2}a_i^2$. Therefore, the total kinetic energy $K$ is the sum of the average energies of these modes:\n$$K = \\sum_{i=1}^{f_{\\mathrm{eff}}} \\left\\langle \\frac{1}{2}a_i^2 \\right\\rangle = f_{\\mathrm{eff}} \\cdot \\left(\\frac{1}{2}k_{\\mathrm{B}}T\\right)$$\nHere, $T$ is the kinetic temperature, a standard measure used in molecular dynamics simulations as an estimator for the thermodynamic temperature. From this relationship, we derive the formula for the constrained microcanonical temperature:\n$$T = \\frac{2K}{f_{\\mathrm{eff}}k_{\\mathrm{B}}}$$\nThis formula is valid provided that $f_{\\mathrm{eff}}  0$. If $f_{\\mathrm{eff}} = 0$, the system has no kinetic degrees of freedom, implying $K=0$ and a temperature of absolute zero, $T=0\\,\\mathrm{K}$.\n\n**Part 3: Algorithmic Implementation**\n\nThe derivation directly informs the following algorithm:\n1.  For a given system (masses $\\mathbf{m}$ and constraint Jacobian $\\mathbf{J}$), construct the diagonal mass matrix $\\mathbf{M}$ and its inverse square root $\\mathbf{M}^{-1/2}$.\n2.  Form the mass-weighted constraint operator $\\mathbf{A} = \\mathbf{J}\\mathbf{M}^{-1/2}$.\n3.  Numerically determine the rank of $\\mathbf{A}$. The effective number of degrees of freedom is $f_{\\mathrm{eff}} = n - \\mathrm{rank}(\\mathbf{A})$, where $n$ is the number of particles.\n4.  Compute an orthonormal basis for the null space of $\\mathbf{A}$ using a Singular Value Decomposition. The columns of the resulting matrix $\\mathbf{N}$ are these basis vectors.\n5.  For a target kinetic energy $K_{\\text{target}}$, construct the modal amplitude vector $\\mathbf{a}$ by enforcing equipartition, i.e., setting each modal energy $K_i = \\frac{1}{2}a_i^2$ to the average value $K_{\\text{target}}/f_{\\mathrm{eff}}$. This yields $|a_i| = \\sqrt{2K_{\\text{target}}/f_{\\mathrm{eff}}}$ for all $i=1, \\dots, f_{\\mathrm{eff}}$. We choose the positive roots without loss of generality.\n6.  Calculate the mode-resolved kinetic energies $K_i$ and the maximum relative deviation $\\delta_{\\max}$ from the mean modal energy.\n7.  Calculate the temperature $T$ using the derived formula $T = 2K_{\\text{target}}/(f_{\\mathrm{eff}}k_{\\mathrm{B}})$.\n\nThis procedure allows for the systematic construction of constrained velocity fields with a specified kinetic energy and the evaluation of the system's kinetic temperature.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes constrained microcanonical temperature, effective degrees of freedom,\n    and energy equipartition deviation for several test cases.\n    \"\"\"\n    # Define the Boltzmann constant in SI units (J/K)\n    k_B = 1.380649e-23\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path with fully constrained chain)\n        {\n            \"m_vals\": np.array([1.66e-27, 3.32e-27, 4.98e-27]),\n            \"J_vals\": np.array([[-1, 1, 0], [0, -1, 1]]),\n            \"K_target\": 1.0e-20,\n        },\n        # Case 2 (partial constraint with remaining internal motion)\n        {\n            \"m_vals\": np.array([1.66e-27, 3.32e-27, 4.98e-27, 6.64e-27]),\n            \"J_vals\": np.array([[-1, 1, 0, 0]]),\n            \"K_target\": 3.0e-21,\n        },\n        # Case 3 (redundant constraints testing numerical rank detection)\n        {\n            \"m_vals\": np.array([1.66e-27, 3.32e-27, 4.98e-27]),\n            \"J_vals\": np.array([[-1, 1, 0], [-2, 2, 0]]),\n            \"K_target\": 2.5e-21,\n        },\n        # Case 4 (zero-energy boundary case)\n        {\n            \"m_vals\": np.array([1.00e-26, 2.00e-26]),\n            \"J_vals\": np.array([[-1, 1]]),\n            \"K_target\": 0.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        m_vals = case[\"m_vals\"]\n        J = case[\"J_vals\"]\n        K_target = case[\"K_target\"]\n\n        num_particles = len(m_vals)\n        \n        # Construct mass matrices\n        M_inv_sqrt = np.diag(1.0 / np.sqrt(m_vals))\n\n        # Construct mass-weighted constraint operator A\n        A = J @ M_inv_sqrt\n\n        # Compute effective number of degrees of freedom (f_eff)\n        # f_eff is the nullity of A, which is n - rank(A)\n        try:\n            rank_A = np.linalg.matrix_rank(A)\n        except np.linalg.LinAlgError:\n            # Handle cases where J might be an empty matrix for a free system\n            rank_A = 0\n            \n        f_eff = num_particles - rank_A\n\n        if f_eff == 0:\n            # No degrees of freedom, kinetic energy must be zero.\n            # Temperature is 0 K. Deviation is 0 as there are no modes.\n            T = 0.0\n            delta_max = 0.0\n            if K_target != 0.0:\n                # This case represents a physical inconsistency but we handle it gracefully.\n                # The problem setter guarantees this won't happen in the tests.\n                pass\n        else:\n            if K_target == 0.0:\n                T = 0.0\n                delta_max = 0.0\n            else:\n                # Compute temperature T\n                T = (2.0 * K_target) / (f_eff * k_B)\n                \n                # Construct the null space basis N from SVD of A\n                # A = U S Vh, where Vh = V.T\n                # The columns of V corresponding to zero singular values form the null space basis.\n                # These are the last f_eff rows of Vh.\n                _U, _s, Vh = np.linalg.svd(A)\n                N = Vh[rank_A:].T\n                \n                # Set modal amplitudes 'a' for equal energy partition\n                # K_i = 0.5 * a_i^2 = K_target / f_eff\n                # a_i = sqrt(2 * K_target / f_eff)\n                a_i_val = np.sqrt(2.0 * K_target / f_eff)\n                a = np.full(f_eff, a_i_val)\n\n                # Compute mode-resolved kinetic energies\n                K_modes = 0.5 * np.square(a)\n\n                # Compute the maximum relative deviation from equipartition\n                mean_modal_energy = K_target / f_eff\n                relative_errors = np.abs((K_modes - mean_modal_energy) / mean_modal_energy)\n                delta_max = np.max(relative_errors)\n\n        results.append([T, f_eff, delta_max])\n\n    # Final print statement in the exact required format.\n    # The format uses default float representation which is what repr() gives.\n    # Joining a list of lists requires this nested structure.\n    print(f\"[{','.join(map(repr, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}