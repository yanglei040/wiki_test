{
    "hands_on_practices": [
        {
            "introduction": "Before we can assess the performance of any parameter estimator, we need a theoretical benchmark. This practice introduces the Cramér–Rao Lower Bound (CRLB), a fundamental concept that establishes the minimum possible variance for any unbiased estimator. By deriving the Fisher Information for a Poisson model of defect nucleation—a common scenario in materials characterization—you will learn how to quantify the maximum information an experiment can yield about a parameter of interest .",
            "id": "3480528",
            "problem": "In a computational materials science study of thin-film reliability, suppose defects nucleate on the film surface according to a spatially uniform Poisson point process with rate parameter $\\lambda$ per unit area. A set of $m$ independent Scanning Electron Microscope (SEM) micrographs with areas $a_{1},a_{2},\\dots,a_{m}$ are collected, and on each micrograph $i$ the observed defect count is $x_{i}$. Under the Poisson point process assumption, the count $x_{i}$ is modeled as a Poisson random variable with mean $\\lambda a_{i}$.\n\nStarting from the fundamental definitions of the Poisson probability mass function, the likelihood for independent observations, the log-likelihood, and Fisher information as the expected negative second derivative of the log-likelihood with respect to the parameter, derive an analytic expression for the Fisher information $I(\\lambda)$ about $\\lambda$ contained in the $m$ observations $\\{x_{i}\\}_{i=1}^{m}$ with known areas $\\{a_{i}\\}_{i=1}^{m}$. Then, using the definition of the Cramér–Rao Lower Bound (CRLB), derive a closed-form expression for the minimum achievable variance of any unbiased estimator of $\\lambda$ under this model.\n\nExpress your final results as analytic expressions. No rounding is required, and no units should be included in your final expressions.",
            "solution": "The user wants me to solve this problem.\n\n### Step 1: Extract Givens\n- A spatially uniform Poisson point process governs defect nucleation with rate parameter $\\lambda$ per unit area.\n- A set of $m$ independent Scanning Electron Microscope (SEM) micrographs are collected.\n- The areas of the micrographs are $a_{1}, a_{2}, \\dots, a_{m}$.\n- The observed defect count on micrograph $i$ is $x_{i}$.\n- The count $x_{i}$ is a Poisson random variable with mean $\\lambda a_{i}$.\n- Derive the Fisher information $I(\\lambda)$ for the observations $\\{x_{i}\\}_{i=1}^{m}$.\n- Derive the Cramér–Rao Lower Bound (CRLB) for the minimum achievable variance of any unbiased estimator of $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. The use of a Poisson point process to model defects is a standard and well-established method in materials science and related fields. The statistical model is clearly defined: $m$ independent observations from Poisson distributions with means proportional to known areas. The questions asked—derivation of Fisher information and the Cramér–Rao Lower Bound—are standard tasks in statistical inference and are well-posed. The problem is self-contained, objective, and does not violate any fundamental principles. There are no contradictions, ambiguities, or missing information.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution will be provided.\n\nThe derivation proceeds from the fundamental definitions as required.\n\nFirst, we define the probability mass function (PMF) for a single observation $x_i$. The problem states that $x_i$ follows a Poisson distribution with mean $\\mu_i = \\lambda a_i$. The PMF for a Poisson random variable $X$ with mean $\\mu$ is given by $P(X=k) = \\frac{\\exp(-\\mu)\\mu^k}{k!}$. Therefore, for our observation $x_i$, the PMF is:\n$$ P(X_i = x_i | \\lambda) = \\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!} $$\nThe parameter we are interested in is $\\lambda$, and the areas $a_i$ are known constants.\n\nNext, we construct the likelihood function $L(\\lambda)$ for the set of $m$ independent observations $\\{x_i\\}_{i=1}^{m}$. Since the observations are independent, the total likelihood is the product of the individual probabilities:\n$$ L(\\lambda; \\{x_i\\}) = \\prod_{i=1}^{m} P(X_i = x_i | \\lambda) = \\prod_{i=1}^{m} \\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!} $$\n\nFor mathematical convenience, we work with the log-likelihood function, $\\ell(\\lambda)$, which is the natural logarithm of the likelihood function:\n$$ \\ell(\\lambda) = \\ln(L(\\lambda)) = \\ln\\left(\\prod_{i=1}^{m} \\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!}\\right) $$\nUsing the properties of logarithms, the product becomes a sum:\n$$ \\ell(\\lambda) = \\sum_{i=1}^{m} \\ln\\left(\\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!}\\right) = \\sum_{i=1}^{m} \\left( \\ln(\\exp(-\\lambda a_i)) + \\ln((\\lambda a_i)^{x_i}) - \\ln(x_i!) \\right) $$\n$$ \\ell(\\lambda) = \\sum_{i=1}^{m} \\left( -\\lambda a_i + x_i \\ln(\\lambda a_i) - \\ln(x_i!) \\right) $$\nWe can expand the term $\\ln(\\lambda a_i)$ as $\\ln(\\lambda) + \\ln(a_i)$:\n$$ \\ell(\\lambda) = \\sum_{i=1}^{m} \\left( -\\lambda a_i + x_i (\\ln(\\lambda) + \\ln(a_i)) - \\ln(x_i!) \\right) $$\nSeparating the terms with respect to the summation:\n$$ \\ell(\\lambda) = -\\lambda \\sum_{i=1}^{m} a_i + \\ln(\\lambda) \\sum_{i=1}^{m} x_i + \\sum_{i=1}^{m} x_i \\ln(a_i) - \\sum_{i=1}^{m} \\ln(x_i!) $$\n\nTo find the Fisher information, we must compute the second derivative of the log-likelihood with respect to $\\lambda$. First, we compute the first derivative (the score function):\n$$ \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left( -\\lambda \\sum_{i=1}^{m} a_i + \\ln(\\lambda) \\sum_{i=1}^{m} x_i + \\sum_{i=1}^{m} x_i \\ln(a_i) - \\sum_{i=1}^{m} \\ln(x_i!) \\right) $$\nThe last two terms do not depend on $\\lambda$, so their derivatives are zero.\n$$ \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = -\\sum_{i=1}^{m} a_i + \\frac{1}{\\lambda} \\sum_{i=1}^{m} x_i $$\n\nNext, we compute the second derivative:\n$$ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = \\frac{\\partial}{\\partial \\lambda} \\left( -\\sum_{i=1}^{m} a_i + \\frac{1}{\\lambda} \\sum_{i=1}^{m} x_i \\right) $$\nThe first term is a constant with respect to $\\lambda$.\n$$ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} \\sum_{i=1}^{m} x_i $$\n\nThe Fisher information $I(\\lambda)$ is defined as the negative of the expected value of the second derivative of the log-likelihood function:\n$$ I(\\lambda) = -E\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right] $$\nSubstituting our expression for the second derivative:\n$$ I(\\lambda) = -E\\left[-\\frac{1}{\\lambda^2} \\sum_{i=1}^{m} x_i\\right] = \\frac{1}{\\lambda^2} E\\left[\\sum_{i=1}^{m} x_i\\right] $$\nBy the linearity of expectation, $E\\left[\\sum_{i=1}^{m} x_i\\right] = \\sum_{i=1}^{m} E[x_i]$. The problem states that $x_i$ is a Poisson random variable with mean $\\lambda a_i$, so $E[x_i] = \\lambda a_i$.\n$$ \\sum_{i=1}^{m} E[x_i] = \\sum_{i=1}^{m} \\lambda a_i = \\lambda \\sum_{i=1}^{m} a_i $$\nSubstituting this back into the expression for $I(\\lambda)$:\n$$ I(\\lambda) = \\frac{1}{\\lambda^2} \\left( \\lambda \\sum_{i=1}^{m} a_i \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^{m} a_i $$\nThis is the analytic expression for the Fisher information.\n\nFinally, we derive the Cramér–Rao Lower Bound (CRLB). The CRLB states that for any unbiased estimator $\\hat{\\lambda}$ of the parameter $\\lambda$, its variance must satisfy the inequality:\n$$ \\mathrm{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)} $$\nThe minimum achievable variance for any unbiased estimator is therefore the inverse of the Fisher information.\n$$ \\mathrm{CRLB} = \\frac{1}{I(\\lambda)} = \\frac{1}{\\frac{1}{\\lambda} \\sum_{i=1}^{m} a_i} = \\frac{\\lambda}{\\sum_{i=1}^{m} a_i} $$\nThis is the closed-form expression for the minimum achievable variance of any unbiased estimator of $\\lambda$.\n\nThe two results are the Fisher information, $I(\\lambda) = \\frac{\\sum_{i=1}^{m} a_i}{\\lambda}$, and the Cramér–Rao Lower Bound, $\\frac{\\lambda}{\\sum_{i=1}^{m} a_i}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^{m} a_i}{\\lambda} & \\frac{\\lambda}{\\sum_{i=1}^{m} a_i}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the CRLB provides a goal, many practical models present challenges that can make even reaching this bound impossible. This exercise explores the crucial concept of parameter identifiability within the context of a Gaussian Mixture Model, often used to describe multimodal microstructures like grain size distributions. Through this practice, you will grapple with subtle but critical issues like label switching and the conditions required to ensure that the parameters you are trying to estimate are, in fact, uniquely defined by the model and data .",
            "id": "3480503",
            "problem": "A polycrystalline alloy produced under two distinct thermal histories exhibits a bimodal grain size distribution. Let $d_1,\\dots,d_n$ denote measured grain diameters for $n$ samples, and define $x_i = \\log d_i$ to stabilize variance. A $2$-component Gaussian mixture is used to model the population distribution of $x_i$, with independent and identically distributed (IID) samples $x_1,\\dots,x_n$ from a density\n$$\nf(x;\\theta) = \\pi\\,\\phi\\!\\left(x;\\mu_1,\\sigma_1^2\\right) + \\left(1-\\pi\\right)\\,\\phi\\!\\left(x;\\mu_2,\\sigma_2^2\\right),\n$$\nwhere $\\phi\\!\\left(x;\\mu,\\sigma^2\\right)$ is the normal density with mean $\\mu$ and variance $\\sigma^2$, and $\\theta = \\left(\\pi,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\right)$ with $0<\\pi<1$, $\\sigma_1^2>0$, $\\sigma_2^2>0$. Let $\\hat{\\theta}_n$ denote the maximum likelihood estimator (MLE) obtained by maximizing the log-likelihood\n$$\n\\ell_n(\\theta) = \\sum_{i=1}^n \\log\\!\\left[\\pi\\,\\phi\\!\\left(x_i;\\mu_1,\\sigma_1^2\\right) + \\left(1-\\pi\\right)\\,\\phi\\!\\left(x_i;\\mu_2,\\sigma_2^2\\right)\\right]\n$$\nover an admissible parameter space.\n\nSelect all statements that correctly characterize identifiability of $\\theta$ from $f(x;\\theta)$ and state conditions ensuring uniqueness of $\\hat{\\theta}_n$ in this computational materials science setting.\n\nA. The parameter vector $\\theta = \\left(\\pi,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\right)$ is identifiable from $f$ up to permutation of component labels, provided $0<\\pi<1$ and $\\left(\\mu_1,\\sigma_1^2\\right) \\neq \\left(\\mu_2,\\sigma_2^2\\right)$.\n\nB. The maximum likelihood estimator $\\hat{\\theta}_n$ is always unique in this model because the log-likelihood $\\ell_n(\\theta)$ is strictly concave in $\\theta$.\n\nC. Imposing the single ordering constraint $\\mu_1 < \\mu_2$ alone suffices to ensure a unique global maximizer of $\\ell_n(\\theta)$ for any sample size $n$.\n\nD. If we constrain $\\mu_1 < \\mu_2$, $\\sigma_k^2 \\ge \\sigma_{\\min}^2 > 0$, and $\\epsilon \\le \\pi \\le 1-\\epsilon$ for fixed $\\sigma_{\\min}>0$ and $\\epsilon \\in \\left(0,\\frac{1}{2}\\right)$, then for well-separated true components and large $n$, the constrained log-likelihood is locally strictly concave near the true parameter and has a unique maximizer in that neighborhood with high probability.\n\nE. When $\\left(\\mu_1,\\sigma_1^2\\right) = \\left(\\mu_2,\\sigma_2^2\\right)$, the mixture reduces to a single Gaussian $N\\!\\left(\\mu_1,\\sigma_1^2\\right)$, and $\\pi$ becomes non-identifiable.",
            "solution": "The problem statement poses a question regarding the parameter identifiability and the uniqueness of the maximum likelihood estimator (MLE) for a $2$-component Gaussian mixture model (GMM). This is a classical problem in statistical inference and is directly applicable to fields like materials science where bimodal distributions are encountered.\n\n### Step 1: Extract Givens\n- **Data:** $d_1, \\dots, d_n$ are measured grain diameters.\n- **Transformation:** $x_i = \\log d_i$.\n- **Model:** $x_1, \\dots, x_n$ are independent and identically distributed (IID) samples.\n- **Population Density:** $f(x;\\theta) = \\pi\\,\\phi\\!\\left(x;\\mu_1,\\sigma_1^2\\right) + \\left(1-\\pi\\right)\\,\\phi\\!\\left(x;\\mu_2,\\sigma_2^2\\right)$, where $\\phi\\!\\left(x;\\mu,\\sigma^2\\right)$ is the normal probability density function.\n- **Parameter Vector:** $\\theta = \\left(\\pi,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\right)$.\n- **Parameter Space Constraints (initial):** $0<\\pi<1$, $\\sigma_1^2>0$, $\\sigma_2^2>0$.\n- **Estimator:** The Maximum Likelihood Estimator (MLE), $\\hat{\\theta}_n$, is obtained by maximizing the log-likelihood function.\n- **Log-Likelihood Function:** $\\ell_n(\\theta) = \\sum_{i=1}^n \\log\\!\\left[\\pi\\,\\phi\\!\\left(x_i;\\mu_1,\\sigma_1^2\\right) + \\left(1-\\pi\\right)\\,\\phi\\!\\left(x_i;\\mu_2,\\sigma_2^2\\right)\\right]$.\n- **Objective:** Evaluate the provided statements about the identifiability of $\\theta$ and the conditions for the uniqueness of $\\hat{\\theta}_n$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound and well-posed.\n- **Scientific Grounding:** The use of logarithmic transformation to stabilize variance and the modeling of a bimodal distribution with a GMM are standard, well-established techniques in statistical data analysis, commonly applied in materials science for characterizing microstructures like grain size distributions.\n- **Well-Posedness:** The problem is a standard theoretical question in statistics concerning the properties of a widely used model. It asks about identifiability and uniqueness of estimators, which are fundamental concepts. The problem does not contain ambiguities or contradictions. The log-likelihood function is correctly stated for an IID sample from the given mixture density.\n- **Objectivity:** The problem is expressed using precise mathematical and statistical language, free from subjective or non-formalizable content.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with the analysis of each option.\n\n### Analysis of the Statements\n\nA parameter $\\theta$ is identifiable if distinct values of $\\theta$ correspond to distinct probability distributions. That is, if $\\theta_A \\neq \\theta_B$, then $f(x;\\theta_A) \\neq f(x;\\theta_B)$ for some $x$. The MLE aims to find the parameter value that best explains the observed data, so identifiability is a crucial prerequisite for meaningful estimation.\n\n**A. The parameter vector $\\theta = \\left(\\pi,\\mu_1,\\sigma_1^2,\\mu_2,\\sigma_2^2\\right)$ is identifiable from $f$ up to permutation of component labels, provided $0<\\pi<1$ and $\\left(\\mu_1,\\sigma_1^2\\right) \\neq \\left(\\mu_2,\\sigma_2^2\\right)$.**\n\nLet $\\theta_A = \\left(\\pi, \\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2\\right)$. Consider a new parameter vector $\\theta_B$ obtained by permuting the component labels: $\\theta_B = \\left(1-\\pi, \\mu_2, \\sigma_2^2, \\mu_1, \\sigma_1^2\\right)$. The density corresponding to $\\theta_B$ is:\n$$f(x;\\theta_B) = (1-\\pi)\\,\\phi\\!\\left(x;\\mu_2,\\sigma_2^2\\right) + \\left(1-(1-\\pi)\\right)\\,\\phi\\!\\left(x;\\mu_1,\\sigma_1^2\\right) = \\pi\\,\\phi\\!\\left(x;\\mu_1,\\sigma_1^2\\right) + (1-\\pi)\\,\\phi\\!\\left(x;\\mu_2,\\sigma_2^2\\right) = f(x;\\theta_A)$$\nIf $\\left(\\mu_1,\\sigma_1^2\\right) \\neq \\left(\\mu_2,\\sigma_2^2\\right)$ and $\\pi \\neq 0.5$, then $\\theta_A \\neq \\theta_B$, but they generate the same density function. This shows that the parameter vector $\\theta$ is not strictly identifiable. This phenomenon is known as \"label switching\". The statement correctly qualifies this by stating it is \"identifiable up to permutation of component labels\". This is a standard result for finite mixture models. The condition $\\left(\\mu_1,\\sigma_1^2\\right) \\neq \\left(\\mu_2,\\sigma_2^2\\right)$ is necessary, as covered in option E. The condition $0 < \\pi < 1$ is also necessary, because if $\\pi=0$ or $\\pi=1$, the mixture collapses to a single component, and the parameters of the other component become non-identifiable. Thus, the statement is a correct and precise characterization of identifiability for this model.\n**Verdict: Correct.**\n\n**B. The maximum likelihood estimator $\\hat{\\theta}_n$ is always unique in this model because the log-likelihood $\\ell_n(\\theta)$ is strictly concave in $\\theta$.**\n\nThe log-likelihood is $\\ell_n(\\theta) = \\sum_{i=1}^n \\log(f(x_i;\\theta))$. For $\\ell_n(\\theta)$ to be strictly concave, its Hessian matrix must be negative definite over the parameter space. The log-likelihood for a mixture model is well-known to be non-concave. It can have multiple local maxima, which is a primary reason that iterative algorithms like Expectation-Maximization (EM) are used for fitting and their results can depend on initialization. The statement's reason, strict concavity, is factually incorrect. Furthermore, as established in A, due to label switching, if $\\hat{\\theta} = \\left(\\hat{\\pi}, \\hat{\\mu}_1, \\hat{\\sigma}_1^2, \\hat{\\mu}_2, \\hat{\\sigma}_2^2\\right)$ is a maximizer, then so is $\\hat{\\theta}' = \\left(1-\\hat{\\pi}, \\hat{\\mu}_2, \\hat{\\sigma}_2^2, \\hat{\\mu}_1, \\hat{\\sigma}_1^2\\right)$. If these two are distinct, the maximizer is not unique. Therefore, the claim that the MLE is \"always unique\" is also false.\n**Verdict: Incorrect.**\n\n**C. Imposing the single ordering constraint $\\mu_1 < \\mu_2$ alone suffices to ensure a unique global maximizer of $\\ell_n(\\theta)$ for any sample size $n$.**\n\nThe constraint $\\mu_1 < \\mu_2$ is a standard method to resolve the non-uniqueness caused by label switching. From any pair of label-swapped solutions, this constraint would select only one. However, this single constraint is not sufficient to guarantee a unique global maximizer for two critical reasons:\n1.  **Unboundedness of the likelihood function:** The log-likelihood function $\\ell_n(\\theta)$ is unbounded on the parameter space where variances can approach zero. Consider a parameter set where $\\mu_1 = x_i$ for one of the data points $x_i$, and let $\\sigma_1^2 \\to 0$. The term $\\phi(x_i; \\mu_1, \\sigma_1^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_1^2}} \\exp(0)$ will approach infinity. Consequently, $\\log[\\pi\\phi(x_i; \\mu_1, \\sigma_1^2) + \\dots] \\to \\infty$, and thus $\\ell_n(\\theta) \\to \\infty$. A function that is unbounded from above does not have a global maximum. The constraint $\\mu_1 < \\mu_2$ does not prevent this pathological behavior.\n2.  **Spurious local maxima:** Even if the unboundedness issue is resolved (e.g., by constraining variances to be bounded below), the log-likelihood surface can still possess multiple, distinct local maxima that are not related by label switching. The existence and number of these spurious modes can depend on the data, and for finite sample sizes $n$, there is no guarantee that only one maximum exists.\nThe claim that this single constraint \"suffices to ensure a unique global maximizer\" for \"any sample size $n$\" is incorrect.\n**Verdict: Incorrect.**\n\n**D. If we constrain $\\mu_1 < \\mu_2$, $\\sigma_k^2 \\ge \\sigma_{\\min}^2 > 0$, and $\\epsilon \\le \\pi \\le 1-\\epsilon$ for fixed $\\sigma_{\\min}>0$ and $\\epsilon \\in \\left(0,\\frac{1}{2}\\right)$, then for well-separated true components and large $n$, the constrained log-likelihood is locally strictly concave near the true parameter and has a unique maximizer in that neighborhood with high probability.**\n\nThis statement presents a comprehensive set of conditions based on asymptotic MLE theory, which addresses the problems mentioned above.\n- The constraints $\\mu_1 < \\mu_2$, $\\sigma_k^2 \\ge \\sigma_{\\min}^2 > 0$, and $\\epsilon \\le \\pi \\le 1-\\epsilon$ define a restricted parameter space. The constraint on $\\sigma_k^2$ prevents the likelihood from being unbounded. The constraint on $\\pi$ keeps the problem away from the boundary where identifiability is lost. The constraint on means resolves label switching.\n- Under these conditions, and assuming the true parameter vector $\\theta_0$ lies in the interior of this constrained space, standard asymptotic theory for MLEs applies. For a large sample size ($n \\to \\infty$), the log-likelihood function, in a small neighborhood of $\\theta_0$, is well-approximated by a quadratic function. The negative of the Hessian of the scaled log-likelihood converges to the Fisher information matrix. If this matrix is positive definite (which is ensured if components are \"well-separated\"), the log-likelihood is locally strictly concave around $\\theta_0$.\n- A major result in the theory of mixture models is that under these conditions, there exists a sequence of solutions to the likelihood equations that is consistent (converges to $\\theta_0$) and asymptotically normal. This solution corresponds to a local maximum near $\\theta_0$. With probability approaching $1$ as $n \\to \\infty$, this is the unique maximizer within that local neighborhood. This statement accurately reflects the established statistical theory for obtaining a well-behaved MLE for mixture models.\n**Verdict: Correct.**\n\n**E. When $\\left(\\mu_1,\\sigma_1^2\\right) = \\left(\\mu_2,\\sigma_2^2\\right)$, the mixture reduces to a single Gaussian $N\\!\\left(\\mu_1,\\sigma_1^2\\right)$, and $\\pi$ becomes non-identifiable.**\n\nLet us substitute the condition $\\left(\\mu_1,\\sigma_1^2\\right) = \\left(\\mu_2,\\sigma_2^2\\right) = (\\mu,\\sigma^2)$ into the mixture density function:\n$$ f(x;\\theta) = \\pi\\,\\phi\\!\\left(x;\\mu,\\sigma^2\\right) + \\left(1-\\pi\\right)\\,\\phi\\!\\left(x;\\mu,\\sigma^2\\right) $$\nFactoring out the common normal density term:\n$$ f(x;\\theta) = \\left[\\pi + (1-\\pi)\\right]\\,\\phi\\!\\left(x;\\mu,\\sigma^2\\right) = 1 \\cdot \\phi\\!\\left(x;\\mu,\\sigma^2\\right) = \\phi\\!\\left(x;\\mu,\\sigma^2\\right) $$\nThe resulting density is that of a single Gaussian distribution $N\\!\\left(\\mu,\\sigma^2\\right)$. The parameter $\\pi$, the mixing proportion, has canceled out of the expression for the density function. This means that the observed data's distribution does not depend on $\\pi$. Consequently, it is impossible to determine the value of $\\pi$ from any amount of data drawn from this distribution. This is the definition of non-identifiability. The statement is a correct description of the loss of identifiability at this specific submanifold of the parameter space.\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "Building on the concepts of Fisher information and identifiability, this final practice provides a computational framework for assessing and improving model calibration in a real-world setting. You will implement tools to quantify parameter identifiability for a constitutive model by analyzing the Fisher Information Matrix and profile likelihood under different experimental designs. This exercise demonstrates how statistical inference is not just for post-processing data, but is a powerful tool for optimal experimental design, helping you decide which measurements will be most informative for constraining your model parameters .",
            "id": "3480468",
            "problem": "You are given a one-dimensional phenomenological hardening law widely used in computational materials science to describe monotonic uniaxial plastic flow beyond initial yield. The constitutive relation is expressed as a mapping from strain to stress, where the plastic strain is approximated by the total strain under monotonic loading. The model is\n$$\n\\sigma(\\epsilon;\\theta) \\;=\\; \\sigma_s \\;-\\; \\big(\\sigma_s - \\sigma_0\\big)\\,e^{-b\\,\\epsilon},\n$$\nwhere $\\epsilon$ denotes strain (dimensionless), $\\sigma$ denotes Cauchy stress (in megapascals), and the parameter vector is $\\theta = (\\sigma_0,\\sigma_s,b)$ with $\\sigma_0$ the initial flow stress (in megapascals), $\\sigma_s$ the saturation stress (in megapascals), and $b$ (dimensionless) the saturation rate. Measurements of stress are corrupted by additive Gaussian noise so that the observed stresses satisfy\n$$\ny_i \\;=\\; \\sigma(\\epsilon_i;\\theta_{\\text{true}}) \\;+\\; \\eta_i,\\quad \\eta_i \\sim \\mathcal{N}(0,\\sigma_n^2),\n$$\nwhere $\\sigma_n$ is the standard deviation of the measurement noise (in megapascals). For identifiability assessment using probability and statistical inference basics, use the profile likelihood curvature for a single parameter and optimal experimental design criteria for the full parameter vector. Specifically:\n\n- Adopt the Gaussian likelihood and its log-likelihood, and define the Fisher Information Matrix (FIM) as the expected negative Hessian of the log-likelihood. For the above model under independent, identically distributed Gaussian noise, the FIM at $\\theta_{\\text{true}}$ reduces to\n$$\n\\mathcal{I}(\\theta_{\\text{true}}) \\;=\\; \\frac{1}{\\sigma_n^2}\\,J(\\theta_{\\text{true}})^\\top J(\\theta_{\\text{true}}),\n$$\nwhere $J(\\theta_{\\text{true}})$ is the Jacobian matrix of $\\sigma(\\epsilon;\\theta)$ with respect to $\\theta$ evaluated at $\\theta_{\\text{true}}$ and at the design strains $\\{\\epsilon_i\\}$.\n\n- To ensure dimensionless identifiability metrics, use parameter scaling. Define a diagonal scaling matrix $S=\\mathrm{diag}(1/s_{\\sigma_0},1/s_{\\sigma_s},1/s_b)$ and form the scaled Jacobian $J_s = J(\\theta_{\\text{true}})\\,S$. The scaled Fisher Information Matrix is\n$$\n\\mathcal{I}_s(\\theta_{\\text{true}}) \\;=\\; \\frac{1}{\\sigma_n^2}\\,J_s^\\top J_s.\n$$\nFrom $\\mathcal{I}_s$, compute the $D$-optimality criterion $D = \\det\\big(\\mathcal{I}_s\\big)$ and the $E$-optimality criterion $E = \\lambda_{\\min}\\big(\\mathcal{I}_s\\big)$, where $\\lambda_{\\min}$ denotes the smallest eigenvalue.\n\n- For the profile likelihood curvature, treat $b$ as the parameter of interest and $\\sigma_0,\\sigma_s$ as nuisance parameters. Given synthetic noise-free data $y_i = \\sigma(\\epsilon_i;\\theta_{\\text{true}})$, define the profile sum of squared errors for a fixed $b$ as\n$$\n\\mathrm{SSE}(b) \\;=\\; \\min_{\\sigma_0,\\sigma_s}\\;\\sum_{i}\\Big(\\sigma(\\epsilon_i;\\sigma_0,\\sigma_s,b) - y_i\\Big)^2,\n$$\nand the corresponding profile log-likelihood as\n$$\n\\ell_p(b) \\;=\\; -\\frac{1}{2\\,\\sigma_n^2}\\,\\mathrm{SSE}(b).\n$$\nCompute the curvature of the profile log-likelihood at $b=b_{\\text{true}}$ via a second-order central finite difference\n$$\n\\kappa_b \\;=\\; -\\frac{\\ell_p(b_{\\text{true}}+h) - 2\\,\\ell_p(b_{\\text{true}}) + \\ell_p(b_{\\text{true}}-h)}{h^2},\n$$\nwith a small step $h>0$. This curvature is a dimensionless local identifiability measure for $b$ under the given design.\n\nUse the following fundamental base for the derivation and algorithmic design:\n- The definition of the Gaussian likelihood and its log-likelihood for independent, identically distributed observations.\n- The Fisher Information Matrix for Gaussian noise models and its relation to the Jacobian of the mean function.\n- The definition of the profile likelihood by optimizing nuisance parameters for each fixed value of the parameter of interest.\n- Ordinary Least Squares (OLS) as the solution of linear regression in the nuisance parameters when the model is linear in those parameters for fixed $b$.\n\nTest Suite and Required Numerical Values:\n- True parameters $\\theta_{\\text{true}} = (\\sigma_0,\\sigma_s,b) = (\\,300.0,\\,600.0,\\,12.0\\,)$, where the stresses are in megapascals and $b$ is dimensionless.\n- Noise standard deviation $\\sigma_n = 5.0$ (in megapascals).\n- Parameter scaling $s_{\\sigma_0} = 300.0$, $s_{\\sigma_s} = 600.0$, $s_b = 12.0$ so that the scaled Fisher Information Matrix is dimensionless.\n- Finite difference step $h=0.5$ (dimensionless) for the profile curvature in $b$.\n- Loading path designs (strain arrays, dimensionless):\n  1. Limited early hardening: $\\epsilon = [\\,0.0000,\\;0.0050,\\;0.0100,\\;0.0150,\\;0.0200\\,]$.\n  2. Mid-range hardening: $\\epsilon = [\\,0.0000,\\;0.0200,\\;0.0500,\\;0.0800,\\;0.1000\\,]$.\n  3. Wide-range saturation: $\\epsilon = [\\,0.0000,\\;0.0200,\\;0.0500,\\;0.1000,\\;0.2000\\,]$.\n\nTasks for each design in the test suite:\n- Compute the scaled Fisher Information Matrix $\\mathcal{I}_s(\\theta_{\\text{true}})$, the $D$-optimality $D$, and the $E$-optimality $E$.\n- Compute the profile log-likelihood curvature $\\kappa_b$ at $b=12.0$ using $h=0.5$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three designs as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\,D_1,\\,E_1,\\,\\kappa_{b,1},\\,D_2,\\,E_2,\\,\\kappa_{b,2},\\,D_3,\\,E_3,\\,\\kappa_{b,3}\\,],\n$$\nwhere the subscripts indicate the design number ($1$, $2$, $3$). All outputs must be floating-point numbers with no units.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in established principles of continuum mechanics and statistical inference, is mathematically well-posed with all necessary data and conditions provided, and is expressed in objective, unambiguous language. The tasks are clearly defined and lead to a unique, verifiable solution. We may therefore proceed with the derivation and computational implementation.\n\nThe solution requires the computation of two sets of identifiability metrics for three different experimental designs (strain paths). The first set, derived from the Fisher Information Matrix (FIM), assesses the overall identifiability of the parameter vector $\\theta = (\\sigma_0, \\sigma_s, b)$. The second, the profile likelihood curvature, assesses the local identifiability of a single parameter, $b$, while accounting for the effect of the nuisance parameters $\\sigma_0$ and $\\sigma_s$.\n\nThe phenomenological hardening model is given by:\n$$\n\\sigma(\\epsilon;\\theta) = \\sigma_s - (\\sigma_s - \\sigma_0)e^{-b\\epsilon}\n$$\nThe true parameters are $\\theta_{\\text{true}} = (\\sigma_{0,\\text{true}}, \\sigma_{s,\\text{true}}, b_{\\text{true}}) = (300.0, 600.0, 12.0)$. The measurement noise standard deviation is $\\sigma_n = 5.0$.\n\nFirst, we address the calculation of the scaled Fisher Information Matrix, $\\mathcal{I}_s$, and the associated optimality criteria, $D$ and $E$. The FIM is a function of the Jacobian of the model with respect to the parameters. The Jacobian matrix, $J$, has elements $J_{ij} = \\frac{\\partial \\sigma(\\epsilon_i;\\theta)}{\\partial \\theta_j}$. The partial derivatives of $\\sigma$ with respect to the parameters $\\theta = (\\sigma_0, \\sigma_s, b)$ are:\n$$\n\\frac{\\partial \\sigma}{\\partial \\sigma_0} = e^{-b\\epsilon}\n$$\n$$\n\\frac{\\partial \\sigma}{\\partial \\sigma_s} = 1 - e^{-b\\epsilon}\n$$\n$$\n\\frac{\\partial \\sigma}{\\partial b} = (\\sigma_s - \\sigma_0)\\epsilon e^{-b\\epsilon}\n$$\nFor each experimental design, an $N \\times 3$ Jacobian matrix $J(\\theta_{\\text{true}})$ is constructed, where $N$ is the number of strain points in the design. The $i$-th row of $J$ consists of the three partial derivatives evaluated at strain $\\epsilon_i$ and the true parameters $\\theta_{\\text{true}}$.\n\nTo ensure the resulting metrics are dimensionless, we use parameter scaling. The scaling matrix is $S = \\mathrm{diag}(1/s_{\\sigma_0}, 1/s_{\\sigma_s}, 1/s_b)$, with scaling factors $s_{\\sigma_0} = 300.0$, $s_{\\sigma_s} = 600.0$, and $s_b = 12.0$. The scaled Jacobian is $J_s = J(\\theta_{\\text{true}}) S$.\n\nThe scaled Fisher Information Matrix is then computed as:\n$$\n\\mathcal{I}_s(\\theta_{\\text{true}}) = \\frac{1}{\\sigma_n^2} J_s^\\top J_s\n$$\nFrom $\\mathcal{I}_s$, we calculate two optimality criteria:\n1.  $D$-optimality: $D = \\det(\\mathcal{I}_s)$, which relates to the volume of the confidence ellipsoid for the parameters. A larger determinant implies better overall parameter identifiability.\n2.  $E$-optimality: $E = \\lambda_{\\min}(\\mathcal{I}_s)$, the smallest eigenvalue of $\\mathcal{I}_s$. This criterion relates to the worst-case variance of a parameter estimate and guards against marginal non-identifiability. A larger smallest eigenvalue is preferred.\n\nSecond, we compute the profile likelihood curvature $\\kappa_b$. This requires calculating the profile log-likelihood $\\ell_p(b)$. For a fixed value of $b$, we must find the values of $\\sigma_0$ and $\\sigma_s$ that minimize the sum of squared errors (SSE) between the model prediction and the data. The data, $y$, are specified to be noise-free, i.e., $y_i = \\sigma(\\epsilon_i; \\theta_{\\text{true}})$.\nThe model can be rewritten to highlight its linearity in $\\sigma_0$ and $\\sigma_s$:\n$$\n\\sigma(\\epsilon_i; \\sigma_0, \\sigma_s, b) = \\sigma_0 e^{-b\\epsilon_i} + \\sigma_s (1 - e^{-b\\epsilon_i})\n$$\nThis is a linear regression problem of the form $\\mathbf{y} = X\\boldsymbol{\\beta}$, where $\\mathbf{y}$ is the vector of stress data, $\\boldsymbol{\\beta} = [\\sigma_0, \\sigma_s]^\\top$ is the vector of nuisance parameters, and $X$ is the design matrix with rows $[e^{-b\\epsilon_i}, 1 - e^{-b\\epsilon_i}]$. The Ordinary Least Squares (OLS) solution for $\\boldsymbol{\\beta}$ is $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$.\n\nWith the optimal $\\hat{\\boldsymbol{\\beta}}(b) = [\\hat{\\sigma}_0(b), \\hat{\\sigma}_s(b)]^\\top$ found for a given $b$, the profile sum of squared errors is:\n$$\n\\mathrm{SSE}(b) = \\sum_{i} \\left( y_i - \\sigma(\\epsilon_i; \\hat{\\sigma}_0(b), \\hat{\\sigma}_s(b), b) \\right)^2\n$$\nThe profile log-likelihood is then $\\ell_p(b) = -\\frac{1}{2\\sigma_n^2} \\mathrm{SSE}(b)$. We note that for $b = b_{\\text{true}}$, the OLS solution will be $\\hat{\\sigma}_0 = \\sigma_{0,\\text{true}}$ and $\\hat{\\sigma}_s = \\sigma_{s,\\text{true}}$, since the data are noise-free. This results in $\\mathrm{SSE}(b_{\\text{true}}) = 0$ and thus $\\ell_p(b_{\\text{true}}) = 0$.\n\nThe curvature $\\kappa_b$ at $b=b_{\\text{true}}$ is calculated using a second-order central finite difference with step $h=0.5$:\n$$\n\\kappa_b = -\\frac{\\ell_p(b_{\\text{true}}+h) - 2\\ell_p(b_{\\text{true}}) + \\ell_p(b_{\\text{true}}-h)}{h^2} = -\\frac{\\ell_p(b_{\\text{true}}+h) + \\ell_p(b_{\\text{true}}-h)}{h^2}\n$$\nThis curvature provides a measure of how sharply the likelihood function is peaked around the true value of $b$, indicating how well $b$ can be identified from the data, given the specified strain path. A higher curvature implies better identifiability.\n\nThe procedure is executed for each of the three strain path designs provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes identifiability metrics for a phenomenological hardening law\n    based on Fisher information and profile likelihood.\n    \"\"\"\n\n    # --- Problem Constants ---\n    # True parameters: sigma_0, sigma_s, b\n    theta_true = np.array([300.0, 600.0, 12.0])\n    # Noise standard deviation\n    sigma_n = 5.0\n    # Finite difference step for profile curvature\n    h_b = 0.5\n    # Parameter scaling factors for dimensionless FIM\n    s_factors = np.array([300.0, 600.0, 12.0])\n    scaling_matrix = np.diag(1.0 / s_factors)\n\n    # --- Test Suite: Experimental Designs (Strain Paths) ---\n    test_cases = [\n        # Design 1: Limited early hardening\n        np.array([0.0000, 0.0050, 0.0100, 0.0150, 0.0200]),\n        # Design 2: Mid-range hardening\n        np.array([0.0000, 0.0200, 0.0500, 0.0800, 0.1000]),\n        # Design 3: Wide-range saturation\n        np.array([0.0000, 0.0200, 0.0500, 0.1000, 0.2000]),\n    ]\n\n    results = []\n    \n    sigma0_true, sigmas_true, b_true = theta_true\n\n    for eps_design in test_cases:\n        # --- Part 1: Fisher Information Matrix  Optimality Criteria ---\n        \n        # Jacobian matrix J = [d(sigma)/d(sigma_0), d(sigma)/d(sigma_s), d(sigma)/d(b)]\n        J = np.zeros((len(eps_design), 3))\n        exp_term = np.exp(-b_true * eps_design)\n        \n        # Column 1: d(sigma)/d(sigma_0)\n        J[:, 0] = exp_term\n        # Column 2: d(sigma)/d(sigma_s)\n        J[:, 1] = 1.0 - exp_term\n        # Column 3: d(sigma)/d(b)\n        J[:, 2] = (sigmas_true - sigma0_true) * eps_design * exp_term\n\n        # Scaled Jacobian\n        J_s = J @ scaling_matrix\n\n        # Scaled Fisher Information Matrix\n        I_s = (1.0 / sigma_n**2) * (J_s.T @ J_s)\n\n        # D-optimality: determinant of I_s\n        D_opt = np.linalg.det(I_s)\n\n        # E-optimality: minimum eigenvalue of I_s\n        # Use eigvalsh for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(I_s)\n        E_opt = np.min(eigenvalues)\n\n        # --- Part 2: Profile Likelihood Curvature for b ---\n\n        # Noise-free stress data\n        # y_true = sigma(eps; theta_true)\n        y_true = sigmas_true - (sigmas_true - sigma0_true) * np.exp(-b_true * eps_design)\n        \n        def get_profile_log_likelihood(b_val):\n            \"\"\"\n            Calculates the profile log-likelihood for a given b by\n            optimizing over sigma_0 and sigma_s via OLS.\n            \"\"\"\n            # Design matrix for linear regression in (sigma_0, sigma_s)\n            # X_ij = [exp(-b*eps_i), 1 - exp(-b*eps_i)]\n            X = np.zeros((len(eps_design), 2))\n            exp_term_b = np.exp(-b_val * eps_design)\n            X[:, 0] = exp_term_b\n            X[:, 1] = 1.0 - exp_term_b\n\n            # OLS solution for beta = [sigma_0, sigma_s]\n            # beta_hat = (X.T @ X)^-1 @ X.T @ y\n            try:\n                # Add a small regularization term for stability if needed, though unlikely here.\n                # C = X.T @ X + 1e-15 * np.eye(2)\n                C = X.T @ X\n                beta_hat = np.linalg.inv(C) @ X.T @ y_true\n            except np.linalg.LinAlgError:\n                # In case of singularity, return a large negative number\n                return -np.inf\n            \n            # Predicted stress values with optimal sigma_0, sigma_s\n            y_pred = X @ beta_hat\n            \n            # Sum of squared errors\n            sse = np.sum((y_true - y_pred)**2)\n            \n            # Profile log-likelihood\n            return -sse / (2.0 * sigma_n**2)\n            \n        # Calculate profile log-likelihood at b_true +- h\n        # Note: ell_p(b_true) is 0 because data is noise-free\n        ell_p_plus_h = get_profile_log_likelihood(b_true + h_b)\n        ell_p_minus_h = get_profile_log_likelihood(b_true - h_b)\n\n        # Curvature using central finite difference\n        kappa_b = -(ell_p_plus_h + ell_p_minus_h) / h_b**2\n        \n        results.extend([D_opt, E_opt, kappa_b])\n\n    # Format the final output string\n    # E.g., \"[D1,E1,kappa1,D2,E2,kappa2,...]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}