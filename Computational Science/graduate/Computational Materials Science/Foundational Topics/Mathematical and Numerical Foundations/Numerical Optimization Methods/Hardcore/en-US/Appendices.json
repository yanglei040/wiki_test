{
    "hands_on_practices": [
        {
            "introduction": "The Nonlinear Conjugate Gradient (NCG) method is a workhorse algorithm for finding energy minima in computational materials science, significantly improving upon simple steepest descent. This exercise guides you through a single, fundamental iteration of the NCG method, using a simple quadratic energy functional as an illustrative model. By performing this calculation , you will gain a hands-on understanding of how optimizers use gradient information and an exact line search to iteratively approach a solution.",
            "id": "3471655",
            "problem": "In computational materials science, quadratic energy functionals often arise from the harmonic approximation of atomistic or continuum models. Consider the unconstrained minimization of the objective function $f(x)=\\frac{1}{2}x^{\\top}A x-b^{\\top}x$, where $A$ is the stiffness matrix and $b$ encodes an effective load vector. Assume $A=\\begin{pmatrix}20\\\\05\\end{pmatrix}$ and $b=\\begin{pmatrix}2\\\\1\\end{pmatrix}$. Using the Nonlinear Conjugate Gradient (NCG) method, defined as an iterative scheme where the initial search direction is $p_{0}=-\\nabla f(x_{0})$ and the next iterate is $x_{1}=x_{0}+\\alpha_{0}p_{0}$ with $\\alpha_{0}$ obtained by an exact line search along $p_{0}$, start from $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ and perform one iteration to compute $x_{1}$. Derive the step size $\\alpha_{0}$ from first principles by minimizing the one-dimensional restriction of $f$ along the search direction. Express your final answer for $x_{1}$ as exact rational numbers. No rounding is required. The answer must be given as a single row vector.",
            "solution": "The user has requested a solution to a numerical optimization problem. I will first perform the mandatory validation step.\n\n### Step 1: Extract Givens\n-   Objective function: $f(x)=\\frac{1}{2}x^{\\top}A x-b^{\\top}x$\n-   Stiffness matrix: $A=\\begin{pmatrix}20\\\\05\\end{pmatrix}$\n-   Load vector: $b=\\begin{pmatrix}2\\\\1\\end{pmatrix}$\n-   Optimization method: Nonlinear Conjugate Gradient (NCG)\n-   Initial point: $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$\n-   Number of iterations: $1$\n-   Initial search direction: $p_{0}=-\\nabla f(x_{0})$\n-   Update rule: $x_{1}=x_{0}+\\alpha_{0}p_{0}$\n-   Step size determination: $\\alpha_{0}$ is obtained by an exact line search.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is scientifically sound. It describes the unconstrained minimization of a quadratic function, a fundamental and well-understood topic in linear algebra and numerical optimization. The matrix $A$ is symmetric and positive definite (eigenvalues are $2$ and $5$), which ensures the existence of a unique minimum, consistent with the physical interpretation of a stable system in computational materials science.\n2.  **Non-Formalizable or Irrelevant**: The problem is formally stated and is directly relevant to numerical optimization methods applied in computational science.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained and provides all necessary information to perform the requested calculation. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The numbers and matrices are simple but represent a plausible small-scale model. There are no physical or dimensional inconsistencies.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The objective function is strictly convex, guaranteeing a unique minimum. The instructions for the single iteration of the NCG method are unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a standard, multi-step calculation that is neither trivial nor artificially complex.\n7.  **Outside Scientific Verifiability**: The result can be mathematically verified through direct calculation.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. Proceeding to the solution.\n\nThe objective function to be minimized is a quadratic form given by:\n$$f(x) = \\frac{1}{2}x^{\\top}A x - b^{\\top}x$$\nwhere $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, $A=\\begin{pmatrix}20\\\\05\\end{pmatrix}$, and $b=\\begin{pmatrix}2\\\\1\\end{pmatrix}$.\n\nThe gradient of this function, $\\nabla f(x)$, is required. For a symmetric matrix $A$, the gradient is given by the general formula:\n$$\\nabla f(x) = Ax - b$$\nThe first step of the Nonlinear Conjugate Gradient (NCG) method is to compute the gradient at the initial point $x_0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$.\n$$\\nabla f(x_0) = A x_0 - b = \\begin{pmatrix}20\\\\05\\end{pmatrix}\\beginpmatrix}0\\\\0\\end{pmatrix} - \\begin{pmatrix}2\\\\1\\end{pmatrix} = \\begin{pmatrix}0\\\\0\\end{pmatrix} - \\begin{pmatrix}2\\\\1\\end{pmatrix} = \\begin{pmatrix}-2\\\\-1\\end{pmatrix}$$\nThe initial search direction $p_0$ is defined as the negative of the gradient at $x_0$, which corresponds to the direction of steepest descent.\n$$p_0 = -\\nabla f(x_0) = - \\begin{pmatrix}-2\\\\-1\\end{pmatrix} = \\begin{pmatrix}2\\\\1\\end{pmatrix}$$\nThe next iterate $x_1$ is found by moving from $x_0$ along the direction $p_0$ by a step size $\\alpha_0$:\n$$x_1 = x_0 + \\alpha_0 p_0$$\nThe step size $\\alpha_0$ is determined by an exact line search, which means we must find the value of $\\alpha$ that minimizes the one-dimensional function $g(\\alpha) = f(x_0 + \\alpha p_0)$.\nLet's express $g(\\alpha)$ explicitly:\n$$g(\\alpha) = f(x_0 + \\alpha p_0) = \\frac{1}{2}(x_0 + \\alpha p_0)^{\\top}A(x_0 + \\alpha p_0) - b^{\\top}(x_0 + \\alpha p_0)$$\nExpanding the terms, we get:\n$$g(\\alpha) = \\frac{1}{2}(x_0^{\\top}A x_0 + 2\\alpha p_0^{\\top}A x_0 + \\alpha^2 p_0^{\\top}A p_0) - (b^{\\top}x_0 + \\alpha b^{\\top}p_0)$$\nThis expression is a quadratic function of $\\alpha$. To find the minimum, we take the derivative with respect to $\\alpha$ and set it to zero.\n$$\\frac{dg}{d\\alpha} = p_0^{\\top}A x_0 + \\alpha p_0^{\\top}A p_0 - b^{\\top}p_0 = 0$$\nWe can regroup the terms as:\n$$\\frac{dg}{d\\alpha} = p_0^{\\top}(A x_0 - b) + \\alpha p_0^{\\top}A p_0 = 0$$\nRecognizing that $A x_0 - b = \\nabla f(x_0)$, we have:\n$$\\frac{dg}{d\\alpha} = p_0^{\\top}\\nabla f(x_0) + \\alpha p_0^{\\top}A p_0 = 0$$\nSolving for the optimal step size, which we call $\\alpha_0$:\n$$\\alpha_0 = -\\frac{p_0^{\\top}\\nabla f(x_0)}{p_0^{\\top}A p_0}$$\nNow, we substitute the given numerical values to compute $\\alpha_0$.\nWe have $p_0 = \\begin{pmatrix}2\\\\1\\end{pmatrix}$ and $\\nabla f(x_0) = \\begin{pmatrix}-2\\\\-1\\end{pmatrix}$.\n\nThe numerator is:\n$$p_0^{\\top}\\nabla f(x_0) = \\begin{pmatrix}2  1\\end{pmatrix}\\begin{pmatrix}-2\\\\-1\\end{pmatrix} = (2)(-2) + (1)(-1) = -4 - 1 = -5$$\nFor the denominator, we first compute the vector $A p_0$:\n$$A p_0 = \\begin{pmatrix}20\\\\05\\end{pmatrix}\\begin{pmatrix}2\\\\1\\end{pmatrix} = \\begin{pmatrix}(2)(2) + (0)(1) \\\\ (0)(2) + (5)(1)\\end{pmatrix} = \\begin{pmatrix}4\\\\5\\end{pmatrix}$$\nThen the denominator is:\n$$p_0^{\\top}A p_0 = p_0^{\\top}(A p_0) = \\begin{pmatrix}2  1\\end{pmatrix}\\begin{pmatrix}4\\\\5\\end{pmatrix} = (2)(4) + (1)(5) = 8 + 5 = 13$$\nSubstituting these values back into the expression for $\\alpha_0$:\n$$\\alpha_0 = -\\frac{-5}{13} = \\frac{5}{13}$$\nFinally, we compute the next iterate $x_1$:\n$$x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix}0\\\\0\\end{pmatrix} + \\frac{5}{13}\\begin{pmatrix}2\\\\1\\end{pmatrix} = \\begin{pmatrix}0 + \\frac{10}{13} \\\\ 0 + \\frac{5}{13}\\end{pmatrix} = \\begin{pmatrix}\\frac{10}{13}\\\\\\frac{5}{13}\\end{pmatrix}$$\nThe problem asks for the answer as a single row vector.\n$$x_1^{\\top} = \\begin{pmatrix} \\frac{10}{13}  \\frac{5}{13} \\end{pmatrix}$$",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{10}{13}  \\frac{5}{13} \\end{pmatrix}} $$"
        },
        {
            "introduction": "In practice, finding the exact minimum along a search direction is often computationally expensive; instead, optimizers take a \"good enough\" step defined by conditions that guarantee convergence. The strong Wolfe conditions are a cornerstone of modern line search algorithms, ensuring both sufficient decrease in energy and a sufficient change in the gradient. This practice  has you compute a step size that satisfies these crucial criteria, providing insight into the practical engineering of robust optimization solvers.",
            "id": "3471643",
            "problem": "In atomistic energy minimization for crystalline solids, small displacements around a mechanically stable configuration can be approximated by a quadratic energy in the harmonic regime. Consider the harmonic elastic energy model $$f(x)=\\frac{1}{2}x^{\\top}Ax,$$ where $x\\in\\mathbb{R}^{2}$ represents a displacement vector of two coupled atomic degrees of freedom and $A\\in\\mathbb{R}^{2\\times 2}$ is a symmetric positive definite stiffness matrix given by $$A=\\begin{pmatrix}4  1 \\\\ 1  2\\end{pmatrix}.$$ We wish to perform one steepest descent step from the initial configuration $$x_{0}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$$ along the steepest descent direction $$p=-\\nabla f(x_{0}).$$ In numerical optimization for energy minimization, the line search step size $\\alpha0$ is typically chosen to satisfy the strong Wolfe conditions, which for a continuously differentiable objective $f$ and search direction $p$ are defined in terms of the univariate function $$\\phi(\\alpha)=f(x_{0}+\\alpha p)$$ as\n- the sufficient decrease condition: $$\\phi(\\alpha)\\leq \\phi(0)+c_{1}\\,\\alpha\\,\\phi'(0),$$\n- the strong curvature condition: $$|\\phi'(\\alpha)|\\leq c_{2}\\,|\\phi'(0)|,$$\nwith constants $c_{1}$ and $c_{2}$ satisfying $0c_{1}c_{2}1$. Use $c_{1}=10^{-4}$ and $c_{2}=0.9$.\n\nStarting from the fundamental definitions of the gradient and the strong Wolfe conditions, derive and compute a step size $\\alpha$ that satisfies these strong Wolfe conditions for the given $f$, $A$, $x_{0}$, and $p$. Express your final answer as an exact rational number. No rounding is required. The step size is dimensionless.",
            "solution": "The problem is to find a step size $\\alpha  0$ that satisfies the strong Wolfe conditions for a steepest descent step from an initial point $x_0$ for a quadratic objective function $f(x)$.\n\nFirst, we validate the problem statement.\nThe givens are:\n- The objective function is $f(x) = \\frac{1}{2}x^{\\top}Ax$, where $x \\in \\mathbb{R}^2$.\n- The stiffness matrix is $A = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix}$. This matrix is symmetric. Its eigenvalues are $\\lambda_{1,2} = 3 \\pm \\sqrt{2}$, which are both positive, so $A$ is positive definite. This guarantees that $f(x)$ is a strictly convex function with a unique minimum at $x=0$.\n- The initial configuration is $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n- The search direction is the steepest descent direction, $p = -\\nabla f(x_0)$.\n- The strong Wolfe conditions for the univariate function $\\phi(\\alpha) = f(x_0 + \\alpha p)$ are:\n  1. $\\phi(\\alpha) \\leq \\phi(0) + c_1 \\alpha \\phi'(0)$ (Sufficient Decrease)\n  2. $|\\phi'(\\alpha)| \\leq c_2 |\\phi'(0)|$ (Strong Curvature)\n- The constants are $c_1 = 10^{-4}$ and $c_2 = 0.9$, which satisfy the requirement $0  c_1  c_2  1$.\nThe problem is scientifically grounded in numerical optimization and computational physics, is well-posed, objective, and contains all necessary information. It is therefore valid.\n\nWe proceed to the solution.\n\nThe gradient of the objective function $f(x) = \\frac{1}{2}x^{\\top}Ax$ is given by $\\nabla f(x) = Ax$.\nWe compute the gradient at the initial point $x_0$:\n$$\n\\nabla f(x_0) = Ax_0 = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(0) \\\\ 1(1) + 2(0) \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}.\n$$\nThe steepest descent direction $p$ is the negative of the gradient at $x_0$:\n$$\np = -\\nabla f(x_0) = -\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix}.\n$$\nNext, we define the univariate function $\\phi(\\alpha) = f(x_0 + \\alpha p)$. Its derivative with respect to $\\alpha$ is given by the chain rule:\n$$\n\\phi'(\\alpha) = \\nabla f(x_0 + \\alpha p)^{\\top} p.\n$$\nFor our quadratic function, $\\nabla f(x_0 + \\alpha p) = A(x_0 + \\alpha p) = Ax_0 + \\alpha Ap$.\nThus, the derivative is:\n$$\n\\phi'(\\alpha) = (Ax_0 + \\alpha Ap)^{\\top} p = (Ax_0)^{\\top}p + \\alpha (Ap)^{\\top}p = (Ax_0)^{\\top}p + \\alpha p^{\\top}Ap,\n$$\nwhere we used the fact that for a scalar result $u^{\\top}v$, $(Av)^{\\top}u = v^{\\top}A^{\\top}u$.\nLet's evaluate the terms. At $\\alpha=0$, we have:\n$$\n\\phi'(0) = (Ax_0)^{\\top}p = \\begin{pmatrix} 4  1 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix} = 4(-4) + 1(-1) = -17.\n$$\nNow we compute the term $p^{\\top}Ap$:\nFirst, $Ap = \\begin{pmatrix} 4  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} -4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 4(-4) + 1(-1) \\\\ 1(-4) + 2(-1) \\end{pmatrix} = \\begin{pmatrix} -17 \\\\ -6 \\end{pmatrix}$.\nThen, $p^{\\top}Ap = \\begin{pmatrix} -4  -1 \\end{pmatrix} \\begin{pmatrix} -17 \\\\ -6 \\end{pmatrix} = (-4)(-17) + (-1)(-6) = 68 + 6 = 74$.\nSo, the derivative $\\phi'(\\alpha)$ is a linear function of $\\alpha$:\n$$\n\\phi'(\\alpha) = -17 + 74\\alpha.\n$$\nThe function $\\phi(\\alpha)$ is quadratic in $\\alpha$. We can find the step size $\\alpha^*$ that exactly minimizes the energy along the direction $p$ by setting $\\phi'(\\alpha^*) = 0$:\n$$\n-17 + 74\\alpha^* = 0 \\implies \\alpha^* = \\frac{17}{74}.\n$$\nThis exact minimizer is a natural candidate for a step size that satisfies the Wolfe conditions. We must verify this.\n\nWe check the two strong Wolfe conditions for $\\alpha = \\alpha^* = \\frac{17}{74}$.\n\n1.  Sufficient Decrease Condition: $\\phi(\\alpha) \\leq \\phi(0) + c_1 \\alpha \\phi'(0)$\n    For a quadratic objective, the function $\\phi(\\alpha)$ can be written as a Taylor expansion around $\\alpha=0$: $\\phi(\\alpha) = \\phi(0) + \\alpha\\phi'(0) + \\frac{1}{2}\\alpha^2\\phi''(0)$. The exact minimizer is $\\alpha^* = -\\phi'(0)/\\phi''(0)$.\n    Substituting $\\alpha^*$ into the expansion gives $\\phi(\\alpha^*) = \\phi(0) + \\alpha^*\\phi'(0) + \\frac{1}{2}(\\alpha^*)^2\\phi''(0) = \\phi(0) + \\alpha^*\\phi'(0) - \\frac{1}{2}\\alpha^*\\phi'(0) = \\phi(0) + \\frac{1}{2}\\alpha^*\\phi'(0)$.\n    The sufficient decrease condition for $\\alpha^*$ becomes:\n    $$\n    \\phi(0) + \\frac{1}{2}\\alpha^*\\phi'(0) \\leq \\phi(0) + c_1 \\alpha^* \\phi'(0).\n    $$\n    Subtracting $\\phi(0)$ from both sides gives $\\frac{1}{2}\\alpha^*\\phi'(0) \\leq c_1 \\alpha^* \\phi'(0)$.\n    Since we are taking a descent step, $\\phi'(0) = (Ax_0)^{\\top}(-Ax_0) = -\\|Ax_0\\|^2 = -17  0$. Also, $\\alpha^* = \\frac{17}{74} > 0$.\n    Dividing by the negative quantity $\\alpha^*\\phi'(0)$ reverses the inequality sign:\n    $$\n    \\frac{1}{2} \\geq c_1.\n    $$\n    With $c_1=10^{-4}$, we have $0.5 \\geq 0.0001$, which is true. Thus, the sufficient decrease condition is satisfied.\n\n2.  Strong Curvature Condition: $|\\phi'(\\alpha)| \\leq c_2 |\\phi'(0)|$\n    For the exact minimizer $\\alpha^* = \\frac{17}{74}$, the derivative is zero by definition:\n    $$\n    \\phi'(\\alpha^*) = -17 + 74\\left(\\frac{17}{74}\\right) = -17 + 17 = 0.\n    $$\n    The condition becomes:\n    $$\n    |0| \\leq c_2 |\\phi'(0)|.\n    $$\n    We have $c_2 = 0.9 > 0$ and $|\\phi'(0)| = |-17| = 17 > 0$. So, $0 \\leq 0.9 \\times 17 = 15.3$. This inequality is true.\n\nBoth strong Wolfe conditions are satisfied for the exact line search step size $\\alpha^* = \\frac{17}{74}$. This value is an exact rational number as requested.",
            "answer": "$$\n\\boxed{\\frac{17}{74}}\n$$"
        },
        {
            "introduction": "Materials design problems are rarely unconstrained, often involving physical laws like mass conservation or boundaries on composition variables. The Karush-Kuhn-Tucker (KKT) conditions provide a powerful and general framework for analyzing and solving such constrained optimization problems. In this exercise , you will construct and solve the KKT system for a constrained energy minimization, learning to translate physical constraints into a solvable set of mathematical conditions.",
            "id": "3471693",
            "problem": "In computational materials design, consider a binary alloy screening step where a composition vector $x \\in \\mathbb{R}^{2}$ collects two normalized, dimensionless design variables (e.g., regularized phase fractions or defect occupation variables) subject to mass conservation and non-negativity of one component. A convex surrogate of the stored elastic energy is modeled by the quadratic functional $f(x) = \\tfrac{1}{2}\\|x\\|_{2}^{2}$, where $\\|x\\|_{2}^{2} = x_{1}^{2} + x_{2}^{2}$. The feasible set is defined by the equality constraint $h(x) = x_{1} + x_{2} - 1 = 0$ and the inequality constraint $g(x) = -x_{1} \\le 0$.\n\nStarting from the fundamental definitions of convex optimization and the Karush-Kuhn-Tucker (KKT) conditions, derive the necessary and sufficient optimality conditions for this problem. Explicitly construct the Lagrangian and the KKT system (stationarity, primal feasibility, dual feasibility, and complementary slackness), justify the constraint qualification, and solve the system to obtain the optimizer $x^{\\star}$ and the associated Lagrange multipliers $\\lambda^{\\star}$ for the equality constraint and $\\mu^{\\star}$ for the inequality constraint.\n\nExpress your final answer as a single row matrix containing $x_{1}^{\\star}$, $x_{2}^{\\star}$, $\\lambda^{\\star}$, and $\\mu^{\\star}$, in that order, using exact values. No rounding is required and all quantities are dimensionless.",
            "solution": "The problem is to find the minimum of a convex function subject to linear equality and inequality constraints. This is a canonical problem in convex optimization. The solution is found by applying the Karush-Kuhn-Tucker (KKT) conditions.\n\nFirst, we formally state the optimization problem.\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$. The problem is:\nMinimize $f(x) = \\frac{1}{2}(x_1^2 + x_2^2)$\nsubject to:\n$h(x) = x_1 + x_2 - 1 = 0$\n$g(x) = -x_1 \\le 0$\n\nThis is a convex optimization problem. The objective function $f(x)$ is strictly convex, as its Hessian matrix, $\\nabla^2 f(x) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, is the identity matrix, which is positive definite for all $x$. The equality constraint function $h(x)$ is affine, and the inequality constraint function $g(x)$ is also affine (and therefore convex). The feasible set is thus convex. Since we are minimizing a strictly convex function over a non-empty, closed convex set, a unique global minimum exists.\n\nTo find this minimum, we use the Karush-Kuhn-Tucker (KKT) conditions. The applicability of the KKT conditions as necessary conditions for optimality is guaranteed by a constraint qualification. The constraints are affine, so for instance, the Linear Independence Constraint Qualification (LICQ) can be checked. The gradients of the constraint functions are:\n$\\nabla h(x) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n$\\nabla g(x) = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$\nThese two vectors are linearly independent. Thus, LICQ holds at all feasible points, which guarantees that the KKT conditions are necessary for a local minimum. Because the problem is convex, these conditions are also sufficient for a global minimum.\n\nWe construct the Lagrangian function, $\\mathcal{L}(x, \\lambda, \\mu)$, where $\\lambda$ is the Lagrange multiplier for the equality constraint $h(x)=0$ and $\\mu$ is the Lagrange multiplier for the inequality constraint $g(x) \\le 0$.\n$$ \\mathcal{L}(x, \\lambda, \\mu) = f(x) + \\lambda h(x) + \\mu g(x) $$\n$$ \\mathcal{L}(x, \\lambda, \\mu) = \\frac{1}{2}(x_1^2 + x_2^2) + \\lambda(x_1 + x_2 - 1) + \\mu(-x_1) $$\n\nThe KKT conditions are:\n1.  **Stationarity**: $\\nabla_x \\mathcal{L}(x, \\lambda, \\mu) = 0$.\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_1} = x_1 + \\lambda - \\mu = 0 $$\n    $$ \\frac{\\partial \\mathcal{L}}{\\partial x_2} = x_2 + \\lambda = 0 $$\n\n2.  **Primal Feasibility**: The original constraints must hold.\n    $$ x_1 + x_2 - 1 = 0 $$\n    $$ -x_1 \\le 0 \\quad (\\text{or } x_1 \\ge 0) $$\n\n3.  **Dual Feasibility**: The multiplier for the inequality constraint must be non-negative.\n    $$ \\mu \\ge 0 $$\n\n4.  **Complementary Slackness**: The product of the inequality constraint and its multiplier must be zero.\n    $$ \\mu(-x_1) = 0 \\quad (\\text{or } \\mu x_1 = 0) $$\n\nWe now solve this system of equations and inequalities.\nFrom the stationarity conditions, we can express $x_1$ and $x_2$ in terms of the multipliers:\n$$ x_1 = \\mu - \\lambda \\quad (1) $$\n$$ x_2 = -\\lambda \\quad (2) $$\nSubstitute these expressions into the primal feasibility equality constraint:\n$$ (\\mu - \\lambda) + (-\\lambda) - 1 = 0 $$\n$$ \\mu - 2\\lambda - 1 = 0 \\implies \\mu = 1 + 2\\lambda \\quad (3) $$\n\nNext, we use the complementary slackness condition, $\\mu x_1 = 0$, which implies that either $\\mu=0$ or $x_1=0$. We analyze these two cases.\n\n**Case 1: Assume $\\mu = 0$.**\nIf $\\mu=0$, we substitute this into equation $(3)$:\n$$ 0 = 1 + 2\\lambda \\implies \\lambda = -\\frac{1}{2} $$\nNow we can find $x_1$ and $x_2$ using equations $(1)$ and $(2)$:\n$$ x_1 = \\mu - \\lambda = 0 - \\left(-\\frac{1}{2}\\right) = \\frac{1}{2} $$\n$$ x_2 = -\\lambda = -\\left(-\\frac{1}{2}\\right) = \\frac{1}{2} $$\nWe must verify that this solution, $(x_1, x_2, \\lambda, \\mu) = (\\frac{1}{2}, \\frac{1}{2}, -\\frac{1}{2}, 0)$, satisfies all KKT conditions.\n- Stationarity: $x_1+\\lambda-\\mu = \\frac{1}{2} - \\frac{1}{2} - 0 = 0$. $x_2+\\lambda = \\frac{1}{2} - \\frac{1}{2} = 0$. (Satisfied)\n- Primal Feasibility: $x_1+x_2-1 = \\frac{1}{2} + \\frac{1}{2} - 1 = 0$. $x_1 = \\frac{1}{2} \\ge 0$. (Satisfied)\n- Dual Feasibility: $\\mu = 0 \\ge 0$. (Satisfied)\n- Complementary Slackness: $\\mu x_1 = 0 \\cdot \\frac{1}{2} = 0$. (Satisfied)\nAll conditions are satisfied. This is a valid KKT point.\n\n**Case 2: Assume $x_1 = 0$.**\nIf $x_1=0$, we use the primal feasibility equality constraint to find $x_2$:\n$$ 0 + x_2 - 1 = 0 \\implies x_2 = 1 $$\nThis gives the candidate point $x=(0, 1)$. Now we find the corresponding multipliers.\nFrom equation $(2)$:\n$$ x_2 = -\\lambda \\implies 1 = -\\lambda \\implies \\lambda = -1 $$\nFrom equation $(1)$:\n$$ x_1 = \\mu - \\lambda \\implies 0 = \\mu - (-1) \\implies \\mu = -1 $$\nLet's check the KKT conditions for $(x_1, x_2, \\lambda, \\mu) = (0, 1, -1, -1)$. The dual feasibility condition is $\\mu \\ge 0$. However, we found $\\mu = -1$, which violates this condition. Therefore, this case does not lead to a valid KKT point.\n\nThe only solution satisfying all KKT conditions is the one found in Case 1. Since the problem is convex, this point corresponds to the unique global minimum.\nThe optimizer is $x^{\\star} = \\begin{pmatrix} x_1^{\\star} \\\\ x_2^{\\star} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$.\nThe associated Lagrange multipliers are $\\lambda^{\\star} = -\\frac{1}{2}$ and $\\mu^{\\star} = 0$.\n\nThe final answer is the ordered set of values for $x_1^{\\star}$, $x_2^{\\star}$, $\\lambda^{\\star}$, and $\\mu^{\\star}$.\n$x_1^{\\star} = \\frac{1}{2}$\n$x_2^{\\star} = \\frac{1}{2}$\n$\\lambda^{\\star} = -\\frac{1}{2}$\n$\\mu^{\\star} = 0$",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  -\\frac{1}{2}  0 \\end{pmatrix}} $$"
        }
    ]
}