{
    "hands_on_practices": [
        {
            "introduction": "Many fundamental phenomena in materials science, such as the equilibrium concentration of point defects, are described by nonlinear equations derived from thermodynamic principles. This first practice bridges the gap between physical theory and numerical implementation by tackling such a problem . You will implement a robust, safeguarded Newton's method to solve for the vacancy fraction, a solver that combines the rapid convergence of derivative-based methods with the guaranteed stability of a bracketing approach. Furthermore, this exercise emphasizes the power of using physical insight, specifically asymptotic analysis in high and low-temperature regimes, to construct intelligent initial guesses that accelerate convergence.",
            "id": "3486023",
            "problem": "You are modeling the equilibrium fraction of point vacancies in an ideal lattice within a crystalline solid using a minimal thermodynamic model appropriate to computational materials science. Let the vacancy site-fraction be denoted by $\\phi \\in (0,1)$ at absolute temperature $T$, and let the vacancy formation energy be $E_f$. In the ideal, non-interacting, single-component lattice gas model, equilibrium is obtained when the net driving force for vacancy creation vanishes. The resulting balance can be formulated as finding a root of the nonlinear function\n$$\nR(\\phi; E_f, T) \\equiv \\exp\\!\\left(-\\dfrac{E_f}{k_B T}\\right) - \\dfrac{\\phi}{1-\\phi} = 0,\n$$\nwhere $k_B$ is the Boltzmann constant. Assume $E_f  0$, $T  0$, and that the model is applied in the regime where the ideality assumptions are plausible.\n\nYour task is to implement a robust root-finding solver for $\\phi$ that is reliable across a wide range of temperatures, including extremes where $\\exp(-E_f/(k_B T))$ becomes very small or very close to unity. Your design must be grounded in the following principles:\n- Begin from the fundamental definitions of equilibrium in an ideal lattice gas and the Boltzmann factor $\\exp(-E_f/(k_B T))$.\n- Construct a globally convergent numerical method that respects the physical domain $\\phi \\in (0,1)$. You must combine a derivative-based update (for example, Newton’s method applied to $R(\\phi)$) with a safeguarding step that guarantees the next iterate stays in the admissible interval and converges when the derivative step is not adequate.\n- Derive asymptotic scalings for the equilibrium in the limiting regimes of very low $T$ (where $\\exp(-E_f/(k_B T))$ is extremely small) and very high $T$ relative to $E_f$ (where $\\exp(-E_f/(k_B T))$ is close to unity). Use these asymptotic scalings to construct principled initial guesses $\\phi_0(E_f,T)$ that accelerate convergence in the extreme regimes without violating the domain constraints.\n\nPhysical and numerical units:\n- Use $E_f$ in electronvolts (eV).\n- Use $T$ in kelvin (K).\n- Use $k_B = 8.617333262145\\times 10^{-5}\\ \\mathrm{eV/K}$.\n- The vacancy fraction $\\phi$ is dimensionless. Report numerical results as dimensionless floating-point numbers.\n\nAlgorithmic requirements:\n- Implement a solver that uses a derivative-based iteration for $R(\\phi)$ with its analytical derivative and a safeguard that reverts to a bracketed step (for example, bisection within a maintained bracket) if the derivative step would leave the interval $(0,1)$ or fails to reduce the residual. Ensure that the method is globally convergent for any admissible inputs $E_f0$, $T0$.\n- Choose stopping criteria based on the residual $\\lvert R(\\phi)\\rvert$ and/or the step size, stringent enough to deliver at least $10$ significant digits of accuracy in $\\phi$ across the test suite.\n\nTest suite:\nEvaluate your solver for the following parameter sets, in this exact order:\n1. $(E_f, T) = (0.4\\ \\mathrm{eV}, 3000\\ \\mathrm{K})$,\n2. $(E_f, T) = (1.2\\ \\mathrm{eV}, 300\\ \\mathrm{K})$,\n3. $(E_f, T) = (1.2\\ \\mathrm{eV}, 1200\\ \\mathrm{K})$,\n4. $(E_f, T) = (0.01\\ \\mathrm{eV}, 3000\\ \\mathrm{K})$.\n\nAnswer specification and output format:\n- For each case, compute the equilibrium vacancy fraction $\\phi$ by solving $R(\\phi; E_f, T)=0$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"). Each result must be a floating-point number with at least $10$ significant digits. No additional text or whitespace should be printed.",
            "solution": "The problem requires the design and implementation of a robust numerical solver to find the equilibrium vacancy fraction $\\phi$ in a crystalline solid. The governing equation is a nonlinear function derived from fundamental thermodynamic principles.\n\n**1. Problem Formulation and Physical Basis**\n\nThe equilibrium state of a system at constant temperature and pressure is determined by the minimum of its Gibbs free energy, $G$. For a crystal containing $N$ total lattice sites and $n_v$ vacancies, the Gibbs free energy is modeled as $G(n_v) = n_v E_f - T S_{conf}(n_v)$, where $E_f$ is the vacancy formation energy, $T$ is the absolute temperature, and $S_{conf}$ is the configurational entropy. The configurational entropy arises from the multiplicity of ways to arrange the vacancies, given by $S_{conf} = k_B \\ln W$, where $W = N! / (n_v! (N-n_v)!)$ and $k_B$ is the Boltzmann constant.\n\nUsing Stirling's approximation for factorials ($\\ln z! \\approx z\\ln z - z$), which is accurate for the large number of atoms in a crystal, the entropy per site, $s_{conf} = S_{conf}/N$, can be expressed in terms of the vacancy fraction $\\phi = n_v/N$:\n$$ s_{conf}(\\phi) \\approx -k_B \\left[ \\phi \\ln \\phi + (1-\\phi) \\ln(1-\\phi) \\right] $$\nThe Gibbs free energy per site is then $g(\\phi) = \\phi E_f + k_B T \\left[ \\phi \\ln \\phi + (1-\\phi) \\ln(1-\\phi) \\right]$. Equilibrium corresponds to the minimum of $g(\\phi)$, found by setting its derivative with respect to $\\phi$ to zero:\n$$ \\frac{\\partial g}{\\partial \\phi} = E_f + k_B T \\left( \\ln\\phi + 1 - \\ln(1-\\phi) - 1 \\right) = E_f + k_B T \\ln\\left(\\frac{\\phi}{1-\\phi}\\right) = 0 $$\nRearranging this condition yields the equation to be solved:\n$$ \\frac{\\phi}{1-\\phi} = \\exp\\left(-\\frac{E_f}{k_B T}\\right) $$\nThis is a root-finding problem for the function $R(\\phi; E_f, T) = 0$, where the residual $R(\\phi)$ is defined as:\n$$ R(\\phi) \\equiv \\exp\\left(-\\frac{E_f}{k_B T}\\right) - \\frac{\\phi}{1-\\phi} = 0 $$\nThe solution must lie in the physically meaningful domain $\\phi \\in (0,1)$.\n\n**2. Analytical Solution**\n\nFor validation purposes, we note that the equation can be solved analytically. Let $B = \\exp(-E_f/(k_B T))$. The equation $B = \\phi/(1-\\phi)$ can be rearranged to $\\phi = B(1-\\phi)$, which gives $\\phi(1+B) = B$. The exact solution is therefore:\n$$ \\phi = \\frac{B}{1+B} = \\frac{\\exp(-E_f/(k_B T))}{1+\\exp(-E_f/(k_B T))} $$\nSince $E_f  0$ and $T  0$, the Boltzmann factor $B$ is strictly within the interval $(0, 1)$. Consequently, the exact solution for $\\phi$ is always in the interval $(0, 0.5)$. This analytical result serves as a perfect benchmark for our numerical method, although the core task remains the implementation of the specified numerical solver.\n\n**3. Numerical Solver: A Safeguarded Newton's Method**\n\nWe are tasked with constructing a solver that is both fast and robust. A hybrid Newton-Bisection method is ideal for this purpose. It combines the rapid quadratic convergence of Newton's method near the solution with the guaranteed global convergence of the bisection method.\n\nNewton's method finds a root by iterating $\\phi_{k+1} = \\phi_k - R(\\phi_k)/R'(\\phi_k)$. We first need the derivative of $R(\\phi)$:\n$$ R'(\\phi) = \\frac{d}{d\\phi} \\left( B - \\frac{\\phi}{1-\\phi} \\right) = - \\frac{1 \\cdot (1-\\phi) - \\phi \\cdot (-1)}{(1-\\phi)^2} = -\\frac{1}{(1-\\phi)^2} $$\nThe function $R(\\phi)$ is strictly decreasing ($R'(\\phi)0$) and concave ($R''(\\phi)=-2(1-\\phi)^{-3}0$) on its domain. This concavity means that pure Newton's method can overshoot the root and potentially produce iterates outside the physical domain $(0,1)$, necessitating a safeguard.\n\nOur safeguard consists of maintaining a bracket $[\\phi_a, \\phi_b]$ where the root is known to lie, i.e., $R(\\phi_a) > 0$ and $R(\\phi_b)  0$. Given that the solution is always in $(0,0.5)$, the initial bracket can be set to $[0, 0.5]$, which is verified by checking the endpoints: $R(0) = B > 0$ and $R(0.5) = B-1  0$.\n\nThe algorithm at each iteration is as follows:\n1.  From the current iterate $\\phi_k$, compute the Newton step to find a candidate solution $\\phi_{newton}$.\n2.  If $\\phi_{newton}$ falls outside the current bracket $[\\phi_a, \\phi_b]$, it is rejected. The algorithm reverts to a bisection step, and the next iterate becomes the midpoint $\\phi_{k+1} = (\\phi_a + \\phi_b)/2$.\n3.  If $\\phi_{newton}$ is within the bracket, it is accepted as the next iterate, $\\phi_{k+1} = \\phi_{newton}$.\n4.  The bracket is updated for the subsequent iteration by evaluating $R(\\phi_{k+1})$ and replacing either $\\phi_a$ or $\\phi_b$ with $\\phi_{k+1}$ to maintain the sign-change property.\n\nThis strategy ensures that every step makes progress towards the root within a shrinking, guaranteed interval, thus ensuring global convergence.\n\n**4. Asymptotic Analysis for an Optimal Initial Guess**\n\nThe efficiency of the solver is greatly enhanced by starting with an initial guess $\\phi_0$ that is already close to the true solution. We derive such guesses from asymptotic analysis of the solution in limiting temperature regimes, parameterized by $x = E_f/(k_B T)$.\n\n*   **Low-Temperature Limit ($T \\to 0$, hence $x \\to \\infty$):** In this regime, $B = e^{-x}$ is very small. The analytical solution $\\phi = B/(1+B)$ is well-approximated by its first-order Taylor series term:\n    $$ \\phi \\approx B = \\exp\\left(-\\frac{E_f}{k_B T}\\right) $$\n*   **High-Temperature Limit ($T \\to \\infty$, hence $x \\to 0$):** In this regime, $B = e^{-x}$ is close to $1$. Using the Taylor expansion $e^{-x} \\approx 1 - x$, the analytical solution becomes:\n    $$ \\phi = \\frac{e^{-x}}{1+e^{-x}} \\approx \\frac{1-x}{1 + (1-x)} = \\frac{1-x}{2-x} $$\n    Expanding $(1-x/2)^{-1} \\approx 1+x/2$, we find $\\phi \\approx \\frac{1}{2}(1-x)(1+x/2) \\approx \\frac{1}{2}(1-x/2) = \\frac{1}{2} - \\frac{x}{4}$. This gives the approximation:\n    $$ \\phi \\approx \\frac{1}{2} - \\frac{E_f}{4k_B T} $$\n\nWe employ a simple crossover criterion based on $x=E_f/(k_B T)$. If $x > 1$, we use the low-temperature guess. If $x \\le 1$, we use the high-temperature guess. This provides a highly accurate starting point across all physical conditions, minimizing the number of iterations needed.\n\n**5. Convergence Criterion**\n\nTo satisfy the requirement of at least $10$ significant digits of accuracy, the iterative process is terminated when the absolute value of the residual, $|R(\\phi_k)|$, falls below a strict tolerance, set to $10^{-16}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_vacancy_fraction(Ef, T, tol=1e-16, max_iter=50):\n    \"\"\"\n    Computes the equilibrium vacancy fraction phi using a safeguarded Newton's method.\n\n    Args:\n        Ef (float): Vacancy formation energy in eV.\n        T (float): Absolute temperature in K.\n        tol (float): Convergence tolerance for the residual.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        float: The equilibrium vacancy fraction phi.\n    \"\"\"\n    KB_EV_K = 8.617333262145e-5  # Boltzmann constant in eV/K\n\n    if T = 0 or Ef = 0:\n        raise ValueError(\"Temperature and formation energy must be positive.\")\n\n    # Calculate the dimensionless energy term x and the Boltzmann factor B\n    x = Ef / (KB_EV_K * T)\n    \n    # Use np.exp with care for potential underflow\n    B = np.exp(-x) if x  np.log(np.finfo(float).max) else 0.0\n\n    # Define the residual function R(phi) and its derivative R'(phi)\n    # R(phi) = B - phi / (1 - phi)\n    def R(phi):\n        if phi == 1.0:\n            return -np.inf\n        # To avoid catastrophic cancellation when B is close to phi/(1-phi)\n        # we can rewrite R(phi)\n        # return (B * (1 - phi) - phi) / (1 - phi)\n        # However, the direct form is fine for this problem.\n        return B - phi / (1.0 - phi)\n\n    def R_prime(phi):\n        if phi == 1.0:\n            return -np.inf\n        return -1.0 / (1.0 - phi)**2\n\n    # Asymptotic analysis for a principled initial guess\n    if x  1.0:\n        # Low-temperature regime approximation\n        phi_k = B\n    else:\n        # High-temperature regime approximation\n        phi_k = 0.5 - x / 4.0\n\n    # The analytical solution is always in (0, 0.5), so this is a safe bracket\n    phi_a, phi_b = 0.0, 0.5\n\n    # Check if initial guess is within the bracket; if not, use bisection center\n    if not (phi_a  phi_k  phi_b):\n        phi_k = (phi_a + phi_b) / 2.0\n\n    for _ in range(max_iter):\n        res = R(phi_k)\n\n        # Check for convergence\n        if abs(res)  tol:\n            return phi_k\n\n        # Update the bracket using the sign of the residual\n        if res  0:\n            phi_a = phi_k\n        else:\n            phi_b = phi_k\n\n        # Newton's method step\n        deriv = R_prime(phi_k)\n        \n        # Avoid division by zero, though unlikely for this function\n        if abs(deriv)  np.finfo(float).eps:\n            phi_next = (phi_a + phi_b) / 2.0\n        else:\n            phi_newton = phi_k - res / deriv\n            # Safeguard: if Newton step is out of bounds, use bisection\n            if phi_newton = phi_a or phi_newton = phi_b:\n                phi_next = (phi_a + phi_b) / 2.0\n            else:\n                phi_next = phi_newton\n        \n        # Update the iterate\n        if abs(phi_k - phi_next)  tol * abs(phi_k) + tol:\n            # Step size is very small, likely converged\n            return phi_next\n            \n        phi_k = phi_next\n\n    raise RuntimeError(f\"Solver failed to converge for Ef={Ef}, T={T}\")\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.4, 3000.0),  # (Ef in eV, T in K)\n        (1.2, 300.0),\n        (1.2, 1200.0),\n        (0.01, 3000.0)\n    ]\n\n    results = []\n    for Ef_case, T_case in test_cases:\n        # Calculate the equilibrium vacancy fraction for the case\n        phi_eq = solve_vacancy_fraction(Ef_case, T_case)\n        # Format result to ensure sufficient precision\n        results.append(f\"{phi_eq:.15g}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Moving from single-variable equations to multi-dimensional systems is a necessary step for modeling complex material behavior, particularly in continuum mechanics. This exercise explores the stationary points of a non-convex elastic energy functional, a common task in computational mechanics that translates to solving a system of four coupled nonlinear equations . You will derive the system's gradient and Jacobian from first principles and discover that naive solution methods can easily fail on such complex energy landscapes. This practice highlights the critical importance of globalization strategies, such as the Armijo backtracking line search, in ensuring that your solver robustly navigates the solution space to find physically meaningful equilibria.",
            "id": "3486017",
            "problem": "Consider the elastic energy density in finite deformation for a two-dimensional material given by $W(F) = \\mu \\|F\\|_F^2 + \\lambda (\\det F - 1)^2$, where $F \\in \\mathbb{R}^{2 \\times 2}$ is the deformation gradient, $\\|F\\|_F$ is the Frobenius norm, $\\det F$ is the determinant, and $\\mu  0$, $\\lambda  0$ are material constants. The stationary points of the energy satisfy the nonlinear matrix equation $\\nabla_F W(F) = 0$, which can be treated as a root-finding problem in $\\mathbb{R}^4$ by flattening $F$ into a vector of its entries. Your tasks are as follows:\n\n- Starting from the foundational calculus facts that the derivative of the determinant satisfies $\\frac{\\partial \\det(F)}{\\partial F_{ij}} = \\operatorname{Cof}(F)_{ij}$, where $\\operatorname{Cof}(F)$ is the cofactor matrix, and that the gradient of a scalar field composed with a matrix argument can be taken entry-wise, derive the expression for the gradient $\\nabla_F W(F)$ and the Jacobian of the nonlinear system associated with $\\nabla_F W(F) = 0$ when $F = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$. Do not use or assume any pre-derived shortcut formulas: work from the definitions of the Frobenius norm, the determinant, and the cofactor.\n\n- Design and implement a Newton iteration to solve the $4$-dimensional nonlinear system $\\nabla_F W(F) = 0$, using a merit function based on the residual norm and an Armijo backtracking line search to improve robustness. Explicitly implement a backtracking line search that ensures a sufficient decrease in the merit function. Map the basins of attraction by classifying the converged stationary point. Use the following classification: return $0$ if the iterate converges to the trivial stationary point (numerically characterized by $\\|F\\|_F \\leq \\varepsilon$ for a small tolerance $\\varepsilon$), return $1$ if the iterate converges to the scaled-rotation manifold (numerically characterized by $F \\approx \\operatorname{Cof}(F)$ and $\\det F \\approx r^\\star$ with $r^\\star := 1 - \\mu/\\lambda$), and return $2$ otherwise. The presence of rank-one connectivity should be probed by initializing along rank-one directions $u \\otimes v$ and observing the basin to which the iteration converges.\n\n- Implement a second run for each test case using plain Newton’s method without line search to expose potential failure modes on the nonconvex landscape. Report whether the plain method converged (boolean).\n\nUse the material parameters $\\mu = 0.5$ and $\\lambda = 1.0$. Angles must be expressed in radians. Construct the following test suite of initial deformation gradients:\n1. A perturbation near the scaled-rotation manifold with $a = \\sqrt{1 - \\mu/\\lambda}$: $F_1 = \\begin{pmatrix} a  10^{-4} \\\\ -10^{-4}  a \\end{pmatrix}$, where $a = \\sqrt{0.5}$.\n2. Exactly on the scaled-rotation manifold at zero angle: $F_2 = \\begin{pmatrix} a  0 \\\\ 0  a \\end{pmatrix}$ with $a = \\sqrt{0.5}$.\n3. A small-amplitude rank-one start: $F_3 = u \\otimes v$ with $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, scaled by $10^{-3}$, i.e., $F_3 = \\begin{pmatrix} 0  10^{-3} \\\\ 0  0 \\end{pmatrix}$.\n4. A moderate-amplitude rank-one start: $F_4 = \\alpha \\, u \\otimes v$ with $\\alpha = 0.2$, $u = \\begin{pmatrix} \\cos(\\pi/4) \\\\ \\sin(\\pi/4) \\end{pmatrix}$, $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, i.e., $F_4 = \\begin{pmatrix} 0.14142136  0 \\\\ 0.14142136  0 \\end{pmatrix}$.\n5. A far start with negative determinant: $F_5 = \\begin{pmatrix} -1.5  0.7 \\\\ 0.9  -0.1 \\end{pmatrix}$.\n\nFor each test case, run both solvers (plain Newton and Newton with backtracking line search), and produce as the final result for that test case a list of three values: \n- a boolean indicating whether plain Newton converged within tolerance,\n- an integer classification of the converged solution when using backtracking line search ($0$, $1$, or $2$ as defined above),\n- a float giving the final residual norm (the Euclidean norm of $\\nabla_F W(F)$) for the line-search run, rounded to six decimal places.\n\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list of these three-entry lists enclosed in square brackets (e.g., \"[[true_or_false,classification,residual],...]\" with the residual expressed as a decimal number). There are no physical units involved; angles must be in radians as specified.",
            "solution": "This problem requires the derivation of the governing equations for the stationary points of a given elastic energy density and the implementation of a numerical solver to find them. We will first derive the required analytical expressions and then detail the numerical algorithm.\n\n### 1. Derivation of the Gradient and Jacobian\n\nLet the deformation gradient be denoted by $F \\in \\mathbb{R}^{2 \\times 2}$ with components:\n$$\nF = \\begin{pmatrix} F_{11}  F_{12} \\\\ F_{21}  F_{22} \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\n$$\nThe elastic energy density is given by:\n$$\nW(F) = \\mu \\|F\\|_F^2 + \\lambda (\\det F - 1)^2\n$$\nwhere $\\mu  0$ and $\\lambda  0$ are material constants.\n\nIn terms of the components $\\{a, b, c, d\\}$, the Frobenius norm squared is $\\|F\\|_F^2 = a^2 + b^2 + c^2 + d^2$ and the determinant is $\\det F = ad - bc$. The energy function can be written as:\n$$\nW(a,b,c,d) = \\mu (a^2 + b^2 + c^2 + d^2) + \\lambda (ad - bc - 1)^2\n$$\nThe stationary points of $W$ are found where its gradient with respect to $F$ is zero. The gradient, $\\nabla_F W(F)$, is a $2 \\times 2$ matrix whose $(i,j)$ entry is $\\frac{\\partial W}{\\partial F_{ij}}$.\n\nWe compute the gradient by applying the chain rule. The gradient of the first term is:\n$$\n\\nabla_F (\\mu \\|F\\|_F^2) = \\nabla_F \\left(\\mu \\sum_{i,j=1}^2 F_{ij}^2\\right) = 2\\mu F\n$$\nFor the second term, we use the provided rule $\\frac{\\partial \\det(F)}{\\partial F_{ij}} = \\operatorname{Cof}(F)_{ij}$. The cofactor matrix for $F$ is:\n$$\n\\operatorname{Cof}(F) = \\begin{pmatrix} d  -c \\\\ -b  a \\end{pmatrix}\n$$\nThe gradient of the second term is:\n$$\n\\nabla_F (\\lambda(\\det F - 1)^2) = 2\\lambda(\\det F - 1) \\nabla_F(\\det F) = 2\\lambda(\\det F - 1) \\operatorname{Cof}(F)\n$$\nCombining these, the full gradient of the energy function is:\n$$\n\\nabla_F W(F) = 2\\mu F + 2\\lambda(\\det F - 1) \\operatorname{Cof}(F)\n$$\nThe nonlinear equation to be solved is $\\nabla_F W(F) = 0$. Let us define the residual function $G(F) = \\nabla_F W(F)$. To implement a numerical solver, we flatten the matrix $F$ into a vector $x = (a, b, c, d)^T \\in \\mathbb{R}^4$. The matrix equation $G(F)=0$ becomes a vector equation $g(x)=0$, where $g(x)$ is the flattened version of $G(F)$.\nSubstituting the component expressions for $F$ and $\\operatorname{Cof}(F)$, we get:\n$$\ng(x) = \\begin{pmatrix} g_1 \\\\ g_2 \\\\ g_3 \\\\ g_4 \\end{pmatrix} = \\begin{pmatrix}\n2\\mu a + 2\\lambda(ad-bc-1)d \\\\\n2\\mu b + 2\\lambda(ad-bc-1)(-c) \\\\\n2\\mu c + 2\\lambda(ad-bc-1)(-b) \\\\\n2\\mu d + 2\\lambda(ad-bc-1)a\n\\end{pmatrix} = \\begin{pmatrix}\n2\\mu a + 2\\lambda(ad-bc-1)d \\\\\n2\\mu b - 2\\lambda(ad-bc-1)c \\\\\n2\\mu c - 2\\lambda(ad-bc-1)b \\\\\n2\\mu d + 2\\lambda(ad-bc-1)a\n\\end{pmatrix}\n$$\nThe Jacobian of this system, $J_g(x)$, is the $4 \\times 4$ matrix of partial derivatives, $J_{ij} = \\frac{\\partial g_i}{\\partial x_j}$, where $x = (x_1, x_2, x_3, x_4)^T = (a, b, c, d)^T$. Let $S = ad-bc-1$.\nThe components of the Jacobian are:\n- Row 1 ($g_1 = 2\\mu a + 2\\lambda S d$):\n  - $\\frac{\\partial g_1}{\\partial a} = 2\\mu + 2\\lambda (\\frac{\\partial S}{\\partial a}d) = 2\\mu + 2\\lambda(d)d = 2\\mu + 2\\lambda d^2$\n  - $\\frac{\\partial g_1}{\\partial b} = 2\\lambda (\\frac{\\partial S}{\\partial b}d) = 2\\lambda(-c)d = -2\\lambda cd$\n  - $\\frac{\\partial g_1}{\\partial c} = 2\\lambda (\\frac{\\partial S}{\\partial c}d) = 2\\lambda(-b)d = -2\\lambda bd$\n  - $\\frac{\\partial g_1}{\\partial d} = 2\\lambda(S \\cdot 1 + \\frac{\\partial S}{\\partial d}d) = 2\\lambda(S + ad) = 2\\lambda(ad-bc-1+ad) = 2\\lambda(2ad-bc-1)$\n- By symmetry (since the Jacobian is the Hessian of the scalar potential $W$) or direct calculation, the full Jacobian matrix is:\n$$\nJ_g(x) = 2 \\begin{pmatrix}\n\\mu + \\lambda d^2  -\\lambda cd  -\\lambda bd  \\lambda(2ad-bc-1) \\\\\n-\\lambda cd  \\mu + \\lambda c^2  -\\lambda(ad-2bc-1)  -\\lambda ac \\\\\n-\\lambda bd  -\\lambda(ad-2bc-1)  \\mu + \\lambda b^2  -\\lambda ab \\\\\n\\lambda(2ad-bc-1)  -\\lambda ac  -\\lambda ab  \\mu + \\lambda a^2\n\\end{pmatrix}\n$$\n\n### 2. Newton's Method with Backtracking Line Search\n\nTo solve the nonlinear system $g(x) = 0$, we employ Newton's method. Given an iterate $x_k$, the next iterate $x_{k+1}$ is found by solving the linearized system for a step $\\Delta x_k$:\n$$\nJ_g(x_k) \\Delta x_k = -g(x_k)\n$$\nA plain Newton update would be $x_{k+1} = x_k + \\Delta x_k$. However, for a nonconvex energy landscape such as this one, the iterates can easily be sent to regions where the local quadratic approximation is poor, causing divergence. To globalize the method and improve robustness, we introduce a step length $\\alpha_k \\in (0, 1]$ and update as $x_{k+1} = x_k + \\alpha_k \\Delta x_k$.\n\nThe step length $\\alpha_k$ is determined by a backtracking line search that ensures sufficient decrease in a merit function. A suitable merit function is $m(x) = \\frac{1}{2}\\|g(x)\\|_2^2$. The Newton step $\\Delta x_k$ is a descent direction for this function. We seek $\\alpha_k$ satisfying the Armijo condition:\n$$\nm(x_k + \\alpha_k \\Delta x_k) \\leq m(x_k) + c_1 \\alpha_k \\nabla m(x_k)^T \\Delta x_k\n$$\nwhere $c_1$ is a small constant, typically $10^{-4}$. Noting that $\\nabla m(x_k)^T \\Delta x_k = g(x_k)^T J_g(x_k) \\Delta x_k = -g(x_k)^T g(x_k) = -\\|g(x_k)\\|_2^2$, the condition becomes:\n$$\n\\frac{1}{2}\\|g(x_k + \\alpha_k \\Delta x_k)\\|_2^2 \\leq \\frac{1}{2}\\|g(x_k)\\|_2^2 - c_1 \\alpha_k \\|g(x_k)\\|_2^2\n$$\nThe line search algorithm starts with $\\alpha_k=1$ and repeatedly multiplies it by a backtracking factor $\\tau \\in (0, 1)$ (e.g., $\\tau=0.5$) until this condition is met.\n\n### 3. Classification of Stationary Points\n\nOnce the iteration converges to a solution $F_c$, we classify it.\n- **Class 0 (Trivial):** The zero matrix $F=0$ is a stationary point, since $G(0) = 2\\mu(0) + 2\\lambda(0-1)\\operatorname{Cof}(0) = 0$. Numerically, we classify a solution as trivial if $\\|F_c\\|_F \\leq \\varepsilon$ for a small tolerance $\\varepsilon$.\n- **Class 1 (Scaled-Rotation Manifold):** This class represents physically meaningful, non-degenerate solutions. From the condition $G(F)=0$, we have $F = -\\frac{\\lambda}{\\mu}(\\det F - 1)\\operatorname{Cof}(F)$. A key family of solutions satisfies $F = k \\, \\operatorname{Cof}(F)$. If $k=1$, then $F=\\operatorname{Cof}(F)$, which is the condition for a matrix to be a scaled rotation (i.e., $F_{11}=F_{22}$ and $F_{12}=-F_{21}$). Substituting $F=\\operatorname{Cof}(F)$ into the stationary condition gives $F = -\\frac{\\lambda}{\\mu}(\\det F - 1)F$, which for $F \\neq 0$ implies $1 = -\\frac{\\lambda}{\\mu}(\\det F - 1)$, or $\\det F = 1 - \\mu/\\lambda$. For the given parameters $\\mu=0.5, \\lambda=1.0$, this gives $\\det F = 0.5$. Numerically, we classify a solution as Class 1 if it satisfies both $\\|F_c - \\operatorname{Cof}(F_c)\\|_F \\leq \\varepsilon_{approx}$ and $|\\det F_c - (1-\\mu/\\lambda)| \\leq \\varepsilon_{approx}$.\n- **Class 2 (Other):** Any converged solution that does not fit into Class 0 or Class 1. This may include other stationary points or saddle points of the energy functional.\n\nThis framework allows us to not only find solutions but also to characterize the basins of attraction for different types of stationary states, which is crucial for understanding the material's mechanical behavior.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for stationary points of an elastic energy functional using Newton's method\n    and classifies the basins of attraction.\n    \"\"\"\n    mu = 0.5\n    lmbda = 1.0\n\n    # Define test cases\n    a_sqrt_half = np.sqrt(0.5)\n    test_cases = [\n        np.array([[a_sqrt_half, 1e-4], [-1e-4, a_sqrt_half]]),\n        np.array([[a_sqrt_half, 0.0], [0.0, a_sqrt_half]]),\n        np.array([[0.0, 1e-3], [0.0, 0.0]]),\n        np.array([[0.14142136, 0.0], [0.14142136, 0.0]]),\n        np.array([[-1.5, 0.7], [0.9, -0.1]])\n    ]\n\n    results = []\n    for F_initial in test_cases:\n        x_initial = F_initial.flatten()\n\n        # Run plain Newton's method\n        plain_converged, _ = newton_solver(x_initial, mu, lmbda, use_line_search=False)\n\n        # Run Newton's method with backtracking line search\n        line_search_converged, x_final = newton_solver(x_initial, mu, lmbda, use_line_search=True)\n        \n        F_final = x_final.reshape((2, 2))\n        final_residual_norm = np.linalg.norm(residual_g(x_final, mu, lmbda))\n\n        # Classify the solution from the line search run\n        classification = classify_solution(F_final, mu, lmbda)\n        \n        # Round the residual as required\n        rounded_residual = round(final_residual_norm, 6)\n\n        results.append([plain_converged, classification, rounded_residual])\n\n    # Format and print the final output\n    # Using a custom formatter to avoid trailing zeros for integers\n    def format_item(item):\n        if isinstance(item, bool):\n            return str(item).lower()\n        if isinstance(item, int):\n            return str(item)\n        if isinstance(item, float):\n            return f\"{item:.6f}\"\n        return str(item)\n\n    result_str = \"[\" + \",\".join(\n        \"[\" + \",\".join(map(format_item, r)) + \"]\" for r in results\n    ) + \"]\"\n    print(result_str)\n\ndef residual_g(x, mu, lmbda):\n    \"\"\"Computes the residual vector g(x).\"\"\"\n    a, b, c, d = x\n    det_F = a * d - b * c\n    S = det_F - 1\n    g = np.zeros(4)\n    g[0] = 2 * mu * a + 2 * lmbda * S * d\n    g[1] = 2 * mu * b - 2 * lmbda * S * c\n    g[2] = 2 * mu * c - 2 * lmbda * S * b\n    g[3] = 2 * mu * d + 2 * lmbda * S * a\n    return g\n\ndef jacobian_J(x, mu, lmbda):\n    \"\"\"Computes the Jacobian matrix J(x).\"\"\"\n    a, b, c, d = x\n    det_F = a * d - b * c\n    J = np.zeros((4, 4))\n    \n    # Row 1\n    J[0, 0] = 2 * mu + 2 * lmbda * d**2\n    J[0, 1] = -2 * lmbda * c * d\n    J[0, 2] = -2 * lmbda * b * d\n    J[0, 3] = 2 * lmbda * (2 * a * d - b * c - 1)\n    \n    # Row 2\n    J[1, 0] = -2 * lmbda * c * d\n    J[1, 1] = 2 * mu + 2 * lmbda * c**2\n    J[1, 2] = -2 * lmbda * (a * d - 2 * b * c - 1)\n    J[1, 3] = -2 * lmbda * a * c\n    \n    # Row 3\n    J[2, 0] = -2 * lmbda * b * d\n    J[2, 1] = -2 * lmbda * (a * d - 2 * b * c - 1)\n    J[2, 2] = 2 * mu + 2 * lmbda * b**2\n    J[2, 3] = -2 * lmbda * a * b\n    \n    # Row 4\n    J[3, 0] = 2 * lmbda * (2 * a * d - b * c - 1)\n    J[3, 1] = -2 * lmbda * a * c\n    J[3, 2] = -2 * lmbda * a * b\n    J[3, 3] = 2 * mu + 2 * lmbda * a**2\n    \n    return J\n\ndef newton_solver(x0, mu, lmbda, use_line_search=True, max_iter=100, tol=1e-8, c1=1e-4, tau=0.5):\n    \"\"\"\n    Newton's method solver, with optional backtracking line search.\n    \"\"\"\n    x = np.copy(x0)\n    for _ in range(max_iter):\n        g = residual_g(x, mu, lmbda)\n        if np.linalg.norm(g)  tol:\n            return True, x\n        \n        J = jacobian_J(x, mu, lmbda)\n        \n        try:\n            delta_x = np.linalg.solve(J, -g)\n        except np.linalg.LinAlgError:\n            # Singular Jacobian\n            return False, x\n\n        if use_line_search:\n            alpha = 1.0\n            g_norm_sq = np.dot(g, g)\n            # Armijo condition check\n            while True:\n                x_new = x + alpha * delta_x\n                g_new = residual_g(x_new, mu, lmbda)\n                g_new_norm_sq = np.dot(g_new, g_new)\n                if g_new_norm_sq = g_norm_sq - 2 * c1 * alpha * g_norm_sq:\n                    break\n                alpha *= tau\n                if alpha  1e-8: # Prevent infinite loop\n                    return False, x\n            x += alpha * delta_x\n        else: # Plain Newton\n            x += delta_x\n        \n        # Check for divergence in plain Newton mode for stability\n        if not use_line_search and np.linalg.norm(g)  1e10:\n             return False, x\n\n    # Check for convergence after max_iter\n    g_final = residual_g(x, mu, lmbda)\n    converged = np.linalg.norm(g_final)  tol\n    return converged, x\n\ndef classify_solution(F, mu, lmbda, tol_trivial=1e-6, tol_approx=1e-6):\n    \"\"\"\n    - 0: trivial stationary point\n    - 1: scaled-rotation manifold\n    - 2: other\n    \"\"\"\n    # Class 0: Trivial solution\n    if np.linalg.norm(F, 'fro') = tol_trivial:\n        return 0\n\n    # Class 1: Scaled-rotation manifold\n    a, b, c, d = F.flatten()\n    Cof_F = np.array([[d, -c], [-b, a]])\n    \n    det_F = a * d - b * c\n    r_star = 1.0 - mu / lmbda\n    \n    is_cof_match = np.linalg.norm(F - Cof_F, 'fro') = tol_approx\n    is_det_match = abs(det_F - r_star) = tol_approx\n    \n    if is_cof_match and is_det_match:\n        return 1\n\n    # Class 2: Other\n    return 2\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world computational materials science often involves problems with thousands or even millions of degrees of freedom, such as fitting model parameters to large experimental datasets. For these large-scale systems, methods requiring the explicit storage and inversion of an $N \\times N$ Jacobian matrix become computationally prohibitive. This practice introduces a powerful, industry-standard solution: quasi-Newton methods, specifically the limited-memory Broyden method . You will implement a solver that approximates the Jacobian's action using only a stored history of recent steps, enabling the solution of a high-dimensional parameter estimation problem in a \"matrix-free\" manner, a vital skill for developing scalable and efficient scientific software.",
            "id": "3485986",
            "problem": "Consider a large parameter estimation problem that arises in computational materials science, framed as the recovery of defect level energies in a crystalline solid from observed site occupancies. For each site index $i \\in \\{1,\\dots,N\\}$, the occupancy $f_i$ at temperature $T$ and chemical potential $\\mu$ is modeled by the Fermi–Dirac distribution, a well-tested formula from statistical mechanics: $$f_i(E_i;\\mu,T) = \\frac{1}{1+\\exp\\left(\\frac{E_i - \\mu}{k_B T}\\right)},$$ where $E_i$ is the defect level energy at site $i$ and $k_B$ is the Boltzmann constant. Given a vector of observed occupancies $\\mathbf{f}^{\\mathrm{obs}} \\in \\mathbb{R}^N$, the parameter estimation problem is to find an energy vector $\\mathbf{x} \\in \\mathbb{R}^N$ such that $$F(\\mathbf{x}) = \\left(f_i(x_i;\\mu,T) - f_i^{\\mathrm{obs}}\\right)_{i=1}^N = \\mathbf{0},$$ thereby matching modeled and observed occupancies. Energies must be expressed in electronvolts $\\mathrm{eV}$ and temperature in Kelvin $K$. Occupancies are dimensionless.\n\nYou will implement a root-finding solver based on a limited-memory Broyden strategy (Limited-memory Broyden (L-Broyden)), suitable for high-dimensional nonlinear systems. The solver should be designed to avoid forming dense Jacobian or inverse Jacobian matrices. Instead, it must maintain only the most recent $m$ vector pairs $(s_k, y_k)$, where $s_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and $y_k = F(\\mathbf{x}_{k+1}) - F(\\mathbf{x}_k)$, and it must compute the action of the current inverse-Jacobian approximation on an arbitrary vector using these stored pairs and an initial scalar multiple of the identity. Start from the fundamental secant condition for quasi-Newton updates and principles that ensure minimal change to the inverse operator while satisfying the secant equation; derive the structure needed to store and update $(s_k,y_k)$ efficiently in limited memory. Your program must include a line search that ensures monotonic decrease of the merit function $$\\phi(\\mathbf{x}) = \\frac{1}{2}\\|F(\\mathbf{x})\\|_2^2,$$ and it must stop when the infinity norm $\\|F(\\mathbf{x})\\|_\\infty$ is below the specified tolerance.\n\nConstruct synthetic, but scientifically plausible, test instances as follows. Use a deterministic energy field $$E_i^{\\star} = 0.4 + 0.25\\sin\\left(\\frac{2\\pi i}{N}\\right) + 0.15\\cos\\left(\\frac{4\\pi i}{N}\\right) \\quad \\text{for}\\quad i=1,\\dots,N,$$ expressed in $\\mathrm{eV}$. Generate observed occupancies by $f_i^{\\mathrm{obs}} = f_i(E_i^{\\star};\\mu,T)$ with the given $T$ and $\\mu$. Initialize the solver at $\\mathbf{x}_0 = \\mathbf{E}^{\\star} + \\delta\\,\\mathbf{1}$, where $\\delta$ is a scalar offset in $\\mathrm{eV}$. Use $k_B = 8.617333262145\\times 10^{-5}\\,\\mathrm{eV}/K$.\n\nImplement the L-Broyden solver that:\n- Maintains at most $m$ recent pairs $(s_k,y_k)$.\n- Computes search directions by applying the current limited-memory inverse-Jacobian approximation to $F(\\mathbf{x}_k)$ without ever forming dense matrices.\n- Employs a backtracking line search that ensures $\\phi(\\mathbf{x}_{k+1})  \\phi(\\mathbf{x}_k)$.\n- Terminates when $\\|F(\\mathbf{x}_k)\\|_\\infty  \\text{tol}$ or after a maximum number of iterations.\n\nYour program must run the following test suite and report results in the specified format:\n- Test $1$ (general case): $N=200$, $m=10$, $\\mu=0.4\\,\\mathrm{eV}$, $T=900\\,K$, $\\delta=0.2\\,\\mathrm{eV}$, $\\text{tol}=10^{-10}$, $\\text{maxiter}=200$.\n- Test $2$ (boundary, no memory): $N=300$, $m=0$, $\\mu=0.4\\,\\mathrm{eV}$, $T=900\\,K$, $\\delta=0.2\\,\\mathrm{eV}$, $\\text{tol}=10^{-8}$, $\\text{maxiter}=250$.\n- Test $3$ (edge, steep nonlinearity): $N=150$, $m=5$, $\\mu=0.1\\,\\mathrm{eV}$, $T=300\\,K$, $\\delta=-0.1\\,\\mathrm{eV}$, $\\text{tol}=10^{-8}$, $\\text{maxiter}=300$.\n\nFor each test case, return three items: a boolean indicating whether convergence was achieved (defined by $\\|F(\\mathbf{x})\\|_\\infty  \\text{tol}$), the final value of $\\|F(\\mathbf{x})\\|_\\infty$ (a float), and the number of iterations used (an integer). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of tests $1$, $2$, and $3$, for example, $$[\\text{conv}_1,\\|F\\|_{\\infty,1},\\text{iter}_1,\\text{conv}_2,\\|F\\|_{\\infty,2},\\text{iter}_2,\\text{conv}_3,\\|F\\|_{\\infty,3},\\text{iter}_3].$$",
            "solution": "This problem requires solving a high-dimensional nonlinear system of equations using a limited-memory quasi-Newton method. We must implement the \"good\" Broyden's method in a matrix-free manner, suitable for large-scale problems.\n\n### 1. Problem Formulation and Numerical Strategy\nThe goal is to solve the nonlinear system of equations $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$, where the $i$-th component is given by:\n$$F_i(\\mathbf{x}) = \\frac{1}{1+\\exp\\left(\\frac{x_i - \\mu}{k_B T}\\right)} - f_i^{\\mathrm{obs}} = 0$$\nHere, $\\mathbf{x} \\in \\mathbb{R}^N$ is the vector of unknown defect energies. For large $N$, forming and inverting the $N \\times N$ Jacobian matrix $J(\\mathbf{x})$ at each step of Newton's method is computationally prohibitive. A quasi-Newton method avoids this by constructing an approximation to the Jacobian or its inverse. The iterative step is:\n$$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$$\nwhere $\\mathbf{p}_k$ is the search direction and $\\alpha_k$ is a step length. For methods that approximate the inverse Jacobian $H_k \\approx J(\\mathbf{x}_k)^{-1}$, the search direction is computed as $\\mathbf{p}_k = -H_k \\mathbf{F}(\\mathbf{x}_k)$.\n\n### 2. The Limited-Memory Broyden Method\nThe \"good\" Broyden's method builds an approximation $H_k$ to the inverse Jacobian. At each step, the new approximation $H_{k+1}$ is required to satisfy the secant equation:\n$$H_{k+1} \\mathbf{y}_k = \\mathbf{s}_k$$\nwhere $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ is the step in the solution and $\\mathbf{y}_k = \\mathbf{F}(\\mathbf{x}_{k+1}) - \\mathbf{F}(\\mathbf{x}_k)$ is the corresponding change in the residual. The update formula for $H_{k+1}$ that satisfies this condition while being closest to $H_k$ in the Frobenius norm is a rank-1 update:\n$$H_{k+1} = H_k + \\frac{(\\mathbf{s}_k - H_k \\mathbf{y}_k)\\mathbf{y}_k^T}{\\mathbf{y}_k^T \\mathbf{y}_k}$$\n\n### 3. Matrix-Free Implementation\nIn a limited-memory (L-Broyden) implementation, the dense matrix $H_k$ is never formed or stored. Instead, we only store the $m$ most recent update pairs $(\\mathbf{s}_j, \\mathbf{y}_j)$. The action of $H_k$ on the current residual, needed to find the search direction $\\mathbf{p}_k = -H_k \\mathbf{F}(\\mathbf{x}_k)$, is computed by starting with an initial approximation $H_{\\text{init}}$ (here, the identity matrix $I$) and sequentially applying the $m$ stored rank-1 updates.\n\nThe procedure to compute $\\mathbf{p} = H_k \\mathbf{v}$ (where $\\mathbf{v} = \\mathbf{F}_k$), as implemented in the provided code, is an $O(m^2 N)$ algorithm that recursively builds the product. Let the stored history be the sets of vectors $S = \\{\\mathbf{s}_0, \\dots, \\mathbf{s}_{m-1}\\}$ and $Y = \\{\\mathbf{y}_0, \\dots, \\mathbf{y}_{m-1}\\}$.\n1.  Initialize the result vector $\\mathbf{p} = H_{\\text{init}}\\mathbf{v} = \\mathbf{v}$.\n2.  Initialize an array of auxiliary vectors `gamma`, where `gamma[j]` will hold $H_j \\mathbf{y}_j$. Initially, `gamma[j]`$= H_{\\text{init}} \\mathbf{y}_j = \\mathbf{y}_j$.\n3.  Iterate for $i$ from $0$ to $m-1$:\n    a. Update the result vector $\\mathbf{p}$ to incorporate the $i$-th update: $\\mathbf{p} \\leftarrow \\mathbf{p} + (\\mathbf{s}_i - \\text{gamma}[i]) \\frac{\\mathbf{y}_i^T \\mathbf{v}}{\\mathbf{y}_i^T \\mathbf{y}_i}$.\n    b. Update all subsequent auxiliary vectors `gamma[j]` for $j > i$ to reflect the $i$-th update: $\\text{gamma}[j] \\leftarrow \\text{gamma}[j] + (\\mathbf{s}_i - \\text{gamma}[i]) \\frac{\\mathbf{y}_i^T \\mathbf{y}_j}{\\mathbf{y}_i^T \\mathbf{y}_i}$.\n\nAfter the loop, $\\mathbf{p}$ holds the final result of $H_k \\mathbf{v}$. This allows the computation of the search direction without ever forming a dense matrix.\n\n### 4. Globalization via Line Search\nTo ensure robust convergence, especially when far from the solution, a line search is used to determine the step length $\\alpha_k$. The update is $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$. The line search must ensure a sufficient decrease in a merit function, here chosen as $\\phi(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{F}(\\mathbf{x})\\|_2^2$. The solver employs a simple backtracking strategy: it starts with a full step ($\\alpha_k=1$) and successively reduces it (e.g., by half) until the simple decrease condition $\\phi(\\mathbf{x}_{k+1})  \\phi(\\mathbf{x}_k)$ is satisfied.\n\n### 5. The Case of m=0\nWhen the memory parameter $m$ is set to $0$, no history vectors are stored. The `apply_H` procedure skips the update loop, and the action of $H_k$ is always that of the initial approximation $H_{\\text{init}} = I$. The search direction thus becomes $\\mathbf{p}_k = -I \\cdot \\mathbf{F}(\\mathbf{x}_k) = -\\mathbf{F}(\\mathbf{x}_k)$. This is the steepest-descent direction for the merit function $\\phi(\\mathbf{x})$, and the method reduces to a gradient descent algorithm on the sum-of-squares of the residual.\n\n### 6. Summary of the L-Broyden Algorithm\n1.  **Initialization**: Given $\\mathbf{x}_0$, memory $m$, tolerance `tol`, and `maxiter`. Calculate $\\mathbf{F}_0 = \\mathbf{F}(\\mathbf{x}_0)$. Initialize empty history lists $S$ and $Y$.\n2.  **Iteration Loop**: For $k = 0, 1, \\dots, \\text{maxiter}-1$:\n    a. **Check Convergence**: If $\\|\\mathbf{F}_k\\|_\\infty  \\text{tol}$, terminate with success.\n    b. **Compute Search Direction**: Calculate $\\mathbf{p}_k = -\\text{apply\\_H}(\\mathbf{F}_k, S, Y)$.\n    c. **Line Search**: Find a step length $\\alpha_k$ via backtracking such that $\\phi(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)  \\phi(\\mathbf{x}_k)$.\n    d. **Update Solution**: Set $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n    e. **Update Residual**: Compute $\\mathbf{F}_{k+1} = \\mathbf{F}(\\mathbf{x}_{k+1})$.\n    f. **Update History**: Calculate $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and $\\mathbf{y}_k = \\mathbf{F}_{k+1} - \\mathbf{F}_k$. If the update is stable ($\\mathbf{y}_k^T \\mathbf{y}_k$ is not too small), add $(\\mathbf{s}_k, \\mathbf{y}_k)$ to the history, removing the oldest pair if the memory limit $m$ is exceeded.\n3.  **Termination**: If the loop completes without convergence, terminate with failure.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the L-Broyden solver.\n    \"\"\"\n    KB_EV_K = 8.617333262145e-5  # Boltzmann constant in eV/K\n\n    test_cases = [\n        # Test 1 (general case)\n        {'N': 200, 'm': 10, 'mu': 0.4, 'T': 900, 'delta': 0.2, 'tol': 1e-10, 'maxiter': 200},\n        # Test 2 (boundary, no memory)\n        {'N': 300, 'm': 0, 'mu': 0.4, 'T': 900, 'delta': 0.2, 'tol': 1e-8, 'maxiter': 250},\n        # Test 3 (edge, steep nonlinearity)\n        {'N': 150, 'm': 5, 'mu': 0.1, 'T': 300, 'delta': -0.1, 'tol': 1e-8, 'maxiter': 300},\n    ]\n\n    results = []\n    for params in test_cases:\n        N = params['N']\n        m = params['m']\n        mu = params['mu']\n        T = params['T']\n        delta = params['delta']\n        \n        # Generate synthetic data\n        i_vec = np.arange(1, N + 1)\n        E_star = 0.4 + 0.25 * np.sin(2 * np.pi * i_vec / N) + 0.15 * np.cos(4 * np.pi * i_vec / N)\n        \n        # Fermi-Dirac function\n        def fermi_dirac(E, mu_val, T_val):\n            return 1.0 / (1.0 + np.exp((E - mu_val) / (KB_EV_K * T_val)))\n\n        f_obs = fermi_dirac(E_star, mu, T)\n\n        # The function F whose root we want to find\n        def F_func(x):\n            return fermi_dirac(x, mu, T) - f_obs\n        \n        # Initial guess\n        x0 = E_star + delta\n\n        # Run the solver\n        converged, final_norm, iterations = l_broyden_solver(\n            F_func, \n            x0, \n            m=m, \n            tol=params['tol'], \n            maxiter=params['maxiter']\n        )\n        \n        results.extend([converged, final_norm, iterations])\n    \n    # Format and print the final output\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\ndef l_broyden_solver(F_func, x0, m, tol, maxiter):\n    \"\"\"\n    Implements a limited-memory Broyden solver for F(x) = 0.\n\n    Args:\n        F_func (callable): The vector-valued function to find the root of.\n        x0 (np.ndarray): The initial guess for the solution vector x.\n        m (int): The number of history vectors (s, y) to store.\n        tol (float): The tolerance for the infinity norm of F(x) for convergence.\n        maxiter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: (converged, final_norm, iterations)\n    \"\"\"\n    x_k = np.copy(x0)\n    F_k = F_func(x_k)\n    norm_F = np.linalg.norm(F_k, np.inf)\n\n    # History vectors s_k = x_{k+1}-x_k and y_k = F_{k+1}-F_k\n    S = []\n    Y = []\n    \n    for k in range(maxiter):\n        if norm_F  tol:\n            return True, norm_F, k\n\n        # Compute search direction p_k = -H_k * F_k\n        p_k = -apply_H(F_k, S, Y)\n        \n        # Backtracking line search for step size alpha\n        alpha = 1.0\n        phi_k = 0.5 * np.dot(F_k, F_k)\n        x_next, F_next = None, None\n        \n        line_search_success = False\n        for _ in range(10): # Max 10 backtracking steps\n            x_next = x_k + alpha * p_k\n            F_next = F_func(x_next)\n            phi_next = 0.5 * np.dot(F_next, F_next)\n            \n            if phi_next  phi_k:\n                line_search_success = True\n                break\n            alpha /= 2.0\n            \n        if not line_search_success:\n            return False, norm_F, k + 1\n\n        s_k = x_next - x_k\n        y_k = F_next - F_k\n\n        # Update history S and Y\n        if m  0:\n            denom = np.dot(y_k, y_k)\n            if denom  1e-12: # Avoid unstable updates\n                if len(S) == m:\n                    S.pop(0)\n                    Y.pop(0)\n                S.append(s_k)\n                Y.append(y_k)\n\n        x_k = x_next\n        F_k = F_next\n        norm_F = np.linalg.norm(F_k, np.inf)\n\n    return norm_F  tol, norm_F, maxiter\n\ndef apply_H(v, S, Y):\n    \"\"\"\n    Computes the action of the inverse Jacobian approximation H on a vector v.\n    H is constructed from the history vectors S and Y using the \"good\" Broyden update.\n    The initial approximation H_init is the identity matrix.\n\n    Args:\n        v (np.ndarray): The vector to apply H on.\n        S (list of np.ndarray): History of solution steps (s_k).\n        Y (list of np.ndarray): History of residual changes (y_k).\n\n    Returns:\n        np.ndarray: The result of H * v.\n    \"\"\"\n    m = len(S)\n    if m == 0:\n        return v  # H_init is identity, so H_init * v = v\n\n    # O(m^2) algorithm to compute H_m * v\n    p = np.copy(v)\n    \n    # gamma[j] will store H_j * Y[j]\n    gamma = [np.copy(y) for y in Y]\n    \n    for i in range(m):\n        yi_dot_yi = np.dot(Y[i], Y[i])\n        \n        # Skip update if denominator is too small\n        if yi_dot_yi  1e-12:\n            continue\n\n        # Update p, which represents H_i*v -> H_{i+1}*v\n        c = np.dot(Y[i], v) / yi_dot_yi\n        p += (S[i] - gamma[i]) * c\n\n        # Update gamma vectors for subsequent steps\n        # gamma[j] is updated from H_i*Y[j] to H_{i+1}*Y[j]\n        for j in range(i + 1, m):\n            d_coeff = np.dot(Y[i], Y[j]) / yi_dot_yi\n            gamma[j] += (S[i] - gamma[i]) * d_coeff\n            \n    return p\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}