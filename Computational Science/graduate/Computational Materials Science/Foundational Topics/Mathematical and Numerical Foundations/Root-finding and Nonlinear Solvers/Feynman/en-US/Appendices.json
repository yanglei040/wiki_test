{
    "hands_on_practices": [
        {
            "introduction": "While Newton's method is celebrated for its rapid convergence, its textbook formulation is often fragile in practice. This first exercise serves as a guided tour through its common failure modes, particularly when encountering functions with nearly flat regions or multiple roots . By implementing and testing fundamental \"rescue\" techniques such as step scaling and damping, you will gain a deeper appreciation for the mechanics of robust nonlinear solvers and why safeguards are not optional extras, but essential components.",
            "id": "3164867",
            "problem": "You are to implement and analyze numerical solvers for nonlinear root-finding within one dimension, focusing on the behavior of Newton’s method when the function has a flat region near a root. Let $x^\\*$ denote a simple root of a function $f(x)$, and consider Newton’s iteration $x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$. Your task is to design smooth functions $f(x)$ that exhibit near-flat behavior at or near $x^\\*$, demonstrate how Newton’s method can stall or become inefficient, and implement strategies to rescue convergence via step scaling and damping.\n\nFundamental base assumptions for this task are:\n- Differentiability: $f(x)$ is differentiable where the method is applied, and $f'(x)$ may be small near $x^\\*$ in some cases.\n- For a multiple root, if $f(x) = (x - x^\\*)^m$ with integer $m \\geq 2$, then $f'(x^\\*) = 0$ and the local geometry is flat at the root.\n- The inverse tangent function $\\arctan(y)$ is smooth for all real $y$, and its value is an angle measured in radians.\n\nYou must implement three solvers:\n- A naive Newton solver applying $x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$ directly, with termination when $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n- A step-scaled Newton solver tailored for known root multiplicity $m \\geq 2$, using $x_{n+1} = x_n - m \\frac{f(x_n)}{f'(x_n)}$. This rescales the Newton step to account for multiplicity.\n- A damped Newton solver with backtracking line search: compute the undamped step $\\Delta = -\\frac{f(x_n)}{f'(x_n)}$, choose a step length $\\lambda \\in (0, 1]$ by halving from $1$ until $\\lvert f(x_n + \\lambda \\Delta) \\rvert < \\lvert f(x_n) \\rvert$, then set $x_{n+1} = x_n + \\lambda \\Delta$. Terminate when $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n\nDesign the following test functions and cases:\n- Case A (multiple root, flat at the root):\n  - Function: $f_1(x) = (x - 0.5)^2$, with derivative $f_1'(x) = 2(x - 0.5)$, root $x^\\* = 0.5$, and multiplicity $m = 2$.\n  - Initial guess: $x_0 = 2.0$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $N_{\\max} = 200$.\n  - Compute two quantities:\n    1. The integer iteration count for the naive Newton solver to achieve $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$ (expect slow, linear behavior).\n    2. The integer iteration count for the multiplicity-scaled Newton solver to achieve $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n\n- Case B (near-flat slope at the root without multiplicity):\n  - Function: $f_2(x) = \\arctan(\\alpha x)$, derivative $f_2'(x) = \\frac{\\alpha}{1 + \\alpha^2 x^2}$, root $x^\\* = 0$.\n  - Angle unit: the output of $\\arctan(\\cdot)$ is in radians.\n  - Parameters: $\\alpha = 10^{-3}$, initial guess $x_0 = \\frac{1}{\\alpha}$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $N_{\\max} = 50$ for naive Newton and $N_{\\max} = 500$ for damped Newton.\n  - Compute two quantities:\n    1. A boolean (encoded as integer $1$ for true, $0$ for false) indicating whether the naive Newton solver converges within $N_{\\max}$ iterations to $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n    2. The integer iteration count for the damped Newton solver to achieve $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n\n- Case C (extremely flat slope near the root leading to large undamped steps):\n  - Function: $f_3(x) = \\arctan(\\alpha x)$, derivative $f_3'(x) = \\frac{\\alpha}{1 + \\alpha^2 x^2}$, root $x^\\* = 0$.\n  - Angle unit: the output of $\\arctan(\\cdot)$ is in radians.\n  - Parameters: $\\alpha = 10^{-4}$, initial guess $x_0 = \\frac{10}{\\alpha}$, tolerance $\\varepsilon = 10^{-12}$, maximum iterations $N_{\\max} = 50$ for naive Newton and $N_{\\max} = 2000$ for damped Newton.\n  - Compute two quantities:\n    1. A boolean (encoded as integer $1$ for true, $0$ for false) indicating whether the naive Newton solver converges within $N_{\\max}$ iterations to $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n    2. The integer iteration count for the damped Newton solver to achieve $\\lvert x_n - x^\\* \\rvert \\leq \\varepsilon$.\n\nScientific realism requirements:\n- All computations must use real numbers, with the inverse tangent $\\arctan$ returning values in radians.\n- Detect and treat nonfinite iterates (for example, not-a-number or infinity) as non-convergence in the naive Newton solver.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order:\n  - $[$Case A naive iterations, Case A scaled-step iterations, Case B naive-converged-boolean, Case B damped iterations, Case C naive-converged-boolean, Case C damped iterations$]$.\n- Each entry must be an integer.",
            "solution": "The user-provided problem statement has been evaluated and is determined to be valid. It is scientifically grounded in the principles of numerical analysis, specifically nonlinear root-finding algorithms. The problem is well-posed, providing all necessary functions, parameters, and termination criteria to produce a unique and verifiable set of results. The language is objective and formal. The problem structure is designed to explore known failure modes of Newton's method and the efficacy of standard remedial techniques (step scaling and damping), which is a classic and non-trivial topic in computational science.\n\nThe core of the problem is Newton's method for finding a root $x^\\*$ of a function $f(x)$, where $f(x^\\*) = 0$. The method is an iterative process based on the update rule:\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\nThis formula is derived from finding the root of the first-order Taylor expansion of $f(x)$ around $x_n$, which corresponds geometrically to finding the x-intercept of the tangent line to the curve at $(x_n, f(x_n))$.\n\nThe convergence behavior of Newton's method depends critically on the properties of $f(x)$ near the root $x^\\*$. For a simple root, where $f'(x^\\*) \\neq 0$, the method exhibits quadratic convergence, meaning the number of correct decimal places approximately doubles with each iteration, provided the initial guess $x_0$ is sufficiently close to $x^\\*$. However, its performance degrades under two conditions explored in this problem: multiple roots and regions of small derivative (flat regions).\n\n**1. Analysis of Naive and Scaled Newton for Multiple Roots (Case A)**\n\nFor Case A, the function is $f_1(x) = (x - 0.5)^2$. The root is $x^\\* = 0.5$ with multiplicity $m=2$, because both the function and its derivative, $f_1'(x) = 2(x - 0.5)$, are zero at $x^\\*$.\n\n- **Naive Newton Solver**: The iteration is:\n$$x_{n+1} = x_n - \\frac{(x_n - 0.5)^2}{2(x_n - 0.5)} = x_n - \\frac{1}{2}(x_n - 0.5)$$\nLet the error at step $n$ be $e_n = x_n - x^\\* = x_n - 0.5$. The update rule for the error becomes:\n$$x_{n+1} - 0.5 = (x_n - 0.5) - \\frac{1}{2}(x_n - 0.5)$$\n$$e_{n+1} = \\frac{1}{2} e_n$$\nThis demonstrates that the error is only halved at each step, a characteristic of linear convergence with a rate of $C = \\frac{m-1}{m} = \\frac{2-1}{2} = 0.5$. This is significantly slower than the quadratic convergence seen with simple roots.\n\n- **Multiplicity-Scaled Newton Solver**: By modifying the step size with the known multiplicity $m=2$, the update rule becomes:\n$$x_{n+1} = x_n - m \\frac{f(x_n)}{f'(x_n)} = x_n - 2 \\frac{(x_n - 0.5)^2}{2(x_n - 0.5)} = x_n - (x_n - 0.5) = 0.5$$\nThe method converges exactly to the root $x^\\* = 0.5$ in a single iteration, regardless of the initial guess $x_0$ (as long as $x_0 \\neq x^\\*$). This modification effectively restores quadratic convergence.\n\n**2. Analysis of Naive and Damped Newton for Near-Flat Regions (Case B and Case C)**\n\nFor Cases B and C, the function is $f(x) = \\arctan(\\alpha x)$, which has a simple root at $x^\\* = 0$. The derivative is $f'(x) = \\frac{\\alpha}{1 + (\\alpha x)^2}$. At the root, $f'(0) = \\alpha$. For the small values of $\\alpha$ given ($10^{-3}$ and $10^{-4}$), the derivative is very small near the root, creating a near-flat region.\n\n- **Naive Newton Solver**: The update step is $\\Delta = -\\frac{f(x_n)}{f'(x_n)} = -\\frac{\\arctan(\\alpha x_n)(1 + (\\alpha x_n)^2)}{\\alpha}$.\nWhen the iterate $x_n$ is far from the root, such that $|\\alpha x_n| \\gg 1$, the term $\\arctan(\\alpha x_n)$ approaches its asymptotic value of $\\pm \\frac{\\pi}{2}$. The step becomes approximately:\n$$\\Delta \\approx -\\frac{(\\pm \\frac{\\pi}{2})(1 + (\\alpha x_n)^2)}{\\alpha}$$\nThe term $(\\alpha x_n)^2$ in the numerator causes the step size $|\\Delta|$ to be extremely large. For the initial guesses $x_0 = c/\\alpha$, this leads to a massive first step that overshoots the root region entirely.\n  - In Case B, $x_0 = 1/\\alpha$, so $\\alpha x_0 = 1$. The first step is large but does not cause immediate divergence; instead, it leads to oscillations around the root, failing to converge within the iteration limit.\n  - In Case C, $x_0 = 10/\\alpha$, so $\\alpha x_0 = 10$. The first step is so large that the magnitude of the next iterate, $|x_1|$, is much larger than $|x_0|$. This is a signature of divergence.\n\n- **Damped Newton Solver with Backtracking**: This method mitigates the overshooting problem. It first computes the full, potentially very large, Newton step $\\Delta$. It then introduces a scaling factor $\\lambda \\in (0, 1]$, setting the new iterate as $x_{n+1} = x_n + \\lambda \\Delta$. The key is the line search strategy: $\\lambda$ is initialized to $1$ and repeatedly halved until the condition $|f(x_n + \\lambda \\Delta)| < |f(x_n)|$ is met. This condition, known as the Armijo-Goldstein condition in a simplified form, ensures that each step makes progress by reducing the function's absolute value (a merit function). By systematically reducing the step length, the damped method avoids the uncontrolled jumps of the naive method and forces convergence, albeit potentially requiring many iterations if $\\lambda$ must be made very small. This \"rescue\" mechanism makes the damped method far more robust for functions with flat regions or when starting far from the root.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs numerical solvers for three root-finding test cases.\n    \"\"\"\n\n    # --- Solver Implementations ---\n\n    def newton_naive(f, df, x0, x_star, tol, max_iter):\n        \"\"\"\n        Naive Newton's method.\n        Returns the number of iterations to converge, or max_iter + 1 on failure.\n        \"\"\"\n        x = float(x0)\n        \n        # Check if starting point is already at the root\n        if abs(x - x_star) = tol:\n            return 0\n\n        for i in range(1, max_iter + 1):\n            f_x = f(x)\n            df_x = df(x)\n\n            # Handle division by zero or very small derivative\n            if abs(df_x)  1e-20:\n                return max_iter + 1\n\n            step = f_x / df_x\n            x_new = x - step\n\n            # Handle non-finite results (divergence)\n            if not np.isfinite(x_new):\n                return max_iter + 1\n            \n            x = x_new\n            \n            if abs(x - x_star) = tol:\n                return i\n        \n        return max_iter + 1\n\n    def newton_scaled(f, df, x0, x_star, m, tol, max_iter):\n        \"\"\"\n        Newton's method with step scaling for known root multiplicity m.\n        Returns the number of iterations to converge, or max_iter + 1 on failure.\n        \"\"\"\n        x = float(x0)\n\n        if abs(x - x_star) = tol:\n            return 0\n\n        for i in range(1, max_iter + 1):\n            f_x = f(x)\n            df_x = df(x)\n\n            if abs(df_x)  1e-20:\n                return max_iter + 1\n            \n            step = m * f_x / df_x\n            x = x - step\n\n            if abs(x - x_star) = tol:\n                return i\n        \n        return max_iter + 1\n\n    def newton_damped(f, df, x0, x_star, tol, max_iter):\n        \"\"\"\n        Damped Newton's method with backtracking line search.\n        Returns the number of iterations to converge, or max_iter + 1 on failure.\n        \"\"\"\n        x = float(x0)\n\n        if abs(x - x_star) = tol:\n            return 0\n\n        for i in range(1, max_iter + 1):\n            f_x = f(x)\n            df_x = df(x)\n\n            if abs(df_x)  1e-20:\n                return max_iter + 1\n            \n            delta = -f_x / df_x\n            \n            lambd = 1.0\n            abs_f_x = abs(f_x)\n\n            # Backtracking line search\n            # Keep halving lambda until the step reduces the function's absolute value\n            while abs(f(x + lambd * delta)) >= abs_f_x:\n                lambd /= 2.0\n                # Failsafe for an overly restrictive search\n                if lambd  1e-15:\n                    return max_iter + 1\n\n            x = x + lambd * delta\n\n            if abs(x - x_star) = tol:\n                return i\n        \n        return max_iter + 1\n\n    # --- Problem Definitions  Execution ---\n\n    results = []\n\n    # Case A: Multiple root f(x) = (x - 0.5)^2\n    f1 = lambda x: (x - 0.5)**2\n    df1 = lambda x: 2 * (x - 0.5)\n    x_star1 = 0.5\n    x0_A = 2.0\n    tol_A = 1e-12\n    n_max_A = 200\n\n    iters_A_naive = newton_naive(f1, df1, x0_A, x_star1, tol_A, n_max_A)\n    results.append(iters_A_naive)\n    \n    iters_A_scaled = newton_scaled(f1, df1, x0_A, x_star1, 2, tol_A, n_max_A)\n    results.append(iters_A_scaled)\n\n\n    # Case B: Near-flat slope, f(x) = arctan(alpha*x)\n    alpha_B = 1e-3\n    f2 = lambda x: np.arctan(alpha_B * x)\n    df2 = lambda x: alpha_B / (1 + (alpha_B * x)**2)\n    x_star2 = 0.0\n    x0_B = 1.0 / alpha_B\n    tol_B = 1e-12\n    n_max_B_naive = 50\n    n_max_B_damped = 500\n\n    iters_B_naive = newton_naive(f2, df2, x0_B, x_star2, tol_B, n_max_B_naive)\n    converged_B_naive = 1 if iters_B_naive = n_max_B_naive else 0\n    results.append(converged_B_naive)\n\n    iters_B_damped = newton_damped(f2, df2, x0_B, x_star2, tol_B, n_max_B_damped)\n    results.append(iters_B_damped)\n\n\n    # Case C: Extremely flat slope, f(x) = arctan(alpha*x)\n    alpha_C = 1e-4\n    f3 = lambda x: np.arctan(alpha_C * x)\n    df3 = lambda x: alpha_C / (1 + (alpha_C * x)**2)\n    x_star3 = 0.0\n    x0_C = 10.0 / alpha_C\n    tol_C = 1e-12\n    n_max_C_naive = 50\n    n_max_C_damped = 2000\n\n    iters_C_naive = newton_naive(f3, df3, x0_C, x_star3, tol_C, n_max_C_naive)\n    converged_C_naive = 1 if iters_C_naive = n_max_C_naive else 0\n    results.append(converged_C_naive)\n\n    iters_C_damped = newton_damped(f3, df3, x0_C, x_star3, tol_C, n_max_C_damped)\n    results.append(iters_C_damped)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on an understanding of robustness in one dimension, we now elevate the challenge to a multidimensional system drawn from nonlinear continuum mechanics. This practice involves finding the equilibrium configurations of a material by locating the stationary points of its non-convex energy functional . You will implement a multidimensional Newton solver equipped with a line search, a critical tool for ensuring convergence on complex energy landscapes, and explore the concept of attraction basins, which is fundamental to modeling phenomena like phase transitions and mechanical instability.",
            "id": "3486017",
            "problem": "Consider the elastic energy density in finite deformation for a two-dimensional material given by $W(F) = \\mu \\|F\\|_F^2 + \\lambda (\\det F - 1)^2$, where $F \\in \\mathbb{R}^{2 \\times 2}$ is the deformation gradient, $\\|F\\|_F$ is the Frobenius norm, $\\det F$ is the determinant, and $\\mu  0$, $\\lambda  0$ are material constants. The stationary points of the energy satisfy the nonlinear matrix equation $\\nabla_F W(F) = 0$, which can be treated as a root-finding problem in $\\mathbb{R}^4$ by flattening $F$ into a vector of its entries. Your tasks are as follows:\n\n- Starting from the foundational calculus facts that the derivative of the determinant satisfies $\\frac{\\partial \\det(F)}{\\partial F_{ij}} = \\operatorname{Cof}(F)_{ij}$, where $\\operatorname{Cof}(F)$ is the cofactor matrix, and that the gradient of a scalar field composed with a matrix argument can be taken entry-wise, derive the expression for the gradient $\\nabla_F W(F)$ and the Jacobian of the nonlinear system associated with $\\nabla_F W(F) = 0$ when $F = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$. Do not use or assume any pre-derived shortcut formulas: work from the definitions of the Frobenius norm, the determinant, and the cofactor.\n\n- Design and implement a Newton iteration to solve the $4$-dimensional nonlinear system $\\nabla_F W(F) = 0$, using a merit function based on the residual norm and an Armijo backtracking line search to improve robustness. Explicitly implement a backtracking line search that ensures a sufficient decrease in the merit function. Map the basins of attraction by classifying the converged stationary point. Use the following classification: return $0$ if the iterate converges to the trivial stationary point (numerically characterized by $\\|F\\|_F \\leq \\varepsilon$ for a small tolerance $\\varepsilon$), return $1$ if the iterate converges to the scaled-rotation manifold (numerically characterized by $F \\approx \\operatorname{Cof}(F)$ and $\\det F \\approx r^\\star$ with $r^\\star := 1 - \\mu/\\lambda$), and return $2$ otherwise. The presence of rank-one connectivity should be probed by initializing along rank-one directions $u \\otimes v$ and observing the basin to which the iteration converges.\n\n- Implement a second run for each test case using plain Newton’s method without line search to expose potential failure modes on the nonconvex landscape. Report whether the plain method converged (boolean).\n\nUse the material parameters $\\mu = 0.5$ and $\\lambda = 1.0$. Angles must be expressed in radians. Construct the following test suite of initial deformation gradients:\n1. A perturbation near the scaled-rotation manifold with $a = \\sqrt{1 - \\mu/\\lambda}$: $F_1 = \\begin{pmatrix} a  10^{-4} \\\\ -10^{-4}  a \\end{pmatrix}$, where $a = \\sqrt{0.5}$.\n2. Exactly on the scaled-rotation manifold at zero angle: $F_2 = \\begin{pmatrix} a  0 \\\\ 0  a \\end{pmatrix}$ with $a = \\sqrt{0.5}$.\n3. A small-amplitude rank-one start: $F_3 = u \\otimes v$ with $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, scaled by $10^{-3}$, i.e., $F_3 = \\begin{pmatrix} 0  10^{-3} \\\\ 0  0 \\end{pmatrix}$.\n4. A moderate-amplitude rank-one start: $F_4 = \\alpha \\, u \\otimes v$ with $\\alpha = 0.2$, $u = \\begin{pmatrix} \\cos(\\pi/4) \\\\ \\sin(\\pi/4) \\end{pmatrix}$, $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, i.e., $F_4 = \\begin{pmatrix} 0.14142136  0 \\\\ 0.14142136  0 \\end{pmatrix}$.\n5. A far start with negative determinant: $F_5 = \\begin{pmatrix} -1.5  0.7 \\\\ 0.9  -0.1 \\end{pmatrix}$.\n\nFor each test case, run both solvers (plain Newton and Newton with backtracking line search), and produce as the final result for that test case a list of three values: \n- a boolean indicating whether plain Newton converged within tolerance,\n- an integer classification of the converged solution when using backtracking line search ($0$, $1$, or $2$ as defined above),\n- a float giving the final residual norm (the Euclidean norm of $\\nabla_F W(F)$) for the line-search run, rounded to six decimal places.\n\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list of these three-entry lists enclosed in square brackets (e.g., \"[[true_or_false,classification,residual],...]\" with the residual expressed as a decimal number). There are no physical units involved; angles must be in radians as specified.",
            "solution": "The problem requires the derivation of the governing equations for the stationary points of a given elastic energy density and the implementation of a numerical solver to find these points. We shall proceed by first validating the problem, then presenting the required analytical derivations, and finally detailing the numerical algorithm.\n\nThe problem is deemed valid. It is scientifically grounded in the principles of nonlinear continuum mechanics, specifically concerning hyperelastic material models. The energy functional $W(F) = \\mu \\|F\\|_F^2 + \\lambda (\\det F - 1)^2$ is a well-defined, differentiable function on the space of $2 \\times 2$ matrices, representing a simple model that penalizes both deviation from rigidity (via the Frobenius norm term) and changes in volume (via the determinant term). The task of finding stationary points $\\nabla_F W(F) = 0$ is a standard problem in computational mechanics, addressed by root-finding algorithms. The problem is well-posed, providing all necessary definitions, material parameters, and initial conditions for a numerical investigation. It is objective and free of ambiguity.\n\n### 1. Derivation of the Gradient and Jacobian\n\nLet the deformation gradient be denoted by $F \\in \\mathbb{R}^{2 \\times 2}$ with components:\n$$\nF = \\begin{pmatrix} F_{11}  F_{12} \\\\ F_{21}  F_{22} \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\n$$\nThe elastic energy density is given by:\n$$\nW(F) = \\mu \\|F\\|_F^2 + \\lambda (\\det F - 1)^2\n$$\nwhere $\\mu  0$ and $\\lambda  0$ are material constants.\n\nIn terms of the components $\\{a, b, c, d\\}$, the Frobenius norm squared is $\\|F\\|_F^2 = a^2 + b^2 + c^2 + d^2$ and the determinant is $\\det F = ad - bc$. The energy function can be written as:\n$$\nW(a,b,c,d) = \\mu (a^2 + b^2 + c^2 + d^2) + \\lambda (ad - bc - 1)^2\n$$\nThe stationary points of $W$ are found where its gradient with respect to $F$ is zero. The gradient, $\\nabla_F W(F)$, is a $2 \\times 2$ matrix whose $(i,j)$ entry is $\\frac{\\partial W}{\\partial F_{ij}}$.\n\nWe compute the gradient by applying the chain rule. The gradient of the first term is:\n$$\n\\nabla_F (\\mu \\|F\\|_F^2) = \\nabla_F \\left(\\mu \\sum_{i,j=1}^2 F_{ij}^2\\right) = 2\\mu F\n$$\nFor the second term, we use the provided rule $\\frac{\\partial \\det(F)}{\\partial F_{ij}} = \\operatorname{Cof}(F)_{ij}$. The cofactor matrix for $F$ is:\n$$\n\\operatorname{Cof}(F) = \\begin{pmatrix} d  -c \\\\ -b  a \\end{pmatrix}\n$$\nThe gradient of the second term is:\n$$\n\\nabla_F (\\lambda(\\det F - 1)^2) = 2\\lambda(\\det F - 1) \\nabla_F(\\det F) = 2\\lambda(\\det F - 1) \\operatorname{Cof}(F)\n$$\nCombining these, the full gradient of the energy function is:\n$$\n\\nabla_F W(F) = 2\\mu F + 2\\lambda(\\det F - 1) \\operatorname{Cof}(F)\n$$\nThe nonlinear equation to be solved is $\\nabla_F W(F) = 0$. Let us define the residual function $G(F) = \\nabla_F W(F)$. To implement a numerical solver, we flatten the matrix $F$ into a vector $x = (a, b, c, d)^T \\in \\mathbb{R}^4$. The matrix equation $G(F)=0$ becomes a vector equation $g(x)=0$, where $g(x)$ is the flattened version of $G(F)$.\nSubstituting the component expressions for $F$ and $\\operatorname{Cof}(F)$, we get:\n$$\ng(x) = \\begin{pmatrix} g_1 \\\\ g_2 \\\\ g_3 \\\\ g_4 \\end{pmatrix} = \\begin{pmatrix}\n2\\mu a + 2\\lambda(ad-bc-1)d \\\\\n2\\mu b + 2\\lambda(ad-bc-1)(-c) \\\\\n2\\mu c + 2\\lambda(ad-bc-1)(-b) \\\\\n2\\mu d + 2\\lambda(ad-bc-1)a\n\\end{pmatrix} = \\begin{pmatrix}\n2\\mu a + 2\\lambda(ad-bc-1)d \\\\\n2\\mu b - 2\\lambda(ad-bc-1)c \\\\\n2\\mu c - 2\\lambda(ad-bc-1)b \\\\\n2\\mu d + 2\\lambda(ad-bc-1)a\n\\end{pmatrix}\n$$\nThe Jacobian of this system, $J_g(x)$, is the $4 \\times 4$ matrix of partial derivatives, $J_{ij} = \\frac{\\partial g_i}{\\partial x_j}$, where $x = (x_1, x_2, x_3, x_4)^T = (a, b, c, d)^T$. Let $S = ad-bc-1$.\nThe components of the Jacobian are:\n- Row 1 ($g_1 = 2\\mu a + 2\\lambda S d$):\n  - $\\frac{\\partial g_1}{\\partial a} = 2\\mu + 2\\lambda (\\frac{\\partial S}{\\partial a}d) = 2\\mu + 2\\lambda(d)d = 2\\mu + 2\\lambda d^2$\n  - $\\frac{\\partial g_1}{\\partial b} = 2\\lambda (\\frac{\\partial S}{\\partial b}d) = 2\\lambda(-c)d = -2\\lambda cd$\n  - $\\frac{\\partial g_1}{\\partial c} = 2\\lambda (\\frac{\\partial S}{\\partial c}d) = 2\\lambda(-b)d = -2\\lambda bd$\n  - $\\frac{\\partial g_1}{\\partial d} = 2\\lambda(S \\cdot 1 + \\frac{\\partial S}{\\partial d}d) = 2\\lambda(S + ad) = 2\\lambda(ad-bc-1+ad) = 2\\lambda(2ad-bc-1)$\n- Row 2 ($g_2 = 2\\mu b - 2\\lambda S c$):\n  - $\\frac{\\partial g_2}{\\partial a} = -2\\lambda(\\frac{\\partial S}{\\partial a}c) = -2\\lambda(d)c = -2\\lambda cd$\n  - $\\frac{\\partial g_2}{\\partial b} = 2\\mu - 2\\lambda(\\frac{\\partial S}{\\partial b}c) = 2\\mu - 2\\lambda(-c)c = 2\\mu + 2\\lambda c^2$\n  - $\\frac{\\partial g_2}{\\partial c} = -2\\lambda(S \\cdot 1 + \\frac{\\partial S}{\\partial c}c) = -2\\lambda(S - bc) = -2\\lambda(ad-2bc-1)$\n  - $\\frac{\\partial g_2}{\\partial d} = -2\\lambda(\\frac{\\partial S}{\\partial d}c) = -2\\lambda(a)c = -2\\lambda ac$\n- By symmetry (e.g. $\\frac{\\partial g_3}{\\partial a} = \\frac{\\partial g_1}{\\partial c}$ requires swapping $b \\leftrightarrow c$) or direct calculation, we can find the remaining rows. The resulting Jacobian is symmetric, as it is the Hessian of the scalar potential $W$. The full Jacobian matrix is:\n$$\nJ_g(x) = 2 \\begin{pmatrix}\n\\mu + \\lambda d^2  -\\lambda cd  -\\lambda bd  \\lambda(2ad-bc-1) \\\\\n-\\lambda cd  \\mu + \\lambda c^2  -\\lambda(ad-2bc-1)  -\\lambda ac \\\\\n-\\lambda bd  -\\lambda(ad-2bc-1)  \\mu + \\lambda b^2  -\\lambda ab \\\\\n\\lambda(2ad-bc-1)  -\\lambda ac  -\\lambda ab  \\mu + \\lambda a^2\n\\end{pmatrix}\n$$\n\n### 2. Newton's Method with Backtracking Line Search\n\nTo solve the nonlinear system $g(x) = 0$, we employ Newton's method. Given an iterate $x_k$, the next iterate $x_{k+1}$ is found by solving the linearized system for a step $\\Delta x_k$:\n$$\nJ_g(x_k) \\Delta x_k = -g(x_k)\n$$\nA plain Newton update would be $x_{k+1} = x_k + \\Delta x_k$. However, for a nonconvex energy landscape such as this one, the iterates can easily be sent to regions where the local quadratic approximation is poor, causing divergence. To globalize the method and improve robustness, we introduce a step length $\\alpha_k \\in (0, 1]$ and update as $x_{k+1} = x_k + \\alpha_k \\Delta x_k$.\n\nThe step length $\\alpha_k$ is determined by a backtracking line search that ensures sufficient decrease in a merit function. A suitable merit function is $m(x) = \\frac{1}{2}\\|g(x)\\|_2^2$. The Newton step $\\Delta x_k$ is a descent direction for this function. We seek $\\alpha_k$ satisfying the Armijo condition:\n$$\nm(x_k + \\alpha_k \\Delta x_k) \\leq m(x_k) + c_1 \\alpha_k \\nabla m(x_k)^T \\Delta x_k\n$$\nwhere $c_1$ is a small constant, typically $10^{-4}$. Noting that $\\nabla m(x_k)^T \\Delta x_k = g(x_k)^T J_g(x_k) \\Delta x_k = -g(x_k)^T g(x_k) = -\\|g(x_k)\\|_2^2$, the condition becomes:\n$$\n\\frac{1}{2}\\|g(x_k + \\alpha_k \\Delta x_k)\\|_2^2 \\leq \\frac{1}{2}\\|g(x_k)\\|_2^2 - c_1 \\alpha_k \\|g(x_k)\\|_2^2\n$$\nThe line search algorithm starts with $\\alpha_k=1$ and repeatedly multiplies it by a backtracking factor $\\tau \\in (0, 1)$ (e.g., $\\tau=0.5$) until this condition is met.\n\n### 3. Classification of Stationary Points\n\nOnce the iteration converges to a solution $F_c$, we classify it.\n- **Class 0 (Trivial):** The zero matrix $F=0$ is a stationary point, since $G(0) = 2\\mu(0) + 2\\lambda(0-1)\\operatorname{Cof}(0) = 0$. Numerically, we classify a solution as trivial if $\\|F_c\\|_F \\leq \\varepsilon$ for a small tolerance $\\varepsilon$.\n- **Class 1 (Scaled-Rotation Manifold):** This class represents physically meaningful, non-degenerate solutions. From the condition $G(F)=0$, we have $F = -\\frac{\\lambda}{\\mu}(\\det F - 1)\\operatorname{Cof}(F)$. A key family of solutions satisfies $F = k \\, \\operatorname{Cof}(F)$. This implies $k = \\pm 1$. If $k=1$, then $F=\\operatorname{Cof}(F)$, which is the condition for a matrix to be a scaled rotation (i.e., $F_{11}=F_{22}$ and $F_{12}=-F_{21}$). Substituting $F=\\operatorname{Cof}(F)$ into the stationary condition gives $F = -\\frac{\\lambda}{\\mu}(\\det F - 1)F$, which for $F \\neq 0$ implies $1 = -\\frac{\\lambda}{\\mu}(\\det F - 1)$, or $\\det F = 1 - \\mu/\\lambda$. For the given parameters $\\mu=0.5, \\lambda=1.0$, this gives $\\det F = 0.5$. Numerically, we classify a solution as Class 1 if it satisfies both $\\|F_c - \\operatorname{Cof}(F_c)\\|_F \\leq \\varepsilon_{approx}$ and $|\\det F_c - (1-\\mu/\\lambda)| \\leq \\varepsilon_{approx}$.\n- **Class 2 (Other):** Any converged solution that does not fit into Class 0 or Class 1. This may include other stationary points or saddle points of the energy functional.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for stationary points of an elastic energy functional using Newton's method\n    and classifies the basins of attraction.\n    \"\"\"\n    mu = 0.5\n    lmbda = 1.0\n\n    # Define test cases\n    a_sqrt_half = np.sqrt(0.5)\n    test_cases = [\n        np.array([[a_sqrt_half, 1e-4], [-1e-4, a_sqrt_half]]),\n        np.array([[a_sqrt_half, 0.0], [0.0, a_sqrt_half]]),\n        np.array([[0.0, 1e-3], [0.0, 0.0]]),\n        np.array([[0.14142136, 0.0], [0.14142136, 0.0]]),\n        np.array([[-1.5, 0.7], [0.9, -0.1]])\n    ]\n\n    results = []\n    for F_initial in test_cases:\n        x_initial = F_initial.flatten()\n\n        # Run plain Newton's method\n        plain_converged, _ = newton_solver(x_initial, mu, lmbda, use_line_search=False)\n\n        # Run Newton's method with backtracking line search\n        line_search_converged, x_final = newton_solver(x_initial, mu, lmbda, use_line_search=True)\n        \n        F_final = x_final.reshape((2, 2))\n        final_residual_norm = np.linalg.norm(residual_g(x_final, mu, lmbda))\n\n        # Classify the solution from the line search run\n        classification = classify_solution(F_final, mu, lmbda)\n        \n        # Round the residual as required\n        rounded_residual = round(final_residual_norm, 6)\n\n        results.append([plain_converged, classification, rounded_residual])\n\n    # Format and print the final output\n    # Using a custom formatter to avoid trailing zeros for integers\n    def format_item(item):\n        if isinstance(item, bool):\n            return str(item).lower()\n        if isinstance(item, int):\n            return str(item)\n        if isinstance(item, float):\n            return f\"{item:.6f}\"\n        return str(item)\n\n    result_str = \"[\" + \",\".join(\n        \"[\" + \",\".join(map(format_item, r)) + \"]\" for r in results\n    ) + \"]\"\n    print(result_str)\n\ndef residual_g(x, mu, lmbda):\n    \"\"\"Computes the residual vector g(x).\"\"\"\n    a, b, c, d = x\n    det_F = a * d - b * c\n    S = det_F - 1\n    g = np.zeros(4)\n    g[0] = 2 * mu * a + 2 * lmbda * S * d\n    g[1] = 2 * mu * b - 2 * lmbda * S * c\n    g[2] = 2 * mu * c - 2 * lmbda * S * b\n    g[3] = 2 * mu * d + 2 * lmbda * S * a\n    return g\n\ndef jacobian_J(x, mu, lmbda):\n    \"\"\"Computes the Jacobian matrix J(x).\"\"\"\n    a, b, c, d = x\n    det_F = a * d - b * c\n    J = np.zeros((4, 4))\n    \n    # Row 1\n    J[0, 0] = 2 * mu + 2 * lmbda * d**2\n    J[0, 1] = -2 * lmbda * c * d\n    J[0, 2] = -2 * lmbda * b * d\n    J[0, 3] = 2 * lmbda * (2 * a * d - b * c - 1)\n    \n    # Row 2\n    J[1, 0] = -2 * lmbda * c * d\n    J[1, 1] = 2 * mu + 2 * lmbda * c**2\n    J[1, 2] = -2 * lmbda * (a * d - 2 * b * c - 1)\n    J[1, 3] = -2 * lmbda * a * c\n    \n    # Row 3\n    J[2, 0] = -2 * lmbda * b * d\n    J[2, 1] = -2 * lmbda * (a * d - 2 * b * c - 1)\n    J[2, 2] = 2 * mu + 2 * lmbda * b**2\n    J[2, 3] = -2 * lmbda * a * b\n    \n    # Row 4\n    J[3, 0] = 2 * lmbda * (2 * a * d - b * c - 1)\n    J[3, 1] = -2 * lmbda * a * c\n    J[3, 2] = -2 * lmbda * a * b\n    J[3, 3] = 2 * mu + 2 * lmbda * a**2\n    \n    return J\n\ndef newton_solver(x0, mu, lmbda, use_line_search=True, max_iter=100, tol=1e-8, c1=1e-4, tau=0.5):\n    \"\"\"\n    Newton's method solver, with optional backtracking line search.\n    \"\"\"\n    x = np.copy(x0)\n    for _ in range(max_iter):\n        g = residual_g(x, mu, lmbda)\n        if np.linalg.norm(g)  tol:\n            return True, x\n        \n        J = jacobian_J(x, mu, lmbda)\n        \n        try:\n            delta_x = np.linalg.solve(J, -g)\n        except np.linalg.LinAlgError:\n            # Singular Jacobian\n            return False, x\n\n        if use_line_search:\n            alpha = 1.0\n            g_norm_sq = np.dot(g, g)\n            # Armijo condition check\n            while True:\n                x_new = x + alpha * delta_x\n                g_new = residual_g(x_new, mu, lmbda)\n                g_new_norm_sq = np.dot(g_new, g_new)\n                if g_new_norm_sq = g_norm_sq - 2 * c1 * alpha * g_norm_sq:\n                    break\n                alpha *= tau\n                if alpha  1e-8: # Prevent infinite loop\n                    return False, x\n            x += alpha * delta_x\n        else: # Plain Newton\n            x += delta_x\n        \n        # Check for divergence in plain Newton mode for stability\n        if not use_line_search and np.linalg.norm(x) > 1e10:\n             return False, x\n\n    # Check for convergence after max_iter\n    g_final = residual_g(x, mu, lmbda)\n    converged = np.linalg.norm(g_final)  tol\n    return converged, x\n\ndef classify_solution(F, mu, lmbda, tol_trivial=1e-6, tol_approx=1e-6):\n    \"\"\"\n    - 0: trivial stationary point\n    - 1: scaled-rotation manifold\n    - 2: other\n    \"\"\"\n    # Class 0: Trivial solution\n    if np.linalg.norm(F, 'fro') = tol_trivial:\n        return 0\n\n    # Class 1: Scaled-rotation manifold\n    a, b, c, d = F.flatten()\n    Cof_F = np.array([[d, -c], [-b, a]])\n    \n    det_F = a * d - b * c\n    r_star = 1.0 - mu / lmbda\n    \n    is_cof_match = np.linalg.norm(F - Cof_F, 'fro') = tol_approx\n    is_det_match = abs(det_F - r_star) = tol_approx\n    \n    if is_cof_match and is_det_match:\n        return 1\n\n    # Class 2: Other\n    return 2\n\nsolve()\n```"
        },
        {
            "introduction": "Many frontier problems in materials science involve systems with thousands or even millions of degrees of freedom, where forming and inverting a Jacobian matrix is computationally infeasible. This final practice introduces the powerful class of quasi-Newton methods designed for such large-scale challenges . You will implement a limited-memory Broyden solver, learning how to construct and apply an approximate inverse Jacobian \"on the fly\" using only a history of recent steps, a technique that is a cornerstone of modern computational physics and materials modeling.",
            "id": "3485986",
            "problem": "Consider a large parameter estimation problem that arises in computational materials science, framed as the recovery of defect level energies in a crystalline solid from observed site occupancies. For each site index $i \\in \\{1,\\dots,N\\}$, the occupancy $f_i$ at temperature $T$ and chemical potential $\\mu$ is modeled by the Fermi–Dirac distribution, a well-tested formula from statistical mechanics: $$f_i(E_i;\\mu,T) = \\frac{1}{1+\\exp\\left(\\frac{E_i - \\mu}{k_B T}\\right)},$$ where $E_i$ is the defect level energy at site $i$ and $k_B$ is the Boltzmann constant. Given a vector of observed occupancies $\\mathbf{f}^{\\mathrm{obs}} \\in \\mathbb{R}^N$, the parameter estimation problem is to find an energy vector $\\mathbf{x} \\in \\mathbb{R}^N$ such that $$F(\\mathbf{x}) = \\left(f_i(x_i;\\mu,T) - f_i^{\\mathrm{obs}}\\right)_{i=1}^N = \\mathbf{0},$$ thereby matching modeled and observed occupancies. Energies must be expressed in electronvolts $\\mathrm{eV}$ and temperature in Kelvin $K$. Occupancies are dimensionless.\n\nYou will implement a root-finding solver based on a limited-memory Broyden strategy (Limited-memory Broyden (L-Broyden)), suitable for high-dimensional nonlinear systems. The solver should be designed to avoid forming dense Jacobian or inverse Jacobian matrices. Instead, it must maintain only the most recent $m$ vector pairs $(s_k, y_k)$, where $s_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and $y_k = F(\\mathbf{x}_{k+1}) - F(\\mathbf{x}_k)$, and it must compute the action of the current inverse-Jacobian approximation on an arbitrary vector using these stored pairs and an initial scalar multiple of the identity. Start from the fundamental secant condition for quasi-Newton updates and principles that ensure minimal change to the inverse operator while satisfying the secant equation; derive the structure needed to store and update $(s_k,y_k)$ efficiently in limited memory. Your program must include a line search that ensures monotonic decrease of the merit function $$\\phi(\\mathbf{x}) = \\frac{1}{2}\\|F(\\mathbf{x})\\|_2^2,$$ and it must stop when the infinity norm $\\|F(\\mathbf{x})\\|_\\infty$ is below the specified tolerance.\n\nConstruct synthetic, but scientifically plausible, test instances as follows. Use a deterministic energy field $$E_i^{\\star} = 0.4 + 0.25\\sin\\left(\\frac{2\\pi i}{N}\\right) + 0.15\\cos\\left(\\frac{4\\pi i}{N}\\right) \\quad \\text{for}\\quad i=1,\\dots,N,$$ expressed in $\\mathrm{eV}$. Generate observed occupancies by $f_i^{\\mathrm{obs}} = f_i(E_i^{\\star};\\mu,T)$ with the given $T$ and $\\mu$. Initialize the solver at $\\mathbf{x}_0 = \\mathbf{E}^{\\star} + \\delta\\,\\mathbf{1}$, where $\\delta$ is a scalar offset in $\\mathrm{eV}$. Use $k_B = 8.617333262145\\times 10^{-5}\\,\\mathrm{eV}/K$.\n\nImplement the L-Broyden solver that:\n- Maintains at most $m$ recent pairs $(s_k,y_k)$.\n- Computes search directions by applying the current limited-memory inverse-Jacobian approximation to $F(\\mathbf{x}_k)$ without ever forming dense matrices.\n- Employs a backtracking line search that ensures $\\phi(\\mathbf{x}_{k+1})  \\phi(\\mathbf{x}_k)$.\n- Terminates when $\\|F(\\mathbf{x}_k)\\|_\\infty  \\text{tol}$ or after a maximum number of iterations.\n\nYour program must run the following test suite and report results in the specified format:\n- Test $1$ (general case): $N=200$, $m=10$, $\\mu=0.4\\,\\mathrm{eV}$, $T=900\\,K$, $\\delta=0.2\\,\\mathrm{eV}$, $\\text{tol}=10^{-10}$, $\\text{maxiter}=200$.\n- Test $2$ (boundary, no memory): $N=300$, $m=0$, $\\mu=0.4\\,\\mathrm{eV}$, $T=900\\,K$, $\\delta=0.2\\,\\mathrm{eV}$, $\\text{tol}=10^{-8}$, $\\text{maxiter}=250$.\n- Test $3$ (edge, steep nonlinearity): $N=150$, $m=5$, $\\mu=0.1\\,\\mathrm{eV}$, $T=300\\,K$, $\\delta=-0.1\\,\\mathrm{eV}$, $\\text{tol}=10^{-8}$, $\\text{maxiter}=300$.\n\nFor each test case, return three items: a boolean indicating whether convergence was achieved (defined by $\\|F(\\mathbf{x})\\|_\\infty  \\text{tol}$), the final value of $\\|F(\\mathbf{x})\\|_\\infty$ (a float), and the number of iterations used (an integer). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of tests $1$, $2$, and $3$, for example, $$[\\text{conv}_1,\\|F\\|_{\\infty,1},\\text{iter}_1,\\text{conv}_2,\\|F\\|_{\\infty,2},\\text{iter}_2,\\text{conv}_3,\\|F\\|_{\\infty,3},\\text{iter}_3].$$",
            "solution": "The problem requires the implementation of a limited-memory Broyden (L-Broyden) solver for a high-dimensional parameter estimation problem rooted in statistical mechanics. The problem is scientifically valid and well-posed. Its scientific grounding is in the Fermi-Dirac distribution, a fundamental concept used to model site occupancies in materials. The problem is well-posed because the underlying function is monotonic for each component, guaranteeing a unique solution for any physically reasonable input.\n\nThis task is structured as a pedagogical exercise to build a non-trivial numerical algorithm. While the system of equations is diagonal (and thus each component could be solved independently), this structure serves as a clean testbed for the main goal: implementing a general-purpose, large-scale solver. The focus is on the correct implementation of the L-Broyden method, a cornerstone of modern scientific computing, rather than the complexity of the physical model itself.\n\n### Solution Derivation\n\nThe problem requires solving the nonlinear system of equations $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$, where the $i$-th component is given by:\n$$F_i(\\mathbf{x}) = \\frac{1}{1+\\exp\\left(\\frac{x_i - \\mu}{k_B T}\\right)} - f_i^{\\mathrm{obs}} = 0$$\nHere, $\\mathbf{x} \\in \\mathbb{R}^N$ is the vector of defect energies to be found. This is a root-finding problem. We will implement a limited-memory quasi-Newton method, specifically the \"good\" Broyden's method, as requested.\n\nA quasi-Newton method approximates the Jacobian matrix $J(\\mathbf{x})$ or its inverse $H(\\mathbf{x}) = J(\\mathbf{x})^{-1}$. The iteration proceeds as:\n$$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$$\nwhere $\\mathbf{p}_k$ is the search direction and $\\alpha_k$ is a step length determined by a line search. The search direction is computed by solving the linear system $B_k \\mathbf{p}_k = -\\mathbf{F}(\\mathbf{x}_k)$, or more directly, by $\\mathbf{p}_k = -H_k \\mathbf{F}(\\mathbf{x}_k)$, where $B_k \\approx J(\\mathbf{x}_k)$ and $H_k \\approx J(\\mathbf{x}_k)^{-1}$.\n\nThe inverse Hessian approximation $H_k$ is updated at each step to satisfy the secant equation:\n$$H_{k+1} \\mathbf{y}_k = \\mathbf{s}_k$$\nwhere $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$ and $\\mathbf{y}_k = \\mathbf{F}(\\mathbf{x}_{k+1}) - \\mathbf{F}(\\mathbf{x}_k)$.\n\nBroyden's \"good\" method provides an update for $H_k$ that satisfies the secant equation and minimizes the Frobenius norm of the change, $\\|H_{k+1} - H_k\\|_F$. The update formula is:\n$$H_{k+1} = H_k + \\frac{(\\mathbf{s}_k - H_k \\mathbf{y}_k)\\mathbf{y}_k^T}{\\mathbf{y}_k^T \\mathbf{y}_k}$$\nThis is a rank-$1$ update. For a limited-memory implementation (L-Broyden), we do not form or store the dense $N \\times N$ matrix $H_k$. Instead, we store the most recent $m$ vector pairs $(\\mathbf{s}_j, \\mathbf{y}_j)$ and use them to compute the action of $H_k$ on a vector, specifically $\\mathbf{p}_k = -H_k \\mathbf{F}(\\mathbf{x}_k)$.\n\nLet the stored pairs be $(\\mathbf{s}_0, \\mathbf{y}_0), \\dots, (\\mathbf{s}_{m-1}, \\mathbf{y}_{m-1})$, representing the updates from an initial approximation $H_{\\text{init}}$. We take $H_{\\text{init}}$ to be the identity matrix $I$, as suggested by the problem. The action of $H_k$ (which we denote $H_m$ after $m$ updates) on a vector $\\mathbf{v}$ can be computed by sequentially applying the updates. Let $H_i$ be the approximation after $i$ updates.\n$$H_{i+1}\\mathbf{v} = H_i\\mathbf{v} + (\\mathbf{s}_i - H_i \\mathbf{y}_i) \\frac{\\mathbf{y}_i^T \\mathbf{v}}{\\mathbf{y}_i^T \\mathbf{y}_i}$$\nTo compute $H_m\\mathbf{v}$, we need $H_{m-1}\\mathbf{v}$ and $H_{m-1}\\mathbf{y}_{m-1}$. This recursive structure can be implemented with an iterative algorithm that avoids recomputation.\n\n**Algorithm to Compute $H_m \\mathbf{v}$:**\nLet the stored history be the sets of vectors $S = \\{\\mathbf{s}_0, \\dots, \\mathbf{s}_{m-1}\\}$ and $Y = \\{\\mathbf{y}_0, \\dots, \\mathbf{y}_{m-1}\\}$.\n1.  Initialize the target vector $\\mathbf{p} = H_{\\text{init}}\\mathbf{v} = \\mathbf{v}$.\n2.  Initialize an auxiliary vector array `gamma` of length $m$: $\\gamma_j = H_{\\text{init}}\\mathbf{y}_j = \\mathbf{y}_j$ for $j=0, \\dots, m-1$.\n3.  Iterate for $i$ from $0$ to $m-1$:\n    a. The vector $\\gamma_i$ currently holds $H_i \\mathbf{y}_i$.\n    b. Calculate the denominator $\\text{denom} = \\mathbf{y}_i^T \\mathbf{y}_i$. If $\\text{denom}$ is close to zero, this update is unstable and should be skipped.\n    c. Update the target vector $\\mathbf{p}$ (which represents $H_i\\mathbf{v}$) to $H_{i+1}\\mathbf{v}$:\n       $$\\mathbf{p} \\leftarrow \\mathbf{p} + (\\mathbf{s}_i - \\gamma_i) \\frac{\\mathbf{y}_i^T \\mathbf{v}}{\\text{denom}}$$\n    d. For all subsequent auxiliary vectors $\\gamma_j$ ($j  i$), update them from representing $H_i\\mathbf{y}_j$ to $H_{i+1}\\mathbf{y}_j$:\n       $$\\gamma_j \\leftarrow \\gamma_j + (\\mathbf{s}_i - \\gamma_i) \\frac{\\mathbf{y}_i^T \\mathbf{y}_j}{\\text{denom}} \\quad \\text{for } j=i+1, \\dots, m-1$$\nAfter the loop finishes, $\\mathbf{p}$ will hold the result of $H_m\\mathbf{v}$.\n\n**Line Search:**\nTo ensure convergence, a line search is employed to find a step size $\\alpha_k$ that guarantees a sufficient decrease in a merit function. The problem specifies the merit function $\\phi(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{F}(\\mathbf{x})\\|_2^2$ and the condition $\\phi(\\mathbf{x}_{k+1})  \\phi(\\mathbf{x}_k)$. We use a simple backtracking line search:\n1.  Start with a full step, $\\alpha = 1$.\n2.  Calculate the trial point $\\mathbf{x}_{\\text{trial}} = \\mathbf{x}_k + \\alpha \\mathbf{p}_k$.\n3.  If $\\phi(\\mathbf{x}_{\\text{trial}})  \\phi(\\mathbf{x}_k)$, accept the step: $\\alpha_k = \\alpha$, $\\mathbf{x}_{k+1} = \\mathbf{x}_{\\text{trial}}$.\n4.  Otherwise, reduce the step size, e.g., $\\alpha \\leftarrow \\alpha/2$, and repeat from step 2.\n\n**Special Case $m=0$:**\nWhen the memory parameter $m$ is $0$, no history is stored. The `apply_H` algorithm does not enter the loop, and the search direction is simply $\\mathbf{p}_k = -H_{\\text{init}}\\mathbf{F}(\\mathbf{x}_k) = -\\mathbf{F}(\\mathbf{x}_k)$. This reduces the method to a simple iterative scheme with a line search, specifically, a type of steepest descent method on the merit function $\\phi(\\mathbf{x})$.\n\n**Overall L-Broyden Algorithm:**\n1.  **Initialize**: Choose $\\mathbf{x}_0$, tolerance `tol`, max iterations `maxiter`, memory $m$. Set $k=0$, $S=\\{\\}$, $Y=\\{\\}$. Calculate $\\mathbf{F}_0 = \\mathbf{F}(\\mathbf{x}_0)$.\n2.  **Iterate**: For $k=0, 1, 2, \\dots, \\text{maxiter}-1$:\n    a. Check for convergence: If $\\|\\mathbf{F}_k\\|_\\infty  \\text{tol}$, stop and report success.\n    b. Compute search direction $\\mathbf{p}_k = -\\text{apply\\_H}(\\mathbf{F}_k, S, Y)$.\n    c. Perform line search to find $\\alpha_k$ such that $\\phi(\\mathbf{x}_k+\\alpha_k \\mathbf{p}_k)  \\phi(\\mathbf{x}_k)$. If no such $\\alpha_k$ is found, stop and report failure.\n    d. Update solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$.\n    e. Compute new residual: $\\mathbf{F}_{k+1} = \\mathbf{F}(\\mathbf{x}_{k+1})$.\n    f. Define step vectors: $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$, $\\mathbf{y}_k = \\mathbf{F}_{k+1} - \\mathbf{F}_k$.\n    g. Update history: If $\\|\\mathbf{y}_k\\|^2  0$:\n       - If $|S| = m$, remove the oldest $(\\mathbf{s}, \\mathbf{y})$ pair from $S$ and $Y$.\n       - Append $\\mathbf{s}_k$ to $S$ and $\\mathbf{y}_k$ to $Y$.\n3.  **Termination**: If loop finishes due to `maxiter`, report non-convergence.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the L-Broyden solver.\n    \"\"\"\n    KB_EV_K = 8.617333262145e-5  # Boltzmann constant in eV/K\n\n    test_cases = [\n        # Test 1 (general case)\n        {'N': 200, 'm': 10, 'mu': 0.4, 'T': 900, 'delta': 0.2, 'tol': 1e-10, 'maxiter': 200},\n        # Test 2 (boundary, no memory)\n        {'N': 300, 'm': 0, 'mu': 0.4, 'T': 900, 'delta': 0.2, 'tol': 1e-8, 'maxiter': 250},\n        # Test 3 (edge, steep nonlinearity)\n        {'N': 150, 'm': 5, 'mu': 0.1, 'T': 300, 'delta': -0.1, 'tol': 1e-8, 'maxiter': 300},\n    ]\n\n    results = []\n    for params in test_cases:\n        N = params['N']\n        m = params['m']\n        mu = params['mu']\n        T = params['T']\n        delta = params['delta']\n        \n        # Generate synthetic data\n        i_vec = np.arange(1, N + 1)\n        E_star = 0.4 + 0.25 * np.sin(2 * np.pi * i_vec / N) + 0.15 * np.cos(4 * np.pi * i_vec / N)\n        \n        # Fermi-Dirac function\n        def fermi_dirac(E, mu_val, T_val):\n            return 1.0 / (1.0 + np.exp((E - mu_val) / (KB_EV_K * T_val)))\n\n        f_obs = fermi_dirac(E_star, mu, T)\n\n        # The function F whose root we want to find\n        def F_func(x):\n            return fermi_dirac(x, mu, T) - f_obs\n        \n        # Initial guess\n        x0 = E_star + delta\n\n        # Run the solver\n        converged, final_norm, iterations = l_broyden_solver(\n            F_func, \n            x0, \n            m=m, \n            tol=params['tol'], \n            maxiter=params['maxiter']\n        )\n        \n        results.extend([converged, final_norm, iterations])\n    \n    # Format and print the final output\n    output_str = ','.join(map(str, results))\n    print(f\"[{output_str}]\")\n\ndef l_broyden_solver(F_func, x0, m, tol, maxiter):\n    \"\"\"\n    Implements a limited-memory Broyden solver for F(x) = 0.\n\n    Args:\n        F_func (callable): The vector-valued function to find the root of.\n        x0 (np.ndarray): The initial guess for the solution vector x.\n        m (int): The number of history vectors (s, y) to store.\n        tol (float): The tolerance for the infinity norm of F(x) for convergence.\n        maxiter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: (converged, final_norm, iterations)\n    \"\"\"\n    x_k = np.copy(x0)\n    F_k = F_func(x_k)\n    norm_F = np.linalg.norm(F_k, np.inf)\n\n    # History vectors s_k = x_{k+1}-x_k and y_k = F_{k+1}-F_k\n    S = []\n    Y = []\n    \n    for k in range(maxiter):\n        if norm_F  tol:\n            return True, norm_F, k\n\n        # Compute search direction p_k = -H_k * F_k\n        p_k = -apply_H(F_k, S, Y)\n        \n        # Backtracking line search for step size alpha\n        alpha = 1.0\n        phi_k = 0.5 * np.dot(F_k, F_k)\n        x_next, F_next = None, None\n        \n        line_search_success = False\n        for _ in range(10): # Max 10 backtracking steps\n            x_next = x_k + alpha * p_k\n            F_next = F_func(x_next)\n            phi_next = 0.5 * np.dot(F_next, F_next)\n            \n            if phi_next  phi_k:\n                line_search_success = True\n                break\n            alpha /= 2.0\n            \n        if not line_search_success:\n            return False, norm_F, k + 1\n\n        s_k = x_next - x_k\n        y_k = F_next - F_k\n\n        # Update history S and Y\n        if m > 0:\n            denom = np.dot(y_k, y_k)\n            if denom > 1e-12: # Avoid unstable updates\n                if len(S) == m:\n                    S.pop(0)\n                    Y.pop(0)\n                S.append(s_k)\n                Y.append(y_k)\n\n        x_k = x_next\n        F_k = F_next\n        norm_F = np.linalg.norm(F_k, np.inf)\n\n    return norm_F  tol, norm_F, maxiter\n\ndef apply_H(v, S, Y):\n    \"\"\"\n    Computes the action of the inverse Jacobian approximation H on a vector v.\n    H is constructed from the history vectors S and Y using the \"good\" Broyden update.\n    The initial approximation H_init is the identity matrix.\n\n    Args:\n        v (np.ndarray): The vector to apply H on.\n        S (list of np.ndarray): History of solution steps (s_k).\n        Y (list of np.ndarray): History of residual changes (y_k).\n\n    Returns:\n        np.ndarray: The result of H * v.\n    \"\"\"\n    m_hist = len(S)\n    if m_hist == 0:\n        return v  # H_init is identity, so H_init * v = v\n\n    # O(m^2) algorithm to compute H_m * v\n    p = np.copy(v)\n    \n    # gamma[j] will store H_j * Y[j]\n    gamma = [np.copy(y) for y in Y]\n    \n    for i in range(m_hist):\n        yi_dot_yi = np.dot(Y[i], Y[i])\n        \n        # Skip update if denominator is too small\n        if yi_dot_yi  1e-12:\n            continue\n\n        # Update p, which represents H_i*v -> H_{i+1}*v\n        c = np.dot(Y[i], v) / yi_dot_yi\n        p += (S[i] - gamma[i]) * c\n\n        # Update gamma vectors for subsequent steps\n        # gamma[j] is updated from H_i*Y[j] to H_{i+1}*Y[j]\n        for j in range(i + 1, m_hist):\n            d_coeff = np.dot(Y[i], Y[j]) / yi_dot_yi\n            gamma[j] += (S[i] - gamma[i]) * d_coeff\n            \n    return p\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}