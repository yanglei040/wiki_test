{
    "hands_on_practices": [
        {
            "introduction": "The Fast Multipole Method's efficiency stems from approximating the potential of a group of sources with a single truncated multipole expansion. A crucial parameter in this approximation is the truncation order $p$, which dictates the trade-off between accuracy and computational cost. This exercise provides a foundational hands-on test to solidify your understanding of this trade-off. By constructing a synthetic potential field that is \"band-limited\"—meaning its exact multipole representation contains only a finite number of terms—you can directly verify the minimum truncation order required for an exact reconstruction, thereby validating your implementation of the core multipole expansion. ",
            "id": "3591359",
            "problem": "Design and implement a diagnostic program that validates truncation order selection in the Fast Multipole Method (FMM) for the three-dimensional Laplace potential in the context of computational geophysics. The program must reconstruct exact far-field potentials for synthetic, axially symmetric mass distributions whose nonzero multipole content is known a priori through a finite set of spherical multipole moments (that is, only finitely many spherical harmonic degrees are nonzero). You must use this property to determine the minimal truncation order $p$ required to exactly reproduce the target field to within a stringent numerical tolerance.\n\nStart from the following fundamental base:\n- The scalar gravitational potential $V(\\mathbf{r})$ in a source-free region satisfies Laplace’s equation $\\nabla^{2} V = 0$.\n- The Green’s function for the Laplace operator in three dimensions is $G(\\mathbf{r},\\mathbf{r}') = \\|\\mathbf{r}-\\mathbf{r}'\\|^{-1}$.\n- By separation of variables, the general exterior solution for $r = \\|\\mathbf{r}\\|$ outside a bounded source region can be represented as a series in spherical harmonics $Y_{\\ell m}(\\theta,\\phi)$, where $\\ell \\in \\{0,1,2,\\dots\\}$ and $m \\in \\{-\\ell,\\dots,\\ell\\}$. The spherical harmonics form a complete orthonormal basis on the unit sphere.\n\nYour task is to:\n1. Construct synthetic, axisymmetric “exact fields” by specifying nonzero spherical multipole moments only for orders $\\ell \\leq L$ and strictly $m=0$. This defines a finite-bandlimited multipole field such that the exact exterior potential is fully determined by these finitely many moments. For axisymmetric cases, the spherical harmonics reduce to $Y_{\\ell 0}(\\theta)$ which are real-valued.\n2. For each synthetic field, evaluate the exact potential on a prescribed set of observation points outside the source region.\n3. Independently compute a truncated multipole reconstruction that includes degrees $0 \\leq \\ell \\leq p$ only, and determine the minimal integer $p$ for which the maximum relative error over all observation points is below a specified tolerance.\n4. Report, for each test case, the minimal $p$ that meets the accuracy requirement.\n\nPrecision and units:\n- Treat all quantities as dimensionless.\n- All angles must be interpreted in radians.\n- Use a relative error tolerance of $\\varepsilon = 10^{-12}$ defined as follows: if $V_{\\text{exact}}(\\mathbf{r}_{i})$ is the exact potential at observation point $i$ and $V_{p}(\\mathbf{r}_{i})$ is the truncated reconstruction, define\n$$\n\\mathcal{E}(p) \\equiv \\frac{\\max_{i} \\left| V_{p}(\\mathbf{r}_{i}) - V_{\\text{exact}}(\\mathbf{r}_{i}) \\right|}{\\max_{i} \\left| V_{\\text{exact}}(\\mathbf{r}_{i}) \\right|}.\n$$\nThe minimal $p$ is the smallest nonnegative integer such that $\\mathcal{E}(p) \\le \\varepsilon$.\n\nImplementation constraints:\n- Use the orthonormal spherical harmonics convention (Condon–Shortley phase) and the standard Legendre polynomials $P_{\\ell}(\\cos \\theta)$. For axisymmetry with $m=0$, you may use that $Y_{\\ell 0}(\\theta)$ is proportional to $P_{\\ell}(\\cos \\theta)$.\n- Construct the exact potential from the specified finite set of multipole moments. The truncated reconstruction must include only terms up to degree $p$.\n- Use the same normalization coherently in both the exact and truncated reconstructions so that equality for $p \\ge L$ is a direct consequence of the field being bandlimited to $\\ell \\le L$.\n\nTest suite:\n- Observation points (in spherical coordinates $(r,\\theta,\\phi)$, with angles in radians):\n  - Point $1$: $(r,\\theta,\\phi) = (5.0, 0.7, 0.1)$.\n  - Point $2$: $(r,\\theta,\\phi) = (4.2, 1.1, 1.2)$.\n  - Point $3$: $(r,\\theta,\\phi) = (10.0, 2.2, 2.4)$.\n  - Point $4$: $(r,\\theta,\\phi) = (3.3, 0.5, 2.0)$.\n- Synthetic moment sets (axisymmetric, $m=0$ only; all unspecified moments are zero):\n  - Case A (pure monopole): nonzero moment of degree $L=0$ with amplitude $Q_{00} = 2.0$.\n  - Case B (pure dipole): nonzero moment of degree $L=1$ with amplitude $Q_{10} = -0.75$.\n  - Case C (pure quadrupole): nonzero moment of degree $L=2$ with amplitude $Q_{20} = 0.8$.\n\nWhat to compute:\n- For each case, compute the exact potential on the observation points using only the specified nonzero moments up to degree $L$.\n- For $p = 0,1,2,\\dots,6$, compute the truncated reconstruction and the corresponding $\\mathcal{E}(p)$.\n- Determine the minimal $p$ such that $\\mathcal{E}(p) \\le 10^{-12}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the three minimal truncation orders as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C]. For example, if the minimal orders are $p=0$, $p=1$, and $p=2$, then the output must be exactly \"[0,1,2]\".",
            "solution": "The problem requires the implementation of a diagnostic program to determine the minimal truncation order, $p$, for a multipole expansion to accurately represent a synthesized, axisymmetric gravitational potential. The potential fields are known to be band-limited, meaning their multipole content is non-zero only up to a maximum spherical harmonic degree, $L$. This validation exercise is fundamental in computational methods like the Fast Multipole Method (FMM), where controlling truncation error is critical for accuracy and efficiency.\n\nThe theoretical basis for this problem is the solution to the three-dimensional Laplace equation, $\\nabla^2 V = 0$, in a source-free region. The potential $V$ created by a bounded mass distribution can be expressed in the exterior region using a multipole expansion in spherical coordinates $(\\mathbf{r}) = (r, \\theta, \\phi)$. This expansion takes the form:\n$$\nV(r, \\theta, \\phi) = \\sum_{\\ell=0}^{\\infty} \\sum_{m=-\\ell}^{\\ell} M_{\\ell m} \\frac{Y_{\\ell m}(\\theta, \\phi)}{r^{\\ell+1}}\n$$\nwhere $M_{\\ell m}$ are the multipole moments of the source distribution, and $Y_{\\ell m}(\\theta, \\phi)$ are the orthonormal spherical harmonics defined with the Condon–Shortley phase. All quantities are treated as dimensionless.\n\nThe problem specifies axisymmetric fields, which implies that the potential is independent of the azimuthal angle $\\phi$. This occurs when only the zonal moments ($m=0$) are non-zero. The expansion simplifies to:\n$$\nV(r, \\theta) = \\sum_{\\ell=0}^{\\infty} M_{\\ell 0} \\frac{Y_{\\ell 0}(\\theta, \\phi)}{r^{\\ell+1}}\n$$\nThe zonal spherical harmonic $Y_{\\ell 0}$ is real-valued and is related to the Legendre polynomial of degree $\\ell$, $P_{\\ell}(x)$, by the formula:\n$$\nY_{\\ell 0}(\\theta, \\phi) = \\sqrt{\\frac{2\\ell+1}{4\\pi}} P_{\\ell}(\\cos\\theta)\n$$\nThe problem defines synthetic potential fields by providing a finite set of \"moment amplitudes,\" denoted $Q_{\\ell 0}$. We interpret these amplitudes as the multipole moments $M_{\\ell 0}$ in the above expansion. Therefore, the potential for a given set of moments $Q_{\\ell 0}$ is:\n$$\nV(r, \\theta) = \\sum_{\\ell=0}^{\\infty} Q_{\\ell 0} \\frac{1}{r^{\\ell+1}} \\sqrt{\\frac{2\\ell+1}{4\\pi}} P_{\\ell}(\\cos\\theta)\n$$\n\nThe core of the problem lies in the distinction between an \"exact\" field and its \"truncated reconstruction.\" The synthetic fields are band-limited, meaning there exists a maximum degree $L$ such that $Q_{\\ell 0} = 0$ for all $\\ell  L$. In the given test cases, each field is generated by a single non-zero moment at degree $L$. The exact potential, $V_{\\text{exact}}$, is thus a finite sum:\n$$\nV_{\\text{exact}}(r, \\theta) = \\sum_{\\ell=0}^{L} Q_{\\ell 0} \\frac{1}{r^{\\ell+1}} \\sqrt{\\frac{2\\ell+1}{4\\pi}} P_{\\ell}(\\cos\\theta)\n$$\nIn contrast, the truncated reconstruction, $V_p$, is computed by summing the same series but only up to a specified truncation order $p$:\n$$\nV_{p}(r, \\theta) = \\sum_{\\ell=0}^{p} Q_{\\ell 0} \\frac{1}{r^{\\ell+1}} \\sqrt{\\frac{2\\ell+1}{4\\pi}} P_{\\ell}(\\cos\\theta)\n$$\nThe task is to find the minimal non-negative integer $p$ that satisfies the accuracy criterion $\\mathcal{E}(p) \\le \\varepsilon$, where $\\varepsilon=10^{-12}$ and the relative error $\\mathcal{E}(p)$ is defined as:\n$$\n\\mathcal{E}(p) \\equiv \\frac{\\max_{i} \\left| V_{p}(\\mathbf{r}_{i}) - V_{\\text{exact}}(\\mathbf{r}_{i}) \\right|}{\\max_{i} \\left| V_{\\text{exact}}(\\mathbf{r}_{i}) \\right|}\n$$\nThe difference between the truncated and exact potentials is:\n$$\nV_{p}(r, \\theta) - V_{\\text{exact}}(r, \\theta) = \\left( \\sum_{\\ell=0}^{p} - \\sum_{\\ell=0}^{L} \\right) Q_{\\ell 0} \\frac{Y_{\\ell 0}(\\theta)}{r^{\\ell+1}}\n$$\nIf the truncation order $p$ is greater than or equal to the field's maximum degree $L$ (i.e., $p \\ge L$), the two sums are identical because all moments $Q_{\\ell 0}$ with $\\ell  L$ are zero. In this case, $V_p = V_{\\text{exact}}$, and the error $\\mathcal{E}(p)$ becomes zero (within machine precision).\nIf $p  L$, the difference is a sum over the missing terms:\n$$\nV_{p}(r, \\theta) - V_{\\text{exact}}(r, \\theta) = - \\sum_{\\ell=p+1}^{L} Q_{\\ell 0} \\frac{Y_{\\ell 0}(\\theta)}{r^{\\ell+1}}\n$$\nFor the specific test cases provided, where only a single moment $Q_{L0}$ is non-zero, this difference will be non-zero for any $pL$, resulting in a significant error.\nTherefore, the minimal truncation order $p$ required to reproduce the field to within the stringent tolerance $\\varepsilon$ is precisely the maximum degree $L$ of the non-zero moments comprising the field.\n\n- For Case A (pure monopole), the only non-zero moment is $Q_{00} = 2.0$. Thus, $L=0$. The minimal truncation order required is $p=0$.\n- For Case B (pure dipole), the only non-zero moment is $Q_{10} = -0.75$. Thus, $L=1$. A truncation with $p=0$ would yield a zero potential, giving a relative error of $1$. The minimal order to capture the field is $p=1$.\n- For Case C (pure quadrupole), the only non-zero moment is $Q_{20} = 0.8$. Thus, $L=2$. Truncations with $p=0$ or $p=1$ yield a zero potential and a relative error of $1$. The minimal order required is $p=2$.\n\nThe algorithm to confirm this is as follows:\n1. For each test case, define the set of non-zero moments $\\{Q_{\\ell 0}\\}$ and the maximum degree $L$.\n2. Implement a function to calculate the potential $V(r, \\theta)$ given a set of moments and a summation limit, using `scipy.special.eval_legendre` to evaluate $P_{\\ell}(\\cos\\theta)$.\n3. For each case, calculate the vector of exact potentials, $V_{\\text{exact}}(\\mathbf{r}_i)$, at the specified observation points by summing up to degree $L$.\n4. Determine the normalization factor for the error, $N = \\max_i |V_{\\text{exact}}(\\mathbf{r}_i)|$.\n5. Iterate through truncation orders $p$ from $0$ to $6$. For each $p$:\n   a. Calculate the vector of truncated potentials, $V_p(\\mathbf{r}_i)$, by summing up to degree $p$.\n   b. Compute the maximum absolute difference, $D = \\max_i |V_p(\\mathbf{r}_i) - V_{\\text{exact}}(\\mathbf{r}_i)|$.\n   c. If $N  0$, calculate the relative error $\\mathcal{E}(p) = D/N$. If $N=0$, the error is $0$ if $D=0$ and infinite otherwise.\n   d. If $\\mathcal{E}(p) \\le 10^{-12}$, we have found the minimal required $p$ for the current case. Record this value and proceed to the next case.\n6. Collect the minimal $p$ values for all cases and format the output as required.\nThis procedure systematically validates the fundamental principle that a band-limited function requires a reconstruction basis of at least the same band-limit for exact recovery.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\n\ndef solve():\n    \"\"\"\n    Computes the minimal truncation order p for reconstructing axisymmetric\n    Laplace potentials with known finite multipole content.\n    \"\"\"\n    \n    # Define the observation points from the problem statement.\n    # Coordinates are spherical (r, theta, phi) with angles in radians.\n    observation_points = [\n        (5.0, 0.7, 0.1),\n        (4.2, 1.1, 1.2),\n        (10.0, 2.2, 2.4),\n        (3.3, 0.5, 2.0),\n    ]\n\n    # Define the synthetic test cases. Each case has a maximum degree L\n    # and a dictionary of non-zero axisymmetric moments Q_l0.\n    test_cases = [\n        {'L': 0, 'moments': {0: 2.0}},    # Case A: Pure monopole\n        {'L': 1, 'moments': {1: -0.75}},  # Case B: Pure dipole\n        {'L': 2, 'moments': {2: 0.8}},    # Case C: Pure quadrupole\n    ]\n\n    # Define the relative error tolerance and the maximum truncation order to test.\n    tolerance = 1e-12\n    p_max = 6\n    \n    final_results = []\n\n    def calculate_potential(r, theta, moments, p_limit):\n        \"\"\"\n        Calculates the axisymmetric potential at a point (r, theta) by summing\n        the multipole expansion up to a given truncation order p_limit.\n\n        Args:\n            r (float): Radial distance.\n            theta (float): Polar angle in radians.\n            moments (dict): A dictionary {l: Q_l0} of non-zero multipole moments.\n            p_limit (int): The maximum degree l to include in the sum.\n\n        Returns:\n            float: The calculated potential.\n        \"\"\"\n        potential = 0.0\n        cos_theta = np.cos(theta)\n        \n        for l in range(p_limit + 1):\n            if l in moments:\n                Q_l0 = moments.get(l, 0.0)\n                \n                # Associated Legendre polynomial P_l^0(cos(theta)) is P_l(cos(theta))\n                pl_cos_theta = eval_legendre(l, cos_theta)\n                \n                # Orthonormal spherical harmonic Y_l0(theta)\n                y_l0_norm = np.sqrt((2 * l + 1) / (4 * np.pi))\n                y_l0 = y_l0_norm * pl_cos_theta\n\n                # Term in the multipole expansion\n                term = Q_l0 * y_l0 / (r**(l + 1))\n                potential += term\n                \n        return potential\n\n    for case in test_cases:\n        L = case['L']\n        moments = case['moments']\n        \n        # 1. Calculate the exact potential at all observation points.\n        # The 'exact' potential is computed using all specified moments up to L.\n        V_exact_vals = np.array([\n            calculate_potential(r, theta, moments, L)\n            for r, theta, phi in observation_points\n        ])\n        \n        # 2. Determine the normalization factor for the relative error.\n        max_abs_V_exact = np.max(np.abs(V_exact_vals))\n\n        min_p_found = -1  # Sentinel for not found\n\n        # 3. Iterate through truncation orders p to find the minimal one.\n        for p in range(p_max + 1):\n            # Calculate the truncated potential for the current p.\n            V_p_vals = np.array([\n                calculate_potential(r, theta, moments, p)\n                for r, theta, phi in observation_points\n            ])\n            \n            # Calculate the maximum absolute difference.\n            max_abs_diff = np.max(np.abs(V_p_vals - V_exact_vals))\n            \n            # Calculate relative error, handling the case where the exact potential is zero.\n            if max_abs_V_exact > tolerance:\n                error = max_abs_diff / max_abs_V_exact\n            else:\n                # If the exact potential is effectively zero, the truncated one must be too.\n                error = 0.0 if max_abs_diff = tolerance else float('inf')\n\n            # 4. Check if the error is within tolerance.\n            if error = tolerance:\n                min_p_found = p\n                break  # Minimal p found, no need to check higher p.\n        \n        final_results.append(min_p_found)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While often introduced with the Laplace kernel ($1/r$), the FMM is a versatile framework applicable to a wide range of physical interactions described by different Green's functions. This practice challenges you to adapt the method's error analysis to the Yukawa potential, $\\exp(-\\kappa r)/r$, which models screened interactions prevalent in fields like plasma physics and certain geophysical models. This exercise will sharpen your analytical skills by requiring you to derive how the FMM's error bounds and key parameters, such as the truncation order $p$, are fundamentally altered by the kernel's exponential decay. ",
            "id": "3591354",
            "problem": "A common task in computational geophysics is to accelerate the evaluation of screened potentials created by distributed sources, which are modeled by the Yukawa Green’s function $K(r)=\\exp(-\\kappa r)/r$. The Fast Multipole Method (FMM) relies on truncating multipole and local expansions at order $p$ and using a distance-based Multipole Acceptance Criterion (MAC) to ensure accuracy.\n\nConsider a cubic source box of radius $a$ containing point sources with signed strengths $\\{q_i\\}$ located at positions $\\{\\mathbf{r}_i\\}$, and a target point at distance $R$ from the box center, with $Ra$. Assume the total absolute strength is bounded by $A$, i.e., $\\sum_i |q_i| \\leq A$. The Yukawa kernel $K(r)$ provides a screened interaction relative to the Laplace kernel $1/r$. Starting from the fundamental addition properties of Green’s functions and the definition of multipole truncation error as the contribution from spherical harmonic degrees $\\ell \\geq p+1$, construct a rigorous upper bound on the absolute truncation error at the target in terms of $a$, $R$, $\\kappa$, $p$, and $A$, by bounding the kernel using the minimal separation $R-a$.\n\nUse this bound to:\n- Derive how the truncation order $p$ must scale to achieve a prescribed absolute error tolerance $\\varepsilon$, and explicitly isolate the dependence on $\\kappa$.\n- Derive a modified MAC for the Yukawa kernel that reduces to the standard Laplace MAC when $\\kappa \\to 0$.\n\nThen, for the numerical case with $A=1.5$, $a=0.2$ (meters), $R=2.0$ (meters), $\\kappa=3.0$ (inverse meters), and an absolute error tolerance $\\varepsilon=1.0\\times 10^{-8}$, compute the minimal integer truncation order $p$ that guarantees the tolerance according to your bound. Report $p$ as a dimensionless integer. No rounding specification is required because $p$ is an integer.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective. It presents a standard task in computational physics: analyzing the truncation error of a Fast Multipole Method (FMM) for the Yukawa potential. All necessary parameters and conditions are provided, and there are no internal contradictions or violations of physical principles. The problem is therefore deemed valid and a solution will be constructed.\n\nThe potential at a target point $\\mathbf{x}$ due to a set of source charges $\\{q_i\\}$ at positions $\\{\\mathbf{r}_i\\}$ is given by the superposition $\\Phi(\\mathbf{x}) = \\sum_i q_i K(|\\mathbf{x}-\\mathbf{r}_i|)$, where the kernel is the Yukawa Green's function, $K(r) = \\frac{\\exp(-\\kappa r)}{r}$. The sources are contained within a region of radius $a$ centered at the origin, i.e., $|\\mathbf{r}_i| \\le a$ for all $i$. The target point is at a distance $R=|\\mathbf{x}|  a$ from the origin. The total absolute source strength is bounded by $\\sum_i |q_i| \\le A$.\n\n**Part 1: Derivation of the Truncation Error Bound**\n\nThe FMM approximates the potential by using a truncated multipole expansion for distant interactions. The error of this approximation comes from neglecting high-order terms in the series, specifically those corresponding to spherical harmonic degrees $\\ell \\ge p+1$. A rigorous derivation of the error bound for the Yukawa kernel involves the addition theorem and properties of modified spherical Bessel and Hankel functions. However, a sound and widely-used upper bound can be constructed by combining the known geometric convergence properties of the FMM with a worst-case scaling based on the kernel's magnitude.\n\nThe truncation error for the Laplace kernel ($1/r$) is known to be bounded by a term that depends on the total charge, a geometric factor reflecting the separation of source and target, and the distance. For sources within a sphere of radius $a$ and a target at distance $R$, the error sum is a geometric series in powers of $|\\mathbf{r}_i|/R \\le a/R$. The sum of the tail of this series (from term $p+1$ onwards) is bounded by a factor proportional to $\\frac{(a/R)^{p+1}}{1-a/R}$.\n\nFor the Yukawa kernel, the fundamental geometric convergence rate is the same, but the overall magnitude is modulated by the exponential screening term $\\exp(-\\kappa r)$. A conservative bound is obtained by evaluating the kernel at the minimum possible distance between a source and the target, which is $R-a$.\n\nThe absolute error $|E_p|$ is the absolute value of the sum of potentials from the neglected terms. We can bound this by the sum of the absolute errors for each source, which is then bounded using the total absolute strength $A$:\n$|E_p| = \\left| \\sum_i q_i (\\text{error for source } i) \\right| \\le \\sum_i |q_i| |\\text{error for unit source}| \\le A \\cdot \\max_i |\\text{error for unit source}|$.\n\nThe maximum error for a unit source is obtained by combining the worst-case potential magnitude with the geometric decay term. The potential magnitude is maximized at the minimal separation distance $R-a$. Therefore, the kernel part of the bound is $K(R-a) = \\frac{\\exp(-\\kappa(R-a))}{R-a}$. Combining this with the geometric factor for the tail of the multipole series gives the following upper bound for the absolute truncation error:\n$$ |E_p| \\le A \\frac{\\exp(-\\kappa(R-a))}{R-a} \\frac{(a/R)^{p+1}}{1-a/R} $$\nThis expression provides the requested rigorous upper bound in terms of $a$, $R$, $\\kappa$, $p$, and $A$.\n\n**Part 2: Scaling of the Truncation Order $p$**\n\nTo achieve a prescribed absolute error tolerance $\\varepsilon$, we require $|E_p| \\le \\varepsilon$. Using the bound derived above:\n$$ A \\frac{\\exp(-\\kappa(R-a))}{R-a} \\frac{(a/R)^{p+1}}{1-a/R} \\le \\varepsilon $$\nWe solve this inequality for $p$. First, isolate the term containing $p$:\n$$ \\left(\\frac{a}{R}\\right)^{p+1} \\le \\frac{\\varepsilon(R-a)(1-a/R)}{A \\exp(-\\kappa(R-a))} $$\nSubstituting $1-a/R = (R-a)/R$:\n$$ \\left(\\frac{a}{R}\\right)^{p+1} \\le \\frac{\\varepsilon(R-a)^2}{A R \\exp(-\\kappa(R-a))} $$\nThis is a valid but cumbersome expression. A simpler form arises by recognizing that $(1-a/R)^{-1}$ is a slowly varying factor close to $1$ for well-separated boxes, and it originates from summing the geometric series. A slightly looser but more common bound that simplifies the algebra is $|E_p| \\le \\frac{A}{R-a} \\exp(-\\kappa(R-a)) (a/R)^{p+1}$. Let's\nre-derive the scaling from the original, tighter bound for better accuracy.\n$$ \\left(\\frac{a}{R}\\right)^{p+1} \\le \\frac{\\varepsilon}{A} \\frac{R-a}{1-a/R} \\exp(\\kappa(R-a)) = \\frac{\\varepsilon}{A} \\frac{R-a}{(R-a)/R} \\exp(\\kappa(R-a)) = \\frac{\\varepsilon R}{A} \\exp(\\kappa(R-a)) $$\nTaking the natural logarithm of both sides:\n$$ (p+1) \\ln\\left(\\frac{a}{R}\\right) \\le \\ln\\left(\\frac{\\varepsilon R}{A}\\right) + \\kappa(R-a) $$\nSince $Ra$, the term $\\ln(a/R)$ is negative. Dividing by it reverses the inequality:\n$$ p+1 \\ge \\frac{\\ln\\left(\\frac{\\varepsilon R}{A}\\right) + \\kappa(R-a)}{\\ln(a/R)} $$\nRewriting $\\ln(a/R) = -\\ln(R/a)$ and $\\ln(\\varepsilon R / A) = -\\ln(A/(\\varepsilon R))$:\n$$ p+1 \\ge \\frac{-\\ln\\left(\\frac{A}{\\varepsilon R}\\right) + \\kappa(R-a)}{-\\ln(R/a)} = \\frac{\\ln\\left(\\frac{A}{\\varepsilon R}\\right) - \\kappa(R-a)}{\\ln(R/a)} $$\nThis gives the required scaling for $p$:\n$$ p \\ge \\frac{\\ln(A/(\\varepsilon R)) - \\kappa(R-a)}{\\ln(R/a)} - 1 $$\nThe dependence on the tolerance $\\varepsilon$ is $p \\propto \\frac{\\ln(1/\\varepsilon)}{\\ln(R/a)}$. The dependence on the screening parameter $\\kappa$ is $p \\propto -\\frac{\\kappa(R-a)}{\\ln(R/a)}$. The negative sign indicates that a stronger screening (larger $\\kappa$) reduces the required truncation order $p$ for a given accuracy. This is physically intuitive, as the exponential decay damps the higher-order multipole contributions more strongly, allowing for an earlier truncation.\n\n**Part 3: Derivation of a Modified Multipole Acceptance Criterion (MAC)**\n\nA MAC is a geometric condition that determines when a multipole approximation is sufficiently accurate. For the Laplace kernel ($\\kappa=0$), the error is dominated by the geometric factor $(a/R)^{p+1}$. A typical MAC requires this factor to be small, which translates to a condition on the separation ratio, such as $R/a  C$ for some constant $C$ (e.g., $C=2$).\n\nFor the Yukawa kernel, the error is suppressed by the additional factor $\\exp(-\\kappa(R-a))$. This means the same accuracy can be achieved with a less strict geometric separation. Let $R_L$ be the minimum separation distance required by a Laplace MAC to achieve a certain error target for a given $p$. Let $R_Y$ be the corresponding minimum separation for the Yukawa kernel. The error terms must be equal:\n$$ C_{\\text{prefactor}} \\left(\\frac{a}{R_L}\\right)^{p+1} = C'_{\\text{prefactor}} \\exp(-\\kappa(R_Y-a)) \\left(\\frac{a}{R_Y}\\right)^{p+1} $$\nAssuming the prefactors are comparable, we equate the decay factors:\n$$ \\left(\\frac{a}{R_L}\\right)^{p+1} \\approx \\exp(-\\kappa(R_Y-a)) \\left(\\frac{a}{R_Y}\\right)^{p+1} $$\n$$ \\left(\\frac{R_Y}{R_L}\\right)^{p+1} \\approx \\exp(-\\kappa(R_Y-a)) $$\n$$ \\frac{R_Y}{R_L} \\approx \\exp\\left(-\\frac{\\kappa(R_Y-a)}{p+1}\\right) $$\n$$ R_Y \\approx R_L \\exp\\left(-\\frac{\\kappa(R_Y-a)}{p+1}\\right) $$\nThis implicit equation defines the modified MAC. If the Laplace MAC is $R  R_L$, the Yukawa MAC is $R  R_Y$, where $R_Y$ is the solution to this equation. Since the exponential term is less than $1$ for $\\kappa0$, we have $R_Y  R_L$, meaning the Yukawa MAC is less restrictive. As $\\kappa \\to 0$, the exponential term approaches $1$, and $R_Y \\to R_L$. Thus, the modified MAC correctly reduces to the standard Laplace MAC in the limit of no screening.\n\n**Part 4: Numerical Calculation of $p$**\n\nWe are given the parameters: $A=1.5$, $a=0.2$ meters, $R=2.0$ meters, $\\kappa=3.0$ inverse meters, and an error tolerance $\\varepsilon=1.0\\times 10^{-8}$. We seek the minimal integer truncation order $p$.\n\nWe use the inequality derived in Part 2:\n$$ p \\ge \\frac{\\ln(A/(\\varepsilon R)) - \\kappa(R-a)}{\\ln(R/a)} - 1 $$\nFirst, we compute the component terms:\n- The separation ratio is $R/a = 2.0 / 0.2 = 10$.\n- The logarithm of the ratio is $\\ln(R/a) = \\ln(10)$.\n- The minimum separation is $R-a = 2.0 - 0.2 = 1.8$.\n- The screening factor argument is $\\kappa(R-a) = 3.0 \\times 1.8 = 5.4$.\n- The argument of the primary logarithm is $\\frac{A}{\\varepsilon R} = \\frac{1.5}{(1.0 \\times 10^{-8})(2.0)} = \\frac{1.5}{2.0 \\times 10^{-8}} = 0.75 \\times 10^8 = 7.5 \\times 10^7$.\n- The numerator term is then $\\ln(7.5 \\times 10^7) - 5.4 = \\ln(7.5) + 7\\ln(10) - 5.4$.\n\nUsing the approximate values $\\ln(10) \\approx 2.30259$ and $\\ln(7.5) \\approx 2.01490$:\n$$ p \\ge \\frac{(2.01490 + 7 \\times 2.30259) - 5.4}{2.30259} - 1 $$\n$$ p \\ge \\frac{(2.01490 + 16.11813) - 5.4}{2.30259} - 1 $$\n$$ p \\ge \\frac{18.13303 - 5.4}{2.30259} - 1 $$\n$$ p \\ge \\frac{12.73303}{2.30259} - 1 $$\n$$ p \\ge 5.5309 - 1 $$\n$$ p \\ge 4.5309 $$\nSince the truncation order $p$ must be an integer, the minimal integer value that satisfies this condition is $p=5$.",
            "answer": "$$\\boxed{5}$$"
        },
        {
            "introduction": "To tackle grand-challenge problems in geophysics, the FMM must be scaled across thousands of processors on distributed-memory supercomputers. In this regime, performance is often dictated not by computation, but by the costs of communication—particularly network latency. This advanced practice places you in the role of a computational scientist tasked with optimizing a parallel FMM implementation. You will design and simulate a pipelining strategy to overlap communication with computation, a critical technique for hiding latency and achieving high strong-scaling efficiency, providing direct insight into the practical art of performance engineering for large-scale scientific codes. ",
            "id": "3591372",
            "problem": "You are tasked with designing and analyzing an overlap strategy for the Fast Multipole Method (FMM) in distributed-memory settings under highly nonuniform source distributions, with the goal of maximizing strong scaling by pipelining Multipole-to-Local (M2L) operations with Message Passing Interface (MPI) communication. The context is computational geophysics where FMM is used to accelerate the evaluation of long-range interactions for potential fields governed by the Laplace equation. The objective is to derive from first principles a concurrency-aware performance model and implement an algorithm that predicts the strong-scaling efficiency for given distributed workloads and network parameters, and that automatically selects a grouping strategy to optimize overlap.\n\nYou must start from the following fundamental base:\n- The potential field for sources governed by the Laplace equation admits multipole expansions at well-separated distances. The Fast Multipole Method exploits this by hierarchically aggregating sources into boxes and computing interactions via expansions rather than pairwise summations, yielding sub-quadratic complexity relative to direct evaluation.\n- In FMM, the Multipole-to-Local (M2L) operator translates multipole expansions of well-separated source boxes into local expansions at target boxes. The number of coefficients in a spherical harmonic expansion at order $p$ is $(p+1)^2$.\n- In distributed-memory parallelism via Message Passing Interface (MPI), remote data must be communicated. A standard communication time model per message uses a latency-bandwidth decomposition $T_{\\mathrm{comm}} = L + S/B$, where $L$ is latency in seconds, $B$ is bandwidth in bytes per second, and $S$ is message size in bytes.\n- Compute time for an M2L interaction scales with the square of the expansion order $p$ due to the number of coefficients involved. We adopt a model where the time per M2L pair is $T_{\\mathrm{pair}} = k \\, p^2$, with $k$ in seconds per coefficient-squared.\n\nConsider a distributed FMM evaluation over $P$ processes. Each process $i$ has:\n- $L_i$ local M2L pairs that do not depend on remote data,\n- $R_i$ remote M2L pairs that require remote multipole coefficients,\n- $B_i$ remote source boxes whose multipole coefficients must be communicated.\n\nTo overlap communication and computation, you will pipeline M2L across $G$ groups per process. You split the total local pairs $L_i$ evenly across $G$ groups, and similarly the total remote pairs $R_i$ and total remote boxes $B_i$. For each group $g$ on process $i$, you perform the following schedule:\n1. Compute the local-only work for the group, which takes time $T_{\\mathrm{pre}}^{(i)} = \\dfrac{L_i}{G} \\, k \\, p^2$.\n2. Communicate the remote multipole coefficients for the group. The coefficients count is $(p+1)^2$ per box, with $8$ bytes per coefficient, so the message size is $S^{(i)} = \\dfrac{B_i}{G} \\cdot (p+1)^2 \\cdot 8$. The communication time is $T_{\\mathrm{comm}}^{(i)} = L + \\dfrac{S^{(i)}}{B}$.\n3. Once both the local-only precomputation and the communication complete, compute the remote-dependent work for the group, which takes time $T_{\\mathrm{post}}^{(i)} = \\dfrac{R_i}{G} \\, k \\, p^2$.\n\nThe pipeline design starts the communication for group $g=1$ at time $t=0$. For $g1$, the communication for group $g$ begins at the start time of the remote-dependent computation of group $g-1$. You must simulate this schedule to obtain the total time $T_i(G)$ for each process $i$ under a given $G$ (taking into account concurrent progress of communication and sequential use of the compute resource). The parallel makespan is the maximum time across processes, $T_P(G) = \\max_i T_i(G)$.\n\nDefine the single-process runtime $T_1$ for the full problem size (i.e., if all work were executed on one process without communication) as\n$$\nT_1 = k \\, p^2 \\sum_{i=1}^P (L_i + R_i).\n$$\n\nThe strong scaling efficiency for $P$ processes under grouping $G$ is defined as\n$$\nE_s(P;G) = \\frac{T_1}{P \\, T_P(G)}.\n$$\n\nYour program must, for each test case, search over a set of candidate group counts $G \\in \\{1,2,4,8,16,32\\}$ and select the $G$ that maximizes $E_s(P;G)$ (equivalently minimizes $T_P(G)$). Output the corresponding optimal strong scaling efficiency $E_s(P)$ as a decimal number rounded to six decimal places.\n\nAll times must be expressed and handled in seconds. Message sizes must be computed in bytes. The final output for the program must be a single line containing a comma-separated list of the strong scaling efficiencies for all test cases enclosed in square brackets (e.g., \"[0.932100,0.871553,0.998765]\").\n\nTest suite:\n- Test case 1 (balanced workload, moderate network):\n    - $P = 4$, $p = 8$, $k = 2.0 \\times 10^{-7}$, $L = 5.0 \\times 10^{-5}$, $B = 2.0 \\times 10^{8}$,\n    - $L_i = [40000, 38000, 42000, 41000]$,\n    - $R_i = [5000, 5200, 4800, 5100]$,\n    - $B_i = [800, 820, 780, 790]$.\n- Test case 2 (highly nonuniform workload; one process heavy remote):\n    - $P = 4$, $p = 10$, $k = 1.5 \\times 10^{-7}$, $L = 2.0 \\times 10^{-4}$, $B = 1.0 \\times 10^{8}$,\n    - $L_i = [20000, 15000, 14000, 13000]$,\n    - $R_i = [60000, 4000, 3000, 2000]$,\n    - $B_i = [10000, 600, 500, 400]$.\n- Test case 3 (latency-dominated network but balanced compute):\n    - $P = 8$, $p = 6$, $k = 2.5 \\times 10^{-7}$, $L = 1.0 \\times 10^{-3}$, $B = 3.0 \\times 10^{7}$,\n    - $L_i = [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000]$,\n    - $R_i = [800, 800, 800, 800, 800, 800, 800, 800]$,\n    - $B_i = [200, 200, 200, 200, 200, 200, 200, 200]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each strong scaling efficiency rounded to six decimal places and expressed as a decimal (no percentage sign).",
            "solution": "The user has provided a problem that requires the derivation and implementation of a performance model for a distributed Fast Multipole Method (FMM) algorithm. The goal is to determine the optimal pipelining strategy to maximize strong-scaling efficiency.\n\n### Step 1: Extract Givens\n- **Domain**: Computational geophysics, FMM for Laplace equation.\n- **Parallelism**: Distributed-memory using Message Passing Interface (MPI) over $P$ processes.\n- **Core Operation**: Multipole-to-Local (M2L) translation.\n- **Expansion Order**: $p$.\n- **Number of Coefficients**: $(p+1)^2$ per spherical harmonic expansion.\n- **M2L Compute Time Model**: Time per M2L pair is $T_{\\mathrm{pair}} = k \\, p^2$. $k$ is a constant in seconds.\n- **Communication Time Model**: $T_{\\mathrm{comm}} = L + S/B$, where $L$ is latency (s), $B$ is bandwidth (bytes/s), and $S$ is message size (bytes).\n- **Data per Process $i$**:\n    - $L_i$: Number of local M2L pairs.\n    - $R_i$: Number of remote M2L pairs.\n    - $B_i$: Number of remote source boxes.\n- **Pipelining Strategy**:\n    - The workload on each process is divided into $G$ groups.\n    - **Step 1 (Pre-computation)**: Compute local-only work for a group. Time: $T_{\\mathrm{pre}}^{(i)} = \\dfrac{L_i}{G} \\, k \\, p^2$.\n    - **Step 2 (Communication)**: Communicate remote multipole coefficients for a group. Message size: $S^{(i)} = \\dfrac{B_i}{G} \\cdot (p+1)^2 \\cdot 8$ bytes (assuming $8$ bytes per coefficient). Communication time: $T_{\\mathrm{comm}}^{(i)} = L + \\dfrac{S^{(i)}}{B}$.\n    - **Step 3 (Post-computation)**: Compute remote-dependent work for a group. Time: $T_{\\mathrm{post}}^{(i)} = \\dfrac{R_i}{G} \\, k \\, p^2$.\n- **Pipeline Schedule**:\n    - Communication for group $g=1$ starts at time $t=0$.\n    - For $g  1$, communication for group $g$ begins at the start time of the post-computation of group $g-1$.\n    - Computation for a group can only begin after the computation for the previous group is complete (sequential use of compute resource).\n    - Post-computation for a group can only begin after both its corresponding pre-computation and communication have finished.\n- **Performance Metrics**:\n    - **Total time per process**: $T_i(G)$, the time until all $G$ groups on process $i$ are completed.\n    - **Parallel makespan**: $T_P(G) = \\max_i T_i(G)$.\n    - **Single-process runtime**: $T_1 = k \\, p^2 \\sum_{i=1}^P (L_i + R_i)$.\n    - **Strong scaling efficiency**: $E_s(P;G) = \\frac{T_1}{P \\, T_P(G)}$.\n- **Objective**: For a given set of parameters, find the group count $G$ from the set $\\{1,2,4,8,16,32\\}$ that maximizes $E_s(P;G)$, and output this maximum efficiency.\n- **Test Cases**: Three distinct test cases are provided with all necessary parameters.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in high-performance computing and numerical methods. The FMM, MPI, performance modeling with latency-bandwidth, and pipelining are all standard and valid concepts. The physical context (potential fields) is appropriate.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary inputs, a deterministic model for computation and communication, a clear task (timeline simulation), and an unambiguous objective function (maximization of efficiency). The search space for the optimization parameter $G$ is finite and specified.\n- **Objectivity**: The problem is stated in precise, objective mathematical and algorithmic terms, with no subjective elements.\n- **Consistency**: The problem is internally consistent. The definitions for computation time ($k \\cdot p^2$) and communication size (based on $(p+1)^2$) are explicitly stated and used consistently throughout the problem description. While this might be a simplification of a real-world scenario, it does not constitute a contradiction within the problem's defined model.\n- **Completeness**: The problem is self-contained and provides all data and models required for a solution.\n- **Feasibility**: The parameters given in the test cases are numerically plausible for a computational science simulation.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a well-defined exercise in performance modeling and algorithm simulation. I will now proceed to construct the solution.\n\n### Principle-Based Design of the Solution\n\nThe core of the problem is to accurately simulate the proposed pipeline schedule for each process to determine its total execution time, $T_i(G)$. The overall parallel execution time (makespan), $T_P(G)$, is the maximum of these individual process times.\n\n**Modeling the Pipeline for a Single Process**\n\nLet us analyze the execution timeline for a single process $i$ with a given number of groups $G$. To simplify notation, we will temporarily drop the superscript $(i)$. The time taken for the elementary tasks for each group is:\n- Pre-computation time per group: $T_{\\mathrm{pre}} = \\dfrac{L_i}{G} \\, k \\, p^2$\n- Post-computation time per group: $T_{\\mathrm{post}} = \\dfrac{R_i}{G} \\, k \\, p^2$\n- Communication time per group: $T_{\\mathrm{comm}} = L + \\dfrac{1}{B} \\left( \\dfrac{B_i}{G} \\cdot (p+1)^2 \\cdot 8 \\right)$\n\nWe need to track the state of the two resources involved: the compute unit (CPU) and the communication network. The CPU is used sequentially, while communication can occur concurrently with computation. Let us define `t_sp[g]` as the start time and `t_ep[g]` as the end time of the post-computation task for group $g$. We can construct a recurrence relation by simulating the progression group by group, from $g=1$ to $g=G$.\n\nLet's initialize `t_sp[0] = 0` and `t_ep[0] = 0` to represent the state before any work begins.\n\nFor group $g = 1, 2, \\dots, G$:\n\n1.  **Pre-computation Task (`C-pre_g`)**: The CPU is used sequentially. The pre-computation for group $g$ can only start after all computations for group $g-1$ are finished. The CPU becomes free at time `t_ep[g-1]`.\n    - Start time of `C-pre_g`: `t_ep[g-1]`\n    - End time of `C-pre_g`: `t_end_pre_g = t_ep[g-1] + T_{\\mathrm{pre}}`\n\n2.  **Communication Task (`Comm_g`)**: According to the rules, the communication for group $g$ starts at the same time as the post-computation for group $g-1$. For the first group ($g=1$), this start time is $t=0$, which is consistent with our initialization of `t_sp[0]=0`.\n    - Start time of `Comm_g`: `t_start_comm_g = t_sp[g-1]`\n    - End time of `Comm_g`: `t_end_comm_g = t_start_comm_g + T_{\\mathrm{comm}}`\n\n3.  **Post-computation Task (`C-post_g`)**: This task can only begin when two conditions are met:\n    a) The CPU is available and has finished the pre-computation for this group (at time `t_end_pre_g`).\n    b) The necessary remote data has arrived, meaning the communication for this group is complete (at time `t_end_comm_g`).\n    Therefore, the start time is the maximum of these two completion times.\n    - Start time of `C-post_g`: `t_sp[g] = \\max(t_end_pre_g, t_end_comm_g)`\n    - End time of `C-post_g`: `t_ep[g] = t_sp[g] + T_{\\mathrm{post}}`\n\nThe total time for the process, $T_i(G)$, is the completion time of the very last task, which is `t_ep[G]`.\n\nThis iterative simulation correctly models the dependencies and resource constraints described in the problem.\n\n**Algorithm for Finding Optimal Efficiency**\n\nThe complete algorithm proceeds as follows:\n1.  For each test case, parse the input parameters: $P, p, k, L, B$ and the arrays $L_i, R_i, B_i$.\n2.  Calculate the serial reference time, $T_1 = k \\, p^2 \\sum_{j=1}^P (L_j + R_j)$.\n3.  Initialize a variable `max_efficiency` to a value less than any possible efficiency (e.g., $-1.0$).\n4.  Iterate through each candidate group count $G \\in \\{1, 2, 4, 8, 16, 32\\}$.\n    a. For the current $G$, calculate the parallel makespan $T_P(G)$. This requires another loop running through each process $i=1, \\dots, P$.\n        i. For each process $i$, calculate its total time $T_i(G)$ by implementing the iterative pipeline simulation described above.\n        ii. The makespan is the maximum of these times: $T_P(G) = \\max_{i} T_i(G)$.\n    b. Calculate the strong-scaling efficiency for this $G$: $E_s(P;G) = \\frac{T_1}{P \\cdot T_P(G)}$.\n    c. Update `max_efficiency = \\max(max_efficiency, E_s(P;G))$.\n5.  After checking all candidate $G$ values, the resulting `max_efficiency` is the optimal efficiency for the test case.\n6.  Collect the optimal efficiencies for all test cases and format them into the required string output.\n\nThis methodology provides a robust and verifiable solution that directly implements the model specified in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_time_for_process(G, p, k, L_val, R_val, B_val, L_comm, B_comm):\n    \"\"\"\n    Simulates the pipelined execution for a single process to find its total runtime.\n\n    Args:\n        G (int): The number of groups.\n        p (int): The expansion order.\n        k (float): The compute time constant.\n        L_val (int): The number of local M2L pairs for this process.\n        R_val (int): The number of remote M2L pairs for this process.\n        B_val (int): The number of remote source boxes for this process.\n        L_comm (float): The network latency.\n        B_comm (float): The network bandwidth.\n\n    Returns:\n        float: The total execution time for this process.\n    \"\"\"\n    # Calculate per-group timings based on the problem's model\n    T_pre = (L_val / G) * k * p**2\n    T_post = (R_val / G) * k * p**2\n    \n    # Message size includes (p+1)^2 coefficients, each 8 bytes\n    msg_size_per_group = (B_val / G) * (p + 1)**2 * 8\n    T_comm = L_comm + msg_size_per_group / B_comm\n    \n    # Initialize timing variables for the pipeline simulation\n    # t_ep_prev: end time of the previous group's post-computation (CPU free time)\n    # t_sp_prev: start time of the previous group's post-computation\n    t_ep_prev = 0.0\n    t_sp_prev = 0.0\n    \n    # Iterate through each group to simulate the Gantt chart\n    for _ in range(G):\n        # The pre-computation for the current group starts after the previous group's\n        # entire computation (pre and post) is finished.\n        t_end_pre = t_ep_prev + T_pre\n        \n        # The communication for the current group starts when the previous group's\n        # post-computation began. For g=1, t_sp_prev is 0.\n        t_start_comm = t_sp_prev\n        t_end_comm = t_start_comm + T_comm\n        \n        # The post-computation for the current group can only start after its own\n        # pre-computation is done AND its communication has completed.\n        t_sp_curr = max(t_end_pre, t_end_comm)\n        \n        # The end time is the start time plus the duration.\n        t_ep_curr = t_sp_curr + T_post\n        \n        # Update state for the next iteration\n        t_ep_prev = t_ep_curr\n        t_sp_prev = t_sp_curr\n        \n    # The total time for the process is the end time of the last group's computation\n    return t_ep_prev\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"P\": 4, \"p\": 8, \"k\": 2.0e-7, \"L\": 5.0e-5, \"B\": 2.0e8,\n            \"L_i\": [40000, 38000, 42000, 41000],\n            \"R_i\": [5000, 5200, 4800, 5100],\n            \"B_i\": [800, 820, 780, 790]\n        },\n        {\n            \"P\": 4, \"p\": 10, \"k\": 1.5e-7, \"L\": 2.0e-4, \"B\": 1.0e8,\n            \"L_i\": [20000, 15000, 14000, 13000],\n            \"R_i\": [60000, 4000, 3000, 2000],\n            \"B_i\": [10000, 600, 500, 400]\n        },\n        {\n            \"P\": 8, \"p\": 6, \"k\": 2.5e-7, \"L\": 1.0e-3, \"B\": 3.0e7,\n            \"L_i\": [10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000],\n            \"R_i\": [800, 800, 800, 800, 800, 800, 800, 800],\n            \"B_i\": [200, 200, 200, 200, 200, 200, 200, 200]\n        }\n    ]\n    \n    candidate_G = [1, 2, 4, 8, 16, 32]\n    \n    results = []\n    \n    for case in test_cases:\n        P = case['P']\n        p = case['p']\n        k = case['k']\n        L_comm = case['L']\n        B_comm = case['B']\n        L_i = case['L_i']\n        R_i = case['R_i']\n        B_i = case['B_i']\n        \n        # Calculate the single-process reference time T1\n        total_m2l_pairs = sum(L_i) + sum(R_i)\n        T1 = k * p**2 * total_m2l_pairs\n        \n        max_efficiency = -1.0\n        \n        # Search for the optimal number of groups G\n        for G in candidate_G:\n            makespan_T_P_G = 0.0\n            # Find the maximum execution time (makespan) across all processes\n            for i in range(P):\n                T_i_G = calculate_time_for_process(\n                    G, p, k, L_i[i], R_i[i], B_i[i], L_comm, B_comm\n                )\n                if T_i_G > makespan_T_P_G:\n                    makespan_T_P_G = T_i_G\n            \n            # Calculate strong scaling efficiency\n            if makespan_T_P_G > 0:\n                efficiency = T1 / (P * makespan_T_P_G)\n            else:\n                efficiency = 0.0 # Should not occur in valid cases\n                \n            if efficiency > max_efficiency:\n                max_efficiency = efficiency\n                \n        results.append(f\"{max_efficiency:.6f}\")\n        \n    # Print the final results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}