## Introduction
Seismic migration is a cornerstone of exploration [geophysics](@entry_id:147342), transforming raw seismic data into interpretable images of the Earth's subsurface. While conventional methods like Reverse-Time Migration (RTM) provide a robust first look, they often suffer from artifacts, uneven illumination, and limited resolution, which hinder quantitative interpretation. This knowledge gap calls for a more sophisticated approach that can produce a true-amplitude, higher-fidelity representation of subsurface reflectivity. Least-squares migration (LSM) addresses this challenge by reformulating migration as a formal inverse problem, seeking a model that best explains the observed data in a [least-squares](@entry_id:173916) sense.

This article provides a comprehensive overview of least-squares migration, designed for graduate-level students and researchers in [computational geophysics](@entry_id:747618). You will learn not just the 'what' but the 'why' and 'how' of this powerful imaging technique. We will begin in the "Principles and Mechanisms" chapter by deriving the linearized [forward model](@entry_id:148443) and framing [seismic imaging](@entry_id:273056) as a least-squares inverse problem, explaining the fundamental roles of the adjoint and normal operators. The "Applications and Interdisciplinary Connections" chapter will then explore how advanced regularization, high-performance computing, and enhanced wave physics extend the LSM framework to tackle real-world complexities. Finally, the "Hands-On Practices" section provides a bridge from theory to application, outlining practical coding exercises that reinforce the core concepts discussed. By the end, you will have a deep understanding of how LSM improves [image quality](@entry_id:176544) and its place within the modern seismic processing workflow.

## Principles and Mechanisms

This chapter elucidates the foundational principles and operational mechanisms of [least-squares](@entry_id:173916) migration (LSM). We begin by establishing the linear forward model that relates subsurface reflectivity to measured seismic data. Subsequently, we formulate [seismic migration](@entry_id:754641) as a linear [inverse problem](@entry_id:634767) within a [least-squares](@entry_id:173916) framework. The roles of the adjoint and normal operators are detailed, providing insight into the nature of conventional migration images and their limitations. Finally, we describe the iterative process that defines LSM, explaining how it systematically improves [image quality](@entry_id:176544) by minimizing [data misfit](@entry_id:748209), and we explore the practical aspects and challenges of this advanced imaging methodology.

### The Linearized Forward Model

The physical basis for [seismic imaging](@entry_id:273056) is the propagation of acoustic or [elastic waves](@entry_id:196203) through the Earth's subsurface. For many exploration geophysics applications, the wavefield can be described by the constant-density [acoustic wave equation](@entry_id:746230). The relationship between the subsurface properties and the recorded seismic data is fundamentally nonlinear. However, for the purpose of imaging sharp contrasts in material properties, it is extraordinarily useful to linearize this relationship.

We begin by representing the subsurface as a smooth, known **background medium** upon which small, localized perturbations are superimposed. Let the squared slowness of the medium (the reciprocal of the squared velocity, $1/c^2$) be denoted by $m(\mathbf{x})$. We can decompose this into a known background component $m_0(\mathbf{x})$ and a small perturbation $r(\mathbf{x})$, which we will identify as the **reflectivity**:
$m(\mathbf{x}) = m_0(\mathbf{x}) + r(\mathbf{x})$.

The total pressure field $p(\mathbf{x}, t)$ is the solution to the [acoustic wave equation](@entry_id:746230). Under the assumption that the reflectivity $r(\mathbf{x})$ is small, the total field can be decomposed into the background field $p_0(\mathbf{x}, t)$ (the wavefield that would propagate in the background medium alone) and a scattered field $p_s(\mathbf{x}, t)$ (the wavefield generated by the interaction with the reflectivity). This leads to the **first Born approximation**, a [linearization](@entry_id:267670) of the wave equation. This approximation neglects all events that involve scattering more than once (i.e., multiple scattering).

Under the Born approximation, the scattered data $d$ is linearly related to the reflectivity model $r$. We can express this relationship through a linear operator, known as the **linearized Born modeling operator**, denoted by $A$:
$d = A r$.

In the temporal-frequency domain, the action of this operator can be expressed as an integral equation . For a source at location $\mathbf{x}_s$ with frequency spectrum $S(\omega)$, and a receiver at $\mathbf{x}_r$, the scattered data $d(\omega, \mathbf{x}_r, \mathbf{x}_s)$ is given by:
$$
d(\omega, \mathbf{x}_r, \mathbf{x}_s) = A[r](\omega, \mathbf{x}_r, \mathbf{x}_s) = \omega^2 S(\omega) \int_{\Omega} r(\mathbf{x}) \, G_0(\mathbf{x}_r, \mathbf{x}, \omega) \, G_0(\mathbf{x}, \mathbf{x}_s, \omega) \, \mathrm{d}\mathbf{x}
$$
Here, $G_0(\mathbf{x}_a, \mathbf{x}_b, \omega)$ is the frequency-domain Green's function of the background medium $m_0(\mathbf{x})$, which represents the wavefield at $\mathbf{x}_a$ due to a [point source](@entry_id:196698) at $\mathbf{x}_b$. Physically, this equation describes the single-scattering process: a wave propagates from the source $\mathbf{x}_s$ to a scatterer at $\mathbf{x}$ (described by $G_0(\mathbf{x}, \mathbf{x}_s, \omega)$), interacts with the reflectivity $r(\mathbf{x})$, and then propagates from the scatterer to the receiver $\mathbf{x}_r$ (described by $G_0(\mathbf{x}_r, \mathbf{x}, \omega)$). The integral sums the contributions from all possible scatterer locations $\mathbf{x}$ in the model domain $\Omega$. The source signature, or **source [wavelet](@entry_id:204342)**, also plays a critical role, shaping the frequency content of the data through the multiplication by $S(\omega)$ .

It is crucial to distinguish this linearized model from **full-[waveform modeling](@entry_id:756631)**. The latter directly solves the fully [nonlinear wave equation](@entry_id:189472) in the total medium $m(\mathbf{x})$, accounting for all orders of scattering and complex wave phenomena. Least-squares migration operates on the premise that the simpler, linear relationship $d = Ar$ is a sufficiently accurate model for the primary reflections in the data.

### The Least-Squares Inverse Problem

With the forward model established, [seismic migration](@entry_id:754641) can be framed as an inverse problem: given the measured seismic data $d_{obs}$, estimate the subsurface reflectivity model $r$. The linearity of the [forward model](@entry_id:148443) $d = Ar$ suggests a direct inversion, but in practice, the operator $A$ is rarely invertible in a stable manner. A more robust approach is to find a model $r$ that, when mapped to the data space, provides the best possible fit to the observed data.

This "best fit" is typically quantified by minimizing a data [misfit functional](@entry_id:752011). The standard choice is a **least-squares objective function**, which measures the squared $L^2$-norm of the difference between the predicted data $Ar$ and the observed data $d_{obs}$:
$$
J(r) = \frac{1}{2} \| Ar - d_{obs} \|_2^2
$$
In practice, seismic data are often contaminated by noise with varying characteristics. To account for this, we introduce a **weighted [least-squares](@entry_id:173916)** formulation. If we have knowledge about the statistical properties of the noise, we can incorporate a weighting operator $W$ to down-weight noisy data components and up-weight reliable ones . The objective function becomes:
$$
J(r) = \frac{1}{2} \| W^{1/2} (Ar - d_{obs}) \|_2^2 = \frac{1}{2} (Ar - d_{obs})^{\top} W (Ar - d_{obs})
$$
From the perspective of **maximum likelihood estimation**, if we assume the noise is additive, zero-mean, and Gaussian with a [data covariance](@entry_id:748192) matrix $C_d$, the optimal choice for the weighting matrix is the inverse of the [data covariance](@entry_id:748192), $W = C_d^{-1}$. This matrix, known as the **[precision matrix](@entry_id:264481)**, ensures that the estimation process is statistically optimal, giving less influence to data components with high variance and properly handling correlations in the noise.

### The Adjoint Operator and Reverse-Time Migration

To minimize the [objective function](@entry_id:267263) $J(r)$, we typically employ [gradient-based optimization](@entry_id:169228) methods. The gradient of $J(r)$ is given by:
$$
\nabla J(r) = A^{\top} W (Ar - d_{obs})
$$
Here, $A^{\top}$ is the **adjoint** (or transpose, in the case of real-valued discrete operators) of the [forward modeling](@entry_id:749528) operator $A$. The [adjoint operator](@entry_id:147736) is formally defined with respect to the inner products on the model and data spaces. For any model $r$ and data $d$, it must satisfy the identity:
$$
\langle Ar, d \rangle_{D} = \langle r, A^{\top} d \rangle_{M}
$$
where $\langle \cdot, \cdot \rangle_{D}$ and $\langle \cdot, \cdot \rangle_{M}$ are the data-space and model-space inner products, respectively.

The construction of the [adjoint operator](@entry_id:147736) is a critical step and has a profound physical interpretation . The process, known as the **[adjoint-state method](@entry_id:633964)**, involves:
1.  **Source Injection**: The data $d$ (or, in optimization, the data residuals) are injected as time-[dependent sources](@entry_id:267114) at the receiver locations.
2.  **Backward Propagation**: An adjoint wave equation, which is mathematically the adjoint of the forward wave equation, is solved backward in time, from the final recording time $T$ to time $0$. The solution to this is the **adjoint wavefield**.
3.  **Imaging Condition**: The reflectivity image is formed by the **zero-lag cross-correlation** of the adjoint wavefield with the background forward-propagating wavefield.

This procedure, when applied directly to the observed data, $m_{RTM} = A^{\top}d_{obs}$, is precisely the algorithm for **Reverse-Time Migration (RTM)**. RTM is thus revealed not as an inversion itself, but as the application of the [adjoint operator](@entry_id:147736) to the data. This insight is fundamental: the RTM image is the steepest-descent direction of the [misfit functional](@entry_id:752011) evaluated at $r=0$.

The numerical implementation of the adjoint requires careful attention to detail. For the adjoint identity to hold for discrete operators, every component of the [forward modeling](@entry_id:749528) code must be correctly "transposed". This includes replacing discrete derivative stencils with their transposes and, crucially, handling boundary conditions correctly. For instance, if a **Perfectly Matched Layer (PML)** is used in the [forward model](@entry_id:148443) to absorb outgoing waves, the adjoint model must include the transpose of the PML operator. For a standard split-field PML, this means the adjoint simulation must also be damped with the exact same damping profiles, as the damping operator is self-adjoint . Failure to do so constitutes an "adjoint crime" and results in an incorrect gradient.

### The Normal Operator and the Point-Spread Function

If RTM is just the application of the adjoint, what is the relationship between the RTM image $m_{RTM}$ and the true reflectivity $r$? If we assume the data are perfectly modeled by $d_{obs} = Ar_{true}$, then the RTM image is:
$$
m_{RTM} = A^{\top} d_{obs} = A^{\top} A r_{true} = H r_{true}
$$
The operator $H = A^{\top} A$ is the **[normal operator](@entry_id:270585)**, which is also the Hessian of the unweighted [least-squares](@entry_id:173916) [objective function](@entry_id:267263). This equation shows that the RTM image is not the true reflectivity, but the true reflectivity convolved with or "blurred" by the action of the [normal operator](@entry_id:270585).

The effect of this blurring is characterized by the **Point-Spread Function (PSF)**. The PSF at a location $\mathbf{x}_0$ is defined as the image of a single point diffractor (an impulse in reflectivity, $\delta(\mathbf{x}-\mathbf{x}_0)$) after it has passed through the full [forward modeling](@entry_id:749528) and migration sequence . Mathematically, the PSF is the action of the [normal operator](@entry_id:270585) on an impulse:
$$
\text{PSF}(\mathbf{x}; \mathbf{x}_0) = (H \delta_{\mathbf{x}_0})(\mathbf{x})
$$
The PSF reveals how the imaging system maps energy from a single point in the subsurface back into the image space. An ideal imaging system would have a PSF that is a perfect impulse. In reality, the PSF has a finite width and sidelobes.
-   The **width of the PSF's central lobe** determines the spatial resolution of the image.
-   The **sidelobes and asymmetries** of the PSF create imaging artifacts, such as [crosstalk](@entry_id:136295) between different reflectors.

The shape of the PSF is determined by physical limitations of the seismic experiment, including the finite bandwidth of the source, the limited spatial [aperture](@entry_id:172936) of the sources and receivers, and any inaccuracies in the background velocity model.

### Iterative Inversion: The Mechanism of Least-Squares Migration

Least-squares migration can be understood as an attempt to deconvolve the PSF from the RTM image by approximately solving the [normal equations](@entry_id:142238), $Hr = A^{\top}d$. This inversion seeks to find an image $r$ that is free from the predictable blurring and illumination effects encoded in the [normal operator](@entry_id:270585) $H$. Since $H$ is often massive and ill-conditioned, this inversion is performed iteratively.

A common approach is a **[gradient descent](@entry_id:145942)** (or steepest descent) method. Starting with an initial model (e.g., $r_0=0$), the model is updated iteratively in the direction opposite to the gradient of the objective function:
$$
r_{k+1} = r_k - \alpha_k \nabla J(r_k) = r_k - \alpha_k A^{\top} W (A r_k - d_{obs})
$$
where $k$ is the iteration number and $\alpha_k$ is a scalar step length.

The physical workflow for a single iteration of LSM is as follows :
1.  **Synthesize Data**: Using the current reflectivity model $r_k$, simulate the predicted seismic data, $d_k = A r_k$. This requires one forward wave propagation simulation for each source.
2.  **Calculate Residual**: Compute the difference between the observed data and the synthesized data, $e_k = A r_k - d_{obs}$. This residual represents the part of the data not yet explained by the current model.
3.  **Compute Gradient**: Migrate the weighted residual by applying the [adjoint operator](@entry_id:147736), $g_k = A^{\top} W e_k$. This requires one adjoint (backward-in-time) [wave propagation](@entry_id:144063) for each source, creating the gradient image.
4.  **Update Model**: Update the reflectivity model using the computed gradient and a suitable step length $\alpha_k$: $r_{k+1} = r_k - \alpha_k g_k$.

This iterative process progressively refines the model $r_k$ to better explain the observed data. In doing so, it produces an image with more balanced amplitudes, reduced acquisition footprint, and higher resolution compared to a conventional RTM image. Essentially, each iteration serves to deconvolve another "layer" of the PSF's blurring effect . However, this [deconvolution](@entry_id:141233) process is not without its perils. The ill-conditioning of $H$ means that a naive inversion can dramatically amplify noise and produce [ringing artifacts](@entry_id:147177), similar to the Gibbs phenomenon, especially where the operator's spectrum has sharp cutoffs. Regularization is almost always required to stabilize the inversion.

### Practical Aspects of Implementation

The convergence and efficiency of the iterative LSM process depend on several factors. Two key elements are the choice of step length $\alpha_k$ and the conditioning of the [normal operator](@entry_id:270585) $H$.

The step length $\alpha_k$ is determined at each iteration via a **[line search](@entry_id:141607)**. For a quadratic [objective function](@entry_id:267263), a [closed-form expression](@entry_id:267458) for the optimal step length that minimizes the objective along the gradient direction can be derived :
$$
\alpha_k = \frac{\| g_k \|_2^2}{\| W^{1/2} A g_k \|_2^2} = \frac{g_k^{\top} g_k}{g_k^{\top} H g_k}
$$
This [optimal step size](@entry_id:143372) requires computing $Ag_k$, which involves an additional set of forward propagations. However, by pre-computing and storing certain quantities, this can be done efficiently. The ability to find a good step length is critical for ensuring the convergence of the algorithm.

The speed of convergence of [gradient-based methods](@entry_id:749986) is governed by the **condition number** of the Hessian, $\kappa(H) = \lambda_{\max}(H) / \lambda_{\min}(H)$, where $\lambda_{\max}$ and $\lambda_{\min}$ are the maximum and minimum eigenvalues of $H$. A large condition number indicates an [ill-conditioned problem](@entry_id:143128) and leads to very slow convergence for simple [gradient descent](@entry_id:145942) . More advanced methods, such as the [conjugate gradient method](@entry_id:143436) or [preconditioned gradient descent](@entry_id:753678), are often used to accelerate convergence by incorporating information from previous iterations or by applying a preconditioner that approximates $H^{-1}$.

### Challenges and Limitations in Practice

The success of LSM relies on the accuracy of its underlying assumptions, primarily the linearity of the model and the correctness of the background velocity $v_0(\mathbf{x})$.

A key assumption is that the source [wavelet](@entry_id:204342) is known. The wavelet filters the data, and to obtain a **true-amplitude** reflectivity image suitable for [quantitative analysis](@entry_id:149547), its effects must be removed. This can be done by including the known wavelet in the operators $A$ and $A^{\top}$ so that LSM inverts its effect, or by explicit deconvolution. If the wavelet is unknown, it must first be estimated from the data .

The most significant challenge is the dependency on the background velocity model. The forward operator $A$ and its adjoint $A^{\top}$ are built using an assumed velocity $v_0(\mathbf{x})$. If this velocity is incorrect, the operator $A$ is a mismatched representation of the true physics. When the velocity errors are small, LSM can partially compensate by making small adjustments to reflector amplitudes and positions to improve the data fit. However, large velocity errors violate the linearization assumption itself. The true data will contain components that lie outside the range of the mismatched operator $A$, meaning no choice of reflectivity $r$ can fully explain the data. This results in persistent kinematic artifacts, such as **residual moveout** in common-image gathers and a general defocusing of the image, which cannot be corrected by iterating on $r$ alone . Overcoming this limitation requires moving beyond the fixed-velocity assumption of LSM and into the realm of velocity model building and nonlinear inversion, such as Full-Waveform Inversion (FWI).