{
    "hands_on_practices": [
        {
            "introduction": "A stable numerical simulation is the bedrock of any wave-based inversion method. This exercise focuses on the Courant–Friedrichs–Lewy (CFL) stability condition, a fundamental principle governing explicit finite-difference time-domain schemes. By deriving the discrete update for the 2D acoustic wave equation and performing a von Neumann stability analysis, you will gain hands-on experience in ensuring that both your forward and adjoint wavefield simulations do not diverge, a critical first step in building a reliable inversion code .",
            "id": "3598910",
            "problem": "Consider constant-density acoustic Full Waveform Inversion (FWI), where the forward and adjoint wavefields are propagated by the two-dimensional acoustic wave equation. Let the pressure field be denoted by $p(x,z,t)$, and consider a rectangular computational domain discretized on a uniform but possibly anisotropic Cartesian grid with spacings $\\Delta x$ and $\\Delta z$ in the horizontal and vertical directions, respectively, and with a uniform time step $\\Delta t$. The acoustic wave equation is\n$$\\frac{\\partial^{2} p}{\\partial t^{2}}(x,z,t) = v(x,z)^{2} \\nabla^{2} p(x,z,t) + s(x,z,t),$$\nwhere $v(x,z)$ is the spatially varying wavespeed and $s(x,z,t)$ is a source term. In adjoint-state FWI, stable time stepping is required both for forward propagation and adjoint back-propagation to ensure a well-defined gradient of the objective function.\n\nStarting from the above continuous partial differential equation and using only second-order central differences in space and time on the given grid, derive the explicit discrete update formula that advances $p_{i,j}^{n}$ to $p_{i,j}^{n+1}$ at grid node $(i,j)$ and time index $n$, in terms of $p$ at neighboring spatial nodes at time index $n$ and the previous time level $p_{i,j}^{n-1}$. Then, perform a Fourier (von Neumann) stability analysis for the homogeneous case $s(x,z,t)=0$ with constant $v(x,z) = v_{\\max}$ to obtain the Courant–Friedrichs–Lewy (CFL) stability condition expressed in terms of $v_{\\max}$, $\\Delta t$, $\\Delta x$, and $\\Delta z$.\n\nFinally, evaluate the maximum stable time step for the following physically realistic modeling parameters used in a production FWI workflow: $v_{\\max} = 3600 \\,\\text{m/s}$, $\\Delta x = 7.5 \\,\\text{m}$, and $\\Delta z = 10 \\,\\text{m}$. Report the maximum allowable $\\Delta t$ in seconds, rounded to four significant figures. Express the final answer in seconds.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to proceed with a unique solution. We begin by discretizing the given partial differential equation, then perform a stability analysis to find the constraint on the time step, and finally evaluate this constraint for the given parameters.\n\nThe continuous two-dimensional acoustic wave equation is given by:\n$$\n\\frac{\\partial^{2} p}{\\partial t^{2}}(x,z,t) = v(x,z)^{2} \\nabla^{2} p(x,z,t) + s(x,z,t)\n$$\nwhere $\\nabla^{2} = \\frac{\\partial^{2}}{\\partial x^{2}} + \\frac{\\partial^{2}}{\\partial z^{2}}$.\n\nFirst, we derive the explicit discrete update formula. We use a second-order central difference approximation for the partial derivatives on a grid with points $(x_i, z_j, t_n) = (i\\Delta x, j\\Delta z, n\\Delta t)$. The pressure at such a point is denoted by $p_{i,j}^{n}$.\n\nThe second time derivative is approximated as:\n$$\n\\frac{\\partial^{2} p}{\\partial t^{2}} \\bigg|_{i,j,n} \\approx \\frac{p_{i,j}^{n+1} - 2 p_{i,j}^{n} + p_{i,j}^{n-1}}{\\Delta t^2}\n$$\n\nThe second spatial derivatives are approximated as:\n$$\n\\frac{\\partial^{2} p}{\\partial x^{2}} \\bigg|_{i,j,n} \\approx \\frac{p_{i+1,j}^{n} - 2 p_{i,j}^{n} + p_{i-1,j}^{n}}{\\Delta x^2}\n$$\n$$\n\\frac{\\partial^{2} p}{\\partial z^{2}} \\bigg|_{i,j,n} \\approx \\frac{p_{i,j+1}^{n} - 2 p_{i,j}^{n} + p_{i,j-1}^{n}}{\\Delta z^2}\n$$\n\nSubstituting these approximations into the wave equation at grid point $(i,j)$ and time level $n$, and letting $v_{i,j} = v(x_i, z_j)$ and $s_{i,j}^n = s(x_i, z_j, t_n)$, we obtain:\n$$\n\\frac{p_{i,j}^{n+1} - 2 p_{i,j}^{n} + p_{i,j}^{n-1}}{\\Delta t^2} = v_{i,j}^{2} \\left( \\frac{p_{i+1,j}^{n} - 2 p_{i,j}^{n} + p_{i-1,j}^{n}}{\\Delta x^2} + \\frac{p_{i,j+1}^{n} - 2 p_{i,j}^{n} + p_{i,j-1}^{n}}{\\Delta z^2} \\right) + s_{i,j}^n\n$$\nTo find the explicit update formula for $p_{i,j}^{n+1}$, we rearrange the equation:\n$$\np_{i,j}^{n+1} = 2 p_{i,j}^{n} - p_{i,j}^{n-1} + (v_{i,j} \\Delta t)^{2} \\left( \\frac{p_{i+1,j}^{n} - 2 p_{i,j}^{n} + p_{i-1,j}^{n}}{\\Delta x^2} + \\frac{p_{i,j+1}^{n} - 2 p_{i,j}^{n} + p_{i,j-1}^{n}}{\\Delta z^2} \\right) + \\Delta t^2 s_{i,j}^n\n$$\nThis is the required discrete update formula.\n\nNext, we perform a Fourier (von Neumann) stability analysis for the homogeneous case ($s(x,z,t)=0$) with a constant wavespeed ($v(x,z) = v_{\\max}$). The discrete equation simplifies to:\n$$\np_{i,j}^{n+1} = 2 p_{i,j}^{n} - p_{i,j}^{n-1} + (v_{\\max} \\Delta t)^{2} \\left( \\frac{p_{i+1,j}^{n} - 2 p_{i,j}^{n} + p_{i-1,j}^{n}}{\\Delta x^2} + \\frac{p_{i,j+1}^{n} - 2 p_{i,j}^{n} + p_{i,j-1}^{n}}{\\Delta z^2} \\right)\n$$\nWe consider a single Fourier mode solution of the form:\n$$\np_{i,j}^{n} = G^{n} \\exp(I (k_x i \\Delta x + k_z j \\Delta z))\n$$\nwhere $G$ is the amplification factor, $k_x$ and $k_z$ are the wavenumbers in the $x$ and $z$ directions, and $I = \\sqrt{-1}$. For the scheme to be stable, the magnitude of the amplification factor must satisfy $|G| \\leq 1$ for all wavenumbers.\n\nSubstituting the Fourier mode into the homogeneous discrete equation and dividing by $p_{i,j}^{n-1} = G^{n-1} \\exp(I(k_x i \\Delta x + k_z j \\Delta z))$ yields an equation for $G$:\n$$\nG^2 = 2G - 1 + G \\frac{(v_{\\max} \\Delta t)^2}{\\Delta x^2} \\left[ \\exp(I k_x \\Delta x) - 2 + \\exp(-I k_x \\Delta x) \\right] + G \\frac{(v_{\\max} \\Delta t)^2}{\\Delta z^2} \\left[ \\exp(I k_z \\Delta z) - 2 + \\exp(-I k_z \\Delta z) \\right]\n$$\nUsing the identity $\\exp(I\\theta) + \\exp(-I\\theta) = 2\\cos(\\theta)$, the terms in the square brackets become $2\\cos(k_x\\Delta x) - 2 = -2(1-\\cos(k_x\\Delta x))$ and $2\\cos(k_z\\Delta z) - 2 = -2(1-\\cos(k_z\\Delta z))$. Further, using the half-angle identity $1-\\cos(\\theta) = 2\\sin^2(\\theta/2)$, these simplify to $-4\\sin^2(k_x\\Delta x/2)$ and $-4\\sin^2(k_z\\Delta z/2)$, respectively.\n\nSubstituting these back gives:\n$$\nG^2 = 2G - 1 - 4G(v_{\\max} \\Delta t)^2 \\left[ \\frac{\\sin^2(k_x\\Delta x/2)}{\\Delta x^2} + \\frac{\\sin^2(k_z\\Delta z/2)}{\\Delta z^2} \\right]\n$$\nRearranging this into a standard quadratic form $aG^2+bG+c=0$ for $G$:\n$$\nG^2 - 2G \\left( 1 - 2(v_{\\max} \\Delta t)^2 \\left[ \\frac{\\sin^2(k_x\\Delta x/2)}{\\Delta x^2} + \\frac{\\sin^2(k_z\\Delta z/2)}{\\Delta z^2} \\right] \\right) + 1 = 0\n$$\nThis is a quadratic equation of the form $G^2 - 2\\alpha G + 1 = 0$, where\n$$\n\\alpha = 1 - 2(v_{\\max} \\Delta t)^2 \\left[ \\frac{\\sin^2(k_x\\Delta x/2)}{\\Delta x^2} + \\frac{\\sin^2(k_z\\Delta z/2)}{\\Delta z^2} \\right]\n$$\nThe roots are $G = \\alpha \\pm \\sqrt{\\alpha^2-1}$. For the stability condition $|G| \\leq 1$ to hold, the roots must be complex or on the unit circle, which requires the term under the square root to be non-positive, i.e., $\\alpha^2 - 1 \\leq 0$. This is equivalent to $|\\alpha| \\leq 1$.\n\nThe condition $\\alpha \\leq 1$ is always satisfied because the term being subtracted from $1$ is a sum of non-negative quantities. The critical condition for stability comes from $\\alpha \\geq -1$:\n$$\n1 - 2(v_{\\max} \\Delta t)^2 \\left[ \\frac{\\sin^2(k_x\\Delta x/2)}{\\Delta x^2} + \\frac{\\sin^2(k_z\\Delta z/2)}{\\Delta z^2} \\right] \\geq -1\n$$\n$$\n2 \\geq 2(v_{\\max} \\Delta t)^2 \\left[ \\frac{\\sin^2(k_x\\Delta x/2)}{\\Delta x^2} + \\frac{\\sin^2(k_z\\Delta z/2)}{\\Delta z^2} \\right]\n$$\n$$\n(v_{\\max} \\Delta t)^2 \\left[ \\frac{\\sin^2(k_x\\Delta x/2)}{\\Delta x^2} + \\frac{\\sin^2(k_z\\Delta z/2)}{\\Delta z^2} \\right] \\leq 1\n$$\nThis inequality must hold for all wavenumbers $k_x$ and $k_z$. The most restrictive constraint on $\\Delta t$ occurs when the term in the square brackets is maximized. This happens when $\\sin^2(k_x\\Delta x/2)=1$ and $\\sin^2(k_z\\Delta z/2)=1$, which corresponds to the highest frequencies resolvable on the grid (Nyquist frequencies, $k_x = \\pi/\\Delta x$ and $k_z = \\pi/\\Delta z$).\nThe maximum value of the bracketed term is thus $\\frac{1}{\\Delta x^2} + \\frac{1}{\\Delta z^2}$.\n\nThe stability condition becomes:\n$$\n(v_{\\max} \\Delta t)^2 \\left( \\frac{1}{\\Delta x^2} + \\frac{1}{\\Delta z^2} \\right) \\leq 1\n$$\nTaking the square root, we obtain the Courant–Friedrichs–Lewy (CFL) stability condition:\n$$\n\\Delta t \\leq \\frac{1}{v_{\\max} \\sqrt{\\frac{1}{\\Delta x^2} + \\frac{1}{\\Delta z^2}}}\n$$\n\nFinally, we evaluate the maximum stable time step, $\\Delta t_{\\max}$, for the given parameters: $v_{\\max} = 3600 \\,\\text{m/s}$, $\\Delta x = 7.5 \\,\\text{m}$, and $\\Delta z = 10 \\,\\text{m}$.\n$$\n\\Delta t_{\\max} = \\frac{1}{3600 \\sqrt{\\frac{1}{7.5^2} + \\frac{1}{10^2}}}\n$$\nWe compute the term under the square root:\n$$\n\\frac{1}{7.5^2} + \\frac{1}{10^2} = \\frac{1}{56.25} + \\frac{1}{100} = \\frac{16}{900} + \\frac{9}{900} = \\frac{25}{900} = \\frac{1}{36}\n$$\nThe square root is:\n$$\n\\sqrt{\\frac{1}{36}} = \\frac{1}{6}\n$$\nSubstituting this back into the expression for $\\Delta t_{\\max}$:\n$$\n\\Delta t_{\\max} = \\frac{1}{3600 \\times \\frac{1}{6}} = \\frac{6}{3600} = \\frac{1}{600} \\,\\text{s}\n$$\nAs a decimal, this is $\\Delta t_{\\max} = 0.001666... \\,\\text{s}$. Rounding to four significant figures, we get:\n$$\n\\Delta t_{\\max} \\approx 0.001667 \\,\\text{s}\n$$",
            "answer": "$$\\boxed{0.001667}$$"
        },
        {
            "introduction": "The choice of objective function is pivotal in determining the robustness and convergence of an inversion. While the standard least-squares ($L_2$) misfit is common, it is highly sensitive to non-Gaussian noise and data outliers. This practice explores a more robust alternative based on the Student-$t$ distribution, requiring you to derive the corresponding adjoint source from first principles and analyze its influence function, which automatically down-weights large-amplitude errors, a highly desirable property for processing real-world seismic data .",
            "id": "3598874",
            "problem": "Consider Full-Waveform Inversion (FWI), where the forward wavefield operator maps model parameters $m$ to predicted data $d_{\\mathrm{pred}}(t)$ at receiver locations via a partial differential equation such as the acoustic wave equation. In the adjoint-state method, the gradient with respect to $m$ is obtained by solving an adjoint wave equation driven by an adjoint source at the receivers that depends on the misfit functional’s derivative with respect to $d_{\\mathrm{pred}}(t)$. Suppose the data noise is modeled as Student-$t$ distributed with degrees of freedom $\\nu$ and scale parameter $\\sigma$, and let the residual be $r(t) = d_{\\mathrm{pred}}(t) - d_{\\mathrm{obs}}(t)$, with $d_{\\mathrm{obs}}(t)$ the observed data. Use the negative log-likelihood formulation as the misfit. In the Iteratively Reweighted Least Squares (IRLS) approach, the weight function per sample is defined as $w(t) = \\dfrac{\\nu + 1}{\\nu + r(t)^2/\\sigma^2}$.\n\nTasks:\n1. Starting from the Student-$t$ negative log-likelihood for independent samples, derive the expression for the adjoint source time series $s_{\\mathrm{adj}}(t)$ at receiver locations that must be injected into the adjoint wave equation to compute the gradient. Your derivation must proceed from first principles of likelihood-based misfit construction and the chain rule, and connect the result to the IRLS weight $w(t)$ provided above. Do not assume a pre-known adjoint source formula; derive it.\n2. Contrast the derived adjoint source with the Gaussian noise (least-squares) case. Define the influence function for a single sample as the derivative of the misfit with respect to $d_{\\mathrm{pred}}(t)$, and analyze its dependence on $r(t)$ for both Student-$t$ and Gaussian noise. Explain the robustness to outliers in terms of boundedness or growth of the influence function as $|r(t)|$ increases.\n3. Implement a program that, for each test case, computes:\n   - The IRLS weight $w(t)$ per time sample.\n   - The Student-$t$ adjoint source time series $s_{\\mathrm{adj}}^{\\mathrm{t}}(t)$ using your derived formula.\n   - The least-squares adjoint source $s_{\\mathrm{adj}}^{\\mathrm{ls}}(t)$ obtained from Gaussian noise modeling.\n   - The outlier suppression factor $\\mathrm{SF}$ defined as the ratio of the absolute adjoint amplitude at the time index of the largest absolute residual $t_{\\mathrm{out}}$ for Student-$t$ versus least-squares:\n     $$\\mathrm{SF} = \\dfrac{\\left|s_{\\mathrm{adj}}^{\\mathrm{t}}(t_{\\mathrm{out}})\\right|}{\\left|s_{\\mathrm{adj}}^{\\mathrm{ls}}(t_{\\mathrm{out}})\\right|}.$$\n     If $\\left|s_{\\mathrm{adj}}^{\\mathrm{ls}}(t_{\\mathrm{out}})\\right| = 0$, define $\\mathrm{SF} = 1.0$ by convention.\n   - The global norm ratio $\\mathrm{NR}$ defined as the ratio of the Euclidean norms (square root of sum of squares) of the Student-$t$ and least-squares adjoint source time series:\n     $$\\mathrm{NR} = \\dfrac{\\left\\|s_{\\mathrm{adj}}^{\\mathrm{t}}\\right\\|_2}{\\left\\|s_{\\mathrm{adj}}^{\\mathrm{ls}}\\right\\|_2}.$$\n     If $\\left\\|s_{\\mathrm{adj}}^{\\mathrm{ls}}\\right\\|_2 = 0$, define $\\mathrm{NR} = 1.0$ by convention.\n4. The program should use the following test suite, where all residuals are dimensionless and time sampling is uniform:\n   - Test case $1$: $\\nu = 3$, $\\sigma = 1.0$, $r(t) = [0.1, -0.2, 0.3, 5.0, -0.1]$.\n   - Test case $2$: $\\nu = 1$, $\\sigma = 1.0$, $r(t) = [0.0, 0.0, 10.0, 0.0]$.\n   - Test case $3$: $\\nu = 100$, $\\sigma = 1.0$, $r(t) = [0.5, -0.5, 3.0]$.\n   - Test case $4$: $\\nu = 3$, $\\sigma = 1.0$, $r(t) = [0.0, 0.0]$.\n5. For each test case, compute $\\mathrm{SF}$ and $\\mathrm{NR}$ as real numbers (floats). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\mathrm{SF}_1, \\mathrm{NR}_1, \\mathrm{SF}_2, \\mathrm{NR}_2, \\mathrm{SF}_3, \\mathrm{NR}_3, \\mathrm{SF}_4, \\mathrm{NR}_4]$.\n\nNo physical units are required because the residuals are dimensionless and normalized. Angles are not involved.\n\nYour final answer must be a complete, runnable program that performs the above computations and produces the exact output format specified.",
            "solution": "The problem is evaluated as valid, being scientifically grounded in computational geophysics and statistical optimization, well-posed, and internally consistent. We proceed with a full solution.\n\nThe solution is presented in two parts as requested. First, a theoretical derivation and analysis, followed by the implementation details which bridge to the final computational code.\n\n**1. Derivation of the Adjoint Source for a Student-$t$ Likelihood**\n\nThe adjoint-state method for gradient computation in Full-Waveform Inversion (FWI) requires an adjoint source term, which is the derivative of the misfit functional $\\mathcal{J}$ with respect to the predicted data $d_{\\mathrm{pred}}(t)$. We are given that the misfit is the negative log-likelihood of the data, assuming the noise follows a Student-$t$ distribution.\n\nThe probability density function (PDF) for a random variable $x$ following a Student-$t$ distribution with $\\nu$ degrees of freedom and scale parameter $\\sigma$ is given by:\n$$\np(x | \\nu, \\sigma) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma} \\left(1 + \\frac{x^2}{\\nu\\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}\n$$\nwhere $\\Gamma$ is the gamma function.\n\nWe model the noise at each discrete time sample $t$ as an independent realization of this distribution, where the variable $x$ is the residual $r(t) = d_{\\mathrm{pred}}(t) - d_{\\mathrm{obs}}(t)$. The total likelihood for all time samples is the product of the individual probabilities, and the log-likelihood is the sum:\n$$\n\\log \\mathcal{L} = \\sum_t \\log p(r(t) | \\nu, \\sigma)\n$$\nThe misfit functional $\\mathcal{J}$ is the negative log-likelihood:\n$$\n\\mathcal{J} = -\\log \\mathcal{L} = -\\sum_t \\left[ \\log(C) - \\frac{\\nu+1}{2} \\log\\left(1 + \\frac{r(t)^2}{\\nu\\sigma^2}\\right) \\right]\n$$\nwhere $C$ is the normalization constant, which does not depend on the model parameters $m$ (via $d_{\\mathrm{pred}}(t)$ and $r(t)$). When computing gradients, this constant term vanishes. We thus consider the misfit functional:\n$$\n\\mathcal{J} = \\frac{\\nu+1}{2} \\sum_t \\log\\left(1 + \\frac{r(t)^2}{\\nu\\sigma^2}\\right)\n$$\nThe adjoint source at a specific time $t$, denoted $s_{\\mathrm{adj}}(t)$, is the derivative of $\\mathcal{J}$ with respect to the corresponding predicted data sample $d_{\\mathrm{pred}}(t)$. Since the sum is over independent samples, the derivative only acts on the term for that specific time:\n$$\ns_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{\\partial \\mathcal{J}}{\\partial d_{\\mathrm{pred}}(t)} = \\frac{\\partial}{\\partial d_{\\mathrm{pred}}(t)} \\left[ \\frac{\\nu+1}{2} \\log\\left(1 + \\frac{r(t)^2}{\\nu\\sigma^2}\\right) \\right]\n$$\nUsing the chain rule, and noting that $\\frac{\\partial r(t)}{\\partial d_{\\mathrm{pred}}(t)} = \\frac{\\partial}{\\partial d_{\\mathrm{pred}}(t)} (d_{\\mathrm{pred}}(t) - d_{\\mathrm{obs}}(t)) = 1$, we get:\n$$\ns_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r(t)^2}{\\nu\\sigma^2}} \\cdot \\frac{\\partial}{\\partial r(t)}\\left( \\frac{r(t)^2}{\\nu\\sigma^2} \\right) \\cdot \\frac{\\partial r(t)}{\\partial d_{\\mathrm{pred}}(t)}\n$$\n$$\ns_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{\\nu+1}{2} \\cdot \\frac{1}{\\frac{\\nu\\sigma^2 + r(t)^2}{\\nu\\sigma^2}} \\cdot \\frac{2r(t)}{\\nu\\sigma^2} \\cdot 1\n$$\nSimplifying the expression yields the adjoint source for the Student-$t$ misfit:\n$$\ns_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{(\\nu+1)r(t)}{\\nu\\sigma^2 + r(t)^2}\n$$\nThe problem provides the Iteratively Reweighted Least Squares (IRLS) weight function $w(t) = \\dfrac{\\nu + 1}{\\nu + r(t)^2/\\sigma^2}$. We can express our derived adjoint source in terms of $w(t)$:\n$$\ns_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{(\\nu+1)r(t)}{\\nu\\sigma^2 + r(t)^2} = \\frac{1}{\\sigma^2} \\cdot \\frac{(\\nu+1)r(t)}{\\nu + r(t)^2/\\sigma^2} = \\frac{w(t)r(t)}{\\sigma^2}\n$$\nThis relationship shows that the adjoint source is the residual $r(t)$ weighted by the function $w(t)$ (and scaled by $1/\\sigma^2$). The weight $w(t)$ is large for small residuals and decreases as the residual magnitude increases, which is the mechanism for robustness.\n\n**2. Comparison with Gaussian Noise (Least-Squares) and Robustness Analysis**\n\nFor the standard least-squares (L2) misfit, the noise is assumed to be Gaussian-distributed with zero mean. The misfit functional is proportional to the sum of squared residuals:\n$$\n\\mathcal{J}_{\\mathrm{ls}} = \\frac{1}{2} \\sum_t r(t)^2\n$$\nThe corresponding adjoint source, $s_{\\mathrm{adj}}^{\\mathrm{ls}}(t)$, is its derivative with respect to $d_{\\mathrm{pred}}(t)$:\n$$\ns_{\\mathrm{adj}}^{\\mathrm{ls}}(t) = \\frac{\\partial \\mathcal{J}_{\\mathrm{ls}}}{\\partial d_{\\mathrm{pred}}(t)} = \\frac{\\partial}{\\partial d_{\\mathrm{pred}}(t)} \\left( \\frac{1}{2}r(t)^2 \\right) = r(t) \\cdot \\frac{\\partial r(t)}{\\partial d_{\\mathrm{pred}}(t)} = r(t)\n$$\nThus, for the least-squares case, the adjoint source is simply the residual itself.\n\nThe problem defines the influence function $\\psi(r(t))$ as the derivative of the misfit with respect to a data sample, which is precisely the adjoint source.\n-   **Gaussian/Least-Squares Influence Function:** $\\psi_{\\mathrm{ls}}(r(t)) = s_{\\mathrm{adj}}^{\\mathrm{ls}}(t) = r(t)$.\n-   **Student-$t$ Influence Function:** $\\psi_{\\mathrm{t}}(r(t)) = s_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{(\\nu+1)r(t)}{\\nu\\sigma^2 + r(t)^2}$.\n\n**Analysis of Robustness:**\nRobustness to outliers is analyzed by examining the behavior of the influence function as the residual magnitude $|r(t)|$ becomes large.\n\n-   For the **Gaussian** case, $\\psi_{\\mathrm{ls}}(r(t))$ is a linear function of $r(t)$. It is **unbounded**: as $|r(t)| \\to \\infty$, $|\\psi_{\\mathrm{ls}}(r(t))| \\to \\infty$. This means a large residual (an outlier) exerts a proportionally large influence on the gradient, potentially corrupting the model update and destabilizing the inversion.\n\n-   For the **Student-$t$** case, the influence function is **bounded**. To see this, we examine its limit for large residuals:\n    $$\n    \\lim_{|r(t)| \\to \\infty} |\\psi_{\\mathrm{t}}(r(t))| = \\lim_{|r(t)| \\to \\infty} \\left|\\frac{(\\nu+1)r(t)}{\\nu\\sigma^2 + r(t)^2}\\right| = \\lim_{|r(t)| \\to \\infty} \\frac{(\\nu+1)|r(t)|}{r(t)^2} = \\lim_{|r(t)| \\to \\infty} \\frac{\\nu+1}{|r(t)|} = 0\n    $$\n    The influence function for the Student-$t$ distribution initially grows with $|r(t)|$ but then decays towards zero for very large residuals. This property makes the method robust: outliers are automatically down-weighted, and their influence on the gradient is suppressed. For small $\\nu$ (e.g., $\\nu=1$ for the Cauchy distribution), this suppression is very strong. As $\\nu \\to \\infty$, the Student-$t$ distribution approaches a Gaussian distribution, and its influence function $\\psi_{\\mathrm{t}}(r(t)) \\to r(t)/\\sigma^2$, which recovers the linear behavior of the least-squares case (up to a scaling factor).\n\n**3. Formulas for Implementation**\n\nThe program will implement the following computations for each test case:\n-   **Student-$t$ Adjoint Source:** $s_{\\mathrm{adj}}^{\\mathrm{t}}(t) = \\frac{(\\nu+1)r(t)}{\\nu\\sigma^2 + r(t)^2}$\n-   **Least-Squares Adjoint Source:** $s_{\\mathrm{adj}}^{\\mathrm{ls}}(t) = r(t)$\n-   **Outlier Suppression Factor (SF):** $\\mathrm{SF} = \\dfrac{\\left|s_{\\mathrm{adj}}^{\\mathrm{t}}(t_{\\mathrm{out}})\\right|}{\\left|s_{\\mathrm{adj}}^{\\mathrm{ls}}(t_{\\mathrm{out}})\\right|}$, where $t_{\\mathrm{out}}$ is the index of the maximum absolute residual. If the denominator is $0$, $\\mathrm{SF}=1.0$.\n-   **Global Norm Ratio (NR):** $\\mathrm{NR} = \\dfrac{\\left\\|s_{\\mathrm{adj}}^{\\mathrm{t}}\\right\\|_2}{\\left\\|s_{\\mathrm{adj}}^{\\mathrm{ls}}\\right\\|_2}$, where $\\|\\cdot\\|_2$ is the Euclidean ($L_2$) norm. If the denominator is $0$, $\\mathrm{NR}=1.0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required or permitted.\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating outlier suppression factors (SF) and\n    norm ratios (NR) for several test cases comparing Student-t and\n    Least-Squares adjoint sources in Full-Waveform Inversion.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (nu, sigma, r)\n        (3, 1.0, [0.1, -0.2, 0.3, 5.0, -0.1]),\n        (1, 1.0, [0.0, 0.0, 10.0, 0.0]),\n        (100, 1.0, [0.5, -0.5, 3.0]),\n        (3, 1.0, [0.0, 0.0]),\n    ]\n\n    results = []\n    for nu, sigma, r_list in test_cases:\n        # Convert residual list to a numpy array for vectorized operations.\n        r = np.array(r_list, dtype=np.float64)\n\n        # --- Calculate Adjoint Sources ---\n        # Least-squares adjoint source is simply the residual.\n        s_adj_ls = r\n\n        # Student-t adjoint source, using the derived formula.\n        numerator_t = (nu + 1) * r\n        denominator_t = nu * sigma**2 + r**2\n        # Handle division by zero for cases where r[i] might be zero,\n        # though the denominator is only zero if nu=0 and r=0, not an issue here.\n        s_adj_t = numerator_t / denominator_t\n        \n        # --- Calculate Outlier Suppression Factor (SF) ---\n        # Find the index of the largest absolute residual.\n        if r.size == 0 or np.all(r == 0):\n            # Handle case with no residuals or all-zero residuals\n            t_outlier_idx = 0\n            s_adj_ls_at_outlier = 0.0\n        else:\n            t_outlier_idx = np.argmax(np.abs(r))\n            s_adj_ls_at_outlier = s_adj_ls[t_outlier_idx]\n        \n        s_adj_t_at_outlier = s_adj_t[t_outlier_idx]\n\n        # Compute SF with the special convention for zero denominator.\n        if np.abs(s_adj_ls_at_outlier) == 0.0:\n            sf = 1.0\n        else:\n            sf = np.abs(s_adj_t_at_outlier) / np.abs(s_adj_ls_at_outlier)\n        \n        results.append(sf)\n\n        # --- Calculate Global Norm Ratio (NR) ---\n        # Compute the L2 norm (Euclidean norm) of both adjoint source time series.\n        norm_ls = np.linalg.norm(s_adj_ls)\n        norm_t = np.linalg.norm(s_adj_t)\n\n        # Compute NR with the special convention for zero denominator.\n        if norm_ls == 0.0:\n            nr = 1.0\n        else:\n            nr = norm_t / norm_ls\n\n        results.append(nr)\n\n    # Final print statement in the exact required format.\n    # The format [SF1,NR1,SF2,NR2,...] is achieved by flattening the results list.\n    print(f\"[{','.join(f'{x:.7f}' for x in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Implementing the adjoint-state method is a complex task where subtle coding errors can lead to incorrect gradients and failed inversions. Therefore, rigorous code verification is not just good practice—it is essential. This capstone exercise guides you through the process of building a complete synthetic benchmark to validate your 1D FWI implementation, including performing gradient and Hessian-vector tests. Successfully completing these validation steps provides the necessary confidence that your discrete adjoint gradient accurately reflects your objective function, paving the way for meaningful model updates .",
            "id": "3598829",
            "problem": "You are tasked with designing and validating a synthetic benchmark for Full-Waveform Inversion (FWI) using the adjoint method in one spatial dimension. The goal is to check the correctness of gradients and Hessian-vector products, and to demonstrate inversion convergence against known ground truth data. All quantities must be defined and implemented from first principles appropriate to computational geophysics, and all mathematical entities must be rendered using LaTeX notation.\n\nThe physical foundation is the one-dimensional acoustic wave equation for a scalar particle displacement field $u(x,t)$ in a medium with spatially varying wave speed $c(x)$, expressed via the squared slowness $m(x) = 1/c(x)^2$:\n$$\nm(x)\\,\\frac{\\partial^2 u(x,t)}{\\partial t^2} - \\frac{\\partial^2 u(x,t)}{\\partial x^2} = s(x,t),\n$$\nwhere $s(x,t)$ is a prescribed source term. The observation operator is sampling at a finite set of receiver positions $\\{x_r\\}$, providing data $u(x_r,t)$ with the objective function defined as the least-squares misfit between simulated and observed data:\n$$\nJ(m) = \\frac{1}{2}\\sum_{r}\\int_{0}^{T}\\left(u(x_r,t;m) - d_r(t)\\right)^2 \\,\\mathrm{d}t,\n$$\nwhere $d_r(t)$ are observed data generated from the ground truth model $m^\\star(x)$, and $T$ is the recording duration.\n\nThe adjoint-state method for this objective, using the same wave operator, introduces the adjoint field $\\lambda(x,t)$ satisfying the adjoint partial differential equation\n$$\nm(x)\\,\\frac{\\partial^2 \\lambda(x,t)}{\\partial t^2} - \\frac{\\partial^2 \\lambda(x,t)}{\\partial x^2} = \\sum_{r}\\delta(x-x_r)\\,\\left(u(x_r,t;m)-d_r(t)\\right),\n$$\nwith homogeneous terminal conditions at $t = T$ and $t = 0$. Under these conditions, the gradient of the objective with respect to $m(x)$ is\n$$\n\\frac{\\delta J}{\\delta m}(x) = -\\int_{0}^{T}\\frac{\\partial^2 u(x,t)}{\\partial t^2}\\,\\lambda(x,t)\\,\\mathrm{d}t.\n$$\n\nYour program must discretize the above equations on a uniform spatial grid with spacing $\\Delta x$ and time step $\\Delta t$ using a leapfrog scheme that is consistent with the Courant–Friedrichs–Lewy condition. The discrete Laplacian must be implemented by a second-order central difference operator. You must impose fixed-value boundary conditions $u(0,t)=0$ and $u(L,t)=0$ for scientific consistency. The source $s(x,t)$ must be a band-limited Ricker wavelet with central frequency $f_0$ located at a specified source position $x_s$. Receivers must be placed at specified positions $\\{x_r\\}$ away from boundaries. All lengths must be in meters and all times in seconds. The misfit $J(m)$ is dimensionless.\n\nYou must perform three validation tasks against synthetic data generated from a known ground truth model $m^\\star(x)$:\n\n1. Gradient test via directional derivative consistency. For a nontrivial perturbation direction $p(x)$, verify the identity\n$$\n\\frac{J(m+\\epsilon p)-J(m)}{\\epsilon} \\approx \\int_{0}^{L}\\frac{\\delta J}{\\delta m}(x)\\,p(x)\\,\\mathrm{d}x,\n$$\nfor a sequence of decreasing step sizes $\\epsilon$. Report the maximum relative discrepancy over the test sequence and whether the discrepancy decreases monotonically with $\\epsilon$.\n\n2. Hessian-vector test via second-order Taylor remainder. For a perturbation direction $v(x)$, define the finite-difference Hessian-vector scalar\n$$\nv^\\top H(m)v \\approx \\int_{0}^{L} v(x)\\,\\frac{\\frac{\\delta J}{\\delta m}(x;m+\\epsilon v)-\\frac{\\delta J}{\\delta m}(x;m-\\epsilon v)}{2\\epsilon}\\,\\mathrm{d}x,\n$$\nand test the second-order Taylor model\n$$\nJ(m+\\epsilon v) \\approx J(m) + \\epsilon\\,\\int_{0}^{L}\\frac{\\delta J}{\\delta m}(x)\\,v(x)\\,\\mathrm{d}x + \\frac{1}{2}\\epsilon^2\\,v^\\top H(m)v.\n$$\nCompute the relative error of this approximation at the smallest $\\epsilon$ used.\n\n3. Inversion convergence. Starting from an initial model $m_0(x)$, run a gradient-based iterative update with a backtracking line search satisfying the Armijo condition to reduce $J(m)$, for a fixed number of iterations. Use a simple smoothing preconditioner on the gradient to stabilize the update. Report the ratio $J(m_{\\text{final}})/J(m_0)$ after the iterations.\n\nYou must implement a self-contained test suite consisting of three cases, each fully specified by physically meaningful parameters. Answers must be unitless booleans indicating pass or fail based on acceptance thresholds provided here:\n\n- Gradient test acceptance: the maximum relative discrepancy at the smallest $\\epsilon$ must not exceed $0.10$, and the discrepancy must monotonically decrease over the tested $\\epsilon$ values.\n- Hessian test acceptance: the relative error of the second-order Taylor model at the smallest $\\epsilon$ must not exceed $0.25$.\n- Inversion convergence acceptance: the final misfit ratio must be less than or equal to a case-specific threshold.\n\nAll answers must be aggregated into a single line as a comma-separated list enclosed in square brackets. The result for each case must be a list of three booleans of the form $[\\text{gradient\\_pass},\\text{hessian\\_pass},\\text{convergence\\_pass}]$.\n\nTest Suite Specifications:\n\n- Case 1 (happy path): Domain length $L=1000\\,\\mathrm{m}$, number of grid points $N_x=201$, source position $x_s=200\\,\\mathrm{m}$, Ricker frequency $f_0=10\\,\\mathrm{Hz}$, recording duration $T=1.0\\,\\mathrm{s}$. Ground truth wave speed profile $c^\\star(x)$ is a two-layer model: $c^\\star(x)=2000\\,\\mathrm{m/s}$ for $x<500\\,\\mathrm{m}$ and $c^\\star(x)=2500\\,\\mathrm{m/s}$ otherwise. Initial model $c_0(x)=2200\\,\\mathrm{m/s}$ everywhere. Receivers at positions $x_r=\\{600,700,800\\}\\,\\mathrm{m}$. Convergence threshold: report pass if $J(m_{\\text{final}})/J(m_0)\\le 0.60$.\n- Case 2 (near-Courant boundary): Domain length $L=500\\,\\mathrm{m}$, number of grid points $N_x=101$, source position $x_s=100\\,\\mathrm{m}$, Ricker frequency $f_0=15\\,\\mathrm{Hz}$, recording duration $T=0.6\\,\\mathrm{s}$. Ground truth wave speed profile $c^\\star(x)$ is linear: $c^\\star(x)=2200+600x/L\\,\\mathrm{m/s}$. Initial model $c_0(x)=2300\\,\\mathrm{m/s}$ everywhere. Receivers at positions $x_r=\\{300,400\\}\\,\\mathrm{m}$. Choose $\\Delta t$ as $0.99$ times the stability limit for the maximum speed. Convergence threshold: report pass if $J(m_{\\text{final}})/J(m_0)\\le 0.70$.\n- Case 3 (low-contrast anomaly): Domain length $L=800\\,\\mathrm{m}$, number of grid points $N_x=161$, source position $x_s=150\\,\\mathrm{m}$, Ricker frequency $f_0=12\\,\\mathrm{Hz}$, recording duration $T=0.8\\,\\mathrm{s}$. Ground truth wave speed profile $c^\\star(x)=2500+100\\exp\\left(-\\frac{(x-500)^2}{2\\cdot 50^2}\\right)\\,\\mathrm{m/s}$. Initial model $c_0(x)=2500\\,\\mathrm{m/s}$ everywhere. Receivers at positions $x_r=\\{450,550,650\\}\\,\\mathrm{m}$. Convergence threshold: report pass if $J(m_{\\text{final}})/J(m_0)\\le 0.85$.\n\nAngle units are not involved in this problem. All reported booleans are unitless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example: \"[[true,true,true],[true,true,false],[true,true,true]]\" using Python boolean literals.",
            "solution": "The user has provided a well-defined and scientifically rigorous problem statement for designing and validating a 1D Full-Waveform Inversion (FWI) benchmark. The problem is valid according to all specified criteria.\n\n-   **Scientific Grounding**: The problem is built upon the standard 1D acoustic wave equation, a least-squares objective function, and the adjoint-state method for gradient computation. These are fundamental and correct principles in computational geophysics.\n-   **Well-Posedness**: The problem is fully specified, providing all necessary physical parameters, model definitions, and numerical settings for three distinct test cases. The validation tasks (gradient test, Hessian-vector test, and inversion convergence) are standard and objective procedures for verifying numerical optimization codes.\n-   **Objectivity and Completeness**: The problem is stated in precise, unambiguous language. All constraints, boundary conditions, source definitions, and acceptance thresholds are explicitly given, leaving no room for subjective interpretation. All required parameters for a full implementation are present.\n\nThe problem is free of contradictions, physically plausible, and computationally feasible. Therefore, I will proceed with a full solution.\n\nThe solution involves several key components:\n1.  **Wave Equation Solver**: A finite-difference time-domain (FDTD) solver for the 1D acoustic wave equation $m(x)\\,\\frac{\\partial^2 u}{\\partial t^2} - \\frac{\\partial^2 u}{\\partial x^2} = s(x,t)$. A second-order accurate central difference is used for the spatial derivative $\\frac{\\partial^2 u}{\\partial x^2}$, and a second-order leapfrog scheme is used for time-stepping. This updater is given by:\n    $$\n    u_i^{j+1} = 2u_i^j - u_i^{j-1} + \\frac{(\\Delta t)^2}{m_i} \\left( \\frac{u_{i+1}^j - 2u_i^j + u_{i-1}^j}{(\\Delta x)^2} + s_i^j \\right)\n    $$\n    where $i$ is the spatial index and $j$ is the time index. Fixed boundary conditions $u(0,t)=0$ and $u(L,t)=0$ are enforced.\n\n2.  **Adjoint Solver**: The adjoint wave equation, $m(x)\\,\\frac{\\partial^2 \\lambda}{\\partial t^2} - \\frac{\\partial^2 \\lambda}{\\partial x^2} = \\sum_{r}\\delta(x-x_r)\\,(u(x_r,t)-d_r(t))$, is solved backward in time. The numerical scheme is identical to the forward solver but marches from $t=T$ to $t=0$ with zero terminal conditions $\\lambda(x,T)=0$ and $\\frac{\\partial \\lambda}{\\partial t}(x,T)=0$. The source term is the data residual injected at receiver locations.\n\n3.  **Objective Function and Gradient**: The objective function $J(m)$ is the discrete sum-of-squares of the residuals:\n    $$\n    J(m) = \\frac{1}{2}\\sum_{r}\\sum_{j=0}^{N_t-1} \\left(u(x_r, t_j; m) - d_r(t_j)\\right)^2 \\Delta t\n    $$\n    The gradient is computed by cross-correlating the forward and adjoint wavefields, as per the formula provided:\n    $$\n    \\frac{\\delta J}{\\delta m}(x_i) = -\\sum_{j=0}^{N_t-1} \\left(\\frac{u_i^{j+1} - 2u_i^j + u_i^{j-1}}{(\\Delta t)^2}\\right)\\,\\lambda_i^j\\,\\Delta t\n    $$\n    This requires storing the full forward wavefield $u(x,t)$.\n\n4.  **Validation Tests**:\n    -   **Gradient Test**: This verifies the gradient computation by comparing the adjoint-based directional derivative with a finite-difference approximation. The Taylor expansion $J(m+\\epsilon p) = J(m) + \\epsilon \\langle \\nabla J, p \\rangle + \\mathcal{O}(\\epsilon^2)$ implies that $(\\frac{J(m+\\epsilon p)-J(m)}{\\epsilon}) / \\langle \\nabla J, p \\rangle \\to 1$ as $\\epsilon \\to 0$. We test for second-order convergence by checking for monotonic decrease of the relative discrepancy for a sequence of $\\epsilon$.\n    -   **Hessian-vector Test**: This verifies the consistency of the gradient by testing the second-order Taylor expansion of the objective function. The Hessian-vector product $H v$ is approximated via a central difference on the gradient: $\\frac{\\nabla J(m+\\epsilon v) - \\nabla J(m-\\epsilon v)}{2\\epsilon}$. The test then checks the relative error of the approximation: $J(m+\\epsilon v) \\approx J(m) + \\epsilon \\langle \\nabla J, v \\rangle + \\frac{1}{2} \\epsilon^2 \\langle v, H v \\rangle$.\n    -   **Inversion Test**: A simple gradient descent optimization is performed for a fixed number of iterations. The update direction is given by the preconditioned negative gradient. A backtracking line search is used to find a suitable step size $\\alpha_k$ that satisfies the Armijo condition, ensuring sufficient decrease in the objective function. The model is updated via $m_{k+1} = m_k - \\alpha_k P \\nabla J(m_k)$, where $P$ is a smoothing preconditioner (a Gaussian filter).\n\nThese components are encapsulated within a class structure to handle the parameters and methods for each test case cleanly. The final script executes the three specified test cases and formats the boolean pass/fail results as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter1d\n\n# Define global constants for inversion\nARMIJO_C1 = 1e-4\nARMIJO_TAU = 0.5\nINVERSION_ITERATIONS = 10\nGRADIENT_TEST_EPSILONS = [1e-4, 1e-5, 1e-6, 1e-7]\nHESSIAN_TEST_EPSILON = 1e-5\n\nclass FWI1DBenchmark:\n    \"\"\"\n    Encapsulates the 1D FWI problem setup and validation tests.\n    \"\"\"\n    def __init__(self, L, Nx, T, f0, xs_pos, xr_pos, c_true_func, c_init_func, cfl_factor=0.5):\n        # Spatial grid\n        self.L = L\n        self.Nx = Nx\n        self.x = np.linspace(0, L, Nx, dtype=np.float64)\n        self.dx = self.x[1] - self.x[0]\n\n        # Velocity and slowness models\n        self.c_true = c_true_func(self.x)\n        self.m_true = 1.0 / self.c_true**2\n        self.c_init = c_init_func(self.x)\n        self.m_init = 1.0 / self.c_init**2\n\n        # Time grid (CFL condition)\n        c_max = np.max([np.max(self.c_true), np.max(self.c_init)])\n        self.dt = cfl_factor * self.dx / c_max\n        self.T = T\n        self.Nt = int(T / self.dt)\n        self.t = np.arange(self.Nt) * self.dt\n\n        # Source wavelet\n        self.f0 = f0\n        self.src_idx = np.argmin(np.abs(self.x - xs_pos))\n        self.source_wavelet = self._create_ricker()\n\n        # Receiver setup\n        self.rec_indices = [np.argmin(np.abs(self.x - pos)) for pos in xr_pos]\n\n        # Generate \"observed\" data from the true model\n        self.d_obs, _ = self._wave_solver(self.m_true, store_full_field=False)\n\n    def _create_ricker(self):\n        t0 = 1.2 / self.f0  # Shift to ensure wavelet peak is not at t=0\n        arg = (np.pi * self.f0 * (self.t - t0))**2\n        return (1.0 - 2.0 * arg) * np.exp(-arg)\n\n    def _wave_solver(self, m, store_full_field=True):\n        u_prev = np.zeros(self.Nx, dtype=np.float64)\n        u_curr = np.zeros(self.Nx, dtype=np.float64)\n        m_inv_dt2 = (self.dt**2) / m\n\n        rec_data = np.zeros((len(self.rec_indices), self.Nt), dtype=np.float64)\n        full_field = np.zeros((self.Nx, self.Nt), dtype=np.float64) if store_full_field else None\n\n        for j in range(self.Nt):\n            laplacian = np.zeros(self.Nx, dtype=np.float64)\n            laplacian[1:-1] = (u_curr[2:] - 2*u_curr[1:-1] + u_curr[:-2]) / self.dx**2\n            \n            source_term = np.zeros(self.Nx, dtype=np.float64)\n            source_term[self.src_idx] = self.source_wavelet[j]\n\n            u_next = 2*u_curr - u_prev + m_inv_dt2 * (laplacian + source_term)\n            u_next[0] = 0.0\n            u_next[-1] = 0.0\n\n            u_prev, u_curr = u_curr, u_next\n\n            rec_data[:, j] = u_curr[self.rec_indices]\n            if store_full_field:\n                full_field[:, j] = u_curr\n        \n        return rec_data, full_field\n\n    def _adjoint_solver(self, m, residual):\n        lambda_prev = np.zeros(self.Nx, dtype=np.float64)\n        lambda_curr = np.zeros(self.Nx, dtype=np.float64)\n        m_inv_dt2 = (self.dt**2) / m\n\n        adj_src_grid = np.zeros((self.Nx, self.Nt), dtype=np.float64)\n        for i, rec_idx in enumerate(self.rec_indices):\n            adj_src_grid[rec_idx, :] = residual[i, :]\n\n        full_adjoint_field = np.zeros((self.Nx, self.Nt), dtype=np.float64)\n\n        for j in range(self.Nt - 1, -1, -1):\n            laplacian = np.zeros(self.Nx, dtype=np.float64)\n            laplacian[1:-1] = (lambda_curr[2:] - 2*lambda_curr[1:-1] + lambda_curr[:-2]) / self.dx**2\n            \n            lambda_next = 2*lambda_curr - lambda_prev + m_inv_dt2 * (laplacian + adj_src_grid[:, j])\n            lambda_next[0] = 0.0\n            lambda_next[-1] = 0.0\n\n            lambda_prev, lambda_curr = lambda_curr, lambda_next\n            full_adjoint_field[:, j] = lambda_curr\n            \n        return full_adjoint_field\n\n    def _compute_objective(self, m):\n        sim_data, _ = self._wave_solver(m, store_full_field=False)\n        residual = sim_data - self.d_obs\n        return 0.5 * np.sum(residual**2) * self.dt\n\n    def _compute_gradient(self, m):\n        sim_data, u_full = self._wave_solver(m, store_full_field=True)\n        residual = sim_data - self.d_obs\n        lambda_full = self._adjoint_solver(m, residual)\n\n        u_tt = np.zeros_like(u_full)\n        u_tt[:, 1:-1] = (u_full[:, 2:] - 2*u_full[:, 1:-1] + u_full[:, :-2]) / self.dt**2\n        \n        grad = -np.sum(u_tt * lambda_full, axis=1) * self.dt\n        return grad\n\n    def run_gradient_test(self):\n        m = self.m_init\n        p = np.random.randn(self.Nx)\n        p = gaussian_filter1d(p, sigma=2.0)\n        p[0] = p[-1] = 0.0\n        p /= np.linalg.norm(p)\n\n        grad_m = self._compute_gradient(m)\n        grad_dot_p = np.sum(grad_m * p) * self.dx\n\n        discrepancies = []\n        J_m = self._compute_objective(m)\n        \n        for eps in GRADIENT_TEST_EPSILONS:\n            J_m_eps_p = self._compute_objective(m + eps * p)\n            fd_dot_prod = (J_m_eps_p - J_m) / eps\n            if abs(grad_dot_p) > 1e-15:\n                discrepancy = np.abs(fd_dot_prod - grad_dot_p) / np.abs(grad_dot_p)\n                discrepancies.append(discrepancy)\n        \n        if len(discrepancies) < 2: return False\n\n        monotonic = all(discrepancies[i] > discrepancies[i+1] for i in range(len(discrepancies)-1))\n        pass_test = monotonic and (discrepancies[-1] < 0.10)\n        return pass_test\n\n    def run_hessian_test(self):\n        m = self.m_init\n        eps = HESSIAN_TEST_EPSILON\n        \n        v = np.random.randn(self.Nx)\n        v = gaussian_filter1d(v, sigma=2.0)\n        v[0] = v[-1] = 0.0\n        v /= np.linalg.norm(v)\n\n        J_m = self._compute_objective(m)\n        grad_m = self._compute_gradient(m)\n        T1 = eps * np.sum(grad_m * v) * self.dx\n\n        grad_plus = self._compute_gradient(m + eps * v)\n        grad_minus = self._compute_gradient(m - eps * v)\n        Hv = (grad_plus - grad_minus) / (2 * eps)\n        vHv = np.sum(v * Hv) * self.dx\n        T2 = 0.5 * eps**2 * vHv\n\n        J_approx = J_m + T1 + T2\n        J_true = self._compute_objective(m + eps * v)\n        \n        denominator = np.abs(J_true)\n        if denominator < 1e-15: return np.abs(J_true - J_approx) < 1e-15\n\n        relative_error = np.abs(J_true - J_approx) / denominator\n        return relative_error < 0.25\n\n    def run_inversion_test(self, conv_threshold):\n        m = self.m_init.copy()\n        J_initial = self._compute_objective(m)\n        if J_initial < 1e-15: return True\n\n        for _ in range(INVERSION_ITERATIONS):\n            grad = self._compute_gradient(m)\n            search_dir = -gaussian_filter1d(grad, sigma=1.0)\n            search_dir[0] = search_dir[-1] = 0.0\n\n            alpha = 1.0\n            J_current = self._compute_objective(m)\n            grad_dot_s = np.sum(grad * search_dir) * self.dx\n            \n            for _ in range(10): # Max backtracking steps\n                if self._compute_objective(m + alpha * search_dir) <= J_current + ARMIJO_C1 * alpha * grad_dot_s:\n                    break\n                alpha *= ARMIJO_TAU\n            else: # Line search failed\n                break\n            \n            m += alpha * search_dir\n        \n        J_final = self._compute_objective(m)\n        return (J_final / J_initial) <= conv_threshold\n\ndef solve():\n    np.random.seed(0)\n\n    test_specs = [\n        { # Case 1: Happy path\n            \"L\": 1000.0, \"Nx\": 201, \"T\": 1.0, \"f0\": 10.0, \"xs_pos\": 200.0,\n            \"xr_pos\": [600.0, 700.0, 800.0],\n            \"c_true_func\": lambda x: np.where(x < 500.0, 2000.0, 2500.0),\n            \"c_init_func\": lambda x: np.full_like(x, 2200.0),\n            \"cfl_factor\": 0.5, \"conv_thresh\": 0.60\n        },\n        { # Case 2: Near-Courant boundary\n            \"L\": 500.0, \"Nx\": 101, \"T\": 0.6, \"f0\": 15.0, \"xs_pos\": 100.0,\n            \"xr_pos\": [300.0, 400.0],\n            \"c_true_func\": lambda x: 2200.0 + 600.0 * x / 500.0,\n            \"c_init_func\": lambda x: np.full_like(x, 2300.0),\n            \"cfl_factor\": 0.99, \"conv_thresh\": 0.70\n        },\n        { # Case 3: Low-contrast anomaly\n            \"L\": 800.0, \"Nx\": 161, \"T\": 0.8, \"f0\": 12.0, \"xs_pos\": 150.0,\n            \"xr_pos\": [450.0, 550.0, 650.0],\n            \"c_true_func\": lambda x: 2500.0 + 100.0 * np.exp(-(x-500.0)**2 / (2 * 50.0**2)),\n            \"c_init_func\": lambda x: np.full_like(x, 2500.0),\n            \"cfl_factor\": 0.5, \"conv_thresh\": 0.85\n        }\n    ]\n\n    all_results = []\n    for spec in test_specs:\n        benchmark = FWI1DBenchmark(\n            L=spec[\"L\"], Nx=spec[\"Nx\"], T=spec[\"T\"], f0=spec[\"f0\"],\n            xs_pos=spec[\"xs_pos\"], xr_pos=spec[\"xr_pos\"],\n            c_true_func=spec[\"c_true_func\"], c_init_func=spec[\"c_init_func\"],\n            cfl_factor=spec[\"cfl_factor\"]\n        )\n        \n        grad_pass = benchmark.run_gradient_test()\n        hess_pass = benchmark.run_hessian_test()\n        conv_pass = benchmark.run_inversion_test(spec[\"conv_thresh\"])\n        \n        all_results.append([grad_pass, hess_pass, conv_pass])\n\n    output_str = \",\".join([f'[{\",\".join(map(lambda b: str(b).lower(), res))}]' for res in all_results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        }
    ]
}