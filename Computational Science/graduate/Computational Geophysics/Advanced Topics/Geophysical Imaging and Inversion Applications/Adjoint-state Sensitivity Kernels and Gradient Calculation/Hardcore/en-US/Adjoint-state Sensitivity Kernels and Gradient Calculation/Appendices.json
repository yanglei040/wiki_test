{
    "hands_on_practices": [
        {
            "introduction": "Before trusting a complex gradient calculation within an inversion framework, you must rigorously verify its correctness. This exercise guides you through the most fundamental validation procedure: the Taylor test. By comparing the adjoint-state gradient's directional derivative with a finite-difference approximation, you can confirm your implementation's accuracy and understand the critical interplay between truncation and round-off errors in numerical differentiation. ",
            "id": "3574132",
            "problem": "Consider an isotropic acoustic wave propagation model in a bounded domain $\\Omega \\subset \\mathbb{R}^3$ with absorbing boundaries. The temporally forced pressure field $u(\\mathbf{x}, t)$ satisfies a Partial Differential Equation (PDE) of the form\n$$\n\\partial_t^2 u(\\mathbf{x}, t) - \\nabla \\cdot \\left( c^2(\\mathbf{x}) \\nabla u(\\mathbf{x}, t) \\right) = s(\\mathbf{x}, t), \\quad \\mathbf{x} \\in \\Omega, \\; t \\in [0, T],\n$$\nwith appropriate initial and boundary conditions, where $c(\\mathbf{x})$ is the spatially varying wave speed and $s(\\mathbf{x}, t)$ is a known source. Let the model parameter be $m(\\mathbf{x}) = c^{-2}(\\mathbf{x})$ and denote by $R$ the sampling operator that extracts synthetic data at receiver locations, so that predicted data are $d_{\\text{pred}}(t) = R u(m; t)$. A standard least-squares misfit functional is\n$$\nJ(m) = \\frac{1}{2} \\sum_{r=1}^{N_r} \\int_0^T \\left( R u(m; t; \\mathbf{x}_r) - d_{\\text{obs}}(t; \\mathbf{x}_r) \\right)^2 \\, dt,\n$$\nwhere $d_{\\text{obs}}$ are observed data and $N_r$ is the number of receivers. The gradient $\\nabla J(m)$ with respect to $m$ is computed via the adjoint-state method and defines the directional derivative at $m$ in direction $p$ through the Euclidean inner product $\\langle \\nabla J(m), p \\rangle$.\n\nYou wish to verify the correctness of the computed gradient by a finite-difference directional-derivative check. Define the central finite-difference quantity\n$$\n\\Delta(\\epsilon, p) = \\frac{J(m + \\epsilon p) - J(m - \\epsilon p)}{2 \\epsilon},\n$$\nand compare it to the adjoint-state inner product $G(p) = \\langle \\nabla J(m), p \\rangle$. Under smoothness of $J$, the expected convergence of $\\Delta(\\epsilon, p)$ to $G(p)$ as $\\epsilon \\to 0$ is second order.\n\nWhich option correctly formulates the check and specifies choices of $\\epsilon$ and $p$ that will robustly exhibit the expected second-order convergence in a realistic computational geophysics setting?\n\nA. Use the central finite-difference $\\Delta(\\epsilon, p)$ as defined above and compare it to $G(p) = \\langle \\nabla J(m), p \\rangle$. Choose $p$ as a random vector with unit norm in parameter space, i.e., $\\|p\\|_2 = 1$, with entries scaled consistently with the units of $m$. Sweep $\\epsilon$ over a geometric sequence, e.g., $\\epsilon \\in \\{10^{-1}, 10^{-2}, \\dots, 10^{-8}\\}$, while holding the forward and adjoint PDE solver tolerances fixed and sufficiently tight so that the discretization error is negligible relative to the truncation error $\\mathcal{O}(\\epsilon^2)$ for the range of $\\epsilon$. Plot the relative error $|\\Delta(\\epsilon, p) - G(p)| / |G(p)|$ versus $\\epsilon$ and verify a slope of approximately $2$ (quadratic decay) until a plateau due to round-off appears. Optionally, select $\\epsilon$ near the balance point $\\epsilon \\approx \\sqrt{\\epsilon_{\\text{mach}}} \\, (1 + \\|m\\|_2) / \\|p\\|_2$ to ensure truncation error dominates round-off, where $\\epsilon_{\\text{mach}}$ is machine precision.\n\nB. Use the central finite-difference $\\Delta(\\epsilon, p)$, but to emphasize sensitivity along the steepest descent direction, set $p = \\nabla J(m)$ and pick $\\epsilon = 10^{-16}$ (double-precision machine epsilon), ensuring the perturbations are as small as possible. Rely on the smallest $\\epsilon$ to reveal the highest-order consistency, and accept any deviations as solver noise.\n\nC. Replace the central finite difference with a one-sided finite difference $[J(m + \\epsilon p) - J(m)]/\\epsilon$ to avoid doubling computational cost. Choose $\\epsilon$ moderately small, e.g., $\\epsilon = 10^{-6}$, and pick $p$ as a sparse perturbation localized around the receivers for enhanced sensitivity. Expect second-order convergence because $J$ is smooth.\n\nD. Use the central finite difference $\\Delta(\\epsilon, p)$ but choose $p$ unnormalized with large amplitude in regions where the wavefield has low energy, and fix $\\epsilon = 10^{-3}$ to be consistent across runs. If the error does not decrease with $\\epsilon$, increase the time-step size in the forward solver to compensate and rely on visual agreement between $\\Delta(\\epsilon, p)$ and $G(p)$ rather than quantitative error scaling.",
            "solution": "### Solution Explanation\n\nThe core of the problem is to verify the identity $\\langle \\nabla J(m), p \\rangle = \\lim_{\\epsilon \\to 0} \\frac{J(m + \\epsilon p) - J(m - \\epsilon p)}{2 \\epsilon}$. This involves understanding the behavior of the central finite-difference approximation in the presence of numerical errors.\n\n#### Taylor Series Analysis\nAssuming the functional $J(m)$ is sufficiently smooth, we can use a Taylor series expansion for $J(m + \\epsilon p)$:\n$$\nJ(m + \\epsilon p) = J(m) + \\epsilon \\langle \\nabla J(m), p \\rangle + \\frac{\\epsilon^2}{2} \\langle p, H(m) p \\rangle + \\mathcal{O}(\\epsilon^3)\n$$\nwhere $H(m)$ is the Hessian operator at $m$. Similarly, for the perturbation $-\\epsilon p$:\n$$\nJ(m - \\epsilon p) = J(m) - \\epsilon \\langle \\nabla J(m), p \\rangle + \\frac{\\epsilon^2}{2} \\langle p, H(m) p \\rangle - \\mathcal{O}(\\epsilon^3)\n$$\nSubstituting these into the central-difference formula $\\Delta(\\epsilon, p)$ shows that the odd-powered terms cancel, leaving:\n$$\n\\Delta(\\epsilon, p) = \\langle \\nabla J(m), p \\rangle + \\mathcal{O}(\\epsilon^2)\n$$\nThis confirms that the truncation error, $\\Delta(\\epsilon, p) - G(p)$, is of order $\\mathcal{O}(\\epsilon^2)$, implying second-order convergence.\n\n#### Sources of Numerical Error\nIn a practical setting, two other errors are critical:\n1.  **Numerical PDE Solver Error**: Evaluating $J(m)$ requires solving the wave equation numerically, introducing a discretization error. This error must be small enough not to interfere with the test. This is achieved by using tight solver tolerances (e.g., fine grids, small time steps).\n2.  **Round-off Error**: Computers use finite-precision arithmetic. When $\\epsilon$ is very small, the subtraction $J(m+\\epsilon p) - J(m-\\epsilon p)$ suffers from a catastrophic loss of significant digits. This round-off error scales as $\\mathcal{O}(\\epsilon_{\\text{mach}}/\\epsilon)$, where $\\epsilon_{\\text{mach}}$ is machine precision (approx. $10^{-16}$ for double precision).\n\nA robust verification must navigate the trade-off between truncation error (which dominates for large $\\epsilon$) and round-off error (which dominates for small $\\epsilon$). The best practice is to sweep $\\epsilon$ over several orders of magnitude and plot the total error versus $\\epsilon$ on a log-log scale. This plot should exhibit a characteristic V-shape: a line with slope 2 (truncation error regime) followed by a line with slope -1 (round-off error regime).\n\n#### Analysis of Options\n\n*   **Option A** describes this best practice perfectly. It correctly uses the second-order central difference, advocates for a generic random perturbation vector $p$, and proposes sweeping $\\epsilon$ to observe the convergence rate. It also correctly notes the need for tight solver tolerances and describes the expected V-shaped error curve. This is the gold standard for a gradient check.\n*   **Option B** is flawed because choosing $\\epsilon = 10^{-16}$ places the computation directly into the round-off dominated regime, producing numerical noise rather than a meaningful comparison. The idea that the smallest possible $\\epsilon$ is best is a common but serious misconception.\n*   **Option C** is incorrect because it uses a first-order one-sided difference but expects second-order convergence, which is a contradiction. A one-sided difference has a truncation error of $\\mathcal{O}(\\epsilon)$. Also, testing with a sparse, localized perturbation is not a robust check of the full gradient.\n*   **Option D** recommends several unsound practices. Using a single fixed $\\epsilon$ prevents verification of the convergence rate. Increasing the solver time-step size would increase, not compensate for, numerical errors. Relying on \"visual agreement\" instead of quantitative error analysis is not scientifically rigorous.\n\nTherefore, Option A is the only one that describes a mathematically correct and numerically robust procedure.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The sensitivity kernel, or gradient, is more than a mathematical abstraction; it is a map that reveals where the model is most sensitive to data misfit. This practice moves from verification to interpretation, exploring how the physics of the measurement process directly shapes the gradient. By deriving and comparing the adjoint sources for different receiver types—point samplers versus spatial arrays—you will build crucial intuition about how survey design influences the spatial resolution of an inversion result. ",
            "id": "3574116",
            "problem": "Consider a two-dimensional synthetic sedimentary basin represented by a domain $\\Omega \\subset \\mathbb{R}^2$ with absorbing boundaries. The acoustic pressure field $u(\\mathbf{x}, t)$ is governed by the constant-density acoustic wave equation\n$$\n\\partial_t^2 u(\\mathbf{x}, t) - m(\\mathbf{x}) \\nabla^2 u(\\mathbf{x}, t) = f(\\mathbf{x}, t),\n$$\nwhere $m(\\mathbf{x}) = c(\\mathbf{x})^2$ is the spatially varying squared wavespeed and $f(\\mathbf{x}, t) = s(t)\\,\\delta(\\mathbf{x} - \\mathbf{x}_s)$ is a point source at $\\mathbf{x}_s$ with source time function $s(t)$ of peak frequency $f_0$. The misfit functional is defined as\n$$\nJ(u) = \\frac{1}{2} \\int_0^T \\| B u(t) - d(t) \\|_{\\mathbf{W}}^2 \\, dt,\n$$\nwhere $B$ is a linear measurement operator mapping the wavefield to observed data at the receiver system, $d(t)$ are observed time series, $\\mathbf{W}$ is a symmetric positive-definite weighting, and $\\|\\mathbf{y}\\|_{\\mathbf{W}}^2 = \\mathbf{y}^\\top \\mathbf{W} \\mathbf{y}$. Two choices of $B$ are considered:\n\n$1.$ Point sampling at $N$ receiver locations $\\{\\mathbf{x}_r\\}_{r=1}^N$:\n$$\n(B_{\\mathrm{pt}} u)_r(t) = u(\\mathbf{x}_r, t), \\quad r = 1, \\dots, N.\n$$\n\n$2.$ Spatial averaging over receiver patches $\\{\\Gamma_r\\}_{r=1}^N$ with nonnegative, normalized weights $w_r(\\mathbf{x})$ supported on $\\Gamma_r$:\n$$\n(B_{\\mathrm{avg}} u)_r(t) = \\int_{\\Gamma_r} w_r(\\mathbf{x})\\, u(\\mathbf{x}, t)\\, d\\mathbf{x}, \\quad \\int_{\\Gamma_r} w_r(\\mathbf{x})\\, d\\mathbf{x} = 1.\n$$\n\nAssume $\\mathbf{W}$ is diagonal with entries $W_r > 0$, the time window $[0, T]$ is sufficiently long to capture the waveforms, and boundary terms vanish upon integration by parts. The basin has slower wavespeed $c_{\\mathrm{basin}}$ than the surrounding basement $c_{\\mathrm{basement}}$, with $c_{\\mathrm{basin}} = 2\\,\\mathrm{km/s}$, $c_{\\mathrm{basement}} = 4\\,\\mathrm{km/s}$, and $f_0 = 5\\,\\mathrm{Hz}$. Receiver patch widths satisfy $a \\approx 0.2\\,\\mathrm{km}$, while the characteristic wavelength inside the basin is $\\lambda_{\\mathrm{basin}} \\approx c_{\\mathrm{basin}}/f_0$.\n\nTasks:\n$1.$ Starting from the stated governing equation and the misfit definition, derive the adjoint-state equation and the gradient $\\nabla J(\\mathbf{x})$ with respect to $m(\\mathbf{x})$ for a generic linear operator $B$. Then, specialize your result to each measurement operator $B_{\\mathrm{pt}}$ and $B_{\\mathrm{avg}}$ by writing the corresponding adjoint sources in space-time.\n\n$2.$ Based on the derived forms, analyze how the measurement operator choice affects the spatial resolution of $\\nabla J(\\mathbf{x})$ inside the basin. Use the given numerical scales to argue the relative ability to resolve features with characteristic size smaller than or comparable to $a$ and $\\lambda_{\\mathrm{basin}}$.\n\nWhich of the following statements is most consistent with the derivation and resolution analysis?\n\nA. For point sampling, the adjoint source is a sum of spatial Dirac distributions at the receiver locations weighted by residuals, i.e., $r_{\\mathrm{pt}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r\\,[u(\\mathbf{x}_r, t) - d_r(t)]\\,\\delta(\\mathbf{x} - \\mathbf{x}_r)$. For spatial averaging over patches with weights $w_r(\\mathbf{x})$, the adjoint source is $r_{\\mathrm{avg}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r\\left[\\int_{\\Gamma_r} w_r(\\mathbf{y})\\,u(\\mathbf{y}, t)\\, d\\mathbf{y} - d_r(t)\\right] w_r(\\mathbf{x})$. In both cases, the gradient is $\\nabla J(\\mathbf{x}) = -\\int_0^T \\nabla u(\\mathbf{x}, t)\\cdot \\nabla \\lambda(\\mathbf{x}, t)\\, dt$. Spatial averaging broadens the adjoint field and therefore low-pass filters the gradient at approximately the averaging window scale $a$, so features smaller than $a$ are less resolvable compared to point sampling inside the basin with $\\lambda_{\\mathrm{basin}} \\approx 0.4\\,\\mathrm{km}$ and $a \\approx 0.2\\,\\mathrm{km}$.\n\nB. The measurement operator does not affect the adjoint source, which is always $r(\\mathbf{x}, t) = \\sum_{r=1}^N W_r[u(\\mathbf{x}, t) - d_r(t)]$ uniformly distributed over $\\Omega$, so the resolution of $\\nabla J(\\mathbf{x})$ is identical for point sampling and spatial averaging.\n\nC. Spatial averaging introduces an additional Laplacian into the adjoint source, $r_{\\mathrm{avg}}(\\mathbf{x}, t) = \\nabla^2\\left(\\sum_{r=1}^N W_r[(B_{\\mathrm{avg}} u)_r(t) - d_r(t)]\\right)$, which increases high-frequency content in $\\nabla J(\\mathbf{x})$ and enhances resolution within the basin.\n\nD. For point sampling, the gradient reduces to $\\nabla J(\\mathbf{x}) = -\\int_0^T u(\\mathbf{x}, t)\\,\\lambda(\\mathbf{x}, t)\\, dt$, whereas for spatial averaging it remains $\\nabla J(\\mathbf{x}) = -\\int_0^T \\nabla u(\\mathbf{x}, t)\\cdot \\nabla \\lambda(\\mathbf{x}, t)\\, dt$, implying that spatial averaging yields sharper localization of sensitivity.\n\nE. Both measurement operators produce adjoint sources that are nonzero only at $t = T$ due to terminal conditions, so any resolution differences depend solely on the source bandwidth and not on the spatial support of $B$.",
            "solution": "### Solution Explanation\n\n#### Task 1: Derivation of Adjoint Equation and Gradient\n\nTo find the gradient of the misfit functional $J$ with respect to the model parameter $m(\\mathbf{x})$, we use the adjoint-state method. The key is to define an adjoint field $\\lambda$ that enforces the stationarity of a Lagrangian functional. This process yields the **adjoint equation**, which is a wave equation run backward in time, driven by an **adjoint source**. The adjoint source is given by applying the adjoint of the measurement operator, $B^\\dagger$, to the weighted data residuals:\n$$\nr_\\lambda(\\mathbf{x}, t) = B^\\dagger (\\mathbf{W}(B u(t) - d(t)))\n$$\nThe gradient itself, for the given wave equation parameterized by $m=c^2$, is found to be the time-integrated correlation of the gradients of the forward and adjoint fields (assuming the physically more complete wave operator $\\partial_t^2 u - \\nabla \\cdot(m \\nabla u) = f$):\n$$\n\\nabla J(\\mathbf{x}) = -\\int_0^T \\nabla u(\\mathbf{x}, t) \\cdot \\nabla \\lambda(\\mathbf{x}, t) dt\n$$\nThis gradient expression is independent of the measurement operator $B$. However, the adjoint field $\\lambda$ (and thus the gradient) depends critically on $B$ through the adjoint source. We now find the specific adjoint sources for the two cases.\n\n1.  **Point Sampling ($B_{\\mathrm{pt}}$)**: The operator $B_{\\mathrm{pt}}$ samples the wavefield at points $\\mathbf{x}_r$. Its adjoint, $B_{\\mathrm{pt}}^\\dagger$, injects data back into the domain at those same points. This corresponds to using Dirac delta functions as the spatial basis. The resulting adjoint source is:\n    $$ r_{\\mathrm{pt}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r [u(\\mathbf{x}_r, t) - d_r(t)] \\delta(\\mathbf{x} - \\mathbf{x}_r) $$\n\n2.  **Spatial Averaging ($B_{\\mathrm{avg}}$)**: The operator $B_{\\mathrm{avg}}$ averages the wavefield over a patch $\\Gamma_r$ using a weight function $w_r(\\mathbf{x})$. Its adjoint, $B_{\\mathrm{avg}}^\\dagger$, injects a residual back into the domain with a spatial distribution given by that same weight function. The adjoint source is:\n    $$ r_{\\mathrm{avg}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r \\left[ (B_{\\mathrm{avg}}u)_r(t) - d_r(t) \\right] w_r(\\mathbf{x}) $$\n\n#### Task 2: Resolution Analysis\n\nThe spatial characteristics of the adjoint source directly impact the spatial resolution of the gradient.\n-   For **point sampling**, the adjoint source is a series of spatial delta functions. The resulting adjoint wavefield $\\lambda$ emanates from these infinitely sharp points, meaning its spatial frequency content is broad, limited only by the temporal frequencies in the residuals and by diffraction.\n-   For **spatial averaging**, the adjoint source is \"smeared out\" over the patch $\\Gamma_r$ according to the function $w_r(\\mathbf{x})$. This is equivalent to convolving a point-source response with the shape of the patch. In the spatial frequency (wavenumber) domain, this convolution acts as a low-pass filter, suppressing high wavenumbers. The characteristic scale of this filter is the patch width, $a$.\n\n**Numerical Evaluation**:\nThe characteristic wavelength in the basin is $\\lambda_{\\mathrm{basin}} \\approx c_{\\mathrm{basin}}/f_0 = (2\\,\\mathrm{km/s}) / (5\\,\\mathrm{Hz}) = 0.4\\,\\mathrm{km}$. The patch width is $a \\approx 0.2\\,\\mathrm{km}$. Since the spatial averaging smooths the adjoint source at the scale of $a$, it inherently limits the resolution of the final gradient. Features smaller than $a$ in the model will have their sensitivity signature blurred out. Point sampling does not have this intrinsic source-side smoothing, so it allows for potentially higher resolution, ultimately limited by the wavelength $\\lambda_{\\mathrm{basin}}$.\n\n#### Conclusion\n**Option A** correctly identifies the mathematical forms for both adjoint sources and the gradient. It also correctly concludes that the spatial averaging operator acts as a low-pass filter on the gradient, degrading resolution at the scale of the averaging window $a$, and correctly calculates the characteristic wavelength. All other options contain fundamental errors in either the derivation or the physical interpretation.\n- **B** incorrectly claims the adjoint source is independent of the measurement operator.\n- **C** incorrectly introduces a Laplacian operator into the source.\n- **D** incorrectly changes the form of the gradient kernel based on the measurement operator.\n- **E** incorrectly confuses the adjoint source with the terminal conditions of the adjoint field.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Modern geophysical inverse problems are computationally immense, often involving thousands of sources and terabytes of data, demanding execution on large-scale parallel computers. This final practice challenges you to design an efficient, end-to-end workflow that is both mathematically sound and computationally scalable. You must orchestrate source-level parallelism, asynchronous checkpointing, and efficient communication to minimize wall-clock time, bridging the gap between adjoint-state theory and its practical implementation on high-performance computing (HPC) systems. ",
            "id": "3574197",
            "problem": "Consider time-domain full waveform inversion governed by the variable-coefficient acoustic wave equation on a bounded domain with absorbing boundaries. For each source index $s \\in \\{1,\\dots,N_s\\}$, the forward field $u_s(\\mathbf{x},t)$ satisfies a linear partial differential equation of the form\n$$\n\\mathcal{L}(m)\\,u_s(\\mathbf{x},t)=f_s(\\mathbf{x},t), \\quad u_s(\\mathbf{x},0)=0, \\quad \\partial_t u_s(\\mathbf{x},0)=0,\n$$\nwhere $m(\\mathbf{x})$ is the model parameter (e.g., squared slowness), $\\mathcal{L}(m)$ is a differential operator that is linear in $u_s$ and depends smoothly on $m$, and $f_s$ is a known source term. Observed data are $d_s^{\\mathrm{obs}}(t)$ at $N_r$ receivers for each $s$. A standard least-squares misfit is\n$$\nJ(m)=\\frac{1}{2}\\sum_{s=1}^{N_s}\\sum_{r=1}^{N_r}\\int_{0}^{T}\\left(u_s(\\mathbf{x}_r,t;m)-d_{s,r}^{\\mathrm{obs}}(t)\\right)^2\\,\\mathrm{d}t,\n$$\nwith receiver locations $\\mathbf{x}_r$. Assume the adjoint-state method applies and that the gradient of $J$ can be expressed as a space-time bilinear interaction between a forward field and an adjoint field that is linear in the data residuals. The campaign must be executed on a distributed-memory High Performance Computing (HPC) system with the following specifications and constraints:\n\n- There are `P=64` identical compute nodes (processors) available throughout.\n- Each node has $128\\,\\mathrm{GB}$ of memory and a node-local solid state drive with sustained write and read bandwidth of $2\\,\\mathrm{GB/s}$.\n- A single forward or adjoint time step on a subdomain assigned to one node costs `t_step=0.012` of compute time. The total number of time steps is `T_steps=8000`.\n- To compute the gradient via the adjoint-state method without full recomputation of forward fields, checkpointing is used: storing a snapshot of the forward state every `k=100` time steps per source. Each stored checkpoint per node is $1\\,\\mathrm{GB}$.\n- Checkpoints are written to and read from node-local storage. Input/Output (I/O) can be issued synchronously (blocking) or asynchronously (nonblocking) and may overlap with compute if not bandwidth-bound.\n- The gradient is a spatial field distributed across nodes. Its local portion per node has size $2\\,\\mathrm{GB}$. A global sum across all nodes is required to assemble the full gradient. The interconnect supports a tree-based all-reduce with effective bandwidth $10\\,\\mathrm{GB/s}$ and latency negligible compared to bulk transfer.\n\nYou are asked to choose, among the options below, the workflow that both (i) produces the mathematically correct gradient for the given misfit and forward model via the adjoint-state method, and (ii) minimizes wall-clock time under the constraints by using parallelism over sources, batching, asynchronous checkpoint I/O, and efficient reduction of partial gradients. You may assume the following additional details for time estimates:\n\n- With subcommunicators of size `G` nodes each, $K=\\lfloor P/G \\rfloor$ sources can be simulated concurrently as a batch. Compute time scales inversely with `G` for fixed global grid size (ideal strong scaling within these limits).\n- Between two checkpoints, the compute time per node is `t_comp,int`=`k`*`t_step`=`1.2` of compute time.\n- Writing or reading a single checkpoint per node takes `t_io,cp=0.5` of compute time, which can be fully hidden if overlapped with compute of duration at least `t_io,cp`.\n\nOptions:\n\nA. Use all `P` nodes for each source sequentially. For each source, run the forward simulation, writing checkpoints synchronously to a parallel file system so that the simulation blocks for `t_io,cp` at each checkpoint. Then run the adjoint simulation with synchronous reads, blocking by `t_io,cp` at each checkpoint. After each source’s adjoint completes, perform a blocking all-reduce of the $2\\,\\mathrm{GB}$ local gradient across all `P` nodes to sum into the global gradient. Proceed source by source until $N_s$ sources are processed.\n\nB. Partition the `P` nodes into $K=P/G$ subcommunicators of size `G=8` nodes each to process $K=8$ sources concurrently as a batch. For each batch, on each subcommunicator and for each assigned source: run the forward simulation and issue asynchronous, nonblocking writes of checkpoints to node-local storage at every `k` steps, overlapping I/O with the `t_comp,int` compute between checkpoints. After the batch’s forward runs finish, for each source on its subcommunicator, run the adjoint simulation backward in time, issuing asynchronous prefetch reads of the next needed checkpoint while correlating the current window to accumulate the local gradient for that source. Sum the `K` per-source local gradients within each subcommunicator to a single subcommunicator-local partial gradient. At the end of the batch, launch a nonblocking tree-based all-reduce to sum the `K` subcommunicator-local partial gradients across all `P` nodes into the global gradient, while immediately starting the next batch’s forward runs to overlap the reduction with compute. Repeat for all $\\lceil N_s/K \\rceil$ batches.\n\nC. For all sources, run the forward simulation once using all `P` nodes, but instead of checkpointing, stream the full time history of forward fields for all time steps to disk for every source. After all forwards complete, form a single composite adjoint source by summing residuals over all sources and receivers, and run a single adjoint simulation. Correlate that single adjoint with a single forward field computed from an averaged “composite” source to produce the gradient in one pass. Perform a single all-reduce at the end.\n\nD. Partition the `P` nodes into subcommunicators of size `G=8` and process `K=8` sources concurrently as a batch. Use asynchronous I/O for checkpoints as in option B. During adjoint back-propagation, to reduce reduction costs, within each subcommunicator retain, at each grid point, the maximum (in absolute value) over the `K` sources’ local gradient contributions instead of their sum. At the end of each batch, perform a blocking all-reduce of these per-subcommunicator maxima to form the global gradient, then proceed to the next batch.\n\nWhich option satisfies both mathematical correctness and the stated performance objectives?\n\nChoices:\n- A. The sequential, synchronous-I/O workflow with per-source blocking reductions.\n- B. The batched, subcommunicator workflow with fully overlapped asynchronous checkpoint I/O and a single nonblocking all-reduce per batch overlapped with compute.\n- C. The full time-history write with a single composite adjoint and a single forward correlation.\n- D. The batched, asynchronous workflow that replaces per-source summation by per-point maxima before global reduction.",
            "solution": "### Solution Explanation\n\n#### Mathematical Correctness\nThe misfit functional is a sum over sources: $J(m) = \\sum_{s=1}^{N_s} J_s(m)$. Due to the linearity of differentiation, the total gradient must be the sum of the per-source gradients:\n$$\n\\nabla J(m) = \\sum_{s=1}^{N_s} \\nabla J_s(m)\n$$\nEach per-source gradient, $\\nabla J_s(m)$, is computed via the standard adjoint-state method (forward simulation for source $s$, adjoint simulation driven by the residual for source $s$, followed by correlation). Any workflow that does not compute this sum is mathematically incorrect.\n\n#### Performance Analysis\nThe goal is to minimize wall-clock time by maximizing parallelism and overlapping operations (computation, I/O, communication). The key opportunities for optimization are:\n1.  **Source-level Parallelism**: Since the gradient contributions from different sources are independent, simulations for multiple sources can be run concurrently on different subsets of compute nodes.\n2.  **Overlapping I/O and Computation**: The checkpointing process involves writing and reading large files. Using asynchronous (nonblocking) I/O allows the simulation to continue computing while data is being transferred to/from disk, effectively hiding the I/O cost if the compute time between I/O operations is long enough.\n3.  **Overlapping Communication and Computation**: The final step involves summing gradient contributions from all nodes. A nonblocking all-reduce operation allows the next batch of computations to begin while this communication happens in the background.\n\n#### Analysis of Options\n\n*   **A. The sequential, synchronous-I/O workflow:**\n    *   **Correctness**: **Yes**. It computes $\\sum_s \\nabla J_s(m)$ sequentially, which is mathematically correct.\n    *   **Performance**: **Poor**. It fails on all performance metrics. It processes sources one by one (no source parallelism). It uses synchronous I/O, so the CPU is idle during checkpoint writes/reads. It uses a blocking reduction, so all nodes wait for communication to finish before starting the next source.\n\n*   **B. The batched, subcommunicator workflow with asynchronous I/O:**\n    *   **Correctness**: **Yes**. It correctly parallelizes the computation of $\\nabla J_s(m)$ for batches of sources and sums the results, correctly computing the total gradient.\n    *   **Performance**: **Excellent**. This workflow embodies best practices for HPC. It uses source-level parallelism by batching. It uses asynchronous I/O to hide the checkpointing cost, as the compute time (`t_comp,int` = 1.2s) is greater than the I/O time (`t_io,cp` = 0.5s). It uses a nonblocking all-reduce to overlap the final communication with the start of the next computational batch. This design minimizes idle time and maximizes hardware utilization.\n\n*   **C. The full time-history write with a composite adjoint:**\n    *   **Correctness**: **No**. The gradient is a bilinear form $\\sum_s \\langle \\lambda_s, (\\partial_m \\mathcal{L}) u_s \\rangle$. This workflow proposes computing $\\langle \\sum_s \\lambda_s, (\\partial_m \\mathcal{L}) u_{\\text{composite}} \\rangle$, which is not mathematically equivalent. It incorrectly mixes forward and adjoint fields from different sources.\n    *   **Performance**: **Extremely poor**. Storing the entire time history of the wavefield is prohibitively expensive in terms of storage and I/O bandwidth, which is precisely the problem that checkpointing was invented to solve.\n\n*   **D. The batched workflow with max-reduction:**\n    *   **Correctness**: **No**. Instead of summing the per-source gradients, it takes the maximum of their absolute values. The resulting vector is not the gradient and does not represent a valid descent direction for the objective function $J(m)$. This invalidates the entire optimization algorithm.\n    *   **Performance**: The motivation to \"reduce reduction costs\" is baseless, as a max-reduction has the same communication cost as a sum-reduction.\n\n**Conclusion:**\nOption B is the only workflow that is both mathematically correct and designed to achieve optimal performance on the described HPC system by fully exploiting parallelism and overlapping operations.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}