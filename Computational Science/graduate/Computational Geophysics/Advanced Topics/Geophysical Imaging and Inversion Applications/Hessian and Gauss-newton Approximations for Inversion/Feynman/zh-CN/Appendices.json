{
    "hands_on_practices": [
        {
            "introduction": "本练习使用解析方法，旨在探讨在目标函数非凸的临界点附近，精确牛顿步和高斯-牛顿步的行为。通过执行泰勒展开，您将揭示高斯-牛顿近似在某些情况下更具鲁棒性的根本原因，并识别其自身的潜在失效模式 ()。",
            "id": "3603042",
            "problem": "在一个代表了计算地球物理学中局部振幅校准子问题的单参数非线性反问题中，考虑正演模型 $F(m) = \\sin(m)$，其中 $m$ 以弧度为单位。观测到单个数据 $d \\in \\mathbb{R}$，残差和最小二乘目标函数分别定义为 $r(m) = F(m) - d$ 和 $\\phi(m) = \\tfrac{1}{2} r(m)^{2}$。取 $d = \\tfrac{1}{2}$ 并记 $m^{\\star} = \\tfrac{\\pi}{2}$。注意 $F^{\\prime}(m^{\\star}) = 0$ 且 $F^{\\prime\\prime}(m^{\\star}) = -1$，因此 $\\phi(m)$ 在 $m^{\\star}$ 处有一个非凸驻点，该点的精确曲率为负。令 $m = m^{\\star} + \\varepsilon$ 且 $|\\varepsilon| \\ll 1$。\n\n仅从 $r(m)$ 和 $\\phi(m)$ 的定义出发，并使用 $F$ 及其导数在 $m^{\\star}$ 处的泰勒展开，完成以下任务：\n\n1. 推导梯度 $g(m) = \\nabla \\phi(m)$ 和精确Hessian矩阵 $H(m) = \\nabla^{2} \\phi(m)$ 的表达式，用 $F^{\\prime}(m)$、$F^{\\prime\\prime}(m)$ 和 $r(m)$ 表示。\n\n2. 使用 $\\phi$ 在 $m$ 处的二阶泰勒模型，最小化该模型以获得精确牛顿步长 $\\delta m_{\\mathrm{N}}$，作为 $\\varepsilon$ 的函数。\n\n3. 使用残差的一阶线性化 $r(m + \\delta m) \\approx r(m) + F^{\\prime}(m)\\,\\delta m$，最小化相应的二次模型以获得高斯-牛顿步长 $\\delta m_{\\mathrm{GN}}$，作为 $\\varepsilon$ 的函数。\n\n4. 计算 $\\delta m_{\\mathrm{N}}(\\varepsilon)$ 和 $\\delta m_{\\mathrm{GN}}(\\varepsilon)$ 在 $\\varepsilon = 0$ 附近的两项渐近展开式，对于 $\\delta m_{\\mathrm{N}}$ 保留至 $\\varepsilon^{3}$ 阶项（含），对于 $\\delta m_{\\mathrm{GN}}$ 保留至 $\\varepsilon^{-1}$ 阶和 $\\varepsilon^{1}$ 阶项（含）。\n\n将你的最终结果表示为单个 $1 \\times 2$ 的行向量 $\\big[\\delta m_{\\mathrm{N}}(\\varepsilon),\\, \\delta m_{\\mathrm{GN}}(\\varepsilon)\\big]$。无需单位。角度以弧度为单位。",
            "solution": "该问题经验证是自洽的、一致的且科学合理的。这是应用于地球物理反演的数值优化领域中的一个适定问题。\n\n最小二乘目标函数为 $\\phi(m) = \\frac{1}{2}r(m)^2$，其中残差为 $r(m) = F(m) - d$。正演模型是 $F(m) = \\sin(m)$，数据是 $d = \\frac{1}{2}$。展开点是 $m^{\\star} = \\frac{\\pi}{2}$，我们在 $m = m^{\\star} + \\varepsilon = \\frac{\\pi}{2} + \\varepsilon$ 处（对于 $|\\varepsilon| \\ll 1$）分析该问题。\n\n**1. 梯度和精确Hessian矩阵**\n\n首先，我们推导目标函数 $\\phi(m)$ 的梯度 $g(m)$ 和Hessian矩阵 $H(m)$ 的一般表达式。\n梯度 $g(m) = \\phi^{\\prime}(m)$ 可通过使用链式法则对 $\\phi(m)$ 关于 $m$ 求导得到：\n$$\ng(m) = \\frac{d}{dm} \\left( \\frac{1}{2} r(m)^2 \\right) = \\frac{1}{2} \\cdot 2 r(m) \\cdot r^{\\prime}(m) = r(m) r^{\\prime}(m)\n$$\n由于 $r(m) = F(m) - d$，其导数为 $r^{\\prime}(m) = F^{\\prime}(m)$。代入此式可得梯度：\n$$\ng(m) = r(m) F^{\\prime}(m)\n$$\nHessian矩阵 $H(m) = \\phi^{\\prime\\prime}(m)$ 是梯度 $g(m)$ 关于 $m$ 的导数。使用乘法法则：\n$$\nH(m) = \\frac{d}{dm} \\left( r(m) F^{\\prime}(m) \\right) = r^{\\prime}(m) F^{\\prime}(m) + r(m) F^{\\prime\\prime}(m)\n$$\n代入 $r^{\\prime}(m) = F^{\\prime}(m)$ 得到精确Hessian矩阵：\n$$\nH(m) = (F^{\\prime}(m))^2 + r(m) F^{\\prime\\prime}(m)\n$$\n该表达式由两项组成。第一项 $(F^{\\prime}(m))^2$ 是Hessian矩阵的高斯-牛顿近似。第二项 $r(m) F^{\\prime\\prime}(m)$ 在实践中常常被省略，特别是在残差 $r(m)$ 很小或模型接近线性（即 $F^{\\prime\\prime}(m)$ 很小）的情况下。\n\n**2. 精确牛顿步长 $\\delta m_{\\mathrm{N}}$**\n\n牛顿步长 $\\delta m_{\\mathrm{N}}$ 是通过最小化 $\\phi(m)$ 在当前点 $m$ 附近的二阶泰勒展开式得到的：\n$$\n\\phi(m + \\delta m) \\approx \\phi(m) + g(m) \\delta m + \\frac{1}{2} H(m) (\\delta m)^2\n$$\n为求驻点，将关于 $\\delta m$ 的导数设为零，可得：\n$$\ng(m) + H(m) \\delta m = 0 \\implies \\delta m_{\\mathrm{N}} = -H(m)^{-1} g(m) = -\\frac{g(m)}{H(m)}\n$$\n为了将其表示为 $\\varepsilon$ 的函数，我们计算在 $m = \\frac{\\pi}{2} + \\varepsilon$ 处的 $F(m)$、$F^{\\prime}(m)$、$F^{\\prime\\prime}(m)$ 和 $r(m)$ 的值：\n$F(m) = \\sin(\\frac{\\pi}{2} + \\varepsilon) = \\cos(\\varepsilon)$\n$F^{\\prime}(m) = \\cos(\\frac{\\pi}{2} + \\varepsilon) = -\\sin(\\varepsilon)$\n$F^{\\prime\\prime}(m) = -\\sin(\\frac{\\pi}{2} + \\varepsilon) = -\\cos(\\varepsilon)$\n$r(m) = F(m) - d = \\cos(\\varepsilon) - \\frac{1}{2}$\n\n现在，我们用 $\\varepsilon$ 来表示 $g(m)$ 和 $H(m)$：\n$g(m) = (\\cos(\\varepsilon) - \\frac{1}{2})(-\\sin(\\varepsilon))$\n$H(m) = (-\\sin(\\varepsilon))^2 + (\\cos(\\varepsilon) - \\frac{1}{2})(-\\cos(\\varepsilon)) = \\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)$\n\n将这些代入牛顿步长的公式中：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = -\\frac{(\\cos(\\varepsilon) - \\frac{1}{2})(-\\sin(\\varepsilon))}{\\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)} = \\frac{(\\cos(\\varepsilon) - \\frac{1}{2})\\sin(\\varepsilon)}{\\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)}\n$$\n\n**3. 高斯-牛顿步长 $\\delta m_{\\mathrm{GN}}$**\n\n高斯-牛顿步长是通过最小化由残差的一阶线性化 $r(m + \\delta m) \\approx r(m) + F^{\\prime}(m) \\delta m$ 构成的目标函数来推导的。相应的二次模型是 $\\phi_{\\mathrm{lin}}(\\delta m) = \\frac{1}{2}(r(m) + F^{\\prime}(m) \\delta m)^2$。对此关于 $\\delta m$ 进行最小化：\n$$\n\\frac{d\\phi_{\\mathrm{lin}}}{d(\\delta m)} = (r(m) + F^{\\prime}(m) \\delta m) F^{\\prime}(m) = 0\n$$\n这导致 $r(m)F'(m) + (F'(m))^2 \\delta m = 0$，这等价于求解 $g(m) + H_{\\mathrm{GN}}(m)\\delta m = 0$，其中 $H_{\\mathrm{GN}}(m) = (F^{\\prime}(m))^2$。因此，高斯-牛顿步长是：\n$$\n\\delta m_{\\mathrm{GN}} = -\\frac{g(m)}{H_{\\mathrm{GN}}(m)} = -\\frac{r(m) F^{\\prime}(m)}{(F^{\\prime}(m))^2} = -\\frac{r(m)}{F^{\\prime}(m)}\n$$\n代入以 $\\varepsilon$ 表示的表达式：\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = -\\frac{\\cos(\\varepsilon) - \\frac{1}{2}}{-\\sin(\\varepsilon)} = \\frac{\\cos(\\varepsilon) - \\frac{1}{2}}{\\sin(\\varepsilon)}\n$$\n\n**4. 渐近展开**\n\n我们使用以下在 $\\varepsilon = 0$ 附近的泰勒级数展开：\n$\\sin(\\varepsilon) = \\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)$\n$\\cos(\\varepsilon) = 1 - \\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^4}{24} + \\mathcal{O}(\\varepsilon^6)$\n\n**$\\delta m_{\\mathrm{N}}(\\varepsilon)$ 的展开：**\n分子为 $N(\\varepsilon) = (\\cos(\\varepsilon) - \\frac{1}{2})\\sin(\\varepsilon) = (\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4))(\\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)) = \\frac{1}{2}\\varepsilon - \\frac{\\varepsilon^3}{12} - \\frac{\\varepsilon^3}{2} + \\mathcal{O}(\\varepsilon^5) = \\frac{1}{2}\\varepsilon - \\frac{7}{12}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)$。\n分母为 $D(\\varepsilon) = \\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon) = (\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)) - (1 - \\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)) + \\frac{1}{2}(1 - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)) = -\\frac{1}{2} + \\frac{7}{4}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)$。\n进行级数除法：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = \\frac{\\frac{1}{2}\\varepsilon - \\frac{7}{12}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)}{-\\frac{1}{2} + \\frac{7}{4}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)} = \\left(-\\varepsilon + \\frac{7}{6}\\varepsilon^3\\right) \\frac{1}{1 - \\frac{7}{2}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)}\n$$\n使用几何级数展开 $(1-x)^{-1} = 1+x+x^2+\\dots$：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = \\left(-\\varepsilon + \\frac{7}{6}\\varepsilon^3\\right) \\left(1 + \\frac{7}{2}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)\\right) = -\\varepsilon - \\frac{7}{2}\\varepsilon^3 + \\frac{7}{6}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5) = -\\varepsilon - \\frac{14}{6}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)\n$$\n因此，牛顿步长的两项展开式为：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = -\\varepsilon - \\frac{7}{3}\\varepsilon^3\n$$\n\n**$\\delta m_{\\mathrm{GN}}(\\varepsilon)$ 的展开：**\n分子为 $N(\\varepsilon) = \\cos(\\varepsilon) - \\frac{1}{2} = \\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^4}{24} + \\mathcal{O}(\\varepsilon^6)$。\n分母为 $D(\\varepsilon) = \\sin(\\varepsilon) = \\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)$。\n进行级数除法：\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)}{\\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)} = \\frac{1}{\\varepsilon} \\frac{\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)}{1 - \\frac{\\varepsilon^2}{6} + \\mathcal{O}(\\varepsilon^4)}\n$$\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} - \\frac{\\varepsilon^2}{2}\\right)\\left(1 + \\frac{\\varepsilon^2}{6}\\right) + \\mathcal{O}(\\varepsilon^3) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} + \\frac{\\varepsilon^2}{12} - \\frac{\\varepsilon^2}{2}\\right) + \\mathcal{O}(\\varepsilon^3) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} - \\frac{5}{12}\\varepsilon^2\\right) + \\mathcal{O}(\\varepsilon^3)\n$$\n因此，高斯-牛顿步长的两项展开式为：\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{1}{2}\\varepsilon^{-1} - \\frac{5}{12}\\varepsilon\n$$\n\n牛顿步长通过大约 $-\\varepsilon$ 来校正当前位置 $m = m^\\star+\\varepsilon$，表明它试图返回到驻点 $m^\\star$。高阶项显示了曲率的影响。当 $\\varepsilon \\to 0$ 时，高斯-牛顿步长变为奇异的，这是当 $F^{\\prime}(m) \\to 0$ 时的典型行为。\n\n最终结果是包含这两个渐近展开式的行向量。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\varepsilon - \\frac{7}{3}\\varepsilon^{3}  \\frac{1}{2}\\varepsilon^{-1} - \\frac{5}{12}\\varepsilon \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Levenberg-Marquardt (LM) 算法是一种自适应地使用高斯-牛顿近似的经典方法。本实践将引导您完成一次完整的 LM 迭代，从计算步长到根据步长质量更新阻尼参数 ()。这个练习让您具体理解理论概念是如何在一个广泛使用的优化算法中付诸实践的。",
            "id": "3603085",
            "problem": "在单参数地球物理模型的非线性最小二乘反演中，考虑目标函数 $\\Phi(m) = \\frac{1}{2}\\| \\mathbf{r}(m) \\|_2^2$，其残差向量为 $\\mathbf{r}(m) = \\mathbf{d} - \\mathbf{f}(m)$，其中预测数据由非线性正演映射 $f_i(m) = \\exp(-a_i m)$ 给出（$i=1,2$）。设观测数据为 $\\mathbf{d} = \\begin{pmatrix} 0.55 \\\\ 0.40 \\end{pmatrix}$，系数为 $a_1 = 1$ 和 $a_2 = 2$，当前模型估计值为 $m_k = 0.5$。Levenberg-Marquardt (LM) 方法使用Hessian矩阵的Gauss-Newton (GN) 近似，即 $H \\approx J^\\top J$，其中雅可比矩阵 $J = \\frac{\\partial \\mathbf{r}}{\\partial m}$，并通过求解阻尼法方程来计算步长 $s$\n$$(J^\\top J + \\mu I)s = -J^\\top \\mathbf{r},$$\n当前阻尼参数为 $\\mu_k = 0.1$。\n\n执行一次LM迭代，步骤如下：\n- 从基本原理出发，计算残差向量 $\\mathbf{r}(m_k)$ 和雅可比矩阵 $J(m_k)$。\n- 通过求解 $(J^\\top J + \\mu I)s = -J^\\top \\mathbf{r}$ 来计算LM步长 $s$。\n- 定义Gauss-Newton二次模型 $\\varphi(s) = \\frac{1}{2}\\|\\mathbf{r}(m_k) + J(m_k)s\\|_2^2$ 并计算预测下降量 $\\mathrm{pr} = \\varphi(0) - \\varphi(s)$。\n- 使用正演模型评估更新后的残差 $\\mathbf{r}(m_k + s)$ 并计算实际下降量 $\\mathrm{ar} = \\Phi(m_k) - \\Phi(m_k + s)$。\n- 计算比率 $\\rho = \\mathrm{ar}/\\mathrm{pr}$ 并使用以下规则更新 $\\mu$：如果 $\\rho  0.75$，则设 $\\mu_{k+1} = \\mu_k/2$；如果 $\\rho  0.25$，则设 $\\mu_{k+1} = 2\\mu_k$；否则设 $\\mu_{k+1} = \\mu_k$。\n\n只报告更新后的阻尼参数 $\\mu_{k+1}$。将您的答案四舍五入到四位有效数字。无需单位。",
            "solution": "问题陈述构成了一个适定且科学上合理的数值优化练习，具体是Levenberg-Marquardt (LM) 算法在非线性最小二乘地球物理反演问题中的应用。所有必要的数据、函数和参数都已明确无误地给出，没有歧义或矛盾。因此，我们可以直接进行求解。\n\n任务是执行一次LM方法迭代，以确定更新后的阻尼参数 $\\mu_{k+1}$。给定的待最小化的目标函数是 $\\Phi(m) = \\frac{1}{2}\\|\\mathbf{r}(m)\\|_2^2$，其中残差向量为 $\\mathbf{r}(m) = \\mathbf{d} - \\mathbf{f}(m)$，参数 $m$ 是一个标量。\n\n提供的信息如下：\n- 观测数据向量：$\\mathbf{d} = \\begin{pmatrix} 0.55 \\\\ 0.40 \\end{pmatrix}$。\n- 非线性正演模型：$f_i(m) = \\exp(-a_i m)$，系数为 $a_1=1$ 和 $a_2=2$。这给出 $\\mathbf{f}(m) = \\begin{pmatrix} \\exp(-m) \\\\ \\exp(-2m) \\end{pmatrix}$。\n- 当前模型估计值：$m_k = 0.5$。\n- 当前阻尼参数：$\\mu_k = 0.1$。\n\n计算过程分为五个连续的步骤。\n\n**步骤1：计算残差向量 $\\mathbf{r}(m_k)$ 和雅可比矩阵 $J(m_k)$**\n\n首先，我们在当前估计值 $m_k=0.5$ 处计算正演模型 $\\mathbf{f}(m)$：\n$$\n\\mathbf{f}(m_k) = \\mathbf{f}(0.5) = \\begin{pmatrix} \\exp(-0.5) \\\\ \\exp(-2 \\times 0.5) \\end{pmatrix} = \\begin{pmatrix} \\exp(-0.5) \\\\ \\exp(-1) \\end{pmatrix}\n$$\n残差向量 $\\mathbf{r}(m_k)$ 是观测数据 $\\mathbf{d}$ 和预测数据 $\\mathbf{f}(m_k)$ 之间的差：\n$$\n\\mathbf{r}(m_k) = \\mathbf{d} - \\mathbf{f}(m_k) = \\begin{pmatrix} 0.55 - \\exp(-0.5) \\\\ 0.40 - \\exp(-1) \\end{pmatrix} \\approx \\begin{pmatrix} 0.55 - 0.60653066 \\\\ 0.40 - 0.36787944 \\end{pmatrix} = \\begin{pmatrix} -0.05653066 \\\\ 0.03212056 \\end{pmatrix}\n$$\n接下来，我们计算雅可比矩阵 $J(m)$。由于 $m$ 是一个标量参数，$J$ 是一个列向量，表示残差向量关于 $m$ 的导数：\n$$\nJ(m) = \\frac{\\partial \\mathbf{r}}{\\partial m} = \\frac{\\partial}{\\partial m} (\\mathbf{d} - \\mathbf{f}(m)) = -\\frac{\\partial \\mathbf{f}}{\\partial m}\n$$\n正演模型的导数是：\n$$\n\\frac{\\partial \\mathbf{f}}{\\partial m} = \\begin{pmatrix} \\frac{d}{dm} \\exp(-m) \\\\ \\frac{d}{dm} \\exp(-2m) \\end{pmatrix} = \\begin{pmatrix} -1 \\cdot \\exp(-m) \\\\ -2 \\cdot \\exp(-2m) \\end{pmatrix}\n$$\n因此，雅可比矩阵是：\n$$\nJ(m) = - \\begin{pmatrix} -\\exp(-m) \\\\ -2\\exp(-2m) \\end{pmatrix} = \\begin{pmatrix} \\exp(-m) \\\\ 2\\exp(-2m) \\end{pmatrix}\n$$\n我们在 $m_k = 0.5$ 处计算雅可比矩阵：\n$$\nJ(m_k) = J(0.5) = \\begin{pmatrix} \\exp(-0.5) \\\\ 2\\exp(-1) \\end{pmatrix} \\approx \\begin{pmatrix} 0.60653066 \\\\ 0.73575888 \\end{pmatrix}\n$$\n\n**步骤2：计算LM步长 $s$**\n\nLM步长 $s$ 通过求解阻尼法方程 $(J^\\top J + \\mu I)s = -J^\\top \\mathbf{r}$ 得到。由于 $m$ 是一个标量，$I$ 是标量 $1$，$s$ 也是一个标量。我们首先计算 $J^\\top J$ 和 $J^\\top \\mathbf{r}$ 这两项：\n$$\nJ^\\top J = \\begin{pmatrix} \\exp(-0.5)  2\\exp(-1) \\end{pmatrix} \\begin{pmatrix} \\exp(-0.5) \\\\ 2\\exp(-1) \\end{pmatrix} = (\\exp(-0.5))^2 + (2\\exp(-1))^2 = \\exp(-1) + 4\\exp(-2) \\approx 0.90922057\n$$\n$$\nJ^\\top \\mathbf{r} = \\begin{pmatrix} \\exp(-0.5)  2\\exp(-1) \\end{pmatrix} \\begin{pmatrix} 0.55 - \\exp(-0.5) \\\\ 0.40 - \\exp(-1) \\end{pmatrix}\n$$\n$$\nJ^\\top \\mathbf{r} = 0.55\\exp(-0.5) - \\exp(-1) + 0.8\\exp(-1) - 2\\exp(-2) = 0.55\\exp(-0.5) - 0.2\\exp(-1) - 2\\exp(-2) \\approx -0.01065459\n$$\n现在我们使用 $\\mu_k = 0.1$ 求解步长 $s$：\n$$\ns = \\frac{-J^\\top \\mathbf{r}}{J^\\top J + \\mu_k} \\approx \\frac{-(-0.01065459)}{0.90922057 + 0.1} = \\frac{0.01065459}{1.00922057} \\approx 0.01055724\n$$\n\n**步骤3：计算预测下降量 $\\mathrm{pr}$**\n\n预测下降量 $\\mathrm{pr} = \\varphi(0) - \\varphi(s)$ 是Gauss-Newton二次模型 $\\varphi(s) = \\frac{1}{2}\\|\\mathbf{r}(m_k) + J(m_k)s\\|_2^2$ 的下降量。这可以表示为 $\\mathrm{pr} = -s J^\\top \\mathbf{r} - \\frac{1}{2} s^2 J^\\top J$。一个从LM系统推导出的代数上等价且数值上更稳定的形式是 $\\mathrm{pr} = s^2 (\\frac{1}{2} J^\\top J + \\mu_k)$。我们使用后一种形式。\n$$\n\\mathrm{pr} \\approx (0.01055724)^2 \\left( \\frac{1}{2}(0.90922057) + 0.1 \\right)\n$$\n$$\n\\mathrm{pr} \\approx (0.0001114553) (0.454610285 + 0.1) = (0.0001114553)(0.554610285) \\approx 0.0000618128\n$$\n\n**步骤4：计算实际下降量 $\\mathrm{ar}$**\n\n实际下降量 $\\mathrm{ar} = \\Phi(m_k) - \\Phi(m_k+s)$ 是真实目标函数的下降量。首先，我们计算 $\\Phi(m_k)$：\n$$\n\\Phi(m_k) = \\frac{1}{2} \\|\\mathbf{r}(m_k)\\|_2^2 = \\frac{1}{2} ((-0.05653066)^2 + (0.03212056)^2) \\approx \\frac{1}{2}(0.004227445) \\approx 0.00211372\n$$\n接下来，我们计算 $\\Phi(m_k+s)$。更新后的模型是 $m_{k+1} = m_k + s \\approx 0.5 + 0.01055724 = 0.51055724$。在这个新点上的残差是 $\\mathbf{r}(m_{k+1}) = \\mathbf{d} - \\mathbf{f}(m_{k+1})$：\n$$\n\\mathbf{f}(m_{k+1}) = \\begin{pmatrix} \\exp(-0.51055724) \\\\ \\exp(-2 \\times 0.51055724) \\end{pmatrix} \\approx \\begin{pmatrix} 0.59995574 \\\\ 0.36018306 \\end{pmatrix}\n$$\n$$\n\\mathbf{r}(m_{k+1}) \\approx \\begin{pmatrix} 0.55 - 0.59995574 \\\\ 0.40 - 0.36018306 \\end{pmatrix} = \\begin{pmatrix} -0.04995574 \\\\ 0.03981694 \\end{pmatrix}\n$$\n在新点处的目标函数值为：\n$$\n\\Phi(m_k+s) = \\frac{1}{2} \\|\\mathbf{r}(m_{k+1})\\|_2^2 \\approx \\frac{1}{2} ((-0.04995574)^2 + (0.03981694)^2) \\approx \\frac{1}{2}(0.00408096) \\approx 0.00204048\n$$\n因此，实际下降量是：\n$$\n\\mathrm{ar} = \\Phi(m_k) - \\Phi(m_k+s) \\approx 0.00211372 - 0.00204048 = 0.00007324\n$$\n\n**步骤5：计算比率 $\\rho$ 并更新 $\\mu$**\n\n比率 $\\rho$ 比较了实际下降量和预测下降量，提供了对LM步长质量的一种度量。\n$$\n\\rho = \\frac{\\mathrm{ar}}{\\mathrm{pr}} \\approx \\frac{0.00007324}{0.0000618128} \\approx 1.18488\n$$\n问题指定了以下用于更新阻尼参数 $\\mu$ 的规则：\n- 如果 $\\rho  0.75$，则设 $\\mu_{k+1} = \\mu_k/2$。\n- 如果 $\\rho  0.25$，则设 $\\mu_{k+1} = 2\\mu_k$。\n- 否则，设 $\\mu_{k+1} = \\mu_k$。\n\n由于我们计算出的比率 $\\rho \\approx 1.18488$ 大于阈值 $0.75$，因此该模型步长被认为是“非常成功”的。算法通过减小阻尼参数来继续，以便在下一次迭代中采取更接近Gauss-Newton的步长。\n$$\n\\mu_{k+1} = \\frac{\\mu_k}{2} = \\frac{0.1}{2} = 0.05\n$$\n问题要求将最终答案四舍五入到四位有效数字。数值 $0.05$ 表示为四位有效数字是 $0.05000$。",
            "answer": "$$\\boxed{0.05000}$$"
        },
        {
            "introduction": "当精确 Hessian 矩阵具有负曲率时，优化过程可能变得极具挑战性。这个编程练习将演示信任域方法（一种现代且鲁棒的优化框架）在使用精确 Hessian 与始终为凸的高斯-牛顿近似时的行为差异 ()。您将通过实现算法并分析其性能，亲眼见证为何在强非线性场景下，高斯-牛顿近似通常是更受青睐的选择。",
            "id": "3603124",
            "problem": "要求您构建并分析一个在精确Hessian矩阵中表现出负曲率的单参数非线性最小二乘反演示例，并比较使用精确Hessian矩阵与高斯-牛顿近似计算得到的信任域步长。其基础是标准的非线性最小二乘目标函数和链式法则微积分。您必须实现一个程序，该程序针对一个指定的合成残差模型和一个小型测试集，计算两种Hessian矩阵选择下的信任域步长，并报告目标函数的预测减少量和实际减少量，以量化高斯-牛顿近似在何种情况下能避免在强非线性区域中产生有害步长。\n\n反演设置如下。考虑一个标量模型参数 $m \\in \\mathbb{R}$。定义一个双分量残差向量 $r(m) = [r_1(m), r_2(m)]^\\top$，其中\n- $r_1(m) = a \\exp(-b m^2) - d$，\n- $r_2(m) = c m - e$，\n\n其中 $a$、$b$、$c$、$d$ 和 $e$ 是固定的正常数。目标函数是非线性最小二乘拟合差\n$$\n\\phi(m) = \\tfrac{1}{2} \\|r(m)\\|_2^2 = \\tfrac{1}{2}\\big(r_1(m)^2 + r_2(m)^2\\big).\n$$\n梯度由链式法则定义为\n$$\ng(m) = \\nabla \\phi(m) = J(m)^\\top r(m),\n$$\n其中 $J(m)$ 是 $r(m)$ 的雅可比矩阵。精确Hessian矩阵为\n$$\nH(m) = \\nabla^2 \\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m),\n$$\n而高斯-牛顿（GN）近似使用\n$$\nH_{\\mathrm{GN}}(m) = J(m)^\\top J(m),\n$$\n也就是说，它忽略了二阶导数项 $\\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m)$。\n\n在迭代点 $m$ 处的信任域（TR）步长 $p$ 被定义为一维二次子问题的解\n$$\n\\min_{p \\in \\mathbb{R}} \\; q(p) = g(m)\\,p + \\tfrac{1}{2} H_\\star(m)\\,p^2 \\quad \\text{subject to} \\quad |p| \\le \\Delta,\n$$\n其中 $H_\\star(m)$ 表示精确Hessian矩阵 $H(m)$ 或高斯-牛顿Hessian矩阵 $H_{\\mathrm{GN}}(m)$，$\\Delta  0$ 是信任域半径。对于一维问题，该子问题有一个初等的闭式解：如果无约束极小化子 $p^\\mathrm{N} = -g/H_\\star$ 位于区间 $[-\\Delta,\\Delta]$ 内且 $H_\\star  0$，则 $p = p^\\mathrm{N}$；否则，$p$ 是集合 $\\{-\\Delta, +\\Delta\\}$ 中使 $q(p)$ 最小化的边界点。\n\n对于任何计算出的步长 $p$，定义：\n- 使用模型 $q(p)$ 的预测减少量为\n$$\n\\mathrm{pred} = -\\big(g(m)\\,p + \\tfrac{1}{2} H_\\star(m)\\,p^2\\big),\n$$\n- 使用真实目标函数的实际减少量为\n$$\n\\mathrm{ared} = \\phi(m) - \\phi(m + p),\n$$\n- 比率\n$$\n\\rho = \\frac{\\mathrm{ared}}{\\mathrm{pred}},\n$$\n约定如果分母为0，则将该比率视为0。\n\n您的程序必须仅使用上述定义中的标准微积分来实现以下内容：\n- 计算 $r_1(m)$ 和 $r_2(m)$，\n- 计算 $J(m)$、$g(m)$、$H(m)$ 和 $H_{\\mathrm{GN}}(m)$，\n- 对两种选择 $H_\\star \\in \\{H, H_{\\mathrm{GN}}\\}$ 求解一维信任域子问题，\n- 对两种选择计算 $\\mathrm{pred}$、$\\mathrm{ared}$ 和 $\\rho$。\n\n使用以下固定参数和测试集：\n- 常数：$a = 1.0$, $b = 5.0$, $c = 1.0$, $d = 0.1$, $e = 0.0$，\n- 测试用例是序偶 $(m, \\Delta)$：\n  - 用例1：$(m, \\Delta) = (0.1, 0.05)$，\n  - 用例2：$(m, \\Delta) = (0.1, 0.4)$，\n  - 用例3：$(m, \\Delta) = (0.1, 1.0)$，\n  - 用例4：$(m, \\Delta) = (0.3, 0.2)$。\n\n所有量纲均为无量纲。对于每个用例，您的程序必须按以下顺序输出一个包含六个实数的列表\n$$\n[p_H, \\; p_{\\mathrm{GN}}, \\; \\mathrm{ared}_H, \\; \\mathrm{ared}_{\\mathrm{GN}}, \\; \\rho_H, \\; \\rho_{\\mathrm{GN}}],\n$$\n其中 $p_H$ 和 $p_{\\mathrm{GN}}$ 分别是使用 $H$ 和 $H_{\\mathrm{GN}}$ 计算的信任域步长。最终输出必须是包含这些列表的单行列表，每个测试用例一个，顺序与上文相同。每个实数必须四舍五入到六位小数。例如，语法模板为\n$$\n\\big[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\big].\n$$",
            "solution": "用户提供了一个关于非线性最小二乘反演的计算地球物理学中的有效问题陈述。任务是分析一个单参数模型，比较使用精确Hessian矩阵与其高斯-牛顿近似计算的信任域步长。此分析需要推导必要的数学量，实现信任域步长计算，并通过目标函数的预测减少量和实际减少量来评估步长的质量。\n\n问题的核心在于标量参数 $m \\in \\mathbb{R}$ 的非线性最小二乘目标函数 $\\phi(m)$：\n$$\n\\phi(m) = \\frac{1}{2} \\|r(m)\\|_2^2 = \\frac{1}{2}\\left(r_1(m)^2 + r_2(m)^2\\right)\n$$\n其中残差分量由以下公式给出：\n$$\n\\begin{aligned}\nr_1(m) = a \\exp(-b m^2) - d \\\\\nr_2(m) = c m - e\n\\end{aligned}\n$$\n其中 $a$、$b$、$c$、$d$ 和 $e$ 是固定的正常数。\n\n要实现信任域方法，我们必须首先计算目标函数 $\\phi(m)$ 的梯度 $g(m)$ 和Hessian矩阵 $H(m)$。梯度由链式法则给出：\n$$\ng(m) = \\nabla \\phi(m) = J(m)^\\top r(m)\n$$\n其中 $J(m)$ 是残差向量 $r(m)$ 的雅可比矩阵。对于标量参数 $m$，雅可比矩阵是一个 $2 \\times 1$ 的矩阵，其元素是残差分量关于 $m$ 的导数：\n$$\nJ(m) = \\begin{pmatrix} \\frac{\\partial r_1}{\\partial m} \\\\ \\frac{\\partial r_2}{\\partial m} \\end{pmatrix} = \\begin{pmatrix} -2abm \\exp(-b m^2) \\\\ c \\end{pmatrix}\n$$\n我们将雅可比矩阵的分量表示为 $J_1(m) = -2abm \\exp(-b m^2)$ 和 $J_2(m) = c$。梯度则是一个标量：\n$$\ng(m) = J_1(m) r_1(m) + J_2(m) r_2(m)\n$$\n\n精确Hessian矩阵 $H(m)$ 由以下公式给出：\n$$\nH(m) = \\nabla^2 \\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m)\n$$\n第一项 $J(m)^\\top J(m)$ 是对Hessian矩阵的高斯-牛顿（GN）近似：\n$$\nH_{\\mathrm{GN}}(m) = J_1(m)^2 + J_2(m)^2 = \\left(-2abm \\exp(-b m^2)\\right)^2 + c^2 = 4a^2b^2m^2 \\exp(-2bm^2) + c^2\n$$\n请注意，$H_{\\mathrm{GN}}(m)$ 总是非负的，如果 $c \\neq 0$，则严格为正。\n\n精确Hessian矩阵中的第二项需要残差的二阶导数：\n$$\n\\frac{d^2 r_1}{dm^2} = \\frac{d}{dm}\\left(-2abm \\exp(-b m^2)\\right) = -2ab \\exp(-b m^2) (1 - 2bm^2)\n$$\n$$\n\\frac{d^2 r_2}{dm^2} = \\frac{d}{dm}(c) = 0\n$$\n因此，精确Hessian矩阵为：\n$$\nH(m) = H_{\\mathrm{GN}}(m) + r_1(m) \\frac{d^2 r_1}{dm^2}\n$$\n第二项 $r_1(m) \\frac{d^2 r_1}{dm^2}$ 可以是负数。如果这一项足够大且为负，整个Hessian矩阵 $H(m)$ 可能会变为负值，这种情况被称为负曲率。这表明目标函数是局部凹的，而作为正定矩阵的高斯-牛顿近似提供了一个较差的局部模型。\n\n信任域（TR）步长 $p$ 是一维二次子问题的解：\n$$\n\\min_{p \\in \\mathbb{R}} \\; q(p) = g(m)\\,p + \\frac{1}{2} H_\\star(m)\\,p^2 \\quad \\text{subject to} \\quad |p| \\le \\Delta\n$$\n其中 $H_\\star$ 是 $H(m)$ 或 $H_{\\mathrm{GN}}(m)$，且 $\\Delta  0$ 是信任域半径。此子问题的求解策略取决于 $H_\\star$ 的符号。\n1.  如果 $H_\\star  0$，二次模型 $q(p)$ 是凸的。无约束极小化子是 $p^\\mathrm{N} = -g(m)/H_\\star$。如果 $|p^\\mathrm{N}| \\le \\Delta$，则最优步长为 $p=p^\\mathrm{N}$。如果 $|p^\\mathrm{N}|  \\Delta$，则步长在信任域的边界上，$p = \\Delta \\cdot \\mathrm{sign}(p^\\mathrm{N}) = \\mathrm{copysign}(\\Delta, p^\\mathrm{N})$。\n2.  如果 $H_\\star \\le 0$，二次模型是线性或凹的。在区间 $[-\\Delta, \\Delta]$ 上的最小值必定出现在边界上。为了最小化 $q(p)$，我们选择使主要线性项 $g(m)p$ 最小（最负）的边界点。因此，步长为 $p = -\\Delta \\cdot \\mathrm{sign}(g(m)) = \\mathrm{copysign}(\\Delta, -g(m))$。\n\n一个用于寻找步长 $p$ 的统一算法是：\n- 如果 $H_\\star  0$，计算无约束步长 $p^\\mathrm{N} = -g/H_\\star$。如果 $|p^\\mathrm{N}| \\le \\Delta$，则 $p = p^\\mathrm{N}$。\n- 否则（如果 $H_\\star \\le 0$ 或 $|p^\\mathrm{N}|  \\Delta$），步长位于边界上：$p = \\mathrm{copysign}(\\Delta, -g)$。\n\n一旦计算出步长 $p$，我们使用两个指标来评估其质量：\n- 基于二次模型的预测减少量：$\\mathrm{pred} = -\\left(g(m)\\,p + \\frac{1}{2} H_\\star(m)\\,p^2\\right)$。\n- 基于真实目标函数的实际减少量：$\\mathrm{ared} = \\phi(m) - \\phi(m + p)$。\n\n比率 $\\rho = \\mathrm{ared}/\\mathrm{pred}$ 衡量了模型与实际函数之间的一致性。$\\rho$ 值接近1表示这是一个极好的局部近似。\n\n程序将对每个测试用例 $(m, \\Delta)$ 执行以下步骤：\n1.  使用推导出的公式和提供的常数，计算 $g(m)$、$H(m)$ 和 $H_{\\mathrm{GN}}(m)$ 的数值。\n2.  使用 $H_\\star = H(m)$ 求解信任域子问题，得到 $p_H$。\n3.  使用 $H_\\star = H_{\\mathrm{GN}}(m)$ 求解信任域子问题，得到 $p_{\\mathrm{GN}}$。\n4.  对每个步长（$p_H$ 和 $p_{\\mathrm{GN}}$），计算相应的 $\\mathrm{ared}$ 和 $\\rho$ 值。\n5.  收集六个结果值：$p_H, p_{\\mathrm{GN}}, \\mathrm{ared}_H, \\mathrm{ared}_{\\mathrm{GN}}, \\rho_H, \\rho_{\\mathrm{GN}}$。\n6.  按要求格式化结果。",
            "answer": "$$\n\\boxed{\\begin{bmatrix} 0.050000  0.050000  0.015948  0.015948  0.999784  0.999784 \\\\ -0.400000  0.288722  -0.638520  0.134599  0.174249  1.026402 \\\\ -1.000000  0.288722  -1.972304  0.134599  0.088894  1.026402 \\\\ -0.200000  -0.165039  0.053153  0.059286  1.050020  1.000213 \\end{bmatrix}}\n$$"
        }
    ]
}