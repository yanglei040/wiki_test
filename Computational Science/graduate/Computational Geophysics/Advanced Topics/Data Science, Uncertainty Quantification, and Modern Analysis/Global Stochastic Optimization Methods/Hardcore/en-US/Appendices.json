{
    "hands_on_practices": [
        {
            "introduction": "Geophysical inverse problems often present complex, multimodal misfit landscapes where numerous local minima can trap conventional optimization algorithms. This exercise provides a foundational, hands-on demonstration of this challenge, known as basin entrapment. By implementing and comparing a simple stochastic global search against a classic gradient-based local search on a toy one-dimensional nonconvex objective function, you will gain a concrete understanding of why global exploration is crucial for finding reliable solutions. ",
            "id": "3600613",
            "problem": "You are to implement and compare two global stochastic optimization strategies on a one-dimensional, nonconvex objective function commonly used in computational geophysics as a toy model for basin entrapment. Consider the scalar objective defined on the bounded domain $\\mathcal{X}=[-4,4]$ by $f(x)=\\sin(5x)+0.1\\,x^2$, where all angles are in radians and $x$ is dimensionless. Your goal is to approximate a global minimizer and to illustrate the difference between a stochastic global search and a local, single-start gradient descent method.\n\nBegin from the following fundamental bases:\n- The global minimization problem is to compute $$\\min_{x\\in\\mathcal{X}} f(x).$$\n- Independent and identically distributed uniform sampling on an interval uses a pseudorandom generator to draw $x_i\\sim \\mathrm{Uniform}([-4,4])$ and selects the best sample according to the objective.\n- In one dimension, gradient descent with a constant step size uses the update $$x_{k+1}=\\Pi_{\\mathcal{X}}\\bigl(x_k-\\alpha\\,f'(x_k)\\bigr),$$ where $\\Pi_{\\mathcal{X}}$ denotes the Euclidean projection onto the interval $\\mathcal{X}$ (that is, clamping to $[-4,4]$), $\\alpha>0$ is a fixed step size, and $f'(x)$ is the first derivative computed using the chain rule and linearity of differentiation.\n\nTasks to implement:\n1) Use the definition of the derivative and the chain rule to derive and implement $f'(x)$ for the given $f(x)$.\n2) Implement a stochastic global search that draws $n$ independent uniform samples on $[-4,4]$ using a pseudorandom number generator with a specified integer seed $s$, evaluates $f(x)$ on each sample, and returns the sample location with minimal objective value together with that value.\n3) Implement single-start projected gradient descent with a given initial point $x_0\\in[-4,4]$, constant step size $\\alpha>0$, and a fixed number of iterations $K\\in\\mathbb{N}$. After each update, project the iterate back to $[-4,4]$ via $\\Pi_{\\mathcal{X}}(x)=\\min(\\max(x,-4),4)$.\n\nComparison metric and decision rule:\n- For each test case, compute the approximate best value from random sampling, $f_{\\mathrm{rand}}:=\\min_{i=1,\\dots,n} f(x_i),$ at its corresponding point $x_{\\mathrm{rand}}$, and the final value after gradient descent, $f_{\\mathrm{gd}}:=f(x_K),$ at its final point $x_{\\mathrm{gd}}$.\n- Define $\\Delta:=f_{\\mathrm{gd}}-f_{\\mathrm{rand}}.$ Declare that gradient descent is trapped in a suboptimal basin if $\\Delta>\\tau$, where $\\tau=10^{-3}$.\n\nNumerical and output requirements:\n- Use radians for all trigonometric evaluations. There are no physical units.\n- For each test case, return a list $[x_{\\mathrm{rand}}, f_{\\mathrm{rand}}, x_{\\mathrm{gd}}, f_{\\mathrm{gd}}, \\text{trapped}]$, where the first four entries are floats rounded to six decimal places, and the last entry is a boolean computed using the threshold $\\tau=10^{-3}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the lists from all test cases in order, for example $[r_1,r_2,\\dots,r_m]$.\n\nTest suite to implement and evaluate:\n- Case A (baseline success near the best basin): $s=123$, $n=500$, $x_0=0.0$, $\\alpha=0.05$, $K=200$.\n- Case B (entrapment in a distant local basin): $s=7$, $n=2000$, $x_0=2.8$, $\\alpha=0.05$, $K=200$.\n- Case C (undersampling stress test): $s=42$, $n=20$, $x_0=-3.5$, $\\alpha=0.02$, $K=150$.\n- Case D (zero-iteration gradient descent boundary): $s=0$, $n=3000$, $x_0=1.0$, $\\alpha=0.05$, $K=0$.\n\nYour program must implement the above logic exactly and print a single line of the aggregated results in the format $[[x_{\\mathrm{rand}}^{(A)},f_{\\mathrm{rand}}^{(A)},x_{\\mathrm{gd}}^{(A)},f_{\\mathrm{gd}}^{(A)},\\mathrm{trapped}^{(A)}],\\dots,[x_{\\mathrm{rand}}^{(D)},f_{\\mathrm{rand}}^{(D)},x_{\\mathrm{gd}}^{(D)},f_{\\mathrm{gd}}^{(D)},\\mathrm{trapped}^{(D)}]].$",
            "solution": "The problem is valid as it presents a well-defined, self-contained, and scientifically sound computational task in the field of numerical optimization. It requires the implementation and comparison of two standard optimization algorithms, stochastic global search and gradient descent, on a specified nonconvex function. All parameters, conditions, and evaluation metrics are provided, ensuring the problem is objective and allows for a unique, verifiable solution.\n\nThe core of the problem is to compare a global optimization strategy with a local one on a multimodal objective function. The chosen function, $f(x)=\\sin(5x)+0.1\\,x^2$, defined on the compact domain $\\mathcal{X}=[-4,4]$, is nonconvex. This means it possesses multiple local minima, making it a suitable test case to illustrate the phenomenon of a local search method getting \"trapped\" in a suboptimal basin of attraction, while a global search has a higher probability of identifying the global minimizer.\n\nThe methodological steps are as follows: first, we derive the analytical form of the function's derivative, which is required for the gradient descent algorithm. Second, we formalize the procedures for both the stochastic search and the projected gradient descent. Finally, we apply these algorithms to the specified test cases and use the defined comparison metric to evaluate their performance.\n\n1.  **Objective Function and its Derivative**\nThe objective function is given as:\n$$f(x)=\\sin(5x)+0.1\\,x^2$$\nTo implement gradient descent, we must compute its first derivative, $f'(x)$. We apply the principle of linearity of differentiation, which states that the derivative of a sum is the sum of the derivatives, and the chain rule for the trigonometric term.\nThe derivative of the first term, $\\sin(5x)$, with respect to $x$ is found using the chain rule, $d/dx(\\sin(u)) = \\cos(u) \\cdot du/dx$. Here, $u=5x$, so $du/dx=5$.\n$$\\frac{d}{dx}\\left(\\sin(5x)\\right) = \\cos(5x) \\cdot 5 = 5\\cos(5x)$$\nThe derivative of the second term, $0.1x^2$, is found using the power rule:\n$$\\frac{d}{dx}\\left(0.1x^2\\right) = 0.1 \\cdot 2x = 0.2x$$\nCombining these results gives the full derivative:\n$$f'(x) = 5\\cos(5x) + 0.2x$$\nAll trigonometric calculations are performed in radians, as specified.\n\n2.  **Stochastic Global Search: Uniform Random Sampling**\nThis method performs a global exploration of the search space $\\mathcal{X}=[-4,4]$. Its principle is based on probabilistic coverage. By drawing a sufficiently large number, $n$, of independent and identically distributed samples $x_i \\sim \\mathrm{Uniform}([-4,4])$, the algorithm is likely to place at least one sample within the basin of attraction of the global minimum. This method is \"blind\" to the local structure of the function (like its gradient) and relies purely on function evaluations at random points. The algorithm is:\n-   Initialize a pseudorandom number generator with a specified integer seed, $s$, to ensure reproducibility.\n-   Generate $n$ random samples $\\{x_1, x_2, \\dots, x_n\\}$ uniformly from the interval $[-4,4]$.\n-   Evaluate $f(x_i)$ for each sample $x_i$.\n-   Determine the sample $x_{\\mathrm{rand}}$ that yields the minimum function value, $f_{\\mathrm{rand}} = \\min_{i=1,\\dots,n} f(x_i)$.\n\n3.  **Local Search: Projected Gradient Descent**\nThis is a first-order iterative optimization algorithm designed to find a local minimum. It starts at an initial point $x_0$ and iteratively moves in the direction of the negative gradient, which is the direction of steepest local descent.\nThe iterative update rule is:\n$$x_{k+1} = x_k - \\alpha f'(x_k)$$\nwhere $k$ is the iteration number, $x_k$ is the current point, $f'(x_k)$ is the gradient at that point, and $\\alpha>0$ is a constant step size that controls the magnitude of each step.\nSince the problem is constrained to the domain $\\mathcal{X}=[-4,4]$, we must ensure that each new iterate $x_{k+1}$ remains within this domain. This is achieved by applying a Euclidean projection operator, $\\Pi_{\\mathcal{X}}$, after each update. For a one-dimensional interval, this projection is equivalent to clamping the value:\n$$\\Pi_{\\mathcal{X}}(z) = \\min(\\max(z,-4),4)$$\nThe complete algorithm is:\n-   Initialize the iterate $x$ with the starting point $x_0$.\n-   For a fixed number of iterations, $K$:\n    1.  Compute the gradient $g_k = f'(x_k)$.\n    2.  Perform the descent step: $x_{\\text{temp}} = x_k - \\alpha g_k$.\n    3.  Project the result back into the domain: $x_{k+1} = \\Pi_{\\mathcal{X}}(x_{\\text{temp}})$.\n-   The final iterate, $x_{\\mathrm{gd}} = x_K$, and its corresponding function value, $f_{\\mathrm{gd}} = f(x_K)$, are the results of the algorithm.\n\n4.  **Comparison and Decision Rule**\nThe purpose of the comparison is to determine if the local gradient descent method has become \"trapped\" in a local minimum that is significantly worse than the best minimum found by the global random search. The performance difference is quantified by $\\Delta = f_{\\mathrm{gd}} - f_{\\mathrm{rand}}$.\nA positive $\\Delta$ indicates that the random search found a better solution (a lower minimum) than gradient descent. The problem defines a specific criterion for entrapment: gradient descent is considered trapped if this difference exceeds a given threshold, $\\tau=10^{-3}$.\nThe decision rule is:\n$$\\text{trapped} = (\\Delta > \\tau)$$\nThis boolean result, along with the coordinates and function values from both methods, provides a complete picture for each test case.\nFor Case D, where the number of iterations $K=0$, the gradient descent procedure terminates immediately. Thus, its final point $x_{\\mathrm{gd}}$ is simply its initial point $x_0$, and $f_{\\mathrm{gd}} = f(x_0)$. This case effectively compares a single, predetermined point against a large-scale random search.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares stochastic global search and projected gradient descent\n    on a nonconvex objective function for several test cases.\n    \"\"\"\n\n    # Define the objective function and its derivative.\n    # f(x) = sin(5x) + 0.1 * x^2\n    def f(x: np.ndarray | float) -> np.ndarray | float:\n        return np.sin(5 * x) + 0.1 * x**2\n\n    # f'(x) = 5*cos(5x) + 0.2*x\n    def df(x: np.ndarray | float) -> np.ndarray | float:\n        return 5 * np.cos(5 * x) + 0.2 * x\n\n    def stochastic_search(n: int, s: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs a stochastic global search using uniform random sampling.\n\n        Args:\n            n: Number of samples.\n            s: Seed for the pseudorandom number generator.\n            domain: The search interval (min, max).\n\n        Returns:\n            A tuple (x_rand, f_rand) containing the location and value of the\n            best sample found.\n        \"\"\"\n        rng = np.random.default_rng(s)\n        samples = rng.uniform(domain[0], domain[1], n)\n        values = f(samples)\n        min_index = np.argmin(values)\n        x_rand = samples[min_index]\n        f_rand = values[min_index]\n        return x_rand, f_rand\n\n    def gradient_descent(x0: float, alpha: float, K: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs projected gradient descent.\n\n        Args:\n            x0: Initial point.\n            alpha: Constant step size.\n            K: Number of iterations.\n            domain: The search interval (min, max) for projection.\n\n        Returns:\n            A tuple (x_gd, f_gd) containing the final location and value.\n        \"\"\"\n        x_k = x0\n        for _ in range(K):\n            grad = df(x_k)\n            x_k = x_k - alpha * grad\n            # Project back onto the domain (clamping)\n            x_k = np.clip(x_k, domain[0], domain[1])\n        \n        x_gd = x_k\n        f_gd = f(x_gd)\n        return x_gd, f_gd\n\n    # Define common parameters and test cases.\n    domain = (-4.0, 4.0)\n    tau = 1e-3\n\n    test_cases = [\n        # Case A (baseline success near the best basin)\n        {'s': 123, 'n': 500, 'x0': 0.0, 'alpha': 0.05, 'K': 200},\n        # Case B (entrapment in a distant local basin)\n        {'s': 7, 'n': 2000, 'x0': 2.8, 'alpha': 0.05, 'K': 200},\n        # Case C (undersampling stress test)\n        {'s': 42, 'n': 20, 'x0': -3.5, 'alpha': 0.02, 'K': 150},\n        # Case D (zero-iteration gradient descent boundary)\n        {'s': 0, 'n': 3000, 'x0': 1.0, 'alpha': 0.05, 'K': 0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Run stochastic search\n        x_rand, f_rand = stochastic_search(case['n'], case['s'], domain)\n\n        # Run gradient descent\n        x_gd, f_gd = gradient_descent(case['x0'], case['alpha'], case['K'], domain)\n\n        # Compare results and determine if trapped\n        delta = f_gd - f_rand\n        trapped = delta > tau\n\n        # Format the result list for this case\n        result_list = [\n            round(float(x_rand), 6),\n            round(float(f_rand), 6),\n            round(float(x_gd), 6),\n            round(float(f_gd), 6),\n            trapped\n        ]\n        all_results.append(result_list)\n    \n    # Print the aggregated results in the specified format\n    # The format is a string representation of a Python list of lists.\n    # e.g., [[-1.427..., -0.893..., ...], [...]]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Simulated Annealing is a powerful global optimization method that emulates the physical process of annealing, allowing probabilistic 'uphill' moves to escape local minima. The success of this strategy hinges on the cooling schedule, which governs the algorithm's transition from broad exploration to focused exploitation. This practice delves into the theoretical heart of this process, challenging you to determine the minimal cooling schedule required to ensure that the algorithm has a non-vanishing probability of crossing energy barriers, thereby guaranteeing its ability to explore the global landscape. ",
            "id": "3600630",
            "problem": "A canonical toy misfit in computational geophysics exhibits two plausible model explanations separated by an energetic barrier. Consider the scalar objective modeled by a double-well energy\n$f(x)=x^{4}-2x^{2}$,\nwhich has two basins of attraction corresponding to two competing geophysical models. You apply Metropolis simulated annealing, a Markov Chain Monte Carlo (MCMC) method, with a decreasing temperature sequence $\\{T_{k}\\}_{k\\geq 1}$, and at each iteration $k$ use a symmetric local proposal that with probability bounded below by a constant $q\\in(0,1]$ proposes a move that attempts to ascend from the current well to the barrier top before possibly descending into the other well. The Metropolis acceptance probability for a proposed move that increases the objective by $\\Delta f$ at temperature $T$ is\n$$\n\\alpha(\\Delta f, T)=\\exp\\!\\left(-\\frac{\\Delta f}{T}\\right).\n$$\n\n1) Compute the barrier height $B$ of $f(x)$, defined as the difference between the energy at the saddle (the barrier top) and the energy at either minimum.\n\n2) Let $N(n)$ denote the expected number of accepted barrier-ascending moves (i.e., accepted moves that surmount the barrier top at least once) in the first $n$ iterations. Using only the definition of the Metropolis acceptance probability and standard comparison tests for series, derive a condition on the cooling schedule $\\{T_{k}\\}$ under which $\\lim_{n\\to\\infty}\\mathbb{E}[N(n)]$ does not converge to a finite limit. In other words, find the asymptotically minimal schedule that ensures a nonzero (in the sense of being unbounded with $n$) expected cumulative count of barrier-crossing events as $k\\to\\infty$.\n\nExpress your final answer as a single closed-form asymptotic expression $T_{k}$ in terms of $k$ for the specific $f(x)$ above. No rounding is required.",
            "solution": "The analysis of the problem is performed in two parts as requested. First, the barrier height of the potential energy function is computed. Second, the condition on the cooling schedule for the divergence of the expected number of barrier crossings is derived.\n\nPart 1: Computation of the Barrier Height $B$\n\nThe objective function is given by $f(x) = x^{4} - 2x^{2}$. To find the minima and the saddle point, we must find the critical points by setting the first derivative of $f(x)$ with respect to $x$ to zero.\n\nThe first derivative is:\n$$\nf'(x) = \\frac{d}{dx}(x^{4} - 2x^{2}) = 4x^{3} - 4x\n$$\nSetting $f'(x) = 0$ to find the critical points:\n$$\n4x^{3} - 4x = 0\n$$\n$$\n4x(x^{2} - 1) = 0\n$$\n$$\n4x(x - 1)(x + 1) = 0\n$$\nThe critical points are $x=0$, $x=1$, and $x=-1$.\n\nTo classify these critical points, we use the second derivative test. The second derivative of $f(x)$ is:\n$$\nf''(x) = \\frac{d}{dx}(4x^{3} - 4x) = 12x^{2} - 4\n$$\nWe evaluate $f''(x)$ at each critical point:\n- For $x=0$: $f''(0) = 12(0)^{2} - 4 = -4$. Since $f''(0)  0$, the point $x=0$ is a local maximum, which corresponds to the top of the energy barrier (a saddle point in higher dimensions).\n- For $x=1$: $f''(1) = 12(1)^{2} - 4 = 8$. Since $f''(1)  0$, the point $x=1$ is a local minimum.\n- For $x=-1$: $f''(-1) = 12(-1)^{2} - 4 = 8$. Since $f''(-1)  0$, the point $x=-1$ is also a local minimum.\n\nThese two minima correspond to the two basins of attraction. Now, we evaluate the energy $f(x)$ at these points:\n- Energy at the saddle point: $f_{\\text{saddle}} = f(0) = 0^{4} - 2(0)^{2} = 0$.\n- Energy at the minima: $f_{\\text{min}} = f(1) = 1^{4} - 2(1)^{2} = 1 - 2 = -1$. And $f(-1) = (-1)^{4} - 2(-1)^{2} = 1 - 2 = -1$.\n\nThe barrier height $B$ is defined as the difference between the energy at the saddle and the energy at either minimum.\n$$\nB = f_{\\text{saddle}} - f_{\\text{min}} = 0 - (-1) = 1\n$$\nThus, the barrier height is $B=1$.\n\nPart 2: Derivation of the Cooling Schedule\n\nLet $A_{k}$ be the event that a barrier-ascending move is proposed and accepted at iteration $k$. The expected number of such events in the first $n$ iterations is given by the linearity of expectation:\n$$\n\\mathbb{E}[N(n)] = \\mathbb{E}\\left[\\sum_{k=1}^{n} I(A_{k})\\right] = \\sum_{k=1}^{n} \\mathbb{E}[I(A_{k})] = \\sum_{k=1}^{n} P(A_{k})\n$$\nwhere $I(A_{k})$ is the indicator function for the event $A_{k}$.\n\nThe problem requires finding a condition on the cooling schedule $\\{T_{k}\\}$ such that $\\lim_{n\\to\\infty} \\mathbb{E}[N(n)]$ does not converge to a finite limit. This is equivalent to the condition that the series of probabilities diverges:\n$$\n\\sum_{k=1}^{\\infty} P(A_{k}) = \\infty\n$$\nThe probability of event $A_k$ is the product of the probability of proposing a barrier-ascending move and the probability of accepting it. The problem states that a move attempting to ascend the barrier is proposed with a probability $p_k$ bounded below by a constant $q > 0$. Such a move, starting from near a minimum, must increase its energy by at least the barrier height $B$ to reach the saddle point. We consider the minimal such increase, $\\Delta f = B$. The Metropolis acceptance probability for this move at temperature $T_k$ is $\\alpha(B, T_k) = \\exp(-B/T_k)$.\n\nThe probability of proposing and accepting such a move at step $k$ is therefore bounded below:\n$$\nP(A_k) \\ge q \\exp\\left(-\\frac{B}{T_k}\\right)\n$$\nBy the comparison test for series, if the series formed by the lower bound diverges, so does the original series. Since $q$ is a positive constant, we need to find the condition on $\\{T_k\\}$ that ensures:\n$$\n\\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{B}{T_k}\\right) = \\infty\n$$\nThis is a classical condition in the theory of simulated annealing for guaranteeing convergence to the global minimum, as it ensures an infinite number of expected escapes from any local minimum.\n\nTo find the asymptotic form of $T_k$ that satisfies this, we can use the integral test for convergence. The sequence of terms $a_k = \\exp(-B/T_k)$ is positive. Since $\\{T_k\\}_{k\\ge 1}$ is a decreasing sequence approaching $0$, $1/T_k$ is an increasing sequence, $-B/T_k$ is a decreasing sequence, and thus $a_k = \\exp(-B/T_k)$ is a decreasing sequence. The integral test is applicable. The divergence of the sum is equivalent to the divergence of the corresponding integral:\n$$\n\\int_{1}^{\\infty} \\exp\\left(-\\frac{B}{T_x}\\right) dx = \\infty\n$$\nA standard choice for the cooling schedule at the critical boundary of convergence is of the form $T_k = \\frac{C}{\\ln k}$ for some constant $C$ and large $k$. Let us analyze this form. Substituting $T_x = \\frac{C}{\\ln x}$ into the term gives:\n$$\n\\exp\\left(-\\frac{B}{T_x}\\right) = \\exp\\left(-\\frac{B}{C/\\ln x}\\right) = \\exp\\left(-\\frac{B}{C}\\ln x\\right) = \\exp\\left(\\ln x^{-B/C}\\right) = x^{-B/C}\n$$\nThe series becomes $\\sum_{k=k_0}^{\\infty} k^{-B/C}$, which is a p-series with exponent $p = B/C$. A p-series diverges if and only if its exponent $p \\le 1$. Therefore, for the expected number of crossings to be infinite, we require:\n$$\n\\frac{B}{C} \\le 1 \\implies C \\ge B\n$$\nThe problem asks for the \"asymptotically minimal schedule\" that ensures this divergence. This corresponds to the fastest cooling (smallest $T_k$) that still satisfies the condition. A smaller $T_k$ corresponds to a smaller constant $C$. The minimal value of $C$ that satisfies $C \\ge B$ is $C = B$.\n\nThis gives the boundary case, or the critical cooling schedule:\n$$\nT_k = \\frac{B}{\\ln k}\n$$\nUsing the value $B=1$ calculated in Part 1, the specific asymptotic schedule is:\n$$\nT_k = \\frac{1}{\\ln k}\n$$\nThis schedule represents the slowest rate of cooling of the form $C/\\ln(k)$ that fails to guarantee an infinite number of expected crossings from any local minimum. Any schedule that cools slower than this (i.e., $T_k' \\ge T_k$ asymptotically, meaning $C' \\ge B$) will also satisfy the condition. The question asks for the minimal schedule that ensures a non-zero (unbounded) count, which is this boundary case.",
            "answer": "$$\n\\boxed{\\frac{1}{\\ln(k)}}\n$$"
        },
        {
            "introduction": "Particle Swarm Optimization (PSO) is a population-based method where a 'swarm' of candidate solutions collaboratively explores the search space, with behavior governed by a balance of individual, social, and inertial influences. This exercise guides you through a formal stability analysis of the PSO dynamics to derive the conditions under which the swarm is guaranteed to converge, preventing its particles from diverging or oscillating endlessly. By investigating the renowned Clerc–Kennedy constriction factor, you will learn how to theoretically ground the choice of algorithm parameters to ensure stable and effective optimization. ",
            "id": "3600610",
            "problem": "Consider a one-dimensional inversion of a scalar subsurface property in computational geophysics using Particle Swarm Optimization (PSO). The goal is to minimize a misfit function by iteratively updating a particle’s position. Start from the standard PSO update rule, which (in one dimension) is defined by the velocity-position recursion\n$$\nv_{t+1} = w\\,v_t + c_1\\,r_1\\,(p - x_t) + c_2\\,r_2\\,(g - x_t), \\quad x_{t+1} = x_t + v_{t+1},\n$$\nwhere $w$ is the inertia weight, $c_1$ and $c_2$ are the cognitive and social acceleration coefficients, $r_1$ and $r_2$ are independent random variables uniformly distributed on $\\left[0,1\\right]$, $p$ is the particle’s best-known position, and $g$ is the global best position.\n\nIn deterministic stability analysis, replace $r_1$ and $r_2$ by their means, and linearize the dynamics about a fixed point $x^\\star$ satisfying $x^\\star = p = g$. Define the position error $e_t = x_t - x^\\star$ and derive the linear state-space form of the error dynamics. Compute the eigenvalues of the linearized system and use a discrete-time stability criterion to obtain parameter conditions under which the fixed point is asymptotically stable.\n\nNext, consider the constriction-factor PSO variant, whose velocity recursion is\n$$\nv_{t+1} = \\chi\\big[v_t + c_1\\,r_1\\,(p - x_t) + c_2\\,r_2\\,(g - x_t)\\big], \\quad x_{t+1} = x_t + v_{t+1},\n$$\nwith the same definitions as above and $\\chi$ a scalar constriction factor. By repeating the deterministic linearization about $x^\\star = p = g$ and replacing $r_1, r_2$ by their means, derive the characteristic polynomial of the error dynamics and enforce asymptotic convergence. Using the Clerc–Kennedy constriction analysis, express $\\chi$ as a closed-form function of $\\phi = c_1 + c_2$ that guarantees stability when $\\phi  4$.\n\nProvide the final expression for the constriction factor $\\chi(\\phi)$ in closed form. No numerical evaluation is required. The final answer must be a single analytic expression.",
            "solution": "The problem requires a stability analysis of the Particle Swarm Optimization (PSO) algorithm, specifically for the constriction factor variant. The analysis is performed in a deterministic setting by linearizing the system's dynamics around a fixed point.\n\n1.  **Linearization of Constriction-Factor PSO Dynamics**\n\nThe velocity and position update rules for the constriction-factor PSO are given as:\n$$\nv_{t+1} = \\chi\\big[v_t + c_1\\,r_1\\,(p - x_t) + c_2\\,r_2\\,(g - x_t)\\big]\n$$\n$$\nx_{t+1} = x_t + v_{t+1}\n$$\nFor the deterministic analysis, the random variables $r_1$ and $r_2$ are replaced by their mean value, which is $E[r_1] = E[r_2] = 1/2$ for a uniform distribution on $[0,1]$. We linearize the dynamics around a fixed point $x^\\star$, where the particle has converged, meaning its personal best position $p$ and the global best position $g$ are both at $x^\\star$. Thus, $p = g = x^\\star$.\n\nSubstituting these into the velocity update rule:\n$$\nv_{t+1} = \\chi\\left[v_t + \\frac{c_1}{2}(x^\\star - x_t) + \\frac{c_2}{2}(x^\\star - x_t)\\right]\n$$\nLet $\\phi = c_1 + c_2$. The equation simplifies to:\n$$\nv_{t+1} = \\chi\\left[v_t - \\frac{\\phi}{2}(x_t - x^\\star)\\right]\n$$\n\n2.  **State-Space Representation of Error Dynamics**\n\nWe define the error state vector as $\\mathbf{z}_t = \\begin{pmatrix} e_t \\\\ v_t \\end{pmatrix}$, where $e_t = x_t - x^\\star$ is the position error. At the fixed point, the velocity is zero, so the velocity component of the state vector is simply $v_t$.\n\nThe update equation for the velocity error is the equation for $v_{t+1}$ itself:\n$$\nv_{t+1} = -\\frac{\\chi\\phi}{2}e_t + \\chi v_t\n$$\nThe update equation for the position error $e_t$ is derived from the position update rule:\n$$\ne_{t+1} = x_{t+1} - x^\\star = (x_t + v_{t+1}) - x^\\star = (x_t - x^\\star) + v_{t+1} = e_t + v_{t+1}\n$$\nSubstituting the expression for $v_{t+1}$ into the equation for $e_{t+1}$:\n$$\ne_{t+1} = e_t + \\left(-\\frac{\\chi\\phi}{2}e_t + \\chi v_t\\right) = \\left(1 - \\frac{\\chi\\phi}{2}\\right)e_t + \\chi v_t\n$$\nThese two equations form the linear state-space representation of the error dynamics:\n$$\n\\begin{pmatrix} e_{t+1} \\\\ v_{t+1} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{\\chi\\phi}{2}  \\chi \\\\ -\\frac{\\chi\\phi}{2}  \\chi \\end{pmatrix} \\begin{pmatrix} e_t \\\\ v_t \\end{pmatrix}\n$$\n\n3.  **Characteristic Polynomial and Stability**\n\nThe stability of this discrete-time linear system is determined by the eigenvalues of the state transition matrix. The eigenvalues are the roots of the characteristic polynomial, $\\det(J - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix} 1 - \\frac{\\chi\\phi}{2} - \\lambda  \\chi \\\\ -\\frac{\\chi\\phi}{2}  \\chi - \\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(1 - \\frac{\\chi\\phi}{2} - \\lambda\\right)(\\chi - \\lambda) - (\\chi)\\left(-\\frac{\\chi\\phi}{2}\\right) = 0\n$$\nExpanding this gives:\n$$\n\\chi - \\lambda - \\frac{\\chi^2\\phi}{2} + \\frac{\\chi\\phi\\lambda}{2} - \\chi\\lambda + \\lambda^2 + \\frac{\\chi^2\\phi}{2} = 0\n$$\nCollecting terms yields the characteristic polynomial:\n$$\n\\lambda^2 - \\lambda\\left(1 + \\chi - \\frac{\\chi\\phi}{2}\\right) + \\chi = 0\n$$\nFor the system to be asymptotically stable (i.e., for the error to converge to zero), the magnitude of all eigenvalues must be less than 1 ($|\\lambda|  1$).\n\n4.  **Clerc-Kennedy Constriction Factor**\n\nThe problem asks to use the Clerc-Kennedy constriction analysis to find an expression for $\\chi$ that guarantees stability for $\\phi > 4$. This analysis provides a standard formula for the constriction factor $\\chi$:\n$$\n\\chi = \\frac{2}{|2-\\phi-\\sqrt{\\phi^2-4\\phi}|}\n$$\nTo simplify this expression for the condition $\\phi > 4$, we analyze the term inside the absolute value:\n-   Since $\\phi > 4$, the term $2-\\phi$ is negative.\n-   The term under the square root, $\\phi^2 - 4\\phi = \\phi(\\phi-4)$, is positive, so its square root is real and positive.\n-   Therefore, the entire expression $2-\\phi-\\sqrt{\\phi^2-4\\phi}$ is the sum of two negative numbers, making it negative.\n\nFor a negative number $X$, its absolute value is $|X| = -X$. Applying this:\n$$\n|2-\\phi-\\sqrt{\\phi^2-4\\phi}| = -(2-\\phi-\\sqrt{\\phi^2-4\\phi}) = \\phi - 2 + \\sqrt{\\phi^2-4\\phi}\n$$\nSubstituting this back into the formula for $\\chi$ gives the final closed-form expression:\n$$\n\\chi(\\phi) = \\frac{2}{\\phi - 2 + \\sqrt{\\phi^{2} - 4\\phi}}\n$$\nThis choice of $\\chi$ is designed to control the system's eigenvalues to ensure convergent behavior.",
            "answer": "$$\\boxed{\\frac{2}{\\phi - 2 + \\sqrt{\\phi^{2} - 4\\phi}}}$$"
        }
    ]
}