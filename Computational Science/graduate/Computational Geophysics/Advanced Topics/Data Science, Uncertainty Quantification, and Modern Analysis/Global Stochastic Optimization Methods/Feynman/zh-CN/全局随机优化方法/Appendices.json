{
    "hands_on_practices": [
        {
            "introduction": "要掌握全局优化，第一步是亲身体会其核心挑战：局部最优陷阱。这个实践练习将通过一个具体的编码任务来阐明这一问题。我们将对比一个基于梯度的局部搜索方法和一个简单的全局随机搜索方法，在一个非凸函数上的表现，从而直观地揭示为何局部优化方法在复杂地球物理反演问题中常常会失效，并强调全局搜索策略的必要性 。",
            "id": "3600613",
            "problem": "您需要实现并比较两种全局随机优化策略，它们将应用于一个一维非凸目标函数。该函数在计算地球物理学中通常用作盆地陷阱的简化模型。考虑在有界域 $$\\mathcal{X}=[-4,4]$$ 上定义的标量目标 $$f(x)=\\sin(5x)+0.1\\,x^2,$$ 其中所有角度均以弧度为单位，且 $x$ 是无量纲的。您的目标是近似求解一个全局最小值点，并阐明随机全局搜索与局部、单起点梯度下降法之间的差异。\n\n基于以下基本原理：\n- 全局最小化问题是计算 $$\\min_{x\\in\\mathcal{X}} f(x)$$。\n- 区间上的独立同分布均匀采样使用伪随机数生成器抽取 $$x_i\\sim \\mathrm{Uniform}([-4,4])$$，并根据目标函数选择最佳样本。\n- 在一维情况下，具有恒定步长的梯度下降法使用更新规则 $$x_{k+1}=\\Pi_{\\mathcal{X}}\\bigl(x_k-\\alpha\\,f'(x_k)\\bigr),$$ 其中 $$\\Pi_{\\mathcal{X}}$$ 表示到区间 $$\\mathcal{X}$$ 上的欧几里得投影（即，截断到 $$[-4,4]$$ 范围内），$$\\alpha>0$$ 是一个固定的步长，$$f'(x)$$ 是使用链式法则和微分的线性性质计算出的一阶导数。\n\n待实现的任务：\n1) 使用导数的定义和链式法则，为给定的 $$f(x)$$ 推导并实现 $$f'(x)$$。\n2) 实现一个随机全局搜索，使用一个指定整数种子 $$s$$ 的伪随机数生成器，在 $$[-4,4]$$ 上抽取 $$n$$ 个独立的均匀样本，在每个样本上评估 $$f(x)$$，并返回具有最小目标值的样本位置及该值。\n3) 实现单起点投影梯度下降法，使用给定的初始点 $$x_0\\in[-4,4]$$、恒定步长 $$\\alpha>0$$ 以及固定的迭代次数 $$K\\in\\mathbb{N}$$。每次更新后，通过 $$\\Pi_{\\mathcal{X}}(x)=\\min(\\max(x,-4),4)$$ 将迭代点投影回 $$[-4,4]$$。\n\n比较指标与决策规则：\n- 对于每个测试用例，计算随机采样得到的近似最优值 $$f_{\\mathrm{rand}}:=\\min_{i=1,\\dots,n} f(x_i),$$ 及其对应的点 $$x_{\\mathrm{rand}}$$，以及梯度下降后的最终值 $$f_{\\mathrm{gd}}:=f(x_K),$$ 及其最终点 $$x_{\\mathrm{gd}}$$。\n- 定义 $$\\Delta:=f_{\\mathrm{gd}}-f_{\\mathrm{rand}}$$。如果 $$\\Delta>\\tau,$$ 则声明梯度下降法陷入了次优盆地，其中 $$\\tau=10^{-3}$$。\n\n数值和输出要求：\n- 所有三角函数求值均使用弧度。没有物理单位。\n- 对于每个测试用例，返回一个列表 $$[x_{\\mathrm{rand}}, f_{\\mathrm{rand}}, x_{\\mathrm{gd}}, f_{\\mathrm{gd}}, \\text{trapped}]$$，其中前四个条目是四舍五入到六位小数的浮点数，最后一个条目是使用阈值 $$\\tau=10^{-3}$$ 计算出的布尔值。\n- 您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，按顺序汇总所有测试用例的列表，例如 $$[r_1,r_2,\\dots,r_m]$$。\n\n待实现和评估的测试套件：\n- 用例 A（在最佳盆地附近的基线成功）：$$s=123$$, $$n=500$$, $$x_0=0.0$$, $$\\alpha=0.05$$, $$K=200$$。\n- 用例 B（在较远的局部盆地中陷入）：$$s=7$$, $$n=2000$$, $$x_0=2.8$$, $$\\alpha=0.05$$, $$K=200$$。\n- 用例 C（欠采样压力测试）：$$s=42$$, $$n=20$$, $$x_0=-3.5$$, $$\\alpha=0.02$$, $$K=150$$。\n- 用例 D（零迭代梯度下降边界情况）：$$s=0$$, $$n=3000$$, $$x_0=1.0$$, $$\\alpha=0.05$$, $$K=0$$。\n\n您的程序必须精确实现上述逻辑，并以 $$[[x_{\\mathrm{rand}}^{(A)},f_{\\mathrm{rand}}^{(A)},x_{\\mathrm{gd}}^{(A)},f_{\\mathrm{gd}}^{(A)},\\mathrm{trapped}^{(A)}],\\dots,[x_{\\mathrm{rand}}^{(D)},f_{\\mathrm{rand}}^{(D)},x_{\\mathrm{gd}}^{(D)},f_{\\mathrm{gd}}^{(D)},\\mathrm{trapped}^{(D)}]]$$ 格式打印单行的汇总结果。",
            "solution": "该问题是有效的，因为它在数值优化领域提出了一个定义明确、自成体系且科学上合理的计算任务。它要求实现并比较两种标准的优化算法——随机全局搜索和梯度下降法，并将它们应用于一个指定的非凸函数。所有参数、条件和评估指标都已提供，确保了问题的客观性，并能够得出一个唯一的、可验证的解。\n\n问题的核心是在一个多峰目标函数上，比较一种全局优化策略与一种局部优化策略。所选函数 $$f(x)=\\sin(5x)+0.1\\,x^2$$，定义在紧致域 $$\\mathcal{X}=[-4,4]$$ 上，是非凸的。这意味着它拥有多个局部极小值，使其成为一个合适的测试用例，用以说明局部搜索方法“陷入”次优吸引盆地的现象，而全局搜索则有更高的概率识别出全局最小值点。\n\n方法步骤如下：首先，我们推导函数导数的解析形式，这是梯度下降算法所必需的。其次，我们规范化随机搜索和投影梯度下降的过程。最后，我们将这些算法应用于指定的测试用例，并使用定义的比较指标来评估它们的性能。\n\n1.  **目标函数及其导数**\n目标函数给定为：\n$$f(x)=\\sin(5x)+0.1\\,x^2$$\n为了实现梯度下降，我们必须计算其一阶导数 $$f'(x)$$。我们应用微分的线性性质（即和的导数等于导数的和），并对三角函数项应用链式法则。\n第一项 $$\\sin(5x)$$ 对 $$x$$ 的导数使用链式法则 $$d/dx(\\sin(u)) = \\cos(u) \\cdot du/dx$$ 求得。这里，$$u=5x$$，所以 $$du/dx=5$$。\n$$\\frac{d}{dx}\\left(\\sin(5x)\\right) = \\cos(5x) \\cdot 5 = 5\\cos(5x)$$\n第二项 $$0.1x^2$$ 的导数使用幂法则求得：\n$$\\frac{d}{dx}\\left(0.1x^2\\right) = 0.1 \\cdot 2x = 0.2x$$\n综合这些结果，得到完整的导数：\n$$f'(x) = 5\\cos(5x) + 0.2x$$\n按照规定，所有三角函数计算都以弧度为单位执行。\n\n2.  **随机全局搜索：均匀随机采样**\n该方法对搜索空间 $$\\mathcal{X}=[-4,4]$$ 进行全局探索。其原理基于概率覆盖。通过抽取足够大量的（$$n$$ 个）独立同分布样本 $$x_i \\sim \\mathrm{Uniform}([-4,4])$$，该算法很可能将至少一个样本置于全局最小值的吸引盆地内。该方法对函数的局部结构（如其梯度）是“盲目”的，纯粹依赖于在随机点上的函数求值。该算法是：\n-   用指定的整数种子 $$s$$ 初始化伪随机数生成器，以确保可复现性。\n-   从区间 $$[-4,4]$$ 中均匀生成 $$n$$ 个随机样本 $$\\{x_1, x_2, \\dots, x_n\\}$$。\n-   对每个样本 $$x_i$$ 计算 $$f(x_i)$$。\n-   确定产生最小函数值 $$f_{\\mathrm{rand}} = \\min_{i=1,\\dots,n} f(x_i)$$ 的样本 $$x_{\\mathrm{rand}}$$。\n\n3.  **局部搜索：投影梯度下降**\n这是一种旨在寻找局部最小值的一阶迭代优化算法。它从一个初始点 $$x_0$$ 开始，并沿负梯度方向（即局部最陡下降的方向）迭代移动。\n迭代更新规则为：\n$$x_{k+1} = x_k - \\alpha f'(x_k)$$\n其中 $$k$$ 是迭代次数，$$x_k$$ 是当前点，$$f'(x_k)$$ 是该点的梯度，$$\\alpha>0$$ 是控制每一步大小的恒定步长。\n由于问题被约束在域 $$\\mathcal{X}=[-4,4]$$ 上，我们必须确保每个新的迭代点 $$x_{k+1}$$ 都保持在该域内。这可以通过在每次更新后应用一个欧几里得投影算子 $$\\Pi_{\\mathcal{X}}$$ 来实现。对于一维区间，此投影等效于对值进行截断：\n$$\\Pi_{\\mathcal{X}}(z) = \\min(\\max(z,-4),4)$$\n完整的算法如下：\n-   用起始点 $$x_0$$ 初始化迭代点 $$x$$。\n-   对于固定的迭代次数 $$K$$：\n    1.  计算梯度 $$g_k = f'(x_k)$$。\n    2.  执行下降步骤：$$x_{\\text{temp}} = x_k - \\alpha g_k$$。\n    3.  将结果投影回域内：$$x_{k+1} = \\Pi_{\\mathcal{X}}(x_{\\text{temp}})$$。\n-   最终的迭代点 $$x_{\\mathrm{gd}} = x_K$$ 及其对应的函数值 $$f_{\\mathrm{gd}} = f(x_K)$$ 是该算法的结果。\n\n4.  **比较与决策规则**\n比较的目的是确定局部梯度下降法是否“陷入”了一个局部最小值，该最小值显著劣于全局随机搜索找到的最佳最小值。性能差异由 $$\\Delta = f_{\\mathrm{gd}} - f_{\\mathrm{rand}}$$ 量化。\n正的 $$\\Delta$$ 表示随机搜索找到了比梯度下降更好的解（一个更低的最小值）。问题为陷入定义了一个具体标准：如果这个差值超过给定阈值 $$\\tau=10^{-3}$$，则认为梯度下降法被陷入。\n决策规则是：\n$$\\text{trapped} = (\\Delta > \\tau)$$\n这个布尔结果，连同两种方法的坐标和函数值，为每个测试用例提供了一个完整的画面。\n对于用例 D，其中迭代次数 $$K=0$$，梯度下降过程立即终止。因此，其最终点 $$x_{\\mathrm{gd}}$$ 就是其初始点 $$x_0$$，且 $$f_{\\mathrm{gd}} = f(x_0)$$。该用例实际上是将一个单一的、预先确定的点与大规模随机搜索进行比较。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares stochastic global search and projected gradient descent\n    on a nonconvex objective function for several test cases.\n    \"\"\"\n\n    # Define the objective function and its derivative.\n    # f(x) = sin(5x) + 0.1 * x^2\n    def f(x: np.ndarray | float) -> np.ndarray | float:\n        return np.sin(5 * x) + 0.1 * x**2\n\n    # f'(x) = 5*cos(5x) + 0.2*x\n    def df(x: np.ndarray | float) -> np.ndarray | float:\n        return 5 * np.cos(5 * x) + 0.2 * x\n\n    def stochastic_search(n: int, s: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs a stochastic global search using uniform random sampling.\n\n        Args:\n            n: Number of samples.\n            s: Seed for the pseudorandom number generator.\n            domain: The search interval (min, max).\n\n        Returns:\n            A tuple (x_rand, f_rand) containing the location and value of the\n            best sample found.\n        \"\"\"\n        rng = np.random.default_rng(s)\n        samples = rng.uniform(domain[0], domain[1], n)\n        values = f(samples)\n        min_index = np.argmin(values)\n        x_rand = samples[min_index]\n        f_rand = values[min_index]\n        return x_rand, f_rand\n\n    def gradient_descent(x0: float, alpha: float, K: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs projected gradient descent.\n\n        Args:\n            x0: Initial point.\n            alpha: Constant step size.\n            K: Number of iterations.\n            domain: The search interval (min, max) for projection.\n\n        Returns:\n            A tuple (x_gd, f_gd) containing the final location and value.\n        \"\"\"\n        x_k = x0\n        for _ in range(K):\n            grad = df(x_k)\n            x_k = x_k - alpha * grad\n            # Project back onto the domain (clamping)\n            x_k = np.clip(x_k, domain[0], domain[1])\n        \n        x_gd = x_k\n        f_gd = f(x_gd)\n        return x_gd, f_gd\n\n    # Define common parameters and test cases.\n    domain = (-4.0, 4.0)\n    tau = 1e-3\n\n    test_cases = [\n        # Case A (baseline success near the best basin)\n        {'s': 123, 'n': 500, 'x0': 0.0, 'alpha': 0.05, 'K': 200},\n        # Case B (entrapment in a distant local basin)\n        {'s': 7, 'n': 2000, 'x0': 2.8, 'alpha': 0.05, 'K': 200},\n        # Case C (undersampling stress test)\n        {'s': 42, 'n': 20, 'x0': -3.5, 'alpha': 0.02, 'K': 150},\n        # Case D (zero-iteration gradient descent boundary)\n        {'s': 0, 'n': 3000, 'x0': 1.0, 'alpha': 0.05, 'K': 0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Run stochastic search\n        x_rand, f_rand = stochastic_search(case['n'], case['s'], domain)\n\n        # Run gradient descent\n        x_gd, f_gd = gradient_descent(case['x0'], case['alpha'], case['K'], domain)\n\n        # Compare results and determine if trapped\n        delta = f_gd - f_rand\n        trapped = delta > tau\n\n        # Format the result list for this case\n        result_list = [\n            round(float(x_rand), 6),\n            round(float(f_rand), 6),\n            round(float(x_gd), 6),\n            round(float(f_gd), 6),\n            trapped\n        ]\n        all_results.append(result_list)\n    \n    # Print the aggregated results in the specified format\n    # The format is a string representation of a Python list of lists.\n    # e.g., [[-1.427..., -0.893..., ...], [...]]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "既然我们已经理解了局部最优陷阱的问题，下一个问题便是：如何设计一个能够逃离这些陷阱的算法？本练习将深入探讨模拟退火（Simulated Annealing, SA）算法的理论核心。通过分析一个经典的双阱势模型，我们将推导出保证算法能够持续探索整个搜索空间并最终逃离任何局部陷阱的关键条件——即临界冷却策略。这个推导过程将揭示模拟退火算法全局收敛性的概率论基础 。",
            "id": "3600630",
            "problem": "计算地球物理学中的一个典型玩具失配问题表现出两种由能量势垒隔开的合理解释模型。考虑由双阱能量建模的标量目标函数\n$$\nf(x)=x^{4}-2x^{2},\n$$\n它有两个吸引盆，对应两个相互竞争的地球物理模型。你应用 Metropolis 模拟退火法，这是一种马尔可夫链蒙特卡洛 (MCMC) 方法，使用递减的温度序列 $\\{T_{k}\\}_{k\\geq 1}$。在每次迭代 $k$ 中，使用一个对称的局部提议，该提议以一个由常数 $q\\in(0,1]$ 为下界的概率，提出一个试图从当前阱爬升到势垒顶部，然后可能下降到另一个阱的移动。在温度 $T$ 下，对于一个使目标函数增加 $\\Delta f$ 的提议移动，其 Metropolis 接受概率为\n$$\n\\alpha(\\Delta f, T)=\\exp\\!\\left(-\\frac{\\Delta f}{T}\\right).\n$$\n\n1) 计算 $f(x)$ 的势垒高度 $B$，定义为鞍点（势垒顶部）的能量与任一极小值点能量之差。\n\n2) 令 $N(n)$ 表示前 $n$ 次迭代中被接受的越过势垒的移动（即，至少一次越过势垒顶部的被接受的移动）的期望数量。仅使用 Metropolis 接受概率的定义和级数的标准比较检验法，推导出一个关于冷却方案 $\\{T_{k}\\}$ 的条件，使得 $\\lim_{n\\to\\infty}\\mathbb{E}[N(n)]$ 不收敛于一个有限极限。换句话说，找到一个渐近最小的方案，确保当 $k\\to\\infty$ 时，跨越势垒事件的期望累积计数为非零（在随 $n$ 无界的意义上）。\n\n对于上面给定的 $f(x)$，将你的最终答案表示为关于 $k$ 的单个闭式渐近表达式 $T_{k}$。不需要四舍五入。",
            "solution": "根据要求，问题的分析分两部分进行。首先，计算势能函数的势垒高度。其次，推导使期望穿越势垒次数发散的冷却方案条件。\n\n第一部分：计算势垒高度 $B$\n\n目标函数为 $f(x) = x^{4} - 2x^{2}$。为了找到极小值点和鞍点，我们必须通过将 $f(x)$ 对 $x$ 的一阶导数设为零来找到临界点。\n\n一阶导数为：\n$$\nf'(x) = \\frac{d}{dx}(x^{4} - 2x^{2}) = 4x^{3} - 4x\n$$\n设 $f'(x) = 0$ 来寻找临界点：\n$$\n4x^{3} - 4x = 0\n$$\n$$\n4x(x^{2} - 1) = 0\n$$\n$$\n4x(x - 1)(x + 1) = 0\n$$\n临界点是 $x=0$，$x=1$ 和 $x=-1$。\n\n为了对这些临界点进行分类，我们使用二阶导数检验法。$f(x)$ 的二阶导数是：\n$$\nf''(x) = \\frac{d}{dx}(4x^{3} - 4x) = 12x^{2} - 4\n$$\n我们在每个临界点计算 $f''(x)$ 的值：\n- 对于 $x=0$：$f''(0) = 12(0)^{2} - 4 = -4$。由于 $f''(0)  0$，点 $x=0$ 是一个局部极大值点，对应于能量势垒的顶部（在更高维度中是一个鞍点）。\n- 对于 $x=1$：$f''(1) = 12(1)^{2} - 4 = 8$。由于 $f''(1)  0$，点 $x=1$ 是一个局部极小值点。\n- 对于 $x=-1$：$f''(-1) = 12(-1)^{2} - 4 = 8$。由于 $f''(-1)  0$，点 $x=-1$ 也是一个局部极小值点。\n\n这两个极小值点对应于两个吸引盆。现在，我们计算这些点的能量 $f(x)$：\n- 鞍点的能量：$f_{\\text{saddle}} = f(0) = 0^{4} - 2(0)^{2} = 0$。\n- 极小值点的能量：$f_{\\text{min}} = f(1) = 1^{4} - 2(1)^{2} = 1 - 2 = -1$。并且 $f(-1) = (-1)^{4} - 2(-1)^{2} = 1 - 2 = -1$。\n\n势垒高度 $B$ 定义为鞍点的能量与任一极小值点能量之差。\n$$\nB = f_{\\text{saddle}} - f_{\\text{min}} = 0 - (-1) = 1\n$$\n因此，势垒高度为 $B=1$。\n\n第二部分：推导冷却方案\n\n令 $A_{k}$ 为在第 $k$ 次迭代时提出并接受了一个越过势垒的移动的事件。根据期望的线性性，前 $n$ 次迭代中此类事件的期望数量为：\n$$\n\\mathbb{E}[N(n)] = \\mathbb{E}\\left[\\sum_{k=1}^{n} I(A_{k})\\right] = \\sum_{k=1}^{n} \\mathbb{E}[I(A_{k})] = \\sum_{k=1}^{n} P(A_{k})\n$$\n其中 $I(A_{k})$ 是事件 $A_{k}$ 的指示函数。\n\n问题要求找到冷却方案 $\\{T_{k}\\}$ 的一个条件，使得 $\\lim_{n\\to\\infty} \\mathbb{E}[N(n)]$ 不收敛于一个有限极限。这等价于概率级数发散的条件：\n$$\n\\sum_{k=1}^{\\infty} P(A_{k}) = \\infty\n$$\n事件 $A_k$ 的概率是提出一个越过势垒的移动的概率与接受它的概率的乘积。问题陈述，一个试图越过势垒的移动被提出的概率 $p_k$ 有一个正常数 $q > 0$作为下界。这样的移动，从一个极小值点附近开始，必须将其能量增加至少势垒高度 $B$ 才能到达鞍点。我们考虑最小的这种增加量 $\\Delta f = B$。在温度 $T_k$ 下，此移动的 Metropolis 接受概率为 $\\alpha(B, T_k) = \\exp(-B/T_k)$。\n\n因此，在第 $k$ 步提出并接受这样一个移动的概率有如下下界：\n$$\nP(A_k) \\ge q \\exp\\left(-\\frac{B}{T_k}\\right)\n$$\n根据级数的比较检验法，如果由下界构成的级数发散，那么原级数也发散。由于 $q$ 是一个正常数，我们需要找到 $\\{T_k\\}$ 的条件以确保：\n$$\n\\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{B}{T_k}\\right) = \\infty\n$$\n这是模拟退火理论中保证收敛到全局最小值的经典条件，因为它确保了从任何局部最小值逃逸的期望次数是无限的。\n\n为了找到满足此条件的 $T_k$ 的渐近形式，我们可以使用积分判敛法。项的序列 $a_k = \\exp(-B/T_k)$ 是正的。由于 $\\{T_k\\}_{k\\ge 1}$ 是一个趋近于 $0$ 的递减序列，$1/T_k$ 是一个递增序列，$-B/T_k$ 是一个递减序列，因此 $a_k = \\exp(-B/T_k)$ 是一个递减序列。积分判敛法适用。级数的发散等价于相应积分的发散：\n$$\n\\int_{1}^{\\infty} \\exp\\left(-\\frac{B}{T_x}\\right) dx = \\infty\n$$\n在收敛的临界边界上，冷却方案的一个标准选择形式是 $T_k = \\frac{C}{\\ln k}$，其中 $C$ 是某个常数，且 $k$ 很大。我们来分析这种形式。将 $T_x = \\frac{C}{\\ln x}$ 代入该项得到：\n$$\n\\exp\\left(-\\frac{B}{T_x}\\right) = \\exp\\left(-\\frac{B}{C/\\ln x}\\right) = \\exp\\left(-\\frac{B}{C}\\ln x\\right) = \\exp\\left(\\ln x^{-B/C}\\right) = x^{-B/C}\n$$\n该级数变为 $\\sum_{k=k_0}^{\\infty} k^{-B/C}$，这是一个指数为 $p = B/C$ 的 p-级数。一个 p-级数发散当且仅当其指数 $p \\le 1$。因此，为了使期望穿越次数为无穷大，我们需要：\n$$\n\\frac{B}{C} \\le 1 \\implies C \\ge B\n$$\n问题要求找到确保这种发散的“渐近最小方案”。这对应于仍然满足条件的最快冷却（最小的 $T_k$）。较小的 $T_k$ 对应于较小的常数 $C$。满足 $C \\ge B$ 的 $C$ 的最小值为 $C = B$。\n\n这给出了边界情况，或称临界冷却方案：\n$$\nT_k = \\frac{B}{\\ln k}\n$$\n使用第一部分计算出的值 $B=1$，具体的渐近方案是：\n$$\nT_k = \\frac{1}{\\ln k}\n$$\n这个方案是保证期望穿越次数发散的临界方案。任何比这冷却得更慢的方案（即，渐近地 $T_k' > T_k$）也能确保发散，而任何更快的方案则不能。因此，这是满足问题要求的“渐近最小方案”。",
            "answer": "$$\n\\boxed{\\frac{1}{\\ln(k)}}\n$$"
        },
        {
            "introduction": "超越了简单的逃逸机制，更先进的全局优化方法能够学习并适应目标函数的地形，从而实现更高效的搜索。本练习聚焦于当前最先进的优化器之一——协方差矩阵自适应演化策略（CMA-ES）。通过在一个简化的、但能抓住问题本质的场景下分析其更新规则，我们将精确地看到该算法是如何根据成功的搜索方向来调整其搜索分布的形状与方向（即协方差矩阵 $\\mathbf{C}_k$），从而在复杂的高维问题上加速收敛 。",
            "id": "3600595",
            "problem": "在建模为失配泛函黑箱最小化的全局地震波形反演中，一种常用的全局随机优化器是协方差矩阵自适应演化策略 (CMA-ES)。考虑一个简化情景，其中在第 $k$ 次迭代时的搜索分布是一个均值为 $\\mathbf{m}_{k} \\in \\mathbb{R}^{2}$、协方差为 $\\mathbf{C}_{k} \\in \\mathbb{R}^{2 \\times 2}$ 的高斯分布。协方差自适应结合了基于演化路径 $\\mathbf{p}_{c,k}$ 的秩一更新，以及基于所选步长 $\\{\\mathbf{y}_{i,k}\\}_{i=1}^{\\mu}$（其正权重 $\\{w_{i}\\}_{i=1}^{\\mu}$ 之和为1）的加权样本协方差的秩-$\\mu$ 更新。从协方差作为外积期望和指数移动平均作为凸组合的基本定义出发，推导当存在单一主导搜索方向时，秩一更新和秩-$\\mu$ 更新的相互作用是如何沿着其特征基修改 $\\mathbf{C}_{k}$ 的。\n\n假设以下科学上真实且内部一致的情景，该情景捕捉了由数据失配景观导致的主导敏感性方向：\n- 当前协方差 $\\mathbf{C}_{k}$ 具有标准正交特征向量 $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ 和对应的特征值 $\\lambda_{1}$ 和 $\\lambda_{2}$，因此 $\\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$。\n- 在第 $k$ 次迭代时，所选步长的加权二阶矩集中在单位向量 $\\mathbf{u} = \\mathbf{e}_{1}$ 上，即 $\\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top}$，其中 $\\alpha  0$。\n- 演化路径与 $\\mathbf{u}$ 对齐，即 $\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u}$。\n\n设秩一和秩-$\\mu$分量的学习率分别为 $c_1$ 和 $c_\\mu$，其中 $c_1 > 0$, $c_\\mu > 0$ 且 $c_1 + c_\\mu  1$。通过从凸组合和协方差作为外积平均值的定义进行推导，在上述主导方向假设下，用 $\\lambda_{1}$、$\\lambda_{2}$、$c_{1}$、$c_{\\mu}$、$\\|\\mathbf{p}_{c,k}\\|^{2}$ 和 $\\alpha$ 来表示下一个协方差 $\\mathbf{C}_{k+1}$ 的更新后特征值 $\\lambda_{1}^{\\prime}$ 和 $\\lambda_{2}^{\\prime}$。\n\n然后，对于具体参数值 $c_{1} = \\tfrac{1}{5}$、$c_{\\mu} = \\tfrac{3}{10}$、$\\lambda_{1} = 2$、$\\lambda_{2} = 1$、$\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$ 和 $\\alpha = \\tfrac{5}{2}$，计算各向异性比 $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$ 的精确值。你的最终结果应为一个单一的精确值，不进行四舍五入且不带单位。",
            "solution": "用户希望我解决一个关于协方差矩阵自适应演化策略 (CMA-ES) 的问题。\n\n### 问题验证\n\n我将首先验证问题陈述。\n\n**步骤1：提取已知条件**\n- 第 $k$ 次迭代的搜索分布是均值为 $\\mathbf{m}_{k} \\in \\mathbb{R}^{2}$、协方差为 $\\mathbf{C}_{k} \\in \\mathbb{R}^{2 \\times 2}$ 的高斯分布。\n- 协方差自适应结合了秩一更新（来自演化路径 $\\mathbf{p}_{c,k}$）和秩-$\\mu$ 更新（来自所选步长 $\\{\\mathbf{y}_{i,k}\\}_{i=1}^{\\mu}$）。\n- 权重 $\\{w_{i}\\}_{i=1}^{\\mu}$ 是正数且总和为 1。\n- 当前协方差矩阵具有谱分解 $\\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$，其中 $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ 是标准正交特征向量，$\\lambda_{1}, \\lambda_{2}$ 是特征值。\n- 所选步长的加权二阶矩由 $\\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top}$ 给出，其中 $\\alpha  0$。\n- 假设了一个主导搜索方向：$\\mathbf{u} = \\mathbf{e}_{1}$。\n- 演化路径与该主导方向对齐：$\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u}$。\n- 秩一和秩-$\\mu$ 更新的学习率分别为 $c_{1}$ 和 $c_{\\mu}$，且 $c_{1} > 0$，$c_{\\mu} > 0$，以及 $c_{1} + c_{\\mu}  1$。\n- 给定了具体的参数值：$c_{1} = \\tfrac{1}{5}$、$c_{\\mu} = \\tfrac{3}{10}$、$\\lambda_{1} = 2$、$\\lambda_{2} = 1$、$\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$ 和 $\\alpha = \\tfrac{5}{2}$。\n- 任务是推导新协方差矩阵 $\\mathbf{C}_{k+1}$ 的更新后特征值 $\\lambda_{1}^{\\prime}$、$\\lambda_{2}^{\\prime}$，然后为给定参数计算各向异性比 $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学性**：该问题描述了 CMA-ES 算法（一种先进的随机优化方法）的核心机制。其更新规则及其组成部分（秩一和秩-$\\mu$ 更新）是该算法的标准部分。所假设的主导搜索方向情景是一种常用的简化方法，用于分析研究该算法的行为，特别是其使协方差矩阵适应目标函数局部拓扑结构的能力。这是一个合理且相关的科学设定。\n- **适定性**：问题陈述清晰。所提供的信息充分且一致，能够得出一个关于更新后特征值及其比率的唯一解。\n- **客观性**：问题以精确、客观的数学语言陈述。\n- **其他缺陷**：该问题没有说明中列出的导致无效的缺陷。它是可形式化的，主题相关（因为它明确地关于一个全局随机优化方法），完整、一致，并且在其数学背景下是可行的。这是一个非平凡的问题，考验了对 CMA-ES 更新机制的理解。\n\n**步骤3：结论和行动**\n- 问题是**有效的**。我将继续进行完整解答。\n\n### 解题推导\n\n该问题要求在特定假设下，推导 CMA-ES 框架中协方差矩阵 $\\mathbf{C}_{k}$ 更新后的特征值。协方差矩阵从迭代 $k$ 到 $k+1$ 的更新规则是 CMA-ES 的基石。它被表述为指数移动平均（一种凸组合形式），并融合了搜索步长的信息。\n\n协方差矩阵 $\\mathbf{C}_{k+1}$ 的通用更新方程为：\n$$ \\mathbf{C}_{k+1} = (1 - c_{1} - c_{\\mu}) \\mathbf{C}_{k} + c_{1} \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top} + c_{\\mu} \\sum_{i=1}^{\\mu} w_{i} \\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} $$\n此方程与问题的描述一致。项 $(1 - c_{1} - c_{\\mu}) \\mathbf{C}_{k}$ 是“记忆”或“遗忘”部分，保留了前一个协方差矩阵的一部分。项 $c_{1} \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top}$ 是秩一更新，它包含了来自演化路径 $\\mathbf{p}_{c,k}$（搜索方向的长期记忆）的信息。项 $c_{\\mu} \\sum_{i=1}^{\\mu} w_{i} \\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top}$ 是秩-$\\mu$ 更新，它是当前代中最成功的搜索步长外积的加权平均。该项可被解释为成功步长协方差的基于样本的估计，直接关联到协方差作为外积期望的定义。条件 $c_1 > 0, c_\\mu > 0, c_1 + c_\\mu  1$ 确保这是一个凸组合，其中如果更新项是半正定的（它们作为外积，通过构造是半正定的），则新矩阵 $\\mathbf{C}_{k+1}$ 保持半正定。\n\n现在我们将给定的假设代入这个通用方程。\n初始协方差矩阵是：\n$$ \\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top} $$\n秩-$\\mu$ 更新项被给定为：\n$$ \\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top} $$\n根据主导方向 $\\mathbf{u} = \\mathbf{e}_{1}$，这变为：\n$$ \\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} $$\n演化路径与同一方向对齐：$\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{e}_{1}$。因此，秩一更新项为：\n$$ \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top} = (\\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{e}_{1}) (\\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{e}_{1})^{\\top} = \\|\\mathbf{p}_{c,k}\\|^{2} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} $$\n现在，我们将这些表达式代入 $\\mathbf{C}_{k+1}$ 的更新方程中：\n$$ \\mathbf{C}_{k+1} = (1 - c_{1} - c_{\\mu}) (\\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}) + c_{1} (\\|\\mathbf{p}_{c,k}\\|^{2} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}) + c_{\\mu} (\\alpha \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}) $$\n我们可以根据标准正交投影算子 $\\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}$ 和 $\\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$ 对各项进行分组：\n$$ \\mathbf{C}_{k+1} = \\left[ (1 - c_{1} - c_{\\mu})\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha \\right] \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\left[ (1 - c_{1} - c_{\\mu})\\lambda_{2} \\right] \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top} $$\n这个表达式是新协方差矩阵 $\\mathbf{C}_{k+1}$ 的谱分解。在这些特定的简化假设下，特征向量 $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ 保持不变。投影算子的系数就是新的特征值 $\\lambda_{1}^{\\prime}$ 和 $\\lambda_{2}^{\\prime}$。\n\n因此，更新后的特征值为：\n$$ \\lambda_{1}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha $$\n$$ \\lambda_{2}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{2} $$\n这个结果显示了协方差矩阵是如何沿着其特征基被修改的。与主导方向 $\\mathbf{e}_{1}$ 相关联的特征值 $\\lambda_{1}^{\\prime}$ 因演化路径和成功步长的贡献而增加。与正交方向 $\\mathbf{e}_{2}$ 相关联的特征值 $\\lambda_{2}^{\\prime}$ 仅被“遗忘因子” $(1 - c_{1} - c_{\\mu})$ 缩小。这种机制使得 CMA-ES 能够沿着有进展的方向拉长搜索分布。\n\n现在，我们代入给定的数值来计算各向异性比 $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$。\n参数为：$c_{1} = \\tfrac{1}{5}$、$c_{\\mu} = \\tfrac{3}{10}$、$\\lambda_{1} = 2$、$\\lambda_{2} = 1$、$\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$ 和 $\\alpha = \\tfrac{5}{2}$。\n\n首先，我们计算遗忘因子：\n$$ 1 - c_{1} - c_{\\mu} = 1 - \\frac{1}{5} - \\frac{3}{10} = \\frac{10}{10} - \\frac{2}{10} - \\frac{3}{10} = \\frac{5}{10} = \\frac{1}{2} $$\n接下来，我们计算新的特征值 $\\lambda_{1}^{\\prime}$：\n$$ \\lambda_{1}^{\\prime} = \\left(\\frac{1}{2}\\right)\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha $$\n$$ \\lambda_{1}^{\\prime} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{3}{10}\\right)\\left(\\frac{5}{2}\\right) $$\n$$ \\lambda_{1}^{\\prime} = 1 + \\frac{3}{10} + \\frac{15}{20} = 1 + \\frac{3}{10} + \\frac{3}{4} $$\n为了将这些分数相加，我们找到一个公分母，即 20：\n$$ \\lambda_{1}^{\\prime} = \\frac{20}{20} + \\frac{6}{20} + \\frac{15}{20} = \\frac{20+6+15}{20} = \\frac{41}{20} $$\n接下来，我们计算新的特征值 $\\lambda_{2}^{\\prime}$：\n$$ \\lambda_{2}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{2} = \\left(\\frac{1}{2}\\right)(1) = \\frac{1}{2} $$\n最后，我们计算各向异性比 $r$：\n$$ r = \\frac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}} = \\frac{41/20}{1/2} = \\frac{41}{20} \\times \\frac{2}{1} = \\frac{41}{10} $$\n分析表明，各向异性在单步内显著增加，从 $\\lambda_1/\\lambda_2 = 2/1 = 2$ 增加到 $\\lambda_1'/\\lambda_2' = 41/10 = 4.1$，这展示了搜索分布的快速自适应能力。",
            "answer": "$$\\boxed{\\frac{41}{10}}$$"
        }
    ]
}