{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental challenge in global optimization is avoiding entrapment in suboptimal local minima. This hands-on coding exercise provides a concrete demonstration of this very issue by pitting a simple global method (random sampling) against a classic local optimizer (gradient descent). By implementing and comparing these two strategies on a multimodal test function, you will gain a practical understanding of why sophisticated global search methods are indispensable for the complex, non-convex landscapes encountered in geophysical inversion .",
            "id": "3600613",
            "problem": "You are to implement and compare two global stochastic optimization strategies on a one-dimensional, non-convex objective commonly used in computational geophysics as a toy model for basin entrapment. Consider the scalar objective defined on the bounded domain $$\\mathcal{X}=[-4,4]$$ by $$f(x)=\\sin(5x)+0.1\\,x^2,$$ where all angles are in radians and $x$ is dimensionless. Your goal is to approximate a global minimizer and to illustrate the difference between a stochastic global search and a local, single-start gradient descent method.\n\nBegin from the following fundamental bases:\n- The global minimization problem is to compute $$\\min_{x\\in\\mathcal{X}} f(x).$$\n- Independent and identically distributed uniform sampling on an interval uses a pseudorandom generator to draw $$x_i\\sim \\mathrm{Uniform}([-4,4])$$ and selects the best sample according to the objective.\n- In one dimension, gradient descent with a constant step size uses the update $$x_{k+1}=\\Pi_{\\mathcal{X}}\\bigl(x_k-\\alpha\\,f'(x_k)\\bigr),$$ where $$\\Pi_{\\mathcal{X}}$$ denotes the Euclidean projection onto the interval $$\\mathcal{X}$$ (that is, clamping to $$[-4,4]$$), $$\\alpha>0$$ is a fixed step size, and $$f'(x)$$ is the first derivative computed using the chain rule and linearity of differentiation.\n\nTasks to implement:\n1) Use the definition of the derivative and the chain rule to derive and implement $$f'(x)$$ for the given $$f(x)$$.\n2) Implement a stochastic global search that draws $$n$$ independent uniform samples on $$[-4,4]$$ using a pseudorandom number generator with a specified integer seed $$s$$, evaluates $$f(x)$$ on each sample, and returns the sample location with minimal objective value together with that value.\n3) Implement single-start projected gradient descent with a given initial point $$x_0\\in[-4,4]$$, constant step size $$\\alpha>0$$, and a fixed number of iterations $$K\\in\\mathbb{N}$$. After each update, project the iterate back to $$[-4,4]$$ via $$\\Pi_{\\mathcal{X}}(x)=\\min(\\max(x,-4),4)$$.\n\nComparison metric and decision rule:\n- For each test case, compute the approximate best value from random sampling, $$f_{\\mathrm{rand}}:=\\min_{i=1,\\dots,n} f(x_i),$$ at its corresponding point $$x_{\\mathrm{rand}}$$, and the final value after gradient descent, $$f_{\\mathrm{gd}}:=f(x_K),$$ at its final point $$x_{\\mathrm{gd}}$$.\n- Define $$\\Delta:=f_{\\mathrm{gd}}-f_{\\mathrm{rand}}.$$ Declare that gradient descent is trapped in a suboptimal basin if $$\\Delta>\\tau,$$ where $$\\tau=10^{-3}$$.\n\nNumerical and output requirements:\n- Use radians for all trigonometric evaluations. There are no physical units.\n- For each test case, return a list $$[x_{\\mathrm{rand}}, f_{\\mathrm{rand}}, x_{\\mathrm{gd}}, f_{\\mathrm{gd}}, \\text{trapped}]$$, where the first four entries are floats rounded to six decimal places, and the last entry is a boolean computed using the threshold $$\\tau=10^{-3}$$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the lists from all test cases in order, for example $$[r_1,r_2,\\dots,r_m]$$.\n\nTest suite to implement and evaluate:\n- Case A (baseline success near the best basin): $$s=123$$, $$n=500$$, $$x_0=0.0$$, $$\\alpha=0.05$$, $$K=200$$.\n- Case B (entrapment in a distant local basin): $$s=7$$, $$n=2000$$, $$x_0=2.8$$, $$\\alpha=0.05$$, $$K=200$$.\n- Case C (undersampling stress test): $$s=42$$, $$n=20$$, $$x_0=-3.5$$, $$\\alpha=0.02$$, $$K=150$$.\n- Case D (zero-iteration gradient descent boundary): $$s=0$$, $$n=3000$$, $$x_0=1.0$$, $$\\alpha=0.05$$, $$K=0$$.\n\nYour program must implement the above logic exactly and print a single line of the aggregated results in the format $$[[x_{\\mathrm{rand}}^{(A)},f_{\\mathrm{rand}}^{(A)},x_{\\mathrm{gd}}^{(A)},f_{\\mathrm{gd}}^{(A)},\\mathrm{trapped}^{(A)}],\\dots,[x_{\\mathrm{rand}}^{(D)},f_{\\mathrm{rand}}^{(D)},x_{\\mathrm{gd}}^{(D)},f_{\\mathrm{gd}}^{(D)},\\mathrm{trapped}^{(D)}]].$$",
            "solution": "The problem is valid as it presents a well-defined, self-contained, and scientifically sound computational task in the field of numerical optimization. It requires the implementation and comparison of two standard optimization algorithms, stochastic global search and gradient descent, on a specified non-convex function. All parameters, conditions, and evaluation metrics are provided, ensuring the problem is objective and allows for a unique, verifiable solution.\n\nThe core of the problem is to compare a global optimization strategy with a local one on a multimodal objective function. The chosen function, $$f(x)=\\sin(5x)+0.1\\,x^2$$, defined on the compact domain $$\\mathcal{X}=[-4,4]$$, is non-convex. This means it possesses multiple local minima, making it a suitable test case to illustrate the phenomenon of a local search method getting \"trapped\" in a suboptimal basin of attraction, while a global search has a higher probability of identifying the global minimizer.\n\nThe methodological steps are as follows: first, we derive the analytical form of the function's derivative, which is required for the gradient descent algorithm. Second, we formalize the procedures for both the stochastic search and the projected gradient descent. Finally, we apply these algorithms to the specified test cases and use the defined comparison metric to evaluate their performance.\n\n1.  **Objective Function and its Derivative**\nThe objective function is given as:\n$$f(x)=\\sin(5x)+0.1\\,x^2$$\nTo implement gradient descent, we must compute its first derivative, $$f'(x)$$. We apply the principle of linearity of differentiation, which states that the derivative of a sum is the sum of the derivatives, and the chain rule for the trigonometric term.\nThe derivative of the first term, $$\\sin(5x)$$, with respect to $$x$$ is found using the chain rule, $$d/dx(\\sin(u)) = \\cos(u) \\cdot du/dx$$. Here, $$u=5x$$, so $$du/dx=5$$.\n$$\\frac{d}{dx}\\left(\\sin(5x)\\right) = \\cos(5x) \\cdot 5 = 5\\cos(5x)$$\nThe derivative of the second term, $$0.1x^2$$, is found using the power rule:\n$$\\frac{d}{dx}\\left(0.1x^2\\right) = 0.1 \\cdot 2x = 0.2x$$\nCombining these results gives the full derivative:\n$$f'(x) = 5\\cos(5x) + 0.2x$$\nAll trigonometric calculations are performed in radians, as specified.\n\n2.  **Stochastic Global Search: Uniform Random Sampling**\nThis method performs a global exploration of the search space $$\\mathcal{X}=[-4,4]$$. Its principle is based on probabilistic coverage. By drawing a sufficiently large number, $$n$$, of independent and identically distributed samples $$x_i \\sim \\mathrm{Uniform}([-4,4])$$, the algorithm is likely to place at least one sample within the basin of attraction of the global minimum. This method is \"blind\" to the local structure of the function (like its gradient) and relies purely on function evaluations at random points. The algorithm is:\n-   Initialize a pseudorandom number generator with a specified integer seed, $$s$$, to ensure reproducibility.\n-   Generate $$n$$ random samples $$\\{x_1, x_2, \\dots, x_n\\}$$ uniformly from the interval $$[-4,4]$$.\n-   Evaluate $$f(x_i)$$ for each sample $$x_i$$.\n-   Determine the sample $$x_{\\mathrm{rand}}$$ that yields the minimum function value, $$f_{\\mathrm{rand}} = \\min_{i=1,\\dots,n} f(x_i)$$.\n\n3.  **Local Search: Projected Gradient Descent**\nThis is a first-order iterative optimization algorithm designed to find a local minimum. It starts at an initial point $$x_0$$ and iteratively moves in the direction of the negative gradient, which is the direction of steepest local descent.\nThe iterative update rule is:\n$$x_{k+1} = x_k - \\alpha f'(x_k)$$\nwhere $$k$$ is the iteration number, $$x_k$$ is the current point, $$f'(x_k)$$ is the gradient at that point, and $$\\alpha>0$$ is a constant step size that controls the magnitude of each step.\nSince the problem is constrained to the domain $$\\mathcal{X}=[-4,4]$$, we must ensure that each new iterate $$x_{k+1}$$ remains within this domain. This is achieved by applying a Euclidean projection operator, $$\\Pi_{\\mathcal{X}}$$, after each update. For a one-dimensional interval, this projection is equivalent to clamping the value:\n$$\\Pi_{\\mathcal{X}}(z) = \\min(\\max(z,-4),4)$$\nThe complete algorithm is:\n-   Initialize the iterate $$x$$ with the starting point $$x_0$$.\n-   For a fixed number of iterations, $$K$$:\n    1.  Compute the gradient $$g_k = f'(x_k)$$.\n    2.  Perform the descent step: $$x_{\\text{temp}} = x_k - \\alpha g_k$$.\n    3.  Project the result back into the domain: $$x_{k+1} = \\Pi_{\\mathcal{X}}(x_{\\text{temp}})$$.\n-   The final iterate, $$x_{\\mathrm{gd}} = x_K$$, and its corresponding function value, $$f_{\\mathrm{gd}} = f(x_K)$$, are the results of the algorithm.\n\n4.  **Comparison and Decision Rule**\nThe purpose of the comparison is to determine if the local gradient descent method has become \"trapped\" in a local minimum that is significantly worse than the best minimum found by the global random search. The performance difference is quantified by $$\\Delta = f_{\\mathrm{gd}} - f_{\\mathrm{rand}}$$.\nA positive $$\\Delta$$ indicates that the random search found a better solution (a lower minimum) than gradient descent. The problem defines a specific criterion for entrapment: gradient descent is considered trapped if this difference exceeds a given threshold, $$\\tau=10^{-3}$$.\nThe decision rule is:\n$$\\text{trapped} = (\\Delta > \\tau)$$\nThis boolean result, along with the coordinates and function values from both methods, provides a complete picture for each test case.\nFor Case D, where the number of iterations $$K=0$$, the gradient descent procedure terminates immediately. Thus, its final point $$x_{\\mathrm{gd}}$$ is simply its initial point $$x_0$$, and $$f_{\\mathrm{gd}} = f(x_0)$$. This case effectively compares a single, predetermined point against a large-scale random search.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares stochastic global search and projected gradient descent\n    on a nonconvex objective function for several test cases.\n    \"\"\"\n\n    # Define the objective function and its derivative.\n    # f(x) = sin(5x) + 0.1 * x^2\n    def f(x: np.ndarray | float) -> np.ndarray | float:\n        return np.sin(5 * x) + 0.1 * x**2\n\n    # f'(x) = 5*cos(5x) + 0.2*x\n    def df(x: np.ndarray | float) -> np.ndarray | float:\n        return 5 * np.cos(5 * x) + 0.2 * x\n\n    def stochastic_search(n: int, s: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs a stochastic global search using uniform random sampling.\n\n        Args:\n            n: Number of samples.\n            s: Seed for the pseudorandom number generator.\n            domain: The search interval (min, max).\n\n        Returns:\n            A tuple (x_rand, f_rand) containing the location and value of the\n            best sample found.\n        \"\"\"\n        rng = np.random.default_rng(s)\n        samples = rng.uniform(domain[0], domain[1], n)\n        values = f(samples)\n        min_index = np.argmin(values)\n        x_rand = samples[min_index]\n        f_rand = values[min_index]\n        return x_rand, f_rand\n\n    def gradient_descent(x0: float, alpha: float, K: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs projected gradient descent.\n\n        Args:\n            x0: Initial point.\n            alpha: Constant step size.\n            K: Number of iterations.\n            domain: The search interval (min, max) for projection.\n\n        Returns:\n            A tuple (x_gd, f_gd) containing the final location and value.\n        \"\"\"\n        x_k = x0\n        for _ in range(K):\n            grad = df(x_k)\n            x_k = x_k - alpha * grad\n            # Project back onto the domain (clamping)\n            x_k = np.clip(x_k, domain[0], domain[1])\n        \n        x_gd = x_k\n        f_gd = f(x_gd)\n        return x_gd, f_gd\n\n    # Define common parameters and test cases.\n    domain = (-4.0, 4.0)\n    tau = 1e-3\n\n    test_cases = [\n        # Case A (baseline success near the best basin)\n        {'s': 123, 'n': 500, 'x0': 0.0, 'alpha': 0.05, 'K': 200},\n        # Case B (entrapment in a distant local basin)\n        {'s': 7, 'n': 2000, 'x0': 2.8, 'alpha': 0.05, 'K': 200},\n        # Case C (undersampling stress test)\n        {'s': 42, 'n': 20, 'x0': -3.5, 'alpha': 0.02, 'K': 150},\n        # Case D (zero-iteration gradient descent boundary)\n        {'s': 0, 'n': 3000, 'x0': 1.0, 'alpha': 0.05, 'K': 0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Run stochastic search\n        x_rand, f_rand = stochastic_search(case['n'], case['s'], domain)\n\n        # Run gradient descent\n        x_gd, f_gd = gradient_descent(case['x0'], case['alpha'], case['K'], domain)\n\n        # Compare results and determine if trapped\n        delta = f_gd - f_rand\n        trapped = delta > tau\n\n        # Format the result list for this case\n        result_list = [\n            round(float(x_rand), 6),\n            round(float(f_rand), 6),\n            round(float(x_gd), 6),\n            round(float(f_gd), 6),\n            trapped\n        ]\n        all_results.append(result_list)\n    \n    # Print the aggregated results in the specified format\n    # The format is a string representation of a Python list of lists.\n    # e.g., [[-1.427..., -0.893..., ...], [...]]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having established the reality of local minima, we now explore the theoretical underpinnings of a classic algorithm designed to escape them: Simulated Annealing (SA). This exercise uses a canonical double-well potential, a simple yet powerful analogue for geophysical problems with two competing model explanations. By deriving the minimal cooling schedule required for the algorithm to cross the energy barrier, you will uncover the mathematical principle that gives SA its global exploration power .",
            "id": "3600630",
            "problem": "A canonical toy misfit in computational geophysics exhibits two plausible model explanations separated by an energetic barrier. Consider the scalar objective modeled by a double-well energy\n$$\nf(x)=x^{4}-2x^{2},\n$$\nwhich has two basins of attraction corresponding to two competing geophysical models. You apply Metropolis simulated annealing, a Markov Chain Monte Carlo (MCMC) method, with a decreasing temperature sequence $\\{T_{k}\\}_{k\\geq 1}$, and at each iteration $k$ use a symmetric local proposal that with probability bounded below by a constant $q\\in(0,1]$ proposes a move that attempts to ascend from the current well to the barrier top before possibly descending into the other well. The Metropolis acceptance probability for a proposed move that increases the objective by $\\Delta f$ at temperature $T$ is\n$$\n\\alpha(\\Delta f, T)=\\exp\\!\\left(-\\frac{\\Delta f}{T}\\right).\n$$\n\n1) Compute the barrier height $B$ of $f(x)$, defined as the difference between the energy at the saddle (the barrier top) and the energy at either minimum.\n\n2) Let $N(n)$ denote the expected number of accepted barrier-ascending moves (i.e., accepted moves that surmount the barrier top at least once) in the first $n$ iterations. Using only the definition of the Metropolis acceptance probability and standard comparison tests for series, derive a condition on the cooling schedule $\\{T_{k}\\}$ under which $\\lim_{n\\to\\infty}\\mathbb{E}[N(n)]$ does not converge to a finite limit. In other words, find the asymptotically minimal schedule that ensures a nonzero (in the sense of being unbounded with $n$) expected cumulative count of barrier-crossing events as $k\\to\\infty$.\n\nExpress your final answer as a single closed-form asymptotic expression $T_{k}$ in terms of $k$ for the specific $f(x)$ above. No rounding is required.",
            "solution": "The analysis of the problem is performed in two parts as requested. First, the barrier height of the potential energy function is computed. Second, the condition on the cooling schedule for the divergence of the expected number of barrier crossings is derived.\n\nPart 1: Computation of the Barrier Height $B$\n\nThe objective function is given by $f(x) = x^{4} - 2x^{2}$. To find the minima and the saddle point, we must find the critical points by setting the first derivative of $f(x)$ with respect to $x$ to zero.\n\nThe first derivative is:\n$$\nf'(x) = \\frac{d}{dx}(x^{4} - 2x^{2}) = 4x^{3} - 4x\n$$\nSetting $f'(x) = 0$ to find the critical points:\n$$\n4x^{3} - 4x = 0\n$$\n$$\n4x(x^{2} - 1) = 0\n$$\n$$\n4x(x - 1)(x + 1) = 0\n$$\nThe critical points are $x=0$, $x=1$, and $x=-1$.\n\nTo classify these critical points, we use the second derivative test. The second derivative of $f(x)$ is:\n$$\nf''(x) = \\frac{d}{dx}(4x^{3} - 4x) = 12x^{2} - 4\n$$\nWe evaluate $f''(x)$ at each critical point:\n- For $x=0$: $f''(0) = 12(0)^{2} - 4 = -4$. Since $f''(0) < 0$, the point $x=0$ is a local maximum, which corresponds to the top of the energy barrier (a saddle point in higher dimensions).\n- For $x=1$: $f''(1) = 12(1)^{2} - 4 = 8$. Since $f''(1) > 0$, the point $x=1$ is a local minimum.\n- For $x=-1$: $f''(-1) = 12(-1)^{2} - 4 = 8$. Since $f''(-1) > 0$, the point $x=-1$ is also a local minimum.\n\nThese two minima correspond to the two basins of attraction. Now, we evaluate the energy $f(x)$ at these points:\n- Energy at the saddle point: $f_{\\text{saddle}} = f(0) = 0^{4} - 2(0)^{2} = 0$.\n- Energy at the minima: $f_{\\text{min}} = f(1) = 1^{4} - 2(1)^{2} = 1 - 2 = -1$. And $f(-1) = (-1)^{4} - 2(-1)^{2} = 1 - 2 = -1$.\n\nThe barrier height $B$ is defined as the difference between the energy at the saddle and the energy at either minimum.\n$$\nB = f_{\\text{saddle}} - f_{\\text{min}} = 0 - (-1) = 1\n$$\nThus, the barrier height is $B=1$.\n\nPart 2: Derivation of the Cooling Schedule\n\nLet $A_{k}$ be the event that a barrier-ascending move is proposed and accepted at iteration $k$. The expected number of such events in the first $n$ iterations is given by the linearity of expectation:\n$$\n\\mathbb{E}[N(n)] = \\mathbb{E}\\left[\\sum_{k=1}^{n} I(A_{k})\\right] = \\sum_{k=1}^{n} \\mathbb{E}[I(A_{k})] = \\sum_{k=1}^{n} P(A_{k})\n$$\nwhere $I(A_{k})$ is the indicator function for the event $A_{k}$.\n\nThe problem requires finding a condition on the cooling schedule $\\{T_{k}\\}$ such that $\\lim_{n\\to\\infty} \\mathbb{E}[N(n)]$ does not converge to a finite limit. This is equivalent to the condition that the series of probabilities diverges:\n$$\n\\sum_{k=1}^{\\infty} P(A_{k}) = \\infty\n$$\nThe probability of event $A_k$ is the product of the probability of proposing a barrier-ascending move and the probability of accepting it. The problem states that a move attempting to ascend the barrier is proposed with a probability $p_k$ bounded below by a constant $q > 0$. Such a move, starting from near a minimum, must increase its energy by at least the barrier height $B$ to reach the saddle point. We consider the minimal such increase, $\\Delta f = B$. The Metropolis acceptance probability for this move at temperature $T_k$ is $\\alpha(B, T_k) = \\exp(-B/T_k)$.\n\nThe probability of proposing and accepting such a move at step $k$ is therefore bounded below:\n$$\nP(A_k) \\ge q \\exp\\left(-\\frac{B}{T_k}\\right)\n$$\nBy the comparison test for series, if the series formed by the lower bound diverges, so does the original series. Since $q$ is a positive constant, we need to find the condition on $\\{T_k\\}$ that ensures:\n$$\n\\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{B}{T_k}\\right) = \\infty\n$$\nThis is a classical condition in the theory of simulated annealing for guaranteeing convergence to the global minimum, as it ensures an infinite number of expected escapes from any local minimum.\n\nTo find the asymptotic form of $T_k$ that satisfies this, we can use the integral test for convergence. The sequence of terms $a_k = \\exp(-B/T_k)$ is positive. Since $\\{T_k\\}_{k\\ge 1}$ is a decreasing sequence approaching $0$, $1/T_k$ is an increasing sequence, $-B/T_k$ is a decreasing sequence, and thus $a_k = \\exp(-B/T_k)$ is a decreasing sequence. The integral test is applicable. The divergence of the sum is equivalent to the divergence of the corresponding integral:\n$$\n\\int_{1}^{\\infty} \\exp\\left(-\\frac{B}{T_x}\\right) dx = \\infty\n$$\nA standard choice for the cooling schedule at the critical boundary of convergence is of the form $T_k = \\frac{C}{\\ln k}$ for some constant $C$ and large $k$. Let us analyze this form. Substituting $T_x = \\frac{C}{\\ln x}$ into the term gives:\n$$\n\\exp\\left(-\\frac{B}{T_x}\\right) = \\exp\\left(-\\frac{B}{C/\\ln x}\\right) = \\exp\\left(-\\frac{B}{C}\\ln x\\right) = \\exp\\left(\\ln x^{-B/C}\\right) = x^{-B/C}\n$$\nThe series becomes $\\sum_{k=k_0}^{\\infty} k^{-B/C}$, which is a p-series with exponent $p = B/C$. A p-series diverges if and only if its exponent $p \\le 1$. Therefore, for the expected number of crossings to be infinite, we require:\n$$\n\\frac{B}{C} \\le 1 \\implies C \\ge B\n$$\nThe problem asks for the \"asymptotically minimal schedule\" that ensures this divergence. This corresponds to the fastest cooling (smallest $T_k$) that still satisfies the condition. A smaller $T_k$ corresponds to a smaller constant $C$. The minimal value of $C$ that satisfies $C \\ge B$ is $C = B$.\n\nThis gives the boundary case, or the critical cooling schedule:\n$$\nT_k = \\frac{B}{\\ln k}\n$$\nUsing the value $B=1$ calculated in Part 1, the specific asymptotic schedule is:\n$$\nT_k = \\frac{1}{\\ln k}\n$$\nThis schedule represents the slowest rate of cooling of the form $C/\\ln(k)$ that fails to guarantee an infinite number of expected crossings from any local minimum. Any schedule that cools slower than this (i.e., $T_k' \\ge T_k$ asymptotically, meaning $C' \\ge B$) will also satisfy the condition. The question asks for the minimal schedule that ensures a non-zero (unbounded) count, which is this boundary case.",
            "answer": "$$\n\\boxed{\\frac{1}{\\ln(k)}}\n$$"
        },
        {
            "introduction": "Beyond simply escaping local minima, modern optimizers actively learn the structure of the problem landscape to guide the search more efficiently. This practice delves into the mechanics of a state-of-the-art method, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Under the simplifying assumption of a single dominant search direction, you will analytically derive how the algorithm's covariance matrix adapts, effectively elongating the search distribution along promising valleys in the misfit function, revealing the core of its power .",
            "id": "3600595",
            "problem": "In global seismic waveform inversion cast as a black-box minimization of a misfit functional, a common global stochastic optimizer is the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Consider a simplified setting where the search distribution at iteration $k$ is Gaussian with mean $\\mathbf{m}_{k} \\in \\mathbb{R}^{2}$ and covariance $\\mathbf{C}_{k} \\in \\mathbb{R}^{2 \\times 2}$. The covariance adaptation combines a rank-one update based on the evolution path $\\mathbf{p}_{c,k}$ and a rank-$\\mu$ update based on the weighted sample covariance of selected steps $\\{\\mathbf{y}_{i,k}\\}_{i=1}^{\\mu}$ with positive weights $\\{w_{i}\\}_{i=1}^{\\mu}$ summing to $1$. Starting from the foundational definitions of covariance as an expectation of outer products and of an exponential moving average as a convex combination, derive how the interaction of the rank-one and rank-$\\mu$ updates modifies $\\mathbf{C}_{k}$ along its eigenbasis when a single dominant search direction is present.\n\nAssume the following scientifically realistic and internally consistent scenario capturing a dominant sensitivity direction due to the data misfit landscape:\n- The current covariance $\\mathbf{C}_{k}$ has orthonormal eigenvectors $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ with eigenvalues $\\lambda_{1}$ and $\\lambda_{2}$, so that $\\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$.\n- The weighted second-moment of the selected steps at iteration $k$ concentrates along the unit vector $\\mathbf{u} = \\mathbf{e}_{1}$, i.e., $\\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top}$ with $\\alpha > 0$.\n- The evolution path aligns with $\\mathbf{u}$, i.e., $\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u}$.\n\nLet the learning rates for the rank-one and rank-$\\mu$ components be $c_{1}$ and $c_{\\mu}$, respectively, with $c_{1} > 0$, $c_{\\mu} > 0$, and $c_{1} + c_{\\mu} < 1$. By deriving from the definitions of a convex combination and covariance as an average of outer products, express the updated eigenvalues $\\lambda_{1}^{\\prime}$ and $\\lambda_{2}^{\\prime}$ of the next covariance $\\mathbf{C}_{k+1}$ in terms of $\\lambda_{1}$, $\\lambda_{2}$, $c_{1}$, $c_{\\mu}$, $\\|\\mathbf{p}_{c,k}\\|^{2}$, and $\\alpha$ under the dominant direction assumption above.\n\nThen, for the concrete parameter values $c_{1} = \\tfrac{1}{5}$, $c_{\\mu} = \\tfrac{3}{10}$, $\\lambda_{1} = 2$, $\\lambda_{2} = 1$, $\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$, and $\\alpha = \\tfrac{5}{2}$, compute the exact value of the anisotropy ratio $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$. Provide your final result as a single exact value with no rounding and no units.",
            "solution": "The general update equation for the covariance matrix $\\mathbf{C}_{k+1}$ in CMA-ES is:\n$$ \\mathbf{C}_{k+1} = (1 - c_{1} - c_{\\mu}) \\mathbf{C}_{k} + c_{1} \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top} + c_{\\mu} \\sum_{i=1}^{\\mu} w_{i} \\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} $$\nThis equation describes how the covariance is updated as a convex combination of the previous covariance (the \"memory\" term), a rank-one update from the evolution path $\\mathbf{p}_{c,k}$, and a rank-$\\mu$ update which is a weighted estimate of the covariance of successful steps.\n\nWe substitute the problem's assumptions into this equation. The initial covariance is $\\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$. The rank-$\\mu$ update term simplifies to $\\alpha\\, \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}$ and the rank-one update term becomes $\\|\\mathbf{p}_{c,k}\\|^{2} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}$, since the dominant search direction is $\\mathbf{u} = \\mathbf{e}_{1}$.\n\nSubstituting these into the update rule gives:\n$$ \\mathbf{C}_{k+1} = (1 - c_{1} - c_{\\mu}) (\\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}) + c_{1} \\|\\mathbf{p}_{c,k}\\|^{2} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + c_{\\mu} \\alpha \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} $$\nWe group terms by the orthonormal projectors $\\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}$ and $\\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$:\n$$ \\mathbf{C}_{k+1} = \\left[ (1 - c_{1} - c_{\\mu})\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha \\right] \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\left[ (1 - c_{1} - c_{\\mu})\\lambda_{2} \\right] \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top} $$\nThis is the spectral decomposition of the new covariance matrix $\\mathbf{C}_{k+1}$. The coefficients of the projectors are the new eigenvalues. Thus, the updated eigenvalues are:\n$$ \\lambda_{1}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha $$\n$$ \\lambda_{2}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{2} $$\nThis result demonstrates how the eigenvalue associated with the dominant search direction is increased, while the eigenvalue for the orthogonal direction is decreased, thereby elongating the search distribution.\n\nNow, we substitute the numerical values: $c_{1} = \\tfrac{1}{5}$, $c_{\\mu} = \\tfrac{3}{10}$, $\\lambda_{1} = 2$, $\\lambda_{2} = 1$, $\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$, and $\\alpha = \\tfrac{5}{2}$.\n\nThe forgetting factor is:\n$$ 1 - c_{1} - c_{\\mu} = 1 - \\frac{1}{5} - \\frac{3}{10} = \\frac{10 - 2 - 3}{10} = \\frac{5}{10} = \\frac{1}{2} $$\nWe calculate the new eigenvalues:\n$$ \\lambda_{1}^{\\prime} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{3}{10}\\right)\\left(\\frac{5}{2}\\right) = 1 + \\frac{3}{10} + \\frac{15}{20} = 1 + \\frac{6}{20} + \\frac{15}{20} = \\frac{20+6+15}{20} = \\frac{41}{20} $$\n$$ \\lambda_{2}^{\\prime} = \\left(\\frac{1}{2}\\right)(1) = \\frac{1}{2} $$\nFinally, we compute the anisotropy ratio $r$:\n$$ r = \\frac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}} = \\frac{41/20}{1/2} = \\frac{41}{20} \\times 2 = \\frac{41}{10} $$",
            "answer": "$$\\boxed{\\frac{41}{10}}$$"
        }
    ]
}