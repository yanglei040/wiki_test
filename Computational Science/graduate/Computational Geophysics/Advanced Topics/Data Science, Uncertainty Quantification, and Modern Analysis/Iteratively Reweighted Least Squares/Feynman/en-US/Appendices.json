{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of Iteratively Reweighted Least Squares (IRLS), our first practice breaks the process down into its fundamental steps. By working through a single iteration on a small-scale linear system with a significant outlier, you will gain direct experience in calculating residuals, determining weights using the robust Huber loss function, and solving the resulting weighted normal equations. This exercise provides a concrete, hands-on demonstration of how IRLS mechanistically identifies and mitigates the influence of contaminated data points .",
            "id": "3393330",
            "problem": "Consider a linear data assimilation setting with a forward operator represented by a matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and observations $b \\in \\mathbb{R}^{3}$, where the third observation is a large outlier relative to the first two. The goal is to robustly estimate the state $x \\in \\mathbb{R}^{2}$ in the presence of this outlier using one iteration of Iteratively Reweighted Least Squares (IRLS) based on the Huber loss. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}.\n$$\nThe Huber loss with threshold parameter $\\delta>0$ is defined by the piecewise function\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2}, & \\text{if } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2}, & \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nUse $\\delta = 2$. Starting from the definition of $\\rho_{\\delta}(r)$ above and the principle of robust M-estimation, perform one IRLS iteration as follows:\n1. Compute the residuals at $x^{(0)}$, denoted $r^{(0)} \\in \\mathbb{R}^{3}$, using the misfit $r = A x - b$.\n2. Derive the corresponding IRLS weights $w_{i}$ from the Huber influence function associated with $\\rho_{\\delta}(r)$, and assemble the diagonal weight matrix $W = \\operatorname{diag}(w_{1}, w_{2}, w_{3})$.\n3. Form the weighted normal equations $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$ and solve the resulting $2 \\times 2$ linear system to obtain $x^{(1)}$.\n\nReport only the value of the second component $x_{2}^{(1)}$ of the updated estimate. No rounding is required, and no physical units are involved.",
            "solution": "We begin from the robust M-estimation formulation in inverse problems, where the estimate $x \\in \\mathbb{R}^{2}$ minimizes the sum of robust penalties applied to data misfits. For residuals $r_{i} = a_{i}^{\\top} x - b_{i}$, the Huber loss with threshold parameter $\\delta > 0$ is defined by\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2}, & \\text{if } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2}, & \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nThe corresponding influence function (the derivative with respect to $r$) is\n$$\n\\psi_{\\delta}(r) \\;=\\; \\frac{\\mathrm{d}}{\\mathrm{d}r}\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\nr, & \\text{if } |r| \\leq \\delta, \\\\\n\\delta \\,\\operatorname{sign}(r), & \\text{if } |r| > \\delta.\n\\end{cases}\n$$\nIn Iteratively Reweighted Least Squares (IRLS), weights are constructed as $w_{i} = \\frac{\\psi_{\\delta}(r_{i})}{r_{i}}$ for $r_{i} \\neq 0$; this yields\n$$\nw_{i} \\;=\\;\n\\begin{cases}\n1, & \\text{if } |r_{i}| \\leq \\delta, \\\\\n\\frac{\\delta}{|r_{i}|}, & \\text{if } |r_{i}| > \\delta,\n\\end{cases}\n$$\nand $w_{i} = 1$ if $r_{i} = 0$ by continuity.\n\nStep 1: Compute residuals at $x^{(0)}$ using $r = A x - b$. With\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix},\n$$\nwe obtain\n$$\nr^{(0)} \\;=\\; A x^{(0)} - b \\;=\\; \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ -1 \\\\ 20\\end{pmatrix} \\;=\\; \\begin{pmatrix} -1 \\\\ 1 \\\\ -20 \\end{pmatrix}.\n$$\n\nStep 2: Derive IRLS weights from the Huber influence function with $\\delta = 2$. The magnitudes of the residuals are $|r_{1}^{(0)}| = 1$, $|r_{2}^{(0)}| = 1$, and $|r_{3}^{(0)}| = 20$. Hence,\n- For $i = 1$: $|r_{1}^{(0)}| = 1 \\leq \\delta$ implies $w_{1} = 1$.\n- For $i = 2$: $|r_{2}^{(0)}| = 1 \\leq \\delta$ implies $w_{2} = 1$.\n- For $i = 3$: $|r_{3}^{(0)}| = 20 > \\delta$ implies $w_{3} = \\frac{\\delta}{|r_{3}^{(0)}|} = \\frac{2}{20} = 0.1$.\n\nTherefore,\n$$\nW \\;=\\; \\operatorname{diag}(1,\\, 1,\\, 0.1).\n$$\n\nStep 3: Form and solve the weighted normal equations $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$.\n\nFirst compute $A^{\\top} W A$. Denote the rows of $A$ by $a_{1}^{\\top} = (1, 0)$, $a_{2}^{\\top} = (0, 1)$, $a_{3}^{\\top} = (1, 1)$. Then\n$$\nA^{\\top} W A \\;=\\; \\sum_{i=1}^{3} w_{i} \\, a_{i} a_{i}^{\\top}.\n$$\nWe have\n$$\nw_{1} a_{1} a_{1}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1 & 0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix},\n$$\n$$\nw_{2} a_{2} a_{2}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 0 & 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix},\n$$\n$$\nw_{3} a_{3} a_{3}^{\\top} \\;=\\; 0.1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.1 & 0.1 \\\\ 0.1 & 0.1 \\end{pmatrix}.\n$$\nSumming,\n$$\nA^{\\top} W A \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} 0.1 & 0.1 \\\\ 0.1 & 0.1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1.1 & 0.1 \\\\ 0.1 & 1.1 \\end{pmatrix}.\n$$\n\nNext compute $A^{\\top} W b$. Note that $W b = \\begin{pmatrix} 1 \\cdot 1 \\\\ 1 \\cdot (-1) \\\\ 0.1 \\cdot 20 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$. Then\n$$\nA^{\\top} W b \\;=\\; A^{\\top} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 \\cdot 1 + 0 \\cdot (-1) + 1 \\cdot 2 \\\\\n0 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 2\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n\nWe must solve the $2 \\times 2$ linear system\n$$\n\\begin{pmatrix} 1.1 & 0.1 \\\\ 0.1 & 1.1 \\end{pmatrix}\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nThe determinant of the coefficient matrix is\n$$\n\\det \\;=\\; 1.1 \\cdot 1.1 \\;-\\; 0.1 \\cdot 0.1 \\;=\\; 1.21 - 0.01 \\;=\\; 1.20.\n$$\nThe inverse is\n$$\n\\left(\\begin{pmatrix} 1.1 & 0.1 \\\\ 0.1 & 1.1 \\end{pmatrix}\\right)^{-1}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1 & -0.1 \\\\\n-0.1 & 1.1\n\\end{pmatrix}.\n$$\nThus\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1 & -0.1 \\\\\n-0.1 & 1.1\n\\end{pmatrix}\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1 \\cdot 3 + (-0.1) \\cdot 1 \\\\\n(-0.1) \\cdot 3 + 1.1 \\cdot 1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.3 - 0.1 \\\\\n-0.3 + 1.1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.2 \\\\\n0.8\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{3.2}{1.2} \\\\\n\\frac{0.8}{1.2}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{8}{3} \\\\\n\\frac{2}{3}\n\\end{pmatrix}.\n$$\n\nTherefore, the requested value is the second component $x_{2}^{(1)} = \\frac{2}{3}$.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "This practice elevates the concept of IRLS to a more applied, albeit synthetic, geophysical scenario involving traveltime tomography. You will perform an IRLS iteration to invert for subsurface slowness parameters from data containing outliers. The problem is designed with a special structure that simplifies the weighted normal equations, revealing the updated model as an intuitive weighted average of the data, which powerfully illustrates the effect of the weights in a practical context .",
            "id": "3605220",
            "problem": "Consider a synthetic linearized traveltime tomography experiment with forward operator $G \\in \\mathbb{R}^{50 \\times 20}$, model vector $m \\in \\mathbb{R}^{20}$ representing cell slownesses, and data vector $d \\in \\mathbb{R}^{50}$ representing measured traveltimes. Assume the linear observation model $d = G m + \\varepsilon$, where $\\varepsilon$ represents measurement noise and outliers. For this synthetic design, each of the $50$ rays traverses exactly one model cell with unit path length and no other cells, so that each row of $G$ contains a single $1$ and zeros elsewhere. The mapping between rays and cells is such that:\n- For cells $j \\in \\{1,2,\\dots,10\\}$, exactly $3$ rays traverse cell $j$.\n- For cells $j \\in \\{11,12,\\dots,20\\}$, exactly $2$ rays traverse cell $j$.\n\nSuppose we perform robust inversion using one iteration of Iteratively Reweighted Least Squares (IRLS) with the Huber loss function with threshold parameter $\\delta = 1$. Let the initial model be $m^{(0)} = 0$. The corresponding initial residuals are $r^{(0)} = d - G m^{(0)} = d$. The measured residuals are contaminated by outliers in the following cell-wise pattern:\n- For each cell $j \\in \\{1,2,\\dots,10\\}$, the $3$ residuals associated with the $3$ rays in that cell are $\\{0.1, 0.1, 5.0\\}$.\n- For each cell $j \\in \\{11,12,\\dots,20\\}$, the $2$ residuals associated with the $2$ rays in that cell are $\\{0.1, 5.0\\}$.\n\nUsing the Huber loss function\n$$\n\\rho_{\\delta}(r) \\;=\\; \n\\begin{cases}\n\\dfrac{1}{2}\\, r^{2}, & |r| \\le \\delta, \\\\[6pt]\n\\delta\\,|r| - \\dfrac{1}{2}\\,\\delta^{2}, & |r| > \\delta,\n\\end{cases}\n$$\nperform one IRLS iteration starting from $m^{(0)}$ to obtain the updated model $m^{(1)}$. Specifically:\n1. Compute the initial residuals $r^{(0)}$ and the corresponding IRLS weights derived from the Huber loss.\n2. Form the weighted normal equations associated with the weighted least squares surrogate at this iteration.\n3. Solve these weighted normal equations to obtain $m^{(1)}$.\n\nExpress the final updated model vector $m^{(1)}$ as a single row vector with $20$ entries. No rounding is required; provide exact values if they arise naturally. The final answer must be the updated model vector only, as specified. Do not include physical units in your answer.",
            "solution": "The problem asks for the updated model vector $m^{(1)}$ after one iteration of Iteratively Reweighted Least Squares (IRLS) starting from an initial model $m^{(0)} = 0$. The linear model is given by $d = G m + \\varepsilon$.\n\nAn IRLS iteration solves a weighted least squares problem to update the model. The updated model $m^{(k+1)}$ is found by minimizing the weighted sum of squared residuals, which is equivalent to solving the weighted normal equations:\n$$ (G^T W^{(k)} G) m^{(k+1)} = G^T W^{(k)} d $$\nwhere $W^{(k)}$ is a diagonal matrix of weights computed from the residuals of the previous iteration, $r^{(k)} = d - G m^{(k)}$.\n\nIn our case, we perform one iteration starting with $k=0$. The initial model is $m^{(0)} = 0 \\in \\mathbb{R}^{20}$.\nThe initial residuals are therefore $r^{(0)} = d - G m^{(0)} = d$. The problem provides the values of these residuals.\n\nFirst, we determine the weights. The weights are derived from the Huber loss function $\\rho_{\\delta}(r)$ with threshold $\\delta=1$. The weight function $w(r)$ for IRLS is given by $w(r) = \\frac{\\psi(r)}{r}$, where $\\psi(r) = \\rho'_{\\delta}(r)$ is the influence function (the derivative of the loss function).\nThe Huber loss function is:\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2}, & |r| \\le \\delta \\\\\n\\delta|r| - \\frac{1}{2}\\delta^{2}, & |r| > \\delta\n\\end{cases}\n$$\nIts derivative is:\n$$\n\\psi(r) = \n\\begin{cases}\nr, & |r| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(r), & |r| > \\delta\n\\end{cases}\n$$\nThe corresponding IRLS weight is:\n$$\nw(r) = \\frac{\\psi(r)}{r} = \n\\begin{cases}\n1, & |r| \\le \\delta \\\\\n\\frac{\\delta}{|r|}, & |r| > \\delta\n\\end{cases}\n$$\nGiven $\\delta=1$, the weight for the $i$-th residual $r_i$ is $w_i = \\min(1, 1/|r_i|)$.\n\nWe are given two sets of initial residuals $r^{(0)}$: $0.1$ and $5.0$. Let's calculate their weights:\n- For a residual $r_i = 0.1$, we have $|r_i| = 0.1 \\le 1$. The weight is $w_i = 1$.\n- For a residual $r_i = 5.0$, we have $|r_i| = 5.0 > 1$. The weight is $w_i = \\frac{1}{|5.0|} = \\frac{1}{5} = 0.2$.\n\nNext, we analyze the structure of the weighted normal equations. The matrix $A = G^T W^{(0)} G$ is a $20 \\times 20$ matrix. The problem states that each row of $G$ contains a single $1$ and zeros elsewhere. This means that for each ray $i$, there is exactly one cell $j$ for which $G_{ij} = 1$, and $G_{ik} = 0$ for all $k \\neq j$.\nThe $(j,k)$-th element of $A$ is $A_{jk} = \\sum_{i=1}^{50} G_{ij} w_i G_{ik}$. This sum is non-zero only if $j=k$, because if $j \\neq k$, for any given row $i$, at least one of $G_{ij}$ or $G_{ik}$ must be zero. Therefore, $A = G^T W^{(0)} G$ is a diagonal matrix.\n\nThe diagonal elements are:\n$$ A_{jj} = (G^T W^{(0)} G)_{jj} = \\sum_{i=1}^{50} G_{ij}^2 w_i^{(0)} = \\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} $$\nThe right-hand side vector is $b = G^T W^{(0)} d$. Its $j$-th component is:\n$$ b_j = (G^T W^{(0)} d)_j = \\sum_{i=1}^{50} G_{ij} w_i^{(0)} d_i = \\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} d_i $$\nSince $A$ is diagonal, the system of equations $A m^{(1)} = b$ decouples, and we can solve for each component $m_j^{(1)}$ independently:\n$$ m_j^{(1)} = \\frac{b_j}{A_{jj}} = \\frac{\\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} d_i}{\\sum_{i \\in \\text{rays in cell } j} w_i^{(0)}} $$\nThis shows that the updated slowness for cell $j$ is the weighted average of the initial residuals (data) for the rays passing through that cell.\n\nNow we compute the values of $m_j^{(1)}$ for the two groups of cells.\n\nGroup 1: Cells $j \\in \\{1, 2, \\dots, 10\\}$\nFor each of these cells, there are $3$ associated rays. The initial residuals are $\\{0.1, 0.1, 5.0\\}$. The corresponding weights are $\\{1, 1, 0.2\\}$.\nThe sum of the weights is:\n$$ \\sum w_i^{(0)} = 1 + 1 + 0.2 = 2.2 $$\nThe sum of the weighted residuals is:\n$$ \\sum w_i^{(0)} d_i = (1 \\times 0.1) + (1 \\times 0.1) + (0.2 \\times 5.0) = 0.1 + 0.1 + 1.0 = 1.2 $$\nSo, for $j \\in \\{1, \\dots, 10\\}$, the updated model component is:\n$$ m_j^{(1)} = \\frac{1.2}{2.2} = \\frac{12}{22} = \\frac{6}{11} $$\n\nGroup 2: Cells $j \\in \\{11, 12, \\dots, 20\\}$\nFor each of these cells, there are $2$ associated rays. The initial residuals are $\\{0.1, 5.0\\}$. The corresponding weights are $\\{1, 0.2\\}$.\nThe sum of the weights is:\n$$ \\sum w_i^{(0)} = 1 + 0.2 = 1.2 $$\nThe sum of the weighted residuals is:\n$$ \\sum w_i^{(0)} d_i = (1 \\times 0.1) + (0.2 \\times 5.0) = 0.1 + 1.0 = 1.1 $$\nSo, for $j \\in \\{11, \\dots, 20\\}$, the updated model component is:\n$$ m_j^{(1)} = \\frac{1.1}{1.2} = \\frac{11}{12} $$\n\nThe final updated model vector $m^{(1)} \\in \\mathbb{R}^{20}$ is composed of these values. The first $10$ entries are $\\frac{6}{11}$ and the remaining $10$ entries are $\\frac{11}{12}$.\n$$ m^{(1)} = \\left[ \\underbrace{\\frac{6}{11}, \\dots, \\frac{6}{11}}_{10 \\text{ times}}, \\underbrace{\\frac{11}{12}, \\dots, \\frac{11}{12}}_{10 \\text{ times}} \\right]^T $$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{6}{11} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} & \\frac{11}{12} \\end{pmatrix} } $$"
        },
        {
            "introduction": "Moving beyond robust estimation, this advanced practice explores the use of IRLS in nonconvex optimization, a common strategy for promoting sparse solutions in geophysical inversion. You will analyze a stylized problem where an $\\ell_p$ regularizer with $p \\lt 1$ is used, revealing a critical challenge: the risk of the algorithm becoming trapped in a suboptimal local minimum. This exercise will deepen your analytical skills by examining the algorithm's fixed points and prompting you to consider sophisticated mitigation techniques like continuation methods that are essential for successfully navigating complex, nonconvex objective functions .",
            "id": "3605298",
            "problem": "Consider a stylized amplitude-fitting inverse problem drawn from seismic amplitude inversion, where the forward operator is the identity mapping. A single unknown scalar reflectivity amplitude $m \\in \\mathbb{R}$ is to be estimated from data $a \\in \\mathbb{R}$ by minimizing a nonconvex sparsity-promoting objective. The objective function is\n$$\nJ(m) = \\frac{1}{2}\\,|m - a|^{2} + \\lambda\\,|m|^{p},\n$$\nwith $0<p<1$, data amplitude $a>0$, and regularization parameter $\\lambda>0$. The nonconvexity reflects a preference for sparse reflectivity typical in computational geophysics. Iteratively Reweighted Least Squares (IRLS) constructs a quadratic surrogate of $|m|^{p}$ that is reweighted at each iteration to majorize the nonconvex term, often stabilized by a small differentiability parameter $\\epsilon>0$.\n\nTasks:\n1. Starting from the definition of $J(m)$ and the properties of the function $|m|^{p}$ for $0<p<1$, use calculus to determine the behavior of $J(m)$ in a neighborhood of $m=0$. Based on first-order analysis from the left and right of $m=0$, argue whether $m=0$ is a local extremum and classify it.\n2. Derive, from the concavity of $x \\mapsto (x+\\epsilon)^{\\frac{p}{2}}$ in $x=m^{2}$ for $0<p<1$ and $\\epsilon>0$, a quadratic majorizer of $|m|^{p}$ at a current iterate $m^{(k)}$. Show that a valid surrogate of the form\n$$\n|m|^{p} \\;\\le\\; w\\!\\left(m^{(k)}\\right)\\,m^{2} + c\\!\\left(m^{(k)}\\right)\n$$\ncan be constructed, and obtain the corresponding weight $w(m^{(k)})$ by differentiating the concave function with respect to $m^{2}$ at $m^{(k)}$.\n3. Using the surrogate in Task 2, write down and solve the scalar quadratic subproblem that defines the IRLS update $m^{(k+1)}$ in closed form, as a function of $a$, $\\lambda$, $p$, $\\epsilon$, and $m^{(k)}$.\n4. Specialize to the parameter choice $a=1$, $\\lambda=0.1$, $p=\\frac{1}{2}$, $\\epsilon=10^{-8}$, initialized at $m^{(0)}=0$. Analyze the resulting fixed-point equation and show that IRLS admits a stable fixed point near $m=0$. Compute the closed-form value of the IRLS fixed point under the assumption that $m^{(k)}$ remains sufficiently small that the stabilizer $\\epsilon$ dominates the weight formula.\n5. For the same parameters, determine (numerically or by qualitative monotonicity arguments) whether there exists a positive stationary point $m^{\\star}>0$ of $J(m)$ solving the first-order necessary condition for optimality. Compare $J(m^{\\star})$ with $J(0)$ and with $J$ at the IRLS fixed point found in Task 4 to confirm that the IRLS fixed point is suboptimal.\n6. Propose a scientifically motivated continuation strategy over the nonconvexity (for example, on $p$ and/or $\\lambda$) that reduces the risk of converging to the suboptimal local minimum near $m=0$ in this and related geophysical inverse problems. Clearly state the scheduling principle and stopping criterion you would use.\n\nExpress the final numerical answer for Task 4 as a pure number, rounded to four significant figures, using scientific notation.",
            "solution": "The objective function to be minimized is:\n$$\nJ(m) = \\frac{1}{2}(m - a)^{2} + \\lambda|m|^{p}\n$$\nwith given parameters $m, a \\in \\mathbb{R}$, $0 < p < 1$, $a > 0$, and $\\lambda > 0$.\n\n### Task 1: Behavior of $J(m)$ near $m=0$\n\nTo analyze the behavior of $J(m)$ near $m=0$, we examine its first derivative. The function $|m|^p$ is not differentiable at $m=0$ for $0<p<1$, so we must consider one-sided derivatives.\n\nFor $m > 0$, the objective function is $J(m) = \\frac{1}{2}(m-a)^2 + \\lambda m^p$. Its derivative is:\n$$\nJ'(m) = (m-a) + \\lambda p m^{p-1}\n$$\nThe right-hand derivative at $m=0$ is the limit of $J'(m)$ as $m \\to 0^{+}$:\n$$\nJ'_{+}(0) = \\lim_{m \\to 0^{+}} \\left( m-a + \\lambda p m^{p-1} \\right)\n$$\nSince $0 < p < 1$, the exponent $p-1$ is negative. Given that $\\lambda > 0$ and $p > 0$, the term $\\lambda p m^{p-1}$ approaches $+\\infty$ as $m \\to 0^{+}$. Thus,\n$$\nJ'_{+}(0) = -a + \\lim_{m \\to 0^{+}} (\\lambda p m^{p-1}) = +\\infty\n$$\n\nFor $m < 0$, the objective function is $J(m) = \\frac{1}{2}(m-a)^2 + \\lambda (-m)^p$. Its derivative is:\n$$\nJ'(m) = (m-a) + \\lambda \\frac{d}{dm}(-m)^p = (m-a) + \\lambda p(-m)^{p-1}(-1) = m-a - \\lambda p(-m)^{p-1}\n$$\nThe left-hand derivative at $m=0$ is the limit of $J'(m)$ as $m \\to 0^{-}$:\n$$\nJ'_{-}(0) = \\lim_{m \\to 0^{-}} \\left( m-a - \\lambda p (-m)^{p-1} \\right)\n$$\nAs $m \\to 0^{-}$, $-m \\to 0^{+}$, so $(-m)^{p-1}$ approaches $+\\infty$. Thus,\n$$\nJ'_{-}(0) = -a - \\lim_{m \\to 0^{-}} (\\lambda p (-m)^{p-1}) = -\\infty\n$$\nThe derivative of $J(m)$ changes sign from negative (approaching from the left of $0$) to positive (approaching from the right of $0$). Specifically, the slope approaches $-\\infty$ from the left and $+\\infty$ from the right, forming a cusp. According to the first derivative test, $m=0$ is a local minimum of $J(m)$.\n\n### Task 2: Derivation of the Quadratic Majorizer\n\nWe need to find a quadratic majorizer for $|m|^p$ of the form $w(m^{(k)})m^2 + c(m^{(k)})$. We use the concavity of a related function. Let $x=m^2$ and consider the function $\\phi(x) = (x+\\epsilon)^{p/2}$ for $x \\ge 0$, where $\\epsilon>0$. Its derivatives with respect to $x$ are:\n$$\n\\phi'(x) = \\frac{p}{2}(x+\\epsilon)^{\\frac{p}{2}-1}\n$$\n$$\n\\phi''(x) = \\frac{p}{2}\\left(\\frac{p}{2}-1\\right)(x+\\epsilon)^{\\frac{p}{2}-2}\n$$\nSince $0<p<1$, we have $p/2 > 0$ and $p/2-1 < 0$. Therefore, $\\phi''(x) < 0$ for all $x \\ge 0$, which confirms that $\\phi(x)$ is a strictly concave function.\n\nFor any concave function, the tangent line at a point $x_k$ provides an upper bound (a majorizer) for the function:\n$$\n\\phi(x) \\le \\phi(x_k) + \\phi'(x_k)(x - x_k)\n$$\nLet's choose $x_k = (m^{(k)})^2$ and $x=m^2$. This gives a majorizer for $(m^2+\\epsilon)^{p/2}$:\n$$\n(m^2+\\epsilon)^{p/2} \\le ((m^{(k)})^2+\\epsilon)^{p/2} + \\left[\\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p}{2}-1}\\right](m^2 - (m^{(k)})^2)\n$$\nSince $|m|^p = (m^2)^{p/2} \\le (m^2+\\epsilon)^{p/2}$ for any $\\epsilon>0$, the right-hand side also majorizes $|m|^p$. Rearranging the expression into the desired form $w m^2 + c$:\n$$\n|m|^p \\le \\left[\\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p}{2}-1}\\right]m^2 + \\left[((m^{(k)})^2+\\epsilon)^{p/2} - \\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p}{2}-1}(m^{(k)})^2\\right]\n$$\nThis inequality is of the form $|m|^p \\le w(m^{(k)})m^2 + c(m^{(k)})$, where the weight $w(m^{(k)})$ is the coefficient of the $m^2$ term:\n$$\nw(m^{(k)}) = \\frac{p}{2}\\left((m^{(k)})^2+\\epsilon\\right)^{\\frac{p-2}{2}}\n$$\n\n### Task 3: IRLS Update Rule\n\nThe IRLS algorithm solves a sequence of quadratic subproblems.\nAt iteration $k$, we minimize a surrogate objective function $J_{surr}(m; m^{(k)})$ which majorizes the true objective $J(m)$.\n$$\nJ_{surr}(m; m^{(k)}) = \\frac{1}{2}(m-a)^2 + \\lambda \\left( w(m^{(k)})m^2 + c(m^{(k)}) \\right)\n$$\nTo find the minimizer $m^{(k+1)}$, we differentiate $J_{surr}$ with respect to $m$ and set the result to zero. The term $c(m^{(k)})$ is constant with respect to $m$ and does not affect the location of the minimum.\n$$\n\\frac{\\partial J_{surr}}{\\partial m} = (m-a) + 2\\lambda w(m^{(k)})m = 0\n$$\nSolving for $m$:\n$$\nm(1 + 2\\lambda w(m^{(k)})) = a\n$$\n$$\nm^{(k+1)} = \\frac{a}{1 + 2\\lambda w(m^{(k)})}\n$$\nSubstituting the expression for $w(m^{(k)})$ from Task 2:\n$$\nm^{(k+1)} = \\frac{a}{1 + 2\\lambda \\left( \\frac{p}{2}((m^{(k)})^2+\\epsilon)^{\\frac{p-2}{2}} \\right)} = \\frac{a}{1 + p\\lambda((m^{(k)})^2+\\epsilon)^{\\frac{p-2}{2}}}\n$$\nThis is the closed-form IRLS update rule.\n\n### Task 4: Fixed-Point Analysis\n\nWe are given $a=1$, $\\lambda=0.1$, $p=1/2$, $\\epsilon=10^{-8}$, and $m^{(0)}=0$. A fixed point $m^*$ of the IRLS iteration satisfies the equation $m^* = F(m^*)$, where $F(m)$ is the update map from Task 3:\n$$\nm^* = \\frac{1}{1 + (1/2)(0.1)((m^*)^2+10^{-8})^{\\frac{1/2-2}{2}}} = \\frac{1}{1 + 0.05((m^*)^2+10^{-8})^{-3/4}}\n$$\nWe are asked to analyze the fixed point near $m=0$. This implies $(m^*)^2 \\ll \\epsilon$. Under this assumption, the fixed-point equation simplifies to:\n$$\nm^* \\approx \\frac{1}{1 + 0.05(\\epsilon)^{-3/4}} = \\frac{1}{1 + 0.05(10^{-8})^{-3/4}} = \\frac{1}{1 + 0.05 \\times 10^{6}} = \\frac{1}{1 + 50000} = \\frac{1}{50001}\n$$\nLet's check the validity of the assumption $(m^*)^2 \\ll \\epsilon$. We have $m^* \\approx 1/50001 \\approx 2 \\times 10^{-5}$. Then $(m^*)^2 \\approx 4 \\times 10^{-10}$. Since $4 \\times 10^{-10} \\ll 10^{-8}$, the assumption is self-consistent.\n\nTo show the fixed point is stable, we analyze the derivative of the update map $F(m) = a/(1+p\\lambda(m^2+\\epsilon)^{(p-2)/2})$.\n$$\nF'(m) = -a p \\lambda (p-2) m (m^2+\\epsilon)^{\\frac{p-4}{2}} \\left[1+p\\lambda(m^2+\\epsilon)^{\\frac{p-2}{2}}\\right]^{-2}\n$$\nAt $m=0$, $F'(0) = 0$. Since the derivative is continuous, for a fixed point $m^*$ very close to $0$, we will have $|F'(m^*)| \\ll 1$, which is the condition for a stable fixed point. The iteration will converge to this point if started sufficiently close, which is the case for $m^{(0)}=0$.\n\nThe numerical value of the IRLS fixed point is $m^* = 1/50001 \\approx 1.9999600 \\times 10^{-5}$. Rounded to four significant figures, this is $2.000 \\times 10^{-5}$.\n\n### Task 5: Comparison with Stationary Points\n\nA stationary point $m^\\star > 0$ of the original objective function $J(m)$ satisfies $J'(m^\\star) = 0$:\n$$\nm^\\star - a + \\lambda p (m^\\star)^{p-1} = 0\n$$\nUsing the given parameters, we have:\n$$\nm^\\star - 1 + (0.1)(1/2)(m^\\star)^{-1/2} = 0 \\implies m^\\star - 1 + 0.05(m^\\star)^{-1/2} = 0\n$$\nMultiplying by $(m^\\star)^{1/2}$ gives $ (m^\\star)^{3/2} - (m^\\star)^{1/2} + 0.05 = 0$. Let $u = (m^\\star)^{1/2}$. The equation becomes a cubic polynomial in $u$:\n$$\nu^3 - u + 0.05 = 0\n$$\nThe positive roots of this polynomial can be found numerically. They are $u_1 \\approx 0.05006$ and $u_2 \\approx 0.9756$. These correspond to stationary points $m_1=u_1^2 \\approx 0.0025$ and $m_2=u_2^2 \\approx 0.9518$. Analysis of the second derivative $J''(m) = 1 + \\lambda p(p-1)m^{p-2} = 1 - 0.0125 m^{-3/2}$ reveals that $m_1$ is a local maximum ($J''(m_1)<0$) and $m_2$ is a local minimum ($J''(m_2)>0$). Thus, the relevant positive stationary point is $m^\\star = m_2 \\approx 0.9518$. No stationary points exist for $m<0$.\n\nWe now compare the objective function values at the three points of interest:\n1.  The local minimum at $m=0$: $J(0) = \\frac{1}{2}(0-1)^2 + 0.1\\sqrt{0} = 0.5$.\n2.  The IRLS fixed point $m_{IRLS} = 1/50001$:\n    $J(m_{IRLS}) = \\frac{1}{2}\\left(\\frac{1}{50001}-1\\right)^2 + 0.1\\sqrt{\\frac{1}{50001}} \\approx \\frac{1}{2}(1)^2 + \\frac{0.1}{\\sqrt{50001}} \\approx 0.5 + \\frac{0.1}{223.6} \\approx 0.5 + 0.000447 = 0.500447$.\n3.  The global minimum $m^\\star \\approx 0.9518$:\n    $J(m^\\star) = \\frac{1}{2}(0.9518-1)^2 + 0.1\\sqrt{0.9518} \\approx \\frac{1}{2}(-0.0482)^2 + 0.1(0.9756) \\approx 0.00116 + 0.09756 = 0.09872$.\n\nThe values are $J(m^\\star) \\approx 0.09872$, $J(0)=0.5$, and $J(m_{IRLS}) \\approx 0.500447$. The ordering $J(m^\\star) < J(0) < J(m_{IRLS})$ confirms that the IRLS fixed point is suboptimal. It represents convergence to a local minimum of the stabilized objective function, which is not a minimum of the original function and is inferior to both the local minimum at $m=0$ and especially the global minimum at $m^\\star$.\n\n### Task 6: Continuation Strategy\n\nThe failure of IRLS to find the global minimum is due to the nonconvexity of the objective function, which creates a strong local minimum at $m=0$ that traps the algorithm when initialized at or near zero. To overcome this, a continuation (or homotopy) method can be employed. This strategy starts with a simpler, convex version of the problem and gradually deforms it into the target nonconvex problem, guiding the solution towards the global minimum.\n\nA scientifically well-motivated strategy is continuation on the regularization exponent $p$.\n\n**Principle:**\n1.  **Start with a Convex Problem:** Begin with an exponent $p_0 \\ge 1$, which makes the regularizer $|m|^{p_0}$ convex. A standard choice is $p_0=1$. The problem becomes $\\min_m \\frac{1}{2}(m-a)^2 + \\lambda |m|$, which is a convex L1-regularization problem (LASSO). Its unique global minimum is given by the soft-thresholding operator: $m_0^* = \\text{sgn}(a)(|a|-\\lambda)_+$. With the given parameters, $m_0^* = (1-0.1)_+ = 0.9$. This provides an excellent initial guess, far from the basin of attraction of the zero minimum.\n2.  **Gradual Annealing of $p$:** Define a sequence of exponents $p_k$ decreasing from $p_0=1$ to the target value $p_{final}=1/2$. A simple schedule is an arithmetic progression: $p_k = 1 - k \\cdot \\Delta p$ for a small step $\\Delta p$.\n3.  **Iterative Refinement:** For each $k=1, 2, \\dots, N$, solve the subproblem $\\min_m \\left\\{ \\frac{1}{2}(m-a)^2 + \\lambda|m|^{p_k} \\right\\}$ using an iterative method like IRLS. Crucially, the initial guess for solving the subproblem at step $k$ is the solution from the previous step, $m_{k-1}^*$. This ensures the solution path is tracked continuously as the landscape deforms.\n4.  **Final Solution:** The final output of the continuation procedure, $m_N^*$, is the estimate for the global minimum of the original problem with $p=1/2$.\n\n**Stopping Criterion:** The main continuation loop terminates when the exponent $p_k$ reaches its target value $p_{final}=1/2$. Each inner IRLS solver for a fixed $p_k$ is run until a standard convergence criterion is met, for example, when the relative change in successive iterates falls below a predefined tolerance (e.g., $\\|m^{(j+1)} - m^{(j)}\\| / \\|m^{(j)}\\| < \\delta$). This strategy effectively avoids the suboptimal minimum by starting in a region of the parameter space where the solution is unique and easy to find, and then carefully tracking this solution as the problem's complexity (nonconvexity) is gradually introduced.",
            "answer": "$$\\boxed{2.000 \\times 10^{-5}}$$"
        }
    ]
}