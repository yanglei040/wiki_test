{
    "hands_on_practices": [
        {
            "introduction": "第一个练习将带你直接、逐步地体验一次迭代重加权最小二乘（IRLS）迭代的核心机制。通过处理一个带有明显异常值的小规模问题 ()，你将清楚地看到Huber损失函数如何识别并降低异常数据点的影响。这个基础练习对于理解IRLS在实践中如何实现稳健性至关重要。",
            "id": "3393330",
            "problem": "考虑一个线性数据同化设定，其中正向算子由矩阵 $A \\in \\mathbb{R}^{3 \\times 2}$ 表示，观测值为 $b \\in \\mathbb{R}^{3}$，且第三个观测值相对于前两个是一个大的离群值。目标是使用基于Huber损失的迭代重加权最小二乘法（IRLS）的一次迭代，在存在此离群值的情况下稳健地估计状态 $x \\in \\mathbb{R}^{2}$。令\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}.\n$$\n阈值参数为 $\\delta0$ 的Huber损失由以下分段函数定义\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2},  \\text{若 } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  \\text{若 } |r|  \\delta.\n\\end{cases}\n$$\n使用 $\\delta = 2$。从上面 $\\rho_{\\delta}(r)$ 的定义和稳健M估计的原理出发，按如下方式执行一次IRLS迭代：\n1. 使用失配 $r = A x - b$ 计算在 $x^{(0)}$ 处的残差，记为 $r^{(0)} \\in \\mathbb{R}^{3}$。\n2. 从与 $\\rho_{\\delta}(r)$ 相关的Huber影响函数中推导出相应的IRLS权重 $w_{i}$，并组装对角权重矩阵 $W = \\operatorname{diag}(w_{1}, w_{2}, w_{3})$。\n3. 建立加权正规方程 $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$，并求解得到的 $2 \\times 2$ 线性系统以获得 $x^{(1)}$。\n\n只报告更新后估计的第二个分量 $x_{2}^{(1)}$ 的值。无需四舍五入，且不涉及物理单位。",
            "solution": "我们从反问题中的稳健M估计公式开始，其中估计值 $x \\in \\mathbb{R}^{2}$ 最小化了应用于数据失配的稳健惩罚项之和。对于残差 $r_{i} = a_{i}^{\\top} x - b_{i}$，阈值参数为 $\\delta  0$ 的Huber损失定义为\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2},  \\text{若 } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  \\text{若 } |r|  \\delta.\n\\end{cases}\n$$\n相应的影响函数（即关于 $r$ 的导数）是\n$$\n\\psi_{\\delta}(r) \\;=\\; \\frac{\\mathrm{d}}{\\mathrm{d}r}\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\nr,  \\text{若 } |r| \\leq \\delta, \\\\\n\\delta \\,\\operatorname{sign}(r),  \\text{若 } |r|  \\delta.\n\\end{cases}\n$$\n在迭代重加权最小二乘法（IRLS）中，当 $r_{i} \\neq 0$ 时，权重构造为 $w_{i} = \\frac{\\psi_{\\delta}(r_{i})}{r_{i}}$；这得到\n$$\nw_{i} \\;=\\;\n\\begin{cases}\n1,  \\text{若 } |r_{i}| \\leq \\delta, \\\\\n\\frac{\\delta}{|r_{i}|},  \\text{若 } |r_{i}|  \\delta,\n\\end{cases}\n$$\n且根据连续性，若 $r_{i} = 0$，则 $w_{i} = 1$。\n\n步骤1：使用 $r = A x - b$ 计算在 $x^{(0)}$ 处的残差。对于\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix},\n$$\n我们得到\n$$\nr^{(0)} \\;=\\; A x^{(0)} - b \\;=\\; \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ -1 \\\\ 20\\end{pmatrix} \\;=\\; \\begin{pmatrix} -1 \\\\ 1 \\\\ -20 \\end{pmatrix}.\n$$\n\n步骤2：从 $\\delta = 2$ 的Huber影响函数中推导IRLS权重。残差的绝对值为 $|r_{1}^{(0)}| = 1$、 $|r_{2}^{(0)}| = 1$ 和 $|r_{3}^{(0)}| = 20$。因此，\n- 对于 $i = 1$：$|r_{1}^{(0)}| = 1 \\leq \\delta$ 意味着 $w_{1} = 1$。\n- 对于 $i = 2$：$|r_{2}^{(0)}| = 1 \\leq \\delta$ 意味着 $w_{2} = 1$。\n- 对于 $i = 3$：$|r_{3}^{(0)}| = 20  \\delta$ 意味着 $w_{3} = \\frac{\\delta}{|r_{3}^{(0)}|} = \\frac{2}{20} = 0.1$。\n\n因此，\n$$\nW \\;=\\; \\operatorname{diag}(1,\\, 1,\\, 0.1).\n$$\n\n步骤3：建立并求解加权正规方程 $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$。\n\n首先计算 $A^{\\top} W A$。将 $A$ 的行表示为 $a_{1}^{\\top} = (1, 0)$、$a_{2}^{\\top} = (0, 1)$、$a_{3}^{\\top} = (1, 1)$。那么\n$$\nA^{\\top} W A \\;=\\; \\sum_{i=1}^{3} w_{i} \\, a_{i} a_{i}^{\\top}.\n$$\n我们有\n$$\nw_{1} a_{1} a_{1}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix},\n$$\n$$\nw_{2} a_{2} a_{2}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 0  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix},\n$$\n$$\nw_{3} a_{3} a_{3}^{\\top} \\;=\\; 0.1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.1  0.1 \\\\ 0.1  0.1 \\end{pmatrix}.\n$$\n求和得，\n$$\nA^{\\top} W A \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0.1  0.1 \\\\ 0.1  0.1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}.\n$$\n\n接下来计算 $A^{\\top} W b$。注意到 $W b = \\begin{pmatrix} 1 \\cdot 1 \\\\ 1 \\cdot (-1) \\\\ 0.1 \\cdot 20 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$。那么\n$$\nA^{\\top} W b \\;=\\; A^{\\top} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 \\cdot 1 + 0 \\cdot (-1) + 1 \\cdot 2 \\\\\n0 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 2\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n\n我们必须求解这个 $2 \\times 2$ 线性系统\n$$\n\\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n系数矩阵的行列式为\n$$\n\\det \\;=\\; 1.1 \\cdot 1.1 \\;-\\; 0.1 \\cdot 0.1 \\;=\\; 1.21 - 0.01 \\;=\\; 1.20.\n$$\n其逆矩阵为\n$$\n\\left(\\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}\\right)^{-1}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1  -0.1 \\\\\n-0.1  1.1\n\\end{pmatrix}.\n$$\n因此\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1  -0.1 \\\\\n-0.1  1.1\n\\end{pmatrix}\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1 \\cdot 3 + (-0.1) \\cdot 1 \\\\\n(-0.1) \\cdot 3 + 1.1 \\cdot 1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.3 - 0.1 \\\\\n-0.3 + 1.1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.2 \\\\\n0.8\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{3.2}{1.2} \\\\\n\\frac{0.8}{1.2}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{8}{3} \\\\\n\\frac{2}{3}\n\\end{pmatrix}.\n$$\n\n因此，所求的值是第二个分量 $x_{2}^{(1)} = \\frac{2}{3}$。",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "在掌握了基本机制之后，这个练习将IRLS置于一个地球物理合成问题——走时层析成像的背景中。该问题的设计使得底层方程得以简化 ()，清晰地揭示出每个模型参数的解是相关数据的加权平均。这个练习有助于你建立一个强大的直觉，理解IRLS如何隔离并减轻异常值对最终模型估计的影响。",
            "id": "3605220",
            "problem": "考虑一个合成的线性化走时层析成像实验，其正演算子为 $G \\in \\mathbb{R}^{50 \\times 20}$，模型向量为 $m \\in \\mathbb{R}^{20}$ (代表网格慢度)，数据向量为 $d \\in \\mathbb{R}^{50}$ (代表观测走时)。假设线性观测模型为 $d = G m + \\varepsilon$，其中 $\\varepsilon$ 代表测量噪声和离群值。在此合成设计中，50 条射线中的每一条都恰好穿过一个模型网格，路径长度为单位长度，且不穿过其他任何网格。因此，$G$ 的每一行都只包含一个 $1$，其余元素均为 0。射线和网格之间的映射关系如下：\n- 对于网格 $j \\in \\{1,2,\\dots,10\\}$，恰好有 $3$ 条射线穿过网格 $j$。\n- 对于网格 $j \\in \\{11,12,\\dots,20\\}$，恰好有 $2$ 条射线穿过网格 $j$。\n\n假设我们使用迭代重加权最小二乘法（IRLS）进行稳健反演，迭代一次。使用 Huber 损失函数，其阈值参数为 $\\delta = 1$。设初始模型为 $m^{(0)} = 0$。相应的初始残差为 $r^{(0)} = d - G m^{(0)} = d$。测量残差被离群值污染，其按网格分布的模式如下：\n- 对于每个网格 $j \\in \\{1,2,\\dots,10\\}$，穿过该网格的 3 条射线所对应的 3 个残差为 $\\{0.1, 0.1, 5.0\\}$。\n- 对于每个网格 $j \\in \\{11,12,\\dots,20\\}$，穿过该网格的 2 条射线所对应的 2 个残差为 $\\{0.1, 5.0\\}$。\n\n使用 Huber 损失函数\n$$\n\\rho_{\\delta}(r) \\;=\\; \n\\begin{cases}\n\\dfrac{1}{2}\\, r^{2},  |r| \\le \\delta, \\\\\n\\delta\\,|r| - \\dfrac{1}{2}\\,\\delta^{2},  |r|  \\delta,\n\\end{cases}\n$$\n从 $m^{(0)}$ 开始执行一次 IRLS 迭代，以获得更新后的模型 $m^{(1)}$。具体步骤如下：\n1. 计算初始残差 $r^{(0)}$ 和从 Huber 损失函数导出的相应 IRLS 权重。\n2. 建立与此次迭代中加权最小二乘代理问题相关的加权正规方程。\n3. 求解这些加权正规方程以获得 $m^{(1)}$。\n\n将最终更新后的模型向量 $m^{(1)}$ 表示为一个包含 $20$ 个条目的单行向量。无需四舍五入；如果出现自然产生的精确值，请提供精确值。最终答案必须仅为所指定的更新后模型向量。请勿在答案中包含物理单位。",
            "solution": "问题要求在从初始模型 $m^{(0)} = 0$ 开始，经过一次迭代重加权最小二乘法（IRLS）迭代后，得到更新后的模型向量 $m^{(1)}$。线性模型由 $d = G m + \\varepsilon$ 给出。\n\n一次 IRLS 迭代通过求解一个加权最小二乘问题来更新模型。更新后的模型 $m^{(k+1)}$ 是通过最小化加权残差平方和求得的，这等价于求解加权正规方程：\n$$ (G^T W^{(k)} G) m^{(k+1)} = G^T W^{(k)} d $$\n其中 $W^{(k)}$ 是一个对角权重矩阵，其权重由上一次迭代的残差 $r^{(k)} = d - G m^{(k)}$ 计算得出。\n\n在本例中，我们从 $k=0$ 开始执行一次迭代。初始模型为 $m^{(0)} = 0 \\in \\mathbb{R}^{20}$。\n因此，初始残差为 $r^{(0)} = d - G m^{(0)} = d$。题目给出了这些残差的值。\n\n首先，我们确定权重。权重由阈值为 $\\delta=1$ 的 Huber 损失函数 $\\rho_{\\delta}(r)$ 导出。IRLS 的权重函数 $w(r)$ 由 $w(r) = \\frac{\\psi(r)}{r}$ 给出，其中 $\\psi(r) = \\rho'_{\\delta}(r)$ 是影响函数（损失函数的导数）。\nHuber 损失函数为：\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2},  |r| \\le \\delta \\\\\n\\delta|r| - \\frac{1}{2}\\delta^{2},  |r|  \\delta\n\\end{cases}\n$$\n其导数为：\n$$\n\\psi(r) = \n\\begin{cases}\nr,  |r| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(r),  |r|  \\delta\n\\end{cases}\n$$\n相应的 IRLS 权重为：\n$$\nw(r) = \\frac{\\psi(r)}{r} = \n\\begin{cases}\n1,  |r| \\le \\delta \\\\\n\\frac{\\delta}{|r|},  |r|  \\delta\n\\end{cases}\n$$\n给定 $\\delta=1$，第 $i$ 个残差 $r_i$ 的权重为 $w_i = \\min(1, 1/|r_i|)$。\n\n我们得到了两组初始残差 $r^{(0)}$：$0.1$ 和 $5.0$。我们来计算它们的权重：\n- 对于残差 $r_i = 0.1$，我们有 $|r_i| = 0.1 \\le 1$。权重为 $w_i = 1$。\n- 对于残差 $r_i = 5.0$，我们有 $|r_i| = 5.0  1$。权重为 $w_i = \\frac{1}{|5.0|} = \\frac{1}{5} = 0.2$。\n\n接下来，我们分析加权正规方程的结构。矩阵 $A = G^T W^{(0)} G$ 是一个 $20 \\times 20$ 的矩阵。题目指出，$G$ 的每一行都只包含一个 1，其余元素均为 0。这意味着对于每条射线 $i$，都恰好存在一个网格 $j$ 使得 $G_{ij} = 1$，并且对于所有 $k \\neq j$ 都有 $G_{ik} = 0$。\n$A$ 的第 $(j,k)$ 个元素是 $A_{jk} = \\sum_{i=1}^{50} G_{ij} w_i G_{ik}$。这个和仅在 $j=k$ 时非零，因为如果 $j \\neq k$，对于任何给定的行 $i$，$G_{ij}$ 或 $G_{ik}$ 中至少有一个必须为零。因此，$A = G^T W^{(0)} G$ 是一个对角矩阵。\n\n对角元素为：\n$$ A_{jj} = (G^T W^{(0)} G)_{jj} = \\sum_{i=1}^{50} G_{ij}^2 w_i^{(0)} = \\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} $$\n右端向量为 $b = G^T W^{(0)} d$。其第 $j$ 个分量为：\n$$ b_j = (G^T W^{(0)} d)_j = \\sum_{i=1}^{50} G_{ij} w_i^{(0)} d_i = \\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} d_i $$\n由于 $A$ 是对角矩阵，方程组 $A m^{(1)} = b$ 解耦，我们可以独立求解每个分量 $m_j^{(1)}$：\n$$ m_j^{(1)} = \\frac{b_j}{A_{jj}} = \\frac{\\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} d_i}{\\sum_{i \\in \\text{rays in cell } j} w_i^{(0)}} $$\n这表明网格 $j$ 的更新慢度是穿过该网格的射线的初始残差（数据）的加权平均值。\n\n现在我们为两组网格计算 $m_j^{(1)}$ 的值。\n\n第一组：网格 $j \\in \\{1, 2, \\dots, 10\\}$\n对于这些网格中的每一个，都有 $3$ 条相关射线。初始残差为 $\\{0.1, 0.1, 5.0\\}$。对应的权重为 $\\{1, 1, 0.2\\}$。\n权重之和为：\n$$ \\sum w_i^{(0)} = 1 + 1 + 0.2 = 2.2 $$\n加权残差之和为：\n$$ \\sum w_i^{(0)} d_i = (1 \\times 0.1) + (1 \\times 0.1) + (0.2 \\times 5.0) = 0.1 + 0.1 + 1.0 = 1.2 $$\n因此，对于 $j \\in \\{1, \\dots, 10\\}$，更新后的模型分量为：\n$$ m_j^{(1)} = \\frac{1.2}{2.2} = \\frac{12}{22} = \\frac{6}{11} $$\n\n第二组：网格 $j \\in \\{11, 12, \\dots, 20\\}$\n对于这些网格中的每一个，都有 $2$ 条相关射线。初始残差为 $\\{0.1, 5.0\\}$。对应的权重为 $\\{1, 0.2\\}$。\n权重之和为：\n$$ \\sum w_i^{(0)} = 1 + 0.2 = 1.2 $$\n加权残差之和为：\n$$ \\sum w_i^{(0)} d_i = (1 \\times 0.1) + (0.2 \\times 5.0) = 0.1 + 1.0 = 1.1 $$\n因此，对于 $j \\in \\{11, \\dots, 20\\}$，更新后的模型分量为：\n$$ m_j^{(1)} = \\frac{1.1}{1.2} = \\frac{11}{12} $$\n\n最终更新后的模型向量 $m^{(1)} \\in \\mathbb{R}^{20}$ 由这些值组成。前 $10$ 个条目是 $\\frac{6}{11}$，其余 $10$ 个条目是 $\\frac{11}{12}$。\n$$ m^{(1)} = \\left[ \\underbrace{\\frac{6}{11}, \\dots, \\frac{6}{11}}_{10 \\text{ times}}, \\underbrace{\\frac{11}{12}, \\dots, \\frac{11}{12}}_{10 \\text{ times}} \\right] $$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12} \\end{pmatrix} } $$"
        },
        {
            "introduction": "最后的练习将从手动计算过渡到算法实现，搭建起理论与实际应用之间的桥梁。你将首先探索IRLS与半二次最小化之间的理论联系，然后为Geman-McClure惩罚函数推导更新规则——这是稳健反演中一个常用的选择 ()。通过实现并测试一个完整的求解器，你将巩固对这些强大算法在计算地球物理学中如何构建和验证的理解。",
            "id": "3605213",
            "problem": "考虑一个源于大地电磁学的稳健线性化反演问题，其中多个频率下阻抗的实部和虚部被拼接成一个单一的实值数据向量。设 $m \\in \\mathbb{R}^{n}$ 是一个对数电阻率扰动向量（无量纲），设 $d \\in \\mathbb{R}^{N}$ 是堆叠的数据向量（单位为欧姆，记作 $\\Omega$）。假设存在一个固定的线性化灵敏度矩阵 $G \\in \\mathbb{R}^{N \\times n}$，它将模型扰动映射到预测数据。设 $L \\in \\mathbb{R}^{p \\times n}$ 是一个强制 $m$ 平滑性的离散一阶差分算子。我们旨在最小化稳健正则化目标函数\n$$\nJ(m) \\;=\\; \\sum_{i=1}^{N} \\rho\\!\\left(r_i(m)\\right) \\;+\\; \\frac{\\lambda}{2} \\,\\|L m\\|_2^2,\n$$\n其中 $r(m) = G m - d$，$\\rho$ 是一个稳健的数据失配惩罚项，选择为 Geman–McClure 函数，其尺度为 $c  0$，\n$$\n\\rho(r) \\;=\\; \\frac{1}{2}\\,\\frac{c^2\\, r^2}{r^2 + c^2}.\n$$\n在本设定中，$G$ 被视为大地电磁（MT）正演映射关于某个背景模型的局部线性化所得的雅可比矩阵；因此，该反演是 Gauss–Newton 框架中的单个稳健更新步骤。\n\n任务A（推导）：从最小二乘法方程的定义和凸共轭的概念出发，证明如果 $\\rho$ 是可微的、偶函数、非负，并且在 $\\lvert r \\rvert$ 上是递增的，那么存在一个函数 $\\phi$ 使得\n$$\n\\rho(r) \\;=\\; \\min_{w \\ge 0} \\; \\frac{1}{2}\\, w\\, r^2 \\;+\\; \\phi(w),\n$$\n并且最小化子满足\n$$\nw^\\star(r) \\;=\\; \\begin{cases}\n\\dfrac{\\rho'(r)}{r},  r \\neq 0, \\\\\n\\rho''(0),  r = 0.\n\\end{cases}\n$$\n结论是，迭代重加权最小二乘法（IRLS）——在更新 $w_i \\leftarrow \\rho'\\!\\left(r_i\\right)/r_i$ 和求解加权最小二乘问题之间交替进行——与半二次最小化等价。半二次最小化是在使用相同的二次代理函数的情况下，在最小化 $w$ 和最小化 $m$ 之间交替进行。两者的等价性体现在，在给定相同初始化的条件下，它们对 $m$ 生成相同的迭代序列。将您推导的 $w^\\star(r)$ 表达式特化到上述的 Geman–McClure 惩罚项，并将其简化为关于 $r$ 和 $c$ 的函数。\n\n任务B（算法设计）：仅使用线性代数和加权最小二乘法方程的定义，推导在每次迭代中，给定对角权重 $W = \\mathrm{diag}(w_1,\\dots,w_N)$ 和正则化参数 $\\lambda  0$ 时，求解 $m$ 所需的线性系统。解释如何在权重更新中以数值稳健的方式处理 $r = 0$ 的情况。\n\n任务C（实现与测试套件）：为所述问题实现两个求解器，一个基于迭代重加权最小二乘法，另一个基于半二次最小化，两者都使用您为 Geman–McClure 惩罚项推导的权重更新公式。两个求解器都必须：\n- 使用 $m^{(0)} = 0$ 进行初始化，\n- 迭代进行权重更新和加权法方程求解，直到 $\\|m\\|_2$ 的相对变化小于 $10^{-8}$ 或达到最大迭代次数 $200$ 次，\n- 在法方程中加入一个小的岭项 $\\epsilon I$（其中 $\\epsilon = 10^{-8}$）以确保数值稳定性，\n- 将目标函数定义为 $J(m) = \\sum_i \\rho(r_i(m)) + \\frac{\\lambda}{2}\\|L m\\|_2^2$ 并监控其收敛性。\n\n为了符合大地电磁学的实际情况，将 $N = 2 N_f$ 解释为通过堆叠 $N_f$ 个频率的阻抗实部和虚部分量获得的实值数据点数量。对于下文的测试套件，您将使用指定的种子，通过伪随机数确定性地生成矩阵和向量。电阻率以对数扰动表示，因此 $m$ 是无量纲的；数据 $d$ 的单位是 $\\Omega$，但您最终报告的指标是无量纲差异，因此无需进行单位转换。\n\n构建以下四个测试用例，每个用例产生一个标量结果，用于量化两种方法的等价性：\n\n- 测试1（理想路径，温和离群值）：取 $n=5, N=20$（因此 $N_f=10$）。使用种子 $13$ 通过采样独立的标准正态分布条目来生成 $G$，并缩放其行，使平均行2-范数等于 $1$。设 $L$ 为大小为 $(n-1) \\times n$ 的一阶差分算子。使用种子 $17$ 通过采样均值为 $0$、标准差为 $0.2$ 的独立正态分布条目来定义一个“真实”模型 $m_{\\mathrm{true}}$。设 $d_{\\mathrm{clean}} = G m_{\\mathrm{true}}$。使用种子 $19$ 添加标准差为 $0.05$ 的独立高斯噪声，然后在索引 $3$ 和 $12$（从零开始计数）处添加两个振幅为 $+5$ 的离群值，以形成 $d$。使用 $c=1$ 和 $\\lambda=10$。运行两个求解器，并报告它们最终 $m$ 向量之间的最大绝对差。\n\n- 测试2（边界情况，精确数据）：使用与测试1相同的 $G$ 和 $L$，设 $d = G m_{\\mathrm{true}}$，其中 $m_{\\mathrm{true}} = 0$, $c = 1$, 且 $\\lambda = 1$。运行两个求解器，并报告它们最终 $m$ 向量之间的最大绝对差。\n\n- 测试3（强离群值）：使用与测试1相同的 $G$ 和 $L$，按测试1的方式构造 $d$，但需在索引 $1$、$7$ 和 $18$（从零开始计数）处额外添加三个振幅为 $+20$ 的大离群值。使用 $c=1$ 和 $\\lambda=5$。运行两个求解器，并报告它们最终 $m$ 向量之间的最大绝对差。\n\n- 测试4（小尺度参数）：重复测试1，但使用 $c = 0.1$ 和 $\\lambda=10$。运行两个求解器，并报告它们最终 $m$ 向量之间的最大绝对差。\n\n角度单位不适用。输出中不应报告其他物理单位。您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔的结果列表，按测试1到4的顺序排列，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是一个浮点数，等于测试 $k$ 中迭代重加权最小二乘法和半二次最小化最终 $m$ 向量之间的最大绝对差。",
            "solution": "该问题是有效的，因为它具有科学依据、是适定的、客观的，并为计算地球物理学和数值优化中的一个标准问题提供了完整且一致的设置。\n\n### 任务A：等价性与权重函数的推导\n\n此任务要求证明对于一类稳健目标函数，迭代重加权最小二乘法（IRLS）与半二次最小化之间的等价性，并为 Geman–McClure 惩罚项推导出具体的权重函数。\n\n设 $\\rho(r)$ 是一个可微、偶、非负且对非负自变量递增的函数。我们定义一个新函数 $g(x) = \\rho(\\sqrt{2x})$，其中 $x \\ge 0$。$\\rho$ 的性质确保了 $g$ 是非负且单调递增的。对于许多常见的稳健估计量，包括 Geman-McClure， $g(x)$ 也是一个凹函数。\n\n一个凹函数 $g(x)$ 可以表示为其一系列仿射函数（其支撑超平面）的下确界：\n$$\ng(x) = \\min_{w \\ge 0} \\{wx + \\phi(w)\\}\n$$\n其中 $\\phi(w)$ 是某个函数。项 $wx + \\phi(w)$ 是 $g(x)$ 的半二次代理。代入 $x = r^2/2$（其中 $r$ 是残差），得到 $\\rho(r)$ 的半二次表示：\n$$\n\\rho(r) = g\\left(\\frac{r^2}{2}\\right) = \\min_{w \\ge 0} \\left\\{\\frac{1}{2} w r^2 + \\phi(w)\\right\\}\n$$\n这就是所期望的形式。为了找到对于给定 $r$（或 $x = r^2/2$）能达到此最小值的最优权重 $w^\\star$，我们可以将最小值表达式内部对 $w$ 求导并令其为零，但使用凹函数的性质更为直接。在切点处，仿射函数 $wx + \\phi(w)$ 的斜率必须与 $g(x)$ 的斜率匹配。因此，最小化子 $w^\\star$ 必须满足：\n$$\nw^\\star(x) = g'(x)\n$$\n我们可以将 $g'(x)$ 与 $\\rho(r)$ 的导数联系起来。对 $g(x) = \\rho(\\sqrt{2x})$ 使用链式法则，并设 $r = \\sqrt{2x}$：\n$$\ng'(x) = \\frac{d}{dx} \\rho(\\sqrt{2x}) = \\rho'(\\sqrt{2x}) \\cdot \\frac{d}{dx}\\sqrt{2x} = \\rho'(\\sqrt{2x}) \\cdot \\frac{1}{2} (2x)^{-1/2} \\cdot 2 = \\frac{\\rho'(\\sqrt{2x})}{\\sqrt{2x}}\n$$\n代入 $r = \\sqrt{2x}$，对于 $r \\neq 0$ 我们得到：\n$$\nw^\\star(r) = \\frac{\\rho'(r)}{r}\n$$\n对于 $r=0$ 的情况，对应于 $x=0$，我们有 $w^\\star(0) = g'(0) = \\lim_{x \\to 0^+} g'(x) = \\lim_{r \\to 0} \\frac{\\rho'(r)}{r}$。由于 $\\rho(r)$ 是一个偶函数，其导数 $\\rho'(r)$ 是一个奇函数，意味着 $\\rho'(0)=0$。该极限是一个 $0/0$ 的不定式。应用洛必达法则：\n$$\n\\lim_{r \\to 0} \\frac{\\rho'(r)}{r} = \\lim_{r \\to 0} \\frac{\\rho''(r)}{1} = \\rho''(0)\n$$\n因此，最优辅助权重变量的完整表达式是：\n$$\nw^\\star(r) = \\begin{cases}\n\\dfrac{\\rho'(r)}{r},  r \\neq 0, \\\\\n\\rho''(0),  r = 0.\n\\end{cases}\n$$\n\n现在，我们可以建立 IRLS 和半二次最小化之间的等价性。原始目标函数是：\n$$\nJ(m) = \\sum_{i=1}^{N} \\rho(r_i(m)) + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\n对每个 $\\rho(r_i)$ 使用半二次表示：\n$$\nJ(m) = \\sum_{i=1}^{N} \\min_{w_i \\ge 0} \\left\\{\\frac{1}{2} w_i r_i(m)^2 + \\phi(w_i)\\right\\} + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\n通过交换求和和求最小值的顺序，我们可以等价地最小化一个关于 $m$ 和辅助权重 $w = (w_1, \\dots, w_N)$ 的联合目标函数：\n$$\nJ_{HQ}(m, w) = \\sum_{i=1}^{N} \\left(\\frac{1}{2} w_i r_i(m)^2 + \\phi(w_i)\\right) + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\n半二次最小化算法在关于 $w$ 最小化 $J_{HQ}$（固定 $m$）和关于 $m$ 最小化 $J_{HQ}$（固定 $w$）之间交替进行。\n1.  **关于 $w$ 最小化（固定 $m$）：** 在迭代 $k$ 次时，给定 $m^{(k)}$，我们计算残差 $r^{(k)} = G m^{(k)} - d$。$J_{HQ}(m^{(k)}, w)$ 的最小化问题对每个 $w_i$ 都是解耦的。对每个 $i$ 的最小值在 $w_i^{(k+1)} = w^\\star(r_i^{(k)}) = \\rho'(r_i^{(k)})/r_i^{(k)}$ 处达到。\n2.  **关于 $m$ 最小化（固定 $w$）：** 给定 $w^{(k+1)}$，我们关于 $m$ 最小化 $J_{HQ}(m, w^{(k+1)})$。这等价于最小化加权最小二乘目标函数： $$m^{(k+1)} = \\arg\\min_m \\sum_{i=1}^{N} \\frac{1}{2} w_i^{(k+1)} r_i(m)^2 + \\frac{\\lambda}{2} \\|L m\\|_2^2$$\n\n根据定义，IRLS 算法是一个在第 $k$ 次迭代时计算残差 $r_i^{(k)} = r_i(m^{(k)})$，将权重定义为 $w_i^{(k+1)} = \\rho'(r_i^{(k)})/r_i^{(k)}$，然后通过求解相同的加权最小二乘问题来找到下一个迭代值 $m^{(k+1)}$ 的过程。由于对 $w$ 和 $m$ 的更新规则是相同的，并且假设初始值 $m^{(0)}$ 也相同，IRLS 和半二次最小化会生成完全相同的迭代序列 $\\{m^{(k)}\\}$。它们在算法上是等价的。\n\n最后，我们将 $w^\\star(r)$ 特化为 Geman–McClure 惩罚项：\n$$\n\\rho(r) = \\frac{1}{2}\\,\\frac{c^2\\, r^2}{r^2 + c^2}\n$$\n其一阶导数是：\n$$\n\\rho'(r) = \\frac{d}{dr} \\left( \\frac{1}{2} c^2 \\frac{r^2}{r^2+c^2} \\right) = \\frac{c^2}{2} \\left( \\frac{2r(r^2+c^2) - r^2(2r)}{(r^2+c^2)^2} \\right) = \\frac{c^2}{2} \\frac{2rc^2}{(r^2+c^2)^2} = \\frac{c^4 r}{(r^2+c^2)^2}\n$$\n对于 $r \\neq 0$，权重是：\n$$\nw^\\star(r) = \\frac{\\rho'(r)}{r} = \\frac{c^4}{(r^2+c^2)^2}\n$$\n为了找到 $r=0$ 时的权重，我们可以计算 $\\rho''(0)$。其二阶导数是：\n$$\n\\rho''(r) = \\frac{d}{dr} \\left( \\frac{c^4 r}{(r^2+c^2)^2} \\right) = c^4 \\frac{(r^2+c^2)^2 - r \\cdot 2(r^2+c^2)(2r)}{(r^2+c^2)^4} = c^4 \\frac{(r^2+c^2) - 4r^2}{(r^2+c^2)^3} = \\frac{c^4(c^2-3r^2)}{(r^2+c^2)^3}\n$$\n在 $r=0$ 处求值：\n$$\nw^\\star(0) = \\rho''(0) = \\frac{c^4(c^2)}{(c^2)^3} = \\frac{c^6}{c^6} = 1\n$$\n值得注意的是，如果我们取 $w^\\star(r)$ 的表达式在 $r \\to 0$ 时的极限，我们得到 $\\lim_{r\\to 0} \\frac{c^4}{(r^2+c^2)^2} = \\frac{c^4}{(c^2)^2} = 1$。由于该表达式在 $r=0$ 处连续且有良好定义，对于 Geman-McClure 惩罚项，可以使用单个简单的公式计算所有 $r$ 的权重：\n$$\nw(r) = \\frac{c^4}{(r^2+c^2)^2}\n$$\n\n### 任务B：算法设计\n\n在 IRLS/半二次算法的每次迭代中，我们必须为模型更新 $m$ 求解一个加权正则化最小二乘问题。对于固定的权重集 $w_i$，需要关于 $m$ 最小化的目标函数是：\n$$\nF(m) = \\sum_{i=1}^{N} \\frac{1}{2} w_i \\left( (Gm)_i - d_i \\right)^2_2 + \\frac{\\lambda}{2} \\|L m\\|_2^2\n$$\n设 $W$ 是对角线上为 $w_i$ 的对角矩阵，$W = \\mathrm{diag}(w_1, \\dots, w_N)$。我们可以用矩阵向量表示法写出 $F(m)$：\n$$\nF(m) = \\frac{1}{2} (Gm - d)^T W (Gm-d) + \\frac{\\lambda}{2} m^T L^T L m\n$$\n这是关于 $m$ 的二次函数。为了找到最小值，我们计算它关于 $m$ 的梯度并令其为零。\n$$\n\\nabla_m F(m) = G^T W (Gm - d) + \\lambda L^T L m\n$$\n令 $\\nabla_m F(m) = 0$：\n$$\nG^T W G m - G^T W d + \\lambda L^T L m = 0\n$$\n重新排列各项，形成 $Ax=b$ 形式的线性系统：\n$$\n\\left( G^T W G + \\lambda L^T L \\right) m = G^T W d\n$$\n这就是法方程组。问题指定在系统矩阵中加入一个小的岭项 $\\epsilon I$（其中 $I$ 是大小为 $n \\times n$ 的单位矩阵，$\\epsilon=10^{-8}$）以保证数值稳定性，特别是当问题是病态的时候。每次迭代中需要求解的最终 $m$ 的线性系统是：\n$$\n\\left( G^T W G + \\lambda L^T L + \\epsilon I \\right) m = G^T W d\n$$\n关于权重更新 $w_i \\leftarrow \\rho'(r_i)/r_i$ 中 $r=0$ 情况的处理：对于一个通用的惩罚函数 $\\rho$，当 $r_i \\approx 0$ 时，这个表达式在数值上是不稳定的。一个稳健的实现会检查 $|r_i|$ 是否低于某个小的、类似机器ε的容差。如果是，则应将权重设置为其极限值 $w_i \\leftarrow \\rho''(0)$。然而，正如任务A中所推导的，对于 Geman–McClure 惩罚项，其简化表达式 $w(r) = c^4 / (r^2+c^2)^2$ 对于所有 $r$（包括 $r=0$）都是数值稳定且有效的。因此，不需要特殊的条件逻辑；这个单一的公式可以用来计算所有权重。\n\n### 任务C：实现与测试套件\n实现将包括两个相同的求解器函数 `solve_irls` 和 `solve_hq`，分别反映两种概念框架，并将应用于四个测试用例。每个求解器的核心是一个迭代循环，该循环更新权重并求解任务B中推导出的线性系统。对于每个测试用例，将计算两个求解器最终模型向量之间的最大绝对差。由于算法是相同的，这个差值预计为 $0.0$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are used.\n\ndef _generate_test_data(n, N, seed_G, seed_m_true, seed_noise, std_noise, outliers, m_true_is_zero):\n    \"\"\"Generates a single test case dataset.\"\"\"\n    # Construct the first-difference operator L\n    L = np.zeros((n - 1, n))\n    for i in range(n - 1):\n        L[i, i] = -1.0\n        L[i, i + 1] = 1.0\n\n    # Generate G matrix\n    rng_G = np.random.default_rng(seed=seed_G)\n    G = rng_G.standard_normal((N, n))\n    \n    # Scale G rows so that the average row 2-norm is 1\n    row_norms = np.linalg.norm(G, axis=1)\n    scale_factor = 1.0 / np.mean(row_norms)\n    G = G * scale_factor\n\n    # Generate true model m_true\n    if m_true_is_zero:\n        m_true = np.zeros(n)\n    else:\n        rng_m_true = np.random.default_rng(seed=seed_m_true)\n        m_true = rng_m_true.normal(loc=0.0, scale=0.2, size=n)\n\n    # Generate data vector d\n    d_clean = G @ m_true\n    \n    if std_noise > 0:\n        rng_noise = np.random.default_rng(seed=seed_noise)\n        noise = rng_noise.normal(loc=0.0, scale=std_noise, size=N)\n        d = d_clean + noise\n    else:\n        d = d_clean\n\n    for index, amplitude in outliers:\n        d[index] += amplitude\n        \n    return G, L, d\n\ndef _solver_core(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol):\n    \"\"\"The core iterative solver logic.\"\"\"\n    n = G.shape[1]\n    m = m_init.copy()\n    m_prev_norm = 0.0\n    \n    # Store L.T @ L as it's constant\n    LtL = L.T @ L\n\n    for _ in range(max_iter):\n        # 1. Calculate residuals\n        r = G @ m - d\n        \n        # 2. Update weights (for Geman-McClure)\n        # w_i = c^4 / (r_i^2 + c^2)^2\n        w = c**4 / (r**2 + c**2)**2\n        W = np.diag(w)\n        \n        # 3. Form and solve the linear system\n        # (G.T @ W @ G + lambda * L.T @ L + epsilon * I) m = G.T @ W @ d\n        A = G.T @ W @ G + lambda_reg * LtL + epsilon * np.identity(n)\n        b = G.T @ W @ d\n        \n        m_new = np.linalg.solve(A, b)\n        m = m_new\n\n        # 4. Check for convergence\n        m_norm = np.linalg.norm(m)\n        if m_prev_norm > 0:\n            rel_change = np.abs(m_norm - m_prev_norm) / m_prev_norm\n            if rel_change  tol:\n                break\n        m_prev_norm = m_norm\n        \n    return m\n\ndef solve_irls(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol):\n    \"\"\"Solver based on the Iteratively Reweighted Least Squares perspective.\"\"\"\n    return _solver_core(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n\ndef solve_hq(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol):\n    \"\"\"Solver based on the Half-Quadratic Minimization perspective.\"\"\"\n    return _solver_core(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n\ndef solve():\n    \"\"\"Main function to run test suite and print results.\"\"\"\n    # Common parameters\n    n = 5\n    N = 20\n    max_iter = 200\n    tol = 1e-8\n    epsilon = 1e-8\n    m_init = np.zeros(n)\n\n    # Define the test cases from the problem statement.\n    test_cases_params = [\n        # Test 1 (happy path, mild outliers)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.05, \n         'outliers': [(3, 5.0), (12, 5.0)], 'm_true_is_zero': False, \n         'c': 1.0, 'lambda_reg': 10.0},\n        \n        # Test 2 (boundary case, exact data)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.0,\n         'outliers': [], 'm_true_is_zero': True, \n         'c': 1.0, 'lambda_reg': 1.0},\n        \n        # Test 3 (strong outliers)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.05,\n         'outliers': [(3, 5.0), (12, 5.0), (1, 20.0), (7, 20.0), (18, 20.0)], \n         'm_true_is_zero': False, 'c': 1.0, 'lambda_reg': 5.0},\n        \n        # Test 4 (small scale parameter)\n        {'seed_G': 13, 'seed_m_true': 17, 'seed_noise': 19, 'std_noise': 0.05,\n         'outliers': [(3, 5.0), (12, 5.0)], 'm_true_is_zero': False, \n         'c': 0.1, 'lambda_reg': 10.0}\n    ]\n\n    results = []\n    for params in test_cases_params:\n        # Generate data for the current test case\n        G, L, d = _generate_test_data(n=n, N=N, seed_G=params['seed_G'], \n                                    seed_m_true=params['seed_m_true'],\n                                    seed_noise=params['seed_noise'], \n                                    std_noise=params['std_noise'],\n                                    outliers=params['outliers'], \n                                    m_true_is_zero=params['m_true_is_zero'])\n        \n        c = params['c']\n        lambda_reg = params['lambda_reg']\n        \n        # Run both solvers\n        m_irls = solve_irls(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n        m_hq = solve_hq(G, L, d, c, lambda_reg, epsilon, m_init, max_iter, tol)\n        \n        # Calculate and store the maximum absolute difference\n        max_abs_diff = np.max(np.abs(m_irls - m_hq))\n        results.append(max_abs_diff)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}