## 引言
在计算科学，尤其是在[地球物理反演](@entry_id:749866)中，我们常常面临从含有噪声和异常值的数据中提取可靠模型信息的挑战。传统的[最小二乘法](@entry_id:137100)在这种情况下表现不佳，因为它对异常数据点极为敏感。迭代重[加权最小二乘法](@entry_id:177517)（Iteratively Reweighted Least Squares, IRLS）应运而生，它是一种功能强大且灵活的优化框架，通过自适应地调整数据或模型参数的权重，有效地解决了这一难题，并能进一步用于构建具有特定结构（如稀疏性）的模型。本文旨在为读者提供一个关于IRLS的全面指南。在接下来的章节中，我们将首先在“原理与机制”一章中，从[加权最小二乘法](@entry_id:177517)出发，逐步揭示IRLS的核心思想、数学推导及其在[稳健估计](@entry_id:261282)和稀疏促进中的双重角色。随后，“应用与跨学科联系”一章将展示IRLS在地球物理、统计学、机器学习等多个领域的实际应用案例，突显其普适性。最后，“动手实践”部分将提供具体的计算问题，帮助读者将理论知识转化为实践技能。

## 原理与机制

在上一章引言的基础上，本章将深入探讨迭代重[加权最小二乘法](@entry_id:177517)（Iteratively Reweighted Least Squares, IRLS）的核心工作原理与数学机制。我们将从经典的[加权最小二乘法](@entry_id:177517)出发，阐明其局限性，进而引出IRLS作为一种更通用、更强大的优化策略的必要性。本章的目标是系统性地构建一个关于IRLS的理论框架，涵盖其在[稳健估计](@entry_id:261282)和稀疏反演两大领域的应用，并探讨其收敛性。

### 从加权最小二乘到其局限性

在[计算地球物理学](@entry_id:747618)中，许多反演问题可以被线性化或本身就是线性的，其形式为求解一个超定或欠定[线性方程组](@entry_id:148943) $Gm \approx d$，其中 $m \in \mathbb{R}^{n_m}$ 是待求的模型参数向量（例如，地下介质的速度或密度），$d \in \mathbb{R}^{n_d}$ 是观测数据向量（例如，[地震波](@entry_id:164985)走时），$G \in \mathbb{R}^{n_d \times n_m}$ 是敏感度矩阵或称雅可比矩阵，它描述了模型参数的微小变化如何影响预测数据。

当观测数据中的噪声是高斯的、零均值且不相关的，我们通常通过最小化残差的欧几里得范数（$L_2$范数）的平方来求解，即[普通最小二乘法](@entry_id:137121)（Ordinary Least Squares, OLS）。然而，在更现实的情况下，数据点的噪声水平可能不同（[异方差性](@entry_id:136378)）或相互关联。此时，一个更合理的方法是[加权最小二乘法](@entry_id:177517)（Weighted Least Squares, WLS）。WLS通过引入一个**加权矩阵** $W_d$ 来最小化加权[残差范数](@entry_id:754273)，[目标函数](@entry_id:267263)形如：

$J(m) = \frac{1}{2}\| W_d(Gm - d) \|_2^2$

这里，$W_d$ 通常是一个[对角矩阵](@entry_id:637782)，其对角元素反比于对应数据点的噪声[方差](@entry_id:200758)，作用是“白化”残差，使得噪声较大的数据点在[目标函数](@entry_id:267263)中的权重较小。从概率论的角度看，如果噪声是高斯的，其[协方差矩阵](@entry_id:139155)为 $C_d$，那么[最大似然估计](@entry_id:142509)等价于选择权重矩阵 $W$ 满足 $W^T W = C_d^{-1}$。

为了找到使 $J(m)$ 最小化的模型 $m$，我们需求解其梯度为零的平稳点条件 $\nabla_m J(m) = 0$。通过链式法则，我们可以推导出这个条件  ：

$J(m) = \frac{1}{2}(Gm - d)^T W_d^T W_d (Gm - d)$

$\nabla_m J(m) = G^T W_d^T W_d (Gm - d)$

令梯度为零，我们得到**正规方程组**（Normal Equations）：

$(G^T W_d^T W_d G) m = G^T W_d^T W_d d$

这是一个关于 $m$ 的[线性方程组](@entry_id:148943)。为了使该[方程组](@entry_id:193238)存在唯一解，矩阵 $G^T W_d^T W_d G$ 必须是可逆的。由于权重矩阵 $W_d^T W_d$ 通常是正定的（因为它代表逆协[方差](@entry_id:200758)），唯一解存在的充分必要条件是矩阵 $G$ 具有**列满秩**，即其所有列向量线性无关 。

然而，传统的WLS方法有一个根本性的限制：它假设权重矩阵 $W_d$ 是**固定的**和**先验已知的**。在许多地球物理问题中，数据的“质量”或“可靠性”并不仅仅取决于仪器的[固有噪声](@entry_id:261197)，还可能取决于当前的模型估计 $m$ 本身。例如，在地震[走时层析成像](@entry_id:756150)中，可能会出现“周波跳跃”（cycle-skipping）现象，即观测到的波至与模型预测的波至之间相差一个或多个波长。这种偏差极大的数据点就是所谓的**离群值**（outlier）。一个数据点是否成为离群值，取决于当前模型 $m$ 预测的走时与观测走时的差异。当我们调整模型 $m$ 时，预测走时会改变，这可能导致一个原本拟合良好的数据点变成离群值，反之亦然。这种与状态相关的离群值无法通过一个固定的权重矩阵 $W_d$ 来有效处理，因为它不能根据模型迭代过程中残差的变化自适应地调整对数据点的信任程度 。这正是IRLS方法要解决的核心问题。

### M估计与IRLS的核心机制

为了解决固定权重的局限性，我们需要一个更通用的框架，能够根据残差的大小自适应地调整惩罚力度。这个框架就是**M估计**（M-estimation）。在M估计中，我们不再最小化残差的平方和，而是最小化一个更广义的惩[罚函数](@entry_id:638029) $\rho(r)$ 的总和：

$J(m) = \sum_{i=1}^{n_d} \rho(r_i(m))$

其中 $r_i(m) = (Gm - d)_i$ 是第 $i$ 个残差。[普通最小二乘法](@entry_id:137121)是M估计的一个特例，对应于二次惩罚函数 $\rho(r) = \frac{1}{2}r^2$。

同样，我们通过令目标函数 $J(m)$ 的梯度为零来寻找最优解。定义**[影响函数](@entry_id:168646)**（influence function）为 $\psi(r) = \rho'(r)$，利用链式法则，我们得到平稳点条件：

$\nabla_m J(m) = G^T \Psi(r(m)) = 0$

其中 $\Psi(r(m))$ 是一个向量，其第 $i$ 个元素为 $\psi(r_i(m))$。由于 $\psi$ 函数通常是[非线性](@entry_id:637147)的，这是一个关于 $m$ 的非线性方程组，无法直接求解。

**迭代重[加权最小二乘法](@entry_id:177517)（IRLS）** 的核心思想，就是通过一个迭代过程来求解这个非线性方程组。其关键技巧在于将[影响函数](@entry_id:168646) $\psi(r_i)$ 巧妙地分解为一个权重 $w_i$ 和残差 $r_i$ 的乘积。我们定义权重函数 $w(r)$ 如下  ：

$w(r) = \frac{\psi(r)}{r}$

通过这个定义，平稳点条件 $G^T \Psi(r(m)) = 0$ 可以被重写为：

$G^T W(m) r(m) = 0$

其中 $W(m)$ 是一个[对角矩阵](@entry_id:637782)，其对角元素为 $w(r_i(m))$。将 $r(m) = Gm - d$ 代入，我们得到：

$(G^T W(m) G) m = G^T W(m) d$

这个方程形式上与WLS的正规方程非常相似，但有一个本质区别：这里的权重矩阵 $W(m)$ **依赖于模型 $m$**，这使得方程仍然是关于 $m$ 的[非线性方程](@entry_id:145852)。

IRLS通过一种“冻结”权重的策略将此[非线性](@entry_id:637147)问题转化为一系列线性问题。在第 $k$ 次迭代中，算法使用当前的模型估计 $m^{(k)}$ 计算出残差 $r^{(k)} = Gm^{(k)} - d$，然后基于这些残差计算出一组固定的权重 $w_i^{(k)} = w(r_i^{(k)})$，构成对角矩阵 $W^{(k)}$。随后，算法求解一个标准的加权[最小二乘问题](@entry_id:164198)来获得下一次迭代的模型 $m^{(k+1)}$：

$m^{(k+1)} = \arg\min_m \frac{1}{2} \sum_{i=1}^{n_d} w_i^{(k)} ( (Gm)_i - d_i )^2$

其解由以下线性正规方程组给出：

$(G^T W^{(k)} G) m^{(k+1)} = G^T W^{(k)} d$

这个过程不断重复，通过迭代更新权重和求解加权[最小二乘问题](@entry_id:164198)，逐步逼近原始[非线性](@entry_id:637147)问题的解。这个迭代过程清晰地展示了“迭代”和“重加权”的含义，也区分了IRLS与固定权重的WLS 。

为了更具体地理解单步计算，考虑以下实例 ：
假设在某次迭代中，我们有矩阵 $A$（等同于此处的 $G$），权重矩阵 $W$（等同于 $W^{(k)}$）和数据向量 $b$（等同于 $d$）如下：
$A = \begin{pmatrix} 1  0 \\ 1  1 \\ 0  1 \end{pmatrix}, \quad W = \operatorname{diag}(2,1,3), \quad b = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$
为了求解 $m^{(k+1)}$，我们需要计算 $A^T W A$ 和 $A^T W b$。
$A^T W A = \begin{pmatrix} 1  1  0 \\ 0  1  1 \end{pmatrix} \begin{pmatrix} 2  0  0 \\ 0  1  0 \\ 0  0  3 \end{pmatrix} \begin{pmatrix} 1  0 \\ 1  1 \\ 0  1 \end{pmatrix} = \begin{pmatrix} 3  1 \\ 1  4 \end{pmatrix}$
$A^T W b = \begin{pmatrix} 1  1  0 \\ 0  1  1 \end{pmatrix} \begin{pmatrix} 2  0  0 \\ 0  1  0 \\ 0  0  3 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} = \begin{pmatrix} 4 \\ 11 \end{pmatrix}$
然后求解线性方程组 $\begin{pmatrix} 3  1 \\ 1  4 \end{pmatrix} m^{(k+1)} = \begin{pmatrix} 4 \\ 11 \end{pmatrix}$，得到 $m^{(k+1)} = \begin{pmatrix} 5/11 \\ 29/11 \end{pmatrix}$。这就是[IRLS算法](@entry_id:750839)中的一次迭代更新。

### IRLS的应用与诠释

IRLS框架极为灵活，其具体行为完全由惩[罚函数](@entry_id:638029) $\rho(r)$ 的选择决定。这使得IRLS能够被应用于解决两类重要问题：[稳健估计](@entry_id:261282)和稀疏反演。

#### [稳健估计](@entry_id:261282)：对数据残差进行重加权

当IRLS用于处理数据中的离群值时，其核心在于选择一个能够降低大残差影响的惩[罚函数](@entry_id:638029) $\rho(r)$。

**稳健性的基本原理**

稳健性的关键在于[影响函数](@entry_id:168646) $\psi(r) = \rho'(r)$ 的性质。对于[普通最小二乘法](@entry_id:137121)，$\rho(r) = \frac{1}{2}r^2$，其[影响函数](@entry_id:168646) $\psi(r) = r$ 是无界的。这意味着一个残差极大的离群值会对梯度产生极大的影响，从而将解“拉向”自己。

为了获得稳健性，我们需要选择一个[影响函数](@entry_id:168646) $\psi(r)$，其增长速度慢于[线性增长](@entry_id:157553)，甚至是**有界的**。当 $\psi(r)$ 有界时，无论残差 $|r|$ 多大，它对梯度的贡献都被限制在一个常数范围内。从权重的角度看，$w(r) = \psi(r)/r$。如果 $\psi(r)$ 的增长慢于 $r$，那么当 $|r|$ 增大时，权重 $w(r)$ 就会减小。这就实现了对大残差（离群值）的**自动降权** 。

**典型的稳健惩[罚函数](@entry_id:638029)**

1.  **$L_p$ 范数 ($1  p  2$)**:
    选择惩[罚函数](@entry_id:638029)为 $\rho(r) = \frac{1}{p}|r|^p$。其[影响函数](@entry_id:168646)为 $\psi(r) = |r|^{p-1}\mathrm{sgn}(r)$。对应的权重函数为：
    $w(r) = \frac{|r|^{p-1}\mathrm{sgn}(r)}{r} = |r|^{p-2}$
    由于 $p  2$，指数 $p-2$ 为负。因此，当残差的[绝对值](@entry_id:147688) $|r|$ 增大时，其权重会减小，从而达到[稳健估计](@entry_id:261282)的效果。当 $p=2$ 时，权重恒为1，退化为[普通最小二乘法](@entry_id:137121) 。

2.  **Huber 惩[罚函数](@entry_id:638029)**:
    Huber损失是一种混合惩[罚函数](@entry_id:638029)，它在小残差区域表现得像 $L_2$ 范数，在大残差区域表现得像 $L_1$ 范数，从而结合了二者的优点。其定义为：
    $\rho_{\delta}(r) = \begin{cases} \frac{1}{2} r^{2},  |r| \leq \delta, \\ \delta |r| - \frac{1}{2} \delta^{2},  |r| > \delta. \end{cases}$
    其中 $\delta$ 是一个阈值参数。其对应的权重函数可以被推导为 ：
    $w(r) = \min(1, \frac{\delta}{|r|})$
    这个权重函数直观地体现了Huber损失的混合特性：对于残差小于阈值 $\delta$ 的“正常”数据点，权重为1，等同于最小二乘；对于残差大于 $\delta$ 的“离群”数据点，权重小于1并随着 $|r|$ 的增大而减小，起到了抑制离群值的作用。例如，若 $\delta=1$，对于残[差集](@entry_id:140904)合 $\{0.2, 2, 20\}$，对应的Huber权重为 $\{1, 0.5, 0.05\}$，而最小二乘的权重始终为 $\{1, 1, 1\}$ 。

**概率论诠释：[高斯尺度混合](@entry_id:749760)模型**

许多稳健的惩[罚函数](@entry_id:638029)都可以从概率论的角度得到深刻的解释。它们往往对应于假设数据噪声服从**[重尾分布](@entry_id:142737)**（heavy-tailed distribution），而不是高斯分布。[重尾分布](@entry_id:142737)允许以比高斯分布高得多的概率出现极端值（即离群值）。

一个强大的概念是**[高斯尺度混合](@entry_id:749760)（Gaussian Scale Mixture, GSM）**。它将一个[重尾分布](@entry_id:142737)表示为一个[高斯分布](@entry_id:154414)与一个尺度（或[方差](@entry_id:200758)）变量的混合。具体来说，我们假设每个残差 $r_i$ 来自一个高斯分布 $\mathcal{N}(0, \sigma^2/\tau_i)$，但其精度（[方差](@entry_id:200758)的倒数）$\tau_i$ 本身是一个[随机变量](@entry_id:195330)，服从某个[混合分布](@entry_id:276506) $p(\tau_i)$。通过对 $\tau_i$ 进行积分，可以得到 $r_i$ 的[边际分布](@entry_id:264862)。

*   **[学生t分布](@entry_id:267063) ([Student's t-distribution](@entry_id:142096))**: 如果我们假设潜在精度 $\tau_i$ 服从一个**伽马[分布](@entry_id:182848)**（Gamma distribution），那么残差 $r_i$ 的[边际分布](@entry_id:264862)就是[学生t分布](@entry_id:267063)。学生t分布是典型的[重尾分布](@entry_id:142737)。在这种情况下，M估计的目标函数 $\rho(r)$ 对应于[学生t分布](@entry_id:267063)的[负对数似然](@entry_id:637801) ：
    $\rho(r) = \frac{\nu + 1}{2} \ln\left(1 + \frac{r^{2}}{\nu \sigma^{2}}\right)$
    其中 $\nu$ 是自由度，$\sigma$是[尺度参数](@entry_id:268705)。对应的IRLS权重为 ：
    $w(r) = \frac{\nu + 1}{\nu + r^2/\sigma^2}$
    从这个角度看，[IRLS算法](@entry_id:750839)可以被严格地解释为**期望-最大化（EM）算法**。在E步，我们根据当前残差估计潜在精度 $\tau_i$ 的[期望值](@entry_id:153208)；在[M步](@entry_id:178892)，我们利用这个期望精度作为权重，求解一个加权最小二乘问题 。

*   **[柯西分布](@entry_id:266469) (Cauchy distribution)**: 柯西分布是另一个[重尾分布](@entry_id:142737)的例子，其尾部比[学生t分布](@entry_id:267063)更“重”。其对应的惩[罚函数](@entry_id:638029)为：
    $\rho(r) = \frac{c^{2}}{2}\,\ln\bigl(1 + (r/c)^{2}\bigr)$
    其中 $c$ 是[尺度参数](@entry_id:268705)。对应的IRLS权重为 ：
    $w(r) = \frac{1}{1 + (r/c)^2}$
    该[分布](@entry_id:182848)的概率密度以[幂律](@entry_id:143404) $|r|^{-c^2}$ 的形式衰减，远慢于高斯分布的指数衰减，因此对极端离群值具有极高的容忍度。

#### 稀疏促进：对模型参数进行重加权

IRLS框架的强大之处在于，其重加权的对象不限于数据残差，也可以是模型参数本身。当我们将惩[罚函数](@entry_id:638029)应用于模型参数 $m_j$ 而非残差 $r_i$ 时，IRLS就成为了一种[促进模型](@entry_id:147560)**[稀疏性](@entry_id:136793)**（sparsity）的有力工具。稀疏性意味着模型的大部分分量为零，这在许多地球物理问题（如压缩感知、[基追踪](@entry_id:200728)）中是期望的性质。

此时的[目标函数](@entry_id:267263)变为：

$J(m) = \frac{1}{2}\|Gm - d\|_2^2 + \lambda \sum_{j=1}^{n_m} \rho(m_j)$

*   **$L_1$ 正则化 (LASSO)**:
    当惩罚函数为 $L_1$ 范数 $\rho(m_j) = |m_j|$ 时，我们得到著名的LASSO（Least Absolute Shrinkage and Selection Operator）问题。应用IRLS框架，其[影响函数](@entry_id:168646)为 $\psi(m_j) = \mathrm{sgn}(m_j)$，对应的权重为：
    $w_j = \frac{1}{|m_j|}$
    在每次迭代中，我们求解一个加权的[Tikhonov正则化](@entry_id:140094)问题，其权重 $w_j^{(k)} = 1/|m_j^{(k)}|$ 会对当前迭代中[绝对值](@entry_id:147688)较小的模型分量施加非常大的惩罚，从而驱使它们向零收缩，而对[绝对值](@entry_id:147688)较大的分量施加较小的惩罚，保留其数值。
    与[稳健估计](@entry_id:261282)类似，这也有深刻的[贝叶斯解释](@entry_id:265644)。$L_1$ 正则化等价于为模型参数假定一个**拉普拉斯先验**（Laplace prior）。而[拉普拉斯分布](@entry_id:266437)本身可以被表示为一个[高斯尺度混合](@entry_id:749760)，其中潜在[方差](@entry_id:200758)服从[指数分布](@entry_id:273894)。因此，用于$L_1$正则化的[IRLS算法](@entry_id:750839)同样可以被看作是一个[EM算法](@entry_id:274778) 。

*   **超越$L_1$：逼近$L_0$范数**:
    $L_1$ 范数是促进[稀疏性](@entry_id:136793)的最简单的凸函数，但真正的[稀疏性](@entry_id:136793)度量是 $L_0$ 伪范数，即模型中非零元素的个数。由于 $L_0$ 范数是非凸、非连续的，直接优化极为困难。因此，研究者们提出了许多非凸的惩罚函数来更好地逼近 $L_0$ 范数，例如对数和惩罚：
    $\rho(m_j) = \log(|m_j| + \epsilon)$
    对于这类非凸问题，[IRLS算法](@entry_id:750839)（此时常被称为重加权$L_1$算法）同样适用。其权重为 $w_j^{(k)} = 1/(|m_j^{(k)}| + \epsilon)$。这种方法比普通$L_1$更有效地促进[稀疏性](@entry_id:136793)，因为它对小系数施加的惩罚相对更强，而对大系数的收缩偏置更小。从理论上看，这种算法可以被理解为一种**主化-最小化（Majorization-Minimization, MM）** 算法。它在每次迭代中用一个加权的$L_1$范数（一个凸函数）来构造原始[非凸惩罚](@entry_id:752554)函数的一个上界（主化函数），然后最小化这个[上界](@entry_id:274738) 。

### 收敛性与理论分析

一个[迭代算法](@entry_id:160288)的实用性最终取决于其收敛性。IRLS的收敛行为与目标函数 $J(m)$ 的性质密切相关。

我们可以将IRLS的更新步骤看作一个**[不动点迭代](@entry_id:749443)**过程 $m^{(k+1)} = T(m^{(k)})$，其中映射 $T$ 定义为：
$T(m) = (G^T W(m) G)^{-1} G^T W(m) d$
[不动点](@entry_id:156394)（即解）$m^*$ 满足 $m^* = T(m^*)$。根据[不动点理论](@entry_id:157862)，如果映射 $T$ 在解 $m^*$ 附近是一个**[压缩映射](@entry_id:139989)**（contraction mapping），即其雅可比矩阵的[谱范数](@entry_id:143091)小于1，那么从一个足够接近 $m^*$ 的初始点出发，迭代序列必将收敛到 $m^*$。

**凸问题下的收敛性**

当惩罚函数 $\rho(r)$ 是**严格凸**且光滑时（例如Huber损失或$L_p$范数，$p1$），[目标函数](@entry_id:267263) $J(m)$ 在适当条件下（例如 $G$ 列满秩或加入了[Tikhonov正则化](@entry_id:140094)）也是严格凸的。一个严格凸且强制（coercive）的函数具有唯一的[全局最小值](@entry_id:165977)。

在这种“良好”的情况下：
1.  IRLS的每一步迭代方向都是[目标函数](@entry_id:267263)的[下降方向](@entry_id:637058)。
2.  通过结合适当的[全局化策略](@entry_id:177837)（如线搜索或[信赖域方法](@entry_id:138393)），可以保证IRLS生成的序列 $J(m^{(k)})$ 单调递减，并最终收敛到该唯一的全局最小值 。

对于$L_p$范数惩罚（$1  p \leq 2$）这一重要特例，可以进行更精细的局部收敛分析。通过计算[不动点](@entry_id:156394)映射的[雅可比矩阵](@entry_id:264467)在解 $m^*$ 处的值，可以证明其收缩因子（雅可比矩阵的[谱范数](@entry_id:143091)）恰好为 $2-p$。由于 $1  p \leq 2$，收缩因子 $2-p$ 的范围是 $[0, 1)$。只要 $p1$，收缩因子就严格小于1，从而保证了局部收敛 。当 $p$ 越接近1，收缩因子越接近1，收敛越慢；当 $p=2$（最小二乘）时，收缩因子为0，一步收敛。

**非凸问题下的挑战**

当惩罚函数 $\rho(r)$ 是**非凸**的时，情况变得复杂。例如，当[影响函数](@entry_id:168646) $\psi(r)$ 是**红降的**（redescending），即当 $|r| \to \infty$ 时 $\psi(r) \to 0$（如Tukey's biweight惩罚），这意味着 $\rho(r)$ 在 $|r|$ 很大时会变平，甚至是下降的。这种惩罚函数对极端离群值具有最强的抑制能力（完全忽略它们的影响），但代价是牺牲了[目标函数](@entry_id:267263)的[凸性](@entry_id:138568)。

对于非凸问题：
1.  [目标函数](@entry_id:267263) $J(m)$ 可能拥有多个[局部极小值](@entry_id:143537)、极大值或[鞍点](@entry_id:142576)。
2.  IRLS作为一个局部[优化方法](@entry_id:164468)，其收敛结果将强烈依赖于初始猜测 $m^{(0)}$。从不同的初始点出发，算法可能会收敛到不同的[局部极小值](@entry_id:143537)。
3.  在这种情况下，不能保证找到[全局最优解](@entry_id:175747)，但IRLS仍然是寻找高质量局部解的有效启发式方法 。

总而言之，IRLS是一个强大而灵活的框架，它将一类广泛的（可能[非线性](@entry_id:637147)的、非凸的）[优化问题](@entry_id:266749)转化为一系列易于求解的线性加权[最小二乘问题](@entry_id:164198)。通过选择不同的惩[罚函数](@entry_id:638029)和重加权对象，IRLS能够有效地解决[稳健估计](@entry_id:261282)和稀疏促进这两大类在地球物理及其他科学领域中至关重要的问题。理解其背后的数学机制和收敛特性，是成功应用该方法的关键。