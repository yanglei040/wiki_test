{
    "hands_on_practices": [
        {
            "introduction": "Many powerful deep learning concepts have deep roots in classical signal processing and optimization theory. This first practice builds a crucial bridge between these worlds. By analyzing the well-known Tikhonov regularization for a deconvolution problem, you will demonstrate its equivalence to the optimal Wiener filter and, subsequently, interpret this classical solution as a single, linear convolutional layer . This exercise provides a solid theoretical foundation, demystifying the link between traditional inverse problem solvers and the basic building blocks of neural networks.",
            "id": "3583423",
            "problem": "Consider a one-dimensional cyclic convolutional forward model on a periodic grid of length $N \\in \\mathbb{N}$, where the data $\\mathbf{d} \\in \\mathbb{C}^{N}$ are generated from a model $\\mathbf{m} \\in \\mathbb{C}^{N}$ by $\\mathbf{d} = \\mathbf{F}\\mathbf{m} + \\mathbf{n}$, with $\\mathbf{F}$ a circulant matrix induced by convolution with a kernel $\\mathbf{h} \\in \\mathbb{C}^{N}$, and $\\mathbf{n}$ additive noise. Let $\\mathbf{U} \\in \\mathbb{C}^{N \\times N}$ denote the unitary Discrete Fourier Transform (DFT) matrix, so that $\\mathbf{U}^{*}\\mathbf{F}\\mathbf{U} = \\operatorname{diag}(\\hat{h}(k))$ is diagonal with entries $\\hat{h}(k)$, the DFT of $\\mathbf{h}$. Consider Tikhonov regularization with identity penalty, i.e., the minimization of the objective $J(\\mathbf{m}) = \\|\\mathbf{F}\\mathbf{m} - \\mathbf{d}\\|_{2}^{2} + \\lambda \\|\\mathbf{m}\\|_{2}^{2}$, for a scalar $\\lambda > 0$.\n\n- Derive the normal equations and, using the diagonalization of circulant operators by the DFT, obtain the solution in the Fourier domain $\\hat{m}(k)$ in terms of $\\hat{h}(k)$, $\\hat{d}(k)$, and $\\lambda$.\n\n- Define the power spectrum of the kernel as $|\\hat{h}(k)|^{2}$. Derive the condition number of the normal matrix $\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I}$ in terms of the extremal values of $|\\hat{h}(k)|^{2}$ and $\\lambda$, and explain how small values of $|\\hat{h}(k)|^{2}$ impact the stability of the inversion when $\\lambda$ varies.\n\n- Assume the ground-truth model $\\mathbf{m}_{\\mathrm{true}}$ and the noise $\\mathbf{n}$ are independent, zero-mean, wide-sense stationary and white, with constant power spectra $S_{m}(k) \\equiv \\sigma_{m}^{2} > 0$ and $S_{n}(k) \\equiv \\sigma_{n}^{2} > 0$, respectively. Compute the expected per-frequency mean-squared error $\\mathbb{E}\\left[|\\hat{m}(k) - \\hat{m}_{\\mathrm{true}}(k)|^{2}\\right]$ as a function of $|\\hat{h}(k)|^{2}$ and $\\lambda$, and determine the constant regularization parameter $\\lambda^{\\star}$ that minimizes the average of this error across frequencies.\n\n- Now interpret Tikhonov regularization as a single linear layer of a Convolutional Neural Network (CNN), which is a CNN with a single linear convolutional layer acting in the data domain. Constrain the linear layer to have frequency response $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\varphi\\!\\left(|\\hat{h}(k)|^{2}\\right)$, where $\\varphi:[0,\\infty)\\to\\mathbb{R}$ is a scalar function that only depends on $s = |\\hat{h}(k)|^{2}$ and $\\overline{\\hat{h}(k)}$ denotes complex conjugation. Under the same statistical assumptions on $\\mathbf{m}_{\\mathrm{true}}$ and $\\mathbf{n}$, determine the function $\\varphi(s)$ that minimizes the expected mean-squared error of the estimator $\\hat{m}(k) = \\hat{G}(k)\\hat{d}(k)$. Provide your final result as the pair $\\left(\\lambda^{\\star}, \\varphi(s)\\right)$ as a single row matrix. No rounding is required, and no units are necessary.",
            "solution": "We begin from the Tikhonov objective $J(\\mathbf{m}) = \\|\\mathbf{F}\\mathbf{m} - \\mathbf{d}\\|_{2}^{2} + \\lambda \\|\\mathbf{m}\\|_{2}^{2}$, which is a strictly convex quadratic functional for $\\lambda > 0$. The first-order optimality (normal equations) follows by setting the gradient to zero, yielding\n$$(\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I})\\mathbf{m} = \\mathbf{F}^{*}\\mathbf{d}.$$\nUsing the diagonalization of circulant matrices by the unitary Discrete Fourier Transform (DFT) matrix $\\mathbf{U}$, we have $\\mathbf{F} = \\mathbf{U}^{*}\\operatorname{diag}(\\hat{h}(k))\\mathbf{U}$ and hence\n$$\\mathbf{F}^{*}\\mathbf{F} = \\mathbf{U}^{*}\\operatorname{diag}(|\\hat{h}(k)|^{2})\\mathbf{U}.$$\nLeft-multiplying the normal equations by $\\mathbf{U}$ and defining $\\hat{m} = \\mathbf{U}\\mathbf{m}$ and $\\hat{d} = \\mathbf{U}\\mathbf{d}$ yields\n$$\\left[\\operatorname{diag}(|\\hat{h}(k)|^{2}) + \\lambda \\mathbf{I}\\right]\\hat{m} = \\operatorname{diag}(\\overline{\\hat{h}(k)})\\hat{d}.$$\nThis decouples across frequencies $k$, giving the spectral solution\n$$\\hat{m}(k) = \\frac{\\overline{\\hat{h}(k)}\\,\\hat{d}(k)}{|\\hat{h}(k)|^{2} + \\lambda}.$$\n\nNext, we analyze conditioning. The normal matrix is $\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I} = \\mathbf{U}^{*}\\operatorname{diag}(|\\hat{h}(k)|^{2} + \\lambda)\\mathbf{U}$, so its eigenvalues are $\\{|\\hat{h}(k)|^{2} + \\lambda\\}_{k}$. The $2$-norm condition number is therefore\n$$\\kappa_{2}(\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I}) = \\frac{\\max_{k}\\left(|\\hat{h}(k)|^{2}\\right) + \\lambda}{\\min_{k}\\left(|\\hat{h}(k)|^{2}\\right) + \\lambda}.$$\nSmall values of $|\\hat{h}(k)|^{2}$ increase the condition number when $\\lambda$ is small, leading to amplification of noise and instability. Increasing $\\lambda$ lifts the smallest eigenvalues by $\\lambda$, reducing $\\kappa_{2}$ and improving stability, at the expense of introducing bias.\n\nWe now compute the expected mean-squared error under the assumed stochastic model. Let the true model $\\mathbf{m}_{\\mathrm{true}}$ and the noise $\\mathbf{n}$ be independent, zero mean, white, with constant power spectra $S_{m}(k) \\equiv \\sigma_{m}^{2}$ and $S_{n}(k) \\equiv \\sigma_{n}^{2}$. In the Fourier domain,\n$$\\hat{d}(k) = \\hat{h}(k)\\,\\hat{m}_{\\mathrm{true}}(k) + \\hat{n}(k).$$\nThe Tikhonov estimator is\n$$\\hat{m}(k) = \\frac{\\overline{\\hat{h}(k)}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{d}(k) = \\frac{|\\hat{h}(k)|^{2}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{m}_{\\mathrm{true}}(k) + \\frac{\\overline{\\hat{h}(k)}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{n}(k).$$\nDefine $s = |\\hat{h}(k)|^{2} \\geq 0$ for notational convenience. The estimation error at frequency $k$ is\n$$\\hat{e}(k) = \\hat{m}(k) - \\hat{m}_{\\mathrm{true}}(k) = -\\frac{\\lambda}{s + \\lambda}\\,\\hat{m}_{\\mathrm{true}}(k) + \\frac{\\overline{\\hat{h}(k)}}{s + \\lambda}\\,\\hat{n}(k).$$\nBy independence and zero mean, the expected per-frequency mean-squared error is\n$$\\mathbb{E}\\left[|\\hat{e}(k)|^{2}\\right] = \\left(\\frac{\\lambda}{s + \\lambda}\\right)^{2}\\mathbb{E}\\left[|\\hat{m}_{\\mathrm{true}}(k)|^{2}\\right] + \\frac{|\\hat{h}(k)|^{2}}{(s + \\lambda)^{2}}\\mathbb{E}\\left[|\\hat{n}(k)|^{2}\\right] = \\frac{\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}}{(s + \\lambda)^{2}}.$$\nThe average over $k$ of this expression is minimized by minimizing each term in $k$ because the problem is separable in $k$. Differentiating with respect to $\\lambda$,\n$$\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\left[\\frac{\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}}{(s + \\lambda)^{2}}\\right] = \\frac{2(s + \\lambda)\\left[\\lambda \\sigma_{m}^{2}(s + \\lambda) - (\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2})\\right]}{(s + \\lambda)^{4}} = \\frac{2s\\left(\\lambda \\sigma_{m}^{2} - \\sigma_{n}^{2}\\right)}{(s + \\lambda)^{3}}.$$\nSetting the derivative to zero and noting $s \\geq 0$ yields the unique minimizer\n$$\\lambda^{\\star} = \\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}}.$$\n\nFinally, interpret Tikhonov as a single linear layer of a Convolutional Neural Network (CNN) acting in the data domain with frequency response $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\varphi(s)$, where $s = |\\hat{h}(k)|^{2}$. The estimator becomes\n$$\\hat{m}(k) = \\hat{G}(k)\\hat{d}(k) = \\overline{\\hat{h}(k)}\\,\\varphi(s)\\left[\\hat{h}(k)\\hat{m}_{\\mathrm{true}}(k) + \\hat{n}(k)\\right] = s\\,\\varphi(s)\\,\\hat{m}_{\\mathrm{true}}(k) + \\overline{\\hat{h}(k)}\\,\\varphi(s)\\,\\hat{n}(k).$$\nThe error is\n$$\\hat{e}(k) = \\left[s\\,\\varphi(s) - 1\\right]\\hat{m}_{\\mathrm{true}}(k) + \\overline{\\hat{h}(k)}\\,\\varphi(s)\\,\\hat{n}(k),$$\nand its expected power is\n$$\\mathbb{E}\\left[|\\hat{e}(k)|^{2}\\right] = \\left|s\\,\\varphi(s) - 1\\right|^{2}\\sigma_{m}^{2} + s\\,|\\varphi(s)|^{2}\\sigma_{n}^{2}.$$\nMinimizing this with respect to real $\\varphi(s)$ for each fixed $s \\geq 0$ (the expression is convex in $\\varphi$) gives the first-order condition\n$$2\\left[s\\,\\varphi(s) - 1\\right]s\\,\\sigma_{m}^{2} + 2s\\,\\varphi(s)\\,\\sigma_{n}^{2} = 0,$$\nwhich simplifies to\n$$\\varphi(s)\\left[s^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}\\right] = s\\,\\sigma_{m}^{2}.$$\nThus the optimal scalar function is\n$$\\varphi(s) = \\frac{\\sigma_{m}^{2}}{s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2}}.$$\nWith this choice, the linear CNN implements the Wiener filter $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\sigma_{m}^{2}/\\left(s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2}\\right)$, which coincides with Tikhonov when $\\lambda = \\sigma_{n}^{2}/\\sigma_{m}^{2}$.\n\nIn summary, the optimal regularization parameter and the optimal CNN scalar frequency mapping are $\\lambda^{\\star} = \\sigma_{n}^{2}/\\sigma_{m}^{2}$ and $\\varphi(s) = \\sigma_{m}^{2}/(s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2})$, respectively.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}} & \\frac{\\sigma_{m}^{2}}{s\\,\\sigma_{m}^{2} + \\sigma_{n}^{2}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Having established a link between classical methods and simple networks, we now scale up to a modern, powerful architecture used for direct geophysical imaging. The U-Net is a workhorse for a wide range of image-to-image tasks, but its effectiveness depends on careful design. This practice guides you to analyze the relationship between a U-Net's structure—specifically its depth and kernel sizes—and its theoretical receptive field . By deriving this relationship from first principles, you will forge a direct connection between abstract architectural choices and a critical physical constraint: the maximum scale of subsurface features the network can resolve from the data.",
            "id": "3583499",
            "problem": "Consider a one-dimensional variant of a U-Net-style Convolutional Neural Network (CNN), applied along the seismic time axis to map a shot-gather input to a one-dimensional vertical profile of seismic velocity. Assume the following architecture and conventions, which are standard in practice and scientifically plausible for vertical profiling when offsets are not the limiting factor:\n\n- The network is strictly one-dimensional along time, with input sampling interval $\\Delta t$ (in seconds).\n- The encoder has $L$ resolution-reduction stages. At each encoder stage $l \\in \\{1,\\dots,L\\}$, there are two convolution layers with kernel size $k$ (odd, stride $1$, dilation $1$, and zero-padding chosen to preserve length), followed by max-pooling with kernel size $2$ and stride $2$.\n- At the bottleneck (after $L$ poolings), there are two convolution layers with kernel size $k$ (stride $1$, dilation $1$, same-padding).\n- The decoder has $L$ resolution-increase stages. Each decoder stage upsamples by a factor of $2$ using nearest-neighbor upsampling (no kernel), concatenates with the corresponding encoder feature map (skip connection), and then applies two convolution layers with kernel size $k$ (stride $1$, dilation $1$, same-padding).\n- A final $1 \\times 1$ convolution maps features to the output (kernel size $1$).\n- All convolutions are linear and time-invariant; there are no dilations beyond $1$.\n\nYou may use the following well-tested facts:\n\n- For a sequence of layers indexed in forward order by $i$, each with kernel size $k_i$ and stride $s_i$, the length of the theoretical receptive field (in input samples) of an output sample is given by the base $1$ plus, for each layer, the incremental span contributed by its kernel, scaled by the product of the strides of all layers before it.\n- A max-pooling layer with kernel size $2$ and stride $2$ contributes a receptive-field increment equal to the current effective input stride and doubles that stride thereafter.\n- Nearest-neighbor upsampling by a factor of $2$ does not itself expand the receptive field, but it halves the effective input stride that propagates to subsequent layers.\n\nAssume a constant background velocity $v_0$ (in meters per second) and near-normal-incidence single scattering (Born approximation), so that two-way travel time $t$ and depth $z$ are related by $t = \\frac{2 z}{v_0}$. Under this mapping, a sinusoidal velocity perturbation of depth-wavelength $\\lambda$ corresponds to a sinusoid in two-way time with period $T = \\frac{2 \\lambda}{v_0}$. For the network to capture at least one full cycle of such a component from the shot-gather input, the theoretical receptive field along time must span at least one period, i.e., $R_t \\, \\Delta t \\geq T$, where $R_t$ is the receptive field in input samples along time.\n\nTasks:\n\n- Derive from first principles an explicit, closed-form expression for the theoretical receptive field $R_t(L,k)$ (in input samples) of a single output sample with respect to the input, given the above U-Net architecture.\n- Using the time-depth relation $t = \\frac{2 z}{v_0}$ and the one-cycle criterion, express the maximum recoverable depth-wavelength $\\lambda_{\\max}$ (in meters) as a closed-form analytic function of $L$, $k$, $\\Delta t$, and $v_0$.\n\nState any additional minimal assumptions you require. Express the final wavelength in meters. The final answer must be a single closed-form expression. No rounding is required.",
            "solution": "The problem statement has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, internally consistent, and contains all necessary information to derive a unique solution. The language is precise, and the physical and computational models are standard simplifications used in the field.\n\nA minimal assumption is required regarding the receptive field calculation in a U-Net architecture with skip connections: The theoretical receptive field of an output neuron is determined by the longest path of influence from the input. In the specified U-Net architecture, this corresponds to the path traversing the full depth of the encoder, the bottleneck, and the full depth of the decoder. At each decoder stage, an upsampled feature map is concatenated with a feature map from a skip connection. The receptive field of a neuron in the subsequent convolutional layer is the union of its receptive fields with respect to both concatenated inputs. Since the upsampled path is deeper, its receptive field is strictly larger and encompasses that of the skip-connection path. Therefore, the overall receptive field is determined by tracing this deepest path, and this is the standard interpretation.\n\nThe first task is to derive an expression for the theoretical receptive field, $R_t(L,k)$, in input samples. We use the provided formula for receptive field growth: the total receptive field is $1$ plus the sum of incremental contributions from all layers. The contribution of a layer $i$ with kernel size $k_i$ is $(k_i-1)$ scaled by the product of strides of all preceding layers, $S_{i-1}$.\n$R_t = 1 + \\sum_{i} (k_i - 1) S_{i-1}$.\n\nLet's trace the receptive field growth and cumulative stride through the network's longest path.\n\n$1$. **Encoder**: The encoder has $L$ stages. For each stage $l \\in \\{1, \\dots, L\\}$:\nThe cumulative stride at the input of stage $l$ is $S_{\\text{in},l} = 2^{l-1}$.\nEach stage has two convolution layers with kernel size $k$ and stride $1$, and one max-pooling layer with kernel size $2$ and stride $2$.\n- The first convolution layer (stride $1$) contributes $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$.\n- The second convolution layer (stride $1$) contributes $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$.\n- The max-pooling layer (stride $2$) contributes $(2-1)S_{\\text{in},l} = 2^{l-1}$.\nThe total contribution from encoder stage $l$ is $\\Delta R_l^{\\text{enc}} = (k-1)2^{l-1} + (k-1)2^{l-1} + 2^{l-1} = (2k-1)2^{l-1}$.\nThe total contribution from all $L$ encoder stages is the sum over $l$:\n$$ \\Delta R_{\\text{enc}} = \\sum_{l=1}^{L} (2k-1)2^{l-1} = (2k-1) \\sum_{l=1}^{L} 2^{l-1} $$\nThis is a geometric series $\\sum_{j=0}^{L-1} 2^j = \\frac{2^L-1}{2-1} = 2^L - 1$.\n$$ \\Delta R_{\\text{enc}} = (2k-1)(2^L - 1) $$\nAfter $L$ encoder stages, the cumulative stride is $2^L$.\n\n$2$. **Bottleneck**: The bottleneck follows the encoder. The cumulative stride at its input is $S_{\\text{bottle}} = 2^L$.\nIt consists of two convolution layers, each with kernel size $k$ and stride $1$.\n- The first convolution contributes $(k-1)S_{\\text{bottle}} = (k-1)2^L$.\n- The second convolution contributes $(k-1)S_{\\text{bottle}} = (k-1)2^L$.\nThe total contribution from the bottleneck is:\n$$ \\Delta R_{\\text{bottle}} = 2(k-1)2^L $$\nThe stride does not change through the bottleneck.\n\n$3$. **Decoder**: The decoder has $L$ stages. We index them as $l \\in \\{1, \\dots, L\\}$ from the one closest to the bottleneck to the one closest to the output.\nFor each decoder stage $l$, there is an upsampling step followed by two convolution layers.\nThe cumulative stride entering decoder stage $l$ is $2^{L-l+1}$. The upsampling by a factor of $2$ reduces this stride to $S_{\\text{dec},l} = \\frac{1}{2} \\cdot 2^{L-l+1} = 2^{L-l}$. This stride applies to the subsequent convolutions.\nEach of the two convolution layers has kernel size $k$ and stride $1$.\nThe total contribution from decoder stage $l$ is:\n$\\Delta R_l^{\\text{dec}} = (k-1)S_{\\text{dec},l} + (k-1)S_{\\text{dec},l} = 2(k-1)2^{L-l}$.\nThe total contribution from all $L$ decoder stages is:\n$$ \\Delta R_{\\text{dec}} = \\sum_{l=1}^{L} 2(k-1)2^{L-l} = 2(k-1) \\sum_{l=1}^{L} 2^{L-l} $$\nLet $j=L-l$. The sum becomes $\\sum_{j=0}^{L-1} 2^j = 2^L - 1$.\n$$ \\Delta R_{\\text{dec}} = 2(k-1)(2^L - 1) $$\n\n$4$. **Final Convolution**: A final $1 \\times 1$ convolution (kernel size $1$, stride $1$) is applied. The stride entering this layer is $1$. Its contribution to the receptive field is $(1-1) \\cdot 1 = 0$.\n\n$5$. **Total Receptive Field**: The total receptive field $R_t(L,k)$ is the sum of the initial $1$ and all contributions:\n$$ R_t(L,k) = 1 + \\Delta R_{\\text{enc}} + \\Delta R_{\\text{bottle}} + \\Delta R_{\\text{dec}} $$\n$$ R_t(L,k) = 1 + (2k-1)(2^L-1) + 2(k-1)2^L + 2(k-1)(2^L-1) $$\nWe can group terms with $(2^L-1)$:\n$$ R_t(L,k) = 1 + \\left( (2k-1) + 2(k-1) \\right)(2^L-1) + 2(k-1)2^L $$\n$$ R_t(L,k) = 1 + (2k-1+2k-2)(2^L-1) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 + (4k-3)(2^L-1) + (2k-2)2^L $$\nExpanding the terms:\n$$ R_t(L,k) = 1 + (4k-3)2^L - (4k-3) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 - 4k + 3 + (4k-3 + 2k-2)2^L $$\n$$ R_t(L,k) = 4 - 4k + (6k-5)2^L $$\n$$ R_t(L,k) = (6k-5)2^L - 4(k-1) $$\nThis is the closed-form expression for the theoretical receptive field.\n\nThe second task is to find the maximum recoverable depth-wavelength $\\lambda_{\\max}$.\nThe condition for capturing a feature of temporal period $T$ is that the receptive field in time, $R_t \\cdot \\Delta t$, must be at least one period: $R_t \\Delta t \\ge T$.\nThe maximum period $T_{\\max}$ the network can resolve is limited by its receptive field:\n$$ T_{\\max} = R_t(L,k) \\Delta t $$\nThe problem provides the physical relationship between the two-way travel time period $T$ and the depth-wavelength $\\lambda$ of a velocity perturbation: $T = \\frac{2\\lambda}{v_0}$.\nTherefore, the maximum recoverable wavelength $\\lambda_{\\max}$ corresponds to $T_{\\max}$:\n$$ T_{\\max} = \\frac{2\\lambda_{\\max}}{v_0} $$\nEquating the two expressions for $T_{\\max}$:\n$$ \\frac{2\\lambda_{\\max}}{v_0} = R_t(L,k) \\Delta t $$\nSolving for $\\lambda_{\\max}$:\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} R_t(L,k) $$\nSubstituting the derived expression for $R_t(L,k)$:\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} \\left[ (6k-5)2^L - 4(k-1) \\right] $$\nThis expression can also be written as:\n$$ \\lambda_{\\max} = v_0 \\Delta t \\left[ (6k-5)2^{L-1} - 2(k-1) \\right] $$\nThis is the final closed-form analytic function for the maximum recoverable depth-wavelength.",
            "answer": "$$\\boxed{\\frac{v_0 \\Delta t}{2} \\left( (6k-5)2^L - 4(k-1) \\right)}$$"
        },
        {
            "introduction": "This final practice explores a sophisticated paradigm where deep learning is used not just to represent the solution, but to learn the optimization algorithm itself. In these \"unrolled\" or \"learned optimization\" architectures, each iteration of a classical algorithm like gradient descent is modeled as a network layer with learnable parameters. The central challenge in such models is ensuring stability and convergence . This exercise will guide you through the theoretical analysis required to derive sufficient conditions for descent and stability, connecting the design of recurrent network architectures directly to the fundamental principles of optimization theory.",
            "id": "3583447",
            "problem": "Design a mathematically precise, unrolled gradient-like architecture for full-waveform inversion where each layer performs one step $$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k),$$ with objective $$J(m) = \\lVert F(m) - d \\rVert_2^2 + \\lambda R(m),$$ assuming the forward operator and regularizer are such that the gradient of $J$ is globally Lipschitz continuous with constant $L$ in the Euclidean norm. The matrices $P_k$ represent learned linear preconditioners and the scalars $\\alpha_k$ are learned step sizes. Use the fundamental smoothness inequality for functions with $L$-Lipschitz gradients, the properties of symmetric positive semidefinite matrices, and operator norm bounds as the base of your derivation.\n\nYou must perform the following.\n\n1) Starting from the smoothness inequality for an $L$-smooth function $$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2,$$ derive a sufficient descent condition for the update $$m^{+} = m - \\alpha P \\nabla J(m),$$ expressed as an explicit inequality coupling the step size $\\alpha$, the learned matrix $P$, and the Lipschitz constant $L$. Your derivation must treat the general case where $P$ is not necessarily symmetric by working with its symmetric part $$S \\equiv \\frac{P + P^\\top}{2},$$ and must end in an implementable bound of the form\n$$\\text{if } \\mu \\equiv \\lambda_{\\min}(S) > 0 \\text{ and } M \\equiv \\lVert P \\rVert_2, \\text{ then } J(m^{+}) < J(m) \\text{ whenever } 0 < \\alpha < \\frac{2\\mu}{L M^2}.$$\n\n2) Provide a Lipschitz stability bound for the single-layer mapping $$T(m) \\equiv m - \\alpha P \\nabla J(m).$$ Specifically, derive a sufficient upper bound on the Lipschitz constant of $T$ in the Euclidean norm of the form $$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2,$$ and express $K$ explicitly in terms of $\\alpha$, $L$, and $\\lVert P \\rVert_2$. State clearly any additional sufficient conditions that make this bound finite and, if available, any conservative parameter range that makes $T$ nonexpansive or contractive in an appropriate norm.\n\n3) Implement and evaluate the theory on a quadratic surrogate that is standard in linearized full-waveform inversion with Tikhonov regularization, namely $$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2,$$ where $A \\in \\mathbb{R}^{n \\times n}$, $b \\in \\mathbb{R}^n$, $L \\in \\mathbb{R}^{n \\times n}$, and $\\lambda > 0$ are fixed. For this quadratic, the Hessian is constant, $$H = \\nabla^2 J(m) = 2\\left(A^\\top A + \\lambda L^\\top L\\right),$$ so the global Lipschitz constant of $\\nabla J$ equals $$L = \\lVert H \\rVert_2.$$\n\nFor numerical evaluation, use dimension $n = 3$ with the following test suite (these choices are dimensionless, so no physical units are required):\n\n- Shared data for all cases:\n  - $$A = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.0 & 0.3 \\\\ 0.0 & 0.3 & 1.5 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}, \\quad L = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & -1.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}, \\quad \\lambda = 0.1,$$\n  - initial model $$m^{0} = \\begin{bmatrix} -0.2 \\\\ 0.5 \\\\ 1.0 \\end{bmatrix}.$$\n- Four learned layer configurations:\n  - Case $1$: $$P = \\operatorname{diag}(0.5, 1.0, 2.0), \\quad \\alpha = \\frac{1}{4} \\cdot \\frac{2 \\mu}{L M^2}.$$\n  - Case $2$: same $P$ as Case $1$, $$\\alpha = \\frac{2 \\mu}{L M^2}.$$\n  - Case $3$: same $P$ as Case $1$, $$\\alpha = 1.1 \\cdot \\frac{2 \\mu}{L M^2}.$$\n  - Case $4$: $$P = \\begin{bmatrix} 1.0 & 0.5 & 0.0 \\\\ 0.0 & 1.0 & 0.2 \\\\ 0.0 & 0.0 & 0.8 \\end{bmatrix}, \\quad \\alpha = \\frac{1}{2} \\cdot \\frac{2 \\mu}{L M^2},$$ where for nonsymmetric $P$, $\\mu = \\lambda_{\\min}\\!\\left(\\frac{P + P^\\top}{2}\\right)$ and $M = \\lVert P \\rVert_2.$\n\nFor each case:\n  - Compute the theoretical threshold $$\\alpha_\\star = \\frac{2 \\mu}{L M^2}.$$\n  - Report a boolean for the theoretical strict descent condition, i.e., whether $$\\alpha < \\alpha_\\star \\text{ and } \\mu > 0.$$\n  - Perform one update $$m^{1} = m^{0} - \\alpha P \\nabla J(m^{0})$$ and report a boolean for actual strict decrease, i.e., $$J(m^{1}) < J(m^{0}).$$\n  - Report the conservative Lipschitz bound $$K_{\\mathrm{bound}} = 1 + \\alpha L \\lVert P \\rVert_2.$$\n  - For this quadratic $J$, the single-layer map is linear: $$T(m) = \\left(I - \\alpha P H\\right)m + \\alpha P \\cdot \\mathrm{const},$$ so its exact Euclidean Lipschitz constant equals $$\\lVert I - \\alpha P H \\rVert_2.$$ Report this value.\n\nYour program must output a single line containing all per-case results concatenated into one list, in the exact order\n$$[\\text{theory\\_descent}_1, \\text{actual\\_descent}_1, K_{\\mathrm{bound},1}, K_{\\mathrm{actual},1}, \\; \\ldots \\;, \\text{theory\\_descent}_4, \\text{actual\\_descent}_4, K_{\\mathrm{bound},4}, K_{\\mathrm{actual},4}],$$\nwhere booleans are in lower case and floats are decimal numbers rounded to $6$ decimal places. The output must be a single line, exactly a comma-separated list enclosed in square brackets, with no extra text.",
            "solution": "The problem is valid as it is scientifically grounded in optimization theory, well-posed with all necessary data and conditions, and objectively stated. It presents a standard yet rigorous exercise in the analysis of learned optimization algorithms.\n\nThe solution proceeds in three parts as requested.\n\n### Part 1: Derivation of the Sufficient Descent Condition\n\nWe begin with the fundamental inequality for a function $J(m)$ with an $L$-Lipschitz continuous gradient $\\nabla J$, which holds for any two points $x$ and $y$ in its domain:\n$$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2.$$\nWe are interested in the single update step of a preconditioned gradient-like method, given by\n$$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k).$$\nTo simplify notation for a single step, let $m = m^k$, $m^{+} = m^{k+1}$, $\\alpha = \\alpha_k$, and $P = P_k$. The update is $m^{+} = m - \\alpha P \\nabla J(m)$.\n\nLet us substitute $x = m$ and $y = m^{+}$ into the smoothness inequality:\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (m^{+} - m) + \\frac{L}{2} \\lVert m^{+} - m \\rVert_2^2.$$\nThe displacement vector is $m^{+} - m = -\\alpha P \\nabla J(m)$. Substituting this expression into the inequality yields:\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (-\\alpha P \\nabla J(m)) + \\frac{L}{2} \\lVert -\\alpha P \\nabla J(m) \\rVert_2^2.$$\nLet $g \\equiv \\nabla J(m)$ for brevity. The inequality becomes:\n$$J(m^{+}) \\le J(m) - \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2.$$\nFor strict descent, we require $J(m^{+}) < J(m)$. This is guaranteed if the sum of the last two terms on the right-hand side is strictly negative:\n$$- \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2 < 0.$$\nAssuming the step size $\\alpha > 0$ and that we are not at a stationary point (i.e., $g \\neq 0$), we can divide by $\\alpha$:\n$$- g^\\top P g + \\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < 0,$$\nwhich can be rearranged to:\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top P g.$$\nThe term $g^\\top P g$ is a quadratic form. Since the preconditioner $P$ is not guaranteed to be symmetric, this form is not directly related to the eigenvalues of $P$. However, for any real matrix $P$ and vector $g$, the quadratic form is determined by the symmetric part of $P$. Let $S \\equiv \\frac{P + P^\\top}{2}$. Then:\n$$g^\\top S g = g^\\top \\left(\\frac{P + P^\\top}{2}\\right) g = \\frac{1}{2} (g^\\top P g + g^\\top P^\\top g) = \\frac{1}{2} (g^\\top P g + (P g)^\\top g).$$\nSince $(P g)^\\top g$ is a scalar, it equals its transpose, $(P g)^\\top g = g^\\top P g$. Thus, $g^\\top S g = g^\\top P g$.\n\nThe descent inequality can be rewritten using $S$:\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top S g.$$\nTo establish a sufficient condition that depends only on properties of $P$ and not on the specific gradient $g$, we use the provided bounds. We are given $\\mu \\equiv \\lambda_{\\min}(S) > 0$, where $\\lambda_{\\min}(S)$ is the smallest eigenvalue of the symmetric part of $P$. From the Rayleigh quotient theorem for the symmetric matrix $S$, we have:\n$$g^\\top S g \\ge \\lambda_{\\min}(S) \\lVert g \\rVert_2^2 = \\mu \\lVert g \\rVert_2^2.$$\nWe are also given $M \\equiv \\lVert P \\rVert_2$, which is the spectral norm of $P$. By the definition of an induced matrix norm:\n$$\\lVert P g \\rVert_2 \\le \\lVert P \\rVert_2 \\lVert g \\rVert_2 = M \\lVert g \\rVert_2 \\implies \\lVert P g \\rVert_2^2 \\le M^2 \\lVert g \\rVert_2^2.$$\nTo guarantee the inequality $\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top S g$ holds for any $g \\neq 0$, it is sufficient to satisfy the more conservative inequality obtained by replacing the left side with an upper bound and the right side with a lower bound:\n$$\\frac{L \\alpha}{2} (M^2 \\lVert g \\rVert_2^2) < \\mu \\lVert g \\rVert_2^2.$$\nSince we assume $g \\neq 0$, we can divide by the positive scalar $\\lVert g \\rVert_2^2$:\n$$\\frac{L \\alpha M^2}{2} < \\mu.$$\nSolving for $\\alpha$, we obtain the sufficient condition on the step size:\n$$\\alpha < \\frac{2\\mu}{L M^2}.$$\nCombining this with the requirements that $\\alpha > 0$ and that the lower bound on the quadratic form is positive (i.e., $\\mu > 0$), we arrive at the final condition for guaranteed strict descent:\n$$J(m^{+}) < J(m) \\text{ whenever } \\mu > 0 \\text{ and } 0 < \\alpha < \\frac{2\\mu}{L M^2}.$$\n\n### Part 2: Lipschitz Stability Bound\n\nWe aim to find a Lipschitz constant $K$ for the single-layer mapping $T(m) \\equiv m - \\alpha P \\nabla J(m)$ in the Euclidean norm. That is, we seek a bound $K$ such that for any two points $x$ and $y$:\n$$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2.$$\nLet's analyze the difference $T(x) - T(y)$:\n$$T(x) - T(y) = (x - \\alpha P \\nabla J(x)) - (y - \\alpha P \\nabla J(y)) = (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)).$$\nTaking the $L_2$-norm and applying the triangle inequality:\n$$\\lVert T(x) - T(y) \\rVert_2 = \\lVert (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2.$$\nWe analyze the second term using properties of matrix and vector norms:\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 = |\\alpha| \\cdot \\lVert P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot \\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2.$$\nThe gradient $\\nabla J$ is globally Lipschitz continuous with constant $L$, meaning $\\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2 \\le L \\lVert x - y \\rVert_2$. Substituting this into our expression:\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot (L \\lVert x - y \\rVert_2).$$\nAssuming a positive step size $\\alpha > 0$, we substitute this back into the main inequality:\n$$\\lVert T(x) - T(y) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\alpha L \\lVert P \\rVert_2 \\lVert x - y \\rVert_2.$$\nFactoring out $\\lVert x-y \\rVert_2$:\n$$\\lVert T(x) - T(y) \\rVert_2 \\le (1 + \\alpha L \\lVert P \\rVert_2) \\lVert x - y \\rVert_2.$$\nThis establishes a sufficient upper bound on the Lipschitz constant of $T$, given by\n$$K = 1 + \\alpha L \\lVert P \\rVert_2.$$\nThis bound is finite provided that the step size $\\alpha$, the gradient's Lipschitz constant $L$, and the preconditioner's norm $\\lVert P \\rVert_2$ are all finite.\nThis particular bound, being derived from the triangle inequality, is conservative. Since $\\alpha > 0$, $L > 0$, and $\\lVert P \\rVert_2 \\ge 0$, this bound $K$ is always greater than or equal to $1$. Consequently, it cannot be used to demonstrate that the map $T$ is a contraction ($K < 1$). To show contractivity, a tighter analysis is needed, often specific to the structure of the problem. For the special case where $J$ is a quadratic function, as in Part 3, the map $T$ becomes affine, and its exact Lipschitz constant can be computed, which may indeed be less than $1$.\n\n### Part 3: Numerical Evaluation Setup\n\nFor the numerical evaluation, we are given the quadratic objective function:\n$$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2.$$\nThis can be expanded as $J(m) = (A m - b)^\\top(A m - b) + \\lambda (L m)^\\top(L m)$, which simplifies to a standard quadratic form:\n$$J(m) = m^\\top A^\\top A m - 2b^\\top A m + b^\\top b + \\lambda m^\\top L^\\top L m = m^\\top(A^\\top A + \\lambda L^\\top L) m - 2 (A^\\top b)^\\top m + b^\\top b.$$\nThe gradient $\\nabla J(m)$ is found by differentiating with respect to $m$:\n$$\\nabla J(m) = 2(A^\\top A + \\lambda L^\\top L)m - 2A^\\top b.$$\nThe Hessian $\\nabla^2 J(m)$ is the derivative of the gradient:\n$$H \\equiv \\nabla^2 J(m) = 2(A^\\top A + \\lambda L^\\top L).$$\nSince the Hessian $H$ is a constant matrix, the gradient $\\nabla J(m)$ is a linear map of $m$ plus a constant, making it globally Lipschitz. The Lipschitz constant of $\\nabla J(m)$ is the operator norm of its derivative matrix, which is $H$. Thus, the global Lipschitz constant $L$ is given by:\n$$L = \\lVert H \\rVert_2 = \\lVert 2(A^\\top A + \\lambda L^\\top L) \\rVert_2.$$\nThe single-layer map $T(m)$ becomes an affine transformation:\n$$T(m) = m - \\alpha P \\nabla J(m) = m - \\alpha P (H m - 2A^\\top b) = (I - \\alpha P H) m + 2\\alpha P A^\\top b.$$\nThe Lipschitz constant of an affine map $f(x) = M x + c$ is $\\lVert M \\rVert_2$. Therefore, the exact Lipschitz constant for our map $T(m)$ is:\n$$K_{\\mathrm{actual}} = \\lVert I - \\alpha P H \\rVert_2.$$\nThe following implementation will compute the quantities for each specified case using these formulas.",
            "answer": "[true,true,1.125000,0.963428,false,true,1.500000,0.853696,false,true,1.550000,0.840455,true,true,1.524956,0.778844]"
        }
    ]
}