## 引言
从[天气预报](@entry_id:270166)到地下资源勘探，从飞机设计到金融建模，[偏微分方程](@entry_id:141332)（PDEs）是描述我们世界物理现象的通用语言。然而，求解这些方程，尤其是当它们耦合、[非线性](@entry_id:637147)或涉及未知参数（即逆问题）时，对传统的数值方法构成了巨大的挑战。这些方法通常需要精确的几何网格、大量的计算资源，并且在处理数据稀疏或带噪声的现实场景时常常力不从心。

近年来，[深度学习](@entry_id:142022)的兴起为科学计算提供了一个全新的[范式](@entry_id:161181)。[物理信息神经网络](@entry_id:145229)（PINN）正是在这一浪潮中应运而生的革命性方法。它巧妙地将数据驱动的[神经网](@entry_id:276355)络与物理学的第一性原理相结合，解决了传统方法面临的许多难题。PINN的核心思想是将控制方程和边界条件直接编码到[神经网](@entry_id:276355)络的损失函数中，将求解PDE的问题转化为一个[优化问题](@entry_id:266749)。这使得我们能够在没有网格的情况下，仅利用稀疏的观测数据和已知的物理定律，就能找到问题的解。

本文旨在为计算地球物理领域的研究生提供一份关于PINN的全面指南，从基本原理到前沿应用。通过学习本文，您将能够：

- 在第一章“原理与机制”中，深入理解PINN的基石——如何构建物理约束的损失函数，以及[自动微分](@entry_id:144512)在其中扮演的关键角色。
- 在第二章“应用与跨学科连接”中，探索PINN在解决地球物理[逆问题](@entry_id:143129)、[多物理场耦合](@entry_id:171389)等真实世界挑战中的强大能力，并了解其与[数值分析](@entry_id:142637)、统计学等领域的深刻联系。
- 在第三章“动手实践”中，通过具体的编程练习，将理论知识转化为解决实际问题的技能。

让我们一同开启这段旅程，探索如何利用PINN这座桥梁，连接数据科学与物理建模，解决科学与工程中的前沿挑战。

## 原理与机制

本章深入探讨构成物理信息神经网络（PINN）的基本原理及其运行的核心机制。我们将从 PINN 的核心思想——将物理定律编码到损失函数中——出发，逐步解析其运作所需的关键技术，例如[自动微分](@entry_id:144512)。随后，我们将讨论网络架构的选择如何影响解的性质，并介绍施加约束的各种策略。最后，我们将探索更高级的公式化方法和理论诠释，并阐述在训练过程中可能遇到的关键挑战及其缓解策略。

### 核心原理：[损失函数](@entry_id:634569)中的物理编码

[物理信息神经网络](@entry_id:145229)的基石思想是利用[神经网](@entry_id:276355)络的[函数逼近](@entry_id:141329)能力，直接寻找一个满足给定[偏微分方程](@entry_id:141332)（PDE）及其边界和初始条件的解。这与传统的纯数据驱动的代理模型形成鲜明对比，后者仅通[过拟合](@entry_id:139093)已有的数据点进行学习，而对数据点之间区域的物理行为一无所知。PINN 通过其独特的[损失函数](@entry_id:634569)设计，将物理定律作为一种强大的[归纳偏置](@entry_id:137419)（inductive bias）引入学习过程。

#### 强形式[偏微分方程](@entry_id:141332)残差

假设我们要求解一个由微分算子 $\mathcal{N}$ 定义的[偏微分方程](@entry_id:141332)，其形式为 $\mathcal{N}[u(\boldsymbol{x})] = 0$，其中 $\boldsymbol{x}$ 是定义在某个区域 $\Omega \subset \mathbb{R}^d$ 上的[自变量](@entry_id:267118)（例如空间和时间坐标），$u$ 是待求解的未知场。PINN 的第一步是用一个参数为 $\theta$ 的[神经网](@entry_id:276355)络 $u_\theta(\boldsymbol{x})$ 来表示解 $u(\boldsymbol{x})$。

为了评估网络 $u_\theta$ 在多大程度上满足了物理定律，我们定义了**强形式[偏微分方程](@entry_id:141332)残差（strong-form PDE residual）**。这个残差通过将网络解 $u_\theta$ 代入[微分算子](@entry_id:140145) $\mathcal{N}$ 得到：

$$
r_\theta(\boldsymbol{x}) = \mathcal{N}[u_\theta(\boldsymbol{x})]
$$

如果网络 $u_\theta$ 是 PDE 的精确解，那么在区域 $\Omega$ 内的每一点 $\boldsymbol{x}$，残差 $r_\theta(\boldsymbol{x})$ 都应该为零。因此，PINN 的训练目标之一就是最小化这个残差的大小。值得强调的是，这里的[微分算子](@entry_id:140145) $\mathcal{N}$ 是作用在**近似解** $u_\theta$ 上的**[连续算子](@entry_id:143297)**。这与传统数值方法（如[有限差分法](@entry_id:147158)）中的概念有本质区别。在[有限差分法](@entry_id:147158)中，我们关注的是**截断误差（truncation error）**，它定义为将**精确解** $u$ 代入**离散算子** $\mathcal{N}_h$ 所产生的误差，即 $\tau_h(\boldsymbol{x}) = \mathcal{N}_h[u](\boldsymbol{x}) - \mathcal{N}[u](\boldsymbol{x})$。因此，PINN 的残差衡量的是近似解对物理定律的违反程度，而截断误差衡量的是离散化方案对[连续算子](@entry_id:143297)的逼近精度。

#### 复合[损失函数](@entry_id:634569)

一个完整的物理问题不仅包含控制方程，还包括在区域边界 $\partial\Omega$ 和初始时刻（如果适用）的约束条件。PINN 通过一个**复合损失函数（composite loss function）**来统一处理这些约束。这个[损失函数](@entry_id:634569)通常是多个损失项的加权和，每一项对应一个物理约束。

考虑一个时空域上的问题 $\mathcal{N}[u(x,t)]=0$，其初始条件为 $u(x,0)=u_0(x)$，边界条件分为 Dirichlet 型 $u(x,t)=g_D(x,t)$ 和 Neumann 型 $\nabla u(x,t)\cdot \boldsymbol{n}(x)=g_N(x,t)$。复合损失函数 $L(\theta)$ 可以构建如下：

$$
L(\theta) = w_r L_r(\theta) + w_0 L_0(\theta) + w_D L_D(\theta) + w_N L_N(\theta)
$$

其中：

*   **物理残差损失 $L_r$**：在区域内部的配点（collocation points）$\mathcal{S}_r$ 上惩罚 PDE 残差。
    $$
    L_r(\theta) = \frac{1}{|\mathcal{S}_r|} \sum_{(x,t) \in \mathcal{S}_r} ||\mathcal{N}[u_\theta(x,t;\theta)]||^2
    $$

*   **初始条件损失 $L_0$**：在初始时刻的配点 $\mathcal{S}_0$ 上惩罚与初始状态的偏差。
    $$
    L_0(\theta) = \frac{1}{|\mathcal{S}_0|} \sum_{(x,0) \in \mathcal{S}_0} ||u_\theta(x,0;\theta) - u_0(x)||^2
    $$

*   **Dirichlet 边界损失 $L_D$**：在 Dirichlet 边界上的配点 $\mathcal{S}_D$ 上惩罚与给定值的偏差。
    $$
    L_D(\theta) = \frac{1}{|\mathcal{S}_D|} \sum_{(x,t) \in \mathcal{S}_D} ||u_\theta(x,t;\theta) - g_D(x,t)||^2
    $$

*   **Neumann 边界损失 $L_N$**：在 Neumann 边界上的配点 $\mathcal{S}_N$ 上惩罚[法向导数](@entry_id:169511)与给定值的偏差。
    $$
    L_N(\theta) = \frac{1}{|\mathcal{S}_N|} \sum_{(x,t) \in \mathcal{S}_N} ||\nabla u_\theta(x,t;\theta) \cdot \boldsymbol{n}(x) - g_N(x,t)||^2
    $$

这里的权重 $w_r, w_0, w_D, w_N$ 是超参数，用于平衡不同损失项的重要性。通过最小化这个复合损失函数，优化算法会驱使[神经网](@entry_id:276355)络 $u_\theta$ 同时逼近控制方程和所有约束条件。这种方法的核心优势在于，它将求解 PDE 的问题转化为了一个无约束的[优化问题](@entry_id:266749)。

这种方法论可以自然地推广到**多物理场耦合问题**。例如，如果一个系统在不同[子域](@entry_id:155812)由算子 $\mathcal{N}_1$ 和 $\mathcal{N}_2$ 控制，并在交界面 $\Gamma_I$ 上满足连续性和[通量平衡](@entry_id:637776)等耦合条件，我们只需在复合[损失函数](@entry_id:634569)中加入对应于各[子域](@entry_id:155812)物理残差和交[界面条件](@entry_id:750725)的损失项即可。

### [微分](@entry_id:158718)的引擎：[自动微分](@entry_id:144512)

为了计算上述[损失函数](@entry_id:634569)中的物理残差项 $L_r$ 和 Neumann 边界损失项 $L_N$，我们必须能够计算[神经网](@entry_id:276355)络输出 $u_\theta$ 关于其输入（如空间坐标 $x, y$ 和时间 $t$）的偏导数，甚至是[高阶偏导数](@entry_id:142432)。例如，[拉普拉斯算子](@entry_id:146319) $\Delta u$ 需要[二阶导数](@entry_id:144508) $\frac{\partial^2 u}{\partial x^2}$ 和 $\frac{\partial^2 u}{\partial y^2}$。

**[自动微分](@entry_id:144512)（Automatic Differentiation, AD）**是实现这一目标的关键机制。与基于差分格式的[数值微分](@entry_id:144452)不同，AD 不是一种近似方法。它是一种在计算机程序中精确计算函数导数的技术。通过在程序执行过程中系统地应用链式法则，AD 能够计算出解析导数的精确值，其精度仅受限于机器[浮点数](@entry_id:173316)的精度。

[神经网](@entry_id:276355)络本质上是一个由基本运算（[线性变换](@entry_id:149133)、[激活函数](@entry_id:141784)等）构成的复杂[复合函数](@entry_id:147347)。AD 正好可以作用于这样的[计算图](@entry_id:636350)。AD 主要有两种模式：

*   **前向模式（Forward Mode）**：它从输入到输出传播导数信息，高效地计算[雅可比-向量积](@entry_id:162748)（Jacobian-vector product, JVP），即 $J_f(x)v$。一次[前向传播](@entry_id:193086)的计算成本与一次函数自身求值的成本相当。

*   **反向模式（Reverse Mode）**：它从输出到输入传播伴随灵敏度（adjoint sensitivities），高效地计算向量-雅可比积（vector-Jacobian product, VJP），即 $v^\top J_f(x)$。对于一个从多维输入到标量输出的函数（如 PINN 的[损失函数](@entry_id:634569)或解 $u_\theta$），一次[反向传播](@entry_id:199535)可以计算出输出关于所有输入的梯度 $\nabla f(x)$，其成本也与一次函数求值相当。[深度学习](@entry_id:142022)中广泛使用的[反向传播算法](@entry_id:198231)（backpropagation）就是反向模式 AD 的一种特例。

在 PINN 中，我们常常需要计算二阶或更高阶的导数。这可以通过组合使用 AD 模式来实现。例如，要计算[二阶导数](@entry_id:144508) $\frac{\partial^2 u_\theta}{\partial x_i^2}$ 和 $\frac{\partial^2 u_\theta}{\partial x_i \partial x_j}$，一个高效的策略是计算**海森矩阵-向量积（Hessian-vector products, HVP）**。这可以通过“前向模式-反向模式之上”（forward-over-reverse）的 AD 实现。具体步骤如下：

1.  将[梯度算子](@entry_id:275922) $\nabla u_\theta(\boldsymbol{x})$ 视为一个从 $\mathbb{R}^d$ 到 $\mathbb{R}^d$ 的新函数 $g(\boldsymbol{x})$。它的[雅可比矩阵](@entry_id:264467)正是 $u_\theta$ 的[海森矩阵](@entry_id:139140) $H(\boldsymbol{x})$。
2.  通过一次反向模式 AD 传递，我们可以得到计算梯度 $g(\boldsymbol{x})$ 的[计算图](@entry_id:636350)。
3.  然后，在这个新的[计算图](@entry_id:636350)上应用前向模式 AD，以[标准基向量](@entry_id:152417) $\boldsymbol{e}_i$ 作为种子向量，就可以高效地计算出 HVP $H(\boldsymbol{x})\boldsymbol{e}_i$，这正是[海森矩阵](@entry_id:139140)的第 $i$ 列。
4.  从[海森矩阵](@entry_id:139140)的第 $i$ 列中，我们可以直接读取所需的[二阶导数](@entry_id:144508)值，如对角元素 $H_{ii} = \frac{\partial^2 u_\theta}{\partial x_i^2}$ 和非对角元素 $H_{ji} = \frac{\partial^2 u_\theta}{\partial x_j \partial x_i}$。

由于一次反向模式和一次前向模式的计算成本都与网络的一次前向评估（其复杂度为 $O(dW + LW^2)$，其中 $d$ 为输入维度，$W$ 为隐藏层宽度，$L$ 为层数）相当，因此计算任意一个 HVP 的总成本也是 $O(dW + LW^2)$。这意味着我们可以用与计算一阶梯度几乎相同的成本来获取特定的[二阶导数](@entry_id:144508)，而无需显式构造整个庞大的海森矩阵。

### 架构考量：正则性与约束

PINN 的成功不仅取决于[损失函数](@entry_id:634569)的设计，还与[神经网络架构](@entry_id:637524)本身的选择密切相关。两个关键的架构考量是激活函数的选择（它决定了解的正则性）和边界条件的施加方式。

#### [激活函数](@entry_id:141784)与正则性

强形式 PINN 的一个基本要求是，其网络解 $u_\theta$ 必须足够光滑（即具有足够高阶的连续导数），以便物理残差 $\mathcal{N}[u_\theta]$ 能够被经典地（pointwise）定义。一个 $m$ 阶的[微分算子](@entry_id:140145) $\mathcal{N}$ 通常要求解至少是 $m$ 次连续可微的，即 $u \in C^m(\Omega)$。

[神经网](@entry_id:276355)络的正则性直接取决于其[激活函数](@entry_id:141784)的正则性。

*   **光滑激活函数**：如[双曲正切函数](@entry_id:634307) ($\tanh$)、sigmoid 函数或 swish 函数，它们都是无限次可微的（$C^\infty$）。由这些激活函数构成的[神经网](@entry_id:276355)络，其输出 $u_\theta$ 也是一个 $C^\infty$ 函数。因此，对于任何阶数的微分算子，我们都可以通过 AD 计算出其在任意点的经典导数值。

*   **非光滑激活函数**：如[修正线性单元](@entry_id:636721) ($\mathrm{ReLU}(z) = \max\{0, z\}$)，它在 $z=0$ 处不可微。由 ReLU 构成的网络是一个连续的[分段线性](@entry_id:201467)（或仿射）函数，属于 $C^0$ 类。它的[二阶导数](@entry_id:144508)在几乎所有地方都为零，在“扭结”处（即分段的边界）则无定义。

对于需要[二阶导数](@entry_id:144508)的 PDE（例如[声波方程](@entry_id:746230)或[热传导方程](@entry_id:194763)），使用 ReLU 作为[激活函数](@entry_id:141784)会产生严重问题。在评估强形式残差时，AD 将在几乎所有配点上计算出零的[二阶导数](@entry_id:144508)值。这将导致网络无法学习具有非零曲率的解，从而无法正确地表示物理过程。因此，对于求解高阶 PDE 的强形式 PINN，**选择如 $\tanh$ 这样的光滑激活函数至关重要**。

#### 边界条件的施加方式

对于 Dirichlet 边界条件 $u|_{\partial\Omega}=g$，有两种主流的施加策略：软约束和硬约束。

*   **软约束（Soft Enforcement）**：这是前文介绍的默认方法，即将边界条件的偏差作为一个惩罚项 $\lambda_b L_b(\theta)$ 加入总[损失函数](@entry_id:634569)中。在这种方法中，网络的[假设空间](@entry_id:635539) $\mathcal{H} = \{u_\theta : \theta \in \Theta\}$（即网络架构能表示的所有函数的集合）事先没有改变。优化过程会“鼓励”网络去满足边界条件，但并不能严格保证。最终解对边界条件的满足程度取决于权重 $\lambda_b$ 的大小和优化的成功与否。

*   **硬约束（Hard Enforcement）**：这种策略通过特殊设计[网络架构](@entry_id:268981)，使得任何参数 $\theta$ 下的输出 $u_\theta$ 都**自动地、精确地**满足边界条件。一个常见的构造方法是：
    $$
    u_\theta(\boldsymbol{x}) = g(\boldsymbol{x}) + d(\boldsymbol{x}) N_\theta(\boldsymbol{x})
    $$
    这里，$N_\theta(\boldsymbol{x})$ 是一个标准的[神经网](@entry_id:276355)络，$d(\boldsymbol{x})$ 是一个预先定义的函数，它在边界 $\partial\Omega$ 上为零，但在区域内部 $\Omega$ 不为零（例如，$d(\boldsymbol{x})$ 可以是点 $\boldsymbol{x}$到边界的距离函数）。由于 $d(\boldsymbol{x})$ 在边界上为零，第二项消失，使得 $u_\theta(\boldsymbol{x})|_{\partial\Omega} = g(\boldsymbol{x})$ 恒成立。通过这种方式，我们将搜索空间**限制**在一个已经满足边界条件的[子集](@entry_id:261956)上。训练时，[损失函数](@entry_id:634569)中不再需要边界损失项，只需最小化 PDE 残差。

这两种方法的选择是一个权衡。硬约束将约束问题转化为了无约束问题，消除了平衡权重 $\lambda_b$ 的需要，但可能引入更复杂的梯度计算（因为需要对 $d(\boldsymbol{x})N_\theta(\boldsymbol{x})$ 的乘积求导），并且如果真实的解不能被该构造形式很好地逼近，可能会限制模型的[表达能力](@entry_id:149863)。软约束更具灵活性和普适性，但引入了额外的超参数和潜在的训练动态问题。从优化理论的角度看，软约束是一种惩罚方法，当惩罚权重 $\lambda_b \to \infty$ 时，其解会收敛到硬约束问题的解。

### 高级公式化与诠释

除了基本的强形式 PINN，还存在一些更高级的变体和理论视角，它们为处理特定问题和理解 PINN 的工作机制提供了更深刻的见解。

#### [变分形式](@entry_id:166033) (vPINNs)

对于某些问题，尤其是当解的正则性较低或系数 $\kappa(x)$ 不连续时，强形式的 PDE 可能没有很好的定义。在这种情况下，可以采用**弱形式（weak formulation）**或**[变分形式](@entry_id:166033)（variational formulation）**。这就是[变分物理信息神经网络](@entry_id:756443)（vPINNs）的基础。

考虑一个二阶[椭圆方程](@entry_id:169190) $-\nabla\cdot(\kappa\nabla u) = f$。其[弱形式](@entry_id:142897)是通过将方程乘以一个**测试函数** $w$ 并在整个区域 $\Omega$ 上积分得到的。通过使用 **Green 恒等式（即[分部积分](@entry_id:136350)）**，我们可以将一个[微分](@entry_id:158718)从解 $u$ 转移到测试函数 $w$ 上：

$$
\int_\Omega \kappa\nabla u \cdot \nabla w \,dx = \int_\Omega f w \,dx
$$

这个等式需要对所有属于某个合适[函数空间](@entry_id:143478)（如 $H_0^1(\Omega)$）的测试函数 $w$ 都成立。

vPINN 的核心优势在于**降低了对解的正则性要求**。在强形式中，二阶算子 $\nabla\cdot(\kappa\nabla u)$ 要求 $u$ 至少是二阶可微的。而在弱形式中，积分只涉及 $u$ 和 $w$ 的[一阶导数](@entry_id:749425) $\nabla u$ 和 $\nabla w$。这意味着我们只需要解 $u_\theta$ 是一阶可微的（即 $u_\theta \in H^1(\Omega)$），这大大放宽了对网络[激活函数](@entry_id:141784)和解行为的限制。

弱形式残差是一个线性泛函 $R_\theta[w] = \int_\Omega \kappa\nabla u_\theta\cdot\nabla w\,dx - \int_\Omega f w\,dx$。当且仅当对所有测试函数 $w$，都有 $R_\theta[w]=0$ 时，PDE 才被满足。在[分布](@entry_id:182848)的意义上，强形式残差 $r_\theta = \nabla\cdot(\kappa\nabla u_\theta)+f$ 在对偶空间 $H^{-1}(\Omega)$ 中为零，这与[弱形式](@entry_id:142897)残差恒为零是等价的。

在实践中，vPINN 的[损失函数](@entry_id:634569)是通过选择一组测试函数 $\{w_i\}$ 并最小化弱形式残差的平方和来构造的。这需要通过**数值积分（numerical quadrature）**（如[高斯求积法](@entry_id:146011)）来近似计算积分项。值得注意的是，弱形式本身通常不施加边界条件，因此在 vPINN 中，边界条件仍需通过软约束或硬约束的方式另外强制执行。 

#### [损失函数](@entry_id:634569)的[贝叶斯诠释](@entry_id:265644)

PINN 的复合[损失函数](@entry_id:634569)可以从贝叶斯推断的角度得到深刻的诠释。我们可以将求解 PDE 的过程看作是根据物理先验和观测数据来估计最可能的解。

在这个框架下，最小化加权[损失函数](@entry_id:634569)等价于寻找一个**[最大后验概率](@entry_id:268939)（Maximum A Posteriori, MAP）**估计。具体来说：

*   **数据损失项**：如果有来自传感器的带噪声的测量数据 $\{(\boldsymbol{x}_i, y_i)\}$，其中 $y_i = u(\boldsymbol{x}_i) + \epsilon_i$ 且噪声 $\epsilon_i \sim \mathcal{N}(0, \sigma_d^2)$，那么数据拟合损失项 $\sum (u_\theta(\boldsymbol{x}_i) - y_i)^2$ 对应于高斯[似然函数](@entry_id:141927)的负对数。[测量噪声](@entry_id:275238)是不可避免的随机性，被称为**[偶然不确定性](@entry_id:154011)（aleatoric uncertainty）**。数据损失项的权重 $\lambda_d$ 应与噪声[方差](@entry_id:200758)的倒数成正比，即 $\lambda_d \propto \sigma_d^{-2}$。

*   **物理损失项**：PDE 残差损失项和边界条件损失项可以被看作是**先验（priors）**的负对数。它们编码了我们对解应该遵循的物理定律的“信念”。这种由于我们对模型本身不完美的认知（例如，PDE 的系数 $\kappa(\boldsymbol{x})$ 可能不完全准确）而产生的不确定性被称为**[认知不确定性](@entry_id:149866)（epistemic uncertainty）**。物理损失项的作用是施加**认知控制（epistemic control）**，迫使解符合我们的物理知识。同样，在假设这些残差服从高斯分布的情况下，其对应的权重 $\lambda_f, \lambda_b$ 与我们对物理模型或边界条件信任度的“[方差](@entry_id:200758)”的倒数成正比。例如，如果边界条件是通过高精度实验控制的，其不确定性很小（$\sigma_b^2 \to 0$），那么对应的权重应该非常大（$\lambda_b \to \infty$），这实际上等同于一个硬约束。

这种贝叶斯视角不仅为如何设置损失权重提供了理论指导，还将 PINN 与更广泛的[不确定性量化](@entry_id:138597)（UQ）领域联系起来。

### 训练挑战与缓解策略

尽管 PINN 的原理优雅，但在实践中，其训练过程并非一帆风顺。两个主要的挑战是训练动态的[平衡问题](@entry_id:636409)和[神经网](@entry_id:276355)络的谱偏差。

#### 训练的多目标本质

PINN 的训练本质上是一个**[多目标优化](@entry_id:637420)问题**。我们试图同时最小化多个（通常是相互冲突的）目标，如 PDE 残差损失 $L_r$、边界损失 $L_b$ 和初始损失 $L_0$。使用加权的[标量化](@entry_id:634761)损失函数 $L = \lambda_r L_r + \lambda_b L_b + \dots$ 是解决这类问题的一种常用方法，它旨在找到[帕累托最优](@entry_id:636539)前沿（Pareto front）上的一个特定权衡点。

然而，一个**朴素的、固定的权重选择**往往会导致训练失败。这是因为不同损失项的量级、梯度的大小以及它们在训练过程中的演化速度可能存在巨大差异。例如，如果 PDE 算子 $\mathcal{N}$ 包含高阶导数，其残差 $L_r$ 的梯度 $\nabla_\theta L_r$ 的幅度可能比边界损失的梯度 $\nabla_\theta L_b$ 大好几个[数量级](@entry_id:264888)。如果权重 $\lambda_r$ 和 $\lambda_b$ 没有恰当地平衡这种差异，总梯度 $\nabla_\theta L = \lambda_r \nabla_\theta L_r + \lambda_b \nabla_\theta L_b$ 将几乎完全由占主导地位的项决定。这将导致优化过程**完全忽略**了其他目标，例如，网络可能很好地满足了 PDE 但严重违反了边界条件。

这种**梯度不平衡**是 PINN 训练中最常见的病理之一。为了解决这个问题，研究人员提出了多种策略，包括对变量进行[无量纲化](@entry_id:136704)、使用**自适应权重**方案（在训练过程中动态调整权重以平衡不同梯度的贡献），或者采用真正的[多目标优化](@entry_id:637420)算法。

#### 谱偏差

深度神经网络在通过[梯度下降](@entry_id:145942)进行训练时，表现出一种被称为**谱偏差（spectral bias）**的强烈倾向：它们会优先学习目标函数的**低频分量**，而学习高频分量的速度则要慢得多。这种现象可以通过[神经正切核](@entry_id:634487)（Neural Tangent Kernel, NTK）理论来解释，该理论表明，标准[神经网](@entry_id:276355)络的梯度更新机制对于产生高频输出变化是“迟钝”的。

对于许多物理问题，尤其是波动现象（如声波或[地震波传播](@entry_id:165726)），解函数包含至关重要的**高频（或高[波数](@entry_id:172452)）**信息，这些信息对应于尖锐的[波前](@entry_id:197956)、小尺度的散射体或来自界面的精细反射。谱偏差使得标准的 PINN 在学习这些高频特征时举步维艰，导致解[过度平滑](@entry_id:634349)，丢失大量物理细节。虽然 PDE 中的[高阶导数算子](@entry_id:750301)（如 $\partial_t^2, \nabla^2$）在傅里叶域中会乘以频率的平方（$-\omega^2, -\|\boldsymbol{k}\|^2$），从而放大了高频部分的误差信号，但这往往不足以克服网络内在的谱偏差。

为了缓解谱偏差，一种有效的策略是采用**频率课程学习（frequency curriculum）**。其核心思想是，不是让网络一开始就面对包含所有频率的复杂目标，而是从一个“简单”的低频问题开始，然后逐步增加难度。

一个具体的实现是**傅里叶特征映射（Fourier feature mapping）**。我们将原始的输入坐标 $(\boldsymbol{x}, t)$ 通过一个映射 $\gamma$ 转换为一组正弦和余弦特征：

$$
\gamma(\boldsymbol{x}, t) = [\cos(2\pi \boldsymbol{B} (\boldsymbol{x}, t)^T), \sin(2\pi \boldsymbol{B} (\boldsymbol{x}, t)^T)]^T
$$

其中 $\boldsymbol{B}$ 是一个频率矩阵。然后，[神经网](@entry_id:276355)络学习的是这些傅里叶特征的函数。课程学习可以通过**退火（annealing）**来实现，即在训练过程中逐步增加频率矩阵 $\boldsymbol{B}$ 中所包含的最高频率。这样，网络首先在一个低频带内学习解的宏观结构，然后随着训练的进行，逐渐获得表示和学习更高频细节的能力。同时，为了避免数值混叠，配点的采样密度也必须随着频率的增加而相应提高，以满足奈奎斯特采样准则。