{
    "hands_on_practices": [
        {
            "introduction": "At the heart of most modern Bayesian inference algorithms, from variational methods to Hamiltonian Monte Carlo, lies the gradient of the log-posterior density. For complex physical models common in geophysics, computing this gradient efficiently is non-trivial. This practice introduces the adjoint-state method, the cornerstone for calculating gradients in large-scale inverse problems, by having you implement and numerically verify it for a simplified tomography problem. Mastering this technique by implementing it from first principles is essential for developing and debugging robust, gradient-based inference codes. ",
            "id": "3577517",
            "problem": "You are given a linearized one-dimensional travel-time tomography setup that serves as a simplified forward model in computational geophysics. The unknown subsurface slowness model is represented by a vector $m \\in \\mathbb{R}^N$, where each component parameterizes a cell with unit length. Each data point is a travel time measurement, which is modeled as a sum over a contiguous block of cells. This gives a linear forward operator $A \\in \\mathbb{R}^{M \\times N}$, where each row selects a contiguous block of $m$ and sums its entries with weight $1$. Observations are generated as $d = A m_{\\mathrm{true}} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_d^2 I_M)$.\n\nAssume a Gaussian prior $m \\sim \\mathcal{N}(m_0, C_m)$ with $m_0 \\in \\mathbb{R}^N$ and $C_m = \\sigma_m^2 I_N$. With the Gaussian likelihood and prior, the negative log-posterior (up to an additive constant independent of $m$) is\n$$\n\\Phi(m) = \\frac{1}{2} \\left\\| \\frac{A m - d}{\\sigma_d} \\right\\|_2^2 + \\frac{1}{2} \\left\\| \\frac{m - m_0}{\\sigma_m} \\right\\|_2^2.\n$$\nYour task is to implement and verify adjoint methods and gradient consistency for this objective $\\Phi(m)$ via first principles.\n\nFundamental base to be used:\n- Bayes’ theorem relating posterior, likelihood, and prior under Gaussian assumptions.\n- Definition of the negative log-posterior as an objective function for inference.\n- Definition of the gradient of a scalar functional via the Fréchet derivative.\n- Definition of the adjoint operator $A^\\ast$ with respect to the Euclidean inner product, characterized by $\\langle A x, y \\rangle = \\langle x, A^\\ast y \\rangle$ for all $x, y$.\n- Central difference approximation for a directional derivative:\n$$\n\\frac{\\partial \\Phi(m)}{\\partial s} \\approx \\frac{\\Phi(m + h s) - \\Phi(m - h s)}{2 h},\n$$\nfor a direction $s$ and small step $h$.\n\nImplement the following in a complete, runnable program:\n1. Construction of the forward operator $A \\in \\mathbb{R}^{M \\times N}$ from a prescribed random seed by generating $M$ rays; each ray corresponds to a contiguous block of indices. For each row $i$, draw a block length $\\ell_i$ uniformly from $\\{1, 2, \\dots, \\lfloor N/2 \\rfloor\\}$, then a start index $b_i$ uniformly from $\\{0, 1, \\dots, N - \\ell_i\\}$, and set $A_{i,j} = 1$ if $j \\in \\{b_i, b_i+1, \\dots, b_i+\\ell_i-1\\}$ and $A_{i,j} = 0$ otherwise.\n2. A function to evaluate $\\Phi(m)$ for any $m$ given $A$, $d$, $m_0$, $\\sigma_d$, and $\\sigma_m$.\n3. A function to evaluate the gradient $\\nabla \\Phi(m)$ using only the definitions above and the adjoint operator characterization. You must not hard-code a pre-derived closed-form gradient; instead, use the chain rule and the adjoint notion to map residuals back to the parameter space.\n4. An adjoint test that verifies the property $\\langle A x, y \\rangle = \\langle x, A^\\ast y \\rangle$ for random vectors $x \\in \\mathbb{R}^N$ and $y \\in \\mathbb{R}^M$, using the Euclidean inner product $\\langle u, v \\rangle = u^\\top v$. Report the maximum relative discrepancy over several random draws:\n$$\n\\mathrm{err}_{\\mathrm{adj}} = \\max_{k} \\frac{|\\langle A x_k, y_k \\rangle - \\langle x_k, A^\\ast y_k \\rangle|}{\\max\\{1, |\\langle A x_k, y_k \\rangle|, |\\langle x_k, A^\\ast y_k \\rangle|\\}}.\n$$\n5. A gradient consistency test that checks, for a random unit-norm direction $s \\in \\mathbb{R}^N$ and a set of step sizes $\\{h_j\\}$, whether the central-difference directional derivative matches the inner product $s^\\top \\nabla \\Phi(m)$. For each $h_j$, compute\n$$\n\\mathrm{err}_j = \\frac{\\left| \\frac{\\Phi(m + h_j s) - \\Phi(m - h_j s)}{2 h_j} - s^\\top \\nabla \\Phi(m) \\right|}{\\max\\{1, \\left| \\frac{\\Phi(m + h_j s) - \\Phi(m - h_j s)}{2 h_j} \\right|, |s^\\top \\nabla \\Phi(m)|\\}},\n$$\nand report $\\min_j \\mathrm{err}_j$ as the gradient consistency error.\n\nTest suite and required behavior:\nImplement three independent test cases. For each case, independently construct $A$, draw a ground truth $m_{\\mathrm{true}}$ and noise to generate $d$, and choose a test point $m$ and a direction $s$. Unless otherwise stated, set $m_0 = 0$. Use the following parameter sets:\n\n- Case $1$ (general case):\n  - $N = 64$, $M = 48$.\n  - Random seeds: $A$: $1$, $m_{\\mathrm{true}}$: $2$, noise: $3$, gradient direction and test point: $4$.\n  - Noise standard deviation: $\\sigma_d = 0.05$.\n  - Prior standard deviation: $\\sigma_m = 0.5$.\n  - Central-difference step sizes: $[10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}]$.\n  - Tolerances: $\\mathrm{tol}_{\\mathrm{adj}} = 10^{-12}$, $\\mathrm{tol}_{\\mathrm{grad}} = 10^{-6}$.\n\n- Case $2$ (larger, stronger regularization):\n  - $N = 128$, $M = 96$.\n  - Random seeds: $A$: $10$, $m_{\\mathrm{true}}$: $20$, noise: $30$, gradient direction and test point: $40$.\n  - Noise standard deviation: $\\sigma_d = 0.10$.\n  - Prior standard deviation: $\\sigma_m = 0.2$.\n  - Central-difference step sizes: $[10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}]$.\n  - Tolerances: $\\mathrm{tol}_{\\mathrm{adj}} = 10^{-11}$, $\\mathrm{tol}_{\\mathrm{grad}} = 5 \\cdot 10^{-6}$.\n\n- Case $3$ (edge case with few data and weaker regularization):\n  - $N = 32$, $M = 8$.\n  - Random seeds: $A$: $100$, $m_{\\mathrm{true}}$: $200$, noise: $300$, gradient direction and test point: $400$.\n  - Noise standard deviation: $\\sigma_d = 0.20$.\n  - Prior standard deviation: $\\sigma_m = 1.0$.\n  - Central-difference step sizes: $[10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}]$.\n  - Tolerances: $\\mathrm{tol}_{\\mathrm{adj}} = 10^{-12}$, $\\mathrm{tol}_{\\mathrm{grad}} = 10^{-5}$.\n\nFor each case:\n- Perform the adjoint test using $K = 5$ independent draws of $x$ and $y$ (e.g., standard normal entries).\n- For the gradient test, draw $m$ and $s$ independently with standard normal entries, then set $s \\leftarrow s / \\|s\\|_2$.\n\nAcceptance criteria per case:\n- Adjoint test passes if $\\mathrm{err}_{\\mathrm{adj}} \\le \\mathrm{tol}_{\\mathrm{adj}}$.\n- Gradient test passes if $\\min_j \\mathrm{err}_j \\le \\mathrm{tol}_{\\mathrm{grad}}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain six Boolean values in the following order: $[\\text{adj\\_pass\\_1}, \\text{grad\\_pass\\_1}, \\text{adj\\_pass\\_2}, \\text{grad\\_pass\\_2}, \\text{adj\\_pass\\_3}, \\text{grad\\_pass\\_3}]$.",
            "solution": "The problem statement has been meticulously reviewed and is deemed valid. It is scientifically grounded in the principles of Bayesian inverse problems and numerical linear algebra, is well-posed with a complete and consistent set of definitions and parameters, and is formalizable into a concrete computational task. The problem asks for the implementation and verification of fundamental numerical methods—specifically, the adjoint method for gradient computation and its verification via a Taylor test—which are central to the field of computational geophysics.\n\nThe solution proceeds by first deriving the necessary mathematical expressions from first principles and then implementing them in a structured program that executes the specified test cases.\n\n**1. Theoretical Formulation**\n\nThe core of the task is to compute the gradient of the negative log-posterior objective function, $\\Phi(m)$, and to verify its correctness. The objective function is given by:\n$$\n\\Phi(m) = \\frac{1}{2} \\left\\| \\frac{A m - d}{\\sigma_d} \\right\\|_2^2 + \\frac{1}{2} \\left\\| \\frac{m - m_0}{\\sigma_m} \\right\\|_2^2\n$$\nThis function can be decomposed into two parts: a likelihood term $\\Phi_{\\text{like}}(m)$ and a prior term $\\Phi_{\\text{prior}}(m)$.\n\n$\\Phi_{\\text{like}}(m) = \\frac{1}{2\\sigma_d^2} \\| A m - d \\|_2^2$\n$\\Phi_{\\text{prior}}(m) = \\frac{1}{2\\sigma_m^2} \\| m - m_0 \\|_2^2$\n\nThe gradient $\\nabla \\Phi(m)$ is the sum of the gradients of these two terms: $\\nabla \\Phi(m) = \\nabla \\Phi_{\\text{like}}(m) + \\nabla \\Phi_{\\text{prior}}(m)$.\n\n**Gradient of the Prior Term**\n\nThe prior term is a simple quadratic function of $m$. Its gradient is found directly:\n$\\nabla \\Phi_{\\text{prior}}(m) = \\nabla_m \\left( \\frac{1}{2\\sigma_m^2} (m - m_0)^\\top(m - m_0) \\right) = \\frac{1}{2\\sigma_m^2} \\cdot 2(m-m_0) = \\frac{1}{\\sigma_m^2} (m - m_0)$.\n\n**Gradient of the Likelihood Term via the Adjoint Method**\n\nThe gradient of the likelihood term is derived using the chain rule and the definition of the adjoint operator. The Fréchet derivative of $\\Phi_{\\text{like}}(m)$ in a direction $s \\in \\mathbb{R}^N$ is given by the directional derivative:\n$$\nD\\Phi_{\\text{like}}(m)[s] = \\lim_{h \\to 0} \\frac{\\Phi_{\\text{like}}(m + hs) - \\Phi_{\\text{like}}(m)}{h}\n$$\nBy definition, this directional derivative is equal to the inner product $\\langle \\nabla \\Phi_{\\text{like}}(m), s \\rangle$. Let the data residual be $r(m) = Am - d$. Then $\\Phi_{\\text{like}}(m) = \\frac{1}{2\\sigma_d^2} \\langle r(m), r(m) \\rangle$.\nUsing the chain rule, the derivative of $\\Phi_{\\text{like}}$ is composed of the derivative of the norm-squared function and the derivative of the residual function $r(m)$. The derivative of $r(m)$ with respect to $m$ in direction $s$ is $A s$.\n$$\nD\\Phi_{\\text{like}}(m)[s] = \\frac{1}{\\sigma_d^2} \\langle r(m), A s \\rangle = \\frac{1}{\\sigma_d^2} \\langle Am - d, A s \\rangle\n$$\nThe \"adjoint method\" consists of using the property of the adjoint operator $A^\\ast$ to move the operator $A$ from the second argument of the inner product to the first. For the standard Euclidean inner product, the adjoint $A^\\ast$ is the transpose $A^\\top$.\n$$\n\\langle Am - d, A s \\rangle = \\langle A^\\ast (Am - d), s \\rangle = \\langle A^\\top (Am - d), s \\rangle\n$$\nThus, we have:\n$$\n\\langle \\nabla \\Phi_{\\text{like}}(m), s \\rangle = D\\Phi_{\\text{like}}(m)[s] = \\frac{1}{\\sigma_d^2} \\langle A^\\top (Am - d), s \\rangle\n$$\nSince this equality must hold for all directions $s$, we can identify the gradient as:\n$$\n\\nabla \\Phi_{\\text{like}}(m) = \\frac{1}{\\sigma_d^2} A^\\top (Am - d)\n$$\nThis derivation illustrates the principle: to find the gradient's effect on the parameter space ($m$), we first compute the residual in the data space ($Am - d$) and then map it back to the parameter space using the adjoint operator ($A^\\top$).\n\n**Total Gradient**\n\nCombining the two components, the full gradient is:\n$$\n\\nabla \\Phi(m) = \\frac{1}{\\sigma_d^2} A^\\top (Am - d) + \\frac{1}{\\sigma_m^2} (m - m_0)\n$$\nThe implementation will use functions `apply_A(m)` for the forward operation $Am$ and `apply_A_adjoint(y)` for the adjoint operation $A^\\top y$, adhering to the \"adjoint-state\" methodology.\n\n**2. Numerical Verification Protocol**\n\n**Adjoint Test**\n\nThis test verifies that the implemented adjoint operator, `apply_A_adjoint`, is indeed the adjoint of the forward operator, `apply_A`. For real matrices and the Euclidean inner product ($\\langle u, v \\rangle = u^\\top v$), the adjoint is the transpose, so we must verify $\\langle Ay, x \\rangle = \\langle y, A^\\top x \\rangle$. The test is performed for $K=5$ pairs of random vectors $x_k \\in \\mathbb{R}^N$ and $y_k \\in \\mathbb{R}^M$. The test passes if the maximum relative discrepancy is below a given tolerance $\\mathrm{tol}_{\\mathrm{adj}}$.\n$$\n\\mathrm{err}_{\\mathrm{adj}} = \\max_{k} \\frac{|\\langle A x_k, y_k \\rangle - \\langle x_k, A^\\top y_k \\rangle|}{\\max\\{1, |\\langle A x_k, y_k \\rangle|, |\\langle x_k, A^\\top y_k \\rangle|\\}} \\le \\mathrm{tol}_{\\mathrm{adj}}\n$$\nA value near machine precision for $\\mathrm{err}_{\\mathrm{adj}}$ confirms the correctness of the adjoint implementation.\n\n**Gradient Consistency Test (Taylor Test)**\n\nThis test verifies that the implemented gradient function, `evaluate_grad_Phi`, is correct. It compares the analytical directional derivative, $g_{\\text{analytic}} = s^\\top \\nabla \\Phi(m)$, with a numerical approximation obtained via a central finite difference, $g_{\\text{fd}}$.\n$$\ng_{\\text{fd}}(h) = \\frac{\\Phi(m + h s) - \\Phi(m - h s)}{2 h}\n$$\nThe central difference scheme has a truncation error of order $O(h^2)$. For very small $h$, round-off error from floating-point arithmetic becomes dominant. The test calculates the relative error for a series of step sizes $h_j$ and reports the minimum error found.\n$$\n\\mathrm{err}_{\\text{grad}} = \\min_{j} \\frac{| g_{\\text{fd}}(h_j) - g_{\\text{analytic}} |}{\\max\\{1, |g_{\\text{fd}}(h_j)|, |g_{\\text{analytic}}|\\}}\n$$\nThe test passes if this minimum error is below a specified tolerance $\\mathrm{tol}_{\\mathrm{grad}}$. A small error confirms that the derived gradient expression is correctly implemented.\n\nThe implementation below follows this protocol for the three specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef construct_A(N, M, seed):\n    \"\"\"\n    Constructs the forward operator matrix A.\n    \n    Args:\n        N (int): Number of model parameters (columns).\n        M (int): Number of data points (rows).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        np.ndarray: The MxN forward operator matrix A.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = np.zeros((M, N))\n    max_len = N // 2\n    for i in range(M):\n        length = rng.integers(1, max_len + 1)\n        start = rng.integers(0, N - length + 1)\n        A[i, start : start + length] = 1.0\n    return A\n\ndef apply_A(A, m):\n    \"\"\"Applies the forward operator A to a model vector m.\"\"\"\n    return A @ m\n\ndef apply_A_adjoint(A, y):\n    \"\"\"Applies the adjoint of the operator A to a data vector y.\"\"\"\n    return A.T @ y\n\ndef test_adjoint(A, K, seed):\n    \"\"\"\n    Performs the adjoint test for the operator A and its adjoint.\n\n    Args:\n        A (np.ndarray): The MxN forward operator matrix.\n        K (int): Number of random trials.\n        seed (int): Random seed for generating vectors.\n\n    Returns:\n        float: The maximum relative discrepancy found.\n    \"\"\"\n    M, N = A.shape\n    rng = np.random.default_rng(seed)\n    max_rel_err = 0.0\n    \n    for _ in range(K):\n        x = rng.standard_normal(size=N)\n        y = rng.standard_normal(size=M)\n\n        lhs = np.dot(y, apply_A(A, x))\n        rhs = np.dot(x, apply_A_adjoint(A, y))\n\n        diff = np.abs(lhs - rhs)\n        denom = max(1.0, np.abs(lhs), np.abs(rhs))\n        rel_err = diff / denom\n        \n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            \n    return max_rel_err\n\ndef evaluate_Phi(m, A, d, m0, sigma_d, sigma_m):\n    \"\"\"\n    Evaluates the negative log-posterior objective function Phi(m).\n    \"\"\"\n    residual_data = apply_A(A, m) - d\n    term_like = 0.5 * np.sum((residual_data / sigma_d)**2)\n    \n    residual_prior = m - m0\n    term_prior = 0.5 * np.sum((residual_prior / sigma_m)**2)\n    \n    return term_like + term_prior\n\ndef evaluate_grad_Phi(m, A, d, m0, sigma_d, sigma_m):\n    \"\"\"\n    Evaluates the gradient of Phi(m) using the adjoint method.\n    \"\"\"\n    # Gradient of likelihood term\n    residual_data = apply_A(A, m) - d\n    grad_like = apply_A_adjoint(A, residual_data) / (sigma_d**2)\n    \n    # Gradient of prior term\n    residual_prior = m - m0\n    grad_prior = residual_prior / (sigma_m**2)\n    \n    return grad_like + grad_prior\n\ndef test_gradient(A, d, m0, sigma_d, sigma_m, m_test, s, h_values):\n    \"\"\"\n    Performs the gradient consistency test (Taylor test).\n\n    Returns:\n        float: The minimum relative error found.\n    \"\"\"\n    # Analytical directional derivative\n    grad_val = evaluate_grad_Phi(m_test, A, d, m0, sigma_d, sigma_m)\n    grad_proj = np.dot(s, grad_val)\n\n    min_rel_err = np.inf\n    \n    for h in h_values:\n        phi_plus = evaluate_Phi(m_test + h * s, A, d, m0, sigma_d, sigma_m)\n        phi_minus = evaluate_Phi(m_test - h * s, A, d, m0, sigma_d, sigma_m)\n        \n        fd_approx = (phi_plus - phi_minus) / (2 * h)\n        \n        diff = np.abs(fd_approx - grad_proj)\n        denom = max(1.0, np.abs(fd_approx), np.abs(grad_proj))\n        rel_err = diff / denom\n        \n        if rel_err  min_rel_err:\n            min_rel_err = rel_err\n            \n    return min_rel_err\n\ndef run_case(params):\n    \"\"\"\n    Runs a single test case with given parameters.\n    \"\"\"\n    N, M = params[\"N\"], params[\"M\"]\n    seeds = params[\"seeds\"]\n    sigma_d, sigma_m = params[\"sigma_d\"], params[\"sigma_m\"]\n    h_values = params[\"h_values\"]\n    tol_adj, tol_grad = params[\"tol_adj\"], params[\"tol_grad\"]\n\n    # 1. Construct A\n    A = construct_A(N, M, seeds[\"A\"])\n\n    # 2. Adjoint test\n    # The seed for adjoint test vectors is part of the grad/test point seed dict key\n    err_adj = test_adjoint(A, K=5, seed=seeds[\"grad\"])\n    adj_pass = err_adj = tol_adj\n\n    # 3. Generate data\n    rng_m_true = np.random.default_rng(seeds[\"m_true\"])\n    m_true = rng_m_true.standard_normal(size=N)\n    \n    rng_noise = np.random.default_rng(seeds[\"noise\"])\n    noise = rng_noise.standard_normal(size=M) * sigma_d\n\n    d = apply_A(A, m_true) + noise\n    m0 = np.zeros(N)\n\n    # 4. Gradient test\n    rng_grad = np.random.default_rng(seeds[\"grad\"])\n    m_test = rng_grad.standard_normal(size=N)\n    s = rng_grad.standard_normal(size=N)\n    s /= np.linalg.norm(s)\n\n    err_grad = test_gradient(A, d, m0, sigma_d, sigma_m, m_test, s, h_values)\n    grad_pass = err_grad = tol_grad\n    \n    return adj_pass, grad_pass\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 64, \"M\": 48,\n            \"seeds\": {\"A\": 1, \"m_true\": 2, \"noise\": 3, \"grad\": 4},\n            \"sigma_d\": 0.05, \"sigma_m\": 0.5,\n            \"h_values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n            \"tol_adj\": 1e-12, \"tol_grad\": 1e-6\n        },\n        {\n            \"N\": 128, \"M\": 96,\n            \"seeds\": {\"A\": 10, \"m_true\": 20, \"noise\": 30, \"grad\": 40},\n            \"sigma_d\": 0.10, \"sigma_m\": 0.2,\n            \"h_values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n            \"tol_adj\": 1e-11, \"tol_grad\": 5e-6\n        },\n        {\n            \"N\": 32, \"M\": 8,\n            \"seeds\": {\"A\": 100, \"m_true\": 200, \"noise\": 300, \"grad\": 400},\n            \"sigma_d\": 0.20, \"sigma_m\": 1.0,\n            \"h_values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n            \"tol_adj\": 1e-12, \"tol_grad\": 1e-5\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        adj_pass, grad_pass = run_case(case_params)\n        results.extend([adj_pass, grad_pass])\n    \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Geophysical data can sometimes be consistent with several distinct subsurface models, leading to multimodal posterior distributions that pose a major challenge for uncertainty quantification. This exercise explores this scenario through a simple model where the prior encodes competing geological hypotheses, creating a bimodal posterior. You will implement and analyze the effect of tempering, a technique that flattens the likelihood to facilitate exploration between modes, providing a tangible understanding of how to navigate complex posterior landscapes in Bayesian inversion. ",
            "id": "3577524",
            "problem": "Consider a one-dimensional Bayesian inversion problem for straight-ray travel time in a homogeneous medium, a canonical setting in computational geophysics. The unknown is the seismic velocity $v$ of a homogeneous layer. A ray of length $L$ induces a noisily measured travel time $t_{\\text{obs}}$. The forward operator follows basic kinematics: the travel time $t$ satisfies $t = L / v$. Assume independent and identically distributed Gaussian measurement noise with known standard deviation $\\sigma_t$, so that the likelihood for a given $v$ is $p(t_{\\text{obs}} \\mid v) = \\mathcal{N}\\left(t_{\\text{obs}}; L/v, \\sigma_t^2\\right)$. The geological prior encodes mutually exclusive lithologies as a mixture of two Gaussian components in $v$, with component means $v_1$ and $v_2$, standard deviations $s_1$ and $s_2$, and mixture weights $w_1$ and $w_2 = 1 - w_1$. Explicitly, the prior density is\n$$\np(v) = w_1 \\, \\mathcal{N}\\left(v; v_1, s_1^2\\right) + w_2 \\, \\mathcal{N}\\left(v; v_2, s_2^2\\right),\n$$\nwhich produces multimodality in the posterior under certain configurations of $(t_{\\text{obs}}, \\sigma_t)$.\n\nTo explore tempering strategies that address multimodality, define the tempered posterior for a power temperature parameter $\\beta \\in (0,1]$ by\n$$\np_{\\beta}(v \\mid t_{\\text{obs}}) \\propto p(v) \\, \\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta}.\n$$\nFor $\\beta = 1$ this coincides with the usual posterior, while smaller $\\beta$ values flatten the likelihood and reduce posterior concentration, which can aid exploration across modes.\n\nYour task is to implement a complete program that, for the given test suite of cases, computes the fraction of the tempered posterior probability mass that lies in the lower-velocity mode region, defined as $v \\leq v_{\\text{mid}}$ with $v_{\\text{mid}} = (v_1 + v_2)/2$. For each case and for three temperatures $\\beta \\in \\{1.0, 0.5, 0.1\\}$, the program must discretize the velocity domain on a uniform grid, evaluate the unnormalized tempered posterior $p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta}$, and then compute the mass fraction\n$$\nf_{1}(\\beta) = \\frac{\\int_{v_{\\min}}^{v_{\\text{mid}}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v}{\\int_{v_{\\min}}^{v_{\\max}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v},\n$$\nwhere $[v_{\\min}, v_{\\max}]$ is a fixed velocity range chosen to contain the bulk of the prior probability. Report these fractions as unitless decimals rounded to six digits after the decimal point. Use the following physical units and constants: $L$ in meters (m), $v$ in meters per second (m/s), $t_{\\text{obs}}$ and $\\sigma_t$ in seconds (s).\n\nUse a uniform velocity grid with $v_{\\min} = 1000$ m/s, $v_{\\max} = 5000$ m/s, and grid spacing $\\Delta v = 1$ m/s for numerical integration. The Gaussian normal density must be evaluated with its full normalizing constant to ensure scientific consistency.\n\nTest suite:\n1. Ambiguous case with significant likelihood width enabling both lithologies to be plausible:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.33$ s, $\\sigma_t = 0.08$ s,\n   - $w_1 = 0.5$, $v_1 = 2000$ m/s, $s_1 = 200$ m/s,\n   - $w_2 = 0.5$, $v_2 = 3500$ m/s, $s_2 = 300$ m/s.\n2. Strongly informative case favoring the higher-velocity lithology:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.286$ s, $\\sigma_t = 0.01$ s,\n   - $w_1 = 0.5$, $v_1 = 2000$ m/s, $s_1 = 200$ m/s,\n   - $w_2 = 0.5$, $v_2 = 3500$ m/s, $s_2 = 300$ m/s.\n3. Diffuse case where the likelihood is wide and the posterior approaches the prior mixture:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.40$ s, $\\sigma_t = 0.20$ s,\n   - $w_1 = 0.5$, $v_1 = 2000$ m/s, $s_1 = 200$ m/s,\n   - $w_2 = 0.5$, $v_2 = 3500$ m/s, $s_2 = 300$ m/s.\n4. Skewed prior case where the prior favors the lower-velocity lithology but data are more consistent with higher velocity:\n   - $L = 1000$ m, $t_{\\text{obs}} = 0.32$ s, $\\sigma_t = 0.03$ s,\n   - $w_1 = 0.7$, $v_1 = 2200$ m/s, $s_1 = 150$ m/s,\n   - $w_2 = 0.3$, $v_2 = 3800$ m/s, $s_2 = 400$ m/s.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output exactly three numbers: $f_1(1.0)$, $f_1(0.5)$, and $f_1(0.1)$, in that order and rounded to six decimals.\n- Aggregate the results for all test cases in sequence. For example, the output must look like\n$[f_{1}^{(1)}(1.0),f_{1}^{(1)}(0.5),f_{1}^{(1)}(0.1),f_{1}^{(2)}(1.0),\\dots,f_{1}^{(4)}(0.1)]$,\nwhere superscripts index the test cases $1$ to $4$.\n\nAll answers are unitless decimals. No percentage signs are permitted anywhere in the output.",
            "solution": "The design begins from basic principles relevant to Bayesian inversion and uncertainty quantification in computational geophysics. The forward operator is defined by the kinematic relation $t = L/v$ for straight-ray travel time in a homogeneous medium, where $L$ is a known path length and $v$ is the unknown velocity. The data model assumes Gaussian noise with standard deviation $\\sigma_t$, hence the likelihood for observing $t_{\\text{obs}}$ given $v$ is\n$$\np(t_{\\text{obs}} \\mid v) = \\frac{1}{\\sqrt{2\\pi}\\sigma_t} \\exp\\left( -\\frac{\\left(t_{\\text{obs}} - L/v\\right)^2}{2\\sigma_t^2} \\right).\n$$\nWe posit a bimodal prior for $v$ reflecting two competing geological hypotheses. This is modeled as a mixture of Gaussian densities,\n$$\np(v) = w_1 \\frac{1}{\\sqrt{2\\pi}s_1} \\exp\\left( -\\frac{(v - v_1)^2}{2 s_1^2} \\right) + w_2 \\frac{1}{\\sqrt{2\\pi}s_2} \\exp\\left( -\\frac{(v - v_2)^2}{2 s_2^2} \\right),\n$$\nwith $w_2 = 1 - w_1$, $v_1, v_2$ the modal velocities, and $s_1, s_2$ their standard deviations. Under Bayes' theorem, the posterior density is proportional to the product $p(v) p(t_{\\text{obs}} \\mid v)$. Multimodality arises because the prior itself has two separated modes, and the likelihood does not necessarily eliminate either mode if it is sufficiently broad relative to the separation of $v_1$ and $v_2$ or if $t_{\\text{obs}}$ is consistent with both hypotheses.\n\nA tempering strategy modifies the posterior by a power parameter $\\beta \\in (0,1]$ applied to the likelihood, yielding the tempered posterior density\n$$\np_{\\beta}(v \\mid t_{\\text{obs}}) \\propto p(v) \\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta}.\n$$\nThis formulation is known as the power posterior or tempered posterior. For $\\beta = 1$, $p_{\\beta}$ coincides with the usual posterior; for smaller $\\beta$, the effect of the likelihood is diminished, producing a flatter distribution that allows the exploration of multiple modes. Such tempering is common in Markov Chain Monte Carlo (MCMC) methods like parallel tempering to facilitate transitions between modes.\n\nTo compute the posterior mass fraction associated with the lower-velocity mode region, we partition the velocity space at the midpoint $v_{\\text{mid}} = (v_1 + v_2)/2$. Define the fraction\n$$\nf_1(\\beta) = \\frac{\\int_{v_{\\min}}^{v_{\\text{mid}}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v}{\\int_{v_{\\min}}^{v_{\\max}} p(v)\\left[p(t_{\\text{obs}} \\mid v)\\right]^{\\beta} \\, \\mathrm{d}v}\n$$\nfor a fixed domain $[v_{\\min}, v_{\\max}]$ that contains the bulk of the prior mass. Because all densities are nonnegative, $f_1(\\beta) \\in [0,1]$, and the complement $1 - f_1(\\beta)$ represents the mass in the upper-velocity mode region.\n\nAlgorithmic approach:\n1. Choose a uniform velocity grid $v_k = v_{\\min} + k \\Delta v$ for $k = 0, 1, \\dots, K$ with $v_{\\min} = 1000$ m/s, $v_{\\max} = 5000$ m/s, and $\\Delta v = 1$ m/s. This grid is sufficiently fine for smooth Gaussian densities.\n2. For each grid point $v_k$, compute the prior density $p(v_k)$ as the sum of weighted Gaussian components with their proper normalizing constants.\n3. Compute the likelihood $p(t_{\\text{obs}} \\mid v_k)$ using the Gaussian formula centered at $L/v_k$ with standard deviation $\\sigma_t$.\n4. For each temperature $\\beta \\in \\{1.0, 0.5, 0.1\\}$, compute the unnormalized tempered posterior $u_{\\beta}(v_k) = p(v_k)\\left[p(t_{\\text{obs}} \\mid v_k)\\right]^{\\beta}$.\n5. Compute the normalization constant $Z_{\\beta} = \\sum_k u_{\\beta}(v_k) \\Delta v$ and the lower-mode mass $M_{\\beta}^{\\text{low}} = \\sum_{v_k \\leq v_{\\text{mid}}} u_{\\beta}(v_k) \\Delta v$.\n6. The fraction is $f_1(\\beta) = M_{\\beta}^{\\text{low}} / Z_{\\beta}$. Because the grid is uniform, including $\\Delta v$ ensures dimensional consistency; note that it cancels in the ratio, but its presence is scientifically correct when expressing numerical integrals.\n\nNumerical and physical consistency:\n- The Gaussian densities are evaluated with their full normalizing constants, which is important for scientific realism, even though ratios will not depend on these constants.\n- The forward model $t = L/v$ stems from basic kinematics and is valid for a homogeneous medium and straight rays, which is a common reference case in geophysics.\n- The chosen test cases ensure coverage of multimodality handling:\n  - An ambiguous scenario where both modes can carry significant mass at $\\beta = 1$ or under tempering.\n  - A strongly informative scenario where the likelihood selects the higher-velocity mode.\n  - A diffuse scenario where the posterior approaches the prior mixture due to broad likelihood.\n  - A skewed prior scenario testing the interplay between prior weights and data consistency.\n\nImplementation details:\n- The program defines the test cases as a list, evaluates $f_1(\\beta)$ for each, and prints a single line list of floats rounded to six decimals, as required.\n- All outputs are unitless decimals; grid integration uses $\\Delta v = 1$ m/s to produce consistent ratios.\n\nThis procedure from first principles demonstrates how multimodality emerges from mixture priors in Bayesian inversion and how tempering modifies the posterior to facilitate uncertainty quantification across competing geological hypotheses.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef gaussian_pdf(x, mean, std):\n    \"\"\"Compute the Gaussian density with full normalizing constant.\"\"\"\n    coef = 1.0 / (np.sqrt(2.0 * np.pi) * std)\n    return coef * np.exp(-0.5 * ((x - mean) / std) ** 2)\n\ndef prior_mixture_pdf(v, w1, v1, s1, w2, v2, s2):\n    \"\"\"Mixture of two Gaussian components in velocity.\"\"\"\n    return w1 * gaussian_pdf(v, v1, s1) + w2 * gaussian_pdf(v, v2, s2)\n\ndef likelihood_t_given_v(t_obs, L, v, sigma_t):\n    \"\"\"Gaussian likelihood in travel time given velocity via t = L / v.\"\"\"\n    mean_t = L / v\n    return gaussian_pdf(t_obs, mean_t, sigma_t)\n\ndef tempered_mass_fraction_lower_mode(L, t_obs, sigma_t, w1, v1, s1, w2, v2, s2, betas, vmin=1000.0, vmax=5000.0, dv=1.0):\n    \"\"\"\n    Compute f1(beta): fraction of tempered posterior mass in lower-velocity region v = (v1+v2)/2.\n    \"\"\"\n    # Velocity grid\n    v_grid = np.arange(vmin, vmax + dv, dv)\n    # Prior on grid\n    p_prior = prior_mixture_pdf(v_grid, w1, v1, s1, w2, v2, s2)\n    # Likelihood on grid\n    p_like = likelihood_t_given_v(t_obs, L, v_grid, sigma_t)\n    # Midpoint between modes\n    v_mid = 0.5 * (v1 + v2)\n    # Indicator for lower-mode region\n    lower_mask = v_grid = v_mid\n\n    fractions = []\n    for beta in betas:\n        # Tempered unnormalized posterior\n        u_beta = p_prior * (p_like ** beta)\n        # Integrals via Riemann sum (uniform grid)\n        Z_beta = np.sum(u_beta) * dv\n        M_beta_low = np.sum(u_beta[lower_mask]) * dv\n        f1 = M_beta_low / Z_beta if Z_beta > 0.0 else 0.0\n        fractions.append(f1)\n    return fractions\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (L, t_obs, sigma_t, w1, v1, s1, w2, v2, s2)\n    test_cases = [\n        # 1. Ambiguous case\n        (1000.0, 0.33, 0.08, 0.5, 2000.0, 200.0, 0.5, 3500.0, 300.0),\n        # 2. Strongly informative case favoring higher velocity\n        (1000.0, 0.286, 0.01, 0.5, 2000.0, 200.0, 0.5, 3500.0, 300.0),\n        # 3. Diffuse case\n        (1000.0, 0.40, 0.20, 0.5, 2000.0, 200.0, 0.5, 3500.0, 300.0),\n        # 4. Skewed prior case\n        (1000.0, 0.32, 0.03, 0.7, 2200.0, 150.0, 0.3, 3800.0, 400.0),\n    ]\n\n    betas = [1.0, 0.5, 0.1]\n\n    results = []\n    for case in test_cases:\n        L, t_obs, sigma_t, w1, v1, s1, w2, v2, s2 = case\n        fractions = tempered_mass_fraction_lower_mode(\n            L=L,\n            t_obs=t_obs,\n            sigma_t=sigma_t,\n            w1=w1,\n            v1=v1,\n            s1=s1,\n            w2=w2,\n            v2=v2,\n            s2=s2,\n            betas=betas,\n            vmin=1000.0,\n            vmax=5000.0,\n            dv=1.0\n        )\n        results.extend([f\"{val:.6f}\" for val in fractions])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Advanced samplers like Hamiltonian Monte Carlo (HMC) rely on accurate gradients to explore the posterior distribution. In practice, the gradients computed by adjoint solvers may contain numerical errors, raising questions about the sampler's reliability. This practice delves into the practical robustness of HMC by asking you to inject controlled errors into the gradient calculation within a setting where the true posterior is known. By analyzing the results, you will gain critical insight into how numerical errors can degrade sampling performance and learn to diagnose potential failures of the inference process. ",
            "id": "3577488",
            "problem": "Consider a Bayesian inversion problem in computational geophysics for an unknown model vector $m \\in \\mathbb{R}^2$, observed through a linear forward operator. Let the observed data vector be $d \\in \\mathbb{R}^2$, the forward operator be the matrix $G \\in \\mathbb{R}^{2 \\times 2}$, the data-noise covariance be the symmetric positive-definite matrix $C_n \\in \\mathbb{R}^{2 \\times 2}$, and the Gaussian prior on $m$ have mean $m_0 \\in \\mathbb{R}^2$ and covariance $C_m \\in \\mathbb{R}^{2 \\times 2}$. The posterior density $p(m \\mid d)$ is proportional to $\\exp(-\\Phi(m))$, where the potential function is defined by the sum of the data misfit and the prior penalty, both in the quadratic form arising from Gaussian assumptions. Denote by Hamiltonian Monte Carlo (HMC) the Markov chain Monte Carlo method that proposes new states by numerically integrating Hamiltonian dynamics involving the gradient $\\nabla \\Phi(m)$, coupled with a Metropolis acceptance rule using the exact potential $\\Phi(m)$.\n\nIn realistic computational geophysics workflows, the gradient $\\nabla \\Phi(m)$ is often computed via an adjoint solver. To assess the role of adjoint solver error on posterior accuracy and Markov chain stationarity, introduce a controlled bias $\\eta \\in \\mathbb{R}^2$ into the gradient used by the HMC integrator, so that the integrator uses the biased gradient $\\tilde{\\nabla}\\Phi(m) = \\nabla\\Phi(m) + \\eta$. Additionally, consider a stochastic bias model where each evaluation of $\\tilde{\\nabla}\\Phi(m)$ includes an additive Gaussian perturbation $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$, independent across gradient calls, so that $\\tilde{\\nabla}\\Phi(m) = \\nabla\\Phi(m) + \\xi$. In all cases, the Metropolis acceptance step must be performed using the exact potential $\\Phi(m)$, not the biased gradient.\n\nStarting only from the fundamental definitions stated above, derive the posterior density for the linear-Gaussian case and the corresponding potential $\\Phi(m)$ and its exact gradient $\\nabla \\Phi(m)$. Using these, implement HMC with standard leapfrog integration, where the leapfrog updates use $\\tilde{\\nabla}\\Phi(m)$ as specified for each test case, while the Metropolis acceptance probability uses the exact Hamiltonian defined by $\\Phi(m)$ and the kinetic energy under a unit mass matrix. Quantify the effects of the gradient bias on:\n- the stationarity of the chain, through a distributional stability diagnostic comparing the first and second halves of the post-burn-in samples using a univariate two-sample distributional distance for each coordinate of $m$ and reporting the maximum across coordinates, and\n- the accuracy of the sampled posterior $p(m \\mid d)$, by comparing the empirical mean and covariance from the HMC samples against the analytically derived posterior mean and covariance.\n\nFor numerical experimentation, use the following fixed problem setup:\n- $G = \\begin{bmatrix} 1.0  0.5 \\\\ 0.3  1.2 \\end{bmatrix}$,\n- $d = \\begin{bmatrix} 1.2 \\\\ -0.7 \\end{bmatrix}$,\n- $C_n = \\mathrm{diag}(0.1^2, 0.2^2)$,\n- $m_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $C_m = \\mathrm{diag}(0.5^2, 0.5^2)$.\n\nUse HMC with unit mass matrix, leapfrog step size $\\epsilon = 0.2$, number of leapfrog steps $L = 20$, total number of samples $N = 2000$, and burn-in of $N_{\\mathrm{burn}} = 500$. Initialize the chain at the prior mean $m_0$. Use a fixed random seed for reproducibility.\n\nDesign a test suite comprising five cases that explore deterministic and stochastic gradient biases:\n1. Unbiased integrator: $\\eta = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\sigma = 0$.\n2. Deterministic small bias: $\\eta = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}$, $\\sigma = 0$.\n3. Deterministic larger bias: $\\eta = \\begin{bmatrix} 0.2 \\\\ -0.2 \\end{bmatrix}$, $\\sigma = 0$.\n4. Stochastic small noise: $\\eta = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\sigma = 0.05$.\n5. Stochastic larger noise: $\\eta = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\sigma = 0.2$.\n\nFor each test case, compute:\n- the acceptance rate as a float in $[0,1]$,\n- the Euclidean norm of the error in the empirical mean of $m$ relative to the exact posterior mean,\n- the Frobenius norm of the error in the empirical covariance of $m$ relative to the exact posterior covariance,\n- the maximum two-sample Kolmogorov–Smirnov distance across the two coordinates between the first and second halves of the post-burn-in samples,\n- a boolean stationarity flag indicating whether the maximum Kolmogorov–Smirnov distance is below the threshold $0.08$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order described above. For example, the output format must be exactly like:\n[[acc1,mean_err1,cov_err1,ks1,flag1],[acc2,mean_err2,cov_err2,ks2,flag2],...]\nNo physical units or angle units are involved in this problem, and all numerical answers must be expressed as floats or booleans as specified.",
            "solution": "We begin from the fundamental Bayesian inversion setting for linear-Gaussian models. The forward operator maps the model vector $m \\in \\mathbb{R}^2$ to predicted data $Gm \\in \\mathbb{R}^2$. The observed data $d \\in \\mathbb{R}^2$ are modeled as $d = Gm + \\varepsilon$, where the data noise $\\varepsilon$ is zero-mean Gaussian with covariance $C_n$, that is, $\\varepsilon \\sim \\mathcal{N}(0, C_n)$. The prior on $m$ is Gaussian with mean $m_0$ and covariance $C_m$, that is, $m \\sim \\mathcal{N}(m_0, C_m)$.\n\nBy Bayes' rule, the posterior density is proportional to likelihood times prior, $p(m \\mid d) \\propto p(d \\mid m)\\, p(m)$. Under the Gaussian assumptions, the likelihood is $p(d \\mid m) \\propto \\exp\\left(-\\tfrac{1}{2} (d - Gm)^\\top C_n^{-1} (d - Gm)\\right)$ and the prior is $p(m) \\propto \\exp\\left(-\\tfrac{1}{2} (m - m_0)^\\top C_m^{-1} (m - m_0)\\right)$. Therefore, the posterior density is\n$$\np(m \\mid d) \\propto \\exp\\left(-\\Phi(m)\\right),\n$$\nwhere the potential (negative log posterior up to an additive constant) is\n$$\n\\Phi(m) = \\tfrac{1}{2} (d - Gm)^\\top C_n^{-1} (d - Gm) + \\tfrac{1}{2} (m - m_0)^\\top C_m^{-1} (m - m_0).\n$$\nThe gradient of the potential is obtained by differentiating the quadratic forms with respect to $m$. Using the identity $\\nabla_m \\tfrac{1}{2} (Ax - b)^\\top W (Ax - b) = A^\\top W (Ax - b)$ for appropriate matrices $A$ and $W$, we have\n$$\n\\nabla \\Phi(m) = -G^\\top C_n^{-1} (d - Gm) + C_m^{-1} (m - m_0).\n$$\nCollecting linear terms, this can be expressed as\n$$\n\\nabla \\Phi(m) = \\left(G^\\top C_n^{-1} G + C_m^{-1}\\right)m - \\left(G^\\top C_n^{-1} d + C_m^{-1} m_0\\right).\n$$\nSince both the likelihood and prior are Gaussian and the forward operator is linear, the posterior is also Gaussian, $p(m \\mid d) = \\mathcal{N}(\\mu_{\\text{post}}, C_{\\text{post}})$, where the posterior covariance and mean follow from completing the square in the exponent:\n$$\nC_{\\text{post}} = \\left(G^\\top C_n^{-1} G + C_m^{-1}\\right)^{-1},\n\\quad\n\\mu_{\\text{post}} = C_{\\text{post}} \\left(G^\\top C_n^{-1} d + C_m^{-1} m_0\\right).\n$$\n\nWe now outline Hamiltonian Monte Carlo (HMC) with leapfrog integration. Introduce an auxiliary momentum $p \\in \\mathbb{R}^2$ with density $\\mathcal{N}(0, M)$ for a mass matrix $M$. We take $M = I$ (the identity). The Hamiltonian is\n$$\nH(m, p) = \\Phi(m) + \\tfrac{1}{2} p^\\top M^{-1} p = \\Phi(m) + \\tfrac{1}{2} p^\\top p.\n$$\nThe exact continuous-time dynamics satisfy $\\dot{m} = \\partial H / \\partial p = p$, and $\\dot{p} = - \\partial H / \\partial m = - \\nabla \\Phi(m)$. HMC simulates these dynamics approximately via the leapfrog integrator using a time step $\\epsilon$ and $L$ steps, generating a proposal $(m^\\ast, p^\\ast)$ from an initial $(m, p)$, and then accepts or rejects this proposal with probability\n$$\n\\alpha = \\min\\left\\{1, \\exp\\left( -H(m^\\ast, p^\\ast) + H(m, p) \\right)\\right\\}.\n$$\nThis acceptance step restores detailed balance with respect to the target density when the integrator is volume-preserving and reversible.\n\nIn our investigation, the leapfrog updates do not use the exact gradient $\\nabla \\Phi(m)$, but instead a biased gradient $\\tilde{\\nabla}\\Phi(m) = \\nabla \\Phi(m) + \\eta$ for a deterministic bias vector $\\eta$, or a stochastic-biased gradient $\\tilde{\\nabla}\\Phi(m) = \\nabla \\Phi(m) + \\xi$, where $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$ is an independent Gaussian noise added at each gradient evaluation. The acceptance probability uses the exact potential $\\Phi(m)$.\n\nThe leapfrog scheme with biased gradient proceeds as follows. Given current $m$ and a freshly sampled $p \\sim \\mathcal{N}(0, I)$, perform:\n1. $p \\leftarrow p - \\tfrac{\\epsilon}{2} \\tilde{\\nabla}\\Phi(m)$,\n2. For $l = 1, \\ldots, L$:\n   - $m \\leftarrow m + \\epsilon p$,\n   - If $l  L$: $p \\leftarrow p - \\epsilon \\tilde{\\nabla}\\Phi(m)$,\n   - Else: $p \\leftarrow p - \\tfrac{\\epsilon}{2} \\tilde{\\nabla}\\Phi(m)$.\n3. Negate $p$ to enforce symmetry: $p \\leftarrow -p$.\nCompute the acceptance probability using $H(m, p)$ and $H(m^\\ast, p^\\ast)$ with the exact $\\Phi(m)$.\n\nFor the fixed problem setup with specified $G$, $d$, $C_n$, $m_0$, and $C_m$, we compute $\\mu_{\\text{post}}$ and $C_{\\text{post}}$ by the formulas above. We then run HMC for $N = 2000$ iterations with burn-in $N_{\\mathrm{burn}} = 500$, step size $\\epsilon = 0.2$, and number of leapfrog steps $L = 20$, initializing at $m_0$. We analyze five test cases:\n1. Unbiased integrator: $\\eta = 0$, $\\sigma = 0$.\n2. Deterministic small bias: $\\eta = [0.05, -0.05]^\\top$, $\\sigma = 0$.\n3. Deterministic larger bias: $\\eta = [0.2, -0.2]^\\top$, $\\sigma = 0$.\n4. Stochastic small noise: $\\eta = 0$, $\\sigma = 0.05$.\n5. Stochastic larger noise: $\\eta = 0$, $\\sigma = 0.2$.\n\nFor each case, we collect the following diagnostics:\n- Acceptance rate: the fraction of proposals accepted.\n- Posterior mean error: $\\|\\bar{m} - \\mu_{\\text{post}}\\|_2$, where $\\bar{m}$ is the empirical mean of the post-burn-in samples.\n- Posterior covariance error: $\\| \\widehat{C} - C_{\\text{post}} \\|_F$, where $\\widehat{C}$ is the empirical covariance of the post-burn-in samples and $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n- Stationarity diagnostic: we split post-burn-in samples into two halves, compute for each coordinate of $m$ the two-sample Kolmogorov–Smirnov statistic between the halves, and report the maximum across coordinates. We also provide a boolean stationarity flag indicating whether this maximum is below the threshold $0.08$.\n\nThe rationale for these diagnostics is as follows. If the integrator uses the exact gradient and the acceptance step is applied, HMC should sample the exact posterior distribution efficiently; in our quadratic case, the leapfrog is highly accurate and acceptance rates are expected to be near unity, with small mean and covariance errors, and stationary behavior across halves. When a deterministic bias $\\eta$ is injected into the gradient used by the integrator, the proposal dynamics deviate from the true Hamiltonian flow. Although the leapfrog remains volume-preserving and reversible under a fixed deterministic bias (hence stationarity can often be restored by the Metropolis step), the induced energy errors can reduce acceptance rates and impair sampling efficiency, possibly inflating the bias in empirical moments for finite chains. Under stochastic gradient noise, the integrator evaluations vary randomly across steps, violating strict reversibility. The Metropolis correction still applies but can face significant energy errors, reducing acceptance rates and potentially causing distributional differences between halves, which we quantify via the Kolmogorov–Smirnov statistics.\n\nThe required program implements the derivation of $\\Phi(m)$ and $\\nabla \\Phi(m)$, calculates the exact posterior parameters $(\\mu_{\\text{post}}, C_{\\text{post}})$, runs HMC with biased gradients as specified, computes the diagnostics for each test case, and outputs all results in the prescribed single-line format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ks_2samp\n\ndef posterior_params(G, Cn, m0, Cm, d):\n    Cn_inv = np.linalg.inv(Cn)\n    Cm_inv = np.linalg.inv(Cm)\n    A = G.T @ Cn_inv @ G + Cm_inv\n    b = G.T @ Cn_inv @ d + Cm_inv @ m0\n    C_post = np.linalg.inv(A)\n    mu_post = C_post @ b\n    return mu_post, C_post, A, b, Cn_inv, Cm_inv\n\ndef phi(m, G, Cn_inv, m0, Cm_inv, d):\n    r = d - G @ m\n    term1 = 0.5 * (r.T @ Cn_inv @ r)\n    dm = m - m0\n    term2 = 0.5 * (dm.T @ Cm_inv @ dm)\n    return float(term1 + term2)\n\ndef grad_phi(m, A, b):\n    # Exact gradient of Phi(m) = 0.5*(d - Gm)^T Cn^{-1} (d - Gm) + 0.5*(m - m0)^T Cm^{-1} (m - m0)\n    # grad = A m - b\n    return A @ m - b\n\ndef leapfrog_with_bias(m, p, epsilon, L, grad_fun, bias_type, eta, sigma, rng):\n    # Perform L leapfrog steps using biased gradients; return proposed (m_prop, p_prop)\n    # bias_type: \"deterministic\" or \"stochastic\" or \"none\"\n    # eta: deterministic bias vector\n    # sigma: std dev for stochastic bias\n    def biased_grad(x):\n        g = grad_fun(x)\n        if bias_type == \"deterministic\":\n            return g + eta\n        elif bias_type == \"stochastic\":\n            noise = rng.normal(loc=0.0, scale=sigma, size=g.shape)\n            return g + noise\n        else:\n            return g\n\n    p = p - 0.5 * epsilon * biased_grad(m)\n    for l in range(1, L + 1):\n        m = m + epsilon * p\n        if l  L:\n            p = p - epsilon * biased_grad(m)\n        else:\n            p = p - 0.5 * epsilon * biased_grad(m)\n    p = -p  # Momentum flip for reversibility\n    return m, p\n\ndef hmc_run(G, Cn, m0, Cm, d, epsilon, L, N, burn_in, bias_type, eta, sigma, seed=12345):\n    rng = np.random.default_rng(seed)\n    mu_post, C_post, A, b, Cn_inv, Cm_inv = posterior_params(G, Cn, m0, Cm, d)\n    def grad_fun(x):\n        return grad_phi(x, A, b)\n    def phi_fun(x):\n        return phi(x, G, Cn_inv, m0, Cm_inv, d)\n\n    dim = G.shape[1]\n    m_current = m0.copy()\n    samples = []\n    accepted = 0\n\n    for i in range(N):\n        p_current = rng.normal(loc=0.0, scale=1.0, size=dim)\n        current_H = phi_fun(m_current) + 0.5 * np.dot(p_current, p_current)\n\n        m_prop, p_prop = leapfrog_with_bias(\n            m_current.copy(),\n            p_current.copy(),\n            epsilon,\n            L,\n            grad_fun,\n            bias_type,\n            eta,\n            sigma,\n            rng\n        )\n\n        proposed_H = phi_fun(m_prop) + 0.5 * np.dot(p_prop, p_prop)\n        # Metropolis acceptance with exact Hamiltonian\n        accept_prob = np.exp(min(0.0, current_H - proposed_H))\n        if rng.uniform()  accept_prob:\n            m_current = m_prop\n            accepted += 1\n\n        samples.append(m_current.copy())\n\n    samples = np.array(samples)\n    post_samples = samples[burn_in:]\n\n    # Diagnostics\n    acc_rate = accepted / float(N)\n\n    emp_mean = np.mean(post_samples, axis=0)\n    mean_err_norm = float(np.linalg.norm(emp_mean - mu_post))\n\n    # Empirical covariance\n    centered = post_samples - emp_mean\n    emp_cov = (centered.T @ centered) / (post_samples.shape[0] - 1)\n    cov_err_fro = float(np.linalg.norm(emp_cov - C_post, ord='fro'))\n\n    # Stationarity diagnostic: KS between first and second halves (per coordinate), report max\n    half = post_samples.shape[0] // 2\n    first_half = post_samples[:half]\n    second_half = post_samples[half:]\n    ks_stats = []\n    for k in range(first_half.shape[1]):\n        stat, _ = ks_2samp(first_half[:, k], second_half[:, k], alternative='two-sided', mode='auto')\n        ks_stats.append(stat)\n    ks_max = float(np.max(ks_stats))\n    stationary_flag = (ks_max  0.08)\n\n    return [acc_rate, mean_err_norm, cov_err_fro, ks_max, stationary_flag]\n\ndef solve():\n    # Fixed problem setup\n    G = np.array([[1.0, 0.5],\n                  [0.3, 1.2]])\n    d = np.array([1.2, -0.7])\n    Cn = np.diag([0.1**2, 0.2**2])\n    m0 = np.array([0.0, 0.0])\n    Cm = np.diag([0.5**2, 0.5**2])\n\n    # HMC parameters\n    epsilon = 0.2\n    L = 20\n    N = 2000\n    burn_in = 500\n\n    # Test suite\n    test_cases = [\n        # bias_type, eta, sigma\n        (\"none\", np.array([0.0, 0.0]), 0.0),\n        (\"deterministic\", np.array([0.05, -0.05]), 0.0),\n        (\"deterministic\", np.array([0.2, -0.2]), 0.0),\n        (\"stochastic\", np.array([0.0, 0.0]), 0.05),\n        (\"stochastic\", np.array([0.0, 0.0]), 0.2),\n    ]\n\n    results = []\n    # Use distinct seeds for each case to avoid identical stochastic sequences where needed\n    base_seed = 20231010\n    for idx, (bias_type, eta, sigma) in enumerate(test_cases):\n        res = hmc_run(\n            G, Cn, m0, Cm, d,\n            epsilon, L, N, burn_in,\n            bias_type, eta, sigma,\n            seed=base_seed + idx\n        )\n        results.append(res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}