{
    "hands_on_practices": [
        {
            "introduction": "主成分分析（PCA）是探索和降低地球物理数据集维度的基石。然而，一个核心的实践挑战在于如何区分承载重要信号的主成分与仅代表噪声的主成分。本练习将引导您超越“肘部法则”等启发式方法，通过推导和应用“断棍模型”，为您提供一种基于统计原理的准则来确定数据的内在维度。",
            "id": "3615523",
            "problem": "一次海洋可控源电磁勘探生成了一个数据矩阵，其列是沿着一条大陆架剖面计算的八个频率归一化的属性通道。设均值中心化的数据矩阵为 $X \\in \\mathbb{R}^{n \\times r}$，其中有 $r=8$ 个通道和 $n \\gg r$ 个观测值。对 $X$ 应用主成分分析 (PCA)，以识别嵌入在电磁响应中的低维海洋学结构。\n\n从样本协方差矩阵的定义 $C = \\frac{1}{n-1} X^{\\top} X$ 和奇异值分解 (SVD) $X = U \\Sigma V^{\\top}$（其中奇异值为 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} \\geq 0$）出发，推导前 $k$ 个主成分的解释方差比关于 $\\{\\sigma_{i}\\}_{i=1}^{r}$ 的表达式。\n\n接下来，假设属性通道的潜在零结构是它们是可交换的且没有优先方向，这可以通过一个断棍过程来建模：一根单位长度的棍子在 $r-1$ 个独立的 Uniform$(0,1)$ 位置上被折断，产生 $r$ 个片段，其长度按降序排列。在此零假设下，设计一个使用预期的有序片段长度来选择 $k$ 个数据驱动主成分的准则。你的准则必须根据观测到的各分量方差比例和断棍模型下预期的有序片段长度进行明确陈述；并从 PCA 的第一性原理和断棍零假设的定义出发，论证其构建的合理性。\n\n最后，将你的准则应用于从 $X$ 获得的以下奇异值（单位任意）：\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$。\n将选定的 $k$ 以整数形式报告。如果需要任何中间数值，请保留足够的精度以确保选择是明确的。将你最终选定的 $k$ 表示为一个不带单位的整数。",
            "solution": "该问题被评估为有效，因为它具有科学依据、提法明确且客观。它包括一个标准的理论推导，一个基于已知零模型的统计准则的构建，以及将其应用于给定数据集。所有必要信息均已提供。\n\n按照要求，该问题分三部分解决：推导解释方差比，设计断棍选择准则，并将其应用于所提供的数据。\n\n首先，我们推导前 $k$ 个主成分的解释方差比的表达式。\n样本协方差矩阵定义为 $C = \\frac{1}{n-1} X^{\\top} X$，其中 $X \\in \\mathbb{R}^{n \\times r}$ 是均值中心化的数据矩阵，有 $n$ 个观测值和 $r$ 个通道。主成分分析 (PCA) 旨在寻找该协方差矩阵的特征向量和特征值。设 $X$ 的奇异值分解 (SVD) 为 $X = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times r}$ 的列是标准正交的 ($U^{\\top}U=I_r$)，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是由奇异值 $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq 0$ 构成的对角矩阵，而 $V \\in \\mathbb{R}^{r \\times r}$ 是一个正交矩阵 ($V^{\\top}V = VV^{\\top} = I_r$)。\n\n我们可以将 $X$ 的 SVD 代入 $C$ 的表达式中：\n$$ C = \\frac{1}{n-1} (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) $$\n使用性质 $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$，我们得到：\n$$ C = \\frac{1}{n-1} (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) $$\n由于 $\\Sigma$ 是一个对角矩阵，因此 $\\Sigma^{\\top} = \\Sigma$。$U$ 的列是标准正交的，所以 $U^{\\top}U = I_r$。\n$$ C = \\frac{1}{n-1} V \\Sigma (U^{\\top}U) \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma I_r \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma^2 V^{\\top} $$\n这个方程是 $C$ 的特征分解。$V$ 的列是 $C$ 的特征向量，即主成分方向。$C$ 的特征值，记作 $\\lambda_i$，是对角矩阵 $\\frac{1}{n-1}\\Sigma^2$ 的对角线元素。因此，对应于第 $i$ 个主成分的特征值为：\n$$ \\lambda_i = \\frac{\\sigma_i^2}{n-1} $$\n数据中的总方差是协方差矩阵的迹，即 $\\text{Tr}(C)$。\n$$ \\text{Tr}(C) = \\text{Tr}\\left(\\frac{1}{n-1} V \\Sigma^2 V^{\\top}\\right) $$\n使用迹的循环性质 $\\text{Tr}(ABC) = \\text{Tr}(CAB)$：\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 V^{\\top}V) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 I_r) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2) $$\n对角矩阵的迹是其对角线元素之和，所以：\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2 $$\n第 $i$ 个主成分解释的方差比例是其特征值 $\\lambda_i$ 与总方差 $\\text{Tr}(C)$ 的比率：\n$$ p_i = \\frac{\\lambda_i}{\\text{Tr}(C)} = \\frac{\\frac{\\sigma_i^2}{n-1}}{\\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2} = \\frac{\\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\n前 $k$ 个主成分的解释方差比是各独立比例之和：\n$$ R(k) = \\sum_{i=1}^{k} p_i = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\n\n第二，我们基于断棍模型设计一个选择主成分数量 $k$ 的准则。\n零假设 ($H_0$) 是数据中没有主导结构，这意味着总方差在 $r$ 个分量中是随机分配的。断棍模型为这一零假设提供了一种形式化的表述。在该模型中，一根单位长度的棍子在从 Uniform$(0,1)$ 分布中抽取的 $r-1$ 个独立的随机位置上被折断，产生 $r$ 个片段。当这些片段的长度按降序排列时，它们代表了在 $H_0$ 下有序主成分所解释的预期方差比例。\n\n设 $b_i$ 是将一根棍子折成 $r$ 段后第 $i$ 长片段的期望长度。$b_i$ 的解析表达式为：\n$$ b_i = \\frac{1}{r} \\sum_{j=i}^{r} \\frac{1}{j} $$\n这些 $b_i$ 值可作为没有优先结构的数据集的基准比例。选择准则是通过将每个分量的观测方差比例 $p_i$ 与断棍模型得出的期望比例 $b_i$ 进行比较来构建的。如果一个分量解释的方差比例大于随机情况下预期的值，则该分量被认为捕获了一个显著的、非随机的结构。\n\n该准则如下：只要主成分的观测方差比例超过断棍零模型预测的比例，就保留该主成分。选择主成分数量 $k$ 为满足对于所有分量 $j=1, \\dots, k$ 条件 $p_j > b_j$ 均成立的最大整数。鉴于 $p_j$ 和 $b_j$ 都是单调递减序列，这等价于找到第一个满足 $p_i \\le b_i$ 的分量 $i$，然后选择 $k=i-1$。\n\n第三，我们将此准则应用于给定的 $r=8$ 个通道的奇异值：\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$。\n\n第1步：计算观测到的方差比例 $p_i$。\n首先，我们计算奇异值的平方 $\\sigma_i^2$：\n$\\sigma_1^2 = 12.3^2 = 151.29$\n$\\sigma_2^2 = 7.8^2 = 60.84$\n$\\sigma_3^2 = 5.1^2 = 26.01$\n$\\sigma_4^2 = 3.3^2 = 10.89$\n$\\sigma_5^2 = 2.6^2 = 6.76$\n$\\sigma_6^2 = 2.2^2 = 4.84$\n$\\sigma_7^2 = 2.1^2 = 4.41$\n$\\sigma_8^2 = 2.0^2 = 4.00$\n\n总平方和为 $\\sum_{j=1}^{8} \\sigma_j^2 = 151.29 + 60.84 + 26.01 + 10.89 + 6.76 + 4.84 + 4.41 + 4.00 = 269.04$。\n观测到的比例 $p_i = \\sigma_i^2 / 269.04$ 为：\n$p_1 \\approx 0.5623$\n$p_2 \\approx 0.2261$\n$p_3 \\approx 0.0967$\n$p_4 \\approx 0.0405$\n$p_5 \\approx 0.0251$\n$p_6 \\approx 0.0180$\n$p_7 \\approx 0.0164$\n$p_8 \\approx 0.0149$\n\n第2步：计算 $r=8$ 时预期的断棍比例 $b_i$。\n使用公式 $b_i = \\frac{1}{8} \\sum_{j=i}^{8} \\frac{1}{j}$：\n$b_1 = \\frac{1}{8} (\\frac{1}{1} + \\frac{1}{2} + \\dots + \\frac{1}{8}) = \\frac{1}{8}(\\frac{761}{280}) \\approx 0.3397$\n$b_2 = \\frac{1}{8} (\\frac{1}{2} + \\dots + \\frac{1}{8}) \\approx 0.2147$\n$b_3 = \\frac{1}{8} (\\frac{1}{3} + \\dots + \\frac{1}{8}) \\approx 0.1522$\n$b_4 = \\frac{1}{8} (\\frac{1}{4} + \\dots + \\frac{1}{8}) \\approx 0.1106$\n$b_5 = \\frac{1}{8} (\\frac{1}{5} + \\dots + \\frac{1}{8}) \\approx 0.0793$\n$b_6 = \\frac{1}{8} (\\frac{1}{6} + \\frac{1}{7} + \\frac{1}{8}) \\approx 0.0543$\n$b_7 = \\frac{1}{8} (\\frac{1}{7} + \\frac{1}{8}) \\approx 0.0335$\n$b_8 = \\frac{1}{8} (\\frac{1}{8}) \\approx 0.0156$\n\n第3步：比较 $p_i$ 和 $b_i$ 以确定 $k$。\n我们逐个分量比较这些值：\n- 对于 $i=1$：$p_1 \\approx 0.5623 > b_1 \\approx 0.3397$。条件成立。\n- 对于 $i=2$：$p_2 \\approx 0.2261 > b_2 \\approx 0.2147$。条件成立。\n- 对于 $i=3$：$p_3 \\approx 0.0967  b_3 \\approx 0.1522$。条件不成立。\n\n由于条件 $p_i > b_i$ 在 $i=3$ 时首次不成立，我们只保留条件成立的分量。该条件对 $j=1$ 和 $j=2$ 成立，但对 $j=3$ 不成立。因此，使得对所有 $j=1, \\dots, k$ 都有 $p_j > b_j$ 成立的最大 $k$ 值为 $2$。\n\n选定的主成分数量为 $k=2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "奇异值分解（SVD）不仅是数据探索的工具，也是解决地球物理反演问题的关键。由于地球物理反演问题通常是病态的，噪声会在求解过程中被过度放大。本练习将向您展示如何利用截断奇异值分解（TSVD）作为一种正则化方法，通过精确推导偏差-方差权衡关系，来确定最佳的截断阈值以最小化模型重构误差。",
            "id": "3615522",
            "problem": "考虑一个线性化的地球物理反演问题，其正演算子为 $G \\in \\mathbb{R}^{m \\times n}$，数据模型为 $d = G m_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon$ 是均值为零、协方差为 $\\sigma^{2} I_{m}$ 的噪声，$I_{m}$ 是 $m \\times m$ 的单位矩阵。设 $G$ 的奇异值分解（SVD）为 $G = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ 具有奇异值 $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$。在许多离散化的地球物理积分算子中，奇异值可以很好地用幂律来近似。假设离散化后 $n = 60$，且奇异值遵循\n$$\n\\sigma_{i} = 2.5 \\, i^{-1.3}, \\quad i = 1, 2, \\dots, 60.\n$$\n截断指数为 $k$ 的截断奇异值分解估计量定义为\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} \\, v_{i},\n$$\n其中 $u_{i}$ 和 $v_{i}$ 分别是左、右奇异向量。为了分析偏差-方差权衡，将 $m_{\\text{true}}$ 在右奇异向量基中展开为 $m_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}$。假设一个阶数为 $p = 1$ 的平滑先验，使得系数的先验二阶矩满足\n$$\n\\mathbb{E}[c_{i}^{2}] = C^{2} \\, i^{-2p} = C^{2} \\, i^{-2}, \\quad \\text{其中 } C = 0.8.\n$$\n设噪声水平为 $\\sigma = 2.0 \\times 10^{-2}$。从上述定义以及 $U$ 和 $V$ 的正交性出发，推导期望均方重构误差 $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$ 作为 $k$ 的函数，将其在奇异向量基中分解为方差项和偏差项，并确定最优截断指数 $k^{\\star}$ 的选择准则，该准则能在期望上最小化此误差。最后，根据给定参数计算最优截断指数 $k^{\\star}$ 的数值。将最终答案表示为单个整数 $k^{\\star}$。",
            "solution": "该问题陈述是地球物理学中一个正则化线性反演问题的标准表述。它具有科学依据、是适定的、客观的，并且包含了获得唯一解所需的所有信息。因此，该问题被认为是有效的。我们开始推导。\n\n目标是找到最优截断指数 $k^{\\star}$，以最小化期望均方重构误差 $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$。\n\n首先，我们将重构误差向量 $\\hat{m}_{k} - m_{\\text{true}}$ 在右奇异向量基 $\\{v_i\\}_{i=1}^n$ 中表示。\n真实模型由其在该基中的展开式给出：\n$$\nm_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}\n$$\n截断奇异值分解（TSVD）估计量定义为：\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} v_{i}\n$$\n我们将数据模型 $d = G m_{\\text{true}} + \\varepsilon$ 代入 $\\hat{m}_{k}$ 的表达式中。$u_{i}^{\\top} d$ 项变为：\n$$\nu_{i}^{\\top} d = u_{i}^{\\top} (G m_{\\text{true}} + \\varepsilon) = u_{i}^{\\top} G m_{\\text{true}} + u_{i}^{\\top}\\varepsilon\n$$\n使用 $G$ 的 SVD $G = U \\Sigma V^{\\top} = \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top}$，我们可以计算第一项：\n$$\nu_{i}^{\\top} G m_{\\text{true}} = u_{i}^{\\top} \\left( \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top} \\right) \\left( \\sum_{l=1}^{n} c_{l} v_{l} \\right)\n$$\n由于奇异向量的正交性（$u_i^{\\top}u_j = \\delta_{ij}$ 和 $v_j^{\\top}v_l = \\delta_{jl}$），上式可简化为：\n$$\nu_{i}^{\\top} G m_{\\text{true}} = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} (u_{i}^{\\top} u_{j}) (v_{j}^{\\top} v_{l}) = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} \\delta_{ij} \\delta_{jl} = \\sigma_{i} c_{i}\n$$\n因此，$u_{i}^{\\top} d = \\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon$。将此结果代回估计量 $\\hat{m}_{k}$：\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{\\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i}\n$$\n现在，我们可以写出误差向量：\n$$\n\\hat{m}_{k} - m_{\\text{true}} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i} - \\sum_{i=1}^{n} c_{i} v_{i} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} - \\sum_{i=k+1}^{n} c_{i} v_{i}\n$$\n误差向量由两个正交分量组成：一个位于由 $\\{v_1, \\dots, v_k\\}$ 张成的子空间中，另一个位于由 $\\{v_{k+1}, \\dots, v_n\\}$ 张成的子空间中。第一个分量是由于噪声传播，第二个分量是由于解展开式的截断。\n\n接下来，我们计算误差向量的 $L_2$ 范数的平方。由于这两个和的正交性（函数空间中的勾股定理）：\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\left\\| \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} \\right\\|_{2}^{2} + \\left\\| \\sum_{i=k+1}^{n} c_{i} v_{i} \\right\\|_{2}^{2}\n$$\n因为 $\\{v_i\\}$ 是一个标准正交基，范数简化为系数的平方和：\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\sum_{i=1}^{k} \\left( \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right)^{2} + \\sum_{i=k+1}^{n} c_{i}^{2}\n$$\n我们现在对噪声分布 $\\varepsilon$ 和真实模型系数 $c_i$ 的先验分布求期望。总期望均方误差 $E(k)$ 为：\n$$\nE(k) = \\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{k} \\frac{(u_{i}^{\\top}\\varepsilon)^{2}}{\\sigma_{i}^{2}}\\right] + \\mathbb{E}\\left[\\sum_{i=k+1}^{n} c_{i}^{2}\\right]\n$$\n期望算子是线性的，所以我们可以将其移入求和号内。假设项 $c_i$ 和 $\\varepsilon$ 是独立的。\n对于第一项（方差），我们需要 $\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]$。由于 $\\varepsilon$ 的均值为零，$\\mathbb{E}[u_{i}^{\\top}\\varepsilon] = u_i^{\\top}\\mathbb{E}[\\varepsilon] = 0$。该表达式是随机变量 $u_{i}^{\\top}\\varepsilon$ 的方差。\n$$\n\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}] = \\text{Var}(u_{i}^{\\top}\\varepsilon) = u_{i}^{\\top} \\text{Cov}(\\varepsilon) u_{i} = u_{i}^{\\top} (\\sigma^{2} I_{m}) u_{i} = \\sigma^{2} (u_{i}^{\\top} u_{i}) = \\sigma^{2}\n$$\n$E(k)$ 的第一项是估计量的方差：\n$$\n\\text{Var}(k) = \\sum_{i=1}^{k} \\frac{\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]}{\\sigma_{i}^{2}} = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}\n$$\n$E(k)$ 的第二项是期望偏差平方：\n$$\n\\text{Bias}^{2}(k) = \\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]\n$$\n因此，总期望均方误差分解为：\n$$\nE(k) = \\underbrace{\\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}}_{\\text{方差}} + \\underbrace{\\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]}_{\\text{期望偏差平方}}\n$$\n给定模型 $\\sigma_{i} = 2.5 \\, i^{-1.3}$ 和 $\\mathbb{E}[c_{i}^{2}] = C^{2} i^{-2p}$，其中 $C=0.8$ 且 $p=1$。记 $A=2.5$ 和 $a=1.3$。\n$$\nE(k) = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{(A i^{-a})^{2}} + \\sum_{i=k+1}^{n} C^{2} i^{-2p} = \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p}\n$$\n为了找到使 $E(k)$ 最小化的最优整数 $k^{\\star}$，我们考察差值 $E(k+1) - E(k)$：\n$$\nE(k+1) - E(k) = \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k+1} i^{2a} + C^{2}\\sum_{i=k+2}^{n} i^{-2p} \\right) - \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p} \\right)\n$$\n$$\nE(k+1) - E(k) = \\frac{\\sigma^{2}}{A^{2}}(k+1)^{2a} - C^{2}(k+1)^{-2p}\n$$\n最小值出现在 $k=k^{\\star}$ 处，此时误差在减小后开始增大。这对应于差值 $E(k+1)-E(k)$ 的符号由负变正的点。我们寻找整数 $k$ 使得 $E(k+1)-E(k) > 0$ 且 $E(k)-E(k-1)  0$。最优的 $k^{\\star}$ 将接近于该差值的连续近似为零时的值。\n$$\n\\frac{\\sigma^{2}}{A^{2}}(k^{\\star}+1)^{2a} - C^{2}(k^{\\star}+1)^{-2p} \\approx 0 \\implies (k^{\\star}+1)^{2a+2p} \\approx \\frac{C^{2}A^{2}}{\\sigma^{2}}\n$$\n这给出了最优截断指数的选择准则：\n$$\nk^{\\star}+1 \\approx \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{2}{2a+2p}} = \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{1}{a+p}}\n$$\n现在，我们代入给定的数值：\n$C = 0.8$, $A = 2.5$, $\\sigma = 2.0 \\times 10^{-2} = 0.02$, $a = 1.3$, 且 $p = 1$。\n$$\n\\frac{CA}{\\sigma} = \\frac{0.8 \\times 2.5}{0.02} = \\frac{2}{0.02} = 100\n$$\n$$\na+p = 1.3 + 1 = 2.3\n$$\n$$\nk^{\\star}+1 \\approx (100)^{\\frac{1}{2.3}} \\approx 10^{\\frac{2}{2.3}} \\approx 10^{0.869565} \\approx 7.4056\n$$\n$$\nk^{\\star} \\approx 6.4056\n$$\n由于 $k^{\\star}$ 必须是整数，它很可能是 $6$ 或 $7$。我们必须对这些值检查离散差分 $\\Delta(k) = E(k+1)-E(k)$ 的符号。\n$$\n\\Delta(k) = \\frac{\\sigma^2}{A^2}(k+1)^{2.6} - C^2(k+1)^{-2} = \\frac{(0.02)^2}{(2.5)^2}(k+1)^{2.6} - (0.8)^2(k+1)^{-2}\n$$\n$$\n\\Delta(k) = (6.4 \\times 10^{-5})(k+1)^{2.6} - 0.64(k+1)^{-2}\n$$\n对于 $k=6$：\n$$\n\\Delta(6) = E(7)-E(6) = (6.4 \\times 10^{-5})(7)^{2.6} - 0.64(7)^{-2} \\approx (6.4 \\times 10^{-5})(157.486) - 0.64(0.020408)\n$$\n$$\n\\Delta(6) \\approx 0.010079 - 0.013061 = -0.002982\n$$\n由于 $\\Delta(6)  0$，我们有 $E(7)  E(6)$。误差仍在减小，所以最小值在 $k^{\\star} \\geq 7$ 处取得。\n\n对于 $k=7$：\n$$\n\\Delta(7) = E(8)-E(7) = (6.4 \\times 10^{-5})(8)^{2.6} - 0.64(8)^{-2} \\approx (6.4 \\times 10^{-5})(222.848) - 0.64(0.015625)\n$$\n$$\n\\Delta(7) \\approx 0.014262 - 0.01 = 0.004262\n$$\n由于 $\\Delta(7) > 0$，我们有 $E(8) > E(7)$。误差在 $k=7$ 之后开始增大。\n误差序列满足 $E(6) > E(7)$ 且 $E(7)  E(8)$。因此，最小期望均方误差在整数截断指数 $k^{\\star}=7$ 处取得。",
            "answer": "$$\\boxed{7}$$"
        },
        {
            "introduction": "从数据降维和反演转向用于分类的统计建模，一个常见的地球物理任务是从测井数据中自动识别岩性单元。本练习将介绍高斯混合模型（GMM），这是一种用于无监督聚类的强大概率方法。您将从第一性原理出发，完整推导期望最大化（EM）算法——拟合这些模型的关键引擎，从而深刻理解如何利用统计学习从数据中自动发现潜在的地质结构。",
            "id": "3615487",
            "problem": "考虑一个带有深度索引的多通道地球物理测井的钻孔，其中每个深度样本表示为一个向量 $y_{i} \\in \\mathbb{R}^{d}$（$i=1,\\dots,N$），该向量收集了 $d$个具有物理意义的响应（例如，伽马射线、中子孔隙度、体积密度、声波时差）。假设地层由 $K$ 个岩性类别组成，并且任何深度的测井响应都源于一个潜在的岩性类别。将观测数据建模为 $d$ 维高斯分量的有限混合，\n$$\np(y_{i} \\mid \\Theta) \\;=\\; \\sum_{k=1}^{K} \\pi_{k}\\,\\mathcal{N}\\!\\big(y_{i} \\mid \\mu_{k}, \\Sigma_{k}\\big),\n$$\n其中 $\\Theta = \\big\\{ \\pi_{1:K}, \\mu_{1:K}, \\Sigma_{1:K} \\big\\}$ 包含了混合权重 $\\pi_{k}$（非负且总和为1）、均值 $\\mu_{k} \\in \\mathbb{R}^{d}$以及对称正定的全协方差矩阵 $\\Sigma_{k} \\in \\mathbb{R}^{d \\times d}$。令 $z_{i} \\in \\{1,\\dots,K\\}$ 表示指定深度 $i$ 处岩性类别的未观测到的分类潜变量。\n\n从似然原理和高斯密度的定义出发，根据第一性原理推导用于$\\Theta$的最大似然估计的期望最大化（EM）算法。具体来说：\n- 从联合模型 $p(y_{i}, z_{i} \\mid \\Theta)$ 引入完全数据似然及其对数。\n- 使用基于琴生不等式的有效变分下界构造来阐述期望最大化过程的动机。\n- 在期望步（E-step），使用贝叶斯法则计算每个分量对每个样本的后验责任。\n- 在最大化步（M-step），在约束条件 $\\sum_{k=1}^{K} \\pi_{k} = 1$ 下，关于 $\\pi_{k}$ 最大化期望完全数据对数似然，并关于 $\\mu_{k}$ 和 $\\Sigma_{k}$ 最大化，从而获得闭式更新表达式。\n\n你的最终答案必须是 $\\pi_{k}$, $\\mu_{k}$ 和 $\\Sigma_{k}$ ($k=1,\\dots,K$) 的EM更新的闭式解析表达式，表示为当前参数值和数据 $\\{y_{i}\\}_{i=1}^{N}$ 的函数。不要提供数值。最终答案必须是单一的解析表达式；不要包含任何单位。",
            "solution": "问题是推导用于估计 $d$ 维高斯混合模型（GMM）参数 $\\Theta = \\{ \\pi_{1:K}, \\mu_{1:K}, \\Sigma_{1:K} \\}$ 的期望最大化（EM）算法。数据由 $N$ 个观测值 $\\{y_{i}\\}_{i=1}^{N}$ 组成，其中 $y_i \\in \\mathbb{R}^d$。\n\n在GMM下，观测数据的对数似然由以下公式给出：\n$$\n\\mathcal{L}(\\Theta) = \\ln p(Y \\mid \\Theta) = \\sum_{i=1}^{N} \\ln p(y_{i} \\mid \\Theta) = \\sum_{i=1}^{N} \\ln \\left( \\sum_{k=1}^{K} \\pi_{k}\\,\\mathcal{N}(y_{i} \\mid \\mu_{k}, \\Sigma_{k}) \\right)\n$$\n由于对数内部存在求和，直接对参数 $\\Theta$ 最大化 $\\mathcal{L}(\\Theta)$ 在解析上是难以处理的。EM算法提供了一种迭代方法来寻找该对数似然的局部最大值。\n\n**引入潜变量和完全数据似然**\n\n核心思想是引入一组潜变量 $Z = \\{z_1, \\dots, z_N\\}$，其中每个 $z_i$ 是一个 $K$ 维二元随机向量。我们使用 1-of-K 表示法，因此对于生成 $y_i$ 的分量 $k$，$z_i$ 有一个元素 $z_{ik}=1$，而所有其他元素 $z_{ij}=0$（$j \\neq k$）。$z_{ik}=1$ 的先验概率是混合权重 $\\pi_k$，因此 $p(z_{ik}=1) = \\pi_k$。\n\n观测值 $y_i$ 及其潜变量 $z_i$ 的联合分布可以写为：\n$$\np(y_i, z_i \\mid \\Theta) = p(y_i \\mid z_i, \\Theta) p(z_i \\mid \\Theta) = \\prod_{k=1}^{K} \\left[ \\pi_k \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k) \\right]^{z_{ik}}\n$$\n集合 $(Y,Z)$ 被称为“完全数据”。完全数据对数似然为：\n$$\n\\ln p(Y, Z \\mid \\Theta) = \\sum_{i=1}^{N} \\ln p(y_i, z_i \\mid \\Theta) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} z_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n该表达式关于潜变量 $z_{ik}$ 是线性的，这使其更容易处理。\n\n**变分下界与EM算法**\n\n由于潜变量 $Z$ 是未观测到的，我们不能直接最大化完全数据对数似然。EM算法通过迭代地最大化完全数据对数似然的条件期望来解决这个问题。这可以通过构造观测数据对数似然的一个下界来更正式地说明。\n\n对于潜变量上的任意概率分布 $q(Z)$，我们可以写出：\n$$\n\\mathcal{L}(\\Theta) = \\ln p(Y \\mid \\Theta) = \\sum_{i=1}^{N} \\ln \\sum_{z_i} p(y_i, z_i \\mid \\Theta) = \\sum_{i=1}^{N} \\ln \\left( \\sum_{z_i} q(z_i) \\frac{p(y_i, z_i \\mid \\Theta)}{q(z_i)} \\right)\n$$\n其中 $\\sum_{z_i}$ 表示对向量 $z_i$ 所有可能状态的求和。根据琴生不等式，由于对数是凹函数，我们有 $\\ln(\\mathbb{E}[X]) \\ge \\mathbb{E}[\\ln(X)]$。这给出了对数似然的一个下界：\n$$\n\\mathcal{L}(\\Theta) \\ge \\sum_{i=1}^{N} \\sum_{z_i} q(z_i) \\ln \\frac{p(y_i, z_i \\mid \\Theta)}{q(z_i)} \\equiv \\mathcal{J}(q, \\Theta)\n$$\nEM算法包含两个步骤，迭代地最大化这个下界 $\\mathcal{J}(q, \\Theta)$。令 $\\Theta^{(t)}$ 表示在第 $t$ 次迭代时的参数值。\n\n**期望步（E-Step）**\n\n在E步中，我们固定参数 $\\Theta^{(t)}$，并关于变分分布 $q(Z)$ 最大化下界 $\\mathcal{J}(q, \\Theta^{(t)})$。$\\mathcal{L}(\\Theta^{(t)})$ 和 $\\mathcal{J}(q, \\Theta^{(t)})$ 之间的差距是 $q(Z)$ 与真实后验 $p(Z \\mid Y, \\Theta^{(t)})$ 之间的Kullback-Leibler（KL）散度。当此KL散度为零时，下界最紧（即 $\\mathcal{J}$ 被最大化），这发生在我们将 $q(Z)$ 设置为精确的后验分布时：\n$$\nq(z_i) = p(z_i \\mid y_i, \\Theta^{(t)})\n$$\n数据点 $y_i$ 属于分量 $k$ 的后验概率，记为“责任”（responsibility）$r_{ik}$，使用贝叶斯法则计算：\n$$\nr_{ik} \\equiv p(z_{ik}=1 \\mid y_i, \\Theta^{(t)}) = \\frac{p(y_i \\mid z_{ik}=1, \\Theta^{(t)}) p(z_{ik}=1 \\mid \\Theta^{(t)})}{\\sum_{j=1}^{K} p(y_i \\mid z_{ij}=1, \\Theta^{(t)}) p(z_{ij}=1 \\mid \\Theta^{(t)})}\n$$\n代入模型定义，我们得到：\n$$\nr_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(y_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(y_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})}\n$$\n这一步给出了在当前参数下潜变量的期望值，即 $\\mathbb{E}[z_{ik}] = r_{ik}$。\n\n**最大化步（M-Step）**\n\n在M步中，我们固定责任 $r_{ik}$（即固定 $q(Z)$），并关于模型参数 $\\Theta$ 最大化下界 $\\mathcal{J}(q, \\Theta)$。这等价于最大化完全数据对数似然的期望值，即所谓的 $Q$-函数：\n$$\nQ(\\Theta, \\Theta^{(t)}) = \\mathbb{E}_{Z \\mid Y, \\Theta^{(t)}}[\\ln p(Y, Z \\mid \\Theta)] = \\sum_{i=1}^{N} \\sum_{k=1}^{K} r_{ik} \\left( \\ln \\pi_k + \\ln \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n我们通过关于 $\\pi_k$, $\\mu_k$ 和 $\\Sigma_k$ 最大化 $Q(\\Theta, \\Theta^{(t)})$ 来找到新的参数 $\\Theta^{(t+1)}$。\n\n1.  **关于 $\\pi_k$ 的最大化**：\n    我们在约束条件 $\\sum_{k=1}^{K} \\pi_k = 1$ 下最大化 $\\sum_{i=1}^{N} \\sum_{k=1}^{K} r_{ik} \\ln \\pi_k$。使用拉格朗日乘子 $\\lambda$，我们构造拉格朗日函数 $L = \\sum_{i,k} r_{ik} \\ln \\pi_k + \\lambda(\\sum_{k} \\pi_k - 1)$。令 $\\frac{\\partial L}{\\partial \\pi_k}$ 等于零，得到 $\\pi_k = -(\\sum_i r_{ik})/\\lambda$。令 $N_k = \\sum_{i=1}^N r_{ik}$ 为聚类 $k$ 中的有效点数。则 $\\pi_k = -N_k/\\lambda$。使用约束条件，$\\sum_k \\pi_k = 1 = -(\\sum_k N_k)/\\lambda = -N/\\lambda$，所以 $\\lambda = -N$。这给出了更新公式：\n    $$\n    \\pi_k^{(t+1)} = \\frac{N_k}{N} = \\frac{1}{N} \\sum_{i=1}^{N} r_{ik}\n    $$\n2.  **关于 $\\mu_k$ 的最大化**：\n    $Q$ 中依赖于 $\\mu_k$ 的项是 $\\sum_i r_{ik} \\ln \\mathcal{N}(y_i \\mid \\mu_k, \\Sigma_k)$。最大化此项等价于最小化 $\\sum_i r_{ik} (y_i - \\mu_k)^T \\Sigma_k^{-1} (y_i - \\mu_k)$。对 $\\mu_k$求导并令其为零，得到：\n    $$\n    \\sum_{i=1}^{N} r_{ik} \\Sigma_k^{-1} (y_i - \\mu_k) = 0 \\quad\\implies\\quad \\sum_{i=1}^{N} r_{ik} y_i = \\left(\\sum_{i=1}^{N} r_{ik}\\right) \\mu_k\n    $$\n    这给出了均值的更新公式，它是数据点的加权平均值：\n    $$\n    \\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^{N} r_{ik} y_i}{\\sum_{i=1}^{N} r_{ik}} = \\frac{1}{N_k} \\sum_{i=1}^{N} r_{ik} y_i\n    $$\n3.  **关于 $\\Sigma_k$ 的最大化**：\n    我们最大化 $Q$ 中与 $\\Sigma_k$ 相关的项：$\\sum_i r_{ik} [-\\frac{1}{2} \\ln |\\Sigma_k| - \\frac{1}{2}(y_i - \\mu_k)^T \\Sigma_k^{-1} (y_i - \\mu_k)]$。标准的M步过程在同一步驟中更新 $\\mu_k$ 和 $\\Sigma_k$，所以我们在这里使用新的均值 $\\mu_k^{(t+1)}$。将此表达式对 $\\Sigma_k^{-1}$ 求导并令其为零，我们得到高斯分布协方差的最大似然估计：\n    $$\n    \\Sigma_k^{(t+1)} = \\frac{\\sum_{i=1}^{N} r_{ik} (y_i - \\mu_k^{(t+1)})(y_i - \\mu_k^{(t+1)})^T}{\\sum_{i=1}^{N} r_{ik}} = \\frac{1}{N_k} \\sum_{i=1}^{N} r_{ik} (y_i - \\mu_k^{(t+1)})(y_i - \\mu_k^{(t+1)})^T\n    $$\n这组新参数 $\\Theta^{(t+1)} = \\{\\pi_k^{(t+1)}, \\mu_k^{(t+1)}, \\Sigma_k^{(t+1)}\\}$ 保证满足 $\\mathcal{L}(\\Theta^{(t+1)}) \\ge \\mathcal{L}(\\Theta^{(t)})$。E步和M步被重复执行，直到对数似然或参数收敛。最终的更新方程是在M步中找到的表达式。",
            "answer": "$$\n\\boxed{\n\\pmatrix{\n\\frac{1}{N}\\sum_{i=1}^{N} r_{ik}  \\frac{\\sum_{i=1}^{N} r_{ik} y_i}{\\sum_{i=1}^{N} r_{ik}}  \\frac{\\sum_{i=1}^{N} r_{ik} (y_i - \\mu_k^{\\text{new}})(y_i - \\mu_k^{\\text{new}})^T}{\\sum_{i=1}^{N} r_{ik}}\n}\n}\n$$"
        }
    ]
}