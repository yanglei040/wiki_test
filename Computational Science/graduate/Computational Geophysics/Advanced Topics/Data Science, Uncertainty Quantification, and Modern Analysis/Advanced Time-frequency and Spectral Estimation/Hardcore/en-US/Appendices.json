{
    "hands_on_practices": [
        {
            "introduction": "The Continuous Wavelet Transform (CWT) is an indispensable tool for analyzing the time-varying frequency content of non-stationary signals. To correctly interpret a CWT scalogram, however, one must understand the relationship between the analysis scale $a$ and physical frequency $f$, as well as the regions where the transform is affected by edge effects. This practice guides you through the derivation of these two fundamental properties for the Morlet wavelet, connecting the mathematical formalism of the CWT to its practical application and visual interpretation .",
            "id": "3574571",
            "problem": "Consider a finite-duration seismic time series of length $T$ seconds. Let $\\psi(t)$ be the Morlet mother wavelet defined by\n$$\n\\psi(t) = \\pi^{-1/4} \\exp\\!\\big(i \\,\\omega_{0}\\, t\\big)\\,\\exp\\!\\left(-\\frac{t^{2}}{2}\\right),\n$$\nwhere $\\omega_{0} > 0$ is the central angular frequency of the wavelet. The scale-$a$ wavelet is given by\n$$\n\\psi_{a}(t) = \\frac{1}{\\sqrt{a}}\\,\\psi\\!\\left(\\frac{t}{a}\\right),\n$$\nand the Continuous Wavelet Transform (CWT) of a signal $x(t)$ with respect to $\\psi(t)$ at scale $a$ and time $b$ is\n$$\nW_{x}(a,b) = \\int_{-\\infty}^{\\infty} x(t)\\,\\frac{1}{\\sqrt{a}}\\,\\psi^{*}\\!\\left(\\frac{t-b}{a}\\right)\\,\\mathrm{d}t,\n$$\nwhere $\\psi^{*}$ denotes complex conjugation. Assume standard Fourier transform conventions in angular frequency $\\omega$ such that the transform of a time-scaled function $f(t/a)/\\sqrt{a}$ is $\\sqrt{a}\\,F(a\\,\\omega)$, where $F(\\omega)$ is the Fourier transform of $f(t)$.\n\nStarting from these definitions and the scaling and modulation properties of the Fourier transform, derive the mapping between the wavelet scale $a$ and the physical frequency $f$ (in cycles per second) corresponding to the peak of the Morlet wavelet’s spectral response. Then, using the Gaussian envelope of the Morlet wavelet, define the cone of influence as the locus where the wavelet power has decayed to $\\exp(-2)$ of its maximum due to the finite time support of the wavelet, and compute the explicit boundary offset $t_{\\mathrm{COI}}(a)$ from each edge for a record of duration $T$.\n\nProvide your final answer as a two-entry row matrix containing the frequency mapping and the cone-of-influence boundary offset as functions of $a$ and $\\omega_{0}$. Express the frequency in cycles per second. No numerical approximation or rounding is required.",
            "solution": "The problem requires the derivation of two quantities related to the Continuous Wavelet Transform (CWT) using the Morlet wavelet: the mapping between wavelet scale and physical frequency, and the cone of influence boundary offset.\n\n**Part 1: Derivation of the Scale-to-Frequency Mapping**\n\nThe objective is to find the relationship between the wavelet scale $a$ and the physical frequency $f$ (in cycles per second) at which the spectral response of the scaled wavelet $\\psi_{a}(t)$ is maximum. This requires finding the Fourier transform of $\\psi_{a}(t)$.\n\nFirst, we find the Fourier transform of the Morlet mother wavelet, $\\psi(t)$, which is given as:\n$$\n\\psi(t) = \\pi^{-1/4} \\exp(i\\omega_{0}t) \\exp\\left(-\\frac{t^{2}}{2}\\right)\n$$\nLet $\\Psi(\\omega)$ denote the Fourier transform of $\\psi(t)$. The expression for $\\psi(t)$ is a product of a Gaussian function and a complex exponential. We use the frequency-shifting property of the Fourier transform, which states that if $g(t)$ has Fourier transform $G(\\omega)$, then the Fourier transform of $g(t)\\exp(i\\omega_{0}t)$ is $G(\\omega - \\omega_{0})$.\n\nHere, let $g(t) = \\pi^{-1/4} \\exp(-t^{2}/2)$. The standard Fourier transform of a Gaussian function $\\exp(-\\alpha t^{2})$ is $\\sqrt{\\pi/\\alpha} \\exp(-\\omega^{2}/(4\\alpha))$. For $g(t)$, the constant pre-factor is $\\pi^{-1/4}$ and the parameter $\\alpha$ in the exponent is $1/2$.\nThe Fourier transform of $\\exp(-t^{2}/2)$ is:\n$$\n\\mathcal{F}\\left\\{\\exp\\left(-\\frac{t^{2}}{2}\\right)\\right\\} = \\sqrt{\\frac{\\pi}{1/2}} \\exp\\left(-\\frac{\\omega^{2}}{4(1/2)}\\right) = \\sqrt{2\\pi} \\exp\\left(-\\frac{\\omega^{2}}{2}\\right)\n$$\nTherefore, the Fourier transform of $g(t)$ is:\n$$\nG(\\omega) = \\pi^{-1/4} \\left( \\sqrt{2\\pi} \\exp\\left(-\\frac{\\omega^{2}}{2}\\right) \\right) = 2^{1/2} \\pi^{1/4} \\exp\\left(-\\frac{\\omega^{2}}{2}\\right)\n$$\nApplying the frequency-shifting property, the Fourier transform of the mother wavelet $\\psi(t)$ is:\n$$\n\\Psi(\\omega) = G(\\omega - \\omega_{0}) = 2^{1/2} \\pi^{1/4} \\exp\\left(-\\frac{(\\omega - \\omega_{0})^{2}}{2}\\right)\n$$\nThe peak of this spectral response occurs where the magnitude $|\\Psi(\\omega)|$ is maximum. This corresponds to the minimum of the term $(\\omega - \\omega_{0})^{2}$ in the exponent, which occurs at $\\omega = \\omega_{0}$.\n\nNext, we find the Fourier transform of the scaled wavelet, $\\psi_{a}(t) = \\frac{1}{\\sqrt{a}}\\psi(t/a)$. The problem statement provides the scaling property of the Fourier transform: the transform of $f(t/a)/\\sqrt{a}$ is $\\sqrt{a}F(a\\omega)$. Let $\\Psi_{a}(\\omega)$ be the Fourier transform of $\\psi_{a}(t)$. Applying this property, we get:\n$$\n\\Psi_{a}(\\omega) = \\sqrt{a} \\Psi(a\\omega) = \\sqrt{a} \\left( 2^{1/2} \\pi^{1/4} \\exp\\left(-\\frac{(a\\omega - \\omega_{0})^{2}}{2}\\right) \\right)\n$$\n$$\n\\Psi_{a}(\\omega) = \\sqrt{2a} \\, \\pi^{1/4} \\exp\\left(-\\frac{(a\\omega - \\omega_{0})^{2}}{2}\\right)\n$$\nThe peak of the spectral magnitude $|\\Psi_{a}(\\omega)|$ occurs when the term $(a\\omega - \\omega_{0})^{2}$ in the exponent is minimized. This happens when $a\\omega - \\omega_{0} = 0$. Let $\\omega_{\\text{peak}}$ denote the angular frequency corresponding to this peak.\n$$\na\\omega_{\\text{peak}} = \\omega_{0} \\implies \\omega_{\\text{peak}} = \\frac{\\omega_{0}}{a}\n$$\nThe problem asks for the physical frequency $f$ in cycles per second (Hz). The relationship between angular frequency $\\omega$ (in radians per second) and physical frequency $f$ is $\\omega = 2\\pi f$. Therefore, the peak physical frequency $f_{\\text{peak}}$ is:\n$$\n2\\pi f_{\\text{peak}} = \\frac{\\omega_{0}}{a} \\implies f_{\\text{peak}} = \\frac{\\omega_{0}}{2\\pi a}\n$$\nThis is the mapping between the wavelet scale $a$ and the corresponding physical frequency $f$.\n\n**Part 2: Derivation of the Cone of Influence Boundary Offset**\n\nThe cone of influence (COI) represents the region in the time-scale plane where edge effects from the finite duration of the signal become significant. The problem defines the COI boundary as the locus where the wavelet power has decayed to $\\exp(-2)$ of its maximum. This decay is due to the Gaussian envelope of the wavelet.\n\nThe scaled wavelet is:\n$$\n\\psi_{a}(t) = \\frac{1}{\\sqrt{a}}\\psi\\left(\\frac{t}{a}\\right) = \\frac{1}{\\sqrt{a}} \\pi^{-1/4} \\exp\\left(i\\omega_{0}\\frac{t}{a}\\right) \\exp\\left(-\\frac{(t/a)^{2}}{2}\\right) = \\frac{1}{\\sqrt{a}} \\pi^{-1/4} \\exp\\left(i\\frac{\\omega_{0}t}{a}\\right) \\exp\\left(-\\frac{t^{2}}{2a^{2}}\\right)\n$$\nThe power of the wavelet at time $t$ is proportional to $|\\psi_{a}(t)|^{2}$.\n$$\n|\\psi_{a}(t)|^{2} = \\left|\\frac{1}{\\sqrt{a}} \\pi^{-1/4} \\exp\\left(i\\frac{\\omega_{0}t}{a}\\right) \\exp\\left(-\\frac{t^{2}}{2a^{2}}\\right)\\right|^{2}\n$$\nSince $|\\exp(i\\theta)| = 1$ for any real $\\theta$, the complex exponential term has a magnitude of $1$. The power envelope is:\n$$\n|\\psi_{a}(t)|^{2} = \\left(\\frac{1}{\\sqrt{a}} \\pi^{-1/4}\\right)^{2} \\left(\\exp\\left(-\\frac{t^{2}}{2a^{2}}\\right)\\right)^{2} = \\frac{1}{a\\sqrt{\\pi}} \\exp\\left(-\\frac{t^{2}}{a^{2}}\\right)\n$$\nThe power is a Gaussian function of time $t$, centered at $t=0$. The maximum power occurs at $t=0$:\n$$\n|\\psi_{a}(0)|^{2} = \\frac{1}{a\\sqrt{\\pi}}\n$$\nAccording to the problem's definition, the COI boundary offset, which we denote $t_{\\mathrm{COI}}(a)$, is the time at which the wavelet power drops to $\\exp(-2)$ times its maximum value. We set up the equation:\n$$\n|\\psi_{a}(t_{\\mathrm{COI}}(a))|^{2} = |\\psi_{a}(0)|^{2} \\times \\exp(-2)\n$$\nSubstituting the expressions for the power:\n$$\n\\frac{1}{a\\sqrt{\\pi}} \\exp\\left(-\\frac{t_{\\mathrm{COI}}(a)^{2}}{a^{2}}\\right) = \\frac{1}{a\\sqrt{\\pi}} \\exp(-2)\n$$\nFor this equality to hold, the exponents must be equal:\n$$\n-\\frac{t_{\\mathrm{COI}}(a)^{2}}{a^{2}} = -2\n$$\nSolving for $t_{\\mathrm{COI}}(a)$:\n$$\nt_{\\mathrm{COI}}(a)^{2} = 2a^{2}\n$$\nSince the offset must be a positive quantity representing a time duration, we take the positive root:\n$$\nt_{\\mathrm{COI}}(a) = \\sqrt{2}a\n$$\nThis time $t_{\\mathrm{COI}}(a)$ is the e-folding time for the wavelet power at scale $a$. It defines the width of the region, measured inward from each edge of the time series (at $t=0$ and $t=T$), where the wavelet analysis is affected by boundary artifacts. Thus, the explicit boundary offset from each edge is $\\sqrt{2}a$.\n\n**Final Answer Formulation**\n\nThe two requested quantities are the frequency mapping $f(a)$ and the cone-of-influence boundary offset $t_{\\mathrm{COI}}(a)$.\n1.  Frequency mapping: $f = \\frac{\\omega_{0}}{2\\pi a}$\n2.  COI boundary offset: $t_{\\mathrm{COI}}(a) = \\sqrt{2}a$\n\nThese will be presented as a two-entry row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\omega_{0}}{2\\pi a}  \\sqrt{2}a\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the multitaper method provides high-quality spectral estimates, a reliable estimate is incomplete without a corresponding measure of its uncertainty. This exercise tackles the critical task of variance estimation for a spectral line component, contrasting a classic analytical formula with the computationally intensive but more robust jackknife resampling technique. By exploring scenarios where the underlying noise model deviates from the ideal (white noise), you will gain practical insight into the reliability of different uncertainty quantification methods in realistic geophysical applications .",
            "id": "3574609",
            "problem": "Consider the estimation of a single complex narrowband line component embedded in noise using the multitaper method with Discrete Prolate Spheroidal Sequences (DPSS). Let the discrete-time complex signal be modeled as $x_n = A \\exp(i \\omega_0 n) + \\varepsilon_n$ for $n = 0, 1, \\dots, N-1$, where $A \\in \\mathbb{C}$ is the complex amplitude of the line, $\\omega_0$ is the true angular frequency in radians per sample, and $\\varepsilon_n$ is a zero-mean complex noise process. The time-half-bandwidth parameter is $NW$, and $K$ orthonormal DPSS tapers $w_k[n]$, for $k = 1, \\dots, K$, are applied. The demodulated tapered measurements at the line frequency are $y_k = \\sum_{n=0}^{N-1} w_k[n] x_n \\exp(-i \\omega_0 n)$, and the coherent gains are $c_k = \\sum_{n=0}^{N-1} w_k[n]$.\n\nStarting only from fundamental definitions and well-tested facts about linear estimation under additive noise and resampling-based uncertainty quantification, perform the following tasks:\n\n1. Derive a principled estimator $\\hat{A}$ for the complex amplitude $A$ using the $K$ demodulated tapered measurements $y_k$ and the coherent gains $c_k$, assuming independent additive noise across tapers. Then, under the assumption of independent and identically distributed complex circular Gaussian white noise $\\varepsilon_n$ with per-sample second moment $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$, derive an analytical expression for the variance of $\\hat{A}$ in terms of $\\sigma^2$ and $\\{c_k\\}$.\n\n2. Construct a jackknife variance estimator for $\\hat{A}$ based on the $K$ leave-one-out estimators obtained by excluding each taper in turn. Treat the complex amplitude as two real parameters (real and imaginary parts), and combine their jackknife variances into a single scalar variance for the complex estimator by summation.\n\n3. Implement a Monte Carlo experiment to evaluate the reliability of the analytical variance and the jackknife variance for narrowband peaks. For each test case specified below, synthesize $M$ independent realizations of the data, compute the multitaper least-squares amplitude estimator $\\hat{A}$ for each realization, compute the jackknife variance from each realization, and estimate the empirical variance of $\\hat{A}$ across realizations. Compare the average jackknife variance to the empirical variance and compare the analytical variance to the empirical variance. Define jackknife uncertainty as \"more reliable\" if its average absolute relative error (as a decimal) with respect to the empirical variance is strictly smaller than the absolute relative error of the analytical variance.\n\nUse these well-posed test cases:\n- Case 1 (general, happy path): $N = 512$, $NW = 4$, $K = 7$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, white noise with $\\sigma^2 = 0.2$.\n- Case 2 (few tapers, boundary): $N = 512$, $NW = 2$, $K = 3$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, white noise with $\\sigma^2 = 0.2$.\n- Case 3 (colored noise, narrowband peak): $N = 512$, $NW = 4$, $K = 7$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, autoregressive of order $1$ (AR(1)) noise with parameter $\\phi = 0.9$ and stationary per-sample second moment $\\sigma^2 = 0.2$.\n- Case 4 (boundary, very few tapers and strong color): $N = 512$, $NW = 1.5$, $K = 2$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, AR(1) noise with parameter $\\phi = 0.95$ and stationary per-sample second moment $\\sigma^2 = 0.2$.\n\nIn the AR(1) cases, the complex noise is $\\varepsilon_n = \\phi \\varepsilon_{n-1} + \\eta_n$, where $\\eta_n$ is independent complex circular Gaussian innovation and chosen so that the stationary per-sample second moment satisfies $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$. All angular frequencies must be treated in radians. Amplitudes and variances are dimensionless in this purely mathematical setup.\n\nImplementation details required:\n- Use $M = 200$ Monte Carlo realizations per test case.\n- DPSS tapers must be normalized to unit $\\ell_2$ norm, i.e., $\\sum_{n=0}^{N-1} |w_k[n]|^2 = 1$ for each $k$.\n- The estimator $\\hat{A}$ must be the linear least-squares estimator under the linear model defined by $\\{y_k\\}$ and $\\{c_k\\}$.\n\nYour program should produce a single line of output containing four boolean values as a comma-separated list enclosed in square brackets, in the order of the four cases above, where each boolean indicates whether the jackknife variance is more reliable than the analytical variance for that case (i.e., whether the jackknife’s average relative error is strictly smaller than the analytical variance’s relative error). For example, a valid output looks like \"[True,False,True,False]\".",
            "solution": "The problem requires the derivation and comparison of two variance estimators for the complex amplitude of a narrowband signal component estimated via the multitaper method. The first estimator is analytical, derived under a white noise assumption, while the second is a non-parametric jackknife resampling estimator. Their reliability is to be evaluated via a Monte Carlo simulation under different noise conditions.\n\nThe solution proceeds in three parts: first, the derivation of the least-squares amplitude estimator and its analytical variance; second, the construction of the jackknife variance estimator; and third, the design of the numerical experiment to compare them.\n\n**Part 1: Derivation of the Amplitude Estimator and Analytical Variance**\n\nThe discrete-time signal model is given by $x_n = A \\exp(i \\omega_0 n) + \\varepsilon_n$ for $n = 0, 1, \\dots, N-1$. Here, $A \\in \\mathbb{C}$ is the unknown complex amplitude, $\\omega_0$ is the known angular frequency, and $\\varepsilon_n$ is a complex noise process.\n\nThe $K$ demodulated tapered measurements are defined as $y_k = \\sum_{n=0}^{N-1} w_k[n] x_n \\exp(-i \\omega_0 n)$ for $k=1, \\dots, K$. Substituting the signal model into this definition yields:\n$$\ny_k = \\sum_{n=0}^{N-1} w_k[n] (A e^{i \\omega_0 n} + \\varepsilon_n) e^{-i \\omega_0 n} = A \\sum_{n=0}^{N-1} w_k[n] + \\sum_{n=0}^{N-1} w_k[n] \\varepsilon_n e^{-i \\omega_0 n}\n$$\nThis can be written as a linear model. Let $c_k = \\sum_{n=0}^{N-1} w_k[n]$ be the coherent gain of the $k$-th taper and $\\nu_k = \\sum_{n=0}^{N-1} w_k[n] \\varepsilon_n e^{-i \\omega_0 n}$ be the corresponding noise term. The model for the $K$ measurements becomes:\n$$\ny_k = A c_k + \\nu_k, \\quad k = 1, \\dots, K\n$$\nWe seek the linear least-squares estimator $\\hat{A}$ for the complex parameter $A$, which minimizes the sum of squared residuals, $J(A) = \\sum_{k=1}^K |y_k - A c_k|^2$. Note that the tapers $w_k[n]$ are real, so their sums $c_k$ are also real.\nTo find the minimum, we can use complex calculus, setting the derivative with respect to the complex conjugate $A^*$ to zero:\n$$\n\\frac{\\partial J(A)}{\\partial A^*} = \\frac{\\partial}{\\partial A^*} \\sum_{k=1}^K (y_k - A c_k)(y_k^* - A^* c_k) = \\sum_{k=1}^K (y_k - A c_k)(-c_k) = 0\n$$\n$$\n- \\sum_{k=1}^K y_k c_k + A \\sum_{k=1}^K c_k^2 = 0\n$$\nSolving for $A$ gives the least-squares estimator $\\hat{A}$:\n$$\n\\hat{A} = \\frac{\\sum_{k=1}^K y_k c_k}{\\sum_{k=1}^K c_k^2}\n$$\nNext, we derive the variance of this estimator, $\\mathrm{Var}(\\hat{A}) = \\mathbb{E}[|\\hat{A} - \\mathbb{E}[\\hat{A}]|^2]$. First, we find the expected value of $\\hat{A}$. Since $\\mathbb{E}[\\varepsilon_n] = 0$, it follows that $\\mathbb{E}[\\nu_k] = 0$.\n$$\n\\mathbb{E}[\\hat{A}] = \\frac{\\sum_{k=1}^K \\mathbb{E}[y_k] c_k}{\\sum_{j=1}^K c_j^2} = \\frac{\\sum_{k=1}^K (A c_k + \\mathbb{E}[\\nu_k]) c_k}{\\sum_{j=1}^K c_j^2} = \\frac{A \\sum_{k=1}^K c_k^2}{\\sum_{j=1}^K c_j^2} = A\n$$\nThe estimator is unbiased. The variance is therefore $\\mathrm{Var}(\\hat{A}) = \\mathbb{E}[|\\hat{A} - A|^2]$.\n$$\n\\hat{A} - A = \\frac{\\sum_{k=1}^K (A c_k + \\nu_k) c_k}{\\sum_{j=1}^K c_j^2} - A = \\frac{\\sum_{k=1}^K \\nu_k c_k}{\\sum_{j=1}^K c_j^2}\n$$\n$$\n\\mathrm{Var}(\\hat{A}) = \\mathbb{E}\\left[ \\left| \\frac{\\sum_k \\nu_k c_k}{\\sum_j c_j^2} \\right|^2 \\right] = \\frac{1}{\\left(\\sum_j c_j^2\\right)^2} \\mathbb{E}\\left[ \\left(\\sum_k \\nu_k c_k\\right) \\left(\\sum_l \\nu_l^* c_l\\right) \\right] = \\frac{1}{\\left(\\sum_j c_j^2\\right)^2} \\sum_{k,l} c_k c_l \\mathbb{E}[\\nu_k \\nu_l^*]\n$$\nTo evaluate $\\mathbb{E}[\\nu_k \\nu_l^*]$, we use the model assumption for this part: $\\varepsilon_n$ is IID complex circular Gaussian white noise with $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$. This implies $\\mathbb{E}[\\varepsilon_n \\varepsilon_m^*] = \\sigma^2 \\delta_{nm}$.\n$$\n\\mathbb{E}[\\nu_k \\nu_l^*] = \\mathbb{E}\\left[ \\left(\\sum_n w_k[n] \\varepsilon_n e^{-i \\omega_0 n}\\right) \\left(\\sum_m w_l[m] \\varepsilon_m^* e^{i \\omega_0 m}\\right) \\right] = \\sum_{n,m} w_k[n] w_l[m] e^{-i \\omega_0 (n-m)} \\mathbb{E}[\\varepsilon_n \\varepsilon_m^*]\n$$\n$$\n\\mathbb{E}[\\nu_k \\nu_l^*] = \\sum_{n,m} w_k[n] w_l[m] e^{-i \\omega_0 (n-m)} \\sigma^2 \\delta_{nm} = \\sigma^2 \\sum_n w_k[n] w_l[n]\n$$\nThe problem states that the DPSS tapers $w_k[n]$ are orthonormal, which for real-valued tapers means $\\sum_n w_k[n] w_l[n] = \\delta_{kl}$. Therefore, $\\mathbb{E}[\\nu_k \\nu_l^*] = \\sigma^2 \\delta_{kl}$. The noise terms $\\nu_k$ are uncorrelated. Substituting this back into the variance expression:\n$$\n\\mathrm{Var}(\\hat{A}) = \\frac{1}{\\left(\\sum_j c_j^2\\right)^2} \\sum_{k,l} c_k c_l (\\sigma^2 \\delta_{kl}) = \\frac{\\sigma^2}{\\left(\\sum_j c_j^2\\right)^2} \\sum_k c_k^2 = \\frac{\\sigma^2}{\\sum_{k=1}^K c_k^2}\n$$\nThis is the analytical expression for the variance of $\\hat{A}$ under the white noise assumption.\n\n**Part 2: Construction of the Jackknife Variance Estimator**\n\nThe jackknife is a resampling technique for estimating the variance of a statistic. It involves systematically recomputing the statistic with one observation left out. Let $\\hat{A}$ be the full estimate. The leave-one-out estimators $\\hat{A}_{(j)}$ are computed by omitting the $j$-th measurement pair $(y_j, c_j)$:\n$$\n\\hat{A}_{(j)} = \\frac{\\sum_{k \\neq j} y_k c_k}{\\sum_{k \\neq j} c_k^2}, \\quad j = 1, \\dots, K\n$$\nThe problem specifies treating the complex amplitude $\\hat{A} = \\hat{A}_R + i \\hat{A}_I$ as two real parameters and summing their individual jackknife variances. The jackknife variance of a real-valued estimator $\\hat{\\theta}$ based on $K$ samples is $\\mathrm{Var}_{\\text{jack}}(\\hat{\\theta}) = \\frac{K-1}{K} \\sum_{j=1}^K (\\hat{\\theta}_{(j)} - \\bar{\\theta}_{(\\cdot)})^2$, where $\\bar{\\theta}_{(\\cdot)} = \\frac{1}{K}\\sum_j \\hat{\\theta}_{(j)}$.\nApplying this to the real and imaginary parts of $\\hat{A}$:\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}_R) = \\frac{K-1}{K} \\sum_{j=1}^K (\\mathrm{Re}(\\hat{A}_{(j)}) - \\mathrm{Re}(\\bar{A}_{(\\cdot)}))^2\n$$\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}_I) = \\frac{K-1}{K} \\sum_{j=1}^K (\\mathrm{Im}(\\hat{A}_{(j)}) - \\mathrm{Im}(\\bar{A}_{(\\cdot)}))^2\n$$\nwhere $\\bar{A}_{(\\cdot)} = \\frac{1}{K} \\sum_{j=1}^K \\hat{A}_{(j)}$. The total jackknife variance is the sum:\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}) = \\mathrm{Var}_{\\text{jack}}(\\hat{A}_R) + \\mathrm{Var}_{\\text{jack}}(\\hat{A}_I)\n$$\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}) = \\frac{K-1}{K} \\sum_{j=1}^K \\left[ (\\mathrm{Re}(\\hat{A}_{(j)}) - \\mathrm{Re}(\\bar{A}_{(\\cdot)}))^2 + (\\mathrm{Im}(\\hat{A}_{(j)}) - \\mathrm{Im}(\\bar{A}_{(\\cdot)}))^2 \\right]\n$$\nThis expression simplifies to:\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}) = \\frac{K-1}{K} \\sum_{j=1}^K |\\hat{A}_{(j)} - \\bar{A}_{(\\cdot)}|^2\n$$\nThis formula provides a non-parametric estimate of the variance of $\\hat{A}$ from a single data realization. Unlike the analytical variance, its derivation does not assume that the noise components $\\nu_k$ are uncorrelated, making it potentially more robust when the underlying noise $\\varepsilon_n$ is colored.\n\n**Part 3: Monte Carlo Experiment Design**\n\nTo evaluate the reliability of the two variance estimators, we perform a Monte Carlo simulation for each of the four test cases. The procedure for a single test case is as follows:\n\n1.  **Setup**: The parameters $N, NW, K, \\omega_0, A, \\sigma^2,$ and noise type (white or AR(1) with parameter $\\phi$) are fixed. The $K$ DPSS tapers $w_k[n]$ are generated once, and their coherent gains $c_k$ are computed. The analytical variance, $\\mathrm{Var}_{\\text{an}} = \\sigma^2 / \\sum c_k^2$, is calculated. This analytical formula is theoretically valid only for white noise cases. For colored noise cases, it is based on a violated assumption and is expected to be inaccurate.\n\n2.  **Simulation Loop**: $M=200$ independent realizations are generated. For each realization $m=1, \\dots, M$:\n    a.  A noise sequence $\\varepsilon_n$ of length $N$ is synthesized.\n        -   **White Noise**: Complex circular Gaussian noise is generated by creating independent real and imaginary parts from a normal distribution $\\mathcal{N}(0, \\sigma^2/2)$.\n        -   **AR(1) Noise**: The process is $\\varepsilon_n = \\phi \\varepsilon_{n-1} + \\eta_n$. To achieve a stationary variance of $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$, the complex circular white noise innovations $\\eta_n$ must have variance $\\mathbb{E}[|\\eta_n|^2] = \\sigma^2 (1-|\\phi|^2)$. The sequence $\\varepsilon_n$ is generated by filtering the innovations $\\eta_n$ with a recursive filter defined by the denominator polynomial $1 - \\phi z^{-1}$.\n    b.  The full signal $x_n = A e^{i \\omega_0 n} + \\varepsilon_n$ is formed.\n    c.  The $K$ measurements $y_k$ are computed, and from these, the full amplitude estimate $\\hat{A}^{(m)}$ and the single-realization jackknife variance estimate $\\mathrm{Var}_{\\text{jack}}^{(m)}$ are calculated using the formulas derived above.\n\n3.  **Analysis**: After the loop, we have a collection of $M$ estimates $\\{\\hat{A}^{(m)}\\}$ and $M$ jackknife variances $\\{\\mathrm{Var}_{\\text{jack}}^{(m)}\\}$.\n    a.  The **empirical variance** of $\\hat{A}$ is computed from the sample of estimates. This serves as our \"ground truth\" for the variance: $\\mathrm{Var}_{\\text{emp}} = \\frac{1}{M-1} \\sum_{m=1}^M |\\hat{A}^{(m)} - \\bar{A}|^2$, where $\\bar{A} = \\frac{1}{M} \\sum_m \\hat{A}^{(m)}$.\n    b.  The **average jackknife variance** is computed: $\\overline{\\mathrm{Var}}_{\\text{jack}} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{Var}_{\\text{jack}}^{(m)}$.\n    c.  The reliabilities are compared using the absolute relative error with respect to the empirical variance:\n        $$\n        E_{\\text{an}} = \\left| \\frac{\\mathrm{Var}_{\\text{an}} - \\mathrm{Var}_{\\text{emp}}}{\\mathrm{Var}_{\\text{emp}}} \\right|, \\quad E_{\\text{jack}} = \\left| \\frac{\\overline{\\mathrm{Var}}_{\\text{jack}} - \\mathrm{Var}_{\\text{emp}}}{\\mathrm{Var}_{\\text{emp}}} \\right|\n        $$\n    d.  The jackknife method is deemed \"more reliable\" for a given test case if $E_{\\text{jack}}  E_{\\text{an}}$. The boolean result is recorded.\n\nThis process is repeated for all four cases, and the final output is a list of four booleans.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal.windows import dpss\nfrom scipy.signal import lfilter\n\ndef run_monte_carlo(N, NW, K, omega0, A_true, sigma_sq, phi, M):\n    \"\"\"\n    Performs a Monte Carlo simulation for one test case.\n\n    Returns:\n        bool: True if the jackknife variance is more reliable than the analytical variance.\n    \"\"\"\n    # 1. Generate DPSS tapers and coherent gains\n    # The dpss function returns int(2*NW) tapers. We select the first K.\n    tapers = dpss(N, NW, K)\n    \n    # Coherent gains (c_k)\n    c = np.sum(tapers, axis=1)\n\n    # 2. Calculate analytical variance (based on white noise assumption)\n    var_analytical = sigma_sq / np.sum(c**2)\n\n    # 3. Monte Carlo loop\n    estimates_A_hat = np.zeros(M, dtype=np.complex128)\n    variances_jack = np.zeros(M)\n    \n    n_indices = np.arange(N)\n    signal_component = A_true * np.exp(1j * omega0 * n_indices)\n\n    # Set up random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n\n    for m in range(M):\n        # a. Generate noise\n        if phi == 0:  # White noise case\n            noise_real = rng.normal(scale=np.sqrt(sigma_sq / 2), size=N)\n            noise_imag = rng.normal(scale=np.sqrt(sigma_sq / 2), size=N)\n            noise = noise_real + 1j * noise_imag\n        else:  # AR(1) colored noise case\n            var_eta = sigma_sq * (1 - phi**2)\n            eta_real = rng.normal(scale=np.sqrt(var_eta / 2), size=N)\n            eta_imag = rng.normal(scale=np.sqrt(var_eta / 2), size=N)\n            eta = eta_real + 1j * eta_imag\n            # Generate AR(1) process using a filter\n            # The filter implements eps[n] = phi * eps[n-1] + eta[n]\n            # with initial condition eps[-1] = 0\n            noise = lfilter([1], [1, -phi], eta)\n        \n        # b. Synthesize signal\n        x = signal_component + noise\n\n        # c. Compute demodulated measurements y_k\n        y_k = np.zeros(K, dtype=np.complex128)\n        demodulator = np.exp(-1j * omega0 * n_indices)\n        for k in range(K):\n            y_k[k] = np.sum(tapers[k, :] * x * demodulator)\n\n        # d. Compute full estimate A_hat\n        A_hat_m = np.sum(y_k * c) / np.sum(c**2)\n        estimates_A_hat[m] = A_hat_m\n\n        # e. Compute jackknife variance for this realization\n        A_hat_leave_one_out = np.zeros(K, dtype=np.complex128)\n        k_indices = np.arange(K)\n        for j in range(K):\n            # Indices for leave-one-out subset\n            subset_idx = k_indices != j\n            y_subset = y_k[subset_idx]\n            c_subset = c[subset_idx]\n            A_hat_leave_one_out[j] = np.sum(y_subset * c_subset) / np.sum(c_subset**2)\n        \n        A_hat_loo_mean = np.mean(A_hat_leave_one_out)\n        var_jack_m = (K - 1) / K * np.sum(np.abs(A_hat_leave_one_out - A_hat_loo_mean)**2)\n        variances_jack[m] = var_jack_m\n\n    # 4. Post-processing\n    # Empirical variance (sample variance with ddof=1)\n    var_empirical = np.var(estimates_A_hat, ddof=1)\n\n    # Average jackknife variance\n    avg_var_jack = np.mean(variances_jack)\n\n    # 5. Compare errors\n    err_analytical = np.abs((var_analytical - var_empirical) / var_empirical)\n    err_jackknife = np.abs((avg_var_jack - var_empirical) / var_empirical)\n\n    return err_jackknife  err_analytical\n\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: white noise, general\n        {'N': 512, 'NW': 4, 'K': 7, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.0},\n        # Case 2: white noise, few tapers\n        {'N': 512, 'NW': 2, 'K': 3, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.0},\n        # Case 3: colored noise, general\n        {'N': 512, 'NW': 4, 'K': 7, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.9},\n        # Case 4: colored noise, few tapers, strong color\n        {'N': 512, 'NW': 1.5, 'K': 2, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.95},\n    ]\n\n    M = 200  # Number of Monte Carlo realizations\n\n    results = []\n    for case in test_cases:\n        is_jackknife_more_reliable = run_monte_carlo(\n            N=case['N'],\n            NW=case['NW'],\n            K=case['K'],\n            omega0=case['omega0'],\n            A_true=case['A_true'],\n            sigma_sq=case['sigma_sq'],\n            phi=case['phi'],\n            M=M\n        )\n        results.append(is_jackknife_more_reliable)\n\n    # Final print statement in the exact required format.\n    # The output format must be a string like \"[True,False,True,False]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The Capon method, or Minimum Variance Distortionless Response (MVDR) beamforming, offers significantly higher resolution than conventional methods for estimating the spatial spectrum of wavefields. However, this power comes with vulnerabilities, most notably a severe performance degradation in the presence of coherent multipath signals—a common scenario in layered media. This problem challenges you to derive the analytical bias of the Capon estimator in such a scenario, providing a deep understanding of why the method fails and how to recognize its biased output in practice .",
            "id": "3574574",
            "problem": "Consider a one-dimensional uniform linear array observing a narrowband seismic plane wave in a stratified (layered) medium that induces a single coherent multipath return. Adopt the narrowband array data model at fixed temporal frequency $f$:\n- The array snapshot vector is $x(t) \\in \\mathbb{C}^{M}$, where $M$ is the number of sensors.\n- The direct-arrival steering vector is $a(k_d,f) \\in \\mathbb{C}^{M}$, parameterized by the horizontal wavenumber $k_d$ (in radians per meter).\n- The coherent reflected-arrival steering vector is $a(k_r,f) \\in \\mathbb{C}^{M}$ at horizontal wavenumber $k_r$ (in radians per meter).\n- The source signal is $s(t) \\in \\mathbb{C}$ with zero mean and variance $\\mathbb{E}[|s(t)|^2] = P_s$, where $P_s$ is the source power (dimensionless).\n- The coherent reflection coefficient is $r \\in \\mathbb{C}$, where $|r|$ is the reflectivity magnitude and $\\arg(r)$ is the phase in radians.\n- The additive noise is $n(t) \\sim \\mathcal{CN}(0,\\sigma^2 I_M)$, where $\\sigma^2$ is the noise power (dimensionless) and $I_M$ is the $M \\times M$ identity matrix.\n\nUnder these assumptions, the received array snapshot is modeled by\n$$\nx(t) \\;=\\; s(t)\\,a(k_d,f) \\;+\\; s(t)\\,r\\,a(k_r,f) \\;+\\; n(t).\n$$\nDefine the ensemble covariance\n$$\nR(f) \\;=\\; \\mathbb{E}\\{x(t)\\,x(t)^{\\mathrm{H}}\\} \\;=\\; \\sigma^2 I_M \\;+\\; P_s\\,u(f)\\,u(f)^{\\mathrm{H}},\n$$\nwhere\n$$\nu(f) \\;=\\; a(k_d,f) \\;+\\; r\\,a(k_r,f).\n$$\nThe Capon (Minimum Variance Distortionless Response (MVDR)) spatial spectrum at a look wavenumber $k$ is defined by\n$$\n\\hat S_{\\mathrm{Capon}}(k,f) \\;=\\; \\frac{1}{a(k,f)^{\\mathrm{H}}\\,R(f)^{-1}\\,a(k,f)}.\n$$\nFor a known white-noise level $\\sigma^2$ and unnormalized steering vectors, a standard noise-corrected power estimate at $k_d$ is\n$$\n\\hat S(k_d,f) \\;=\\; \\hat S_{\\mathrm{Capon}}(k_d,f) \\;-\\; \\frac{\\sigma^2}{\\|a(k_d,f)\\|_2^2}.\n$$\nLet the target spatial-spectral density at the direct arrival be defined as\n$$\nS(k_d,f) \\;=\\; P_s.\n$$\nYour tasks:\n- Starting from the above definitions and the narrowband array data model, derive an explicit, closed-form expression for the bias\n$$\nB(k_d,f) \\;=\\; \\mathbb{E}[\\hat S(k_d,f)] \\;-\\; S(k_d,f),\n$$\nas a function of the reflectivity $r$, the array geometry encoded by $a(k_d,f)$ and $a(k_r,f)$, the source power $P_s$, and the noise power $\\sigma^2$. Express the final answer using only inner products and norms of the steering vectors and the parameters $P_s$, $\\sigma^2$, and $r$.\n- Then, implement a program that evaluates this bias for a uniform linear array (ULA) with inter-sensor spacing $d$ (in meters), sensor positions $p_m = m d$ for $m \\in \\{0,1,\\ldots,M-1\\}$, and steering vector\n$$\na(k,f) \\;=\\; \\begin{bmatrix}\ne^{j k p_0}  e^{j k p_1}  \\cdots  e^{j k p_{M-1}}\n\\end{bmatrix}^{\\mathrm{T}}.\n$$\nUse angles in radians and wavenumber in radians per meter. The program must compute the theoretical ensemble bias $B(k_d,f)$ you derived, without simulating snapshots.\n\nTest suite and parameter sets:\n- Use the following four cases. All angles are in radians, all distances in meters, and all wavenumbers in radians per meter. The answers are dimensionless.\n    1) Case $1$: $M = 8$, $d = 10$, $k_d = 0.12$, $k_r = 0.12$, $|r| = 0$, $\\arg(r) = 0$, $P_s = 1.0$, $\\sigma^2 = 0.1$.\n    2) Case $2$: $M = 8$, $d = 10$, $k_d = 0.12$, $k_r = 0.12$, $|r| = 0.5$, $\\arg(r) = \\pi/4$, $P_s = 1.0$, $\\sigma^2 = 0.1$.\n    3) Case $3$: $M = 12$, $d = 8$, $k_d = 0.15$, $k_r = 0.21$, $|r| = 0.3$, $\\arg(r) = 0$, $P_s = 1.0$, $\\sigma^2 = 0.05$.\n    4) Case $4$: $M = 10$, $d = 9$, $k_d = 0.09$, $k_r = k_d + \\frac{2\\pi}{M d}$, $|r| = 0.8$, $\\arg(r) = \\pi/2$, $P_s = 0.7$, $\\sigma^2 = 0.2$.\n\nProgram requirements:\n- Implement the derived closed-form bias function for a ULA as specified above.\n- Produce a single line of output containing a list of four floating-point numbers corresponding to the bias $B(k_d,f)$ for cases $1$ through $4$, in that order.\n- Format: one line exactly in the form \"[x1,x2,x3,x4]\" where each $x_i$ is a real number rounded to $6$ decimal places.\n\nNotes:\n- Ensure that your implementation uses the ensemble expression you derived, not a sample covariance estimate.\n- Clearly treat all angles in radians, and all wavenumbers in radians per meter.",
            "solution": "The objective is to derive a closed-form expression for the bias $B(k_d,f)$ of a noise-corrected Capon power estimator. The bias is defined as\n$$\nB(k_d,f) \\;=\\; \\mathbb{E}[\\hat S(k_d,f)] \\;-\\; S(k_d,f)\n$$\nThe problem is formulated using the ensemble covariance matrix $R(f)$, which is deterministic. Therefore, the estimator $\\hat S(k_d,f)$ is also a deterministic quantity, not a random variable. In this context, the expectation operator is superfluous, and we have $\\mathbb{E}[\\hat S(k_d,f)] = \\hat S(k_d,f)$. The problem thus simplifies to calculating the difference between the estimator's output and the target power:\n$$\nB(k_d,f) \\;=\\; \\hat S(k_d,f) \\;-\\; S(k_d,f)\n$$\nThe givens are:\n- Target power: $S(k_d,f) = P_s$.\n- Noise-corrected power estimate: $\\hat S(k_d,f) = \\hat S_{\\mathrm{Capon}}(k_d,f) - \\frac{\\sigma^2}{\\|a(k_d,f)\\|_2^2}$.\n- Capon spectrum: $\\hat S_{\\mathrm{Capon}}(k,f) = \\frac{1}{a(k,f)^{\\mathrm{H}}R(f)^{-1}a(k,f)}$.\n- Ensemble covariance matrix: $R(f) = \\sigma^2 I_M + P_s u(f) u(f)^{\\mathrm{H}}$, where $u(f) = a(k_d,f) + r a(k_r,f)$.\n\nFor notational conciseness, we will drop the explicit dependency on $f$ and use the shorthands $a_d = a(k_d,f)$, $a_r = a(k_r,f)$, and $R = R(f)$. The covariance matrix is a rank-$1$ update to a scaled identity matrix. Its inverse $R^{-1}$ can be found using the Sherman-Morrison-Woodbury matrix inversion lemma:\n$$\n(A + vw^{\\mathrm{H}})^{-1} = A^{-1} - \\frac{A^{-1}vw^{\\mathrm{H}}A^{-1}}{1 + w^{\\mathrm{H}}A^{-1}v}\n$$\nLet $A = \\sigma^2 I_M$, $v = P_s^{1/2} u$, and $w = P_s^{1/2} u$. Then $A^{-1} = \\frac{1}{\\sigma^2}I_M$. Applying the lemma:\n$$\nR^{-1} \\;=\\; \\left(\\sigma^2 I_M + P_s u u^{\\mathrm{H}}\\right)^{-1} \\;=\\; \\frac{1}{\\sigma^2} I_M - \\frac{\\frac{1}{\\sigma^4} P_s u u^{\\mathrm{H}}}{1 + \\frac{P_s}{\\sigma^2} u^{\\mathrm{H}} u} \\;=\\; \\frac{1}{\\sigma^2} \\left( I_M - \\frac{P_s u u^{\\mathrm{H}}}{\\sigma^2 + P_s \\|u\\|_2^2} \\right)\n$$\nNext, we compute the quadratic form in the denominator of the Capon spectrum definition, evaluated at the look wavenumber $k_d$:\n$$\na_d^{\\mathrm{H}} R^{-1} a_d \\;=\\; a_d^{\\mathrm{H}} \\left[ \\frac{1}{\\sigma^2} \\left( I_M - \\frac{P_s u u^{\\mathrm{H}}}{\\sigma^2 + P_s \\|u\\|_2^2} \\right) \\right] a_d \\;=\\; \\frac{1}{\\sigma^2} \\left( a_d^{\\mathrm{H}} a_d - \\frac{P_s a_d^{\\mathrm{H}} u u^{\\mathrm{H}} a_d}{\\sigma^2 + P_s \\|u\\|_2^2} \\right)\n$$\nThis simplifies to:\n$$\na_d^{\\mathrm{H}} R^{-1} a_d \\;=\\; \\frac{1}{\\sigma^2} \\left( \\|a_d\\|_2^2 - \\frac{P_s |a_d^{\\mathrm{H}} u|^2}{\\sigma^2 + P_s \\|u\\|_2^2} \\right) \\;=\\; \\frac{\\|a_d\\|_2^2(\\sigma^2 + P_s \\|u\\|_2^2) - P_s |a_d^{\\mathrm{H}} u|^2}{\\sigma^2 (\\sigma^2 + P_s \\|u\\|_2^2)}\n$$\n$$\na_d^{\\mathrm{H}} R^{-1} a_d \\;=\\; \\frac{\\sigma^2 \\|a_d\\|_2^2 + P_s (\\|a_d\\|_2^2 \\|u\\|_2^2 - |a_d^{\\mathrm{H}} u|^2)}{\\sigma^2 (\\sigma^2 + P_s \\|u\\|_2^2)}\n$$\nLet's simplify the term $\\|a_d\\|_2^2 \\|u\\|_2^2 - |a_d^{\\mathrm{H}} u|^2$. Substituting $u = a_d + r a_r$:\n$$\n\\|a_d\\|_2^2 \\|u\\|_2^2 - |a_d^{\\mathrm{H}} u|^2 = \\|a_d\\|_2^2 \\|a_d + r a_r\\|_2^2 - |a_d^{\\mathrm{H}}(a_d + r a_r)|^2\n$$\nUsing $\\|x+y\\|_2^2 = \\|x\\|_2^2+\\|y\\|_2^2+2\\mathrm{Re}(x^{\\mathrm{H}}y)$ and $|z_1+z_2|^2 = |z_1|^2+|z_2|^2+2\\mathrm{Re}(z_1^*z_2)$:\n$$\n= \\|a_d\\|_2^2 (\\|a_d\\|_2^2 + |r|^2 \\|a_r\\|_2^2 + 2\\mathrm{Re}(r^* a_d^{\\mathrm{H}} a_r)) - |\\|a_d\\|_2^2 + r a_d^{\\mathrm{H}} a_r|^2\n$$\n$$\n= \\|a_d\\|_2^4 + |r|^2 \\|a_d\\|_2^2 \\|a_r\\|_2^2 + 2\\|a_d\\|_2^2 \\mathrm{Re}(r^* a_d^{\\mathrm{H}} a_r) - (\\|a_d\\|_2^4 + |r a_d^{\\mathrm{H}} a_r|^2 + 2\\mathrm{Re}(\\|a_d\\|_2^2 r^* a_d^{\\mathrm{H}} a_r))\n$$\n$$\n= |r|^2 \\|a_d\\|_2^2 \\|a_r\\|_2^2 - |r|^2 |a_d^{\\mathrm{H}} a_r|^2 = |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)\n$$\nThe right-hand side is $|r|^2$ times a term related to the Cauchy-Schwarz inequality, which is zero only if $a_d$ and $a_r$ are collinear.\nSubstituting this back into $a_d^{\\mathrm{H}} R^{-1} a_d$:\n$$\na_d^{\\mathrm{H}} R^{-1} a_d = \\frac{\\sigma^2 \\|a_d\\|_2^2 + P_s |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)}{\\sigma^2 (\\sigma^2 + P_s \\|u\\|_2^2)}\n$$\nThe Capon power is the reciprocal of this expression:\n$$\n\\hat S_{\\mathrm{Capon}}(k_d,f) = \\frac{\\sigma^2 (\\sigma^2 + P_s \\|u\\|_2^2)}{\\sigma^2 \\|a_d\\|_2^2 + P_s |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)}\n$$\nThe noise-corrected estimate is $\\hat S(k_d,f) = \\hat S_{\\mathrm{Capon}}(k_d,f) - \\frac{\\sigma^2}{\\|a_d\\|_2^2}$. Placing this over a common denominator gives:\n$$\n\\hat S(k_d,f) = \\frac{\\sigma^2 \\|a_d\\|_2^2 (\\sigma^2 + P_s \\|u\\|_2^2) - \\sigma^2[\\sigma^2 \\|a_d\\|_2^2 + P_s |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)]}{\\|a_d\\|_2^2 [\\sigma^2 \\|a_d\\|_2^2 + P_s |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)]}\n$$\nThe numerator simplifies to:\n$$\n\\text{Num} = \\sigma^2 P_s \\left[ \\|a_d\\|_2^2 \\|u\\|_2^2 - |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2) \\right]\n$$\nSubstituting $\\|u\\|_2^2 = \\|a_d\\|_2^2 + |r|^2 \\|a_r\\|_2^2 + 2\\mathrm{Re}(r^* a_d^{\\mathrm{H}} a_r)$ inside the brackets:\n$$\n[\\dots] = \\|a_d\\|_2^2 (\\|a_d\\|_2^2 + |r|^2 \\|a_r\\|_2^2 + 2\\mathrm{Re}(r^* a_d^{\\mathrm{H}} a_r)) - |r|^2 \\|a_d\\|_2^2 \\|a_r\\|_2^2 + |r|^2 |a_d^{\\mathrm{H}} a_r|^2\n$$\n$$\n= \\|a_d\\|_2^4 + 2\\|a_d\\|_2^2 \\mathrm{Re}(r^* a_d^{\\mathrm{H}} a_r) + |r|^2 |a_d^{\\mathrm{H}} a_r|^2 = |\\|a_d\\|_2^2 + r a_d^{\\mathrm{H}} a_r|^2 = |a_d^{\\mathrm{H}} u|^2\n$$\nSo, the estimated power is:\n$$\n\\hat S(k_d,f) = \\frac{\\sigma^2 P_s |a_d^{\\mathrm{H}} u|^2}{\\|a_d\\|_2^2 \\left[\\sigma^2 \\|a_d\\|_2^2 + P_s |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)\\right]}\n$$\nThe bias is $B(k_d,f) = \\hat S(k_d,f) - P_s$.\n$$\nB(k_d,f) = P_s \\left( \\frac{\\sigma^2 |a_d^{\\mathrm{H}} u|^2}{\\|a_d\\|_2^2 \\left[\\sigma^2 \\|a_d\\|_2^2 + P_s |r|^2 (\\|a_d\\|_2^2 \\|a_r\\|_2^2 - |a_d^{\\mathrm{H}} a_r|^2)\\right]} - 1 \\right)\n$$\nwhere $u = a_d + r a_r$. We can expand $|a_d^{\\mathrm{H}}u|^2 = |\\|a_d\\|_2^2 + r(a_d^{\\mathrm{H}}a_r)|^2$. This is the final closed-form expression for the bias.\n\nFor a uniform linear array (ULA) with $M$ sensors, the steering vector elements are pure complex exponentials, so $\\|a(k,f)\\|_2^2 = M$. Thus, $\\|a_d\\|_2^2=M$ and $\\|a_r\\|_2^2=M$. The inner product is $\\rho_c = a_d^{\\mathrm{H}}a_r = \\sum_{m=0}^{M-1} e^{j(k_r-k_d)md}$. The bias expression for a ULA simplifies to:\n$$\nB(k_d,f) = P_s \\left( \\frac{\\sigma^2 |M + r \\rho_c|^2}{M \\left[\\sigma^2 M + P_s |r|^2 (M^2 - |\\rho_c|^2)\\right]} - 1 \\right)\n$$\nThis is the formula to be implemented.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the bias of a noise-corrected Capon estimator in the presence\n    of a single coherent multipath return for a uniform linear array.\n    \"\"\"\n\n    def calculate_bias(M, d, k_d, k_r, r_mag, r_arg, P_s, sigma_sq):\n        \"\"\"\n        Calculates the theoretical bias B(k_d, f) using the derived closed-form expression.\n        \n        The formula for a Uniform Linear Array (ULA) is:\n        B = P_s * ( (sigma_sq * |M + r*rho_c|^2) / (M * (sigma_sq*M + P_s*|r|^2*(M^2 - |rho_c|^2))) - 1 )\n        where rho_c = a_d^H * a_r.\n        \"\"\"\n        \n        # Calculate the complex reflection coefficient\n        r_val = r_mag * np.exp(1j * r_arg)\n\n        # For a ULA, the norm-squared of the steering vectors is M\n        norm_sq_a = M\n\n        # Calculate the inner product rho_c = a_d^H * a_r\n        # rho_c = sum_{m=0}^{M-1} exp(j*(k_r - k_d)*m*d)\n        m_arr = np.arange(M)\n        delta_k = k_r - k_d\n        rho_c = np.sum(np.exp(1j * delta_k * m_arr * d))\n\n        # Magnitudes squared\n        mag_sq_r = r_mag**2\n        mag_sq_rho_c = np.abs(rho_c)**2\n        \n        # Term |M + r*rho_c|^2 which is |a_d^H * u|^2 for a ULA\n        adh_u = norm_sq_a + r_val * rho_c\n        mag_sq_adh_u = np.abs(adh_u)**2\n        \n        # Numerator of the fraction inside the bias expression parentheses\n        numerator_frac = sigma_sq * mag_sq_adh_u\n        \n        # Denominator of the fraction\n        # The term (M^2 - |rho_c|^2) corresponds to (||a_d||^2 ||a_r||^2 - |a_d^H a_r|^2)\n        # for a ULA.\n        denominator_frac = norm_sq_a * (sigma_sq * norm_sq_a + P_s * mag_sq_r * (norm_sq_a**2 - mag_sq_rho_c))\n\n        # The bias is P_s * (fraction - 1)\n        bias = P_s * (numerator_frac / denominator_frac - 1.0)\n        \n        return bias\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # M, d, k_d, k_r, |r|, arg(r), P_s, sigma^2\n        (8, 10, 0.12, 0.12, 0.0, 0.0, 1.0, 0.1),\n        (8, 10, 0.12, 0.12, 0.5, np.pi/4, 1.0, 0.1),\n        (12, 8, 0.15, 0.21, 0.3, 0.0, 1.0, 0.05),\n        # For case 4, k_r is defined relative to k_d, M, and d\n        (10, 9, 0.09, 0.09 + (2 * np.pi) / (10 * 9), 0.8, np.pi/2, 0.7, 0.2)\n    ]\n\n    results = []\n    for params in test_cases:\n        M, d, k_d, k_r, r_mag, r_arg, P_s, sigma_sq = params\n        bias_val = calculate_bias(M, d, k_d, k_r, r_mag, r_arg, P_s, sigma_sq)\n        results.append(bias_val)\n\n    # Format the output as a list of floating-point numbers rounded to 6 decimal places.\n    print(f\"[{','.join(f'{res:.6f}' for res in results)}]\")\n\nsolve()\n```"
        }
    ]
}