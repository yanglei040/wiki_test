{
    "hands_on_practices": [
        {
            "introduction": "高斯过程代理模型的核心在于其预测方程，它融合了先验知识和观测数据。本练习将通过一个简单的双点训练集案例，引导您从第一性原理出发，推导高斯过程的预测均值和方差 。完成此推导将帮助您深入理解模型预测与不确定性量化的数学基础，为后续更复杂的应用打下坚实基础。",
            "id": "3615806",
            "problem": "一个地球物理正演模型将一个标量控制参数 $x \\in \\mathbb{R}$（例如，体积模量代理）映射到一个标量响应 $y \\in \\mathbb{R}$（例如，走时失配）。为了加速不确定性量化，我们采用一个零均值高斯过程 (GP) 代理模型：潜函数 $f$ 被建模为 $f \\sim \\mathcal{GP}(0, k)$，其中 $k(\\cdot,\\cdot)$ 是一个对称正定协方差核。观测值受到独立高斯噪声的干扰：对于 $i \\in \\{1,2\\}$，$y_i = f(x_i) + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ 且 $\\epsilon_1 \\perp \\epsilon_2$。给定 $2$ 个训练输入 $x_1,x_2$ 及其对应的输出 $y_1,y_2$，以及一个测试输入 $x_\\star$。令 $k_{11} = k(x_1,x_1)$、$k_{22} = k(x_2,x_2)$、$k_{12} = k(x_1,x_2)$、$k_{\\star 1} = k(x_\\star,x_1)$、$k_{\\star 2} = k(x_\\star,x_2)$ 和 $k_{\\star\\star} = k(x_\\star,x_\\star)$。假设 $k$ 是严格正定的且 $\\sigma_n^2 > 0$，从而所有需要的逆矩阵都存在。仅利用以下基本事实：(i) 联合高斯变量的仿射变换和边缘化产生高斯变量，以及 (ii) 对联合高斯分布进行条件化可得到条件均值和协方差的闭式表达式，推导出在 $x_\\star$ 处的高斯过程预测均值 $m_\\star$ 和预测方差 $s_\\star^2$ 关于 $k_{11}$、$k_{22}$、$k_{12}$、$k_{\\star 1}$、$k_{\\star 2}$、$k_{\\star\\star}$、$\\sigma_n^2$、$y_1$ 和 $y_2$ 的显式表达式。你的最终表达式必须是仅包含这些符号和标准算术运算的闭式标量公式，且不含任何矩阵的逆。请使用 pmatrix 环境，将你的最终答案表示为一个二元行矩阵 $\\big(m_\\star,\\; s_\\star^2\\big)$。无需进行数值计算或四舍五入。",
            "solution": "该问题是针对一个含 $2$ 个训练点的高斯过程 (GP) 回归的预测分布的标准推导。它具有科学依据，问题设定良好且客观。所有必要信息均已提供，约束条件明确。该问题是有效的。\n\n目标是求解在测试点 $x_\\star$ 处的潜函数值 $f_\\star = f(x_\\star)$ 的预测均值 $m_\\star$ 和预测方差 $s_\\star^2$，其条件是来自训练输入 $\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix}^T$ 的观测数据 $\\mathbf{y} = \\begin{pmatrix} y_1 & y_2 \\end{pmatrix}^T$。\n\nGP 回归的基础在于任何有限个函数值的集合都服从联合高斯分布。我们关心的是条件分布 $p(f_\\star | \\mathbf{y})$。我们首先建立测试点处的潜函数值 $f_\\star$ 和带噪声的观测值 $\\mathbf{y}$ 的联合分布。\n\n潜函数 $f$ 被建模为一个零均值高斯过程，$f \\sim \\mathcal{GP}(0, k)$。观测值由 $y_i = f(x_i) + \\epsilon_i$ 给出，其中噪声 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ 是独立同分布的高斯噪声。令 $\\mathbf{f} = \\begin{pmatrix} f(x_1) & f(x_2) \\end{pmatrix}^T$。包含训练点和测试点潜函数值的向量 $\\begin{pmatrix} \\mathbf{f}^T & f_\\star \\end{pmatrix}^T$ 服从一个多元高斯分布：\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^T & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\n其中 $K = \\begin{pmatrix} k(x_1, x_1) & k(x_1, x_2) \\\\ k(x_2, x_1) & k(x_2, x_2) \\end{pmatrix} = \\begin{pmatrix} k_{11} & k_{12} \\\\ k_{12} & k_{22} \\end{pmatrix}$，$\\mathbf{k}_\\star = \\begin{pmatrix} k(x_\\star, x_1) \\\\ k(x_\\star, x_2) \\end{pmatrix} = \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$，且 $k_{\\star\\star} = k(x_\\star, x_\\star)$。\n\n观测向量为 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$，其中 $\\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1 & \\epsilon_2 \\end{pmatrix}^T$。噪声向量 $\\boldsymbol{\\epsilon}$ 也是高斯的，$\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。由于 $\\mathbf{f}$ 和 $\\boldsymbol{\\epsilon}$ 是独立的，它们的和 $\\mathbf{y}$ 也是高斯的。\n\n我们现在构建观测值 $\\mathbf{y}$ 和测试潜函数值 $f_\\star$ 的联合分布。\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{joint}}, \\Sigma_{\\text{joint}})\n$$\n均值为 $\\boldsymbol{\\mu}_{\\text{joint}} = E\\left[\\begin{pmatrix} \\mathbf{f} + \\boldsymbol{\\epsilon} \\\\ f_\\star \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f}] + E[\\boldsymbol{\\epsilon}] \\\\ E[f_\\star] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}$。\n\n协方差矩阵 $\\Sigma_{\\text{joint}}$ 是一个分块矩阵：\n$$\n\\Sigma_{\\text{joint}} = \\begin{pmatrix} \\text{Cov}(\\mathbf{y}, \\mathbf{y}) & \\text{Cov}(\\mathbf{y}, f_\\star) \\\\ \\text{Cov}(f_\\star, \\mathbf{y}) & \\text{Cov}(f_\\star, f_\\star) \\end{pmatrix}\n$$\n这些分块计算如下：\n$\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = E[\\mathbf{y}\\mathbf{y}^T] = E[(\\mathbf{f} + \\boldsymbol{\\epsilon})(\\mathbf{f} + \\boldsymbol{\\epsilon})^T] = E[\\mathbf{f}\\mathbf{f}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = K + \\sigma_n^2 I$。\n$\\text{Cov}(\\mathbf{y}, f_\\star) = E[\\mathbf{y}f_\\star] = E[(\\mathbf{f} + \\boldsymbol{\\epsilon})f_\\star] = E[\\mathbf{f}f_\\star] = \\mathbf{k}_\\star$。\n$\\text{Cov}(f_\\star, f_\\star) = E[f_\\star^2] = k_{\\star\\star}$。\n所以联合分布是：\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma_n^2 I & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^T & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\n我们寻求给定 $\\mathbf{y}$ 时 $f_\\star$ 的条件分布。对于一个一般的已分块高斯变量 $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix}\\right)$，给定 $\\mathbf{a}$ 时 $\\mathbf{b}$ 的条件分布是 $\\mathbf{b} | \\mathbf{a} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$，其中：\n$\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba}\\Sigma_{aa}^{-1}(\\mathbf{a} - \\boldsymbol{\\mu}_a)$\n$\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}$\n\n将此应用于我们的问题，通过替换 $\\mathbf{a} \\to \\mathbf{y}$，$\\mathbf{b} \\to f_\\star$ 以及相应的均值和协方差分块：\n$m_\\star = 0 + \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} (\\mathbf{y} - \\mathbf{0}) = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}$\n$s_\\star^2 = k_{\\star\\star} - \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_\\star$\n\n问题要求一个不含矩阵逆的显式标量公式。我们必须计算 $2 \\times 2$ 矩阵 $K + \\sigma_n^2 I$ 的逆。\n令 $A = K + \\sigma_n^2 I = \\begin{pmatrix} k_{11} + \\sigma_n^2 & k_{12} \\\\ k_{12} & k_{22} + \\sigma_n^2 \\end{pmatrix}$。\n$A$ 的行列式是：\n$\\det(A) = (k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2$。\n$A$ 的逆是：\n$A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} k_{22} + \\sigma_n^2 & -k_{12} \\\\ -k_{12} & k_{11} + \\sigma_n^2 \\end{pmatrix}$。\n\n现在，我们将其代入 $m_\\star$ 和 $s_\\star^2$ 的表达式中。\n对于预测均值 $m_\\star$：\n$m_\\star = \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} A^{-1} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$\n$m_\\star = \\frac{1}{\\det(A)} \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} \\begin{pmatrix} k_{22} + \\sigma_n^2 & -k_{12} \\\\ -k_{12} & k_{11} + \\sigma_n^2 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$\n$m_\\star = \\frac{1}{\\det(A)} \\left[ (k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12})y_1 + (-k_{\\star 1}k_{12} + k_{\\star 2}(k_{11} + \\sigma_n^2))y_2 \\right]$\n重新整理括号中的项，得到：\n$m_\\star = \\frac{(k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12})y_1 + (k_{\\star 2}(k_{11} + \\sigma_n^2) - k_{\\star 1}k_{12})y_2}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2}$\n\n对于预测方差 $s_\\star^2$：\n$s_\\star^2 = k_{\\star\\star} - \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} A^{-1} \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$\n$s_\\star^2 = k_{\\star\\star} - \\frac{1}{\\det(A)} \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} \\begin{pmatrix} k_{22} + \\sigma_n^2 & -k_{12} \\\\ -k_{12} & k_{11} + \\sigma_n^2 \\end{pmatrix} \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$\n分子中的二次型是：\n$k_{\\star 1} ( (k_{22} + \\sigma_n^2)k_{\\star 1} - k_{12}k_{\\star 2} ) + k_{\\star 2} ( -k_{12}k_{\\star 1} + (k_{11} + \\sigma_n^2)k_{\\star 2} )$\n$= k_{\\star 1}^2(k_{22} + \\sigma_n^2) - k_{\\star 1}k_{12}k_{\\star 2} - k_{\\star 2}k_{12}k_{\\star 1} + k_{\\star 2}^2(k_{11} + \\sigma_n^2)$\n$= k_{\\star 1}^2(k_{22} + \\sigma_n^2) + k_{\\star 2}^2(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}$\n因此，预测方差是：\n$s_\\star^2 = k_{\\star\\star} - \\frac{k_{\\star 1}^2(k_{22} + \\sigma_n^2) + k_{\\star 2}^2(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2}$\n这些就是预测均值和方差的最终显式标量公式。",
            "answer": "$$\n\\boxed{\\begin{pmatrix}\n\\frac{\\left(k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12}\\right)y_1 + \\left(k_{\\star 2}(k_{11} + \\sigma_n^2) - k_{\\star 1}k_{12}\\right)y_2}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^{2}} & k_{\\star\\star} - \\frac{k_{\\star 1}^{2}(k_{22} + \\sigma_n^2) + k_{\\star 2}^{2}(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^{2}}\n\\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在地球物理学建模中，来自模拟器或现场测量的数据通常带有非均匀（异方差）噪声，这给不确定性量化带来了挑战。本实践将指导您编写代码，从重复的模拟数据中估计局部噪声方差，并构建一个能够处理异方差的GP模型 。通过将该模型与标准的同方差模型进行比较，您将掌握一种提升代理模型真实性的关键技术，并理解精确噪声建模对于可靠不确定性评估的重要性。",
            "id": "3615819",
            "problem": "考虑一个一维正向响应，其被建模为一个潜函数 $f(x)$，该函数被赋予一个零均值和平稳平方指数（径向基函数）协方差核的高斯过程（GP）先验。观测是带有噪声的模拟器输出，其噪声与输入相关，并且有几个输入具有重复运行。目标是从重复观测中估计局部噪声方差，将它们整合到一个异方差高斯过程中，并评估其相对于朴素同方差高斯过程对预测方差和边际似然的影响。\n\n基本原理：\n- 高斯过程（GP）先验指定任何有限的潜值集合 $(f(x_1),\\dots,f(x_n))$ 均为多元高斯分布，其均值向量为 $\\mathbf{0}$，协方差矩阵由一个正定核 $k(\\cdot,\\cdot)$ 决定。\n- 带噪声的观测模型为 $y_i = f(x_i) + \\varepsilon_i$，其中噪声 $\\varepsilon_i$ 在索引 $i$ 上是独立的，并遵循均值为零、方差为 $\\sigma^2(x_i)$ 的高斯分布。\n- 当观测协方差包括核协方差和噪声协方差时，对多元正态分布进行条件化，可以得到给定观测数据时测试输入处潜函数的后验分布。\n\n模型设定：\n- 核函数：$k(x,x') = \\sigma_f^2 \\exp\\!\\left(-\\dfrac{(x-x')^2}{2\\ell^2}\\right)$，其中超参数固定为 $\\ell = 0.25$ 和 $\\sigma_f^2 = 1.0$。\n- 先验均值在所有位置均为零。\n- 未指定物理单位；所有量均为无量纲。\n\n训练数据：\n- 唯一输入及其重复观测值：\n  - $x = 0.0$，观测值为 $[0.56,\\,0.49,\\,0.61]$。\n  - $x = 0.2$，观测值为 $[1.30,\\,1.36,\\,1.29,\\,1.35,\\,1.33]$。\n  - $x = 0.5$，观测值为 $[0.03,\\,-0.02]$。\n  - $x = 0.8$，观测值为 $[-1.27,\\,-1.38,\\,-1.36,\\,-1.31]$。\n  - $x = 1.1$，观测值为 $[0.09]$。\n\n从重复观测中估计噪声方差：\n- 对于一个具有 $m \\ge 2$ 个重复观测 $\\{y_j\\}_{j=1}^m$ 的位置 $x$，通过无偏样本方差来估计局部噪声方差 $\\hat{\\sigma}^2(x) = \\dfrac{1}{m-1}\\sum_{j=1}^m \\left(y_j - \\bar{y}\\right)^2$，其中 $\\bar{y} = \\dfrac{1}{m}\\sum_{j=1}^m y_j$。\n- 对于一个具有 $m \\le 1$ 个重复观测的位置，使用在所有至少有两个重复观测的位置上计算的合并方差：$\\hat{\\sigma}^2_{\\mathrm{pool}} = \\dfrac{\\sum_{i} \\sum_{j=1}^{m_i} (y_{ij} - \\bar{y}_i)^2}{\\sum_i (m_i - 1)}$，其中求和仅针对那些 $m_i \\ge 2$ 的位置。\n\n异方差高斯过程：\n- 构建观测值的协方差矩阵为 $C = K + \\Sigma$，其中 $K$ 是由所有重复输入构成的核矩阵，$\\Sigma$ 是一个对角矩阵，其对角线元素是对应于 $x$ 处每个重复观测的局部噪声方差 $\\hat{\\sigma}^2(x)$。\n\n同方差高斯过程基准：\n- 构建协方差矩阵为 $C_{\\mathrm{homo}} = K + \\hat{\\sigma}^2_{\\mathrm{pool}} I$，其中 $I$ 是维度匹配的单位矩阵。\n\n后验预测方差：\n- 对于一个测试输入 $x_\\star$，通过对 $(\\mathbf{y}, f(x_\\star))$ 的联合高斯分布进行条件化，计算潜函数 $f(x_\\star)$ 在异方差和同方差模型下的预测方差；异方差情况下的观测协方差为 $C$，同方差情况下为 $C_{\\mathrm{homo}}$。返回的方差必须是潜函数 $f(x_\\star)$ 的方差，不包括任何观测噪声。\n\n对数边际似然：\n- 对于观测数据向量 $\\mathbf{y}$，在零均值高斯过程和观测协方差为 $C$ 的情况下，对数边际似然为 $\\log p(\\mathbf{y}) = -\\dfrac{1}{2}\\mathbf{y}^\\top C^{-1}\\mathbf{y} - \\dfrac{1}{2}\\log\\det C - \\dfrac{n}{2}\\log(2\\pi)$，其中 $n$ 是观测总数。\n\n要实现和测试的任务：\n- 使用上述训练数据和固定的核超参数 $\\ell = 0.25$, $\\sigma_f^2 = 1.0$。\n- 根据上述规则，在每个唯一输入处估计局部噪声方差，并在需要时使用合并方差作为备用。\n- 构建所述的异方差和同方差高斯过程模型。\n- 计算以下量（测试套件）：\n  1. $x = 0.2$ 处的估计局部噪声方差，以浮点数形式表示。\n  2. 潜函数 $f$ 在 $x_\\star = 0.2$ 处的异方差预测方差，以浮点数形式表示。\n  3. 潜函数 $f$ 在 $x_\\star = 1.8$ 处的异方差预测方差，以浮点数形式表示。\n  4. 在 $x_\\star = 0.2$ 处，同方差预测方差与异方差预测方差之比，即 $\\dfrac{\\mathrm{Var}_{\\mathrm{homo}}[f(x_\\star)]}{\\mathrm{Var}_{\\mathrm{hetero}}[f(x_\\star)]}$，以浮点数形式表示。\n  5. 负对数边际似然之差 $\\mathrm{NLL}_{\\mathrm{homo}} - \\mathrm{NLL}_{\\mathrm{hetero}}$，其中 $\\mathrm{NLL} = -\\log p(\\mathbf{y})$，以浮点数形式表示。\n\n数值细节：\n- 在用于线性代数的所有协方差矩阵的对角线上添加一个小的正抖动 $\\epsilon = 10^{-10}$，以确保数值稳定性。\n- 使用基于 Cholesky 的线性代数来求解系统和计算对数行列式。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含按上述五项顺序排列的结果，以逗号分隔并用方括号括起。例如，输出必须是 $[r_1,r_2,r_3,r_4,r_5]$ 的形式。",
            "solution": "我们从高斯过程（GP）先验和高斯观测噪声的定义开始。一个零均值和协方差函数为 $k(\\cdot,\\cdot)$ 的高斯过程先验意味着，对于任何有限输入集 $\\{x_i\\}_{i=1}^n$，潜向量 $\\mathbf{f} = [f(x_1),\\dots,f(x_n)]^\\top$ 服从多元正态分布 $\\mathcal{N}(\\mathbf{0}, K)$，其中 $K_{ij} = k(x_i, x_j)$。观测值服从 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2(x_i))$ 在索引 $i$ 上是独立的。\n\n对于与输入相关的（异方差）噪声方差，我们构建观测协方差矩阵为 $C = K + \\Sigma$，其中 $\\Sigma$ 是对角矩阵，其元素为 $\\Sigma_{ii} = \\sigma^2(x_i)$。在此基础上，对于单个测试输入 $x_\\star$，$\\mathbf{y}$ 和 $f(x_\\star)$ 的联合分布是高斯的：\n\n$$\n\\begin{bmatrix}\n\\mathbf{y} \\\\\nf(x_\\star)\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\n\\left(\n\\begin{bmatrix}\n\\mathbf{0} \\\\\n0\n\\end{bmatrix},\n\\begin{bmatrix}\nC & \\mathbf{k}_\\star \\\\\n\\mathbf{k}_\\star^\\top & k_{\\star \\star}\n\\end{bmatrix}\n\\right),\n$$\n\n其中 $\\mathbf{k}_\\star$ 是一个 $n \\times 1$ 的向量，其元素为 $k(x_i, x_\\star)$，并且 $k_{\\star \\star} = k(x_\\star, x_\\star)$。\n\n通过对多元正态分布进行条件化，给定 $\\mathbf{y}$ 时 $x_\\star$ 处潜函数的后验分布是高斯的，其均值和方差为：\n\n$$\n\\mathbb{E}[f(x_\\star)\\mid \\mathbf{y}] = \\mathbf{k}_\\star^\\top C^{-1} \\mathbf{y}, \\quad\n\\mathrm{Var}[f(x_\\star)\\mid \\mathbf{y}] = k_{\\star \\star} - \\mathbf{k}_\\star^\\top C^{-1} \\mathbf{k}_\\star.\n$$\n\n这些公式源于条件高斯分布的 Schur 补恒等式。\n\n对于同方差基准，我们将 $\\Sigma$ 替换为 $\\sigma_{\\mathrm{pool}}^2 I$，其中 $\\sigma_{\\mathrm{pool}}^2$ 是在至少有两个重复观测的组上估计的合并方差。这产生了 $C_{\\mathrm{homo}} = K + \\sigma_{\\mathrm{pool}}^2 I$。\n\n从重复观测中估计噪声方差的过程如下。对于每个具有 $m \\ge 2$ 个重复观测 $\\{y_j\\}_{j=1}^m$ 的唯一输入 $x$，我们使用无偏样本方差：\n\n$$\n\\hat{\\sigma}^2(x) = \\frac{1}{m-1} \\sum_{j=1}^m (y_j - \\bar{y})^2,\\quad \\bar{y} = \\frac{1}{m}\\sum_{j=1}^m y_j.\n$$\n\n对于 $m \\le 1$ 的位置，我们使用在所有至少有两个重复观测的位置上计算的合并方差：\n\n$$\n\\hat{\\sigma}^2_{\\mathrm{pool}} = \\frac{\\sum_i \\sum_{j=1}^{m_i} (y_{ij} - \\bar{y}_i)^2}{\\sum_i (m_i - 1)},\n$$\n\n其中求和仅包括 $m_i \\ge 2$ 的位置，$\\bar{y}_i$ 是位置 $x_i$ 处的样本均值。在组内残差为高斯分布的假设下，这个合并估计量是公共方差的最大似然估计。\n\n核函数是平方指数（径向基函数）核：\n\n$$\nk(x, x') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(x - x')^2}{2\\ell^2}\\right),\n$$\n\n其超参数固定为 $\\ell = 0.25$ 和 $\\sigma_f^2 = 1.0$。为保证数值稳定性和正定性，我们在分解前向 $C$（和 $C_{\\mathrm{homo}}$）的对角线添加一个小的抖动 $\\epsilon = 10^{-10}$。\n\n为了高效地计算预测方差，我们使用 Cholesky 分解。设 $C = L L^\\top$ 是 $C$ 的 Cholesky 分解。那么 $C^{-1}\\mathbf{k}_\\star$ 可以通过求解三角系统 $L \\mathbf{z} = \\mathbf{k}_\\star$ 和 $L^\\top \\mathbf{w} = \\mathbf{z}$ 来计算，从而得到 $\\mathbf{w} = C^{-1}\\mathbf{k}_\\star$。预测方差为 $k_{\\star \\star} - \\mathbf{k}_\\star^\\top \\mathbf{w}$。同样的过程也适用于 $C_{\\mathrm{homo}}$ 以获得同方差预测方差。\n\n对于零均值和协方差 $C$，对数边际似然为：\n\n$$\n\\log p(\\mathbf{y}) = -\\frac{1}{2}\\mathbf{y}^\\top C^{-1}\\mathbf{y} - \\frac{1}{2}\\log\\det C - \\frac{n}{2}\\log(2\\pi),\n$$\n\n其中 $n$ 等于观测总数（包括重复观测）。使用 Cholesky 因子 $L$，我们有 $\\log\\det C = 2 \\sum_{i=1}^n \\log L_{ii}$，并且 $\\mathbf{y}^\\top C^{-1}\\mathbf{y}$ 的计算方式与预测方差的计算类似，通过两次三角求解完成。负对数边际似然为 $\\mathrm{NLL} = -\\log p(\\mathbf{y})$。\n\n算法步骤：\n1. 通过根据重复次数重复每个唯一输入，组合成长度为 $n$ 的完整训练输入向量 $\\mathbf{x}$，以及相应的观测向量 $\\mathbf{y}$。\n2. 对于每个唯一输入，如果 $m \\ge 2$，则计算 $\\hat{\\sigma}^2(x)$。累加残差平方和与自由度以计算 $\\hat{\\sigma}^2_{\\mathrm{pool}}$。\n3. 为每个重复观测分配噪声方差：当可用时使用局部的 $\\hat{\\sigma}^2(x)$；否则使用 $\\hat{\\sigma}^2_{\\mathrm{pool}}$。\n4. 使用在 $\\mathbf{x}$ 上的核函数构建 $K$，并形成 $C = K + \\Sigma + \\epsilon I$ 和 $C_{\\mathrm{homo}} = K + \\hat{\\sigma}^2_{\\mathrm{pool}} I + \\epsilon I$。\n5. 使用基于 Cholesky 的求解方法，计算：\n   - 在 $x_\\star = 0.2$ 和 $x_\\star = 1.8$ 处的异方差预测方差。\n   - 在 $x_\\star = 0.2$ 处的同方差预测方差。\n   - 比率 $\\dfrac{\\mathrm{Var}_{\\mathrm{homo}}[f(0.2)]}{\\mathrm{Var}_{\\mathrm{hetero}}[f(0.2)]}$。\n6. 使用 $C$ 计算 $\\mathrm{NLL}_{\\mathrm{hetero}}$，使用 $C_{\\mathrm{homo}}$ 计算 $\\mathrm{NLL}_{\\mathrm{homo}}$，然后计算差值 $\\mathrm{NLL}_{\\mathrm{homo}} - \\mathrm{NLL}_{\\mathrm{hetero}}$。\n7. 按指定的顺序和格式报告所要求的五个量。\n\n这些步骤遵循了高斯过程和多元正态条件化的基本属性，整合了基于重复观测的噪声估计以减轻预测不确定性中的偏差，并为不确定性量化提供了异方差和同方差建模之间的直接比较。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef rbf_kernel(xa: np.ndarray, xb: np.ndarray, length_scale: float, sigma_f2: float) -> np.ndarray:\n    \"\"\"\n    Squared-exponential (RBF) kernel matrix between vectors xa and xb.\n    \"\"\"\n    xa = xa[:, None]\n    xb = xb[None, :]\n    d2 = (xa - xb) ** 2\n    return sigma_f2 * np.exp(-0.5 * d2 / (length_scale ** 2))\n\ndef unbiased_sample_variance(y: np.ndarray) -> float:\n    \"\"\"\n    Unbiased sample variance for a 1D array y with length m >= 2.\n    \"\"\"\n    m = y.size\n    mean = y.mean()\n    return np.sum((y - mean) ** 2) / (m - 1)\n\ndef compute_pooled_variance(groups: list) -> float:\n    \"\"\"\n    Compute pooled variance across groups with at least two replicates.\n    Each element in groups is a numpy array of observations at a unique input.\n    \"\"\"\n    ss_total = 0.0\n    df_total = 0\n    for g in groups:\n        m = g.size\n        if m >= 2:\n            mean = g.mean()\n            ss = np.sum((g - mean) ** 2)\n            ss_total += ss\n            df_total += (m - 1)\n    if df_total == 0:\n        raise ValueError(\"No groups with at least two replicates to compute pooled variance.\")\n    return ss_total / df_total\n\ndef gp_predictive_variance(x_train: np.ndarray, noise_var: np.ndarray, x_star: float,\n                           length_scale: float, sigma_f2: float, jitter: float = 1e-10) -> float:\n    \"\"\"\n    Compute the predictive variance of latent f(x_star) using heteroscedastic noise variances.\n    \"\"\"\n    K = rbf_kernel(x_train, x_train, length_scale, sigma_f2)\n    C = K + np.diag(noise_var) + jitter * np.eye(x_train.size)\n    c_factor = cho_factor(C, lower=False, check_finite=False, overwrite_a=False)\n    k_star = rbf_kernel(x_train, np.array([x_star]), length_scale, sigma_f2).reshape(-1)\n    v = cho_solve(c_factor, k_star, check_finite=False, overwrite_b=False)\n    k_ss = sigma_f2  # since kernel evaluated at (x_star, x_star)\n    var = float(k_ss - k_star @ v)\n    # Numerical guard for tiny negative due to floating error\n    return max(var, 0.0)\n\ndef gp_nll(x_train: np.ndarray, noise_var: np.ndarray, y_train: np.ndarray,\n           length_scale: float, sigma_f2: float, jitter: float = 1e-10) -> float:\n    \"\"\"\n    Compute negative log marginal likelihood for zero-mean GP with given noise variances.\n    \"\"\"\n    K = rbf_kernel(x_train, x_train, length_scale, sigma_f2)\n    C = K + np.diag(noise_var) + jitter * np.eye(x_train.size)\n    c_factor = cho_factor(C, lower=False, check_finite=False, overwrite_a=False)\n    alpha = cho_solve(c_factor, y_train, check_finite=False, overwrite_b=False)\n    # log determinant from Cholesky: logdet(C) = 2 * sum(log(diag(R))) for upper-triangular R\n    R = c_factor[0]\n    logdet = 2.0 * np.sum(np.log(np.diag(R)))\n    n = y_train.size\n    quad = float(y_train @ alpha)\n    log2pi = np.log(2.0 * np.pi)\n    log_marg = -0.5 * quad - 0.5 * logdet - 0.5 * n * log2pi\n    return -log_marg\n\ndef solve():\n    # Define unique inputs and their replicate observations.\n    unique_x = np.array([0.0, 0.2, 0.5, 0.8, 1.1], dtype=float)\n    groups = [\n        np.array([0.56, 0.49, 0.61], dtype=float),                     # x = 0.0\n        np.array([1.30, 1.36, 1.29, 1.35, 1.33], dtype=float),         # x = 0.2\n        np.array([0.03, -0.02], dtype=float),                          # x = 0.5\n        np.array([-1.27, -1.38, -1.36, -1.31], dtype=float),           # x = 0.8\n        np.array([0.09], dtype=float),                                 # x = 1.1\n    ]\n\n    # Kernel hyperparameters (fixed)\n    length_scale = 0.25\n    sigma_f2 = 1.0\n    jitter = 1e-10\n\n    # Build full training arrays by repeating inputs per replicate and stacking observations.\n    x_train_list = []\n    y_train_list = []\n    for x, g in zip(unique_x, groups):\n        x_train_list.extend([x] * g.size)\n        y_train_list.extend(g.tolist())\n    x_train = np.array(x_train_list, dtype=float)\n    y_train = np.array(y_train_list, dtype=float)\n\n    # Estimate local noise variances where possible, and pooled variance for fallback.\n    local_var = {}\n    for x, g in zip(unique_x, groups):\n        if g.size >= 2:\n            local_var[x] = unbiased_sample_variance(g)\n        else:\n            local_var[x] = np.nan  # placeholder; will be replaced by pooled variance\n\n    pooled_var = compute_pooled_variance(groups)\n\n    # Construct per-observation noise variances for heteroscedastic model\n    hetero_noise = np.empty_like(y_train)\n    idx = 0\n    for x, g in zip(unique_x, groups):\n        var_x = local_var[x]\n        if not np.isfinite(var_x):\n            var_x = pooled_var\n        hetero_noise[idx:idx + g.size] = var_x\n        idx += g.size\n\n    # Construct per-observation noise variances for homoscedastic model\n    homo_noise = np.full_like(y_train, pooled_var)\n\n    # Test suite computations:\n    # 1) Estimated local noise variance at x = 0.2\n    var_at_02 = float(local_var[0.2])\n\n    # 2) Heteroscedastic predictive variance at x* = 0.2\n    var_hetero_at_02 = gp_predictive_variance(x_train, hetero_noise, 0.2, length_scale, sigma_f2, jitter)\n\n    # 3) Heteroscedastic predictive variance at x* = 1.8\n    var_hetero_at_18 = gp_predictive_variance(x_train, hetero_noise, 1.8, length_scale, sigma_f2, jitter)\n\n    # 4) Ratio Var_homo / Var_hetero at x* = 0.2\n    var_homo_at_02 = gp_predictive_variance(x_train, homo_noise, 0.2, length_scale, sigma_f2, jitter)\n    ratio_homo_to_hetero_at_02 = float(var_homo_at_02 / var_hetero_at_02) if var_hetero_at_02 > 0 else float('inf')\n\n    # 5) Difference in negative log marginal likelihoods: NLL_homo - NLL_hetero\n    nll_hetero = gp_nll(x_train, hetero_noise, y_train, length_scale, sigma_f2, jitter)\n    nll_homo = gp_nll(x_train, homo_noise, y_train, length_scale, sigma_f2, jitter)\n    delta_nll = float(nll_homo - nll_hetero)\n\n    results = [var_at_02, var_hetero_at_02, var_hetero_at_18, ratio_homo_to_hetero_at_02, delta_nll]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "传统的GP模型假设函数具有平滑性，难以准确捕捉地球物理系统中常见的突变，例如地层相变边界。本高级编程练习将介绍一种强大的“单位分解”方法，即为不同地质相（facies）分别训练局部GP模型，再将其平滑地融合起来 。通过构建这种“感知相变”的混合模型，您将学会如何将领域知识融入GP框架，从而有效处理非平稳和不连续问题，显著提升模型在复杂地质边界附近的预测精度。",
            "id": "3615847",
            "problem": "考虑一个计算地球物理学中的一维正演模拟场景，其中地下被位于 $x = 0$ 处的地层边界划分为两个地质相。输入坐标为 $x \\in [-1,1]$（无量纲），正演响应是一个标量函数 $f(x)$（无量纲）。这两个相由区域 $x  0$ 和 $x \\ge 0$ 定义。您的任务是为每个相构建局部高斯过程（GP）代理模型，并通过单位分解方案将它们融合。您将把融合的相感知代理模型与一个忽略相信息的单体高斯过程进行比较，并评估相感知能力是否减少了跨地层边界的外推误差。\n\n请仅使用以下基本定义和公认事实作为您推导和实现的基础：\n\n- 高斯过程（GP）是随机变量的集合，其中任意有限个数的随机变量都服从联合高斯分布。函数 $f(\\cdot)$ 上的一个高斯过程先验表示为 $f \\sim \\mathcal{GP}(m(\\cdot), k(\\cdot,\\cdot))$，其中 $m(\\cdot)$ 是均值函数，$k(\\cdot,\\cdot)$ 是协方差函数。\n- 对于一个协方差为 $k(\\cdot,\\cdot)$ 的零均值高斯过程先验，给定观测到的训练输入 $\\mathbf{X} \\in \\mathbb{R}^{N \\times 1}$ 和输出 $\\mathbf{y} \\in \\mathbb{R}^{N}$，并带有方差为 $\\sigma_n^2$ 的独立高斯噪声，在测试输入 $\\mathbf{X}_\\star$ 处的后验预测均值由下式给出\n$$\n\\mathbf{m}_\\star \\;=\\; \\mathbf{K}_{\\star N} \\left(\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y},\n$$\n其中 $\\mathbf{K}_{N N}$ 是 $N \\times N$ 的格拉姆矩阵，其元素为 $[\\mathbf{K}_{N N}]_{ij} = k(x_i, x_j)$，$\\mathbf{K}_{\\star N}$ 是 $N_\\star \\times N$ 的互协方差矩阵，其元素为 $[\\mathbf{K}_{\\star N}]_{ij} = k(x^\\star_i, x_j)$。\n- 一种常用的平稳协方差是平方指数（径向基函数）核函数\n$$\nk_\\text{RBF}(x, x'; \\ell, \\sigma_f^2) \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right),\n$$\n其中长度尺度 $\\ell  0$，信号方差 $\\sigma_f^2  0$。\n- 单位分解是一组非负权重函数 $\\{ w_i(x) \\}_{i=1}^M$，对于所有 $x$ 满足 $\\sum_{i=1}^M w_i(x) = 1$。\n\n您将在一个程序中实现以下内容：\n\n- 定义一个依赖于相的正演模型。令基函数为 $b(x) = \\sin(6 x) - 0.1 x$。定义相响应函数\n$$\nf_0(x) \\;=\\; b(x) \\quad \\text{for} \\; x  0,\n$$\n$$\nf_1(x; J, S) \\;=\\; (1 - S)\\, b(x) + S\\,\\big(0.8 \\cos(6 x + 0.5) + 0.1 x\\big) + J \\quad \\text{for} \\; x \\ge 0,\n$$\n其中 $J \\in \\mathbb{R}$ 控制边界上的偏移跳跃，$S \\in \\{0,1\\}$ 切换相之间的形状差异。真实的正演模型是\n$$\nf(x; J, S) \\;=\\; \\begin{cases}\nf_0(x),  x  0,\\\\\nf_1(x; J, S),  x \\ge 0.\n\\end{cases}\n$$\n- 为每个相生成训练输入。对相 0 使用在 $[-1, -0.02]$ 上均匀分布的 $N_0 = 30$ 个点，对相 1 使用在 $[0.02, 1]$ 上均匀分布的 $N_1 = 30$ 个点。在这些输入点上计算来自 $f(x; J, S)$ 的无噪声训练输出。仅在高斯过程线性代数中为了数值稳定性，使用一个小的数值扰动项 $\\sigma_n^2 = 10^{-8}$。\n- 构建两个局部高斯过程代理模型，每个相一个，使用平方指数核函数，长度尺度可能不同（$\\ell_0$ 和 $\\ell_1$），但信号方差相同，为 $\\sigma_f^2 = 1$。对于忽略相信息的单体高斯过程，使用单一核函数，其长度尺度为 $\\ell_g$，信号方差为 $\\sigma_f^2 = 1$，并在所有汇集在一起的训练数据上进行训练。\n- 使用逻辑斯蒂权重在边界 $x_b = 0$ 处定义一个平滑的单位分解融合\n$$\nw_1(x;\\beta) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-\\beta\\,(x - x_b)\\big)}, \\qquad\nw_0(x;\\beta) \\;=\\; 1 - w_1(x;\\beta),\n$$\n其中锐度参数 $\\beta  0$。在测试输入 $x$ 处的相感知融合预测均值为\n$$\n\\hat{f}_\\text{blend}(x) \\;=\\; w_0(x;\\beta)\\,\\hat{f}_0(x) \\;+\\; w_1(x;\\beta)\\,\\hat{f}_1(x),\n$$\n其中 $\\hat{f}_0$ 和 $\\hat{f}_1$ 分别是相 0 和相 1 的局部高斯过程后验均值。\n- 定义一个由 $[-0.15, 0.15]$ 上均匀间隔的 $N_\\star = 201$ 个输入组成的测试集，以探测跨地层边界的外推和内插行为。计算这些测试输入对应的真实输出 $f(x; J, S)$。\n- 在测试集上计算两种代理模型的均方根误差 (RMSE)，\n$$\n\\mathrm{RMSE}_\\text{blend} \\;=\\; \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}\\big(\\hat{f}_\\text{blend}(x^\\star_i) - f(x^\\star_i; J, S)\\big)^2}, \\quad\n\\mathrm{RMSE}_\\text{mono} \\;=\\; \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}\\big(\\hat{f}_\\text{mono}(x^\\star_i) - f(x^\\star_i; J, S)\\big)^2}.\n$$\n- 通过检查是否满足 $\\mathrm{RMSE}_\\text{blend} + \\epsilon  \\mathrm{RMSE}_\\text{mono}$（容差 $\\epsilon = 10^{-3}$）来判断相感知能力是否减少了跨地层边界的外推误差。\n\n您的程序必须为以下参数元组 $(J, S, \\ell_0, \\ell_1, \\ell_g, \\beta)$ 的测试套件实现上述内容：\n\n- 测试 A（具有偏移跳跃和形状变化的一般情况）：$(1.0, 1, 0.15, 0.35, 0.50, 60.0)$。\n- 测试 B（无跳跃、无形状差异的边界情况）：$(0.0, 0, 0.20, 0.20, 0.20, 60.0)$。\n- 测试 C（跨相的不同平滑度和中等跳跃）：$(0.5, 1, 0.10, 0.40, 0.30, 30.0)$。\n\n您的程序应生成单行输出，其中包含布尔结果，形式为方括号括起来的逗号分隔列表，例如 $[\\text{True},\\text{False},\\text{True}]$。不涉及物理单位；所有量均为无量纲。三角函数中使用的所有角度均为弧度。最终输出必须是布尔值，并完全按照上述规定对每个测试用例按给定顺序计算得出。",
            "solution": "该问题提出了一个有效且适定的计算任务。它在科学上基于高斯过程（GP）回归的原理及其在地球物理学中代理建模的应用。其目标是比较单体高斯过程与相感知的融合高斯过程在对一个具有潜在不连续性的函数进行建模时的性能。所有参数、函数和评估标准都得到了明确定义，使得该问题是自洽且可解的。\n\n求解过程首先定义真实的地球物理正演模型，并生成规定的训练和测试数据集。然后，构建两种类型的高斯过程代理模型：一个是在所有数据上训练的单体高斯过程，以及两个局部高斯过程，每个都在单个地质相的数据上进行训练。随后，使用平滑的单位分解融合方案将局部高斯过程的预测结果组合起来。最后，在一个跨越相边界的测试集上评估单体和融合代理模型的准确性，并根据哪个模型产生更低的均方根误差（RMSE）来做出决策，这表明该模型能更好地处理边界的外推挑战。\n\n解析和算法步骤如下：\n\n**1. 正演模型和数据生成**\n\n真实的正演模型 $f(x; J, S)$ 是基于 $x=0$ 处的相边界进行分段定义的。\n基函数为 $b(x) = \\sin(6x) - 0.1x$。\n对于相 0（$x  0$），响应为 $f_0(x) = b(x)$。\n对于相 1（$x \\ge 0$），响应为 $f_1(x; J, S) = (1 - S)b(x) + S(0.8 \\cos(6x + 0.5) + 0.1x) + J$。此处，$J$ 引入了一个数值跳跃，$S$ 切换了函数形式的变化。\n\n为每个相分别生成训练数据。\n对于相 0，$N_0 = 30$ 个输入点 $\\mathbf{X}_0$ 在 $[-1, -0.02]$ 上均匀采样。对应的输出为 $\\mathbf{y}_0 = f(\\mathbf{X}_0; J, S)$。\n对于相 1，$N_1 = 30$ 个输入点 $\\mathbf{X}_1$ 在 $[0.02, 1]$ 上均匀采样。输出为 $\\mathbf{y}_1 = f(\\mathbf{X}_1; J, S)$。\n用于单体模型的数据集合并为 $\\mathbf{X}_g = [\\mathbf{X}_0^T, \\mathbf{X}_1^T]^T$ 和 $\\mathbf{y}_g = [\\mathbf{y}_0^T, \\mathbf{y}_1^T]^T$。\n\n**2. 高斯过程代理建模**\n\n一个高斯过程由其均值和协方差函数完全指定。我们假设所有高斯过程的先验均为零均值。协方差由信号方差为 $\\sigma_f^2=1$ 的平方指数核函数给出：\n$$k(x, x'; \\ell) = \\exp\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right)$$\n对于一组训练输入 $\\mathbf{X}$ 和输出 $\\mathbf{y}$，以及测试输入 $\\mathbf{X}_\\star$，高斯过程的后验预测均值 $\\mathbf{m}_\\star$ 由下式给出：\n$$\\mathbf{m}_\\star = \\mathbf{K}_{\\star N} (\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}$$\n其中 $[\\mathbf{K}_{N N}]_{ij} = k(x_i, x_j)$，$[\\mathbf{K}_{\\star N}]_{ij} = k(x^\\star_i, x_j)$，$\\sigma_n^2=10^{-8}$ 是用于数值稳定性的小扰动项。\n\n在计算上，我们首先求解线性系统 $(\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I})\\alpha = \\mathbf{y}$ 以获得权重向量 $\\alpha$。然后，预测均值计算为 $\\mathbf{m}_\\star = \\mathbf{K}_{\\star N} \\alpha$。\n\n我们构建三组预测：\n-   **单体高斯过程 ($\\hat{f}_\\text{mono}$)**：使用单一核函数（长度尺度为 $\\ell_g$）在合并数据 $(\\mathbf{X}_g, \\mathbf{y}_g)$ 上训练。\n    $$\n    \\alpha_g = (\\mathbf{K}_{gg} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_g, \\quad \\text{其中} \\; \\mathbf{K}_{gg} = k(\\mathbf{X}_g, \\mathbf{X}_g; \\ell_g)\n    $$\n    $$\n    \\hat{f}_\\text{mono}(x_\\star) = k(x_\\star, \\mathbf{X}_g; \\ell_g) \\alpha_g\n    $$\n\n-   **相 0 的局部高斯过程 ($\\hat{f}_0$)**：使用核函数（长度尺度为 $\\ell_0$）在 $(\\mathbf{X}_0, \\mathbf{y}_0)$ 上训练。\n    $$\n    \\alpha_0 = (\\mathbf{K}_{00} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_0, \\quad \\text{其中} \\; \\mathbf{K}_{00} = k(\\mathbf{X}_0, \\mathbf{X}_0; \\ell_0)\n    $$\n    $$\n    \\hat{f}_0(x_\\star) = k(x_\\star, \\mathbf{X}_0; \\ell_0) \\alpha_0\n    $$\n\n-   **相 1 的局部高斯过程 ($\\hat{f}_1$)**：使用核函数（长度尺度为 $\\ell_1$）在 $(\\mathbf{X}_1, \\mathbf{y}_1)$ 上训练。\n    $$\n    \\alpha_1 = (\\mathbf{K}_{11} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_1, \\quad \\text{其中} \\; \\mathbf{K}_{11} = k(\\mathbf{X}_1, \\mathbf{X}_1; \\ell_1)\n    $$\n    $$\n    \\hat{f}_1(x_\\star) = k(x_\\star, \\mathbf{X}_1; \\ell_1) \\alpha_1\n    $$\n\n**3. 通过单位分解进行融合**\n\n局部高斯过程的预测结果被组合成一个单一的、连续的预测，使用的是基于逻辑斯蒂函数的单位分解方案。权重取决于到边界 $x_b=0$ 的距离：\n$$\nw_1(x;\\beta) = \\frac{1}{1 + \\exp(-\\beta x)}, \\quad w_0(x;\\beta) = 1 - w_1(x;\\beta)\n$$\n锐度参数 $\\beta$ 控制过渡的陡峭程度。融合后的预测均值 $\\hat{f}_\\text{blend}(x)$ 是局部高斯过程均值的加权平均：\n$$\n\\hat{f}_\\text{blend}(x) = w_0(x;\\beta)\\,\\hat{f}_0(x) + w_1(x;\\beta)\\,\\hat{f}_1(x)\n$$\n\n**4. 评估和决策**\n\n两种代理模型都在一个由 $N_\\star = 201$ 个在 $[-0.15, 0.15]$ 上均匀间隔的点 $\\mathbf{X}_\\star$ 组成的密集测试集上进行评估。选择此区间是为了仔细检查模型在边界附近和跨越边界时的性能，这些地方需要从不相交的训练集进行外推。\n\n性能通过与真实函数值 $f(\\mathbf{X}_\\star; J, S)$ 的均方根误差（RMSE）进行量化。\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}(\\hat{f}(x^\\star_i) - f(x^\\star_i))^2}\n$$\n我们计算 $\\mathrm{RMSE}_\\text{blend}$ 和 $\\mathrm{RMSE}_\\text{mono}$。如果相感知方法的误差显著更小，即满足以下条件，则认为该方法更优：\n$$\n\\mathrm{RMSE}_\\text{blend} + \\epsilon  \\mathrm{RMSE}_\\text{mono}\n$$\n容差为 $\\epsilon = 10^{-3}$。对测试套件中提供的每个参数集重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for comparing monolithic vs. blended GP surrogates.\n    \"\"\"\n    test_cases = [\n        # Test A (general case with offset jump and shape change)\n        (1.0, 1.0, 0.15, 0.35, 0.50, 60.0),\n        # Test B (boundary case with no jump and no shape difference)\n        (0.0, 0.0, 0.20, 0.20, 0.20, 60.0),\n        # Test C (different smoothness across facies and moderate jump)\n        (0.5, 1.0, 0.10, 0.40, 0.30, 30.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        J, S, l0, l1, lg, beta = params\n\n        # Constants and model parameters\n        N0, N1 = 30, 30\n        sigma_f_sq = 1.0\n        nugget = 1e-8\n        epsilon = 1e-3\n        N_star = 201\n\n        # Define the base function and the full forward model\n        def base_func(x):\n            return np.sin(6 * x) - 0.1 * x\n\n        def forward_model(x, J_val, S_val):\n            x = np.asarray(x)\n            result = np.zeros_like(x, dtype=float)\n            \n            mask0 = x  0\n            result[mask0] = base_func(x[mask0])\n            \n            mask1 = x = 0\n            f1_part1 = (1 - S_val) * base_func(x[mask1])\n            f1_part2 = S_val * (0.8 * np.cos(6 * x[mask1] + 0.5) + 0.1 * x[mask1])\n            result[mask1] = f1_part1 + f1_part2 + J_val\n            \n            return result\n\n        # Generate training data\n        X0 = np.linspace(-1.0, -0.02, N0)[:, np.newaxis]\n        y0 = forward_model(X0.flatten(), J, S)\n        \n        X1 = np.linspace(0.02, 1.0, N1)[:, np.newaxis]\n        y1 = forward_model(X1.flatten(), J, S)\n        \n        Xg = np.vstack((X0, X1))\n        yg = np.concatenate((y0, y1))\n\n        # Generate test data\n        X_star = np.linspace(-0.15, 0.15, N_star)[:, np.newaxis]\n        y_star = forward_model(X_star.flatten(), J, S)\n\n        # Define RBF kernel\n        def rbf_kernel(X1, X2, length_scale, variance):\n            sq_dist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n            return variance * np.exp(-0.5 / length_scale**2 * sq_dist)\n\n        # GP prediction function\n        def gp_predict(X_train, y_train, X_test, length_scale, variance, noise_var):\n            K_NN = rbf_kernel(X_train, X_train, length_scale, variance)\n            K_NN += noise_var * np.eye(X_train.shape[0])\n            \n            try:\n                # Solve (K_NN) * alpha = y_train\n                alpha = linalg.solve(K_NN, y_train, assume_a='pos')\n            except linalg.LinAlgError:\n                # Fallback for singular matrix\n                alpha = linalg.lstsq(K_NN, y_train)[0]\n                \n            K_star_N = rbf_kernel(X_test, X_train, length_scale, variance)\n            y_pred = K_star_N.dot(alpha)\n            return y_pred\n\n        # 1. Monolithic GP\n        y_hat_mono = gp_predict(Xg, yg, X_star, lg, sigma_f_sq, nugget)\n        \n        # 2. Local GPs\n        y_hat_0 = gp_predict(X0, y0, X_star, l0, sigma_f_sq, nugget)\n        y_hat_1 = gp_predict(X1, y1, X_star, l1, sigma_f_sq, nugget)\n\n        # 3. Blended GP\n        x_b = 0.0\n        w1 = 1.0 / (1.0 + np.exp(-beta * (X_star.flatten() - x_b)))\n        w0 = 1.0 - w1\n        y_hat_blend = w0 * y_hat_0 + w1 * y_hat_1\n\n        # Calculate RMSE for both models\n        rmse_mono = np.sqrt(np.mean((y_hat_mono - y_star)**2))\n        rmse_blend = np.sqrt(np.mean((y_hat_blend - y_star)**2))\n\n        # Decision\n        is_better = (rmse_blend + epsilon)  rmse_mono\n        results.append(is_better)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}