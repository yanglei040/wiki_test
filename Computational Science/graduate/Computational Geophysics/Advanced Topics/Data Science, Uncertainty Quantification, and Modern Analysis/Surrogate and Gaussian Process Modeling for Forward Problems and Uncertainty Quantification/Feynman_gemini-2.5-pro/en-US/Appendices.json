{
    "hands_on_practices": [
        {
            "introduction": "To truly master Gaussian Process (GP) modeling, it is essential to look beyond pre-built software packages and understand the underlying mechanics. This first practice provides a foundational, first-principles derivation of the GP predictive equations for a simple case with two data points . By working through the symbolic manipulation of Gaussian distributions, you will build a concrete intuition for how a GP combines prior knowledge (the kernel) with observed data to make predictions and quantify uncertainty.",
            "id": "3615806",
            "problem": "A geophysical forward model maps a scalar control parameter $x \\in \\mathbb{R}$ (e.g., a bulk modulus proxy) to a scalar response $y \\in \\mathbb{R}$ (e.g., a travel-time misfit). To accelerate uncertainty quantification, a zero-mean Gaussian Process (GP) surrogate is adopted: the latent function $f$ is modeled as $f \\sim \\mathcal{GP}(0, k)$, where $k(\\cdot,\\cdot)$ is a symmetric, positive-definite covariance kernel. Observations are corrupted by independent Gaussian noise: for $i \\in \\{1,2\\}$, $y_i = f(x_i) + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ and $\\epsilon_1 \\perp \\epsilon_2$. You are given $2$ training inputs $x_1,x_2$ with corresponding outputs $y_1,y_2$, and a test input $x_\\star$. Let $k_{11} = k(x_1,x_1)$, $k_{22} = k(x_2,x_2)$, $k_{12} = k(x_1,x_2)$, $k_{\\star 1} = k(x_\\star,x_1)$, $k_{\\star 2} = k(x_\\star,x_2)$, and $k_{\\star\\star} = k(x_\\star,x_\\star)$. Assume $k$ is strictly positive definite and $\\sigma_n^2 > 0$ so that all required inverses exist. Using only the foundational facts that (i) affine transformations and marginalization of jointly Gaussian variables produce Gaussian variables, and (ii) conditioning a joint Gaussian yields closed-form expressions for the conditional mean and covariance, derive the Gaussian Process predictive mean $m_\\star$ and predictive variance $s_\\star^2$ at $x_\\star$ explicitly in terms of $k_{11}$, $k_{22}$, $k_{12}$, $k_{\\star 1}$, $k_{\\star 2}$, $k_{\\star\\star}$, $\\sigma_n^2$, $y_1$, and $y_2$. Your final expressions must be closed-form scalar formulas involving only these symbols and standard arithmetic operations, with no matrix inverses remaining. Provide your final answer as a two-entry row matrix $\\big(m_\\star,\\; s_\\star^2\\big)$ using the $\\text{pmatrix}$ environment. No numerical evaluation or rounding is required.",
            "solution": "The problem is a standard derivation of the predictive distribution for Gaussian Process (GP) regression with a $2$-point training set. It is scientifically grounded, well-posed, and objective. All necessary information is provided, and the constraints are clear. The problem is valid.\n\nThe objective is to find the predictive mean $m_\\star$ and predictive variance $s_\\star^2$ for the latent function value $f_\\star = f(x_\\star)$ at a test point $x_\\star$, conditioned on the observed data $\\mathbf{y} = \\begin{pmatrix} y_1 & y_2 \\end{pmatrix}^T$ from training inputs $\\mathbf{x} = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix}^T$.\n\nThe foundation of GP regression lies in the property that any finite set of function values is jointly Gaussian. We are interested in the conditional distribution $p(f_\\star | \\mathbf{y})$. We begin by establishing the joint distribution of the latent function value at the test point, $f_\\star$, and the noisy observations, $\\mathbf{y}$.\n\nThe latent function $f$ is modeled as a zero-mean Gaussian Process, $f \\sim \\mathcal{GP}(0, k)$. The observations are given by $y_i = f(x_i) + \\epsilon_i$, with independent and identically distributed Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. Let $\\mathbf{f} = \\begin{pmatrix} f(x_1) & f(x_2) \\end{pmatrix}^T$. The vector containing the latent values at the training and test points, $\\begin{pmatrix} \\mathbf{f}^T & f_\\star \\end{pmatrix}^T$, follows a multivariate Gaussian distribution:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^T & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\nwhere $K = \\begin{pmatrix} k(x_1, x_1) & k(x_1, x_2) \\\\ k(x_2, x_1) & k(x_2, x_2) \\end{pmatrix} = \\begin{pmatrix} k_{11} & k_{12} \\\\ k_{12} & k_{22} \\end{pmatrix}$, $\\mathbf{k}_\\star = \\begin{pmatrix} k(x_\\star, x_1) \\\\ k(x_\\star, x_2) \\end{pmatrix} = \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$, and $k_{\\star\\star} = k(x_\\star, x_\\star)$.\n\nThe vector of observations is $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1 & \\epsilon_2 \\end{pmatrix}^T$. The noise vector $\\boldsymbol{\\epsilon}$ is also Gaussian, $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$, where $I$ is the $2 \\times 2$ identity matrix. Since $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$ are independent, their sum $\\mathbf{y}$ is also Gaussian.\n\nWe now construct the joint distribution of the observations $\\mathbf{y}$ and the test latent value $f_\\star$.\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{joint}}, \\Sigma_{\\text{joint}})\n$$\nThe mean is $\\boldsymbol{\\mu}_{\\text{joint}} = E\\left[\\begin{pmatrix} \\mathbf{f} + \\boldsymbol{\\epsilon} \\\\ f_\\star \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f}] + E[\\boldsymbol{\\epsilon}] \\\\ E[f_\\star] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}$.\n\nThe covariance matrix $\\Sigma_{\\text{joint}}$ is a partitioned matrix:\n$$\n\\Sigma_{\\text{joint}} = \\begin{pmatrix} \\text{Cov}(\\mathbf{y}, \\mathbf{y}) & \\text{Cov}(\\mathbf{y}, f_\\star) \\\\ \\text{Cov}(f_\\star, \\mathbf{y}) & \\text{Cov}(f_\\star, f_\\star) \\end{pmatrix}\n$$\nThe blocks are computed as:\n$\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = E[\\mathbf{y}\\mathbf{y}^T] = E[(\\mathbf{f} + \\boldsymbol{\\epsilon})(\\mathbf{f} + \\boldsymbol{\\epsilon})^T] = E[\\mathbf{f}\\mathbf{f}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = K + \\sigma_n^2 I$.\n$\\text{Cov}(\\mathbf{y}, f_\\star) = E[\\mathbf{y}f_\\star] = E[(\\mathbf{f} + \\boldsymbol{\\epsilon})f_\\star] = E[\\mathbf{f}f_\\star] = \\mathbf{k}_\\star$.\n$\\text{Cov}(f_\\star, f_\\star) = E[f_\\star^2] = k_{\\star\\star}$.\nSo the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma_n^2 I & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^T & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\nWe seek the conditional distribution of $f_\\star$ given $\\mathbf{y}$. For a general partitioned Gaussian variable $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix}\\right)$, the conditional distribution of $\\mathbf{b}$ given $\\mathbf{a}$ is $\\mathbf{b} | \\mathbf{a} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$, where:\n$\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba}\\Sigma_{aa}^{-1}(\\mathbf{a} - \\boldsymbol{\\mu}_a)$\n$\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}$\n\nApplying this to our problem by substituting $\\mathbf{a} \\to \\mathbf{y}$, $\\mathbf{b} \\to f_\\star$, and the corresponding mean and covariance blocks:\n$m_\\star = 0 + \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} (\\mathbf{y} - \\mathbf{0}) = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}$\n$s_\\star^2 = k_{\\star\\star} - \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_\\star$\n\nThe problem requires an explicit scalar formula without matrix inverses. We must compute the inverse of the $2 \\times 2$ matrix $K + \\sigma_n^2 I$.\nLet $A = K + \\sigma_n^2 I = \\begin{pmatrix} k_{11} + \\sigma_n^2 & k_{12} \\\\ k_{12} & k_{22} + \\sigma_n^2 \\end{pmatrix}$.\nThe determinant of $A$ is:\n$\\det(A) = (k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2$.\nThe inverse of $A$ is:\n$A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} k_{22} + \\sigma_n^2 & -k_{12} \\\\ -k_{12} & k_{11} + \\sigma_n^2 \\end{pmatrix}$.\n\nNow, we substitute this into the expressions for $m_\\star$ and $s_\\star^2$.\nFor the predictive mean $m_\\star$:\n$m_\\star = \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} A^{-1} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$\n$m_\\star = \\frac{1}{\\det(A)} \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} \\begin{pmatrix} k_{22} + \\sigma_n^2 & -k_{12} \\\\ -k_{12} & k_{11} + \\sigma_n^2 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$\n$m_\\star = \\frac{1}{\\det(A)} \\left[ (k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12})y_1 + (-k_{\\star 1}k_{12} + k_{\\star 2}(k_{11} + \\sigma_n^2))y_2 \\right]$\nRearranging the terms in parentheses gives:\n$m_\\star = \\frac{(k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12})y_1 + (k_{\\star 2}(k_{11} + \\sigma_n^2) - k_{\\star 1}k_{12})y_2}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2}$\n\nFor the predictive variance $s_\\star^2$:\n$s_\\star^2 = k_{\\star\\star} - \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} A^{-1} \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$\n$s_\\star^2 = k_{\\star\\star} - \\frac{1}{\\det(A)} \\begin{pmatrix} k_{\\star 1} & k_{\\star 2} \\end{pmatrix} \\begin{pmatrix} k_{22} + \\sigma_n^2 & -k_{12} \\\\ -k_{12} & k_{11} + \\sigma_n^2 \\end{pmatrix} \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$\nThe quadratic form in the numerator is:\n$k_{\\star 1} ( (k_{22} + \\sigma_n^2)k_{\\star 1} - k_{12}k_{\\star 2} ) + k_{\\star 2} ( -k_{12}k_{\\star 1} + (k_{11} + \\sigma_n^2)k_{\\star 2} )$\n$= k_{\\star 1}^2(k_{22} + \\sigma_n^2) - k_{\\star 1}k_{12}k_{\\star 2} - k_{\\star 2}k_{12}k_{\\star 1} + k_{\\star 2}^2(k_{11} + \\sigma_n^2)$\n$= k_{\\star 1}^2(k_{22} + \\sigma_n^2) + k_{\\star 2}^2(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}$\nThus, the predictive variance is:\n$s_\\star^2 = k_{\\star\\star} - \\frac{k_{\\star 1}^2(k_{22} + \\sigma_n^2) + k_{\\star 2}^2(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2}$\nThese are the final explicit scalar formulas for the predictive mean and variance.",
            "answer": "$$\n\\boxed{\\begin{pmatrix}\n\\frac{\\left(k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12}\\right)y_1 + \\left(k_{\\star 2}(k_{11} + \\sigma_n^2) - k_{\\star 1}k_{12}\\right)y_2}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^{2}} & k_{\\star\\star} - \\frac{k_{\\star 1}^{2}(k_{22} + \\sigma_n^2) + k_{\\star 2}^{2}(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^{2}}\n\\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world data from physical simulations or field measurements rarely conforms to the idealized assumption of uniform, constant noise. This exercise moves from pure theory to a crucial practical challenge: handling input-dependent, or heteroscedastic, noise . You will learn to estimate local noise levels from replicated data and incorporate this information into the GP model, leading to more accurate and reliable uncertainty estimates.",
            "id": "3615819",
            "problem": "Consider a one-dimensional forward response modeled as a latent function $f(x)$ endowed with a Gaussian process (GP) prior with zero mean and a stationary squared-exponential (radial basis function) covariance kernel. Observations are noisy simulator outputs with input-dependent noise, and several inputs have replicated runs. The goal is to estimate local noise variances from replicates, incorporate them into a heteroscedastic GP, and assess the impact on predictive variance and the marginal likelihood relative to a naively homoscedastic GP.\n\nFundamental base:\n- A Gaussian process (GP) prior specifies that any finite collection of latent values $(f(x_1),\\dots,f(x_n))$ is multivariate Gaussian with mean vector $\\mathbf{0}$ and covariance matrix determined by a positive definite kernel $k(\\cdot,\\cdot)$.\n- The noisy observation model is $y_i = f(x_i) + \\varepsilon_i$, where the noise $\\varepsilon_i$ is independent across $i$ and follows a zero-mean Gaussian distribution with variance $\\sigma^2(x_i)$.\n- Conditioning of a multivariate normal distribution yields the posterior distribution for the latent function at test inputs given observed data, when the observation covariance includes both the kernel covariance and the noise covariance.\n\nModel specification:\n- Kernel: $k(x,x') = \\sigma_f^2 \\exp\\!\\left(-\\dfrac{(x-x')^2}{2\\ell^2}\\right)$ with fixed hyperparameters $\\ell = 0.25$ and $\\sigma_f^2 = 1.0$.\n- Prior mean is zero everywhere.\n- No physical units are specified; all quantities are dimensionless.\n\nTraining data:\n- Unique inputs and their replicated observations:\n  - $x = 0.0$ with observations $[0.56,\\,0.49,\\,0.61]$.\n  - $x = 0.2$ with observations $[1.30,\\,1.36,\\,1.29,\\,1.35,\\,1.33]$.\n  - $x = 0.5$ with observations $[0.03,\\,-0.02]$.\n  - $x = 0.8$ with observations $[-1.27,\\,-1.38,\\,-1.36,\\,-1.31]$.\n  - $x = 1.1$ with observations $[0.09]$.\n\nNoise variance estimation from replicates:\n- For a location $x$ with $m \\ge 2$ replicates $\\{y_j\\}_{j=1}^m$, estimate the local noise variance by the unbiased sample variance $\\hat{\\sigma}^2(x) = \\dfrac{1}{m-1}\\sum_{j=1}^m \\left(y_j - \\bar{y}\\right)^2$, where $\\bar{y} = \\dfrac{1}{m}\\sum_{j=1}^m y_j$.\n- For a location with $m \\le 1$ replicate, use the pooled variance computed across all locations with at least two replicates: $\\hat{\\sigma}^2_{\\mathrm{pool}} = \\dfrac{\\sum_{i} \\sum_{j=1}^{m_i} (y_{ij} - \\bar{y}_i)^2}{\\sum_i (m_i - 1)}$, where the sums run over only those locations with $m_i \\ge 2$.\n\nHeteroscedastic GP:\n- Build the covariance matrix for the observations as $C = K + \\Sigma$, where $K$ is the kernel matrix formed from all replicated inputs and $\\Sigma$ is a diagonal matrix whose diagonal entries are the local noise variances $\\hat{\\sigma}^2(x)$ repeated for each replicate at $x$.\n\nHomoscedastic GP baseline:\n- Build the covariance matrix as $C_{\\mathrm{homo}} = K + \\hat{\\sigma}^2_{\\mathrm{pool}} I$, where $I$ is the identity matrix of matching dimension.\n\nPosterior predictive variance:\n- For a test input $x_\\star$, compute the predictive variance of the latent function $f(x_\\star)$ under both the heteroscedastic and homoscedastic models by conditioning the joint Gaussian distribution of $(\\mathbf{y}, f(x_\\star))$; the observation covariance is $C$ for the heteroscedastic case and $C_{\\mathrm{homo}}$ for the homoscedastic case. The returned variance must be that of the latent function $f(x_\\star)$, not including any observation noise.\n\nLog marginal likelihood:\n- For observed data vector $\\mathbf{y}$, the log marginal likelihood under a zero-mean GP with observation covariance $C$ is $\\log p(\\mathbf{y}) = -\\dfrac{1}{2}\\mathbf{y}^\\top C^{-1}\\mathbf{y} - \\dfrac{1}{2}\\log\\det C - \\dfrac{n}{2}\\log(2\\pi)$, where $n$ is the total number of observations.\n\nTasks to implement and test:\n- Use the above training data and fixed kernel hyperparameters $\\ell = 0.25$, $\\sigma_f^2 = 1.0$.\n- Estimate local noise variances at each unique input using the rule above, with pooled variance fallback where needed.\n- Construct both the heteroscedastic and homoscedastic GP models as described.\n- Compute the following quantities (test suite):\n  1. The estimated local noise variance at $x = 0.2$ as a float.\n  2. The heteroscedastic predictive variance for the latent $f$ at $x_\\star = 0.2$ as a float.\n  3. The heteroscedastic predictive variance for the latent $f$ at $x_\\star = 1.8$ as a float.\n  4. The ratio of homoscedastic to heteroscedastic predictive variances at $x_\\star = 0.2$, i.e., $\\dfrac{\\mathrm{Var}_{\\mathrm{homo}}[f(x_\\star)]}{\\mathrm{Var}_{\\mathrm{hetero}}[f(x_\\star)]}$, as a float.\n  5. The difference in negative log marginal likelihoods $\\mathrm{NLL}_{\\mathrm{homo}} - \\mathrm{NLL}_{\\mathrm{hetero}}$, where $\\mathrm{NLL} = -\\log p(\\mathbf{y})$, as a float.\n\nNumerical details:\n- Add a small positive jitter $\\epsilon = 10^{-10}$ to the diagonal of all covariance matrices used in linear algebra to ensure numerical stability.\n- Use Cholesky-based linear algebra for solving systems and computing log determinants.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the five items above. For example, the output must be of the form $[r_1,r_2,r_3,r_4,r_5]$.",
            "solution": "We start from the definition of a Gaussian process (GP) prior and Gaussian observation noise. A Gaussian process prior with zero mean and covariance function $k(\\cdot,\\cdot)$ implies that for any finite set of inputs $\\{x_i\\}_{i=1}^n$, the latent vector $\\mathbf{f} = [f(x_1),\\dots,f(x_n)]^\\top$ follows a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, K)$, where $K_{ij} = k(x_i, x_j)$. The observations follow $y_i = f(x_i) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2(x_i))$ independent across $i$.\n\nWith input-dependent (heteroscedastic) noise variance, we construct the observation covariance matrix as $C = K + \\Sigma$, where $\\Sigma$ is diagonal with entries $\\Sigma_{ii} = \\sigma^2(x_i)$. Given this, the joint distribution of $\\mathbf{y}$ and $f(x_\\star)$ for a single test input $x_\\star$ is Gaussian:\n\n$$\n\\begin{bmatrix}\n\\mathbf{y} \\\\\nf(x_\\star)\n\\end{bmatrix}\n\\sim\n\\mathcal{N}\n\\left(\n\\begin{bmatrix}\n\\mathbf{0} \\\\\n0\n\\end{bmatrix},\n\\begin{bmatrix}\nC & \\mathbf{k}_\\star \\\\\n\\mathbf{k}_\\star^\\top & k_{\\star \\star}\n\\end{bmatrix}\n\\right),\n$$\n\nwhere $\\mathbf{k}_\\star$ is the $n \\times 1$ vector with entries $k(x_i, x_\\star)$ and $k_{\\star \\star} = k(x_\\star, x_\\star)$.\n\nBy conditioning a multivariate normal distribution, the posterior for the latent function at $x_\\star$ given $\\mathbf{y}$ is Gaussian with mean and variance\n\n$$\n\\mathbb{E}[f(x_\\star)\\mid \\mathbf{y}] = \\mathbf{k}_\\star^\\top C^{-1} \\mathbf{y}, \\quad\n\\mathrm{Var}[f(x_\\star)\\mid \\mathbf{y}] = k_{\\star \\star} - \\mathbf{k}_\\star^\\top C^{-1} \\mathbf{k}_\\star.\n$$\n\nThese formulas follow from the Schur complement identities for conditional Gaussians.\n\nFor the homoscedastic baseline, we replace $\\Sigma$ by $\\sigma_{\\mathrm{pool}}^2 I$, where $\\sigma_{\\mathrm{pool}}^2$ is a pooled variance estimated across groups with at least two replicates. This produces $C_{\\mathrm{homo}} = K + \\sigma_{\\mathrm{pool}}^2 I$.\n\nEstimation of noise variances from replicates proceeds as follows. For each unique input $x$ with $m \\ge 2$ replicates $\\{y_j\\}_{j=1}^m$, we use the unbiased sample variance\n\n$$\n\\hat{\\sigma}^2(x) = \\frac{1}{m-1} \\sum_{j=1}^m (y_j - \\bar{y})^2,\\quad \\bar{y} = \\frac{1}{m}\\sum_{j=1}^m y_j.\n$$\n\nFor locations with $m \\le 1$, we use the pooled variance computed over all locations with at least two replicates:\n\n$$\n\\hat{\\sigma}^2_{\\mathrm{pool}} = \\frac{\\sum_i \\sum_{j=1}^{m_i} (y_{ij} - \\bar{y}_i)^2}{\\sum_i (m_i - 1)},\n$$\n\nwhere the sums include only locations with $m_i \\ge 2$, and $\\bar{y}_i$ is the sample mean at location $x_i$. This pooled estimator is the maximum likelihood estimator for a common variance under Gaussian within-group residuals.\n\nThe kernel is the squared-exponential (radial basis function) kernel:\n\n$$\nk(x, x') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(x - x')^2}{2\\ell^2}\\right),\n$$\n\nwith fixed hyperparameters $\\ell = 0.25$ and $\\sigma_f^2 = 1.0$. For numerical stability and positive definiteness, we add a small jitter $\\epsilon = 10^{-10}$ to the diagonal of $C$ (and $C_{\\mathrm{homo}}$) before factorization.\n\nTo compute the predictive variance efficiently, we use Cholesky factorization. Let $C = L L^\\top$ be the Cholesky factorization of $C$. Then $C^{-1}\\mathbf{k}_\\star$ can be computed by solving the triangular systems $L \\mathbf{z} = \\mathbf{k}_\\star$ and $L^\\top \\mathbf{w} = \\mathbf{z}$, yielding $\\mathbf{w} = C^{-1}\\mathbf{k}_\\star$. The predictive variance is $k_{\\star \\star} - \\mathbf{k}_\\star^\\top \\mathbf{w}$. The same procedure applies to $C_{\\mathrm{homo}}$ to obtain the homoscedastic predictive variance.\n\nThe log marginal likelihood for zero mean and covariance $C$ is\n\n$$\n\\log p(\\mathbf{y}) = -\\frac{1}{2}\\mathbf{y}^\\top C^{-1}\\mathbf{y} - \\frac{1}{2}\\log\\det C - \\frac{n}{2}\\log(2\\pi),\n$$\n\nwith $n$ equal to the total number of observations (including replicates). Using the Cholesky factor $L$, we have $\\log\\det C = 2 \\sum_{i=1}^n \\log L_{ii}$ and $\\mathbf{y}^\\top C^{-1}\\mathbf{y}$ computed via two triangular solves analogous to the predictive variance computation. The negative log marginal likelihood is $\\mathrm{NLL} = -\\log p(\\mathbf{y})$.\n\nAlgorithmic steps:\n1. Assemble the full training input vector $\\mathbf{x}$ of length $n$ by repeating each unique input according to its number of replicates, and the corresponding observation vector $\\mathbf{y}$.\n2. For each unique input, compute $\\hat{\\sigma}^2(x)$ if $m \\ge 2$. Accumulate residual sums of squares and degrees of freedom to compute $\\hat{\\sigma}^2_{\\mathrm{pool}}$.\n3. Assign the noise variance for each replicate: use the local $\\hat{\\sigma}^2(x)$ when available; otherwise use $\\hat{\\sigma}^2_{\\mathrm{pool}}$.\n4. Construct $K$ using the kernel on $\\mathbf{x}$, and form $C = K + \\Sigma + \\epsilon I$ and $C_{\\mathrm{homo}} = K + \\hat{\\sigma}^2_{\\mathrm{pool}} I + \\epsilon I$.\n5. Using Cholesky-based solves, compute:\n   - The heteroscedastic predictive variances at $x_\\star = 0.2$ and $x_\\star = 1.8$.\n   - The homoscedastic predictive variance at $x_\\star = 0.2$.\n   - The ratio $\\dfrac{\\mathrm{Var}_{\\mathrm{homo}}[f(0.2)]}{\\mathrm{Var}_{\\mathrm{hetero}}[f(0.2)]}$.\n6. Compute $\\mathrm{NLL}_{\\mathrm{hetero}}$ with $C$ and $\\mathrm{NLL}_{\\mathrm{homo}}$ with $C_{\\mathrm{homo}}$, and then the difference $\\mathrm{NLL}_{\\mathrm{homo}} - \\mathrm{NLL}_{\\mathrm{hetero}}$.\n7. Report the five requested quantities in the specified order and format.\n\nThese steps adhere to the foundational properties of Gaussian processes and multivariate normal conditioning, incorporate replicate-based noise estimation to mitigate bias in predictive uncertainty, and provide a direct comparison between heteroscedastic and homoscedastic modeling for uncertainty quantification.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef rbf_kernel(xa: np.ndarray, xb: np.ndarray, length_scale: float, sigma_f2: float) -> np.ndarray:\n    \"\"\"\n    Squared-exponential (RBF) kernel matrix between vectors xa and xb.\n    \"\"\"\n    xa = xa[:, None]\n    xb = xb[None, :]\n    d2 = (xa - xb) ** 2\n    return sigma_f2 * np.exp(-0.5 * d2 / (length_scale ** 2))\n\ndef unbiased_sample_variance(y: np.ndarray) -> float:\n    \"\"\"\n    Unbiased sample variance for a 1D array y with length m >= 2.\n    \"\"\"\n    m = y.size\n    mean = y.mean()\n    return np.sum((y - mean) ** 2) / (m - 1)\n\ndef compute_pooled_variance(groups: list) -> float:\n    \"\"\"\n    Compute pooled variance across groups with at least two replicates.\n    Each element in groups is a numpy array of observations at a unique input.\n    \"\"\"\n    ss_total = 0.0\n    df_total = 0\n    for g in groups:\n        m = g.size\n        if m >= 2:\n            mean = g.mean()\n            ss = np.sum((g - mean) ** 2)\n            ss_total += ss\n            df_total += (m - 1)\n    if df_total == 0:\n        raise ValueError(\"No groups with at least two replicates to compute pooled variance.\")\n    return ss_total / df_total\n\ndef gp_predictive_variance(x_train: np.ndarray, noise_var: np.ndarray, x_star: float,\n                           length_scale: float, sigma_f2: float, jitter: float = 1e-10) -> float:\n    \"\"\"\n    Compute the predictive variance of latent f(x_star) using heteroscedastic noise variances.\n    \"\"\"\n    K = rbf_kernel(x_train, x_train, length_scale, sigma_f2)\n    C = K + np.diag(noise_var) + jitter * np.eye(x_train.size)\n    c_factor = cho_factor(C, lower=False, check_finite=False, overwrite_a=False)\n    k_star = rbf_kernel(x_train, np.array([x_star]), length_scale, sigma_f2).reshape(-1)\n    v = cho_solve(c_factor, k_star, check_finite=False, overwrite_b=False)\n    k_ss = sigma_f2  # since kernel evaluated at (x_star, x_star)\n    var = float(k_ss - k_star @ v)\n    # Numerical guard for tiny negative due to floating error\n    return max(var, 0.0)\n\ndef gp_nll(x_train: np.ndarray, noise_var: np.ndarray, y_train: np.ndarray,\n           length_scale: float, sigma_f2: float, jitter: float = 1e-10) -> float:\n    \"\"\"\n    Compute negative log marginal likelihood for zero-mean GP with given noise variances.\n    \"\"\"\n    K = rbf_kernel(x_train, x_train, length_scale, sigma_f2)\n    C = K + np.diag(noise_var) + jitter * np.eye(x_train.size)\n    c_factor = cho_factor(C, lower=False, check_finite=False, overwrite_a=False)\n    alpha = cho_solve(c_factor, y_train, check_finite=False, overwrite_b=False)\n    # log determinant from Cholesky: logdet(C) = 2 * sum(log(diag(R))) for upper-triangular R\n    R = c_factor[0]\n    logdet = 2.0 * np.sum(np.log(np.diag(R)))\n    n = y_train.size\n    quad = float(y_train @ alpha)\n    log2pi = np.log(2.0 * np.pi)\n    log_marg = -0.5 * quad - 0.5 * logdet - 0.5 * n * log2pi\n    return -log_marg\n\ndef solve():\n    # Define unique inputs and their replicate observations.\n    unique_x = np.array([0.0, 0.2, 0.5, 0.8, 1.1], dtype=float)\n    groups = [\n        np.array([0.56, 0.49, 0.61], dtype=float),                     # x = 0.0\n        np.array([1.30, 1.36, 1.29, 1.35, 1.33], dtype=float),         # x = 0.2\n        np.array([0.03, -0.02], dtype=float),                          # x = 0.5\n        np.array([-1.27, -1.38, -1.36, -1.31], dtype=float),           # x = 0.8\n        np.array([0.09], dtype=float),                                 # x = 1.1\n    ]\n\n    # Kernel hyperparameters (fixed)\n    length_scale = 0.25\n    sigma_f2 = 1.0\n    jitter = 1e-10\n\n    # Build full training arrays by repeating inputs per replicate and stacking observations.\n    x_train_list = []\n    y_train_list = []\n    for x, g in zip(unique_x, groups):\n        x_train_list.extend([x] * g.size)\n        y_train_list.extend(g.tolist())\n    x_train = np.array(x_train_list, dtype=float)\n    y_train = np.array(y_train_list, dtype=float)\n\n    # Estimate local noise variances where possible, and pooled variance for fallback.\n    local_var = {}\n    for x, g in zip(unique_x, groups):\n        if g.size >= 2:\n            local_var[x] = unbiased_sample_variance(g)\n        else:\n            local_var[x] = np.nan  # placeholder; will be replaced by pooled variance\n\n    pooled_var = compute_pooled_variance(groups)\n\n    # Construct per-observation noise variances for heteroscedastic model\n    hetero_noise = np.empty_like(y_train)\n    idx = 0\n    for x, g in zip(unique_x, groups):\n        var_x = local_var[x]\n        if not np.isfinite(var_x):\n            var_x = pooled_var\n        hetero_noise[idx:idx + g.size] = var_x\n        idx += g.size\n\n    # Construct per-observation noise variances for homoscedastic model\n    homo_noise = np.full_like(y_train, pooled_var)\n\n    # Test suite computations:\n    # 1) Estimated local noise variance at x = 0.2\n    var_at_02 = float(local_var[0.2])\n\n    # 2) Heteroscedastic predictive variance at x* = 0.2\n    var_hetero_at_02 = gp_predictive_variance(x_train, hetero_noise, 0.2, length_scale, sigma_f2, jitter)\n\n    # 3) Heteroscedastic predictive variance at x* = 1.8\n    var_hetero_at_18 = gp_predictive_variance(x_train, hetero_noise, 1.8, length_scale, sigma_f2, jitter)\n\n    # 4) Ratio Var_homo / Var_hetero at x* = 0.2\n    var_homo_at_02 = gp_predictive_variance(x_train, homo_noise, 0.2, length_scale, sigma_f2, jitter)\n    ratio_homo_to_hetero_at_02 = float(var_homo_at_02 / var_hetero_at_02) if var_hetero_at_02 > 0 else float('inf')\n\n    # 5) Difference in negative log marginal likelihoods: NLL_homo - NLL_hetero\n    nll_hetero = gp_nll(x_train, hetero_noise, y_train, length_scale, sigma_f2, jitter)\n    nll_homo = gp_nll(x_train, homo_noise, y_train, length_scale, sigma_f2, jitter)\n    delta_nll = float(nll_homo - nll_hetero)\n\n    results = [var_at_02, var_hetero_at_02, var_hetero_at_18, ratio_homo_to_hetero_at_02, delta_nll]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Geophysical systems are often characterized by sharp changes in material properties across boundaries, a feature known as non-stationarity that violates the assumptions of standard GP kernels. This advanced practice tackles this challenge by constructing a 'mixture of experts' surrogate, where local GPs are trained for distinct geological facies and blended across boundaries . This powerful technique is critical for building accurate surrogates of complex, multi-physics forward models encountered in realistic geophysical settings.",
            "id": "3615847",
            "problem": "Consider a one-dimensional forward modeling scenario in computational geophysics where the subsurface is partitioned into two geological facies separated by a stratigraphic boundary at location $x = 0$. The input coordinate is $x \\in [-1,1]$ (dimensionless), and the forward response is a scalar function $f(x)$ (dimensionless). The two facies are defined by the regions $x < 0$ and $x \\ge 0$. You are tasked with constructing local Gaussian Process (GP) surrogates per facies and blending them via a partition-of-unity scheme. You will compare the blended facies-aware surrogate against a monolithic GP that ignores facies, and you will evaluate whether facies awareness reduces extrapolation error across the stratigraphic boundary.\n\nUse only the following fundamental definitions and accepted facts as the base of your derivation and implementation:\n\n- A Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP prior over a function $f(\\cdot)$ is denoted $f \\sim \\mathcal{GP}(m(\\cdot), k(\\cdot,\\cdot))$, with mean function $m(\\cdot)$ and covariance function $k(\\cdot,\\cdot)$.\n- For a zero-mean GP prior with covariance $k(\\cdot,\\cdot)$, observed training inputs $\\mathbf{X} \\in \\mathbb{R}^{N \\times 1}$ and outputs $\\mathbf{y} \\in \\mathbb{R}^{N}$ with independent Gaussian noise of variance $\\sigma_n^2$, the posterior predictive mean at test inputs $\\mathbf{X}_\\star$ is given by\n$$\n\\mathbf{m}_\\star \\;=\\; \\mathbf{K}_{\\star N} \\left(\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y},\n$$\nwhere $\\mathbf{K}_{N N}$ is the $N \\times N$ Gram matrix with entries $[\\mathbf{K}_{N N}]_{ij} = k(x_i, x_j)$, and $\\mathbf{K}_{\\star N}$ is the $N_\\star \\times N$ cross-covariance matrix with entries $[\\mathbf{K}_{\\star N}]_{ij} = k(x^\\star_i, x_j)$.\n- A commonly used stationary covariance is the squared exponential (radial basis function) kernel\n$$\nk_\\text{RBF}(x, x'; \\ell, \\sigma_f^2) \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right),\n$$\nwith length-scale $\\ell > 0$ and signal variance $\\sigma_f^2 > 0$.\n- A partition of unity is a set of nonnegative weight functions $\\{ w_i(x) \\}_{i=1}^M$ satisfying $\\sum_{i=1}^M w_i(x) = 1$ for all $x$.\n\nYou will implement the following in a program:\n\n- Define a facies-dependent forward model. Let the base function be $b(x) = \\sin(6 x) - 0.1 x$. Define the facies response functions\n$$\nf_0(x) \\;=\\; b(x) \\quad \\text{for} \\; x < 0,\n$$\n$$\nf_1(x; J, S) \\;=\\; (1 - S)\\, b(x) + S\\,\\big(0.8 \\cos(6 x + 0.5) + 0.1 x\\big) + J \\quad \\text{for} \\; x \\ge 0,\n$$\nwhere $J \\in \\mathbb{R}$ controls an offset jump across the boundary and $S \\in \\{0,1\\}$ toggles a shape difference between facies. The true forward model is\n$$\nf(x; J, S) \\;=\\; \\begin{cases}\nf_0(x), & x < 0,\\\\\nf_1(x; J, S), & x \\ge 0.\n\\end{cases}\n$$\n- Generate training inputs per facies. Use $N_0 = 30$ points uniformly on $[-1, -0.02]$ for facies $0$ and $N_1 = 30$ points uniformly on $[0.02, 1]$ for facies $1$. Evaluate the noise-free training outputs from $f(x; J, S)$ at these inputs. Use a small numerical nugget $\\sigma_n^2 = 10^{-8}$ only for numerical stability in Gaussian Process linear algebra.\n- Construct two local GP surrogates, one per facies, using the squared exponential kernel with possibly different length-scales $\\ell_0$ and $\\ell_1$ and common signal variance $\\sigma_f^2 = 1$. For the monolithic GP that ignores facies, use a single kernel with length-scale $\\ell_g$ and signal variance $\\sigma_f^2 = 1$ trained on all training data pooled together.\n- Define a smooth partition-of-unity blending across the boundary at $x_b = 0$ using logistic weights\n$$\nw_1(x;\\beta) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-\\beta\\,(x - x_b)\\big)}, \\qquad\nw_0(x;\\beta) \\;=\\; 1 - w_1(x;\\beta),\n$$\nwith sharpness parameter $\\beta > 0$. The facies-aware blended predictive mean at a test input $x$ is\n$$\n\\hat{f}_\\text{blend}(x) \\;=\\; w_0(x;\\beta)\\,\\hat{f}_0(x) \\;+\\; w_1(x;\\beta)\\,\\hat{f}_1(x),\n$$\nwhere $\\hat{f}_0$ and $\\hat{f}_1$ are the local GP posterior means for facies $0$ and facies $1$, respectively.\n- Define a test set of $N_\\star = 201$ inputs uniformly spaced on $[-0.15, 0.15]$ to probe the extrapolation and interpolation behavior across the stratigraphic boundary. Compute the corresponding true outputs $f(x; J, S)$ at these test inputs.\n- Compute the Root Mean Square Error (RMSE) for both surrogates on the test set,\n$$\n\\mathrm{RMSE}_\\text{blend} \\;=\\; \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}\\big(\\hat{f}_\\text{blend}(x^\\star_i) - f(x^\\star_i; J, S)\\big)^2}, \\quad\n\\mathrm{RMSE}_\\text{mono} \\;=\\; \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}\\big(\\hat{f}_\\text{mono}(x^\\star_i) - f(x^\\star_i; J, S)\\big)^2}.\n$$\n- Decide whether facies awareness reduces extrapolation error across the stratigraphic boundary by checking whether $\\mathrm{RMSE}_\\text{blend} + \\epsilon < \\mathrm{RMSE}_\\text{mono}$ with a tolerance $\\epsilon = 10^{-3}$.\n\nYour program must implement the above for the following test suite of parameter tuples $(J, S, \\ell_0, \\ell_1, \\ell_g, \\beta)$:\n\n- Test A (general case with offset jump and shape change): $(1.0, 1, 0.15, 0.35, 0.50, 60.0)$.\n- Test B (boundary case with no jump and no shape difference): $(0.0, 0, 0.20, 0.20, 0.20, 60.0)$.\n- Test C (different smoothness across facies and moderate jump): $(0.5, 1, 0.10, 0.40, 0.30, 30.0)$.\n\nYour program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{False},\\text{True}]$. No physical units are involved; all quantities are dimensionless. All angles used within trigonometric functions are in radians. The final outputs must be booleans computed exactly as specified above for each test case in the given order.",
            "solution": "The problem presents a valid and well-posed computational task. It is scientifically grounded in the principles of Gaussian Process (GP) regression and its application to surrogate modeling in geophysics. The objective is to compare the performance of a monolithic GP against a facies-aware, blended GP for modeling a function with a potential discontinuity. All parameters, functions, and evaluation criteria are explicitly defined, making the problem self-contained and solvable.\n\nThe solution proceeds by first defining the true geophysical forward model and generating the prescribed training and testing datasets. Then, two types of GP surrogates are constructed: a single monolithic GP trained on all data, and two local GPs, each trained on data from a single geological facies. The predictions from the local GPs are then combined using a smooth partition-of-unity blending scheme. Finally, the accuracy of both the monolithic and the blended surrogates is evaluated on a test set spanning the facies boundary, and a decision is made based on which model yields a lower Root Mean Square Error (RMSE), indicating better handling of the boundary's extrapolation challenge.\n\nThe analytical and algorithmic steps are as follows:\n\n**1. Forward Model and Data Generation**\n\nThe true forward model $f(x; J, S)$ is defined piecewise based on the facies boundary at $x=0$.\nThe base function is $b(x) = \\sin(6x) - 0.1x$.\nFor facies $0$ ($x < 0$), the response is $f_0(x) = b(x)$.\nFor facies $1$ ($x \\ge 0$), the response is $f_1(x; J, S) = (1 - S)b(x) + S(0.8 \\cos(6x + 0.5) + 0.1x) + J$. Here, $J$ introduces a value jump and $S$ toggles a change in the functional form.\n\nTraining data is generated separately for each facies.\nFor facies $0$, $N_0 = 30$ input points $\\mathbf{X}_0$ are sampled uniformly from $[-1, -0.02]$. The corresponding outputs are $\\mathbf{y}_0 = f(\\mathbf{X}_0; J, S)$.\nFor facies $1$, $N_1 = 30$ input points $\\mathbf{X}_1$ are sampled uniformly from $[0.02, 1]$. The outputs are $\\mathbf{y}_1 = f(\\mathbf{X}_1; J, S)$.\nThe combined dataset for the monolithic model is $\\mathbf{X}_g = [\\mathbf{X}_0^T, \\mathbf{X}_1^T]^T$ and $\\mathbf{y}_g = [\\mathbf{y}_0^T, \\mathbf{y}_1^T]^T$.\n\n**2. Gaussian Process Surrogate Modeling**\n\nA GP is fully specified by its mean and covariance function. We assume a zero-mean prior for all GPs. The covariance is given by the squared exponential kernel with signal variance $\\sigma_f^2=1$:\n$$k(x, x'; \\ell) = \\exp\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right)$$\nFor a set of training inputs $\\mathbf{X}$ and outputs $\\mathbf{y}$, and test inputs $\\mathbf{X}_\\star$, the GP posterior predictive mean $\\mathbf{m}_\\star$ is given by:\n$$\\mathbf{m}_\\star = \\mathbf{K}_{\\star N} (\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}$$\nwhere $[\\mathbf{K}_{N N}]_{ij} = k(x_i, x_j)$, $[\\mathbf{K}_{\\star N}]_{ij} = k(x^\\star_i, x_j)$, and $\\sigma_n^2=10^{-8}$ is a small nugget for numerical stability.\n\nComputationally, we first solve the linear system $(\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I})\\alpha = \\mathbf{y}$ for the weight vector $\\alpha$. Then, the predictive mean is calculated as $\\mathbf{m}_\\star = \\mathbf{K}_{\\star N} \\alpha$.\n\nWe construct three sets of predictions:\n-   **Monolithic GP ($\\hat{f}_\\text{mono}$)**: Trained on the pooled data $(\\mathbf{X}_g, \\mathbf{y}_g)$ using a single kernel with length-scale $\\ell_g$.\n    $$\n    \\alpha_g = (\\mathbf{K}_{gg} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_g, \\quad \\text{where} \\; \\mathbf{K}_{gg} = k(\\mathbf{X}_g, \\mathbf{X}_g; \\ell_g)\n    $$\n    $$\n    \\hat{f}_\\text{mono}(x_\\star) = k(x_\\star, \\mathbf{X}_g; \\ell_g) \\alpha_g\n    $$\n\n-   **Local GP for Facies 0 ($\\hat{f}_0$)**: Trained on $(\\mathbf{X}_0, \\mathbf{y}_0)$ using a kernel with length-scale $\\ell_0$.\n    $$\n    \\alpha_0 = (\\mathbf{K}_{00} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_0, \\quad \\text{where} \\; \\mathbf{K}_{00} = k(\\mathbf{X}_0, \\mathbf{X}_0; \\ell_0)\n    $$\n    $$\n    \\hat{f}_0(x_\\star) = k(x_\\star, \\mathbf{X}_0; \\ell_0) \\alpha_0\n    $$\n\n-   **Local GP for Facies 1 ($\\hat{f}_1$)**: Trained on $(\\mathbf{X}_1, \\mathbf{y}_1)$ using a kernel with length-scale $\\ell_1$.\n    $$\n    \\alpha_1 = (\\mathbf{K}_{11} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_1, \\quad \\text{where} \\; \\mathbf{K}_{11} = k(\\mathbf{X}_1, \\mathbf{X}_1; \\ell_1)\n    $$\n    $$\n    \\hat{f}_1(x_\\star) = k(x_\\star, \\mathbf{X}_1; \\ell_1) \\alpha_1\n    $$\n\n**3. Blending via Partition of Unity**\n\nThe local GP predictions are combined into a single, continuous prediction using a partition-of-unity scheme based on logistic functions. The weights depend on the distance to the boundary $x_b=0$:\n$$\nw_1(x;\\beta) = \\frac{1}{1 + \\exp(-\\beta x)}, \\quad w_0(x;\\beta) = 1 - w_1(x;\\beta)\n$$\nThe sharpness parameter $\\beta$ controls the steepness of the transition. The blended predictive mean $\\hat{f}_\\text{blend}(x)$ is a weighted average of the local GP means:\n$$\n\\hat{f}_\\text{blend}(x) = w_0(x;\\beta)\\,\\hat{f}_0(x) + w_1(x;\\beta)\\,\\hat{f}_1(x)\n$$\n\n**4. Evaluation and Decision**\n\nBoth surrogates are evaluated on a dense test set of $N_\\star = 201$ points $\\mathbf{X}_\\star$ uniformly spaced on $[-0.15, 0.15]$. This interval is chosen to scrutinize model performance near and across the boundary, where extrapolation from the disjoint training sets is required.\n\nThe performance is quantified by the Root Mean Square Error (RMSE) against the true function values $f(\\mathbf{X}_\\star; J, S)$.\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}(\\hat{f}(x^\\star_i) - f(x^\\star_i))^2}\n$$\nWe compute $\\mathrm{RMSE}_\\text{blend}$ and $\\mathrm{RMSE}_\\text{mono}$. The facies-aware approach is considered superior if its error is meaningfully smaller, as determined by the condition:\n$$\n\\mathrm{RMSE}_\\text{blend} + \\epsilon < \\mathrm{RMSE}_\\text{mono}\n$$\nwith a tolerance of $\\epsilon = 10^{-3}$. This procedure is repeated for each parameter set provided in the test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for comparing monolithic vs. blended GP surrogates.\n    \"\"\"\n    test_cases = [\n        # Test A (general case with offset jump and shape change)\n        (1.0, 1.0, 0.15, 0.35, 0.50, 60.0),\n        # Test B (boundary case with no jump and no shape difference)\n        (0.0, 0.0, 0.20, 0.20, 0.20, 60.0),\n        # Test C (different smoothness across facies and moderate jump)\n        (0.5, 1.0, 0.10, 0.40, 0.30, 30.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        J, S, l0, l1, lg, beta = params\n\n        # Constants and model parameters\n        N0, N1 = 30, 30\n        sigma_f_sq = 1.0\n        nugget = 1e-8\n        epsilon = 1e-3\n        N_star = 201\n\n        # Define the base function and the full forward model\n        def base_func(x):\n            return np.sin(6 * x) - 0.1 * x\n\n        def forward_model(x, J_val, S_val):\n            x = np.asarray(x)\n            result = np.zeros_like(x, dtype=float)\n            \n            mask0 = x < 0\n            result[mask0] = base_func(x[mask0])\n            \n            mask1 = x >= 0\n            f1_part1 = (1 - S_val) * base_func(x[mask1])\n            f1_part2 = S_val * (0.8 * np.cos(6 * x[mask1] + 0.5) + 0.1 * x[mask1])\n            result[mask1] = f1_part1 + f1_part2 + J_val\n            \n            return result\n\n        # Generate training data\n        X0 = np.linspace(-1.0, -0.02, N0)[:, np.newaxis]\n        y0 = forward_model(X0.flatten(), J, S)\n        \n        X1 = np.linspace(0.02, 1.0, N1)[:, np.newaxis]\n        y1 = forward_model(X1.flatten(), J, S)\n        \n        Xg = np.vstack((X0, X1))\n        yg = np.concatenate((y0, y1))\n\n        # Generate test data\n        X_star = np.linspace(-0.15, 0.15, N_star)[:, np.newaxis]\n        y_star = forward_model(X_star.flatten(), J, S)\n\n        # Define RBF kernel\n        def rbf_kernel(X1, X2, length_scale, variance):\n            sq_dist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n            return variance * np.exp(-0.5 / length_scale**2 * sq_dist)\n\n        # GP prediction function\n        def gp_predict(X_train, y_train, X_test, length_scale, variance, noise_var):\n            K_NN = rbf_kernel(X_train, X_train, length_scale, variance)\n            K_NN += noise_var * np.eye(X_train.shape[0])\n            \n            try:\n                # Solve (K_NN) * alpha = y_train\n                alpha = linalg.solve(K_NN, y_train, assume_a='pos')\n            except linalg.LinAlgError:\n                # Fallback for singular matrix\n                alpha = linalg.lstsq(K_NN, y_train)[0]\n                \n            K_star_N = rbf_kernel(X_test, X_train, length_scale, variance)\n            y_pred = K_star_N.dot(alpha)\n            return y_pred\n\n        # 1. Monolithic GP\n        y_hat_mono = gp_predict(Xg, yg, X_star, lg, sigma_f_sq, nugget)\n        \n        # 2. Local GPs\n        y_hat_0 = gp_predict(X0, y0, X_star, l0, sigma_f_sq, nugget)\n        y_hat_1 = gp_predict(X1, y1, X_star, l1, sigma_f_sq, nugget)\n\n        # 3. Blended GP\n        x_b = 0.0\n        w1 = 1.0 / (1.0 + np.exp(-beta * (X_star.flatten() - x_b)))\n        w0 = 1.0 - w1\n        y_hat_blend = w0 * y_hat_0 + w1 * y_hat_1\n\n        # Calculate RMSE for both models\n        rmse_mono = np.sqrt(np.mean((y_hat_mono - y_star)**2))\n        rmse_blend = np.sqrt(np.mean((y_hat_blend - y_star)**2))\n\n        # Decision\n        is_better = (rmse_blend + epsilon) < rmse_mono\n        results.append(is_better)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}