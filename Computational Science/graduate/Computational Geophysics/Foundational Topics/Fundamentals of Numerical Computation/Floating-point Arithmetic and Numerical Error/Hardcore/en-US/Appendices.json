{
    "hands_on_practices": [
        {
            "introduction": "Summing a long sequence of floating-point numbers is a ubiquitous operation in computational geophysics, from aggregating seismic energies to calculating log-likelihoods in inversions. While seemingly simple, naive summation can accumulate significant round-off error, particularly when numbers of vastly different magnitudes are involved. This exercise demonstrates the failure modes of naive summation and introduces powerful alternatives like Kahan and Neumaier compensated summation, which track and correct for the error in each step . By implementing and comparing these algorithms across different data distributions, you will gain a practical understanding of how to perform summations accurately and why robust methods are essential for scientific computation.",
            "id": "3596759",
            "problem": "You are given a computational task grounded in floating-point arithmetic and numerical error within computational geophysics. Consider a discrete wavefield amplitude sequence $\\{A_i\\}_{i=1}^N$ and define the energy sequence $\\{E_i\\}_{i=1}^N$ by $E_i = A_i^2$, with $E_i \\ge 0$. In finite precision arithmetic conforming to Institute of Electrical and Electronics Engineers (IEEE) 754 double precision, each basic operation is subject to rounding to the nearest representable floating-point number. Let the unit roundoff $u$ denote the maximum relative error of a single rounding, where $u = 2^{-53}$ for double precision.\n\nYour task is to implement and compare three summation schemes applied to $\\sum_{i=1}^N E_i$:\n- Naive sequential summation in double precision.\n- Kahan compensated summation in double precision.\n- Neumaier compensated summation in double precision.\n\nFor each scheme, define the summation error for a realized sequence as $\\varepsilon = S_{\\text{scheme}} - S_{\\text{ref}}$, where $S_{\\text{ref}}$ is a high-accuracy reference sum computed in software-emulated high-precision for the same finite-precision inputs, and $S_{\\text{scheme}}$ is the double-precision sum produced by the specified scheme. Over multiple independent realizations with different random seeds, compute the sample bias and sample variance of $\\varepsilon$. The bias is the sample mean of $\\varepsilon$, and the variance is the sample mean of $(\\varepsilon - \\text{bias})^2$.\n\nThe goal is to quantify how both bias and variance of the summation error depend on the distribution of the magnitudes of $E_i$ and on the order in which terms are summed. You must use three scientifically plausible energy magnitude distributions relevant to wavefield energies:\n1. Gaussian amplitude model: $A_i \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 1$, so that $E_i = A_i^2$ has a chi-square-like distribution of energy magnitudes.\n2. Log-normal energy model: $E_i \\sim \\operatorname{LogNormal}(\\mu,\\sigma)$ with $(\\mu,\\sigma) = (-2,2)$ to model heavy-tailed energy magnitudes typical in heterogeneous media.\n3. Bimodal energy mixture: $E_i$ takes $E_{\\text{small}}$ with probability $p$ and $E_{\\text{large}}$ with probability $1-p$, with $(E_{\\text{small}}, E_{\\text{large}}, p) = (10^{-12}, 1, 0.999)$, to create extreme scale separation typical when small scattered energy is overwhelmed by a few strong coherent components.\n\nIn addition to the distribution, for the bimodal case you must test sensitivity to order by summing in two deterministic orders: all small terms first followed by large terms, and all large terms first followed by small terms. For the Gaussian amplitude and log-normal cases, sum in the natural generation order.\n\nYou must implement the following:\n- A robust random number generator for reproducibility via fixed seeds.\n- Deterministic implementations of naive, Kahan, and Neumaier summation, all in double precision arithmetic.\n- A high-accuracy reference sum computed from the same double-precision inputs using a software algorithm that significantly reduces rounding error compared to naive double-precision summation.\n\nUse a test suite with the following parameter sets:\n- Case 1 (Gaussian amplitude energies): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 1.\n- Case 2 (Log-normal energies): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 2.\n- Case 3 (Bimodal, small-first order): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 3, deterministic order with all $E_{\\text{small}}$ first followed by all $E_{\\text{large}}$.\n- Case 4 (Bimodal, large-first order): $N = 500000$, seeds $\\{777, 888\\}$, distribution as in item 3, deterministic order with all $E_{\\text{large}}$ first followed by all $E_{\\text{small}}$.\n\nFor each case, your program must:\n- Generate the energy sequence $\\{E_i\\}$ in arbitrary units (a.u.).\n- Compute $S_{\\text{ref}}$ using a high-accuracy summation algorithm applied to the realized floating-point sequence.\n- Compute $S_{\\text{naive}}$, $S_{\\text{Kahan}}$, $S_{\\text{Neumaier}}$.\n- Compute the summation errors $\\varepsilon_{\\text{naive}}$, $\\varepsilon_{\\text{Kahan}}$, $\\varepsilon_{\\text{Neumaier}}$ for each seed.\n- Compute the sample bias and sample variance across the two seeds for each scheme.\n\nExpress all outputs in arbitrary units (a.u.) without any unit symbol and without rounding; the outputs are pure floating-point numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sub-list of six floats in the order $[\\text{bias}_{\\text{naive}}, \\text{var}_{\\text{naive}}, \\text{bias}_{\\text{Kahan}}, \\text{var}_{\\text{Kahan}}, \\text{bias}_{\\text{Neumaier}}, \\text{var}_{\\text{Neumaier}}]$. The final output must therefore be a list of four sub-lists, one per case, for example:\n`[[b1,v1,b2,v2,b3,v3],[b1,v1,b2,v2,b3,v3],[b1,v1,b2,v2,b3,v3],[b1,v1,b2,v2,b3,v3]]`.\n\nAll mathematical symbols that appear in this problem statement are expressed in LaTeX. No angles are involved. No percentages are involved.\n\nYour program must be a complete, runnable program that requires no user input and prints only the final single line of output in the specified format.",
            "solution": "The problem presents a valid and well-posed computational task in the domain of numerical analysis as applied to computational geophysics. It requires the implementation and comparison of three distinct summation algorithms—naive sequential sum, Kahan compensated summation, and Neumaier compensated summation—to quantify their accuracy when summing a sequence of floating-point numbers representing wavefield energy. The problem is scientifically grounded, providing plausible models for energy distributions and a clear methodology for assessing numerical error through statistical measures of bias and variance. All necessary parameters and conditions are specified, allowing for a unique and verifiable solution.\n\nThe core of the problem lies in understanding the accumulation of rounding errors in floating-point arithmetic. When summing a sequence of numbers $\\{x_i\\}_{i=1}^N$, the naive approach, $S_k = S_{k-1} + x_k$, can suffer from significant error accumulation, especially if small numbers are added to a running sum of large magnitude. This is because the low-order bits of the small number are lost during the floating-point addition.\n\nThe summation schemes are defined as follows:\n\n1.  **Naive Summation**: This is the most straightforward method. The sum $S$ is initialized to $0$ and each element $x_i$ of the sequence is added to it sequentially.\n    $$ S \\leftarrow S + x_i $$\n    In IEEE 754 double precision, a floating-point addition $a+b$ is computed as $\\text{fl}(a+b) = (a+b)(1+\\delta)$, where $|\\delta| \\le u$ and $u = 2^{-53}$ is the unit roundoff. When summing $N$ positive numbers, the forward error for naive summation can be bounded, but in the worst case, the relative error can grow proportionally to $N$.\n\n2.  **Kahan Summation**: This algorithm introduces a compensation variable, $c$, to accumulate the error lost in each addition.\n    Let $S$ be the running sum and $c$ be the running compensation, both initialized to $0$. For each element $x_i$:\n    \\begin{enumerate}\n        \\item $y \\leftarrow x_i - c$: Subtract the previous error from the current term.\n        \\item $T \\leftarrow S + y$: Add the corrected term to the sum.\n        \\item $c \\leftarrow (T - S) - y$: The new error is the part of $y$ that was lost when adding it to $S$. The term $(T - S)$ algebraically recovers the high-order part of $y$ that was successfully added to $S$. Subtracting $y$ from this isolates the negative of the low-order part of $y$ that was truncated.\n        \\item $S \\leftarrow T$: Update the sum.\n    \\end{enumerate}\n    The final sum is $S$. Kahan summation dramatically reduces the error growth, making the final error bound independent of $N$ and dependent only on the error in the input values themselves, plus a small constant factor times the sum of their magnitudes.\n\n3.  **Neumaier Summation**: This is a refinement of Kahan's algorithm that provides greater accuracy in cases where the magnitude of the next term to be added, $|x_i|$, is much larger than the magnitude of the running sum, $|S|$. In such scenarios, the Kahan step $y \\leftarrow x_i - c$ can lose precision if $|x_i| \\gg |c|$. Neumaier's algorithm avoids this.\n    Let $S$ be the running sum and $c$ be the running compensation, both initialized to $0$. For each element $x_i$:\n    \\begin{enumerate}\n        \\item $T \\leftarrow S + x_i$: Add the next term to the sum, potentially losing precision.\n        \\item If $|S| \\ge |x_i|$: $c \\leftarrow c + ((S - T) + x_i)$.\n        \\item Else: $c \\leftarrow c + ((x_i - T) + S)$.\n        \\item $S \\leftarrow T$: Update the sum.\n    \\end{enumerate}\n    The final sum is $S+c$. By checking the magnitudes of $S$ and $x_i$, Neumaier ensures that the subtraction to find the compensatory term is always done between the larger-magnitude number and the intermediate sum, preserving the most precision.\n\nThe problem requires testing these algorithms on three distributions of energy values $E_i = A_i^2 \\ge 0$.\n-   **Gaussian amplitude model**: $A_i \\sim \\mathcal{N}(0, 1^2)$ results in $E_i = A_i^2$ following a chi-squared distribution with one degree of freedom, $\\chi^2(1)$. This represents a mix of small and moderate energy values.\n-   **Log-normal energy model**: $E_i \\sim \\operatorname{LogNormal}(-2, 2)$ creates a heavy-tailed distribution with a few very large values and many small ones. This is a challenging case for naive summation.\n-   **Bimodal energy mixture**: A vast majority ($99.9\\%$) of terms are tiny ($10^{-12}$), and a few are large ($1$). This case is designed to expose a critical weakness of naive summation: summing small numbers after large ones leads to their contribution being completely lost (swamped). The two specified orderings, small-first and large-first, will highlight this dependency.\n\nFor reference, a high-accuracy sum $S_{\\text{ref}}$ is computed. An appropriate\nmethod for this is to use an algorithm that maintains precision, such as the one implemented in Python's `math.fsum`, which is based on an error-free transformation approach. The error for each scheme is $\\varepsilon = S_{\\text{scheme}} - S_{\\text{ref}}$. We analyze the sample bias, $\\mathbb{E}[\\varepsilon]$, and sample variance, $\\text{Var}[\\varepsilon]$, over two independent realizations (from seeds $777$ and $888$) to assess the statistical properties of the error. The bias measures the systematic error, while the variance measures the random component of the error.\n\nThe implementation will proceed by first setting up the random number generators, then defining Python functions for each of the three summation schemes. For each test case, we will generate the data for each seed, perform the summations, and compute the errors against the reference `math.fsum` result. Finally, the bias and variance for each scheme are calculated from the errors obtained across the two seeds. The final output will be a formatted list containing these six statistical measures for each of the four test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef naive_sum(arr: np.ndarray) - np.float64:\n    \"\"\"Computes the sum using native Python float summation.\"\"\"\n    # Using np.sum with a specified dtype is a fast and standard\n    # way to perform naive sequential summation in double precision.\n    return np.sum(arr, dtype=np.float64)\n\ndef kahan_sum(arr: np.ndarray) - np.float64:\n    \"\"\"Computes the sum using Kahan's compensated summation algorithm.\"\"\"\n    s = np.float64(0.0)\n    c = np.float64(0.0)  # A running compensation for the lost low-order bits.\n    for x in arr:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef neumaier_sum(arr: np.ndarray) - np.float64:\n    \"\"\"Computes the sum using Neumaier's improved summation algorithm.\"\"\"\n    s = np.float64(0.0)\n    c = np.float64(0.0)  # A running compensation.\n    for x in arr:\n        t = s + x\n        if abs(s) = abs(x):\n            # If s is bigger, c collects the error of x.\n            c += (s - t) + x\n        else:\n            # If x is bigger, c collects the error of s.\n            c += (x - t) + s\n        s = t\n    return s + c\n\ndef solve():\n    \"\"\"Main function to run the simulation and produce the output.\"\"\"\n    test_cases = [\n        {\n            \"name\": \"Gaussian amplitude\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"gaussian_amplitude\",\n            \"params\": {\"sigma\": 1.0},\n            \"order\": \"natural\",\n        },\n        {\n            \"name\": \"Log-normal\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"log_normal\",\n            \"params\": {\"mu\": -2.0, \"sigma\": 2.0},\n            \"order\": \"natural\",\n        },\n        {\n            \"name\": \"Bimodal, small-first\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"bimodal\",\n            \"params\": {\"E_small\": 1e-12, \"E_large\": 1.0, \"p\": 0.999},\n            \"order\": \"small_first\",\n        },\n        {\n            \"name\": \"Bimodal, large-first\",\n            \"N\": 500000,\n            \"seeds\": [777, 888],\n            \"distribution\": \"bimodal\",\n            \"params\": {\"E_small\": 1e-12, \"E_large\": 1.0, \"p\": 0.999},\n            \"order\": \"large_first\",\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        seeds = case[\"seeds\"]\n        dist = case[\"distribution\"]\n        params = case[\"params\"]\n        order = case[\"order\"]\n\n        errors_naive = []\n        errors_kahan = []\n        errors_neumaier = []\n\n        for seed in seeds:\n            rng = np.random.default_rng(seed)\n            E = None\n\n            if dist == \"gaussian_amplitude\":\n                A = rng.normal(loc=0.0, scale=params[\"sigma\"], size=N)\n                E = np.square(A, dtype=np.float64)\n            elif dist == \"log_normal\":\n                E = rng.lognormal(mean=params[\"mu\"], sigma=params[\"sigma\"], size=N).astype(np.float64)\n            elif dist == \"bimodal\":\n                p = params[\"p\"]\n                E_small = np.float64(params[\"E_small\"])\n                E_large = np.float64(params[\"E_large\"])\n                \n                # Generate a boolean mask by comparing uniform random numbers to p\n                is_small = rng.random(size=N)  p\n                E = np.where(is_small, E_small, E_large)\n\n            if order == \"small_first\":\n                E = np.sort(E)\n            elif order == \"large_first\":\n                E = np.sort(E)[::-1]\n\n            # Ensure all values are float64 for consistent input\n            E = E.astype(np.float64)\n\n            # High-accuracy reference sum\n            s_ref = math.fsum(E)\n\n            # Compute sums for each scheme\n            s_naive = naive_sum(E)\n            s_kahan = kahan_sum(E)\n            s_neumaier = neumaier_sum(E)\n\n            # Compute and store errors\n            errors_naive.append(s_naive - s_ref)\n            errors_kahan.append(s_kahan - s_ref)\n            errors_neumaier.append(s_neumaier - s_ref)\n\n        # Compute sample bias and variance\n        bias_naive = np.mean(errors_naive)\n        var_naive = np.var(errors_naive, ddof=0)\n        \n        bias_kahan = np.mean(errors_kahan)\n        var_kahan = np.var(errors_kahan, ddof=0)\n\n        bias_neumaier = np.mean(errors_neumaier)\n        var_neumaier = np.var(errors_neumaier, ddof=0)\n\n        case_results = [\n            bias_naive, var_naive,\n            bias_kahan, var_kahan,\n            bias_neumaier, var_neumaier\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string as per requirements\n    # e.g., \"[[r1,r2,...],[r1,r2,...]]\"\n    output_str = \"[\" + \",\".join(\n        \"[\" + \",\".join(f\"{val:.17g}\" for val in res) + \"]\" for res in all_results\n    ) + \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The calculation of a vector's Euclidean norm, $\\|x\\|_2$, is a fundamental building block in countless geophysical algorithms, including least-squares inversion and signal processing. A direct computation, however, is fragile; squaring vector components can easily cause intermediate overflow or underflow, yielding an incorrect result even if the true norm is well within the representable range of floating-point numbers. This practice guides you through the development of a robust, one-pass algorithm that uses dynamic scaling to avoid these pitfalls . Through derivation, proof, and implementation, you will master a technique that is central to high-quality numerical software.",
            "id": "3596774",
            "problem": "Consider the task of evaluating the Euclidean norm of a real-valued vector in double-precision floating-point arithmetic, which is widely used throughout computational geophysics for operations such as amplitude normalization and least-squares residual evaluation. Let a vector be denoted by $x = (x_1, x_2, \\dots, x_n) \\in \\mathbb{R}^n$ and its Euclidean norm be defined by\n$$\n\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}.\n$$\nNaively computing $\\sum_{i=1}^n x_i^2$ and then taking the square root can overflow or underflow in double-precision arithmetic when entries span many orders of magnitude.\n\nAssume arithmetic follows the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) in base $\\beta = 2$ with precision $p = 53$ (binary64, also known as double precision) and rounding to nearest, ties-to-even. Let the unit roundoff be $u = \\tfrac{1}{2}\\beta^{1-p} = 2^{-53}$. Assume a finite exponent range with smallest positive normal number approximately $2.225\\times 10^{-308}$ and largest finite number approximately $1.798\\times 10^{308}$, so that underflow to subnormal representations is possible. Throughout, when modeling rounding, use the standard floating-point model: for a basic operation $\\mathrm{op}\\in\\{+,-,\\times,\\div\\}$,\n$$\n\\mathrm{fl}(a \\ \\mathrm{op}\\ b) = (a \\ \\mathrm{op}\\ b)\\,(1+\\delta), \\quad |\\delta| \\le u,\n$$\nand for square root,\n$$\n\\mathrm{fl}(\\sqrt{y}) = \\sqrt{y}\\,(1+\\delta), \\quad |\\delta| \\le u,\n$$\nwhenever the exact result is within the representable range. Define the quantity\n$$\n\\gamma_k = \\frac{k u}{1 - k u}\n$$\nfor nonnegative integers $k$ such that $k u  1$.\n\nYou must devise and analyze a scaling-based algorithm to compute $\\|x\\|_2$ robustly in the presence of entries spanning many orders of magnitude. Your task comprises the following steps:\n\n1. Starting only from the definition of $\\|x\\|_2$ and the floating-point model above, derive a scaling strategy that ensures all intermediate operations stay within representable range whenever the exact $\\|x\\|_2$ is representable in binary64. The derivation must be purely from first principles, without introducing any shortcut formulas. Explain why this strategy avoids overflow and underflow during intermediate steps.\n\n2. Prove the correctness of your algorithm by establishing a loop invariant that relates the maintained scale and accumulated sum-of-squares to the exact quantity $\\|x\\|_2$. Your proof must not rely on external lemmas beyond the floating-point model specified above.\n\n3. Derive a forward error bound for the computed norm $\\widehat{\\|x\\|_2}$ of the form\n$$\n\\frac{\\left|\\widehat{\\|x\\|_2} - \\|x\\|_2\\right|}{\\|x\\|_2} \\le C(n) u + \\mathcal{O}(u^2),\n$$\nwhere $C(n)$ is an explicit, simple function of $n$. You must show how $C(n)$ arises from counting the floating-point operations in your algorithm and applying the stated model, and you must express the bound in terms of $\\gamma_k$.\n\n4. Implement your algorithm in a complete, runnable program. The implementation must follow your derivation and use double-precision arithmetic. The program must evaluate $\\|x\\|_2$ for each vector in the test suite below, using your scaling algorithm, and output the results.\n\nThe test suite is the following set of vectors, each specified explicitly to exercise different numerical regimes. All values are dimensionless real numbers:\n- Happy-path mixed magnitudes:\n  $$\n  x^{(1)} = \\left[\\,1.0,\\ 10^{-16},\\ 10^{16},\\ -10^{-3},\\ 3.141592653589793,\\ -2.718281828,\\ 10^{-200},\\ -10^{200}\\,\\right].\n  $$\n- Near-overflow regime with exact result representable:\n  $$\n  x^{(2)} = \\left[\\,10^{308},\\ 10^{308}\\,\\right].\n  $$\n- Near-underflow regime including subnormals:\n  $$\n  x^{(3)} = \\left[\\,10^{-320},\\ -10^{-320},\\ 2\\cdot 10^{-320},\\ -3\\cdot 10^{-320}\\,\\right].\n  $$\n- All zeros (boundary case):\n  $\n  x^{(4)} = \\left[\\,0.0,\\ 0.0,\\ 0.0\\,\\right].\n  $\n- Mixed extremes dominated by a very large entry:\n  $\n  x^{(5)} = \\left[\\,10^{308},\\ 10^{-320},\\ 3.0,\\ -4.0,\\ 10^{-150}\\,\\right].\n  $\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The results must be the five computed norms, in order, each as a floating-point number. For example, the output format must be exactly like\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5].\n$$",
            "solution": "The task is to devise, analyze, and implement a robust algorithm for computing the Euclidean norm $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$ of a vector $x \\in \\mathbb{R}^n$ in double-precision floating-point arithmetic. The analysis must proceed from first principles as laid out in the problem statement.\n\n### 1. Derivation of a Scaling-Based Algorithm\n\nThe naive computation of $\\|x\\|_2$ involves first summing the squares of the elements, $\\sum_{i=1}^n x_i^2$, and then taking the square root. In floating-point arithmetic, the intermediate squaring operation $x_i^2$ can easily lead to overflow if $|x_i|$ is large (e.g., $10^{155}$) or harmful underflow if $|x_i|$ is small (e.g., $10^{-155}$), even when the final norm $\\|x\\|_2$ is perfectly representable. For instance, if $x = (10^{200}, 10^{200})$, $\\|x\\|_2 = \\sqrt{2} \\cdot 10^{200}$ is representable, but the intermediate term $x_i^2 = 10^{400}$ overflows the approximate range of double-precision numbers, which is up to $1.8 \\times 10^{308}$.\n\nTo prevent this, we introduce a scaling factor. The key identity is:\n$$\n\\|x\\|_2 = s \\cdot \\sqrt{\\sum_{i=1}^n \\left(\\frac{x_i}{s}\\right)^2} \\quad \\text{for any } s  0.\n$$\nA two-pass algorithm could first find the maximum absolute value $s = \\max_i |x_i|$, and then, in a second pass, compute the sum of squares of the scaled vector $y = x/s$. In this case, all elements of $y$ satisfy $|y_i| \\le 1$, so squaring them cannot cause overflow.\n\nA more efficient one-pass algorithm can be developed by dynamically updating the scale as we iterate through the vector elements. We maintain a running scale, let's call it $s$, and a running sum of squares, let's call it $\\sigma$. The state $(s, \\sigma)$ at any point represents the norm of the subvector processed so far.\n\nLet's formalize the one-pass algorithm. We initialize a scale variable $s \\leftarrow 0$ and a sum-of-squares variable $\\sigma \\leftarrow 0$. We iterate through the elements $x_i$ for $i=1, \\dots, n$. For each non-zero element $x_i$, we let $a = |x_i|$ and update $s$ and $\\sigma$ as follows:\n\n1.  **Initialization**: If $s=0$ (i.e., this is the first non-zero element encountered), we set the initial scale $s \\leftarrow a$ and initialize the sum of squares $\\sigma \\leftarrow 1$.\n\n2.  **Scale Update**: If $s  a$, the current element $a$ is larger in magnitude than any element seen so far. We must adopt $a$ as the new scale. The current accumulated squared norm is $s^2\\sigma$. The new squared norm will be $s^2\\sigma + a^2$. To prevent overflow, we factor out the new scale $a$:\n    $$\n    s^2\\sigma + a^2 = a^2 \\left( \\sigma \\left(\\frac{s}{a}\\right)^2 + 1 \\right).\n    $$\n    So, we update the state by setting the new sum of squares $\\sigma \\leftarrow 1 + \\sigma(s/a)^2$ and the new scale $s \\leftarrow a$. The ratio $|s/a|  1$, preventing overflow in its squaring.\n\n3.  **Summation**: If $s \\ge a$, the current scale $s$ is sufficient. The current accumulated squared norm is $s^2\\sigma$. The new squared norm is $s^2\\sigma + a^2$. We factor out the existing scale $s$:\n    $$\n    s^2\\sigma + a^2 = s^2 \\left( \\sigma + \\left(\\frac{a}{s}\\right)^2 \\right).\n    $$\n    So, we update the sum of squares $\\sigma \\leftarrow \\sigma + (a/s)^2$, while the scale $s$ remains unchanged. The ratio $|a/s| \\le 1$, again preventing overflow.\n\nAfter iterating through all elements, the final norm is computed as $s \\sqrt{\\sigma}$. If the input vector contains only zeros, the scale $s$ will remain $0$, and the result is correctly $0$.\n\n**Why this strategy is robust:**\n-   **Overflow Avoidance**: All divisions involve a ratio of a smaller-or-equal magnitude number to a larger one. The result of the division is always in $[-1, 1]$. Squaring this value gives a result in $[0, 1]$, which cannot overflow. The sum of squares $\\sigma$ can grow, but it is bounded by $1 \\le \\sigma \\le n$. For any practical vector dimension $n$, $\\sigma$ will not overflow. The final multiplication $s \\cdot \\sqrt{\\sigma}$ will only overflow if the true norm $\\|x\\|_2$ itself is not representable, which is the correct behavior.\n-   **Underflow Management**: If $|x_i|$ is much smaller than the current scale $s$, the ratio $|x_i/s|$ will be tiny, and its square may underflow to $0$. This is known as \"benign underflow.\" It means the contribution of $x_i^2$ is negligible compared to the squared norm of the largest elements. The loss of such a small term introduces a very small relative error in the final result, which is an acceptable trade-off for robustness.\n\n### 2. Proof of Correctness\n\nWe prove the correctness of the algorithm in exact arithmetic using a loop invariant. Let $(s_k, \\sigma_k)$ be the state of the scale and sum-of-squares variables after processing the first $k$ elements $(x_1, \\dots, x_k)$. We handle the all-zero vector as a trivial case, so assume there is at least one non-zero element.\n\n**Loop Invariant**: For $k \\ge 1$ where $x_k$ is the last element processed from a non-zero subvector $(x_1, \\dots, x_k)$:\n1.  $s_k = \\max_{i=1}^k |x_i|$.\n2.  $\\|(x_1, \\dots, x_k)\\|_2^2 = s_k^2 \\sigma_k$.\n\n**Base Case**: Let $x_j$ be the first non-zero element. After processing it, the state becomes $(s_j, \\sigma_j)$. The algorithm sets $s_j = |x_j|$ and $\\sigma_j = 1$.\n1.  $s_j = |x_j| = \\max_{i=1}^j |x_i|$ holds.\n2.  $\\|(x_1, \\dots, x_j)\\|_2^2 = x_j^2 = |x_j|^2 \\cdot 1 = s_j^2 \\sigma_j$ holds.\nThe invariant holds for the base case.\n\n**Inductive Step**: Assume the invariant holds for step $k$. We process element $x_{k+1}$ and show the invariant holds for the new state $(s_{k+1}, \\sigma_{k+1})$. Let $a = |x_{k+1}|$. By the inductive hypothesis, $\\|(x_1, \\dots, x_k)\\|_2^2 = s_k^2 \\sigma_k$. The target squared norm is $\\|(x_1, \\dots, x_{k+1})\\|_2^2 = \\|(x_1, \\dots, x_k)\\|_2^2 + x_{k+1}^2 = s_k^2 \\sigma_k + a^2$.\n\n**Case 1: $s_k  a$**\nThe algorithm updates $s_{k+1} \\leftarrow a$ and $\\sigma_{k+1} \\leftarrow 1 + \\sigma_k (s_k/a)^2$.\n1.  $s_{k+1} = a = |x_{k+1}|$. Since $s_k = \\max_{i=1}^k |x_i|$ and $s_k  |x_{k+1}|$, we have $s_{k+1} = \\max_{i=1}^{k+1} |x_i|$.\n2.  The new computed squared norm is $s_{k+1}^2 \\sigma_{k+1} = a^2 \\left(1 + \\sigma_k \\left(\\frac{s_k}{a}\\right)^2\\right) = a^2 + a^2\\sigma_k\\frac{s_k^2}{a^2} = a^2 + s_k^2 \\sigma_k$. This matches the target squared norm.\n\n**Case 2: $s_k \\ge a$**\nThe algorithm updates $s_{k+1} \\leftarrow s_k$ and $\\sigma_{k+1} \\leftarrow \\sigma_k + (a/s_k)^2$.\n1.  $s_{k+1} = s_k$. Since $s_k = \\max_{i=1}^k |x_i|$ and $s_k \\ge |x_{k+1}|$, we have $s_{k+1} = \\max_{i=1}^{k+1} |x_i|$.\n2.  The new computed squared norm is $s_{k+1}^2 \\sigma_{k+1} = s_k^2 \\left(\\sigma_k + \\left(\\frac{a}{s_k}\\right)^2\\right) = s_k^2 \\sigma_k + s_k^2\\frac{a^2}{s_k^2} = s_k^2 \\sigma_k + a^2$. This also matches the target squared norm.\n\nSince the invariant holds for the base case and the inductive step, it holds for all $k$ from $1$ to $n$. At the end of the loop ($k=n$), we have $s_n = \\max_{i=1}^n |x_i|$ and $\\|x\\|_2^2 = s_n^2 \\sigma_n$. Taking the square root, we get $\\|x\\|_2 = s_n \\sqrt{\\sigma_n}$, which is precisely what the algorithm computes as its final step. This proves the algorithm's correctness in exact arithmetic.\n\n### 3. Forward Error Bound\n\nWe derive a forward error bound for the computed norm $\\widehat{\\|x\\|_2}$. The analysis of the one-pass algorithm is complex due to the state-dependent updates. A simplified but insightful analysis can be performed on the equivalent two-pass algorithm, which has similar numerical properties.\n\n1.  **Scaling**: Let $s = \\max_i |x_i|$. This value is one of the $|x_i|$ and is assumed to be represented exactly in floating point.\n2.  **Division**: For each $i$, we compute $y_i = x_i/s$. The floating-point operation gives $\\hat{y}_i = \\mathrm{fl}(x_i/s) = (x_i/s)(1+\\delta_i)$ with $|\\delta_i| \\le u$.\n3.  **Squaring**: We compute $\\hat{z}_i = \\mathrm{fl}(\\hat{y}_i^2) = \\hat{y}_i^2(1+\\epsilon_i)$. Substituting $\\hat{y}_i$, we get $\\hat{z}_i = (x_i/s)^2(1+\\delta_i)^2(1+\\epsilon_i) = z_i(1 + 2\\delta_i + \\epsilon_i + \\mathcal{O}(u^2))$, where $z_i=(x_i/s)^2$. The relative error for computing each squared term $\\hat{z}_i$ is bounded by $3u + \\mathcal{O}(u^2)$.\n4.  **Summation**: We compute the sum $\\hat{\\sigma} = \\mathrm{fl}(\\sum_{i=1}^n \\hat{z}_i)$. Using the standard model for floating-point summation, the computed sum is $\\hat{\\sigma} = \\sum_{i=1}^n \\hat{z}_i(1+\\theta_i)$, where $|\\theta_i| \\le \\gamma_{n-1}$ is a conservative bound (a tighter bound depends on the order of summation).\n5.  **Error in $\\sigma$**: Combining these errors, $\\hat{\\sigma} = \\sum_{i=1}^n z_i(1+3u+\\mathcal{O}(u^2))(1+\\gamma_{n-1})$. Let $\\sigma = \\sum z_i$. The relative error in $\\hat{\\sigma}$ is:\n    $$\n    \\frac{|\\hat{\\sigma} - \\sigma|}{\\sigma} = \\frac{|\\sum z_i(1+\\eta_i) - \\sum z_i|}{\\sum z_i} \\le \\max_i |\\eta_i| \\approx 3u + \\gamma_{n-1} \\approx (3+n-1)u = (n+2)u.\n    $$\n    So we can write $\\hat{\\sigma} = \\sigma (1 + \\Delta_\\sigma)$ where $|\\Delta_\\sigma| \\le \\gamma_{n+2} + \\mathcal{O}(u^2)$.\n6.  **Square Root**: We compute $\\hat{\\rho} = \\mathrm{fl}(\\sqrt{\\hat{\\sigma}}) = \\sqrt{\\hat{\\sigma}}(1+\\delta_\\rho)$, $|\\delta_\\rho| \\le u$.\n    $$\n    \\hat{\\rho} = \\sqrt{\\sigma(1+\\Delta_\\sigma)}(1+\\delta_\\rho) = \\sqrt{\\sigma}\\sqrt{1+\\Delta_\\sigma}(1+\\delta_\\rho) \\approx \\sqrt{\\sigma}(1+\\Delta_\\sigma/2)(1+\\delta_\\rho) \\approx \\sqrt{\\sigma}(1+\\Delta_\\sigma/2 + \\delta_\\rho).\n    $$\n7.  **Final Multiplication**: The final result is $\\widehat{\\|x\\|_2} = \\mathrm{fl}(s \\cdot \\hat{\\rho}) = s\\hat{\\rho}(1+\\delta_m)$, $|\\delta_m| \\le u$.\n    $$\n    \\widehat{\\|x\\|_2} \\approx s\\sqrt{\\sigma}(1 + \\Delta_\\sigma/2 + \\delta_\\rho + \\delta_m) = \\|x\\|_2 (1 + \\Delta_\\sigma/2 + \\delta_\\rho + \\delta_m).\n    $$\nThe total relative error is thus bounded by:\n$$\n\\frac{|\\widehat{\\|x\\|_2} - \\|x\\|_2|}{\\|x\\|_2} \\le \\frac{|\\Delta_\\sigma|}{2} + |\\delta_\\rho| + |\\delta_m| \\le \\frac{\\gamma_{n+2}}{2} + u + u.\n$$\nFor small $n u$, $\\gamma_{n+2} \\approx (n+2)u$. The bound becomes:\n$$\n\\frac{|\\widehat{\\|x\\|_2} - \\|x\\|_2|}{\\|x\\|_2} \\le \\frac{(n+2)u}{2} + 2u = \\left(\\frac{n}{2} + 1 + 2\\right)u = \\frac{n+6}{2} u + \\mathcal{O}(u^2).\n$$\nThus, we found an explicit function $C(n) = \\frac{n+6}{2}$. The derived bound demonstrates that the algorithm is numerically stable, with the error growing linearly with the vector dimension $n$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of robustly computing the Euclidean norm for a suite of test vectors.\n    \"\"\"\n\n    def robust_euclidean_norm(x: np.ndarray) - np.float64:\n        \"\"\"\n        Computes the Euclidean norm of a vector using a one-pass, scaling-based algorithm\n        to prevent unnecessary overflow and underflow.\n\n        This algorithm is based on the principle of dynamically maintaining a scale factor\n        and a sum of squares. It is equivalent to LAPACK's 'xnrm2' routine.\n\n        Args:\n            x: A numpy array of float64 values.\n\n        Returns:\n            The computed Euclidean norm as a float64.\n        \"\"\"\n        if x.size == 0:\n            return np.float64(0.0)\n\n        # Initialize scale and sum_sq. Using the LAPACK 'dnrm2' initialization.\n        # scale is the largest absolute value encountered so far.\n        # sum_sq is the sum of squares of elements scaled by 1/scale.\n        scale = np.float64(0.0)\n        sum_sq = np.float64(1.0)\n        \n        # Another initialization strategy also works:\n        # scale = np.float64(0.0)\n        # sum_sq = np.float64(0.0)\n        # This requires an explicit check for scale == 0.0 inside the loop.\n        # The LAPACK method combines initialization and update more seamlessly.\n        \n        for val in x:\n            # We must use Python's built-in abs() or numpy's for intermediate\n            # calculations to maintain precision before casting to float64 if needed.\n            abs_val = np.abs(val)\n            \n            if abs_val  0:\n                if scale  abs_val:\n                    # Current element is larger than the current scale.\n                    # Rescale the sum of squares and update the scale.\n                    # The ratio is guaranteed to be  1.\n                    ratio = scale / abs_val\n                    sum_sq = np.float64(1.0) + sum_sq * (ratio * ratio)\n                    scale = abs_val\n                else:\n                    # Current element is smaller than or equal to the scale.\n                    # Add its contribution to the sum of squares.\n                    # The ratio is guaranteed to be = 1.\n                    ratio = abs_val / scale\n                    sum_sq = sum_sq + (ratio * ratio)\n\n        # After the loop, the norm is scale * sqrt(sum_sq).\n        # If the vector was all zeros, scale remains 0.0.\n        # Multiplying by 0.0 gives the correct result of 0.0.\n        return scale * np.sqrt(sum_sq)\n\n    # Define the test cases from the problem statement as numpy arrays of double precision floats.\n    test_cases = [\n        np.array([1.0, 1e-16, 1e16, -1e-3, 3.141592653589793, -2.718281828, 1e-200, -1e200], dtype=np.float64),\n        np.array([1e308, 1e308], dtype=np.float64),\n        np.array([1e-320, -1e-320, 2e-320, -3e-320], dtype=np.float64),\n        np.array([0.0, 0.0, 0.0], dtype=np.float64),\n        np.array([1e308, 1e-320, 3.0, -4.0, 1e-150], dtype=np.float64),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = robust_euclidean_norm(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # We use a custom formatting to avoid scientific notation for some numbers\n    # and ensure the output matches typical floating point representations.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Implementing physical models often requires translating complex mathematical expressions into code, a process where numerical stability is paramount. The damping coefficients in Perfectly Matched Layers (PMLs), essential for absorbing outgoing waves in simulations, provide a perfect case study. Their direct evaluation can suffer from both catastrophic cancellation in expressions like $1 - \\exp(-z)$ and premature underflow in polynomial terms . This hands-on practice challenges you to diagnose these issues and implement a robust solution using mathematically equivalent reformulations, such as Taylor series approximations and specialized library functions, thereby ensuring the physical model remains accurate in the finite-precision world of the computer.",
            "id": "3596709",
            "problem": "Consider a one-dimensional Perfectly Matched Layer (PML) used in time-domain wave simulations, where the damping profile is defined on a dimensionless normalized coordinate $\\xi \\in [0,1]$ by\n$$\n\\sigma(\\xi) = \\sigma_{\\max} \\, \\xi^{m},\n$$\nwith $\\sigma_{\\max}  0$ and integer $m \\ge 1$. For enforcing boundary conditions in a discrete update with time step $\\Delta t  0$, a common pair of coefficients is\n$$\nA(\\xi) = \\exp\\!\\big(-\\sigma(\\xi)\\,\\Delta t\\big), \\qquad B(\\xi) = \\frac{1 - \\exp\\!\\big(-\\sigma(\\xi)\\,\\Delta t\\big)}{\\sigma(\\xi)},\n$$\nwith the continuous limiting value $B(0) = \\Delta t$ when $\\sigma(0) = 0$. You will examine how floating-point rounding and subnormal numbers affect the evaluation of these coefficients near $\\xi = 0$, and you will construct numerically robust approximations.\n\nFundamental base and assumptions:\n- IEEE 754 binary64 (double precision) arithmetic is used, where the unit roundoff is $u \\approx 2^{-53}$ and the subnormal range yields degraded relative accuracy and increased susceptibility to underflow.\n- The rounding model for a single elementary operation $\\operatorname{fl}(\\cdot)$ is $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\le u$ when the result is normal; when the result is subnormal, this bound can be violated.\n- The Taylor expansion of the exponential function about $0$ is valid: $\\exp(z) = \\sum_{k=0}^{\\infty} \\frac{z^{k}}{k!}$ for all real $z$.\n\nYour tasks are:\n1) Implement a naive baseline that directly evaluates $\\sigma(\\xi) = \\sigma_{\\max}\\,\\xi^{m}$ using the built-in power, then computes $A(\\xi) = \\exp(-\\sigma(\\xi)\\Delta t)$ and $B(\\xi) = \\big(1 - \\exp(-\\sigma(\\xi)\\Delta t)\\big)/\\sigma(\\xi)$ with the convention $B(0) = \\Delta t$.\n2) Derive and implement a robust evaluation based on first principles:\n   - For $\\xi^{m}$, use scaling to avoid excessive underflow before the final result: let $\\xi = s \\cdot 2^{e}$ with $s \\in [\\tfrac{1}{2},1)$ and integer $e$ (this is the standard binary decomposition). Then compute\n     $$\n     \\xi^{m} = s^{m}\\,2^{em}\n     $$\n     and form $\\sigma(\\xi) = \\sigma_{\\max} \\, s^{m} \\, 2^{em}$ using a base that maintains normal intermediate values when possible.\n   - For small $z = \\sigma(\\xi)\\Delta t$, avoid catastrophic cancellation in $1 - \\exp(-z)$ by using a polynomial approximation of the function\n     $$\n     \\varphi_{1}(z) = \\frac{1 - \\exp(-z)}{z}\n     $$\n     derived from the Taylor series of $\\exp(-z)$ and evaluated by Horner's method. Use a scaled variable if needed so that coefficients and arguments stay within a normal range. For moderate and large $|z|$, use a formulation that avoids subtractive cancellation by replacing $1 - \\exp(-z)$ with $-\\operatorname{expm1}(-z)$, where $\\operatorname{expm1}$ denotes the accurately rounded computation of $\\exp(w) - 1$.\n   - Combine these to compute robust approximations to $A(\\xi)$ and $B(\\xi) = \\Delta t \\, \\varphi_{1}(z)$ that remain accurate near $\\xi = 0$, including in the subnormal regime.\n3) For accuracy assessment, use a high-precision reference computed in base-10 arbitrary precision with at least $100$ significant digits. Compare both naive and robust double-precision results to this reference using the relative error\n$$\n\\varepsilon_{\\mathrm{rel}}(u) = \\frac{|u_{\\mathrm{float}} - u_{\\mathrm{ref}}|}{|u_{\\mathrm{ref}}|},\n$$\nor the absolute error $|u_{\\mathrm{float}} - u_{\\mathrm{ref}}|$ if $u_{\\mathrm{ref}} = 0$.\n4) Evaluate the maximum error over a prescribed grid of $\\xi$-values that heavily samples the region near $\\xi = 0$, and report the following four scalars for each parameter set: the maximum relative error for $A(\\xi)$ with the naive method, the maximum relative error for $B(\\xi)$ with the naive method, and the same two maxima for the robust method.\n\nTest suite and required outputs:\n- Use the following three parameter sets, where all quantities are dimensionless:\n  - Case 1 (happy path, moderate damping exponent): $(\\sigma_{\\max}, m, \\Delta t) = (200, 3, 10^{-3})$.\n  - Case 2 (strong damping exponent, subnormal-prone): $(\\sigma_{\\max}, m, \\Delta t) = (1000, 8, 10^{-4})$.\n  - Case 3 (very small time step, cancellation-prone): $(\\sigma_{\\max}, m, \\Delta t) = (10^{6}, 2, 10^{-12})$.\n- For each case, evaluate on the grid\n$$\n\\Xi = \\{\\,0\\,\\} \\cup \\{\\,2^{-10},\\,2^{-30},\\,2^{-60},\\,2^{-120},\\,2^{-240},\\,2^{-480},\\,2^{-960},\\,2^{-1060}\\,\\} \\cup \\{\\,1\\,\\}.\n$$\n- For each case, compute and return a list\n$$\n\\left[ E^{A}_{\\mathrm{naive}},\\; E^{B}_{\\mathrm{naive}},\\; E^{A}_{\\mathrm{robust}},\\; E^{B}_{\\mathrm{robust}} \\right],\n$$\nwhere each entry is the maximum error over $\\Xi$, using relative error except absolute error if the reference is zero.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the four-entry list for one case, in the order Case 1, Case 2, Case 3. For example, a syntactically correct format is `[[e11,e12,e13,e14],[e21,e22,e23,e24],[e31,e32,e33,e34]]`. No additional text should be printed.",
            "solution": "The problem requires a critical analysis and robust implementation of expressions used for Perfectly Matched Layer (PML) boundary conditions in numerical wave simulations. The core task is to mitigate floating-point errors, specifically underflow and catastrophic cancellation, that arise when evaluating the coefficients $A(\\xi)$ and $B(\\xi)$ for small values of the spatial coordinate $\\xi$.\n\nThe analysis will proceed in three stages: first, establishing a high-precision reference for accuracy assessment; second, analyzing the failure modes of a naive, direct implementation; and third, deriving and implementing a numerically robust algorithm based on first principles.\n\nLet the given expressions be:\n$$\n\\sigma(\\xi) = \\sigma_{\\max} \\, \\xi^{m}\n$$\n$$\nA(\\xi) = \\exp\\!\\big(-\\sigma(\\xi)\\,\\Delta t\\big)\n$$\n$$\nB(\\xi) = \\frac{1 - \\exp\\!\\big(-\\sigma(\\xi)\\,\\Delta t\\big)}{\\sigma(\\xi)}\n$$\nwith the limiting value $B(0) = \\Delta t$. Let $z = \\sigma(\\xi)\\Delta t$. The expression for $B(\\xi)$ can be rewritten in terms of $z$ and the time step $\\Delta t$ as $B(\\xi) = \\Delta t \\cdot \\varphi_1(z)$, where $\\varphi_1(z) = \\frac{1 - \\exp(-z)}{z}$.\n\n### High-Precision Reference Calculation\n\nTo quantitatively assess the accuracy of floating-point implementations, we first compute reference values for $A(\\xi)$ and $B(\\xi)$ using arbitrary-precision arithmetic. A precision of 120 decimal digits is chosen, which is substantially higher than the approximately $16$ digits of IEEE $754$ double precision, ensuring that these reference values can be treated as effectively exact for the purpose of error calculation. For each pair of parameters $(\\sigma_{\\max}, m, \\Delta t)$ and each $\\xi$ in the evaluation grid $\\Xi$, the values of $\\sigma(\\xi)$, $A(\\xi)$, and $B(\\xi)$ are calculated directly from their definitions using a high-precision library. For $\\xi=0$, we have $\\sigma(0)=0$, $A(0)=1$, and $B(0)=\\Delta t$.\n\n### Analysis of the Naive Implementation\n\nA naive implementation translates the mathematical formulas directly into standard floating-point operations.\n\n$1$. **$\\sigma(\\xi) = \\sigma_{\\max} \\cdot \\xi^m$**: This is computed as `sigma_max * xi**m`. For very small $\\xi$, such as $\\xi = 2^{-240}$, and a moderately large exponent $m$, say $m=8$, the intermediate result $\\xi^m = (2^{-240})^8 = 2^{-1920}$ overwhelmingly underflows to $0.0$ in double precision. The smallest positive normal double-precision number is approximately $2^{-1022}$, and the smallest subnormal is approximately $2^{-1074}$. Consequently, $\\sigma(\\xi)$ is prematurely evaluated as $0$ for many small, non-zero values of $\\xi$.\n\n$2$. **$A(\\xi) = \\exp(-\\sigma(\\xi)\\Delta t)$**: The primary source of error in $A(\\xi)$ is the error in its argument, which stems from the flawed computation of $\\sigma(\\xi)$. If $\\sigma(\\xi)$ incorrectly underflows to $0$, $A(\\xi)$ will be incorrectly computed as $\\exp(0) = 1$.\n\n$3$. **$B(\\xi) = (1 - A(\\xi)) / \\sigma(\\xi)$**: This expression suffers from two major issues.\n- **Catastrophic Cancellation**: When $z = \\sigma(\\xi)\\Delta t$ is close to $0$, $\\exp(-z)$ is close to $1$. The subtraction $1 - \\exp(-z)$ loses most of its significant digits. For instance, if $z \\approx 10^{-12}$, then $\\exp(-z) \\approx 1 - 10^{-12}$, and $1 - \\exp(-z)$ will be computed with large relative error.\n- **Division by Zero**: If the naive computation of $\\sigma(\\xi)$ underflows to $0$ for a non-zero $\\xi$, this expression leads to a division by zero or an indeterminate form $0/0$. The special case handling for $\\xi=0$ is insufficient to address this premature underflow for $\\xi  0$.\n\nThese failure modes are expected to cause large relative errors, particularly for the test cases with small $\\xi$ and large $m$.\n\n### Derivation of the Robust Implementation\n\nA robust implementation must systematically address each of the identified failure points.\n\n$1$. **Robust Computation of $\\sigma(\\xi)$**:\nThe problem suggests avoiding intermediate underflow in $\\xi^m$ by decomposing $\\xi$ into its binary representation, $\\xi = s \\cdot 2^e$, where $s \\in [0.5, 1)$ is the significand and $e$ is the integer exponent. This decomposition can be achieved using the `frexp` function. Then, the power can be computed as $\\xi^m = (s \\cdot 2^e)^m = s^m \\cdot 2^{em}$. Multiplying by $\\sigma_{\\max}$ gives $\\sigma(\\xi) = \\sigma_{\\max} \\cdot s^m \\cdot 2^{em}$. This product can be computed accurately using the `ldexp(x, exp)` function, which calculates $x \\cdot 2^{\\text{exp}}$, effectively scaling the number by a power of $2$ without performing a multiplication that might underflow or overflow. The robust calculation is thus $\\sigma(\\xi) = \\operatorname{ldexp}(\\sigma_{\\max} \\cdot s^m, em)$. This form keeps the intermediate values within the normal floating-point range as long as possible, deferring potential underflow to the final result, which is the best possible outcome. For $\\xi=0$, $\\operatorname{frexp}(0)$ returns $(0, 0)$, correctly yielding $\\sigma(0)=0$.\n\n$2$. **Robust Computation of $A(\\xi)$**:\nWith an accurate value of $\\sigma(\\xi)$ from the robust method above, the argument $z = \\sigma(\\xi)\\Delta t$ can be computed with high fidelity. The standard library function $\\exp(-z)$ is itself highly accurate. Therefore, the robust computation of $A(\\xi)$ simply involves using the robustly computed $\\sigma(\\xi)$ as input:\n$$\nA_{\\text{robust}}(\\xi) = \\exp(-\\sigma_{\\text{robust}}(\\xi)\\,\\Delta t)\n$$\nThis directly circumvents the issue of $\\sigma(\\xi)$ prematurely becoming zero.\n\n$3$. **Robust Computation of $B(\\xi)$**:\nThe expression for $B(\\xi)$ is best analyzed through the function $\\varphi_1(z) = \\frac{1-\\exp(-z)}{z}$. The goal is to compute $B(\\xi) = \\Delta t \\cdot \\varphi_1(z)$ robustly.\n\n- **For small $|z|$**: We use the Taylor series expansion of $\\varphi_1(z)$ around $z=0$:\n$$\n\\exp(-z) = \\sum_{k=0}^{\\infty} \\frac{(-z)^k}{k!} = 1 - z + \\frac{z^2}{2!} - \\frac{z^3}{3!} + \\cdots\n$$\n$$\n1 - \\exp(-z) = z - \\frac{z^2}{2!} + \\frac{z^3}{3!} - \\cdots = \\sum_{k=1}^{\\infty} \\frac{(-1)^{k+1}z^k}{k!}\n$$\nDividing by $z$ gives the series for $\\varphi_1(z)$:\n$$\n\\varphi_1(z) = 1 - \\frac{z}{2!} + \\frac{z^2}{3!} - \\cdots = \\sum_{k=0}^{\\infty} \\frac{(-z)^k}{(k+1)!}\n$$\nThis series avoids the subtractive cancellation. It can be evaluated efficiently and accurately using Horner's method for a fixed number of terms. A threshold must be chosen for $|z|$ below which this approximation is used. A value like $10^{-8}$ is a conservative choice, as for $|z|  10^{-8}$, the direct computation of $1-\\exp(-z)$ starts to be more accurate than the first-order approximation $z$, and the series may require more terms. Let's set a threshold $Z_{\\text{thr}}$. For $|z|  Z_{\\text{thr}}$, we use a truncated Taylor series polynomial $P_N(z)$ to approximate $\\varphi_1(z)$. For double precision, $N=15$ terms are more than sufficient.\n\n- **For moderate and large $|z|$**: When $|z| \\ge Z_{\\text{thr}}$, the catastrophic cancellation in $1 - \\exp(-z)$ is no longer an issue. However, the problem statement provides a more robust alternative for all cases where cancellation could be a problem: using the `expm1` function, which is engineered to accurately compute $\\exp(w)-1$ for small $w$. We have $1 - \\exp(-z) = -(\\exp(-z) - 1) = -\\operatorname{expm1}(-z)$.\nThus, for $|z| \\ge Z_{\\text{thr}}$, we compute:\n$$\n\\varphi_1(z) = \\frac{-\\operatorname{expm1}(-z)}{z}\n$$\nThis formulation is accurate for a wide range of $z$.\n\n- **Special Case $\\xi=0$**: When $\\xi=0$, we have $\\sigma(0)=0$ and thus $z=0$. The limiting value is $B(0) = \\Delta t$. This corresponds to $\\varphi_1(0) = 1$. Our robust strategy must respect this limit. The Taylor series naturally gives $\\varphi_1(0)=1$.\n\nCombining these pieces, the robust algorithm for $B(\\xi)$ is:\n1.  Compute $\\sigma_{\\text{robust}}(\\xi)$ as described above. If $\\xi=0$, then $\\sigma=0$ and $B=\\Delta t$.\n2.  Compute $z = \\sigma_{\\text{robust}}(\\xi) \\cdot \\Delta t$.\n3.  If $|z|  Z_{\\text{thr}}$, evaluate $B(\\xi) = \\Delta t \\cdot \\left( \\sum_{k=0}^{N} \\frac{(-z)^k}{(k+1)!} \\right)$ using Horner's method.\n4.  If $|z| \\ge Z_{\\text{thr}}$, evaluate $B(\\xi) = \\Delta t \\cdot \\frac{-\\operatorname{expm1}(-z)}{z}$.\n\nThis combined strategy provides high accuracy across the entire range of $\\xi$ values, from numbers close to unity down into the subnormal floating-point range. The error analysis will compare the maximum errors from this robust approach to those from the naive approach, demonstrating the significant improvement in numerical stability.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import expm1\nimport decimal\nimport math\n\ndef solve():\n    \"\"\"\n    Main solver function to perform the numerical analysis of PML coefficients.\n    \"\"\"\n    # Define test cases: (sigma_max, m, dt)\n    test_cases = [\n        (200.0, 3, 1.0e-3),\n        (1000.0, 8, 1.0e-4),\n        (1.0e6, 2, 1.0e-12),\n    ]\n\n    # Define the evaluation grid Xi for double-precision floats\n    xi_powers = [-10, -30, -60, -120, -240, -480, -960, -1060]\n    xi_grid_float = np.array([0.0] + [2.0**p for p in xi_powers] + [1.0])\n\n    # For high-precision reference, define Xi using strings to avoid premature rounding\n    xi_grid_str = ['0.0'] + [f'2.0**{p}' for p in xi_powers] + ['1.0']\n\n    # Precompute Taylor series coefficients for phi_1(z) = (1-exp(-z))/z\n    # phi_1(z) = sum_{k=0 to inf} (-z)^k / (k+1)!\n    # Using 20 terms is more than sufficient for double precision.\n    TAYLOR_COEFFS = np.array([(-1)**k / math.factorial(k + 1) for k in range(20)])\n    Z_THRESHOLD = 1e-8\n\n    def horner(coeffs, x):\n        \"\"\"Evaluates a polynomial using Horner's method.\"\"\"\n        res = 0.0\n        for c in reversed(coeffs):\n            res = res * x + c\n        return res\n\n    def compute_reference(sigma_max, m, dt, xi_grid_str_local):\n        \"\"\"Computes A and B using high-precision arithmetic.\"\"\"\n        # Set precision to 120 digits\n        decimal.getcontext().prec = 120\n        d_sigma_max = decimal.Decimal(str(sigma_max))\n        d_m = decimal.Decimal(str(m))\n        d_dt = decimal.Decimal(str(dt))\n        d_2 = decimal.Decimal('2')\n\n        A_ref, B_ref = [], []\n        for xi_str in xi_grid_str_local:\n            if xi_str == '0.0':\n                d_xi = decimal.Decimal(0)\n            elif 'e' in xi_str:\n                 d_xi = decimal.Decimal(xi_str)\n            elif '**' in xi_str:\n                base, p = xi_str.split('**')\n                d_xi = decimal.Decimal(base) ** decimal.Decimal(p)\n            else:\n                 d_xi = decimal.Decimal(xi_str)\n\n\n            if d_xi == 0:\n                A_ref.append(decimal.Decimal(1))\n                B_ref.append(d_dt)\n                continue\n\n            d_sigma = d_sigma_max * (d_xi ** d_m)\n            \n            if d_sigma == 0:\n                 d_A = decimal.Decimal(1)\n                 d_B = d_dt\n            else:\n                arg = -d_sigma * d_dt\n                d_A = arg.exp()\n                d_B = (decimal.Decimal(1) - d_A) / d_sigma\n\n            A_ref.append(d_A)\n            B_ref.append(d_B)\n        return A_ref, B_ref\n\n    def compute_naive(sigma_max, m, dt, xi_grid_float_local):\n        \"\"\"Computes A and B using a direct, naive implementation.\"\"\"\n        A_naive, B_naive = [], []\n        for xi in xi_grid_float_local:\n            if xi == 0.0:\n                A_naive.append(1.0)\n                B_naive.append(dt)\n                continue\n\n            sigma = sigma_max * (xi ** m)\n            \n            if sigma == 0.0:\n                A = 1.0\n                B = dt\n            else:\n                # Naive calculation prone to cancellation and underflow issues\n                z = sigma * dt\n                A = np.exp(-z)\n                B = (1.0 - A) / sigma\n            \n            A_naive.append(A)\n            B_naive.append(B)\n        return A_naive, B_naive\n\n    def compute_robust(sigma_max, m, dt, xi_grid_float_local):\n        \"\"\"Computes A and B using a numerically robust implementation.\"\"\"\n        A_robust, B_robust = [], []\n        for xi in xi_grid_float_local:\n            if xi == 0.0:\n                A_robust.append(1.0)\n                B_robust.append(dt)\n                continue\n            \n            # Robust sigma calculation to avoid premature underflow\n            s, e = np.frexp(xi)\n            # sigma = sigma_max * s**m * 2**(e*m)\n            sigma = np.ldexp(sigma_max * (s ** m), e * m)\n\n            # Robust A\n            z = sigma * dt\n            A = np.exp(-z)\n            A_robust.append(A)\n\n            # Robust B\n            if sigma == 0.0: # If final sigma underflows\n                B = dt\n            else:\n                if abs(z)  Z_THRESHOLD:\n                    # Use Taylor series via Horner's method for small z\n                    phi1 = horner(TAYLOR_COEFFS, -z)\n                    B = dt * phi1\n                else:\n                    # Use expm1 for moderate/large z\n                    B = dt * (-expm1(-z) / z)\n            B_robust.append(B)\n        return A_robust, B_robust\n\n    def get_max_error(vals_float, vals_ref):\n        \"\"\"Calculates the maximum relative error (or absolute if ref is 0).\"\"\"\n        errors = []\n        for v_f, v_r_dec in zip(vals_float, vals_ref):\n            v_r = float(v_r_dec)\n            if v_r == 0.0:\n                error = abs(v_f - v_r)\n            else:\n                error = abs(v_f - v_r) / abs(v_r)\n            errors.append(error)\n        return np.max(errors) if errors else 0.0\n\n    all_results = []\n    for sigma_max, m, dt in test_cases:\n        # 1. Compute reference values\n        A_ref, B_ref = compute_reference(sigma_max, m, dt, xi_grid_str)\n        \n        # 2. Compute naive values\n        A_naive, B_naive = compute_naive(sigma_max, m, dt, xi_grid_float)\n        \n        # 3. Compute robust values\n        A_robust, B_robust = compute_robust(sigma_max, m, dt, xi_grid_float)\n        \n        # 4. Calculate maximum errors\n        max_err_A_naive = get_max_error(A_naive, A_ref)\n        max_err_B_naive = get_max_error(B_naive, B_ref)\n        max_err_A_robust = get_max_error(A_robust, A_ref)\n        max_err_B_robust = get_max_error(B_robust, B_ref)\n        \n        case_results = [\n            max_err_A_naive,\n            max_err_B_naive,\n            max_err_A_robust,\n            max_err_B_robust,\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string to be exactly as required (no spaces)\n    results_str = ','.join([str(r).replace(' ', '') for r in all_results])\n    print(f\"[{results_str}]\")\n\nsolve()\n```"
        }
    ]
}