{
    "hands_on_practices": [
        {
            "introduction": "To truly master the Conjugate Gradient (CG) method, it is essential to move beyond viewing it as a black-box algorithm. This first practice invites you to do just that by performing a single, complete iteration by hand, starting from first principles . By explicitly calculating the step size, the updated solution, the new residual, and the factor for the next search direction, you will solidify your understanding of the fundamental mechanics that drive the method's convergence.",
            "id": "3371621",
            "problem": "Consider a linear system arising from a symmetric positive definite discretization of an elliptic operator in computational fluid dynamics, such as the pressure Poisson equation on a minimal control-volume stencil. Let the matrix be $A=\\begin{pmatrix}4&1\\\\1&3\\end{pmatrix}$, the right-hand side be $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$, and the initial iterate be $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The Conjugate Gradient method (CG) is derived by minimizing the quadratic functional $\\phi(x)=\\frac{1}{2}x^{\\top}Ax-b^{\\top}x$ over Krylov subspaces, with residuals $r_{k}=b-Ax_{k}$, mutually $A$-conjugate search directions, and step sizes chosen so that the new residual is orthogonal to the current search direction in the Euclidean inner product. Starting from these principles, and without assuming any shortcut formulas, compute the first-step quantities $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$ explicitly for the given $A$, $b$, and $x_{0}$. Provide exact values with no rounding. For reporting, express the final answer as the row $\\left(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0}\\right)$, where $x_{1,i}$ and $r_{1,i}$ denote the components of $x_{1}$ and $r_{1}$, respectively.",
            "solution": "The problem is well-posed and scientifically sound. It requires the computation of the first iteration of the Conjugate Gradient (CG) method for a given linear system, starting from the fundamental principles of the algorithm rather than relying on a pre-packaged algorithm summary.\n\nThe system to be solved is $Ax=b$, where the matrix $A$ is symmetric and positive definite (SPD). The CG method iteratively constructs a solution by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2}x^{\\top}Ax - b^{\\top}x$. The gradient of this functional is $\\nabla\\phi(x) = Ax - b$, which is the negative of the residual, $r(x) = b - Ax$. Thus, minimizing $\\phi(x)$ is equivalent to finding $x$ such that $\\nabla\\phi(x) = 0$, which is the solution to $Ax=b$.\n\nThe givens are:\nThe matrix $A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\nThe right-hand side vector $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial guess for the solution $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm proceeds as follows for iteration $k=0, 1, 2, ...$:\n1. Update the solution: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n2. Update the residual: $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n3. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\nThe parameters $\\alpha_k$ and $\\beta_k$ are derived from core principles.\n\n**Step 0: Initialization**\n\nFirst, we compute the initial residual $r_0$ based on the initial guess $x_0$.\n$$r_{0} = b - Ax_{0}$$\nWith $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the initial residual is simply $b$:\n$$r_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe first search direction $p_0$ is chosen to be the direction of steepest descent, which is the initial residual:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n\n**Step 1: First Iteration ($k=0$)**\n\nWe need to compute $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$.\n\n**Computing the step size $\\alpha_{0}$**\nThe step size $\\alpha_{0}$ is chosen to minimize $\\phi(x_{1}) = \\phi(x_{0} + \\alpha_{0} p_{0})$ along the search direction $p_{0}$. This minimum is achieved when the new residual $r_{1}$ is orthogonal to the current search direction $p_{0}$, i.e., $p_{0}^{\\top}r_{1} = 0$.\nThe new residual is given by $r_{1} = b - Ax_{1} = b - A(x_{0} + \\alpha_{0} p_{0}) = (b - Ax_{0}) - \\alpha_{0}Ap_{0} = r_0 - \\alpha_0 A p_0$.\nSubstituting this into the orthogonality condition:\n$$p_{0}^{\\top}(r_{0} - \\alpha_{0} A p_{0}) = 0$$\n$$p_{0}^{\\top}r_{0} - \\alpha_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\alpha_0$ yields:\n$$\\alpha_{0} = \\frac{p_{0}^{\\top}r_{0}}{p_{0}^{\\top}A p_{0}}$$\nSince $p_0 = r_0$, this becomes:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top}r_{0}}{r_{0}^{\\top}A r_{0}}$$\nWe calculate the necessary quantities:\n$r_{0}^{\\top}r_{0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1)(1) + (2)(2) = 1 + 4 = 5$.\n$A p_{0} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\n$p_{0}^{\\top}A p_{0} = r_{0}^{\\top}A p_{0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (1)(6) + (2)(7) = 6 + 14 = 20$.\nSubstituting these values:\n$$\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$$\n\n**Computing the new iterate $x_{1}$**\nThe new solution estimate $x_1$ is found by moving from $x_0$ along the direction $p_0$ by the step size $\\alpha_0$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nSo, $x_{1,1} = \\frac{1}{4}$ and $x_{1,2} = \\frac{1}{2}$.\n\n**Computing the new residual $r_{1}$**\nThe new residual $r_1$ can be computed using the update formula:\n$$r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{6}{4} \\\\ 2 - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{4} - \\frac{6}{4} \\\\ \\frac{8}{4} - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$$\nSo, $r_{1,1} = -\\frac{1}{2}$ and $r_{1,2} = \\frac{1}{4}$.\n\n**Computing the coefficient $\\beta_{0}$**\nThe coefficient $\\beta_0$ is used to construct the next search direction, $p_1 = r_1 + \\beta_0 p_0$. The fundamental principle is that the new search direction $p_1$ must be $A$-conjugate to the previous direction $p_0$, meaning $p_{1}^{\\top}A p_{0} = 0$.\n$$(r_{1} + \\beta_{0} p_{0})^{\\top}A p_{0} = 0$$\n$$r_{1}^{\\top}A p_{0} + \\beta_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = -\\frac{r_{1}^{\\top}A p_{0}}{p_{0}^{\\top}A p_{0}}$$\nWe have the terms from the previous calculations: $A p_0 = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$ and $p_{0}^{\\top}A p_{0} = 20$.\nWe need to calculate the numerator:\n$r_{1}^{\\top}A p_{0} = \\begin{pmatrix} -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (-\\frac{1}{2})(6) + (\\frac{1}{4})(7) = -3 + \\frac{7}{4} = -\\frac{12}{4} + \\frac{7}{4} = -\\frac{5}{4}$.\nNow we can compute $\\beta_0$:\n$$\\beta_{0} = - \\frac{-\\frac{5}{4}}{20} = \\frac{5}{4 \\cdot 20} = \\frac{5}{80} = \\frac{1}{16}$$\n\nThe requested quantities are $\\alpha_{0} = \\frac{1}{4}$, $x_{1} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$, $r_{1} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$, and $\\beta_{0} = \\frac{1}{16}$.\nThe final answer is assembled into the specified row vector format $(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0})$.\nThis gives the row vector $(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{16})$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{4} & \\frac{1}{16} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The Conjugate Gradient method's celebrated efficiency and robustness are guaranteed only when the system matrix is symmetric positive definite (SPD). This exercise is a critical thought experiment designed to explore what happens when these conditions are not met . By analyzing specific cases with a symmetric indefinite matrix, you will uncover the precise failure modes—such as division by zero or moving towards a maximum instead of a minimum—and thereby gain a much deeper appreciation for why the SPD property is a non-negotiable requirement for the classical CG algorithm.",
            "id": "3586875",
            "problem": "Consider the Conjugate Gradient (CG) method for solving a linear system $A x = b$ by iteratively minimizing the quadratic $q(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$ along search directions. The classical finite-termination and monotonic decrease properties of the Conjugate Gradient (CG) method require that $A$ be symmetric positive definite (SPD). In exact arithmetic, the step length at iteration $k$ is obtained by choosing $\\alpha_k$ to make $q(x_k + \\alpha p_k)$ stationary with respect to $\\alpha$, where $p_k$ is the search direction. For SPD $A$, the curvature $p_k^\\top A p_k$ is strictly positive, ensuring that the stationary point is a minimizer along the line; for non-SPD $A$, $p_k^\\top A p_k$ can be nonpositive.\n\nWork in exact arithmetic. Let\n$$\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & -1\n\\end{bmatrix}, \\quad x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n$$\nConsider two right-hand sides\n$$\nb^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\qquad b^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n$$\nIn both cases, the initial residual is $r_0 = b - A x_0 = b$, and the initial search direction is $p_0 = r_0$.\n\nUsing only first principles (the definition of $q(x)$ and the fact that $\\alpha_0$ is chosen to make $q(x_0 + \\alpha p_0)$ stationary in $\\alpha$), analyze $p_0^\\top A p_0$ and the sign or definability of $\\alpha_0$ for $b^{(1)}$ and $b^{(2)}$, and then determine which of the following statements are correct about how and why Conjugate Gradient (CG) can break down for non-SPD $A$; select all that apply.\n\nA. There exist symmetric indefinite matrices $A$ and initial residuals $r_0 \\neq 0$ for which $p_0^\\top A p_0 = 0$, making $\\alpha_0$ undefined (division by $0$) and causing immediate breakdown. The specific data above with $b^{(1)}$ is such an example.\n\nB. For any symmetric indefinite $A$ and any nonzero $r_0$, one must have $p_0^\\top A p_0 < 0$, hence $\\alpha_0 < 0$ always; immediate breakdown in the sense of undefined $\\alpha_0$ cannot occur.\n\nC. If at some iteration $k$ one has $p_k^\\top A p_k < 0$, then the stationary point of $q(x_k + \\alpha p_k)$ occurs at a negative $\\alpha_k$ and is a maximizer along the line, so the usual descent and monotonic decrease guarantees of Conjugate Gradient (CG) fail, even though the iteration may still converge in special cases.\n\nD. If $p_k^\\top A p_k = 0$ for some $k$ with $r_k \\neq 0$, then setting $\\alpha_k = 0$ and proceeding preserves well-definedness and convergence of Conjugate Gradient (CG) in exact arithmetic.\n\nE. For nonsymmetric $A$, even if $p_k^\\top A p_k > 0$ for all $k$, the standard Conjugate Gradient (CG) three-term recurrences no longer guarantee $A$-conjugacy or finite termination; one should instead use methods such as Generalized Minimal Residual (GMRES) or Biconjugate Gradient (BiCG).\n\nF. If $A$ is symmetric indefinite but $r_0$ happens to lie entirely in the invariant subspace spanned by eigenvectors of $A$ with positive eigenvalues, then the Krylov subspaces remain in that subspace, $p_k^\\top A p_k > 0$ for all $k$ until convergence, and Conjugate Gradient (CG) behaves as in the SPD case, converging in at most $n$ steps.",
            "solution": "The problem asks for an analysis of the Conjugate Gradient (CG) method's behavior when the matrix $A$ is not symmetric positive definite (SPD), using a specific example and evaluating several general statements.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Linear system: $A x = b$\n- Quadratic functional: $q(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$\n- Algorithm: Conjugate Gradient (CG)\n- Iteration step length $\\alpha_k$ is chosen to make $q(x_k + \\alpha p_k)$ stationary with respect to $\\alpha$.\n- Arithmetic: Exact arithmetic is assumed.\n- Matrix: $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}$\n- Initial guess: $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$\n- Right-hand sides: $b^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ and $b^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n- Initial residual: $r_0 = b - A x_0 = b$\n- Initial search direction: $p_0 = r_0$\n\n**Step 2: Validate Using Extracted Givens**\n- The problem statement is scientifically grounded. It correctly describes the CG method, its underlying principle of minimizing a quadratic form, and the standard condition for its guaranteed success ($A$ is SPD). The matrix $A$ is a valid symmetric indefinite matrix (eigenvalues are $1$ and $-1$).\n- The problem is well-posed. All information required to perform the first step of the CG algorithm and analyze its behavior is provided.\n- The problem is objective and uses precise mathematical language. There are no ambiguities or subjective claims in the problem setup.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the analysis.\n\n**Derivation from First Principles**\n\nThe objective is to find the step length $\\alpha_k$ that makes the function $f(\\alpha) = q(x_k + \\alpha p_k)$ stationary.\n$$f(\\alpha) = \\frac{1}{2}(x_k + \\alpha p_k)^\\top A (x_k + \\alpha p_k) - b^\\top(x_k + \\alpha p_k)$$\nExpanding this expression, and using the symmetry of $A$ ($p_k^\\top A x_k = x_k^\\top A p_k$):\n$$f(\\alpha) = \\frac{1}{2}(x_k^\\top A x_k + 2\\alpha p_k^\\top A x_k + \\alpha^2 p_k^\\top A p_k) - (b^\\top x_k + \\alpha b^\\top p_k)$$\nTo find the stationary point, we differentiate with respect to $\\alpha$ and set the result to zero:\n$$\\frac{df}{d\\alpha} = p_k^\\top A x_k + \\alpha (p_k^\\top A p_k) - b^\\top p_k = 0$$\nSolving for $\\alpha$:\n$$\\alpha (p_k^\\top A p_k) = b^\\top p_k - p_k^\\top A x_k = p_k^\\top (b - A x_k)$$\nRecognizing that $r_k = b - A x_k$ is the residual at iteration $k$, we get:\n$$\\alpha (p_k^\\top A p_k) = p_k^\\top r_k$$\nThus, the step length is:\n$$\\alpha_k = \\frac{p_k^\\top r_k}{p_k^\\top A p_k}$$\nFor the first iteration ($k=0$), we are given $x_0 = 0$, $r_0 = b$, and $p_0 = r_0$. The formula becomes:\n$$\\alpha_0 = \\frac{p_0^\\top r_0}{p_0^\\top A p_0} = \\frac{r_0^\\top r_0}{r_0^\\top A r_0}$$\nA breakdown occurs if the denominator, the 'curvature' $p_k^\\top A p_k$, is zero, while the numerator is nonzero.\n\n**Analysis of Case 1: $b = b^{(1)}$**\n- $b^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- Since $x_0 = 0$, $r_0 = b^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- The initial search direction is $p_0 = r_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- We compute the denominator for $\\alpha_0$:\n$$p_0^\\top A p_0 = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = 1 \\cdot 1 + 1 \\cdot (-1) = 0$$\n- We compute the numerator for $\\alpha_0$:\n$$r_0^\\top r_0 = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1^2 + 1^2 = 2$$\n- Since $r_0 \\neq 0$, the numerator is non-zero. The step length is $\\alpha_0 = \\frac{2}{0}$, which is undefined. The CG algorithm breaks down immediately due to division by zero.\n\n**Analysis of Case 2: $b = b^{(2)}$**\n- $b^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- $r_0 = b^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- $p_0 = r_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- We compute the denominator for $\\alpha_0$:\n$$p_0^\\top A p_0 = \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix} = 0 \\cdot 0 + 1 \\cdot (-1) = -1$$\n- The curvature $p_0^\\top A p_0$ is negative.\n- We compute the numerator for $\\alpha_0$:\n$$r_0^\\top r_0 = \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 0^2 + 1^2 = 1$$\n- The step length is $\\alpha_0 = \\frac{1}{-1} = -1$.\n- In this case, $\\alpha_0$ is well-defined. However, the negative curvature $p_0^\\top A p_0 = -1$ has implications for the minimization property of CG.\n\n**Evaluation of Options**\n\n**A. There exist symmetric indefinite matrices $A$ and initial residuals $r_0 \\neq 0$ for which $p_0^\\top A p_0 = 0$, making $\\alpha_0$ undefined (division by $0$) and causing immediate breakdown. The specific data above with $b^{(1)}$ is such an example.**\n- Our analysis of Case 1 provides a direct confirmation of this statement. The matrix $A$ is symmetric indefinite, $r_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ is non-zero, and we calculated $p_0^\\top A p_0 = 0$, leading to an undefined $\\alpha_0$.\n- **Verdict: Correct.**\n\n**B. For any symmetric indefinite $A$ and any nonzero $r_0$, one must have $p_0^\\top A p_0 < 0$, hence $\\alpha_0 < 0$ always; immediate breakdown in the sense of undefined $\\alpha_0$ cannot occur.**\n- This statement is directly contradicted by our analysis of Case 1, where we found $p_0^\\top A p_0 = 0$ for a specific symmetric indefinite $A$ and a non-zero $r_0$. An indefinite matrix $A$ possesses directions $x$ for which $x^\\top A x > 0$, $x^\\top A x < 0$, and $x^\\top A x = 0$. Breakdown due to zero curvature is a known failure mode.\n- **Verdict: Incorrect.**\n\n**C. If at some iteration $k$ one has $p_k^\\top A p_k < 0$, then the stationary point of $q(x_k + \\alpha p_k)$ occurs at a negative $\\alpha_k$ and is a maximizer along the line, so the usual descent and monotonic decrease guarantees of Conjugate Gradient (CG) fail, even though the iteration may still converge in special cases.**\n- The nature of a stationary point of a single-variable function $f(\\alpha)$ is determined by its second derivative. Here, $f(\\alpha) = q(x_k + \\alpha p_k)$, and we calculated $\\frac{d^2f}{d\\alpha^2} = p_k^\\top A p_k$. If $p_k^\\top A p_k < 0$, the stationary point is a maximum.\n- The step length is $\\alpha_k = \\frac{r_k^\\top p_k}{p_k^\\top A p_k}$. In the CG algorithm, it can be shown that $r_k^\\top p_k = r_k^\\top r_k = \\|r_k\\|_2^2 \\ge 0$. If $r_k \\neq 0$, the numerator is positive. With a negative denominator, $\\alpha_k$ must be negative.\n- Taking a step that maximizes the quadratic form $q(x)$ along the search direction violates the fundamental principle of CG for SPD matrices, which is to minimize $q(x)$ at every step. This means the guarantee of monotonic decrease of $q(x)$ is lost. However, this does not absolutely preclude convergence in all scenarios, but the classical properties are no longer assured. The statement is accurate.\n- **Verdict: Correct.**\n\n**D. If $p_k^\\top A p_k = 0$ for some $k$ with $r_k \\neq 0$, then setting $\\alpha_k = 0$ and proceeding preserves well-definedness and convergence of Conjugate Gradient (CG) in exact arithmetic.**\n- If $p_k^\\top A p_k = 0$ and $r_k \\neq 0$, the step length $\\alpha_k$ is undefined. Setting $\\alpha_k=0$ is an ad-hoc fix. Let's analyze its consequences:\n  - $x_{k+1} = x_k + \\alpha_k p_k = x_k$. The solution estimate stagnates.\n  - $r_{k+1} = r_k - \\alpha_k A p_k = r_k$. The residual also stagnates.\n- Since the residual does not decrease, the algorithm cannot converge to the solution (unless $x_k$ was already the solution, which is ruled out by $r_k \\neq 0$). The fundamental convergence properties of CG, which rely on the reduction of the error or residual at each step, are broken. This strategy leads to stagnation, not convergence.\n- **Verdict: Incorrect.**\n\n**E. For nonsymmetric $A$, even if $p_k^\\top A p_k > 0$ for all $k$, the standard Conjugate Gradient (CG) three-term recurrences no longer guarantee $A$-conjugacy or finite termination; one should instead use methods such as Generalized Minimal Residual (GMRES) or Biconjugate Gradient (BiCG).**\n- The derivation of the CG algorithm, including the orthogonality of residuals ($r_i^\\top r_j=0$ for $i \\neq j$) and the $A$-conjugacy of search directions ($p_i^\\top A p_j=0$ for $i \\neq j$), relies critically on the symmetry of $A$. When $A \\neq A^\\top$, these properties are lost.\n- Consequently, the standard CG algorithm is not guaranteed to converge in at most $n$ iterations for a nonsymmetric $n \\times n$ matrix $A$. In fact, applying CG to $Ax=b$ is equivalent to solving $\\frac{1}{2}(A+A^\\top)x=b$, which is not the original problem.\n- Methods such as GMRES (which minimizes the residual norm over the Krylov subspace) and BiCG (which uses a dual \"shadow\" residual sequence to enforce a biorthogonality condition) are specifically designed for nonsymmetric systems. This statement is a correct and standard piece of knowledge in numerical linear algebra.\n- **Verdict: Correct.**\n\n**F. If $A$ is symmetric indefinite but $r_0$ happens to lie entirely in the invariant subspace spanned by eigenvectors of $A$ with positive eigenvalues, then the Krylov subspaces remain in that subspace, $p_k^\\top A p_k > 0$ for all $k$ until convergence, and Conjugate Gradient (CG) behaves as in the SPD case, converging in at most $n$ steps.**\n- Let $V_+$ be the invariant subspace spanned by the eigenvectors of $A$ corresponding to its positive eigenvalues. If $r_0 \\in V_+$, then any vector formed by applying powers of $A$ to $r_0$ will also be in $V_+$. Specifically, $A^j r_0 \\in V_+$ for all $j \\ge 0$.\n- The Krylov subspace $\\mathcal{K}_m(A, r_0) = \\text{span}\\{r_0, A r_0, \\dots, A^{m-1}r_0\\}$ is therefore entirely contained in $V_+$.\n- All vectors generated by the CG algorithm, including the residuals $r_k$ and search directions $p_k$, are elements of these Krylov subspaces, so they also lie in $V_+$.\n- The restriction of the matrix $A$ to the subspace $V_+$ is, by definition, symmetric and positive definite.\n- For any non-zero vector $p \\in V_+$, the quadratic form $p^\\top A p$ is strictly positive. Since $p_k \\in V_+$ (and $p_k \\neq 0$ before convergence), the curvature $p_k^\\top A p_k$ will always be positive.\n- Therefore, the CG algorithm proceeds as if it were solving a problem on a smaller, SPD system. It will not encounter breakdown from non-positive curvature and will converge in at most $\\text{dim}(V_+)$ steps, which is less than or equal to $n$. The statement is entirely correct.\n- **Verdict: Correct.**",
            "answer": "$$\\boxed{ACEF}$$"
        },
        {
            "introduction": "This final practice bridges the gap between abstract theory and applied computational science, a crucial step for any computational geophysicist. You will implement the CG algorithm to solve the Poisson equation—a cornerstone of many physical models—and conduct a numerical experiment to verify its theoretical convergence rate . This comprehensive exercise will not only test your coding skills but also demonstrate how theoretical bounds on performance, which depend on the matrix's spectral condition number $\\kappa(A)$, translate into tangible results in a practical setting.",
            "id": "3371602",
            "problem": "Consider the two-dimensional steady diffusion model on a rectangular domain with homogeneous Dirichlet boundary conditions. Let the physical domain be the rectangle with side lengths $L_x$ and $L_y$, and let the governing equation be the canonical elliptic model $-\\Delta u = f$ with $u = 0$ on the boundary. Discretize this model using a uniform grid of $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction, with central differences. This yields a linear system $A \\, x = b$, where $A$ is Symmetric Positive Definite (SPD), $x$ is the vector of nodal values of the discrete solution, and $b$ arises from the discrete forcing. The following tasks define a self-contained numerical study that connects the predicted convergence of the Conjugate Gradient method to the spectral condition number of $A$ in the setting of slender domains.\n\nTasks:\n1. Start from the discrete Laplacian on the rectangle and its separable structure. Using fundamental facts about symmetric Toeplitz tridiagonal matrices and the discrete sine basis associated with homogeneous Dirichlet boundary conditions, derive the exact eigenpairs of the two-dimensional operator constructed by central differences on a tensor-product grid. From these eigenpairs, express the spectral condition number $\\kappa(A)$ in terms of the extremal eigenvalues $\\lambda_{\\min}$ and $\\lambda_{\\max}$, which depend on $L_x$, $L_y$, $N_x$, and $N_y$.\n2. Recall the defining property of the Conjugate Gradient method for SPD systems: after $k$ iterations from the zero initial guess, the relative error measured in the energy norm satisfies an extremal-polynomial bound that depends only on $\\kappa(A)$. From first principles, connect the minimax polynomial characterization to an explicit convergence envelope and explain how the dominant scaling with $\\sqrt{\\kappa(A)}$ emerges for small tolerances.\n3. Manufacture an exact solution by choosing a smooth function that satisfies the boundary conditions and computing the corresponding right-hand side. Specifically, let the manufactured solution be $u(x,y) = \\sin\\!\\big(\\pi x / L_x\\big)\\,\\sin\\!\\big(\\pi y / L_y\\big)$ sampled at interior grid points $x_i = i\\,h_x$ and $y_j = j\\,h_y$ with $h_x = L_x/(N_x+1)$ and $h_y = L_y/(N_y+1)$. Construct $b$ exactly as $b = A\\,x_{\\text{true}}$ so that the discrete solution is exactly $x_{\\text{true}}$.\n4. Implement the Conjugate Gradient method from the zero vector $x_0 = 0$ and terminate when the relative error in the energy norm satisfies $\\|x_k - x_{\\text{true}}\\|_A / \\|x_0 - x_{\\text{true}}\\|_A \\le \\varepsilon$ for a given tolerance $\\varepsilon$. Here, the energy norm is defined by $\\|e\\|_A = \\sqrt{e^\\top A e}$.\n5. For each test case below, compute: \n   - the iteration count $k_{\\text{CG}}$ required to meet the stopping criterion,\n   - the spectral condition number $\\kappa(A)$ using the exact extremal eigenvalues derived in Task $1$,\n   - the normalized ratio $s = k_{\\text{CG}} \\big/ \\big(\\tfrac{1}{2}\\,\\sqrt{\\kappa(A)}\\,\\log(2/\\varepsilon)\\big)$, which removes the leading-order dependence on $\\sqrt{\\kappa(A)}$ predicted by theory for small $\\varepsilon$.\n6. Your program must implement the matrix-vector action of $A$ without forming $A$ explicitly as a dense matrix. Use a five-point stencil consistent with the uniform grid and homogeneous Dirichlet boundary conditions.\n7. Use the following test suite with tolerance $\\varepsilon = 10^{-8}$:\n   - Case $1$: $L_x = 1$, $L_y = 1$, $N_x = 31$, $N_y = 31$.\n   - Case $2$: $L_x = 1$, $L_y = 1$, $N_x = 127$, $N_y = 127$.\n   - Case $3$: $L_x = 1$, $L_y = 0.1$, $N_x = 127$, $N_y = 127$.\n   - Case $4$: $L_x = 1$, $L_y = 0.02$, $N_x = 127$, $N_y = 127$.\n   - Case $5$: $L_x = 5$, $L_y = 1$, $N_x = 255$, $N_y = 51$.\n8. Final output format requirement: Your program should produce a single line of output containing the normalized ratios $s$ for the five test cases as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, with no spaces. For example, the output must look like $[s_1,s_2,s_3,s_4,s_5]$. Each $s_i$ must be a floating-point number.\n\nNotes:\n- Angles, if any appear, must be in radians. No physical units are required in this problem.\n- The only acceptable stopping criterion is the relative energy-norm error as specified in Task $4$.\n- The discrete operator must strictly correspond to the uniform-grid five-point stencil with homogeneous Dirichlet boundary conditions on all sides.",
            "solution": "The user has provided a valid, well-posed, and scientifically grounded problem in computational mathematics. The tasks involve deriving theoretical properties of a discretized partial differential equation and validating them through a numerical experiment using the Conjugate Gradient method.\n\n### Theoretical Foundation\n\n#### Task 1: Eigenvalues and Condition Number of the Discrete Laplacian\n\nThe governing equation is the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on a rectangular domain $[0, L_x] \\times [0, L_y]$:\n$$\n-\\Delta u = -\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x,y)\n$$\nDiscretizing this equation on a uniform grid with $N_x \\times N_y$ interior points, using mesh spacings $h_x = L_x / (N_x+1)$ and $h_y = L_y / (N_y+1)$, and employing a second-order central difference approximation for the derivatives, we obtain the five-point stencil for the negative Laplacian operator $A$ at an interior node $(i,j)$:\n$$\n(A u)_{i,j} = \\frac{-u_{i-1,j} + 2u_{i,j} - u_{i+1,j}}{h_x^2} + \\frac{-u_{i,j-1} + 2u_{i,j} - u_{i,j+1}}{h_y^2}\n$$\nwhere $u_{i,j}$ is the discrete approximation of $u(i h_x, j h_y)$. This system can be written as a linear system $A x = b$, where $x$ is the vector of all $u_{i,j}$ values.\n\nThe operator $A$ is symmetric and positive definite (SPD). Its eigenpairs can be found by leveraging its separable structure. The operator is a sum of two commuting operators, one for each spatial dimension, which can be formally expressed using Kronecker products: $A = A_x \\otimes I_y + I_x \\otimes A_y$. The eigenvectors of $A$ are the tensor products of the eigenvectors of the one-dimensional discrete Laplacian.\n\nFor the one-dimensional problem on an $N$-point grid with spacing $h$, the discrete Laplacian matrix is proportional to the symmetric tridiagonal Toeplitz matrix $\\text{tridiag}(-1, 2, -1)$. The eigenvectors are the discrete sine functions, and the corresponding eigenvalues are well-known. For the operator $-\\frac{d^2}{dx^2}$ discretized with $N_x$ points, the eigenvalues are:\n$$\n\\mu_{k_x} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{k_x \\pi}{2(N_x+1)}\\right), \\quad k_x = 1, 2, \\ldots, N_x\n$$\nSimilarly, for the $y$-direction:\n$$\n\\mu_{k_y} = \\frac{4}{h_y^2} \\sin^2\\left(\\frac{k_y \\pi}{2(N_y+1)}\\right), \\quad k_y = 1, 2, \\ldots, N_y\n$$\nThe eigenvalues $\\lambda_{k_x, k_y}$ of the two-dimensional operator $A$ are the sums of the one-dimensional eigenvalues:\n$$\n\\lambda_{k_x, k_y} = \\mu_{k_x} + \\mu_{k_y} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{k_x \\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{k_y \\pi}{2(N_y+1)}\\right)\n$$\nfor $1 \\le k_x \\le N_x$ and $1 \\le k_y \\le N_y$.\n\nThe extremal eigenvalues are required to compute the spectral condition number $\\kappa(A) = \\lambda_{\\max} / \\lambda_{\\min}$.\nThe minimum eigenvalue, $\\lambda_{\\min}$, corresponds to the lowest frequencies, i.e., $k_x=1$ and $k_y=1$:\n$$\n\\lambda_{\\min} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{\\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{\\pi}{2(N_y+1)}\\right)\n$$\nThe maximum eigenvalue, $\\lambda_{\\max}$, corresponds to the highest frequencies, i.e., $k_x=N_x$ and $k_y=N_y$. Using the identity $\\sin(\\frac{N \\pi}{2(N+1)}) = \\sin(\\frac{\\pi}{2} - \\frac{\\pi}{2(N+1)}) = \\cos(\\frac{\\pi}{2(N+1)})$, we get:\n$$\n\\lambda_{\\max} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{N_x \\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{N_y \\pi}{2(N_y+1)}\\right) = \\frac{4}{h_x^2} \\cos^2\\left(\\frac{\\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\cos^2\\left(\\frac{\\pi}{2(N_y+1)}\\right)\n$$\nSubstituting $h_x = L_x/(N_x+1)$ and $h_y = L_y/(N_y+1)$ provides the final expressions in terms of the given parameters. The spectral condition number is then $\\kappa(A) = \\lambda_{\\max} / \\lambda_{\\min}$.\n\n#### Task 2: Conjugate Gradient Convergence Envelope\n\nThe Conjugate Gradient (CG) method for solving an SPD system $A x = b$ is an iterative method that, when starting with an initial guess $x_0$, generates a sequence of approximations $x_k$. The error $e_k = x_k - x_{\\text{true}}$ (where $x_{\\text{true}} = A^{-1}b$ is the exact solution) satisfies a key optimality property with respect to the energy norm $\\|v\\|_A = \\sqrt{v^\\top A v}$:\n$$\n\\|e_k\\|_A = \\min_{P \\in \\mathcal{P}_k, P(0)=1} \\|P(A) e_0\\|_A\n$$\nwhere $\\mathcal{P}_k$ is the space of polynomials of degree at most $k$. This property leads to an upper bound on the relative error reduction that depends only on the spectrum of $A$:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{P \\in \\mathcal{P}_k, P(0)=1} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |P(\\lambda)|\n$$\nThe solution to this minimax polynomial problem is given by a scaled and shifted Chebyshev polynomial of the first kind, which yields the tight bound:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)}\n$$\nwhere $T_k$ is the Chebyshev polynomial of degree $k$ and $\\kappa = \\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}$. A more commonly used, slightly weaker bound is:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k\n$$\nTo find the number of iterations $k$ required to reach a relative error tolerance $\\varepsilon$, we set the right-hand side to be less than or equal to $\\varepsilon$:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k \\le \\varepsilon \\implies k \\log\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right) \\le \\log(\\frac{\\varepsilon}{2})\n$$\nSolving for $k$ and using $\\log\\left( \\frac{a-1}{a+1} \\right) = -\\log\\left( \\frac{a+1}{a-1} \\right)$ gives:\n$$\nk \\ge \\frac{\\log(2/\\varepsilon)}{\\log\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)}\n$$\nFor large condition numbers $\\kappa \\gg 1$, we can use the approximation $\\log\\left( \\frac{z+1}{z-1} \\right) \\approx \\frac{2}{z}$ for large $z$. Setting $z=\\sqrt{\\kappa}$, we obtain:\n$$\nk \\gtrsim \\frac{\\log(2/\\varepsilon)}{2/\\sqrt{\\kappa}} = \\frac{1}{2}\\sqrt{\\kappa} \\log(2/\\varepsilon)\n$$\nThis derivation explains the leading-order dependence of the iteration count $k_{\\text{CG}}$ on $\\sqrt{\\kappa(A)}$ and motivates the definition of the normalized ratio $s = k_{\\text{CG}} \\big/ \\big(\\tfrac{1}{2}\\,\\sqrt{\\kappa(A)}\\,\\log(2/\\varepsilon)\\big)$, which is expected to be close to $1$.\n\n### Implementation Strategy\n\nThe numerical study is implemented following the problem tasks.\n- **Manufactured Solution (Task 3)**: The true discrete solution vector $x_{\\text{true}}$ is constructed by sampling the function $u(x,y) = \\sin(\\pi x/L_x)\\sin(\\pi y/L_y)$ on the $N_y \\times N_x$ grid of interior points. The right-hand side vector $b$ is then computed exactly as $b = A x_{\\text{true}}$, using a matrix-free implementation of the action of $A$.\n- **Matrix-Free Operator (Task 6)**: The matrix-vector product $v \\mapsto Av$ is implemented as a function. This function takes a flattened vector, reshapes it into a $N_y \\times N_x$ grid, applies the five-point stencil by operating on the 2D array with appropriate padding to handle the homogeneous Dirichlet boundaries, and then flattens the result. This avoids the prohibitive memory cost of storing the full $A$ matrix, which has dimensions $(N_xN_y) \\times (N_xN_y)$.\n- **Conjugate Gradient Method (Task 4)**: A standard implementation of the CG algorithm is used, starting from the zero vector $x_0=0$. The iteration is terminated based on the explicit criterion on the relative error in the energy norm: $\\|x_k - x_{\\text{true}}\\|_A / \\|x_0 - x_{\\text{true}}\\|_A \\le \\varepsilon$. This requires computing the error vector $e_k = x_k - x_{\\text{true}}$ at each iteration and its energy norm $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$, which necessitates one additional matrix-vector product per iteration. The denominator $\\|x_0 - x_{\\text{true}}\\|_A^2 = (-x_{\\text{true}})^\\top A (-x_{\\text{true}}) = x_{\\text{true}}^\\top A x_{\\text{true}} = x_{\\text{true}}^\\top b$ is pre-computed.\n- **Computation and Analysis (Task 5 & 7)**: For each of the five test cases, the exact spectral condition number $\\kappa(A)$ is calculated using the formulas derived in Task 1. The CG solver is run to determine the iteration count $k_{\\text{CG}}$. Finally, the normalized ratio $s$ is computed and collected for output.\nThe entire process is automated in a Python script using the NumPy library for numerical computations.",
            "answer": "```python\nimport numpy as np\n\ndef get_matvec_operator(Nx, Ny, hx, hy):\n    \"\"\"\n    Returns a matrix-free function for the action of the 2D discrete Laplacian A.\n    \"\"\"\n    def matvec(x_flat):\n        \"\"\"\n        Computes the matrix-vector product A*x for the 2D discrete Laplacian.\n        -x_pp -x_mm + 2*x on each direction, scaled by 1/h^2.\n        \n        Args:\n            x_flat (np.ndarray): A 1D vector of shape (Nx*Ny,).\n\n        Returns:\n            np.ndarray: The result of A*x as a 1D vector.\n        \"\"\"\n        # Reshape the flat vector to a 2D grid representation\n        U = x_flat.reshape((Ny, Nx))\n        \n        # Pad the grid with zeros to enforce homogeneous Dirichlet boundary conditions\n        U_padded = np.zeros((Ny + 2, Nx + 2))\n        U_padded[1:-1, 1:-1] = U\n        \n        # Apply the five-point stencil for the negative Laplacian\n        laplacian_U = (\n            (2 * U - U_padded[1:-1, :-2] - U_padded[1:-1, 2:]) / (hx**2) +\n            (2 * U - U_padded[:-2, 1:-1] - U_padded[2:, 1:-1]) / (hy**2)\n        )\n        \n        return laplacian_U.flatten()\n\n    return matvec\n\ndef solve_cg(matvec, b, xtrue, epsilon, max_iter):\n    \"\"\"\n    Solves Ax = b using the Conjugate Gradient method.\n\n    Args:\n        matvec (function): A function that computes the matrix-vector product A*x.\n        b (np.ndarray): The right-hand side vector.\n        xtrue (np.ndarray): The true solution vector for error calculation.\n        epsilon (float): The tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations taken (k_CG).\n    \"\"\"\n    x = np.zeros_like(b)\n    r = b.copy()  # Since x0=0, r0 = b - A*x0 = b\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    # Pre-calculate the denominator for the stopping criterion\n    # ||e0||_A^2 = ||-xtrue||_A^2 = xtrue.T * A * xtrue = xtrue.T * b\n    norm_A_e0_sq = np.dot(xtrue, b)\n    if norm_A_e0_sq == 0:\n        return 0\n\n    for k in range(1, max_iter + 1):\n        Ap = matvec(p)\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x += alpha * p\n        r -= alpha * Ap\n        \n        # Check stopping criterion: ||x_k - xtrue||_A / ||x_0 - xtrue||_A <= epsilon\n        e_k = x - xtrue\n        Ae_k = matvec(e_k)\n        norm_A_ek_sq = np.dot(e_k, Ae_k)\n        \n        rel_error_A = np.sqrt(norm_A_ek_sq / norm_A_e0_sq)\n\n        if rel_error_A <= epsilon:\n            return k\n\n        rs_new = np.dot(r, r)\n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return max_iter # Return max_iter if convergence is not reached\n\ndef run_case(Lx, Ly, Nx, Ny, epsilon):\n    \"\"\"\n    Runs a single test case for the numerical study.\n    \"\"\"\n    # 1. Grid and model parameters\n    hx = Lx / (Nx + 1)\n    hy = Ly / (Ny + 1)\n    \n    # 2. Compute exact extremal eigenvalues and condition number\n    sin_term_x_min = np.sin(np.pi / (2 * (Nx + 1)))**2\n    sin_term_y_min = np.sin(np.pi / (2 * (Ny + 1)))**2\n    cos_term_x_max = np.cos(np.pi / (2 * (Nx + 1)))**2\n    cos_term_y_max = np.cos(np.pi / (2 * (Ny + 1)))**2\n    \n    lambda_min = 4/hx**2 * sin_term_x_min + 4/hy**2 * sin_term_y_min\n    lambda_max = 4/hx**2 * cos_term_x_max + 4/hy**2 * cos_term_y_max\n    \n    kappa = lambda_max / lambda_min\n\n    # 3. Construct manufactured solution and RHS\n    x_coords = np.arange(1, Nx + 1) * hx\n    y_coords = np.arange(1, Ny + 1) * hy\n    XX, YY = np.meshgrid(x_coords, y_coords)\n    \n    U_true = np.sin(np.pi * XX / Lx) * np.sin(np.pi * YY / Ly)\n    xtrue = U_true.flatten()\n    \n    matvec = get_matvec_operator(Nx, Ny, hx, hy)\n    b = matvec(xtrue)\n    \n    # 4. Solve with CG and get iteration count\n    max_iter = 2 * (Nx + Ny) # A reasonable upper bound for iterations\n    k_cg = solve_cg(matvec, b, xtrue, epsilon, max_iter)\n    \n    # 5. Compute the normalized ratio s\n    theory_factor = 0.5 * np.sqrt(kappa) * np.log(2 / epsilon)\n    s = k_cg / theory_factor\n    \n    return s\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    epsilon = 1e-8\n    test_cases = [\n        # (Lx, Ly, Nx, Ny)\n        (1.0, 1.0, 31, 31),      # Case 1\n        (1.0, 1.0, 127, 127),    # Case 2\n        (1.0, 0.1, 127, 127),    # Case 3\n        (1.0, 0.02, 127, 127),   # Case 4\n        (5.0, 1.0, 255, 51),     # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        Lx, Ly, Nx, Ny = case\n        s_val = run_case(Lx, Ly, Nx, Ny, epsilon)\n        results.append(s_val)\n\n    # Format the final output as specified\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}