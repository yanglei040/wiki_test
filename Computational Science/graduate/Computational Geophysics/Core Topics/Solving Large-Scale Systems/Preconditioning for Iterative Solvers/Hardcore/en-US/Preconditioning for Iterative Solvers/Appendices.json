{
    "hands_on_practices": [
        {
            "introduction": "Incomplete LU (ILU) factorizations are popular \"black-box\" preconditioners, but their stability is not guaranteed, especially for matrices that are not diagonally dominant. This practice problem confronts a common failure mode where the factorization process encounters a zero pivot, halting the algorithm. By analyzing the ILU(0) factorization for a matrix arising from a groundwater flow model, you will diagnose this instability and learn to remedy it using diagonal shifting, a robust and widely used stabilization technique .",
            "id": "3613319",
            "problem": "In modeling steady single-phase groundwater flow in a heterogeneous, weakly advective medium, a cell-centered finite-volume discretization on a short one-dimensional subgrid produces a nonsymmetric, weakly diagonally dominant linear system for the pressure correction. Consider the resulting sparse coefficient matrix $A \\in \\mathbb{R}^{4 \\times 4}$ in the natural node ordering, with entries\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  -0.5  0  0\\\\\n-1  0.5  -0.3  0\\\\\n0  -0.2  1  -0.4\\\\\n0  0  -0.6  0.9\n\\end{pmatrix}.\n$$\nSuch matrices are routinely preconditioned using Incomplete Lower-Upper (ILU) factorization to accelerate Krylov subspace solvers such as the Generalized Minimal Residual (GMRES) method. You will examine the stability of the zero-fill incomplete factorization and design a stabilizing modification.\n\nTasks:\n- Starting from the definition of Gaussian elimination and restricting all updates to the original sparsity pattern of $A$ (i.e., zero fill), construct the Incomplete Lower-Upper factorization with zero fill, denoted $\\text{ILU}(0)$, in natural ordering for $A$. Identify any pivot breakdown risk and explain at what step and why it occurs.\n- A common stabilization is to apply a diagonal shift, forming $A_{\\alpha} = A + \\alpha I$ with $\\alpha \\ge 0$, and then computing $\\text{ILU}(0)$ of $A_{\\alpha}$. Let the pivot magnitude safeguard threshold be $\\tau = 0.1$. Derive, from first principles of elimination restricted to the sparsity pattern, explicit expressions for the intermediate pivots $u_{11}(\\alpha)$, $u_{22}(\\alpha)$, $u_{33}(\\alpha)$, and $u_{44}(\\alpha)$ that arise in the Doolittle form of $\\text{ILU}(0)$ applied to $A_{\\alpha}$.\n- Using these expressions, determine the minimal nonnegative diagonal shift $\\alpha^{\\star}$ such that the $\\text{ILU}(0)$ of $A_{\\alpha^{\\star}}$ exists in natural ordering without zero pivots and satisfies $|u_{kk}(\\alpha^{\\star})| \\ge \\tau$ for $k \\in \\{1,2,3,4\\}$. Provide the exact value of $\\alpha^{\\star}$ in closed form.\n- Briefly justify, based on graph and sparsity considerations, one reordering or modified $\\text{ILU}$ strategy that would further stabilize the preconditioner for matrices of this type, and explain why it is expected to reduce pivot breakdown risk.\n\nAnswer specification: Your final answer must be the single exact closed-form value of $\\alpha^{\\star}$, with no units. Do not provide an inequality or an equation. Do not round; give the exact algebraic form.",
            "solution": "The problem requires an analysis of the Incomplete Lower-Upper factorization with zero fill, denoted $\\text{ILU}(0)$, for a given $4 \\times 4$ matrix $A$. The analysis involves identifying pivot breakdown, deriving a stabilization strategy using a diagonal shift $\\alpha$, determining the minimal shift $\\alpha^{\\star}$ to satisfy a pivot threshold, and suggesting an alternative stabilization method.\n\nThe given matrix is:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  -0.5  0  0\\\\\n-1  0.5  -0.3  0\\\\\n0  -0.2  1  -0.4\\\\\n0  0  -0.6  0.9\n\\end{pmatrix}\n$$\n\n**Part 1: ILU(0) Factorization of $A$ and Pivot Breakdown**\n\nThe $\\text{ILU}(0)$ factorization finds a unit lower triangular matrix $L$ and an upper triangular matrix $U$ such that the product $M = LU$ matches $A$ on its original sparsity pattern. All other entries of $M$, known as fill-in, are discarded. The factors $L$ and $U$ are constrained to have the same sparsity pattern as the lower and upper triangular parts of $A$, respectively.\n\nLet the factors be:\n$$\nL = \\begin{pmatrix} 1  0  0  0 \\\\ l_{21}  1  0  0 \\\\ 0  l_{32}  1  0 \\\\ 0  0  l_{43}  1 \\end{pmatrix}, \\quad\nU = \\begin{pmatrix} u_{11}  u_{12}  0  0 \\\\ 0  u_{22}  u_{23}  0 \\\\ 0  0  u_{33}  u_{34} \\\\ 0  0  0  u_{44} \\end{pmatrix}\n$$\nThe product $LU$ is:\n$$\nLU = \\begin{pmatrix}\nu_{11}  u_{12}  0  0 \\\\\nl_{21}u_{11}  l_{21}u_{12} + u_{22}  u_{23}  0 \\\\\n0  l_{32}u_{22}  l_{32}u_{23} + u_{33}  u_{34} \\\\\n0  0  l_{43}u_{33}  l_{43}u_{34} + u_{44}\n\\end{pmatrix}\n$$\nWe equate the entries of $LU$ to the corresponding non-zero entries of $A$ sequentially.\n\nStep 1 (Row 1):\n$u_{11} = A_{11} = 1$\n$u_{12} = A_{12} = -0.5$\n\nStep 2 (Row 2):\n$l_{21}u_{11} = A_{21} \\implies l_{21}(1) = -1 \\implies l_{21} = -1$\n$l_{21}u_{12} + u_{22} = A_{22} \\implies (-1)(-0.5) + u_{22} = 0.5 \\implies 0.5 + u_{22} = 0.5 \\implies u_{22} = 0$\n\nAt this point, a pivot breakdown occurs. The second pivot, $u_{22}$, is zero. The $\\text{ILU}(0)$ algorithm cannot proceed because the next step would require division by $u_{22}$. Specifically, to compute $l_{32}$, we would need to solve $l_{32}u_{22} = A_{32}$, which is $l_{32}(0) = -0.2$, an impossible equation. Thus, the $\\text{ILU}(0)$ factorization of $A$ in natural ordering does not exist.\n\n**Part 2: Pivots of the Shifted Matrix $A_{\\alpha}$**\n\nWe consider the shifted matrix $A_{\\alpha} = A + \\alpha I$ with $\\alpha \\ge 0$:\n$$\nA_{\\alpha} \\;=\\;\n\\begin{pmatrix}\n1+\\alpha  -0.5  0  0\\\\\n-1  0.5+\\alpha  -0.3  0\\\\\n0  -0.2  1+\\alpha  -0.4\\\\\n0  0  -0.6  0.9+\\alpha\n\\end{pmatrix}\n$$\nWe perform the $\\text{ILU}(0)$ factorization of $A_{\\alpha}$. The general recurrence for the pivots $u_{kk}$ in $\\text{ILU}(0)$ for a tridiagonal matrix is:\n$$ u_{11} = (A_{\\alpha})_{11} $$\n$$ u_{kk} = (A_{\\alpha})_{kk} - \\frac{(A_{\\alpha})_{k,k-1} (A_{\\alpha})_{k-1,k}}{u_{k-1,k-1}} \\quad \\text{for } k > 1. $$\n\nFor $k=1$:\n$u_{11}(\\alpha) = (A_{\\alpha})_{11} = 1+\\alpha$\n\nFor $k=2$:\n$u_{22}(\\alpha) = (A_{\\alpha})_{22} - \\frac{(A_{\\alpha})_{21} (A_{\\alpha})_{12}}{u_{11}(\\alpha)} = (0.5+\\alpha) - \\frac{(-1)(-0.5)}{1+\\alpha} = 0.5+\\alpha - \\frac{0.5}{1+\\alpha}$\n$u_{22}(\\alpha) = \\frac{(0.5+\\alpha)(1+\\alpha) - 0.5}{1+\\alpha} = \\frac{0.5 + 1.5\\alpha + \\alpha^2 - 0.5}{1+\\alpha} = \\frac{\\alpha^2 + 1.5\\alpha}{1+\\alpha}$\n\nFor $k=3$:\n$u_{33}(\\alpha) = (A_{\\alpha})_{33} - \\frac{(A_{\\alpha})_{32} (A_{\\alpha})_{23}}{u_{22}(\\alpha)} = (1+\\alpha) - \\frac{(-0.2)(-0.3)}{u_{22}(\\alpha)} = 1+\\alpha - \\frac{0.06}{u_{22}(\\alpha)}$\nSubstituting the expression for $u_{22}(\\alpha)$:\n$u_{33}(\\alpha) = 1+\\alpha - \\frac{0.06(1+\\alpha)}{\\alpha^2 + 1.5\\alpha} = (1+\\alpha) \\left( 1 - \\frac{0.06}{\\alpha(\\alpha+1.5)} \\right) = (1+\\alpha) \\frac{\\alpha^2+1.5\\alpha-0.06}{\\alpha(\\alpha+1.5)}$\n\nFor $k=4$:\n$u_{44}(\\alpha) = (A_{\\alpha})_{44} - \\frac{(A_{\\alpha})_{43} (A_{\\alpha})_{34}}{u_{33}(\\alpha)} = (0.9+\\alpha) - \\frac{(-0.6)(-0.4)}{u_{33}(\\alpha)} = 0.9+\\alpha - \\frac{0.24}{u_{33}(\\alpha)}$\n\n**Part 3: Minimal Diagonal Shift $\\alpha^{\\star}$**\n\nWe need to find the minimal $\\alpha \\ge 0$ such that $|u_{kk}(\\alpha)| \\ge \\tau = 0.1$ for all $k \\in \\{1, 2, 3, 4\\}$.\n\nCondition for $u_{11}(\\alpha)$:\n$|u_{11}(\\alpha)| = |1+\\alpha| \\ge 0.1$. Since $\\alpha \\ge 0$, $1+\\alpha \\ge 1$, so this condition is always satisfied.\n\nCondition for $u_{22}(\\alpha)$:\n$|u_{22}(\\alpha)| = |\\frac{\\alpha^2 + 1.5\\alpha}{1+\\alpha}| \\ge 0.1$. For $\\alpha \\ge 0$, $u_{22}(\\alpha) \\ge 0$, so we solve $\\frac{\\alpha(\\alpha+1.5)}{1+\\alpha} \\ge 0.1$.\nMultiplying by $10(1+\\alpha)$ (which is positive for $\\alpha \\ge 0$):\n$10\\alpha(\\alpha+1.5) \\ge 1+\\alpha$\n$10\\alpha^2 + 15\\alpha \\ge 1+\\alpha$\n$10\\alpha^2 + 14\\alpha - 1 \\ge 0$\nThe roots of the quadratic $10\\alpha^2 + 14\\alpha - 1 = 0$ are $\\alpha = \\frac{-14 \\pm \\sqrt{14^2 - 4(10)(-1)}}{20} = \\frac{-14 \\pm \\sqrt{196+40}}{20} = \\frac{-14 \\pm \\sqrt{236}}{20} = \\frac{-7 \\pm \\sqrt{59}}{10}$.\nSince we require $\\alpha \\ge 0$, we take the positive root. The quadratic opens upwards, so the inequality holds for $\\alpha \\ge \\frac{-7+\\sqrt{59}}{10}$. Let this value be $\\alpha_{c2}$.\n\nCondition for $u_{33}(\\alpha)$:\nThe function $u_{22}(\\alpha)$ is strictly increasing for $\\alpha > 0$. Hence, $u_{33}(\\alpha) = 1+\\alpha - \\frac{0.06}{u_{22}(\\alpha)}$ is also strictly increasing for $\\alpha > 0$.\nLet's evaluate $u_{33}$ at the critical value $\\alpha_{c2}$. At this point, $u_{22}(\\alpha_{c2})=0.1$.\n$u_{33}(\\alpha_{c2}) = (1+\\alpha_{c2}) - \\frac{0.06}{0.1} = 1+\\alpha_{c2} - 0.6 = 0.4+\\alpha_{c2}$.\n$u_{33}(\\alpha_{c2}) = 0.4 + \\frac{-7+\\sqrt{59}}{10} = \\frac{4-7+\\sqrt{59}}{10} = \\frac{-3+\\sqrt{59}}{10}$.\nSince $5 = \\sqrt{25}  \\sqrt{59}  \\sqrt{64} = 8$, this value is positive.\n$|\\frac{-3+\\sqrt{59}}{10}| \\ge 0.1 \\implies -3+\\sqrt{59} \\ge 1 \\implies \\sqrt{59} \\ge 4 \\implies 59 \\ge 16$. This is true.\nSince $u_{33}(\\alpha)$ is increasing for $\\alpha > 0$, for any $\\alpha \\ge \\alpha_{c2}$, we have $u_{33}(\\alpha) \\ge u_{33}(\\alpha_{c2})  0.1$. Thus, the condition for $u_{33}$ is satisfied for all $\\alpha \\ge \\alpha_{c2}$.\n\nCondition for $u_{44}(\\alpha)$:\nThe function $u_{33}(\\alpha)$ is positive and increasing for $\\alpha \\ge \\alpha_{c2}$. The function $u_{44}(\\alpha) = 0.9+\\alpha - \\frac{0.24}{u_{33}(\\alpha)}$ is therefore also strictly increasing for $\\alpha \\ge \\alpha_{c2}$.\nLet's evaluate $u_{44}$ at $\\alpha_{c2}$:\n$u_{44}(\\alpha_{c2}) = 0.9+\\alpha_{c2} - \\frac{0.24}{u_{33}(\\alpha_{c2})} = 0.9+\\alpha_{c2} - \\frac{0.24}{0.4+\\alpha_{c2}}$.\nSubstituting $\\alpha_{c2} = \\frac{-7+\\sqrt{59}}{10}$:\n$0.9+\\alpha_{c2} = \\frac{9}{10} + \\frac{-7+\\sqrt{59}}{10} = \\frac{2+\\sqrt{59}}{10}$.\n$0.4+\\alpha_{c2} = \\frac{4}{10} + \\frac{-7+\\sqrt{59}}{10} = \\frac{-3+\\sqrt{59}}{10}$.\n$u_{44}(\\alpha_{c2}) = \\frac{2+\\sqrt{59}}{10} - \\frac{0.24}{(-3+\\sqrt{59})/10} = \\frac{2+\\sqrt{59}}{10} - \\frac{2.4}{-3+\\sqrt{59}}$.\n$u_{44}(\\alpha_{c2}) = \\frac{2+\\sqrt{59}}{10} - \\frac{2.4(-3-\\sqrt{59})}{9-59} = \\frac{2+\\sqrt{59}}{10} + \\frac{2.4(3+\\sqrt{59})}{40} = \\frac{2+\\sqrt{59}}{10} + 0.06(3+\\sqrt{59})$.\n$u_{44}(\\alpha_{c2}) = 0.2 + 0.1\\sqrt{59} + 0.18 + 0.06\\sqrt{59} = 0.38 + 0.16\\sqrt{59}$.\nThis value is clearly positive and much larger than $0.1$. Since $u_{44}(\\alpha)$ is increasing for $\\alpha \\ge \\alpha_{c2}$, the condition for $u_{44}$ is also satisfied for all $\\alpha \\ge \\alpha_{c2}$.\n\nThe set of $\\alpha \\ge 0$ values satisfying all four conditions is $[\\alpha_{c2}, \\infty)$. The minimal such value is $\\alpha_{c2}$.\nThus, the minimal non-negative diagonal shift is $\\alpha^{\\star} = \\frac{-7+\\sqrt{59}}{10}$.\n\n**Part 4: Alternative Stabilization Strategy**\n\nAn alternative strategy to stabilize the ILU factorization is to apply a **symmetric reordering** to the matrix before factorization. The goal of this reordering is to place entries with large magnitudes onto the diagonal. This can be formalized by finding a permutation matrix $P$ and applying the factorization to $P^TAP$. For instance, a maximum weight bipartite matching algorithm on the graph of the matrix can find a permutation that maximizes the product of the diagonal magnitudes.\n\nThis strategy directly addresses the root cause of pivot breakdown: small diagonal entries relative to off-diagonal entries. By permuting the matrix to make it more diagonally dominant, the initial pivots $u_{kk}$ (which are related to $A_{kk}$) are larger, and subsequent pivots are less likely to become dangerously small or zero during the elimination process. This is in contrast to the diagonal shift, which modifies the matrix values rather than reordering them. For matrices arising from physical problems, such orderings often have a physical interpretation and can be very effective at improving the robustness of ILU preconditioners.",
            "answer": "$$\\boxed{\\frac{-7+\\sqrt{59}}{10}}$$"
        },
        {
            "introduction": "In many real-world applications, such as inverse problems or data assimilation, a baseline physical model must be updated to incorporate new information, often leading to a low-rank modification of the system matrix. Recomputing a computationally expensive preconditioner from scratch is inefficient. This exercise guides you through deriving and analyzing an updated preconditioner for a rank-$r$ modification of a system, applying the Sherman-Morrison-Woodbury formula to construct an efficient update and showing that it can, in an ideal case, perfectly preserve the preconditioning quality .",
            "id": "3613332",
            "problem": "Consider a symmetric positive definite (SPD) linear system in computational geophysics arising from discretization of a variable-coefficient diffusion operator, where the baseline system matrix is $A \\in \\mathbb{R}^{n \\times n}$ and a right preconditioner inverse $M^{-1}$ is available that approximates $A^{-1}$ with a nontrivial accuracy. A low-rank, positive semidefinite perturbation models the insertion of $r$ data-consistency constraints or localized physics corrections, yielding the updated system $A_{\\mathrm{new}} = A + U U^{T}$, where $U \\in \\mathbb{R}^{n \\times r}$ has full column rank and $r \\ll n$. Your goal is to construct and analyze a preconditioner for $A_{\\mathrm{new}}$ starting only from widely accepted linear algebra facts and the availability of $M^{-1}$.\n\nTasks:\n- Derive, from first principles, a closed-form expression for an updated right preconditioner inverse $M^{-1}_{\\mathrm{upd}}$ that leverages $M^{-1}$ to approximate $(A + U U^{T})^{-1}$ using only low-dimensional operations in $\\mathbb{R}^{r}$ and applications of $M^{-1}$.\n- Let the cost of one application of $M^{-1}$ to a vector be $t_{M}$, the cost of multiplying $U^{T}$ by a vector in $\\mathbb{R}^{n}$ be $t_{U^{T}}$, the cost of multiplying $U$ by a vector in $\\mathbb{R}^{r}$ be $t_{U}$, and the cost of solving an $r \\times r$ linear system with a fixed, pre-factorized symmetric positive definite coefficient matrix be $t_{r}$. Using these symbolic costs, estimate the cost of applying $M^{-1}_{\\mathrm{upd}}$ to an arbitrary vector in $\\mathbb{R}^{n}$ when the $r \\times r$ system that arises in your derivation is factorized once and reused for all applications.\n- Under the idealization $M^{-1} = A^{-1}$ and assuming $U$ has full column rank, compute the condition number (with respect to the Euclidean norm) of the right-preconditioned operator $P = M^{-1}_{\\mathrm{upd}} (A + U U^{T})$.\n\nProvide your final answer as a single row matrix whose entries, in order, are: the expression for $M^{-1}_{\\mathrm{upd}}$, the symbolic application cost in terms of $t_{M}$, $t_{U^{T}}$, $t_{U}$, and $t_{r}$, and the condition number of $P$. No rounding is required. Express the condition number as a pure number without units. The final answer must be a single closed-form analytic expression or a row matrix of such expressions.",
            "solution": "The problem requires the derivation and analysis of a preconditioner for a linear system that has been updated by a low-rank, positive semidefinite matrix. The system is described by the matrix $A_{\\mathrm{new}} = A + U U^{T}$, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $U \\in \\mathbb{R}^{n \\times r}$ has full column rank, and $r \\ll n$. We are given an approximate inverse $M^{-1}$ for the baseline matrix $A$.\n\n### Task 1: Derivation of the Updated Preconditioner Inverse\n\nThe first task is to derive a closed-form expression for an updated right preconditioner inverse, denoted $M^{-1}_{\\mathrm{upd}}$, that approximates $(A + U U^{T})^{-1}$. The derivation must be based on first principles and leverage the available preconditioner inverse $M^{-1}$ and low-dimensional operations.\n\nThe primary tool for finding the inverse of a rank-$r$ updated matrix is the Sherman-Morrison-Woodbury (SMW) formula. For invertible matrices $B$ and $(I + Y^{T}B^{-1}X)$, the formula states:\n$$ (B + XY^{T})^{-1} = B^{-1} - B^{-1}X(I + Y^{T}B^{-1}X)^{-1}Y^{T}B^{-1} $$\nWe wish to find the inverse of $A_{\\mathrm{new}} = A + U U^{T}$. Applying the SMW formula with $B=A$, $X=U$, and $Y=U$, we obtain the exact expression for the inverse of $A_{\\mathrm{new}}$:\n$$ (A + U U^{T})^{-1} = A^{-1} - A^{-1}U(I_{r} + U^{T}A^{-1}U)^{-1}U^{T}A^{-1} $$\nwhere $I_{r}$ is the $r \\times r$ identity matrix. The expression involves the inversion of an $r \\times r$ matrix, which is computationally efficient since $r \\ll n$.\n\nThe problem states that we do not have direct access to $A^{-1}$, but rather an approximation $M^{-1} \\approx A^{-1}$. A standard and effective method for constructing an updated preconditioner is to substitute the approximation $M^{-1}$ for the exact inverse $A^{-1}$ in the SMW formula. This yields the desired expression for the updated preconditioner inverse, $M^{-1}_{\\mathrm{upd}}$:\n$$ M^{-1}_{\\mathrm{upd}} = M^{-1} - M^{-1}U(I_{r} + U^{T}M^{-1}U)^{-1}U^{T}M^{-1} $$\nThis expression for $M^{-1}_{\\mathrm{upd}}$ relies only on applications of the original preconditioner inverse $M^{-1}$ and operations in the low-dimensional space $\\mathbb{R}^{r}$ (i.e., forming and solving with the $r \\times r$ matrix $S = I_{r} + U^{T}M^{-1}U$), as required.\n\n### Task 2: Cost Analysis of Applying the Preconditioner\n\nThe second task is to estimate the computational cost of applying $M^{-1}_{\\mathrm{upd}}$ to an arbitrary vector $v \\in \\mathbb{R}^{n}$. The cost is to be expressed in terms of the given symbolic costs: $t_{M}$ (for applying $M^{-1}$), $t_{U}$ (for multiplying by $U$), $t_{U^{T}}$ (for multiplying by $U^{T}$), and $t_{r}$ (for solving the small $r \\times r$ system).\n\nTo compute $x = M^{-1}_{\\mathrm{upd}}v$, we follow an efficient sequence of operations based on the expression:\n$$ x = M^{-1}v - M^{-1}U(I_{r} + U^{T}M^{-1}U)^{-1}U^{T}M^{-1}v $$\nTo compute this efficiently, we evaluate the expression from right to left to reuse terms:\n1.  Compute $w_1 = M^{-1}v$. This is a vector in $\\mathbb{R}^{n}$. Cost: $t_{M}$.\n2.  Compute $w_2 = U^{T}w_1$. This is a vector in $\\mathbb{R}^{r}$. Cost: $t_{U^{T}}$.\n3.  Solve the $r \\times r$ system $S w_3 = w_2$ for $w_3$, where $S = I_{r} + U^{T}M^{-1}U$. Since $A$ is SPD, $A^{-1}$ is SPD. It is standard to assume a good preconditioner $M^{-1}$ is also SPD. As $U$ has full column rank, the matrix $U^{T}M^{-1}U$ is positive semidefinite (and typically positive definite), making $S$ SPD and well-conditioned. The problem states that this system is solved with a pre-factorized matrix, so the cost for this step is $t_{r}$.\n4.  Compute $w_4 = U w_3$. This is a vector in $\\mathbb{R}^{n}$. Cost: $t_{U}$.\n5.  Compute $w_5 = M^{-1} w_4$. This is another application of the base preconditioner. Cost: $t_{M}$.\n6.  Compute the final result $x = w_1 - w_5$. The cost is negligible compared to matrix-vector operations.\n\nSumming the costs of these steps, the total cost to apply $M^{-1}_{\\mathrm{upd}}$ to a vector is:\n$$ \\text{Cost} = t_{M} + t_{U^{T}} + t_{r} + t_{U} + t_{M} = 2t_{M} + t_{U} + t_{U^{T}} + t_{r} $$\n\n### Task 3: Condition Number Analysis\n\nThe third task is to compute the condition number of the right-preconditioned operator $P = M^{-1}_{\\mathrm{upd}} (A + U U^{T})$ under the idealization $M^{-1} = A^{-1}$. The condition number is with respect to the Euclidean norm ($\\ell_2$-norm).\n\nUnder the ideal assumption $M^{-1} = A^{-1}$, our derived expression for the updated preconditioner inverse $M^{-1}_{\\mathrm{upd}}$ becomes:\n$$ M^{-1}_{\\mathrm{upd}} = A^{-1} - A^{-1}U(I_{r} + U^{T}A^{-1}U)^{-1}U^{T}A^{-1} $$\nAs established in Task 1, this is precisely the analytical expression for the inverse of the updated matrix from the SMW formula. Therefore, under this idealization, our preconditioner inverse is the exact inverse of the new system matrix:\n$$ M^{-1}_{\\mathrm{upd}} = (A + U U^{T})^{-1} = A_{\\mathrm{new}}^{-1} $$\nNow we can evaluate the preconditioned operator $P$:\n$$ P = M^{-1}_{\\mathrm{upd}} (A + U U^{T}) = A_{\\mathrm{new}}^{-1} A_{\\mathrm{new}} = I_{n} $$\nwhere $I_n$ is the $n \\times n$ identity matrix.\n\nThe condition number of a matrix $P$ with respect to the Euclidean norm, $\\kappa_2(P)$, is defined as the product of the spectral norm of $P$ and its inverse, $\\kappa_2(P) = \\|P\\|_2 \\|P^{-1}\\|_2$. For the identity matrix $P = I_n$, its inverse is $P^{-1} = I_n$. The spectral norm of the identity matrix is:\n$$ \\|I_n\\|_2 = \\sup_{\\|x\\|_2=1} \\|I_n x\\|_2 = \\sup_{\\|x\\|_2=1} \\|x\\|_2 = 1 $$\nTherefore, the condition number is:\n$$ \\kappa_2(P) = \\kappa_2(I_n) = \\|I_n\\|_2 \\|I_n^{-1}\\|_2 = 1 \\cdot 1 = 1 $$\nThis result signifies a perfectly conditioned system, which is the expected outcome when the preconditioner is the exact inverse of the operator.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} M^{-1} - M^{-1}U(I_{r} + U^{T}M^{-1}U)^{-1}U^{T}M^{-1}  2t_{M} + t_{U} + t_{U^{T}} + t_{r}  1 \\end{pmatrix} } $$"
        },
        {
            "introduction": "Solving the frequency-domain Helmholtz equation is fundamental to modeling wave phenomena in geophysics, but the resulting linear systems are highly indefinite and notoriously challenging for iterative solvers. This exercise explores the design of a complex-shifted Laplacian preconditioner, a cornerstone physics-based technique in computational seismology and acoustics. You will analyze how adding an imaginary \"damping\" term to the operator transforms its spectrum, mitigating the numerical resonance that plagues iterative methods and enabling efficient simulations .",
            "id": "3613279",
            "problem": "Consider the frequency-domain Helmholtz operator $A = -\\Delta - k^2$ discretized on a one-dimensional unit interval with homogeneous Dirichlet boundary conditions using the standard second-order centered finite difference scheme on a uniform grid of $n$ interior points. You are to derive the complex-shifted Laplacian preconditioner $M = -\\Delta + \\alpha k^2 + i \\beta k^2$, explain the damping effect introduced by the complex shift, and quantify its impact on the spectral properties of the preconditioned operator.\n\nYour tasks are as follows:\n\n1. Starting from the definition of the second-order centered finite difference approximation of the Laplacian operator, derive the discrete one-dimensional operator $L$ that approximates $-\\Delta$ on a grid with spacing $h = 1/(n+1)$, and recall the well-tested eigen-decomposition of $L$ under homogeneous Dirichlet boundary conditions. Show that its eigenvalues are given by\n$$\n\\lambda_j = \\frac{4}{h^2} \\sin^2\\left(\\frac{j \\pi}{2 (n+1)}\\right), \\quad j = 1,2,\\ldots,n,\n$$\nand that the corresponding eigenvectors form an orthogonal basis.\n\n2. Using the operators $A = L - k^2 I$ and $M = L + (\\alpha + i \\beta) k^2 I$ with $I$ the identity, prove that $A$ and $M$ commute and are simultaneously diagonalizable by the eigenvectors of $L$. Derive the eigenvalues of the right-preconditioned operator $M^{-1} A$ as\n$$\n\\rho_j = \\frac{\\lambda_j - k^2}{\\lambda_j + (\\alpha + i \\beta) k^2}, \\quad j = 1,2,\\ldots,n.\n$$\n\n3. Explain, based on the structure of the eigenvalues $\\rho_j$, why adding a strictly positive imaginary shift (that is, choosing $\\beta  0$) introduces damping that improves the spectral properties relevant for Krylov subspace methods such as the Generalized Minimal Residual method (GMRES). In particular, explain why the imaginary part in the denominator $\\lambda_j + (\\alpha + i \\beta) k^2$ prevents near-resonant small denominators when $\\lambda_j \\approx k^2$, and why choosing $\\alpha \\approx -1$ makes $M$ a consistent approximation of $A$ augmented with damping.\n\n4. To quantify spectral clustering relevant to iterative solver performance, define the cluster radius around $1$ as\n$$\nr(n,k,\\alpha,\\beta) = \\max_{1 \\le j \\le n} \\left|1 - \\rho_j\\right|.\n$$\nA smaller value of $r(n,k,\\alpha,\\beta)$ indicates that the eigenvalues $\\rho_j$ are more tightly clustered around $1$, which generally benefits the convergence of Krylov subspace methods for normal or near-normal operators.\n\nYour program must compute $r(n,k,\\alpha,\\beta)$ using the formula for $\\rho_j$ for the following test suite of parameter values, which is designed to cover distinct regimes:\n\n- Test case $1$ (baseline complex shift approximating the Helmholtz operator): $n = 200$, $k = 3.2$, $\\alpha = -1.0$, $\\beta = 0.5$.\n- Test case $2$ (no real shift, only damping): $n = 200$, $k = 3.2$, $\\alpha = 0.0$, $\\beta = 0.5$.\n- Test case $3$ (higher wavenumber with weak damping): $n = 200$, $k = 20.0$, $\\alpha = -1.0$, $\\beta = 0.05$.\n- Test case $4$ (near-resonant case with extremely small damping): $n = 200$, $k = 3.14159$, $\\alpha = -1.0$, $\\beta = 10^{-6}$.\n\nImplement a program that computes $r(n,k,\\alpha,\\beta)$ for each test case using the closed-form eigenvalues $\\lambda_j$ and the expression for $\\rho_j$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$). The outputs must be real numbers (floats). No physical units are required for this problem, and there are no angles. The result for each test case must be a float.",
            "solution": "The problem is evaluated as valid. It is scientifically grounded in the field of numerical analysis for partial differential equations, well-posed with a clear objective, and all necessary information for derivation and computation is provided.\n\nThe solution proceeds by following the four specified tasks.\n\n### Task 1: Discrete Laplacian and its Eigendecomposition\n\nWe consider a function $u(x)$ defined on the unit interval $[0, 1]$ with homogeneous Dirichlet boundary conditions, $u(0) = 0$ and $u(1) = 0$. The interval is discretized by a uniform grid of $n$ interior points $x_j = j h$ for $j=1, \\ldots, n$, where the grid spacing is $h = 1/(n+1)$. The boundary points are $x_0 = 0$ and $x_{n+1} = 1$. Let $u_j$ be the approximation of $u(x_j)$.\n\nThe one-dimensional Laplacian operator is $\\Delta = \\frac{d^2}{dx^2}$. The standard second-order centered finite difference approximation for the second derivative at a point $x_j$ is:\n$$\n\\frac{d^2 u}{dx^2}\\bigg|_{x_j} \\approx \\frac{u(x_{j+1}) - 2u(x_j) + u(x_{j-1})}{h^2} = \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2}.\n$$\nThe operator $-\\Delta$ is therefore approximated by $-\\frac{1}{h^2}(u_{j+1} - 2u_j + u_{j-1})$. Applying this to each interior grid point $j=1, \\ldots, n$ and incorporating the boundary conditions $u_0 = 0$ and $u_{n+1} = 0$, we obtain a system of linear equations. This system can be written in matrix form as $L \\mathbf{u} = \\mathbf{f}$, where $\\mathbf{u} = [u_1, u_2, \\ldots, u_n]^T$ and $L$ is the discrete Laplacian operator. The matrix $L$ is an $n \\times n$ symmetric tridiagonal matrix given by:\n$$\nL = \\frac{1}{h^2}\n\\begin{pmatrix}\n2  -1    \\\\\n-1  2  -1   \\\\\n \\ddots  \\ddots  \\ddots  \\\\\n  -1  2  -1 \\\\\n   -1  2\n\\end{pmatrix}.\n$$\nThis matrix, a scaled version of the tridiagonal Toeplitz matrix with $(2, -1, -1)$ on its diagonals, has a well-known eigendecomposition. Its eigenvalues $\\lambda_j$ and corresponding eigenvectors $v_j$ for $j=1, \\ldots, n$ are:\n$$\n\\lambda_j = \\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{j\\pi}{n+1}\\right)\\right).\n$$\nUsing the half-angle trigonometric identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$, we can rewrite the eigenvalues as:\n$$\n\\lambda_j = \\frac{4}{h^2} \\sin^2\\left(\\frac{j\\pi}{2(n+1)}\\right), \\quad j = 1, 2, \\ldots, n.\n$$\nThis matches the formula provided in the problem statement. The corresponding eigenvectors have components $(v_j)_k$ given by the discrete sine transform:\n$$\n(v_j)_k = \\sqrt{\\frac{2}{n+1}} \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad k = 1, 2, \\ldots, n.\n$$\nThese eigenvectors $\\{v_j\\}_{j=1}^n$ form an orthonormal basis for $\\mathbb{R}^n$.\n\n### Task 2: Commutativity and Eigenvalues of the Preconditioned Operator\n\nThe discrete Helmholtz operator is $A = L - k^2 I$, where $I$ is the $n \\times n$ identity matrix. The complex-shifted Laplacian preconditioner is defined as $M = L + (\\alpha + i\\beta)k^2 I$. Let $c = \\alpha + i\\beta$ for notational convenience, so $M = L + ck^2 I$.\n\nTo prove that $A$ and $M$ commute, we compute their product in both orders:\n$$\nAM = (L - k^2 I)(L + ck^2 I) = L^2 + ck^2 L - k^2 L - ck^4 I = L^2 + (c-1)k^2 L - ck^4 I.\n$$\n$$\nMA = (L + ck^2 I)(L - k^2 I) = L^2 - k^2 L + ck^2 L - ck^4 I = L^2 + (c-1)k^2 L - ck^4 I.\n$$\nSince $AM = MA$, the operators commute.\n\nA fundamental theorem in linear algebra states that two commuting diagonalizable matrices are simultaneously diagonalizable. The matrix $L$ is real and symmetric, hence it is diagonalizable with the orthonormal basis of eigenvectors $\\{v_j\\}$. The matrices $A$ and $M$ are both polynomials of $L$ ($A = p_1(L)$ and $M = p_2(L)$ with $p_1(x) = x-k^2$ and $p_2(x) = x+ck^2$). Therefore, they share the same eigenvectors $v_j$ as $L$.\n\nLet's find the eigenvalues of $A$ and $M$. Let $v_j$ be an eigenvector of $L$ with eigenvalue $\\lambda_j$.\nThe eigenvalue of $A$ corresponding to $v_j$, let's call it $\\mu_j$, is:\n$$\nA v_j = (L - k^2 I)v_j = L v_j - k^2 I v_j = \\lambda_j v_j - k^2 v_j = (\\lambda_j - k^2)v_j.\n$$\nSo, $\\mu_j = \\lambda_j - k^2$.\n\nThe eigenvalue of $M$ corresponding to $v_j$, let's call it $\\nu_j$, is:\n$$\nM v_j = (L + ck^2 I)v_j = L v_j + ck^2 I v_j = \\lambda_j v_j + ck^2 v_j = (\\lambda_j + ck^2)v_j.\n$$\nSo, $\\nu_j = \\lambda_j + (\\alpha + i\\beta)k^2$.\n\nThe right-preconditioned operator is $M^{-1}A$. Its eigenvalues, denoted by $\\rho_j$, can be found by applying the operator to the common eigenvector $v_j$:\n$$\nM^{-1} A v_j = M^{-1} (\\mu_j v_j) = \\mu_j (M^{-1} v_j).\n$$\nSince $M v_j = \\nu_j v_j$, it follows that $M^{-1} v_j = \\frac{1}{\\nu_j}v_j$. Substituting this back, we get:\n$$\nM^{-1} A v_j = \\mu_j \\left(\\frac{1}{\\nu_j}v_j\\right) = \\frac{\\mu_j}{\\nu_j} v_j.\n$$\nThus, the eigenvalues of the preconditioned operator $M^{-1}A$ are:\n$$\n\\rho_j = \\frac{\\mu_j}{\\nu_j} = \\frac{\\lambda_j - k^2}{\\lambda_j + (\\alpha + i\\beta)k^2}, \\quad j = 1, 2, \\ldots, n.\n$$\n\n### Task 3: Damping Effect of the Complex Shift\n\nIterative solvers like GMRES are used to solve the linear system $A\\mathbf{u} = \\mathbf{f}$. The convergence of such methods depends strongly on the spectral properties of the system matrix $A$. The Helmholtz operator $A = L - k^2 I$ is indefinite: its eigenvalues $\\mu_j = \\lambda_j - k^2$ can be positive, negative, or zero. When the wavenumber $k$ is such that $k^2$ is close to an eigenvalue $\\lambda_j$ of the Laplacian $L$, the corresponding eigenvalue $\\mu_j$ of $A$ is close to zero. This near-singularity, or resonance, severely degrades the performance of iterative solvers.\n\nPreconditioning aims to transform the system into an equivalent one, $M^{-1}A\\mathbf{u} = M^{-1}\\mathbf{f}$, where the preconditioned matrix $M^{-1}A$ has more favorable spectral properties. Ideally, its eigenvalues $\\{\\rho_j\\}$ should be clustered around $1$ and away from the origin.\n\nLet's examine the eigenvalues of the preconditioned system:\n$$\n\\rho_j = \\frac{\\lambda_j - k^2}{\\lambda_j + \\alpha k^2 + i \\beta k^2}.\n$$\nThe choice $\\alpha \\approx -1$ is natural because it makes the real part of the preconditioner's \"shifted\" term, $\\alpha k^2$, match the shift in the original operator $A$, $-k^2$. If we set $\\alpha = -1$, the preconditioner becomes $M = L - k^2 I + i \\beta k^2 I = A + i \\beta k^2 I$. This means $M$ is a direct, regularized approximation of $A$. In this case, the eigenvalues are:\n$$\n\\rho_j = \\frac{\\lambda_j - k^2}{(\\lambda_j - k^2) + i \\beta k^2}.\n$$\nThe resonance problem occurs when $\\lambda_j \\approx k^2$, making the numerator $\\lambda_j - k^2$ small. Without a complex shift ($\\beta = 0$), the denominator would also be $\\lambda_j - k^2$, and $\\rho_j$ would be close to $1$. However, for eigenvalues $\\lambda_l \\neq \\lambda_j$, the ratio can be very different, leading to a wide spectral spread. If there is a slight mismatch in discretization between $A$ and the preconditioner, the denominator could become very small while the numerator is not, leading to large eigenvalues.\n\nWith a strictly positive imaginary shift, $\\beta > 0$, the term $i \\beta k^2$ is added to the denominator. Even when the real part $\\lambda_j - k^2$ is close to zero, the magnitude of the denominator is:\n$$\n|\\lambda_j - k^2 + i \\beta k^2| = \\sqrt{(\\lambda_j - k^2)^2 + (\\beta k^2)^2}.\n$$\nSince $\\beta > 0$ and $k \\neq 0$, we have $(\\beta k^2)^2 > 0$. This ensures that the denominator is bounded away from zero, regardless of how close $\\lambda_j$ is to $k^2$. This \"damping\" effect prevents the eigenvalues $\\rho_j$ from becoming excessively large or undefined, thus stabilizing the preconditioned system. The eigenvalues are mapped into a bounded region in the complex plane, which is highly beneficial for the convergence of GMRES. Specifically, for $\\alpha=-1$, all eigenvalues $\\rho_j$ lie on the circle in the complex plane with diameter connecting $0$ and $1$.\n\n### Task 4: Quantification of Spectral Clustering\n\nThe cluster radius $r(n,k,\\alpha,\\beta)$ is defined as the maximum deviation of the preconditioned eigenvalues from $1$:\n$$\nr(n,k,\\alpha,\\beta) = \\max_{1 \\le j \\le n} |1 - \\rho_j|.\n$$\nWe first derive a simplified expression for $|1 - \\rho_j|$:\n$$\n1 - \\rho_j = 1 - \\frac{\\lambda_j - k^2}{\\lambda_j + \\alpha k^2 + i \\beta k^2} = \\frac{(\\lambda_j + \\alpha k^2 + i \\beta k^2) - (\\lambda_j - k^2)}{\\lambda_j + \\alpha k^2 + i \\beta k^2} = \\frac{(\\alpha+1)k^2 + i \\beta k^2}{\\lambda_j + \\alpha k^2 + i \\beta k^2}.\n$$\nTaking the complex modulus:\n$$\n|1 - \\rho_j| = \\frac{|(\\alpha+1)k^2 + i \\beta k^2|}{|\\lambda_j + \\alpha k^2 + i \\beta k^2|} = \\frac{\\sqrt{((\\alpha+1)k^2)^2 + (\\beta k^2)^2}}{\\sqrt{(\\lambda_j + \\alpha k^2)^2 + (\\beta k^2)^2}} = \\frac{k^2 \\sqrt{(\\alpha+1)^2 + \\beta^2}}{\\sqrt{(\\lambda_j + \\alpha k^2)^2 + (\\beta k^2)^2}}.\n$$\nTo calculate $r$, we need to find the maximum of this quantity over all $j \\in \\{1, \\ldots, n\\}$. The numerator is constant with respect to $j$. Therefore, maximizing the fraction is equivalent to minimizing its denominator, $\\sqrt{(\\lambda_j + \\alpha k^2)^2 + (\\beta k^2)^2}$. Since $(\\beta k^2)^2$ is also constant, we must find the minimum of $(\\lambda_j + \\alpha k^2)^2$. This is achieved when $\\lambda_j$ is closest to $-\\alpha k^2$.\nLet $d_{min}^2 = \\min_{1 \\le j \\le n} (\\lambda_j + \\alpha k^2)^2$. Then the cluster radius is:\n$$\nr(n,k,\\alpha,\\beta) = \\frac{k^2 \\sqrt{(\\alpha+1)^2 + \\beta^2}}{\\sqrt{d_{min}^2 + (\\beta k^2)^2}}.\n$$\nThe provided Python program implements this formula to compute the cluster radius for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral cluster radius for a complex-shifted Laplacian\n    preconditioner applied to the 1D Helmholtz equation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, alpha, beta)\n        (200, 3.2, -1.0, 0.5),      # Test case 1\n        (200, 3.2, 0.0, 0.5),       # Test case 2\n        (200, 20.0, -1.0, 0.05),    # Test case 3\n        (200, 3.14159, -1.0, 1e-6), # Test case 4\n    ]\n\n    results = []\n    for params in test_cases:\n        n, k, alpha, beta = params\n\n        # Grid spacing\n        h = 1.0 / (n + 1.0)\n\n        # Vector of indices j = 1, 2, ..., n\n        j = np.arange(1, n + 1)\n\n        # Eigenvalues of the discrete Laplacian operator L\n        # lambda_j = (4/h^2) * sin^2(j*pi / (2*(n+1)))\n        lambdas = (4.0 / h**2) * np.sin(j * np.pi / (2.0 * (n + 1.0)))**2\n\n        # The cluster radius r is max(|1 - rho_j|).\n        # |1 - rho_j| = (k^2 * sqrt((alpha+1)^2 + beta^2)) / sqrt((lambda_j + alpha*k^2)^2 + (beta*k^2)^2)\n        # To maximize this, we need to minimize the denominator, which means minimizing\n        # (lambda_j + alpha*k^2)^2.\n\n        # Calculate the numerator (constant with respect to j)\n        k_sq = k**2\n        numerator = k_sq * np.sqrt((alpha + 1.0)**2 + beta**2)\n\n        # Find the minimum of the real part of the denominator's argument squared.\n        # This corresponds to finding the lambda_j closest to -alpha*k^2.\n        real_part_sq = (lambdas + alpha * k_sq)**2\n        d_min_sq = np.min(real_part_sq)\n\n        # Calculate the denominator using the minimum value found.\n        imag_part_sq = (beta * k_sq)**2\n        denominator = np.sqrt(d_min_sq + imag_part_sq)\n\n        # Calculate the cluster radius for this case.\n        # Handle the case where the denominator might be zero, though unlikely with beta  0.\n        if denominator == 0:\n            # This would imply a division by zero, which is not expected\n            # in a well-posed physical problem with damping.\n            # Assigning infinity or a very large number might be appropriate.\n            # However, for the given problem parameters, this case will not occur.\n            radius = np.inf\n        else:\n            radius = numerator / denominator\n\n        results.append(radius)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}