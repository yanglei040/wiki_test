{
    "hands_on_practices": [
        {
            "introduction": "Before tackling advanced preconditioners, it is essential to master the fundamentals of stationary iterative methods. This exercise  grounds your understanding by tasking you with the derivation of the classic Jacobi iteration from a general matrix splitting. You will then establish the fundamental spectral radius condition for convergence, a cornerstone of iterative method analysis, and apply it to determine the exact convergence rate for a model problem arising from the discretization of the Laplacian operator.",
            "id": "3605469",
            "problem": "In modeling steady, one-dimensional conductive heat flow through a uniform lithospheric column, a standard second-order centered finite-difference discretization of the differential operator with Dirichlet boundary conditions yields a linear system $A x = b$ of size $n \\times n$, where $A$ is symmetric positive definite (SPD). Consider a generic SPD matrix split as $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower-triangular part of $A$, and $U$ is the strictly upper-triangular part of $A$.\n\n(a) Starting from the definition of a stationary iteration associated with the matrix splitting $A = D + L + U$ and the linear system $A x = b$, derive the Jacobi iteration in matrix form, and identify the corresponding iteration matrix.\n\n(b) Using the linear error-propagation model for stationary iterations, state the necessary and sufficient spectral condition on the iteration matrix that guarantees convergence for any initial guess. Your reasoning should be grounded in the spectral radius characterization of powers of a matrix and norms that are consistent and submultiplicative.\n\n(c) Specialize to the discrete operator arising from the uniform-grid second-order finite-difference approximation of the one-dimensional Laplacian with Dirichlet boundaries, namely the Toeplitz tridiagonal matrix $A = \\operatorname{tridiag}(-1, 2, -1)$ of size $n \\times n$. Compute, in closed form as a function of $n$, the spectral radius of the Jacobi iteration matrix derived in part (a). Provide this spectral radius as your final boxed answer, as a single closed-form analytic expression. No numerical rounding is required.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard problem in numerical linear algebra with a clear context in computational geophysics. We may therefore proceed with a full solution. The problem will be addressed in three parts as outlined.\n\n(a) Derivation of the Jacobi Iteration and Iteration Matrix\n\nA stationary iterative method for solving the linear system $A x = b$ is defined by a splitting of the matrix $A$ into $A = M - N$, where $M$ is non-singular. The iteration is then given by $M x^{(k+1)} = N x^{(k)} + b$.\n\nThe problem specifies the standard splitting $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is the strictly lower-triangular part of $A$, and $U$ is the strictly upper-triangular part of $A$.\n\nThe Jacobi method selects the easily invertible diagonal part of $A$ for $M$. Thus, we set $M = D$. Consequently, $N = M - A = D - (D + L + U) = -(L+U)$.\n\nSubstituting these into the general form of a stationary iteration:\n$$D x^{(k+1)} = -(L+U) x^{(k)} + b$$\nFor the iteration to be well-defined, $M=D$ must be invertible. In the context of the heat equation discretized by finite differences, the matrix $A$ is strictly diagonally dominant, which ensures that all its diagonal entries are non-zero. Hence, $D$ is invertible. We can pre-multiply by $D^{-1}$ to obtain the explicit iteration formula:\n$$x^{(k+1)} = -D^{-1}(L+U) x^{(k)} + D^{-1} b$$\nThis is the matrix form of the Jacobi iteration. By comparing this to the standard form of a linear iteration $x^{(k+1)} = B x^{(k)} + c$, we can identify the Jacobi iteration matrix, $B_J$, and the constant vector, $c_J$:\n$$B_J = -D^{-1}(L+U)$$\n$$c_J = D^{-1} b$$\n\n(b) Convergence Condition for Stationary Iterations\n\nLet $x^{(k+1)} = B x^{(k)} + c$ be a stationary iteration, and let $x^*$ be the exact solution to the system $A x = b$. The exact solution must be a fixed point of the iteration, so it satisfies $x^* = B x^* + c$.\n\nThe error at the $k$-th step is defined as $e^{(k)} = x^{(k)} - x^*$. To derive the error propagation model, we subtract the fixed-point equation from the iteration equation:\n$$x^{(k+1)} - x^* = (B x^{(k)} + c) - (B x^* + c)$$\n$$e^{(k+1)} = B x^{(k)} - B x^* = B(x^{(k)} - x^*) = B e^{(k)}$$\nApplying this relationship recursively, we find the error at step $k$ in terms of the initial error $e^{(0)} = x^{(0)} - x^*$:\n$$e^{(k)} = B^k e^{(0)}$$\nFor the iteration to converge for any arbitrary initial guess $x^{(0)}$ (and thus any initial error $e^{(0)}$), the error vector $e^{(k)}$ must approach the zero vector as $k \\to \\infty$. This requires that the matrix power $B^k$ converges to the zero matrix as $k \\to \\infty$.\n\nA fundamental result in matrix analysis states that for any square matrix $B$, $\\lim_{k \\to \\infty} B^k = 0$ if and only if the spectral radius of $B$, denoted $\\rho(B)$, is strictly less than $1$. The spectral radius is defined as the maximum absolute value of the eigenvalues of $B$: $\\rho(B) = \\max_i |\\lambda_i(B)|$.\n\nThis condition is both necessary and sufficient.\n- **Necessity**: If $\\rho(B) \\geq 1$, there exists at least one eigenvalue $\\lambda$ of $B$ such that $|\\lambda| \\geq 1$. Let $v$ be a corresponding eigenvector. If we choose the initial error to be this eigenvector, $e^{(0)} = v$, then the error at step $k$ is $e^{(k)} = B^k v = \\lambda^k v$. The norm of the error is $\\|e^{(k)}\\| = \\|\\lambda^k v\\| = |\\lambda|^k \\|v\\|$. Since $|\\lambda| \\geq 1$, $\\|e^{(k)}\\|$ does not converge to $0$, and thus the iteration does not converge for this initial choice.\n- **Sufficiency**: If $\\rho(B) < 1$, convergence is guaranteed. For any vector norm and its induced (or consistent) matrix norm, the Gelfand formula states that $\\rho(B) = \\lim_{k\\to\\infty} \\|B^k\\|^{1/k}$. It can also be shown that for any $B$ and any $\\epsilon > 0$, there exists a consistent, submultiplicative matrix norm $\\|\\cdot\\|_*$ such that $\\|B\\|_* \\leq \\rho(B) + \\epsilon$. If we choose $\\epsilon$ small enough such that $\\gamma = \\rho(B) + \\epsilon < 1$, then we have $\\|e^{(k)}\\|_* \\leq \\|B^k\\|_* \\|e^{(0)}\\|_* \\leq \\|B\\|_*^k \\|e^{(0)}\\|_* \\leq \\gamma^k \\|e^{(0)}\\|_*$. Since $\\gamma < 1$, $\\gamma^k \\to 0$ as $k \\to \\infty$, which implies $\\|e^{(k)}\\|_* \\to 0$. Due to the equivalence of norms in finite-dimensional spaces, the error converges to zero in any norm.\n\nTherefore, the necessary and sufficient condition for a stationary iterative method to converge for any initial guess is that the spectral radius of its iteration matrix is strictly less than $1$.\n\n(c) Spectral Radius Calculation for the 1D Laplacian\n\nThe problem specifies the matrix $A = \\operatorname{tridiag}(-1, 2, -1)$, an $n \\times n$ matrix:\n$$A = \\begin{pmatrix}\n2 & -1 & 0 & \\cdots & 0 \\\\\n-1 & 2 & -1 & \\cdots & 0 \\\\\n0 & -1 & 2 & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & -1 \\\\\n0 & \\cdots & 0 & -1 & 2\n\\end{pmatrix}$$\nUsing the standard splitting $A=D+L+U$:\n- $D$ is the diagonal of $A$, so $D = 2I$, where $I$ is the $n \\times n$ identity matrix.\n- $L$ contains the strictly lower-triangular entries of $A$. This matrix has $-1$s on its first sub-diagonal and zeros elsewhere.\n- $U$ contains the strictly upper-triangular entries of $A$. This matrix has $-1$s on its first super-diagonal and zeros elsewhere.\n\nThe Jacobi iteration matrix is $B_J = -D^{-1}(L+U)$.\nSince $D = 2I$, its inverse is $D^{-1} = \\frac{1}{2}I$.\nThe matrix $-(L+U)$ is a tridiagonal matrix with zeros on the main diagonal and ones on the first sub- and super-diagonals.\n$$-(L+U) = \\begin{pmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n1 & 0 & 1 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 1 \\\\\n0 & \\cdots & 0 & 1 & 0\n\\end{pmatrix}$$\nTherefore, the Jacobi matrix is:\n$$B_J = \\frac{1}{2} \\times (-(L+U)) = \\operatorname{tridiag}(\\frac{1}{2}, 0, \\frac{1}{2})$$\nTo find the spectral radius of $B_J$, we need its eigenvalues. An efficient method is to relate them to the eigenvalues of $A$. From the definition $B_J = -D^{-1}(L+U)$ and $A=D+L+U \\implies L+U = A-D$, we have:\n$$B_J = -D^{-1}(A-D) = -D^{-1}A + I = I - D^{-1}A$$\nGiven that $D = 2I$, this simplifies to $B_J = I - \\frac{1}{2}A$.\nIf $\\mu$ is an eigenvalue of $A$ with eigenvector $v$, then $Av = \\mu v$. Then for the same eigenvector $v$:\n$$B_J v = (I - \\frac{1}{2}A)v = Iv - \\frac{1}{2}Av = v - \\frac{1}{2}\\mu v = (1 - \\frac{\\mu}{2})v$$\nThus, the eigenvalues $\\lambda_k$ of $B_J$ are related to the eigenvalues $\\mu_k$ of $A$ by $\\lambda_k = 1 - \\frac{\\mu_k}{2}$.\n\nThe eigenvalues of the matrix $A = \\operatorname{tridiag}(-1, 2, -1)$ are well-known and are given by:\n$$\\mu_k = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right) \\quad \\text{for } k = 1, 2, \\ldots, n$$\nSubstituting this into the expression for $\\lambda_k$:\n$$\\lambda_k = 1 - \\frac{1}{2}\\left(2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right) = 1 - 1 + \\cos\\left(\\frac{k\\pi}{n+1}\\right) = \\cos\\left(\\frac{k\\pi}{n+1}\\right)$$\nThe spectral radius of $B_J$ is the maximum of the absolute values of these eigenvalues:\n$$\\rho(B_J) = \\max_{k \\in \\{1,\\ldots,n\\}} \\left| \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right|$$\nThe arguments of the cosine function, $\\frac{k\\pi}{n+1}$, lie in the interval $(0, \\pi)$ for $k=1, \\ldots, n$. In this interval, the function $|\\cos(x)|$ is maximized when the argument $x$ is closest to $0$ or $\\pi$.\nThe set of arguments is $\\left\\{\\frac{\\pi}{n+1}, \\frac{2\\pi}{n+1}, \\ldots, \\frac{n\\pi}{n+1}\\right\\}$.\nThe argument closest to $0$ occurs at $k=1$, giving $\\frac{\\pi}{n+1}$.\nThe argument closest to $\\pi$ occurs at $k=n$, giving $\\frac{n\\pi}{n+1} = \\pi - \\frac{\\pi}{n+1}$.\nThe absolute values of the cosine at these points are:\n$|\\cos(\\frac{\\pi}{n+1})| = \\cos(\\frac{\\pi}{n+1})$ (since the argument is in $(0, \\pi/2)$ for $n \\ge 1$).\n$|\\cos(\\frac{n\\pi}{n+1})| = |\\cos(\\pi - \\frac{\\pi}{n+1})| = |-\\cos(\\frac{\\pi}{n+1})| = \\cos(\\frac{\\pi}{n+1})$.\nThe values of $|\\cos(x)|$ for arguments between these two extremes will be smaller, reaching a minimum of $0$ if $n+1$ is even and $k=(n+1)/2$.\nThus, the maximum absolute value is achieved at $k=1$ and $k=n$.\n$$\\rho(B_J) = \\cos\\left(\\frac{\\pi}{n+1}\\right)$$\nThis is the closed-form expression for the spectral radius of the Jacobi iteration matrix for the given problem. For any finite integer $n \\ge 1$, we have $0 < \\frac{\\pi}{n+1} \\le \\frac{\\pi}{2}$, which implies $0 \\le \\cos(\\frac{\\pi}{n+1}) < 1$. This confirms that the Jacobi method is convergent for this problem.",
            "answer": "$$\\boxed{\\cos\\left(\\frac{\\pi}{n+1}\\right)}$$"
        },
        {
            "introduction": "While the Jacobi method is simple, its convergence can be slow, motivating the development of more effective preconditioners for symmetric positive definite (SPD) systems. This practice  guides you through the algebraic construction of the Symmetric Successive Over-Relaxation (SSOR) preconditioner from the underlying forward and backward SOR sweeps. The derivation illuminates the structure of SSOR and its important special case, the Symmetric Gauss-Seidel (SGS) method, which are essential tools for accelerating the Conjugate Gradient solver.",
            "id": "3605522",
            "problem": "Consider a large, sparse, symmetric positive definite linear system $A x = b$ arising from a finite-difference discretization of an anisotropic diffusion equation used in subsurface conductivity modeling in computational geophysics. Let the matrix splitting be $A = D + L + U$, where $D$ is the diagonal of $A$, $L$ is its strictly lower triangular part, and $U$ is its strictly upper triangular part. The Jacobi preconditioner uses $M_{\\mathrm{J}} = D$ and the Gauss-Seidel iteration updates are built from triangular solves with $D + L$ or $D + U$. The Successive Over-Relaxation (SOR) method introduces a relaxation factor $\\omega \\in (0,2)$ into the Gauss-Seidel updates, and its symmetric counterpart, the Symmetric Successive Over-Relaxation (SSOR) method, composes a forward and a backward SOR sweep to construct a symmetric positive definite preconditioner $M_{\\mathrm{SSOR}}(\\omega)$ for use in preconditioned Conjugate Gradient.\n\nStarting from the matrix splitting $A = D + L + U$, the definitions of Gauss-Seidel and Successive Over-Relaxation iterations, and the principle that a preconditioner $M$ is constructed so that one application of $M^{-1}$ to a residual $r$ is realized by two triangular solves and a diagonal scaling, derive the explicit factorization of the SSOR preconditioner $M_{\\mathrm{SSOR}}(\\omega)$ in terms of $D$, $L$, and $U$. Then, by specializing to $\\omega = 1$, show that $M_{\\mathrm{SSOR}}(1)$ reduces to the symmetric Gauss-Seidel preconditioner and write its explicit factors in terms of $D$, $L$, and $U$ as a single closed-form analytic expression. Your final answer must be the explicit product expression for $M_{\\mathrm{SSOR}}(1)$ as a function of $D$, $L$, and $U$ with no additional commentary or symbols. No rounding is required and no units are involved.",
            "solution": "The problem requires the derivation of the Symmetric Successive Over-Relaxation (SSOR) preconditioner, $M_{\\mathrm{SSOR}}(\\omega)$, and its specific form for $\\omega=1$, known as the Symmetric Gauss-Seidel (SGS) preconditioner. The derivation proceeds from the definition of the SSOR iterative method.\n\nThe linear system to be solved is $Ax = b$, where $A$ is a symmetric positive definite matrix. The matrix $A$ is split into its diagonal ($D$), strictly lower triangular ($L$), and strictly upper triangular ($U$) parts, such that $A = D + L + U$. Since $A$ is symmetric, $U = L^T$.\n\nAn iterative method can be used to define a preconditioner $M$. The action of applying the inverse of the preconditioner to a residual vector $r$, denoted as calculating $z = M^{-1}r$, is defined as performing one full iteration of the method on the system $Az=r$, starting from an initial guess of $z_{initial}=0$.\n\nThe SSOR iteration consists of a forward SOR sweep followed by a backward SOR sweep.\n\n1.  **Forward SOR Sweep:** The update equation for an iterate $x^{(k)}$ to an intermediate iterate $x^{(k+1/2)}$ for the system $Ax=b$ is:\n    $$ (D + \\omega L) x^{(k+1/2)} = ((1-\\omega)D - \\omega U) x^{(k)} + \\omega b $$\n    where $\\omega \\in (0,2)$ is the relaxation parameter.\n\n2.  **Backward SOR Sweep:** The update from the intermediate iterate $x^{(k+1/2)}$ to the new iterate $x^{(k+1)}$ is:\n    $$ (D + \\omega U) x^{(k+1)} = ((1-\\omega)D - \\omega L) x^{(k+1/2)} + \\omega b $$\n\nTo find the action of the preconditioner's inverse, $z = M_{\\mathrm{SSOR}}^{-1}(\\omega)r$, we apply one full SSOR iteration to the system $Az=r$ starting with an initial guess of zero. Let the initial guess be $z^{(k)}=0$. The result of the iteration will be $z$.\n\nFirst, we perform the forward sweep with $b \\to r$ and $z^{(k)} \\to 0$. Let the intermediate result be $z_{inter}$.\n$$ (D + \\omega L) z_{inter} = ((1-\\omega)D - \\omega U)(0) + \\omega r = \\omega r $$\nSolving for $z_{inter}$, we get:\n$$ z_{inter} = \\omega (D + \\omega L)^{-1} r $$\n\nNext, we perform the backward sweep, using $z_{inter}$ as the input, to find the final result $z$.\n$$ (D + \\omega U) z = ((1-\\omega)D - \\omega L) z_{inter} + \\omega r $$\nSubstitute the expression for $z_{inter}$:\n$$ (D + \\omega U) z = ((1-\\omega)D - \\omega L) \\left[ \\omega (D + \\omega L)^{-1} r \\right] + \\omega r $$\nWe can factor out $\\omega$ and $r$ on the right-hand side:\n$$ (D + \\omega U) z = \\omega \\left[ ((1-\\omega)D - \\omega L) (D + \\omega L)^{-1} + I \\right] r $$\nHere, $I$ is the identity matrix. To simplify the expression in the square brackets, we use a common right-denominator of $(D + \\omega L)$:\n$$ ((1-\\omega)D - \\omega L) (D + \\omega L)^{-1} + I = \\left[ ((1-\\omega)D - \\omega L) + (D + \\omega L) \\right] (D + \\omega L)^{-1} $$\nCombining terms inside the brackets:\n$$ [ (1-\\omega)D + D - \\omega L + \\omega L ] (D + \\omega L)^{-1} = (2-\\omega)D (D + \\omega L)^{-1} $$\nSubstituting this simplified expression back into the equation for $z$:\n$$ (D + \\omega U) z = \\omega (2-\\omega)D (D + \\omega L)^{-1} r $$\nSolving for $z$ gives the action of the inverse preconditioner:\n$$ z = \\omega(2-\\omega) (D + \\omega U)^{-1} D (D + \\omega L)^{-1} r $$\nFrom this, we identify the inverse of the SSOR preconditioner:\n$$ M_{\\mathrm{SSOR}}^{-1}(\\omega) = \\omega(2-\\omega) (D + \\omega U)^{-1} D (D + \\omega L)^{-1} $$\n\nTo obtain the SSOR preconditioner $M_{\\mathrm{SSOR}}(\\omega)$, we take the inverse of $M_{\\mathrm{SSOR}}^{-1}(\\omega)$. Using the property $(cXYZ)^{-1} = c^{-1}Z^{-1}Y^{-1}X^{-1}$:\n$$ M_{\\mathrm{SSOR}}(\\omega) = \\left[ \\omega(2-\\omega) (D + \\omega U)^{-1} D (D + \\omega L)^{-1} \\right]^{-1} $$\n$$ M_{\\mathrm{SSOR}}(\\omega) = \\frac{1}{\\omega(2-\\omega)} ((D + \\omega L)^{-1})^{-1} (D)^{-1} ((D + \\omega U)^{-1})^{-1} $$\n$$ M_{\\mathrm{SSOR}}(\\omega) = \\frac{1}{\\omega(2-\\omega)} (D + \\omega L) D^{-1} (D + \\omega U) $$\nThis expression represents the explicit factorization of the SSOR preconditioner in terms of $D$, $L$, and $U$. The preconditioner is symmetric because $A$ is symmetric (so $U=L^T$) and $D$ is diagonal, making $M_{\\mathrm{SSOR}}(\\omega)$ of the form $C K D^{-1} K^T$.\n\nThe problem then asks to specialize this result to $\\omega=1$. This corresponds to the Symmetric Gauss-Seidel (SGS) preconditioner, which we denote as $M_{\\mathrm{SSOR}}(1)$. Substituting $\\omega = 1$ into the derived formula:\n$$ M_{\\mathrm{SSOR}}(1) = \\frac{1}{1(2-1)} (D + 1 \\cdot L) D^{-1} (D + 1 \\cdot U) $$\n$$ M_{\\mathrm{SSOR}}(1) = (D + L) D^{-1} (D + U) $$\nThis is the explicit product expression for the Symmetric Gauss-Seidel preconditioner.",
            "answer": "$$\\boxed{(D + L) D^{-1} (D + U)}$$"
        },
        {
            "introduction": "Theoretical elegance must meet practical reality in computational geophysics, where discretized operators are not always well-behaved. This hands-on coding challenge  moves from ideal theory to robust implementation, focusing on the Helmholtz equation relevant to wave propagation, where the system matrix may not be positive definite. You will implement the SSOR preconditioner and learn the critical skill of testing for the symmetric positive definite (SPD) property, building a fallback strategy for cases where the preconditioner is unsuitable for the Conjugate Gradient method.",
            "id": "3605479",
            "problem": "Consider the one-dimensional frequency-domain Helmholtz operator arising in wave propagation with mild damping, discretized on a uniform grid with Dirichlet boundary conditions. Let the continuum operator be defined on the interval $[0,L]$ by\n$$\n\\mathcal{H}u(x) = -\\frac{d^2 u}{dx^2}(x) - k(x)^2 u(x) + \\delta u(x),\n$$\nwhere $k(x)$ is the spatially varying wavenumber and $\\delta \\ge 0$ is a small real damping. Using standard second-order centered finite differences on $N$ interior grid points with spacing $h = L/(N+1)$, the discrete linear system is $A u = f$, where $A \\in \\mathbb{R}^{N \\times N}$ is the symmetric tridiagonal matrix with stencil coefficients\n$$\nA_{i,i} = \\frac{2}{h^2} - k_i^2 + \\delta,\\quad A_{i,i-1} = A_{i,i+1} = -\\frac{1}{h^2},\\quad 1 \\le i \\le N,\n$$\nwith $k_i = k(x_i)$ and $x_i = i h$. The matrix $A$ can be Symmetric Positive Definite (SPD), indefinite, or poorly conditioned depending on $k(x)$ and $\\delta$.\n\nDefine the diagonal $D = \\operatorname{diag}(A)$, the strictly lower triangular part $L$ of $A$, and the strictly upper triangular part $U$ of $A$. For a relaxation parameter $\\omega$, the Symmetric Successive Over-Relaxation (SSOR) preconditioner is\n$$\nM_{\\mathrm{SSOR}}(\\omega) = \\frac{1}{\\omega(2 - \\omega)} \\left( D + \\omega L \\right) D^{-1} \\left( D + \\omega U \\right),\n$$\nprovided $0 < \\omega < 2$. For a Symmetric Gauss–Seidel preconditioner, set $\\omega = 1$. The Jacobi preconditioner is $M_{\\mathrm{J}} = D$. In Preconditioned Conjugate Gradient, $M$ must be SPD. For indefinite $A$, $M_{\\mathrm{SSOR}}(\\omega)$ can lose SPD, which is detectable by failure of a Cholesky factorization.\n\nYour task is to write a complete program that:\n- Constructs $A$ for several test cases using the formula above.\n- Forms $M_{\\mathrm{SSOR}}(\\omega)$ for each case.\n- Detects whether $M_{\\mathrm{SSOR}}(\\omega)$ is SPD by attempting a Cholesky factorization of $M_{\\mathrm{SSOR}}(\\omega)$.\n- If $M_{\\mathrm{SSOR}}(\\omega)$ is not SPD, partitions the index set $\\{1,2,\\dots,N\\}$ into contiguous blocks of equal size $B$ and, for each block, inspects the principal submatrix $A_{\\mathrm{block}}$. If $A_{\\mathrm{block}}$ is SPD, recommend symmetric Gauss–Seidel for that block; otherwise recommend Jacobi. Report the number of blocks recommended for symmetric Gauss–Seidel and for Jacobi.\n\nUse the following test suite, which varies the wavenumber and relaxation parameter to exercise SPD, indefinite, and boundary cases. In all cases, set $L = 1$:\n- Case $1$: $N = 64$, uniform $k(x) \\equiv 0$, $\\delta = 0$, $\\omega = 1$, block size $B = 16$.\n- Case $2$: $N = 64$, uniform $k(x) \\equiv 5$, $\\delta = 0.1$, $\\omega = 1.2$, block size $B = 16$.\n- Case $3$: $N = 64$, piecewise $k(x)$ with $k(x) \\equiv 2$ on $x \\in [0,0.5)$ and $k(x) \\equiv 5$ on $x \\in [0.5,1]$, $\\delta = 0.05$, $\\omega = 1$, block size $B = 8$.\n- Case $4$: $N = 64$, uniform $k(x) \\equiv 2$, $\\delta = 0$, $\\omega = 1.99$, block size $B = 16$.\n- Case $5$: $N = 64$, uniform $k(x) \\equiv 2$, $\\delta = 0$, $\\omega = 2.1$, block size $B = 16$.\n\nFor each case, the program must produce a list containing:\n- A boolean indicating whether $M_{\\mathrm{SSOR}}(\\omega)$ is SPD.\n- An integer count of blocks recommended for symmetric Gauss–Seidel.\n- An integer count of blocks recommended for Jacobi.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the form $[ \\text{boolean}, \\text{integer}, \\text{integer} ]$. For example, an output with two hypothetical cases would look like $[[\\text{True},2,3],[\\text{False},1,4]]$. No units must be included in the output.",
            "solution": "The present task is to analyze the Symmetric Positive Definite (SPD) property of the Symmetric Successive Over-Relaxation (SSOR) preconditioner for a discretized one-dimensional Helmholtz operator. If the SSOR preconditioner is not SPD, a fallback strategy based on a block-wise analysis of the system matrix is to be employed.\n\nThe continuum Helmholtz operator is given by:\n$$\n\\mathcal{H}u(x) = -\\frac{d^2 u}{dx^2}(x) - k(x)^2 u(x) + \\delta u(x)\n$$\non the domain $x \\in [0,L]$, with $k(x)$ being the wavenumber and $\\delta \\ge 0$ a damping term. A second-order centered finite difference scheme on a uniform grid of $N$ interior points with spacing $h = L/(N+1)$ transforms this into a linear system $A u = f$. The matrix $A$ is a real, symmetric, tridiagonal matrix of size $N \\times N$, with its non-zero entries defined by the stencil:\n$$\nA_{i,i} = \\frac{2}{h^2} - k_i^2 + \\delta, \\quad A_{i,i-1} = A_{i,i+1} = -\\frac{1}{h^2}\n$$\nfor $1 \\le i \\le N$, where $k_i = k(x_i)$ at grid points $x_i = i h$. Depending on the values of $k_i$, the matrix $A$ can be SPD or indefinite.\n\nThe SSOR preconditioner, $M_{\\mathrm{SSOR}}(\\omega)$, is constructed from $A$ using a relaxation parameter $\\omega$. We first decompose $A$ into its diagonal ($D$), strictly lower triangular ($L$), and strictly upper triangular ($U$) parts, such that $A = D + L + U$. The SSOR preconditioner matrix is then defined as:\n$$\nM_{\\mathrm{SSOR}}(\\omega) = \\frac{1}{\\omega(2 - \\omega)} \\left( D + \\omega L \\right) D^{-1} \\left( D + \\omega U \\right)\n$$\nThis formula is valid for $\\omega \\in \\mathbb{R} \\setminus \\{0, 2\\}$. For its common use in accelerating iterative methods for SPD systems, $\\omega$ is typically restricted to the interval $0  \\omega  2$, which ensures the pre-factor $1/(\\omega(2-\\omega))$ is positive. For $\\omega=1$, this preconditioner is known as the Symmetric Gauss-Seidel (SGS) preconditioner. The simplest preconditioner is the Jacobi preconditioner, $M_{\\mathrm{J}} = D$.\n\nA crucial requirement for using a preconditioner $M$ with the Preconditioned Conjugate Gradient (PCG) method is that $M$ must be SPD. A standard and numerically robust method to verify if a symmetric matrix is positive definite is to attempt its Cholesky factorization. A matrix $M$ is SPD if and only if it has a Cholesky factorization $M = R^T R$, where $R$ is an upper triangular matrix with positive diagonal entries. Numerical libraries that compute this factorization will fail and raise an error if the matrix is not SPD.\n\nThe algorithmic procedure for each test case is as follows:\n1.  **Construct the Matrix $A$**: Given the parameters $N$, $L$, $\\delta$, and the function $k(x)$, the grid spacing $h = L/(N+1)$ is calculated. The grid points $x_i$ and corresponding wavenumbers $k_i$ are determined. The tridiagonal matrix $A$ is then constructed according to its definition.\n2.  **Construct the Preconditioner $M_{\\mathrm{SSOR}}(\\omega)$**: The matrices $D$, $L$, and $U$ are extracted from $A$. The inverse of the diagonal matrix, $D^{-1}$, is computed by inverting its diagonal elements. These components are then combined using the formula for $M_{\\mathrm{SSOR}}(\\omega)$. This step is performed for any specified $\\omega \\neq 0, 2$. Note that if $\\omega > 2$ or $\\omega  0$, the pre-factor $1/(\\omega(2-\\omega))$ is negative, which will typically render $M_{\\mathrm{SSOR}}(\\omega)$ not positive definite.\n3.  **Test for SPD Property**: A Cholesky factorization of $M_{\\mathrm{SSOR}}(\\omega)$ is attempted. If the factorization succeeds, $M_{\\mathrm{SSOR}}(\\omega)$ is confirmed to be SPD. If it fails, the matrix is not SPD.\n4.  **Fallback Block-wise Analysis**: If $M_{\\mathrm{SSOR}}(\\omega)$ is found not to be SPD, the problem specifies a fallback procedure. The index set $\\{1, 2, \\dots, N\\}$ is partitioned into contiguous blocks of a given size $B$. For each block, the corresponding principal submatrix of the original system matrix, $A_{\\mathrm{block}}$, is extracted. Each $A_{\\mathrm{block}}$ is then tested for the SPD property via Cholesky factorization.\n5.  **Recommendation and Reporting**: Based on the block-wise analysis:\n    - If a submatrix $A_{\\mathrm{block}}$ is SPD, the recommendation for that block is to use a Symmetric Gauss-Seidel (SGS) preconditioner, which is known to be a good choice for SPD problems.\n    - If $A_{\\mathrm{block}}$ is not SPD, the safer and simpler Jacobi preconditioner ($M_J=D$) is recommended.\n    The final output for the case is a list containing a boolean for the SPD status of $M_{\\mathrm{SSOR}}(\\omega)$, and the integer counts of blocks for which SGS and Jacobi preconditioners are recommended, respectively. If $M_{\\mathrm{SSOR}}(\\omega)$ is SPD, these counts are both zero as the fallback is not triggered.\n\nThis process is systematically applied to each test case, varying the physical and numerical parameters to explore different regimes of the problem.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, LinAlgError\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def run_case(N, k_func, delta, omega, B):\n        \"\"\"\n        Processes a single test case.\n\n        Args:\n            N (int): Number of interior grid points.\n            k_func (callable): Function defining k(x).\n            delta (float): Damping coefficient.\n            omega (float): SSOR relaxation parameter.\n            B (int): Block size for fallback analysis.\n\n        Returns:\n            list: A list containing [is_M_spd, sgs_count, jacobi_count].\n        \"\"\"\n        L_domain = 1.0\n        h = L_domain / (N + 1)\n        x = np.arange(1, N + 1) * h\n        k = k_func(x)\n\n        # Construct the matrix A\n        diag_A = 2.0 / h**2 - k**2 + delta\n        off_diag_A = -1.0 / h**2\n        \n        A = np.diag(diag_A)\n        if N  1:\n            A += np.diag([off_diag_A] * (N - 1), k=1)\n            A += np.diag([off_diag_A] * (N - 1), k=-1)\n\n        # Initialize results\n        is_M_spd = False\n        sgs_count = 0\n        jacobi_count = 0\n        \n        # Check for invalid omega that leads to division by zero\n        if omega == 2.0 or omega == 0.0:\n            is_M_spd = False\n        else:\n            # Construct the SSOR preconditioner M_ssor\n            D = np.diag(np.diag(A))\n            if np.any(np.diag(D) == 0):\n                # D is not invertible, M_ssor cannot be formed correctly. Treat as not SPD.\n                is_M_spd = False\n            else:\n                L_mat = np.tril(A, k=-1)\n                U_mat = np.triu(A, k=1)\n                D_inv = np.diag(1.0 / np.diag(D))\n                \n                prefactor = 1.0 / (omega * (2.0 - omega))\n                \n                # Using @ for matrix multiplication\n                term1 = D + omega * L_mat\n                term2 = D + omega * U_mat\n                M_ssor = prefactor * (term1 @ D_inv @ term2)\n\n                # Test if M_ssor is SPD using Cholesky factorization\n                try:\n                    cholesky(M_ssor, lower=True)\n                    is_M_spd = True\n                except LinAlgError:\n                    is_M_spd = False\n\n        # If M_ssor is not SPD, perform block-wise analysis of A\n        if not is_M_spd:\n            num_blocks = N // B\n            for i in range(num_blocks):\n                start_idx = i * B\n                end_idx = (i + 1) * B\n                A_block = A[start_idx:end_idx, start_idx:end_idx]\n                \n                try:\n                    cholesky(A_block, lower=True)\n                    sgs_count += 1\n                except LinAlgError:\n                    jacobi_count += 1\n                    \n        return [is_M_spd, sgs_count, jacobi_count]\n\n    test_cases = [\n        # Case 1: N=64, k(x)=0, delta=0, omega=1, B=16\n        (64, lambda x: np.zeros_like(x), 0.0, 1.0, 16),\n        # Case 2: N=64, k(x)=5, delta=0.1, omega=1.2, B=16\n        (64, lambda x: np.full_like(x, 5.0), 0.1, 1.2, 16),\n        # Case 3: N=64, piecewise k(x), delta=0.05, omega=1, B=8\n        (64, lambda x: np.where(x  0.5, 2.0, 5.0), 0.05, 1.0, 8),\n        # Case 4: N=64, k(x)=2, delta=0, omega=1.99, B=16\n        (64, lambda x: np.full_like(x, 2.0), 0.0, 1.99, 16),\n        # Case 5: N=64, k(x)=2, delta=0, omega=2.1, B=16\n        (64, lambda x: np.full_like(x, 2.0), 0.0, 2.1, 16),\n    ]\n\n    results = []\n    for case in test_cases:\n        N_val, k_f, delta_val, omega_val, B_val = case\n        result = run_case(N_val, k_f, delta_val, omega_val, B_val)\n        results.append(result)\n\n    # Format the final output string\n    results_str = [f\"[{'True' if res[0] else 'False'},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(results_str)}]\")\n\n\nsolve()\n```"
        }
    ]
}