{
    "hands_on_practices": [
        {
            "introduction": "To truly master an algorithm, it is essential to trace its execution by hand on a small, manageable example. This first exercise provides a direct, hands-on walkthrough of the Generalized Minimal Residual (GMRES) method. By performing the first few steps of the Arnoldi process and solving the resulting least-squares problem using Givens rotations, you will build a concrete understanding of how GMRES constructs its solution and minimizes the residual in the Krylov subspace. ",
            "id": "3616841",
            "problem": "Consider a small nonsymmetric linear system that arises from a coarse finite-volume discretization of a one-dimensional steady advection–diffusion operator used in linearized seismic travel-time tomography. Let the system matrix be\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 3 & 0 \\\\\n0 & 1 & 2\n\\end{pmatrix},\n$$\nthe right-hand side be\n$$\nb = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n$$\nand the initial guess be\n$$\nx_0 = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nPerform two steps of the Arnoldi process starting from the initial residual $r_0 = b - A x_0$, forming the orthonormal basis $V_{m+1}$ with $m = 2$ and the $(m+1) \\times m$ upper Hessenberg matrix $\\bar{H}_m$. Then, apply two successive Givens rotations to $\\bar{H}_m$ to eliminate its subdiagonal entries and obtain an upper triangular $R_m$, while simultaneously updating the transformed right-hand side. Use this to compute the Generalized Minimal Residual (GMRES) iterate $x_m = x_0 + V_m y$ and the residual $r_m = b - A x_m$ explicitly. Report the three components of $x_m$ followed by the three components of $r_m$ as a single row matrix. No rounding is required, and no physical units are needed. Define Generalized Minimal Residual (GMRES) on its first use and proceed from first principles, starting from the Krylov subspace construction and orthonormalization.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and self-contained.\n\n### Step 1: Extract Givens\n- System matrix: $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$\n- Right-hand side vector: $b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$\n- Initial guess: $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$\n- Number of GMRES steps (Krylov subspace dimension): $m = 2$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem addresses the Generalized Minimal Residual (GMRES) method, a standard and fundamental algorithm in numerical linear algebra for solving nonsymmetric linear systems. The context of seismic tomography is a valid application area for such methods. The problem is mathematically and scientifically sound.\n- **Well-Posed**: The matrix $A$ is a $3 \\times 3$ lower triangular matrix. Its determinant is the product of its diagonal entries, $\\det(A) = 1 \\cdot 3 \\cdot 2 = 6 \\neq 0$. Since the matrix is non-singular, a unique solution to the system $Ax = b$ exists. The problem asks for the execution of a specific number of steps of a well-defined algorithm (GMRES), which leads to a unique iterate $x_2$.\n- **Objective**: The problem is stated using precise mathematical definitions and contains no subjective or ambiguous language.\n- **Completeness**: All necessary data ($A$, $b$, $x_0$, and $m$) are provided to perform the requested calculations.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided.\n\n---\n\nThe Generalized Minimal Residual (GMRES) method is an iterative algorithm for solving the linear system of equations $Ax = b$, particularly when the matrix $A$ is large, sparse, and nonsymmetric. Starting with an initial guess $x_0$, GMRES finds an approximate solution $x_m$ from the affine space $x_0 + K_m(A, r_0)$, where $K_m(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{m-1}r_0\\}$ is the $m$-th Krylov subspace generated by $A$ and the initial residual $r_0 = b - Ax_0$. The iterate $x_m$ is chosen to minimize the Euclidean norm of the residual, i.e., $x_m = \\arg \\min_{z \\in x_0 + K_m} \\|b - Az\\|_2$.\n\nThe solution process involves three main stages:\n1.  Use the Arnoldi process to generate an orthonormal basis $V_{m+1}$ for the Krylov subspace $K_{m+1}(A, r_0)$ and an upper Hessenberg matrix $\\bar{H}_m$.\n2.  Solve a small least-squares problem to find the vector $y_m$ that minimizes the residual in the Krylov subspace.\n3.  Compute the approximate solution $x_m = x_0 + V_m y_m$ and the corresponding residual $r_m = b - A x_m$.\n\nWe will now execute these steps for the given problem with $m=2$.\n\n**1. Initial Setup and Arnoldi Process**\n\nFirst, compute the initial residual $r_0$:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe norm is $\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1$.\nThe first basis vector of the Krylov subspace is:\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, we perform $m=2$ steps of the Arnoldi process.\n\n**Step k=1:**\nCompute $w_1 = A v_1$:\n$$\nw_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nOrthogonalize $w_1$ against the previous basis vectors (only $v_1$):\n$$\nh_{1,1} = v_1^T w_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 1\n$$\n$$\n\\tilde{w}_1 = w_1 - h_{1,1} v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nCompute the norm and the next basis vector:\n$$\nh_{2,1} = \\|\\tilde{w}_1\\|_2 = \\sqrt{0^2 + 1^2 + 0^2} = 1\n$$\n$$\nv_2 = \\frac{\\tilde{w}_1}{h_{2,1}} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n**Step k=2:**\nCompute $w_2 = A v_2$:\n$$\nw_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\nOrthogonalize $w_2$ against $v_1$ and $v_2$:\n$$\nh_{1,2} = v_1^T w_2 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = 0\n$$\n$$\nh_{2,2} = v_2^T w_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = 3\n$$\n$$\n\\tilde{w}_2 = w_2 - h_{1,2} v_1 - h_{2,2} v_2 = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 3 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nCompute the norm and the next basis vector:\n$$\nh_{3,2} = \\|\\tilde{w}_2\\|_2 = \\sqrt{0^2 + 0^2 + 1^2} = 1\n$$\n$$\nv_3 = \\frac{\\tilde{w}_2}{h_{3,2}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe Arnoldi process yields the orthonormal basis matrix $V_3 = [v_1, v_2, v_3] = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I$ and the $(m+1) \\times m = 3 \\times 2$ upper Hessenberg matrix $\\bar{H}_2$:\n$$\n\\bar{H}_2 = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix}\n$$\n\n**2. Least-Squares Problem and Givens Rotations**\n\nThe GMRES iterate is $x_m = x_0 + V_m y_m$, where $y_m$ solves the least-squares problem:\n$$\ny_m = \\arg \\min_{y \\in \\mathbb{R}^m} \\| \\|r_0\\|_2 e_1 - \\bar{H}_m y \\|_2\n$$\nFor $m=2$, with $\\|r_0\\|_2=1$, this becomes:\n$$\ny_2 = \\arg \\min_{y \\in \\mathbb{R}^2} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix} y \\right\\|_2\n$$\nWe solve this by applying a sequence of Givens rotations to transform $\\bar{H}_2$ into an upper triangular matrix $R_2$.\n\n**First Givens Rotation $\\Omega_1$:**\nTo eliminate the element $h_{2,1}=1$, we apply a rotation on rows $1$ and $2$.\nLet $a = h_{1,1} = 1$ and $b = h_{2,1} = 1$. The rotation coefficients are $c = \\frac{a}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{2}}$ and $s = \\frac{b}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{2}}$.\nThe rotation matrix is $\\Omega_1 = \\begin{pmatrix} c & s & 0 \\\\ -s & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nApplying $\\Omega_1$ to $\\bar{H}_2$:\n$$\n\\Omega_1 \\bar{H}_2 = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & 3/\\sqrt{2} \\\\ 0 & 1 \\end{pmatrix}\n$$\nApplying $\\Omega_1$ to the right-hand side vector $g = \\|r_0\\|_2 e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$:\n$$\n\\Omega_1 g = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix}\n$$\n\n**Second Givens Rotation $\\Omega_2$:**\nTo eliminate the element at position $(3,2)$, which is $1$, we apply a rotation on rows $2$ and $3$ of the transformed system.\nLet $a = 3/\\sqrt{2}$ and $b = 1$. The rotation coefficients are $c = \\frac{a}{\\sqrt{a^2+b^2}} = \\frac{3/\\sqrt{2}}{\\sqrt{9/2+1}} = \\frac{3/\\sqrt{2}}{\\sqrt{11/2}} = \\frac{3}{\\sqrt{11}}$ and $s = \\frac{b}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{11/2}} = \\frac{\\sqrt{2}}{\\sqrt{11}}$.\nThe rotation matrix is $\\Omega_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & c & s \\\\ 0 & -s & c \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix}$.\nApplying $\\Omega_2$ to $\\Omega_1 \\bar{H}_2$:\n$$\n\\Omega_2 (\\Omega_1 \\bar{H}_2) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & 3/\\sqrt{2} \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\frac{9}{\\sqrt{22}} + \\frac{\\sqrt{2}}{\\sqrt{11}} \\\\ 0 & \\frac{-3\\sqrt{2}}{\\sqrt{22}} + \\frac{3}{\\sqrt{11}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis gives the upper triangular matrix $R_2 = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\end{pmatrix}$.\nApplying $\\Omega_2$ to the transformed right-hand side $\\Omega_1 g$:\n$$\ng' = \\Omega_2 (\\Omega_1 g) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -3/\\sqrt{22} \\\\ 1/\\sqrt{11} \\end{pmatrix}\n$$\nThe least-squares problem is now equivalent to solving $R_2 y_2 = g'_1$, where $g'_1$ consists of the first two components of $g'$.\n$$\n\\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -3/\\sqrt{22} \\end{pmatrix}\n$$\nSolving by back substitution:\nFrom the second row:\n$\\sqrt{11/2} \\cdot y_2 = -3/\\sqrt{22} \\implies y_2 = \\frac{-3}{\\sqrt{22}} \\frac{\\sqrt{2}}{\\sqrt{11}} = \\frac{-3}{\\sqrt{11}\\sqrt{2}} \\frac{\\sqrt{2}}{\\sqrt{11}} = -\\frac{3}{11}$.\nFrom the first row:\n$\\sqrt{2} \\cdot y_1 + (3/\\sqrt{2}) y_2 = 1/\\sqrt{2} \\implies \\sqrt{2} \\cdot y_1 + (3/\\sqrt{2})(-3/11) = 1/\\sqrt{2}$.\n$\\sqrt{2} \\cdot y_1 = 1/\\sqrt{2} + 9/(11\\sqrt{2}) = \\frac{11+9}{11\\sqrt{2}} = \\frac{20}{11\\sqrt{2}}$.\n$y_1 = \\frac{20}{11\\sqrt{2}} \\frac{1}{\\sqrt{2}} = \\frac{20}{22} = \\frac{10}{11}$.\nSo, $y_2 = \\begin{pmatrix} 10/11 \\\\ -3/11 \\end{pmatrix}$.\n\n**3. Compute Solution and Residual**\n\nThe GMRES iterate $x_2$ is:\n$$\nx_2 = x_0 + V_2 y_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 10/11 \\\\ -3/11 \\end{pmatrix} = \\begin{pmatrix} 10/11 \\\\ -3/11 \\\\ 0 \\end{pmatrix}\n$$\nThe final residual $r_2$ is:\n$$\nr_2 = b - A x_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 10/11 \\\\ -3/11 \\\\ 0 \\end{pmatrix}\n$$\n$$\nA x_2 = \\begin{pmatrix} 1 \\cdot (10/11) \\\\ 1 \\cdot (10/11) + 3 \\cdot (-3/11) \\\\ 1 \\cdot (-3/11) \\end{pmatrix} = \\begin{pmatrix} 10/11 \\\\ 1/11 \\\\ -3/11 \\end{pmatrix}\n$$\n$$\nr_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 10/11 \\\\ 1/11 \\\\ -3/11 \\end{pmatrix} = \\begin{pmatrix} 1/11 \\\\ -1/11 \\\\ 3/11 \\end{pmatrix}\n$$\nThe final result consists of the three components of $x_2$ followed by the three components of $r_2$.\n$x_2 = (10/11, -3/11, 0)$\n$r_2 = (1/11, -1/11, 3/11)$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{10}{11} & -\\frac{3}{11} & 0 & \\frac{1}{11} & -\\frac{1}{11} & \\frac{3}{11} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While GMRES guarantees a non-increasing residual norm, it does not always guarantee a strict decrease in every step. This practice delves into the important phenomenon of stagnation, where the algorithm makes no progress in reducing the residual norm. You will first explain the underlying mechanism from first principles and then construct a specific example where the Krylov subspace's alignment with an invariant subspace of the system matrix leads to this exact behavior. ",
            "id": "3588155",
            "problem": "Consider the Generalized Minimal Residual method (GMRES), which for a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and a linear system $A x = b$, constructs iterates $x_m \\in x_0 + \\mathcal{K}_m(A,r_0)$ that minimize the $2$-norm of the residual $r_m = b - A x_m$, where the Krylov subspace is defined by $\\mathcal{K}_m(A,r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{m-1} r_0\\}$ and $r_0 = b - A x_0$. Explain from first principles why GMRES stagnation can occur, in the sense that $\\|r_m\\|_2 = \\|r_{m-1}\\|_2$ for some $m$ even though $r_m \\neq 0$, when $\\mathcal{K}_m(A,r_0)$ aligns with an invariant subspace of $A$ that does not reduce the residual under the GMRES update. Your explanation must start from the core definition that GMRES minimizes $\\|r_m\\|_2$ over $x_0 + \\mathcal{K}_m(A,r_0)$ and must justify the mechanism by which the additional Krylov direction fails to provide any reduction in residual norm.\n\nThen, for an explicit construction illustrating this mechanism, take\n$$\nA = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{pmatrix}, \\quad\nx_0 = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nVerify that the Krylov subspaces $\\mathcal{K}_m(A,r_0)$ align with an $A$-invariant subspace that does not reduce the residual norm, and prove that GMRES stagnates starting at the first step while the residual remains nonzero. Compute the exact value of $\\|r_2\\|_2$. Your final answer must be a single real number with no units.",
            "solution": "The analysis of this problem proceeds in two parts. First, we will explain from first principles the mechanism of GMRES stagnation when the Krylov subspace is confined to a particular type of invariant subspace of the matrix $A$. Second, we will apply this understanding to the specific numerical example provided, demonstrating the stagnation and calculating the required residual norm.\n\nThe Generalized Minimal Residual method (GMRES) is an iterative method for solving a linear system of equations $Ax = b$, where $A \\in \\mathbb{R}^{n \\times n}$ and $b, x \\in \\mathbb{R}^n$. Starting with an initial guess $x_0$, GMRES generates a sequence of approximations $x_m$ for $m=1, 2, \\dots$. At each step $m$, the iterate $x_m$ is chosen from the affine Krylov subspace $x_0 + \\mathcal{K}_m(A, r_0)$, where $r_0 = b - A x_0$ is the initial residual and $\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{m-1} r_0\\}$ is the $m$-th Krylov subspace. The defining property of GMRES is that the iterate $x_m$ is chosen to minimize the Euclidean norm (2-norm) of the corresponding residual, $r_m = b - A x_m$.\n\nLet's formalize the minimization problem. An iterate $x_m$ can be written as $x_m = x_0 + y_m$ for some vector $y_m \\in \\mathcal{K}_m(A, r_0)$. The corresponding residual is\n$$\nr_m = b - A x_m = b - A(x_0 + y_m) = (b - A x_0) - A y_m = r_0 - A y_m.\n$$\nThus, the GMRES problem at step $m$ is to find a vector $y_m \\in \\mathcal{K}_m(A, r_0)$ that solves the minimization problem:\n$$\n\\min_{y \\in \\mathcal{K}_m(A, r_0)} \\|r_0 - A y\\|_2.\n$$\nThis is a standard linear least-squares problem. Geometrically, the vector $A y_m$ is the orthogonal projection of the initial residual $r_0$ onto the subspace $U_m = A\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{A r_0, A^2 r_0, \\dots, A^m r_0\\}$. The resulting residual $r_m$ is given by $r_m = r_0 - P_{U_m}(r_0)$, where $P_{U_m}$ denotes the orthogonal projector onto $U_m$. Consequently, $r_m$ is orthogonal to the subspace $U_m$. The norm of the residual is non-increasing, i.e., $\\|r_m\\|_2 \\le \\|r_{m-1}\\|_2$, because $\\mathcal{K}_{m-1}(A, r_0) \\subset \\mathcal{K}_m(A, r_0)$ which implies $U_{m-1} \\subset U_m$.\n\nStagnation, in the sense that $\\|r_m\\|_2 = \\|r_{m-1}\\|_2$ for some $m$ while $r_m \\neq 0$, occurs when the inclusion of the new search direction in $\\mathcal{K}_m$ provides no further reduction in the residual norm. This happens if the optimal solution $y_m$ to the minimization problem over $\\mathcal{K}_m(A, r_0)$ already lies in the smaller subspace $\\mathcal{K}_{m-1}(A, r_0)$. This is equivalent to stating that the projection of $r_0$ onto $U_m$ is the same as its projection onto $U_{m-1}$. This occurs if the new basis vector spanning the orthogonal complement of $U_{m-1}$ in $U_m$ is itself orthogonal to $r_0$.\n\nThe problem states that stagnation can occur when the Krylov subspace \"aligns with an invariant subspace of $A$\". Let $S$ be an $A$-invariant subspace, meaning $A(S) \\subseteq S$. A particularly illustrative case for stagnation arises when the initial residual $r_0$ lies within an invariant subspace $S$ which has the additional property that $r_0$ is orthogonal to the image of $S$ under $A$, i.e., $r_0 \\perp A(S)$.\n\nIf $r_0 \\in S$, then since $S$ is $A$-invariant, all subsequent Krylov vectors $A^k r_0$ for $k \\ge 1$ also lie in $S$. Therefore, the entire Krylov subspace $\\mathcal{K}_m(A, r_0)$ is contained within $S$ for all $m \\ge 1$.\nThe GMRES update vector $y_m$ is sought in $\\mathcal{K}_m(A, r_0)$, so $y_m \\in S$. The vector $Ay_m$ is then in $A(S)$.\nIf we have the condition $r_0 \\perp A(S)$, then for any $y_m \\in \\mathcal{K}_m(A,r_0) \\subseteq S$, we have $Ay_m \\in A(S)$, which implies $\\langle r_0, Ay_m \\rangle = 0$.\nThe residual norm squared at step $m$ is\n$$\n\\|r_m\\|_2^2 = \\|r_0 - Ay_m\\|_2^2 = \\langle r_0 - Ay_m, r_0 - Ay_m \\rangle = \\|r_0\\|_2^2 - 2\\langle r_0, Ay_m \\rangle + \\|Ay_m\\|_2^2.\n$$\nSince $\\langle r_0, Ay_m \\rangle = 0$, this simplifies to\n$$\n\\|r_m\\|_2^2 = \\|r_0\\|_2^2 + \\|Ay_m\\|_2^2.\n$$\nTo minimize this quantity, we must choose $y_m$ such that $\\|Ay_m\\|_2^2$ is minimized. The minimum possible value is $0$, which may be achievable if $0 \\in A(\\mathcal{K}_m(A,r_0))$. For instance, we may choose $y_m=0$, which is always in $\\mathcal{K}_m(A,r_0)$. This choice yields the minimum possible residual norm under these conditions, which is $\\|r_0\\|_2$.\nThe resulting residual is $r_m = r_0 - A(0) = r_0$. Since this holds for any $m \\ge 1$, we have $r_m = r_{m-1} = \\dots = r_0$. Consequently, $\\|r_m\\|_2 = \\|r_0\\|_2$ for all $m \\ge 1$. This constitutes complete stagnation from the very first step, with the residual norm remaining fixed at its initial value, provided $r_0 \\neq 0$. The mechanism is that the search directions $y_m$ are chosen from a subspace $S$ which is mapped by $A$ into a subspace $A(S)$ that is orthogonal to the initial residual $r_0$. Therefore, no linear combination of vectors of the form $Ay$ can have a component in the direction of $r_0$, making any reduction of the residual impossible.\n\nNow, we verify this mechanism with the provided example:\n$$\nA = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nFirst, we compute the initial residual $r_0$:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - A \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe 2-norm of the initial residual is $\\|r_0\\|_2 = 1$. Let $e_1 = (1, 0, 0)^T$. So, $r_0 = e_1$.\n\nThe matrix $A$ is diagonal. Its eigenvalues are $\\lambda_1=0$, $\\lambda_2=2$, and $\\lambda_3=3$, with corresponding eigenvectors $e_1$, $e_2$, and $e_3$. The subspace $S = \\operatorname{span}\\{e_1\\}$ is the eigenspace for $\\lambda_1=0$. An eigenspace is always an invariant subspace. In this case, $S$ is also the null space of $A$, so for any vector $v \\in S$, $Av = 0$.\n\nThe initial residual $r_0 = e_1$ is in the invariant subspace $S$. This means the Krylov sequence will be confined to $S$. Let's compute the Krylov vectors:\n$$\nA r_0 = A e_1 = \\lambda_1 e_1 = 0 \\cdot e_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n$$\nA^k r_0 = 0 \\quad \\text{for all } k \\ge 1.\n$$\nThe Krylov subspaces are:\n$$\n\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots\\} = \\operatorname{span}\\{e_1, 0, \\dots\\} = \\operatorname{span}\\{e_1\\} = S\n$$\nfor all $m \\ge 1$. The Krylov subspaces are identical to the invariant subspace $S$.\n\nThe image of this subspace under $A$ is $A(S) = \\{Av \\mid v \\in S\\}$. Since $S$ is the null space of $A$, $A(S) = \\{0\\}$. The initial residual is $r_0 = e_1$. The subspace $A(S)=\\{0\\}$ is trivially orthogonal to $r_0$. We are now precisely in the situation described in the general explanation.\n\nAt any step $m \\ge 1$, GMRES seeks to find $y_m \\in \\mathcal{K}_m(A, r_0) = S$ to minimize $\\|r_0 - Ay_m\\|_2$.\nSince $y_m \\in S$, we have $Ay_m = 0$. The minimization problem becomes:\n$$\n\\min_{y_m \\in S} \\|r_0 - 0\\|_2 = \\min_{y_m \\in S} \\|r_0\\|_2.\n$$\nThe objective function is constant. The minimum value is $\\|r_0\\|_2 = 1$. The optimal residual is $r_m = r_0 - Ay_m = r_0 - 0 = r_0$. This holds for any $m \\ge 1$.\n\nSo, we have $r_1 = r_0$, which means $\\|r_1\\|_2 = \\|r_0\\|_2 = 1$. This demonstrates stagnation starting at the first step. The residual remains nonzero, as $r_m = r_0 = (1, 0, 0)^T \\neq 0$.\nThe problem asks for the exact value of $\\|r_2\\|_2$. Following the same logic for $m=2$:\nThe iterate $x_2$ is sought in $x_0 + \\mathcal{K}_2(A,r_0)$. Let $x_2 = x_0 + y_2$ with $y_2 \\in \\mathcal{K}_2(A,r_0)=S$.\nThe residual is $r_2 = r_0 - Ay_2$. Since $y_2 \\in S$, $Ay_2=0$.\nThus, $r_2 = r_0$.\nThe norm is $\\|r_2\\|_2 = \\|r_0\\|_2 = 1$.\n\nIn fact, the process stagnates completely, with $r_m = r_0$ and $\\|r_m\\|_2 = 1$ for all $m \\ge 1$. When GMRES is implemented with the Arnoldi process, this corresponds to a breakdown where the Hessenberg matrix entry $h_{m+1,m}$ becomes zero. Here, $h_{21}$ would be zero, and GMRES would terminate at step $m=1$, returning the current best approximation.\n\nThe value of $\\|r_2\\|_2$ is therefore $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "In practical applications, GMRES is rarely used without a preconditioner, and knowing when to terminate the iteration is crucial for efficiency and accuracy. This exercise challenges you to critically evaluate several common stopping criteria in the context of both left and right preconditioning. By analyzing how these criteria relate to the true residual and the normwise backward error, you will gain essential insight into the reliable and robust implementation of preconditioned GMRES. ",
            "id": "3588195",
            "problem": "Consider solving a nonsingular linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$ by the Generalized Minimal Residual (GMRES) method, possibly with left or right preconditioning using a nonsingular preconditioner $M \\in \\mathbb{R}^{n \\times n}$. Denote the $m$-th GMRES iterate by $x_m$ and the corresponding true residual by $r_m = b - A x_m$. In left-preconditioned GMRES, one applies GMRES to $M^{-1} A x = M^{-1} b$, monitors the preconditioned residual $M^{-1} r_m$, and computes the projected residual norm from the small Hessenberg least-squares problem. In right-preconditioned GMRES, one applies GMRES to $A M^{-1} y = b$ with $x = M^{-1} y$, monitors the true residual $r_m$, and computes the projected residual norm accordingly. The $2$-norm $\\|\\cdot\\|_2$ is used throughout, and $\\kappa_2(M) = \\|M\\|_2 \\|M^{-1}\\|_2$ denotes the $2$-norm condition number of $M$. A tolerance $\\tau > 0$ is prescribed.\n\nWhich of the following statements propose stopping criteria that are theoretically justified and practically reliable, with explicit justification of their relationship to the true residual and normwise backward error, under the indicated preconditioning regime? Select all that apply.\n\nA. Stop when $\\|r_m\\|_2 / \\|b\\|_2 \\le \\tau$. Under left preconditioning, replacing $\\|r_m\\|_2$ and $\\|b\\|_2$ by $\\|M^{-1} r_m\\|_2$ and $\\|M^{-1} b\\|_2$ yields a criterion equivalent to the original up to the factor $\\kappa_2(M)$.\n\nB. Stop when the normwise backward error estimate $\\|r_m\\|_2 / (\\|A\\|_2 \\|x_m\\|_2 + \\|b\\|_2) \\le \\tau$. For left preconditioning, using $\\|M^{-1} r_m\\|_2 / (\\|M^{-1} A\\|_2 \\|x_m\\|_2 + \\|M^{-1} b\\|_2) \\le \\tau$ provides a bound on the original normwise backward error by at most a factor $\\kappa_2(M)$.\n\nC. In right-preconditioned GMRES, the projected residual norm obtained from the Hessenberg least-squares problem equals the true residual norm $\\|r_m\\|_2$ exactly; therefore, stopping when that quantity falls below $\\tau$ is a reliable criterion.\n\nD. Stop when $\\|M^{-1} r_m\\|_2 / \\|b\\|_2 \\le \\tau$ in left-preconditioned GMRES; this directly controls the true relative residual $\\|r_m\\|_2 / \\|b\\|_2$ uniformly for any nonsingular $M$, without dependence on $\\|M\\|_2$ or $\\|M^{-1}\\|_2$.\n\nE. In left-preconditioned GMRES, the projected residual norm computable from the Hessenberg least-squares problem equals $\\|M^{-1} r_m\\|_2$; since $\\|r_m\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_m\\|_2$, stopping when the projected residual relative to $\\|M^{-1} b\\|_2$ is below $\\tau$ ensures the true relative residual is below $\\kappa_2(M) \\tau$.\n\nF. The normwise backward error computed in the preconditioned system, $\\|M^{-1} r_m\\|_2 / (\\|M^{-1} A\\|_2 \\|x_m\\|_2 + \\|M^{-1} b\\|_2)$, equals the original normwise backward error $\\|r_m\\|_2 / (\\|A\\|_2 \\|x_m\\|_2 + \\|b\\|_2)$ exactly for any nonsingular $M$; hence it is invariant under left preconditioning.",
            "solution": "The user wants to evaluate the theoretical justification and practical reliability of several proposed stopping criteria for the Generalized Minimal Residual (GMRES) method, both with and without preconditioning.\n\n### Step 1: Extract Givens\n-   Linear system: $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is nonsingular and $b \\in \\mathbb{R}^{n}$.\n-   Method: Generalized Minimal Residual (GMRES).\n-   Preconditioner: A nonsingular matrix $M \\in \\mathbb{R}^{n \\times n}$.\n-   Notation:\n    -   $m$-th iterate: $x_m$.\n    -   True residual: $r_m = b - A x_m$.\n    -   Left preconditioning: Apply GMRES to $M^{-1} A x = M^{-1} b$. The monitored residual is $M^{-1} r_m$.\n    -   Right preconditioning: Apply GMRES to $A M^{-1} y = b$ with $x = M^{-1} y$. The monitored residual is $r_m$.\n    -   Norm: Euclidean $2$-norm, $\\|\\cdot\\|_2$.\n    -   Condition number: $\\kappa_2(M) = \\|M\\|_2 \\|M^{-1}\\|_2$.\n    -   Tolerance: $\\tau > 0$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is set in the context of numerical linear algebra, specifically iterative methods for linear systems. All concepts—GMRES, preconditioning, residuals, backward error, matrix norms, and condition numbers—are standard, well-defined, and fundamental to this field. The problem is scientifically sound.\n-   **Well-Posed**: The task is to evaluate the logical and mathematical validity of several statements. Each statement is a specific claim that can be proven or disproven using the definitions and properties of the entities involved. The problem is well-posed.\n-   **Objective**: The language is precise, technical, and free of ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It presents a standard scenario in numerical analysis and asks for an evaluation of common practices and their theoretical underpinnings. I will proceed with the option-by-option analysis.\n\n***\n\n### Analysis of the Options\n\n**Core Concepts:**\n1.  **GMRES Minimization Property**: In its $m$-th step, GMRES finds an approximate solution $x_m$ from an affine subspace $x_0 + \\mathcal{K}_m$ such that the $2$-norm of the residual is minimized.\n    -   **Standard GMRES**: Minimizes $\\|r_m\\|_2 = \\|b - A x_m\\|_2$. The computed \"projected residual\" from the internal Hessenberg least-squares problem is exactly $\\|r_m\\|_2$.\n    -   **Left-Preconditioned (LP) GMRES**: Applies GMRES to $M^{-1} A x = M^{-1} b$. It minimizes $\\|M^{-1} b - M^{-1} A x_m\\|_2 = \\|M^{-1} r_m\\|_2$. The projected residual is $\\|M^{-1} r_m\\|_2$.\n    -   **Right-Preconditioned (RP) GMRES**: Applies GMRES to $(A M^{-1}) y = b$. It finds $y_m$ to minimize $\\|b - (A M^{-1}) y_m\\|_2$. The solution iterate is $x_m = M^{-1} y_m$. The true residual is $r_m = b - A x_m = b - A (M^{-1} y_m)$, which is the quantity being minimized. The projected residual is $\\|r_m\\|_2$.\n2.  **Norm Inequalities**: For any vector $v$ and nonsingular matrix $N$, we have $\\|v\\|_2 \\le \\|N\\|_2 \\|N^{-1}v\\|_2$ and $\\|N^{-1}v\\|_2 \\le \\|N^{-1}\\|_2 \\|v\\|_2$.\n3.  **Normwise Backward Error**: The problem uses the common practical upper bound for the normwise backward error of an approximate solution $\\tilde{x}$: $\\eta(\\tilde{x}) = \\frac{\\|b - A \\tilde{x}\\|_2}{\\|A\\|_2 \\|\\tilde{x}\\|_2 + \\|b\\|_2}$.\n\n---\n**Option A: Stop when $\\|r_m\\|_2 / \\|b\\|_2 \\le \\tau$. Under left preconditioning, replacing $\\|r_m\\|_2$ and $\\|b\\|_2$ by $\\|M^{-1} r_m\\|_2$ and $\\|M^{-1} b\\|_2$ yields a criterion equivalent to the original up to the factor $\\kappa_2(M)$.**\n\nThe original criterion is based on the true relative residual, $\\frac{\\|r_m\\|_2}{\\|b\\|_2}$. The proposed preconditioned criterion is based on the preconditioned relative residual, $\\frac{\\|M^{-1} r_m\\|_2}{\\|M^{-1} b\\|_2}$. We must analyze the ratio of these two quantities:\n$$ \\frac{\\|r_m\\|_2 / \\|b\\|_2}{\\|M^{-1} r_m\\|_2 / \\|M^{-1} b\\|_2} = \\frac{\\|r_m\\|_2 \\|M^{-1} b\\|_2}{\\|b\\|_2 \\|M^{-1} r_m\\|_2} $$\nWe use the norm inequalities: $\\|r_m\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_m\\|_2$ and $\\|M^{-1} b\\|_2 \\le \\|M^{-1}\\|_2 \\|b\\|_2$.\nFor the upper bound on the ratio:\n$$ \\frac{\\|r_m\\|_2 \\|M^{-1} b\\|_2}{\\|b\\|_2 \\|M^{-1} r_m\\|_2} \\le \\frac{(\\|M\\|_2 \\|M^{-1} r_m\\|_2) (\\|M^{-1}\\|_2 \\|b\\|_2)}{\\|b\\|_2 \\|M^{-1} r_m\\|_2} = \\|M\\|_2 \\|M^{-1}\\|_2 = \\kappa_2(M) $$\nFor the lower bound, we use $\\|r_m\\|_2 \\ge \\frac{\\|M^{-1} r_m\\|_2}{\\|M^{-1}\\|_2}$ and $\\|M^{-1} b\\|_2 \\ge \\frac{\\|b\\|_2}{\\|M\\|_2}$:\n$$ \\frac{\\|r_m\\|_2 \\|M^{-1} b\\|_2}{\\|b\\|_2 \\|M^{-1} r_m\\|_2} \\ge \\frac{(\\|M^{-1} r_m\\|_2 / \\|M^{-1}\\|_2) (\\|b\\|_2 / \\|M\\|_2)}{\\|b\\|_2 \\|M^{-1} r_m\\|_2} = \\frac{1}{\\|M\\|_2 \\|M^{-1}\\|_2} = \\frac{1}{\\kappa_2(M)} $$\nThus, we have $\\frac{1}{\\kappa_2(M)} \\le \\frac{\\|r_m\\|_2 / \\|b\\|_2}{\\|M^{-1} r_m\\|_2 / \\|M^{-1} b\\|_2} \\le \\kappa_2(M)$. This means the two criteria are equivalent up to a factor of $\\kappa_2(M)$, so satisfying one with tolerance $\\tau$ implies satisfying the other with tolerance $\\tau \\cdot \\kappa_2(M)$. The statement is justified.\n\nVerdict on A: **Correct**.\n\n---\n**Option B: Stop when the normwise backward error estimate $\\|r_m\\|_2 / (\\|A\\|_2 \\|x_m\\|_2 + \\|b\\|_2) \\le \\tau$. For left preconditioning, using $\\|M^{-1} r_m\\|_2 / (\\|M^{-1} A\\|_2 \\|x_m\\|_2 + \\|M^{-1} b\\|_2) \\le \\tau$ provides a bound on the original normwise backward error by at most a factor $\\kappa_2(M)$.**\n\nLet the original backward error be $\\eta_m = \\frac{\\|r_m\\|_2}{\\|A\\|_2 \\|x_m\\|_2 + \\|b\\|_2}$ and the preconditioned backward error be $\\tilde{\\eta}_m = \\frac{\\|M^{-1} r_m\\|_2}{\\|M^{-1} A\\|_2 \\|x_m\\|_2 + \\|M^{-1} b\\|_2}$. We want to find the relationship between $\\eta_m$ and $\\tilde{\\eta}_m$.\n$$ \\eta_m = \\frac{\\|M(M^{-1} r_m)\\|_2}{\\|M(M^{-1} A)\\|_2 \\|x_m\\|_2 + \\|M(M^{-1} b)\\|_2} $$\nUsing norm inequalities, we bound the numerator from above and the denominator from below:\n- Numerator: $\\|M(M^{-1} r_m)\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_m\\|_2$.\n- Denominator: $\\|M(M^{-1}A)\\|_2 \\ge \\frac{\\|M^{-1}A\\|_2}{\\|M^{-1}\\|_2}$ and $\\|M(M^{-1}b)\\|_2 \\ge \\frac{\\|M^{-1}b\\|_2}{\\|M^{-1}\\|_2}$.\nSo, the denominator is bounded below by $\\frac{\\|M^{-1}A\\|_2}{\\|M^{-1}\\|_2} \\|x_m\\|_2 + \\frac{\\|M^{-1}b\\|_2}{\\|M^{-1}\\|_2} = \\frac{1}{\\|M^{-1}\\|_2} (\\|M^{-1}A\\|_2 \\|x_m\\|_2 + \\|M^{-1}b\\|_2)$.\nCombining these bounds:\n$$ \\eta_m \\le \\frac{\\|M\\|_2 \\|M^{-1} r_m\\|_2}{\\frac{1}{\\|M^{-1}\\|_2} (\\|M^{-1}A\\|_2 \\|x_m\\|_2 + \\|M^{-1}b\\|_2)} = \\|M\\|_2 \\|M^{-1}\\|_2 \\left( \\frac{\\|M^{-1} r_m\\|_2}{\\|M^{-1}A\\|_2 \\|x_m\\|_2 + \\|M^{-1}b\\|_2} \\right) = \\kappa_2(M) \\tilde{\\eta}_m $$\nThus, if the preconditioned criterion $\\tilde{\\eta}_m \\le \\tau$ is met, it guarantees that the true backward error satisfies $\\eta_m \\le \\kappa_2(M) \\tau$. The statement is justified.\n\nVerdict on B: **Correct**.\n\n---\n**Option C: In right-preconditioned GMRES, the projected residual norm obtained from the Hessenberg least-squares problem equals the true residual norm $\\|r_m\\|_2$ exactly; therefore, stopping when that quantity falls below $\\tau$ is a reliable criterion.**\n\nIn RP-GMRES, the algorithm is applied to the system $(A M^{-1}) y = b$. The GMRES procedure generates an iterate $y_m$ that minimizes the norm of the residual for this system, which is $\\hat{r}_m = b - (A M^{-1}) y_m$. The quantity computed from the internal Hessenberg problem is exactly $\\|\\hat{r}_m\\|_2$.\nThe iterate for the original problem is $x_m = M^{-1} y_m$. The true residual is:\n$$ r_m = b - A x_m = b - A(M^{-1} y_m) = b - (A M^{-1}) y_m = \\hat{r}_m $$\nTherefore, the norms are identical: $\\|r_m\\|_2 = \\|\\hat{r}_m\\|_2$. The statement that the projected residual norm equals the true residual norm is correct. Consequently, a stopping criterion based on this computable quantity, such as $\\|r_m\\|_2 < \\tau$, directly and reliably controls the true residual norm without any unknown scaling factors.\n\nVerdict on C: **Correct**.\n\n---\n**Option D: Stop when $\\|M^{-1} r_m\\|_2 / \\|b\\|_2 \\le \\tau$ in left-preconditioned GMRES; this directly controls the true relative residual $\\|r_m\\|_2 / \\|b\\|_2$ uniformly for any nonsingular $M$, without dependence on $\\|M\\|_2$ or $\\|M^{-1}\\|_2$.**\n\nThe proposed criterion is $\\|M^{-1} r_m\\|_2 \\le \\tau \\|b\\|_2$. We want to see how this relates to the true relative residual $\\|r_m\\|_2 / \\|b\\|_2$.\nFrom the triangle inequality, $\\|r_m\\|_2 = \\|M(M^{-1} r_m)\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_m\\|_2$.\nSubstituting the stopping criterion:\n$$ \\|r_m\\|_2 \\le \\|M\\|_2 (\\tau \\|b\\|_2) $$\nDividing by $\\|b\\|_2$ (assuming $b \\neq 0$):\n$$ \\frac{\\|r_m\\|_2}{\\|b\\|_2} \\le \\|M\\|_2 \\tau $$\nThe bound on the true relative residual explicitly depends on $\\|M\\|_2$. If $\\|M\\|_2$ is large, the guarantee on the true relative residual is very poor. The statement's claim that the control is \"without dependence on $\\|M\\|_2$ or $\\|M^{-1}\\|_2$\" is false.\n\nVerdict on D: **Incorrect**.\n\n---\n**Option E: In left-preconditioned GMRES, the projected residual norm computable from the Hessenberg least-squares problem equals $\\|M^{-1} r_m\\|_2$; since $\\|r_m\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_m\\|_2$, stopping when the projected residual relative to $\\|M^{-1} b\\|_2$ is below $\\tau$ ensures the true relative residual is below $\\kappa_2(M) \\tau$.**\n\nThis statement presents a chain of reasoning that we must verify step by step.\n1.  \"the projected residual norm ... equals $\\|M^{-1} r_m\\|_2$\": As established in the core concepts, in LP-GMRES, the minimized quantity is indeed the norm of the preconditioned residual, $\\|M^{-1} r_m\\|_2$. This part is correct.\n2.  The stopping criterion is that \"the projected residual relative to $\\|M^{-1} b\\|_2$ is below $\\tau$\". This means $\\frac{\\|M^{-1} r_m\\|_2}{\\|M^{-1} b\\|_2} \\le \\tau$.\n3.  The claim is that this criterion ensures the true relative residual, $\\frac{\\|r_m\\|_2}{\\|b\\|_2}$, is below $\\kappa_2(M) \\tau$. Let's derive this.\nFrom the criterion, we have $\\|M^{-1} r_m\\|_2 \\le \\tau \\|M^{-1} b\\|_2$.\nWe use two norm inequalities:\n(i) $\\|r_m\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_ m\\|_2$\n(ii) $\\|M^{-1} b\\|_2 \\le \\|M^{-1}\\|_2 \\|b\\|_2$\nCombining these with the criterion:\n$$ \\|r_m\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_ m\\|_2 \\le \\|M\\|_2 (\\tau \\|M^{-1} b\\|_2) \\le \\|M\\|_2 \\tau (\\|M^{-1}\\|_2 \\|b\\|_2) $$\n$$ \\|r_m\\|_2 \\le (\\|M\\|_2 \\|M^{-1}\\|_2) \\tau \\|b\\|_2 = \\kappa_2(M) \\tau \\|b\\|_2 $$\nDividing by $\\|b\\|_2$ gives the desired result: $\\frac{\\|r_m\\|_2}{\\|b\\|_2} \\le \\kappa_2(M) \\tau$.\nEvery part of the statement is mathematically correct.\n\nVerdict on E: **Correct**.\n\n---\n**Option F: The normwise backward error computed in the preconditioned system, $\\|M^{-1} r_m\\|_2 / (\\|M^{-1} A\\|_2 \\|x_m\\|_2 + \\|M^{-1} b\\|_2)$, equals the original normwise backward error $\\|r_m\\|_2 / (\\|A\\|_2 \\|x_m\\|_2 + \\|b\\|_2)$ exactly for any nonsingular $M$; hence it is invariant under left preconditioning.**\n\nThis statement claims that $\\eta_m = \\tilde{\\eta}_m$, where these are the backward error estimates defined in the analysis of option B. As shown in that analysis, the two quantities are related by $\\eta_m \\le \\kappa_2(M) \\tilde{\\eta}_m$. Equality holds only in special cases, for instance if $M = cI$ for some scalar $c \\neq 0$, where $\\kappa_2(M) = 1$. In general, the norms do not scale linearly, i.e., $\\|Mv\\|_2$ is not necessarily $\\|M\\|_2 \\|v\\|_2$. For a general nonsingular $M$, we do not have $\\|M(M^{-1}r_m)\\|_2 = \\|M\\|_2 \\|M^{-1}r_m\\|_2$, nor do we have $\\|M(M^{-1}A)\\|_2 = \\|M\\|_2 \\|M^{-1}A\\|_2$. The claim of exact equality is incorrect. We can easily construct a counterexample. Let $A=I \\in \\mathbb{R}^{2 \\times 2}$, $b = [1, 0]^T$, $x_m = [0, 0]^T$. Then $r_m = b$. Let $M = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nOriginal backward error: $\\eta_m = \\frac{\\|b\\|_2}{\\|A\\|_2 \\|x_m\\|_2+\\|b\\|_2} = \\frac{1}{1 \\cdot 0 + 1} = 1$.\nPreconditioned backward error: $M^{-1} = \\begin{pmatrix} 1/2 & 0 \\\\ 0 & 1 \\end{pmatrix}$, so $M^{-1}r_m = [1/2, 0]^T$, $M^{-1}A = M^{-1}$, $M^{-1}b = [1/2, 0]^T$.\n$\\tilde{\\eta}_m = \\frac{\\|M^{-1}r_m\\|_2}{\\|M^{-1}A\\|_2 \\|x_m\\|_2+\\|M^{-1}b\\|_2} = \\frac{1/2}{1 \\cdot 0 + 1/2} = 1$. In this case, they are equal.\nLet's change $x_m$. Let $x_m=[0,1]^T$. Then $r_m = b-x_m = [1,-1]^T$.\n$\\|r_m\\|_2 = \\sqrt{2}$. $\\|x_m\\|_2 = 1$.\n$\\eta_m = \\frac{\\sqrt{2}}{1 \\cdot 1 + 1} = \\frac{\\sqrt{2}}{2}$.\n$M^{-1}r_m = [1/2, -1]^T$. $\\|M^{-1}r_m\\|_2 = \\sqrt{1/4+1} = \\sqrt{5}/2$.\n$\\tilde{\\eta}_m = \\frac{\\|M^{-1}r_m\\|_2}{\\|M^{-1}A\\|_2 \\|x_m\\|_2+\\|M^{-1}b\\|_2} = \\frac{\\sqrt{5}/2}{1 \\cdot 1 + 1/2} = \\frac{\\sqrt{5}/2}{3/2} = \\frac{\\sqrt{5}}{3}$.\nSince $\\frac{\\sqrt{2}}{2} \\neq \\frac{\\sqrt{5}}{3}$, the backward error is not invariant.\n\nVerdict on F: **Incorrect**.",
            "answer": "$$\\boxed{ABCE}$$"
        }
    ]
}