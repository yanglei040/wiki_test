{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly understand a complex numerical algorithm is to trace its execution on a simple, well-defined example. This first practice demystifies the core mechanics of the Generalized Minimal Residual (GMRES) method by guiding you through two complete steps by hand. By explicitly constructing the orthonormal basis with the Arnoldi process and solving the resulting least-squares problem using Givens rotations, you will gain a concrete, step-by-step understanding of how GMRES builds its approximation and minimizes the residual in the Krylov subspace .",
            "id": "3616841",
            "problem": "Consider a small nonsymmetric linear system that arises from a coarse finite-volume discretization of a one-dimensional steady advectionâ€“diffusion operator used in linearized seismic travel-time tomography. Let the system matrix be\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 3 & 0 \\\\\n0 & 1 & 2\n\\end{pmatrix},\n$$\nthe right-hand side be\n$$\nb = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n$$\nand the initial guess be\n$$\nx_0 = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nPerform two steps of the Arnoldi process starting from the initial residual $r_0 = b - A x_0$, forming the orthonormal basis $V_{m+1}$ with $m = 2$ and the $(m+1) \\times m$ upper Hessenberg matrix $\\bar{H}_m$. Then, apply two successive Givens rotations to $\\bar{H}_m$ to eliminate its subdiagonal entries and obtain an upper triangular $R_m$, while simultaneously updating the transformed right-hand side. Use this to compute the Generalized Minimal Residual (GMRES) iterate $x_m = x_0 + V_m y$ and the residual $r_m = b - A x_m$ explicitly. Report the three components of $x_m$ followed by the three components of $r_m$ as a single row matrix. No rounding is required, and no physical units are needed. Define Generalized Minimal Residual (GMRES) on its first use and proceed from first principles, starting from the Krylov subspace construction and orthonormalization.",
            "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and self-contained.\n\n### Step 1: Extract Givens\n- System matrix: $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$\n- Right-hand side vector: $b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$\n- Initial guess: $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$\n- Number of GMRES steps (Krylov subspace dimension): $m = 2$\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem addresses the Generalized Minimal Residual (GMRES) method, a standard and fundamental algorithm in numerical linear algebra for solving nonsymmetric linear systems. The context of seismic tomography is a valid application area for such methods. The problem is mathematically and scientifically sound.\n- **Well-Posed**: The matrix $A$ is a $3 \\times 3$ lower triangular matrix. Its determinant is the product of its diagonal entries, $\\det(A) = 1 \\cdot 3 \\cdot 2 = 6 \\neq 0$. Since the matrix is non-singular, a unique solution to the system $Ax = b$ exists. The problem asks for the execution of a specific number of steps of a well-defined algorithm (GMRES), which leads to a unique iterate $x_2$.\n- **Objective**: The problem is stated using precise mathematical definitions and contains no subjective or ambiguous language.\n- **Completeness**: All necessary data ($A$, $b$, $x_0$, and $m$) are provided to perform the requested calculations.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed solution will be provided.\n\n---\n\nThe Generalized Minimal Residual (GMRES) method is an iterative algorithm for solving the linear system of equations $Ax = b$, particularly when the matrix $A$ is large, sparse, and nonsymmetric. Starting with an initial guess $x_0$, GMRES finds an approximate solution $x_m$ from the affine space $x_0 + K_m(A, r_0)$, where $K_m(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{m-1}r_0\\}$ is the $m$-th Krylov subspace generated by $A$ and the initial residual $r_0 = b - Ax_0$. The iterate $x_m$ is chosen to minimize the Euclidean norm of the residual, i.e., $x_m = \\arg \\min_{z \\in x_0 + K_m} \\|b - Az\\|_2$.\n\nThe solution process involves three main stages:\n1.  Use the Arnoldi process to generate an orthonormal basis $V_{m+1}$ for the Krylov subspace $K_{m+1}(A, r_0)$ and an upper Hessenberg matrix $\\bar{H}_m$.\n2.  Solve a small least-squares problem to find the vector $y_m$ that minimizes the residual in the Krylov subspace.\n3.  Compute the approximate solution $x_m = x_0 + V_m y_m$ and the corresponding residual $r_m = b - A x_m$.\n\nWe will now execute these steps for the given problem with $m=2$.\n\n**1. Initial Setup and Arnoldi Process**\n\nFirst, compute the initial residual $r_0$:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe norm is $\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1$.\nThe first basis vector of the Krylov subspace is:\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, we perform $m=2$ steps of the Arnoldi process.\n\n**Step k=1:**\nCompute $w_1 = A v_1$:\n$$\nw_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nOrthogonalize $w_1$ against the previous basis vectors (only $v_1$):\n$$\nh_{1,1} = v_1^T w_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 1\n$$\n$$\n\\tilde{w}_1 = w_1 - h_{1,1} v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nCompute the norm and the next basis vector:\n$$\nh_{2,1} = \\|\\tilde{w}_1\\|_2 = \\sqrt{0^2 + 1^2 + 0^2} = 1\n$$\n$$\nv_2 = \\frac{\\tilde{w}_1}{h_{2,1}} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n**Step k=2:**\nCompute $w_2 = A v_2$:\n$$\nw_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\nOrthogonalize $w_2$ against $v_1$ and $v_2$:\n$$\nh_{1,2} = v_1^T w_2 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = 0\n$$\n$$\nh_{2,2} = v_2^T w_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = 3\n$$\n$$\n\\tilde{w}_2 = w_2 - h_{1,2} v_1 - h_{2,2} v_2 = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 3 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nCompute the norm and the next basis vector:\n$$\nh_{3,2} = \\|\\tilde{w}_2\\|_2 = \\sqrt{0^2 + 0^2 + 1^2} = 1\n$$\n$$\nv_3 = \\frac{\\tilde{w}_2}{h_{3,2}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe Arnoldi process yields the orthonormal basis matrix $V_3 = [v_1, v_2, v_3] = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I$ and the $(m+1) \\times m = 3 \\times 2$ upper Hessenberg matrix $\\bar{H}_2$:\n$$\n\\bar{H}_2 = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix}\n$$\n\n**2. Least-Squares Problem and Givens Rotations**\n\nThe GMRES iterate is $x_m = x_0 + V_m y_m$, where $y_m$ solves the least-squares problem:\n$$\ny_m = \\arg \\min_{y \\in \\mathbb{R}^m} \\| \\|r_0\\|_2 e_1 - \\bar{H}_m y \\|_2\n$$\nFor $m=2$, with $\\|r_0\\|_2=1$, this becomes:\n$$\ny_2 = \\arg \\min_{y \\in \\mathbb{R}^2} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix} y \\right\\|_2\n$$\nWe solve this by applying a sequence of Givens rotations to transform $\\bar{H}_2$ into an upper triangular matrix $R_2$.\n\n**First Givens Rotation $\\Omega_1$:**\nTo eliminate the element $h_{2,1}=1$, we apply a rotation on rows $1$ and $2$.\nLet $a = h_{1,1} = 1$ and $b = h_{2,1} = 1$. The rotation coefficients are $c = \\frac{a}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{2}}$ and $s = \\frac{b}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{2}}$.\nThe rotation matrix is $\\Omega_1 = \\begin{pmatrix} c & s & 0 \\\\ -s & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nApplying $\\Omega_1$ to $\\bar{H}_2$:\n$$\n\\Omega_1 \\bar{H}_2 = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & 3/\\sqrt{2} \\\\ 0 & 1 \\end{pmatrix}\n$$\nApplying $\\Omega_1$ to the right-hand side vector $g = \\|r_0\\|_2 e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$:\n$$\n\\Omega_1 g = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix}\n$$\n\n**Second Givens Rotation $\\Omega_2$:**\nTo eliminate the element at position $(3,2)$, which is $1$, we apply a rotation on rows $2$ and $3$ of the transformed system.\nLet $a = 3/\\sqrt{2}$ and $b = 1$. The rotation coefficients are $c = \\frac{a}{\\sqrt{a^2+b^2}} = \\frac{3/\\sqrt{2}}{\\sqrt{9/2+1}} = \\frac{3/\\sqrt{2}}{\\sqrt{11/2}} = \\frac{3}{\\sqrt{11}}$ and $s = \\frac{b}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{11/2}} = \\frac{\\sqrt{2}}{\\sqrt{11}}$.\nThe rotation matrix is $\\Omega_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & c & s \\\\ 0 & -s & c \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix}$.\nApplying $\\Omega_2$ to $\\Omega_1 \\bar{H}_2$:\n$$\n\\Omega_2 (\\Omega_1 \\bar{H}_2) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & 3/\\sqrt{2} \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\frac{9}{\\sqrt{22}} + \\frac{\\sqrt{2}}{\\sqrt{11}} \\\\ 0 & \\frac{-3\\sqrt{2}}{\\sqrt{22}} + \\frac{3}{\\sqrt{11}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis gives the upper triangular matrix $R_2 = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\end{pmatrix}$.\nApplying $\\Omega_2$ to the transformed right-hand side $\\Omega_1 g$:\n$$\ng' = \\Omega_2 (\\Omega_1 g) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -3/\\sqrt{22} \\\\ 1/\\sqrt{11} \\end{pmatrix}\n$$\nThe least-squares problem is now equivalent to solving $R_2 y_2 = g'_1$, where $g'_1$ consists of the first two components of $g'$.\n$$\n\\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -3/\\sqrt{22} \\end{pmatrix}\n$$\nSolving by back substitution:\nFrom the second row:\n$\\sqrt{11/2} \\cdot y_2 = -3/\\sqrt{22} \\implies y_2 = \\frac{-3}{\\sqrt{22}} \\frac{\\sqrt{2}}{\\sqrt{11}} = \\frac{-3}{\\sqrt{11}\\sqrt{2}} \\frac{\\sqrt{2}}{\\sqrt{11}} = -\\frac{3}{11}$.\nFrom the first row:\n$\\sqrt{2} \\cdot y_1 + (3/\\sqrt{2}) y_2 = 1/\\sqrt{2} \\implies \\sqrt{2} \\cdot y_1 + (3/\\sqrt{2})(-3/11) = 1/\\sqrt{2}$.\n$\\sqrt{2} \\cdot y_1 = 1/\\sqrt{2} + 9/(11\\sqrt{2}) = \\frac{11+9}{11\\sqrt{2}} = \\frac{20}{11\\sqrt{2}}$.\n$y_1 = \\frac{20}{11\\sqrt{2}} \\frac{1}{\\sqrt{2}} = \\frac{20}{22} = \\frac{10}{11}$.\nSo, $y_2 = \\begin{pmatrix} 10/11 \\\\ -3/11 \\end{pmatrix}$.\n\n**3. Compute Solution and Residual**\n\nThe GMRES iterate $x_2$ is:\n$$\nx_2 = x_0 + V_2 y_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 10/11 \\\\ -3/11 \\end{pmatrix} = \\begin{pmatrix} 10/11 \\\\ -3/11 \\\\ 0 \\end{pmatrix}\n$$\nThe final residual $r_2$ is:\n$$\nr_2 = b - A x_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 10/11 \\\\ -3/11 \\\\ 0 \\end{pmatrix}\n$$\n$$\nA x_2 = \\begin{pmatrix} 1 \\cdot (10/11) \\\\ 1 \\cdot (10/11) + 3 \\cdot (-3/11) \\\\ 1 \\cdot (-3/11) \\end{pmatrix} = \\begin{pmatrix} 10/11 \\\\ 1/11 \\\\ -3/11 \\end{pmatrix}\n$$\n$$\nr_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 10/11 \\\\ 1/11 \\\\ -3/11 \\end{pmatrix} = \\begin{pmatrix} 1/11 \\\\ -1/11 \\\\ 3/11 \\end{pmatrix}\n$$\nThe final result consists of the three components of $x_2$ followed by the three components of $r_2$.\n$x_2 = (10/11, -3/11, 0)$\n$r_2 = (1/11, -1/11, 3/11)$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{10}{11} & -\\frac{3}{11} & 0 & \\frac{1}{11} & -\\frac{1}{11} & \\frac{3}{11} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While GMRES guarantees convergence in at most $n$ iterations in exact arithmetic, its practical effectiveness hinges on reducing the residual norm significantly in just a few steps. This thought experiment explores a critical failure mode: stagnation, where the residual norm ceases to decrease . By constructing a specific example where the Krylov subspace is trapped within an invariant subspace of the matrix, you will uncover the geometric conditions that prevent the algorithm from making progress, providing a deeper conceptual understanding of its convergence behavior.",
            "id": "3588155",
            "problem": "Consider the Generalized Minimal Residual method (GMRES), which for a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and a linear system $A x = b$, constructs iterates $x_m \\in x_0 + \\mathcal{K}_m(A,r_0)$ that minimize the $2$-norm of the residual $r_m = b - A x_m$, where the Krylov subspace is defined by $\\mathcal{K}_m(A,r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{m-1} r_0\\}$ and $r_0 = b - A x_0$. Explain from first principles why GMRES stagnation can occur, in the sense that $\\|r_m\\|_2 = \\|r_{m-1}\\|_2$ for some $m$ even though $r_m \\neq 0$, when $\\mathcal{K}_m(A,r_0)$ aligns with an invariant subspace of $A$ that does not reduce the residual under the GMRES update. Your explanation must start from the core definition that GMRES minimizes $\\|r_m\\|_2$ over $x_0 + \\mathcal{K}_m(A,r_0)$ and must justify the mechanism by which the additional Krylov direction fails to provide any reduction in residual norm.\n\nThen, for an explicit construction illustrating this mechanism, take\n$$\nA = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 0 \\\\ 0\n\\end{pmatrix}, \\quad\nx_0 = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nVerify that the Krylov subspaces $\\mathcal{K}_m(A,r_0)$ align with an $A$-invariant subspace that does not reduce the residual norm, and prove that GMRES stagnates starting at the first step while the residual remains nonzero. Compute the exact value of $\\|r_2\\|_2$. Your final answer must be a single real number with no units.",
            "solution": "The analysis of this problem proceeds in two parts. First, we will explain from first principles the mechanism of GMRES stagnation when the Krylov subspace is confined to a particular type of invariant subspace of the matrix $A$. Second, we will apply this understanding to the specific numerical example provided, demonstrating the stagnation and calculating the required residual norm.\n\nThe Generalized Minimal Residual method (GMRES) is an iterative method for solving a linear system of equations $Ax = b$, where $A \\in \\mathbb{R}^{n \\times n}$ and $b, x \\in \\mathbb{R}^n$. Starting with an initial guess $x_0$, GMRES generates a sequence of approximations $x_m$ for $m=1, 2, \\dots$. At each step $m$, the iterate $x_m$ is chosen from the affine Krylov subspace $x_0 + \\mathcal{K}_m(A, r_0)$, where $r_0 = b - A x_0$ is the initial residual and $\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{m-1} r_0\\}$ is the $m$-th Krylov subspace. The defining property of GMRES is that the iterate $x_m$ is chosen to minimize the Euclidean norm (2-norm) of the corresponding residual, $r_m = b - A x_m$.\n\nLet's formalize the minimization problem. An iterate $x_m$ can be written as $x_m = x_0 + y_m$ for some vector $y_m \\in \\mathcalK_m(A, r_0)$. The corresponding residual is\n$$\nr_m = b - A x_m = b - A(x_0 + y_m) = (b - A x_0) - A y_m = r_0 - A y_m.\n$$\nThus, the GMRES problem at step $m$ is to find a vector $y_m \\in \\mathcal{K}_m(A, r_0)$ that solves the minimization problem:\n$$\n\\min_{y \\in \\mathcal{K}_m(A, r_0)} \\|r_0 - A y\\|_2.\n$$\nThis is a standard linear least-squares problem. Geometrically, the vector $A y_m$ is the orthogonal projection of the initial residual $r_0$ onto the subspace $U_m = A\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{A r_0, A^2 r_0, \\dots, A^m r_0\\}$. The resulting residual $r_m$ is given by $r_m = r_0 - P_{U_m}(r_0)$, where $P_{U_m}$ denotes the orthogonal projector onto $U_m$. Consequently, $r_m$ is orthogonal to the subspace $U_m$. The norm of the residual is non-increasing, i.e., $\\|r_m\\|_2 \\le \\|r_{m-1}\\|_2$, because $\\mathcal{K}_{m-1}(A, r_0) \\subset \\mathcal{K}_m(A, r_0)$ which implies $U_{m-1} \\subset U_m$.\n\nStagnation, in the sense that $\\|r_m\\|_2 = \\|r_{m-1}\\|_2$ for some $m$ while $r_m \\neq 0$, occurs when the inclusion of the new search direction in $\\mathcal{K}_m$ provides no further reduction in the residual norm. This happens if the optimal solution $y_m$ to the minimization problem over $\\mathcal{K}_m(A, r_0)$ already lies in the smaller subspace $\\mathcal{K}_{m-1}(A, r_0)$. This is equivalent to stating that the projection of $r_0$ onto $U_m$ is the same as its projection onto $U_{m-1}$. This occurs if the new basis vector spanning the orthogonal complement of $U_{m-1}$ in $U_m$ is itself orthogonal to $r_0$.\n\nThe problem states that stagnation can occur when the Krylov subspace \"aligns with an invariant subspace of $A$\". Let $S$ be an $A$-invariant subspace, meaning $A(S) \\subseteq S$. A particularly illustrative case for stagnation arises when the initial residual $r_0$ lies within an invariant subspace $S$ which has the additional property that $r_0$ is orthogonal to the image of $S$ under $A$, i.e., $r_0 \\perp A(S)$.\n\nIf $r_0 \\in S$, then since $S$ is $A$-invariant, all subsequent Krylov vectors $A^k r_0$ for $k \\ge 1$ also lie in $S$. Therefore, the entire Krylov subspace $\\mathcal{K}_m(A, r_0)$ is contained within $S$ for all $m \\ge 1$.\nThe GMRES update vector $y_m$ is sought in $\\mathcal{K}_m(A, r_0)$, so $y_m \\in S$. The vector $Ay_m$ is then in $A(S)$.\nIf we have the condition $r_0 \\perp A(S)$, then for any $y_m \\in \\mathcal{K}_m(A,r_0) \\subseteq S$, we have $Ay_m \\in A(S)$, which implies $\\langle r_0, Ay_m \\rangle = 0$.\nThe residual norm squared at step $m$ is\n$$\n\\|r_m\\|_2^2 = \\|r_0 - Ay_m\\|_2^2 = \\langle r_0 - Ay_m, r_0 - Ay_m \\rangle = \\|r_0\\|_2^2 - 2\\langle r_0, Ay_m \\rangle + \\|Ay_m\\|_2^2.\n$$\nSince $\\langle r_0, Ay_m \\rangle = 0$, this simplifies to\n$$\n\\|r_m\\|_2^2 = \\|r_0\\|_2^2 + \\|Ay_m\\|_2^2.\n$$\nTo minimize this quantity, we must choose $y_m$ such that $\\|Ay_m\\|_2^2$ is minimized. The minimum possible value is $0$, which may be achievable if $0 \\in A(\\mathcal{K}_m(A,r_0))$. For instance, we may choose $y_m=0$, which is always in $\\mathcal{K}_m(A,r_0)$. This choice yields the minimum possible residual norm under these conditions, which is $\\|r_0\\|_2$.\nThe resulting residual is $r_m = r_0 - A(0) = r_0$. Since this holds for any $m \\ge 1$, we have $r_m = r_{m-1} = \\dots = r_0$. Consequently, $\\|r_m\\|_2 = \\|r_0\\|_2$ for all $m \\ge 1$. This constitutes complete stagnation from the very first step, with the residual norm remaining fixed at its initial value, provided $r_0 \\neq 0$. The mechanism is that the search directions $y_m$ are chosen from a subspace $S$ which is mapped by $A$ into a subspace $A(S)$ that is orthogonal to the initial residual $r_0$. Therefore, no linear combination of vectors of the form $Ay$ can have a component in the direction of $r_0$, making any reduction of the residual impossible.\n\nNow, we verify this mechanism with the provided example:\n$$\nA = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nFirst, we compute the initial residual $r_0$:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - A \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe 2-norm of the initial residual is $\\|r_0\\|_2 = 1$. Let $e_1 = (1, 0, 0)^T$. So, $r_0 = e_1$.\n\nThe matrix $A$ is diagonal. Its eigenvalues are $\\lambda_1=0$, $\\lambda_2=2$, and $\\lambda_3=3$, with corresponding eigenvectors $e_1$, $e_2$, and $e_3$. The subspace $S = \\operatorname{span}\\{e_1\\}$ is the eigenspace for $\\lambda_1=0$. An eigenspace is always an invariant subspace. In this case, $S$ is also the null space of $A$, so for any vector $v \\in S$, $Av = 0$.\n\nThe initial residual $r_0 = e_1$ is in the invariant subspace $S$. This means the Krylov sequence will be confined to $S$. Let's compute the Krylov vectors:\n$$\nA r_0 = A e_1 = \\lambda_1 e_1 = 0 \\cdot e_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n$$\nA^k r_0 = 0 \\quad \\text{for all } k \\ge 1.\n$$\nThe Krylov subspaces are:\n$$\n\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots\\} = \\operatorname{span}\\{e_1, 0, \\dots\\} = \\operatorname{span}\\{e_1\\} = S\n$$\nfor all $m \\ge 1$. The Krylov subspaces are identical to the invariant subspace $S$.\n\nThe image of this subspace under $A$ is $A(S) = \\{Av \\mid v \\in S\\}$. Since $S$ is the null space of $A$, $A(S) = \\{0\\}$. The initial residual is $r_0 = e_1$. The subspace $A(S)=\\{0\\}$ is trivially orthogonal to $r_0$. We are now precisely in the situation described in the general explanation.\n\nAt any step $m \\ge 1$, GMRES seeks to find $y_m \\in \\mathcal{K}_m(A, r_0) = S$ to minimize $\\|r_0 - Ay_m\\|_2$.\nSince $y_m \\in S$, we have $Ay_m = 0$. The minimization problem becomes:\n$$\n\\min_{y_m \\in S} \\|r_0 - 0\\|_2 = \\min_{y_m \\in S} \\|r_0\\|_2.\n$$\nThe objective function is constant. The minimum value is $\\|r_0\\|_2 = 1$. The optimal residual is $r_m = r_0 - Ay_m = r_0 - 0 = r_0$. This holds for any $m \\ge 1$.\n\nSo, we have $r_1 = r_0$, which means $\\|r_1\\|_2 = \\|r_0\\|_2 = 1$. This demonstrates stagnation starting at the first step. The residual remains nonzero, as $r_m = r_0 = (1, 0, 0)^T \\neq 0$.\nThe problem asks for the exact value of $\\|r_2\\|_2$. Following the same logic for $m=2$:\nThe iterate $x_2$ is sought in $x_0 + \\mathcal{K}_2(A,r_0)$. Let $x_2 = x_0 + y_2$ with $y_2 \\in \\mathcal{K}_2(A,r_0)=S$.\nThe residual is $r_2 = r_0 - Ay_2$. Since $y_2 \\in S$, $Ay_2=0$.\nThus, $r_2 = r_0$.\nThe norm is $\\|r_2\\|_2 = \\|r_0\\|_2 = 1$.\n\nIn fact, the process stagnates completely, with $r_m = r_0$ and $\\|r_m\\|_2 = 1$ for all $m \\ge 1$. When GMRES is implemented with the Arnoldi process, this corresponds to a breakdown where the Hessenberg matrix entry $h_{m+1,m}$ becomes zero. Here, $h_{21}$ would be zero, and GMRES would terminate at step $m=1$, returning the current best approximation.\n\nThe value of $\\|r_2\\|_2$ is therefore $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Moving from theory to computational practice, we must confront the realities of floating-point arithmetic. The theoretical orthogonality of the basis vectors generated by the Arnoldi process can quickly degrade in practice, a phenomenon known as loss of orthogonality, which can corrupt the solution. This coding exercise provides a hands-on approach to implementing the Arnoldi process and diagnosing its numerical stability by measuring the angles between successive Krylov vectors for a physically relevant Helmholtz operator . This practice is essential for developing the skills to not only implement numerical methods but also to validate their reliability.",
            "id": "3616879",
            "problem": "You are tasked with designing and implementing a computation that quantifies the angular relationship between successive Krylov vectors generated by the generalized minimal residual method (GMRES) via the Arnoldi process for a discrete one-dimensional Helmholtz operator. The goal is to measure how close successive Krylov directions are to being collinear prior to orthonormalization, and how close the computed Arnoldi basis vectors are to being orthogonal, and to interpret these measures in the context of numerical stability.\n\nStart from the following fundamental base:\n- A linear system is of the form $A x = b$, where $A$ is a square matrix, $x$ is the unknown vector, and $b$ is a given right-hand side vector.\n- The Krylov subspace of dimension $m$ generated by $A$ and an initial residual $r_0$ is $\\mathcal{K}_m(A,r_0) = \\operatorname{span}\\{ r_0, A r_0, A^2 r_0, \\ldots, A^{m-1} r_0\\}$.\n- The Arnoldi process constructs an orthonormal basis $\\{ v_1, v_2, \\ldots, v_m \\}$ of $\\mathcal{K}_m(A,r_0)$ and an upper Hessenberg matrix $H_m$ satisfying $A V_m = V_{m+1} H_m$, where $V_m = [v_1,\\ldots,v_m]$ and $V_{m+1} = [v_1,\\ldots,v_{m+1}]$.\n- The Euclidean inner product between two vectors $u$ and $v$ is $\\langle u, v \\rangle = u^\\top v$, the Euclidean norm is $\\| v \\|_2 = \\sqrt{v^\\top v}$, and the angle $\\theta$ between nonzero vectors $u$ and $v$ is defined by $\\cos(\\theta) = \\frac{|\\langle u, v \\rangle|}{\\|u\\|_2 \\|v\\|_2}$ with $\\theta \\in [0, \\pi/2]$.\n\nOperator and discretization:\n- Consider the one-dimensional frequency-domain acoustic Helmholtz operator with homogeneous Dirichlet boundary conditions on a unit interval of length $L = 1$ given by\n$$\nA = -\\frac{d^2}{dx^2} - k^2,\n$$\nwhere $k$ is the wavenumber. Use a second-order centered finite-difference discretization on $n$ interior grid points with spacing $h = \\frac{L}{n+1}$ to obtain the $n \\times n$ matrix\n$$\nA = \\frac{1}{h^2} \\operatorname{tridiag}(-1, 2, -1) - k^2 I,\n$$\nwhere $I$ is the identity matrix and $\\operatorname{tridiag}(-1,2,-1)$ denotes the tridiagonal matrix with $-1$ on the first sub- and super-diagonals and $2$ on the diagonal.\n\nInitialization and Arnoldi process:\n- Use the zero initial guess $x_0 = 0$ so that the initial residual is $r_0 = b$, and choose the right-hand side entries as $b_i = \\sin(\\pi x_i)$ with $x_i = i h$ for $i = 1, 2, \\ldots, n$.\n- Implement $m$ steps of the Arnoldi process using Classical Gram-Schmidt without reorthogonalization to produce the Arnoldi vectors $\\{v_1, \\ldots, v_m\\}$. Normalize $v_1 = r_0 / \\| r_0 \\|_2$.\n\nAngle computations and stability metrics:\n- At each Arnoldi iteration $i$ for $i = 1, 2, \\ldots, m-1$, compute the raw angle between the current Krylov vector $v_i$ and the next Krylov direction before orthonormalization, namely $A v_i$, using\n$$\n\\theta_i^{\\text{raw}} = \\arccos\\left( \\frac{|\\langle v_i, A v_i \\rangle|}{\\| v_i \\|_2 \\, \\| A v_i \\|_2} \\right).\n$$\n- Also compute the angle between successive Arnoldi basis vectors after orthonormalization,\n$$\n\\theta_i^{\\text{basis}} = \\arccos\\left( |\\langle v_i, v_{i+1} \\rangle| \\right),\n$$\nwhich ideally should equal $\\frac{\\pi}{2}$ in exact arithmetic. Quantify the deviation from orthogonality as\n$$\n\\delta_i = \\left| \\theta_i^{\\text{basis}} - \\frac{\\pi}{2} \\right|.\n$$\n- For each test case, report the pair of stability metrics: the minimum raw angle over iterations, $\\min_{1 \\le i \\le m-1} \\theta_i^{\\text{raw}}$, and the maximum deviation from orthogonality over iterations, $\\max_{1 \\le i \\le m-1} \\delta_i$. All angles must be reported in radians.\n\nTest suite and output format:\n- Use the following test cases, each given as a triple $(n, k, m)$:\n    1. $(n, k, m) = (64, 0.0, 30)$: the pure Laplacian case with $k = 0$ (symmetric positive definite operator).\n    2. $(n, k, m) = (64, 3.13, 30)$: wavenumber near the square root of the smallest eigenvalue of $-\\frac{d^2}{dx^2}$ for $n = 64$ (near-resonant regime).\n    3. $(n, k, m) = (128, 80.0, 50)$: high-wavenumber indefinite regime.\n- Your program should produce a single line of output for the three test cases. This output must be a list of lists with no spaces, in the exact format `[[a1,b1],[a2,b2],[a3,b3]]`, where each inner list `[a,b]` contains the floats for $[\\min_i \\theta_i^{\\text{raw}}, \\max_i \\delta_i]$ in radians.",
            "solution": "The problem is valid. It presents a well-posed, scientifically grounded task in numerical linear algebra, specifically concerning the stability of the Arnoldi process when applied to a discretized Helmholtz operator. All necessary data, definitions, and conditions are provided, and the problem is free of contradictions or ambiguities.\n\nThe solution proceeds by first constructing the discrete operator, then implementing the Arnoldi process using Classical Gram-Schmidt as specified, and finally computing the required stability metrics for each of the test cases.\n\n### Operator Discretization\nThe one-dimensional Helmholtz operator $A = -\\frac{d^2}{dx^2} - k^2$ on the unit interval $L=1$ with homogeneous Dirichlet boundary conditions is discretized using a second-order centered finite-difference scheme on a grid of $n$ interior points. The grid spacing is $h = \\frac{1}{n+1}$. This results in an $n \\times n$ matrix representation of the operator given by:\n$$\nA = \\frac{1}{h^2} \\operatorname{tridiag}(-1, 2, -1) - k^2 I\n$$\nwhere $I$ is the $n \\times n$ identity matrix and $\\operatorname{tridiag}(-1, 2, -1)$ is a symmetric tridiagonal matrix with $2$ on the main diagonal and $-1$ on the sub- and super-diagonals. The properties of this matrix $A$ depend critically on the wavenumber $k$:\n1.  For $k=0$, the operator $A$ is the discrete Laplacian, which is a symmetric positive definite (SPD) matrix.\n2.  For $k > 0$, the matrix $A$ becomes $A = L - k^2 I$, where $L$ is the discrete Laplacian. The matrix is symmetric but may be indefinite if $k^2$ is larger than the smallest eigenvalue of $L$. The case where $k^2$ is close to an eigenvalue of $L$ corresponds to a near-resonant physical regime, where the matrix $A$ is nearly singular, posing a significant challenge for iterative solvers.\n3.  For large $k$, the matrix $A$ is highly indefinite with eigenvalues distributed on both sides of zero, which is also a computationally demanding scenario.\n\n### The Arnoldi Process and Stability Metrics\nThe Arnoldi process generates an orthonormal basis $\\{v_1, v_2, \\ldots, v_m\\}$ for the Krylov subspace $\\mathcal{K}_m(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\ldots, A^{m-1}r_0\\}$. The initial vector is derived from the initial residual. With an initial guess $x_0 = 0$, the residual is $r_0 = b - Ax_0 = b$. The first basis vector is normalized as $v_1 = r_0 / \\|r_0\\|_2$.\n\nThe subsequent basis vectors $v_{i+1}$ for $i=1, \\ldots, m-1$ are generated by the Classical Gram-Schmidt (CGS) procedure, which involves taking the vector $Av_i$ and orthogonalizing it against all previously generated basis vectors $\\{v_1, \\ldots, v_i\\}$. The algorithm is as follows:\n1.  Start with $v_1 = r_0 / \\|r_0\\|_2$.\n2.  For $i = 1, 2, \\ldots, m-1$:\n    a. Compute the next direction: $w = Av_i$.\n    b. Orthogonalize $w$ against the current basis $\\{v_1, \\ldots, v_i\\}$:\n       For $j=1, \\ldots, i$:\n       $h_{ji} = \\langle v_j, w \\rangle$\n       $w \\leftarrow w - h_{ji} v_j$\n    c. Normalize the new vector: $h_{i+1,i} = \\|w\\|_2$.\n    d. The a new basis vector is $v_{i+1} = w / h_{i+1,i}$.\n\nThis process is implemented for $m-1$ steps to generate the basis $\\{v_1, \\ldots, v_m\\}$. At each step $i$, two stability metrics are computed:\n\n1.  **Raw Angle $\\theta_i^{\\text{raw}}$**: This metric is the angle between the current basis vector $v_i$ and its image under $A$, $Av_i$, calculated as:\n    $$\n    \\theta_i^{\\text{raw}} = \\arccos\\left( \\frac{|\\langle v_i, Av_i \\rangle|}{\\|v_i\\|_2 \\|Av_i\\|_2} \\right)\n    $$\n    Since $\\|v_i\\|_2=1$, this simplifies to $\\arccos(|\\langle v_i, Av_i \\rangle| / \\|Av_i\\|_2)$. A small value of $\\theta_i^{\\textraw}$ indicates that $Av_i$ is nearly aligned with $v_i$, which suggests that $v_i$ is an approximation to an eigenvector of $A$. This can also imply that the Krylov basis is becoming ill-conditioned, as the new information provided by $Av_i$ is not strongly independent of the existing subspace. We report $\\min_i \\theta_i^{\\text{raw}}$.\n\n2.  **Orthogonality Deviation $\\delta_i$**: This metric quantifies the loss of orthogonality between successively generated basis vectors, a known issue with the CGS algorithm due to floating-point arithmetic. It is defined based on the angle $\\theta_i^{\\text{basis}} = \\arccos(|\\langle v_i, v_{i+1} \\rangle|)$:\n    $$\n    \\delta_i = \\left| \\theta_i^{\\text{basis}} - \\frac{\\pi}{2} \\right|\n    $$\n    In exact arithmetic, $\\langle v_i, v_{i+1} \\rangle = 0$, so $\\theta_i^{\\text{basis}}$ would be $\\frac{\\pi}{2}$, and $\\delta_i$ would be $0$. A non-zero $\\delta_i$ indicates a numerical loss of orthogonality. A large value of $\\delta_i$ signals a severe stability problem in the basis construction. We report $\\max_i \\delta_i$.\n\n### Computational Implementation\nThe solution is implemented in Python using the `NumPy` library.\n- The matrix $A$ is constructed using `numpy.diag` for efficiency.\n- The right-hand side vector $b$ is formed using the specified sine function over the grid points.\n- A main loop iterates from $i=1$ to $m-1$. In each iteration, it performs one step of the CGS-based Arnoldi process to compute $v_{i+1}$ from $v_i$ and the existing basis $\\{v_1, \\ldots, v_i\\}$.\n- Within this loop, the values for $\\theta_i^{\\text{raw}}$ and $\\delta_i$ are computed and stored.\n- After the loop completes, the minimum of all computed $\\theta_i^{\\textraw}$ and the maximum of all $\\delta_i$ are determined.\n- This procedure is applied to each of the three test cases $(n, k, m)$ provided in the problem statement, and the final results are formatted into the required string output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_stability_analysis(n, k, m):\n    \"\"\"\n    Performs Arnoldi process using Classical Gram-Schmidt and computes stability metrics.\n\n    Args:\n        n (int): Number of interior grid points.\n        k (float): Wavenumber.\n        m (int): Number of Arnoldi iterations to produce m basis vectors.\n\n    Returns:\n        A list containing two floats: [min_raw_angle, max_ortho_deviation].\n    \"\"\"\n    # Part 1: Setup Operator and Initial Vectors\n    h = 1.0 / (n + 1.0)\n    \n    # Construct the discrete Helmholtz operator A\n    main_diag = np.full(n, 2.0 / h**2 - k**2)\n    off_diag = np.full(n - 1, -1.0 / h**2)\n    A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    \n    # Construct the right-hand side vector b\n    x_grid = np.arange(1, n + 1) * h\n    b = np.sin(np.pi * x_grid)\n    \n    # Initialize for Arnoldi: r0 = b, v1 = r0 / ||r0||\n    r0 = b\n    norm_r0 = np.linalg.norm(r0)\n    v1 = r0 / norm_r0\n    \n    # Store basis vectors in a list\n    v_list = [v1]\n    \n    raw_angles = []\n    ortho_deviations = []\n    \n    # Part 2: Arnoldi Iteration Loop\n    # The loop runs m-1 times to produce v_2, ..., v_m and the corresponding metrics.\n    # The problem indices are i = 1, ..., m-1.\n    # Loop index `j` from 0 to m-2 corresponds to problem index i = j+1.\n    for j in range(m - 1):\n        # Current vector is v_{j+1}, which is v_list[j]\n        v_current = v_list[j]\n        \n        # Compute the next Krylov direction before orthonormalization\n        Av_current = A @ v_current\n        \n        # 2a: Compute raw angle theta_{j+1}^raw\n        norm_Av = np.linalg.norm(Av_current)\n        if norm_Av  np.finfo(float).eps:\n            # This would be a breakdown, unlikely here.\n            break\n            \n        cos_theta_raw_arg = np.abs(np.dot(v_current, Av_current)) / norm_Av\n        # Clamp argument to arccos to handle potential floating point inaccuracies\n        cos_theta_raw_arg = min(1.0, cos_theta_raw_arg)\n        \n        theta_raw = np.arccos(cos_theta_raw_arg)\n        raw_angles.append(theta_raw)\n        \n        # 2b: Classical Gram-Schmidt to compute v_{j+2}\n        w_ortho = Av_current\n        # Orthogonalize against v_1, ..., v_{j+1}\n        for v_prev in v_list:\n            h_val = np.dot(v_prev, w_ortho)\n            w_ortho = w_ortho - h_val * v_prev\n            \n        norm_w = np.linalg.norm(w_ortho)\n        if norm_w  np.finfo(float).eps:\n            # Breakdown of Arnoldi process\n            break\n            \n        v_next = w_ortho / norm_w\n        \n        # 2c: Compute orthogonality deviation delta_{j+1}\n        cos_theta_basis_arg = np.abs(np.dot(v_current, v_next))\n        # Clamp argument to arccos\n        cos_theta_basis_arg = min(1.0, cos_theta_basis_arg)\n        \n        theta_basis = np.arccos(cos_theta_basis_arg)\n        delta = np.abs(theta_basis - np.pi / 2.0)\n        ortho_deviations.append(delta)\n        \n        # Add the new basis vector to the list\n        v_list.append(v_next)\n        \n    # Part 3: Calculate final metrics\n    min_raw_angle = np.min(raw_angles) if raw_angles else np.nan\n    max_ortho_dev = np.max(ortho_deviations) if ortho_deviations else np.nan\n    \n    return [min_raw_angle, max_ortho_dev]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (64, 0.0, 30),\n        (64, 3.13, 30),\n        (128, 80.0, 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_val, k_val, m_val = case\n        result = arnoldi_stability_analysis(n_val, k_val, m_val)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # [[a1,b1],[a2,b2],[a3,b3]] with no spaces.\n    results_str_list = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        }
    ]
}