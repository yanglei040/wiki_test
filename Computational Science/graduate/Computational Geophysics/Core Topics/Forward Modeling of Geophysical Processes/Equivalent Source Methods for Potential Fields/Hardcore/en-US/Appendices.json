{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any equivalent source method is the ability to accurately compute the forward response of the elementary sources. For methods that represent source distributions on a grid, the fundamental building block is often a uniformly dense rectangular patch. This exercise guides you through the first-principles derivation of the gravitational potential and vertical field for such a patch, yielding a closed-form expression that is essential for constructing the forward operator matrix in practical inversion codes .",
            "id": "3589271",
            "problem": "Consider the equivalent source method that represents a mass distribution on a plane by a tessellation of uniformly dense rectangular patches. Let a single rectangular patch lie in the plane $z=0$, with uniform areal density $\\sigma$ (mass per unit area), and be bounded by $x \\in [x_1, x_2]$ and $y \\in [y_1, y_2]$. Let the observation point be $\\mathbf{r}_0 = (x_0, y_0, z_0)$ with $z_0 > 0$ (the vertical coordinate is positive upward). Using Newtonian gravitation with gravitational constant $G$, the gravitational potential $U(\\mathbf{r}_0)$ due to the patch is defined by\n$$\nU(\\mathbf{r}_0) = G \\sigma \\iint_{\\text{patch}} \\frac{1}{\\|\\mathbf{r}_0 - \\mathbf{r}\\|} \\, \\mathrm{d}A,\n$$\nand the gravitational acceleration is $\\mathbf{g}(\\mathbf{r}_0) = \\nabla U(\\mathbf{r}_0)$, whose vertical component is $g_z(\\mathbf{r}_0) = \\frac{\\partial U}{\\partial z_0}(\\mathbf{r}_0)$.\n\nStarting from these definitions and first principles of potential theory, derive closed-form analytic expressions for $U(\\mathbf{r}_0)$ and $g_z(\\mathbf{r}_0)$ in terms of $x_1$, $x_2$, $y_1$, $y_2$, $x_0$, $y_0$, $z_0$, $\\sigma$, and $G$. Your derivation must be valid for an arbitrary observation point with $z_0>0$. Express your final result using corner-based inclusion-exclusion with four signed contributions that depend on the relative offsets to the observation point. No numerical evaluation is required. Provide the final answer as analytic expressions. Do not include units in the final boxed expressions.",
            "solution": "The problem requires the derivation of closed-form expressions for the gravitational potential $U(\\mathbf{r}_0)$ and the vertical component of gravitational acceleration $g_z(\\mathbf{r}_0)$ produced by a uniformly dense rectangular patch. The derivation must start from first principles and be expressed using a corner-based inclusion-exclusion formulation.\n\nLet the observation point be $\\mathbf{r}_0 = (x_0, y_0, z_0)$ with $z_0 > 0$. The rectangular patch is in the $z=0$ plane, bounded by $x \\in [x_1, x_2]$ and $y \\in [y_1, y_2]$, with uniform areal density $\\sigma$. A point on the patch is $\\mathbf{r} = (x, y, 0)$.\n\nThe distance between the observation point and a point on the patch is:\n$$ \\|\\mathbf{r}_0 - \\mathbf{r}\\| = \\sqrt{(x-x_0)^2 + (y-y_0)^2 + (z_0-0)^2} $$\nFor simplicity, we introduce relative coordinates: $\\xi = x - x_0$ and $\\eta = y - y_0$. The integration is performed over the shifted rectangle $[\\xi_1, \\xi_2] \\times [\\eta_1, \\eta_2]$, where $\\xi_i = x_i - x_0$ and $\\eta_j = y_j - y_0$.\n\nThe gravitational potential $U(\\mathbf{r}_0)$ is given by the integral:\n$$ U(\\mathbf{r}_0) = G \\sigma \\int_{y_1}^{y_2} \\int_{x_1}^{x_2} \\frac{1}{\\sqrt{(x-x_0)^2 + (y-y_0)^2 + z_0^2}} \\, dx \\, dy $$\nIn the shifted coordinates:\n$$ U(\\mathbf{r}_0) = G \\sigma \\int_{\\eta_1}^{\\eta_2} \\int_{\\xi_1}^{\\xi_2} \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} \\, d\\xi \\, d\\eta $$\n\nThe vertical component of gravitational acceleration, $g_z(\\mathbf{r}_0)$, is the partial derivative of the potential with respect to $z_0$:\n$$ g_z(\\mathbf{r}_0) = \\frac{\\partial U}{\\partial z_0} = \\frac{\\partial}{\\partial z_0} \\left( G \\sigma \\iint \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} \\, d\\xi \\, d\\eta \\right) $$\nUnder the condition $z_0 > 0$, the integrand is continuously differentiable, allowing us to interchange differentiation and integration (Leibniz integral rule):\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\iint \\frac{\\partial}{\\partial z_0} \\left( (\\xi^2 + \\eta^2 + z_0^2)^{-1/2} \\right) \\, d\\xi \\, d\\eta $$\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\iint -\\frac{1}{2} (\\xi^2 + \\eta^2 + z_0^2)^{-3/2} (2 z_0) \\, d\\xi \\, d\\eta $$\n$$ g_z(\\mathbf{r}_0) = -G \\sigma z_0 \\int_{\\eta_1}^{\\eta_2} \\int_{\\xi_1}^{\\xi_2} (\\xi^2 + \\eta^2 + z_0^2)^{-3/2} \\, d\\xi \\, d\\eta $$\n\n**Derivation of the Vertical Acceleration $g_z(\\mathbf{r}_0)$**\n\nWe solve the integral for $g_z$ by integrating first with respect to $\\xi$ and then $\\eta$. The order is interchangeable. Let's integrate with respect to $\\eta$ first.\nLet $A^2 = \\xi^2 + z_0^2$. The inner integral is:\n$$ I_\\eta = \\int \\frac{d\\eta}{(\\eta^2 + A^2)^{3/2}} $$\nThis is a standard integral, which evaluates to:\n$$ I_\\eta = \\frac{\\eta}{A^2 \\sqrt{\\eta^2 + A^2}} = \\frac{\\eta}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta^2+z_0^2}} $$\nEvaluating this over the interval $[\\eta_1, \\eta_2]$:\n$$ \\int_{\\eta_1}^{\\eta_2} (\\xi^2 + \\eta^2 + z_0^2)^{-3/2} \\, d\\eta = \\frac{\\eta_2}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta_2^2+z_0^2}} - \\frac{\\eta_1}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta_1^2+z_0^2}} $$\nNow, we integrate this result with respect to $\\xi$ over $[\\xi_1, \\xi_2]$. This requires evaluating an integral of the form:\n$$ I_\\xi = \\int \\frac{\\eta}{(\\xi^2+z_0^2)\\sqrt{\\xi^2+\\eta^2+z_0^2}} \\, d\\xi $$\nThis is also a standard, though less common, integral. It evaluates to:\n$$ I_\\xi = \\frac{1}{z_0} \\arctan\\left(\\frac{\\xi\\eta}{z_0\\sqrt{\\xi^2+\\eta^2+z_0^2}}\\right) $$\nLet's define the kernel for the indefinite integral of the $g_z$ integrand as $\\Psi_{g_z}(\\xi, \\eta, z_0)$.\nThe indefinite double integral of $-z_0(\\xi^2 + \\eta^2 + z_0^2)^{-3/2}$ is:\n$$ \\Psi_{g_z}(\\xi, \\eta, z_0) = -z_0 \\left( \\frac{1}{z_0} \\arctan\\left(\\frac{\\xi\\eta}{z_0\\sqrt{\\xi^2+\\eta^2+z_0^2}}\\right) \\right) = -\\arctan\\left(\\frac{\\xi\\eta}{z_0\\sqrt{\\xi^2+\\eta^2+z_0^2}}\\right) $$\nThe definite double integral over the rectangle $[\\xi_1, \\xi_2] \\times [\\eta_1, \\eta_2]$ is found by the inclusion-exclusion principle:\n$$ \\int_{\\eta_1}^{\\eta_2} \\int_{\\xi_1}^{\\xi_2} f(\\xi, \\eta) d\\xi d\\eta = F(\\xi_2, \\eta_2) - F(\\xi_2, \\eta_1) - F(\\xi_1, \\eta_2) + F(\\xi_1, \\eta_1) $$\nwhere $F$ is the indefinite double integral of $f$.\nApplying this to $g_z$:\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\left[ \\Psi_{g_z}(\\xi_2, \\eta_2) - \\Psi_{gz}(\\xi_2, \\eta_1) - \\Psi_{gz}(\\xi_1, \\eta_2) + \\Psi_{gz}(\\xi_1, \\eta_1) \\right] $$\nThis can be written as a sum over the four corners $(i,j)$ where $i,j \\in \\{1,2\\}$:\n$$ g_z(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\Psi_{g_z}(\\xi_i, \\eta_j, z_0) $$\nSubstituting the expression for $\\Psi_{g_z}$:\n$$ g_z(\\mathbf{r}_0) = -G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) $$\nwhere $\\xi_i = x_i - x_0$, $\\eta_j = y_j - y_0$, and $r_{ij} = \\sqrt{\\xi_i^2 + \\eta_j^2 + z_0^2}$.\n\n**Derivation of the Gravitational Potential $U(\\mathbf{r}_0)$**\n\nThe direct integration of $1/r$ is more involved.\n$$ U(\\mathbf{r}_0) = G \\sigma \\int_{\\eta_1}^{\\eta_2} \\left[ \\int_{\\xi_1}^{\\xi_2} \\frac{d\\xi}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} \\right] d\\eta $$\nThe inner integral with respect to $\\xi$ is:\n$$ \\int \\frac{d\\xi}{\\sqrt{\\xi^2 + (\\eta^2 + z_0^2)}} = \\ln(\\xi + \\sqrt{\\xi^2 + \\eta^2 + z_0^2}) $$\nThe subsequent integration with respect to $\\eta$ of the $\\ln(\\cdot)$ term is complex. A more robust method is to identify the indefinite double integral kernel, $\\Phi_U(\\xi, \\eta, z_0)$, and verify it. The kernel must satisfy:\n$$ \\frac{\\partial^2 \\Phi_U}{\\partial\\xi \\partial\\eta} = \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} $$\nThe known kernel for this integral is:\n$$ \\Phi_U(\\xi, \\eta, z_0) = \\xi \\ln(\\eta + r) + \\eta \\ln(\\xi + r) - z_0 \\arctan\\left(\\frac{\\xi\\eta}{z_0 r}\\right) $$\nwhere $r = \\sqrt{\\xi^2+\\eta^2+z_0^2}$. We can verify this by differentiation. The result is:\n$$ \\frac{\\partial^2 \\Phi_U}{\\partial\\xi \\partial\\eta} = \\frac{1}{r} = \\frac{1}{\\sqrt{\\xi^2 + \\eta^2 + z_0^2}} $$\nThe verification is successful. The kernel $\\Phi_U(\\xi, \\eta, z_0)$ is indeed the indefinite double integral of $1/r$.\nThe potential $U(\\mathbf{r}_0)$ is obtained by applying the inclusion-exclusion principle to this kernel:\n$$ U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\Phi_U(\\xi_i, \\eta_j, z_0) $$\n$$ U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\left[ \\xi_i \\ln(\\eta_j+r_{ij}) + \\eta_j \\ln(\\xi_i+r_{ij}) - z_0 \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) \\right] $$\nwith $\\xi_i = x_i - x_0$, $\\eta_j = y_j - y_0$, and $r_{ij} = \\sqrt{\\xi_i^2 + \\eta_j^2 + z_0^2}$.\n\nThis completes the derivation from first principles.\n\nFinal Expressions:\nLet $\\xi_i = x_i - x_0$ and $\\eta_j = y_j - y_0$ for $i,j \\in \\{1,2\\}$. Let $r_{ij} = \\sqrt{\\xi_i^2 + \\eta_j^2 + z_0^2}$.\nThe potential is:\n$$ U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\left[ \\xi_i \\ln(\\eta_j+r_{ij}) + \\eta_j \\ln(\\xi_i+r_{ij}) - z_0 \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) \\right] $$\nThe vertical gravitational acceleration is:\n$$ g_z(\\mathbf{r}_0) = -G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\arctan\\left(\\frac{\\xi_i\\eta_j}{z_0 r_{ij}}\\right) $$",
            "answer": "$$ \\boxed{ \\begin{pmatrix} U(\\mathbf{r}_0) = G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\left[ (x_i-x_0) \\ln(y_j-y_0+r_{ij}) + (y_j-y_0) \\ln(x_i-x_0+r_{ij}) - z_0 \\arctan\\left(\\frac{(x_i-x_0)(y_j-y_0)}{z_0 r_{ij}}\\right) \\right] \\\\ g_z(\\mathbf{r}_0) = -G \\sigma \\sum_{i=1}^2 \\sum_{j=1}^2 (-1)^{i+j} \\arctan\\left(\\frac{(x_i-x_0)(y_j-y_0)}{z_0 r_{ij}}\\right) \\end{pmatrix} } $$"
        },
        {
            "introduction": "A major challenge in potential field inversion is the natural decay of the signal with distance, which makes sources at greater depths far less detectable than shallow ones. This disparity leads to poorly conditioned inverse problems where solutions are unstable and dominated by shallow artifacts. This practice directly addresses this issue by having you derive a depth-weighting function, a key technique to balance the sensitivity of the inversion to sources at all depths and regularize the problem from a physical standpoint .",
            "id": "3589251",
            "problem": "Consider a three-dimensional ($3$D) potential field measured on a horizontal observation plane at $z=0$ above a homogeneous half-space. An equivalent layer is used to represent the field by a distribution of scalar monopoles with surface density $\\sigma(x',y')$ located on a plane at depth $z=z_{0}0$. Let $U(x,y,z)$ denote the scalar potential at $(x,y,z)$ produced by the layer, and let the measured datum be the $n$th vertical derivative of the potential evaluated on the observation plane, that is $d_{n}(x,y)=\\partial^{n}U(x,y,0)/\\partial z^{n}$, where $n$ is a positive integer.\n\nYou will model the forward response using the fundamental Green’s function of the Laplace equation in three dimensions, $G(\\mathbf{r})=1/(4\\pi|\\mathbf{r}|)$, and you may assume that the sources are sufficiently localized that the on-axis behavior (the receiver directly above a source element, i.e., $(x,y)=(x',y')$) controls the leading-order depth dependence of the sensitivity for large $z_{0}$. The equivalent-layer sensitivity kernel for $d_{n}$ with respect to a unit source at $(x',y',z_{0})$ is $K_{n}(x-x',y-y';z_{0})=\\partial^{n}G(\\mathbf{r})/\\partial z^{n}$ with $\\mathbf{r}=(x-x',y-y',0-z_{0})$.\n\nDerive, from first principles and the asymptotic behavior of $G(\\mathbf{r})$ for large $z_{0}$, an explicit depth-weighting function $w(z_{0})$ that counteracts the decay of sensitivity with depth in the sense that $w(z_{0})\\,K_{n}(0,0;z_{0})$ is independent of $z_{0}$ to leading order as $z_{0}\\to\\infty$. Normalize your answer by imposing $w(1)=1$ so that the weighting is dimensionless. Provide your final expression for $w(z_{0})$ in closed form. No numerical evaluation is required, and your answer must be a single analytical expression.",
            "solution": "The problem requires the derivation of a depth-weighting function, $w(z_{0})$, for an equivalent-layer representation of a potential field. The function must counteract the decay of the on-axis sensitivity kernel with depth, $z_{0}$. The derivation will proceed from the fundamental principles laid out in the problem statement.\n\nFirst, we identify the key components. The potential field is modeled using the Green's function for the $3$D Laplace equation, given as $G(\\mathbf{r}) = \\frac{1}{4\\pi|\\mathbf{r}|}$. The source layer is at a constant depth $z = z_{0} > 0$, and the observation plane is at $z=0$. The vector $\\mathbf{r}$ connects a source point $(x', y', z_{0})$ to an observation point $(x, y, z)$, so $\\mathbf{r} = (x-x', y-y', z-z_{0})$. The magnitude of this vector is $|\\mathbf{r}| = \\sqrt{(x-x')^{2} + (y-y')^{2} + (z-z_{0})^{2}}$.\n\nThe sensitivity kernel for the $n$-th vertical derivative of the potential, $d_{n}(x,y)$, is given by $K_{n}(x-x',y-y';z_{0})$. This kernel represents the response at $(x,y,0)$ due to a unit source at $(x',y',z_{0})$. It is defined as the $n$-th partial derivative of the Green's function with respect to the vertical observation coordinate $z$, evaluated at the observation plane $z=0$. Formally,\n$$\nK_{n}(x-x', y-y'; z_{0}) = \\left. \\frac{\\partial^{n}}{\\partial z^{n}} G(x-x', y-y', z-z_{0}) \\right|_{z=0}\n$$\nThe problem specifies that the leading-order depth dependence is controlled by the on-axis behavior, where the receiver is directly above the source element. This corresponds to the case where $(x,y) = (x',y')$. We therefore need to find the expression for the on-axis kernel, $K_{n}(0,0;z_{0})$.\n\nFor the on-axis case, $x-x'=0$ and $y-y'=0$. The magnitude of the position vector simplifies to $|\\mathbf{r}| = \\sqrt{0^{2} + 0^{2} + (z-z_{0})^{2}} = |z-z_{0}|$. Since the observation plane is at $z=0$ and the source plane is at $z_{0}0$, we are concerned with points in space where $z \\le 0$ or at least $z  z_{0}$. In this regime, $z-z_{0}$ is always negative, so $|z-z_{0}| = -(z-z_{0}) = z_{0}-z$.\n\nThe on-axis Green's function is then:\n$$\nG_{\\text{on-axis}}(z; z_{0}) = \\frac{1}{4\\pi(z_{0}-z)} = \\frac{1}{4\\pi}(z_{0}-z)^{-1}\n$$\nNow, we compute the $n$-th partial derivative of this function with respect to $z$. We perform the first few differentiations to establish a pattern.\nFor $n=1$:\n$$\n\\frac{\\partial}{\\partial z} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{1}{4\\pi} (-1)(z_{0}-z)^{-2}(-1) = \\frac{1}{4\\pi}(z_{0}-z)^{-2}\n$$\nFor $n=2$:\n$$\n\\frac{\\partial^{2}}{\\partial z^{2}} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{\\partial}{\\partial z} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-2} \\right) = \\frac{1}{4\\pi} (-2)(z_{0}-z)^{-3}(-1) = \\frac{2}{4\\pi}(z_{0}-z)^{-3}\n$$\nFor $n=3$:\n$$\n\\frac{\\partial^{3}}{\\partial z^{3}} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{\\partial}{\\partial z} \\left( \\frac{2}{4\\pi}(z_{0}-z)^{-3} \\right) = \\frac{2}{4\\pi} (-3)(z_{0}-z)^{-4}(-1) = \\frac{6}{4\\pi}(z_{0}-z)^{-4}\n$$\nBy inspection and induction, the general formula for the $n$-th derivative is:\n$$\n\\frac{\\partial^{n}}{\\partial z^{n}} \\left( \\frac{1}{4\\pi}(z_{0}-z)^{-1} \\right) = \\frac{n!}{4\\pi}(z_{0}-z)^{-(n+1)}\n$$\nTo obtain the on-axis sensitivity kernel $K_{n}(0,0;z_{0})$, we evaluate this expression at the observation plane $z=0$:\n$$\nK_{n}(0,0;z_{0}) = \\left. \\frac{n!}{4\\pi}(z_{0}-z)^{-(n+1)} \\right|_{z=0} = \\frac{n!}{4\\pi}z_{0}^{-(n+1)}\n$$\nThis expression describes the decay of the on-axis sensitivity with source depth $z_{0}$ for the $n$-th vertical derivative of the potential.\n\nThe problem requires us to find a depth-weighting function $w(z_{0})$ such that the product $w(z_{0})K_{n}(0,0;z_{0})$ is independent of $z_{0}$ for large $z_{0}$. This means the product must be a constant. Let this constant be $C$.\n$$\nw(z_{0}) K_{n}(0,0;z_{0}) = C\n$$\nSubstituting the derived expression for $K_{n}(0,0;z_{0})$:\n$$\nw(z_{0}) \\left( \\frac{n!}{4\\pi}z_{0}^{-(n+1)} \\right) = C\n$$\nTo make the left-hand side independent of $z_{0}$, $w(z_{0})$ must be structured to cancel the $z_{0}^{-(n+1)}$ term. Therefore, $w(z_{0})$ must be proportional to the reciprocal of this term:\n$$\nw(z_{0}) \\propto \\frac{1}{z_{0}^{-(n+1)}} = z_{0}^{n+1}\n$$\nWe can express this relationship with a proportionality constant, $A$:\n$$\nw(z_{0}) = A \\, z_{0}^{n+1}\n$$\nThe problem provides a normalization condition to determine the constant $A$: $w(1)=1$. We apply this condition:\n$$\nw(1) = A \\cdot (1)^{n+1} = A\n$$\nThis implies that $A=1$.\n\nTherefore, the explicit, closed-form expression for the normalized depth-weighting function is:\n$$\nw(z_{0}) = z_{0}^{n+1}\n$$\nThis function, when multiplied by the on-axis sensitivity kernel $K_{n}(0,0;z_{0})$, yields a result $\\frac{n!}{4\\pi}$ which is indeed independent of the depth $z_{0}$.",
            "answer": "$$\\boxed{z_{0}^{n+1}}$$"
        },
        {
            "introduction": "After formulating a forward model and addressing its conditioning, solving the inverse problem requires balancing data fidelity with solution simplicity, a trade-off controlled by the Tikhonov regularization parameter $\\lambda$. Choosing an appropriate $\\lambda$ is non-trivial and critical for obtaining a meaningful result. This practice explores the discrepancy principle, a powerful method for selecting $\\lambda$ when the data noise variance is known, and asks you to derive a Newton's method algorithm to find the optimal value, providing both theoretical insight and a practical computational tool .",
            "id": "3589321",
            "problem": "Consider a two-dimensional scalar potential field measured at $n$ observation points located at height $z=z_{0}$ above a horizontal plane containing $p$ equivalent point sources at $z=0$. Let the vector of observed data be $\\mathbf{d} \\in \\mathbb{R}^{n}$, and let the vector of equivalent source strengths be $\\mathbf{m} \\in \\mathbb{R}^{p}$. The forward operator mapping source strengths to observations is the linear matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times p}$. Assume the data noise is zero-mean Gaussian with covariance $\\mathbf{C}_{\\epsilon} = \\sigma^{2} \\mathbf{I}_{n}$, where $\\sigma^{2}$ is known and $\\mathbf{I}_{n}$ is the $n \\times n$ identity matrix.\n\nTo estimate $\\mathbf{m}$ from $\\mathbf{d}$, consider Tikhonov regularization with a zero-order stabilizer, solving\n$$\n\\min_{\\mathbf{m}} \\left\\| \\mathbf{W}_{d} \\left( \\mathbf{G} \\mathbf{m} - \\mathbf{d} \\right) \\right\\|_{2}^{2} + \\lambda^{2} \\left\\| \\mathbf{m} \\right\\|_{2}^{2},\n$$\nwhere $\\mathbf{W}_{d} = \\sigma^{-1} \\mathbf{I}_{n}$ whitens the data, and $\\lambda  0$ is the regularization parameter. Define the whitened quantities $\\tilde{\\mathbf{G}} = \\mathbf{W}_{d} \\mathbf{G}$ and $\\tilde{\\mathbf{d}} = \\mathbf{W}_{d} \\mathbf{d}$. Let the singular value decomposition (SVD) of $\\tilde{\\mathbf{G}}$ be $\\tilde{\\mathbf{G}} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}$, where $\\mathbf{U} \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{V} \\in \\mathbb{R}^{p \\times p}$ are orthogonal, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times p}$ has nonzero singular values $\\sigma_{i}  0$ for $i=1,\\dots,p$ on its diagonal. Denote $a_{i} = \\mathbf{u}_{i}^{\\top} \\tilde{\\mathbf{d}}$, where $\\mathbf{u}_{i}$ is the $i$-th column of $\\mathbf{U}$.\n\nThe discrepancy principle (also known as Morozov’s principle) prescribes choosing $\\lambda$ such that the whitened residual norm satisfies\n$$\n\\left\\| \\tilde{\\mathbf{r}}(\\lambda) \\right\\|_{2}^{2} = n,\n$$\nwhere $\\tilde{\\mathbf{r}}(\\lambda) = \\tilde{\\mathbf{d}} - \\tilde{\\mathbf{G}} \\mathbf{m}(\\lambda)$ is the whitened residual and $\\mathbf{m}(\\lambda)$ is the regularized solution. Starting from the above definitions and the structure of Tikhonov regularization, derive the explicit expression for the discrepancy function\n$$\n\\Phi(\\lambda) = \\left\\| \\tilde{\\mathbf{r}}(\\lambda) \\right\\|_{2}^{2}\n$$\nin terms of $\\{ \\sigma_{i} \\}_{i=1}^{p}$ and $\\{ a_{i} \\}_{i=1}^{n}$, and its derivative $\\Phi'(\\lambda)$. Then, derive a one-step Newton update formula for $\\lambda$ that solves $\\Phi(\\lambda) = n$.\n\nIn your derivation, start from the linearity of the forward operator, properties of the singular value decomposition, and the definition of Tikhonov regularization. Clearly justify each step of the construction of $\\Phi(\\lambda)$ and $\\Phi'(\\lambda)$, and how they lead to the Newton update for $\\lambda$.\n\nFinally, briefly analyze how selecting $\\lambda$ via the discrepancy principle compares, in principle, to selecting $\\lambda$ via Generalized Cross-Validation (GCV) and the L-curve method, with attention to the role of known noise variance and the effective degrees of freedom of the fit.\n\nProvide as your final answer a single closed-form analytical expression for the Newton update $\\lambda_{k+1}$ in terms of $\\lambda_{k}$, $\\{ \\sigma_{i} \\}$, and $\\{ a_{i} \\}$, suitable for iterative computation of $\\lambda$ under the discrepancy principle. No numerical evaluation is required. Your final answer must be a single expression and must not include any units.",
            "solution": "The Tikhonov regularization problem is formulated to minimize the objective function $J(\\mathbf{m})$:\n$$\nJ(\\mathbf{m}) = \\left\\| \\tilde{\\mathbf{G}} \\mathbf{m} - \\tilde{\\mathbf{d}} \\right\\|_{2}^{2} + \\lambda^{2} \\left\\| \\mathbf{m} \\right\\|_{2}^{2}\n$$\nwhere $\\tilde{\\mathbf{G}} = \\sigma^{-1}\\mathbf{G}$ and $\\tilde{\\mathbf{d}} = \\sigma^{-1}\\mathbf{d}$ are the whitened operator and data, respectively. To find the minimum, we compute the gradient of $J(\\mathbf{m})$ with respect to $\\mathbf{m}$ and set it to zero.\n$$\n\\nabla_{\\mathbf{m}} J(\\mathbf{m}) = \\nabla_{\\mathbf{m}} \\left( (\\tilde{\\mathbf{G}}\\mathbf{m} - \\tilde{\\mathbf{d}})^{\\top}(\\tilde{\\mathbf{G}}\\mathbf{m} - \\tilde{\\mathbf{d}}) + \\lambda^2 \\mathbf{m}^{\\top}\\mathbf{m} \\right) = 2\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}}\\mathbf{m} - 2\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{d}} + 2\\lambda^2 \\mathbf{m} = \\mathbf{0}\n$$\nThis yields the normal equations for the regularized solution $\\mathbf{m}(\\lambda)$:\n$$\n(\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}} + \\lambda^2 \\mathbf{I}_{p}) \\mathbf{m}(\\lambda) = \\tilde{\\mathbf{G}}^{\\top} \\tilde{\\mathbf{d}}\n$$\nThe solution for the model parameters is thus:\n$$\n\\mathbf{m}(\\lambda) = (\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}} + \\lambda^2 \\mathbf{I}_{p})^{-1} \\tilde{\\mathbf{G}}^{\\top} \\tilde{\\mathbf{d}}\n$$\nTo proceed, we substitute the SVD of $\\tilde{\\mathbf{G}} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}$.\nThe term $\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}}$ becomes:\n$$\n\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}} = (\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top})^{\\top}(\\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^{\\top}) = \\mathbf{V}\\boldsymbol{\\Sigma}^{\\top}\\mathbf{U}^{\\top}\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top} = \\mathbf{V}(\\boldsymbol{\\Sigma}^{\\top}\\boldsymbol{\\Sigma})\\mathbf{V}^{\\top}\n$$\nSince $\\boldsymbol{\\Sigma}$ is an $n \\times p$ matrix with diagonal entries $\\sigma_{i}$ for $i=1,\\dots,p$, $\\boldsymbol{\\Sigma}^{\\top}\\boldsymbol{\\Sigma}$ is a $p \\times p$ diagonal matrix with entries $\\sigma_{i}^{2}$. So, $(\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}} + \\lambda^2 \\mathbf{I}_{p}) = \\mathbf{V}\\text{diag}(\\sigma_{i}^{2})\\mathbf{V}^{\\top} + \\lambda^2\\mathbf{V}\\mathbf{V}^{\\top} = \\mathbf{V}\\text{diag}(\\sigma_{i}^{2} + \\lambda^{2})\\mathbf{V}^{\\top}$.\nThe inverse is $(\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{G}} + \\lambda^2 \\mathbf{I}_{p})^{-1} = \\mathbf{V}\\text{diag}\\left(\\frac{1}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)\\mathbf{V}^{\\top}$.\nAlso, $\\tilde{\\mathbf{G}}^{\\top}\\tilde{\\mathbf{d}} = \\mathbf{V}\\boldsymbol{\\Sigma}^{\\top}\\mathbf{U}^{\\top}\\tilde{\\mathbf{d}}$.\nCombining these, the solution $\\mathbf{m}(\\lambda)$ is:\n$$\n\\mathbf{m}(\\lambda) = \\mathbf{V}\\text{diag}\\left(\\frac{1}{\\sigma_{i}^{2} + \\lambda^{2}}\\right)\\mathbf{V}^{\\top} \\mathbf{V}\\boldsymbol{\\Sigma}^{\\top}\\mathbf{U}^{\\top}\\tilde{\\mathbf{d}} = \\mathbf{V}\\text{diag}\\left(\\frac{1}{\\sigma_{i}^{2} + \\lambda^{2}}\\right) \\boldsymbol{\\Sigma}^{\\top}\\mathbf{U}^{\\top}\\tilde{\\mathbf{d}}\n$$\nThe vector $\\mathbf{U}^{\\top}\\tilde{\\mathbf{d}}$ has components $a_i = \\mathbf{u}_{i}^{\\top}\\tilde{\\mathbf{d}}$. The product $\\boldsymbol{\\Sigma}^{\\top}(\\mathbf{U}^{\\top}\\tilde{\\mathbf{d}})$ results in a vector in $\\mathbb{R}^{p}$ with components $\\sigma_{i}a_{i}$. Therefore, $\\mathbf{m}(\\lambda)$ can be expressed as a sum over the basis vectors $\\mathbf{v}_{i}$:\n$$\n\\mathbf{m}(\\lambda) = \\sum_{i=1}^{p} \\frac{\\sigma_{i} a_{i}}{\\sigma_{i}^{2} + \\lambda^{2}} \\mathbf{v}_{i}\n$$\nNow we derive the discrepancy function $\\Phi(\\lambda) = \\left\\| \\tilde{\\mathbf{r}}(\\lambda) \\right\\|_{2}^{2} = \\|\\tilde{\\mathbf{d}} - \\tilde{\\mathbf{G}}\\mathbf{m}(\\lambda)\\|_2^2$. The squared L2-norm is invariant under the orthogonal transformation $\\mathbf{U}^{\\top}$, so:\n$$\n\\Phi(\\lambda) = \\|\\mathbf{U}^{\\top}(\\tilde{\\mathbf{d}} - \\tilde{\\mathbf{G}}\\mathbf{m}(\\lambda))\\|_2^2 = \\|\\mathbf{U}^{\\top}\\tilde{\\mathbf{d}} - \\mathbf{U}^{\\top}\\tilde{\\mathbf{G}}\\mathbf{m}(\\lambda)\\|_2^2\n$$\nWe have $\\mathbf{U}^{\\top}\\tilde{\\mathbf{G}} = \\mathbf{U}^{\\top}\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top} = \\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$. The predicted data in the transformed domain, $\\mathbf{U}^{\\top}\\tilde{\\mathbf{G}}\\mathbf{m}(\\lambda)$, can be computed by substituting the expression for $\\mathbf{m}(\\lambda)$. The $i$-th component of the resulting vector is $\\frac{\\sigma_i^2 a_i}{\\sigma_i^2 + \\lambda^2}$ for $i=1,\\dots,p$, and $0$ for $i>p$.\nThe components of the residual vector $\\mathbf{U}^{\\top}\\tilde{\\mathbf{r}}(\\lambda)$ are:\n$$\n(\\mathbf{U}^{\\top}\\tilde{\\mathbf{r}}(\\lambda))_{i} = a_{i} - \\frac{\\sigma_{i}^{2} a_{i}}{\\sigma_{i}^{2} + \\lambda^{2}} = a_{i}\\left(1 - \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}\\right) = \\frac{\\lambda^{2} a_{i}}{\\sigma_{i}^{2} + \\lambda^{2}} \\quad \\text{for } i=1,\\dots,p\n$$\n$$\n(\\mathbf{U}^{\\top}\\tilde{\\mathbf{r}}(\\lambda))_{i} = a_{i} - 0 = a_{i} \\quad \\text{for } i=p+1,\\dots,n\n$$\nThe squared norm $\\Phi(\\lambda)$ is the sum of squares of these components:\n$$\n\\Phi(\\lambda) = \\sum_{i=1}^{p} \\left( \\frac{\\lambda^{2} a_{i}}{\\sigma_{i}^{2} + \\lambda^{2}} \\right)^{2} + \\sum_{i=p+1}^{n} a_{i}^{2}\n$$\nThis is the explicit expression for the discrepancy function. Next, we find its derivative with respect to $\\lambda$, $\\Phi'(\\lambda)$:\n$$\n\\Phi'(\\lambda) = \\frac{d}{d\\lambda} \\left( \\sum_{i=1}^{p} \\frac{\\lambda^{4} a_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}} + \\sum_{i=p+1}^{n} a_{i}^{2} \\right)\n$$\nThe second sum is constant with respect to $\\lambda$. We differentiate the term inside the first sum using the quotient rule:\n$$\n\\frac{d}{d\\lambda} \\left( \\frac{\\lambda^{4}}{(\\sigma_{i}^{2} + \\lambda^{2})^{2}} \\right) = \\frac{4\\lambda^{3}(\\sigma_{i}^{2} + \\lambda^{2})^{2} - \\lambda^{4} \\cdot 2(\\sigma_{i}^{2} + \\lambda^{2})(2\\lambda)}{(\\sigma_{i}^{2} + \\lambda^{2})^{4}} = \\frac{4\\lambda^{3}(\\sigma_{i}^{2} + \\lambda^{2}) - 4\\lambda^{5}}{(\\sigma_{i}^{2} + \\lambda^{2})^{3}} = \\frac{4\\lambda^{3}\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{3}}\n$$\nThus, the derivative of the discrepancy function is:\n$$\n\\Phi'(\\lambda) = \\sum_{i=1}^{p} a_{i}^{2} \\frac{4\\lambda^{3}\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{3}} = 4\\lambda^{3} \\sum_{i=1}^{p} \\frac{a_{i}^{2}\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda^{2})^{3}}\n$$\nThe discrepancy principle requires finding $\\lambda$ such that $\\Phi(\\lambda) = n$. This is a nonlinear root-finding problem for the function $f(\\lambda) = \\Phi(\\lambda) - n$. Newton's method provides an iterative update scheme:\n$$\n\\lambda_{k+1} = \\lambda_{k} - \\frac{f(\\lambda_{k})}{f'(\\lambda_{k})} = \\lambda_{k} - \\frac{\\Phi(\\lambda_{k}) - n}{\\Phi'(\\lambda_{k})}\n$$\nSubstituting the expressions for $\\Phi(\\lambda_k)$ and $\\Phi'(\\lambda_k)$ gives the one-step Newton update formula.\n\nThe **discrepancy principle** is statistically motivated. It sets the whitened residual norm $\\|\\tilde{\\mathbf{r}}(\\lambda)\\|_2^2$ equal to its expected value, $n$. This is because the whitened noise vector $\\tilde{\\mathbf{\\epsilon}} = \\sigma^{-1}\\mathbf{\\epsilon}$ has a covariance of $\\mathbf{I}_{n}$, and $E[\\|\\tilde{\\mathbf{\\epsilon}}\\|_2^2] = \\text{Tr}(\\mathbf{I}_{n}) = n$. This method requires an accurate, a priori estimate of the noise variance $\\sigma^2$. If the estimate of $\\sigma^2$ is poor, the resulting $\\lambda$ can be suboptimal.\n\nThe **GCV method** does not require knowledge of $\\sigma^2$. It seeks to minimize the GCV function, which is a proxy for leave-one-out cross-validation error. The GCV function is $V(\\lambda) = \\frac{\\|\\mathbf{G}\\mathbf{m}(\\lambda)-\\mathbf{d}\\|_2^2}{(\\text{Tr}(\\mathbf{I}-\\mathbf{A}(\\lambda)))^2}$, where $\\mathbf{A}(\\lambda)$ is the influence matrix that maps data to predictions. The term in the denominator, $\\text{Tr}(\\mathbf{I}-\\mathbf{A}(\\lambda))$, is the effective degrees of freedom of the residual. GCV thus seeks a value of $\\lambda$ that minimizes the residual norm, but penalizes models that are too complex.\n\nThe **L-curve method** is a heuristic technique that also does not require knowledge of $\\sigma^2$. It involves plotting the solution norm $\\|\\mathbf{m}(\\lambda)\\|_2$ against the residual norm $\\|\\mathbf{G}\\mathbf{m}(\\lambda)-\\mathbf{d}\\|_2$ on a log-log scale for a range of $\\lambda$ values. The resulting curve typically has an 'L' shape. The corner of this 'L' is interpreted as the point of optimal balance between minimizing the residual (data fit) and minimizing the solution norm (regularization). While intuitive, it is less statistically rigorous than the other two methods.",
            "answer": "$$\n\\boxed{\\lambda_{k+1} = \\lambda_{k} - \\frac{\\left( \\sum_{i=1}^{p} \\left( \\frac{\\lambda_{k}^{2} a_{i}}{\\sigma_{i}^{2} + \\lambda_{k}^{2}} \\right)^{2} + \\sum_{i=p+1}^{n} a_{i}^{2} \\right) - n}{4\\lambda_{k}^{3} \\sum_{i=1}^{p} \\frac{a_{i}^{2}\\sigma_{i}^{2}}{(\\sigma_{i}^{2} + \\lambda_{k}^{2})^{3}}}}\n$$"
        }
    ]
}