{
    "hands_on_practices": [
        {
            "introduction": "In many geophysical inversion problems, we aim to build a model that explains multiple, physically distinct datasets. This exercise  explores how to apply the steepest descent method to such a multi-objective scenario, a common task in joint inversion. You will learn a standard technique for combining different least-squares misfit functionals by normalizing them based on their initial gradient magnitudes, creating a balanced aggregate objective function to guide the descent. This practice provides a concrete example of adapting a fundamental optimization algorithm to handle the complexities of integrating diverse geophysical data.",
            "id": "3617285",
            "problem": "Consider a two-parameter linearized joint inversion in computational geophysics for combining travel-time and gravity-anomaly residuals. Let the model vector be $m \\in \\mathbb{R}^{2}$ and consider two least-squares (LS) component misfit functionals derived from independent Gaussian error models for each data type,\n$$\n\\phi_{1}(m) \\equiv \\tfrac{1}{2}\\|A m - r_{1}\\|_{2}^{2}, \n\\qquad\n\\phi_{2}(m) \\equiv \\tfrac{1}{2}\\|B m - r_{2}\\|_{2}^{2},\n$$\nwith $A = I_{2}$ and $B = I_{2}$, where $I_{2}$ denotes the $2 \\times 2$ identity matrix. The linearized residuals are\n$$\nr_{1} = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}, \n\\qquad\nr_{2} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix},\n$$\nand the initial model is $m^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nTo handle the different scales of the two objectives in a principled way, define normalization factors based on the initial gradient magnitudes,\n$$\ns_{k} \\equiv \\|\\nabla \\phi_{k}(m^{(0)})\\|_{2}, \\quad k \\in \\{1,2\\},\n$$\nand the weighted, normalized aggregate objective\n$$\n\\Phi(m) \\equiv \\sum_{k=1}^{2} w_{k} \\,\\frac{\\phi_{k}(m)}{s_{k}},\n$$\nwith weights $w_{1} = 2$ and $w_{2} = 1$.\n\nUsing the steepest descent method with exact line search applied to $\\Phi$, form the first steepest descent direction $p^{(0)} \\equiv -\\nabla \\Phi(m^{(0)})$ and determine the optimal step length $\\alpha^{\\star}$ that minimizes $\\Phi(m^{(0)} + \\alpha p^{(0)})$ along this direction. Provide the value of $\\alpha^{\\star}$ as a single real number. No rounding is required, and no units are needed.",
            "solution": "To find the optimal step length $\\alpha^{\\star}$, we follow these steps:\n\n1.  **Compute the gradients of the component functionals.**\n    For $\\phi_{1}(m) = \\frac{1}{2}\\|m - r_{1}\\|_{2}^{2}$, the gradient is $\\nabla \\phi_{1}(m) = m - r_{1}$.\n    For $\\phi_{2}(m) = \\frac{1}{2}\\|m - r_{2}\\|_{2}^{2}$, the gradient is $\\nabla \\phi_{2}(m) = m - r_{2}$.\n\n2.  **Evaluate the gradients at the initial model** $m^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n    $$\n    \\nabla \\phi_{1}(m^{(0)}) = m^{(0)} - r_{1} = -\\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix}\n    $$\n    $$\n    \\nabla \\phi_{2}(m^{(0)}) = m^{(0)} - r_{2} = -\\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}\n    $$\n\n3.  **Calculate the normalization factors** $s_1$ and $s_2$.\n    $$\n    s_{1} = \\|\\nabla \\phi_{1}(m^{(0)})\\|_{2} = \\sqrt{(-3)^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n    $$\n    $$\n    s_{2} = \\|\\nabla \\phi_{2}(m^{(0)})\\|_{2} = \\sqrt{(-4)^2 + (-3)^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5\n    $$\n\n4.  **Construct the aggregate objective function** $\\Phi(m)$.\n    With weights $w_1 = 2$, $w_2 = 1$ and normalization factors $s_1=5$, $s_2=5$:\n    $$\n    \\Phi(m) = w_{1} \\frac{\\phi_{1}(m)}{s_{1}} + w_{2} \\frac{\\phi_{2}(m)}{s_{2}} = \\frac{2}{5}\\phi_{1}(m) + \\frac{1}{5}\\phi_{2}(m)\n    $$\n\n5.  **Compute the gradient of the aggregate function** $\\nabla \\Phi(m)$.\n    $$\n    \\nabla \\Phi(m) = \\frac{2}{5}\\nabla \\phi_{1}(m) + \\frac{1}{5}\\nabla \\phi_{2}(m) = \\frac{2}{5}(m - r_{1}) + \\frac{1}{5}(m - r_{2}) = \\frac{3}{5}m - \\frac{2}{5}r_{1} - \\frac{1}{5}r_{2}\n    $$\n\n6.  **Evaluate the gradient at the initial model** to find the steepest descent direction.\n    $$\n    \\nabla \\Phi(m^{(0)}) = -\\frac{2}{5}r_{1} - \\frac{1}{5}r_{2} = -\\frac{1}{5} \\left( 2\\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} + \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} \\right) = -\\frac{1}{5} \\left( \\begin{pmatrix} 6 \\\\ -8 \\end{pmatrix} + \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} \\right) = -\\frac{1}{5}\\begin{pmatrix} 10 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}\n    $$\n    The steepest descent direction is $p^{(0)} = -\\nabla \\Phi(m^{(0)}) = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\n\n7.  **Determine the optimal step length** $\\alpha^{\\star}$ using an exact line search.\n    The objective function $\\Phi(m)$ is quadratic. For a quadratic function, the optimal step length $\\alpha^{\\star}$ along the direction $p^{(0)}$ is given by the formula:\n    $$\n    \\alpha^{\\star} = \\frac{(\\nabla \\Phi(m^{(0)}))^{T} (\\nabla \\Phi(m^{(0)}))}{(\\nabla \\Phi(m^{(0)}))^{T} H (\\nabla \\Phi(m^{(0)}))}\n    $$\n    where $H$ is the Hessian of $\\Phi(m)$.\n    First, let's find the Hessian:\n    $$\n    H = \\nabla^{2} \\Phi(m) = \\nabla \\left( \\frac{3}{5}m - \\frac{2}{5}r_{1} - \\frac{1}{5}r_{2} \\right) = \\frac{3}{5}I_{2} = \\begin{pmatrix} 3/5 & 0 \\\\ 0 & 3/5 \\end{pmatrix}\n    $$\n    Now, we compute the numerator and denominator for $\\alpha^{\\star}$.\n    Numerator:\n    $$\n    (\\nabla \\Phi(m^{(0)}))^{T} (\\nabla \\Phi(m^{(0)})) = \\begin{pmatrix} -2 & 1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = (-2)^2 + 1^2 = 5\n    $$\n    Denominator:\n    $$\n    (\\nabla \\Phi(m^{(0)}))^{T} H (\\nabla \\Phi(m^{(0)})) = \\begin{pmatrix} -2 & 1 \\end{pmatrix} \\begin{pmatrix} 3/5 & 0 \\\\ 0 & 3/5 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 & 1 \\end{pmatrix} \\begin{pmatrix} -6/5 \\\\ 3/5 \\end{pmatrix} = (-2)(-\\frac{6}{5}) + (1)(\\frac{3}{5}) = \\frac{12}{5} + \\frac{3}{5} = \\frac{15}{5} = 3\n    $$\n    Finally, we calculate the optimal step length:\n    $$\n    \\alpha^{\\star} = \\frac{5}{3}\n    $$",
            "answer": "$$\n\\boxed{\\frac{5}{3}}\n$$"
        },
        {
            "introduction": "The direction of steepest descent is fundamentally tied to how we measure distances and angles, a choice defined by the inner product. While the standard Euclidean inner product is common, it is often not the most effective choice for problems discretized on a grid. This exercise  delves into this concept by asking you to derive a steepest descent direction with respect to a non-Euclidean inner product that is consistent with a finite element discretization. This practice illuminates the principle of preconditioning, demonstrating how tailoring the optimization geometry to the problem's structure can lead to more physically meaningful and efficient algorithms.",
            "id": "3617265",
            "problem": "Consider the one-dimensional diffusion model representative of steady groundwater flow in computational geophysics on the domain $[0,1]$ with homogeneous Dirichlet boundary conditions at $x=0$ and $x=1$. Discretize using continuous, piecewise-linear finite elements on an irregular mesh with nodes $x_0=0$, $x_1=\\epsilon$, $x_2=1-\\epsilon$, and $x_3=1$, where $\\epsilon \\in (0,\\tfrac{1}{2})$. The element lengths are $h_1=\\epsilon$, $h_2=1-2\\epsilon$, and $h_3=\\epsilon$. The two interior unknowns are the nodal values at $x_1$ and $x_2$, denoted by the vector $u \\in \\mathbb{R}^2$.\n\nLet the discrete inner product be defined by $\\langle u,v\\rangle_h=\\sum_{i=1}^2 w_i u_i v_i$ with unknown positive weights $w_i$. Your objective is to ensure that $\\langle \\cdot,\\cdot\\rangle_h$ is a consistent approximation of the continuum $L^2$ inner product $\\int_0^1 u v \\, dx$ when $u$ and $v$ are approximated by the finite element basis functions. That is, the discrete inner product must arise from mass lumping of the continuum $L^2$ product.\n\nAssume a constant source term $f(x)=f_0$ with $f_0>0$. The algebraic right-hand side entries are $b_i=\\int_0^1 f_0 \\,\\phi_i(x)\\,dx$, where $\\phi_i$ are the usual nodal hat functions associated with the two interior nodes $x_1$ and $x_2$. Consider the discrete quadratic functional $J(u)=\\tfrac{1}{2}u^\\top K u - b^\\top u$, where $K$ is the symmetric positive-definite stiffness matrix arising from the finite element discretization and $b\\in\\mathbb{R}^2$ is the load vector defined above. The steepest descent direction at a point $u$ with respect to the discrete inner product $\\langle \\cdot,\\cdot\\rangle_h$ is defined as the direction that maximizes the first-order decrease in $J$ among all directions of unit norm measured in $\\langle \\cdot,\\cdot\\rangle_h$.\n\nTasks:\n- Determine the weights $w_1$ and $w_2$ that make $\\langle \\cdot,\\cdot\\rangle_h$ a consistent mass-lumped $L^2$ inner product on this mesh.\n- Starting from the initial guess $u^{(0)}=0$, compute the ratio $\\rho(\\epsilon)=\\frac{|p^{(0)}_1|}{|p^{(0)}_2|}$ of the magnitudes of the two components of the steepest descent direction $p^{(0)}$ at the first iteration, when the steepest descent direction is taken with respect to your discrete inner product $\\langle \\cdot,\\cdot\\rangle_h$.\n\nGive your final answer for $\\rho(\\epsilon)$ as a single real number (no units). No rounding is required.",
            "solution": "The solution involves two main parts: determining the weights for the mass-lumped inner product and then computing the initial steepest descent direction.\n\n**Part 1: Determine the weights $w_1$ and $w_2$**\n\nThe discrete inner product is a mass-lumped approximation of the continuum $L^2$ inner product. For piecewise-linear finite elements, a standard way to perform mass lumping is to define each weight $w_i$ as the integral of the corresponding basis function $\\phi_i(x)$ over the entire domain.\n$$\nw_i = \\int_0^1 \\phi_i(x) dx\n$$\nThe basis function $\\phi_i(x)$ is a \"hat\" function which is 1 at node $x_i$ and 0 at all other nodes. Its integral is simply the area under the triangle, which can be calculated as half the sum of the lengths of the two elements adjacent to node $x_i$.\n\nThe mesh nodes are $x_0=0$, $x_1=\\epsilon$, $x_2=1-\\epsilon$, $x_3=1$. The element lengths are:\n- $h_1 = x_1 - x_0 = \\epsilon$\n- $h_2 = x_2 - x_1 = (1-\\epsilon) - \\epsilon = 1-2\\epsilon$\n- $h_3 = x_3 - x_2 = 1 - (1-\\epsilon) = \\epsilon$\n\nFor the weight $w_1$ at the interior node $x_1$, the adjacent elements are of length $h_1$ and $h_2$:\n$$\nw_1 = \\frac{1}{2}(h_1 + h_2) = \\frac{1}{2}(\\epsilon + (1 - 2\\epsilon)) = \\frac{1-\\epsilon}{2}\n$$\n\nFor the weight $w_2$ at the interior node $x_2$, the adjacent elements are of length $h_2$ and $h_3$:\n$$\nw_2 = \\frac{1}{2}(h_2 + h_3) = \\frac{1}{2}((1 - 2\\epsilon) + \\epsilon) = \\frac{1-\\epsilon}{2}\n$$\nSo, the weights are identical: $w_1 = w_2 = \\frac{1-\\epsilon}{2}$.\n\n**Part 2: Compute the ratio $\\rho(\\epsilon)$**\n\nThe steepest descent direction $p$ with respect to the inner product defined by the diagonal weight matrix $W = \\text{diag}(w_1, w_2)$ is given by the preconditioned gradient:\n$$\np = -W^{-1}\\nabla J(u)\n$$\nwhere $\\nabla J(u)$ is the standard Euclidean gradient of the functional $J(u)=\\frac{1}{2}u^\\top K u - b^\\top u$. The Euclidean gradient is $\\nabla J(u) = Ku - b$.\n\nWe compute the direction at the first iteration, starting from $u^{(0)} = 0$. The initial gradient is:\n$$\n\\nabla J(u^{(0)}) = K u^{(0)} - b = K(0) - b = -b\n$$\nThe initial steepest descent direction $p^{(0)}$ is therefore:\n$$\np^{(0)} = -W^{-1}(\\nabla J(u^{(0)})) = -W^{-1}(-b) = W^{-1}b\n$$\nSince $W$ is a diagonal matrix, its inverse is $W^{-1} = \\text{diag}(1/w_1, 1/w_2)$. The components of $p^{(0)}$ are:\n$$\np^{(0)}_1 = \\frac{b_1}{w_1} \\quad \\text{and} \\quad p^{(0)}_2 = \\frac{b_2}{w_2}\n$$\nNext, we compute the components of the load vector $b$. Given a constant source term $f(x) = f_0$, the components $b_i$ are:\n$$\nb_i = \\int_0^1 f_0 \\phi_i(x) dx = f_0 \\int_0^1 \\phi_i(x) dx\n$$\nFrom Part 1, we know that $\\int_0^1 \\phi_i(x) dx = w_i$. Thus, we have:\n$$\nb_1 = f_0 w_1 \\quad \\text{and} \\quad b_2 = f_0 w_2\n$$\nNow we can substitute these into the expressions for the components of $p^{(0)}$:\n$$\np^{(0)}_1 = \\frac{f_0 w_1}{w_1} = f_0\n$$\n$$\np^{(0)}_2 = \\frac{f_0 w_2}{w_2} = f_0\n$$\nFinally, we compute the required ratio $\\rho(\\epsilon)$. Since $f_0 > 0$, $|f_0|=f_0$.\n$$\n\\rho(\\epsilon) = \\frac{|p^{(0)}_1|}{|p^{(0)}_2|} = \\frac{|f_0|}{|f_0|} = \\frac{f_0}{f_0} = 1\n$$\nThe ratio is 1, independent of the mesh parameter $\\epsilon$. This result is due to the symmetry of the mesh and the constancy of the source term.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Standard least-squares optimization assumes that our forward model is perfect, which is rarely the case in practice. This hands-on problem  introduces a robust optimization framework designed to account for known bounds on modeling errors. You will first derive a closed-form expression for a \"worst-case\" misfit functional, which involves an inner maximization over all possible modeling errors up to a certain magnitude. This exercise will challenge you to think beyond simple data fitting and develop a steepest descent strategy for an objective function that explicitly incorporates and mitigates the impact of model uncertainty.",
            "id": "3617243",
            "problem": "In seismic parameter estimation, suppose one seeks to estimate a model vector $\\mathbf{m} \\in \\mathbb{R}^{n}$ from data $\\mathbf{d} \\in \\mathbb{R}^{p}$, where the predicted data are given by a differentiable forward map $F(\\mathbf{m})$. Consider a robust formulation in which the predicted data suffer an additive modeling error $\\delta F$ that is unknown but bounded in Euclidean norm by a radius $\\eta > 0$. The robust misfit is defined as\n$$\nJ(\\mathbf{m}) \\equiv \\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\frac{1}{2}\\,\\|F(\\mathbf{m}) + \\delta F - \\mathbf{d}\\|_{2}^{2}.\n$$\nStarting from the definitions of the Euclidean norm and the Cauchyâ€“Schwarz inequality, and using a first-order linearization for $F(\\mathbf{m})$ about a current iterate $\\mathbf{m}_{k}$, address the following:\n\n1. For a fixed residual $\\mathbf{r}(\\mathbf{m}) \\equiv F(\\mathbf{m}) - \\mathbf{d}$, evaluate the inner maximization over $\\delta F$ exactly and express $J(\\mathbf{m})$ in closed form in terms of $\\|\\mathbf{r}(\\mathbf{m})\\|_{2}$ and $\\eta$. Also identify a maximizer $\\delta F^{\\star}$ that attains the maximum.\n\n2. Let $J_{F}(\\mathbf{m}_{k})$ denote the Jacobian of $F$ at $\\mathbf{m}_{k}$ and consider the first-order approximation $F(\\mathbf{m}) \\approx F(\\mathbf{m}_{k}) + J_{F}(\\mathbf{m}_{k})(\\mathbf{m} - \\mathbf{m}_{k})$. Using the result from part 1 and the chain rule, derive an approximate gradient $\\nabla J(\\mathbf{m}_{k})$ in terms of $\\mathbf{r}_{k} \\equiv F(\\mathbf{m}_{k}) - \\mathbf{d}$ and $J_{F}(\\mathbf{m}_{k})$.\n\n3. Specialize to the linear identity forward map $F(\\mathbf{m}) = \\mathbf{m}$ in $\\mathbb{R}^{2}$, with current model $\\mathbf{m}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, data $\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, and modeling-error radius $\\eta = 1$. Using the gradient from part 2, form the steepest descent direction $-\\nabla J(\\mathbf{m}_{0})$ and perform an exact line search along this direction to minimize the robust objective $J(\\mathbf{m}_{0} - \\alpha \\nabla J(\\mathbf{m}_{0}))$ over $\\alpha \\geq 0$. Report the exact optimal step length $\\alpha^{\\star}$ as a single number. Do not round.",
            "solution": "### Part 1: Evaluation of the Inner Maximization\n\nWe want to maximize $L(\\delta F) = \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2}$ subject to $\\|\\delta F\\|_{2} \\leq \\eta$, where $\\mathbf{r}(\\mathbf{m}) = F(\\mathbf{m}) - \\mathbf{d}$. Maximizing $L(\\delta F)$ is equivalent to maximizing its argument:\n$$\n\\|\\mathbf{r} + \\delta F\\|_{2}^{2} = (\\mathbf{r} + \\delta F)^T (\\mathbf{r} + \\delta F) = \\|\\mathbf{r}\\|_{2}^{2} + 2 \\mathbf{r}^T \\delta F + \\|\\delta F\\|_{2}^{2}\n$$\nTo maximize this expression, we should choose $\\delta F$ to make the term $2 \\mathbf{r}^T \\delta F$ as large as possible. By the Cauchy-Schwarz inequality, $\\mathbf{r}^T \\delta F \\le \\|\\mathbf{r}\\|_{2} \\|\\delta F\\|_{2}$. The maximum is achieved when $\\delta F$ is parallel to $\\mathbf{r}$, i.e., $\\delta F = c \\mathbf{r}$ for some $c \\ge 0$.\nThe term $\\|\\delta F\\|_{2}^{2}$ is also maximized for the largest possible norm. Both terms are maximized when $\\delta F$ has the maximum allowed norm, $\\|\\delta F\\|_{2} = \\eta$, and is aligned with $\\mathbf{r}$.\nThus, the maximizing error vector is:\n$$\n\\delta F^{\\star} = \\eta \\frac{\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\n$$\nSubstituting this into the expression, we get:\n$$\n\\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = \\left\\| \\mathbf{r}(\\mathbf{m}) + \\eta \\frac{\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}} \\right\\|_{2}^{2} = \\left\\| \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right) \\mathbf{r}(\\mathbf{m}) \\right\\|_{2}^{2}\n$$\n$$\n= \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)^{2} \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} = (\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta)^{2}\n$$\nThe closed-form expression for the robust misfit is therefore:\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}(\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta)^{2} = \\frac{1}{2}(\\|F(\\mathbf{m}) - \\mathbf{d}\\|_{2} + \\eta)^{2}\n$$\n\n### Part 2: Derivation of the Approximate Gradient\n\nWe use the chain rule to find the gradient of $J(\\mathbf{m})$ at $\\mathbf{m}_k$. Let $u(\\mathbf{m}) = \\|F(\\mathbf{m}) - \\mathbf{d}\\|_{2} = \\|\\mathbf{r}(\\mathbf{m})\\|_{2}$. Then $J(\\mathbf{m}) = \\frac{1}{2}(u(\\mathbf{m}) + \\eta)^{2}$.\n$$\n\\nabla J(\\mathbf{m}) = \\frac{dJ}{du} \\nabla u(\\mathbf{m})\n$$\nThe first term is $\\frac{dJ}{du} = u(\\mathbf{m}) + \\eta = \\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta$.\nThe second term is the gradient of the Euclidean norm of the residual (assuming $\\mathbf{r}(\\mathbf{m}) \\neq \\mathbf{0}$):\n$$\n\\nabla u(\\mathbf{m}) = \\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2} = \\frac{J_{\\mathbf{r}}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\n$$\nwhere $J_{\\mathbf{r}}(\\mathbf{m}) = J_F(\\mathbf{m})$ is the Jacobian of the forward map $F$. Combining these gives:\n$$\n\\nabla J(\\mathbf{m}) = (\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta) \\frac{J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}} = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right) J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})\n$$\nEvaluating at $\\mathbf{m}_k$ with $\\mathbf{r}_k = F(\\mathbf{m}_k) - \\mathbf{d}$, we get the approximate gradient:\n$$\n\\nabla J(\\mathbf{m}_{k}) = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}_{k}\\|_{2}}\\right) J_{F}(\\mathbf{m}_{k})^{T}\\mathbf{r}_{k}\n$$\n\n### Part 3: Specific Case and Exact Line Search\n\nGiven:\n- $F(\\mathbf{m}) = \\mathbf{m}$, so the Jacobian $J_F(\\mathbf{m})$ is the identity matrix $I$.\n- $\\mathbf{m}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, $\\eta = 1$.\n\n1.  **Compute the initial residual and its norm:**\n    $$\n    \\mathbf{r}_{0} = F(\\mathbf{m}_{0}) - \\mathbf{d} = \\mathbf{m}_{0} - \\mathbf{d} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n    $$\n    $$\n    \\|\\mathbf{r}_{0}\\|_{2} = \\sqrt{2^2 + 0^2} = 2\n    $$\n\n2.  **Compute the gradient at $\\mathbf{m}_0$:**\n    $$\n    \\nabla J(\\mathbf{m}_{0}) = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}_{0}\\|_{2}}\\right) J_{F}(\\mathbf{m}_{0})^{T}\\mathbf{r}_{0} = \\left(1 + \\frac{1}{2}\\right) I^T \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\frac{3}{2}\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\n    $$\n\n3.  **Define the steepest descent direction:**\n    The direction is $\\mathbf{p} = -\\nabla J(\\mathbf{m}_{0}) = \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix}$.\n\n4.  **Perform exact line search:**\n    We need to minimize $J(\\mathbf{m}(\\alpha))$ for $\\alpha \\ge 0$, where $\\mathbf{m}(\\alpha) = \\mathbf{m}_{0} + \\alpha\\mathbf{p}$.\n    $$\n    \\mathbf{m}(\\alpha) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\alpha\\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 3\\alpha \\\\ 1 \\end{pmatrix}\n    $$\n    The residual as a function of $\\alpha$ is:\n    $$\n    \\mathbf{r}(\\alpha) = \\mathbf{m}(\\alpha) - \\mathbf{d} = \\begin{pmatrix} 1 - 3\\alpha \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 - 3\\alpha \\\\ 0 \\end{pmatrix}\n    $$\n    The norm of the residual is:\n    $$\n    \\|\\mathbf{r}(\\alpha)\\|_{2} = \\sqrt{(2 - 3\\alpha)^2 + 0^2} = |2 - 3\\alpha|\n    $$\n    The objective function to minimize over $\\alpha$ is:\n    $$\n    J(\\alpha) = \\frac{1}{2}(\\|\\mathbf{r}(\\alpha)\\|_{2} + \\eta)^2 = \\frac{1}{2}(|2 - 3\\alpha| + 1)^2\n    $$\n    Minimizing $J(\\alpha)$ for $\\alpha \\ge 0$ is equivalent to minimizing its base, $|2 - 3\\alpha| + 1$, which in turn is equivalent to minimizing $|2 - 3\\alpha|$. The minimum value of $|2 - 3\\alpha|$ is $0$. This occurs when:\n    $$\n    2 - 3\\alpha = 0 \\implies 3\\alpha = 2 \\implies \\alpha = \\frac{2}{3}\n    $$\n    Since $\\alpha = 2/3 \\ge 0$, this is the valid optimal step length.\n    $$\n    \\alpha^{\\star} = \\frac{2}{3}\n    $$",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        }
    ]
}