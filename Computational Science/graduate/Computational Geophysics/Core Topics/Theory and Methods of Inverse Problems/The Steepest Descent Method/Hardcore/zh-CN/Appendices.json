{
    "hands_on_practices": [
        {
            "introduction": "在最速下降法中，“最速”方向的定义取决于我们如何衡量向量的长度与角度，即我们选择的内积。在处理非均匀网格（例如，在有限元法中）时，标准的欧几里得内积可能导致下降方向受网格几何形状的严重影响，从而产生不符合物理直觉的结果。此练习  将引导您为非均匀网格定义一个更合适的离散内积，该内积通过“质量集中”方法从连续的 $L^2$ 内积中导出，以确保下降方向的网格无关性，从而提高算法的物理真实性和收敛效果。",
            "id": "3617265",
            "problem": "考虑计算地球物理学中代表稳定地下水流的一维扩散模型，其定义域为 $[0,1]$，在 $x=0$ 和 $x=1$ 处具有齐次 Dirichlet 边界条件。在一个不规则网格上使用连续分片线性有限元进行离散化，该网格的节点为 $x_0=0$、$x_1=\\epsilon$、$x_2=1-\\epsilon$ 和 $x_3=1$，其中 $\\epsilon \\in (0,\\tfrac{1}{2})$。单元长度为 $h_1=\\epsilon$、$h_2=1-2\\epsilon$ 和 $h_3=\\epsilon$。两个内部未知数是位于 $x_1$ 和 $x_2$ 的节点值，用向量 $u \\in \\mathbb{R}^2$ 表示。\n\n设离散内积定义为 $\\langle u,v\\rangle_h=\\sum_{i=1}^2 w_i u_i v_i$，其中 $w_i$ 是未知的正权重。你的目标是确保当 $u$ 和 $v$ 由有限元基函数近似时，$\\langle \\cdot,\\cdot\\rangle_h$ 是连续 $L^2$ 内积 $\\int_0^1 u v \\, dx$ 的一个相容近似。也就是说，离散内积必须由连续 $L^2$ 积的质量集中（mass lumping）产生。\n\n假设源项为常数 $f(x)=f_0$，且 $f_0>0$。代数方程右侧的项为 $b_i=\\int_0^1 f_0 \\,\\phi_i(x)\\,dx$，其中 $\\phi_i$ 是与两个内部节点 $x_1$ 和 $x_2$ 相关联的常用节点帽函数。考虑离散二次泛函 $J(u)=\\tfrac{1}{2}u^\\top K u - b^\\top u$，其中 $K$ 是由有限元离散化产生的对称正定刚度矩阵，$b\\in\\mathbb{R}^2$ 是如上定义的载荷向量。在点 $u$ 处，关于离散内积 $\\langle \\cdot,\\cdot\\rangle_h$ 的最速下降方向定义为：在所有以 $\\langle \\cdot,\\cdot\\rangle_h$ 衡量的单位范数方向中，使 $J$ 的一阶减小量最大的方向。\n\n任务：\n- 确定权重 $w_1$ 和 $w_2$，使得 $\\langle \\cdot,\\cdot\\rangle_h$ 在此网格上是相容的质量集中 $L^2$ 内积。\n- 从初始猜测 $u^{(0)}=0$ 开始，计算第一次迭代时最速下降方向 $p^{(0)}$ 的两个分量大小之比 $\\rho(\\epsilon)=\\frac{|p^{(0)}_1|}{|p^{(0)}_2|}$，其中最速下降方向是相对于你所求的离散内积 $\\langle \\cdot,\\cdot\\rangle_h$ 来确定的。\n\n给出 $\\rho(\\epsilon)$ 的最终答案，为一个实数（无单位）。无需四舍五入。",
            "solution": "用户需要一个计算地球物理学问题的详细解答。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- **模型：** 用于稳定地下水流的一维扩散模型。\n- **定义域：** $[0,1]$。\n- **边界条件 (BCs)：** 在 $x=0$ 和 $x=1$ 处的齐次 Dirichlet 条件。\n- **离散化：** 连续分片线性有限元。\n- **网格节点：** $x_0=0$, $x_1=\\epsilon$, $x_2=1-\\epsilon$, $x_3=1$，其中 $\\epsilon \\in (0,\\frac{1}{2})$。\n- **单元长度：** $h_1=\\epsilon$, $h_2=1-2\\epsilon$, $h_3=\\epsilon$。\n- **未知数：** $u = [u_1, u_2]^\\top \\in \\mathbb{R}^2$，代表内部节点 $x_1$ 和 $x_2$ 处的节点值。\n- **离散内积：** $\\langle u,v\\rangle_h=\\sum_{i=1}^2 w_i u_i v_i$，其中 $w_i$ 是未知的正权重。这必须是连续 $L^2$ 内积 $\\int_0^1 u v \\, dx$ 的相容质量集中近似。\n- **源项：** $f(x)=f_0$，一个常数，且 $f_0>0$。\n- **载荷向量：** $b_i=\\int_0^1 f_0 \\,\\phi_i(x)\\,dx$，其中 $\\phi_i$ 是节点 $x_1$ 和 $x_2$ 的节点帽函数。\n- **泛函：** $J(u)=\\frac{1}{2}u^\\top K u - b^\\top u$，其中 $K$ 是对称正定刚度矩阵。\n- **最速下降方向：** $p$ 在所有以 $\\langle \\cdot,\\cdot\\rangle_h$ 内积衡量的单位范数方向中，使 $J$ 的一阶减小量最大。\n- **初始猜测：** $u^{(0)}=0$。\n- **任务：**\n  1. 确定权重 $w_1$ 和 $w_2$。\n  2. 计算第一次迭代时最速下降方向 $p^{(0)}$ 的比率 $\\rho(\\epsilon)=\\frac{|p^{(0)}_1|}{|p^{(0)}_2|}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题在科学上和数学上都是合理的。它描述了将有限元方法（FEM）应用于二阶边值问题以及使用优化程序（最速下降法）求解所得线性系统的标准应用。\n\n- **科学基础：** 该问题使用了有限元方法、数值线性代数和优化的既定原则。一维扩散方程是包括地球物理学在内的许多 STEM 领域的基础模型。\n- **适定性：** 该问题提供了确定权重和指定比率所需的所有信息。质量集中和使用广义内积的最速下降法的概念是明确定义的。\n- **客观性：** 问题陈述使用了精确的数学语言，没有主观性或歧义。\n- **完整性：** 问题是自洽的。网格、基函数（隐含为标准的线性“帽”函数）、内积和目标泛函的定义是充分的。\n- **一致性：** 给定信息中没有矛盾之处。条件 $\\epsilon \\in (0, \\frac{1}{2})$ 确保所有单元长度为正，并且节点顺序得以保持（$x_0  x_1  x_2  x_3$）。\n\n该问题不违反任何无效性标准。尽管由于问题的对称性，最终答案很简单，但其推导过程需要正确应用数值分析中的几个非平凡概念。\n\n**步骤 3：结论与行动**\n该问题是**有效的**。将提供完整的解答。\n\n### 解答\n\n问题分为两部分：首先，确定离散内积的权重 $w_1$ 和 $w_2$；其次，计算初始最速下降方向分量的比率。\n\n**第一部分：确定权重 $w_1$ 和 $w_2$**\n\n离散内积 $\\langle u,v\\rangle_h = \\sum_{i=1}^2 w_i u_i v_i$ 被指定为连续 $L^2$ 内积 $\\int_0^1 u(x)v(x)dx$ 的相容质量集中近似。在有限元背景下，函数 $u(x)$近似为 $u(x) \\approx \\sum_j u_j \\phi_j(x)$。连续内积变为：\n$$ \\int_0^1 u(x)v(x)dx \\approx \\int_0^1 \\left(\\sum_i u_i \\phi_i(x)\\right) \\left(\\sum_j v_j \\phi_j(x)\\right) dx = \\sum_{i,j} u_i v_j \\int_0^1 \\phi_i(x)\\phi_j(x)dx = u^\\top M v $$\n其中 $M$ 是一致质量矩阵，其元素为 $M_{ij} = \\int_0^1 \\phi_i(x)\\phi_j(x)dx$。\n\n质量集中用对角矩阵 $M_L$ 近似完整矩阵 $M$。离散内积是根据这个集中矩阵定义的，因此 $u^\\top W v = u^\\top M_L v$，意味着 $W=M_L$ 且 $w_i = (M_L)_{ii}$。\n\n对于连续分片线性单元，一种标准的、有物理动机且广泛使用的质量集中方法是将对角线元素定义为相应基函数的积分。这等效于使用节点求积规则来近似定义质量矩阵的积分。\n$$ w_i = \\int_0^1 \\phi_i(x) dx $$\n在几何上，这个积分代表了“帽”基函数 $\\phi_i(x)$ 下的面积。与节点 $x_i$ 相关联的帽函数 $\\phi_i$ 在区间 $(x_{i-1}, x_{i+1})$ 上非零，并由两个三角形部分组成，这两个部分分别位于长度为 $h_{i} = x_i-x_{i-1}$ 和 $h_{i+1} = x_{i+1}-x_i$ 的单元上。面积是这两个三角形面积之和，每个三角形的高都为1：\n$$ w_i = \\frac{1}{2} \\times (\\text{base}_1) \\times 1 + \\frac{1}{2} \\times (\\text{base}_2) \\times 1 = \\frac{1}{2}(h_{i} + h_{i+1}) $$\n网格节点为 $x_0=0$, $x_1=\\epsilon$, $x_2=1-\\epsilon$ 和 $x_3=1$。单元长度为 $h_1=x_1-x_0=\\epsilon$, $h_2=x_2-x_1=1-2\\epsilon$ 和 $h_3=x_3-x_2=\\epsilon$。\n\n对于内部节点 $x_1$ 处的权重 $w_1$：\n相邻的单元是单元1（长度 $h_1=\\epsilon$）和单元2（长度 $h_2=1-2\\epsilon$）。\n$$ w_1 = \\int_0^1 \\phi_1(x) dx = \\frac{1}{2}(h_1 + h_2) = \\frac{1}{2}(\\epsilon + (1-2\\epsilon)) = \\frac{1-\\epsilon}{2} $$\n\n对于内部节点 $x_2$ 处的权重 $w_2$：\n相邻的单元是单元2（长度 $h_2=1-2\\epsilon$）和单元3（长度 $h_3=\\epsilon$）。\n$$ w_2 = \\int_0^1 \\phi_2(x) dx = \\frac{1}{2}(h_2 + h_3) = \\frac{1}{2}((1-2\\epsilon) + \\epsilon) = \\frac{1-\\epsilon}{2} $$\n因此，权重为 $w_1 = w_2 = \\frac{1-\\epsilon}{2}$。\n\n**第二部分：计算比率 $\\rho(\\epsilon)$**\n\n最速下降法旨在最小化泛函 $J(u) = \\frac{1}{2}u^\\top K u - b^\\top u$。下降方向 $p$ 被选为在归一化约束下使下降幅度最大的方向。$J(u)$ 关于标准欧几里得内积的梯度是 $\\nabla J(u) = Ku - b$。\n\n在点 $u$ 处，关于由对角矩阵 $W = \\text{diag}(w_1, w_2)$ 定义的内积 $\\langle \\cdot, \\cdot \\rangle_h$ 的最速下降方向由负预处理梯度给出：\n$$ p = -W^{-1} \\nabla J(u) $$\n我们从初始猜测 $u^{(0)}=0$ 开始。此时的梯度是：\n$$ g^{(0)} = \\nabla J(u^{(0)}) = K u^{(0)} - b = K(0) - b = -b $$\n因此，第一次迭代的最速下降方向 $p^{(0)}$ 是：\n$$ p^{(0)} = -W^{-1}g^{(0)} = -W^{-1}(-b) = W^{-1}b $$\n由于 $W$ 是一个对角矩阵，$W = \\begin{pmatrix} w_1  0 \\\\ 0  w_2 \\end{pmatrix}$，其逆矩阵为 $W^{-1} = \\begin{pmatrix} 1/w_1  0 \\\\ 0  1/w_2 \\end{pmatrix}$。\n方向向量 $p^{(0)}$ 的分量是：\n$$ p^{(0)}_1 = \\frac{b_1}{w_1} \\quad \\text{和} \\quad p^{(0)}_2 = \\frac{b_2}{w_2} $$\n接下来，我们计算载荷向量 $b$ 的分量。定义为 $b_i = \\int_0^1 f(x) \\phi_i(x) dx$。鉴于源项为常数 $f(x)=f_0$：\n$$ b_i = \\int_0^1 f_0 \\phi_i(x) dx = f_0 \\int_0^1 \\phi_i(x) dx $$\n从第一部分，我们已经确定 $\\int_0^1 \\phi_i(x) dx = w_i$。因此，载荷向量分量和权重之间存在直接关系：\n$$ b_1 = f_0 w_1 \\quad \\text{和} \\quad b_2 = f_0 w_2 $$\n现在我们可以计算最速下降方向 $p^{(0)}$ 的分量：\n$$ p^{(0)}_1 = \\frac{b_1}{w_1} = \\frac{f_0 w_1}{w_1} = f_0 $$\n$$ p^{(0)}_2 = \\frac{b_2}{w_2} = \\frac{f_0 w_2}{w_2} = f_0 $$\n问题要求计算比率 $\\rho(\\epsilon) = \\frac{|p^{(0)}_1|}{|p^{(0)}_2|}$。\n$$ \\rho(\\epsilon) = \\frac{|f_0|}{|f_0|} $$\n鉴于 $f_0 > 0$，我们有 $|f_0| = f_0$。\n$$ \\rho(\\epsilon) = \\frac{f_0}{f_0} = 1 $$\n这个结果与 $\\epsilon$ 无关。网格的对称性（$h_1=h_3$）和源项的对称性（$f(x)$ 是常数）导致了权重的对称性（$w_1=w_2$）和载荷向量分量的对称性（$b_1=b_2$），这又导致了初始最速下降方向的分量相同。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "确定了下降方向后，下一步是选择合适的步长。回溯线搜索是一种确保足够下降的常用策略，但其效率依赖于初始步长的选择。一个好的初始步长通常与目标函数梯度的利普希茨常数 $L$ 的倒数成正比，然而直接计算 $L$（即Hessian矩阵的最大特征值）在计算上可能非常昂贵。此编程练习  旨在通过一种实用的方式，利用回溯线搜索过程本身的信息来动态估计 $L$ 的值，从而让算法能够自适应地调整步长，这是构建高效优化器的关键一步。",
            "id": "3617208",
            "problem": "考虑一个简化的时域全波形反演（FWI）一维线性化模型，其目标函数是一个凸最小二乘泛函，定义为\n$$\nJ(\\mathbf{x}) \\equiv \\tfrac{1}{2}\\,\\lVert A(\\mathcal{E},h)\\,\\mathbf{x} - \\mathbf{b}\\rVert_2^2,\n$$\n其中 $\\mathbf{x}\\in\\mathbb{R}^N$ 是模型向量，$\\mathbf{b}\\in\\mathbb{R}^{M}$ 是数据向量，$A(\\mathcal{E},h)\\in\\mathbb{R}^{M\\times N}$ 是一个线性正演算子，它依赖于波场能量尺度 $\\mathcal{E}>0$ 和空间网格分辨率 $h>0$。在此问题中，映射 $A(\\mathcal{E},h)$ 被指定为一个按比例缩放的前向差分算子\n$$\nA(\\mathcal{E},h) \\equiv \\mathcal{E}\\,D_h,\n$$\n其中 $D_h\\in\\mathbb{R}^{(N-1)\\times N}$ 是一阶前向有限差分矩阵，其元素为\n$$\n(D_h)_{i,i} = -\\frac{1}{h},\\quad (D_h)_{i,i+1}=\\frac{1}{h},\\quad \\text{for } i=1,\\dots,N-1,\n$$\n且所有其他元素均为零。这种选择编码了数据对模型空间梯度的基本敏感性，其中通过 $\\mathcal{E}$ 进行的缩放代表了波场能量。假设通过该模板隐式地施加了齐次狄利克雷条件，并且空间域是单位区间，因此 $h=\\tfrac{1}{N+1}$。\n\n您需要使用一次带有回溯线搜索的最速下降迭代来估计梯度 $\\nabla J$ 的利普希茨常数 $L$，该线搜索强制执行 Armijo 充分下降条件。具体来说，从 $\\mathbf{x}_0=\\mathbf{0}\\in\\mathbb{R}^N$ 开始，下降方向为 $\\mathbf{p}_0=-\\nabla J(\\mathbf{x}_0)$，使用回溯序列 $\\alpha_k=\\beta^k\\alpha_0$，其中初始试探步长 $\\alpha_0>0$，缩减因子 $\\beta\\in(0,1)$，以及 Armijo 参数 $c\\in(0,1)$，以找到第一个满足以下条件的可接受步长 $\\alpha_{\\mathrm{acc}}$：\n$$\nJ(\\mathbf{x}_0+\\alpha_{\\mathrm{acc}}\\,\\mathbf{p}_0)\\le J(\\mathbf{x}_0)+c\\,\\alpha_{\\mathrm{acc}}\\,\\nabla J(\\mathbf{x}_0)^\\top \\mathbf{p}_0.\n$$\n仅使用针对具有利普希茨连续梯度函数的标准下降引理和上述 Armijo 不等式，根据观测到的量 $J(\\mathbf{x}_0)$、$J(\\mathbf{x}_1)$（其中 $\\mathbf{x}_1=\\mathbf{x}_0+\\alpha_{\\mathrm{acc}}\\mathbf{p}_0$）、范数 $\\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2$、可接受的步长 $\\alpha_{\\mathrm{acc}}$ 和参数 $c$，构造一个可计算的 $L$ 的下界和上界。通过取下界和上界的几何平均值，将这两个界合并为一个标量估计值 $\\widehat{L}$。必须使用几何平均值。\n\n对于每个测试用例，通过 $(\\mathbf{x}_\\star)_i=\\sin(\\pi i h)$（其中 $i=1,\\dots,N$）定义真实模型，将数据设置为 $\\mathbf{b}=A(\\mathcal{E},h)\\,\\mathbf{x}_\\star$，并使用 $\\mathbf{x}_0=\\mathbf{0}$。所有用例均使用相同的回溯配置，即 $\\alpha_0=1$，$\\beta=\\tfrac{1}{2}$，以及 $c=10^{-4}$。\n\n您的程序必须实现以下测试套件：\n- 用例 1：$(\\mathcal{E},N)=(1.0,64)$。\n- 用例 2：$(\\mathcal{E},N)=(2.0,64)$。\n- 用例 3：$(\\mathcal{E},N)=(1.0,128)$。\n- 用例 4：$(\\mathcal{E},N)=(0.1,64)$。\n- 用例 5：$(\\mathcal{E},N)=(1.0,16)$。\n\n对于每个用例，如上所述估计 $\\widehat{L}$。您的程序应生成单行输出，其中包含一个逗号分隔的浮点数列表，该列表用方括号括起来，按上述用例的顺序排列，每个浮点数四舍五入到 $6$ 位小数（例如，$[x_1,x_2,x_3,x_4,x_5]$）。输出中不需要角度或物理单位；报告纯数字。\n\n您的实现应纯粹是数学上的，并且无需任何外部输入即可复现。它必须遵守上述数值定义和过程，除了最速下降、Armijo 回溯和利普希茨连续梯度的基本定义外，不使用任何未指定的公式。通过尊重波场能量 $\\mathcal{E}$、网格分辨率 $h$ 和在 $A(\\mathcal{E},h)$ 中引发的灵敏度之间的关系，确保数值稳定性和科学真实性。",
            "solution": "该问题经评估为有效，因为它在数学上和科学上都是合理的，是适定的，并且解决该问题所需的所有信息都已提供。任务是利用一次带有 Armijo 回溯线搜索的最速下降迭代所产生的数据，来估计一个给定二次目标函数梯度的利普希茨常数。\n\n设目标函数为 $J(\\mathbf{x}) = \\frac{1}{2}\\lVert A\\mathbf{x} - \\mathbf{b}\\rVert_2^2$。由于这是一个二次函数，其梯度 $\\nabla J(\\mathbf{x})$ 和黑塞矩阵 $\\nabla^2 J(\\mathbf{x})$ 很容易推导。\n梯度由以下公式给出：\n$$\n\\nabla J(\\mathbf{x}) = A^\\top(A\\mathbf{x} - \\mathbf{b})\n$$\n对于所有 $\\mathbf{x}$，黑塞矩阵是常数：\n$$\n\\nabla^2 J(\\mathbf{x}) = A^\\top A\n$$\n梯度 $\\nabla J(\\mathbf{x})$ 的利普希茨常数 $L$ 定义为满足 $\\lVert \\nabla J(\\mathbf{y}) - \\nabla J(\\mathbf{x})\\rVert_2 \\le L \\lVert \\mathbf{y} - \\mathbf{x}\\rVert_2$ 对所有 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^N$ 成立的最小值 $L$。对于二次函数，这等价于黑塞矩阵的算子范数（谱范数）：\n$$\nL = \\lVert \\nabla^2 J \\rVert_2 = \\lVert A^\\top A \\rVert_2 = \\lambda_{\\max}(A^\\top A)\n$$\n其中 $\\lambda_{\\max}(A^\\top A)$ 是矩阵 $A^\\top A$ 的最大特征值。\n\n该问题要求从 $\\mathbf{x}_0 = \\mathbf{0}$ 开始的一次最速下降法迭代中推导出 $L$ 的估计值。更新公式为 $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_{\\mathrm{acc}} \\mathbf{p}_0$，其中下降方向为 $\\mathbf{p}_0 = -\\nabla J(\\mathbf{x}_0)$，$\\alpha_{\\mathrm{acc}}$ 是通过回溯线搜索找到的步长。\n\n在初始点 $\\mathbf{x}_0 = \\mathbf{0}$，我们有：\n$$\nJ(\\mathbf{x}_0) = \\frac{1}{2}\\lVert -\\mathbf{b} \\rVert_2^2 = \\frac{1}{2}\\lVert \\mathbf{b} \\rVert_2^2\n$$\n$$\n\\nabla J(\\mathbf{x}_0) = A^\\top(A\\mathbf{0} - \\mathbf{b}) = -A^\\top\\mathbf{b}\n$$\n$$\n\\mathbf{p}_0 = -\\nabla J(\\mathbf{x}_0) = A^\\top\\mathbf{b}\n$$\n在 $\\mathbf{x}_0$ 沿 $\\mathbf{p}_0$ 方向的方向导数为 $\\nabla J(\\mathbf{x}_0)^\\top \\mathbf{p}_0 = -\\nabla J(\\mathbf{x}_0)^\\top \\nabla J(\\mathbf{x}_0) = -\\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2$。\n\n$L$ 的估计值将是下界和上界的几何平均值，即 $\\widehat{L} = \\sqrt{L_{\\mathrm{lower}} L_{\\mathrm{upper}}}$。我们现在推导这两个界。\n\n**1. 下界 ($L_{\\mathrm{lower}}$)**\n\n对于具有 $L$-利普希茨连续梯度的函数，其下降引理指出：\n$$\nJ(\\mathbf{y}) \\le J(\\mathbf{x}) + \\nabla J(\\mathbf{x})^\\top(\\mathbf{y}-\\mathbf{x}) + \\frac{L}{2}\\lVert \\mathbf{y}-\\mathbf{x}\\rVert_2^2\n$$\n令 $\\mathbf{x} = \\mathbf{x}_0$ 和 $\\mathbf{y} = \\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_{\\mathrm{acc}}\\mathbf{p}_0$，我们有 $\\mathbf{y}-\\mathbf{x} = \\alpha_{\\mathrm{acc}}\\mathbf{p}_0$。将此代入引理：\n$$\nJ(\\mathbf{x}_1) \\le J(\\mathbf{x}_0) + \\alpha_{\\mathrm{acc}} \\nabla J(\\mathbf{x}_0)^\\top \\mathbf{p}_0 + \\frac{L}{2} \\alpha_{\\mathrm{acc}}^2 \\lVert \\mathbf{p}_0 \\rVert_2^2\n$$\n使用 $\\mathbf{p}_0 = -\\nabla J(\\mathbf{x}_0)$，上式变为：\n$$\nJ(\\mathbf{x}_1) \\le J(\\mathbf{x}_0) - \\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0) \\rVert_2^2 + \\frac{L}{2} \\alpha_{\\mathrm{acc}}^2 \\lVert \\nabla J(\\mathbf{x}_0) \\rVert_2^2\n$$\n重排此不等式以求解 $L$，可得到 $L$ 的一个下界：\n$$\nL \\ge \\frac{2(J(\\mathbf{x}_1) - J(\\mathbf{x}_0) + \\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2)}{\\alpha_{\\mathrm{acc}}^2 \\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2}\n$$\n我们将我们的下界定义为这个可以从观测值计算出的量：\n$$\nL_{\\mathrm{lower}} = \\frac{2(J(\\mathbf{x}_1) - J(\\mathbf{x}_0) + \\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2)}{\\alpha_{\\mathrm{acc}}^2 \\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2}\n$$\n对于二次目标函数，此下界恰好是黑塞矩阵 $A^\\top A$ 关于梯度向量 $\\nabla J(\\mathbf{x}_0)$ 的瑞利商，这是 $L = \\lambda_{\\max}(A^\\top A)$ 的一个严格下界。\n\n**2. 上界 ($L_{\\mathrm{upper}}$)**\n\nArmijo 条件要求可接受的步长 $\\alpha_{\\mathrm{acc}}$ 满足：\n$$\nJ(\\mathbf{x}_1) \\le J(\\mathbf{x}_0) + c\\, \\alpha_{\\mathrm{acc}} \\nabla J(\\mathbf{x}_0)^\\top \\mathbf{p}_0\n$$\n$$\nJ(\\mathbf{x}_0 + \\alpha_{\\mathrm{acc}}\\mathbf{p}_0) \\le J(\\mathbf{x}_0) - c\\, \\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0) \\rVert_2^2\n$$\n对于我们的二次函数，我们可以使用围绕 $\\mathbf{x}_0$ 的泰勒展开精确地表示 $J(\\mathbf{x}_0 + \\alpha\\mathbf{p}_0)$：\n$$\nJ(\\mathbf{x}_0 + \\alpha\\mathbf{p}_0) = J(\\mathbf{x}_0) + \\alpha \\nabla J(\\mathbf{x}_0)^\\top \\mathbf{p}_0 + \\frac{\\alpha^2}{2} \\mathbf{p}_0^\\top (\\nabla^2 J) \\mathbf{p}_0\n$$\n代入 $\\mathbf{p}_0 = -\\nabla J(\\mathbf{x}_0)$ 和 $\\nabla^2 J=A^\\top A$：\n$$\nJ(\\mathbf{x}_0 + \\alpha\\mathbf{p}_0) = J(\\mathbf{x}_0) - \\alpha \\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2 + \\frac{\\alpha^2}{2} \\nabla J(\\mathbf{x}_0)^\\top (A^\\top A) \\nabla J(\\mathbf{x}_0)\n$$\n将此代入 $\\alpha=\\alpha_{\\mathrm{acc}}$ 时的 Armijo 条件：\n$$\nJ(\\mathbf{x}_0) - \\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0)\\rVert_2^2 + \\frac{\\alpha_{\\mathrm{acc}}^2}{2} \\mathbf{p}_0^\\top (A^\\top A) \\mathbf{p}_0 \\le J(\\mathbf{x}_0) - c\\, \\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0) \\rVert_2^2\n$$\n化简并重排得到：\n$$\n\\frac{\\alpha_{\\mathrm{acc}}^2}{2} \\mathbf{p}_0^\\top (A^\\top A) \\mathbf{p}_0 \\le (1-c)\\alpha_{\\mathrm{acc}} \\lVert \\nabla J(\\mathbf{x}_0) \\rVert_2^2 = (1-c)\\alpha_{\\mathrm{acc}} \\lVert \\mathbf{p}_0 \\rVert_2^2\n$$\n两边同除以 $\\frac{\\alpha_{\\mathrm{acc}}}{2}\\lVert \\mathbf{p}_0 \\rVert_2^2$ 得到瑞利商 $R = \\frac{\\mathbf{p}_0^\\top (A^\\top A) \\mathbf{p}_0}{\\lVert \\mathbf{p}_0 \\rVert_2^2}$ 的一个上界：\n$$\nR \\le \\frac{2(1-c)}{\\alpha_{\\mathrm{acc}}}\n$$\n虽然这是沿最速下降方向曲率的一个上界，但它不是 $L = \\lambda_{\\max}(A^\\top A)$ 的一个严格上界。然而，这是从 Armijo 条件推导出的一个标准启发式估计，考虑到问题的约束，我们将其作为我们估计程序的上界：\n$$\nL_{\\mathrm{upper}} = \\frac{2(1-c)}{\\alpha_{\\mathrm{acc}}}\n$$\n\n**3. 最终估计 ($\\widehat{L}$)**\n\n$L$ 的最终估计值是下界和上界的几何平均值：\n$$\n\\widehat{L} = \\sqrt{L_{\\mathrm{lower}} \\cdot L_{\\mathrm{upper}}}\n$$\n\n每个测试用例的步骤如下：\n- 对于给定的 $(\\mathcal{E},N)$，定义 $h=\\frac{1}{N+1}$、真实模型 $\\mathbf{x}_\\star$、矩阵 $A(\\mathcal{E},h)$ 和数据向量 $\\mathbf{b}=A\\mathbf{x}_\\star$。\n- 设置 $\\mathbf{x}_0=\\mathbf{0}$ 并计算 $J(\\mathbf{x}_0)$ 和 $\\nabla J(\\mathbf{x}_0)$。\n- 使用参数 $\\alpha_0=1$, $\\beta=0.5$, $c=10^{-4}$ 执行回溯，找到可接受的步长 $\\alpha_{\\mathrm{acc}}$。\n- 计算 $J(\\mathbf{x}_1)$，其中 $\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha_{\\mathrm{acc}}\\nabla J(\\mathbf{x}_0)$。\n- 使用推导出的公式计算 $L_{\\mathrm{lower}}$ 和 $L_{\\mathrm{upper}}$。\n- 计算估计值 $\\widehat{L}$ 并存储它。\n然后收集并打印所有用例的结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating the Lipschitz constant L for several test cases.\n    \"\"\"\n    test_cases = [\n        (1.0, 64),  # Case 1\n        (2.0, 64),  # Case 2\n        (1.0, 128), # Case 3\n        (0.1, 64),  # Case 4\n        (1.0, 16),  # Case 5\n    ]\n\n    results = []\n    for E_val, N_val in test_cases:\n        # Backtracking parameters\n        alpha0 = 1.0\n        beta = 0.5\n        c = 1e-4\n\n        # Model and data setup\n        h = 1.0 / (N_val + 1)\n        \n        # Construct the finite-difference matrix D_h\n        D_h = np.zeros((N_val - 1, N_val))\n        # This can be done more efficiently with sparse matrices for large N,\n        # but for N up to 128, a dense matrix is acceptable.\n        for i in range(N_val - 1):\n            D_h[i, i] = -1.0 / h\n            D_h[i, i+1] = 1.0 / h\n        \n        # Construct the forward operator A\n        A = E_val * D_h\n        \n        # Define the true model x_star\n        i_vals = np.arange(1, N_val + 1)\n        x_star = np.sin(np.pi * i_vals * h)\n        \n        # Generate the data vector b\n        b = A @ x_star\n        \n        # Set initial guess x0 = 0\n        x0 = np.zeros(N_val)\n        \n        # Calculate quantities at the initial point x0\n        # J(x0) = 0.5 * ||A*x0 - b||^2 = 0.5 * ||-b||^2\n        J_x0 = 0.5 * np.linalg.norm(b)**2\n        \n        # grad J(x0) = A^T * (A*x0 - b) = -A^T * b\n        grad_J_x0 = -A.T @ b\n        grad_J_x0_norm_sq = np.dot(grad_J_x0, grad_J_x0)\n\n        # Descent direction p0 = -grad J(x0)\n        p0 = -grad_J_x0\n        \n        # Directional derivative at x0: grad_J_x0.T @ p0 = -||grad_J_x0||^2\n        dir_deriv = -grad_J_x0_norm_sq\n\n        # Perform backtracking line search to find alpha_acc\n        alpha = alpha0\n        while True:\n            # Candidate point\n            x_candidate = x0 + alpha * p0\n            \n            # Evaluate objective function at the candidate point\n            residual = A @ x_candidate - b\n            J_candidate = 0.5 * np.dot(residual, residual)\n            \n            # Check the Armijo sufficient decrease condition\n            if J_candidate = J_x0 + c * alpha * dir_deriv:\n                alpha_acc = alpha\n                J_x1 = J_candidate\n                break\n            \n            # Reduce step size\n            alpha *= beta\n\n            # Safety break to prevent potential infinite loops with extreme parameters\n            if alpha  1e-16:\n                alpha_acc = alpha\n                J_x1 = J_candidate\n                break\n\n        # Calculate lower and upper bounds for the Lipschitz constant L\n        \n        # Lower bound\n        numerator_lower = 2.0 * (J_x1 - J_x0 + alpha_acc * grad_J_x0_norm_sq)\n        denominator_lower = alpha_acc**2 * grad_J_x0_norm_sq\n        \n        # Handle the case where the initial gradient is zero (i.e., we are at the minimum)\n        if denominator_lower  1e-12: \n             L_lower = 0.0\n        else:\n             L_lower = numerator_lower / denominator_lower\n        \n        # Upper bound\n        L_upper = 2.0 * (1.0 - c) / alpha_acc\n        \n        # The estimated L is the geometric mean of the bounds\n        # Ensure arguments to sqrt are non-negative\n        if L_lower  0: L_lower = 0 \n        L_hat = np.sqrt(L_lower * L_upper)\n        \n        results.append(round(L_hat, 6))\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```"
        },
        {
            "introduction": "经典的优化方法通常假定我们的正演模型 $F(\\mathbf{m})$ 是完全准确的，但这在现实世界中很少成立。模型本身的不确定性是地球物理反演中的一个重要误差来源。此练习  介绍了一种鲁棒优化的思想，它通过最小化一个“最坏情况”下的目标函数来显式地处理有界的模型误差。您将学习如何构建这个鲁棒目标函数，推导其梯度，并将其应用于一个具体问题中，最终得到一个对模型不确定性具有更强韧性的最速下降算法。",
            "id": "3617243",
            "problem": "在地震参数估计中，假设我们希望从数据 $\\mathbf{d} \\in \\mathbb{R}^{p}$ 估计一个模型向量 $\\mathbf{m} \\in \\mathbb{R}^{n}$，其中预测数据由一个可微的正演映射 $F(\\mathbf{m})$ 给出。考虑一个稳健的公式，其中预测数据存在一个附加的建模误差 $\\delta F$，该误差未知，但在欧几里得范数下受一个半径 $\\eta  0$ 的限制。稳健失配定义为\n$$\nJ(\\mathbf{m}) \\equiv \\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\frac{1}{2}\\,\\|F(\\mathbf{m}) + \\delta F - \\mathbf{d}\\|_{2}^{2}.\n$$\n从欧几里得范数和柯西-施瓦茨不等式的定义出发，并使用 $F(\\mathbf{m})$ 关于当前迭代点 $\\mathbf{m}_{k}$ 的一阶线性化，解决以下问题：\n\n1. 对于一个固定的残差 $\\mathbf{r}(\\mathbf{m}) \\equiv F(\\mathbf{m}) - \\mathbf{d}$，精确求解关于 $\\delta F$ 的内部最大化问题，并用 $\\|\\mathbf{r}(\\mathbf{m})\\|_{2}$ 和 $\\eta$ 表示 $J(\\mathbf{m})$ 的闭合形式。同时，找出达到最大值的最大化子 $\\delta F^{\\star}$。\n\n2. 设 $J_{F}(\\mathbf{m}_{k})$ 表示 $F$ 在 $\\mathbf{m}_{k}$ 处的雅可比矩阵，并考虑一阶近似 $F(\\mathbf{m}) \\approx F(\\mathbf{m}_{k}) + J_{F}(\\mathbf{m}_{k})(\\mathbf{m} - \\mathbf{m}_{k})$。使用第1部分的结果和链式法则，用 $\\mathbf{r}_{k} \\equiv F(\\mathbf{m}_{k}) - \\mathbf{d}$ 和 $J_{F}(\\mathbf{m}_{k})$ 推导出近似梯度 $\\nabla J(\\mathbf{m}_{k})$。\n\n3. 特化为 $\\mathbb{R}^{2}$ 中的线性恒等正演映射 $F(\\mathbf{m}) = \\mathbf{m}$，其中当前模型为 $\\mathbf{m}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，数据为 $\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$，建模误差半径为 $\\eta = 1$。使用第2部分得到的梯度，构造最速下降方向 $-\\nabla J(\\mathbf{m}_{0})$，并沿此方向进行精确线搜索，以最小化稳健目标函数 $J(\\mathbf{m}_{0} - \\alpha \\nabla J(\\mathbf{m}_{0}))$（其中 $\\alpha \\geq 0$）。将精确的最优步长 $\\alpha^{\\star}$ 报告为一个单独的数字。不要四舍五入。",
            "solution": "该问题被验证为内容完整，其科学基础是优化理论和线性代数，并且是适定的。所有需要的数据和定义均已提供。该问题是可形式化且客观的。我现在将给出完整解答。\n\n该问题分为三个部分。我们将按顺序解答。\n\n### 第1部分：内部最大化求值\n\n稳健失配函数定义为\n$$\nJ(\\mathbf{m}) \\equiv \\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\frac{1}{2}\\,\\|F(\\mathbf{m}) + \\delta F - \\mathbf{d}\\|_{2}^{2}\n$$\n其中 $\\eta > 0$。对于一个固定的模型向量 $\\mathbf{m}$，我们定义残差向量为 $\\mathbf{r}(\\mathbf{m}) \\equiv F(\\mathbf{m}) - \\mathbf{d}$。需要对 $\\delta F$ 最大化的表达式是 $\\frac{1}{2}\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2}$。最大化此表达式等价于最大化其中的欧几里得范数的平方项 $\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2}$。\n\n我们展开该平方范数：\n$$\n\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = (\\mathbf{r}(\\mathbf{m}) + \\delta F)^{T}(\\mathbf{r}(\\mathbf{m}) + \\delta F) = \\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}) + 2\\mathbf{r}(\\mathbf{m})^{T}\\delta F + \\delta F^{T}\\delta F\n$$\n这可以写成：\n$$\n\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} + 2\\mathbf{r}(\\mathbf{m})^{T}\\delta F + \\|\\delta F\\|_{2}^{2}\n$$\n对于固定的 $\\mathbf{m}$，项 $\\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2}$ 是一个常数。为了最大化该表达式，我们必须在约束 $\\|\\delta F\\|_{2} \\leq \\eta$ 下选择 $\\delta F$。\n\n根据柯西-施瓦茨不等式，项 $\\mathbf{r}(\\mathbf{m})^{T}\\delta F$ 是有界的：\n$$\n|\\mathbf{r}(\\mathbf{m})^{T}\\delta F| \\leq \\|\\mathbf{r}(\\mathbf{m})\\|_{2} \\|\\delta F\\|_{2}\n$$\n当 $\\delta F$ 与 $\\mathbf{r}(\\mathbf{m})$ 共线且同向时，$\\mathbf{r}(\\mathbf{m})^{T}\\delta F$ 达到其最大值 $\\|\\mathbf{r}(\\mathbf{m})\\|_{2} \\|\\delta F\\|_{2}$。也就是说，$\\delta F$ 必须具有 $\\delta F = c \\mathbf{r}(\\mathbf{m})$ 的形式，其中 $c \\geq 0$ 是某个标量。\n\n我们来分析待最大化的函数 $f(\\delta F) = \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} + 2\\mathbf{r}(\\mathbf{m})^{T}\\delta F + \\|\\delta F\\|_{2}^{2}$。当 $\\delta F$ 被选择为与 $\\mathbf{r}(\\mathbf{m})$ 对齐并具有最大可能的模长时，项 $2\\mathbf{r}(\\mathbf{m})^{T}\\delta F$ 和 $\\|\\delta F\\|_{2}^{2}$ 都达到最大值。约束条件是 $\\|\\delta F\\|_{2} \\leq \\eta$。因此，最大值必定出现在可行集的边界上，即当 $\\|\\delta F\\|_{2} = \\eta$ 时。\n\n因此，最大化子 $\\delta F^{\\star}$ 必须是一个长度为 $\\eta$ 且与 $\\mathbf{r}(\\mathbf{m})$ 平行的向量。假设 $\\mathbf{r}(\\mathbf{m}) \\neq \\mathbf{0}$，这个向量是唯一的：\n$$\n\\delta F^{\\star} = \\eta \\frac{\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\n$$\n如果 $\\mathbf{r}(\\mathbf{m}) = \\mathbf{0}$，则待最大化的项是 $\\frac{1}{2}\\|\\delta F\\|_{2}^{2}$，它在 $\\|\\delta F\\|_{2} = \\eta$ 时达到最大值。在这种情况下，任何满足 $\\|\\delta F^{\\star}\\|_{2} = \\eta$ 的向量 $\\delta F^{\\star}$ 都是最大化子。\n\n将 $\\delta F^{\\star}$ 代回到平方范数的表达式中：\n$$\n\\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = \\left\\|\\mathbf{r}(\\mathbf{m}) + \\eta \\frac{\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right\\|_{2}^{2} = \\left\\|\\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)\\mathbf{r}(\\mathbf{m})\\right\\|_{2}^{2}\n$$\n$$\n= \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)^{2} \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} = \\left(\\frac{\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)^{2} \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} = (\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta)^{2}\n$$\n该公式在 $\\mathbf{r}(\\mathbf{m}) = \\mathbf{0}$ 的情况下也成立。因此，稳健失配的闭合形式表达式为：\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}(\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta)^{2} = \\frac{1}{2}(\\|F(\\mathbf{m}) - \\mathbf{d}\\|_{2} + \\eta)^{2}\n$$\n\n### 第2部分：近似梯度的推导\n\n我们要求解 $J(\\mathbf{m})$ 关于 $\\mathbf{m}$ 在点 $\\mathbf{m}_{k}$ 的梯度。我们使用链式法则。令 $u(\\mathbf{m}) = \\|F(\\mathbf{m}) - \\mathbf{d}\\|_{2} = \\|\\mathbf{r}(\\mathbf{m})\\|_{2}$。那么 $J(\\mathbf{m}) = \\frac{1}{2}(u(\\mathbf{m}) + \\eta)^{2}$。\n\n梯度为 $\\nabla J(\\mathbf{m}) = \\frac{dJ}{du} \\nabla u(\\mathbf{m})$。\n第一项是：\n$$\n\\frac{dJ}{du} = \\frac{1}{2} \\cdot 2(u(\\mathbf{m}) + \\eta) = u(\\mathbf{m}) + \\eta = \\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta\n$$\n第二项是残差欧几里得范数的梯度，$\\nabla u(\\mathbf{m}) = \\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2}$。假设 $\\mathbf{r}(\\mathbf{m}) \\neq \\mathbf{0}$：\n$$\n\\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2} = \\nabla (\\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}))^{1/2} = \\frac{1}{2}(\\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}))^{-1/2} \\nabla(\\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}))\n$$\n平方范数的梯度为 $\\nabla(\\mathbf{r}^{T}\\mathbf{r}) = 2 J_{\\mathbf{r}}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})$，其中 $J_{\\mathbf{r}}(\\mathbf{m})$ 是 $\\mathbf{r}(\\mathbf{m})$ 关于 $\\mathbf{m}$ 的雅可比矩阵。\n由于 $\\mathbf{r}(\\mathbf{m}) = F(\\mathbf{m}) - \\mathbf{d}$，其雅可比矩阵为 $J_{\\mathbf{r}}(\\mathbf{m}) = J_{F}(\\mathbf{m})$，即正演映射 $F$ 的雅可比矩阵。\n因此，\n$$\n\\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2} = \\frac{1}{2\\|\\mathbf{r}(\\mathbf{m})\\|_{2}} (2 J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})) = \\frac{J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\n$$\n合并各项，$J(\\mathbf{m})$ 的梯度为：\n$$\n\\nabla J(\\mathbf{m}) = (\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta) \\frac{J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}} = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right) J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})\n$$\n在迭代点 $\\mathbf{m}_{k}$ 处求值，其中 $\\mathbf{r}_{k} \\equiv F(\\mathbf{m}_{k}) - \\mathbf{d}$，我们得到所需的梯度：\n$$\n\\nabla J(\\mathbf{m}_{k}) = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}_{k}\\|_{2}}\\right) J_{F}(\\mathbf{m}_{k})^{T}\\mathbf{r}_{k}\n$$\n使用 $F(\\mathbf{m})$ 在 $\\mathbf{m}_k$ 附近的一阶线性化，为在诸如最速下降法之类的迭代优化方案中使用此梯度提供了理论基础，因为它用一个在 $\\mathbf{m}_k$ 处的梯度恰好是我们所推导出的梯度的函数来近似 $J(\\mathbf{m})$。\n\n### 第3部分：具体案例与精确线搜索\n\n我们已知以下条件：\n- 正演映射：$F(\\mathbf{m}) = \\mathbf{m}$。这是一个 $\\mathbb{R}^{2}$ 中的线性映射。\n- 初始模型：$\\mathbf{m}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n- 数据：$\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$。\n- 建模误差半径：$\\eta = 1$。\n\n首先，我们计算在 $\\mathbf{m}_{0}$ 处的梯度。\n$F(\\mathbf{m}) = \\mathbf{m}$ 的雅可比矩阵对于所有 $\\mathbf{m}$ 都是单位矩阵 $I$。因此，$J_{F}(\\mathbf{m}_{0}) = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n在 $\\mathbf{m}_{0}$ 处的残差是：\n$$\n\\mathbf{r}_{0} = F(\\mathbf{m}_{0}) - \\mathbf{d} = \\mathbf{m}_{0} - \\mathbf{d} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\n残差的欧几里得范数是：\n$$\n\\|\\mathbf{r}_{0}\\|_{2} = \\sqrt{2^{2} + 0^{2}} = 2\n$$\n使用第2部分的公式，梯度 $\\nabla J(\\mathbf{m}_{0})$ 是：\n$$\n\\nabla J(\\mathbf{m}_{0}) = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}_{0}\\|_{2}}\\right) J_{F}(\\mathbf{m}_{0})^{T}\\mathbf{r}_{0} = \\left(1 + \\frac{1}{2}\\right) I^{T} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\frac{3}{2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\frac{3}{2} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\n$$\n最速下降方向是 $\\mathbf{p} = -\\nabla J(\\mathbf{m}_{0}) = \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix}$。\n\n接下来，我们执行精确线搜索来找到最优步长 $\\alpha^{\\star} \\geq 0$，以最小化 $J(\\mathbf{m}_{0} + \\alpha \\mathbf{p})$。我们将模型定义为 $\\alpha$ 的函数：\n$$\n\\mathbf{m}(\\alpha) = \\mathbf{m}_{0} + \\alpha \\mathbf{p} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\alpha \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 3\\alpha \\\\ 1 \\end{pmatrix}\n$$\n待最小化的目标函数是 $J(\\alpha) = J(\\mathbf{m}(\\alpha))$。我们计算作为 $\\alpha$ 函数的残差：\n$$\n\\mathbf{r}(\\alpha) = F(\\mathbf{m}(\\alpha)) - \\mathbf{d} = \\mathbf{m}(\\alpha) - \\mathbf{d} = \\begin{pmatrix} 1 - 3\\alpha \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 - 3\\alpha \\\\ 0 \\end{pmatrix}\n$$\n该残差的范数是：\n$$\n\\|\\mathbf{r}(\\alpha)\\|_{2} = \\sqrt{(2 - 3\\alpha)^{2} + 0^{2}} = |2 - 3\\alpha|\n$$\n现在我们将目标函数用 $\\alpha$ 表示：\n$$\nJ(\\alpha) = \\frac{1}{2}(\\|\\mathbf{r}(\\alpha)\\|_{2} + \\eta)^{2} = \\frac{1}{2}(|2 - 3\\alpha| + 1)^{2}\n$$\n我们需要找到 $\\alpha^{\\star} = \\arg\\min_{\\alpha \\geq 0} J(\\alpha)$。由于平方函数和因子 $\\frac{1}{2}$ 对于非负自变量是单调的，最小化 $J(\\alpha)$ 等价于最小化幂的底数 $|2 - 3\\alpha| + 1$。这又等价于最小化 $|2 - 3\\alpha|$。\n函数 $|2 - 3\\alpha|$ 是一个V形函数，其最小值为 $0$。当绝对值内的表达式为零时达到最小值：\n$$\n2 - 3\\alpha = 0 \\implies 3\\alpha = 2 \\implies \\alpha = \\frac{2}{3}\n$$\n由于这个 $\\alpha$ 的值是非负的（$\\frac{2}{3} \\geq 0$），它就是最优步长。目标函数的最小值在此点达到。\n精确的最优步长是 $\\alpha^{\\star} = \\frac{2}{3}$。",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        }
    ]
}