{
    "hands_on_practices": [
        {
            "introduction": "In any gradient-based optimization, the correctness of the gradient is paramount. For inverse problems governed by partial differential equations (PDEs), the gradient is computed efficiently using the adjoint-state method. This practice guides you through the implementation of the discrete adjoint test, an essential verification tool to ensure your code is free of common but subtle errors. By numerically verifying the identity $\\langle J \\delta \\mathbf{m}, \\delta \\mathbf{d} \\rangle = \\langle \\delta \\mathbf{m}, J^T \\delta \\mathbf{d} \\rangle$, where $J$ is the Jacobian, you will gain hands-on experience with the practical consequences of discretizing continuous forward and adjoint operators .",
            "id": "3585132",
            "problem": "Consider the one-dimensional Helmholtz equation on an interval with homogeneous Dirichlet boundary conditions as the forward model in an inverse problem. The continuous forward problem is: find a field $u(x)$ on the interval $[0,L]$ such that\n$$\n-\\frac{d^2 u}{dx^2} - \\omega^2 m(x) u(x) = q(x), \\quad \\text{for } x \\in (0,L), \\quad \\text{with } u(0) = 0 \\text{ and } u(L) = 0.\n$$\nHere $L$ is the domain length, $\\omega$ is the angular frequency in radians per second, $m(x)$ is the unknown parameter (e.g., slowness squared) and $q(x)$ is a source term. The weak form involves test functions $v(x)$ in $H_0^1([0,L])$ and is given by\n$$\n\\int_0^L \\frac{du}{dx} \\frac{dv}{dx}\\,dx - \\omega^2 \\int_0^L m(x) u(x) v(x)\\,dx = \\int_0^L q(x) v(x)\\,dx.\n$$\nDiscretize the interval $[0,L]$ into $N$ uniform elements and use linear finite elements with homogeneous Dirichlet boundary conditions at $x=0$ and $x=L$. Let there be $N+1$ nodes, and let the two boundary nodes be eliminated, yielding $N-1$ interior degrees of freedom. Represent the parameter as piecewise constant per element, with values $\\{m_e\\}_{e=0}^{N-1}$, one per element. Assemble the global stiffness matrix $K \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ and the parameter-weighted mass matrix $M_m \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ constructed as a sum of local element mass matrices weighted by $m_e$. The discrete forward operator is\n$$\nA(m) u = b, \\quad \\text{with } A(m) = K - \\omega^2 M_m,\n$$\nwhere $b \\in \\mathbb{R}^{N-1}$ is a discrete right-hand side representing the source $q(x)$.\n\nDefine a linear sampling operator $P \\in \\mathbb{R}^{n_d \\times (N-1)}$ that selects values of $u$ at specified interior node indices to form the data vector $d = P u \\in \\mathbb{R}^{n_d}$. Consider perturbations in the parameter, $\\delta m \\in \\mathbb{R}^{N}$ (one entry per element), and perturbations in the data, $\\delta d \\in \\mathbb{R}^{n_d}$. Let $J$ denote the Jacobian of the parameter-to-data map at the background $m$, and $J^T$ its adjoint under the standard Euclidean inner products in model space and data space.\n\nYour task is to implement the discrete adjoint test for this finite element Helmholtz solver to numerically verify, for given perturbations $\\delta m$ and $\\delta d$, the equality\n$$\n\\langle J \\delta m, \\delta d \\rangle = \\langle \\delta m, J^T \\delta d \\rangle,\n$$\nwhere $\\langle \\cdot,\\cdot \\rangle$ denotes the standard Euclidean inner product in the corresponding discrete spaces. You must start from the weak form and the finite element assembly rules (local stiffness matrix and local mass matrix for linear elements on a uniform mesh) as the fundamental base, derive expressions for the discrete forward operator $A(m)$, the linearized state perturbation $\\delta u$ driven by $\\delta m$, and the adjoint field associated with $\\delta d$, and then implement these in code. The program must compute both inner products and report a relative discrepancy measure\n$$\ne = \\frac{\\left| \\langle J \\delta m, \\delta d \\rangle - \\langle \\delta m, J^T \\delta d \\rangle \\right|}{\\max\\left(1, \\left|\\langle J \\delta m, \\delta d \\rangle\\right|, \\left|\\langle \\delta m, J^T \\delta d \\rangle\\right|\\right)}.\n$$\n\nAngle unit: $\\omega$ must be used in radians per second. No physical units are required for the final output since the discrepancy $e$ is dimensionless.\n\nImplement the test in the following test suite. For reproducibility, use a single uniform random number generator per test case initialized with the given seed, and draw independent random variables from it for the parameter vector $m$ and perturbations $\\delta m$ and $\\delta d$.\n\n- Test Case 1 (happy path): $L = 1.0$, $N = 64$, $\\omega = 5.0$ rad/s. Element parameters $m_e$ drawn uniformly from $[0.8, 1.2]$. Receivers at interior node indices $\\{16, 32, 48\\}$ (indices count from $1$ to $N-1$). Seed $314159$.\n- Test Case 2 (coarse mesh boundary case): $L = 1.0$, $N = 8$, $\\omega = 5.0$ rad/s. Element parameters $m_e$ drawn uniformly from $[0.8, 1.2]$. Receivers at interior node indices $\\{2, 4, 6\\}$. Seed $271828$.\n- Test Case 3 (higher frequency, more ill-conditioned): $L = 1.0$, $N = 64$, $\\omega = 20.0$ rad/s. Element parameters $m_e$ drawn uniformly from $[0.1, 0.3]$. Receivers at interior node indices $\\{10, 30, 50\\}$. Seed $161803$.\n- Test Case 4 (minimal degrees of freedom): $L = 1.0$, $N = 2$, $\\omega = 3.0$ rad/s. Element parameters $m_e$ drawn uniformly from $[0.9, 1.1]$. Receivers at the only interior node index $\\{1\\}$. Seed $99$.\n\nUse a discrete source $b$ defined as a unit load applied at the central interior degree of freedom. For each test case, compute the discrepancy $e$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[e1,e2,e3,e4]\"), with each $e_i$ a floating-point number.",
            "solution": "The fundamental base is the weak form of the Helmholtz equation and the standard finite element assembly for linear (piecewise affine) basis functions on a uniform mesh. On a closed interval $[0,L]$ with homogeneous Dirichlet boundary conditions, the weak formulation of the forward problem reads: find $u \\in H_0^1([0,L])$ such that\n$$\n\\int_0^L \\frac{du}{dx}\\frac{dv}{dx}\\,dx - \\omega^2 \\int_0^L m(x) u(x) v(x)\\,dx = \\int_0^L q(x) v(x)\\,dx \\quad \\text{for all } v \\in H_0^1([0,L]).\n$$\nFor a uniform mesh with $N$ elements, the local element length is $h = L/N$. The linear (two-node) element has standard local matrices:\n$$\nK^{(e)} = \\frac{1}{h} \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}, \\quad M^{(e)} = \\frac{h}{6} \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}.\n$$\nLet $m_e$ denote the piecewise constant parameter on element $e$. The parameter-weighted local mass matrix for element $e$ is $m_e M^{(e)}$. Assembly over all elements (adding contributions according to connectivity and applying homogeneous Dirichlet boundary conditions by eliminating boundary degrees of freedom) yields the global stiffness matrix $K$ and the parameter-weighted mass matrix $M_m = \\sum_{e=0}^{N-1} m_e \\mathcal{A}(M^{(e)})$, where $\\mathcal{A}$ denotes insertion into the global matrix at the appropriate interior degrees of freedom. The discrete forward operator is\n$$\nA(m) = K - \\omega^2 M_m \\in \\mathbb{R}^{(N-1)\\times(N-1)},\n$$\nand the discrete forward problem is\n$$\nA(m)\\,u = b, \\quad u \\in \\mathbb{R}^{N-1},\n$$\nwhere $b$ is a discrete representation of the source $q$. In our implementation, $b$ is chosen as a unit load at the central interior node, which is consistent with the weak form when $q$ is localized and the test space includes nodal basis functions.\n\nDefine the data sampling operator $P \\in \\mathbb{R}^{n_d \\times (N-1)}$ that selects specified interior nodal values of $u$; that is, if the receiver interior indices are $\\{i_r\\}$, then $P$ has rows that are standard basis vectors $e_{i_r}^T$. The data vector is $d = P u$.\n\nTo derive the Jacobian $J$ of the parameter-to-data map and its adjoint $J^T$, start from the implicit definition of the forward map $F(m) = P u(m)$ with $u(m)$ solving $A(m) u(m) = b$. Differentiate the discrete forward equation with respect to $m$ in a direction $\\delta m = (\\delta m_e)_{e=0}^{N-1}$. The derivative of $A(m)$ with respect to $m_e$ is\n$$\n\\frac{\\partial A}{\\partial m_e} = -\\omega^2\\,\\mathcal{A}\\!\\left(M^{(e)}\\right),\n$$\nsince only the parameter-weighted mass term depends on $m_e$ linearly. The linearized state perturbation $\\delta u$ satisfies\n$$\nA(m)\\,\\delta u + \\left( \\sum_{e=0}^{N-1} \\delta m_e \\frac{\\partial A}{\\partial m_e} \\right) u = 0,\n$$\nso\n$$\nA(m)\\,\\delta u = -\\sum_{e=0}^{N-1} \\delta m_e \\frac{\\partial A}{\\partial m_e} \\, u = \\omega^2 \\sum_{e=0}^{N-1} \\delta m_e \\,\\mathcal{A}\\!\\left(M^{(e)}\\right) u.\n$$\nTherefore,\n$$\n\\delta u = A(m)^{-1} \\left( \\omega^2 \\sum_{e=0}^{N-1} \\delta m_e \\,\\mathcal{A}\\!\\left(M^{(e)}\\right) u \\right),\n$$\nand the Jacobian acting on $\\delta m$ is $J \\delta m = P\\,\\delta u$.\n\nFor the discrete adjoint, consider an arbitrary data perturbation $\\delta d \\in \\mathbb{R}^{n_d}$ and define the adjoint field $\\lambda \\in \\mathbb{R}^{N-1}$ by\n$$\nA(m)^T \\lambda = P^T \\delta d.\n$$\nSince $A(m)$ is assembled from symmetric local matrices, it is symmetric, and thus $A(m)^T = A(m)$. Using the expressions above,\n$$\n\\langle J \\delta m, \\delta d \\rangle = \\langle P \\delta u, \\delta d \\rangle = \\langle \\delta u, P^T \\delta d \\rangle = \\left\\langle A(m)^{-1} \\left( \\omega^2 \\sum_{e=0}^{N-1} \\delta m_e \\,\\mathcal{A}\\!\\left(M^{(e)}\\right) u \\right),\\, P^T \\delta d \\right\\rangle.\n$$\nLet $\\lambda$ be the unique solution of $A(m)\\lambda = P^T \\delta d$. Then\n$$\n\\langle J \\delta m, \\delta d \\rangle = \\omega^2 \\sum_{e=0}^{N-1} \\delta m_e \\, \\left\\langle \\mathcal{A}\\!\\left(M^{(e)}\\right) u,\\, \\lambda \\right\\rangle.\n$$\nInterpreting the inner product by summation over interior degrees of freedom and noting that $\\mathcal{A}\\!\\left(M^{(e)}\\right)$ is symmetric, one obtains\n$$\n\\langle J \\delta m, \\delta d \\rangle = \\sum_{e=0}^{N-1} \\delta m_e \\,\\left( \\omega^2\\, u^T \\,\\mathcal{A}\\!\\left(M^{(e)}\\right)\\, \\lambda \\right).\n$$\nThus the adjoint action $J^T \\delta d$ on the parameter space has components\n$$\n\\left[J^T \\delta d\\right]_e = \\omega^2\\, u^T \\,\\mathcal{A}\\!\\left(M^{(e)}\\right)\\, \\lambda, \\quad e = 0,\\dots,N-1,\n$$\nand consequently\n$$\n\\langle \\delta m, J^T \\delta d \\rangle = \\sum_{e=0}^{N-1} \\delta m_e \\,\\left[J^T \\delta d\\right]_e.\n$$\nThis establishes the discrete adjoint relation in the finite element context.\n\nAlgorithmic design follows directly:\n- Assemble $K$ and $M_m$ from $K^{(e)}$ and $m_e M^{(e)}$, and form $A(m) = K - \\omega^2 M_m$.\n- Solve the forward problem $A(m)u = b$.\n- For a given perturbation $\\delta m$, assemble the linearized right-hand side $r = \\omega^2 \\sum_e \\delta m_e \\,\\mathcal{A}\\!\\left(M^{(e)}\\right) u$, and solve $A(m)\\delta u = r$.\n- For a given $\\delta d$, form $P^T \\delta d$ by inserting $\\delta d$ at the receiver indices and zeros elsewhere, solve $A(m) \\lambda = P^T \\delta d$, and compute the element-wise adjoint action $\\left[J^T \\delta d\\right]_e = \\omega^2\\, u^T \\,\\mathcal{A}\\!\\left(M^{(e)}\\right)\\, \\lambda$.\n- Compute the inner products to obtain $\\langle J \\delta m, \\delta d \\rangle$ and $\\langle \\delta m, J^T \\delta d \\rangle$, and report the relative discrepancy $e$.\n\nOn tolerances: in exact arithmetic, the discrete adjoint identity holds exactly. In floating-point arithmetic, deviations arise from round-off and solver errors. A principled tolerance is tied to the conditioning of $A(m)$ and machine precision $\\epsilon$. If the linear solves are performed to near machine precision, a discrepancy of size on the order of\n$$\n\\tau \\approx c \\,\\kappa(A) \\,\\epsilon\n$$\nis expected, where $\\kappa(A)$ is the condition number of $A(m)$ in the $2$-norm and $c$ is a modest constant accounting for multiple solves and summations (e.g., $c$ between $10$ and $100$). For well-conditioned cases (e.g., small $\\omega$ and sufficiently fine mesh), $\\kappa(A)$ is small and $\\tau$ is in the range of $10^{-12}$ to $10^{-10}$ in double precision. For more ill-conditioned cases (e.g., higher $\\omega$ or coarser mesh), $\\kappa(A)$ increases and tolerances on the order of $10^{-9}$ to $10^{-7}$ may be necessary. Discrepancies exceeding such tolerance estimates are indicative of coding errors (e.g., incorrect assembly, mismatched inner products, or misapplied boundary conditions) rather than discretization effects. In the provided implementation, a direct sparse solver is used, so the observed discrepancies should be close to machine precision scaled by conditioning.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csr_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef assemble_stiffness(L, N):\n    \"\"\"\n    Assemble global stiffness matrix K for 1D linear finite elements\n    with homogeneous Dirichlet boundary conditions, on a uniform mesh.\n    \"\"\"\n    n = N - 1  # interior DOFs\n    h = L / N\n    K = lil_matrix((n, n))\n    # Standard FE stiffness assembly for interior nodes: tridiagonal matrix\n    for i in range(n):\n        K[i, i] += 2.0 / h\n        if i > 0:\n            K[i, i - 1] += -1.0 / h\n        if i < n - 1:\n            K[i, i + 1] += -1.0 / h\n    return csr_matrix(K)\n\ndef local_mass_matrix(h):\n    \"\"\"\n    Local element mass matrix for linear elements on an interval of length h.\n    \"\"\"\n    return (h / 6.0) * np.array([[2.0, 1.0],\n                                 [1.0, 2.0]])\n\ndef assemble_param_mass_and_locals(L, N, m_elem):\n    \"\"\"\n    Assemble the global parameter-weighted mass matrix M_m and\n    return the list of unweighted local mass matrices per element and\n    the mapping of element-local nodes to interior DOF indices.\n    \"\"\"\n    n = N - 1\n    h = L / N\n    M_m = lil_matrix((n, n))\n    M_local_plain = []\n    interior_pairs = []  # (i0, i1) interior indices for element's two nodes; None if boundary\n    M_plain = local_mass_matrix(h)\n    for e in range(N):\n        # Global node indices for element e are e and e+1\n        g0 = e\n        g1 = e + 1\n        # Map to interior DOF indices (1..N-1 -> 0..N-2)\n        i0 = g0 - 1 if 1 <= g0 <= N - 1 else None\n        i1 = g1 - 1 if 1 <= g1 <= N - 1 else None\n        interior_pairs.append((i0, i1))\n        # Parameter-weighted local mass\n        M_loc_weighted = M_plain * m_elem[e]\n        # Assemble into global M_m\n        if i0 is not None:\n            M_m[i0, i0] += M_loc_weighted[0, 0]\n            if i1 is not None:\n                M_m[i0, i1] += M_loc_weighted[0, 1]\n        if i1 is not None:\n            if i0 is not None:\n                M_m[i1, i0] += M_loc_weighted[1, 0]\n            M_m[i1, i1] += M_loc_weighted[1, 1]\n        # Store plain local mass (unweighted) for derivative computations\n        M_local_plain.append(M_plain.copy())\n    return csr_matrix(M_m), M_local_plain, interior_pairs\n\ndef build_A(K, omega, M_m):\n    \"\"\"\n    Construct Helmholtz operator A(m) = K - omega^2 * M_m.\n    \"\"\"\n    return K - (omega**2) * M_m\n\ndef rhs_source(n):\n    \"\"\"\n    Build a discrete right-hand side with unit load at central DOF.\n    \"\"\"\n    b = np.zeros(n)\n    center = n // 2\n    b[center] = 1.0\n    return b\n\ndef compute_linearized_rhs(u, delta_m, M_local_plain, interior_pairs, omega):\n    \"\"\"\n    Compute r = omega^2 * sum_e delta_m[e] * (A(M^{(e)}) * u),\n    where A(M^{(e)}) denotes assembly of unweighted local mass onto the global vector.\n    \"\"\"\n    n = u.shape[0]\n    r = np.zeros(n)\n    for e, (i0, i1) in enumerate(interior_pairs):\n        M_loc = M_local_plain[e]\n        dm = delta_m[e]\n        # Contributions to r at i0 and i1\n        if i0 is not None:\n            val0 = 0.0\n            if i0 is not None:\n                val0 += M_loc[0, 0] * u[i0]\n            if i1 is not None:\n                val0 += M_loc[0, 1] * u[i1]\n            r[i0] += (omega**2) * dm * val0\n        if i1 is not None:\n            val1 = 0.0\n            if i0 is not None:\n                val1 += M_loc[1, 0] * u[i0]\n            if i1 is not None:\n                val1 += M_loc[1, 1] * u[i1]\n            r[i1] += (omega**2) * dm * val1\n    return r\n\ndef compute_adjoint_element_contrib(u, lam, M_local_plain, interior_pairs, omega):\n    \"\"\"\n    Compute J^T * delta_d evaluated per element:\n    g[e] = omega^2 * u^T * A(M^{(e)}) * lam,\n    where A(M^{(e)}) is assembly of local mass.\n    \"\"\"\n    N = len(M_local_plain)\n    g = np.zeros(N)\n    for e, (i0, i1) in enumerate(interior_pairs):\n        M_loc = M_local_plain[e]\n        u0 = u[i0] if i0 is not None else 0.0\n        u1 = u[i1] if i1 is not None else 0.0\n        l0 = lam[i0] if i0 is not None else 0.0\n        l1 = lam[i1] if i1 is not None else 0.0\n        u_vec = np.array([u0, u1])\n        l_vec = np.array([l0, l1])\n        s = float(u_vec.dot(M_loc.dot(l_vec)))\n        g[e] = (omega**2) * s\n    return g\n\ndef adjoint_test_case(L, N, omega, m_range, recv_indices, seed):\n    \"\"\"\n    Execute the adjoint test for a single case and return the relative discrepancy e.\n    recv_indices are interior node indices in 1..N-1 (global interior indexing).\n    \"\"\"\n    # Random generators\n    rng = np.random.default_rng(seed)\n    # Draw parameter per element\n    m_elem = rng.uniform(m_range[0], m_range[1], size=N)\n    # Assemble matrices\n    K = assemble_stiffness(L, N)\n    M_m, M_local_plain, interior_pairs = assemble_param_mass_and_locals(L, N, m_elem)\n    A = build_A(K, omega, M_m)\n    n = N - 1\n    # Source and forward solve\n    b = rhs_source(n)\n    u = spsolve(A, b)\n    # Receivers: convert interior node indices (1..N-1) to DOF indices (0..N-2)\n    recv_dofs = [idx - 1 for idx in recv_indices]\n    # Draw perturbations\n    delta_m = rng.standard_normal(N)\n    delta_d = rng.standard_normal(len(recv_dofs))\n    # Compute J * delta_m: solve for delta_u from linearized RHS\n    r = compute_linearized_rhs(u, delta_m, M_local_plain, interior_pairs, omega)\n    delta_u = spsolve(A, r)\n    Jdm = delta_u[recv_dofs]  # P * delta_u\n    # Compute inner product <J delta_m, delta_d>\n    inner1 = float(np.dot(Jdm, delta_d))\n    # Compute adjoint field: solve A * lam = P^T delta_d\n    rhs_adj = np.zeros(n)\n    for j, dof in enumerate(recv_dofs):\n        rhs_adj[dof] += delta_d[j]\n    lam = spsolve(A, rhs_adj)\n    # Compute J^T * delta_d per element\n    g = compute_adjoint_element_contrib(u, lam, M_local_plain, interior_pairs, omega)\n    inner2 = float(np.dot(delta_m, g))\n    # Relative discrepancy\n    denom = max(1.0, abs(inner1), abs(inner2))\n    rel_err = abs(inner1 - inner2) / denom\n    return rel_err\n\ndef solve():\n    # Define test cases as specified\n    test_cases = [\n        # (L, N, omega, (m_min, m_max), receiver_indices, seed)\n        (1.0, 64, 5.0, (0.8, 1.2), [16, 32, 48], 314159),\n        (1.0, 8, 5.0, (0.8, 1.2), [2, 4, 6], 271828),\n        (1.0, 64, 20.0, (0.1, 0.3), [10, 30, 50], 161803),\n        (1.0, 2, 3.0, (0.9, 1.1), [1], 99),\n    ]\n\n    results = []\n    for (L, N, omega, m_range, recv_indices, seed) in test_cases:\n        e = adjoint_test_case(L, N, omega, m_range, recv_indices, seed)\n        results.append(e)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While first-order optimization methods rely only on the gradient, second-order methods like the Gauss-Newton algorithm can offer much faster convergence by incorporating curvature information from the Hessian matrix. For large-scale inverse problems, explicitly forming the Hessian is computationally infeasible. This exercise demonstrates the power of the adjoint-state method to compute Hessian-vector products without ever forming the Hessian matrix, a technique central to modern Full Waveform Inversion (FWI). You will derive and implement this Jacobian-free approach, contrasting its linear complexity with the quadratic cost of explicit assembly .",
            "id": "3585169",
            "problem": "Consider the linearized acoustic frequency-domain Full Waveform Inversion (FWI) problem under the Gauss-Newton method. Let the forward state be governed by the one-dimensional Helmholtz-type equation on a uniform grid with Dirichlet boundary conditions, discretized by second-order centered finite differences. The discrete system is defined as follows. For $N$ interior grid points, grid spacing $h$, a discrete Laplacian operator $D_{xx}$ with entries $-2/h^2$ on the diagonal and $1/h^2$ on the immediate off-diagonals, and a positive mass term $\\omega^2 \\operatorname{diag}(m)$, the forward operator is\n$$\nA(m) \\equiv -D_{xx} + \\omega^2 \\operatorname{diag}(m),\n$$\nwith angular frequency $\\omega$ in radians per second. The forward state $u$ satisfies\n$$\nA(m) u = s,\n$$\nwhere $s$ is a discrete source vector with a single nonzero at the source index.\n\nDefine the data sampling operator $P$ that maps the full field $u$ to receiver data $d = P u$ by extracting components at specified receiver indices, and let the data weighting $W$ be the identity (for simplicity). The Gauss-Newton approximation of the Hessian is $\\mathcal{H} \\approx J^\\top W J$, where $J$ is the Jacobian of the data with respect to the model $m$. A perturbation $p$ to $m$ induces an incremental state $\\delta u$ satisfying the tangent-linear (Born) equation\n$$\nA(m)\\, \\delta u = \\omega^2 \\operatorname{diag}(p)\\, u,\n$$\nand the corresponding data perturbation is $\\delta d = P\\, \\delta u = J p$.\n\nYour tasks:\n1. Starting from the above discrete forward model and the definition of the Gauss-Newton approximation, derive a Jacobian-free expression for the Hessian-vector product $J^\\top W J p$ in terms of the forward state solve, one incremental (tangent-linear) solve, and one adjoint-state solve. The adjoint state $v$ is defined by the adjoint equation\n$$\nA(m)^\\top v = P^\\top W (J p),\n$$\nand the Hessian-vector product must be expressed without forming $J$ explicitly.\n2. Provide a computational complexity analysis in terms of the number of linear solves and asymptotic operation counts for a tridiagonal solver, comparing the Jacobian-free approach (one forward solve to obtain $u$, one incremental solve for $\\delta u$, and one adjoint solve for $v$ per Hessian-vector product) to the explicit Jacobian assembly approach (assembling all columns of $J$ by solving the incremental problem for each canonical basis perturbation and then applying $J^\\top W J$ to $p$ by matrix-vector products).\n\nDiscretization details:\n- Use $N$ interior points and uniform grid spacing $h = L/(N+1)$ over a domain of length $L$ in meters.\n- The operator $A(m)$ is tridiagonal with subdiagonal and superdiagonal entries equal to $-1/h^2$, and diagonal entries equal to $2/h^2 + \\omega^2 m_i$, where $m_i$ is the $i$-th component of the model vector $m$.\n- The source vector $s$ has a unit amplitude at the specified source index and zeros elsewhere.\n- The receiver operator $P$ samples the solution at specified interior indices.\n\nUnits and output:\n- Angular frequency $\\omega$ must be interpreted in radians per second. All other quantities are nondimensionalized within the discretization; the final numerical outputs are unitless floats or booleans as specified below.\n\nTest suite:\nImplement a program that constructs the above operators and computes the Jacobian-free Hessian-vector product and, for verification, an explicit Jacobian-based Hessian-vector product for the following three cases. For each case, compute:\n- The relative $\\ell_2$-error between the Jacobian-free Hessian-vector product and the explicit Jacobian-based Hessian-vector product, defined as\n$$\n\\frac{\\|h_{\\mathrm{JF}} - h_{\\mathrm{EX}}\\|_2}{\\max(\\|h_{\\mathrm{EX}}\\|_2, \\epsilon)},\n$$\nwith $\\epsilon = 10^{-14}$ to avoid division by zero.\n- The ratio of the number of linear solves required by explicit Jacobian assembly to that required by the Jacobian-free approach for a single Hessian-vector product (not counting the common forward solve for $u$), which is $N/2$.\n\nCase A (happy path):\n- $N = 50$, $L = 1$ meter, $\\omega = 20$ radians/second, constant wave speed $c = 2$ meters/second, so $m_i = 1/c^2$ for all $i$.\n- Source index $10$, receiver indices $\\{20, 35, 45\\}$.\n- Perturbation $p_i = \\sin\\left(\\frac{2\\pi i}{N}\\right)$ for $i=0,\\dots,N-1$.\n\nCase B (small system boundary case):\n- $N = 5$, $L = 1$ meter, $\\omega = 5$ radians/second, constant wave speed $c = 1$ meter/second, so $m_i = 1$ for all $i$.\n- Source index $1$, receiver indices $\\{2\\}$.\n- Perturbation $p = (1, 0, 0, 0, 0)$.\n\nCase C (edge case with zero perturbation):\n- $N = 30$, $L = 1$ meter, $\\omega = 10$ radians/second, constant wave speed $c = 1.5$ meters/second, so $m_i = 1/c^2$ for all $i$.\n- Source index $5$, receiver indices $\\{10, 20, 25\\}$.\n- Perturbation $p = \\mathbf{0}$ (all zeros).\n\nFinal output format:\nYour program should produce a single line of output containing six comma-separated entries enclosed in square brackets. These entries correspond to the three cases in order: for each case, first the relative $\\ell_2$-error as a float, then the complexity ratio $N/2$ as a float. For example, the output should look like\n$$\n[\\text{errA}, \\text{ratioA}, \\text{errB}, \\text{ratioB}, \\text{errC}, \\text{ratioC}],\n$$\nwhere each quantity is a float. No other text should be printed.",
            "solution": "This problem requires two main tasks: first, to derive a \"Jacobian-free\" method for computing the Gauss-Newton Hessian-vector product, and second, to analyze its computational complexity compared to explicit Jacobian assembly.\n\n### 1. Jacobian-Free Hessian-Vector Product Derivation\n\nThe Gauss-Newton approximation of the Hessian is given by $\\mathcal{H} \\approx J^\\top W J$. Since the data weighting matrix $W$ is the identity, this simplifies to $\\mathcal{H} \\approx J^\\top J$. We want to compute the action of this matrix on an arbitrary model perturbation vector $p$, i.e., the product $J^\\top J p$, without explicitly forming the Jacobian matrix $J$.\n\nThe overall product can be computed in two stages:\n1.  **Action of the Jacobian, $Jp$**: First, we compute the vector $y = Jp$. From the problem statement, the action of the Jacobian on a perturbation $p$ is defined via the tangent-linear (or Born) equation. The result is a data perturbation $\\delta d = P \\delta u$, where $\\delta u$ is the incremental state that solves:\n    $$\n    A(m)\\, \\delta u = \\omega^2 \\operatorname{diag}(p)\\, u\n    $$\n    Therefore, $y = Jp = P \\delta u$. This step requires one linear solve for $\\delta u$.\n\n2.  **Action of the Adjoint Jacobian, $J^\\top y$**: Next, we compute the product $J^\\top y$. The action of the adjoint of the Jacobian on a data-space vector $y$ can be derived from the definition of an adjoint operator, which states that for any two vectors $p'$ and $y$, the inner product must satisfy $(J^\\top y, p') = (y, Jp')$. Using the definition of $Jp'$ from the first stage, we have:\n    $$\n    (y, Jp') = (y, P \\delta u') = (P^\\top y, \\delta u')\n    $$\n    where $\\delta u'$ is the solution to $A(m) \\delta u' = \\omega^2 \\operatorname{diag}(p') u$. Substituting for $\\delta u'$ gives:\n    $$\n    (P^\\top y, \\delta u') = (P^\\top y, A(m)^{-1} (\\omega^2 \\operatorname{diag}(p') u))\n    $$\n    Using the property that $(b, A^{-1}c) = ((A^{-1})^\\top b, c)$, we can move the inverse operator to the other side of the inner product:\n    $$\n    ( (A(m)^{-1})^\\top P^\\top y, \\omega^2 \\operatorname{diag}(p') u )\n    $$\n    Let's define an **adjoint state** $v$ as the solution to the adjoint equation $A(m)^\\top v = P^\\top y$. Since the operator $A(m)$ is real and symmetric, $A(m)^\\top = A(m)$, so the equation is $A(m) v = P^\\top y$. With this definition, $v = (A(m)^{-1})^\\top P^\\top y$. The inner product becomes:\n    $$\n    ( v, \\omega^2 \\operatorname{diag}(p') u ) = \\sum_i v_i (\\omega^2 p'_i u_i) = \\sum_i p'_i (\\omega^2 u_i v_i) = (\\omega^2 u \\odot v, p')\n    $$\n    where $\\odot$ denotes the element-wise (Hadamard) product. By equating the start and end of this chain of equalities, we have $(J^\\top y, p') = (\\omega^2 u \\odot v, p')$. Since this holds for any $p'$, we conclude that:\n    $$\n    J^\\top y = \\omega^2 u \\odot v\n    $$\nCombining both stages, the full Hessian-vector product $J^\\top J p$ is computed with the following algorithm:\n1.  Compute the forward field $u$ by solving $A(m)u = s$.\n2.  Given the perturbation $p$, compute the incremental field $\\delta u$ by solving the tangent-linear equation $A(m) \\delta u = \\omega^2 \\operatorname{diag}(p) u$.\n3.  Compute the incremental data $y = Jp = P \\delta u$.\n4.  Compute the adjoint field $v$ by solving the adjoint equation $A(m) v = P^\\top y$.\n5.  The final Hessian-vector product is given by the element-wise product $J^\\top J p = \\omega^2 u \\odot v$.\n\n### 2. Computational Complexity Analysis\n\nWe compare the number of linear system solves, which is the dominant cost for large $N$. The linear systems involve the $N \\times N$ tridiagonal matrix $A(m)$, and each solve can be performed efficiently in $\\mathcal{O}(N)$ operations using a tridiagonal solver (like the one in `scipy.linalg.solve_banded`).\n\n*   **Jacobian-Free Approach**:\n    -   To compute a single Hessian-vector product $J^\\top J p$ (after the initial forward field $u$ is computed), the algorithm requires:\n        1.  One solve for the incremental field $\\delta u$.\n        2.  One solve for the adjoint field $v$.\n    -   The total is **2 linear solves** per Hessian-vector product. The overall complexity is $\\mathcal{O}(N)$.\n\n*   **Explicit Jacobian Assembly Approach**:\n    -   To construct the full $N_{rec} \\times N$ Jacobian matrix $J$, we must find its action on each of the $N$ canonical basis vectors $e_k$.\n    -   The $k$-th column of $J$ is $J e_k$, which requires solving the tangent-linear equation for the perturbation $p = e_k$.\n    -   This requires **N linear solves** just to assemble the matrix $J$.\n    -   After assembly, computing $J^\\top J p$ involves matrix-vector products, which cost $\\mathcal{O}(N_{rec} N)$.\n    -   The dominant cost is the assembly, with a complexity of $\\mathcal{O}(N^2)$.\n\n*   **Comparison**:\n    -   The Jacobian-free approach requires 2 solves, while the explicit approach requires $N$ solves to obtain the necessary information for one product.\n    -   The ratio of solves is therefore $N/2$, highlighting the immense computational advantage of the adjoint-state method for large-scale problems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef compute_hessian_vector_products(N, L, omega, c, src_idx, rec_indices, p):\n    \"\"\"\n    Computes the Gauss-Newton Hessian-vector product for a 1D FWI problem\n    using both a Jacobian-free (adjoint-state) method and an explicit\n    Jacobian-based method.\n\n    Args:\n        N (int): Number of interior grid points.\n        L (float): Length of the domain.\n        omega (float): Angular frequency in radians/second.\n        c (float): Constant wave speed.\n        src_idx (int): 0-based index of the source.\n        rec_indices (list[int]): List of 0-based indices for receivers.\n        p (np.ndarray): The model perturbation vector.\n\n    Returns:\n        tuple: A tuple containing:\n            - h_JF (np.ndarray): Hessian-vector product from the Jacobian-free method.\n            - h_EX (np.ndarray): Hessian-vector product from the explicit Jacobian method.\n    \"\"\"\n    # Discretization and model setup\n    h = L / (N + 1.0)\n    m = np.full(N, 1.0 / c**2)\n\n    # Construct the Helmholtz operator A in banded format for solve_banded\n    # ab[0,:] is the super-diagonal, ab[1,:] is the main diagonal, ab[2,:] is the sub-diagonal\n    diag = 2.0 / h**2 + omega**2 * m\n    off_diag_val = -1.0 / h**2\n    ab = np.zeros((3, N))\n    ab[0, 1:] = off_diag_val\n    ab[1, :] = diag\n    ab[2, :-1] = off_diag_val\n    \n    # Define the source vector s\n    s = np.zeros(N)\n    s[src_idx] = 1.0\n\n    # --- Common Part: Solve for the forward state u ---\n    # Solve A(m)u = s for u\n    u = solve_banded((1, 1), ab, s)\n\n    # --- Jacobian-Free (Adjoint-State) Method ---\n    # 1. Solve the tangent-linear equation for the incremental state delta_u\n    # A(m) * delta_u = omega^2 * diag(p) * u\n    rhs_tl = omega**2 * p * u\n    delta_u = solve_banded((1, 1), ab, rhs_tl)\n\n    # 2. Action of the sampling operator P to get incremental data delta_d\n    # delta_d = P * delta_u = J * p\n    delta_d = delta_u[rec_indices]\n\n    # 3. Define the adjoint source. W is identity.\n    # s_adj = P^T * W * delta_d = P^T * delta_d\n    s_adj = np.zeros(N)\n    # The action of P.T is to place receiver data back at receiver locations\n    np.add.at(s_adj, rec_indices, delta_d) \n\n    # 4. Solve the adjoint equation for the adjoint state v\n    # A(m)^T * v = s_adj. Since A(m) is symmetric, A(m)^T = A(m).\n    v = solve_banded((1, 1), ab, s_adj)\n\n    # 5. Compute the Hessian-vector product\n    # h_JF = omega^2 * u * v (element-wise product)\n    h_JF = omega**2 * u * v\n\n    # --- Explicit Jacobian Assembly Method ---\n    # Allocate memory for the Jacobian matrix J\n    num_receivers = len(rec_indices)\n    J = np.zeros((num_receivers, N))\n\n    # Assemble each column of J\n    for k in range(N):\n        # The perturbation is the k-th canonical basis vector e_k\n        # The right-hand side for the tangent-linear equation is omega^2 * diag(e_k) * u\n        # This is a vector with a single non-zero entry, omega^2 * u[k], at index k\n        rhs_k = np.zeros(N)\n        rhs_k[k] = omega**2 * u[k]\n\n        # Solve for the k-th incremental state\n        delta_u_k = solve_banded((1, 1), ab, rhs_k)\n\n        # The k-th column of J is the sampled incremental state\n        J[:, k] = delta_u_k[rec_indices]\n\n    # Compute the Hessian-vector product explicitly\n    # h_EX = J^T * W * J * p. Since W is identity, h_EX = J^T * J * p.\n    Jp = J @ p\n    h_EX = J.T @ Jp\n\n    return h_JF, h_EX\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    # Case A: Happy path\n    N_A = 50\n    p_A = np.sin(2 * np.pi * np.arange(N_A) / N_A)\n    case_A = {\n        \"N\": N_A, \"L\": 1.0, \"omega\": 20.0, \"c\": 2.0, \"src_idx\": 10,\n        \"rec_indices\": [20, 35, 45], \"p\": p_A\n    }\n\n    # Case B: Small system boundary case\n    N_B = 5\n    p_B = np.zeros(N_B)\n    p_B[0] = 1.0\n    case_B = {\n        \"N\": N_B, \"L\": 1.0, \"omega\": 5.0, \"c\": 1.0, \"src_idx\": 1,\n        \"rec_indices\": [2], \"p\": p_B\n    }\n\n    # Case C: Edge case with zero perturbation\n    N_C = 30\n    p_C = np.zeros(N_C)\n    case_C = {\n        \"N\": N_C, \"L\": 1.0, \"omega\": 10.0, \"c\": 1.5, \"src_idx\": 5,\n        \"rec_indices\": [10, 20, 25], \"p\": p_C\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    results = []\n    \n    epsilon = 1e-14\n\n    for case in test_cases:\n        h_JF, h_EX = compute_hessian_vector_products(\n            case[\"N\"], case[\"L\"], case[\"omega\"], case[\"c\"],\n            case[\"src_idx\"], case[\"rec_indices\"], case[\"p\"]\n        )\n\n        # Compute relative l2-error\n        error_norm = np.linalg.norm(h_JF - h_EX)\n        h_EX_norm = np.linalg.norm(h_EX)\n        relative_error = error_norm / max(h_EX_norm, epsilon)\n        \n        # Compute complexity ratio\n        complexity_ratio = case[\"N\"] / 2.0\n        \n        results.append(relative_error)\n        results.append(complexity_ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful inversion yields a model that fits the data, but how much can we trust that model? This practice explores the concept of the 'inverse crime,' a common pitfall where using the same discretization to generate synthetic data and to perform the inversion leads to unrealistically optimistic results. By systematically varying the forward and inverse discretizations for a linear inverse problem, you will quantify this optimistic bias and learn to critically assess the reliability of your inversion. This exercise introduces the model resolution matrix, a powerful diagnostic tool for understanding what aspects of the true model are recoverable given your data and inversion setup .",
            "id": "3585154",
            "problem": "Consider a one-dimensional linear geophysical forward model where data $d(x)$ are generated by a smoothing integral operator applied to a spatial model $m(s)$ on the domain $s \\in [0,1]$, so that $d(x) = \\int_{0}^{1} K(x,s) m(s) \\, ds$. The kernel $K(x,s)$ is a Gaussian smoothing kernel of width parameter $\\sigma > 0$, specifically $K(x,s) = \\exp\\!\\big( -\\tfrac{(x - s)^2}{2 \\sigma^2} \\big)$. Assume $m(s)$ is dimensionless and $d(x)$ is dimensionless. The inversion proceeds by discretizing both the data and model spaces, leading to a linear system with matrix operators that depend on the chosen discretizations. \n\nYou will demonstrate the phenomenon known as an \"inverse crime\" (using identical forward and inverse discretizations) and quantify the optimistic bias it induces, by systematically varying the forward/inverse discretization pair and comparing the recovered model and resolution matrix. The derivation and implementation must start from the following fundamental bases:\n- The linear forward relationship $d = G m$ in the discretized setting, where $G$ is the discretized forward operator derived from the continuous integral model.\n- Tikhonov regularization (also known as ridge regression), with quadratic penalty on model roughness, specified by a discrete first-difference operator, and a scalar regularization parameter $\\lambda > 0$.\n\nYour program must implement the following:\n1. Discretize the model domain $[0,1]$ using a set of model nodes $\\{ s_j \\}_{j=1}^{N_m}$, which define the inversion model vector $m \\in \\mathbb{R}^{N_m}$ as nodal values $m_j = m(s_j)$.\n2. Discretize the integral for forward modeling using a quadrature rule defined on a (generally distinct) set of quadrature nodes $\\{ s_k^{(q)} \\}_{k=1}^{N_q}$ with associated weights $\\{ w_k \\}_{k=1}^{N_q}$. Two quadrature types must be supported: trapezoidal and Simpson’s rule (Simpson’s rule requires an odd number of nodes and uniform spacing).\n3. Use linear interpolation to evaluate model nodal values $m_j$ at quadrature nodes $s_k^{(q)}$ when the forward and inverse discretizations use different node sets. This produces a rectangular interpolation matrix that maps from model nodes to quadrature nodes.\n4. Assemble the forward operator matrix $G \\in \\mathbb{R}^{M \\times N_m}$ that maps the inversion model $m$ to a data vector $d \\in \\mathbb{R}^M$ at $M$ data locations $\\{ x_i \\}_{i=1}^{M}$ uniformly spaced on $[0,1]$. Specifically, $G$ must embody the discretized integral $d_i \\approx \\sum_{k=1}^{N_q} K(x_i, s_k^{(q)}) \\, w_k \\, m(s_k^{(q)})$, with $m(s_k^{(q)})$ obtained from model nodes by interpolation if necessary.\n5. Implement Tikhonov inversion with a discrete first-difference penalty. Let $L \\in \\mathbb{R}^{(N_m-1) \\times N_m}$ be the first-difference matrix so that $(L m)_j = m_{j+1} - m_j$. The recovered model must solve the variational problem $\\min_{m} \\| G_{\\text{inv}} m - d \\|_2^2 + \\lambda^2 \\| L m \\|_2^2$, where $G_{\\text{inv}}$ is the forward operator that the inversion assumes. The solution must be computed via linear algebra from first principles.\n6. For resolution analysis, define the resolution matrix $R \\in \\mathbb{R}^{N_m \\times N_m}$ as the linear map from the true model (on the inversion grid) to the recovered model under noise-free data, taking into account potential mismatch between the true forward operator $G_{\\text{fwd}}$ and the inversion operator $G_{\\text{inv}}$. That is, compute $R$ corresponding to the mapping $m \\mapsto \\hat{m}$ when $d = G_{\\text{fwd}} m$ and $\\hat{m}$ solves the Tikhonov problem with $G_{\\text{inv}}$. \n7. Define an analytic \"ground truth\" model $m_{\\text{true}}(s)$ that is continuous and independent of the discretization: $m_{\\text{true}}(s) = \\exp\\!\\big( -\\tfrac{(s - 0.3)^2}{2 \\times 0.02^2} \\big) + 0.5 \\exp\\!\\big( -\\tfrac{(s - 0.7)^2}{2 \\times 0.04^2} \\big) + 0.2 s$. Use this to generate noise-free synthetic data via the chosen forward discretization by evaluating $m_{\\text{true}}$ at the quadrature nodes. Then perform inversion on the chosen inversion grid.\n8. Quantify optimistic bias by reporting numerical metrics:\n   - The relative model error $E = \\| \\hat{m} - m_{\\text{true},\\text{inv}} \\|_2 / \\| m_{\\text{true},\\text{inv}} \\|_2$, where $m_{\\text{true},\\text{inv}} \\in \\mathbb{R}^{N_m}$ is $m_{\\text{true}}$ sampled on the inversion grid.\n   - The normalized Frobenius norm of the resolution deviation $N_R = \\| R - I \\|_F / \\| I \\|_F$, where $I$ is the identity matrix of size $N_m \\times N_m$.\n   - For each non-baseline test case, compute the \"optimism bias\" differences relative to the inverse-crime baseline: $\\Delta E = E - E_{\\text{crime}}$ and $\\Delta N_R = N_R - N_{R,\\text{crime}}$, where the baseline is defined in the test suite below.\n9. Use the following fixed parameters for all runs unless specified in the test suite:\n   - Kernel width $\\sigma = 0.07$.\n   - Number of data locations $M = 50$ with $x_i$ uniformly spaced on $[0,1]$.\n   - Regularization parameter $\\lambda = 10^{-2}$.\n   - Model and quadrature nodes must be uniformly spaced on $[0,1]$.\n\nTest Suite:\nProvide results for the following four discretization pairs. Each pair specifies the inversion model grid size $N_m$, the inversion quadrature type, the forward quadrature grid size $N_q$, and the forward quadrature type.\n\n- Case 1 (Inverse crime baseline): $N_m = 64$, inversion quadrature type = trapezoidal, $N_q = 64$, forward quadrature type = trapezoidal.\n- Case 2 (Forward coarser than inverse): $N_m = 64$, inversion quadrature type = trapezoidal, $N_q = 32$, forward quadrature type = trapezoidal.\n- Case 3 (Forward finer than inverse): $N_m = 64$, inversion quadrature type = trapezoidal, $N_q = 128$, forward quadrature type = trapezoidal.\n- Case 4 (Quadrature mismatch and odd inversion grid): $N_m = 65$, inversion quadrature type = Simpson’s rule, $N_q = 128$, forward quadrature type = trapezoidal.\n\nRequired final output format:\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets. For each case, output the four floats in the following order: $[E, N_R, \\Delta E, \\Delta N_R]$, flattened across cases in the sequence Case 1, Case 2, Case 3, Case 4. For the inverse crime baseline (Case 1), report zero for the biases (i.e., $\\Delta E = 0$ and $\\Delta N_R = 0$). The final output must therefore be a single flat list of $16$ floats in one pair of square brackets. No units are required and no angles appear in this problem.",
            "solution": "The core of the problem is to formulate, solve, and analyze a linear inverse problem under different discretization choices for the forward model and the inversion procedure. We will systematically develop the mathematical and algorithmic framework required.\n\n### 1. The Continuous and Discretized Forward Problem\n\nThe physical process is described by a Fredholm integral of the first kind:\n$$\nd(x) = \\int_{0}^{1} K(x,s) m(s) \\, ds\n$$\nwhere $d(x)$ is the observed data at location $x$, $m(s)$ is the underlying model property at location $s$, and $K(x,s)$ is the kernel function. The kernel is given as a Gaussian:\n$$\nK(x,s) = \\exp\\left( -\\frac{(x - s)^2}{2 \\sigma^2} \\right)\n$$\nThis represents a smoothing operation. To solve this computationally, we must discretize the integral. We approximate the integral using a numerical quadrature rule defined by a set of $N_q$ nodes $\\{s_k^{(q)}\\}_{k=1}^{N_q}$ and corresponding weights $\\{w_k\\}_{k=1}^{N_q}$. For a single data point $d_i = d(x_i)$, the integral is approximated as:\n$$\nd_i \\approx \\sum_{k=1}^{N_q} w_k K(x_i, s_k^{(q)}) m(s_k^{(q)})\n$$\nThe model $m(s)$ is itself represented by its values at a set of $N_m$ discrete nodes $\\{s_j\\}_{j=1}^{N_m}$. We denote this vector of model parameters as $\\mathbf{m} \\in \\mathbb{R}^{N_m}$, where $m_j = m(s_j)$.\n\nA crucial step arises when the quadrature nodes $\\{s_k^{(q)}\\}$ do not coincide with the model nodes $\\{s_j\\}$. To evaluate $m(s_k^{(q)})$, we must interpolate from the model nodes. Using linear interpolation, we can write this relationship as a matrix-vector product. Let $\\mathbf{m}_{\\text{quad}} \\in \\mathbb{R}^{N_q}$ be the vector of model values at the quadrature nodes. This can be expressed in terms of the model parameter vector $\\mathbf{m}$ via a rectangular interpolation matrix $\\mathbf{I} \\in \\mathbb{R}^{N_q \\times N_m}$:\n$$\n\\mathbf{m}_{\\text{quad}} = \\mathbf{I} \\mathbf{m}\n$$\nSubstituting this into the discretized integral equation, we obtain the fully discrete linear forward model. Let $\\mathbf{d} \\in \\mathbb{R}^M$ be the vector of data at $M$ locations $\\{x_i\\}_{i=1}^M$. The relationship is $\\mathbf{d} = \\mathbf{G} \\mathbf{m}$, where $\\mathbf{G} \\in \\mathbb{R}^{M \\times N_m}$ is the forward operator matrix. Its elements $G_{ij}$ are given by:\n$$\nG_{ij} = \\sum_{k=1}^{N_q} w_k K(x_i, s_k^{(q)}) I_{kj}\n$$\nThis can be expressed compactly. Let $\\mathbf{K}' \\in \\mathbb{R}^{M \\times N_q}$ be the matrix with elements $K'_{ik} = K(x_i, s_k^{(q)})$, and $\\mathbf{W} \\in \\mathbb{R}^{N_q \\times N_q}$ be a diagonal matrix of quadrature weights $w_k$. Then the forward operator is $\\mathbf{G} = \\mathbf{K}' \\mathbf{W} \\mathbf{I}$.\n\n### 2. Tikhonov Regularization for Inversion\n\nThe inverse problem of finding $\\mathbf{m}$ from $\\mathbf{d}$ is typically ill-posed, especially for smoothing kernels. We stabilize the inversion using Tikhonov regularization. The estimated model $\\hat{\\mathbf{m}}$ is the one that minimizes a composite objective function:\n$$\nJ(\\mathbf{m}) = \\| \\mathbf{G}_{\\text{inv}} \\mathbf{m} - \\mathbf{d} \\|_2^2 + \\lambda^2 \\| \\mathbf{L} \\mathbf{m} \\|_2^2\n$$\nHere, $\\mathbf{G}_{\\text{inv}}$ is the forward operator matrix assumed by the inversion process, which may differ from the true forward operator $\\mathbf{G}_{\\text{fwd}}$ that generated the data. The parameter $\\lambda > 0$ controls the trade-off between fitting the data and satisfying the prior constraint. The matrix $\\mathbf{L} \\in \\mathbb{R}^{(N_m-1) \\times N_m}$ is a finite-difference operator that penalizes model roughness. For a first-difference operator, its action is $(\\mathbf{L}\\mathbf{m})_j = m_{j+1} - m_j$.\n\nThe objective function $J(\\mathbf{m})$ is quadratic in $\\mathbf{m}$. Its minimum is found by setting its gradient with respect to $\\mathbf{m}$ to zero:\n$$\n\\nabla_{\\mathbf{m}} J(\\mathbf{m}) = 2 \\mathbf{G}_{\\text{inv}}^T (\\mathbf{G}_{\\text{inv}} \\mathbf{m} - \\mathbf{d}) + 2 \\lambda^2 \\mathbf{L}^T \\mathbf{L} \\mathbf{m} = \\mathbf{0}\n$$\nRearranging gives the normal equations for the regularized problem:\n$$\n(\\mathbf{G}_{\\text{inv}}^T \\mathbf{G}_{\\text{inv}} + \\lambda^2 \\mathbf{L}^T \\mathbf{L}) \\mathbf{m} = \\mathbf{G}_{\\text{inv}}^T \\mathbf{d}\n$$\nThe solution for the estimated model $\\hat{\\mathbf{m}}$ is then:\n$$\n\\hat{\\mathbf{m}} = (\\mathbf{G}_{\\text{inv}}^T \\mathbf{G}_{\\text{inv}} + \\lambda^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}_{\\text{inv}}^T \\mathbf{d}\n$$\n\n### 3. Model Resolution Analysis\n\nThe model resolution matrix $\\mathbf{R}$ describes how the estimated model $\\hat{\\mathbf{m}}$ relates to the true model $\\mathbf{m}_{\\text{true}}$. It quantifies the blurring and distortion introduced by the inversion process. We consider noise-free data generated from a model $\\mathbf{m}$ on the inversion grid using the \"true\" forward operator $\\mathbf{G}_{\\text{fwd}}$: $\\mathbf{d} = \\mathbf{G}_{\\text{fwd}} \\mathbf{m}$.\nSubstituting this into the expression for $\\hat{\\mathbf{m}}$:\n$$\n\\hat{\\mathbf{m}} = \\left[ (\\mathbf{G}_{\\text{inv}}^T \\mathbf{G}_{\\text{inv}} + \\lambda^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}_{\\text{inv}}^T \\mathbf{G}_{\\text{fwd}} \\right] \\mathbf{m}\n$$\nThe matrix in brackets is the resolution matrix $\\mathbf{R} \\in \\mathbb{R}^{N_m \\times N_m}$:\n$$\n\\mathbf{R} = (\\mathbf{G}_{\\text{inv}}^T \\mathbf{G}_{\\text{inv}} + \\lambda^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}_{\\text{inv}}^T \\mathbf{G}_{\\text{fwd}}\n$$\nAn \"inverse crime\" occurs when the discretization used for the inversion exactly matches the one assumed for the forward model, i.e., $\\mathbf{G}_{\\text{inv}} = \\mathbf{G}_{\\text{fwd}} = \\mathbf{G}$. In this case, the resolution matrix simplifies to the standard form $\\mathbf{R}_{\\text{crime}} = (\\mathbf{G}^T \\mathbf{G} + \\lambda^2 \\mathbf{L}^T \\mathbf{L})^{-1} \\mathbf{G}^T \\mathbf{G}$. An ideal inversion would yield $\\mathbf{R} = \\mathbf{I}$, the identity matrix, meaning the estimated model perfectly recovers the true model.\n\n### 4. Quantifying Bias and Error\n\nTo quantify the performance of the inversion and the optimistic bias of an inverse crime, we use two metrics:\n1.  **Relative Model Error ($E$)**: This measures the discrepancy between the recovered model $\\hat{\\mathbf{m}}$ and the true model sampled on the inversion grid, $\\mathbf{m}_{\\text{true,inv}}$. The synthetic data $\\mathbf{d}$ is generated from the continuous true model $m_{\\text{true}}(s)$ using the forward quadrature scheme.\n    $$\n    E = \\frac{\\| \\hat{\\mathbf{m}} - \\mathbf{m}_{\\text{true,inv}} \\|_2}{\\| \\mathbf{m}_{\\text{true,inv}} \\|_2}\n    $$\n2.  **Normalized Resolution Deviation ($N_R$)**: This measures how far the resolution matrix $\\mathbf{R}$ deviates from the ideal identity matrix $\\mathbf{I}$, normalized by the norm of the identity matrix. We use the Frobenius norm.\n    $$\n    N_R = \\frac{\\| \\mathbf{R} - \\mathbf{I} \\|_F}{\\| \\mathbf{I} \\|_F} = \\frac{\\sqrt{\\sum_{i,j}(R_{ij} - \\delta_{ij})^2}}{\\sqrt{N_m}}\n    $$\nThe optimistic bias is then quantified by the differences $\\Delta E = E - E_{\\text{crime}}$ and $\\Delta N_R = N_R - N_{R,\\text{crime}}$, where $E_{\\text{crime}}$ and $N_{R,\\text{crime}}$ are the values from the baseline case where $\\mathbf{G}_{\\text{inv}} = \\mathbf{G}_{\\text{fwd}}$. These differences are expected to be positive, indicating that the inverse crime case yields unrealistically low error and resolution deviation values.\n\nThe implementation will proceed by constructing the necessary matrices ($\\mathbf{G}_{\\text{fwd}}$, $\\mathbf{G}_{\\text{inv}}$, $\\mathbf{L}$) for each case, generating the synthetic data from the analytical true model, solving for $\\hat{\\mathbf{m}}$ and $\\mathbf{R}$, and finally computing the specified metrics.",
            "answer": "```python\nimport numpy as np\n\ndef m_true_func(s):\n    \"\"\"\n    Computes the ground truth model m_true(s) at given locations s.\n    \"\"\"\n    s = np.asarray(s)\n    term1 = np.exp(-(s - 0.3)**2 / (2 * 0.02**2))\n    term2 = 0.5 * np.exp(-(s - 0.7)**2 / (2 * 0.04**2))\n    term3 = 0.2 * s\n    return term1 + term2 + term3\n\ndef kernel_func(x, s, sigma):\n    \"\"\"\n    Computes the Gaussian kernel K(x,s).\n    \"\"\"\n    return np.exp(-(x - s)**2 / (2 * sigma**2))\n\ndef get_quadrature(N, q_type):\n    \"\"\"\n    Generates nodes and weights for trapezoidal or Simpson's quadrature on [0,1].\n    \"\"\"\n    if N < 1:\n        return np.array([]), np.array([])\n    if N == 1: # Center point rule\n        return np.array([0.5]), np.array([1.0])\n        \n    nodes = np.linspace(0.0, 1.0, N)\n    h = 1.0 / (N - 1)\n    \n    if q_type.lower() == 'trapezoidal':\n        weights = np.full(N, h)\n        weights[0] *= 0.5\n        weights[-1] *= 0.5\n    elif q_type.lower() == 'simpson':\n        if N % 2 == 0:\n            raise ValueError(\"Simpson's rule requires an odd number of nodes.\")\n        weights = np.ones(N)\n        weights[1:-1:2] = 4.0\n        weights[2:-2:2] = 2.0\n        weights *= h / 3.0\n    else:\n        raise ValueError(f\"Unknown quadrature type: {q_type}\")\n        \n    return nodes, weights\n\ndef build_interp_matrix(target_nodes, source_nodes):\n    \"\"\"\n    Builds the linear interpolation matrix to map values from source_nodes to target_nodes.\n    \"\"\"\n    Nt, Ns = len(target_nodes), len(source_nodes)\n    if Ns > 0 and np.array_equal(target_nodes, source_nodes):\n        return np.identity(Ns)\n    \n    interp_matrix = np.zeros((Nt, Ns))\n    if Ns > 1: # np.interp requires at least 2 points in source_nodes\n        identity_source = np.identity(Ns)\n        for j in range(Ns):\n            interp_matrix[:, j] = np.interp(target_nodes, source_nodes, identity_source[:, j])\n    elif Ns == 1 and Nt > 0: # Constant extrapolation\n        interp_matrix[:, 0] = 1.0\n\n    return interp_matrix\n\ndef assemble_G_matrix(x_nodes, model_nodes, quad_nodes, quad_weights, sigma):\n    \"\"\"\n    Assembles the forward operator matrix G.\n    \"\"\"\n    interp_mat = build_interp_matrix(quad_nodes, model_nodes)\n    \n    kernel_mat = kernel_func(x_nodes[:, None], quad_nodes[None, :], sigma)\n    \n    # G = (K * weights) @ I\n    # K is (M, Nq), weights is (Nq,), I is (Nq, Nm)\n    # (K * weights) is an (M, Nq) matrix\n    G = (kernel_mat * quad_weights) @ interp_mat\n    return G\n\ndef build_L_matrix(Nm):\n    \"\"\"\n    Builds the first-difference matrix L.\n    \"\"\"\n    if Nm <= 1:\n        return np.empty((0, Nm))\n    L = np.eye(Nm - 1, Nm, k=1) - np.eye(Nm - 1, Nm, k=0)\n    return L\n    \ndef process_case(case_params, fixed_params):\n    \"\"\"\n    Runs a full inversion scenario for one test case.\n    \"\"\"\n    Nm, inv_q_type, Nq, fwd_q_type = case_params\n    M, sigma, lambda_reg = fixed_params['M'], fixed_params['sigma'], fixed_params['lambda_reg']\n\n    # 1. Setup grids\n    x_grid = np.linspace(0.0, 1.0, M)\n    s_inv_grid = np.linspace(0.0, 1.0, Nm)\n    \n    # 2. Assemble forward operator for inversion (G_inv)\n    s_inv_quad_grid, w_inv_quad = get_quadrature(Nm, inv_q_type)\n    G_inv = assemble_G_matrix(x_grid, s_inv_grid, s_inv_quad_grid, w_inv_quad, sigma)\n\n    # 3. Assemble forward operator for data generation (G_fwd)\n    s_fwd_quad_grid, w_fwd_quad = get_quadrature(Nq, fwd_q_type)\n    G_fwd = assemble_G_matrix(x_grid, s_inv_grid, s_fwd_quad_grid, w_fwd_quad, sigma)\n    \n    # 4. Generate synthetic data from continuous true model\n    m_true_at_fwd_quad = m_true_func(s_fwd_quad_grid)\n    kernel_for_data = kernel_func(x_grid[:, None], s_fwd_quad_grid[None, :], sigma)\n    d = (kernel_for_data * w_fwd_quad) @ m_true_at_fwd_quad\n\n    # 5. Perform Tikhonov inversion\n    L = build_L_matrix(Nm)\n    H = G_inv.T @ G_inv\n    if L.shape[0] > 0:\n        H += lambda_reg**2 * (L.T @ L)\n    \n    Gtd = G_inv.T @ d\n    m_hat = np.linalg.solve(H, Gtd)\n    \n    # 6. Compute Resolution Matrix R\n    RHS_R = G_inv.T @ G_fwd\n    R = np.linalg.solve(H, RHS_R)\n    \n    # 7. Compute Metrics\n    m_true_inv_grid = m_true_func(s_inv_grid)\n    \n    model_error_norm = np.linalg.norm(m_hat - m_true_inv_grid)\n    true_model_norm = np.linalg.norm(m_true_inv_grid)\n    E = model_error_norm / true_model_norm if true_model_norm > 1e-12 else 0.0\n\n    I = np.identity(Nm)\n    resolution_dev_norm = np.linalg.norm(R - I, 'fro')\n    identity_norm = np.linalg.norm(I, 'fro') # sqrt(Nm)\n    NR = resolution_dev_norm / identity_norm if identity_norm > 1e-12 else 0.0\n    \n    return E, NR\n    \ndef solve():\n    \"\"\"\n    Main solver function to orchestrate the test cases and print results.\n    \"\"\"\n    fixed_params = {\n        'sigma': 0.07,\n        'M': 50,\n        'lambda_reg': 1e-2\n    }\n    \n    test_cases = [\n        # (Nm, inv_q_type, Nq, fwd_q_type)\n        (64, 'trapezoidal', 64, 'trapezoidal'),\n        (64, 'trapezoidal', 32, 'trapezoidal'),\n        (64, 'trapezoidal', 128, 'trapezoidal'),\n        (65, 'Simpson', 128, 'trapezoidal')\n    ]\n\n    E_crime, NR_crime = process_case(test_cases[0], fixed_params)\n    results = [E_crime, NR_crime, 0.0, 0.0]\n\n    for i in range(1, len(test_cases)):\n        E, NR = process_case(test_cases[i], fixed_params)\n        delta_E = E - E_crime\n        delta_NR = NR - NR_crime\n        results.extend([E, NR, delta_E, delta_NR])\n\n    print(f\"[{','.join(f'{x:.8f}' for x in results)}]\")\n\nsolve()\n```"
        }
    ]
}