{
    "hands_on_practices": [
        {
            "introduction": "To truly understand ill-posedness, we must move beyond abstract definitions and see it in action. This first practice provides a direct computational experience with the stability criterion of well-posedness. By constructing a discrete forward operator for a seismic tomography problem and analyzing its singular value spectrum, you will quantitatively diagnose the onset of instability as the model becomes more detailed relative to the available data . This exercise makes the abstract concept of error amplification tangible, showing how the operator norm of the pseudoinverse, $\\|R^\\dagger\\|_2$, is a direct measure of a problem's sensitivity.",
            "id": "3618830",
            "problem": "Consider two-dimensional, straight-ray travel-time tomography in a rectangular domain of width $L_x$ and height $L_y$. Let the model be the cellwise constant slowness field $m \\in \\mathbb{R}^n$ with units seconds per kilometer ($\\mathrm{s}/\\mathrm{km}$), discretized on an $N_x \\times N_y$ uniform grid so that $n = N_x N_y$. For a fixed survey geometry of sources and receivers on the domain boundaries, the data $d \\in \\mathbb{R}^m$ are the travel times in seconds ($\\mathrm{s}$) between each source-receiver pair under the straight-ray approximation. The discrete ray transform $R \\in \\mathbb{R}^{m \\times n}$ maps slowness to data by linear superposition of intersection lengths, i.e., each entry $R_{ij}$ equals the length in kilometers of the $i$-th ray inside the $j$-th cell. The forward model is\n$$\nd = R m.\n$$\nDefine the singular value decomposition (SVD) as the factorization\n$$\nR = U \\Sigma V^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns, $r = \\min(m,n)$, and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r)$ has nonnegative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r \\ge 0$ with units kilometers ($\\mathrm{km}$). The Moore–Penrose pseudoinverse is\n$$\nR^\\dagger = V \\Sigma^\\dagger U^\\top,\n$$\nwhere $\\Sigma^\\dagger = \\mathrm{diag}(1/\\sigma_1, \\ldots, 1/\\sigma_r)$ acts as reciprocal on nonzero singular values and zero on exact zeros. According to the Hadamard definition of well-posedness, stability requires small perturbations in $d$ to produce proportionally small changes in $m$. In this linear setting, the worst-case amplification from data perturbations $\\delta d$ (in seconds, $\\mathrm{s}$) to model perturbations $\\delta m$ (in seconds per kilometer, $\\mathrm{s}/\\mathrm{km}$) via $\n\\delta m = R^\\dagger \\delta d\n$ is controlled by the operator norm of $R^\\dagger$,\n$$\n\\|R^\\dagger\\|_2 = \\frac{1}{\\sigma_{\\min}},\n$$\nexpressed in inverse kilometers ($\\mathrm{km}^{-1}$), where $\\sigma_{\\min}$ denotes the smallest nonzero singular value of $R$. As the mesh is refined ($N_x, N_y$ increase) while the survey geometry (number of rays $m$) is fixed, the matrix $R$ typically develops a growing numerical nullspace, with $\\sigma_{\\min}$ approaching zero and $\\|R^\\dagger\\|_2$ increasing, indicating ill-posedness due to instability.\n\nYour task is to implement the following, starting from these foundational definitions and facts:\n- Construct $R$ by computing exact intersection lengths of straight rays with axis-aligned grid cells. The domain is $[0,L_x] \\times [0,L_y]$ with $L_x = L_y = 1.0$ kilometers, and rays are straight segments between source and receiver points.\n- Use singular value decomposition (SVD) to compute the singular value spectrum of $R$, determine the numerical rank $\\mathrm{rank}(R)$ using the threshold\n$$\n\\tau = \\max(m,n)\\,\\epsilon\\,\\sigma_{\\max},\n$$\nwhere $\\epsilon$ is machine precision for double precision and $\\sigma_{\\max}$ is the largest singular value, and estimate the numerical nullspace dimension $n - \\mathrm{rank}(R)$.\n- Define a deterministic perturbation $\\delta d$ aligned with the left singular vector associated with $\\sigma_{\\min}$, scaled to a magnitude $\\|\\delta d\\|_2 = \\varepsilon$, where $\\varepsilon = 0.005$ seconds. Compute $\\delta m = R^\\dagger \\delta d$ using SVD factors, with reciprocals applied only to singular values exceeding the threshold $\\tau$ and zeros otherwise. Report the amplification ratio\n$$\n\\frac{\\|\\delta m\\|_2}{\\|\\delta d\\|_2},\n$$\nin inverse kilometers ($\\mathrm{km}^{-1}$).\n\nSurvey geometry:\n- Place $N_s$ sources evenly along the left boundary at positions $(0, y_i)$ with $y_i = \\left(i+\\tfrac{1}{2}\\right)\\tfrac{L_y}{N_s}$ for $i=0,\\ldots,N_s-1$ and $N_r$ receivers evenly along the right boundary at positions $(L_x, y_j)$ with $y_j = \\left(j+\\tfrac{1}{2}\\right)\\tfrac{L_y}{N_r}$ for $j=0,\\ldots,N_r-1$. Include all left-to-right source–receiver pairs.\n- Place $N_s$ sources evenly along the bottom boundary at positions $(x_i, 0)$ with $x_i = \\left(i+\\tfrac{1}{2}\\right)\\tfrac{L_x}{N_s}$ and $N_r$ receivers evenly along the top boundary at positions $(x_j, L_y)$. Include all bottom-to-top source–receiver pairs.\n- Use $N_s = N_r = 6$ for all test cases.\n\nTest suite:\n- Case 1 (happy path, mildly overdetermined): $N_x = 8$, $N_y = 8$.\n- Case 2 (underdetermined, moderate refinement): $N_x = 16$, $N_y = 16$.\n- Case 3 (underdetermined, finer refinement): $N_x = 24$, $N_y = 24$.\n- Case 4 (underdetermined, fine refinement boundary): $N_x = 32$, $N_y = 32$.\n\nFor each case, compute and return the list\n$$\n\\left[\\sigma_{\\min}, \\ \\|R^\\dagger\\|_2, \\ \\frac{\\|\\delta m\\|_2}{\\|\\delta d\\|_2}, \\ n - \\mathrm{rank}(R)\\right],\n$$\nwhere $\\sigma_{\\min}$ is the smallest singular value above the threshold $\\tau$ (units kilometers), $\\|R^\\dagger\\|_2$ is the operator $2$-norm of the pseudoinverse (units inverse kilometers), $\\|\\delta m\\|_2 / \\|\\delta d\\|_2$ is the computed amplification ratio (units inverse kilometers), and $n - \\mathrm{rank}(R)$ is the integer numerical nullspace dimension. Express all floating-point outputs rounded to six decimal places, with $\\sigma_{\\min}$ in kilometers and the last two floating-point quantities in inverse kilometers, and the integer quantity as an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, each inner list corresponding to a test case in the specified order, with no spaces, for example:\n$$\n\\texttt{[[0.123456,8.100000,8.100000,12],[\\dots],[\\dots],[\\dots]]}.\n$$",
            "solution": "The forward model for straight-ray travel-time tomography is described by the integral of slowness along ray paths. For a ray $\\gamma$ connecting a source point to a receiver point, the travel time is\n$$\nt_\\gamma = \\int_\\gamma s(\\mathbf{x})\\, \\mathrm{d}\\ell,\n$$\nwhere $s(\\mathbf{x})$ is slowness in seconds per kilometer and $\\mathrm{d}\\ell$ is an infinitesimal path length in kilometers. Discretizing $s(\\mathbf{x})$ on an $N_x \\times N_y$ grid of axis-aligned rectangular cells with constant slowness per cell yields\n$$\nd_i = \\sum_{j=1}^{n} R_{ij} m_j,\n$$\nwhere $d_i$ is the travel time of the $i$-th ray, $m_j$ is the slowness of the $j$-th cell, and $R_{ij}$ is the intersection length in kilometers of ray $i$ within cell $j$. Stacking all ray equations, $d = R m$ with $R \\in \\mathbb{R}^{m \\times n}$.\n\nHadamard's well-posedness demands existence, uniqueness, and stability. In the linear discrete setting, stability in the $2$-norm is governed by the singular value decomposition (SVD) $R = U \\Sigma V^\\top$, with singular values $\\sigma_k \\ge 0$. The Moore–Penrose pseudoinverse $R^\\dagger = V \\Sigma^\\dagger U^\\top$ maps data perturbations to model perturbations. The operator $2$-norm satisfies\n$$\n\\|R^\\dagger\\|_2 = \\max_{\\|x\\|_2=1} \\|R^\\dagger x\\|_2 = \\frac{1}{\\sigma_{\\min}},\n$$\nwhere $\\sigma_{\\min}$ is the smallest nonzero singular value of $R$. Therefore, if $\\sigma_{\\min}$ is small, any data perturbation component aligned with the corresponding left singular vector $u_{\\min}$ is amplified by approximately $1/\\sigma_{\\min}$ in the model space. As the mesh is refined (larger $N_x, N_y$) while the number of rays $m$ is held fixed, the number of unknowns $n$ grows and the matrix $R$ becomes increasingly rank-deficient. More singular values are near zero, and the numerical nullspace dimension $n - \\mathrm{rank}(R)$ increases. This leads to ill-posedness due to instability, as reflected by the growth of $\\|R^\\dagger\\|_2$.\n\nAlgorithmic construction:\n1. Geometry and rays: Define the domain $[0, L_x] \\times [0, L_y]$ with $L_x = L_y = 1.0$ kilometers. Place $N_s = 6$ sources along the left boundary at $(0, y_i)$ with $y_i = \\left(i + \\tfrac{1}{2}\\right) \\tfrac{L_y}{N_s}$ for $i = 0, \\ldots, 5$, and $N_r = 6$ receivers along the right boundary at $(L_x, y_j)$ with $y_j = \\left(j + \\tfrac{1}{2}\\right) \\tfrac{L_y}{N_r}$. Include all $N_s N_r$ left-to-right rays. Similarly, place $N_s$ sources along the bottom boundary at $(x_i, 0)$ with $x_i = \\left(i + \\tfrac{1}{2}\\right) \\tfrac{L_x}{N_s}$ and $N_r$ receivers along the top boundary at $(x_j, L_y)$, including all $N_s N_r$ bottom-to-top rays. The total number of rays is $m = 2 N_s N_r = 72$.\n2. Discrete ray transform: For each ray from point $\\mathbf{p}_0 = (x_0, y_0)$ to $\\mathbf{p}_1 = (x_1, y_1)$, parametrize the line segment as $\\mathbf{p}(t) = \\mathbf{p}_0 + t (\\mathbf{p}_1 - \\mathbf{p}_0)$ with $t \\in [0, 1]$. Compute all intersection parameters $t$ where the ray crosses vertical and horizontal grid lines $x = x_k$ and $y = y_\\ell$. Include $t=0$ and $t=1$. Sort the unique $t$ values within $[0, 1]$ to get subsegments $[t_k, t_{k+1}]$. Each subsegment lies entirely within a single cell. For each subsegment, compute its length as $L_\\mathrm{seg} = \\|\\mathbf{p}_1 - \\mathbf{p}_0\\|_2 (t_{k+1} - t_k)$ in kilometers and assign this length to the corresponding cell index determined by the midpoint $\\mathbf{p}(\\tfrac{t_k + t_{k+1}}{2})$. Accumulate lengths for all segments of the ray to form its row in $R$.\n3. Singular value analysis: Compute the SVD $R = U \\Sigma V^\\top$ with $\\Sigma = \\mathrm{diag}(\\sigma_1, \\ldots, \\sigma_r)$, $r = \\min(m, n)$. Let $\\sigma_{\\max} = \\sigma_1$. Define the numerical threshold\n$$\n\\tau = \\max(m,n) \\, \\epsilon \\, \\sigma_{\\max},\n$$\nwhere $\\epsilon$ is machine precision for double precision (approximately $2.22 \\times 10^{-16}$). The numerical rank is $\\mathrm{rank}(R) = \\#\\{k: \\sigma_k  \\tau \\}$. The numerical nullspace dimension is $n - \\mathrm{rank}(R)$. The smallest nonzero singular value is $\\sigma_{\\min} = \\min\\{\\sigma_k: \\sigma_k  \\tau\\}$.\n4. Pseudoinverse and amplification: Construct $\\Sigma^\\dagger$ by setting $(\\Sigma^\\dagger)_{kk} = 1/\\sigma_k$ if $\\sigma_k  \\tau$, and $(\\Sigma^\\dagger)_{kk} = 0$ otherwise. The pseudoinverse is $R^\\dagger = V \\Sigma^\\dagger U^\\top$. Choose a deterministic perturbation in data space aligned with the left singular vector for $\\sigma_{\\min}$, i.e., $\\delta d = \\varepsilon \\, u_{\\min}$ with $\\|\\delta d\\|_2 = \\varepsilon = 0.005$ seconds. Compute the model perturbation $\\delta m = R^\\dagger \\delta d$. The amplification ratio is\n$$\n\\frac{\\|\\delta m\\|_2}{\\|\\delta d\\|_2} = \\frac{\\|R^\\dagger \\delta d\\|_2}{\\varepsilon},\n$$\nwhich equals $1/\\sigma_{\\min}$ in the ideal case and, numerically, matches $\\|R^\\dagger\\|_2$ when $\\delta d$ is aligned with $u_{\\min}$.\n\nExpected behavior across the test suite:\n- Case 1 ($N_x = N_y = 8$) has $n = 64$ unknowns and $m = 72$ rays; it is mildly overdetermined and generally better conditioned, with a relatively larger $\\sigma_{\\min}$ and smaller $\\|R^\\dagger\\|_2$.\n- Cases 2–4 ($N_x = N_y$ equal to $16$, $24$, $32$) are increasingly underdetermined with $n = 256$, $576$, and $1024$ unknowns but fixed $m = 72$. The numerical nullspace dimension increases, $\\sigma_{\\min}$ typically decreases, and both $\\|R^\\dagger\\|_2$ and the amplification ratio grow, revealing ill-posedness due to instability under mesh refinement.\n\nImplementation details:\n- Units: Entries of $R$ are in kilometers ($\\mathrm{km}$). Singular values $\\sigma_k$ are in kilometers. The operator norm $\\|R^\\dagger\\|_2$ and the amplification ratio $\\|\\delta m\\|_2/\\|\\delta d\\|_2$ are in inverse kilometers ($\\mathrm{km}^{-1}$).\n- Numerical thresholding ensures that near-zero singular values (below $\\tau$) are treated as zero to avoid numerical blow-up from spurious reciprocals.\n- Outputs: For each case, report $\\sigma_{\\min}$ (kilometers), $\\|R^\\dagger\\|_2$ ($\\mathrm{km}^{-1}$), $\\|\\delta m\\|_2/\\|\\delta d\\|_2$ ($\\mathrm{km}^{-1}$), and $n - \\mathrm{rank}(R)$ (integer), rounding the floating-point values to six decimal places. Aggregate all case results into a single list of lists printed on one line without spaces.\n\nThis principled approach connects the core definition of travel-time integrations to a discrete operator, uses the singular value decomposition to infer stability properties, and quantifies ill-posedness through mesh refinement by directly computing how $\\delta d$ amplifies into $\\delta m$ via $R^\\dagger$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_sources_receivers(Lx, Ly, Ns, Nr):\n    # Left-to-right rays\n    ys_src = (np.arange(Ns) + 0.5) * (Ly / Ns)\n    ys_rcv = (np.arange(Nr) + 0.5) * (Ly / Nr)\n    rays_lr = [((0.0, ys), (Lx, yr)) for ys in ys_src for yr in ys_rcv]\n    # Bottom-to-top rays\n    xs_src = (np.arange(Ns) + 0.5) * (Lx / Ns)\n    xs_rcv = (np.arange(Nr) + 0.5) * (Lx / Nr)\n    rays_bt = [((xs, 0.0), (xr, Ly)) for xs in xs_src for xr in xs_rcv]\n    return rays_lr + rays_bt\n\ndef ray_cell_lengths(p0, p1, x_edges, y_edges, nx, ny):\n    x0, y0 = p0\n    x1, y1 = p1\n    dx = x1 - x0\n    dy = y1 - y0\n    # Full ray length\n    L = np.hypot(dx, dy)\n    # Compute intersection parameters t where the ray crosses grid lines\n    t_vals = [0.0, 1.0]\n    if abs(dx)  0.0:\n        tx = (x_edges - x0) / dx\n        # filter within [0,1]\n        t_vals.extend([t for t in tx if 0.0 = t = 1.0])\n    if abs(dy)  0.0:\n        ty = (y_edges - y0) / dy\n        t_vals.extend([t for t in ty if 0.0 = t = 1.0])\n    # Sort unique t's\n    t_vals = np.array(sorted(set(t_vals)))\n    # Accumulate lengths into cells\n    lengths = {}\n    for k in range(len(t_vals) - 1):\n        t0 = t_vals[k]\n        t1 = t_vals[k + 1]\n        if t1 = t0:\n            continue\n        tm = 0.5 * (t0 + t1)\n        xm = x0 + dx * tm\n        ym = y0 + dy * tm\n        # Determine cell indices using edges\n        ix = np.searchsorted(x_edges, xm, side='right') - 1\n        iy = np.searchsorted(y_edges, ym, side='right') - 1\n        if ix  0 or ix = nx or iy  0 or iy = ny:\n            continue\n        seg_len = L * (t1 - t0)\n        idx = ix + iy * nx\n        lengths[idx] = lengths.get(idx, 0.0) + seg_len\n    return lengths\n\ndef build_R_matrix(nx, ny, Lx, Ly, rays):\n    x_edges = np.linspace(0.0, Lx, nx + 1)\n    y_edges = np.linspace(0.0, Ly, ny + 1)\n    num_rays = len(rays)\n    R = np.zeros((num_rays, nx * ny), dtype=float)\n    for i, (p0, p1) in enumerate(rays):\n        lengths = ray_cell_lengths(p0, p1, x_edges, y_edges, nx, ny)\n        if lengths:\n            idxs = np.fromiter(lengths.keys(), dtype=int)\n            vals = np.fromiter(lengths.values(), dtype=float)\n            R[i, idxs] = vals\n    return R\n\ndef svd_metrics(R, eps= np.finfo(float).eps):\n    # Compute SVD\n    U, s, Vt = np.linalg.svd(R, full_matrices=False)\n    m, n = R.shape\n    smax = s[0] if s.size  0 else 0.0\n    tau = max(m, n) * eps * smax\n    # Numerical rank and nullity\n    above = s  tau\n    rank = int(np.sum(above))\n    nullity = n - rank\n    if rank  0:\n        smin_nonzero = float(np.min(s[above]))\n        # index of smallest nonzero singular value\n        idx_min = int(np.argmin(np.where(above, s, np.inf)))\n    else:\n        smin_nonzero = 0.0\n        idx_min = None\n    # Operator norm of pseudoinverse\n    op_norm_pinv = np.inf if smin_nonzero == 0.0 else (1.0 / smin_nonzero)\n    return U, s, Vt, smin_nonzero, op_norm_pinv, tau, rank, nullity, idx_min\n\ndef apply_pseudoinverse(U, s, Vt, tau, delta_d):\n    # Build filtered reciprocal of singular values\n    inv_s = np.zeros_like(s)\n    mask = s  tau\n    inv_s[mask] = 1.0 / s[mask]\n    # Compute R^dagger * delta_d via SVD factors: V * diag(inv_s) * U^T * delta_d\n    Ut_dd = U.T @ delta_d\n    temp = inv_s * Ut_dd\n    delta_m = Vt.T @ temp\n    return delta_m\n\ndef format_results(results):\n    # Format nested list without spaces, floats rounded to 6 decimals\n    parts = []\n    for sm, opn, ampr, nullity in results:\n        parts.append(f\"[{sm:.6f},{opn:.6f},{ampr:.6f},{int(nullity)}]\")\n    return \"[\" + \",\".join(parts) + \"]\"\n\ndef solve():\n    # Define parameters\n    Lx = 1.0  # kilometers\n    Ly = 1.0  # kilometers\n    Ns = 6\n    Nr = 6\n    rays = generate_sources_receivers(Lx, Ly, Ns, Nr)\n    # Test suite: list of (Nx, Ny)\n    test_cases = [\n        (8, 8),    # Case 1: happy path, mildly overdetermined\n        (16, 16),  # Case 2: underdetermined, moderate refinement\n        (24, 24),  # Case 3: underdetermined, finer refinement\n        (32, 32),  # Case 4: underdetermined, fine refinement boundary\n    ]\n    epsilon = 0.005  # seconds magnitude for ||delta d||_2\n\n    results = []\n    for Nx, Ny in test_cases:\n        R = build_R_matrix(Nx, Ny, Lx, Ly, rays)\n        U, s, Vt, smin, opnorm, tau, rank, nullity, idx_min = svd_metrics(R)\n        # Define delta d aligned with left singular vector associated with smallest nonzero singular value\n        if idx_min is not None:\n            u_min = U[:, idx_min]\n            # Scale to have ||delta d||_2 = epsilon seconds\n            delta_d = (epsilon / np.linalg.norm(u_min)) * u_min\n            delta_m = apply_pseudoinverse(U, s, Vt, tau, delta_d)\n            amp_ratio = np.linalg.norm(delta_m) / np.linalg.norm(delta_d)\n        else:\n            # No nonzero singular values; set amplification to infinity\n            amp_ratio = np.inf\n        results.append((smin, opnorm, amp_ratio, nullity))\n\n    print(format_results(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Instability is not the only challenge; non-uniqueness poses an equally fundamental problem, where infinitely many models can explain the data perfectly. This exercise isolates the issue of non-uniqueness by having you analyze the null-space of a simplified tomography operator . You will use the rank-nullity theorem to determine precisely which aspects of the model are unconstrained by the data and explore how a clever reparameterization can restore uniqueness, effectively focusing the inversion on only what can be known.",
            "id": "3618829",
            "problem": "A two-dimensional acoustic travel-time tomography experiment is conducted over a rectangular area with a limited source-receiver aperture. The slowness model is a perturbation field $m(x,z)$ defined on a uniform grid with $N_x$ lateral columns and $N_z$ depth layers. Under the high-frequency (geometric) approximation and for near-vertical rays (small maximum opening angle), the first-order travel-time perturbation at lateral position $x_i$ is well-approximated by a vertically integrated sensitivity of the form\n$$\nd_i \\approx \\int_{0}^{Z_{\\max}} w(z)\\, m(x_i,z)\\, dz,\n$$\nwhere $w(z) \\ge 0$ is a known depth-dependent weight reflecting path length and sensitivity for the limited-aperture geometry. After discretization, this results in the linear forward map\n$$\nd_i = \\sum_{j=1}^{N_z} w_j\\, m_{i,j}, \\quad i = 1,\\dots,N_x,\n$$\nwhere $w_j  0$ for all $j$ and $m_{i,j}$ is the model value in column $i$ and layer $j$. Let the model vector be the column-stacked array $m \\in \\mathbb{R}^{N_x N_z}$ and the forward operator $A \\in \\mathbb{R}^{N_x \\times N_x N_z}$ satisfy\n$$\n(A m)_i = \\sum_{j=1}^{N_z} w_j\\, m_{i,j},\n$$\nthat is, $A$ has $N_x$ disjoint row blocks, each equal to the row vector $w^{\\top} \\in \\mathbb{R}^{N_z}$ acting on the layers of a single column.\n\n- Using only core definitions from linear algebra and the stated discretized forward map, determine the dimension of the model null-space $\\mathcal{N}(A)$ for the case $N_x = 42$ and $N_z = 27$.\n\n- Propose a reparameterization that reduces the null-space by selecting parameters that retain only the component of $m$ observable under the limited-aperture geometry. Then, compute the dimension of the null-space for the reparameterized model.\n\nReport your final result as a two-entry row matrix containing, in order, the original null-space dimension and the reparameterized null-space dimension. No rounding is required.",
            "solution": "The problem asks for the dimension of the null-space of a linear forward operator $A$ associated with a simplified travel-time tomography experiment, first for the original parameterization and then for a reparameterized model.\n\nFirst, we validate the problem statement.\nThe problem provides a discretized linear forward map, $d_i = \\sum_{j=1}^{N_z} w_j m_{i,j}$, which models a simplified 2D acoustic tomography experiment. The model vector $m \\in \\mathbb{R}^{N_x N_z}$ represents the slowness perturbation on a grid, and the data vector $d \\in \\mathbb{R}^{N_x}$ represents travel-time perturbations. The forward operator $A$ maps the model space to the data space, $d = Am$. The structure of $A$ is explicitly defined, along with the dimensions $N_x = 42$ and $N_z = 27$, and the condition that the weights $w_j  0$. The questions posed are precise and can be answered using principles of linear algebra. The context is a standard, albeit simplified, scenario in computational geophysics. The problem is scientifically grounded, self-contained, and well-posed. The verdict is that the problem is valid.\n\nWe proceed with the solution in two parts.\n\nPart 1: Dimension of the null-space $\\mathcal{N}(A)$ for the original model.\n\nThe dimension of the null-space of a linear operator (or matrix) is related to its rank and the dimension of its domain by the rank-nullity theorem:\n$$\n\\text{dim}(\\text{domain of } A) = \\text{rank}(A) + \\text{dim}(\\mathcal{N}(A))\n$$\nThe domain of the operator $A$ is the space of all possible model vectors $m$. The model vector $m$ is a column-stacked array of the slowness values $m_{i,j}$ for $i=1,\\dots,N_x$ and $j=1,\\dots,N_z$. The total number of parameters in the model is $N_x N_z$. Therefore, the model space is $\\mathbb{R}^{N_x N_z}$, and its dimension is:\n$$\n\\text{dim}(\\text{domain of } A) = N_x N_z\n$$\nThe operator $A$ is a matrix of size $N_x \\times (N_x N_z)$. Its action on the model vector $m$ is given by $(A m)_i = \\sum_{j=1}^{N_z} w_j m_{i,j}$ for each row $i=1,\\dots,N_x$. Let us denote the model vector as a concatenation of column vectors $m_i \\in \\mathbb{R}^{N_z}$, where $m_i = (m_{i,1}, \\dots, m_{i,N_z})^{\\top}$. Let $w = (w_1, \\dots, w_{N_z})^{\\top}$. The forward problem for the $i$-th data point is $d_i = w^{\\top} m_i$.\n\nThe matrix $A$ can be expressed in block form. The $i$-th row of $A$ acts only on the $i$-th block of the model vector, $m_i$. The portion of the $i$-th row corresponding to the entries of $m_i$ is the row vector $w^{\\top}$. All other entries in the $i$-th row are zero. Thus, the matrix $A$ has the structure:\n$$\nA = \\begin{pmatrix}\nw^{\\top}  \\mathbf{0}  \\cdots  \\mathbf{0} \\\\\n\\mathbf{0}  w^{\\top}  \\cdots  \\mathbf{0} \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n\\mathbf{0}  \\mathbf{0}  \\cdots  w^{\\top}\n\\end{pmatrix}\n$$\nwhere each $w^{\\top}$ is a $1 \\times N_z$ row vector and each $\\mathbf{0}$ is a $1 \\times N_z$ zero vector.\n\nThe rank of $A$ is the dimension of its row space, which is the number of linearly independent rows. The $i$-th row of $A$ has non-zero entries only in columns $(i-1)N_z + 1$ to $iN_z$. The $k$-th row ($k \\neq i$) has non-zero entries only in columns $(k-1)N_z + 1$ to $kN_z$. Since these sets of column indices are disjoint for different rows, the rows are linearly independent, provided no row is the zero vector.\nThe problem states that $w_j  0$ for all $j=1,\\dots,N_z$. This implies that the vector $w$ is not the zero vector, and thus the row vector $w^{\\top}$ is not the zero vector. Consequently, all $N_x$ rows of $A$ are non-zero and linearly independent.\nThe rank of $A$ is therefore equal to the number of rows:\n$$\n\\text{rank}(A) = N_x\n$$\nWe now apply the rank-nullity theorem to find the dimension of the null-space $\\mathcal{N}(A)$:\n$$\n\\text{dim}(\\mathcal{N}(A)) = \\text{dim}(\\text{domain of } A) - \\text{rank}(A) = N_x N_z - N_x = N_x (N_z - 1)\n$$\nSubstituting the given values $N_x = 42$ and $N_z = 27$:\n$$\n\\text{dim}(\\mathcal{N}(A)) = 42 \\times (27 - 1) = 42 \\times 26 = 1092\n$$\nThe null-space consists of all model perturbations that are invisible to the data. For each column $i$, any perturbation $\\delta m_i$ satisfying $w^{\\top} \\delta m_i=0$ is in the null-space. For each column, this is one linear constraint on $N_z$ variables, leaving a subspace of dimension $N_z-1$. Since there are $N_x$ independent columns, the total dimension of the null-space is $N_x(N_z-1)$.\n\nPart 2: Reparameterization and the new null-space dimension.\n\nThe problem asks for a reparameterization that retains only the component of $m$ observable by the experiment. The data $d_i$ for each column $i$ is precisely the weighted average $d_i = \\sum_{j=1}^{N_z} w_j m_{i,j}$. This single value for each column is the only information that can be retrieved about that column's slowness profile.\n\nA natural reparameterization is to define a new model vector, let's call it $\\tilde{m}$, whose components are these observable quantities themselves. Let the new model parameters be $\\tilde{m}_i$ for $i=1,\\dots,N_x$, defined as:\n$$\n\\tilde{m}_i = \\sum_{j=1}^{N_z} w_j m_{i,j}\n$$\nThe new model vector is $\\tilde{m} = (\\tilde{m}_1, \\dots, \\tilde{m}_{N_x})^{\\top} \\in \\mathbb{R}^{N_x}$. The forward problem, expressed in terms of this new model, becomes:\n$$\nd_i = \\tilde{m}_i\n$$\nIn matrix form, this is $d = \\tilde{A} \\tilde{m}$, where $\\tilde{A}$ is the new forward operator. From the equation $d_i = \\tilde{m}_i$, it is clear that $\\tilde{A}$ is the identity matrix of size $N_x \\times N_x$:\n$$\n\\tilde{A} = I_{N_x}\n$$\nThe null-space of this new operator, $\\mathcal{N}(\\tilde{A})$, consists of all vectors $\\tilde{m} \\in \\mathbb{R}^{N_x}$ such that $\\tilde{A}\\tilde{m} = \\mathbf{0}$.\n$$\nI_{N_x} \\tilde{m} = \\mathbf{0} \\implies \\tilde{m} = \\mathbf{0}\n$$\nThe only vector in the null-space is the zero vector. The null-space is the trivial space $\\{\\mathbf{0}\\}$.\nThe dimension of the null-space for the reparameterized model is therefore:\n$$\n\\text{dim}(\\mathcal{N}(\\tilde{A})) = 0\n$$\nThis reparameterization has effectively eliminated the null-space by reformulating the problem to solve only for the parts of the model that the data can constrain. The resulting inverse problem for $\\tilde{m}$ would be well-posed.\n\nThe two requested values are the original null-space dimension, $1092$, and the reparameterized null-space dimension, $0$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1092  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having diagnosed instability and non-uniqueness, a natural step is to seek a cure, often through regularization. This final practice delves into the philosophical and practical implications of this approach using a thought-provoking counterfactual scenario . It forces a crucial distinction between achieving mathematical well-posedness—which a Tikhonov-style prior can provide—and ensuring the physical plausibility of the solution, revealing that the two are not always equivalent.",
            "id": "3618847",
            "problem": "In linearized seismic traveltime tomography, the unknown slowness field $s \\in \\mathbb{R}^n$ (with physical requirement $s_j \\ge 0$ for all $j$) is related to data $d \\in \\mathbb{R}^m$ via a discretized forward operator $G \\in \\mathbb{R}^{m \\times n}$ by $d = G s + \\varepsilon$, where $\\varepsilon$ is observational noise. Assume that the noise is mean-zero Gaussian with covariance $C_n \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. Because ray coverage is limited and $G$ effectively smooths $s$, the inverse map is unstable under small perturbations of $d$ when posed as an unregularized least-squares problem.\n\nConsider the following counterfactual: instead of imposing the physically correct positivity set $\\{ s \\in \\mathbb{R}^n : s_j \\ge 0 \\ \\forall j \\}$, you adopt a zero-mean Gaussian prior $s \\sim \\mathcal{N}(0, C_s)$ with covariance $C_s \\in \\mathbb{R}^{n \\times n}$, where the prior precision $R := C_s^{-1}$ exists, is symmetric, and satisfies the coercivity condition $\\langle R x, x \\rangle \\ge \\alpha \\|x\\|_2^2$ for all $x \\in \\mathbb{R}^n$ and some $\\alpha  0$. This prior assigns nonzero probability to negative components of $s$, thereby violating the physics of the slowness being nonnegative, but is informative in the sense that it penalizes rough or large-norm models through $R$.\n\nUsing only the standard definitions of Hadamard well-posedness (existence, uniqueness, and stability, where stability means continuous dependence of the solution on the data in appropriate norms) and basic properties of linear algebra for symmetric positive definite matrices, select all statements that are true about this setup.\n\nA. If the prior precision $R$ is coercive as specified and $C_n$ is symmetric positive definite, then the estimator obtained from combining the likelihood and this Gaussian prior defines a linear solution map $d \\mapsto \\hat{s}$ that is Lipschitz continuous with respect to the data norm, thus stabilizing the inversion, even though the prior admits negative components of $s$.\n\nB. Because the Gaussian prior allows $s_j  0$, the inverse problem necessarily remains ill-posed in the sense of Hadamard; in particular, stability cannot hold when the prior violates the physical positivity constraint.\n\nC. Well-posedness is a property of the mathematical formulation and chosen norms; an estimator can be well-posed (existence, uniqueness, stability) without ensuring $s_j \\ge 0$. Therefore, mathematical well-posedness is not equivalent to physical plausibility.\n\nD. In the linear Gaussian setting with fixed informative Gaussian prior and fixed survey geometry, as the noise level tends to $0$, the estimator necessarily converges in norm to the true nonnegative slowness $s^\\star$, because the likelihood dominates any unphysical prior bias in all linear inverse problems, including those arising from compact forward operators.\n\nE. Replacing the Gaussian prior by the positivity constraint set $\\{ s \\in \\mathbb{R}^n : s_j \\ge 0 \\ \\forall j \\}$ always makes the problem ill-posed, because that constraint set is nonconvex and forces nonuniqueness in the solution.",
            "solution": "The user has requested a critical validation of the problem statement, followed by a full derivation and evaluation of the provided options.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Unknown: Slowness field $s \\in \\mathbb{R}^n$.\n- Physical Constraint: $s_j \\ge 0$ for all $j$.\n- Data: $d \\in \\mathbb{R}^m$.\n- Forward Operator: $G \\in \\mathbb{R}^{m \\times n}$.\n- Forward Model: $d = G s + \\varepsilon$.\n- Noise: $\\varepsilon$ is a mean-zero Gaussian random vector with covariance $C_n \\in \\mathbb{R}^{m \\times m}$.\n- Noise Covariance Property: $C_n$ is symmetric positive definite.\n- Problem Characteristic: The unregularized inverse problem is unstable.\n- Counterfactual Prior: A zero-mean Gaussian prior is adopted for $s$, i.e., $s \\sim \\mathcal{N}(0, C_s)$, where $C_s \\in \\mathbb{R}^{n \\times n}$.\n- Prior Precision: $R := C_s^{-1}$ exists, is symmetric, and is coercive, meaning there exists a constant $\\alpha  0$ such that $\\langle R x, x \\rangle \\ge \\alpha \\|x\\|_2^2$ for all $x \\in \\mathbb{R}^n$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is set in the context of linearized seismic traveltime tomography, a standard topic in computational geophysics. The mathematical framework, involving a linear forward model, Gaussian noise, and a Gaussian prior, represents a classic Bayesian approach to solving linear inverse problems, leading to Tikhonov regularization. The use of a \"counterfactual\" prior that violates a physical constraint is a valid theoretical construct for exploring the properties of different mathematical formulations. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The question is well-posed. It asks for an analysis of a specific mathematical setup against the clear and established criteria of Hadamard well-posedness.\n- **Objective:** The language is formal, precise, and objective. It relies on standard mathematical definitions and avoids ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is self-contained, scientifically grounded, and objective. There are no contradictions, missing information, or other flaws that would prevent a rigorous analysis. I will proceed to the solution.\n\n**Derivation of the Estimator and Analysis of Well-Posedness**\n\nThe problem combines a Gaussian likelihood with a Gaussian prior. In a Bayesian framework, the solution is typically taken as the Maximum A Posteriori (MAP) estimate, which maximizes the posterior probability density $p(s|d)$. By Bayes' theorem, $p(s|d) \\propto p(d|s) p(s)$. Maximizing the posterior is equivalent to minimizing its negative logarithm.\n\nThe likelihood function, derived from the Gaussian noise model $\\varepsilon \\sim \\mathcal{N}(0, C_n)$, is:\n$$ p(d|s) \\propto \\exp\\left(-\\frac{1}{2} (d - Gs)^T C_n^{-1} (d - Gs)\\right) $$\nThe prior probability for $s \\sim \\mathcal{N}(0, C_s)$ is:\n$$ p(s) \\propto \\exp\\left(-\\frac{1}{2} s^T C_s^{-1} s\\right) = \\exp\\left(-\\frac{1}{2} s^T R s\\right) $$\nThe negative log-posterior function to be minimized is therefore:\n$$ J(s) = \\frac{1}{2} (d - Gs)^T C_n^{-1} (d - Gs) + \\frac{1}{2} s^T R s $$\nThis is a quadratic function of $s$. To find the minimum, we compute the gradient with respect to $s$ and set it to zero:\n$$ \\nabla_s J(s) = -G^T C_n^{-1} (d - Gs) + R s $$\n$$ \\nabla_s J(s) = (G^T C_n^{-1} G + R)s - G^T C_n^{-1} d $$\nSetting the gradient to zero gives the normal equations for the MAP estimator $\\hat{s}$:\n$$ (G^T C_n^{-1} G + R) \\hat{s} = G^T C_n^{-1} d $$\nLet $H = G^T C_n^{-1} G + R$. The estimator is given by $\\hat{s} = H^{-1} G^T C_n^{-1} d$, provided $H$ is invertible.\n\nNow, we check the Hadamard criteria for well-posedness (existence, uniqueness, stability) for the solution map $d \\mapsto \\hat{s}$.\n\n1.  **Existence and Uniqueness**: A unique solution $\\hat{s}$ exists for any data vector $d \\in \\mathbb{R}^m$ if and only if the matrix $H = G^T C_n^{-1} G + R$ is invertible.\n    - It is given that $C_n$ is symmetric positive definite (SPD), so its inverse $C_n^{-1}$ is also SPD.\n    - The matrix $G^T C_n^{-1} G$ is symmetric positive semi-definite (SPSD), because for any $x \\in \\mathbb{R}^n$, we have $\\langle (G^T C_n^{-1} G)x, x \\rangle = (Gx)^T C_n^{-1} (Gx) = \\langle C_n^{-1} y, y \\rangle$ where $y = Gx$. Since $C_n^{-1}$ is SPD, this quantity is $\\ge 0$.\n    - It is given that the prior precision $R$ is symmetric and coercive, i.e., $\\langle Rx, x \\rangle \\ge \\alpha \\|x\\|_2^2$ for some $\\alpha  0$. This implies that $R$ is SPD.\n    - The matrix $H$ is the sum of an SPSD matrix ($G^T C_n^{-1} G$) and an SPD matrix ($R$). The sum of an SPSD matrix and an SPD matrix is always SPD. Specifically, for any non-zero $x \\in \\mathbb{R}^n$:\n      $$ \\langle Hx, x \\rangle = \\langle (G^T C_n^{-1} G)x, x \\rangle + \\langle Rx, x \\rangle \\ge 0 + \\alpha \\|x\\|_2^2  0 $$\n    - Since $H$ is SPD, it is invertible. Therefore, a unique solution $\\hat{s}$ exists for any given $d \\in \\mathbb{R}^m$.\n\n2.  **Stability**: Stability requires that the solution $\\hat{s}$ depends continuously on the data $d$. The solution map is given by the linear transformation $\\hat{s}(d) = M d$, where $M = (G^T C_n^{-1} G + R)^{-1} G^T C_n^{-1}$.\n    - In finite-dimensional vector spaces, any linear map is Lipschitz continuous. For two data vectors $d_1, d_2 \\in \\mathbb{R}^m$, the corresponding solutions $\\hat{s}_1, \\hat{s}_2$ satisfy:\n      $$ \\|\\hat{s}_1 - \\hat{s}_2\\|_2 = \\|M d_1 - M d_2\\|_2 = \\|M(d_1 - d_2)\\|_2 \\le \\|M\\|_{op} \\|d_1 - d_2\\|_2 $$\n      where $\\|M\\|_{op}$ is the finite operator norm of the matrix $M$.\n    - This Lipschitz continuity is a strong form of continuous dependence on the data. Thus, the solution is stable.\n\nConclusion: The formulation of the inverse problem using the specified Gaussian prior leads to a MAP estimator that is well-posed in the sense of Hadamard.\n\n**Option-by-Option Analysis**\n\n**A. If the prior precision $R$ is coercive as specified and $C_n$ is symmetric positive definite, then the estimator obtained from combining the likelihood and this Gaussian prior defines a linear solution map $d \\mapsto \\hat{s}$ that is Lipschitz continuous with respect to the data norm, thus stabilizing the inversion, even though the prior admits negative components of $s$.**\nOur analysis confirms every part of this statement. The estimator is indeed $\\hat{s} = (G^T C_n^{-1} G + R)^{-1} G^T C_n^{-1} d$, a linear map from $d$ to $\\hat{s}$. The coercivity of $R$ ensures the key matrix is invertible, guaranteeing existence and uniqueness. The linearity of the map in a finite-dimensional setting ensures Lipschitz continuity, which means the solution is stable. This regularization stabilizes the inversion, which was originally unstable. This mathematical property holds regardless of the physical plausibility (or lack thereof) of the Gaussian prior, which indeed assigns non-zero probability to unphysical negative slowness values.\nVerdict: **Correct**.\n\n**B. Because the Gaussian prior allows $s_j  0$, the inverse problem necessarily remains ill-posed in the sense of Hadamard; in particular, stability cannot hold when the prior violates the physical positivity constraint.**\nThis statement is incorrect. It confuses the mathematical property of well-posedness with the physical property of plausibility. As demonstrated above, the problem *as formulated* with the Gaussian prior is well-posed: a unique, stable solution exists. The fact that this solution may not satisfy the physical constraint $s_j \\ge 0$ does not negate the mathematical well-posedness of the chosen formulation. Stability is achieved through the regularization term $s^T R s$, which makes the total objective function strongly convex.\nVerdict: **Incorrect**.\n\n**C. Well-posedness is a property of the mathematical formulation and chosen norms; an estimator can be well-posed (existence, uniqueness, stability) without ensuring $s_j \\ge 0$. Therefore, mathematical well-posedness is not equivalent to physical plausibility.**\nThis statement accurately describes a fundamental concept in inverse problem theory, which is perfectly illustrated by this problem. Our analysis for option A shows that the estimator is well-posed. However, the estimator $\\hat{s} = M d$ is a linear combination of the data elements, and there is no guarantee that all components $\\hat{s}_j$ will be non-negative for an arbitrary data vector $d$. Therefore, the mathematically well-posed solution can be physically implausible. The statement correctly distinguishes between these two concepts.\nVerdict: **Correct**.\n\n**D. In the linear Gaussian setting with fixed informative Gaussian prior and fixed survey geometry, as the noise level tends to $0$, the estimator necessarily converges in norm to the true nonnegative slowness $s^\\star$, because the likelihood dominates any unphysical prior bias in all linear inverse problems, including those arising from compact forward operators.**\nThis statement is false. The regularized solution is biased. The estimator is $\\hat{s} = (G^T C_n^{-1} G + R)^{-1} G^T C_n^{-1} d$. Let the true data be $d_{true} = Gs^\\star$. As noise tends to zero, $d \\to d_{true}$. The estimator converges to $\\hat{s}_{limit} = (G^T C_n^{-1} G + R)^{-1} G^T C_n^{-1} Gs^\\star$. This is not equal to $s^\\star$ in general. Specifically, if the operator $G$ has a non-trivial nullspace (i.e., $\\ker(G) \\neq \\{0\\}$), which is typical for problems where \"ray coverage is limited\", the data contains no information about the nullspace components of $s^\\star$. The behavior of the solution in the nullspace is determined entirely by the prior. With a zero-mean prior, the solution's nullspace components are biased towards zero, regardless of their true values in $s^\\star$. The likelihood never \"dominates\" the prior in the nullspace because it is completely insensitive to it. Therefore, the estimator does not necessarily converge to $s^\\star$.\nVerdict: **Incorrect**.\n\n**E. Replacing the Gaussian prior by the positivity constraint set $\\{ s \\in \\mathbb{R}^n : s_j \\ge 0 \\ \\forall j \\}$ always makes the problem ill-posed, because that constraint set is nonconvex and forces nonuniqueness in the solution.**\nThis statement is incorrect because its premise is factually wrong. The constraint set $S = \\{ s \\in \\mathbb{R}^n : s_j \\ge 0 \\ \\forall j \\}$, known as the non-negative orthant, is a classic example of a **convex** set. For any $s_1, s_2 \\in S$ and any $\\lambda \\in [0, 1]$, the convex combination $s_3 = \\lambda s_1 + (1-\\lambda)s_2$ has components $(s_3)_j = \\lambda (s_1)_j + (1-\\lambda)(s_2)_j \\ge 0$, so $s_3 \\in S$. Since the justification (\"because that constraint set is nonconvex\") is false, the entire statement is invalid as a logical proposition. While solving the problem with only a positivity constraint can still result in an ill-posed problem (e.g., non-unique or unstable solutions), the reason provided in the option is incorrect.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}