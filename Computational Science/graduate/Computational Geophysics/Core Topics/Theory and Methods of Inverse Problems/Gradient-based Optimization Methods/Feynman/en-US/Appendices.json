{
    "hands_on_practices": [
        {
            "introduction": "Geophysical inverse problems are often ill-posed, meaning their solution can be highly sensitive to noise in the data. This manifests as an ill-conditioned Hessian matrix in the objective function, which can drastically slow the convergence of gradient-based methods. This practice  provides a hands-on coding exercise to explore Tikhonov regularization, a fundamental technique for mitigating ill-conditioning by balancing data misfit with solution simplicity, and to directly observe its impact on convergence behavior.",
            "id": "3601017",
            "problem": "Consider a linear inverse problem frequently encountered in computational geophysics in which a model parameter vector $\\mathbf{m} \\in \\mathbb{R}^n$ is related to observed data $\\mathbf{d} \\in \\mathbb{R}^p$ through a forward operator $\\mathbf{G} \\in \\mathbb{R}^{p \\times n}$. A standard quadratic objective function with Tikhonov regularization is defined as\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}\\left\\|\\mathbf{C}_d^{-\\frac{1}{2}}(\\mathbf{G}\\mathbf{m} - \\mathbf{d})\\right\\|_2^2 + \\frac{\\lambda}{2}\\left\\|\\mathbf{L}\\mathbf{m}\\right\\|_2^2,\n$$\nwhere $\\mathbf{C}_d \\in \\mathbb{R}^{p \\times p}$ is a symmetric positive definite data covariance matrix, $\\mathbf{L} \\in \\mathbb{R}^{q \\times n}$ is a regularization operator, and $\\lambda > 0$ is the trade-off parameter that balances data misfit and regularization. The gradient of $J(\\mathbf{m})$ is\n$$\n\\nabla J(\\mathbf{m}) = \\mathbf{G}^\\top\\mathbf{C}_d^{-1}(\\mathbf{G}\\mathbf{m} - \\mathbf{d}) + \\lambda \\mathbf{L}^\\top \\mathbf{L}\\mathbf{m},\n$$\nand, for linear $\\mathbf{G}$ and quadratic regularization, the Hessian is the symmetric positive definite matrix\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top\\mathbf{C}_d^{-1}\\mathbf{G} + \\lambda \\mathbf{L}^\\top \\mathbf{L}.\n$$\nThe spectral conditioning of $\\mathbf{H}(\\lambda)$ is quantified by the condition number\n$$\n\\kappa(\\mathbf{H}(\\lambda)) = \\frac{\\lambda_{\\max}(\\mathbf{H}(\\lambda))}{\\lambda_{\\min}(\\mathbf{H}(\\lambda))},\n$$\nwhere $\\lambda_{\\max}$ and $\\lambda_{\\min}$ denote the largest and smallest eigenvalues respectively. In gradient-based methods such as steepest descent with a fixed step length $\\tau > 0$, the update is\n$$\n\\mathbf{m}_{k+1} = \\mathbf{m}_k - \\tau \\nabla J(\\mathbf{m}_k),\n$$\nand the contraction factor for a quadratic objective is controlled by the spectrum of $\\mathbf{H}(\\lambda)$ via the term $\\left|1 - \\tau \\lambda_i(\\mathbf{H}(\\lambda))\\right|$ for each eigenvalue $\\lambda_i(\\mathbf{H}(\\lambda))$, which directly influences convergence rate per iteration. Excessively small $\\lambda$ can lead to poor conditioning due to small eigenvalues of $\\mathbf{G}^\\top\\mathbf{C}_d^{-1}\\mathbf{G}$, slowing convergence for a fixed $\\tau$, while excessively large $\\lambda$ increases curvature, potentially causing instability or divergence for a fixed $\\tau$ if $\\tau$ is not reduced accordingly.\n\nYour task is to write a program that (i) analyzes the effect of the trade-off parameter $\\lambda$ on the conditioning of $\\mathbf{H}(\\lambda)$ for a simple ill-conditioned linear operator and (ii) illustrates, with a one-dimensional recovery problem, how too large or too small $\\lambda$ changes convergence behavior, including step sizes and iteration counts, when using fixed-step gradient descent.\n\nYou must use the following configurations:\n\n- Conditioning analysis: let $\\mathbf{G} = \\mathrm{diag}(1, 10^{-3}) \\in \\mathbb{R}^{2 \\times 2}$, $\\mathbf{C}_d = \\mathbf{I}_2$, and $\\mathbf{L} = \\mathbf{I}_2$. Then\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top \\mathbf{G} + \\lambda \\mathbf{I}_2 = \\mathrm{diag}(1 + \\lambda, 10^{-6} + \\lambda),\n$$\nand the condition number is\n$$\n\\kappa(\\mathbf{H}(\\lambda)) = \\frac{1 + \\lambda}{10^{-6} + \\lambda}.\n$$\n\n- One-dimensional recovery: consider the scalar parameter $m \\in \\mathbb{R}$, forward operator $g \\in \\mathbb{R}$, data covariance $c_d \\in \\mathbb{R}$, and regularization operator $l = 1$. Use $g = 10^{-3}$, $c_d = 1$, and generate noiseless data $d = g m_{\\text{true}}$ with $m_{\\text{true}} = 2$. Consider the objective\n$$\nJ(m) = \\frac{1}{2}(g m - d)^2 + \\frac{\\lambda}{2} m^2,\n$$\nwhose gradient and Hessian are\n$$\n\\nabla J(m) = g(g m - d) + \\lambda m, \\quad H(\\lambda) = g^2 + \\lambda.\n$$\nFor fixed-step gradient descent with step length $\\tau = 0.5$, the update is\n$$\nm_{k+1} = m_k - \\tau\\left[g(g m_k - d) + \\lambda m_k\\right],\n$$\nstarting from $m_0 = 0$. The exact minimizer is\n$$\nm^\\star(\\lambda) = \\frac{g d}{g^2 + \\lambda} = \\frac{g^2 m_{\\text{true}}}{g^2 + \\lambda}.\n$$\nUse the relative error stopping criterion $\\left|m_k - m^\\star(\\lambda)\\right|/\\left|m^\\star(\\lambda)\\right| < 10^{-8}$ and a maximum of $10^{4}$ iterations. For each $\\lambda$, compute:\n- the condition number $\\kappa(\\mathbf{H}(\\lambda))$ from the $2 \\times 2$ configuration described above,\n- the number of iterations needed to reach the tolerance (use $-1$ if the method is non-contractive or fails to reach tolerance within the maximum iterations),\n- the final iterate $m_{\\text{final}}$ (the estimate when stopping or after $5$ iterations if divergence is detected early),\n- the average magnitude of the first $5$ update steps, defined as $\\frac{1}{5}\\sum_{k=0}^{4}\\left|m_{k+1} - m_k\\right|$ (if fewer than $5$ iterations occur before stopping, average over the available steps),\n- a boolean indicating whether the method converged under the given tolerance and iteration cap.\n\nTest suite:\nUse the following trade-off parameter values:\n$$\n\\lambda \\in \\left\\{0, 10^{-6}, 10^{-2}, 1, 10\\right\\}.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results for the test suite as a comma-separated list enclosed in square brackets, where each entry corresponds to one $\\lambda$ and is itself a list:\n$$\n[\\,[\\lambda,\\ \\kappa,\\ \\text{iters},\\ m_{\\text{final}},\\ \\text{avg\\_first5\\_step},\\ \\text{converged}],\\ \\ldots\\,].\n$$\nAll entries must be numeric or boolean types as specified above, and the order must match the order of $\\lambda$ in the test suite.",
            "solution": "The problem requires an analysis of the Tikhonov-regularized linear least-squares problem, focusing on the role of the regularization parameter $\\lambda$. We will first validate the problem's formulation and then proceed to a principled solution. The problem is scientifically grounded, well-posed, and all necessary parameters and equations are provided and consistent. Hence, it is valid.\n\nThe core of the problem lies in minimizing the objective function:\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}\\left\\|\\mathbf{C}_d^{-\\frac{1}{2}}(\\mathbf{G}\\mathbf{m} - \\mathbf{d})\\right\\|_2^2 + \\frac{\\lambda}{2}\\left\\|\\mathbf{L}\\mathbf{m}\\right\\|_2^2\n$$\nThe first term measures the misfit between the predicted data $\\mathbf{G}\\mathbf{m}$ and the observed data $\\mathbf{d}$, weighted by the inverse of the data covariance matrix $\\mathbf{C}_d$. The second term is a regularization (or penalty) term that penalizes undesirable model characteristics, controlled by the operator $\\mathbf{L}$. The trade-off parameter $\\lambda > 0$ balances the importance of fitting the data versus satisfying the regularization constraint.\n\nFor gradient-based optimization methods, the convergence rate is intimately linked to the properties of the Hessian matrix of the objective function. The update step for gradient descent is given by:\n$$\n\\mathbf{m}_{k+1} = \\mathbf{m}_k - \\tau \\nabla J(\\mathbf{m}_k)\n$$\nwhere $\\tau$ is the step length. For a quadratic objective function, the error propagates according to the eigenvalues $\\lambda_i$ of the Hessian matrix $\\mathbf{H}$. The convergence is governed by the contraction factor, which for each eigen-component is $\\left|1 - \\tau\\lambda_i(\\mathbf{H})\\right|$. For the method to converge, this factor must be less than $1$ for all eigenvalues, which requires $0 < \\tau < 2/\\lambda_{\\max}(\\mathbf{H})$. The overall rate of convergence is determined by the slowest-converging component, which corresponds to $\\max_i \\left|1 - \\tau\\lambda_i(\\mathbf{H})\\right|$. An optimal fixed step length $\\tau_{\\text{opt}} = 2/(\\lambda_{\\min} + \\lambda_{\\max})$ yields a convergence rate dictated by the condition number $\\kappa(\\mathbf{H}) = \\lambda_{\\max}/\\lambda_{\\min}$. A larger condition number implies a slower convergence rate.\n\nThe Hessian for this problem is:\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top\\mathbf{C}_d^{-1}\\mathbf{G} + \\lambda \\mathbf{L}^\\top \\mathbf{L}\n$$\nThe parameter $\\lambda$ directly influences the Hessian's spectrum and thus its conditioning.\n\n**1. Conditioning Analysis**\n\nWe are given a simplified $2 \\times 2$ case to analyze the effect of $\\lambda$ on conditioning: $\\mathbf{G} = \\mathrm{diag}(1, 10^{-3})$, $\\mathbf{C}_d = \\mathbf{I}_2$, and $\\mathbf{L} = \\mathbf{I}_2$. The Hessian becomes:\n$$\n\\mathbf{H}(\\lambda) = \\mathbf{G}^\\top \\mathbf{G} + \\lambda \\mathbf{I}_2 = \\mathrm{diag}(1^2, (10^{-3})^2) + \\lambda \\mathrm{diag}(1, 1) = \\mathrm{diag}(1 + \\lambda, 10^{-6} + \\lambda)\n$$\nThe eigenvalues are the diagonal entries: $\\lambda_{\\max} = 1 + \\lambda$ and $\\lambda_{\\min} = 10^{-6} + \\lambda$. The condition number is:\n$$\n\\kappa(\\mathbf{H}(\\lambda)) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1 + \\lambda}{10^{-6} + \\lambda}\n$$\n- If $\\lambda \\to 0$, $\\kappa(\\mathbf{H}(\\lambda)) \\to 1/10^{-6} = 10^6$. The Hessian is very ill-conditioned, which predicts slow convergence for gradient methods.\n- If $\\lambda \\to \\infty$, $\\kappa(\\mathbf{H}(\\lambda)) \\to \\lambda/\\lambda = 1$. The Hessian becomes perfectly conditioned. However, a very large $\\lambda$ excessively penalizes the model norm, leading to a\nsolution $m^\\star(\\lambda)$ that is heavily biased towards zero and may not fit the data well.\n\n**2. One-Dimensional Recovery and Convergence Behavior**\n\nWe now examine a scalar problem ($n=p=1$) with $g = 10^{-3}$, $c_d = 1$, $l=1$, $m_{\\text{true}} = 2$, and data $d = g m_{\\text{true}} = 2 \\times 10^{-3}$. The Hessian is a scalar $H(\\lambda) = g^2 + \\lambda = 10^{-6} + \\lambda$. The gradient descent update with step length $\\tau = 0.5$ has a contraction factor:\n$$\n\\rho(\\lambda) = \\left|1 - \\tau H(\\lambda)\\right| = \\left|1 - 0.5(10^{-6} + \\lambda)\\right|\n$$\nConvergence requires $\\rho(\\lambda) < 1$.\n- For small $\\lambda$ (e.g., $\\lambda=0$ or $\\lambda=10^{-6}$), $H(\\lambda)$ is very small. The contraction factor $\\rho(\\lambda)$ is just slightly less than $1$. This leads to extremely slow convergence.\n- For a moderate $\\lambda$ (e.g., $\\lambda=1$), $H(\\lambda) \\approx 1$. The contraction factor is $\\rho(1) \\approx |1 - 0.5(1)| = 0.5$. This is a healthy contraction factor, promising rapid convergence. This corresponds to a well-conditioned problem where the step size is appropriate for the curvature.\n- For large $\\lambda$ (e.g., $\\lambda=10$), $H(\\lambda) \\approx 10$. The step size $\\tau = 0.5$ is too large for the increased curvature of the objective function. The convergence condition on $\\tau$ is $0 < \\tau < 2/H(\\lambda)$. For $\\lambda=10$, this means $\\tau < 2/10.000001 \\approx 0.2$. Since we use a fixed step $\\tau=0.5$, this condition is violated. The contraction factor becomes $\\rho(10) \\approx |1 - 0.5(10)| = 4 > 1$. The method will diverge.\n\nThe program will implement the gradient descent iteration for the specified values of $\\lambda$ and compute the requested metrics, which are expected to align with this theoretical analysis.\n- $\\lambda = 0, 10^{-6}$: High condition number, very slow convergence (will hit max iterations).\n- $\\lambda = 10^{-2}, 1$: Lower condition number, faster convergence observed.\n- $\\lambda = 10$: Divergence due to overly large step size relative to the problem's curvature.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of the Tikhonov regularization parameter lambda on the\n    conditioning and convergence of a gradient descent method for a linear\n    inverse problem.\n    \"\"\"\n    # Test suite of trade-off parameters\n    test_lambdas = [0.0, 1e-6, 1e-2, 1.0, 10.0]\n\n    # Parameters for the 1D recovery problem\n    g = 1e-3\n    m_true = 2.0\n    d = g * m_true\n    tau = 0.5\n    m0 = 0.0\n    max_iter = 10000\n    tol = 1e-8\n\n    final_results = []\n\n    for lam in test_lambdas:\n        # (i) Conditioning analysis from the 2x2 configuration\n        kappa = (1.0 + lam) / (1e-6 + lam)\n\n        # (ii) One-dimensional recovery problem\n        # Hessian and contraction factor\n        H_lambda = g**2 + lam\n        rho = abs(1.0 - tau * H_lambda)\n        \n        # Exact minimizer\n        m_star_num = g**2 * m_true\n        m_star_den = g**2 + lam\n        m_star = m_star_num / m_star_den if m_star_den != 0 else float('inf')\n\n        # Initialize iteration variables\n        m = m0\n        iters = 0\n        converged = False\n        steps = []\n        \n        # Handle the theoretically divergent case\n        if rho >= 1.0:\n            # As per the problem, run for 5 iterations to get m_final and avg step\n            loop_limit = 5\n            for k in range(loop_limit):\n                grad = (g**2 + lam) * m - g * d\n                m_next = m - tau * grad\n                steps.append(abs(m_next - m))\n                m = m_next\n            \n            m_final = m\n            avg_first5_step = np.mean(steps)\n            iters = -1\n            converged = False\n            \n            final_results.append([lam, kappa, iters, m_final, avg_first5_step, converged])\n            continue\n\n        # Handle convergent or slowly converging cases\n        for k in range(max_iter):\n            grad = (g**2 + lam) * m - g * d\n            m_next = m - tau * grad\n            \n            # Store step magnitude for the first 5 steps\n            if k < 5:\n                steps.append(abs(m_next - m))\n            \n            m = m_next\n            iters += 1\n            \n            # Check for convergence using the relative error criterion\n            # The case m_star == 0 is not possible for lam >= 0 but this is robust\n            relative_error = abs(m - m_star) / abs(m_star) if m_star != 0.0 else abs(m)\n            if relative_error < tol:\n                converged = True\n                break\n        \n        m_final = m\n        # If the loop finished due to max_iter, it did not converge\n        if not converged:\n            iters = -1\n        \n        # Calculate average of available step magnitudes\n        avg_first5_step = np.mean(steps) if steps else 0.0\n        \n        final_results.append([lam, kappa, iters, m_final, avg_first5_step, converged])\n\n    # Format the output as a single string\n    output_str = \"[\" + ','.join(map(str, final_results)) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While standard gradient descent takes a one-size-fits-all approach to step sizes, the objective functions in geophysical problems often resemble long, narrow valleys with drastically different curvature in different directions. This exercise  lets you directly compare the performance of classic Gradient Descent against modern adaptive methods like Adagrad, RMSProp, and Adam in such an environment. By implementing these algorithms, you will gain an intuitive understanding of how per-parameter learning rate adaptation enables faster and more robust navigation of complex, anisotropic loss surfaces.",
            "id": "3601039",
            "problem": "Consider a two-parameter linearized inverse problem representative of small-perturbation acoustic velocity inversion in computational geophysics. Let the unknown parameter vector be $m = [v_1, v_2]^\\top$ where $v_1$ and $v_2$ are P-wave velocities in meters per second ($\\mathrm{m/s}$). Assume synthetic observations are generated by a linear forward operator $G \\in \\mathbb{R}^{2 \\times 2}$ acting on $m$ with additive zero-mean Gaussian noise of identity covariance. Under this noise model, the Maximum Likelihood estimation reduces to minimizing the least-squares objective\n$$\nJ(m) = \\frac{1}{2} \\left\\| G m - d_{\\mathrm{obs}} \\right\\|_2^2,\n$$\nwhere $d_{\\mathrm{obs}} \\in \\mathbb{R}^2$ are the observed data. For testing, we will use noiseless synthetic data $d_{\\mathrm{obs}} = G m^\\star$ with a prescribed ground-truth $m^\\star$ so that the global minimizer is $m^\\star$. The operator $G$ will be chosen diagonal to control curvature anisotropy via its singular values. The drastically different curvature arises because the Hessian of $J(m)$ is $H = G^\\top G$, whose eigenvalues are the squares of the diagonal entries of $G$; this produces strong anisotropy when these entries differ by orders of magnitude.\n\nYour task is to implement four gradient-based optimization methods to minimize $J(m)$ from a given initial guess $m^{(0)}$: Gradient Descent (GD), Adaptive Gradient (Adagrad), Root Mean Square Propagation (RMSProp), and Adaptive Moment Estimation (Adam). For each method, run exactly $10$ iterations. For each iteration $t = 1, 2, \\ldots, 10$, compute the squared Euclidean error\n$$\nE^{(t)} = \\left\\| m^{(t)} - m^\\star \\right\\|_2^2,\n$$\nand report the sequence $\\{E^{(t)}\\}_{t=1}^{10}$ for each method. All error values $E^{(t)}$ must be reported in units of $(\\mathrm{m/s})^2$.\n\nImplement the methods using their standard update definitions with the specified hyperparameters below. Do not use any shortcut formulas beyond the necessary implementation of these well-established methods. Derive the gradient of $J(m)$ from first principles. Use the same gradient for all methods. The learning rates and other hyperparameters for the optimizers are given per test case. No line search or additional heuristics are allowed.\n\nTest Suite and Hyperparameters:\n- Let $G = \\mathrm{diag}(g_1, g_2)$ with $g_1, g_2 > 0$, and $d_{\\mathrm{obs}} = G m^\\star$.\n- Define three test cases as follows:\n    1. Test Case A (happy path, strong but not extreme anisotropy):\n       - $g_1 = 1000$, $g_2 = 1$.\n       - $m^\\star = [2000.0, 1500.0]^\\top$ $(\\mathrm{m/s})$.\n       - $m^{(0)} = [3000.0, 500.0]^\\top$ $(\\mathrm{m/s})$.\n       - GD learning rate $\\alpha_{\\mathrm{GD}} = 10^{-6}$.\n       - Adagrad learning rate $\\alpha_{\\mathrm{Ada}} = 10^{-1}$, numerical stabilizer $\\epsilon = 10^{-8}$.\n       - RMSProp learning rate $\\alpha_{\\mathrm{RMS}} = 10^{-3}$, decay parameter $\\rho = 0.9$, stabilizer $\\epsilon = 10^{-8}$.\n       - Adam learning rate $\\alpha_{\\mathrm{Adam}} = 10^{-3}$, first-moment decay $\\beta_1 = 0.9$, second-moment decay $\\beta_2 = 0.999$, stabilizer $\\epsilon = 10^{-8}$.\n    2. Test Case B (edge case with extreme anisotropy):\n       - $g_1 = 10000$, $g_2 = 0.1$.\n       - $m^\\star = [2200.0, 1400.0]^\\top$ $(\\mathrm{m/s})$.\n       - $m^{(0)} = [1800.0, 2000.0]^\\top$ $(\\mathrm{m/s})$.\n       - GD learning rate $\\alpha_{\\mathrm{GD}} = 10^{-10}$.\n       - Adagrad learning rate $\\alpha_{\\mathrm{Ada}} = 10^{-2}$, $\\epsilon = 10^{-8}$.\n       - RMSProp learning rate $\\alpha_{\\mathrm{RMS}} = 10^{-3}$, $\\rho = 0.9$, $\\epsilon = 10^{-8}$.\n       - Adam learning rate $\\alpha_{\\mathrm{Adam}} = 10^{-3}$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$.\n    3. Test Case C (boundary condition: initialized at the solution):\n       - $g_1 = 1000$, $g_2 = 1$.\n       - $m^\\star = [2500.0, 1000.0]^\\top$ $(\\mathrm{m/s})$.\n       - $m^{(0)} = [2500.0, 1000.0]^\\top$ $(\\mathrm{m/s})$.\n       - GD learning rate $\\alpha_{\\mathrm{GD}} = 10^{-6}$.\n       - Adagrad learning rate $\\alpha_{\\mathrm{Ada}} = 10^{-1}$, $\\epsilon = 10^{-8}$.\n       - RMSProp learning rate $\\alpha_{\\mathrm{RMS}} = 10^{-3}$, $\\rho = 0.9$, $\\epsilon = 10^{-8}$.\n       - Adam learning rate $\\alpha_{\\mathrm{Adam}} = 10^{-3}$, $\\beta_1 = 0.9$, $\\beta_2 = 0.999$, $\\epsilon = 10^{-8}$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Represent each test case as a list of four inner lists in the order $[\\text{GD}, \\text{Adagrad}, \\text{RMSProp}, \\text{Adam}]$.\n- Each inner list must contain $10$ floating-point values (the sequence $\\{E^{(t)}\\}_{t=1}^{10}$ in $(\\mathrm{m/s})^2$), with no unit strings and no additional text.\n- The overall output is thus a list of length $3$, one entry per test case. For example, the formatting pattern is\n$$\n[\\,[e_{1,\\mathrm{GD}}^{(1)},\\ldots,e_{1,\\mathrm{GD}}^{(10)}],\\,[e_{1,\\mathrm{Ada}}^{(1)},\\ldots,e_{1,\\mathrm{Ada}}^{(10)}],\\,[e_{1,\\mathrm{RMS}}^{(1)},\\ldots,e_{1,\\mathrm{RMS}}^{(10)}],\\,[e_{1,\\mathrm{Adam}}^{(1)},\\ldots,e_{1,\\mathrm{Adam}}^{(10)}]\\, , \\, \\ldots \\, ]\n$$\nwhere each $e_{k,\\cdot}^{(t)}$ is a float in $(\\mathrm{m/s})^2$ and $k \\in \\{1,2,3\\}$ indexes the test case.",
            "solution": "The problem statement is assessed to be **valid**. It is a well-posed, scientifically grounded, and objective problem in the field of computational geophysics and numerical optimization.\n\n- **Givens**:\n    - Parameter vector: $m = [v_1, v_2]^\\top$, where $v_1, v_2$ are P-wave velocities in $\\mathrm{m/s}$.\n    - Objective function: $J(m) = \\frac{1}{2} \\left\\| G m - d_{\\mathrm{obs}} \\right\\|_2^2$.\n    - Forward operator: $G = \\mathrm{diag}(g_1, g_2) \\in \\mathbb{R}^{2 \\times 2}$ with $g_1, g_2 > 0$.\n    - Observed data: $d_{\\mathrm{obs}} = G m^\\star$, where $m^\\star$ is the ground-truth parameter vector.\n    - Error metric: $E^{(t)} = \\left\\| m^{(t)} - m^\\star \\right\\|_2^2$ in $(\\mathrm{m/s})^2$.\n    - Number of iterations per method: exactly $10$.\n    - Optimization methods: Gradient Descent (GD), Adagrad, RMSProp, Adam.\n    - Test Case A:\n        - $g_1 = 1000$, $g_2 = 1$.\n        - $m^\\star = [2000.0, 1500.0]^\\top$.\n        - $m^{(0)} = [3000.0, 500.0]^\\top$.\n        - Hyperparameters: $\\alpha_{\\mathrm{GD}} = 10^{-6}$; $\\alpha_{\\mathrm{Ada}} = 10^{-1}, \\epsilon = 10^{-8}$; $\\alpha_{\\mathrm{RMS}} = 10^{-3}, \\rho = 0.9, \\epsilon = 10^{-8}$; $\\alpha_{\\mathrm{Adam}} = 10^{-3}, \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$.\n    - Test Case B:\n        - $g_1 = 10000$, $g_2 = 0.1$.\n        - $m^\\star = [2200.0, 1400.0]^\\top$.\n        - $m^{(0)} = [1800.0, 2000.0]^\\top$.\n        - Hyperparameters: $\\alpha_{\\mathrm{GD}} = 10^{-10}$; $\\alpha_{\\mathrm{Ada}} = 10^{-2}, \\epsilon = 10^{-8}$; $\\alpha_{\\mathrm{RMS}} = 10^{-3}, \\rho = 0.9, \\epsilon = 10^{-8}$; $\\alpha_{\\mathrm{Adam}} = 10^{-3}, \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}$.\n    - Test Case C:\n        - $g_1 = 1000$, $g_2 = 1$.\n        - $m^\\star = [2500.0, 1000.0]^\\top$.\n        - $m^{(0)} = [2500.0, 1000.0]^\\top$.\n        - Hyperparameters: Same as Test Case A.\n\n- **Validation**:\n    - **Scientifically Grounded**: The problem uses a standard linear least-squares formulation common in inverse problems and is based on established gradient-based optimization algorithms. It is free of scientific flaws.\n    - **Well-Posed**: The problem is well-posed. For each test case, all required data, initial conditions, and parameters are provided, leading to a unique and computable solution sequence. The use of noiseless synthetic data ensures the ground truth $m^\\star$ is the unique minimizer of $J(m)$.\n    - **Objective**: The problem is stated with precise mathematical and numerical terminology, free of any subjectivity or ambiguity.\n\nThe problem is valid and can proceed to a solution.\n\n### Principle-Based Solution\n\nThe core of the problem is to minimize the least-squares objective function $J(m)$ using four different gradient-based optimization algorithms. All these methods rely on the gradient of the objective function, which we must first derive.\n\n_1. Gradient Derivation_\n\nThe objective function is\n$$\nJ(m) = \\frac{1}{2} \\| G m - d_{\\mathrm{obs}} \\|_2^2\n$$\nExpanding the squared Euclidean norm gives:\n$$\nJ(m) = \\frac{1}{2} (G m - d_{\\mathrm{obs}})^\\top (G m - d_{\\mathrm{obs}}) = \\frac{1}{2} (m^\\top G^\\top G m - 2 m^\\top G^\\top d_{\\mathrm{obs}} + d_{\\mathrm{obs}}^\\top d_{\\mathrm{obs}})\n$$\nTo find the gradient $\\nabla_m J(m)$, we differentiate with respect to the vector $m$:\n$$\n\\nabla_m J(m) = \\frac{1}{2} (2 G^\\top G m - 2 G^\\top d_{\\mathrm{obs}})\n$$\nThis simplifies to the expression for the gradient, which will be used by all four optimization methods:\n$$\n\\nabla_m J(m) = G^\\top(G m - d_{\\mathrm{obs}})\n$$\nSince $d_{\\mathrm{obs}} = G m^\\star$, we can also write the gradient as $\\nabla_m J(m) = G^\\top G (m - m^\\star)$. This confirms that the gradient is zero if and only if $m = m^\\star$, since $G^\\top G$ is invertible ($g_1, g_2 > 0$).\n\n_2. Optimization Algorithms_\n\nAll algorithms follow an iterative update scheme. Starting with an initial guess $m^{(0)}$, they generate a sequence of estimates $m^{(1)}, m^{(2)}, \\ldots, m^{(10)}$. For each iteration $t \\in \\{1, 2, \\ldots, 10\\}$, we compute the gradient $g^{(t)} = \\nabla_m J(m^{(t-1)})$ and then update the model parameters to obtain $m^{(t)}$.\n\n_a. Gradient Descent (GD)_\n\nThis is the simplest gradient-based method. The update rule is a step in the negative gradient direction, scaled by a learning rate $\\alpha$.\n$$\nm^{(t)} = m^{(t-1)} - \\alpha_{\\mathrm{GD}} \\, g^{(t)}\n$$\n\n_b. Adaptive Gradient (Adagrad)_\n\nAdagrad adapts the learning rate for each parameter, scaling it inversely proportional to the square root of the sum of all past squared gradients. It is effective for problems with sparse gradients or varying gradient scales among parameters.\n- Initialize the gradient accumulation vector: $S^{(0)} = \\mathbf{0}$.\n- At iteration $t$:\n    $$\n    S^{(t)} = S^{(t-1)} + g^{(t)} \\odot g^{(t)}\n    $$\n    $$\n    m^{(t)} = m^{(t-1)} - \\frac{\\alpha_{\\mathrm{Ada}}}{\\sqrt{S^{(t)}} + \\epsilon} \\odot g^{(t)}\n    $$\nHere, $\\odot$ denotes element-wise multiplication, and division is also element-wise. The small constant $\\epsilon$ prevents division by zero.\n\n_c. Root Mean Square Propagation (RMSProp)_\n\nRMSProp modifies Adagrad to prevent the learning rates from monotonically decreasing. It uses an exponentially decaying average of squared gradients, giving more weight to recent gradients.\n- Initialize the moving average of squared gradients: $v^{(0)} = \\mathbf{0}$.\n- At iteration $t$:\n    $$\n    v^{(t)} = \\rho \\, v^{(t-1)} + (1-\\rho) \\, g^{(t)} \\odot g^{(t)}\n    $$\n    $$\n    m^{(t)} = m^{(t-1)} - \\frac{\\alpha_{\\mathrm{RMS}}}{\\sqrt{v^{(t)}} + \\epsilon} \\odot g^{(t)}\n    $$\nThe parameter $\\rho$ is the decay rate for the moving average.\n\n_d. Adaptive Moment Estimation (Adam)_\n\nAdam combines the ideas of RMSProp (adapting per-parameter learning rates based on the second moment of the gradients) and momentum (using an exponentially decaying average of past gradients, the first moment).\n- Initialize first moment vector: $p^{(0)} = \\mathbf{0}$.\n- Initialize second moment vector: $v^{(0)} = \\mathbf{0}$.\n- At iteration $t$:\n    $$\n    p^{(t)} = \\beta_1 \\, p^{(t-1)} + (1-\\beta_1) \\, g^{(t)}\n    $$\n    $$\n    v^{(t)} = \\beta_2 \\, v^{(t-1)} + (1-\\beta_2) \\, g^{(t)} \\odot g^{(t)}\n    $$\n    The moments are corrected for initialization bias:\n    $$\n    \\hat{p}^{(t)} = \\frac{p^{(t)}}{1 - \\beta_1^t}\n    $$\n    $$\n    \\hat{v}^{(t)} = \\frac{v^{(t)}}{1 - \\beta_2^t}\n    $$\n    The parameter update is then:\n    $$\n    m^{(t)} = m^{(t-1)} - \\alpha_{\\mathrm{Adam}} \\frac{\\hat{p}^{(t)}}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon}\n    $$\nHere, $\\beta_1$ and $\\beta_2$ are the decay rates for the first and second moment estimates, respectively.\n\n_3. Computational Procedure_\n\nFor each of the three test cases, the following procedure is executed:\n1. Initialize the parameters $g_1, g_2, m^\\star, m^{(0)}$ and the hyperparameters for each of the four optimization methods.\n2. Construct the matrix $G = \\mathrm{diag}(g_1, g_2)$ and compute the synthetic data $d_{\\mathrm{obs}} = G m^\\star$.\n3. For each of the four methods (GD, Adagrad, RMSProp, Adam):\n    a. Initialize the parameter vector $m$ to $m^{(0)}$ and any method-specific state variables (e.g., $S, v, p$) to zero vectors.\n    b. Create a list to store the sequence of squared errors, $\\{E^{(t)}\\}_{t=1}^{10}$.\n    c. Loop for $t$ from $1$ to $10$:\n        i. Compute the gradient $g^{(t)} = G^\\top(G m - d_{\\mathrm{obs}})$ using the current parameter vector $m$.\n        ii. Update the parameter vector $m$ to a new one, $m_{new}$, using the specific algorithm's update rule. This $m_{new}$ becomes $m^{(t)}$.\n        iii. Calculate the squared error $E^{(t)} = \\|m^{(t)} - m^\\star\\|_2^2$.\n        iv. Store $E^{(t)}$ in the error list.\n        v. Replace the current parameter vector $m$ with $m_{new}$ for the next iteration.\n4. The collected lists of errors for each method within a test case are aggregated, and finally, the results for all three test cases are formatted into the required string output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates four gradient-based optimization methods (GD, Adagrad, RMSProp, Adam) \n    on a 2D linearized inverse problem for three distinct test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case A\n        {\n            \"g\": np.array([1000.0, 1.0]),\n            \"m_star\": np.array([2000.0, 1500.0]),\n            \"m0\": np.array([3000.0, 500.0]),\n            \"hyperparams\": {\n                \"gd\": {\"alpha\": 1e-6},\n                \"adagrad\": {\"alpha\": 1e-1, \"epsilon\": 1e-8},\n                \"rmsprop\": {\"alpha\": 1e-3, \"rho\": 0.9, \"epsilon\": 1e-8},\n                \"adam\": {\"alpha\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8},\n            }\n        },\n        # Test Case B\n        {\n            \"g\": np.array([10000.0, 0.1]),\n            \"m_star\": np.array([2200.0, 1400.0]),\n            \"m0\": np.array([1800.0, 2000.0]),\n            \"hyperparams\": {\n                \"gd\": {\"alpha\": 1e-10},\n                \"adagrad\": {\"alpha\": 1e-2, \"epsilon\": 1e-8},\n                \"rmsprop\": {\"alpha\": 1e-3, \"rho\": 0.9, \"epsilon\": 1e-8},\n                \"adam\": {\"alpha\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8},\n            }\n        },\n        # Test Case C\n        {\n            \"g\": np.array([1000.0, 1.0]),\n            \"m_star\": np.array([2500.0, 1000.0]),\n            \"m0\": np.array([2500.0, 1000.0]),\n            \"hyperparams\": {\n                \"gd\": {\"alpha\": 1e-6},\n                \"adagrad\": {\"alpha\": 1e-1, \"epsilon\": 1e-8},\n                \"rmsprop\": {\"alpha\": 1e-3, \"rho\": 0.9, \"epsilon\": 1e-8},\n                \"adam\": {\"alpha\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"epsilon\": 1e-8},\n            }\n        }\n    ]\n\n    num_iterations = 10\n    \n    def gradient_J(m, G, d_obs):\n        return G.T @ (G @ m - d_obs)\n\n    def run_gd(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        for _ in range(num_iterations):\n            grad = gradient_J(m, G, d_obs)\n            m -= alpha * grad\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    def run_adagrad(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        epsilon = hyperparams['epsilon']\n        S = np.zeros_like(m0)\n        for _ in range(num_iterations):\n            grad = gradient_J(m, G, d_obs)\n            S += grad**2\n            m -= (alpha / (np.sqrt(S) + epsilon)) * grad\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    def run_rmsprop(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        rho = hyperparams['rho']\n        epsilon = hyperparams['epsilon']\n        v = np.zeros_like(m0)\n        for _ in range(num_iterations):\n            grad = gradient_J(m, G, d_obs)\n            v = rho * v + (1 - rho) * grad**2\n            m -= (alpha / (np.sqrt(v) + epsilon)) * grad\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    def run_adam(m0, m_star, G, d_obs, hyperparams):\n        errors = []\n        m = m0.copy()\n        alpha = hyperparams['alpha']\n        beta1 = hyperparams['beta1']\n        beta2 = hyperparams['beta2']\n        epsilon = hyperparams['epsilon']\n        p = np.zeros_like(m0)\n        v = np.zeros_like(m0)\n        for t in range(1, num_iterations + 1):\n            grad = gradient_J(m, G, d_obs)\n            p = beta1 * p + (1 - beta1) * grad\n            v = beta2 * v + (1 - beta2) * grad**2\n            p_hat = p / (1 - beta1**t)\n            v_hat = v / (1 - beta2**t)\n            m -= alpha * p_hat / (np.sqrt(v_hat) + epsilon)\n            errors.append(np.sum((m - m_star)**2))\n        return errors\n\n    all_results = []\n    for case in test_cases:\n        G = np.diag(case['g'])\n        m_star = case['m_star']\n        m0 = case['m0']\n        d_obs = G @ m_star\n        hyperparams = case['hyperparams']\n        \n        case_results = []\n        \n        # GD\n        case_results.append(run_gd(m0, m_star, G, d_obs, hyperparams['gd']))\n        # Adagrad\n        case_results.append(run_adagrad(m0, m_star, G, d_obs, hyperparams['adagrad']))\n        # RMSProp\n        case_results.append(run_rmsprop(m0, m_star, G, d_obs, hyperparams['rmsprop']))\n        # Adam\n        case_results.append(run_adam(m0, m_star, G, d_obs, hyperparams['adam']))\n        \n        all_results.append(case_results)\n\n    # Format the output string as specified\n    case_strings = []\n    for case_result in all_results:\n        method_strings = []\n        for method_errors in case_result:\n            # Use a general formatter that avoids scientific notation for smaller numbers\n            # but handles large ones. str() is usually sufficient.\n            error_str = '[' + ','.join(map(str, method_errors)) + ']'\n            method_strings.append(error_str)\n        case_strings.append('[' + ','.join(method_strings) + ']')\n    \n    final_output = '[' + ','.join(case_strings) + ']'\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The power of modern optimization algorithms comes with a responsibility to apply them correctly within a physical context. This thought experiment  challenges you to move beyond the abstract mathematics and consider the physical dimensions of the parameters you are inverting for, such as seismic velocity and density. By performing a rigorous dimensional analysis, you will uncover why naively applying unit-agnostic adaptive optimizers can lead to physically inconsistent results and learn the proper way to ensure your model updates are scale-invariant and meaningful.",
            "id": "3601067",
            "problem": "Consider a waveform inversion setup in computational geophysics where the model fields are the spatially varying seismic velocity $v(\\mathbf{x})$ with units $[\\mathrm{m}/\\mathrm{s}]$ and mass density $\\rho(\\mathbf{x})$ with units $[\\mathrm{kg}/\\mathrm{m}^3]$ over a three-dimensional domain $\\Omega \\subset \\mathbb{R}^3$ with volume element $d\\mathbf{x}$ having units $[\\mathrm{m}^3]$. The predicted pressure data $p(t,\\mathbf{x}_r)$ at receiver locations $\\mathbf{x}_r$ have units of pascals $[\\mathrm{Pa}]$. The data residual at each receiver and time is defined as $r(t,\\mathbf{x}_r;m) = p(t,\\mathbf{x}_r;m) - d(t,\\mathbf{x}_r)$, with $d(t,\\mathbf{x}_r)$ the observed pressure. The misfit functional is defined by a weighted least-squares form\n$$\nJ(m) = \\frac{1}{2}\\sum_{r}\\int_{0}^{T} \\left( W_d\\, r(t,\\mathbf{x}_r;m) \\right)^2 \\, dt,\n$$\nwhere $W_d$ is a data-weighting operator chosen so that $W_d\\, r$ is dimensionless (for example, $W_d = C_d^{-1/2}$ with $C_d$ a data covariance operator having units $[\\mathrm{Pa}^2]$). By construction $J(m)$ is dimensionless. The gradient with respect to the model parameters is defined as the Riesz representer of the Gateaux derivative under the standard $L^2(\\Omega)$ inner product over space. Specifically, perturbations $\\delta v$ and $\\delta \\rho$ produce the variation\n$$\n\\delta J = \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} + \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x},\n$$\nwhich is dimensionless. A gradient-based update with per-parameter adaptivity is written as\n$$\nv_{k+1}(\\mathbf{x}) = v_k(\\mathbf{x}) - \\alpha_v \\, g_v(\\mathbf{x}), \\quad \\rho_{k+1}(\\mathbf{x}) = \\rho_k(\\mathbf{x}) - \\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x}),\n$$\nwith $\\alpha_v$ and $\\alpha_{\\rho}$ being scalar step sizes that may differ between parameters. Assume that the forward operator and adjoint-state machinery are consistent with the above definitions and produce $g_v$ and $g_{\\rho}$ that realize the stated Gateaux derivative under the spatial $L^2$ inner product.\n\nUsing only dimensional analysis grounded in the above definitions and without invoking any specific wave equation, answer the following:\n\n- Determine the physical units of $g_v$ and $g_{\\rho}$ implied by the requirement that $\\delta J$ be dimensionless when the inner product is $\\int_{\\Omega}(\\cdot)\\, d\\mathbf{x}$.\n- From those units, infer the units required of $\\alpha_v$ and $\\alpha_{\\rho}$ for the updates to be physically consistent on the raw (dimensional) parameters $v$ and $\\rho$.\n- Based on this reasoning, argue whether per-parameter adaptivity (i.e., choosing $\\alpha_v \\neq \\alpha_{\\rho}$) preserves the correct dimensionless gradient step in a properly scaled misfit, and under what condition(s) such adaptivity yields dimensionless, scale-invariant updates.\n\nWhich option best captures the correct conclusion?\n\nA. In a properly nondimensionalized misfit, per-parameter adaptivity preserves a dimensionless gradient step only if the update is carried out in nondimensional variables, for example $\\tilde v = v/v_0$ and $\\tilde \\rho = \\rho/\\rho_0$ with reference scales $v_0$ and $\\rho_0$, or equivalently if $\\alpha_v$ and $\\alpha_{\\rho}$ are endowed with compensating physical units so that $\\alpha_v g_v$ and $\\alpha_{\\rho} g_{\\rho}$ have units $[\\mathrm{m}/\\mathrm{s}]$ and $[\\mathrm{kg}/\\mathrm{m}^3]$, respectively. Naively applying adaptivity on raw variables with unitless $\\alpha_v$ and $\\alpha_{\\rho}$ does not ensure dimensionless, scale-invariant updates.\n\nB. Because the misfit $J(m)$ is dimensionless, the gradients $g_v$ and $g_{\\rho}$ are also dimensionless, so any choice of dimensionless per-parameter adaptive factors automatically preserves the correct dimensionless gradient step.\n\nC. The gradient $g_v$ has units $[\\mathrm{m}/\\mathrm{s}]$ and $g_{\\rho}$ has units $[\\mathrm{kg}/\\mathrm{m}^3]$ by definition of the gradient, so a single dimensionless step size applied to both parameters always preserves physical scaling.\n\nD. Under the spatial $L^2(\\Omega)$ inner product, $g_v$ has units $[\\mathrm{s}/\\mathrm{m}^4]$ and $g_{\\rho}$ has units $[1/\\mathrm{kg}]$, but choosing adaptive step sizes $\\alpha_v \\propto 1/\\|g_v\\|$ and $\\alpha_{\\rho} \\propto 1/\\|g_{\\rho}\\|$ automatically removes units, making the updates unitless and invariant regardless of parameterization.",
            "solution": "The user wants a rigorous validation and solution for a problem concerning dimensional analysis in gradient-based optimization for waveform inversion.\n\n### Step 1: Extract Givens\n-   Model parameters: Seismic velocity $v(\\mathbf{x})$ with units $[\\mathrm{m}/\\mathrm{s}]$, mass density $\\rho(\\mathbf{x})$ with units $[\\mathrm{kg}/\\mathrm{m}^3]$.\n-   Domain: Three-dimensional domain $\\Omega \\subset \\mathbb{R}^3$.\n-   Volume element: $d\\mathbf{x}$ with units $[\\mathrm{m}^3]$.\n-   Predicted data: $p(t,\\mathbf{x}_r;m)$ with units of pascals, $[\\mathrm{Pa}]$.\n-   Observed data: $d(t,\\mathbf{x}_r)$ with units of pascals, $[\\mathrm{Pa}]$.\n-   Data residual: $r(t,\\mathbf{x}_r;m) = p(t,\\mathbf{x}_r;m) - d(t,\\mathbf{x}_r)$.\n-   Misfit functional: $J(m) = \\frac{1}{2}\\sum_{r}\\int_{0}^{T} \\left( W_d\\, r(t,\\mathbf{x}_r;m) \\right)^2 \\, dt$.\n-   Data-weighting operator $W_d$: Defined such that $W_d\\, r$ is dimensionless.\n-   Misfit functional $J(m)$ is dimensionless.\n-   Variation of the misfit functional: $\\delta J = \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} + \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x}$.\n-   The variation $\\delta J$ is dimensionless.\n-   Gradient definition: The gradient $(g_v, g_{\\rho})$ is the Riesz representer of the Gateaux derivative under the standard $L^2(\\Omega)$ inner product.\n-   Gradient-based update: $v_{k+1}(\\mathbf{x}) = v_k(\\mathbf{x}) - \\alpha_v \\, g_v(\\mathbf{x})$ and $\\rho_{k+1}(\\mathbf{x}) = \\rho_k(\\mathbf{x}) - \\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x})$.\n-   Step sizes: $\\alpha_v$ and $\\alpha_{\\rho}$ are scalar step sizes.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a standard setup for a multiparameter inverse problem, specifically full-waveform inversion in geophysics. The definitions of the misfit functional, its variation (Gateaux derivative), and the gradient-based update rule are canonical. The question posed is a rigorous exercise in dimensional analysis, grounded in these definitions.\n\n-   **Scientifically Grounded:** The problem is firmly based on the established mathematical and physical principles of variational calculus and optimization as applied to wave-based inversion. The setup is a simplified but conceptually accurate representation of real-world problems.\n-   **Well-Posed:** The problem provides sufficient information to determine the units of the quantities involved and analyze the dimensional consistency of the update rules. A unique conclusion can be derived from the given equations.\n-   **Objective:** The problem is stated in precise, technical language, free from subjectivity or ambiguity.\n-   **Completeness and Consistency:** All necessary definitions for the analysis are provided. There is a slight terminological tension between the explicit formula for $\\delta J$, which defines $g_v$ and $g_\\rho$ as integral kernels, and the statement that they are Riesz representers under a \"standard $L^2$ inner product\" (which is dimensionally problematic for a multiparameter space with disparate units). However, a rigorous analysis must proceed from the explicit mathematical expressions given. The primary definition for the quantities $g_v$ and $g_\\rho$ is their role in the integral expression for $\\delta J$. This is how they are derived in practice using the adjoint-state method. Therefore, the problem is solvable by adhering strictly to the provided equations.\n\n### Step 3: Verdict and Action\nThe problem is valid. The internal tension regarding the definition of the inner product can be resolved by prioritizing the explicit formula for $\\delta J$ as the definition of the gradient kernels $g_v$ and $g_{\\rho}$, which reflects the computational practice. I will proceed with the detailed solution.\n\n### Derivation\n\nThe problem requires a three-part analysis based on dimensional consistency.\n\n**Part 1: Determine the physical units of $g_v$ and $g_{\\rho}$.**\n\nThe analysis begins with the definition of the variation of the misfit functional, $\\delta J$:\n$$\n\\delta J = \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} + \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x}\n$$\nWe are given that $J(m)$ is dimensionless, and therefore its variation $\\delta J$ is also dimensionless. Let the units of a quantity $Q$ be denoted by $[Q]$. Thus, $[\\delta J] = 1$.\n\nFor the sum on the right-hand side to be dimensionless, each integral term must also be dimensionless (assuming they are not defined to have equal and opposite non-dimensionless units, which would be physically nonsensical). Let's analyze the first term:\n$$\n\\left[ \\int_{\\Omega} g_v(\\mathbf{x}) \\, \\delta v(\\mathbf{x}) \\, d\\mathbf{x} \\right] = 1\n$$\nThe units of an integral are the product of the units of the integrand and the units of the measure.\n$$\n[g_v] \\cdot [\\delta v] \\cdot [d\\mathbf{x}] = 1\n$$\nFrom the problem statement, we have the units for the velocity perturbation, $[\\delta v] = [v] = [\\mathrm{m}/\\mathrm{s}]$, and the volume element, $[d\\mathbf{x}] = [\\mathrm{m}^3]$. Substituting these gives:\n$$\n[g_v] \\cdot \\left[\\frac{\\mathrm{m}}{\\mathrm{s}}\\right] \\cdot [\\mathrm{m}^3] = 1\n$$\n$$\n[g_v] \\cdot \\left[\\frac{\\mathrm{m}^4}{\\mathrm{s}}\\right] = 1\n$$\nSolving for the units of $g_v$:\n$$\n[g_v] = \\frac{1}{[\\mathrm{m}^4/\\mathrm{s}]} = \\left[\\frac{\\mathrm{s}}{\\mathrm{m}^4}\\right]\n$$\nSimilarly, for the second term involving density:\n$$\n\\left[ \\int_{\\Omega} g_{\\rho}(\\mathbf{x}) \\, \\delta \\rho(\\mathbf{x}) \\, d\\mathbf{x} \\right] = 1\n$$\n$$\n[g_{\\rho}] \\cdot [\\delta \\rho] \\cdot [d\\mathbf{x}] = 1\n$$\nUsing the units for the density perturbation, $[\\delta \\rho] = [\\rho] = [\\mathrm{kg}/\\mathrm{m}^3]$, and the volume element, $[d\\mathbf{x}] = [\\mathrm{m}^3]$:\n$$\n[g_{\\rho}] \\cdot \\left[\\frac{\\mathrm{kg}}{\\mathrm{m}^3}\\right] \\cdot [\\mathrm{m}^3] = 1\n$$\n$$\n[g_{\\rho}] \\cdot [\\mathrm{kg}] = 1\n$$\nSolving for the units of $g_{\\rho}$:\n$$\n[g_{\\rho}] = \\frac{1}{[\\mathrm{kg}]} = [\\mathrm{kg}^{-1}]\n$$\n\n**Part 2: Infer the units required of $\\alpha_v$ and $\\alpha_{\\rho}$.**\n\nThe gradient-based updates are given by:\n$$\nv_{k+1}(\\mathbf{x}) = v_k(\\mathbf{x}) - \\alpha_v \\, g_v(\\mathbf{x})\n$$\n$$\n\\rho_{k+1}(\\mathbf{x}) = \\rho_k(\\mathbf{x}) - \\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x})\n$$\nFor these equations to be physically consistent, all terms in each equation must have the same units (principle of dimensional homogeneity). For the velocity update:\n$$\n[v_{k+1}] = [v_k] = [\\alpha_v \\, g_v(\\mathbf{x})]\n$$\nWe know $[v_k] = [\\mathrm{m}/\\mathrm{s}]$ and we found $[g_v] = [\\mathrm{s}/\\mathrm{m}^4]$. Therefore:\n$$\n\\left[\\frac{\\mathrm{m}}{\\mathrm{s}}\\right] = [\\alpha_v] \\cdot \\left[\\frac{\\mathrm{s}}{\\mathrm{m}^4}\\right]\n$$\nSolving for the units of $\\alpha_v$:\n$$\n[\\alpha_v] = \\frac{[\\mathrm{m}/\\mathrm{s}]}{[\\mathrm{s}/\\mathrm{m}^4]} = \\left[\\frac{\\mathrm{m}^5}{\\mathrm{s}^2}\\right]\n$$\nFor the density update:\n$$\n[\\rho_{k+1}] = [\\rho_k] = [\\alpha_{\\rho} \\, g_{\\rho}(\\mathbf{x})]\n$$\nWe know $[\\rho_k] = [\\mathrm{kg}/\\mathrm{m}^3]$ and we found $[g_{\\rho}] = [\\mathrm{kg}^{-1}]$. Therefore:\n$$\n\\left[\\frac{\\mathrm{kg}}{\\mathrm{m}^3}\\right] = [\\alpha_{\\rho}] \\cdot [\\mathrm{kg}^{-1}]\n$$\nSolving for the units of $\\alpha_{\\rho}$:\n$$\n[\\alpha_{\\rho}] = \\frac{[\\mathrm{kg}/\\mathrm{m}^3]}{[\\mathrm{kg}^{-1}]} = \\left[\\frac{\\mathrm{kg}^2}{\\mathrm{m}^3}\\right]\n$$\nThis demonstrates that the step sizes $\\alpha_v$ and $\\alpha_{\\rho}$ must be dimensional quantities, and they have different physical units.\n\n**Part 3: Argument on per-parameter adaptivity.**\n\nPer-parameter adaptivity, as implemented in methods like Adam or RMSProp, typically computes adaptive scaling factors for each parameter based on the history of its gradient magnitudes. These factors are generally treated as dimensionless scalars that modulate a global learning rate.\n\nThe analysis above shows that if one works with the raw physical parameters ($v, \\rho$), the \"step sizes\" $\\alpha_v$ and $\\alpha_\\rho$ in the update rule are not simple scalars but must carry specific, and different, physical dimensions to ensure the updates $\\delta v = -\\alpha_v g_v$ and $\\delta \\rho = -\\alpha_\\rho g_\\rho$ have the correct units of velocity and density, respectively. A naive application of an adaptive algorithm that generates dimensionless adaptive factors for $\\alpha_v$ and $\\alpha_\\rho$ would result in dimensionally inconsistent updates. It is also physically meaningless to compare the magnitudes of $g_v$ and $g_{\\rho}$ (e.g., $\\|g_v\\|$ vs $\\|g_{\\rho}\\|$) as they have unrelated units. Such an algorithm would not be scale-invariant; changing the base units (e.g., meters to kilometers) would change the physical result of the optimization.\n\nTo correctly implement adaptivity, one must ensure dimensional consistency and scale invariance. There are two primary ways to achieve this:\n\n1.  **Nondimensionalization:** Define dimensionless model parameters, e.g., $\\tilde{v}(\\mathbf{x}) = v(\\mathbf{x})/v_0$ and $\\tilde{\\rho}(\\mathbf{x}) = \\rho(\\mathbf{x})/\\rho_0$, where $v_0$ and $\\rho_0$ are characteristic reference values. The optimization is then performed on $\\tilde{v}$ and $\\tilde{\\rho}$. The gradients with respect to these dimensionless parameters will have units that are directly comparable, and adaptive dimensionless step sizes can be meaningfully applied. The resulting step in the model space, $(\\delta \\tilde{v}, \\delta \\tilde{\\rho})$, is dimensionless.\n\n2.  **Dimensional Preconditioning:** Continue to work with the raw parameters $(v, \\rho)$ but recognize that $\\alpha_v$ and $\\alpha_\\rho$ are not step sizes but embodiments of a preconditioning operator. That is, the update is $\\delta m = -\\alpha P g$, where $P$ is a preconditioning matrix/operator and $\\alpha$ is a dimensionless step length. The problem's \"step sizes\" correspond to the entries of $P$: $\\alpha_v = \\alpha P_v$ and $\\alpha_\\rho = \\alpha P_\\rho$. Here, $P_v$ and $P_\\rho$ must carry the appropriate units $([\\mathrm{m}^5/\\mathrm{s}^2]$ and $[\\mathrm{kg}^2/\\mathrm{m}^3]$, respectively) to make the total update dimensionally correct. Adaptivity in this context means designing appropriate dimensional preconditioners, which is a more advanced concept than simply applying dimensionless factors.\n\nBoth approaches lead to the same conclusion: naively applying per-parameter adaptivity with dimensionless step sizes to raw physical parameters is incorrect. The update must either be performed on dimensionless variables, or the adaptive \"step sizes\" must be correctly constructed as dimensional operators.\n\n### Option-by-Option Analysis\n\n**A. In a properly nondimensionalized misfit, per-parameter adaptivity preserves a dimensionless gradient step only if the update is carried out in nondimensional variables, for example $\\tilde v = v/v_0$ and $\\tilde \\rho = \\rho/\\rho_0$ with reference scales $v_0$ and $\\rho_0$, or equivalently if $\\alpha_v$ and $\\alpha_{\\rho}$ are endowed with compensating physical units so that $\\alpha_v g_v$ and $\\alpha_{\\rho} g_{\\rho}$ have units $[\\mathrm{m}/\\mathrm{s}]$ and $[\\mathrm{kg}/\\mathrm{m}^3]$, respectively. Naively applying adaptivity on raw variables with unitless $\\alpha_v$ and $\\alpha_{\\rho}$ does not ensure dimensionless, scale-invariant updates.**\nThis option correctly describes the two valid approaches: nondimensionalization or using dimensional step sizes (preconditioners). It correctly identifies that the purpose of the dimensional step sizes is to ensure the model updates $\\alpha_v g_v$ and $\\alpha_\\rho g_\\rho$ have the correct physical units. It also correctly states that the naive approach is flawed.\n**Verdict: Correct.**\n\n**B. Because the misfit $J(m)$ is dimensionless, the gradients $g_v$ and $g_{\\rho}$ are also dimensionless, so any choice of dimensionless per-parameter adaptive factors automatically preserves the correct dimensionless gradient step.**\nThe premise is false. As derived above, the dimensionless nature of $J(m)$ necessitates that the gradient kernels $g_v$ and $g_{\\rho}$ be dimensional to compensate for the dimensions of the model parameters and the volume element in the defining integral. Since the premise is false, the conclusion is unfounded.\n**Verdict: Incorrect.**\n\n**C. The gradient $g_v$ has units $[\\mathrm{m}/\\mathrm{s}]$ and $g_{\\rho}$ has units $[\\mathrm{kg}/\\mathrm{m}^3]$ by definition of the gradient, so a single dimensionless step size applied to both parameters always preserves physical scaling.**\nThis option incorrectly assumes the gradient kernels $g_v, g_\\rho$ (as defined by the integral for $\\delta J$) must have the same units as the model parameters. This would only be true if the gradient were defined with respect to a specific, physically-weighted inner product, which is not what the provided formula for $\\delta J$ implies. Based on the given equations, the units are $[g_v] = [\\mathrm{s}/\\mathrm{m}^4]$ and $[g_\\rho] = [\\mathrm{kg}^{-1}]$. Therefore, the premise is incorrect.\n**Verdict: Incorrect.**\n\n**D. Under the spatial $L^2(\\Omega)$ inner product, $g_v$ has units $[\\mathrm{s}/\\mathrm{m}^4]$ and $g_{\\rho}$ has units $[1/\\mathrm{kg}]$, but choosing adaptive step sizes $\\alpha_v \\propto 1/\\|g_v\\|$ and $\\alpha_{\\rho} \\propto 1/\\|g_{\\rho}\\|$ automatically removes units, making the updates unitless and invariant regardless of parameterization.**\nThe first part of this statement correctly identifies the units of $g_v$ and $g_\\rho$ as derived from the problem's definition of $\\delta J$. However, the second part is incorrect. A step size proportional to $1/\\|g_v\\|$ does not \"automatically remove units\" in a way that makes the update dimensionally correct. As shown in the thought process, a choice like $\\alpha_v \\propto 1/\\|g_v\\|_{L^2}$ results in an update $\\delta v = \\alpha_v g_v$ with incorrect physical units. The update is not made \"unitless\"; the goal is for the update $\\delta v$ to have the same units as $v$. This option misrepresents the result of dimensional analysis on adaptive steps.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}