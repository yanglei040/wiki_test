{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with a direct application of the definition of the Moore-Penrose pseudoinverse. This exercise  asks you to compute the pseudoinverse of a simple, rank-deficient diagonal matrix. By working through this fundamental case, you will see precisely how the singular value decomposition (SVD) allows us to construct a stable inverse by selectively inverting the non-zero singular values, providing a clear illustration of the core mechanic.",
            "id": "1049323",
            "problem": "Compute the Moore-Penrose pseudoinverse of the matrix $A = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$ using singular value decomposition. Present your answer as a 2x2 matrix.",
            "solution": "1. Singular value decomposition of $A$:  \n   Since $A = \\begin{pmatrix}10\\\\00\\end{pmatrix}$ is already diagonal, we may choose  \n   \n$$\n   U = I,\\quad \\Sigma = \\begin{pmatrix}10\\\\00\\end{pmatrix},\\quad V = I,\n   $$\n  \n   so that $A = U\\,\\Sigma\\,V^T$.  \n2. Form the pseudoinverse of $\\Sigma$:  \n   \n$$\n   \\Sigma^+ = \\begin{pmatrix}1^{-1}0\\\\00\\end{pmatrix}\n            = \\begin{pmatrix}10\\\\00\\end{pmatrix},\n   $$\n  \n   where the reciprocal of the zero singular value is set to zero.  \n3. Compute $A^+$:  \n   \n$$\n   A^+ = V\\,\\Sigma^+\\,U^T\n       = I\\,\\begin{pmatrix}10\\\\00\\end{pmatrix}\\,I\n       = \\begin{pmatrix}10\\\\00\\end{pmatrix}.\n   $$",
            "answer": "$$\\boxed{\\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}}$$"
        },
        {
            "introduction": "Moving from pure computation to physical interpretation, we now explore how SVD serves as a powerful diagnostic tool in geophysics. This practice  situates the generalized inverse within a realistic, albeit simplified, seismic refraction experiment. Your task is to analyze the SVD of the forward operator to determine which model parameters are well-resolved, poorly resolved, or unresolved, connecting the mathematical properties of singular vectors and values to tangible physical constraints.",
            "id": "3616825",
            "problem": "Consider a linearized seismic refraction experiment with a single refracting interface over a half-space. Let the model perturbation vector be $\\delta \\mathbf{m} = [\\delta s_1,\\ \\delta s_2,\\ \\delta z]^\\top$, where $\\delta s_1$ and $\\delta s_2$ are perturbations in the upper-layer and refractor slownesses (seconds per meter), and $\\delta z$ is the perturbation in interface depth (meters). For offsets $x_k$ at large ranges (head-wave regime), the travel-time perturbations $\\delta t_k$ can be approximated by a first-order linearization around a known reference model as\n$$\n\\delta \\mathbf{d} \\approx G \\, \\delta \\mathbf{m},\n$$\nwith $G \\in \\mathbb{R}^{N \\times 3}$ collecting the partial derivatives $\\partial t_k / \\partial m_j$. In a synthetic test with $N=5$ offsets $x_k \\in \\{500,\\ 1000,\\ 1500,\\ 2000,\\ 2500\\}$ meters, suppose the sensitivities are dominated by the refractor slowness through the slope of the travel-time curve and weakly sensitive to the intercept time through depth and upper-layer slowness. Take\n$$\nG = \\begin{bmatrix}\na  x_1  b \\\\\na  x_2  b \\\\\na  x_3  b \\\\\na  x_4  b \\\\\na  x_5  b\n\\end{bmatrix},\n$$\nwith $a = 0.02$ seconds, $b = 0.01$ seconds, and $x_k$ as given above. Assume the data are generated by a true perturbation $\\delta \\mathbf{m}_{\\text{true}} = [0,\\ \\delta s_2^{\\star},\\ 0]^\\top$ with $\\delta s_2^{\\star} = 2 \\times 10^{-4}$ seconds per meter, so that $\\delta \\mathbf{d} = G \\, \\delta \\mathbf{m}_{\\text{true}}$.\n\nUsing the definition of the Singular Value Decomposition (SVD) and the generalized inverse (Moore–Penrose pseudoinverse), analyze the relative magnitudes of the singular values and the alignment of the right singular vectors $\\mathbf{v}_i$ of $G$. From this analysis, determine which statement below best describes the behavior of the generalized inverse $G^{+}$ and its physical interpretation in this simple refraction scenario (large-offset head-wave data).\n\nA. Only the component along the leading right singular vector $\\mathbf{v}_1$ aligned with $\\delta s_2$ contributes significantly to $G^{+}$, so $\\delta s_2$ is stably recovered, while $\\delta s_1$ and $\\delta z$ remain poorly resolved due to tiny singular values. This reflects that large-offset head waves primarily constrain the slope (refractor slowness), not the intercept.\n\nB. Because the two constant columns are collinear, the matrix $G$ is exactly rank-$1$, so both $\\delta z$ and $\\delta s_1$ are uniquely recoverable once $\\delta s_2$ is known.\n\nC. The leading right singular vector aligns with the constant columns ($\\delta z$ and $\\delta s_1$), implying that large-offset data primarily constrain the intercept time and not the slope, so $\\delta s_2$ is unresolvable.\n\nD. In the absence of regularization, the generalized inverse amplifies contributions from small singular values more than from the largest one, so estimates of $\\delta s_1$ and $\\delta z$ are more stable than $\\delta s_2$.",
            "solution": "### Step 1: Extract Givens\n\nThe problem provides a linearized model for seismic refraction travel-time perturbations, $\\delta \\mathbf{d} \\approx G \\, \\delta \\mathbf{m}$.\n\n-   Model perturbation vector: $\\delta \\mathbf{m} = [\\delta s_1, \\delta s_2, \\delta z]^\\top$, where $\\delta s_1$ is the upper-layer slowness perturbation, $\\delta s_2$ is the refractor slowness perturbation, and $\\delta z$ is the interface depth perturbation.\n-   Data perturbation vector: $\\delta \\mathbf{d} = [\\delta t_1, ..., \\delta t_N]^\\top$, with $N=5$.\n-   Offsets: $x_k \\in \\{500, 1000, 1500, 2000, 2500\\}$ meters.\n-   Sensitivity matrix $G$:\n    $$\n    G = \\begin{bmatrix}\n    a  x_1  b \\\\\n    a  x_2  b \\\\\n    a  x_3  b \\\\\n    a  x_4  b \\\\\n    a  x_5  b\n    \\end{bmatrix}\n    $$\n-   Constants: $a = 0.02$ seconds, $b = 0.01$ seconds.\n-   True model perturbation: $\\delta \\mathbf{m}_{\\text{true}} = [0, \\delta s_2^{\\star}, 0]^\\top$, with $\\delta s_2^{\\star} = 2 \\times 10^{-4}$ s/m.\n-   Data generation model: $\\delta \\mathbf{d} = G \\, \\delta \\mathbf{m}_{\\text{true}}$.\n\nThe task is to analyze the Singular Value Decomposition (SVD) of $G$ and the behavior of the generalized inverse $G^{+}$ to understand the resolution of the model parameters.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is a simplified but physically motivated representation of a linearized seismic refraction inverse problem. The travel time of a head wave is approximately linear with offset $x$, $t(x) \\approx s_2 x + t_i$, where $s_2$ is the refractor slowness (slope) and $t_i$ is the intercept time, which depends on the upper-layer slowness $s_1$ and depth $z$. The given matrix $G$ represents the partial derivatives $\\partial t / \\partial m_j$. The entries $G_{k1} = \\partial t_k / \\partial s_1 = a$ and $G_{k3} = \\partial t_k / \\partial z = b$ being constant is a simplification, but captures the fact that $s_1$ and $z$ primarily affect the intercept. The entry $G_{k2} = \\partial t_k / \\partial s_2 = x_k$ correctly captures that refractor slowness $s_2$ primarily affects the slope. The setup is a valid and common trope in teaching inverse theory.\n-   **Well-Posed**: The matrix $G$ is explicitly defined. The problem asks for an analysis of its properties (SVD, generalized inverse), which is a well-defined mathematical task.\n-   **Objective**: The problem is stated with precise numerical values and mathematical definitions. It is free from ambiguity or subjectivity.\n\nThe problem is self-contained and scientifically/mathematically sound for its stated purpose. It does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. Proceeding with the solution.\n\n### Derivation and Analysis\n\nThe sensitivity matrix $G$ is a $5 \\times 3$ matrix defined as:\n$$\nG = \\begin{bmatrix}\n0.02  500  0.01 \\\\\n0.02  1000  0.01 \\\\\n0.02  1500  0.01 \\\\\n0.02  2000  0.01 \\\\\n0.02  2500  0.01\n\\end{bmatrix}\n$$\nLet the columns of $G$ be $\\mathbf{g}_1$, $\\mathbf{g}_2$, and $\\mathbf{g}_3$. These columns are the sensitivity vectors for the model parameters $\\delta s_1$, $\\delta s_2$, and $\\delta z$ respectively.\n$$\n\\mathbf{g}_1 = 0.02 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\n\\mathbf{g}_2 = \\begin{bmatrix} 500 \\\\ 1000 \\\\ 1500 \\\\ 2000 \\\\ 2500 \\end{bmatrix}, \\quad\n\\mathbf{g}_3 = 0.01 \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n$$\n**1. Rank and Null Space of G**\n\nWe observe that the first and third columns are multiples of the vector of ones, and are therefore collinear: $\\mathbf{g}_1 = 2 \\mathbf{g}_3$. This linear dependence means the rank of $G$ is less than $3$. The second column, $\\mathbf{g}_2$, is not a multiple of the others. Thus, the columns of $G$ span a two-dimensional space. The rank of $G$ is $2$.\n\nSince $G$ is a $5 \\times 3$ matrix of rank $2$, it has a one-dimensional null space. A vector $\\delta\\mathbf{m}_{null} = [c_1, c_2, c_3]^\\top$ is in the null space if $G \\delta\\mathbf{m}_{null} = \\mathbf{0}$, which means $c_1 \\mathbf{g}_1 + c_2 \\mathbf{g}_2 + c_3 \\mathbf{g}_3 = \\mathbf{0}$.\nSubstituting $\\mathbf{g}_1 = 2\\mathbf{g}_3$:\n$c_1 (2\\mathbf{g}_3) + c_2 \\mathbf{g}_2 + c_3 \\mathbf{g}_3 = \\mathbf{0} \\implies c_2 \\mathbf{g}_2 + (2c_1 + c_3) \\mathbf{g}_3 = \\mathbf{0}$.\nSince $\\mathbf{g}_2$ and $\\mathbf{g}_3$ are linearly independent, we must have $c_2 = 0$ and $2c_1 + c_3 = 0$. We can choose $c_1=1$, which implies $c_3=-2$.\nSo, a basis for the null space is $\\delta\\mathbf{m}_{null} = [1, 0, -2]^\\top$. This means any model perturbation of the form $[k, 0, -2k]^\\top$ for any constant $k$ produces zero data perturbation. Physically, the data are insensitive to this specific trade-off between the upper-layer slowness $\\delta s_1$ and the depth $\\delta z$.\n\n**2. Singular Value Decomposition (SVD) Analysis**\n\nThe SVD of $G$ is $G = U \\Sigma V^\\top$, where the columns of $V=[\\mathbf{v}_1, \\mathbf{v}_2, \\mathbf{v}_3]$ are the right singular vectors (an orthonormal basis for the model space $\\mathbb{R}^3$), and $\\Sigma$ contains the singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$.\nThe squares of the singular values, $\\sigma_i^2$, are the eigenvalues of $G^\\top G$.\n$$\nG^\\top G = \\begin{bmatrix} \\mathbf{g}_1^\\top \\mathbf{g}_1  \\mathbf{g}_1^\\top \\mathbf{g}_2  \\mathbf{g}_1^\\top \\mathbf{g}_3 \\\\ \\mathbf{g}_2^\\top \\mathbf{g}_1  \\mathbf{g}_2^\\top \\mathbf{g}_2  \\mathbf{g_2}^\\top \\mathbf{g}_3 \\\\ \\mathbf{g}_3^\\top \\mathbf{g}_1  \\mathbf{g}_3^\\top \\mathbf{g}_2  \\mathbf{g}_3^\\top \\mathbf{g}_3 \\end{bmatrix}\n$$\nLet's compute the squared norms of the columns of $G$:\n$\\|\\mathbf{g}_1\\|^2 = 5 \\times (0.02)^2 = 0.002$.\n$\\|\\mathbf{g}_3\\|^2 = 5 \\times (0.01)^2 = 0.0005$.\n$\\|\\mathbf{g}_2\\|^2 = 500^2 + 1000^2 + 1500^2 + 2000^2 + 2500^2 = 250000(1+4+9+16+25) = 13,750,000$.\n\nThe norm of $\\mathbf{g}_2$ is vastly larger than the norms of $\\mathbf{g}_1$ and $\\mathbf{g}_3$. This indicates that the system is most sensitive to perturbations in $\\delta s_2$. In the SVD, this translates to the largest singular value, $\\sigma_1$, being primarily associated with the direction of $\\mathbf{g}_2$. The corresponding right singular vector, $\\mathbf{v}_1$, will be closely aligned with the $\\delta s_2$ axis, i.e., $\\mathbf{v}_1 \\approx [0, 1, 0]^\\top$. The singular value will be approximately $\\|\\mathbf{g}_2\\|$: $\\sigma_1 \\approx \\sqrt{13,750,000} \\approx 3708$.\n\nSince the rank is $2$, there are two non-zero singular values, $\\sigma_1$ and $\\sigma_2$, and one zero singular value, $\\sigma_3=0$. The right singular vector $\\mathbf{v}_3$ corresponding to $\\sigma_3=0$ must span the null space. Normalizing the null space vector gives $\\mathbf{v}_3 = \\frac{1}{\\sqrt{1^2+0^2+(-2)^2}}[1, 0, -2]^\\top = \\frac{1}{\\sqrt{5}}[1, 0, -2]^\\top$.\n\nThe remaining right singular vector, $\\mathbf{v}_2$, corresponds to a combination of $\\delta s_1$ and $\\delta z$. It must be orthogonal to $\\mathbf{v}_1$ and $\\mathbf{v}_3$. Since $\\mathbf{v}_1 \\approx [0,1,0]^\\top$ and $\\mathbf{v}_3$ is in the $s_1-z$ plane, $\\mathbf{v}_2$ must also be in the $s_1-z$ plane and orthogonal to $\\mathbf{v}_3$, so $\\mathbf{v}_2 \\propto [2, 0, 1]^\\top$. The corresponding singular value $\\sigma_2$ will be much smaller than $\\sigma_1$. It is associated with resolving the intercept time, which is controlled by a linear combination of $\\delta s_1$ and $\\delta z$. Comparing $\\sigma_1 \\approx 3708$ to $\\sigma_2$, whose square is on the order of $\\|\\mathbf{g}_1\\|^2$ and $\\|\\mathbf{g}_3\\|^2$ (i.e., $\\sigma_2^2 \\sim 0.0025$, so $\\sigma_2 \\sim 0.05$), we see a very large condition number $\\sigma_1/\\sigma_2 \\gg 1$.\n\n**3. The Generalized Inverse and Model Resolution**\n\nThe generalized inverse solution for the model parameters is $\\delta\\mathbf{m}_{est} = G^+ \\delta\\mathbfd$. The generalized inverse $G^+$ is given by $G^+ = V \\Sigma^+ U^\\top$. In terms of the SVD components, the solution is:\n$$\n\\delta\\mathbf{m}_{est} = \\sum_{i=1}^{\\text{rank(G)}} \\frac{1}{\\sigma_i} \\mathbf{v}_i (\\mathbf{u}_i^\\top \\delta\\mathbf{d})\n$$\nFor this problem, with rank $2$:\n$$\n\\delta\\mathbf{m}_{est} = \\frac{1}{\\sigma_1} (\\mathbf{u}_1^\\top \\delta\\mathbf{d}) \\mathbf{v}_1 + \\frac{1}{\\sigma_2} (\\mathbf{u}_2^\\top \\delta\\mathbf{d}) \\mathbf{v}_2\n$$\nThe variance of the estimated parameters (assuming uncorrelated data noise with variance $\\sigma_d^2$) is given by $\\text{Cov}(\\delta\\mathbf{m}_{est}) = \\sigma_d^2 (G^\\top G)^+ = \\sigma_d^2 \\sum_{i=1}^2 \\frac{1}{\\sigma_i^2} \\mathbf{v}_i \\mathbf{v}_i^\\top$.\n\n-   The component of the solution along $\\mathbf{v}_1$ (which is almost purely $\\delta s_2$) is scaled by $1/\\sigma_1$. Since $\\sigma_1$ is large, this component is stable and well-resolved. Its variance is low ($\\propto 1/\\sigma_1^2$).\n-   The component of the solution along $\\mathbf{v}_2$ (a combination of $\\delta s_1$ and $\\delta z$) is scaled by $1/\\sigma_2$. Since $\\sigma_2$ is small, this component is unstable and highly sensitive to noise in the data $\\delta\\mathbf{d}$. Its variance is high ($\\propto 1/\\sigma_2^2$).\n-   The component of the solution along $\\mathbf{v}_3$ (another combination of $\\delta s_1$ and $\\delta z$) is completely unconstrained, as $\\sigma_3=0$. The generalized inverse solution has zero component in this direction. This is an unresolved ambiguity.\n\nIn summary, $\\delta s_2$ is well-resolved and stable. The parameters $\\delta s_1$ and $\\delta z$ are poorly resolved due to one small singular value ($\\sigma_2$) and one zero singular value ($\\sigma_3$). This is the mathematical manifestation of the physical principle that long baseline refraction data constrain the slope of the travel-time curve ($s_2$) very well, but are less sensitive to the intercept time (a function of $s_1$ and $z$).\n\n### Option-by-Option Analysis\n\n**A. Only the component along the leading right singular vector $\\mathbf{v}_1$ aligned with $\\delta s_2$ contributes significantly to $G^{+}$, so $\\delta s_2$ is stably recovered, while $\\delta s_1$ and $\\delta z$ remain poorly resolved due to tiny singular values. This reflects that large-offset head waves primarily constrain the slope (refractor slowness), not the intercept.**\n-   The leading right singular vector $\\mathbf{v}_1$ corresponds to the largest singular value $\\sigma_1$ and is aligned with the $\\delta s_2$ direction. Correct.\n-   The term \"contributes significantly to $G^{+}$\" can be interpreted as the part of the solution that is stable and well-determined. With this reasonable interpretation, the component related to $\\sigma_1$ and $\\mathbf{v}_1$ is the significant part of a useful solution.\n-   $\\delta s_2$ is stably recovered. Correct, as this corresponds to the large $\\sigma_1$.\n-   $\\delta s_1$ and $\\delta z$ are poorly resolved due to tiny/zero singular values. Correct, their combinations are associated with the small $\\sigma_2$ and zero $\\sigma_3$.\n-   The physical interpretation is correct.\nThis statement accurately describes the situation.\nVerdict: **Correct**.\n\n**B. Because the two constant columns are collinear, the matrix $G$ is exactly rank-$1$, so both $\\delta z$ and $\\delta s_1$ are uniquely recoverable once $\\delta s_2$ is known.**\n-   The two constant columns are indeed collinear.\n-   However, the matrix $G$ is not rank-$1$. The column $\\mathbf{g}_2$ is linearly independent of the other two. The rank is $2$. This is a factual error.\n-   Because of the null space, even if $\\delta s_2$ is known, we can only solve for the combination $2\\delta s_1 + \\delta z$, not for $\\delta s_1$ and $\\delta z$ individually. The statement that they are uniquely recoverable is false.\nVerdict: **Incorrect**.\n\n**C. The leading right singular vector aligns with the constant columns ($\\delta z$ and $\\delta s_1$), implying that large-offset data primarily constrain the intercept time and not the slope, so $\\delta s_2$ is unresolvable.**\n-   The leading right singular vector $\\mathbf{v}_1$ (associated with $\\sigma_1$) aligns with the direction of the column with the largest norm, which is $\\mathbf{g}_2$ (the offset vector), not the constant columns. This statement is the opposite of the truth.\n-   The physical implication is also backwards. Large-offset data constrain the slope, not the intercept.\n-   Therefore, the conclusion that $\\delta s_2$ is unresolvable is false; it is the most resolvable parameter.\nVerdict: **Incorrect**.\n\n**D. In the absence of regularization, the generalized inverse amplifies contributions from small singular values more than from the largest one, so estimates of $\\delta s_1$ and $\\delta z$ are more stable than $\\delta s_2$.**\n-   The first part is correct: the generalized inverse contains factors of $1/\\sigma_i$, so small singular values lead to large amplification factors.\n-   The conclusion is incorrect. This large amplification applies to noise and makes the estimates *unstable*, not more stable. The parameters associated with small singular values ($\\delta s_1$ and $\\delta z$) are less stable than the parameter associated with the large singular value ($\\delta s_2$).\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Our final practice advances from diagnosis to a quantitative analysis of inversion quality, introducing the indispensable concepts of the model resolution matrix $R$ and the point-spread function (PSF). This comprehensive exercise  challenges you to derive and implement the resolution matrix for several common estimators, including truncated SVD and Tikhonov regularization. By computing the PSF for a geophysical blurring operator, you will gain hands-on experience in evaluating how regularization choices create a trade-off between solution stability and the smearing of model features.",
            "id": "3616758",
            "problem": "Consider a linear forward mapping in computational geophysics where the relationship between the data vector $\\mathbf{d} \\in \\mathbb{R}^{n_d}$ and the model vector $\\mathbf{m} \\in \\mathbb{R}^{n_m}$ is $\\mathbf{d} = G \\mathbf{m}$, with $G \\in \\mathbb{R}^{n_d \\times n_m}$ known. Let the singular value decomposition (SVD) of $G$ be $G = U \\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{n_d \\times n_d}$ and $V \\in \\mathbb{R}^{n_m \\times n_m}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n_d \\times n_m}$ contains nonnegative singular values on its main diagonal. The generalized inverse is understood in the sense of the Moore–Penrose pseudoinverse and its regularized variants that minimize $2$-norm norms subject to data fitting. The model resolution matrix is defined as $R \\in \\mathbb{R}^{n_m \\times n_m}$, which quantifies how true model parameters are mapped into the estimated model.\n\nYour task is to start from these definitions and basic properties of orthogonal projections and least-squares solutions and derive an expression for the point-spread function (also called the resolution kernel) for a single model parameter. Specifically:\n\n1. Using only the foundational definitions provided above (the SVD of $G$, orthogonality of $U$ and $V$, and the characterization of the Moore–Penrose pseudoinverse and standard quadratic regularization), derive the expression for the model resolution matrix $R$ as a function of the singular values and singular vectors for the following estimators:\n   - The undamped least-squares estimator corresponding to the Moore–Penrose pseudoinverse.\n   - The truncated SVD estimator, which retains only the largest $k$ singular values.\n   - The zeroth-order Tikhonov-damped least-squares estimator with damping parameter $\\lambda  0$.\n2. Let $\\mathbf{e}_j \\in \\mathbb{R}^{n_m}$ denote the unit basis vector with a $1$ at index $j$ and zeros elsewhere (use a zero-based indexing convention for $j$ in any computation). Define the point-spread function for index $j$ as $\\mathbf{r}^{(j)} = R \\mathbf{e}_j$. Derive the functional dependence of $\\mathbf{r}^{(j)}$ on the right singular vectors of $G$ and the singular values through coefficients that depend on whether the estimator is undamped, truncated, or damped. Explain qualitatively how the right singular vectors weight the spread of $\\mathbf{e}_j$ in the estimated model.\n\nThen, implement a program that computes $\\mathbf{r}^{(j)}$ using the SVD for a specified $G$ and estimator choice. Construct $G$ as a Gaussian blur operator on a one-dimensional grid of size $n$ with entries\n$$\nG_{pq} = \\exp\\!\\left(-\\frac{(p - q)^2}{2 s^2}\\right), \\quad p, q \\in \\{0, 1, \\dots, n-1\\},\n$$\nand normalize the columns of $G$ to unit $2$-norm to avoid trivial scaling effects in the singular spectrum. Use $n_d = n_m = n$ with $n = 20$, and choose $s$ as specified per test case.\n\nFor each test case, compute the SVD of $G$, form the resolution matrix $R$ implied by the estimator, compute the point-spread function $\\mathbf{r}^{(j)} = R \\mathbf{e}_j$, and then return:\n- The full vector $\\mathbf{r}^{(j)}$.\n- The off-diagonal $1$-norm mass defined as $\\sum_{i=0}^{n-1} \\left| r^{(j)}_i \\right| - \\left| r^{(j)}_j \\right|$.\n- The index of the largest component of $\\mathbf{r}^{(j)}$ in absolute value (zero-based), i.e., $\\arg\\max_i \\left| r^{(j)}_i \\right|$.\n\nNo physical units are involved; all quantities are unitless. Angles are not used. There are no percentages.\n\nTest Suite:\nUse $n = 20$ and construct $G$ from the Gaussian blur with the following parameter sets. Each test case is a triple specifying the estimator type, its parameter, and the index $j$:\n- Case $1$: Undamped pseudoinverse, $s = 1.25$, $j = 10$.\n- Case $2$: Truncated SVD with $k = 8$ retained singular values, $s = 1.25$, $j = 10$.\n- Case $3$: Zeroth-order Tikhonov damping with $\\lambda = 0.25$, $s = 1.25$, $j = 3$.\n- Case $4$: Zeroth-order Tikhonov damping with $\\lambda = 1.00$, $s = 1.25$, $j = 17$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list containing the three requested items $[\\mathbf{r}^{(j)}, \\text{off\\_diag\\_mass}, \\text{argmax\\_index}]$, where $\\mathbf{r}^{(j)}$ is represented as a Python list of floats. For example, the top-level output will look like\n$$\n[\\,[\\,[r_0,\\dots,r_{n-1}],\\,\\text{off\\_diag\\_mass},\\,\\text{argmax\\_index}\\,],\\; \\dots\\; ].\n$$",
            "solution": "The user has provided a valid problem. The definitions and tasks are grounded in the established principles of linear inverse theory and numerical linear algebra, specifically as applied in computational geophysics. The problem is well-posed, objective, and self-contained. I will now proceed with the derivation and solution.\n\nThe core of the problem is to analyze the model resolution matrix $R$ and the associated point-spread function (PSF) for different linear estimators of a model vector $\\mathbf{m}$ from a data vector $\\mathbf{d}$, given the forward relationship $\\mathbf{d} = G \\mathbf{m}$. The estimated model $\\mathbf{m}_{\\text{est}}$ is obtained via a generalized inverse $G_{\\text{gen}}$ as $\\mathbf{m}_{\\text{est}} = G_{\\text{gen}} \\mathbf{d}$. Substituting the true relationship, we get $\\mathbf{m}_{\\text{est}} = G_{\\text{gen}} (G \\mathbf{m}_{\\text{true}})$. The model resolution matrix $R$ is defined by the mapping from the true model to the estimated model: $\\mathbf{m}_{\\text{est}} = R \\mathbf{m}_{\\text{true}}$. Therefore, the resolution matrix is given by $R = G_{\\text{gen}} G$.\n\nWe are given the singular value decomposition (SVD) of the matrix $G \\in \\mathbb{R}^{n_d \\times n_m}$ as $G = U \\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{n_d \\times n_d}$ and $V \\in \\mathbb{R}^{n_m \\times n_m}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n_d \\times n_m}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_i$ on its main diagonal. The columns of $U$, $\\mathbf{u}_i$, are the left singular vectors and the columns of $V$, $\\mathbf{v}_i$, are the right singular vectors. For this problem, $n_d = n_m = n$, so all matrices are square, $n \\times n$.\n\n### 1. Derivation of the Model Resolution Matrix $R$\n\nWe derive the expression for $R$ for three different estimators.\n\n**a) Undamped Least-Squares (Moore-Penrose Pseudoinverse)**\n\nThe undamped least-squares solution minimizes $\\|G \\mathbf{m} - \\mathbf{d}\\|_2^2$. The estimator is $\\mathbf{m}_{\\text{est}} = G^{+} \\mathbf{d}$, where $G^{+}$ is the Moore-Penrose pseudoinverse. Thus, $G_{\\text{gen}} = G^{+}$. The pseudoinverse is defined via the SVD as $G^{+} = V \\Sigma^{+} U^{\\mathsf{T}}$. The matrix $\\Sigma^{+}$ is obtained by taking the reciprocal of the non-zero singular values in $\\Sigma$ and transposing the resulting matrix. For a square $n \\times n$ matrix $\\Sigma$ with singular values $\\sigma_i$, the matrix $\\Sigma^{+}$ is also $n \\times n$ and diagonal, with entries $(\\Sigma^{+})_{ii} = 1/\\sigma_i$ if $\\sigma_i  0$ and $0$ if $\\sigma_i = 0$.\n\nThe resolution matrix is $R = G^{+} G$.\n$$\nR = (V \\Sigma^{+} U^{\\mathsf{T}}) (U \\Sigma V^{\\mathsf{T}}) = V \\Sigma^{+} (U^{\\mathsf{T}}U) \\Sigma V^{\\mathsf{T}}\n$$\nSince $U$ is an orthogonal matrix, $U^{\\mathsf{T}}U = I$, the identity matrix.\n$$\nR = V (\\Sigma^{+} \\Sigma) V^{\\mathsf{T}}\n$$\nThe product $\\Sigma^{+} \\Sigma$ is a diagonal matrix. Its diagonal entries are $(\\Sigma^{+} \\Sigma)_{ii} = (1/\\sigma_i) \\cdot \\sigma_i = 1$ if $\\sigma_i  0$, and $0$ if $\\sigma_i = 0$. Let $r$ be the rank of $G$ (the number of non-zero singular values). Then $\\Sigma^{+} \\Sigma$ is a diagonal matrix with $r$ ones and $n-r$ zeros on its diagonal. We denote this projection matrix as $I_r$.\nThus, the resolution matrix for the undamped case is:\n$$\nR = V I_r V^{\\mathsf{T}} = \\sum_{i=0}^{r-1} \\mathbf{v}_i \\mathbf{v}_i^{\\mathsf{T}}\n$$\nThis matrix represents the orthogonal projection onto the subspace spanned by the first $r$ right singular vectors. If $G$ is of full rank ($r=n$), then $I_r=I$ and $R = VIV^{\\mathsf{T}} = I$, implying perfect resolution.\n\n**b) Truncated SVD (TSVD) Estimator**\n\nThe TSVD estimator uses only the first $k$ largest singular values, effectively filtering out components associated with small singular values that amplify noise. The generalized inverse is $G_k^{+} = V \\Sigma_k^{+} U^{\\mathsf{T}}$, where $(\\Sigma_k^{+})_{ii} = 1/\\sigma_i$ for $i=0, \\dots, k-1$ and $0$ otherwise.\n\nThe resolution matrix $R_k$ is:\n$$\nR_k = G_k^{+} G = (V \\Sigma_k^{+} U^{\\mathsf{T}}) (U \\Sigma V^{\\mathsf{T}}) = V (\\Sigma_k^{+} \\Sigma) V^{\\mathsf{T}}\n$$\nThe product $\\Sigma_k^{+} \\Sigma$ is a diagonal matrix $I_k$ with ones at the first $k$ diagonal positions and zeros elsewhere.\n$$\nR_k = V I_k V^{\\mathsf{T}} = \\sum_{i=0}^{k-1} \\mathbf{v}_i \\mathbf{v}_i^{\\mathsf{T}}\n$$\nThis is an orthogonal projection onto the subspace spanned by the first $k$ right singular vectors.\n\n**c) Zeroth-Order Tikhonov-Damped Least-Squares Estimator**\n\nThis estimator finds the model $\\mathbf{m}$ that minimizes the combined objective function $\\|G \\mathbf{m} - \\mathbf{d}\\|_2^2 + \\lambda^2 \\|\\mathbf{m}\\|_2^2$, where $\\lambda  0$ is the damping parameter. The solution is given by $\\mathbf{m}_{\\text{est}} = (G^{\\mathsf{T}}G + \\lambda^2 I)^{-1} G^{\\mathsf{T}} \\mathbf{d}$.\nSo, $G_{\\text{gen}} = (G^{\\mathsf{T}}G + \\lambda^2 I)^{-1} G^{\\mathsf{T}}$.\n\nThe resolution matrix $R_\\lambda$ is:\n$$\nR_\\lambda = G_{\\text{gen}} G = (G^{\\mathsf{T}}G + \\lambda^2 I)^{-1} G^{\\mathsf{T}}G\n$$\nUsing the SVD, we substitute $G = U \\Sigma V^{\\mathsf{T}}$ and $G^{\\mathsf{T}} = V \\Sigma^{\\mathsf{T}} U^{\\mathsf{T}}$. For our square case, $\\Sigma$ is diagonal and $\\Sigma^{\\mathsf{T}} = \\Sigma$.\n$$\nG^{\\mathsf{T}}G = (V \\Sigma U^{\\mathsf{T}})(U \\Sigma V^{\\mathsf{T}}) = V \\Sigma^2 V^{\\mathsf{T}}\n$$\nSubstituting this into the expression for $R_\\lambda$:\n$$\nR_\\lambda = (V \\Sigma^2 V^{\\mathsf{T}} + \\lambda^2 I)^{-1} (V \\Sigma^2 V^{\\mathsf{T}})\n$$\nUsing $I = VIV^{\\mathsf{T}}$, we can factor out $V$ and $V^{\\mathsf{T}}$:\n$$\nR_\\lambda = (V (\\Sigma^2 + \\lambda^2 I) V^{\\mathsf{T}})^{-1} (V \\Sigma^2 V^{\\mathsf{T}}) = V (\\Sigma^2 + \\lambda^2 I)^{-1} V^{\\mathsf{T}} V \\Sigma^2 V^{\\mathsf{T}}\n$$\nSince $V^{\\mathsf{T}}V = I$:\n$$\nR_\\lambda = V \\left[ (\\Sigma^2 + \\lambda^2 I)^{-1} \\Sigma^2 \\right] V^{\\mathsf{T}}\n$$\nThe term in the brackets is a diagonal matrix, let's call it $F_\\lambda$, whose diagonal entries are the filter factors $f_i = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$.\nThus, the Tikhonov resolution matrix is:\n$$\nR_\\lambda = V F_\\lambda V^{\\mathsf{T}} = \\sum_{i=0}^{n-1} \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} \\mathbf{v}_i \\mathbf{v}_i^{\\mathsf{T}}\n$$\n\n### 2. Derivation and Interpretation of the Point-Spread Function (PSF)\n\nThe point-spread function (PSF) for the $j$-th model parameter, $\\mathbf{r}^{(j)}$, is defined as the response of the estimator to an impulse in the true model at that parameter, i.e., $\\mathbf{m}_{\\text{true}} = \\mathbf{e}_j$, where $\\mathbf{e}_j$ is the $j$-th standard basis vector.\n$$\n\\mathbf{r}^{(j)} = R \\mathbf{e}_j\n$$\nThis is simply the $j$-th column of the resolution matrix $R$.\n\nAll three derived resolution matrices can be written in the general form $R = \\sum_{i=0}^{n-1} d_i \\mathbf{v}_i \\mathbf{v}_i^{\\mathsf{T}}$, where $d_i$ are the filter factors specific to each estimator.\n-   Undamped: $d_i = 1$ if $\\sigma_i  0$, and $0$ otherwise.\n-   TSVD: $d_i = 1$ for $i=0, \\dots, k-1$, and $0$ otherwise.\n-   Tikhonov: $d_i = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$.\n\nLet's derive the expression for $\\mathbf{r}^{(j)}$:\n$$\n\\mathbf{r}^{(j)} = \\left( \\sum_{i=0}^{n-1} d_i \\mathbf{v}_i \\mathbf{v}_i^{\\mathsf{T}} \\right) \\mathbf{e}_j = \\sum_{i=0}^{n-1} d_i \\mathbf{v}_i (\\mathbf{v}_i^{\\mathsf{T}} \\mathbf{e}_j)\n$$\nThe term $\\mathbf{v}_i^{\\mathsf{T}} \\mathbf{e}_j$ is a scalar representing the dot product of $\\mathbf{v}_i$ and $\\mathbf{e}_j$, which extracts the $j$-th component of the vector $\\mathbf{v}_i$. Let $V_{ji}$ denote the $j$-th component of the $i$-th right singular vector $\\mathbf{v}_i$. Then $\\mathbf{v}_i^{\\mathsf{T}} \\mathbf{e}_j = V_{ji}$.\nThe PSF is therefore a linear combination of the right singular vectors:\n$$\n\\mathbf{r}^{(j)} = \\sum_{i=0}^{n-1} (d_i V_{ji}) \\mathbf{v}_i\n$$\n**Qualitative Explanation:**\nThis expression reveals how the PSF is constructed. The right singular vectors $\\{\\mathbf{v}_i\\}$ form an orthonormal basis for the model space $\\mathbb{R}^{n_m}$. The PSF $\\mathbf{r}^{(j)}$ is a vector in this space, synthesized as a weighted sum of these basis vectors. The weight for each basis vector $\\mathbf{v}_i$ is $c_i = d_i V_{ji}$.\n\n-   The term $V_{ji}$ is the $j$-th component of $\\mathbf{v}_i$. It quantifies how much the $j$-th canonical model parameter direction $\\mathbf{e}_j$ is \"represented\" by the $i$-th singular vector $\\mathbf{v}_i$.\n-   The term $d_i$ is the filter factor determined by the chosen estimator. It acts as a gate or a damper, controlling the influence of each singular vector in the final solution. For undamped least squares, if $G$ is full rank, all $d_i=1$, $R=I$, and $\\mathbf{r}^{(j)} = \\sum_{i} V_{ji} \\mathbf{v}_i = (\\sum_i \\mathbf{v}_i \\mathbf{v}_i^{\\mathsf{T}}) \\mathbf{e}_j = V V^{\\mathsf{T}} \\mathbf{e}_j = I \\mathbf{e}_j = \\mathbf{e}_j$, which is a delta function representing perfect resolution.\n-   For regularized estimators like TSVD or Tikhonov, some or all $d_i$ are less than $1$. This suppression of singular vectors (especially those corresponding to small $\\sigma_i$) causes a loss of information, resulting in a \"smeared\" or \"spread-out\" PSF. The shape of this spread is dictated by the structure of the singular vectors $\\mathbf{v}_i$ that are allowed to contribute by the filter factors. The PSF is no longer a perfect delta-function $\\mathbf{e}_j$, and its deviation from $\\mathbf{e}_j$ is a direct measure of the loss of resolution at model parameter $j$.",
            "answer": "```python\nimport numpy as np\nimport json\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    n = 20\n    test_cases = [\n        {'estimator': 'undamped', 'param': None, 's': 1.25, 'j': 10},\n        {'estimator': 'tsvd', 'param': 8, 's': 1.25, 'j': 10},\n        {'estimator': 'tikhonov', 'param': 0.25, 's': 1.25, 'j': 3},\n        {'estimator': 'tikhonov', 'param': 1.00, 's': 1.25, 'j': 17},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_psf(\n            n=n,\n            s=case['s'],\n            estimator=case['estimator'],\n            param=case['param'],\n            j=case['j']\n        )\n        results.append(result)\n\n    # Use json.dumps for a robust, single-line, syntactically correct representation\n    # of the list of lists, as the format implies.\n    print(json.dumps(results))\n\n\ndef compute_psf(n, s, estimator, param, j):\n    \"\"\"\n    Computes the point-spread function and related metrics for a given case.\n\n    Args:\n        n (int): The size of the model and data spaces.\n        s (float): The width parameter for the Gaussian blur kernel.\n        estimator (str): The type of estimator ('undamped', 'tsvd', 'tikhonov').\n        param (float or int): The parameter for the estimator (k for TSVD, lambda for Tikhonov).\n        j (int): The index of the model parameter for which to compute the PSF.\n\n    Returns:\n        list: A list containing [r_j_vec, off_diag_mass, argmax_index].\n    \"\"\"\n    # Step 1: Construct the Gaussian blur operator G\n    p = np.arange(n).reshape(-1, 1)\n    q = np.arange(n).reshape(1, -1)\n    G = np.exp(-(p - q)**2 / (2 * s**2))\n\n    # Step 2: Normalize the columns of G to unit 2-norm\n    column_norms = np.linalg.norm(G, axis=0)\n    # Avoid division by zero, though unlikely for this G\n    column_norms[column_norms == 0] = 1.0\n    G_norm = G / column_norms\n    \n    # Step 3: Compute the Singular Value Decomposition of the normalized G\n    U, s_vals, Vt = np.linalg.svd(G_norm)\n    V = Vt.T\n\n    # Step 4: Determine the filter factors 'd_i' based on the estimator\n    d = np.zeros(n)\n    if estimator == 'undamped':\n        # For a full-rank square matrix, the pseudoinverse is the inverse,\n        # R = G_inv * G = I. The filter factors are all 1.\n        d.fill(1.0)\n    elif estimator == 'tsvd':\n        k = param\n        d[:k] = 1.0\n    elif estimator == 'tikhonov':\n        lam = param\n        d = s_vals**2 / (s_vals**2 + lam**2)\n\n    # Step 5: Compute the point-spread function r^(j)\n    # r_j = sum_i (d_i * V_ji) * v_i\n    # V_ji is the j-th component of the i-th singular vector v_i.\n    # This corresponds to the element V[j, i].\n    # The coefficients for the linear combination of v_i are c_i = d_i * V[j,i].\n    # In vector form, c = d * V[j, :].\n    # The PSF vector is then r_j = V @ c.\n    V_j_row = V[j, :]\n    c = d * V_j_row\n    r_j_vec = V @ c\n\n    # Step 6: Compute the required output metrics\n    off_diag_mass = np.sum(np.abs(r_j_vec)) - np.abs(r_j_vec[j])\n    argmax_index = int(np.argmax(np.abs(r_j_vec)))\n\n    return [r_j_vec.tolist(), off_diag_mass, argmax_index]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}