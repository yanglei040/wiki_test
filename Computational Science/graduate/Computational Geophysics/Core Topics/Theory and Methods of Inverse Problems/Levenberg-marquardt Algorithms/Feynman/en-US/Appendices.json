{
    "hands_on_practices": [
        {
            "introduction": "Physical parameters in geophysics, like wave speed, are often strictly positive, a constraint that standard optimization must honor. This exercise demonstrates how to enforce such constraints using a logarithmic change of variables, requiring you to derive the Levenberg-Marquardt step in the new parameter space via the chain rule . This practice solidifies the core mechanics of the algorithm while tackling a common and important practical challenge.",
            "id": "3607403",
            "problem": "Consider a two-parameter linear forward model used in a simplified amplitude-versus-offset calibration in computational geophysics. The predicted data are modeled as $f(\\mathbf{m}) = \\mathbf{G}\\mathbf{m}$, where $\\mathbf{G} \\in \\mathbb{R}^{2 \\times 2}$ is known and $\\mathbf{m} \\in \\mathbb{R}^2$ represents strictly positive subsurface parameters. To enforce positivity, introduce the change of variables $\\mathbf{m} = \\exp(\\mathbf{p})$, interpreted elementwise. The data misfit is defined as the nonlinear least-squares objective $\\Phi(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{r}(\\mathbf{p})\\|_2^2$, where $\\mathbf{r}(\\mathbf{p}) = f(\\exp(\\mathbf{p})) - \\mathbf{d}_{\\text{obs}}$ and $\\mathbf{d}_{\\text{obs}} \\in \\mathbb{R}^2$ are fixed observations.\n\nStarting from the first-order linearization of the residual and a Tikhonov-regularized quadratic model in parameter space, derive the damped Gauss–Newton normal equations that define the Levenberg–Marquardt (LM) step in $\\mathbf{p}$-space, explicitly using the chain rule to express the Jacobian with respect to $\\mathbf{p}$. Then, for the specific numerical instance\n$$\n\\mathbf{G} = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}, \\quad \\mathbf{d}_{\\text{obs}} = \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix}, \\quad \\mathbf{p}_k = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1,\n$$\ncompute the LM step $\\delta \\mathbf{p}$ at $\\mathbf{p}_k$. To select a step length along the LM direction, consider the back-transformed update in $\\mathbf{m}$-space,\n$$\n\\mathbf{m}(\\alpha) = \\exp(\\mathbf{p}_k + \\alpha\\,\\delta \\mathbf{p}),\n$$\nwith candidate step lengths $\\alpha \\in \\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}\\}$. Enforce a trust-region type constraint directly in $\\mathbf{m}$-space by requiring the Euclidean change to satisfy\n$$\n\\|\\mathbf{m}(\\alpha) - \\mathbf{m}(\\mathbf{0})\\|_2 \\le \\Delta_m, \\quad \\text{with} \\quad \\Delta_m = 0.65,\n$$\nwhere $\\mathbf{m}(\\mathbf{0}) = \\exp(\\mathbf{p}_k)$. Among the candidates, choose the largest $\\alpha$ that satisfies the constraint. Provide the accepted step length $\\alpha$ as the final answer. Do not include any units and do not round unless necessary; if rounding is needed, round to four significant figures.",
            "solution": "The user wants to find the largest acceptable step length $\\alpha$ from a given set of candidates, subject to a trust-region-like constraint in the context of a Levenberg-Marquardt (LM) optimization.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n-   Forward model: $f(\\mathbf{m}) = \\mathbf{G}\\mathbf{m}$\n-   Parameter transformation: $\\mathbf{m} = \\exp(\\mathbf{p})$ (elementwise), where $\\mathbf{m} \\in \\mathbb{R}^2$ and $\\mathbf{p} \\in \\mathbb{R}^2$.\n-   Objective function: $\\Phi(\\mathbf{p}) = \\tfrac{1}{2}\\|\\mathbf{r}(\\mathbf{p})\\|_2^2$\n-   Residual vector: $\\mathbf{r}(\\mathbf{p}) = f(\\exp(\\mathbf{p})) - \\mathbf{d}_{\\text{obs}}$\n-   Numerical values:\n    -   $\\mathbf{G} = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}$\n    -   $\\mathbf{d}_{\\text{obs}} = \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix}$\n    -   Current iterate: $\\mathbf{p}_k = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n    -   Damping parameter: $\\lambda = 1$\n-   Line search parameters:\n    -   Update in $\\mathbf{m}$-space: $\\mathbf{m}(\\alpha) = \\exp(\\mathbf{p}_k + \\alpha\\,\\delta \\mathbf{p})$\n    -   Candidate step lengths: $\\alpha \\in \\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}\\}$\n    -   Constraint: $\\|\\mathbf{m}(\\alpha) - \\mathbf{m}(\\mathbf{0})\\|_2 \\le \\Delta_m$\n    -   Constraint radius: $\\Delta_m = 0.65$\n    -   Reference model: $\\mathbf{m}(\\mathbf{0}) = \\exp(\\mathbf{p}_k)$\n-   Task: Find the largest $\\alpha$ from the candidate set that satisfies the constraint.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Groundedness**: The problem is well-grounded in numerical optimization, specifically nonlinear least-squares using the Levenberg-Marquardt algorithm. The change of variables $\\mathbf{m}=\\exp(\\mathbf{p})$ is a standard technique to enforce positivity constraints. The context of computational geophysics is appropriate.\n-   **Well-Posedness**: The problem provides all necessary data and a clear, unambiguous objective. A unique solution for the largest acceptable $\\alpha$ from the given discrete set exists.\n-   **Objectivity**: The problem is stated in precise, objective mathematical language.\n-   **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, well-posed, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe Levenberg-Marquardt step $\\delta \\mathbf{p}$ at an iteration $k$ is found by solving the regularized linear system known as the damped Gauss-Newton normal equations:\n$$\n(\\mathbf{J}_k^T \\mathbf{J}_k + \\lambda \\mathbf{I}) \\delta \\mathbf{p} = -\\mathbf{J}_k^T \\mathbf{r}_k\n$$\nwhere $\\mathbf{J}_k$ is the Jacobian of the residual vector $\\mathbf{r}(\\mathbf{p})$ evaluated at $\\mathbf{p}_k$, and $\\mathbf{r}_k = \\mathbf{r}(\\mathbf{p}_k)$.\n\nFirst, we must derive the expression for the Jacobian $\\mathbf{J}(\\mathbf{p})$. The residual is $\\mathbf{r}(\\mathbf{p}) = \\mathbf{G}\\mathbf{m}(\\mathbf{p}) - \\mathbf{d}_{\\text{obs}}$, with $\\mathbf{m}(\\mathbf{p}) = \\exp(\\mathbf{p})$. We apply the chain rule:\n$$\n\\mathbf{J}(\\mathbf{p}) = \\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{p}} = \\frac{\\partial \\mathbf{r}}{\\partial \\mathbf{m}} \\frac{\\partial \\mathbf{m}}{\\partial \\mathbf{p}}\n$$\nThe first term is $\\frac{\\partial}{\\partial \\mathbf{m}}(\\mathbf{G}\\mathbf{m} - \\mathbf{d}_{\\text{obs}}) = \\mathbf{G}$.\nThe second term, for $\\mathbf{m} = (m_1, m_2)^T$ and $\\mathbf{p} = (p_1, p_2)^T$ where $m_i = \\exp(p_i)$, is a diagonal matrix:\n$$\n\\frac{\\partial \\mathbf{m}}{\\partial \\mathbf{p}} = \\begin{pmatrix} \\frac{\\partial m_1}{\\partial p_1}  \\frac{\\partial m_1}{\\partial p_2} \\\\ \\frac{\\partial m_2}{\\partial p_1}  \\frac{\\partial m_2}{\\partial p_2} \\end{pmatrix} = \\begin{pmatrix} \\exp(p_1)  0 \\\\ 0  \\exp(p_2) \\end{pmatrix} = \\text{diag}(\\exp(\\mathbf{p}))\n$$\nSo, the Jacobian is $\\mathbf{J}(\\mathbf{p}) = \\mathbf{G} \\, \\text{diag}(\\exp(\\mathbf{p}))$.\n\nNow, we evaluate the components of the LM system at the specific instance $\\mathbf{p}_k = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe model parameters in $\\mathbf{m}$-space are:\n$$\n\\mathbf{m}_k = \\exp(\\mathbf{p}_k) = \\exp\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\right) = \\begin{pmatrix} \\exp(0) \\\\ \\exp(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nThe Jacobian at $\\mathbf{p}_k$ is:\n$$\n\\mathbf{J}_k = \\mathbf{J}(\\mathbf{p}_k) = \\mathbf{G} \\, \\text{diag}(\\exp(\\mathbf{p}_k)) = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix}\n$$\nThe residual vector at $\\mathbf{p}_k$ is:\n$$\n\\mathbf{r}_k = \\mathbf{r}(\\mathbf{p}_k) = \\mathbf{G}\\mathbf{m}_k - \\mathbf{d}_{\\text{obs}} = \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 7 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix}\n$$\nNext, we construct the matrix $\\mathbf{J}_k^T \\mathbf{J}_k$:\n$$\n\\mathbf{J}_k^T \\mathbf{J}_k = \\begin{pmatrix} 1  3 \\\\ 2  4 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ 3  4 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 3 \\cdot 3  1 \\cdot 2 + 3 \\cdot 4 \\\\ 2 \\cdot 1 + 4 \\cdot 3  2 \\cdot 2 + 4 \\cdot 4 \\end{pmatrix} = \\begin{pmatrix} 10  14 \\\\ 14  20 \\end{pmatrix}\n$$\nThe left-hand side matrix of the LM system, with $\\lambda=1$, is:\n$$\n\\mathbf{A} = \\mathbf{J}_k^T \\mathbf{J}_k + \\lambda \\mathbf{I} = \\begin{pmatrix} 10  14 \\\\ 14  20 \\end{pmatrix} + 1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 11  14 \\\\ 14  21 \\end{pmatrix}\n$$\nThe right-hand side vector is:\n$$\n\\mathbf{b} = -\\mathbf{J}_k^T \\mathbf{r}_k = - \\begin{pmatrix} 1  3 \\\\ 2  4 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -3 \\end{pmatrix} = - \\begin{pmatrix} -10 \\\\ -14 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 14 \\end{pmatrix}\n$$\nWe solve the system $\\mathbf{A} \\delta \\mathbf{p} = \\mathbf{b}$ for the LM step $\\delta \\mathbf{p}$. The determinant of $\\mathbf{A}$ is $\\det(\\mathbf{A}) = (11)(21) - (14)(14) = 231 - 196 = 35$. The inverse is:\n$$\n\\mathbf{A}^{-1} = \\frac{1}{35} \\begin{pmatrix} 21  -14 \\\\ -14  11 \\end{pmatrix}\n$$\nThe LM step is:\n$$\n\\delta \\mathbf{p} = \\mathbf{A}^{-1} \\mathbf{b} = \\frac{1}{35} \\begin{pmatrix} 21  -14 \\\\ -14  11 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 14 \\end{pmatrix} = \\frac{1}{35} \\begin{pmatrix} 210 - 196 \\\\ -140 + 154 \\end{pmatrix} = \\frac{1}{35} \\begin{pmatrix} 14 \\\\ 14 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix}\n$$\nNow, we must find the largest step length $\\alpha \\in \\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}\\}$ that satisfies the constraint $\\|\\mathbf{m}(\\alpha) - \\mathbf{m}(\\mathbf{0})\\|_2 \\le 0.65$.\nFirst, $\\mathbf{m}(\\mathbf{0}) = \\exp(\\mathbf{p}_k) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe updated model for a given $\\alpha$ is:\n$$\n\\mathbf{m}(\\alpha) = \\exp(\\mathbf{p}_k + \\alpha\\,\\delta \\mathbf{p}) = \\exp\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 2/5 \\\\ 2/5 \\end{pmatrix}\\right) = \\exp\\left(\\begin{pmatrix} 0.4\\alpha \\\\ 0.4\\alpha \\end{pmatrix}\\right) = \\begin{pmatrix} \\exp(0.4\\alpha) \\\\ \\exp(0.4\\alpha) \\end{pmatrix}\n$$\nThe change in the model is $\\mathbf{m}(\\alpha) - \\mathbf{m}(\\mathbf{0}) = \\begin{pmatrix} \\exp(0.4\\alpha) - 1 \\\\ \\exp(0.4\\alpha) - 1 \\end{pmatrix}$.\nThe norm of this change is:\n$$\n\\|\\mathbf{m}(\\alpha) - \\mathbf{m}(\\mathbf{0})\\|_2 = \\sqrt{ (\\exp(0.4\\alpha) - 1)^2 + (\\exp(0.4\\alpha) - 1)^2 } = \\sqrt{2(\\exp(0.4\\alpha) - 1)^2} = \\sqrt{2}|\\exp(0.4\\alpha) - 1|\n$$\nSince $\\alpha > 0$, $\\exp(0.4\\alpha) > 1$, so the absolute value can be removed. The constraint becomes:\n$$\n\\sqrt{2}(\\exp(0.4\\alpha) - 1) \\le 0.65\n$$\nWe test the candidate values of $\\alpha$ in descending order.\n\nTest $\\alpha=1$:\n$$\n\\sqrt{2}(\\exp(0.4 \\cdot 1) - 1) = \\sqrt{2}(\\exp(0.4) - 1) \\approx 1.4142(1.4918 - 1) = 1.4142(0.4918) \\approx 0.6955\n$$\nSince $0.6955 > 0.65$, this step length is rejected.\n\nTest $\\alpha=\\frac{1}{2}$:\n$$\n\\sqrt{2}(\\exp(0.4 \\cdot \\tfrac{1}{2}) - 1) = \\sqrt{2}(\\exp(0.2) - 1) \\approx 1.4142(1.2214 - 1) = 1.4142(0.2214) \\approx 0.3131\n$$\nSince $0.3131 \\le 0.65$, this step length is accepted.\n\nAs we are seeking the largest candidate $\\alpha$ that satisfies the condition and we are testing in descending order, the first accepted value is the answer.\n\nThe largest accepted step length is $\\alpha = \\frac{1}{2}$.",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "An inverse problem's solution is more than just a single estimate; it's a model with inherent uncertainty and limited resolution. This practice delves into the crucial role of the LM damping parameter, not just as an algorithmic device, but as a regularizer that shapes the statistical properties of the result . By analyzing the model resolution matrix and posterior covariance, you will gain quantitative insight into the fundamental trade-off between solution variance and bias.",
            "id": "3607391",
            "problem": "Consider a linearized form of a travel-time tomography problem in computational geophysics with two model parameters $m_1$ and $m_2$ representing cell slownesses. At a given iteration, the sensitivity (Jacobian) matrix is\n$$\nJ \\;=\\; \\begin{bmatrix} 1.0  0.8 \\\\ 0.8  1.0 \\end{bmatrix},\n$$\nand assume a unit data precision weighting $W_d = I$ (that is, Gaussian data errors with identity covariance scaled to the units used). Impose a first-difference roughness penalty with $W_m = L$, where\n$$\nL \\;=\\; \\begin{bmatrix} 1  -1 \\end{bmatrix},\n$$\nand set the roughness regularization strength to $\\lambda = 0.2$. You will perform model resolution analysis using the inverse of $J^T W_d J + \\lambda W_m^T W_m$ and then assess how the Levenberg-Marquardt (LM) damping modifies resolution and uncertainty.\n\nUsing only first principles of linear inverse theory and the definition of the Levenberg-Marquardt algorithm, do the following:\n\n1. From the Gauss-Markov and Gaussian Bayesian linear inversion framework, construct the normal matrix $H = J^T W_d J + \\lambda W_m^T W_m$ and compute the model resolution matrix\n$$\nR \\;=\\; H^{-1} J^T W_d J,\n$$\nas well as the posterior covariance\n$$\nC_{\\mathrm{post}} \\;=\\; H^{-1}.\n$$\n\n2. Introduce the Levenberg-Marquardt damping parameter $\\mu$ as an isotropic trust-region augmentation of the normal matrix, giving $H_\\mu = H + \\mu I$, and recompute\n$$\nR_\\mu \\;=\\; H_\\mu^{-1} J^T W_d J, \\quad C_{\\mathrm{post},\\mu} \\;=\\; H_\\mu^{-1}\n$$\nfor $\\mu = 0.5$.\n\n3. Based on these computations and the underlying principles, decide which of the following statements are correct:\n\nA. For $\\mu = 0.5$, the diagonal elements of $R_\\mu$ decrease compared to $\\mu = 0$, and the posterior standard deviations $\\sqrt{\\operatorname{diag}(C_{\\mathrm{post},\\mu})}$ also decrease; this reflects that Levenberg-Marquardt damping reduces data influence and shrinks uncertainty if interpreted as an implicit isotropic prior precision.\n\nB. As $\\mu$ increases, $R_\\mu$ tends to the identity matrix $I$ and $C_{\\mathrm{post},\\mu}$ tends to $I$, indicating perfect resolution and unchanged uncertainty.\n\nC. If $\\mu$ is treated as a purely algorithmic parameter rather than a physically justified prior precision, reporting $C_{\\mathrm{post},\\mu} = (J^T W_d J + \\lambda W_m^T W_m + \\mu I)^{-1}$ can yield overconfident uncertainty estimates, because variance decreases even while bias increases.\n\nD. The ratio of off-diagonal to diagonal elements of the resolution matrix can increase with $\\mu$, indicating relatively greater cross-talk, even as the absolute magnitude of resolution decreases.\n\nE. The Levenberg-Marquardt damping $\\mu$ has no effect on resolution; it only affects step length and leaves $R$ invariant.\n\nSelect all correct options.",
            "solution": "The user has provided a well-posed problem in linear inverse theory, specifically concerning model resolution analysis in the context of Tikhonov regularization and the Levenberg-Marquardt algorithm. All provided data and definitions are scientifically sound and sufficient for a unique solution. I will proceed with the quantitative analysis.\n\nThe problem asks to analyze the effects of Levenberg-Marquardt (LM) damping on the model resolution and posterior covariance matrices. We are given the Jacobian matrix $J$, the data weighting matrix $W_d$, the model regularization matrix $W_m$, the regularization strength $\\lambda$, and the LM damping parameter $\\mu$.\n\n**Given:**\n- Jacobian matrix: $J = \\begin{bmatrix} 1.0  0.8 \\\\ 0.8  1.0 \\end{bmatrix}$\n- Data precision matrix: $W_d = I = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$\n- Model roughness matrix: $W_m = \\begin{bmatrix} 1  -1 \\end{bmatrix}$\n- Regularization strength: $\\lambda = 0.2$\n- LM damping parameter: $\\mu=0.5$\n\nThe analysis will be conducted in two parts: first for the standard Tikhonov-regularized case ($\\mu=0$), and second for the LM-damped case ($\\mu=0.5$).\n\n### Part 1: Standard Regularization ($\\mu = 0$)\n\nFirst, we construct the components of the normal matrix $H = J^T W_d J + \\lambda W_m^T W_m$.\n\nThe data sensitivity part is $J^T W_d J = J^T I J = J^T J$.\n$$\nJ^T J = \\begin{bmatrix} 1.0  0.8 \\\\ 0.8  1.0 \\end{bmatrix} \\begin{bmatrix} 1.0  0.8 \\\\ 0.8  1.0 \\end{bmatrix} = \\begin{bmatrix} 1.0^2 + 0.8^2  1.0 \\cdot 0.8 + 0.8 \\cdot 1.0 \\\\ 0.8 \\cdot 1.0 + 1.0 \\cdot 0.8  0.8^2 + 1.0^2 \\end{bmatrix} = \\begin{bmatrix} 1.64  1.60 \\\\ 1.60  1.64 \\end{bmatrix}\n$$\nThe regularization part is $\\lambda W_m^T W_m$.\n$$\nW_m^T W_m = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\begin{bmatrix} 1  -1 \\end{bmatrix} = \\begin{bmatrix} 1  -1 \\\\ -1  1 \\end{bmatrix}\n$$\n$$\n\\lambda W_m^T W_m = 0.2 \\begin{bmatrix} 1  -1 \\\\ -1  1 \\end{bmatrix} = \\begin{bmatrix} 0.2  -0.2 \\\\ -0.2  0.2 \\end{bmatrix}\n$$\nThe normal matrix $H$ is the sum of these two matrices:\n$$\nH = \\begin{bmatrix} 1.64  1.60 \\\\ 1.60  1.64 \\end{bmatrix} + \\begin{bmatrix} 0.2  -0.2 \\\\ -0.2  0.2 \\end{bmatrix} = \\begin{bmatrix} 1.84  1.40 \\\\ 1.40  1.84 \\end{bmatrix}\n$$\nThe posterior covariance matrix is $C_{\\mathrm{post}} = H^{-1}$.\n$$\n\\det(H) = 1.84^2 - 1.40^2 = 3.3856 - 1.96 = 1.4256\n$$\n$$\nC_{\\mathrm{post}} = H^{-1} = \\frac{1}{1.4256} \\begin{bmatrix} 1.84  -1.40 \\\\ -1.40  1.84 \\end{bmatrix} \\approx \\begin{bmatrix} 1.2907  -0.9820 \\\\ -0.9820  1.2907 \\end{bmatrix}\n$$\nThe posterior standard deviations are the square roots of the diagonal elements of $C_{\\mathrm{post}}$.\n$$\n\\sigma_i = \\sqrt{\\operatorname{diag}(C_{\\mathrm{post}})_i} = \\sqrt{1.2907} \\approx 1.136\n$$\nThe model resolution matrix is $R = H^{-1} J^T W_d J = C_{\\mathrm{post}} (J^T J)$.\n$$\nR = \\frac{1}{1.4256} \\begin{bmatrix} 1.84  -1.40 \\\\ -1.40  1.84 \\end{bmatrix} \\begin{bmatrix} 1.64  1.60 \\\\ 1.60  1.64 \\end{bmatrix} = \\frac{1}{1.4256} \\begin{bmatrix} 0.7776  0.6480 \\\\ 0.6480  0.7776 \\end{bmatrix} \\approx \\begin{bmatrix} 0.5455  0.4545 \\\\ 0.4545  0.5455 \\end{bmatrix}\n$$\nUsing exact fractions for precision, one can find $R = \\begin{bmatrix} 6/11  5/11 \\\\ 5/11  6/11 \\end{bmatrix}$. The diagonal elements are $R_{ii} \\approx 0.5455$.\n\n### Part 2: Levenberg-Marquardt Damped Case ($\\mu = 0.5$)\n\nThe LM algorithm augments the normal matrix with a damping term $\\mu I$. The damped normal matrix is $H_\\mu = H + \\mu I$. With $\\mu = 0.5$:\n$$\nH_\\mu = \\begin{bmatrix} 1.84  1.40 \\\\ 1.40  1.84 \\end{bmatrix} + 0.5 \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 2.34  1.40 \\\\ 1.40  2.34 \\end{bmatrix}\n$$\nThe corresponding posterior covariance is $C_{\\mathrm{post},\\mu} = H_\\mu^{-1}$.\n$$\n\\det(H_\\mu) = 2.34^2 - 1.40^2 = 5.4756 - 1.96 = 3.5156\n$$\n$$\nC_{\\mathrm{post},\\mu} = H_\\mu^{-1} = \\frac{1}{3.5156} \\begin{bmatrix} 2.34  -1.40 \\\\ -1.40  2.34 \\end{bmatrix} \\approx \\begin{bmatrix} 0.6657  -0.3982 \\\\ -0.3982  0.6657 \\end{bmatrix}\n$$\nThe new posterior standard deviations are:\n$$\n\\sigma_{i,\\mu} = \\sqrt{\\operatorname{diag}(C_{\\mathrm{post},\\mu})_i} = \\sqrt{0.6657} \\approx 0.816\n$$\nThe damped resolution matrix is $R_\\mu = H_\\mu^{-1} J^T W_d J = C_{\\mathrm{post},\\mu} (J^T J)$.\n$$\nR_\\mu = \\frac{1}{3.5156} \\begin{bmatrix} 2.34  -1.40 \\\\ -1.40  2.34 \\end{bmatrix} \\begin{bmatrix} 1.64  1.60 \\\\ 1.60  1.64 \\end{bmatrix} = \\frac{1}{3.5156} \\begin{bmatrix} 1.5976  1.4480 \\\\ 1.4480  1.5976 \\end{bmatrix} \\approx \\begin{bmatrix} 0.4544  0.4119 \\\\ 0.4119  0.4544 \\end{bmatrix}\n$$\nThe diagonal elements are $R_{\\mu,ii} \\approx 0.4544$.\n\n### Part 3: Option-by-Option Analysis\n\n**A. For $\\mu = 0.5$, the diagonal elements of $R_\\mu$ decrease compared to $\\mu = 0$, and the posterior standard deviations $\\sqrt{\\operatorname{diag}(C_{\\mathrm{post},\\mu})}$ also decrease; this reflects that Levenberg-Marquardt damping reduces data influence and shrinks uncertainty if interpreted as an implicit isotropic prior precision.**\n-   **Resolution diagonals**: We calculated that the diagonal of $R$ decreased from $\\approx 0.5455$ to $\\approx 0.4544$. This is a decrease.\n-   **Posterior standard deviations**: We calculated that the standard deviations decreased from $\\approx 1.136$ to $\\approx 0.816$. This is a decrease.\n-   **Interpretation**: Adding the term $\\mu I$ to the Hessian is equivalent to adding a zero-mean Gaussian prior with precision $\\mu I$. This extra information constrains the model parameters, shrinking their posterior variance. Simultaneously, the resolution matrix $R_\\mu = (H + \\mu I)^{-1} J^T J$ approaches the zero matrix as $\\mu \\to \\infty$, meaning the data's influence on the solution is diminished. The statement is consistent with both the calculations and the principles of Bayesian inference.\n-    verdict: **Correct**.\n\n**B. As $\\mu$ increases, $R_\\mu$ tends to the identity matrix $I$ and $C_{\\mathrm{post},\\mu}$ tends to $I$, indicating perfect resolution and unchanged uncertainty.**\n-   As $\\mu \\to \\infty$, $H_\\mu = H + \\mu I \\approx \\mu I$.\n-   Then $R_\\mu = (H + \\mu I)^{-1} J^T J \\approx (\\frac{1}{\\mu}I) J^T J \\to 0$ (the zero matrix), not the identity matrix $I$. This indicates a complete loss of resolution from the data.\n-   And $C_{\\mathrm{post},\\mu} = (H + \\mu I)^{-1} \\approx \\frac{1}{\\mu}I \\to 0$ (the zero matrix), not the identity matrix $I$. This indicates a vanishing of posterior variance.\n-   The statement makes claims opposite to the correct limiting behavior.\n-   verdict: **Incorrect**.\n\n**C. If $\\mu$ is treated as a purely algorithmic parameter rather than a physically justified prior precision, reporting $C_{\\mathrm{post},\\mu} = (J^T W_d J + \\lambda W_m^T W_m + \\mu I)^{-1}$ can yield overconfident uncertainty estimates, because variance decreases even while bias increases.**\n-   This statement addresses a critical interpretational issue. The LM parameter $\\mu$ is primarily an algorithmic tool to stabilize inversion, not necessarily a representation of prior physical knowledge.\n-   Our calculations show that the variance (diagonal of $C_{\\mathrm{post},\\mu}$) decreases as $\\mu$ increases ($0.6657  1.2907$).\n-   The bias of an estimator $\\hat{m}$ is related to the deviation of the resolution matrix $R$ from identity, i.e., bias $\\propto (R-I)m^{\\text{true}}$. As $\\mu$ increases, $R_\\mu$ moves further from $I$ (its elements get smaller), so the bias introduced by the regularization increases. This increased bias is the price paid for the reduction in variance.\n-   Reporting $C_{\\mathrm{post},\\mu}$ as the model covariance without accounting for the increased bias is misleading. It reports a smaller uncertainty (variance) for an estimate that is systematically further from the true solution. This is a classic example of overconfidence in uncertainty estimation.\n-   verdict: **Correct**.\n\n**D. The ratio of off-diagonal to diagonal elements of the resolution matrix can increase with $\\mu$, indicating relatively greater cross-talk, even as the absolute magnitude of resolution decreases.**\n-   For $\\mu=0$, the ratio is $|R_{12}/R_{11}| = (5/11)/(6/11) = 5/6 \\approx 0.8333$.\n-   For $\\mu=0.5$, the ratio is $|R_{\\mu,12}/R_{\\mu,11}| \\approx 0.4119 / 0.4544 \\approx 0.9065$.\n-   The calculation confirms that the ratio increased. An increase in this ratio means that the smearing between parameters $m_1$ and $m_2$ becomes more significant relative to the direct resolution of each parameter.\n-   Meanwhile, the absolute resolution, as measured by the diagonal elements $R_{ii}$, has decreased (from $0.5455$ to $0.4544$).\n-   The statement accurately describes a possible, and in this case actual, consequence of applying isotropic damping to an anisotropic problem.\n-   verdict: **Correct**.\n\n**E. The Levenberg-Marquardt damping $\\mu$ has no effect on resolution; it only affects step length and leaves $R$ invariant.**\n-   The resolution matrix is $R_\\mu = (J^T W_d J + \\lambda W_m^T W_m + \\mu I)^{-1} J^T W_d J$.\n-   The presence of $\\mu$ in the formula directly shows that $R_\\mu$ is a function of $\\mu$. Our calculations explicitly showed that $R_{0.5} \\neq R_0$.\n-   While damping does control the step length, it does so by modifying the effective Hessian, which in turn fundamentally alters the properties of the solution update, including its resolution. The statement is therefore false.\n-   verdict: **Incorrect**.\n\nThe correct statements are A, C, and D.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "Realistic geophysical inversions can involve millions of model parameters, making the explicit formation of the Jacobian and Gauss-Newton Hessian matrices computationally impossible. This exercise explores the advanced, matrix-free techniques that make large-scale LM optimization feasible . You will analyze the computational cost and algorithmic structure of an implementation that relies on the adjoint-state method to compute the required matrix-vector products on-the-fly, a cornerstone of modern computational geophysics.",
            "id": "3607315",
            "problem": "Consider a frequency-domain three-dimensional electromagnetic inversion in which the discretized state for source index $s \\in \\{1,\\dots,n_s\\}$ is given by the linear system $A(\\mathbf{m})\\,\\mathbf{x}_s=\\mathbf{b}_s$, where $A(\\mathbf{m}) \\in \\mathbb{C}^{n_\\phi \\times n_\\phi}$ is the curl-curl system matrix depending on the model parameters $\\mathbf{m} \\in \\mathbb{R}^{n_m}$, $\\mathbf{x}_s \\in \\mathbb{C}^{n_\\phi}$ is the state (electric field) for source $s$, and $\\mathbf{b}_s$ encodes the source. The predicted data for source $s$ are $\\mathbf{d}_s(\\mathbf{m})=P\\,\\mathbf{x}_s \\in \\mathbb{C}^{n_r}$ with sampling operator $P \\in \\mathbb{R}^{n_r \\times n_\\phi}$. The full data vector stacks $\\mathbf{d}(\\mathbf{m})=\\left[\\mathbf{d}_1(\\mathbf{m});\\dots;\\mathbf{d}_{n_s}(\\mathbf{m})\\right] \\in \\mathbb{C}^{n_d}$ with $n_d=n_s n_r$. Let $J(\\mathbf{m}) \\in \\mathbb{C}^{n_d \\times n_m}$ denote the Jacobian (sensitivity) of $\\mathbf{d}(\\mathbf{m})$ with respect to $\\mathbf{m}$, and consider a Levenberg-Marquardt (LM) step defined by solving the linearized system\n$$(J^\\top J + \\lambda I)\\,\\delta \\mathbf{m} = -J^\\top \\mathbf{r},$$\nwhere $\\lambda0$ is the LM damping parameter, $I$ is the identity, and $\\mathbf{r}=\\mathbf{d}(\\mathbf{m})-\\mathbf{d}^{\\text{obs}}$ is the residual.\n\nUsing only fundamental definitions of the Jacobian $J$, the structure of the Gram matrix $J^\\top J$, and the adjoint-state principle for linear partial differential equation (PDE) constraints, answer the following. You should:\n- Estimate the arithmetic cost and memory footprint required to explicitly form the matrix $J^\\top J$ in terms of $n_m$ and $n_d$ if $J$ is treated as dense, ignoring the cost of computing $J$ itself from the PDE. Then, discuss how the cost is impacted if one attempts to compute and store $J$ explicitly for a three-dimensional electromagnetic problem.\n- Propose a scientifically sound matrix-free LM implementation that never forms $J$ or $J^\\top J$, but instead applies the operator $\\mathbf{v}\\mapsto (J^\\top J + \\lambda I)\\mathbf{v}$ through on-the-fly products $\\mathbf{w}=J\\,\\mathbf{v}$ and $\\mathbf{g}=J^\\top \\mathbf{u}$. Specify, in terms of solves with $A(\\mathbf{m})$ and its conjugate transpose $A(\\mathbf{m})^\\ast$, the steps to compute $J\\,\\mathbf{v}$ and $J^\\top \\mathbf{u}$, and characterize the dominant per-iteration cost of a Krylov solver for the LM step in units of forward/adjoint linear solves with $A$ and $A^\\ast$.\n\nSelect the option that provides both a correct complexity estimate and a correct matrix-free LM procedure with justified per-iteration costs.\n\nA. Forming $J^\\top J$ explicitly from a dense $J \\in \\mathbb{C}^{n_d \\times n_m}$ costs $O(n_m^2 n_d)$ floating-point operations and requires $O(n_m n_d)$ memory to store $J$; storing $J^\\top J$ itself adds $O(n_m^2)$ memory. In a three-dimensional electromagnetic setting, computing and storing $J$ explicitly is prohibitive because it entails either $O(n_m)$ tangent-linear solves per source (finite-difference or direct columnwise sensitivities) and $O(n_m n_d)$ storage. A matrix-free LM uses a Krylov method on $(J^\\top J + \\lambda I)$, where each application computes $(J^\\top J + \\lambda I)\\mathbf{v} = J^\\top(J\\mathbf{v}) + \\lambda \\mathbf{v}$. To compute $J\\mathbf{v}$, for each source $s$ solve the tangent-linear system $A(\\mathbf{m})\\,\\delta \\mathbf{x}_s = -\\left(\\sum_{j=1}^{n_m} v_j \\,\\partial A/\\partial m_j\\right)\\mathbf{x}_s$ and then set the data perturbation $\\delta \\mathbf{d}_s = P\\,\\delta \\mathbf{x}_s$. To compute $J^\\top \\mathbf{u}$ for $\\mathbf{u}=[\\mathbf{u}_1;\\dots;\\mathbf{u}_{n_s}]$, solve for each source $s$ the adjoint system $A(\\mathbf{m})^\\ast \\mathbf{y}_s = P^\\top \\mathbf{u}_s$ and then accumulate components via the adjoint-state identity $[J^\\top \\mathbf{u}]_j = -\\sum_{s=1}^{n_s} \\Re\\{\\mathbf{x}_s^\\ast\\,(\\partial A/\\partial m_j)\\,\\mathbf{y}_s\\}$. One Krylov iteration applies $J$ once and $J^\\top$ once, costing $O(n_s)$ solves with $A$ and $O(n_s)$ solves with $A^\\ast$ (plus the negligible cost of applying $P$ and parameter-derivative contractions), and requires no storage of $J$ or $J^\\top J$.\n\nB. Forming $J^\\top J$ explicitly costs $O(n_m n_d)$ floating-point operations and $O(n_m^2)$ memory, because one can multiply $J^\\top$ and $J$ in linear time by sharing computations across columns. In three-dimensional electromagnetics, storing $J$ is practical if $n_m \\ll n_d$. A matrix-free LM can evaluate $J\\mathbf{v}$ by a single forward solve with $A(\\mathbf{m})$ independent of the number of sources, while $J^\\top \\mathbf{u}$ is obtained by transposing the same forward solve; thus each Krylov iteration costs $O(1)$ solves.\n\nC. Forming $J^\\top J$ can be avoided by estimating it with $k$ random probes using Hutchinson’s method at a cost $O(k n_d)$ and $O(n_m)$ memory, which is asymptotically exact as $k \\to \\infty$. The LM step can then be solved by replacing $(J^\\top J + \\lambda I)$ with a diagonal consisting of the randomized estimates, so no forward or adjoint solves are needed beyond computing the $k$ probes.\n\nD. The best way to form $J^\\top J$ is to first Cholesky factor $J$ at a cost $O(n_m^3)$ independent of $n_d$ and then square the factor to get $J^\\top J$ in $O(n_m^2)$ time. A matrix-free LM should then directly factor $(J^\\top J + \\lambda I)$ once per iteration using a sparse Cholesky decomposition of cost $O(n_m^3)$, avoiding iterative methods and adjoint solves entirely.",
            "solution": "The problem asks for an analysis of the computational cost of forming the Gauss-Newton Hessian approximation explicitly versus using a matrix-free approach for a Levenberg-Marquardt (LM) step in the context of 3D frequency-domain electromagnetic inversion.\n\n### Problem Validation\n\nFirst, a critical validation of the problem statement is required.\n\n-   **Extraction of Givens**:\n    -   State equation: $A(\\mathbf{m})\\,\\mathbf{x}_s=\\mathbf{b}_s$ for $n_s$ sources.\n    -   Model parameters: $\\mathbf{m} \\in \\mathbb{R}^{n_m}$.\n    -   State (electric field): $\\mathbf{x}_s \\in \\mathbb{C}^{n_\\phi}$.\n    -   System matrix: $A(\\mathbf{m}) \\in \\mathbb{C}^{n_\\phi \\times n_\\phi}$.\n    -   Predicted data: $\\mathbf{d}_s(\\mathbf{m})=P\\,\\mathbf{x}_s \\in \\mathbb{C}^{n_r}$.\n    -   Full data vector: $\\mathbf{d}(\\mathbf{m}) \\in \\mathbb{C}^{n_d}$ where $n_d = n_s n_r$.\n    -   Jacobian: $J(\\mathbf{m}) = \\frac{\\partial \\mathbf{d}}{\\partial \\mathbf{m}} \\in \\mathbb{C}^{n_d \\times n_m}$.\n    -   LM system: $(J^\\top J + \\lambda I)\\,\\delta \\mathbf{m} = -J^\\top \\mathbf{r}$, with $\\mathbf{r} = \\mathbf{d}(\\mathbf{m}) - \\mathbf{d}^{\\text{obs}}$.\n\n-   **Validation and Verdict**:\n    -   The problem is scientifically grounded in the established field of geophysical inversion and PDE-constrained optimization. The use of finite-dimensional discretizations of Maxwell's equations, adjoint-state methods, and the Levenberg-Marquardt algorithm are standard.\n    -   The problem is well-posed; it asks for a standard analysis of computational complexity and algorithmic design.\n    -   The problem is objective and uses precise technical language.\n    -   There is a minor notational ambiguity: the use of $J^\\top$ (transpose) instead of $J^H$ or $J^\\ast$ (conjugate transpose) for a complex Jacobian $J$. The standard Gauss-Newton approximation to the Hessian for a complex-valued residual function with real parameters is $\\Re(J^H J)$, and the gradient is $\\Re(J^H \\mathbf{r})$. The LM system for a real parameter update $\\delta \\mathbf{m}$ is typically written as $(\\Re(J^H J) + \\lambda I)\\delta\\mathbf{m} = -\\Re(J^H \\mathbf{r})$. This notational inaccuracy is a common shorthand in the literature. The structure of the question and the details provided in the options clarify that the standard adjoint-state methods are intended, which correspond to actions of $J$ and $J^H$. Therefore, we interpret $J^\\top$ as a shorthand for the operations that ultimately produce the standard real-valued LM system. The problem is deemed valid under this reasonable interpretation.\n\n### Derivation of Solution\n\n**Part 1: Cost of Explicitly Forming the Gauss-Newton Hessian**\n\nLet us analyze the cost of forming the matrix $H_{GN} = J^H J \\in \\mathbb{C}^{n_m \\times n_m}$.\n-   **Memory to store $J$**: The Jacobian $J$ is a dense matrix of size $n_d \\times n_m$. Storing it requires memory proportional to the number of its elements, which is $O(n_d n_m)$. Since the entries are complex, this corresponds to $2 n_d n_m$ floating-point numbers.\n-   **Arithmetic cost to form $J^H J$**: The product of an $n_m \\times n_d$ matrix ($J^H$) and an $n_d \\times n_m$ matrix ($J$) results in an $n_m \\times n_m$ matrix. The computation of each of the $n_m^2$ entries involves an inner product of two vectors of length $n_d$. Each inner product costs $O(n_d)$ floating-point operations (flops). Thus, the total arithmetic cost is $O(n_m^2 n_d)$ flops.\n-   **Memory to store $J^H J$**: The resulting matrix is of size $n_m \\times n_m$, requiring $O(n_m^2)$ memory.\n-   **Impact in 3D EM**: To compute the full Jacobian $J$ explicitly, one would need to determine its $n_m$ columns. The $j$-th column, $\\partial \\mathbf{d} / \\partial m_j$, can be computed using a tangent-linear model. This requires solving a linear system with the matrix $A(\\mathbf{m})$ for each of the $n_s$ sources to find the perturbed fields, and doing this for each of the $n_m$ model parameters. The total number of computationally expensive linear solves would be $O(n_m n_s)$. For large-scale problems where $n_m$ (e.g., $10^5$) and $n_s$ (e.g., $10^2$) are large, this is computationally prohibitive. The memory to store $J$, $O(n_d n_m) = O(n_s n_r n_m)$, also becomes prohibitive.\n\n**Part 2: Matrix-Free Levenberg-Marquardt Implementation**\n\nA matrix-free approach avoids forming $J$ and $J^H J$. The LM system is solved with an iterative method, such as the Conjugate Gradient (CG) algorithm, which only requires the ability to compute matrix-vector products of the form $\\mathbf{v} \\mapsto (\\Re(J^H J) + \\lambda I)\\mathbf{v}$. This is decomposed into $\\mathbf{v} \\mapsto \\Re(J^H(J\\mathbf{v})) + \\lambda\\mathbf{v}$. We need efficient procedures for the actions of $J$ and $J^H$.\n\n-   **Action of $J$: computing $\\mathbf{u} = J\\mathbf{v}$ for $\\mathbf{v} \\in \\mathbb{R}^{n_m}$**\n    This operation corresponds to finding the Gateaux derivative of the data vector $\\mathbf{d}$ in the direction of the model perturbation $\\mathbf{v}$. For each source $s$, we must find the data perturbation $\\mathbf{u}_s = P\\,\\delta\\mathbf{x}_s$. The field perturbation $\\delta\\mathbf{x}_s$ is found by differentiating the state equation $A(\\mathbf{m})\\mathbf{x}_s = \\mathbf{b}_s$:\n    $$ \\frac{\\partial A}{\\partial \\mathbf{m}}[\\mathbf{v}]\\,\\mathbf{x}_s + A(\\mathbf{m})\\,\\delta\\mathbf{x}_s = 0 \\implies A(\\mathbf{m})\\,\\delta\\mathbf{x}_s = - \\left(\\sum_{j=1}^{n_m} v_j \\frac{\\partial A}{\\partial m_j}\\right)\\mathbf{x}_s $$\n    This is the **tangent-linear equation**. The procedure requires one solve with the matrix $A(\\mathbf{m})$ for each of the $n_s$ sources. The total cost is dominated by $n_s$ forward solves.\n\n-   **Action of $J^H$: computing $\\mathbf{g} = J^H\\mathbf{u}$ for $\\mathbf{u} \\in \\mathbb{C}^{n_d}$**\n    This is the adjoint operation. Using the adjoint-state method, the $j$-th component of the vector $J^H\\mathbf{u}$ can be computed as:\n    $$ (J^H\\mathbf{u})_j = \\sum_{s=1}^{n_s} - \\mathbf{x}_s^H \\left(\\frac{\\partial A}{\\partial m_j}\\right)^H \\mathbf{y}_s $$\n    where the adjoint field $\\mathbf{y}_s$ for each source is obtained by solving the **adjoint equation**:\n    $$ A(\\mathbf{m})^H \\mathbf{y}_s = P^T \\mathbf{u}_s $$\n    Here $A(\\mathbf{m})^H$ is the conjugate transpose of $A(\\mathbf{m})$, denoted in the problem as $A(\\mathbf{m})^\\ast$. The procedure requires solving this system for each of the $n_s$ sources. The total cost is dominated by $n_s$ adjuvant solves. After obtaining all $\\mathbf{y}_s$, the components of $\\mathbf{g}$ are computed via summations involving inner products, which are computationally less expensive than the solves.\n\n-   **Per-iteration Cost of Krylov Solver**:\n    Each iteration of CG on the LM system requires one application of the operator $\\mathbf{v} \\mapsto \\Re(J^H(J\\mathbf{v})) + \\lambda\\mathbf{v}$. This involves one $J$-vector product and one $J^H$-vector product. Therefore, the dominant cost per Krylov iteration is $n_s$ solves with $A(\\mathbf{m})$ and $n_s$ solves with $A(\\mathbf{m})^\\ast$. This matrix-free approach avoids storing $J$ and $J^H J$.\n\n### Evaluation of Options\n\n-   **A.** This option correctly states the $O(n_m^2 n_d)$ cost and $O(n_m n_d)$ memory for the explicit approach, and correctly identifies this as prohibitive. It accurately describes the matrix-free implementation. The formula for the Jacobian-vector product $J\\mathbf{v}$ via the tangent-linear equation is correct. The formula for the adjoint-Jacobian-vector product $J^H\\mathbf{u}$ using the adjoint system is also correct. As worked out in the scratchpad, the expression given for $[J^\\top \\mathbf{u}]_j$ as $-\\sum \\Re\\{\\mathbf{x}_s^\\ast (\\partial A/\\partial m_j) \\mathbf{y}_s\\}$ correctly computes the components of the real-valued vector $-\\Re(J^H \\mathbf{r})$ needed for the right-hand side of the LM system, resolving the notational ambiguity appropriately for this problem class. The final cost characterization of one Krylov iteration is $O(n_s)$ forward and $O(n_s)$ adjoint solves, which is also correct. This option is internally consistent and factually correct.\n\n-   **B.** This option provides an incorrect arithmetic cost of $O(n_m n_d)$ for forming $J^H J$. It incorrectly claims that a Jacobian-vector product $J\\mathbf{v}$ can be computed with a single solve independent of the number of sources ($n_s$). The cost is proportional to $n_s$. Thus, this option is incorrect.\n\n-   **C.** This option proposes using Hutchinson's method to estimate $J^H J$. Hutchinson's method is for estimating the trace, not the full matrix. This is fundamentally wrong. C is incorrect.\n\n-   **D.** This option makes several fundamental errors. It suggests a Cholesky factorization of the rectangular matrix $J$, which is not defined. It confuses matrix-free iterative methods with direct factorization methods. To perform a Cholesky factorization of $(J^H J + \\lambda I)$, one must first form the matrix $J^H J$, which is what matrix-free methods avoid. The claim that this avoids adjoint solves is also false, as forming $J^H J$ efficiently requires $O(n_m n_s)$ adjoint solves. This option is incorrect.\n\nBased on this detailed analysis, Option A is the only one that presents a correct and consistent account of the computational complexities and algorithmic procedures.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}