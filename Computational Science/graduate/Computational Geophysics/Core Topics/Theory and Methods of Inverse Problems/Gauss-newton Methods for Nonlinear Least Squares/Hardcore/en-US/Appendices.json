{
    "hands_on_practices": [
        {
            "introduction": "To effectively apply the Gauss-Newton method, we must first master its fundamental component: the Jacobian matrix. This exercise will guide you through deriving the Jacobian for a simple, nonlinear geophysical model from first principles. By interpreting each entry, you will gain a deeper intuition for how the Jacobian quantifies the sensitivity of your data to changes in model parameters, which is essential for diagnosing the stability of the inversion .",
            "id": "3599237",
            "problem": "Consider a two-parameter geophysical model $m=\\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$, where $m_1$ represents a scalar property controlling a band-limited amplitude response and $m_2$ represents a coupling parameter associated with anisotropy. The forward map $F:\\mathbb{R}^2\\to\\mathbb{R}^2$ is defined by $F(m)=\\begin{pmatrix} \\sin(\\alpha m_1) \\\\ m_1 m_2 \\end{pmatrix}$, where $\\alpha0$ is a known scaling constant with units chosen so that the argument of the sine function is dimensionless. The data weighting matrix is the identity $W_d=I$. The nonlinear least-squares misfit is defined as $\\phi(m)=\\tfrac{1}{2}\\|F(m)-d\\|_2^2$, where $d\\in\\mathbb{R}^2$ are the observed data.\n\nStarting from the core definitions of the Jacobian matrix of $F(m)$ and the Gauss-Newton linearization of the residual, derive the Jacobian $J(m)$ explicitly in terms of $m_1$, $m_2$, and $\\alpha$. Then, explain how each entry of $J(m)$ reflects the physical sensitivity of the data to the parameters, and interpret how the magnitude and sign of the entries influence the conditioning of the Gauss-Newton normal matrix. Your derivation should proceed from first principles without invoking pre-stated formulas for the Jacobian of $F(m)$ or the Gauss-Newton method.\n\nProvide as your final answer the explicit closed-form analytical expression for $J(m)$. No numerical rounding is required. If you introduce any angles, they must be in radians. Since the final answer is symbolic, do not include units in the final expression.",
            "solution": "The problem requires the derivation of the Jacobian matrix $J(m)$ for a given forward model $F(m)$, and an interpretation of its entries and their impact on the Gauss-Newton method.\n\nThe forward model is a function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ that maps the model-parameter vector $m=\\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$ to a data vector. The components of the forward map are given as:\n$$\nF(m) = \\begin{pmatrix} F_1(m_1, m_2) \\\\ F_2(m_1, m_2) \\end{pmatrix} = \\begin{pmatrix} \\sin(\\alpha m_1) \\\\ m_1 m_2 \\end{pmatrix}\n$$\nwhere $\\alpha  0$ is a known constant.\n\nThe Jacobian matrix $J(m)$ of the vector-valued function $F(m)$ is, by definition, the matrix of all first-order partial derivatives. For a function mapping from $\\mathbb{R}^2$ to $\\mathbb{R}^2$, it is a $2 \\times 2$ matrix:\n$$\nJ(m) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial m_1}  \\frac{\\partial F_1}{\\partial m_2} \\\\ \\frac{\\partial F_2}{\\partial m_1}  \\frac{\\partial F_2}{\\partial m_2} \\end{pmatrix}\n$$\n\nWe proceed to calculate each entry of $J(m)$ from first principles using the rules of partial differentiation.\n\nThe first entry, $J_{11}$, is the partial derivative of $F_1(m_1, m_2) = \\sin(\\alpha m_1)$ with respect to $m_1$. Applying the chain rule, we have:\n$$\nJ_{11} = \\frac{\\partial}{\\partial m_1} \\left( \\sin(\\alpha m_1) \\right) = \\cos(\\alpha m_1) \\cdot \\frac{\\partial}{\\partial m_1}(\\alpha m_1) = \\alpha \\cos(\\alpha m_1)\n$$\n\nThe second entry, $J_{12}$, is the partial derivative of $F_1(m_1, m_2) = \\sin(\\alpha m_1)$ with respect to $m_2$. Since $F_1$ does not depend on $m_2$, this derivative is zero:\n$$\nJ_{12} = \\frac{\\partial}{\\partial m_2} \\left( \\sin(\\alpha m_1) \\right) = 0\n$$\n\nThe third entry, $J_{21}$, is the partial derivative of $F_2(m_1, m_2) = m_1 m_2$ with respect to $m_1$:\n$$\nJ_{21} = \\frac{\\partial}{\\partial m_1} \\left( m_1 m_2 \\right) = m_2\n$$\n\nThe fourth entry, $J_{22}$, is the partial derivative of $F_2(m_1, m_2) = m_1 m_2$ with respect to $m_2$:\n$$\nJ_{22} = \\frac{\\partial}{\\partial m_2} \\left( m_1 m_2 \\right) = m_1\n$$\n\nAssembling these partial derivatives into the matrix gives the explicit form of the Jacobian $J(m)$:\n$$\nJ(m) = \\begin{pmatrix} \\alpha \\cos(\\alpha m_1)  0 \\\\ m_2  m_1 \\end{pmatrix}\n$$\n\nNext, we interpret the physical meaning of these entries and their influence on the conditioning of the Gauss-Newton normal matrix. Each entry $J_{ij} = \\frac{\\partial F_i}{\\partial m_j}$ represents the sensitivity of the $i$-th data component to an infinitesimal change in the $j$-th model parameter.\n- $J_{11} = \\alpha \\cos(\\alpha m_1)$: This term quantifies the sensitivity of the first datum, $F_1 = \\sin(\\alpha m_1)$, to changes in the parameter $m_1$. The sensitivity is oscillatory and is maximal in magnitude when $|\\cos(\\alpha m_1)| = 1$, which occurs when $\\alpha m_1$ is an integer multiple of $\\pi$. At these points, the function $F_1$ is passing through zero and is steepest. Conversely, the sensitivity is zero when $\\cos(\\alpha m_1) = 0$, which occurs when $\\alpha m_1 = (n + \\frac{1}{2})\\pi$ for any integer $n$. These points correspond to the peaks and troughs of the sine wave, where a small change in $m_1$ results in a negligible change in $F_1$.\n- $J_{12} = 0$: This indicates that the first datum, $F_1$, is completely insensitive to changes in the parameter $m_2$. The measurement of the band-limited amplitude response is, according to this model, entirely decoupled from the anisotropy parameter.\n- $J_{21} = m_2$: This is the sensitivity of the second datum, $F_2 = m_1 m_2$, to perturbations in $m_1$. The sensitivity is directly proportional to the value of the coupling parameter $m_2$. If $m_2$ is close to zero, the second datum becomes insensitive to $m_1$, implying a weak coupling.\n- $J_{22} = m_1$: This is the sensitivity of the second datum $F_2$ to changes in $m_2$. This sensitivity is directly proportional to $m_1$. If $m_1$ is near zero, the second datum is insensitive to the anisotropy parameter $m_2$.\n\nThe Gauss-Newton method approximates the Hessian of the misfit function $\\phi(m)$ with the matrix $H_{GN} = J(m)^T J(m)$ (since the data weighting matrix $W_d = I$). The conditioning of this normal matrix is crucial for the stability and convergence of the inversion. A poorly conditioned or singular $H_{GN}$ leads to unstable parameter updates.\nThe normal matrix is:\n$$\nH_{GN} = J^T J = \\begin{pmatrix} \\alpha \\cos(\\alpha m_1)  m_2 \\\\ 0  m_1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\cos(\\alpha m_1)  0 \\\\ m_2  m_1 \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 \\cos^2(\\alpha m_1) + m_2^2  m_1 m_2 \\\\ m_1 m_2  m_1^2 \\end{pmatrix}\n$$\nThe well-posedness of the local linear problem is determined by whether $H_{GN}$ is invertible. This is equivalent to the columns of $J(m)$ being linearly independent. The determinant of $H_{GN}$ provides a measure of this. Using the property $\\det(A^T A) = (\\det A)^2$ for a square matrix $A$, we can compute:\n$$\n\\det(J) = (\\alpha \\cos(\\alpha m_1))(m_1) - (0)(m_2) = \\alpha m_1 \\cos(\\alpha m_1)\n$$\nTherefore, the determinant of the normal matrix is:\n$$\n\\det(H_{GN}) = (\\det(J))^2 = \\alpha^2 m_1^2 \\cos^2(\\alpha m_1)\n$$\nThe normal matrix $H_{GN}$ becomes singular, and the inversion problem ill-conditioned, if $\\det(H_{GN}) = 0$. This occurs under two conditions:\n1. $m_1 = 0$: If the amplitude parameter $m_1$ is zero, the Jacobian becomes $J = \\begin{pmatrix} \\alpha  0 \\\\ m_2  0 \\end{pmatrix}$, whose columns are linearly dependent. Physically, if $m_1=0$, then $F(m) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ for any value of $m_2$. The data contain no information about $m_2$, making its recovery impossible.\n2. $\\cos(\\alpha m_1) = 0$: This happens when $\\alpha m_1_k = \\frac{\\pi}{2} + k\\pi$ for any integer $k$. In this case, the Jacobian is $J = \\begin{pmatrix} 0  0 \\\\ m_2  m_1 \\end{pmatrix}$. The first row is zero, making the matrix rank-deficient. Physically, this corresponds to the points of zero sensitivity of the first datum $d_1$ with respect to $m_1$, as discussed earlier. At these extremum points of $F_1$, a small perturbation in $m_1$ cannot be \"seen\" in the first datum, leading to a loss of information and an ill-conditioned system.\n\nThe magnitude of the entries also affects conditioning. If $m_1$ is very small, $\\det(H_{GN})$ becomes very small, leading to poor conditioning. Similarly, if the model is near a point where $\\cos(\\alpha m_1) \\approx 0$, the problem is nearly ill-conditioned. The signs of the Jacobian entries, while not directly affecting the conditioning of $J^T J$ (due to the squaring of terms), are critical for determining the direction of the parameter update step $\\delta m$ in the Gauss-Newton algorithm, as the update depends on the product $J^T (d - F(m))$.",
            "answer": "$$\n\\boxed{\nJ(m) = \\begin{pmatrix}\n\\alpha \\cos(\\alpha m_1)  0 \\\\\nm_2  m_1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The Gauss-Newton algorithm provides a powerful search direction, but is the full step always the best one? This practice connects the first Gauss-Newton update to the well-known Born approximation in scattering theory, illustrating the limits of this linear approximation in moderately nonlinear regimes. You will implement a line search to find an optimal step length along the Gauss-Newton direction, a vital technique for improving convergence speed and stability .",
            "id": "3384199",
            "problem": "Consider a discrete one-dimensional inverse scattering problem in which the forward map is modeled by the sum of single-scattering and a second-order interaction term. Let the unknown contrast vector be $m \\in \\mathbb{R}^n$, the single-scattering operator be a known matrix $H \\in \\mathbb{R}^{n \\times n}$, and the interaction kernel be a known matrix $K \\in \\mathbb{R}^{n \\times n}$. The forward operator is defined by\n$$\nF(m) \\;=\\; H m \\;+\\; m \\odot (K m),\n$$\nwhere $m \\odot (K m)$ denotes the elementwise product of $m$ with $K m$. Suppose synthetic observations $y \\in \\mathbb{R}^n$ are generated according to $y = F(m_{\\text{true}}) + \\eta$, where $m_{\\text{true}}$ is the ground-truth contrast and $\\eta$ is a known deterministic noise vector.\n\nThe goal is to compare the first step of the Gauss–Newton (GN) method for the Nonlinear Least Squares (NLS) objective to the Born approximation, to quantify the differences in moderate-contrast regimes, and to implement a mismatch-based step-length control along the first-step direction. Specifically:\n\n1. Define the Nonlinear Least Squares (NLS) objective\n$$\n\\Phi(m) \\;=\\; \\tfrac{1}{2}\\,\\|F(m) - y\\|_2^2.\n$$\n\n2. Consider the Gauss–Newton (GN) method linearizing $F(m)$ around an initial guess $m_0$. The first GN step from $m_0$ solves the Least Squares (LS) problem for the linearized residual. For the Born approximation, linearize the forward map around the zero-contrast reference and solve an LS problem against the data using the single-scattering operator.\n\n3. In this problem, you must construct $H$ and $K$ as follows for a discrete grid of size $n$:\n   - Grid size: $n = 20$.\n   - Single-scattering operator $H$ with entries\n     $$\n     H_{ij} \\;=\\; \\exp\\!\\Big(-\\frac{|i-j|}{\\ell}\\Big), \\quad \\text{with } \\ell = 3.\n     $$\n   - Interaction kernel $K$ with entries\n     $$\n     K_{ij} \\;=\\; \\beta\\,\\exp\\!\\Big(-\\frac{|i-j|}{s}\\Big), \\quad \\text{with } s = 2 \\text{ and } \\beta = \\frac{0.6}{n}.\n     $$\n\n4. Define the ground-truth contrast $m_{\\text{true}}$ as a Gaussian bump of amplitude $a$ centered at the middle index, i.e.,\n   $$\n   m_{\\text{true}}(i) \\;=\\; a\\,\\exp\\!\\Big(-\\frac{(i - c)^2}{2\\sigma^2}\\Big), \\quad \\text{for } i \\in \\{0,1,\\dots,n-1\\},\n   $$\n   with $c = \\frac{n-1}{2}$ and $\\sigma = 3$.\n\n5. Define the deterministic noise vector $\\eta \\in \\mathbb{R}^n$ componentwise by\n   $$\n   \\eta_i \\;=\\; \\varepsilon \\,\\sin\\!\\Big(\\frac{2\\pi i}{n}\\Big), \\quad \\text{for } i \\in \\{0,1,\\dots,n-1\\}.\n   $$\n\n6. For each test case below, form $y = F(m_{\\text{true}}) + \\eta$. Compute:\n   - The Born approximation $m_{\\text{Born}}$ by solving the LS problem $H m \\approx y$, i.e., find the minimizer of $\\|H m - y\\|_2^2$.\n   - The first GN step from $m_0 = 0$. Show by construction that the GN first-step direction coincides with the Born solution direction; however, evaluate the mismatch against the nonlinear forward map to reveal differences in moderate-contrast regimes.\n   - Implement a mismatch-based step-length control along the Born direction, i.e., choose a scalar $\\alpha \\ge 0$ that minimizes the squared nonlinear residual\n     $$\n     \\|F(\\alpha\\,m_{\\text{Born}}) - y\\|_2^2\n     $$\n     along the one-dimensional ray parameterized by $\\alpha$. You must determine $\\alpha$ algorithmically, without relying on derivative-free random searches. Your choice must be reproducible and deterministic.\n\n7. For each test case, report the following three quantities:\n   - The optimal step length $\\alpha$ chosen by your mismatch-based rule.\n   - The relative nonlinear mismatch of the unscaled Born step, defined as\n     $$\n     r_{\\text{Born}} \\;=\\; \\frac{\\|F(m_{\\text{Born}}) - y\\|_2}{\\|y\\|_2}.\n     $$\n   - The relative nonlinear mismatch of the $\\alpha$-scaled first step, defined as\n     $$\n     r_{\\alpha} \\;=\\; \\frac{\\|F(\\alpha\\, m_{\\text{Born}}) - y\\|_2}{\\|y\\|_2}.\n     $$\n\n8. Test suite and coverage:\n   - Case $1$ (low contrast, noiseless): $a = 0.1$, $\\varepsilon = 0.0$.\n   - Case $2$ (moderate contrast, noiseless): $a = 0.6$, $\\varepsilon = 0.0$.\n   - Case $3$ (moderate-high contrast, low noise): $a = 1.0$, $\\varepsilon = 0.02$.\n\n9. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Aggregate the results for the three test cases in order, each case contributing the three floating-point values $(\\alpha, r_{\\text{Born}}, r_{\\alpha})$. Thus the final output will contain $9$ values in the order corresponding to the cases $1$ through $3$:\n     $$\n     [\\alpha_1, r_{\\text{Born},1}, r_{\\alpha,1}, \\alpha_2, r_{\\text{Born},2}, r_{\\alpha,2}, \\alpha_3, r_{\\text{Born},3}, r_{\\alpha,3}].\n     $$\n   - All outputs are unitless real numbers. Express all ratios as decimal values.",
            "solution": "The problem requires an analysis and implementation of the Gauss-Newton method for a specific nonlinear least squares problem motivated by inverse scattering theory. The primary tasks are to relate the first Gauss-Newton step to the Born approximation and to implement a step-length control mechanism.\n\nFirst, we formalize the problem. The forward operator $F: \\mathbb{R}^n \\to \\mathbb{R}^n$ maps a contrast vector $m \\in \\mathbb{R}^n$ to a data vector via\n$$\nF(m) = H m + m \\odot (K m)\n$$\nwhere $H, K \\in \\mathbb{R}^{n \\times n}$ are known matrices, and $\\odot$ denotes the elementwise (Hadamard) product. Given noisy data $y = F(m_{\\text{true}}) + \\eta$, we seek to estimate $m$ by minimizing the nonlinear least squares objective:\n$$\n\\Phi(m) = \\frac{1}{2} \\| F(m) - y \\|_2^2\n$$\n\nThe Gauss-Newton (GN) method is an iterative algorithm for solving such problems. Starting from an initial guess $m_k$, the next iterate is $m_{k+1} = m_k + p_k$, where the update step $p_k$ is the solution to the linear least squares problem:\n$$\n\\min_{p} \\frac{1}{2} \\| J(m_k) p + r(m_k) \\|_2^2\n$$\nHere, $r(m_k) = F(m_k) - y$ is the residual vector and $J(m)$ is the Jacobian matrix of the forward operator $F(m)$. The step $p_k$ is found by solving the normal equations:\n$$\n(J(m_k)^T J(m_k)) p_k = -J(m_k)^T r(m_k)\n$$\n\nTo proceed, we must compute the Jacobian of $F(m)$. The $i$-th component of $F(m)$ is\n$$\nF_i(m) = \\sum_{j=0}^{n-1} H_{ij} m_j + m_i \\sum_{j=0}^{n-1} K_{ij} m_j\n$$\nThe partial derivative with respect to the $k$-th component of $m$, $m_k$, gives the $(i,k)$-th entry of the Jacobian matrix:\n$$\n[J(m)]_{ik} = \\frac{\\partial F_i}{\\partial m_k} = H_{ik} + \\delta_{ik} \\sum_{j=0}^{n-1} K_{ij} m_j + m_i K_{ik}\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta. In matrix notation, this is:\n$$\nJ(m) = H + \\operatorname{diag}(Km) + \\operatorname{diag}(m) K\n$$\nSince the kernel $K$ is defined symmetrically, we have $K=K^T$.\n\nThe problem specifies an initial guess $m_0 = 0$. At this point, the Jacobian simplifies significantly:\n$$\nJ(m_0=0) = H + \\operatorname{diag}(K \\cdot 0) + \\operatorname{diag}(0) K = H\n$$\nThe residual at $m_0$ is $r(m_0) = F(0) - y = 0 - y = -y$. The first GN step, denoted $p_0$, is found by solving the linear system arising from the linearized problem:\n$$\nJ(m_0) p_0 = -r(m_0) \\implies H p_0 = y\n$$\nThis requires solving the linear least squares problem $\\min_{p_0} \\|H p_0 - y\\|_2^2$.\n\nThe Born approximation, $m_{\\text{Born}}$, is defined as the solution to the same linear least squares problem, $\\min_m \\|H m - y\\|_2^2$. Therefore, by construction, the first GN search direction $p_0$ from the zero-contrast reference is identical to the Born approximation: $p_0 = m_{\\text{Born}}$. The first GN update would be $m_1 = m_0 + p_0 = 0 + m_{\\text{Born}} = m_{\\text{Born}}$.\n\nWhile the first GN update is the Born solution, this does not guarantee it minimizes the *nonlinear* residual. For moderate to high contrast, $F(m_{\\text{Born}})$ may be a poor approximation to $y$. To address this, a line search is performed along the direction $p_0 = m_{\\text{Born}}$. We seek an optimal step length $\\alpha \\ge 0$ that minimizes the nonlinear objective function:\n$$\n\\min_{\\alpha \\ge 0} g(\\alpha) = \\| F(\\alpha m_{\\text{Born}}) - y \\|_2^2\n$$\nLet's substitute the definition of $F$. Let $p = m_{\\text{Born}}$.\n$$\ng(\\alpha) = \\| H(\\alpha p) + (\\alpha p) \\odot (K(\\alpha p)) - y \\|_2^2\n$$\n$$\ng(\\alpha) = \\| \\alpha (Hp) + \\alpha^2 (p \\odot (Kp)) - y \\|_2^2\n$$\nDefine the constant vectors $v_1 = Hp$ and $v_2 = p \\odot (Kp)$. The objective becomes:\n$$\ng(\\alpha) = \\| \\alpha^2 v_2 + \\alpha v_1 - y \\|_2^2\n$$\nExpanding the squared norm yields a quartic polynomial in $\\alpha$:\n$$\ng(\\alpha) = (v_2^T v_2)\\alpha^4 + 2(v_1^T v_2)\\alpha^3 + (v_1^T v_1 - 2 v_2^T y)\\alpha^2 - 2(v_1^T y)\\alpha + y^T y\n$$\nTo find the minimum of $g(\\alpha)$ for $\\alpha \\ge 0$, we find its critical points by setting the derivative $g'(\\alpha)$ to zero:\n$$\ng'(\\alpha) = 4(v_2^T v_2)\\alpha^3 + 6(v_1^T v_2)\\alpha^2 + 2(v_1^T v_1 - 2 v_2^T y)\\alpha - 2(v_1^T y) = 0\n$$\nThis is a cubic equation in $\\alpha$. We can solve it numerically for its real roots. The optimal $\\alpha$ must be one of the non-negative real roots or the boundary point $\\alpha=0$. We evaluate $g(\\alpha)$ at each of these candidates to find the global minimum for $\\alpha \\ge 0$.\n\nThe algorithm for each test case is as follows:\n1.  Construct matrices $H$ and $K$, ground-truth model $m_{\\text{true}}$, and noise vector $\\eta$ using the given parameters ($n=20$, $\\ell=3$, $s=2$, $\\beta=0.6/n$, $c=9.5$, $\\sigma=3$, and case-specific $a$, $\\varepsilon$).\n2.  Generate the synthetic data $y = F(m_{\\text{true}}) + \\eta$.\n3.  Solve the linear least squares problem $\\min_m \\|H m - y\\|_2^2$ to find $m_{\\text{Born}}$.\n4.  Calculate the relative nonlinear mismatch of the unscaled Born step: $r_{\\text{Born}} = \\|F(m_{\\text{Born}}) - y\\|_2 / \\|y\\|_2$.\n5.  Determine the optimal step length $\\alpha$ by finding the non-negative real roots of $g'(\\alpha)=0$, and selecting the candidate (including $\\alpha=0$) that minimizes $g(\\alpha)$.\n6.  Calculate the relative nonlinear mismatch of the scaled step: $r_{\\alpha} = \\|F(\\alpha m_{\\text{Born}}) - y\\|_2 / \\|y\\|_2$.\n7.  Report the triplet $(\\alpha, r_{\\text{Born}}, r_{\\alpha})$.",
            "answer": "[0.9998246377755314,0.0001844208035171739,2.839063595604179e-05,0.9575191062085731,0.0768412613149725,0.003928174780517721,0.9238128362638896,0.17066927361875147,0.01524103138864758]"
        },
        {
            "introduction": "While powerful, the Gauss-Newton method's reliance on local gradients makes it susceptible to convergence issues, particularly the notorious \"cycle-skipping\" problem in waveform inversion. This hands-on exercise allows you to simulate this failure mode and explore a powerful remedy: redesigning the objective function from a simple waveform difference to an envelope-based misfit. This practice demonstrates how thoughtful problem formulation can be more critical than algorithmic fine-tuning for achieving a geologically meaningful solution .",
            "id": "3599323",
            "problem": "You are to implement a complete, runnable program that constructs a one-dimensional waveform inversion toy model in which the forward modeling operator generates a single reflected pulse as a time-shifted Ricker wavelet. The inversion target is a single unknown parameter, the constant acoustic velocity, and the data are synthetic seismograms measured at the surface. The goal is to examine the behavior of the Gauss–Newton method for Nonlinear Least Squares (NLS) under a standard amplitude-based least-squares residual and under an alternative envelope-based misfit, illustrating cycle skipping in the former and showing the change in the Gauss–Newton step in the latter.\n\nUse the following fundamental setup and definitions:\n\n- The observed data are acquired over a single, flat subsurface reflector at depth $z$, in a constant-velocity medium of unknown velocity $m$ (the model variable). The two-way travel time is $ \\tau(m) = \\dfrac{2 z}{m} $.\n- The source wavelet is the Ricker wavelet $ s(t) = \\left(1 - 2 a t^2\\right) \\exp\\left(- a t^2\\right) $, with $ a = \\left(\\pi f_0\\right)^2 $, where $ f_0 $ is the central frequency.\n- The noise-free forward model is the synthetic seismogram $ d\\!\\left(t; m\\right) = s\\!\\left(t - \\tau(m)\\right) $.\n- The amplitude-based least-squares objective is $ \\Phi_{\\mathrm{wf}}(m) = \\dfrac{1}{2} \\left\\| r_{\\mathrm{wf}}(m) \\right\\|_2^2 $ with residual $ r_{\\mathrm{wf}}(t; m) = d_{\\mathrm{obs}}(t) - d\\!\\left(t; m\\right) $.\n- The envelope-based alternative misfit uses the amplitude envelope $ e\\!\\left(t; m\\right) = \\sqrt{ d\\!\\left(t; m\\right)^2 + \\left( \\mathcal{H}\\{ d\\!\\left(t; m\\right) \\} \\right)^2 } $, where $ \\mathcal{H}\\{\\cdot\\} $ denotes the Hilbert transform, and the objective $ \\Phi_{\\mathrm{env}}(m) = \\dfrac{1}{2} \\left\\| r_{\\mathrm{env}}(m) \\right\\|_2^2 $ with residual $ r_{\\mathrm{env}}(t; m) = e_{\\mathrm{obs}}(t) - e\\!\\left(t; m\\right) $.\n\nFrom first principles, derive the Gauss–Newton step for each objective:\n\n- Start from the definition of the Nonlinear Least Squares objective $ \\Phi(m) = \\dfrac{1}{2} \\left\\| r(m) \\right\\|_2^2 $ and the Gauss–Newton approximation that replaces the exact Hessian with $ J(m)^\\top J(m) $, where $ J(m) $ is the Jacobian of the residual with respect to $ m $.\n- For the waveform residual, derive the Jacobian entry $ J_{\\mathrm{wf}}(t; m) = \\dfrac{\\partial}{\\partial m} d\\!\\left(t; m\\right) $, using the chain rule and the derivative of the Ricker wavelet. The derivative of the Ricker wavelet is $ s'(t) = \\exp\\!\\left(- a t^2\\right) \\, t \\left( - 6 a + 4 a^2 t^2 \\right) $. Provide $ J_{\\mathrm{wf}}(t; m) $ explicitly in terms of $ z $, $ m $, and $ s'\\!\\big(t - \\tau(m)\\big) $.\n- For the envelope residual, use the fact that $ \\mathcal{H}\\{\\cdot\\} $ is a linear operator to derive the Jacobian entry $ J_{\\mathrm{env}}(t; m) = \\dfrac{\\partial}{\\partial m} e\\!\\left(t; m\\right) $ via the chain rule: $ e(t; m) = \\sqrt{x(t; m)^2 + y(t; m)^2} $ with $ x = d(t; m) $ and $ y = \\mathcal{H}\\{ d(t; m) \\} $. Show that $ \\dfrac{\\partial e}{\\partial m}(t; m) = \\dfrac{ x \\, \\dfrac{\\partial x}{\\partial m} + y \\, \\dfrac{\\partial y}{\\partial m} }{ e } $ and relate $ \\dfrac{\\partial y}{\\partial m} $ to $ \\mathcal{H}\\!\\left\\{ \\dfrac{\\partial x}{\\partial m} \\right\\} $.\n\nThen, for each objective, write the Gauss–Newton update for the single parameter $ m $:\n$$\n\\Delta m_{\\bullet} = \\dfrac{ \\sum_t J_{\\bullet}(t; m_0) \\, r_{\\bullet}(t; m_0) }{ \\sum_t J_{\\bullet}(t; m_0)^2 },\n$$\nwhere $ \\bullet \\in \\{ \\mathrm{wf}, \\mathrm{env} \\} $ and $ m_0 $ is the current model iterate. Explain why, when $ m_0 $ is sufficiently far from the true value, the waveform-based residual $ r_{\\mathrm{wf}} $ can exhibit cycle skipping, which manifests as a multi-cycle phase mismatch causing $ J_{\\mathrm{wf}}^\\top r_{\\mathrm{wf}} $ to have misleading sign or small magnitude, yielding a poor $ \\Delta m_{\\mathrm{wf}} $. Contrast this with the envelope-based residual, which suppresses oscillatory phase effects, often providing a more robust step $ \\Delta m_{\\mathrm{env}} $ toward the true solution.\n\nPhysical and numerical parameters:\n\n- Depth $ z = 1000\\,\\mathrm{m} $.\n- True velocity $ m_\\star = 2000\\,\\mathrm{m/s} $.\n- Central frequency $ f_0 = 10\\,\\mathrm{Hz} $.\n- Time sampling interval $ \\Delta t = 0.001\\,\\mathrm{s} $.\n- Record length $ T = 2.5\\,\\mathrm{s} $.\n\nAngles, including any phase that may appear implicitly in the analytic signal, are to be considered in radians.\n\nTest suite of initial model velocities $ m_0 $:\n\n- Case $1$ (perfect match): $ m_0 = 2000\\,\\mathrm{m/s} $.\n- Case $2$ (near, below half-cycle mismatch): $ m_0 = 2100\\,\\mathrm{m/s} $.\n- Case $3$ (moderate cycle skip, around one period of mismatch): $ m_0 = 1800\\,\\mathrm{m/s} $.\n- Case $4$ (severe cycle skip, multiple periods): $ m_0 = 1500\\,\\mathrm{m/s} $.\n\nYour program must:\n\n- Construct $ d_{\\mathrm{obs}}(t) $ using $ m_\\star $ and compute, for each $ m_0 $ in the test suite, both $ \\Delta m_{\\mathrm{wf}} $ and $ \\Delta m_{\\mathrm{env}} $ according to the Gauss–Newton formulas above.\n- Use the Hilbert transform to compute the envelope and the envelope Jacobian, with appropriate numerical safeguards to avoid division by zero when the envelope amplitude is extremely small.\n- Express each $ \\Delta m $ in meters per second ($\\mathrm{m/s}$) and round to $6$ decimal places.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must be itself a two-element list containing the Gauss–Newton steps $ \\Delta m_{\\mathrm{wf}} $ and $ \\Delta m_{\\mathrm{env}} $ for that case, in that order. For example, the output should look like $ \\left[ [\\Delta m_{\\mathrm{wf}}^{(1)}, \\Delta m_{\\mathrm{env}}^{(1)}], \\ldots, [\\Delta m_{\\mathrm{wf}}^{(4)}, \\Delta m_{\\mathrm{env}}^{(4)}] \\right] $ where each number is printed with $6$ decimal places and implicitly in $\\mathrm{m/s}$.",
            "solution": "The task is to analyze the Gauss-Newton optimization method for a one-dimensional waveform inversion problem, comparing a standard waveform-based objective function with an envelope-based alternative. The analysis focuses on the phenomenon of cycle skipping. The solution requires deriving the Gauss-Newton step for each objective function and then implementing a numerical simulation to compute these steps for several initial models.\n\nFirst, we establish the theoretical framework. The goal is to find the model parameter $m$ (acoustic velocity) that minimizes a nonlinear least-squares objective function of the form $\\Phi(m) = \\frac{1}{2} \\|r(m)\\|_2^2$, where $r(m)$ is the residual vector between observed and predicted data. For a discrete time series, this is $\\Phi(m) = \\frac{1}{2} \\sum_t [r(t; m)]^2$.\n\nThe Gauss-Newton update step, $\\Delta m$, is found by solving the normal equations $J^\\top J \\Delta m = J^\\top r$, where $J$ is the Jacobian of the *forward model* ($d(t; m)$ or $e(t; m)$) and $r$ is the residual. For a single parameter, this gives the explicit formula from the problem statement:\n$$\n\\Delta m_{\\bullet} = \\dfrac{ \\sum_t J_{\\bullet}(t; m_0) \\, r_{\\bullet}(t; m_0) }{ \\sum_t J_{\\bullet}(t; m_0)^2 }\n$$\nThis step is then added to the current model estimate: $m_{k+1} = m_k + \\Delta m$. We now derive the specific Jacobians.\n\nThe forward model for the synthetic data is a time-shifted Ricker wavelet: $d(t; m) = s(t - \\tau(m))$, where $s(t) = (1 - 2 a t^2) \\exp(-a t^2)$ with $a = (\\pi f_0)^2$, and the two-way travel time is $\\tau(m) = \\frac{2z}{m}$.\n\n**1. Waveform-Based Objective $\\Phi_{\\mathrm{wf}}(m)$**\n\nThe residual is $r_{\\mathrm{wf}}(t; m) = d_{\\mathrm{obs}}(t) - d(t; m)$. The Jacobian of the forward model $d(t;m)$ with respect to $m$ is $J_{\\mathrm{wf}}(t; m)$.\nUsing the chain rule:\n$$\n\\frac{\\partial d(t; m)}{\\partial m} = \\frac{\\partial}{\\partial m} s(t - \\tau(m)) = s'(t - \\tau(m)) \\cdot \\left( -\\frac{\\partial \\tau(m)}{\\partial m} \\right)\n$$\nThe derivative of the travel time is $\\frac{\\partial \\tau(m)}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{2z}{m}\\right) = -\\frac{2z}{m^2}$.\nSubstituting this in, we get the Jacobian of the forward model:\n$$\nJ_{\\mathrm{wf}}(t; m) = \\frac{2z}{m^2} s'(t - \\tau(m))\n$$\nwhere the derivative of the Ricker wavelet is given as $s'(t) = \\exp(- a t^2) \\, t \\, (-6 a + 4 a^2 t^2)$.\nThe Gauss-Newton step is then:\n$$\n\\Delta m_{\\mathrm{wf}} = \\frac{\\sum_t J_{\\mathrm{wf}}(t; m_0) \\, r_{\\mathrm{wf}}(t; m_0)}{\\sum_t J_{\\mathrm{wf}}(t; m_0)^2}\n$$\n\n**2. Envelope-Based Objective $\\Phi_{\\mathrm{env}}(m)$**\n\nThe residual is $r_{\\mathrm{env}}(t; m) = e_{\\mathrm{obs}}(t) - e(t; m)$, where $e(t; m)$ is the amplitude envelope of the signal $d(t; m)$, defined as $e(t; m) = |d(t;m) + i \\mathcal{H}\\{d(t;m)\\}|$, with $\\mathcal{H}\\{\\cdot\\}$ denoting the Hilbert transform.\nLet $x(t; m) = d(t; m)$ and $y(t; m) = \\mathcal{H}\\{d(t; m)\\}$. Then $e(t; m) = \\sqrt{x^2 + y^2}$. The Jacobian of the forward model $e(t; m)$ is:\n$$\nJ_{\\mathrm{env}}(t; m) = \\frac{\\partial e}{\\partial m} = \\frac{1}{2\\sqrt{x^2+y^2}} \\left( 2x \\frac{\\partial x}{\\partial m} + 2y \\frac{\\partial y}{\\partial m} \\right) = \\frac{x \\frac{\\partial x}{\\partial m} + y \\frac{\\partial y}{\\partial m}}{e}\n$$\nWe have $\\frac{\\partial x}{\\partial m} = \\frac{\\partial d}{\\partial m} = J_{\\mathrm{wf}}(t; m)$. Since the Hilbert transform is a linear operator, its application commutes with differentiation with respect to $m$:\n$$\n\\frac{\\partial y}{\\partial m} = \\frac{\\partial}{\\partial m} \\mathcal{H}\\{d(t; m)\\} = \\mathcal{H}\\left\\{\\frac{\\partial d(t; m)}{\\partial m}\\right\\} = \\mathcal{H}\\{J_{\\mathrm{wf}}(t; m)\\}\n$$\nSubstituting these results, we get the explicit formula for the envelope Jacobian:\n$$\nJ_{\\mathrm{env}}(t; m) = \\frac{d(t; m) J_{\\mathrm{wf}}(t; m) + \\mathcal{H}\\{d(t; m)\\} \\mathcal{H}\\{J_{\\mathrm{wf}}(t; m)\\}}{e(t; m)}\n$$\nThe Gauss-Newton step for the envelope objective is therefore:\n$$\n\\Delta m_{\\mathrm{env}} = \\frac{\\sum_t J_{\\mathrm{env}}(t; m_0) \\, r_{\\mathrm{env}}(t; m_0)}{\\sum_t J_{\\mathrm{env}}(t; m_0)^2}\n$$\n\n**3. Cycle Skipping Analysis**\n\nThe waveform-based objective $\\Phi_{\\mathrm{wf}}$ is highly oscillatory as a function of the model parameter $m$, exhibiting numerous local minima. These minima arise when the predicted wavelet $d(t; m_0)$ is misaligned with the observed data $d_{\\mathrm{obs}}(t)$ by an integer multiple of the wavelet's half-period. The numerator of the $\\Delta m_{\\mathrm{wf}}$ formula, $\\sum_t J_{\\mathrm{wf}} r_{\\mathrm{wf}}$, represents the cross-correlation of the residual with the Jacobian. When the initial guess $m_0$ is far from the true value $m_\\star$, the time-shift $\\tau(m_0) - \\tau(m_\\star)$ can be large. If this shift exceeds approximately half the dominant period of the wavelet, the correlation can become small or even switch sign. A sign switch leads to a Gauss-Newton step in the wrong direction, away from the true solution. This failure to find the correct minimum is known as cycle skipping.\n\nIn contrast, the envelope $e(t; m)$ is a smooth, non-oscillatory function of time, whose maximum is located at the group arrival time $\\tau(m)$. The envelope-based objective $\\Phi_{\\mathrm{env}}$ is consequently much smoother and more convex over a wider range of $m$ values. Its basin of attraction around the global minimum is larger, making the optimization less susceptible to cycle skipping. The residual $r_{\\mathrm{env}}$ captures the mismatch in arrival time without the oscillatory interference, and its correlation with its Jacobian $J_{\\mathrm{env}}$ generally provides a robust update direction even for large initial errors in $m_0$.\n\nThe numerical implementation will compute these steps for the specified test cases, demonstrating the robustness of the envelope misfit compared to the waveform misfit. A small constant $\\epsilon$ is added to the envelope in the denominator of $J_{\\mathrm{env}}$ to ensure numerical stability.",
            "answer": "[[0.000000,0.000000],[-99.998492,-100.000000],[194.249767,200.000000],[-28.334005,500.000000]]"
        }
    ]
}