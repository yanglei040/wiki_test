{
    "hands_on_practices": [
        {
            "introduction": "在理论上，最小二乘法为所有数据点提供了最优拟合。然而在实践中，并非所有数据点的作用都是均等的。某些观测值由于其在实验设计中的独特位置，可能对模型解产生不成比例的巨大影响，这些点被称为“高杠杆点”。本练习将通过一个具体实例，带您亲手诊断这些关键数据点，并运用加权最小二乘法来减轻其过度影响，从而更稳健地估计模型参数 。",
            "id": "3606778",
            "problem": "在计算地球物理学中的一维折射实验中，假设存在一个未知的恒定地下慢度 $s$（单位为秒/公里，$\\mathrm{s/km}$）。一组震源-接收器偏移距 $L_i$（单位为公里）和相应的观测走时 $t_i$（单位为秒）遵循线性数据模型 $t_i = L_i s + \\epsilon_i$，其中 $\\epsilon_i$ 是独立的、零均值、有限方差的误差。考虑设计矩阵 $G \\in \\mathbb{R}^{N \\times 1}$，其第 $i$ 行为 $[L_i]$，以及数据向量 $d \\in \\mathbb{R}^{N}$，其分量为 $t_i$。给定以下 $N=4$ 的特定数据集：\n- 偏移距：$L = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$，\n- 观测时间：$t = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$。\n\n原则上将该问题视为一个 $N \\gg 1$ 的超定系统，但为了便于分析，此处使用 $N=4$。从普通最小二乘法 (OLS) 定义为残差平方和的最小化器以及帽子矩阵的定义 $H = G\\,(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$ 出发，完成以下任务：\n\n1. 直接通过最小化残差平方和 $\\sum_{i=1}^{N} (t_i - L_i s)^2$ 来推导此单参数问题的 OLS 估计值 $\\hat{s}$。使用你的结果计算给定数据的 $\\hat{s}$。\n\n2. 使用帽子矩阵的定义，计算杠杆值 $h_{ii}$ 并解释此数据集的逐行杠杆作用。以小数形式量化第四行所占总对角杠杆的比例。\n\n3. 为了减轻高杠杆行的主导作用，考虑使用对角权重 $W = \\mathrm{diag}(w_1,\\dots,w_N)$ 的加权最小二乘法 (WLS)，该方法最小化 $\\sum_{i=1}^{N} w_i (t_i - L_i s)^2$。选择权重 $w_1 = w_2 = w_3 = 1$ 和 $w_4 = 0.01$ 来降低第四个观测值的权重。推导 WLS 估计量 $\\hat{s}_w$ 并计算其在给定数据下的值。\n\n4. 使用带有 Huber 损失的 M 估计量的方程，简要解释迭代重加权最小二乘法 (IRLS) 过程如何通过分配一个较小的有效权重来自适应地减少像第四个数据点这样的离群残差的影响，而无需硬编码 $w_4$。你不需要执行迭代。\n\n作为最终答案，提供加权估计值 $\\hat{s}_w$ 的数值。结果以 $\\mathrm{s/km}$ 为单位，并四舍五入到四位有效数字。",
            "solution": "问题陈述已经过验证，被认为是有效的。它科学地基于应用于地球物理学的线性反演理论原理，是适定的，并使用客观、正式的语言。所有必要的数据均已提供且一致。我们可以开始求解。\n\n该问题要求使用特定数据集对一个简单的线性反演问题进行四部分分析。走时 $t$ 和偏移距 $L$ 之间的关系被建模为一条通过原点的直线，$t = Ls$，其中 $s$ 是慢度。\n\n**1. 普通最小二乘法 (OLS) 估计**\n\n参数 $s$ 的普通最小二乘法 (OLS) 估计值是使残差平方和 $S(s)$ 最小化的值 $\\hat{s}$。第 $i$ 次测量的残差是 $r_i = t_i - L_i s$。目标函数为：\n$$\nS(s) = \\sum_{i=1}^{N} r_i^2 = \\sum_{i=1}^{N} (t_i - L_i s)^2\n$$\n为了找到最小值，我们计算 $S(s)$ 关于 $s$ 的导数并将其设为零：\n$$\n\\frac{dS}{ds} = \\sum_{i=1}^{N} \\frac{d}{ds} (t_i - L_i s)^2 = \\sum_{i=1}^{N} 2(t_i - L_i s)(-L_i) = -2 \\sum_{i=1}^{N} (L_i t_i - L_i^2 s)\n$$\n令 $\\frac{dS}{ds} = 0$：\n$$\n\\sum_{i=1}^{N} (L_i t_i - L_i^2 s) = 0 \\implies \\sum_{i=1}^{N} L_i t_i - s \\sum_{i=1}^{N} L_i^2 = 0\n$$\n求解 $s$ 得到 OLS 估计值 $\\hat{s}$：\n$$\n\\hat{s} = \\frac{\\sum_{i=1}^{N} L_i t_i}{\\sum_{i=1}^{N} L_i^2}\n$$\n这是通过原点的单参数线性拟合的通用公式。对于给定的数据集 $L = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$ 和 $t = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$，我们计算和：\n$$\n\\sum_{i=1}^{4} L_i t_i = (1)(0.5) + (1)(0.5) + (1)(0.5) + (10)(6.0) = 0.5 + 0.5 + 0.5 + 60.0 = 61.5\n$$\n$$\n\\sum_{i=1}^{4} L_i^2 = 1^2 + 1^2 + 1^2 + 10^2 = 1 + 1 + 1 + 100 = 103\n$$\n因此，OLS 估计值为：\n$$\n\\hat{s} = \\frac{61.5}{103} \\approx 0.597087\\; \\mathrm{s/km}\n$$\n\n**2. 帽子矩阵与杠杆值**\n\n帽子矩阵 $H$ 将观测数据向量 $d$ 映射到预测数据向量 $\\hat{d}$，即 $\\hat{d} = Hd$。对于一般的线性模型 $d = Gm$，帽子矩阵定义为 $H = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$。$H$ 的对角元素，记为 $h_{ii}$，是杠杆值。\n\n在这个问题中，模型参数向量 $m$ 就是标量 $s$。设计矩阵 $G$ 是由偏移距 $L_i$ 组成的列向量：\n$$\nG = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix}\n$$\n首先，我们计算 $G^{\\mathsf{T}}G$：\n$$\nG^{\\mathsf{T}}G = \\begin{pmatrix} 1  & 1 & 1 & 10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} = 1^2 + 1^2 + 1^2 + 10^2 = 103\n$$\n这是一个标量，所以它的逆就是：\n$$\n(G^{\\mathsf{T}}G)^{-1} = \\frac{1}{103}\n$$\n现在我们构建帽子矩阵 $H$：\n$$\nH = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} \\left(\\frac{1}{103}\\right) \\begin{pmatrix} 1 & 1 & 1 & 10 \\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1\\cdot1 & 1\\cdot1 & 1\\cdot1 & 1\\cdot10 \\\\\n1\\cdot1 & 1\\cdot1 & 1\\cdot1 & 1\\cdot10 \\\\\n1\\cdot1 & 1\\cdot1 & 1\\cdot1 & 1\\cdot10 \\\\\n10\\cdot1 & 10\\cdot1 & 10\\cdot1 & 10\\cdot10\n\\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1 & 1 & 1 & 10 \\\\\n1 & 1 & 1 & 10 \\\\\n1 & 1 & 1 & 10 \\\\\n10 & 10 & 10 & 100\n\\end{pmatrix}\n$$\n杠杆值 $h_{ii}$ 是 $H$ 的对角元素：\n$h_{11} = \\frac{1}{103}$，$h_{22} = \\frac{1}{103}$，$h_{33} = \\frac{1}{103}$，以及 $h_{44} = \\frac{100}{103}$。\n\n杠杆值 $h_{ii}$ 量化了第 $i$ 个观测值 $t_i$ 对其自身预测值 $\\hat{t}_i$ 的影响。接近 1 的值表示该观测值具有高影响力，有效地将模型拟合拉向其自身。在这里，前三个数据点的杠杆值非常低（$h_{11}=h_{22}=h_{33} \\approx 0.0097$），而第四个数据点的杠杆值极高（$h_{44} \\approx 0.9709$）。这意味着 OLS 解几乎完全由在 $L_4=10$ 处的第四个测量值决定。\n\n帽子矩阵的对角元素之和 $\\mathrm{Tr}(H)$ 等于模型参数的数量，本例中为 1。$\\sum_{i=1}^{4} h_{ii} = \\frac{1}{103} + \\frac{1}{103} + \\frac{1}{103} + \\frac{100}{103} = \\frac{103}{103} = 1$。与第四行相关的总对角杠杆的比例是 $h_{44} / \\mathrm{Tr}(H) = h_{44} / 1 = h_{44}$。以小数表示，这是：\n$$\n\\frac{100}{103} \\approx 0.9709\n$$\n\n**3. 加权最小二乘法 (WLS) 估计**\n\n为了减轻高杠杆第四点的影响，我们使用加权最小二乘法 (WLS)。WLS 估计值 $\\hat{s}_w$ 最小化加权残差平方和：\n$$\nS_w(s) = \\sum_{i=1}^{N} w_i (t_i - L_i s)^2\n$$\n遵循与 OLS 相同的微分过程，我们找到 WLS 估计量：\n$$\n\\hat{s}_w = \\frac{\\sum_{i=1}^{N} w_i L_i t_i}{\\sum_{i=1}^{N} w_i L_i^2}\n$$\n使用指定的权重 $w_1 = w_2 = w_3 = 1$ 和 $w_4 = 0.01$，我们计算加权和：\n$$\n\\sum_{i=1}^{4} w_i L_i t_i = (1)(1)(0.5) + (1)(1)(0.5) + (1)(1)(0.5) + (0.01)(10)(6.0) = 0.5 + 0.5 + 0.5 + 0.6 = 2.1\n$$\n$$\n\\sum_{i=1}^{4} w_i L_i^2 = (1)(1^2) + (1)(1^2) + (1)(1^2) + (0.01)(10^2) = 1 + 1 + 1 + (0.01)(100) = 3 + 1 = 4\n$$\n因此，WLS 估计值为：\n$$\n\\hat{s}_w = \\frac{2.1}{4} = 0.525\\; \\mathrm{s/km}\n$$\n这个值远接近于前三个点所暗示的 $0.5\\; \\mathrm{s/km}$ 的慢度，表明成功地降低了高杠杆第四点的影响。\n\n**4. M估计量与迭代重加权最小二乘法 (IRLS)**\n\nM估计量旨在最小化一个更通用的目标函数 $\\sum_{i=1}^{N} \\rho(r_i)$，其中 $r_i$ 是残差，$\\rho(r)$ 是一个稳健的损失函数，对于大的 $r$，其增长速度比 $r^2$ 慢。Huber 损失函数是一个常见的选择：\n$$\n\\rho(r) = \\begin{cases}\n  \\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n  \\delta(|r| - \\frac{1}{2}\\delta) & \\text{if } |r| > \\delta\n\\end{cases}\n$$\n其中 $\\delta$ 是一个调节参数。最小化此目标函数会得到非线性估计方程 $\\sum_{i=1}^{N} L_i \\psi(r_i) = 0$，其中 $\\psi(r) = \\rho'(r)$ 是影响函数。\n\n迭代重加权最小二乘法 (IRLS) 是求解该方程的一种程序。它的工作原理是将估计方程重构为一个加权最小二乘问题，其中权重取决于残差本身。我们定义一个自适应权重函数 $W(r) = \\psi(r)/r$。对于 Huber 损失，相关的权重函数是：\n$$\nW(r) = \\frac{\\psi(r)}{r} = \\begin{cases}\n  1 & \\text{if } |r| \\le \\delta \\\\\n  \\frac{\\delta}{|r|} & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nIRLS 过程如下：\n1.  获得一个初始估计，例如 OLS 估计值 $\\hat{s}^{(0)}$。\n2.  对于迭代 $k=1, 2, ...$：\n    a. 计算残差：$r_i^{(k-1)} = t_i - L_i \\hat{s}^{(k-1)}$。\n    b. 计算新权重：$w_i^{(k)} = W(r_i^{(k-1)})$。具有大残差（$|r_i| > \\delta$）的数据点将被赋予小于 1 的权重。\n    c. 使用这些新权重解决 WLS 问题以获得更新的估计值：\n       $$\n       \\hat{s}^{(k)} = \\frac{\\sum_{i=1}^N w_i^{(k)} L_i t_i}{\\sum_{i=1}^N w_i^{(k)} L_i^2}\n       $$\n3.  重复此过程，直到估计值 $\\hat{s}^{(k)}$ 收敛。\n\n在此问题的背景下，如果一个数据点的残差相对于数据的某种稳健尺度度量而言较大，则该数据点被视为离群值。前三个点暗示慢度为 $s=0.5\\;\\mathrm{s/km}$。相对于这一趋势，第四个点产生了一个大残差：$r_4 = 6.0 - 10(0.5) = 1.0$。一个 IRLS 程序，在初始步骤后，会识别出这个大残差，赋予一个较小的有效权重 $w_4 = \\delta/|r_4| < 1$，并重新计算解。这个过程自适应地、自动地减少任何与大部分数据不一致的数据点（离群值）的影响，而不需要用户像在 WLS 中那样硬编码权重。",
            "answer": "$$\\boxed{0.5250}$$"
        },
        {
            "introduction": "一个线性系统的稳定性直接决定了其最小二乘解的可靠性。当设计矩阵 $G$ 的列向量近似线性相关时，系统被称为“病态的”，这会导致解的方差急剧增大，对数据中的微小噪声变得极其敏感。本练习通过一个含有参数 $\\varepsilon$ 的可控示例，让您直观地理解病态问题如何在解空间中形成一个“扁平的山谷”，并推导量化这种方差放大效应的关键指标 。",
            "id": "3606781",
            "problem": "在线性化的双单元介质走时层析成像中，假设数据残差向量 $d \\in \\mathbb{R}^{3}$ 建模为 $d = G(\\varepsilon) \\, m_{\\mathrm{true}} + n$，其中 $m_{\\mathrm{true}} \\in \\mathbb{R}^{2}$ 是标度单位下单元慢度的扰动，而 $n$ 是观测噪声。考虑设计矩阵\n$$\nG(\\varepsilon) \\;=\\;\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 1+\\varepsilon \\\\\n0 & \\varepsilon\n\\end{pmatrix},\n$$\n其中 $\\varepsilon > 0$ 很小，代表三条不同的射线，其路径长度敏感度使得当 $\\varepsilon \\to 0$ 时，$G(\\varepsilon)$ 的两列近似共线。假设 $n$ 是零均值高斯分布，其分量独立且方差相等，即对于某个 $\\sigma^{2} > 0$，$n \\sim \\mathcal{N}(0, \\sigma^{2} I)$。\n\n普通最小二乘 (OLS) 估计 $\\hat{m}(\\varepsilon)$ 最小化残差的平方范数 $J(m) = \\| G(\\varepsilon) \\, m - d \\|_{2}^{2}$。在极限 $\\varepsilon \\to 0$ 下，矩阵 $G(\\varepsilon)$ 趋于秩亏，因此失配景观会形成一个由近似等效的极小值点构成的长而浅的谷。\n\n任务：\n- 仅使用最小二乘法的定义和奇异值分解 (SVD, singular value decomposition) 导出的几何结构，给出一个构造，展示 $G(\\varepsilon)$ 的近秩亏性如何产生多个近似等效的极小值点。特别地，论证存在一个单位方向 $v_{\\varepsilon} \\in \\mathbb{R}^{2}$，在该方向上，$J$ 在极小值点处的二阶方向导数为 $O(\\varepsilon^{2})$，并给出一对明确的 $m_{\\star}$ 和 $m_{\\star} + \\delta v_{\\varepsilon}$（其中 $\\delta$ 与 $\\varepsilon$ 无关），其失配差异在 $\\varepsilon \\to 0$ 时为 $O(\\varepsilon^{2})$。\n- 在上述假设下，从第一性原理（OLS 和协方差为 $\\sigma^{2} I$ 的高斯噪声的定义）出发，推导无量纲方差放大因子\n$$\nA(\\varepsilon) \\;=\\; \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{m}(\\varepsilon)]\\big),\n$$\n作为仅关于 $\\varepsilon$ 的函数的精确表达式。\n\n通过给出 $A(\\varepsilon)$ 的精确闭式解析表达式来提供最终答案。不需要数值四舍五入，所求量为无量纲量，因此无需报告单位。",
            "solution": "首先对问题陈述进行验证。\n\n### 步骤1：提取已知条件\n- 数据向量：$d \\in \\mathbb{R}^{3}$。\n- 模型向量：$m_{\\mathrm{true}} \\in \\mathbb{R}^{2}$。\n- 线性模型：$d = G(\\varepsilon) \\, m_{\\mathrm{true}} + n$。\n- 设计矩阵：$G(\\varepsilon) = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\varepsilon \\\\ 0 & \\varepsilon \\end{pmatrix}$。\n- 参数 $\\varepsilon > 0$ 很小。\n- 噪声向量 $n$ 的分布为 $\\mathcal{N}(0, \\sigma^{2} I)$，其中 $\\sigma^{2} > 0$，$I$ 是 $3 \\times 3$ 的单位矩阵。\n- 普通最小二乘 (OLS) 估计 $\\hat{m}(\\varepsilon)$ 最小化成本函数 $J(m) = \\| G(\\varepsilon) \\, m - d \\|_{2}^{2}$。\n- 任务1：使用 SVD 原理构造一个论证，说明 $G(\\varepsilon)$ 的近秩亏性如何导致近似等效的极小值点。确定一个单位方向 $v_{\\varepsilon}$，在该方向上，$J$ 在极小值点处的二阶方向导数为 $O(\\varepsilon^2)$。给出一对模型 $m_{\\star}$ 和 $m_{\\star} + \\delta v_{\\varepsilon}$，其失配差异为 $O(\\varepsilon^2)$。\n- 任务2：推导方差放大因子 $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{m}(\\varepsilon)]\\big)$ 的精确表达式。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题是线性代数和统计学中一个明确定义的练习，特别地，它应用于地球物理学和其他实验科学中常见的线性反演理论。\n\n- **科学上成立**：该问题是分析病态线性系统 $Gm=d$ 的一个典型例子。矩阵 $G(\\varepsilon)$ 代表了一个简化但物理上合理的场景，即在层析成像中，射线以非常相似的灵敏度对两个单元进行采样，导致 $G$ 的列近似线性相关。对估计量协方差的分析是反演问题理论的一个标准和基本组成部分。\n- **适定的**：该问题在数学上是适定的。它提供了所有必要的定义和矩阵。任务明确，要求一个论证性构造和一个具体推导。$A(\\varepsilon)$ 存在唯一的解析解。\n- **客观的**：问题以客观、数学的语言陈述，没有歧义或主观内容。\n\n满足有效问题的所有标准。未发现任何缺陷。\n\n### 步骤3：结论与行动\n问题是**有效的**。将提供完整解答。\n\n### 解答推导\n\n解答分为两部分，对应于两个任务。\n\n#### 第一部分：失配景观与近似等效的极小值点\n\n需要最小化的成本函数是残差向量的 $L_2$-范数的平方：\n$$\nJ(m) = \\| G(\\varepsilon)m - d \\|_{2}^{2} = (G(\\varepsilon)m - d)^\\mathsf{T} (G(\\varepsilon)m - d)\n$$\n这是模型参数 $m$ 的二次函数。$J(m)$ 的梯度和黑塞矩阵为：\n$$\n\\nabla J(m) = 2 G(\\varepsilon)^\\mathsf{T} (G(\\varepsilon)m - d)\n$$\n$$\n\\nabla^2 J(m) = 2 G(\\varepsilon)^\\mathsf{T} G(\\varepsilon)\n$$\n黑塞矩阵是常数，意味着失配曲面是一个抛物碗。这个碗在任何方向上的“平坦度”由二阶方向导数决定。对于一个单位方向向量 $v \\in \\mathbb{R}^2$，$J$ 在任意点 $m_0$ 处的二阶方向导数由下式给出：\n$$\nD_v^2 J(m_0) = v^\\mathsf{T} (\\nabla^2 J) v = 2 v^\\mathsf{T} G(\\varepsilon)^\\mathsf{T} G(\\varepsilon) v = 2 \\|G(\\varepsilon)v\\|_2^2\n$$\n问题陈述中关于“长而浅的谷”对应于在模型空间中找到一个方向 $v$，使得该二阶导数非常小。这意味着我们正在寻找一个方向 $v$，使得 $G(\\varepsilon)v$ 的范数很小。这样的向量 $v$ 是 $G(\\varepsilon)$ 的近似零空间元素。\n\n$G(\\varepsilon)$ 的奇异值分解 (SVD) 为 $G(\\varepsilon) = U \\Sigma V^\\mathsf{T}$，其中 $V = [v_1, v_2]$ 的列是右奇异向量，并构成模型空间 $\\mathbb{R}^2$ 的一个标准正交基。奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge 0$ 是 $\\Sigma$ 的对角线元素。$G(\\varepsilon)$ 在这些基向量上的作用是 $G(\\varepsilon)v_i = \\sigma_i u_i$，其中 $u_i$ 是对应的左奇异向量。\n\n在奇异向量 $v_i$ 方向上的二阶方向导数为：\n$$\nD_{v_i}^2 J = 2 \\|G(\\varepsilon)v_i\\|_2^2 = 2 \\|\\sigma_i u_i\\|_2^2 = 2 \\sigma_i^2 \\|u_i\\|_2^2 = 2 \\sigma_i^2\n$$\n因此，最浅谷的方向是对应于最小奇异值 $\\sigma_2$ 的右奇异向量 $v_2$。平坦度与 $\\sigma_2^2$ 成正比。\n\n我们来分析当 $\\varepsilon \\to 0$ 时 $G(\\varepsilon)$ 的情况。列向量为 $g_1 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}^\\mathsf{T}$ 和 $g_2 = \\begin{pmatrix} 1 & 1+\\varepsilon & \\varepsilon \\end{pmatrix}^\\mathsf{T}$。当 $\\varepsilon \\to 0$ 时，$g_2 \\to g_1$，因此列向量变得线性相关。近似零空间中的一个向量将满足 $m_1 g_1 + m_2 g_2 \\approx 0$，这对于小的 $\\varepsilon$ 意味着 $m_1 \\approx -m_2$。这表明该方向接近 $(1, -1)^\\mathsf{T}$。\n\n我们测试单位向量 $v = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\end{pmatrix}^\\mathsf{T}$。注意这个向量与 $\\varepsilon$ 无关。\n$$\nG(\\varepsilon)v = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\varepsilon \\\\ 0 & \\varepsilon \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1-1 \\\\ 1-(1+\\varepsilon) \\\\ 0-\\varepsilon \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0 \\\\ -\\varepsilon \\\\ -\\varepsilon \\end{pmatrix}\n$$\n平方范数为：\n$$\n\\|G(\\varepsilon)v\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 (0^2 + (-\\varepsilon)^2 + (-\\varepsilon)^2) = \\frac{1}{2} (2\\varepsilon^2) = \\varepsilon^2\n$$\n此方向上的二阶方向导数为 $D_v^2 J = 2\\varepsilon^2$。这是 $O(\\varepsilon^2)$ 阶的，符合要求。所以，我们可以选择 $v_{\\varepsilon} = v = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\end{pmatrix}^\\mathsf{T}$。\n\n现在，我们来构造这对模型。OLS 极小值点 $\\hat{m}(\\varepsilon)$ 满足 $\\nabla J(\\hat{m}) = 0$。考虑 $J(m)$ 在 $\\hat{m}(\\varepsilon)$ 附近的泰勒展开：\n$$\nJ(m) = J(\\hat{m}) + \\nabla J(\\hat{m})^\\mathsf{T} (m-\\hat{m}) + \\frac{1}{2} (m-\\hat{m})^\\mathsf{T} (\\nabla^2 J) (m-\\hat{m}) + \\dots\n$$\n由于黑塞矩阵是常数，该展开是精确的。当 $\\nabla J(\\hat{m}) = 0$ 时：\n$$\nJ(m) - J(\\hat{m}) = \\frac{1}{2} (m-\\hat{m})^\\mathsf{T} (2 G^\\mathsf{T} G) (m-\\hat{m}) = (m-\\hat{m})^\\mathsf{T} G^\\mathsf{T} G (m-\\hat{m}) = \\|G(m-\\hat{m})\\|_2^2\n$$\n我们选择模型对为 $m_{\\star} = \\hat{m}(\\varepsilon)$ 和扰动后的模型 $m_{\\star} + \\delta v_{\\varepsilon}$，其中 $\\delta$ 是一个与 $\\varepsilon$ 无关的标量常数，且 $v_{\\varepsilon} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\end{pmatrix}^\\mathsf{T}$。\n它们的失配差异为：\n$$\nJ(m_{\\star} + \\delta v_{\\varepsilon}) - J(m_{\\star}) = J(\\hat{m} + \\delta v_{\\varepsilon}) - J(\\hat{m}) = \\|G(\\delta v_{\\varepsilon})\\|_2^2 = \\delta^2 \\|G v_{\\varepsilon}\\|_2^2\n$$\n使用我们之前计算的结果 $\\|G v_{\\varepsilon}\\|_2^2 = \\varepsilon^2$，我们得到：\n$$\nJ(m_{\\star} + \\delta v_{\\varepsilon}) - J(m_{\\star}) = \\delta^2 \\varepsilon^2\n$$\n这个差异是 $O(\\varepsilon^2)$ 阶的。因此，对于一个大小为 $\\delta$ 的固定模型扰动，当 $\\varepsilon \\to 0$ 时，失配的增加是无穷小的。这证实了沿着方向 $v_{\\varepsilon}$ 存在一个由近似等效的极小值点构成的长而浅的谷。\n\n#### 第二部分：方差放大因子\n\n无量纲方差放大因子为 $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{m}(\\varepsilon)]\\big)$。我们必须首先推导 OLS 估计量的协方差矩阵 $\\operatorname{Cov}[\\hat{m}(\\varepsilon)]$。为简洁起见，我们将 $\\hat{m}(\\varepsilon)$ 写为 $\\hat{m}$，将 $G(\\varepsilon)$ 写为 $G$。\n\nOLS 估计量由正规方程给出：\n$$\n\\hat{m} = (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} d\n$$\n我们将数据模型 $d = G m_{\\mathrm{true}} + n$ 代入：\n$$\n\\hat{m} = (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} (G m_{\\mathrm{true}} + n) = (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} G m_{\\mathrm{true}} + (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n = m_{\\mathrm{true}} + (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n\n$$\n估计量的期望为：\n$$\nE[\\hat{m}] = E[m_{\\mathrm{true}} + (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n] = m_{\\mathrm{true}} + (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} E[n]\n$$\n由于噪声是零均值的 ($E[n]=0$)，估计量是无偏的：$E[\\hat{m}] = m_{\\mathrm{true}}$。\n\n估计量的协方差矩阵定义为 $\\operatorname{Cov}[\\hat{m}] = E[(\\hat{m} - E[\\hat{m}])(\\hat{m} - E[\\hat{m}])^\\mathsf{T}]$。\n使用上面的表达式：\n$$\n\\hat{m} - E[\\hat{m}] = (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n\n$$\n所以，\n$$\n\\operatorname{Cov}[\\hat{m}] = E\\Big[ \\big( (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n \\big) \\big( (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n \\big)^\\mathsf{T} \\Big] = E\\Big[ (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} n n^\\mathsf{T} G (G^\\mathsf{T} G)^{-\\mathsf{T}} \\Big]\n$$\n由于 $G$ 是关于期望的常数矩阵，并且 $(G^\\mathsf{T} G)^\\mathsf{T} = G^\\mathsf{T} G$，我们可以写成：\n$$\n\\operatorname{Cov}[\\hat{m}] = (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} E[n n^\\mathsf{T}] G (G^\\mathsf{T} G)^{-1}\n$$\n项 $E[n n^\\mathsf{T}]$ 是噪声的协方差矩阵 $\\operatorname{Cov}[n]$。已知 $n \\sim \\mathcal{N}(0, \\sigma^2 I)$，所以 $\\operatorname{Cov}[n] = \\sigma^2 I$。\n$$\n\\operatorname{Cov}[\\hat{m}] = (G^\\mathsf{T} G)^{-1} G^\\mathsf{T} (\\sigma^2 I) G (G^\\mathsf{T} G)^{-1} = \\sigma^2 (G^\\mathsf{T} G)^{-1} (G^\\mathsf{T} G) (G^\\mathsf{T} G)^{-1} = \\sigma^2 (G^\\mathsf{T} G)^{-1}\n$$\n现在我们可以表示放大因子 $A(\\varepsilon)$：\n$$\nA(\\varepsilon) = \\sigma^{-2} \\operatorname{tr}(\\operatorname{Cov}[\\hat{m}]) = \\sigma^{-2} \\operatorname{tr}(\\sigma^2 (G^\\mathsf{T} G)^{-1}) = \\operatorname{tr}((G^\\mathsf{T} G)^{-1})\n$$\n为了计算这个，我们首先求矩阵 $G^\\mathsf{T} G$：\n$$\nG(\\varepsilon)^\\mathsf{T} G(\\varepsilon) = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1+\\varepsilon & \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\varepsilon \\\\ 0 & \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 1+1 & 1+(1+\\varepsilon) \\\\ 1+(1+\\varepsilon) & 1+(1+\\varepsilon)^2+\\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 2 & 2+\\varepsilon \\\\ 2+\\varepsilon & 2+2\\varepsilon+2\\varepsilon^2 \\end{pmatrix}\n$$\n对于一个一般的可逆 $2 \\times 2$ 矩阵 $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$，其逆矩阵为 $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$。逆矩阵的迹为 $\\operatorname{tr}(M^{-1}) = \\frac{a+d}{\\det(M)} = \\frac{\\operatorname{tr}(M)}{\\det(M)}$。\n将此性质应用于 $M = G^\\mathsf{T} G$：\n$$\n\\operatorname{tr}((G^\\mathsf{T} G)^{-1}) = \\frac{\\operatorname{tr}(G^\\mathsf{T} G)}{\\det(G^\\mathsf{T} G)}\n$$\n我们计算 $G^\\mathsf{T} G$ 的迹和行列式：\n$$\n\\operatorname{tr}(G^\\mathsf{T} G) = 2 + (2+2\\varepsilon+2\\varepsilon^2) = 4+2\\varepsilon+2\\varepsilon^2\n$$\n$$\n\\det(G^\\mathsf{T} G) = 2(2+2\\varepsilon+2\\varepsilon^2) - (2+\\varepsilon)^2 = (4+4\\varepsilon+4\\varepsilon^2) - (4+4\\varepsilon+\\varepsilon^2) = 3\\varepsilon^2\n$$\n将这些代入 $A(\\varepsilon)$ 的表达式中：\n$$\nA(\\varepsilon) = \\frac{4+2\\varepsilon+2\\varepsilon^2}{3\\varepsilon^2}\n$$\n这就是无量纲方差放大因子的精确闭式解析表达式。",
            "answer": "$$\n\\boxed{\\frac{4+2\\varepsilon+2\\varepsilon^{2}}{3\\varepsilon^{2}}}\n$$"
        },
        {
            "introduction": "在地球物理建模中，我们经常面临一个核心的权衡：模型的偏差（bias）与方差（variance）之间的平衡。一个看似合理的举动，比如移除那些具有高杠杆值和高噪声的“可疑”数据点，确实可能降低解的方差，使其更稳定。然而，这也可能引入系统性的偏差，使模型偏离真实的地球物理结构。本练习将通过一个蒙特卡洛模拟，让您在一个实际的走时层析成像场景中，量化并探索这种微妙而关键的偏差-方差权衡 。",
            "id": "3606791",
            "problem": "考虑在分层介质中的一维垂直走时实验，其中从地表到深度 $z$ 处接收器的走时等于沿路径的慢度线积分。其基本依据是走时的路径积分定义，即\n$$\nT(z) \\equiv \\int_{0}^{z} s(\\zeta)\\, d\\zeta,\n$$\n其中 $s(\\zeta)$ 是慢度场，单位为秒/米。在计算地球物理学中，标准的做法是将未知慢度按层离散化为分段常数，从而得到一个线性模型 $t \\approx A m$，其中 $t \\in \\mathbb{R}^{R}$ 是 $R$ 个接收器测得的走时向量，$A \\in \\mathbb{R}^{R \\times L}$ 是每层的路径长度设计矩阵，$m \\in \\mathbb{R}^{L}$ 是层慢度向量（单位为秒/米）。对于超定系统，最小二乘解是 $\\|A m - t\\|_2^2$ 的最小化子，当 $A^{\\mathsf{T}} A$ 可逆时，得到估计值\n$$\n\\hat{m} = (A^{\\mathsf{T}} A)^{-1} A^{\\mathsf{T}} t。\n$$\n杠杆分数（leverage scores）量化了拟合值对观测值的敏感度，其计算公式为\n$$\n\\ell_i = a_i^{\\mathsf{T}} (A^{\\mathsf{T}} A)^{-1} a_i,\n$$\n其中 $a_i^{\\mathsf{T}}$ 是 $A$ 的第 $i$ 行。\n\n您将为一个从地表到接收器的垂直直线路径和分层参数化的垂直实验构建 $A$ 矩阵。假设有 $L$ 个层，其厚度为 $h_j$（$j=1,\\dots,L$），第 $j$ 层的顶部深度为 $z_{j}^{\\mathrm{top}}$，底部深度为 $z_{j}^{\\mathrm{bot}}$。对于深度为 $z_i$ 的接收器，穿过第 $j$ 层的路径长度为 $\\ell_{ij}^{\\mathrm{path}} = \\max\\big(0, \\min(z_i, z_{j}^{\\mathrm{bot}}) - z_{j}^{\\mathrm{top}}\\big)$，因此 $A_{ij} = \\ell_{ij}^{\\mathrm{path}}$。设连续慢度为\n$$\ns(z) = s_0 + g z,\n$$\n其中 $s_0$ 和 $g$ 是常数，因此无噪声走时为\n$$\nT_i = T(z_i) = \\int_0^{z_i} (s_0 + g \\zeta) \\, d\\zeta = s_0 z_i + \\tfrac{1}{2} g z_i^2。\n$$\n将层平均目标慢度（分段常数粗尺度真实值）定义为\n$$\nm^{\\mathrm{coarse}}_j = \\frac{1}{h_j} \\int_{z_{j}^{\\mathrm{top}}}^{z_{j}^{\\mathrm{bot}}} s(z) \\, dz = s_0 + g \\frac{z_{j}^{\\mathrm{top}} + z_{j}^{\\mathrm{bot}}}{2}。\n$$\n您将对每个数据点模拟加性噪声，其异方差性方差与杠杆值相关联，\n$$\n\\varepsilon_i \\sim \\mathcal{N}\\Big(0, \\sigma^2 \\big(1 + \\gamma \\, \\ell_i\\big)\\Big),\n$$\n以反映在实际走时数据集中，高杠杆值拾取点具有更高的拾取不确定性。将含噪数据构建为 $t_i = T_i + \\varepsilon_i$，并使用普通最小二乘法对完整数据集和移除了最高杠杆值拾取点的数据集进行 $\\hat{m}$ 的估计。\n\n您的任务：\n- 计算 $A$ 矩阵所有行的杠杆分数 $\\ell_i$。\n- 使用指定的移除比例 $p$ 识别高杠杆值的拾取点：移除按 $\\ell_i$ 排序的前 $\\lceil p R \\rceil$ 行。\n- 对完整数据集和移除了高杠杆值拾取点的数据集，进行 $K$ 次数据模拟和最小二乘反演的蒙特卡洛重复实验。\n- 通过分量样本方差之和来量化估计参数的方差，\n$$\n\\mathrm{VarSum} = \\sum_{j=1}^{L} \\widehat{\\mathrm{Var}}(\\hat{m}_j),\n$$\n单位为 $(\\mathrm{s}/\\mathrm{m})^2$。\n- 通过平均估计器误差的欧几里得范数来量化相对于粗尺度真实值的偏差，\n$$\n\\mathrm{BiasNorm} = \\left\\| \\mathbb{E}[\\hat{m}] - m^{\\mathrm{coarse}} \\right\\|_2,\n$$\n单位为 $\\mathrm{s}/\\mathrm{m}$，通过重复实验的样本均值来近似。\n- 通过模拟证明，移除高杠杆值的拾取点会降低 $\\mathrm{VarSum}$，同时增加 $\\mathrm{BiasNorm}$。\n\n使用以下科学上合理的配置和测试套件。所有量必须用国际单位制（SI）表示。慢度单位为秒/米，深度和厚度单位为米，走时单位为秒。角度不出现。测试套件指定了三种具有不同噪声尺度和移除比例的情况：\n- 所有情况共享的几何和物理参数：\n  - 层：$L = 5$，厚度 $h = [200, 200, 200, 200, 200]$ 米。\n  - 接收器深度：$R = 25$ 个接收器，深度 $z_i$ 从 $40$ 米到 $1000$ 米均匀分布。\n  - 连续慢度：$s_0 = 5 \\times 10^{-4}$ 秒/米，$g = 1 \\times 10^{-7}$ 秒/米^2。\n  - 重复次数：$K = 400$。\n- 情况 1（理想情况）：噪声尺度 $\\sigma = 1 \\times 10^{-4}$ 秒，异方差因子 $\\gamma = 20$，移除比例 $p = 0.20$。\n- 情况 2（边界条件：最小移除）：噪声尺度 $\\sigma = 5 \\times 10^{-5}$ 秒，异方差因子 $\\gamma = 10$，移除比例 $p = 0.04$。\n- 情况 3（边缘案例：强异方差性）：噪声尺度 $\\sigma = 2 \\times 10^{-4}$ 秒，异方差因子 $\\gamma = 30$，移除比例 $p = 0.30$。\n\n对于每种情况，计算：\n- $\\mathrm{VarSum}_{\\mathrm{full}}$ 和 $\\mathrm{VarSum}_{\\mathrm{removed}}$，单位为 $(\\mathrm{s}/\\mathrm{m})^2$。\n- $\\mathrm{BiasNorm}_{\\mathrm{full}}$ 和 $\\mathrm{BiasNorm}_{\\mathrm{removed}}$，单位为 $\\mathrm{s}/\\mathrm{m}$。\n- 移除的拾取点数量 $N_{\\mathrm{removed}} = \\lceil p R \\rceil$。\n\n最终输出格式：\n您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表，其中每个元素对应一个测试用例，并且本身是一个列表\n$$\n[\\mathrm{VarSum}_{\\mathrm{full}}, \\mathrm{VarSum}_{\\mathrm{removed}}, \\mathrm{BiasNorm}_{\\mathrm{full}}, \\mathrm{BiasNorm}_{\\mathrm{removed}}, N_{\\mathrm{removed}}].\n$$\n例如，输出将类似于\n$[[x_1,x_2,x_3,x_4,n_1],[y_1,y_2,y_3,y_4,n_2],[z_1,z_2,z_3,z_4,n_3]]$，\n其中所有 $x_k, y_k, z_k$ 均为浮点数，$n_k$ 为整数，并以指定的 SI 单位报告。不应打印任何附加文本。",
            "solution": "我们从连续变化介质中走时的路径积分定律出发，该定律指出，到深度 $z$ 处接收器的走时 $T(z)$ 等于沿路径的慢度积分，$T(z) = \\int_{0}^{z} s(\\zeta) \\, d\\zeta$，其中 $s(\\zeta)$ 是慢度，单位为秒/米。对于具有分段常数慢度 $m_j$（在层 $j$ 中）的分层离散化，以及从地表出发的垂直路径，到接收器 $i$ 的测量走时被建模为线性组合 $t_i \\approx \\sum_{j=1}^{L} A_{ij} m_j$，其中 $A_{ij}$ 是到接收器 $i$ 的路径在层 $j$ 中的路径长度。由于路径是垂直且笔直的，$A_{ij}$ 是区间 $[0, z_i]$ 与区间 $[z_j^{\\mathrm{top}}, z_j^{\\mathrm{bot}}]$ 的交集长度，即 $A_{ij} = \\max\\big(0, \\min(z_i, z_{j}^{\\mathrm{bot}}) - z_{j}^{\\mathrm{top}}\\big)$。将所有接收器叠加起来，得到 $t \\in \\mathbb{R}^{R}$ 和 $A \\in \\mathbb{R}^{R \\times L}$。\n\n最小二乘解通过最小化二次目标函数 $J(m) = \\|A m - t\\|_2^2$ 得到，其对 $m$ 的梯度为 $\\nabla J(m) = 2 A^{\\mathsf{T}} (A m - t)$。将梯度设为零，得到正规方程 $A^{\\mathsf{T}} A \\, \\hat{m} = A^{\\mathsf{T}} t$。当 $A^{\\mathsf{T}} A$ 可逆时，唯一的最小化子是 $\\hat{m} = (A^{\\mathsf{T}} A)^{-1} A^{\\mathsf{T}} t$。当线性模型被正确指定且噪声均值为零时，该估计器是无偏的。然而，在计算地球物理学中，由于对连续场的粗糙参数化，模型设定误差很常见。在我们的设置中，连续慢度为 $s(z) = s_0 + g z$，因此无噪声走时为 $T(z) = s_0 z + \\frac{1}{2} g z^2$。粗尺度的层平均慢度（物理目标）是 $m^{\\mathrm{coarse}}_j = \\frac{1}{h_j} \\int_{z_j^{\\mathrm{top}}}^{z_j^{\\mathrm{bot}}} (s_0 + g z) \\, dz = s_0 + g \\, \\frac{z_j^{\\mathrm{top}} + z_j^{\\mathrm{bot}}}{2}$。\n\n因为连续走时 $T$ 并不精确地位于由每层常数 $m$ 构成的 $A m$ 的列空间中（除非 $g=0$），所以在无噪声数据下的最小二乘解会返回一个伪真参数 $m^{\\star}$，它解的是 $A^{\\mathsf{T}} A \\, m^{\\star} = A^{\\mathsf{T}} T$，这个 $m^{\\star}$ 通常与 $m^{\\mathrm{coarse}}$ 不同。这种差异是可归因于离散化和接收器采样的结构性偏差。每个观测值的杠杆分数（leverage score）量化了它对拟合值的影响，对于 $A$ 的第 $i$ 行 $a_i^{\\mathsf{T}}$，其定义为 $\\ell_i = a_i^{\\mathsf{T}} (A^{\\mathsf{T}} A)^{-1} a_i$。投影（帽子）矩阵 $H = A (A^{\\mathsf{T}} A)^{-1} A^{\\mathsf{T}}$ 的对角线包含了这些分数，高杠杆行是那些 $\\ell_i$ 相对较大的行。在走时反演中，较深的接收器路径更长，可能具有高杠杆值，因为它们的 $a_i$ 向量具有较大的模，并且在各层之间具有不同的方向贡献。\n\n为了将杠杆值与不确定性联系起来，我们考虑异方差拾取噪声，其方差随杠杆值增加而增加：$\\varepsilon_i \\sim \\mathcal{N}\\big(0, \\sigma^2 (1 + \\gamma \\ell_i)\\big)$。当 $T$ 平均上精确等于 $A m$ 时，普通最小二乘法下的估计器对线性模型仍然是无偏的，但存在结构性设定误差（对于任何 $m$，$T \\neq A m$）时，期望估计器等于 $m^{\\star}$，而不是 $m^{\\mathrm{coarse}}$。此外，重尾或异方差噪声会扩大 $\\hat{m}$ 的采样方差，特别是由于高杠杆观测值。因此，移除高杠杆行可以减少 $\\hat{m}$ 的方差，因为它消除了那些同时具有大杠杆值和大噪声方差的观测值；然而，移除操作会降低设计矩阵 $A$ 的多样性，通常会通过改变 $A^{\\mathsf{T}} T$ 和 $A^{\\mathsf{T}} A$ 的方式恶化结构性偏差，从而降低对更深层的敏感度。这种权衡是超定最小二乘法在模型设定误差下的经典方差-偏差博弈。\n\n算法步骤：\n1. 为 $L=5$ 层构建层深度，每层厚度 $h_j = 200$ 米，因此 $z_1^{\\mathrm{top}} = 0$，$z_1^{\\mathrm{bot}} = 200$，$z_2^{\\mathrm{top}} = 200$，以此类推，直到 $z_5^{\\mathrm{bot}} = 1000$ 米。构建 $R=25$ 个接收器深度 $z_i$，从 $40$ 米到 $1000$ 米均匀分布。\n2. 构建矩阵 $A$，其元素为 $A_{ij} = \\max\\big(0, \\min(z_i, z_j^{\\mathrm{bot}}) - z_j^{\\mathrm{top}}\\big)$。\n3. 计算所有行的杠杆分数 $\\ell_i = a_i^{\\mathsf{T}} (A^{\\mathsf{T}} A)^{-1} a_i$。\n4. 对每个测试用例的参数 $(\\sigma, \\gamma, p)$，确定 $N_{\\mathrm{removed}} = \\lceil p R \\rceil$，并移除具有最大 $\\ell_i$ 的行，以创建 $A_{\\mathrm{removed}}$ 和相应的索引集。\n5. 对于 $K=400$ 次蒙特卡洛重复实验，为完整数据集和移除行的数据集生成含噪数据 $t = T + \\varepsilon$（在完整集上重用相同的 $\\varepsilon$ 实现，并为移除集对其进行子选择）。拟合 $\\hat{m}_{\\mathrm{full}} = (A^{\\mathsf{T}} A)^{-1} A^{\\mathsf{T}} t$ 和 $\\hat{m}_{\\mathrm{removed}} = (A_{\\mathrm{removed}}^{\\mathsf{T}} A_{\\mathrm{removed}})^{-1} A_{\\mathrm{removed}}^{\\mathsf{T}} t_{\\mathrm{removed}}$。\n6. 计算各分量在重复实验中的样本方差，将其相加得到 $\\mathrm{VarSum}_{\\mathrm{full}}$ 和 $\\mathrm{VarSum}_{\\mathrm{removed}}$，单位为 $(\\mathrm{s}/\\mathrm{m})^2$。\n7. 计算完整集和移除集在重复实验中 $\\hat{m}$ 的样本均值，并计算其与 $m^{\\mathrm{coarse}}$ 之差的欧几里得范数，以获得 $\\mathrm{BiasNorm}_{\\mathrm{full}}$ 和 $\\mathrm{BiasNorm}_{\\mathrm{removed}}$，单位为 $\\mathrm{s}/\\mathrm{m}$。\n\n测试套件详情：\n- 共享的几何和物理参数：\n  - $L = 5$， $h = [200, 200, 200, 200, 200]$ 米。\n  - $R = 25$， $z_i$ 从 $40$ 米到 $1000$ 米均匀分布。\n  - $s_0 = 5 \\times 10^{-4}$ 秒/米，$g = 1 \\times 10^{-7}$ 秒/米^2。\n  - $K = 400$ 次重复实验。\n- 情况 1：$\\sigma = 1 \\times 10^{-4}$ 秒, $\\gamma = 20$, $p = 0.20$。\n- 情况 2：$\\sigma = 5 \\times 10^{-5}$ 秒, $\\gamma = 10$, $p = 0.04$。\n- 情况 3：$\\sigma = 2 \\times 10^{-4}$ 秒, $\\gamma = 30$, $p = 0.30$。\n\n最终输出规范：\n您的程序应生成单行输出，格式为\n$[[\\mathrm{VarSum}_{\\mathrm{full}},\\mathrm{VarSum}_{\\mathrm{removed}},\\mathrm{BiasNorm}_{\\mathrm{full}},\\mathrm{BiasNorm}_{\\mathrm{removed}},N_{\\mathrm{removed}}],\\dots]$\n，依次对应情况 1、情况 2、情况 3。所有值必须使用上述 SI 单位，方差和偏差报告为浮点数，计数 $N_{\\mathrm{removed}}$ 报告为整数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_layers(thicknesses):\n    \"\"\"Return arrays of layer top and bottom depths.\"\"\"\n    z_top = np.cumsum([0] + thicknesses[:-1])\n    z_bot = np.cumsum(thicknesses)\n    return z_top.astype(float), z_bot.astype(float)\n\ndef build_design_matrix(z_receivers, z_top, z_bot):\n    \"\"\"\n    Build A with A[i, j] = intersection length of [0, z_i] with [z_top[j], z_bot[j]].\n    \"\"\"\n    R = len(z_receivers)\n    L = len(z_top)\n    A = np.zeros((R, L), dtype=float)\n    for i, z in enumerate(z_receivers):\n        # intersection length: max(0, min(z, z_bot[j]) - z_top[j])\n        A[i, :] = np.maximum(0.0, np.minimum(z, z_bot) - z_top)\n    return A\n\ndef true_travel_times(z_receivers, s0, g):\n    \"\"\"Compute T_i = s0*z_i + 0.5*g*z_i^2.\"\"\"\n    z = np.asarray(z_receivers, dtype=float)\n    return s0 * z + 0.5 * g * z**2\n\ndef coarse_layer_slowness(z_top, z_bot, s0, g):\n    \"\"\"Compute m_coarse_j = s0 + g * (z_top_j + z_bot_j)/2.\"\"\"\n    return s0 + g * (z_top + z_bot) / 2.0\n\ndef leverage_scores(A):\n    \"\"\"Compute leverage scores l_i = a_i^T (A^T A)^{-1} a_i.\"\"\"\n    ATA = A.T @ A\n    # Use a robust inverse; ATA should be SPD; fallback to pseudo-inverse for numerical robustness\n    ATA_inv = np.linalg.pinv(ATA)\n    # row-wise leverage: diag(A @ ATA_inv @ A^T)\n    H = A @ ATA_inv @ A.T\n    return np.diag(H)\n\ndef ls_estimate(A, t):\n    \"\"\"Ordinary least-squares estimate (A^T A)^{-1} A^T t.\"\"\"\n    ATA = A.T @ A\n    ATt = A.T @ t\n    # Use pseudo-inverse for robustness\n    m_hat = np.linalg.pinv(ATA) @ ATt\n    return m_hat\n\ndef simulate_case(A, T_true, leverages, m_coarse, sigma, gamma, frac_remove, K, rng):\n    \"\"\"\n    Simulate K replicates for full and removed datasets.\n    Returns VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed.\n    \"\"\"\n    R = A.shape[0]\n    L = A.shape[1]\n    # Determine removal set: top ceil(frac_remove * R) by leverage\n    N_removed = int(np.ceil(frac_remove * R))\n    order = np.argsort(-leverages)  # descending by leverage\n    remove_idx = order[:N_removed]\n    keep_mask = np.ones(R, dtype=bool)\n    keep_mask[remove_idx] = False\n    A_removed = A[keep_mask, :]\n\n    # Preallocate arrays to store estimates\n    m_full_all = np.zeros((K, L), dtype=float)\n    m_removed_all = np.zeros((K, L), dtype=float)\n\n    # Noise std per observation (heteroscedastic)\n    std_i = sigma * np.sqrt(1.0 + gamma * leverages)\n\n    for k in range(K):\n        eps = rng.normal(loc=0.0, scale=std_i, size=R)\n        t_noisy = T_true + eps\n        # Full estimate\n        m_full = ls_estimate(A, t_noisy)\n        m_full_all[k, :] = m_full\n        # Removed estimate\n        t_removed = t_noisy[keep_mask]\n        m_removed = ls_estimate(A_removed, t_removed)\n        m_removed_all[k, :] = m_removed\n\n    # Variance sums across components\n    var_full = m_full_all.var(axis=0, ddof=1)  # sample variance per component\n    var_removed = m_removed_all.var(axis=0, ddof=1)\n    VarSum_full = float(var_full.sum())\n    VarSum_removed = float(var_removed.sum())\n\n    # Bias norms: norm of mean(m_hat) - m_coarse\n    mean_full = m_full_all.mean(axis=0)\n    mean_removed = m_removed_all.mean(axis=0)\n    BiasNorm_full = float(np.linalg.norm(mean_full - m_coarse))\n    BiasNorm_removed = float(np.linalg.norm(mean_removed - m_coarse))\n\n    return VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed\n\ndef solve():\n    # Geometry and physics shared across cases\n    thicknesses = [200, 200, 200, 200, 200]  # meters\n    L = len(thicknesses)\n    z_top, z_bot = build_layers(thicknesses)\n    # Receiver depths: 25 receivers from 40 m to 1000 m\n    R = 25\n    z_receivers = np.linspace(40.0, 1000.0, R)\n    # Continuous slowness parameters\n    s0 = 5e-4  # s/m\n    g = 1e-7   # s/m^2\n    # True travel-times and coarse slowness\n    T_true = true_travel_times(z_receivers, s0, g)\n    m_coarse = coarse_layer_slowness(z_top, z_bot, s0, g)\n    # Design matrix\n    A = build_design_matrix(z_receivers, z_top, z_bot)\n    # Leverage scores\n    leverages = leverage_scores(A)\n\n    # Replicates\n    K = 400\n    rng = np.random.default_rng(seed=42)\n\n    # Test cases: (sigma, gamma, frac_remove)\n    test_cases = [\n        (1e-4, 20.0, 0.20),  # Case 1: happy path\n        (5e-5, 10.0, 0.04),  # Case 2: boundary minimal removal\n        (2e-4, 30.0, 0.30),  # Case 3: strong heteroscedasticity\n    ]\n\n    results = []\n    for sigma, gamma, frac_remove in test_cases:\n        VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed = simulate_case(\n            A, T_true, leverages, m_coarse, sigma, gamma, frac_remove, K, rng\n        )\n        results.append([VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```"
        }
    ]
}