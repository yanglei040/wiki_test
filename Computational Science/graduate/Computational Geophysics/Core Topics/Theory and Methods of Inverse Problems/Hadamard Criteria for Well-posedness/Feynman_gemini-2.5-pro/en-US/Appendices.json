{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational analysis of how random noise propagates through an ill-posed linear system. By calculating the expected squared norm of the reconstruction error, you will quantitatively link the singular values of the forward operator to the stability of the solution . This practice is crucial for understanding how the geometry of a problem, encoded in its singular value spectrum, dictates its sensitivity to measurement uncertainty.",
            "id": "3602516",
            "problem": "In a linear inverse problem arising in computational geophysics, consider a compact forward operator represented in finite dimensions by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ that maps a model vector $x \\in \\mathbb{R}^{n}$ to data $d \\in \\mathbb{R}^{m}$ through $d = A x$. The Moore–Penrose pseudoinverse $A^{\\dagger} \\in \\mathbb{R}^{n \\times m}$ is used to reconstruct the model from noisy data by $x_{\\text{rec}} = A^{\\dagger} d$, with reconstruction error $e = x_{\\text{rec}} - x = A^{\\dagger} \\eta$ when the measurement noise is additive, $d = A x + \\eta$. Assume the noise $\\eta \\in \\mathbb{R}^{m}$ is zero-mean Additive White Gaussian Noise (AWGN) with covariance $\\mathbb{E}[\\eta \\eta^{\\top}] = \\sigma_{\\eta}^{2} I_{m}$, where $I_{m}$ is the $m \\times m$ identity and $\\sigma_{\\eta}^{2} > 0$ is the noise variance. Denote by the Singular Value Decomposition (SVD) the factorization $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains the singular values on its diagonal. Let the nonzero singular values be $\\{\\sigma_{k}\\}_{k=1}^{r}$, with $r = \\operatorname{rank}(A)$.\n\nStarting from the definitions of Hadamard’s criteria for well-posedness (existence, uniqueness, and continuous dependence on data), explain how small singular values of $A$ affect the continuous dependence criterion through noise amplification in the reconstruction $A^{\\dagger} \\eta$. Using only the fundamental properties of orthogonal projectors, the Singular Value Decomposition, and the second-moment characterization of AWGN, derive an explicit expression for the expected squared reconstruction error $\\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2}$ in terms of $\\{\\sigma_{k}\\}_{k=1}^{r}$ and $\\sigma_{\\eta}^{2}$. Your final answer must be a single closed-form analytic expression. Do not include units. No rounding is required.",
            "solution": "The problem asks for an explanation of how small singular values of a matrix $A$ in a linear inverse problem affect the stability of the solution, as defined by Hadamard's criteria for well-posedness. It then requires the derivation of an explicit expression for the expected squared reconstruction error due to additive noise.\n\nA problem is considered well-posed in the sense of Hadamard if it satisfies three criteria:\n$1$. **Existence**: A solution exists for any admissible data.\n$2$. **Uniqueness**: The solution is unique.\n$3$. **Stability**: The solution depends continuously on the data. This means that small changes in the data should result in only small changes in the solution.\n\nIn the context of the given linear inverse problem, the data is the vector $d \\in \\mathbb{R}^{m}$ and the solution is the reconstructed model vector $x_{\\text{rec}} \\in \\mathbb{R}^{n}$. The reconstruction is given by the application of the Moore-Penrose pseudoinverse operator, $x_{\\text{rec}} = A^{\\dagger} d$. When the data is corrupted by additive noise $\\eta$, such that $d = Ax + \\eta$, the reconstructed model is $x_{\\text{rec}} = A^{\\dagger}(Ax + \\eta)$. The true model is $x$. A part of the reconstruction error is due to the noise, given by $e_{\\eta} = A^{\\dagger}\\eta$. The stability criterion concerns how the magnitude of this error, $\\|e_{\\eta}\\|$, behaves in relation to the magnitude of the data perturbation, $\\|\\eta\\|$. For a stable problem, a \"small\" $\\|\\eta\\|$ should guarantee a \"small\" $\\|e_{\\eta}\\|$.\n\nThe Singular Value Decomposition (SVD) of the matrix $A$ is $A = U \\Sigma V^{\\top}$. The Moore-Penrose pseudoinverse is then given by $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$. The singular values of $A^{\\dagger}$ are the reciprocals of the non-zero singular values of $A$. Specifically, if $\\{\\sigma_k\\}_{k=1}^{r}$ are the non-zero singular values of $A$, then $\\{1/\\sigma_k\\}_{k=1}^{r}$ are the non-zero singular values of $A^{\\dagger}$. If $A$ has one or more very small singular values ($\\sigma_k \\approx 0$), then $A^{\\dagger}$ will have corresponding very large singular values ($1/\\sigma_k \\gg 1$). This implies that $A^{\\dagger}$ can greatly amplify certain components of any vector it acts upon. In our case, the noise vector $\\eta$ is amplified. If the noise has components aligned with the singular vectors corresponding to large singular values of $A^{\\dagger}$, the reconstruction error $e_{\\eta}$ can be very large, even if the overall noise magnitude $\\|\\eta\\|$ is small. This phenomenon indicates a violation of the stability criterion and is characteristic of an ill-posed problem.\n\nWe can quantify this noise amplification by calculating the expected squared Euclidean norm of the reconstruction error, $\\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2}$.\n\nThe squared norm of the error is given by $\\|A^{\\dagger} \\eta\\|^{2}$. We substitute the SVD of the pseudoinverse:\n$$ \\|A^{\\dagger} \\eta\\|^{2} = \\|V \\Sigma^{\\dagger} U^{\\top} \\eta\\|^{2} $$\nSince $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix, it represents an isometry, meaning it preserves the Euclidean norm of any vector it multiplies. That is, for any vector $z \\in \\mathbb{R}^n$, we have $\\|Vz\\|^2 = (Vz)^{\\top}(Vz) = z^{\\top}V^{\\top}Vz = z^{\\top}I_n z = \\|z\\|^2$. Applying this property, we get:\n$$ \\|A^{\\dagger} \\eta\\|^{2} = \\|\\Sigma^{\\dagger} U^{\\top} \\eta\\|^{2} $$\nLet us define a transformed noise vector $\\eta' \\in \\mathbb{R}^{m}$ as $\\eta' = U^{\\top} \\eta$. Since $U \\in \\mathbb{R}^{m \\times m}$ is also orthogonal, this transformation is a rotation of the noise vector $\\eta$. We can determine the statistical properties of $\\eta'$. The mean is $\\mathbb{E}[\\eta'] = \\mathbb{E}[U^{\\top}\\eta] = U^{\\top}\\mathbb{E}[\\eta] = U^{\\top}0 = 0$. The covariance matrix of $\\eta'$ is:\n$$ \\mathbb{E}[\\eta' (\\eta')^{\\top}] = \\mathbb{E}[(U^{\\top}\\eta)(U^{\\top}\\eta)^{\\top}] = \\mathbb{E}[U^{\\top}\\eta \\eta^{\\top}U] = U^{\\top}\\mathbb{E}[\\eta \\eta^{\\top}]U $$\nUsing the given noise covariance $\\mathbb{E}[\\eta \\eta^{\\top}] = \\sigma_{\\eta}^{2} I_{m}$, we have:\n$$ \\mathbb{E}[\\eta' (\\eta')^{\\top}] = U^{\\top}(\\sigma_{\\eta}^{2} I_{m})U = \\sigma_{\\eta}^{2} U^{\\top}U = \\sigma_{\\eta}^{2} I_{m} $$\nThis demonstrates that the transformed noise vector $\\eta'$ has the same statistical properties as the original noise $\\eta$: its components are uncorrelated, have zero mean, and each has a variance of $\\sigma_{\\eta}^{2}$. Specifically, for any component $k$, $\\mathbb{E}[(\\eta'_{k})^{2}] = \\sigma_{\\eta}^{2}$.\n\nNow we can express the squared error norm in terms of $\\eta'$:\n$$ \\|A^{\\dagger} \\eta\\|^{2} = \\|\\Sigma^{\\dagger} \\eta'\\|^{2} $$\nThe matrix $\\Sigma^{\\dagger} \\in \\mathbb{R}^{n \\times m}$ has its non-zero entries on the main diagonal, which are $(\\Sigma^{\\dagger})_{kk} = 1/\\sigma_k$ for $k=1, \\dots, r$, where $r=\\operatorname{rank}(A)$. Let's write out the squared norm:\n$$ \\|\\Sigma^{\\dagger} \\eta'\\|^{2} = \\sum_{i=1}^{n} \\left( (\\Sigma^{\\dagger} \\eta')_i \\right)^2 = \\sum_{i=1}^{n} \\left( \\sum_{j=1}^{m} (\\Sigma^{\\dagger})_{ij} \\eta'_{j} \\right)^2 $$\nDue to the structure of $\\Sigma^{\\dagger}$, the $i$-th component of the vector $\\Sigma^{\\dagger}\\eta'$ is $(\\Sigma^{\\dagger}\\eta')_i = (1/\\sigma_i)\\eta'_i$ for $i \\le r$, and $0$ for $i > r$. Therefore, the sum becomes:\n$$ \\|\\Sigma^{\\dagger} \\eta'\\|^{2} = \\sum_{k=1}^{r} \\left( \\frac{1}{\\sigma_k} \\eta'_{k} \\right)^2 = \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} (\\eta'_{k})^2 $$\nNow we take the expectation of this expression. By the linearity of expectation:\n$$ \\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2} = \\mathbb{E}\\left[ \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} (\\eta'_{k})^2 \\right] = \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} \\mathbb{E}[(\\eta'_{k})^2] $$\nAs we established, the variance of each component of the transformed noise is $\\mathbb{E}[(\\eta'_{k})^2] = \\sigma_{\\eta}^{2}$. Substituting this into the sum:\n$$ \\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2} = \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} \\sigma_{\\eta}^{2} $$\nThis leads to the final expression for the expected squared reconstruction error:\n$$ \\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2} = \\sigma_{\\eta}^{2} \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} $$\nThis result quantitatively confirms that the expected error is directly proportional to the noise variance $\\sigma_{\\eta}^2$ and to the sum of the squared reciprocals of the non-zero singular values of $A$. Small singular values $\\sigma_k$ lead to large terms $1/\\sigma_k^2$ in the sum, causing a large expected error and thus demonstrating the instability of the inversion.",
            "answer": "$$\\boxed{\\sigma_{\\eta}^{2} \\sum_{k=1}^{r} \\frac{1}{\\sigma_{k}^{2}}}$$"
        },
        {
            "introduction": "While the previous exercise explored instability from a statistical viewpoint, this practice challenges you to consider the worst-case scenario. You will design a specific data perturbation that maximally exploits the system's instability, revealing that the largest possible error amplification is governed directly by the smallest singular value . This provides a deterministic and highly intuitive understanding of the stability criterion in Hadamard's definition of well-posedness.",
            "id": "3602542",
            "problem": "In a linearized seismic travel-time tomography setting, consider a discretized, full-rank linear forward operator $A \\in \\mathbb{R}^{N \\times N}$ that maps model perturbations $m \\in \\mathbb{R}^{N}$ to data perturbations $f \\in \\mathbb{R}^{N}$ via $f = A m$. Suppose the inversion recovers $m$ from $f$ by applying $A^{-1}$, so that for a small data perturbation $\\delta f$ the induced model perturbation is $\\delta m = A^{-1} \\delta f$. Let $A$ admit a singular value decomposition (SVD) of the form $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{N \\times N}$ are orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{N})$ with $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{N} > 0$. Assume the Euclidean norm $\\|\\cdot\\|_{2}$ on both data and model spaces.\n\nFrom first principles and using only core definitions, design a worst-case data perturbation $\\delta f$ of fixed nonzero norm $0 < \\|\\delta f\\|_{2} < \\infty$ such that the induced model perturbation $\\delta m = A^{-1} \\delta f$ is aligned with the right singular vector $v_{N}$ and the amplification ratio\n$$\n\\frac{\\|A^{-1} \\delta f\\|_{2}}{\\|\\delta f\\|_{2}}\n$$\nis maximized over all possible data perturbations $\\delta f \\neq 0$. Then, quantify the instability of the inversion by providing the resulting maximal amplification factor explicitly in terms of the smallest singular value $\\sigma_{N}$. Express your final answer as a single closed-form analytic expression. No rounding is required, and no physical units are to be used.",
            "solution": "The mapping $A : \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ is linear and invertible, and by the singular value decomposition (SVD) we have $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\dots, \\sigma_{N})$ with $\\sigma_{1} \\geq \\dots \\geq \\sigma_{N} > 0$. The inverse exists and is given by\n$$\nA^{-1} = V \\Sigma^{-1} U^{\\top},\n$$\nwhere $\\Sigma^{-1} = \\operatorname{diag}(\\sigma_{1}^{-1}, \\dots, \\sigma_{N}^{-1})$.\n\nWe seek to maximize the ratio\n$$\n\\mathcal{R}(\\delta f) = \\frac{\\|A^{-1} \\delta f\\|_{2}}{\\|\\delta f\\|_{2}}\n$$\nover $\\delta f \\neq 0$, subject to the requirement that the induced model perturbation is aligned with $v_{N}$. The alignment condition $\\delta m = A^{-1} \\delta f \\parallel v_{N}$ means that $A^{-1} \\delta f$ must be proportional to $v_{N}$. Using the decomposition of $\\delta f$ in the orthonormal basis formed by the columns of $U$, write\n$$\n\\delta f = \\sum_{i=1}^{N} \\alpha_{i} u_{i},\n$$\nwhere $u_{i}$ denotes the $i$-th column of $U$ and $\\alpha_{i} \\in \\mathbb{R}$. Applying $A^{-1}$,\n$$\nA^{-1} \\delta f = V \\Sigma^{-1} U^{\\top} \\left( \\sum_{i=1}^{N} \\alpha_{i} u_{i} \\right)\n= V \\Sigma^{-1} \\left( \\sum_{i=1}^{N} \\alpha_{i} e_{i} \\right)\n= \\sum_{i=1}^{N} \\alpha_{i} \\sigma_{i}^{-1} v_{i},\n$$\nwhere $e_{i}$ is the $i$-th standard basis vector and $v_{i}$ is the $i$-th column of $V$.\n\nFor $A^{-1} \\delta f$ to be aligned with $v_{N}$, all components along $v_{i}$ for $i \\neq N$ must vanish, which requires $\\alpha_{i} = 0$ for $i \\neq N$. Thus, the only admissible $\\delta f$ that produces $\\delta m \\parallel v_{N}$ is of the form\n$$\n\\delta f = \\alpha_{N} u_{N}, \\quad \\alpha_{N} \\neq 0.\n$$\nUnder this choice,\n$$\nA^{-1} \\delta f = \\alpha_{N} \\sigma_{N}^{-1} v_{N}.\n$$\nTherefore,\n$$\n\\|A^{-1} \\delta f\\|_{2} = |\\alpha_{N}| \\sigma_{N}^{-1}, \\quad \\|\\delta f\\|_{2} = |\\alpha_{N}|.\n$$\nThe amplification ratio simplifies to\n$$\n\\mathcal{R}(\\delta f) = \\frac{|\\alpha_{N}| \\sigma_{N}^{-1}}{|\\alpha_{N}|} = \\sigma_{N}^{-1}.\n$$\n\nTo confirm maximality, consider any arbitrary nonzero $\\delta f = \\sum_{i=1}^{N} \\alpha_{i} u_{i}$. Then\n$$\n\\|A^{-1} \\delta f\\|_{2}^{2} = \\left\\| \\sum_{i=1}^{N} \\alpha_{i} \\sigma_{i}^{-1} v_{i} \\right\\|_{2}^{2}\n= \\sum_{i=1}^{N} \\alpha_{i}^{2} \\sigma_{i}^{-1^{2}},\n$$\nby orthonormality of the $v_{i}$. Also,\n$$\n\\|\\delta f\\|_{2}^{2} = \\left\\| \\sum_{i=1}^{N} \\alpha_{i} u_{i} \\right\\|_{2}^{2} = \\sum_{i=1}^{N} \\alpha_{i}^{2},\n$$\nby orthonormality of the $u_{i}$. Hence\n$$\n\\mathcal{R}(\\delta f)^{2} = \\frac{\\sum_{i=1}^{N} \\alpha_{i}^{2} \\sigma_{i}^{-2}}{\\sum_{i=1}^{N} \\alpha_{i}^{2}}\n\\leq \\max_{1 \\leq i \\leq N} \\sigma_{i}^{-2} = \\sigma_{N}^{-2},\n$$\nwith equality if and only if all the weight is on the smallest singular value, that is, $\\alpha_{i} = 0$ for $i \\neq N$. This establishes that the maximal amplification ratio over all nonzero $\\delta f$ equals $\\sigma_{N}^{-1}$, and it is achieved by choosing\n$$\n\\delta f \\parallel u_{N} \\quad \\text{so that} \\quad A^{-1} \\delta f \\parallel v_{N}.\n$$\n\nIn terms of Hadamard's criteria for well-posedness, continuous dependence on data requires that the inverse mapping be bounded. The operator norm of $A^{-1}$ in the Euclidean norm is\n$$\n\\|A^{-1}\\|_{2} = \\sup_{\\delta f \\neq 0} \\frac{\\|A^{-1} \\delta f\\|_{2}}{\\|\\delta f\\|_{2}} = \\sigma_{N}^{-1}.\n$$\nIf $\\sigma_{N}$ is small, the instability $\\sigma_{N}^{-1}$ is large, indicating lack of continuous dependence and, therefore, ill-posedness in the sense of Hadamard for the inversion even though existence and uniqueness hold in the noiseless, full-rank case.",
            "answer": "$$\\boxed{\\sigma_{N}^{-1}}$$"
        },
        {
            "introduction": "This final practice moves from theoretical analysis to practical application in computational geophysics. You will engage in a complete seismic tomography experiment design, where the objective is to optimize the placement of receivers to maximize the stability of the resulting inverse problem . By directly manipulating the forward operator's singular value spectrum through experimental geometry, you will bridge the gap between the abstract Hadamard criteria and tangible strategies for acquiring high-quality geophysical data.",
            "id": "3602530",
            "problem": "You will implement and run a complete program that, for a set of straight-ray seismic tomography experiments, designs optimal receiver layouts to maximize the minimum singular value of the forward operator and uses this to assess stability under Hadamard's criteria for well-posedness. The setting is linearized, two-dimensional, constant slowness per cell, straight-ray travel time tomography. Your task must be accomplished from first principles without relying on any pre-specified shortcut formulas.\n\nThe fundamental base is as follows. In a two-dimensional rectangular domain $[0,1]\\times[0,1]$ measured in meters, discretized into a uniform grid of $N_x\\times N_y$ rectangular cells, assume wave slowness is constant within each cell and denote the vector of cell slownesses by $m\\in\\mathbb{R}^{N_x N_y}$. For any source location $s\\in\\mathbb{R}^2$ and receiver location $r\\in\\mathbb{R}^2$ both strictly interior to the domain, the first-arrival straight-ray travel time $t$ satisfies the line-integral identity $t=\\int_{\\text{ray}(s,r)} u(\\mathbf{x})\\,\\mathrm{d}\\ell$, where $u(\\mathbf{x})$ is slowness and $\\mathrm{d}\\ell$ is the differential arclength element. Under uniform-cell discretization, this reduces to a linear map $t = G m$, where $G\\in\\mathbb{R}^{M\\times N}$, $M$ is the number of source-receiver rays and $N=N_xN_y$, and each row of $G$ contains the sequence of path lengths inside each cell along the straight line segment from source to receiver. The Singular Value Decomposition (SVD) of $G$ yields singular values $\\sigma_1\\ge \\sigma_2\\ge \\cdots \\ge \\sigma_{\\min(M,N)}\\ge 0$. In the linear setting, Hadamard's criteria for well-posedness consist of existence, uniqueness, and continuous dependence of the solution. Assuming noiseless, consistent data, existence is guaranteed. Uniqueness of $m$ is equivalent to $G$ having full column rank (that is, $\\sigma_{\\min}>0$ when $M\\ge N$), and continuous dependence with respect to the $\\ell_2$-norm is equivalent to the existence of a finite Lipschitz bound on the inverse, which is controlled by $\\sigma_{\\min}$ via the inequality $\\|m\\|_2 \\le \\sigma_{\\min}^{-1}\\|Gm\\|_2$ when $M\\ge N$ and $\\sigma_{\\min}>0$.\n\nYou must:\n- Construct $G$ by computing, for each ray, the exact lengths of intersection of the straight line segment between source and receiver with each grid cell. Use only geometric line-grid intersection and arclength, with no shortcuts.\n- From a finite set of candidate receiver locations, select exactly $R$ receivers to maximize $\\sigma_{\\min}$ of the resulting matrix $G$ formed from all rays generated by the fixed sources and the chosen receivers.\n- Treat the case $M<N$ as having effective smallest singular value $\\sigma_{\\min}^{\\mathrm{eff}}=0$ for the purpose of uniqueness and stability assessment, because the nullspace is nontrivial in that case.\n- Use a numerical tolerance $\\varepsilon=10^{-9}$: interpret any singular value less than $\\varepsilon$ as zero in the assessment.\n\nYour program must solve the following test suite of three cases. All coordinates are in meters. Angles, if any appear internally, must be in radians. Singular values must be reported in meters. For each case, you will output two values: the optimal minimum singular value $\\sigma_{\\min}^{\\star}$ in meters rounded to six decimal places, and a stability indicator $b\\in\\{0,1\\}$, where $b=1$ if and only if $M\\ge N$ and $\\sigma_{\\min}^{\\star}>\\varepsilon$.\n\nDefinitions common to all cases:\n- Domain: $[0,1]\\times[0,1]$ meters.\n- For a grid $N_x\\times N_y$, the number of model parameters is $N=N_xN_y$.\n- For a given set of sources $\\{s_k\\}_{k=1}^{S}$ and a chosen set of receivers $\\{r_\\ell\\}_{\\ell=1}^{R}$, the number of rays is $M=S\\cdot R$.\n- The forward matrix $G\\in\\mathbb{R}^{M\\times N}$ has entries $G_{p,q}$ equal to the length in meters that ray $p$ travels within cell $q$, indexed in row-major order (first index $x$ then $y$).\n\nCase $1$ (happy path geometry):\n- Grid: $N_x=2$, $N_y=2$.\n- Sources $S=2$: $s_1=(0.05,0.05)$, $s_2=(0.05,0.95)$.\n- Candidate receivers (choose exactly $R=4$ out of $6$):\n  $\\{(0.95,0.10), (0.95,0.28), (0.95,0.46), (0.95,0.64), (0.95,0.82), (0.95,0.90)\\}$.\n- Expectation: $M=8$, $N=4$, so $M\\ge N$; a well-chosen layout should yield $\\sigma_{\\min}^{\\star}>0$.\n\nCase $2$ (underdetermined boundary case):\n- Grid: $N_x=3$, $N_y=3$.\n- Sources $S=2$: $s_1=(0.05,0.05)$, $s_2=(0.05,0.95)$.\n- Candidate receivers (choose exactly $R=3$ out of $5$):\n  $\\{(0.95,0.20), (0.95,0.50), (0.95,0.80), (0.80,0.95), (0.80,0.05)\\}$.\n- Expectation: $M=6$, $N=9$, so $M<N$ and thus $\\sigma_{\\min}^{\\mathrm{eff}}=0$ regardless of layout; uniqueness fails.\n\nCase $3$ (degenerate illumination edge case):\n- Grid: $N_x=2$, $N_y=2$.\n- Sources $S=2$: $s_1=(0.10,0.25)$, $s_2=(0.10,0.75)$.\n- Candidate receivers (choose exactly $R=4$ out of $5$):\n  $\\{(0.49,0.20), (0.49,0.40), (0.49,0.60), (0.49,0.80), (0.49,0.50)\\}$.\n- Expectation: all rays remain in the left half of the domain; the columns of $G$ corresponding to right-half cells are exactly zero, so rank deficiency occurs and $\\sigma_{\\min}^{\\star}=0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the six entries in the order:\n  $[\\sigma_{\\min,1}^{\\star}, b_1, \\sigma_{\\min,2}^{\\star}, b_2, \\sigma_{\\min,3}^{\\star}, b_3]$,\n  where each $\\sigma_{\\min,i}^{\\star}$ is expressed in meters and rounded to six decimal places, and each $b_i$ is an integer ($0$ or $1$).",
            "solution": "This problem requires a multi-step computational approach to find an optimal experimental design that maximizes stability. The solution involves a combinatorial search over receiver layouts, geometric construction of the forward operator for each layout, and a stability analysis using Singular Value Decomposition (SVD).\n\n### Methodology\n\n**1. Optimal Receiver Layout: Combinatorial Search**\n\nThe core of the problem is to select the best subset of $R$ receivers from a list of candidates to maximize the minimum singular value, $\\sigma_{\\min}$, of the forward matrix $G$. Since the number of candidates and choices is small, this optimization is solved by an exhaustive search. We generate all possible combinations of $R$ receivers from the candidate set. For each combination, we construct the corresponding matrix $G$ and compute its $\\sigma_{\\min}$. The combination yielding the largest $\\sigma_{\\min}$ is the optimal one.\n\n**2. Forward Operator Construction: Ray-Grid Intersection**\n\nThe matrix $G \\in \\mathbb{R}^{M \\times N}$ is constructed from first principles. Each entry $G_{ij}$ represents the path length of the $i$-th ray within the $j$-th grid cell. This is achieved using a ray-tracing algorithm based on the principles of a Digital Differential Analyzer (DDA), similar to the Amanatides and Woo algorithm. For each ray from a source to a receiver, the algorithm iteratively steps through the grid cells intersected by the ray, calculating the exact path length within each one by finding the parametric intersection points with cell boundaries. These lengths are then stored in the appropriate row of the $G$ matrix.\n\n**3. Stability Analysis via SVD**\n\nThe stability and uniqueness of the inverse problem $Gm = t$ are assessed using the SVD of $G$.\n- **Uniqueness & Stability:** Both uniqueness and stability are tied to the smallest singular value, $\\sigma_{\\min}$. A non-zero $\\sigma_{\\min}$ (above a numerical tolerance) is required for full column rank, ensuring a unique solution. A larger $\\sigma_{\\min}$ implies better stability (a smaller condition number), meaning the inversion is less sensitive to noise.\n- **Evaluation:** As per the problem, if the number of rays $M$ is less than the number of model cells $N$, the system is underdetermined, guaranteeing a non-unique solution. In this case, the effective minimum singular value is treated as zero ($\\sigma_{\\min}^{\\mathrm{eff}}=0$), and the stability indicator $b$ is 0. Otherwise ($M \\ge N$), the stability indicator $b$ is 1 if and only if the numerically computed $\\sigma_{\\min}$ is greater than the tolerance $\\varepsilon = 10^{-9}$.\n\nThe following Python code implements this entire process, iterating through all receiver combinations for each case to find the optimal $\\sigma_{\\min}^{\\star}$ and its corresponding stability indicator $b$.\n\n```python\nimport numpy as np\nfrom itertools import combinations\n\ndef get_ray_path_lengths(start_pos, end_pos, grid_dims, domain_dims=(0.0, 1.0, 0.0, 1.0)):\n    \"\"\"\n    Computes the lengths of a straight ray's path through each cell of a 2D grid.\n    This implementation uses a 2D DDA-like algorithm (Amanatides & Woo style).\n    This function adheres to the \"first principles\" requirement.\n\n    Args:\n        start_pos (tuple): (x, y) coordinates of the ray's start point.\n        end_pos (tuple): (x, y) coordinates of the ray's end point.\n        grid_dims (tuple): (Nx, Ny), number of cells in x and y directions.\n        domain_dims (tuple): (x_min, x_max, y_min, y_max) of the domain.\n\n    Returns:\n        numpy.ndarray: A 1D array of size Nx*Ny with path lengths in each cell.\n                       Cell indexing is row-major, with x as the first index:\n                       cell_index = ix * Ny + iy.\n    \"\"\"\n    x_min, x_max, y_min, y_max = domain_dims\n    nx, ny = grid_dims\n    x1, y1 = start_pos\n    x2, y2 = end_pos\n\n    dx = (x_max - x_min) / nx\n    dy = (y_max - y_min) / ny\n    \n    path_lengths = np.zeros(nx * ny)\n\n    vx = x2 - x1\n    vy = y2 - y1\n    \n    ray_length = np.sqrt(vx**2 + vy**2)\n    if ray_length == 0:\n        return path_lengths\n\n    # Initial cell index. Clamp to be within the grid.\n    ix = int((x1 - x_min) / dx)\n    iy = int((y1 - y_min) / dy)\n    if ix >= nx: ix = nx - 1\n    if iy >= ny: iy = ny - 1\n\n    step_x = 1 if vx > 0 else -1 if vx < 0 else 0\n    step_y = 1 if vy > 0 else -1 if vy < 0 else 0\n\n    # Parametric distance to next grid line\n    if vx != 0:\n        next_vert_boundary = x_min + (ix + (1 if step_x > 0 else 0)) * dx\n        t_max_x = (next_vert_boundary - x1) / vx\n        t_delta_x = dx / abs(vx)\n    else:\n        t_max_x = np.inf\n        t_delta_x = np.inf\n\n    if vy != 0:\n        next_horz_boundary = y_min + (iy + (1 if step_y > 0 else 0)) * dy\n        t_max_y = (next_horz_boundary - y1) / vy\n        t_delta_y = dy / abs(vy)\n    else:\n        t_max_y = np.inf\n        t_delta_y = np.inf\n\n    t_current = 0.0\n    while t_current < 1.0:\n        cell_index = ix * ny + iy\n        \n        if not (0 <= ix < nx and 0 <= iy < ny):\n            break\n\n        t_exit = min(t_max_x, t_max_y, 1.0)\n        \n        segment_len = ray_length * (t_exit - t_current)\n        path_lengths[cell_index] += segment_len\n        \n        t_current = t_exit\n        \n        if t_current >= 1.0:\n            break\n\n        # Move to the next cell. A small epsilon prevents floating point errors\n        # at corners, ensuring consistent diagonal steps.\n        if t_max_x < t_max_y - 1e-12:\n            ix += step_x\n            t_max_x += t_delta_x\n        elif t_max_y < t_max_x - 1e-12:\n            iy += step_y\n            t_max_y += t_delta_y\n        else:\n            ix += step_x\n            iy += step_y\n            t_max_x += t_delta_x\n            t_max_y += t_delta_y\n\n    return path_lengths\n\ndef analyze_geometry(grid_dims, sources, receiver_candidates, R_chosen):\n    \"\"\"\n    Finds the optimal receiver layout to maximize the minimum singular value.\n    \"\"\"\n    nx, ny = grid_dims\n    N = nx * ny\n    S = len(sources)\n    M = S * R_chosen\n    \n    epsilon = 1e-9\n\n    if M < N:\n        return 0.0, 0\n\n    max_sigma_min = -1.0\n    \n    receiver_combinations = combinations(receiver_candidates, R_chosen)\n\n    for rec_set in receiver_combinations:\n        G = np.zeros((M, N))\n        ray_idx = 0\n        for source in sources:\n            for receiver in rec_set:\n                G[ray_idx, :] = get_ray_path_lengths(source, receiver, grid_dims)\n                ray_idx += 1\n        \n        _, s, _ = np.linalg.svd(G, full_matrices=False)\n        sigma_min = s[-1] if len(s) > 0 else 0.0\n\n        if sigma_min > max_sigma_min:\n            max_sigma_min = sigma_min\n\n    is_stable = 1 if M >= N and max_sigma_min > epsilon else 0\n    \n    return max_sigma_min, is_stable\n\ndef solve():\n    test_cases = [\n        {\n            \"id\": 1,\n            \"grid\": (2, 2),\n            \"sources\": [(0.05, 0.05), (0.05, 0.95)],\n            \"candidates\": [(0.95,0.10), (0.95,0.28), (0.95,0.46), (0.95,0.64), (0.95,0.82), (0.95,0.90)],\n            \"R\": 4\n        },\n        {\n            \"id\": 2,\n            \"grid\": (3, 3),\n            \"sources\": [(0.05, 0.05), (0.05, 0.95)],\n            \"candidates\": [(0.95,0.20), (0.95,0.50), (0.95,0.80), (0.80,0.95), (0.80,0.05)],\n            \"R\": 3\n        },\n        {\n            \"id\": 3,\n            \"grid\": (2, 2),\n            \"sources\": [(0.10, 0.25), (0.10, 0.75)],\n            \"candidates\": [(0.49,0.20), (0.49,0.40), (0.49,0.60), (0.49,0.80), (0.49,0.50)],\n            \"R\": 4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma_min_star, b = analyze_geometry(\n            grid_dims=case[\"grid\"],\n            sources=case[\"sources\"],\n            receiver_candidates=case[\"candidates\"],\n            R_chosen=case[\"R\"]\n        )\n        \n        results.append(f\"{sigma_min_star:.6f}\")\n        results.append(str(b))\n\n    print(f\"[{','.join(results)}]\")\n\n# To generate the answer, one would run solve()\n# solve()\n```",
            "answer": "[0.443903,1,0.000000,0,0.000000,0]"
        }
    ]
}