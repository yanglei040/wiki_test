{
    "hands_on_practices": [
        {
            "introduction": "To build a foundational understanding of instability, we begin by exploring a worst-case scenario. This exercise challenges you to design a specific data perturbation that maximizes the error in the reconstructed model, revealing the mechanism of instability in its most extreme form. By doing so, you will establish a direct, quantitative link between the smallest singular value of the forward operator and the maximum possible error amplification, a concept central to Hadamard's stability criterion .",
            "id": "3602542",
            "problem": "In a linearized seismic travel-time tomography setting, consider a discretized, full-rank linear forward operator $A \\in \\mathbb{R}^{N \\times N}$ that maps model perturbations $m \\in \\mathbb{R}^{N}$ to data perturbations $f \\in \\mathbb{R}^{N}$ via $f = A m$. Suppose the inversion recovers $m$ from $f$ by applying $A^{-1}$, so that for a small data perturbation $\\delta f$ the induced model perturbation is $\\delta m = A^{-1} \\delta f$. Let $A$ admit a Singular Value Decomposition (SVD) of the form $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{N \\times N}$ are orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{N})$ with $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{N} > 0$. Assume the Euclidean norm $\\|\\cdot\\|_{2}$ on both data and model spaces.\n\nFrom first principles and using only core definitions, design a worst-case data perturbation $\\delta f$ of fixed nonzero norm $0  \\|\\delta f\\|_{2}  \\infty$ such that the induced model perturbation $\\delta m = A^{-1} \\delta f$ is aligned with the right singular vector $v_{N}$ and the amplification ratio\n$$\n\\frac{\\|A^{-1} \\delta f\\|_{2}}{\\|\\delta f\\|_{2}}\n$$\nis maximized over all possible data perturbations $\\delta f \\neq 0$. Then, quantify the instability of the inversion by providing the resulting maximal amplification factor explicitly in terms of the smallest singular value $\\sigma_{N}$. Express your final answer as a single closed-form analytic expression. No rounding is required, and no physical units are to be used.",
            "solution": "The mapping $A : \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ is linear and invertible, and by the Singular Value Decomposition (SVD) we have $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\dots, \\sigma_{N})$ with $\\sigma_{1} \\geq \\dots \\geq \\sigma_{N} > 0$. The inverse exists and is given by\n$$\nA^{-1} = V \\Sigma^{-1} U^{\\top},\n$$\nwhere $\\Sigma^{-1} = \\operatorname{diag}(\\sigma_{1}^{-1}, \\dots, \\sigma_{N}^{-1})$.\n\nWe seek to maximize the ratio\n$$\n\\mathcal{R}(\\delta f) = \\frac{\\|A^{-1} \\delta f\\|_{2}}{\\|\\delta f\\|_{2}}\n$$\nover $\\delta f \\neq 0$, subject to the requirement that the induced model perturbation is aligned with $v_{N}$. The alignment condition $\\delta m = A^{-1} \\delta f \\parallel v_{N}$ means that $A^{-1} \\delta f$ must be proportional to $v_{N}$. Using the decomposition of $\\delta f$ in the orthonormal basis formed by the columns of $U$, write\n$$\n\\delta f = \\sum_{i=1}^{N} \\alpha_{i} u_{i},\n$$\nwhere $u_{i}$ denotes the $i$-th column of $U$ and $\\alpha_{i} \\in \\mathbb{R}$. Applying $A^{-1}$,\n$$\nA^{-1} \\delta f = V \\Sigma^{-1} U^{\\top} \\left( \\sum_{i=1}^{N} \\alpha_{i} u_{i} \\right)\n= V \\Sigma^{-1} \\left( \\sum_{i=1}^{N} \\alpha_{i} e_{i} \\right)\n= \\sum_{i=1}^{N} \\alpha_{i} \\sigma_{i}^{-1} v_{i},\n$$\nwhere $e_{i}$ is the $i$-th standard basis vector and $v_{i}$ is the $i$-th column of $V$.\n\nFor $A^{-1} \\delta f$ to be aligned with $v_{N}$, all components along $v_{i}$ for $i \\neq N$ must vanish, which requires $\\alpha_{i} = 0$ for $i \\neq N$. Thus, the only admissible $\\delta f$ that produces $\\delta m \\parallel v_{N}$ is of the form\n$$\n\\delta f = \\alpha_{N} u_{N}, \\quad \\alpha_{N} \\neq 0.\n$$\nUnder this choice,\n$$\nA^{-1} \\delta f = \\alpha_{N} \\sigma_{N}^{-1} v_{N}.\n$$\nTherefore,\n$$\n\\|A^{-1} \\delta f\\|_{2} = |\\alpha_{N}| \\sigma_{N}^{-1}, \\quad \\|\\delta f\\|_{2} = |\\alpha_{N}|.\n$$\nThe amplification ratio simplifies to\n$$\n\\mathcal{R}(\\delta f) = \\frac{|\\alpha_{N}| \\sigma_{N}^{-1}}{|\\alpha_{N}|} = \\sigma_{N}^{-1}.\n$$\n\nTo confirm maximality, consider any arbitrary nonzero $\\delta f = \\sum_{i=1}^{N} \\alpha_{i} u_{i}$. Then\n$$\n\\|A^{-1} \\delta f\\|_{2}^{2} = \\left\\| \\sum_{i=1}^{N} \\alpha_{i} \\sigma_{i}^{-1} v_{i} \\right\\|_{2}^{2}\n= \\sum_{i=1}^{N} \\alpha_{i}^{2} \\sigma_{i}^{-2},\n$$\nby orthonormality of the $v_{i}$. Also,\n$$\n\\|\\delta f\\|_{2}^{2} = \\left\\| \\sum_{i=1}^{N} \\alpha_{i} u_{i} \\right\\|_{2}^{2} = \\sum_{i=1}^{N} \\alpha_{i}^{2},\n$$\nby orthonormality of the $u_{i}$. Hence\n$$\n\\mathcal{R}(\\delta f)^{2} = \\frac{\\sum_{i=1}^{N} \\alpha_{i}^{2} \\sigma_{i}^{-2}}{\\sum_{i=1}^{N} \\alpha_{i}^{2}}\n\\leq \\max_{1 \\leq i \\leq N} \\sigma_{i}^{-2} = \\sigma_{N}^{-2},\n$$\nwith equality if and only if all the weight is on the smallest singular value, that is, $\\alpha_{i} = 0$ for $i \\neq N$. This establishes that the maximal amplification ratio over all nonzero $\\delta f$ equals $\\sigma_{N}^{-1}$, and it is achieved by choosing\n$$\n\\delta f \\parallel u_{N} \\quad \\text{so that} \\quad A^{-1} \\delta f \\parallel v_{N}.\n$$\n\nIn terms of Hadamard's criteria for well-posedness, continuous dependence on data requires that the inverse mapping be bounded. The operator norm of $A^{-1}$ in the Euclidean norm is\n$$\n\\|A^{-1}\\|_{2} = \\sup_{\\delta f \\neq 0} \\frac{\\|A^{-1} \\delta f\\|_{2}}{\\|\\delta f\\|_{2}} = \\sigma_{N}^{-1}.\n$$\nIf $\\sigma_{N}$ is small, the instability $\\sigma_{N}^{-1}$ is large, indicating lack of continuous dependence and, therefore, ill-posedness in the sense of Hadamard for the inversion even though existence and uniqueness hold in the noiseless, full-rank case.",
            "answer": "$$\\boxed{\\sigma_{N}^{-1}}$$"
        },
        {
            "introduction": "While worst-case analysis is insightful, real-world data is corrupted by random noise, not adversarially designed perturbations. This practice moves from the deterministic worst-case to a more realistic statistical framework, asking you to quantify the expected reconstruction error in the presence of additive white noise. Completing this derivation will demonstrate how the entire spectrum of singular values contributes to the solution's variance, providing a statistical measure of instability for ill-posed problems .",
            "id": "3602516",
            "problem": "In a linear inverse problem arising in computational geophysics, consider a compact forward operator represented in finite dimensions by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ that maps a model vector $x \\in \\mathbb{R}^{n}$ to data $d \\in \\mathbb{R}^{m}$ through $d = A x$. The Moore–Penrose pseudoinverse $A^{\\dagger} \\in \\mathbb{R}^{n \\times m}$ is used to reconstruct the model from noisy data by $x_{\\text{rec}} = A^{\\dagger} d$, with reconstruction error $e = x_{\\text{rec}} - x = A^{\\dagger} \\eta$ when the measurement noise is additive, $d = A x + \\eta$. Assume the noise $\\eta \\in \\mathbb{R}^{m}$ is zero-mean Additive White Gaussian Noise (AWGN) with covariance $\\mathbb{E}[\\eta \\eta^{\\top}] = \\sigma_{\\eta}^{2} I_{m}$, where $I_{m}$ is the $m \\times m$ identity and $\\sigma_{\\eta}^{2} > 0$ is the noise variance. Let the Singular Value Decomposition (SVD) of $A$ be the factorization $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains the singular values on its diagonal. Let the nonzero singular values be $\\{\\sigma_{k}\\}_{k=1}^{r}$, with $r = \\operatorname{rank}(A)$.\n\nStarting from the definitions of Hadamard’s criteria for well-posedness (existence, uniqueness, and continuous dependence on data), explain how small singular values of $A$ affect the continuous dependence criterion through noise amplification in the reconstruction $A^{\\dagger} \\eta$. Using only the fundamental properties of orthogonal projectors, the Singular Value Decomposition, and the second-moment characterization of AWGN, derive an explicit expression for the expected squared reconstruction error $\\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2}$ in terms of $\\{\\sigma_{k}\\}_{k=1}^{r}$ and $\\sigma_{\\eta}^{2}$. Your final answer must be a single closed-form analytic expression. Do not include units. No rounding is required.",
            "solution": "The problem asks for an explanation of how small singular values of a matrix $A$ in a linear inverse problem affect the stability of the solution, as defined by Hadamard's criteria for well-posedness. It then requires the derivation of an explicit expression for the expected squared reconstruction error due to additive noise.\n\nA problem is considered well-posed in the sense of Hadamard if it satisfies three criteria:\n1.  **Existence**: A solution exists for any admissible data.\n2.  **Uniqueness**: The solution is unique.\n3.  **Stability**: The solution depends continuously on the data. This means that small changes in the data should result in only small changes in the solution.\n\nIn the context of the given linear inverse problem, the data is the vector $d \\in \\mathbb{R}^{m}$ and the solution is the reconstructed model vector $x_{\\text{rec}} \\in \\mathbb{R}^{n}$. The reconstruction is given by the application of the Moore-Penrose pseudoinverse operator, $x_{\\text{rec}} = A^{\\dagger} d$. When the data is corrupted by additive noise $\\eta$, such that $d = Ax + \\eta$, the reconstructed model is $x_{\\textrec} = A^{\\dagger}(Ax + \\eta)$. The true model is $x$. A part of the reconstruction error is due to the noise, given by $e_{\\eta} = A^{\\dagger}\\eta$. The stability criterion concerns how the magnitude of this error, $\\|e_{\\eta}\\|$, behaves in relation to the magnitude of the data perturbation, $\\|\\eta\\|$. For a stable problem, a \"small\" $\\|\\eta\\|$ should guarantee a \"small\" $\\|e_{\\eta}\\|$.\n\nThe SVD of the matrix $A$ is $A = U \\Sigma V^{\\top}$. The Moore-Penrose pseudoinverse is then given by $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$. The singular values of $A^{\\dagger}$ are the reciprocals of the non-zero singular values of $A$. Specifically, if $\\{\\sigma_k\\}_{k=1}^{r}$ are the non-zero singular values of $A$, then $\\{1/\\sigma_k\\}_{k=1}^{r}$ are the non-zero singular values of $A^{\\dagger}$. If $A$ has one or more very small singular values ($\\sigma_k \\approx 0$), then $A^{\\dagger}$ will have corresponding very large singular values ($1/\\sigma_k \\gg 1$). This implies that $A^{\\dagger}$ can greatly amplify certain components of any vector it acts upon. In our case, the noise vector $\\eta$ is amplified. If the noise has components aligned with the singular vectors corresponding to large singular values of $A^{\\dagger}$, the reconstruction error $e_{\\eta}$ can be very large, even if the overall noise magnitude $\\|\\eta\\|$ is small. This phenomenon indicates a violation of the stability criterion and is characteristic of an ill-posed problem.\n\nWe can quantify this noise amplification by calculating the expected squared Euclidean norm of the reconstruction error, $\\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2}$.\n\nThe squared norm of the error is given by $\\|A^{\\dagger} \\eta\\|^{2}$. We substitute the SVD of the pseudoinverse:\n$$ \\|A^{\\dagger} \\eta\\|^{2} = \\|V \\Sigma^{\\dagger} U^{\\top} \\eta\\|^{2} $$\nSince $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix, it represents an isometry, meaning it preserves the Euclidean norm of any vector it multiplies. That is, for any vector $z \\in \\mathbb{R}^n$, we have $\\|Vz\\|^2 = (Vz)^{\\top}(Vz) = z^{\\top}V^{\\top}Vz = z^{\\top}I_n z = \\|z\\|^2$. Applying this property, we get:\n$$ \\|A^{\\dagger} \\eta\\|^{2} = \\|\\Sigma^{\\dagger} U^{\\top} \\eta\\|^{2} $$\nLet us define a transformed noise vector $\\eta' \\in \\mathbb{R}^{m}$ as $\\eta' = U^{\\top} \\eta$. Since $U \\in \\mathbb{R}^{m \\times m}$ is also orthogonal, this transformation is a rotation of the noise vector $\\eta$. We can determine the statistical properties of $\\eta'$. The mean is $\\mathbb{E}[\\eta'] = \\mathbb{E}[U^{\\top}\\eta] = U^{\\top}\\mathbb{E}[\\eta] = U^{\\top}0 = 0$. The covariance matrix of $\\eta'$ is:\n$$ \\mathbb{E}[\\eta' (\\eta')^{\\top}] = \\mathbb{E}[(U^{\\top}\\eta)(U^{\\top}\\eta)^{\\top}] = \\mathbb{E}[U^{\\top}\\eta \\eta^{\\top}U] = U^{\\top}\\mathbb{E}[\\eta \\eta^{\\top}]U $$\nUsing the given noise covariance $\\mathbb{E}[\\eta \\eta^{\\top}] = \\sigma_{\\eta}^{2} I_{m}$, we have:\n$$ \\mathbb{E}[\\eta' (\\eta')^{\\top}] = U^{\\top}(\\sigma_{\\eta}^{2} I_{m})U = \\sigma_{\\eta}^{2} U^{\\top}U = \\sigma_{\\eta}^{2} I_{m} $$\nThis demonstrates that the transformed noise vector $\\eta'$ has the same statistical properties as the original noise $\\eta$: its components are uncorrelated, have zero mean, and each has a variance of $\\sigma_{\\eta}^{2}$. Specifically, for any component $k$, $\\mathbb{E}[(\\eta'_{k})^{2}] = \\sigma_{\\eta}^{2}$.\n\nNow we can express the squared error norm in terms of $\\eta'$:\n$$ \\|A^{\\dagger} \\eta\\|^{2} = \\|\\Sigma^{\\dagger} \\eta'\\|^{2} $$\nThe matrix $\\Sigma^{\\dagger} \\in \\mathbb{R}^{n \\times m}$ has its non-zero entries on the main diagonal, which are $(\\Sigma^{\\dagger})_{kk} = 1/\\sigma_k$ for $k=1, \\dots, r$, where $r=\\operatorname{rank}(A)$. Let's write out the squared norm:\n$$ \\|\\Sigma^{\\dagger} \\eta'\\|^{2} = \\sum_{i=1}^{n} \\left( (\\Sigma^{\\dagger} \\eta')_i \\right)^2 = \\sum_{i=1}^{n} \\left( \\sum_{j=1}^{m} (\\Sigma^{\\dagger})_{ij} \\eta'_{j} \\right)^2 $$\nDue to the structure of $\\Sigma^{\\dagger}$, the $i$-th component of the vector $\\Sigma^{\\dagger}\\eta'$ is $(\\Sigma^{\\dagger}\\eta')_i = (1/\\sigma_i)\\eta'_i$ for $i \\le r$, and $0$ for $i > r$. Therefore, the sum becomes:\n$$ \\|\\Sigma^{\\dagger} \\eta'\\|^{2} = \\sum_{k=1}^{r} \\left( \\frac{1}{\\sigma_k} \\eta'_{k} \\right)^2 = \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} (\\eta'_{k})^2 $$\nNow we take the expectation of this expression. By the linearity of expectation:\n$$ \\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2} = \\mathbb{E}\\left[ \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} (\\eta'_{k})^2 \\right] = \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} \\mathbb{E}[(\\eta'_{k})^2] $$\nAs we established, the variance of each component of the transformed noise is $\\mathbb{E}[(\\eta'_{k})^2] = \\sigma_{\\eta}^{2}$. Substituting this into the sum:\n$$ \\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2} = \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} \\sigma_{\\eta}^{2} $$\nThis leads to the final expression for the expected squared reconstruction error:\n$$ \\mathbb{E}\\|A^{\\dagger} \\eta\\|^{2} = \\sigma_{\\eta}^{2} \\sum_{k=1}^{r} \\frac{1}{\\sigma_k^2} $$\nThis result quantitatively confirms that the expected error is directly proportional to the noise variance $\\sigma_{\\eta}^2$ and to the sum of the squared reciprocals of the non-zero singular values of $A$. Small singular values $\\sigma_k$ lead to large terms $1/\\sigma_k^2$ in the sum, causing a large expected error and thus demonstrating the instability of the inversion.",
            "answer": "$$\\boxed{\\sigma_{\\eta}^{2} \\sum_{k=1}^{r} \\frac{1}{\\sigma_{k}^{2}}}$$"
        },
        {
            "introduction": "This final practice transitions from analysis to synthesis, challenging you to apply your understanding of instability to a practical problem in geophysical experimental design. You will write a program to construct the forward operator for a seismic tomography experiment and then optimize the placement of receivers to maximize the system's stability. This hands-on coding exercise powerfully demonstrates how the abstract concept of singular values can be used as a concrete design tool to engineer more well-posed measurement systems from the ground up .",
            "id": "3602530",
            "problem": "You will implement and run a complete program that, for a set of straight-ray seismic tomography experiments, designs optimal receiver layouts to maximize the minimum singular value of the forward operator and uses this to assess stability under Hadamard's criteria for well-posedness. The setting is linearized, two-dimensional, constant slowness per cell, straight-ray travel time tomography. Your task must be accomplished from first principles without relying on any pre-specified shortcut formulas.\n\nThe fundamental base is as follows. In a two-dimensional rectangular domain $[0,1]\\times[0,1]$ measured in meters, discretized into a uniform grid of $N_x\\times N_y$ rectangular cells, assume wave slowness is constant within each cell and denote the vector of cell slownesses by $m\\in\\mathbb{R}^{N_x N_y}$. For any source location $s\\in\\mathbb{R}^2$ and receiver location $r\\in\\mathbb{R}^2$ both strictly interior to the domain, the first-arrival straight-ray travel time $t$ satisfies the line-integral identity $t=\\int_{\\text{ray}(s,r)} u(\\mathbf{x})\\,\\mathrm{d}\\ell$, where $u(\\mathbf{x})$ is slowness and $\\mathrm{d}\\ell$ is the differential arclength element. Under uniform-cell discretization, this reduces to a linear map $t = G m$, where $G\\in\\mathbb{R}^{M\\times N}$, $M$ is the number of source-receiver rays and $N=N_xN_y$, and each row of $G$ contains the sequence of path lengths inside each cell along the straight line segment from source to receiver. The Singular Value Decomposition (SVD) of $G$ yields singular values $\\sigma_1\\ge \\sigma_2\\ge \\cdots \\ge \\sigma_{\\min(M,N)}\\ge 0$. In the linear setting, Hadamard's criteria for well-posedness consist of existence, uniqueness, and continuous dependence of the solution. Assuming noiseless, consistent data, existence is guaranteed. Uniqueness of $m$ is equivalent to $G$ having full column rank (that is, $\\sigma_{\\min}0$ when $M\\ge N$), and continuous dependence with respect to the $\\ell_2$-norm is equivalent to the existence of a finite Lipschitz bound on the inverse, which is controlled by $\\sigma_{\\min}$ via the inequality $\\|m\\|_2 \\le \\sigma_{\\min}^{-1}\\|Gm\\|_2$ when $M\\ge N$ and $\\sigma_{\\min}0$.\n\nYou must:\n- Construct $G$ by computing, for each ray, the exact lengths of intersection of the straight line segment between source and receiver with each grid cell. Use only geometric line-grid intersection and arclength, with no shortcuts.\n- From a finite set of candidate receiver locations, select exactly $R$ receivers to maximize $\\sigma_{\\min}$ of the resulting matrix $G$ formed from all rays generated by the fixed sources and the chosen receivers.\n- Treat the case $MN$ as having effective smallest singular value $\\sigma_{\\min}^{\\mathrm{eff}}=0$ for the purpose of uniqueness and stability assessment, because the nullspace is nontrivial in that case.\n- Use a numerical tolerance $\\varepsilon=10^{-9}$: interpret any singular value less than $\\varepsilon$ as zero in the assessment.\n\nYour program must solve the following test suite of three cases. All coordinates are in meters. Angles, if any appear internally, must be in radians. Singular values must be reported in meters. For each case, you will output two values: the optimal minimum singular value $\\sigma_{\\min}^{\\star}$ in meters rounded to six decimal places, and a stability indicator $b\\in\\{0,1\\}$, where $b=1$ if and only if $M\\ge N$ and $\\sigma_{\\min}^{\\star}\\varepsilon$.\n\nDefinitions common to all cases:\n- Domain: $[0,1]\\times[0,1]$ meters.\n- For a grid $N_x\\times N_y$, the number of model parameters is $N=N_xN_y$.\n- For a given set of sources $\\{s_k\\}_{k=1}^{S}$ and a chosen set of receivers $\\{r_\\ell\\}_{\\ell=1}^{R}$, the number of rays is $M=S\\cdot R$.\n- The forward matrix $G\\in\\mathbb{R}^{M\\times N}$ has entries $G_{p,q}$ equal to the length in meters that ray $p$ travels within cell $q$, indexed in row-major order (first index $x$ then $y$).\n\nCase $1$ (happy path geometry):\n- Grid: $N_x=2$, $N_y=2$.\n- Sources $S=2$: $s_1=(0.05,0.05)$, $s_2=(0.05,0.95)$.\n- Candidate receivers (choose exactly $R=4$ out of $6$):\n  $\\{(0.95,0.10), (0.95,0.28), (0.95,0.46), (0.95,0.64), (0.95,0.82), (0.95,0.90)\\}$.\n- Expectation: $M=8$, $N=4$, so $M\\ge N$; a well-chosen layout should yield $\\sigma_{\\min}^{\\star}0$.\n\nCase $2$ (underdetermined boundary case):\n- Grid: $N_x=3$, $N_y=3$.\n- Sources $S=2$: $s_1=(0.05,0.05)$, $s_2=(0.05,0.95)$.\n- Candidate receivers (choose exactly $R=3$ out of $5$):\n  $\\{(0.95,0.20), (0.95,0.50), (0.95,0.80), (0.80,0.95), (0.80,0.05)\\}$.\n- Expectation: $M=6$, $N=9$, so $MN$ and thus $\\sigma_{\\min}^{\\mathrm{eff}}=0$ regardless of layout; uniqueness fails.\n\nCase $3$ (degenerate illumination edge case):\n- Grid: $N_x=2$, $N_y=2$.\n- Sources $S=2$: $s_1=(0.10,0.25)$, $s_2=(0.10,0.75)$.\n- Candidate receivers (choose exactly $R=4$ out of $5$):\n  $\\{(0.49,0.20), (0.49,0.40), (0.49,0.60), (0.49,0.80), (0.49,0.50)\\}$.\n- Expectation: all rays remain in the left half of the domain; the columns of $G$ corresponding to right-half cells are exactly zero, so rank deficiency occurs and $\\sigma_{\\min}^{\\star}=0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the six entries in the order:\n  $[\\sigma_{\\min,1}^{\\star}, b_1, \\sigma_{\\min,2}^{\\star}, b_2, \\sigma_{\\min,3}^{\\star}, b_3]$,\n  where each $\\sigma_{\\min,i}^{\\star}$ is expressed in meters and rounded to six decimal places, and each $b_i$ is an integer ($0$ or $1$).",
            "solution": "This problem requires implementing a program to solve a geophysical experimental design task. The solution involves three main components: (1) a combinatorial search for the optimal receiver layout, (2) the construction of the forward operator matrix $G$ for each layout based on first geometric principles, and (3) a stability analysis of the resulting linear system using Singular Value Decomposition (SVD) in the context of Hadamard's criteria.\n\n### Methodology\n\n1.  **Optimal Receiver Layout:** For each test case, we must select a subset of $R$ receivers from a list of candidates. The goal is to maximize the minimum singular value, $\\sigma_{\\min}$, of the forward matrix $G$. Since the candidate pools are small, we perform an exhaustive search over all possible combinations of $R$ receivers. For each combination, we construct the matrix $G$ and compute its singular values. The combination yielding the largest $\\sigma_{\\min}$ is deemed optimal.\n\n2.  **Forward Operator Construction:** The core of the program is building the matrix $G$, where each entry $G_{ij}$ is the path length of ray $i$ inside cell $j$. This is achieved by implementing a ray-tracing algorithm that tracks a straight ray's path across the grid. For each ray from a source to a receiver, the algorithm iteratively calculates the intersection points with grid lines, determining the segment length within each traversed cell. This method adheres to the problem's \"first principles\" requirement.\n\n3.  **Stability Analysis:** Hadamard's stability criterion is directly related to the smallest singular value, $\\sigma_{\\min}$.\n    - If the number of rays $M$ is less than the number of cells $N$, the system is underdetermined, the nullspace is non-trivial, and uniqueness fails. Per the problem, we treat this as having an effective $\\sigma_{\\min}^{\\mathrm{eff}}=0$.\n    - If $M \\ge N$, a non-zero $\\sigma_{\\min}$ is required for uniqueness and stability. We compute the SVD of $G$ and find its smallest singular value.\n    - A stability indicator $b$ is set to 1 if and only if $M \\ge N$ and the optimal $\\sigma_{\\min}^{\\star}$ is greater than a numerical tolerance $\\varepsilon=10^{-9}$. Otherwise, $b=0$.\n\nThe following Python code implements this logic and executes it for the three test cases.\n\n```python\nimport numpy as np\nfrom itertools import combinations\n\ndef get_ray_path_lengths(start_pos, end_pos, grid_dims, domain_dims=(0.0, 1.0, 0.0, 1.0)):\n    \"\"\"\n    Computes the lengths of a straight ray's path through each cell of a 2D grid\n    using a DDA-style algorithm (Amanatides  Woo).\n    \"\"\"\n    x_min, x_max, y_min, y_max = domain_dims\n    nx, ny = grid_dims\n    x1, y1 = start_pos\n    x2, y2 = end_pos\n\n    dx_cell = (x_max - x_min) / nx\n    dy_cell = (y_max - y_min) / ny\n    \n    path_lengths = np.zeros(nx * ny)\n\n    vx = x2 - x1\n    vy = y2 - y1\n    \n    ray_length = np.sqrt(vx**2 + vy**2)\n    if ray_length == 0:\n        return path_lengths\n\n    ix = int((x1 - x_min) // dx_cell)\n    iy = int((y1 - y_min) // dy_cell)\n    if ix >= nx: ix = nx - 1\n    if iy >= ny: iy = ny - 1\n    \n    step_x = 1 if vx > 0 else -1 if vx  0 else 0\n    step_y = 1 if vy > 0 else -1 if vy  0 else 0\n\n    if vx != 0:\n        t_max_x = ((x_min + (ix + (1 if step_x > 0 else 0)) * dx_cell) - x1) / vx\n        t_delta_x = dx_cell / abs(vx)\n    else:\n        t_max_x = float('inf')\n        t_delta_x = float('inf')\n\n    if vy != 0:\n        t_max_y = ((y_min + (iy + (1 if step_y > 0 else 0)) * dy_cell) - y1) / vy\n        t_delta_y = dy_cell / abs(vy)\n    else:\n        t_max_y = float('inf')\n        t_delta_y = float('inf')\n\n    t_current = 0.0\n    while t_current  1.0:\n        if not (0 = ix  nx and 0 = iy  ny):\n            break\n        \n        cell_index = ix * ny + iy\n        t_exit = min(t_max_x, t_max_y, 1.0)\n        segment_len = ray_length * (t_exit - t_current)\n        path_lengths[cell_index] += segment_len\n        t_current = t_exit\n\n        if t_current >= 1.0:\n            break\n\n        if abs(t_max_x - t_max_y)  1e-12:\n            ix += step_x\n            iy += step_y\n            t_max_x += t_delta_x\n            t_max_y += t_delta_y\n        elif t_max_x  t_max_y:\n            ix += step_x\n            t_max_x += t_delta_x\n        else:\n            iy += step_y\n            t_max_y += t_delta_y\n            \n    return path_lengths\n\ndef analyze_geometry(grid_dims, sources, receiver_candidates, R_chosen):\n    \"\"\"\n    Finds the optimal receiver layout to maximize the minimum singular value.\n    \"\"\"\n    nx, ny = grid_dims\n    N = nx * ny\n    S = len(sources)\n    M = S * R_chosen\n    epsilon = 1e-9\n\n    if M  N:\n        return 0.0, 0\n\n    max_sigma_min = -1.0\n    receiver_combinations = combinations(receiver_candidates, R_chosen)\n\n    for rec_set in receiver_combinations:\n        G = np.zeros((M, N))\n        ray_idx = 0\n        for source in sources:\n            for receiver in rec_set:\n                G[ray_idx, :] = get_ray_path_lengths(source, receiver, grid_dims)\n                ray_idx += 1\n        \n        singular_values = np.linalg.svd(G, compute_uv=False)\n        sigma_min = singular_values[-1] if len(singular_values) >= N else 0.0\n\n        if sigma_min > max_sigma_min:\n            max_sigma_min = sigma_min\n\n    is_stable = 1 if max_sigma_min > epsilon else 0\n    return max_sigma_min, is_stable\n\ndef solve_all_cases():\n    test_cases = [\n        {\"grid\": (2, 2), \"sources\": [(0.05, 0.05), (0.05, 0.95)], \"candidates\": [(0.95,0.10), (0.95,0.28), (0.95,0.46), (0.95,0.64), (0.95,0.82), (0.95,0.90)], \"R\": 4},\n        {\"grid\": (3, 3), \"sources\": [(0.05, 0.05), (0.05, 0.95)], \"candidates\": [(0.95,0.20), (0.95,0.50), (0.95,0.80), (0.80,0.95), (0.80,0.05)], \"R\": 3},\n        {\"grid\": (2, 2), \"sources\": [(0.10, 0.25), (0.10, 0.75)], \"candidates\": [(0.49,0.20), (0.49,0.40), (0.49,0.60), (0.49,0.80), (0.49,0.50)], \"R\": 4}\n    ]\n    results = []\n    for case in test_cases:\n        sigma_min_star, b = analyze_geometry(\n            grid_dims=case[\"grid\"],\n            sources=case[\"sources\"],\n            receiver_candidates=case[\"candidates\"],\n            R_chosen=case[\"R\"]\n        )\n        results.append(f\"{sigma_min_star:.6f}\")\n        results.append(str(b))\n    return f\"[{','.join(results)}]\"\n\n# The output from the code is computed and placed in the answer tag.\n```",
            "answer": "[0.126466,1,0.000000,0,0.000000,0]"
        }
    ]
}