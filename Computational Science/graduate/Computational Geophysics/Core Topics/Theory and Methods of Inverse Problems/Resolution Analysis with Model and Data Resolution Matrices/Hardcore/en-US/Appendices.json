{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds our understanding of resolution analysis in the data space. We will derive the data resolution matrix, $R_d$, for a simple unregularized least-squares problem and verify its fundamental property of idempotence. This exercise solidifies the algebraic form of $R_d$ and provides a concrete link to its geometric interpretation as a projection operator, which maps observed data onto the subspace of data that can be perfectly fit by the model .",
            "id": "3613671",
            "problem": "Consider a linearized geophysical inverse problem in which observed data $d \\in \\mathbb{R}^{3}$ are related to a model update $m \\in \\mathbb{R}^{2}$ through a sensitivity (Jacobian) matrix $G \\in \\mathbb{R}^{3 \\times 2}$ by $d = Gm + e$, where $e$ denotes additive data errors. Assume zero-mean errors and an identity data covariance so that unweighted least squares is appropriate. Starting from the normal equations for the least-squares estimator and the definition of the fitted data $d_{\\mathrm{fit}}$, derive the data resolution matrix $R_{d}$ that maps observed data $d$ to fitted data $d_{\\mathrm{fit}}$.\n\nLet the sensitivity matrix be\n$$\nG = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nCompute the explicit form of the data resolution matrix $R_{d}$ for this $G$ and then numerically verify idempotence by evaluating the Frobenius norm\n$$\n\\|R_{d}^{2} - R_{d}\\|_{F},\n$$\nwhere the Frobenius norm is defined by $\\|\\mathbf{A}\\|_{F} = \\sqrt{\\sum_{i,j} A_{ij}^{2}}$. Report the value of $\\|R_{d}^{2} - R_{d}\\|_{F}$ as a single real number. No units are required in the final answer.",
            "solution": "The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Step 1: Extract Givens\n- The relationship between observed data $d \\in \\mathbb{R}^{3}$ and model parameters $m \\in \\mathbb{R}^{2}$ is given by the linear model $d = Gm + e$.\n- $G \\in \\mathbb{R}^{3 \\times 2}$ is the sensitivity matrix.\n- $e$ represents additive, zero-mean data errors with an identity covariance matrix.\n- The cost function is unweighted least squares.\n- The fitted data is denoted by $d_{\\mathrm{fit}}$.\n- The data resolution matrix $R_{d}$ is defined by the relation $d_{\\mathrm{fit}} = R_{d}d$.\n- The specific sensitivity matrix is given as:\n$$\nG = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n$$\n- The task is to derive the general form of $R_{d}$, compute it for the given $G$, and then evaluate the Frobenius norm $\\|R_{d}^{2} - R_{d}\\|_{F}$, where $\\|\\mathbf{A}\\|_{F} = \\sqrt{\\sum_{i,j} A_{ij}^{2}}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard exercise in linear inverse theory, a fundamental topic in computational geophysics and many other STEM fields. All concepts, such as least-squares estimation, normal equations, and resolution matrices, are well-established.\n- **Well-Posed**: The problem is clearly stated. The matrix $G$ has full column rank, which ensures that $G^{T}G$ is invertible and a unique least-squares solution exists. The tasks are specific and lead to a single numerical answer.\n- **Objective**: The problem is expressed in precise mathematical language, free of ambiguity or subjective claims.\n- **Completeness**: All necessary information is provided.\nThe problem is deemed **valid** and a solution will be constructed.\n\n### Derivation of the Data Resolution Matrix\nThe unweighted least-squares method seeks to find the model $m$ that minimizes the squared Euclidean norm of the residual vector $r = d - Gm$. The objective function $S(m)$ is:\n$$\nS(m) = \\|d - Gm\\|_{2}^{2} = (d - Gm)^{T}(d - Gm)\n$$\nExpanding this expression gives:\n$$\nS(m) = d^{T}d - d^{T}Gm - m^{T}G^{T}d + m^{T}G^{T}Gm\n$$\nTo find the minimum, we take the derivative of $S(m)$ with respect to $m$ and set it to zero. Using standard matrix calculus results, we get:\n$$\n\\frac{\\partial S}{\\partial m} = -2G^{T}d + 2G^{T}Gm = \\mathbf{0}\n$$\nThis simplifies to the normal equations:\n$$\nG^{T}Gm = G^{T}d\n$$\nThe least-squares estimate for the model, $m_{\\mathrm{est}}$, is obtained by solving for $m$:\n$$\nm_{\\mathrm{est}} = (G^{T}G)^{-1}G^{T}d\n$$\nThe fitted data, $d_{\\mathrm{fit}}$, is the prediction from the estimated model:\n$$\nd_{\\mathrm{fit}} = Gm_{\\mathrm{est}}\n$$\nSubstituting the expression for $m_{\\mathrm{est}}$ into the equation for $d_{\\mathrm{fit}}$:\n$$\nd_{\\mathrm{fit}} = G \\left( (G^{T}G)^{-1}G^{T}d \\right)\n$$\nBy regrouping the matrices, we can express $d_{\\mathrm{fit}}$ as a linear transformation of the observed data $d$:\n$$\nd_{\\mathrm{fit}} = \\left( G(G^{T}G)^{-1}G^{T} \\right) d\n$$\nFrom the problem definition, $d_{\\mathrm{fit}} = R_{d}d$. By comparison, we identify the data resolution matrix $R_{d}$ as:\n$$\nR_{d} = G(G^{T}G)^{-1}G^{T}\n$$\nThis matrix is a projection matrix that projects the data vector $d$ onto the subspace spanned by the columns of $G$.\n\n### Computation for the Specific Jacobian Matrix $G$\nWe are given the matrix:\n$$\nG = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix}\n$$\nFirst, we compute its transpose $G^{T}$:\n$$\nG^{T} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nNext, we compute the product $G^{T}G$:\n$$\nG^{T}G = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(1)(1)+(0)(0) & (1)(0)+(1)(1)+(0)(1) \\\\ (0)(1)+(1)(1)+(1)(0) & (0)(0)+(1)(1)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nNow, we find the inverse of $G^{T}G$. The determinant is $\\det(G^{T}G) = (2)(2) - (1)(1) = 3$.\n$$\n(G^{T}G)^{-1} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\nFinally, we assemble the data resolution matrix $R_{d}$:\n$$\nR_{d} = G(G^{T}G)^{-1}G^{T} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\left( \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\right) \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\n$$\nR_{d} = \\frac{1}{3} \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ 1 & 1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\n$$\nR_{d} = \\frac{1}{3} \\begin{pmatrix} (2)(1)+(-1)(0) & (2)(1)+(-1)(1) & (2)(0)+(-1)(1) \\\\ (1)(1)+(1)(0) & (1)(1)+(1)(1) & (1)(0)+(1)(1) \\\\ (-1)(1)+(2)(0) & (-1)(1)+(2)(1) & (-1)(0)+(2)(1) \\end{pmatrix}\n$$\n$$\nR_{d} = \\frac{1}{3} \\begin{pmatrix} 2 & 1 & -1 \\\\ 1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix}\n$$\n\n### Verification of Idempotence and Frobenius Norm Calculation\nAn important property of a projection matrix is idempotence, i.e., $R_{d}^{2} = R_{d}$. We verify this by direct computation.\n$$\nR_{d}^{2} = \\left( \\frac{1}{3} \\begin{pmatrix} 2 & 1 & -1 \\\\ 1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix} \\right) \\left( \\frac{1}{3} \\begin{pmatrix} 2 & 1 & -1 \\\\ 1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix} \\right) = \\frac{1}{9} \\begin{pmatrix} 2 & 1 & -1 \\\\ 1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 2 & 1 & -1 \\\\ 1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix}\n$$\nThe matrix product is:\n$$\n\\begin{pmatrix} 4+1+1 & 2+2-1 & -2+1-2 \\\\ 2+2-1 & 1+4+1 & -1+2+2 \\\\ -2+1-2 & -1+2+2 & 1+1+4 \\end{pmatrix} = \\begin{pmatrix} 6 & 3 & -3 \\\\ 3 & 6 & 3 \\\\ -3 & 3 & 6 \\end{pmatrix}\n$$\nSo, $R_{d}^{2}$ is:\n$$\nR_{d}^{2} = \\frac{1}{9} \\begin{pmatrix} 6 & 3 & -3 \\\\ 3 & 6 & 3 \\\\ -3 & 3 & 6 \\end{pmatrix} = \\begin{pmatrix} 6/9 & 3/9 & -3/9 \\\\ 3/9 & 6/9 & 3/9 \\\\ -3/9 & 3/9 & 6/9 \\end{pmatrix} = \\begin{pmatrix} 2/3 & 1/3 & -1/3 \\\\ 1/3 & 2/3 & 1/3 \\\\ -1/3 & 1/3 & 2/3 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & 1 & -1 \\\\ 1 & 2 & 1 \\\\ -1 & 1 & 2 \\end{pmatrix}\n$$\nWe see that $R_{d}^{2} = R_{d}$, confirming the property of idempotence.\nThe problem requires computing the Frobenius norm of $R_{d}^{2} - R_{d}$.\nSince $R_{d}^{2} = R_{d}$, we have:\n$$\nR_{d}^{2} - R_{d} = \\mathbf{0}\n$$\nwhere $\\mathbf{0}$ is the $3 \\times 3$ zero matrix.\nThe Frobenius norm of the zero matrix is:\n$$\n\\|R_{d}^{2} - R_{d}\\|_{F} = \\|\\mathbf{0}\\|_{F} = \\sqrt{\\sum_{i=1}^{3}\\sum_{j=1}^{3} 0_{ij}^{2}} = \\sqrt{0} = 0\n$$\nThe value of the Frobenius norm is $0$.",
            "answer": "$$\n\\boxed{0}\n$$"
        },
        {
            "introduction": "We now shift our focus to the model space and the effect of regularization by examining the model resolution matrix, $R_m$. This exercise uses a simple, analytically solvable $2 \\times 2$ problem to explore how ill-conditioning, manifesting as a near-nullspace, degrades resolution. By analyzing the eigenvalues of $R_m$, you will gain a deep, quantitative understanding of how resolution matrices diagnose ambiguity in an inversion and how regularization manages the trade-off between model detail and stability .",
            "id": "3613752",
            "problem": "Consider a linear inverse problem in computational geophysics where the data vector $d \\in \\mathbb{R}^{2}$ is modeled by $d = G m + e$, with model vector $m \\in \\mathbb{R}^{2}$, forward operator $G \\in \\mathbb{R}^{2 \\times 2}$, and data noise $e$ modeled as a zero-mean Gaussian random vector with covariance matrix $C_{d} = \\sigma^{2} I$, where $\\sigma > 0$ and $I$ is the identity matrix. The forward operator is\n$$\nG = \\begin{bmatrix} 1 & 1 \\\\ 0 & \\alpha \\end{bmatrix},\n$$\nwith $\\alpha > 0$. To stabilize the inversion in the presence of potential ill-conditioning, consider zero-order Tikhonov regularization with damping parameter $\\lambda > 0$, where the model estimate is obtained by minimizing the objective functional\n$$\n\\|d - G m\\|_{C_{d}^{-1}}^{2} + \\lambda^{2} \\|m\\|^{2},\n$$\nand the model resolution matrix $R_{m}$ is defined in terms of the estimator’s action on the true model. Starting from first principles of weighted least squares and regularization, derive the model resolution matrix $R_{m}$ and analyze its eigenstructure.\n\nYour task is to provide the exact closed-form expression for the smallest eigenvalue of the model resolution matrix $R_{m}$ as a function of $\\alpha$, $\\sigma$, and $\\lambda$. Do not round; provide your final expression in exact symbolic form. Additionally, in your derivation, explain how the behavior as $\\alpha \\to 0$ reveals the emergence of a near-nullspace in the inverse problem. The final answer must be a single closed-form analytic expression for the smallest eigenvalue of $R_{m}$.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It represents a standard formulation of a regularized linear inverse problem in computational geophysics. All provided information is self-contained and consistent.\n\nThe objective is to find the smallest eigenvalue of the model resolution matrix $R_{m}$ for a linear inverse problem regularized using Tikhonov's method. The model estimate $\\hat{m}$ is found by minimizing the objective functional $J(m)$:\n$$\nJ(m) = \\|d - G m\\|_{C_{d}^{-1}}^{2} + \\lambda^{2} \\|m\\|^{2}\n$$\nGiven the data covariance matrix is $C_{d} = \\sigma^{2} I$, its inverse is $C_{d}^{-1} = \\frac{1}{\\sigma^{2}} I$. The functional becomes:\n$$\nJ(m) = \\frac{1}{\\sigma^{2}} (d - G m)^{T} (d - G m) + \\lambda^{2} m^{T} m\n$$\nTo find the model estimate $\\hat{m}$ that minimizes $J(m)$, we compute the gradient with respect to $m$ and set it to zero:\n$$\n\\nabla_{m} J(m) = \\frac{1}{\\sigma^{2}} \\nabla_{m} (d^{T}d - 2d^{T}Gm + m^{T}G^{T}Gm) + \\lambda^{2} \\nabla_{m} (m^{T}m)\n$$\n$$\n\\nabla_{m} J(m) = \\frac{1}{\\sigma^{2}} (-2G^{T}d + 2G^{T}Gm) + 2\\lambda^{2}m = \\mathbf{0}\n$$\nMultiplying by $\\frac{\\sigma^{2}}{2}$ and rearranging gives:\n$$\n(G^{T}G)m - G^{T}d + \\sigma^{2}\\lambda^{2}m = \\mathbf{0}\n$$\n$$\n(G^{T}G + \\sigma^{2}\\lambda^{2}I)m = G^{T}d\n$$\nThe estimated model $\\hat{m}$ is therefore:\n$$\n\\hat{m} = (G^{T}G + \\sigma^{2}\\lambda^{2}I)^{-1} G^{T}d\n$$\nThe model resolution matrix $R_{m}$ relates the expected value of the estimator $\\hat{m}$ to the true model $m_{\\text{true}}$. We substitute the forward model $d = G m_{\\text{true}} + e$ into the expression for $\\hat{m}$:\n$$\n\\hat{m} = (G^{T}G + \\sigma^{2}\\lambda^{2}I)^{-1} G^{T} (G m_{\\text{true}} + e)\n$$\n$$\n\\hat{m} = (G^{T}G + \\sigma^{2}\\lambda^{2}I)^{-1} G^{T}G m_{\\text{true}} + (G^{T}G + \\sigma^{2}\\lambda^{2}I)^{-1} G^{T} e\n$$\nTaking the expectation, and using the fact that the noise is zero-mean ($\\mathbb{E}[e] = \\mathbf{0}$):\n$$\n\\mathbb{E}[\\hat{m}] = (G^{T}G + \\sigma^{2}\\lambda^{2}I)^{-1} G^{T}G m_{\\text{true}}\n$$\nBy definition, $\\mathbb{E}[\\hat{m}] = R_{m} m_{\\text{true}}$, so the model resolution matrix is:\n$$\nR_{m} = (G^{T}G + \\sigma^{2}\\lambda^{2}I)^{-1} G^{T}G\n$$\nTo find the eigenvalues of $R_{m}$, we first find the eigenvalues of the matrix $A = G^{T}G$. Let $\\eta$ be an eigenvalue of $A$ with corresponding eigenvector $v$, such that $Av = \\eta v$. Then we can analyze the action of $R_{m}$ on $v$:\n$$\nR_{m}v = (A + \\sigma^{2}\\lambda^{2}I)^{-1} A v = (A + \\sigma^{2}\\lambda^{2}I)^{-1} (\\eta v) = \\eta (A + \\sigma^{2}\\lambda^{2}I)^{-1} v\n$$\nSince $(A + \\sigma^{2}\\lambda^{2}I)v = Av + \\sigma^{2}\\lambda^{2}v = (\\eta + \\sigma^{2}\\lambda^{2})v$, we have $(A + \\sigma^{2}\\lambda^{2}I)^{-1}v = \\frac{1}{\\eta + \\sigma^{2}\\lambda^{2}}v$.\nSubstituting this back, we find:\n$$\nR_{m}v = \\eta \\left(\\frac{1}{\\eta + \\sigma^{2}\\lambda^{2}}\\right) v = \\frac{\\eta}{\\eta + \\sigma^{2}\\lambda^{2}} v\n$$\nThus, the eigenvalues of $R_{m}$, denoted $\\rho$, are related to the eigenvalues of $G^{T}G$, denoted $\\eta$, by the expression $\\rho = \\frac{\\eta}{\\eta + \\sigma^{2}\\lambda^{2}}$. The function $f(\\eta) = \\frac{\\eta}{\\eta + \\mu}$ (with $\\mu = \\sigma^{2}\\lambda^{2} > 0$) is monotonically increasing for $\\eta > 0$. Therefore, the smallest eigenvalue of $R_{m}$ corresponds to the smallest eigenvalue of $G^{T}G$.\n\nWe now compute $G^{T}G$ for the given forward operator $G = \\begin{bmatrix} 1 & 1 \\\\ 0 & \\alpha \\end{bmatrix}$:\n$$\nG^{T}G = \\begin{bmatrix} 1 & 0 \\\\ 1 & \\alpha \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & \\alpha \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1+\\alpha^{2} \\end{bmatrix}\n$$\nThe eigenvalues $\\eta$ of $G^{T}G$ are the roots of the characteristic equation $\\det(G^{T}G - \\eta I) = 0$:\n$$\n\\det \\begin{bmatrix} 1-\\eta & 1 \\\\ 1 & 1+\\alpha^{2}-\\eta \\end{bmatrix} = 0\n$$\n$$\n(1-\\eta)(1+\\alpha^{2}-\\eta) - 1 = 0\n$$\n$$\n\\eta^{2} - (1+\\alpha^{2}+1)\\eta + (1+\\alpha^{2}) - 1 = 0\n$$\n$$\n\\eta^{2} - (2+\\alpha^{2})\\eta + \\alpha^{2} = 0\n$$\nUsing the quadratic formula to solve for $\\eta$:\n$$\n\\eta = \\frac{(2+\\alpha^{2}) \\pm \\sqrt{(2+\\alpha^{2})^{2} - 4\\alpha^{2}}}{2} = \\frac{2+\\alpha^{2} \\pm \\sqrt{4+4\\alpha^{2}+\\alpha^{4} - 4\\alpha^{2}}}{2} = \\frac{2+\\alpha^{2} \\pm \\sqrt{4+\\alpha^{4}}}{2}\n$$\nThe two eigenvalues of $G^{T}G$ are $\\eta_{1} = \\frac{2+\\alpha^{2} + \\sqrt{4+\\alpha^{4}}}{2}$ and $\\eta_{2} = \\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2}$. The smallest eigenvalue is $\\eta_{\\min}$:\n$$\n\\eta_{\\min} = \\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2}\n$$\nThe corresponding smallest eigenvalue of the model resolution matrix, $\\rho_{\\min}$, is:\n$$\n\\rho_{\\min} = \\frac{\\eta_{\\min}}{\\eta_{\\min} + \\sigma^{2}\\lambda^{2}} = \\frac{\\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2}}{\\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2} + \\sigma^{2}\\lambda^{2}}\n$$\nMultiplying the numerator and denominator by $2$ yields the final expression:\n$$\n\\rho_{\\min} = \\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}} + 2\\sigma^{2}\\lambda^{2}}\n$$\nAs $\\alpha \\to 0$, the forward operator $G$ approaches $G_0 = \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}$, which is singular. The nullspace of $G_0$ consists of vectors $v$ satisfying $v_{1} + v_{2} = 0$, spanned by the vector $[1, -1]^{T}$. This vector represents a component of the model that produces no data, and is thus completely unconstrained. The limit $\\alpha \\to 0$ signifies the emergence of a nullspace, making the problem ill-posed.\nIn this limit, the smallest eigenvalue of $G^{T}G$ becomes:\n$$\n\\lim_{\\alpha \\to 0} \\eta_{\\min} = \\lim_{\\alpha \\to 0} \\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2} = \\frac{2 - \\sqrt{4}}{2} = 0\n$$\nAn eigenvalue of $G^T G$ approaching zero is the hallmark of an emerging nullspace. The corresponding eigenvector of $G^T G$ approaches the nullspace vector $[1, -1]^T$. Consequently, the smallest eigenvalue of the resolution matrix $R_{m}$ also approaches zero (given $\\lambda > 0$):\n$$\n\\lim_{\\alpha \\to 0} \\rho_{\\min} = \\frac{0}{0 + 2\\sigma^{2}\\lambda^{2}} = 0\n$$\nAn eigenvalue of the resolution matrix equal to $0$ indicates a total lack of resolution for the corresponding model component (eigenvector). For small $\\alpha > 0$, $\\rho_{\\min}$ will be close to zero, signifying that the component of the model associated with the near-nullspace is very poorly resolved by the data. The regularization, controlled by $\\lambda$, stabilizes the inversion by suppressing this unconstrained component, but at the cost of resolution, as quantified by the small value of $\\rho_{\\min}$.",
            "answer": "$$\\boxed{\\frac{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}}}{2+\\alpha^{2} - \\sqrt{4+\\alpha^{4}} + 2\\sigma^{2}\\lambda^{2}}}$$"
        },
        {
            "introduction": "To bridge the gap between abstract theory and practical application, this final exercise guides you through building and interpreting a point-spread function (PSF). You will implement a 2D tomographic inversion and discover that the PSF is simply a column of the model resolution matrix, revealing how the inversion process \"smears\" a single point in the true model. This hands-on coding problem provides direct visual and quantitative insight into how survey geometry and regularization choices control the resolution and anisotropy of your final model estimate .",
            "id": "3613732",
            "problem": "You are given a two-dimensional linear inverse problem on a unit square domain with a uniform rectangular grid of cells, typical of straight-ray traveltime tomography in computational geophysics. The forward model is a linear operator that maps a cellwise model to path integrals of that model along straight rays. You will analyze the resolution properties of a damped and smoothed least squares estimate via point-spread functions, and quantify the anisotropy of the spread in a model-centered manner.\n\nStart from the following foundational base: the linear system of equations is given by $d = G m + \\epsilon$, where $G$ is the sensitivity matrix formed by line integrals of cell contributions along straight ray paths, $m$ is the unknown model sampled uniformly on a grid, $d$ is the data vector (one datum per ray), and $\\epsilon$ is additive noise. The estimate is obtained by minimizing a weighted least squares objective with a quadratic smoothing penalty. The smoothing operator $L$ is constructed from first-order differences between neighboring cells along the two grid axes, and can be made anisotropic by choosing different weights for the horizontal and vertical differences.\n\nConstruct point-spread functions by injecting a delta model at a chosen cell and performing the inversion to obtain the estimated model. This estimated model is the point-spread function, and it equals the corresponding column of the model resolution matrix. Then, quantify the anisotropy of each spread function by computing a second-moment tensor about the central cell and forming a square-root eigenvalue ratio that measures the elongation of the spread.\n\nImplementation details to follow exactly:\n- Domain and grid:\n  - The domain is the unit square $[0,1]\\times[0,1]$.\n  - Use a uniform grid with $n\\times n$ cells. Denote $N = n^2$.\n  - The coordinate of the center of the cell at grid indices $(i,j)$ with zero-based indexing is $\\big((j+1/2)/n, (i+1/2)/n\\big)$.\n  - Use $n = 16$.\n  - The analysis cell (into which the delta model is injected) is the grid cell at indices $(i_0,j_0)=(\\lfloor n/2 \\rfloor,\\lfloor n/2 \\rfloor)$. This is deterministically $(8,8)$ for $n=16$.\n- Rays and forward operator $G$:\n  - Each ray is defined as a straight line segment connecting two points on the boundary of the unit square.\n  - The forward operator $G$ has one row per ray and one column per cell. The entry $G_{r,c}$ equals the approximate path length of ray $r$ through cell $c$.\n  - Approximate line integrals by uniform sampling along each ray’s segment: let the endpoints be $p_0=(x_0,y_0)$ and $p_1=(x_1,y_1)$. Use $S=200$ equally spaced interior sample points along the segment, with segment length $\\ell=\\lVert p_1-p_0\\rVert_2$ and step size $\\Delta s = \\ell/S$. For the $s$-th sample, $p=p_0 + (s+1/2)(p_1-p_0)/S$ for $s\\in\\{0,1,\\dots,S-1\\}$. Assign $\\Delta s$ to the cell containing $p$ (by integer truncation of $x \\cdot n$ and $y \\cdot n$, with out-of-bounds samples discarded).\n- Smoothing operator $L$:\n  - Build $L$ as a stack of first-order differences between neighboring cells. For each horizontal neighbor pair at common row $i$ and adjacent columns $j-1$ and $j$, include a row that equals $w_x(m_{i,j}-m_{i,j-1})$. For each vertical neighbor pair at common column $j$ and adjacent rows $i-1$ and $i$, include a row that equals $w_y(m_{i,j}-m_{i-1,j})$. Here $w_x$ and $w_y$ are nonnegative weights that control anisotropy of smoothing. The smoothing penalty is $\\lVert Lm\\rVert_2^2$.\n- Estimation:\n  - Use a standard damped and smoothed least squares estimate that minimizes the sum of squared residuals $+$ a nonnegative multiple $\\lambda^2$ of the smoothing penalty. Treat data weights as identity. Solve the associated normal equations to obtain the estimated model for each injected delta model.\n- Point-spread function and anisotropy:\n  - For a given index $c_0$ (corresponding to cell $(i_0,j_0)$), form the delta model $e_{c_0}$.\n  - Form synthetic data $d=Ge_{c_0}$ and solve for the estimated model $\\widehat{m}$ using the same smoothing and damping. The vector $\\widehat{m}$ is the point-spread function.\n  - To quantify anisotropy, compute a weighted second-moment tensor of the spread about the center cell coordinates $(x_c,y_c)$ using nonnegative weights $w_k = \\widehat{m}_k^2$ where $\\widehat{m}_k$ is the value at cell $k$ and $(x_k,y_k)$ is its center. Let $\\Delta x_k = x_k - x_c$ and $\\Delta y_k = y_k - y_c$. Define the $2\\times 2$ symmetric tensor\n    $$\\mathbf{M} = \\frac{1}{\\sum_k w_k}\\sum_k w_k \\begin{bmatrix} \\Delta x_k^2 & \\Delta x_k \\Delta y_k \\\\ \\Delta x_k \\Delta y_k & \\Delta y_k^2 \\end{bmatrix}.$$\n    Let $\\mu_{\\max}$ and $\\mu_{\\min}$ be the eigenvalues of $\\mathbf{M}$ with $\\mu_{\\max}\\ge \\mu_{\\min}\\ge 0$. Define the anisotropy ratio as\n    $$\\rho = \\sqrt{\\frac{\\mu_{\\max}}{\\mu_{\\min} + \\epsilon}},$$\n    where $\\epsilon$ is a small positive regularization constant that you should take as $\\epsilon = 10^{-12}$ to avoid division by zero. The anisotropy ratio $\\rho$ is dimensionless.\n- Units:\n  - All quantities in this problem are dimensionless. Angles, where used in any internal implementation, should be treated in radians or degrees as convenient, but your implementation should adhere to the ray definitions below, which are specified as boundary point pairs and do not require explicit angle inputs.\n\nTest suite. For all cases below, use $n=16$, $S=200$ samples per ray, the analysis cell at $(i_0,j_0)=(8,8)$, and compute the anisotropy ratio $\\rho$ as defined above. Rays are defined by families of boundary-to-boundary segments; each family generates $8$ rays.\n\nDefine the following ray families:\n- Family $\\mathrm{LR\\_hor}$: left-to-right horizontal rays. For $k\\in\\{1,3,5,7,9,11,13,15\\}$, define $y_0 = k/16$ and the segment from $(0,y_0)$ to $(1,y_0)$.\n- Family $\\mathrm{LR\\_pos}$: left-to-right positive-slope rays. Using the same $y_0$ set, define the segment from $(0,y_0)$ to $(1,\\min(1,y_0+0.4))$.\n- Family $\\mathrm{LR\\_neg}$: left-to-right negative-slope rays. Using the same $y_0$ set, define the segment from $(0,y_0)$ to $(1,\\max(0,y_0-0.4))$.\n- Family $\\mathrm{BT\\_ver}$: bottom-to-top vertical rays. For $k\\in\\{1,3,5,7,9,11,13,15\\}$, define $x_0 = k/16$ and the segment from $(x_0,0)$ to $(x_0,1)$.\n- Family $\\mathrm{BT\\_pos}$: bottom-to-top positive-slope rays. Using the same $x_0$ set, define the segment from $(x_0,0)$ to $(\\min(1,x_0+0.4),1)$.\n- Family $\\mathrm{BT\\_neg}$: bottom-to-top negative-slope rays. Using the same $x_0$ set, define the segment from $(x_0,0)$ to $(\\max(0,x_0-0.4),1)$.\n\nThe four test cases to implement are:\n- Case $1$ (balanced coverage, isotropic smoothing): use all six families $\\{\\mathrm{LR\\_hor},\\mathrm{LR\\_pos},\\mathrm{LR\\_neg},\\mathrm{BT\\_ver},\\mathrm{BT\\_pos},\\mathrm{BT\\_neg}\\}$ for a total of $48$ rays; use smoothing weights $w_x = 1$ and $w_y = 1$; damping parameter $\\lambda = 0.1$.\n- Case $2$ (directionally limited coverage): use only family $\\{\\mathrm{BT\\_ver}\\}$ for a total of $8$ rays; use smoothing weights $w_x = 1$ and $w_y = 1$; damping parameter $\\lambda = 0.1$.\n- Case $3$ (balanced coverage, strong smoothing): use the same rays as Case $1$; use smoothing weights $w_x = 1$ and $w_y = 1$; damping parameter $\\lambda = 1.0$.\n- Case $4$ (balanced coverage, anisotropic smoothing): use the same rays as Case $1$; use smoothing weights $w_x = 5$ and $w_y = 1$; damping parameter $\\lambda = 0.1$.\n\nYour tasks:\n- Implement construction of $G$ via the sampling scheme above.\n- Implement construction of $L$ via first-order differences with weights $w_x$ and $w_y$.\n- For each case, inject the delta model at $(i_0,j_0)$, synthesize $d$, solve the normal equations corresponding to identity data weights and the specified $\\lambda$ and $L$ to obtain the point-spread function $\\widehat{m}$, and compute the anisotropy ratio $\\rho$ with $\\epsilon=10^{-12}$.\n- The requested output of your program is the list $[\\rho_1,\\rho_2,\\rho_3,\\rho_4]$ corresponding to Cases $1$ through $4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each anisotropy ratio rounded to three decimal places, for example $[\\rho_1,\\rho_2,\\rho_3,\\rho_4]$. The anisotropy ratios are dimensionless and must be printed as decimal numbers without additional text.",
            "solution": "The user-provided problem is a well-defined task in computational geophysics, specifically concerning the resolution analysis of a linear inverse problem. The problem is scientifically grounded, logically consistent, and provides all necessary parameters and definitions to proceed with a unique, computable solution. All steps are clearly specified, from the grid and ray geometry to the mathematical formulation of the regularized least-squares estimator and the method for quantifying anisotropy. The problem is therefore deemed **valid**.\n\nThe solution proceeds as follows, adhering strictly to the outlined methodology.\n\n**1. Model Discretization and Geometry**\nThe domain is a unit square, $[0,1]\\times[0,1]$, discretized into a uniform grid of $n \\times n$ cells, where $n=16$. The total number of model parameters (cells) is $N = n^2 = 256$. A two-dimensional cell index $(i,j)$, where $i$ is the row (y-axis) and $j$ is the column (x-axis), is mapped to a single vector index $c = i \\cdot n + j$. The center of cell $(i,j)$ is located at coordinate $\\big((j+1/2)/n, (i+1/2)/n\\big)$. The analysis is centered on the cell at index $(i_0,j_0) = (\\lfloor n/2 \\rfloor, \\lfloor n/2 \\rfloor) = (8,8)$, which corresponds to the linear index $c_0 = 8 \\cdot 16 + 8 = 136$.\n\n**2. Forward Operator, $G$**\nThe forward problem is described by the linear system $d = Gm$, where $G$ is the sensitivity matrix. The entry $G_{r,c}$ represents the length of the path of ray $r$ through cell $c$. This is approximated by sampling each ray at $S=200$ equidistant points. For a ray segment from $p_0$ to $p_1$ of length $\\ell$, each sample point represents a path segment of length $\\Delta s = \\ell/S$. For each sample point $p=(x,y)$ that falls within the domain, the corresponding cell indices are determined by $j = \\lfloor x \\cdot n \\rfloor$ and $i = \\lfloor y \\cdot n \\rfloor$. The value $\\Delta s$ is then added to the entry $G_{r,c}$ where $c = i \\cdot n + j$. Rays are organized into families as specified, and the matrix $G$ is constructed for the ray set of each test case.\n\n**3. Smoothing Operator, $L$**\nA quadratic smoothing penalty, $\\lVert Lm\\rVert_2^2$, is incorporated to regularize the solution. The operator $L$ enforces smoothness by penalizing first-order differences between adjacent cell values. It is constructed as a sparse matrix where each row corresponds to a difference between a pair of neighboring cells.\n- For a horizontal pair of cells at $(i, j-1)$ and $(i, j)$, a row in $L$ is created with entries $-w_x$ and $+w_x$ at the columns corresponding to these cells, respectively. This is done for all $i \\in \\{0, \\dots, n-1\\}$ and $j \\in \\{1, \\dots, n-1\\}$.\n- For a vertical pair of cells at $(i-1, j)$ and $(i, j)$, a row in $L$ is created with entries $-w_y$ and $+w_y$ at the corresponding columns. This is done for all $i \\in \\{1, \\dots, n-1\\}$ and $j \\in \\{0, \\dots, n-1\\}$.\nThe weights $w_x$ and $w_y$ control the degree of smoothing anisotropy.\n\n**4. Regularized Least-Squares Estimation and Point-Spread Function (PSF)**\nThe estimated model $\\widehat{m}$ is found by minimizing the objective function:\n$$ J(m) = \\lVert Gm - d \\rVert_2^2 + \\lambda^2 \\lVert Lm \\rVert_2^2 $$\nThe minimizer is the solution to the normal equations:\n$$ (G^TG + \\lambda^2 L^TL)\\widehat{m} = G^Td $$\nThe point-spread function (PSF) at a target cell $c_0$ is defined as the estimated model $\\widehat{m}$ resulting from a true model that is a delta function, $m_{\\text{true}} = e_{c_0}$, where $e_{c_0}$ is the canonical basis vector with a $1$ at index $c_0$ and zeros elsewhere. The synthetic data for this model is $d = Ge_{c_0} = g_{c_0}$, where $g_{c_0}$ is the $c_0$-th column of $G$. The PSF, which we denote $\\widehat{m}_{\\text{psf}}$, is therefore obtained by solving:\n$$ (G^TG + \\lambda^2 L^TL)\\widehat{m}_{\\text{psf}} = G^Tg_{c_0} $$\nThis linear system is solved for $\\widehat{m}_{\\text{psf}}$ for each test case.\n\n**5. Anisotropy Quantification**\nThe spatial spread of the PSF is analyzed to determine its anisotropy. A weighted second-moment tensor $\\mathbf{M}$ is computed. The weights for each cell $k$ are given by the square of the PSF value, $w_k = (\\widehat{m}_{\\text{psf}, k})^2$. The tensor components are calculated with respect to the center of the target cell, $(x_c, y_c)$:\n$$ \\mathbf{M} = \\frac{1}{\\sum_k w_k} \\sum_k w_k \\begin{bmatrix} (\\Delta x_k)^2 & \\Delta x_k \\Delta y_k \\\\ \\Delta x_k \\Delta y_k & (\\Delta y_k)^2 \\end{bmatrix} $$\nwhere $\\Delta x_k = x_k - x_c$ and $\\Delta y_k = y_k - y_c$. The eigenvalues of this $2 \\times 2$ symmetric tensor, $\\mu_{\\max}$ and $\\mu_{\\min}$, represent the variance of the spread along its principal axes. The anisotropy ratio $\\rho$ is defined as:\n$$ \\rho = \\sqrt{\\frac{\\mu_{\\max}}{\\mu_{\\min} + \\epsilon}} $$\nwith $\\epsilon = 10^{-12}$ to prevent division by zero. A value of $\\rho \\approx 1$ indicates an isotropic (circular) spread, while $\\rho > 1$ indicates an anisotropic (elongated) spread.\n\nThis complete procedure is implemented in the following Python code and applied to each of the four specified test cases to compute and report the anisotropy ratios.\n\n```python\nimport numpy as np\n\ndef build_ray_families(n):\n    \"\"\"\n    Defines the 6 ray families based on the problem description for a grid of size n.\n    \"\"\"\n    k_vals = [1, 3, 5, 7, 9, 11, 13, 15]\n    \n    families = {\n        \"LR_hor\": [], \"LR_pos\": [], \"LR_neg\": [],\n        \"BT_ver\": [], \"BT_pos\": [], \"BT_neg\": []\n    }\n\n    # Left-to-Right families\n    y0_vals = [k / n for k in k_vals]\n    for y0 in y0_vals:\n        # LR_hor\n        families[\"LR_hor\"].append((np.array([0.0, y0]), np.array([1.0, y0])))\n        # LR_pos\n        families[\"LR_pos\"].append((np.array([0.0, y0]), np.array([1.0, min(1.0, y0 + 0.4)])))\n        # LR_neg\n        families[\"LR_neg\"].append((np.array([0.0, y0]), np.array([1.0, max(0.0, y0 - 0.4)])))\n\n    # Bottom-to-Top families\n    x0_vals = [k / n for k in k_vals]\n    for x0 in x0_vals:\n        # BT_ver\n        families[\"BT_ver\"].append((np.array([x0, 0.0]), np.array([x0, 1.0])))\n        # BT_pos\n        families[\"BT_pos\"].append((np.array([x0, 0.0]), np.array([min(1.0, x0 + 0.4), 1.0])))\n        # BT_neg\n        families[\"BT_neg\"].append((np.array([x0, 0.0]), np.array([max(0.0, x0 - 0.4), 1.0])))\n        \n    return families\n\ndef build_G(rays, n, S):\n    \"\"\"\n    Constructs the sensitivity matrix G by sampling rays.\n    \"\"\"\n    num_rays = len(rays)\n    N = n * n\n    G = np.zeros((num_rays, N))\n    \n    for r_idx, (p0, p1) in enumerate(rays):\n        length = np.linalg.norm(p1 - p0)\n        if length == 0:\n            continue\n        ds = length / S\n        direction_vec = (p1 - p0)\n        \n        for s in range(S):\n            # Sample point at the midpoint of the sub-segment\n            p = p0 + (s + 0.5) * direction_vec / S\n            x, y = p\n            \n            # Assign path length ds to the cell containing the sample point\n            if 0 <= x < 1.0 and 0 <= y < 1.0:\n                j = int(x * n)\n                i = int(y * n)\n                c_idx = i * n + j\n                G[r_idx, c_idx] += ds\n                \n    return G\n\ndef build_L(n, wx, wy):\n    \"\"\"\n    Constructs the first-order difference smoothing operator L.\n    \"\"\"\n    N = n * n\n    num_horiz_diffs = n * (n - 1)\n    num_vert_diffs = n * (n - 1)\n    L = np.zeros((num_horiz_diffs + num_vert_diffs, N))\n    \n    row_idx = 0\n    # Horizontal differences\n    for i in range(n):\n        for j in range(1, n):\n            c1 = i * n + j\n            c2 = i * n + (j - 1)\n            L[row_idx, c1] = wx\n            L[row_idx, c2] = -wx\n            row_idx += 1\n            \n    # Vertical differences\n    for i in range(1, n):\n        for j in range(n):\n            c1 = i * n + j\n            c2 = (i - 1) * n + j\n            L[row_idx, c1] = wy\n            L[row_idx, c2] = -wy\n            row_idx += 1\n            \n    return L\n    \ndef calculate_anisotropy(m_hat, n, i0, j0, epsilon):\n    \"\"\"\n    Computes the anisotropy ratio rho for a given point-spread function m_hat.\n    \"\"\"\n    N = n * n\n    \n    # Center of the analysis cell (c_0)\n    xc = (j0 + 0.5) / n\n    yc = (i0 + 0.5) / n\n    \n    # All cell center coordinates\n    j_coords = np.tile(np.arange(n), n)\n    i_coords = np.repeat(np.arange(n), n)\n    x_k = (j_coords + 0.5) / n\n    y_k = (i_coords + 0.5) / n\n    \n    # Displacements from the center\n    dx_k = x_k - xc\n    dy_k = y_k - yc\n    \n    # Weights for second-moment tensor are the squared PSF values\n    w_k = m_hat**2\n    w_sum = np.sum(w_k)\n    \n    if w_sum <= 1e-15: # Effectively zero, spread is undefined\n        return 1.0 # Default to isotropic\n        \n    # Second-moment tensor M\n    M = np.zeros((2, 2))\n    M[0, 0] = np.sum(w_k * dx_k**2) / w_sum\n    M[1, 1] = np.sum(w_k * dy_k**2) / w_sum\n    M[0, 1] = M[1, 0] = np.sum(w_k * dx_k * dy_k) / w_sum\n    \n    # Eigenvalues of M\n    eigvals = np.linalg.eigvalsh(M)\n    mu_min, mu_max = eigvals[0], eigvals[1]\n    \n    # Anisotropy ratio rho\n    rho = np.sqrt(mu_max / (mu_min + epsilon))\n    \n    return rho\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and compute anisotropy ratios.\n    \"\"\"\n    n = 16\n    S = 200\n    i0, j0 = 8, 8\n    c0 = i0 * n + j0\n    epsilon = 1e-12\n    \n    all_ray_families = build_ray_families(n)\n    \n    test_cases = [\n        {\"families\": [\"LR_hor\", \"LR_pos\", \"LR_neg\", \"BT_ver\", \"BT_pos\", \"BT_neg\"], \"wx\": 1.0, \"wy\": 1.0, \"lambda_val\": 0.1},\n        {\"families\": [\"BT_ver\"], \"wx\": 1.0, \"wy\": 1.0, \"lambda_val\": 0.1},\n        {\"families\": [\"LR_hor\", \"LR_pos\", \"LR_neg\", \"BT_ver\", \"BT_pos\", \"BT_neg\"], \"wx\": 1.0, \"wy\": 1.0, \"lambda_val\": 1.0},\n        {\"families\": [\"LR_hor\", \"LR_pos\", \"LR_neg\", \"BT_ver\", \"BT_pos\", \"BT_neg\"], \"wx\": 5.0, \"wy\": 1.0, \"lambda_val\": 0.1},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Setup rays and build the sensitivity matrix G\n        rays = []\n        for fam_name in case[\"families\"]:\n            rays.extend(all_ray_families[fam_name])\n        G = build_G(rays, n, S)\n        \n        # 2. Build the smoothing operator L\n        L = build_L(n, case[\"wx\"], case[\"wy\"])\n        \n        # 3. Form and solve the normal equations for the Point-Spread Function (PSF)\n        lambda_sq = case[\"lambda_val\"]**2\n        \n        # System matrix A = G^T G + lambda^2 L^T L\n        A = G.T @ G + lambda_sq * (L.T @ L)\n        \n        # Right-hand side b = G^T g_{c0}, where g_{c0} is the c0-th column of G\n        g_c0 = G[:, c0]\n        b = G.T @ g_c0\n        \n        # Solve A * m_hat = b for the PSF m_hat\n        m_hat = np.linalg.solve(A, b)\n        \n        # 4. Calculate the anisotropy ratio rho\n        rho = calculate_anisotropy(m_hat, n, i0, j0, epsilon)\n        results.append(rho)\n        \n    # Format and print the final results as specified\n    return f\"[{','.join(f'{r:.3f}' for r in results)}]\"\n\n# The following line is for conceptual execution; the final answer is pre-computed and placed in the answer tag.\n# print(solve())\n```",
            "answer": "$$\n\\boxed{[1.066, 6.331, 1.018, 2.771]}\n$$"
        }
    ]
}