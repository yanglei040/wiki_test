{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we address the most fundamental question in model-based sparse approximation: how to find the best approximation of a given signal within a specific structured-sparsity model. This exercise asks you to compute the exact Euclidean projection of a vector onto a set of signals that are sparse on a connected subtree. By working through this problem, you will gain a concrete understanding of what a tree-structured model entails and how the optimal approximation is found by identifying the support that preserves the maximum signal energy .",
            "id": "3450706",
            "problem": "Consider a hierarchical model of tree-structured sparsity as follows. Let $v \\in \\mathbb{R}^{8}$ be the data vector given by $v = (0.9, -0.3, 2.1, 0.2, -0.1, 0.5, -3.0, 2.6)^{\\top}$. Let the tree $\\mathcal{T}$ on indices $\\{1,2,3,4,5,6,7,8\\}$ be a rooted tree of depth $3$ with root at index $1$, children of index $1$ equal to indices $2$ and $3$, children of index $2$ equal to indices $4$ and $5$, and children of index $3$ equal to indices $6$, $7$, and $8$. A subset $A \\subseteq \\{1,\\dots,8\\}$ is said to be a connected subtree support if the induced subgraph of $\\mathcal{T}$ on $A$ is connected.\n\nDefine the model $\\mathcal{M}_{3}$ to be the union of all coordinate subspaces corresponding to connected subtree supports of cardinality $3$, that is, every $x \\in \\mathcal{M}_{3}$ has support equal to some $A \\subseteq \\{1,\\dots,8\\}$ satisfying $|A| = 3$ and $A$ is a connected subtree support.\n\nUsing the Euclidean norm $\\|\\cdot\\|_{2}$, compute the exact Euclidean projection of $v$ onto $\\mathcal{M}_{3}$, understood as the element $w^{\\star} \\in \\mathcal{M}_{3}$ that minimizes $\\|v - w\\|_{2}$ over $w \\in \\mathcal{M}_{3}$. Your derivation must start from first principles (definitions of Euclidean projection onto a union of coordinate subspaces and connectedness in trees), and must justify why the selected support is optimal under the connected subtree constraint.\n\nFor your final answer, concatenate the $3$ selected indices in increasing order followed by the $8$ entries of the projected vector into a single row matrix. No rounding is required. The final answer must be a single expression.",
            "solution": "The problem requires the computation of the Euclidean projection of a given vector $v \\in \\mathbb{R}^{8}$ onto a non-convex set $\\mathcal{M}_{3}$. The set $\\mathcal{M}_{3}$ is defined as the union of specific coordinate subspaces, selected according to a tree-structured sparsity model.\n\nLet the vector be $v = (0.9, -0.3, 2.1, 0.2, -0.1, 0.5, -3.0, 2.6)^{\\top}$. The tree $\\mathcal{T}$ on the index set $\\{1, 2, 3, 4, 5, 6, 7, 8\\}$ is defined by the following parent-child relationships: the root is node $1$; node $1$ has children $\\{2, 3\\}$; node $2$ has children $\\{4, 5\\}$; and node $3$ has children $\\{6, 7, 8\\}$.\n\nThe model $\\mathcal{M}_{3}$ is the union of all coordinate subspaces $S_A$ where the support set $A \\subseteq \\{1, \\dots, 8\\}$ has cardinality $|A|=3$ and forms a connected subtree in $\\mathcal{T}$. Formally, $\\mathcal{M}_{3} = \\bigcup_{A \\in \\mathcal{A}_3} S_A$, where $\\mathcal{A}_3 = \\{ A \\subseteq \\{1, \\dots, 8\\} : |A|=3 \\text{ and } A \\text{ is a connected subtree support} \\}$ and $S_A = \\{ x \\in \\mathbb{R}^8 : x_i = 0 \\text{ for all } i \\notin A \\}$.\n\nWe seek to find the vector $w^{\\star} \\in \\mathcal{M}_{3}$ that minimizes the Euclidean distance to $v$, i.e.,\n$$w^{\\star} = \\arg\\min_{w \\in \\mathcal{M}_{3}} \\|v - w\\|_{2}$$\nSince $\\mathcal{M}_{3}$ is a union of linear subspaces, the optimal vector $w^{\\star}$ must be the orthogonal projection of $v$ onto one of these subspaces. Let $P_{S_A}(v)$ denote the orthogonal projection of $v$ onto the subspace $S_A$. The optimization problem is equivalent to finding the subspace $S_{A^{\\star}}$ such that the projection is closest to $v$:\n$$w^{\\star} = P_{S_{A^{\\star}}}(v) \\quad \\text{where} \\quad A^{\\star} = \\arg\\min_{A \\in \\mathcal{A}_3} \\|v - P_{S_A}(v)\\|_{2}$$\nMinimizing the distance is equivalent to minimizing the squared distance. The squared distance from $v$ to its projection on $S_A$ is given by the sum of squares of the components of $v$ that are zeroed out:\n$$\\|v - P_{S_A}(v)\\|_{2}^{2} = \\sum_{i \\notin A} v_i^2$$\nUsing the Pythagorean theorem, $\\|v\\|_{2}^{2} = \\|P_{S_A}(v)\\|_{2}^{2} + \\|v - P_{S_A}(v)\\|_{2}^{2}$, we can rewrite the expression as:\n$$\\|v - P_{S_A}(v)\\|_{2}^{2} = \\|v\\|_{2}^{2} - \\|P_{S_A}(v)\\|_{2}^{2} = \\sum_{i=1}^{8} v_i^2 - \\sum_{i \\in A} v_i^2$$\nSince $\\sum_{i=1}^{8} v_i^2$ is a constant with respect to the choice of $A$, minimizing the distance is equivalent to maximizing the sum of squares of the components of $v$ over the support set $A$:\n$$A^{\\star} = \\arg\\max_{A \\in \\mathcal{A}_3} \\sum_{i \\in A} v_i^2$$\n\nOur first step is to enumerate all possible connected subtree supports $A$ of cardinality $3$. A connected graph of $3$ nodes on a tree can only have two topologies: a path of length $2$ (grandparent-parent-child) or a star graph with a central node and two children (parent-and-two-children).\n\nThe tree structure is specified as:\n\\begin{itemize}\n    \\item Edges: $(1,2), (1,3), (2,4), (2,5), (3,6), (3,7), (3,8)$.\n\\end{itemize}\nWe list all connected subtrees of size $3$:\n\\begin{enumerate}\n    \\item Path-like subtrees (grandparent-parent-child):\n    \\begin{itemize}\n        \\item $\\{1, 2, 4\\}$\n        \\item $\\{1, 2, 5\\}$\n        \\item $\\{1, 3, 6\\}$\n        \\item $\\{1, 3, 7\\}$\n        \\item $\\{1, 3, 8\\}$\n    \\end{itemize}\n    \\item Star-like subtrees (parent and two children):\n    \\begin{itemize}\n        \\item $\\{1, 2, 3\\}$ (parent $1$, children $2, 3$)\n        \\item $\\{2, 4, 5\\}$ (parent $2$, children $4, 5$)\n        \\item $\\{3, 6, 7\\}$ (parent $3$, children $6, 7$)\n        \\item $\\{3, 6, 8\\}$ (parent $3$, children $6, 8$)\n        \\item $\\{3, 7, 8\\}$ (parent $3$, children $7, 8$)\n    \\end{itemize}\n\\end{enumerate}\nThis constitutes the complete set $\\mathcal{A}_3$ of ten possible supports.\n\nNext, we calculate the squared values of the components of $v$:\n$v_1^2 = (0.9)^2 = 0.81$\n$v_2^2 = (-0.3)^2 = 0.09$\n$v_3^2 = (2.1)^2 = 4.41$\n$v_4^2 = (0.2)^2 = 0.04$\n$v_5^2 = (-0.1)^2 = 0.01$\n$v_6^2 = (0.5)^2 = 0.25$\n$v_7^2 = (-3.0)^2 = 9.00$\n$v_8^2 = (2.6)^2 = 6.76$\n\nNow, we compute the sum $\\sum_{i \\in A} v_i^2$ for each support $A \\in \\mathcal{A}_3$:\n\\begin{itemize}\n    \\item $A = \\{1, 2, 4\\}: \\sum v_i^2 = 0.81 + 0.09 + 0.04 = 0.94$\n    \\item $A = \\{1, 2, 5\\}: \\sum v_i^2 = 0.81 + 0.09 + 0.01 = 0.91$\n    \\item $A = \\{1, 3, 6\\}: \\sum v_i^2 = 0.81 + 4.41 + 0.25 = 5.47$\n    \\item $A = \\{1, 3, 7\\}: \\sum v_i^2 = 0.81 + 4.41 + 9.00 = 14.22$\n    \\item $A = \\{1, 3, 8\\}: \\sum v_i^2 = 0.81 + 4.41 + 6.76 = 11.98$\n    \\item $A = \\{1, 2, 3\\}: \\sum v_i^2 = 0.81 + 0.09 + 4.41 = 5.31$\n    \\item $A = \\{2, 4, 5\\}: \\sum v_i^2 = 0.09 + 0.04 + 0.01 = 0.14$\n    \\item $A = \\{3, 6, 7\\}: \\sum v_i^2 = 4.41 + 0.25 + 9.00 = 13.66$\n    \\item $A = \\{3, 6, 8\\}: \\sum v_i^2 = 4.41 + 0.25 + 6.76 = 11.42$\n    \\item $A = \\{3, 7, 8\\}: \\sum v_i^2 = 4.41 + 9.00 + 6.76 = 20.17$\n\\end{itemize}\n\nComparing these sums, the maximum value is $20.17$, which corresponds to the support set $A^{\\star} = \\{3, 7, 8\\}$. The induced subgraph on these vertices consists of edges $(3,7)$ and $(3,8)$, which is indeed a connected tree structure. Because this sum is uniquely the maximum, the projection is unique.\n\nThe projection $w^{\\star}$ of $v$ onto $\\mathcal{M}_{3}$ is therefore the projection onto the subspace $S_{A^{\\star}}$, which is obtained by keeping the components of $v$ at indices $\\{3, 7, 8\\}$ and setting all other components to zero.\nGiven $v = (0.9, -0.3, 2.1, 0.2, -0.1, 0.5, -3.0, 2.6)^{\\top}$, the projected vector $w^{\\star}$ is:\n$$w^{\\star} = (0, 0, v_3, 0, 0, 0, v_7, v_8)^{\\top} = (0, 0, 2.1, 0, 0, 0, -3.0, 2.6)^{\\top}$$\n\nThe final answer requires concatenating the $3$ selected indices in increasing order ($3, 7, 8$) followed by the $8$ entries of the projected vector $w^{\\star}$.\nThis results in a row vector of $3+8=11$ elements: $(3, 7, 8, 0, 0, 2.1, 0, 0, 0, -3.0, 2.6)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 7 & 8 & 0 & 0 & 2.1 & 0 & 0 & 0 & -3.0 & 2.6\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Having explored how to find a tree-sparse approximation, we now turn to the theoretical foundations of signal recovery. This problem challenges you to determine the minimum number of measurements required to reliably distinguish between different tree-sparse signals in the presence of noise, a core question in compressed sensing. You will use a classic volume packing argument, a powerful tool from information theory, to derive a lower bound, revealing the fundamental trade-off between measurement resources and the complexity of the signal class being recovered .",
            "id": "3450711",
            "problem": "Consider a fixed linear measurement operator $A \\in \\mathbb{R}^{m \\times n}$ with operator norm $\\|A\\|_{2 \\to 2} \\leq 1$, whose $n$ columns are indexed by the nodes of a rooted, balanced $b$-ary tree $\\mathcal{T}$ of depth $L$, so that $n = \\frac{b^{L+1} - 1}{b - 1}$. A support set is said to satisfy subtree closure if, whenever a node is in the set, all of its ancestors (up to the root) are also in the set. A vector $x \\in \\mathbb{R}^{n}$ is said to be tree-sparse if its support set satisfies subtree closure. Measurements are corrupted by additive noise $\\eta \\in \\mathbb{R}^{m}$ with $\\|\\eta\\|_{2} \\leq \\sigma$, and the observation is $y = A x + \\eta$.\n\nFix integers $r \\geq 1$ and $\\ell$ with $1 \\leq \\ell \\leq L - 1$. Construct a family $\\mathcal{S}$ of $M = b^{r}$ distinct unit-norm, tree-sparse vectors as follows: choose $r$ distinct internal nodes at depth $\\ell$, and for each such node independently pick exactly one of its $b$ children to include; the support of each vector consists of the chosen children together with all ancestors (ensuring subtree closure), so that each $x \\in \\mathcal{S}$ has exactly $k = r(\\ell + 1)$ nonzero entries. The coefficients of each $x \\in \\mathcal{S}$ are chosen so that most of the $\\ell_{2}$ energy is concentrated at the selected children (branching points), and then the vector is scaled to satisfy $\\|x\\|_{2} = 1$.\n\nAssume recovery must succeed for every $x \\in \\mathcal{S}$ and every noise realization $\\eta$ with $\\|\\eta\\|_{2} \\leq \\sigma$. In particular, to rule out confusion caused by noise, it is required that the measurement images of distinct signals in $\\mathcal{S}$ be sufficiently separated so that any two distinct $x_{1}, x_{2} \\in \\mathcal{S}$ satisfy $\\|A x_{1} - A x_{2}\\|_{2} \\geq 4 \\sigma$. Using only first principles about Euclidean volume packing and the above construction, derive a lower bound on the necessary measurement dimension $m$ for such guaranteed recovery in terms of $b$, $r$, and $\\sigma$.\n\nProvide your final answer as a single closed-form analytical expression in $b$, $r$, and $\\sigma$.",
            "solution": "The problem asks for a lower bound on the measurement dimension $m$ using a geometric argument known as a volume packing bound.\n\nLet $\\mathcal{S}$ be the family of $M = b^r$ distinct signals constructed as described. For each signal $x \\in \\mathcal{S}$, its image under the measurement operator is $Ax \\in \\mathbb{R}^m$. Let $\\mathcal{Z} = \\{Ax \\mid x \\in \\mathcal{S}\\}$ be this set of images in the measurement space.\n\nWe are given two crucial properties of this set $\\mathcal{Z}$:\n1.  All points in $\\mathcal{Z}$ are contained within a ball of a certain radius centered at the origin. Since $\\|A\\|_{2 \\to 2} \\le 1$ and $\\|x\\|_2 = 1$ for all $x \\in \\mathcal{S}$, we have $\\|Ax\\|_2 \\le \\|A\\|_{2 \\to 2} \\|x\\|_2 \\le 1 \\cdot 1 = 1$. Thus, all points in $\\mathcal{Z}$ lie within the closed unit ball in $\\mathbb{R}^m$, denoted $B_m(0, 1)$.\n\n2.  The points in $\\mathcal{Z}$ are well-separated. The robust recovery condition states that for any two distinct signals $x_1, x_2 \\in \\mathcal{S}$, we must have $\\|Ax_1 - Ax_2\\|_2 \\ge 4\\sigma$.\n\nThis separation condition allows us to place disjoint open balls around each point in $\\mathcal{Z}$. Let us center an open ball of radius $\\rho = 2\\sigma$ at each point $Ax_i \\in \\mathcal{Z}$. The distance between the centers of any two such balls, $Ax_i$ and $Ax_j$ for $i \\ne j$, is $\\|Ax_i - Ax_j\\|_2 \\ge 4\\sigma = 2\\rho$. Since the distance between centers is at least the sum of their radii, these $M$ balls are disjoint.\n\nNow, we must find a larger ball that contains this collection of $M$ disjoint balls. Consider any point $p$ inside one of these small balls, say $p \\in B(Ax_i, 2\\sigma)$. By the triangle inequality, its distance from the origin is bounded:\n$$\n\\|p\\|_2 = \\|p - Ax_i + Ax_i\\|_2 \\le \\|p - Ax_i\\|_2 + \\|Ax_i\\|_2.\n$$\nSince $p$ is in the open ball, $\\|p - Ax_i\\|_2  2\\sigma$. We also know that $\\|Ax_i\\|_2 \\le 1$. Therefore,\n$$\n\\|p\\|_2  2\\sigma + 1.\n$$\nThis shows that all $M$ disjoint balls, each of radius $2\\sigma$, are contained within a single larger ball of radius $1+2\\sigma$ centered at the origin, $B_m(0, 1+2\\sigma)$.\n\nThe volume packing principle states that the sum of the volumes of disjoint bodies must be less than or equal to the volume of any body that contains them. The volume of an $m$-dimensional ball of radius $R$ is $V_m(R) = C_m R^m$, where $C_m = \\frac{\\pi^{m/2}}{\\Gamma(m/2+1)}$ is a constant dependent only on the dimension $m$.\n\nApplying the principle, we get:\n$$\n\\sum_{i=1}^M \\text{Volume}(B(Ax_i, 2\\sigma)) \\le \\text{Volume}(B_m(0, 1+2\\sigma))\n$$\n$$\nM \\cdot V_m(2\\sigma) \\le V_m(1+2\\sigma)\n$$\nSubstituting the formula for the volume and the value of $M=b^r$:\n$$\nb^r \\cdot C_m (2\\sigma)^m \\le C_m (1+2\\sigma)^m\n$$\nAssuming $\\sigma > 0$, we can cancel the positive term $C_m(2\\sigma)^m$ after rearranging:\n$$\nb^r \\le \\frac{(1+2\\sigma)^m}{(2\\sigma)^m} = \\left(\\frac{1+2\\sigma}{2\\sigma}\\right)^m = \\left(1 + \\frac{1}{2\\sigma}\\right)^m\n$$\nTo solve for $m$, we take the natural logarithm of both sides. As $\\ln(\\cdot)$ is a monotonically increasing function and both sides are greater than 1 (for $b>1, r \\ge 1$), the inequality is preserved:\n$$\n\\ln(b^r) \\le \\ln\\left(\\left(1 + \\frac{1}{2\\sigma}\\right)^m\\right)\n$$\n$$\nr \\ln(b) \\le m \\ln\\left(1 + \\frac{1}{2\\sigma}\\right)\n$$\nSince $\\sigma > 0$, the term $\\ln\\left(1 + \\frac{1}{2\\sigma}\\right)$ is positive, so we can divide by it without changing the inequality's direction:\n$$\nm \\ge \\frac{r \\ln(b)}{\\ln\\left(1 + \\frac{1}{2\\sigma}\\right)}\n$$\nThis provides the required lower bound on the measurement dimension $m$.",
            "answer": "$$\n\\boxed{\\frac{r \\ln(b)}{\\ln\\left(1 + \\frac{1}{2\\sigma}\\right)}}\n$$"
        },
        {
            "introduction": "Our final practice bridges the gap between theory and scalable computation, addressing the limitations of the brute-force approach from our first exercise. We explore how to efficiently solve large-scale tree-structured optimization problems using the elegant framework of submodular optimization and convex relaxation. This advanced problem guides you through deriving a convex surrogate for the non-convex sparsity penalty and shows how its associated proximal operator can be computed efficiently using a minimum cut algorithm on an auxiliary graph, demonstrating a beautiful synergy between continuous optimization and discrete algorithms .",
            "id": "3450704",
            "problem": "Consider a rooted tree $\\mathcal{T}=(V,E)$ with directed edges from parent to child. Let $V=\\{1,2,\\dots,n\\}$ denote the nodes. Define the set function (penalty) $F:2^{V}\\to\\mathbb{R}_{\\ge 0}$ by\n$$\nF(S) \\;=\\; \\sum_{(p,c)\\in E} w_{pc}\\,\\mathbf{1}\\{c\\in S,\\,p\\notin S\\},\n$$\nwhere $(p,c)\\in E$ denotes a directed edge from parent $p$ to child $c$, $w_{pc}0$ are given edge weights, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This penalty charges a cost whenever a child node is activated without its parent being activated, thereby penalizing disconnected activations in the hierarchy induced by $\\mathcal{T}$.\n\nProblem tasks:\n\n1) Starting from the definitions of submodularity and the Lovász extension, show that $F$ is submodular and derive the Lovász extension $f^{L}:\\mathbb{R}^{n}\\to\\mathbb{R}$ of $F$. Prove that $f^{L}$ is convex. Your derivation must start from the fundamental definition that a set function $G$ is submodular if and only if for all subsets $A\\subseteq B\\subseteq V$ and elements $i\\in V\\setminus B$, one has $G(A\\cup\\{i\\})-G(A)\\ge G(B\\cup\\{i\\})-G(B)$, and from the definition of the Lovász extension via level sets. Avoid using shortcut formulas not justified from these bases.\n\n2) Consider the proximal mapping of the convex surrogate $f^{L}$:\n$$\n\\operatorname{prox}_{\\lambda f^{L}}(y)\\;=\\;\\arg\\min_{z\\in\\mathbb{R}^{n}}\\;\\frac{1}{2}\\,\\|z-y\\|_{2}^{2}\\;+\\;\\lambda\\, f^{L}(z),\n$$\nwhere $\\lambda0$ and $y\\in\\mathbb{R}^{n}$ is given. Derive the dual formulation showing that\n$$\n\\operatorname{prox}_{\\lambda f^{L}}(y)\\;=\\;y-\\lambda\\,s^{\\star},\\quad\\text{where}\\quad s^{\\star}=\\operatorname{proj}_{B(F)}\\!\\left(\\frac{y}{\\lambda}\\right),\n$$\nand $B(F)$ is the base polyhedron of $F$ defined by\n$$\nB(F)\\;=\\;\\left\\{s\\in\\mathbb{R}^{n}:\\;\\sum_{i\\in S}s_{i}\\le F(S)\\quad\\forall\\,S\\subseteq V,\\quad\\sum_{i\\in V}s_{i}=F(V)\\right\\}.\n$$\nYou must start from the convex conjugacy fact that the convex conjugate of the Lovász extension is the indicator of the base polyhedron, and rigorously derive the projection characterization.\n\n3) Implement an algorithm to compute $s^{\\star}=\\operatorname{proj}_{B(F)}(y/\\lambda)$ using an active-set cutting-plane method whose separation oracle is a Minimum Cut (min-cut) on an auxiliary graph. Specifically, for any vector $s\\in\\mathbb{R}^{n}$, the most violated inequality of the form $\\sum_{i\\in S} s_{i}\\le F(S)$ can be found by minimizing $F(S)-\\sum_{i\\in S}s_{i}$ over $S\\subseteq V$. Show that this minimization reduces to an $s$-$t$ min-cut on a directed graph constructed as follows:\n- Create a source node and a sink node.\n- For each $i\\in V$, add a directed edge from source to $i$ with capacity $\\max(s_{i},0)$ and a directed edge from $i$ to sink with capacity $\\max(-s_{i},0)$.\n- For each edge $(p,c)\\in E$, add a directed edge from $c$ to $p$ with capacity $w_{pc}$.\nArgue that the cut cost equals $F(S)-\\sum_{i\\in S}s_{i}$ up to a constant independent of $S$, and therefore a minimum $s$-$t$ cut yields the minimizer $S$.\n\nUsing this, implement the active-set projection algorithm:\n- Start with the equality constraint $\\sum_{i\\in V}s_{i}=F(V)$.\n- Iteratively find the most violated inequality via min-cut; if violated, add it as an active equality constraint and project $y/\\lambda$ onto the intersection of the current active constraints; repeat until no violation remains.\nReturn $s^{\\star}$ and compute $\\operatorname{prox}_{\\lambda f^{L}}(y)=y-\\lambda s^{\\star}$.\n\nImplementation requirements:\n\n- Your final answer must be a complete, runnable program that performs the proximal mapping for the following test suite. For each test case, the input is specified by $(V,E,w,y,\\lambda)$; treat $V$ as the set $\\{1,\\dots,n\\}$ in node index order.\n\nTest suite:\n\n- Case 1 (General hierarchical smoothing on a 5-node tree): $n=5$, edges $E=\\{(1,2),(1,3),(3,4),(3,5)\\}$ with weights $w_{pc}=1$ for all edges, $y=[0.9,-0.2,0.4,1.1,-0.5]$, $\\lambda=0.7$.\n\n- Case 2 (Boundary condition $\\lambda=0$): Same tree and weights, $y=[0.9,-0.2,0.4,1.1,-0.5]$, $\\lambda=0$.\n\n- Case 3 (Uniform $y$ where the hinge surrogate can be nullified): Same tree and weights, $y=[0.5,0.5,0.5,0.5,0.5]$, $\\lambda=1.0$.\n\n- Case 4 (Small tree with mixed signs): $n=4$, edges $E=\\{(1,2),(2,3),(2,4)\\}$ with weights $w_{pc}=\\{(1,2):2.0,(2,3):0.5,(2,4):1.5\\}$, $y=[-0.3,0.8,-1.2,0.6]$, $\\lambda=0.6$.\n\n- Case 5 (Stronger regularization): Same as Case 1 but with $\\lambda=2.5$.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the proximal results for each test case, aggregated as a comma-separated list of lists enclosed in square brackets. Each inner list must contain the $n$ floating-point entries of $\\operatorname{prox}_{\\lambda f^{L}}(y)$ for that case, rounded to six decimal places, with no spaces. For example: \"[[0.123456,0.654321],[...],...]\".",
            "solution": "The problem asks for a theoretical derivation and algorithmic implementation for computing the proximal operator of a tree-structured sparse penalty function. The problem is valid as it is scientifically grounded in convex optimization and sparse modeling, well-posed, objective, and provides a complete and consistent setup. We proceed with the solution.\n\n### Part 1: Submodularity and Lovász Extension\n\n**Submodularity of $F(S)$**\n\nA set function $G: 2^V \\to \\mathbb{R}$ is submodular if for all subsets $A \\subseteq B \\subseteq V$ and any element $i \\in V \\setminus B$, the diminishing returns property holds:\n$$\nG(A \\cup \\{i\\}) - G(A) \\ge G(B \\cup \\{i\\}) - G(B).\n$$\nWe define the marginal gain of adding element $i$ to a set $S$ (where $i \\notin S$) as $\\Delta_G(S, i) = G(S \\cup \\{i\\}) - G(S)$. The condition for submodularity is that $\\Delta_G(S, i)$ is a non-increasing function of $S$.\n\nThe given set function is $F(S) = \\sum_{(p,c)\\in E} w_{pc}\\,\\mathbf{1}\\{c\\in S,\\,p\\notin S\\}$. Let us compute the marginal gain $\\Delta_F(S, i)$ for some $i \\notin S$.\n$$\n\\Delta_F(S, i) = F(S \\cup \\{i\\}) - F(S) = \\sum_{(p,c)\\in E} w_{pc} \\left( \\mathbf{1}\\{c\\in S\\cup\\{i\\},\\,p\\notin S\\cup\\{i\\}\\} - \\mathbf{1}\\{c\\in S,\\,p\\notin S\\} \\right).\n$$\nThe terms in the sum are non-zero only if the edge $(p,c)$ is incident to the node $i$. We consider two cases for an edge $(p,c)$:\n1.  Node $i$ is the child, $c=i$. The contribution to the sum from all such edges is:\n    $$\n    \\sum_{p:(p,i)\\in E} w_{pi} \\left( \\mathbf{1}\\{i\\in S\\cup\\{i\\},\\,p\\notin S\\cup\\{i\\}\\} - \\mathbf{1}\\{i\\in S,\\,p\\notin S\\} \\right).\n    $$\n    Since $i \\notin S$, the second indicator is always $0$. For the first indicator, $i\\in S\\cup\\{i\\}$ is true, and $p\\notin S\\cup\\{i\\}$ is equivalent to $p\\notin S$ (since $p\\neq i$). So, this part contributes $\\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin S\\}$.\n2.  Node $i$ is the parent, $p=i$. Let $C(i)$ be the set of children of $i$. The contribution is:\n    $$\n    \\sum_{c\\in C(i)} w_{ic} \\left( \\mathbf{1}\\{c\\in S\\cup\\{i\\},\\,i\\notin S\\cup\\{i\\}\\} - \\mathbf{1}\\{c\\in S,\\,i\\notin S\\} \\right).\n    $$\n    Since $i\\notin S$, the condition $i\\notin S\\cup\\{i\\}$ in the first indicator is false, making it $0$. The condition $i\\notin S$ in the second indicator is true. So, this part contributes $\\sum_{c\\in C(i)} w_{ic} (0 - \\mathbf{1}\\{c\\in S\\}) = -\\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in S\\}$.\n\nCombining these two cases, the marginal gain is:\n$$\n\\Delta_F(S, i) = \\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin S\\} - \\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in S\\}.\n$$\nNow, let $A \\subseteq B \\subseteq V$ with $i \\notin B$. We compare $\\Delta_F(A, i)$ and $\\Delta_F(B, i)$.\nSince $A \\subseteq B$, we have:\n- For any node $p$, $\\mathbf{1}\\{p\\notin A\\} \\ge \\mathbf{1}\\{p\\notin B\\}$.\n- For any node $c$, $\\mathbf{1}\\{c\\in A\\} \\le \\mathbf{1}\\{c\\in B\\}$.\n\nGiven that weights $w_{pc} > 0$, we can conclude:\n- $\\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin A\\} \\ge \\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin B\\}$.\n- $-\\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in A\\} \\ge -\\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in B\\}$.\n\nSumming these inequalities, we get $\\Delta_F(A, i) \\ge \\Delta_F(B, i)$. Thus, $F$ is a submodular function.\n\n**Lovász Extension and its Convexity**\n\nThe Lovász extension $f^L: \\mathbb{R}^n \\to \\mathbb{R}$ of a set function $F$ can be defined via its level sets:\n$$\nf^L(z) = \\int_{-\\infty}^{\\infty} F(\\{i \\in V \\mid z_i \\ge t\\}) dt.\n$$\nSubstituting the definition of $F(S)$, we have:\n$$\nf^L(z) = \\int_{-\\infty}^{\\infty} \\left( \\sum_{(p,c)\\in E} w_{pc}\\,\\mathbf{1}\\{c\\in \\{i \\mid z_i \\ge t\\},\\,p\\notin \\{i \\mid z_i \\ge t\\}\\} \\right) dt.\n$$\nBy linearity of integration, we can swap the sum and the integral:\n$$\nf^L(z) = \\sum_{(p,c)\\in E} w_{pc} \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{c\\in \\{i \\mid z_i \\ge t\\}\\ \\text{and}\\ p\\notin \\{i \\mid z_i \\ge t\\}\\} dt.\n$$\nThe condition inside the indicator function is equivalent to $z_c \\ge t$ and $z_p  t$, which simplifies to $z_p  t \\le z_c$. The integral of the indicator function over $t$ calculates the length of the interval $(z_p, z_c]$. The length is $z_c - z_p$ if $z_c > z_p$, and $0$ otherwise. This is precisely the function $\\max(z_c - z_p, 0)$.\nTherefore, the Lovász extension of $F$ is:\n$$\nf^L(z) = \\sum_{(p,c)\\in E} w_{pc} \\max(z_c - z_p, 0).\n$$\nTo prove that $f^L(z)$ is convex, we note that for each edge $(p,c)$, the function $z \\mapsto z_c - z_p$ is an affine, and hence convex, function. The function $x \\mapsto \\max(x, 0)$ is convex. The composition of a convex function with an affine map is convex, so $z \\mapsto \\max(z_c - z_p, 0)$ is convex. Since $f^L(z)$ is a non-negative weighted sum of convex functions (as $w_{pc} > 0$), it is also a convex function.\n\n### Part 2: Proximal Operator Duality\n\nThe proximal mapping is defined as the solution to the optimization problem:\n$$\n\\operatorname{prox}_{\\lambda f^{L}}(y) = \\arg\\min_{z\\in\\mathbb{R}^{n}}\\left\\{ \\frac{1}{2}\\,\\|z-y\\|_{2}^{2}\\;+\\;\\lambda\\, f^{L}(z) \\right\\}.\n$$\nThis problem is of the form $\\min_z G(z) + H(z)$, where $G(z) = \\frac{1}{2}\\|z-y\\|_2^2$ and $H(z) = \\lambda f^L(z)$. We can solve this using Fenchel-Rockafellar duality. The dual problem is $\\max_s -G^*(-s) - H^*(s)$, where $G^*$ and $H^*$ are the convex conjugates.\n\nFirst, we find the conjugate of $G(z)$:\n$$\nG^*(s) = \\sup_{z} \\left( \\langle s, z \\rangle - \\frac{1}{2}\\|z-y\\|_2^2 \\right).\n$$\nThe supremum is achieved when the gradient with respect to $z$ is zero: $s - (z-y) = 0 \\implies z = s+y$. Substituting back yields:\n$$\nG^*(s) = \\langle s, s+y \\rangle - \\frac{1}{2}\\|(s+y)-y\\|_2^2 = \\|s\\|_2^2 + \\langle s, y \\rangle - \\frac{1}{2}\\|s\\|_2^2 = \\frac{1}{2}\\|s\\|_2^2 + \\langle s, y \\rangle.\n$$\nNext, we find the conjugate of $H(z)$:\n$$\nH^*(s) = \\sup_{z} \\left( \\langle s, z \\rangle - \\lambda f^L(z) \\right) = \\lambda \\sup_{z} \\left( \\langle s/\\lambda, z \\rangle - f^L(z) \\right) = \\lambda (f^L)^*(s/\\lambda).\n$$\nWe are given that the conjugate of the Lovász extension, $(f^L)^*$, is the indicator function of the base polyhedron $B(F)$, denoted $\\delta_{B(F)}$. Thus, $(f^L)^*(v) = 0$ if $v \\in B(F)$ and $+\\infty$ otherwise.\n$H^*(s) = \\lambda \\delta_{B(F)}(s/\\lambda)$, which is the indicator function for the scaled polyhedron $\\lambda B(F)$. So, $H^*(s) = 0$ if $s/\\lambda \\in B(F)$ and $+\\infty$ otherwise.\n\nThe dual problem is:\n$$\n\\max_{s} \\left\\{ -G^*(-s) - H^*(s) \\right\\} = \\max_{s \\in \\lambda B(F)} \\left\\{ -(\\frac{1}{2}\\|-s\\|_2^2 + \\langle -s, y \\rangle) \\right\\} = \\max_{s \\in \\lambda B(F)} \\left\\{ -\\frac{1}{2}\\|s\\|_2^2 + \\langle s, y \\rangle \\right\\}.\n$$\nThis maximization is equivalent to the minimization problem:\n$$\n\\min_{s \\in \\lambda B(F)} \\left\\{ \\frac{1}{2}\\|s\\|_2^2 - \\langle s, y \\rangle \\right\\}.\n$$\nBy completing the square, this becomes:\n$$\n\\min_{s \\in \\lambda B(F)} \\left\\{ \\frac{1}{2}\\|s-y\\|_2^2 - \\frac{1}{2}\\|y\\|_2^2 \\right\\}.\n$$\nLet $s_{dual}^\\star$ be the solution to this problem. Then $s_{dual}^\\star = \\operatorname{proj}_{\\lambda B(F)}(y)$.\nLet's define a new variable $s' = s/\\lambda$. The optimization is over $s' \\in B(F)$:\n$$\n\\min_{s' \\in B(F)} \\frac{1}{2}\\|\\lambda s' - y\\|_2^2 = \\min_{s' \\in B(F)} \\frac{\\lambda^2}{2} \\|s' - y/\\lambda\\|_2^2.\n$$\nThis is equivalent to finding the projection of $y/\\lambda$ onto $B(F)$. Let $s^\\star = \\operatorname{proj}_{B(F)}(y/\\lambda)$. Then the optimal dual variable is $s_{dual}^\\star = \\lambda s^\\star$.\n\nThe primal-dual optimality conditions state that the primal solution $z^\\star = \\operatorname{prox}_{\\lambda f^{L}}(y)$ is related to the dual solution $s_{dual}^\\star$ by $z^\\star = \\nabla G^*(-s_{dual}^\\star)$.\nSince $\\nabla_s G^*(s) = s+y$, we have:\n$$\nz^\\star = (-s_{dual}^\\star) + y = y - s_{dual}^\\star = y - \\lambda s^\\star.\n$$\nThis completes the derivation.\n\n### Part 3: Algorithmic Implementation via Min-Cut\n\nThe core of the computation is finding $s^\\star = \\operatorname{proj}_{B(F)}(v)$ where $v=y/\\lambda$. This is a projection onto a polytope defined by an exponential number of inequalities, requiring a cutting-plane method. The separation oracle for this task is to find, for a given point $s$, the most violated inequality:\n$$\n\\min_{S \\subseteq V} \\left( F(S) - \\sum_{i \\in S} s_i \\right).\n$$\nWe need to show this can be solved via min-cut on the specified auxiliary graph. Let us construct the graph $\\mathcal{G}'=(V',E')$ with $V' = V \\cup \\{src, t\\}$, source $src$ and sink $t$. The edge capacities are defined as:\n1. $C(src, i) = \\max(s_i, 0)$ for each $i \\in V$.\n2. $C(i, t) = \\max(-s_i, 0)$ for each $i \\in V$.\n3. $C(c, p) = w_{pc}$ for each edge $(p,c) \\in E$.\n\nConsider an $s-t$ cut, which partitions $V'$ into two sets, $A$ and $\\bar{A}$, with $src \\in A$ and $t \\in \\bar{A}$. Let $S = V \\cap A$. Then $V\\setminus S = V \\cap \\bar{A}$. The capacity of the cut is $C(A, \\bar{A}) = \\sum_{u \\in A, v \\in \\bar{A}} C(u,v)$.\nThe total capacity is the sum of capacities from the three types of edges crossing the cut:\n1. Edges $(src, i)$: cross if $i \\in \\bar{A}$ (i.e., $i \\in V \\setminus S$). Contribution: $\\sum_{i \\in V\\setminus S} \\max(s_i, 0)$.\n2. Edges $(i, t)$: cross if $i \\in A$ (i.e., $i \\in S$). Contribution: $\\sum_{i \\in S} \\max(-s_i, 0)$.\n3. Edges $(c, p)$: cross if $c \\in A$ (i.e., $c \\in S$) and $p \\in \\bar{A}$ (i.e., $p \\in V \\setminus S$). Contribution: $\\sum_{(p,c) \\in E} w_{pc} \\mathbf{1}\\{c \\in S, p \\notin S\\} = F(S)$.\n\nSo the total cut capacity is:\n$$\nC(A, \\bar{A}) = F(S) + \\sum_{i \\in S} \\max(-s_i, 0) + \\sum_{i \\in V\\setminus S} \\max(s_i, 0).\n$$\nWe use the identity $s_i = \\max(s_i, 0) - \\max(-s_i, 0)$.\nLet's rewrite the sum involving $s_i$:\n\\begin{align*}\n \\sum_{i \\in S} \\max(-s_i, 0) + \\sum_{i \\in V} \\max(s_i, 0) - \\sum_{i \\in S} \\max(s_i, 0) \\\\\n=  \\sum_{i \\in V} \\max(s_i, 0) - \\sum_{i \\in S} (\\max(s_i, 0) - \\max(-s_i, 0)) \\\\\n=  \\sum_{i \\in V} \\max(s_i, 0) - \\sum_{i \\in S} s_i.\n\\end{align*}\nThe cut capacity is therefore $C(A, \\bar{A}) = F(S) - \\sum_{i \\in S} s_i + K$, where $K = \\sum_{i \\in V} \\max(s_i, 0)$ is a constant with respect to the choice of $S$. Minimizing the cut capacity is equivalent to finding $\\min_{S \\subseteq V} (F(S) - \\sum_{i \\in S} s_i)$. The set $S$ that achieves the minimum cut is the set of nodes on the source side of the partition (excluding the source itself), which is precisely the most violated set required by the separation oracle.\n\n**Active-Set Algorithm for Projection:**\nThe algorithm to compute $s^\\star = \\operatorname{proj}_{B(F)}(v)$ is as follows:\n1. **Initialization:** The base polyhedron $B(F)$ contains the constraint $\\sum_{i\\in V} s_i = F(V)$. For a tree, $F(V) = 0$ since for any edge $(p,c)\\in E$, both $p$ and $c$ are in $V$. The initial active set of constraints is $\\mathcal{A}_0 = \\{V\\}$. The initial point $s_0$ is the projection of $v$ onto the hyperplane $\\mathbf{1}^T s = 0$.\n2. **Iteration:** At step $k$, let the current point be $s_k$ and the active set of constraints be $\\mathcal{A}_k$.\n   a. **Separation Oracle:** Construct the auxiliary graph using $s_k$. Find the min-cut to determine the set $S_k^*$ that minimizes $F(S) - \\mathbf{1}_S^T s_k$.\n   b. **Check for termination:** Calculate the violation $\\delta_k = F(S_k^*) - \\mathbf{1}_{S_k^*}^T s_k$. If $\\delta_k \\ge \\epsilon$ for a small negative tolerance $\\epsilon$ (e.g., $\\epsilon=-10^{-9}$), then $s_k$ is feasible and thus the optimal projection. Terminate and set $s^\\star=s_k$.\n   c. **Add Constraint:** If $\\delta_k  \\epsilon$, the constraint for $S_k^*$ is violated. Add $S_k^*$ to the active set: $\\mathcal{A}_{k+1} = \\mathcal{A}_k \\cup \\{S_k^*\\}$.\n   d. **Update Projection:** Compute the new point $s_{k+1}$ by projecting $v$ onto the affine subspace formed by the intersection of all constraints in $\\mathcal{A}_{k+1}$ holding with equality. This is done by solving a small system of linear equations derived from the KKT conditions of the constrained quadratic program. Repeat the iteration.\n\nThis iterative process is guaranteed to converge in at most $n$ iterations, as a new linearly independent constraint is added at each step. Once $s^\\star$ is found, the final result is computed as $\\operatorname{prox}_{\\lambda f^{L}}(y) = y - \\lambda s^\\star$.",
            "answer": "[[0.585714,-0.200000,0.400000,0.785714,-0.500000],[0.900000,-0.200000,0.400000,1.100000,-0.500000],[0.500000,0.500000,0.500000,0.500000,0.500000],[-0.300000,0.500000,-0.600000,0.300000],[0.100000,0.100000,0.100000,0.100000,0.100000]]"
        }
    ]
}