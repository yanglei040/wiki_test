{
    "hands_on_practices": [
        {
            "introduction": "The fundamental goal of sparse approximation is to represent a signal using only a few non-zero coefficients. When these coefficients have a known relational structure, such as the parent-child relationships in a tree, we can seek an approximation that respects this hierarchy. This first exercise provides a concrete, hands-on calculation for finding the best tree-sparse approximation, forcing us to grapple with the combinatorial nature of the problem and solidifying the core principle of projection onto a structured model class. ",
            "id": "3450706",
            "problem": "Consider a hierarchical model of tree-structured sparsity as follows. Let $v \\in \\mathbb{R}^{8}$ be the data vector given by $v = (0.9, -0.3, 2.1, 0.2, -0.1, 0.5, -3.0, 2.6)^{\\top}$. Let the tree $\\mathcal{T}$ on indices $\\{1,2,3,4,5,6,7,8\\}$ be a rooted tree of depth $3$ with root at index $1$, children of index $1$ equal to indices $2$ and $3$, children of index $2$ equal to indices $4$ and $5$, and children of index $3$ equal to indices $6$, $7$, and $8$. A subset $A \\subseteq \\{1,\\dots,8\\}$ is said to be a connected subtree support if the induced subgraph of $\\mathcal{T}$ on $A$ is connected.\n\nDefine the model $\\mathcal{M}_{3}$ to be the union of all coordinate subspaces corresponding to connected subtree supports of cardinality $3$, that is, every $x \\in \\mathcal{M}_{3}$ has support equal to some $A \\subseteq \\{1,\\dots,8\\}$ satisfying $|A| = 3$ and $A$ is a connected subtree support.\n\nUsing the Euclidean norm $\\|\\cdot\\|_{2}$, compute the exact Euclidean projection of $v$ onto $\\mathcal{M}_{3}$, understood as the element $w^{\\star} \\in \\mathcal{M}_{3}$ that minimizes $\\|v - w\\|_{2}$ over $w \\in \\mathcal{M}_{3}$. Your derivation must start from first principles (definitions of Euclidean projection onto a union of coordinate subspaces and connectedness in trees), and must justify why the selected support is optimal under the connected subtree constraint.\n\nFor your final answer, concatenate the $3$ selected indices in increasing order followed by the $8$ entries of the projected vector into a single row matrix. No rounding is required. The final answer must be a single expression.",
            "solution": "The problem requires the computation of the Euclidean projection of a given vector $v \\in \\mathbb{R}^{8}$ onto a non-convex set $\\mathcal{M}_{3}$. The set $\\mathcal{M}_{3}$ is defined as the union of specific coordinate subspaces, selected according to a tree-structured sparsity model.\n\nLet the vector be $v = (0.9, -0.3, 2.1, 0.2, -0.1, 0.5, -3.0, 2.6)^{\\top}$. The tree $\\mathcal{T}$ on the index set $\\{1, 2, 3, 4, 5, 6, 7, 8\\}$ is defined by the following parent-child relationships: the root is node $1$; node $1$ has children $\\{2, 3\\}$; node $2$ has children $\\{4, 5\\}$; and node $3$ has children $\\{6, 7, 8\\}$.\n\nThe model $\\mathcal{M}_{3}$ is the union of all coordinate subspaces $S_A$ where the support set $A \\subseteq \\{1, \\dots, 8\\}$ has cardinality $|A|=3$ and forms a connected subtree in $\\mathcal{T}$. Formally, $\\mathcal{M}_{3} = \\bigcup_{A \\in \\mathcal{A}_3} S_A$, where $\\mathcal{A}_3 = \\{ A \\subseteq \\{1, \\dots, 8\\} : |A|=3 \\text{ and } A \\text{ is a connected subtree support} \\}$ and $S_A = \\{ x \\in \\mathbb{R}^8 : x_i = 0 \\text{ for all } i \\notin A \\}$.\n\nWe seek to find the vector $w^{\\star} \\in \\mathcal{M}_{3}$ that minimizes the Euclidean distance to $v$, i.e.,\n$$w^{\\star} = \\arg\\min_{w \\in \\mathcal{M}_{3}} \\|v - w\\|_{2}$$\nSince $\\mathcal{M}_{3}$ is a union of linear subspaces, the optimal vector $w^{\\star}$ must be the orthogonal projection of $v$ onto one of these subspaces. Let $P_{S_A}(v)$ denote the orthogonal projection of $v$ onto the subspace $S_A$. The optimization problem is equivalent to finding the subspace $S_{A^{\\star}}$ such that the projection is closest to $v$:\n$$w^{\\star} = P_{S_{A^{\\star}}}(v) \\quad \\text{where} \\quad A^{\\star} = \\arg\\min_{A \\in \\mathcal{A}_3} \\|v - P_{S_A}(v)\\|_{2}$$\nMinimizing the distance is equivalent to minimizing the squared distance. The squared distance from $v$ to its projection on $S_A$ is given by the sum of squares of the components of $v$ that are zeroed out:\n$$\\|v - P_{S_A}(v)\\|_{2}^{2} = \\sum_{i \\notin A} v_i^2$$\nUsing the Pythagorean theorem, $\\|v\\|_{2}^{2} = \\|P_{S_A}(v)\\|_{2}^{2} + \\|v - P_{S_A}(v)\\|_{2}^{2}$, we can rewrite the expression as:\n$$\\|v - P_{S_A}(v)\\|_{2}^{2} = \\|v\\|_{2}^{2} - \\|P_{S_A}(v)\\|_{2}^{2} = \\sum_{i=1}^{8} v_i^2 - \\sum_{i \\in A} v_i^2$$\nSince $\\sum_{i=1}^{8} v_i^2$ is a constant with respect to the choice of $A$, minimizing the distance is equivalent to maximizing the sum of squares of the components of $v$ over the support set $A$:\n$$A^{\\star} = \\arg\\max_{A \\in \\mathcal{A}_3} \\sum_{i \\in A} v_i^2$$\n\nOur first step is to enumerate all possible connected subtree supports $A$ of cardinality $3$. A connected graph of $3$ nodes on a tree can only have two topologies: a path of length $2$ (grandparent-parent-child) or a star graph with a central node and two children (parent-and-two-children).\n\nThe tree structure is specified as:\n\\begin{itemize}\n    \\item Edges: $(1,2), (1,3), (2,4), (2,5), (3,6), (3,7), (3,8)$.\n\\end{itemize}\nWe list all connected subtrees of size $3$:\n\\begin{enumerate}\n    \\item Path-like subtrees (grandparent-parent-child):\n    \\begin{itemize}\n        \\item $\\{1, 2, 4\\}$\n        \\item $\\{1, 2, 5\\}$\n        \\item $\\{1, 3, 6\\}$\n        \\item $\\{1, 3, 7\\}$\n        \\item $\\{1, 3, 8\\}$\n    \\end{itemize}\n    \\item Star-like subtrees (parent and two children):\n    \\begin{itemize}\n        \\item $\\{1, 2, 3\\}$ (parent $1$, children $2, 3$)\n        \\item $\\{2, 4, 5\\}$ (parent $2$, children $4, 5$)\n        \\item $\\{3, 6, 7\\}$ (parent $3$, children $6, 7$)\n        \\item $\\{3, 6, 8\\}$ (parent $3$, children $6, 8$)\n        \\item $\\{3, 7, 8\\}$ (parent $3$, children $7, 8$)\n    \\end{itemize}\n\\end{enumerate}\nThis constitutes the complete set $\\mathcal{A}_3$ of ten possible supports.\n\nNext, we calculate the squared values of the components of $v$:\n$v_1^2 = (0.9)^2 = 0.81$\n$v_2^2 = (-0.3)^2 = 0.09$\n$v_3^2 = (2.1)^2 = 4.41$\n$v_4^2 = (0.2)^2 = 0.04$\n$v_5^2 = (-0.1)^2 = 0.01$\n$v_6^2 = (0.5)^2 = 0.25$\n$v_7^2 = (-3.0)^2 = 9.00$\n$v_8^2 = (2.6)^2 = 6.76$\n\nNow, we compute the sum $\\sum_{i \\in A} v_i^2$ for each support $A \\in \\mathcal{A}_3$:\n\\begin{itemize}\n    \\item $A = \\{1, 2, 4\\}: \\sum v_i^2 = 0.81 + 0.09 + 0.04 = 0.94$\n    \\item $A = \\{1, 2, 5\\}: \\sum v_i^2 = 0.81 + 0.09 + 0.01 = 0.91$\n    \\item $A = \\{1, 3, 6\\}: \\sum v_i^2 = 0.81 + 4.41 + 0.25 = 5.47$\n    \\item $A = \\{1, 3, 7\\}: \\sum v_i^2 = 0.81 + 4.41 + 9.00 = 14.22$\n    \\item $A = \\{1, 3, 8\\}: \\sum v_i^2 = 0.81 + 4.41 + 6.76 = 11.98$\n    \\item $A = \\{1, 2, 3\\}: \\sum v_i^2 = 0.81 + 0.09 + 4.41 = 5.31$\n    \\item $A = \\{2, 4, 5\\}: \\sum v_i^2 = 0.09 + 0.04 + 0.01 = 0.14$\n    \\item $A = \\{3, 6, 7\\}: \\sum v_i^2 = 4.41 + 0.25 + 9.00 = 13.66$\n    \\item $A = \\{3, 6, 8\\}: \\sum v_i^2 = 4.41 + 0.25 + 6.76 = 11.42$\n    \\item $A = \\{3, 7, 8\\}: \\sum v_i^2 = 4.41 + 9.00 + 6.76 = 20.17$\n\\end{itemize}\n\nComparing these sums, the maximum value is $20.17$, which corresponds to the support set $A^{\\star} = \\{3, 7, 8\\}$. The induced subgraph on these vertices consists of edges $(3,7)$ and $(3,8)$, which is indeed a connected tree structure. Because this sum is uniquely the maximum, the projection is unique.\n\nThe projection $w^{\\star}$ of $v$ onto $\\mathcal{M}_{3}$ is therefore the projection onto the subspace $S_{A^{\\star}}$, which is obtained by keeping the components of $v$ at indices $\\{3, 7, 8\\}$ and setting all other components to zero.\nGiven $v = (0.9, -0.3, 2.1, 0.2, -0.1, 0.5, -3.0, 2.6)^{\\top}$, the projected vector $w^{\\star}$ is:\n$$w^{\\star} = (0, 0, v_3, 0, 0, 0, v_7, v_8)^{\\top} = (0, 0, 2.1, 0, 0, 0, -3.0, 2.6)^{\\top}$$\n\nThe final answer requires concatenating the $3$ selected indices in increasing order ($3, 7, 8$) followed by the $8$ entries of the projected vector $w^{\\star}$.\nThis results in a row vector of $3+8=11$ elements: $(3, 7, 8, 0, 0, 2.1, 0, 0, 0, -3.0, 2.6)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 7 & 8 & 0 & 0 & 2.1 & 0 & 0 & 0 & -3.0 & 2.6\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond finding a single best approximation, we can also think of tree-structured signals from a probabilistic perspective. In many Bayesian models, sparsity patterns are not chosen deterministically but are assumed to be drawn from a prior distribution that favors certain structures. This exercise shifts our focus to such a generative viewpoint, demonstrating how to compute the probability of a specific tree-structured support under a hierarchical prior, a key skill for developing and understanding Bayesian inference methods for structured sparsity. ",
            "id": "3450736",
            "problem": "Consider a rooted, directed tree of depth $3$ with the following nodes and parent-child structure. The root is $r$. Level $1$ consists of nodes $A$ and $B$, where $r$ is the parent of $A$ and $B$. Level $2$ consists of nodes $C$ and $D$ (children of $A$), and $E$ and $F$ (children of $B$). Level $3$ consists of nodes $G$ and $H$ (children of $C$), $I$ and $J$ (children of $D$), $K$ and $L$ (children of $E$), and $M$ and $N$ (children of $F$). For each node $i$, there is a binary inclusion variable $z_i \\in \\{0,1\\}$ indicating whether node $i$ is in the support.\n\nAssume a hierarchical prior that enforces connected subtree sparsity via the following rules:\n- The root inclusion variable $z_r$ is drawn as a Bernoulli random variable with parameter $\\pi_{\\text{root}} \\in (0,1)$, i.e., $\\mathbb{P}(z_r = 1) = \\pi_{\\text{root}}$ and $\\mathbb{P}(z_r = 0) = 1 - \\pi_{\\text{root}}$.\n- For each directed edge from a parent $i$ to a child $j$, conditional on $z_i = 1$, the child inclusion variable $z_j$ is drawn independently across siblings with\n$$\n\\mathbb{P}(z_j = 1 \\mid z_i = 1) = p_{j \\mid i}, \\quad \\mathbb{P}(z_j = 0 \\mid z_i = 1) = 1 - p_{j \\mid i},\n$$\nwhere $p_{j \\mid i} \\in (0,1)$ is the transition probability associated with edge $i \\to j$. Conditional independence across children given the parent is assumed.\n- Connectedness is enforced by disallowing child inclusion when the parent is excluded: for all edges $i \\to j$,\n$$\n\\mathbb{P}(z_j = 1 \\mid z_i = 0) = 0, \\quad \\mathbb{P}(z_j = 0 \\mid z_i = 0) = 1.\n$$\n\nLet the edge-specific transition probabilities be denoted by\n$$\np_{A \\mid r} = a, \\quad p_{B \\mid r} = b, \\quad p_{C \\mid A} = c, \\quad p_{D \\mid A} = d, \\quad p_{E \\mid B} = e, \\quad p_{F \\mid B} = f,\n$$\n$$\np_{G \\mid C} = g, \\quad p_{H \\mid C} = h, \\quad p_{I \\mid D} = i, \\quad p_{J \\mid D} = j, \\quad p_{K \\mid E} = k, \\quad p_{L \\mid E} = \\ell, \\quad p_{M \\mid F} = m, \\quad p_{N \\mid F} = n,\n$$\nall in $(0,1)$, and write $\\pi_{\\text{root}} = \\pi$.\n\nConsider the connected subtree support\n$$\nS = \\{ r, A, C, G \\},\n$$\ni.e., the event that $z_r = 1$, $z_A = 1$, $z_C = 1$, $z_G = 1$, and all other inclusion variables are zero:\n$$\nz_B = 0, \\; z_D = 0, \\; z_H = 0, \\; z_E = 0, \\; z_F = 0, \\; z_I = 0, \\; z_J = 0, \\; z_K = 0, \\; z_L = 0, \\; z_M = 0, \\; z_N = 0.\n$$\n\nStarting only from the basic rules of probability for directed trees and the model assumptions stated above, derive and compute the marginal prior probability of exactly this support $S$, expressed as a closed-form analytic expression in terms of $\\pi$ and the transition probabilities listed. No numerical rounding is required and no units apply. Your final answer must be a single analytic expression.",
            "solution": "The problem asks for the marginal prior probability of a specific connected subtree support $S = \\{ r, A, C, G \\}$. This corresponds to the joint event where the inclusion variables for the nodes in $S$ are $1$ and the inclusion variables for all other nodes in the tree are $0$. The probabilistic model described is a Bayesian network on a directed tree. The fundamental principle for computing the joint probability of a specific configuration of variables in a Bayesian network is the chain rule, which states that the joint probability is the product of the conditional probabilities of each variable given its parents.\n\nThe set of all nodes in the tree is $V = \\{r, A, B, C, D, E, F, G, H, I, J, K, L, M, N\\}$. The total number of nodes is $15$. The specified support $S = \\{ r, A, C, G \\}$ defines the state of each inclusion variable $z_i$ for $i \\in V$. Specifically:\n$z_r = 1, z_A = 1, z_C = 1, z_G = 1$.\nAll other inclusion variables are $0$:\n$z_B = 0, z_D = 0, z_H = 0, z_E = 0, z_F = 0, z_I = 0, z_J = 0, z_K = 0, z_L = 0, z_M = 0, z_N = 0$.\n\nThe joint probability of this specific configuration, which we denote $\\mathbb{P}(S)$, can be factored as the product of the probability of the root's state and the conditional probabilities of each child's state given its parent's state. Due to the specified conditional independence of siblings given their parent, the factorization is:\n$$\n\\mathbb{P}(S) = \\mathbb{P}(z_r) \\prod_{j \\in V \\setminus \\{r\\}} \\mathbb{P}(z_j \\mid z_{\\text{parent}(j)})\n$$\nExpanding this product for the entire tree structure gives:\n\\begin{align*}\n\\mathbb{P}(S) = \\; & \\mathbb{P}(z_r=1) \\\\\n& \\times \\mathbb{P}(z_A=1 \\mid z_r=1) \\cdot \\mathbb{P}(z_B=0 \\mid z_r=1) \\\\\n& \\times \\mathbb{P}(z_C=1 \\mid z_A=1) \\cdot \\mathbb{P}(z_D=0 \\mid z_A=1) \\\\\n& \\times \\mathbb{P}(z_E=0 \\mid z_B=0) \\cdot \\mathbb{P}(z_F=0 \\mid z_B=0) \\\\\n& \\times \\mathbb{P}(z_G=1 \\mid z_C=1) \\cdot \\mathbb{P}(z_H=0 \\mid z_C=1) \\\\\n& \\times \\mathbb{P}(z_I=0 \\mid z_D=0) \\cdot \\mathbb{P}(z_J=0 \\mid z_D=0) \\\\\n& \\times \\mathbb{P}(z_K=0 \\mid z_E=0) \\cdot \\mathbb{P}(z_L=0 \\mid z_E=0) \\\\\n& \\times \\mathbb{P}(z_M=0 \\mid z_F=0) \\cdot \\mathbb{P}(z_N=0 \\mid z_F=0)\n\\end{align*}\nWe now evaluate each term in this product using the model's rules and given parameters.\n\n1.  **Root Node ($r$)**:\n    The problem states $\\mathbb{P}(z_r=1) = \\pi_{\\text{root}} = \\pi$.\n\n2.  **Children of $r$ ($A, B$)**: The parent is $r$, and its state is $z_r=1$.\n    - For node A: $\\mathbb{P}(z_A=1 \\mid z_r=1) = p_{A \\mid r} = a$.\n    - For node B: $\\mathbb{P}(z_B=0 \\mid z_r=1) = 1 - \\mathbb{P}(z_B=1 \\mid z_r=1) = 1 - p_{B \\mid r} = 1 - b$.\n\n3.  **Children of $A$ ($C, D$)**: The parent is $A$, and its state is $z_A=1$.\n    - For node C: $\\mathbb{P}(z_C=1 \\mid z_A=1) = p_{C \\mid A} = c$.\n    - For node D: $\\mathbb{P}(z_D=0 \\mid z_A=1) = 1 - \\mathbb{P}(z_D=1 \\mid z_A=1) = 1 - p_{D \\mid A} = 1 - d$.\n\n4.  **Children of $C$ ($G, H$)**: The parent is $C$, and its state is $z_C=1$.\n    - For node G: $\\mathbb{P}(z_G=1 \\mid z_C=1) = p_{G \\mid C} = g$.\n    - For node H: $\\mathbb{P}(z_H=0 \\mid z_C=1) = 1 - \\mathbb{P}(z_H=1 \\mid z_C=1) = 1 - p_{H \\mid C} = 1 - h$.\n\n5.  **Nodes whose parents are inactive**: The model enforces that if a parent node $i$ is not in the support ($z_i=0$), then any child node $j$ cannot be in the support. This is given by the rule $\\mathbb{P}(z_j=1 \\mid z_i=0) = 0$, which implies $\\mathbb{P}(z_j=0 \\mid z_i=0) = 1$. We apply this rule to all subtrees rooted at nodes with $z_i=0$.\n    - Children of $B$ ($E, F$): $z_B=0$. So, $\\mathbb{P}(z_E=0 \\mid z_B=0) = 1$ and $\\mathbb{P}(z_F=0 \\mid z_B=0) = 1$.\n    - Children of $D$ ($I, J$): $z_D=0$. So, $\\mathbb{P}(z_I=0 \\mid z_D=0) = 1$ and $\\mathbb{P}(z_J=0 \\mid z_D=0) = 1$.\n    - Children of $E$ ($K, L$): $z_E=0$. So, $\\mathbb{P}(z_K=0 \\mid z_E=0) = 1$ and $\\mathbb{P}(z_L=0 \\mid z_E=0) = 1$.\n    - Children of $F$ ($M, N$): $z_F=0$. So, $\\mathbb{P}(z_M=0 \\mid z_F=0) = 1$ and $\\mathbb{P}(z_N=0 \\mid z_F=0) = 1$.\n\nFinally, we multiply all these probabilities together to find the total probability $\\mathbb{P}(S)$:\n$$\n\\mathbb{P}(S) = \\pi \\cdot [a \\cdot (1-b)] \\cdot [c \\cdot (1-d)] \\cdot [1 \\cdot 1] \\cdot [g \\cdot (1-h)] \\cdot [1 \\cdot 1] \\cdot [1 \\cdot 1] \\cdot [1 \\cdot 1]\n$$\nSimplifying this expression yields the final result:\n$$\n\\mathbb{P}(S) = \\pi a c g (1-b)(1-d)(1-h)\n$$\nThis expression represents the marginal probability of observing exactly the support set $S = \\{r, A, C, G\\}$ and no other nodes.",
            "answer": "$$\n\\boxed{\\pi a c g (1-b)(1-d)(1-h)}\n$$"
        },
        {
            "introduction": "While enumerating all possible subtrees is feasible for small examples, it is computationally intractable for larger problems. To build scalable algorithms, we need more sophisticated tools. This advanced practice introduces a powerful strategy: reformulating the non-convex tree-sparsity penalty as a convex function using the Lovász extension. You will see how this elegant theoretical connection to submodular optimization leads to a practical algorithm for computing the proximal operator, a fundamental building block in modern convex optimization for signal processing and machine learning. ",
            "id": "3450704",
            "problem": "Consider a rooted tree $\\mathcal{T}=(V,E)$ with directed edges from parent to child. Let $V=\\{1,2,\\dots,n\\}$ denote the nodes. Define the set function (penalty) $F:2^{V}\\to\\mathbb{R}_{\\ge 0}$ by\n$$\nF(S) \\;=\\; \\sum_{(p,c)\\in E} w_{pc}\\,\\mathbf{1}\\{c\\in S,\\,p\\notin S\\},\n$$\nwhere $(p,c)\\in E$ denotes a directed edge from parent $p$ to child $c$, $w_{pc}>0$ are given edge weights, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This penalty charges a cost whenever a child node is activated without its parent being activated, thereby penalizing disconnected activations in the hierarchy induced by $\\mathcal{T}$.\n\nProblem tasks:\n\n1) Starting from the definitions of submodularity and the Lovász extension, show that $F$ is submodular and derive the Lovász extension $f^{L}:\\mathbb{R}^{n}\\to\\mathbb{R}$ of $F$. Prove that $f^{L}$ is convex. Your derivation must start from the fundamental definition that a set function $G$ is submodular if and only if for all subsets $A\\subseteq B\\subseteq V$ and elements $i\\in V\\setminus B$, one has $G(A\\cup\\{i\\})-G(A)\\ge G(B\\cup\\{i\\})-G(B)$, and from the definition of the Lovász extension via level sets. Avoid using shortcut formulas not justified from these bases.\n\n2) Consider the proximal mapping of the convex surrogate $f^{L}$:\n$$\n\\operatorname{prox}_{\\lambda f^{L}}(y)\\;=\\;\\arg\\min_{z\\in\\mathbb{R}^{n}}\\;\\frac{1}{2}\\,\\|z-y\\|_{2}^{2}\\;+\\;\\lambda\\, f^{L}(z),\n$$\nwhere $\\lambda>0$ and $y\\in\\mathbb{R}^{n}$ is given. Derive the dual formulation showing that\n$$\n\\operatorname{prox}_{\\lambda f^{L}}(y)\\;=\\;y-\\lambda\\,s^{\\star},\\quad\\text{where}\\quad s^{\\star}=\\operatorname{proj}_{B(F)}\\!\\left(\\frac{y}{\\lambda}\\right),\n$$\nand $B(F)$ is the base polyhedron of $F$ defined by\n$$\nB(F)\\;=\\;\\left\\{s\\in\\mathbb{R}^{n}:\\;\\sum_{i\\in S}s_{i}\\le F(S)\\quad\\forall\\,S\\subseteq V,\\quad\\sum_{i\\in V}s_{i}=F(V)\\right\\}.\n$$\nYou must start from the convex conjugacy fact that the convex conjugate of the Lovász extension is the indicator of the base polyhedron, and rigorously derive the projection characterization.\n\n3) Implement an algorithm to compute $s^{\\star}=\\operatorname{proj}_{B(F)}(y/\\lambda)$ using an active-set cutting-plane method whose separation oracle is a Minimum Cut (min-cut) on an auxiliary graph. Specifically, for any vector $s\\in\\mathbb{R}^{n}$, the most violated inequality of the form $\\sum_{i\\in S} s_{i}\\le F(S)$ can be found by minimizing $F(S)-\\sum_{i\\in S}s_{i}$ over $S\\subseteq V$. Show that this minimization reduces to an $s$-$t$ min-cut on a directed graph constructed as follows:\n- Create a source node and a sink node.\n- For each $i\\in V$, add a directed edge from source to $i$ with capacity $\\max(s_{i},0)$ and a directed edge from $i$ to sink with capacity $\\max(-s_{i},0)$.\n- For each edge $(p,c)\\in E$, add a directed edge from $c$ to $p$ with capacity $w_{pc}$.\nArgue that the cut cost equals $F(S)-\\sum_{i\\in S}s_{i}$ up to a constant independent of $S$, and therefore a minimum $s$-$t$ cut yields the minimizer $S$.\n\nUsing this, implement the active-set projection algorithm:\n- Start with the equality constraint $\\sum_{i\\in V}s_{i}=F(V)$.\n- Iteratively find the most violated inequality via min-cut; if violated, add it as an active equality constraint and project $y/\\lambda$ onto the intersection of the current active constraints; repeat until no violation remains.\nReturn $s^{\\star}$ and compute $\\operatorname{prox}_{\\lambda f^{L}}(y)=y-\\lambda s^{\\star}$.\n\nImplementation requirements:\n\n- Your final answer must be a complete, runnable program that performs the proximal mapping for the following test suite. For each test case, the input is specified by $(V,E,w,y,\\lambda)$; treat $V$ as the set $\\{1,\\dots,n\\}$ in node index order.\n\nTest suite:\n\n- Case 1 (General hierarchical smoothing on a 5-node tree): $n=5$, edges $E=\\{(1,2),(1,3),(3,4),(3,5)\\}$ with weights $w_{pc}=1$ for all edges, $y=[0.9,-0.2,0.4,1.1,-0.5]$, $\\lambda=0.7$.\n\n- Case 2 (Boundary condition $\\lambda=0$): Same tree and weights, $y=[0.9,-0.2,0.4,1.1,-0.5]$, $\\lambda=0$.\n\n- Case 3 (Uniform $y$ where the hinge surrogate can be nullified): Same tree and weights, $y=[0.5,0.5,0.5,0.5,0.5]$, $\\lambda=1.0$.\n\n- Case 4 (Small tree with mixed signs): $n=4$, edges $E=\\{(1,2),(2,3),(2,4)\\}$ with weights $w_{pc}=\\{(1,2):2.0,(2,3):0.5,(2,4):1.5\\}$, $y=[-0.3,0.8,-1.2,0.6]$, $\\lambda=0.6$.\n\n- Case 5 (Stronger regularization): Same as Case 1 but with $\\lambda=2.5$.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the proximal results for each test case, aggregated as a comma-separated list of lists enclosed in square brackets. Each inner list must contain the $n$ floating-point entries of $\\operatorname{prox}_{\\lambda f^{L}}(y)$ for that case, rounded to six decimal places, with no spaces. For example: \"[[0.123456,0.654321],[...],...]\".",
            "solution": "The problem asks for a theoretical derivation and algorithmic implementation for computing the proximal operator of a tree-structured sparse penalty function. The problem is valid as it is scientifically grounded in convex optimization and sparse modeling, well-posed, objective, and provides a complete and consistent setup. We proceed with the solution.\n\n### Part 1: Submodularity and Lovász Extension\n\n**Submodularity of $F(S)$**\n\nA set function $G: 2^V \\to \\mathbb{R}$ is submodular if for all subsets $A \\subseteq B \\subseteq V$ and any element $i \\in V \\setminus B$, the diminishing returns property holds:\n$$\nG(A \\cup \\{i\\}) - G(A) \\ge G(B \\cup \\{i\\}) - G(B).\n$$\nWe define the marginal gain of adding element $i$ to a set $S$ (where $i \\notin S$) as $\\Delta_G(S, i) = G(S \\cup \\{i\\}) - G(S)$. The condition for submodularity is that $\\Delta_G(S, i)$ is a non-increasing function of $S$.\n\nThe given set function is $F(S) = \\sum_{(p,c)\\in E} w_{pc}\\,\\mathbf{1}\\{c\\in S,\\,p\\notin S\\}$. Let us compute the marginal gain $\\Delta_F(S, i)$ for some $i \\notin S$.\n$$\n\\Delta_F(S, i) = F(S \\cup \\{i\\}) - F(S) = \\sum_{(p,c)\\in E} w_{pc} \\left( \\mathbf{1}\\{c\\in S\\cup\\{i\\},\\,p\\notin S\\cup\\{i\\}\\} - \\mathbf{1}\\{c\\in S,\\,p\\notin S\\} \\right).\n$$\nThe terms in the sum are non-zero only if the edge $(p,c)$ is incident to the node $i$. We consider two cases for an edge $(p,c)$:\n1.  Node $i$ is the child, $c=i$. The contribution to the sum from all such edges is:\n    $$\n    \\sum_{p:(p,i)\\in E} w_{pi} \\left( \\mathbf{1}\\{i\\in S\\cup\\{i\\},\\,p\\notin S\\cup\\{i\\}\\} - \\mathbf{1}\\{i\\in S,\\,p\\notin S\\} \\right).\n    $$\n    Since $i \\notin S$, the second indicator is always $0$. For the first indicator, $i\\in S\\cup\\{i\\}$ is true, and $p\\notin S\\cup\\{i\\}$ is equivalent to $p\\notin S$ (since $p\\neq i$). So, this part contributes $\\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin S\\}$.\n2.  Node $i$ is the parent, $p=i$. Let $C(i)$ be the set of children of $i$. The contribution is:\n    $$\n    \\sum_{c\\in C(i)} w_{ic} \\left( \\mathbf{1}\\{c\\in S\\cup\\{i\\},\\,i\\notin S\\cup\\{i\\}\\} - \\mathbf{1}\\{c\\in S,\\,i\\notin S\\} \\right).\n    $$\n    Since $i\\notin S$, the condition $i\\notin S\\cup\\{i\\}$ in the first indicator is false, making it $0$. The condition $i\\notin S$ in the second indicator is true. So, this part contributes $\\sum_{c\\in C(i)} w_{ic} (0 - \\mathbf{1}\\{c\\in S\\}) = -\\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in S\\}$.\n\nCombining these two cases, the marginal gain is:\n$$\n\\Delta_F(S, i) = \\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin S\\} - \\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in S\\}.\n$$\nNow, let $A \\subseteq B \\subseteq V$ with $i \\notin B$. We compare $\\Delta_F(A, i)$ and $\\Delta_F(B, i)$.\nSince $A \\subseteq B$, we have:\n- For any node $p$, $\\mathbf{1}\\{p\\notin A\\} \\ge \\mathbf{1}\\{p\\notin B\\}$.\n- For any node $c$, $\\mathbf{1}\\{c\\in A\\} \\le \\mathbf{1}\\{c\\in B\\}$.\n\nGiven that weights $w_{pc} > 0$, we can conclude:\n- $\\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin A\\} \\ge \\sum_{p:(p,i)\\in E} w_{pi} \\mathbf{1}\\{p\\notin B\\}$.\n- $-\\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in A\\} \\ge -\\sum_{c\\in C(i)} w_{ic} \\mathbf{1}\\{c\\in B\\}$.\n\nSumming these inequalities, we get $\\Delta_F(A, i) \\ge \\Delta_F(B, i)$. Thus, $F$ is a submodular function.\n\n**Lovász Extension and its Convexity**\n\nThe Lovász extension $f^L: \\mathbb{R}^n \\to \\mathbb{R}$ of a set function $F$ can be defined via its level sets:\n$$\nf^L(z) = \\int_{-\\infty}^{\\infty} F(\\{i \\in V \\mid z_i \\ge t\\}) dt.\n$$\nSubstituting the definition of $F(S)$, we have:\n$$\nf^L(z) = \\int_{-\\infty}^{\\infty} \\left( \\sum_{(p,c)\\in E} w_{pc}\\,\\mathbf{1}\\{c\\in \\{i \\mid z_i \\ge t\\},\\,p\\notin \\{i \\mid z_i \\ge t\\}\\} \\right) dt.\n$$\nBy linearity of integration, we can swap the sum and the integral:\n$$\nf^L(z) = \\sum_{(p,c)\\in E} w_{pc} \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{c\\in \\{i \\mid z_i \\ge t\\}\\ \\text{and}\\ p\\notin \\{i \\mid z_i \\ge t\\}\\} dt.\n$$\nThe condition inside the indicator function is equivalent to $z_c \\ge t$ and $z_p < t$, which simplifies to $z_p < t \\le z_c$. The integral of the indicator function over $t$ calculates the length of the interval $(z_p, z_c]$. The length is $z_c - z_p$ if $z_c > z_p$, and $0$ otherwise. This is precisely the function $\\max(z_c - z_p, 0)$.\nTherefore, the Lovász extension of $F$ is:\n$$\nf^L(z) = \\sum_{(p,c)\\in E} w_{pc} \\max(z_c - z_p, 0).\n$$\nTo prove that $f^L(z)$ is convex, we note that for each edge $(p,c)$, the function $z \\mapsto z_c - z_p$ is an affine, and hence convex, function. The function $x \\mapsto \\max(x, 0)$ is convex. The composition of a convex function with an affine map is convex, so $z \\mapsto \\max(z_c - z_p, 0)$ is convex. Since $f^L(z)$ is a non-negative weighted sum of convex functions (as $w_{pc} > 0$), it is also a convex function.\n\n### Part 2: Proximal Operator Duality\n\nThe proximal mapping is defined as the solution to the optimization problem:\n$$\n\\operatorname{prox}_{\\lambda f^{L}}(y) = \\arg\\min_{z\\in\\mathbb{R}^{n}}\\left\\{ \\frac{1}{2}\\,\\|z-y\\|_{2}^{2}\\;+\\;\\lambda\\, f^{L}(z) \\right\\}.\n$$\nThis problem is of the form $\\min_z G(z) + H(z)$, where $G(z) = \\frac{1}{2}\\|z-y\\|_2^2$ and $H(z) = \\lambda f^L(z)$. We can solve this using Fenchel-Rockafellar duality. The dual problem is $\\max_s -G^*(-s) - H^*(s)$, where $G^*$ and $H^*$ are the convex conjugates.\n\nFirst, we find the conjugate of $G(z)$:\n$$\nG^*(s) = \\sup_{z} \\left( \\langle s, z \\rangle - \\frac{1}{2}\\|z-y\\|_2^2 \\right).\n$$\nThe supremum is achieved when the gradient with respect to $z$ is zero: $s - (z-y) = 0 \\implies z = s+y$. Substituting back yields:\n$$\nG^*(s) = \\langle s, s+y \\rangle - \\frac{1}{2}\\|(s+y)-y\\|_2^2 = \\|s\\|_2^2 + \\langle s, y \\rangle - \\frac{1}{2}\\|s\\|_2^2 = \\frac{1}{2}\\|s\\|_2^2 + \\langle s, y \\rangle.\n$$\nNext, we find the conjugate of $H(z)$:\n$$\nH^*(s) = \\sup_{z} \\left( \\langle s, z \\rangle - \\lambda f^L(z) \\right) = \\lambda \\sup_{z} \\left( \\langle s/\\lambda, z \\rangle - f^L(z) \\right) = \\lambda (f^L)^*(s/\\lambda).\n$$\nWe are given that the conjugate of the Lovász extension, $(f^L)^*$, is the indicator function of the base polyhedron $B(F)$, denoted $\\delta_{B(F)}$. Thus, $(f^L)^*(v) = 0$ if $v \\in B(F)$ and $+\\infty$ otherwise.\n$H^*(s) = \\lambda \\delta_{B(F)}(s/\\lambda)$, which is the indicator function for the scaled polyhedron $\\lambda B(F)$. So, $H^*(s) = 0$ if $s/\\lambda \\in B(F)$ and $+\\infty$ otherwise.\n\nThe dual problem is:\n$$\n\\max_{s} \\left\\{ -G^*(-s) - H^*(s) \\right\\} = \\max_{s \\in \\lambda B(F)} \\left\\{ -(\\frac{1}{2}\\|-s\\|_2^2 + \\langle -s, y \\rangle) \\right\\} = \\max_{s \\in \\lambda B(F)} \\left\\{ -\\frac{1}{2}\\|s\\|_2^2 + \\langle s, y \\rangle \\right\\}.\n$$\nThis maximization is equivalent to the minimization problem:\n$$\n\\min_{s \\in \\lambda B(F)} \\left\\{ \\frac{1}{2}\\|s\\|_2^2 - \\langle s, y \\rangle \\right\\}.\n$$\nBy completing the square, this becomes:\n$$\n\\min_{s \\in \\lambda B(F)} \\left\\{ \\frac{1}{2}\\|s-y\\|_2^2 - \\frac{1}{2}\\|y\\|_2^2 \\right\\}.\n$$\nLet $s_{dual}^\\star$ be the solution to this problem. Then $s_{dual}^\\star = \\operatorname{proj}_{\\lambda B(F)}(y)$.\nLet's define a new variable $s' = s/\\lambda$. The optimization is over $s' \\in B(F)$:\n$$\n\\min_{s' \\in B(F)} \\frac{1}{2}\\|\\lambda s' - y\\|_2^2 = \\min_{s' \\in B(F)} \\frac{\\lambda^2}{2} \\|s' - y/\\lambda\\|_2^2.\n$$\nThis is equivalent to finding the projection of $y/\\lambda$ onto $B(F)$. Let $s^\\star = \\operatorname{proj}_{B(F)}(y/\\lambda)$. Then the optimal dual variable is $s_{dual}^\\star = \\lambda s^\\star$.\n\nThe primal-dual optimality conditions state that the primal solution $z^\\star = \\operatorname{prox}_{\\lambda f^{L}}(y)$ is related to the dual solution $s_{dual}^\\star$ by $z^\\star = \\nabla G^*(-s_{dual}^\\star)$.\nSince $\\nabla_s G^*(s) = s+y$, we have:\n$$\nz^\\star = (-s_{dual}^\\star) + y = y - s_{dual}^\\star = y - \\lambda s^\\star.\n$$\nThis completes the derivation.\n\n### Part 3: Algorithmic Implementation via Min-Cut\n\nThe core of the computation is finding $s^\\star = \\operatorname{proj}_{B(F)}(v)$ where $v=y/\\lambda$. This is a projection onto a polytope defined by an exponential number of inequalities, requiring a cutting-plane method. The separation oracle for this task is to find, for a given point $s$, the most violated inequality:\n$$\n\\min_{S \\subseteq V} \\left( F(S) - \\sum_{i \\in S} s_i \\right).\n$$\nWe need to show this can be solved via min-cut on the specified auxiliary graph. Let us construct the graph $\\mathcal{G}'=(V',E')$ with $V' = V \\cup \\{src, t\\}$, source $src$ and sink $t$. The edge capacities are defined as:\n1. $C(src, i) = \\max(s_i, 0)$ for each $i \\in V$.\n2. $C(i, t) = \\max(-s_i, 0)$ for each $i \\in V$.\n3. $C(c, p) = w_{pc}$ for each edge $(p,c) \\in E$.\n\nConsider an $s-t$ cut, which partitions $V'$ into two sets, $A$ and $\\bar{A}$, with $src \\in A$ and $t \\in \\bar{A}$. Let $S = V \\cap A$. Then $V\\setminus S = V \\cap \\bar{A}$. The capacity of the cut is $C(A, \\bar{A}) = \\sum_{u \\in A, v \\in \\bar{A}} C(u,v)$.\nThe total capacity is the sum of capacities from the three types of edges crossing the cut:\n1. Edges $(src, i)$: cross if $i \\in \\bar{A}$ (i.e., $i \\in V \\setminus S$). Contribution: $\\sum_{i \\in V\\setminus S} \\max(s_i, 0)$.\n2. Edges $(i, t)$: cross if $i \\in A$ (i.e., $i \\in S$). Contribution: $\\sum_{i \\in S} \\max(-s_i, 0)$.\n3. Edges $(c, p)$: cross if $c \\in A$ (i.e., $c \\in S$) and $p \\in \\bar{A}$ (i.e., $p \\in V \\setminus S$). Contribution: $\\sum_{(p,c) \\in E} w_{pc} \\mathbf{1}\\{c \\in S, p \\notin S\\} = F(S)$.\n\nSo the total cut capacity is:\n$$\nC(A, \\bar{A}) = F(S) + \\sum_{i \\in S} \\max(-s_i, 0) + \\sum_{i \\in V\\setminus S} \\max(s_i, 0).\n$$\nWe use the identity $s_i = \\max(s_i, 0) - \\max(-s_i, 0)$.\nLet's rewrite the sum involving $s_i$:\n\\begin{align*}\n& \\sum_{i \\in S} \\max(-s_i, 0) + \\sum_{i \\in V} \\max(s_i, 0) - \\sum_{i \\in S} \\max(s_i, 0) \\\\\n= & \\sum_{i \\in V} \\max(s_i, 0) - \\sum_{i \\in S} (\\max(s_i, 0) - \\max(-s_i, 0)) \\\\\n= & \\sum_{i \\in V} \\max(s_i, 0) - \\sum_{i \\in S} s_i.\n\\end{align*}\nThe cut capacity is therefore $C(A, \\bar{A}) = F(S) - \\sum_{i \\in S} s_i + K$, where $K = \\sum_{i \\in V} \\max(s_i, 0)$ is a constant with respect to the choice of $S$. Minimizing the cut capacity is equivalent to finding $\\min_{S \\subseteq V} (F(S) - \\sum_{i \\in S} s_i)$. The set $S$ that achieves the minimum cut is the set of nodes on the source side of the partition (excluding the source itself), which is precisely the most violated set required by the separation oracle.\n\n**Active-Set Algorithm for Projection:**\nThe algorithm to compute $s^\\star = \\operatorname{proj}_{B(F)}(v)$ is as follows:\n1. **Initialization:** The base polyhedron $B(F)$ contains the constraint $\\sum_{i\\in V} s_i = F(V)$. For a tree, $F(V) = 0$ since for any edge $(p,c)\\in E$, both $p$ and $c$ are in $V$. The initial active set of constraints is $\\mathcal{A}_0 = \\{V\\}$. The initial point $s_0$ is the projection of $v$ onto the hyperplane $\\mathbf{1}^T s = 0$.\n2. **Iteration:** At step $k$, let the current point be $s_k$ and the active set of constraints be $\\mathcal{A}_k$.\n   a. **Separation Oracle:** Construct the auxiliary graph using $s_k$. Find the min-cut to determine the set $S_k^*$ that minimizes $F(S) - \\mathbf{1}_S^T s_k$.\n   b. **Check for termination:** Calculate the violation $\\delta_k = F(S_k^*) - \\mathbf{1}_{S_k^*}^T s_k$. If $\\delta_k \\ge \\epsilon$ for a small negative tolerance $\\epsilon$ (e.g., $\\epsilon=-10^{-9}$), then $s_k$ is feasible and thus the optimal projection. Terminate and set $s^\\star=s_k$.\n   c. **Add Constraint:** If $\\delta_k < \\epsilon$, the constraint for $S_k^*$ is violated. Add $S_k^*$ to the active set: $\\mathcal{A}_{k+1} = \\mathcal{A}_k \\cup \\{S_k^*\\}$.\n   d. **Update Projection:** Compute the new point $s_{k+1}$ by projecting $v$ onto the affine subspace formed by the intersection of all constraints in $\\mathcal{A}_{k+1}$ holding with equality. This is done by solving a small system of linear equations derived from the KKT conditions of the constrained quadratic program. Repeat the iteration.\n\nThis iterative process is guaranteed to converge in at most $n$ iterations, as a new linearly independent constraint is added at each step. Once $s^\\star$ is found, the final result is computed as $\\operatorname{prox}_{\\lambda f^{L}}(y) = y - \\lambda s^\\star$.",
            "answer": "```python\nimport numpy as np\nimport collections\n\ndef edmonds_karp(capacity, source, sink):\n    \"\"\"\n    Implementation of the Edmonds-Karp algorithm for max-flow/min-cut.\n    \"\"\"\n    n = len(capacity)\n    flow = 0\n    residual = np.copy(capacity)\n    while True:\n        parent = {node: None for node in range(n)}\n        queue = collections.deque([source])\n        path_found = False\n        while queue:\n            u = queue.popleft()\n            if u == sink:\n                path_found = True\n                break\n            for v in range(n):\n                if parent[v] is None and residual[u, v] > 1e-12: # Use tolerance\n                    parent[v] = u\n                    queue.append(v)\n        \n        if not path_found:\n            break\n\n        path_flow = float('inf')\n        v = sink\n        while v != source:\n            u = parent[v]\n            path_flow = min(path_flow, residual[u, v])\n            v = u\n        \n        flow += path_flow\n\n        v = sink\n        while v != source:\n            u = parent[v]\n            residual[u, v] -= path_flow\n            residual[v, u] += path_flow\n            v = u\n\n    # Find the min-cut partition (source side)\n    source_set_nodes = set()\n    q = collections.deque([source])\n    visited = {source}\n    while q:\n        u = q.popleft()\n        source_set_nodes.add(u)\n        for v in range(n):\n            if v not in visited and residual[u, v] > 1e-12: # Use tolerance\n                visited.add(v)\n                q.append(v)\n    \n    return flow, source_set_nodes\n\n\ndef calculate_F(S, edges, weights_dict):\n    \"\"\"\n    Computes the value of the set function F(S).\n    \"\"\"\n    cost = 0.0\n    # S contains 0-indexed nodes\n    for p, c in edges:\n        if c in S and p not in S:\n            cost += weights_dict.get((p, c), 0.0)\n    return cost\n\ndef solve_prox(n, edges, weights_dict, y, lam):\n    \"\"\"\n    Computes the proximal operator using the active-set projection algorithm.\n    \"\"\"\n    if lam == 0:\n        return y.astype(float)\n\n    v = y / lam\n    \n    # Initial active set: sum(s_i) = F(V) = 0\n    active_constraints_A = np.ones((1, n))\n    active_constraints_b = np.array([0.0]) # F(V) = 0 for a tree\n    active_sets = {frozenset(range(n))}\n\n    s_star = None\n    \n    for _ in range(n + 2): # Loop with a safe upper bound\n        \n        # Project v onto the affine subspace defined by active constraints\n        # Solve: s = argmin ||s-v||^2 s.t. As = b\n        # Solution: s = v + A.T * inv(A A.T) * (b - A v)\n        M = active_constraints_A @ active_constraints_A.T\n        rhs = active_constraints_b - active_constraints_A @ v\n        \n        try:\n            # Use pseudo-inverse for stability against near-singular matrices\n            mu = np.linalg.pinv(M) @ rhs\n        except np.linalg.LinAlgError:\n            mu = np.linalg.lstsq(M, rhs, rcond=None)[0]\n\n        s = v + active_constraints_A.T @ mu\n\n        # Separation oracle: find most violated constraint\n        # min_S { F(S) - <s, 1_S> }\n        \n        # Build graph for min-cut\n        num_graph_nodes = n + 2\n        source, sink = n, n + 1\n        capacity = np.zeros((num_graph_nodes, num_graph_nodes))\n\n        for i in range(n):\n            if s[i] > 0:\n                capacity[source, i] = s[i]\n            if s[i] < 0:\n                capacity[i, sink] = -s[i]\n\n        for p, c in edges:\n            capacity[c, p] = weights_dict.get((p, c), 0.0)\n        \n        # Solve min-cut\n        _, source_partition_nodes = edmonds_karp(capacity, source, sink)\n        \n        S_star_nodes = source_partition_nodes - {source, sink}\n        \n        f_s_val = calculate_F(S_star_nodes, edges, weights_dict)\n        s_sum = np.sum(s[list(S_star_nodes)]) if S_star_nodes else 0.0\n        violation = f_s_val - s_sum\n\n        if violation >= -1e-9:\n            s_star = s\n            break\n            \n        S_star_fs = frozenset(S_star_nodes)\n        if S_star_fs not in active_sets:\n            indicator_S = np.zeros(n)\n            if S_star_nodes:\n                indicator_S[list(S_star_nodes)] = 1.0\n            \n            # Add to active set\n            active_constraints_A = np.vstack([active_constraints_A, indicator_S])\n            active_constraints_b = np.append(active_constraints_b, f_s_val)\n            active_sets.add(S_star_fs)\n    \n    if s_star is None: # Failsafe if loop finishes\n        s_star = s\n\n    prox_result = y - lam * s_star\n    return prox_result\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Nodes are 1-based in problem, converting to 0-based for implementation.\n    test_cases_raw = [\n        {'n': 5, 'E': [(1,2),(1,3),(3,4),(3,5)], 'w_all': 1.0, 'y': [0.9,-0.2,0.4,1.1,-0.5], 'lambda': 0.7},\n        {'n': 5, 'E': [(1,2),(1,3),(3,4),(3,5)], 'w_all': 1.0, 'y': [0.9,-0.2,0.4,1.1,-0.5], 'lambda': 0.0},\n        {'n': 5, 'E': [(1,2),(1,3),(3,4),(3,5)], 'w_all': 1.0, 'y': [0.5,0.5,0.5,0.5,0.5], 'lambda': 1.0},\n        {'n': 4, 'E': [(1,2),(2,3),(2,4)], 'w': {(1,2):2.0,(2,3):0.5,(2,4):1.5}, 'y': [-0.3,0.8,-1.2,0.6], 'lambda': 0.6},\n        {'n': 5, 'E': [(1,2),(1,3),(3,4),(3,5)], 'w_all': 1.0, 'y': [0.9,-0.2,0.4,1.1,-0.5], 'lambda': 2.5},\n    ]\n\n    results = []\n    for case in test_cases_raw:\n        n = case['n']\n        # Convert to 0-indexed edges\n        edges = [(p-1, c-1) for p, c in case['E']]\n        y = np.array(case['y'])\n        lam = case['lambda']\n        \n        weights_dict = {}\n        if 'w_all' in case:\n            for p, c in edges:\n                weights_dict[(p, c)] = case['w_all']\n        else:\n             # Convert to 0-indexed weights dictionary\n            for (p,c), w in case['w'].items():\n                weights_dict[(p-1, c-1)] = w\n        \n        prox_result = solve_prox(n, edges, weights_dict, y, lam)\n        \n        result_str = f\"[{','.join([f'{x:.6f}' for x in prox_result])}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}