{
    "hands_on_practices": [
        {
            "introduction": "Greedy algorithms like Simultaneous Orthogonal Matching Pursuit (SOMP) build a solution by iteratively selecting the most relevant atoms from a dictionary. This first exercise takes you to the heart of this process, asking you to derive the fundamental selection rule from first principles and apply it to a concrete example. Understanding this core mechanism is crucial for grasping how many joint sparse recovery algorithms operate and make their decisions. ",
            "id": "3455704",
            "problem": "Consider a joint sparse recovery setting with $m=3$ sensing dimensions and $L=3$ measurement vectors. Let the dictionary $A \\in \\mathbb{R}^{3 \\times 5}$ have unit-norm columns $A_{:,j}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{3}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}}  0 \\\\\n0  0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}}  1\n\\end{pmatrix},\n$$\nso that $A_{:,1} = (1,0,0)^\\top$, $A_{:,2} = \\big(\\tfrac{1}{\\sqrt{2}},\\tfrac{1}{\\sqrt{2}},0\\big)^\\top$, $A_{:,3} = \\big(0,\\tfrac{1}{\\sqrt{2}},\\tfrac{1}{\\sqrt{2}}\\big)^\\top$, $A_{:,4} = \\big(\\tfrac{1}{\\sqrt{3}},\\tfrac{1}{\\sqrt{3}},\\tfrac{1}{\\sqrt{3}}\\big)^\\top$, and $A_{:,5} = (0,0,1)^\\top$. The unknown jointly sparse coefficient matrix $X^\\star \\in \\mathbb{R}^{5 \\times 3}$ has exactly two nonzero rows located at indices $\\{2,4\\}$, with row-$2$ equal to\n$$\nv_2 \\;=\\; \\begin{pmatrix} 2  0  -1 \\end{pmatrix}\n$$\nand row-$4$ equal to\n$$\nv_4 \\;=\\; \\begin{pmatrix} 1  -2  2 \\end{pmatrix},\n$$\nand all other rows equal to the zero row. The noise-free measurements are given by $Y = A X^\\star \\in \\mathbb{R}^{3 \\times 3}$.\n\nAt the initialization of Simultaneous Orthogonal Matching Pursuit (SOMP), the residual is $R = Y - A X$ with $X$ equal to the zero matrix, hence $R=Y$. The algorithm selects the first column (atom) by greedily maximizing the immediate decrease of the residual Frobenius norm $\\|R\\|_{F}$ produced by a least-squares update restricted to a single column subspace.\n\nTasks:\n- Starting only from the definitions of the Frobenius norm and the orthogonal projection onto a one-dimensional subspace, derive the one-step greedy selection rule in terms of $A$, $R$, and inner products; do not assume any pre-stated SOMP formula.\n- Then, apply your derived rule to the specific $A$ and $X^\\star$ above to compute the index $j^\\star \\in \\{1,2,3,4,5\\}$ of the first selected atom. You may utilize linearity to compute $A^\\top Y$ without explicitly forming $Y$.\n\nProvide your final answer as the single integer $j^\\star$; no rounding is required and no units are involved.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of sparse recovery, is well-posed with a clear objective, and provides a complete and consistent set of data for its resolution.\n\nThe first task is to derive the greedy selection rule for the first step of the Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm. The algorithm starts with an empty support set, $S_0 = \\emptyset$, a zero coefficient matrix estimate, $X_0 = \\mathbf{0}_{5 \\times 3}$, and an initial residual $R_0 = Y - A X_0 = Y$. The goal is to select an index $j \\in \\{1, 2, 3, 4, 5\\}$ that, when added to the support, leads to the maximum decrease in the Frobenius norm of the residual.\n\nLet's consider adding a single atom, corresponding to the index $j$, to the support. The new support set would be $S_1 = \\{j\\}$. The updated coefficient matrix, which we denote as $X_1^{(j)}$, will have only its $j$-th row being non-zero. Let this row be $x_j^\\top \\in \\mathbb{R}^{1 \\times 3}$. The corresponding signal estimate is $A X_1^{(j)} = A_{:,j} x_j^\\top$.\n\nTo find the optimal coefficient row $x_j^\\top$, we solve a least-squares problem to minimize the norm of the new residual, $R_1^{(j)} = R_0 - A_{:,j} x_j^\\top = Y - A_{:,j} x_j^\\top$:\n$$\n\\min_{x_j^\\top \\in \\mathbb{R}^{1 \\times 3}} \\|Y - A_{:,j} x_j^\\top\\|_F^2\n$$\nThe Frobenius norm squared is the sum of the squared Euclidean norms of the matrix columns. Let $Y = \\begin{pmatrix} y_1  y_2  y_3 \\end{pmatrix}$ and $x_j^\\top = \\begin{pmatrix} x_{j,1}  x_{j,2}  x_{j,3} \\end{pmatrix}$. The optimization problem can be decoupled for each column $l \\in \\{1, 2, 3\\}$:\n$$\n\\min_{x_{j,1}, x_{j,2}, x_{j,3}} \\sum_{l=1}^{3} \\|y_l - A_{:,j} x_{j,l}\\|_2^2\n$$\nThis is equivalent to solving three independent one-dimensional least-squares problems:\n$$\n\\min_{x_{j,l} \\in \\mathbb{R}} \\|y_l - A_{:,j} x_{j,l}\\|_2^2 \\quad \\text{for } l \\in \\{1, 2, 3\\}\n$$\nThe solution to each of these is the orthogonal projection of the vector $y_l$ onto the subspace spanned by the vector $A_{:,j}$. The optimal coefficient $x_{j,l}^{\\text{opt}}$ is given by:\n$$\nx_{j,l}^{\\text{opt}} = \\frac{A_{:,j}^\\top y_l}{\\|A_{:,j}\\|_2^2}\n$$\nThe problem states that all columns of $A$ are unit-norm, so $\\|A_{:,j}\\|_2^2 = 1$. This simplifies the optimal coefficient to $x_{j,l}^{\\text{opt}} = A_{:,j}^\\top y_l$.\n\nCombining the results for all columns $l$, the optimal coefficient row vector $x_j^{\\text{opt}\\top}$ is given by:\n$$\nx_j^{\\text{opt}\\top} = \\begin{pmatrix} A_{:,j}^\\top y_1  A_{:,j}^\\top y_2  A_{:,j}^\\top y_3 \\end{pmatrix} = A_{:,j}^\\top Y\n$$\nThe new residual is $R_1^{(j)} = Y - A_{:,j} (A_{:,j}^\\top Y)$. The term $A_{:,j} (A_{:,j}^\\top Y)$ represents the orthogonal projection of the columns of $Y$ onto the subspace spanned by $A_{:,j}$. Let $P_j = A_{:,j} A_{:,j}^\\top$ be the projection matrix onto this subspace. Then $R_1^{(j)} = (I - P_j) Y$.\n\nBy the Pythagorean theorem for orthogonal projections, the energy of $Y$ can be decomposed as:\n$$\n\\|Y\\|_F^2 = \\|P_j Y\\|_F^2 + \\|(I - P_j) Y\\|_F^2 = \\|P_j Y\\|_F^2 + \\|R_1^{(j)}\\|_F^2\n$$\nThe decrease in the squared Frobenius norm of the residual is $\\|R_0\\|_F^2 - \\|R_1^{(j)}\\|_F^2 = \\|Y\\|_F^2 - \\|R_1^{(j)}\\|_F^2 = \\|P_j Y\\|_F^2$.\n\nThe SOMP selection rule is to choose the index $j$ that maximizes this decrease:\n$$\nj^\\star = \\arg\\max_{j \\in \\{1, ..., 5\\}} \\|P_j Y\\|_F^2 = \\arg\\max_{j \\in \\{1, ..., 5\\}} \\|A_{:,j} (A_{:,j}^\\top Y)\\|_F^2\n$$\nLet's analyze the term being maximized. Let $c_j^\\top = A_{:,j}^\\top Y$ be the row vector of coefficients. The matrix $A_{:,j} c_j^\\top$ is a rank-one matrix. Its Frobenius norm squared is:\n$$\n\\|A_{:,j} c_j^\\top\\|_F^2 = \\mathrm{Tr}((A_{:,j} c_j^\\top)^\\top (A_{:,j} c_j^\\top)) = \\mathrm{Tr}(c_j A_{:,j}^\\top A_{:,j} c_j^\\top) = \\mathrm{Tr}(c_j \\|A_{:,j}\\|_2^2 c_j^\\top)\n$$\nSince $\\|A_{:,j}\\|_2^2=1$, this becomes $\\mathrm{Tr}(c_j c_j^\\top) = \\mathrm{Tr}(c_j^\\top c_j) = \\|c_j\\|_2^2$.\nSo, the selection rule is to choose the index $j$ that maximizes the squared Euclidean norm of the coefficient row vector $c_j^\\top = A_{:,j}^\\top Y$. This row vector is the $j$-th row of the matrix product $A^\\top Y$. Therefore, the selection rule is:\n$$\nj^\\star = \\arg\\max_{j \\in \\{1, ..., 5\\}} \\| (A^\\top R_0)_{j,:} \\|_2^2 = \\arg\\max_{j \\in \\{1, ..., 5\\}} \\| (A^\\top Y)_{j,:} \\|_2^2\n$$\nThis completes the derivation.\n\nThe second task is to apply this rule. We need to compute $C = A^\\top Y$ and find the row with the largest $\\ell_2$-norm. We are advised to use linearity. The measurement matrix $Y$ is given by $Y = A X^\\star$. The structure of $X^\\star$ is such that only its second and fourth rows are non-zero. Let these rows be $v_2 = \\begin{pmatrix} 2  0  -1 \\end{pmatrix}$ and $v_4 = \\begin{pmatrix} 1  -2  2 \\end{pmatrix}$. We can write $Y$ as a sum of outer products:\n$$\nY = \\sum_{k=1}^{5} A_{:,k} (X^\\star)_{k,:} = A_{:,2} v_2 + A_{:,4} v_4\n$$\nNow, we compute $C = A^\\top Y$:\n$$\nC = A^\\top (A_{:,2} v_2 + A_{:,4} v_4) = (A^\\top A_{:,2}) v_2 + (A^\\top A_{:,4}) v_4\n$$\nLet $G = A^\\top A$ be the Gram matrix of $A$. The term $A^\\top A_{:,k}$ is the $k$-th column of $G$, denoted $G_{:,k}$. Thus:\n$$\nC = G_{:,2} v_2 + G_{:,4} v_4\n$$\nThe $j$-th row of $C$, denoted $C_{j,:}$, is a row vector given by:\n$$\nC_{j,:} = G_{j2} v_2 + G_{j4} v_4\n$$\nwhere $G_{jk}$ is the element in the $j$-th row and $k$-th column of $G$. We need to compute $\\|C_{j,:}\\|_2^2$:\n$$\n\\|C_{j,:}\\|_2^2 = \\|G_{j2} v_2 + G_{j4} v_4\\|_2^2 = G_{j2}^2 \\|v_2\\|_2^2 + G_{j4}^2 \\|v_4\\|_2^2 + 2 G_{j2} G_{j4} \\langle v_2, v_4 \\rangle\n$$\nLet's compute the norms and inner product of $v_2$ and $v_4$:\n$\\|v_2\\|_2^2 = 2^2 + 0^2 + (-1)^2 = 4 + 1 = 5$.\n$\\|v_4\\|_2^2 = 1^2 + (-2)^2 + 2^2 = 1 + 4 + 4 = 9$.\n$\\langle v_2, v_4 \\rangle = (2)(1) + (0)(-2) + (-1)(2) = 2 - 2 = 0$.\nSince $v_2$ and $v_4$ are orthogonal, the expression for the squared norm simplifies to:\n$$\n\\|C_{j,:}\\|_2^2 = 5 G_{j2}^2 + 9 G_{j4}^2\n$$\nNext, we compute the necessary elements of the Gram matrix $G=A^\\top A$ with $A_{:,j}^\\top A_{:,k} = G_{jk}$.\n$A = \\begin{pmatrix} 1  \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{3}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}}  0 \\\\ 0  0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{3}}  1 \\end{pmatrix}$.\nThe required entries are from the second and fourth columns of $G$. The diagonal entries are $G_{jj}=1$.\n$G_{12} = A_{:,1}^\\top A_{:,2} = \\frac{1}{\\sqrt{2}}$.\n$G_{22} = 1$.\n$G_{32} = A_{:,3}^\\top A_{:,2} = \\frac{1}{2}$.\n$G_{42} = A_{:,4}^\\top A_{:,2} = \\frac{1}{\\sqrt{3}}\\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{3}}\\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{6}} = \\sqrt{\\frac{2}{3}}$.\n$G_{52} = A_{:,5}^\\top A_{:,2} = 0$.\n\n$G_{14} = A_{:,1}^\\top A_{:,4} = \\frac{1}{\\sqrt{3}}$.\n$G_{24} = G_{42} = \\sqrt{\\frac{2}{3}}$.\n$G_{34} = A_{:,3}^\\top A_{:,4} = \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{3}} + \\frac{1}{\\sqrt{2}}\\frac{1}{\\sqrt{3}} = \\frac{2}{\\sqrt{6}} = \\sqrt{\\frac{2}{3}}$.\n$G_{44} = 1$.\n$G_{54} = A_{:,5}^\\top A_{:,4} = \\frac{1}{\\sqrt{3}}$.\n\nNow we compute $f(j) = 5 G_{j2}^2 + 9 G_{j4}^2$ for each $j \\in \\{1, 2, 3, 4, 5\\}$.\nFor $j=1$: $f(1) = 5(G_{12})^2 + 9(G_{14})^2 = 5(\\frac{1}{\\sqrt{2}})^2 + 9(\\frac{1}{\\sqrt{3}})^2 = 5(\\frac{1}{2}) + 9(\\frac{1}{3}) = \\frac{5}{2} + 3 = 5.5$.\nFor $j=2$: $f(2) = 5(G_{22})^2 + 9(G_{24})^2 = 5(1)^2 + 9(\\sqrt{\\frac{2}{3}})^2 = 5(1) + 9(\\frac{2}{3}) = 5 + 6 = 11$.\nFor $j=3$: $f(3) = 5(G_{32})^2 + 9(G_{34})^2 = 5(\\frac{1}{2})^2 + 9(\\sqrt{\\frac{2}{3}})^2 = 5(\\frac{1}{4}) + 9(\\frac{2}{3}) = \\frac{5}{4} + 6 = 7.25$.\nFor $j=4$: $f(4) = 5(G_{42})^2 + 9(G_{44})^2 = 5(\\sqrt{\\frac{2}{3}})^2 + 9(1)^2 = 5(\\frac{2}{3}) + 9 = \\frac{10}{3} + 9 = \\frac{10+27}{3} = \\frac{37}{3} \\approx 12.33$.\nFor $j=5$: $f(5) = 5(G_{52})^2 + 9(G_{54})^2 = 5(0)^2 + 9(\\frac{1}{\\sqrt{3}})^2 = 0 + 9(\\frac{1}{3}) = 3$.\n\nComparing the values:\n$f(1) = 5.5$\n$f(2) = 11$\n$f(3) = 7.25$\n$f(4) = \\frac{37}{3} \\approx 12.33$\n$f(5) = 3$\nThe maximum value is $f(4) = \\frac{37}{3}$. This corresponds to the index $j=4$. Therefore, the first atom selected by SOMP is $j^\\star = 4$.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "While algorithms tell us *how* to find a solution, theoretical guarantees tell us *when* that solution is likely to be correct. The Block Restricted Isometry Property (BRIP) is a cornerstone of these guarantees, characterizing how well a sensing matrix preserves the energy of block-sparse signals. This coding exercise provides a practical method for empirically calculating the Block Restricted Isometry Constant (BRIC), giving you a tangible sense of what this important theoretical property means for a given matrix. ",
            "id": "3455725",
            "problem": "Consider a real or complex sensing matrix $A \\in \\mathbb{C}^{m \\times n}$ whose columns are partitioned into $G$ disjoint groups (blocks) $\\{ \\mathcal{B}_1, \\ldots, \\mathcal{B}_G \\}$ such that $\\bigcup_{g=1}^G \\mathcal{B}_g = \\{1,2,\\ldots,n\\}$ and $\\mathcal{B}_g \\cap \\mathcal{B}_{g'} = \\emptyset$ for all $g \\neq g'$. A vector $x \\in \\mathbb{C}^n$ is said to be $s$-block-sparse if its support is contained in the union of at most $s$ blocks. The Block Restricted Isometry Property (RIP) constant, also called the Block Restricted Isometry Constant (BRIC), denoted $\\delta_s^B$, is defined as the smallest $\\delta \\ge 0$ such that for every $s$-block-sparse $x \\in \\mathbb{C}^n$, the following inequality holds:\n$$\n(1 - \\delta)\\,\\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta)\\,\\|x\\|_2^2.\n$$\nEquivalently, for each union of at most $s$ blocks $S \\subseteq \\{1,\\ldots,G\\}$, letting $A_S$ denote the submatrix formed by the columns of $A$ indexed by $\\bigcup_{g \\in S} \\mathcal{B}_g$, the extremal quadratic form of the Gram matrix $A_S^* A_S$ controls $\\delta_s^B$ via its eigenvalues $\\{\\lambda_i\\}$ (which are the squares of the singular values $\\{\\sigma_i\\}$ of $A_S$):\n$$\n\\delta_s^B = \\sup_{\\substack{S \\subseteq \\{1,\\ldots,G\\} \\\\ |S| \\le s}} \\max\\left\\{\\lambda_{\\max}(A_S^*A_S) - 1,\\; 1 - \\lambda_{\\min}(A_S^*A_S)\\right\\} = \\sup_{\\substack{S \\subseteq \\{1,\\ldots,G\\} \\\\ |S| \\le s}} \\max\\left\\{\\sigma_{\\max}(A_S)^2 - 1,\\; 1 - \\sigma_{\\min}(A_S)^2\\right\\}.\n$$\nIn practice, one often empirically estimates $\\delta_s^B$ for a fixed $A$ and block partition by enumerating all supports consisting of $s$ blocks, computing the singular values of each corresponding submatrix, and taking the worst-case deviation from $1$. To place results on a common scale, it is standard to first normalize each column of $A$ to unit $\\ell_2$-norm before computing the Block Restricted Isometry Constant.\n\nYour task is to write a complete, runnable program that computes an empirical estimate of $\\delta_2^B$ for a small matrix $A$ and a given group partition by evaluating the maximal and minimal singular values over all supports consisting of exactly two blocks, and reporting the resulting bound $\\widehat{\\delta}_2^B = \\max\\{\\beta - 1,\\, 1 - \\alpha\\}$, where\n$$\n\\alpha = \\min_{\\substack{S \\subseteq \\{1,\\ldots,G\\} \\\\ |S| = 2}} \\sigma_{\\min}(A_S)^2, \\quad \\beta = \\max_{\\substack{S \\subseteq \\{1,\\ldots,G\\} \\\\ |S| = 2}} \\sigma_{\\max}(A_S)^2.\n$$\nThe program must perform column normalization of $A$ (i.e., each column scaled to have $\\ell_2$-norm equal to $1$) prior to the computation.\n\nDesign the program to process the following test suite, where each test case specifies how to construct $A$ and the block partition:\n\n- Test case 1 (general case): Let $m = 20$, $n = 24$, and $G = 6$ blocks with equal sizes $|\\mathcal{B}_g| = 4$. Construct $A$ with independent and identically distributed real Gaussian entries drawn from $\\mathcal{N}(0,1)$ using a fixed random seed $0$. Normalize columns to unit $\\ell_2$-norm, then compute $\\widehat{\\delta}_2^B$ by enumerating all pairs of blocks.\n\n- Test case 2 (rank-deficient edge case): Let $m = 3$, $n = 5$, and $G = 3$ blocks with sizes $|\\mathcal{B}_1| = 2$, $|\\mathcal{B}_2| = 2$, $|\\mathcal{B}_3| = 1$. Construct $A$ with independent and identically distributed real Gaussian entries drawn from $\\mathcal{N}(0,1)$ using a fixed random seed $1$. Normalize columns and compute $\\widehat{\\delta}_2^B$. Note that any two-block union selecting $4$ columns into a matrix with only $3$ rows will yield at least one zero singular value, resulting in $1 - \\sigma_{\\min}(A_S)^2 \\ge 1$ for such supports.\n\n- Test case 3 (ideal case): Let $m = 12$, $n = 12$, and $G = 3$ blocks with equal sizes $|\\mathcal{B}_g| = 4$. Set $A$ equal to the $12 \\times 12$ identity matrix. Columns are already normalized. Compute $\\widehat{\\delta}_2^B$ by enumerating all pairs of blocks. In this case, $\\widehat{\\delta}_2^B$ should be exactly $0$ because all singular values of any submatrix formed by columns of the identity are equal to $1$.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list of floats enclosed in square brackets, rounded to six decimal places, in the order of the test cases given above (e.g., \"[0.123456,1.000000,0.000000]\"). No other output should be produced.",
            "solution": "The problem requires the computation of an empirical estimate of the Block Restricted Isometry Constant (BRIC), denoted $\\widehat{\\delta}_2^B$, for a given sensing matrix $A \\in \\mathbb{C}^{m \\times n}$ and a specified partition of its columns into $G$ blocks. The constant is defined over all submatrices formed by selecting exactly two blocks of columns.\n\nThe provided definition for the empirical constant is:\n$$\n\\widehat{\\delta}_2^B = \\max\\{\\beta - 1,\\, 1 - \\alpha\\}\n$$\nwhere\n$$\n\\alpha = \\min_{\\substack{S \\subseteq \\{1,\\ldots,G\\} \\\\ |S| = 2}} \\sigma_{\\min}(A_S)^2, \\quad \\beta = \\max_{\\substack{S \\subseteq \\{1,\\ldots,G\\} \\\\ |S| = 2}} \\sigma_{\\max}(A_S)^2\n$$\nHere, $S$ is a set of two block indices, $A_S$ is the submatrix of $A$ formed by columns from the blocks in $S$, and $\\sigma_{\\min}(A_S)$ and $\\sigma_{\\max}(A_S)$ are the minimum and maximum singular values of $A_S$, respectively. The relationship between the singular values of $A_S$ and the eigenvalues of the Gram matrix $A_S^* A_S$ is given by $\\sigma_i(A_S)^2 = \\lambda_i(A_S^* A_S)$.\n\nThe computational procedure involves several distinct steps, which will be followed for each test case.\n\n**Step 1: Column Normalization**\nAs a standard preprocessing step in the context of the Restricted Isometry Property, each column of the matrix $A$ must be normalized to have a unit $\\ell_2$-norm. For each column $a_j$ of $A$, where $j \\in \\{1, \\ldots, n\\}$, the normalized column $a'_j$ is computed as:\n$$\na'_j = \\frac{a_j}{\\|a_j\\|_2}\n$$\nThis ensures that the trivial case of a single column submatrix $A_{\\{j\\}}$ has $\\sigma_{\\max}(A_{\\{j\\}})^2 = \\|a'_j\\|_2^2 = 1$, aligning the baseline for the RIP condition. All subsequent calculations are performed on this normalized matrix, which we will continue to denote as $A$ for simplicity.\n\n**Step 2: Block Partitioning and Submatrix Enumeration**\nThe columns of the $m \\times n$ matrix $A$ are partitioned into $G$ disjoint sets of indices, $\\{\\mathcal{B}_1, \\ldots, \\mathcal{B}_G\\}$. The\ncomputation of $\\widehat{\\delta}_2^B$ requires examining all possible submatrices formed by the union of exactly two of these blocks. The number of such pairs of blocks is given by the binomial coefficient $\\binom{G}{2}$. For each pair of block indices $\\{g_1, g_2\\}$, where $1 \\le g_1  g_2 \\le G$, we form a column index set $I_S = \\mathcal{B}_{g_1} \\cup \\mathcal{B}_{g_2}$. The submatrix $A_S$ is then constructed by selecting the columns of $A$ whose indices are in $I_S$.\n\n**Step 3: Singular Value Analysis and Extremal Value Tracking**\nFor each submatrix $A_S$ constructed in the previous step, we must compute its singular values. The squared singular values, $\\sigma_i(A_S)^2$, correspond to the eigenvalues of the Gram matrix $A_S^* A_S$. We are interested in the extremal squared singular values: $\\sigma_{\\min}(A_S)^2$ and $\\sigma_{\\max}(A_S)^2$.\n\nTo find $\\alpha$ and $\\beta$, we initialize two variables: $\\alpha$ to a value larger than any possible squared singular value (e.g., $\\infty$) and $\\beta$ to a value smaller than any possible squared singular value (e.g., $0$, since they are non-negative). We then iterate through all $\\binom{G}{2}$ unique pairs of blocks. For each corresponding submatrix $A_S$, we perform the following updates:\n$$\n\\alpha \\leftarrow \\min(\\alpha, \\sigma_{\\min}(A_S)^2)\n$$\n$$\n\\beta \\leftarrow \\max(\\beta, \\sigma_{\\max}(A_S)^2)\n$$\nAfter iterating through all pairs, $\\alpha$ will hold the global minimum of the smallest squared singular values, and $\\beta$ will hold the global maximum of the largest squared singular values.\n\n**Step 4: Final Calculation**\nWith the final values of $\\alpha$ and $\\beta$, the empirical constant $\\widehat{\\delta}_2^B$ is calculated as per its definition:\n$$\n\\widehat{\\delta}_2^B = \\max(\\beta - 1, 1 - \\alpha)\n$$\n\nThis algorithm is applied to each of the three test cases.\n\n**Test Case 1 (General Case):**\n$m = 20$, $n = 24$, $G = 6$ blocks of size $4$. A random matrix $A$ is generated from a Gaussian distribution $\\mathcal{N}(0,1)$ with a seed of $0$. After normalization, we iterate through the $\\binom{6}{2} = 15$ pairs of blocks. Each submatrix $A_S$ will have dimension $20 \\times 8$. Since $m > n_S$ where $n_S=8$, the submatrices are tall and likely to be full rank, yielding non-zero minimal singular values. The algorithm proceeds as described above.\n\n**Test Case 2 (Rank-Deficient Edge Case):**\n$m = 3$, $n = 5$, $G = 3$, with block sizes $2$, $2$, and $1$. The block indices are $\\mathcal{B}_1 = \\{1, 2\\}$, $\\mathcal{B}_2 = \\{3, 4\\}$, $\\mathcal{B}_3 = \\{5\\}$. There are $\\binom{3}{2} = 3$ pairs of blocks to consider.\n\\begin{enumerate}\n    \\item $S = \\{1, 2\\}$: $A_S$ is formed from columns indexed by $\\mathcal{B}_1 \\cup \\mathcal{B}_2$, which has size $4$. The matrix $A_S$ is $3 \\times 4$. Since the number of columns ($4$) exceeds the number of rows ($3$), the rank of $A_S$ is at most $3$. Therefore, $A_S$ must have at least $4-3=1$ zero singular value. This means $\\sigma_{\\min}(A_S) = 0$.\n    \\item $S = \\{1, 3\\}$: $A_S$ is formed from columns indexed by $\\mathcal{B}_1 \\cup \\mathcal{B}_3$, which has size $3$. The matrix $A_S$ is $3 \\times 3$.\n    \\item $S = \\{2, 3\\}$: $A_S$ is formed from columns indexed by $\\mathcal{B}_2 \\cup \\mathcal{B}_3$, which has size $3$. The matrix $A_S$ is $3 \\times 3$.\n\\end{enumerate}\nThe presence of the rank-deficient submatrix for $S = \\{1, 2\\}$ guarantees that the global minimum squared singular value is $\\alpha = 0$. Consequently, $1 - \\alpha = 1$. The final value will be $\\widehat{\\delta}_2^B = \\max(\\beta - 1, 1)$, which must be at least $1$.\n\n**Test Case 3 (Ideal Case):**\n$m = 12$, $n = 12$, $G = 3$ blocks of size $4$. The matrix $A$ is the $12 \\times 12$ identity matrix, $I_{12}$. Its columns are already normalized. Any submatrix $A_S$ formed by picking two blocks will consist of $8$ distinct columns from the identity matrix. These columns are, by definition, orthonormal. For any such $12 \\times 8$ submatrix $A_S$, the Gram matrix is $A_S^* A_S = I_8$, the $8 \\times 8$ identity matrix. The eigenvalues of $I_8$ are all equal to $1$. Therefore, for every possible choice of $S$, we have $\\sigma_{\\max}(A_S)^2 = 1$ and $\\sigma_{\\min}(A_S)^2 = 1$. This leads to global values of $\\alpha = 1$ and $\\beta = 1$. The resulting constant is $\\widehat{\\delta}_2^B = \\max(1 - 1, 1 - 1) = 0$, an ideal value indicating no distortion.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef compute_empirical_bric(A: np.ndarray, block_partition: list[list[int]]) - float:\n    \"\"\"\n    Computes the empirical Block Restricted Isometry Constant (BRIC) for s=2.\n\n    Args:\n        A: The sensing matrix (m x n).\n        block_partition: A list of lists, where each inner list contains the\n                         column indices for a block.\n\n    Returns:\n        The empirical BRIC value, delta_hat_2^B.\n    \"\"\"\n    # Step 1: Normalize columns to unit l2-norm.\n    # We add a small epsilon to avoid division by zero for potential zero columns,\n    # though not expected with the given test cases.\n    col_norms = np.linalg.norm(A, axis=0, keepdims=True)\n    epsilon = 1e-12\n    A_norm = A / (col_norms + epsilon)\n\n    num_blocks = len(block_partition)\n    \n    # Step 3: Initialize alpha and beta for extremal value tracking.\n    # Squared singular values are non-negative.\n    # min of min_squared_svd\n    alpha = np.inf\n    # max of max_squared_svd\n    beta = 0.0\n\n    # Step 2: Enumerate all pairs of blocks.\n    block_indices = range(num_blocks)\n    for g1, g2 in combinations(block_indices, 2):\n        # Form the submatrix index set\n        s_indices = block_partition[g1] + block_partition[g2]\n        \n        # Construct the submatrix A_S\n        A_S = A_norm[:, s_indices]\n\n        # Step 3 (cont.): Compute singular values of A_S.\n        # np.linalg.svd returns singular values in descending order.\n        s_values = np.linalg.svd(A_S, compute_uv=False)\n        \n        # Get squared extremal singular values\n        sigma_min_sq = s_values[-1]**2\n        sigma_max_sq = s_values[0]**2\n        \n        # Update alpha and beta\n        if sigma_min_sq  alpha:\n            alpha = sigma_min_sq\n        if sigma_max_sq  beta:\n            beta = sigma_max_sq\n            \n    # Step 4: Final calculation\n    # For a valid BRIC, alpha must be non-negative.\n    # The value 1-alpha represents the deviation from isometry on the lower bound.\n    # The value beta-1 represents the deviation from isometry on the upper bound.\n    delta_hat = max(beta - 1, 1 - alpha)\n    \n    return delta_hat\n\n\ndef solve():\n    \"\"\"\n    Solves the problem for the three specified test cases.\n    \"\"\"\n    results = []\n\n    # Test Case 1: General case\n    m1, n1, G1 = 20, 24, 6\n    seed1 = 0\n    rng1 = np.random.default_rng(seed1)\n    A1 = rng1.standard_normal((m1, n1))\n    block_size1 = n1 // G1\n    partition1 = [list(range(i * block_size1, (i + 1) * block_size1)) for i in range(G1)]\n    result1 = compute_empirical_bric(A1, partition1)\n    results.append(f\"{result1:.6f}\")\n\n    # Test Case 2: Rank-deficient edge case\n    m2, n2, G2 = 3, 5, 3\n    seed2 = 1\n    rng2 = np.random.default_rng(seed2)\n    A2 = rng2.standard_normal((m2, n2))\n    partition2 = [\n        [0, 1],\n        [2, 3],\n        [4]\n    ]\n    result2 = compute_empirical_bric(A2, partition2)\n    results.append(f\"{result2:.6f}\")\n\n    # Test Case 3: Ideal case\n    m3, n3, G3 = 12, 12, 3\n    A3 = np.identity(n3)\n    block_size3 = n3 // G3\n    partition3 = [list(range(i * block_size3, (i + 1) * block_size3)) for i in range(G3)]\n    result3 = compute_empirical_bric(A3, partition3)\n    results.append(f\"{result3:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Beyond general properties like the BRIP, more specific conditions can guarantee exact recovery for particular algorithms like the Group Lasso. The group irrepresentable condition provides a sharp, verifiable criterion for success, ensuring that inactive groups do not wrongly appear more correlated with the residual than the active ones. This practice problem demystifies this condition by guiding you through a direct numerical verification, connecting the abstract theory of model selection consistency to a concrete calculation. ",
            "id": "3455740",
            "problem": "Consider a joint block-sparse recovery setting for the Group Lasso with a measurement matrix $A \\in \\mathbb{R}^{m \\times p}$, where $m = 4$ and $p = 8$. The columns of $A$ are partitioned into $4$ disjoint groups of equal size $2$: $G_1 = \\{1,2\\}$, $G_2 = \\{3,4\\}$, $G_3 = \\{5,6\\}$, and $G_4 = \\{7,8\\}$. The matrix $A$ is given by\n$$\nA \\;=\\; \n\\begin{pmatrix}\n1  0  0.2  0  0  0  -0.25  0 \\\\\n0  1  0  0.05  0  0  0  0.1 \\\\\n0  0  0.1  0  1  0  0.15  -0.2 \\\\\n0  0  0.1  0.3  0  1  0  0.1\n\\end{pmatrix}.\n$$\nAssume the true block-sparse vector $x^\\star \\in \\mathbb{R}^{8}$ has active groups $S = \\{G_1, G_3\\}$ and is specified by\n$$\nx^\\star \\;=\\; \\begin{pmatrix} 3 \\\\ -4 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 6 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nDefine the block-sign operator $\\,\\operatorname{sgn}_G(\\cdot)\\,$ on a group $G$ of size $d$ by $\\,\\operatorname{sgn}_G(x_G) \\,=\\, x_G / \\|x_G\\|_2\\,$ when $\\,\\|x_G\\|_2 \\neq 0\\,$. Let $z \\in \\mathbb{R}^{|S|_d}$ denote the concatenation of $\\,\\operatorname{sgn}_G(x^\\star_G)\\,$ over the active groups in $S$ (here $|S|_d$ is the total number of coordinates in the active groups). Write $A_S$ for the submatrix of $A$ formed by the columns in the active groups $S$, and $A_{S^c}$ for the submatrix of $A$ formed by the columns in the inactive groups $S^c = \\{G_2, G_4\\}$. The group irrepresentable term for Group Lasso is\n$$\nw \\;=\\; A_{S^c}^\\top A_S \\big(A_S^\\top A_S\\big)^{-1} z \\;\\in\\; \\mathbb{R}^{|S^c|_d},\n$$\nwhere $|S^c|_d$ is the total number of coordinates in the inactive groups. The mixed norm $\\|\\cdot\\|_{2,\\infty}$ on a vector $u \\in \\mathbb{R}^{|S^c|_d}$, grouped according to $S^c$, is defined as the maximum over the inactive groups of the Euclidean norm of the corresponding group subvector.\n\nCompute the numerical value of $\\,\\big\\|w\\big\\|_{2,\\infty}\\,$ for the data above. Round your answer to four significant figures. Provide only the value of $\\,\\big\\|w\\big\\|_{2,\\infty}\\,$ as your final answer.",
            "solution": "### Step 1: Extract Givens\n\n*   Measurement matrix $A \\in \\mathbb{R}^{m \\times p}$ with $m = 4$, $p = 8$.\n    $$\n    A \\;=\\;\n    \\begin{pmatrix}\n    1  0  0.2  0  0  0  -0.25  0 \\\\\n    0  1  0  0.05  0  0  0  0.1 \\\\\n    0  0  0.1  0  1  0  0.15  -0.2 \\\\\n    0  0  0.1  0.3  0  1  0  0.1\n    \\end{pmatrix}\n    $$\n*   Column partitions (groups): $G_1 = \\{1,2\\}$, $G_2 = \\{3,4\\}$, $G_3 = \\{5,6\\}$, $G_4 = \\{7,8\\}$. Each group has size $d=2$.\n*   True block-sparse vector $x^\\star \\in \\mathbb{R}^{8}$:\n    $$\n    x^\\star \\;=\\; \\begin{pmatrix} 3 \\\\ -4 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 6 \\\\ 0 \\\\ 0 \\end{pmatrix}\n    $$\n*   Set of active groups: $S = \\{G_1, G_3\\}$.\n*   Block-sign operator: $\\operatorname{sgn}_G(x_G) = x_G / \\|x_G\\|_2$ for $\\|x_G\\|_2 \\neq 0$.\n*   Vector $z \\in \\mathbb{R}^{|S|_d}$ is the concatenation of $\\operatorname{sgn}_G(x^\\star_G)$ for $G \\in S$.\n*   $A_S$ is the submatrix of $A$ with columns from $S$.\n*   $A_{S^c}$ is the submatrix of $A$ with columns from $S^c = \\{G_2, G_4\\}$.\n*   Group irrepresentable term: $w = A_{S^c}^\\top A_S (A_S^\\top A_S)^{-1} z \\in \\mathbb{R}^{|S^c|_d}$.\n*   Mixed norm: $\\|u\\|_{2,\\infty} = \\max_{G \\in S^c} \\|u_G\\|_2$.\n*   Task: Compute $\\|w\\|_{2,\\infty}$ and round to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n\n*   **Scientifically Grounded**: The problem is set in the context of compressed sensing and sparse optimization, specifically analyzing the Group Lasso. The concepts of group sparsity, irrepresentable condition, and mixed norms are standard in this field. The mathematics involved are matrix algebra and vector norms. The problem is scientifically and mathematically sound.\n*   **Well-Posed**: The problem provides all necessary matrices, vectors, and definitions to compute the final quantity. The submatrix $A_S$ needs to be checked to ensure $A_S^\\top A_S$ is invertible. The columns of $A_S$ correspond to indices $\\{1, 2, 5, 6\\}$. Let's inspect $A_S$:\n    $$\n    A_S = \\begin{pmatrix}\n    1  0  0  0 \\\\\n    0  1  0  0 \\\\\n    0  0  1  0 \\\\\n    0  0  0  1\n    \\end{pmatrix}\n    $$\n    This is the $4 \\times 4$ identity matrix. Thus, its columns are linearly independent. $A_S^\\top A_S$ will be the identity matrix, which is clearly invertible. So the problem is well-posed.\n*   **Objective**: The problem is stated in precise mathematical terms, with no subjective or ambiguous language.\n*   **Completeness**: All required data ($A$, $x^\\star$, group definitions) are provided. All formulas are given.\n*   **Consistency**: The dimensions match. $m=4, p=8$. $4$ groups of size $2$. $S$ has 2 groups, so $|S|_d = 2 \\times 2 = 4$. $A_S$ is $4 \\times 4$. $S^c$ has 2 groups, so $|S^c|_d = 2 \\times 2 = 4$. $A_{S^c}$ is $4 \\times 4$. $w$ will be in $\\mathbb{R}^4$. All dimensions are consistent.\n*   **Unrealistic/Infeasible**: The values are just numbers in a matrix. The set up is a standard theoretical model. No physical implausibility.\n*   **Pseudo-profound/Trivial**: The problem is not trivial. It requires several steps of matrix and vector calculations. The fact that $A_S$ is the identity matrix simplifies the calculation of $(A_S^\\top A_S)^{-1}$, but this is a common feature in designed examples to make them tractable. It doesn't make the problem trivial, as the rest of the calculations still need to be done. It's a \"clean\" problem, not a trivial one.\n\nThe problem is valid.\n\n### Step 3: Proceed to Solution\n\n**1. Identify Active and Inactive Submatrices**\nThe active groups are $S = \\{G_1, G_3\\}$, which correspond to columns $\\{1, 2\\}$ and $\\{5, 6\\}$.\nThe inactive groups are $S^c = \\{G_2, G_4\\}$, which correspond to columns $\\{3, 4\\}$ and $\\{7, 8\\}$.\n\nThe submatrix $A_S$ consists of columns $1, 2, 5, 6$ of $A$:\n$$\nA_S =\n\\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1\n\\end{pmatrix} = I_4\n$$\nThis is the $4 \\times 4$ identity matrix.\n\nThe submatrix $A_{S^c}$ consists of columns $3, 4, 7, 8$ of $A$:\n$$\nA_{S^c} =\n\\begin{pmatrix}\n0.2  0  -0.25  0 \\\\\n0  0.05  0  0.1 \\\\\n0.1  0  0.15  -0.2 \\\\\n0.1  0.3  0  0.1\n\\end{pmatrix}\n$$\n\n**2. Calculate the block-sign vector `z`**\nThe true vector is $x^\\star = (3, -4, 0, 0, 0, 6, 0, 0)^\\top.\nThe active groups are $G_1$ and $G_3$.\nThe subvector for group $G_1$ is $x^\\star_{G_1} = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}$.\nThe Euclidean norm is $\\|x^\\star_{G_1}\\|_2 = \\sqrt{3^2 + (-4)^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$.\nThe sign vector for $G_1$ is $\\operatorname{sgn}_{G_1}(x^\\star_{G_1}) = \\frac{1}{5} \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ -0.8 \\end{pmatrix}$.\n\nThe subvector for group $G_3$ is $x^\\star_{G_3}$. From $x^\\star$, the 5th and 6th components are $(0, 6)$.\nSo $x^\\star_{G_3} = \\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix}$.\nThe Euclidean norm is $\\|x^\\star_{G_3}\\|_2 = \\sqrt{0^2 + 6^2} = 6$.\nThe sign vector for $G_3$ is $\\operatorname{sgn}_{G_3}(x^\\star_{G_3}) = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe problem states that $z$ is the concatenation of $\\operatorname{sgn}_G(x_G^\\star)$ over active groups in $S$. We assume the conventional ordering $G_1, G_3$.\n$$\nz = \\begin{pmatrix} \\operatorname{sgn}_{G_1}(x^\\star_{G_1}) \\\\ \\operatorname{sgn}_{G_3}(x^\\star_{G_3}) \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ -0.8 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n$z$ is a vector in $\\mathbb{R}^{|S|_d} = \\mathbb{R}^4$.\n\n**3. Calculate the irrepresentable term `w`**\nThe formula is $w = A_{S^c}^\\top A_S (A_S^\\top A_S)^{-1} z$.\nWe first compute the term $(A_S^\\top A_S)^{-1}$. Since $A_S = I_4$, the $4 \\times 4$ identity matrix:\n$A_S^\\top A_S = I_4^\\top I_4 = I_4$.\nThe inverse is $(A_S^\\top A_S)^{-1} = I_4^{-1} = I_4$.\n\nThe formula for $w$ simplifies to:\n$w = A_{S^c}^\\top A_S (I_4) z = A_{S^c}^\\top I_4 z = A_{S^c}^\\top z$.\n\nNow we compute this matrix-vector product.\nThe transpose of $A_{S^c}$ is:\n$$\nA_{S^c}^\\top =\n\\begin{pmatrix}\n0.2  0  0.1  0.1 \\\\\n0  0.05  0  0.3 \\\\\n-0.25  0  0.15  0 \\\\\n0  0.1  -0.2  0.1\n\\end{pmatrix}\n$$\nWe multiply $A_{S^c}^\\top$ by $z = (0.6, -0.8, 0, 1)^\\top$:\n$$\nw = \\begin{pmatrix}\n0.2  0  0.1  0.1 \\\\\n0  0.05  0  0.3 \\\\\n-0.25  0  0.15  0 \\\\\n0  0.1  -0.2  0.1\n\\end{pmatrix}\n\\begin{pmatrix}\n0.6 \\\\\n-0.8 \\\\\n0 \\\\\n1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n(0.2)(0.6) + (0)(-0.8) + (0.1)(0) + (0.1)(1) \\\\\n(0)(0.6) + (0.05)(-0.8) + (0)(0) + (0.3)(1) \\\\\n(-0.25)(0.6) + (0)(-0.8) + (0.15)(0) + (0)(1) \\\\\n(0)(0.6) + (0.1)(-0.8) + (-0.2)(0) + (0.1)(1)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.12 + 0.1 \\\\\n-0.04 + 0.3 \\\\\n-0.15 \\\\\n-0.08 + 0.1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.22 \\\\\n0.26 \\\\\n-0.15 \\\\\n0.02\n\\end{pmatrix}\n$$\nSo, $w = (0.22, 0.26, -0.15, 0.02)^\\top$.\n\n**4. Compute the mixed norm `||w||_{2,infinity}`**\nThe vector $w \\in \\mathbb{R}^{|S^c|_d} = \\mathbb{R}^4$. Its components are indexed by the columns in $S^c$, which belong to the inactive groups $G_2 = \\{3,4\\}$ and $G_4 = \\{7,8\\}$.\nWe partition the vector $w$ according to these groups:\nThe subvector corresponding to group $G_2$ is $w_{G_2} = \\begin{pmatrix} 0.22 \\\\ 0.26 \\end{pmatrix}$.\nThe subvector corresponding to group $G_4$ is $w_{G_4} = \\begin{pmatrix} -0.15 \\\\ 0.02 \\end{pmatrix}$.\n\nThe mixed norm is defined as $\\|w\\|_{2,\\infty} = \\max_{G \\in S^c} \\|w_G\\|_2$. We compute the Euclidean norm of each subvector.\n\nFor group $G_2$:\n$$\n\\|w_{G_2}\\|_2 = \\sqrt{(0.22)^2 + (0.26)^2} = \\sqrt{0.0484 + 0.0676} = \\sqrt{0.116}\n$$\n\nFor group $G_4$:\n$$\n\\|w_{G_4}\\|_2 = \\sqrt{(-0.15)^2 + (0.02)^2} = \\sqrt{0.0225 + 0.0004} = \\sqrt{0.0229}\n$$\n\nThe mixed norm $\\|w\\|_{2,\\infty}$ is the maximum of these two values:\n$$\n\\|w\\|_{2,\\infty} = \\max\\left(\\sqrt{0.116}, \\sqrt{0.0229}\\right)\n$$\nSince $0.116 > 0.0229$, the maximum is $\\sqrt{0.116}$.\n\n**5. Final Calculation and Rounding**\nThe problem requires the numerical value of $\\|w\\|_{2,\\infty}$ rounded to four significant figures.\n$$\n\\|w\\|_{2,\\infty} = \\sqrt{0.116} \\approx 0.3405877...\n$$\nRounding this value to four significant figures gives $0.3406$.",
            "answer": "$$\n\\boxed{0.3406}\n$$"
        }
    ]
}