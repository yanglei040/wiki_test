## 引言
在经典的压缩感知理论中，信号的[稀疏性](@entry_id:136793)通常被假设为非零元素数量少，但其位置是随机的。然而，在众多科学与工程问题中，从基因表达数据到[阵列信号处理](@entry_id:197159)，稀疏模式本身往往呈现出高度的结构性——非零系数倾向于成块出现或在多个相关测量中共享相同的支撑位置。如何有效利用这种先验结构信息，以更少的测量实现更精确的[信号恢复](@entry_id:195705)，是[稀疏优化](@entry_id:166698)领域一个核心且前沿的挑战。

本文旨在系统性地解决这一挑战，深入探讨**联合与块[稀疏恢复算法](@entry_id:189308)**的理论基础、实现方法及其在多学科中的应用。读者将通过本文踏上一段从理论到实践的完整学习之旅。在第一章**“原理与机制”**中，我们将建立结构化稀疏的数学模型，阐明其[恢复保证](@entry_id:754159)的几何与代数条件，并剖析核心算法的内在逻辑。接着，在第二章**“应用与跨学科连接”**中，我们将展示这些理论如何在[波达方向估计](@entry_id:189084)、动态系统和贝叶斯推断等实际场景中发挥作用，揭示其强大的问题解决能力。最后，通过第三章**“动手实践”**中的编程与理论练习，你将有机会亲手实现和验证这些关键概念，将抽象知识转化为具体的技能。

本文将为你构建一个关于联合与[块稀疏恢复](@entry_id:746892)的坚实知识框架，无论你是致力于算法研究的学者，还是希望在自己的领域应用这些先进工具的工程师或科学家，都将从中获益。让我们从结构化稀疏的基本原理开始。

## 原理与机制

本章深入探讨联合与[块稀疏恢复](@entry_id:746892)的核心原理和机制。我们将从结构化稀疏的基本模型出发，建立确保信号精确恢复的理论保证，并详细阐述实现这些恢复的实用算法。最后，我们将揭示[多测量向量](@entry_id:752318)（MMV）模型相对于单测量向量（SMV）模型在提高恢[复性](@entry_id:162752)能方面的深刻原因。

### 定义结构化稀疏：从块到联合支撑

传统的稀疏性假设信号向量中只有少数几个非零元素。然而，在许多实际应用中，非零系数并非随机[分布](@entry_id:182848)，而是呈现出特定的结构或模式。**块[稀疏性](@entry_id:136793) (Block sparsity)** 是其中最基本和最普遍的结构之一。

在一个块[稀疏模型](@entry_id:755136)中，信号向量 $x \in \mathbb{R}^n$ 的索引 $\{1, \dots, n\}$ 被划分为一组不相交的**块 (blocks)** 或**组 (groups)**，记为 $\{G_j\}_{j=1}^J$。一个向量被称为块稀疏，是指其非零元素仅集中在少数几个块中。衡量这种[结构化稀疏性](@entry_id:636211)的理想度量是**块 $\ell_0$ “范数”**，定义为非零块的数量：
$$ \|x\|_{2,0} = \sum_{j=1}^J \mathbf{1}\{\|x_{G_j}\|_2 > 0\} $$
其中 $x_{G_j}$ 表示向量 $x$ 在块 $G_j$ 上的子向量，$\mathbf{1}\{\cdot\}$ 是指示函数。这个度量是非凸的，直接最小化它是一个组合优化难题。因此，在实践中，我们通常使用其最紧密的凸近似，即**[组套索](@entry_id:170889) (group Lasso)** [罚函数](@entry_id:638029)，它是一种混合范数：
$$ \|x\|_{2,1} = \sum_{j=1}^J \|x_{G_j}\|_2 $$
这个范数首先在每个块内部计算[欧几里得范数](@entry_id:172687) ($\ell_2$ 范数)，然后在不同块之间求和 ($\ell_1$ 范数)，因此被称为 $\ell_{2,1}$ 范数。它能有效地促进整个块的系数同时为零，从而实现块稀疏性。

**[联合稀疏性](@entry_id:750955) (Joint sparsity)** 是块稀疏性在**[多测量向量](@entry_id:752318) (Multiple Measurement Vector, MMV)** 模型中的一种特殊体现。在MMV模型中，我们试图从一组测量值 $Y \in \mathbb{R}^{m \times L}$ 中恢复一组信号 $X \in \mathbb{R}^{n \times L}$。这些信号由同一个传感矩阵 $A \in \mathbb{R}^{m \times n}$ 产生，即 $Y = AX$。联合稀疏的核心假设是，信号矩阵 $X$ 的所有列向量 $x^{(\ell)}$ 共享一个共同的稀疏支撑。换句话说，存在一个小的索引集合 $S \subset \{1, \dots, n\}$，使得对于所有的 $\ell=1, \dots, L$，向量 $x^{(\ell)}$ 的非零元素都位于 $S$ 内。这意味着 $X$ 的非零行恰好是索引在 $S$ 中的那些行。

从块稀疏的角度看，联合[稀疏模型](@entry_id:755136)可以将 $X$ 的每一行视为一个块。这样，模型就变成了寻找一个行稀疏的矩阵 $X$。用于促进这种结构的凸罚函数正是前面提到的混合范数，只是应用在矩阵 $X$ 的行上：
$$ \|X\|_{2,1} = \sum_{i=1}^n \|X_{i,:}\|_2 $$
其中 $X_{i,:}$ 是矩阵 $X$ 的第 $i$ 行。这个罚函数会促使 $X$ 的许多行向量完全变为[零向量](@entry_id:156189)，从而实现联合稀疏。与之相对，标准的块[稀疏模型](@entry_id:755136)通常处理单个向量，并根据预定义的、与问题结构相关的分组来耦合坐标，而不是跨多个测量向量共享支撑 。

这些模型还可以推广到**重叠组稀疏 (overlapping group sparsity)** 的情况，即索引块 $\{G_j\}$ 之间可以有重叠。此时，罚函数形式上保持为 $\Omega(x) = \sum_{j=1}^J \|x_{G_j}\|_2$。为了便于优化，这种非分离的[罚函数](@entry_id:638029)可以通过**变量分裂 (variable splitting)** 的技术来处理。我们可以为每个组引入一个辅助变量 $u^{(j)} \in \mathbb{R}^{|G_j|}$，并施加一致性约束 $u^{(j)} = R_{G_j}x$，其中 $R_{G_j}$ 是将 $x$ 限制到组 $G_j$ 的操作符。这样，原问题可以等价地写成一个约束优化问题，其[目标函数](@entry_id:267263)变为 $\sum_{j=1}^J \|u^{(j)}\|_2$，这是一个可分离的凸函数，非常适合像[交替方向乘子法](@entry_id:163024) ([ADMM](@entry_id:163024)) 这样的分裂算法 。

另一种与重叠组稀疏相关的[罚函数](@entry_id:638029)是基于**[隐变量](@entry_id:150146)分解 (latent variable decomposition)** 的范数，定义为：
$$ \Omega_{\mathrm{lat}}(x) \triangleq \inf\left\{\sum_{j=1}^{J} \|v^{(j)}\|_{2} \,:\, x = \sum_{j=1}^{J} v^{(j)}, \quad \mathrm{supp}(v^{(j)}) \subseteq G_{j} \quad \forall j\right\} $$
这个范数将向量 $x$ 分解为多个分量 $v^{(j)}$ 的和，每个分量只在对应的组 $G_j$ 上有支撑。可以证明，这个隐式范数总是小于或等于直接的重叠组范数 $\Omega(x)$，即 $\Omega_{\mathrm{lat}}(x) \le \Omega(x)$。当且仅当所有组都不重叠时，两者才相等 。

### 联合[稀疏信号](@entry_id:755125)的几何结构

为了更深入地理解联合[稀疏模型](@entry_id:755136)，我们可以从几何角度来描述其结构。所有具有联合 $k$-[稀疏性](@entry_id:136793)的信号矩阵 $X \in \mathbb{R}^{n \times L}$ (即非零行的数量不超过 $k$) 构成了一个复杂的非凸集合。这个集合可以被精确地描述为一个**[子空间](@entry_id:150286)的并集 (union of subspaces)**。

对于任意一个固定的行支撑集 $S \subseteq \{1, \dots, n\}$，所有行支撑包含于 $S$ 的矩阵集合
$$ \mathcal{V}(S) = \{ X \in \mathbb{R}^{n \times L} : \mathrm{rowsupp}(X) \subseteq S \} $$
构成一个[线性子空间](@entry_id:151815)。这是因为任何两个行支撑在 $S$ 内的矩阵的[线性组合](@entry_id:154743)，其行支撑仍然在 $S$ 内。这个[子空间](@entry_id:150286)的**维度 (dimension)** 是多少呢？一个属于 $\mathcal{V}(S)$ 的矩阵 $X$ 的唯一约束是，所有索引不在 $S$ 内的行必须为零。而索引在 $S$ 内的行，其所有 $L$ 个元素都可以自由取值。因此，可以自由选择的参数总数等于行数 $|S|$ 乘以列数 $L$。所以，$\mathrm{dim}(\mathcal{V}(S)) = |S|L$ 。

所有联合 $k$-[稀疏矩阵](@entry_id:138197)的集合 $\mathcal{U}_k$ 就可以表示为所有这些[子空间](@entry_id:150286)的并集，其中支撑集 $S$ 的大小不超过 $k$：
$$ \mathcal{U}_k = \bigcup_{S \subseteq \{1,\dots,n\},\, |S| \le k} \mathcal{V}(S) $$
这种几何视角揭示了联合稀疏信号的内在结构：它们并非[分布](@entry_id:182848)在单一的低维[子空间](@entry_id:150286)中，而是居住在一个由许多低维[子空间](@entry_id:150286)拼接而成的复杂结构上。

### 精确与稳定恢复的保证

一个核心问题是：在什么条件下，我们能从测量值 $y$ (或 $Y$) 中唯一且稳定地恢复出块稀疏或联合稀疏的信号 $x$ (或 $X$)？对于基于[凸优化](@entry_id:137441)的方法，这等价于问：在什么条件下，最小化凸[罚函数](@entry_id:638029)（如 $\|x\|_{2,1}$）的解与最小化非凸的理想度量（如 $\|x\|_{2,0}$）的解是一致的？

#### 组[零空间性质](@entry_id:752758)

**组[零空间性质](@entry_id:752758) (Group Null Space Property, NSP)** 为上述问题提供了直接的答案。对于一个传感矩阵 $A$，如果对于其[零空间](@entry_id:171336) $\mathrm{Null}(A)$ 中的任何非[零向量](@entry_id:156189) $h$，以及任意[基数](@entry_id:754020)不超过 $s$ 的组索引集 $T$，都满足不等式：
$$ \sum_{j \in T} \|h_{G_j}\|_2  \sum_{j \notin T} \|h_{G_j}\|_2 $$
那么我们说矩阵 $A$ 满足 $s$ 阶的组 NSP。可以证明，组 NSP 是确保对于任何具有至多 $s$ 个非零块的真实信号 $x^\star$，它都是 $Ax=Ax^\star$ 约束下 $\|x\|_{2,1}$ 范数的唯一最小解的**充要条件**。如果该性质不成立，我们总可以构造一个反例，使得 $\|x\|_{2,1}$ 最小化失败，即找到一个更不稀疏但 $\|x\|_{2,1}$ 范数更小或相等的解，这导致了所谓的**松弛间隙 (relaxation gap)** 。

#### 块[限制等距性质](@entry_id:184548)

虽然组 NSP 是一个根本性的条件，但它依赖于[矩阵的零空间](@entry_id:152429)，难以直接验证。**块[限制等距性质](@entry_id:184548) (Block Restricted Isometry Property, Block-RIP)** 或组 RIP (GRIP) 提供了一个更实用、可验证的充分条件。矩阵 $A$ 的块 $k$-阶[限制等距常数](@entry_id:754314) $\delta_k^B$ 被定义为满足以下不等式的最小 $\delta \ge 0$：
$$ (1-\delta)\|x\|_2^2 \le \|Ax\|_2^2 \le (1+\delta)\|x\|_2^2 $$
该不等式对所有块 $k$-稀疏的向量 $x$ 成立。这个定义是标准 RIP 的直接推广 。

Block-RIP 与标准 RIP 之间有几个重要关系：
1.  **退化情况**: 如果每个块的大小都为 1，那么块 $k$-稀疏就等同于标准 $k$-稀疏，此时 $\delta_k^B = \delta_k$ 。
2.  **包含关系**: 如果所有块的大小都为 $r$，那么任何一个块 $k$-稀疏的向量，其总稀疏度（非零元素个数）不会超过 $kr$。这意味着块 $k$-稀疏向量的集合是标准 $kr$-稀疏向量集合的[子集](@entry_id:261956)。因此，$\delta_k^B \le \delta_{kr}$ 。
3.  **可逆性**: 如果 $\delta_k^B  1$，那么对于任何由至多 $k$ 个块的列构成的子矩阵 $A_T$，它都是列满秩的。这意味着 $A$ 在所有块 $k$-稀疏向量上是[单射](@entry_id:183792)的，保证了不同[稀疏信号](@entry_id:755125)不会产生相同的测量结果 。

Block-RIP 的一个关键作用是，一个足够小的 $\delta_{2s}^B$ 常数（例如 $\delta_{2s}^B  1$）可以保证 $s$ 阶组 NSP 的成立。因此，Block-RIP 为[凸松弛](@entry_id:636024)方法的成功提供了一个关于传感矩阵 $A$ 本身的、可验证的充分条件。如果 $\delta_{2s}^B$ 足够小，那么对于任何 $s$-块[稀疏信号](@entry_id:755125)，它都是对应 $\ell_{2,1}$ 最小化问题的唯一解 。

#### 块不可表示条件

在带噪的统计设定下 ($y = Ax^\star + w$)，我们关心的是 group Lasso 估计量在何种条件下能够实现**[模型选择一致性](@entry_id:752084) (model selection consistency)**，即以趋于 1 的概率正确地识别出真实的非零块集合 $S$。**块不可表示条件 (Block Irrepresentable Condition, BIC)** 是一个关键的充分条件。对于一个给定的真实信号 $x^\star$，该条件要求：
$$ \left\| A_{S^{c}}^{\top} A_{S} (A_{S}^{\top} A_{S})^{-1} z_{S} \right\|_{2,\infty}  1 $$
其中 $A_S$ 和 $A_{S^c}$ 分别是 $A$ 中对应于活动块集 $S$ 和非活动块集 $S^c$ 的列构成的子矩阵，$z_S$ 是一个块向量，其分量 $z_g$ 是真实信号块的归一化方向 $x_g^\star/\|x_g^\star\|_2$。$\|\cdot\|_{2,\infty}$ 是一个混合范数，定义为 $\max_g \|v_g\|_2$。这个条件直观上限制了活动块与非活动块之间的“混淆”程度。

如果 BIC 成立，并且[正则化参数](@entry_id:162917) $\lambda$ 的选择与噪声水平和问题维度适配（例如，对于[高斯噪声](@entry_id:260752)，$\lambda \sim \sigma \sqrt{(\log G)/n}$），同时真实信号的能量足够大（即满足最小信号条件 $\min_{g \in S} \|x_g^\star\|_2 \gtrsim \lambda$），那么 group Lasso 就能实现[模型选择一致性](@entry_id:752084) 。当所有块大小为 1 时，BIC 精确地退化为经典 Lasso 的不可表示条件 。反之，在无噪声情况下，如果 group Lasso 能够成功恢复支撑集，那么一个非严格版本的 BIC（不等式为 $\le 1$）也必须成立，表明该条件是必要的 。

### 结构化[稀疏恢复](@entry_id:199430)的算法

有了理论保证后，我们转向求解块稀疏和[联合稀疏恢复](@entry_id:750954)问题的实用算法。主要有两大类：[贪心算法](@entry_id:260925)和凸[优化算法](@entry_id:147840)。

#### 贪心算法：[块正交匹配追踪](@entry_id:746870)

**[块正交匹配追踪](@entry_id:746870) (Block Orthogonal Matching Pursuit, Block OMP)** 是标准 OMP 算法的扩展，用于恢复块[稀疏信号](@entry_id:755125)。该算法迭代地进行：
1.  **块选择**: 在每一步，选择与当前残差 $r_t$ 相关性最强的块。这个选择准则是最大化块相关向量的 $\ell_2$ 范数：
    $$ j^\star = \arg\max_{j} \|A_{G_j}^\top r_t\|_2 $$
    这个准则综合了块内所有列与残差的关联度，而不是像标准 OMP 那样简单地选择单个最相关的列。例如，一个拥有多个中等相关列的块，其 $\|A_{G_j}^\top r_t\|_2$ 可能大于另一个仅含一个高度相关列的块 。
2.  **支撑更新**: 将选中的块加入到活动块集合中。
3.  **信号估计**: 通过求解一个[最小二乘问题](@entry_id:164198)，将测量信号 $y$ **正交投影**到当前所有已选块的列所张成的[子空间](@entry_id:150286)上，从而得到当前对信号的估计。
4.  **残差更新**: 用 $y$ 减去投影结果，得到新的残差，该残差与已选[子空间](@entry_id:150286)正交。

在 MMV 设定下，Block OMP 的一个直接应用是**同步[正交匹配追踪](@entry_id:202036) (Simultaneous OMP, SOMP)**。SOMP 在选择步骤中，聚合所有 $L$ 个测量向量的残差信息来共同决定下一个要加入支撑集的索引，这正是利用了联合稀疏的结构 。

#### 凸优化算法与邻近算子

凸[优化方法](@entry_id:164468)旨在求解如 group Lasso 这样的问题：
$$ \min_{x \in \mathbb{R}^{n}} \frac{1}{2}\|Ax - y\|_{2}^{2} + \lambda \|x\|_{2,1} $$
许多高效的一阶算法，如[迭代收缩阈值算法](@entry_id:750898) (ISTA) 及其加速版 (FISTA)，以及 ADMM，其核心都依赖于[罚函数](@entry_id:638029)项的**邻近算子 (proximal operator)**。

要理解邻近算子，首先需要分析 $\ell_{2,1}$ 范数的**[次微分](@entry_id:175641) (subdifferential)**。$\|x\|_{2,1}$ 的[次微分](@entry_id:175641) $\partial \|x\|_{2,1}$ 具有清晰的块分离结构。对于一个向量 $g \in \partial \|x\|_{2,1}$，其对应于块 $G_j$ 的子向量 $g_{G_j}$ 必须满足：
$$
g_{G_j} = \begin{cases}
\frac{x_{G_j}}{\|x_{G_j}\|_{2}}  \text{如果 } x_{G_j} \neq 0 \quad (\text{活动块}) \\
\text{满足 } \|g_{G_j}\|_{2} \le 1  \text{如果 } x_{G_j} = 0 \quad (\text{非活动块})
\end{cases}
$$
这个结构是推导 group Lasso 问题[最优性条件](@entry_id:634091)（KKT 条件）的基础。最优解 $x^*$ 必须满足 $-A^T(Ax^*-y) \in \lambda \partial \|x^*\|_{2,1}$。这意味着，对于每个非活动块 $G_j$，其与残差的“集体相关性”必须小于一个阈值，即 $\|A_j^T(y-Ax^*)\|_2 \le \lambda$；而对于每个活动块，这个相关性必须恰好达到阈值 $\lambda$ 。

[罚函数](@entry_id:638029) $\lambda \|x\|_{2,1}$ 的邻近算子 $\mathrm{prox}_{\lambda\|\cdot\|_{2,1}}(v)$ 被定义为以下[优化问题](@entry_id:266749)的解：
$$ \mathrm{prox}_{\lambda\|\cdot\|_{2,1}}(v) = \arg\min_{x} \left\{ \lambda\|x\|_{2,1} + \frac{1}{2}\|x-v\|_2^2 \right\} $$
由于目标函数在块上是可分离的，这个算子可以分解为对每个块独立的**[块软阈值](@entry_id:746891) (block soft-thresholding)** 操作：
$$ (\mathrm{prox}_{\lambda\|\cdot\|_{2,1}}(v))_{G_j} = \mathrm{prox}_{\lambda\|\cdot\|_2}(v_{G_j}) = \left(1 - \frac{\lambda}{\|v_{G_j}\|_2}\right)_+ v_{G_j} $$
其中 $(\cdot)_+ = \max(0, \cdot)$。这个操作的含义是：如果一个块向量 $v_{G_j}$ 的 $\ell_2$ 范数小于或等于 $\lambda$，则整个块被置为零；否则，该块向量被整体地向原点收缩一个因子。

例如，考虑一个 $\mathbb{R}^7$ 中的向量 $v = (3, 4, 5, 12, 0, 1, 2)^T$，块划分为 $G_1=\{1,2\}, G_2=\{3,4,5\}, G_3=\{6,7\}$，且 $\lambda=4$。
- 对于 $G_1$，$v_{G_1}=(3,4)^T$，其范数为 $\|v_{G_1}\|_2=5 > 4$。收缩后的块为 $(1-4/5)(3,4)^T = (3/5, 4/5)^T$。
- 对于 $G_2$，$v_{G_2}=(5,12,0)^T$，其范数为 $\|v_{G_2}\|_2=13 > 4$。收缩后的块为 $(1-4/13)(5,12,0)^T = (45/13, 108/13, 0)^T$。
- 对于 $G_3$，$v_{G_3}=(1,2)^T$，其范数为 $\|v_{G_3}\|_2=\sqrt{5}  4$。因此，该块被置为零。
最终得到的向量为 $(3/5, 4/5, 45/13, 108/13, 0, 0, 0)^T$ 。
这个块[软阈值算子](@entry_id:755010)是构建求解 group Lasso 问题一阶算法的关键构件。

### 多次测量的力量：更深层次的审视

MMV 模型的一个显著优势是，随着测量向量数量 $L$ 的增加，成功恢复联合支撑所需的测量数 $m$ 会显著降低。这一现象的背后是深刻的统计学和随机矩阵理论原理。

考虑一种基于[子空间](@entry_id:150286)的 MMV 恢复方法。该方法首先构建经验协方差矩阵 $\widehat{C} = \frac{1}{L}YY^T$。在无噪声情况下，$Y=A_S X_{S,:}$，因此 $\widehat{C} = A_S (\frac{1}{L}X_{S,:}X_{S,:}^T) A_S^T$。只要 $m \ge k$ 且 $L \ge k$，该[矩阵的秩](@entry_id:155507)为 $k$，其列空间精确地等于[信号子空间](@entry_id:185227) $\mathrm{span}(A_S)$。

在有噪声的情况下，$Y=AX+W=A_S X_{S,:} + W$。根据大数定律，当 $L$ 很大时，经验[协方差矩阵](@entry_id:139155) $\widehat{C}$ 会向其总体期望 $C_\star$ **集中 (concentrate)**：
$$ \widehat{C} \to C_\star = A_S \Sigma_S A_S^T + \sigma^2 I_m $$
其中 $\Sigma_S$ 是信号行向量的[协方差矩阵](@entry_id:139155)，$\sigma^2$ 是噪声[方差](@entry_id:200758)。$C_\star$ 的谱结构非常清晰：它有 $k$ 个与[信号相关](@entry_id:274796)的、大于 $\sigma^2$ 的[特征值](@entry_id:154894)，以及 $m-k$ 个等于 $\sigma^2$ 的、与噪声相关的[特征值](@entry_id:154894)。这就在信号[特征值](@entry_id:154894)和噪声[特征值](@entry_id:154894)之间形成了一个**特征间隙 (eigen-gap)**。

对于有限的 $L$，$\widehat{C}$ 是 $C_\star$ 的随机扰动。根据随机矩阵理论中的 **Marchenko-Pastur 定理**，噪声部分的协方差矩阵 $\frac{1}{L}WW^T$ 的谱会[分布](@entry_id:182848)在一个区间内，其上界近似为 $\sigma^2(1+\sqrt{m/L})^2$。当 $L$ 增加时，$m/L$ 减小，这个谱区间会向 $\sigma^2$ 这个单点收缩。这意味着，随着 $L$ 的增长，$\widehat{C}$ 中噪声部分的[特征值](@entry_id:154894)会更紧密地聚集在 $\sigma^2$ 附近。

这种谱收缩效应极大地增强了信号与噪声[特征值](@entry_id:154894)之间的分离。根据 **Davis-Kahan 定理**，估计[信号子空间](@entry_id:185227)（由 $\widehat{C}$ 的前 $k$ 个[特征向量](@entry_id:151813)张成）的误差与特征间隙的大小成反比。更大的 $L$ 导致更清晰的特征间隙，从而得到对真实[信号子空间](@entry_id:185227) $\mathrm{span}(A_S)$ 更精确的估计。这使得通过投影测试来区分支撑内外的列变得更加可靠，即使在测量数 $m$ 较小的情况下也是如此。

相比之下，在 SMV 模型 ($L=1$) 中，不存在这种通过平均来“去噪”和增强谱结构的机会。恢复完全依赖于传感矩阵 $A$ 的全局性质，如 RIP，这通常要求 $m$ 的规模达到 $\mathcal{O}(k \log(n/k))$。MMV 模型则打破了这一限制，允许用更多的测量向量 $L$ 来换取更少的测量数 $m$，从而在恢复相图中实现了更有利的权衡 。