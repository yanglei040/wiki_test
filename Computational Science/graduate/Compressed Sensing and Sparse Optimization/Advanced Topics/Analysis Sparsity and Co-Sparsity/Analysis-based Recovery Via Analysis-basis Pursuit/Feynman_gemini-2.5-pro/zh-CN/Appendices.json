{
    "hands_on_practices": [
        {
            "introduction": "分析零空间性质（Analysis Null Space Property, NSP）是确保分析基追踪能够成功恢复信号的核心理论条件。这个练习将抽象的NSP条件具体化，通过一个简单的实例，让你亲手验证对于一个给定的传感矩阵$A$和支撑集$T$，其零空间中的向量是否满足该性质，从而加深对恢复保证理论的理解。",
            "id": "3431451",
            "problem": "考虑一个压缩感知的分析模型，其中分析算子 $\\Omega \\in \\mathbb{R}^{p \\times n}$ 作用于信号 $x \\in \\mathbb{R}^{n}$，产生分析系数 $\\Omega x \\in \\mathbb{R}^{p}$。分析基追踪 (ABP) 程序是一个凸优化问题，旨在寻找一个与测量值一致的解 $\\widehat{x}$，同时最小化分析系数的 $\\ell_{1}$ 范数，即在满足 $A x = y$ 的条件下最小化 $\\|\\Omega x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是传感矩阵，$y \\in \\mathbb{R}^{m}$ 是测量向量。分析零空间性质 (Analysis NSP) 提供了一个核心的理论保证，它非正式地指出，如果对于与真实分析系数相关联的给定支撑集 $T \\subset \\{1,2,\\dots,p\\}$，$\\ker(A)$ 中的每个非零向量 $h$ 都满足一个严格不等式，该不等式比较了 $\\Omega h$ 在 $T$ 内部和外部的分量的 $\\ell_{1}$ 范数，那么通过 ABP 进行精确恢复是可能的。\n\n从基本定义开始：\n- 矩阵 $A$ 的零空间（核），记作 $\\ker(A)$，是满足 $A h = 0$ 的向量 $h$ 的集合。\n- 向量 $z \\in \\mathbb{R}^{p}$ 的 $\\ell_{1}$ 范数是 $\\|z\\|_{1} = \\sum_{i=1}^{p} |z_{i}|$。\n- 向量 $z \\in \\mathbb{R}^{p}$ 的支撑集是其非零元素对应的索引集合。\n\n关于支撑集 $T$ 的分析零空间性质 (NSP) 由不等式\n$$\n\\|(\\Omega h)_{T}\\|_{1}  \\|(\\Omega h)_{T^{c}}\\|_{1}\n$$\n定义，适用于所有 $h \\in \\ker(A) \\setminus \\{0\\}$，其中 $(\\Omega h)_{T}$ 表示将 $\\Omega h$ 限制在 $T$ 中的索引所形成的向量，$T^{c}$ 是 $T$ 在 $\\{1,2,\\dots,p\\}$ 中的补集。\n\n给定一个维度为 $n = 4$，$p = 5$ 的示例实例，以及以下分析算子和传感矩阵：\n\n- 分析算子 $\\Omega \\in \\mathbb{R}^{5 \\times 4}$，其行向量为\n$$\n\\Omega =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  \\phantom{-}1  -1  0 \\\\\n0  0  \\phantom{-}1  -1 \\\\\n1  0  0  0 \\\\\n0  0  0  1\n\\end{bmatrix}.\n$$\n\n- 传感矩阵 $A \\in \\mathbb{R}^{3 \\times 4}$ 由其列向量 $a_{1}, a_{2}, a_{3}, a_{4} \\in \\mathbb{R}^{3}$ 指定，\n$$\na_{1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\na_{2} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\na_{3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\na_{4} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\n$$\n因此\n$$\nA = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  1  0 \\\\\n0  0  1  1\n\\end{bmatrix}.\n$$\n\n由于矩阵 $A$ 的秩为 $3$，其零空间 $\\ker(A)$ 是一维的，并且等于 $\\operatorname{span}\\{h_{0}\\}$，其中 $h_{0} \\in \\mathbb{R}^{4}$ 是一个非零向量。在这种情况下，$\\ker(A)$ 中的每个单位范数向量 $h$ 要么是 $h = \\frac{h_{0}}{\\|h_{0}\\|_{2}}$，要么是 $h = -\\frac{h_{0}}{\\|h_{0}\\|_{2}}$，因此对 $\\ker(A)$ 中所有单位范数向量的分析 NSP 验证就简化为检查这两种情况。\n\n你的任务是编写一个程序，对于每个给定的真实信号 $x^{(i)} \\in \\mathbb{R}^{4}$，构造其分析系数 $\\Omega x^{(i)}$ 的支撑集 $T^{(i)} \\subset \\{1,2,3,4,5\\}$，然后对所有单位范数向量 $h \\in \\ker(A)$ 验证分析 NSP 不等式\n$$\n\\|(\\Omega h)_{T^{(i)}}\\|_{1}  \\|(\\Omega h)_{(T^{(i)})^{c}}\\|_{1}\n$$\n。程序应返回一个布尔值，指示该性质是否对 $h$ 和 $-h$ 都成立。\n\n使用以下真实信号测试套件：\n- 情况1（正常路径）：$x^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n- 情况2（边界条件，不等式变为等式，因此不成立）：$x^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 情况3（具有大支撑集的边缘情况）：$x^{(3)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$。\n\n程序必须：\n- 计算 $\\ker(A)$ 的一个基并将其归一化以获得单位向量 $h$；对 $h$ 和 $-h$ 都进行验证。\n- 对于每种情况 $i \\in \\{1,2,3\\}$，将 $T^{(i)}$ 构建为 $\\Omega x^{(i)}$ 的支撑集（非零条目的索引）。\n- 对 $h$ 和 $-h$ 评估上述严格不等式，并生成一个布尔值 $b_{i}$，当且仅当不等式在两个方向上都成立时，该值为真。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含三个布尔值 $[b_{1},b_{2},b_{3}]$，以逗号分隔的列表形式包含在方括号中，按给定顺序对应三个测试用例。不允许有其他输出。\n\n此问题不涉及物理单位。不涉及角度。不涉及百分比。所有答案都以指定的单行最终输出格式的布尔值表示。",
            "solution": "目标是针对一个指定的分析算子 $\\Omega \\in \\mathbb{R}^{5 \\times 4}$ 和传感矩阵 $A \\in \\mathbb{R}^{3 \\times 4}$，验证关于从三个不同真实信号 $x^{(i)} \\in \\mathbb{R}^{4}$ 派生出的支撑集的分析零空间性质 (Analysis NSP)。分析 NSP 要求对于给定的支撑集 $T$，不等式 $\\|(\\Omega h)_{T}\\|_{1}  \\|(\\Omega h)_{T^{c}}\\|_{1}$ 对 $A$ 的零空间（记为 $\\ker(A)$）中的所有非零向量 $h$ 都成立。\n\n给定的矩阵是：\n$$\n\\Omega =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  \\phantom{-}1  -1  0 \\\\\n0  0  \\phantom{-}1  -1 \\\\\n1  0  0  0 \\\\\n0  0  0  1\n\\end{bmatrix}, \\quad\nA = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  1  0 \\\\\n0  0  1  1\n\\end{bmatrix}.\n$$\n问题陈述 $\\ker(A)$ 是一维的。因此，分析 NSP 的验证简化为检查张成该空间的两个单位范数向量（我们记为 $h$ 和 $-h$）是否满足不等式。\n\n首先，我们通过求解线性方程组 $A h = 0$ 来确定 $\\ker(A)$ 的一个基，其中 $h = [h_1, h_2, h_3, h_4]^T \\in \\mathbb{R}^4$。\n$$\n\\begin{cases}\nh_1 + h_3 = 0 \\\\\nh_2 + h_3 = 0 \\\\\nh_3 + h_4 = 0\n\\end{cases}\n$$\n设自由变量 $h_3 = c$（其中 $c \\in \\mathbb{R}$ 为某个标量），我们得到 $h_1 = -c$，$h_2 = -c$ 和 $h_4 = -c$。因此，零空间中的任何向量都具有 $c \\cdot [-1, -1, 1, -1]^T$ 的形式。$\\ker(A)$ 的一个基向量是 $h_0 = [-1, -1, 1, -1]^T$。\n\n为了获得单位范数向量 $h$，我们用 $h_0$ 的欧几里得范数对其进行归一化：\n$$\n\\|h_0\\|_2 = \\sqrt{(-1)^2 + (-1)^2 + 1^2 + (-1)^2} = \\sqrt{4} = 2.\n$$\n令 $h = \\frac{h_0}{\\|h_0\\|_2} = \\frac{1}{2}[-1, -1, 1, -1]^T = [-0.5, -0.5, 0.5, -0.5]^T$。$\\ker(A)$ 中所有单位范数向量的集合是 $\\{h, -h\\}$。\n\n接下来，我们计算向量 $\\Omega h$，它将用于所有测试用例：\n$$\n\\Omega h =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  \\phantom{-}1  -1  0 \\\\\n0  0  \\phantom{-}1  -1 \\\\\n1  0  0  0 \\\\\n0  0  0  1\n\\end{bmatrix}\n\\begin{bmatrix}\n-0.5 \\\\\n-0.5 \\\\\n0.5 \\\\\n-0.5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1(-0.5) - 1(-0.5) \\\\\n1(-0.5) - 1(0.5) \\\\\n1(0.5) - 1(-0.5) \\\\\n1(-0.5) \\\\\n1(-0.5)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n-1 \\\\\n1 \\\\\n-0.5 \\\\\n-0.5\n\\end{bmatrix}.\n$$\n用于 $-h$ 检查的向量就是 $\\Omega(-h) = -(\\Omega h) = [0, 1, -1, 0.5, 0.5]^T$。注意，对于任何向量 $z$ 和任何索引集 $S$，都有 $\\|(-z)_S\\|_1 = \\|z_S\\|_1$。因此，NSP 不等式对 $h$ 成立当且仅当它对 $-h$ 成立。尽管如此，该过程仍将为每种情况正式确认这一点。\n\n我们现在继续为三个测试用例中的每一个验证分析 NSP。\n\n情况 1: $x^{(1)} = [1, 1, 1, 1]^T$。\n首先，我们找到分析系数 $\\Omega x^{(1)}$：\n$$\n\\Omega x^{(1)} = \\begin{bmatrix} 1-1 \\\\ 1-1 \\\\ 1-1 \\\\ 1 \\\\ 1 \\end{bmatrix} = [0, 0, 0, 1, 1]^T.\n$$\n$\\Omega x^{(1)}$ 的支撑集由其非零元素的索引组成。按照问题描述中使用基于1的索引，支撑集为 $T^{(1)} = \\{4, 5\\}$。其补集为 $(T^{(1)})^c = \\{1, 2, 3\\}$。\n我们根据 $T^{(1)}$ 和 $(T^{(1)})^c$ 划分向量 $\\Omega h = [0, -1, 1, -0.5, -0.5]^T$：\n$$\n\\|(\\Omega h)_{T^{(1)}}\\|_1 = \\|[-0.5, -0.5]^T\\|_1 = |-0.5| + |-0.5| = 1.\n$$\n$$\n\\|(\\Omega h)_{(T^{(1)})^c}\\|_1 = \\|[0, -1, 1]^T\\|_1 = |0| + |-1| + |1| = 2.\n$$\nNSP 不等式为 $1  2$，这是成立的。如前所述，该不等式对 $-h$ 也成立。因此，对于这种情况，布尔结果是 $b_1 = \\text{True}$。\n\n情况 2: $x^{(2)} = [2, 1, 0, 0]^T$。\n分析系数为：\n$$\n\\Omega x^{(2)} = \\begin{bmatrix} 2-1 \\\\ 1-0 \\\\ 0-0 \\\\ 2 \\\\ 0 \\end{bmatrix} = [1, 1, 0, 2, 0]^T.\n$$\n支撑集为 $T^{(2)} = \\{1, 2, 4\\}$。其补集为 $(T^{(2)})^c = \\{3, 5\\}$。\n我们用 $\\Omega h = [0, -1, 1, -0.5, -0.5]^T$ 检查不等式：\n$$\n\\|(\\Omega h)_{T^{(2)}}\\|_1 = \\|[0, -1, -0.5]^T\\|_1 = |0| + |-1| + |-0.5| = 1.5.\n$$\n$$\n\\|(\\Omega h)_{(T^{(2)})^c}\\|_1 = \\|[1, -0.5]^T\\|_1 = |1| + |-0.5| = 1.5.\n$$\nNSP 不等式为 $1.5  1.5$，这是不成立的。该性质不成立。布尔结果是 $b_2 = \\text{False}$。\n\n情况 3: $x^{(3)} = [0, 1, -2, 3]^T$。\n分析系数为：\n$$\n\\Omega x^{(3)} = \\begin{bmatrix} 0-1 \\\\ 1-(-2) \\\\ -2-3 \\\\ 0 \\\\ 3 \\end{bmatrix} = [-1, 3, -5, 0, 3]^T.\n$$\n支撑集为 $T^{(3)} = \\{1, 2, 3, 5\\}$。其补集为 $(T^{(3)})^c = \\{4\\}$。\n我们用 $\\Omega h = [0, -1, 1, -0.5, -0.5]^T$ 检查不等式：\n$$\n\\|(\\Omega h)_{T^{(3)}}\\|_1 = \\|[0, -1, 1, -0.5]^T\\|_1 = |0| + |-1| + |1| + |-0.5| = 2.5.\n$$\n$$\n\\|(\\Omega h)_{(T^{(3)})^c}\\|_1 = \\|[-0.5]^T\\|_1 = |-0.5| = 0.5.\n$$\nNSP 不等式为 $2.5  0.5$，这是不成立的。该性质不成立。布尔结果是 $b_3 = \\text{False}$。\n\n总之，分析 NSP 对从 $x^{(1)}$ 派生的支撑集成立，但对从 $x^{(2)}$ 和 $x^{(3)}$ 派生的支撑集不成立。最终的布尔结果是 $[b_1, b_2, b_3] = [\\text{True}, \\text{False}, \\text{False}]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import null_space\n\ndef solve():\n    \"\"\"\n    Verifies the Analysis Null Space Property (Analysis NSP) for a given\n    setup and three test cases.\n    \"\"\"\n    # Define the analysis operator Omega and the sensing matrix A.\n    Omega = np.array([\n        [1, -1, 0, 0],\n        [0, 1, -1, 0],\n        [0, 0, 1, -1],\n        [1, 0, 0, 0],\n        [0, 0, 0, 1]\n    ], dtype=np.float64)\n\n    A = np.array([\n        [1, 0, 1, 0],\n        [0, 1, 1, 0],\n        [0, 0, 1, 1]\n    ], dtype=np.float64)\n\n    # Define the three ground-truth signals for the test cases.\n    test_cases = [\n        np.array([1, 1, 1, 1], dtype=np.float64),\n        np.array([2, 1, 0, 0], dtype=np.float64),\n        np.array([0, 1, -2, 3], dtype=np.float64)\n    ]\n\n    # Compute a unit-norm basis vector for the 1D null space of A.\n    # scipy.linalg.null_space returns an orthonormal basis as columns of a matrix.\n    # For a 1D null space, this is a single column vector of unit norm.\n    h = null_space(A).flatten()\n\n    results = []\n    for x in test_cases:\n        # Step 1: Compute analysis coefficients and determine the support set T.\n        omega_x = Omega @ x\n        \n        # The support T is the set of indices where omega_x is non-zero.\n        # A small tolerance is used for robust floating-point comparison.\n        tol = 1e-9\n        support_indices_T = np.where(np.abs(omega_x)  tol)[0]\n\n        # Step 2: Verify the Analysis NSP inequality for both h and -h.\n        \n        # Check for h: ||(Omega h)_T||_1  ||(Omega h)_Tc||_1\n        omega_h = Omega @ h\n        \n        # Partition omega_h into components inside and outside the support T.\n        all_indices = np.arange(Omega.shape[0])\n        support_indices_Tc = np.setdiff1d(all_indices, support_indices_T, assume_unique=True)\n\n        omega_h_T = omega_h[support_indices_T]\n        omega_h_Tc = omega_h[support_indices_Tc]\n\n        # Calculate the l1 norms of the partitions.\n        norm_T_h = np.sum(np.abs(omega_h_T))\n        norm_Tc_h = np.sum(np.abs(omega_h_Tc))\n        \n        # Evaluate the strict inequality for h.\n        holds_for_h = norm_T_h  norm_Tc_h\n\n        # Check for -h: ||(Omega (-h))_T||_1  ||(Omega (-h))_Tc||_1\n        # Since ||(-z)_S||_1 = ||z_S||_1, the result is identical to the one for h.\n        # This check is included for formal completeness as per the problem statement.\n        omega_minus_h = -omega_h\n        omega_minus_h_T = omega_minus_h[support_indices_T]\n        omega_minus_h_Tc = omega_minus_h[support_indices_Tc]\n\n        norm_T_minus_h = np.sum(np.abs(omega_minus_h_T))\n        norm_Tc_minus_h = np.sum(np.abs(omega_minus_h_Tc))\n\n        holds_for_minus_h = norm_T_minus_h  norm_Tc_minus_h\n        \n        # The property must hold for all unit-norm vectors in ker(A), i.e., for both h and -h.\n        final_boolean = holds_for_h and holds_for_minus_h\n        results.append(final_boolean)\n\n    # Format the final output string as specified.\n    # Python's str(True) is 'True', str(False) is 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在了解了信号可恢复的理论条件之后，下一个关键问题是如何验证一个候选解是否确实是优化问题的最优解。本练习将引导你使用凸优化中的基本工具——Karush-Kuhn-Tucker（KKT）条件，通过构建拉格朗日函数和求解对偶变量，来检验一个给定的点是否满足分析基追踪问题的最优性条件。",
            "id": "3431448",
            "problem": "考虑压缩感知和稀疏优化中的分析基追踪问题：在满足线性等式约束的条件下，最小化分析系数的1-范数。具体来说，考虑以下问题\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y,\n$$\n其中分析算子 $\\Omega \\in \\mathbb{R}^{3 \\times 4}$ 是一阶差分算子\n$$\n\\Omega = \\begin{pmatrix}\n1  -1  0  0 \\\\\n0  1  -1  0 \\\\\n0  0  1  -1\n\\end{pmatrix},\n$$\n测量矩阵 $A \\in \\mathbb{R}^{2 \\times 4}$ 为\n$$\nA = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  0  0  1\n\\end{pmatrix},\n$$\n以及测量向量 $y \\in \\mathbb{R}^{2}$ 为\n$$\ny = \\begin{pmatrix}\n2 \\\\\n3\n\\end{pmatrix}.\n$$\n考虑由下式给出的候选原始点 $x^{\\star} \\in \\mathbb{R}^{4}$\n$$\nx^{\\star} = \\begin{pmatrix}\n2 \\\\\n2 \\\\\n3 \\\\\n3\n\\end{pmatrix}.\n$$\n从等式约束问题的凸优化基本原理出发——特别是针对具有非光滑目标的凸规划的 Karush-Kuhn-Tucker (KKT) 条件以及1-范数的次梯度刻画——执行以下操作：\n\n- 确定一个与 $x^{\\star}$ 处的分析系数一致的次梯度向量 $s \\in \\partial \\|\\Omega x^{\\star}\\|_{1}$，并强制要求 $s$ 的每个分量都在区间 $[-1,1]$ 内。\n- 使用由1-范数目标和等式约束构成的拉格朗日函数的KKT平稳性条件，求解一个拉格朗日乘子向量 $\\lambda \\in \\mathbb{R}^{2}$，使得对于 $x^{\\star}$ 处某个有效的 $s$ 选择，平稳性条件成立。\n- 对于偶对 $(x^{\\star}, \\lambda)$ 和构造出的 $s$，验证其原始可行性、对偶可行性和互补松弛性。\n\n计算满足这些条件的拉格朗日乘子向量 $\\lambda$ 的显式值。使用 $\\mathrm{pmatrix}$ 环境将最终答案表示为行向量。无需进行四舍五入。",
            "solution": "该问题旨在通过找到一个满足 Karush-Kuhn-Tucker (KKT) 条件的相应拉格朗日乘子向量 $\\lambda$，来验证给定的点 $x^{\\star}$ 是否为分析基追踪优化问题的一个解。该优化问题表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\n这是一个凸优化问题，其目标函数 $f(x) = \\|\\Omega x\\|_{1}$ 是非光滑的，并带有仿射等式约束 $h(x) = Ax - y = 0$。KKT 条件为这类问题提供了充分必要的最优性条件。要使一个点 $x^{\\star}$ 是最优的，必须存在一个拉格朗日乘子向量 $\\lambda$，使得以下条件成立：\n\n1. **原始可行性：** $A x^{\\star} = y$\n2. **平稳性：** $0 \\in \\partial_{x} L(x^{\\star}, \\lambda)$，其中 $L(x, \\lambda) = \\|\\Omega x\\|_{1} + \\lambda^T(Ax - y)$ 是拉格朗日函数。\n\n拉格朗日函数关于 $x$ 的次微分由 $\\partial_{x} L(x, \\lambda) = \\partial (\\|\\Omega x\\|_{1}) + A^T \\lambda$ 给出。根据次梯度的链式法则，我们有 $\\partial (\\|\\Omega x\\|_{1}) = \\Omega^T \\partial (\\|z\\|_{1})$，其中 $z = \\Omega x$。因此，平稳性条件变为：\n$$\n0 \\in \\Omega^T \\partial (\\|\\Omega x^{\\star}\\|_{1}) + A^T \\lambda\n$$\n这等价于存在一个向量 $s \\in \\partial (\\|\\Omega x^{\\star}\\|_{1})$，使得：\n$$\n\\Omega^T s + A^T \\lambda = 0\n$$\n\n我们现在将根据给定的问题数据验证这些条件。\n\n**步骤1：验证原始可行性**\n我们必须检查是否 $A x^{\\star} = y$。给定的量为：\n$$\nA = \\begin{pmatrix} 1  0  0  0 \\\\ 0  0  0  1 \\end{pmatrix}, \\quad x^{\\star} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n执行矩阵-向量乘法：\n$$\nA x^{\\star} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(2) + (0)(3) + (0)(3) \\\\ (0)(2) + (0)(2) + (0)(3) + (1)(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\n由于 $A x^{\\star} = y$，原始可行性条件得到满足。\n\n**步骤2：刻画平稳性条件中的次梯度**\n首先，我们计算分析系数向量 $z^{\\star} = \\Omega x^{\\star}$：\n$$\nz^{\\star} = \\Omega x^{\\star} = \\begin{pmatrix} 1  -1  0  0 \\\\ 0  1  -1  0 \\\\ 0  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2 - 2 \\\\ 2 - 3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\n$\\ell_1$-范数 $\\|z\\|_{1} = \\sum_{i} |z_i|$ 的次微分是所有向量 $s$ 的集合，其分量 $s_i$ 满足：\n$$\ns_i = \\begin{cases} \\text{sgn}(z_i)  \\text{if } z_i \\neq 0 \\\\ c_i \\in [-1, 1]  \\text{if } z_i = 0 \\end{cases}\n$$\n对于我们的向量 $z^{\\star} = (0, -1, 0)^T$，一个向量 $s = (s_1, s_2, s_3)^T$ 属于 $\\partial \\|z^{\\star}\\|_{1}$ 的充要条件是：\n- $s_1 \\in [-1, 1]$ (因为 $z^{\\star}_1 = 0$)\n- $s_2 = \\text{sgn}(-1) = -1$\n- $s_3 \\in [-1, 1]$ (因为 $z^{\\star}_3 = 0$)\n\n$s$ 的每个分量都位于区间 $[-1, 1]$ 内的条件，有时被称为次梯度变量的对偶可行性。对于 $z^{\\star}$ 的非零分量，此条件自然满足，并成为对零分量的选择的一个约束。\n\n**步骤3：使用平稳性条件求解拉格朗日乘子 $\\lambda$**\n平稳性条件是 $\\Omega^T s + A^T \\lambda = 0$，对于某个 $s \\in \\partial \\|z^{\\star}\\|_{1}$ 和某个 $\\lambda \\in \\mathbb{R}^2$ 成立。设 $\\lambda = (\\lambda_1, \\lambda_2)^T$。我们写出这些矩阵：\n$$\n\\Omega^T = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  -1  1 \\\\ 0  0  -1 \\end{pmatrix}, \\quad A^T = \\begin{pmatrix} 1  0 \\\\ 0  0 \\\\ 0  0 \\\\ 0  1 \\end{pmatrix}\n$$\n代入平稳性方程：\n$$\n\\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  -1  1 \\\\ 0  0  -1 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ -1 \\\\ s_3 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  0 \\\\ 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给出了一个关于未知数 $s_1$、$s_3$、$\\lambda_1$ 和 $\\lambda_2$ 的四个线性方程组：\n1. $s_1 + \\lambda_1 = 0$\n2. $-s_1 + (1)(-1) = -s_1 - 1 = 0$\n3. $(-1)(-1) + s_3 = 1 + s_3 = 0$\n4. $-s_3 + \\lambda_2 = 0$\n\n我们可以求解这个方程组。\n由方程(2)可得：$-s_1 - 1 = 0 \\implies s_1 = -1$。该值与次梯度条件 $s_1 \\in [-1, 1]$ 一致。\n由方程(3)可得：$1 + s_3 = 0 \\implies s_3 = -1$。该值也与次梯度条件 $s_3 \\in [-1, 1]$ 一致。\n\n现在我们已经确定了使得 KKT 条件在 $x^{\\star}$ 处成立所必需的特定次梯度向量 $s = (-1, -1, -1)^T$。我们现在可以使用剩余的方程来求解 $\\lambda_1$ 和 $\\lambda_2$。\n由方程(1)可得：$s_1 + \\lambda_1 = 0 \\implies -1 + \\lambda_1 = 0 \\implies \\lambda_1 = 1$。\n由方程(4)可得：$-s_3 + \\lambda_2 = 0 \\implies -(-1) + \\lambda_2 = 0 \\implies 1 + \\lambda_2 = 0 \\implies \\lambda_2 = -1$。\n\n因此，我们找到了一个拉格朗日乘子向量 $\\lambda = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n**步骤4：最终验证**\n我们找到了一个偶对 $(x^{\\star}, \\lambda)$ 和一个次梯度 $s$，它们同时满足所有 KKT 条件：\n- **原始可行性：** $A x^{\\star} = y$ (已验证)。\n- **平稳性：** $\\Omega^T s + A^T \\lambda = 0$ 对于 $x^{\\star} = (2, 2, 3, 3)^T$、$\\lambda = (1, -1)^T$ 和 $s = (-1, -1, -1)^T$ 成立。\n- **对偶可行性 / 互补松弛性：** 次梯度 $s$ 是有效的，因为 $s \\in \\partial \\|\\Omega x^{\\star}\\|_{1}$。分量 $s_1 = -1$ 和 $s_3 = -1$ 都在要求的区间 $[-1, 1]$ 内，且 $s_2 = -1$ 与 $\\text{sgn}((\\Omega x^{\\star})_2)$ 相匹配。\n\n由于所有 KKT 条件都得到满足，点 $x^{\\star}$ 是该问题的极小点，相应的拉格朗日乘子向量为 $\\lambda = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。题目要求计算这个显式值。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  -1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "重加权$\\ell_1$最小化是一种有效的迭代算法，它通过在前一次迭代解的基础上更新权重，能够更好地促进解的稀疏性，从而通常优于标准的分析基追踪。这个实践将带你完整地执行一次迭代过程，从计算权重到求解加权子问题，让你亲身体验这一高级算法的内部工作机制。",
            "id": "3431429",
            "problem": "考虑通过分析基追踪进行基于分析的恢复，使用一维总变分 (TV) 先验。设分析算子为一阶前向差分 $$D \\in \\mathbb{R}^{2 \\times 3}, \\quad D = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix},$$ 作用于信号 $$x \\in \\mathbb{R}^{3}.$$ 给定数据向量 $$y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix},$$ 你需要执行一次带有最小二乘数据保真度的重加权分析 $\\ell_{1}$ 最小化迭代。将重加权分析子问题定义为严格凸目标函数 $$\\min_{x \\in \\mathbb{R}^{3}} \\; \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{2} w^{(1)}_{i} \\, \\big| (D x)_{i} \\big|$$ 的最小化解，其中正则化参数为 $$\\lambda = 1.$$ 本次迭代的权重由给定的初始估计 $$x^{(0)} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix}$$ 根据标准规则 $$w^{(1)}_{i} = \\frac{1}{\\left| (D x^{(0)})_{i} \\right| + \\epsilon}, \\quad i \\in \\{1, 2\\},$$ 计算得出，其中 $$\\epsilon = 1.$$ 仅使用这些定义和原理，计算权重 $$w^{(1)}$$，然后精确求解该加权子问题以获得 $$x^{(1)}$$。请使用单个行矩阵，以精确形式报告 $$x^{(1)}$$ 的行向量元素作为你的最终答案。不要进行四舍五入。",
            "solution": "用户提供了一个在重加权分析 $\\ell_1$ 最小化背景下的定义明确的优化问题，该主题属于稀疏优化和压缩感知领域。所有参数和矩阵都已明确定义，目标函数是严格凸的（确保了唯一的最小值），并且该问题在科学上基于已建立的数学原理。因此该问题是有效的。\n\n任务是找到以下最小化问题的解 $x^{(1)}$：\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\; F(x) = \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{2} w^{(1)}_{i} \\, \\big| (D x)_{i} \\big|\n$$\n其中 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$。\n\n给定的参数如下：\n- 数据向量：$y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n- 分析算子：$D = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix}$\n- 正则化参数：$\\lambda = 1$\n- 用于计算权重的初始估计：$x^{(0)} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix}$\n- 用于计算权重的平滑参数：$\\epsilon = 1$\n\n首先，我们使用规则 $w^{(1)}_{i} = \\frac{1}{\\left| (D x^{(0)})_{i} \\right| + \\epsilon}$ 计算权重 $w^{(1)} = \\begin{pmatrix} w^{(1)}_1 \\\\ w^{(1)}_2 \\end{pmatrix}$。\n我们计算 $x^{(0)}$ 的分析系数：\n$$\nD x^{(0)} = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} (-1)(0) + (1)(2) + (0)(2) \\\\ (0)(0) + (-1)(2) + (1)(2) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\n所以，$(D x^{(0)})_1 = 2$ 且 $(D x^{(0)})_2 = 0$。\n现在，我们使用 $\\epsilon = 1$ 计算权重：\n$$\nw^{(1)}_{1} = \\frac{1}{|(D x^{(0)})_1| + \\epsilon} = \\frac{1}{|2| + 1} = \\frac{1}{3}\n$$\n$$\nw^{(1)}_{2} = \\frac{1}{|(D x^{(0)})_2| + \\epsilon} = \\frac{1}{|0| + 1} = \\frac{1}{1} = 1\n$$\n\n使用这些权重和 $\\lambda = 1$，目标函数变为：\n$$\nF(x) = \\frac{1}{2} \\left( (x_1 - 0)^2 + (x_2 - 1)^2 + (x_3 - 1)^2 \\right) + \\frac{1}{3} |(Dx)_1| + 1 \\cdot |(Dx)_2|\n$$\n其中 $(Dx)_1 = -x_1 + x_2$ 且 $(Dx)_2 = -x_2 + x_3$。\n\n目标函数 $F(x)$ 是凸函数，但并非处处可微。通过将 $F(x)$ 的次梯度设为零：$0 \\in \\partial F(x^{(1)})$，可以找到其最小化解 $x^{(1)}$。其次微分由下式给出：\n$$\n\\partial F(x) = (x - y) + \\lambda D^T v\n$$\n其中 $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ 是一个次梯度向量。分量 $v_i$ 必须满足 $v_i \\in w^{(1)}_i \\partial (|(Dx)_i|)$，这意味着：\n- $v_1 \\in \\frac{1}{3} \\partial (|-x_1+x_2|)$\n- $v_2 \\in 1 \\cdot \\partial (|-x_2+x_3|)$\n\n绝对值函数的次微分为：如果 $z \\neq 0$，则 $\\partial |z| = \\text{sgn}(z)$；如果 $z=0$，则 $\\partial|0| = [-1, 1]$。\n最优性条件 $0 = x^{(1)} - y + \\lambda D^T v$ 可以写为 $x^{(1)} - y = -\\lambda D^T v$。令 $x = x^{(1)}$。由于 $\\lambda=1$：\n$$\n\\begin{pmatrix} x_1 - 0 \\\\ x_2 - 1 \\\\ x_3 - 1 \\end{pmatrix} = - \\begin{pmatrix} -1  0 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} v_1 \\\\ -v_1 + v_2 \\\\ -v_2 \\end{pmatrix}\n$$\n这给出了关于 $x$ 的分量的方程组：\n1. $x_1 = v_1$\n2. $x_2 - 1 = -v_1 + v_2 \\implies x_2 = 1 - v_1 + v_2$\n3. $x_3 - 1 = -v_2 \\implies x_3 = 1 - v_2$\n\n现在，我们将绝对值函数的自变量 $z_1 = -x_1+x_2$ 和 $z_2 = -x_2+x_3$ 用 $v_1$ 和 $v_2$ 表示：\n$$\nz_1 = -x_1 + x_2 = -v_1 + (1 - v_1 + v_2) = 1 - 2v_1 + v_2\n$$\n$$\nz_2 = -x_2 + x_3 = -(1 - v_1 + v_2) + (1 - v_2) = v_1 - 2v_2\n$$\n\n对 $v_1$ 和 $v_2$ 的次梯度条件为：\n- 如果 $z_1 \\neq 0$，则 $v_1 = \\frac{1}{3} \\text{sgn}(z_1)$；如果 $z_1 = 0$，则 $v_1 \\in [-\\frac{1}{3}, \\frac{1}{3}]$。\n- 如果 $z_2 \\neq 0$，则 $v_2 = \\text{sgn}(z_2)$；如果 $z_2 = 0$，则 $v_2 \\in [-1, 1]$。\n\n我们分析 $z_1$ 和 $z_2$ 的可能情况。在探讨了两者都非零的情况后，我们研究其中一个为零的情况。让我们测试 $z_2=0$ 且 $z_1 \\neq 0$ 的情况。\n如果 $z_2 = 0$，则 $v_1 - 2v_2 = 0 \\implies v_1 = 2v_2$。对于 $z_2=0$ 的次梯度条件是 $v_2 \\in [-1, 1]$。\n对 $v_1$ 的条件取决于 $z_1 = 1 - 2v_1 + v_2$ 的符号。代入 $v_2 = v_1/2$：\n$$\nz_1 = 1 - 2v_1 + \\frac{v_1}{2} = 1 - \\frac{3}{2}v_1\n$$\n由于我们假设 $z_1 \\neq 0$，我们有 $v_1 = \\frac{1}{3}\\text{sgn}(z_1) = \\frac{1}{3}\\text{sgn}(1 - \\frac{3}{2}v_1)$。此外，当 $z_1 \\neq 0$ 时，对 $v_1$ 的条件是 $|v_1| = 1/3$。\n我们来测试 $v_1 = 1/3$。那么 $v_2 = v_1/2 = 1/6$。我们检查这是否一致。\n- 对于 $z_2=0$ 的情况，$v_2 = 1/6$ 是一个有效选择吗？是的，因为 $1/6 \\in [-1, 1]$。\n- 当 $v_1 = 1/3$ 时，$z_1$ 是多少？$z_1 = 1 - \\frac{3}{2}(\\frac{1}{3}) = 1 - \\frac{1}{2} = \\frac{1}{2}$。\n- $v_1=1/3$ 与 $z_1=1/2$ 是否一致？条件是 $v_1 = \\frac{1}{3}\\text{sgn}(z_1)$。\n  $\\frac{1}{3} = \\frac{1}{3}\\text{sgn}(\\frac{1}{2}) \\implies \\frac{1}{3} = \\frac{1}{3}(1)$。这是成立的。\n\n两个条件都得到满足。因此，对偶变量的解是 $v_1 = 1/3$ 和 $v_2 = 1/6$。\n\n最后，我们使用这些值计算原始解 $x^{(1)}$：\n$$\nx_1^{(1)} = v_1 = \\frac{1}{3}\n$$\n$$\nx_2^{(1)} = 1 - v_1 + v_2 = 1 - \\frac{1}{3} + \\frac{1}{6} = \\frac{6-2+1}{6} = \\frac{5}{6}\n$$\n$$\nx_3^{(1)} = 1 - v_2 = 1 - \\frac{1}{6} = \\frac{5}{6}\n$$\n解向量为 $x^{(1)} = \\begin{pmatrix} 1/3 \\\\ 5/6 \\\\ 5/6 \\end{pmatrix}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}  \\frac{5}{6}  \\frac{5}{6} \\end{pmatrix}}\n$$"
        }
    ]
}