{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation in the co-sparsity model, we begin with the ideal recovery scenario. This exercise asks you to compute the \"oracle estimator,\" which represents the best possible signal estimate achievable when the true co-support is known beforehand. By solving this constrained least-squares problem , you will apply fundamental optimization principles, such as deriving the Karush-Kuhn-Tucker (KKT) conditions, to find a concrete solution and verify its uniqueness, providing a clear benchmark for understanding more complex recovery methods.",
            "id": "3486309",
            "problem": "Consider the co-sparsity analysis model in compressed sensing, where an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$ acts on a signal $x \\in \\mathbb{R}^{n}$, and the co-support $\\Lambda \\subseteq \\{1,2,\\dots,p\\}$ is the set of indices where the analysis coefficients vanish, that is, $\\Omega_{\\Lambda} x = 0$. The oracle estimator is defined as the unique minimizer $x^{\\star}$ of the equality-constrained least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2}$ subject to $\\Omega_{\\Lambda} x = 0$, when the true co-support $\\Lambda$ is known.\n\nWork with the following small instance at dimension $n=5$ and measurements $m=3$. Let the analysis operator $\\Omega \\in \\mathbb{R}^{4 \\times 5}$ be the first-difference operator\n$$\n\\Omega \\;=\\;\n\\begin{pmatrix}\n1  -1  0  0  0 \\\\\n0  1  -1  0  0 \\\\\n0  0  1  -1  0 \\\\\n0  0  0  1  -1\n\\end{pmatrix},\n$$\nand let the co-support be $\\Lambda = \\{1,3,4\\}$, so that the exact linear constraints are\n$$\n\\Omega_{\\Lambda} x = 0 \\quad \\Longleftrightarrow \\quad\n\\begin{cases}\nx_{1} - x_{2} = 0, \\\\\nx_{3} - x_{4} = 0, \\\\\nx_{4} - x_{5} = 0.\n\\end{cases}\n$$\nLet the measurement operator $A \\in \\mathbb{R}^{3 \\times 5}$ and the measurement vector $y \\in \\mathbb{R}^{3}$ be\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  0  1  0  0 \\\\\n0  1  0  1  0 \\\\\n0  0  0  0  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n2 \\\\\n2 \\\\\n3\n\\end{pmatrix}.\n$$\n\nTasks:\n1. Starting from first principles, derive the equality-constrained least-squares optimality conditions by forming the Lagrangian and the Karush-Kuhn-Tucker (KKT) system, and explicitly solve the resulting stacked linear system to obtain the oracle estimator $x^{\\star}$.\n2. Verify the uniqueness of the solution by computing the rank of the vertically stacked matrix $\\begin{pmatrix} A \\\\ \\Omega_{\\Lambda} \\end{pmatrix}$ and interpreting it in terms of the intersection of null spaces.\n3. Report the value of the third component $x^{\\star}_{3}$ of the oracle estimator as your final answer.\n\nNo rounding is required. Provide your final answer as a single real number without units.",
            "solution": "The problem is validated as being scientifically grounded, well-posed, objective, and complete. It is a standard exercise in optimization and linear algebra within the context of compressed sensing. I will proceed with the solution.\n\nThe problem asks for the oracle estimator $x^{\\star} \\in \\mathbb{R}^{n}$, which is the unique solution to the equality-constrained least-squares problem:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|A x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\Omega_{\\Lambda} x = 0 $$\nwhere $n=5$. The measurement operator $A \\in \\mathbb{R}^{3 \\times 5}$ and vector $y \\in \\mathbb{R}^{3}$ are given by:\n$$ A = \\begin{pmatrix} 1  0  1  0  0 \\\\ 0  1  0  1  0 \\\\ 0  0  0  0  1 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} $$\nThe analysis operator is $\\Omega \\in \\mathbb{R}^{4 \\times 5}$ and the co-support is $\\Lambda = \\{1,3,4\\}$. The constraint matrix $\\Omega_{\\Lambda}$ consists of the rows of $\\Omega$ indexed by $\\Lambda$:\n$$ \\Omega_{\\Lambda} = \\begin{pmatrix} 1  -1  0  0  0 \\\\ 0  0  1  -1  0 \\\\ 0  0  0  1  -1 \\end{pmatrix} $$\nThe constraint $\\Omega_{\\Lambda} x = 0$ corresponds to the system of linear equations:\n$$ \\begin{cases} x_{1} - x_{2} = 0 \\\\ x_{3} - x_{4} = 0 \\\\ x_{4} - x_{5} = 0 \\end{cases} $$\n\n**1. Derivation of Optimality Conditions and Solution**\n\nTo solve this constrained optimization problem, we form the Lagrangian $\\mathcal{L}(x, \\nu)$, where $\\nu \\in \\mathbb{R}^{3}$ is the vector of Lagrange multipliers. The objective function is $f(x) = \\|A x - y\\|_{2}^{2} = (Ax-y)^T(Ax-y)$.\n$$ \\mathcal{L}(x, \\nu) = \\|A x - y\\|_{2}^{2} + \\nu^T (\\Omega_{\\Lambda} x) $$\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality are obtained by setting the gradients of the Lagrangian with respect to $x$ and $\\nu$ to zero.\n\nThe gradient with respect to $x$ is:\n$$ \\nabla_{x} \\mathcal{L}(x, \\nu) = \\nabla_{x} (x^T A^T A x - 2y^T A x + y^T y) + \\nabla_{x} (\\nu^T \\Omega_{\\Lambda} x) = 2A^T A x - 2A^T y + \\Omega_{\\Lambda}^T \\nu $$\nSetting this to zero gives the stationarity condition:\n$$ 2A^T A x + \\Omega_{\\Lambda}^T \\nu = 2A^T y $$\nThe gradient with respect to $\\nu$ gives the primal feasibility condition, which is simply the original constraint:\n$$ \\nabla_{\\nu} \\mathcal{L}(x, \\nu) = \\Omega_{\\Lambda} x = 0 $$\nThese two conditions form a system of linear equations in $x$ and $\\nu$, known as the KKT system:\n$$ \\begin{pmatrix} 2A^T A  \\Omega_{\\Lambda}^T \\\\ \\Omega_{\\Lambda}  0 \\end{pmatrix} \\begin{pmatrix} x \\\\ \\nu \\end{pmatrix} = \\begin{pmatrix} 2A^T y \\\\ 0 \\end{pmatrix} $$\nThis constitutes the derivation from first principles as requested.\n\nTo solve for $x^{\\star}$, we can directly use the constraints to simplify the problem. The constraints imply $x_1 = x_2$ and $x_3 = x_4 = x_5$. This means the solution vector $x^{\\star}$ lies in the subspace spanned by the vectors $(1, 1, 0, 0, 0)^T$ and $(0, 0, 1, 1, 1)^T$. Any vector $x$ in this subspace can be parameterized as $x = (c_1, c_1, c_2, c_2, c_2)^T$ for some scalars $c_1, c_2 \\in \\mathbb{R}$.\n\nSubstituting this form of $x$ into the expression $Ax$:\n$$ Ax = \\begin{pmatrix} 1  0  1  0  0 \\\\ 0  1  0  1  0 \\\\ 0  0  0  0  1 \\end{pmatrix} \\begin{pmatrix} c_1 \\\\ c_1 \\\\ c_2 \\\\ c_2 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} c_1 + c_2 \\\\ c_1 + c_2 \\\\ c_2 \\end{pmatrix} $$\nThe objective function becomes a function of $c_1$ and $c_2$:\n$$ \\|Ax - y\\|_{2}^{2} = \\left\\| \\begin{pmatrix} c_1 + c_2 \\\\ c_1 + c_2 \\\\ c_2 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\end{pmatrix} \\right\\|_{2}^{2} = 2(c_1 + c_2 - 2)^2 + (c_2 - 3)^2 $$\nThis is an unconstrained quadratic minimization problem in $c_1$ and $c_2$. We find the minimum by setting the partial derivatives to zero:\n$$ \\frac{\\partial}{\\partial c_1} [2(c_1 + c_2 - 2)^2 + (c_2 - 3)^2] = 4(c_1 + c_2 - 2) = 0 $$\n$$ \\frac{\\partial}{\\partial c_2} [2(c_1 + c_2 - 2)^2 + (c_2 - 3)^2] = 4(c_1 + c_2 - 2) + 2(c_2 - 3) = 0 $$\nFrom the first equation, we get $c_1 + c_2 - 2 = 0$. Substituting this into the second equation gives:\n$$ 4(0) + 2(c_2 - 3) = 0 \\implies c_2 = 3 $$\nSubstituting $c_2 = 3$ back into $c_1 + c_2 - 2 = 0$ gives $c_1 + 3 - 2 = 0$, which implies $c_1 = -1$.\n\nThus, the optimal coefficients are $c_1 = -1$ and $c_2 = 3$. The oracle estimator is:\n$$ x^{\\star} = \\begin{pmatrix} c_1 \\\\ c_1 \\\\ c_2 \\\\ c_2 \\\\ c_2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 3 \\\\ 3 \\\\ 3 \\end{pmatrix} $$\n\n**2. Verification of Uniqueness**\n\nThe uniqueness of the solution $x^{\\star}$ is guaranteed if the matrix formed by vertically stacking $A$ and $\\Omega_{\\Lambda}$ has full column rank. Let this matrix be $M$:\n$$ M = \\begin{pmatrix} A \\\\ \\Omega_{\\Lambda} \\end{pmatrix} = \\begin{pmatrix}\n1  0  1  0  0 \\\\\n0  1  0  1  0 \\\\\n0  0  0  0  1 \\\\\n1  -1  0  0  0 \\\\\n0  0  1  -1  0 \\\\\n0  0  0  1  -1\n\\end{pmatrix} $$\nThis is a $6 \\times 5$ matrix. It has full column rank if its rank is $5$, which is equivalent to its null space being trivial, i.e., $\\text{ker}(M) = \\{0\\}$. Let's solve $Mx=0$ for $x \\in \\mathbb{R}^5$:\n1. $x_1 + x_3 = 0$\n2. $x_2 + x_4 = 0$\n3. $x_5 = 0$\n4. $x_1 - x_2 = 0 \\implies x_1 = x_2$\n5. $x_3 - x_4 = 0 \\implies x_3 = x_4$\n6. $x_4 - x_5 = 0 \\implies x_4 = x_5$\n\nFrom $(3)$, $x_5 = 0$. Using $(6)$, we find $x_4 = 0$. Using $(5)$, we find $x_3 = 0$. Using $(2)$, we have $x_2 + 0 = 0$, so $x_2 = 0$. Finally, using $(4)$, we get $x_1 = 0$.\nThe only solution to $Mx=0$ is $x=(0,0,0,0,0)^T$. Thus, $\\text{ker}(M)=\\{0\\}$ and $\\text{rank}(M) = 5$.\nThis condition is equivalent to the intersection of the null spaces of $A$ and $\\Omega_{\\Lambda}$ being trivial: $\\text{ker}(A) \\cap \\text{ker}(\\Omega_{\\Lambda}) = \\{0\\}$. This ensures that there is at most one vector $x$ satisfying both the measurement model and the co-sparsity constraints, guaranteeing a unique solution to the oracle problem.\n\n**3. Final Answer**\n\nThe oracle estimator is $x^{\\star} = (-1, -1, 3, 3, 3)^T$. The third component is $x_3^{\\star}$.\n$$ x_3^{\\star} = 3 $$",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "While the co-sparsity model provides a powerful framework for signal representation, its effectiveness hinges on the properties of the analysis operator $\\Omega$. This practice explores a fundamental limitation: the failure of unique signal identification when the nullspace of $\\Omega$ is non-trivial. By constructing a case where different signals produce the same analysis coefficients , you will gain a deeper appreciation for why the condition $\\mathrm{null}(\\Omega) = \\{0\\}$ is crucial for ensuring that a signal is uniquely defined by its analysis representation.",
            "id": "3486293",
            "problem": "Let $n \\in \\mathbb{N}$ and $p \\in \\mathbb{N}$ with $p  n$. Consider the analysis (co-sparsity) model in compressed sensing and sparse optimization, where an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$ acts on signals $x \\in \\mathbb{R}^{n}$ to produce analysis coefficients $\\Omega x \\in \\mathbb{R}^{p}$. A signal is called $\\ell$-co-sparse if exactly $\\ell$ rows of $\\Omega$ annihilate $x$. The fundamental facts you may use are: the definition of the nullspace $\\mathrm{null}(\\Omega) = \\{x \\in \\mathbb{R}^{n} : \\Omega x = 0\\}$, the rank-nullity theorem $\\dim(\\mathrm{null}(\\Omega)) = n - \\mathrm{rank}(\\Omega)$ for linear maps, and that for any $x_{0} \\in \\mathbb{R}^{n}$ the affine set $\\{x \\in \\mathbb{R}^{n} : \\Omega x = \\Omega x_{0}\\}$ equals $x_{0} + \\mathrm{null}(\\Omega)$.\n\nConstruct the following explicit case and analyze it from first principles.\n\n- Let $n = 3$ and $p = 2$. Define the analysis operator\n$$\n\\Omega \\;=\\; \\begin{pmatrix}\n1  -1  0 \\\\\n2  -2  0\n\\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3},\n$$\nand the full measurement operator $A = I_{3} \\in \\mathbb{R}^{3 \\times 3}$ (the identity matrix). Let the ground-truth signal be $x_{0} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 5 \\end{pmatrix} \\in \\mathbb{R}^{3}$, so the full measurements are $y = A x_{0} = x_{0}$ and the analysis coefficients are $u_{0} = \\Omega x_{0}$.\n\n- Suppose that, instead of fitting the full measurements $y$ directly, one attempts to identify $x$ from the co-sparsity constraints encoded only by the analysis equality $\\Omega x = \\Omega x_{0}$ (that is, by enforcing that $x$ matches the observed analysis coefficients $u_{0}$), without further structural restrictions. Explain, using only linear algebra and the definition of co-sparsity, why the nontrivial nullspace $\\mathrm{null}(\\Omega) \\neq \\{0\\}$ causes the failure of uniqueness of $x$ in this identification, even though the measurement operator is full ($A = I_{3}$). Derive the explicit affine set of indistinguishable signals\n$$\n\\mathcal{S} \\;=\\; \\{ x \\in \\mathbb{R}^{3} \\;:\\; \\Omega x = \\Omega x_{0} \\},\n$$\nand then determine its dimension.\n\nYour answer should be the single real number equal to $\\dim(\\mathcal{S})$. No rounding is required. Report only this number.",
            "solution": "The problem requires an analysis of the uniqueness of a signal $x \\in \\mathbb{R}^{3}$ given the constraint $\\Omega x = \\Omega x_{0}$, where $\\Omega$ is a specified analysis operator and $x_0$ is a ground-truth signal. We are asked to explain the failure of uniqueness, derive the explicit set of all possible solutions, and determine its dimension.\n\nThe givens are:\n- The dimension of the signal space is $n=3$.\n- The dimension of the analysis space is $p=2$.\n- The analysis operator is $\\Omega = \\begin{pmatrix} 1  -1  0 \\\\ 2  -2  0 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}$.\n- The ground-truth signal is $x_{0} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 5 \\end{pmatrix} \\in \\mathbb{R}^{3}$.\n\nThe problem is to characterize the set of signals $\\mathcal{S} = \\{ x \\in \\mathbb{R}^{3} : \\Omega x = \\Omega x_{0} \\}$. This set represents all signals that are indistinguishable from $x_0$ based solely on the analysis coefficients.\n\nFirst, let's address the reason for the failure of uniqueness. A unique solution for $x$ in the equation $\\Omega x = \\Omega x_{0}$ would exist if and only if the corresponding homogeneous equation $\\Omega z = 0$ has only the trivial solution $z = 0$. The set of solutions to $\\Omega z = 0$ is the nullspace of $\\Omega$, denoted $\\mathrm{null}(\\Omega)$. If $\\mathrm{null}(\\Omega)$ is non-trivial (i.e., it contains non-zero vectors), then for any non-zero $z \\in \\mathrm{null}(\\Omega)$, the signal $\\tilde{x} = x_{0} + z$ is distinct from $x_{0}$ but yields the same analysis coefficients:\n$$\n\\Omega \\tilde{x} = \\Omega (x_{0} + z) = \\Omega x_{0} + \\Omega z = \\Omega x_{0} + 0 = \\Omega x_{0}\n$$\nThus, if $\\mathrm{null}(\\Omega) \\neq \\{0\\}$, there are infinitely many solutions, and uniqueness fails. The set of all solutions $\\mathcal{S}$ is the affine set formed by translating the nullspace $\\mathrm{null}(\\Omega)$ by the particular solution $x_{0}$, as given by the identity $\\{x \\in \\mathbb{R}^{n} : \\Omega x = \\Omega x_{0}\\} = x_{0} + \\mathrm{null}(\\Omega)$.\n\nTo determine if $\\mathrm{null}(\\Omega)$ is non-trivial, we compute its dimension using the rank-nullity theorem: $\\dim(\\mathrm{null}(\\Omega)) = n - \\mathrm{rank}(\\Omega)$. Here, $n=3$. We need to find the rank of $\\Omega$.\nThe matrix is $\\Omega = \\begin{pmatrix} 1  -1  0 \\\\ 2  -2  0 \\end{pmatrix}$. The second row, $(2, -2, 0)$, is exactly $2$ times the first row, $(1, -1, 0)$. The rows are linearly dependent. The row space is spanned by the single vector $(1, -1, 0)$. Therefore, the dimension of the row space, which equals the rank of the matrix, is $1$.\n$$\n\\mathrm{rank}(\\Omega) = 1\n$$\nApplying the rank-nullity theorem:\n$$\n\\dim(\\mathrm{null}(\\Omega)) = n - \\mathrm{rank}(\\Omega) = 3 - 1 = 2\n$$\nSince $\\dim(\\mathrm{null}(\\Omega)) = 2 > 0$, the nullspace is non-trivial, which proves the failure of uniqueness. The fact that the measurement operator is the identity, $A = I_3$, is irrelevant because the problem explicitly states that we are to use only the information from the analysis constraint $\\Omega x = \\Omega x_0$.\n\nNext, we derive the explicit affine set $\\mathcal{S}$. We first compute the analysis coefficients $u_0 = \\Omega x_{0}$:\n$$\nu_{0} = \\Omega x_{0} = \\begin{pmatrix} 1  -1  0 \\\\ 2  -2  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1(1) + (-1)(2) + 0(5) \\\\ 2(1) + (-2)(2) + 0(5) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix}\n$$\nThe set $\\mathcal{S}$ consists of all vectors $x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$ that satisfy $\\Omega x = u_{0}$:\n$$\n\\begin{pmatrix} 1  -1  0 \\\\ 2  -2  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix}\n$$\nThis corresponds to the system of linear equations:\n$1. \\quad x_1 - x_2 + 0x_3 = -1$\n$2. \\quad 2x_1 - 2x_2 + 0x_3 = -2$\nThe second equation is a multiple of the first and is therefore redundant. The system reduces to the single constraint $x_1 - x_2 = -1$, or $x_1 = x_2 - 1$. The variable $x_3$ is not constrained by any equation, so it can be any real number.\nWe can express the general solution by introducing two free parameters, say $s, t \\in \\mathbb{R}$. Let $x_2 = s$ and $x_3 = t$. Then $x_1 = s - 1$. The set of solutions $\\mathcal{S}$ can be written in parametric vector form as:\n$$\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} s - 1 \\\\ s \\\\ t \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix} + s \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nSo, the explicit affine set is:\n$$\n\\mathcal{S} = \\left\\{ \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix} + s \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\;\\bigg|\\; s, t \\in \\mathbb{R} \\right\\}\n$$\nThis represents a plane in $\\mathbb{R}^{3}$.\n\nFinally, we determine the dimension of this affine set, $\\dim(\\mathcal{S})$. The dimension of an affine set is defined as the dimension of the vector subspace of which it is a translation. In this case, $\\mathcal{S}$ is a translation of the subspace spanned by the two linearly independent vectors $\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. This subspace is, by definition, the nullspace of $\\Omega$. As this spanning set contains two linearly independent vectors, the dimension of the subspace is $2$.\nTherefore, the dimension of the affine set $\\mathcal{S}$ is $2$. This is consistent with our earlier calculation that $\\dim(\\mathcal{S}) = \\dim(\\mathrm{null}(\\Omega)) = 2$.",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "Moving from theory to practice, we often don't know the true co-support of a signal. Instead, we use convex relaxation techniques to promote solutions with sparse analysis coefficients. This exercise  introduces one of the most successful applications of the co-sparsity model: Total Variation (TV) regularization, which uses an $\\ell_1$-norm penalty on signal differences to promote piecewise-constant solutions. By deriving the optimality conditions and solving for the TV-regularized estimator, you will see how the abstract co-sparsity concept translates into a powerful algorithm for practical signal denoising and reconstruction.",
            "id": "3486321",
            "problem": "Consider the analysis co-sparsity formulation in compressed sensing, where one seeks a signal $x \\in \\mathbb{R}^{n}$ whose analysis coefficients $B x$ are sparse with respect to a fixed linear operator $B$. In one dimension, a common choice is the first-order difference operator, which induces piecewise-constant solutions through Total Variation (TV) regularization. Given a measurement operator $A \\in \\mathbb{R}^{m \\times n}$ and observed data $y \\in \\mathbb{R}^{m}$, the TV-regularized estimator is defined as the unique minimizer of the convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|B x\\|_{1},\n$$\nwhere $\\lambda  0$ is a regularization parameter and $B \\in \\mathbb{R}^{(n-1) \\times n}$ is the one-dimensional forward difference matrix.\n\nWork with the specific case $n=5$, $A = I_{5}$, $\\lambda = 2$, and\n$$\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nLet $B \\in \\mathbb{R}^{4 \\times 5}$ be the 1D difference matrix whose rows are $(-1, 1, 0, 0, 0)$, $(0, -1, 1, 0, 0)$, $(0, 0, -1, 1, 0)$, and $(0, 0, 0, -1, 1)$, so that $(B x)_{i} = x_{i+1} - x_{i}$ for $i=1,2,3,4$.\n\nStarting from first principles of convex optimization and the definition of subgradients, derive the Karush–Kuhn–Tucker (KKT) conditions for this problem, write them explicitly for the given $n=5$ instance, and use them to compute the TV-regularized estimator $\\widehat{x}$. Express the final estimator as an exact row vector with rational entries. No rounding is required, and no units are involved.",
            "solution": "The user-provided problem is a valid optimization problem grounded in the principles of compressed sensing and convex optimization. All parameters are well-defined, and the problem is well-posed.\n\nThe problem is to find the unique minimizer $\\widehat{x}$ of the objective function:\n$$\nF(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|B x\\|_{1}\n$$\nGiven the parameters $n=5$, $A=I_5$, $\\lambda=2$, and $y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$, the objective function becomes:\n$$\nF(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + 2 \\|B x\\|_{1}\n$$\nwhere $x \\in \\mathbb{R}^{5}$ and $B \\in \\mathbb{R}^{4 \\times 5}$ is the first-order forward difference matrix. The term $\\frac{1}{2}\\|x - y\\|_{2}^{2}$ is strictly convex and differentiable. The term $2\\|Bx\\|_{1}$ is convex but not differentiable everywhere. The sum $F(x)$ is therefore strictly convex, which guarantees the existence of a unique minimizer $\\widehat{x}$.\n\nThe first-order necessary and sufficient condition for a point $\\widehat{x}$ to be the minimizer of $F(x)$ is that the zero vector must be in the subdifferential of $F(x)$ at $\\widehat{x}$:\n$$\n0 \\in \\partial F(\\widehat{x})\n$$\nThe subdifferential of a sum of two convex functions is the sum of their subdifferentials (by the Moreau-Rockafellar theorem). The first term is differentiable, so its subdifferential is just its gradient.\n$$\n\\partial F(x) = \\nabla \\left(\\frac{1}{2}\\|x - y\\|_{2}^{2}\\right) + \\partial \\left(2\\|B x\\|_{1}\\right)\n$$\nThe gradient of the first term is $\\nabla \\left(\\frac{1}{2}\\|x-y\\|_2^2\\right) = x-y$.\nFor the second term, we use the chain rule for subdifferentials. If $h(x) = g(L(x))$ where $g(z) = 2\\|z\\|_1$ and $L(x) = Bx$, then $\\partial h(x) = L^T \\partial g(L(x))$. Here, $L^T = B^T$.\nThe subdifferential of $g(z) = 2\\|z\\|_1$ is a set of vectors $v \\in \\mathbb{R}^{4}$ such that for each component $v_i$:\n\\begin{itemize}\n    \\item $v_i = 2 \\cdot \\text{sign}(z_i)$ if $z_i \\neq 0$\n    \\item $v_i \\in [-2, 2]$ if $z_i = 0$\n\\end{itemize}\nwhere $z=Bx$. Combining these, the optimality condition $0 \\in \\partial F(\\widehat{x})$ becomes:\n$$\n0 \\in (\\widehat{x} - y) + B^T v\n$$\nwhere $v$ is a vector in $\\mathbb{R}^4$ whose components $v_i$ satisfy the conditions above with $z=B\\widehat{x}$. This can be written as:\n$$\ny - \\widehat{x} = B^T v\n$$\nThese are the Karush–Kuhn–Tucker (KKT) conditions for this problem. Let's write them explicitly for $n=5$. The matrix $B^T$ is:\n$$\nB^T = \\begin{pmatrix} -1  0  0  0 \\\\ 1  -1  0  0 \\\\ 0  1  -1  0 \\\\ 0  0  1  -1 \\\\ 0  0  0  1 \\end{pmatrix}\n$$\nThe equation $y - \\widehat{x} = B^T v$ component-wise is:\n\\begin{align*}\ny_1 - \\widehat{x}_1 = -v_1 \\\\\ny_2 - \\widehat{x}_2 = v_1 - v_2 \\\\\ny_3 - \\widehat{x}_3 = v_2 - v_3 \\\\\ny_4 - \\widehat{x}_4 = v_3 - v_4 \\\\\ny_5 - \\widehat{x}_5 = v_4\n\\end{align*}\nA key property of $B^T$ is that the sum of its columns is the zero vector, which implies that $\\mathbf{1}^T B^T = \\mathbf{0}^T$. Therefore, left-multiplying the KKT equation by $\\mathbf{1}^T$:\n$$\n\\mathbf{1}^T (y - \\widehat{x}) = \\mathbf{1}^T (B^T v) = (\\mathbf{1}^T B^T) v = \\mathbf{0}^T v = 0\n$$\nThis gives an important condition on the solution $\\widehat{x}$:\n$$\n\\sum_{i=1}^{5} (y_i - \\widehat{x}_i) = 0 \\implies \\sum_{i=1}^{5} \\widehat{x}_i = \\sum_{i=1}^{5} y_i\n$$\nThe sum of the components of the given data vector $y$ is $\\sum y_i = 1+2+2+1+0 = 6$. Thus, we must have $\\sum \\widehat{x}_i = 6$.\n\nThe nature of Total Variation regularization suggests that the solution $\\widehat{x}$ is piecewise constant. A simple hypothesis is that the solution is fully constant, i.e., $\\widehat{x}_i = c$ for all $i=1, \\dots, 5$.\nIf $\\widehat{x} = c \\cdot \\mathbf{1}$, then $\\sum \\widehat{x}_i = 5c$. From the condition above, $5c = 6$, which gives $c = \\frac{6}{5}$.\nSo, let's test the candidate solution $\\widehat{x} = (\\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5})^T$.\n\nFor this candidate, all differences are zero: $(B\\widehat{x})_i = \\widehat{x}_{i+1} - \\widehat{x}_i = 0$ for $i=1,2,3,4$.\nAccording to the KKT conditions, this requires the existence of a vector $v \\in \\mathbb{R}^4$ such that $y - \\widehat{x} = B^T v$ and its components satisfy $|v_i| \\le \\lambda=2$ for all $i$.\n\nLet's compute the residual vector $r = y - \\widehat{x}$:\n$$\nr = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 6/5 \\\\ 6/5 \\\\ 6/5 \\\\ 6/5 \\\\ 6/5 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/5 \\\\ 2 - 6/5 \\\\ 2 - 6/5 \\\\ 1 - 6/5 \\\\ 0 - 6/5 \\end{pmatrix} = \\begin{pmatrix} -1/5 \\\\ 4/5 \\\\ 4/5 \\\\ -1/5 \\\\ -6/5 \\end{pmatrix}\n$$\nNow we must solve for $v$ from the system $r = B^T v$:\n\\begin{align*}\n-v_1 = r_1 = -1/5 \\implies v_1 = 1/5 \\\\\nv_1 - v_2 = r_2 = 4/5 \\implies 1/5 - v_2 = 4/5 \\implies v_2 = -3/5 \\\\\nv_2 - v_3 = r_3 = 4/5 \\implies -3/5 - v_3 = 4/5 \\implies v_3 = -7/5 \\\\\nv_3 - v_4 = r_4 = -1/5 \\implies -7/5 - v_4 = -1/5 \\implies v_4 = -6/5 \\\\\nv_4 = r_5 = -6/5\n\\end{align*}\nThe system is consistent and yields a unique solution for $v$:\n$$\nv = \\begin{pmatrix} 1/5 \\\\ -3/5 \\\\ -7/5 \\\\ -6/5 \\end{pmatrix}\n$$\nThe final step is to check if this vector $v$ satisfies the subgradient condition, which for this case is $|v_i| \\le 2$ for all $i=1,2,3,4$.\n\\begin{itemize}\n    \\item $|v_1| = |1/5| = 1/5 \\le 2$\n    \\item $|v_2| = |-3/5| = 3/5 \\le 2$\n    \\item $|v_3| = |-7/5| = 7/5 \\le 2$\n    \\item $|v_4| = |-6/5| = 6/5 \\le 2$\n\\end{itemize}\nAll conditions are satisfied. Since we have found a pair $(\\widehat{x}, v)$ that satisfies the KKT conditions for this strictly convex problem, we have found the unique minimizer.\n\nThe TV-regularized estimator is $\\widehat{x} = (\\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5}, \\frac{6}{5})^T$. The problem asks for the answer as an exact row vector with rational entries.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{5}  \\frac{6}{5}  \\frac{6}{5}  \\frac{6}{5}  \\frac{6}{5} \\end{pmatrix}}\n$$"
        }
    ]
}