{
    "hands_on_practices": [
        {
            "introduction": "To move from abstract definitions to concrete understanding, we begin by exploring the geometry of the analysis sparsity model. This first practice uses the finite-difference operator, a common tool for analyzing signals, to illustrate how a signal's structure is defined by the null space of the analysis operator. By computing the subspaces corresponding to different cosupports, you will gain a hands-on appreciation for how the analysis model characterizes signals based on which parts of their transformed representation are zero .",
            "id": "3431186",
            "problem": "Consider the one-dimensional first-order forward finite-difference analysis operator $\\Omega \\in \\mathbb{R}^{3 \\times 4}$ defined by\n$$\n\\Omega \\triangleq \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  -1  1  0 \\\\\n0  0  -1  1\n\\end{pmatrix}.\n$$\nIn the analysis sparsity model, for a signal $x \\in \\mathbb{R}^{4}$ and an index set (cosupport) $S \\subset \\{1,2,3\\}$ of size $\\ell = 2$, the constraint $\\Omega_{S} x = 0$ specifies a linear subspace $\\ker(\\Omega_{S}) \\subset \\mathbb{R}^{4}$, where $\\Omega_{S}$ denotes the submatrix of $\\Omega$ formed by the rows indexed by $S$.\n\nUsing only the core definitions of analysis cosparsity (i.e., the notion that an analysis-sparse $x$ satisfies $\\Omega_{S} x = 0$ for its cosupport $S$) and standard linear algebra facts such as the rank-nullity theorem, proceed as follows:\n- Take the two cosupports $S_{1} = \\{1,2\\}$ and $S_{2} = \\{2,3\\}$, each of size $\\ell = 2$.\n- For each $S_{i}$, explicitly form $\\Omega_{S_{i}}$ and compute the corresponding subspace $\\ker(\\Omega_{S_{i}})$ by solving the linear constraints. Provide a basis for each subspace.\n- Finally, determine the dimension of the intersection $\\ker(\\Omega_{S_{1}}) \\cap \\ker(\\Omega_{S_{2}})$.\n\nReport the dimension of $\\ker(\\Omega_{S_{1}}) \\cap \\ker(\\Omega_{S_{2}})$ as your final answer. No units are required. The final answer must be a single number.",
            "solution": "The problem requires us to determine the dimension of the intersection of two subspaces, $\\ker(\\Omega_{S_{1}})$ and $\\ker(\\Omega_{S_{2}})$, where $\\Omega_{S_{i}}$ are submatrices of a given analysis operator $\\Omega$. The analysis operator is $\\Omega \\in \\mathbb{R}^{3 \\times 4}$ defined as:\n$$\n\\Omega \\triangleq \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  -1  1  0 \\\\\n0  0  -1  1\n\\end{pmatrix}\n$$\nThe two cosupports are given as $S_{1} = \\{1,2\\}$ and $S_{2} = \\{2,3\\}$.\n\nFirst, we will construct the submatrix $\\Omega_{S_{1}}$ and characterize its kernel, $\\ker(\\Omega_{S_{1}})$. The cosupport $S_{1} = \\{1,2\\}$ indicates that we take the first and second rows of $\\Omega$.\n$$\n\\Omega_{S_{1}} = \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  -1  1  0\n\\end{pmatrix}\n$$\nThe kernel of $\\Omega_{S_{1}}$ is the set of all vectors $x = (x_1, x_2, x_3, x_4)^T \\in \\mathbb{R}^{4}$ such that $\\Omega_{S_{1}} x = 0$. This gives us the following system of linear equations:\n\\begin{align*}\n-x_1 + x_2 = 0 \\\\\n-x_2 + x_3 = 0\n\\end{align*}\nFrom the first equation, we get $x_1 = x_2$. From the second equation, we get $x_2 = x_3$. Combining these, we have the condition $x_1 = x_2 = x_3$. The variable $x_4$ is not constrained by these equations, so it is a free variable. Let $x_1 = x_2 = x_3 = a$ and $x_4 = b$, where $a, b \\in \\mathbb{R}$. Any vector $x \\in \\ker(\\Omega_{S_{1}})$ can be written as:\n$$\nx = \\begin{pmatrix} a \\\\ a \\\\ a \\\\ b \\end{pmatrix} = a \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix} + b \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThus, a basis for $\\ker(\\Omega_{S_{1}})$ is $\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\right\\}$. The dimension of $\\ker(\\Omega_{S_{1}})$ is $2$.\n\nNext, we construct the submatrix $\\Omega_{S_{2}}$ and characterize its kernel, $\\ker(\\Omega_{S_{2}})$. The cosupport $S_{2} = \\{2,3\\}$ indicates that we take the second and third rows of $\\Omega$.\n$$\n\\Omega_{S_{2}} = \\begin{pmatrix}\n0  -1  1  0 \\\\\n0  0  -1  1\n\\end{pmatrix}\n$$\nThe kernel of $\\Omega_{S_{2}}$ is the set of all vectors $x = (x_1, x_2, x_3, x_4)^T \\in \\mathbb{R}^{4}$ such that $\\Omega_{S_{2}} x = 0$. This gives us the system:\n\\begin{align*}\n-x_2 + x_3 = 0 \\\\\n-x_3 + x_4 = 0\n\\end{align*}\nFrom the first equation, $x_2 = x_3$. From the second, $x_3 = x_4$. Combining these gives the condition $x_2 = x_3 = x_4$. The variable $x_1$ is unconstrained and thus free. Let $x_2 = x_3 = x_4 = c$ and $x_1 = d$, where $c, d \\in \\mathbb{R}$. Any vector $x \\in \\ker(\\Omega_{S_{2}})$ can be written as:\n$$\nx = \\begin{pmatrix} d \\\\ c \\\\ c \\\\ c \\end{pmatrix} = c \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} + d \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThus, a basis for $\\ker(\\Omega_{S_{2}})$ is $\\left\\{ \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\}$. The dimension of $\\ker(\\Omega_{S_{2}})$ is $2$.\n\nFinally, we need to find the dimension of the intersection $\\ker(\\Omega_{S_{1}}) \\cap \\ker(\\Omega_{S_{2}})$. A vector $x$ lies in this intersection if and only if it satisfies the conditions for membership in both kernels.\nFor $x \\in \\ker(\\Omega_{S_{1}})$, we must have $x_1 = x_2 = x_3$.\nFor $x \\in \\ker(\\Omega_{S_{2}})$, we must have $x_2 = x_3 = x_4$.\nFor a vector to be in the intersection, it must satisfy both sets of conditions simultaneously. Combining them, we get:\n$$\nx_1 = x_2 = x_3 = x_4\n$$\nLet this common value be $a \\in \\mathbb{R}$. Then any vector $x$ in the intersection has the form:\n$$\nx = \\begin{pmatrix} a \\\\ a \\\\ a \\\\ a \\end{pmatrix} = a \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe intersection subspace is spanned by the single nonzero vector $\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. Therefore, a basis for $\\ker(\\Omega_{S_{1}}) \\cap \\ker(\\Omega_{S_{2}})$ is $\\left\\{ \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right\\}$.\nThe dimension of a vector space is the number of vectors in its basis. In this case, the basis contains one vector.\n\nAlternatively, the intersection of the two kernels is the kernel of the matrix formed by stacking the rows of $\\Omega_{S_{1}}$ and $\\Omega_{S_{2}}$ and removing duplicates. The rows of $\\Omega_{S_{1}}$ are rows $1$ and $2$ of $\\Omega$. The rows of $\\Omega_{S_{2}}$ are rows $2$ and $3$ of $\\Omega$. The set of unique rows is $\\{1, 2, 3\\}$, which constitutes the original matrix $\\Omega$. Therefore, $\\ker(\\Omega_{S_{1}}) \\cap \\ker(\\Omega_{S_{2}}) = \\ker(\\Omega)$.\nThe rank of $\\Omega$ is $3$, as its rows are linearly independent. By the rank-nullity theorem, for a matrix in $\\mathbb{R}^{m \\times n}$, we have $\\operatorname{rank}(\\Omega) + \\dim(\\ker(\\Omega)) = n$. Here, $n=4$.\n$$\n\\dim(\\ker(\\Omega)) = 4 - \\operatorname{rank}(\\Omega) = 4 - 3 = 1\n$$\nBoth methods lead to the same conclusion. The dimension of the intersection $\\ker(\\Omega_{S_{1}}) \\cap \\ker(\\Omega_{S_{2}})$ is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Next, we turn our attention to a fundamental characteristic of the synthesis model: the potential for non-uniqueness. When a synthesis dictionary is overcomplete or contains linearly dependent atoms, a single signal can have multiple different sparse representations. This exercise guides you through a scenario with a redundant dictionary, demonstrating how to find the entire family of sparse coefficients that generate the same signal and revealing the critical role of dictionary properties in ensuring a unique solution .",
            "id": "3431233",
            "problem": "Consider the synthesis model in compressed sensing, where a signal $y \\in \\mathbb{R}^{m}$ is written as $y = D x$ with a dictionary $D \\in \\mathbb{R}^{m \\times n}$ and a coefficient vector $x \\in \\mathbb{R}^{n}$. Let the dictionary $D \\in \\mathbb{R}^{3 \\times 4}$ be\n$$\nD \\;=\\; \\begin{bmatrix}\n1  0  1  1 \\\\\n0  1  1  -1 \\\\\n1  1  2  0\n\\end{bmatrix},\n$$\nwith columns $d_{1}, d_{2}, d_{3}, d_{4}$ corresponding to the $4$ columns of $D$. Let the support set be $S = \\{1,2,3\\}$ and the sparsity level be $s = 3$. Consider the signal $y \\in \\mathbb{R}^{3}$ defined by $y = D x^{\\star}$ for the coefficient vector\n$$\nx^{\\star} \\;=\\; \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nUsing only the basic definitions of the synthesis model, linear dependence, rank, nullspace, and the rank-nullity theorem, do the following:\n\n1. Verify that the columns of $D$ indexed by $S$ are linearly dependent and determine $\\operatorname{rank}(D_{S})$, where $D_{S}$ denotes the submatrix of $D$ with columns in $S$.\n\n2. Write the general form of all synthesis coefficient vectors $x \\in \\mathbb{R}^{4}$ with support contained in $S$ (that is, with $x_{4} = 0$) that satisfy $D x = y$, and argue how the existence of a nontrivial nullspace for $D_{S}$ undermines uniqueness of the $s$-sparse synthesis representation.\n\n3. Provide the dimension of the affine solution set\n$$\n\\mathcal{X}_{S}(y) \\;=\\; \\left\\{ x \\in \\mathbb{R}^{4} \\,:\\, D x = y,\\; \\operatorname{supp}(x) \\subseteq S \\right\\}.\n$$\n\nIn your discussion for item $2$, briefly contrast this failure of uniqueness in the synthesis model with how an appropriately chosen analysis operator in the analysis model could, in principle, still yield uniqueness for $y$ under co-sparsity constraints, without invoking any specialized theorems beyond the core definitions.\n\nYour final answer must be the single real number equal to the dimension of $\\mathcal{X}_{S}(y)$. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded in the principles of linear algebra and sparse representation theory, and contains all necessary information to derive a unique solution.\n\nThe synthesis model is given by $y = D x$, where $y \\in \\mathbb{R}^{3}$ is the signal, $D \\in \\mathbb{R}^{3 \\times 4}$ is the dictionary, and $x \\in \\mathbb{R}^{4}$ is the coefficient vector. The dictionary is\n$$\nD \\;=\\; \\begin{bmatrix}\n1  0  1  1 \\\\\n0  1  1  -1 \\\\\n1  1  2  0\n\\end{bmatrix}\n$$\nwith columns denoted $d_1, d_2, d_3, d_4$. The support set is $S = \\{1,2,3\\}$ and the sparsity level is $s = |S| = 3$. The signal $y$ is generated by $x^{\\star} = \\begin{bmatrix} 2  1  0  0 \\end{bmatrix}^T$.\n\nFirst, we calculate the signal $y$:\n$$\ny = D x^{\\star} = \\begin{bmatrix}\n1  0  1  1 \\\\\n0  1  1  -1 \\\\\n1  1  2  0\n\\end{bmatrix}\n\\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = 2 d_1 + 1 d_2 = 2 \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} + 1 \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}.\n$$\n\n1. We are asked to verify the linear dependence of the columns of $D$ indexed by $S = \\{1,2,3\\}$ and find the rank of the corresponding submatrix $D_S$. The submatrix $D_S$ is formed by the first three columns of $D$:\n$$\nD_S = \\begin{bmatrix} d_1  d_2  d_3 \\end{bmatrix} = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  2\n\\end{bmatrix}.\n$$\nTo check for linear dependence, we inspect the columns. We can observe that the third column is the sum of the first two columns: $d_3 = d_1 + d_2$. This can be written as a non-trivial linear combination of the columns that equals the zero vector:\n$$\n1 \\cdot d_1 + 1 \\cdot d_2 - 1 \\cdot d_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nSince there exists a non-trivial linear combination of the columns of $D_S$ that results in the zero vector, the columns $\\{d_1, d_2, d_3\\}$ are linearly dependent.\n\nTo determine the rank of $D_S$, we find the size of the largest set of linearly independent columns. The columns $d_1 = \\begin{bmatrix} 1  0  1 \\end{bmatrix}^T$ and $d_2 = \\begin{bmatrix} 0  1  1 \\end{bmatrix}^T$ are not scalar multiples of each other, so they are linearly independent. Since $d_3$ is a linear combination of $d_1$ and $d_2$, the span of $\\{d_1, d_2, d_3\\}$ is the same as the span of $\\{d_1, d_2\\}$. The dimension of this span, which is the rank of $D_S$, is $2$.\n$$\n\\operatorname{rank}(D_S) = 2.\n$$\n\n2. We need to find the general form of all synthesis coefficient vectors $x \\in \\mathbb{R}^4$ with support contained in $S$ that satisfy $D x = y$. The condition $\\operatorname{supp}(x) \\subseteq S = \\{1,2,3\\}$ implies that $x_4 = 0$. The equation $Dx = y$ thus reduces to $D_S x_S = y$, where $x_S = \\begin{bmatrix} x_1  x_2  x_3 \\end{bmatrix}^T$. This is a system of linear equations:\n$$\n\\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  2\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\end{bmatrix}.\n$$\nWe solve this system by forming the augmented matrix $[D_S | y]$ and performing Gaussian elimination:\n$$\n\\left[\\begin{array}{ccc|c} 1  0  1  2 \\\\ 0  1  1  1 \\\\ 1  1  2  3 \\end{array}\\right] \\xrightarrow{R_3 \\to R_3-R_1} \\left[\\begin{array}{ccc|c} 1  0  1  2 \\\\ 0  1  1  1 \\\\ 0  1  1  1 \\end{array}\\right] \\xrightarrow{R_3 \\to R_3-R_2} \\left[\\begin{array}{ccc|c} 1  0  1  2 \\\\ 0  1  1  1 \\\\ 0  0  0  0 \\end{array}\\right].\n$$\nThe reduced system is:\n$$\nx_1 + x_3 = 2 \\\\\nx_2 + x_3 = 1\n$$\nLet $x_3 = \\alpha$ be a free parameter, where $\\alpha \\in \\mathbb{R}$. Then $x_1 = 2 - \\alpha$ and $x_2 = 1 - \\alpha$. The general solution for $x_S$ is\n$$\nx_S = \\begin{bmatrix} 2 - \\alpha \\\\ 1 - \\alpha \\\\ \\alpha \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\end{bmatrix} + \\alpha \\begin{bmatrix} -1 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n$$\nThe general form for $x \\in \\mathbb{R}^4$ with $\\operatorname{supp}(x) \\subseteq S$ is therefore\n$$\nx(\\alpha) = \\begin{bmatrix} 2 - \\alpha \\\\ 1 - \\alpha \\\\ \\alpha \\\\ 0 \\end{bmatrix}.\n$$\nThe existence of a non-trivial nullspace for $D_S$ is the root cause of the non-uniqueness of the $s$-sparse synthesis representation. From our analysis of $D_S$, we saw that $d_1+d_2-d_3=0$, which implies that the vector $z = \\begin{bmatrix} 1  1  -1 \\end{bmatrix}^T$ is in the nullspace of $D_S$. Any multiple of this vector, such as $\\begin{bmatrix} -1  -1  1 \\end{bmatrix}^T$, is also in the nullspace. The general solution is a sum of a particular solution (e.g., $x_S_p = \\begin{bmatrix} 2  1  0 \\end{bmatrix}^T$) and any vector from $\\mathcal{N}(D_S)$. Since $\\mathcal{N}(D_S)$ is non-trivial, there are infinitely many vectors $x_S$ that generate the same signal $y$. For example, for $\\alpha=0$, we get $x = \\begin{bmatrix} 2  1  0  0 \\end{bmatrix}^T$ (which is $x^\\star$, a $2$-sparse vector). For $\\alpha=1$, we get $x = \\begin{bmatrix} 1  0  1  0 \\end{bmatrix}^T$ (another $2$-sparse vector). Since multiple sparse vectors generate the same signal $y$, the sparse representation is not unique.\n\nThis failure of uniqueness in the synthesis model stems from the redundancy of the dictionary $D$ and, more specifically, the linear dependence of the columns in the sub-dictionary $D_S$. In contrast, the analysis model characterizes a signal $y$ via an analysis operator $\\Omega$, yielding a coefficient vector $z = \\Omega y$. For a given signal $y$, the analysis vector $z$ is always unique. The question of uniqueness in the analysis framework typically arises when recovering $y$ from incomplete measurements. However, in terms of representation alone, if one chooses an analysis operator $\\Omega$ that is a basis for $\\mathbb{R}^m$ (i.e., an invertible $m \\times m$ matrix), then the mapping from a signal $y$ to its representation $z = \\Omega y$ is one-to-one. In this case, every signal $y$ has a unique representation $z$, and the non-uniqueness issue seen in the overcomplete synthesis model does not arise. The primary challenge in the analysis model becomes finding an operator $\\Omega$ for which $z$ is sparse for the signals of interest.\n\n3. The affine solution set is given by\n$$\n\\mathcal{X}_{S}(y) \\;=\\; \\left\\{ x \\in \\mathbb{R}^{4} \\,:\\, D x = y,\\; \\operatorname{supp}(x) \\subseteq S \\right\\}.\n$$\nAs shown in part $2$, any element $x \\in \\mathcal{X}_{S}(y)$ is of the form $x(\\alpha) = \\begin{bmatrix} 2-\\alpha  1-\\alpha  \\alpha  0 \\end{bmatrix}^T$. This set can be written as\n$$\n\\mathcal{X}_{S}(y) = \\left\\{ \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + \\alpha \\begin{bmatrix} -1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\,:\\, \\alpha \\in \\mathbb{R} \\right\\}.\n$$\nThis is an affine subspace of $\\mathbb{R}^4$. The dimension of an affine subspace is defined as the dimension of its associated vector subspace. Here, the associated vector subspace is the one spanned by the direction vector, which is $\\operatorname{span}\\left( \\begin{bmatrix} -1  -1  1  0 \\end{bmatrix}^T \\right)$. This is a $1$-dimensional subspace.\n\nAlternatively, the dimension of the affine solution set to $D_S x_S = y$ is equal to the dimension of the nullspace of $D_S$, denoted $\\dim(\\mathcal{N}(D_S))$. By the rank-nullity theorem,\n$$\n\\dim(\\mathcal{N}(D_S)) = (\\text{number of columns of } D_S) - \\operatorname{rank}(D_S).\n$$\nThe matrix $D_S$ has $3$ columns and, as we found in part $1$, its rank is $2$.\n$$\n\\dim(\\mathcal{N}(D_S)) = 3 - 2 = 1.\n$$\nThe dimension of the affine solution set $\\mathcal{X}_S(y)$ is therefore $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "This final practice serves as a capstone, directly connecting the synthesis and analysis frameworks within a single problem. You will construct signals using the synthesis model and then evaluate them using analysis tools, calculating the analysis residual to see how \"cosparse\" they are with respect to a different structure. This exercise highlights the duality between the two models by showing an instance where a synthesis subspace, $\\operatorname{span}(D_S)$, is equivalent to an analysis subspace, $\\ker(\\Omega_T)$, solidifying the deep connection between these two perspectives on sparsity .",
            "id": "3431224",
            "problem": "Consider a synthesis dictionary $D \\in \\mathbb{R}^{4 \\times 6}$ with columns $d_1, d_2, d_3, d_4, d_5, d_6 \\in \\mathbb{R}^{4}$ given by\n$$\nd_1 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix},\\quad\nd_2 = \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix},\\quad\nd_3 = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix},\\quad\nd_4 = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nd_5 = \\begin{pmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nd_6 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{pmatrix}.\n$$\nLet the support sets be $S_1 = \\{1,3\\}$ and $S_2 = \\{4,6\\}$, which are disjoint and each has size $2$. In the synthesis model, a signal $x \\in \\mathbb{R}^{4}$ belongs to the subspace $\\operatorname{span}(D_{S})$ if it can be written as $x = D_S \\alpha$ for some coefficient vector $\\alpha \\in \\mathbb{R}^{|S|}$. In the analysis model, given an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times 4}$, a signal is cosparse with cosupport $T \\subseteq \\{1,\\dots,p\\}$ when $\\Omega_T x = 0$, and it then lies in the subspace $\\ker(\\Omega_T)$.\n\nUsing these core definitions, do the following:\n- Explicitly construct vectors $x_1 \\in \\operatorname{span}(D_{S_1})$ and $x_2 \\in \\operatorname{span}(D_{S_2})$ by taking $x_1 = D_{S_1} \\alpha_1$ with $\\alpha_1 = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$ and $x_2 = D_{S_2} \\alpha_2$ with $\\alpha_2 = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$.\n- Verify each subspace membership by computing the orthogonal projectors onto $\\operatorname{span}(D_{S_1})$ and $\\operatorname{span}(D_{S_2})$, respectively, and showing that applying the appropriate projector to $x_1$ and $x_2$ returns the original vector. Use only the foundational projector identity $P_S = D_S (D_S^{\\top} D_S)^{-1} D_S^{\\top}$ when the columns of $D_S$ are linearly independent.\n- Let the analysis operator be the identity matrix $\\Omega = I_4 \\in \\mathbb{R}^{4 \\times 4}$, and take the cosupport $T_1 = \\{3,4\\}$. Interpret $\\ker(\\Omega_{T_1})$ and relate it to $\\operatorname{span}(D_{S_1})$ using first principles.\n\nFinally, compute the squared Euclidean norm of the analysis residual for $x_2$ with respect to $T_1$, namely the value of $\\|\\Omega_{T_1} x_2\\|_2^2$. Your final answer must be a single real number. No rounding is required.",
            "solution": "The problem is first validated against the established criteria.\n\n### Step 1: Extract Givens\n- Synthesis dictionary: $D \\in \\mathbb{R}^{4 \\times 6}$\n- Columns of $D$:\n$$\nd_1 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0\\end{pmatrix},\\quad\nd_2 = \\begin{pmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix},\\quad\nd_3 = \\begin{pmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix},\\quad\nd_4 = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nd_5 = \\begin{pmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nd_6 = \\begin{pmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1\\end{pmatrix}\n$$\n- Support sets: $S_1 = \\{1,3\\}$ and $S_2 = \\{4,6\\}$.\n- Synthesis model definition: $x = D_S \\alpha$ for $x \\in \\operatorname{span}(D_{S})$.\n- Analysis model definition: $\\Omega_T x = 0$ for a cosparse signal $x \\in \\ker(\\Omega_T)$.\n- Vectors to construct: $x_1 = D_{S_1} \\alpha_1$ with $\\alpha_1 = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$ and $x_2 = D_{S_2} \\alpha_2$ with $\\alpha_2 = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$.\n- Orthogonal projector formula: $P_S = D_S (D_S^{\\top} D_S)^{-1} D_S^{\\top}$, given that the columns of $D_S$ are linearly independent.\n- Analysis operator: $\\Omega = I_4 \\in \\mathbb{R}^{4 \\times 4}$.\n- Cosupport: $T_1 = \\{3,4\\}$.\n- Task: Compute $\\|\\Omega_{T_1} x_2\\|_2^2$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the theory of sparse representations and compressed sensing, using standard definitions for synthesis and analysis models, dictionaries, support, and cosupport.\n- **Well-Posed**: The problem provides all necessary data and definitions. The tasks are specific, computational, and lead to unique, verifiable results.\n- **Objective**: The problem is stated using precise mathematical language and definitions, free of any subjectivity.\n- **Completeness and Consistency**: The dimensions of all matrices and vectors are consistent. The definitions are standard and not contradictory.\n- **Feasibility**: All required computations are mathematically sound and feasible. The prerequisite for using the projector formula (linear independence of sub-dictionary columns) is a property to be verified, which is a standard part of such a problem.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds by following the tasks outlined in the problem statement.\n\nFirst, we construct the vectors $x_1$ and $x_2$.\nFor $x_1$, we use the support set $S_1 = \\{1,3\\}$. The sub-dictionary $D_{S_1}$ is formed by columns $d_1$ and $d_3$ of $D$.\n$$\nD_{S_1} = \\begin{pmatrix} d_1  d_3 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\nThe vector $x_1$ is computed using the given coefficient vector $\\alpha_1$:\n$$\nx_1 = D_{S_1} \\alpha_1 = \\begin{pmatrix} 1  1 \\\\ 0  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(1) + (-1)(1) \\\\ 2(0) + (-1)(1) \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFor $x_2$, we use the support set $S_2 = \\{4,6\\}$. The sub-dictionary $D_{S_2}$ is formed by columns $d_4$ and $d_6$ of $D$.\n$$\nD_{S_2} = \\begin{pmatrix} d_4  d_6 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe vector $x_2$ is computed using the given coefficient vector $\\alpha_2$:\n$$\nx_2 = D_{S_2} \\alpha_2 = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2(0) + (-1)(0) \\\\ 2(1) + (-1)(0) \\\\ 2(1) + (-1)(0) \\\\ 2(0) + (-1)(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ -1 \\end{pmatrix}\n$$\n\nNext, we verify the subspace memberships by constructing the orthogonal projectors.\nFor $S_1$, we first check if the columns of $D_{S_1}$ are linearly independent. The columns are $\\begin{pmatrix}1000\\end{pmatrix}^{\\top}$ and $\\begin{pmatrix}1100\\end{pmatrix}^{\\top}$, which are clearly linearly independent. We compute the projector $P_{S_1} = D_{S_1} (D_{S_1}^{\\top} D_{S_1})^{-1} D_{S_1}^{\\top}$.\n$$\nD_{S_1}^{\\top} D_{S_1} = \\begin{pmatrix} 1  0  0  0 \\\\ 1  1  0  0 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}\n$$\nThe inverse is:\n$$\n(D_{S_1}^{\\top} D_{S_1})^{-1} = \\frac{1}{(1)(2) - (1)(1)} \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix}\n$$\nNow, we find the projector $P_{S_1}$:\n$$\nP_{S_1} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0  0 \\\\ 1  1  0  0 \\end{pmatrix} =  \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0  0 \\\\ 1  1  0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}\n$$\nApplying this projector to $x_1$:\n$$\nP_{S_1} x_1 = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = x_1\n$$\nThis confirms that $x_1 \\in \\operatorname{span}(D_{S_1})$.\n\nFor $S_2$, the columns of $D_{S_2}$ are $\\begin{pmatrix}0110\\end{pmatrix}^{\\top}$ and $\\begin{pmatrix}0001\\end{pmatrix}^{\\top}$, which are linearly independent. We compute the projector $P_{S_2} = D_{S_2} (D_{S_2}^{\\top} D_{S_2})^{-1} D_{S_2}^{\\top}$.\n$$\nD_{S_2}^{\\top} D_{S_2} = \\begin{pmatrix} 0  1  1  0 \\\\ 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe inverse is:\n$$\n(D_{S_2}^{\\top} D_{S_2})^{-1} = \\begin{pmatrix} 1/2  0 \\\\ 0  1 \\end{pmatrix}\n$$\nNow, we find the projector $P_{S_2}$:\n$$\nP_{S_2} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\\\ 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1/2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0  1  1  0 \\\\ 0  0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 1/2  0 \\\\ 1/2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0  1  1  0 \\\\ 0  0  0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  0 \\\\ 0  1/2  1/2  0 \\\\ 0  1/2  1/2  0 \\\\ 0  0  0  1 \\end{pmatrix}\n$$\nApplying this projector to $x_2$:\n$$\nP_{S_2} x_2 = \\begin{pmatrix} 0  0  0  0 \\\\ 0  1/2  1/2  0 \\\\ 0  1/2  1/2  0 \\\\ 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ (1/2)(2) + (1/2)(2) \\\\ (1/2)(2) + (1/2)(2) \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ -1 \\end{pmatrix} = x_2\n$$\nThis confirms that $x_2 \\in \\operatorname{span}(D_{S_2})$.\n\nNext, we relate the analysis and synthesis subspaces. The analysis operator is $\\Omega = I_4$, the $4 \\times 4$ identity matrix. The cosupport is $T_1 = \\{3,4\\}$. The sub-operator $\\Omega_{T_1}$ consists of the 3rd and 4th rows of $I_4$:\n$$\n\\Omega_{T_1} = \\begin{pmatrix} 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix}\n$$\nThe analysis subspace is $\\ker(\\Omega_{T_1})$. A vector $x = \\begin{pmatrix} x^{(1)}  x^{(2)}  x^{(3)}  x^{(4)} \\end{pmatrix}^{\\top}$ lies in this kernel if $\\Omega_{T_1} x = 0$.\n$$\n\\Omega_{T_1} x = \\begin{pmatrix} x^{(3)} \\\\ x^{(4)} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis implies that any vector in $\\ker(\\Omega_{T_1})$ must be of the form $\\begin{pmatrix} x^{(1)}  x^{(2)}  0  0 \\end{pmatrix}^{\\top}$. This is the subspace spanned by the standard basis vectors $e_1 = \\begin{pmatrix}1000\\end{pmatrix}^{\\top}$ and $e_2 = \\begin{pmatrix}0100\\end{pmatrix}^{\\top}$.\n\nNow we consider the synthesis subspace $\\operatorname{span}(D_{S_1})$. The basis vectors for this subspace are $d_1 = \\begin{pmatrix}1000\\end{pmatrix}^{\\top}$ and $d_3 = \\begin{pmatrix}1100\\end{pmatrix}^{\\top}$. Any vector in this span is a linear combination $c_1 d_1 + c_2 d_3$:\n$$\nc_1 \\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix} + c_2 \\begin{pmatrix}1\\\\1\\\\0\\\\0\\end{pmatrix} = \\begin{pmatrix}c_1+c_2 \\\\ c_2 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nSince $c_1$ and $c_2$ can be any real numbers, any vector of the form $\\begin{pmatrix}ab00\\end{pmatrix}^{\\top}$ can be represented by choosing $c_2 = b$ and $c_1 = a-b$. This shows that $\\operatorname{span}(D_{S_1})$ is also the subspace of vectors whose last two components are zero. Therefore, $\\ker(\\Omega_{T_1}) = \\operatorname{span}(D_{S_1})$. This demonstrates an instance where a synthesis-sparse model with dictionary $D$ and support $S_1$ is equivalent to an analysis-cosparse model with operator $\\Omega=I_4$ and cosupport $T_1$.\n\nFinally, we compute the squared Euclidean norm of the analysis residual for $x_2$ with respect to $T_1$. The residual is $\\Omega_{T_1} x_2$.\n$$\n\\Omega_{T_1} x_2 = \\begin{pmatrix} 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (0)(0)+(0)(2)+(1)(2)+(0)(-1) \\\\ (0)(0)+(0)(2)+(0)(2)+(1)(-1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nThe squared Euclidean norm is:\n$$\n\\|\\Omega_{T_1} x_2\\|_2^2 = \\left\\| \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right\\|_2^2 = (2)^2 + (-1)^2 = 4 + 1 = 5\n$$\nThe value indicates how much $x_2$ fails to be cosparse with respect to the analysis operator $\\Omega$ and cosupport $T_1$. Since the result is non-zero, $x_2$ is not in $\\ker(\\Omega_{T_1})$.",
            "answer": "$$\\boxed{5}$$"
        }
    ]
}