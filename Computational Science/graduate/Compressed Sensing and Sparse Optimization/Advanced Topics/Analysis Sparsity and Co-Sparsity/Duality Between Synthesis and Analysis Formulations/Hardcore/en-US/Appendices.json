{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the synthesis model, it's essential to work through a simple case from first principles. This first exercise  presents a small-scale synthesis Basis Pursuit (BP) problem. While the primal solution can be found by simple inspection, the core of the task is to formally certify its optimality using the powerful framework of Lagrangian duality, providing a concrete look at the conditions that guarantee a solution is not just feasible, but optimal.",
            "id": "3445067",
            "problem": "Consider the synthesis and analysis viewpoints in compressed sensing and sparse optimization. In the synthesis model, a signal $x \\in \\mathbb{R}^{n}$ is represented as $x = D z$ where $D \\in \\mathbb{R}^{n \\times p}$ is a synthesis dictionary and $z \\in \\mathbb{R}^{p}$ is a coefficient vector expected to be sparse. Given noiseless measurements $y \\in \\mathbb{R}^{m}$ and a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, the synthesis Basis Pursuit (BP) problem is to find $z$ with minimal $\\ell_{1}$-norm such that the consistency constraint $A D z = y$ holds. In the analysis model, an analysis operator $\\Omega \\in \\mathbb{R}^{q \\times n}$ acts on $x$ to produce $\\Omega x$ expected to be sparse; the analysis BP minimizes the $\\ell_{1}$-norm of $\\Omega x$ under the measurement constraints.\n\nWork in the concrete setting with $n = 2$, $m = 2$, $p = 2$, $A = I \\in \\mathbb{R}^{2 \\times 2}$, $D = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}$, and $y = (1, 1)^{\\top} \\in \\mathbb{R}^{2}$. Starting from the definition of synthesis BP as the constrained minimization of the $\\ell_{1}$-norm of $z$ subject to the linear consistency constraint, derive the synthesis BP solution $z^{\\star} \\in \\mathbb{R}^{2}$ and the corresponding signal $x^{\\star} = D z^{\\star} \\in \\mathbb{R}^{2}$. Your derivation should be grounded in first principles, including feasibility and optimality certification via Lagrangian duality, and should make explicit the relationship to the analysis viewpoint through the dual feasibility condition. Express the final answer as the pair $(z^{\\star}, x^{\\star})$. No rounding is required, and no physical units are involved.",
            "solution": "The problem asks for the solution $(z^{\\star}, x^{\\star})$ to a specific instance of the synthesis Basis Pursuit (BP) problem, with a derivation based on first principles of Lagrangian duality and an explicit connection to the analysis viewpoint.\n\nThe synthesis BP problem is formulated as the minimization of the $\\ell_1$-norm of a coefficient vector $z$ subject to a linear consistency constraint:\n$$ \\min_{z \\in \\mathbb{R}^{p}} \\|z\\|_1 \\quad \\text{subject to} \\quad ADz = y $$\n\nFirst, we substitute the specified values for the matrices and vectors:\n- $n = 2$, $m = 2$, $p = 2$\n- Sensing matrix: $A = I \\in \\mathbb{R}^{2 \\times 2}$ (the identity matrix)\n- Synthesis dictionary: $D = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- Measurement vector: $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^{2}$\n\nThe constraint $ADz = y$ simplifies to $Iz = Dz = y$. The primal optimization problem is therefore:\n$$ \\min_{z = (z_1, z_2)^{\\top} \\in \\mathbb{R}^{2}} |z_1| + |z_2| \\quad \\text{subject to} \\quad \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\nThe constraint represents a system of two linear equations in two variables:\n1. $z_1 + z_2 = 1$\n2. $z_2 = 1$\n\nFrom the second equation, we have $z_2 = 1$. Substituting this into the first equation gives $z_1 + 1 = 1$, which implies $z_1 = 0$.\nThus, the feasible set for the optimization problem consists of a single point, $z = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. Since this is the only feasible point, it must be the solution to the minimization problem. We denote this primal solution as:\n$$ z^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\n\nThe corresponding signal $x^{\\star}$ is found by applying the synthesis dictionary $D$:\n$$ x^{\\star} = D z^{\\star} = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\nTo formally validate this solution and fulfill the problem's requirements, we use the framework of Lagrangian duality. The Lagrangian for the primal problem is:\n$$ \\mathcal{L}(z, \\nu) = \\|z\\|_1 + \\nu^{\\top}(y - Dz) $$\nwhere $\\nu \\in \\mathbb{R}^2$ is the Lagrange multiplier (or dual variable) associated with the equality constraint.\n\nThe Karush-Kuhn-Tucker (KKT) conditions provide necessary and sufficient conditions for optimality for this convex problem. A pair $(z^{\\star}, \\nu^{\\star})$ is optimal if it satisfies:\n1.  **Primal feasibility**: $Dz^{\\star} = y$.\n    As shown above, $z^{\\star} = (0, 1)^{\\top}$ satisfies this condition: $D z^{\\star} = (1, 1)^{\\top} = y$.\n2.  **Stationarity**: The subgradient of the Lagrangian with respect to $z$ at $z^{\\star}$ must contain the zero vector. This implies $0 \\in \\partial \\|z^\\star\\|_1 - D^{\\top}\\nu^{\\star}$, or equivalently, $D^{\\top}\\nu^{\\star} \\in \\partial \\|z^\\star\\|_1$.\n    The subgradient of the $\\ell_1$-norm $\\|z\\|_1 = |z_1| + |z_2|$ at $z^{\\star} = (0, 1)^{\\top}$ is the set of vectors $s=(s_1, s_2)^{\\top}$ where $s_1 \\in [-1, 1]$ and $s_2 = \\text{sign}(z_2^{\\star}) = 1$.\n    So, we must find a dual vector $\\nu^{\\star}$ such that $D^{\\top}\\nu^{\\star} = s$ for some $s$ with $s_1 \\in [-1, 1]$ and $s_2 = 1$.\n    The transpose of $D$ is $D^{\\top} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$. The condition is:\n    $$ \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} \\nu_1^{\\star} \\\\ \\nu_2^{\\star} \\end{bmatrix} = \\begin{bmatrix} s_1 \\\\ 1 \\end{bmatrix} $$\n    This yields the system $\\nu_1^{\\star} = s_1$ and $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$.\n3.  **Dual feasibility**: The dual variable $\\nu^{\\star}$ must be feasible for the dual problem. The dual problem is $\\max_{\\nu} y^{\\top}\\nu$ subject to $\\|D^{\\top}\\nu\\|_{\\infty} \\le 1$. So the condition is $\\|D^{\\top}\\nu^{\\star}\\|_{\\infty} \\le 1$.\n    From the stationarity condition, we have $D^{\\top}\\nu^{\\star} = (s_1, 1)^{\\top}$. Thus, the dual feasibility condition is $\\|(s_1, 1)^{\\top}\\|_{\\infty} = \\max(|s_1|, |1|) \\le 1$. Since we require $s_1 \\in [-1, 1]$, $|s_1| \\le 1$, and therefore $\\max(|s_1|, 1) = 1$. The condition is satisfied for any choice of $s_1 \\in [-1, 1]$.\n\nWe can choose a specific value for $s_1$, for instance $s_1=0$. This gives $\\nu_1^{\\star} = 0$. From $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$, we get $\\nu_2^{\\star} = 1$. So, a valid dual certificate is $\\nu^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\nWith this choice, all KKT conditions are satisfied, formally proving that $z^{\\star} = (0, 1)^{\\top}$ is the unique optimal solution.\n\nFinally, we address the relationship to the analysis viewpoint. The analysis BP problem is $\\min_x \\|\\Omega x\\|_1$ subject to $Ax=y$. The dual of the synthesis problem is:\n$$ (\\text{D}_{\\text{synth}}): \\quad \\max_{\\nu} y^{\\top}\\nu \\quad \\text{subject to} \\quad \\|D^{\\top}\\nu\\|_{\\infty} \\le 1 $$\nWhen the dictionary $D$ is square and invertible and $A=I$, the synthesis problem is equivalent to an analysis problem with the operator $\\Omega = D^{-1}$. The dual of this analysis problem is:\n$$ (\\text{D}_{\\text{anal}}): \\quad \\max_{\\lambda} y^{\\top}\\lambda \\quad \\text{subject to} \\quad \\lambda = \\Omega^{\\top}\\mu \\text{ for some } \\mu \\text{ with } \\|\\mu\\|_{\\infty} \\le 1 $$\nThe constraint on the dual variable $\\lambda$ can be rewritten. Since $\\lambda = (D^{-1})^\\top \\mu$, we have $D^\\top \\lambda = \\mu$. The norm constraint $\\|\\mu\\|_\\infty \\le 1$ thus translates to $\\|D^\\top \\lambda\\|_\\infty \\le 1$. This is precisely the same feasibility constraint as for the synthesis dual variable $\\nu$. This shows that the dual problems are identical, unifying the two perspectives.\n\nThe derived synthesis BP solution is $z^{\\star} = (0, 1)^{\\top}$ and the corresponding signal is $x^{\\star} = (1, 1)^{\\top}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} & \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Just as we explored the synthesis model, this exercise  provides a hands-on look at the mechanics of the analysis formulation. We will examine a case where the measurement constraints uniquely determine the solution, making the minimization seem trivial. The true pedagogical value lies in the process: converting the problem into a standard linear program and deriving its dual from scratch, which illuminates the structure of the optimality conditions and the central concept of the analysis cosupport.",
            "id": "3445026",
            "problem": "Consider the analysis basis pursuit (ABP) problem, defined as minimizing the analysis one-norm subject to linear measurements: given a linear operator $A \\in \\mathbb{R}^{m \\times n}$, an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$, and data $y \\in \\mathbb{R}^{m}$, the ABP problem is\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y.\n$$\nYou will analyze the concrete instance with $n=3$, \n$$\n\\Omega=\\begin{bmatrix}1 & -1 & 0\\\\ 0 & 1 & -1\\end{bmatrix}, \\quad A=I_{3}, \\quad y=\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}.\n$$\nStarting from first principles, do the following:\n1) Introduce an auxiliary variable $t \\in \\mathbb{R}^{2}$ to rewrite the problem as a linear program in standard inequality form using only linear equalities/inequalities and a linear objective. Justify the equivalence of the formulations rigorously from the definition of the one-norm.\n2) Derive the Lagrangian dual of your linear program by constructing the Lagrangian, enforcing finiteness of the infimum over the primal variables, and writing down the dual feasibility conditions and the dual objective. Do not invoke any pre-memorized dual formulas; derive the dual directly from the Lagrangian.\n3) Solve the primal and dual problems explicitly for the given data. Identify a primal optimal solution $x^{\\star}$ and corresponding optimal slack $t^{\\star}$, as well as a dual optimal solution, and verify complementary slackness and strong duality.\n4) Identify the cosupport (the set of indices $i$ such that $(\\Omega x^{\\star})_{i}=0$). Explain how your identification follows from your computations and the Karush–Kuhn–Tucker conditions.\n\nProvide $x^{\\star}$ as your final reported answer in the form of a row vector. No rounding is required. The cosupport should be identified and justified in your solution, but it is not required in the final boxed answer.",
            "solution": "The problem is to solve:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\nwith the specified data:\n$$\n\\Omega=\\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{bmatrix}, \\quad A=I_{3}=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad y=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nThe constraint $A x = y$ becomes $I_3 x = y$, which uniquely determines the feasible solution to be $x = y = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}^T$. While this immediately gives the primal optimal solution, we will follow the requested four-step analysis.\n\n**1) Linear Programming Formulation**\n\nThe objective function is the $\\ell_1$-norm of the vector $\\Omega x \\in \\mathbb{R}^2$. Let $z = \\Omega x$. The objective is $\\|z\\|_1 = |z_1| + |z_2|$.\nWe introduce an auxiliary variable $t = \\begin{bmatrix} t_1 \\\\ t_2 \\end{bmatrix} \\in \\mathbb{R}^2$. The expression $\\min \\| \\Omega x \\|_1$ is equivalent to $\\min (t_1 + t_2)$ subject to $t_1 \\ge |(\\Omega x)_1|$ and $t_2 \\ge |(\\Omega x)_2|$.\n\nThe justification for this equivalence is as follows. A value $c$ is an upper bound on $|a|$ if and only if $c \\ge a$ and $c \\ge -a$. Thus, the condition $t \\ge |\\Omega x|$ (element-wise) is equivalent to the pair of linear inequalities: $\\Omega x \\le t$ and $-\\Omega x \\le t$.\n\nGiven any feasible $x$ for the original problem, let us choose $t_i = |(\\Omega x)_i|$ for $i=1,2$. This choice of $t$ satisfies the inequalities $t \\ge \\Omega x$ and $t \\ge -\\Omega x$. The objective value for this $(x,t)$ pair in the new formulation is $\\sum_i t_i = \\sum_i |(\\Omega x)_i| = \\|\\Omega x\\|_1$.\nConversely, consider any feasible pair $(x,t)$ for the new formulation. The constraints require $t_i \\ge |(\\Omega x)_i|$. Since the objective is to minimize $\\sum_i t_i$ and the components $t_i$ are not coupled in any other constraint, the minimum will be achieved when each $t_i$ is as small as possible, i.e., when $t_i = |(\\Omega x)_i|$. Therefore, minimizing $\\sum_i t_i$ over the feasible set for $(x,t)$ is equivalent to minimizing $\\|\\Omega x\\|_1$ over the feasible set for $x$.\n\nThe problem is thus transformed into the following linear program (LP):\n$$\n\\min_{x \\in \\mathbb{R}^3, t \\in \\mathbb{R}^2} \\quad t_1 + t_2\n$$\nsubject to:\n$$\n\\begin{align*}\nA x &= y \\\\\n\\Omega x - t &\\le 0 \\\\\n-\\Omega x - t &\\le 0\n\\end{align*}\n$$\nwhere the inequalities are component-wise and $0$ is the zero vector in $\\mathbb{R}^2$.\n\n**2) Derivation of the Lagrangian Dual**\n\nTo derive the dual, we form the Lagrangian by associating dual variables with each constraint. Let $\\nu \\in \\mathbb{R}^3$ be the Lagrange multiplier for the equality constraint $y - Ax = 0$. Let $\\lambda_1, \\lambda_2 \\in \\mathbb{R}^2$ be the Lagrange multipliers for the inequality constraints, with the non-negativity constraints $\\lambda_1 \\ge 0$ and $\\lambda_2 \\ge 0$.\n\nThe Lagrangian $L(x, t, \\nu, \\lambda_1, \\lambda_2)$ is:\n$$\nL = \\mathbf{1}^T t + \\nu^T(y - Ax) + \\lambda_1^T(\\Omega x - t) + \\lambda_2^T(-\\Omega x - t)\n$$\nwhere $\\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. We rearrange the terms to group primal variables $x$ and $t$:\n$$\nL = \\nu^T y + (\\lambda_1^T \\Omega - \\lambda_2^T \\Omega - \\nu^T A) x + (\\mathbf{1}^T - \\lambda_1^T - \\lambda_2^T) t\n$$\nThe Lagrange dual function $g(\\nu, \\lambda_1, \\lambda_2)$ is the infimum of the Lagrangian over the primal variables $x$ and $t$. For the infimum to be finite (i.e., not $-\\infty$), the terms linear in $x$ and $t$ must vanish. This gives us the dual feasibility conditions:\n1. Coefficient of $x$: $\\Omega^T\\lambda_1 - \\Omega^T\\lambda_2 - A^T\\nu = 0 \\implies A^T\\nu = \\Omega^T(\\lambda_1 - \\lambda_2)$.\n2. Coefficient of $t$: $\\mathbf{1} - \\lambda_1 - \\lambda_2 = 0 \\implies \\lambda_1 + \\lambda_2 = \\mathbf{1}$.\n\nSubject to these conditions, the Lagrangian becomes $L = \\nu^T y$. The dual problem is to maximize this value.\nThe dual feasibility conditions can be simplified. Let $\\mu = \\lambda_1 - \\lambda_2$. From $\\lambda_1 + \\lambda_2 = \\mathbf{1}$ and the non-negativity constraints, we can express $\\lambda_1 = \\frac{1}{2}(\\mathbf{1} + \\mu)$ and $\\lambda_2 = \\frac{1}{2}(\\mathbf{1} - \\mu)$, which requires that each component of $\\mu$ is in $[-1, 1]$, i.e., $\\|\\mu\\|_\\infty \\le 1$. The stationarity condition with respect to $x$ becomes $A^T\\nu = \\Omega^T \\mu$. Since $A=I_3$, this simplifies to $\\nu = \\Omega^T \\mu$. Substituting this into the dual objective $\\nu^T y$ gives $(\\Omega^T \\mu)^T y = \\mu^T(\\Omega y)$. The dual problem is therefore:\n$$\n\\max_{\\mu} \\quad \\mu^T (\\Omega y) \\quad \\text{subject to} \\quad \\|\\mu\\|_\\infty \\le 1.\n$$\n\n**3) Primal and Dual Solutions**\n\n**Primal Solution:**\nThe constraint $Ax=y$ with $A=I_3$ fixes the primal solution to be $x^\\star = y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\nThe vector $\\Omega x^\\star$ is:\n$$\n\\Omega x^\\star = \\begin{bmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nFor the LP formulation, the optimal slack variable $t^\\star$ must satisfy $t^\\star_i = |(\\Omega x^\\star)_i|$.\n$$\nt^\\star = \\begin{bmatrix} |1| \\\\ |0| \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nThe optimal value of the primal problem is $t^\\star_1 + t^\\star_2 = 1+0=1$.\n\n**Dual Solution:**\nThe dual problem is $\\max_{\\mu} \\mu^T (\\Omega y)$ subject to $\\|\\mu\\|_\\infty \\le 1$. We have $\\Omega y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nSo we must solve:\n$$\n\\max_{\\mu_1, \\mu_2} \\quad \\mu_1(1) + \\mu_2(0) \\quad \\text{subject to} \\quad -1 \\le \\mu_1 \\le 1, \\quad -1 \\le \\mu_2 \\le 1.\n$$\nThe objective is simply $\\mu_1$. To maximize it, we choose the largest possible value, $\\mu_1^\\star = 1$. The variable $\\mu_2$ does not affect the objective, so any value in its feasible range $[-1, 1]$ is optimal. A valid choice is $\\mu_2^\\star = 0$.\nAn optimal dual solution is $\\mu^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThe optimal dual value is $1$. This confirms strong duality, as the primal and dual optimal values are equal.\n\nWe can find the corresponding full set of dual variables:\n$\\nu^\\star = \\Omega^T \\mu^\\star = \\begin{bmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$.\n$\\lambda_1^\\star = \\frac{1}{2}(\\mathbf{1}+\\mu^\\star) = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}$.\n$\\lambda_2^\\star = \\frac{1}{2}(\\mathbf{1}-\\mu^\\star) = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 0 \\\\ 1/2 \\end{bmatrix}$.\n\n**Verification of Complementary Slackness:**\nThe KKT complementary slackness conditions for the LP are $\\lambda_1^T(\\Omega x - t)=0$ and $\\lambda_2^T(-\\Omega x - t)=0$.\nLet's check these conditions at the optimal solutions $(x^\\star, t^\\star)$ and $(\\lambda_1^\\star, \\lambda_2^\\star)$.\nPrimal slacks:\n$\\Omega x^\\star - t^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n$-\\Omega x^\\star - t^\\star = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 0 \\end{bmatrix}$.\nDual variables: $\\lambda_1^\\star = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}$, $\\lambda_2^\\star = \\begin{bmatrix} 0 \\\\ 1/2 \\end{bmatrix}$.\n\nCondition 1: $(\\lambda_1^\\star)_i (\\Omega x^\\star - t^\\star)_i = 0$.\n- For $i=1$: $1 \\times 0 = 0$.\n- For $i=2$: $(1/2) \\times 0 = 0$.\nCondition 2: $(\\lambda_2^\\star)_i (-\\Omega x^\\star - t^\\star)_i = 0$.\n- For $i=1$: $0 \\times (-2) = 0$.\n- For $i=2$: $(1/2) \\times 0 = 0$.\nAll conditions are satisfied, verifying the optimality of our solutions.\n\n**4) Cosupport and Karush–Kuhn–Tucker (KKT) Conditions**\n\nThe cosupport of a vector $z$ is the set of indices $i$ for which $z_i=0$. For the optimal solution $x^\\star$, we examine the vector $\\Omega x^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. The first component is non-zero, and the second component is zero. Thus, the cosupport of $\\Omega x^\\star$ is the set $\\{2\\}$.\n\nThe connection to the KKT conditions for the original non-smooth problem $\\min_x \\|\\Omega x\\|_1$ s.t. $Ax=y$ provides insight. The stationarity condition is $0 \\in \\partial_x L(x, \\nu)$, where $L(x,\\nu) = \\|\\Omega x\\|_1 + \\nu^T(y-Ax)$. This gives:\n$$\n0 \\in \\Omega^T \\partial(\\|\\cdot\\|_1)|_{\\Omega x} - A^T\\nu\n$$\nwhich means there must exist a vector $\\mu \\in \\partial(\\|\\cdot\\|_1)|_{\\Omega x}$ such that $A^T\\nu = \\Omega^T\\mu$.\nThe subgradient of the $\\ell_1$-norm at a point $z$ is the set of vectors $\\mu$ where:\n- $\\mu_i = \\text{sign}(z_i)$ if $z_i \\ne 0$ (i.e., $i$ is in the support).\n- $\\mu_i \\in [-1, 1]$ if $z_i = 0$ (i.e., $i$ is in the cosupport).\n\nAt the optimal solution $x^\\star$, we have $\\Omega x^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. The KKT conditions require the existence of an optimal dual variable $\\mu^\\star$ such that:\n- $\\mu^\\star_1 = \\text{sign}(1) = 1$.\n- $\\mu^\\star_2 \\in [-1, 1]$.\n\nOur dual problem solution yielded a set of optimal dual variables $\\mu$ of the form $\\begin{bmatrix} 1 \\\\ \\mu_2 \\end{bmatrix}$ where $\\mu_2 \\in [-1, 1]$. This is perfectly consistent with the KKT conditions. Our specific choice, $\\mu^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, is one such valid certificate.\n\nThe identification of the cosupport follows from this relationship. The fact that the optimal dual variable $\\mu^\\star$ is not uniquely determined for its second component (any $\\mu_2 \\in [-1,1]$ is valid) implies that the second component of $\\Omega x^\\star$ must lie on the point of non-differentiability of the absolute value function, which is $0$. More precisely, if we find an optimal dual solution $\\mu^\\star$ where $|\\mu_i^\\star| < 1$, it is necessary that $(\\Omega x^\\star)_i=0$, so $i$ must be in the cosupport. In our case, we found $\\mu^\\star=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$. Since $|\\mu_2^\\star|=0 < 1$, we can conclude that the index $2$ is in the cosupport of $\\Omega x^\\star$. Since $|\\mu_1^\\star|=1$, the index $1$ is in the support. This matches our direct computation.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0 & 0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theory comes to life through implementation. This practice  moves from exact recovery models to the more practical LASSO formulation, comparing the synthesis and analysis approaches numerically. By implementing standard convex optimization algorithms—FISTA for the synthesis LASSO and ADMM for the analysis LASSO—you will investigate how each model recovers signals with different underlying structures, revealing the practical consequences of their distinct modeling assumptions.",
            "id": "3445015",
            "problem": "You are asked to implement a complete and runnable program to numerically compare the synthesis and analysis formulations for sparse recovery in a small-scale setting. The comparison must be performed on multiple test cases to highlight the structural differences between the two formulations, grounded in the context of compressed sensing and sparse optimization. The fundamental base is the convex optimization framework for the two formulations and the associated proximal algorithms.\n\nLet $A \\in \\mathbb{R}^{2 \\times 3}$ be a random Gaussian sensing matrix, $D = I_3$ be the identity dictionary for synthesis sparsity, and let $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ be the first-difference operator defined by\n$$\n\\Omega = \\begin{bmatrix}\n-1 & 1 & 0 \\\\\n0 & -1 & 1\n\\end{bmatrix}.\n$$\nFor a given ground-truth signal $x^\\star \\in \\mathbb{R}^{3}$, the measurement is $y = A x^\\star$ (noise-free). You will compute the following two estimators (Least Absolute Shrinkage and Selection Operator (LASSO)) from the same measurement $y$:\n- Synthesis LASSO with $D=I_3$:\n$$\n\\widehat{x}_{\\text{synth}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n- Analysis LASSO with first differences:\n$$\n\\widehat{x}_{\\text{anal}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|\\Omega x\\|_1.\n$$\n\nYou must compute numerical solutions using correct convex optimization algorithms consistent with the above objectives. For the synthesis LASSO, use a proximal gradient method such as the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). For the analysis LASSO, use a splitting scheme such as the Alternating Direction Method of Multipliers (ADMM) with the splitting variable $v = \\Omega x$.\n\nThe sensing matrix $A$ must be generated once using a fixed pseudorandom seed for reproducibility as follows:\n- Draw the entries of $A$ independently from a zero-mean, unit-variance Gaussian distribution and scale by $1/\\sqrt{2}$, that is, $A = \\frac{1}{\\sqrt{2}} G$ where $G_{ij} \\sim \\mathcal{N}(0,1)$.\n- Use the pseudorandom seed $7$ for this generation, and keep the same $A$ for all test cases.\n\nNo physical units or angles are involved; no percentages are used.\n\nTest Suite:\n- Use the following four test cases. In each case, compute $y = A x^\\star$. Report, for each case, three floats:\n    1. The Euclidean distance between the two solutions, $\\|\\widehat{x}_{\\text{synth}} - \\widehat{x}_{\\text{anal}}\\|_2$.\n    2. The Euclidean error of the synthesis solution with respect to the ground truth, $\\|\\widehat{x}_{\\text{synth}} - x^\\star\\|_2$.\n    3. The Euclidean error of the analysis solution with respect to the ground truth, $\\|\\widehat{x}_{\\text{anal}} - x^\\star\\|_2$.\n- The cases are:\n    - Case $1$: $x^\\star = [1.0, 1.0, 0.0]^\\top$, $\\lambda = 0.2$.\n    - Case $2$: $x^\\star = [1.5, 0.0, 0.0]^\\top$, $\\lambda = 0.2$.\n    - Case $3$: $x^\\star = [0.5, 0.5, 0.5]^\\top$, $\\lambda = 0.5$.\n    - Case $4$: $x^\\star = [0.8, -0.2, 0.8]^\\top$, $\\lambda = 5.0$.\n\nAlgorithmic requirements:\n- For the synthesis LASSO, implement a provably convergent proximal gradient algorithm with a stepsize that respects the Lipschitz constant of the gradient of the data-fidelity term.\n- For the analysis LASSO, implement Alternating Direction Method of Multipliers (ADMM) with the splitting $v = \\Omega x$, soft-thresholding on $v$, and a linear solve for the $x$-update using the positive definite system that arises.\n- Use absolute tolerance $10^{-6}$ and relative tolerance $10^{-5}$ for ADMM stopping, and a fixed iteration cap no smaller than $5000$ iterations for robustness. For the proximal gradient method, use a fixed iteration cap no smaller than $5000$ and a fixed stopping rule based on successive iterate differences with tolerance $10^{-9}$.\n\nFinal Output Specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of four items (one per test case), where each item is itself a three-element list in the order described above. All floating-point numbers must be rounded to exactly six digits after the decimal point. The format must be exactly:\n\"[ [d1,e1s,e1a], [d2,e2s,e2a], [d3,e3s,e3a], [d4,e4s,e4a] ]\"\nwithout any additional text, where $d_k$ is the Euclidean distance between the two solutions for case $k$, $e_{ks}$ is the synthesis error for case $k$, and $e_{ka}$ is the analysis error for case $k$.\n\nNotes:\n- The problem is purely mathematical and algorithmic. All quantities are deterministic under the given seed. Ensure numerical stability in the linear solves that arise in ADMM by using a method appropriate for small symmetric positive definite systems.",
            "solution": "We are asked to compare two standard sparse recovery models, the synthesis and analysis LASSO, using appropriate and specified iterative algorithms. First, we define the mathematical models and then detail the numerical algorithms used to solve them.\n\n### 1. Problem Formulation\n\nWe are given a linear measurement model $y = A x^\\star$, where $x^\\star \\in \\mathbb{R}^3$ is a ground-truth signal, $A \\in \\mathbb{R}^{2 \\times 3}$ is a sensing matrix, and $y \\in \\mathbb{R}^2$ is the measurement vector. The goal is to recover an estimate of $x^\\star$ from $y$ and $A$ using two different sparsity-promoting optimization problems.\n\n**Synthesis LASSO:** This model assumes that the signal $x$ is sparse in a synthesis dictionary $D$, which is given as the identity matrix $D=I_3$. The optimization problem is:\n$$\n\\widehat{x}_{\\text{synth}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1\n$$\nHere, the $\\ell_1$-norm $\\|x\\|_1$ promotes sparsity directly in the signal's coefficients.\n\n**Analysis LASSO:** This model assumes that the signal $x$ has a sparse representation after being transformed by an analysis operator $\\Omega$. In this problem, $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ is the first-difference operator. The optimization problem is:\n$$\n\\widehat{x}_{\\text{anal}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|\\Omega x\\|_1\n$$\nThis formulation encourages sparsity in the gradient of the signal, which is effective for piecewise-constant signals.\n\n### 2. Synthesis LASSO: The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)\n\nThe synthesis LASSO problem is a convex optimization problem of the form $\\min_x f(x) + g(x)$, where:\n- $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ is a smooth, convex, differentiable function. Its gradient is $\\nabla f(x) = A^\\top(A x - y)$. The gradient is Lipschitz continuous with constant $L = \\|A^\\top A\\|_2$, where $\\|\\cdot\\|_2$ denotes the spectral norm.\n- $g(x) = \\lambda \\|x\\|_1$ is a convex, non-differentiable function.\n\nThis structure is ideal for proximal gradient methods. FISTA is an accelerated version of the basic Iterative Shrinkage-Thresholding Algorithm (ISTA). The core of these methods is the proximal operator of $g(x)$, which is the soft-thresholding function:\n$$\n\\text{prox}_{\\gamma g}(z) = \\text{soft}(z, \\gamma\\lambda)_i = \\text{sgn}(z_i) \\max(|z_i| - \\gamma\\lambda, 0)\n$$\nFISTA introduces a momentum term to accelerate convergence. The iterative updates are as follows, starting with $x_0 \\in \\mathbb{R}^3$, $y_1=x_0$, $t_1=1$:\nFor $k = 1, 2, \\ldots$:\n1. Compute the next iterate $x_k$ by taking a gradient step from the momentum point $y_k$ and applying the proximal operator:\n   $$x_k = \\text{prox}_{\\frac{1}{L}g}(y_k - \\frac{1}{L}\\nabla f(y_k)) = \\text{soft}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - y), \\frac{\\lambda}{L}\\right)$$\n2. Update the momentum parameter $t_{k+1}$:\n   $$t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$$\n3. Update the momentum point $y_{k+1}$:\n   $$y_{k+1} = x_k + \\frac{t_k-1}{t_{k+1}}(x_k - x_{k-1})$$\n\nThe algorithm is initialized with $x_0 = x_{-1} = 0 \\in \\mathbb{R}^3$ and $t_1 = 1$. The iteration terminates when the Euclidean distance between successive iterates falls below a tolerance, i.e., $\\|x_k - x_{k-1}\\|_2 < 10^{-9}$, or a maximum number of iterations ($5000$) is reached.\n\n### 3. Analysis LASSO: The Alternating Direction Method of Multipliers (ADMM)\n\nThe analysis LASSO problem is reformulated as a constrained optimization problem to apply ADMM. We introduce a splitting variable $v \\in \\mathbb{R}^2$ such that $v = \\Omega x$:\n$$\n\\min_{x, v} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|v\\|_1 \\quad \\text{subject to} \\quad \\Omega x - v = 0\n$$\nThe augmented Lagrangian for this problem (in scaled dual form) is:\n$$\nL_\\rho(x, v, u) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda\\|v\\|_1 + \\frac{\\rho}{2}\\|\\Omega x - v + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\nwhere $u \\in \\mathbb{R}^2$ is the scaled dual variable and $\\rho > 0$ is the penalty parameter. ADMM proceeds by iteratively minimizing $L_\\rho$ with respect to $x$ and $v$ and then updating the dual variable $u$.\n\nThe iterative updates, starting with $x_0$, $v_0$, $u_0$ (typically zero vectors), are:\n1.  **$x$-update**: Minimize $L_\\rho$ with respect to $x$:\n    $$x_{k+1} = \\arg\\min_x \\left( \\frac{1}{2}\\|A x - y\\|_2^2 + \\frac{\\rho}{2}\\|\\Omega x - v_k + u_k\\|_2^2 \\right)$$\n    This is a quadratic problem whose solution is found by solving the linear system obtained by setting the gradient to zero:\n    $$(A^\\top A + \\rho \\Omega^\\top \\Omega) x_{k+1} = A^\\top y + \\rho \\Omega^\\top(v_k - u_k)$$\n    The matrix $P = A^\\top A + \\rho \\Omega^\\top \\Omega$ is a small ($3 \\times 3$) symmetric positive definite matrix, so its inverse can be pre-computed for efficient updates.\n\n2.  **$v$-update**: Minimize $L_\\rho$ with respect to $v$:\n    $$v_{k+1} = \\arg\\min_v \\left( \\lambda\\|v\\|_1 + \\frac{\\rho}{2}\\|\\Omega x_{k+1} - v + u_k\\|_2^2 \\right)$$\n    The solution is given by the soft-thresholding operator:\n    $$v_{k+1} = \\text{soft}\\left(\\Omega x_{k+1} + u_k, \\frac{\\lambda}{\\rho}\\right)$$\n\n3.  **$u$-update**: Update the dual variable:\n    $$u_{k+1} = u_k + \\Omega x_{k+1} - v_{k+1}$$\n\nThe algorithm terminates based on primal and dual residuals.\n- Primal residual: $r_{k+1} = \\Omega x_{k+1} - v_{k+1}$\n- Dual residual: $s_{k+1} = \\rho \\Omega^\\top(v_{k+1} - v_k)$\n\nThe stopping conditions are $\\|r_{k+1}\\|_2 \\leq \\epsilon^{\\text{pri}}$ and $\\|s_{k+1}\\|_2 \\leq \\epsilon^{\\text{dual}}$, where the tolerances are:\n- $\\epsilon^{\\text{pri}} = \\sqrt{2}\\epsilon^{\\text{abs}} + \\epsilon^{\\text{rel}} \\max(\\|\\Omega x_{k+1}\\|_2, \\|v_{k+1}\\|_2)$\n- $\\epsilon^{\\text{dual}} = \\sqrt{3}\\epsilon^{\\text{abs}} + \\epsilon^{\\text{rel}} \\|\\rho \\Omega^\\top u_{k+1}\\|_2$\n\nThe specified tolerances are $\\epsilon^{\\text{abs}} = 10^{-6}$ and $\\epsilon^{\\text{rel}} = 10^{-5}$. A value of $\\rho=1.0$ is used, and the maximum number of iterations is $5000$.\n\n### 4. Numerical Implementation and Evaluation\n\nFor each of the four test cases, the following steps are performed:\n1.  The sensing matrix $A$ is generated once using a fixed random seed of $7$.\n2.  The ground-truth signal $x^\\star$ and regularization parameter $\\lambda$ are set.\n3.  The measurement vector $y = A x^\\star$ is computed.\n4.  $\\widehat{x}_{\\text{synth}}$ is computed using the FISTA implementation.\n5.  $\\widehat{x}_{\\text{anal}}$ is computed using the ADMM implementation.\n6.  The three required metrics are calculated:\n    - Distance between solutions: $d = \\|\\widehat{x}_{\\text{synth}} - \\widehat{x}_{\\text{anal}}\\|_2$\n    - Synthesis error: $e_{s} = \\|\\widehat{x}_{\\text{synth}} - x^\\star\\|_2$\n    - Analysis error: $e_{a} = \\|\\widehat{x}_{\\text{anal}} - x^\\star\\|_2$\n\nThe results from all four cases are then formatted into the specified string output.",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(u, t):\n    \"\"\"\n    Soft-thresholding operator for vectors.\n    \"\"\"\n    return np.sign(u) * np.maximum(np.abs(u) - t, 0)\n\ndef fista_solver(A, y, lambda_val, max_iter=5000, tol=1e-9):\n    \"\"\"\n    Solves the synthesis LASSO problem using FISTA.\n    min_x 0.5 * ||Ax - y||^2 + lambda * ||x||_1\n    \"\"\"\n    n, p = A.shape\n    \n    # Lipschitz constant of the gradient of the smooth term\n    L = np.linalg.norm(A.T @ A, ord=2)\n    step_size = 1.0 / L\n\n    x_k = np.zeros((p, 1))\n    x_k_minus_1 = np.zeros((p, 1))\n    y_k = np.zeros((p, 1))\n    t_k = 1.0\n\n    for _ in range(max_iter):\n        grad_f_y = A.T @ (A @ y_k - y)\n        x_k_plus_1 = soft_threshold(y_k - step_size * grad_f_y, step_size * lambda_val)\n\n        if np.linalg.norm(x_k_plus_1 - x_k) < tol:\n            break\n\n        t_k_plus_1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n        y_k_plus_1 = x_k_plus_1 + ((t_k - 1.0) / t_k_plus_1) * (x_k_plus_1 - x_k)\n\n        x_k_minus_1 = x_k\n        x_k = x_k_plus_1\n        y_k = y_k_plus_1\n        t_k = t_k_plus_1\n        \n    return x_k\n\ndef admm_solver(A, y, Omega, lambda_val, rho=1.0, max_iter=5000, eps_abs=1e-6, eps_rel=1e-5):\n    \"\"\"\n    Solves the analysis LASSO problem using ADMM.\n    min_x 0.5 * ||Ax - y||^2 + lambda * ||Omega * x||_1\n    \"\"\"\n    n, p = A.shape\n    q, _ = Omega.shape\n\n    # Pre-compute matrices for x-update\n    AtA = A.T @ A\n    Aty = A.T @ y\n    OmegatOmega = Omega.T @ Omega\n    P_inv = np.linalg.inv(AtA + rho * OmegatOmega)\n\n    x_k = np.zeros((p, 1))\n    v_k = np.zeros((q, 1))\n    u_k = np.zeros((q, 1))\n\n    for k in range(max_iter):\n        # x-update\n        rhs_x = Aty + rho * Omega.T @ (v_k - u_k)\n        x_k_plus_1 = P_inv @ rhs_x\n\n        # v-update\n        v_k_old = v_k\n        v_k_plus_1 = soft_threshold(Omega @ x_k_plus_1 + u_k, lambda_val / rho)\n\n        # u-update\n        u_k_plus_1 = u_k + Omega @ x_k_plus_1 - v_k_plus_1\n\n        # Stopping criteria\n        # Primal residual\n        r_k_plus_1 = Omega @ x_k_plus_1 - v_k_plus_1\n        eps_pri = np.sqrt(q) * eps_abs + eps_rel * np.maximum(np.linalg.norm(Omega @ x_k_plus_1), np.linalg.norm(v_k_plus_1))\n        \n        # Dual residual\n        s_k_plus_1 = rho * Omega.T @ (v_k_plus_1 - v_k_old)\n        eps_dual = np.sqrt(p) * eps_abs + eps_rel * np.linalg.norm(rho * Omega.T @ u_k_plus_1)\n\n        if np.linalg.norm(r_k_plus_1) < eps_pri and np.linalg.norm(s_k_plus_1) < eps_dual:\n            x_k = x_k_plus_1\n            break\n            \n        x_k = x_k_plus_1\n        v_k = v_k_plus_1\n        u_k = u_k_plus_1\n    \n    return x_k\n\ndef solve():\n    # Set up the problem parameters\n    np.random.seed(7)\n    G = np.random.randn(2, 3)\n    A = G / np.sqrt(2)\n    \n    Omega = np.array([[-1., 1., 0.], [0., -1., 1.]])\n\n    test_cases = [\n        {'x_star': np.array([1.0, 1.0, 0.0]), 'lambda': 0.2},\n        {'x_star': np.array([1.5, 0.0, 0.0]), 'lambda': 0.2},\n        {'x_star': np.array([0.5, 0.5, 0.5]), 'lambda': 0.5},\n        {'x_star': np.array([0.8, -0.2, 0.8]), 'lambda': 5.0}\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        x_star = case['x_star'].reshape(3, 1)\n        lambda_val = case['lambda']\n        \n        # Generate measurements\n        y = A @ x_star\n        \n        # Solve for synthesis LASSO\n        x_synth = fista_solver(A, y, lambda_val)\n        \n        # Solve for analysis LASSO\n        x_anal = admm_solver(A, y, Omega, lambda_val)\n\n        # Calculate metrics\n        dist_sol = np.linalg.norm(x_synth - x_anal)\n        err_synth = np.linalg.norm(x_synth - x_star)\n        err_anal = np.linalg.norm(x_anal - x_star)\n        \n        all_results.append([dist_sol, err_synth, err_anal])\n\n    # Format the final output string\n    results_str_list = []\n    for res_tuple in all_results:\n        formatted_tuple = [f\"{x:.6f}\" for x in res_tuple]\n        results_str_list.append(f\"[{','.join(formatted_tuple)}]\")\n    final_output = f\"[{','.join(results_str_list)}]\"\n        \n    print(final_output)\n\nsolve()\n\n```"
        }
    ]
}