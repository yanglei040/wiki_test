{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of sparse recovery is identifying the correct support of the unknown signal. This first exercise strips the problem down to its essentials, exploring how a simple correlation-based detector can be made robust against adversarial perturbations. By working from first principles, you will derive a detection threshold that explicitly links a fundamental property of the sensing matrix—its mutual coherence—to the level of adversarial noise it can withstand, providing a clear illustration of the trade-offs at the heart of robust design .",
            "id": "3430326",
            "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with unit $\\ell_{2}$-norm columns and mutual coherence $\\mu(A) \\triangleq \\max_{i \\neq j} | \\langle a_{i}, a_{j} \\rangle |$, where $a_{i}$ denotes the $i$-th column of $A$. Let the unknown signal $x^{\\star} \\in \\mathbb{R}^{n}$ be $k$-sparse with support $S \\subset \\{1,\\dots,n\\}$, minimum nonzero magnitude $m_{\\star} \\triangleq \\min_{i \\in S} |x^{\\star}_{i}|$, and a known a priori bound on its $\\ell_{1}$-norm $\\|x^{\\star}\\|_{1} \\leq L$, where $L  0$ is given. Measurements are noiseless and given by $y = A x^{\\star}$.\n\nA simple support detector computes the correlation proxy $g \\triangleq A^{\\mathsf{T}} y = A^{\\mathsf{T}} A x^{\\star}$ and then selects indices by hard thresholding. An adversary perturbs the proxy before thresholding, yielding the attacked proxy $v \\triangleq g + e$, where the adversary is $\\ell_{\\infty}$-bounded: $\\|e\\|_{\\infty} \\leq \\eta$, with given $\\eta \\geq 0$. The detector outputs the support estimate\n$$\nT(\\tau) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |v_{i}|  \\tau \\},\n$$\nfor a threshold $\\tau  0$ to be designed.\n\nStarting only from the definitions of mutual coherence and the triangle inequality, and without assuming any specific distribution of the nonzero entries of $x^{\\star}$ beyond the given bounds, derive a deterministic threshold $\\tau$ as a function of $\\mu(A)$, $L$, and $\\eta$ that guarantees $T(\\tau) = S$ provided that $m_{\\star}$ is sufficiently large relative to $\\mu(A)$, $L$, and $\\eta$. Your derivation should explicitly bound the largest possible spurious correlation off-support and the smallest possible attenuated correlation on-support under worst-case signs and adversarial perturbations, and then choose $\\tau$ to separate these bounds.\n\nGive your final answer as a single closed-form symbolic expression for the threshold $\\tau$ in terms of $\\mu(A)$, $L$, and $\\eta$. Do not provide conditions or inequalities in the final answer. No rounding is required.",
            "solution": "The problem requires the derivation of a deterministic threshold $\\tau$, as a function of the mutual coherence $\\mu(A)$, the $\\ell_1$-norm bound $L$, and the adversarial perturbation bound $\\eta$. This threshold must ensure that the support estimate $T(\\tau) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |v_{i}|  \\tau \\}$ correctly identifies the true support $S$ of a $k$-sparse signal $x^{\\star}$. The condition for perfect support recovery, $T(\\tau) = S$, mandates that two criteria are met: firstly, for every index $i$ within the true support $S$, the inequality $|v_i|  \\tau$ must hold; secondly, for every index $j$ outside the support, $j \\in S^c$, the inequality $|v_j| \\leq \\tau$ must hold.\n\nTo satisfy these conditions for any valid signal $x^{\\star}$ and perturbation $e$ that conform to the given bounds, we must choose $\\tau$ such that it separates the worst-case (smallest) on-support magnitude from the worst-case (largest) off-support magnitude. That is, we seek a $\\tau$ such that:\n$$\n\\max_{j \\notin S, \\|x^\\star\\|_1 \\le L, \\|e\\|_\\infty \\le \\eta} |v_j| \\leq \\tau  \\min_{i \\in S, \\|x^\\star\\|_1 \\le L, \\|e\\|_\\infty \\le \\eta} |v_i|\n$$\nThe problem states we can assume the minimum non-zero magnitude of the signal, $m_{\\star}$, is sufficiently large to ensure such a separation exists. Our task is to find an explicit expression for $\\tau$ that depends only on $\\mu(A)$, $L$, and $\\eta$.\n\nThe attacked proxy vector is $v = A^{\\mathsf{T}} A x^{\\star} + e$. The $i$-th component is given by:\n$$\nv_i = (A^{\\mathsf{T}} A x^{\\star})_i + e_i = \\sum_{l=1}^{n} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i\n$$\nwhere $a_i$ is the $i$-th column of $A$. Since the columns of $A$ have unit $\\ell_2$-norm, we have $\\langle a_i, a_i \\rangle = \\|a_i\\|_{2}^{2} = 1$.\n\nFirst, we derive an upper bound on the magnitude of the off-support components, $|v_j|$ for $j \\notin S$.\nFor an index $j \\notin S$, the signal component $x^{\\star}_j$ is $0$. The sum is therefore restricted to the support set $S$:\n$$\nv_j = \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l + e_j\n$$\nUsing the triangle inequality, we can bound the magnitude of $v_j$:\n$$\n|v_j| = \\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l + e_j \\right| \\leq \\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l \\right| + |e_j|\n$$\nApplying the triangle inequality to the sum and using the definition of mutual coherence, $|\\langle a_j, a_l \\rangle| \\leq \\mu(A)$ for $j \\neq l$ (which is always true for $l \\in S$ and $j \\notin S$), we obtain:\n$$\n\\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l \\right| \\leq \\sum_{l \\in S} |\\langle a_j, a_l \\rangle| |x^{\\star}_l| \\leq \\sum_{l \\in S} \\mu(A) |x^{\\star}_l| = \\mu(A) \\sum_{l \\in S} |x^{\\star}_l|\n$$\nThe sum $\\sum_{l \\in S} |x^{\\star}_l|$ is the $\\ell_1$-norm of the signal, $\\|x^{\\star}\\|_1$. The adversarial perturbation is bounded by $|e_j| \\leq \\|e\\|_{\\infty} \\leq \\eta$.\nCombining these results, we get:\n$$\n|v_j| \\leq \\mu(A) \\|x^{\\star}\\|_1 + \\eta\n$$\nTo obtain a worst-case bound, we use the given prior information that $\\|x^{\\star}\\|_1 \\leq L$. The right-hand side is maximized when $\\|x^{\\star}\\|_1$ reaches its maximum allowed value, $L$. Thus, for any $j \\notin S$, we have the deterministic upper bound:\n$$\n|v_j| \\leq \\mu(A) L + \\eta\n$$\nLet us denote this worst-case upper bound for off-support components as $B_{off} = \\mu(A) L + \\eta$.\n\nNext, we derive a lower bound on the magnitude of the on-support components, $|v_i|$ for $i \\in S$. For an index $i \\in S$, we separate the term corresponding to $x^{\\star}_i$:\n$$\nv_i = \\langle a_i, a_i \\rangle x^{\\star}_i + \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i = x^{\\star}_i + \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i\n$$\nUsing the reverse triangle inequality, $|a+b| \\geq |a| - |b|$, we have:\n$$\n|v_i| \\geq |x^{\\star}_i| - \\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i \\right|\n$$\nApplying the triangle inequality to the subtracted term:\n$$\n\\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i \\right| \\leq \\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l \\right| + |e_i|\n$$\nThe magnitude of the sum is bounded using mutual coherence:\n$$\n\\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l \\right| \\leq \\sum_{l \\in S, l \\neq i} |\\langle a_i, a_l \\rangle| |x^{\\star}_l| \\leq \\mu(A) \\sum_{l \\in S, l \\neq i} |x^{\\star}_l|\n$$\nThe sum can be expressed as $\\sum_{l \\in S, l \\neq i} |x^{\\star}_l| = \\|x^{\\star}\\|_1 - |x^{\\star}_i|$. The worst-case (largest) interference for an on-support element is thus bounded by $\\mu(A) (\\|x^{\\star}\\|_1 - |x^{\\star}_i|) + \\eta$. This leads to a lower bound on $|v_i|$:\n$$\n|v_i| \\geq |x^{\\star}_i| - \\left( \\mu(A) (\\|x^{\\star}\\|_1 - |x^{\\star}_i|) + \\eta \\right) = |x^{\\star}_i|(1 + \\mu(A)) - \\mu(A)\\|x^{\\star}\\|_1 - \\eta\n$$\nTo find a general lower bound, we must consider the worst-case choice of $x^\\star$. The expression is minimized when $\\|x^{\\star}\\|_1$ is maximal (up to $L$) and $|x^{\\star}_i|$ is minimal (down to $m_\\star$). Therefore, for any $i \\in S$:\n$$\n|v_i| \\geq m_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta\n$$\nLet us denote this worst-case lower bound for on-support components as $B_{on} = m_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta$.\n\nOur goal is to find a threshold $\\tau$ such that $|v_j| \\leq \\tau$ for all $j \\notin S$ and $|v_i|  \\tau$ for all $i \\in S$. These conditions are guaranteed if we select $\\tau$ such that $B_{off} \\leq \\tau  B_{on}$.\nThe problem asks for a threshold $\\tau$ that is a function of only $\\mu(A)$, $L$, and $\\eta$. The bound $B_{off} = \\mu(A) L + \\eta$ satisfies this requirement, whereas $B_{on}$ depends on $m_{\\star}$. A suitable and direct choice for the threshold is to set it equal to the maximum possible magnitude of an off-support component.\nLet us choose the threshold $\\tau = B_{off}$:\n$$\n\\tau = \\mu(A) L + \\eta\n$$\nWith this choice, the condition for off-support indices, $|v_j| \\leq \\tau$, is guaranteed to hold because we have rigorously shown that $|v_j| \\leq B_{off} = \\tau$ for all $j \\notin S$. Thus, no off-support index will satisfy $|v_j|  \\tau$ and be incorrectly selected.\n\nFor the on-support indices, we must ensure that $|v_i|  \\tau$. Since we know $|v_i| \\geq B_{on}$, this condition is satisfied if the lower bound $B_{on}$ is strictly greater than our chosen threshold $\\tau$:\n$$\nm_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta  \\mu(A) L + \\eta\n$$\n$$\nm_{\\star}(1 + \\mu(A))  2\\mu(A)L + 2\\eta\n$$\n$$\nm_{\\star}  \\frac{2\\mu(A)L + 2\\eta}{1 + \\mu(A)}\n$$\nThis inequality is the precise condition that \"$m_{\\star}$ is sufficiently large\". As the problem statement permits us to assume such a condition holds, the choice of $\\tau = \\mu(A) L + \\eta$ is valid and guarantees $T(\\tau) = S$ under this condition. This expression for $\\tau$ is a closed-form symbolic expression in terms of the required parameters.",
            "answer": "$$\n\\boxed{\\mu(A)L + \\eta}\n$$"
        },
        {
            "introduction": "Moving beyond simple support detection, we often need to bound the total estimation error for more sophisticated recovery algorithms. This practice introduces a powerful and general technique for certifying the robustness of estimators like the elastic-net Lasso, which are defined as solutions to convex optimization problems. You will leverage the fixed-point characterization of proximal gradient methods to derive a formal error bound, demonstrating how concepts like restricted strong convexity and non-expansiveness provide rigorous guarantees on estimator stability in the face of adversarial data corruption .",
            "id": "3430319",
            "problem": "Consider a measurement model $y = A x^{\\star}$ with $A \\in \\mathbb{R}^{m \\times n}$ and a $k$-sparse ground truth vector $x^{\\star} \\in \\mathbb{R}^{n}$. Define the elastic-net Lasso estimator\n$$\nx^{\\mathrm{rec}}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\|x\\|_{2}^{2} + \\gamma \\|x\\|_{1} \\right\\},\n$$\nwith $\\lambda  0$ and $\\gamma \\ge 0$. Assume that the matrix $A$ satisfies the following restricted eigenvalue bounds: for any vector $u \\in \\mathbb{R}^{n}$ supported on at most $2k$ coordinates,\n$$\n\\alpha_{2k} \\|u\\|_{2}^{2} \\le \\|A u\\|_{2}^{2} \\le \\beta_{2k} \\|u\\|_{2}^{2},\n$$\nwith $\\alpha_{2k}  0$ and $\\beta_{2k} \\ge \\alpha_{2k}$, so that the smooth part of the objective is restricted strongly convex with parameter $\\alpha_{2k} + \\lambda$ and restricted smooth with parameter $\\beta_{2k} + \\lambda$. Consider solving the above estimator by a proximal gradient map\n$$\nT_{y}(x) := \\mathrm{prox}_{\\eta \\gamma \\|\\cdot\\|_{1}}\\!\\left(x - \\eta \\nabla \\left( \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\|x\\|_{2}^{2} \\right) \\right) = \\mathrm{soft}\\!\\left(x - \\eta \\left(A^{\\top}(A x - y) + \\lambda x\\right), \\, \\eta \\gamma\\right),\n$$\nwhere $\\mathrm{prox}$ denotes the proximal operator and $\\mathrm{soft}(\\cdot, \\theta)$ is the componentwise soft-thresholding at level $\\theta$. Let the step size be chosen as $\\eta = \\dfrac{2}{(\\beta_{2k} + \\lambda) + (\\alpha_{2k} + \\lambda)} = \\dfrac{2}{\\beta_{2k} + \\alpha_{2k} + 2\\lambda}$.\n\nSuppose the regularization $\\gamma$ is such that in the clean case $y = A x^{\\star}$, the estimator $x^{\\mathrm{rec}}(y)$ exactly recovers the ground truth, namely $x^{\\mathrm{rec}}(y) = x^{\\star}$, which is consistent with standard sparse recovery conditions under the irrepresentable-type assumptions and restricted convexity.\n\nNow consider an adversarial perturbation $e \\in \\mathbb{R}^{m}$ added to the measurements, so the corrupted measurements are $y + e$, with a known bound $\\|e\\|_{p} \\le \\epsilon$ for some $p \\in [1, \\infty]$. Using only fundamental properties of Lipschitz continuity, restricted strong convexity and smoothness, and non-expansiveness of proximal mappings, derive a certified bound $\\tau(\\epsilon)$ such that\n$$\n\\|x^{\\mathrm{rec}}(y+e) - x^{\\star}\\|_{2} \\le \\tau(\\epsilon)\n$$\nthat holds for all $e$ satisfying $\\|e\\|_{p} \\le \\epsilon$. Express $\\tau(\\epsilon)$ in closed-form in terms of $\\epsilon$, the operator norm $\\|A^{\\top}\\|_{p \\to 2} := \\sup_{\\|v\\|_{p} \\le 1} \\|A^{\\top} v\\|_{2}$, and the restricted strong convexity parameter $\\alpha_{2k}$ together with $\\lambda$. Your final answer must be a single analytical expression. No rounding is required and no physical units are involved.",
            "solution": "The problem is well-posed and scientifically grounded within the field of sparse optimization and compressed sensing. We can proceed with a formal derivation.\n\nLet the objective function for a given measurement vector $z \\in \\mathbb{R}^{m}$ be denoted by\n$$\nF(x; z) := \\frac{1}{2}\\|A x - z\\|_{2}^{2} + \\frac{\\lambda}{2}\\|x\\|_{2}^{2} + \\gamma \\|x\\|_{1}.\n$$\nThe problem defines the estimator $x^{\\mathrm{rec}}(z)$ as the minimizer of $F(x; z)$.\nLet $\\hat{x} = x^{\\mathrm{rec}}(y+e)$ be the solution for the adversarially corrupted measurements $y+e$, where $y=Ax^\\star$. By the problem's assumption, the solution for the clean measurements $y$ is the ground truth vector $x^{\\star}$, i.e., $x^{\\star} = x^{\\mathrm{rec}}(y)$.\n\nThe minimizer of a composite objective like $F(x;z)$ can be characterized as a fixed point of the corresponding proximal gradient map. For the corrupted case, $\\hat{x}$ is the fixed point of the map $T_{y+e}(x)$:\n$$\n\\hat{x} = T_{y+e}(\\hat{x}).\n$$\nSimilarly, for the clean case, $x^{\\star}$ is the fixed point of the map $T_y(x)$:\n$$\nx^{\\star} = T_y(x^{\\star}).\n$$\nOur goal is to bound the Euclidean distance $\\|\\hat{x} - x^{\\star}\\|_{2}$. Using the fixed-point characterizations, we can write:\n$$\n\\|\\hat{x} - x^{\\star}\\|_{2} = \\|T_{y+e}(\\hat{x}) - T_y(x^{\\star})\\|_{2}.\n$$\nWe apply the triangle inequality to separate the effects of changing the input to the map ($ \\hat{x} \\to x^{\\star} $) from the effects of changing the map itself ($ T_{y+e} \\to T_y $):\n$$\n\\|\\hat{x} - x^{\\star}\\|_{2} \\le \\|T_{y+e}(\\hat{x}) - T_{y+e}(x^{\\star})\\|_{2} + \\|T_{y+e}(x^{\\star}) - T_y(x^{\\star})\\|_{2}.\n$$\nLet's analyze each term separately.\n\nThe first term, $\\|T_{y+e}(\\hat{x}) - T_{y+e}(x^{\\star})\\|_{2}$, measures the stability of the map $T_{y+e}$ with respect to its argument. The map $T_z(x)$ is defined as a proximal gradient step. The smooth part of the objective $F(x;z)$ is $f(x;z) = \\frac{1}{2}\\|A x - z\\|_{2}^{2} + \\frac{\\lambda}{2}\\|x\\|_{2}^{2}$. The problem states that this function is restricted strongly convex with parameter $\\mu = \\alpha_{2k} + \\lambda$ and restricted smooth with parameter $L = \\beta_{2k} + \\lambda$.\nFor a function that is $\\mu$-strongly convex and $L$-smooth, the proximal gradient map with step size $\\eta = \\frac{2}{L+\\mu}$ is a contraction with rate $\\kappa = \\frac{L-\\mu}{L+\\mu}$. We assume that the error vector $\\hat{x} - x^{\\star}$ lies in the set of vectors for which these restricted properties hold, a standard assumption in such analyses.\nThe map $T_{y+e}$ is a composition of a gradient step and a proximal operator. The proximal operator is non-expansive. The gradient step map $G(x) = x - \\eta \\nabla f(x)$ is a contraction with factor $\\kappa$. Therefore, the overall map $T_{y+e}$ is also a contraction with at least the same factor:\n$$\n\\|T_{y+e}(\\hat{x}) - T_{y+e}(x^{\\star})\\|_{2} \\le \\kappa \\|\\hat{x} - x^{\\star}\\|_{2},\n$$\nwhere $\\kappa = \\frac{L-\\mu}{L+\\mu} = \\frac{(\\beta_{2k}+\\lambda) - (\\alpha_{2k}+\\lambda)}{(\\beta_{2k}+\\lambda) + (\\alpha_{2k}+\\lambda)} = \\frac{\\beta_{2k}-\\alpha_{2k}}{\\beta_{2k}+\\alpha_{2k}+2\\lambda}$.\n\nThe second term, $\\|T_{y+e}(x^{\\star}) - T_y(x^{\\star})\\|_{2}$, captures the shift in the fixed point due to the perturbation $e$.\nBy definition:\n$$\nT_{y+e}(x^{\\star}) = \\mathrm{soft}\\!\\left(x^{\\star} - \\eta \\left(A^{\\top}(A x^{\\star} - (y+e)) + \\lambda x^{\\star}\\right), \\, \\eta \\gamma\\right).\n$$\n$$\nT_{y}(x^{\\star}) = \\mathrm{soft}\\!\\left(x^{\\star} - \\eta \\left(A^{\\top}(A x^{\\star} - y) + \\lambda x^{\\star}\\right), \\, \\eta \\gamma\\right).\n$$\nThe soft-thresholding operator, $\\mathrm{soft}(\\cdot, \\theta)$, is non-expansive with respect to the $\\|\\cdot\\|_2$ norm. Thus,\n$$\n\\|T_{y+e}(x^{\\star}) - T_y(x^{\\star})\\|_{2} \\le \\left\\| \\left(x^{\\star} - \\eta (A^{\\top}(A x^{\\star} - y - e) + \\lambda x^{\\star})\\right) - \\left(x^{\\star} - \\eta (A^{\\top}(A x^{\\star} - y) + \\lambda x^{\\star})\\right) \\right\\|_{2}.\n$$\nSimplifying the argument of the norm:\n$$\n-\\eta( A^{\\top}(A x^{\\star} - y - e) - A^{\\top}(A x^{\\star} - y)) = -\\eta( -A^{\\top}e ) = \\eta A^{\\top}e.\n$$\nSo, the inequality becomes:\n$$\n\\|T_{y+e}(x^{\\star}) - T_y(x^{\\star})\\|_{2} \\le \\|\\eta A^{\\top}e\\|_{2} = \\eta \\|A^{\\top}e\\|_{2}.\n$$\nNow, we substitute these two bounds back into the main inequality:\n$$\n\\|\\hat{x} - x^{\\star}\\|_{2} \\le \\kappa \\|\\hat{x} - x^{\\star}\\|_{2} + \\eta \\|A^{\\top}e\\|_{2}.\n$$\nRearranging the terms, we get:\n$$\n(1-\\kappa) \\|\\hat{x} - x^{\\star}\\|_{2} \\le \\eta \\|A^{\\top}e\\|_{2}.\n$$\n$$\n\\|\\hat{x} - x^{\\star}\\|_{2} \\le \\frac{\\eta}{1-\\kappa} \\|A^{\\top}e\\|_{2}.\n$$\nLet us compute the factor $\\frac{\\eta}{1-\\kappa}$:\n$$\n1-\\kappa = 1 - \\frac{L-\\mu}{L+\\mu} = \\frac{(L+\\mu) - (L-\\mu)}{L+\\mu} = \\frac{2\\mu}{L+\\mu}.\n$$\nWith the step size $\\eta = \\frac{2}{L+\\mu}$, the factor becomes:\n$$\n\\frac{\\eta}{1-\\kappa} = \\frac{2/(L+\\mu)}{2\\mu/(L+\\mu)} = \\frac{1}{\\mu}.\n$$\nSince $\\mu = \\alpha_{2k} + \\lambda$, we have:\n$$\n\\|\\hat{x} - x^{\\star}\\|_{2} \\le \\frac{1}{\\alpha_{2k} + \\lambda} \\|A^{\\top}e\\|_{2}.\n$$\nFinally, we bound $\\|A^{\\top}e\\|_{2}$ using the given information. We are given the operator norm $\\|A^{\\top}\\|_{p \\to 2} := \\sup_{\\|v\\|_{p} \\le 1} \\|A^{\\top} v\\|_{2}$ and the constraint on the perturbation $\\|e\\|_{p} \\le \\epsilon$. By definition of the operator norm:\n$$\n\\|A^{\\top}e\\|_{2} \\le \\|A^{\\top}\\|_{p \\to 2} \\|e\\|_{p} \\le \\|A^{\\top}\\|_{p \\to 2} \\epsilon.\n$$\nCombining this with our previous result, we arrive at the certified bound:\n$$\n\\|\\hat{x} - x^{\\star}\\|_{2} \\le \\frac{\\|A^{\\top}\\|_{p \\to 2}}{\\alpha_{2k} + \\lambda} \\epsilon.\n$$\nTherefore, the certified bound $\\tau(\\epsilon)$ is given by the expression on the right-hand side.\n$$\n\\tau(\\epsilon) = \\frac{\\|A^{\\top}\\|_{p \\to 2}}{\\alpha_{2k} + \\lambda} \\epsilon.\n$$\nThis bound holds for all adversarial perturbations $e$ satisfying $\\|e\\|_{p} \\le \\epsilon$.",
            "answer": "$$\n\\boxed{\\frac{\\|A^{\\top}\\|_{p \\to 2}}{\\alpha_{2k} + \\lambda} \\epsilon}\n$$"
        },
        {
            "introduction": "Theoretical bounds provide crucial guarantees, but a full understanding of robustness requires bridging the gap between theory and practice. This hands-on exercise challenges you to do just that by taking on the roles of both defender and attacker for the Least Absolute Deviations (LAD)-Lasso estimator. You will implement the estimator as a linear program, computationally search for the worst-case sparse outlier attack, and compare the resulting empirical error to a theoretically derived upper bound, gaining valuable insight into the practical impact of adversarial attacks and the tightness of analytical guarantees .",
            "id": "3430330",
            "problem": "Consider the measurement model in compressed sensing with adversarial outliers. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known sensing matrix, $x \\in \\mathbb{R}^{n}$ be a fixed but unknown signal, and $e_s \\in \\mathbb{R}^{m}$ be an adversarial outlier vector with $\\|e_s\\|_0 = s$. The observation is $y = A x + e_s$. For recovery, use the Least Absolute Deviations (LAD)-Lasso estimator, defined as the solution to the convex optimization problem\n$$\n\\widehat{x}(y) \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\|A z - y\\|_1 + \\lambda \\|z\\|_1 \\right\\},\n$$\nwhere $\\lambda  0$ is a regularization parameter. Assume the adversary is constrained by an amplitude budget $\\alpha  0$ so that each nonzero entry of $e_s$ satisfies $|e_{s,i}| \\le \\alpha$. The adversary chooses the support and signs of $e_s$ to maximize the recovery error $\\|\\widehat{x}(y) - x\\|_2$.\n\nYour tasks are:\n- Construct, for each test case, a worst-case adversarial outlier vector $e_s$ with $\\|e_s\\|_0 = s$ and $|e_{s,i}| = \\alpha$ on its support, that maximizes $\\|\\widehat{x}(A x + e_s) - x\\|_2$ under the LAD-Lasso estimator. Justify why extremal amplitudes $|e_{s,i}| = \\alpha$ are sufficient for worst-case design under the given constraint.\n- Derive from first principles an analytic upper bound on the achievable error inflation $\\|\\widehat{x}(A x + e_s) - x\\|_2$, expressed in terms of $A$, $\\lambda$, $s$, and $\\alpha$. Use only fundamental inequalities, norm relations, and singular value properties. The bound must be a function that can be evaluated numerically from $A$, $\\lambda$, $s$, and $\\alpha$.\n- Implement a program that computes, for each test case, both the worst-case recovery error over all sparse adversaries with the specified $s$ and amplitude budget $\\alpha$, and the derived analytic bound. Your implementation of the LAD-Lasso must be exact by reformulating the optimization into a linear program.\n\nThe LAD-Lasso problem can be reformulated using auxiliary variables $r \\in \\mathbb{R}^{m}$, $t \\in \\mathbb{R}^{m}$, and $u \\in \\mathbb{R}^{n}$ to handle absolute values:\n$$\n\\min_{z, r, t, u} \\sum_{i=1}^{m} t_i + \\lambda \\sum_{j=1}^{n} u_j \\quad \\text{subject to} \\quad r = A z - y,\\; -t \\le r \\le t,\\; -u \\le z \\le u,\\; t \\ge 0,\\; u \\ge 0.\n$$\nThis is a linear program.\n\nYou must start your derivation from the following base facts and definitions:\n- The triangle inequality for norms and the definition of $\\ell_1$ and $\\ell_2$ norms: for any $v \\in \\mathbb{R}^{k}$, $\\|v\\|_1 = \\sum_{i=1}^{k} |v_i|$ and $\\|v\\|_2 = \\left(\\sum_{i=1}^{k} v_i^2\\right)^{1/2}$ with $\\|v\\|_1 \\ge \\|v\\|_2$.\n- Singular values: for any $A \\in \\mathbb{R}^{m \\times n}$ and any $w \\in \\mathbb{R}^{n}$, $\\|A w\\|_2 \\ge \\sigma_{\\min}(A) \\|w\\|_2$, where $\\sigma_{\\min}(A)$ is the smallest singular value of $A$.\n- For any $w \\in \\mathbb{R}^{n}$, $\\|w\\|_1 \\le \\sqrt{n} \\|w\\|_2$.\n\nYou must not use any unproven shortcut formulas or bounds beyond these base facts.\n\nThe adversary’s design space for each test case is the set\n$$\n\\mathcal{E}(s, \\alpha) = \\left\\{ e \\in \\mathbb{R}^{m} : \\|e\\|_0 = s,\\; e_i \\in \\{-\\alpha, +\\alpha\\} \\text{ on the support},\\; e_i = 0 \\text{ off the support} \\right\\}.\n$$\nEnumerate $\\mathcal{E}(s, \\alpha)$ exactly to find the worst-case error.\n\nNo physical or angle units are involved. All outputs must be real numbers. Round all program outputs to six decimal places.\n\nTest suite:\n- Case $1$: $m = 7$, $n = 5$, \n$$\nA = \\begin{bmatrix}\n0.50  -0.10  0.30  0.20  -0.40 \\\\\n0.00  0.40  -0.20  0.50  0.10 \\\\\n0.30  0.60  0.10  -0.30  0.20 \\\\\n-0.20  0.10  0.50  0.10  0.60 \\\\\n0.40  -0.30  -0.10  0.70  -0.20 \\\\\n-0.10  0.50  0.40  -0.20  0.30 \\\\\n0.20  -0.20  0.60  0.40  -0.10\n\\end{bmatrix},\\quad\nx = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad s = 2,\\quad \\alpha = 0.5,\\quad \\lambda = 0.02.\n$$\n- Case $2$: $m = 7$, $n = 5$,\n$$\nA = \\begin{bmatrix}\n0.60  0.10  -0.30  0.25  0.20 \\\\\n0.20  -0.50  0.40  -0.10  0.30 \\\\\n-0.10  0.40  0.20  0.50  -0.20 \\\\\n0.30  0.30  -0.20  0.60  0.10 \\\\\n0.50  -0.20  0.10  -0.30  0.40 \\\\\n0.00  0.60  -0.10  0.20  0.50 \\\\\n0.40  -0.10  0.60  -0.20  0.00\n\\end{bmatrix},\\quad\nx = \\begin{bmatrix} 0.7 \\\\ -0.3 \\\\ 0.0 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix},\\quad s = 3,\\quad \\alpha = 0.3,\\quad \\lambda = 0.02.\n$$\n- Case $3$: Same $A$ and $x$ as Case $1$, with $s = 0$, $\\alpha = 0.0$, $\\lambda = 0.02$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of lists, one per test case, where each inner list is $[\\text{worst\\_error}, \\text{bound}]$ with both values rounded to six decimals. For example: $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$.",
            "solution": "The problem requires an analysis of the worst-case error for the Least Absolute Deviations (LAD)-Lasso estimator under sparse adversarial outliers, the derivation of an analytic bound for this error, and a numerical implementation to compute both quantities.\n\nThe analysis proceeds in three parts: justification of the adversary model, derivation of the error bound, and description of the computational method.\n\n**1. Worst-Case Adversarial Outlier Design**\n\nThe adversary's objective is to choose an outlier vector $e_s$ to maximize the recovery error $\\|\\widehat{x}(A x + e_s) - x\\|_2$. The adversary is constrained by a sparsity budget, $\\|e_s\\|_0 = s$, and an amplitude budget, where non-zero entries satisfy $|e_{s,i}| \\le \\alpha$. The problem specifies reducing the search space to adversaries where non-zero entries have extremal magnitude, i.e., $|e_{s,i}| = \\alpha$.\n\nThis reduction is justified by the structure of the optimization problem that the adversary aims to disrupt. The LAD-Lasso estimator solves:\n$$ \\widehat{x}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\|A z - y\\|_1 + \\lambda \\|z\\|_1 \\right\\} $$\nLet $h = \\widehat{x}(y) - x$ be the recovery error. The LAD-Lasso problem can be expressed in terms of $h$ as finding the minimizer of $\\|A(x+h) - (Ax+e_s)\\|_1 + \\lambda \\|x+h\\|_1$, which simplifies to minimizing $\\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1$. The term $e_s$ acts as an affine perturbation within the $\\ell_1$-norm term.\n\nThe function being minimized is convex. The adversary's problem is to choose $e_s$ from the feasible set $\\mathcal{C} = \\{e \\in \\mathbb{R}^m : \\|e\\|_0 = s, \\|e\\|_\\infty \\le \\alpha\\}$ to maximize the norm of the resulting error, $\\|\\widehat{h}\\|_2$. For a fixed support set for $e_s$, the feasible values form a hypercube $[-\\alpha, \\alpha]^s$. For many classes of parametric optimization problems, including Linear Programs (LPs) as the LAD-Lasso can be formulated, the maximum of a response function over a polyhedral uncertainty set is achieved at a vertex of that set. Intuitively, to maximize the deviation of the solution from the true signal $x$, the adversary should apply the largest possible perturbation. The vertices of the hypercube correspond to extremal values, where each non-zero component of $e_s$ is either $-\\alpha$ or $+\\alpha$. Therefore, it is sufficient to consider the finite set of adversaries $\\mathcal{E}(s, \\alpha)$ to find the worst-case error.\n\n**2. Derivation of the Analytic Error Bound**\n\nWe are tasked to derive an upper bound on the recovery error $\\|\\widehat{x}(y) - x\\|_2$ from first principles. Let $h = \\widehat{x}(y) - x$ be the error vector, where $y = Ax + e_s$.\n\nBy the definition of $\\widehat{x}(y)$ as a minimizer, the value of the objective function at $\\widehat{x}$ must be less than or equal to its value at any other point, including the true signal $x$.\n$$ \\|A\\widehat{x} - y\\|_1 + \\lambda \\|\\widehat{x}\\|_1 \\le \\|Ax - y\\|_1 + \\lambda \\|x\\|_1 $$\nSubstitute $\\widehat{x} = x + h$ and $y = Ax + e_s$:\n$$ \\|A(x+h) - (Ax+e_s)\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|Ax - (Ax+e_s)\\|_1 + \\lambda \\|x\\|_1 $$\n$$ \\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|-e_s\\|_1 + \\lambda \\|x\\|_1 $$\nUsing $\\| -v \\|_1 = \\|v\\|_1$, we simplify to:\n$$ \\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|e_s\\|_1 + \\lambda \\|x\\|_1 $$\nWe employ the reverse triangle inequality, $\\|u-v\\|_1 \\ge \\|u\\|_1 - \\|v\\|_1$, on the term $\\|Ah - e_s\\|_1$:\n$$ \\|Ah\\|_1 - \\|e_s\\|_1 \\le \\|Ah - e_s\\|_1 $$\nSubstituting this into the previous inequality yields:\n$$ \\|Ah\\|_1 - \\|e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|e_s\\|_1 + \\lambda \\|x\\|_1 $$\nRearranging the terms to group $\\|e_s\\|_1$ on one side gives the basic inequality:\n$$ \\|Ah\\|_1 + \\lambda (\\|x+h\\|_1 - \\|x\\|_1) \\le 2\\|e_s\\|_1 $$\nTo obtain a bound independent of $x$, we again use the reverse triangle inequality, $\\|x+h\\|_1 \\ge \\|x\\|_1 - \\|h\\|_1$, which implies $\\|x+h\\|_1 - \\|x\\|_1 \\ge -\\|h\\|_1$. Substituting this creates a looser, but more tractable, inequality:\n$$ \\|Ah\\|_1 - \\lambda \\|h\\|_1 \\le 2\\|e_s\\|_1 $$\nNow, we use the provided base facts to relate the $\\ell_1$-norms to the $\\ell_2$-norm of $h$.\n1.  The $\\ell_1$-norm is an upper bound on the $\\ell_2$-norm: $\\|Ah\\|_1 \\ge \\|Ah\\|_2$.\n2.  The $\\ell_2$-norm of the transformed vector $Ah$ is bounded by the smallest singular value of $A$, $\\sigma_{\\min}(A)$: $\\|Ah\\|_2 \\ge \\sigma_{\\min}(A) \\|h\\|_2$.\n3.  Combining these gives: $\\|Ah\\|_1 \\ge \\sigma_{\\min}(A) \\|h\\|_2$.\n\n4.  The $\\ell_1$-norm of $h$ is related to its $\\ell_2$-norm by: $\\|h\\|_1 \\le \\sqrt{n} \\|h\\|_2$.\n\nSubstituting these relationships into the inequality $\\|Ah\\|_1 - \\lambda \\|h\\|_1 \\le 2\\|e_s\\|_1$:\n$$ \\sigma_{\\min}(A) \\|h\\|_2 - \\lambda \\sqrt{n} \\|h\\|_2 \\le 2\\|e_s\\|_1 $$\nFactoring out $\\|h\\|_2$:\n$$ (\\sigma_{\\min}(A) - \\lambda \\sqrt{n}) \\|h\\|_2 \\le 2\\|e_s\\|_1 $$\nThe $\\ell_1$-norm of the adversary vector $e_s$ is $\\|e_s\\|_1 = \\sum_{i \\in \\text{supp}(e_s)} |e_{s,i}| = s \\alpha$.\nThus, we have:\n$$ (\\sigma_{\\min}(A) - \\lambda \\sqrt{n}) \\|h\\|_2 \\le 2 s \\alpha $$\nProvided that $\\sigma_{\\min}(A) - \\lambda \\sqrt{n}  0$, we can isolate $\\|h\\|_2$ to obtain the final upper bound on the recovery error:\n$$ \\|\\widehat{x}(y) - x\\|_2 \\le \\frac{2 s \\alpha}{\\sigma_{\\min}(A) - \\lambda \\sqrt{n}} $$\nThis bound depends only on the problem parameters $A$, $\\lambda$, $s$, and $\\alpha$, as required.\n\n**3. Computational Strategy**\n\nThe numerical implementation consists of two main parts: computing the worst-case error by enumeration and evaluating the derived analytic bound.\n\n**a. LAD-Lasso via Linear Programming (LP)**\nThe LAD-Lasso optimization problem is solved exactly by reformulating it as an LP. The variable vector for the LP is $X = [z^T, r^T, t^T, u^T]^T \\in \\mathbb{R}^{2n+2m}$. The LP is:\n$$ \\min_{X} c^T X \\quad \\text{subject to} \\quad A_{\\text{eq}}X = b_{\\text{eq}}, \\quad A_{\\text{ub}}X \\le b_{\\text{ub}}, \\quad l \\le X \\le u $$\n- **Objective vector $c$**: $c = [0_n^T, 0_m^T, 1_m^T, \\lambda 1_n^T]^T$.\n- **Equality constraints $A_{\\text{eq}}, b_{\\text{eq}}$**: From $r = Az - y \\iff -Az + r = -y$.\n  $A_{\\text{eq}} = [-A, I_m, 0_{m \\times m}, 0_{m \\times n}]$, $b_{\\text{eq}} = -y$.\n- **Inequality constraints $A_{\\text{ub}}, b_{\\text{ub}}$**: From $-t \\le r \\le t$ and $-u \\le z \\le u$.\n  $r-t \\le 0$, $-r-t \\le 0$, $z-u \\le 0$, $-z-u \\le 0$. These are stacked into a block matrix $A_{\\text{ub}}$ and a zero vector $b_{\\text{ub}}$.\n- **Bounds $l, u$**: $z$ and $r$ are unbounded. $t \\ge 0$ and $u \\ge 0$. This defines the lower and upper bounds for each component of $X$.\n\n**b. Worst-Case Error Calculation**\nThe worst-case error is found by searching over the entire specified adversarial set $\\mathcal{E}(s, \\alpha)$. This is done by:\n1.  Generating all combinations of $s$ indices from $\\{1, \\dots, m\\}$ to serve as the support of $e_s$. The number of such supports is $\\binom{m}{s}$.\n2.  For each support, generating all $2^s$ possible sign patterns for the non-zero entries.\n3.  For each resulting $e_s$, computing $y = Ax+e_s$, solving for $\\widehat{x}(y)$ using the LP solver, calculating the error $\\|\\widehat{x}(y)-x\\|_2$, and keeping track of the maximum error found.\n\nThe total number of adversaries to test is $\\binom{m}{s} 2^s$, which is computationally feasible for the given test cases.\n\n**c. Analytic Bound Calculation**\nThe derived bound $B = \\frac{2 s \\alpha}{\\sigma_{\\min}(A) - \\lambda \\sqrt{n}}$ is computed by:\n1.  Calculating the singular values of $A$ and finding the minimum, $\\sigma_{\\min}(A)$.\n2.  Substituting the values of $s, \\alpha, \\lambda, n$, and $\\sigma_{\\min}(A)$ into the formula. For $s=0$, the bound is $0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\nfrom itertools import combinations, product\n\ndef solve_lad_lasso(A, y, lambda_val):\n    \"\"\"\n    Solves the LAD-Lasso problem by reformulating it as a Linear Program.\n    min ||Az - y||_1 + lambda * ||z||_1\n    \"\"\"\n    m, n = A.shape\n    \n    # LP variable vector: X = [z, r, t, u]\n    # Dimensions: z (n), r (m), t (m), u (n)\n    # Total variables: 2n + 2m\n\n    # Objective function: min sum(t_i) + lambda * sum(u_j)\n    c = np.concatenate([\n        np.zeros(n),           # Cost for z\n        np.zeros(m),           # Cost for r\n        np.ones(m),            # Cost for t\n        lambda_val * np.ones(n) # Cost for u\n    ])\n\n    # Equality constraint: r = Az - y  => -Az + r = -y\n    A_eq = np.hstack([-A, np.eye(m), np.zeros((m, m)), np.zeros((m, n))])\n    b_eq = -y\n\n    # Inequality constraints: A_ub @ X = b_ub\n    # -t = r = t  =>  r - t = 0  and -r - t = 0\n    # -u = z = u  =>  z - u = 0  and -z - u = 0\n    \n    # r - t = 0\n    A_ub1 = np.hstack([np.zeros((m, n)), np.eye(m), -np.eye(m), np.zeros((m, n))])\n    # -r - t = 0\n    A_ub2 = np.hstack([np.zeros((m, n)), -np.eye(m), -np.eye(m), np.zeros((m, n))])\n    # z - u = 0\n    A_ub3 = np.hstack([np.eye(n), np.zeros((n, m)), np.zeros((n, m)), -np.eye(n)])\n    # -z - u = 0\n    A_ub4 = np.hstack([-np.eye(n), np.zeros((n, m)), np.zeros((n, m)), -np.eye(n)])\n    \n    A_ub = np.vstack([A_ub1, A_ub2, A_ub3, A_ub4])\n    b_ub = np.zeros(A_ub.shape[0])\n\n    # Variable bounds: z, r are free; t, u are non-negative.\n    bounds = ([(None, None)] * (n + m)) + ([(0, None)] * (m + n))\n\n    # Solve the LP\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n\n    if not res.success:\n        # The problem formulation should guarantee a solution exists.\n        # This case is included for robustness but not expected to be hit.\n        raise RuntimeError(\"LP solver failed to find a solution.\")\n\n    # Extract the solution for z\n    z = res.x[:n]\n    return z\n\ndef calculate_worst_error(A, x, s, alpha, lambda_val):\n    \"\"\"\n    Finds the worst-case recovery error by enumerating all adversaries.\n    \"\"\"\n    m, n = A.shape\n    \n    # Base case with no adversary\n    if s == 0:\n        e_s = np.zeros(m)\n        y = A @ x + e_s\n        x_hat = solve_lad_lasso(A, y, lambda_val)\n        return np.linalg.norm(x_hat - x)\n\n    max_sq_error = 0.0\n    indices = np.arange(m)\n\n    # Iterate over all possible supports for the outlier vector e_s\n    for support in combinations(indices, s):\n        # Iterate over all sign patterns (+/- alpha) on the support\n        for signs in product([-1, 1], repeat=s):\n            e_s = np.zeros(m)\n            support_list = list(support)\n            e_s[support_list] = np.array(signs) * alpha\n            \n            y = A @ x + e_s\n            x_hat = solve_lad_lasso(A, y, lambda_val)\n            \n            sq_error = np.sum((x_hat - x)**2)\n            if sq_error > max_sq_error:\n                max_sq_error = sq_error\n                \n    return np.sqrt(max_sq_error)\n\ndef calculate_bound(A, n, s, alpha, lambda_val):\n    \"\"\"\n    Calculates the derived analytic upper bound on the error.\n    \"\"\"\n    if s == 0:\n        return 0.0\n        \n    singular_values = np.linalg.svd(A, compute_uv=False)\n    sigma_min_A = np.min(singular_values)\n    \n    denominator = sigma_min_A - lambda_val * np.sqrt(n)\n    \n    if denominator = 1e-9: # Add a small tolerance for stability\n        return float('inf')\n        \n    bound = (2 * s * alpha) / denominator\n    return bound\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.50, -0.10, 0.30, 0.20, -0.40],\n                [0.00, 0.40, -0.20, 0.50, 0.10],\n                [0.30, 0.60, 0.10, -0.30, 0.20],\n                [-0.20, 0.10, 0.50, 0.10, 0.60],\n                [0.40, -0.30, -0.10, 0.70, -0.20],\n                [-0.10, 0.50, 0.40, -0.20, 0.30],\n                [0.20, -0.20, 0.60, 0.40, -0.10]\n            ]),\n            \"x\": np.array([1.0, 0.0, 0.5, 0.0, 0.0]),\n            \"s\": 2, \"alpha\": 0.5, \"lambda\": 0.02\n        },\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.60, 0.10, -0.30, 0.25, 0.20],\n                [0.20, -0.50, 0.40, -0.10, 0.30],\n                [-0.10, 0.40, 0.20, 0.50, -0.20],\n                [0.30, 0.30, -0.20, 0.60, 0.10],\n                [0.50, -0.20, 0.10, -0.30, 0.40],\n                [0.00, 0.60, -0.10, 0.20, 0.50],\n                [0.40, -0.10, 0.60, -0.20, 0.00]\n            ]),\n            \"x\": np.array([0.7, -0.3, 0.0, 0.5, 0.0]),\n            \"s\": 3, \"alpha\": 0.3, \"lambda\": 0.02\n        },\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.50, -0.10, 0.30, 0.20, -0.40],\n                [0.00, 0.40, -0.20, 0.50, 0.10],\n                [0.30, 0.60, 0.10, -0.30, 0.20],\n                [-0.20, 0.10, 0.50, 0.10, 0.60],\n                [0.40, -0.30, -0.10, 0.70, -0.20],\n                [-0.10, 0.50, 0.40, -0.20, 0.30],\n                [0.20, -0.20, 0.60, 0.40, -0.10]\n            ]),\n            \"x\": np.array([1.0, 0.0, 0.5, 0.0, 0.0]),\n            \"s\": 0, \"alpha\": 0.0, \"lambda\": 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, x, s, alpha, lambda_val, n = case[\"A\"], case[\"x\"], case[\"s\"], case[\"alpha\"], case[\"lambda\"], case[\"n\"]\n        \n        worst_error = calculate_worst_error(A, x, s, alpha, lambda_val)\n        bound = calculate_bound(A, n, s, alpha, lambda_val)\n        \n        results.append([round(worst_error, 6), round(bound, 6)])\n\n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]:.6f},{results[0][1]:.6f}],[{results[1][0]:.6f},{results[1][1]:.6f}],[{results[2][0]:.6f},{results[2][1]:.6f}]]\")\n\nsolve()\n```"
        }
    ]
}