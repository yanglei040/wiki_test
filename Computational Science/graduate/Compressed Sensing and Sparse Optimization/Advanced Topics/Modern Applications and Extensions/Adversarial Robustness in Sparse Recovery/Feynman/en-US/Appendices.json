{
    "hands_on_practices": [
        {
            "introduction": "Before delving into complex recovery algorithms, it's crucial to understand the core challenge an adversary poses: corrupting the evidence used to identify a signal's structure. This first practice  examines a simple yet fundamental method for support recovery—thresholding the correlations between measurements and sensing vectors. By deriving a detection threshold under a bounded adversarial attack, you will gain first-hand insight into how a matrix's mutual coherence, represented as $\\mu(A)$, dictates its vulnerability and how to design a basic defense.",
            "id": "3430326",
            "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with unit $\\ell_{2}$-norm columns and mutual coherence $\\mu(A) \\triangleq \\max_{i \\neq j} | \\langle a_{i}, a_{j} \\rangle |$, where $a_{i}$ denotes the $i$-th column of $A$. Let the unknown signal $x^{\\star} \\in \\mathbb{R}^{n}$ be $k$-sparse with support $S \\subset \\{1,\\dots,n\\}$, minimum nonzero magnitude $m_{\\star} \\triangleq \\min_{i \\in S} |x^{\\star}_{i}|$, and a known a priori bound on its $\\ell_{1}$-norm $\\|x^{\\star}\\|_{1} \\leq L$, where $L  0$ is given. Measurements are noiseless and given by $y = A x^{\\star}$.\n\nA simple support detector computes the correlation proxy $g \\triangleq A^{\\mathsf{T}} y = A^{\\mathsf{T}} A x^{\\star}$ and then selects indices by hard thresholding. An adversary perturbs the proxy before thresholding, yielding the attacked proxy $v \\triangleq g + e$, where the adversary is $\\ell_{\\infty}$-bounded: $\\|e\\|_{\\infty} \\leq \\eta$, with given $\\eta \\geq 0$. The detector outputs the support estimate\n$$\nT(\\tau) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |v_{i}|  \\tau \\},\n$$\nfor a threshold $\\tau  0$ to be designed.\n\nStarting only from the definitions of mutual coherence and the triangle inequality, and without assuming any specific distribution of the nonzero entries of $x^{\\star}$ beyond the given bounds, derive a deterministic threshold $\\tau$ as a function of $\\mu(A)$, $L$, and $\\eta$ that guarantees $T(\\tau) = S$ provided that $m_{\\star}$ is sufficiently large relative to $\\mu(A)$, $L$, and $\\eta$. Your derivation should explicitly bound the largest possible spurious correlation off-support and the smallest possible attenuated correlation on-support under worst-case signs and adversarial perturbations, and then choose $\\tau$ to separate these bounds.\n\nGive your final answer as a single closed-form symbolic expression for the threshold $\\tau$ in terms of $\\mu(A)$, $L$, and $\\eta$. Do not provide conditions or inequalities in the final answer. No rounding is required.",
            "solution": "The problem requires the derivation of a deterministic threshold $\\tau$, as a function of the mutual coherence $\\mu(A)$, the $\\ell_1$-norm bound $L$, and the adversarial perturbation bound $\\eta$. This threshold must ensure that the support estimate $T(\\tau) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |v_{i}|  \\tau \\}$ correctly identifies the true support $S$ of a $k$-sparse signal $x^{\\star}$. The condition for perfect support recovery, $T(\\tau) = S$, mandates that two criteria are met: firstly, for every index $i$ within the true support $S$, the inequality $|v_i|  \\tau$ must hold; secondly, for every index $j$ outside the support, $j \\in S^c$, the inequality $|v_j| \\leq \\tau$ must hold.\n\nTo satisfy these conditions for any valid signal $x^{\\star}$ and perturbation $e$ that conform to the given bounds, we must choose $\\tau$ such that it separates the worst-case (smallest) on-support magnitude from the worst-case (largest) off-support magnitude. That is, we seek a $\\tau$ such that:\n$$\n\\max_{j \\notin S, \\|x^\\star\\|_1 \\le L, \\|e\\|_\\infty \\le \\eta} |v_j| \\leq \\tau  \\min_{i \\in S, \\|x^\\star\\|_1 \\le L, \\|e\\|_\\infty \\le \\eta} |v_i|\n$$\nThe problem states we can assume the minimum non-zero magnitude of the signal, $m_{\\star}$, is sufficiently large to ensure such a separation exists. Our task is to find an explicit expression for $\\tau$ that depends only on $\\mu(A)$, $L$, and $\\eta$.\n\nThe attacked proxy vector is $v = A^{\\mathsf{T}} A x^{\\star} + e$. The $i$-th component is given by:\n$$\nv_i = (A^{\\mathsf{T}} A x^{\\star})_i + e_i = \\sum_{l=1}^{n} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i\n$$\nwhere $a_i$ is the $i$-th column of $A$. Since the columns of $A$ have unit $\\ell_2$-norm, we have $\\langle a_i, a_i \\rangle = \\|a_i\\|_{2}^{2} = 1$.\n\nFirst, we derive an upper bound on the magnitude of the off-support components, $|v_j|$ for $j \\notin S$.\nFor an index $j \\notin S$, the signal component $x^{\\star}_j$ is $0$. The sum is therefore restricted to the support set $S$:\n$$\nv_j = \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l + e_j\n$$\nUsing the triangle inequality, we can bound the magnitude of $v_j$:\n$$\n|v_j| = \\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l + e_j \\right| \\leq \\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l \\right| + |e_j|\n$$\nApplying the triangle inequality to the sum and using the definition of mutual coherence, $|\\langle a_j, a_l \\rangle| \\leq \\mu(A)$ for $j \\neq l$ (which is always true for $l \\in S$ and $j \\notin S$), we obtain:\n$$\n\\left| \\sum_{l \\in S} \\langle a_j, a_l \\rangle x^{\\star}_l \\right| \\leq \\sum_{l \\in S} |\\langle a_j, a_l \\rangle| |x^{\\star}_l| \\leq \\sum_{l \\in S} \\mu(A) |x^{\\star}_l| = \\mu(A) \\sum_{l \\in S} |x^{\\star}_l|\n$$\nThe sum $\\sum_{l \\in S} |x^{\\star}_l|$ is the $\\ell_1$-norm of the signal, $\\|x^{\\star}\\|_1$. The adversarial perturbation is bounded by $|e_j| \\leq \\|e\\|_{\\infty} \\leq \\eta$.\nCombining these results, we get:\n$$\n|v_j| \\leq \\mu(A) \\|x^{\\star}\\|_1 + \\eta\n$$\nTo obtain a worst-case bound, we use the given prior information that $\\|x^{\\star}\\|_1 \\leq L$. The right-hand side is maximized when $\\|x^{\\star}\\|_1$ reaches its maximum allowed value, $L$. Thus, for any $j \\notin S$, we have the deterministic upper bound:\n$$\n|v_j| \\leq \\mu(A) L + \\eta\n$$\nLet us denote this worst-case upper bound for off-support components as $B_{off} = \\mu(A) L + \\eta$.\n\nNext, we derive a lower bound on the magnitude of the on-support components, $|v_i|$ for $i \\in S$. For an index $i \\in S$, we separate the term corresponding to $x^{\\star}_i$:\n$$\nv_i = \\langle a_i, a_i \\rangle x^{\\star}_i + \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i = x^{\\star}_i + \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i\n$$\nUsing the reverse triangle inequality, $|a+b| \\geq |a| - |b|$, we have:\n$$\n|v_i| \\geq |x^{\\star}_i| - \\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i \\right|\n$$\nApplying the triangle inequality to the subtracted term:\n$$\n\\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l + e_i \\right| \\leq \\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l \\right| + |e_i|\n$$\nThe magnitude of the sum is bounded using mutual coherence:\n$$\n\\left| \\sum_{l \\in S, l \\neq i} \\langle a_i, a_l \\rangle x^{\\star}_l \\right| \\leq \\sum_{l \\in S, l \\neq i} |\\langle a_i, a_l \\rangle| |x^{\\star}_l| \\leq \\mu(A) \\sum_{l \\in S, l \\neq i} |x^{\\star}_l|\n$$\nThe sum can be expressed as $\\sum_{l \\in S, l \\neq i} |x^{\\star}_l| = \\|x^{\\star}\\|_1 - |x^{\\star}_i|$. The worst-case (largest) interference for an on-support element is thus bounded by $\\mu(A) (\\|x^{\\star}\\|_1 - |x^{\\star}_i|) + \\eta$. This leads to a lower bound on $|v_i|$:\n$$\n|v_i| \\geq |x^{\\star}_i| - \\left( \\mu(A) (\\|x^{\\star}\\|_1 - |x^{\\star}_i|) + \\eta \\right) = |x^{\\star}_i|(1 + \\mu(A)) - \\mu(A)\\|x^{\\star}\\|_1 - \\eta\n$$\nTo find a general lower bound, we must consider the worst-case choice of $x^\\star$. The expression is minimized when $\\|x^{\\star}\\|_1$ is maximal (up to $L$) and $|x^{\\star}_i|$ is minimal (down to $m_\\star$). Therefore, for any $i \\in S$:\n$$\n|v_i| \\geq m_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta\n$$\nLet us denote this worst-case lower bound for on-support components as $B_{on} = m_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta$.\n\nOur goal is to find a threshold $\\tau$ such that $|v_j| \\leq \\tau$ for all $j \\notin S$ and $|v_i|  \\tau$ for all $i \\in S$. These conditions are guaranteed if we select $\\tau$ such that $B_{off} \\leq \\tau  B_{on}$.\nThe problem asks for a threshold $\\tau$ that is a function of only $\\mu(A)$, $L$, and $\\eta$. The bound $B_{off} = \\mu(A) L + \\eta$ satisfies this requirement, whereas $B_{on}$ depends on $m_{\\star}$. A suitable and direct choice for the threshold is to set it equal to the maximum possible magnitude of an off-support component.\nLet us choose the threshold $\\tau = B_{off}$:\n$$\n\\tau = \\mu(A) L + \\eta\n$$\nWith this choice, the condition for off-support indices, $|v_j| \\leq \\tau$, is guaranteed to hold because we have rigorously shown that $|v_j| \\leq B_{off} = \\tau$ for all $j \\notin S$. Thus, no off-support index will satisfy $|v_j|  \\tau$ and be incorrectly selected.\n\nFor the on-support indices, we must ensure that $|v_i|  \\tau$. Since we know $|v_i| \\geq B_{on}$, this condition is satisfied if the lower bound $B_{on}$ is strictly greater than our chosen threshold $\\tau$:\n$$\nm_{\\star}(1 + \\mu(A)) - \\mu(A)L - \\eta  \\mu(A) L + \\eta\n$$\n$$\nm_{\\star}(1 + \\mu(A))  2\\mu(A)L + 2\\eta\n$$\n$$\nm_{\\star}  \\frac{2\\mu(A)L + 2\\eta}{1 + \\mu(A)}\n$$\nThis inequality is the precise condition that \"$m_{\\star}$ is sufficiently large\". As the problem statement permits us to assume such a condition holds, the choice of $\\tau = \\mu(A) L + \\eta$ is valid and guarantees $T(\\tau) = S$ under this condition. This expression for $\\tau$ is a closed-form symbolic expression in terms of the required parameters.",
            "answer": "$$\n\\boxed{\\mu(A)L + \\eta}\n$$"
        },
        {
            "introduction": "Real-world recovery is often performed iteratively, where each step can either refine the solution or amplify adversarial noise. This exercise  investigates this dynamic interplay within the popular Iterative Hard Thresholding (IHT) algorithm. You will discover the perhaps counter-intuitive principle that running an algorithm to full convergence can be detrimental, and that early stopping can act as a powerful form of regularization against adversarial error amplification.",
            "id": "3430320",
            "problem": "Consider a standard compressed sensing measurement model with an unknown $k$-sparse signal $x^{\\star} \\in \\mathbb{R}^{n}$, a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, and adversarial measurement corruption $\\delta \\in \\mathbb{R}^{m}$ satisfying $\\|\\delta\\|_{2} \\le \\rho$, so that the measurements are $y = A x^{\\star} + \\delta$. Let the recovery be performed by Iterative Hard Thresholding (IHT), that is, the recursion\n$$\nx_{t+1} \\;=\\; H_{k}\\Big(x_{t} \\;+\\; \\mu \\, A^{\\top}(y - A x_{t})\\Big),\n$$\nwhere $H_{k}(\\cdot)$ is the hard-thresholding operator to $k$ largest-magnitude coordinates and $\\mu  0$ is a fixed stepsize. Assume the following scientifically standard and widely used conditions.\n\n- The Restricted Isometry Property (RIP) of order $k$ holds with constant $\\delta_{k} \\in (0,1)$, so that for every $k$-sparse vector $z$, one has $(1 - \\delta_{k}) \\|z\\|_{2}^{2} \\le \\|A z\\|_{2}^{2} \\le (1 + \\delta_{k}) \\|z\\|_{2}^{2}$.\n\n- There is a regime (possibly after a warm start that you may ignore in the analysis) in which the support $T := \\mathrm{supp}(x^{\\star})$ is correctly identified and remains fixed; in that regime, $H_{k}$ acts as the orthogonal projector onto the coordinates $T$. You should analyze the iterations entirely within this fixed-support regime.\n\n- Denote $A_{T} \\in \\mathbb{R}^{m \\times k}$ the restriction of $A$ to the columns indexed by $T$, and assume the stepsize satisfies $0  \\mu  \\frac{2}{1 + \\delta_{k}}$. Let $L := 1 + \\delta_{k}$ and $m := 1 - \\delta_{k}$, and define the contraction factor\n$$\n\\alpha \\;:=\\; \\max\\{\\,|1 - \\mu m|,\\; |1 - \\mu L|\\,\\},\n$$\nwhich is guaranteed to satisfy $0  \\alpha  1$ under the stated stepsize bound.\n\nDefine the adversarial error amplification at iteration $t$ by\n$$\nG(t) \\;:=\\; \\sup_{\\|\\delta\\|_{2} \\le \\rho} \\frac{\\|x_{t}(\\delta) - x_{t}(0)\\|_{2}}{\\rho},\n$$\nwhere $x_{t}(\\delta)$ denotes the IHT iterate produced when the measurements are $y = A x^{\\star} + \\delta$, and $x_{t}(0)$ denotes the iterate produced when $\\delta = 0$. This $G(t)$ quantifies the worst-case $\\ell_{2}$-sensitivity of the $t$-th iterate to adversarial perturbations of the measurements, and its steady-state limit $G(\\infty)$ quantifies the full-convergence adversarial amplification.\n\nYour tasks:\n\n- Starting from the fixed-support linear recursion and the Restricted Isometry Property, derive a rigorous upper bound on $G(t)$ of the form\n$$\nG(t) \\;\\le\\; \\mu \\,\\|A_{T}\\|_{2} \\sum_{i=0}^{t-1} \\alpha^{i},\n$$\nand then use $\\|A_{T}\\|_{2} \\le \\sqrt{1 + \\delta_{k}}$ to express $G(t)$ in closed form in terms of $\\alpha$, $\\mu$, and $\\delta_{k}$. Conclude conditions under which $G(t)  G(\\infty)$ for any finite $t$.\n\n- To quantify the tradeoff between optimization bias and adversarial amplification, fix a desired fraction $\\kappa \\in (0,1)$ and define $t_{\\max}(\\kappa)$ to be the largest integer $t$ such that $G(t) \\le \\kappa \\, G(\\infty)$. Compute a closed-form expression for $t_{\\max}(\\kappa)$ in terms of $\\alpha$ and $\\kappa$ only.\n\nAnswer specification: Your final answer must consist solely of the exact analytic expression you obtain for $t_{\\max}(\\kappa)$ as a function of $\\alpha$ and $\\kappa$. No numerical rounding is required.",
            "solution": "The user-provided problem is rigorously validated and confirmed to be a valid, well-posed, and scientifically grounded problem in the field of compressed sensing and sparse optimization. All assumptions and definitions are standard in the literature on Iterative Hard Thresholding (IHT) and its robustness analysis. The problem is self-contained and provides a clear path to a unique, verifiable solution.\n\nWe begin by analyzing the IHT recursion under the specified fixed-support assumption. The problem states that the support of the iterates, $T := \\mathrm{supp}(x^{\\star})$, is correctly identified and remains fixed. In this regime, the hard-thresholding operator $H_{k}(\\cdot)$ acts as an orthogonal projection onto the subspace of vectors supported on $T$. Let $x_{t,T}$ denote the restriction of the vector $x_t$ to the coordinates in $T$, which is a $k$-dimensional vector. Similarly, let $A_T$ be the matrix $A$ restricted to the columns indexed by $T$. The recursion for the non-zero components of the iterates can be written as:\n$$\nx_{t+1, T} = x_{t, T} + \\mu A_T^{\\top}(y - A_T x_{t, T})\n$$\nWe are interested in the sensitivity of the iterates to the adversarial perturbation $\\delta$. Let $x_t(\\delta)$ be the iterate at step $t$ when the measurements are $y = Ax^{\\star} + \\delta$, and let $x_t(0)$ be the iterate when $\\delta=0$, i.e., $y = Ax^{\\star}$. Both sequences of iterates are assumed to be supported on $T$. Let their restrictions to $T$ be denoted $x_{t,T}(\\delta)$ and $x_{t,T}(0)$, respectively.\n\nThe recursions for these two sequences are:\n$$\nx_{t+1, T}(\\delta) = x_{t, T}(\\delta) + \\mu A_T^{\\top}(A_T x^{\\star}_T + \\delta - A_T x_{t, T}(\\delta))\n$$\n$$\nx_{t+1, T}(0) = x_{t, T}(0) + \\mu A_T^{\\top}(A_T x^{\\star}_T - A_T x_{t, T}(0))\n$$\nLet the error vector on the support set be $e_t := x_{t, T}(\\delta) - x_{t, T}(0)$. Subtracting the second equation from the first gives the dynamics of this error:\n$$\ne_{t+1} = e_t + \\mu A_T^{\\top}(\\delta - A_T(x_{t, T}(\\delta) - x_{t, T}(0)))\n$$\n$$\ne_{t+1} = e_t - \\mu A_T^{\\top} A_T e_t + \\mu A_T^{\\top} \\delta\n$$\n$$\ne_{t+1} = (I_k - \\mu A_T^{\\top} A_T) e_t + \\mu A_T^{\\top} \\delta\n$$\nHere, $I_k$ is the $k \\times k$ identity matrix. We analyze the growth of the $\\ell_2$-norm of this error. Using the triangle inequality:\n$$\n\\|e_{t+1}\\|_2 \\le \\|(I_k - \\mu A_T^{\\top} A_T) e_t\\|_2 + \\|\\mu A_T^{\\top} \\delta\\|_2\n$$\n$$\n\\|e_{t+1}\\|_2 \\le \\|I_k - \\mu A_T^{\\top} A_T\\|_2 \\|e_t\\|_2 + \\mu \\|A_T^{\\top}\\|_2 \\|\\delta\\|_2\n$$\nThe Restricted Isometry Property (RIP) of order $k$ states that for any $k$-sparse vector $z$, $(1-\\delta_k)\\|z\\|_2^2 \\le \\|Az\\|_2^2 \\le (1+\\delta_k)\\|z\\|_2^2$. This is equivalent to stating that all eigenvalues $\\lambda_i$ of the Gram matrix $A_T^{\\top}A_T$ are bounded as $1-\\delta_k \\le \\lambda_i \\le 1+\\delta_k$. Let $m=1-\\delta_k$ and $L=1+\\delta_k$.\nThe matrix $I_k - \\mu A_T^{\\top} A_T$ is symmetric, so its $\\ell_2$-norm is its spectral radius (maximum absolute eigenvalue). The eigenvalues are $1 - \\mu\\lambda_i$. With the given bounds on $\\lambda_i$, we have $1-\\mu L \\le 1-\\mu\\lambda_i \\le 1-\\mu m$.\nThe operator norm is therefore $\\|I_k - \\mu A_T^{\\top} A_T\\|_2 = \\max\\{|1-\\mu m|, |1-\\mu L|\\}$, which is defined as the contraction factor $\\alpha$. The problem states that the stepsize choice ensures $\\alpha \\in (0,1)$.\n\nThe norm recursion becomes:\n$$\n\\|e_{t+1}\\|_2 \\le \\alpha \\|e_t\\|_2 + \\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\nAssuming a common initialization $x_0(\\delta)=x_0(0)$, we have $e_0=0$. Unrolling the recursion:\n$$\n\\|e_1\\|_2 \\le \\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\n$$\n\\|e_2\\|_2 \\le \\alpha\\|e_1\\|_2 + \\mu \\|A_T\\|_2 \\|\\delta\\|_2 \\le (\\alpha+1)\\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\nBy induction, for any $t \\ge 1$:\n$$\n\\|e_t\\|_2 \\le \\left(\\sum_{i=0}^{t-1} \\alpha^i\\right) \\mu \\|A_T\\|_2 \\|\\delta\\|_2\n$$\nThe problem defines the adversarial error amplification as $G(t) = \\sup_{\\|\\delta\\|_{2} \\le \\rho} \\frac{\\|x_{t}(\\delta) - x_{t}(0)\\|_{2}}{\\rho}$. Since the iterates are supported on $T$, $\\|x_{t}(\\delta) - x_{t}(0)\\|_{2} = \\|e_t\\|_2$.\n$$\n\\frac{\\|e_t\\|_2}{\\rho} \\le \\left(\\sum_{i=0}^{t-1} \\alpha^i\\right) \\mu \\|A_T\\|_2 \\frac{\\|\\delta\\|_2}{\\rho}\n$$\nTaking the supremum over all $\\|\\delta\\|_2 \\le \\rho$ yields the desired upper bound on $G(t)$:\n$$\nG(t) \\le \\mu \\|A_T\\|_2 \\sum_{i=0}^{t-1} \\alpha^i\n$$\nThis matches the form requested in the first task. To find the closed-form expression, we use the geometric series formula $\\sum_{i=0}^{t-1} \\alpha^i = \\frac{1-\\alpha^t}{1-\\alpha}$ and the RIP-based bound $\\|A_T\\|_2 \\le \\sqrt{\\sigma_{\\max}(A_T^\\top A_T)} \\le \\sqrt{1+\\delta_k}$. This gives the upper bound:\n$$\nG(t) \\le \\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha}\n$$\nThe problem's structure implies that for subsequent tasks, we identify $G(t)$ with this derived upper bound. The steady-state amplification $G(\\infty)$ is the limit of this bound as $t \\to \\infty$. Since $\\alpha \\in (0,1)$, $\\alpha^t \\to 0$ as $t \\to \\infty$.\n$$\nG(\\infty) = \\lim_{t\\to\\infty} \\left( \\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha} \\right) = \\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha}\n$$\nTo determine when $G(t)  G(\\infty)$, we compare the expressions:\n$$\n\\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha}  \\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha}\n$$\nCanceling the positive common factors gives $1-\\alpha^t  1$, or $-\\alpha^t  0$. As $\\alpha \\in (0,1)$, $\\alpha^t$ is positive for any finite $t$. Thus, this inequality holds for all finite $t \\ge 1$. For $t=0$, $G(0)=0$ (since $e_0=0$) and $G(\\infty)0$, so the inequality also holds. Therefore, $G(t)  G(\\infty)$ for any finite $t$.\n\nFor the second task, we need to find the largest integer $t_{\\max}(\\kappa)$ such that $G(t) \\le \\kappa G(\\infty)$, for a given $\\kappa \\in (0,1)$. Using the expressions for $G(t)$ and $G(\\infty)$:\n$$\n\\mu \\sqrt{1+\\delta_k} \\frac{1-\\alpha^t}{1-\\alpha} \\le \\kappa \\left( \\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha} \\right)\n$$\nThe positive term $\\mu \\frac{\\sqrt{1+\\delta_k}}{1-\\alpha}$ cancels from both sides, leaving a simple inequality in terms of $\\alpha$, $\\kappa$, and $t$:\n$$\n1 - \\alpha^t \\le \\kappa\n$$\nRearranging the terms, we get:\n$$\n1 - \\kappa \\le \\alpha^t\n$$\nTo solve for $t$, we take the natural logarithm of both sides. Since $\\alpha \\in (0,1)$, its logarithm $\\ln(\\alpha)$ is negative. Therefore, when we divide the inequality by $\\ln(\\alpha)$, we must reverse the inequality's direction.\n$$\n\\ln(1-\\kappa) \\le t \\ln(\\alpha)\n$$\n$$\nt \\le \\frac{\\ln(1-\\kappa)}{\\ln(\\alpha)}\n$$\nNote that since $\\kappa \\in (0,1)$, $1-\\kappa$ is also in $(0,1)$, making $\\ln(1-\\kappa)$ negative. The ratio of two negative numbers is positive, as expected for $t$.\nThe problem asks for $t_{\\max}(\\kappa)$, the largest integer $t$ that satisfies this condition. This is given by the floor of the expression on the right-hand side.\n$$\nt_{\\max}(\\kappa) = \\left\\lfloor \\frac{\\ln(1 - \\kappa)}{\\ln(\\alpha)} \\right\\rfloor\n$$\nThis provides the required closed-form expression for $t_{\\max}(\\kappa)$ in terms of only $\\alpha$ and $\\kappa$.",
            "answer": "$$\n\\boxed{\\left\\lfloor \\frac{\\ln(1 - \\kappa)}{\\ln(\\alpha)} \\right\\rfloor}\n$$"
        },
        {
            "introduction": "Theory provides guidance, but practical robustness requires implementation and empirical verification. In this final practice , you will bridge theory and computation by tackling the Least Absolute Deviations (LAD)-Lasso estimator, known for its resilience to large, sparse outliers. Your task is to not only derive a theoretical bound on the worst-case error but also to implement the estimator as a linear program, design the most potent adversarial attack, and computationally determine the true maximum error, offering a clear view of the gap between theoretical guarantees and practical performance.",
            "id": "3430330",
            "problem": "Consider the measurement model in compressed sensing with adversarial outliers. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known sensing matrix, $x \\in \\mathbb{R}^{n}$ be a fixed but unknown signal, and $e_s \\in \\mathbb{R}^{m}$ be an adversarial outlier vector with $\\|e_s\\|_0 = s$. The observation is $y = A x + e_s$. For recovery, use the Least Absolute Deviations (LAD)-Lasso estimator, defined as the solution to the convex optimization problem\n$$\n\\widehat{x}(y) \\in \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\|A z - y\\|_1 + \\lambda \\|z\\|_1 \\right\\},\n$$\nwhere $\\lambda  0$ is a regularization parameter. Assume the adversary is constrained by an amplitude budget $\\alpha  0$ so that each nonzero entry of $e_s$ satisfies $|e_{s,i}| \\le \\alpha$. The adversary chooses the support and signs of $e_s$ to maximize the recovery error $\\|\\widehat{x}(y) - x\\|_2$.\n\nYour tasks are:\n- Construct, for each test case, a worst-case adversarial outlier vector $e_s$ with $\\|e_s\\|_0 = s$ and $|e_{s,i}| = \\alpha$ on its support, that maximizes $\\|\\widehat{x}(A x + e_s) - x\\|_2$ under the LAD-Lasso estimator. Justify why extremal amplitudes $|e_{s,i}| = \\alpha$ are sufficient for worst-case design under the given constraint.\n- Derive from first principles an analytic upper bound on the achievable error inflation $\\|\\widehat{x}(A x + e_s) - x\\|_2$, expressed in terms of $A$, $\\lambda$, $s$, and $\\alpha$. Use only fundamental inequalities, norm relations, and singular value properties. The bound must be a function that can be evaluated numerically from $A$, $\\lambda$, $s$, and $\\alpha$.\n- Implement a program that computes, for each test case, both the worst-case recovery error over all sparse adversaries with the specified $s$ and amplitude budget $\\alpha$, and the derived analytic bound. Your implementation of the LAD-Lasso must be exact by reformulating the optimization into a linear program.\n\nThe LAD-Lasso problem can be reformulated using auxiliary variables $r \\in \\mathbb{R}^{m}$, $t \\in \\mathbb{R}^{m}$, and $u \\in \\mathbb{R}^{n}$ to handle absolute values:\n$$\n\\min_{z, r, t, u} \\sum_{i=1}^{m} t_i + \\lambda \\sum_{j=1}^{n} u_j \\quad \\text{subject to} \\quad r = A z - y,\\; -t \\le r \\le t,\\; -u \\le z \\le u,\\; t \\ge 0,\\; u \\ge 0.\n$$\nThis is a linear program.\n\nYou must start your derivation from the following base facts and definitions:\n- The triangle inequality for norms and the definition of $\\ell_1$ and $\\ell_2$ norms: for any $v \\in \\mathbb{R}^{k}$, $\\|v\\|_1 = \\sum_{i=1}^{k} |v_i|$ and $\\|v\\|_2 = \\left(\\sum_{i=1}^{k} v_i^2\\right)^{1/2}$ with $\\|v\\|_1 \\ge \\|v\\|_2$.\n- Singular values: for any $A \\in \\mathbb{R}^{m \\times n}$ and any $w \\in \\mathbb{R}^{n}$, $\\|A w\\|_2 \\ge \\sigma_{\\min}(A) \\|w\\|_2$, where $\\sigma_{\\min}(A)$ is the smallest singular value of $A$.\n- For any $w \\in \\mathbb{R}^{n}$, $\\|w\\|_1 \\le \\sqrt{n} \\|w\\|_2$.\n\nYou must not use any unproven shortcut formulas or bounds beyond these base facts.\n\nThe adversary’s design space for each test case is the set\n$$\n\\mathcal{E}(s, \\alpha) = \\left\\{ e \\in \\mathbb{R}^{m} : \\|e\\|_0 = s,\\; e_i \\in \\{-\\alpha, +\\alpha\\} \\text{ on the support},\\; e_i = 0 \\text{ off the support} \\right\\}.\n$$\nEnumerate $\\mathcal{E}(s, \\alpha)$ exactly to find the worst-case error.\n\nNo physical or angle units are involved. All outputs must be real numbers. Round all program outputs to six decimal places.\n\nTest suite:\n- Case $1$: $m = 7$, $n = 5$, \n$$\nA = \\begin{bmatrix}\n0.50  -0.10  0.30  0.20  -0.40 \\\\\n0.00  0.40  -0.20  0.50  0.10 \\\\\n0.30  0.60  0.10  -0.30  0.20 \\\\\n-0.20  0.10  0.50  0.10  0.60 \\\\\n0.40  -0.30  -0.10  0.70  -0.20 \\\\\n-0.10  0.50  0.40  -0.20  0.30 \\\\\n0.20  -0.20  0.60  0.40  -0.10\n\\end{bmatrix},\\quad\nx = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad s = 2,\\quad \\alpha = 0.5,\\quad \\lambda = 0.02.\n$$\n- Case $2$: $m = 7$, $n = 5$,\n$$\nA = \\begin{bmatrix}\n0.60  0.10  -0.30  0.25  0.20 \\\\\n0.20  -0.50  0.40  -0.10  0.30 \\\\\n-0.10  0.40  0.20  0.50  -0.20 \\\\\n0.30  0.30  -0.20  0.60  0.10 \\\\\n0.50  -0.20  0.10  -0.30  0.40 \\\\\n0.00  0.60  -0.10  0.20  0.50 \\\\\n0.40  -0.10  0.60  -0.20  0.00\n\\end{bmatrix},\\quad\nx = \\begin{bmatrix} 0.7 \\\\ -0.3 \\\\ 0.0 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix},\\quad s = 3,\\quad \\alpha = 0.3,\\quad \\lambda = 0.02.\n$$\n- Case $3$: Same $A$ and $x$ as Case $1$, with $s = 0$, $\\alpha = 0.0$, $\\lambda = 0.02$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of lists, one per test case, where each inner list is $[\\text{worst\\_error}, \\text{bound}]$ with both values rounded to six decimals. For example: $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$.",
            "solution": "The problem requires an analysis of the worst-case error for the Least Absolute Deviations (LAD)-Lasso estimator under sparse adversarial outliers, the derivation of an analytic bound for this error, and a numerical implementation to compute both quantities.\n\nThe analysis proceeds in three parts: justification of the adversary model, derivation of the error bound, and description of the computational method.\n\n**1. Worst-Case Adversarial Outlier Design**\n\nThe adversary's objective is to choose an outlier vector $e_s$ to maximize the recovery error $\\|\\widehat{x}(A x + e_s) - x\\|_2$. The adversary is constrained by a sparsity budget, $\\|e_s\\|_0 = s$, and an amplitude budget, where non-zero entries satisfy $|e_{s,i}| \\le \\alpha$. The problem specifies reducing the search space to adversaries where non-zero entries have extremal magnitude, i.e., $|e_{s,i}| = \\alpha$.\n\nThis reduction is justified by the structure of the optimization problem that the adversary aims to disrupt. The LAD-Lasso estimator solves:\n$$ \\widehat{x}(y) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\|A z - y\\|_1 + \\lambda \\|z\\|_1 \\right\\} $$\nLet $h = \\widehat{x}(y) - x$ be the recovery error. The LAD-Lasso problem can be expressed in terms of $h$ as finding the minimizer of $\\|A(x+h) - (Ax+e_s)\\|_1 + \\lambda \\|x+h\\|_1$, which simplifies to minimizing $\\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1$. The term $e_s$ acts as an affine perturbation within the $\\ell_1$-norm term.\n\nThe function being minimized is convex. The adversary's problem is to choose $e_s$ from the feasible set $\\mathcal{C} = \\{e \\in \\mathbb{R}^m : \\|e\\|_0 = s, \\|e\\|_\\infty \\le \\alpha\\}$ to maximize the norm of the resulting error, $\\|\\widehat{h}\\|_2$. For a fixed support set for $e_s$, the feasible values form a hypercube $[-\\alpha, \\alpha]^s$. For many classes of parametric optimization problems, including Linear Programs (LPs) as the LAD-Lasso can be formulated, the maximum of a response function over a polyhedral uncertainty set is achieved at a vertex of that set. Intuitively, to maximize the deviation of the solution from the true signal $x$, the adversary should apply the largest possible perturbation. The vertices of the hypercube correspond to extremal values, where each non-zero component of $e_s$ is either $-\\alpha$ or $+\\alpha$. Therefore, it is sufficient to consider the finite set of adversaries $\\mathcal{E}(s, \\alpha)$ to find the worst-case error.\n\n**2. Derivation of the Analytic Error Bound**\n\nWe are tasked to derive an upper bound on the recovery error $\\|\\widehat{x}(y) - x\\|_2$ from first principles. Let $h = \\widehat{x}(y) - x$ be the error vector, where $y = Ax + e_s$.\n\nBy the definition of $\\widehat{x}(y)$ as a minimizer, the value of the objective function at $\\widehat{x}$ must be less than or equal to its value at any other point, including the true signal $x$.\n$$ \\|A\\widehat{x} - y\\|_1 + \\lambda \\|\\widehat{x}\\|_1 \\le \\|Ax - y\\|_1 + \\lambda \\|x\\|_1 $$\nSubstitute $\\widehat{x} = x + h$ and $y = Ax + e_s$:\n$$ \\|A(x+h) - (Ax+e_s)\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|Ax - (Ax+e_s)\\|_1 + \\lambda \\|x\\|_1 $$\n$$ \\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|-e_s\\|_1 + \\lambda \\|x\\|_1 $$\nUsing $\\| -v \\|_1 = \\|v\\|_1$, we simplify to:\n$$ \\|Ah - e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|e_s\\|_1 + \\lambda \\|x\\|_1 $$\nWe employ the reverse triangle inequality, $\\|u-v\\|_1 \\ge \\|u\\|_1 - \\|v\\|_1$, on the term $\\|Ah - e_s\\|_1$:\n$$ \\|Ah\\|_1 - \\|e_s\\|_1 \\le \\|Ah - e_s\\|_1 $$\nSubstituting this into the previous inequality yields:\n$$ \\|Ah\\|_1 - \\|e_s\\|_1 + \\lambda \\|x+h\\|_1 \\le \\|e_s\\|_1 + \\lambda \\|x\\|_1 $$\nRearranging the terms to group $\\|e_s\\|_1$ on one side gives the basic inequality:\n$$ \\|Ah\\|_1 + \\lambda (\\|x+h\\|_1 - \\|x\\|_1) \\le 2\\|e_s\\|_1 $$\nTo obtain a bound independent of $x$, we again use the reverse triangle inequality, $\\|x+h\\|_1 \\ge \\|x\\|_1 - \\|h\\|_1$, which implies $\\|x+h\\|_1 - \\|x\\|_1 \\ge -\\|h\\|_1$. Substituting this creates a looser, but more tractable, inequality:\n$$ \\|Ah\\|_1 - \\lambda \\|h\\|_1 \\le 2\\|e_s\\|_1 $$\nNow, we use the provided base facts to relate the $\\ell_1$-norms to the $\\ell_2$-norm of $h$.\n1.  The $\\ell_1$-norm is an upper bound on the $\\ell_2$-norm: $\\|Ah\\|_1 \\ge \\|Ah\\|_2$.\n2.  The $\\ell_2$-norm of the transformed vector $Ah$ is bounded by the smallest singular value of $A$, $\\sigma_{\\min}(A)$: $\\|Ah\\|_2 \\ge \\sigma_{\\min}(A) \\|h\\|_2$.\n3.  Combining these gives: $\\|Ah\\|_1 \\ge \\sigma_{\\min}(A) \\|h\\|_2$.\n\n4.  The $\\ell_1$-norm of $h$ is related to its $\\ell_2$-norm by: $\\|h\\|_1 \\le \\sqrt{n} \\|h\\|_2$.\n\nSubstituting these relationships into the inequality $\\|Ah\\|_1 - \\lambda \\|h\\|_1 \\le 2\\|e_s\\|_1$:\n$$ \\sigma_{\\min}(A) \\|h\\|_2 - \\lambda \\sqrt{n} \\|h\\|_2 \\le 2\\|e_s\\|_1 $$\nFactoring out $\\|h\\|_2$:\n$$ (\\sigma_{\\min}(A) - \\lambda \\sqrt{n}) \\|h\\|_2 \\le 2\\|e_s\\|_1 $$\nThe $\\ell_1$-norm of the adversary vector $e_s$ is $\\|e_s\\|_1 = \\sum_{i \\in \\text{supp}(e_s)} |e_{s,i}| = s \\alpha$.\nThus, we have:\n$$ (\\sigma_{\\min}(A) - \\lambda \\sqrt{n}) \\|h\\|_2 \\le 2 s \\alpha $$\nProvided that $\\sigma_{\\min}(A) - \\lambda \\sqrt{n}  0$, we can isolate $\\|h\\|_2$ to obtain the final upper bound on the recovery error:\n$$ \\|\\widehat{x}(y) - x\\|_2 \\le \\frac{2 s \\alpha}{\\sigma_{\\min}(A) - \\lambda \\sqrt{n}} $$\nThis bound depends only on the problem parameters $A$, $\\lambda$, $s$, and $\\alpha$, as required.\n\n**3. Computational Strategy**\n\nThe numerical implementation consists of two main parts: computing the worst-case error by enumeration and evaluating the derived analytic bound.\n\n**a. LAD-Lasso via Linear Programming (LP)**\nThe LAD-Lasso optimization problem is solved exactly by reformulating it as an LP. The variable vector for the LP is $X = [z^T, r^T, t^T, u^T]^T \\in \\mathbb{R}^{2n+2m}$. The LP is:\n$$ \\min_{X} c^T X \\quad \\text{subject to} \\quad A_{\\text{eq}}X = b_{\\text{eq}}, \\quad A_{\\text{ub}}X \\le b_{\\text{ub}}, \\quad l \\le X \\le u $$\n- **Objective vector $c$**: $c = [0_n^T, 0_m^T, 1_m^T, \\lambda 1_n^T]^T$.\n- **Equality constraints $A_{\\text{eq}}, b_{\\text{eq}}$**: From $r = Az - y \\iff -Az + r = -y$.\n  $A_{\\text{eq}} = [-A, I_m, 0_{m \\times m}, 0_{m \\times n}]$, $b_{\\text{eq}} = -y$.\n- **Inequality constraints $A_{\\text{ub}}, b_{\\text{ub}}$**: From $-t \\le r \\le t$ and $-u \\le z \\le u$.\n  $r-t \\le 0$, $-r-t \\le 0$, $z-u \\le 0$, $-z-u \\le 0$. These are stacked into a block matrix $A_{\\text{ub}}$ and a zero vector $b_{\\text{ub}}$.\n- **Bounds $l, u$**: $z$ and $r$ are unbounded. $t \\ge 0$ and $u \\ge 0$. This defines the lower and upper bounds for each component of $X$.\n\n**b. Worst-Case Error Calculation**\nThe worst-case error is found by searching over the entire specified adversarial set $\\mathcal{E}(s, \\alpha)$. This is done by:\n1.  Generating all combinations of $s$ indices from $\\{1, \\dots, m\\}$ to serve as the support of $e_s$. The number of such supports is $\\binom{m}{s}$.\n2.  For each support, generating all $2^s$ possible sign patterns for the non-zero entries.\n3.  For each resulting $e_s$, computing $y = Ax+e_s$, solving for $\\widehat{x}(y)$ using the LP solver, calculating the error $\\|\\widehat{x}(y)-x\\|_2$, and keeping track of the maximum error found.\n\nThe total number of adversaries to test is $\\binom{m}{s} 2^s$, which is computationally feasible for the given test cases.\n\n**c. Analytic Bound Calculation**\nThe derived bound $B = \\frac{2 s \\alpha}{\\sigma_{\\min}(A) - \\lambda \\sqrt{n}}$ is computed by:\n1.  Calculating the singular values of $A$ and finding the minimum, $\\sigma_{\\min}(A)$.\n2.  Substituting the values of $s, \\alpha, \\lambda, n$, and $\\sigma_{\\min}(A)$ into the formula. For $s=0$, the bound is $0$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\nfrom itertools import combinations, product\n\ndef solve_lad_lasso(A, y, lambda_val):\n    \"\"\"\n    Solves the LAD-Lasso problem by reformulating it as a Linear Program.\n    min ||Az - y||_1 + lambda * ||z||_1\n    \"\"\"\n    m, n = A.shape\n    \n    # LP variable vector: X = [z, r, t, u]\n    # Dimensions: z (n), r (m), t (m), u (n)\n    # Total variables: 2n + 2m\n\n    # Objective function: min sum(t_i) + lambda * sum(u_j)\n    c = np.concatenate([\n        np.zeros(n),           # Cost for z\n        np.zeros(m),           # Cost for r\n        np.ones(m),            # Cost for t\n        lambda_val * np.ones(n) # Cost for u\n    ])\n\n    # Equality constraint: r = Az - y  => -Az + r = -y\n    A_eq = np.hstack([-A, np.eye(m), np.zeros((m, m)), np.zeros((m, n))])\n    b_eq = -y\n\n    # Inequality constraints: A_ub @ X = b_ub\n    # -t = r = t  =>  r - t = 0  and -r - t = 0\n    # -u = z = u  =>  z - u = 0  and -z - u = 0\n    \n    # r - t = 0\n    A_ub1 = np.hstack([np.zeros((m, n)), np.eye(m), -np.eye(m), np.zeros((m, n))])\n    # -r - t = 0\n    A_ub2 = np.hstack([np.zeros((m, n)), -np.eye(m), -np.eye(m), np.zeros((m, n))])\n    # z - u = 0\n    A_ub3 = np.hstack([np.eye(n), np.zeros((n, m)), np.zeros((n, m)), -np.eye(n)])\n    # -z - u = 0\n    A_ub4 = np.hstack([-np.eye(n), np.zeros((n, m)), np.zeros((n, m)), -np.eye(n)])\n    \n    A_ub = np.vstack([A_ub1, A_ub2, A_ub3, A_ub4])\n    b_ub = np.zeros(A_ub.shape[0])\n\n    # Variable bounds: z, r are free; t, u are non-negative.\n    bounds = ([(None, None)] * (n + m)) + ([(0, None)] * (m + n))\n\n    # Solve the LP\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n\n    if not res.success:\n        # The problem formulation should guarantee a solution exists.\n        # This case is included for robustness but not expected to be hit.\n        raise RuntimeError(\"LP solver failed to find a solution.\")\n\n    # Extract the solution for z\n    z = res.x[:n]\n    return z\n\ndef calculate_worst_error(A, x, s, alpha, lambda_val):\n    \"\"\"\n    Finds the worst-case recovery error by enumerating all adversaries.\n    \"\"\"\n    m, n = A.shape\n    \n    # Base case with no adversary\n    if s == 0:\n        e_s = np.zeros(m)\n        y = A @ x + e_s\n        x_hat = solve_lad_lasso(A, y, lambda_val)\n        return np.linalg.norm(x_hat - x)\n\n    max_sq_error = 0.0\n    indices = np.arange(m)\n\n    # Iterate over all possible supports for the outlier vector e_s\n    for support in combinations(indices, s):\n        # Iterate over all sign patterns (+/- alpha) on the support\n        for signs in product([-1, 1], repeat=s):\n            e_s = np.zeros(m)\n            support_list = list(support)\n            e_s[support_list] = np.array(signs) * alpha\n            \n            y = A @ x + e_s\n            x_hat = solve_lad_lasso(A, y, lambda_val)\n            \n            sq_error = np.sum((x_hat - x)**2)\n            if sq_error > max_sq_error:\n                max_sq_error = sq_error\n                \n    return np.sqrt(max_sq_error)\n\ndef calculate_bound(A, n, s, alpha, lambda_val):\n    \"\"\"\n    Calculates the derived analytic upper bound on the error.\n    \"\"\"\n    if s == 0:\n        return 0.0\n        \n    singular_values = np.linalg.svd(A, compute_uv=False)\n    sigma_min_A = np.min(singular_values)\n    \n    denominator = sigma_min_A - lambda_val * np.sqrt(n)\n    \n    if denominator = 1e-9: # Add a small tolerance for stability\n        return float('inf')\n        \n    bound = (2 * s * alpha) / denominator\n    return bound\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.50, -0.10, 0.30, 0.20, -0.40],\n                [0.00, 0.40, -0.20, 0.50, 0.10],\n                [0.30, 0.60, 0.10, -0.30, 0.20],\n                [-0.20, 0.10, 0.50, 0.10, 0.60],\n                [0.40, -0.30, -0.10, 0.70, -0.20],\n                [-0.10, 0.50, 0.40, -0.20, 0.30],\n                [0.20, -0.20, 0.60, 0.40, -0.10]\n            ]),\n            \"x\": np.array([1.0, 0.0, 0.5, 0.0, 0.0]),\n            \"s\": 2, \"alpha\": 0.5, \"lambda\": 0.02\n        },\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.60, 0.10, -0.30, 0.25, 0.20],\n                [0.20, -0.50, 0.40, -0.10, 0.30],\n                [-0.10, 0.40, 0.20, 0.50, -0.20],\n                [0.30, 0.30, -0.20, 0.60, 0.10],\n                [0.50, -0.20, 0.10, -0.30, 0.40],\n                [0.00, 0.60, -0.10, 0.20, 0.50],\n                [0.40, -0.10, 0.60, -0.20, 0.00]\n            ]),\n            \"x\": np.array([0.7, -0.3, 0.0, 0.5, 0.0]),\n            \"s\": 3, \"alpha\": 0.3, \"lambda\": 0.02\n        },\n        {\n            \"m\": 7, \"n\": 5,\n            \"A\": np.array([\n                [0.50, -0.10, 0.30, 0.20, -0.40],\n                [0.00, 0.40, -0.20, 0.50, 0.10],\n                [0.30, 0.60, 0.10, -0.30, 0.20],\n                [-0.20, 0.10, 0.50, 0.10, 0.60],\n                [0.40, -0.30, -0.10, 0.70, -0.20],\n                [-0.10, 0.50, 0.40, -0.20, 0.30],\n                [0.20, -0.20, 0.60, 0.40, -0.10]\n            ]),\n            \"x\": np.array([1.0, 0.0, 0.5, 0.0, 0.0]),\n            \"s\": 0, \"alpha\": 0.0, \"lambda\": 0.02\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, x, s, alpha, lambda_val, n = case[\"A\"], case[\"x\"], case[\"s\"], case[\"alpha\"], case[\"lambda\"], case[\"n\"]\n        \n        worst_error = calculate_worst_error(A, x, s, alpha, lambda_val)\n        bound = calculate_bound(A, n, s, alpha, lambda_val)\n        \n        results.append([round(worst_error, 6), round(bound, 6)])\n\n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]:.6f},{results[0][1]:.6f}],[{results[1][0]:.6f},{results[1][1]:.6f}],[{results[2][0]:.6f},{results[2][1]:.6f}]]\")\n\nsolve()\n```"
        }
    ]
}