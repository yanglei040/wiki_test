{
    "hands_on_practices": [
        {
            "introduction": "The engine behind many algorithms for Robust Principal Component Analysis is the proximal operator of the nuclear norm, known as the Singular Value Thresholding (SVT) operator. This exercise provides a foundational practice by asking you to derive the SVT operation from first principles and then apply it to a concrete example . Mastering this derivation solidifies your understanding of how the mathematical formulation of RPCA translates into a concrete computational step.",
            "id": "3474854",
            "problem": "Consider the canonical Robust Principal Component Analysis (RPCA) decomposition model, which seeks a low-rank matrix and a sparse matrix via convex optimization. A standard proximal step for updating the low-rank component uses the proximal operator of the nuclear norm. Let the nuclear norm be defined by $\\|X\\|_{*} = \\sum_{i} \\sigma_{i}(X)$, where $\\sigma_{i}(X)$ are the singular values of the matrix $X$. The proximal operator of a proper, lower semicontinuous, convex function $f$ at a point $Y$ is defined by $\\operatorname{prox}_{f}(Y) = \\arg\\min_{X} \\left\\{ f(X) + \\tfrac{1}{2}\\|X - Y\\|_{F}^{2} \\right\\}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. In the RPCA low-rank update, one uses the proximal operator of the scaled nuclear norm $f(X) = \\tau \\|X\\|_{*}$, where $\\tau > 0$ is a threshold parameter. \n\nStarting only from these base definitions, together with the unitary invariance of the Frobenius norm and the nuclear norm, derive the transformation induced on the singular values by $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y)$, also known as the Singular Value Thresholding (SVT) operator (define this acronym on first use). Then apply your derivation to a matrix $Y$ whose Singular Value Decomposition (SVD) is $Y = U \\Sigma V^{\\top}$ with singular values $(\\sigma_{1}, \\sigma_{2}, \\sigma_{3}) = (5, 3, 0)$, using the threshold $\\tau = 2$. Compute the singular values of the proximal point $L^{\\star} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y)$ and the rank of $L^{\\star}$. \n\nExpress your final answer as a single expression listing the singular values of $L^{\\star}$ in nonincreasing order followed by the rank of $L^{\\star}$. No rounding is required and no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It consists of a standard derivation and application within the field of convex optimization and matrix analysis. All necessary definitions and data are provided.\n\nThe task is to derive the proximal operator of the scaled nuclear norm and apply it to a specific matrix. The proximal operator of the function $f(X) = \\tau \\|X\\|_{*}$ is defined as the solution to the following optimization problem:\n$$\nL^{\\star} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y) = \\arg\\min_{X} \\left\\{ \\tau \\|X\\|_{*} + \\tfrac{1}{2}\\|X - Y\\|_{F}^{2} \\right\\}\n$$\nwhere $\\|X\\|_{*} = \\sum_{i} \\sigma_{i}(X)$ is the nuclear norm, with $\\sigma_i(X)$ being the singular values of $X$, and $\\|A\\|_{F} = \\sqrt{\\sum_{i,j} |A_{ij}|^2} = \\sqrt{\\operatorname{Tr}(A^{\\top}A)}$ is the Frobenius norm.\n\nLet the Singular Value Decomposition (SVD) of the matrix $Y \\in \\mathbb{R}^{m \\times n}$ be $Y = U \\Sigma_Y V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are unitary matrices, and $\\Sigma_Y$ is a rectangular diagonal matrix of size $m \\times n$ with non-negative diagonal entries $\\sigma_i(Y)$ (the singular values of $Y$) sorted in non-increasing order.\n\nThe objective function can be expanded as:\n$$\nJ(X) = \\tau \\|X\\|_{*} + \\tfrac{1}{2} \\left( \\|X\\|_{F}^{2} - 2\\langle X, Y \\rangle_F + \\|Y\\|_{F}^{2} \\right)\n$$\nwhere $\\langle X, Y \\rangle_F = \\operatorname{Tr}(X^{\\top}Y)$ is the Frobenius inner product. The term $\\|Y\\|_{F}^{2}$ is a constant with respect to $X$ and can be ignored in the minimization. The norms can be expressed in terms of singular values: $\\|X\\|_{*} = \\sum_i \\sigma_i(X)$ and $\\|X\\|_{F}^{2} = \\sum_i \\sigma_i(X)^2$. The problem is equivalent to minimizing:\n$$\n\\tilde{J}(X) = \\tau \\sum_i \\sigma_i(X) + \\tfrac{1}{2} \\sum_i \\sigma_i(X)^2 - \\operatorname{Tr}(X^{\\top}Y)\n$$\nThe von Neumann trace inequality states that for any two matrices $A$ and $B$, $\\operatorname{Tr}(A^{\\top}B) \\le \\sum_i \\sigma_i(A)\\sigma_i(B)$. Equality is achieved if $A$ and $B$ share the same singular vectors, i.e., $A = U\\Sigma_A V^{\\top}$ and $B = U\\Sigma_B V^{\\top}$ for some unitary matrices $U$ and $V$. To minimize $\\tilde{J}(X)$, we must maximize the term $\\operatorname{Tr}(X^{\\top}Y)$. Based on the von Neumann inequality, this maximum is achieved when $X$ has the same singular vectors as $Y$. Let us therefore seek a solution of the form $X = U \\Sigma_X V^{\\top}$, where $U$ and $V$ are the singular vector matrices from the SVD of $Y$.\n\nUsing the unitary invariance of the Frobenius norm, the term $\\|X - Y\\|_{F}^{2}$ simplifies:\n$$\n\\|X - Y\\|_{F}^{2} = \\|U \\Sigma_X V^{\\top} - U \\Sigma_Y V^{\\top}\\|_{F}^{2} = \\|U(\\Sigma_X - \\Sigma_Y)V^{\\top}\\|_{F}^{2} = \\|\\Sigma_X - \\Sigma_Y\\|_{F}^{2}\n$$\nSince $\\Sigma_X$ and $\\Sigma_Y$ are diagonal matrices with diagonal entries $\\sigma_i(X)$ and $\\sigma_i(Y)$ respectively, this becomes:\n$$\n\\|\\Sigma_X - \\Sigma_Y\\|_{F}^{2} = \\sum_i (\\sigma_i(X) - \\sigma_i(Y))^2\n$$\nThe nuclear norm of $X$ is $\\|X\\|_{*} = \\sum_i \\sigma_i(X)$.\nSubstituting these into the original optimization problem, we now minimize over the singular values $\\sigma_i(X)$:\n$$\n\\min_{\\{\\sigma_i(X) \\ge 0\\}} \\left\\{ \\tau \\sum_i \\sigma_i(X) + \\tfrac{1}{2} \\sum_i (\\sigma_i(X) - \\sigma_i(Y))^2 \\right\\}\n$$\nThis problem is separable, meaning we can solve for each $\\sigma_i(X)$ independently. For each $i$, we solve:\n$$\n\\min_{\\sigma \\ge 0} \\left\\{ \\tau \\sigma + \\tfrac{1}{2} (\\sigma - \\sigma_i(Y))^2 \\right\\}\n$$\nLet $g(\\sigma) = \\tau \\sigma + \\tfrac{1}{2} (\\sigma - \\sigma_i(Y))^2$. This is a convex quadratic function. To find the minimum, we compute the derivative with respect to $\\sigma$ and set it to zero (ignoring the non-negativity constraint for a moment):\n$$\n\\frac{dg}{d\\sigma} = \\tau + (\\sigma - \\sigma_i(Y)) = 0 \\implies \\sigma = \\sigma_i(Y) - \\tau\n$$\nNow, we must enforce the constraint $\\sigma \\ge 0$.\n1. If $\\sigma_i(Y) - \\tau > 0$, the unconstrained minimizer is positive, so it is the valid solution. The optimal value is $\\sigma^{\\star} = \\sigma_i(Y) - \\tau$.\n2. If $\\sigma_i(Y) - \\tau \\le 0$, the unconstrained minimizer is non-positive. Since $g(\\sigma)$ is a parabola opening upwards with its vertex at $\\sigma_i(Y) - \\tau$, the function is increasing for $\\sigma > \\sigma_i(Y) - \\tau$. Therefore, on the domain $\\sigma \\ge 0$, the minimum is achieved at the boundary, i.e., at $\\sigma^{\\star} = 0$.\n\nCombining these two cases, the solution for each singular value is:\n$$\n\\sigma_i(L^{\\star}) = \\max(0, \\sigma_i(Y) - \\tau) = (\\sigma_i(Y) - \\tau)_+\n$$\nThis operation is known as soft-thresholding. The resulting matrix $L^{\\star}$ is constructed by applying this transformation to the singular values of $Y$ while keeping the singular vectors the same:\n$$\nL^{\\star} = U \\mathcal{S}_{\\tau}(\\Sigma_Y) V^{\\top}\n$$\nwhere $\\mathcal{S}_{\\tau}(\\Sigma_Y)$ is the diagonal matrix with diagonal entries $\\max(0, \\sigma_i(Y)-\\tau)$. This transformation is called the Singular Value Thresholding (SVT) operator.\n\nNow we apply this result to the given problem. We have a matrix $Y$ with singular values $(\\sigma_1, \\sigma_2, \\sigma_3) = (5, 3, 0)$ and a threshold $\\tau = 2$.\nWe compute the singular values of $L^{\\star} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{*}}(Y)$ using the derived rule. Let's denote the singular values of $L^{\\star}$ as $\\sigma_i^{\\star}$.\n\nFor $\\sigma_1 = 5$:\n$$\n\\sigma_1^{\\star} = \\max(0, 5 - 2) = \\max(0, 3) = 3\n$$\nFor $\\sigma_2 = 3$:\n$$\n\\sigma_2^{\\star} = \\max(0, 3 - 2) = \\max(0, 1) = 1\n$$\nFor $\\sigma_3 = 0$:\n$$\n\\sigma_3^{\\star} = \\max(0, 0 - 2) = \\max(0, -2) = 0\n$$\nThe singular values of $L^{\\star}$, in nonincreasing order, are $(3, 1, 0)$.\n\nThe rank of a matrix is the number of its non-zero singular values. For $L^{\\star}$, the non-zero singular values are $3$ and $1$. There are two such values.\nTherefore, the rank of $L^{\\star}$ is $2$.\n\nThe final answer requires the singular values of $L^{\\star}$ and the rank of $L^{\\star}$. These are $(3, 1, 0)$ and $2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 1 & 0 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With the core operators understood, the next step is to assemble them into a working algorithm. This hands-on coding practice challenges you to implement an alternating proximal minimization scheme to solve the RPCA problem, applying the singular value thresholding and soft-thresholding operators iteratively . By building the solver from the ground up and testing it on synthetic data, you will gain invaluable practical insight into how RPCA performs in different scenarios.",
            "id": "3474831",
            "problem": "You are asked to implement and analyze a simple alternating proximal minimization algorithm for Robust Principal Component Analysis (RPCA), a canonical problem in compressed sensing and sparse optimization. Consider a data matrix $M \\in \\mathbb{R}^{m \\times n}$ that decomposes as $M = L_{\\star} + S_{\\star}$, where $L_{\\star}$ is low-rank and $S_{\\star}$ is sparse. Starting from the foundational base of convex regularization with the nuclear norm and the elementwise absolute-value norm, consider the unconstrained penalized objective\n$$\n\\min_{L, S \\in \\mathbb{R}^{m \\times n}} \\; \\frac{1}{2} \\lVert M - L - S \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast} + \\lambda \\lVert S \\rVert_{1},\n$$\nwhere $\\lVert \\cdot \\rVert_{F}$ is the Frobenius norm, $\\lVert \\cdot \\rVert_{\\ast}$ is the nuclear norm (sum of singular values), and $\\lVert \\cdot \\rVert_{1}$ is the elementwise $\\ell_{1}$ norm. Your program must implement an alternating proximal minimization scheme that alternates between minimizing the objective with respect to $L$ (holding $S$ fixed) and minimizing with respect to $S$ (holding $L$ fixed). The proximal operator for the nuclear norm corresponds to singular value thresholding, and the proximal operator for the elementwise $\\ell_{1}$ norm corresponds to elementwise soft-thresholding.\n\nFrom first principles, base your derivation on the definitions of the proximal operator, the nuclear norm, and the $\\ell_{1}$ norm, together with the decomposition of a matrix by the singular value decomposition. Do not assume any special algorithmic template beyond alternating minimization and proximal updates defined by these norms.\n\nYour program must:\n- Construct synthetic test matrices $M = L_{\\star} + S_{\\star}$ for a small test suite where $L_{\\star}$ has a prescribed rank and $S_{\\star}$ has a prescribed sparse support. Use orthonormal factors for $L_{\\star}$ (obtained from a random Gaussian matrix followed by a QR factorization) and fixed singular values, and select a random support for $S_{\\star}$ with nonzero entries of fixed magnitude and random signs. Use fixed pseudorandom seeds for reproducibility.\n- Run the alternating proximal minimization updates\n  - Update $L$ by the proximal map of $\\mu \\lVert \\cdot \\rVert_{\\ast}$ applied to $M - S$.\n  - Update $S$ by the proximal map of $\\lambda \\lVert \\cdot \\rVert_{1}$ applied to $M - L$.\n- Initialize $L$ and $S$ as the zero matrices.\n- At each iteration, check whether the estimated low-rank matrix has the correct rank (that is, the count of singular values greater than a fixed threshold matches the true rank) and whether the estimated sparse matrix has exactly the correct support (the set of indices with magnitude greater than a fixed threshold matches the true support). Define a small absolute threshold for both checks as $\\tau_{\\text{rank}} = 10^{-6}$ and $\\tau_{\\text{supp}} = 10^{-6}$.\n- Report the iteration count at which both the correct rank and the correct support are first simultaneously achieved. If this does not occur within a maximum of $1000$ iterations, report $-1$ for that test case. Use a convergence safeguard to update iterates until either the maximum iteration cap is reached or the relative change in $(L,S)$ drops below $10^{-8}$.\n\nTest Suite:\nImplement four test cases with the following parameters. For each case, generate $L_{\\star}$ and $S_{\\star}$ as specified, set $M = L_{\\star} + S_{\\star}$, and run the algorithm with the stated $(\\mu,\\lambda)$.\n\n- Case A (happy path, mixed low-rank and sparse):\n  - Dimensions: $m = 12$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 2$ with singular values $[8.0, 6.0]$.\n  - Sparse support size: $12$ nonzeros with magnitude $5.0$ and random $\\pm 1$ signs.\n  - Parameters: $\\mu = 1.0$, $\\lambda = 0.35$.\n  - Random seed: $0$.\n\n- Case B (boundary condition with no sparse component):\n  - Dimensions: $m = 15$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 3$ with singular values $[5.0, 4.0, 3.0]$.\n  - Sparse support size: $0$.\n  - Parameters: $\\mu = 0.8$, $\\lambda = 0.30$.\n  - Random seed: $1$.\n\n- Case C (boundary condition with no low-rank component):\n  - Dimensions: $m = 10$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 0$.\n  - Sparse support size: $15$ nonzeros with magnitude $4.0$ and random $\\pm 1$ signs.\n  - Parameters: $\\mu = 1.0$, $\\lambda = 0.50$.\n  - Random seed: $2$.\n\n- Case D (near-threshold harder case):\n  - Dimensions: $m = 10$, $n = 10$.\n  - Rank of $L_{\\star}$: $r = 2$ with singular values $[1.2, 1.1]$.\n  - Sparse support size: $8$ nonzeros with magnitude $0.6$ and random $\\pm 1$ signs.\n  - Parameters: $\\mu = 0.8$, $\\lambda = 0.45$.\n  - Random seed: $3$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the iteration counts for the four cases as a comma-separated list enclosed in square brackets (for example, $\\texttt{[7,4,5,-1]}$). No additional text should be printed. No physical units are involved in this problem, and no angles must be specified. All numerical outputs are pure integers. The use of any language is permitted in principle, but your final answer must be a complete and runnable program that adheres to the provided execution environment constraints.",
            "solution": "The problem requires the implementation of an alternating proximal minimization algorithm to solve the Robust Principal Component Analysis (RPCA) problem. The solution seeks to decompose a given data matrix $M \\in \\mathbb{R}^{m \\times n}$ into a low-rank component $L$ and a sparse component $S$. This is achieved by solving the following convex optimization problem:\n$$\n\\min_{L, S \\in \\mathbb{R}^{m \\times n}} \\; f(L, S) = \\frac{1}{2} \\lVert M - L - S \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast} + \\lambda \\lVert S \\rVert_{1}\n$$\nHere, $\\lVert \\cdot \\rVert_{F}$ is the Frobenius norm, which measures the data fidelity, $\\lVert \\cdot \\rVert_{\\ast}$ is the nuclear norm (the sum of the singular values), which serves as a convex surrogate for the rank of $L$, and $\\lVert \\cdot \\rVert_{1}$ is the elementwise $\\ell_{1}$ norm (the sum of the absolute values of the entries), which promotes sparsity in $S$. The parameters $\\mu > 0$ and $\\lambda > 0$ control the trade-off between data fidelity and the low-rank and sparse regularizers.\n\nThe chosen solution method is an alternating minimization scheme, a form of block coordinate descent. In each iteration, we minimize the objective function with respect to one variable while keeping the other fixed. This breaks the joint minimization problem into two simpler subproblems.\n\n**Step 1: Minimization with respect to $L$**\n\nFixing $S$ to its value from the previous iteration, $S_k$, the subproblem for updating $L$ is:\n$$\nL_{k+1} = \\arg\\min_{L} \\frac{1}{2} \\lVert M - L - S_k \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast}\n$$\nThis can be rewritten as:\n$$\nL_{k+1} = \\arg\\min_{L} \\frac{1}{2} \\lVert (M - S_k) - L \\rVert_{F}^{2} + \\mu \\lVert L \\rVert_{\\ast}\n$$\nThis is the definition of the proximal operator of the nuclear norm, scaled by $\\mu$, applied to the matrix $M - S_k$. The proximal operator, denoted $\\text{prox}_{\\mu \\lVert \\cdot \\rVert_{\\ast}}$, is given by the Singular Value Thresholding (SVT) operator. For any matrix $X \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition (SVD) $X = U \\Sigma V^T$, where $\\Sigma = \\text{diag}(\\{\\sigma_i\\}_{i=1}^{\\min(m,n)})$, the SVT operator is defined as:\n$$\n\\text{prox}_{\\mu \\lVert \\cdot \\rVert_{\\ast}}(X) = U \\mathcal{S}_{\\mu}(\\Sigma) V^T\n$$\nwhere $\\mathcal{S}_{\\mu}(\\Sigma)$ is a diagonal matrix with entries $(\\mathcal{S}_{\\mu}(\\Sigma))_{ii} = \\max(0, \\sigma_i - \\mu)$. This operation effectively shrinks the singular values of $X$ towards zero by $\\mu$ and sets any singular values smaller than $\\mu$ to zero, thereby promoting a low-rank structure.\n\n**Step 2: Minimization with respect to $S$**\n\nNext, we fix $L$ to its newly computed value, $L_{k+1}$, and solve for $S$:\n$$\nS_{k+1} = \\arg\\min_{S} \\frac{1}{2} \\lVert M - L_{k+1} - S \\rVert_{F}^{2} + \\lambda \\lVert S \\rVert_{1}\n$$\nThis can be rewritten as:\n$$\nS_{k+1} = \\arg\\min_{S} \\frac{1}{2} \\lVert (M - L_{k+1}) - S \\rVert_{F}^{2} + \\lambda \\lVert S \\rVert_{1}\n$$\nThis is the definition of the proximal operator of the $\\ell_1$ norm, scaled by $\\lambda$, applied to the matrix $M - L_{k+1}$. Due to the separability of the Frobenius norm and the $\\ell_1$ norm over the matrix elements, this problem can be solved elementwise. For each entry $(i,j)$, the problem is:\n$$\n(S_{k+1})_{ij} = \\arg\\min_{s_{ij}} \\frac{1}{2} ((M-L_{k+1})_{ij} - s_{ij})^2 + \\lambda |s_{ij}|\n$$\nThe solution is given by the elementwise soft-thresholding operator, $\\mathcal{S}_{\\lambda}$:\n$$\n(S_{k+1})_{ij} = \\mathcal{S}_{\\lambda}((M-L_{k+1})_{ij}) = \\text{sign}((M-L_{k+1})_{ij}) \\max(0, |(M-L_{k+1})_{ij}| - \\lambda)\n$$\nThis operation shrinks each element's magnitude towards zero by $\\lambda$, promoting sparsity.\n\n**Algorithm and Implementation**\n\nThe complete algorithm proceeds as follows:\n1. Initialize $L_0 = 0$ and $S_0 = 0$.\n2. For $k=0, 1, 2, \\dots$ until convergence or maximum iterations:\n   a. Update $L$: $L_{k+1} = \\text{prox}_{\\mu \\lVert \\cdot \\rVert_{\\ast}}(M - S_k)$.\n   b. Update $S$: $S_{k+1} = \\text{prox}_{\\lambda \\lVert \\cdot \\rVert_{1}}(M - L_{k+1})$.\n\nThe implementation creates synthetic data $M = L_{\\star} + S_{\\star}$ according to the specified parameters for each test case. $L_{\\star}$ is constructed from random orthonormal bases and prescribed singular values. $S_{\\star}$ is constructed by populating a random support with entries of a fixed magnitude and random signs. Fixed random seeds ensure reproducibility.\n\nThe algorithm iterates, and at each step $k$, it checks if the estimated $L_k$ has the correct rank $r$ (number of singular values $> \\tau_{\\text{rank}} = 10^{-6}$) and if the estimated $S_k$ has the exactly correct support (the set of indices where $|(S_k)_{ij}| > \\tau_{\\text{supp}} = 10^{-6}$ matches the true support of $S_{\\star}$). The iteration count at which both conditions are first met is recorded. The process stops if these conditions are met and the algorithm has subsequently converged (relative change in $(L,S)$ below $10^{-8}$), or if the maximum iteration limit of $1000$ is reached. If the success conditions are not met within the iteration limit, $-1$ is reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... # No scipy needed for this implementation.\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for Robust PCA and print results.\n    \"\"\"\n\n    def generate_data(m, n, r, singular_values, sparse_size, sparse_mag, seed):\n        \"\"\"Generates synthetic data matrix M = L_star + S_star.\"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Generate L_star (low-rank matrix)\n        if r > 0:\n            U_rand = rng.standard_normal(size=(m, r))\n            U, _ = np.linalg.qr(U_rand)\n            \n            V_rand = rng.standard_normal(size=(n, r))\n            V, _ = np.linalg.qr(V_rand)\n            \n            Sigma_star = np.diag(singular_values)\n            L_star = U @ Sigma_star @ V.T\n        else:\n            L_star = np.zeros((m, n))\n\n        # Generate S_star (sparse matrix)\n        S_star = np.zeros((m, n))\n        if sparse_size > 0:\n            indices = rng.choice(m * n, size=sparse_size, replace=False)\n            row_indices, col_indices = np.unravel_index(indices, (m, n))\n            \n            signs = rng.choice([-1.0, 1.0], size=sparse_size)\n            \n            S_star[row_indices, col_indices] = signs * sparse_mag\n        \n        true_support_indices = np.where(S_star != 0)\n        \n        M = L_star + S_star\n        \n        return M, true_support_indices\n\n    def prox_nuclear(X, mu):\n        \"\"\"Singular Value Thresholding (proximal operator for nuclear norm).\"\"\"\n        U, s, Vt = np.linalg.svd(X, full_matrices=False, compute_uv=True)\n        s_thresh = np.maximum(0, s - mu)\n        return U @ np.diag(s_thresh) @ Vt\n\n    def prox_l1(X, lam):\n        \"\"\"Element-wise soft-thresholding (proximal operator for l1 norm).\"\"\"\n        return np.sign(X) * np.maximum(np.abs(X) - lam, 0)\n\n    def run_rpca_instance(m, n, true_rank, sv, sp_size, sp_mag, mu, lam, seed):\n        \"\"\"Runs the alternating proximal minimization for one RPCA instance.\"\"\"\n        M, true_support_indices = generate_data(m, n, true_rank, sv, sp_size, sp_mag, seed)\n        \n        L = np.zeros((m, n))\n        S = np.zeros((m, n))\n        \n        max_iter = 1000\n        conv_tol = 1e-8\n        rank_tol = 1e-6\n        supp_tol = 1e-6\n        \n        found_iter = -1\n        \n        true_support_mask = np.zeros((m, n), dtype=bool)\n        if sp_size > 0:\n            true_support_mask[true_support_indices] = True\n        \n        for k in range(1, max_iter + 1):\n            L_old, S_old = L.copy(), S.copy()\n            \n            # L-update\n            L = prox_nuclear(M - S, mu)\n            \n            # S-update\n            S = prox_l1(M - L, lam)\n            \n            # Check for correct rank and support (if not already found)\n            if found_iter == -1:\n                singular_values_L = np.linalg.svd(L, compute_uv=False)\n                estimated_rank = np.sum(singular_values_L > rank_tol)\n                rank_ok = (estimated_rank == true_rank)\n                \n                estimated_support_mask = np.abs(S) > supp_tol\n                support_ok = np.array_equal(estimated_support_mask, true_support_mask)\n                \n                if rank_ok and support_ok:\n                    found_iter = k\n                    \n            # Check for convergence\n            norm_L_fro_sq = np.linalg.norm(L_old, 'fro')**2\n            norm_S_fro_sq = np.linalg.norm(S_old, 'fro')**2\n            norm_val_sq = norm_L_fro_sq + norm_S_fro_sq\n\n            norm_diff_L_fro_sq = np.linalg.norm(L - L_old, 'fro')**2\n            norm_diff_S_fro_sq = np.linalg.norm(S - S_old, 'fro')**2\n            norm_diff_sq = norm_diff_L_fro_sq + norm_diff_S_fro_sq\n            \n            if norm_val_sq > 0: # Avoid division by zero, especially at first iteration\n                rel_change = np.sqrt(norm_diff_sq / norm_val_sq)\n                if rel_change < conv_tol:\n                    break\n        return found_iter\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: Happy path\n        {'m': 12, 'n': 10, 'true_rank': 2, 'sv': [8.0, 6.0], 'sp_size': 12, 'sp_mag': 5.0, 'mu': 1.0, 'lam': 0.35, 'seed': 0},\n        # Case B: No sparse component\n        {'m': 15, 'n': 10, 'true_rank': 3, 'sv': [5.0, 4.0, 3.0], 'sp_size': 0, 'sp_mag': 0.0, 'mu': 0.8, 'lam': 0.30, 'seed': 1},\n        # Case C: No low-rank component\n        {'m': 10, 'n': 10, 'true_rank': 0, 'sv': [], 'sp_size': 15, 'sp_mag': 4.0, 'mu': 1.0, 'lam': 0.50, 'seed': 2},\n        # Case D: Harder case\n        {'m': 10, 'n': 10, 'true_rank': 2, 'sv': [1.2, 1.1], 'sp_size': 8, 'sp_mag': 0.6, 'mu': 0.8, 'lam': 0.45, 'seed': 3},\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_rpca_instance(**case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "An algorithm is only as good as the guarantees that support it. This practice connects the algorithmic side of RPCA with its rich underlying theory by asking you to perform a numerical feasibility check based on established recovery conditions . By plugging concrete parameters for a hypothetical problem into the theoretical bounds, you will develop an intuition for the interplay between rank, sparsity, and incoherence that determines whether exact recovery is possible.",
            "id": "3474843",
            "problem": "Consider the Robust Principal Component Analysis (RPCA) model in which a data matrix $M \\in \\mathbb{R}^{m \\times n}$ decomposes as $M = L_{0} + S_{0}$, where $L_{0}$ is a low-rank matrix and $S_{0}$ is a sparse matrix. The recovery method is Principal Component Pursuit (PCP), which solves the convex program $\\min_{L,S} \\|L\\|_{*} + \\lambda \\|S\\|_{1}$ subject to $M = L + S$, with the regularization parameter chosen as $\\lambda = 1/\\sqrt{\\max\\{m,n\\}}$. Assume $m = n$, and that $L_{0}$ has rank $r$ and satisfies the standard $\\mu$-incoherence model with parameter $\\mu$, while the support of $S_{0}$ is drawn i.i.d. Bernoulli with sparsity fraction $\\rho$ over the $mn$ entries.\n\nA widely used sufficient condition from the RPCA literature states that there exist absolute constants $c_{r} > 0$ and $c_{\\rho} > 0$ such that PCP exactly recovers $(L_{0}, S_{0})$ with high probability provided that\n$$\nr \\leq c_{r} \\frac{n}{\\mu (\\ln n)^{2}}\n\\quad \\text{and} \\quad\n\\rho \\leq c_{\\rho}.\n$$\nFor the purpose of a conservative numerical feasibility check, adopt the explicit constants $c_{r} = 0.4$ and $c_{\\rho} = 0.1$.\n\nFor a square instance with $m = n = 100$, rank $r = 5$, incoherence parameter $\\mu = 3$, and sparsity fraction $\\rho = 0.05$, compute the indicator\n$$\nI \\equiv \\mathbf{1}\\!\\left\\{\\, r \\leq c_{r} \\frac{n}{\\mu (\\ln n)^{2}} \\ \\text{and} \\ \\rho \\leq c_{\\rho} \\,\\right\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is $1$ if the event holds and $0$ otherwise. Report the value of $I$ as your final answer. No rounding is required.",
            "solution": "The problem requires the computation of an indicator function, $I$, which is defined based on the satisfaction of a set of sufficient conditions for the exact recovery of a low-rank matrix $L_{0}$ and a sparse matrix $S_{0}$ from their sum $M = L_{0} + S_{0}$. The recovery is performed using Principal Component Pursuit (PCP).\n\nThe indicator function is given by:\n$$\nI \\equiv \\mathbf{1}\\!\\left\\{\\, r \\leq c_{r} \\frac{n}{\\mu (\\ln n)^{2}} \\ \\text{and} \\ \\rho \\leq c_{\\rho} \\,\\right\\}\n$$\nThis function evaluates to $1$ if both inequalities inside the curly braces are true, and $0$ otherwise. We are given the following specific values for the parameters:\n- Matrix dimension: $m = n = 100$\n- Rank of $L_{0}$: $r = 5$\n- Incoherence parameter of $L_{0}$: $\\mu = 3$\n- Sparsity fraction of $S_{0}$: $\\rho = 0.05$\n- Constant for the rank condition: $c_{r} = 0.4$\n- Constant for the sparsity condition: $c_{\\rho} = 0.1$\n\nTo determine the value of $I$, we must evaluate each of the two inequalities separately.\n\nFirst, let us evaluate the rank condition:\n$$\nr \\leq c_{r} \\frac{n}{\\mu (\\ln n)^{2}}\n$$\nSubstituting the given values into the left-hand side (LHS) and the right-hand side (RHS) of the inequality:\n- LHS: $r = 5$\n- RHS: $c_{r} \\frac{n}{\\mu (\\ln n)^{2}} = (0.4) \\frac{100}{3 (\\ln(100))^{2}}$\n\nWe need to compute the numerical value of the RHS. The natural logarithm of $100$ is $\\ln(100) \\approx 4.60517$.\nTherefore, $(\\ln(100))^{2} \\approx (4.60517)^{2} \\approx 21.20758$.\n\nNow, we can substitute this value back into the expression for the RHS:\n$$\n\\text{RHS} \\approx (0.4) \\frac{100}{3 \\times 21.20758} = \\frac{40}{63.62274} \\approx 0.6287\n$$\nThe inequality to check is thus:\n$$\n5 \\leq 0.6287\n$$\nThis statement is unequivocally false.\n\nSecond, let us evaluate the sparsity condition:\n$$\n\\rho \\leq c_{\\rho}\n$$\nSubstituting the given values:\n- LHS: $\\rho = 0.05$\n- RHS: $c_{\\rho} = 0.1$\n\nThe inequality to check is:\n$$\n0.05 \\leq 0.1\n$$\nThis statement is true.\n\nThe indicator function $I$ is $1$ only if both conditions are met. The overall logical condition is the conjunction of the two individual checks:\n$$\n\\left( r \\leq c_{r} \\frac{n}{\\mu (\\ln n)^{2}} \\right) \\land \\left( \\rho \\leq c_{\\rho} \\right)\n$$\nIn our case, this corresponds to:\n$$\n(\\text{false}) \\land (\\text{true})\n$$\nThe result of this logical AND operation is false.\n\nTherefore, the condition within the indicator function is not satisfied. By the definition of the indicator function, $\\mathbf{1}\\{\\text{false}\\} = 0$.\nThus, the value of $I$ is $0$. This implies that for the given parameters, the sufficient condition for guaranteed exact recovery via PCP is not met.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}