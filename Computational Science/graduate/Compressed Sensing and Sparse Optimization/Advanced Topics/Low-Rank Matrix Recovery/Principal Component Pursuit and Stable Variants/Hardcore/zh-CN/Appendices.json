{
    "hands_on_practices": [
        {
            "introduction": "主成分追踪（PCP）的许多理论优势最终需要通过有效的算法来实现。交替方向乘子法（ADMM）是解决这一问题的标准且强大的工具。本练习旨在通过从第一性原理推导其更新步骤，并对一个具体的数值例子执行单次迭代，来巩固对PCP的ADMM求解器核心机制的理解。这有助于将抽象的优化理论与具体的计算过程联系起来。",
            "id": "3468093",
            "problem": "考虑主成分追踪（Principal Component Pursuit, PCP）问题，该问题通过求解以下优化问题，将一个数据矩阵分解为一个低秩分量和一个稀疏分量：\n$$\n\\min_{L,S} \\|L\\|_{*} + \\lambda \\|S\\|_{1} \\quad \\text{subject to} \\quad M = L + S,\n$$\n其中给定矩阵 $M \\in \\mathbb{R}^{m \\times n}$，$\\|L\\|_{*}$ 表示 $L$ 的核范数（奇异值之和），$\\|S\\|_{1}$ 表示逐元素的 $\\ell_{1}$ 范数，即 $\\|S\\|_{1} = \\sum_{i,j} |S_{ij}|$。请使用带有缩放对偶变量 $U$ 和惩罚参数 $\\rho  0$ 的缩放形式的交替方向乘子法（ADMM）。\n\n仅从等式约束凸优化的增广拉格朗日公式以及核范数和逐元素 $\\ell_{1}$ 范数的近端算子定义出发，首先从第一性原理推导PCP问题对应的单次迭代的缩放ADMM中 $L$ 和 $S$ 的更新步骤。然后，对于以下具体实例\n$$\nM = \\begin{pmatrix} 3  0 \\\\ 0  0.4 \\end{pmatrix}, \\quad \\lambda = 1, \\quad \\rho = 2, \\quad S^{k} = \\begin{pmatrix} 1  0 \\\\ 0  0.1 \\end{pmatrix}, \\quad U^{k} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix},\n$$\n计算一次完整的ADMM迭代，以获得 $L^{k+1}$ 和 $S^{k+1}$。\n\n最后，计算更新后的 $(L^{k+1}, S^{k+1})$ 对的标量目标函数值\n$$\nJ = \\|L^{k+1}\\|_{*} + \\lambda \\|S^{k+1}\\|_{1},\n$$\n并将 $J$ 作为你的最终答案。请给出精确值（不要四舍五入）。",
            "solution": "主成分追踪（PCP）问题被表述为以下凸优化问题：\n$$\n\\min_{L,S} \\|L\\|_{*} + \\lambda \\|S\\|_{1} \\quad \\text{subject to} \\quad M = L + S\n$$\n其中 $L, S, M \\in \\mathbb{R}^{m \\times n}$。为了使用交替方向乘子法（ADMM）求解此问题，我们首先构造增广拉格朗日函数。在其缩放形式下，使用缩放对偶变量 $U$ 和惩罚参数 $\\rho  0$，增广拉格朗日函数为：\n$$\n\\mathcal{L}_{\\rho}(L, S, U) = \\|L\\|_{*} + \\lambda \\|S\\|_{1} + \\frac{\\rho}{2} \\|L + S - M + U\\|_{F}^{2} - \\frac{\\rho}{2} \\|U\\|_{F}^{2}\n$$\nADMM算法包括迭代更新变量 $L$、$S$ 和 $U$。在第 $k+1$ 次迭代中，更新是按顺序执行的。\n\n**1. $L$-更新的推导**\n\n$L$ 的更新通过最小化关于 $L$ 的 $\\mathcal{L}_{\\rho}$ 来找到，同时将 $S$ 和 $U$ 固定在前一次迭代的值 $S^k$ 和 $U^k$。\n$$\nL^{k+1} = \\arg\\min_{L} \\left( \\|L\\|_{*} + \\frac{\\rho}{2} \\|L + S^k - M + U^k\\|_{F}^{2} \\right)\n$$\n这可以重写为：\n$$\nL^{k+1} = \\arg\\min_{L} \\left( \\|L\\|_{*} + \\frac{\\rho}{2} \\|L - (M - S^k - U^k)\\|_{F}^{2} \\right)\n$$\n这是核范数的近端算子的定义，缩放因子为 $1/\\rho$。其解由奇异值阈值（SVT）算子给出，记为 $\\mathcal{D}_{\\tau}$。\n令 $X = M - S^k - U^k$。该最小化问题等价于求解 $\\text{prox}_{\\frac{1}{\\rho}\\|\\cdot\\|_{*}}(X)$。其解为：\n$$\nL^{k+1} = \\mathcal{D}_{1/\\rho}(M - S^k - U^k)\n$$\nSVT算子 $\\mathcal{D}_{\\tau}(X)$ 的作用方式是：首先计算 $X$ 的奇异值分解（SVD），即 $X = W\\Sigma V^T$；然后对 $\\Sigma$ 中的奇异值应用阈值为 $\\tau$ 的软阈值操作；最后重新组合成矩阵。如果 $\\Sigma = \\text{diag}(\\sigma_i)$，则经过阈值处理的奇异值为 $\\hat{\\sigma}_i = \\max(0, \\sigma_i - \\tau)$。得到的矩阵为 $W \\, \\text{diag}(\\hat{\\sigma}_i) \\, V^T$。\n\n**2. $S$-更新的推导**\n\n$S$ 的更新通过最小化关于 $S$ 的 $\\mathcal{L}_{\\rho}$ 来找到，其中使用新计算出的 $L^{k+1}$ 和前一次迭代的 $U^k$。\n$$\nS^{k+1} = \\arg\\min_{S} \\left( \\lambda \\|S\\|_{1} + \\frac{\\rho}{2} \\|L^{k+1} + S - M + U^k\\|_{F}^{2} \\right)\n$$\n这可以重写为：\n$$\nS^{k+1} = \\arg\\min_{S} \\left( \\lambda \\|S\\|_{1} + \\frac{\\rho}{2} \\|S - (M - L^{k+1} - U^k)\\|_{F}^{2} \\right)\n$$\n这是逐元素 $\\ell_1$ 范数的近端算子，缩放因子为 $\\lambda/\\rho$。这个优化问题是可分的，可以对每个元素 $S_{ij}$ 独立求解。其解由逐元素的软阈值算子给出，记为 $\\mathcal{S}_{\\tau}$。\n令 $Y = M - L^{k+1} - U^k$。该最小化问题等价于求解 $\\text{prox}_{\\frac{\\lambda}{\\rho}\\|\\cdot\\|_{1}}(Y)$。其解为：\n$$\nS^{k+1} = \\mathcal{S}_{\\lambda/\\rho}(M - L^{k+1} - U^k)\n$$\n对于一个标量 $y$，软阈值算子 $\\mathcal{S}_{\\tau}(y)$ 定义为 $\\mathcal{S}_{\\tau}(y) = \\text{sign}(y)\\max(0, |y|-\\tau)$。对于一个矩阵，该操作是逐元素应用的。\n\n**3. 对偶变量 $U$ 的更新**\n\n缩放的对偶变量更新如下：\n$$\nU^{k+1} = U^k + L^{k+1} + S^{k+1} - M\n$$\n\n**针对具体实例的计算**\n\n我们给定以下值：\n$M = \\begin{pmatrix} 3  0 \\\\ 0  0.4 \\end{pmatrix}$，$\\lambda = 1$，$\\rho = 2$， $S^{k} = \\begin{pmatrix} 1  0 \\\\ 0  0.1 \\end{pmatrix}$， $U^{k} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$。\n\n近端算子的阈值为：\n- 对于 $L$ 的更新：$\\tau_L = 1/\\rho = 1/2 = 0.5$。\n- 对于 $S$ 的更新：$\\tau_S = \\lambda/\\rho = 1/2 = 0.5$。\n\n**步骤 1：计算 $L^{k+1}$**\n\n首先，我们计算需要进行阈值处理的矩阵：\n$$\nX = M - S^k - U^k = \\begin{pmatrix} 3  0 \\\\ 0  0.4 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  0.1 \\end{pmatrix} - \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0.3 \\end{pmatrix}\n$$\n矩阵 $X$ 是一个对角矩阵，所以其奇异值为对角元素的绝对值：$\\sigma_1 = 2$ 和 $\\sigma_2 = 0.3$。其SVD为 $X = I \\begin{pmatrix} 2  0 \\\\ 0  0.3 \\end{pmatrix} I^T$。\n我们使用 $\\tau_L = 0.5$ 进行奇异值阈值处理：\n$$\n\\hat{\\sigma}_1 = \\max(0, 2 - 0.5) = 1.5\n$$\n$$\n\\hat{\\sigma}_2 = \\max(0, 0.3 - 0.5) = 0\n$$\n所以，更新后的低秩分量为：\n$$\nL^{k+1} = I \\begin{pmatrix} 1.5  0 \\\\ 0  0 \\end{pmatrix} I^T = \\begin{pmatrix} 1.5  0 \\\\ 0  0 \\end{pmatrix}\n$$\n\n**步骤 2：计算 $S^{k+1}$**\n\n接下来，我们计算用于逐元素软阈值处理的矩阵：\n$$\nY = M - L^{k+1} - U^k = \\begin{pmatrix} 3  0 \\\\ 0  0.4 \\end{pmatrix} - \\begin{pmatrix} 1.5  0 \\\\ 0  0 \\end{pmatrix} - \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1.5  0 \\\\ 0  0.4 \\end{pmatrix}\n$$\n我们使用 $\\tau_S = 0.5$ 进行逐元素的软阈值处理：\n$$\nS^{k+1}_{11} = \\mathcal{S}_{0.5}(1.5) = \\text{sign}(1.5)\\max(0, |1.5| - 0.5) = 1.0\n$$\n$$\nS^{k+1}_{12} = \\mathcal{S}_{0.5}(0) = \\text{sign}(0)\\max(0, |0| - 0.5) = 0\n$$\n$$\nS^{k+1}_{21} = \\mathcal{S}_{0.5}(0) = 0\n$$\n$$\nS^{k+1}_{22} = \\mathcal{S}_{0.5}(0.4) = \\text{sign}(0.4)\\max(0, |0.4| - 0.5) = 0\n$$\n所以，更新后的稀疏分量为：\n$$\nS^{k+1} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}\n$$\n\n**步骤 3：计算目标函数值 $J$**\n\n最后一步是计算目标函数值 $J = \\|L^{k+1}\\|_{*} + \\lambda \\|S^{k+1}\\|_{1}$，其中 $\\lambda = 1$。\n\n$L^{k+1}$ 的核范数是其奇异值之和。$L^{k+1} = \\begin{pmatrix} 1.5  0 \\\\ 0  0 \\end{pmatrix}$ 的奇异值为 $1.5$ 和 $0$。\n$$\n\\|L^{k+1}\\|_{*} = 1.5 + 0 = 1.5\n$$\n$S^{k+1}$ 的逐元素 $\\ell_1$ 范数是其各元素绝对值之和。\n$$\n\\|S^{k+1}\\|_{1} = |1| + |0| + |0| + |0| = 1\n$$\n目标函数值 $J$ 为：\n$$\nJ = \\|L^{k+1}\\|_{*} + \\lambda \\|S^{k+1}\\|_{1} = 1.5 + (1)(1) = 2.5\n$$",
            "answer": "$$\n\\boxed{2.5}\n$$"
        },
        {
            "introduction": "现实世界的数据往往是不完整的，这要求我们使用稳定主成分追踪（Stable PCP）等更稳健的模型。本练习将基础的PCP框架扩展到处理缺失数据的场景，通过一个具体的块次采样算子为例。通过这个实践，你将学习如何将线性算子及其伴随算子整合到ADMM框架中，并理解如何利用算子的结构（例如投影）来简化计算，这是解决实际应用问题的关键技能。",
            "id": "3468078",
            "problem": "考虑在压缩感知中带有缺失数据的稳定主成分追踪 (PCP) 变体，该问题在 $\\mathbb{R}^{n \\times n}$ 上表述为将一个测量矩阵分解为一个低秩分量和一个稀疏分量。设 $n$ 为 $4$，并将任意一个 $4 \\times 4$ 矩阵划分为四个相邻的 $2 \\times 2$ 块。定义块索引集\n$$\n\\Omega_{1} = \\{(i,j) \\in \\{1,2\\} \\times \\{1,2\\}\\}, \\quad \\Omega_{4} = \\{(i,j) \\in \\{3,4\\} \\times \\{3,4\\}\\}.\n$$\n令块子采样算子 $\\mathcal{A} : \\mathbb{R}^{4 \\times 4} \\to \\mathbb{R}^{8}$ 对矩阵的向量化形式作​​用如下：对于任意 $X \\in \\mathbb{R}^{4 \\times 4}$，\n$$\n\\mathcal{A}(X) = \\begin{pmatrix} \\mathrm{vec}(X|_{\\Omega_{1}}) \\\\ \\mathrm{vec}(X|_{\\Omega_{4}}) \\end{pmatrix},\n$$\n其中 $\\mathrm{vec}$ 表示对一个 $2 \\times 2$ 块的列向向量化，而 $X|_{\\Omega}$ 表示将 $X$ 限制在索引集 $\\Omega$ 上，并且其伴随算子 $\\mathcal{A}^{*} : \\mathbb{R}^{8} \\to \\mathbb{R}^{4 \\times 4}$ 将两个 $2 \\times 2$ 块散布回其原始位置，并在其他位置置零。\n\n设观测数据为 $\\mathcal{A}(M)$，其中\n$$\nM = \\begin{pmatrix}\n4  -1  0  0 \\\\\n2  3  0  0 \\\\\n0  0  1  -2 \\\\\n0  0  0  7\n\\end{pmatrix},\n$$\n这样，只有 $M$ 中位于 $\\Omega_{1}$ 和 $\\Omega_{4}$ 的元素被观测到。考虑针对带缺失数据的稳定PCP的以下分裂方法：引入 $D \\in \\mathbb{R}^{4 \\times 4}$ 使得 $D = L + S$，并施加测量约束 $\\mathcal{A}(D) = \\mathcal{A}(M)$，其中 $L$ 是低秩的，$S$ 是稀疏的。使用交替方向乘子法 (ADMM, Alternating Direction Method of Multipliers)，其增广拉格朗日函数分别用正常数 $\\rho$ 和 $\\eta$ 惩罚约束 $D = L + S$ 和 $\\mathcal{A}(D) = \\mathcal{A}(M)$。假设缩放对偶变量初始为零，并初始化 $L^{(0)} = 0$，$S^{(0)} = 0$，$D^{(0)} = 0$，其中 $\\rho = 2$ 和 $\\eta = 3$。\n\n从第一性原理出发，推导显式包含 $\\mathcal{A}^{*}$ 的 $D$-更新最优性条件，并计算第一次 $D$-更新后 $D$ 的 $(4,4)$-元素的值，记为 $D^{(1)}_{4,4}$。将最终数值答案表示为单个实数。无需四舍五入，不涉及单位。重点应在于说明 $\\mathcal{A}$ 的块子采样结构如何通过刻画 $\\mathcal{A}^{*}\\mathcal{A}$ 来提高计算效率。",
            "solution": "该问题采用了一个分裂变量 $D \\in \\mathbb{R}^{4 \\times 4}$ 并施加了两个约束：\n1.  $D - L - S = 0$\n2.  $\\mathcal{A}(D) - \\mathcal{A}(M) = 0$\n\n该问题的增广拉格朗日函数（使用缩放对偶变量 $U_1, U_2$ 和惩罚参数 $\\rho, \\eta$）是：\n$$\n\\mathcal{L}_{\\rho, \\eta}(L, S, D, U_1, U_2) = f(L) + g(S) + \\frac{\\rho}{2}\\|D - L - S + U_1\\|_F^2 + \\frac{\\eta}{2}\\|\\mathcal{A}(D) - \\mathcal{A}(M) + U_2\\|_2^2 - C\n$$\n其中 $f(L)$ 和 $g(S)$ 是正则化项，$C$ 是常数项。\n\nADMM算法顺序更新 $L, S, D$。我们关注 $D$ 的第一次更新 $D^{(1)}$。\n初始条件为 $L^{(0)}=0, S^{(0)}=0, D^{(0)}=0, U_1^{(0)}=0, U_2^{(0)}=0$。\n在一个标准的ADMM流程中，首先更新$L$和$S$。\n$L^{(1)} = \\arg\\min_L f(L) + \\frac{\\rho}{2}\\|D^{(0)} - L - S^{(0)} + U_1^{(0)}\\|_F^2 = \\arg\\min_L f(L) + \\frac{\\rho}{2}\\|-L\\|_F^2$。对于典型的正则项如核范数，此更新得到 $L^{(1)}=0$。\n$S^{(1)} = \\arg\\min_S g(S) + \\frac{\\rho}{2}\\|D^{(0)} - L^{(1)} - S + U_1^{(0)}\\|_F^2 = \\arg\\min_S g(S) + \\frac{\\rho}{2}\\|-S\\|_F^2$。对于典型的正则项如$\\ell_1$范数，此更新得到 $S^{(1)}=0$。\n\n接下来，我们推导 $D$ 的更新。$D^{(1)}$ 的子问题是最小化关于 $D$ 的拉格朗日函数，固定其他变量为最新值：\n$$\nD^{(1)} = \\arg\\min_D \\left( \\frac{\\rho}{2}\\|D - L^{(1)} - S^{(1)} + U_1^{(0)}\\|_F^2 + \\frac{\\eta}{2}\\|\\mathcal{A}(D) - \\mathcal{A}(M) + U_2^{(0)}\\|_2^2 \\right).\n$$\n代入已知值（$L^{(1)}=0, S^{(1)}=0, U_1^{(0)}=0, U_2^{(0)}=0$）：\n$$\nD^{(1)} = \\arg\\min_D J(D) \\quad \\text{其中} \\quad J(D) = \\frac{\\rho}{2}\\|D\\|_F^2 + \\frac{\\eta}{2}\\|\\mathcal{A}(D) - \\mathcal{A}(M)\\|_2^2.\n$$\n这是一个二次可微的凸函数，其最小值点满足梯度为零的条件。我们对 $J(D)$ 求关于 $D$ 的梯度，并令其为零。使用链式法则和伴随算子 $\\mathcal{A}^*$ 的性质（$\\nabla_X \\|\\mathcal{A}(X)-y\\|_2^2 = 2\\mathcal{A}^*(\\mathcal{A}(X)-y)$），我们得到 $D$-更新的最优性条件：\n$$\n\\nabla_D J(D) = \\rho D + \\eta \\mathcal{A}^*(\\mathcal{A}(D) - \\mathcal{A}(M)) = 0.\n$$\n为了求解 $D$，我们重新整理该方程：\n$$\n(\\rho I + \\eta \\mathcal{A}^*\\mathcal{A}) D = \\eta \\mathcal{A}^*(\\mathcal{A}(M)).\n$$\n这里的关键在于理解算子 $\\mathcal{A}^*\\mathcal{A}$ 的结构。算子 $\\mathcal{A}$ 从矩阵中提取位于 $\\Omega = \\Omega_1 \\cup \\Omega_4$ 的块。其伴随算子 $\\mathcal{A}^*$ 将这些块放回一个零矩阵的相应位置。因此，复合算子 $\\mathcal{A}^*\\mathcal{A}$ 的作用是将矩阵中所有不在 $\\Omega$ 内的元素置零，这正是一个正交投影算子 $P_\\Omega$。同样地，$\\mathcal{A}^*(\\mathcal{A}(M))$ 就是 $P_\\Omega(M)$。\n方程简化为：\n$$\n(\\rho I + \\eta P_\\Omega) D = \\eta P_\\Omega(M).\n$$\n这个方程是逐元素可解的。\n\n**情况 1: 索引 $(i,j) \\in \\Omega$**\n对于这些索引，$(P_\\Omega D)_{ij} = D_{ij}$ 且 $(P_\\Omega M)_{ij} = M_{ij}$。方程变为：\n$(\\rho + \\eta) D_{ij} = \\eta M_{ij} \\implies D_{ij} = \\frac{\\eta}{\\rho + \\eta} M_{ij}$。\n\n**情况 2: 索引 $(i,j) \\notin \\Omega$**\n对于这些索引，$(P_\\Omega D)_{ij} = 0$ 且 $(P_\\Omega M)_{ij} = 0$。方程变为：\n$\\rho D_{ij} = 0 \\implies D_{ij} = 0$。\n\n我们需要计算 $D^{(1)}_{4,4}$。索引 $(4,4)$ 属于块 $\\Omega_4$，因此属于情况 1。\n使用给定的值 $\\rho = 2$, $\\eta = 3$ 和 $M_{4,4} = 7$：\n$$\nD^{(1)}_{4,4} = \\frac{3}{2 + 3} \\times 7 = \\frac{3}{5} \\times 7 = \\frac{21}{5}.\n$$\n因此，第一次更新后 $D$ 的 $(4,4)$-元素的值是 $\\frac{21}{5}$。",
            "answer": "$$\\boxed{\\frac{21}{5}}$$"
        },
        {
            "introduction": "任何模型的成功都关键在于超参数的正确选择。最后一个练习将重点从数值计算转向概念分析，探讨正则化参数 $\\lambda$ 如何控制低秩与稀疏分量之间的权衡。通过分析极端情况下模型的最优性条件，我们将对PCP模型的行为获得深刻的直觉，这种“思想实验”是连接理论理解与成功应用的重要桥梁。",
            "id": "3468116",
            "problem": "考虑主成分追踪 (Principal Component Pursuit, PCP) 模型，该模型通过求解以下凸优化问题，将一个给定的数据矩阵 $M \\in \\mathbb{R}^{n_1 \\times n_2}$ 分解为一个低秩分量 $L$ 和一个稀疏分量 $S$ 的和 $M = L + S$：\n在等式约束 $L + S = M$ 下，对 $(L,S)$ 最小化目标函数 $\\|L\\|_{*} + \\lambda \\|S\\|_{1}$，\n其中 $\\|L\\|_{*}$ 表示核范数（奇异值之和），$\\|S\\|_{1}$ 表示逐项 $\\ell_{1}$ 范数。假设标准的凸对偶和次梯度最优性条件适用。仅使用凸分析的基本原理（特别是等式约束凸规划的次梯度最优性）以及核范数和逐项 $\\ell_{1}$ 范数次梯度的几何性质，分析选择正则化参数 $\\lambda$ 过小或过大的影响。\n\n选择所有正确的陈述。您的论证必须依赖于最优性下次梯度的符号和幅值，而不借助核心次梯度演算和算子范数界之外的任何外部定理。\n\nA. 如果 $\\lambda$ 相对于 $M$ 符号模式的算子范数足够小，那么 $(L^{\\star},S^{\\star}) = (0,M)$ 满足一阶最优性条件并且是一个最优解。特别地，可以通过取对偶变量 $Y^{\\star} = \\lambda \\,\\operatorname{sign}(M)$ 来证明其最优性，只要 $\\lambda \\le 1/\\|\\operatorname{sign}(M)\\|$，该变量就属于 $\\partial \\|L\\|_{*}(0)$，这样所有的质量都被置于 $S^{\\star}$ 中。\n\nB. 存在一个通用常数 $C \\in \\mathbb{R}_{+}$（独立于 $M$）使得如果 $\\lambda \\ge C$，那么 $(L^{\\star},S^{\\star}) = (M,0)$ 满足一阶最优性条件并且是一个最优解。特别地，对于 $C = 2$，可以取任意 $Y^{\\star} \\in \\partial \\|L\\|_{*}(M)$，逐项界 $\\|Y^{\\star}\\|_{\\infty} \\le \\|Y^{\\star}\\| \\le 2$ 确保 $Y^{\\star} \\in \\lambda \\,\\partial \\|S\\|_{1}(0)$，这样所有的质量都被置于 $L^{\\star}$ 中。\n\nC. 存在一个通用常数 $c \\in \\mathbb{R}_{+}$（独立于 $M$ 及其维度），对于所有问题，如果 $\\lambda \\in (0,c)$，那么每个最优解都满足 $L^{\\star} = 0$ 和 $S^{\\star} = M$。\n\nD. 在任何最优解 $(L^{\\star},S^{\\star})$ 处，平稳性条件要求对偶凭证 $Y^{\\star}$ 在 $S^{\\star}$ 的支撑集上（即，在所有 $S^{\\star}_{ij} \\ne 0$ 的 $(i,j)$ 处）满足不等式 $|Y^{\\star}_{ij}| \\le \\lambda$，但并不要求在该处严格相等。\n\n选择所有正确的选项。",
            "solution": "该问题要求使用最优性条件来分析正则化参数 $\\lambda$ 的作用。最优解 $(L^{\\star}, S^{\\star})$ 必须满足存在一个对偶矩阵（或对偶凭证）$Y^{\\star}$，使得：\n1.  $L^{\\star} + S^{\\star} = M$ (原始可行性)\n2.  $Y^{\\star} \\in \\partial \\|L^{\\star}\\|_{*}$ (关于 $L$ 的平稳性)\n3.  $Y^{\\star} \\in \\lambda \\partial \\|S^{\\star}\\|_{1}$ (关于 $S$ 的平稳性)\n\n我们将逐一分析每个选项。\n\n**A. 如果 $\\lambda$ ... 足够小, 那么 $(L^{\\star},S^{\\star}) = (0,M)$ ... 是一个最优解。**\n我们来验证建议的解 $(L^{\\star},S^{\\star}) = (0,M)$ 是否满足最优性条件。\n1.  原始可行性: $0 + M = M$。满足。\n2.  为了同时满足两个平稳性条件，我们需要找到一个 $Y^{\\star}$ 使得 $Y^{\\star} \\in \\partial \\|0\\|_{*}$ 且 $Y^{\\star} \\in \\lambda \\partial \\|M\\|_{1}$。\n    -   $Y^{\\star} \\in \\lambda \\partial \\|M\\|_{1}$：我们可以通过选择 $Y^{\\star} = \\lambda \\operatorname{sign}(M)$ 来满足此条件（这里 $\\operatorname{sign}(M)$ 是一个与 $M$ 符号一致的有效次梯度）。\n    -   $Y^{\\star} \\in \\partial \\|0\\|_{*}$：这个条件要求所选的 $Y^{\\star}$ 满足算子范数界 $\\|Y^{\\star}\\| \\le 1$。\n    将我们选择的 $Y^{\\star}$ 代入，得到 $\\|\\lambda \\operatorname{sign}(M)\\| \\le 1$，即 $\\lambda \\|\\operatorname{sign}(M)\\| \\le 1$，或 $\\lambda \\le 1/\\|\\operatorname{sign}(M)\\|$。\n因此，如果 $\\lambda$ 满足这个条件，那么解 $(0,M)$ 就是最优的。陈述A正确。\n\n**B. 存在一个通用常数 $C$ ... 使得如果 $\\lambda \\ge C$, 那么 $(L^{\\star},S^{\\star}) = (M,0)$ ... 是一个最优解。**\n我们验证建议的解 $(L^{\\star},S^{\\star}) = (M,0)$。\n1.  原始可行性: $M + 0 = M$。满足。\n2.  我们需要找到一个 $Y^{\\star}$ 使得 $Y^{\\star} \\in \\partial \\|M\\|_{*}$ 且 $Y^{\\star} \\in \\lambda \\partial \\|0\\|_{1}$。\n    -   $Y^{\\star} \\in \\lambda \\partial \\|0\\|_{1}$ 等价于 $Y^{\\star}/\\lambda \\in \\partial \\|0\\|_{1}$，这意味着对所有 $(i,j)$ 都有 $|(Y^{\\star}/\\lambda)_{ij}| \\le 1$，即 $\\|Y^{\\star}\\|_{\\infty} \\le \\lambda$。\n    -   $Y^{\\star} \\in \\partial \\|M\\|_{*}$：我们需要从这个集合中找到一个满足上述无穷范数约束的 $Y^{\\star}$。\n    核范数次梯度的一个基本性质是，对于任何 $Y^{\\star} \\in \\partial \\|M\\|_{*}$，其算子范数满足 $\\|Y^{\\star}\\| \\le 1$。由于无穷范数总是小于等于算子范数（$\\|Y^{\\star}\\|_{\\infty} \\le \\|Y^{\\star}\\|$），我们有 $\\|Y^{\\star}\\|_{\\infty} \\le 1$。\n    因此，只要我们选择 $\\lambda \\ge 1$，条件 $\\|Y^{\\star}\\|_{\\infty} \\le \\lambda$ 就对任何 $Y^{\\star} \\in \\partial \\|M\\|_{*}$ 自动满足。所以，存在一个通用常数 $C=1$ 使得该陈述为真。陈述中建议 $C=2$ 及其推导也是一个有效的（尽管不紧凑的）论证。陈述B正确。\n\n**C. 存在一个通用常数 $c$ ... 如果 $\\lambda \\in (0,c)$，那么每个最优解都满足 $L^{\\star} = 0$ 和 $S^{\\star} = M$。**\n这个陈述是错误的。从对A的分析可知，$(0,M)$ 是一个最优解的条件是 $\\lambda \\le 1/\\|\\operatorname{sign}(M)\\|$。这个阈值明确地依赖于矩阵 $M$ 及其维度。例如，如果 $M$ 是 $n \\times n$ 的全1矩阵，则 $\\|\\operatorname{sign}(M)\\| = n$，阈值变为 $1/n$。当 $n \\to \\infty$ 时，这个阈值趋于0。因此，不存在一个独立于 $M$ 和其维度的通用常数 $c > 0$ 能保证对于所有 $\\lambda  c$，解总是 $(0,M)$。\n\n**D. 在任何最优解 ... $Y^{\\star}$ 在 $S^{\\star}$ 的支撑集上 ... 满足不等式 $|Y^{\\star}_{ij}| \\le \\lambda$，但并不要求在该处严格相等。**\n这个陈述是错误的。平稳性条件 $Y^{\\star} \\in \\lambda \\partial \\|S^{\\star}\\|_{1}$ 意味着 $Y^{\\star}/\\lambda \\in \\partial \\|S^{\\star}\\|_{1}$。根据 $\\ell_1$ 范数次梯度的定义，对于任何在 $S^{\\star}$ 支撑集上的索引 $(i,j)$（即 $S^{\\star}_{ij} \\ne 0$），$\\ell_1$ 范数在该点是可微的，其（次）梯度是唯一的，即 $(\\partial \\|S^{\\star}\\|_{1})_{ij} = \\operatorname{sign}(S^{\\star}_{ij})$。因此，在这些点上必须有 $(Y^{\\star}/\\lambda)_{ij} = \\operatorname{sign}(S^{\\star}_{ij})$，这意味着 $|Y^{\\star}_{ij}| = \\lambda$。等式是严格要求的。\n\n综上，正确的选项是 A 和 B。",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}