{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how Principal Component Pursuit (PCP) works, we must look under the hood of its most common solver: the Alternating Direction Method of Multipliers (ADMM). This first exercise demystifies the algorithm by guiding you through a single, concrete iteration. By manually applying the singular value thresholding operator to the low-rank component and the soft-thresholding operator to the sparse component, you will solidify your understanding of the core mechanics that drive the decomposition .",
            "id": "3468093",
            "problem": "Consider the Principal Component Pursuit (PCP) problem, which decomposes a data matrix into a low-rank component and a sparse component by solving\n$$\n\\min_{L,S} \\lVert L \\rVert_{*} + \\lambda \\lVert S \\rVert_{1} \\quad \\text{subject to} \\quad M = L + S,\n$$\nwhere $M \\in \\mathbb{R}^{m \\times n}$ is given, $\\lVert L \\rVert_{*}$ denotes the nuclear norm (the sum of singular values) of $L$, and $\\lVert S \\rVert_{1}$ denotes the elementwise $\\ell_{1}$ norm $\\lVert S \\rVert_{1} = \\sum_{i,j} |S_{ij}|$. Use the Alternating Direction Method of Multipliers (ADMM) in its scaled form with a scaled dual variable $U$ and penalty parameter $\\rho > 0$.\n\nStarting only from the augmented Lagrangian formulation of equality-constrained convex optimization and the proximal definitions for the nuclear norm and the elementwise $\\ell_{1}$ norm, first derive, from first principles, the single-iteration scaled ADMM updates for $L$ and $S$ corresponding to the PCP problem. Then, for the specific instance\n$$\nM = \\begin{pmatrix} 3 & 0 \\\\ 0 & 0.4 \\end{pmatrix}, \\quad \\lambda = 1, \\quad \\rho = 2, \\quad S^{k} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0.1 \\end{pmatrix}, \\quad U^{k} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix},\n$$\ncompute one full ADMM iteration to obtain $L^{k+1}$ and $S^{k+1}$.\n\nFinally, compute the scalar objective value at the updated pair,\n$$\nJ = \\lVert L^{k+1} \\rVert_{*} + \\lambda \\lVert S^{k+1} \\rVert_{1},\n$$\nand report $J$ as your final answer. Express your final answer exactly (do not round).",
            "solution": "The Principal Component Pursuit (PCP) problem is formulated as the following convex optimization problem:\n$$\n\\min_{L,S} \\lVert L \\rVert_{*} + \\lambda \\lVert S \\rVert_{1} \\quad \\text{subject to} \\quad M = L + S\n$$\nwhere $L, S, M \\in \\mathbb{R}^{m \\times n}$. To solve this using the Alternating Direction Method of Multipliers (ADMM), we first form the augmented Lagrangian. In its scaled form, with a scaled dual variable $U$ and penalty parameter $\\rho > 0$, the augmented Lagrangian is:\n$$\n\\mathcal{L}_{\\rho}(L, S, U) = \\lVert L \\rVert_{*} + \\lambda \\lVert S \\rVert_{1} + \\frac{\\rho}{2} \\lVert L + S - M + U \\rVert_{F}^{2} - \\frac{\\rho}{2} \\lVert U \\rVert_{F}^{2}\n$$\nThe ADMM algorithm consists of iteratively updating the variables $L$, $S$, and $U$. At iteration $k+1$, the updates are performed sequentially.\n\n**1. Derivation of the $L$-update**\n\nThe update for $L$ is found by minimizing $\\mathcal{L}_{\\rho}$ with respect to $L$, keeping $S$ and $U$ fixed at their values from the previous iteration, $S^k$ and $U^k$.\n$$\nL^{k+1} = \\arg\\min_{L} \\left( \\lVert L \\rVert_{*} + \\frac{\\rho}{2} \\lVert L + S^k - M + U^k \\rVert_{F}^{2} \\right)\n$$\nThis can be rewritten as:\n$$\nL^{k+1} = \\arg\\min_{L} \\left( \\lVert L \\rVert_{*} + \\frac{\\rho}{2} \\lVert L - (M - S^k - U^k) \\rVert_{F}^{2} \\right)\n$$\nThis is the definition of the proximal operator of the nuclear norm, scaled by $1/\\rho$. The solution is given by the Singular Value Thresholding (SVT) operator, denoted by $\\mathcal{D}_{\\tau}$.\nLet $X = M - S^k - U^k$. The minimization problem is equivalent to finding $\\text{prox}_{\\frac{1}{\\rho}\\lVert\\cdot\\rVert_{*}}(X)$. The solution is:\n$$\nL^{k+1} = \\mathcal{D}_{1/\\rho}(M - S^k - U^k)\n$$\nThe SVT operator $\\mathcal{D}_{\\tau}(X)$ acts by computing the Singular Value Decomposition (SVD) of $X$ as $X = W\\Sigma V^T$, applying soft-thresholding to the singular values in $\\Sigma$ with threshold $\\tau$, and then re-forming the matrix. If $\\Sigma = \\text{diag}(\\sigma_i)$, the thresholded singular values are $\\hat{\\sigma}_i = \\max(0, \\sigma_i - \\tau)$. The resulting matrix is $W \\, \\text{diag}(\\hat{\\sigma}_i) \\, V^T$.\n\n**2. Derivation of the $S$-update**\n\nThe update for $S$ is found by minimizing $\\mathcal{L}_{\\rho}$ with respect to $S$, using the newly computed $L^{k+1}$ and the previous $U^k$.\n$$\nS^{k+1} = \\arg\\min_{S} \\left( \\lambda \\lVert S \\rVert_{1} + \\frac{\\rho}{2} \\lVert L^{k+1} + S - M + U^k \\rVert_{F}^{2} \\right)\n$$\nThis can be rewritten as:\n$$\nS^{k+1} = \\arg\\min_{S} \\left( \\lambda \\lVert S \\rVert_{1} + \\frac{\\rho}{2} \\lVert S - (M - L^{k+1} - U^k) \\rVert_{F}^{2} \\right)\n$$\nThis is the proximal operator of the elementwise $\\ell_1$ norm, scaled by $\\lambda/\\rho$. This optimization problem is separable and can be solved for each element $S_{ij}$ independently. The solution is given by the elementwise soft-thresholding operator, denoted by $\\mathcal{S}_{\\tau}$.\nLet $Y = M - L^{k+1} - U^k$. The minimization is equivalent to finding $\\text{prox}_{\\frac{\\lambda}{\\rho}\\lVert\\cdot\\rVert_{1}}(Y)$. The solution is:\n$$\nS^{k+1} = \\mathcal{S}_{\\lambda/\\rho}(M - L^{k+1} - U^k)\n$$\nThe soft-thresholding operator $\\mathcal{S}_{\\tau}(y)$ is defined for a scalar $y$ as $\\mathcal{S}_{\\tau}(y) = \\text{sign}(y)\\max(0, |y|-\\tau)$. For a matrix, it is applied elementwise.\n\n**3. Dual variable $U$-update**\n\nThe scaled dual variable is updated as:\n$$\nU^{k+1} = U^k + L^{k+1} + S^{k+1} - M\n$$\n\n**Computation for the specific instance**\n\nWe are given the following values:\n$M = \\begin{pmatrix} 3 & 0 \\\\ 0 & 0.4 \\end{pmatrix}$, $\\lambda = 1$, $\\rho = 2$, $S^{k} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0.1 \\end{pmatrix}$, $U^{k} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n\nThe thresholds for the proximal operators are:\n- For $L$-update: $\\tau_L = 1/\\rho = 1/2 = 0.5$.\n- For $S$-update: $\\tau_S = \\lambda/\\rho = 1/2 = 0.5$.\n\n**Step 1: Compute $L^{k+1}$**\n\nFirst, we compute the matrix to be thresholded:\n$$\nX = M - S^k - U^k = \\begin{pmatrix} 3 & 0 \\\\ 0 & 0.4 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0.1 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 0.3 \\end{pmatrix}\n$$\nThe matrix $X$ is diagonal, so its singular values are the absolute values of the diagonal entries: $\\sigma_1 = 2$ and $\\sigma_2 = 0.3$. The SVD is $X = I \\begin{pmatrix} 2 & 0 \\\\ 0 & 0.3 \\end{pmatrix} I^T$.\nWe apply singular value thresholding with $\\tau_L = 0.5$:\n$$\n\\hat{\\sigma}_1 = \\max(0, 2 - 0.5) = 1.5\n$$\n$$\n\\hat{\\sigma}_2 = \\max(0, 0.3 - 0.5) = 0\n$$\nSo, the updated low-rank component is:\n$$\nL^{k+1} = I \\begin{pmatrix} 1.5 & 0 \\\\ 0 & 0 \\end{pmatrix} I^T = \\begin{pmatrix} 1.5 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\n**Step 2: Compute $S^{k+1}$**\n\nNext, we compute the matrix for elementwise soft-thresholding:\n$$\nY = M - L^{k+1} - U^k = \\begin{pmatrix} 3 & 0 \\\\ 0 & 0.4 \\end{pmatrix} - \\begin{pmatrix} 1.5 & 0 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1.5 & 0 \\\\ 0 & 0.4 \\end{pmatrix}\n$$\nWe apply elementwise soft-thresholding with $\\tau_S = 0.5$:\n$$\nS^{k+1}_{11} = \\mathcal{S}_{0.5}(1.5) = \\text{sign}(1.5)\\max(0, |1.5| - 0.5) = 1.0\n$$\n$$\nS^{k+1}_{12} = \\mathcal{S}_{0.5}(0) = \\text{sign}(0)\\max(0, |0| - 0.5) = 0\n$$\n$$\nS^{k+1}_{21} = \\mathcal{S}_{0.5}(0) = 0\n$$\n$$\nS^{k+1}_{22} = \\mathcal{S}_{0.5}(0.4) = \\text{sign}(0.4)\\max(0, |0.4| - 0.5) = 0\n$$\nSo, the updated sparse component is:\n$$\nS^{k+1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\n**Step 3: Compute the objective value $J$**\n\nThe final step is to compute the objective value $J = \\lVert L^{k+1} \\rVert_{*} + \\lambda \\lVert S^{k+1} \\rVert_{1}$ with $\\lambda = 1$.\n\nThe nuclear norm of $L^{k+1}$ is the sum of its singular values. The singular values of $L^{k+1} = \\begin{pmatrix} 1.5 & 0 \\\\ 0 & 0 \\end{pmatrix}$ are $1.5$ and $0$.\n$$\n\\lVert L^{k+1} \\rVert_{*} = 1.5 + 0 = 1.5\n$$\nThe elementwise $\\ell_1$ norm of $S^{k+1}$ is the sum of the absolute values of its elements.\n$$\n\\lVert S^{k+1} \\rVert_{1} = |1| + |0| + |0| + |0| = 1\n$$\nThe objective value $J$ is:\n$$\nJ = \\lVert L^{k+1} \\rVert_{*} + \\lambda \\lVert S^{k+1} \\rVert_{1} = 1.5 + (1)(1) = 2.5\n$$",
            "answer": "$$\n\\boxed{2.5}\n$$"
        },
        {
            "introduction": "Real-world data is often incomplete, and a powerful feature of the PCP framework is its ability to handle missing entries, a scenario common in compressed sensing. This practice extends the basic ADMM algorithm to the stable PCP variant, where observations are made through a linear operator $\\mathcal{A}$. You will derive an update step that explicitly involves the adjoint operator $\\mathcal{A}^*$, revealing how the structure of the measurement process directly influences the computational solution and enables efficient implementation .",
            "id": "3468078",
            "problem": "Consider the stable variant of Principal Component Pursuit (PCP) in compressed sensing with missing data, formulated on $\\mathbb{R}^{n \\times n}$ as the decomposition of a measurement matrix into a low-rank component and a sparse component. Let $n$ be $4$, and partition any $4 \\times 4$ matrix into four contiguous $2 \\times 2$ blocks. Define the block index sets\n$$\n\\Omega_{1} = \\{(i,j) \\in \\{1,2\\} \\times \\{1,2\\}\\}, \\quad \\Omega_{4} = \\{(i,j) \\in \\{3,4\\} \\times \\{3,4\\}\\}.\n$$\nLet the block-subsampling operator $\\mathcal{A} : \\mathbb{R}^{4 \\times 4} \\to \\mathbb{R}^{8}$ act on the vectorized form of a matrix as follows: for any $X \\in \\mathbb{R}^{4 \\times 4}$,\n$$\n\\mathcal{A}(X) = \\begin{pmatrix} \\mathrm{vec}(X|_{\\Omega_{1}}) \\\\ \\mathrm{vec}(X|_{\\Omega_{4}}) \\end{pmatrix},\n$$\nwhere $\\mathrm{vec}$ denotes columnwise vectorization of a $2 \\times 2$ block and $X|_{\\Omega}$ denotes restriction of $X$ to index set $\\Omega$, and the adjoint $\\mathcal{A}^{*} : \\mathbb{R}^{8} \\to \\mathbb{R}^{4 \\times 4}$ scatters two $2 \\times 2$ blocks back to their original positions, placing zeros elsewhere.\n\nLet the observed data be $\\mathcal{A}(M)$ for\n$$\nM = \\begin{pmatrix}\n4 & -1 & 0 & 0 \\\\\n2 & 3 & 0 & 0 \\\\\n0 & 0 & 1 & -2 \\\\\n0 & 0 & 0 & 7\n\\end{pmatrix},\n$$\nso that only $\\Omega_{1}$ and $\\Omega_{4}$ entries of $M$ are observed. Consider the following splitting for stable PCP with missing data: introduce $D \\in \\mathbb{R}^{4 \\times 4}$ so that $D = L + S$, and enforce the measurement constraint $\\mathcal{A}(D) = \\mathcal{A}(M)$, where $L$ is low-rank and $S$ is sparse. Use Alternating Direction Method of Multipliers (ADMM) with the augmented Lagrangian penalizing the constraints $D = L + S$ and $\\mathcal{A}(D) = \\mathcal{A}(M)$ with positive parameters $\\rho$ and $\\eta$, respectively. Assume scaled dual variables are initially zero and initialize $L^{(0)} = 0$, $S^{(0)} = 0$, $D^{(0)} = 0$, with $\\rho = 2$ and $\\eta = 3$.\n\nFrom first principles, derive the $D$-update optimality condition that explicitly involves $\\mathcal{A}^{*}$ and compute the value of the $(4,4)$-entry of $D$ after this first $D$-update, denoted $D^{(1)}_{4,4}$. Express your final numerical answer as a single real number. No rounding is required, and no units are involved. The emphasis should be on how the block-subsampling structure of $\\mathcal{A}$ makes the computation efficient by characterizing $\\mathcal{A}^{*}\\mathcal{A}$.",
            "solution": "The problem setup introduces a splitting variable $D \\in \\mathbb{R}^{4 \\times 4}$ and two constraints:\n1. $D = L + S$, which can be written as $D - L - S = 0$.\n2. $\\mathcal{A}(D) = \\mathcal{A}(M)$, which can be written as $\\mathcal{A}(D) - \\mathcal{A}(M) = 0$.\n\nThe Alternating Direction Method of Multipliers (ADMM) addresses these constraints by forming an augmented Lagrangian. Let $f(L)$ and $g(S)$ be convex penalty functions for the low-rank and sparse components, respectively. The scaled-form augmented Lagrangian $\\mathcal{L}_{\\rho, \\eta}$ is constructed using penalty parameters $\\rho, \\eta > 0$ and scaled dual variables $U_1$ (for $D-L-S=0$) and $U_2$ (for $\\mathcal{A}(D) - \\mathcal{A}(M)=0$):\n$$\n\\mathcal{L}_{\\rho, \\eta}(L, S, D, U_1, U_2) = f(L) + g(S) + \\frac{\\rho}{2}\\lVert D - L - S + U_1 \\rVert_F^2 + \\frac{\\eta}{2}\\lVert\\mathcal{A}(D) - \\mathcal{A}(M) + U_2\\rVert_2^2 - C\n$$\nwhere $C$ contains terms that do not depend on the optimization variables $L, S, D$.\n\nThe ADMM procedure involves iteratively minimizing $\\mathcal{L}$ with respect to each of the primal variables $L, S, D$ in sequence. Given the initial conditions $L^{(0)} = 0$, $S^{(0)} = 0$, $D^{(0)} = 0$, and dual variables $U_1^{(0)} = 0$, $U_2^{(0)} = 0$, the first updates for $L$ and $S$ are:\n$$L^{(1)} = \\arg\\min_L f(L) + \\frac{\\rho}{2}\\lVert -L \\rVert_F^2$$\n$$S^{(1)} = \\arg\\min_S g(S) + \\frac{\\rho}{2}\\lVert -S \\rVert_F^2$$\nFor standard choices of $f(L)$ and $g(S)$ that are norms (e.g., nuclear norm, $\\ell_1$-norm), the solution to these subproblems is the zero matrix, as they involve applying a proximal operator to the zero matrix. Thus, we have $L^{(1)} = 0$ and $S^{(1)} = 0$.\n\nNow, we can derive the $D$-update. The subproblem for $D^{(1)}$ is:\n$$\nD^{(1)} = \\arg\\min_D \\left( \\frac{\\rho}{2}\\lVert D - L^{(1)} - S^{(1)} + U_1^{(0)}\\rVert_F^2 + \\frac{\\eta}{2}\\lVert\\mathcal{A}(D) - \\mathcal{A}(M) + U_2^{(0)}\\rVert_2^2 \\right).\n$$\nSubstituting the initial and updated values ($L^{(1)}=0, S^{(1)}=0, U_1^{(0)}=0, U_2^{(0)}=0$):\n$$\nD^{(1)} = \\arg\\min_D J(D) \\quad \\text{where} \\quad J(D) = \\frac{\\rho}{2}\\lVert D \\rVert_F^2 + \\frac{\\eta}{2}\\lVert\\mathcal{A}(D) - \\mathcal{A}(M)\\rVert_2^2.\n$$\nTo find the minimum, we set the gradient of $J(D)$ with respect to $D$ to zero. The gradient of a squared Frobenius norm $\\lVert X \\rVert_F^2$ is $2X$. The gradient of the second term requires the chain rule and the concept of an adjoint operator $\\mathcal{A}^*$. The gradient of $\\lVert\\mathcal{A}(D) - y\\rVert_2^2$ with respect to $D$ is $2\\mathcal{A}^*(\\mathcal{A}(D) - y)$.\n$$\n\\nabla_D J(D) = \\rho D + \\eta \\mathcal{A}^*(\\mathcal{A}(D) - \\mathcal{A}(M)) = 0.\n$$\nThis is the requested $D$-update optimality condition. We can rearrange it to solve for $D$:\n$$\n(\\rho I + \\eta \\mathcal{A}^*\\mathcal{A}) D = \\eta \\mathcal{A}^*(\\mathcal{A}(M)).\n$$\nThe problem emphasizes understanding the structure of $\\mathcal{A}^*\\mathcal{A}$. The operator $\\mathcal{A}$ extracts the blocks of its matrix argument corresponding to the index sets $\\Omega_1 = \\{1,2\\}\\times\\{1,2\\}$ and $\\Omega_4 = \\{3,4\\}\\times\\{3,4\\}$. The adjoint operator $\\mathcal{A}^*$ takes these vectorized blocks and places them back into a $4 \\times 4$ matrix, with zeros in all other positions.\n\nConsequently, the composite operator $\\mathcal{A}^*\\mathcal{A}$ acts on a matrix $X$ by first extracting its $\\Omega_1$ and $\\Omega_4$ blocks and then placing them back in their original positions, effectively zeroing out all entries outside of these blocks. This means $\\mathcal{A}^*\\mathcal{A}$ is an orthogonal projection operator $P_\\Omega$ onto the subspace of matrices supported on $\\Omega = \\Omega_1 \\cup \\Omega_4$.\n$$\n(P_\\Omega(X))_{ij} = \\begin{cases} X_{ij} & \\text{if } (i,j) \\in \\Omega_1 \\cup \\Omega_4 \\\\ 0 & \\text{otherwise} \\end{cases}.\n$$\nSubstituting $P_\\Omega$ for $\\mathcal{A}^*\\mathcal{A}$ into the optimality condition yields:\n$$\n(\\rho I + \\eta P_\\Omega) D = \\eta P_\\Omega(M).\n$$\nThis structure is key, as the operator $(\\rho I + \\eta P_\\Omega)$ is diagonal in the standard basis of matrix entries, which makes the system easy to solve. We can analyze its action element-wise for $D = D^{(1)}$.\n\nFor an entry $(i, j)$ where $(i, j) \\in \\Omega_1 \\cup \\Omega_4$, the projection $P_\\Omega$ acts as the identity. The element-wise equation is:\n$(\\rho + \\eta) D_{ij} = \\eta M_{ij} \\implies D_{ij} = \\frac{\\eta}{\\rho + \\eta} M_{ij}$.\n\nFor an entry $(i, j)$ where $(i, j) \\notin \\Omega_1 \\cup \\Omega_4$, the projection $P_\\Omega$ acts as zero. The element-wise equation is:\n$\\rho D_{ij} = 0 \\implies D_{ij} = 0$.\n\nWe need to compute the entry $D^{(1)}_{4,4}$. The index $(4,4)$ is in the set $\\{3,4\\} \\times \\{3,4\\}$, which is $\\Omega_4$. Thus, it falls into the first case.\n$$\nD^{(1)}_{4,4} = \\frac{\\eta}{\\rho + \\eta} M_{4,4}.\n$$\nThe problem provides the values $\\rho = 2$ and $\\eta = 3$. The matrix $M$ is\n$$\nM = \\begin{pmatrix}\n4 & -1 & 0 & 0 \\\\\n2 & 3 & 0 & 0 \\\\\n0 & 0 & 1 & -2 \\\\\n0 & 0 & 0 & 7\n\\end{pmatrix},\n$$\nfrom which we identify $M_{4,4} = 7$.\nSubstituting these numerical values into our expression for $D^{(1)}_{4,4}$:\n$$\nD^{(1)}_{4,4} = \\frac{3}{2 + 3} \\times 7 = \\frac{3}{5} \\times 7 = \\frac{21}{5}.\n$$\nThe value of the $(4,4)$-entry of $D$ after the first update is $\\frac{21}{5}$.",
            "answer": "$$\\boxed{\\frac{21}{5}}$$"
        },
        {
            "introduction": "The remarkable success of PCP is not guaranteed; it relies on a crucial, geometric assumption of 'incoherence' between the low-rank and sparse components. This advanced exercise challenges you to probe the theoretical limits of the method by analyzing a scenario where this assumption is violated. By quantifying the resulting failure to separate the components, you will gain a deeper appreciation for the fundamental principles, such as the transversality condition, that guarantee PCP's performance .",
            "id": "3468061",
            "problem": "Consider a data matrix $D \\in \\mathbb{R}^{n \\times n}$ that decomposes as $D = L_0 + S_0$, where $L_0$ is a rank-one matrix and $S_0$ is an entrywise sparse matrix. Let $L_0 = \\sigma\\, u v^{\\top}$ with $\\sigma > 0$ and unit vectors $u, v \\in \\mathbb{R}^{n}$. Suppose $S_0$ contains two components $S_{\\mathrm{sp}}$ and $S_{\\mathrm{lr}}$, where $S_{\\mathrm{sp}}$ is entrywise sparse with operator norm bounded by $\\lVert S_{\\mathrm{sp}}\\rVert_2 \\leq \\varepsilon$, and $S_{\\mathrm{lr}}$ is a low-rank component given by\n$$\nS_{\\mathrm{lr}} \\;=\\; \\alpha \\big( \\cos(\\psi)\\, u x^{\\top} \\;+\\; \\sin(\\psi)\\, u_{\\perp} y^{\\top} \\big),\n$$\nwith $\\alpha > 0$, $\\psi \\in [0,\\pi/2]$, unit vectors $x, y \\in \\mathbb{R}^{n}$, a unit vector $u_{\\perp} \\in \\mathbb{R}^{n}$ satisfying $u_{\\perp} \\perp u$, and $v \\perp x$. Principal Component Pursuit (PCP) is the convex program\n$$\n\\min_{L,S} \\;\\lVert L \\rVert_{*} + \\lambda \\lVert S \\rVert_{1} \\quad \\text{subject to} \\quad D = L + S,\n$$\nwhere $\\lVert\\cdot\\rVert_{*}$ denotes the nuclear norm and $\\lVert\\cdot\\rVert_{1}$ denotes the entrywise $\\ell_1$ norm, and take $\\lambda = n^{-1/2}$.\n\nYou are asked to analyze failure of PCP to separate the low-rank and sparse components when $S_0$ contains a low-rank component $S_{\\mathrm{lr}}$ whose left singular direction partially aligns with $u$. Work from the following fundamental bases:\n\n- Definitions of nuclear norm and $\\ell_1$ norm, and the PCP objective.\n- The tangent space at a rank-one matrix $L_0 = \\sigma u v^{\\top}$, defined as $T = \\{u w^{\\top} + z v^{\\top} : w, z \\in \\mathbb{R}^{n}\\}$.\n- The definition of principal angles between subspaces and the Davis–Kahan–Wedin perturbation bound for singular subspaces: for a rank-$r$ matrix $A$ perturbed by $E$, if $\\sigma_{r}(A) > \\sigma_{r+1}(A)$, then the largest principal angle $\\theta$ between the column space of $A$ and the column space of $A+E$ satisfies $\\sin(\\theta) \\leq \\lVert E \\rVert_{2} / \\sigma_{r}(A)$ under appropriate separation.\n\nTasks:\n\n1. Using the tangent space $T$, explain why the presence of $S_{\\mathrm{lr}}$ with a nonzero aligned component $\\cos(\\psi) u x^{\\top} \\in T$ violates the transversality condition $T \\cap \\Omega = \\{0\\}$ (where $\\Omega$ denotes the subspace of matrices supported on the sparse pattern), thereby demonstrating that PCP can fail to uniquely separate $L_0$ and $S_0$.\n\n2. Quantify the severity of this failure by deriving an explicit upper bound for the largest principal angle $\\theta$ between the true left singular direction $\\mathrm{span}\\{u\\}$ of $L_0$ and the column space of any rank-one low-rank component consistent with $D$ that PCP may recover. Use the perturbation viewpoint in which the aligned part $\\cos(\\psi)\\,u x^{\\top}$ is absorbed into the rank-one baseline, and treat the misaligned part $\\sin(\\psi)\\,u_{\\perp} y^{\\top}$ together with $S_{\\mathrm{sp}}$ as the perturbation. Assume $v \\perp x$ so that cross-terms vanish in computing the singular value of the absorbed baseline.\n\nProvide your final bound as a single closed-form analytic expression for $\\theta$ in terms of $\\sigma$, $\\alpha$, $\\psi$, and $\\varepsilon$. No rounding is required. The final answer must be a single analytic expression.",
            "solution": "### Part 1: Failure of Separation due to Transversality Violation\n\nPrincipal Component Pursuit (PCP) aims to recover a low-rank matrix $L_0$ and a sparse matrix $S_0$ from their sum $D = L_0 + S_0$ by solving the convex optimization problem:\n$$\n\\min_{L,S} \\;\\lVert L \\rVert_{*} + \\lambda \\lVert S \\rVert_{1} \\quad \\text{subject to} \\quad D = L + S\n$$\nThe success of PCP hinges on the assumption that the low-rank and sparse components are sufficiently \"incoherent\" or \"transverse\". Geometrically, this means that the space of matrices that are sparse (on a given support) and the tangent space to the manifold of low-rank matrices at $L_0$ should only intersect at the zero matrix.\n\nThe tangent space $T$ to the manifold of rank-one matrices at $L_0 = \\sigma u v^{\\top}$ is given as:\n$$\nT = \\{u w^{\\top} + z v^{\\top} : w, z \\in \\mathbb{R}^{n}\\}\n$$\nThis is the set of matrices that can be added to $L_0$ without increasing its rank, to first order.\n\nThe error matrix $S_0$ is composed of two parts, $S_0 = S_{\\mathrm{sp}} + S_{\\mathrm{lr}}$. The component $S_{\\mathrm{sp}}$ is entrywise sparse. The component $S_{\\mathrm{lr}}$ contains a term that is structurally similar to $L_0$:\n$$\nS_{\\mathrm{lr}} = \\alpha \\big( \\cos(\\psi)\\, u x^{\\top} \\;+\\; \\sin(\\psi)\\, u_{\\perp} y^{\\top} \\big)\n$$\nLet us denote the component of $S_{\\mathrm{lr}}$ that is aligned with the left singular space of $L_0$ as $\\Delta_a$:\n$$\n\\Delta_a = \\alpha \\cos(\\psi)\\, u x^{\\top}\n$$\nThis matrix $\\Delta_a$ belongs to the tangent space $T$. We can see this by choosing $w = \\alpha \\cos(\\psi) x$ and $z=0$ in the definition of $T$.\n\nThe fundamental problem is that a non-zero part of the \"error\" matrix $S_0$ lies in the tangent space of the \"signal\" matrix $L_0$. If $\\alpha \\cos(\\psi) \\neq 0$, then $\\Delta_a \\neq 0$. This violates the principle of transversality. In the standard PCP theory, where $S_0$ is purely sparse, transversality is expressed as $T \\cap \\Omega = \\{0\\}$, where $\\Omega$ is the space of matrices with the same support pattern as $S_0$. The presence of the low-rank, dense component $\\Delta_a$ in $S_0$ which also lies in $T$ breaks this separation.\n\nThis creates an ambiguity for the PCP optimizer. Consider the alternative decomposition:\n$$\nD = (L_0 + \\Delta_a) + (S_0 - \\Delta_a)\n$$\nLet $L' = L_0 + \\Delta_a$ and $S' = S_0 - \\Delta_a$.\nThe new low-rank component is $L' = \\sigma u v^{\\top} + \\alpha \\cos(\\psi) u x^{\\top} = u (\\sigma v + \\alpha \\cos(\\psi) x)^{\\top}$, which is still a rank-one matrix.\nThe new error component is $S' = S_0 - \\Delta_a = S_{\\mathrm{sp}} + \\alpha \\sin(\\psi) u_{\\perp} y^{\\top}$.\n\nThe matrix $\\Delta_a$ is generally dense, so its entrywise $\\ell_1$-norm, $\\lVert\\Delta_a\\rVert_1$, is large. By moving $\\Delta_a$ from the error matrix $S$ to the low-rank matrix $L$, the optimizer can potentially achieve a large reduction in the $\\lambda \\lVert S \\rVert_1$ term of the objective function. While $\\lVert L' \\rVert_*$ is slightly larger than $\\lVert L_0 \\rVert_*$, the reduction in $\\lambda \\lVert S \\rVert_1$ can easily dominate, causing the PCP algorithm to prefer the decomposition $(L', S')$ over the true one, $(L_0, S_0)$. This demonstrates that PCP can fail to uniquely separate $L_0$ and $S_0$.\n\n### Part 2: Bounding the Principal Angle of Failure\n\nTo quantify the severity of this failure, we adopt the perturbation viewpoint suggested. We consider an \"ideal\" baseline matrix $A$ that has absorbed the aligned part of the error, and treat the remaining parts as a perturbation $E$.\n\nLet the new baseline low-rank matrix be $A$:\n$$\nA = L_0 + \\alpha \\cos(\\psi)\\, u x^{\\top} = \\sigma u v^{\\top} + \\alpha \\cos(\\psi)\\, u x^{\\top} = u (\\sigma v + \\alpha \\cos(\\psi) x)^{\\top}\n$$\nThe matrix $A$ is rank-one. Its column space is $\\mathrm{span}\\{u\\}$, which is identical to the column space of the true low-rank component $L_0$.\n\nThe singular value of $A$, denoted $\\sigma_1(A)$, is given by the norm of its row space generator:\n$$\n\\sigma_1(A) = \\lVert \\sigma v + \\alpha \\cos(\\psi) x \\rVert_2\n$$\nUsing the given condition that $v \\perp x$ (i.e., $v^{\\top}x = 0$), we have by the Pythagorean theorem:\n$$\n\\sigma_1(A) = \\sqrt{\\lVert\\sigma v\\rVert_2^2 + \\lVert\\alpha \\cos(\\psi) x\\rVert_2^2} = \\sqrt{\\sigma^2 \\lVert v\\rVert_2^2 + \\alpha^2 \\cos^2(\\psi) \\lVert x\\rVert_2^2}\n$$\nSince $u, v, x$ are unit vectors, $\\lVert v \\rVert_2 = 1$ and $\\lVert x \\rVert_2=1$. Thus,\n$$\n\\sigma_1(A) = \\sqrt{\\sigma^2 + \\alpha^2 \\cos^2(\\psi)}\n$$\n\nThe total data matrix is $D = A + E$, where $E$ is the perturbation.\n$$\nE = D - A = (L_0 + S_0) - (L_0 + \\alpha \\cos(\\psi)\\, u x^{\\top}) = S_0 - \\alpha \\cos(\\psi)\\, u x^{\\top}\n$$\nSubstituting $S_0 = S_{\\mathrm{sp}} + S_{\\mathrm{lr}}$:\n$$\nE = S_{\\mathrm{sp}} + \\alpha \\big( \\cos(\\psi)\\, u x^{\\top} \\;+\\; \\sin(\\psi)\\, u_{\\perp} y^{\\top} \\big) - \\alpha \\cos(\\psi)\\, u x^{\\top}\n$$\n$$\nE = S_{\\mathrm{sp}} + \\alpha \\sin(\\psi)\\, u_{\\perp} y^{\\top}\n$$\nWe need an upper bound on the operator norm $\\lVert E \\rVert_2$. Using the triangle inequality:\n$$\n\\lVert E \\rVert_2 \\leq \\lVert S_{\\mathrm{sp}}\\rVert_2 + \\lVert\\alpha \\sin(\\psi)\\, u_{\\perp} y^{\\top}\\rVert_2\n$$\nWe are given $\\lVert S_{\\mathrm{sp}}\\rVert_2 \\leq \\varepsilon$. The second term is the norm of a rank-one matrix. Since $u_{\\perp}$ and $y$ are unit vectors, $\\lVert u_{\\perp} y^{\\top} \\rVert_2 = \\lVert u_{\\perp} \\rVert_2 \\lVert y \\rVert_2 = 1$. As $\\alpha > 0$ and $\\psi \\in [0, \\pi/2]$, $\\sin(\\psi) \\geq 0$.\n$$\n\\lVert\\alpha \\sin(\\psi)\\, u_{\\perp} y^{\\top}\\rVert_2 = \\alpha \\sin(\\psi) \\lVert u_{\\perp} y^{\\top} \\rVert_2 = \\alpha \\sin(\\psi)\n$$\nSo, an upper bound on the perturbation norm is:\n$$\n\\lVert E \\rVert_2 \\leq \\varepsilon + \\alpha \\sin(\\psi)\n$$\nWe apply the Davis–Kahan–Wedin perturbation bound, as suggested by the problem, to find the largest principal angle $\\theta$ between the column space of $A$ (which is $\\mathrm{span}\\{u\\}$) and the leading column space of the perturbed matrix $A+E=D$. The bound is given in the form $\\sin(\\theta) \\leq \\lVert E \\rVert_{2} / \\sigma_{1}(A)$.\n$$\n\\sin(\\theta) \\leq \\frac{\\lVert E \\rVert_2}{\\sigma_1(A)} \\leq \\frac{\\varepsilon + \\alpha \\sin(\\psi)}{\\sqrt{\\sigma^2 + \\alpha^2 \\cos^2(\\psi)}}\n$$\nTo obtain an explicit upper bound for the angle $\\theta$ itself, we take the arcsin of the right-hand side. This bound is tightest for a given bound on $\\sin(\\theta)$.\n$$\n\\theta \\leq \\arcsin\\left(\\frac{\\varepsilon + \\alpha \\sin(\\psi)}{\\sqrt{\\sigma^2 + \\alpha^2 \\cos^2(\\psi)}}\\right)\n$$\nThis expression provides the required upper bound for the angle of deviation of the recovered left singular subspace from the true one, quantifying the extent of PCP's failure.",
            "answer": "$$\\boxed{\\arcsin\\left(\\frac{\\varepsilon + \\alpha \\sin(\\psi)}{\\sqrt{\\sigma^{2} + \\alpha^{2} \\cos^{2}(\\psi)}}\\right)}$$"
        }
    ]
}