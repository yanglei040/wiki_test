{
    "hands_on_practices": [
        {
            "introduction": "Proximal operators are a cornerstone of modern convex optimization, forming the building blocks for many first-order algorithms. For problems involving the nuclear norm, the associated proximal operator performs a procedure known as singular value thresholding (SVT). This exercise will guide you through the formal derivation of the SVT operator, revealing the elegant connection between nuclear norm regularization and a simple soft-thresholding operation applied directly to a matrix's singular values .",
            "id": "3458273",
            "problem": "Let $X \\in \\mathbb{R}^{m \\times n}$ be arbitrary and let its singular value decomposition (SVD) be $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{r})$ with $r = \\min\\{m,n\\}$ and $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} \\ge 0$. Consider the proximal operator of the nuclear norm defined for $\\tau  0$ by\n$$\n\\mathrm{prox}_{\\tau \\|\\cdot\\|_{*}}(X) \\equiv \\arg\\min_{M \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\tau \\|M\\|_{*} + \\frac{1}{2} \\|M - X\\|_{F}^{2} \\right\\},\n$$\nwhere $\\|\\cdot\\|_{*}$ is the nuclear norm and $\\|\\cdot\\|_{F}$ is the Frobenius norm.\n\nStarting from the fundamental definitions of the nuclear norm, the Frobenius norm, and the proximal operator, and using only well-tested facts about orthogonal invariance of these norms and basic spectral inequalities, derive a closed-form expression for $\\mathrm{prox}_{\\tau \\|\\cdot\\|_{*}}(X)$ in terms of $U$, $V$, and the singular values $\\{\\sigma_{i}\\}_{i=1}^{r}$. Express your final result using the soft-thresholding operator $(a)_{+} \\equiv \\max\\{a,0\\}$ applied to the singular values.\n\nProvide your final answer as a single closed-form analytic expression in terms of $U$, $V$, $\\{\\sigma_{i}\\}$, and $\\tau$. Do not provide intermediate steps in the final answer.",
            "solution": "The problem as stated is valid. It is a well-posed problem in convex optimization, specifically concerning the proximal operator of the nuclear norm, a fundamental concept in low-rank matrix recovery. All terms are mathematically well-defined, and the problem is self-contained and consistent.\n\nWe are asked to find the solution to the following optimization problem, which defines the proximal operator of the nuclear norm $\\|\\cdot\\|_{*}$:\n$$\nM^* = \\mathrm{prox}_{\\tau \\|\\cdot\\|_{*}}(X) = \\arg\\min_{M \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\tau \\|M\\|_{*} + \\frac{1}{2} \\|M - X\\|_{F}^{2} \\right\\}\n$$\nLet the objective function be $F(M) = \\tau \\|M\\|_{*} + \\frac{1}{2} \\|M - X\\|_{F}^{2}$. We are given the singular value decomposition (SVD) of $X$ as $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with the singular values $\\sigma_1 \\ge \\dots \\ge \\sigma_r \\ge 0$ on its main diagonal, where $r = \\min\\{m, n\\}$.\n\nA key property of both the nuclear norm and the Frobenius norm is their invariance under orthogonal transformations. For any matrix $A \\in \\mathbb{R}^{m \\times n}$ and any orthogonal matrices $Q_1 \\in \\mathbb{R}^{m \\times m}$ and $Q_2 \\in \\mathbb{R}^{n \\times n}$, we have:\n$$\n\\|Q_1 A Q_2^{\\top}\\|_{*} = \\|A\\|_{*} \\quad \\text{and} \\quad \\|Q_1 A Q_2^{\\top}\\|_{F} = \\|A\\|_{F}\n$$\nWe can use this property to simplify the objective function. Let's analyze the Frobenius norm term first:\n$$\n\\|M - X\\|_{F}^{2} = \\|M - U \\Sigma V^{\\top}\\|_{F}^{2}\n$$\nSince $U$ and $V$ are orthogonal, $U^\\top U = I_m$ and $V^\\top V = I_n$. We can multiply the argument of the norm by $U^{\\top}$ on the left and $V$ on the right without changing the value of the norm:\n$$\n\\|M - X\\|_{F}^{2} = \\|U^{\\top} (M - U \\Sigma V^{\\top}) V\\|_{F}^{2} = \\|U^{\\top} M V - U^{\\top} U \\Sigma V^{\\top} V\\|_{F}^{2} = \\|U^{\\top} M V - \\Sigma\\|_{F}^{2}\n$$\nNow, let's consider the nuclear norm term. We can apply the same transformation:\n$$\n\\|M\\|_{*} = \\|U (U^{\\top} M V) V^{\\top}\\|_{*} = \\|U^{\\top} M V\\|_{*}\n$$\nLet's define a new matrix variable $\\tilde{M} = U^{\\top} M V$. Since $U$ and $V$ are invertible, the mapping $M \\mapsto \\tilde{M}$ is a bijection on $\\mathbb{R}^{m \\times n}$. Therefore, minimizing the objective function with respect to $M$ is equivalent to minimizing its transformed version with respect to $\\tilde{M}$. The optimization problem becomes:\n$$\n\\min_{\\tilde{M} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\tau \\|\\tilde{M}\\|_{*} + \\frac{1}{2} \\|\\tilde{M} - \\Sigma\\|_{F}^{2} \\right\\}\n$$\nLet the SVD of $\\tilde{M}$ be $\\tilde{M} = P \\tilde{\\Sigma} Q^{\\top}$, where $\\tilde{\\Sigma}$ is a diagonal matrix with singular values $\\tilde{\\sigma}_i$. The nuclear norm is $\\|\\tilde{M}\\|_{*} = \\sum_{i=1}^r \\tilde{\\sigma}_i$. The Frobenius norm term can be expanded:\n$$\n\\|\\tilde{M} - \\Sigma\\|_{F}^{2} = \\langle \\tilde{M} - \\Sigma, \\tilde{M} - \\Sigma \\rangle = \\|\\tilde{M}\\|_{F}^{2} - 2\\langle \\tilde{M}, \\Sigma \\rangle + \\|\\Sigma\\|_{F}^{2}\n$$\nwhere $\\langle A, B \\rangle = \\mathrm{Tr}(A^{\\top} B)$ is the Frobenius inner product. We have $\\|\\tilde{M}\\|_{F}^{2} = \\sum_{i=1}^r \\tilde{\\sigma}_i^2$ and $\\|\\Sigma\\|_{F}^{2} = \\sum_{i=1}^r \\sigma_i^2$. The inner product term is $\\langle \\tilde{M}, \\Sigma \\rangle = \\mathrm{Tr}(\\tilde{M}^{\\top} \\Sigma)$.\nBy the von Neumann trace inequality, $\\mathrm{Tr}(A^{\\top} B) \\le \\sum_i \\sigma_i(A) \\sigma_i(B)$. Equality holds if and only if $A$ and $B$ can be simultaneously diagonalized by the same pair of orthogonal matrices.\n$$\n\\langle \\tilde{M}, \\Sigma \\rangle = \\mathrm{Tr}(\\tilde{M}^{\\top} \\Sigma) \\le \\sum_{i=1}^r \\tilde{\\sigma}_i \\sigma_i\n$$\nTo minimize the objective function, we must maximize the term $\\langle \\tilde{M}, \\Sigma \\rangle$. This maximum is achieved when $\\tilde{M}$ and $\\Sigma$ share the same singular vectors. Since $\\Sigma$ is diagonal, its singular vectors are the standard basis vectors. This implies that the optimal $\\tilde{M}$ must also be a diagonal matrix. Let us denote this optimal diagonal matrix as $\\tilde{\\Sigma}_{\\mathrm{opt}} = \\mathrm{diag}(\\tilde{\\sigma}_1, \\dots, \\tilde{\\sigma}_r)$, where $\\tilde{\\sigma}_i \\ge 0$ are the singular values to be determined.\n\nWith $\\tilde{M}$ being a diagonal matrix $\\mathrm{diag}(\\tilde{\\sigma}_1, \\dots, \\tilde{\\sigma}_r)$, the problem decouples into $r$ independent scalar optimization problems, one for each singular value:\n$$\n\\min_{\\tilde{\\sigma}_1, \\dots, \\tilde{\\sigma}_r \\ge 0} \\left\\{ \\tau \\sum_{i=1}^r \\tilde{\\sigma}_i + \\frac{1}{2} \\sum_{i=1}^r (\\tilde{\\sigma}_i - \\sigma_i)^2 \\right\\} = \\sum_{i=1}^r \\min_{\\tilde{\\sigma}_i \\ge 0} \\left\\{ \\tau \\tilde{\\sigma}_i + \\frac{1}{2} (\\tilde{\\sigma}_i - \\sigma_i)^2 \\right\\}\n$$\nFor each $i \\in \\{1, \\dots, r\\}$, we must solve:\n$$\n\\min_{\\tilde{\\sigma}_i \\ge 0} g(\\tilde{\\sigma}_i) \\quad \\text{where} \\quad g(\\tilde{\\sigma}_i) = \\tau \\tilde{\\sigma}_i + \\frac{1}{2} (\\tilde{\\sigma}_i - \\sigma_i)^2\n$$\nThis is a simple quadratic optimization problem with a non-negativity constraint. First, we find the unconstrained minimum by setting the derivative to zero:\n$$\n\\frac{dg}{d\\tilde{\\sigma}_i} = \\tau + (\\tilde{\\sigma}_i - \\sigma_i) = 0 \\implies \\tilde{\\sigma}_i = \\sigma_i - \\tau\n$$\nSince singular values must be non-negative, we must consider the constraint $\\tilde{\\sigma}_i \\ge 0$.\nCase 1: If $\\sigma_i - \\tau \\ge 0$, the unconstrained minimizer is non-negative and thus is the solution to the constrained problem. So, $\\tilde{\\sigma}_i^* = \\sigma_i - \\tau$.\nCase 2: If $\\sigma_i - \\tau  0$, the unconstrained minimizer is negative. The function $g(\\tilde{\\sigma}_i)$ is a parabola opening upwards, so on the domain $[0, \\infty)$, the minimum must occur at the boundary point $\\tilde{\\sigma}_i = 0$. So, $\\tilde{\\sigma}_i^* = 0$.\n\nCombining these two cases gives the solution for each singular value:\n$$\n\\tilde{\\sigma}_i^* = \\max\\{0, \\sigma_i - \\tau\\}\n$$\nThis operation is known as soft-thresholding and is denoted by $(\\sigma_i - \\tau)_+$.\nThe optimal matrix $\\tilde{M}^*$ is therefore the diagonal matrix whose diagonal entries are the soft-thresholded singular values of $X$:\n$$\n\\tilde{M}^* = \\mathrm{diag}\\left((\\sigma_1 - \\tau)_+, (\\sigma_2 - \\tau)_+, \\dots, (\\sigma_r - \\tau)_+\\right)\n$$\nTo obtain the final solution $M^*$, we must transform back from the $\\tilde{M}$ coordinates to the original $M$ coordinates using the relation $M = U \\tilde{M} V^{\\top}$:\n$$\nM^* = U \\tilde{M}^* V^{\\top} = U \\ \\mathrm{diag}\\left((\\sigma_1 - \\tau)_+, (\\sigma_2 - \\tau)_+, \\dots, (\\sigma_r - \\tau)_+\\right) \\ V^{\\top}\n$$\nThis expression provides the closed-form solution for the proximal operator of the nuclear norm. It operates by computing the SVD of the input matrix $X$, applying the soft-thresholding operator to its singular values, and then reconstructing the matrix. This process is also known as singular value thresholding (SVT).",
            "answer": "$$\n\\boxed{U \\ \\mathrm{diag}\\left((\\sigma_1 - \\tau)_+, (\\sigma_2 - \\tau)_+, \\dots, (\\sigma_r - \\tau)_+\\right) \\ V^{\\top}}\n$$"
        },
        {
            "introduction": "While the SVT operator provides the core computational step, a complete algorithm is needed to handle the constraints of a matrix recovery problem. The Alternating Direction Method of Multipliers (ADMM) is a powerful and versatile framework for this purpose, breaking a complex problem into a sequence of simpler subproblems. This practice demonstrates how to formulate the low-rank matrix recovery problem for ADMM and reveals how the SVT operator from the previous exercise naturally appears as one of the key iterative updates .",
            "id": "3458294",
            "problem": "Consider the convex optimization problem of recovering a low-rank matrix by nuclear norm minimization with an explicit consensus copy:\nminimize $\\|Z\\|_{*}$ subject to $\\mathcal{A}(X)=b$ and $X=Z$,\nwhere $\\|\\cdot\\|_{*}$ denotes the nuclear norm, $\\mathcal{A}:\\mathbb{R}^{n\\times n}\\to\\mathbb{R}^{m}$ is a linear sensing operator, and $b\\in\\mathbb{R}^{m}$. Start from the core definitions of the Lagrangian and augmented Lagrangian for equality-constrained convex problems, and the definition of a proximal operator. Using these, perform the following tasks:\n\n1. Formulate the augmented Lagrangian for the problem by introducing a matrix-valued Lagrange multiplier for the consensus constraint $X=Z$ and a quadratic penalty with parameter $\\rho0$, while representing the measurement constraint $\\mathcal{A}(X)=b$ as the indicator of an affine set.\n\n2. Derive the Alternating Direction Method of Multipliers (ADMM) updates for $X$, $Z$, and the dual variable associated with the constraint $X=Z$, in their unscaled form. Your derivation must start from the augmented Lagrangian in item $1$ and use only fundamental properties of projections in Euclidean spaces and the proximal operator of a convex function.\n\n3. Specialize your updates to the following concrete instance:\n- Dimension $n=2$.\n- Sensing operator $\\mathcal{A}:\\mathbb{R}^{2\\times 2}\\to\\mathbb{R}^{2}$ defined by $\\mathcal{A}(X)=\\begin{bmatrix}X_{11}+X_{22}\\\\ X_{12}\\end{bmatrix}$.\n- Measurements $b=\\begin{bmatrix}2\\\\ 1\\end{bmatrix}$.\n- Penalty parameter $\\rho=1$.\n- Initialization $Z^{0}=0_{2\\times 2}$ and $Y^{0}=0_{2\\times 2}$ for the dual variable.\n\nCarry out one full ADMM iteration to compute $X^{1}$ and $Z^{1}$ explicitly. Then, using only this first iteration, compute the nuclear norm $\\|Z^{1}\\|_{*}$ in exact, closed form.\n\nYour final answer must be the single exact expression for $\\|Z^{1}\\|_{*}$ (no numerical rounding). No units are required.",
            "solution": "The problem is valid as it is scientifically grounded in the field of convex optimization, specifically low-rank matrix recovery. It is well-posed, objective, and contains all necessary information to proceed with a unique solution.\n\nThe optimization problem can be written in a consensus form:\n$$\n\\text{minimize} \\quad f(X) + g(Z) \\quad \\text{subject to} \\quad X - Z = 0\n$$\nwhere $f(X) = \\mathcal{I}_{\\mathcal{C}}(X)$ is the indicator function for the affine set $\\mathcal{C} = \\{X \\in \\mathbb{R}^{n \\times n} \\mid \\mathcal{A}(X) = b\\}$, and $g(Z) = \\|Z\\|_{*}$ is the nuclear norm of $Z$.\n\n**1. Augmented Lagrangian Formulation**\n\nThe standard Lagrangian for the equality-constrained problem is:\n$$\nL(X, Z, Y) = f(X) + g(Z) + \\langle Y, X - Z \\rangle\n$$\nwhere $Y$ is the matrix of Lagrange multipliers (the dual variable), and $\\langle A, B \\rangle = \\text{tr}(A^T B)$ denotes the Frobenius inner product.\n\nThe augmented Lagrangian is formed by adding a quadratic penalty term for the constraint violation:\n$$\nL_{\\rho}(X, Z, Y) = f(X) + g(Z) + \\langle Y, X - Z \\rangle + \\frac{\\rho}{2}\\|X - Z\\|_F^2\n$$\nwhere $\\rho  0$ is the penalty parameter and $\\|\\cdot\\|_F$ is the Frobenius norm. Substituting the definitions of $f(X)$ and $g(Z)$, we get:\n$$\nL_{\\rho}(X, Z, Y) = \\mathcal{I}_{\\mathcal{C}}(X) + \\|Z\\|_{*} + \\langle Y, X - Z \\rangle + \\frac{\\rho}{2}\\|X - Z\\|_F^2\n$$\n\n**2. Derivation of ADMM Updates**\n\nThe Alternating Direction Method of Multipliers (ADMM) is an iterative algorithm that sequentially minimizes the augmented Lagrangian with respect to each primal variable, followed by an update of the dual variable.\n\n**$X$-update:** At iteration $k+1$, we update $X$ by minimizing $L_{\\rho}(X, Z^k, Y^k)$:\n$$\nX^{k+1} = \\arg\\min_{X} L_{\\rho}(X, Z^k, Y^k) = \\arg\\min_{X} \\left( \\mathcal{I}_{\\mathcal{C}}(X) + \\langle Y^k, X - Z^k \\rangle + \\frac{\\rho}{2}\\|X - Z^k\\|_F^2 \\right)\n$$\nWe ignore terms not depending on $X$. The minimization is over $X$ such that $\\mathcal{A}(X)=b$. The expression to minimize becomes:\n$$\n\\langle Y^k, X \\rangle + \\frac{\\rho}{2}\\|X - Z^k\\|_F^2 = \\frac{\\rho}{2} \\left( \\frac{2}{\\rho}\\langle Y^k, X \\rangle + \\|X\\|_F^2 - 2\\langle X, Z^k \\rangle + \\|Z^k\\|_F^2 \\right)\n$$\nCompleting the square for the terms involving $X$:\n$$\n\\frac{\\rho}{2} \\left( \\|X\\|_F^2 - 2\\langle X, Z^k - \\frac{1}{\\rho}Y^k \\rangle + \\text{const} \\right) = \\frac{\\rho}{2} \\|X - (Z^k - \\frac{1}{\\rho}Y^k)\\|_F^2 + \\text{const}\n$$\nThe minimization problem for $X$ is therefore equivalent to finding the Euclidean projection of the matrix $Z^k - \\frac{1}{\\rho}Y^k$ onto the affine set $\\mathcal{C}$:\n$$\nX^{k+1} = \\arg\\min_{X} \\left( \\mathcal{I}_{\\mathcal{C}}(X) + \\frac{\\rho}{2} \\|X - (Z^k - \\frac{1}{\\rho}Y^k)\\|_F^2 \\right) = \\mathcal{P}_{\\mathcal{C}}\\left(Z^k - \\frac{1}{\\rho}Y^k\\right)\n$$\n\n**$Z$-update:** Next, we update $Z$ by minimizing $L_{\\rho}(X^{k+1}, Z, Y^k)$:\n$$\nZ^{k+1} = \\arg\\min_{Z} L_{\\rho}(X^{k+1}, Z, Y^k) = \\arg\\min_{Z} \\left( \\|Z\\|_{*} + \\langle Y^k, X^{k+1} - Z \\rangle + \\frac{\\rho}{2}\\|X^{k+1} - Z\\|_F^2 \\right)\n$$\nSimilarly, we complete the square for terms involving $Z$:\n$$\n\\|Z\\|_{*} - \\langle Y^k, Z \\rangle + \\frac{\\rho}{2}\\|X^{k+1} - Z\\|_F^2 = \\|Z\\|_{*} + \\frac{\\rho}{2} \\left( \\|Z\\|_F^2 - 2\\langle Z, X^{k+1} + \\frac{1}{\\rho}Y^k \\rangle + \\text{const} \\right)\n$$\n$$\n= \\|Z\\|_{*} + \\frac{\\rho}{2} \\|Z - (X^{k+1} + \\frac{1}{\\rho}Y^k)\\|_F^2 + \\text{const}\n$$\nThe minimization problem for $Z$ is:\n$$\nZ^{k+1} = \\arg\\min_{Z} \\left( \\|Z\\|_{*} + \\frac{\\rho}{2} \\|Z - (X^{k+1} + \\frac{1}{\\rho}Y^k)\\|_F^2 \\right)\n$$\nThis corresponds to the definition of the proximal operator of the function $\\frac{1}{\\rho}\\|\\cdot\\|_{*}$. Specifically, it is the singular value thresholding (SVT) operator with threshold $\\lambda = 1/\\rho$:\n$$\nZ^{k+1} = \\text{prox}_{\\frac{1}{\\rho}\\|\\cdot\\|_{*}}\\left(X^{k+1} + \\frac{1}{\\rho}Y^k\\right) = \\text{SVT}_{1/\\rho}\\left(X^{k+1} + \\frac{1}{\\rho}Y^k\\right)\n$$\nThe SVT operator acts on a matrix $M$ by computing its singular value decomposition (SVD), $M = U\\Sigma V^T$, soft-thresholding the singular values, $\\hat{\\Sigma}_{ii} = \\max(0, \\Sigma_{ii} - \\lambda)$, and reconstructing the matrix, $\\text{SVT}_{\\lambda}(M) = U\\hat{\\Sigma}V^T$.\n\n**$Y$-update:** Finally, the dual variable is updated using a gradient ascent step on the dual problem:\n$$\nY^{k+1} = Y^k + \\rho(X^{k+1} - Z^{k+1})\n$$\n\n**3. Specific Instance and First Iteration**\n\nWe are given:\n- $n=2$\n- $\\mathcal{A}(X) = \\begin{bmatrix} X_{11}+X_{22} \\\\ X_{12} \\end{bmatrix}$ (i.e., trace and the $(1,2)$ element)\n- $b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$\n- $\\rho = 1$\n- $Z^0 = 0_{2\\times 2}$ (zero matrix)\n- $Y^0 = 0_{2\\times 2}$\n\nWe perform the first iteration ($k=0$):\n\n**Compute $X^1$**:\n$$\nX^1 = \\mathcal{P}_{\\mathcal{C}}\\left(Z^0 - \\frac{1}{\\rho}Y^0\\right) = \\mathcal{P}_{\\mathcal{C}}\\left(0_{2\\times 2} - \\frac{1}{1}0_{2\\times 2}\\right) = \\mathcal{P}_{\\mathcal{C}}(0_{2\\times 2})\n$$\nWe need to find the matrix $X \\in \\mathbb{R}^{2\\times 2}$ that minimizes $\\|X-0_{2\\times 2}\\|_F^2 = \\|X\\|_F^2$ subject to the constraints $\\text{tr}(X) = X_{11}+X_{22}=2$ and $X_{12}=1$.\n$$\n\\|X\\|_F^2 = X_{11}^2 + X_{12}^2 + X_{21}^2 + X_{22}^2\n$$\nSubstituting $X_{12}=1$, we minimize $X_{11}^2 + 1^2 + X_{21}^2 + X_{22}^2$ subject to $X_{11}+X_{22}=2$.\nTo minimize the expression, we must have $X_{21}=0$. The problem reduces to minimizing $X_{11}^2 + X_{22}^2$ subject to $X_{11}+X_{22}=2$. Using substitution $X_{22}=2-X_{11}$, we minimize $h(X_{11}) = X_{11}^2 + (2-X_{11})^2$. Differentiating with respect to $X_{11}$ and setting to zero gives $2X_{11} - 2(2-X_{11}) = 4X_{11}-4 = 0$, so $X_{11}=1$. This implies $X_{22}=2-1=1$.\nThus, the resulting matrix is:\n$$\nX^1 = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}\n$$\n\n**Compute $Z^1$**:\n$$\nZ^1 = \\text{SVT}_{1/\\rho}\\left(X^1 + \\frac{1}{\\rho}Y^0\\right) = \\text{SVT}_{1}\\left(X^1 + \\frac{1}{1}0_{2\\times 2}\\right) = \\text{SVT}_{1}(X^1)\n$$\nWe need the SVD of $X^1 = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$. The singular values $\\sigma_i$ are the square roots of the eigenvalues of $X^{1T}X^1$.\n$$\nX^{1T}X^1 = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}\n$$\nThe characteristic equation is $\\det(X^{1T}X^1 - \\lambda I) = (1-\\lambda)(2-\\lambda) - 1 = \\lambda^2 - 3\\lambda + 1 = 0$.\nThe eigenvalues are $\\lambda = \\frac{3 \\pm \\sqrt{9-4}}{2} = \\frac{3 \\pm \\sqrt{5}}{2}$.\nThe singular values of $X^1$ are:\n$$\n\\sigma_1 = \\sqrt{\\frac{3+\\sqrt{5}}{2}} = \\frac{1+\\sqrt{5}}{2} \\quad (\\approx 1.618)\n$$\n$$\n\\sigma_2 = \\sqrt{\\frac{3-\\sqrt{5}}{2}} = \\frac{\\sqrt{5}-1}{2} \\quad (\\approx 0.618)\n$$\nWe apply soft-thresholding with threshold $\\lambda=1$:\n$$\n\\hat{\\sigma}_1 = \\max(0, \\sigma_1 - 1) = \\frac{1+\\sqrt{5}}{2} - 1 = \\frac{\\sqrt{5}-1}{2}\n$$\n$$\n\\hat{\\sigma}_2 = \\max(0, \\sigma_2 - 1) = \\max\\left(0, \\frac{\\sqrt{5}-1}{2} - 1\\right) = \\max\\left(0, \\frac{\\sqrt{5}-3}{2}\\right) = 0\n$$\nThe singular values of $Z^1$ are $\\hat{\\sigma}_1 = \\frac{\\sqrt{5}-1}{2}$ and $\\hat{\\sigma}_2=0$.\nTo compute $Z^1$ explicitly, we would reconstruct the matrix from its SVD with the new singular values. However, to find the nuclear norm $\\|Z^1\\|_{*}$, we only need to sum its singular values.\n\n**Compute $\\|Z^1\\|_{*}$**:\nThe nuclear norm is the sum of the singular values.\n$$\n\\|Z^1\\|_{*} = \\hat{\\sigma}_1 + \\hat{\\sigma}_2 = \\frac{\\sqrt{5}-1}{2} + 0 = \\frac{\\sqrt{5}-1}{2}\n$$\nFor completeness, the explicit form of $Z^1$ is $Z^1 = \\frac{5-\\sqrt{5}}{10} \\begin{pmatrix} 1  \\frac{1+\\sqrt{5}}{2} \\\\ \\frac{\\sqrt{5}-1}{2}  1 \\end{pmatrix}$. Its rank is $1$, and its single non-zero singular value is indeed $\\frac{\\sqrt{5}-1}{2}$.",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{5}-1}{2}}\n$$"
        },
        {
            "introduction": "The success of nuclear norm minimization depends on more than just the matrix's rank; the structure of its singular vectors is also critical. This is formalized by the concept of \"incoherence,\" which measures how \"spread out\" the singular vectors are. This exercise provides a striking quantitative demonstration of this principle by comparing the sample complexity required for recovering two matrices that share the same rank and singular values but possess vastly different coherence properties .",
            "id": "3458286",
            "problem": "Consider the standard matrix completion setup in low-rank matrix recovery via nuclear norm minimization. Let $X^{\\star} \\in \\mathbb{R}^{n \\times n}$ be rank $r$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r}  0$ and $\\sigma_{j} = 0$ for $j  r$. Observations consist of a subset of entries sampled uniformly at random without replacement, and the estimator is the nuclear norm minimizer subject to equality constraints on the observed entries. The sampling model is specified by a set $\\Omega \\subseteq \\{1,\\dots,n\\} \\times \\{1,\\dots,n\\}$ with $|\\Omega| = m$, drawn uniformly at random among all subsets of size $m$.\n\nDefine the leverage scores for an orthonormal matrix $U \\in \\mathbb{R}^{n \\times r}$ by $\\ell_{i}(U) = \\|U_{i,:}\\|_{2}^{2}$ for $i \\in \\{1,\\dots,n\\}$, and the coherence by $\\mu_{U} = \\frac{n}{r} \\max_{i} \\ell_{i}(U)$; similarly define $\\mu_{V}$ for $V \\in \\mathbb{R}^{n \\times r}$. The overall incoherence parameter is $\\mu = \\max\\{\\mu_{U}, \\mu_{V}\\}$. Also define the spikiness via the entrywise infinity norm $\\|X^{\\star}\\|_{\\infty} = \\max_{i,j} |X^{\\star}_{ij}|$. Assume $n$ is an even integer and that a normalized Hadamard matrix $H \\in \\mathbb{R}^{n \\times n}$ with entries in $\\{\\pm \\frac{1}{\\sqrt{n}}\\}$ exists.\n\nConstruct two rank-$r$ matrices with identical singular spectrum:\n1. A uniformly incoherent matrix $X_{\\mathrm{coh}}^{\\star} = U_{H} \\Sigma V_{H}^{\\top}$ where $U_{H}, V_{H} \\in \\mathbb{R}^{n \\times r}$ are the first $r$ columns of $H$, and $\\Sigma = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{r})$.\n2. A spiky, highly coherent matrix $X_{\\mathrm{spk}}^{\\star} = U_{E} \\Sigma V_{E}^{\\top}$ where $U_{E}, V_{E} \\in \\mathbb{R}^{n \\times r}$ are the first $r$ columns of the canonical basis, i.e., $(U_{E})_{ij} = \\delta_{ij}$ for $1 \\leq i \\leq n$, $1 \\leq j \\leq r$, and likewise for $V_{E}$.\n\nUsing only fundamental definitions and well-tested facts about leverage scores, incoherence, and the recovery behavior of nuclear norm minimization under uniform entry sampling, determine the ratio\n$$\n\\rho \\;=\\; \\frac{m_{\\mathrm{spk}}}{m_{\\mathrm{coh}}},\n$$\nwhere $m_{\\mathrm{coh}}$ and $m_{\\mathrm{spk}}$ denote the minimal sample sizes required to achieve exact recovery with high probability for $X_{\\mathrm{coh}}^{\\star}$ and $X_{\\mathrm{spk}}^{\\star}$, respectively, up to universal constants independent of $n$, $r$, and $(\\sigma_{1},\\dots,\\sigma_{r})$. Express $\\rho$ in closed form as a function of $n$ and $r$. No rounding is required. Your final answer must be a single expression.",
            "solution": "The problem requires us to determine the ratio of the minimum sample sizes, $\\rho = \\frac{m_{\\mathrm{spk}}}{m_{\\mathrm{coh}}}$, needed for the exact recovery of two different rank-$r$ matrices, $X_{\\mathrm{coh}}^{\\star}$ and $X_{\\mathrm{spk}}^{\\star}$, via nuclear norm minimization. The recovery is from $m$ entries sampled uniformly at random.\n\nThe theoretical results for low-rank matrix completion establish a sufficient number of samples for exact recovery with high probability. A standard result states that for a rank-$r$ matrix $X^{\\star} \\in \\mathbb{R}^{n \\times n}$ with incoherence parameter $\\mu$, the number of samples $m$ should satisfy\n$$\nm \\ge C \\mu n r \\cdot \\mathrm{polylog}(n)\n$$\nfor some universal constant $C$. The problem asks for the ratio up to universal constants, which implies we can neglect the constant $C$ and any polylogarithmic factors in $n$. Thus, we can establish the scaling of the minimal sample size as\n$$\nm \\propto \\mu n r\n$$\n\nOur task reduces to calculating the incoherence parameter $\\mu$ for each of the two constructed matrices, $X_{\\mathrm{coh}}^{\\star}$ and $X_{\\mathrm{spk}}^{\\star}$. The incoherence parameter $\\mu$ is defined as $\\mu = \\max\\{\\mu_{U}, \\mu_{V}\\}$, where $\\mu_U = \\frac{n}{r} \\max_{i} \\ell_{i}(U)$ and $\\ell_{i}(U) = \\|U_{i,:}\\|_{2}^{2}$ is the leverage score of the $i$-th row of the matrix $U$. The matrices $U, V \\in \\mathbb{R}^{n \\times r}$ contain the left and right singular vectors of $X^\\star$, respectively, corresponding to its non-zero singular values.\n\n**1. Analysis of the Incoherent Matrix $X_{\\mathrm{coh}}^{\\star}$**\n\nThe first matrix is given by $X_{\\mathrm{coh}}^{\\star} = U_{H} \\Sigma V_{H}^{\\top}$, where $U_{H}$ and $V_{H}$ are the first $r$ columns of a normalized Hadamard matrix $H \\in \\mathbb{R}^{n \\times n}$. By definition, all entries of $H$ have magnitude $|H_{ij}| = \\frac{1}{\\sqrt{n}}$. The matrix $U_H$ is an $n \\times r$ matrix whose entries are $(U_H)_{ij} = H_{ij}$ for $i \\in \\{1,\\dots,n\\}$ and $j \\in \\{1,\\dots,r\\}$.\n\nWe calculate the leverage scores for $U_H$. The $i$-th leverage score is the squared Euclidean norm of the $i$-th row of $U_H$:\n$$\n\\ell_{i}(U_{H}) = \\|(U_{H})_{i,:}\\|_{2}^{2} = \\sum_{j=1}^{r} |(U_{H})_{ij}|^2\n$$\nSince $(U_H)_{ij} = H_{ij}$ and $|H_{ij}| = \\frac{1}{\\sqrt{n}}$, we have:\n$$\n\\ell_{i}(U_{H}) = \\sum_{j=1}^{r} \\left(\\frac{1}{\\sqrt{n}}\\right)^2 = \\sum_{j=1}^{r} \\frac{1}{n} = \\frac{r}{n}\n$$\nThis leverage score is the same for all rows $i \\in \\{1,\\dots,n\\}$. Therefore, the maximum leverage score is $\\max_{i} \\ell_{i}(U_{H}) = \\frac{r}{n}$.\n\nNow, we compute the coherence parameter $\\mu_{U_H}$:\n$$\n\\mu_{U_H} = \\frac{n}{r} \\max_{i} \\ell_{i}(U_{H}) = \\frac{n}{r} \\left(\\frac{r}{n}\\right) = 1\n$$\nBy an identical argument for $V_H$, we find $\\mu_{V_H} = 1$. The overall incoherence for $X_{\\mathrm{coh}}^{\\star}$ is:\n$$\n\\mu_{\\mathrm{coh}} = \\max\\{\\mu_{U_H}, \\mu_{V_H}\\} = 1\n$$\nThis represents the lowest possible coherence. Using the sample complexity scaling, we have:\n$$\nm_{\\mathrm{coh}} \\propto \\mu_{\\mathrm{coh}} n r = 1 \\cdot n r = nr\n$$\n\n**2. Analysis of the Spiky Matrix $X_{\\mathrm{spk}}^{\\star}$**\n\nThe second matrix is $X_{\\mathrm{spk}}^{\\star} = U_{E} \\Sigma V_{E}^{\\top}$, where $U_E$ and $V_E$ are the first $r$ columns of the $n \\times n$ identity matrix $I_n$. The matrix $U_E \\in \\mathbb{R}^{n \\times r}$ has entries $(U_E)_{ij} = \\delta_{ij}$ for $1 \\le i \\le n, 1 \\le j \\le r$. This means the first $r$ rows of $U_E$ are the canonical basis vectors $e_1^{\\top}, \\dots, e_r^{\\top}$ of $\\mathbb{R}^r$, and the remaining $n-r$ rows are zero vectors.\n\nWe calculate the leverage scores for $U_E$. For $i \\in \\{1, \\dots, r\\}$, the $i$-th row of $U_E$ is the vector $e_i^{\\top} \\in \\mathbb{R}^r$. Its squared norm is:\n$$\n\\ell_{i}(U_{E}) = \\|e_i^{\\top}\\|_{2}^{2} = 1, \\quad \\text{for } 1 \\le i \\le r\n$$\nFor $i \\in \\{r+1, \\dots, n\\}$, the $i$-th row of $U_E$ is the zero vector. Its squared norm is:\n$$\n\\ell_{i}(U_{E}) = \\|0\\|_{2}^{2} = 0, \\quad \\text{for } r+1 \\le i \\le n\n$$\nThe maximum leverage score is the maximum over all rows:\n$$\n\\max_{i} \\ell_{i}(U_{E}) = \\max\\{1, \\dots, 1, 0, \\dots, 0\\} = 1\n$$\nNow, we compute the coherence parameter $\\mu_{U_E}$:\n$$\n\\mu_{U_E} = \\frac{n}{r} \\max_{i} \\ell_{i}(U_{E}) = \\frac{n}{r} \\cdot 1 = \\frac{n}{r}\n$$\nBy an identical argument for $V_E$, we find $\\mu_{V_E} = \\frac{n}{r}$. The overall incoherence for $X_{\\mathrm{spk}}^{\\star}$ is:\n$$\n\\mu_{\\mathrm{spk}} = \\max\\{\\mu_{U_E}, \\mu_{V_E}\\} = \\frac{n}{r}\n$$\nUsing the sample complexity scaling, we have:\n$$\nm_{\\mathrm{spk}} \\propto \\mu_{\\mathrm{spk}} n r = \\left(\\frac{n}{r}\\right) n r = n^2\n$$\n\n**3. Calculation of the Ratio $\\rho$**\n\nWe now have the scaling for the minimal sample sizes for both matrices, up to a common universal constant and polylogarithmic factors:\n$$\nm_{\\mathrm{coh}} \\propto nr\n$$\n$$\nm_{\\mathrm{spk}} \\propto n^2\n$$\nThe ratio $\\rho$ is therefore:\n$$\n\\rho = \\frac{m_{\\mathrm{spk}}}{m_{\\mathrm{coh}}} = \\frac{C' n^2}{C' nr} = \\frac{n}{r}\n$$\nwhere $C'$ represents the proportionality factor including the universal constant and any ignored polylogarithmic terms, which cancels out in the ratio.",
            "answer": "$$\\boxed{\\frac{n}{r}}$$"
        }
    ]
}