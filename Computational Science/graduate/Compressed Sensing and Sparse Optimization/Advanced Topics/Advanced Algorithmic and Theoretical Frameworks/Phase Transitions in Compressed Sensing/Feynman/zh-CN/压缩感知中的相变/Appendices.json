{
    "hands_on_practices": [
        {
            "introduction": "压缩感知中观察到的急剧相变并非偶然，而是高维几何的直接产物。这项实践将引导你推导统计维度（statistical dimension）——这一预测相变精确位置的核心数学工具——在$\\ell_1$范数下降锥上的变分表示。 通过完成此练习，你将把抽象的凸锥几何与一个具体的、可计算的公式联系起来，该公式决定了成功进行稀疏恢复所需的最小测量次数。",
            "id": "3466268",
            "problem": "令 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 为 $\\ell_{1}$ 范数 $f(x)=\\|x\\|_{1}$。固定一个向量 $x_{0}\\in\\mathbb{R}^{n}$，其支撑集为 $S=\\{i:x_{0,i}\\neq 0\\}$，基数为 $|S|=k$，并假设对于 $i\\in S$，$\\operatorname{sign}(x_{0,i})\\in\\{-1,+1\\}$ 是固定的。$f$ 在 $x_{0}$ 处的下降锥定义为\n$$\nD(f,x_{0})=\\operatorname{cl}\\,\\{\\,t(u-x_{0}): t\\geq 0,\\ f(u)\\leq f(x_{0})\\,\\},\n$$\n一个闭凸锥 $C\\subset\\mathbb{R}^{n}$ 的统计维度为\n$$\n\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big],\n$$\n其中 $g\\sim\\mathcal{N}(0,I_{n})$，$I_{n}$是$n$维单位矩阵，$\\Pi_{C}$ 表示到 $C$ 上的欧几里得投影。你可以不加证明地使用以下经过充分检验的事实：\n- 对于任意闭凸锥 $C$，$\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$，其中 $C^{\\circ}$ 是其极锥。\n- 对于任意正常凸函数 $f$，$D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$，其中 $\\partial f(x_{0})$ 是次微分。\n- 对于任意不包含原点的非空紧集 $K\\subset\\mathbb{R}^{n}$ 和任意 $y\\in\\mathbb{R}^{n}$，$\\operatorname{dist}(y,\\operatorname{cone}(K))=\\inf_{\\tau\\geq 0}\\operatorname{dist}(y,\\tau K)$。\n\n从上述定义和这些事实出发，仅使用 $\\ell_{1}$ 次微分和高斯期望的基本性质，推导统计维度 $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ 的一个显式一维变分表示。该表示应仅依赖于 $n$、$k$ 和一个标量参数，并包含一个关于标准正态随机变量的一维期望。你的最终结果应该表示为一个单一的闭式解析表达式，用 $n$、$k$、一个关于非负标量参数的下确界以及一个涉及应用于标准正态随机变量绝对值的正部算子的一维期望来表示。不需要进行数值计算，也不需要四舍五入。请提供最终表达式作为你的答案。",
            "solution": "问题要求我们求出 $\\ell_{1}$ 范数在点 $x_{0}$ 处下降锥的统计维度 $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ 的一维变分表示。我们已知闭凸锥 $C \\subset \\mathbb{R}^n$ 的统计维度定义为 $\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big]$，其中 $g\\sim\\mathcal{N}(0,I_{n})$，$I_{n}$是$n$维单位矩阵，$\\Pi_{C}$ 是到 $C$ 上的欧几里得投影。\n\n我们的推导将从给定的定义和事实出发，分几步进行。\n\n令 $f(x) = \\|x\\|_{1}$。下降锥为 $C_0 = D(f,x_{0})$。\n\n第 1 步：将统计维度与极锥联系起来。\n提供的第一个事实是，对于任何闭凸锥 $C$，其统计维度可以用其极锥 $C^{\\circ}$ 表示为 $\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$。\n将此应用于下降锥 $C_0$，我们得到：\n$$\n\\delta(C_0) = \\mathbb{E}_{g \\sim \\mathcal{N}(0,I_n)}\\big[\\operatorname{dist}^{2}(g, C_0^{\\circ})\\big]\n$$\n其中 $\\operatorname{dist}(y, A) = \\inf_{a \\in A} \\|y-a\\|_2$ 是从点 $y$ 到集合 $A$ 的欧几里得距离。\n\n第 2 步：刻画下降锥的极锥。\n第二个事实指出，对于一个正常凸函数 $f$，下降锥的极锥是其次微分的锥包：$D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$。\n$\\ell_1$ 范数 $f(x) = \\|x\\|_1$ 是一个正常凸函数。因此，\n$$\nC_0^{\\circ} = \\operatorname{cone}(\\partial \\|x_{0}\\|_{1})\n$$\n\n第 3 步：确定 $\\ell_1$ 范数的次微分。\n函数为 $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$。次微分 $\\partial f(x_0)$是满足对所有 $x \\in \\mathbb{R}^n$ 都有 $\\|x\\|_1 \\ge \\|x_0\\|_1 + v^T(x-x_0)$ 的向量 $v \\in \\mathbb{R}^n$ 的集合。$\\ell_1$ 范数在 $x_0$ 处的次微分的一个标准刻画由下式给出：\n$$\n\\partial \\|x_{0}\\|_{1} = \\{v \\in \\mathbb{R}^n : v_i = \\operatorname{sign}(x_{0,i}) \\text{ for } i \\in S, \\text{ and } v_i \\in [-1, 1] \\text{ for } i \\notin S\\}\n$$\n其中 $S = \\{i : x_{0,i} \\neq 0\\}$ 是 $x_0$ 的支撑集，大小为 $|S|=k$。我们用 $K$ 表示这个紧凸集。令 $s_i = \\operatorname{sign}(x_{0,i})$ 对于 $i \\in S$，其中 $s_i \\in \\{-1, +1\\}$。\n\n第 4 步：用变分形式表示到锥包的距离。\n我们现在有 $C_0^{\\circ} = \\operatorname{cone}(K)$。问题要求我们计算 $\\mathbb{E}[\\operatorname{dist}^{2}(g, \\operatorname{cone}(K))]$。\n集合 $K$ 是非空紧集。我们假设 $k = |S| \\ge 1$，这在问题设定中已强烈暗示。如果 $k \\ge 1$，对于任意 $v \\in K$，$\\|v\\|_2^2 = \\sum_{i \\in S} v_i^2 + \\sum_{i \\notin S} v_i^2 = \\sum_{i \\in S} s_i^2 + \\sum_{i \\notin S} v_i^2 = k + \\sum_{i \\notin S} v_i^2 \\ge k \\ge 1$。因此，$K$ 不包含原点。我们从而可以应用第三个事实：\n$$\n\\operatorname{dist}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}(g, \\tau K)\n$$\n其中 $\\tau K = \\{\\tau v : v \\in K\\}$。两边平方得到：\n$$\n\\operatorname{dist}^{2}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}^{2}(g, \\tau K) = \\inf_{\\tau \\ge 0} \\left( \\inf_{v \\in K} \\|g - \\tau v\\|_2^2 \\right)\n$$\n\n第 5 步：解决内部最小化问题。\n对于固定的 $\\tau \\ge 0$，我们需要计算 $\\inf_{v \\in K} \\|g - \\tau v\\|_2^2$。平方范数在坐标上是可分的：\n$$\n\\|g - \\tau v\\|_2^2 = \\sum_{i=1}^{n} (g_i - \\tau v_i)^2 = \\sum_{i \\in S} (g_i - \\tau v_i)^2 + \\sum_{i \\notin S} (g_i - \\tau v_i)^2\n$$\n我们逐项对 $v \\in K$ 进行最小化。\n对于 $i \\in S$，$v_i$ 固定为 $s_i$。该项为 $(g_i - \\tau s_i)^2$。\n对于 $i \\notin S$，我们必须在 $v_i \\in [-1, 1]$ 上最小化 $(g_i - \\tau v_i)^2$。这等价于在区间 $[-\\tau, \\tau]$ 中找到离 $g_i$ 最近的点。最小平方距离由 $(|g_i|-\\tau)_+^2$ 给出，其中 $(x)_+ = \\max(0,x)$ 是正部算子。\n综合这些结果，我们得到：\n$$\n\\operatorname{dist}^{2}(g, \\tau K) = \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2\n$$\n将其代回到到锥的距离表达式中：\n$$\n\\operatorname{dist}^{2}(g, C_0^{\\circ}) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\}\n$$\n\n第 6 步：计算期望。\n统计维度是上述数量的期望值。\n$$\n\\delta(C_0) = \\mathbb{E}_{g} \\left[ \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\} \\right]\n$$\n为了获得一维变分表示，我们交换期望和下确界。这是此类推导中的一个标准步骤，其合理性由高维高斯向量的测度集中现象保证。\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\mathbb{E}_{g} \\left[ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right]\n$$\n根据期望的线性性质，我们可以将其写为：\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} \\mathbb{E}_{g_i}[(g_i - \\tau s_i)^2] + \\sum_{i \\notin S} \\mathbb{E}_{g_i}[(|g_i| - \\tau)_+^2] \\right\\}\n$$\n$g$ 的分量 $g_i$ 是独立同分布的，服从 $g_i \\sim \\mathcal{N}(0,1)$。\n\n我们来计算期望。对于 $i \\in S$，$s_i^2 = (\\pm 1)^2 = 1$。由于 $\\mathbb{E}[g_i] = 0$ 和 $\\mathbb{E}[g_i^2] = 1$：\n$$\n\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2 - 2\\tau s_i g_i + \\tau^2 s_i^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2(1) = 1 + \\tau^2\n$$\n因为有 $|S|=k$ 个这样的项，它们的和是 $k(1 + \\tau^2)$。\n\n对于 $i \\notin S$，这些项是相同的。令 $Z \\sim \\mathcal{N}(0,1)$。我们有 $(n-k)$ 个形式为 $\\mathbb{E}[(|Z|-\\tau)_+^2]$ 的项。问题要求用这个一维期望来表示结果。\n\n第 7 步：最后组装。\n结合计算出的期望，我们得到统计维度的最终表达式：\n$$\n\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big) = \\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} [(|Z|-\\tau)_+^2] \\right\\}\n$$\n这就是所要求的一维变分表示，用 $n$、$k$、标量参数 $\\tau$ 和一个一维期望表示。\n该表达式按要求使用了应用于标准正态随机变量绝对值的正部算子 $(x)_+ = \\max(0,x)$。",
            "answer": "$$\n\\boxed{\\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} \\left[\\left(\\max\\left(0, |Z|-\\tau\\right)\\right)^2\\right] \\right\\}}\n$$"
        },
        {
            "introduction": "虽然统计维度预测了恢复何时可能成功，但理解相变也需要知道恢复为何会失败。这项练习提供了一个具体的思维实验，通过构建一个违反零空间性质（Null Space Property, NSP）的向量来展示$\\ell_1$最小化在特定条件下的必然失败，而NSP是保证恢复成功的关键条件。 这种动手构建使恢复失败的抽象概念变得具体可感，并突显了测量矩阵性质的关键作用。",
            "id": "3466208",
            "problem": "设 $n \\in \\mathbb{N}$ 和 $k \\in \\mathbb{N}$ 满足 $n \\geq 3$ 及 $2 \\leq k  n$。考虑由单行感知规则 $A x = \\sum_{i=1}^{n} x_{i}$ 定义的线性测量映射 $A \\in \\mathbb{R}^{1 \\times n}$。在压缩感知中，如果对于每个非零向量 $h \\in \\ker A$ 和每个索引集 $S \\subset \\{1,\\dots,n\\}$（其中 $|S| \\leq k$），都有 $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$，则称感知矩阵 $A$ 满足 $k$ 阶零空间性质（NSP）。$\\mathbb{R}^{n}$ 中的 $\\ell_{1}$ 球具有与坐标支撑集和符号模式相对应的面；与固定的 $S$ 和全正符号模式相关联的 $k$ 维面由这样的向量张成：其支撑集为 $S$，在 $S$ 上的坐标严格为正，在 $S^{c}$ 上的坐标为零。\n  \n固定任意集合 $S \\subset \\{1,\\dots,n\\}$，其满足 $|S| = k$。构造一个非零向量 $h \\in \\ker A$，该向量与对应于 $S$ 的 $\\ell_{1}$ 球的面在以下意义上对齐：对于所有 $i \\in S$，有 $h_{i} > 0$，并且 $h$ 在 $S^{c}$ 中至多有一个非零坐标。仅根据基本定义，验证当 $m=1  k$ 和 $n \\ge 3$ 时，这种构造总是可能的，然后计算比率\n$$\nR(k) \\triangleq \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}\n$$\n的精确值。将您的最终答案表示为单个精确实数或单个闭式解析表达式。无需四舍五入。",
            "solution": "该问题要求构造一个满足一组条件的特定向量 $h \\in \\mathbb{R}^{n}$，然后计算其在互补索引集上的 $\\ell_1$ 范数的比率。\n\n首先，我们验证问题陈述。该问题属于压缩感知和线性代数这一标准数学学科。所提供的定义和参数是自洽、明确且数学上合理的。\n参数为自然数 $n$ 和 $k$，满足 $n \\geq 3$ 和 $2 \\leq k  n$。线性测量映射为 $A \\in \\mathbb{R}^{1 \\times n}$，其对向量 $x \\in \\mathbb{R}^{n}$ 的作用定义为 $A x = \\sum_{i=1}^{n} x_{i}$。这定义了 $A$ 是一个所有分量都为 $1$ 的行向量。$A$ 的核空间是其分量之和为零的向量集合：$\\ker A = \\{h \\in \\mathbb{R}^{n} \\mid \\sum_{i=1}^{n} h_{i} = 0\\}$。问题陈述为零空间性质（NSP）提供了正确的定义。\n任务是固定一个基数为 $|S|=k$ 的任意索引集 $S \\subset \\{1, \\dots, n\\}$，并构造一个非零向量 $h \\in \\ker A$，该向量还满足以下两个性质：\n1. 对于所有 $i \\in S$，有 $h_{i} > 0$。\n2. 子向量 $h_{S^c}$（$h$ 在补集 $S^c$ 中的分量）至多有一个非零项。\n问题要求我们验证在给定约束下这种构造总是可能的，然后计算比率 $R(k) = \\|h_{S}\\|_{1} / \\|h_{S^{c}}\\|_{1}$。\n该问题是适定的、客观的且有科学依据的。没有矛盾或缺失信息。该问题是有效的。\n\n我们开始求解。设 $S \\subset \\{1, \\dots, n\\}$ 是一个基数为 $|S| = k$ 的任意索引集。我们将构造一个向量 $h \\in \\mathbb{R}^{n}$，使其满足所有陈述的条件。\n\n为满足第一个性质，即对所有 $i \\in S$ 有 $h_i > 0$，我们设 $h$ 在这些索引上的分量为 $h_i = a_i$，其中每个 $a_i$ 是任意正实数（$a_i > 0$）。\n\n为满足第二个性质，即 $h$ 在 $S^c$ 中至多有一个非零坐标，我们首先注意到补集 $S^c = \\{1, \\dots, n\\} \\setminus S$ 的基数为 $|S^c| = n-k$。因为给定 $k  n$，所以有 $n-k \\ge 1$，因此 $S^c$ 非空。于是我们可以选择一个索引 $j_0 \\in S^c$。我们设分量 $h_{j_0} = \\alpha$（对于某个实数 $\\alpha$），并将 $S^c$ 中的所有其他分量设为零，即对于所有 $j \\in S^c \\setminus \\{j_0\\}$，有 $h_j = 0$。\n\n为满足条件 $h \\in \\ker A$，$h$ 的所有分量之和必须为零：\n$$\n\\sum_{i=1}^{n} h_{i} = 0\n$$\n我们可以将这个和分解到集合 $S$ 和 $S^c$ 上：\n$$\n\\sum_{i \\in S} h_{i} + \\sum_{j \\in S^{c}} h_{j} = 0\n$$\n代入我们构造的分量：\n$$\n\\left( \\sum_{i \\in S} a_{i} \\right) + \\left( h_{j_0} + \\sum_{j \\in S^{c}, j \\neq j_0} h_j \\right) = 0\n$$\n$$\n\\left( \\sum_{i \\in S} a_{i} \\right) + ( \\alpha + 0 ) = 0\n$$\n这个方程决定了 $\\alpha$ 的值：\n$$\n\\alpha = - \\sum_{i \\in S} a_{i}\n$$\n由于 $k \\ge 2$，在 $S$ 中至少有两个索引。因为每个 $a_i > 0$，它们的和 $\\sum_{i \\in S} a_{i}$ 严格为正。因此，$\\alpha$ 严格为负，从而非零。这证实了我们的向量 $h$ 在 $S^c$ 中恰好有一个非零坐标，并且总体上是一个非零向量。对于任何 $S$（满足 $|S|=k$）、任何 $j_0 \\in S^c$ 以及任何正值 $a_i$ 的选择，这种构造总是可能的。\n\n接下来，我们计算所要求的比率 $R(k) = \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}$。\n子向量 $h_S$ 的 $\\ell_1$-范数是其分量绝对值之和：\n$$\n\\|h_{S}\\|_{1} = \\sum_{i \\in S} |h_{i}|\n$$\n根据我们的构造，对所有 $i \\in S$ 有 $h_i = a_i > 0$。因此， $|h_i| = a_i$。\n$$\n\\|h_{S}\\|_{1} = \\sum_{i \\in S} a_{i}\n$$\n子向量 $h_{S^c}$ 的 $\\ell_1$-范数是：\n$$\n\\|h_{S^{c}}\\|_{1} = \\sum_{j \\in S^{c}} |h_{j}|\n$$\n根据构造，该子向量中只有一个分量非零，即 $h_{j_0} = \\alpha$。\n$$\n\\|h_{S^{c}}\\|_{1} = |h_{j_{0}}| + \\sum_{j \\in S^{c}, j \\neq j_{0}} |0| = |\\alpha|\n$$\n代入 $\\alpha$ 的表达式：\n$$\n\\|h_{S^{c}}\\|_{1} = \\left| - \\sum_{i \\in S} a_{i} \\right|\n$$\n因为正数之和 $\\sum_{i \\in S} a_{i}$ 是正的，所以其绝对值就是它本身。\n$$\n\\|h_{S^{c}}\\|_{1} = \\sum_{i \\in S} a_{i}\n$$\n最后，我们计算比率 $R(k)$：\n$$\nR(k) = \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}} = \\frac{\\sum_{i \\in S} a_{i}}{\\sum_{i \\in S} a_{i}}\n$$\n由于和 $\\sum_{i \\in S} a_{i}$ 严格为正，该比率是良定义的，并简化为：\n$$\nR(k) = 1\n$$\n这个结果与集合 $S$ 的具体选择、索引 $j_0 \\in S^c$、正值 $a_i$ 以及参数 $n$ 和 $k$（在其指定范围内）无关。因此，所求的值是 $1$。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "理论预测提供了地图，而计算实验则是验证和探索这片领域的旅程。这个综合性的编码实践将挑战你通过经验性方法生成并比较凸的$\\ell_1$最小化和更强大的非凸$\\ell_p$最小化的相图。 通过这次模拟，你不仅能将理论上的相变可视化，还能量化非凸方法在增强恢复能力与牺牲算法稳定性之间的实际权衡。",
            "id": "3466212",
            "problem": "考虑一个无噪声压缩感知模型，其中测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 具有独立同分布的高斯条目（经归一化以使每列的期望范数为单位1），用于获取测量值 $y = A x^\\star$，其中目标向量 $x^\\star \\in \\mathbb{R}^n$ 是 $k$-稀疏的。我们比较两种重建策略：一种是凸基准策略，它在约束 $A x = y$ 下最小化 $\\ell_1$ 范数；另一种是非凸替代策略，它在约束 $A x = y$ 下最小化 $\\ell_p$ 拟范数（其中 $p \\in (0,1)$）。总体目标是生成一个计算相图，并量化非凸 $\\ell_p$ 策略在相变边界上实现的经验增益是否超过了由非凸性引入的算法不稳定性代价。\n\n该问题的基础是：\n- 稀疏性的定义：如果一个向量 $x^\\star$ 恰好有 $k$ 个非零项，则称其为 $k$-稀疏。\n- 凸正则化子相变的锥几何解释：在 $x^\\star$ 处的下降锥（或等价的锥代理）的统计维度预测了以高概率实现精确重建所需的测量数 $m$ 的近似阈值，该阈值是 $(n,k)$ 和正则化子几何形状的函数。\n- 对于 $p \\in (0,1)$ 的非凸 $\\ell_p$ 拟范数比 $\\ell_1$ 范数更积极地促进稀疏性，但它不是凸的，这可能导致算法不稳定。任何针对非凸下降集的广义统计维度近似都必须依赖于从 $x^\\star$ 和正则化子的局部几何构造出的有原则的代理。\n\n程序要求：\n1. 实现一个经验相图生成器。对于固定的 $(n,k,p)$ 和指定的 $x^\\star$ 非零项幅值模式，该生成器估计 $\\ell_1$ 最小化和 $\\ell_p$ 最小化的经验相变边界。相变边界近似为在一个设计的网格中，成功率在多次随机试验中至少达到指定阈值的最小 $m$ 值。\n2. 为非凸 $\\ell_p$ 下降集构造一个广义统计维度近似，方法是用一个从 $x^\\star$ 的幅值分布计算出的有效稀疏度量来替换基数 $k$，该计算使用从 $\\ell_p$ 几何导出的权重。使用此近似来选择预测的 $\\ell_1$ 和 $\\ell_p$ 相变点附近的 $m$ 网格，确保网格在预期的边界之下、之上和附近进行探测。\n3. 通过线性规划公式实现 $\\ell_1$ 最小化，并通过针对等式约束的迭代重加权最小二乘法（IRLS）实现 $\\ell_p$ 最小化。确保使用数值稳定的线性代数，并检测IRLS算法中的不收敛或数值不稳定性。\n4. 将单次试验的经验成功定义为重建向量 $\\hat{x}$ 与 $x^\\star$ 之间的相对 $\\ell_2$ 误差小于 $10^{-3}$ 的事件，即 $\\| \\hat{x} - x^\\star \\|_2 / \\| x^\\star \\|_2  10^{-3}$。\n5. 将一个测试案例中 $\\ell_p$ 方法的算法不稳定性代价定义为，在所选 $m$ 网格内，IRLS 运行未能在固定迭代预算内收敛到稳定解或产生数值上无效迭代的平均比例。\n6. 将相变边界的经验增益定义为 $\\ell_1$ 和 $\\ell_p$ 的经验临界测量数之差，并用 $n$ 进行归一化（即，计算 $(m_{\\text{crit},\\ell_1} - m_{\\text{crit},\\ell_p})/n$，其中 $m_{\\text{crit},\\cdot}$ 是成功率超过阈值的最小 $m$）。如果此归一化增益严格大于不稳定性代价，则得出增益超过代价的结论。\n7. 您的程序应生成一行输出，其中包含结果，格式为一个方括号内用逗号分隔的列表（例如，\"[result1,result2,result3]\"），其中每个条目是对应一个测试案例的布尔值，指示经验增益是否超过算法不稳定性代价。\n\n广义统计维度近似规范：\n- 使用一个为非凸 $\\ell_p$ 几何定制的参与率类型代理，从 $x^\\star$ 的非零项幅值计算有效稀疏度。设 $a_i = |x^\\star_i|^p$ 用于支撑集内的索引，其他索引则 $a_i = 0$。将有效稀疏度定义为\n$$\ns_{\\mathrm{eff}}(p) = \\frac{\\left( \\sum_{i=1}^n a_i \\right)^2}{\\sum_{i=1}^n a_i^2}.\n$$\n- 在一个凸统计维度代理中用此有效稀疏度代替 $k$，以近似 $\\ell_p$ 的相变边界，作为 $(n, s_{\\mathrm{eff}}(p))$ 的函数。凸基准使用实际基数 $k$。这将产生近似的临界测量计数，程序应使用这些计数来为经验探测设定 $m$ 网格的中心。\n\n重建算法：\n- 对于 $\\ell_1$ 基准，通过使用标准上镜图技巧的线性规划来求解优化问题 $\\min \\|x\\|_1$ subject to $A x = y$。确保在无噪声设置下精确满足所有约束。\n- 对于 $\\ell_p$ 方法，实现IRLS以近似最小化 $\\sum_i |x_i|^p$ subject to $A x = y$，方法是求解一系列带等式约束的加权最小二乘问题。通过对相关线性求解施加一个小的Tikhonov正则化来稳定该过程，如果迭代的相对变化在指定的迭代限制内没有降到容差以下，则检测为不收敛。\n\n测试套件：\n对于每个测试案例，程序必须从所有 $k$-子集中均匀随机选择 $x^\\star$ 的支撑集，并根据指定的模式分配幅值。非零项的符号必须在 $\\{-1,+1\\}$ 中均匀随机。测量矩阵 $A$ 必须具有按 $1/\\sqrt{m}$ 缩放的独立同分布高斯条目。\n\n使用以下测试套件：\n- 案例1（理想路径）：$n = 64$, $k = 8$, $p = 0.5$，支撑集上幅值相等。\n- 案例2（强非凸，重尾）：$n = 64$, $k = 8$, $p = 0.25$，支撑集上幅值从Laplace分布中抽取。\n- 案例3（中度非凸，结构化衰减）：$n = 64$, $k = 12$, $p = 0.5$，支撑集上幅值呈几何衰减。\n- 案例4（近凸基准）：$n = 64$, $k = 8$, $p = 0.9$，支撑集上幅值相等。\n\n对于每个案例，通过以从 $\\ell_p$ 的广义统计维度近似和 $\\ell_1$ 的凸代理导出的预测相变计数为中心来构建 $m$ 网格，并探测至少三个覆盖预测边界之下、附近和之上的不同 $m$ 值。\n\n阈值和数值参数：\n- 在给定 $m$ 下，多次试验的成功阈值：成功率至少为 $0.6$。\n- IRLS最大迭代次数：$200$，迭代容差：$10^{-6}$，等式求解正则化：$10^{-12}$。\n- 每个测试案例中每个 $m$ 的试验次数：$5$。\n- 相对误差成功标准：严格小于 $10^{-3}$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含四个测试案例的布尔结果，格式为方括号内的逗号分隔列表，顺序与上面列出的案例一致。示例格式：\"[true,false,true,false]\"。实际大小写必须匹配Python布尔值格式，即\"True\"或\"False\"。",
            "solution": "我们从无噪声线性模型 $y = A x^\\star$ 开始，其中 $A \\in \\mathbb{R}^{m \\times n}$ 具有按 $1/\\sqrt{m}$ 缩放的独立同分布高斯条目，且目标 $x^\\star \\in \\mathbb{R}^n$ 是一个 $k$-稀疏向量。稀疏重建旨在从 $y$ 和 $A$ 中重建 $x^\\star$。对于像 $\\ell_1$ 范数这样的凸正则化子，其相变现象可以通过锥几何来解释：当测量数量 $m$ 超过一个由正则化子在 $x^\\star$ 处的下降锥的统计维度决定的阈值时，精确重建会以高概率发生；低于该阈值时，重建通常会失败。一个闭凸锥 $C \\subset \\mathbb{R}^n$ 的统计维度定义为\n$$\n\\delta(C) = \\mathbb{E}\\left[ \\| \\Pi_C(g) \\|_2^2 \\right],\n$$\n其中 $\\Pi_C$ 表示到 $C$ 上的欧几里得投影，$g \\sim \\mathcal{N}(0, I_n)$ 是一个标准高斯向量。对于凸 $\\ell_1$ 正则化，这个统计维度取决于 $x^\\star$ 的支撑集大小和 $\\ell_1$ 范数的几何形状，并产生一个近似阈值 $m \\approx \\delta(C)/n$。\n\n对于 $p \\in (0,1)$ 的非凸 $\\ell_p$ 拟范数正则化，其非凸下降集没有闭式解的统计维度，而且这些下降集不是凸的，可能高度不规则。然而，为了生成一个实用的相图，可以通过构造一个尊重 $\\ell_p$ 正则化子在 $x^\\star$ 处局部几何的凸代理来采用一个广义统计维度近似。出发点是由 $\\ell_p$ 几何在幅值上导出的权重序列：为支撑集内的条目定义 $a_i = |x^\\star_i|^p$，为支撑集外的条目定义 $a_i = 0$。一个量化有多少条目对 $\\ell_p$ 几何有显著贡献的有效稀疏度由一个参与率类型的量给出\n$$\ns_{\\mathrm{eff}}(p) = \\frac{\\left( \\sum_{i=1}^n a_i \\right)^2}{\\sum_{i=1}^n a_i^2}.\n$$\n当支撑集上的幅值相等时，该量简化为 $s_{\\mathrm{eff}}(p) = k$；当幅值异构或重尾时，该量减小，这反映了非凸惩罚项比凸 $\\ell_1$ 范数更强烈地将权重集中在更少的条目上。\n\n为了近似 $\\ell_p$ 最小化的相变阈值，用 $s_{\\mathrm{eff}}(p)$ 替换凸统计维度代理中的基数 $k$。一个广泛使用的凸代理，用于环境维度 $n$ 中 $k$-稀疏向量处 $\\ell_1$ 下降锥的统计维度，其形式为\n$$\n\\delta_{\\ell_1}(n,k) \\approx 2 k \\log\\left( \\frac{n}{k} \\right) + c k,\n$$\n其中 $c$ 是一个捕获曲率效应的适中常数（在经验表征中通常取值接近 $7/5$）。类似地，我们提出广义近似\n$$\n\\delta_{\\ell_p}(n,s_{\\mathrm{eff}}(p)) \\approx 2 s_{\\mathrm{eff}}(p) \\log\\left( \\frac{n}{s_{\\mathrm{eff}}(p)} \\right) + c s_{\\mathrm{eff}}(p).\n$$\n这种替换体现了一种启发式思想，即在 $x^\\star$ 处的非凸下降集类似于一个具有较少有效自由度的正则化子的凸下降锥。这种近似起到两个作用：在预测的相变附近选择 $m$ 网格，并提供一个用于比较经验边界的基准。\n\n经验相图生成过程如下：\n- 对于每个测试案例，我们根据指定的模式随机生成 $x^\\star$ 的支撑集和幅值，确保非零项具有随机符号。我们用 $y = A x^\\star$ 构建测量值，其中 $A$ 是高斯矩阵，其条目按 $1/\\sqrt{m}$ 缩放。\n- 对于凸 $\\ell_1$ 基准，我们使用上镜图公式求解线性规划 $\\min \\|x\\|_1$ subject to $A x = y$，其中我们引入辅助变量 $u \\in \\mathbb{R}^n$ 满足 $u_i \\ge |x_i|$ 并最小化 $\\sum_i u_i$。这产生一个决策变量为 $(x,u)$、等式约束为 $A x = y$、不等式为 $x_i \\le u_i$ 和 $-x_i \\le u_i$ 的线性规划。现代求解器可以在无噪声等式约束设置中可靠地处理此公式。\n- 对于非凸 $\\ell_p$ 方法，我们使用带等式约束的迭代重加权最小二乘法（IRLS）来近似最小化 $\\sum_i |x_i|^p$ subject to $A x = y$。在第 $t$ 次迭代，给定当前迭代点 $x^{(t)}$，定义权重\n$$\nw_i^{(t)} = \\left( |x_i^{(t)}|^2 + \\varepsilon \\right)^{\\frac{p}{2} - 1},\n$$\n其中 $\\varepsilon  0$ 是一个小的平滑参数以避免奇异点。下一个迭代点通过最小化 $\\sum_i w_i^{(t)} x_i^2$ subject to $A x = y$ 获得，这是一个加权最小范数解：\n$$\nx^{(t+1)} = W^{-1} A^\\top \\left( A W^{-1} A^\\top \\right)^{-1} y,\n$$\n其中 $W = \\operatorname{diag}(w^{(t)})$。为了稳定 $A W^{-1} A^\\top$ 的求逆过程，我们添加一个小的Tikhonov正则化 $\\gamma I_m$。当相对变化 $\\|x^{(t+1)} - x^{(t)}\\|_2 / \\|x^{(t+1)}\\|_2$ 低于某个容差时，我们宣告收敛。\n- 对于网格中的每个 $m$，我们运行多次试验并计算每种方法的成功率。我们将经验临界测量计数 $m_{\\text{crit},\\cdot}$ 定义为在试验中成功率至少为 $0.6$ 的最小 $m$。如果没有 $m$ 达到此要求，我们设置 $m_{\\text{crit},\\cdot} = +\\infty$。\n- 每个测试案例中 $\\ell_p$ 的算法不稳定性代价计算为，在 $m$ 网格中，IRLS 运行未能在迭代预算内收敛或产生数值上无效迭代的平均比例。这直接从观测到的算法行为中量化不稳定性。\n\n决策标准：\n- 相变边界的经验增益是\n$$\ng = \\frac{m_{\\text{crit},\\ell_1} - m_{\\text{crit},\\ell_p}}{n},\n$$\n如果 $m_{\\text{crit},\\ell_p} \\ge m_{\\text{crit},\\ell_1}$ 或任一临界计数为无穷大，则 $g$ 在0处截断。算法不稳定性代价是 $\\ell_p$ 在整个网格上的平均不收敛率。如果 $g$ 严格大于不稳定性代价，我们得出增益超过代价的结论。\n\n测试套件覆盖范围：\n- 案例1 探测了一个典型的中等稀疏度情景，其中幅值相等，$p = 0.5$；预计非凸方法将带来增益，且不稳定性可控。\n- 案例2 关注一个强非凸设置，其中 $p = 0.25$ 且幅值为重尾（Laplace），这应该会加强有效稀疏度的降低，同时可能增加不稳定性。\n- 案例3 在 $p = 0.5$ 和更大的 $k$ 值下引入了结构化的几何衰减幅值，测试由幅值偏斜引起的有效稀疏度降低是否能抵消增加的稀疏度。\n- 案例4 检验了近凸情景，其中 $p = 0.9$ 且幅值相等，预计增益很小，不稳定性也最小。\n\n算法和数值细节：\n- 测量矩阵按 $1/\\sqrt{m}$ 缩放可以稳定Gram矩阵，并将列范数保持在合理范围内。\n- 对于 $p \\in (0,1)$，IRLS权重指数 $\\frac{p}{2} - 1$ 是负数，因此大系数的权重更小，受到的惩罚也更少，这通过更多地抑制小系数来鼓励稀疏性。平滑参数 $\\varepsilon$ 避免了分量接近零时的奇异权重，而Tikhonov正则化 $\\gamma$ 则稳定了加权正规方程的等式约束线性求解。\n- $\\ell_1$ 的线性规划公式使用了一个上镜图技巧，变量为 $(x,u)$，等式约束为 $A x = y$，不等式约束为 $x - u \\le 0$ 和 $-x - u \\le 0$，这在无噪声情况下是数值可靠的。\n\n该程序实现了以上所有组件，执行四个测试案例，并按顺序打印指示每个案例的经验增益是否超过算法不稳定性代价的布尔值列表。",
            "answer": "```python\nimport numpy as np\nfrom numpy.random import default_rng\nfrom scipy.optimize import linprog\nfrom scipy.linalg import solve\n\n# Seeded random generator for reproducibility\nrng = default_rng(12345)\n\ndef generate_support(n, k):\n    return rng.choice(n, size=k, replace=False)\n\ndef generate_amplitudes(k, pattern):\n    if pattern == \"equal\":\n        amps = np.ones(k)\n    elif pattern == \"geo\":\n        # Geometric decay amplitudes\n        rho = 0.7\n        amps = rho ** np.arange(k)\n        # Shuffle to avoid ordered alignment\n        rng.shuffle(amps)\n        # Normalize to maintain comparable scale\n        amps = amps / np.linalg.norm(amps) * np.sqrt(k)\n    elif pattern == \"laplace\":\n        # Laplace distribution |Laplace(0,1)|, with floor to avoid zeros\n        raw = rng.laplace(loc=0.0, scale=1.0, size=k)\n        amps = np.abs(raw) + 0.1\n        # Normalize\n        amps = amps / np.linalg.norm(amps) * np.sqrt(k)\n    else:\n        # Default equal\n        amps = np.ones(k)\n    return amps\n\ndef generate_sparse_vector(n, k, pattern):\n    x = np.zeros(n)\n    S = generate_support(n, k)\n    amps = generate_amplitudes(k, pattern)\n    signs = rng.choice([-1.0, 1.0], size=k)\n    x[S] = signs * amps\n    return x\n\ndef gaussian_matrix(m, n):\n    # Scale by 1/sqrt(m) to stabilize columns\n    return rng.normal(loc=0.0, scale=1.0/np.sqrt(m), size=(m, n))\n\ndef basis_pursuit_lp(A, y):\n    m, n = A.shape\n    # Variables: [x (n), u (n)]\n    c = np.hstack([np.zeros(n), np.ones(n)])\n    A_eq = np.hstack([A, np.zeros((m, n))])\n    b_eq = y.copy()\n    # Inequalities:\n    # x - u = 0 - [I, -I]\n    # -x - u = 0 - [-I, -I]\n    I = np.eye(n)\n    A_ub_top = np.hstack([ I, -I ])\n    A_ub_bottom = np.hstack([ -I, -I ])\n    A_ub = np.vstack([A_ub_top, A_ub_bottom])\n    b_ub = np.zeros(2*n)\n\n    bounds = []\n    # x free: (None, None), u = 0: (0, None)\n    for _ in range(n):\n        bounds.append((None, None))\n    for _ in range(n):\n        bounds.append((0.0, None))\n\n    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    if not res.success:\n        return None\n    x_rec = res.x[:n]\n    return x_rec\n\ndef irls_lp_equality(A, y, p, max_iter=200, tol=1e-6, eps0=1e-2, gamma=1e-12):\n    m, n = A.shape\n    # Initialize with weighted least squares solution (unweighted)\n    try:\n        G = A @ A.T\n        lam0 = solve(G + gamma*np.eye(m), y, assume_a='sym')\n        x = A.T @ lam0\n    except Exception:\n        # Fallback to least squares via pseudo-inverse\n        x = A.T @ y\n\n    eps = eps0\n    converged = True\n    for it in range(max_iter):\n        # Compute weights\n        w = (np.abs(x)**2 + eps)**(p/2.0 - 1.0)\n        # Avoid singular weights\n        w = np.maximum(w, 1e-12)\n        W_inv = 1.0 / w\n        # Build B = A W^{-1} A^T efficiently by column scaling\n        # A_scaled = A * W_inv along columns\n        A_scaled = A * W_inv\n        B = A_scaled @ A.T\n        try:\n            lam = solve(B + gamma*np.eye(m), y, assume_a='sym')\n        except Exception:\n            converged = False\n            break\n        x_new = W_inv * (A.T @ lam)\n        if not np.all(np.isfinite(x_new)):\n            converged = False\n            break\n        # Check convergence\n        norm_new = np.linalg.norm(x_new)\n        if norm_new == 0:\n            # Degenerate; treat as nonconvergent\n            converged = False\n            break\n        rel_change = np.linalg.norm(x_new - x) / norm_new\n        x = x_new\n        if rel_change  tol:\n            break\n        # Slow decrease of epsilon to allow sharper weights\n        if (it+1) % 10 == 0:\n            eps *= 0.9\n    else:\n        # Max iterations reached\n        converged = False\n    return x, converged\n\ndef relative_error(x, x_true):\n    denom = np.linalg.norm(x_true)\n    if denom == 0:\n        return np.inf\n    return np.linalg.norm(x - x_true) / denom\n\ndef effective_sparsity(x_true, p):\n    a = np.abs(x_true)**p\n    s_num = np.sum(a)**2\n    s_den = np.sum(a**2)\n    if s_den == 0:\n        return 0.0\n    return s_num / s_den\n\ndef delta_surrogate(n, s_eff):\n    # Convex-like surrogate: delta ≈ 2 s log(n/s) + c s, with c ≈ 7/5.\n    s = max(s_eff, 1e-9)\n    return 2.0 * s * np.log(n / s) + (7.0/5.0) * s\n\ndef run_trials(n, k, m, p, amp_pattern, trials=5, tol=1e-3):\n    succ_l1 = 0\n    succ_lp = 0\n    conv_lp = 0\n    for _ in range(trials):\n        x_true = generate_sparse_vector(n, k, amp_pattern)\n        A = gaussian_matrix(m, n)\n        y = A @ x_true\n\n        # l1 recovery\n        x_l1 = basis_pursuit_lp(A, y)\n        if x_l1 is not None and np.all(np.isfinite(x_l1)):\n            err1 = relative_error(x_l1, x_true)\n            if err1  tol:\n                succ_l1 += 1\n\n        # lp recovery via IRLS\n        x_lp, converged = irls_lp_equality(A, y, p)\n        if converged:\n            conv_lp += 1\n        if x_lp is not None and np.all(np.isfinite(x_lp)):\n            errp = relative_error(x_lp, x_true)\n            if errp  tol:\n                succ_lp += 1\n\n    rate_l1 = succ_l1 / trials\n    rate_lp = succ_lp / trials\n    conv_rate_lp = conv_lp / trials\n    return rate_l1, rate_lp, conv_rate_lp\n\ndef empirical_critical_m(n, k, p, amp_pattern, m_grid, trials=5, tol=1e-3, threshold=0.6):\n    crit_l1 = np.inf\n    crit_lp = np.inf\n    conv_rates = []\n    for m in sorted(m_grid):\n        rate_l1, rate_lp, conv_rate_lp = run_trials(n, k, m, p, amp_pattern, trials=trials, tol=tol)\n        conv_rates.append(conv_rate_lp)\n        if np.isinf(crit_l1) and rate_l1 = threshold:\n            crit_l1 = m\n        if np.isinf(crit_lp) and rate_lp = threshold:\n            crit_lp = m\n    # Instability cost: average failure rate across grid\n    if len(conv_rates) == 0:\n        inst_cost = 1.0\n    else:\n        inst_cost = 1.0 - (np.mean(conv_rates))\n    return crit_l1, crit_lp, inst_cost\n\ndef build_m_grid(n, k, p, amp_pattern):\n    # Use a representative x_true to compute effective sparsity and predicted deltas\n    x_true = generate_sparse_vector(n, k, amp_pattern)\n    s_eff = effective_sparsity(x_true, p)\n    # Surrogates for both methods\n    delta_lp = delta_surrogate(n, s_eff)\n    delta_l1 = delta_surrogate(n, float(k))\n    # Center grid near both predictions\n    m_candidates = set()\n    # Ensure m is within [2, n-1] bounds to be meaningful\n    for delta in [delta_lp, delta_l1]:\n        m0 = int(np.clip(np.round(delta), 2, n-1))\n        m_candidates.update([\n            int(np.clip(np.round(0.9 * m0), 2, n-1)),\n            int(np.clip(m0, 2, n-1)),\n            int(np.clip(np.round(1.1 * m0), 2, n-1)),\n        ])\n    # Also include a slightly larger point to probe above boundary\n    m_candidates.add(int(np.clip(np.round(1.25 * delta_lp), 2, n-1)))\n    return sorted(m_candidates)\n\ndef evaluate_case(n, k, p, amp_pattern):\n    m_grid = build_m_grid(n, k, p, amp_pattern)\n    crit_l1, crit_lp, inst_cost = empirical_critical_m(n, k, p, amp_pattern, m_grid)\n    if np.isinf(crit_l1) or np.isinf(crit_lp):\n        gain_frac = 0.0\n    else:\n        gain = crit_l1 - crit_lp\n        gain_frac = max(gain / n, 0.0)\n    # Gains outweigh costs if normalized gain  instability cost\n    decision = gain_frac  inst_cost\n    return decision\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, p, amplitude_pattern)\n        (64, 8, 0.5, \"equal\"),    # Case 1\n        (64, 8, 0.25, \"laplace\"), # Case 2\n        (64, 12, 0.5, \"geo\"),     # Case 3\n        (64, 8, 0.9, \"equal\"),    # Case 4\n    ]\n\n    results = []\n    for n, k, p, pattern in test_cases:\n        decision = evaluate_case(n, k, p, pattern)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}