{
    "hands_on_practices": [
        {
            "introduction": "The remarkable success of $\\ell_1$ minimization in sparse recovery is not magic; it is a direct consequence of the specific geometry of the $\\ell_1$ norm. This first practice invites you to explore the fundamental geometric structures that govern recovery. By deriving the subdifferential and the descent cone at a sparse vector, you will be dissecting the local landscape of the $\\ell_1$ ball, which is the first step toward understanding why moving in certain directions from a sparse solution either preserves or increases the norm. Mastering this foundational calculation () provides the essential geometric building blocks required for the more advanced probabilistic arguments that precisely characterize phase transitions.",
            "id": "3466252",
            "problem": "Consider the convex function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$. Let $x_{0}\\in\\mathbb{R}^{n}$ be $k$-sparse with support $S\\subseteq\\{1,\\dots,n\\}$, $|S|=k$, and define the sign vector $s\\in\\mathbb{R}^{n}$ by $s_{i}=\\operatorname{sign}(x_{0,i})$ for $i\\in S$ and leaving $s_{j}$ undefined for $j\\in S^{c}$ where $x_{0,j}=0$. Denote by $d\\in\\mathbb{R}^{n}$ a direction, by $d_{S}$ and $d_{S^{c}}$ the restrictions of $d$ to $S$ and its complement $S^{c}$, respectively, and similarly for any vector $g\\in\\mathbb{R}^{n}$. The subdifferential of a proper lower semicontinuous convex function $f$ at a point $x$ is defined by\n$$\n\\partial f(x)=\\left\\{g\\in\\mathbb{R}^{n}: f(y)\\geq f(x)+\\langle g,y-x\\rangle\\ \\text{for all}\\ y\\in\\mathbb{R}^{n}\\right\\}.\n$$\nThe descent cone of $f$ at $x$ is defined by\n$$\nD(f,x)=\\left\\{d\\in\\mathbb{R}^{n}:\\exists\\,t>0\\ \\text{such that}\\ f(x+td)\\leq f(x)\\right\\}.\n$$\nStarting from these definitions and elementary properties of the absolute value and convexity, derive a complete characterization of the subdifferential $\\partial\\|x\\|_{1}(x_{0})$ in terms of the support $S$ and signs $s_{i}$, and then characterize the descent cone $D(\\|\\cdot\\|_{1},x_{0})$ as the conic hull of directions $d$ that satisfy a single explicit inequality written in terms of $s_{S}$, $d_{S}$, and $\\|d_{S^{c}}\\|_{1}$. Express your final result as a single closed-form analytic expression collecting both characterizations in a compact form. No numerical approximation is required, and no physical units are involved. Your final answer must be a single expression.",
            "solution": "We begin from the fundamental definitions of subdifferential and descent cone for convex functions and apply them to the $\\ell_{1}$ norm, using the structure induced by the support $S$ of the sparse vector $x_{0}$ and its sign pattern.\n\nCharacterization of the subdifferential. The function $f(x)=\\|x\\|_{1}$ is separable across coordinates: $f(x)=\\sum_{i=1}^{n}|x_{i}|$. For a single-coordinate absolute value, the subdifferential at a point $a\\in\\mathbb{R}$ is well known and follows from the convex subdifferential definition:\n- If $a\\neq 0$, then $\\partial|a|=\\{\\operatorname{sign}(a)\\}$.\n- If $a=0$, then $\\partial|a|=[-1,1]$.\n\nThis can be derived directly. For $a\\neq 0$, convexity and differentiability give $\\partial|a|=\\{\\operatorname{sign}(a)\\}$, because $|y|\\geq|a|+\\operatorname{sign}(a)(y-a)$ for all $y\\in\\mathbb{R}$, which follows from the supporting hyperplane at a nonzero point of $|\\cdot|$. For $a=0$, the subdifferential is the interval $[-1,1]$, because for any $g\\in[-1,1]$ and any $y\\in\\mathbb{R}$ we have $|y|\\geq g\\,y$ (indeed, $\\sup_{g\\in[-1,1]}g\\,y=|y|$, so every $g$ in the interval satisfies the subgradient inequality at $0$). Conversely, if $g\\notin[-1,1]$, one can violate $|y|\\geq g\\,y$ by choosing $y$ with the same sign as $g$ and sufficiently large magnitude, so $g\\notin\\partial|0|$.\n\nSince $f$ is a sum of absolute values, its subdifferential at $x_{0}$ is the Cartesian product of the coordinatewise subdifferentials, which yields\n$$\n\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{i}=\\operatorname{sign}(x_{0,i})\\ \\text{for all}\\ i\\in S,\\ \\text{and}\\ g_{j}\\in[-1,1]\\ \\text{for all}\\ j\\in S^{c}\\right\\}.\n$$\nEquivalently, writing $s_{S}\\in\\mathbb{R}^{S}$ for the sign vector on $S$, this can be expressed compactly as\n$$\n\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{S}=s_{S},\\ \\|g_{S^{c}}\\|_{\\infty}\\leq 1\\right\\}.\n$$\nTo verify necessity, assume $g\\in\\partial\\|x\\|_{1}(x_{0})$. Consider any $i\\in S$. Fix $y=x_{0}+t\\,e_{i}$ where $e_{i}$ is the $i$th coordinate vector and $t$ has the same sign as $x_{0,i}$. The subgradient inequality gives\n$$\n\\|x_{0}+t\\,e_{i}\\|_{1}\\geq\\|x_{0}\\|_{1}+g_{i}\\,t.\n$$\nBut $\\|x_{0}+t\\,e_{i}\\|_{1}=\\|x_{0}\\|_{1}+|x_{0,i}+t|-|x_{0,i}|=\\|x_{0}\\|_{1}+t\\,\\operatorname{sign}(x_{0,i})$ for sufficiently small $t$ not crossing zero in the $i$th coordinate. Hence $t\\,\\operatorname{sign}(x_{0,i})\\geq g_{i}\\,t$ for arbitrarily small nonzero $t$ of the same sign, which implies $g_{i}=\\operatorname{sign}(x_{0,i})$. For $j\\in S^{c}$, set $y=t\\,e_{j}$. Then the subgradient inequality yields $|t|\\geq g_{j}\\,t$ for all $t\\in\\mathbb{R}$, implying $|g_{j}|\\leq 1$. Thus the characterization is both sufficient and necessary.\n\nCharacterization of the descent cone. By definition,\n$$\nD(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}:\\exists\\,t>0\\ \\text{such that}\\ \\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}\\right\\}.\n$$\nWe analyze $\\|x_{0}+t\\,d\\|_{1}$ by splitting indices according to $S$ and $S^{c}$. Write\n$$\n\\|x_{0}+t\\,d\\|_{1}=\\sum_{i\\in S}|x_{0,i}+t\\,d_{i}|+\\sum_{j\\in S^{c}}|t\\,d_{j}|.\n$$\nFor $i\\in S$ with $x_{0,i}\\neq 0$, convexity of $|\\cdot|$ and the supporting hyperplane inequality at $x_{0,i}$ imply\n$$\n|x_{0,i}+t\\,d_{i}|\\geq |x_{0,i}|+t\\,\\operatorname{sign}(x_{0,i})\\,d_{i}.\n$$\nFor $j\\in S^{c}$, we have the exact identity $|t\\,d_{j}|=t\\,|d_{j}|$. Summing over coordinates, we obtain\n$$\n\\|x_{0}+t\\,d\\|_{1}\\geq \\|x_{0}\\|_{1}+t\\left(s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\right).\n$$\nTherefore, the inequality $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$ can hold for some $t>0$ only if the linear term satisfies $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0$. Conversely, if $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}0$, then the right-hand side is strictly less than $\\|x_{0}\\|_{1}$ for sufficiently small $t>0$, ensuring $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$. If $s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}=0$, the inequality holds to first order, and by continuity and convexity, one can choose a sequence $t\\downarrow 0$ such that $\\|x_{0}+t\\,d\\|_{1}\\leq\\|x_{0}\\|_{1}$. Hence the necessary and sufficient condition is\n$$\ns_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0.\n$$\nThis set of directions is a cone, because for any $\\alpha>0$,\n$$\ns_{S}^{\\top}(\\alpha d_{S})+\\|\\alpha d_{S^{c}}\\|_{1}=\\alpha\\left(s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\right)\\leq 0,\n$$\nso $\\alpha d$ also satisfies the inequality. Therefore,\n$$\nD(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}.\n$$\n\nCollecting the two characterizations, we have expressed both the subdifferential at $x_{0}$ and the descent cone at $x_{0}$ in closed form in terms of $S$, $s_{S}$, and the norms $\\|\\cdot\\|_{\\infty}$ and $\\|\\cdot\\|_{1}$ on $S^{c}$. These characterizations are central in analyzing phase transitions in compressed sensing, because the geometry of $D(\\|\\cdot\\|_{1},x_{0})$ controls measurement thresholds via quantities such as the statistical dimension; however, the derivation above relies only on convexity and coordinatewise properties of the absolute value.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\partial\\|x\\|_{1}(x_{0})=\\left\\{g\\in\\mathbb{R}^{n}: g_{S}=s_{S},\\ \\|g_{S^{c}}\\|_{\\infty}\\leq 1\\right\\}  D(\\|\\cdot\\|_{1},x_{0})=\\left\\{d\\in\\mathbb{R}^{n}: s_{S}^{\\top}d_{S}+\\|d_{S^{c}}\\|_{1}\\leq 0\\right\\}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Building upon the geometric objects from the previous exercise, we can now quantify the \"size\" of the descent cone to predict the exact location of the phase transition. This is where abstract geometry meets a precise probabilistic formula. In this practice, you will derive the celebrated one-dimensional variational formula for the statistical dimension of the $\\ell_1$ descent cone (). This powerful result connects the number of measurements $m$ needed for successful recovery to the signal's sparsity $k$ and the ambient dimension $n$, encapsulating the complex high-dimensional geometry in a simple, computable expression. This derivation is central to the modern understanding of phase transitions and demystifies the origin of the sharp performance thresholds observed in practice.",
            "id": "3466268",
            "problem": "Let $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ be the $\\ell_{1}$ norm $f(x)=\\|x\\|_{1}$. Fix a vector $x_{0}\\in\\mathbb{R}^{n}$ with support set $S=\\{i:x_{0,i}\\neq 0\\}$ of cardinality $|S|=k$, and assume that $\\operatorname{sign}(x_{0,i})\\in\\{-1,+1\\}$ is fixed for $i\\in S$. The descent cone of $f$ at $x_{0}$ is defined by\n$$\nD(f,x_{0})=\\operatorname{cl}\\,\\{\\,t(u-x_{0}): t\\geq 0,\\ f(u)\\leq f(x_{0})\\,\\},\n$$\nand the statistical dimension of a closed convex cone $C\\subset\\mathbb{R}^{n}$ is\n$$\n\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big],\n$$\nwhere $g\\sim\\mathcal{N}(0,I_{n})$ and $\\Pi_{C}$ denotes the Euclidean projection onto $C$. You may use the following well-tested facts without proof:\n- For any closed convex cone $C$, $\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$, where $C^{\\circ}$ is the polar cone.\n- For any proper convex function $f$, $D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$, where $\\partial f(x_{0})$ is the subdifferential.\n- For any nonempty compact set $K\\subset\\mathbb{R}^{n}$ that does not contain the origin, and any $y\\in\\mathbb{R}^{n}$, $\\operatorname{dist}(y,\\operatorname{cone}(K))=\\inf_{\\tau\\geq 0}\\operatorname{dist}(y,\\tau K)$.\n\nStarting from the definitions above and these facts, and using only fundamental properties of the $\\ell_{1}$ subdifferential and Gaussian expectation, derive an explicit one-dimensional variational representation for the statistical dimension $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ that depends only on $n$, $k$, and a scalar parameter, together with a one-dimensional expectation with respect to a standard normal random variable. You should express your final result as a single closed-form analytic expression in terms of $n$, $k$, an infimum over a nonnegative scalar parameter, and a one-dimensional expectation involving the positive part operator applied to the absolute value of a standard normal random variable. No numerical evaluation is required, and no rounding is needed. Provide the final expression as your answer.",
            "solution": "The problem asks for a one-dimensional variational representation for the statistical dimension $\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big)$ of the descent cone of the $\\ell_{1}$ norm at a point $x_{0}$. We are given the definition of the statistical dimension of a closed convex cone $C \\subset \\mathbb{R}^n$ as $\\delta(C)=\\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|_{2}^{2}\\big]$, where $g\\sim\\mathcal{N}(0,I_{n})$ and $\\Pi_{C}$ is the Euclidean projection onto $C$.\n\nOur derivation will proceed in several steps, starting from the given definitions and facts.\n\nLet $f(x) = \\|x\\|_{1}$. The descent cone is $C_0 = D(f,x_{0})$.\n\nStep 1: Relate the statistical dimension to the polar cone.\nThe first fact provided is that for any closed convex cone $C$, its statistical dimension can be expressed in terms of its polar cone $C^{\\circ}$ as $\\delta(C)=\\mathbb{E}[\\operatorname{dist}^{2}(g,C^{\\circ})]$.\nApplying this to the descent cone $C_0$, we have:\n$$\n\\delta(C_0) = \\mathbb{E}_{g \\sim \\mathcal{N}(0,I_n)}\\big[\\operatorname{dist}^{2}(g, C_0^{\\circ})\\big]\n$$\nwhere $\\operatorname{dist}(y, A) = \\inf_{a \\in A} \\|y-a\\|_2$ is the Euclidean distance from a point $y$ to a set $A$.\n\nStep 2: Characterize the polar cone of the descent cone.\nThe second fact states that for a proper convex function $f$, the polar of the descent cone is the conic hull of the subdifferential: $D(f,x_{0})^{\\circ}=\\operatorname{cone}(\\partial f(x_{0}))$.\nThe $\\ell_1$ norm $f(x) = \\|x\\|_1$ is a proper convex function. Therefore,\n$$\nC_0^{\\circ} = \\operatorname{cone}(\\partial \\|x_{0}\\|_{1})\n$$\n\nStep 3: Determine the subdifferential of the $\\ell_1$ norm.\nThe function is $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$. The subdifferential $\\partial f(x_0)$ is the set of vectors $v \\in \\mathbb{R}^n$ such that $\\|x\\|_1 \\ge \\|x_0\\|_1 + v^T(x-x_0)$ for all $x \\in \\mathbb{R}^n$. A standard characterization of the subdifferential for the $\\ell_1$ norm at $x_0$ is given by:\n$$\n\\partial \\|x_{0}\\|_{1} = \\{v \\in \\mathbb{R}^n : v_i = \\operatorname{sign}(x_{0,i}) \\text{ for } i \\in S, \\text{ and } v_i \\in [-1, 1] \\text{ for } i \\notin S\\}\n$$\nwhere $S = \\{i : x_{0,i} \\neq 0\\}$ is the support of $x_0$ with size $|S|=k$. Let's denote this compact convex set by $K$. Let $s_i = \\operatorname{sign}(x_{0,i})$ for $i \\in S$, where $s_i \\in \\{-1, +1\\}$.\n\nStep 4: Express the distance to the conic hull variationally.\nWe now have $C_0^{\\circ} = \\operatorname{cone}(K)$. The problem asks us to compute $\\mathbb{E}[\\operatorname{dist}^{2}(g, \\operatorname{cone}(K))]$.\nThe set $K$ is nonempty and compact. We assume $k = |S| \\ge 1$, which is strongly implied by the problem setup. If $k \\ge 1$, for any $v \\in K$, $\\|v\\|_2^2 = \\sum_{i \\in S} v_i^2 + \\sum_{i \\notin S} v_i^2 = \\sum_{i \\in S} s_i^2 + \\sum_{i \\notin S} v_i^2 = k + \\sum_{i \\notin S} v_i^2 \\ge k \\ge 1$. Thus, $K$ does not contain the origin. We can therefore apply the third fact:\n$$\n\\operatorname{dist}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}(g, \\tau K)\n$$\nwhere $\\tau K = \\{\\tau v : v \\in K\\}$. Squaring both sides gives:\n$$\n\\operatorname{dist}^{2}(g, \\operatorname{cone}(K)) = \\inf_{\\tau \\ge 0} \\operatorname{dist}^{2}(g, \\tau K) = \\inf_{\\tau \\ge 0} \\left( \\inf_{v \\in K} \\|g - \\tau v\\|_2^2 \\right)\n$$\n\nStep 5: Solve the inner minimization problem.\nFor a fixed $\\tau \\ge 0$, we need to compute $\\inf_{v \\in K} \\|g - \\tau v\\|_2^2$. The squared norm is separable over the coordinates:\n$$\n\\|g - \\tau v\\|_2^2 = \\sum_{i=1}^{n} (g_i - \\tau v_i)^2 = \\sum_{i \\in S} (g_i - \\tau v_i)^2 + \\sum_{i \\notin S} (g_i - \\tau v_i)^2\n$$\nWe minimize this term by term with respect to $v \\in K$.\nFor $i \\in S$, $v_i$ is fixed to $s_i$. The term is $(g_i - \\tau s_i)^2$.\nFor $i \\notin S$, we must minimize $(g_i - \\tau v_i)^2$ over $v_i \\in [-1, 1]$. This is equivalent to finding the point in the interval $[-\\tau, \\tau]$ that is closest to $g_i$. The minimum squared distance is given by $(|g_i|-\\tau)_+^2$, where $(x)_+ = \\max(0,x)$ is the positive part operator.\nCombining these results, we get:\n$$\n\\operatorname{dist}^{2}(g, \\tau K) = \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2\n$$\nPlugging this back into the expression for the distance to the cone:\n$$\n\\operatorname{dist}^{2}(g, C_0^{\\circ}) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\}\n$$\n\nStep 6: Compute the expectation.\nThe statistical dimension is the expectation of the above quantity.\n$$\n\\delta(C_0) = \\mathbb{E}_{g} \\left[ \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right\\} \\right]\n$$\nTo obtain a one-dimensional variational representation, we exchange the expectation and the infimum. This is a standard step in such derivations, justified by concentration of measure phenomena for high-dimensional Gaussian vectors.\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\mathbb{E}_{g} \\left[ \\sum_{i \\in S} (g_i - \\tau s_i)^2 + \\sum_{i \\notin S} (|g_i| - \\tau)_+^2 \\right]\n$$\nBy linearity of expectation, we can write this as:\n$$\n\\delta(C_0) = \\inf_{\\tau \\ge 0} \\left\\{ \\sum_{i \\in S} \\mathbb{E}_{g_i}[(g_i - \\tau s_i)^2] + \\sum_{i \\notin S} \\mathbb{E}_{g_i}[(|g_i| - \\tau)_+^2] \\right\\}\n$$\nThe components $g_i$ of $g$ are independent and identically distributed as $g_i \\sim \\mathcal{N}(0,1)$.\n\nLet's evaluate the expectations. For $i \\in S$, $s_i^2 = (\\pm 1)^2 = 1$. Since $\\mathbb{E}[g_i] = 0$ and $\\mathbb{E}[g_i^2] = 1$:\n$$\n\\mathbb{E}[(g_i - \\tau s_i)^2] = \\mathbb{E}[g_i^2 - 2\\tau s_i g_i + \\tau^2 s_i^2] = \\mathbb{E}[g_i^2] - 2\\tau s_i \\mathbb{E}[g_i] + \\tau^2 s_i^2 = 1 - 0 + \\tau^2(1) = 1 + \\tau^2\n$$\nSince there are $|S|=k$ such terms, their sum is $k(1 + \\tau^2)$.\n\nFor $i \\notin S$, the terms are identical. Let $Z \\sim \\mathcal{N}(0,1)$. We have $(n-k)$ terms of the form $\\mathbb{E}[(|Z|-\\tau)_+^2]$. The problem asks for the result in terms of this one-dimensional expectation.\n\nStep 7: Final Assembly.\nCombining the computed expectations, we obtain the final expression for the statistical dimension:\n$$\n\\delta\\big(D(\\|\\cdot\\|_{1},x_{0})\\big) = \\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} [(|Z|-\\tau)_+^2] \\right\\}\n$$\nThis is the required one-dimensional variational representation, expressed in terms of $n$, $k$, a scalar parameter $\\tau$, and a one-dimensional expectation.\nThe expression uses the positive part operator $(x)_+ = \\max(0,x)$ applied to the absolute value of a standard normal random variable, as requested.",
            "answer": "$$\n\\boxed{\\inf_{\\tau \\ge 0} \\left\\{ k(1+\\tau^2) + (n-k) \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)} \\left[\\left(\\max\\left(0, |Z|-\\tau\\right)\\right)^2\\right] \\right\\}}\n$$"
        },
        {
            "introduction": "While the geometric approach explains why recovery succeeds when the number of measurements $m$ is sufficiently large, it is equally crucial to understand precisely why it fails when $m$ is too small. This practice shifts the perspective from probabilistic geometry to a deterministic, algebraic condition known as the Null Space Property (NSP). The NSP provides a necessary and sufficient condition for a sensing matrix $A$ to guarantee successful recovery for all sparse signals. By constructing an explicit vector that violates the NSP for a simple underdetermined system (), you will gain a concrete understanding of the failure mechanism of $\\ell_1$ minimization, demonstrating how an insufficient number of measurements creates \"bad\" directions in the null space that can fool the recovery algorithm.",
            "id": "3466208",
            "problem": "Let $n \\in \\mathbb{N}$ and $k \\in \\mathbb{N}$ satisfy $n \\geq 3$ and $2 \\leq k  n$. Consider the linear measurement map $A \\in \\mathbb{R}^{1 \\times n}$ defined by the single-row sensing rule $A x = \\sum_{i=1}^{n} x_{i}$. In compressed sensing, a sensing matrix $A$ is said to satisfy the Null Space Property (NSP) of order $k$ if, for every nonzero vector $h \\in \\ker A$ and every index set $S \\subset \\{1,\\dots,n\\}$ with $|S| \\leq k$, one has $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$. The $\\ell_{1}$ ball in $\\mathbb{R}^{n}$ has faces corresponding to coordinate supports and sign patterns; the $k$-dimensional face associated with a fixed $S$ and all-positive sign pattern is spanned by vectors supported on $S$ with strictly positive coordinates on $S$ and zeros on $S^{c}$.\n  \nFix any set $S \\subset \\{1,\\dots,n\\}$ with $|S| = k$. Construct a nonzero vector $h \\in \\ker A$ that is aligned with the face of the $\\ell_{1}$ ball corresponding to $S$ in the sense that $h_{i}  0$ for all $i \\in S$, and $h$ has at most one nonzero coordinate in $S^{c}$. From fundamental definitions alone, verify that this construction is always possible when $m = 1  k$ and $n \\geq 3$, and then compute the exact value of the ratio\n$$\nR(k) \\triangleq \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}.\n$$\nExpress your final answer as a single exact real number or a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem requires the construction of a specific vector $h \\in \\mathbb{R}^{n}$ subject to a set of conditions, and then the computation of a ratio of its $\\ell_1$ norms on complementary index sets.\n\nFirst, we validate the problem statement. The problem is situated within the standard mathematical discipline of compressed sensing and linear algebra. The provided definitions and parameters are self-contained, unambiguous, and mathematically sound.\nThe parameters are given as natural numbers $n$ and $k$ with $n \\geq 3$ and $2 \\leq k  n$. The linear measurement map is $A \\in \\mathbb{R}^{1 \\times n}$, defined by its action on a vector $x \\in \\mathbb{R}^{n}$ as $A x = \\sum_{i=1}^{n} x_{i}$. This defines $A$ as a row vector with all entries equal to $1$. The null space of $A$ is the set of vectors whose components sum to zero: $\\ker A = \\{h \\in \\mathbb{R}^{n} \\mid \\sum_{i=1}^{n} h_{i} = 0\\}$. The problem statement provides a correct definition for the Null Space Property (NSP).\nThe task is to fix an arbitrary index set $S \\subset \\{1, \\dots, n\\}$ with $|S|=k$ and construct a nonzero vector $h \\in \\ker A$ satisfying two further properties:\n1. $h_{i}  0$ for all $i \\in S$.\n2. The subvector $h_{S^c}$ (the components of $h$ with indices in the complement set $S^c$) has at most one nonzero entry.\nThe problem requires us to verify that such a construction is always possible under the given constraints and then to compute the ratio $R(k) = \\|h_{S}\\|_{1} / \\|h_{S^{c}}\\|_{1}$.\nThe problem is well-posed, objective, and scientifically grounded. There are no contradictions or missing information. The problem is valid.\n\nWe proceed with the solution. Let $S \\subset \\{1, \\dots, n\\}$ be an arbitrary index set with cardinality $|S| = k$. We shall construct a vector $h \\in \\mathbb{R}^{n}$ that satisfies all the stated conditions.\n\nTo satisfy the first property, $h_i  0$ for all $i \\in S$, let us set the components of $h$ for these indices to be $h_i = a_i$, where each $a_i$ is an arbitrary positive real number ($a_i  0$).\n\nTo satisfy the second property, that $h$ has at most one nonzero coordinate in $S^c$, we first note that the complement set $S^c = \\{1, \\dots, n\\} \\setminus S$ has cardinality $|S^c| = n-k$. Since it is given that $k  n$, it follows that $n-k \\ge 1$, and thus $S^c$ is non-empty. We can therefore choose an index $j_0 \\in S^c$. Let us set the component $h_{j_0} = \\alpha$ for some real number $\\alpha$, and set all other components in $S^c$ to zero, i.e., $h_j = 0$ for all $j \\in S^c \\setminus \\{j_0\\}$.\n\nTo satisfy the condition that $h \\in \\ker A$, the sum of all components of $h$ must be zero:\n$$\n\\sum_{i=1}^{n} h_{i} = 0\n$$\nWe can partition this sum over the sets $S$ and $S^c$:\n$$\n\\sum_{i \\in S} h_{i} + \\sum_{j \\in S^{c}} h_{j} = 0\n$$\nSubstituting the components from our construction:\n$$\n\\left( \\sum_{i \\in S} a_{i} \\right) + \\left( h_{j_0} + \\sum_{j \\in S^{c}, j \\neq j_0} h_j \\right) = 0\n$$\n$$\n\\left( \\sum_{i \\in S} a_{i} \\right) + ( \\alpha + 0 ) = 0\n$$\nThis equation determines the value of $\\alpha$:\n$$\n\\alpha = - \\sum_{i \\in S} a_{i}\n$$\nSince $k \\ge 2$, there are at least two indices in $S$. As each $a_i  0$, their sum $\\sum_{i \\in S} a_{i}$ is strictly positive. Consequently, $\\alpha$ is strictly negative and thus nonzero. This confirms that our vector $h$ has exactly one nonzero coordinate in $S^c$ and is a nonzero vector overall. The construction is always possible for any choice of $S$ (with $|S|=k$), any choice of $j_0 \\in S^c$, and any choice of positive values $a_i$.\n\nNext, we compute the required ratio $R(k) = \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}$.\nThe $\\ell_1$-norm of the subvector $h_S$ is the sum of the absolute values of its components:\n$$\n\\|h_{S}\\|_{1} = \\sum_{i \\in S} |h_{i}|\n$$\nAccording to our construction, $h_i = a_i  0$ for all $i \\in S$. Thus, $|h_i| = a_i$.\n$$\n\\|h_{S}\\|_{1} = \\sum_{i \\in S} a_{i}\n$$\nThe $\\ell_1$-norm of the subvector $h_{S^c}$ is:\n$$\n\\|h_{S^{c}}\\|_{1} = \\sum_{j \\in S^{c}} |h_{j}|\n$$\nBy construction, only one component in this subvector is nonzero, $h_{j_0} = \\alpha$.\n$$\n\\|h_{S^{c}}\\|_{1} = |h_{j_{0}}| + \\sum_{j \\in S^{c}, j \\neq j_{0}} |0| = |\\alpha|\n$$\nSubstituting the expression for $\\alpha$:\n$$\n\\|h_{S^{c}}\\|_{1} = \\left| - \\sum_{i \\in S} a_{i} \\right|\n$$\nSince the sum of positive numbers $\\sum_{i \\in S} a_{i}$ is positive, its absolute value is itself.\n$$\n\\|h_{S^{c}}\\|_{1} = \\sum_{i \\in S} a_{i}\n$$\nFinally, we compute the ratio $R(k)$:\n$$\nR(k) = \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}} = \\frac{\\sum_{i \\in S} a_{i}}{\\sum_{i \\in S} a_{i}}\n$$\nSince the sum $\\sum_{i \\in S} a_{i}$ is strictly positive, the ratio is well-defined and simplifies to:\n$$\nR(k) = 1\n$$\nThis result is independent of the particular choice of the set $S$, the index $j_0 \\in S^c$, the positive values $a_i$, and the parameters $n$ and $k$ (within their specified ranges). The requested value is therefore $1$.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}