{
    "hands_on_practices": [
        {
            "introduction": "While the $\\ell_1$-norm promotes sparsity at the level of individual coefficients, many applications require structured sparsity, such as selecting entire groups of variables together. This exercise guides you through constructing the widely-used group lasso norm from the fundamental definition of a gauge function applied to a carefully chosen atomic set . By deriving the norm and its dual from first principles, you will gain a deep geometric intuition for how complex regularizers are built and analyzed.",
            "id": "3452404",
            "problem": "Let $n \\in \\mathbb{N}$, let $\\{G_j\\}_{j=1}^{m}$ be a partition of $\\{1,\\dots,n\\}$, and let $w_j0$ for all $j \\in \\{1,\\dots,m\\}$. Consider the function on $\\mathbb{R}^n$ given by $x \\mapsto \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$, where $x_{G_j} \\in \\mathbb{R}^n$ denotes the vector obtained by keeping the coordinates of $x$ in $G_j$ and setting all other coordinates to $0$. Work entirely from the foundational definitions of the gauge function of a convex set, the support function, and the indicator function. In particular, use only the following as starting points:\n\n- The gauge (Minkowski functional) of a nonempty, closed, convex, balanced, and absorbing set $C \\subset \\mathbb{R}^n$ is $\\gamma_C(x) \\coloneqq \\inf\\{t0 : x \\in t C\\}$.\n- The indicator function of a set $C$ is $\\delta_C(x) \\coloneqq 0$ if $x \\in C$ and $\\delta_C(x) \\coloneqq +\\infty$ otherwise.\n- The support function of a set $C$ is $\\sigma_C(y) \\coloneqq \\sup\\{\\langle y,x \\rangle : x \\in C\\}$, which equals the Fenchel conjugate of the indicator function of $C$.\n\nDefine, for each $j \\in \\{1,\\dots,m\\}$, the atomic set $A_j \\coloneqq \\{a \\in \\mathbb{R}^n : \\mathrm{supp}(a) \\subseteq G_j,\\ \\|a\\|_2 = 1/w_j\\}$ and the union $A \\coloneqq \\bigcup_{j=1}^{m} A_j$. Let $B \\coloneqq \\mathrm{conv}(A)$ denote the convex hull of $A$. Starting from the above definitions and properties of the Euclidean inner product and norm, do the following:\n\n- Prove that the gauge $\\gamma_B(x)$ equals $\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$ for all $x \\in \\mathbb{R}^n$.\n- Using only the fact that the support function of a convex hull equals the supremum of supports over the generating set and the definition of the dual norm as the support function of the unit ball, derive the dual norm of $x \\mapsto \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$ in a closed-form analytic expression.\n\nFinally, evaluate the dual norm you derived at the specific point $y \\in \\mathbb{R}^7$ with partition and weights given by $G_1=\\{1,2\\}$, $G_2=\\{3,4,5\\}$, $G_3=\\{6,7\\}$, $w_1=2$, $w_2=3$, $w_3=7$, and \n$$\ny=\\big(3,4,2,-1,2,5,12\\big).\n$$\nYour final answer must be the single real number equal to the value of the dual norm at this $y$. Do not round your answer.",
            "solution": "We begin from the definitions of gauge and support functions and standard properties of the Euclidean inner product and norm. We define the atomic sets $A_j \\coloneqq \\{a \\in \\mathbb{R}^n : \\mathrm{supp}(a) \\subseteq G_j,\\ \\|a\\|_2 = 1/w_j\\}$ for each $j \\in \\{1,\\dots,m\\}$ and let $A \\coloneqq \\bigcup_{j=1}^{m} A_j$. Let $B \\coloneqq \\mathrm{conv}(A)$ denote the convex hull of $A$. The gauge of $B$ is $\\gamma_B(x) \\coloneqq \\inf\\{t0 : x \\in t B\\}$. It is a standard fact in convex analysis that the gauge of the convex hull of a centrally symmetric atomic set $A$ coincides with the so-called atomic norm induced by $A$. Concretely, by the definition of gauge of a convex hull, we can represent \n$$\n\\gamma_B(x) \\;=\\; \\inf\\Big\\{\\sum_{i=1}^{N} c_i \\;:\\; N \\in \\mathbb{N},\\ c_i \\ge 0,\\ a_i \\in A,\\ x = \\sum_{i=1}^{N} c_i a_i \\Big\\}.\n$$\nWe will show that this gauge equals $\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$.\n\nTo prove the upper bound, fix $x \\in \\mathbb{R}^n$ and for each $j \\in \\{1,\\dots,m\\}$ consider the group component $x_{G_j}$. If $x_{G_j} \\neq 0$, define $a_j \\in A_j$ by $a_j \\coloneqq \\frac{x_{G_j}}{\\|x_{G_j}\\|_2} \\cdot \\frac{1}{w_j}$. Note that $\\mathrm{supp}(a_j) \\subseteq G_j$ and $\\|a_j\\|_2 = 1/w_j$, hence $a_j \\in A_j \\subset A$. Let $c_j \\coloneqq w_j \\|x_{G_j}\\|_2$. Then $c_j a_j = x_{G_j}$ for each such $j$. If $x_{G_j} = 0$, we set $c_j = 0$ and choose any $a_j \\in A_j$ arbitrarily (the choice does not contribute to the sum). Summing over $j$ yields\n$$\nx \\;=\\; \\sum_{j=1}^{m} x_{G_j} \\;=\\; \\sum_{j=1}^{m} c_j a_j,\n$$\nwith $\\sum_{j=1}^{m} c_j = \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$. By the definition of $\\gamma_B(x)$ as the infimum of such sums of coefficients, we obtain\n$$\n\\gamma_B(x) \\;\\le\\; \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2.\n$$\n\nTo prove the lower bound, consider any representation $x = \\sum_{i=1}^{N} c_i a_i$ with $c_i \\ge 0$ and $a_i \\in A$. Partition the index set $\\{1,\\dots,N\\}$ according to group membership by defining $I_j \\coloneqq \\{i \\in \\{1,\\dots,N\\} : a_i \\in A_j\\}$. Due to disjoint supports of different groups, projecting onto coordinates $G_j$ yields\n$$\nx_{G_j} \\;=\\; \\sum_{i \\in I_j} c_i a_i,\n$$\nwith each $a_i$ supported in $G_j$. Applying the triangle inequality for the Euclidean norm and the fact that $\\|a_i\\|_2 = 1/w_j$ for $i \\in I_j$, we obtain\n$$\n\\|x_{G_j}\\|_2 \\;\\le\\; \\sum_{i \\in I_j} c_i \\|a_i\\|_2 \\;=\\; \\sum_{i \\in I_j} c_i \\cdot \\frac{1}{w_j}.\n$$\nMultiplying both sides by $w_j$ and summing over $j$ yields\n$$\n\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2 \\;\\le\\; \\sum_{j=1}^{m} \\sum_{i \\in I_j} c_i \\;=\\; \\sum_{i=1}^{N} c_i.\n$$\nSince this inequality holds for every decomposition $x = \\sum_{i=1}^{N} c_i a_i$ with $a_i \\in A$ and $c_i \\ge 0$, taking the infimum over all such representations implies\n$$\n\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2 \\;\\le\\; \\gamma_B(x).\n$$\nCombining the upper and lower bounds proves that\n$$\n\\gamma_B(x) \\;=\\; \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2 \\quad \\text{for all } x \\in \\mathbb{R}^n.\n$$\n\nWe now derive the dual norm. By definition, the dual norm of a norm with unit ball $B$ is\n$$\n\\|y\\|_{*} \\;=\\; \\sup\\{\\langle y, x \\rangle : x \\in B\\} \\;=\\; \\sigma_{B}(y),\n$$\nthat is, the support function of the unit ball $B$. Since $B = \\mathrm{conv}(A)$, the support function satisfies\n$$\n\\sigma_{B}(y) \\;=\\; \\sup\\{\\langle y, x \\rangle : x \\in \\mathrm{conv}(A)\\} \\;=\\; \\sup\\{\\langle y, a \\rangle : a \\in A\\},\n$$\nusing the fact that the support function of a convex hull equals the supremum of inner products over the generating set. Because $A = \\bigcup_{j=1}^{m} A_j$, we have\n$$\n\\sigma_{B}(y) \\;=\\; \\sup_{j \\in \\{1,\\dots,m\\}} \\ \\sup\\{\\langle y, a \\rangle : a \\in A_j\\}.\n$$\nFor a fixed $j$, the set $A_j$ is the Euclidean sphere of radius $1/w_j$ in the subspace supported on $G_j$. Therefore, by the Cauchy–Schwarz inequality and the characterization of equality, we obtain\n$$\n\\sup\\{\\langle y, a \\rangle : a \\in A_j\\} \\;=\\; \\frac{1}{w_j} \\|y_{G_j}\\|_2.\n$$\nTaking the supremum over $j$ yields\n$$\n\\|y\\|_{*} \\;=\\; \\sigma_{B}(y) \\;=\\; \\max_{j \\in \\{1,\\dots,m\\}} \\frac{\\|y_{G_j}\\|_2}{w_j}.\n$$\nThus, the dual norm of $x \\mapsto \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$ is $y \\mapsto \\max_{j} \\|y_{G_j}\\|_2/w_j$.\n\nFinally, we evaluate this dual norm at the specified point. We are given $n=7$, $G_1=\\{1,2\\}$, $G_2=\\{3,4,5\\}$, $G_3=\\{6,7\\}$, $w_1=2$, $w_2=3$, $w_3=7$, and \n$$\ny=\\big(3,4,2,-1,2,5,12\\big).\n$$\nWe compute the groupwise Euclidean norms:\n$$\n\\|y_{G_1}\\|_2 \\;=\\; \\sqrt{3^2 + 4^2} \\;=\\; \\sqrt{9+16} \\;=\\; 5,\n$$\n$$\n\\|y_{G_2}\\|_2 \\;=\\; \\sqrt{2^2 + (-1)^2 + 2^2} \\;=\\; \\sqrt{4+1+4} \\;=\\; 3,\n$$\n$$\n\\|y_{G_3}\\|_2 \\;=\\; \\sqrt{5^2 + 12^2} \\;=\\; \\sqrt{25+144} \\;=\\; 13.\n$$\nDivide by the corresponding weights:\n$$\n\\frac{\\|y_{G_1}\\|_2}{w_1} \\;=\\; \\frac{5}{2}, \\quad \\frac{\\|y_{G_2}\\|_2}{w_2} \\;=\\; \\frac{3}{3} \\;=\\; 1, \\quad \\frac{\\|y_{G_3}\\|_2}{w_3} \\;=\\; \\frac{13}{7}.\n$$\nTaking the maximum gives\n$$\n\\|y\\|_{*} \\;=\\; \\max\\Big\\{\\frac{5}{2},\\, 1,\\, \\frac{13}{7}\\Big\\} \\;=\\; \\frac{5}{2}.\n$$\nTherefore, the value of the dual norm at the specified $y$ is $\\frac{5}{2}$.",
            "answer": "$$\\boxed{\\frac{5}{2}}$$"
        },
        {
            "introduction": "Proximal operators are fundamental building blocks in algorithms for solving sparse optimization problems, but deriving them can be complex. This practice demonstrates the power of Fenchel duality, specifically through the Moreau decomposition, to find an elegant expression for the proximal operator of any support function . You will see how this abstract duality theorem translates into a concrete and powerful identity connecting the proximal operator to the geometric operation of Euclidean projection.",
            "id": "3452391",
            "problem": "Let $C \\subset \\mathbb{R}^{n}$ be a nonempty set, and let its support function be defined by $\\sigma_{C}(x) \\triangleq \\sup_{c \\in C} \\langle x, c \\rangle$. Let $\\delta_{S}$ denote the indicator function of a set $S$, equal to $0$ on $S$ and $+\\infty$ otherwise. For a proper, lower semicontinuous, convex function $f$, recall the convex conjugate $f^{\\ast}(y) \\triangleq \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle x, y \\rangle - f(x)\\}$ and the proximal mapping $\\mathrm{prox}_{\\lambda f}(x) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ f(u) + \\frac{1}{2 \\lambda} \\|u - x\\|_{2}^{2} \\right\\}$ for any $\\lambda  0$. The Moreau decomposition states that for any such $f$ and any $\\lambda  0$, one has $\\mathrm{prox}_{\\lambda f}(x) + \\lambda\\, \\mathrm{prox}_{f^{\\ast}/\\lambda}(x/\\lambda) = x$, where $f^{\\ast}/\\lambda$ denotes the function $y \\mapsto \\frac{1}{\\lambda} f^{\\ast}(y)$. Let $\\mathrm{cl\\,conv}\\,C$ denote the closed convex hull of $C$, and let $P_{S}$ denote the Euclidean projector onto a nonempty closed convex set $S$, i.e., $P_{S}(z) \\triangleq \\arg\\min_{y \\in S} \\|y - z\\|_{2}^{2}$.\n\nTask:\n- Starting only from the above definitions and the Moreau decomposition, derive a closed-form expression for $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$ in terms of the Euclidean projection onto $\\mathrm{cl\\,conv}\\,C$. Your final result must be a single analytic expression in $x$, $\\lambda$, and $C$.\n- Then verify the derived expression for the special case where $C$ is polyhedral by using Karush–Kuhn–Tucker (KKT) conditions. Concretely, take $C = \\{c_{1}, \\dots, c_{m}\\}$ with $c_{i} \\in \\mathbb{R}^{n}$, so that $\\mathrm{cl\\,conv}\\,C = \\mathrm{conv}\\{c_{1}, \\dots, c_{m}\\}$, and confirm via KKT analysis that your expression holds by recasting the proximal problem as an equivalent quadratic program over the probability simplex in barycentric coordinates.\n\nYour final answer must be the single closed-form expression for $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$. No numerical rounding is required.",
            "solution": "The task is to derive a closed-form expression for the proximal mapping of a scaled support function, $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$, and then to verify this expression in a specific polyhedral case using Karush–Kuhn–Tucker (KKT) conditions. The derivation will proceed from the provided definitions and the Moreau decomposition.\n\nFirst, we address the derivation of $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$. The problem provides the Moreau decomposition for a proper, lower semicontinuous, convex function $f$:\n$$ \\mathrm{prox}_{\\lambda f}(x) + \\lambda\\, \\mathrm{prox}_{f^{\\ast}/\\lambda}(x/\\lambda) = x $$\nwhere $f^{\\ast}$ is the convex conjugate of $f$, and $f^{\\ast}/\\lambda$ is the function $y \\mapsto \\frac{1}{\\lambda} f^{\\ast}(y)$. We can rearrange this identity to express the desired proximal map:\n$$ \\mathrm{prox}_{\\lambda f}(x) = x - \\lambda\\, \\mathrm{prox}_{f^{\\ast}/\\lambda}(x/\\lambda) $$\nWe apply this formula by setting $f(x) = \\sigma_{C}(x) = \\sup_{c \\in C} \\langle x, c \\rangle$. The support function $\\sigma_{C}$ is the supremum of a collection of linear (and thus convex and continuous) functions, so it is convex and lower semicontinuous. As $C$ is nonempty, $\\sigma_{C}(0) = 0$, so $\\sigma_{C}$ is a proper function. Thus, the Moreau decomposition is applicable.\n\nThe first step is to compute the convex conjugate of $\\sigma_{C}$, denoted $(\\sigma_{C})^{\\ast}$. It is a standard result in convex analysis that the conjugate of the support function of a set $C$ is the indicator function of the closed convex hull of $C$, denoted $\\mathrm{cl\\,conv}\\,C$. Let us derive this.\nThe conjugate is defined as:\n$$ (\\sigma_{C})^{\\ast}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle x, y \\rangle - \\sigma_{C}(x)\\} $$\nThe support function of a set $C$ is identical to the support function of its closed convex hull, i.e., $\\sigma_{C}(x) = \\sigma_{\\mathrm{cl\\,conv}\\,C}(x)$. Let $K = \\mathrm{cl\\,conv}\\,C$. Then\n$$ (\\sigma_{C})^{\\ast}(y) = (\\sigma_{K})^{\\ast}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle x, y \\rangle - \\sigma_{K}(x)\\} $$\nCase 1: $y \\in K$. By the definition of the support function, for any $x \\in \\mathbb{R}^{n}$, we have $\\sigma_{K}(x) = \\sup_{z \\in K} \\langle x, z \\rangle \\ge \\langle x, y \\rangle$. Therefore, $\\langle x, y \\rangle - \\sigma_{K}(x) \\le 0$. The supremum is attained at $x=0$, yielding a value of $0$. Thus, if $y \\in K$, $(\\sigma_{C})^{\\ast}(y) = 0$.\n\nCase 2: $y \\notin K$. Since $K$ is a nonempty closed convex set, by the separating hyperplane theorem, there exists a vector $x_0 \\in \\mathbb{R}^{n}$ and a scalar $\\alpha \\in \\mathbb{R}$ such that $\\langle x_0, y \\rangle  \\alpha$ and $\\langle x_0, z \\rangle \\le \\alpha$ for all $z \\in K$. Taking the supremum over $z \\in K$ gives $\\sigma_{K}(x_0) \\le \\alpha  \\langle x_0, y \\rangle$. Let $\\delta = \\langle x_0, y \\rangle - \\sigma_{K}(x_0)  0$. Now consider the expression for the conjugate with $x = tx_0$ for $t  0$:\n$$ \\langle tx_0, y \\rangle - \\sigma_{K}(tx_0) = t \\langle x_0, y \\rangle - t \\sigma_{K}(x_0) = t(\\langle x_0, y \\rangle - \\sigma_{K}(x_0)) = t\\delta $$\nAs $t \\to +\\infty$, this quantity tends to $+\\infty$. Therefore, the supremum over all $x$ is $+\\infty$. Thus, if $y \\notin K$, $(\\sigma_{C})^{\\ast}(y) = +\\infty$.\n\nCombining these two cases, we see that $(\\sigma_{C})^{\\ast}(y)$ is $0$ if $y \\in \\mathrm{cl\\,conv}\\,C$ and $+\\infty$ otherwise. This is precisely the definition of the indicator function $\\delta_{\\mathrm{cl\\,conv}\\,C}(y)$. So, we have established:\n$$ (\\sigma_{C})^{\\ast}(y) = \\delta_{\\mathrm{cl\\,conv}\\,C}(y) $$\nNow, we must evaluate the term $\\mathrm{prox}_{(\\sigma_{C})^{\\ast}/\\lambda}(x/\\lambda)$. Let $g(u) = (\\sigma_{C})^{\\ast}(u)/\\lambda = \\frac{1}{\\lambda}\\delta_{\\mathrm{cl\\,conv}\\,C}(u)$. The proximal operator $\\mathrm{prox}_{g}$ (with parameter $1$) is defined as:\n$$ \\mathrm{prox}_{g}(z) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ g(u) + \\frac{1}{2} \\|u - z\\|_{2}^{2} \\right\\} $$\nSubstituting $g(u)$ and setting $z = x/\\lambda$, we get:\n$$ \\mathrm{prox}_{(\\sigma_{C})^{\\ast}/\\lambda}(x/\\lambda) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{\\lambda}\\delta_{\\mathrm{cl\\,conv}\\,C}(u) + \\frac{1}{2} \\left\\|u - \\frac{x}{\\lambda}\\right\\|_{2}^{2} \\right\\} $$\nThe term involving the indicator function is $0$ if $u \\in \\mathrm{cl\\,conv}\\,C$ and $+\\infty$ otherwise. The minimization problem is therefore equivalent to minimizing the quadratic term over the set where the indicator function is finite:\n$$ \\arg\\min_{u \\in \\mathrm{cl\\,conv}\\,C} \\left\\{ \\frac{1}{2} \\left\\|u - \\frac{x}{\\lambda}\\right\\|_{2}^{2} \\right\\} $$\nThis is the definition of the Euclidean projection of the point $x/\\lambda$ onto the closed convex set $\\mathrm{cl\\,conv}\\,C$, which is denoted by $P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda)$.\n$$ \\mathrm{prox}_{(\\sigma_{C})^{\\ast}/\\lambda}(x/\\lambda) = P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda) $$\nSubstituting this result back into the rearranged Moreau identity, we obtain the final expression:\n$$ \\mathrm{prox}_{\\lambda \\sigma_{C}}(x) = x - \\lambda P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda) $$\n\nNext, we verify this expression for the special case where $C$ is a finite set of points, $C = \\{c_1, \\dots, c_m\\}$. In this case, $\\mathrm{cl\\,conv}\\,C$ is the polytope $\\mathrm{conv}\\{c_1, \\dots, c_m\\}$. The support function is $\\sigma_{C}(u) = \\max_{i=1,\\dots,m} \\langle c_i, u \\rangle$.\nThe proximal problem is to find $u^{\\ast} = \\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$, which is the solution to:\n$$ \\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\lambda \\sigma_{C}(u) + \\frac{1}{2} \\|u-x\\|_{2}^{2} \\right\\} = \\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\lambda \\max_{i=1,\\dots,m} \\langle c_i, u \\rangle + \\frac{1}{2} \\|u-x\\|_{2}^{2} \\right\\} $$\nThis can be reformulated as a constrained optimization problem by introducing an auxiliary variable $t \\in \\mathbb{R}$:\n$$ \\min_{u \\in \\mathbb{R}^{n}, t \\in \\mathbb{R}} \\quad \\lambda t + \\frac{1}{2} \\|u-x\\|_{2}^{2} \\quad \\text{subject to} \\quad \\langle c_i, u \\rangle \\le t \\quad \\forall i \\in \\{1,\\dots,m\\} $$\nThis is a convex quadratic program. We form the Lagrangian with multipliers $\\mu_i \\ge 0$:\n$$ L(u, t, \\mu) = \\lambda t + \\frac{1}{2} \\|u-x\\|_{2}^{2} + \\sum_{i=1}^{m} \\mu_i (\\langle c_i, u \\rangle - t) $$\nThe KKT optimality conditions are:\n1. Primal feasibility: $\\langle c_i, u \\rangle \\le t$ for all $i$.\n2. Dual feasibility: $\\mu_i \\ge 0$ for all $i$.\n3. Complementary slackness: $\\mu_i(\\langle c_i, u \\rangle - t) = 0$ for all $i$.\n4. Stationarity:\n   a) $\\nabla_u L = u - x + \\sum_{i=1}^{m} \\mu_i c_i = 0 \\implies u = x - \\sum_{i=1}^{m} \\mu_i c_i$\n   b) $\\nabla_t L = \\lambda - \\sum_{i=1}^{m} \\mu_i = 0 \\implies \\sum_{i=1}^{m} \\mu_i = \\lambda$\n\nFrom the stationarity conditions, we can express the optimal solution $u$ (which we denote $u^{\\ast}$) as $u^{\\ast} = x - \\sum_{i=1}^m \\mu_i c_i$. Let us define a new vector of variables $\\beta = (\\beta_1, \\dots, \\beta_m)^T$ by $\\beta_i = \\mu_i / \\lambda$. The conditions on $\\mu$ imply that $\\beta_i \\ge 0$ and $\\sum_{i=1}^m \\beta_i = 1$. This means $\\beta$ lies in the standard probability simplex. Let $v = \\sum_{i=1}^m \\beta_i c_i$. By definition, $v$ is a vector in $\\mathrm{conv}\\{c_1, \\dots, c_m\\}$.\nThe expression for $u^{\\ast}$ becomes:\n$$ u^{\\ast} = x - \\lambda \\left(\\sum_{i=1}^m \\frac{\\mu_i}{\\lambda} c_i\\right) = x - \\lambda \\left(\\sum_{i=1}^m \\beta_i c_i\\right) = x - \\lambda v $$\nNow we must identify the specific vector $v \\in \\mathrm{conv}\\{C\\}$. From complementary slackness, if $\\mu_i  0$ (and hence $\\beta_i  0$), we must have $\\langle c_i, u^{\\ast} \\rangle = t$. If $\\mu_i = 0$, the constraint is simply $\\langle c_i, u^{\\ast} \\rangle \\le t$. This means that $t = \\max_{j} \\langle c_j, u^{\\ast} \\rangle$ and this maximum is achieved for all indices $i$ for which $\\beta_i  0$.\n\nLet's substitute $u^{\\ast} = x - \\lambda v$.\nFor any $j \\in \\{1,\\dots,m\\}$, we have $\\langle c_j, x - \\lambda v \\rangle \\le t$.\nThis is equivalent to $\\langle c_j, x/\\lambda - v \\rangle \\le t/\\lambda$.\nIf $\\beta_i  0$, we have equality: $\\langle c_i, x/\\lambda - v \\rangle = t/\\lambda$.\nNow, consider the inner product $\\langle x/\\lambda - v, v \\rangle$:\n$$ \\langle x/\\lambda - v, v \\rangle = \\left\\langle x/\\lambda - v, \\sum_i \\beta_i c_i \\right\\rangle = \\sum_i \\beta_i \\langle x/\\lambda - v, c_i \\rangle $$\nSince $\\beta_i=0$ for the terms where the inner product is not maximal, we can write the sum over indices where $\\beta_i0$, for which we have equality $\\langle c_i, x/\\lambda - v \\rangle = t/\\lambda$.\n$$ \\sum_{i:\\beta_i0} \\beta_i (t/\\lambda) = (t/\\lambda) \\sum_{i:\\beta_i0} \\beta_i = (t/\\lambda) \\sum_i \\beta_i = t/\\lambda $$\nSo, we have $\\langle x/\\lambda - v, v \\rangle = t/\\lambda$. Combining our findings, we have for any $j \\in \\{1, \\dots, m\\}$:\n$$ \\langle c_j, x/\\lambda - v \\rangle \\le t/\\lambda = \\langle v, x/\\lambda - v \\rangle $$\nRearranging this inequality gives $\\langle c_j - v, x/\\lambda - v \\rangle \\le 0$.\nSince any element $y \\in \\mathrm{conv}\\{C\\}$ can be written as a convex combination $y = \\sum_j \\alpha_j c_j$ with $\\alpha_j \\ge 0, \\sum_j \\alpha_j = 1$, we have:\n$$ \\langle y - v, x/\\lambda - v \\rangle = \\left\\langle \\sum_j \\alpha_j c_j - v, x/\\lambda - v \\right\\rangle = \\sum_j \\alpha_j \\langle c_j - v, x/\\lambda - v \\rangle \\le 0 $$\nThe condition $\\langle y - v, x/\\lambda - v \\rangle \\le 0$ for all $y \\in \\mathrm{conv}\\{C\\}$ is the variational inequality that uniquely characterizes $v$ as the Euclidean projection of $x/\\lambda$ onto the closed convex set $\\mathrm{conv}\\{C\\}$.\nTherefore, $v = P_{\\mathrm{conv}\\{C\\}}(x/\\lambda)$. Since $C$ is a finite set, its convex hull is closed, so $\\mathrm{conv}\\{C\\} = \\mathrm{cl\\,conv}\\,C$.\nThe optimal solution is $u^{\\ast} = x - \\lambda v = x - \\lambda P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda)$.\nThis confirms the general formula derived from the Moreau decomposition for this polyhedral case.",
            "answer": "$$\n\\boxed{x - \\lambda P_{\\mathrm{cl\\,conv}\\,C}\\left(\\frac{x}{\\lambda}\\right)}\n$$"
        },
        {
            "introduction": "A theoretical identity becomes truly powerful when it is translated into a reliable numerical algorithm. This exercise challenges you to bridge the gap between theory and practice by implementing the proximal operator for the support function of a polytope, using the identity connecting it to Euclidean projection . You will design a routine to project onto a convex hull and use it to build and verify one of the key computational tools in sparse optimization.",
            "id": "3452370",
            "problem": "Consider a nonempty compact convex polytope $C \\subset \\mathbb{R}^n$ described as the convex hull $C=\\mathrm{conv}\\{v_1,\\dots,v_m\\}$ of a finite set of vertices $\\{v_i\\}_{i=1}^m$. Let the support function of $C$ be $\\sigma_C(x)=\\sup_{c\\in C}\\langle c,x \\rangle$ and the indicator function be $\\delta_C(x)=0$ if $x\\in C$ and $+\\infty$ otherwise. For $\\lambda \\ge 0$, the proximal operator of a proper, lower semicontinuous convex function $f$ at $x\\in \\mathbb{R}^n$ is defined by $\\mathrm{prox}_{\\lambda f}(x)=\\arg\\min_{z\\in \\mathbb{R}^n}\\tfrac{1}{2}\\|z-x\\|_2^2+\\lambda f(z)$. You may regard as foundational the following widely used facts in convex analysis: Fenchel conjugacy of the support and indicator, and the Moreau decomposition identity for proximal operators, but you must derive any specific identity used in your implementation from these principles.\n\nYour tasks are:\n\n- Derive, from the above fundamental definitions and facts, a usable identity for $\\mathrm{prox}_{\\lambda \\sigma_C}(x)$ in terms of the Euclidean projection onto $C$, denoted $P_C(y)=\\arg\\min_{z\\in C}\\|z-y\\|_2$.\n\n- Design and implement an algorithm to compute $P_C(y)$ for a polytope represented by its vertices $\\{v_i\\}_{i=1}^m$ without enumerating faces. A valid reduction is to an optimization problem over the probability simplex for convex coefficients, using only deterministic numerical methods.\n\n- Implement two independent computations of $\\mathrm{prox}_{\\lambda \\sigma_C}(x)$:\n  1. Using your derived identity and your projection routine $P_C(\\cdot)$.\n  2. As the minimizer of the epigraph reformulation $\\min_{z\\in \\mathbb{R}^n,\\, t\\in \\mathbb{R}} \\tfrac{1}{2}\\|z-x\\|_2^2+\\lambda t$ subject to $\\langle v_i, z\\rangle \\le t$ for all $i\\in\\{1,\\dots,m\\}$.\n\n- Verify numerical correctness by comparing the two results and by checking the Moreau decomposition relationship instantiated for this problem. Each verification should be quantified by a boolean that is true precisely when the discrepancy is below a small tolerance (for example, when the Euclidean norm of a difference is less than $10^{-6}$).\n\nYour program must run deterministically and evaluate the following test suite. In each case, $C$ is given by its vertex list, $x\\in \\mathbb{R}^n$ and $\\lambda\\in \\mathbb{R}_+$:\n\n- Test $1$ (two-dimensional square): $C=\\mathrm{conv}\\{(-1,-1),(1,-1),(1,1),(-1,1)\\}$, $x=(0.3,-2.1)$, $\\lambda=0.5$.\n\n- Test $2$ (single-point polytope): $C=\\mathrm{conv}\\{(2,-1)\\}$, $x=(3,4)$, $\\lambda=1.2$.\n\n- Test $3$ (line segment): $C=\\mathrm{conv}\\{(0,0),(2,0)\\}$, $x=(3,4)$, $\\lambda=0.5$.\n\n- Test $4$ (five-dimensional simplex with an extra vertex at the origin): $C=\\mathrm{conv}\\{(1,0,0,0,0),(0,1,0,0,0),(0,0,1,0,0),(0,0,0,1,0),(0,0,0,0,1),(0,0,0,0,0)\\}$, $x=(1,-2,0.5,-0.7,3)$, $\\lambda=1$.\n\n- Test $5$ (boundary case $\\lambda=0$): $C=\\mathrm{conv}\\{(0,0,0),(1,0,0),(0,1,0)\\}$, $x=(0.2,0.3,0.4)$, $\\lambda=0$.\n\n- Test $6$ (large scaling): $C=\\mathrm{conv}\\{(-1,-1),(1,-1),(1,1),(-1,1)\\}$, $x=(10,-10)$, $\\lambda=10$.\n\nFor each test, produce two booleans:\n\n- The first boolean is true if the two computed proximal points agree within tolerance, that is, if $\\|\\hat{z}_{\\mathrm{id}}-\\hat{z}_{\\mathrm{epi}}\\|_2 \\le 10^{-6}$, where $\\hat{z}_{\\mathrm{id}}$ is obtained from the identity using $P_C(\\cdot)$ and $\\hat{z}_{\\mathrm{epi}}$ is obtained from the epigraph minimization.\n\n- The second boolean is true if the Moreau decomposition equality instantiated for your derived identity holds within tolerance, concretely if $\\|\\hat{z}_{\\mathrm{id}}+\\lambda\\,P_C(x/\\lambda)-x\\|_2 \\le 10^{-6}$ for $\\lambda0$, and if $\\|\\hat{z}_{\\mathrm{id}}-x\\|_2 \\le 10^{-12}$ for $\\lambda=0$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\mathrm{b}_{1,1},\\mathrm{b}_{1,2},\\mathrm{b}_{2,1},\\mathrm{b}_{2,2},\\dots,\\mathrm{b}_{6,1},\\mathrm{b}_{6,2}]$, where $\\mathrm{b}_{k,1}$ and $\\mathrm{b}_{k,2}$ are the two booleans for test $k$ in the order defined above. No physical units or angles are involved; all quantities are real numbers with the Euclidean norm understood in standard form.",
            "solution": "The problem is well-posed and scientifically grounded in the principles of convex analysis. It provides all necessary definitions and data to derive the required identities and implement the specified algorithms. There are no contradictions, ambiguities, or violations of scientific principles. The problem is valid, and a solution will be provided.\n\nThe solution is presented in three parts: first, the derivation of a computable identity for the proximal operator of the support function; second, the design of an algorithm to compute the Euclidean projection onto a polytope defined by its vertices; and third, the formulation of a comparative computation based on an epigraph reformulation.\n\nA key component of the solution is the Moreau decomposition, which provides a fundamental relationship between the proximal operator of a proper, lower semicontinuous, convex function $f$ and that of its Fenchel conjugate $f^*$. For any $x \\in \\mathbb{R}^n$ and $\\lambda  0$, the identity is:\n$$x = \\mathrm{prox}_{\\lambda f}(x) + \\lambda \\mathrm{prox}_{\\frac{1}{\\lambda} f^*}\\left(\\frac{x}{\\lambda}\\right)$$\nWe are interested in the case where $f$ is the support function $\\sigma_C$ of a nonempty compact convex set $C \\subset \\mathbb{R}^n$. The Fenchel conjugate of the support function $\\sigma_C$ is the indicator function $\\delta_C$ of the set $C$, i.e., $(\\sigma_C)^* = \\delta_C$. The indicator function is defined as $\\delta_C(z) = 0$ if $z \\in C$ and $\\delta_C(z) = +\\infty$ if $z \\notin C$. Substituting $f = \\sigma_C$ and $f^* = \\delta_C$ into the Moreau decomposition yields:\n$$x = \\mathrm{prox}_{\\lambda \\sigma_C}(x) + \\lambda \\mathrm{prox}_{\\frac{1}{\\lambda} \\delta_C}\\left(\\frac{x}{\\lambda}\\right)$$\nTo make this identity useful, we must find a simpler expression for the term $\\mathrm{prox}_{\\gamma \\delta_C}(y)$ for a generic argument $y \\in \\mathbb{R}^n$ and scalar $\\gamma  0$. By definition of the proximal operator:\n$$\\mathrm{prox}_{\\gamma \\delta_C}(y) = \\arg\\min_{z \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\|z-y\\|_2^2 + \\gamma \\delta_C(z) \\right\\}$$\nThe objective function is infinite for any $z \\notin C$. Therefore, the minimizer must lie within $C$. For $z \\in C$, the term $\\gamma \\delta_C(z)$ is zero. The minimization problem thus simplifies to:\n$$\\mathrm{prox}_{\\gamma \\delta_C}(y) = \\arg\\min_{z \\in C} \\frac{1}{2}\\|z-y\\|_2^2$$\nThis is precisely the definition of the Euclidean projection of the point $y$ onto the convex set $C$, denoted $P_C(y)$. The constant factor $\\frac{1}{2}$ does not affect the minimizer. Thus, for any $\\gamma  0$, we have $\\mathrm{prox}_{\\gamma \\delta_C}(y) = P_C(y)$. Applying this result to our Moreau identity with $\\gamma = \\frac{1}{\\lambda}$ and $y = \\frac{x}{\\lambda}$, we get:\n$$x = \\mathrm{prox}_{\\lambda \\sigma_C}(x) + \\lambda P_C\\left(\\frac{x}{\\lambda}\\right)$$\nRearranging this equation gives the desired identity for $\\mathrm{prox}_{\\lambda \\sigma_C}(x)$ for $\\lambda  0$:\n$$\\mathrm{prox}_{\\lambda \\sigma_C}(x) = x - \\lambda P_C\\left(\\frac{x}{\\lambda}\\right)$$\nFor the boundary case $\\lambda = 0$, the proximal operator is defined as:\n$$\\mathrm{prox}_{0 \\cdot \\sigma_C}(x) = \\arg\\min_{z \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\|z-x\\|_2^2 + 0 \\cdot \\sigma_C(z) \\right\\} = \\arg\\min_{z \\in \\mathbb{R}^n} \\frac{1}{2}\\|z-x\\|_2^2 = x$$\nThis completes the derivation for all $\\lambda \\ge 0$.\n\nTo compute the projection $P_C(y) = \\arg\\min_{z \\in C} \\|z-y\\|_2^2$, we leverage the representation of $C$ as the convex hull of its vertices, $C = \\mathrm{conv}\\{v_1, \\dots, v_m\\}$. Any point $z \\in C$ can be expressed as a convex combination $z = \\sum_{i=1}^m \\alpha_i v_i = V\\alpha$, where $V$ is the $n \\times m$ matrix with columns $\\{v_i\\}_{i=1}^m$, and $\\alpha$ lies in the probability simplex $\\Delta_m = \\{ \\alpha \\in \\mathbb{R}^m \\mid \\sum_{i=1}^m \\alpha_i = 1, \\alpha_i \\ge 0 \\}$. The projection problem is thus reduced to a convex Quadratic Program (QP) over $\\alpha$:\n$$\\min_{\\alpha \\in \\Delta_m} \\|V\\alpha - y\\|_2^2$$\nThis QP can be solved using standard deterministic numerical optimization algorithms, such as Sequential Least Squares Programming (SLSQP), without enumerating the faces of the polytope. Once the optimal coefficient vector $\\alpha^*$ is found, the projection point is recovered as $P_C(y) = V\\alpha^*$.\n\nFor an independent verification, we compute $\\mathrm{prox}_{\\lambda \\sigma_C}(x)$ by solving its defining optimization problem, $\\hat{z} = \\arg\\min_{z \\in \\mathbb{R}^n} \\{ \\frac{1}{2}\\|z-x\\|_2^2 + \\lambda \\sigma_C(z) \\}$. Since $\\sigma_C(z) = \\max_{i \\in \\{1,\\dots,m\\}} \\langle v_i, z \\rangle$, we can reformulate this using an epigraph variable $t \\in \\mathbb{R}$ as a constrained QP:\n$$\\min_{z \\in \\mathbb{R}^n, t \\in \\mathbb{R}} \\quad \\frac{1}{2}\\|z-x\\|_2^2 + \\lambda t$$\n$$\\text{subject to} \\quad \\langle v_i, z \\rangle \\le t, \\quad \\text{for } i=1, \\dots, m$$\nThe solution for the proximal point, $\\hat{z}_{\\mathrm{epi}}$, is the $z$ component of the optimal vector $(z^*, t^*)$.\n\nThe numerical verification will consist of two checks. First, the agreement between the two methods: $\\|\\hat{z}_{\\mathrm{id}} - \\hat{z}_{\\mathrm{epi}}\\|_2 \\le 10^{-6}$. Second, the satisfaction of the Moreau identity using the computed values: for $\\lambda  0$, $\\|\\hat{z}_{\\mathrm{id}} + \\lambda P_C(x/\\lambda) - x\\|_2 \\le 10^{-6}$, and for $\\lambda=0$, $\\|\\hat{z}_{\\mathrm{id}} - x\\|_2 \\le 10^{-12}$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\nTOL_AGREEMENT = 1e-6\nTOL_MOREAU_GT0 = 1e-6\nTOL_MOREAU_EQ0 = 1e-12\n\ndef project_onto_polytope(y: np.ndarray, V: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of a point y onto the convex hull of vertices V.\n    This solves the QP: min_{alpha} ||V*alpha - y||^2_2 s.t. sum(alpha)=1, alpha=0.\n    \n    Args:\n        y: The point to project, shape (n,).\n        V: The matrix of vertices, shape (n, m).\n\n    Returns:\n        The projection of y onto conv(V), shape (n,).\n    \"\"\"\n    n, m = V.shape\n    if m == 0:\n        raise ValueError(\"Vertex matrix V cannot be empty.\")\n    \n    # Objective function and its gradient (Jacobian)\n    def obj_fun(alpha):\n        return np.linalg.norm(V @ alpha - y)**2\n    \n    def obj_jac(alpha):\n        return 2 * V.T @ (V @ alpha - y)\n\n    # Constraints: sum(alpha) = 1\n    eq_cons = {'type': 'eq', 'fun': lambda alpha: np.sum(alpha) - 1.0}\n\n    # Bounds: alpha_i = 0 for all i\n    bnds = tuple((0, None) for _ in range(m))\n\n    # Initial guess for alpha (center of the simplex)\n    alpha0 = np.ones(m) / m\n\n    # Solve the QP\n    res = minimize(obj_fun, alpha0, jac=obj_jac,\n                   bounds=bnds, constraints=[eq_cons],\n                   method='SLSQP',\n                   options={'ftol': 1e-12, 'disp': False})\n\n    if not res.success:\n        # Fallback with higher precision if needed, or raise error.\n        # For this problem set, the default precision is expected to be sufficient.\n        pass\n\n    # The projection is V * alpha_optimal\n    return V @ res.x\n\ndef compute_prox_identity(x: np.ndarray, V: np.ndarray, lam: float) - np.ndarray:\n    \"\"\"\n    Computes prox_{lambda * sigma_C}(x) using the Moreau decomposition-based identity.\n    prox(x) = x - lambda * P_C(x / lambda).\n    \"\"\"\n    if lam == 0:\n        return x\n    \n    y = x / lam\n    proj_point = project_onto_polytope(y, V)\n    return x - lam * proj_point\n\ndef compute_prox_epigraph(x: np.ndarray, V: np.ndarray, lam: float) - np.ndarray:\n    \"\"\"\n    Computes prox_{lambda * sigma_C}(x) by solving the epigraph formulation.\n    min_{z,t} 0.5*||z-x||^2 + lambda*t  s.t. V.T*z = t.\n    \"\"\"\n    n, m = V.shape\n    \n    # Optimization variable u = [z_1, ..., z_n, t] of size n+1\n    def obj_fun(u):\n        z, t = u[:n], u[n]\n        return 0.5 * np.linalg.norm(z - x)**2 + lam * t\n\n    def obj_jac(u):\n        z, t = u[:n], u[n]\n        grad_z = z - x\n        grad_t = np.array([lam])\n        return np.concatenate((grad_z, grad_t))\n\n    # Constraints: V.T @ z = t  = t - V.T @ z = 0\n    # Scipy SLSQP expects constraints in the form g(u) = 0.\n    ineq_cons = {\n        'type': 'ineq',\n        'fun': lambda u: u[n] - V.T @ u[:n],\n        'jac': lambda u: np.hstack((-V.T, np.ones((m, 1))))\n    }\n\n    # Initial guess (feasible point u0 = [x, max(V.T @ x)])\n    t0 = np.max(V.T @ x) if m > 0 else 0.0\n    u0 = np.concatenate((x, [t0]))\n    \n    # Solve the QP\n    res = minimize(obj_fun, u0, jac=obj_jac,\n                   constraints=[ineq_cons],\n                   method='SLSQP',\n                   options={'ftol': 1e-12, 'disp': False})\n    \n    if not res.success:\n        pass # As before, rely on default success for this problem set.\n\n    return res.x[:n]\n\ndef solve():\n    \"\"\"\n    Runs the full test suite and prints the final results.\n    \"\"\"\n    test_cases = [\n        # Test 1: 2D square\n        {\n            'V': np.array([[-1, 1, 1, -1], [-1, -1, 1, 1]]),\n            'x': np.array([0.3, -2.1]),\n            'lambda': 0.5\n        },\n        # Test 2: Single-point polytope\n        {\n            'V': np.array([[2], [-1]]),\n            'x': np.array([3, 4]),\n            'lambda': 1.2\n        },\n        # Test 3: Line segment\n        {\n            'V': np.array([[0, 2], [0, 0]]),\n            'x': np.array([3, 4]),\n            'lambda': 0.5\n        },\n        # Test 4: 5D simplex + origin\n        {\n            'V': np.vstack([np.eye(5), np.zeros(5)]).T,\n            'x': np.array([1, -2, 0.5, -0.7, 3]),\n            'lambda': 1.0\n        },\n        # Test 5: Boundary case lambda=0\n        {\n            'V': np.array([[0, 1, 0], [0, 0, 1], [0, 0, 0]]),\n            'x': np.array([0.2, 0.3, 0.4]),\n            'lambda': 0.0\n        },\n        # Test 6: Large scaling\n        {\n            'V': np.array([[-1, 1, 1, -1], [-1, -1, 1, 1]]),\n            'x': np.array([10, -10]),\n            'lambda': 10.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        V, x, lam = case['V'], case['x'], case['lambda']\n        \n        # Compute proximal point using the two methods\n        z_id = compute_prox_identity(x, V, lam)\n        z_epi = compute_prox_epigraph(x, V, lam)\n        \n        # Verification 1: Compare the two results\n        discrepancy1 = np.linalg.norm(z_id - z_epi)\n        bool1 = discrepancy1 = TOL_AGREEMENT\n        \n        # Verification 2: Check Moreau decomposition identity\n        if lam > 0:\n            proj_point = project_onto_polytope(x / lam, V)\n            discrepancy2 = np.linalg.norm(z_id + lam * proj_point - x)\n            bool2 = discrepancy2 = TOL_MOREAU_GT0\n        else:  # lam == 0\n            discrepancy2 = np.linalg.norm(z_id - x)\n            bool2 = discrepancy2 = TOL_MOREAU_EQ0\n            \n        results.extend([bool1, bool2])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}