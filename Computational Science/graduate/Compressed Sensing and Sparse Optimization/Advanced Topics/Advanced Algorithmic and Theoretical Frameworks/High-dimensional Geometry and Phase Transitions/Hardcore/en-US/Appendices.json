{
    "hands_on_practices": [
        {
            "introduction": "To begin our exploration, we first establish a concrete, deterministic condition for when sparse recovery is guaranteed. This practice guides you through the construction of a \"dual certificate,\" a crucial object from optimization theory that provides a proof of correctness for $\\ell_1$ minimization. By analyzing this certificate, you will derive a sufficient condition based on the sensing matrix's mutual coherence, forging a tangible link between the geometry of the matrix columns and the success of recovery .",
            "id": "3451442",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix with columns $\\{a_{j}\\}_{j=1}^{n}$ normalized so that $\\|a_{j}\\|_{2}=1$ for all $j$, and mutual coherence $\\mu = \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$. Let $x \\in \\mathbb{R}^{n}$ be a $k$-sparse vector with support $S \\subset \\{1,\\dots,n\\}$ of size $|S|=k$, and define the sign vector $s = \\operatorname{sign}(x_{S}) \\in \\{-1,0,1\\}^{k}$ restricted to the support $S$. Consider the candidate dual certificate $y \\in \\mathbb{R}^{m}$ defined as the minimum Euclidean norm solution of the linear system $A_{S}^{\\top} y = s$, i.e., choose\n$$\ny = A_{S} \\left(A_{S}^{\\top} A_{S}\\right)^{-1} s.\n$$\nUsing only fundamental definitions and well-tested matrix norm bounds, and without invoking any specialized compressed sensing theorems, do the following:\n- Derive an upper bound on $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}$ in terms of $k$ and $\\mu$ by appropriately bounding $\\left\\|\\left(A_{S}^{\\top} A_{S}\\right)^{-1}\\right\\|_{\\infty}$.\n- Explain why the strict inequality $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1$ constitutes a sufficient Exact Recovery Condition (ERC) for Basis Pursuit (BP), relating your bound to this condition.\n- From your bound, determine the largest mutual coherence $\\mu$ (as a function of $k$) that guarantees the existence of such a dual certificate $y$ satisfying $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1$ for every sign pattern $s \\in \\{-1,1\\}^{k}$.\n\nYour final answer must be a single closed-form analytical expression in terms of $k$. Do not provide an inequality. No rounding is required.",
            "solution": "The problem requires a three-part analysis concerning the recovery of a sparse signal using Basis Pursuit. We will address each part in sequence after validating the problem statement. The validation confirms that the problem is well-posed, scientifically grounded, and contains all necessary information for a rigorous solution.\n\n### Part 1: Derivation of the Upper Bound\n\nWe are asked to derive an upper bound on $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}$ where $y = A_{S} (A_{S}^{\\top} A_{S})^{-1} s$.\nThe expression to be bounded is $\\|A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s\\|_{\\infty}$.\nWe use the sub-multiplicative property of compatible matrix and vector norms:\n$$\n\\|A_{S^{c}}^{\\top} A_{S} (A_{S}^{\\top} A_{S})^{-1} s\\|_{\\infty} \\le \\|A_{S^{c}}^{\\top} A_{S}\\|_{\\infty} \\|(A_{S}^{\\top} A_{S})^{-1}\\|_{\\infty} \\|s\\|_{\\infty}\n$$\nWe proceed by bounding each term on the right-hand side.\n\n**Bounding $\\|s\\|_{\\infty}$**:\nThe problem specifies considering sign patterns $s \\in \\{-1, 1\\}^k$. The vector $s$ thus has entries with magnitude equal to $1$. The infinity norm is the maximum absolute value of the components, so $\\|s\\|_{\\infty} = 1$.\n\n**Bounding $\\|A_{S^{c}}^{\\top} A_{S}\\|_{\\infty}$**:\nLet the matrix $C = A_{S^{c}}^{\\top} A_{S}$. This is a matrix of size $(n-k) \\times k$, where $k = |S|$. An entry $C_{ij}$ of this matrix corresponds to $a_i^{\\top} a_j$, where the column index $j$ is in the support set $S$ and the row index $i$ is in its complement $S^c$. Since $i \\in S^c$ and $j \\in S$, we always have $i \\neq j$. By the definition of mutual coherence $\\mu$, we have $|C_{ij}| = |a_i^{\\top} a_j| \\le \\mu$.\nThe infinity norm of a matrix is its maximum absolute row sum. For any row $i$ of $C$, corresponding to an index $i \\in S^c$:\n$$\n\\sum_{j \\in S} |C_{ij}| = \\sum_{j \\in S} |a_i^{\\top} a_j| \\le \\sum_{j \\in S} \\mu = k \\mu\n$$\nSince this holds for any row, the maximum absolute row sum is also bounded by this value. Therefore,\n$$\n\\|A_{S^{c}}^{\\top} A_{S}\\|_{\\infty} \\le k \\mu.\n$$\n\n**Bounding $\\|(A_{S}^{\\top} A_{S})^{-1}\\|_{\\infty}$**:\nLet $G = A_{S}^{\\top} A_{S}$. This is a $k \\times k$ Gram matrix. Its diagonal entries are $G_{jj} = a_j^{\\top} a_j = \\|a_j\\|_2^2 = 1$ for $j \\in S$. Its off-diagonal entries are $G_{ij} = a_i^{\\top} a_j$ for $i, j \\in S, i \\neq j$. The magnitude of these off-diagonal entries is bounded by $\\mu$, i.e., $|G_{ij}| \\le \\mu$.\nWe can write $G = I + H$, where $I$ is the $k \\times k$ identity matrix and $H$ is a matrix with zeros on its diagonal ($H_{jj}=0$) and off-diagonal entries $H_{ij} = G_{ij}$ for $i \\neq j$.\nThe infinity norm of $H$ is its maximum absolute row sum:\n$$\n\\|H\\|_{\\infty} = \\max_{i \\in S} \\sum_{j \\in S, j \\neq i} |H_{ij}| \\le \\max_{i \\in S} \\sum_{j \\in S, j \\neq i} \\mu\n$$\nIn each row, there are $k-1$ off-diagonal terms. Thus, $\\|H\\|_{\\infty} \\le (k-1)\\mu$.\nIf $\\|H\\|_{\\infty}  1$, which requires $(k-1)\\mu  1$, the matrix $G = I+H$ is invertible. Its inverse can be bounded using the Neumann series expansion. A standard result for matrix norms states that if $\\|H\\|_{\\infty}  1$, then $\\|(I+H)^{-1}\\|_{\\infty} \\le \\frac{1}{1-\\|H\\|_{\\infty}}$.\nApplying this result, we obtain:\n$$\n\\|(A_{S}^{\\top} A_{S})^{-1}\\|_{\\infty} = \\|G^{-1}\\|_{\\infty} \\le \\frac{1}{1-\\|H\\|_{\\infty}} \\le \\frac{1}{1-(k-1)\\mu}\n$$\nThis bound is valid under the condition $\\mu  1/(k-1)$.\n\n**Combining the Bounds**:\nSubstituting the individual bounds back into the original inequality, we find:\n$$\n\\|A_{S^{c}}^{\\top} y\\|_{\\infty} \\le (k\\mu) \\left( \\frac{1}{1-(k-1)\\mu} \\right) (1) = \\frac{k\\mu}{1-(k-1)\\mu}\n$$\nThis is the derived upper bound for $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}$ in terms of $k$ and $\\mu$.\n\n### Part 2: Explanation of the Exact Recovery Condition (ERC)\n\nBasis Pursuit (BP) finds the sparsest solution to an underdetermined system of linear equations by solving the $\\ell_1$-norm minimization problem:\n$$\n\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1 \\quad \\text{subject to} \\quad Az = b\n$$\nwhere $b=Ax$ for some true $k$-sparse signal $x$. The goal is to show that $x$ is the unique solution to this problem.\nThe optimality conditions for this convex optimization problem (the Karush-Kuhn-Tucker or KKT conditions) state that a feasible point $x$ is optimal if and only if there exists a dual vector (Lagrange multiplier) $y^* \\in \\mathbb{R}^m$ such that the subgradient of the Lagrangian with respect to $z$ at $z=x$ contains the zero vector. The Lagrangian is $L(z, y^*) = \\|z\\|_1 + (y^*)^\\top(b-Az)$. The subgradient condition translates to $A^\\top y^* \\in \\partial \\|x\\|_1$, where $\\partial \\|x\\|_1$ is the subdifferential of the $\\ell_1$-norm at $x$.\nThe subdifferential of $\\|z\\|_1$ at $z=x$ is the set of vectors $v$ such that:\n$$\nv_j = \\begin{cases} \\operatorname{sign}(x_j)  \\text{if } j \\in S \\text{ (where } x_j \\neq 0) \\\\ v_j \\in [-1, 1]  \\text{if } j \\in S^c \\text{ (where } x_j = 0) \\end{cases}\n$$\nFor $x$ to be the unique solution, a stricter condition is required: the subgradient must be unique on the inactive set $S^c$. This leads to the Exact Recovery Condition (ERC), which is the existence of a dual vector $y$ that satisfies:\n1. $A_j^{\\top} y = \\operatorname{sign}(x_j)$ for all $j \\in S$. This can be written compactly as $A_S^\\top y = \\operatorname{sign}(x_S)$.\n2. $|A_j^{\\top} y|  1$ for all $j \\in S^c$. This is equivalent to $\\|A_{S^c}^\\top y\\|_{\\infty}  1$.\n\nThe problem defines a specific candidate dual certificate $y = A_{S} (A_{S}^{\\top} A_{S})^{-1} s$, with $s = \\operatorname{sign}(x_S)$. Let's verify the first condition:\n$$\nA_S^\\top y = A_S^\\top \\left( A_S (A_S^\\top A_S)^{-1} s \\right) = (A_S^\\top A_S) (A_S^\\top A_S)^{-1} s = s\n$$\nThe construction of $y$ is precisely to satisfy the first condition of the ERC. Therefore, the sufficiency of the ERC for unique recovery reduces to verifying that the second condition, $\\|A_{S^c}^\\top y\\|_{\\infty}  1$, holds for this choice of $y$.\n\n### Part 3: Determination of the Largest Mutual Coherence\n\nTo guarantee exact recovery for any $k$-sparse signal, the condition $\\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1$ must hold for any support set $S$ of size $k$ and any sign pattern $s \\in \\{-1,1\\}^k$. Using the bound derived in Part 1, a sufficient condition to ensure this is:\n$$\n\\frac{k\\mu}{1-(k-1)\\mu}  1\n$$\nFor this inequality to be meaningful, the denominator must be positive, which implies $1 - (k-1)\\mu > 0$, or $\\mu  \\frac{1}{k-1}$.\nAssuming the denominator is positive, we can multiply both sides by it without changing the direction of the inequality:\n$$\nk\\mu  1 - (k-1)\\mu\n$$\n$$\nk\\mu + (k-1)\\mu  1\n$$\n$$\n(2k-1)\\mu  1\n$$\n$$\n\\mu  \\frac{1}{2k-1}\n$$\nThis condition is stricter than the condition $\\mu  \\frac{1}{k-1}$ for any $k > 1$. Thus, if $\\mu  \\frac{1}{2k-1}$, the bound holds and the ERC is satisfied.\nThe question asks for the largest mutual coherence $\\mu$ that guarantees the existence of the dual certificate $y$ satisfying the ERC. The derived condition, $\\mu  \\frac{1}{2k-1}$, provides a uniform guarantee for all $k$-sparse signals. The limit on $\\mu$ is the supremum of this interval. Therefore, the largest value of $\\mu$ is the boundary of this condition.",
            "answer": "$$\\boxed{\\frac{1}{2k-1}}$$"
        },
        {
            "introduction": "While deterministic conditions are foundational, they are often too strict for the random matrices used in modern applications. We now shift our perspective to the probabilistic realm, where the behavior of recovery is governed by the geometry of high-dimensional cones. This exercise introduces the statistical dimension, a powerful concept that quantifies the \"size\" of a cone, and asks you to compute it for the descent cone associated with $\\ell_1$ minimization . Your result will reveal how this single geometric quantity predicts the phase transition for sparse recovery.",
            "id": "3451445",
            "problem": "Consider the convex function $f:\\mathbb{R}^{n} \\to \\mathbb{R}$ defined by $f(\\boldsymbol{w}) = \\|\\boldsymbol{w}\\|_{1}$, and fix a point $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ whose support set $S := \\{i : x_{i} \\neq 0\\}$ has cardinality $|S| = k$, with $1 \\leq k  n$. Let $\\mathcal{D}$ denote the descent cone of $f$ at $\\boldsymbol{x}$, defined by\n$$\n\\mathcal{D} := \\left\\{\\boldsymbol{d} \\in \\mathbb{R}^{n} : \\exists\\, t > 0 \\text{ such that } f(\\boldsymbol{x} + t \\boldsymbol{d}) \\leq f(\\boldsymbol{x}) \\right\\}.\n$$\nThe statistical dimension of a closed convex cone $\\mathcal{C} \\subset \\mathbb{R}^{n}$ is defined as\n$$\n\\delta(\\mathcal{C}) := \\mathbb{E}\\left[ \\|\\boldsymbol{\\Pi}_{\\mathcal{C}}(\\boldsymbol{g})\\|_{2}^{2} \\right],\n$$\nwhere $\\boldsymbol{\\Pi}_{\\mathcal{C}}$ is the Euclidean projection onto $\\mathcal{C}$ and $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_{n})$ is a standard Gaussian vector.\n\nStarting from these definitions and the characterization of the subdifferential of $f$ at $\\boldsymbol{x}$, derive an expression for $\\delta(\\mathcal{D})$ as an infimum of an expectation over a nonnegative scalar parameter. Use high-dimensional asymptotics in the regime where $n \\to \\infty$ and $k/n \\to 0$ to determine the first-order term of $\\delta(\\mathcal{D})$ as a function of $n$ and $k$. Your derivation should proceed from the definitions above, employ properties of the standard normal distribution, and deliver an explicit leading-order analytical approximation. Use the natural logarithm $\\ln(\\cdot)$. State your final answer as a single closed-form analytic expression, with no rounding required. Then, in your reasoning, compare your first-order expression to $2k \\ln(n/k)$ and determine whether they match at leading order.",
            "solution": "The problem requires the calculation of the statistical dimension, $\\delta(\\mathcal{D})$, of the descent cone $\\mathcal{D}$ for the function $f(\\boldsymbol{w}) = \\|\\boldsymbol{w}\\|_{1}$ at a point $\\boldsymbol{x}$. We are asked to derive an expression for $\\delta(\\mathcal{D})$ as an infimum of an expectation, and then find its leading-order term in the high-dimensional limit where $n \\to \\infty$ and $k/n \\to 0$.\n\n### Step 1: Characterization of the Descent Cone $\\mathcal{D}$\n\nThe descent cone $\\mathcal{D}$ at a point $\\boldsymbol{x}$ for a convex function $f$ is given by the set of directions $\\boldsymbol{d}$ for which the directional derivative $f'(\\boldsymbol{x};\\boldsymbol{d})$ is non-positive. The directional derivative is the support function of the subdifferential $\\partial f(\\boldsymbol{x})$, i.e., $f'(\\boldsymbol{x};\\boldsymbol{d}) = \\sup_{\\boldsymbol{v} \\in \\partial f(\\boldsymbol{x})} \\langle \\boldsymbol{v}, \\boldsymbol{d} \\rangle$. Thus, the descent cone can be expressed as:\n$$\n\\mathcal{D} = \\{ \\boldsymbol{d} \\in \\mathbb{R}^n : \\sup_{\\boldsymbol{v} \\in \\partial f(\\boldsymbol{x})} \\langle \\boldsymbol{v}, \\boldsymbol{d} \\rangle \\leq 0 \\}\n$$\nThe subdifferential of the $\\ell_1$-norm $f(\\boldsymbol{w}) = \\|\\boldsymbol{w}\\|_1$ at $\\boldsymbol{x}$ is defined by the set of vectors $\\boldsymbol{v}$ such that:\n$$\nv_i = \\begin{cases} \\text{sign}(x_i)  \\text{if } x_i \\neq 0 \\\\ c_i  \\text{if } x_i = 0, \\text{ where } c_i \\in [-1, 1] \\end{cases}\n$$\nLet $S = \\{i : x_i \\neq 0\\}$ be the support of $\\boldsymbol{x}$ with $|S|=k$, and let $S^c$ be its complement. Let $\\boldsymbol{s} = \\text{sign}(\\boldsymbol{x}_S)$ be the vector of signs of the non-zero components of $\\boldsymbol{x}$. A subgradient $\\boldsymbol{v} \\in \\partial f(\\boldsymbol{x})$ has components $\\boldsymbol{v}_S = \\boldsymbol{s}$ and $\\|\\boldsymbol{v}_{S^c}\\|_\\infty \\leq 1$.\n\nThe condition for a direction $\\boldsymbol{d}$ to be in $\\mathcal{D}$ is:\n$$\n\\sup_{\\boldsymbol{v}_S = \\boldsymbol{s}, \\|\\boldsymbol{v}_{S^c}\\|_\\infty \\leq 1} \\left( \\langle \\boldsymbol{v}_S, \\boldsymbol{d}_S \\rangle + \\langle \\boldsymbol{v}_{S^c}, \\boldsymbol{d}_{S^c} \\rangle \\right) \\leq 0\n$$\nThe supremum is achieved when $\\boldsymbol{v}_i = \\text{sign}(d_i)$ for $i \\in S^c$. This gives:\n$$\n\\langle \\boldsymbol{s}, \\boldsymbol{d}_S \\rangle + \\sum_{i \\in S^c} |\\boldsymbol{d}_i| = \\langle \\boldsymbol{s}, \\boldsymbol{d}_S \\rangle + \\|\\boldsymbol{d}_{S^c}\\|_1 \\leq 0\n$$\nSo, the descent cone is $\\mathcal{D} = \\{ \\boldsymbol{d} \\in \\mathbb{R}^n : \\langle \\boldsymbol{s}, \\boldsymbol{d}_S \\rangle + \\|\\boldsymbol{d}_{S^c}\\|_1 \\leq 0 \\}$.\n\n### Step 2: Variational Formula for the Statistical Dimension $\\delta(\\mathcal{D})$\n\nThe statistical dimension of $\\mathcal{D}$ is $\\delta(\\mathcal{D}) = \\mathbb{E}[\\|\\boldsymbol{\\Pi}_{\\mathcal{D}}(\\boldsymbol{g})\\|_2^2]$ for $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_n)$. By the Moreau decomposition for projections onto cones, we have $\\boldsymbol{g} = \\boldsymbol{\\Pi}_{\\mathcal{D}}(\\boldsymbol{g}) + \\boldsymbol{\\Pi}_{\\mathcal{D}^\\circ}(\\boldsymbol{g})$, where $\\mathcal{D}^\\circ$ is the polar cone of $\\mathcal{D}$. Since the two projected vectors are orthogonal, $\\|\\boldsymbol{g}\\|_2^2 = \\|\\boldsymbol{\\Pi}_{\\mathcal{D}}(\\boldsymbol{g})\\|_2^2 + \\|\\boldsymbol{\\Pi}_{\\mathcal{D}^\\circ}(\\boldsymbol{g})\\|_2^2$. Taking expectations, we get $\\delta(\\mathcal{D}) = \\mathbb{E}[\\|\\boldsymbol{g}\\|_2^2] - \\mathbb{E}[\\|\\boldsymbol{\\Pi}_{\\mathcal{D}^\\circ}(\\boldsymbol{g})\\|_2^2] = n - \\delta(\\mathcal{D}^\\circ)$.\n\nHowever, a more direct approach relies on a variational formula derived from Gaussian process theory (e.g., Gordon's inequality), which allows interchanging expectation and optimization operators under certain conditions. For the cone $\\mathcal{D}$, this theory yields:\n$$\n\\delta(\\mathcal{D}) = \\mathbb{E}\\left[\\text{dist}(\\boldsymbol{g}, \\mathcal{D}^\\circ)^2\\right] = \\mathbb{E}\\left[ \\min_{\\boldsymbol{w} \\in \\mathcal{D}^\\circ} \\|\\boldsymbol{g} - \\boldsymbol{w}\\|_2^2 \\right] = \\inf_{\\lambda \\geq 0} \\mathbb{E}\\left[ \\|\\boldsymbol{g}_S - \\lambda\\boldsymbol{s}\\|_2^2 + \\sum_{i \\in S^c} (|g_i|-\\lambda)_+^2 \\right]\n$$\nThis expression provides the required form of an infimum of an expectation over a non-negative scalar parameter $\\lambda$. We define the objective function $\\Psi(\\lambda)$ as the term inside the infimum:\n$$\n\\Psi(\\lambda) := \\mathbb{E}\\left[ \\|\\boldsymbol{g}_S - \\lambda\\boldsymbol{s}\\|_2^2 + \\sum_{i \\in S^c} (|g_i|-\\lambda)_+^2 \\right]\n$$\n\n### Step 3: Calculation of the Expectation $\\Psi(\\lambda)$\n\nWe compute the expected value of each term in $\\Psi(\\lambda)$.\nFor the first term, concerning the support set $S$:\n$$\n\\mathbb{E}[\\|\\boldsymbol{g}_S - \\lambda\\boldsymbol{s}\\|_2^2] = \\mathbb{E}[\\|\\boldsymbol{g}_S\\|_2^2 - 2\\lambda\\langle\\boldsymbol{g}_S, \\boldsymbol{s}\\rangle + \\lambda^2\\|\\boldsymbol{s}\\|_2^2]\n$$\nSince $\\boldsymbol{g}_S$ is a vector of $k$ i.i.d. standard normal variables, $\\mathbb{E}[\\|\\boldsymbol{g}_S\\|_2^2] = k$ and $\\mathbb{E}[\\langle\\boldsymbol{g}_S, \\boldsymbol{s}\\rangle] = \\langle\\mathbb{E}[\\boldsymbol{g}_S], \\boldsymbol{s}\\rangle = 0$. The vector $\\boldsymbol{s}$ has entries $\\pm 1$, so $\\|\\boldsymbol{s}\\|_2^2 = k$. Therefore:\n$$\n\\mathbb{E}[\\|\\boldsymbol{g}_S - \\lambda\\boldsymbol{s}\\|_2^2] = k + \\lambda^2 k = k(1+\\lambda^2)\n$$\nFor the second term, concerning the complement $S^c$:\n$$\n\\mathbb{E}\\left[\\sum_{i \\in S^c} (|g_i|-\\lambda)_+^2\\right] = (n-k)\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[(|Z|-\\lambda)_+^2]\n$$\nLet $E_\\lambda = \\mathbb{E}[(|Z|-\\lambda)_+^2]$. We calculate this expectation:\n$$\nE_\\lambda = \\int_{-\\infty}^{\\infty} (\\max(0, |z|-\\lambda))^2 \\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2} dz = 2\\int_{\\lambda}^{\\infty} (z-\\lambda)^2 \\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2} dz\n$$\nLet $\\phi(z)$ be the standard normal PDF and $Q(\\lambda) = \\int_\\lambda^\\infty \\phi(z)dz$ be the Q-function. Integration by parts yields:\n$$\nE_\\lambda = (1+\\lambda^2) \\cdot 2Q(\\lambda) - 2\\lambda\\phi(\\lambda)\n$$\nCombining these results, we get the objective function:\n$$\n\\Psi(\\lambda) = k(1+\\lambda^2) + (n-k) \\left( (1+\\lambda^2)2Q(\\lambda) - 2\\lambda\\phi(\\lambda) \\right)\n$$\nWe seek $\\delta(\\mathcal{D}) = \\inf_{\\lambda \\geq 0} \\Psi(\\lambda)$.\n\n### Step 4: High-Dimensional Asymptotic Analysis\n\nWe now analyze $\\Psi(\\lambda)$ in the regime $n \\to \\infty$ with $k/n \\to 0$. The optimal $\\lambda$ is found from $\\Psi'(\\lambda) = 0$. The derivative of $E_\\lambda$ is $E'_\\lambda = 4(\\lambda Q(\\lambda) - \\phi(\\lambda))$.\nSo, $\\Psi'(\\lambda) = 2k\\lambda + 4(n-k)(\\lambda Q(\\lambda) - \\phi(\\lambda)) = 0$.\nIn the limit $n/k \\to \\infty$, the optimal value $\\lambda^*$ will be large. We use the asymptotic expansion of the Q-function for large $\\lambda$: $Q(\\lambda) \\sim \\frac{\\phi(\\lambda)}{\\lambda}(1-\\lambda^{-2})$.\nThis gives $\\lambda Q(\\lambda) - \\phi(\\lambda) \\sim \\phi(\\lambda)(1-\\lambda^{-2}) - \\phi(\\lambda) = -\\phi(\\lambda)/\\lambda^2$.\nThe optimality condition becomes, to leading order:\n$$\n2k\\lambda^* + 4(n-k)(-\\phi(\\lambda^*)/(\\lambda^*)^2) \\approx 0 \\implies k(\\lambda^*)^3 \\approx 2n\\phi(\\lambda^*)\n$$\nSubstituting $\\phi(\\lambda^*) = \\frac{1}{\\sqrt{2\\pi}} e^{-(\\lambda^*)^2/2}$ and taking logarithms:\n$$\n\\ln(k) + 3\\ln(\\lambda^*) \\approx \\ln(n) - (\\lambda^*)^2/2 + \\mathcal{O}(1)\n$$\nThe dominant balance yields $(\\lambda^*)^2/2 \\approx \\ln(n/k)$, so the leading-order behavior of $\\lambda^*$ is:\n$$\n(\\lambda^*)^2 \\approx 2\\ln(n/k)\n$$\nNow, we evaluate $\\Psi(\\lambda^*)$ at this leading order. For large $\\lambda$, a more detailed asymptotic expansion reveals $E_\\lambda \\approx 4\\phi(\\lambda)/\\lambda^3$.\n$$\n\\Psi(\\lambda^*) \\approx k(1+(\\lambda^*)^2) + (n-k)E_{\\lambda^*} \\approx k(1+(\\lambda^*)^2) + (n-k)\\frac{4\\phi(\\lambda^*)}{(\\lambda^*)^3}\n$$\nFrom the optimality condition, $k(\\lambda^*)^3 \\approx 2(n-k)\\phi(\\lambda^*)$, which gives $(n-k) \\approx \\frac{k(\\lambda^*)^3}{2\\phi(\\lambda^*)}$.\nSubstituting this into the second term:\n$$\n(n-k)\\frac{4\\phi(\\lambda^*)}{(\\lambda^*)^3} \\approx \\frac{k(\\lambda^*)^3}{2\\phi(\\lambda^*)} \\frac{4\\phi(\\lambda^*)}{(\\lambda^*)^3} = 2k\n$$\nTherefore, the asymptotic value of $\\Psi(\\lambda^*)$ is:\n$$\n\\delta(\\mathcal{D}) \\approx k(1 + (\\lambda^*)^2) + 2k = k(1 + 2\\ln(n/k)) + 2k = 2k\\ln(n/k) + 3k\n$$\nThe first-order (leading) term is the one that grows with $n$ and $k$. Since $k/n \\to 0$, $\\ln(n/k) \\to \\infty$, so the leading term is $2k\\ln(n/k)$.\n\n### Step 5: Comparison\nThe problem asks to compare this result to the expression $2k\\ln(n/k)$. Our analysis shows that the first-order term of $\\delta(\\mathcal{D})$ in the specified high-dimensional regime is precisely $2k\\ln(n/k)$. The two expressions match at leading order. The next order term in our expansion is $3k$. Thus, the requested first-order closed-form analytic expression is $2k\\ln(n/k)$.",
            "answer": "$$\\boxed{2k\\ln\\left(\\frac{n}{k}\\right)}$$"
        },
        {
            "introduction": "The theoretical predictions derived from statistical dimension lead to a remarkable and deep insight: the universality phenomenon, which posits that the sharp phase transition separating success from failure is independent of the specific family of random matrices used. This final practice moves from analytical derivation to computational verification, tasking you with implementing an experiment to test the universality conjecture firsthand. By comparing the empirical performance of Gaussian and structured Fourier ensembles, you will see how abstract geometric concepts manifest in practical algorithms and gain an appreciation for the robustness of compressed sensing theory .",
            "id": "3451296",
            "problem": "Consider the compressed sensing model in which an unknown vector $x_0 \\in \\mathbb{R}^n$ is $k$-sparse, meaning it has at most $k$ nonzero entries. Measurements $y \\in \\mathbb{C}^m$ are obtained through a linear operator $A \\in \\mathbb{C}^{m \\times n}$, with $y = A x_0$. Recovery is performed via the basis pursuit program that minimizes the $\\ell_1$ norm subject to exact measurement constraints. The geometry of high-dimensional convex sets suggests that the success or failure of $\\ell_1$ recovery exhibits a sharp transition in the plane of undersampling ratio and sparsity ratio, commonly summarized by a phase transition curve $\\delta_c(\\rho)$, where $\\delta = m/n$ and $\\rho = k/m$. This curve is conjectured to be universal across a class of measurement ensembles after suitable randomization.\n\nStarting from fundamental definitions and well-tested facts:\n- Sparse recovery via $\\ell_1$ minimization is defined as the convex optimization problem $\\min \\|x\\|_1$ subject to $A x = y$, where $\\|\\cdot\\|_1$ denotes the sum of absolute values of components.\n- Universality in compressed sensing posits that the empirical phase transition behavior is largely determined by geometric properties of random polytopes and is largely independent of the specific distribution of the measurement matrix, provided certain incoherence and isotropy conditions hold.\n- The Discrete Fourier Transform (DFT) matrix $F \\in \\mathbb{C}^{n \\times n}$ is defined by $F_{j,k} = \\exp\\!\\left(-2\\pi i \\frac{jk}{n}\\right)/\\sqrt{n}$, with $i$ denoting the imaginary unit. A partial Fourier subsampling matrix is formed by selecting $m$ rows uniformly at random from $F$. Random modulation is applied via a diagonal matrix $R \\in \\mathbb{R}^{n \\times n}$ where diagonal entries are independent Rademacher variables (each entry equal to $+1$ or $-1$ with equal probability), yielding $A = P F R$, where $P$ denotes row selection.\n\nYour task is to implement a program that empirically tests universality by comparing the estimated phase transition between two measurement ensembles:\n1. A Gaussian ensemble where entries of $A$ are independent and identically distributed normal random variables with mean $0$ and variance $1/m$, denoted $A_{ij} \\sim \\mathcal{N}(0, 1/m)$.\n2. A randomly modulated partial Fourier ensemble where $A = P F R$ as defined above, with $F$ orthonormal.\n\nFor each test case, do the following:\n- Fix $n$ and $k$. For each candidate $m$ in a provided list, perform $T$ independent trials. In each trial:\n  - Sample a $k$-sparse $x_0 \\in \\mathbb{R}^n$ with support chosen uniformly at random and nonzero values drawn independently from a standard normal distribution $\\mathcal{N}(0,1)$.\n  - Form measurements $y = A x_0$ for each ensemble separately.\n  - Solve the basis pursuit problem $\\min \\|x\\|_1$ subject to $A x = y$ via an equivalent linear programming formulation using the variable split $x = u - v$ with $u \\ge 0$, $v \\ge 0$.\n  - Declare success if the relative $\\ell_2$-error $\\|x^\\star - x_0\\|_2 / \\max(\\|x_0\\|_2, \\varepsilon)$ is at most a specified tolerance, where $x^\\star$ is the recovered solution and $\\varepsilon$ is a small positive number to avoid division by zero.\n- For each ensemble, compute the empirical success rate for each $m$ as the fraction of trials that succeed.\n- For each ensemble, estimate $\\delta_c$ at the given $k$ by selecting the smallest $\\delta = m/n$ among the candidate $m$ values whose success rate is at least a specified threshold $q$. If no candidate $m$ reaches the threshold, declare that $\\delta_c$ is undefined.\n- Return a boolean result for each test case indicating whether the absolute difference between the estimated critical undersampling ratios for the two ensembles, $|\\delta_c^{\\mathrm{Gauss}} - \\delta_c^{\\mathrm{Fourier}}|$, is less than or equal to a specified tolerance $\\epsilon$. If one or both $\\delta_c$ values are undefined, return false for that test case.\n\nAlgorithmic constraints:\n- The basis pursuit problem must be solved via a linear program by introducing $u, v \\in \\mathbb{R}^n$, minimizing $\\mathbf{1}^\\top(u + v)$ subject to $A(u - v) = y$, $u \\ge 0$, $v \\ge 0$. For complex-valued $A$ and $y$, enforce the equality constraints by splitting into real and imaginary parts: $\\operatorname{Re}(A)(u - v) = \\operatorname{Re}(y)$ and $\\operatorname{Im}(A)(u - v) = \\operatorname{Im}(y)$.\n\nSuccess criterion:\n- Use the relative error tolerance $\\tau = 10^{-6}$ and the denominator stabilizer $\\varepsilon = 10^{-12}$.\n\nTest suite:\n- Case $1$: $n = 64$, $k = 4$, candidate $m$ values $[12, 14, 16, 18, 20, 22, 24]$, number of trials $T = 8$, success threshold $q = 0.75$, tolerance $\\epsilon = 0.15$.\n- Case $2$: $n = 96$, $k = 8$, candidate $m$ values $[24, 28, 32, 36, 40, 44]$, number of trials $T = 6$, success threshold $q = 0.67$, tolerance $\\epsilon = 0.20$.\n- Case $3$: $n = 128$, $k = 16$, candidate $m$ values $[36, 44, 52, 60, 68, 76, 84]$, number of trials $T = 5$, success threshold $q = 0.60$, tolerance $\\epsilon = 0.20$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is a boolean corresponding to the test cases in the order listed above.\n\nAll answers are unitless real numbers and booleans; no physical units are involved. Angles, if any, are to be interpreted in radians, but none are required in this task.",
            "solution": "The problem asks for an empirical investigation of the universality conjecture in compressed sensing. This conjecture posits that the phase transition behavior of sparse signal recovery via $\\ell_1$-minimization is largely independent of the specific distribution of the random measurement matrix, provided certain general conditions are met. We are to compare the phase transition points for two common matrix ensembles: a complex Gaussian ensemble and a randomly modulated partial Fourier ensemble.\n\n### Theoretical Framework\n\nThe core problem in compressed sensing is to recover a $k$-sparse signal $x_0 \\in \\mathbb{R}^n$ (a vector with at most $k$ non-zero elements) from a set of linear measurements $y = A x_0$. The measurement matrix $A$ is an $m \\times n$ matrix, where $m  n$, making the system of equations underdetermined.\n\nRecovery is often achieved by solving the basis pursuit convex optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y $$\nwhere $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$ is the $\\ell_1$-norm. This program succeeds in recovering $x_0$ exactly if the number of measurements $m$ is sufficiently large relative to the sparsity $k$ and the signal dimension $n$.\n\nThe success of recovery exhibits a sharp phase transition. For a fixed signal structure, as the undersampling ratio $\\delta = m/n$ increases, the probability of successful recovery transitions rapidly from near $0$ to near $1$. The critical value $\\delta_c$ at which this transition occurs depends on the sparsity ratio. The problem defines this ratio as $\\rho = k/m$. The curve $\\delta_c(\\rho)$ in the $(\\rho, \\delta)$ plane is known as the phase transition curve. The universality conjecture states that this curve is the same for a wide class of random matrix ensembles.\n\n### Methodology and Algorithmic Design\n\nOur task is to estimate and compare the critical undersampling ratio $\\delta_c$ for two ensembles.\n\n1.  **Gaussian Ensemble**: The matrix $A_{\\text{Gauss}} \\in \\mathbb{C}^{m \\times n}$ has entries that are independent and identically distributed. The problem specifies $A_{ij} \\sim \\mathcal{N}(0, 1/m)$. For a complex-valued matrix, this is standardly interpreted as entries being complex normal variables, $A_{ij} \\sim \\mathcal{CN}(0, 1/m)$. This means $A_{ij} = U + iV$ where $U, V$ are i.i.d. real Gaussian random variables $\\mathcal{N}(0, 1/(2m))$. The variance is $\\mathbb{E}[|A_{ij}|^2] = \\mathbb{E}[U^2 + V^2] = \\frac{1}{2m} + \\frac{1}{2m} = \\frac{1}{m}$, consistent with the problem statement.\n\n2.  **Randomly Modulated Partial Fourier Ensemble**: The matrix is constructed as $A_{\\text{Fourier}} = PFR$.\n    -   $F \\in \\mathbb{C}^{n \\times n}$ is the orthonormal Discrete Fourier Transform (DFT) matrix, with entries $F_{j,k} = \\frac{1}{\\sqrt{n}} \\exp(-2\\pi i \\frac{jk}{n})$ for $j, k \\in \\{0, \\dots, n-1\\}$.\n    -   $R \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix whose diagonal entries are independent Rademacher random variables (i.e., $\\pm 1$ with probability $1/2$). This random modulation is crucial for ensuring incoherence.\n    -   $P$ is a projection operator that selects $m$ rows of $FR$ uniformly at random.\n\nThe experimental procedure for a given test case $(n, k, \\{m\\}_{\\text{cand}}, T, q, \\epsilon)$ is as follows:\nFor each candidate number of measurements $m \\in \\{m\\}_{\\text{cand}}$, we perform $T$ independent trials. In each trial:\n- A $k$-sparse signal $x_0 \\in \\mathbb{R}^n$ is generated. Its support (the set of indices of non-zero entries) is chosen uniformly at random, and the non-zero values are drawn from a standard normal distribution $\\mathcal{N}(0,1)$.\n- For each ensemble, the measurement matrix $A$ is generated, and the measurement vector $y = Ax_0$ is computed.\n- The basis pursuit problem is solved to obtain a recovered signal $x^\\star$.\n- Success is declared if the relative $\\ell_2$-error is below a tolerance $\\tau = 10^{-6}$:\n  $$ \\frac{\\|x^\\star - x_0\\|_2}{\\max(\\|x_0\\|_2, \\varepsilon)} \\le \\tau $$\n  with $\\varepsilon = 10^{-12}$ to prevent division by zero.\n\nThe empirical success rate for each $m$ is the fraction of successful trials. The critical undersampling ratio $\\delta_c$ is estimated as $m_c/n$, where $m_c$ is the smallest candidate $m$ for which the success rate is at least the threshold $q$. If no $m$ achieves this rate, $\\delta_c$ is considered undefined for that ensemble.\n\nFinally, universality is tested by checking if the absolute difference between the estimated critical ratios is within a tolerance $\\epsilon$: $|\\delta_c^{\\mathrm{Gauss}} - \\delta_c^{\\mathrm{Fourier}}| \\le \\epsilon$. The test fails if this condition is not met or if either $\\delta_c$ is undefined.\n\n### Basis Pursuit as a Linear Program\n\nThe $\\ell_1$-minimization problem is solved by reformulating it as a linear program (LP). We use the variable splitting technique by representing $x \\in \\in \\mathbb{R}^n$ as the difference of two non-negative vectors, $x = u - v$, where $u,v \\in \\mathbb{R}^n$ and $u,v \\ge 0$. The $\\ell_1$-norm $\\|x\\|_1$ is then equivalent to minimizing $\\mathbf{1}^\\top(u+v)$, where $\\mathbf{1}$ is a vector of ones. This holds because for any optimal solution, for each component $i$, at least one of $u_i$ or $v_i$ must be zero.\n\nThe basis pursuit problem becomes:\n$$ \\min_{u,v \\in \\mathbb{R}^n} \\mathbf{1}^\\top u + \\mathbf{1}^\\top v \\quad \\text{subject to} \\quad A(u-v) = y, \\ u \\ge 0, \\ v \\ge 0. $$\nSince the matrix $A$ and vector $y$ are complex, $A = \\operatorname{Re}(A) + i\\operatorname{Im}(A)$ and $y = \\operatorname{Re}(y) + i\\operatorname{Im}(y)$, the single complex equality constraint $A(u-v) = y$ is equivalent to two real equality constraints:\n$$ \\operatorname{Re}(A)(u-v) = \\operatorname{Re}(y) $$\n$$ \\operatorname{Im}(A)(u-v) = \\operatorname{Im}(y) $$\nThis system is ready to be solved by a standard LP solver. We define a new variable vector $z = [u^\\top, v^\\top]^\\top \\in \\mathbb{R}^{2n}$. The LP is then:\n- **Minimize:** $c^\\top z$, where $c = \\mathbf{1}_{2n} \\in \\mathbb{R}^{2n}$.\n- **Subject to:** $A_{eq} z = b_{eq}$ and $z \\ge 0$.\nThe equality constraint matrix $A_{eq} \\in \\mathbb{R}^{2m \\times 2n}$ and vector $b_{eq} \\in \\mathbb{R}^{2m}$ are constructed as:\n$$ A_{eq} = \\begin{pmatrix} \\operatorname{Re}(A)  -\\operatorname{Re}(A) \\\\ \\operatorname{Im}(A)  -\\operatorname{Im}(A) \\end{pmatrix}, \\quad b_{eq} = \\begin{pmatrix} \\operatorname{Re}(y) \\\\ \\operatorname{Im}(y) \\end{pmatrix} $$\nThe solution of the LP, $z^\\star = [(u^\\star)^\\top, (v^\\star)^\\top]^\\top$, gives the recovered signal $x^\\star = u^\\star - v^\\star$. This procedure is implemented for each trial to find $x^\\star$ and evaluate recovery success.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\nfrom scipy.fft import fft\n\ndef solve_bp_complex(A, y, n):\n    \"\"\"\n    Solves the basis pursuit problem min ||x||_1 s.t. Ax = y for complex A and y\n    using a linear programming formulation.\n    \"\"\"\n    m = A.shape[0]\n\n    # LP variable is z = [u, v] where x = u - v, u, v = 0. Size is 2n.\n    # Objective function: min 1_T * u + 1_T * v, so c = [1, 1, ..., 1].\n    c = np.ones(2 * n)\n\n    # Equality constraints: A(u - v) = y\n    # This splits into real and imaginary parts:\n    # Re(A)(u - v) = Re(y)\n    # Im(A)(u - v) = Im(y)\n    # A_eq * z = b_eq, where z = [u, v]^T\n    A_re = np.real(A)\n    A_im = np.imag(A)\n    \n    A_eq = np.block([\n        [A_re, -A_re],\n        [A_im, -A_im]\n    ])\n\n    b_eq = np.concatenate([np.real(y), np.imag(y)])\n    \n    # All variables are non-negative\n    bounds = (0, None)\n\n    # Solve the linear program\n    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs', options={'presolve': True})\n\n    if res.success:\n        z = res.x\n        u = z[:n]\n        v = z[n:]\n        x_rec = u - v\n        return x_rec\n    else:\n        # Return a vector of NaNs to indicate failure, which will fail the success check.\n        return np.full(n, np.nan)\n\ndef check_success(x_rec, x_0, tau, eps_denom):\n    \"\"\"\n    Checks if the recovery was successful based on relative l2 error.\n    \"\"\"\n    if np.any(np.isnan(x_rec)):\n        return False\n    norm_x0 = np.linalg.norm(x_0)\n    error_norm = np.linalg.norm(x_rec - x_0)\n    relative_error = error_norm / max(norm_x0, eps_denom)\n    return relative_error = tau\n\ndef estimate_critical_delta(ensemble_type, n, k, m_candidates, T, q, tau, eps_denom, rng):\n    \"\"\"\n    Estimates the critical undersampling ratio delta_c for a given ensemble.\n    \"\"\"\n    # Pre-compute the orthonormal DFT matrix for the Fourier case\n    if ensemble_type == 'fourier':\n        F = fft(np.eye(n), norm='ortho', axis=0)\n\n    success_rates = {}\n\n    for m in m_candidates:\n        success_count = 0\n        for _ in range(T):\n            # 1. Generate k-sparse signal x_0\n            x_0 = np.zeros(n)\n            support = rng.choice(n, k, replace=False)\n            x_0[support] = rng.standard_normal(k)\n\n            # 2. Generate measurement matrix A and measurements y\n            if ensemble_type == 'gaussian':\n                # A_ij ~ CN(0, 1/m). Real and imag parts are N(0, 1/(2m)).\n                A = (rng.standard_normal((m, n)) + 1j * rng.standard_normal((m, n))) / np.sqrt(2 * m)\n            elif ensemble_type == 'fourier':\n                # A = PFR\n                signs = rng.choice([-1.0, 1.0], size=n)\n                R_mult = signs # For efficient multiplication\n                \n                rows = rng.choice(n, m, replace=False)\n                # A = (F @ np.diag(signs))[rows, :] is inefficient\n                # A = F_rows @ diag(signs) = F_rows * signs (broadcast)\n                A = F[rows, :] * R_mult\n            \n            y = A @ x_0\n\n            # 3. Solve for x_rec\n            x_rec = solve_bp_complex(A, y, n)\n\n            # 4. Check for success\n            if check_success(x_rec, x_0, tau, eps_denom):\n                success_count += 1\n        \n        success_rates[m] = success_count / T\n    \n    # 5. Estimate delta_c\n    delta_c = float('nan') # Using NaN to represent 'undefined'\n    sorted_m = sorted(m_candidates)\n    for m in sorted_m:\n        if success_rates[m] = q:\n            delta_c = m / n\n            break # Found the smallest m that meets the threshold\n            \n    return delta_c\n\ndef run_single_test_case(n, k, m_candidates, T, q, epsilon, tau, eps_denom, seed):\n    \"\"\"\n    Runs the full comparison for a single test case.\n    \"\"\"\n    # Use a specific seed for each test case for reproducibility\n    rng = np.random.default_rng(seed)\n\n    delta_c_gauss = estimate_critical_delta(\n        'gaussian', n, k, m_candidates, T, q, tau, eps_denom, rng\n    )\n    delta_c_fourier = estimate_critical_delta(\n        'fourier', n, k, m_candidates, T, q, tau, eps_denom, rng\n    )\n\n    if np.isnan(delta_c_gauss) or np.isnan(delta_c_fourier):\n        return False\n        \n    return abs(delta_c_gauss - delta_c_fourier) = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Success criterion parameters\n    tau = 1e-6\n    eps_denom = 1e-12\n\n    # Test suite from the problem statement.\n    test_cases = [\n        {'n': 64, 'k': 4, 'm_candidates': [12, 14, 16, 18, 20, 22, 24], 'T': 8, 'q': 0.75, 'epsilon': 0.15, 'seed': 0},\n        {'n': 96, 'k': 8, 'm_candidates': [24, 28, 32, 36, 40, 44], 'T': 6, 'q': 0.67, 'epsilon': 0.20, 'seed': 1},\n        {'n': 128, 'k': 16, 'm_candidates': [36, 44, 52, 60, 68, 76, 84], 'T': 5, 'q': 0.60, 'epsilon': 0.20, 'seed': 2},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_single_test_case(\n            case['n'], case['k'], case['m_candidates'], case['T'],\n            case['q'], case['epsilon'], tau, eps_denom, case['seed']\n        )\n        # Convert Python booleans (True/False) to JSON-style strings (true/false)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}