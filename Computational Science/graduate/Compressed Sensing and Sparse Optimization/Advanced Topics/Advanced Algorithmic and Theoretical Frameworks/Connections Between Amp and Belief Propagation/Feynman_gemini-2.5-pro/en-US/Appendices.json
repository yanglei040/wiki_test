{
    "hands_on_practices": [
        {
            "introduction": "Belief Propagation (BP) provides a general framework for inference on graphical models, but its application to the dense factor graphs found in compressed sensing is often computationally prohibitive. Approximate Message Passing (AMP) emerges as a powerful simplification of BP for this setting, offering significant computational savings. This exercise  allows you to directly quantify this advantage by calculating the scaling of per-iteration complexity for naive loopy BP and comparing it to the linear complexity of AMP.",
            "id": "3438006",
            "problem": "Consider the standard linear observation model in compressed sensing, given by $y = Ax + w$, where $A \\in \\mathbb{R}^{m \\times n}$ is a dense sensing matrix, $x \\in \\mathbb{R}^{n}$ is an unknown signal with a separable prior, and $w \\in \\mathbb{R}^{m}$ is additive white Gaussian noise. The corresponding factor graph is bipartite with $m$ measurement-factor nodes (indexed by $a \\in \\{1,\\ldots,m\\}$) and $n$ variable nodes (indexed by $i \\in \\{1,\\ldots,n\\}$), where every factor node connects to every variable node (i.e., the graph is the complete bipartite graph with $mn$ edges).\n\nNaive loopy Belief Propagation (BP) (sum-product) on this graph sends two directed messages along every edge per iteration: a variable-to-factor message and a factor-to-variable message. Assume each message is parameterized in the natural domain so that combining messages from neighbors amounts to simple scalar additions of natural parameters. Define one unit of computational cost as one scalar neighbor-aggregation (i.e., the addition of one incoming scalar message into an accumulator). You may ignore all constant factors that do not scale with the node degrees.\n\n1) Using only the fundamental BP rule that each outgoing message on an edge excludes the corresponding incoming message from that edge and aggregates over all other neighbors, compute the exact number of scalar neighbor-aggregations performed per iteration by naive loopy BP on this dense graph as a function of $m$ and $n$.\n\n2) Approximate Message Passing (AMP) performs per-iteration updates that can be implemented using exactly 2 dense matrix-vector multiplications with $A$ and $A^{\\top}$. Under the same unit-cost model (one scalar multiply-accumulate contribution along an edge counts as one aggregation), the per-iteration cost of AMP is exactly $2mn$ aggregations. Compute the exact ratio $R(m,n)$ between the per-iteration aggregation counts of naive loopy BP and AMP.\n\nProvide your final answer as a closed-form expression for $R(m,n)$. No rounding is required, and no physical units are involved. Express your final answer as a single analytic expression.",
            "solution": "The user wants to find the ratio of computational costs per iteration between naive loopy Belief Propagation (BP) and Approximate Message Passing (AMP) for a specific compressed sensing problem.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the given information and validate the problem statement.\n\n**Extracted Givens:**\n-   **Model:** A linear observation model $y = Ax + w$, where $A \\in \\mathbb{R}^{m \\times n}$ is a dense sensing matrix, $x \\in \\mathbb{R}^{n}$ is the signal with a separable prior, and $w \\in \\mathbb{R}^{m}$ is additive white Gaussian noise.\n-   **Factor Graph:** A a complete bipartite graph with $m$ factor nodes and $n$ variable nodes. This means every factor node is connected to every variable node.\n-   **Algorithm 1 (BP):** Naive loopy Belief Propagation (sum-product). An iteration involves computing two messages (variable-to-factor and factor-to-variable) along each of the $mn$ edges.\n-   **BP Update Rule:** An outgoing message on an edge is computed by aggregating messages from all other neighboring nodes, excluding the one connected by the target edge.\n-   **Algorithm 2 (AMP):** Approximate Message Passing. Its updates can be implemented using exactly 2 dense matrix-vector multiplications ($A$ and $A^{\\top}$).\n-   **Cost Model:**\n    -   The unit of computational cost is defined as one \"scalar neighbor-aggregation\", which is specified as \"the addition of one incoming scalar message into an accumulator\".\n    -   For BP, messages are parameterized such that combination is equivalent to scalar additions of the parameters.\n    -   For AMP, the cost model is specified such that \"one scalar multiply-accumulate contribution along an edge counts as one aggregation\".\n    -   The total per-iteration cost of AMP is explicitly given as $2mn$ aggregations.\n-   **Goal:** Compute the ratio $R(m,n) = \\frac{\\text{Cost}_{\\text{BP}}}{\\text{Cost}_{\\text{AMP}}}$.\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem is set in the well-established field of compressed sensing and graphical models. The relationship between loopy BP on dense graphs and the AMP algorithm is a fundamental topic in modern statistical signal processing. The model and algorithms are standard.\n2.  **Well-Posed:** The problem provides a clear objective and a set of definitions (graph structure, algorithms, cost metric) sufficient to derive a unique analytical solution.\n3 The **Objective:** The language is formal, precise, and free of subjective content.\n\nThe problem states that the signal model involves $x \\in \\mathbb{R}^n$ and Gaussian noise, which typically implies that BP messages would be Gaussian distributions, each parameterized by two scalars (e.g., mean and variance). However, the cost unit is defined as \"the addition of one incoming **scalar message**\". While this might seem contradictory, a valid interpretation is that the problem deliberately simplifies the message structure to a single scalar for the purpose of this idealized computational cost analysis. This is a common practice in theoretical exercises to focus on the scaling laws. The problem statement's specific rules for the calculation take precedence over the background context. Under this interpretation, the problem is self-consistent and solvable.\n\n**Verdict:** The problem is valid. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe solution requires calculating the per-iteration cost of naive loopy BP based on the provided cost model and then dividing it by the given cost of AMP.\n\n**Part 1: Computational Cost of Naive Loopy BP ($C_{\\text{BP}}$)**\n\nAn iteration of loopy BP on this graph consists of updating all variable-to-factor messages and all factor-to-variable messages. The graph is a complete bipartite graph with $mn$ edges.\n\n**A. Cost of Variable-to-Factor (V-to-F) Messages:**\n-   Consider a single variable node $x_i$, where $i \\in \\{1, \\ldots, n\\}$. In the complete bipartite graph, it is connected to all $m$ factor nodes.\n-   To compute the outgoing message from variable node $x_i$ to a specific factor node $f_a$, the BP rule requires aggregating the incoming messages from all other factor nodes, i.e., $\\{f_b\\}_{b \\neq a}$.\n-   The number of such neighbors to aggregate is $m-1$.\n-   The cost unit is one scalar aggregation per incoming message. Therefore, the cost to compute one V-to-F message is $m-1$ units.\n-   There are $n$ variable nodes, and each sends messages to all $m$ factor nodes. The total number of V-to-F messages is $nm$.\n-   The total cost for all V-to-F messages in one iteration is the product of the number of messages and the cost per message:\n$$ C_{\\text{V-to-F}} = (\\text{number of messages}) \\times (\\text{cost per message}) = (nm) \\times (m-1) $$\n\n**B. Cost of Factor-to-Variable (F-to-V) Messages:**\n-   Consider a single factor node $f_a$, where $a \\in \\{1, \\ldots, m\\}$. In the complete bipartite graph, it is connected to all $n$ variable nodes.\n-   To compute the outgoing message from factor node $f_a$ to a specific variable node $x_i$, the BP rule requires aggregating the incoming messages from all other variable nodes, i.e., $\\{x_j\\}_{j \\neq i}$.\n-   The number of such neighbors to aggregate is $n-1$.\n-   Following the same cost model, the cost to compute one F-to-V message is $n-1$ units.\n-   There are $m$ factor nodes, and each sends messages to all $n$ variable nodes. The total number of F-to-V messages is $mn$.\n-   The total cost for all F-to-V messages in one iteration is:\n$$ C_{\\text{F-to-V}} = (\\text{number of messages}) \\times (\\text{cost per message}) = (mn) \\times (n-1) $$\n\n**C. Total BP Cost:**\n-   The total cost per iteration for naive loopy BP is the sum of the costs for V-to-F and F-to-V message updates.\n$$ C_{\\text{BP}} = C_{\\text{V-to-F}} + C_{\\text{F-to-V}} = nm(m-1) + mn(n-1) $$\n-   Factoring out the common term $mn$:\n$$ C_{\\text{BP}} = mn \\left( (m-1) + (n-1) \\right) = mn(m+n-2) $$\n\n**Part 2: Ratio of BP Cost to AMP Cost, $R(m,n)$**\n\nThe problem states that the per-iteration cost of AMP, under the same unit-cost model, is:\n$$ C_{\\text{AMP}} = 2mn $$\n\nThe ratio $R(m,n)$ is defined as the cost of naive BP divided by the cost of AMP.\n$$ R(m,n) = \\frac{C_{\\text{BP}}}{C_{\\text{AMP}}} = \\frac{mn(m+n-2)}{2mn} $$\n\nAssuming $m \\ge 1$ and $n \\ge 1$, the term $mn$ is non-zero and can be canceled from the numerator and denominator.\n$$ R(m,n) = \\frac{m+n-2}{2} $$\n\nThis result quantifies the significant computational advantage of AMP over naive loopy BP on dense graphs. While BP has a complexity that scales with $O(m^2n + mn^2)$, AMP achieves a complexity of $O(mn)$. The ratio $R(m,n)$ grows linearly with the problem dimensions $m$ and $n$.",
            "answer": "$$ \\boxed{\\frac{m+n-2}{2}} $$"
        },
        {
            "introduction": "A remarkable feature of AMP is that its performance in the large-system limit can be precisely tracked by a simple scalar recursion known as State Evolution (SE). This powerful analytical tool allows for exact predictions of quantities like mean-squared error without running the full algorithm. This practice problem  explores the concept of \"universality\" by tasking you with deriving and analyzing the SE for an AMP algorithm that uses a mismatched prior, demonstrating the robustness of the SE framework.",
            "id": "3437997",
            "problem": "Consider a linear observation model in compressed sensing: a vector of measurements $y \\in \\mathbb{R}^{m}$ is generated from an unknown signal $x_{0} \\in \\mathbb{R}^{n}$ via $y = Ax_{0} + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, the noise vector $w \\in \\mathbb{R}^{m}$ has independent entries $w_{i} \\sim \\mathcal{N}(0, \\sigma_{w}^{2})$, and the unknown signal components are independent with $x_{0,i} \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$. Let the undersampling ratio be $\\delta = m/n \\in (0, \\infty)$ with $m, n \\to \\infty$ at fixed $\\delta$.\n\nAn Approximate Message Passing (AMP) algorithm can be derived from Belief Propagation (BP) on the dense factor graph of this linear model by invoking the Central Limit Theorem (CLT) and Gaussian approximations to high-dimensional messages. In particular, AMP iterations take the form of a separable denoising of an effective scalar channel $u = x + \\tau z$ at each iteration, where $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$, $z \\sim \\mathcal{N}(0,1)$, and $\\tau^{2}$ denotes an effective noise variance predicted by a one-dimensional recursion known as state evolution. Suppose the AMP algorithm is run with a mismatched Gaussian prior $\\mathcal{N}(0, \\sigma_{q}^{2})$ (rather than the true $\\mathcal{N}(0, \\sigma_{x}^{2})$), so that the scalar denoiser at effective noise variance $\\tau^{2}$ is taken to be the posterior mean under the prior $\\mathcal{N}(0, \\sigma_{q}^{2})$ and the observation model $u = x + \\tau z$.\n\nTasks:\n1. Starting from the BP formulation on the dense linear Gaussian model and using only the CLT to justify Gaussian approximations to aggregate messages, derive the one-dimensional state evolution recursion tracking the effective variance $\\tau_{t}^{2}$ for AMP with a separable scalar denoiser $\\eta(\\cdot; \\tau^{2})$ induced by the mismatched Gaussian prior. Your derivation should make precise why, in the large-system limit, the AMP error at iteration $t$ is distributed as a scalar Gaussian channel $U = X + \\tau_{t} Z$ with $X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$, $Z \\sim \\mathcal{N}(0,1)$ independent, and why the next-iteration effective variance $\\tau_{t+1}^{2}$ is obtained by composing the mean squared error of this scalar denoiser with the measurement ratio $\\delta$ and the measurement noise variance $\\sigma_{w}^{2}$.\n2. Using Bayes' rule for the mismatched Gaussian prior $\\mathcal{N}(0, \\sigma_{q}^{2})$ and the scalar observation model $u = x + \\tau z$, derive the posterior mean denoiser $\\eta(u; \\tau^{2})$ in closed form as a function of $u$, $\\sigma_{q}^{2}$, and $\\tau^{2}$.\n3. Combine your results from parts $1$ and $2$ to obtain, in closed form, the fixed-point equation for the effective variance $\\tau_{\\star}^{2}$ in the noiseless case $\\sigma_{w}^{2} = 0$, in terms of $\\delta$, $\\sigma_{x}^{2}$, and $\\sigma_{q}^{2}$. Then, specialize to the parameter setting $\\delta = 1$, $\\sigma_{x}^{2} = 3$, $\\sigma_{q}^{2} = 1$, and compute the strictly positive fixed-point value of $\\tau_{\\star}^{2}$.\n\nYour final answer must be the single real number equal to this fixed-point value. No units are involved. Do not provide any additional commentary with the final answer.",
            "solution": "The problem asks for the derivation of the state evolution for a mismatched Approximate Message Passing (AMP) algorithm, the specific form of the mismatched denoiser, and the computation of a fixed point of the resulting recursion for a specific set of parameters. We proceed in three parts as requested.\n\n### Part 1: Derivation of the State Evolution Recursion\n\nThe Approximate Message Passing (AMP) algorithm provides an iterative estimate $x^t$ of the signal $x_0$. Its behavior in the large system limit ($m, n \\to \\infty$ with $m/n \\to \\delta$) is precisely characterized by a scalar recursion known as state evolution (SE). This derivation relies on approximating sums of many weakly dependent random variables as Gaussian, justified by the Central Limit Theorem (CLT).\n\nThe core hypothesis of state evolution is that at each iteration $t$, the components of the vector entering the denoising function, $u^t$, are statistically equivalent to samples from a scalar Gaussian channel. Specifically, for each component $i \\in \\{1, \\dots, n\\}$, the effective observation $u_i^t$ is distributed as if $u_i^t = x_{0,i} + \\tau_t z_i$, where $x_{0,i}$ is the true signal component, $z_i \\sim \\mathcal{N}(0, 1)$ is a standard Gaussian random variable independent of $x_{0,i}$, and $\\tau_t^2$ is the effective noise variance at iteration $t$. The signal components $x_{0,i}$ are i.i.d. draws from $\\mathcal{N}(0, \\sigma_x^2)$.\n\nThe AMP update for the estimate $x^{t+1}$ is given by applying a scalar denoising function $\\eta(\\cdot; \\tau_t^2)$ component-wise to this effective observation:\n$$\nx_i^{t+1} = \\eta(u_i^t; \\tau_t^2) = \\eta(x_{0,i} + \\tau_t z_i; \\tau_t^2)\n$$\nThe state evolution recursion describes how the effective variance $\\tau_t^2$ evolves from one iteration to the next. The noise in iteration $t+1$ is constructed from the residual of iteration $t+1$. The standard AMP algorithm includes an \"Onsager\" correction term which ensures that the effective noise added at each stage is asymptotically Gaussian and independent of the signal. The effective noise for stage $t+1$ can be rigorously shown to be related to the vector $A^T z^{t+1}$, where $z^{t+1}$ is a residual-like term. For our purposes, we will analyze the statistical properties of this term.\n\nLet's define the error in the estimate at iteration $t+1$ as $d_i^{t+1} = x_i^{t+1} - x_{0,i} = \\eta(x_{0,i} + \\tau_t z_i; \\tau_t^2) - x_{0,i}$. The mean squared error (MSE) is then given by:\n$$\n\\text{MSE}(\\tau_t^2) = \\mathbb{E}[(d_i^{t+1})^2] = \\mathbb{E}_{X,Z}[(\\eta(X + \\tau_t Z; \\tau_t^2) - X)^2]\n$$\nwhere the expectation is over $X \\sim \\mathcal{N}(0, \\sigma_x^2)$ and $Z \\sim \\mathcal{N}(0, 1)$.\n\nThe next effective noise variance, $\\tau_{t+1}^2$, is the variance of the components of $A^T(y - Ax^{t+1})$. Let's analyze the vector $v^{t+1} = y - Ax^{t+1}$. Substituting $y = Ax_0 + w$:\n$$\nv^{t+1} = Ax_0 + w - Ax^{t+1} = A(x_0 - x^{t+1}) + w = -Ad^{t+1} + w\n$$\nA component $v_j^{t+1}$ of this vector is given by:\n$$\nv_j^{t+1} = -\\sum_{k=1}^n A_{jk} d_k^{t+1} + w_j\n$$\nIn the large system limit, the terms $d_k^{t+1}$ are i.i.d. random variables (by the SE hypothesis) with mean $\\mathbb{E}[d_k^{t+1}]$ and variance $\\text{MSE}(\\tau_t^2)$. The entries $A_{jk}$ are i.i.d. $\\mathcal{N}(0, 1/m)$. The term $\\sum_{k=1}^n A_{jk} d_k^{t+1}$ is a sum of $n$ i.i.d. random variables. By the CLT, this sum converges to a Gaussian random variable. Its variance is:\n$$\n\\text{Var}\\left(\\sum_{k=1}^n A_{jk} d_k^{t+1}\\right) = n \\cdot \\mathbb{E}[A_{jk}^2] \\cdot \\mathbb{E}[(d_k^{t+1})^2] = n \\cdot \\frac{1}{m} \\cdot \\text{MSE}(\\tau_t^2) = \\frac{1}{\\delta} \\text{MSE}(\\tau_t^2)\n$$\n(Here we used that $\\mathbb{E}[d_k^{t+1}]$ can be assumed to be zero without loss of generality, which holds for the posterior mean estimator). Since $w_j$ is independent of $A$ and $d^{t+1}$, the variance of $v_j^{t+1}$ is:\n$$\n\\text{Var}(v_j^{t+1}) = \\text{Var}\\left(-\\sum_k A_{jk} d_k^{t+1}\\right) + \\text{Var}(w_j) = \\frac{1}{\\delta} \\text{MSE}(\\tau_t^2) + \\sigma_w^2\n$$\nThe effective noise for the next iteration, which determines $\\tau_{t+1}^2$, is given by the components of the vector $h^{t+1} = A^T v^{t+1}$. A component $h_i^{t+1}$ is:\n$$\nh_i^{t+1} = \\sum_{j=1}^m A_{ji} v_j^{t+1}\n$$\nAgain, by the CLT, since $v_j^{t+1}$ are i.i.d. Gaussian variables, $h_i^{t+1}$ is also Gaussian. Its variance is $\\tau_{t+1}^2$:\n$$\n\\tau_{t+1}^2 = \\text{Var}(h_i^{t+1}) = \\sum_{j=1}^m \\mathbb{E}[A_{ji}^2] \\mathbb{E}[(v_j^{t+1})^2] = m \\cdot \\frac{1}{m} \\cdot \\text{Var}(v_j^{t+1}) = \\text{Var}(v_j^{t+1})\n$$\nTherefore, the state evolution recursion for the effective variance is:\n$$\n\\tau_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}_{X,Z}[(\\eta(X + \\tau_t Z; \\tau_t^2) - X)^2]\n$$\nwhere $X \\sim \\mathcal{N}(0, \\sigma_x^2)$ and $Z \\sim \\mathcal{N}(0, 1)$. This explains why the next-iteration variance is a function of the measurement noise $\\sigma_w^2$, the measurement ratio $\\delta$, and the mean squared error of the scalar denoiser.\n\n### Part 2: Derivation of the Mismatched Gaussian Denoiser\n\nThe denoiser $\\eta(u; \\tau^2)$ is the posterior mean $\\mathbb{E}[x|u]$ for the scalar observation model $u = x + \\tau z$, where $z \\sim \\mathcal{N}(0,1)$, under the assumption that the prior on $x$ is $\\mathcal{N}(0, \\sigma_q^2)$.\nThe likelihood is $p(u|x) = \\mathcal{N}(u|x, \\tau^2)$ and the prior is $p(x) = \\mathcal{N}(x|0, \\sigma_q^2)$.\nUsing Bayes' rule, the posterior distribution $p(x|u)$ is proportional to the product of the likelihood and the prior:\n$$\np(x|u) \\propto p(u|x) p(x) \\propto \\exp\\left(-\\frac{(u-x)^2}{2\\tau^2}\\right) \\exp\\left(-\\frac{x^2}{2\\sigma_q^2}\\right)\n$$\nWe analyze the exponent, ignoring terms not dependent on $x$:\n$$\n-\\frac{1}{2}\\left(\\frac{u^2 - 2ux + x^2}{\\tau^2} + \\frac{x^2}{\\sigma_q^2}\\right) = -\\frac{1}{2}\\left(x^2\\left(\\frac{1}{\\tau^2} + \\frac{1}{\\sigma_q^2}\\right) - 2x\\frac{u}{\\tau^2}\\right) + \\text{const}\n$$\nThis is a quadratic form in $x$, indicating that the posterior is Gaussian, $p(x|u) = \\mathcal{N}(x|\\mu_{post}, \\sigma_{post}^2)$. The posterior variance $\\sigma_{post}^2$ is found from the coefficient of $x^2$:\n$$\n\\frac{1}{\\sigma_{post}^2} = \\frac{1}{\\tau^2} + \\frac{1}{\\sigma_q^2} = \\frac{\\sigma_q^2+\\tau^2}{\\tau^2\\sigma_q^2} \\implies \\sigma_{post}^2 = \\frac{\\tau^2\\sigma_q^2}{\\sigma_q^2+\\tau^2}\n$$\nThe posterior mean $\\mu_{post}$ is found by matching the coefficient of the linear term in $x$:\n$$\n\\frac{\\mu_{post}}{\\sigma_{post}^2} = \\frac{u}{\\tau^2} \\implies \\mu_{post} = u \\frac{\\sigma_{post}^2}{\\tau^2} = u \\left(\\frac{\\tau^2\\sigma_q^2}{\\sigma_q^2+\\tau^2}\\right) \\frac{1}{\\tau^2} = \\frac{\\sigma_q^2}{\\sigma_q^2+\\tau^2} u\n$$\nThus, the scalar denoiser is the linear shrinkage function:\n$$\n\\eta(u; \\tau^2) = \\frac{\\sigma_q^2}{\\sigma_q^2 + \\tau^2} u\n$$\n\n### Part 3: Fixed-Point Equation and Solution\n\nWe combine the results from the first two parts to find the fixed-point equation for the effective variance $\\tau_\\star^2$. A fixed point satisfies $\\tau_\\star^2 = \\tau_{t+1}^2 = \\tau_t^2$.\nWith $\\sigma_w^2 = 0$, the SE recursion is $\\tau^2 = \\frac{1}{\\delta} \\text{MSE}(\\tau^2)$. Let us compute the MSE:\n$$\n\\text{MSE}(\\tau^2) = \\mathbb{E}[(\\eta(X + \\tau Z; \\tau^2) - X)^2]\n$$\nSubstitute the denoiser: $\\eta(X + \\tau Z; \\tau^2) = \\frac{\\sigma_q^2}{\\sigma_q^2 + \\tau^2}(X + \\tau Z)$.\nThe error is:\n$$\n\\eta(X + \\tau Z; \\tau^2) - X = \\left(\\frac{\\sigma_q^2}{\\sigma_q^2 + \\tau^2} - 1\\right)X + \\frac{\\sigma_q^2 \\tau}{\\sigma_q^2 + \\tau^2} Z = -\\frac{\\tau^2}{\\sigma_q^2 + \\tau^2} X + \\frac{\\sigma_q^2 \\tau}{\\sigma_q^2 + \\tau^2} Z\n$$\nSince $X \\sim \\mathcal{N}(0, \\sigma_x^2)$ and $Z \\sim \\mathcal{N}(0, 1)$ are independent, $\\mathbb{E}[XZ] = 0$. The MSE is the sum of the variances of the two terms:\n$$\n\\text{MSE}(\\tau^2) = \\mathbb{E}\\left[\\left(-\\frac{\\tau^2}{\\sigma_q^2 + \\tau^2} X\\right)^2\\right] + \\mathbb{E}\\left[\\left(\\frac{\\sigma_q^2 \\tau}{\\sigma_q^2 + \\tau^2} Z\\right)^2\\right]\n$$\n$$\n\\text{MSE}(\\tau^2) = \\left(\\frac{\\tau^2}{\\sigma_q^2 + \\tau^2}\\right)^2 \\mathbb{E}[X^2] + \\left(\\frac{\\sigma_q^2 \\tau}{\\sigma_q^2 + \\tau^2}\\right)^2 \\mathbb{E}[Z^2]\n$$\nUsing $\\mathbb{E}[X^2]=\\sigma_x^2$ and $\\mathbb{E}[Z^2]=1$:\n$$\n\\text{MSE}(\\tau^2) = \\frac{(\\tau^2)^2 \\sigma_x^2 + (\\sigma_q^2)^2 \\tau^2}{(\\sigma_q^2 + \\tau^2)^2}\n$$\nThe fixed-point equation $\\tau_\\star^2 = \\frac{1}{\\delta} \\text{MSE}(\\tau_\\star^2)$ becomes:\n$$\n\\tau_\\star^2 = \\frac{1}{\\delta} \\frac{(\\tau_\\star^2)^2 \\sigma_x^2 + (\\sigma_q^2)^2 \\tau_\\star^2}{(\\sigma_q^2 + \\tau_\\star^2)^2}\n$$\nFor a strictly positive fixed point $\\tau_\\star^2 > 0$, we can divide by $\\tau_\\star^2$:\n$$\n1 = \\frac{1}{\\delta} \\frac{\\tau_\\star^2 \\sigma_x^2 + (\\sigma_q^2)^2}{(\\sigma_q^2 + \\tau_\\star^2)^2}\n$$\n$$\n\\delta (\\sigma_q^2 + \\tau_\\star^2)^2 = \\tau_\\star^2 \\sigma_x^2 + (\\sigma_q^2)^2\n$$\nNow we substitute the given parameter values: $\\delta = 1$, $\\sigma_x^2 = 3$, and $\\sigma_q^2 = 1$. Let $T = \\tau_\\star^2$:\n$$\n1 \\cdot (1 + T)^2 = T \\cdot 3 + 1^2\n$$\n$$\n1 + 2T + T^2 = 3T + 1\n$$\n$$\nT^2 - T = 0\n$$\n$$\nT(T-1) = 0\n$$\nThe possible solutions for $T = \\tau_\\star^2$ are $0$ and $1$. The problem asks for the strictly positive fixed-point value.\nTherefore, the strictly positive fixed point is $\\tau_\\star^2 = 1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "As a close relative of loopy Belief Propagation, the AMP algorithm can sometimes exhibit oscillatory or divergent behavior, failing to converge to a solution. A common and effective remedy for this instability is \"damping,\" or under-relaxation of the iterative updates. This exercise  demonstrates how the State Evolution framework can be used not only to diagnose instability but also to analyze the effect of damping and guide the selection of a parameter that guarantees stable convergence.",
            "id": "3437999",
            "problem": "Consider a standard compressed sensing model with a dense random sensing matrix: measurements are generated as $y = Ax_{0} + w$, where the sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, the signal $x_{0} \\in \\mathbb{R}^{n}$ has independent and identically distributed components with prior $P_{X}$, and the noise $w \\in \\mathbb{R}^{m}$ has independent and identically distributed entries $w_{i} \\sim \\mathcal{N}(0, \\sigma_{w}^{2})$. Let the measurement rate be $\\delta = m/n \\in (0, \\infty)$.\n\nApproximate Message Passing (AMP) can be understood as a high-dimensional limit of loopy Belief Propagation (BP) on the dense factor graph induced by $A$, with Gaussian approximations of the cavity messages and Onsager corrections. This limit induces an exact scalar decoupling in the large system limit, described by State Evolution (SE). For a Lipschitz-continuous denoiser $\\eta: \\mathbb{R} \\to \\mathbb{R}$ (with Lipschitz constant $L$ in a neighborhood of the operating point), the SE recursion for the asymptotic mean-squared error can be written as the iteration on the nonnegative sequence $\\{v_{t}\\}_{t \\ge 0}$:\n$$\nv_{t+1} = F(v_{t}) \\equiv \\sigma_{w}^{2} + \\frac{1}{\\delta} \\,\\mathbb{E}\\!\\left[\\big(\\eta\\!\\left(X + \\sqrt{v_{t}}\\,Z\\right) - X\\big)^{2}\\right],\n$$\nwhere $X \\sim P_{X}$, $Z \\sim \\mathcal{N}(0,1)$, and the expectation is over the joint law of $(X,Z)$. Assume that there exists a fixed point $v^{\\ast} \\ge 0$ of the undamped SE, that is, $v^{\\ast} = F(v^{\\ast})$, and that $F$ is differentiable at $v^{\\ast}$ with derivative $a \\equiv F'(v^{\\ast}) \\in \\mathbb{R}$. The Lipschitz property of $\\eta$ guarantees, under standard dominated convergence arguments for SE, that $F$ is locally Lipschitz and differentiable almost everywhere in a neighborhood of $v^{\\ast}$.\n\nIn practice, both AMP and its loopy Belief Propagation ancestor are often stabilized by damping, i.e., under-relaxation of message or state updates. Consider the damped SE defined by\n$$\nv_{t+1} = (1-\\lambda)\\,v_{t} + \\lambda\\,F(v_{t}),\n$$\nwith a damping parameter $\\lambda \\in (0,1]$.\n\nTasks:\n1) Starting from the SE definition above and using only the differentiability of $F$ at $v^{\\ast}$, derive the linearized iteration mapping around $v^{\\ast}$ for the undamped and damped SE, and express the effective linearization factor for the damped SE in terms of $a$ and $\\lambda$.\n\n2) Viewing AMP as a high-dimensional Gaussian-approximate limit of loopy Belief Propagation, explain briefly how damping at the message level corresponds to damping in SE, and why the local linearization factor governs the stability of oscillatory dynamics near $v^{\\ast}$.\n\n3) Suppose that for a particular Lipschitz denoiser and signal prior, the undamped SE linearization slope at the fixed point is $a = -\\tfrac{3}{2}$. Determine the largest damping parameter $\\lambda \\in (0,1]$ that guarantees local linear convergence of the damped SE to $v^{\\ast}$, i.e., that ensures that the magnitude of the damped linearization factor is strictly less than $1$. Express your final answer as an exact fraction. If you choose to approximate, round your result to four significant figures.",
            "solution": "The problem requires an analysis of the Approximate Message Passing (AMP) algorithm's State Evolution (SE) dynamics, focusing on linearization, the role of damping, and the calculation of a stability-ensuring damping parameter.\n\n**Task 1: Linearization of State Evolution Dynamics**\n\nThe dynamics of the system are described by an iterative map on the mean-squared error, $v_t$. We are given a fixed point $v^{\\ast}$ such that $v^{\\ast} = F(v^{\\ast})$, where $F$ is the SE function. The derivative of $F$ at this point is given as $F'(v^{\\ast}) = a$.\n\nFirst, we analyze the undamped SE, given by the recursion:\n$$\nv_{t+1} = F(v_{t})\n$$\nTo study the local stability around the fixed point $v^{\\ast}$, we consider a small perturbation $\\epsilon_{t}$ such that $v_{t} = v^{\\ast} + \\epsilon_{t}$. Substituting this into the recursion and applying a first-order Taylor expansion for $F(v_t)$ around $v^{\\ast}$ yields:\n$$\nv^{\\ast} + \\epsilon_{t+1} = F(v^{\\ast} + \\epsilon_{t}) \\approx F(v^{\\ast}) + F'(v^{\\ast})\\epsilon_{t}\n$$\nUsing the fixed-point condition $v^{\\ast} = F(v^{\\ast})$, the equation for the perturbation simplifies to:\n$$\n\\epsilon_{t+1} \\approx F'(v^{\\ast})\\epsilon_{t}\n$$\nThe linearized iteration mapping for the undamped case is thus $\\epsilon_{t+1} \\approx a\\,\\epsilon_{t}$, and the corresponding linearization factor is $a$.\n\nNext, we analyze the damped SE, which is defined by the recursion:\n$$\nv_{t+1} = (1-\\lambda)v_{t} + \\lambda\\,F(v_{t})\n$$\nwhere $\\lambda \\in (0,1]$ is the damping parameter. We can define the damped iteration map as $G_{\\lambda}(v) = (1-\\lambda)v + \\lambda F(v)$. It is straightforward to verify that $v^{\\ast}$ is also a fixed point of this damped map:\n$$\nG_{\\lambda}(v^{\\ast}) = (1-\\lambda)v^{\\ast} + \\lambda F(v^{\\ast}) = (1-\\lambda)v^{\\ast} + \\lambda v^{\\ast} = v^{\\ast}\n$$\nTo find the linearization factor for the damped system, we differentiate $G_{\\lambda}(v)$ with respect to $v$:\n$$\nG'_{\\lambda}(v) = \\frac{d}{dv}\\left[ (1-\\lambda)v + \\lambda F(v) \\right] = (1-\\lambda) + \\lambda F'(v)\n$$\nEvaluating this derivative at the fixed point $v^{\\ast}$ gives the effective linearization factor for the damped SE, which we denote $a_{\\text{damped}}$:\n$$\na_{\\text{damped}} = G'_{\\lambda}(v^{\\ast}) = 1-\\lambda + \\lambda F'(v^{\\ast}) = 1-\\lambda + \\lambda a\n$$\nThe linearized iteration for the damped dynamics is $\\epsilon_{t+1} \\approx (1-\\lambda + \\lambda a)\\epsilon_{t}$.\n\n**Task 2: Conceptual Connection between BP and SE Damping**\n\nApproximate Message Passing can be understood as an algorithmic simplification of loopy Belief Propagation (BP) applied to the dense factor graph defined by the sensing matrix $A$. In the large system limit ($m, n \\to \\infty$), the central limit theorem justifies approximating the distributions of messages passed in BP as Gaussian. AMP operates on the means and variances of these messages. State Evolution is the deterministic map that describes the evolution of these distributional parameters (specifically, the variance, which corresponds to the MSE $v_t$) from one iteration to the next.\n\nOn dense graphs with many short loops, standard BP is often unstable, exhibiting oscillatory or divergent behavior. A common heuristic to improve convergence is damping, where the message update at iteration $t+1$ is a convex combination of the newly computed message and the message from the previous iteration $t$. This under-relaxation prevents the updates from overshooting and helps stabilize the algorithm.\n\nSince SE tracks the macroscopic state (i.e., the MSE $v_t$) of an algorithm whose microscopic message-passing dynamics are being damped, the damping appears at the SE level as well. The undamped map $v_{t+1} = F(v_t)$ represents the full update to the MSE after one iteration. Damping with parameter $\\lambda$ means the system's state $v_t$ moves only a fraction $\\lambda$ of the way towards the new state $F(v_t)$. This is precisely captured by the damped SE update rule $v_{t+1} = (1-\\lambda)v_{t} + \\lambda F(v_{t})$.\n\nThe local stability of any such iterative map near a fixed point is governed by its linearization. The iteration converges if the magnitude of the linearization factor is strictly less than $1$. For damped SE, this factor is $a_{\\text{damped}} = 1 - \\lambda + \\lambda a$. A negative value of $a$ indicates that the undamped system tends to oscillate by overshooting the fixed point. If $a < -1$, these oscillations are amplified. Damping provides a mechanism to control this behavior by modifying the effective linearization factor, potentially bringing its magnitude below $1$ to ensure convergence.\n\n**Task 3: Calculation of the Damping Parameter**\n\nWe are given that the undamped linearization slope at the fixed point is $a = -\\frac{3}{2}$. We must find the largest damping parameter $\\lambda \\in (0,1]$ that guarantees local linear convergence. This requires the magnitude of the damped linearization factor, $|a_{\\text{damped}}|$, to be strictly less than $1$.\n\nThe expression for $a_{\\text{damped}}$ is:\n$$\na_{\\text{damped}} = 1 - \\lambda + \\lambda a = 1 - \\lambda + \\lambda \\left(-\\frac{3}{2}\\right) = 1 - \\frac{5}{2}\\lambda\n$$\nThe condition for local linear convergence is $|a_{\\text{damped}}| < 1$, which is equivalent to the compound inequality:\n$$\n-1 < 1 - \\frac{5}{2}\\lambda < 1\n$$\nWe solve this inequality for $\\lambda$. The right-hand side, $1 - \\frac{5}{2}\\lambda < 1$, implies $-\\frac{5}{2}\\lambda < 0$, which gives $\\lambda > 0$. This is consistent with the constraint $\\lambda \\in (0,1]$.\n\nThe left-hand side is:\n$$\n-1 < 1 - \\frac{5}{2}\\lambda\n$$\n$$\n-2 < -\\frac{5}{2}\\lambda\n$$\nMultiplying by $-1$ reverses the inequality:\n$$\n2 > \\frac{5}{2}\\lambda\n$$\n$$\n4 > 5\\lambda\n$$\n$$\n\\lambda < \\frac{4}{5}\n$$\nThe conditions for guaranteed local linear convergence are therefore $\\lambda > 0$ and $\\lambda < \\frac{4}{5}$, which can be written as $\\lambda \\in (0, \\frac{4}{5})$. The problem asks for the largest $\\lambda \\in (0,1]$ that satisfies this. The set of valid $\\lambda$ is an open interval $(0, \\frac{4}{5})$, which does not have a largest element. In such contexts, this question is standardly interpreted as finding the supremum of the set of valid parameters, which represents the critical boundary for stability. For any $\\lambda$ in this interval, convergence is guaranteed. The supremum of this set is $\\frac{4}{5}$. Any damping $\\lambda \\ge \\frac{4}{5}$ would result in $|a_{\\text{damped}}| \\geq 1$, failing the strict condition for linear convergence. Thus, the sharp upper bound is $\\frac{4}{5}$.",
            "answer": "$$\\boxed{\\frac{4}{5}}$$"
        }
    ]
}