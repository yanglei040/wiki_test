{
    "hands_on_practices": [
        {
            "introduction": "At the heart of State Evolution (SE) is its recursive formula, which predicts the effective noise variance at each step of the Approximate Message Passing (AMP) algorithm. This recursion depends critically on the Mean-Squared Error (MSE) of the chosen scalar denoiser. This practice focuses on that core relationship by tasking you with optimizing a denoiser's threshold to minimize its MSE, thereby determining the performance of the subsequent AMP iteration as predicted by SE . This exercise makes the abstract SE formula tangible and demonstrates the direct impact of denoiser design.",
            "id": "3481509",
            "problem": "Consider the Approximate Message Passing (AMP) algorithm for compressed sensing with measurement ratio $\\delta \\in (0,1)$ and additive white Gaussian measurement noise of variance $\\sigma_{w}^{2}$. The state evolution (SE) tracks the effective scalar noise at iteration $t$ via\n$$\n\\tau_{t+1}^{2} \\;=\\; \\sigma_{w}^{2} \\;+\\; \\frac{1}{\\delta} \\,\\mathbb{E}\\!\\left[\\big(\\eta_{t}(X_{0}+\\tau_{t} Z) - X_{0}\\big)^{2}\\right],\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is independent of $X_{0}$, and $\\eta_{t}$ is a scalar denoiser applied componentwise.\n\nIn this problem you will determine the scalar denoising threshold that minimizes the mean-squared error (MSE) in the SE update and then use it to predict the next effective variance. Assume the following setup:\n\n- The iteration index $t$ is fixed and known quantities are $\\delta = 0.8$, $\\sigma_{w}^{2} = 1.0 \\times 10^{-2}$, and $\\tau_{t} = 0.5$.\n- The unknown signal entry is nonnegative and deterministic, $X_{0} = \\mu$, with $\\mu = 0.3989422804$.\n- The scalar denoiser is the positive soft-thresholding rule\n$$\n\\eta_{t}(u;\\lambda) \\;=\\; \\max(u-\\lambda,\\,0),\n$$\nwith scalar threshold $\\lambda \\geq 0$ to be optimized.\n\nStarting from the SE definition above and using the laws of conditional expectation for Gaussian random variables and first principles of calculus, derive the threshold $\\lambda^{\\star}$ that minimizes the MSE\n$$\n\\mathbb{E}\\!\\left[\\big(\\eta_{t}(X_{0}+\\tau_{t} Z;\\lambda) - X_{0}\\big)^{2}\\right]\n$$\nover $\\lambda \\geq 0$. Then, plug $\\lambda^{\\star}$ into the SE update to predict $\\tau_{t+1}^{2}$.\n\nCarry out all necessary derivations symbolically before substituting numerical values. Round all numerical results in your final answer to four significant figures. Provide your final answer as the two-entry row vector $\\big(\\lambda^{\\star},\\,\\tau_{t+1}^{2}\\big)$ with no units.",
            "solution": "We begin from the AMP state evolution (SE) recursion\n$$\n\\tau_{t+1}^{2} \\;=\\; \\sigma_{w}^{2} \\;+\\; \\frac{1}{\\delta} \\,\\mathbb{E}\\!\\left[\\big(\\eta_{t}(X_{0}+\\tau_{t} Z) - X_{0}\\big)^{2}\\right],\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ and, in this problem, $X_{0} = \\mu \\geq 0$ is deterministic. The denoiser is the positive soft-threshold\n$$\n\\eta_{t}(u;\\lambda) \\;=\\; \\max(u-\\lambda,\\,0), \\quad \\lambda \\geq 0.\n$$\nLet $Y = X_{0} + \\tau_{t} Z = \\mu + \\tau_{t} Z$. The MSE to be minimized over $\\lambda$ is\n$$\nR(\\lambda) \\;=\\; \\mathbb{E}\\!\\left[\\big(\\eta_{t}(Y;\\lambda) - \\mu\\big)^{2}\\right].\n$$\nSince $Y$ is Gaussian with mean $\\mu$ and variance $\\tau_{t}^{2}$, $Y \\sim \\mathcal{N}(\\mu,\\tau_{t}^{2})$. Define the event partition by the threshold:\n- If $Y \\leq \\lambda$, then $\\eta_{t}(Y;\\lambda) = 0$ and the squared error is $\\mu^{2}$.\n- If $Y  \\lambda$, then $\\eta_{t}(Y;\\lambda) = Y - \\lambda$ and the squared error is $(Y - \\lambda - \\mu)^{2} = (\\tau_{t} Z - \\lambda)^{2}$.\n\nThus we can write\n$$\nR(\\lambda) \\;=\\; \\mu^{2}\\,\\mathbb{P}(Y \\leq \\lambda) \\;+\\; \\mathbb{E}\\!\\left[(\\tau_{t} Z - \\lambda)^{2}\\,\\mathbf{1}\\{Y  \\lambda\\}\\right].\n$$\nIntroduce the standardized threshold parameter\n$$\na \\;=\\; \\frac{\\lambda - \\mu}{\\tau_{t}},\n$$\nso that $Y \\leq \\lambda$ is equivalent to $Z \\leq a$ and $Y  \\lambda$ to $Z  a$. Using $Z \\sim \\mathcal{N}(0,1)$, we denote the standard normal probability density function and cumulative distribution function by\n$$\n\\phi(z) \\;=\\; \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{z^{2}}{2}\\right), \\qquad \\Phi(z) \\;=\\; \\int_{-\\infty}^{z} \\phi(u)\\,du,\n$$\nand define the upper-tail probability $Q(z) = 1 - \\Phi(z)$. With these notations,\n$$\n\\mathbb{P}(Y \\leq \\lambda) \\;=\\; \\mathbb{P}(Z \\leq a) \\;=\\; \\Phi(a), \\qquad \\mathbb{P}(Y  \\lambda) \\;=\\; \\mathbb{P}(Z  a) \\;=\\; Q(a).\n$$\nWe also use the truncated Gaussian moment identities\n$$\n\\mathbb{E}\\!\\left[Z\\,\\mathbf{1}\\{Z  a\\}\\right] = \\phi(a), \\qquad \\mathbb{E}\\!\\left[Z^{2}\\,\\mathbf{1}\\{Z  a\\}\\right] = Q(a) + a\\,\\phi(a).\n$$\nExpanding the quadratic term and applying these identities,\n\\begin{align*}\n\\mathbb{E}\\!\\left[(\\tau_{t} Z - \\lambda)^{2}\\,\\mathbf{1}\\{Z  a\\}\\right]\n= \\mathbb{E}\\!\\left[\\big(\\tau_{t}^{2} Z^{2} - 2 \\lambda \\tau_{t} Z + \\lambda^{2}\\big)\\,\\mathbf{1}\\{Z  a\\}\\right] \\\\\n= \\tau_{t}^{2}\\,\\big(Q(a) + a\\,\\phi(a)\\big) \\;-\\; 2 \\lambda \\tau_{t}\\,\\phi(a) \\;+\\; \\lambda^{2}\\,Q(a).\n\\end{align*}\nTherefore the MSE as a function of $\\lambda$ (equivalently $a$) is\n$$\nR(\\lambda) \\;=\\; \\mu^{2}\\,\\Phi(a) \\;+\\; \\tau_{t}^{2}\\,\\big(Q(a) + a\\,\\phi(a)\\big) \\;-\\; 2 \\lambda \\tau_{t}\\,\\phi(a) \\;+\\; \\lambda^{2}\\,Q(a).\n$$\nTo find the optimal threshold, we differentiate $R(\\lambda)$ with respect to $\\lambda$ and set the derivative to zero. The derivative is given by $dR/d\\lambda = 2(\\lambda Q(a) - \\tau_t \\phi(a))$. Setting $\\frac{dR}{d\\lambda} = 0$ yields the first-order optimality condition\n$$\n\\lambda\\,Q(a) \\;=\\; \\tau_{t}\\,\\phi(a), \\qquad \\text{with} \\quad a \\;=\\; \\frac{\\lambda - \\mu}{\\tau_{t}}.\n$$\nEquivalently,\n$$\n\\lambda \\;=\\; \\tau_{t}\\,\\frac{\\phi(a)}{Q(a)} \\quad \\text{and} \\quad a \\;=\\; \\frac{\\lambda - \\mu}{\\tau_{t}}.\n$$\nEliminating $\\lambda$ gives a scalar fixed-point equation for $a$:\n$$\n\\frac{\\mu}{\\tau_{t}} \\;=\\; \\frac{\\phi(a)}{Q(a)} \\;-\\; a,\n$$\nwhere $\\phi(a)/Q(a)$ is the (upper) Mills ratio. In our numerical instance, $\\tau_{t} = 0.5$ and $\\mu = 0.3989422804$. Note that\n$$\n\\frac{\\mu}{\\tau_{t}} \\;=\\; \\frac{0.3989422804}{0.5} \\;=\\; 0.7978845608 \\;\\approx\\; \\sqrt{\\frac{2}{\\pi}}.\n$$\nAt $a = 0$, we have $\\phi(0) = \\frac{1}{\\sqrt{2\\pi}}$ and $Q(0) = 1 - \\Phi(0) = \\frac{1}{2}$, hence\n$$\n\\frac{\\phi(0)}{Q(0)} \\;-\\; 0 \\;=\\; \\frac{\\frac{1}{\\sqrt{2\\pi}}}{\\frac{1}{2}} \\;=\\; \\sqrt{\\frac{2}{\\pi}} \\;=\\; \\frac{\\mu}{\\tau_{t}}.\n$$\nThus $a^{\\star} = 0$ solves the fixed-point equation, and the optimal threshold follows from $\\lambda^{\\star} = \\tau_{t}\\,\\frac{\\phi(a^{\\star})}{Q(a^{\\star})}$ or directly from $\\lambda^{\\star} = \\mu + a^{\\star}\\tau_{t}$. Using $a^{\\star} = 0$,\n$$\n\\lambda^{\\star} \\;=\\; \\mu + 0 \\cdot \\tau_{t} \\;=\\; \\mu \\;=\\; 0.3989422804.\n$$\nWe now compute the minimized MSE $R(\\lambda^{\\star})$. We can show that $R(\\lambda^\\star) = \\mu^2 + \\frac{\\tau_t^2}{2} - 2\\mu\\tau_t \\frac{1}{\\sqrt{2\\pi}}$. Using the fact that $\\mu/\\tau_t = \\sqrt{2/\\pi}$, this simplifies to\n$$\nR(\\lambda^{\\star}) \\;=\\; \\mu^2 + \\frac{\\tau_t^2}{2} - \\mu \\tau_t \\frac{\\mu}{\\tau_t} = \\frac{\\tau_t^2}{2}.\n$$\nWith $\\tau_{t} = 0.5$, we have $\\tau_{t}^{2} = 0.25$ and hence\n$$\nR(\\lambda^{\\star}) \\;=\\; \\frac{0.25}{2} \\;=\\; 0.125.\n$$\nFinally, plug this minimized MSE into the SE update:\n$$\n\\tau_{t+1}^{2} \\;=\\; \\sigma_{w}^{2} \\;+\\; \\frac{1}{\\delta}\\,R(\\lambda^{\\star}) \\;=\\; 1.0 \\times 10^{-2} \\;+\\; \\frac{1}{0.8}\\times 0.125 \\;=\\; 0.01 \\;+\\; 0.15625 \\;=\\; 0.16625.\n$$\nRounding to four significant figures,\n$$\n\\lambda^{\\star} \\;\\approx\\; 0.3989, \\qquad \\tau_{t+1}^{2} \\;\\approx\\; 0.1663.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}0.3989  0.1663\\end{pmatrix}}$$"
        },
        {
            "introduction": "While tracking the dynamics of AMP iteration by iteration is insightful, we are often most interested in the algorithm's long-term, or steady-state, performance. This is captured by the fixed points of the State Evolution recursion, where the effective noise variance $\\sigma_t^2$ stabilizes. In this exercise, you will explore this concept by deriving the fixed-point equation for a system where the denoiser's regularization is coupled to the state, solving for the final equilibrium variance $\\sigma_*^2$ . This powerful technique allows for the prediction of AMP's asymptotic performance without ever running the algorithm.",
            "id": "3481529",
            "problem": "Consider the linear measurement model $y = A x_{0} + w$ where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/n)$, the signal is identically zero $x_{0} = 0 \\in \\mathbb{R}^{n}$, and the noise $w \\in \\mathbb{R}^{m}$ has independent and identically distributed entries $w_{i} \\sim \\mathcal{N}(0, \\sigma_{w}^{2})$. Assume the measurement ratio $m/n \\to \\delta \\in (0, \\infty)$ as $m, n \\to \\infty$. Consider an Approximate Message Passing (AMP) algorithm whose denoiser at iteration $t$ is the proximal operator for Sorted L-One Penalized Estimation (SLOPE), applied to its pseudo-data $r^{t} \\in \\mathbb{R}^{n}$. The SLOPE proximal is non-separable because it depends on the order statistics of $|r^{t}|$ and a non-increasing sequence of weights $\\{\\lambda_{i}\\}_{i=1}^{n}$. \n\nTo make State Evolution (SE) tractable, approximate the non-separable denoiser by tracking the empirical distribution of the thresholds that act on the order statistics and suppose this empirical distribution concentrates at a single point. Specifically, assume that in the large-system limit, the empirical distribution of sorted thresholds converges to a degenerate distribution at the threshold level $c \\sigma_{t}$, where $c > 0$ is a fixed regularization design parameter and $\\sigma_{t}^{2}$ is the current effective noise variance in SE. Under this approximation, the proximal reduces to componentwise soft-thresholding with threshold $c \\sigma_{t}$, so the scalar denoising function can be written as $\\eta(r; c \\sigma_{t}) = \\mathrm{sign}(r)\\max\\{|r| - c \\sigma_{t}, 0\\}$.\n\nStarting from first principles for AMP SE under i.i.d. Gaussian sensing and the above approximation, derive the fixed-point effective noise variance $\\sigma_{*}^{2}$ as a closed-form analytic expression in terms of $\\delta$, $\\sigma_{w}^{2}$, and $c$. Your derivation must:\n- Begin at the foundational SE description of AMP as an equivalent scalar Gaussian channel with variance tracked by SE.\n- Use the exact soft-thresholding rule to express the mean squared error per coordinate in terms of expectations over the standard normal distribution.\n- Reduce the mean squared error to standard functions of the normal distribution, introducing $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-z^{2}/2)$ and $Q(z) = \\int_{z}^{\\infty} \\phi(u)\\,\\mathrm{d}u$ only as needed.\n\nProvide your final answer as a single closed-form analytic expression for $\\sigma_{*}^{2}$ in terms of $\\delta$, $\\sigma_{w}^{2}$, $c$, $\\phi(c)$, and $Q(c)$. No numerical evaluation is required.",
            "solution": "### Derivation of the Fixed-Point Variance\n\nThe State Evolution (SE) for AMP characterizes the statistical properties of the algorithm's iterates in the large-system limit ($m, n \\to \\infty$, $m/n \\to \\delta$). The core principle of SE is that at each iteration $t$, the pseudo-data vector $r^t$ (the input to the denoiser) is statistically equivalent to the true signal $x_0$ corrupted by additive white Gaussian noise of variance $\\sigma_t^2$. That is, for any component $i$, $r_i^t$ behaves as a random variable drawn from the distribution of $x_{0,i} + \\sigma_t Z_i$, where $Z_i \\sim \\mathcal{N}(0,1)$ is a standard normal random variable.\n\nThe evolution of the effective noise variance from one iteration to the next is described by the SE recursion:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}\\left[ \\left( \\eta(x_0 + \\sigma_t Z; \\tau_t) - x_0 \\right)^2 \\right] $$\nwhere the expectation is over the joint distribution of the signal $x_0$ and the standard normal variable $Z$, and $\\tau_t$ is the thresholding parameter at iteration $t$.\n\nIn this specific problem, we are given several simplifications:\n1.  The true signal is identically zero: $x_0 = 0$.\n2.  The denoising function is componentwise soft-thresholding: $\\eta(r; \\tau) = \\mathrm{sign}(r)\\max\\{|r| - \\tau, 0\\}$.\n3.  The threshold $\\tau_t$ is coupled to the effective noise standard deviation $\\sigma_t$ via $\\tau_t = c \\sigma_t$, where $c$ is a positive constant.\n\nSubstituting these into the general SE recursion, we get:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}\\left[ \\left( \\eta(0 + \\sigma_t Z; c \\sigma_t) - 0 \\right)^2 \\right] $$\nwhere $Z \\sim \\mathcal{N}(0,1)$. This simplifies to:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{1}{\\delta} \\mathbb{E}\\left[ \\eta(\\sigma_t Z; c \\sigma_t)^2 \\right] $$\n\nOur next task is to evaluate the expectation term. Let $V = \\sigma_t Z$. Then $V$ is a Gaussian random variable with mean $0$ and variance $\\sigma_t^2$. The expectation is:\n$$ \\mathbb{E}\\left[ \\eta(\\sigma_t Z; c \\sigma_t)^2 \\right] = \\mathbb{E}\\left[ \\left( \\mathrm{sign}(\\sigma_t Z)\\max\\{|\\sigma_t Z| - c\\sigma_t, 0\\} \\right)^2 \\right] \\\\ = \\mathbb{E}\\left[ \\left( \\max\\{\\sigma_t|Z| - c\\sigma_t, 0\\} \\right)^2 \\right] \\\\ = \\sigma_t^2 \\, \\mathbb{E}\\left[ \\left( \\max\\{|Z| - c, 0\\} \\right)^2 \\right] $$\nThe expectation is over $Z \\sim \\mathcal{N}(0,1)$. Let's compute this expectation by integrating against the standard normal probability density function (PDF), $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\n$$ \\mathbb{E}\\left[ (\\max\\{|Z| - c, 0\\})^2 \\right] = \\int_{-\\infty}^{\\infty} (\\max\\{|z| - c, 0\\})^2 \\phi(z) \\, \\mathrm{d}z $$\nThe integrand is non-zero only for $|z| > c$. Due to the symmetry of the integrand $(|z|-c)^2 \\phi(z)$, we can write the integral as:\n$$ = 2 \\int_{c}^{\\infty} (z - c)^2 \\phi(z) \\, \\mathrm{d}z $$\nExpanding the squared term, $(z - c)^2 = z^2 - 2cz + c^2$, we get:\n$$ = 2 \\left[ \\int_{c}^{\\infty} z^2 \\phi(z) \\, \\mathrm{d}z - 2c \\int_{c}^{\\infty} z \\phi(z) \\, \\mathrm{d}z + c^2 \\int_{c}^{\\infty} \\phi(z) \\, \\mathrm{d}z \\right] $$\nWe evaluate each integral separately:\n1.  The third integral is the definition of the complementary cumulative distribution function (CDF), $Q(z)$:\n    $$ \\int_{c}^{\\infty} \\phi(z) \\, \\mathrm{d}z = Q(c) $$\n2.  The second integral can be solved directly:\n    $$ \\int_{c}^{\\infty} z \\phi(z) \\, \\mathrm{d}z = \\int_{c}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{z^2}{2}\\right) \\, \\mathrm{d}z = \\frac{1}{\\sqrt{2\\pi}} \\left[ -\\exp\\left(-\\frac{z^2}{2}\\right) \\right]_{c}^{\\infty} = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{c^2}{2}\\right) = \\phi(c) $$\n3.  The first integral can be solved using integration by parts, $\\int u \\, \\mathrm{d}v = uv - \\int v \\, \\mathrm{d}u$. Let $u = z$ and $\\mathrm{d}v = z \\phi(z) \\, \\mathrm{d}z$. Then $\\mathrm{d}u = \\mathrm{d}z$ and $v = -\\phi(z)$.\n    $$ \\int_{c}^{\\infty} z^2 \\phi(z) \\, \\mathrm{d}z = \\left[ -z\\phi(z) \\right]_{c}^{\\infty} - \\int_{c}^{\\infty} (-\\phi(z)) \\, \\mathrm{d}z = (0 - (-c\\phi(c))) + \\int_{c}^{\\infty} \\phi(z) \\, \\mathrm{d}z = c\\phi(c) + Q(c) $$\n    where we used the fact that $\\lim_{z\\to\\infty} z\\phi(z) = 0$.\n\nSubstituting these results back into the expression for the expectation:\n$$ \\mathbb{E}\\left[ (\\max\\{|Z| - c, 0\\})^2 \\right] = 2 \\left[ (c\\phi(c) + Q(c)) - 2c(\\phi(c)) + c^2(Q(c)) \\right] \\\\ = 2 \\left[ Q(c) - c\\phi(c) + c^2 Q(c) \\right] \\\\ = 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right] $$\nThe total mean squared error term is therefore:\n$$ \\mathbb{E}\\left[ \\eta(\\sigma_t Z; c \\sigma_t)^2 \\right] = \\sigma_t^2 \\cdot 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right] $$\nLet us define a constant $K(c)$ that depends only on the parameter $c$:\n$$ K(c) = 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right] $$\nThe SE recursion simplifies to a linear map on the variance:\n$$ \\sigma_{t+1}^2 = \\sigma_w^2 + \\frac{K(c)}{\\delta} \\sigma_t^2 $$\nTo find the fixed point $\\sigma_*^2$, we set $\\sigma_{t+1}^2 = \\sigma_t^2 = \\sigma_*^2$:\n$$ \\sigma_*^2 = \\sigma_w^2 + \\frac{K(c)}{\\delta} \\sigma_*^2 $$\nSolving for $\\sigma_*^2$:\n$$ \\sigma_*^2 \\left( 1 - \\frac{K(c)}{\\delta} \\right) = \\sigma_w^2 \\\\ \\sigma_*^2 \\left( \\frac{\\delta - K(c)}{\\delta} \\right) = \\sigma_w^2 \\\\ \\sigma_*^2 = \\frac{\\delta \\sigma_w^2}{\\delta - K(c)} $$\nFinally, substituting the expression for $K(c)$ yields the closed-form solution:\n$$ \\sigma_*^2 = \\frac{\\delta \\sigma_w^2}{\\delta - 2 \\left[ (1+c^2)Q(c) - c\\phi(c) \\right]} $$\n$$ \\sigma_*^2 = \\frac{\\delta \\sigma_w^2}{\\delta - 2(1+c^2)Q(c) + 2c\\phi(c)} $$\nThis is the fixed-point effective noise variance as a function of the problem parameters $\\delta$, $\\sigma_w^2$, and $c$, and the standard normal functions $\\phi(c)$ and $Q(c)$.",
            "answer": "$$\\boxed{\\frac{\\delta \\sigma_{w}^{2}}{\\delta - 2(1+c^{2})Q(c) + 2c\\phi(c)}}$$"
        },
        {
            "introduction": "The stable fixed points of State Evolution can be understood as the minima of an underlying potential, analogous to a \"free energy\" in statistical physics. This profound connection links the dynamics of the AMP algorithm to the variational principles of high-dimensional inference. In this capstone practice, you will derive this Bethe free energy functional and prove that its stationary points are precisely the fixed points of the SE recursion . By constructing this functional and numerically verifying the correspondence through code, you will bridge two powerful theoretical paradigms and gain a much deeper appreciation for the structure of modern estimation problems.",
            "id": "3481503",
            "problem": "Consider the canonical linear inverse model of compressed sensing with a random Gaussian measurement matrix. Let $n \\in \\mathbb{N}$ denote the ambient dimension and $m = \\lfloor \\delta n \\rfloor$ denote the number of measurements with measurement rate $\\delta \\in (0,\\infty)$. The unknown signal entries $X \\in \\mathbb{R}$ are independent and identically distributed according to a specified prior distribution $P_X$. The sensing matrix has entries $A_{ij} \\sim \\mathcal{N}(0, 1/n)$ independent of $X$, and the additive noise is $W \\sim \\mathcal{N}(0, \\sigma_w^2 I_m)$ with noise variance $\\sigma_w^2 \\in [0,\\infty)$. In the high-dimensional limit $n \\to \\infty$ with $m/n \\to \\delta$, the Approximate Message Passing (AMP) algorithm admits a scalar State Evolution (SE) description at iteration $t$ in terms of an effective Gaussian channel with noise variance $\\tau_t \\in [0,\\infty)$. Let $\\eta(\\cdot; \\tau)$ denote the Bayesian posterior mean estimator for the scalar denoising problem $R = X + \\sqrt{\\tau} Z$, where $Z \\sim \\mathcal{N}(0,1)$ is independent of $X \\sim P_X$.\n\nThe scalar State Evolution recursion is given by the update\n$$\n\\tau_{t+1} \\;=\\; \\sigma_w^2 \\;+\\; \\frac{1}{\\delta}\\,\\mathbb{E}\\!\\left[\\left(\\eta\\!\\left(X + \\sqrt{\\tau_t}\\, Z;\\, \\tau_t\\right) - X\\right)^2\\right],\n$$\nwhere the expectation is taken over $X \\sim P_X$ and $Z \\sim \\mathcal{N}(0,1)$. Define the mean-squared error function\n$$\n\\mathrm{mmse}(\\tau) \\;=\\; \\mathbb{E}\\!\\left[\\left(\\eta\\!\\left(X + \\sqrt{\\tau}\\, Z;\\, \\tau\\right) - X\\right)^2\\right].\n$$\nA fixed point $\\tau^\\star$ of State Evolution satisfies the equation\n$$\n\\tau^\\star \\;=\\; \\sigma_w^2 \\;+\\; \\frac{1}{\\delta}\\,\\mathrm{mmse}(\\tau^\\star).\n$$\n\nYour task is to connect State Evolution fixed points to extrema of a Bethe-type free energy functional using only fundamental principles and generally accepted identities, without relying on any pre-packaged formulas for this mapping. Specifically:\n\n- Starting from the definition of the mean-squared error $\\mathrm{mmse}(\\tau)$ for the scalar denoiser and the notion of State Evolution fixed points, construct a differentiable scalar functional $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ of a single variable $\\tau \\in [0,\\infty)$ such that the stationary condition\n$$\n\\frac{d}{d\\tau}\\,\\mathcal{F}_{\\mathrm{B}}(\\tau) \\;=\\; 0\n$$\nis equivalent to the State Evolution fixed-point equation, i.e., it holds if and only if\n$$\n\\tau \\;=\\; \\sigma_w^2 \\;+\\; \\frac{1}{\\delta}\\,\\mathrm{mmse}(\\tau).\n$$\nAmong the infinitely many valid choices, implement the simplest construction that is continuously differentiable on $[0,\\infty)$, bounded below on any compact interval, and reduces to an explicit convex quadratic up to an additive constant when $P_X$ is a standard Gaussian prior.\n\n- Numerically evaluate $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ and verify that its extrema coincide with the State Evolution fixed points $\\tau^\\star$ up to a prescribed numerical tolerance. Your verification must be carried out for multiple priors $P_X$ and parameter settings.\n\nImplementation details and constraints:\n\n- For the denoiser, always use the Bayesian posterior mean $\\eta(r;\\tau) = \\mathbb{E}[X \\mid R=r]$ under the scalar effective channel $R = X + \\sqrt{\\tau}\\,Z$. You may compute expectations either analytically when available or via numerically stable quadrature. You must not assume any closed-form identity for $\\mathrm{mmse}(\\tau)$ unless you derive it from first principles of Bayesian estimation for the specific prior.\n\n- For numerical expectations of the form $\\mathbb{E}[f(\\mu + \\sqrt{v}\\, Z)]$ with $Z \\sim \\mathcal{N}(0,1)$, your code must implement a quadrature rule with proven convergence for Gaussian-weighted integrals; a standard and acceptable choice is Gauss–Hermite quadrature. Ensure numerical stability for small and large arguments where needed.\n\n- Compute State Evolution fixed points by iterating the recursion for $\\tau_t$ until the absolute change is below a given tolerance. Use a damping scheme if necessary to ensure convergence.\n\n- Evaluate the candidate free energy $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ on a sufficiently fine grid that provably contains all fixed points, based on an a priori upper bound derived from $\\mathrm{mmse}(\\tau) \\leq \\mathbb{E}[X^2]$. Use numerical integration consistent with your quadrature to compute any integral terms appearing in $\\mathcal{F}_{\\mathrm{B}}(\\tau)$.\n\nTest suite:\n\nImplement the above for the following three test cases. For each case, verify that the State Evolution fixed point coincides (within tolerance) with an extremum of your constructed $\\mathcal{F}_{\\mathrm{B}}(\\tau)$. The final output for each case must be a boolean indicating whether the coincidence holds.\n\n- Case $\\mathbf{1}$ (Gaussian prior):\n  - Prior: $P_X = \\mathcal{N}(0,1)$.\n  - Measurement rate: $\\delta = 1.0$.\n  - Noise variance: $\\sigma_w^2 = 0.01$.\n\n- Case $\\mathbf{2}$ (Rademacher prior):\n  - Prior: $P_X(\\{+1\\}) = P_X(\\{-1\\}) = 1/2$.\n  - Measurement rate: $\\delta = 0.8$.\n  - Noise variance: $\\sigma_w^2 = 0.05$.\n\n- Case $\\mathbf{3}$ (Bernoulli–Gaussian sparse prior):\n  - Prior: $X = 0$ with probability $1-\\rho$ and $X \\sim \\mathcal{N}(0,1)$ with probability $\\rho$, where $\\rho = 0.1$.\n  - Measurement rate: $\\delta = 0.5$.\n  - Noise variance: $\\sigma_w^2 = 0.01$.\n\nNumerical tolerances and bounds:\n\n- Use an absolute tolerance of $\\varepsilon = 10^{-3}$ when comparing a State Evolution fixed point to an extremum location of $\\mathcal{F}_{\\mathrm{B}}(\\tau)$.\n- Use a Gauss–Hermite quadrature with at least $N_{\\mathrm{gh}} = 50$ nodes for all Gaussian expectations.\n- To define the $\\tau$-grid for evaluating $\\mathcal{F}_{\\mathrm{B}}(\\tau)$, use the deterministic bound\n$$\n\\tau^\\star \\;\\leq\\; \\sigma_w^2 + \\frac{\\mathbb{E}[X^2]}{\\delta},\n$$\nand extend it by an additive safety margin of $0.5$ to set the maximum grid value. Choose a uniform grid with at least $300$ points on $[0, \\tau_{\\max}]$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the three cases above. Each entry must be an integer $1$ if the verification succeeds for that case, or $0$ otherwise. For example, a valid output looks like $[1,0,1]$.",
            "solution": "### Construction of the Free Energy Functional\n\nThe State Evolution fixed-point equation defines the points of equilibrium for the effective noise variance $\\tau$. Let's rearrange the equation to define a function whose roots are these fixed points:\n$$ \\Psi(\\tau) \\;=\\; \\tau - \\sigma_w^2 - \\frac{1}{\\delta}\\mathrm{mmse}(\\tau) \\;=\\; 0 $$\nWe are tasked with finding a potential function, or free energy $\\mathcal{F}_{\\mathrm{B}}(\\tau)$, whose stationary points (where the derivative is zero) coincide with the solutions to $\\Psi(\\tau)=0$. The most direct way to construct such a functional is to define its derivative to be proportional to $\\Psi(\\tau)$. We can define the derivative as $\\delta \\cdot \\Psi(\\tau)$:\n$$ \\frac{d}{d\\tau} \\mathcal{F}_{\\mathrm{B}}(\\tau) \\; \\triangleq \\; \\delta \\left( \\tau - \\sigma_w^2 - \\frac{1}{\\delta}\\mathrm{mmse}(\\tau) \\right) \\;=\\; \\delta\\tau - \\delta\\sigma_w^2 - \\mathrm{mmse}(\\tau) $$\nFor any $\\tau>0$ where $\\delta$ is finite, the condition $\\frac{d}{d\\tau}\\mathcal{F}_{\\mathrm{B}}(\\tau) = 0$ is evidently equivalent to the SE fixed-point equation.\n\nTo obtain the functional $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ itself, we integrate the expression above with respect to $\\tau$. We define the functional relative to its value at $\\tau=0$ (i.e., setting the integration constant to zero):\n$$ \\mathcal{F}_{\\mathrm{B}}(\\tau) \\;=\\; \\int_0^\\tau \\left( \\delta u - \\delta\\sigma_w^2 - \\mathrm{mmse}(u) \\right) du $$\n$$ \\mathcal{F}_{\\mathrm{B}}(\\tau) \\;=\\; \\frac{\\delta}{2}\\tau^2 - \\delta\\sigma_w^2\\tau - \\int_0^\\tau \\mathrm{mmse}(u) du $$\nThis construction is the simplest possible, as it directly integrates the fixed-point condition. It is continuously differentiable, provided $\\mathrm{mmse}(\\tau)$ is continuous.\n\n**Analysis of the Gaussian Prior Case:**\nFor a standard Gaussian prior, $P_X = \\mathcal{N}(0,1)$, the MMSE for the scalar channel $R = X + \\sqrt{\\tau}Z$ is known to be:\n$$ \\mathrm{mmse}(\\tau) = \\frac{\\tau}{1+\\tau} $$\nSubstituting this into our expression for $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ requires computing the integral of the MMSE:\n$$ \\int_0^\\tau \\mathrm{mmse}(u) du = \\int_0^\\tau \\frac{u}{1+u} du = \\int_0^\\tau \\left(1 - \\frac{1}{1+u}\\right) du = \\left[u - \\log(1+u)\\right]_0^\\tau = \\tau - \\log(1+\\tau) $$\nThe functional for the Gaussian case is therefore:\n$$ \\mathcal{F}_{\\mathrm{B}}(\\tau) = \\frac{\\delta}{2}\\tau^2 - \\delta\\sigma_w^2\\tau - (\\tau - \\log(1+\\tau)) = \\frac{\\delta}{2}\\tau^2 - (\\delta\\sigma_w^2 + 1)\\tau + \\log(1+\\tau) $$\nThis function is not a quadratic due to the $\\log(1+\\tau)$ term. This indicates a minor inconsistency in the problem's descriptive constraint. However, the derived functional is correct in that its stationary points correspond to the SE fixed points, and it remains the simplest valid construction.\n\n### Numerical Verification\n\nThe Python code below implements the numerical verification. For each test case, it:\n1. Defines the appropriate MMSE function using Gauss-Hermite quadrature for expectations.\n2. Finds the SE fixed point $\\tau^\\star$ by iterating the SE map until convergence.\n3. Constructs the free energy functional $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ by numerically integrating the MMSE function.\n4. Finds the location of the extremum of $\\mathcal{F}_{\\mathrm{B}}(\\tau)$ on a grid.\n5. Compares the two values to verify they coincide within the specified tolerance.\n\nFor the **Rademacher prior**, the denoiser is $\\eta(r;\\tau) = \\tanh(r/\\tau)$, and the MMSE is computed by integrating $(X - \\eta(X+\\sqrt{\\tau}Z;\\tau))^2$ over $X \\in \\{-1, +1\\}$ and $Z \\sim \\mathcal{N}(0,1)$.\n\nFor the **Bernoulli-Gaussian prior**, the denoiser and MMSE are derived from mixture model principles, requiring a 2D quadrature for one of the expectation terms.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import cumulative_trapezoid\n\n# Set numerical parameters from the problem description\nN_GH = 50  # Number of Gauss-Hermite quadrature nodes\nN_GRID = 500  # Number of points for tau grid\nSE_TOL = 1e-8  # Tolerance for SE fixed point iteration\nSE_MAX_ITER = 2000\nSE_DAMPING = 0.5 # Damping factor for SE iteration\nCOMPARE_TOL = 1e-3 # Tolerance for final comparison\n\n# Pre-compute Gauss-Hermite nodes and weights\n# The nodes x are for integrating against exp(-x^2)\n# We need to integrate against exp(-z^2/2)/sqrt(2pi), where z ~ N(0,1)\n# So, z = sqrt(2)*x. The weight needs adjustment: w' = w / sqrt(pi)\nherm_nodes, herm_weights = np.polynomial.hermite.hermgauss(N_GH)\nZ_NODES = herm_nodes * np.sqrt(2)\nZ_WEIGHTS = herm_weights / np.sqrt(np.pi)\n\ndef solve():\n    \"\"\"\n    Main function to run the verification for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: Gaussian prior\n        {\n            \"name\": \"Gaussian\",\n            \"delta\": 1.0,\n            \"sigma_w_sq\": 0.01,\n            \"prior\": {\n                \"type\": \"gaussian\",\n                \"params\": {},\n                \"E_X2\": 1.0\n            }\n        },\n        # Case 2: Rademacher prior\n        {\n            \"name\": \"Rademacher\",\n            \"delta\": 0.8,\n            \"sigma_w_sq\": 0.05,\n            \"prior\": {\n                \"type\": \"rademacher\",\n                \"params\": {},\n                \"E_X2\": 1.0\n            }\n        },\n        # Case 3: Bernoulli-Gaussian sparse prior\n        {\n            \"name\": \"Bernoulli-Gaussian\",\n            \"delta\": 0.5,\n            \"sigma_w_sq\": 0.01,\n            \"prior\": {\n                \"type\": \"bg\",\n                \"params\": {\"rho\": 0.1},\n                \"E_X2\": 0.1\n            }\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        results.append(run_verification(case))\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_verification(case):\n    \"\"\"\n    Performs the full verification for a single test case.\n    Returns 1 if verification succeeds, 0 otherwise.\n    \"\"\"\n    prior_type = case[\"prior\"][\"type\"]\n    prior_params = case[\"prior\"][\"params\"]\n    delta = case[\"delta\"]\n    sigma_w_sq = case[\"sigma_w_sq\"]\n    E_X2 = case[\"prior\"][\"E_X2\"]\n\n    mmse_func = get_mmse_func(prior_type, prior_params)\n    \n    # 1. Compute State Evolution fixed point\n    tau_star = find_se_fixed_point(mmse_func, delta, sigma_w_sq)\n\n    # 2. Construct the free energy functional on a grid\n    tau_max = sigma_w_sq + E_X2 / delta + 0.5\n    tau_grid = np.linspace(1e-9, tau_max, N_GRID)\n\n    # Evaluate free energy\n    F_B_values = evaluate_free_energy(mmse_func, tau_grid, delta, sigma_w_sq)\n\n    # 3. Find extrema of the free energy on the grid\n    # A stationary point is where the gradient crosses zero.\n    grad_F_B = np.gradient(F_B_values, tau_grid)\n    # Find indices where the sign of the gradient changes\n    sign_changes = np.where(np.diff(np.sign(grad_F_B)))[0]\n\n    # For these simple priors, we expect one relevant stationary point.\n    # We take the first one found.\n    if len(sign_changes) == 0:\n        # If no sign change, extremum might be at the boundary.\n        # This is unlikely for these problems but handled for robustness.\n        extremum_idx = np.argmin(np.abs(grad_F_B))\n    else:\n        # The index is where the gradient is closest to zero around the sign change.\n        idx = sign_changes[0]\n        if np.abs(grad_F_B[idx])  np.abs(grad_F_B[idx+1]):\n             extremum_idx = idx\n        else:\n             extremum_idx = idx + 1\n\n    tau_extremum = tau_grid[extremum_idx]\n\n    # 4. Compare the SE fixed point and the functional's extremum\n    if np.abs(tau_star - tau_extremum)  COMPARE_TOL:\n        return 1\n    else:\n        return 0\n\ndef get_mmse_func(prior_type, params):\n    \"\"\"Returns the appropriate mmse(tau) function for a given prior.\"\"\"\n    if prior_type == \"gaussian\":\n        def mmse_func(tau):\n            # Avoid division by zero if tau is close to 0\n            return tau / (1.0 + tau) if tau > 1e-12 else 0.0\n    \n    elif prior_type == \"rademacher\":\n        def mmse_func(tau):\n            if tau  1e-9: return 1.0 # MMSE is E[X^2]=1 at tau=0\n            r_vals = 1.0 + np.sqrt(tau) * Z_NODES\n            integrand = (1.0 - np.tanh(r_vals / tau))**2\n            return np.dot(integrand, Z_WEIGHTS)\n            \n    elif prior_type == \"bg\":\n        rho = params[\"rho\"]\n        def denoiser_bg(r, tau):\n            if tau  1e-9: return 0.0\n            \n            # log-sum-exp trick for numerical stability of alpha(r,tau)\n            log_p1 = np.log(1.0 - rho) - 0.5 * np.log(2 * np.pi * tau) - r**2 / (2 * tau)\n            log_p2 = np.log(rho) - 0.5 * np.log(2 * np.pi * (1 + tau)) - r**2 / (2 * (1 + tau))\n            \n            max_log = np.maximum(log_p1, log_p2)\n            alpha = np.exp(log_p2 - max_log) / (np.exp(log_p1 - max_log) + np.exp(log_p2 - max_log))\n            \n            return alpha * r / (1.0 + tau)\n\n        def mmse_func(tau):\n            if tau  1e-9: return rho # MMSE is rho*E[G^2] = rho at tau=0\n            \n            # Term for X=0\n            r_vals_noise = np.sqrt(tau) * Z_NODES\n            eta_vals_noise = denoiser_bg(r_vals_noise, tau)\n            mse_zero = np.dot(eta_vals_noise**2, Z_WEIGHTS)\n            \n            # Term for X~N(0,1)\n            # 2D Gauss-Hermite quadrature\n            G_mesh, Z_mesh = np.meshgrid(Z_NODES, Z_NODES)\n            W_mesh = Z_WEIGHTS[:, np.newaxis] @ Z_WEIGHTS[np.newaxis, :]\n            \n            r_vals_signal = G_mesh + np.sqrt(tau) * Z_mesh\n            eta_vals_signal = denoiser_bg(r_vals_signal, tau)\n            mse_nonzero = np.sum(((G_mesh - eta_vals_signal)**2) * W_mesh)\n            \n            return (1.0 - rho) * mse_zero + rho * mse_nonzero\n            \n    else:\n        raise ValueError(\"Unknown prior type\")\n        \n    return np.vectorize(mmse_func)\n\n\ndef find_se_fixed_point(mmse_func, delta, sigma_w_sq):\n    \"\"\"Iterates the SE equation to find a fixed point.\"\"\"\n    tau_t = sigma_w_sq + 1.0 # Initial guess\n    for _ in range(SE_MAX_ITER):\n        tau_next_raw = sigma_w_sq + mmse_func(tau_t) / delta\n        tau_next = (1 - SE_DAMPING) * tau_t + SE_DAMPING * tau_next_raw\n        if np.abs(tau_next - tau_t)  SE_TOL:\n            return tau_next\n        tau_t = tau_next\n    return tau_t\n\ndef evaluate_free_energy(mmse_func, tau_grid, delta, sigma_w_sq):\n    \"\"\"Computes F_B(tau) on a grid.\"\"\"\n    \n    # Use a finer grid for more accurate integration of mmse\n    fine_tau_grid = np.linspace(tau_grid[0], tau_grid[-1], 2 * len(tau_grid))\n    mmse_values_fine = mmse_func(fine_tau_grid)\n    \n    # Numerically integrate mmse(u) from 0 to tau\n    integral_mmse_fine = cumulative_trapezoid(mmse_values_fine, fine_tau_grid, initial=0)\n    \n    # Interpolate integral values back to the original tau grid\n    integral_mmse = np.interp(tau_grid, fine_tau_grid, integral_mmse_fine)\n    \n    # Assemble the free energy functional\n    F_B = 0.5 * delta * tau_grid**2 - delta * sigma_w_sq * tau_grid - integral_mmse\n    return F_B\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        }
    ]
}