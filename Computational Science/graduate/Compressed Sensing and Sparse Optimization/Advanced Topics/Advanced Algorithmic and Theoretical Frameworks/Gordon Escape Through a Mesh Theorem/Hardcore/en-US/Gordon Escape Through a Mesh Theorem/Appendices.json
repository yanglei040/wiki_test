{
    "hands_on_practices": [
        {
            "introduction": "The Gaussian width is the cornerstone of Gordon's theorem, quantifying the \"size\" of a set as seen by random projections. This first exercise provides practice in computing this critical quantity for a fundamental object in compressed sensing: the set of sparse vectors. Mastering this calculation  builds the foundation for understanding how geometry dictates the necessary number of measurements for successful signal recovery.",
            "id": "3448590",
            "problem": "Consider a measurement model in compressed sensing where random linear maps are analyzed via Gordon’s escape through a mesh theorem. Let $n \\in \\mathbb{N}$ and fix an index set $S \\subset \\{1,2,\\dots,n\\}$ with $|S| = s$. Define the subset $T \\subset \\mathbb{R}^{n}$ to be the collection of $s$-sparse vectors with support contained in $S$ and bounded Euclidean norm:\n$$\nT \\triangleq \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S,\\ \\|x\\|_{2} \\leq 1 \\right\\}.\n$$\nLet $g \\in \\mathbb{R}^{n}$ be a standard Gaussian vector whose entries are independent and identically distributed as $\\mathcal{N}(0,1)$, and recall the definition of the Gaussian width of a set $T$:\n$$\nw(T) \\triangleq \\mathbb{E}\\left[\\,\\sup_{x \\in T} \\langle g, x \\rangle\\,\\right],\n$$\nwhere $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product. Starting from fundamental definitions and well-tested facts about Gaussian vectors and radial distributions, derive an exact analytical expression for $w(T)$ in terms of $s$ only. Express your final answer as a single closed-form symbolic expression. No numerical rounding is required, and no physical units are involved.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard calculation in high-dimensional probability and compressed sensing theory. We will now proceed with the derivation of the analytical expression for the Gaussian width $w(T)$.\n\nThe Gaussian width of the set $T$ is defined as:\n$$\nw(T) \\triangleq \\mathbb{E}\\left[\\,\\sup_{x \\in T} \\langle g, x \\rangle\\,\\right]\n$$\nwhere $T \\triangleq \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S,\\ \\|x\\|_{2} \\leq 1 \\right\\}$, the set $S$ has cardinality $|S| = s$, and $g \\in \\mathbb{R}^{n}$ is a standard Gaussian vector with i.i.d. components $g_i \\sim \\mathcal{N}(0, 1)$.\n\nOur first step is to simplify the term $\\sup_{x \\in T} \\langle g, x \\rangle$ for a fixed realization of $g$. The condition $\\operatorname{supp}(x) \\subseteq S$ implies that the components $x_i$ of any vector $x \\in T$ are zero for any index $i \\notin S$. The inner product $\\langle g, x \\rangle$ can thus be written as a sum over the indices in $S$:\n$$\n\\langle g, x \\rangle = \\sum_{i=1}^{n} g_i x_i = \\sum_{i \\in S} g_i x_i.\n$$\nLet us define $g_S \\in \\mathbb{R}^{s}$ as the vector composed of the entries of $g$ whose indices are in $S$, and similarly, let $x_S \\in \\mathbb{R}^{s}$ be the vector of the corresponding entries of $x$. The inner product is then $\\langle g, x \\rangle = \\langle g_S, x_S \\rangle_{\\mathbb{R}^s}$. The constraint $\\|x\\|_{2} \\leq 1$ becomes $\\|x_S\\|_{2} \\leq 1$, since all other components of $x$ are zero.\n\nThe optimization problem inside the expectation is therefore:\n$$\n\\sup_{x \\in T} \\langle g, x \\rangle = \\sup_{x_S \\in \\mathbb{R}^s, \\|x_S\\|_2 \\leq 1} \\langle g_S, x_S \\rangle.\n$$\nThis expression is the definition of the dual norm of $g_S$. For the $\\ell_2$-norm, the dual norm is the $\\ell_2$-norm itself. By the Cauchy-Schwarz inequality, $\\langle g_S, x_S \\rangle \\leq \\|g_S\\|_2 \\|x_S\\|_2$. Given that $\\|x_S\\|_2 \\leq 1$, we have $\\langle g_S, x_S \\rangle \\leq \\|g_S\\|_2$. This maximum value is attained when $x_S$ is aligned with $g_S$ and has the maximum possible norm, i.e., $x_S = g_S / \\|g_S\\|_2$ (for $g_S \\neq 0$). Thus, we have:\n$$\n\\sup_{x \\in T} \\langle g, x \\rangle = \\|g_S\\|_2.\n$$\nNow, we can rewrite the Gaussian width as the expectation of this norm:\n$$\nw(T) = \\mathbb{E}\\left[ \\|g_S\\|_2 \\right].\n$$\nThe vector $g_S$ is a sub-vector of $g$ containing $s$ components, each being an independent standard normal random variable. Therefore, $g_S$ is a standard Gaussian random vector in $\\mathbb{R}^s$. The random variable we are interested in is the Euclidean norm of $g_S$.\n\nLet the random variable $Z$ be the squared norm of $g_S$:\n$$\nZ = \\|g_S\\|_2^2 = \\sum_{i \\in S} g_i^2.\n$$\nSince $Z$ is the sum of the squares of $s$ independent standard normal random variables, it follows a chi-squared distribution with $s$ degrees of freedom, denoted as $Z \\sim \\chi^2(s)$.\n\nThe probability density function (PDF) of $Z$ for $z > 0$ is given by:\n$$\nf_Z(z) = \\frac{1}{2^{s/2} \\Gamma(s/2)} z^{s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right),\n$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function.\n\nWe need to compute the expectation of $\\|g_S\\|_2 = \\sqrt{Z}$:\n$$\nw(T) = \\mathbb{E}[\\sqrt{Z}] = \\int_{0}^{\\infty} \\sqrt{z} f_Z(z) \\, dz.\n$$\nSubstituting the PDF of the $\\chi^2(s)$ distribution into the integral:\n$$\nw(T) = \\int_{0}^{\\infty} z^{1/2} \\left( \\frac{1}{2^{s/2} \\Gamma(s/2)} z^{s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\right) \\, dz.\n$$\nWe can factor out the constants and combine the powers of $z$:\n$$\nw(T) = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\int_{0}^{\\infty} z^{1/2 + s/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\int_{0}^{\\infty} z^{(s+1)/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz.\n$$\nThe integral is a standard form related to the Gamma function. For a shape parameter $k > 0$ and a scale parameter $\\theta > 0$, the following identity holds:\n$$\n\\int_{0}^{\\infty} t^{k-1} \\exp\\left(-\\frac{t}{\\theta}\\right) \\, dt = \\theta^k \\Gamma(k).\n$$\nIn our case, the variable of integration is $z$, the shape parameter is $k = \\frac{s+1}{2}$, and the scale parameter is $\\theta=2$. Applying this identity, the integral evaluates to:\n$$\n\\int_{0}^{\\infty} z^{(s+1)/2 - 1} \\exp\\left(-\\frac{z}{2}\\right) \\, dz = 2^{(s+1)/2} \\Gamma\\left(\\frac{s+1}{2}\\right).\n$$\nSubstituting this back into our expression for $w(T)$:\n$$\nw(T) = \\frac{1}{2^{s/2} \\Gamma(s/2)} \\left( 2^{(s+1)/2} \\Gamma\\left(\\frac{s+1}{2}\\right) \\right).\n$$\nFinally, we simplify the expression by combining the powers of $2$:\n$$\nw(T) = \\frac{2^{(s+1)/2}}{2^{s/2}} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = 2^{((s+1)/2) - (s/2)} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)} = 2^{1/2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}.\n$$\nThis gives the exact analytical expression for the Gaussian width of the set $T$:\n$$\nw(T) = \\sqrt{2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}.\n$$\nThe result depends only on the sparsity level $s$, as required. This is the mean of the chi distribution with $s$ degrees of freedom.",
            "answer": "$$\\boxed{\\sqrt{2} \\frac{\\Gamma\\left(\\frac{s+1}{2}\\right)}{\\Gamma\\left(\\frac{s}{2}\\right)}}$$"
        },
        {
            "introduction": "Gordon’s theorem provides a precise, non-asymptotic prediction for the sample complexity required for recovery, often given in terms of the statistical dimension of a descent cone. This exercise challenges you to compare this rigorous theoretical result with a more intuitive \"degrees-of-freedom\" heuristic in the context of low-rank matrix recovery . By analyzing different measurement models and problem structures, you will develop a deeper appreciation for the theorem's accuracy and the conditions under which it applies.",
            "id": "3448549",
            "problem": "Consider the linear matrix sensing model for low-rank recovery. Let $X_{\\star} \\in \\mathbb{R}^{p \\times q}$ have rank $r$, and suppose we observe $m$ linear measurements $y = \\mathcal{A}(X_{\\star}) \\in \\mathbb{R}^{m}$ where $\\mathcal{A}: \\mathbb{R}^{p \\times q} \\to \\mathbb{R}^{m}$ is a linear operator with i.i.d. Gaussian coordinates in an orthonormal basis, normalized so that each coordinate is $\\mathcal{N}(0, 1)$ when applied to a unit Frobenius-norm matrix. Consider the convex recovery program that minimizes the nuclear norm subject to measurement consistency:\nminimize $\\|X\\|_{\\ast}$ subject to $\\mathcal{A}(X) = y$.\nEscape Through a Mesh (ETM) refers to Gordon’s comparison inequality applied to the event that a random subspace misses a fixed cone. In this setting, write $\\mathcal{D} := \\mathcal{D}(\\|\\cdot\\|_{\\ast}, X_{\\star})$ for the descent cone of the nuclear norm at $X_{\\star}$. Let $S^{pq-1}$ be the unit sphere in $\\mathbb{R}^{p q}$ under the Frobenius norm, and let $w(\\mathcal{C}) := \\mathbb{E}\\sup_{u \\in \\mathcal{C}} \\langle g, u \\rangle$ denote the Gaussian width of a set $\\mathcal{C} \\subset \\mathbb{R}^{p q}$ with $g \\sim \\mathcal{N}(0, I_{pq})$. For a closed convex cone $\\mathcal{C}$, the statistical dimension satisfies $\\delta(\\mathcal{C}) = \\mathbb{E}\\|\\Pi_{\\mathcal{C}}(g)\\|_{2}^{2}$, and it is known that $w^{2}(\\mathcal{C} \\cap S^{pq-1})$ and $\\delta(\\mathcal{C})$ agree up to an additive constant.\nUsing only these foundational facts and the orthogonal invariance of the nuclear norm and the Gaussian ensemble, compare the ETM-based sample complexity threshold for exact recovery with the degrees-of-freedom heuristic $\\mathrm{dof} = r(p+q-r)$. Select all statements that are correct.\n\nA. In the isotropic Gaussian sensing model described, ETM predicts a phase transition at $m \\approx \\delta(\\mathcal{D})$, and for nuclear norm minimization at a rank-$r$ point, $\\delta(\\mathcal{D})$ equals $r(p+q-r)$; therefore, the ETM threshold matches the degrees-of-freedom count.\n\nB. For vector sparsity with $\\ell_{1}$ minimization under isotropic Gaussian measurements, ETM predicts $m \\approx 2k$ for recovery of a $k$-sparse vector in $\\mathbb{R}^{n}$, which coincides with the degrees-of-freedom heuristic $2k$.\n\nC. If the sensing operator is replaced by an isotropic subgaussian ensemble (same covariance as the Gaussian case), then the ETM-based threshold remains proportional to the statistical dimension of the descent cone; for the nuclear norm, this maintains agreement with $\\mathrm{dof} = r(p+q-r)$ up to absolute constants independent of $p$ and $q$.\n\nD. For matrix completion with uniform random sampling of entries and nuclear norm minimization, the ETM-based threshold is still $m \\approx r(p+q-r)$ without additional assumptions, so it always agrees with the degrees-of-freedom heuristic in this setting.\n\nE. If $X_{\\star}$ has repeated singular values, then the descent cone of the nuclear norm strictly enlarges, and ETM predicts a sample complexity strictly larger than $r(p+q-r)$ in general; hence the degrees-of-freedom heuristic can underestimate by more than a constant factor.\n\nF. For anisotropic Gaussian measurements with covariance operator $\\Sigma \\neq I$, the ETM threshold is governed by the statistical dimension computed in the $\\Sigma$-induced geometry and can deviate from $r(p+q-r)$; agreement with the degrees-of-freedom heuristic requires isotropy (i.e., $\\Sigma$ proportional to the identity).",
            "solution": "The problem statement is first validated for scientific soundness, consistency, and completeness.\n\n### Step 1: Extract Givens\n-   **Model**: Linear matrix sensing for low-rank recovery.\n-   **True Matrix**: $X_{\\star} \\in \\mathbb{R}^{p \\times q}$ with $\\mathrm{rank}(X_{\\star}) = r$.\n-   **Measurements**: $y = \\mathcal{A}(X_{\\star}) \\in \\mathbb{R}^{m}$, where $\\mathcal{A}: \\mathbb{R}^{p \\times q} \\to \\mathbb{R}^{m}$ is a linear operator.\n-   **Sensing Ensemble**: The operator $\\mathcal{A}$ has i.i.d. Gaussian coordinates in an orthonormal basis, normalized such that each measurement coordinate $y_i$ is $\\mathcal{N}(0, 1)$ when $\\mathcal{A}$ is applied to a matrix with unit Frobenius norm. This describes the standard isotropic Gaussian ensemble, where $\\mathcal{A}(X)_i = \\langle A_i, X \\rangle_F$ and the entries of the matrices $A_i$ are i.i.d. $\\mathcal{N}(0, 1)$.\n-   **Recovery Method**: A convex program that minimizes the nuclear norm $\\|\\cdot\\|_{\\ast}$ subject to measurement consistency: $\\min \\|X\\|_{\\ast}$ s.t. $\\mathcal{A}(X) = y$.\n-   **Key Concepts**:\n    -   **Descent Cone**: $\\mathcal{D} := \\mathcal{D}(\\|\\cdot\\|_{\\ast}, X_{\\star})$ is the descent cone of the nuclear norm at $X_{\\star}$.\n    -   **Gaussian Width**: $w(\\mathcal{C}) := \\mathbb{E}\\sup_{u \\in \\mathcal{C}} \\langle g, u \\rangle$ for $\\mathcal{C} \\subset \\mathbb{R}^{p q}$ and $g \\sim \\mathcal{N}(0, I_{pq})$.\n    -   **Statistical Dimension**: $\\delta(\\mathcal{C}) = \\mathbb{E}\\|\\Pi_{\\mathcal{C}}(g)\\|_{2}^{2}$ for a closed convex cone $\\mathcal{C}$.\n    -   **Relationship**: $w^{2}(\\mathcal{C} \\cap S^{pq-1})$ and $\\delta(\\mathcal{C})$ agree up to an additive constant, where $S^{pq-1}$ is the unit Frobenius-norm sphere.\n-   **Core Principle to Use**: Escape Through a Mesh (ETM), based on Gordon's inequality, predicts a phase transition for exact recovery when the number of measurements $m$ is approximately equal to the statistical dimension of the descent cone, $m \\approx \\delta(\\mathcal{D})$.\n-   **Heuristic for Comparison**: The degrees-of-freedom heuristic, $\\mathrm{dof} = r(p+q-r)$.\n-   **Question**: Using only these facts, compare the ETM-based sample complexity with the degrees-of-freedom heuristic and identify all correct statements among the options.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is rooted in the established mathematical theory of compressed sensing and high-dimensional probability. All terms—nuclear norm, descent cone, statistical dimension, Gaussian width, Escape Through a Mesh, and the degrees-of-freedom heuristic—are standard in this field. The problem is well-posed, asking for a comparison between a theoretical prediction (ETM) and a known heuristic (DoF) based on foundational results. The setup is self-consistent and provides the definitions required for the analysis. There are no scientific or logical contradictions, no pseudo-profound claims, and the question is objective and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. Proceeding to the solution.\n\n### Derivation and Option Analysis\n\nThe problem asks us to analyze the sample complexity of low-rank matrix recovery using the \"Escape Through a Mesh\" (ETM) framework. The ETM principle provides a precise prediction for the threshold number of measurements $m$ required for successful recovery via convex optimization. For a generic convex program and an isotropic Gaussian measurement operator $\\mathcal{A}$, the phase transition between success and failure occurs at a threshold $m_{\\star}$ given by:\n$$m_{\\star} \\approx \\delta(\\mathcal{D})$$\nwhere $\\mathcal{D}$ is the descent cone of the objective function at the ground truth signal $X_{\\star}$. The statistical dimension $\\delta(\\mathcal{D})$ of a cone $\\mathcal{D}$ is a measure of its size. The problem provides that we must use this fact.\n\nThe degrees-of-freedom heuristic for a rank-$r$ matrix $X_{\\star} \\in \\mathbb{R}^{p \\times q}$ is $\\mathrm{dof} = p r + q r - r^2 = r(p+q-r)$. This counts the number of free parameters specifying a rank-$r$ matrix, accounting for the redundancy in its factorization.\n\nThe core of the problem lies in the value of $\\delta(\\mathcal{D})$ for the nuclear norm descent cone. A central result in this area, established by Amelunxen, Lotz, McCoy, and Tropp, provides an exact formula for the statistical dimension of the descent cone for many popular convex regularizers, assuming an isotropic Gaussian ensemble. For the nuclear norm $\\|\\cdot\\|_\\ast$ at a rank-$r$ matrix $X_{\\star} \\in \\mathbb{R}^{p \\times q}$ (with distinct non-zero singular values), the statistical dimension of the descent cone $\\mathcal{D}(\\|\\cdot\\|_{\\ast}, X_{\\star})$ is precisely:\n$$\\delta(\\mathcal{D}) = r(p+q-r)$$\n\nWith these foundational results, we can evaluate each option.\n\n**A. In the isotropic Gaussian sensing model described, ETM predicts a phase transition at $m \\approx \\delta(\\mathcal{D})$, and for nuclear norm minimization at a rank-$r$ point, $\\delta(\\mathcal{D})$ equals $r(p+q-r)$; therefore, the ETM threshold matches the degrees-of-freedom count.**\n\n-   The ETM prediction is given as $m \\approx \\delta(\\mathcal{D})$. This is a premise of the problem.\n-   The statement that $\\delta(\\mathcal{D})$ for the nuclear norm descent cone equals $r(p+q-r)$ is a correct and fundamental result from the theory of conic geometry and random projections.\n-   The degrees-of-freedom count is given as $\\mathrm{dof} = r(p+q-r)$.\n-   Therefore, the ETM threshold $m \\approx r(p+q-r)$ directly matches the DoF heuristic. The statement is a logical consequence of established facts.\n\n**Verdict: Correct.**\n\n**B. For vector sparsity with $\\ell_{1}$ minimization under isotropic Gaussian measurements, ETM predicts $m \\approx 2k$ for recovery of a $k$-sparse vector in $\\mathbb{R}^{n}$, which coincides with the degrees-of-freedom heuristic $2k$.**\n\n-   This option considers the analogous problem for sparse vector recovery using the $\\ell_1$-norm. The ground truth is a $k$-sparse vector.\n-   The DoF heuristic of $2k$ is a plausible (though informal) count ($k$ non-zero values and their $k$ locations).\n-   However, the ETM prediction for the sample complexity of $\\ell_1$ minimization is not $m \\approx 2k$. The precise phase transition, as determined by Donoho and Tanner and rigorously proven using the ETM framework, is more complex. In the regime where $n \\to \\infty$ and $k/n \\to 0$, the necessary number of measurements scales as $m \\approx 2k \\log(n/k)$, not as $2k$. The absence of the logarithmic factor makes the statement incorrect.\n\n**Verdict: Incorrect.**\n\n**C. If the sensing operator is replaced by an isotropic subgaussian ensemble (same covariance as the Gaussian case), then the ETM-based threshold remains proportional to the statistical dimension of the descent cone; for the nuclear norm, this maintains agreement with $\\mathrm{dof} = r(p+q-r)$ up to absolute constants independent of $p$ and $q$.**\n\n-   This statement addresses the universality of the phase transition phenomenon. Gordon's ETM theory is native to Gaussian ensembles. However, major results in the field have established that these phase transitions are universal, meaning they hold for a much wider class of random measurement ensembles, including isotropic subgaussian ones.\n-   For such ensembles, the threshold for successful recovery is still determined by the same geometric properties of the problem, and is thus proportional to the Gaussian statistical dimension $\\delta(\\mathcal{D})$. That is, $m \\ge C \\cdot \\delta(\\mathcal{D})$ for some universal constant $C$.\n-   Since $\\delta(\\mathcal{D}) = r(p+q-r)$, the sample complexity remains proportional to the DoF heuristic. The phrasing \"proportional to\" and \"up to absolute constants\" correctly captures the nature of these universality results.\n\n**Verdict: Correct.**\n\n**D. For matrix completion with uniform random sampling of entries and nuclear norm minimization, the ETM-based threshold is still $m \\approx r(p+q-r)$ without additional assumptions, so it always agrees with the degrees-of-freedom heuristic in this setting.**\n\n-   Matrix completion involves a specific, highly structured measurement operator corresponding to sampling matrix entries. This is not an isotropic Gaussian or subgaussian ensemble.\n-   The analysis for matrix completion is substantially different. Crucially, successful recovery requires not only a sufficient number of samples but also **incoherence assumptions** on the singular vectors of the matrix $X_{\\star}$. The statement's claim \"without additional assumptions\" is false. A matrix with its mass concentrated in a few entries cannot be recovered from a small number of random samples.\n-   Furthermore, the required sample complexity is not $m \\approx r(p+q-r)$. It is known to be of the order $m \\gtrsim (p+q) r \\log(p+q)$ under the necessary incoherence conditions.\n-   The ETM framework for Gaussian ensembles does not directly apply to give this simple result for the sampling model of matrix completion.\n\n**Verdict: Incorrect.**\n\n**E. If $X_{\\star}$ has repeated singular values, then the descent cone of the nuclear norm strictly enlarges, and ETM predicts a sample complexity strictly larger than $r(p+q-r)$ in general; hence the degrees-of-freedom heuristic can underestimate by more than a constant factor.**\n\n-   The structure of the subdifferential $\\partial \\|X_{\\star}\\|_{\\ast}$ changes if $X_{\\star}$ has repeated singular values. The presence of symmetries (rotational invariance in subspaces corresponding to repeated singular values) enlarges the subdifferential set.\n-   The descent cone $\\mathcal{D}$ consists of directions $\\Delta$ satisfying $\\langle S, \\Delta \\rangle \\le 0$ for all subgradients $S \\in \\partial \\|X_{\\star}\\|_{\\ast}$. If the set of subgradients $\\partial \\|X_{\\star}\\|_{\\ast}$ becomes larger, this imposes more constraints on $\\Delta$. A set defined by more constraints is smaller. Therefore, the descent cone strictly **shrinks**, not enlarges.\n-   A smaller descent cone has a smaller statistical dimension $\\delta(\\mathcal{D})$. According to the ETM principle, this would predict a **lower** sample complexity for recovery, not a larger one. The premise of the statement is wrong.\n\n**Verdict: Incorrect.**\n\n**F. For anisotropic Gaussian measurements with covariance operator $\\Sigma \\neq I$, the ETM threshold is governed by the statistical dimension computed in the $\\Sigma$-induced geometry and can deviate from $r(p+q-r)$; agreement with the degrees-of-freedom heuristic requires isotropy (i.e., $\\Sigma$ proportional to the identity).**\n\n-   Anisotropic measurements mean the rows of the sensing matrix (vectorized $A_i$) are drawn from $\\mathcal{N}(0, \\Sigma)$ where $\\Sigma$ is not proportional to the identity matrix $I$. This breaks the rotational invariance of the measurement ensemble.\n-   The ETM framework can be adapted to this setting. The resulting phase transition threshold is governed by a modified statistical dimension, computed with respect to the geometry induced by $\\Sigma$. This is often denoted $\\delta_{\\Sigma}(\\mathcal{D})$.\n-   This new quantity, $\\delta_{\\Sigma}(\\mathcal{D})$, depends on the interaction between the cone $\\mathcal{D}$ and the covariance structure $\\Sigma$. In general, it will not be equal to the isotropic result $r(p+q-r)$. For instance, if the variance of measurements is high in directions where the descent cone is \"thin\", recovery might be harder (requiring larger $m$), and vice versa.\n-   The simple formula $\\delta(\\mathcal{D}) = r(p+q-r)$ is a direct consequence of the rotational invariance of the standard Gaussian ensemble (isotropy). When this symmetry is broken ($\\Sigma \\neq cI$), the simple formula no longer holds. Thus, agreement with the DoF heuristic is a special feature of the isotropic model.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACF}$$"
        },
        {
            "introduction": "The most compelling demonstration of a theory is witnessing its predictions hold true in a practical setting. This final exercise guides you in bridging theory and practice by writing a program to simulate the sparse signal recovery process . You will compute the theoretical phase transition boundary using the statistical dimension and compare it against the empirical success rate of $\\ell_1$-minimization, bringing the \"escape through a mesh\" phenomenon to life on your own computer.",
            "id": "3481865",
            "problem": "Let $n \\in \\mathbb{N}$ and let $x^\\star \\in \\mathbb{R}^n$ be a $k$-sparse signal, meaning that exactly $k$ coordinates of $x^\\star$ are nonzero. Consider linear measurements $y = A x^\\star$ with $A \\in \\mathbb{R}^{m \\times n}$ having independent and identically distributed standard normal entries. The cone of interest is the descent cone $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$ of the convex function $\\|\\cdot\\|_1$ at $x^\\star$, defined as\n$$\n\\mathcal{D}(\\|\\cdot\\|_1, x^\\star) = \\operatorname{cone}\\left\\{h \\in \\mathbb{R}^n : \\|x^\\star + h\\|_1 \\le \\|x^\\star\\|_1 \\right\\}.\n$$\nThe Gaussian width $w(K)$ of a cone $K$ is defined as\n$$\nw(K) = \\mathbb{E}\\left[ \\sup_{u \\in K \\cap \\mathbb{S}^{n-1}} \\langle g, u \\rangle \\right],\n$$\nwhere $\\mathbb{S}^{n-1}$ is the unit sphere in $\\mathbb{R}^n$ and $g \\sim \\mathcal{N}(0, I_n)$ is a standard Gaussian vector. Gordon’s \"escape through a mesh\" theorem asserts that a random subspace of codimension $m$ is likely to avoid $K$ when $m$ is greater than a function of the Gaussian width of $K$; conversely, for smaller $m$, intersections are likely.\n\nYour task is to write a complete, standalone program to demonstrate this phenomenon for the descent cone $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$ by:\n1. Computing an approximation of the squared Gaussian width $w(K)^2$ via the statistical dimension of the descent cone using first principles of conic geometry. The statistical dimension is defined as\n$$\n\\delta(K) = \\mathbb{E}\\left[ \\|\\Pi_K(g)\\|_2^2 \\right],\n$$\nwhere $\\Pi_K(g)$ denotes the Euclidean projection of $g$ onto $K$, and is closely related to the squared Gaussian width of $K \\cap \\mathbb{S}^{n-1}$.\n2. Empirically estimating whether the random nullspace $\\operatorname{Null}(A)$ intersects $K$ nontrivially, which equivalently indicates failure of $\\ell_1$-minimization recovery. Specifically, for each trial, solve the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad A x = y,\n$$\nand declare recovery success if the minimizer equals $x^\\star$ within a numerical tolerance, and failure otherwise. The presence of a nonzero $h \\in \\operatorname{Null}(A) \\cap K$ indicates that recovery fails.\n3. Comparing the empirical success rate with the theoretical prediction based on whether $m$ is larger or smaller than the computed $\\delta(K)$.\n\nUse the following well-tested definitions and facts as your fundamental base:\n- The descent cone of a proper, convex, lower-semicontinuous function at a point collects directions that do not increase the function value.\n- The Gaussian width and statistical dimension are geometric measures that govern phase transitions in high-dimensional convex recovery.\n- For the $\\ell_1$ norm, the statistical dimension of the descent cone at a $k$-sparse point can be characterized via an expectation involving the subdifferential and a scalar parameter, and is numerically tractable using the distribution of the absolute value of a standard normal random variable.\n\nProgram requirements:\n- Implement a numerical procedure to approximate $\\delta\\!\\left(\\mathcal{D}(\\|\\cdot\\|_1, x^\\star)\\right)$ for given $(n,k)$ by minimizing over a nonnegative scalar parameter a suitable expectation derived from the half-normal distribution of $|g|$, where $g \\sim \\mathcal{N}(0,1)$.\n- For each test case, generate $A$ with independent and identically distributed standard normal entries and a random $k$-sparse $x^\\star$ with nonzero entries drawn from a continuous distribution, then solve the $\\ell_1$-minimization problem using a linear programming formulation and record whether recovery succeeds.\n- For each test case, output two numbers: the predicted indicator $(1$ if $m  \\delta(K)$, $0$ otherwise$)$, and the empirical recovery success fraction across the specified number of trials, rounded to three decimal places.\n\nTest suite:\n- Case 1 (happy path, above threshold): $(n,k,m,T) = (200,10,85,8)$.\n- Case 2 (below threshold): $(n,k,m,T) = (200,10,55,8)$.\n- Case 3 (near threshold): $(n,k,m,T) = (200,10,70,8)$.\n- Case 4 (boundary, zero-sparse): $(n,k,m,T) = (100,0,1,8)$.\n- Case 5 (dense relative sparsity): $(n,k,m,T) = (200,60,150,6)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list of the form $[\\text{indicator}, \\text{fraction}]$. For example, the output should have the form\n$$\n[[i_1,f_1],[i_2,f_2],\\dots,[i_5,f_5]],\n$$\nwhere each $i_j$ is an integer $0$ or $1$ and each $f_j$ is a decimal rounded to three places.",
            "solution": "The problem statement poses a valid and well-defined task in the field of compressed sensing and high-dimensional probability. We first validate the problem and then provide a complete solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Signal and Sparsity**: $x^\\star \\in \\mathbb{R}^n$ is a $k$-sparse signal, where $n \\in \\mathbb{N}$ and $k$ is the number of nonzero coordinates.\n- **Measurement Model**: $y = A x^\\star$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed (i.i.d.) standard normal entries.\n- **Descent Cone**: $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star) = \\operatorname{cone}\\left\\{h \\in \\mathbb{R}^n : \\|x^\\star + h\\|_1 \\le \\|x^\\star\\|_1 \\right\\}$.\n- **Gaussian Width**: $w(K) = \\mathbb{E}\\left[ \\sup_{u \\in K \\cap \\mathbb{S}^{n-1}} \\langle g, u \\rangle \\right]$, with $g \\sim \\mathcal{N}(0, I_n)$.\n- **Statistical Dimension**: $\\delta(K) = \\mathbb{E}\\left[ \\|\\Pi_K(g)\\|_2^2 \\right]$, where $\\Pi_K(g)$ is the Euclidean projection of $g$ onto $K$.\n- **Recovery Problem**: $\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1$ subject to $A x = y$.\n- **Recovery Success Condition**: The minimizer of the recovery problem equals $x^\\star$. This occurs if and only if the nullspace of $A$, $\\operatorname{Null}(A)$, has no nontrivial intersection with the descent cone $K$.\n- **Task**: For given parameters $(n,k,m,T)$, compute a theoretical success indicator ($1$ if $m  \\delta(K)$, $0$ otherwise) and an empirical success fraction over $T$ trials.\n- **Test Cases**:\n    1. $(n,k,m,T) = (200,10,85,8)$\n    2. $(n,k,m,T) = (200,10,55,8)$\n    3. $(n,k,m,T) = (200,10,70,8)$\n    4. $(n,k,m,T) = (100,0,1,8)$\n    5. $(n,k,m,T) = (200,60,150,6)$\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is firmly rooted in the modern theory of compressed sensing. The concepts of descent cones, statistical dimension, Gaussian width, and their connection to phase transitions in convex optimization are central, established results in this field (cf. Amelunxen, Lotz, McCoy, Tropp, 2014, \"Living on the Edge: Phase Transitions in Convex Programs with Random Data\").\n- **Well-Posed**: The problem is well-posed. It asks for a numerical computation and simulation to demonstrate a known theoretical result. The tasks are specified clearly, and the parameters provided allow for a unique set of outputs.\n- **Objective**: The problem is stated in precise, mathematical language, free from subjectivity.\n- **Completeness**: The problem provides all necessary definitions and parameters. It implicitly relies on the known formula for the statistical dimension of the $\\ell_1$ descent cone, which is standard in this context.\n- **No Other Flaws**: The problem is not contradictory, unrealistic (within the mathematical framework), or ill-structured.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. We proceed with a full solution.\n\n### Solution\n\nThe problem explores the phase transition phenomenon in compressed sensing, where the success of recovering a sparse signal $x^\\star$ from linear measurements $y=Ax^\\star$ via $\\ell_1$-minimization undergoes a sharp change as the number of measurements $m$ varies. This transition is precisely characterized by the statistical dimension $\\delta(K)$ of the descent cone $K = \\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$.\n\nThe fundamental principle is that for a random Gaussian matrix $A$, the recovery of $x^\\star$ is highly probable if $m  \\delta(K)$ and highly improbable if $m  \\delta(K)$. The point $m \\approx \\delta(K)$ marks the phase transition boundary. Our solution consists of two parts: first, computing the theoretical threshold $\\delta(K)$, and second, empirically verifying the prediction by simulating the recovery process.\n\n**1. Theoretical Prediction via Statistical Dimension**\n\nThe statistical dimension of the descent cone of the $\\ell_1$-norm at a $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$, denoted $K=\\mathcal{D}(\\|\\cdot\\|_1, x^\\star)$, is independent of the support location and the signs of the nonzero entries of $x^\\star$. It is given by the formula:\n$$\n\\delta(K) = k + \\min_{\\lambda \\ge 0} \\left\\{ (n-k) \\mathbb{E}\\left[(\\lvert Z \\rvert - \\lambda)^2_+\\right] + k\\lambda^2 \\right\\}\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable, and $(t)_+ = \\max(t, 0)$ is the positive part function. The expectation term can be computed in a closed form involving the probability density function (PDF) $\\phi$ and the cumulative distribution function (CDF) $\\Phi$ of a standard normal variable:\n$$\n\\mathbb{E}\\left[(\\lvert Z \\rvert - \\lambda)^2_+\\right] = 2 \\int_{\\lambda}^{\\infty} (z-\\lambda)^2 \\phi(z) dz = 2 \\left[ (1+\\lambda^2)(1-\\Phi(\\lambda)) - \\lambda\\phi(\\lambda) \\right]\n$$\nfor $\\lambda \\ge 0$.\nThe problem of finding $\\delta(K)$ thus reduces to a one-dimensional convex optimization problem over the non-negative scalar $\\lambda$. We solve this numerically to find the value of $\\delta(K)$ for each pair of $(n,k)$ in the test suite. The theoretical prediction for recovery success is then given by an indicator function: $1$ if $m  \\delta(K)$ and $0$ otherwise.\n\nA special case is $k=0$, where $x^\\star = 0$. The descent cone is $K = \\{h : \\|h\\|_1 \\le 0\\} = \\{0\\}$. The projection of any vector onto $\\{0\\}$ is the zero vector, so $\\delta(\\{0\\}) = \\mathbb{E}[\\|\\Pi_{\\{0\\}}(g)\\|_2^2] = 0$.\n\n**2. Empirical Verification via Simulation**\n\nWe perform Monte Carlo simulations to estimate the empirical probability of successful recovery for each test case $(n,k,m,T)$. For each of the $T$ trials:\na. A random $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$ is generated. A support of size $k$ is chosen uniformly at random, and the non-zero entries are drawn from a standard normal distribution.\nb. An $m \\times n$ measurement matrix $A$ is generated with i.i.d. $\\mathcal{N}(0,1)$ entries.\nc. The measurement vector is computed as $y = Ax^\\star$.\nd. The $\\ell_1$-minimization problem (also known as Basis Pursuit) is solved to find an estimate $\\hat{x}$:\n$$\n\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^n} \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = y\n$$\nThis convex optimization problem is recast as a Linear Program (LP) by representing $x$ as the difference of two non-negative vectors, $x = u - v$, where $u_i, v_i \\ge 0$. The LP is:\n$$\n\\min_{u, v \\in \\mathbb{R}^n} \\sum_{i=1}^n (u_i + v_i) \\quad \\text{subject to} \\quad A(u-v) = y, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\nThis LP is solved using standard numerical solvers.\ne. Recovery is declared a \"success\" if the solution $\\hat{x}$ is numerically close to the original signal $x^\\star$. We use a mixed absolute-relative error tolerance: for $k  0$, success is defined by $\\|\\hat{x} - x^\\star\\|_2 / \\|x^\\star\\|_2  10^{-6}$; for $k=0$, success is defined by $\\|\\hat{x}\\|_2  10^{-6}$.\nThe empirical success fraction is the number of successful trials divided by the total number of trials, $T$. This fraction is compared against the theoretical indicator.\n\nThe implementation of this procedure for the specified test cases will demonstrate the sharpness of the phase transition predicted by the statistical dimension.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog, minimize_scalar\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes theoretical and empirical results for l1 recovery phase transitions.\n    \"\"\"\n    TOLERANCE = 1e-6\n\n    def calculate_statistical_dimension(n, k):\n        \"\"\"\n        Calculates the statistical dimension delta(K) for the l1 descent cone.\n        \n        The formula is:\n        delta(K) = k + min_{lambda = 0} { (n-k) * E[(|Z|-lambda)^2_+] + k*lambda^2 }\n        where Z ~ N(0,1).\n        \"\"\"\n        if k == 0:\n            return 0.0\n        if k == n:\n            return float(n)\n\n        def expectation_term(lam):\n            \"\"\"Computes E[(|Z|-lambda)^2_+].\"\"\"\n            if lam  0:\n                # The minimization is over lambda = 0\n                return np.inf\n            # Using the closed-form expression\n            # 2 * [ (1+lam^2)*(1-Phi(lam)) - lam*phi(lam) ]\n            return 2 * ( (1 + lam**2) * (1 - norm.cdf(lam)) - lam * norm.pdf(lam) )\n\n        def objective(lam, n_val, k_val):\n            \"\"\"The function to be minimized over lambda.\"\"\"\n            return (n_val - k_val) * expectation_term(lam) + k_val * lam**2\n\n        # Numerically minimize the objective function to find the minimum value.\n        # The minimization is over lambda = 0.\n        # 'bounded' method is suitable for box constraints.\n        res = minimize_scalar(\n            objective, \n            args=(n, k), \n            bounds=(0, 10), # lambda is unlikely to be large, 10 is a safe upper bound.\n            method='bounded'\n        )\n        \n        min_val = res.fun\n        return k + min_val\n\n    def run_single_trial(n, k, m, rng):\n        \"\"\"\n        Runs a single trial of sparse recovery.\n        \"\"\"\n        # 1. Generate a random k-sparse signal x_star\n        x_star = np.zeros(n)\n        if k  0:\n            support = rng.choice(n, k, replace=False)\n            x_star[support] = rng.standard_normal(k)\n\n        # 2. Generate measurement matrix A and measurements y\n        A = rng.standard_normal((m, n))\n        y = A @ x_star\n        \n        # 3. Solve the l1 minimization problem (Basis Pursuit) via Linear Programming\n        # Problem: min ||x||_1 s.t. Ax = y\n        # LP form:   min c.T @ z s.t. A_eq @ z = b_eq, z = 0\n        # where z = [u, v], x = u - v, ||x||_1 = 1.T @ u + 1.T @ v\n        c_lp = np.ones(2 * n)\n        A_lp = np.hstack([A, -A])\n        b_lp = y\n        \n        # Using 'highs' solver, which is robust and efficient.\n        res = linprog(c=c_lp, A_eq=A_lp, b_eq=b_lp, bounds=(0, None), method='highs')\n\n        if not res.success:\n            return False\n\n        # 4. Reconstruct the solution and check for success\n        x_sol = res.x[:n] - res.x[n:]\n        \n        if k == 0:\n            # Absolute error for the zero vector\n            norm_x_star = 0\n            if np.linalg.norm(x_sol)  TOLERANCE:\n                return True\n        else:\n            # Relative error for non-zero vectors\n            norm_x_star = np.linalg.norm(x_star)\n            if np.linalg.norm(x_sol - x_star) / norm_x_star  TOLERANCE:\n                return True\n        \n        return False\n\n    # Test cases: (n, k, m, T)\n    test_cases = [\n        (200, 10, 85, 8),\n        (200, 10, 55, 8),\n        (200, 10, 70, 8),\n        (100, 0, 1, 8),\n        (200, 60, 150, 6)\n    ]\n    \n    final_results = []\n    rng = np.random.default_rng(seed=42) # Seed for reproducibility\n    \n    # Cache for statistical dimension calculation\n    delta_cache = {}\n\n    for n, k, m, T in test_cases:\n        # Calculate theoretical prediction\n        if (n, k) not in delta_cache:\n            delta_cache[(n, k)] = calculate_statistical_dimension(n, k)\n        \n        delta_K = delta_cache[(n, k)]\n        theoretical_indicator = 1 if m  delta_K else 0\n        \n        # Run empirical simulations\n        success_count = 0\n        if T  0:\n            for _ in range(T):\n                if run_single_trial(n, k, m, rng):\n                    success_count += 1\n            empirical_fraction = success_count / T\n        else:\n            empirical_fraction = 0.0\n\n        final_results.append(f\"[{theoretical_indicator},{empirical_fraction:.3f}]\")\n\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```"
        }
    ]
}