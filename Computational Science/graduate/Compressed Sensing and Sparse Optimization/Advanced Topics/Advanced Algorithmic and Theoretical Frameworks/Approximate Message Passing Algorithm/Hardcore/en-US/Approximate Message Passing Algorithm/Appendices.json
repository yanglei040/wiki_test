{
    "hands_on_practices": [
        {
            "introduction": "The Approximate Message Passing (AMP) algorithm's remarkable performance and precise theoretical characterization hinge on a crucial component: the Onsager correction term. This term is not an ad-hoc modification but arises from a deep statistical principle. This first exercise guides you through the derivation of this term from first principles using a simple, analytically tractable linear denoiser, revealing how it cancels out harmful statistical correlations that build up during the iterative process . Mastering this derivation provides a foundational understanding of the decoupling principle that enables the powerful State Evolution analysis of AMP.",
            "id": "3432158",
            "problem": "Consider the standard compressed sensing linear model $y = A x_{0} + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, the unknown signal $x_{0} \\in \\mathbb{R}^{n}$ is fixed, and the noise $w \\in \\mathbb{R}^{m}$ is independent of $A$. Let $\\delta = m/n$ denote the measurement ratio. The Approximate Message Passing (AMP) iteration with the Onsager correction coefficient is given by\n$$\nx^{t+1} = \\eta_{\\tau_{t}}\\!\\left(x^{t} + A^{\\top} z^{t}\\right), \\qquad z^{t} = y - A x^{t} + \\alpha_{t-1} z^{t-1},\n$$\nwhere $x^{0}$ and $z^{-1}$ are specified initializations, and $\\eta_{\\tau}(r)$ is a separable denoising function applied componentwise. Assume the denoiser is linear and defined by $\\eta_{\\tau}(r) = \\frac{1}{1+\\tau}\\, r$, where $\\tau > 0$ may depend on the iteration index.\n\nStarting from the foundational principles of AMP in high-dimensional Gaussian sensing—specifically, the decoupling principle and the requirement that the effective pre-activation $x^{t} + A^{\\top} z^{t}$ behaves as an additive white Gaussian noise channel under state evolution—derive an explicit analytic expression for the Onsager coefficient $\\alpha_{t}$ associated with the linear denoiser $\\eta_{\\tau_{t}}(r)$. Your derivation should rely on the invariance properties of Gaussian matrices and the divergence (average Jacobian) of the denoiser as the core base. Then, explain how this $\\alpha_{t}$ influences the residual recursion $z^{t} = y - A x^{t} + \\alpha_{t-1} z^{t-1}$ in terms of its stability and memory.\n\nProvide your final answer as a closed-form analytic expression for $\\alpha_{t}$ in terms of $\\delta$ and $\\tau_{t}$. No numerical evaluation is required, and no rounding is necessary. Express your final answer as an analytic expression without units.",
            "solution": "The problem requires the derivation of the Onsager correction coefficient, $\\alpha_{t}$, for an Approximate Message Passing (AMP) algorithm with a specified linear denoiser. The derivation must be founded upon the core principles of AMP, namely the decoupling principle that relies on properties of large Gaussian matrices.\n\nWe begin with the AMP iteration as defined in the problem statement:\n$$x^{t+1} = \\eta_{\\tau_{t}}\\!\\left(x^{t} + A^{\\top} z^{t}\\right)$$\n$$z^{t} = y - A x^{t} + \\alpha_{t-1} z^{t-1}$$\nThe signal model is $y = A x_{0} + w$. Substituting this into the equation for $z^t$ yields:\n$$z^{t} = A x_{0} + w - A x^{t} + \\alpha_{t-1} z^{t-1} = A(x_{0} - x^{t}) + w + \\alpha_{t-1} z^{t-1}$$\nThe fundamental principle of AMP is that the algorithm's behavior in the high-dimensional limit ($m, n \\to \\infty$ with $m/n \\to \\delta$) can be rigorously tracked by a simpler scalar recursion known as state evolution. This analysis is possible because the effective input to the denoiser at each step, $r^t = x^{t} + A^{\\top} z^{t}$, behaves as if it were the true signal $x_0$ corrupted by additive white Gaussian noise. This is the decoupling principle.\n\nFor this decoupling to hold, certain statistical dependencies that arise during the iteration must be actively cancelled. The problematic dependency originates from the term $-A x^t$ in the update for $z^t$. Note that $x^t$ is a function of previous iterates, specifically $x^t = \\eta_{\\tau_{t-1}}(x^{t-1} + A^\\top z^{t-1})$, and thus depends on the matrix $A$. The subsequent multiplication by $A$ in $-A x^t$ creates a correlation that, if uncorrected, would invalidate the simple Gaussian noise model for the denoiser's input.\n\nThe Onsager correction term, $\\alpha_{t-1} z^{t-1}$, is introduced precisely to cancel the statistical bias introduced by the term $-A x^t$. The core idea is to set $\\alpha_{t-1} z^{t-1}$ equal to the conditional expectation of the bias-inducing term, thereby creating a new \"centered\" residual that is statistically well-behaved. The bias term is the conditional expectation of $-A x^t$ given the history of the process up to iteration $t-1$, which we denote by $\\mathcal{F}_{t-1}$. The cancellation requirement is:\n$$\\alpha_{t-1} z^{t-1} = - \\mathbb{E}[ -A x^t \\mid \\mathcal{F}_{t-1} ] = \\mathbb{E}[ A x^t \\mid \\mathcal{F}_{t-1} ]$$\nwhere $x^t = \\eta_{\\tau_{t-1}}(x^{t-1} + A^\\top z^{t-1})$. The vectors $x^{t-1}$ and $z^{t-1}$ are considered fixed when taking the expectation over the random matrix $A$. A central result in the theory of random matrices, a consequence of Stein's Lemma (or Gaussian integration by parts), states that for a matrix $A \\in \\mathbb{R}^{m \\times n}$ with i.i.d. $\\mathcal{N}(0, 1/m)$ entries, and for suitable separable functions $\\eta: \\mathbb{R}^n \\to \\mathbb{R}^n$, and fixed vectors $u \\in \\mathbb{R}^n, v \\in \\mathbb{R}^m$:\n$$\\mathbb{E}_A[ A \\, \\eta(u + A^\\top v) ] = \\frac{1}{m} \\mathbb{E}_A[ \\operatorname{div}(\\eta(u + A^\\top v)) ] v$$\nwhere $\\operatorname{div}(\\eta(r)) = \\sum_{j=1}^n \\frac{\\partial \\eta_j(r_j)}{\\partial r_j}$ is the divergence of $\\eta$.\n\nApplying this identity to our expression for the bias, we identify $u = x^{t-1}$, $v = z^{t-1}$, and $\\eta = \\eta_{\\tau_{t-1}}$. Thus,\n$$\\mathbb{E}[ A x^t \\mid \\mathcal{F}_{t-1} ] \\approx \\frac{1}{m} \\mathbb{E}\\left[ \\sum_{j=1}^n \\eta'_{\\tau_{t-1}}(r_j^{t-1}) \\right] z^{t-1}$$\nwhere $r^{t-1} = x^{t-1} + A^\\top z^{t-1}$ and $\\eta'_{\\tau}(r_j) = \\frac{d\\eta_{\\tau}(r_j)}{dr_j}$. The expectation on the right-hand side is over the randomness in $A$, which makes $r^{t-1}$ a random vector.\n\nEquating the two expressions for the bias cancellation, we find:\n$$\\alpha_{t-1} z^{t-1} = \\frac{1}{m} \\mathbb{E}\\left[ \\sum_{j=1}^n \\eta'_{\\tau_{t-1}}(r_j^{t-1}) \\right] z^{t-1}$$\nAssuming $z^{t-1}$ is not the zero vector, we can identify the scalar coefficient $\\alpha_{t-1}$:\n$$\\alpha_{t-1} = \\frac{1}{m} \\mathbb{E}\\left[ \\sum_{j=1}^n \\eta'_{\\tau_{t-1}}(r_j^{t-1}) \\right]$$\nIn the large system limit, due to a self-averaging property, the term inside the expectation converges to a deterministic value. We can thus approximate the expectation by the value itself:\n$$\\alpha_{t-1} \\approx \\frac{1}{m} \\sum_{j=1}^n \\eta'_{\\tau_{t-1}}(r_j^{t-1}) = \\frac{n}{m} \\left( \\frac{1}{n} \\sum_{j=1}^n \\eta'_{\\tau_{t-1}}(r_j^{t-1}) \\right)$$\nUsing the definition $\\delta = m/n$ and denoting the average derivative as $\\langle \\eta'_{\\tau_{t-1}} \\rangle_n = \\frac{1}{n} \\sum_{j=1}^n \\eta'_{\\tau_{t-1}}(r_j^{t-1})$, we get:\n$$\\alpha_{t-1} = \\frac{1}{\\delta} \\langle \\eta'_{\\tau_{t-1}} \\rangle_n$$\nThe problem asks for an expression for $\\alpha_t$. By shifting the time index, we have the general formula:\n$$\\alpha_{t} = \\frac{1}{\\delta} \\langle \\eta'_{\\tau_{t}} \\rangle_n$$\nNow, we must evaluate this for the specific linear denoiser provided: $\\eta_{\\tau}(r) = \\frac{1}{1+\\tau} r$. The derivative of this function with respect to its argument is a constant:\n$$\\eta'_{\\tau_t}(r) = \\frac{d}{dr}\\left(\\frac{1}{1+\\tau_t} r\\right) = \\frac{1}{1+\\tau_t}$$\nSince the derivative is constant for all components, its average is simply the constant itself:\n$$\\langle \\eta'_{\\tau_{t}} \\rangle_n = \\frac{1}{n} \\sum_{j=1}^n \\frac{1}{1+\\tau_t} = \\frac{1}{n} \\cdot n \\cdot \\frac{1}{1+\\tau_t} = \\frac{1}{1+\\tau_t}$$\nSubstituting this result into the formula for $\\alpha_t$, we obtain the explicit analytic expression:\n$$\\alpha_{t} = \\frac{1}{\\delta} \\left(\\frac{1}{1+\\tau_t}\\right) = \\frac{1}{\\delta(1+\\tau_t)}$$\n\nRegarding the influence of $\\alpha_t$ on the residual recursion, its role is twofold:\n\n1.  **Memory and Decoupling**: The term $\\alpha_{t-1} z^{t-1}$ is an explicit memory term, linking the current residual $z^t$ to the previous one, $z^{t-1}$. The recursion $z^{t} = (y - A x^{t}) + \\alpha_{t-1} z^{t-1}$ can be viewed as an IIR (Infinite Impulse Response) filter acting on the innovation sequence $y - A x^t$. The coefficient $\\alpha_t$ is not arbitrary; it is meticulously calculated to counteract the implicit statistical memory introduced by the term $-A x^t$. By adding this specific \"Onsager reaction term\", the algorithm effectively nullifies the harmful correlations between the iterates and the sensing matrix $A$. This cancellation is the key to ensuring the decoupling property, which allows the high-dimensional algorithm to be analyzed via a simple one-dimensional state evolution, a cornerstone of modern AMP theory.\n\n2.  **Stability**: The magnitude of the sequence of coefficients $\\{\\alpha_t\\}$ is critical for the stability of the AMP iteration. If the Onsager coefficient is chosen incorrectly, the decoupling principle fails, leading to a build-up of complex correlations. This can cause the algorithm's performance to deviate significantly from the theoretical predictions of state evolution and may lead to divergence of the iterates. For the linear IIR filter structure of the residual update, stability generally requires the coefficient magnitude $|\\alpha_t|$ to be less than $1$ over time. In our case, $\\alpha_t = \\frac{1}{\\delta(1+\\tau_t)}$. Since $\\delta > 0$ and $\\tau_t > 0$, we have $\\alpha_t > 0$. The condition $\\alpha_t  1$ implies $\\delta(1+\\tau_t) > 1$. This inequality is, in fact, directly related to the conditions for convergence of the state evolution equations for AMP with this linear denoiser. Thus, the correctly derived Onsager coefficient is intrinsically linked to the stability and convergence of the entire algorithm.",
            "answer": "$$\\boxed{\\frac{1}{\\delta(1+\\tau_t)}}$$"
        },
        {
            "introduction": "Moving from foundational principles to practical applications, we now consider the canonical use case of AMP: sparse signal recovery. This requires using a denoiser that promotes sparsity, the most common of which is the soft-thresholding operator. This exercise challenges you to compute the Onsager coefficient for this non-differentiable denoiser by employing the concept of a weak derivative, a necessary tool for rigorous high-dimensional analysis . By working with a standard Bernoulli-Gaussian signal prior, you will see how the statistical properties of the signal are incorporated into the State Evolution dynamics.",
            "id": "3432141",
            "problem": "Consider the standard linear model of compressed sensing with measurements $y = A x_{0} + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, the signal $x_{0} \\in \\mathbb{R}^{n}$ is random with independent and identically distributed entries distributed as $X$, and the noise $w \\in \\mathbb{R}^{m}$ is independent of $A$ and $x_{0}$. Let the measurement rate be $\\delta = m/n$. In approximate message passing (AMP), the scalar state evolution recursion involves an effective scalar denoising problem with observation $X + \\sqrt{\\tau_{t}} Z$, where $Z \\sim \\mathcal{N}(0,1)$ is independent of $X$ and $\\tau_{t}  0$ is the state variable at iteration $t$. The denoiser is the soft-thresholding function with threshold $\\theta  0$, defined by $\\eta(u; \\theta) = \\operatorname{sign}(u) \\max\\{|u| - \\theta, 0\\}$. The Onsager correction coefficient at iteration $t$ is defined, in distribution, by $\\alpha_{t} = \\frac{1}{\\delta} \\mathbb{E}[\\eta'(X + \\sqrt{\\tau_{t}} Z; \\theta)]$, where $\\eta'$ is to be understood in the appropriate sense compatible with Stein’s lemma for Gaussian perturbations.\n\nUsing only fundamental facts about Gaussian distributions and Stein’s lemma (for $Z \\sim \\mathcal{N}(0,1)$ independent of a random variable $U$, and a function $f$ that is weakly differentiable with at most polynomial growth, $\\mathbb{E}[Z f(U + \\sigma Z)] = \\sigma \\mathbb{E}[f'(U + \\sigma Z)]$), proceed as follows:\n\n1. Give a precise, almost-everywhere formula for the weak derivative $\\eta'(u; \\theta)$ of the soft-thresholding function as an indicator function that excludes the threshold points where differentiability fails. Explain why these exceptional points do not affect the expectation $\\mathbb{E}[\\eta'(X + \\sqrt{\\tau_{t}} Z; \\theta)]$.\n\n2. Specialize to a Bernoulli–Gaussian prior for $X$: with probability $1 - \\rho$, $X = 0$, and with probability $\\rho$, $X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$, where $\\rho \\in (0,1]$ and $\\sigma_{x}^{2}  0$. Using the result of part $1$ together with Gaussian tail probabilities, derive a closed-form analytic expression for\n$$\n\\alpha_{t} = \\frac{1}{\\delta} \\mathbb{E}\\big[\\eta'(X + \\sqrt{\\tau_{t}} Z; \\theta)\\big]\n$$\nin terms of $\\delta$, $\\rho$, $\\theta$, $\\sigma_{x}^{2}$, $\\tau_{t}$, and the standard Gaussian upper tail function $Q(x) = \\int_{x}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^{2}/2) \\, \\mathrm{d}t$.\n\nProvide your final answer as a single, closed-form analytic expression in terms of $\\delta$, $\\rho$, $\\theta$, $\\sigma_{x}^{2}$, $\\tau_{t}$, and $Q(\\cdot)$. No numerical evaluation is required.",
            "solution": "The problem asks for a two-part solution concerning the Onsager correction coefficient in the state evolution of the Approximate Message Passing (AMP) algorithm. We first validate the problem statement, then proceed to the solution. The problem is well-posed, scientifically grounded in the theory of compressed sensing and high-dimensional statistics, and provides all necessary information for a unique solution.\n\nPart 1: The weak derivative of the soft-thresholding function.\n\nThe soft-thresholding function $\\eta(u; \\theta)$ with threshold $\\theta  0$ is defined as $\\eta(u; \\theta) = \\operatorname{sign}(u) \\max\\{|u| - \\theta, 0\\}$. We can write this function in a piecewise form:\n$$\n\\eta(u; \\theta) = \\begin{cases}\nu - \\theta  \\text{if } u  \\theta \\\\\n0  \\text{if } -\\theta \\le u \\le \\theta \\\\\nu + \\theta  \\text{if } u  -\\theta\n\\end{cases}\n$$\nThe classical derivative of $\\eta(u; \\theta)$ with respect to $u$ can be computed for all points where it exists. The function is differentiable everywhere except at the points $u = \\theta$ and $u = -\\theta$. The derivative is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}u} \\eta(u; \\theta) = \\begin{cases}\n1  \\text{if } u  \\theta \\\\\n0  \\text{if } -\\theta  u  \\theta \\\\\n1  \\text{if } u  -\\theta\n\\end{cases}\n$$\nThis can be expressed more compactly using an indicator function. The derivative exists and is equal to $1$ if $|u|  \\theta$, and it exists and is equal to $0$ if $|u|  \\theta$. These two conditions cover all of $\\mathbb{R}$ except for the set $\\{-\\theta, \\theta\\}$. Therefore, the derivative almost everywhere (a.e.) is given by:\n$$\n\\eta'(u; \\theta) = \\mathbf{1}_{|u|  \\theta}(u)\n$$\nwhere $\\mathbf{1}_{S}(u)$ is the indicator function which is $1$ if $u \\in S$ and $0$ otherwise. This a.e. formula is the correct expression for the weak derivative needed for Stein's lemma and related calculations in this context.\n\nWe need to compute the expectation $\\mathbb{E}[\\eta'(X + \\sqrt{\\tau_{t}} Z; \\theta)]$. Let $U = X + \\sqrt{\\tau_{t}} Z$. The expectation is an integral with respect to the probability measure of $U$:\n$$\n\\mathbb{E}[\\eta'(U; \\theta)] = \\int_{-\\infty}^{\\infty} \\eta'(u; \\theta) \\, f_U(u) \\, \\mathrm{d}u\n$$\nwhere $f_U(u)$ is the probability density function (PDF) of $U$. The value of an integral is not affected by changing the integrand on a set of measure zero. Here, the integrand $\\eta'(u; \\theta)$ is defined everywhere except on the set $\\{-\\theta, \\theta\\}$. For the integral to be well-defined and unaffected by these points, we must show that the probability measure of this set is zero, i.e., $\\mathbb{P}(U \\in \\{-\\theta, \\theta\\}) = 0$.\n\nThe random variable $U$ is the sum of $X$ and $\\sqrt{\\tau_{t}} Z$. $Z \\sim \\mathcal{N}(0,1)$ is a continuous random variable. The distribution of $X$ is a mixture: $X=0$ with probability $1-\\rho$, and $X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$ with probability $\\rho$.\nLet's analyze the distribution of $U$ by conditioning on $X$:\n1.  If $X=0$, then $U = \\sqrt{\\tau_t} Z$. Since $Z$ is Gaussian, $U$ is also Gaussian, specifically $U \\sim \\mathcal{N}(0, \\tau_{t})$. This is a continuous distribution.\n2.  If $X \\sim \\mathcal{N}(0, \\sigma_x^2)$, then $U = X + \\sqrt{\\tau_t} Z$ is the sum of two independent Gaussian random variables. The sum is also Gaussian, with mean $0+0=0$ and variance $\\sigma_x^2 + \\tau_t$. Thus, $U \\sim \\mathcal{N}(0, \\sigma_x^2 + \\tau_t)$. This is also a continuous distribution.\n\nThe overall distribution of $U$ is a mixture of two continuous distributions. Its PDF is $f_U(u) = (1-\\rho)f_{\\mathcal{N}(0, \\tau_t)}(u) + \\rho f_{\\mathcal{N}(0, \\sigma_x^2 + \\tau_t)}(u)$, which is a valid PDF. Any random variable with a PDF is continuous. For any continuous random variable, the probability of it taking on any single specific value is zero. Therefore, $\\mathbb{P}(U = \\theta) = 0$ and $\\mathbb{P}(U = -\\theta) = 0$. Since the set of exceptional points $\\{-\\theta, \\theta\\}$ has probability measure zero, we can use the almost-everywhere formula for the derivative inside the expectation without ambiguity.\n\nPart 2: Derivation of the Onsager coefficient $\\alpha_t$.\n\nThe Onsager coefficient is given by $\\alpha_{t} = \\frac{1}{\\delta} \\mathbb{E}[\\eta'(X + \\sqrt{\\tau_{t}} Z; \\theta)]$. Using the result from Part 1, we have:\n$$\n\\alpha_{t} = \\frac{1}{\\delta} \\mathbb{E}\\left[\\mathbf{1}_{|X + \\sqrt{\\tau_{t}} Z|  \\theta}\\right]\n$$\nThe expectation of an indicator function is the probability of the event it indicates. Let $U = X + \\sqrt{\\tau_{t}} Z$.\n$$\n\\alpha_{t} = \\frac{1}{\\delta} \\mathbb{P}\\left(|U|  \\theta\\right)\n$$\nWe compute this probability using the law of total probability, conditioning on the two cases for the prior distribution of $X$:\n$$\n\\mathbb{P}(|U|  \\theta) = \\mathbb{P}(|U|  \\theta \\,|\\, X=0)(1-\\rho) + \\mathbb{P}(|U|  \\theta \\,|\\, X \\sim \\mathcal{N}(0, \\sigma_{x}^{2}))\\rho\n$$\nCase 1: $X=0$.\nIn this case, $U = \\sqrt{\\tau_{t}} Z \\sim \\mathcal{N}(0, \\tau_{t})$. The probability is:\n$$\n\\mathbb{P}(|\\sqrt{\\tau_{t}} Z|  \\theta) = \\mathbb{P}(\\sqrt{\\tau_{t}} Z  \\theta) + \\mathbb{P}(\\sqrt{\\tau_{t}} Z  -\\theta)\n$$\nDue to the symmetry of the zero-mean Gaussian distribution, these two terms are equal.\n$$\n\\mathbb{P}(|\\sqrt{\\tau_{t}} Z|  \\theta) = 2 \\mathbb{P}(\\sqrt{\\tau_{t}} Z  \\theta) = 2 \\mathbb{P}\\left(Z  \\frac{\\theta}{\\sqrt{\\tau_{t}}}\\right)\n$$\nUsing the definition of the standard Gaussian upper tail function, $Q(x) = \\mathbb{P}(Z  x)$ for $Z \\sim \\mathcal{N}(0,1)$, this probability is $2 Q\\left(\\frac{\\theta}{\\sqrt{\\tau_{t}}}\\right)$.\n\nCase 2: $X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$.\nIn this case, $U = X + \\sqrt{\\tau_t} Z$. As established in Part 1, the sum of these two independent, zero-mean Gaussian variables is a zero-mean Gaussian variable with variance equal to the sum of their variances.\n$$\nU \\sim \\mathcal{N}(0, \\sigma_{x}^{2} + \\tau_{t})\n$$\nThe probability is:\n$$\n\\mathbb{P}(|U|  \\theta \\,|\\, X \\sim \\mathcal{N}(0, \\sigma_{x}^{2})) = \\mathbb{P}(|U|  \\theta)\n$$\nBy symmetry, this is $2 \\mathbb{P}(U  \\theta)$. To use the $Q$-function, we standardize $U$:\n$$\n2 \\mathbb{P}(U  \\theta) = 2 \\mathbb{P}\\left(\\frac{U}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}  \\frac{\\theta}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}\\right)\n$$\nThe standardized variable $\\frac{U}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}$ is a standard normal variable $\\mathcal{N}(0,1)$. Thus, the probability is $2 Q\\left(\\frac{\\theta}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}\\right)$.\n\nCombining the results:\nWe substitute the probabilities for the two cases back into the law of total probability expression:\n$$\n\\mathbb{P}(|U|  \\theta) = (1-\\rho) \\left[ 2 Q\\left(\\frac{\\theta}{\\sqrt{\\tau_{t}}}\\right) \\right] + \\rho \\left[ 2 Q\\left(\\frac{\\theta}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}\\right) \\right]\n$$\nFinally, we substitute this into the formula for $\\alpha_{t}$:\n$$\n\\alpha_{t} = \\frac{1}{\\delta} \\left\\{ 2(1-\\rho) Q\\left(\\frac{\\theta}{\\sqrt{\\tau_{t}}}\\right) + 2\\rho Q\\left(\\frac{\\theta}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}\\right) \\right\\}\n$$\nFactoring out the constant $2$ gives the final closed-form expression:\n$$\n\\alpha_{t} = \\frac{2}{\\delta} \\left[ (1-\\rho) Q\\left(\\frac{\\theta}{\\sqrt{\\tau_{t}}}\\right) + \\rho Q\\left(\\frac{\\theta}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}\\right) \\right]\n$$\nThis expression provides the Onsager coefficient in terms of the problem parameters $\\delta$, $\\rho$, $\\theta$, $\\sigma_{x}^{2}$, $\\tau_{t}$, and the Gaussian $Q$-function, as required.",
            "answer": "$$\\boxed{\\frac{2}{\\delta} \\left[ (1-\\rho) Q\\left(\\frac{\\theta}{\\sqrt{\\tau_{t}}}\\right) + \\rho Q\\left(\\frac{\\theta}{\\sqrt{\\sigma_{x}^{2} + \\tau_{t}}}\\right) \\right]}$$"
        },
        {
            "introduction": "While the theoretical predictions of State Evolution are elegant, standard AMP can fail to converge in practice, especially when the sensing matrix is ill-conditioned. This problem addresses this practical challenge by introducing damping, a simple yet effective stabilization technique. You will perform a local stability analysis of the damped AMP algorithm by linearizing its dynamics around a fixed point . This practice offers a concrete lesson in algorithm design, showing how to choose an optimal damping parameter to guarantee convergence by minimizing the spectral radius of the iterative map.",
            "id": "3432089",
            "problem": "Consider linear inverse estimation in compressed sensing with measurements $y = A x_0 + w$, where $x_0 \\in \\mathbb{R}^n$ is a sparse signal, $A \\in \\mathbb{R}^{m \\times n}$ is an ill-conditioned sensing matrix with right singular vectors forming an orthonormal basis and two distinct singular values $s_1$ and $s_2$, and $w \\in \\mathbb{R}^m$ is additive white Gaussian noise. The Approximate Message Passing (AMP) algorithm with damping updates the estimate via $x^{t+1} = (1 - \\alpha_t) x^t + \\alpha_t \\eta(x^t + A^{\\top} r^t)$, where $\\eta$ is a differentiable denoiser, $r^t = y - A x^t + b_t r^{t-1}$ is the residual with the Onsager coefficient $b_t$ equal to the average divergence of the denoiser scaled by the inverse measurement rate (the measurement rate is $\\delta = m/n$), and $\\alpha_t \\in (0,1)$ is the damping parameter. Let the fixed point $(x^{\\star}, r^{\\star})$ satisfy the State Evolution (SE) consistency conditions, and suppose the average denoiser derivative at the fixed point is $\\gamma = \\mathbb{E}[\\eta'(u^{\\star})]$.\n\nLinearize the SE-induced error dynamics in the right singular vector basis of $A$ around the fixed point $x^{\\star}$. For the two ill-conditioned modes associated with singular values $s_1$ and $s_2$, model the undamped Jacobian of the linearized map as $J_0 = I - \\kappa \\Sigma$, where $\\Sigma = \\operatorname{diag}(s_1^2, s_2^2)$ and $\\kappa = \\gamma / \\delta$. With damping $\\alpha \\in (0,1)$ applied to the iterate as above, the linearized damped error map has Jacobian $J(\\alpha) = (1 - \\alpha) I + \\alpha J_0$, whose spectral radius is $\\rho(J(\\alpha)) = \\max\\{ |(1 - \\alpha) + \\alpha \\lambda_1|, |(1 - \\alpha) + \\alpha \\lambda_2| \\}$, where $\\lambda_i = 1 - \\kappa s_i^2$ are the undamped eigenvalues.\n\nGiven an ill-conditioned sensing matrix whose two dominant singular values are $s_1 = 1$ and $s_2 = 3$, a measurement rate $\\delta = 1$, and an average denoiser derivative at the fixed point $\\gamma = \\frac{1}{2}$, choose the damping parameter $\\alpha \\in (0,1)$ that minimizes the spectral radius $\\rho(J(\\alpha))$ of the linearized SE map around the fixed point. Express the final answer as an exact real number without rounding.",
            "solution": "The problem requires finding the optimal damping parameter $\\alpha \\in (0,1)$ that minimizes the spectral radius $\\rho(J(\\alpha))$ of the linearized State Evolution (SE) map for an Approximate Message Passing (AMP) algorithm. The spectral radius governs the local convergence rate of the algorithm around the fixed point, and a smaller spectral radius implies faster convergence.\n\nThe spectral radius is given by the expression:\n$$ \\rho(J(\\alpha)) = \\max\\{ |(1 - \\alpha) + \\alpha \\lambda_1|, |(1 - \\alpha) + \\alpha \\lambda_2| \\} $$\nwhere $\\lambda_1$ and $\\lambda_2$ are the eigenvalues of the undamped Jacobian map $J_0$. These eigenvalues are determined by the singular values of the sensing matrix $A$ and a parameter $\\kappa$.\n\nThe parameter $\\kappa$ is defined as the ratio of the average denoiser derivative at the fixed point, $\\gamma$, to the measurement rate, $\\delta$. The problem provides the following values:\n-   Average denoiser derivative: $\\gamma = \\frac{1}{2}$\n-   Measurement rate: $\\delta = 1$\n\nUsing these values, we calculate $\\kappa$:\n$$ \\kappa = \\frac{\\gamma}{\\delta} = \\frac{1/2}{1} = \\frac{1}{2} $$\n\nThe eigenvalues $\\lambda_i$ are given by the formula $\\lambda_i = 1 - \\kappa s_i^2$, where $s_i$ are the singular values of the matrix $A$. The problem specifies two singular values:\n-   $s_1 = 1$\n-   $s_2 = 3$\n\nWe can now compute the eigenvalues $\\lambda_1$ and $\\lambda_2$:\n$$ \\lambda_1 = 1 - \\kappa s_1^2 = 1 - \\frac{1}{2} (1)^2 = 1 - \\frac{1}{2} = \\frac{1}{2} $$\n$$ \\lambda_2 = 1 - \\kappa s_2^2 = 1 - \\frac{1}{2} (3)^2 = 1 - \\frac{9}{2} = -\\frac{7}{2} $$\n\nSubstituting these eigenvalues into the expression for the spectral radius, we obtain the function of $\\alpha$ to be minimized:\n$$ \\rho(\\alpha) = \\max\\left\\{ \\left|(1 - \\alpha) + \\alpha \\left(\\frac{1}{2}\\right)\\right|, \\left|(1 - \\alpha) + \\alpha \\left(-\\frac{7}{2}\\right)\\right| \\right\\} $$\nSimplifying the terms inside the absolute values:\n$$ \\rho(\\alpha) = \\max\\left\\{ \\left|1 - \\frac{1}{2}\\alpha\\right|, \\left|1 - \\frac{9}{2}\\alpha\\right| \\right\\} $$\nWe need to find the value of $\\alpha \\in (0,1)$ that minimizes this function. Let $g_1(\\alpha) = 1 - \\frac{1}{2}\\alpha$ and $g_2(\\alpha) = 1 - \\frac{9}{2}\\alpha$. The objective is to minimize $\\max(|g_1(\\alpha)|, |g_2(\\alpha)|)$.\n\nThe minimum of a function of the form $\\max(|f_1(x)|, |f_2(x)|)$ is often found where the magnitudes are equal, i.e., $|f_1(x)| = |f_2(x)|$. This is because if one magnitude is larger than the other, we can typically adjust $x$ to decrease the larger magnitude (and thus the maximum) until the magnitudes become equal. This holds true when one function is increasing and the other is decreasing.\n\nLet's analyze the functions on the interval $\\alpha \\in (0,1)$.\nThe function $|g_1(\\alpha)| = |1 - \\frac{1}{2}\\alpha|$ is simply $1 - \\frac{1}{2}\\alpha$ for $\\alpha \\in (0,1)$, as $1 - \\frac{1}{2}\\alpha > 0$. This is a linearly decreasing function.\nThe function $|g_2(\\alpha)| = |1 - \\frac{9}{2}\\alpha|$ has a V-shape. The argument $1 - \\frac{9}{2}\\alpha$ is zero when $\\alpha = \\frac{2}{9}$.\n- For $\\alpha \\in (0, \\frac{2}{9})$, $|g_2(\\alpha)| = 1 - \\frac{9}{2}\\alpha$, which is a decreasing function.\n- For $\\alpha \\in (\\frac{2}{9}, 1)$, $|g_2(\\alpha)| = -(1 - \\frac{9}{2}\\alpha) = \\frac{9}{2}\\alpha - 1$, which is an increasing function.\n\nThe overall function $\\rho(\\alpha)$ is the upper envelope of $|g_1(\\alpha)|$ and $|g_2(\\alpha)|$. The minimum of this upper envelope will occur at the point where the two functions intersect, provided this intersection lies between the individual minima of the two functions. The minimum of $|g_1(\\alpha)|$ is at $\\alpha=2$ and the minimum of $|g_2(\\alpha)|$ is at $\\alpha=2/9$. We seek a minimum for $\\alpha \\in (0,1)$. The minimum of the maximum must lie to the right of $\\alpha=2/9$, where $|g_1(\\alpha)|$ is decreasing and $|g_2(\\alpha)|$ is increasing. The minimum of their maximum is therefore at their intersection point.\n\nWe set the magnitudes equal:\n$$ |1 - \\frac{1}{2}\\alpha| = |1 - \\frac{9}{2}\\alpha| $$\nThis yields two possibilities:\n1.  $1 - \\frac{1}{2}\\alpha = 1 - \\frac{9}{2}\\alpha$. This implies $\\frac{8}{2}\\alpha = 0$, so $\\alpha=0$. This is on the boundary of the domain and does not provide an optimal balance. It corresponds to no update.\n2.  $1 - \\frac{1}{2}\\alpha = -(1 - \\frac{9}{2}\\alpha)$. This corresponds to the intersection of the decreasing function $|g_1(\\alpha)|$ and the increasing part of $|g_2(\\alpha)|$.\n$$ 1 - \\frac{1}{2}\\alpha = -1 + \\frac{9}{2}\\alpha $$\nSolving for $\\alpha$:\n$$ 2 = \\frac{9}{2}\\alpha + \\frac{1}{2}\\alpha $$\n$$ 2 = \\frac{10}{2}\\alpha $$\n$$ 2 = 5\\alpha $$\n$$ \\alpha = \\frac{2}{5} $$\nThe value $\\alpha = \\frac{2}{5}$ is in the specified interval $(0,1)$. At this point, the function $\\rho(\\alpha)$ transitions from being determined by $|g_1(\\alpha)|$ (which is decreasing) to being determined by $|g_2(\\alpha)|$ (which is increasing). Therefore, this point corresponds to the minimum value of $\\rho(\\alpha)$ for $\\alpha \\in (0,1)$.\n\nThe optimal damping parameter is $\\frac{2}{5}$.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        }
    ]
}