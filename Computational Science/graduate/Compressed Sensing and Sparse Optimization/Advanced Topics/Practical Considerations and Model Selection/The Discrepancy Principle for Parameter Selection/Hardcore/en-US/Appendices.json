{
    "hands_on_practices": [
        {
            "introduction": "This problem will start with the foundational theory of $\\ell_1$-regularization and walk through a direct application of the discrepancy principle. By deriving the solution structure for the LASSO problem under the simplifying assumption of an orthonormal design matrix, you will establish a clear, explicit relationship between the residual, the data, and the regularization parameter $\\lambda$. This exercise provides essential practice in connecting first-order optimality conditions to a concrete parameter selection rule, solidifying your understanding of how the principle works at a mechanical level .",
            "id": "3487555",
            "problem": "Consider the $\\ell_{1}$-regularized least squares problem (least absolute shrinkage and selection operator (LASSO))\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwith $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda  0$. The discrepancy principle prescribes choosing $\\lambda$ so that the data misfit meets a known noise level, that is,\n$$\n\\|A x_{\\lambda} - y\\|_{2} = \\delta,\n$$\nfor a given $\\delta  0$. Assume $A$ has orthonormal columns, that is, $A^{\\top}A = I_{n}$.\n\nStarting from the fundamental first-order optimality condition for convex composite minimization (Karush–Kuhn–Tucker (KKT) subgradient stationarity) and the definition of the subdifferential of the $\\ell_{1}$-norm, derive how the residual $r_{\\lambda} := y - A x_{\\lambda}$ encodes dual feasibility through a vector $z \\in \\partial \\|x_{\\lambda}\\|_{1}$ and how this encodes the sparsity pattern of a dual certificate. Specialize your derivation to the case $A = I_{n}$ to obtain an explicit, coordinate-wise relation between $r_{\\lambda}$, $y$, and $\\lambda$.\n\nNow take $n = 5$, $A = I_{5}$, and\n$$\ny = \\begin{pmatrix}4 \\\\ -2 \\\\ 1 \\\\ 0.5 \\\\ -0.25\\end{pmatrix}.\n$$\nSuppose the noise level is known and the discrepancy principle is enforced with\n$$\n\\delta = \\frac{\\sqrt{93}}{4}.\n$$\nUsing only the principles above and your specialized residual relation, determine the exact $\\lambda  0$ selected by the discrepancy principle. Give your final answer as an exact value; do not round or approximate.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of convex optimization and inverse problems, well-posed with sufficient information for a unique solution, and stated using objective, formal language. We proceed with the solution.\n\nThe objective function for the LASSO problem is\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}.\n$$\nThis is a convex function, being the sum of a convex quadratic and the convex $\\ell_1$-norm. A point $x_{\\lambda} \\in \\mathbb{R}^{n}$ is a minimizer if and only if the zero vector is in the subdifferential of $J$ at $x_{\\lambda}$. The first-order optimality condition is:\n$$\n0 \\in \\partial J(x_{\\lambda}).\n$$\nUsing the sum rule for subdifferentials, we have:\n$$\n\\partial J(x_{\\lambda}) = \\nabla\\left(\\frac{1}{2}\\|A x_{\\lambda} - y\\|_{2}^{2}\\right) + \\partial\\left(\\lambda \\|x_{\\lambda}\\|_{1}\\right).\n$$\nThe gradient of the least-squares term is $A^{\\top}(A x_{\\lambda} - y)$. The subdifferential of the $\\ell_1$-term is $\\lambda \\partial \\|x_{\\lambda}\\|_{1}$. So, the optimality condition becomes:\n$$\n0 \\in A^{\\top}(A x_{\\lambda} - y) + \\lambda \\partial \\|x_{\\lambda}\\|_{1}.\n$$\nLet the residual be $r_{\\lambda} := y - A x_{\\lambda}$. The condition can be rewritten as:\n$$\nA^{\\top}r_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}.\n$$\nThis implies that there exists a vector $z \\in \\partial \\|x_{\\lambda}\\|_{1}$ such that $A^{\\top}r_{\\lambda} = \\lambda z$. The vector $z$ is a subgradient of the $\\ell_1$-norm at $x_{\\lambda}$. The components of any such $z$ must satisfy:\n$$\nz_i = \\begin{cases} \\operatorname{sign}((x_{\\lambda})_i)  \\text{if } (x_{\\lambda})_i \\neq 0 \\\\ \\in [-1, 1]  \\text{if } (x_{\\lambda})_i = 0 \\end{cases}\n$$\nfor $i=1, \\dots, n$. The relation $A^{\\top}r_{\\lambda} = \\lambda z$ shows how the residual $r_{\\lambda}$ encodes dual feasibility. The vector $\\nu = \\frac{1}{\\lambda}A^{\\top}r_{\\lambda}$ serves as a dual certificate. The optimality conditions on $\\nu=z$ are $\\|\\nu\\|_{\\infty} \\le 1$, and for any index $i$ in the support of $x_{\\lambda}$ (where $(x_{\\lambda})_i \\neq 0$), we must have $\\nu_i = \\operatorname{sign}((x_{\\lambda})_i)$. This means the active components of the primal solution $x_{\\lambda}$ correspond to the components of the dual certificate $\\nu$ that are at the boundary of the feasible region $[-1, 1]$. This is how the sparsity pattern of $x_{\\lambda}$ is encoded.\n\nNow, we specialize this derivation to the case where $A = I_n$. The condition $A^{\\top}A=I_n$ is satisfied. The optimality condition $A^{\\top}r_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}$ simplifies to $r_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}$, since $A=I_n$.\nThe residual is $r_{\\lambda} = y - A x_{\\lambda} = y - x_{\\lambda}$.\nSo, we have $y - x_{\\lambda} \\in \\lambda \\partial \\|x_{\\lambda}\\|_{1}$, which means there exists $z \\in \\partial \\|x_{\\lambda}\\|_{1}$ such that $y - x_{\\lambda} = \\lambda z$, or\n$$\nx_{\\lambda} = y - \\lambda z.\n$$\nWe analyze this component-wise, $(x_{\\lambda})_i = y_i - \\lambda z_i$.\n1.  If $(x_{\\lambda})_i \\neq 0$, then $z_i = \\operatorname{sign}((x_{\\lambda})_i)$. The equation is $(x_{\\lambda})_i = y_i - \\lambda \\operatorname{sign}((x_{\\lambda})_i)$. Rearranging gives $y_i = (x_{\\lambda})_i + \\lambda \\operatorname{sign}((x_{\\lambda})_i)$. Since $\\lambda  0$, the sign of the right side is $\\operatorname{sign}((x_{\\lambda})_i)$. Therefore, $\\operatorname{sign}((x_{\\lambda})_i) = \\operatorname{sign}(y_i)$. For this to hold, we must have $|y_i|  \\lambda$. If this is the case, $(x_{\\lambda})_i = y_i - \\lambda \\operatorname{sign}(y_i)$.\n2.  If $(x_{\\lambda})_i = 0$, then $z_i \\in [-1, 1]$. The equation becomes $0 = y_i - \\lambda z_i$, which implies $z_i = y_i / \\lambda$. The condition $|z_i| \\le 1$ translates to $|y_i/\\lambda| \\le 1$, or $|y_i| \\le \\lambda$.\n\nSo, the solution $x_{\\lambda}$ is given by the soft-thresholding operator: $(x_{\\lambda})_i = \\operatorname{sign}(y_i)\\max(|y_i|-\\lambda, 0)$.\n\nWe now derive the coordinate-wise relation for the residual $r_{\\lambda} = y - x_{\\lambda}$.\n1.  If $|y_i|  \\lambda$, then $(x_{\\lambda})_i = y_i - \\lambda \\operatorname{sign}(y_i)$. The residual component is $(r_{\\lambda})_i = y_i - (y_i - \\lambda \\operatorname{sign}(y_i)) = \\lambda \\operatorname{sign}(y_i)$.\n2.  If $|y_i| \\le \\lambda$, then $(x_{\\lambda})_i = 0$. The residual component is $(r_{\\lambda})_i = y_i - 0 = y_i$.\n\nThis establishes the required coordinate-wise relation between $r_{\\lambda}$, $y$, and $\\lambda$:\n$$\n(r_{\\lambda})_i = \\begin{cases} \\lambda \\operatorname{sign}(y_i)  \\text{if } |y_i|  \\lambda \\\\ y_i  \\text{if } |y_i| \\le \\lambda \\end{cases}\n$$\nThis can be written compactly as $(r_{\\lambda})_i = \\operatorname{sign}(y_i) \\min(|y_i|, \\lambda)$.\n\nNext, we use this relation to find the specific $\\lambda$ requested. We are given $n=5$, $A=I_5$,\n$y = \\begin{pmatrix}4 \\\\ -2 \\\\ 1 \\\\ 0.5 \\\\ -0.25\\end{pmatrix}$, and $\\delta = \\frac{\\sqrt{93}}{4}$.\nThe discrepancy principle states $\\|A x_{\\lambda} - y\\|_{2} = \\delta$, which is $\\|r_{\\lambda}\\|_{2} = \\delta$. Squaring both sides, we need to solve $\\|r_{\\lambda}\\|_{2}^{2} = \\delta^2 = \\frac{93}{16}$.\nThe squared norm of the residual is:\n$$\n\\|r_{\\lambda}\\|_{2}^2 = \\sum_{i=1}^5 (r_{\\lambda})_i^2 = \\sum_{i=1}^5 (\\min(|y_i|, \\lambda))^2.\n$$\nThe absolute values of the components of $y$ are $|y_1|=4$, $|y_2|=2$, $|y_3|=1$, $|y_4|=0.5=\\frac{1}{2}$, and $|y_5|=0.25=\\frac{1}{4}$.\nLet's define the function $f(\\lambda) = \\sum_{i=1}^5 (\\min(|y_i|, \\lambda))^2$. We need to solve $f(\\lambda) = \\frac{93}{16}$ for $\\lambda  0$. The function $f(\\lambda)$ is continuous and piecewise quadratic, with break-points at $\\lambda \\in \\{\\frac{1}{4}, \\frac{1}{2}, 1, 2, 4\\}$.\n\n- For $0  \\lambda \\le \\frac{1}{4}$: $\\min(|y_i|, \\lambda) = \\lambda$ for all $i$.\n$f(\\lambda) = 5\\lambda^2$.\n$5\\lambda^2 = \\frac{93}{16} \\implies \\lambda^2 = \\frac{93}{80}  1 \\implies \\lambda  1$. No solution in this interval.\n\n- For $\\frac{1}{4}  \\lambda \\le \\frac{1}{2}$: $\\min(|y_5|, \\lambda) = \\frac{1}{4}$, and $\\min(|y_i|, \\lambda) = \\lambda$ for $i=1,2,3,4$.\n$f(\\lambda) = 4\\lambda^2 + (\\frac{1}{4})^2 = 4\\lambda^2 + \\frac{1}{16}$.\n$4\\lambda^2 + \\frac{1}{16} = \\frac{93}{16} \\implies 4\\lambda^2 = \\frac{92}{16} = \\frac{23}{4} \\implies \\lambda^2 = \\frac{23}{16}$.\n$\\lambda = \\frac{\\sqrt{23}}{4} \\approx \\frac{4.8}{4} = 1.2$. This is not in $(\\frac{1}{4}, \\frac{1}{2}]$.\n\n- For $\\frac{1}{2}  \\lambda \\le 1$: $\\min(|y_5|, \\lambda) = \\frac{1}{4}$, $\\min(|y_4|, \\lambda) = \\frac{1}{2}$, and $\\min(|y_i|, \\lambda) = \\lambda$ for $i=1,2,3$.\n$f(\\lambda) = 3\\lambda^2 + (\\frac{1}{2})^2 + (\\frac{1}{4})^2 = 3\\lambda^2 + \\frac{1}{4} + \\frac{1}{16} = 3\\lambda^2 + \\frac{5}{16}$.\n$3\\lambda^2 + \\frac{5}{16} = \\frac{93}{16} \\implies 3\\lambda^2 = \\frac{88}{16} = \\frac{11}{2} \\implies \\lambda^2 = \\frac{11}{6}$.\n$\\lambda = \\sqrt{\\frac{11}{6}} \\approx 1.35$. This is not in $(\\frac{1}{2}, 1]$.\n\n- For $1  \\lambda \\le 2$: $\\min(|y_5|, \\lambda) = \\frac{1}{4}$, $\\min(|y_4|, \\lambda) = \\frac{1}{2}$, $\\min(|y_3|, \\lambda) = 1$, and $\\min(|y_i|, \\lambda) = \\lambda$ for $i=1,2$.\n$f(\\lambda) = 2\\lambda^2 + 1^2 + (\\frac{1}{2})^2 + (\\frac{1}{4})^2 = 2\\lambda^2 + 1 + \\frac{1}{4} + \\frac{1}{16} = 2\\lambda^2 + \\frac{16+4+1}{16} = 2\\lambda^2 + \\frac{21}{16}$.\nWe set this equal to $\\frac{93}{16}$:\n$$\n2\\lambda^2 + \\frac{21}{16} = \\frac{93}{16}\n$$\n$$\n2\\lambda^2 = \\frac{93-21}{16} = \\frac{72}{16} = \\frac{9}{2}\n$$\n$$\n\\lambda^2 = \\frac{9}{4}\n$$\nSince $\\lambda  0$, we take the positive root: $\\lambda = \\sqrt{\\frac{9}{4}} = \\frac{3}{2} = 1.5$.\nThe value $\\lambda = 1.5$ is in the interval $(1, 2]$. Therefore, this is the correct solution.\n\nTo verify, let's calculate $f(1.5)$:\n$f(1.5) = (\\min(4, 1.5))^2 + (\\min(2, 1.5))^2 + (\\min(1, 1.5))^2 + (\\min(0.5, 1.5))^2 + (\\min(0.25, 1.5))^2$\n$f(1.5) = (1.5)^2 + (1.5)^2 + 1^2 + (0.5)^2 + (0.25)^2$\n$f(1.5) = (\\frac{3}{2})^2 + (\\frac{3}{2})^2 + 1^2 + (\\frac{1}{2})^2 + (\\frac{1}{4})^2 = \\frac{9}{4} + \\frac{9}{4} + 1 + \\frac{1}{4} + \\frac{1}{16}$\n$f(1.5) = \\frac{18}{4} + \\frac{1}{4} + 1 + \\frac{1}{16} = \\frac{19}{4} + 1 + \\frac{1}{16} = \\frac{76}{16} + \\frac{16}{16} + \\frac{1}{16} = \\frac{93}{16}$.\nThe calculation is correct.\nThe exact value of $\\lambda$ selected by the discrepancy principle is $\\frac{3}{2}$.",
            "answer": "$$\n\\boxed{\\frac{3}{2}}\n$$"
        },
        {
            "introduction": "While the discrepancy principle often provides a unique parameter, it's crucial to understand when it might not. This problem explores a scenario where the residual function is not strictly monotonic, leading to an entire range of regularization parameters that satisfy the discrepancy equation. By constructing and analyzing a specific case with a rank-deficient sensing matrix, you will learn to identify the conditions that cause this ambiguity and appreciate the need for tie-breaking rules to ensure a well-defined selection procedure .",
            "id": "3487594",
            "problem": "Consider the parameter selection problem for the Least Absolute Shrinkage and Selection Operator (LASSO), defined for a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, a measurement vector $y \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda \\geq 0$ by the convex objective\n$$\nx_{\\lambda} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\nDefine the residual function $r(\\lambda) = \\|A x_{\\lambda} - y\\|_{2}$. The discrepancy principle seeks a parameter $\\lambda$ such that $r(\\lambda) = \\delta$, where $\\delta  0$ is a given noise level estimate.\n\nConstruct a concrete counterexample in which the mapping $\\lambda \\mapsto r(\\lambda)$ has a flat segment, so that multiple values of $\\lambda$ satisfy the discrepancy equation $r(\\lambda) = \\delta$. Specifically, take $m=3$, $n=2$, and\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 2  2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}.\n$$\nFor this instance:\n\n1. Starting from first principles of convex optimization (optimality via subgradients) and linear algebra, derive an explicit formula for $r(\\lambda)$ as a function of $\\lambda$.\n\n2. Show that $r(\\lambda)$ has a flat segment and identify its onset point (the smallest $\\lambda$ after which $r(\\lambda)$ is constant).\n\n3. Let the noise level be set to the Euclidean norm of the measurement, $\\delta = \\|y\\|_{2}$. Determine all $\\lambda \\geq 0$ that satisfy the discrepancy principle $r(\\lambda) = \\delta$.\n\n4. In the presence of multiple admissible $\\lambda$, adopt the tie-breaking rule that selects the smallest $\\lambda$ satisfying the discrepancy equation in order to avoid excessive regularization while honoring the noise constraint. Under this rule, compute the selected parameter value. Provide your final answer as a single real number. No rounding is required.",
            "solution": "The user has provided a valid problem statement. The problem is scientifically grounded in the theory of convex optimization and regularization, is well-posed, and all necessary information is provided.\n\nThe problem asks for an analysis of the residual function associated with the Least Absolute Shrinkage and Selection Operator (LASSO) for a specific problem instance. The LASSO solution, denoted by $x_{\\lambda}$, is defined as a minimizer of the objective function:\n$$\nF(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nwhere $x \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda \\ge 0$ is the regularization parameter.\nThe given data for this problem are:\n$m=3$, $n=2$, and\n$$\nA = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 2  2 \\end{pmatrix}, \\qquad y = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**1. Derivation of the residual function $r(\\lambda)$**\n\nFirst, we analyze the structure of the matrix-vector product $Ax$. Let $x = (x_1, x_2)^T$.\n$$\nAx = \\begin{pmatrix} 1  1 \\\\ 2  2 \\\\ 2  2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + x_2 \\\\ 2x_1 + 2x_2 \\\\ 2x_1 + 2x_2 \\end{pmatrix} = (x_1 + x_2) \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\nLet's define the vector $u = (1, 2, 2)^T$ and the scalar variable $s = x_1 + x_2$. Then $Ax = su$. The range of the linear operator $A$ is a one-dimensional subspace of $\\mathbb{R}^3$ spanned by the vector $u$.\n\nThe LASSO objective function can be rewritten by substituting $Ax = su$:\n$$\nF(x) = \\frac{1}{2} \\|su - y\\|_{2}^{2} + \\lambda ( |x_1| + |x_2| )\n$$\nThe minimization over $x \\in \\mathbb{R}^2$ can be decomposed into a nested minimization. We first minimize over all $x$ that produce a given $s = x_1+x_2$, and then minimize over all possible values of $s$:\n$$\n\\min_{x \\in \\mathbb{R}^2} F(x) = \\min_{s \\in \\mathbb{R}} \\left( \\frac{1}{2} \\|su - y\\|_{2}^{2} + \\lambda \\min_{x_1+x_2 = s} \\{|x_1| + |x_2|\\} \\right)\n$$\nThe inner minimization term $\\min_{x_1+x_2 = s} \\{|x_1| + |x_2|\\}$ can be simplified. By the triangle inequality, $|s| = |x_1 + x_2| \\le |x_1| + |x_2|$. This lower bound is achieved by choosing, for example, $x_1 = s$ and $x_2=0$. Thus, $\\min_{x_1+x_2 = s} \\{|x_1| + |x_2|\\} = |s|$.\n\nThe original $2$-dimensional problem reduces to a $1$-dimensional problem in the variable $s$:\n$$\ns_{\\lambda} = \\arg\\min_{s \\in \\mathbb{R}} \\left\\{ G(s) = \\frac{1}{2} \\|su - y\\|_{2}^{2} + \\lambda |s| \\right\\}\n$$\nTo solve this, we first compute the necessary vector products.\nThe squared norm of $u$ is $\\|u\\|_2^2 = u^T u = 1^2 + 2^2 + 2^2 = 1+4+4=9$.\nThe inner product of $u$ and $y$ is $u^T y = (1)(1) + (2)(2) + (2)(2) = 1+4+4=9$.\nThe term $\\|su-y\\|_2^2$ can be expanded as:\n$$\n\\|su-y\\|_2^2 = (su-y)^T(su-y) = s^2(u^T u) - 2s(u^T y) + y^T y = 9s^2 - 18s + \\|y\\|_2^2\n$$\nThe objective function in $s$, ignoring the constant term $\\frac{1}{2}\\|y\\|_2^2$ which does not affect the minimizer, is:\n$$\nH(s) = \\frac{9}{2}s^2 - 9s + \\lambda |s|\n$$\nWe find the minimizer $s_{\\lambda}$ using the subgradient optimality condition, which states that $0$ must be in the subdifferential of $H(s)$ at $s=s_{\\lambda}$. The subdifferential is:\n$$\n\\partial H(s) = 9s - 9 + \\lambda \\partial|s|\n$$\nwhere $\\partial|s| = \\{\\mathrm{sgn}(s)\\}$ if $s \\neq 0$ and $\\partial|s| = [-1, 1]$ if $s = 0$.\n\nCase 1: $s_{\\lambda}  0$. The condition $0 \\in \\partial H(s_{\\lambda})$ becomes $9s_{\\lambda} - 9 + \\lambda(1) = 0$, which gives $9s_{\\lambda} = 9 - \\lambda$, so $s_{\\lambda} = 1 - \\frac{\\lambda}{9}$. This is consistent with $s_{\\lambda}  0$ if and only if $1 - \\frac{\\lambda}{9}  0$, which means $\\lambda  9$.\n\nCase 2: $s_{\\lambda}  0$. The condition becomes $9s_{\\lambda} - 9 + \\lambda(-1) = 0$, which gives $9s_{\\lambda} = 9 + \\lambda$, so $s_{\\lambda} = 1 + \\frac{\\lambda}{9}$. For $\\lambda \\ge 0$, this yields $s_{\\lambda}  0$, which contradicts the assumption $s_{\\lambda}  0$. So this case is not possible.\n\nCase 3: $s_{\\lambda} = 0$. The condition is $0 \\in 9(0) - 9 + \\lambda[-1, 1]$, which simplifies to $9 \\in [-\\lambda, \\lambda]$. This holds if and only if $\\lambda \\ge 9$.\n\nCombining these cases, the optimal value $s_{\\lambda}$ is:\n$$\ns_{\\lambda} = \\begin{cases} 1 - \\frac{\\lambda}{9}  \\text{if } 0 \\le \\lambda  9 \\\\ 0  \\text{if } \\lambda \\ge 9 \\end{cases}\n$$\nThe residual function is $r(\\lambda) = \\|A x_{\\lambda} - y\\|_{2}$. Since $A x_{\\lambda} = s_{\\lambda}u$, we have $r(\\lambda) = \\|s_{\\lambda}u - y\\|_{2}$.\nWe have $r(\\lambda)^2 = \\|s_{\\lambda}u - y\\|_{2}^2 = 9s_{\\lambda}^2 - 18s_{\\lambda} + \\|y\\|_2^2$. Since $\\|y\\|_2^2=9$, this is $r(\\lambda)^2 = 9s_{\\lambda}^2 - 18s_{\\lambda} + 9 = 9(s_{\\lambda}^2-2s_{\\lambda}+1) = 9(s_{\\lambda}-1)^2$.\nTaking the square root, we get $r(\\lambda) = 3|s_{\\lambda}-1|$.\n\nNow, we substitute the expressions for $s_{\\lambda}$:\n- For $0 \\le \\lambda  9$: $s_{\\lambda} - 1 = (1 - \\frac{\\lambda}{9}) - 1 = -\\frac{\\lambda}{9}$.\n  $r(\\lambda) = 3\\left|-\\frac{\\lambda}{9}\\right| = 3\\frac{\\lambda}{9} = \\frac{\\lambda}{3}$.\n- For $\\lambda \\ge 9$: $s_{\\lambda} - 1 = 0 - 1 = -1$.\n  $r(\\lambda) = 3|-1| = 3$.\n\nThus, the explicit formula for $r(\\lambda)$ is:\n$$\nr(\\lambda) = \\begin{cases} \\frac{\\lambda}{3}  \\text{if } 0 \\le \\lambda  9 \\\\ 3  \\text{if } \\lambda \\ge 9 \\end{cases}\n$$\n\n**2. Flat segment and onset point**\n\nFrom the derived formula, the function $r(\\lambda)$ is constant for all $\\lambda \\ge 9$. The value of the function on this segment is $r(\\lambda)=3$. This is the flat segment. The onset point is the smallest value of $\\lambda$ at which this constant behavior begins, which is $\\lambda=9$.\n\n**3. Solving the discrepancy equation**\n\nThe discrepancy principle requires selecting $\\lambda$ such that $r(\\lambda) = \\delta$, where the noise level is given as $\\delta = \\|y\\|_2$.\nFirst, we compute $\\delta$:\n$$\n\\delta = \\|y\\|_{2} = \\sqrt{1^2 + 2^2 + 2^2} = \\sqrt{1+4+4} = \\sqrt{9} = 3.\n$$\nWe need to solve the equation $r(\\lambda) = 3$.\n\nAgain, we consider the two pieces of the function $r(\\lambda)$:\n- For $0 \\le \\lambda  9$: The equation is $\\frac{\\lambda}{3} = 3$, which implies $\\lambda=9$. However, this value is not in the interval $[0, 9)$, so there is no solution in this range.\n- For $\\lambda \\ge 9$: The equation is $3 = 3$. This is true for all $\\lambda$ in this interval.\n\nTherefore, the set of all values $\\lambda \\ge 0$ that satisfy the discrepancy principle $r(\\lambda)=3$ is the interval $[9, \\infty)$.\n\n**4. Selection of the parameter $\\lambda$**\n\nThe problem specifies a tie-breaking rule for cases where multiple values of $\\lambda$ satisfy the discrepancy equation: select the smallest such $\\lambda$. The set of solutions is $[9, \\infty)$. The smallest value in this set is $9$.\n\nUnder this rule, the selected parameter value is $\\lambda=9$.",
            "answer": "$$\\boxed{9}$$"
        },
        {
            "introduction": "The discrepancy principle's utility extends beyond selecting a continuous parameter like $\\lambda$ in LASSO. This hands-on coding exercise demonstrates how to adapt the principle for iterative algorithms, such as Iterative Hard Thresholding (IHT), where the key parameter is the discrete sparsity level $k$. You will implement and compare two distinct strategies: using the discrepancy condition as an early stopping rule for a fixed-sparsity algorithm versus using it as a criterion to select the optimal sparsity level itself, providing insight into the principle's flexible role in algorithmic design .",
            "id": "3487541",
            "problem": "You are given a linear inverse problem with a sparse ground-truth signal under additive Gaussian noise. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known sensing matrix, $x_\\star \\in \\mathbb{R}^{n}$ be the unknown target vector with at most $s$ nonzero entries, and $y \\in \\mathbb{R}^{m}$ be the measurement vector generated by the model $y = A x_\\star + e$, where $e \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ and $\\sigma  0$ is known. The task is to implement and compare two parameter-selection strategies based on the discrepancy principle for the Iterative Hard Thresholding (IHT) algorithm.\n\nFundamental base for design:\n- The objective for data fidelity is the least-squares function $f(x) = \\tfrac{1}{2} \\|A x - y\\|_2^2$ whose gradient is $\\nabla f(x) = A^\\top (A x - y)$.\n- The gradient $\\nabla f$ is Lipschitz continuous with Lipschitz constant $L = \\|A\\|_2^2$, where $\\|A\\|_2$ is the spectral norm (largest singular value).\n- Iterative Hard Thresholding (IHT) is the projected gradient descent method onto the nonconvex set $S_k = \\{x \\in \\mathbb{R}^n : \\|x\\|_{0} \\le k\\}$ using the update $x^{(t+1)} = \\mathcal{H}_k\\!\\left(x^{(t)} - \\mu \\nabla f\\!\\left(x^{(t)}\\right)\\right)$ with step size $\\mu \\in (0, 2/L)$, where $\\mathcal{H}_k(\\cdot)$ keeps the $k$ largest (by magnitude) entries and sets the rest to zero.\n- The discrepancy principle asserts that, if $\\sigma$ is known, a suitable fit is achieved when the residual norm matches the noise level up to a multiplicative safety factor $\\tau  1$, i.e., when $\\|A x - y\\|_2 \\le \\tau \\sqrt{m}\\, \\sigma$.\n\nYou must implement and compare the following two strategies:\n\n1) Fixed-sparsity IHT with discrepancy stopping:\n- Given a fixed sparsity level $k$, run IHT with step size $\\mu = 1/L$, starting from $x^{(0)} = 0$.\n- Stop as soon as the discrepancy condition $\\|A x^{(t)} - y\\|_2 \\le \\tau \\sqrt{m}\\, \\sigma$ is met, or when a maximum number of iterations $T_{\\max}$ is reached.\n- Return the final residual norm $\\|A \\hat{x}^{(k)} - y\\|_2$, whether the discrepancy condition was reached (boolean), the number of iterations used, and the relative reconstruction error $\\|\\hat{x}^{(k)} - x_\\star\\|_2 / \\|x_\\star\\|_2$.\n\n2) Variable-sparsity selection by discrepancy at convergence:\n- For $k$ ranging from $k_{\\min}$ to $k_{\\max}$, run IHT with step size $\\mu = 1/L$ until convergence (do not use the discrepancy to stop). Convergence is defined by stabilization of the iterate, for example by a small relative change threshold or by support stability, within a maximum number of iterations $T_{\\max}$.\n- After convergence for each $k$, test the discrepancy condition $\\|A x^{(\\infty)} - y\\|_2 \\le \\tau \\sqrt{m}\\, \\sigma$. Select the smallest $k$ that satisfies the discrepancy condition at convergence. If none in $[k_{\\min}, k_{\\max}]$ satisfies the discrepancy, report failure.\n- Return the selected $k$ (or $-1$ on failure), a boolean indicating whether the discrepancy was achieved, the final residual norm, and the relative reconstruction error.\n\nImplementation requirements:\n- Use the hard-thresholding operator $\\mathcal{H}_k$ that keeps the $k$ entries of largest magnitude and zeros out the rest. If $k \\ge n$, then $\\mathcal{H}_k$ is the identity.\n- Use the step size $\\mu = 1/L$ with $L = \\|A\\|_2^2$, computed via the spectral norm $\\|A\\|_2$.\n- For convergence in the variable-$k$ strategy, use a relative change criterion of the form $\\|x^{(t+1)} - x^{(t)}\\|_2 / \\max\\{1, \\|x^{(t)}\\|_2\\} \\le \\varepsilon$, with a small tolerance $\\varepsilon$, or a support-stability condition, with a maximum iteration cap to ensure termination.\n\nTest suite:\nYou must generate synthetic data for each case using independent random seeds, as specified. For each test case, construct $A$ with independent entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, generate a ground-truth $s$-sparse signal $x_\\star$ by choosing $s$ uniformly random support indices and drawing nonzero amplitudes from $\\mathcal{N}(0, 1)$, and then generate $y = A x_\\star + e$ with $e \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. All computations are dimensionless; no physical units are involved.\n\n- Case $1$ (happy path):\n  - $m = 64$, $n = 128$, $s = 8$, $k_{\\text{fixed}} = 8$, $\\sigma = 0.02$, $\\tau = 1.1$, seed $= 1$, $k_{\\min} = 1$, $k_{\\max} = 16$, $T_{\\max} = 1000$, $\\varepsilon = 10^{-6}$.\n\n- Case $2$ (insufficient fixed sparsity):\n  - $m = 64$, $n = 128$, $s = 8$, $k_{\\text{fixed}} = 4$, $\\sigma = 0.02$, $\\tau = 1.1$, seed $= 2$, $k_{\\min} = 1$, $k_{\\max} = 16$, $T_{\\max} = 1000$, $\\varepsilon = 10^{-6}$.\n\n- Case $3$ (overly strict discrepancy):\n  - $m = 64$, $n = 128$, $s = 10$, $k_{\\text{fixed}} = 10$, $\\sigma = 0.05$, $\\tau = 0.9$, seed $= 3$, $k_{\\min} = 1$, $k_{\\max} = 32$, $T_{\\max} = 1200$, $\\varepsilon = 10^{-6}$.\n\nFor each case, compute and report the following outputs:\n- For the fixed-$k$ discrepancy-stopped IHT:\n  - The final residual norm $\\|A \\hat{x}^{(k)} - y\\|_2$ (a real number).\n  - Whether discrepancy was reached before $T_{\\max}$ (a boolean).\n  - The number of iterations used (an integer).\n  - The relative reconstruction error $\\|\\hat{x}^{(k)} - x_\\star\\|_2 / \\|x_\\star\\|_2$ (a real number).\n- For the variable-$k$ selection strategy:\n  - The selected $k$ (an integer, or $-1$ if no $k \\in [k_{\\min}, k_{\\max}]$ achieved discrepancy at convergence).\n  - Whether discrepancy was achieved (a boolean).\n  - The final residual norm (a real number).\n  - The relative reconstruction error (a real number).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sublist in the order:\n  - $\\left[\\|A \\hat{x}^{(k)} - y\\|_2,\\ \\text{disc\\_fixed},\\ \\text{iters\\_fixed},\\ \\text{relErr\\_fixed},\\ k_{\\text{sel}},\\ \\text{disc\\_var},\\ \\|A \\hat{x}^{(\\text{var})} - y\\|_2,\\ \\text{relErr\\_var}\\right]$.\n- For three test cases, the final printed line must therefore be a list of three lists, e.g., $[[\\cdots],[\\cdots],[\\cdots]]$.",
            "solution": "The problem presented requires the implementation and comparison of two distinct strategies for parameter selection within the Iterative Hard Thresholding (IHT) framework, applied to a sparse signal recovery problem. The core of the problem lies in the application of the Morozov discrepancy principle. We shall first formulate the problem rigorously, describe the IHT algorithm, and then detail the two parameter selection schemes.\n\n**Problem Formulation**\n\nWe are given a linear inverse problem described by the model:\n$$ y = A x_\\star + e $$\nwhere:\n- $y \\in \\mathbb{R}^{m}$ is the vector of measured observations.\n- $A \\in \\mathbb{R}^{m \\times n}$ is the known sensing matrix.\n- $x_\\star \\in \\mathbb{R}^{n}$ is the unknown ground-truth signal, which is assumed to be $s$-sparse, meaning it has at most $s$ non-zero entries. This is formally stated as $\\|x_\\star\\|_{0} \\le s$, where $\\|\\cdot\\|_{0}$ denotes the $\\ell_0$-\"norm\" (the count of non-zero elements).\n- $e \\in \\mathbb{R}^{m}$ is an additive noise vector, modeled as white Gaussian noise with $e \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, where $I_m$ is the $m \\times m$ identity matrix and the noise standard deviation $\\sigma  0$ is known.\n\nThe goal is to find an estimate $\\hat{x}$ of $x_\\star$ from the measurements $y$ and the matrix $A$. This is often posed as an optimization problem that balances data fidelity with the sparsity prior. A common approach is to solve:\n$$ \\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|A x - y\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_{0} \\le k $$\nwhere $k$ is an estimate of the true sparsity $s$. The term $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ is the least-squares data fidelity cost function.\n\n**Iterative Hard Thresholding (IHT)**\n\nIHT is a projected gradient descent algorithm designed to solve the sparsity-constrained optimization problem. Each iteration consists of two steps: a standard gradient descent step on the objective function $f(x)$, followed by a projection onto the set of $k$-sparse vectors.\n\nThe gradient of $f(x)$ is given by:\n$$ \\nabla f(x) = A^\\top (A x - y) $$\nThis gradient is Lipschitz continuous with a constant $L$ that can be bounded by $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$, where $\\|A\\|_2$ is the spectral norm of $A$.\n\nThe IHT update rule is given by:\n$$ x^{(t+1)} = \\mathcal{H}_k\\left(x^{(t)} - \\mu \\nabla f\\left(x^{(t)}\\right)\\right) $$\nwhere:\n- $x^{(t)}$ is the estimate at iteration $t$.\n- $\\mu$ is the step size. For guaranteed convergence of the gradient step, we require $\\mu \\in (0, 2/L)$. The problem specifies using $\\mu = 1/L$.\n- $\\mathcal{H}_k(\\cdot)$ is the hard thresholding operator. For a vector $v \\in \\mathbb{R}^n$, $\\mathcal{H}_k(v)$ returns a new vector where only the $k$ components of $v$ with the largest absolute values are retained, and all other components are set to zero. If $k \\geq n$, $\\mathcal{H}_k$ is the identity operator.\n\n**The Discrepancy Principle**\n\nWhen the noise level $\\sigma$ is known, the discrepancy principle provides a disciplined way to choose regularization parameters. It posits that a good reconstruction $\\hat{x}$ should not fit the noisy data $y$ perfectly, as this would involve fitting the noise. Instead, the residual norm $\\|A\\hat{x} - y\\|_2$ should be on the same order as the expected norm of the noise. The expected value of the noise energy is $\\mathbb{E}[\\|e\\|_2^2] = \\sum_{i=1}^m \\mathbb{E}[e_i^2] = m\\sigma^2$, which implies $\\|e\\|_2 \\approx \\sqrt{m}\\sigma$. The discrepancy principle thus sets a target for the residual norm:\n$$ \\|A\\hat{x} - y\\|_2 \\le \\eta $$\nwhere the target residual is defined as $\\eta = \\tau \\sqrt{m}\\, \\sigma$. The parameter $\\tau  1$ is a safety factor to account for the statistical variations of the noise norm. A choice of $\\tau  1$ would demand fitting the data to a level tighter than the typical noise floor, which may be impossible or lead to overfitting.\n\nWe will now detail the two strategies for using this principle with IHT.\n\n**Strategy 1: Fixed-Sparsity IHT with Discrepancy Stopping**\n\nIn this strategy, the model complexity (sparsity $k$) is fixed a priori. The discrepancy principle is used to determine the stopping time for the iterative algorithm.\nThe algorithm proceeds as follows:\n1. Fix the sparsity level $k$.\n2. Initialize the solution estimate, $x^{(0)} = 0$.\n3. Pre-compute the step size $\\mu = 1/\\|A\\|_2^2$ and the discrepancy target $\\eta = \\tau \\sqrt{m}\\sigma$.\n4. Iterate for $t=0, 1, 2, \\dots, T_{\\max}-1$:\n   a. Compute the IHT update: $x^{(t+1)} = \\mathcal{H}_k(x^{(t)} - \\mu A^\\top(Ax^{(t)} - y))$.\n   b. Calculate the residual norm: $r^{(t+1)} = \\|Ax^{(t+1)} - y\\|_2$.\n   c. If $r^{(t+1)} \\le \\eta$, the principle is satisfied. The algorithm terminates, returning $\\hat{x} = x^{(t+1)}$.\n5. If the loop completes without satisfying the condition, the algorithm terminates at $T_{\\max}$ iterations, returning the final iterate $\\hat{x} = x^{(T_{\\max})}$.\n\n**Strategy 2: Variable-Sparsity Selection by Discrepancy at Convergence**\n\nIn this strategy, the discrepancy principle is used to select the model complexity $k$ itself. The idea is to find the simplest model (smallest $k$) that is complex enough to explain the data consistent with the noise level.\nThe algorithm proceeds as follows:\n1. Define a search range for the sparsity level, $[k_{\\min}, k_{\\max}]$.\n2. Pre-compute $\\mu = 1/\\|A\\|_2^2$ and $\\eta = \\tau \\sqrt{m}\\sigma$.\n3. For each $k$ from $k_{\\min}$ to $k_{\\max}$:\n   a. Run the IHT algorithm for the current sparsity $k$. This inner loop does *not* use the discrepancy principle for stopping. Instead, it runs until the iterates converge or a maximum number of iterations $T_{\\max}$ is reached. Convergence is measured by the relative change in the iterate: $\\|x^{(t+1)} - x^{(t)}\\|_2 / \\max\\{1, \\|x^{(t)}\\|_2\\} \\le \\varepsilon$.\n   b. Let the converged solution for sparsity $k$ be $\\hat{x}_k$.\n   c. Check if this solution satisfies the discrepancy principle: $\\|A\\hat{x}_k - y\\|_2 \\le \\eta$.\n   d. If the condition is met, this $k$ is a valid candidate. Since we are iterating from smallest to largest $k$, the first one that satisfies the condition is the selected one. The algorithm terminates, returning $k$ and the corresponding solution $\\hat{x}_k$.\n4. If the loop over all $k$ in $[k_{\\min}, k_{\\max}]$ completes without finding any $k$ that satisfies the condition, the procedure fails. In this case, we report failure by returning $k=-1$.\n\nThe provided test cases will allow a quantitative comparison of these two approaches under different conditions: a well-posed scenario, a scenario with underestimated model complexity for the fixed-k strategy, and a scenario with an overly stringent discrepancy target.",
            "answer": "```python\nimport numpy as np\n\ndef hard_threshold(v, k):\n    \"\"\"\n    Hard-thresholding operator H_k.\n    Keeps the k largest (by magnitude) entries of v and sets others to zero.\n    \"\"\"\n    if k = 0:\n        return np.zeros_like(v)\n    if k = v.shape[0]:\n        return v\n    \n    # Get indices of the k largest magnitude entries\n    indices = np.argsort(np.abs(v))[::-1][:k]\n    \n    v_k = np.zeros_like(v)\n    v_k[indices] = v[indices]\n    return v_k\n\ndef generate_data(m, n, s, sigma, seed):\n    \"\"\"\n    Generates synthetic data for the sparse recovery problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate matrix A\n    A = rng.normal(0, 1.0/np.sqrt(m), (m, n))\n    \n    # Generate sparse signal x_star\n    x_star = np.zeros(n)\n    support = rng.choice(n, s, replace=False)\n    x_star[support] = rng.normal(0, 1, s)\n    \n    # Generate noise e\n    noise = rng.normal(0, sigma, m)\n    \n    # Generate measurements y\n    y = A @ x_star + noise\n    \n    return A, y, x_star\n\ndef iht_fixed_k_discrepancy(A, y, x_star, k, sigma, tau, T_max):\n    \"\"\"\n    Implements Strategy 1: Fixed-sparsity IHT with discrepancy stopping.\n    \"\"\"\n    m, n = A.shape\n    \n    L = np.linalg.norm(A, ord=2)**2\n    mu = 1.0 / L\n    \n    discrepancy_target = tau * np.sqrt(m) * sigma\n    \n    x = np.zeros(n)\n    discrepancy_reached = False\n    \n    iters = 0\n    for t in range(T_max):\n        iters = t + 1\n        grad = A.T @ (A @ x - y)\n        x_update = x - mu * grad\n        x = hard_threshold(x_update, k)\n        \n        residual_norm = np.linalg.norm(A @ x - y)\n        if residual_norm = discrepancy_target:\n            discrepancy_reached = True\n            break\n            \n    final_residual_norm = np.linalg.norm(A @ x - y)\n    relative_error = np.linalg.norm(x - x_star) / np.linalg.norm(x_star)\n    \n    return final_residual_norm, discrepancy_reached, iters, relative_error\n\ndef iht_variable_k_selection(A, y, x_star, k_min, k_max, sigma, tau, T_max, epsilon):\n    \"\"\"\n    Implements Strategy 2: Variable-sparsity selection by discrepancy at convergence.\n    \"\"\"\n    m, n = A.shape\n\n    L = np.linalg.norm(A, ord=2)**2\n    mu = 1.0 / L\n\n    discrepancy_target = tau * np.sqrt(m) * sigma\n\n    selected_k = -1\n    discrepancy_achieved = False\n    final_x = None\n\n    for k in range(k_min, k_max + 1):\n        x = np.zeros(n)\n        for t in range(T_max):\n            x_old = x.copy()\n            grad = A.T @ (A @ x - y)\n            x_update = x - mu * grad\n            x = hard_threshold(x_update, k)\n            \n            norm_x_old = np.linalg.norm(x_old)\n            rel_change = np.linalg.norm(x - x_old) / max(1.0, norm_x_old)\n            \n            if rel_change = epsilon:\n                break\n        \n        # After convergence for sparsity k, check discrepancy\n        residual_norm_k = np.linalg.norm(A @ x - y)\n\n        if residual_norm_k = discrepancy_target:\n            selected_k = k\n            discrepancy_achieved = True\n            final_x = x\n            break \n        \n        # Store the last iterate in case of failure\n        if k == k_max:\n             final_x = x\n             \n    if final_x is None: # This case should not happen if k_max = k_min\n        final_x = np.zeros(n)\n\n    final_residual_norm = np.linalg.norm(A @ final_x - y)\n    relative_error = np.linalg.norm(final_x - x_star) / np.linalg.norm(x_star)\n\n    return selected_k, discrepancy_achieved, final_residual_norm, relative_error\n\ndef solve():\n    \"\"\"\n    Main solver function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: \"happy path\"\n        {'m': 64, 'n': 128, 's': 8, 'k_fixed': 8, 'sigma': 0.02, 'tau': 1.1, \n         'seed': 1, 'k_min': 1, 'k_max': 16, 'T_max': 1000, 'eps': 1e-6},\n        # Case 2: \"insufficient fixed sparsity\"\n        {'m': 64, 'n': 128, 's': 8, 'k_fixed': 4, 'sigma': 0.02, 'tau': 1.1,\n         'seed': 2, 'k_min': 1, 'k_max': 16, 'T_max': 1000, 'eps': 1e-6},\n        # Case 3: \"overly strict discrepancy\"\n        {'m': 64, 'n': 128, 's': 10, 'k_fixed': 10, 'sigma': 0.05, 'tau': 0.9,\n         'seed': 3, 'k_min': 1, 'k_max': 32, 'T_max': 1200, 'eps': 1e-6}\n    ]\n\n    all_case_results_str = []\n\n    for case in test_cases:\n        A, y, x_star = generate_data(case['m'], case['n'], case['s'], case['sigma'], case['seed'])\n        \n        # Strategy 1\n        res_norm_fixed, disc_fixed, iters_fixed, rel_err_fixed = iht_fixed_k_discrepancy(\n            A, y, x_star, case['k_fixed'], case['sigma'], case['tau'], case['T_max']\n        )\n        \n        # Strategy 2\n        k_sel, disc_var, res_norm_var, rel_err_var = iht_variable_k_selection(\n            A, y, x_star, case['k_min'], case['k_max'], case['sigma'], case['tau'], case['T_max'], case['eps']\n        )\n        \n        case_str = (\n            f\"[{res_norm_fixed:.8f},\"\n            f\"{str(disc_fixed).lower()},\"\n            f\"{iters_fixed},\"\n            f\"{rel_err_fixed:.8f},\"\n            f\"{k_sel},\"\n            f\"{str(disc_var).lower()},\"\n            f\"{res_norm_var:.8f},\"\n            f\"{rel_err_var:.8f}]\"\n        )\n        all_case_results_str.append(case_str)\n        \n    final_output = f\"[{','.join(all_case_results_str)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}