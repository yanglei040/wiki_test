{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is designed to give you direct, hands-on experience implementing a backtracking line search. You will apply the proximal gradient method to the classic LASSO problem, using a backtracking procedure to automatically guarantee sufficient decrease in the objective function at each step. This practice solidifies the core mechanism of backtracking and its crucial role in ensuring an optimization algorithm's convergence. ",
            "id": "3432732",
            "problem": "Consider the composite convex objective function $F(x) = f(x) + g(x)$ with $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ and $g(x) = \\lambda \\|x\\|_1$, where $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$, and $\\lambda > 0$. The gradient of $f$ is $\\nabla f(x) = A^\\top (A x - y)$, which is Lipschitz continuous with constant $L = \\|A^\\top A\\|_2$. The Proximal Gradient Method (also known as Iterative Shrinkage-Thresholding Algorithm) updates $x$ via the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding mapping defined entrywise by $S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$.\n\nYou are to implement a hybrid fixed-step and backtracking line search strategy for step size selection in the proximal gradient iteration applied to $F(x)$. The hybrid strategy operates as follows for iteration $k$:\n\n- Given the current iterate $x_k$, choose a trial step size $t \\leftarrow t_{\\text{fixed}}$.\n- Compute the candidate update $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n- Check the sufficient decrease condition derived from the Descent Lemma for Lipschitz gradients:\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}).\n$$\n- If the inequality fails, shrink the step size $t \\leftarrow \\beta t$ with $\\beta \\in (0,1)$ and recompute the candidate until the condition holds. Count each shrink as one backtracking step.\n- The hybrid aspect is that each iteration always restarts the line search from the same prescribed $t_{\\text{fixed}}$ (rather than reusing the previously accepted step size). This prioritizes a fixed-step attempt but guarantees sufficient decrease via backtracking when needed.\n\nImplement this hybrid method to run a fixed number of iterations starting from $x_0 = 0$. For each specified test case, return two quantities: the final objective value $F(x_T)$ after $T$ iterations and the total number of backtracking shrink steps accumulated over all iterations.\n\nUse the following test suite:\n\n- Test case $1$ (well-conditioned matrix, conservative step, minimal backtracking expected):\n$$\nA_1 = \\begin{bmatrix}\n1 & 0 & 0.5 & 0 \\\\\n0 & 1 & 0.5 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{bmatrix},\\quad\ny_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix},\\quad\n\\lambda_1 = 0.1,\\quad\nt_{\\text{fixed},1} = 0.05,\\quad\n\\beta_1 = 0.5,\\quad\nT_1 = 100.\n$$\n\n- Test case $2$ (same data, overly aggressive fixed step to trigger repeated backtracking):\n$$\nA_1 \\text{ and } y_1 \\text{ as above},\\quad\n\\lambda_2 = 0.1,\\quad\nt_{\\text{fixed},2} = 10.0,\\quad\n\\beta_2 = 0.5,\\quad\nT_2 = 100.\n$$\n\n- Test case $3$ (more ill-conditioned matrix, moderately aggressive fixed step):\n$$\nA_3 = \\begin{bmatrix}\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 3\n\\end{bmatrix},\\quad\ny_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix},\\quad\n\\lambda_3 = 0.05,\\quad\nt_{\\text{fixed},3} = 5.0,\\quad\n\\beta_3 = 0.5,\\quad\nT_3 = 120.\n$$\n\nAdditional implementation details and requirements:\n\n- Use the proximal gradient update $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$ with the soft-thresholding operator $S_{\\alpha}$ applied elementwise.\n- Use $x_0 = 0$ for all test cases.\n- For numerical stability in the acceptance check, you may allow a tiny non-negative tolerance $\\varepsilon$ so that the condition is enforced as $F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}) + \\varepsilon$ with, for example, $\\varepsilon = 10^{-12}$.\n- There are no physical units involved.\n- For each test case $i \\in \\{1,2,3\\}$, report two values: the final objective value $F(x_{T_i})$ rounded to $6$ decimal places, and the total integer count of backtracking shrink steps over all iterations.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n`[F(x_{T_1}), backtracks_1, F(x_{T_2}), backtracks_2, F(x_{T_3}), backtracks_3]`,\nwhere each $F(x_{T_i})$ is a float rounded to $6$ decimals and each backtracking count is an integer.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n\n- **Objective Function**: $F(x) = f(x) + g(x)$, a composite convex function.\n- **Smooth Term**: $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$, where $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^m$.\n- **Non-Smooth Term**: $g(x) = \\lambda \\|x\\|_1$, where $\\lambda > 0$.\n- **Gradient of Smooth Term**: $\\nabla f(x) = A^\\top (A x - y)$.\n- **Proximal Operator**: The proximal operator of $g(x)$ is the soft-thresholding function, applied entrywise: $S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$.\n- **Iterative Update Rule**: The proximal gradient update is $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n- **Initial Condition**: $x_0 = 0$.\n- **Line Search Strategy (Hybrid Fixed-Step and Backtracking)**:\n    1. For iteration $k$, start with a trial step size $t \\leftarrow t_{\\text{fixed}}$.\n    2. Compute the candidate update $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n    3. Check the sufficient decrease condition: $F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})$.\n    4. If the condition fails, shrink the step size $t \\leftarrow \\beta t$ with $\\beta \\in (0,1)$, count one backtracking step, and repeat from step 2.\n    5. The next main iteration $k+1$ restarts the search from $t_{\\text{fixed}}$.\n- **Numerical Tolerance**: A tolerance $\\varepsilon = 10^{-12}$ is allowed in the sufficient decrease condition.\n- **Test Case 1**:\n    - $A_1 = \\begin{bmatrix} 1 & 0 & 0.5 & 0 \\\\ 0 & 1 & 0.5 & 0 \\\\ 0 & 0 & 1 & 1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix}$\n    - $\\lambda_1 = 0.1$, $t_{\\text{fixed},1} = 0.05$, $\\beta_1 = 0.5$, $T_1 = 100$.\n- **Test Case 2**:\n    - $A_2 = A_1$, $y_2 = y_1$\n    - $\\lambda_2 = 0.1$, $t_{\\text{fixed},2} = 10.0$, $\\beta_2 = 0.5$, $T_2 = 100$.\n- **Test Case 3**:\n    - $A_3 = \\begin{bmatrix} 1 & 2 & 0 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 0 & 1 & 3 \\end{bmatrix}$, $y_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix}$\n    - $\\lambda_3 = 0.05$, $t_{\\text{fixed},3} = 5.0$, $\\beta_3 = 0.5$, $T_3 = 120$.\n- **Output**: For each test case, report the final objective value $F(x_T)$ rounded to $6$ decimal places and the total integer count of backtracking steps.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is a standard application of the Proximal Gradient Method (also known as the Iterative Shrinkage-Thresholding Algorithm or ISTA) to the LASSO objective function. All components—the objective function, the proximal operator (soft-thresholding), the gradient, and the backtracking line search with the specified sufficient decrease condition—are fundamental and well-established concepts in convex optimization and signal processing. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The objective function $F(x)$ is convex, guaranteeing the existence of a minimizer. The algorithm is specified completely with all necessary parameters ($A, y, \\lambda, t_{\\text{fixed}}, \\beta, T, x_0$), ensuring that the sequence of iterates is uniquely determined. The problem asks for the state of the system after a fixed number of iterations, which is a well-defined quantity.\n- **Objective**: The problem is stated in precise mathematical language. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. All matrices, vectors, constants, and initial conditions are provided. The instructions for the algorithm and the required output format are explicit and consistent.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being trivial, unrealistic, or un-verifiable. The specified parameters in the test cases are chosen to demonstrate different behaviors of the algorithm (e.g., conservative vs. aggressive step sizes), which is a standard pedagogical approach.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires the implementation of the Proximal Gradient Method with a specific line search strategy to solve a LASSO-type optimization problem. The core of the method is an iterative procedure to minimize the objective function $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\|Ax - y\\|_2^2$ is a smooth, differentiable loss term and $g(x) = \\lambda \\|x\\|_1$ is a non-smooth regularization term.\n\nThe iterative update at step $k$ is given by the proximal mapping:\n$$\nx_{k+1} = \\text{prox}_{t g}(x_k - t \\nabla f(x_k))\n$$\nwhere $t > 0$ is the step size. For $g(x) = \\lambda \\|x\\|_1$, the proximal operator $\\text{prox}_{t g}(z)$ corresponds to the soft-thresholding operator $S_{\\lambda t}(z)$, which is applied element-wise:\n$$\nS_{\\alpha}(z_i) = \\text{sign}(z_i)\\max(|z_i| - \\alpha, 0)\n$$\nThus, the update rule becomes:\n$$\nx_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))\n$$\nwith $\\nabla f(x_k) = A^\\top(Ax_k - y)$.\n\nThe step size $t$ is determined by a backtracking line search that ensures sufficient decrease in the objective function. For each iteration $k$, the search begins with a fixed trial step size $t = t_{\\text{fixed}}$. A candidate point $x_{k+1}$ is computed. Then, a sufficient decrease condition must be verified. The condition provided is:\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})\n$$\nThis condition is derived from the majorization-minimization principle. The right-hand side is a quadratic upper bound on $F(x)$ around $x_k$. Since $F(x_{k+1}) = f(x_{k+1}) + g(x_{k+1})$, the term $g(x_{k+1})$ appears on both sides and can be canceled. This simplifies the check to:\n$$\nf(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2\n$$\nThis is the standard backtracking condition for proximal gradient methods. If it holds (within a small tolerance $\\varepsilon$), the candidate $x_{k+1}$ is accepted, and the algorithm proceeds to the next iteration, $k+1$. If it fails, the step size $t$ is reduced by a factor $\\beta \\in (0, 1)$ (i.e., $t \\leftarrow \\beta t$), a new candidate $x_{k+1}$ is computed, and the condition is re-checked. Each reduction of $t$ is counted as a backtracking step. This process guarantees that for a sufficiently small $t$, the condition will eventually be met.\n\nThe overall algorithm proceeds as follows for a total of $T$ iterations:\n1. Initialize $x_0 = 0$ and `total_backtracks` $= 0$.\n2. For $k = 0, 1, \\dots, T-1$:\n    a. Set trial step size $t \\leftarrow t_{\\text{fixed}}$.\n    b. Compute gradient $\\nabla f(x_k) = A^\\top(Ax_k - y)$.\n    c. Compute $f(x_k) = \\frac{1}{2}\\|Ax_k - y\\|_2^2$.\n    d. Enter line search loop:\n        i. Compute candidate $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n        ii. Compute $f(x_{k+1}) = \\frac{1}{2}\\|Ax_{k+1} - y\\|_2^2$.\n        iii. Construct the right-hand side of the acceptance condition: `RHS` $= f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2$.\n        iv. If $f(x_{k+1}) \\le \\text{RHS} + \\varepsilon$, accept $x_{k+1}$ and exit the line search loop.\n        v. Else, shrink step size $t \\leftarrow \\beta t$, increment `total_backtracks`, and repeat from step (i).\n    e. Update $x_k \\leftarrow x_{k+1}$.\n3. After $T$ iterations, compute the final objective value $F(x_T) = f(x_T) + g(x_T)$.\n4. Return $F(x_T)$ and `total_backtracks`.\n\nThis procedure is implemented for each of the three test cases specified. The final objective values are rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the proximal gradient solver.\n    \"\"\"\n\n    TestCase = namedtuple('TestCase', ['A', 'y', 'lambda_', 't_fixed', 'beta', 'T'])\n\n    test_cases = [\n        # Test case 1\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=0.05,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 2\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=10.0,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 3\n        TestCase(\n            A=np.array([[1, 2, 0, 0], [0, 1, 1, 0], [0, 0, 1, 3]]),\n            y=np.array([0, 1, -1]),\n            lambda_=0.05,\n            t_fixed=5.0,\n            beta=0.5,\n            T=120\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        final_F, total_backtracks = proximal_gradient_solver(\n            A=case.A,\n            y=case.y,\n            lambda_=case.lambda_,\n            t_fixed=case.t_fixed,\n            beta=case.beta,\n            T=case.T\n        )\n        results.append(f\"{final_F:.6f}\")\n        results.append(str(total_backtracks))\n\n    print(f\"[{','.join(results)}]\")\n\ndef soft_threshold(z, alpha):\n    \"\"\"\n    Soft-thresholding operator for the L1 norm's proximal operator.\n    S_alpha(z) = sign(z) * max(|z| - alpha, 0)\n    \"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\ndef f(x, A, y):\n    \"\"\"\n    Computes the smooth part of the objective function: f(x) = 1/2 * ||Ax - y||_2^2\n    \"\"\"\n    residual = A @ x - y\n    return 0.5 * np.dot(residual, residual)\n\ndef g(x, lambda_):\n    \"\"\"\n    Computes the non-smooth part of the objective function: g(x) = lambda * ||x||_1\n    \"\"\"\n    return lambda_ * np.linalg.norm(x, 1)\n\ndef F(x, A, y, lambda_):\n    \"\"\"\n    Computes the total objective function F(x) = f(x) + g(x).\n    \"\"\"\n    return f(x, A, y) + g(x, lambda_)\n\ndef proximal_gradient_solver(A, y, lambda_, t_fixed, beta, T):\n    \"\"\"\n    Implements the Proximal Gradient Method with a hybrid fixed-step and backtracking\n    line search strategy.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    total_backtracks = 0\n    epsilon = 1e-12\n\n    for _ in range(T):\n        t = t_fixed\n        \n        # Pre-compute gradient and f(x) for the current iterate x\n        residual = A @ x - y\n        grad_f_x = A.T @ residual\n        f_x = 0.5 * np.dot(residual, residual)\n\n        while True:\n            # Candidate update step using the proximal operator\n            z = x - t * grad_f_x\n            x_next = soft_threshold(z, lambda_ * t)\n\n            # Check the sufficient decrease condition (backtracking line search)\n            # f(x_next) <= f(x) + grad_f(x)^T(x_next - x) + (1/(2t))||x_next - x||^2\n            f_x_next = f(x_next, A, y)\n            \n            # The right-hand side of the inequality\n            diff_x = x_next - x\n            rhs = f_x + np.dot(grad_f_x, diff_x) + (0.5 / t) * np.dot(diff_x, diff_x)\n\n            if f_x_next <= rhs + epsilon:\n                x = x_next\n                break  # Step size t is accepted\n            else:\n                # Shrink step size and try again\n                t = beta * t\n                total_backtracks += 1\n    \n    final_F_val = F(x, A, y, lambda_)\n    return final_F_val, total_backtracks\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Moving beyond basic implementation, this practice delves into the theoretical underpinnings of the backtracking acceptance condition. By deriving and implementing a specialized diagnostic, you will uncover the quantitative relationship between step-size acceptance and the local curvature of the objective function. This exercise will help you develop a deeper intuition for why line searches succeed or fail, particularly as the algorithm explores different sparse supports. ",
            "id": "3432788",
            "problem": "You are asked to design and analyze a proximal gradient method with backtracking line search for a convex composite objective encountered in compressed sensing and sparse optimization. The smooth data-fitting component is a quadratic, and the regularizer is the sparsity-promoting absolute-value norm. Your goals are to reason from first principles about why and when the line search accepts its first attempted step size (that is, when the per-iteration initial Lipschitz estimate remains unchanged), to connect that acceptance to curvature along the discovered sparse structure, and to propose an implementable diagnostic that predicts when first-attempt step acceptance will resume after a support change.\n\nConsider the unconstrained problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\nwhere the smooth term is the quadratic\n$$\nf(x) \\equiv \\frac{1}{2} \\lVert A x - b \\rVert_2^2,\n$$\nwith given data matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$, and where $\\lambda > 0$ is a regularization parameter. The gradient of the smooth term is $\\nabla f(x) = A^\\top(Ax - b)$, and the gradient is globally Lipschitz with Lipschitz constant equal to the squared spectral norm of $A$. The proximity operator of $\\lambda \\lVert x \\rVert_1$ is the coordinatewise soft-thresholding operator.\n\nYou will use a proximal gradient method with backtracking line search in the following form. At iteration $k$ with current iterate $x_k$ and current Lipschitz estimate $L_k > 0$, one attempts the candidate update using the step size $1 / L_k$,\n$$\ny_k \\equiv \\operatorname{soft}\\Big(x_k - \\frac{1}{L_k} \\nabla f(x_k), \\, \\frac{\\lambda}{L_k}\\Big),\n$$\nand accepts this first attempt if and only if the standard sufficient-decrease inequality for the smooth part holds with the same $L_k$ in the quadratic upper model used by backtracking. Otherwise one increases the Lipschitz estimate (e.g., multiplies by a factor greater than $1$) and retries until the inequality holds. A “unit-step acceptance” at iteration $k$ means that the first attempt with the initial $L_k$ is accepted (so $L_k$ remains unchanged for that iteration).\n\nStarting from the core definition of the sufficient-decrease inequality for the smooth part used in proximal gradient backtracking, and from the explicit quadratic form of $f(x)$, derive a necessary-and-sufficient acceptance criterion specialized to this quadratic model that depends only on the attempted update direction. Explain how this criterion quantitatively links accepted unit steps to the curvature encountered along the direction determined by the proximal-gradient update and how this direction is constrained by the discovered sparse support.\n\nUsing this principle, propose a scalar, iteration-wise diagnostic that can be computed solely from the data $(A,b)$, the current iterate $x_k$, the current gradient $\\nabla f(x_k)$, and the first-attempt update $y_k$ produced with the initial $L_k$ (before any backtracking increases are applied). The diagnostic must take the form of a real number that compares against $L_k$ to predict, at iteration $k$, whether a unit step will be accepted at iteration $k$; moreover, after any support change detected at iteration $t$ (that is, the support set of $x_{t+1}$ differs from the support set of $x_t$), your diagnostic must be used to predict the first subsequent iteration index $r > t$ at which unit steps resume. Formally, define a “support change” event at iteration $t$ as\n$$\n\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t),\n$$\nwhere $\\operatorname{supp}(z) \\equiv \\{i : z_i \\neq 0\\}$. Define the “resumption index” $r$ for this event to be the smallest $j > t$ such that the line search accepts its first attempt at iteration $j$. Your diagnostic must predict this resumption index using only the diagnostic values computed at each iteration and the initial per-iteration $L_k$.\n\nYou must implement a program that for a set of specified test cases does the following:\n- Generates a synthetic compressed-sensing instance $(A,b)$ as described below.\n- Runs proximal gradient with backtracking for a fixed number of iterations.\n- Records, at each iteration, whether the first attempt was accepted (unit-step acceptance) and whether the support changed relative to the previous iterate.\n- Computes your proposed diagnostic at each iteration using only the first-attempt update.\n- For every support-change event, computes the true resumption index (based on actual unit-step acceptance outcomes) and the predicted resumption index (based solely on the per-iteration diagnostic and the initial $L_k$ at those iterations).\n- For each test case, outputs a single integer equal to the count of mismatches between the true and predicted resumption indices across all support-change events observed within the fixed iteration budget. If there are no support changes in a test case, the count should be $0$.\n\nFoundational starting points you are allowed to assume and explicitly use in your derivations are:\n- The composite structure $F(x) = f(x) + \\lambda \\lVert x \\rVert_1$, the proximity operator for $\\lambda \\lVert x \\rVert_1$, and the definition of the proximal-gradient iteration.\n- The standard sufficient-decrease inequality used in backtracking line search for the proximal-gradient method, based on the Lipschitz continuity of $\\nabla f$.\n- For quadratic $f(x)$, the exact difference identity $f(x + h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2$.\n\nTest suite. Use the following test cases, each fully specified by integers $(m,n)$, sparsity level for the ground-truth vector $k_{\\text{true}}$, iteration budget $K$, initial Lipschitz estimate $L_0$, backtracking multiplier $\\eta > 1$, and random seed. In all cases, initialize $x_0 = 0$, and generate a ground-truth sparse vector $x^\\star$ by choosing $k_{\\text{true}}$ indices uniformly at random and setting those entries to independent values uniformly distributed in $[1,2]$ with random signs. Set $b = A x^\\star$ for noise-free data. For the first two cases, use a fixed regularization parameter $\\lambda$; for the third, set $\\lambda$ adaptively as a known multiple of a computable quantity to induce the boundary case of an all-zero solution. For all cases, the maximum number of iterations is fixed and you must report the mismatch count as described above. The test cases are:\n\n- Test case 1 (well-conditioned Gaussian design):\n  - $m = 40$, $n = 80$, $k_{\\text{true}} = 5$, $K = 120$, $L_0 = 0.3$, $\\eta = 2.0$, seed $= 1$, $\\lambda = 0.05$.\n  - Generate $A$ with independent standard normal entries and then normalize each column to unit Euclidean norm.\n\n- Test case 2 (strongly correlated design):\n  - $m = 40$, $n = 60$, $k_{\\text{true}} = 8$, $K = 150$, $L_0 = 0.2$, $\\eta = 2.0$, seed $= 7$, $\\lambda = 0.03$.\n  - Generate a Toeplitz covariance with correlation parameter $\\rho = 0.9$ via $\\Sigma_{ij} = \\rho^{\\lvert i - j \\rvert}$ for $1 \\le i,j \\le n$. Draw a raw matrix $Z \\in \\mathbb{R}^{m \\times n}$ with independent entries distributed as $\\mathcal{N}(0,1)$, form $A_{\\text{raw}} = Z \\, C$ where $C$ is any matrix square root of $\\Sigma$ (for example, a Cholesky factor), and then normalize each column of $A_{\\text{raw}}$ to unit Euclidean norm to obtain $A$.\n\n- Test case 3 (boundary case with zero solution):\n  - $m = 30$, $n = 50$, $k_{\\text{true}} = 5$, $K = 60$, $L_0 = 0.5$, $\\eta = 2.0$, seed $= 11$, and set $\\lambda = 1.05 \\, \\lVert A^\\top b \\rVert_\\infty$.\n  - Generate $A$ as in Test case 1 (Gaussian with unit-norm columns), and construct $b = A x^\\star$. Then compute $\\lambda$ as specified.\n\nAngle units are not applicable. There are no physical units in this problem.\n\nFinal program output format. Your program should produce a single line of output containing a comma-separated list of the mismatch counts, one per test case, enclosed in square brackets. For example:\n`[c1,c2,c3]`\nwhere $c1$, $c2$, and $c3$ are integers equal to the mismatch counts for Test cases 1, 2, and 3, respectively. No other output should be produced.",
            "solution": "The problem asks for an analysis of the backtracking line search condition for a proximal gradient method applied to the LASSO objective function, the derivation of a diagnostic for step-size acceptance, and an implementation to verify the diagnostic's predictive power.\n\nThe optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ is the smooth component and $\\lambda \\lVert x \\rVert_1$ is the non-smooth regularizer. The proximal gradient method generates a sequence of iterates $x_k$ via the update rule\n$$\nx_{k+1} = \\operatorname{prox}_{\\alpha_k \\lambda \\lVert \\cdot \\rVert_1}(x_k - \\alpha_k \\nabla f(x_k)),\n$$\nwhere $\\alpha_k > 0$ is the step size. The proximity operator for the $\\ell_1$-norm is the soft-thresholding operator, $\\operatorname{soft}(z, \\tau)_i = \\operatorname{sgn}(z_i) \\max(\\lvert z_i \\rvert - \\tau, 0)$. The step size $\\alpha_k$ is determined by a backtracking line search.\n\nAt iteration $k$, given the current iterate $x_k$, we start with an initial guess for the local Lipschitz constant of $\\nabla f$, denoted $L_{k, \\text{init}}$. We then attempt a step size $\\alpha_k = 1/L$ with $L=L_{k, \\text{init}}$. The candidate update is\n$$\ny_k = \\operatorname{soft}\\Big(x_k - \\frac{1}{L} \\nabla f(x_k), \\, \\frac{\\lambda}{L}\\Big).\n$$\nThis candidate is accepted if it satisfies the sufficient decrease condition for the smooth part $f$. This condition ensures that the quadratic model, which majorizes $f$ at $x_k$, is a valid upper bound for $f$ at the new point $y_k$:\n$$\nf(y_k) \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\nIf this inequality holds for $L = L_{k, \\text{init}}$, the step is a \"unit step\" (or first-attempt acceptance). The new iterate becomes $x_{k+1} = y_k$, and the line search for this iteration terminates. If the inequality fails, we increase $L$ (e.g., $L \\leftarrow \\eta L$ for some $\\eta > 1$) and repeat the process of computing a new candidate $y_k$ and checking the inequality, until it is satisfied.\n\n**Derivation of the Acceptance Criterion**\n\nThe core of the analysis lies in specializing this inequality to our specific quadratic function $f(x)$. The problem provides the following exact identity for a quadratic function:\n$$\nf(x+h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2.\n$$\nLet's apply this identity by setting $x = x_k$ and the displacement $h = y_k - x_k$. The left-hand side of the sufficient decrease inequality, $f(y_k)$, can be rewritten as:\n$$\nf(y_k) = f(x_k + (y_k - x_k)) = f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2.\n$$\nSubstituting this expression into the sufficient decrease inequality gives:\n$$\nf(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2 \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\nAfter canceling the common terms $f(x_k)$ and $\\nabla f(x_k)^\\top (y_k - x_k)$ from both sides and multiplying by $2$, we obtain a simplified, equivalent inequality:\n$$\n\\lVert A(y_k - x_k) \\rVert_2^2 \\le L \\lVert y_k - x_k \\rVert_2^2.\n$$\nThis is a necessary and sufficient condition for the line search to accept the step size $\\alpha_k = 1/L$. Let $d_k = y_k - x_k$ be the update vector. If $d_k = 0$, the inequality holds trivially as $0 \\le 0$. If $d_k \\neq 0$, the condition can be expressed as:\n$$\n\\frac{\\lVert A d_k \\rVert_2^2}{\\lVert d_k \\rVert_2^2} \\le L.\n$$\nThis demonstrates that the line search accepts the step if and only if the Rayleigh quotient of the matrix $A^\\top A$ (the Hessian of $f$) with respect to the update vector $d_k$ is no greater than the Lipschitz estimate $L$.\n\n**Connection to Curvature and Sparse Support**\n\nThe term $\\lVert A d_k \\rVert_2^2$ can be written as $(A d_k)^\\top (A d_k) = d_k^\\top A^\\top A d_k$. Since the Hessian is $\\nabla^2 f(x) = A^\\top A$, this term represents the second-order change, or curvature, of $f$ along the direction $d_k$. The acceptance condition thus states that the effective curvature along the chosen update direction must be bounded by $L$.\n\nThe update vector $d_k = y_k - x_k$ is determined by the proximal gradient step. The soft-thresholding operation ensures that $y_k$ is sparse. Consequently, $d_k$ is also sparse, and its non-zero entries are typically confined to the support of $y_k$. When the algorithm's active set (the support of the iterates) stabilizes, the vector $d_k$ is restricted to a particular subspace. The backtracking line search adapts $L$ to be an upper bound on the curvature within this subspace. A change in the support of $x_k$ means $d_k$ probes a new direction or subspace, potentially one with higher curvature (i.e., a larger Rayleigh quotient). If this new curvature exceeds the current estimate $L$, the line search will fail, forcing an increase in $L$ until it appropriately upper-bounds the curvature in the new active subspace.\n\n**Proposed Diagnostic and Prediction**\n\nBased on the derived criterion, we can define a scalar diagnostic. At iteration $k$, let $L_{k, \\text{init}}$ be the initial Lipschitz estimate used for the first attempt. The first-attempt candidate update is $y_{k, \\text{init}} = \\operatorname{soft}(x_k - \\frac{1}{L_{k, \\text{init}}} \\nabla f(x_k), \\frac{\\lambda}{L_{k, \\text{init}}})$. Let $d_{k, \\text{init}} = y_{k, \\text{init}} - x_k$. The diagnostic is the Rayleigh quotient:\n$$\nD_k = \\begin{cases} \\frac{\\lVert A d_{k, \\text{init}} \\rVert_2^2}{\\lVert d_{k, \\text{init}} \\rVert_2^2} & \\text{if } d_{k, \\text{init}} \\neq 0 \\\\ 0 & \\text{if } d_{k, \\text{init}} = 0 \\end{cases}\n$$\nA \"unit step\" is accepted at iteration $k$ if and only if $D_k \\le L_{k, \\text{init}}$. This is not a heuristic prediction but a direct consequence of the derived necessary and sufficient condition.\n\nA \"support change\" event at iteration $t$ is $\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t)$. The true resumption index is $r = \\min \\{j > t \\mid \\text{the first attempt at step } j \\text{ is accepted}\\}$. Our predicted resumption index, $\\hat{r}$, is found by checking our diagnostic condition at each subsequent iteration: $\\hat{r} = \\min \\{j > t \\mid D_j \\le L_{j, \\text{init}}\\}$.\n\nSince the condition $D_j \\le L_{j, \\text{init}}$ is precisely the condition for first-attempt acceptance, the predicted resumption index $\\hat{r}$ must be identical to the true resumption index $r$ in every case, provided the computations are exact. Therefore, the number of mismatches between the true and predicted indices is expected to be $0$. The implementation serves as a verification of this analytical conclusion.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, toeplitz\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef run_one_case(m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params=None):\n    \"\"\"\n    Runs a single test case for the proximal gradient algorithm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # --- 1. Generate synthetic data ---\n    # Generate ground-truth sparse vector x_star\n    x_star = np.zeros(n)\n    support_star_indices = rng.choice(n, k_true, replace=False)\n    x_star[support_star_indices] = rng.uniform(1, 2, k_true)\n    x_star[support_star_indices] *= rng.choice([-1, 1], k_true)\n\n    # Generate matrix A\n    if A_gen_mode == 'gaussian':\n        A = rng.standard_normal(size=(m, n))\n        col_norms = np.linalg.norm(A, axis=0)\n        A = A / col_norms\n    elif A_gen_mode == 'correlated':\n        rho = 0.9\n        Sigma = toeplitz(rho**np.arange(n))\n        # Add a small identity matrix for stability before Cholesky\n        C = cholesky(Sigma + 1e-10 * np.eye(n), lower=True)\n        Z = rng.standard_normal(size=(m, n))\n        A_raw = Z @ C.T\n        col_norms = np.linalg.norm(A_raw, axis=0)\n        A = A_raw / col_norms\n\n    # Generate measurement vector b\n    b = A @ x_star\n\n    # Set lambda if adaptive\n    if lambda_mode_params == 'adaptive':\n        lambda_val = 1.05 * np.max(np.abs(A.T @ b))\n\n    # --- 2. Run Proximal Gradient with Backtracking ---\n    x_k = np.zeros(n)\n    L_val = L0\n    \n    is_unit_step_accepted = []\n    support_changed = []\n    diagnostics = []\n    L_inits = []\n\n    for k in range(K):\n        # Store support of x_k for later comparison\n        support_k = set(np.where(x_k != 0)[0])\n        \n        # --- At iteration k, perform one step ---\n        grad_fxk = A.T @ (A @ x_k - b)\n        \n        # This is the initial Lipschitz estimate for this iteration\n        L_k_init = L_val\n        L_inits.append(L_k_init)\n\n        # Compute first-attempt update and diagnostic\n        y_k_init = soft_threshold(x_k - (1/L_k_init) * grad_fxk, lambda_val / L_k_init)\n        d_k_init = y_k_init - x_k\n        \n        if np.linalg.norm(d_k_init) < 1e-12:\n            D_k = 0.0\n        else:\n            Adk_norm_sq = np.linalg.norm(A @ d_k_init)**2\n            dk_norm_sq = np.linalg.norm(d_k_init)**2\n            D_k = Adk_norm_sq / dk_norm_sq\n        diagnostics.append(D_k)\n\n        # Backtracking line search\n        L_attempt = L_k_init\n        num_backtracks = 0\n        while True:\n            y_k = soft_threshold(x_k - (1/L_attempt) * grad_fxk, lambda_val / L_attempt)\n            d_k = y_k - x_k\n\n            if np.linalg.norm(d_k) < 1e-12:\n                # If step is zero, condition is trivially satisfied.\n                break \n\n            lhs = np.linalg.norm(A @ d_k)**2\n            rhs = L_attempt * np.linalg.norm(d_k)**2\n            \n            if lhs <= rhs:\n                break\n            \n            L_attempt *= eta\n            num_backtracks += 1\n\n        # Accept the step\n        is_unit_step_accepted.append(num_backtracks == 0)\n        x_k = y_k  # Update x_k to x_{k+1}\n        L_val = L_attempt # Update L for next iteration\n\n        # Check for support change\n        support_k_plus_1 = set(np.where(x_k != 0)[0])\n        support_changed.append(support_k != support_k_plus_1)\n\n    # --- 3. Post-process to count mismatches ---\n    mismatch_count = 0\n    support_change_indices = [t for t, changed in enumerate(support_changed) if changed]\n\n    for t in support_change_indices:\n        r_true = -1\n        for j in range(t + 1, K):\n            if is_unit_step_accepted[j]:\n                r_true = j\n                break\n        \n        r_pred = -1\n        for j in range(t + 1, K):\n            # The prediction check: D_j <= L_{j,init}\n            if diagnostics[j] <= L_inits[j]:\n                r_pred = j\n                break\n        \n        if r_true != r_pred:\n            mismatch_count += 1\n            \n    return mismatch_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params)\n        (40, 80, 5, 120, 0.3, 2.0, 1, 0.05, 'gaussian', None),\n        (40, 60, 8, 150, 0.2, 2.0, 7, 0.03, 'correlated', None),\n        (30, 50, 5, 60, 0.5, 2.0, 11, None, 'gaussian', 'adaptive')\n    ]\n\n    results = []\n    for params in test_cases:\n        mismatch_count = run_one_case(*params)\n        results.append(mismatch_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "This final practice explores a subtle but critical issue that arises in advanced optimization: the interaction between the backtracking line search and inexact computations. You will investigate a scenario where the proximal operator is not computed exactly, a common situation in problems with complex structured sparsity. This exercise reveals the \"masking\" phenomenon, where the standard acceptance test can be satisfied even for a step that is invalid in a stricter sense, offering crucial insights into the algorithm's robustness. ",
            "id": "3432750",
            "problem": "Consider the composite convex optimization problem in compressed sensing and sparse optimization: minimize the objective $F(x) = f(x) + g(x)$, where the data fidelity term is $f(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2$ with $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and the structured sparsity regularizer is the group norm $g(x) = \\lambda \\sum_{G \\in \\mathcal{G}} \\lVert x_G\\rVert_2$, with $\\mathcal{G}$ a partition of the coordinates into groups. Focus on a single-group case $\\mathcal{G} = \\{\\{1,\\dots,n\\}\\}$ so that $g(x) = \\lambda \\lVert x\\rVert_2$. For a given current iterate $x \\in \\mathbb{R}^n$, gradient $\\nabla f(x) = A^\\top(Ax - b)$, step size $t > 0$, and the standard proximal gradient candidate $z = x - t \\nabla f(x)$, the exact proximal update is defined by the block soft-thresholding proximal operator $y_{\\mathrm{exact}} = \\mathrm{prox}_{t g}(z)$, which solves $\\min_{y} \\left\\{ \\tfrac{1}{2t}\\lVert y - z\\rVert_2^2 + g(y) \\right\\}$. An inexact proximal computation can arise from early stopping in an inner iterative method. One principled early-stopping surrogate uses a single forward-backward iteration on the proximal subproblem with an inner step parameter $\\gamma \\in (0,1]$:\n- Form an intermediate point $w = (1 - \\gamma) x + \\gamma z$.\n- Compute $y_{\\mathrm{inexact}} = \\mathrm{prox}_{\\gamma t g}(w)$.\n\nBacktracking line search with an Armijo-type sufficient decrease test for composite objectives accepts a candidate $y$ at step size $t$ if and only if\n$$\nF(y) \\le f(x) + \\nabla f(x)^\\top (y - x) + \\tfrac{1}{2t}\\lVert y - x\\rVert_2^2 + g(y).\n$$\nNote that for any $y$, this inequality is equivalent to the smoothness inequality on $f$:\n$$\nf(y) \\le f(x) + \\nabla f(x)^\\top (y - x) + \\tfrac{1}{2t}\\lVert y - x\\rVert_2^2,\n$$\nwhich holds for all $y$ whenever $1/t$ exceeds a Lipschitz constant of $\\nabla f$.\n\nYour task is to write a complete program that, for each test case below, performs the following:\n1. Constructs a small-dimensional instance with a diagonal sensing matrix $A$ having two singular values that induce anisotropic curvature.\n2. Uses a single group $\\mathcal{G}=\\{\\{1,2\\}\\}$ so that $g(x) = \\lambda \\lVert x\\rVert_2$.\n3. Sets $b$ so that the gradient at the provided $x$ equals a prescribed vector $g^\\star$, by solving $A^\\top(Ax - b) = g^\\star$ exactly.\n4. Runs backtracking line search with geometric reduction factor $\\eta \\in (0,1)$ starting from $t_0 > 0$ until the Armijo test is satisfied, separately for:\n   - The exact proximal update $y_{\\mathrm{exact}}(t)$.\n   - The inexact, early-stopped update $y_{\\mathrm{inexact}}(t; \\gamma)$.\n5. Declares that “masking” occurs if the Armijo test is satisfied by the inexact update at its accepted step size $t_{\\mathrm{inexact}}$, but is not satisfied by the exact update evaluated at the same step size $t_{\\mathrm{inexact}}$. Formally, masking is the boolean event\n$$\n\\text{masking} = \\left[\\, F\\big(y_{\\mathrm{inexact}}(t_{\\mathrm{inexact}})\\big) \\le f(x) + \\nabla f(x)^\\top \\big(y_{\\mathrm{inexact}}(t_{\\mathrm{inexact}}) - x\\big) + \\tfrac{1}{2t_{\\mathrm{inexact}}}\\lVert y_{\\mathrm{inexact}}(t_{\\mathrm{inexact}}) - x\\rVert_2^2 + g\\big(y_{\\mathrm{inexact}}(t_{\\mathrm{inexact}})\\big)\\, \\right]\n$$\nand simultaneously\n$$\n\\left[\\, F\\big(y_{\\mathrm{exact}}(t_{\\mathrm{inexact}})\\big) > f(x) + \\nabla f(x)^\\top \\big(y_{\\mathrm{exact}}(t_{\\mathrm{inexact}}) - x\\big) + \\tfrac{1}{2t_{\\mathrm{inexact}}}\\lVert y_{\\mathrm{exact}}(t_{\\mathrm{inexact}}) - x\\rVert_2^2 + g\\big(y_{\\mathrm{exact}}(t_{\\mathrm{inexact}})\\big)\\, \\right].\n$$\n\nImplement the block soft-thresholding proximal operator for a single group: for any $v \\in \\mathbb{R}^2$ and threshold $\\tau \\ge 0$, define\n$$\n\\mathrm{prox}_{\\tau \\lVert\\cdot\\rVert_2}(v) = \\begin{cases}\n\\left(1 - \\dfrac{\\tau}{\\lVert v\\rVert_2}\\right) v, & \\text{if } \\lVert v\\rVert_2 > \\tau,\\\\[1em]\n0, & \\text{otherwise.}\n\\end{cases}\n$$\n\nUse the following fixed test suite of four cases. In each case, $A = \\mathrm{diag}(a_1,a_2)$, $n=m=2$, and the group is $\\{1,2\\}$:\n- Case $1$ (anisotropic, designed to exhibit masking):\n  - $a_1 = 10$, $a_2 = 1$.\n  - $x = [0,\\, 1.5]^\\top$.\n  - Desired gradient at $x$: $g^\\star = [10,\\, 0]^\\top$.\n  - Regularization: $\\lambda = 0.1$.\n  - Backtracking parameters: $t_0 = 0.9$, $\\eta = 0.5$.\n  - Early-stopping parameter: $\\gamma = 0.05$.\n  - Construct $b$ by solving $A^\\top(Ax - b) = g^\\star$, i.e., $b = A x - A^{-\\top} g^\\star$.\n- Case $2$ (happy path with small step size):\n  - $a_1 = 10$, $a_2 = 1$.\n  - $x = [0.2,\\, -0.3]^\\top$.\n  - $g^\\star = [3,\\, 0]^\\top$.\n  - $\\lambda = 0.1$.\n  - $t_0 = 0.005$, $\\eta = 0.5$.\n  - $\\gamma = 0.3$.\n  - $b = A x - A^{-\\top} g^\\star$.\n- Case $3$ (boundary with zero iterate):\n  - $a_1 = 10$, $a_2 = 1$.\n  - $x = [0,\\, 0]^\\top$.\n  - $g^\\star = [2,\\, 0]^\\top$.\n  - $\\lambda = 0.1$.\n  - $t_0 = 0.5$, $\\eta = 0.5$.\n  - $\\gamma = 0.5$.\n  - $b = A x - A^{-\\top} g^\\star$.\n- Case $4$ (highly ill-conditioned curvature):\n  - $a_1 = 20$, $a_2 = 1$.\n  - $x = [0.5,\\, 2.0]^\\top$.\n  - $g^\\star = [5,\\, 0]^\\top$.\n  - $\\lambda = 0.2$.\n  - $t_0 = 0.05$, $\\eta = 0.5$.\n  - $\\gamma = 0.1$.\n  - $b = A x - A^{-\\top} g^\\star$.\n\nRequirements for the program:\n- For each case, compute the accepted step size under exact proximal backtracking, $t_{\\mathrm{exact}}$, and under inexact proximal backtracking, $t_{\\mathrm{inexact}}$.\n- Determine the masking boolean as defined above.\n- The final output must aggregate the results for all four cases into a single line, formatted as a list of lists:\n  - For each case $i \\in \\{1,2,3,4\\}$, output the triple $[\\text{masking}_i, t_{\\mathrm{exact},i}, t_{\\mathrm{inexact},i}]$.\n  - The complete output should be a single line containing the list `[[...],[...],[...],[...]]` with no spaces.\n- All floating-point outputs must be printed in standard decimal notation (not scientific notation), and the booleans must be standard literals.\n\nNo physical units are involved. All angles, if any, are irrelevant here. Percentages are not used.\n\nYour program should produce exactly one line of output containing the results as a comma-separated list enclosed in square brackets (for example, `[result1,result2,result3]`), where here each $result$ is itself a three-element list $[\\text{masking}, t_{\\mathrm{exact}}, t_{\\mathrm{inexact}}]$.",
            "solution": "The user has provided a well-defined computational problem in the domain of convex optimization and requests a numerical implementation. The task is to analyze the behavior of backtracking line search for a proximal gradient method under both an exact and an inexact proximal update. The phenomenon of \"masking,\" where an inexact update is accepted by the line search while the corresponding exact update is rejected, is to be investigated for a set of specific test cases.\n\n### **Problem Validation**\n\n**Step 1: Extracted Givens**\n- **Objective function**: $F(x) = f(x) + g(x)$, where $f(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2$ and $g(x) = \\lambda \\lVert x\\rVert_2$.\n- **Problem dimensions**: $A \\in \\mathbb{R}^{2 \\times 2}$ is a diagonal matrix, $x, b \\in \\mathbb{R}^2$.\n- **Gradient**: $\\nabla f(x) = A^\\top(Ax - b)$.\n- **Vector $b$ construction**: For a given current point $x$ and a desired gradient $g^\\star$, $b$ is set as $b = A x - A^{-\\top} g^\\star$, which ensures $\\nabla f(x) = g^\\star$.\n- **Exact proximal update**: $y_{\\mathrm{exact}}(t) = \\mathrm{prox}_{t g}(z)$, where $z = x - t \\nabla f(x)$.\n- **Inexact proximal update**: $y_{\\mathrm{inexact}}(t; \\gamma) = \\mathrm{prox}_{\\gamma t g}(w)$, where $w = (1 - \\gamma) x + \\gamma z$.\n- **Proximal operator**: For a single group, $\\mathrm{prox}_{\\tau \\lVert\\cdot\\rVert_2}(v) = \\max(0, 1 - \\tau/\\lVert v\\rVert_2) v$.\n- **Sufficient decrease (Armijo) condition**: $f(y) \\le f(x) + \\nabla f(x)^\\top (y - x) + \\tfrac{1}{2t}\\lVert y - x\\rVert_2^2$.\n- **Backtracking line search**: Start with $t = t_0$ and multiply by $\\eta \\in (0,1)$ until the Armijo condition is met.\n- **Masking definition**: The boolean event where, at the accepted step size $t_{\\mathrm{inexact}}$ for the inexact update, the Armijo condition holds for $y_{\\mathrm{inexact}}(t_{\\mathrm{inexact}})$ but fails for $y_{\\mathrm{exact}}(t_{\\mathrm{inexact}})$.\n- **Test cases**: Four specific instances are defined with parameters for $A$, $x$, $g^\\star$, $\\lambda$, $t_0$, $\\eta$, and $\\gamma$.\n\n**Step 2: Validation of Givens**\n- **Scientific Grounding**: The problem is firmly rooted in the theory of numerical optimization for sparse problems. The functions, operators ($f(x)$, $g(x)$, $\\mathrm{prox}$), and algorithms (proximal gradient, backtracking line search) are standard in this field. All mathematical formulations are correct and consistent with established literature. The setup is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. For each test case, all parameters are specified, leading to a deterministic computational outcome. The backtracking line search is guaranteed to terminate because $\\nabla f$ is Lipschitz continuous (as $f$ is quadratic), ensuring the Armijo condition will be satisfied for a sufficiently small positive step size $t$.\n- **Objectivity**: The problem is stated using precise, unambiguous mathematical terminology. The evaluation criterion (\"masking\") is formally defined.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically grounded, and objective computational task. We shall proceed with constructing the solution.\n\n### **Methodology and Implementation**\n\nFor each test case, we will implement the following procedure:\n\n1.  **Initialization**: Construct the matrix $A = \\mathrm{diag}(a_1, a_2)$ and vectors $x$ and $g^\\star$. Calculate the vector $b$ using the formula $b = Ax - A^{-\\top}g^\\star$. Since $A$ is diagonal, $A^{-\\top} = \\mathrm{diag}(1/a_1, 1/a_2)$. The gradient at $x$ is fixed as $\\nabla f(x) = g^\\star$.\n\n2.  **Define Core Functions**:\n    -   $f(y) = \\tfrac{1}{2}\\lVert A y - b\\rVert_2^2$: The smooth part of the objective.\n    -   $\\mathrm{prox}_{\\tau\\lVert\\cdot\\rVert_2}(v)$: The block soft-thresholding operator. This will be implemented as a function taking a vector $v$ and a threshold $\\tau$.\n    -   An `ArmijoTest(y, t)` function that evaluates the sufficient decrease condition $f(y) \\le f(x) + \\nabla f(x)^\\top (y - x) + \\tfrac{1}{2t}\\lVert y - x\\rVert_2^2$. The values $f(x)$ and $\\nabla f(x)$ are pre-computed for efficiency.\n\n3.  **Backtracking for Exact Update**:\n    -   Initialize step size $t = t_0$.\n    -   In a loop, compute the candidate point $y_{\\mathrm{exact}}(t) = \\mathrm{prox}_{t\\lambda\\lVert\\cdot\\rVert_2}(x - t\\nabla f(x))$.\n    -   If `ArmijoTest(y_exact, t)` passes, store $t$ as $t_{\\mathrm{exact}}$ and exit the loop.\n    -   Otherwise, update $t \\leftarrow t \\cdot \\eta$ and continue.\n\n4.  **Backtracking for Inexact Update**:\n    -   Initialize step size $t = t_0$.\n    -   In a loop, compute the intermediate point $w = (1 - \\gamma)x + \\gamma(x - t\\nabla f(x))$ and the candidate $y_{\\mathrm{inexact}}(t) = \\mathrm{prox}_{\\gamma t\\lambda\\lVert\\cdot\\rVert_2}(w)$.\n    -   If `ArmijoTest(y_inexact, t)` passes, store $t$ as $t_{\\mathrm{inexact}}$ and exit the loop.\n    -   Otherwise, update $t \\leftarrow t \\cdot \\eta$ and continue.\n\n5.  **Check for Masking**:\n    -   By definition, the masking phenomenon occurs if the Armijo test for the exact update *fails* at the step size accepted by the inexact update.\n    -   Compute the exact candidate at $t_{\\mathrm{inexact}}$: $y_{\\mathrm{exact}}' = \\mathrm{prox}_{t_{\\mathrm{inexact}}\\lambda\\lVert\\cdot\\rVert_2}(x - t_{\\mathrm{inexact}}\\nabla f(x))$.\n    -   The masking a-priori is `True` if `ArmijoTest(y_exact', t_inexact)` is `False`.\n\n6.  **Aggregation and Formatting**:\n    -   For each test case, the results $[\\text{masking}, t_{\\mathrm{exact}}, t_{\\mathrm{inexact}}]$ will be collected.\n    -   The final output will be a string representing a list of these results, with no spaces and with floating-point numbers formatted to a fixed number of decimal places to prevent scientific notation, as per the problem requirements.\n\nThis structured procedure will be implemented in a Python program to solve the given problem for all four cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A_diag\": [10.0, 1.0],\n            \"x\": [0.0, 1.5],\n            \"g_star\": [10.0, 0.0],\n            \"lambda_reg\": 0.1,\n            \"t0\": 0.9,\n            \"eta\": 0.5,\n            \"gamma\": 0.05,\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A_diag\": [10.0, 1.0],\n            \"x\": [0.2, -0.3],\n            \"g_star\": [3.0, 0.0],\n            \"lambda_reg\": 0.1,\n            \"t0\": 0.005,\n            \"eta\": 0.5,\n            \"gamma\": 0.3,\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A_diag\": [10.0, 1.0],\n            \"x\": [0.0, 0.0],\n            \"g_star\": [2.0, 0.0],\n            \"lambda_reg\": 0.1,\n            \"t0\": 0.5,\n            \"eta\": 0.5,\n            \"gamma\": 0.5,\n        },\n        {\n            \"name\": \"Case 4\",\n            \"A_diag\": [20.0, 1.0],\n            \"x\": [0.5, 2.0],\n            \"g_star\": [5.0, 0.0],\n            \"lambda_reg\": 0.2,\n            \"t0\": 0.05,\n            \"eta\": 0.5,\n            \"gamma\": 0.1,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_single_case(case)\n        results.append(result)\n\n    # Format the final output string according to the problem specification\n    # (no spaces, standard decimal notation for floats, specified list-of-lists structure).\n    result_strings = []\n    for res in results:\n        mask_val, t_ex, t_in = res\n        # Use fixed-point formatting to avoid scientific notation. 12 decimal places is sufficient.\n        t_ex_str = f\"{t_ex:.12f}\".rstrip('0').rstrip('.') if '.' in f\"{t_ex:.12f}\" else f\"{t_ex:.12f}\"\n        t_in_str = f\"{t_in:.12f}\".rstrip('0').rstrip('.') if '.' in f\"{t_in:.12f}\" else f\"{t_in:.12f}\"\n        result_strings.append(f\"[{str(mask_val).lower()},{t_ex_str},{t_in_str}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\ndef run_single_case(params):\n    \"\"\"\n    Executes the backtracking line search for a single test case.\n    \"\"\"\n    # Unpack parameters\n    A_diag = params[\"A_diag\"]\n    x = np.array(params[\"x\"])\n    g_star = np.array(params[\"g_star\"])\n    lambda_reg = params[\"lambda_reg\"]\n    t0 = params[\"t0\"]\n    eta = params[\"eta\"]\n    gamma = params[\"gamma\"]\n\n    # 1. Setup problem instance\n    A = np.diag(A_diag)\n    # A is diagonal, so A_inv_T = A_inv = diag(1/A_diag)\n    A_inv = np.diag([1.0 / val for val in A_diag])\n    b = A @ x - A_inv @ g_star\n    nabla_fx = g_star\n\n    # 2. Define helper functions\n    def f(y):\n        return 0.5 * np.linalg.norm(A @ y - b)**2\n    \n    def prox_l2(v, tau):\n        norm_v = np.linalg.norm(v)\n        if norm_v > tau:\n            return (1.0 - tau / norm_v) * v\n        else:\n            return np.zeros_like(v)\n\n    fx = f(x)\n    def armijo_test(y, t):\n        lhs = f(y)\n        diff_y_x = y - x\n        # Use cached values of f(x) and nabla_f(x)\n        rhs = fx + nabla_fx.T @ diff_y_x + (1.0 / (2.0 * t)) * np.linalg.norm(diff_y_x)**2\n        # Use a small tolerance for robust floating point comparison\n        return lhs = rhs + 1e-12\n\n    # 3. Backtracking for exact proximal update\n    t = t0\n    t_exact = 0.0\n    while t > 1e-16: # Safety break for extremely small step sizes\n        z = x - t * nabla_fx\n        y_exact = prox_l2(z, t * lambda_reg)\n        if armijo_test(y_exact, t):\n            t_exact = t\n            break\n        t *= eta\n    \n    # 4. Backtracking for inexact proximal update\n    t = t0\n    t_inexact = 0.0\n    while t > 1e-16:\n        z = x - t * nabla_fx\n        w = (1.0 - gamma) * x + gamma * z\n        y_inexact = prox_l2(w, gamma * t * lambda_reg)\n        if armijo_test(y_inexact, t):\n            t_inexact = t\n            break\n        t *= eta\n        \n    # 5. Determine masking\n    # Masking occurs if exact update fails at the step size accepted by inexact update\n    z_at_t_inexact = x - t_inexact * nabla_fx\n    y_exact_at_t_inexact = prox_l2(z_at_t_inexact, t_inexact * lambda_reg)\n    \n    # The first condition for masking is met by definition of t_inexact.\n    # We only need to check the second condition.\n    masking = not armijo_test(y_exact_at_t_inexact, t_inexact)\n\n    return [masking, t_exact, t_inexact]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}