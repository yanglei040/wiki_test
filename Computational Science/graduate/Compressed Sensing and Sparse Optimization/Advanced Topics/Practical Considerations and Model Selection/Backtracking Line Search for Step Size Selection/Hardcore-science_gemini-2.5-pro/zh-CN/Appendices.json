{
    "hands_on_practices": [
        {
            "introduction": "实践出真知。要真正掌握回溯线搜索，最好的方法就是亲手实现它。本练习将指导您为近端梯度法（一种解决 LASSO 等稀疏优化问题的核心算法）实现一个混合了固定初始步长与回溯调整的策略。通过对比不同初始步长选择下的算法行为，您将直观地理解回溯机制如何作为一种安全保障，自动调整步长以确保算法的稳健收敛。",
            "id": "3432732",
            "problem": "考虑复合凸目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ 且 $g(x) = \\lambda \\|x\\|_1$，这里 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$，$ \\lambda > 0$。$f$ 的梯度为 $\\nabla f(x) = A^\\top (A x - y)$，它是 Lipschitz 连续的，其 Lipschitz 常数为 $L = \\|A^\\top A\\|_2$。近端梯度法（也称为迭代收缩阈值算法）通过 $\\ell_1$ 范数的近端算子来更新 $x$，该算子是逐元素定义的软阈值映射 $S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$。\n\n您需要为应用于 $F(x)$ 的近端梯度迭代实现一种混合固定步长与回溯线搜索的策略来选择步长。对于第 $k$ 次迭代，该混合策略的操作如下：\n\n- 给定当前迭代点 $x_k$，选择一个试验步长 $t \\leftarrow t_{\\text{fixed}}$。\n- 计算候选更新 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n- 检查从 Lipschitz 梯度的下降引理派生出的充分下降条件：\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}).\n$$\n- 如果该不等式不成立，则以 $\\beta \\in (0,1)$ 缩小步长 $t \\leftarrow \\beta t$ 并重新计算候选点，直到条件满足。每次缩小计为一步回溯。\n- 该混合策略的特点是，每次迭代总是从同一个预设的 $t_{\\text{fixed}}$ 重新开始线搜索（而不是重用上一次接受的步长）。这优先尝试固定步长，但在需要时通过回溯来保证充分下降。\n\n实现这种混合方法，从 $x_0 = 0$ 开始，运行固定的迭代次数。对于每个指定的测试用例，返回两个量：$T$ 次迭代后的最终目标函数值 $F(x_T)$，以及在所有迭代中累积的回溯收缩总步数。\n\n使用以下测试套件：\n\n- 测试用例 $1$（良态矩阵，保守步长，预计回溯最少）：\n$$\nA_1 = \\begin{bmatrix}\n1  0  0.5  0 \\\\\n0  1  0.5  0 \\\\\n0  0  1  1\n\\end{bmatrix},\\quad\ny_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix},\\quad\n\\lambda_1 = 0.1,\\quad\nt_{\\text{fixed},1} = 0.05,\\quad\n\\beta_1 = 0.5,\\quad\nT_1 = 100.\n$$\n\n- 测试用例 $2$（数据同上，过于激进的固定步长以引发重复回溯）：\n$$\nA_1 \\text{ and } y_1 \\text{ as above},\\quad\n\\lambda_2 = 0.1,\\quad\nt_{\\text{fixed},2} = 10.0,\\quad\n\\beta_2 = 0.5,\\quad\nT_2 = 100.\n$$\n\n- 测试用例 $3$（病态程度更高的矩阵，中等激进的固定步长）：\n$$\nA_3 = \\begin{bmatrix}\n1  2  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  3\n\\end{bmatrix},\\quad\ny_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix},\\quad\n\\lambda_3 = 0.05,\\quad\nt_{\\text{fixed},3} = 5.0,\\quad\n\\beta_3 = 0.5,\\quad\nT_3 = 120.\n$$\n\n附加实现细节和要求：\n\n- 使用近端梯度更新 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$，其中软阈值算子 $S_{\\alpha}$ 逐元素应用。\n- 所有测试用例均使用 $x_0 = 0$。\n- 为了在接受准则中保证数值稳定性，你可以允许一个微小的非负容差 $\\varepsilon$，使得条件被强制为 $F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}) + \\varepsilon$，例如，$\\varepsilon = 10^{-12}$。\n- 不涉及物理单位。\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，报告两个值：最终目标函数值 $F(x_{T_i})$（四舍五入到 6 位小数），以及所有迭代中回溯收缩总步数的整数计数。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。该列表必须按\n$$\n[ F(x_{T_1}),\\ \\text{backtracks}_1,\\ F(x_{T_2}),\\ \\text{backtracks}_2,\\ F(x_{T_3}),\\ \\text{backtracks}_3 ],\n$$\n顺序排列，其中每个 $F(x_{T_i})$ 是一个四舍五入到 6 位小数的浮点数，每个回溯计数是一个整数。",
            "solution": "此问题已经过验证。\n\n### 第 1 步：提取已知信息\n\n- **目标函数**：$F(x) = f(x) + g(x)$，一个复合凸函数。\n- **光滑项**：$f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$。\n- **非光滑项**：$g(x) = \\lambda \\|x\\|_1$，其中 $\\lambda > 0$。\n- **光滑项的梯度**：$\\nabla f(x) = A^\\top (A x - y)$。\n- **近端算子**：$g(x)$ 的近端算子是软阈值函数，逐元素应用：$S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$。\n- **迭代更新规则**：近端梯度更新为 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n- **初始条件**：$x_0 = 0$。\n- **线搜索策略（混合固定步长与回溯）**：\n    1. 对于第 $k$ 次迭代，从试验步长 $t \\leftarrow t_{\\text{fixed}}$ 开始。\n    2. 计算候选更新 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n    3. 检查充分下降条件：$F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})$。\n    4. 如果条件不满足，则缩小步长 $t \\leftarrow \\beta t$（其中 $\\beta \\in (0,1)$），计为一步回溯，并从第 2 步重复。\n    5. 下一个主迭代 $k+1$ 从 $t_{\\text{fixed}}$ 重新开始搜索。\n- **数值容差**：在充分下降条件中允许一个容差 $\\varepsilon = 10^{-12}$。\n- **测试用例 1**：\n    - $A_1 = \\begin{bmatrix} 1  0  0.5  0 \\\\ 0  1  0.5  0 \\\\ 0  0  1  1 \\end{bmatrix}$，$y_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix}$\n    - $\\lambda_1 = 0.1$, $t_{\\text{fixed},1} = 0.05$, $\\beta_1 = 0.5$, $T_1 = 100$。\n- **测试用例 2**：\n    - $A_2 = A_1$, $y_2 = y_1$\n    - $\\lambda_2 = 0.1$, $t_{\\text{fixed},2} = 10.0$, $\\beta_2 = 0.5$, $T_2 = 100$。\n- **测试用例 3**：\n    - $A_3 = \\begin{bmatrix} 1  2  0  0 \\\\ 0  1  1  0 \\\\ 0  0  1  3 \\end{bmatrix}$，$y_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix}$\n    - $\\lambda_3 = 0.05$, $t_{\\text{fixed},3} = 5.0$, $\\beta_3 = 0.5$, $T_3 = 120$。\n- **输出**：对于每个测试用例，报告最终目标函数值 $F(x_T)$（四舍五入到 6 位小数）和回溯总步数的整数计数。\n\n### 第 2 步：使用提取的已知信息进行验证\n\n- **科学性**：该问题是近端梯度法（也称为迭代收缩阈值算法或 ISTA）在 LASSO 目标函数上的标准应用。所有组成部分——目标函数、近端算子（软阈值）、梯度以及带有指定充分下降条件的回溯线搜索——都是凸优化和信号处理中基本且成熟的概念。该问题在科学上是合理的。\n- **适定性**：该问题是适定的。目标函数 $F(x)$ 是凸函数，保证了最小化子的存在。算法被完整地指定了所有必要的参数（$A, y, \\lambda, t_{\\text{fixed}}, \\beta, T, x_0$），确保了迭代序列是唯一确定的。问题要求系统在固定迭代次数后的状态，这是一个明确定义的量。\n- **客观性**：该问题以精确的数学语言陈述。没有主观或含糊的术语。\n- **完整性和一致性**：该问题是自包含的。所有矩阵、向量、常数和初始条件都已提供。算法的指令和要求的输出格式是明确且一致的。\n- **其他缺陷**：该问题没有表现出任何其他缺陷，例如过于简单、不切实际或无法验证。测试用例中指定的参数被选择用来展示算法的不同行为（例如，保守与激进的步长），这是一种标准的教学方法。\n\n### 第 3 步：结论与行动\n此问题有效。将提供完整解答。\n\n该问题要求实现带特定线搜索策略的近端梯度法，以解决一个 LASSO 型优化问题。该方法的核心是一个迭代过程，用于最小化目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\|Ax - y\\|_2^2$ 是一个光滑、可微的损失项，$g(x) = \\lambda \\|x\\|_1$ 是一个非光滑的正则化项。\n\n第 $k$ 步的迭代更新由近端映射给出：\n$$\nx_{k+1} = \\text{prox}_{t g}(x_k - t \\nabla f(x_k))\n$$\n其中 $t > 0$ 是步长。对于 $g(x) = \\lambda \\|x\\|_1$，近端算子 $\\text{prox}_{t g}(z)$ 对应于软阈值算子 $S_{\\lambda t}(z)$，该算子逐元素应用：\n$$\nS_{\\alpha}(z_i) = \\text{sign}(z_i)\\max(|z_i| - \\alpha, 0)\n$$\n因此，更新规则变为：\n$$\nx_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))\n$$\n其中 $\\nabla f(x_k) = A^\\top(Ax_k - y)$。\n\n步长 $t$ 由回溯线搜索确定，以确保目标函数有充分的下降。对于每次迭代 $k$，搜索从一个固定的试验步长 $t = t_{\\text{fixed}}$ 开始。计算一个候选点 $x_{k+1}$。然后，必须验证一个充分下降条件。给出的条件是：\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})\n$$\n这个条件源于大化-最小化原则。不等式右侧是 $F(x)$ 在 $x_k$ 附近的一个二次上界。由于 $F(x_{k+1}) = f(x_{k+1}) + g(x_{k+1})$，项 $g(x_{k+1})$ 出现在不等式两侧，可以消去。这可将检查简化为：\n$$\nf(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2\n$$\n这是近端梯度法的标准回溯条件。如果它成立（在一个小的容差 $\\varepsilon$ 内），候选点 $x_{k+1}$ 被接受，算法进入下一次迭代 $k+1$。如果它不成立，步长 $t$ 将乘以一个因子 $\\beta \\in (0, 1)$（即 $t \\leftarrow \\beta t$），计算一个新的候选点 $x_{k+1}$，并重新检查条件。每次 $t$ 的减小都被计为一次回溯。这个过程保证了对于足够小的 $t$，条件最终会得到满足。\n\n整个算法在总共 $T$ 次迭代中按以下步骤进行：\n1.  初始化 $x_0 = 0$ 和 `total_backtracks` $= 0$。\n2.  对于 $k = 0, 1, \\dots, T-1$：\n    a. 设置试验步长 $t \\leftarrow t_{\\text{fixed}}$。\n    b. 计算梯度 $\\nabla f(x_k) = A^\\top(Ax_k - y)$。\n    c. 计算 $f(x_k) = \\frac{1}{2}\\|Ax_k - y\\|_2^2$。\n    d. 进入线搜索循环：\n        i. 计算候选点 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n        ii. 计算 $f(x_{k+1}) = \\frac{1}{2}\\|Ax_{k+1} - y\\|_2^2$。\n        iii. 构建接受条件的右侧：`RHS` $= f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2$。\n        iv. 如果 $f(x_{k+1}) \\le \\text{RHS} + \\varepsilon$，则接受 $x_{k+1}$ 并退出线搜索循环。\n        v. 否则，缩小步长 $t \\leftarrow \\beta t$，将 `total_backtracks` 加一，并从步骤 (i) 重复。\n    e. 更新 $x_k \\leftarrow x_{k+1}$。\n3.  $T$ 次迭代后，计算最终目标函数值 $F(x_T) = f(x_T) + g(x_T)$。\n4.  返回 $F(x_T)$ 和 `total_backtracks`。\n\n对指定的三个测试用例中的每一个都实现了此过程。最终目标函数值四舍五入到 6 位小数。",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the proximal gradient solver.\n    \"\"\"\n\n    TestCase = namedtuple('TestCase', ['A', 'y', 'lambda_', 't_fixed', 'beta', 'T'])\n\n    test_cases = [\n        # Test case 1\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=0.05,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 2\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=10.0,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 3\n        TestCase(\n            A=np.array([[1, 2, 0, 0], [0, 1, 1, 0], [0, 0, 1, 3]]),\n            y=np.array([0, 1, -1]),\n            lambda_=0.05,\n            t_fixed=5.0,\n            beta=0.5,\n            T=120\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        final_F, total_backtracks = proximal_gradient_solver(\n            A=case.A,\n            y=case.y,\n            lambda_=case.lambda_,\n            t_fixed=case.t_fixed,\n            beta=case.beta,\n            T=case.T\n        )\n        results.append(f\"{final_F:.6f}\")\n        results.append(str(total_backtracks))\n\n    print(f\"[{','.join(results)}]\")\n\ndef soft_threshold(z, alpha):\n    \"\"\"\n    Soft-thresholding operator for the L1 norm's proximal operator.\n    S_alpha(z) = sign(z) * max(|z| - alpha, 0)\n    \"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\ndef f(x, A, y):\n    \"\"\"\n    Computes the smooth part of the objective function: f(x) = 1/2 * ||Ax - y||_2^2\n    \"\"\"\n    residual = A @ x - y\n    return 0.5 * np.dot(residual, residual)\n\ndef g(x, lambda_):\n    \"\"\"\n    Computes the non-smooth part of the objective function: g(x) = lambda * ||x||_1\n    \"\"\"\n    return lambda_ * np.linalg.norm(x, 1)\n\ndef F(x, A, y, lambda_):\n    \"\"\"\n    Computes the total objective function F(x) = f(x) + g(x).\n    \"\"\"\n    return f(x, A, y) + g(x, lambda_)\n\ndef proximal_gradient_solver(A, y, lambda_, t_fixed, beta, T):\n    \"\"\"\n    Implements the Proximal Gradient Method with a hybrid fixed-step and backtracking\n    line search strategy.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    total_backtracks = 0\n    epsilon = 1e-12\n\n    for _ in range(T):\n        t = t_fixed\n        \n        # Pre-compute gradient and f(x) for the current iterate x\n        residual = A @ x - y\n        grad_f_x = A.T @ residual\n        f_x = 0.5 * np.dot(residual, residual)\n\n        while True:\n            # Candidate update step using the proximal operator\n            z = x - t * grad_f_x\n            x_next = soft_threshold(z, lambda_ * t)\n\n            # Check the sufficient decrease condition (backtracking line search)\n            # f(x_next) = f(x) + grad_f(x)^T(x_next - x) + (1/(2t))||x_next - x||^2\n            f_x_next = f(x_next, A, y)\n            \n            # The right-hand side of the inequality\n            diff_x = x_next - x\n            rhs = f_x + np.dot(grad_f_x, diff_x) + (0.5 / t) * np.dot(diff_x, diff_x)\n\n            if f_x_next = rhs + epsilon:\n                x = x_next\n                break  # Step size t is accepted\n            else:\n                # Shrink step size and try again\n                t = beta * t\n                total_backtracks += 1\n    \n    final_F_val = F(x, A, y, lambda_)\n    return final_F_val, total_backtracks\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在实现了基本的回溯线搜索之后，我们可以进一步探究其背后的数学原理。对于光滑部分 $f(x)$ 是二次函数这类在实践中极为常见的情形，抽象的充分下降条件可以被转化为一个关于“曲率”的直观判据。本练习将引导您推导这一等价条件，并利用它来精确预测算法在迭代过程中何时会接受其首次尝试的步长，从而将理论洞察力转化为可验证的预测。",
            "id": "3432788",
            "problem": "您需要为压缩感知和稀疏优化中遇到的一个凸复合目标函数，设计并分析一种带有回溯线搜索的近端梯度法。其中，光滑的数据拟合项是一个二次项，正则化项是促进稀疏性的绝对值范数。您的目标是：从第一性原理出发，推断线搜索为何以及何时会接受其首次尝试的步长（即，每次迭代的初始 Lipschitz 估计值保持不变时）；将这种接受情况与沿已发现稀疏结构上的曲率联系起来；并提出一个可实现的诊断指标，用以预测在支撑集发生变化后，首次尝试步长接受将于何时恢复。\n\n考虑无约束问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\n其中光滑项是二次项\n$$\nf(x) \\equiv \\frac{1}{2} \\lVert A x - b \\rVert_2^2,\n$$\n给定数据矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和向量 $b \\in \\mathbb{R}^m$，且 $\\lambda  0$ 是一个正则化参数。光滑项的梯度为 $\\nabla f(x) = A^\\top(Ax - b)$，该梯度是全局 Lipschitz 连续的，其 Lipschitz 常数等于 $A$ 的谱范数的平方。$\\lambda \\lVert x \\rVert_1$ 的邻近算子是坐标级的软阈值算子。\n\n您将使用如下形式的带有回溯线搜索的近端梯度法。在第 $k$ 次迭代中，给定当前迭代点 $x_k$ 和当前 Lipschitz 估计值 $L_k  0$，首先尝试使用步长 $1 / L_k$ 进行候选更新，\n$$\ny_k \\equiv \\operatorname{soft}\\Big(x_k - \\frac{1}{L_k} \\nabla f(x_k), \\, \\frac{\\lambda}{L_k}\\Big),\n$$\n并且当且仅当对于光滑部分，标准充分下降不等式在使用回溯法所用的二次上界模型中对同一个 $L_k$ 成立时，才接受这次首次尝试。否则，增加 Lipschitz 估计值（例如，乘以一个大于 1 的因子）并重试，直到不等式成立。在第 $k$ 次迭代中的“单位步长接受”意味着使用初始 $L_k$ 的首次尝试被接受（因此在该次迭代中 $L_k$ 保持不变）。\n\n从近端梯度回溯法中使用的光滑部分的充分下降不等式的核心定义出发，并结合 $f(x)$ 的显式二次形式，推导出一个专门针对此二次模型的、仅依赖于尝试的更新方向的充要接受准则。解释该准则如何定量地将接受的单位步长与沿近端梯度更新所确定的方向上遇到的曲率联系起来，以及这个方向如何受到已发现的稀疏支撑集的约束。\n\n利用这一原理，提出一个逐次迭代的标量诊断指标，该指标仅需根据数据 $(A,b)$、当前迭代点 $x_k$、当前梯度 $\\nabla f(x_k)$ 以及使用初始 $L_k$（在应用任何回溯增加之前）产生的首次尝试更新 $y_k$ 来计算。该诊断指标必须是一个实数，通过与 $L_k$ 比较，来预测在第 $k$ 次迭代中是否会接受单位步长；此外，在第 $t$ 次迭代检测到任何支撑集变化后（即 $x_{t+1}$ 的支撑集与 $x_t$ 的支撑集不同），您的诊断指标必须用于预测单位步长接受恢复的第一个后续迭代索引 $r  t$。形式上，将第 $t$ 次迭代的“支撑集变化”事件定义为\n$$\n\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t),\n$$\n其中 $\\operatorname{supp}(z) \\equiv \\{i : z_i \\neq 0\\}$。将此事件的“恢复索引” $r$ 定义为满足线搜索在第 $j$ 次迭代中接受其首次尝试的最小 $j  t$。您的诊断指标必须仅使用每次迭代计算的诊断值和每次迭代的初始 $L_k$ 来预测这个恢复索引。\n\n您必须实现一个程序，该程序对一组指定的测试用例执行以下操作：\n- 按照下述说明生成一个合成的压缩感知实例 $(A,b)$。\n- 运行带有回溯的近端梯度法，迭代固定次数。\n- 在每次迭代中，记录首次尝试是否被接受（单位步长接受），以及支撑集相对于前一个迭代点是否发生变化。\n- 在每次迭代中，仅使用首次尝试的更新来计算您提出的诊断指标。\n- 对于每个支撑集变化事件，计算真实的恢复索引（基于实际的单位步长接受结果）和预测的恢复索引（仅基于逐次迭代的诊断指标和那些迭代中的初始 $L_k$）。\n- 对于每个测试用例，输出一个整数，等于在固定迭代预算内观察到的所有支撑集变化事件中，真实恢复索引和预测恢复索引之间不匹配的总数。如果某个测试用例中没有支撑集变化，则该计数应为 $0$。\n\n在推导过程中，您可以假定并明确使用的基本出发点是：\n- 复合结构 $F(x) = f(x) + \\lambda \\lVert x \\rVert_1$、$\\lambda \\lVert x \\rVert_1$ 的邻近算子以及近端梯度迭代的定义。\n- 用于近端梯度法的回溯线搜索中的标准充分下降不等式，该不等式基于 $\\nabla f$ 的 Lipschitz 连续性。\n- 对于二次函数 $f(x)$，精确的差分恒等式 $f(x + h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2$。\n\n测试套件。使用以下测试用例，每个用例由整数 $(m,n)$、真实向量的稀疏度 $k_{\\text{true}}$、迭代预算 $K$、初始 Lipschitz 估计 $L_0$、回溯乘子 $\\eta  1$ 和随机种子完全指定。在所有情况下，初始化 $x_0 = 0$，并通过均匀随机选择 $k_{\\text{true}}$ 个索引，并将这些条目设置为在 $[1,2]$ 上均匀分布且符号随机的独立值，来生成一个真实的稀疏向量 $x^\\star$。对于无噪声数据，设置 $b = A x^\\star$。对于前两个用例，使用固定的正则化参数 $\\lambda$；对于第三个用例，将 $\\lambda$ 自适应地设置为一个可计算量的已知倍数，以引出全零解的边界情况。对于所有用例，最大迭代次数是固定的，您必须按照上述描述报告不匹配的计数。测试用例如下：\n\n- 测试用例 1 (良态高斯设计):\n  - $m = 40$, $n = 80$, $k_{\\text{true}} = 5$, $K = 120$, $L_0 = 0.3$, $\\eta = 2.0$, seed $= 1$, $\\lambda = 0.05$。\n  - 生成具有独立标准正态分布条目的矩阵 $A$，然后将每列归一化为单位欧几里得范数。\n\n- 测试用例 2 (强相关设计):\n  - $m = 40$, $n = 60$, $k_{\\text{true}} = 8$, $K = 150$, $L_0 = 0.2$, $\\eta = 2.0$, seed $= 7$, $\\lambda = 0.03$。\n  - 通过 $\\Sigma_{ij} = \\rho^{\\lvert i - j \\rvert}$ (其中 $1 \\le i,j \\le n$) 生成相关参数为 $\\rho = 0.9$ 的 Toeplitz 协方差矩阵。抽取一个原始矩阵 $Z \\in \\mathbb{R}^{m \\times n}$，其条目独立分布为 $\\mathcal{N}(0,1)$，构造 $A_{\\text{raw}} = Z \\, C$，其中 $C$ 是 $\\Sigma$ 的任意矩阵平方根（例如 Cholesky 因子），然后将 $A_{\\text{raw}}$ 的每列归一化为单位欧几里得范数以获得 $A$。\n\n- 测试用例 3 (零解边界情况):\n  - $m = 30$, $n = 50$, $k_{\\text{true}} = 5$, $K = 60$, $L_0 = 0.5$, $\\eta = 2.0$, seed $= 11$，并设置 $\\lambda = 1.05 \\, \\lVert A^\\top b \\rVert_\\infty$。\n  - 像测试用例 1 那样生成 $A$ (具有单位范数列的高斯矩阵)，并构造 $b = A x^\\star$。然后按指定计算 $\\lambda$。\n\n角度单位不适用。此问题中没有物理单位。\n\n最终程序输出格式。您的程序应生成单行输出，其中包含一个逗号分隔的不匹配计数值列表，每个测试用例一个，并用方括号括起来。例如：\n\"[c1,c2,c3]\"\n其中 $c1$、$c2$ 和 $c3$ 分别是测试用例 1、2 和 3 的不匹配计数的整数值。不应产生任何其他输出。",
            "solution": "该问题要求对应用于 LASSO 目标函数的近端梯度法的回溯线搜索条件进行分析，推导用于步长接受的诊断指标，并实现一个程序来验证该诊断指标的预测能力。\n\n该优化问题是\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ 是光滑部分，$\\lambda \\lVert x \\rVert_1$ 是非光滑正则化项。近端梯度法通过以下更新规则生成一系列迭代点 $x_k$\n$$\nx_{k+1} = \\operatorname{prox}_{\\alpha_k \\lambda \\lVert \\cdot \\rVert_1}(x_k - \\alpha_k \\nabla f(x_k)),\n$$\n其中 $\\alpha_k  0$ 是步长。$\\ell_1$ 范数的邻近算子是软阈值算子，$\\operatorname{soft}(z, \\tau)_i = \\operatorname{sgn}(z_i) \\max(\\lvert z_i \\rvert - \\tau, 0)$。步长 $\\alpha_k$ 由回溯线搜索确定。\n\n在第 $k$ 次迭代中，给定当前迭代点 $x_k$，我们从 $\\nabla f$ 的局部 Lipschitz 常数的一个初始猜测值 $L_{k, \\text{init}}$ 开始。然后我们尝试步长 $\\alpha_k = 1/L$，其中 $L=L_{k, \\text{init}}$。候选更新为\n$$\ny_k = \\operatorname{soft}\\Big(x_k - \\frac{1}{L} \\nabla f(x_k), \\, \\frac{\\lambda}{L}\\Big).\n$$\n如果该候选点满足光滑部分 $f$ 的充分下降条件，则它被接受。该条件确保在 $x_k$ 处控制 $f$ 的二次模型，在新的点 $y_k$ 处是 $f$ 的一个有效上界：\n$$\nf(y_k) \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\n如果这个不等式对 $L = L_{k, \\text{init}}$ 成立，则该步长是一个“单位步长”（或首次尝试接受）。新的迭代点变为 $x_{k+1} = y_k$，本次迭代的线搜索终止。如果不等式不成立，我们增加 $L$（例如，对于某个 $\\eta  1$，令 $L \\leftarrow \\eta L$），并重复计算新的候选点 $y_k$ 和检查不等式的过程，直到它被满足。\n\n**接受准则的推导**\n\n分析的核心在于将这个不等式特化到我们特定的二次函数 $f(x)$。问题为二次函数提供了以下精确恒等式：\n$$\nf(x+h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2.\n$$\n让我们通过设置 $x = x_k$ 和位移 $h = y_k - x_k$ 来应用这个恒等式。充分下降不等式的左侧 $f(y_k)$ 可以重写为：\n$$\nf(y_k) = f(x_k + (y_k - x_k)) = f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2.\n$$\n将此表达式代入充分下降不等式，得到：\n$$\nf(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2 \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\n消去两侧的公因子 $f(x_k)$ 和 $\\nabla f(x_k)^\\top (y_k - x_k)$ 并乘以 2 后，我们得到一个简化的等价不等式：\n$$\n\\lVert A(y_k - x_k) \\rVert_2^2 \\le L \\lVert y_k - x_k \\rVert_2^2.\n$$\n这是线搜索接受步长 $\\alpha_k = 1/L$ 的一个充要条件。令 $d_k = y_k - x_k$ 为更新向量。如果 $d_k = 0$，不等式 $0 \\le 0$ 平凡成立。如果 $d_k \\neq 0$，该条件可以表示为：\n$$\n\\frac{\\lVert A d_k \\rVert_2^2}{\\lVert d_k \\rVert_2^2} \\le L.\n$$\n这表明，当且仅当矩阵 $A^\\top A$（即 $f$ 的 Hessian 矩阵）关于更新向量 $d_k$ 的瑞利商不大于 Lipschitz 估计值 $L$ 时，线搜索才接受该步长。\n\n**与曲率和稀疏支撑集的关系**\n\n项 $\\lVert A d_k \\rVert_2^2$ 可以写为 $(A d_k)^\\top (A d_k) = d_k^\\top A^\\top A d_k$。由于 Hessian 矩阵是 $\\nabla^2 f(x) = A^\\top A$，该项代表了 $f$ 沿着方向 $d_k$ 的二阶变化，即曲率。因此，接受条件表明，沿所选更新方向的有效曲率必须以 $L$ 为界。\n\n更新向量 $d_k = y_k - x_k$ 由近端梯度步骤确定。软阈值操作确保 $y_k$ 是稀疏的。因此，$d_k$ 也是稀疏的，其非零项通常局限于 $y_k$ 的支撑集内。当算法的活跃集（迭代点的支撑集）稳定时，向量 $d_k$ 被限制在一个特定的子空间内。回溯线搜索会调整 $L$，使其成为该子空间内曲率的一个上界。$x_k$ 支撑集的变化意味着 $d_k$ 探测了一个新的方向或子空间，这个新方向或子空间可能具有更高的曲率（即更大的瑞利商）。如果这个新的曲率超过了当前的估计值 $L$，线搜索将会失败，从而迫使 $L$ 增加，直到它能适当地为新活跃子空间中的曲率提供上界。\n\n**提出的诊断指标和预测**\n\n基于推导出的准则，我们可以定义一个标量诊断指标。在第 $k$ 次迭代中，令 $L_{k, \\text{init}}$ 为首次尝试使用的初始 Lipschitz 估计值。首次尝试的候选更新为 $y_{k, \\text{init}} = \\operatorname{soft}(x_k - \\frac{1}{L_{k, \\text{init}}} \\nabla f(x_k), \\frac{\\lambda}{L_{k, \\text{init}}})$。令 $d_{k, \\text{init}} = y_{k, \\text{init}} - x_k$。诊断指标即为瑞利商：\n$$\nD_k = \\begin{cases} \\frac{\\lVert A d_{k, \\text{init}} \\rVert_2^2}{\\lVert d_{k, \\text{init}} \\rVert_2^2}  \\text{if } d_{k, \\text{init}} \\neq 0 \\\\ 0  \\text{if } d_{k, \\text{init}} = 0 \\end{cases}\n$$\n当且仅当 $D_k \\le L_{k, \\text{init}}$ 时，在第 $k$ 次迭代中接受“单位步长”。这不是一个启发式预测，而是从推导出的充要条件的直接结果。\n\n在第 $t$ 次迭代的“支撑集变化”事件是 $\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t)$。真实的恢复索引是 $r = \\min \\{j  t \\mid \\text{在第 } j \\text{ 步的首次尝试被接受}\\}$。我们的预测恢复索引 $\\hat{r}$ 是通过在每个后续迭代中检查我们的诊断条件找到的：$\\hat{r} = \\min \\{j  t \\mid D_j \\le L_{j, \\text{init}}\\}$。\n\n由于条件 $D_j \\le L_{j, \\text{init}}$ 精确地是首次尝试被接受的条件，因此只要计算是精确的，预测的恢复索引 $\\hat{r}$ 在每种情况下都必须与真实的恢复索引 $r$ 完全相同。因此，真实索引和预测索引之间的不匹配数量预期为 $0$。该实现作为此分析结论的验证。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, toeplitz\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef run_one_case(m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params=None):\n    \"\"\"\n    Runs a single test case for the proximal gradient algorithm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # --- 1. Generate synthetic data ---\n    # Generate ground-truth sparse vector x_star\n    x_star = np.zeros(n)\n    support_star_indices = rng.choice(n, k_true, replace=False)\n    x_star[support_star_indices] = rng.uniform(1, 2, k_true)\n    x_star[support_star_indices] *= rng.choice([-1, 1], k_true)\n\n    # Generate matrix A\n    if A_gen_mode == 'gaussian':\n        A = rng.standard_normal(size=(m, n))\n        col_norms = np.linalg.norm(A, axis=0)\n        A = A / col_norms\n    elif A_gen_mode == 'correlated':\n        rho = 0.9\n        Sigma = toeplitz(rho**np.arange(n))\n        # Add a small identity matrix for stability before Cholesky\n        C = cholesky(Sigma + 1e-10 * np.eye(n), lower=True)\n        Z = rng.standard_normal(size=(m, n))\n        A_raw = Z @ C.T\n        col_norms = np.linalg.norm(A_raw, axis=0)\n        A = A_raw / col_norms\n\n    # Generate measurement vector b\n    b = A @ x_star\n\n    # Set lambda if adaptive\n    if lambda_mode_params == 'adaptive':\n        lambda_val = 1.05 * np.max(np.abs(A.T @ b))\n\n    # --- 2. Run Proximal Gradient with Backtracking ---\n    x_k = np.zeros(n)\n    L_val = L0\n    \n    is_unit_step_accepted = []\n    support_changed = []\n    diagnostics = []\n    L_inits = []\n\n    for k in range(K):\n        # Store support of x_k for later comparison\n        support_k = set(np.where(x_k != 0)[0])\n        \n        # --- At iteration k, perform one step ---\n        grad_fxk = A.T @ (A @ x_k - b)\n        \n        # This is the initial Lipschitz estimate for this iteration\n        L_k_init = L_val\n        L_inits.append(L_k_init)\n\n        # Compute first-attempt update and diagnostic\n        y_k_init = soft_threshold(x_k - (1/L_k_init) * grad_fxk, lambda_val / L_k_init)\n        d_k_init = y_k_init - x_k\n        \n        if np.linalg.norm(d_k_init)  1e-12:\n            D_k = 0.0\n        else:\n            Adk_norm_sq = np.linalg.norm(A @ d_k_init)**2\n            dk_norm_sq = np.linalg.norm(d_k_init)**2\n            D_k = Adk_norm_sq / dk_norm_sq\n        diagnostics.append(D_k)\n\n        # Backtracking line search\n        L_attempt = L_k_init\n        num_backtracks = 0\n        while True:\n            y_k = soft_threshold(x_k - (1/L_attempt) * grad_fxk, lambda_val / L_attempt)\n            d_k = y_k - x_k\n\n            if np.linalg.norm(d_k)  1e-12:\n                # If step is zero, condition is trivially satisfied.\n                break \n\n            lhs = np.linalg.norm(A @ d_k)**2\n            rhs = L_attempt * np.linalg.norm(d_k)**2\n            \n            if lhs = rhs:\n                break\n            \n            L_attempt *= eta\n            num_backtracks += 1\n\n        # Accept the step\n        is_unit_step_accepted.append(num_backtracks == 0)\n        x_k = y_k  # Update x_k to x_{k+1}\n        L_val = L_attempt # Update L for next iteration\n\n        # Check for support change\n        support_k_plus_1 = set(np.where(x_k != 0)[0])\n        support_changed.append(support_k != support_k_plus_1)\n\n    # --- 3. Post-process to count mismatches ---\n    mismatch_count = 0\n    support_change_indices = [t for t, changed in enumerate(support_changed) if changed]\n\n    for t in support_change_indices:\n        r_true = -1\n        for j in range(t + 1, K):\n            if is_unit_step_accepted[j]:\n                r_true = j\n                break\n        \n        r_pred = -1\n        for j in range(t + 1, K):\n            # The prediction check: D_j = L_{j,init}\n            if diagnostics[j] = L_inits[j]:\n                r_pred = j\n                break\n        \n        if r_true != r_pred:\n            mismatch_count += 1\n            \n    return mismatch_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params)\n        (40, 80, 5, 120, 0.3, 2.0, 1, 0.05, 'gaussian', None),\n        (40, 60, 8, 150, 0.2, 2.0, 7, 0.03, 'correlated', None),\n        (30, 50, 5, 60, 0.5, 2.0, 11, None, 'gaussian', 'adaptive')\n    ]\n\n    results = []\n    for params in test_cases:\n        mismatch_count = run_one_case(*params)\n        results.append(mismatch_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "回溯线搜索不仅是基础算法的基石，也是更高级优化方法（如近端牛顿法）中不可或缺的组成部分。近端牛顿法通过构建目标函数的局部二次模型 $m_B(d; x)$ 来生成一个更优的搜索方向 $d$ 。本练习将带您体验这一过程，您需要为不同的二次模型（由不同的矩阵 $B$ 定义）计算搜索方向，并利用回溯线搜索来确定沿该方向前进的最佳步长，最终比较不同模型选择对单步优化效果的影响。",
            "id": "3432780",
            "problem": "考虑压缩感知和稀疏优化中的一个基本复合凸优化问题，该问题定义为在 $x \\in \\mathbb{R}^n$上最小化函数 $F(x)$，其中\n$$\nF(x) = g(x) + \\lambda \\lVert x \\rVert_1, \\quad g(x) = \\tfrac{1}{2} \\lVert A x - y \\rVert_2^2,\n$$\n其中数据矩阵 $A \\in \\mathbb{R}^{m \\times n}$，数据向量 $y \\in \\mathbb{R}^m$，正则化参数 $\\lambda  0$。光滑部分 $g(x)$ 的梯度为 $\\nabla g(x) = A^\\top (A x - y)$，Hessian 矩阵为 $\\nabla^2 g(x) = A^\\top A$。近端牛顿法通过最小化一个局部二次加非光滑模型来生成试验方向 $d \\in \\mathbb{R}^n$\n$$\nm_B(d; x) = \\nabla g(x)^\\top d + \\tfrac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1 - \\lambda \\lVert x \\rVert_1,\n$$\n其中 $B \\in \\mathbb{R}^{n \\times n}$ 是一个近似 $\\nabla^2 g(x)$ 的对称正定矩阵。预测的模型下降量为\n$$\n\\Delta_B(x; d) = m_B(0; x) - m_B(d; x) = - \\nabla g(x)^\\top d - \\tfrac{1}{2} d^\\top B d + \\lambda \\left(\\lVert x \\rVert_1 - \\lVert x + d \\rVert_1\\right).\n$$\n使用参数 $c \\in (0,1)$ 和 $\\beta \\in (0,1)$ 的回溯线搜索选择满足充分下降条件\n$$\nF(x + t d) \\le F(x) - c \\, t \\, \\Delta_B(x; d)\n$$\n的最大 $t \\in \\{1, \\beta, \\beta^2, \\dots \\}$，其中 $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$。假设 $B$ 从由 $A^\\top A$ 构造的三个对称正定选项中选择：单位矩阵的标量倍 $B = L I$，其中 $L$ 等于 $A^\\top A$ 的最大特征值；一个对角主化矩阵 $B = \\mathrm{diag}(A^\\top A)$；以及一个标量倍 $B = s I$，其中 $s = \\tfrac{1}{n} \\mathrm{trace}(A^\\top A)$。对于每个这样的 $B$，试验方向 $d$ 定义为 $m_B(d; x)$ 的精确最小化子。\n\n从给定的初始点 $x \\in \\mathbb{R}^n$ 开始，为每个候选 $B$ 执行以下操作：\n- 计算最小化 $m_B(d; x)$ 的模型最小化方向 $d \\in \\mathbb{R}^n$。\n- 计算预测的模型下降量 $\\Delta_B(x; d)$。\n- 如果 $\\Delta_B(x; d) \\le 0$，则将接受的步长定义为 $t = 0$，新点为 $x$ 本身。否则，使用参数 $c$ 和 $\\beta$ 执行回溯，以找到满足充分下降条件的最大 $t \\in \\{1, \\beta, \\beta^2, \\dots\\}$；如果 $t$ 低于某个数值阈值，则终止回溯，此时设 $t = 0$，新点为 $x$。\n- 记录得到的函数值 $F(x + t d)$。\n\n在三个候选者中，选择能获得最小接受函数值 $F(x + t d)$ 的那一个。如果接受的函数值出现平局，则优先选择接受的步长 $t$ 较大的那一个。如果仍然存在平局，则按照排序 $B = L I$ 为索引 $1$，$B = \\mathrm{diag}(A^\\top A)$ 为索引 $2$，$B = s I$ 为索引 $3$ 的顺序，选择索引最小的那一个。\n\n您的任务是实现一个程序，对于下面定义的每个测试用例，返回一个包含三个条目的列表：所选索引 $k \\in \\{1,2,3\\}$、接受的步长 $t \\in \\mathbb{R}$ 和接受的函数值 $F(x + t d) \\in \\mathbb{R}$。所有浮点输出必须精确到六位小数。\n\n仅使用纯粹的数学和逻辑计算。不涉及物理单位。不使用角度。不使用百分比。\n\n测试套件：\n- 情况 1 (理想路径)：\n  - $A = \\begin{bmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\\\ 2  -1  0 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.1$,\n  - $x = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n- 情况 2 (强稀疏性压力下的边界情况)：\n  - $A$ 如情况 1，\n  - $y$ 如情况 1，\n  - $\\lambda = 5.0$,\n  - $x = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n- 情况 3 (病态几何结构)：\n  - $A = \\begin{bmatrix}\n  10  0  0  0 \\\\\n  0  1  0  0 \\\\\n  0  0  0.1  0 \\\\\n  0  0  0  0.01 \\\\\n  10  1  0.1  0.01\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.05$,\n  - $x = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身都是一个形如 $[k, t, F]$ 的列表，对应一个测试用例。例如，输出必须具有以下形式\n$[\\,[k_1,t_1,F_1],[k_2,t_2,F_2],[k_3,t_3,F_3]\\,]$,\n其中每个 $t_i$ 和 $F_i$ 都精确到六位小数，并且打印输出中没有空格。",
            "solution": "用户提供了一个来自数值优化领域的问题，具体涉及用于解决压缩感知和稀疏优化中普遍存在的复合凸优化问题的近端牛顿法。该问题提法明确，具有科学依据，并包含了获得唯一解所需的所有信息。因此，该问题被视为有效。\n\n核心任务是解决优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\quad \\text{其中} \\quad F(x) = g(x) + \\lambda \\lVert x \\rVert_1\n$$\n这里，$g(x) = \\frac{1}{2} \\lVert Ax - y \\rVert_2^2$ 是一个光滑的凸函数，代表数据保真项，而 $\\lambda \\lVert x \\rVert_1$ 是一个非光滑的凸正则化项，用以促进解 $x$ 的稀疏性。光滑部分的梯度是 $\\nabla g(x) = A^\\top (Ax - y)$。\n\n问题指定使用近端牛顿法。该方法通过最小化函数 $F(x+d)$ 的一个局部模型来在点 $x$ 处生成一个搜索方向 $d$。模型 $m_B(d; x)$ 是通过将 $g(x+d)$ 替换为以 $x$ 为中心的二阶泰勒近似，并保持非光滑项 $\\lambda \\lVert x+d \\rVert_1$ 不变而构造的。$g(x)$ 的 Hessian 矩阵 $\\nabla^2 g(x) = A^\\top A$ 由一个对称正定矩阵 $B$ 近似。需要最小化的模型由下式给出：\n$$\nm_B(d; x) = g(x) + \\nabla g(x)^\\top d + \\frac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1\n$$\n关于 $d$ 最小化 $m_B(d; x)$ 等价于最小化\n$$\nd = \\arg\\min_{d \\in \\mathbb{R}^n} \\left\\{ \\nabla g(x)^\\top d + \\frac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1 \\right\\}\n$$\n这是一个凸优化子问题。我们可以通过找到最小化相应目标的唯一解 $u = x+d$ 来解决它：\n$$\nu = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\nabla g(x)^\\top (u-x) + \\frac{1}{2} (u-x)^\\top B (u-x) + \\lambda \\lVert u \\rVert_1 \\right\\}\n$$\n通过配方法，这可以表示为一个近端算子问题。解的形式取决于矩阵 $B$ 的结构。\n\n**情况 1：各向同性 Hessian 近似 ($B = \\gamma I$)**\n当 $B$ 是单位矩阵的标量倍，$B = \\gamma I$ (对于某个标量 $\\gamma  0$)，子问题简化为：\n$$\nu = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\frac{\\gamma}{2} \\left\\lVert u - \\left(x - \\frac{1}{\\gamma} \\nabla g(x)\\right) \\right\\rVert_2^2 + \\lambda \\lVert u \\rVert_1 \\right\\}\n$$\n解由 $\\ell_1$ 范数的近端算子给出，即软阈值算子 $S_{\\kappa}(\\cdot)$：\n$$\nu = S_{\\lambda/\\gamma}\\left(x - \\frac{1}{\\gamma} \\nabla g(x)\\right)\n$$\n其中 $(S_\\kappa(z))_i = \\mathrm{sign}(z_i) \\max(|z_i| - \\kappa, 0)$。搜索方向则是 $d = u - x$。这种情况适用于两个指定的 $B$ 选项：\n- **索引 1**：$B = L I$，其中 $L$ 是 $A^\\top A$ 的最大特征值。这里，$\\gamma = L$。\n- **索引 3**：$B = s I$，其中 $s = \\frac{1}{n} \\mathrm{trace}(A^\\top A)$。这里，$\\gamma = s$。\n\n**情况 2：对角 Hessian 近似 ($B = \\mathrm{diag}(A^\\top A)$)**\n当 $B$ 是一个对角矩阵时，子问题按坐标解耦：\n$$\nu_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\frac{B_{ii}}{2} \\left( u_i - \\left(x_i - \\frac{(\\nabla g(x))_i}{B_{ii}}\\right) \\right)^2 + \\lambda |u_i| \\right\\}\n$$\n每个坐标的解再次由软阈值给出：\n$$\nu_i = S_{\\lambda/B_{ii}}\\left(x_i - \\frac{(\\nabla g(x))_i}{B_{ii}}\\right)\n$$\n搜索方向是 $d = u - x$。这对应于**索引 2**的选择：$B = \\mathrm{diag}(A^\\top A)$。\n\n一旦对于给定的 $B$ 计算出方向 $d$，我们通过回溯线搜索确定步长 $t$。首先，我们计算采取步长 $d$ 后模型值的预测下降量：\n$$\n\\Delta_B(x; d) = m_B(0; x) - m_B(d; x) = -\\nabla g(x)^\\top d - \\frac{1}{2} d^\\top B d + \\lambda (\\lVert x \\rVert_1 - \\lVert x + d \\rVert_1)\n$$\n如果 $\\Delta_B(x; d) \\le 0$，则该方向不是模型的下降方向，我们采取零步长，$t=0$。否则，我们寻找满足充分下降条件（一个阿米霍型条件）的最大步长 $t \\in \\{1, \\beta, \\beta^2, \\dots\\}$：\n$$\nF(x + t d) \\le F(x) - c \\, t \\, \\Delta_B(x; d)\n$$\n其中 $c \\in (0,1)$ 是一个给定的常数。如果 $t$ 低于某个数值精度阈值，则过程终止，此时 $t$ 被设为 $0$。\n\n对于每个测试用例，整体算法流程如下：\n1.  使用给定的 $A, y, \\lambda, x, c, \\beta$ 进行初始化。\n2.  计算矩阵 $A^\\top A$。\n3.  对于三个候选矩阵 $B_k$ (对于 $k=1,2,3$) 中的每一个：\n    a. 构造 $B_k$。对于 $k=1$，计算 $A^\\top A$ 的最大特征值。对于 $k=2$，取 $A^\\top A$ 的对角线。对于 $k=3$，计算 $A^\\top A$ 的归一化迹。\n    b. 计算梯度 $\\nabla g(x) = A^\\top(Ax-y)$。\n    c. 通过解决与 $B_k$ 关联的近端子问题来计算搜索方向 $d_k$。\n    d. 计算预测的下降量 $\\Delta_{B_k}(x;d_k)$。\n    e. 执行回溯线搜索以找到接受的步长 $t_k$。\n    f. 计算新的函数值 $F(x+t_k d_k)$。\n4. 比较 $k=1,2,3$ 的结果 $(t_k, F(x+t_k d_k))$。根据指定标准选择最佳索引 $k^*$：\n    a. 产生最小最终函数值的那一个。\n    b. 如果出现平局，选择具有最大接受步长 $t_k$ 的那一个。\n    c. 如果仍然平局，选择索引 $k$ 最小的那一个。\n5. 测试用例的最终输出是三元组 $[k^*, t_{k^*}, F(x+t_{k^*}d_{k^*})]$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the proximal Newton step selection problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        (\n            np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]], dtype=float),\n            np.array([3, -1, 2, 0], dtype=float),\n            0.1,\n            np.array([0, 0, 0], dtype=float),\n            0.1,\n            0.5\n        ),\n        # Case 2\n        (\n            np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]], dtype=float),\n            np.array([3, -1, 2, 0], dtype=float),\n            5.0,\n            np.array([1, -2, 3], dtype=float),\n            0.1,\n            0.5\n        ),\n        # Case 3\n        (\n            np.array([\n                [10, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 0.1, 0],\n                [0, 0, 0, 0.01],\n                [10, 1, 0.1, 0.01]\n            ], dtype=float),\n            np.array([1, 0.1, -0.1, 0.05, 0], dtype=float),\n            0.05,\n            np.array([0.5, -0.5, 0.5, -0.5], dtype=float),\n            0.1,\n            0.5\n        ),\n    ]\n\n    all_results = []\n    \n    # Helper functions\n    def soft_threshold(z, kappa):\n        if np.isscalar(kappa):\n            return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n        else: # Element-wise thresholding\n            return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n\n    def F_func(x_vec, A, y, lam):\n        g = 0.5 * np.linalg.norm(A @ x_vec - y)**2\n        h = lam * np.linalg.norm(x_vec, 1)\n        return g + h\n\n    for A, y, lam, x, c, beta in test_cases:\n        m, n = A.shape\n        ATA = A.T @ A\n        \n        # B candidate definitions\n        # B_1: L*I\n        L = np.linalg.eigvalsh(ATA).max()\n        B1 = L * np.identity(n)\n        \n        # B_2: diag(A^T A)\n        diag_ATA = np.diag(ATA)\n        # Assuming positive definite, so no zero diagonal entries\n        B2 = np.diag(diag_ATA)\n\n        # B_3: s*I\n        s = np.trace(ATA) / n\n        B3 = s * np.identity(n)\n\n        B_candidates = [(1, B1, 'isotropic', L), (2, B2, 'diagonal', diag_ATA), (3, B3, 'isotropic', s)]\n\n        grad_g_x = A.T @ (A @ x - y)\n        F_current = F_func(x, A, y, lam)\n        \n        case_results = []\n\n        for k, B, b_type, b_param in B_candidates:\n            d = np.zeros(n)\n            # Compute direction d\n            if b_type == 'isotropic':\n                gamma = b_param\n                u = soft_threshold(x - (1/gamma) * grad_g_x, lam / gamma)\n                d = u - x\n            elif b_type == 'diagonal':\n                # b_param is the diagonal vector\n                u = soft_threshold(x - grad_g_x / b_param, lam / b_param)\n                d = u - x\n\n            # Compute predicted reduction delta_B\n            delta_B = -grad_g_x.T @ d - 0.5 * d.T @ B @ d + lam * (np.linalg.norm(x, 1) - np.linalg.norm(x + d, 1))\n            \n            t_accepted = 0.0\n            F_accepted = F_current\n            \n            if delta_B > 0:\n                t_try = 1.0\n                t_min_thresh = 1e-12\n                while t_try > t_min_thresh:\n                    x_new = x + t_try * d\n                    F_new = F_func(x_new, A, y, lam)\n                    \n                    if F_new = F_current - c * t_try * delta_B:\n                        t_accepted = t_try\n                        F_accepted = F_new\n                        break\n                    \n                    t_try *= beta\n            \n            case_results.append((F_accepted, t_accepted, k))\n\n        # Select the best result based on the criteria\n        # 1. Min F_accepted\n        # 2. Max t_accepted (tie-breaker)\n        # 3. Min k (tie-breaker)\n        case_results.sort(key=lambda res: (res[0], -res[1], res[2]))\n        \n        best_F, best_t, best_k = case_results[0]\n        all_results.append(f\"[{best_k},{best_t:.6f},{best_F:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}