{
    "hands_on_practices": [
        {
            "introduction": "本节的实践练习旨在将理论知识转化为实际技能。第一个练习是实现带回溯线搜索的近端梯度法，这是掌握步长选择策略的基础。通过为 LASSO 问题编写迭代阈值收缩算法（ISTA），您将直接应用在前面章节中学到的充分下降条件，并观察回溯机制如何在初始步长过于激进时保证算法的稳定收敛。",
            "id": "3432732",
            "problem": "考虑复合凸目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ 且 $g(x) = \\lambda \\|x\\|_1$，$A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$，$\\lambda > 0$。$f$ 的梯度为 $\\nabla f(x) = A^\\top (A x - y)$，该梯度是 Lipschitz 连续的，其 Lipschitz 常数为 $L = \\|A^\\top A\\|_2$。近端梯度法（也称为迭代软阈值算法）通过 $\\ell_1$-范数的近端算子来更新 $x$，该算子是逐元素定义的软阈值映射 $S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$。\n\n您需要为应用于 $F(x)$ 的近端梯度迭代实现一种混合了固定步长和回溯线搜索的步长选择策略。该混合策略在第 $k$ 次迭代时按如下方式运行：\n\n- 给定当前迭代点 $x_k$，选择一个试验步长 $t \\leftarrow t_{\\text{fixed}}$。\n- 计算候选更新 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n- 检查从 Lipschitz 梯度的下降引理（Descent Lemma）推导出的充分下降条件：\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}).\n$$\n- 如果该不等式不成立，则将步长缩小 $t \\leftarrow \\beta t$（其中 $\\beta \\in (0,1)$），重新计算候选点，直到条件成立。每次缩小步长计为一次回溯。\n- 该混合策略的特点是，每次迭代总是从相同的预设值 $t_{\\text{fixed}}$ 重新开始线搜索（而不是重用上一次接受的步长）。这优先尝试固定步长，但在需要时通过回溯来保证充分下降。\n\n实现这种混合方法，从 $x_0 = 0$ 开始运行固定次数的迭代。对于每个指定的测试用例，返回两个量：经过 $T$ 次迭代后的最终目标值 $F(x_T)$，以及在所有迭代中累积的回溯收缩步骤的总数。\n\n使用以下测试套件：\n\n- 测试用例 $1$（良态矩阵，保守步长，预计回溯次数最少）：\n$$\nA_1 = \\begin{bmatrix}\n1  0  0.5  0 \\\\\n0  1  0.5  0 \\\\\n0  0  1  1\n\\end{bmatrix},\\quad\ny_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix},\\quad\n\\lambda_1 = 0.1,\\quad\nt_{\\text{fixed},1} = 0.05,\\quad\n\\beta_1 = 0.5,\\quad\nT_1 = 100.\n$$\n\n- 测试用例 $2$（数据相同，但采用过于激进的固定步长以触发重复回溯）：\n$$\nA_1 \\text{ 和 } y_1 \\text{ 如上},\\quad\n\\lambda_2 = 0.1,\\quad\nt_{\\text{fixed},2} = 10.0,\\quad\n\\beta_2 = 0.5,\\quad\nT_2 = 100.\n$$\n\n- 测试用例 $3$（病态程度更高的矩阵，采用中等激进的固定步长）：\n$$\nA_3 = \\begin{bmatrix}\n1  2  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  3\n\\end{bmatrix},\\quad\ny_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix},\\quad\n\\lambda_3 = 0.05,\\quad\nt_{\\text{fixed},3} = 5.0,\\quad\n\\beta_3 = 0.5,\\quad\nT_3 = 120.\n$$\n\n其他实现细节和要求：\n\n- 使用近端梯度更新 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$，其中软阈值算子 $S_{\\alpha}$ 逐元素应用。\n- 对所有测试用例使用 $x_0 = 0$。\n- 为了数值稳定性，在接受性检查中可以允许一个微小的非负容差 $\\varepsilon$，使得条件被强制为 $F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}) + \\varepsilon$，例如 $\\varepsilon = 10^{-12}$。\n- 不涉及物理单位。\n- 对于每个测试用例 $i \\in \\{1,2,3\\}$，报告两个值：四舍五入到 $6$ 位小数的最终目标值 $F(x_{T_i})$，以及所有迭代中回溯收缩步骤的总整数计数。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按以下顺序排列：\n$$\n[ F(x_{T_1}),\\ \\text{backtracks}_1,\\ F(x_{T_2}),\\ \\text{backtracks}_2,\\ F(x_{T_3}),\\ \\text{backtracks}_3 ],\n$$\n其中每个 $F(x_{T_i})$ 是一个四舍五入到 $6$ 位小数的浮点数，每个回溯计数是一个整数。",
            "solution": "对本问题进行验证。\n\n### 步骤 1：提取已知条件\n\n- **目标函数**：$F(x) = f(x) + g(x)$，一个复合凸函数。\n- **光滑项**：$f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $y \\in \\mathbb{R}^m$。\n- **非光滑项**：$g(x) = \\lambda \\|x\\|_1$，其中 $\\lambda > 0$。\n- **光滑项的梯度**：$\\nabla f(x) = A^\\top (A x - y)$。\n- **近端算子**：$g(x)$ 的近端算子是软阈值函数，逐元素应用：$S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$。\n- **迭代更新规则**：近端梯度更新为 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n- **初始条件**：$x_0 = 0$。\n- **线搜索策略（固定步长与回溯的混合策略）**：\n    1. 对于第 $k$ 次迭代，从一个试验步长 $t \\leftarrow t_{\\text{fixed}}$ 开始。\n    2. 计算候选更新 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n    3. 检查充分下降条件：$F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})$。\n    4. 如果条件不满足，则将步长缩小 $t \\leftarrow \\beta t$（其中 $\\beta \\in (0,1)$），计为一次回溯，并从步骤 2 重复。\n    5. 下一次主迭代 $k+1$ 从 $t_{\\text{fixed}}$ 重新开始搜索。\n- **数值容差**：在充分下降条件中允许使用 $\\varepsilon = 10^{-12}$ 的容差。\n- **测试用例 1**：\n    - $A_1 = \\begin{bmatrix} 1  0  0.5  0 \\\\ 0  1  0.5  0 \\\\ 0  0  1  1 \\end{bmatrix}$，$y_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix}$\n    - $\\lambda_1 = 0.1$，$t_{\\text{fixed},1} = 0.05$，$\\beta_1 = 0.5$，$T_1 = 100$。\n- **测试用例 2**：\n    - $A_2 = A_1$，$y_2 = y_1$\n    - $\\lambda_2 = 0.1$，$t_{\\text{fixed},2} = 10.0$，$\\beta_2 = 0.5$，$T_2 = 100$。\n- **测试用例 3**：\n    - $A_3 = \\begin{bmatrix} 1  2  0  0 \\\\ 0  1  1  0 \\\\ 0  0  1  3 \\end{bmatrix}$，$y_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix}$\n    - $\\lambda_3 = 0.05$，$t_{\\text{fixed},3} = 5.0$，$\\beta_3 = 0.5$，$T_3 = 120$。\n- **输出**：对于每个测试用例，报告四舍五入到 $6$ 位小数的最终目标值 $F(x_T)$ 和回溯步骤的总整数计数。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n- **科学依据**：该问题是近端梯度法（也称为迭代软阈值算法或 ISTA）在 LASSO 目标函数上的一个标准应用。所有组成部分——目标函数、近端算子（软阈值）、梯度以及带有指定充分下降条件的回溯线搜索——都是凸优化和信号处理中基本且公认的概念。该问题在科学上是合理的。\n- **适定性**：该问题是适定的。目标函数 $F(x)$ 是凸函数，保证了最小化子的存在。算法被完全指定，包含所有必要的参数（$A, y, \\lambda, t_{\\text{fixed}}, \\beta, T, x_0$），确保了迭代序列是唯一确定的。问题要求的是系统在固定次数迭代后的状态，这是一个明确定义的量。\n- **客观性**：问题以精确的数学语言陈述。没有主观或模棱两可的术语。\n- **完整性与一致性**：问题是自洽的。所有矩阵、向量、常数和初始条件都已提供。算法的指令和要求的输出格式是明确且一致的。\n- **其他缺陷**：该问题没有表现出任何其他缺陷，如过于简单、不切实际或无法验证。测试用例中指定的参数被选择用来展示算法的不同行为（例如，保守步长与激进的步长），这是一种标准的教学方法。\n\n### 步骤 3：结论与操作\n该问题有效。将提供完整解答。\n\n该问题要求实现带特定线搜索策略的近端梯度法，以解决一个 LASSO 类型的优化问题。该方法的核心是一个迭代过程，用以最小化目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2}\\|Ax - y\\|_2^2$ 是一个光滑、可微的损失项，$g(x) = \\lambda \\|x\\|_1$ 是一个非光滑的正则化项。\n\n在步骤 $k$ 的迭代更新由近端映射给出：\n$$\nx_{k+1} = \\text{prox}_{t g}(x_k - t \\nabla f(x_k))\n$$\n其中 $t > 0$ 是步长。对于 $g(x) = \\lambda \\|x\\|_1$，近端算子 $\\text{prox}_{t g}(z)$ 对应于软阈值算子 $S_{\\lambda t}(z)$，该算子逐元素应用：\n$$\nS_{\\alpha}(z_i) = \\text{sign}(z_i)\\max(|z_i| - \\alpha, 0)\n$$\n因此，更新规则变为：\n$$\nx_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))\n$$\n其中 $\\nabla f(x_k) = A^\\top(Ax_k - y)$。\n\n步长 $t$ 由回溯线搜索确定，以确保目标函数有充分的下降。对于每次迭代 $k$，搜索从一个固定的试验步长 $t = t_{\\text{fixed}}$ 开始。计算一个候选点 $x_{k+1}$。然后，必须验证一个充分下降条件。给定的条件是：\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})\n$$\n这个条件源自 Majorization-Minimization 原则。右侧是 $F(x)$ 在 $x_k$ 附近的一个二次上界。由于 $F(x_{k+1}) = f(x_{k+1}) + g(x_{k+1})$，项 $g(x_{k+1})$ 同时出现在两侧，可以消去。这简化了检查条件为：\n$$\nf(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2\n$$\n这是近端梯度法的标准回溯条件。如果它（在一个小的容差 $\\varepsilon$ 内）成立，则接受候选点 $x_{k+1}$，算法进入下一次迭代 $k+1$。如果它不成立，则步长 $t$ 乘以一个因子 $\\beta \\in (0, 1)$（即 $t \\leftarrow \\beta t$），计算一个新的候选点 $x_{k+1}$，并重新检查条件。每次减小 $t$ 都计为一个回溯步骤。这个过程保证了对于足够小的 $t$，该条件最终会得到满足。\n\n整个算法按以下步骤进行，总共 $T$ 次迭代：\n1. 初始化 $x_0 = 0$ 和 `total_backtracks` $= 0$。\n2. 对于 $k = 0, 1, \\dots, T-1$：\n    a. 设置试验步长 $t \\leftarrow t_{\\text{fixed}}$。\n    b. 计算梯度 $\\nabla f(x_k) = A^\\top(Ax_k - y)$。\n    c. 计算 $f(x_k) = \\frac{1}{2}\\|Ax_k - y\\|_2^2$。\n    d. 进入线搜索循环：\n        i. 计算候选点 $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$。\n        ii. 计算 $f(x_{k+1}) = \\frac{1}{2}\\|Ax_{k+1} - y\\|_2^2$。\n        iii. 构建接受条件的右侧：`RHS` $= f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2$。\n        iv. 如果 $f(x_{k+1}) \\le \\text{RHS} + \\varepsilon$，接受 $x_{k+1}$ 并退出线搜索循环。\n        v. 否则，缩小步长 $t \\leftarrow \\beta t$，增加 `total_backtracks`，并从步骤 (i) 重复。\n    e. 更新 $x_k \\leftarrow x_{k+1}$。\n3. 经过 $T$ 次迭代后，计算最终目标值 $F(x_T) = f(x_T) + g(x_T)$。\n4. 返回 $F(x_T)$ 和 `total_backtracks`。\n\n对指定的三个测试用例分别执行此过程。最终的目标值四舍五入到 $6$ 位小数。",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the proximal gradient solver.\n    \"\"\"\n\n    TestCase = namedtuple('TestCase', ['A', 'y', 'lambda_', 't_fixed', 'beta', 'T'])\n\n    test_cases = [\n        # Test case 1\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=0.05,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 2\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=10.0,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 3\n        TestCase(\n            A=np.array([[1, 2, 0, 0], [0, 1, 1, 0], [0, 0, 1, 3]]),\n            y=np.array([0, 1, -1]),\n            lambda_=0.05,\n            t_fixed=5.0,\n            beta=0.5,\n            T=120\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        final_F, total_backtracks = proximal_gradient_solver(\n            A=case.A,\n            y=case.y,\n            lambda_=case.lambda_,\n            t_fixed=case.t_fixed,\n            beta=case.beta,\n            T=case.T\n        )\n        results.append(f\"{final_F:.6f}\")\n        results.append(str(total_backtracks))\n\n    print(f\"[{','.join(results)}]\")\n\ndef soft_threshold(z, alpha):\n    \"\"\"\n    Soft-thresholding operator for the L1 norm's proximal operator.\n    S_alpha(z) = sign(z) * max(|z| - alpha, 0)\n    \"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\ndef f(x, A, y):\n    \"\"\"\n    Computes the smooth part of the objective function: f(x) = 1/2 * ||Ax - y||_2^2\n    \"\"\"\n    residual = A @ x - y\n    return 0.5 * np.dot(residual, residual)\n\ndef g(x, lambda_):\n    \"\"\"\n    Computes the non-smooth part of the objective function: g(x) = lambda * ||x||_1\n    \"\"\"\n    return lambda_ * np.linalg.norm(x, 1)\n\ndef F(x, A, y, lambda_):\n    \"\"\"\n    Computes the total objective function F(x) = f(x) + g(x).\n    \"\"\"\n    return f(x, A, y) + g(x, lambda_)\n\ndef proximal_gradient_solver(A, y, lambda_, t_fixed, beta, T):\n    \"\"\"\n    Implements the Proximal Gradient Method with a hybrid fixed-step and backtracking\n    line search strategy.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    total_backtracks = 0\n    epsilon = 1e-12\n\n    for _ in range(T):\n        t = t_fixed\n        \n        # Pre-compute gradient and f(x) for the current iterate x\n        residual = A @ x - y\n        grad_f_x = A.T @ residual\n        f_x = 0.5 * np.dot(residual, residual)\n\n        while True:\n            # Candidate update step using the proximal operator\n            z = x - t * grad_f_x\n            x_next = soft_threshold(z, lambda_ * t)\n\n            # Check the sufficient decrease condition (backtracking line search)\n            # f(x_next) = f(x) + grad_f(x)^T(x_next - x) + (1/(2t))||x_next - x||^2\n            f_x_next = f(x_next, A, y)\n            \n            # The right-hand side of the inequality\n            diff_x = x_next - x\n            rhs = f_x + np.dot(grad_f_x, diff_x) + (0.5 / t) * np.dot(diff_x, diff_x)\n\n            if f_x_next = rhs + epsilon:\n                x = x_next\n                break  # Step size t is accepted\n            else:\n                # Shrink step size and try again\n                t = beta * t\n                total_backtracks += 1\n    \n    final_F_val = F(x, A, y, lambda_)\n    return final_F_val, total_backtracks\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在掌握了回溯线搜索的基本实现后，下一个练习将引导您深入探索其背后的数学原理。这个练习要求您针对具有二次光滑项的目标函数，推导接受准则的等价形式，从而揭示它与函数在更新方向上的曲率之间的直接联系。通过这项实践，您将不仅知其然，更知其所以然，并学会构建一个精确的诊断工具来预测步长是否会被接受。",
            "id": "3432788",
            "problem": "要求您设计并分析一种带回溯线搜索的近端梯度法，用于解决压缩感知和稀疏优化中遇到的一种凸复合目标函数。光滑数据拟合项是一个二次型，正则化项是促进稀疏性的绝对值范数。您的目标是：从第一性原理出发，推断线搜索为何及何时会接受其首次尝试的步长（即，每次迭代的初始 Lipschitz 估计值保持不变）；将该接受条件与沿发现的稀疏结构上的曲率联系起来；并提出一个可实现的诊断指标，用以预测在支撑集发生变化后，何时会恢复首次尝试步长的接受。\n\n考虑无约束问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\n其中光滑项是二次型\n$$\nf(x) \\equiv \\frac{1}{2} \\lVert A x - b \\rVert_2^2,\n$$\n其中给定数据矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和向量 $b \\in \\mathbb{R}^m$，且 $\\lambda  0$ 是一个正则化参数。光滑项的梯度是 $\\nabla f(x) = A^\\top(Ax - b)$，并且该梯度是全局 Lipschitz 连续的，其 Lipschitz 常数等于 $A$ 的谱范数的平方。$\\lambda \\lVert x \\rVert_1$ 的近端算子是坐标级软阈值算子。\n\n您将使用以下形式的带回溯线搜索的近端梯度法。在第 $k$ 次迭代中，给定当前迭代点 $x_k$ 和当前 Lipschitz 估计值 $L_k  0$，尝试使用步长 $1 / L_k$ 进行候选更新，\n$$\ny_k \\equiv \\operatorname{soft}\\Big(x_k - \\frac{1}{L_k} \\nabla f(x_k), \\, \\frac{\\lambda}{L_k}\\Big),\n$$\n当且仅当对于光滑部分，标准充分下降不等式在回溯所用的二次上界模型中对相同的 $L_k$ 成立时，才接受这首次尝试。否则，增加 Lipschitz 估计值（例如，乘以一个大于 1 的因子）并重试，直到不等式成立。第 $k$ 次迭代的“单位步长接受”意味着使用初始 $L_k$ 的首次尝试被接受（因此该次迭代的 $L_k$ 保持不变）。\n\n从近端梯度回溯中使用的光滑部分充分下降不等式的核心定义出发，并利用 $f(x)$ 的显式二次型形式，推导出一个专门针对此二次模型的充分必要接受准则，该准则仅依赖于尝试的更新方向。解释该准则如何定量地将接受的单位步长与沿近端梯度更新所决定的方向上遇到的曲率联系起来，以及这个方向如何受到所发现的稀疏支撑集的约束。\n\n利用此原理，提出一个逐次迭代的标量诊断指标，该指标仅可由数据 $(A,b)$、当前迭代点 $x_k$、当前梯度 $\\nabla f(x_k)$ 以及使用初始 $L_k$ （在应用任何回溯增加之前）产生的首次尝试更新 $y_k$ 计算得出。该诊断指标必须是一个实数形式，通过与 $L_k$ 比较来预测在第 $k$ 次迭代时单位步长是否会被接受；此外，在第 $t$ 次迭代检测到任何支撑集变化后（即，$x_{t+1}$ 的支撑集与 $x_t$ 的支撑集不同），必须使用您的诊断指标来预测单位步长接受恢复的第一个后续迭代索引 $r  t$。形式上，将第 $t$ 次迭代的“支撑集变化”事件定义为\n$$\n\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t),\n$$\n其中 $\\operatorname{supp}(z) \\equiv \\{i : z_i \\neq 0\\}$。将此事件的“恢复索引” $r$ 定义为满足线搜索在第 $j$ 次迭代时接受其首次尝试的最小 $j  t$。您的诊断指标必须仅使用每次迭代计算的诊断值和每次迭代的初始 $L_k$ 来预测此恢复索引。\n\n您必须实现一个程序，对一组指定的测试用例执行以下操作：\n- 如下文所述，生成一个合成的压缩感知实例 $(A,b)$。\n- 运行带回溯的近端梯度法，迭代固定次数。\n- 在每次迭代中，记录首次尝试是否被接受（单位步长接受），以及支撑集相对于前一个迭代点是否发生变化。\n- 在每次迭代中，仅使用首次尝试的更新来计算您提出的诊断指标。\n- 对于每个支撑集变化事件，计算真实的恢复索引（基于实际的单位步长接受结果）和预测的恢复索引（仅基于逐次迭代的诊断指标和那些迭代的初始 $L_k$）。\n- 对每个测试用例，输出一个整数，等于在固定迭代预算内观察到的所有支撑集变化事件中，真实恢复索引与预测恢复索引之间的错配总数。如果某个测试用例中没有支撑集变化，则该计数应为 $0$。\n\n您可以假设并在推导中明确使用的基本出发点是：\n- 复合结构 $F(x) = f(x) + \\lambda \\lVert x \\rVert_1$，$\\lambda \\lVert x \\rVert_1$ 的近端算子，以及近端梯度迭代的定义。\n- 用于近端梯度法的回溯线搜索中的标准充分下降不等式，该不等式基于 $\\nabla f$ 的 Lipschitz 连续性。\n- 对于二次型 $f(x)$，精确的差分恒等式 $f(x + h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2$。\n\n测试套件。使用以下测试用例，每个用例都由整数 $(m,n)$、真实向量的稀疏度 $k_{\\text{true}}$、迭代预算 $K$、初始 Lipschitz 估计 $L_0$、回溯乘子 $\\eta  1$ 和随机种子完全指定。在所有情况下，初始化 $x_0 = 0$，并通过均匀随机选择 $k_{\\text{true}}$ 个索引，将这些条目设置为在 $[1,2]$ 上均匀分布的独立值并赋予随机符号，来生成一个真实的稀疏向量 $x^\\star$。对于无噪声数据，设置 $b = A x^\\star$。对于前两个用例，使用固定的正则化参数 $\\lambda$；对于第三个用例，将 $\\lambda$ 自适应地设置为一个可计算量的已知倍数，以引出全零解的边界情况。在所有情况下，最大迭代次数是固定的，您必须按上述说明报告错配计数。测试用例如下：\n\n- 测试用例 1（良态高斯设计）：\n  - $m = 40$, $n = 80$, $k_{\\text{true}} = 5$, $K = 120$, $L_0 = 0.3$, $\\eta = 2.0$, 种子 $= 1$, $\\lambda = 0.05$。\n  - 用独立的标准正态分布条目生成 $A$，然后将每列归一化为单位欧几里得范数。\n\n- 测试用例 2（强相关设计）：\n  - $m = 40$, $n = 60$, $k_{\\text{true}} = 8$, $K = 150$, $L_0 = 0.2$, $\\eta = 2.0$, 种子 $= 7$, $\\lambda = 0.03$。\n  - 通过 $\\Sigma_{ij} = \\rho^{\\lvert i - j \\rvert}$（其中 $1 \\le i,j \\le n$）生成相关参数为 $\\rho = 0.9$ 的 Toeplitz 协方差矩阵。抽取一个原始矩阵 $Z \\in \\mathbb{R}^{m \\times n}$，其条目独立服从 $\\mathcal{N}(0,1)$ 分布，形成 $A_{\\text{raw}} = Z \\, C$，其中 $C$ 是 $\\Sigma$ 的任意矩阵平方根（例如，Cholesky 因子），然后将 $A_{\\text{raw}}$ 的每列归一化为单位欧几里得范数以获得 $A$。\n\n- 测试用例 3（零解的边界情况）：\n  - $m = 30$, $n = 50$, $k_{\\text{true}} = 5$, $K = 60$, $L_0 = 0.5$, $\\eta = 2.0$, 种子 $= 11$，并设置 $\\lambda = 1.05 \\, \\lVert A^\\top b \\rVert_\\infty$。\n  - 按照测试用例 1 的方式生成 $A$（单位范数化列的高斯矩阵），并构造 $b = A x^\\star$。然后按指定计算 $\\lambda$。\n\n角度单位不适用。此问题中没有物理单位。\n\n最终程序输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内容为每个测试用例的错配计数。例如：\n\"[c1,c2,c3]\"\n其中 c1、c2 和 c3 分别是测试用例 1、2 和 3 的错配计数的整数值。不应产生任何其他输出。",
            "solution": "该问题要求分析应用于 LASSO 目标函数的近端梯度法的回溯线搜索条件，推导一个用于步长接受的诊断指标，并实现一个程序来验证该诊断指标的预测能力。\n\n优化问题是\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ 是光滑部分，$\\lambda \\lVert x \\rVert_1$ 是非光滑正则化项。近端梯度法通过以下更新规则生成一系列迭代点 $x_k$：\n$$\nx_{k+1} = \\operatorname{prox}_{\\alpha_k \\lambda \\lVert \\cdot \\rVert_1}(x_k - \\alpha_k \\nabla f(x_k)),\n$$\n其中 $\\alpha_k  0$ 是步长。$\\ell_1$-范数的近端算子是软阈值算子，$\\operatorname{soft}(z, \\tau)_i = \\operatorname{sgn}(z_i) \\max(\\lvert z_i \\rvert - \\tau, 0)$。步长 $\\alpha_k$ 由回溯线搜索确定。\n\n在第 $k$ 次迭代中，给定当前迭代点 $x_k$，我们从 $\\nabla f$ 的局部 Lipschitz 常数的一个初始猜测开始，记为 $L_{k, \\text{init}}$。然后我们尝试步长 $\\alpha_k = 1/L$，其中 $L=L_{k, \\text{init}}$。候选更新为\n$$\ny_k = \\operatorname{soft}\\Big(x_k - \\frac{1}{L} \\nabla f(x_k), \\, \\frac{\\lambda}{L}\\Big).\n$$\n如果此候选点满足光滑部分 $f$ 的充分下降条件，则它被接受。该条件确保了在 $x_k$ 处 majorize $f$ 的二次模型，在新的点 $y_k$ 处是 $f$ 的一个有效上界：\n$$\nf(y_k) \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\n如果该不等式对 $L = L_{k, \\text{init}}$ 成立，则该步长是一个“单位步长”（或首次尝试被接受）。新的迭代点变为 $x_{k+1} = y_k$，并且该次迭代的线搜索终止。如果不等式不成立，我们增加 $L$（例如，对于某个 $\\eta  1$，$L \\leftarrow \\eta L$），并重复计算新的候选点 $y_k$ 并检查不等式的过程，直到它被满足。\n\n**接受准则的推导**\n\n分析的核心在于将此不等式专门应用于我们特定的二次函数 $f(x)$。问题为二次函数提供了以下精确恒等式：\n$$\nf(x+h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2.\n$$\n让我们通过设置 $x = x_k$ 和位移 $h = y_k - x_k$ 来应用此恒等式。充分下降不等式的左侧 $f(y_k)$ 可以重写为：\n$$\nf(y_k) = f(x_k + (y_k - x_k)) = f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2.\n$$\n将此表达式代入充分下降不等式，得到：\n$$\nf(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2 \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\n在消去两侧的公因子 $f(x_k)$ 和 $\\nabla f(x_k)^\\top (y_k - x_k)$ 并乘以 $2$ 后，我们得到一个简化的等价不等式：\n$$\n\\lVert A(y_k - x_k) \\rVert_2^2 \\le L \\lVert y_k - x_k \\rVert_2^2.\n$$\n这是线搜索接受步长 $\\alpha_k = 1/L$ 的一个充分必要条件。令 $d_k = y_k - x_k$ 为更新向量。如果 $d_k = 0$，不等式 $0 \\le 0$ 平凡成立。如果 $d_k \\neq 0$，该条件可以表示为：\n$$\n\\frac{\\lVert A d_k \\rVert_2^2}{\\lVert d_k \\rVert_2^2} \\le L.\n$$\n这表明，线搜索接受步长的充要条件是矩阵 $A^\\top A$（$f$ 的 Hessian 矩阵）关于更新向量 $d_k$ 的瑞利商不大于 Lipschitz 估计值 $L$。\n\n**与曲率和稀疏支撑集的关系**\n\n项 $\\lVert A d_k \\rVert_2^2$ 可以写为 $(A d_k)^\\top (A d_k) = d_k^\\top A^\\top A d_k$。由于 Hessian 矩阵是 $\\nabla^2 f(x) = A^\\top A$，该项表示 $f$ 沿方向 $d_k$ 的二阶变化，即曲率。因此，接受条件表明，沿所选更新方向的有效曲率必须以 $L$ 为界。\n\n更新向量 $d_k = y_k - x_k$ 由近端梯度步确定。软阈值操作确保 $y_k$ 是稀疏的。因此，$d_k$ 也是稀疏的，其非零项通常局限于 $y_k$ 的支撑集内。当算法的活动集（迭代点的支撑集）稳定时，向量 $d_k$ 被限制在一个特定的子空间内。回溯线搜索会调整 $L$，使其成为该子空间内曲率的一个上界。$x_k$ 支撑集的变化意味着 $d_k$ 探测了一个新的方向或子空间，这个新方向或子空间可能具有更高的曲率（即更大的瑞利商）。如果这个新曲率超过了当前的估计值 $L$，线搜索将会失败，从而强制增加 $L$，直到它能恰当地为新活动子空间中的曲率提供上界。\n\n**提议的诊断指标和预测**\n\n基于推导出的准则，我们可以定义一个标量诊断指标。在第 $k$ 次迭代中，令 $L_{k, \\text{init}}$ 为首次尝试使用的初始 Lipschitz 估计值。首次尝试的候选更新为 $y_{k, \\text{init}} = \\operatorname{soft}(x_k - \\frac{1}{L_{k, \\text{init}}} \\nabla f(x_k), \\frac{\\lambda}{L_{k, \\text{init}}})$。令 $d_{k, \\text{init}} = y_{k, \\text{init}} - x_k$。诊断指标是瑞利商：\n$$\nD_k = \\begin{cases} \\frac{\\lVert A d_{k, \\text{init}} \\rVert_2^2}{\\lVert d_{k, \\text{init}} \\rVert_2^2}  \\text{if } d_{k, \\text{init}} \\neq 0 \\\\ 0  \\text{if } d_{k, \\text{init}} = 0 \\end{cases}\n$$\n在第 $k$ 次迭代中，一个“单位步长”被接受，当且仅当 $D_k \\le L_{k, \\text{init}}$。这不是一个启发式的预测，而是推导出的充分必要条件的直接结果。\n\n第 $t$ 次迭代的“支撑集变化”事件是 $\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t)$。真实的恢复索引是 $r = \\min \\{j  t \\mid \\text{在第 } j \\text{ 步的首次尝试被接受}\\}$。我们预测的恢复索引 $\\hat{r}$ 是通过在每个后续迭代中检查我们的诊断条件来找到的：$\\hat{r} = \\min \\{j  t \\mid D_j \\le L_{j, \\text{init}}\\}$。\n\n由于条件 $D_j \\le L_{j, \\text{init}}$ 正是首次尝试被接受的条件，只要计算是精确的，预测的恢复索引 $\\hat{r}$ 在任何情况下都必须与真实的恢复索引 $r$ 相同。因此，真实索引和预测索引之间的错配数量预期为 $0$。该实现可作为此分析结论的验证。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, toeplitz\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef run_one_case(m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params=None):\n    \"\"\"\n    Runs a single test case for the proximal gradient algorithm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # --- 1. Generate synthetic data ---\n    # Generate ground-truth sparse vector x_star\n    x_star = np.zeros(n)\n    support_star_indices = rng.choice(n, k_true, replace=False)\n    x_star[support_star_indices] = rng.uniform(1, 2, k_true)\n    x_star[support_star_indices] *= rng.choice([-1, 1], k_true)\n\n    # Generate matrix A\n    if A_gen_mode == 'gaussian':\n        A = rng.standard_normal(size=(m, n))\n        col_norms = np.linalg.norm(A, axis=0)\n        A = A / col_norms\n    elif A_gen_mode == 'correlated':\n        rho = 0.9\n        Sigma = toeplitz(rho**np.arange(n))\n        # Add a small identity matrix for stability before Cholesky\n        C = cholesky(Sigma + 1e-10 * np.eye(n), lower=True)\n        Z = rng.standard_normal(size=(m, n))\n        A_raw = Z @ C.T\n        col_norms = np.linalg.norm(A_raw, axis=0)\n        A = A_raw / col_norms\n\n    # Generate measurement vector b\n    b = A @ x_star\n\n    # Set lambda if adaptive\n    if lambda_mode_params == 'adaptive':\n        lambda_val = 1.05 * np.max(np.abs(A.T @ b))\n\n    # --- 2. Run Proximal Gradient with Backtracking ---\n    x_k = np.zeros(n)\n    L_val = L0\n    \n    is_unit_step_accepted = []\n    support_changed = []\n    diagnostics = []\n    L_inits = []\n\n    for k in range(K):\n        # Store support of x_k for later comparison\n        support_k = set(np.where(x_k != 0)[0])\n        \n        # --- At iteration k, perform one step ---\n        grad_fxk = A.T @ (A @ x_k - b)\n        \n        # This is the initial Lipschitz estimate for this iteration\n        L_k_init = L_val\n        L_inits.append(L_k_init)\n\n        # Compute first-attempt update and diagnostic\n        y_k_init = soft_threshold(x_k - (1/L_k_init) * grad_fxk, lambda_val / L_k_init)\n        d_k_init = y_k_init - x_k\n        \n        if np.linalg.norm(d_k_init)  1e-12:\n            D_k = 0.0\n        else:\n            Adk_norm_sq = np.linalg.norm(A @ d_k_init)**2\n            dk_norm_sq = np.linalg.norm(d_k_init)**2\n            D_k = Adk_norm_sq / dk_norm_sq\n        diagnostics.append(D_k)\n\n        # Backtracking line search\n        L_attempt = L_k_init\n        num_backtracks = 0\n        while True:\n            y_k = soft_threshold(x_k - (1/L_attempt) * grad_fxk, lambda_val / L_attempt)\n            d_k = y_k - x_k\n\n            if np.linalg.norm(d_k)  1e-12:\n                # If step is zero, condition is trivially satisfied.\n                break \n\n            lhs = np.linalg.norm(A @ d_k)**2\n            rhs = L_attempt * np.linalg.norm(d_k)**2\n            \n            if lhs = rhs:\n                break\n            \n            L_attempt *= eta\n            num_backtracks += 1\n\n        # Accept the step\n        is_unit_step_accepted.append(num_backtracks == 0)\n        x_k = y_k  # Update x_k to x_{k+1}\n        L_val = L_attempt # Update L for next iteration\n\n        # Check for support change\n        support_k_plus_1 = set(np.where(x_k != 0)[0])\n        support_changed.append(support_k != support_k_plus_1)\n\n    # --- 3. Post-process to count mismatches ---\n    mismatch_count = 0\n    support_change_indices = [t for t, changed in enumerate(support_changed) if changed]\n\n    for t in support_change_indices:\n        r_true = -1\n        for j in range(t + 1, K):\n            if is_unit_step_accepted[j]:\n                r_true = j\n                break\n        \n        r_pred = -1\n        for j in range(t + 1, K):\n            # The prediction check: D_j = L_{j,init}\n            if diagnostics[j] = L_inits[j]:\n                r_pred = j\n                break\n        \n        if r_true != r_pred:\n            mismatch_count += 1\n            \n    return mismatch_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params)\n        (40, 80, 5, 120, 0.3, 2.0, 1, 0.05, 'gaussian', None),\n        (40, 60, 8, 150, 0.2, 2.0, 7, 0.03, 'correlated', None),\n        (30, 50, 5, 60, 0.5, 2.0, 11, None, 'gaussian', 'adaptive')\n    ]\n\n    results = []\n    for params in test_cases:\n        mismatch_count = run_one_case(*params)\n        results.append(mismatch_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "最后一个练习将展示回溯线搜索框架的通用性，将其应用于更高级的优化算法——近端牛顿法。与标准的近端梯度步不同，近端牛顿法使用目标函数的二阶信息来生成更有效的搜索方向。您的任务是实现一个线搜索过程来确定沿牛顿方向的最佳步长，并比较不同二阶近似（即Hessian近似）对算法性能的影响。",
            "id": "3432780",
            "problem": "考虑压缩感知和稀疏优化中的一个基本复合凸优化问题，其定义为在 $x \\in \\mathbb{R}^n$ 上最小化函数 $F(x)$，其中\n$$\nF(x) = g(x) + \\lambda \\lVert x \\rVert_1, \\quad g(x) = \\tfrac{1}{2} \\lVert A x - y \\rVert_2^2,\n$$\n其中数据矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，数据向量为 $y \\in \\mathbb{R}^m$，正则化参数为 $\\lambda  0$。光滑部分 $g(x)$ 的梯度为 $\\nabla g(x) = A^\\top (A x - y)$，Hessian矩阵为 $\\nabla^2 g(x) = A^\\top A$。近端牛顿法通过最小化一个局部二次加非光滑模型来生成一个试验方向 $d \\in \\mathbb{R}^n$\n$$\nm_B(d; x) = \\nabla g(x)^\\top d + \\tfrac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1 - \\lambda \\lVert x \\rVert_1,\n$$\n其中 $B \\in \\mathbb{R}^{n \\times n}$ 是一个近似 $\\nabla^2 g(x)$ 的对称正定矩阵。预测的模型下降量为\n$$\n\\Delta_B(x; d) = m_B(0; x) - m_B(d; x) = - \\nabla g(x)^\\top d - \\tfrac{1}{2} d^\\top B d + \\lambda \\left(\\lVert x \\rVert_1 - \\lVert x + d \\rVert_1\\right).\n$$\n使用参数 $c \\in (0,1)$ 和 $\\beta \\in (0,1)$ 的回溯线搜索选择满足充分下降条件的最大 $t \\in \\{1, \\beta, \\beta^2, \\dots \\}$\n$$\nF(x + t d) \\le F(x) - c \\, t \\, \\Delta_B(x; d)\n$$\n成立，其中 $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$。假设 $B$ 从三个由 $A^\\top A$ 构造的对称正定选项中选择：单位矩阵的一个标量倍 $B = L I$，其中 $L$ 等于 $A^\\top A$ 的最大特征值；一个对角矩阵 $B = \\mathrm{diag}(A^\\top A)$；以及单位矩阵的一个标量倍 $B = s I$，其中 $s = \\tfrac{1}{n} \\mathrm{trace}(A^\\top A)$。对于每个这样的 $B$，试验方向 $d$ 定义为 $m_B(d; x)$ 的精确最小化子。\n\n从一个给定的初始点 $x \\in \\mathbb{R}^n$ 出发，对每个候选 $B$ 执行以下操作：\n- 计算使 $m_B(d; x)$ 最小化的模型最小化方向 $d \\in \\mathbb{R}^n$。\n- 计算预测的模型下降量 $\\Delta_B(x; d)$。\n- 如果 $\\Delta_B(x; d) \\le 0$，则将接受的步长定义为 $t = 0$，新点为 $x$ 本身。否则，使用参数 $c$ 和 $\\beta$ 执行回溯，以找到满足充分下降条件的最大 $t \\in \\{1, \\beta, \\beta^2, \\dots\\}$；如果 $t$ 低于某个数值阈值，则终止回溯，此时设 $t = 0$，新点为 $x$。\n- 记录得到的目标函数值 $F(x + t d)$。\n\n在三个候选中，选择能达到最小接受函数值 $F(x + t d)$ 的那个。如果接受的函数值相同，则优先选择接受步长 $t$ 较大的那个。如果仍然存在平局，则根据排序 $B = L I$ 为索引 $1$，$B = \\mathrm{diag}(A^\\top A)$ 为索引 $2$，$B = s I$ 为索引 $3$，选择索引最小的那个。\n\n你的任务是实现一个程序，对下面定义的每个测试用例，返回一个包含三个条目的列表：选定的索引 $k \\in \\{1,2,3\\}$、接受的步长 $t \\in \\mathbb{R}$ 和接受的函数值 $F(x + t d) \\in \\mathbb{R}$。所有浮点数输出必须四舍五入到恰好六位小数。\n\n仅使用纯粹的数学和逻辑计算。不涉及物理单位。不使用角度。不使用百分比。\n\n测试套件：\n- 情况1（理想路径）：\n  - $A = \\begin{bmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\\\ 2  -1  0 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.1$,\n  - $x = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n- 情况2（强稀疏性压力下的边界情况）：\n  - $A$ 同情况1,\n  - $y$ 同情况1,\n  - $\\lambda = 5.0$,\n  - $x = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n- 情况3（病态几何）：\n  - $A = \\begin{bmatrix}\n  10  0  0  0 \\\\\n  0  1  0  0 \\\\\n  0  0  0.1  0 \\\\\n  0  0  0  0.01 \\\\\n  10  1  0.1  0.01\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.05$,\n  - $x = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身是对应一个测试用例的 $[k, t, F]$ 形式的列表。例如，输出必须具有以下形式\n$[\\,[k_1,t_1,F_1],[k_2,t_2,F_2],[k_3,t_3,F_3]\\,]$,\n其中每个 $t_i$ 和 $F_i$ 都四舍五入到恰好六位小数，并且打印输出中没有空格。",
            "solution": "用户提供了一个来自数值优化领域的问题，具体涉及使用近端牛顿法解决在压缩感知和稀疏优化中普遍存在的复合凸优化问题。该问题是良定的，有科学依据，并包含获得唯一解所需的所有必要信息。因此，它被认为是有效的。\n\n核心任务是解决以下优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\quad \\text{where} \\quad F(x) = g(x) + \\lambda \\lVert x \\rVert_1\n$$\n此处，$g(x) = \\frac{1}{2} \\lVert Ax - y \\rVert_2^2$ 是一个代表数据保真项的光滑凸函数，而 $\\lambda \\lVert x \\rVert_1$ 是一个促进解 $x$ 稀疏性的非光滑凸正则化项。光滑部分的梯度为 $\\nabla g(x) = A^\\top (Ax - y)$。\n\n该问题指定使用近端牛顿法。此方法在点 $x$ 处通过最小化函数 $F(x+d)$ 的一个局部模型来生成搜索方向 $d$。该模型 $m_B(d; x)$ 的构造方法是，用 $g(x+d)$ 在 $x$ 点的二阶泰勒近似替换它，同时保持非光滑项 $\\lambda \\lVert x+d \\rVert_1$ 不变。$g(x)$ 的Hessian矩阵 $\\nabla^2 g(x) = A^\\top A$ 由一个对称正定矩阵 $B$ 近似。要最小化的模型由下式给出：\n$$\nm_B(d; x) = g(x) + \\nabla g(x)^\\top d + \\frac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1\n$$\n关于 $d$ 最小化 $m_B(d; x)$ 等价于最小化\n$$\nd = \\arg\\min_{d \\in \\mathbb{R}^n} \\left\\{ \\nabla g(x)^\\top d + \\frac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1 \\right\\}\n$$\n这是一个凸优化子问题。我们可以通过找到最小化相应 $u$ 的目标函数的唯一点 $u = x+d$ 来求解它：\n$$\nu = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\nabla g(x)^\\top (u-x) + \\frac{1}{2} (u-x)^\\top B (u-x) + \\lambda \\lVert u \\rVert_1 \\right\\}\n$$\n通过配方法，这可以表示为一个近端算子问题。解的形式取决于矩阵 $B$ 的结构。\n\n**情况1：各向同性Hessian近似 ($B = \\gamma I$)**\n当 $B$ 是单位矩阵的标量倍，即 $B = \\gamma I$（对于某个标量 $\\gamma  0$），子问题简化为：\n$$\nu = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\frac{\\gamma}{2} \\left\\lVert u - \\left(x - \\frac{1}{\\gamma} \\nabla g(x)\\right) \\right\\rVert_2^2 + \\lambda \\lVert u \\rVert_1 \\right\\}\n$$\n解由 $\\ell_1$-范数的近端算子给出，即软阈值算子 $S_{\\kappa}(\\cdot)$：\n$$\nu = S_{\\lambda/\\gamma}\\left(x - \\frac{1}{\\gamma} \\nabla g(x)\\right)\n$$\n其中 $(S_\\kappa(z))_i = \\mathrm{sign}(z_i) \\max(|z_i| - \\kappa, 0)$。搜索方向则为 $d = u - x$。这种情况适用于为 $B$ 指定的两个选项：\n- **索引 1**：$B = L I$，其中 $L$ 是 $A^\\top A$ 的最大特征值。此时，$\\gamma = L$。\n- **索引 3**：$B = s I$，其中 $s = \\frac{1}{n} \\mathrm{trace}(A^\\top A)$。此时，$\\gamma = s$。\n\n**情况2：对角Hessian近似 ($B = \\mathrm{diag}(A^\\top A)$)**\n当 $B$ 是对角矩阵时，子问题按坐标解耦：\n$$\nu_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\frac{B_{ii}}{2} \\left( u_i - \\left(x_i - \\frac{(\\nabla g(x))_i}{B_{ii}}\\right) \\right)^2 + \\lambda |u_i| \\right\\}\n$$\n每个坐标的解同样由软阈值给出：\n$$\nu_i = S_{\\lambda/B_{ii}}\\left(x_i - \\frac{(\\nabla g(x))_i}{B_{ii}}\\right)\n$$\n搜索方向为 $d = u - x$。这对应于**索引 2** 的选择：$B = \\mathrm{diag}(A^\\top A)$。\n\n一旦对于给定的 $B$ 计算出方向 $d$，我们通过回溯线搜索来确定步长 $t$。首先，我们计算采取步长 $d$ 所带来的模型值的预测下降量：\n$$\n\\Delta_B(x; d) = m_B(0; x) - m_B(d; x) = -\\nabla g(x)^\\top d - \\frac{1}{2} d^\\top B d + \\lambda (\\lVert x \\rVert_1 - \\lVert x + d \\rVert_1)\n$$\n如果 $\\Delta_B(x; d) \\le 0$，则该方向不是模型的下降方向，我们采取零步长，即 $t=0$。否则，我们寻找满足充分下降条件（一种Armijo型条件）的最大步长 $t \\in \\{1, \\beta, \\beta^2, \\dots\\}$：\n$$\nF(x + t d) \\le F(x) - c \\, t \\, \\Delta_B(x; d)\n$$\n其中 $c \\in (0,1)$ 是一个给定常数。如果 $t$ 低于数值精度阈值，则过程终止，此时设 $t$ 为 $0$。\n\n对于每个测试用例，整体算法按如下方式进行：\n1.  用给定的 $A, y, \\lambda, x, c, \\beta$ 进行初始化。\n2.  计算矩阵 $A^\\top A$。\n3.  对于三个候选矩阵 $B_k$ 中的每一个（$k=1,2,3$）：\n    a. 构造 $B_k$。对于 $k=1$，计算 $A^\\top A$ 的最大特征值。对于 $k=2$，取 $A^\\top A$ 的对角线元素。对于 $k=3$，计算 $A^\\top A$ 的归一化迹。\n    b. 计算梯度 $\\nabla g(x) = A^\\top(Ax-y)$。\n    c. 通过求解与 $B_k$ 关联的近端子问题来计算搜索方向 $d_k$。\n    d. 计算预测下降量 $\\Delta_{B_k}(x;d_k)$。\n    e. 执行回溯线搜索以找到接受的步长 $t_k$。\n    f. 计算新的函数值 $F(x+t_k d_k)$。\n4. 比较 $k=1,2,3$ 的结果 $(t_k, F(x+t_k d_k))$。根据指定的标准选择最佳索引 $k^*$：\n    a. 产生最小最终函数值的那个。\n    b. 如果出现平局，选择接受步长 $t_k$ 最大的那个。\n    c. 如果仍然平局，选择索引 $k$ 最小的那个。\n5. 该测试用例的最终输出是三元组 $[k^*, t_{k^*}, F(x+t_{k^*}d_{k^*})]$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the proximal Newton step selection problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        (\n            np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]], dtype=float),\n            np.array([3, -1, 2, 0], dtype=float),\n            0.1,\n            np.array([0, 0, 0], dtype=float),\n            0.1,\n            0.5\n        ),\n        # Case 2\n        (\n            np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]], dtype=float),\n            np.array([3, -1, 2, 0], dtype=float),\n            5.0,\n            np.array([1, -2, 3], dtype=float),\n            0.1,\n            0.5\n        ),\n        # Case 3\n        (\n            np.array([\n                [10, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 0.1, 0],\n                [0, 0, 0, 0.01],\n                [10, 1, 0.1, 0.01]\n            ], dtype=float),\n            np.array([1, 0.1, -0.1, 0.05, 0], dtype=float),\n            0.05,\n            np.array([0.5, -0.5, 0.5, -0.5], dtype=float),\n            0.1,\n            0.5\n        ),\n    ]\n\n    all_results = []\n    \n    # Helper functions\n    def soft_threshold(z, kappa):\n        if np.isscalar(kappa):\n            return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n        else: # Element-wise thresholding\n            return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n\n    def F_func(x_vec, A, y, lam):\n        g = 0.5 * np.linalg.norm(A @ x_vec - y)**2\n        h = lam * np.linalg.norm(x_vec, 1)\n        return g + h\n\n    for A, y, lam, x, c, beta in test_cases:\n        m, n = A.shape\n        ATA = A.T @ A\n        \n        # B candidate definitions\n        # B_1: L*I\n        L = np.linalg.eigvalsh(ATA).max()\n        B1 = L * np.identity(n)\n        \n        # B_2: diag(A^T A)\n        diag_ATA = np.diag(ATA)\n        # Assuming positive definite, so no zero diagonal entries\n        B2 = np.diag(diag_ATA)\n\n        # B_3: s*I\n        s = np.trace(ATA) / n\n        B3 = s * np.identity(n)\n\n        B_candidates = [(1, B1, 'isotropic', L), (2, B2, 'diagonal', diag_ATA), (3, B3, 'isotropic', s)]\n\n        grad_g_x = A.T @ (A @ x - y)\n        F_current = F_func(x, A, y, lam)\n        \n        case_results = []\n\n        for k, B, b_type, b_param in B_candidates:\n            d = np.zeros(n)\n            # Compute direction d\n            if b_type == 'isotropic':\n                gamma = b_param\n                u = soft_threshold(x - (1/gamma) * grad_g_x, lam / gamma)\n                d = u - x\n            elif b_type == 'diagonal':\n                # b_param is the diagonal vector\n                u = soft_threshold(x - grad_g_x / b_param, lam / b_param)\n                d = u - x\n\n            # Compute predicted reduction delta_B\n            delta_B = -grad_g_x.T @ d - 0.5 * d.T @ B @ d + lam * (np.linalg.norm(x, 1) - np.linalg.norm(x + d, 1))\n            \n            t_accepted = 0.0\n            F_accepted = F_current\n            \n            if delta_B > 0:\n                t_try = 1.0\n                t_min_thresh = 1e-12\n                while t_try > t_min_thresh:\n                    x_new = x + t_try * d\n                    F_new = F_func(x_new, A, y, lam)\n                    \n                    if F_new = F_current - c * t_try * delta_B:\n                        t_accepted = t_try\n                        F_accepted = F_new\n                        break\n                    \n                    t_try *= beta\n            \n            case_results.append((F_accepted, t_accepted, k))\n\n        # Select the best result based on the criteria\n        # 1. Min F_accepted\n        # 2. Max t_accepted (tie-breaker)\n        # 3. Min k (tie-breaker)\n        case_results.sort(key=lambda res: (res[0], -res[1], res[2]))\n        \n        best_F, best_t, best_k = case_results[0]\n        all_results.append(f\"[{best_k},{best_t:.6f},{best_F:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}