{
    "hands_on_practices": [
        {
            "introduction": "The first step in mastering any numerical algorithm is to implement it. This practice guides you through coding the proximal gradient method with a backtracking line search, a cornerstone for solving many sparse optimization problems. By implementing this algorithm for the LASSO objective, you will gain first-hand experience with the sufficient decrease condition and observe how the choice of an initial trial step size impacts the algorithm's behavior, particularly the number of backtracking adjustments required to ensure convergence. ",
            "id": "3432732",
            "problem": "Consider the composite convex objective function $F(x) = f(x) + g(x)$ with $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ and $g(x) = \\lambda \\|x\\|_1$, where $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^m$, and $\\lambda  0$. The gradient of $f$ is $\\nabla f(x) = A^\\top (A x - y)$, which is Lipschitz continuous with constant $L = \\|A^\\top A\\|_2$. The Proximal Gradient Method (also known as Iterative Shrinkage-Thresholding Algorithm) updates $x$ via the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding mapping defined entrywise by $S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$.\n\nYou are to implement a hybrid fixed-step and backtracking line search strategy for step size selection in the proximal gradient iteration applied to $F(x)$. The hybrid strategy operates as follows for iteration $k$:\n\n- Given the current iterate $x_k$, choose a trial step size $t \\leftarrow t_{\\text{fixed}}$.\n- Compute the candidate update $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n- Check the sufficient decrease condition derived from the Descent Lemma for Lipschitz gradients:\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}).\n$$\n- If the inequality fails, shrink the step size $t \\leftarrow \\beta t$ with $\\beta \\in (0,1)$ and recompute the candidate until the condition holds. Count each shrink as one backtracking step.\n- The hybrid aspect is that each iteration always restarts the line search from the same prescribed $t_{\\text{fixed}}$ (rather than reusing the previously accepted step size). This prioritizes a fixed-step attempt but guarantees sufficient decrease via backtracking when needed.\n\nImplement this hybrid method to run a fixed number of iterations starting from $x_0 = 0$. For each specified test case, return two quantities: the final objective value $F(x_T)$ after $T$ iterations and the total number of backtracking shrink steps accumulated over all iterations.\n\nUse the following test suite:\n\n- Test case $1$ (well-conditioned matrix, conservative step, minimal backtracking expected):\n$$\nA_1 = \\begin{bmatrix}\n1  0  0.5  0 \\\\\n0  1  0.5  0 \\\\\n0  0  1  1\n\\end{bmatrix},\\quad\ny_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix},\\quad\n\\lambda_1 = 0.1,\\quad\nt_{\\text{fixed},1} = 0.05,\\quad\n\\beta_1 = 0.5,\\quad\nT_1 = 100.\n$$\n\n- Test case $2$ (same data, overly aggressive fixed step to trigger repeated backtracking):\n$$\nA_1 \\text{ and } y_1 \\text{ as above},\\quad\n\\lambda_2 = 0.1,\\quad\nt_{\\text{fixed},2} = 10.0,\\quad\n\\beta_2 = 0.5,\\quad\nT_2 = 100.\n$$\n\n- Test case $3$ (more ill-conditioned matrix, moderately aggressive fixed step):\n$$\nA_3 = \\begin{bmatrix}\n1  2  0  0 \\\\\n0  1  1  0 \\\\\n0  0  1  3\n\\end{bmatrix},\\quad\ny_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix},\\quad\n\\lambda_3 = 0.05,\\quad\nt_{\\text{fixed},3} = 5.0,\\quad\n\\beta_3 = 0.5,\\quad\nT_3 = 120.\n$$\n\nAdditional implementation details and requirements:\n\n- Use the proximal gradient update $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$ with the soft-thresholding operator $S_{\\alpha}$ applied elementwise.\n- Use $x_0 = 0$ for all test cases.\n- For numerical stability in the acceptance check, you may allow a tiny non-negative tolerance $\\varepsilon$ so that the condition is enforced as $F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1}) + \\varepsilon$ with, for example, $\\varepsilon = 10^{-12}$.\n- There are no physical units involved.\n- For each test case $i \\in \\{1,2,3\\}$, report two values: the final objective value $F(x_{T_i})$ rounded to $6$ decimal places, and the total integer count of backtracking shrink steps over all iterations.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$\n[ F(x_{T_1}),\\ \\text{backtracks}_1,\\ F(x_{T_2}),\\ \\text{backtracks}_2,\\ F(x_{T_3}),\\ \\text-backtracks}_3 ],\n$$\nwhere each $F(x_{T_i})$ is a float rounded to $6$ decimals and each backtracking count is an integer.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n\n- **Objective Function**: $F(x) = f(x) + g(x)$, a composite convex function.\n- **Smooth Term**: $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$, where $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^m$.\n- **Non-Smooth Term**: $g(x) = \\lambda \\|x\\|_1$, where $\\lambda  0$.\n- **Gradient of Smooth Term**: $\\nabla f(x) = A^\\top (A x - y)$.\n- **Proximal Operator**: The proximal operator of $g(x)$ is the soft-thresholding function, applied entrywise: $S_\\alpha(z)_i = \\operatorname{sign}(z_i)\\max(|z_i| - \\alpha, 0)$.\n- **Iterative Update Rule**: The proximal gradient update is $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n- **Initial Condition**: $x_0 = 0$.\n- **Line Search Strategy (Hybrid Fixed-Step and Backtracking)**:\n    1. For iteration $k$, start with a trial step size $t \\leftarrow t_{\\text{fixed}}$.\n    2. Compute the candidate update $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n    3. Check the sufficient decrease condition: $F(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})$.\n    4. If the condition fails, shrink the step size $t \\leftarrow \\beta t$ with $\\beta \\in (0,1)$, count one backtracking step, and repeat from step 2.\n    5. The next main iteration $k+1$ restarts the search from $t_{\\text{fixed}}$.\n- **Numerical Tolerance**: A tolerance $\\varepsilon = 10^{-12}$ is allowed in the sufficient decrease condition.\n- **Test Case 1**:\n    - $A_1 = \\begin{bmatrix} 1  0  0.5  0 \\\\ 0  1  0.5  0 \\\\ 0  0  1  1 \\end{bmatrix}$, $y_1 = \\begin{bmatrix}1 \\\\ 2 \\\\ 0.5\\end{bmatrix}$\n    - $\\lambda_1 = 0.1$, $t_{\\text{fixed},1} = 0.05$, $\\beta_1 = 0.5$, $T_1 = 100$.\n- **Test Case 2**:\n    - $A_2 = A_1$, $y_2 = y_1$\n    - $\\lambda_2 = 0.1$, $t_{\\text{fixed},2} = 10.0$, $\\beta_2 = 0.5$, $T_2 = 100$.\n- **Test Case 3**:\n    - $A_3 = \\begin{bmatrix} 1  2  0  0 \\\\ 0  1  1  0 \\\\ 0  0  1  3 \\end{bmatrix}$, $y_3 = \\begin{bmatrix}0 \\\\ 1 \\\\ -1\\end{bmatrix}$\n    - $\\lambda_3 = 0.05$, $t_{\\text{fixed},3} = 5.0$, $\\beta_3 = 0.5$, $T_3 = 120$.\n- **Output**: For each test case, report the final objective value $F(x_T)$ rounded to $6$ decimal places and the total integer count of backtracking steps.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is a standard application of the Proximal Gradient Method (also known as the Iterative Shrinkage-Thresholding Algorithm or ISTA) to the LASSO objective function. All components—the objective function, the proximal operator (soft-thresholding), the gradient, and the backtracking line search with the specified sufficient decrease condition—are fundamental and well-established concepts in convex optimization and signal processing. The problem is scientifically sound.\n- **Well-Posed**: The problem is well-posed. The objective function $F(x)$ is convex, guaranteeing the existence of a minimizer. The algorithm is specified completely with all necessary parameters ($A, y, \\lambda, t_{\\text{fixed}}, \\beta, T, x_0$), ensuring that the sequence of iterates is uniquely determined. The problem asks for the state of the system after a fixed number of iterations, which is a well-defined quantity.\n- **Objective**: The problem is stated in precise mathematical language. There are no subjective or ambiguous terms.\n- **Completeness and Consistency**: The problem is self-contained. All matrices, vectors, constants, and initial conditions are provided. The instructions for the algorithm and the required output format are explicit and consistent.\n- **Other Flaws**: The problem does not exhibit any other flaws such as being trivial, unrealistic, or un-verifiable. The specified parameters in the test cases are chosen to demonstrate different behaviors of the algorithm (e.g., conservative vs. aggressive step sizes), which is a standard pedagogical approach.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires the implementation of the Proximal Gradient Method with a specific line search strategy to solve a LASSO-type optimization problem. The core of the method is an iterative procedure to minimize the objective function $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\|Ax - y\\|_2^2$ is a smooth, differentiable loss term and $g(x) = \\lambda \\|x\\|_1$ is a non-smooth regularization term.\n\nThe iterative update at step $k$ is given by the proximal mapping:\n$$\nx_{k+1} = \\text{prox}_{t g}(x_k - t \\nabla f(x_k))\n$$\nwhere $t  0$ is the step size. For $g(x) = \\lambda \\|x\\|_1$, the proximal operator $\\text{prox}_{t g}(z)$ corresponds to the soft-thresholding operator $S_{\\lambda t}(z)$, which is applied element-wise:\n$$\nS_{\\alpha}(z_i) = \\text{sign}(z_i)\\max(|z_i| - \\alpha, 0)\n$$\nThus, the update rule becomes:\n$$\nx_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))\n$$\nwith $\\nabla f(x_k) = A^\\top(Ax_k - y)$.\n\nThe step size $t$ is determined by a backtracking line search that ensures sufficient decrease in the objective function. For each iteration $k$, the search begins with a fixed trial step size $t = t_{\\text{fixed}}$. A candidate point $x_{k+1}$ is computed. Then, a sufficient decrease condition must be verified. The condition provided is:\n$$\nF(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2 + g(x_{k+1})\n$$\nThis condition is derived from the majorization-minimization principle. The right-hand side is a quadratic upper bound on $F(x)$ around $x_k$. Since $F(x_{k+1}) = f(x_{k+1}) + g(x_{k+1})$, the term $g(x_{k+1})$ appears on both sides and can be canceled. This simplifies the check to:\n$$\nf(x_{k+1}) \\le f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2\n$$\nThis is the standard backtracking condition for proximal gradient methods. If it holds (within a small tolerance $\\varepsilon$), the candidate $x_{k+1}$ is accepted, and the algorithm proceeds to the next iteration, $k+1$. If it fails, the step size $t$ is reduced by a factor $\\beta \\in (0, 1)$ (i.e., $t \\leftarrow \\beta t$), a new candidate $x_{k+1}$ is computed, and the condition is re-checked. Each reduction of $t$ is counted as a backtracking step. This process guarantees that for a sufficiently small $t$, the condition will eventually be met.\n\nThe overall algorithm proceeds as follows for a total of $T$ iterations:\n1. Initialize $x_0 = 0$ and `total_backtracks` $= 0$.\n2. For $k = 0, 1, \\dots, T-1$:\n    a. Set trial step size $t \\leftarrow t_{\\text{fixed}}$.\n    b. Compute gradient $\\nabla f(x_k) = A^\\top(Ax_k - y)$.\n    c. Compute $f(x_k) = \\frac{1}{2}\\|Ax_k - y\\|_2^2$.\n    d. Enter line search loop:\n        i. Compute candidate $x_{k+1} = S_{\\lambda t}(x_k - t \\nabla f(x_k))$.\n        ii. Compute $f(x_{k+1}) = \\frac{1}{2}\\|Ax_{k+1} - y\\|_2^2$.\n        iii. Construct the right-hand side of the acceptance condition: `RHS` $= f(x_k) + \\nabla f(x_k)^\\top (x_{k+1} - x_k) + \\frac{1}{2 t} \\|x_{k+1} - x_k\\|_2^2$.\n        iv. If $f(x_{k+1}) \\le \\text{RHS} + \\varepsilon$, accept $x_{k+1}$ and exit the line search loop.\n        v. Else, shrink step size $t \\leftarrow \\beta t$, increment `total_backtracks`, and repeat from step (i).\n    e. Update $x_k \\leftarrow x_{k+1}$.\n3. After $T$ iterations, compute the final objective value $F(x_T) = f(x_T) + g(x_T)$.\n4. Return $F(x_T)$ and `total_backtracks`.\n\nThis procedure is implemented for each of the three test cases specified. The final objective values are rounded to $6$ decimal places.",
            "answer": "```python\nimport numpy as np\nfrom collections import namedtuple\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the proximal gradient solver.\n    \"\"\"\n\n    TestCase = namedtuple('TestCase', ['A', 'y', 'lambda_', 't_fixed', 'beta', 'T'])\n\n    test_cases = [\n        # Test case 1\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=0.05,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 2\n        TestCase(\n            A=np.array([[1, 0, 0.5, 0], [0, 1, 0.5, 0], [0, 0, 1, 1]]),\n            y=np.array([1, 2, 0.5]),\n            lambda_=0.1,\n            t_fixed=10.0,\n            beta=0.5,\n            T=100\n        ),\n        # Test case 3\n        TestCase(\n            A=np.array([[1, 2, 0, 0], [0, 1, 1, 0], [0, 0, 1, 3]]),\n            y=np.array([0, 1, -1]),\n            lambda_=0.05,\n            t_fixed=5.0,\n            beta=0.5,\n            T=120\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        final_F, total_backtracks = proximal_gradient_solver(\n            A=case.A,\n            y=case.y,\n            lambda_=case.lambda_,\n            t_fixed=case.t_fixed,\n            beta=case.beta,\n            T=case.T\n        )\n        results.append(f\"{final_F:.6f}\")\n        results.append(str(total_backtracks))\n\n    print(f\"[{','.join(results)}]\")\n\ndef soft_threshold(z, alpha):\n    \"\"\"\n    Soft-thresholding operator for the L1 norm's proximal operator.\n    S_alpha(z) = sign(z) * max(|z| - alpha, 0)\n    \"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\ndef f(x, A, y):\n    \"\"\"\n    Computes the smooth part of the objective function: f(x) = 1/2 * ||Ax - y||_2^2\n    \"\"\"\n    residual = A @ x - y\n    return 0.5 * np.dot(residual, residual)\n\ndef g(x, lambda_):\n    \"\"\"\n    Computes the non-smooth part of the objective function: g(x) = lambda * ||x||_1\n    \"\"\"\n    return lambda_ * np.linalg.norm(x, 1)\n\ndef F(x, A, y, lambda_):\n    \"\"\"\n    Computes the total objective function F(x) = f(x) + g(x).\n    \"\"\"\n    return f(x, A, y) + g(x, lambda_)\n\ndef proximal_gradient_solver(A, y, lambda_, t_fixed, beta, T):\n    \"\"\"\n    Implements the Proximal Gradient Method with a hybrid fixed-step and backtracking\n    line search strategy.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    total_backtracks = 0\n    epsilon = 1e-12\n\n    for _ in range(T):\n        t = t_fixed\n        \n        # Pre-compute gradient and f(x) for the current iterate x\n        residual = A @ x - y\n        grad_f_x = A.T @ residual\n        f_x = 0.5 * np.dot(residual, residual)\n\n        while True:\n            # Candidate update step using the proximal operator\n            z = x - t * grad_f_x\n            x_next = soft_threshold(z, lambda_ * t)\n\n            # Check the sufficient decrease condition (backtracking line search)\n            # f(x_next) = f(x) + grad_f(x)^T(x_next - x) + (1/(2t))||x_next - x||^2\n            f_x_next = f(x_next, A, y)\n            \n            # The right-hand side of the inequality\n            diff_x = x_next - x\n            rhs = f_x + np.dot(grad_f_x, diff_x) + (0.5 / t) * np.dot(diff_x, diff_x)\n\n            if f_x_next = rhs + epsilon:\n                x = x_next\n                break  # Step size t is accepted\n            else:\n                # Shrink step size and try again\n                t = beta * t\n                total_backtracks += 1\n    \n    final_F_val = F(x, A, y, lambda_)\n    return final_F_val, total_backtracks\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "A robust implementation is necessary, but true expertise comes from understanding why an algorithm behaves the way it does. This exercise moves beyond basic implementation to a deeper analysis of the backtracking line search condition for the specific case of a quadratic objective. You will derive a precise, necessary, and sufficient criterion for step-size acceptance, discovering its direct link to the problem's curvature (defined by the matrix `$A$`) and providing a powerful diagnostic for the algorithm's behavior. ",
            "id": "3432788",
            "problem": "You are asked to design and analyze a proximal gradient method with backtracking line search for a convex composite objective encountered in compressed sensing and sparse optimization. The smooth data-fitting component is a quadratic, and the regularizer is the sparsity-promoting absolute-value norm. Your goals are to reason from first principles about why and when the line search accepts its first attempted step size (that is, when the per-iteration initial Lipschitz estimate remains unchanged), to connect that acceptance to curvature along the discovered sparse structure, and to propose an implementable diagnostic that predicts when first-attempt step acceptance will resume after a support change.\n\nConsider the unconstrained problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\nwhere the smooth term is the quadratic\n$$\nf(x) \\equiv \\frac{1}{2} \\lVert A x - b \\rVert_2^2,\n$$\nwith given data matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$, and where $\\lambda  0$ is a regularization parameter. The gradient of the smooth term is $\\nabla f(x) = A^\\top(Ax - b)$, and the gradient is globally Lipschitz with Lipschitz constant equal to the squared spectral norm of $A$. The proximity operator of $\\lambda \\lVert x \\rVert_1$ is the coordinatewise soft-thresholding operator.\n\nYou will use a proximal gradient method with backtracking line search in the following form. At iteration $k$ with current iterate $x_k$ and current Lipschitz estimate $L_k  0$, one attempts the candidate update using the step size $1 / L_k$,\n$$\ny_k \\equiv \\operatorname{soft}\\Big(x_k - \\frac{1}{L_k} \\nabla f(x_k), \\, \\frac{\\lambda}{L_k}\\Big),\n$$\nand accepts this first attempt if and only if the standard sufficient-decrease inequality for the smooth part holds with the same $L_k$ in the quadratic upper model used by backtracking. Otherwise one increases the Lipschitz estimate (e.g., multiplies by a factor greater than $1$) and retries until the inequality holds. A “unit-step acceptance” at iteration $k$ means that the first attempt with the initial $L_k$ is accepted (so $L_k$ remains unchanged for that iteration).\n\nStarting from the core definition of the sufficient-decrease inequality for the smooth part used in proximal gradient backtracking, and from the explicit quadratic form of $f(x)$, derive a necessary-and-sufficient acceptance criterion specialized to this quadratic model that depends only on the attempted update direction. Explain how this criterion quantitatively links accepted unit steps to the curvature encountered along the direction determined by the proximal-gradient update and how this direction is constrained by the discovered sparse support.\n\nUsing this principle, propose a scalar, iteration-wise diagnostic that can be computed solely from the data $(A,b)$, the current iterate $x_k$, the current gradient $\\nabla f(x_k)$, and the first-attempt update $y_k$ produced with the initial $L_k$ (before any backtracking increases are applied). The diagnostic must take the form of a real number that compares against $L_k$ to predict, at iteration $k$, whether a unit step will be accepted at iteration $k$; moreover, after any support change detected at iteration $t$ (that is, the support set of $x_{t+1}$ differs from the support set of $x_t$), your diagnostic must be used to predict the first subsequent iteration index $r  t$ at which unit steps resume. Formally, define a “support change” event at iteration $t$ as\n$$\n\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t),\n$$\nwhere $\\operatorname{supp}(z) \\equiv \\{i : z_i \\neq 0\\}$. Define the “resumption index” $r$ for this event to be the smallest $j  t$ such that the line search accepts its first attempt at iteration $j$. Your diagnostic must predict this resumption index using only the diagnostic values computed at each iteration and the initial per-iteration $L_k$.\n\nYou must implement a program that for a set of specified test cases does the following:\n- Generates a synthetic compressed-sensing instance $(A,b)$ as described below.\n- Runs proximal gradient with backtracking for a fixed number of iterations.\n- Records, at each iteration, whether the first attempt was accepted (unit-step acceptance) and whether the support changed relative to the previous iterate.\n- Computes your proposed diagnostic at each iteration using only the first-attempt update.\n- For every support-change event, computes the true resumption index (based on actual unit-step acceptance outcomes) and the predicted resumption index (based solely on the per-iteration diagnostic and the initial $L_k$ at those iterations).\n- For each test case, outputs a single integer equal to the count of mismatches between the true and predicted resumption indices across all support-change events observed within the fixed iteration budget. If there are no support changes in a test case, the count should be $0$.\n\nFoundational starting points you are allowed to assume and explicitly use in your derivations are:\n- The composite structure $F(x) = f(x) + \\lambda \\lVert x \\rVert_1$, the proximity operator for $\\lambda \\lVert x \\rVert_1$, and the definition of the proximal-gradient iteration.\n- The standard sufficient-decrease inequality used in backtracking line search for the proximal-gradient method, based on the Lipschitz continuity of $\\nabla f$.\n- For quadratic $f(x)$, the exact difference identity $f(x + h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2$.\n\nTest suite. Use the following test cases, each fully specified by integers $(m,n)$, sparsity level for the ground-truth vector $k_{\\text{true}}$, iteration budget $K$, initial Lipschitz estimate $L_0$, backtracking multiplier $\\eta  1$, and random seed. In all cases, initialize $x_0 = 0$, and generate a ground-truth sparse vector $x^\\star$ by choosing $k_{\\text{true}}$ indices uniformly at random and setting those entries to independent values uniformly distributed in $[1,2]$ with random signs. Set $b = A x^\\star$ for noise-free data. For the first two cases, use a fixed regularization parameter $\\lambda$; for the third, set $\\lambda$ adaptively as a known multiple of a computable quantity to induce the boundary case of an all-zero solution. For all cases, the maximum number of iterations is fixed and you must report the mismatch count as described above. The test cases are:\n\n- Test case 1 (well-conditioned Gaussian design):\n  - $m = 40$, $n = 80$, $k_{\\text{true}} = 5$, $K = 120$, $L_0 = 0.3$, $\\eta = 2.0$, seed $= 1$, $\\lambda = 0.05$.\n  - Generate $A$ with independent standard normal entries and then normalize each column to unit Euclidean norm.\n\n- Test case 2 (strongly correlated design):\n  - $m = 40$, $n = 60$, $k_{\\text{true}} = 8$, $K = 150$, $L_0 = 0.2$, $\\eta = 2.0$, seed $= 7$, $\\lambda = 0.03$.\n  - Generate a Toeplitz covariance with correlation parameter $\\rho = 0.9$ via $\\Sigma_{ij} = \\rho^{\\lvert i - j \\rvert}$ for $1 \\le i,j \\le n$. Draw a raw matrix $Z \\in \\mathbb{R}^{m \\times n}$ with independent entries distributed as $\\mathcal{N}(0,1)$, form $A_{\\text{raw}} = Z \\, C$ where $C$ is any matrix square root of $\\Sigma$ (for example, a Cholesky factor), and then normalize each column of $A_{\\textraw}$ to unit Euclidean norm to obtain $A$.\n\n- Test case 3 (boundary case with zero solution):\n  - $m = 30$, $n = 50$, $k_{\\text{true}} = 5$, $K = 60$, $L_0 = 0.5$, $\\eta = 2.0$, seed $= 11$, and set $\\lambda = 1.05 \\, \\lVert A^\\top b \\rVert_\\infty$.\n  - Generate $A$ as in Test case 1 (Gaussian with unit-norm columns), and construct $b = A x^\\star$. Then compute $\\lambda$ as specified.\n\nAngle units are not applicable. There are no physical units in this problem.\n\nFinal program output format. Your program should produce a single line of output containing a comma-separated list of the mismatch counts, one per test case, enclosed in square brackets. For example:\n\"[c1,c2,c3]\"\nwhere $c1$, $c2$, and $c3$ are integers equal to the mismatch counts for Test cases 1, 2, and 3, respectively. No other output should be produced.",
            "solution": "The problem asks for an analysis of the backtracking line search condition for a proximal gradient method applied to the LASSO objective function, the derivation of a diagnostic for step-size acceptance, and an implementation to verify the diagnostic's predictive power.\n\nThe optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ is the smooth component and $\\lambda \\lVert x \\rVert_1$ is the non-smooth regularizer. The proximal gradient method generates a sequence of iterates $x_k$ via the update rule\n$$\nx_{k+1} = \\operatorname{prox}_{\\alpha_k \\lambda \\lVert \\cdot \\rVert_1}(x_k - \\alpha_k \\nabla f(x_k)),\n$$\nwhere $\\alpha_k  0$ is the step size. The proximity operator for the $\\ell_1$-norm is the soft-thresholding operator, $\\operatorname{soft}(z, \\tau)_i = \\operatorname{sgn}(z_i) \\max(\\lvert z_i \\rvert - \\tau, 0)$. The step size $\\alpha_k$ is determined by a backtracking line search.\n\nAt iteration $k$, given the current iterate $x_k$, we start with an initial guess for the local Lipschitz constant of $\\nabla f$, denoted $L_{k, \\text{init}}$. We then attempt a step size $\\alpha_k = 1/L$ with $L=L_{k, \\text{init}}$. The candidate update is\n$$\ny_k = \\operatorname{soft}\\Big(x_k - \\frac{1}{L} \\nabla f(x_k), \\, \\frac{\\lambda}{L}\\Big).\n$$\nThis candidate is accepted if it satisfies the sufficient decrease condition for the smooth part $f$. This condition ensures that the quadratic model, which majorizes $f$ at $x_k$, is a valid upper bound for $f$ at the new point $y_k$:\n$$\nf(y_k) \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\nIf this inequality holds for $L = L_{k, \\text{init}}$, the step is a \"unit step\" (or first-attempt acceptance). The new iterate becomes $x_{k+1} = y_k$, and the line search for this iteration terminates. If the inequality fails, we increase $L$ (e.g., $L \\leftarrow \\eta L$ for some $\\eta  1$) and repeat the process of computing a new candidate $y_k$ and checking the inequality, until it is satisfied.\n\n**Derivation of the Acceptance Criterion**\n\nThe core of the analysis lies in specializing this inequality to our specific quadratic function $f(x)$. The problem provides the following exact identity for a quadratic function:\n$$\nf(x+h) - f(x) = \\nabla f(x)^\\top h + \\frac{1}{2} \\lVert A h \\rVert_2^2.\n$$\nLet's apply this identity by setting $x = x_k$ and the displacement $h = y_k - x_k$. The left-hand side of the sufficient decrease inequality, $f(y_k)$, can be rewritten as:\n$$\nf(y_k) = f(x_k + (y_k - x_k)) = f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2.\n$$\nSubstituting this expression into the sufficient decrease inequality gives:\n$$\nf(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{1}{2} \\lVert A(y_k - x_k) \\rVert_2^2 \\le f(x_k) + \\nabla f(x_k)^\\top (y_k - x_k) + \\frac{L}{2} \\lVert y_k - x_k \\rVert_2^2.\n$$\nAfter canceling the common terms $f(x_k)$ and $\\nabla f(x_k)^\\top (y_k - x_k)$ from both sides and multiplying by $2$, we obtain a simplified, equivalent inequality:\n$$\n\\lVert A(y_k - x_k) \\rVert_2^2 \\le L \\lVert y_k - x_k \\rVert_2^2.\n$$\nThis is a necessary and sufficient condition for the line search to accept the step size $\\alpha_k = 1/L$. Let $d_k = y_k - x_k$ be the update vector. If $d_k = 0$, the inequality holds trivially as $0 \\le 0$. If $d_k \\neq 0$, the condition can be expressed as:\n$$\n\\frac{\\lVert A d_k \\rVert_2^2}{\\lVert d_k \\rVert_2^2} \\le L.\n$$\nThis demonstrates that the line search accepts the step if and only if the Rayleigh quotient of the matrix $A^\\top A$ (the Hessian of $f$) with respect to the update vector $d_k$ is no greater than the Lipschitz estimate $L$.\n\n**Connection to Curvature and Sparse Support**\n\nThe term $\\lVert A d_k \\rVert_2^2$ can be written as $(A d_k)^\\top (A d_k) = d_k^\\top A^\\top A d_k$. Since the Hessian is $\\nabla^2 f(x) = A^\\top A$, this term represents the second-order change, or curvature, of $f$ along the direction $d_k$. The acceptance condition thus states that the effective curvature along the chosen update direction must be bounded by $L$.\n\nThe update vector $d_k = y_k - x_k$ is determined by the proximal gradient step. The soft-thresholding operation ensures that $y_k$ is sparse. Consequently, $d_k$ is also sparse, and its non-zero entries are typically confined to the support of $y_k$. When the algorithm's active set (the support of the iterates) stabilizes, the vector $d_k$ is restricted to a particular subspace. The backtracking line search adapts $L$ to be an upper bound on the curvature within this subspace. A change in the support of $x_k$ means $d_k$ probes a new direction or subspace, potentially one with higher curvature (i.e., a larger Rayleigh quotient). If this new curvature exceeds the current estimate $L$, the line search will fail, forcing an increase in $L$ until it appropriately upper-bounds the curvature in the new active subspace.\n\n**Proposed Diagnostic and Prediction**\n\nBased on the derived criterion, we can define a scalar diagnostic. At iteration $k$, let $L_{k, \\text{init}}$ be the initial Lipschitz estimate used for the first attempt. The first-attempt candidate update is $y_{k, \\text{init}} = \\operatorname{soft}(x_k - \\frac{1}{L_{k, \\text{init}}} \\nabla f(x_k), \\frac{\\lambda}{L_{k, \\text{init}}})$. Let $d_{k, \\text{init}} = y_{k, \\text{init}} - x_k$. The diagnostic is the Rayleigh quotient:\n$$\nD_k = \\begin{cases} \\frac{\\lVert A d_{k, \\text{init}} \\rVert_2^2}{\\lVert d_{k, \\text{init}} \\rVert_2^2}  \\text{if } d_{k, \\text{init}} \\neq 0 \\\\ 0  \\text{if } d_{k, \\text{init}} = 0 \\end{cases}\n$$\nA \"unit step\" is accepted at iteration $k$ if and only if $D_k \\le L_{k, \\text{init}}$. This is not a heuristic prediction but a direct consequence of the derived necessary and sufficient condition.\n\nA \"support change\" event at iteration $t$ is $\\operatorname{supp}(x_{t+1}) \\neq \\operatorname{supp}(x_t)$. The true resumption index is $r = \\min \\{j  t \\mid \\text{the first attempt at step } j \\text{ is accepted}\\}$. Our predicted resumption index, $\\hat{r}$, is found by checking our diagnostic condition at each subsequent iteration: $\\hat{r} = \\min \\{j  t \\mid D_j \\le L_{j, \\text{init}}\\}$.\n\nSince the condition $D_j \\le L_{j, \\text{init}}$ is precisely the condition for first-attempt acceptance, the predicted resumption index $\\hat{r}$ must be identical to the true resumption index $r$ in every case, provided the computations are exact. Therefore, the number of mismatches between the true and predicted indices is expected to be $0$. The implementation serves as a verification of this analytical conclusion.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, toeplitz\n\ndef soft_threshold(z, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef run_one_case(m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params=None):\n    \"\"\"\n    Runs a single test case for the proximal gradient algorithm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # --- 1. Generate synthetic data ---\n    # Generate ground-truth sparse vector x_star\n    x_star = np.zeros(n)\n    support_star_indices = rng.choice(n, k_true, replace=False)\n    x_star[support_star_indices] = rng.uniform(1, 2, k_true)\n    x_star[support_star_indices] *= rng.choice([-1, 1], k_true)\n\n    # Generate matrix A\n    if A_gen_mode == 'gaussian':\n        A = rng.standard_normal(size=(m, n))\n        col_norms = np.linalg.norm(A, axis=0)\n        A = A / col_norms\n    elif A_gen_mode == 'correlated':\n        rho = 0.9\n        Sigma = toeplitz(rho**np.arange(n))\n        # Add a small identity matrix for stability before Cholesky\n        C = cholesky(Sigma + 1e-10 * np.eye(n), lower=True)\n        Z = rng.standard_normal(size=(m, n))\n        A_raw = Z @ C.T\n        col_norms = np.linalg.norm(A_raw, axis=0)\n        A = A_raw / col_norms\n\n    # Generate measurement vector b\n    b = A @ x_star\n\n    # Set lambda if adaptive\n    if lambda_mode_params == 'adaptive':\n        lambda_val = 1.05 * np.max(np.abs(A.T @ b))\n\n    # --- 2. Run Proximal Gradient with Backtracking ---\n    x_k = np.zeros(n)\n    L_val = L0\n    \n    is_unit_step_accepted = []\n    support_changed = []\n    diagnostics = []\n    L_inits = []\n\n    for k in range(K):\n        # Store support of x_k for later comparison\n        support_k = set(np.where(x_k != 0)[0])\n        \n        # --- At iteration k, perform one step ---\n        grad_fxk = A.T @ (A @ x_k - b)\n        \n        # This is the initial Lipschitz estimate for this iteration\n        L_k_init = L_val\n        L_inits.append(L_k_init)\n\n        # Compute first-attempt update and diagnostic\n        y_k_init = soft_threshold(x_k - (1/L_k_init) * grad_fxk, lambda_val / L_k_init)\n        d_k_init = y_k_init - x_k\n        \n        if np.linalg.norm(d_k_init)  1e-12:\n            D_k = 0.0\n        else:\n            Adk_norm_sq = np.linalg.norm(A @ d_k_init)**2\n            dk_norm_sq = np.linalg.norm(d_k_init)**2\n            D_k = Adk_norm_sq / dk_norm_sq\n        diagnostics.append(D_k)\n\n        # Backtracking line search\n        L_attempt = L_k_init\n        num_backtracks = 0\n        while True:\n            y_k = soft_threshold(x_k - (1/L_attempt) * grad_fxk, lambda_val / L_attempt)\n            d_k = y_k - x_k\n\n            if np.linalg.norm(d_k)  1e-12:\n                # If step is zero, condition is trivially satisfied.\n                break \n\n            lhs = np.linalg.norm(A @ d_k)**2\n            rhs = L_attempt * np.linalg.norm(d_k)**2\n            \n            if lhs = rhs:\n                break\n            \n            L_attempt *= eta\n            num_backtracks += 1\n\n        # Accept the step\n        is_unit_step_accepted.append(num_backtracks == 0)\n        x_k = y_k  # Update x_k to x_{k+1}\n        L_val = L_attempt # Update L for next iteration\n\n        # Check for support change\n        support_k_plus_1 = set(np.where(x_k != 0)[0])\n        support_changed.append(support_k != support_k_plus_1)\n\n    # --- 3. Post-process to count mismatches ---\n    mismatch_count = 0\n    support_change_indices = [t for t, changed in enumerate(support_changed) if changed]\n\n    for t in support_change_indices:\n        r_true = -1\n        for j in range(t + 1, K):\n            if is_unit_step_accepted[j]:\n                r_true = j\n                break\n        \n        r_pred = -1\n        for j in range(t + 1, K):\n            # The prediction check: D_j = L_{j,init}\n            if diagnostics[j] = L_inits[j]:\n                r_pred = j\n                break\n        \n        if r_true != r_pred:\n            mismatch_count += 1\n            \n    return mismatch_count\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k_true, K, L0, eta, seed, lambda_val, A_gen_mode, lambda_mode_params)\n        (40, 80, 5, 120, 0.3, 2.0, 1, 0.05, 'gaussian', None),\n        (40, 60, 8, 150, 0.2, 2.0, 7, 0.03, 'correlated', None),\n        (30, 50, 5, 60, 0.5, 2.0, 11, None, 'gaussian', 'adaptive')\n    ]\n\n    results = []\n    for params in test_cases:\n        mismatch_count = run_one_case(*params)\n        results.append(mismatch_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Backtracking line search is not limited to first-order methods; it is a critical component for stabilizing more powerful techniques like the proximal Newton method. In this practice, you will explore how line search controls the step size along directions generated by different Hessian approximations `$B$`. By comparing the outcomes from various choices of `$B$`, you will develop an appreciation for the trade-off between the computational cost of forming a search direction and the quality of the resulting descent step. ",
            "id": "3432780",
            "problem": "Consider the composite convex optimization problem fundamental to compressed sensing and sparse optimization, defined by minimizing the function $F(x)$ over $x \\in \\mathbb{R}^n$, where\n$$\nF(x) = g(x) + \\lambda \\lVert x \\rVert_1, \\quad g(x) = \\tfrac{1}{2} \\lVert A x - y \\rVert_2^2,\n$$\nwith data matrix $A \\in \\mathbb{R}^{m \\times n}$, data vector $y \\in \\mathbb{R}^m$, and regularization parameter $\\lambda  0$. The smooth part $g(x)$ has gradient $\\nabla g(x) = A^\\top (A x - y)$ and Hessian $\\nabla^2 g(x) = A^\\top A$. Proximal Newton methods generate a trial direction $d \\in \\mathbb{R}^n$ by minimizing a local quadratic-plus-nonsmooth model\n$$\nm_B(d; x) = \\nabla g(x)^\\top d + \\tfrac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1 - \\lambda \\lVert x \\rVert_1,\n$$\nwhere $B \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix that approximates $\\nabla^2 g(x)$. The predicted model reduction is\n$$\n\\Delta_B(x; d) = m_B(0; x) - m_B(d; x) = - \\nabla g(x)^\\top d - \\tfrac{1}{2} d^\\top B d + \\lambda \\left(\\lVert x \\rVert_1 - \\lVert x + d \\rVert_1\\right).\n$$\nA backtracking line search with parameters $c \\in (0,1)$ and $\\beta \\in (0,1)$ selects the largest $t \\in \\{1, \\beta, \\beta^2, \\dots \\}$ such that the sufficient decrease condition\n$$\nF(x + t d) \\le F(x) - c \\, t \\, \\Delta_B(x; d)\n$$\nis satisfied, where $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$. Assume that $B$ is chosen from a set of three symmetric positive definite options constructed from $A^\\top A$: a scalar multiple of the identity $B = L I$ with $L$ equal to the largest eigenvalue of $A^\\top A$, a diagonal majorizer $B = \\mathrm{diag}(A^\\top A)$, and a scalar multiple $B = s I$ with $s = \\tfrac{1}{n} \\mathrm{trace}(A^\\top A)$. For each such $B$, the trial direction $d$ is defined as the exact minimizer of $m_B(d; x)$.\n\nStarting from a given initial point $x \\in \\mathbb{R}^n$, perform the following for each candidate $B$:\n- Compute the model-minimizing direction $d \\in \\mathbb{R}^n$ that minimizes $m_B(d; x)$.\n- Compute the predicted model reduction $\\Delta_B(x; d)$.\n- If $\\Delta_B(x; d) \\le 0$, define the accepted step size as $t = 0$ and the new point as $x$ itself. Otherwise, perform backtracking with parameters $c$ and $\\beta$ to find the largest $t \\in \\{1, \\beta, \\beta^2, \\dots\\}$ satisfying the sufficient decrease condition; terminate the backtracking if $t$ falls below a numerical threshold, in which case set $t = 0$ and the new point as $x$.\n- Record the resulting function value $F(x + t d)$.\n\nAmong the three candidates, select the one that achieves the smallest accepted function value $F(x + t d)$. In case of a tie in the accepted function values, prefer the one with the larger accepted step size $t$. If there is still a tie, choose the one with the smallest index according to the ordering $B = L I$ as index $1$, $B = \\mathrm{diag}(A^\\top A)$ as index $2$, and $B = s I$ as index $3$.\n\nYour task is to implement a program that, for each test case defined below, returns a list with three entries: the selected index $k \\in \\{1,2,3\\}$, the accepted step size $t \\in \\mathbb{R}$, and the accepted function value $F(x + t d) \\in \\mathbb{R}$. All floating-point outputs must be rounded to exactly six decimal places.\n\nUse only purely mathematical and logical computation. There are no physical units involved. Angles are not used. Percentages are not used.\n\nTest suite:\n- Case $1$ (happy path):\n  - $A = \\begin{bmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  1 \\\\ 2  -1  0 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 3 \\\\ -1 \\\\ 2 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.1$,\n  - $x = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n- Case $2$ (boundary with strong sparsity pressure):\n  - $A$ as in case $1$,\n  - $y$ as in case $1$,\n  - $\\lambda = 5.0$,\n  - $x = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n- Case $3$ (ill-conditioned geometry):\n  - $A = \\begin{bmatrix}\n  10  0  0  0 \\\\\n  0  1  0  0 \\\\\n  0  0  0.1  0 \\\\\n  0  0  0  0.01 \\\\\n  10  1  0.1  0.01\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ 0 \\end{bmatrix}$,\n  - $\\lambda = 0.05$,\n  - $x = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\end{bmatrix}$,\n  - $c = 0.1$,\n  - $\\beta = 0.5$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the form $[k, t, F]$ corresponding to a test case. For example, the output must have the form\n$[\\,[k_1,t_1,F_1],[k_2,t_2,F_2],[k_3,t_3,F_3]\\,]$,\nwith each $t_i$ and $F_i$ rounded to exactly six decimal places and no spaces in the printed output.",
            "solution": "The user has provided a problem from the field of numerical optimization, specifically concerning proximal Newton methods for solving composite convex optimization problems of the form prevalent in compressed sensing and sparse optimization. The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. Therefore, it is deemed valid.\n\nThe core task is to solve the optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\quad \\text{where} \\quad F(x) = g(x) + \\lambda \\lVert x \\rVert_1\n$$\nHere, $g(x) = \\frac{1}{2} \\lVert Ax - y \\rVert_2^2$ is a smooth, convex function representing the data fidelity term, and $\\lambda \\lVert x \\rVert_1$ is a non-smooth, convex regularization term promoting sparsity in the solution $x$. The gradient of the smooth part is $\\nabla g(x) = A^\\top (Ax - y)$.\n\nThe problem specifies using a proximal Newton method. This method generates a search direction $d$ at a point $x$ by minimizing a local model of the function $F(x+d)$. The model, $m_B(d; x)$, is constructed by replacing $g(x+d)$ with its second-order Taylor approximation centered at $x$, and keeping the non-smooth term $\\lambda \\lVert x+d \\rVert_1$ as is. The Hessian of $g(x)$, $\\nabla^2 g(x) = A^\\top A$, is approximated by a symmetric positive definite matrix $B$. The model to be minimized is given by:\n$$\nm_B(d; x) = g(x) + \\nabla g(x)^\\top d + \\frac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1\n$$\nMinimizing $m_B(d; x)$ with respect to $d$ is equivalent to minimizing\n$$\nd = \\arg\\min_{d \\in \\mathbb{R}^n} \\left\\{ \\nabla g(x)^\\top d + \\frac{1}{2} d^\\top B d + \\lambda \\lVert x + d \\rVert_1 \\right\\}\n$$\nThis is a convex optimization subproblem. We can solve it by finding the unique point $u = x+d$ that minimizes the corresponding objective in terms of $u$:\n$$\nu = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\nabla g(x)^\\top (u-x) + \\frac{1}{2} (u-x)^\\top B (u-x) + \\lambda \\lVert u \\rVert_1 \\right\\}\n$$\nBy completing the square, this can be expressed as a proximal operator problem. The form of the solution depends on the structure of the matrix $B$.\n\n**Case 1: Isotropic Hessian Approximation ($B = \\gamma I$)**\nWhen $B$ is a scalar multiple of the identity matrix, $B = \\gamma I$ for some scalar $\\gamma  0$, the subproblem simplifies to:\n$$\nu = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\frac{\\gamma}{2} \\left\\lVert u - \\left(x - \\frac{1}{\\gamma} \\nabla g(x)\\right) \\right\\rVert_2^2 + \\lambda \\lVert u \\rVert_1 \\right\\}\n$$\nThe solution is given by the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator $S_{\\kappa}(\\cdot)$:\n$$\nu = S_{\\lambda/\\gamma}\\left(x - \\frac{1}{\\gamma} \\nabla g(x)\\right)\n$$\nwhere $(S_\\kappa(z))_i = \\mathrm{sign}(z_i) \\max(|z_i| - \\kappa, 0)$. The search direction is then $d = u - x$. This case applies to two of the specified choices for $B$:\n- **Index 1**: $B = L I$, where $L$ is the largest eigenvalue of $A^\\top A$. Here, $\\gamma = L$.\n- **Index 3**: $B = s I$, where $s = \\frac{1}{n} \\mathrm{trace}(A^\\top A)$. Here, $\\gamma = s$.\n\n**Case 2: Diagonal Hessian Approximation ($B = \\mathrm{diag}(A^\\top A)$)**\nWhen $B$ is a diagonal matrix, the subproblem decouples coordinate-wise:\n$$\nu_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\frac{B_{ii}}{2} \\left( u_i - \\left(x_i - \\frac{(\\nabla g(x))_i}{B_{ii}}\\right) \\right)^2 + \\lambda |u_i| \\right\\}\n$$\nThe solution for each coordinate is again given by soft-thresholding:\n$$\nu_i = S_{\\lambda/B_{ii}}\\left(x_i - \\frac{(\\nabla g(x))_i}{B_{ii}}\\right)\n$$\nThe search direction is $d = u - x$. This corresponds to the choice for **Index 2**: $B = \\mathrm{diag}(A^\\top A)$.\n\nOnce the direction $d$ is computed for a given $B$, we determine the step size $t$ via a backtracking line search. First, we compute the predicted reduction in the model's value from taking the step $d$:\n$$\n\\Delta_B(x; d) = m_B(0; x) - m_B(d; x) = -\\nabla g(x)^\\top d - \\frac{1}{2} d^\\top B d + \\lambda (\\lVert x \\rVert_1 - \\lVert x + d \\rVert_1)\n$$\nIf $\\Delta_B(x; d) \\le 0$, the direction is not a descent direction for the model, and we take a zero step, $t=0$. Otherwise, we seek the largest step size $t \\in \\{1, \\beta, \\beta^2, \\dots\\}$ that satisfies the sufficient decrease condition (an Armijo-type condition):\n$$\nF(x + t d) \\le F(x) - c \\, t \\, \\Delta_B(x; d)\n$$\nwhere $c \\in (0,1)$ is a given constant. The process terminates if $t$ falls below a numerical precision threshold, in which case $t$ is set to $0$.\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Initialize with the given $A, y, \\lambda, x, c, \\beta$.\n2.  Compute the matrix $A^\\top A$.\n3.  For each of the three candidate matrices $B_k$ (for $k=1,2,3$):\n    a. Construct $B_k$. For $k=1$, compute the max eigenvalue of $A^\\top A$. For $k=2$, take the diagonal of $A^\\top A$. For $k=3$, compute the normalized trace of $A^\\top A$.\n    b. Compute the gradient $\\nabla g(x) = A^\\top(Ax-y)$.\n    c. Compute the search direction $d_k$ by solving the proximal subproblem associated with $B_k$.\n    d. Compute the predicted reduction $\\Delta_{B_k}(x;d_k)$.\n    e. Perform the backtracking line search to find the accepted step size $t_k$.\n    f. Calculate the new function value $F(x+t_k d_k)$.\n4. Compare the results $(t_k, F(x+t_k d_k))$ for $k=1,2,3$. Select the best index $k^*$ based on the specified criteria:\n    a. The one that yields the minimum final function value.\n    b. In case of a tie, the one with the largest accepted step size $t_k$.\n    c. If a tie persists, the one with the smallest index $k$.\n5. The final output for the test case is the triplet $[k^*, t_{k^*}, F(x+t_{k^*}d_{k^*})]$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the proximal Newton step selection problem for the given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        (\n            np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]], dtype=float),\n            np.array([3, -1, 2, 0], dtype=float),\n            0.1,\n            np.array([0, 0, 0], dtype=float),\n            0.1,\n            0.5\n        ),\n        # Case 2\n        (\n            np.array([[1, 0, 2], [0, 1, -1], [1, 1, 1], [2, -1, 0]], dtype=float),\n            np.array([3, -1, 2, 0], dtype=float),\n            5.0,\n            np.array([1, -2, 3], dtype=float),\n            0.1,\n            0.5\n        ),\n        # Case 3\n        (\n            np.array([\n                [10, 0, 0, 0],\n                [0, 1, 0, 0],\n                [0, 0, 0.1, 0],\n                [0, 0, 0, 0.01],\n                [10, 1, 0.1, 0.01]\n            ], dtype=float),\n            np.array([1, 0.1, -0.1, 0.05, 0], dtype=float),\n            0.05,\n            np.array([0.5, -0.5, 0.5, -0.5], dtype=float),\n            0.1,\n            0.5\n        ),\n    ]\n\n    all_results = []\n    \n    # Helper functions\n    def soft_threshold(z, kappa):\n        if np.isscalar(kappa):\n            return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n        else: # Element-wise thresholding\n            return np.sign(z) * np.maximum(np.abs(z) - kappa, 0)\n\n    def F_func(x_vec, A, y, lam):\n        g = 0.5 * np.linalg.norm(A @ x_vec - y)**2\n        h = lam * np.linalg.norm(x_vec, 1)\n        return g + h\n\n    for A, y, lam, x, c, beta in test_cases:\n        m, n = A.shape\n        ATA = A.T @ A\n        \n        # B candidate definitions\n        # B_1: L*I\n        L = np.linalg.eigvalsh(ATA).max()\n        B1 = L * np.identity(n)\n        \n        # B_2: diag(A^T A)\n        diag_ATA = np.diag(ATA)\n        # Assuming positive definite, so no zero diagonal entries\n        B2 = np.diag(diag_ATA)\n\n        # B_3: s*I\n        s = np.trace(ATA) / n\n        B3 = s * np.identity(n)\n\n        B_candidates = [(1, B1, 'isotropic', L), (2, B2, 'diagonal', diag_ATA), (3, B3, 'isotropic', s)]\n\n        grad_g_x = A.T @ (A @ x - y)\n        F_current = F_func(x, A, y, lam)\n        \n        case_results = []\n\n        for k, B, b_type, b_param in B_candidates:\n            d = np.zeros(n)\n            # Compute direction d\n            if b_type == 'isotropic':\n                gamma = b_param\n                u = soft_threshold(x - (1/gamma) * grad_g_x, lam / gamma)\n                d = u - x\n            elif b_type == 'diagonal':\n                # b_param is the diagonal vector\n                u = soft_threshold(x - grad_g_x / b_param, lam / b_param)\n                d = u - x\n\n            # Compute predicted reduction delta_B\n            delta_B = -grad_g_x.T @ d - 0.5 * d.T @ B @ d + lam * (np.linalg.norm(x, 1) - np.linalg.norm(x + d, 1))\n            \n            t_accepted = 0.0\n            F_accepted = F_current\n            \n            if delta_B > 0:\n                t_try = 1.0\n                t_min_thresh = 1e-12\n                while t_try > t_min_thresh:\n                    x_new = x + t_try * d\n                    F_new = F_func(x_new, A, y, lam)\n                    \n                    if F_new = F_current - c * t_try * delta_B:\n                        t_accepted = t_try\n                        F_accepted = F_new\n                        break\n                    \n                    t_try *= beta\n            \n            case_results.append((F_accepted, t_accepted, k))\n\n        # Select the best result based on the criteria\n        # 1. Min F_accepted\n        # 2. Max t_accepted (tie-breaker)\n        # 3. Min k (tie-breaker)\n        case_results.sort(key=lambda res: (res[0], -res[1], res[2]))\n        \n        best_F, best_t, best_k = case_results[0]\n        all_results.append(f\"[{best_k},{best_t:.6f},{best_F:.6f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        }
    ]
}