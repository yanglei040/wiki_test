{
    "hands_on_practices": [
        {
            "introduction": "在运行大规模模拟之前，理解像真阳性率（TPR）和错误发现率（FDR）这样的基本性能指标是如何从恢复过程的统计特性中推导出来的至关重要。这个练习 () 提供了一个简化的、可解析的场景，用于计算这些指标，并观察它们如何随信噪比（SNR）变化。这为理解信号检测中的权衡提供了清晰的直觉。",
            "id": "3446230",
            "problem": "考虑标准线性模型 $y = X \\beta^{\\star} + \\varepsilon$，其中 $X$ 是一个列标准正交的 $n \\times p$ 设计矩阵，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是一个系数向量，它有 $k$ 个大小相等（符号任意）的非零项，大小为 $a$，以及加性噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。在标准正交列的条件下，最小绝对收缩和选择算子 (LASSO) 通过对统计量 $z_{i} = x_{i}^{\\top} y$ 进行软阈值处理来选择坐标 $i$；如果 $|z_{i}|$ 超过阈值 $\\lambda$，则一个特征被宣告为已发现。你将使用 $z_{i}$ 的零分布来确定 $\\lambda$，以控制每个特征的错误包含概率。\n\n给定 $p = 1000$，$k = 40$，$\\sigma = 1$ 以及由 $a / \\sigma \\in \\{0.5, 1.5, 3.0\\}$ 定义的三个信噪比 (SNR)，通过要求每个特征的零分布尾部概率等于 $\\tau = 0.01$ 来选择一个单一阈值 $\\lambda$。也就是说，将一个零特征宣告为已发现的概率为 $\\mathbb{P}(|Z_{0}| > \\lambda) = \\tau$，其中 $Z_{0} \\sim \\mathcal{N}(0, \\sigma^{2})$。对于每个 SNR 值，将信号坐标视为 $Z_{s} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$，其中 $\\mu = a$，并计算：\n- 真阳性率 $\\mathrm{TPR} = \\mathbb{P}(|Z_{s}| > \\lambda)$，\n- 近似期望错误发现率 $\\mathrm{FDR} \\approx \\dfrac{(p - k)\\,\\mathbb{P}(|Z_{0}| > \\lambda)}{(p - k)\\,\\mathbb{P}(|Z_{0}| > \\lambda) + k\\,\\mathbb{P}(|Z_{s}| > \\lambda)}$，\n\n假设坐标之间相互独立。定义每个 SNR 的 $F1$ 分数为 $F1 = \\dfrac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$，其中 $\\mathrm{precision} = 1 - \\mathrm{FDR}$ 且 $\\mathrm{recall} = \\mathrm{TPR}$。最后，报告通过对三个 SNR 的 $F1$ 分数取平均值得到的宏平均 $F1$ 分数。将你的最终答案四舍五入到四位有效数字。",
            "solution": "用户提供了一个在统计信号处理和稀疏恢复领域中定义明确的问题。该问题具有科学依据、内容自洽，并且所有参数都已指定。我将提供完整的解题步骤。\n\n该问题要求计算在简化的标准正交设计设置下，使用软阈值规则进行特征选择的宏平均 $F1$ 分数，这是 LASSO 估计器的一个特征。分析过程分为四个主要步骤：\n1.  根据对错误包含的指定控制，确定选择阈值 $\\lambda$。\n2.  对于每个信噪比 (SNR)，计算真阳性率 (TPR)，也称为召回率或灵敏度。\n3.  对于每个 SNR，计算近似期望错误发现率 (FDR)、精确率和相应的 $F1$ 分数。\n4.  计算 $F1$ 分数的宏平均值。\n\n问题提供了以下参数：总共有 $p=1000$ 个特征，其中 $k=40$ 个是“信号”（非零系数），$p-k=960$ 个是“零”（零系数）。噪声标准差为 $\\sigma=1$。信号幅度由 $a/\\sigma \\in \\{0.5, 1.5, 3.0\\}$ 给出，由于 $\\sigma=1$，这可以简化为 $a \\in \\{0.5, 1.5, 3.0\\}$。\n\n首先，我们确定阈值 $\\lambda$。它被设定为控制每个特征的错误包含概率。对于一个零特征，其对应的统计量 $Z_0$ 服从正态分布 $Z_0 \\sim \\mathcal{N}(0, \\sigma^2)$。如果 $|Z_0| > \\lambda$，则发生一次错误包含。这个概率被设定为 $\\tau = 0.01$。\n$$ \\mathbb{P}(|Z_0| > \\lambda) = \\tau $$\n由于 $Z_0/\\sigma \\sim \\mathcal{N}(0, 1)$，我们可以对该随机变量进行标准化。设 $\\Phi(\\cdot)$ 为标准正态分布的累积分布函数 (CDF)。\n$$ \\mathbb{P}\\left(\\left|\\frac{Z_0}{\\sigma}\\right| > \\frac{\\lambda}{\\sigma}\\right) = \\tau $$\n根据正态分布的对称性，这等价于：\n$$ 2 \\cdot \\mathbb{P}\\left(\\frac{Z_0}{\\sigma} > \\frac{\\lambda}{\\sigma}\\right) = \\tau $$\n$$ 1 - \\Phi\\left(\\frac{\\lambda}{\\sigma}\\right) = \\frac{\\tau}{2} $$\n$$ \\Phi\\left(\\frac{\\lambda}{\\sigma}\\right) = 1 - \\frac{\\tau}{2} $$\n解出 $\\lambda$，我们得到：\n$$ \\lambda = \\sigma \\Phi^{-1}\\left(1 - \\frac{\\tau}{2}\\right) $$\n代入给定值 $\\sigma=1$ 和 $\\tau=0.01$：\n$$ \\lambda = 1 \\cdot \\Phi^{-1}\\left(1 - \\frac{0.01}{2}\\right) = \\Phi^{-1}(0.995) \\approx 2.575829 $$\n\n接下来，我们评估三种信号强度 $a \\in \\{0.5, 1.5, 3.0\\}$ 下的性能。为此，我们需要真阳性率 (TPR)，即正确识别信号特征的概率。信号特征的统计量 $Z_s$ 被建模为 $Z_s \\sim \\mathcal{N}(a, \\sigma^2)$。\n$$ \\mathrm{TPR} = \\mathbb{P}(|Z_s| > \\lambda) = \\mathbb{P}(Z_s > \\lambda) + \\mathbb{P}(Z_s  -\\lambda) $$\n对变量 $Z_s$ 进行标准化：\n$$ \\mathrm{TPR} = \\mathbb{P}\\left(\\frac{Z_s - a}{\\sigma} > \\frac{\\lambda - a}{\\sigma}\\right) + \\mathbb{P}\\left(\\frac{Z_s - a}{\\sigma}  \\frac{-\\lambda - a}{\\sigma}\\right) $$\n由于 $(Z_s - a)/\\sigma \\sim \\mathcal{N}(0, 1)$，我们有：\n$$ \\mathrm{TPR}(a) = \\left(1 - \\Phi\\left(\\frac{\\lambda - a}{\\sigma}\\right)\\right) + \\Phi\\left(\\frac{-\\lambda - a}{\\sigma}\\right) $$\n当 $\\sigma=1$ 时，这简化为：\n$$ \\mathrm{TPR}(a) = 1 - \\Phi(\\lambda - a) + \\Phi(-\\lambda - a) $$\n召回率定义为 $\\mathrm{recall} = \\mathrm{TPR}$。\n\n近似期望错误发现率 (FDR) 由下式给出：\n$$ \\mathrm{FDR} \\approx \\frac{(p - k)\\,\\mathbb{P}(|Z_{0}| > \\lambda)}{(p - k)\\,\\mathbb{P}(|Z_{0}| > \\lambda) + k\\,\\mathbb{P}(|Z_{s}| > \\lambda)} = \\frac{(p - k)\\tau}{(p-k)\\tau + k \\cdot \\mathrm{TPR}} $$\n精确率是 $\\mathrm{precision} = 1 - \\mathrm{FDR}$。$F1$ 分数是精确率和召回率的调和平均数：\n$$ F1 = \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} $$\n我们可以写出 $F1$ 分数的直接公式。首先，注意到 $\\mathrm{precision} = \\frac{k \\cdot \\mathrm{TPR}}{(p-k)\\tau + k \\cdot \\mathrm{TPR}}$。将此式和 $\\mathrm{recall} = \\mathrm{TPR}$ 代入 $F1$ 公式并简化，得到：\n$$ F1 = \\frac{2 k \\cdot \\mathrm{TPR}}{k + (p-k)\\tau + k \\cdot \\mathrm{TPR}} $$\n现在我们使用 $p=1000$，$k=40$，$\\tau=0.01$ 和 $\\lambda \\approx 2.575829$ 来计算每种情况下的值。期望的假阳性数量为 $(p-k)\\tau = (1000-40) \\times 0.01 = 960 \\times 0.01 = 9.6$。\n\n情况 1：$a=0.5$\n$$ \\mathrm{TPR}_1 = 1 - \\Phi(2.575829 - 0.5) + \\Phi(-2.575829 - 0.5) $$\n$$ \\mathrm{TPR}_1 = 1 - \\Phi(2.075829) + \\Phi(-3.075829) \\approx 1 - 0.981042 + 0.001051 \\approx 0.020009 $$\n使用这个 TPR，我们计算 $F1$ 分数：\n$$ F1_1 = \\frac{2 \\cdot 40 \\cdot 0.020009}{40 + 9.6 + 40 \\cdot 0.020009} = \\frac{1.60072}{49.6 + 0.80036} = \\frac{1.60072}{50.40036} \\approx 0.031760 $$\n\n情况 2：$a=1.5$\n$$ \\mathrm{TPR}_2 = 1 - \\Phi(2.575829 - 1.5) + \\Phi(-2.575829 - 1.5) $$\n$$ \\mathrm{TPR}_2 = 1 - \\Phi(1.075829) + \\Phi(-4.075829) \\approx 1 - 0.859000 + 2.34 \\times 10^{-5} \\approx 0.141023 $$\n相应的 $F1$ 分数是：\n$$ F1_2 = \\frac{2 \\cdot 40 \\cdot 0.141023}{40 + 9.6 + 40 \\cdot 0.141023} = \\frac{11.28184}{49.6 + 5.64092} = \\frac{11.28184}{55.24092} \\approx 0.204232 $$\n\n情况 3：$a=3.0$\n$$ \\mathrm{TPR}_3 = 1 - \\Phi(2.575829 - 3.0) + \\Phi(-2.575829 - 3.0) $$\n$$ \\mathrm{TPR}_3 = 1 - \\Phi(-0.424171) + \\Phi(-5.575829) \\approx 1 - 0.335680 + 1.22 \\times 10^{-8} \\approx 0.664320 $$\n相应的 $F1$ 分数是：\n$$ F1_3 = \\frac{2 \\cdot 40 \\cdot 0.664320}{40 + 9.6 + 40 \\cdot 0.664320} = \\frac{53.1456}{49.6 + 26.5728} = \\frac{53.1456}{76.1728} \\approx 0.697746 $$\n\n最后，我们通过对三个单独的分数求平均来计算宏平均 $F1$ 分数：\n$$ \\text{宏平均 } F1 = \\frac{F1_1 + F1_2 + F1_3}{3} $$\n$$ \\text{宏平均 } F1 \\approx \\frac{0.031760 + 0.204232 + 0.697746}{3} = \\frac{0.933738}{3} \\approx 0.311246 $$\n四舍五入到四位有效数字，最终结果是 $0.3112$。",
            "answer": "$$\\boxed{0.3112}$$"
        },
        {
            "introduction": "理论上的收敛是一回事，但实际性能在很大程度上取决于实现选择，例如何时停止一个迭代算法。这个编码练习 () 要求您为 LASSO 问题实现迭代软阈值算法（ISTA），并比较三种常见的停止规则。通过测量准确性（NMSE）、支撑集恢复情况（FDP, MISS）和计算成本，您将直接体验到在计算效率和解的质量之间的权衡。",
            "id": "3446302",
            "problem": "考虑在一个带噪声的线性模型中，使用最小绝对收缩和选择算子 (LASSO) 来恢复稀疏信号。其底层数据生成过程基于以下基本设定：一个测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$，一个 $k$-稀疏的基准真相向量 $x_0 \\in \\mathbb{R}^n$（即 $x_0$ 最多有 $k$ 个非零项），以及一个测量噪声向量 $w \\in \\mathbb{R}^m$，其分量是独立同分布的高斯随机变量，均值为 $0$，方差为 $\\sigma^2$。观测到的测量值为 $y = A x_0 + w$。恢复问题被构建为 LASSO 优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda > 0$ 用于平衡数据保真度和促进稀疏性的正则化。\n\n您必须通过经验比较求解上述问题的近端梯度法（迭代软阈值算法）的三种停止准则：\n- 固定迭代次数停止：始终精确运行 $T_{\\text{fixed}}$ 次迭代。\n- 对偶间隙停止：一旦原始-对偶间隙低于容差 $\\varepsilon_{\\text{dual}}$ 就停止。\n- 基于残差的停止（偏差原则）：一旦残差范数与噪声水平匹配，即当 $\\|A x - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$ 时停止。\n\n迭代软阈值算法 (ISTA) 使用软阈值算子 $S_{\\theta}(\\cdot)$，其按分量定义为 $S_{\\theta}(z)_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\theta, 0\\}$，并执行迭代\n$$\nx^{t+1} \\;=\\; S_{\\lambda \\eta}\\Big(x^{t} - \\eta A^\\top (A x^{t} - y)\\Big),\n$$\n其中 $\\eta > 0$ 是一个步长，选择为 $\\eta = 1/L$，其中 $L$ 是数据保真度项梯度的 Lipschitz 常数，具体为 $L = \\|A\\|_2^2$（$A$ 的谱范数的平方）。\n\n使用 LASSO 问题的 Fenchel 对偶来计算一个有效的对偶下界以及在迭代点 $x$ 处的原始-对偶间隙。设 $r = A x - y$。对偶问题为\n$$\n\\max_{u \\in \\mathbb{R}^m} \\; -\\frac{1}{2} \\|u\\|_2^2 - y^\\top u \\quad \\text{subject to} \\quad \\|A^\\top u\\|_\\infty \\leq \\lambda.\n$$\n给定一个具有残差 $r$ 的原始迭代点 $x$，通过将 $r$ 径向投影到对偶可行集上，构造一个可行的对偶变量：设置 $\\alpha = \\min\\big\\{1, \\lambda / \\|A^\\top r\\|_\\infty \\big\\}$，并定义 $u = \\alpha r$。原始目标值为 $p(x) = \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1$，在 $u$ 处的对偶目标值为 $d(u) = -\\frac{1}{2}\\|u\\|_2^2 - y^\\top u$。原始-对偶间隙则为 $g(x) = p(x) - d(u)$。\n\n对于每种停止准则，通过以下方式量化经验性能：\n- 准确性：归一化均方误差 $\\mathrm{NMSE}(x) = \\|x - x_0\\|_2^2/\\|x_0\\|_2^2$。\n- 过拟合指数：错误发现比例 $\\mathrm{FDP}(x)$，定义为满足 $x_i \\neq 0$ 和 $(x_0)_i = 0$ 的索引 $i$ 的数量除以 $\\max\\{1, |\\mathrm{supp}(x)|\\}$，其中 $\\mathrm{supp}(x) = \\{i : |x_i| > \\theta_{\\text{supp}}\\}$。\n- 欠拟合指数：漏报率 $\\mathrm{MISS}(x)$，定义为满足 $(x_0)_i \\neq 0$ 和 $x_i = 0$ 的索引 $i$ 的数量除以 $|\\mathrm{supp}(x_0)|$，使用相同的支撑集检测规则 $|x_i| > \\theta_{\\text{supp}}$。\n- 与固定迭代基线相比的计算节省量：对于在 $T$ 次迭代后停止的准则，定义 $\\mathrm{SAVE} = (T_{\\text{fixed}} - T)/T_{\\text{fixed}}$。\n\n您必须实现带有三种停止准则的 ISTA，生成如上所述的合成数据，并报告相对于固定迭代基线的上述指标。支撑集检测阈值必须为 $\\theta_{\\text{supp}} = 10^{-3}$。\n\n对所有测试用例使用以下通用设置：维度 $n = 256$，测量次数 $m = 120$，稀疏度 $k = 20$，固定迭代预算 $T_{\\text{fixed}} = 200$，以及通过经验规则选择的软阈值参数 $\\lambda = \\sigma \\sqrt{2 \\log n}$。构造 $A$ 使其条目 $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ 独立，随机选择大小为 $k$ 的支撑集，并将 $x_0$ 的非零分量设置为在 $[1, 2]$ 上均匀分布并带有随机符号的独立样本。\n\n测试套件：\n- 案例 1：随机种子 $0$，噪声水平 $\\sigma = 0.02$，对偶间隙容差 $\\varepsilon_{\\text{dual}} = 10^{-6}$，偏差参数 $\\tau = 1.05$。\n- 案例 2：随机种子 $1$，噪声水平 $\\sigma = 0.1$，对偶间隙容差 $\\varepsilon_{\\text{dual}} = 10^{-4}$，偏差参数 $\\tau = 1.05$。\n- 案例 3：随机种子 $2$，噪声水平 $\\sigma = 0.3$，对偶间隙容差 $\\varepsilon_{\\text{dual}} = 10^{-3}$，偏差参数 $\\tau = 1.05$。\n\n对于每个案例，运行 ISTA 三次：使用固定迭代次数停止（$T_{\\text{fixed}}$ 次迭代），使用对偶间隙停止（一旦 $g(x) \\leq \\varepsilon_{\\text{dual}}$ 或在不满足时达到 $T_{\\text{fixed}}$ 次迭代后停止），以及通过偏差原则使用基于残差的停止（一旦 $\\|A x - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$ 或在不满足时达到 $T_{\\text{fixed}}$ 次迭代后停止）。为每个案例计算以下 $8$ 个浮点输出，并按所列出的确切顺序排列：\n- $\\mathrm{SAVE}_{\\text{dual}}$,\n- $\\mathrm{SAVE}_{\\text{res}}$,\n- $\\mathrm{NMSE}_{\\text{dual}} - \\mathrm{NMSE}_{\\text{fixed}}$,\n- $\\mathrm{NMSE}_{\\text{res}} - \\mathrm{NMSE}_{\\text{fixed}}$,\n- $\\mathrm{FDP}_{\\text{dual}}$,\n- $\\mathrm{MISS}_{\\text{dual}}$,\n- $\\mathrm{FDP}_{\\text{res}}$,\n- $\\mathrm{MISS}_{\\text{res}}$.\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[\\text{result1},\\text{result2},\\text{result3},\\ldots]$）。将有 $3$ 个案例，因此列表必须包含 $24$ 个浮点数。不涉及物理单位。所有角度（如果存在）必须以弧度为单位，但此处未使用角度。所有百分比必须表示为小数，而不是使用百分号。程序必须是自包含的，并且不得读取任何外部输入。",
            "solution": "用户提供的问题是稀疏信号恢复和计算优化领域中一个定义明确的数值实验。它要求对应用于 LASSO 问题的迭代软阈值算法 (ISTA) 的三种不同停止准则进行经验比较。该问题具有科学依据，是自包含的，并且所有参数和过程都得到了明确的规定。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n问题的核心是从带噪声的线性测量值 $y \\in \\mathbb{R}^m$ 中求解稀疏信号 $x \\in \\mathbb{R}^n$ 的 LASSO 优化问题。数据生成模型由下式给出\n$$\ny = A x_0 + w,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个测量矩阵，$x_0 \\in \\mathbb{R}^n$ 是基准真相的 $k$-稀疏信号，$w \\in \\mathbb{R}^m$ 是一个独立同分布 (i.i.d.) 高斯噪声向量，满足 $w_i \\sim \\mathcal{N}(0, \\sigma^2)$。LASSO 公式通过求解以下问题来寻找 $x_0$ 的估计值：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) := \\underbrace{\\frac{1}{2} \\|A x - y\\|_2^2}_{\\text{数据保真度}} + \\underbrace{\\lambda \\|x\\|_1}_{\\text{稀疏性正则化器}}.\n$$\n这里，$\\lambda > 0$ 是一个正则化参数，它平衡了拟合测量值 $y$ 和在解向量 $x$ 中强制稀疏性之间的权衡。\n\n所选的求解器是迭代软阈值算法 (ISTA)，这是一种近端梯度法。数据保真度项的梯度为 $\\nabla(\\frac{1}{2} \\|A x - y\\|_2^2) = A^\\top(Ax-y)$。该项的梯度是 Lipschitz 连续的，其常数为 $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$，其中 $\\|A\\|_2$ 是 $A$ 的谱范数。ISTA 的过程是对目标函数的光滑部分执行梯度下降步骤，然后应用非光滑 $\\ell_1$-范数的近端算子，即软阈值算子 $S_{\\theta}(\\cdot)$。当步长为 $\\eta = 1/L$ 时，第 $t+1$ 次迭代的更新规则为：\n$$\nx^{t+1} = S_{\\lambda \\eta}\\left(x^t - \\eta A^\\top(Ax^t - y)\\right),\n$$\n其中软阈值算子是逐元素应用的：$S_{\\theta}(z)_i = \\operatorname{sign}(z_i) \\max(|z_i| - \\theta, 0)$。所有运行都以 $x^0 = \\mathbf{0}$ 进行初始化。\n\n我们将比较此迭代过程的三种停止准则：\n1.  **固定迭代次数停止**：算法在固定的迭代次数 $T = T_{\\text{fixed}}$ 后终止。这作为计算成本和准确性的基线。\n2.  **对偶间隙停止**：此准则利用了对偶理论。在迭代点 $x^t$ 处的原始目标值为 $p(x^t) = \\frac{1}{2}\\|Ax^t - y\\|_2^2 + \\lambda\\|x^t\\|_1$。Fenchel 对偶问题是 $\\max_{u \\in \\mathbb{R}^m} \\; -\\frac{1}{2} \\|u\\|_2^2 - y^\\top u$ 且满足 $\\|A^\\top u\\|_\\infty \\leq \\lambda$。根据原始迭代点 $x^t$，我们构造一个对偶可行变量 $u^t$。设残差为 $r^t = Ax^t - y$。通过设置 $u^t = \\alpha r^t$ 来构造一个对偶可行变量 $u^t$，其中 $\\alpha = \\min\\left(1, \\frac{\\lambda}{\\|A^\\top r^t\\|_\\infty}\\right)$。这个选择确保了 $\\|A^\\top u^t\\|_\\infty \\le \\lambda$。对偶目标值为 $d(u^t) = -\\frac{1}{2}\\|u^t\\|_2^2 - y^\\top u^t$。原始-对偶间隙为 $g(x^t) = p(x^t) - d(u^t) \\geq 0$。当 $g(x^t) \\leq \\varepsilon_{\\text{dual}}$ 时算法停止。\n3.  **基于残差的停止（偏差原则）**：此准则的动机源于噪声的统计特性。由于噪声能量期望为 $\\|w\\|_2^2 \\approx m \\sigma^2$，我们期望真实解的残差范数 $\\|Ax_0 - y\\|_2 = \\|-w\\|_2$ 约等于 $\\sigma\\sqrt{m}$。因此，当当前迭代点的残差范数 $\\|Ax^t-y\\|_2$ 降至与此噪声水平相关的阈值以下时，即 $\\|Ax^t - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$ 时，算法停止，其中 $\\tau \\ge 1$ 是一个容差因子。\n\n对于所有准则，作为安全措施，最多执行 $T_{\\text{fixed}}$ 次迭代。\n\n每个恢复解 $x$ 的经验性能使用几个指标进行评估：\n-   **归一化均方误差**：$\\mathrm{NMSE}(x) = \\frac{\\|x - x_0\\|_2^2}{\\|x_0\\|_2^2}$，用于衡量重建准确性。\n-   **支撑集恢复指标**：为了评估 $x_0$ 的稀疏模式被恢复得如何，我们首先为一个小的阈值 $\\theta_{\\text{supp}}=10^{-3}$ 定义 $x$ 的估计支撑集为 $\\mathrm{supp}(x) = \\{i : |x_i| > \\theta_{\\text{supp}}\\}$。\n    -   **错误发现比例**：$\\mathrm{FDP}(x) = \\frac{|\\{i : x_i \\neq 0 \\text{ and } (x_0)_i = 0\\}|}{\\max(1, |\\mathrm{supp}(x)|)}$，用于衡量过拟合（虚假的非零项）。\n    -   **漏报率**：$\\mathrm{MISS}(x) = \\frac{|\\{i : (x_0)_i \\neq 0 \\text{ and } x_i = 0\\}|}{|\\mathrm{supp}(x_0)|}$，用于衡量欠拟合（漏掉的真实非零项）。注意，$x_i=0$ 被解释为 $|x_i| \\le \\theta_{\\text{supp}}$。\n-   **计算节省量**：对于在 $T$ 次迭代后停止的准则，$\\mathrm{SAVE} = \\frac{T_{\\text{fixed}} - T}{T_{\\text{fixed}}}$。\n\n该实现根据提供的规格（$n=256$, $m=120$, $k=20$, $T_{\\text{fixed}}=200$）为每个测试用例生成合成数据。矩阵 $A$ 具有来自 $\\mathcal{N}(0, 1/m)$ 的独立同分布条目。基准真相 $x_0$ 具有一个随机选择的大小为 $k$ 的支撑集，其非零值从 $[1, 2]$ 上的均匀分布中抽取，并带有随机符号。正则化参数通过经验规则 $\\lambda = \\sigma \\sqrt{2 \\log n}$ 设置。对于所提供的三个测试用例中的每一个，我们都使用每种停止准则运行 ISTA，计算指定的指标，并计算相对于固定迭代基线的所需 $8$ 个输出值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nTHETA_SUPP = 1e-3\n\ndef soft_threshold(z, theta):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - theta, 0)\n\ndef calculate_nmse(x_est, x0):\n    \"\"\"Calculates Normalized Mean-Squared Error.\"\"\"\n    norm_x0_sq = np.linalg.norm(x0)**2\n    if norm_x0_sq == 0:\n        return np.linalg.norm(x_est)**2\n    return np.linalg.norm(x_est - x0)**2 / norm_x0_sq\n\ndef calculate_fdp_miss(x_est, x0):\n    \"\"\"Calculates False Discovery Proportion and Miss Rate.\"\"\"\n    supp_x0 = np.where(x0 != 0)[0]\n    supp_x_est = np.where(np.abs(x_est) > THETA_SUPP)[0]\n\n    num_supp_x0 = len(supp_x0)\n    num_supp_x_est = len(supp_x_est)\n\n    false_discoveries = len(np.setdiff1d(supp_x_est, supp_x0))\n    misses = len(np.setdiff1d(supp_x0, supp_x_est))\n\n    fdp = false_discoveries / max(1, num_supp_x_est)\n    miss_rate = misses / num_supp_x0 if num_supp_x0 > 0 else 0.0\n\n    return fdp, miss_rate\n\ndef run_ista(A, y, x0, lambda_val, eta, T_fixed, stopping_rule, **kwargs):\n    \"\"\"Runs the ISTA algorithm with a specified stopping rule.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    # Unpack stopping rule parameters from kwargs\n    eps_dual = kwargs.get('eps_dual')\n    tau = kwargs.get('tau')\n    sigma = kwargs.get('sigma')\n\n    for t in range(T_fixed):\n        # ISTA update step\n        residual = A @ x - y\n        grad = A.T @ residual\n        z = x - eta * grad\n        x = soft_threshold(z, lambda_val * eta)\n\n        # Check stopping rule\n        if stopping_rule == 'dual_gap':\n            # Calculate primal-dual gap\n            r = A @ x - y\n            At_r = A.T @ r\n            At_r_inf_norm = np.linalg.norm(At_r, np.inf)\n            \n            # Avoid division by zero\n            alpha = 1.0 if At_r_inf_norm  1e-12 else min(1.0, lambda_val / At_r_inf_norm)\n            \n            u = alpha * r\n            primal_obj = 0.5 * np.dot(r, r) + lambda_val * np.linalg.norm(x, 1)\n            dual_obj = -0.5 * np.dot(u, u) - np.dot(y, u)\n            gap = primal_obj - dual_obj\n            \n            if gap = eps_dual:\n                return x, t + 1\n                \n        elif stopping_rule == 'residual':\n            # Check discrepancy principle\n            res_norm = np.linalg.norm(A @ x - y, 2)\n            res_thresh = tau * sigma * np.sqrt(m)\n            if res_norm = res_thresh:\n                return x, t + 1\n    \n    # If loop completes, return result after T_fixed iterations\n    return x, T_fixed\n\ndef run_case(seed, sigma, eps_dual, tau):\n    \"\"\"Runs a single test case with all three stopping rules.\"\"\"\n    # Common settings from problem statement\n    n = 256\n    m = 120\n    k = 20\n    T_fixed = 200\n\n    # Generate synthetic data\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n)) / np.sqrt(m)\n    \n    x0 = np.zeros(n)\n    support = rng.choice(n, k, replace=False)\n    values = rng.uniform(1, 2, size=k)\n    signs = rng.choice([-1, 1], size=k)\n    x0[support] = values * signs\n    \n    w = rng.standard_normal(m) * sigma\n    y = A @ x0 + w\n    \n    # Algorithm parameters\n    lambda_val = sigma * np.sqrt(2 * np.log(n))\n    L = np.linalg.norm(A, 2)**2\n    eta = 1.0 / L\n\n    # --- Run with Fixed-Iteration Stopping ---\n    x_fixed, T_actual_fixed = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'fixed')\n\n    # --- Run with Dual-Gap Stopping ---\n    x_dual, T_dual = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'dual_gap', eps_dual=eps_dual)\n\n    # --- Run with Residual-Based Stopping ---\n    x_res, T_res = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'residual', tau=tau, sigma=sigma)\n\n    # Calculate metrics for the baseline\n    nmse_fixed = calculate_nmse(x_fixed, x0)\n\n    # Calculate metrics for dual-gap rule\n    nmse_dual = calculate_nmse(x_dual, x0)\n    fdp_dual, miss_dual = calculate_fdp_miss(x_dual, x0)\n    save_dual = (T_fixed - T_dual) / T_fixed\n    nmse_diff_dual = nmse_dual - nmse_fixed\n\n    # Calculate metrics for residual-based rule\n    nmse_res = calculate_nmse(x_res, x0)\n    fdp_res, miss_res = calculate_fdp_miss(x_res, x0)\n    save_res = (T_fixed - T_res) / T_fixed\n    nmse_diff_res = nmse_res - nmse_fixed\n    \n    return [\n        save_dual,\n        save_res,\n        nmse_diff_dual,\n        nmse_diff_res,\n        fdp_dual,\n        miss_dual,\n        fdp_res,\n        miss_res\n    ]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # (seed, sigma, eps_dual, tau)\n        (0, 0.02, 1e-6, 1.05),\n        (1, 0.1,  1e-4, 1.05),\n        (2, 0.3,  1e-3, 1.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = run_case(*case)\n        all_results.extend(case_results)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多压缩感知算法的成功在理论上是由测量矩阵的性质保证的，其中最著名的是限制等距性质（RIP）。这个练习 () 提供了一种动手实践的方法，将这个抽象的理论概念与可观察的性能联系起来。您将编写代码来估计 RIP 常数的一个经验代理，并将其与正交匹配追踪（OMP）算法的成功率相关联，从而直观地看到一个更好（更小）的 RIP 常数如何带来更可靠的信号恢复。",
            "id": "3446301",
            "problem": "实现一个完整、可运行的程序，该程序通过实证研究受限等距性质（RIP）常数的一个代理指标与压缩感知中稀疏恢复算法的观测性能之间的相关性。纯粹在数学模型中进行操作，其中测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 作用于一个 $k$-稀疏向量 $x_{0} \\in \\mathbb{R}^{n}$，产生无噪声测量值 $y = A x_{0}$。目标是估计一个经验受限等距常数代理，并将其与经验恢复概率和重构误差联系起来。\n\n仅使用以下基本基础和定义：\n- 如果一个向量 $x \\in \\mathbb{R}^{n}$ 最多有 $k$ 个非零项，则称其为 $k$-稀疏的。\n- 阶数为 $k$ 的受限等距性质（RIP）常数，记为 $\\delta_{k}$，是满足以下条件的最小非负数：对于所有 $k$-稀疏向量 $x$，\n$$\n(1 - \\delta_{k}) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta_{k}) \\lVert x \\rVert_{2}^{2}.\n$$\n- 等价地，对于任何大小为 $\\lvert S \\rvert = k$ 的支撑集 $S \\subset \\{1,\\dots,n\\}$，如果 $A_{S}$表示由 $A$ 中索引为 $S$ 的列构成的子矩阵，那么当 $A$ 被适当缩放时，格拉姆矩阵 $G_{S} = A_{S}^{\\top} A_{S}$ 的所有特征值都位于区间 $[1 - \\delta_{k}, 1 + \\delta_{k}]$ 内。因此，可以通过对支撑集 $S$进行随机抽样，并计算 $G_{S}$ 的特征值与 1 的极值偏差来获得 $\\delta_{k}$ 的经验代理。\n\n设计程序以对每个测试用例执行以下操作：\n1. 构造一个指定类型的测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$，并将其列归一化，使每列的 $\\ell_{2}$ 范数为 1。\n2. 通过蒙特卡洛方法估计经验RIP代理 $\\widehat{\\delta}_{k}$：\n   - 对从 $\\{1,\\dots,n\\}$ 中无放回均匀采样的 $N_{\\text{RIP}}$ 个大小为 $k$ 的随机支撑集 $S$ 重复以下操作：\n     - 构成 $G_{S} = A_{S}^{\\top} A_{S}$，并计算其最小和最大特征值 $\\lambda_{\\min}(G_{S})$ 和 $\\lambda_{\\max}(G_{S})$。\n     - 对于该支撑集，定义偏差 $d(S) = \\max\\{ \\lvert \\lambda_{\\min}(G_{S}) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_{S}) - 1 \\rvert \\}$。\n   - 在所有采样的支撑集上，设 $\\widehat{\\delta}_{k} = \\max_{S} d(S)$。\n3. 在 $T$ 次独立试验中，使用正交匹配追踪（OMP；正交匹配追踪是一种贪婪稀疏近似算法）评估经验恢复性能：\n   - 对于每次试验，通过均匀随机选择一个大小为 $k$ 的支撑集，在支撑集上赋予独立的标准正态系数，并缩放至 $\\lVert x_{0} \\rVert_{2} = 1$，来生成一个随机的 $k$-稀疏向量 $x_{0} \\in \\mathbb{R}^{n}$。\n   - 生成 $y = A x_{0}$（无噪声）。\n   - 使用 $k$ 次 OMP 迭代从 $(A,y)$ 中恢复 $\\widehat{x}$。\n   - 计算相对重构误差 $e = \\lVert \\widehat{x} - x_{0} \\rVert_{2} / \\lVert x_{0} \\rVert_{2}$。\n   - 如果 $e \\le \\tau$，则声明为成功，其中 $\\tau$ 是一个固定阈值。\n   - 汇总经验成功概率 $\\widehat{p} = (\\text{成功次数}) / T$ 和平均相对误差 $\\overline{e} = \\frac{1}{T} \\sum e$。\n4. 将每个测试用例视为相关性计算中的一个样本，通过计算 $\\widehat{\\delta}_{k}$ 值向量与 $\\widehat{p}$ 值向量之间的皮尔逊相关系数，以及 $\\widehat{\\delta}_{k}$ 值向量与 $\\overline{e}$ 值向量之间的皮尔逊相关系数，来关联经验RIP代理与整个测试套件的性能。\n\n使用的矩阵构造方法（构造后所有列必须归一化为单位 $\\ell_{2}$ 范数）：\n- 类型 “gaussian”：元素独立地从标准正态分布中抽取，然后对列进行归一化。\n- 类型 “bernoulli”：元素独立地从等概率取值 $\\{-1,+1\\}$ 的拉德马赫分布中抽取，然后对列进行归一化。\n- 类型 “correlated”：从一个高斯矩阵开始，然后通过以下方式引入列相关性\n$$\nA = (1 - \\rho) A_{0} + \\rho \\, v \\mathbf{1}^{\\top},\n$$\n其中 $A_{0}$ 具有独立的标准正态元素，$v \\in \\mathbb{R}^{m}$ 是一个随机标准正态向量，$\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全1向量；然后对列进行归一化。\n\n算法约束：\n- 使用正交匹配追踪（OMP）进行恢复。实现的OMP在每次迭代中，选择与当前残差具有最大绝对相关性的列，更新活动集，在活动集上解决最小二乘子问题，并更新残差。在 $k$ 次迭代后或当残差范数低于数值容差时停止。\n- 使用适用于对称矩阵的特征值计算方法来评估 $G_{S}$ 的 $\\lambda_{\\min}$ 和 $\\lambda_{\\max}$。\n\n在所有实验中使用的数值参数：\n- 成功容差：$\\tau = 10^{-3}$。\n- OMP的残差停止容差：$10^{-8}$。\n- 每个测试用例中用于RIP代理的蒙特卡洛支撑集数量：$N_{\\text{RIP}} = 80$。\n- 每个测试用例的恢复试验次数：$T = 40$。\n- 严格按照测试套件中的规定使用提供的种子以保证可复现性。\n\n要实现的测试套件：\n- 用例 A：类型 “gaussian”，$m = 32$，$n = 64$，$k = 4$，$\\rho$ 被忽略，种子 $= 1$。\n- 用例 B：类型 “gaussian”，$m = 32$，$n = 64$，$k = 10$，$\\rho$ 被忽略，种子 $= 2$。\n- 用例 C：类型 “bernoulli”，$m = 32$，$n = 64$，$k = 8$，$\\rho$ 被忽略，种子 $= 3$。\n- 用例 D：类型 “correlated”，$m = 32$，$n = 64$，$k = 8$，$\\rho = 0.6$，种子 $= 4$。\n\n最终输出规范：\n- 对于每个测试用例，生成一个包含三个浮点数的列表 $[\\widehat{\\delta}_{k}, \\widehat{p}, \\overline{e}]$，每个浮点数四舍五入到三位小数。\n- 在所有测试用例之后，计算另外两个浮点数：$\\widehat{\\delta}_{k}$ 值向量与 $\\widehat{p}$ 值向量之间的皮尔逊相关系数，以及 $\\widehat{\\delta}_{k}$ 值向量与 $\\overline{e}$ 值向量之间的皮尔逊相关系数，每个系数四舍五入到三位小数。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表整体用方括号括起来。列表的第一项是按 A, B, C, D 顺序排列的各测试用例的三元组列表，其后是两个相关系数。例如，结构必须是\n$$\n\\big[ \\big[ [\\widehat{\\delta}_{k}^{A}, \\widehat{p}^{A}, \\overline{e}^{A}], [\\widehat{\\delta}_{k}^{B}, \\widehat{p}^{B}, \\overline{e}^{B}], [\\widehat{\\delta}_{k}^{C}, \\widehat{p}^{C}, \\overline{e}^{C}], [\\widehat{\\delta}_{k}^{D}, \\widehat{p}^{D}, \\overline{e}^{D}] \\big], r_{\\delta,p}, r_{\\delta,e} \\big],\n$$\n打印在一行中，所有浮点数四舍五入到三位小数。不涉及单位。不涉及角度。不要用百分比表示任何量；只使用小数。",
            "solution": "我们从受限等距性质（RIP）的核心定义开始。对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，阶数为 $k$ 的RIP常数 $\\delta_{k}$ 是最小的 $\\delta$，使得对于所有 $k$-稀疏向量 $x \\in \\mathbb{R}^{n}$，满足\n$$\n(1 - \\delta) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta) \\lVert x \\rVert_{2}^{2}.\n$$\n等价地，对于任何大小为 $\\lvert S \\rvert = k$ 的支撑集 $S \\subset \\{1,\\dots,n\\}$，如果 $A_{S}$ 是由 $A$ 中索引为 $S$ 的列构成的子矩阵，且 $G_{S} = A_{S}^{\\top} A_{S}$ 是其格拉姆矩阵，则 $\\delta_{k}$ 是使得 $G_{S}$ 的每个特征值都属于 $[1 - \\delta, 1 + \\delta]$ 的最小 $\\delta$。这可以从恒等式 $\\lVert A x \\rVert_{2}^{2} = x^{\\top} A^{\\top} A x$ 和瑞利商界得出，因为对于支撑在 $S$ 上的 $k$-稀疏向量 $x$，有 $\\lVert A x \\rVert_{2}^{2} = x_{S}^{\\top} G_{S} x_{S}$，而其在所有非零 $x_{S}$ 上的极值恰好是 $G_{S}$ 的极值特征值。因此，可以通过对支撑集 $S$ 进行抽样，并测量 $G_{S}$ 的谱与 1 的偏离程度来估计 $\\delta_{k}$ 的代理。\n\n这引出了以下经验估计器。固定 $N_{\\text{RIP}}$，无放回均匀随机抽样大小为 $k$ 的支撑集 $S_{1},\\dots,S_{N_{\\text{RIP}}}$，对每个 $S_{i}$ 计算格拉姆矩阵 $G_{S_{i}} = A_{S_{i}}^{\\top} A_{S_{i}}$，找出 $\\lambda_{\\min}(G_{S_{i}})$ 和 $\\lambda_{\\max}(G_{S_{i}})$，并定义\n$$\nd(S_{i}) = \\max \\{ \\lvert \\lambda_{\\min}(G_{S_{i}}) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_{S_{i}}) - 1 \\rvert \\}.\n$$\n经验RIP代理则为\n$$\n\\widehat{\\delta}_{k} = \\max_{i \\in \\{1,\\dots,N_{\\text{RIP}}\\}} d(S_{i}).\n$$\n将 $A$ 的列归一化为单位 $\\ell_{2}$ 范数是为了保持尺度一致，确保当列近似正交时，格拉姆矩阵 $G_{S}$ 趋近于单位矩阵，使得 1 成为其特征值的自然参考值。\n\n对于恢复性能，我们采用正交匹配追踪（OMP）。正交匹配追踪（OMP）是一种贪婪算法，其合理性在于，对于一个稀疏的 $x_{0}$，由 $x_{0}$ 的支撑集索引的 $A$ 的列往往与测量值 $y = A x_{0}$ 有很强的相关性。具体来说，OMP 迭代地：\n- 计算与残差 $r$ 的相关性 $A^{\\top} r$，\n- 选择具有最大绝对相关性幅值的列索引，\n- 扩充活动集，\n- 在活动集上求解最小二乘问题以更新当前估计，\n- 并更新残差。\n在 $k$ 次迭代后，在有利的条件下，OMP 可以完美地恢复支撑集和系数。虽然严格的保证是以RIP或互相关性界来表述的，但在这里我们研究的是经验性能及其与经验RIP代理的相关性。\n\n该程序为每个测试用例实现以下步骤：\n1. 生成指定类型的矩阵 $A$：\n   - “gaussian”：元素 $A_{ij} \\sim \\mathcal{N}(0,1)$。\n   - “bernoulli”：元素 $A_{ij} \\in \\{-1,+1\\}$，等概率。\n   - “correlated”：抽取 $A_{0}$，其元素为 $\\mathcal{N}(0,1)$，并抽取 $v \\sim \\mathcal{N}(0, I_{m})$，然后设置 $A = (1 - \\rho) A_{0} + \\rho \\, v \\mathbf{1}^{\\top}$，其中 $\\mathbf{1}$ 是 $\\mathbb{R}^{n}$ 中的全1向量。\n   构造后，将 $A$ 的每一列归一化为单位 $\\ell_{2}$ 范数以稳定尺度。\n2. 使用 $N_{\\text{RIP}}$ 个随机支撑集计算 $\\widehat{\\delta}_{k}$，方法是构建 $G_{S}$ 并使用适用于实对称矩阵的对称特征值程序提取其极值特征值。\n3. 进行 $T$ 次独立试验，每次试验中，在一个大小为 $k$ 的随机支撑集上抽取具有标准正态系数的随机 $k$-稀疏向量 $x_{0}$，将 $x_{0}$ 归一化为单位 $\\ell_{2}$ 范数，设置 $y = A x_{0}$，并运行 $k$ 步OMP（如果残差范数低于 $10^{-8}$ 则提前停止）。计算相对误差 $e = \\lVert \\widehat{x} - x_{0} \\rVert_{2} / \\lVert x_{0} \\rVert_{2}$ 和成功指示符 $\\mathbb{I}\\{ e \\le \\tau \\}$，其中 $\\tau = 10^{-3}$。汇总经验成功概率 $\\widehat{p}$ 和平均相对误差 $\\overline{e}$。\n4. 处理完所有测试用例后，计算列表 $[\\widehat{\\delta}_{k}^{A}, \\widehat{\\delta}_{k}^{B}, \\widehat{\\delta}_{k}^{C}, \\widehat{\\delta}_{k}^{D}]$ 和 $[\\widehat{p}^{A}, \\widehat{p}^{B}, \\widehat{p}^{C}, \\widehat{p}^{D}]$ 之间的皮尔逊相关系数 $r_{\\delta,p}$，以及 $\\widehat{\\delta}_{k}$ 值列表和 $[\\overline{e}^{A}, \\overline{e}^{B}, \\overline{e}^{C}, \\overline{e}^{D}]$ 之间的皮尔逊相关系数 $r_{\\delta,e}$。如果由于其中一个列表的方差为零导致相关系数的分母在数值上为零，则将相关性定义为 $0$ 以避免未定义的值。\n\n测试套件规定：\n- 用例 A：类型 “gaussian”，$m = 32$，$n = 64$，$k = 4$，种子 $= 1$。\n- 用例 B：类型 “gaussian”，$m = 32$，$n = 64$，$k = 10$，种子 $= 2$。\n- 用例 C：类型 “bernoulli”，$m = 32$，$n = 64$，$k = 8$，种子 $= 3$。\n- 用例 D：类型 “correlated”，$m = 32$，$n = 64$，$k = 8$，$\\rho = 0.6$，种子 $= 4$。\n\n对于每个用例，我们使用 $N_{\\text{RIP}} = 80$ 个支撑集，$T = 40$ 次试验，$\\tau = 10^{-3}$，以及OMP残差容差 $10^{-8}$。\n\n最终程序输出单行，其结构为\n$$\n\\big[ \\big[ [\\widehat{\\delta}_{k}^{A}, \\widehat{p}^{A}, \\overline{e}^{A}], [\\widehat{\\delta}_{k}^{B}, \\widehat{p}^{B}, \\overline{e}^{B}], [\\widehat{\\delta}_{k}^{C}, \\widehat{p}^{C}, \\overline{e}^{C}], [\\widehat{\\delta}_{k}^{D}, \\widehat{p}^{D}, \\overline{e}^{D}] \\big], r_{\\delta,p}, r_{\\delta,e} \\big],\n$$\n所有浮点数四舍五入到三位小数。这一设计将经验RIP代理与测量的恢复性能直接联系起来，并量化了跨越结构上不同矩阵集的相关性，将研究建立在核心定义和蒙特卡洛估计的基础上，而不依赖于未经证实的启发式方法或无关的假设。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef normalize_columns(A: np.ndarray) -> np.ndarray:\n    \"\"\"Normalize columns of A to unit l2 norm.\"\"\"\n    norms = np.linalg.norm(A, axis=0)\n    norms[norms == 0.0] = 1.0\n    return A / norms\n\ndef make_matrix(m: int, n: int, kind: str, rho: float, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"Construct matrix A of specified kind and normalize columns.\"\"\"\n    if kind == \"gaussian\":\n        A = rng.standard_normal((m, n))\n    elif kind == \"bernoulli\":\n        A = rng.choice([-1.0, 1.0], size=(m, n))\n    elif kind == \"correlated\":\n        A0 = rng.standard_normal((m, n))\n        v = rng.standard_normal((m, 1))\n        A = (1.0 - rho) * A0 + rho * v @ np.ones((1, n))\n    else:\n        raise ValueError(f\"Unknown matrix kind: {kind}\")\n    A = normalize_columns(A)\n    return A\n\ndef empirical_delta_k(A: np.ndarray, k: int, num_samples: int, rng: np.random.Generator) -> float:\n    \"\"\"Monte Carlo estimate of empirical RIP constant proxy via Gram eigenvalues.\"\"\"\n    m, n = A.shape\n    if k = 0 or k > n:\n        return float('nan')\n    delta_max = 0.0\n    # Precompute A^T A could be heavy; we sample supports and compute Gram via A_S.T @ A_S\n    for _ in range(num_samples):\n        S = rng.choice(n, size=k, replace=False)\n        AS = A[:, S]\n        G = AS.T @ AS  # symmetric k x k\n        # Use eigenvalues for symmetric matrices\n        eigvals = np.linalg.eigvalsh(G)\n        dev = max(abs(eigvals[0] - 1.0), abs(eigvals[-1] - 1.0))\n        if dev > delta_max:\n            delta_max = float(dev)\n    return float(delta_max)\n\ndef omp(A: np.ndarray, y: np.ndarray, k: int, tol: float = 1e-8) -> np.ndarray:\n    \"\"\"Orthogonal Matching Pursuit up to k iterations or until residual below tol.\"\"\"\n    m, n = A.shape\n    residual = y.copy()\n    support = []\n    x_s = np.array([])\n    for _ in range(k):\n        correlations = A.T @ residual\n        # Zero out already selected to avoid reselection\n        if support:\n            correlations[np.array(support, dtype=int)] = 0.0\n        idx = int(np.argmax(np.abs(correlations)))\n        # Stop if idx already selected (degenerate), though we zeroed it; safeguard anyway\n        if idx in support:\n            break\n        support.append(idx)\n        AS = A[:, support]\n        # Least squares on active set\n        x_s, _, _, _ = np.linalg.lstsq(AS, y, rcond=None)\n        residual = y - AS @ x_s\n        if np.linalg.norm(residual) = tol:\n            break\n    x_hat = np.zeros(n)\n    if support:\n        x_hat[np.array(support, dtype=int)] = x_s\n    return x_hat\n\ndef generate_k_sparse(n: int, k: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"Generate a random k-sparse unit-norm vector with standard normal nonzeros.\"\"\"\n    x = np.zeros(n)\n    support = rng.choice(n, size=k, replace=False)\n    coeffs = rng.standard_normal(k)\n    x[support] = coeffs\n    norm = np.linalg.norm(x)\n    if norm == 0:\n        # Extremely unlikely; regenerate\n        return generate_k_sparse(n, k, rng)\n    return x / norm\n\ndef pearson_corr(x: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"Compute Pearson correlation; return 0 if variance is zero.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    sx = np.std(x)\n    sy = np.std(y)\n    if sx  1e-15 or sy  1e-15:\n        return 0.0\n    cov = float(np.mean((x - np.mean(x)) * (y - np.mean(y))))\n    return cov / (sx * sy)\n\ndef run_case(kind: str, m: int, n: int, k: int, rho: float, seed: int,\n             num_rip_samples: int, num_trials: int, tau: float) -> tuple[float, float, float]:\n    \"\"\"Run one test case: build A, estimate delta_k, run OMP trials to get success and mean errors.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = make_matrix(m, n, kind, rho, rng)\n    delta_emp = empirical_delta_k(A, k, num_rip_samples, rng)\n    successes = 0\n    errors = []\n    for _ in range(num_trials):\n        x0 = generate_k_sparse(n, k, rng)\n        y = A @ x0\n        xhat = omp(A, y, k, tol=1e-8)\n        rel_err = float(np.linalg.norm(xhat - x0) / (np.linalg.norm(x0) + 1e-16))\n        errors.append(rel_err)\n        if rel_err = tau:\n            successes += 1\n    success_rate = successes / num_trials\n    mean_rel_error = float(np.mean(errors) if errors else float('nan'))\n    return float(delta_emp), float(success_rate), float(mean_rel_error)\n\ndef format_triplet(triplet: tuple[float, float, float]) -> str:\n    \"\"\"Format a (delta, success_rate, mean_error) triplet to three decimals.\"\"\"\n    return \"[\" + \",\".join(f\"{v:.3f}\" for v in triplet) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (kind, m, n, k, rho, seed)\n    test_cases = [\n        (\"gaussian\", 32, 64, 4, 0.0, 1),\n        (\"gaussian\", 32, 64, 10, 0.0, 2),\n        (\"bernoulli\", 32, 64, 8, 0.0, 3),\n        (\"correlated\", 32, 64, 8, 0.6, 4),\n    ]\n    N_RIP = 80\n    T = 40\n    tau = 1e-3\n\n    results = []\n    for kind, m, n, k, rho, seed in test_cases:\n        delta_emp, success_rate, mean_rel_error = run_case(\n            kind, m, n, k, rho, seed, N_RIP, T, tau\n        )\n        results.append((delta_emp, success_rate, mean_rel_error))\n\n    # Compute correlations across test cases\n    deltas = np.array([r[0] for r in results], dtype=float)\n    succs = np.array([r[1] for r in results], dtype=float)\n    errs = np.array([r[2] for r in results], dtype=float)\n    corr_delta_success = pearson_corr(deltas, succs)\n    corr_delta_error = pearson_corr(deltas, errs)\n\n    # Build final output string\n    per_case_str = \"[\" + \",\".join(format_triplet(r) for r in results) + \"]\"\n    final_str = f\"[{per_case_str},{corr_delta_success:.3f},{corr_delta_error:.3f}]\"\n    print(final_str)\n\nsolve()\n```"
        }
    ]
}