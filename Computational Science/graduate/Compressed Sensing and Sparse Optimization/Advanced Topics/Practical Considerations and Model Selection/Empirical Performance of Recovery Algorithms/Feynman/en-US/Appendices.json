{
    "hands_on_practices": [
        {
            "introduction": "The Restricted Isometry Property (RIP) is a cornerstone of compressed sensing theory, providing sufficient conditions for successful sparse recovery. However, directly calculating the RIP constant $\\delta_k$ for a given matrix is computationally intractable. This practice  bridges the gap between theory and application by guiding you to compute an empirical proxy for $\\delta_k$ through Monte Carlo simulation and correlate it with the observed performance of the Orthogonal Matching Pursuit (OMP) algorithm. This exercise illuminates how the geometric properties of the measurement matrix directly influence an algorithm's practical success.",
            "id": "3446301",
            "problem": "Implement a complete, runnable program that empirically investigates how a proxy for the Restricted Isometry Property (RIP) constant correlates with the observed performance of a sparse recovery algorithm in compressed sensing. Work purely in the mathematical model where a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ acts on a $k$-sparse vector $x_{0} \\in \\mathbb{R}^{n}$ to produce noiseless measurements $y = A x_{0}$. The goal is to estimate an empirical restricted isometry constant proxy, and to relate it to empirical recovery probability and reconstruction error.\n\nUse the following fundamental base and definitions only:\n- A vector $x \\in \\mathbb{R}^{n}$ is $k$-sparse if it has at most $k$ nonzero entries.\n- The Restricted Isometry Property (RIP) constant of order $k$, denoted $\\delta_{k}$, is the smallest nonnegative number such that for all $k$-sparse $x$,\n$$\n(1 - \\delta_{k}) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta_{k}) \\lVert x \\rVert_{2}^{2}.\n$$\n- Equivalently, for any support set $S \\subset \\{1,\\dots,n\\}$ with $\\lvert S \\rvert = k$, if $A_{S}$ denotes the submatrix formed by columns of $A$ indexed by $S$, then the Gram matrix $G_{S} = A_{S}^{\\top} A_{S}$ satisfies that all eigenvalues of $G_{S}$ lie in the interval $[1 - \\delta_{k}, 1 + \\delta_{k}]$ when $A$ is appropriately scaled. Thus, an empirical proxy of $\\delta_{k}$ can be obtained by sampling random supports $S$ and computing the extremal deviations of the eigenvalues of $G_{S}$ from $1$.\n\nDesign the program to do the following for each test case:\n1. Construct a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ of a specified type and normalize its columns to have unit $\\ell_{2}$ norm.\n2. Estimate an empirical RIP proxy $\\widehat{\\delta}_{k}$ by Monte Carlo:\n   - Repeat for $N_{\\text{RIP}}$ random supports $S$ of size $k$ sampled uniformly without replacement from $\\{1,\\dots,n\\}$:\n     - Form $G_{S} = A_{S}^{\\top} A_{S}$, and compute its minimum and maximum eigenvalues, $\\lambda_{\\min}(G_{S})$ and $\\lambda_{\\max}(G_{S})$.\n     - For this support, define the deviation $d(S) = \\max\\{ \\lvert \\lambda_{\\min}(G_{S}) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_{S}) - 1 \\rvert \\}$.\n   - Set $\\widehat{\\delta}_{k} = \\max_{S} d(S)$ over the sampled supports.\n3. Evaluate empirical recovery performance using Orthogonal Matching Pursuit (OMP) on $T$ independent trials:\n   - For each trial, draw a random $k$-sparse vector $x_{0} \\in \\mathbb{R}^{n}$ by choosing a support of size $k$ uniformly at random, assigning independent standard normal coefficients on the support, and scaling to $\\lVert x_{0} \\rVert_{2} = 1$.\n   - Generate $y = A x_{0}$ (noiseless).\n   - Recover $\\widehat{x}$ from $(A,y)$ using $k$ iterations of OMP.\n   - Compute the relative reconstruction error $e = \\lVert \\widehat{x} - x_{0} \\rVert_{2} / \\lVert x_{0} \\rVert_{2}$.\n   - Declare success if $e \\le \\tau$, where $\\tau$ is a fixed threshold.\n   - Aggregate the empirical success probability $\\widehat{p} = (\\text{number of successes}) / T$ and the mean relative error $\\overline{e} = \\frac{1}{T} \\sum e$.\n4. Correlate the empirical RIP proxy with performance across the test suite by computing the Pearson correlation coefficients between the vector of $\\widehat{\\delta}_{k}$ values and the vector of $\\widehat{p}$ values, and between the vector of $\\widehat{\\delta}_{k}$ values and the vector of $\\overline{e}$ values, treating each test case as one sample in the correlation calculation.\n\nMatrix constructions to use (all columns must be normalized to unit $\\ell_{2}$ norm after construction):\n- Type “gaussian”: entries drawn independently from a standard normal distribution and then columns normalized.\n- Type “bernoulli”: entries drawn independently from the Rademacher distribution taking values $\\{-1,+1\\}$ with equal probability and then columns normalized.\n- Type “correlated”: start with a Gaussian matrix and then introduce column correlation by\n$$\nA = (1 - \\rho) A_{0} + \\rho \\, v \\mathbf{1}^{\\top},\n$$\nwhere $A_{0}$ has independent standard normal entries, $v \\in \\mathbb{R}^{m}$ is a random standard normal vector, and $\\mathbf{1} \\in \\mathbb{R}^{n}$ is the all-ones vector; then normalize columns.\n\nAlgorithmic constraints:\n- Use Orthogonal Matching Pursuit (OMP) for recovery. Implement OMP that, at each iteration, selects the column with the largest absolute correlation with the current residual, updates the active set, solves the least-squares subproblem on the active set, and updates the residual. Stop after $k$ iterations or when the residual norm is below a numerical tolerance.\n- Use eigenvalue computations appropriate for symmetric matrices to evaluate $\\lambda_{\\min}$ and $\\lambda_{\\max}$ of $G_{S}$.\n\nNumerical parameters to use in all experiments:\n- Tolerance for success: $\\tau = 10^{-3}$.\n- Residual stopping tolerance for OMP: $10^{-8}$.\n- Number of Monte Carlo supports for the RIP proxy per test case: $N_{\\text{RIP}} = 80$.\n- Number of recovery trials per test case: $T = 40$.\n- Use the provided seeds for reproducibility exactly as stated in the test suite.\n\nTest suite to implement:\n- Case A: type “gaussian”, $m = 32$, $n = 64$, $k = 4$, $\\rho$ ignored, seed $= 1$.\n- Case B: type “gaussian”, $m = 32$, $n = 64$, $k = 10$, $\\rho$ ignored, seed $= 2$.\n- Case C: type “bernoulli”, $m = 32$, $n = 64$, $k = 8$, $\\rho$ ignored, seed $= 3$.\n- Case D: type “correlated”, $m = 32$, $n = 64$, $k = 8$, $\\rho = 0.6$, seed $= 4$.\n\nFinal output specification:\n- For each test case, produce a list of three floats $[\\widehat{\\delta}_{k}, \\widehat{p}, \\overline{e}]$, each rounded to three decimal places.\n- After all test cases, compute two additional floats: the Pearson correlation between the vector of $\\widehat{\\delta}_{k}$ values and the vector of $\\widehat{p}$ values, and the Pearson correlation between the vector of $\\widehat{\\delta}_{k}$ values and the vector of $\\overline{e}$ values, each rounded to three decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the first item being the list of per-test-case triplets in order A, B, C, D, followed by the two correlations. For example, the structure must be\n$$\n\\big[ \\big[ [\\widehat{\\delta}_{k}^{A}, \\widehat{p}^{A}, \\overline{e}^{A}], [\\widehat{\\delta}_{k}^{B}, \\widehat{p}^{B}, \\overline{e}^{B}], [\\widehat{\\delta}_{k}^{C}, \\widehat{p}^{C}, \\overline{e}^{C}], [\\widehat{\\delta}_{k}^{D}, \\widehat{p}^{D}, \\overline{e}^{D}] \\big], r_{\\delta,p}, r_{\\delta,e} \\big],\n$$\nprinted in one line, with all floats rounded to three decimals. No units are involved. Angles are not involved. Express no quantities as percentages; use decimals only.",
            "solution": "We begin from the core definition of the Restricted Isometry Property (RIP). For a matrix $A \\in \\mathbb{R}^{m \\times n}$, the RIP constant of order $k$, $\\delta_{k}$, is the smallest $\\delta$ such that for all $k$-sparse vectors $x \\in \\mathbb{R}^{n}$,\n$$\n(1 - \\delta) \\lVert x \\rVert_{2}^{2} \\le \\lVert A x \\rVert_{2}^{2} \\le (1 + \\delta) \\lVert x \\rVert_{2}^{2}.\n$$\nEquivalently, for any support $S \\subset \\{1,\\dots,n\\}$ with $\\lvert S \\rvert = k$, if $A_{S}$ is the submatrix formed by the columns of $A$ indexed by $S$ and $G_{S} = A_{S}^{\\top} A_{S}$ is the Gram matrix, then $\\delta_{k}$ is the smallest $\\delta$ such that every eigenvalue of $G_{S}$ belongs to $[1 - \\delta, 1 + \\delta]$. This follows from the identity $\\lVert A x \\rVert_{2}^{2} = x^{\\top} A^{\\top} A x$ and Rayleigh quotient bounds, since for $k$-sparse $x$ supported on $S$, $\\lVert A x \\rVert_{2}^{2} = x_{S}^{\\top} G_{S} x_{S}$ and the extremal values over all nonzero $x_{S}$ are exactly the extremal eigenvalues of $G_{S}$. Thus, one can estimate a proxy to $\\delta_{k}$ by sampling supports $S$ and measuring how far the spectrum of $G_{S}$ deviates from $1$.\n\nThis leads to the following empirical estimator. Fix $N_{\\text{RIP}}$, sample supports $S_{1},\\dots,S_{N_{\\text{RIP}}}$ of size $k$ uniformly at random without replacement, compute for each $S_{i}$ the Gram matrix $G_{S_{i}} = A_{S_{i}}^{\\top} A_{S_{i}}$, find $\\lambda_{\\min}(G_{S_{i}})$ and $\\lambda_{\\max}(G_{S_{i}})$, and define\n$$\nd(S_{i}) = \\max \\{ \\lvert \\lambda_{\\min}(G_{S_{i}}) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_{S_{i}}) - 1 \\rvert \\}.\n$$\nThe empirical RIP proxy is then\n$$\n\\widehat{\\delta}_{k} = \\max_{i \\in \\{1,\\dots,N_{\\text{RIP}}\\}} d(S_{i}).\n$$\nColumn normalization of $A$ to unit $\\ell_{2}$ norm is applied to keep the scale consistent, ensuring that when columns are nearly orthogonal, the Gram matrices $G_{S}$ tend to be close to the identity, making $1$ a natural reference value for their eigenvalues.\n\nFor recovery performance, we adopt Orthogonal Matching Pursuit (OMP). Orthogonal Matching Pursuit (OMP) is a greedy algorithm justified by the concept that, for a sparse $x_{0}$, the columns of $A$ indexed by the support of $x_{0}$ tend to have large correlations with the measurements $y = A x_{0}$. Specifically, OMP iteratively:\n- computes correlations $A^{\\top} r$ with the residual $r$,\n- selects the column index with maximum absolute correlation magnitude,\n- augments the active set,\n- solves a least-squares problem restricted to the active set to update the current estimate,\n- and updates the residual.\nAfter $k$ iterations, under favorable conditions, OMP can perfectly recover the support and coefficients. While rigorous guarantees are stated in terms of RIP or mutual coherence bounds, here we are investigating empirical performance and its correlation with the empirical RIP proxy.\n\nThe program implements the following steps for each test case:\n1. Generate the matrix $A$ of the specified type:\n   - “gaussian”: entries $A_{ij} \\sim \\mathcal{N}(0,1)$.\n   - “bernoulli”: entries $A_{ij} \\in \\{-1,+1\\}$ with equal probability.\n   - “correlated”: draw $A_{0}$ with $\\mathcal{N}(0,1)$ entries and $v \\sim \\mathcal{N}(0, I_{m})$, then set $A = (1 - \\rho) A_{0} + \\rho \\, v \\mathbf{1}^{\\top}$, where $\\mathbf{1}$ is the all-ones vector in $\\mathbb{R}^{n}$.\n   After construction, normalize each column of $A$ to have unit $\\ell_{2}$ norm to stabilize scale.\n2. Compute $\\widehat{\\delta}_{k}$ using $N_{\\text{RIP}}$ random supports, by forming $G_{S}$ and extracting its extremal eigenvalues using symmetric eigenvalue routines appropriate for real symmetric matrices.\n3. For $T$ independent trials, draw a random $k$-sparse $x_{0}$ with standard normal coefficients on a random support of size $k$, normalize $x_{0}$ to unit $\\ell_{2}$ norm, set $y = A x_{0}$, and run $k$-step OMP (with early stopping if the residual norm falls below $10^{-8}$). Compute the relative error $e = \\lVert \\widehat{x} - x_{0} \\rVert_{2} / \\lVert x_{0} \\rVert_{2}$ and the success indicator $\\mathbb{I}\\{ e \\le \\tau \\}$ with $\\tau = 10^{-3}$. Aggregate the empirical success probability $\\widehat{p}$ and the mean relative error $\\overline{e}$.\n4. After processing all test cases, compute Pearson correlations $r_{\\delta,p}$ between the list $[\\widehat{\\delta}_{k}^{A}, \\widehat{\\delta}_{k}^{B}, \\widehat{\\delta}_{k}^{C}, \\widehat{\\delta}_{k}^{D}]$ and $[\\widehat{p}^{A}, \\widehat{p}^{B}, \\widehat{p}^{C}, \\widehat{p}^{D}]$, and $r_{\\delta,e}$ between the list of $\\widehat{\\delta}_{k}$ values and $[\\overline{e}^{A}, \\overline{e}^{B}, \\overline{e}^{C}, \\overline{e}^{D}]$. If a correlation denominator is numerically zero due to zero variance in one list, the correlation is defined as $0$ to avoid undefined values.\n\nThe test suite specifies:\n- Case A: type “gaussian”, $m = 32$, $n = 64$, $k = 4$, seed $= 1$.\n- Case B: type “gaussian”, $m = 32$, $n = 64$, $k = 10$, seed $= 2$.\n- Case C: type “bernoulli”, $m = 32$, $n = 64$, $k = 8$, seed $= 3$.\n- Case D: type “correlated”, $m = 32$, $n = 64$, $k = 8$, $\\rho = 0.6$, seed $= 4$.\n\nFor each case, we use $N_{\\text{RIP}} = 80$ supports, $T = 40$ trials, $\\tau = 10^{-3}$, and OMP residual tolerance $10^{-8}$.\n\nThe final program outputs a single line with the structure\n$$\n\\big[ \\big[ [\\widehat{\\delta}_{k}^{A}, \\widehat{p}^{A}, \\overline{e}^{A}], [\\widehat{\\delta}_{k}^{B}, \\widehat{p}^{B}, \\overline{e}^{B}], [\\widehat{\\delta}_{k}^{C}, \\widehat{p}^{C}, \\overline{e}^{C}], [\\widehat{\\delta}_{k}^{D}, \\widehat{p}^{D}, \\overline{e}^{D}] \\big], r_{\\delta,p}, r_{\\delta,e} \\big],\n$$\nwith all floats rounded to three decimals. This design directly ties the empirical RIP proxy to measured recovery performance and quantifies the correlations across structurally distinct matrix ensembles, grounding the investigation in the core definitions and Monte Carlo estimation without relying on unproven heuristics or extraneous assumptions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef normalize_columns(A: np.ndarray) - np.ndarray:\n    \"\"\"Normalize columns of A to unit l2 norm.\"\"\"\n    norms = np.linalg.norm(A, axis=0)\n    norms[norms == 0.0] = 1.0\n    return A / norms\n\ndef make_matrix(m: int, n: int, kind: str, rho: float, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Construct matrix A of specified kind and normalize columns.\"\"\"\n    if kind == \"gaussian\":\n        A = rng.standard_normal((m, n))\n    elif kind == \"bernoulli\":\n        A = rng.choice([-1.0, 1.0], size=(m, n))\n    elif kind == \"correlated\":\n        A0 = rng.standard_normal((m, n))\n        v = rng.standard_normal((m, 1))\n        A = (1.0 - rho) * A0 + rho * v @ np.ones((1, n))\n    else:\n        raise ValueError(f\"Unknown matrix kind: {kind}\")\n    A = normalize_columns(A)\n    return A\n\ndef empirical_delta_k(A: np.ndarray, k: int, num_samples: int, rng: np.random.Generator) - float:\n    \"\"\"Monte Carlo estimate of empirical RIP constant proxy via Gram eigenvalues.\"\"\"\n    m, n = A.shape\n    if k = 0 or k  n:\n        return float('nan')\n    delta_max = 0.0\n    # Precompute A^T A could be heavy; we sample supports and compute Gram via A_S.T @ A_S\n    for _ in range(num_samples):\n        S = rng.choice(n, size=k, replace=False)\n        AS = A[:, S]\n        G = AS.T @ AS  # symmetric k x k\n        # Use eigenvalues for symmetric matrices\n        eigvals = np.linalg.eigvalsh(G)\n        dev = max(abs(eigvals[0] - 1.0), abs(eigvals[-1] - 1.0))\n        if dev  delta_max:\n            delta_max = float(dev)\n    return float(delta_max)\n\ndef omp(A: np.ndarray, y: np.ndarray, k: int, tol: float = 1e-8) - np.ndarray:\n    \"\"\"Orthogonal Matching Pursuit up to k iterations or until residual below tol.\"\"\"\n    m, n = A.shape\n    residual = y.copy()\n    support = []\n    x_s = np.array([])\n    for _ in range(k):\n        correlations = A.T @ residual\n        # Zero out already selected to avoid reselection\n        if support:\n            correlations[np.array(support, dtype=int)] = 0.0\n        idx = int(np.argmax(np.abs(correlations)))\n        # Stop if idx already selected (degenerate), though we zeroed it; safeguard anyway\n        if idx in support:\n            break\n        support.append(idx)\n        AS = A[:, support]\n        # Least squares on active set\n        x_s, _, _, _ = np.linalg.lstsq(AS, y, rcond=None)\n        residual = y - AS @ x_s\n        if np.linalg.norm(residual) = tol:\n            break\n    x_hat = np.zeros(n)\n    if support:\n        x_hat[np.array(support, dtype=int)] = x_s\n    return x_hat\n\ndef generate_k_sparse(n: int, k: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate a random k-sparse unit-norm vector with standard normal nonzeros.\"\"\"\n    x = np.zeros(n)\n    support = rng.choice(n, size=k, replace=False)\n    coeffs = rng.standard_normal(k)\n    x[support] = coeffs\n    norm = np.linalg.norm(x)\n    if norm == 0:\n        # Extremely unlikely; regenerate\n        return generate_k_sparse(n, k, rng)\n    return x / norm\n\ndef pearson_corr(x: np.ndarray, y: np.ndarray) - float:\n    \"\"\"Compute Pearson correlation; return 0 if variance is zero.\"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    sx = np.std(x)\n    sy = np.std(y)\n    if sx  1e-15 or sy  1e-15:\n        return 0.0\n    cov = float(np.mean((x - np.mean(x)) * (y - np.mean(y))))\n    return cov / (sx * sy)\n\ndef run_case(kind: str, m: int, n: int, k: int, rho: float, seed: int,\n             num_rip_samples: int, num_trials: int, tau: float) - tuple[float, float, float]:\n    \"\"\"Run one test case: build A, estimate delta_k, run OMP trials to get success and mean errors.\"\"\"\n    rng = np.random.default_rng(seed)\n    A = make_matrix(m, n, kind, rho, rng)\n    delta_emp = empirical_delta_k(A, k, num_rip_samples, rng)\n    successes = 0\n    errors = []\n    for _ in range(num_trials):\n        x0 = generate_k_sparse(n, k, rng)\n        y = A @ x0\n        xhat = omp(A, y, k, tol=1e-8)\n        rel_err = float(np.linalg.norm(xhat - x0) / (np.linalg.norm(x0) + 1e-16))\n        errors.append(rel_err)\n        if rel_err = tau:\n            successes += 1\n    success_rate = successes / num_trials\n    mean_rel_error = float(np.mean(errors) if errors else float('nan'))\n    return float(delta_emp), float(success_rate), float(mean_rel_error)\n\ndef format_triplet(triplet: tuple[float, float, float]) - str:\n    \"\"\"Format a (delta, success_rate, mean_error) triplet to three decimals.\"\"\"\n    return \"[\" + \",\".join(f\"{v:.3f}\" for v in triplet) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (kind, m, n, k, rho, seed)\n    test_cases = [\n        (\"gaussian\", 32, 64, 4, 0.0, 1),\n        (\"gaussian\", 32, 64, 10, 0.0, 2),\n        (\"bernoulli\", 32, 64, 8, 0.0, 3),\n        (\"correlated\", 32, 64, 8, 0.6, 4),\n    ]\n    N_RIP = 80\n    T = 40\n    tau = 1e-3\n\n    results = []\n    for kind, m, n, k, rho, seed in test_cases:\n        delta_emp, success_rate, mean_rel_error = run_case(\n            kind, m, n, k, rho, seed, N_RIP, T, tau\n        )\n        results.append((delta_emp, success_rate, mean_rel_error))\n\n    # Compute correlations across test cases\n    deltas = np.array([r[0] for r in results], dtype=float)\n    succs = np.array([r[1] for r in results], dtype=float)\n    errs = np.array([r[2] for r in results], dtype=float)\n    corr_delta_success = pearson_corr(deltas, succs)\n    corr_delta_error = pearson_corr(deltas, errs)\n\n    # Build final output string\n    per_case_str = \"[\" + \",\".join(format_triplet(r) for r in results) + \"]\"\n    final_str = f\"[{per_case_str},{corr_delta_success:.3f},{corr_delta_error:.3f}]\"\n    print(final_str)\n\nsolve()\n```"
        },
        {
            "introduction": "While matrix properties set the stage for recovery, the performance we observe is ultimately determined by the algorithm used to solve the optimization problem. This hands-on exercise  focuses on the practical implementation of the Iterative Soft-Thresholding Algorithm (ISTA) for solving the LASSO problem. You will investigate and compare different stopping criteria, learning how the choice between a fixed iteration count, a primal-dual gap tolerance, or a residual-based rule creates critical trade-offs between computational efficiency, reconstruction accuracy, and statistical overfitting.",
            "id": "3446302",
            "problem": "Consider the recovery of a sparse signal in a noisy linear model using the Least Absolute Shrinkage and Selection Operator (LASSO). The underlying data-generating process is the following fundamental base: a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$, a $k$-sparse ground-truth vector $x_0 \\in \\mathbb{R}^n$ (that is, $x_0$ has at most $k$ nonzero entries), and a measurement noise vector $w \\in \\mathbb{R}^m$ whose entries are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma^2$. The observed measurements are $y = A x_0 + w$. Recovery is posed as the LASSO optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $\\lambda  0$ balances data fidelity and sparsity-promoting regularization.\n\nYou must empirically compare three stopping rules for a proximal gradient method (Iterative Soft-Thresholding Algorithm) solving the above problem:\n- Fixed-iteration stopping: always run exactly $T_{\\text{fixed}}$ iterations.\n- Dual-gap stopping: stop as soon as the primal-dual gap is below a tolerance $\\varepsilon_{\\text{dual}}$.\n- Residual-based stopping (discrepancy principle): stop as soon as the residual norm matches the noise level, i.e., when $\\|A x - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$.\n\nThe Iterative Soft-Thresholding Algorithm (ISTA) uses the soft-thresholding operator $S_{\\theta}(\\cdot)$ defined component-wise by $S_{\\theta}(z)_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\theta, 0\\}$, and performs iterations\n$$\nx^{t+1} \\;=\\; S_{\\lambda \\eta}\\Big(x^{t} - \\eta A^\\top (A x^{t} - y)\\Big),\n$$\nwhere $\\eta  0$ is a step size chosen as $\\eta = 1/L$ with $L$ the Lipschitz constant of the gradient of the data fidelity term, specifically $L = \\|A\\|_2^2$ (the square of the spectral norm of $A$).\n\nUse the Fenchel dual of the LASSO problem to compute a valid dual lower bound and the primal-dual gap at an iterate $x$. Let $r = A x - y$. The dual problem is\n$$\n\\max_{u \\in \\mathbb{R}^m} \\; -\\frac{1}{2} \\|u\\|_2^2 - y^\\top u \\quad \\text{subject to} \\quad \\|A^\\top u\\|_\\infty \\leq \\lambda.\n$$\nGiven a primal iterate $x$ with residual $r$, form a feasible dual variable by projecting $r$ radially onto the dual feasible set: set $\\alpha = \\min\\big\\{1, \\lambda / \\|A^\\top r\\|_\\infty \\big\\}$, and define $u = \\alpha r$. The primal objective value is $p(x) = \\frac{1}{2}\\|r\\|_2^2 + \\lambda \\|x\\|_1$, and the dual objective value at $u$ is $d(u) = -\\frac{1}{2}\\|u\\|_2^2 - y^\\top u$. The primal-dual gap is then $g(x) = p(x) - d(u)$.\n\nFor each stopping rule, quantify empirical performance through:\n- Accuracy: the normalized mean-squared error $\\mathrm{NMSE}(x) = \\|x - x_0\\|_2^2/\\|x_0\\|_2^2$.\n- Overfitting index: the false discovery proportion $\\mathrm{FDP}(x)$ defined as the number of indices $i$ with $x_i \\neq 0$ and $(x_0)_i = 0$ divided by $\\max\\{1, |\\mathrm{supp}(x)|\\}$, where $\\mathrm{supp}(x) = \\{i : |x_i|  \\theta_{\\text{supp}}\\}$.\n- Underfitting index: the miss rate $\\mathrm{MISS}(x)$ defined as the number of indices $i$ with $(x_0)_i \\neq 0$ and $x_i = 0$ divided by $|\\mathrm{supp}(x_0)|$, with the same support detection rule $|x_i|  \\theta_{\\text{supp}}$.\n- Computational savings versus fixed-iteration baseline: for a stopping rule that halts after $T$ iterations, define $\\mathrm{SAVE} = (T_{\\text{fixed}} - T)/T_{\\text{fixed}}$.\n\nYou must implement ISTA with the three stopping rules, generate synthetic data as described, and report the above metrics relative to the fixed-iteration baseline. The support detection threshold must be $\\theta_{\\text{supp}} = 10^{-3}$.\n\nUse the following common settings for all test cases: dimension $n = 256$, measurements $m = 120$, sparsity level $k = 20$, fixed iteration budget $T_{\\text{fixed}} = 200$, and soft-thresholding parameter $\\lambda$ selected by the empirical rule $\\lambda = \\sigma \\sqrt{2 \\log n}$. Construct $A$ with independent entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$, select a random support of size $k$, and set nonzero components of $x_0$ as independent samples uniformly distributed in $[1, 2]$ with random signs.\n\nTest Suite:\n- Case $1$: random seed $0$, noise level $\\sigma = 0.02$, dual-gap tolerance $\\varepsilon_{\\text{dual}} = 10^{-6}$, discrepancy parameter $\\tau = 1.05$.\n- Case $2$: random seed $1$, noise level $\\sigma = 0.1$, dual-gap tolerance $\\varepsilon_{\\text{dual}} = 10^{-4}$, discrepancy parameter $\\tau = 1.05$.\n- Case $3$: random seed $2$, noise level $\\sigma = 0.3$, dual-gap tolerance $\\varepsilon_{\\text{dual}} = 10^{-3}$, discrepancy parameter $\\tau = 1.05$.\n\nFor each case, run ISTA three times: with fixed-iteration stopping ($T_{\\text{fixed}}$ iterations), with dual-gap stopping (stop as soon as $g(x) \\leq \\varepsilon_{\\text{dual}}$ or after $T_{\\text{fixed}}$ iterations if not satisfied), and with residual-based stopping via the discrepancy principle (stop as soon as $\\|A x - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$ or after $T_{\\text{fixed}}$ iterations if not satisfied). Compute the following $8$ floating-point outputs per case, in the exact order listed:\n- $\\mathrm{SAVE}_{\\text{dual}}$,\n- $\\mathrm{SAVE}_{\\text{res}}$,\n- $\\mathrm{NMSE}_{\\text{dual}} - \\mathrm{NMSE}_{\\text{fixed}}$,\n- $\\mathrm{NMSE}_{\\text{res}} - \\mathrm{NMSE}_{\\text{fixed}}$,\n- $\\mathrm{FDP}_{\\text{dual}}$,\n- $\\mathrm{MISS}_{\\text{dual}}$,\n- $\\mathrm{FDP}_{\\text{res}}$,\n- $\\mathrm{MISS}_{\\text{res}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3},\\ldots]$). There will be $3$ cases, so the list must contain $24$ floating-point numbers. No physical units are involved. All angles, if any were present, must be in radians, but no angles are used here. All percentages must be expressed as decimals, not with a percent sign. The program must be self-contained and must not read any external input.",
            "solution": "The user-provided problem is a well-defined numerical experiment in the field of sparse signal recovery and computational optimization. It asks for an empirical comparison of three different stopping criteria for the Iterative Soft-Thresholding Algorithm (ISTA) applied to the LASSO problem. The problem is scientifically grounded, self-contained, and all parameters and procedures are specified unambiguously. Therefore, it is deemed valid and a full solution is provided below.\n\nThe core of the problem is to solve the LASSO optimization problem for a sparse signal $x \\in \\mathbb{R}^n$ from noisy linear measurements $y \\in \\mathbb{R}^m$. The data generation model is given by\n$$\ny = A x_0 + w,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is a measurement matrix, $x_0 \\in \\mathbb{R}^n$ is the ground-truth $k$-sparse signal, and $w \\in \\mathbb{R}^m$ is a vector of i.i.d. Gaussian noise with $w_i \\sim \\mathcal{N}(0, \\sigma^2)$. The LASSO formulation seeks to find an estimate of $x_0$ by solving:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) := \\underbrace{\\frac{1}{2} \\|A x - y\\|_2^2}_{\\text{data fidelity}} + \\underbrace{\\lambda \\|x\\|_1}_{\\text{sparsity regularizer}}.\n$$\nHere, $\\lambda  0$ is a regularization parameter that balances the trade-off between fitting the measurements $y$ and enforcing sparsity in the solution vector $x$.\n\nThe chosen solver is the Iterative Soft-Thresholding Algorithm (ISTA), a proximal gradient method. The data fidelity term has a gradient $\\nabla(\\frac{1}{2} \\|A x - y\\|_2^2) = A^\\top(Ax-y)$. The gradient of this term is Lipschitz continuous with constant $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$, where $\\|A\\|_2$ is the spectral norm of $A$. ISTA proceeds by taking a gradient descent step on the smooth part of the objective followed by applying the proximal operator of the non-smooth $\\ell_1$-norm, which is the soft-thresholding operator $S_{\\theta}(\\cdot)$. With a step size $\\eta = 1/L$, the update rule for iteration $t+1$ is:\n$$\nx^{t+1} = S_{\\lambda \\eta}\\left(x^t - \\eta A^\\top(Ax^t - y)\\right),\n$$\nwhere the soft-thresholding operator is applied element-wise: $S_{\\theta}(z)_i = \\operatorname{sign}(z_i) \\max(|z_i| - \\theta, 0)$. All runs are initialized with $x^0 = \\mathbf{0}$.\n\nWe will compare three stopping rules for this iterative process:\n1.  **Fixed-Iteration Stopping**: The algorithm is terminated after a fixed number of iterations, $T = T_{\\text{fixed}}$. This serves as a baseline for computational cost and accuracy.\n2.  **Dual-Gap Stopping**: This rule leverages duality theory. The primal objective value at iterate $x^t$ is $p(x^t) = \\frac{1}{2}\\|Ax^t - y\\|_2^2 + \\lambda\\|x^t\\|_1$. The Fenchel dual problem is $\\max_{u \\in \\mathbb{R}^m} \\; -\\frac{1}{2} \\|u\\|_2^2 - y^\\top u$ subject to $\\|A^\\top u\\|_\\infty \\leq \\lambda$. From a primal iterate $x^t$, we construct a dual-feasible variable $u^t$. Let $r^t = Ax^t - y$ be the residual. A dual-feasible variable $u^t$ is formed by setting $u^t = \\alpha r^t$, where $\\alpha = \\min\\left(1, \\frac{\\lambda}{\\|A^\\top r^t\\|_\\infty}\\right)$. This choice ensures $\\|A^\\top u^t\\|_\\infty \\le \\lambda$. The dual objective value is $d(u^t) = -\\frac{1}{2}\\|u^t\\|_2^2 - y^\\top u^t$. The primal-dual gap is $g(x^t) = p(x^t) - d(u^t) \\geq 0$. The algorithm stops when $g(x^t) \\leq \\varepsilon_{\\text{dual}}$.\n3.  **Residual-Based Stopping (Discrepancy Principle)**: This rule is motivated by the statistical properties of the noise. Since the noise energy is expected to be $\\|w\\|_2^2 \\approx m \\sigma^2$, we expect the residual norm of the true solution, $\\|Ax_0 - y\\|_2 = \\|-w\\|_2$, to be around $\\sigma\\sqrt{m}$. The algorithm is therefore stopped when the residual norm of the current iterate, $\\|Ax^t-y\\|_2$, falls below a threshold related to this noise level: $\\|Ax^t - y\\|_2 \\leq \\tau \\sigma \\sqrt{m}$, where $\\tau \\ge 1$ is a tolerance factor.\n\nFor all rules, a maximum of $T_{\\text{fixed}}$ iterations are performed as a safeguard.\n\nThe empirical performance of each recovered solution $x$ is evaluated using several metrics:\n-   **Normalized Mean-Squared Error**: $\\mathrm{NMSE}(x) = \\frac{\\|x - x_0\\|_2^2}{\\|x_0\\|_2^2}$, which measures the reconstruction accuracy.\n-   **Support Recovery Metrics**: To evaluate how well the sparsity pattern of $x_0$ is recovered, we first define the estimated support of $x$ as $\\mathrm{supp}(x) = \\{i : |x_i|  \\theta_{\\text{supp}}\\}$ for a small threshold $\\theta_{\\text{supp}}=10^{-3}$.\n    -   **False Discovery Proportion**: $\\mathrm{FDP}(x) = \\frac{|\\{i : x_i \\neq 0 \\text{ and } (x_0)_i = 0\\}|}{\\max(1, |\\mathrm{supp}(x)|)}$, measuring overfitting (spurious non-zero entries).\n    -   **Miss Rate**: $\\mathrm{MISS}(x) = \\frac{|\\{i : (x_0)_i \\neq 0 \\text{ and } x_i = 0\\}|}{|\\mathrm{supp}(x_0)|}$, measuring underfitting (missed true non-zero entries). Note that $x_i=0$ is interpreted as $|x_i| \\le \\theta_{\\text{supp}}$.\n-   **Computational Savings**: For a rule stopping at $T$ iterations, $\\mathrm{SAVE} = \\frac{T_{\\text{fixed}} - T}{T_{\\text{fixed}}}$.\n\nThe implementation generates synthetic data for each test case according to the provided specifications ($n=256$, $m=120$, $k=20$, $T_{\\text{fixed}}=200$). The matrix $A$ has i.i.d. entries from $\\mathcal{N}(0, 1/m)$. The ground-truth $x_0$ has a randomly chosen support of size $k$, with non-zero values drawn from a uniform distribution over $[1, 2]$ with random signs. The regularization parameter is set via the empirical rule $\\lambda = \\sigma \\sqrt{2 \\log n}$. For each of the three test cases provided, we run ISTA with each stopping rule, calculate the specified metrics, and compute the required $8$ output values relative to the fixed-iteration baseline.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nTHETA_SUPP = 1e-3\n\ndef soft_threshold(z, theta):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - theta, 0)\n\ndef calculate_nmse(x_est, x0):\n    \"\"\"Calculates Normalized Mean-Squared Error.\"\"\"\n    norm_x0_sq = np.linalg.norm(x0)**2\n    if norm_x0_sq == 0:\n        return np.linalg.norm(x_est)**2\n    return np.linalg.norm(x_est - x0)**2 / norm_x0_sq\n\ndef calculate_fdp_miss(x_est, x0):\n    \"\"\"Calculates False Discovery Proportion and Miss Rate.\"\"\"\n    supp_x0 = np.where(x0 != 0)[0]\n    supp_x_est = np.where(np.abs(x_est)  THETA_SUPP)[0]\n\n    num_supp_x0 = len(supp_x0)\n    num_supp_x_est = len(supp_x_est)\n\n    false_discoveries = len(np.setdiff1d(supp_x_est, supp_x0))\n    misses = len(np.setdiff1d(supp_x0, supp_x_est))\n\n    fdp = false_discoveries / max(1, num_supp_x_est)\n    miss_rate = misses / num_supp_x0 if num_supp_x0  0 else 0.0\n\n    return fdp, miss_rate\n\ndef run_ista(A, y, x0, lambda_val, eta, T_fixed, stopping_rule, **kwargs):\n    \"\"\"Runs the ISTA algorithm with a specified stopping rule.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    \n    # Unpack stopping rule parameters from kwargs\n    eps_dual = kwargs.get('eps_dual')\n    tau = kwargs.get('tau')\n    sigma = kwargs.get('sigma')\n\n    for t in range(T_fixed):\n        # ISTA update step\n        residual = A @ x - y\n        grad = A.T @ residual\n        z = x - eta * grad\n        x = soft_threshold(z, lambda_val * eta)\n\n        # Check stopping rule\n        if stopping_rule == 'dual_gap':\n            # Calculate primal-dual gap\n            r = A @ x - y\n            At_r = A.T @ r\n            At_r_inf_norm = np.linalg.norm(At_r, np.inf)\n            \n            # Avoid division by zero\n            alpha = 1.0 if At_r_inf_norm  1e-12 else min(1.0, lambda_val / At_r_inf_norm)\n            \n            u = alpha * r\n            primal_obj = 0.5 * np.dot(r, r) + lambda_val * np.linalg.norm(x, 1)\n            dual_obj = -0.5 * np.dot(u, u) - np.dot(y, u)\n            gap = primal_obj - dual_obj\n            \n            if gap = eps_dual:\n                return x, t + 1\n                \n        elif stopping_rule == 'residual':\n            # Check discrepancy principle\n            res_norm = np.linalg.norm(A @ x - y, 2)\n            res_thresh = tau * sigma * np.sqrt(m)\n            if res_norm = res_thresh:\n                return x, t + 1\n    \n    # If loop completes, return result after T_fixed iterations\n    return x, T_fixed\n\ndef run_case(seed, sigma, eps_dual, tau):\n    \"\"\"Runs a single test case with all three stopping rules.\"\"\"\n    # Common settings from problem statement\n    n = 256\n    m = 120\n    k = 20\n    T_fixed = 200\n\n    # Generate synthetic data\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n)) / np.sqrt(m)\n    \n    x0 = np.zeros(n)\n    support = rng.choice(n, k, replace=False)\n    values = rng.uniform(1, 2, size=k)\n    signs = rng.choice([-1, 1], size=k)\n    x0[support] = values * signs\n    \n    w = rng.standard_normal(m) * sigma\n    y = A @ x0 + w\n    \n    # Algorithm parameters\n    lambda_val = sigma * np.sqrt(2 * np.log(n))\n    L = np.linalg.norm(A, 2)**2\n    eta = 1.0 / L\n\n    # --- Run with Fixed-Iteration Stopping ---\n    x_fixed, T_actual_fixed = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'fixed')\n\n    # --- Run with Dual-Gap Stopping ---\n    x_dual, T_dual = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'dual_gap', eps_dual=eps_dual)\n\n    # --- Run with Residual-Based Stopping ---\n    x_res, T_res = run_ista(A, y, x0, lambda_val, eta, T_fixed, 'residual', tau=tau, sigma=sigma)\n\n    # Calculate metrics for the baseline\n    nmse_fixed = calculate_nmse(x_fixed, x0)\n\n    # Calculate metrics for dual-gap rule\n    nmse_dual = calculate_nmse(x_dual, x0)\n    fdp_dual, miss_dual = calculate_fdp_miss(x_dual, x0)\n    save_dual = (T_fixed - T_dual) / T_fixed\n    nmse_diff_dual = nmse_dual - nmse_fixed\n\n    # Calculate metrics for residual-based rule\n    nmse_res = calculate_nmse(x_res, x0)\n    fdp_res, miss_res = calculate_fdp_miss(x_res, x0)\n    save_res = (T_fixed - T_res) / T_fixed\n    nmse_diff_res = nmse_res - nmse_fixed\n    \n    return [\n        save_dual,\n        save_res,\n        nmse_diff_dual,\n        nmse_diff_res,\n        fdp_dual,\n        miss_dual,\n        fdp_res,\n        miss_res\n    ]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        # (seed, sigma, eps_dual, tau)\n        (0, 0.02, 1e-6, 1.05),\n        (1, 0.1,  1e-4, 1.05),\n        (2, 0.3,  1e-3, 1.05),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = run_case(*case)\n        all_results.extend(case_results)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A successful recovery algorithm must not only produce a sparse estimate but also correctly identify the locations of the non-zero coefficients in the true signal. This exercise  provides a quantitative framework for evaluating this feature selection capability using fundamental statistical metrics. By analyzing a hypothetical scenario with known signal and noise characteristics, you will calculate the True Positive Rate (TPR) and False Discovery Rate (FDR) to understand the delicate balance between detection power and error control that defines an algorithm's performance across different signal-to-noise ratios.",
            "id": "3446230",
            "problem": "Consider the standard linear model $y = X \\beta^{\\star} + \\varepsilon$ with an $n \\times p$ design matrix $X$ whose columns are orthonormal, a coefficient vector $\\beta^{\\star} \\in \\mathbb{R}^{p}$ with $k$ nonzero entries of equal magnitude $a$ (signs arbitrary), and additive noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Under orthonormal columns, the Least Absolute Shrinkage and Selection Operator (LASSO) selects coordinate $i$ by soft-thresholding the statistic $z_{i} = x_{i}^{\\top} y$; a feature is declared discovered if $|z_{i}|$ exceeds a threshold $\\lambda$. You will use the null distribution of $z_{i}$ to determine $\\lambda$ that controls the per-feature false inclusion probability.\n\nGiven $p = 1000$, $k = 40$, $\\sigma = 1$ and three signal-to-noise ratios (SNRs) defined by $a / \\sigma \\in \\{0.5, 1.5, 3.0\\}$, select a single threshold $\\lambda$ by requiring that the per-feature null tail probability equals $\\tau = 0.01$, that is, the probability of declaring a null feature as discovered is $\\mathbb{P}(|Z_{0}|  \\lambda) = \\tau$, where $Z_{0} \\sim \\mathcal{N}(0, \\sigma^{2})$. For each SNR value, treat signal coordinates as $Z_{s} \\sim \\mathcal{N}(\\mu, \\sigma^{2})$ with $\\mu = a$ and compute:\n- the true positive rate $\\mathrm{TPR} = \\mathbb{P}(|Z_{s}|  \\lambda)$,\n- the approximate expected false discovery rate $\\mathrm{FDR} \\approx \\dfrac{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda)}{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda) + k\\,\\mathbb{P}(|Z_{s}|  \\lambda)}$,\n\nassuming independence across coordinates. Define the per-SNR $F1$ score by $F1 = \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}}$ with $\\mathrm{precision} = 1 - \\mathrm{FDR}$ and $\\mathrm{recall} = \\mathrm{TPR}$. Finally, report the macro-average $F1$ score obtained by averaging the three per-SNR $F1$ scores. Round your final answer to four significant figures.",
            "solution": "The user has provided a well-defined problem in statistical signal processing and sparse recovery. The problem is scientifically grounded, self-contained, and all parameters are specified. I will proceed with a full solution.\n\nThe problem asks for the macro-average $F1$ score for feature selection using a soft-thresholding rule, which is characteristic of the LASSO estimator, under a simplified orthonormal design setting. The analysis proceeds in four main steps:\n1.  Determine the selection threshold $\\lambda$ based on the specified control of false inclusions.\n2.  For each Signal-to-Noise Ratio (SNR), calculate the True Positive Rate (TPR), also known as recall or sensitivity.\n3.  For each SNR, calculate the approximate expected False Discovery Rate (FDR), precision, and the corresponding $F1$ score.\n4.  Compute the macro-average of the $F1$ scores.\n\nThe problem provides the following parameters: a total of $p=1000$ features, of which $k=40$ are \"signal\" (non-zero coefficients) and $p-k=960$ are \"null\" (zero coefficients). The noise standard deviation is $\\sigma=1$. The signal amplitudes are given by $a/\\sigma \\in \\{0.5, 1.5, 3.0\\}$, which simplifies to $a \\in \\{0.5, 1.5, 3.0\\}$ since $\\sigma=1$.\n\nFirst, we determine the threshold $\\lambda$. It is set to control the per-feature false inclusion probability. For a null feature, the corresponding statistic $Z_0$ follows a normal distribution $Z_0 \\sim \\mathcal{N}(0, \\sigma^2)$. A false inclusion occurs if $|Z_0|  \\lambda$. This probability is set to $\\tau = 0.01$.\n$$ \\mathbb{P}(|Z_0|  \\lambda) = \\tau $$\nSince $Z_0/\\sigma \\sim \\mathcal{N}(0, 1)$, we can standardize the random variable. Let $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution.\n$$ \\mathbb{P}\\left(\\left|\\frac{Z_0}{\\sigma}\\right|  \\frac{\\lambda}{\\sigma}\\right) = \\tau $$\nBy the symmetry of the normal distribution, this is equivalent to:\n$$ 2 \\cdot \\mathbb{P}\\left(\\frac{Z_0}{\\sigma}  \\frac{\\lambda}{\\sigma}\\right) = \\tau $$\n$$ 1 - \\Phi\\left(\\frac{\\lambda}{\\sigma}\\right) = \\frac{\\tau}{2} $$\n$$ \\Phi\\left(\\frac{\\lambda}{\\sigma}\\right) = 1 - \\frac{\\tau}{2} $$\nSolving for $\\lambda$, we get:\n$$ \\lambda = \\sigma \\Phi^{-1}\\left(1 - \\frac{\\tau}{2}\\right) $$\nSubstituting the given values $\\sigma=1$ and $\\tau=0.01$:\n$$ \\lambda = 1 \\cdot \\Phi^{-1}\\left(1 - \\frac{0.01}{2}\\right) = \\Phi^{-1}(0.995) \\approx 2.575829 $$\n\nNext, we evaluate the performance for each of the three signal strengths $a \\in \\{0.5, 1.5, 3.0\\}$. For this, we need the True Positive Rate (TPR), which is the probability of correctly identifying a signal feature. The statistic for a signal feature, $Z_s$, is modeled as $Z_s \\sim \\mathcal{N}(a, \\sigma^2)$.\n$$ \\mathrm{TPR} = \\mathbb{P}(|Z_s|  \\lambda) = \\mathbb{P}(Z_s  \\lambda) + \\mathbb{P}(Z_s  -\\lambda) $$\nStandardizing the variable $Z_s$:\n$$ \\mathrm{TPR} = \\mathbb{P}\\left(\\frac{Z_s - a}{\\sigma}  \\frac{\\lambda - a}{\\sigma}\\right) + \\mathbb{P}\\left(\\frac{Z_s - a}{\\sigma}  \\frac{-\\lambda - a}{\\sigma}\\right) $$\nSince $(Z_s - a)/\\sigma \\sim \\mathcal{N}(0, 1)$, we have:\n$$ \\mathrm{TPR}(a) = \\left(1 - \\Phi\\left(\\frac{\\lambda - a}{\\sigma}\\right)\\right) + \\Phi\\left(\\frac{-\\lambda - a}{\\sigma}\\right) $$\nWith $\\sigma=1$, this simplifies to:\n$$ \\mathrm{TPR}(a) = 1 - \\Phi(\\lambda - a) + \\Phi(-\\lambda - a) $$\nThe recall is defined as $\\mathrm{recall} = \\mathrm{TPR}$.\n\nThe approximate expected False Discovery Rate (FDR) is given by:\n$$ \\mathrm{FDR} \\approx \\frac{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda)}{(p - k)\\,\\mathbb{P}(|Z_{0}|  \\lambda) + k\\,\\mathbb{P}(|Z_{s}|  \\lambda)} = \\frac{(p - k)\\tau}{(p-k)\\tau + k \\cdot \\mathrm{TPR}} $$\nThe precision is $\\mathrm{precision} = 1 - \\mathrm{FDR}$. The $F1$ score is the harmonic mean of precision and recall:\n$$ F1 = \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} $$\nWe can write a direct formula for the $F1$ score. First, note that $\\mathrm{precision} = \\frac{k \\cdot \\mathrm{TPR}}{(p-k)\\tau + k \\cdot \\mathrm{TPR}}$. Substituting this and $\\mathrm{recall} = \\mathrm{TPR}$ into the $F1$ formula and simplifying gives:\n$$ F1 = \\frac{2 k \\cdot \\mathrm{TPR}}{k + (p-k)\\tau + k \\cdot \\mathrm{TPR}} $$\nWe now compute the values for each case, using $p=1000$, $k=40$, $\\tau=0.01$, and $\\lambda \\approx 2.575829$. The expected number of false positives is $(p-k)\\tau = (1000-40) \\times 0.01 = 960 \\times 0.01 = 9.6$.\n\nCase 1: $a=0.5$\n$$ \\mathrm{TPR}_1 = 1 - \\Phi(2.575829 - 0.5) + \\Phi(-2.575829 - 0.5) $$\n$$ \\mathrm{TPR}_1 = 1 - \\Phi(2.075829) + \\Phi(-3.075829) \\approx 1 - 0.981042 + 0.001051 \\approx 0.020009 $$\nUsing this TPR, we compute the $F1$ score:\n$$ F1_1 = \\frac{2 \\cdot 40 \\cdot 0.020009}{40 + 9.6 + 40 \\cdot 0.020009} = \\frac{1.60072}{49.6 + 0.80036} = \\frac{1.60072}{50.40036} \\approx 0.031760 $$\n\nCase 2: $a=1.5$\n$$ \\mathrm{TPR}_2 = 1 - \\Phi(2.575829 - 1.5) + \\Phi(-2.575829 - 1.5) $$\n$$ \\mathrm{TPR}_2 = 1 - \\Phi(1.075829) + \\Phi(-4.075829) \\approx 1 - 0.859000 + 2.34 \\times 10^{-5} \\approx 0.141023 $$\nThe corresponding $F1$ score is:\n$$ F1_2 = \\frac{2 \\cdot 40 \\cdot 0.141023}{40 + 9.6 + 40 \\cdot 0.141023} = \\frac{11.28184}{49.6 + 5.64092} = \\frac{11.28184}{55.24092} \\approx 0.204232 $$\n\nCase 3: $a=3.0$\n$$ \\mathrm{TPR}_3 = 1 - \\Phi(2.575829 - 3.0) + \\Phi(-2.575829 - 3.0) $$\n$$ \\mathrm{TPR}_3 = 1 - \\Phi(-0.424171) + \\Phi(-5.575829) \\approx 1 - 0.335680 + 1.22 \\times 10^{-8} \\approx 0.664320 $$\nThe corresponding $F1$ score is:\n$$ F1_3 = \\frac{2 \\cdot 40 \\cdot 0.664320}{40 + 9.6 + 40 \\cdot 0.664320} = \\frac{53.1456}{49.6 + 26.5728} = \\frac{53.1456}{76.1728} \\approx 0.697746 $$\n\nFinally, we compute the macro-average $F1$ score by averaging the three individual scores:\n$$ \\text{Macro-average } F1 = \\frac{F1_1 + F1_2 + F1_3}{3} $$\n$$ \\text{Macro-average } F1 \\approx \\frac{0.031760 + 0.204232 + 0.697746}{3} = \\frac{0.933738}{3} \\approx 0.311246 $$\nRounding to four significant figures, the final result is $0.3112$.",
            "answer": "$$\\boxed{0.3112}$$"
        }
    ]
}