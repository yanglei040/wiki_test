{
    "hands_on_practices": [
        {
            "introduction": "This practice explores the deep connection between two cornerstone formulations in sparse optimization: basis pursuit denoising (BPDN) and the LASSO. While cross-validation is typically applied to the LASSO's regularization parameter, $\\lambda$, understanding its equivalence to the BPDN's noise tolerance, $\\epsilon$, provides crucial insight into the geometry of the solution. This exercise challenges you to derive the explicit mapping between $\\lambda$ and $\\epsilon$ under a simplifying but instructive assumption, solidifying your grasp of the underlying Karush-Kuhn-Tucker (KKT) conditions and the structure of the solution path.",
            "id": "3441813",
            "problem": "Consider the basis pursuit denoising formulation in compressed sensing and sparse optimization, which seeks a sparse vector by solving the convex constrained problem $\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1}$ subject to $\\|y - A x\\|_{2} \\leq \\epsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $y \\in \\mathbb{R}^{m}$ is a noisy measurement vector, and $\\epsilon \\geq 0$ is a noise tolerance. A common alternative is the quadratic-penalty Lagrangian (also known as the $\\ell_{1}$-regularized least squares problem), which introduces a regularization parameter $\\lambda > 0$ and minimizes the objective $\\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$. In practice, $\\lambda$ is often selected via $K$-fold cross-validation, a data-driven procedure that partitions the measurements and chooses $\\lambda$ by minimizing an out-of-sample prediction error.\n\nStarting from fundamental convex analysis and optimality principles (convexity of norms, Euclidean projection, and Karush–Kuhn–Tucker (KKT) conditions), derive an explicit mapping $\\epsilon(\\lambda)$ that links the constrained tolerance $\\epsilon$ to the regularization parameter $\\lambda$ by equating solutions of the constrained and penalized problems under the following scientifically plausible structural assumption: the columns of $A$ are orthonormal, that is $A^{\\top} A = I_{n}$. Express $\\epsilon(\\lambda)$ purely in terms of $A$ and $y$, without reference to any numerical algorithm.\n\nThen, briefly explain the conditions under which the constrained and penalized formulations yield equivalent solutions as $\\lambda$ varies (for example, feasibility, convexity, strong duality, and monotonicity along the trade-off curve), and how $K$-fold cross-validation for $\\lambda$ induces a corresponding choice of $\\epsilon$ via your mapping. Your final answer must be the single closed-form analytic expression for $\\epsilon(\\lambda)$, with no units required. If you introduce any approximations, ensure they are rigorously justified; no rounding is required for the final expression.",
            "solution": "The objective is to find a mapping $\\epsilon(\\lambda)$ that ensures the solution of the constrained basis pursuit denoising (BPDN) problem,\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|y - A x\\|_{2} \\leq \\epsilon $$\nis identical to the solution of the penalized Lagrangian (LASSO) problem,\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\} $$\nunder the structural assumption that the columns of $A$ are orthonormal, i.e., $A^{\\top} A = I_{n}$, where $I_n$ is the $n \\times n$ identity matrix.\n\nFirst, we find the solution to the LASSO problem. The objective function $F(x)$ is convex, being the sum of two convex functions. A vector $x^*_{\\lambda}$ is a minimizer if and only if the zero vector is in the subdifferential of $F$ at $x^*_{\\lambda}$, which is the condition $0 \\in \\partial F(x^*_{\\lambda})$.\nThe subdifferential is given by $\\partial F(x) = \\nabla \\left( \\frac{1}{2}\\|y - Ax\\|_{2}^{2} \\right) + \\lambda \\partial \\|x\\|_{1}$.\nThe gradient of the differentiable quadratic term is:\n$$ \\nabla \\left( \\frac{1}{2}\\|y - A x\\|_{2}^{2} \\right) = A^{\\top}(Ax - y) $$\nThe subdifferential of the $\\ell_1$-norm, $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$, is the set of vectors $g \\in \\mathbb{R}^n$ such that $g_i = \\text{sign}(x_i)$ if $x_i \\neq 0$, and $g_i \\in [-1, 1]$ if $x_i = 0$.\n\nThus, the optimality condition for the LASSO solution $x^*_{\\lambda}$ is:\n$$ 0 \\in A^{\\top}(Ax^*_{\\lambda} - y) + \\lambda \\partial \\|x^*_{\\lambda}\\|_{1} $$\n$$ A^{\\top}(y - Ax^*_{\\lambda}) \\in \\lambda \\partial \\|x^*_{\\lambda}\\|_{1} $$\nUsing the given assumption $A^{\\top} A = I_n$, the condition simplifies to:\n$$ A^{\\top}y - x^*_{\\lambda} \\in \\lambda \\partial \\|x^*_{\\lambda}\\|_{1} $$\nThis means there exists a subgradient vector $g \\in \\partial \\|x^*_{\\lambda}\\|_{1}$ such that $A^{\\top}y - x^*_{\\lambda} = \\lambda g$. We can analyze this component-wise for each $i \\in \\{1, \\dots, n\\}$:\n$$(x^*_{\\lambda})_i = (A^{\\top}y)_i - \\lambda g_i$$\n1.  If $(x^*_{\\lambda})_i > 0$, then $g_i = 1$, which implies $(x^*_{\\lambda})_i = (A^{\\top}y)_i - \\lambda$. For this to be consistent, we must have $(A^{\\top}y)_i - \\lambda > 0$, or $(A^{\\top}y)_i > \\lambda$.\n2.  If $(x^*_{\\lambda})_i < 0$, then $g_i = -1$, which implies $(x^*_{\\lambda})_i = (A^{\\top}y)_i + \\lambda$. Consistency requires $(A^{\\top}y)_i + \\lambda < 0$, or $(A^{\\top}y)_i < -\\lambda$.\n3.  If $(x^*_{\\lambda})_i = 0$, then $g_i \\in [-1, 1]$. This implies $0 = (A^{\\top}y)_i - \\lambda g_i$, so $g_i = (A^{\\top}y)_i / \\lambda$. Consistency requires $|(A^{\\top}y)_i / \\lambda| \\leq 1$, or $|(A^{\\top}y)_i| \\leq \\lambda$.\n\nCombining these cases gives the solution for each component of $x^*_{\\lambda}$:\n$$ (x^*_{\\lambda})_i = \\begin{cases} (A^{\\top}y)_i - \\lambda & \\text{if } (A^{\\top}y)_i > \\lambda \\\\ 0 & \\text{if } |(A^{\\top}y)_i| \\leq \\lambda \\\\ (A^{\\top}y)_i + \\lambda & \\text{if } (A^{\\top}y)_i < -\\lambda \\end{cases} $$\nThis is the soft-thresholding operator, denoted $S_{\\lambda}(\\cdot)$. The solution is $x^*_{\\lambda} = S_{\\lambda}(A^{\\top}y)$, where the operator acts component-wise.\n\nFor the solutions of the BPDN and LASSO problems to be equivalent, the LASSO solution $x^*_{\\lambda}$ must satisfy the BPDN constraint with equality, i.e., $\\|y - A x^*_{\\lambda}\\|_{2} = \\epsilon$. This defines the mapping $\\epsilon(\\lambda)$. We now compute this residual norm.\nLet $P = AA^{\\top}$ be the orthogonal projection matrix onto the column space of $A$, $\\mathcal{R}(A)$. We can decompose the vector $y$ into two orthogonal components: $y = Py + (I - P)y$, where $Py \\in \\mathcal{R}(A)$ and $(I-P)y \\in \\mathcal{R}(A)^{\\perp}$. The term $Ax^*_{\\lambda}$ is also in $\\mathcal{R}(A)$.\nThe residual vector is $r = y - Ax^*_{\\lambda} = (Py - Ax^*_{\\lambda}) + (I-P)y$. Since these two components are orthogonal, the squared norm is given by the Pythagorean theorem:\n$$ \\|y - A x^*_{\\lambda}\\|_{2}^{2} = \\|Py - Ax^*_{\\lambda}\\|_{2}^{2} + \\|(I-P)y\\|_{2}^{2} $$\nSubstituting $P=AA^{\\top}$, the first term becomes:\n$$ \\|AA^{\\top}y - A x^*_{\\lambda}\\|_{2}^{2} = \\|A(A^{\\top}y - x^*_{\\lambda})\\|_{2}^{2} $$\nUsing the property $A^{\\top} A = I_n$, this simplifies to:\n$$ (A^{\\top}y - x^*_{\\lambda})^{\\top} A^{\\top} A (A^{\\top}y - x^*_{\\lambda}) = \\|A^{\\top}y - x^*_{\\lambda}\\|_{2}^{2} $$\nSo we have:\n$$ \\epsilon(\\lambda)^2 = \\|y - A x^*_{\\lambda}\\|_{2}^{2} = \\|A^{\\top}y - x^*_{\\lambda}\\|_{2}^{2} + \\|(I - AA^{\\top})y\\|_{2}^{2} $$\nLet $z = A^{\\top}y$. Then $x^*_{\\lambda} = S_{\\lambda}(z)$. We need to compute $\\|z - S_{\\lambda}(z)\\|_{2}^{2}$. Let's examine its $i$-th component:\n- If $|z_i| \\leq \\lambda$, then $(S_{\\lambda}(z))_i = 0$, so the component is $z_i$. The squared value is $z_i^2$. This is incorrect. The component of $z-S_\\lambda(z)$ is $z_i - 0 = z_i$.\n- If $z_i > \\lambda$, then $(S_{\\lambda}(z))_i = z_i - \\lambda$, so the component is $z_i - (z_i - \\lambda) = \\lambda$.\n- If $z_i  -\\lambda$, then $(S_{\\lambda}(z))_i = z_i + \\lambda$, so the component is $z_i - (z_i + \\lambda) = -\\lambda$.\nIn all cases, the squared $i$-th component of $(z - S_{\\lambda}(z))$ is $\\min(|z_i|, \\lambda)^2$.\nTherefore, the squared norm is the sum of these components:\n$$ \\|A^{\\top}y - x^*_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{n} \\min\\left(|(A^{\\top}y)_i|, \\lambda\\right)^{2} $$\nSubstituting this back into the expression for $\\epsilon(\\lambda)^2$, we get the final mapping:\n$$ \\epsilon(\\lambda)^2 = \\|(I - AA^{\\top})y\\|_{2}^{2} + \\sum_{i=1}^{n} \\min\\left(|(A^{\\top}y)_i|, \\lambda\\right)^{2} $$\nTaking the square root gives $\\epsilon(\\lambda)$.\n\nThe equivalence between the constrained (BPDN) and penalized (LASSO) formulations is a general principle in convex optimization. For any $\\lambda  0$, the solution $x^*_{\\lambda}$ of the LASSO problem also solves a corresponding BPDN problem with $\\epsilon = \\|y - Ax^*_{\\lambda}\\|_2$. Conversely, for any $\\epsilon$ within a certain range (from $0$ to $\\|y\\|_2$), the solution $x^*_{\\epsilon}$ of the BPDN problem also solves a corresponding LASSO problem for some $\\lambda \\ge 0$. This correspondence arises from the Karush-Kuhn-Tucker (KKT) conditions for constrained optimization. The existence and uniqueness of solutions are guaranteed by the strict convexity of the squared $\\ell_2$-norm and the convexity of the $\\ell_1$-norm. Strong duality holds (e.g., under Slater's condition), ensuring that the KKT conditions are sufficient for optimality and that the optimal primal and dual values are equal, which solidifies the link between the Lagrange multiplier $\\lambda$ and the constraint bound $\\epsilon$. The mapping $\\epsilon(\\lambda)$ is continuous and monotonically increasing: a stronger penalty $\\lambda$ leads to a sparser solution, which typically fits the data less well, resulting in a larger residual norm $\\epsilon$.\n\n$K$-fold cross-validation (CV) is a data-driven method for choosing the regularization parameter $\\lambda$. It works by partitioning the set of $m$ measurements (rows of $A$ and corresponding entries of $y$) into $K$ disjoint subsets. For each fold, a LASSO model is trained on $K-1$ subsets and its prediction error is evaluated on the held-out subset. This process is repeated for a range of $\\lambda$ values, and the one that minimizes the average prediction error across all folds, denoted $\\lambda^*$, is selected. Our derived mapping provides a direct bridge from this data-driven choice of $\\lambda$ to the BPDN formulation. Once CV yields an optimal $\\lambda^*$, one can compute the corresponding noise tolerance $\\epsilon^*$ using the derived expression for $\\epsilon(\\lambda^*)$. This allows one to solve the (often more intuitive) BPDN problem with a rigorously selected parameter $\\epsilon^* = \\epsilon(\\lambda^*)$ that is expected to provide good out-of-sample prediction performance.",
            "answer": "$$\\boxed{\\sqrt{\\|(I - A A^{\\top}) y\\|_{2}^{2} + \\sum_{i=1}^{n} \\min(|(A^{\\top} y)_i|, \\lambda)^{2}}}$$"
        },
        {
            "introduction": "Beyond theoretical correctness, the practical feasibility of cross-validation often hinges on its computational cost, especially for large datasets or complex models. This problem shifts our focus to the efficiency of the $K$-fold cross-validation procedure when applied to the Lasso regularization path. By analyzing the leading-order complexity and accounting for modern solver optimizations like warm starts, you will derive a surprisingly simple but powerful rule of thumb for the computational overhead of $K$-fold CV, a vital piece of knowledge for any practitioner.",
            "id": "3441833",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (Lasso) regression problem defined for a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$ by the objective\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nevaluated over a decreasing grid of regularization parameters $\\{\\lambda_{\\ell}\\}_{\\ell=1}^{L}$ with $\\lambda_{1}  \\lambda_{2}  \\cdots  \\lambda_{L}  0$. You are to perform $K$-fold cross-validation for regularization parameter selection using a solver that implements warm starts across the grid and reuses data-dependent factorizations within each fold.\n\nAssume the following modeling choices and facts, all of which are widely accepted in sparse optimization practice and analysis:\n- The $K$-fold partition yields $K$ disjoint validation folds of equal size $n/K$, and training folds of size $m = n(K-1)/K$.\n- A coordinate descent Lasso solver is used with cyclic updates and warm starts across $\\lambda$, so that, within a fixed training set, the cost to solve the first grid point $\\lambda_{1}$ from a cold start is $t_{0}$ full sweeps, while each subsequent $\\lambda_{\\ell}$, $\\ell \\geq 2$, requires $t$ sweeps from its warm start. The integers $t_{0}$ and $t$ do not depend on $n$, $p$, $K$, or the fold index, and are treated as constants for leading-order complexity purposes.\n- One full sweep over all $p$ coordinates on a training set of size $m$ has leading-order cost $c \\, m p$ floating-point operations, where $c  0$ is a constant capturing micro-optimizations including factorization reuse such as cached residual updates and stable active-set inner-product maintenance. This constant $c$ is assumed identical across folds and across training on the full dataset, since the same implementation is used.\n- Factorization reuse across $\\lambda$ within a fold is active and captured in the reduced sweep counts $t_{0}$ and $t$; across different folds there is no reuse because the training sets differ. The cost of computing validation losses is negligible compared to fitting costs and may be ignored in the leading-order complexity.\n\nLet $C_{\\mathrm{cv}}$ denote the total leading-order computational cost (in floating-point operations) to compute the $K$-fold cross-validation fits over all $L$ grid points, and let $C_{\\mathrm{full}}$ denote the total leading-order computational cost to compute the full regularization path once on the entire dataset of size $n$ over the same $L$ grid points, using the same solver with warm starts and factorization reuse. Using only the assumptions above and fundamental definitions of $K$-fold cross-validation and Lasso, derive a closed-form expression for the ratio\n$$\nR \\;=\\; \\frac{C_{\\mathrm{cv}}}{C_{\\mathrm{full}}},\n$$\nfully simplified in terms of $K$ only. Your final answer must be this expression. Do not include any intermediate quantities, and do not round or approximate.",
            "solution": "The user wants to find the ratio $R = \\frac{C_{\\mathrm{cv}}}{C_{\\mathrm{full}}}$, where $C_{\\mathrm{cv}}$ is the total computational cost of $K$-fold cross-validation for Lasso regularization path selection, and $C_{\\mathrm{full}}$ is the cost of computing the same path on the full dataset. The derivation will be based on the leading-order costs as defined in the problem statement.\n\nFirst, let's derive an expression for $C_{\\mathrm{full}}$, the cost of fitting the model on the entire dataset of size $n$ over the $L$ regularization parameters.\nThe dataset consists of $n$ samples and $p$ features.\nThe cost of one full sweep of coordinate descent over all $p$ coordinates on a dataset of size $n$ is given as $c n p$ floating-point operations.\nThe solver computes the solution for a grid of $L$ parameters, $\\{\\lambda_{\\ell}\\}_{\\ell=1}^{L}$.\nFor the first parameter, $\\lambda_{1}$, the solver starts from a cold start and requires $t_{0}$ sweeps.\nFor the subsequent $L-1$ parameters, $\\lambda_{2}, \\dots, \\lambda_{L}$, the solver uses warm starts, and each requires $t$ sweeps.\nThe total number of sweeps to compute the full regularization path is therefore $t_{0} + (L-1)t$.\nThe total leading-order cost, $C_{\\mathrm{full}}$, is the product of the cost per sweep and the total number of sweeps:\n$$\nC_{\\mathrm{full}} = (c n p) (t_{0} + (L-1)t)\n$$\n\nNext, we derive an expression for $C_{\\mathrm{cv}}$, the total cost for $K$-fold cross-validation.\nIn $K$-fold cross-validation, the dataset is partitioned into $K$ folds. The procedure is repeated $K$ times. In each iteration $k \\in \\{1, \\dots, K\\}$, one fold is used for validation, and the remaining $K-1$ folds are used for training.\nThe size of each training set is given as $m = n(K-1)/K$.\nThe cost of one full sweep of coordinate descent on a training set of size $m$ is $c m p$. Substituting the expression for $m$:\n$$\n\\text{Cost per sweep per fold} = c \\left(n \\frac{K-1}{K}\\right) p\n$$\nFor each of the $K$ training sets, the solver computes the full regularization path over the same $L$ parameters. The logic for the number of sweeps is identical to the full-dataset case: $t_{0}$ sweeps for $\\lambda_{1}$ and $t$ sweeps for each of the $L-1$ subsequent parameters.\nThe total number of sweeps for one fold's training set is $t_{0} + (L-1)t$.\nThe computational cost to train the model for a single fold, let's call it $C_{\\mathrm{fold}}$, is:\n$$\nC_{\\mathrm{fold}} = \\left(c \\left(n \\frac{K-1}{K}\\right) p\\right) (t_{0} + (L-1)t)\n$$\nThe total cross-validation cost, $C_{\\mathrm{cv}}$, is the sum of the costs for all $K$ folds. Since the training set size is the same for each fold, the cost is the same for each of the $K$ iterations.\n$$\nC_{\\mathrm{cv}} = K \\times C_{\\mathrm{fold}} = K \\left[ \\left(c \\left(n \\frac{K-1}{K}\\right) p\\right) (t_{0} + (L-1)t) \\right]\n$$\nWe can simplify this expression by canceling the $K$ in the numerator with the $K$ in the denominator of the term for the training size:\n$$\nC_{\\mathrm{cv}} = \\left(c n (K-1) p\\right) (t_{0} + (L-1)t)\n$$\nThis can be rewritten as:\n$$\nC_{\\mathrm{cv}} = (K-1) \\left[ (c n p) (t_{0} + (L-1)t) \\right]\n$$\n\nFinally, we compute the desired ratio $R = \\frac{C_{\\mathrm{cv}}}{C_{\\mathrm{full}}}$.\nSubstituting the derived expressions for $C_{\\mathrm{cv}}$ and $C_{\\mathrm{full}}$:\n$$\nR = \\frac{(K-1) \\left[ (c n p) (t_{0} + (L-1)t) \\right]}{(c n p) (t_{0} + (L-1)t)}\n$$\nThe term $(c n p) (t_{0} + (L-1)t)$ is a common factor in both the numerator and the denominator. Since all constants $c, n, p, t_0, t$ are positive and $L \\geq 1$, this factor is non-zero and can be canceled.\nThis leaves:\n$$\nR = K-1\n$$\nThis result is a simplified, closed-form expression in terms of $K$ only, as requested. It indicates that, under the given assumptions, the computational cost of $K$-fold cross-validation is $K-1$ times the cost of fitting the model once on the full dataset.",
            "answer": "$$\n\\boxed{K-1}\n$$"
        },
        {
            "introduction": "The rote application of standard methods can lead to unexpected failures in the challenging regimes where compressed sensing often operates. This exercise explores a critical pitfall of $K$-fold cross-validation when the number of measurements is close to the information-theoretic limit required for stable signal recovery. You will analyze how partitioning the data can inadvertently create training subproblems that are themselves unsolvable, rendering the entire cross-validation procedure invalid and leading to poor parameter selection. This problem will sharpen your critical thinking about the assumptions and limitations inherent in data-driven tuning methods.",
            "id": "3441878",
            "problem": "Consider the linear inverse model in compressed sensing, where one observes $y \\in \\mathbb{R}^m$ via $y = A x_{\\star} + w$, with $A \\in \\mathbb{R}^{m \\times n}$ having independent and identically distributed Gaussian entries with mean $0$ and variance $1/m$, an unknown $s$-sparse vector $x_{\\star} \\in \\mathbb{R}^n$, and additive noise $w \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. A standard estimator is the Least Absolute Shrinkage and Selection Operator (LASSO), which for a given regularization level $\\lambda  0$ solves $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2} \\| A_{\\text{tr}} x - y_{\\text{tr}} \\|_2^2 + \\lambda \\| x \\|_1$ on a training subset $(A_{\\text{tr}}, y_{\\text{tr}})$ of the measurements, and is scored on a disjoint validation subset $(A_{\\text{val}}, y_{\\text{val}})$; the regularization level $\\lambda$ is selected by cross-validation (CV). It is well known and widely used that for Gaussian designs, the minimal sample complexity $m_{\\min}$ required (up to absolute constants and logarithmic factors) for stable recovery of an $s$-sparse vector is on the order of $m_{\\min} \\asymp C \\, s \\log(n/s)$ for some absolute constant $C  0$, and that submatrices formed by selecting rows of $A$ retain the same distributional form. Assume throughout that $\\log$ denotes the natural logarithm.\n\nSuppose $n$, $s$, and $m$ are such that $m$ is only slightly larger than $m_{\\min}$. For concreteness, take $n = 10^5$, $s = 100$, and $C = 2$, and suppose $m = 1500$. You consider $K$-fold cross-validation that holds out a subset of rows of $A$ (and corresponding entries of $y$) as validation and uses the remaining rows for training in each fold.\n\nWhich of the following statements are correct?\n\nA. When $m$ is only slightly above the minimal sample complexity, using equal-sized $K$-fold cross-validation with a small $K$ (for example, $K = 5$) can render each training subproblem under-sampled relative to the information-theoretic threshold, making recovery unstable or impossible; a direct mitigation is to choose $K$ large enough so that every training split uses at least the minimal number of measurements required for stable recovery.\n\nB. Because validation samples are not used in training, cross-validation does not change the sample complexity of the training problems; therefore, if the full problem is above threshold, all training subproblems in $K$-fold cross-validation will also be above threshold regardless of $K$.\n\nC. A suitable stratification to preserve minimal sample complexity is to hold out a random subset of columns (features) for validation while retaining all $m$ measurements for training; this keeps the training subproblems above threshold without changing the distribution of the sensing rows.\n\nD. A stratified split that preserves minimal sample complexity is to form folds whose validation sizes are each at most $m - m_{\\min}$ (so that every training split uses at least $m_{\\min}$ measurements), distributing the validation rows across folds so that each row is used for validation at most once; when $m$ is close to $m_{\\min}$, this generally necessitates using many folds.\n\nE. Near the information-theoretic threshold, two-fold cross-validation is optimal because it maximizes the validation size and therefore reduces variance in the validation error; the training problems remain stable due to the Restricted Isometry Property (RIP), so a small $K$ should be preferred in this regime.\n\nSelect all that apply.",
            "solution": "The core of this problem lies in the requirement that for stable recovery of an $s$-sparse vector via methods like LASSO, the number of measurements must meet or exceed a minimum threshold, $m_{\\min}$. The problem states this threshold is $m_{\\min} \\asymp C s \\log(n/s)$. Given the parameters $n = 10^5$, $s = 100$, $C = 2$, the minimal number of samples is approximately:\n$$m_{\\min} = 2 \\cdot 100 \\cdot \\log\\left(\\frac{10^5}{100}\\right) = 200 \\cdot \\log(1000) \\approx 200 \\cdot 6.9078 = 1381.55$$\nThe total number of available measurements is $m=1500$. Since $m > m_{\\min}$, the full problem is solvable.\n\nIn $K$-fold cross-validation, the $m$ measurements (rows) are partitioned into $K$ disjoint folds of size $m/K$ each. For each run, one fold is used for validation, and the other $K-1$ folds are used for training.\nThe number of measurements for each training subproblem is:\n$$m_{\\text{tr}} = m \\cdot \\frac{K-1}{K} = m \\left(1 - \\frac{1}{K}\\right)$$\nFor the LASSO estimator to be effective in each training run, the number of training measurements $m_{\\text{tr}}$ must itself be sufficient for stable recovery. That is, we must have $m_{\\text{tr}} \\ge m_{\\min}$.\nSubstituting the values:\n$$1500 \\left(1 - \\frac{1}{K}\\right) \\ge 1381.55$$\n$$1 - \\frac{1}{K} \\ge \\frac{1381.55}{1500} \\approx 0.921$$\n$$\\frac{1}{K} \\le 1 - 0.921 = 0.079$$\n$$K \\ge \\frac{1}{0.079} \\approx 12.66$$\nThus, to ensure each training subproblem is well-posed, we must use $K \\ge 13$. This means that a large number of folds is required, a procedure that approaches leave-one-out cross-validation.\n\nNow, we evaluate each option based on this analysis.\n\n**A. When $m$ is only slightly above the minimal sample complexity, using equal-sized $K$-fold cross-validation with a small $K$ (for example, $K = 5$) can render each training subproblem under-sampled relative to the information-theoretic threshold, making recovery unstable or impossible; a direct mitigation is to choose $K$ large enough so that every training split uses at least the minimal number of measurements required for stable recovery.**\nLet's test this with the example $K=5$.\nThe number of training samples would be $m_{\\text{tr}} = 1500 \\left(1 - \\frac{1}{5}\\right) = 1500 \\cdot \\frac{4}{5} = 1200$.\nSince $1200  m_{\\min} \\approx 1382$, the training subproblem is indeed under-sampled. This means that for any value of $\\lambda$, the LASSO solutions on these training sets will be unstable and will not reliably recover the true sparse signal. The proposed mitigation is to choose a large enough $K$ such that $m_{\\text{tr}} \\ge m_{\\min}$, which our derivation shows is $K \\ge 13$. This statement is entirely consistent with our analysis.\n**Verdict: Correct.**\n\n**B. Because validation samples are not used in training, cross-validation does not change the sample complexity of the training problems; therefore, if the full problem is above threshold, all training subproblems in $K$-fold cross-validation will also be above threshold regardless of $K$.**\nThis statement contains a fundamental misunderstanding. The \"sample complexity\" $m_{\\min}$ is an intrinsic property of the problem size ($n, s$) and does not change. However, cross-validation explicitly *reduces* the number of samples available for training ($m_{\\text{tr}} = m(1-1/K)  m$). The core of the issue is precisely that while the full problem is above the threshold ($m > m_{\\min}$), the training subproblems may not be ($m_{\\text{tr}}$ can be less than $m_{\\min}$). As shown with $K=5$, this is false.\n**Verdict: Incorrect.**\n\n**C. A suitable stratification to preserve minimal sample complexity is to hold out a random subset of columns (features) for validation while retaining all $m$ measurements for training; this keeps the training subproblems above threshold without changing the distribution of the sensing rows.**\nThis proposes a different CV strategy: splitting the columns of $A$ (features), not the rows (measurements). In the context of sparse recovery, the goal is to identify the non-zero entries of the vector $x_{\\star} \\in \\mathbb{R}^n$. Holding out columns means that the training procedure would not be able to estimate the coefficients corresponding to those columns. This is not a valid method for tuning the regularization parameter $\\lambda$, whose purpose is to govern sparsity and prediction error over the entire feature space. The standard and proper way to perform CV for this problem is to simulate generalization to new measurements, which requires splitting the rows.\n**Verdict: Incorrect.**\n\n**D. A stratified split that preserves minimal sample complexity is to form folds whose validation sizes are each at most $m - m_{\\min}$ (so that every training split uses at least $m_{\\min}$ measurements), distributing the validation rows across folds so that each row is used for validation at most once; when $m$ is close to $m_{\\min}$, this generally necessitates using many folds.**\nThis statement formalizes the condition for a valid CV split. The size of a training set is $m_{\\text{tr}} = m - m_{\\text{val}}$, where $m_{\\text{val}}$ is the size of the validation set. The condition for stable recovery is $m_{\\text{tr}} \\ge m_{\\min}$, which is equivalent to $m - m_{\\text{val}} \\ge m_{\\min}$, or $m_{\\text{val}} \\le m - m_{\\min}$. This first part of the statement is correct.\nThe second part states that if $m$ is close to $m_{\\min}$ (i.e., $m - m_{\\min}$ is small), this necessitates many folds. If we use a standard $K$-fold CV scheme where all $m$ rows are used for validation exactly once over the $K$ folds, the total number of validation instances is $m$. If each validation fold has size $m_{\\text{val}}$, then $K \\cdot m_{\\text{val}} \\approx m$. Combining this with the constraint $m_{\\text{val}} \\le m - m_{\\min}$, we get $K \\ge m / m_{\\text{val}} \\ge m / (m - m_{\\min})$. When $m$ is close to $m_{\\min}$, the denominator $m - m_{\\min}$ is a small number, making the required $K$ large. Our calculation gives $m/(m-m_{\\min}) \\approx 1500 / (1500-1381.55) \\approx 12.66$, confirming that many folds are needed.\n**Verdict: Correct.**\n\n**E. Near the information-theoretic threshold, two-fold cross-validation is optimal because it maximizes the validation size and therefore reduces variance in the validation error; the training problems remain stable due to the Restricted Isometry Property (RIP), so a small $K$ should be preferred in this regime.**\nLet's analyze $K=2$. The training size becomes $m_{\\text{tr}} = m/2 = 1500/2 = 750$. This is significantly below the required minimum of $m_{\\min} \\approx 1382$. Therefore, the training subproblems are grossly undersampled and will not be stable. The Restricted Isometry Property (RIP) is precisely the property that fails to hold when the number of measurements is insufficient for the given sparsity level. The claim that maximizing validation size is optimal is also wrong in this context; it creates severely biased (in fact, useless) models on the training side.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}