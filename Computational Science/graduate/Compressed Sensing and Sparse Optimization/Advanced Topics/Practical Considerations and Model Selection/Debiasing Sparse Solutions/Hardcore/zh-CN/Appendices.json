{
    "hands_on_practices": [
        {
            "introduction": "Lasso 是稀疏恢复中最常用的工具之一，但其 $\\ell_1$ 惩罚项在筛选变量的同时，也会对非零系数产生收缩，从而引入系统性偏差。本练习旨在通过一个具体的数值例子，让您亲手实现 Lasso 求解器，量化其估计偏差，并应用核心的去偏技术——最小二乘重拟合（least-squares refitting）——来修正这些系数，直观地比较去偏前后的效果。 通过这个实践，您将对 Lasso 固有的偏差问题以及如何通过简单的后处理步骤显著改善估计精度有更深刻的理解。",
            "id": "3442554",
            "problem": "考虑压缩感知和稀疏优化中的典型稀疏恢复设定。设感知矩阵表示为 $A \\in \\mathbb{R}^{m \\times n}$，真实稀疏信号表示为 $x^{\\star} \\in \\mathbb{R}^{n}$。观测到的测量值由 $y = A x^{\\star} + \\eta$ 给出，其中 $\\eta \\in \\mathbb{R}^{m}$ 代表加性噪声。最小绝对收缩和选择算子 (Lasso) 估计 $\\widehat{x}_{\\text{Lasso}}$ 定义为凸泛函\n$$\n\\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n的最小化子，其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\|\\cdot\\|_{1}$ 表示 $\\ell_{1}$ 范数。众所周知，$\\ell_{1}$ 惩罚会引起收缩，从而在估计器中相对于 $x^{\\star}$ 产生坐标级别的偏差。\n\n你需要从第一性原理出发，实现一个迭代软阈值算法 (ISTA) 来计算 $\\widehat{x}_{\\text{Lasso}}$。使用 Lasso 目标函数的分解，将其分为光滑项 $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和非光滑项 $h(x) = \\lambda \\|x\\|_{1}$。$g$ 的梯度是 $\\nabla g(x) = A^{\\top}(A x - y)$，$h$ 的近端算子是坐标级别的软阈值算子。选择一个常数步长 $t$ 满足 $0  t \\le \\frac{1}{L}$，其中 $L$ 是 $\\nabla g$ 的 Lipschitz 常数，等于 $A$ 的谱范数的平方，即 $L = \\|A\\|_{2}^{2}$。从 $x^{(0)} = 0$ 开始，生成迭代\n$$\nx^{(k+1)} = \\operatorname{soft}(x^{(k)} - t \\nabla g(x^{(k)}), \\lambda t),\n$$\n其中对于任何 $z \\in \\mathbb{R}^{n}$ 和 $\\tau \\ge 0$，该算子按坐标定义为\n$$\n\\operatorname{soft}(z_{i}, \\tau) = \\operatorname{sign}(z_{i}) \\cdot \\max(|z_{i}| - \\tau, 0).\n$$\n\n计算出 $\\widehat{x}_{\\text{Lasso}}$ 后，使用一个固定阈值 $\\tau_{\\text{sup}} > 0$ 定义估计的支撑集 $\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}| > \\tau_{\\text{sup}}\\}$。通过求解以下问题，对限制在 $\\widehat{S}$ 上的模型进行最小二乘重拟合\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\n其中 $A_{\\widehat{S}}$ 表示由 $A$ 中索引为 $\\widehat{S}$ 的列组成的子矩阵。使用 Moore-Penrose 伪逆获得 $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$；然后通过设置 $(\\widehat{x}_{\\text{refit}})_{\\widehat{S}} = z^{\\text{LS}}$ 以及对于 $i \\notin \\widehat{S}$ 设置 $(\\widehat{x}_{\\text{refit}})_{i} = 0$ 来定义重拟合估计 $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$。\n\n对于 $\\widehat{x}_{\\text{Lasso}}$ 和 $\\widehat{x}_{\\text{refit}}$，计算坐标级别的偏差向量 $b = \\widehat{x} - x^{\\star}$，并使用平均绝对偏差\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|\n$$\n和最大绝对偏差\n$$\n\\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|\n$$\n来总结偏差。\n\n实现一个程序，对以下测试套件执行上述操作。每个测试用例都指定了 $A$、$x^{\\star}$、$\\eta$ 和 $\\lambda$，并要求使用 $\\tau_{\\text{sup}} = 10^{-6}$。\n\n测试用例 1 (良态，中等正则化):\n- 维度: $m = 8$, $n = 6$。\n- 矩阵 $A$:\n$$\n\\begin{bmatrix}\n0.36  -0.07  0.22  0.10  -0.31  0.41 \\\\\n-0.12  0.25  0.30  -0.41  0.05  -0.08 \\\\\n0.45  0.18  -0.08  0.03  0.26  -0.19 \\\\\n0.05  -0.31  0.41  0.17  -0.02  0.12 \\\\\n-0.27  0.11  -0.36  -0.28  0.44  0.06 \\\\\n0.14  0.39  0.07  -0.02  -0.23  0.28 \\\\\n0.31  -0.22  0.18  -0.35  0.09  -0.27 \\\\\n-0.19  0.33  -0.12  0.29  0.37  -0.15\n\\end{bmatrix}\n$$\n- 真实值 $x^{\\star} = [0.0,\\, 1.5,\\, 0.0,\\, -2.0,\\, 0.0,\\, 0.5]$。\n- 噪声 $\\eta = [0.01,\\,-0.02,\\,0.015,\\,0.0,\\,-0.005,\\,0.008,\\,0.012,\\,-0.009]$。\n- 观测值 $y = A x^{\\star} + \\eta$。\n- 正则化 $\\lambda = 0.1$。\n\n测试用例 2 (极大正则化；空支撑集边缘情况):\n- 使用与测试用例 1 中相同的 $A$。\n- 真实值 $x^{\\star} = [0.0,\\, 0.0,\\, 2.0,\\, 0.0,\\, 0.0,\\, 0.0]$。\n- 噪声 $\\eta = [0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0]$。\n- 观测值 $y = A x^{\\star} + \\eta$。\n- 正则化 $\\lambda = 100.0$。\n\n测试用例 3 (相关列；中等正则化):\n- 维度: $m = 8$, $n = 6$。\n- 矩阵 $A$:\n$$\n\\begin{bmatrix}\n0.40  0.50  0.49  -0.10  0.02  0.33 \\\\\n-0.20  -0.25  -0.24  0.12  -0.18  -0.31 \\\\\n0.35  0.42  0.41  -0.22  0.27  0.05 \\\\\n-0.05  -0.06  -0.06  0.30  0.12  -0.28 \\\\\n0.10  0.12  0.12  -0.26  -0.33  0.18 \\\\\n0.28  0.35  0.34  0.04  0.15  -0.11 \\\\\n-0.17  -0.21  -0.21  0.09  -0.07  0.24 \\\\\n0.22  0.27  0.26  -0.19  0.31  -0.09\n\\end{bmatrix}\n$$\n- 真实值 $x^{\\star} = [0.0,\\, 1.2,\\, 1.0,\\, 0.0,\\, 0.0,\\, 0.0]$。\n- 噪声 $\\eta = [0.003,\\,-0.004,\\,0.002,\\,0.006,\\,-0.005,\\,-0.001,\\,0.004,\\,-0.003]$。\n- 观测值 $y = A x^{\\star} + \\eta$。\n- 正则化 $\\lambda = 0.15$。\n\n对于每个测试用例，计算：\n1. 通过 ISTA 计算 Lasso 估计 $\\widehat{x}_{\\text{Lasso}}$，步长为 $t = 1/\\|A\\|_{2}^{2}$，从零初始化，迭代直到满足收敛条件 $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$ 或达到最大迭代次数 $20000$ 次。\n2. 使用 $\\tau_{\\text{sup}} = 10^{-6}$ 估计支撑集 $\\widehat{S}$。\n3. 使用 Moore-Penrose 伪逆对 $\\widehat{S}$ 进行最小二乘计算，得到重拟合估计 $\\widehat{x}_{\\text{refit}}$。\n4. 计算 $\\widehat{x}_{\\text{Lasso}}$ 和 $\\widehat{x}_{\\text{refit}}$ 的平均绝对偏差和最大绝对偏差。\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。对于每个测试用例，按顺序 $[\\operatorname{MAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MAB}(\\widehat{x}_{\\text{refit}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{refit}} - x^{\\star})]$ 输出一个包含四个浮点数的列表。将三个用例的列表聚合到一个顶级列表中，最终输出格式为\n$$\n\\big[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}], [r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}], [r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}] \\big].\n$$\n不涉及物理单位，所有答案必须是实数。",
            "solution": "该问题要求在压缩感知的背景下，实现并比较两种稀疏信号估计算法：Lasso 及其使用最小二乘重拟合的去偏变体。我们获得了为三个不同测试用例执行此任务所需的完整算法和分析框架。解决方案涉及数值优化、线性代数和统计评估。\n\n核心问题是为线性方程组 $y = Ax + \\eta$ 找到一个稀疏解 $x \\in \\mathbb{R}^{n}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个感知矩阵，$y \\in \\mathbb{R}^{m}$ 是带噪测量值，$\\eta \\in \\mathbb{R}^{m}$ 是加性噪声。假设真实信号 $x^{\\star} \\in \\mathbb{R}^{n}$ 是稀疏的。\n\n首先，我们计算 Lasso 估计 $\\widehat{x}_{\\text{Lasso}}$，它被定义为以下凸优化问题的解：\n$$\n\\widehat{x}_{\\text{Lasso}} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}。\n$$\n在这里，$\\lambda \\ge 0$ 是一个正则化参数，用于控制数据保真项 $\\|A x - y\\|_{2}^{2}$ 和稀疏性诱导惩罚项 $\\|x\\|_{1}$ 之间的权衡。目标函数是一个光滑、凸、可微函数 $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和一个非光滑、凸函数 $h(x) = \\lambda \\|x\\|_{1}$ 的和。\n\n问题指定使用迭代软阈值算法 (ISTA)——一种近端梯度方法——来求解 $\\widehat{x}_{\\text{Lasso}}$。从初始猜测 $x^{(0)} = 0$ 开始，ISTA 通过以下迭代更新规则生成一系列估计值：\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla g(x^{(k)}))。\n$$\n此更新包括两个步骤：对光滑部分 $g(x)$ 进行标准的梯度下降步骤，以及应用非光滑部分 $h(x)$ 的近端算子。$g(x)$ 的梯度是 $\\nabla g(x) = A^{\\top}(A x - y)$。对于 $h(x) = \\lambda \\|x\\|_{1}$，其近端算子是软阈值函数，按坐标方式应用：\n$$\n(\\operatorname{prox}_{t h}(z))_i = \\operatorname{soft}(z_i, \\lambda t) = \\operatorname{sign}(z_i) \\cdot \\max(|z_i| - \\lambda t, 0)。\n$$\n为保证收敛，步长 $t$ 必须满足 $0  t \\le 1/L$，其中 $L$ 是梯度 $\\nabla g(x)$ 的 Lipschitz 常数。对于此问题，$L$ 是矩阵 $A$ 的谱范数的平方，即 $L = \\|A\\|_{2}^{2} = \\sigma_{\\max}^2(A)$，其中 $\\sigma_{\\max}(A)$ 是 $A$ 的最大奇异值。我们将使用步长的上界，$t = 1/L = 1/\\|A\\|_{2}^{2}$。迭代过程将持续进行，直到估计值的变化足够小，即 $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$，或达到 20000 次的最大迭代次数。\n\n众所周知，Lasso 中的 $\\ell_1$ 惩罚会导致收缩，这会在估计的非零系数中引入偏差。为了减轻这种偏差，我们通过一个两阶段过程计算第二个估计器 $\\widehat{x}_{\\text{refit}}$。首先，我们识别 Lasso 解的支撑集（非零系数的索引集）：\n$$\n\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}| > \\tau_{\\text{sup}} \\},\n$$\n其中 $\\tau_{\\text{sup}} = 10^{-6}$ 是一个用于考虑数值精度的小阈值。\n\n其次，我们对这个估计的支撑集进行无惩罚的普通最小二乘 (OLS) 拟合。这涉及求解：\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\n其中 $A_{\\widehat{S}}$ 是由 $A$ 中索引在 $\\widehat{S}$ 内的列构成的子矩阵。该 OLS 问题的解由 $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$ 给出，其中 $A_{\\widehat{S}}^{\\dagger}$ 是 $A_{\\widehat{S}}$ 的 Moore-Penrose 伪逆。即使 $A_{\\widehat{S}}$ 是秩亏的或病态的，使用伪逆也能确保一个唯一、稳定的解。然后，通过将在支撑集 $\\widehat{S}$ 上的系数设置为 $z^{\\text{LS}}$ 并将所有其他系数设置为零来构建重拟合估计 $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$：\n$$\n(\\widehat{x}_{\\text{refit}})_i = \\begin{cases} (z^{\\text{LS}})_j  \\text{if } i \\text{ is the } j\\text{-th index in } \\widehat{S} \\\\ 0  \\text{if } i \\notin \\widehat{S} \\end{cases}.\n$$\n如果估计的支撑集 $\\widehat{S}$ 为空，则重拟合估计为零向量，即 $\\widehat{x}_{\\text{refit}} = 0$。\n\n最后，为了评估两种估计器的性能，我们为每个估计 $\\widehat{x} \\in \\{\\widehat{x}_{\\text{Lasso}}, \\widehat{x}_{\\text{refit}}\\}$ 计算坐标级别的偏差向量 $b = \\widehat{x} - x^{\\star}$。偏差通过两个指标进行总结：平均绝对偏差 (MAB) 和最大绝对偏差 (MaxAB)，定义如下：\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|, \\quad \\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|。\n$$\n\n我们将把这整个过程应用于所提供的三个测试用例中的每一个。对于每个用例，我们首先根据给定的 $A$、$x^{\\star}$ 和 $\\eta$ 构建测量向量 $y = A x^{\\star} + \\eta$。然后，我们实现 ISTA 算法以找到 $\\widehat{x}_{\\text{Lasso}}$，接着进行支撑集识别和最小二乘重拟合以找到 $\\widehat{x}_{\\text{refit}}$。随后，我们计算四个指定的偏差指标，并按要求的格式报告它们。这个系统化的过程使得在不同的问题设置条件下，可以直接比较 Lasso 估计器和去偏重拟合估计器的偏差属性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the debiasing sparse solutions problem for three test cases.\n    For each case, it computes the Lasso estimate via ISTA, performs\n    least-squares refitting, and calculates bias metrics for both estimates.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def soft_threshold(z, tau):\n        \"\"\"\n        Soft-thresholding operator.\n        \"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def ista_solver(A, y, lambda_val, max_iter=20000, tol=1e-8):\n        \"\"\"\n        Iterative Soft-Thresholding Algorithm (ISTA) for solving Lasso.\n        \"\"\"\n        m, n = A.shape\n        # L = np.linalg.norm(A, ord=2)**2 is slow. Using SVD is faster.\n        # As L is the largest eigenvalue of A.T @ A, we compute it directly.\n        L = np.max(np.linalg.eigvalsh(A.T @ A))\n        t = 1.0 / L\n        \n        x_k = np.zeros(n)\n        for _ in range(max_iter):\n            grad = A.T @ (A @ x_k - y)\n            z = x_k - t * grad\n            x_k_plus_1 = soft_threshold(z, lambda_val * t)\n            \n            if np.linalg.norm(x_k_plus_1 - x_k) = tol:\n                break\n            \n            x_k = x_k_plus_1\n        \n        return x_k\n\n    def refit_least_squares(A, y, x_lasso, tau_sup=1e-6):\n        \"\"\"\n        Performs least-squares refitting on the support of the Lasso estimate.\n        \"\"\"\n        n = A.shape[1]\n        support = np.where(np.abs(x_lasso) > tau_sup)[0]\n        \n        x_refit = np.zeros(n)\n        \n        if support.size > 0:\n            A_S = A[:, support]\n            try:\n                z_ls = np.linalg.pinv(A_S) @ y\n                x_refit[support] = z_ls\n            except np.linalg.LinAlgError:\n                # This case is unlikely with pseudoinverse but good practice.\n                pass\n                \n        return x_refit\n\n    def calculate_bias_metrics(x_est, x_star):\n        \"\"\"\n        Calculates MAB and MaxAB for a given estimate.\n        \"\"\"\n        bias = x_est - x_star\n        mab = np.mean(np.abs(bias))\n        max_ab = np.max(np.abs(bias))\n        return mab, max_ab\n\n    # --- Test Case Definitions ---\n\n    A1 = np.array([\n        [0.36, -0.07, 0.22, 0.10, -0.31, 0.41],\n        [-0.12, 0.25, 0.30, -0.41, 0.05, -0.08],\n        [0.45, 0.18, -0.08, 0.03, 0.26, -0.19],\n        [0.05, -0.31, 0.41, 0.17, -0.02, 0.12],\n        [-0.27, 0.11, -0.36, -0.28, 0.44, 0.06],\n        [0.14, 0.39, 0.07, -0.02, -0.23, 0.28],\n        [0.31, -0.22, 0.18, -0.35, 0.09, -0.27],\n        [-0.19, 0.33, -0.12, 0.29, 0.37, -0.15]\n    ])\n\n    A3 = np.array([\n        [0.40, 0.50, 0.49, -0.10, 0.02, 0.33],\n        [-0.20, -0.25, -0.24, 0.12, -0.18, -0.31],\n        [0.35, 0.42, 0.41, -0.22, 0.27, 0.05],\n        [-0.05, -0.06, -0.06, 0.30, 0.12, -0.28],\n        [0.10, 0.12, 0.12, -0.26, -0.33, 0.18],\n        [0.28, 0.35, 0.34, 0.04, 0.15, -0.11],\n        [-0.17, -0.21, -0.21, 0.09, -0.07, 0.24],\n        [0.22, 0.27, 0.26, -0.19, 0.31, -0.09]\n    ])\n\n    test_cases = [\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 1.5, 0.0, -2.0, 0.0, 0.5]),\n            \"eta\": np.array([0.01, -0.02, 0.015, 0.0, -0.005, 0.008, 0.012, -0.009]),\n            \"lambda_val\": 0.1\n        },\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 0.0, 2.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.zeros(8),\n            \"lambda_val\": 100.0\n        },\n        {\n            \"A\": A3,\n            \"x_star\": np.array([0.0, 1.2, 1.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.array([0.003, -0.004, 0.002, 0.006, -0.005, -0.001, 0.004, -0.003]),\n            \"lambda_val\": 0.15\n        }\n    ]\n\n    # --- Main Processing Loop ---\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        x_star = case[\"x_star\"]\n        eta = case[\"eta\"]\n        lambda_val = case[\"lambda_val\"]\n        \n        # 1. Compute measurements\n        y = A @ x_star + eta\n        \n        # 2. Compute Lasso estimate\n        x_lasso = ista_solver(A, y, lambda_val)\n        \n        # 3. Compute refitted estimate\n        x_refit = refit_least_squares(A, y, x_lasso)\n        \n        # 4. Calculate bias metrics\n        mab_lasso, maxab_lasso = calculate_bias_metrics(x_lasso, x_star)\n        mab_refit, maxab_refit = calculate_bias_metrics(x_refit, x_star)\n        \n        case_results = [mab_lasso, mab_refit, maxab_lasso, maxab_refit]\n        results.append(case_results)\n\n    # --- Final Output ---\n    # Convert list of lists to string representation as specified.\n    # The default str(list) includes spaces after commas, which is acceptable.\n    # The join then combines these string representations with a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "作为对标准 Lasso 偏差问题的一种改进，重加权 $\\ell_1$ 方法通过迭代地调整惩罚权重，旨在更精确地逼近 $\\ell_0$ 伪范数，从而在变量选择阶段就减轻对大系数的过度收缩。 这个练习将引导您实现一个重加权 $\\ell_1$ 算法，观察其在迭代过程中支持集和系数的变化，并最终结合最小二乘重拟合来进一步提升估计的准确性，从而探索一种更为精细的去偏策略。",
            "id": "3442510",
            "problem": "您将处理一个在线性逆问题框架下的稀疏恢复与去偏任务。设 $A \\in \\mathbb{R}^{m \\times n}$，$x^\\star \\in \\mathbb{R}^n$ 是一个 $k$-稀疏向量（最多有 $k$ 个非零项），$y \\in \\mathbb{R}^m$ 是由 $y = A x^\\star + \\eta$ 建模的带噪观测，其中 $\\eta$ 是加性噪声。稀疏性诱导点估计通过加权$\\ell_1$正则化最小二乘目标函数来构建，其基本的非加权形式是压缩感知中用于稀疏优化的标准工具。任务是实现两轮重加权$\\ell_1$最小化，然后对最终的支撑集应用普通最小二乘重构，以进一步减小收缩偏差。\n\n基本基础与给定设置：\n- 数据模型为 $y = A x^\\star + \\eta$，且测量矩阵 $A$ 已知。\n- 初始稀疏估计量通过求解一个凸优化问题获得，该问题惩罚系数的$\\ell_1$范数，这是稀疏恢复中对计数($\\ell_0$)伪范数广为接受的替代。\n- 重加权通过迭代地为小系数分配较大权重、为大系数分配较小权重来减小偏差，而在选定的支撑集上进行最小二乘重构可减小由$\\ell_1$惩罚项引起的收缩偏差。\n\n矩阵和向量规范：\n- 使用 $m = 64$，$n = 128$ 和 $k = 10$。\n- 生成矩阵 $A$，其元素独立地从均值为零、经 $1/\\sqrt{m}$ 缩放的正态分布中采样，即 $A_{ij} \\sim \\mathcal{N}(0, 1/m)$。\n- 生成一个支撑集大小为 $k$ 的真实稀疏向量 $x^\\star$，方法是从中无放回地均匀随机选择 $k$ 个索引，并将这些条目设置为从 $\\mathcal{N}(0,1)$ 中独立抽样得到的值；将所有其他条目设置为零。\n- 生成噪声 $\\eta$，其元素独立地从 $\\mathcal{N}(0, \\sigma^2)$ 中抽样，并设置 $y = A x^\\star + \\eta$。\n- 对所有随机数生成使用固定的随机种子 $12345$，以确保可复现性。\n- 在所有测试用例中使用单一固定的噪声标准差 $\\sigma = 0.02$，以保持 $y$ 固定。\n\n需要实现的算法任务：\n1. 在一个重加权方案中，从非加权情况开始，求解加权$\\ell_1$正则化最小二乘问题两次：\n   - 迭代 0：所有权重等于 $1$，求解 $x^{(0)}$。\n   - 迭代 1：使用一个严格为正的参数 $\\epsilon$ 从 $x^{(0)}$ 计算新权重，并求解 $x^{(1)}$。\n   - 迭代 2：从 $x^{(1)}$ 计算新权重，并求解 $x^{(2)}$。\n   加权优化问题具有以下形式\n   $$\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i|,$$\n   其中 $w_i$ 是在每次重加权迭代中确定的正权重。您必须使用源自一阶最优性和近端微积分的原则性算法（迭代软阈值算法）来获得每个加权问题的解，确保步长的选择基于数据保真项梯度的Lipschitz常数。初始权重必须为所有 $i$ 都有 $w_i = 1$。\n2. 将估计值 $x$ 的支撑集定义为幅值超过阈值 $\\tau$ 的条目的索引集：$S(x) = \\{ i \\in \\{1,\\dots,n\\} : |x_i| > \\tau \\}$。\n3. 通过求解以下问题，对最终支撑集 $S(x^{(2)})$ 进行最小二乘重构\n   $$\\min_{z \\in \\mathbb{R}^{|S(x^{(2)})|}} \\|A_{S(x^{(2)})} z - y\\|_2^2,$$\n   然后将解嵌入回 $\\mathbb{R}^n$ 中，方法是将系数放置在 $S(x^{(2)})$ 上，其他位置为零。\n\n报告要求：\n- 对每个测试用例，报告：\n  1. 初始非加权求解后的支撑集大小， $|S(x^{(0)})|$（一个整数）。\n  2. 第二次重加权求解后的支撑集大小， $|S(x^{(2)})|$（一个整数）。\n  3. 从迭代 0 到 1 的支撑集变化幅度，定义为对称差集的大小 $|S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|$（一个整数）。\n  4. 从迭代 1 到 2 的支撑集变化幅度，定义为 $|S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|$（一个整数）。\n  5. $x^{(2)}$ 在真实支撑集 $S(x^\\star)$ 上的平均系数幅值偏差，定义为\n     $$b_{\\text{pre}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{(2)}_i| - |x^\\star_i| \\right) \\in \\mathbb{R}$$\n     （一个浮点数；负值表示相对于真实值的收缩）。\n  6. 最小二乘重构后在真实支撑集上的平均系数幅值偏差，定义为\n     $$b_{\\text{post}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{\\text{LS}}_i| - |x^\\star_i| \\right) \\in \\mathbb{R},$$\n     其中 $x^{\\text{LS}}$ 是限制在 $S(x^{(2)})$ 上的最小二乘重构（一个浮点数）。\n\n测试套件：\n在以下四个测试用例上运行您的程序，所有用例均使用上述方法构造的相同 $A$ 和 $y$（$\\sigma = 0.02$，种子为 $12345$）。每个测试用例指定正则化参数 $\\lambda$、重加权参数 $\\epsilon$ 和支撑集阈值 $\\tau$：\n- 测试用例 1：$\\lambda = 0.05$，$\\epsilon = 10^{-3}$，$\\tau = 10^{-4}$。\n- 测试用例 2：$\\lambda = 0.10$，$\\epsilon = 10^{-3}$，$\\tau = 10^{-4}$。\n- 测试用例 3：$\\lambda = 0.05$，$\\epsilon = 10^{-6}$，$\\tau = 10^{-4}$。\n- 测试用例 4：$\\lambda = 0.05$，$\\epsilon = 10^{-3}$，$\\tau = 10^{-2}$。\n\n输出格式：\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例按给定顺序贡献一个子列表，子列表中的元素严格按照 $[|S(x^{(0)})|, |S(x^{(2)})|, |S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|, |S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|, b_{\\text{pre}}, b_{\\text{post}}]$ 的顺序排列。例如，整体输出应如下所示：\n$$\\texttt{[[s0\\_1,s2\\_1,c01\\_1,c12\\_1,bpre\\_1,bpost\\_1],[s0\\_2,s2\\_2,c01\\_2,c12\\_2,bpre\\_2,bpost\\_2],[s0\\_3,s2\\_3,c01\\_3,c12\\_3,bpre\\_3,bpost\\_3],[s0\\_4,s2\\_4,c01\\_4,c12\\_4,bpre\\_4,bpost\\_4]]}$$\n本问题不涉及物理单位，因此您必须报告纯数字。不涉及角度。百分比（如果出现）必须表示为小数，但本任务不要求输出百分比。",
            "solution": "该问题要求实现并评估一个用于稀疏信号恢复的重加权$\\ell_1$最小化算法，并随后执行一个最小二乘去偏步骤。该任务被构建为一个线性逆问题，这是信号处理、统计学和机器学习中的一个标准框架。\n\n底层数据生成模型由以下线性方程给出：\n$$ y = A x^\\star + \\eta $$\n其中 $x^\\star \\in \\mathbb{R}^n$ 是我们旨在恢复的未知 $k$-稀疏信号，$A \\in \\mathbb{R}^{m \\times n}$ 是测量或传感矩阵，$\\eta \\in \\mathbb{R}^m$ 代表加性噪声，$y \\in \\mathbb{R}^m$ 是观测到的测量向量。维度指定为 $m = 64$，$n = 128$，稀疏度为 $k = 10$。矩阵 $A$ 的元素独立地从正态分布 $\\mathcal{N}(0, 1/m)$ 中抽样，真实信号 $x^\\star$ 的非零元素从 $\\mathcal{N}(0,1)$ 中抽样，噪声分量从 $\\mathcal{N}(0, \\sigma^2)$ 中抽样，其中 $\\sigma = 0.02$。\n\n为了从 $y$ 和 $A$ 中恢复 $x^\\star$ 的估计，我们求解以下加权$\\ell_1$正则化最小二乘优化问题：\n$$ \\min_{x \\in \\mathbb{R}^n} J(x) \\quad \\text{其中} \\quad J(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i| $$\n这里，$\\lambda > 0$ 是一个在数据保真度与稀疏性之间取得平衡的正则化参数，$w_i > 0$ 是可用于改进解的权重。该目标函数是两部分凸函数的和：一个平滑、可微的数据保真项 $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ 和一个非光滑、凸的正则化项 $g(x) = \\lambda \\sum_{i=1}^n w_i |x_i|$。\n\n这种结构使该问题适合使用近端梯度法求解。所采用的具体算法是迭代软阈值算法（ISTA），它遵循以下更新规则：\n$$ x_{t+1} = \\text{prox}_{\\alpha g} \\left( x_t - \\alpha \\nabla f(x_t) \\right) $$\n其中 $t$ 是迭代索引，$\\alpha$ 是步长。数据保真项的梯度是 $\\nabla f(x) = A^T(Ax - y)$。加权$\\ell_1$范数的近端算子是按元素的软阈值函数：\n$$ \\left( \\text{prox}_{\\alpha g}(z) \\right)_i = \\text{S}_{\\alpha \\lambda w_i}(z_i) = \\text{sign}(z_i) \\max(|z_i| - \\alpha \\lambda w_i, 0) $$\n为确保ISTA收敛，步长 $\\alpha$ 的选择必须满足 $0  \\alpha \\le 1/L$，其中 $L$ 是梯度 $\\nabla f(x)$ 的Lipschitz常数。该常数等于 $A^T A$ 的最大特征值，也等于 $A$ 的最大奇异值的平方，即 $L = \\sigma_{\\max}(A)^2$。我们将使用步长 $\\alpha = 1/L$。\n\n因此，完整的ISTA更新步骤是：\n$$ x_{t+1} = \\text{S}_{\\frac{\\lambda}{L} w} \\left( x_t - \\frac{1}{L} A^T(Ax_t - y) \\right) $$\n其中软阈值操作是按元素应用的，阈值向量由权重 $w$ 构成。\n\n问题指定了一个重加权方案来减小$\\ell_1$正则化的内在偏差。该方案包括三个主要求解步骤：\n1.  **迭代 0**：使用非加权问题求解 $x^{(0)}$，这对应于将所有权重设置为 $w_i^{(0)} = 1$。\n2.  **迭代 1**：基于第一步的结果更新权重。$x^{(0)}$ 中较大的系数幅值表明它更可能属于真实支撑集，因此应受到较小的惩罚。权重更新为 $w_i^{(1)} = 1 / (|x_i^{(0)}| + \\epsilon)$，其中 $\\epsilon > 0$ 是一个确保稳定性的微小参数。使用这些新权重执行ISTA求解以获得 $x^{(1)}$。\n3.  **迭代 2**：重复此过程。从 $x^{(1)}$ 计算新权重为 $w_i^{(2)} = 1 / (|x_i^{(1)}| + \\epsilon)$，并通过求解相应的加权问题找到最终的正则化估计 $x^{(2)}$。\n\n从ISTA得到的解向量 $x^{(j)}$ 并非完全稀疏。必须通过阈值化来确定支撑集。一个估计 $x$ 的支撑集定义为 $S(x) = \\{ i : |x_i| > \\tau \\}$，其中 $\\tau$ 是一个很小的阈值。\n\n最后，为了进一步减轻由$\\ell_1$惩罚项引起的收缩偏差，执行一个最小二乘重构步骤。这涉及求解一个限制在最终支撑集 $S_{\\text{final}} = S(x^{(2)})$ 上的非正则化最小二乘问题。设 $A_{S_{\\text{final}}}$ 是 $A$ 中仅包含由 $S_{\\text{final}}$ 索引的列的子矩阵。通过求解以下问题找到去偏系数 $z^{\\text{LS}}$：\n$$ \\min_{z \\in \\mathbb{R}^{|S_{\\text{final}}|}} \\|A_{S_{\\text{final}}} z - y\\|_2^2 $$\n其闭式解为 $z^{\\text{LS}} = (A_{S_{\\text{final}}}^T A_{S_{\\text{final}}})^\\dagger A_{S_{\\text{final}}}^T y$，其中 $\\dagger$ 表示Moore-Penrose伪逆，对于满秩矩阵 $B$，可简化为 $(B^T B)^{-1} B^T$。最终的去偏估计 $x^{\\text{LS}}$ 是通过将 $z^{\\text{LS}}$ 的系数放置到支撑集索引 $S_{\\text{final}}$ 上，并将所有其他条目设为零来构建的。\n\n整个过程针对四个测试用例执行，这些用例的参数 $\\lambda$、$\\epsilon$ 和 $\\tau$ 各不相同。对每个用例，我们报告六个度量指标：$x^{(0)}$ 和 $x^{(2)}$ 的支撑集大小、迭代间的支撑集变化，以及最小二乘重构前后在真实支撑集上的平均系数幅值偏差。所有随机过程均使用种子 $12345$ 以保证可复现性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a sparse recovery and debiasing problem using reweighted l1 minimization.\n    \"\"\"\n\n    # --- Problem Setup ---\n    m, n, k = 64, 128, 10\n    sigma = 0.02\n    seed = 12345\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n\n    # Generate matrix A\n    A = rng.normal(0, 1 / np.sqrt(m), size=(m, n))\n\n    # Generate sparse vector x_star\n    support_star_indices = rng.choice(n, k, replace=False)\n    x_star = np.zeros(n)\n    x_star[support_star_indices] = rng.normal(0, 1, size=k)\n    support_star_set = set(support_star_indices)\n\n    # Generate noise and observation y\n    eta = rng.normal(0, sigma, size=m)\n    y = A @ x_star + eta\n\n    # --- Algorithm Parameters ---\n    # Lipschitz constant of the gradient of the least-squares term\n    L = np.linalg.svd(A, compute_uv=False)[0] ** 2\n    ista_step_size = 1.0 / L\n    ista_iterations = 5000\n\n    # Test cases\n    test_cases = [\n        # (lambda, epsilon, tau)\n        (0.05, 1e-3, 1e-4),\n        (0.10, 1e-3, 1e-4),\n        (0.05, 1e-6, 1e-4),\n        (0.05, 1e-3, 1e-2),\n    ]\n\n    all_results = []\n\n    # --- Helper Functions ---\n    def soft_threshold(z, T):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\n    def ista_solve(A, y, lambda_val, weights):\n        \"\"\"\n        Solves the weighted l1-regularized least-squares problem using ISTA.\n        \"\"\"\n        x = np.zeros(A.shape[1])\n        At = A.T\n        thresholds = lambda_val * weights * ista_step_size\n        \n        for _ in range(ista_iterations):\n            gradient = At @ (A @ x - y)\n            z = x - ista_step_size * gradient\n            x = soft_threshold(z, thresholds)\n        return x\n\n    def get_support(x, tau):\n        \"\"\"Identifies the support of a vector based on a magnitude threshold.\"\"\"\n        return set(np.where(np.abs(x) > tau)[0])\n\n    def ls_refit(A, y, support_indices):\n        \"\"\"Performs least-squares refitting on the identified support.\"\"\"\n        x_ls = np.zeros(A.shape[1])\n        if not support_indices:\n            return x_ls\n        \n        support_list = sorted(list(support_indices))\n        A_S = A[:, support_list]\n        \n        # Solve the least-squares problem: min ||A_S z - y||_2^2\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        \n        x_ls[support_list] = z\n        return x_ls\n\n    for lambda_val, epsilon, tau in test_cases:\n        case_results = []\n\n        # -- Iteration 0 (Unweighted l1) --\n        w0 = np.ones(n)\n        x0 = ista_solve(A, y, lambda_val, w0)\n        S0 = get_support(x0, tau)\n        case_results.append(len(S0))\n\n        # -- Iteration 1 (Reweighted) --\n        w1 = 1.0 / (np.abs(x0) + epsilon)\n        x1 = ista_solve(A, y, lambda_val, w1)\n        S1 = get_support(x1, tau)\n\n        # -- Iteration 2 (Reweighted) --\n        w2 = 1.0 / (np.abs(x1) + epsilon)\n        x2 = ista_solve(A, y, lambda_val, w2)\n        S2 = get_support(x2, tau)\n        case_results.append(len(S2))\n\n        # -- Support Change Metrics --\n        change01 = len(S0.symmetric_difference(S1))\n        case_results.append(change01)\n        change12 = len(S1.symmetric_difference(S2))\n        case_results.append(change12)\n\n        # -- Least-Squares Refit on final support S2 --\n        x_ls = ls_refit(A, y, S2)\n\n        # -- Bias Metrics on True Support --\n        x_star_on_support = x_star[support_star_indices]\n        x2_on_support = x2[support_star_indices]\n        x_ls_on_support = x_ls[support_star_indices]\n\n        # Bias before refit\n        b_pre = np.mean(np.abs(x2_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_pre)\n        \n        # Bias after refit\n        b_post = np.mean(np.abs(x_ls_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_post)\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    # `repr` is used to get the string representation of floats without losing precision.\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{repr(res[4])},{repr(res[5])}]\"\n        for res in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在许多实际问题中，稀疏性不仅体现在单个变量上，更表现为变量分组的结构化形式。组 Lasso (Group Lasso) 正是为处理此类问题而设计的，但它同样会因正则化而产生组水平的收缩偏差。 通过本练习，您将把去偏思想从标准 Lasso 推广到组 Lasso，亲手实现组稀疏模型的求解与去偏，从而掌握将核心原理应用于更复杂、更具结构性的模型的能力。",
            "id": "3442491",
            "problem": "考虑一个具有块结构设计矩阵和预定义分组的线性测量模型。设 $A \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^{n}$，以及列索引 $\\{1,2,\\ldots,p\\}$ 的一个划分为不相交的组 $\\mathcal{G} = \\{g_{1}, g_{2}, \\ldots, g_{m}\\}$，其中每个 $g_{j} \\subset \\{1,2,\\ldots,p\\}$ 且当 $j \\neq k$ 时 $g_{j} \\cap g_{k} = \\emptyset$，并且 $\\bigcup_{j=1}^{m} g_{j} = \\{1,2,\\ldots,p\\}$。未知系数向量为 $x \\in \\mathbb{R}^{p}$，其在组 $g$ 上的限制为 $x_{g} \\in \\mathbb{R}^{|g|}$。假设数据 $y$ 是通过线性模型 $y = A x_{\\star} + \\varepsilon$ 从一个真实向量 $x_{\\star} \\in \\mathbb{R}^{p}$ 生成的，其中 $\\varepsilon$ 是加性噪声。\n\n将组最小绝对收缩和选择算子（group LASSO）估计量定义为以下凸优化问题的任意解\n$$\n\\widehat{x}^{\\mathrm{GL}} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g} \\|x_{g}\\|_{2} \\right\\},\n$$\n其中 $\\lambda > 0$ 是一个正则化参数， $w_{g} = \\sqrt{|g|}$ 是组 $g$ 的权重。如果 $\\|\\widehat{x}^{\\mathrm{GL}}_{g}\\|_{2} > 0$，则认为组 $g$ 被选中。令 $S \\subset \\{1,2,\\ldots,p\\}$ 表示所有被选中组的索引的并集。\n\n将后选择最小二乘（LS）重塑定义为 $\\widehat{x}^{\\mathrm{LS}} \\in \\mathbb{R}^{p}$，其中 $S$ 之外的坐标恰好为零，而 $S$ 之内的坐标通过求解限制在 $S$ 上的残差最小化问题得到：\n$$\n\\widehat{x}^{\\mathrm{LS}}_{S} \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\|A_{S} z - y\\|_{2}, \\quad \\widehat{x}^{\\mathrm{LS}}_{S^{c}} = 0,\n$$\n其中 $A_{S} \\in \\mathbb{R}^{n \\times |S|}$ 表示由 $S$ 索引的列组成的 $A$ 的子矩阵。LS重塑用于对组LASSO估计的收缩诱导偏差进行去偏。\n\n您的任务是实现一个程序，对于下面定义的每个测试用例，该程序能生成 $\\widehat{x}^{\\mathrm{GL}}$，识别被选中的组，构建 $S$，计算限制性LS重塑 $\\widehat{x}^{\\mathrm{LS}}$，并通过标量\n$$\nB = \\|\\widehat{x}^{\\mathrm{GL}} - x_{\\star}\\|_{2} - \\|\\widehat{x}^{\\mathrm{LS}} - x_{\\star}\\|_{2}\n$$\n来量化偏差的减小。\n每个测试用例的输出是实数 $B$。\n\n您必须按规定为每个测试用例构建合成数据 $A$、$x_{\\star}$ 和 $y$。所有随机元素必须通过为每个测试用例固定随机种子来保证可复现性。设计矩阵 $A$ 的列必须被归一化为单位 $\\ell_{2}$ 范数。真实向量 $x_{\\star}$ 必须支撑在一个已知的活动组集合上，并且在每个活动组内，系数应从标准正态分布中抽取，然后进行缩放，使得该组的 $\\ell_{2}$ 范数等于 $1$。\n\n程序必须能处理任意大小的组，并且在数值上是稳定的。如果选定的集合 $S$ 为空，则定义 $\\widehat{x}^{\\mathrm{LS}} = 0$ 并相应地计算偏差减小量。\n\n测试套件规范：\n\n- 测试用例 1：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是 10 个大小为 5 的连续块，活动组的索引为 $[1,4,7]$（零索引组编号），噪声标准差 $\\sigma = 0.1$，正则化 $\\lambda = 0.25$，随机种子 $1$。\n- 测试用例 2：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是 10 个大小为 5 的连续块，活动组的索引为 $[2,3]$，噪声标准差 $\\sigma = 0.5$，正则化 $\\lambda = 0.6$，随机种子 $2$。\n- 测试用例 3：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是 10 个大小为 5 的连续块，活动组的索引为 $[0,5,9]$，噪声标准差 $\\sigma = 0.1$，正则化 $\\lambda = 5.0$，随机种子 $3$。\n- 测试用例 4：$n = 60$，$p = 50$，组 $\\mathcal{G}$ 是大小为 $[3,7,4,6,10,5,5,5,5]$ 的连续块，活动组的索引为 $[1,4,7]$，噪声标准差 $\\sigma = 0.0$，正则化 $\\lambda = 0.15$，随机种子 $4$。\n\n所有角度（如有）必须以弧度为单位，尽管此问题不涉及角度。不涉及物理单位。最终输出必须是实数。\n\n最终输出格式：\n\n您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，即 $[B_{1},B_{2},B_{3},B_{4}]$，其中 $B_{i}$ 是上述定义的测试用例 $i$ 的偏差减小量。输出必须是浮点数。不得打印额外文本。",
            "solution": "用户提供的问题要求在具有分组协变量的线性模型背景下，实现并比较两种估计量：组最小绝对收缩和选择算子（Group LASSO）和后选择最小二乘（LS）重塑。目标是量化LS重塑所实现的偏差减小。\n\n该问题是**有效的**。它在科学上基于高维统计和稀疏优化的理论，问题定义良好、客观，并为可复现的数值实验提供了完整且一致的设置。\n\n在此，我们详细阐述解决此问题的原则性方法。\n\n**1. 模型与数据生成**\n\n该问题基于线性模型 $y = A x_{\\star} + \\varepsilon$，其中 $y \\in \\mathbb{R}^{n}$ 是观测向量，$A \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$x_{\\star} \\in \\mathbb{R}^{p}$ 是未知的稀疏真实向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是加性噪声。系数向量的索引 $\\{1, \\ldots, p\\}$ 被划分为 $m$ 个不相交的组 $\\mathcal{G} = \\{g_1, \\ldots, g_m\\}$。\n\n对于每个测试用例，我们根据规范生成合成数据：\n- 为保证可复现性，播种随机数生成器。\n- 矩阵 $A$ 从标准正态分布中抽取，其列随后被归一化为单位 $\\ell_2$ 范数，即对所有 $j \\in \\{1, \\ldots, p\\}$，$\\|A_{:,j}\\|_{2} = 1$。\n- 真实向量 $x_{\\star}$ 被构造成组稀疏的。对于每个指定的活动组 $g$，从标准正态分布中抽取一个系数向量 $(x_{\\star})_g$，然后进行缩放，使其 $\\ell_2$ 范数为1，即 $\\|(x_{\\star})_g\\|_{2} = 1$。非活动组中的系数为零。\n- 噪声向量 $\\varepsilon$ 从标准差为 $\\sigma$ 的各向同性高斯分布中抽取，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- 然后计算观测向量为 $y = A x_{\\star} + \\varepsilon$。\n\n**2. 组LASSO估计量**\n\n组LASSO估计量 $\\widehat{x}^{\\mathrm{GL}}$ 通过求解以下凸优化问题得到：\n$$\n\\widehat{x}^{\\mathrm{GL}} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} F(x) = \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} \\sqrt{|g|} \\|x_{g}\\|_{2} \\right\\}\n$$\n目标函数 $F(x)$ 是一个光滑、凸的数据保真项 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 和一个非光滑、凸的正则化项 $h(x) = \\lambda \\sum_{g \\in \\mathcal{G}} \\sqrt{|g|} \\|x_{g}\\|_{2}$ 的和。正则化项促进组级别的稀疏性；它鼓励将整组系数 $x_g$ 设置为零。\n\n解决此复合优化问题的一个标准且有效的算法是近端梯度下降（Proximal Gradient Descent, PGD），也称为迭代收缩-阈值算法（Iterative Shrinkage-Thresholding Algorithm, ISTA）。PGD的更新规则是：\n$$\nx^{(k+1)} = \\mathrm{prox}_{\\gamma h}\\left(x^{(k)} - \\gamma \\nabla f(x^{(k)})\\right)\n$$\n其中 $k$ 是迭代索引，$\\gamma$ 是步长，$\\nabla f(x)$ 是光滑项的梯度，$\\mathrm{prox}_{\\gamma h}(\\cdot)$ 是正则化项的近端算子。\n\n- 梯度为 $\\nabla f(x) = A^T(Ax - y)$。\n- 步长 $\\gamma$ 的选择必须满足 $\\gamma  1/L$，其中 $L$ 是 $\\nabla f(x)$ 的利普希茨常数。$L$ 是 $A^T A$ 的最大特征值，即 $L = \\lambda_{\\max}(A^T A) = \\sigma_{\\max}(A)^2$。固定的步长 $\\gamma = 1/L$ 保证收敛。\n- $h(x)$ 的近端算子在各组之间是可分的，并对应于一个分组软阈值操作。对于一个向量 $z \\in \\mathbb{R}^p$ 和每个组 $g \\in \\mathcal{G}$：\n$$\n[\\mathrm{prox}_{\\gamma h}(z)]_g = \\mathrm{prox}_{\\gamma \\lambda \\sqrt{|g|} \\|\\cdot\\|_2}(z_g) = \\left(1 - \\frac{\\gamma \\lambda \\sqrt{|g|}}{\\|z_g\\|_2}\\right)_{+} z_g\n$$\n其中 $(c)_{+} = \\max(0, c)$。此操作会收缩向量 $z_g$ 的范数，如果其范数低于阈值 $\\gamma \\lambda \\sqrt{|g|}$，则将其设置为零。\n\nPGD算法以 $x^{(0)} = 0$ 初始化，并迭代直到估计量的相对变化 $\\|x^{(k+1)} - x^{(k)}\\|_2 / \\|x^{(k)}\\|_2$ 低于指定的容差。\n\n**3. 使用后选择最小二乘进行去偏**\n\n由于正则化项的收缩效应，组LASSO估计量会向零产生偏差。为了减轻这种偏差，一种常用技术是执行后选择普通最小二乘（LS）重塑。\n\n首先，从组LASSO解中识别出被选中的组：如果 $\\|\\widehat{x}^{\\mathrm{GL}}_g\\|_2 > 0$，则组 $g$ 被选中。集合 $S$ 由所有属于这些被选中组的索引的并集构成。\n\n然后，通过求解限制在由 $A$ 的 $S$ 索引列所定义的子空间上的标准最小二乘问题来计算LS估计量 $\\widehat{x}^{\\mathrm{LS}}$：\n$$\n\\widehat{x}^{\\mathrm{LS}}_{S} = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\|A_{S} z - y\\|_{2}\n$$\n$\\widehat{x}^{\\mathrm{LS}}$ 中与 $S$ 中不包含的索引对应的系数被设置为零，即 $\\widehat{x}^{\\mathrm{LS}}_{S^c} = 0$。这个LS问题可以使用例如 `numpy.linalg.lstsq` 函数来稳健地求解，即使子矩阵 $A_S$ 是秩亏的，它也能提供最小范数解。如果集合 $S$ 为空（即组LASSO没有选择任何组），则 $\\widehat{x}^{\\mathrm{LS}}$ 定义为零向量。\n\n**4. 量化偏差减小**\n\n去偏过程的有效性通过比较每个估计量与真实向量 $x_{\\star}$ 的 $\\ell_2$ 距离来衡量。偏差减小量 $B$ 定义为这些估计误差的差值：\n$$\nB = \\|\\widehat{x}^{\\mathrm{GL}} - x_{\\star}\\|_{2} - \\|\\widehat{x}^{\\mathrm{LS}} - x_{\\star}\\|_{2}\n$$\n$B$ 的正值表示LS重塑估计量比组LASSO估计量更接近真实向量 $x_{\\star}$，从而成功地减小了整体估计误差。最终的实现为每个指定的测试用例计算此值 $B$。",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # n, p, group_spec, active_groups, sigma, lambda_reg, seed\n        (60, 50, (10, 5), [1, 4, 7], 0.1, 0.25, 1),\n        (60, 50, (10, 5), [2, 3], 0.5, 0.6, 2),\n        (60, 50, (10, 5), [0, 5, 9], 0.1, 5.0, 3),\n        (60, 50, [3, 7, 4, 6, 10, 5, 5, 5, 5], [1, 4, 7], 0.0, 0.15, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        b_val = calculate_bias_reduction(params)\n        results.append(b_val)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef calculate_bias_reduction(params):\n    \"\"\"\n    Runs a single test case: generates data, computes estimators, and returns bias reduction.\n    \"\"\"\n    n, p, group_spec, active_groups, sigma, lambda_reg, seed = params\n\n    # Create group structure from specification\n    if isinstance(group_spec, tuple):\n        num_groups, size_per_group = group_spec\n        group_sizes = [size_per_group] * num_groups\n    else:\n        group_sizes = group_spec\n    \n    current_idx = 0\n    groups = []\n    for size in group_sizes:\n        groups.append(np.arange(current_idx, current_idx + size))\n        current_idx += size\n\n    # Generate synthetic data\n    A, y, x_star = generate_data(n, p, groups, active_groups, sigma, seed)\n\n    # 1. Solve for Group LASSO estimate\n    x_gl = group_lasso_proximal_gradient(A, y, groups, lambda_reg)\n\n    # 2. Identify selected groups and form the index set S\n    selected_indices = []\n    for g in groups:\n        if np.linalg.norm(x_gl[g]) > 1e-9:  # Use a small tolerance for non-zero check\n            selected_indices.extend(g)\n    \n    S = sorted(list(set(selected_indices)))\n\n    # 3. Compute post-selection Least Squares (LS) refit\n    x_ls = np.zeros(p)\n    if len(S) > 0:\n        A_S = A[:, S]\n        # Use lstsq for numerical stability\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        x_ls[S] = z\n\n    # 4. Quantify the bias reduction\n    error_gl = np.linalg.norm(x_gl - x_star)\n    error_ls = np.linalg.norm(x_ls - x_star)\n    B = error_gl - error_ls\n    \n    return B\n\ndef generate_data(n, p, groups, active_group_indices, sigma, seed):\n    \"\"\"\n    Generates synthetic data (A, y, x_star) for the Group LASSO problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate design matrix A with unit-norm columns\n    A = rng.standard_normal((n, p))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    \n    # Generate ground-truth vector x_star\n    x_star = np.zeros(p)\n    for group_idx in active_group_indices:\n        group_indices = groups[group_idx]\n        group_size = len(group_indices)\n        \n        # Draw coefficients from N(0,1)\n        coeffs = rng.standard_normal(group_size)\n        \n        # Scale group to have unit l2-norm\n        norm_coeffs = np.linalg.norm(coeffs)\n        if norm_coeffs > 0:\n            coeffs /= norm_coeffs\n            \n        x_star[group_indices] = coeffs\n        \n    # Generate noise and the measurement vector y\n    noise = rng.standard_normal(n) * sigma\n    y = A @ x_star + noise\n    \n    return A, y, x_star\n\ndef group_lasso_proximal_gradient(A, y, groups, lambda_reg, max_iter=2000, tol=1e-7):\n    \"\"\"\n    Solves the Group LASSO problem using Proximal Gradient Descent.\n    \"\"\"\n    n, p = A.shape\n    \n    # Precompute terms for efficiency\n    AtA = A.T @ A\n    Aty = A.T @ y\n    \n    # Determine step size from the Lipschitz constant of the gradient\n    L = scipy.linalg.svdvals(AtA)[0]\n    gamma = 1.0 / L\n    \n    x = np.zeros(p)\n    group_weights = np.array([np.sqrt(len(g)) for g in groups])\n    \n    for _ in range(max_iter):\n        x_old = x.copy()\n        \n        # Gradient step\n        grad = AtA @ x - Aty\n        z = x - gamma * grad\n        \n        # Proximal step (group-wise soft-thresholding)\n        for i, g in enumerate(groups):\n            z_g = z[g]\n            norm_z_g = np.linalg.norm(z_g)\n            \n            threshold = gamma * lambda_reg * group_weights[i]\n            \n            if norm_z_g > threshold:\n                shrinkage = 1.0 - threshold / norm_z_g\n                x[g] = shrinkage * z_g\n            else:\n                x[g] = 0.0\n        \n        # Convergence check\n        diff_norm = np.linalg.norm(x - x_old)\n        x_old_norm = np.linalg.norm(x_old)\n        if diff_norm / (x_old_norm + 1e-8)  tol:\n            break\n            \n    return x\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}