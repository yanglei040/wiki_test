{
    "hands_on_practices": [
        {
            "introduction": "This exercise provides a concrete, hands-on opportunity to compare the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). By working through a simplified case with an orthonormal design matrix, you will derive their explicit forms and see firsthand how their different penalty terms lead to the selection of different model sizes. This practice is fundamental to building an intuition for the classic trade-off between model fit and parsimony.",
            "id": "3452903",
            "problem": "Consider a linear observation model in compressed sensing with Gaussian noise given by $y \\in \\mathbb{R}^{n}$ and $X \\in \\mathbb{R}^{n \\times p}$, where the columns of $X$ are orthonormal so that $X^{\\top}X = I_{p}$. Assume $y = X \\beta^{\\star} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Define the best-$k$ term approximation to be the $k$-term linear model that minimizes the empirical squared error over all subsets of $k$ columns of $X$. Let $z = X^{\\top} y \\in \\mathbb{R}^{p}$ and let $|z|_{(1)} \\geq |z|_{(2)} \\geq \\cdots \\geq |z|_{(p)}$ denote the order statistics of the magnitudes of the entries of $z$, with corresponding signed values $z_{(j)}$.\n\nStarting from the Gaussian log-likelihood and the orthonormality of $X$, derive the explicit expression for the residual sum of squares as a function of $k$ along the best-$k$ term path. Then, using the definitions of the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) as information criteria derived from the maximized Gaussian likelihood with a penalty proportional to the number of free parameters, obtain their expressions as functions of $k$ along this path. You may ignore additive constants that do not depend on $k$.\n\nNow specialize to the following data:\n- $n = 120$, $p = 10$,\n- $\\|y\\|^{2} = 300$,\n- $z = X^{\\top} y = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$.\n\nCompute the values of $k$ that minimize AIC and BIC, respectively, along the best-$k$ term path. In case of ties, choose the smaller $k$. Report your final answer as the pair $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$. The final answer must be given as a single entity and does not require rounding.",
            "solution": "The Gaussian log-likelihood for the model $y = X\\beta + \\varepsilon$ is given by:\n$$ \\ell(\\beta, \\sigma^2; y) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\|y - X\\beta\\|^2 $$\nFor a fixed model (i.e., a fixed support for $\\beta$), maximizing the log-likelihood over $\\beta$ is equivalent to minimizing the residual sum of squares (RSS), defined as $\\mathrm{RSS}(\\beta) = \\|y - X\\beta\\|^2$.\n\nLet's consider a model with $k$ non-zero coefficients indexed by a set $S \\subset \\{1, \\dots, p\\}$ with $|S|=k$. The model is $y = X_S \\beta_S + \\varepsilon$, where $X_S$ contains the columns of $X$ indexed by $S$. The least squares estimate for $\\beta_S$ is $\\hat{\\beta}_S = (X_S^{\\top}X_S)^{-1} X_S^{\\top}y$.\nGiven that the columns of $X$ are orthonormal, we have $X^{\\top}X = I_p$. This implies that for any subset of columns $S$, $X_S^{\\top}X_S = I_k$.\nThus, the estimator simplifies to $\\hat{\\beta}_S = X_S^{\\top}y$. The components of $\\hat{\\beta}_S$ are the corresponding components of $z=X^{\\top}y$. That is, for $j \\in S$, $\\hat{\\beta}_j = z_j$. For $j \\notin S$, we have $\\hat{\\beta}_j = 0$.\n\nThe RSS for this model is:\n$$ \\mathrm{RSS}(S) = \\|y - X_S \\hat{\\beta}_S\\|^2 = \\|y - X_S (X_S^{\\top}y)\\|^2 $$\nThe vector $X_S(X_S^{\\top}y)$ is the orthogonal projection of $y$ onto the subspace spanned by the columns in $X_S$. By the Pythagorean theorem, $\\|y\\|^2 = \\|X_S(X_S^{\\top}y)\\|^2 + \\mathrm{RSS}(S)$.\nThe squared norm of the projection is:\n$$ \\|X_S(X_S^{\\top}y)\\|^2 = (X_S^{\\top}y)^{\\top} (X_S^{\\top}X_S) (X_S^{\\top}y) = (X_S^{\\top}y)^{\\top} I_k (X_S^{\\top}y) = \\|X_S^{\\top}y\\|^2 = \\sum_{j \\in S} z_j^2 $$\nTherefore, the RSS is:\n$$ \\mathrm{RSS}(S) = \\|y\\|^2 - \\sum_{j \\in S} z_j^2 $$\nThe best-$k$ term approximation is the one that minimizes the RSS. To minimize $\\mathrm{RSS}(S)$ for a fixed $k$, we must choose the set $S$ that maximizes $\\sum_{j \\in S} z_j^2$. This is achieved by selecting the $k$ indices corresponding to the $k$ largest values of $|z_j|$ (or $z_j^2$).\nLet $S_{(k)}$ be this optimal set of $k$ indices. The RSS for the best-$k$ term model, denoted $\\mathrm{RSS}_k$, is:\n$$ \\mathrm{RSS}_k = \\|y\\|^2 - \\sum_{j=1}^{k} |z|_{(j)}^2 = \\|y\\|^2 - \\sum_{j=1}^{k} z_{(j)}^2 $$\nwhere $z_{(j)}^2$ is the $j$-th largest squared magnitude.\nIf $k=0$ (the null model), we have $\\hat{\\beta}=0$, so $\\mathrm{RSS}_0 = \\|y\\|^2$.\n\nTo find the expressions for AIC and BIC, we first need the maximized log-likelihood. For a model of size $k$, the maximum likelihood estimate for the variance is $\\hat{\\sigma}_k^2 = \\frac{\\mathrm{RSS}_k}{n}$. Substituting $\\hat{\\beta}_{S_{(k)}}$ and $\\hat{\\sigma}_k^2$ into the log-likelihood expression gives:\n$$ \\hat{\\ell}_k = -\\frac{n}{2} \\log(2\\pi) - \\frac{n}{2}\\log(\\hat{\\sigma}_k^2) - \\frac{1}{2\\hat{\\sigma}_k^2}\\mathrm{RSS}_k = -\\frac{n}{2}\\log(2\\pi\\frac{\\mathrm{RSS}_k}{n}) - \\frac{n}{2} = -\\frac{n}{2}\\log(\\mathrm{RSS}_k) + C $$\nwhere $C$ is a constant that does not depend on $k$.\n\nAIC is defined as $-2\\hat{\\ell} + 2 \\times (\\text{number of parameters})$. A model of size $k$ has $k$ non-zero coefficients and an estimated variance $\\sigma^2$, so there are $k+1$ free parameters.\n$$ \\mathrm{AIC}(k) = -2\\hat{\\ell}_k + 2(k+1) = n\\log(\\mathrm{RSS}_k) - 2C + 2(k+1) $$\nIgnoring terms constant in $k$, the expression to minimize is:\n$$ \\mathrm{AIC}(k) \\propto n\\log(\\mathrm{RSS}_k) + 2k $$\n\nBIC is defined as $-2\\hat{\\ell} + \\log(n) \\times (\\text{number of parameters})$.\n$$ \\mathrm{BIC}(k) = -2\\hat{\\ell}_k + (k+1)\\log(n) = n\\log(\\mathrm{RSS}_k) - 2C + (k+1)\\log(n) $$\nIgnoring terms constant in $k$, the expression to minimize is:\n$$ \\mathrm{BIC}(k) \\propto n\\log(\\mathrm{RSS}_k) + k\\log(n) $$\n\nNow we use the provided data to find $k_{\\mathrm{AIC}}$ and $k_{\\mathrm{BIC}}$.\n$n = 120$, $p = 10$, $\\|y\\|^2 = 300$.\n$z = [\\,5.2,\\,-3.1,\\,2.7,\\,-2.2,\\,1.9,\\,-1.5,\\,1.2,\\,0.9,\\,-0.6,\\,0.4\\,]$.\nThe components of $z$ are already sorted by decreasing magnitude. We compute the squared values:\n$z_{(1)}^2 = 5.2^2 = 27.04$\n$z_{(2)}^2 = (-3.1)^2 = 9.61$\n$z_{(3)}^2 = 2.7^2 = 7.29$\n$z_{(4)}^2 = (-2.2)^2 = 4.84$\n$z_{(5)}^2 = 1.9^2 = 3.61$\n$z_{(6)}^2 = (-1.5)^2 = 2.25$\n$z_{(7)}^2 = 1.2^2 = 1.44$\n$z_{(8)}^2 = 0.9^2 = 0.81$\n$z_{(9)}^2 = (-0.6)^2 = 0.36$\n$z_{(10)}^2 = 0.4^2 = 0.16$\n\nLet $S_k = \\sum_{j=1}^{k} z_{(j)}^2$. We compute the cumulative sums:\n$S_0 = 0$\n$S_1 = 27.04$\n$S_2 = 36.65$\n$S_3 = 43.94$\n$S_4 = 48.78$\n$S_5 = 52.39$\n$S_6 = 54.64$\n$S_7 = 56.08$\n$S_8 = 56.89$\n$S_9 = 57.25$\n$S_{10} = 57.41$\n\nNow, we compute $\\mathrm{RSS}_k = \\|y\\|^2 - S_k = 300 - S_k$:\n$\\mathrm{RSS}_0 = 300$\n$\\mathrm{RSS}_1 = 272.96$\n$\\mathrm{RSS}_2 = 263.35$\n$\\mathrm{RSS}_3 = 256.06$\n$\\mathrm{RSS}_4 = 251.22$\n$\\mathrm{RSS}_5 = 247.61$\n$\\mathrm{RSS}_6 = 245.36$\n$\\mathrm{RSS}_7 = 243.92$\n$\\mathrm{RSS}_8 = 243.11$\n$\\mathrm{RSS}_9 = 242.75$\n$\\mathrm{RSS}_{10} = 242.59$\n\nTo find $k_{\\mathrm{AIC}}$, we minimize $\\mathrm{AIC}(k) \\propto 120\\log(\\mathrm{RSS}_k) + 2k$.\n$\\mathrm{AIC}(0) \\propto 120\\log(300) + 0 \\approx 684.46$\n$\\mathrm{AIC}(1) \\propto 120\\log(272.96) + 2 \\approx 673.12 + 2 = 675.12$\n$\\mathrm{AIC}(2) \\propto 120\\log(263.35) + 4 \\approx 668.82 + 4 = 672.82$\n$\\mathrm{AIC}(3) \\propto 120\\log(256.06) + 6 \\approx 665.45 + 6 = 671.45$\n$\\mathrm{AIC}(4) \\propto 120\\log(251.22) + 8 \\approx 663.16 + 8 = 671.16$\n$\\mathrm{AIC}(5) \\propto 120\\log(247.61) + 10 \\approx 661.42 + 10 = 671.42$\n$\\mathrm{AIC}(6) \\propto 120\\log(245.36) + 12 \\approx 660.32 + 12 = 672.32$\nThe minimum value occurs at $k=4$. The value at $k=5$ is slightly larger. Thus, $k_{\\mathrm{AIC}} = 4$.\n\nTo find $k_{\\mathrm{BIC}}$, we minimize $\\mathrm{BIC}(k) \\propto 120\\log(\\mathrm{RSS}_k) + k\\log(120)$.\nWe have $\\log(120) \\approx 4.7875$.\n$\\mathrm{BIC}(0) \\propto 120\\log(300) + 0 \\approx 684.46$\n$\\mathrm{BIC}(1) \\propto 120\\log(272.96) + \\log(120) \\approx 673.12 + 4.79 = 677.91$\n$\\mathrm{BIC}(2) \\propto 120\\log(263.35) + 2\\log(120) \\approx 668.82 + 9.58 = 678.40$\n$\\mathrm{BIC}(3) \\propto 120\\log(256.06) + 3\\log(120) \\approx 665.45 + 14.36 = 679.81$\nThe values of $\\mathrm{BIC}(k)$ increase after $k=1$, because the penalty term $k\\log(120)$ grows faster than the goodness-of-fit term $120\\log(\\mathrm{RSS}_k)$ decreases. The minimum is at $k=1$. Thus, $k_{\\mathrm{BIC}} = 1$.\n\nThe pair of optimal model sizes is $(k_{\\mathrm{AIC}}, k_{\\mathrm{BIC}})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4 & 1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "To truly master information criteria, it is essential to understand that their underlying principles extend beyond the standard Gaussian noise assumption. This problem challenges you to derive an AIC-like criterion for a linear model with Laplace-distributed errors, a common scenario in robust regression. This exercise solidifies your understanding of how information criteria are derived from the asymptotic properties of maximum likelihood estimation, regardless of the specific probability distribution.",
            "id": "3452916",
            "problem": "Consider linear regression with independent and identically distributed noise modeled by the Laplace distribution. Specifically, suppose $y_{i} \\in \\mathbb{R}$ and $x_{i} \\in \\mathbb{R}^{p}$ satisfy $y_{i} = x_{i}^{\\top}\\beta + \\epsilon_{i}$ for $i=1,\\dots,n$, where $\\epsilon_{i}$ are independent and identically distributed Laplace random variables with mean $0$ and scale parameter $b>0$, having density $f(\\epsilon) = \\frac{1}{2b}\\exp\\!\\big(-|\\epsilon|/b\\big)$. Assume a fixed design matrix $X \\in \\mathbb{R}^{n \\times p}$ and consider a candidate sparse model that uses only an index set $S \\subset \\{1,\\dots,p\\}$ with $|S| = k$, with the corresponding parameter vector $\\beta_{S} \\in \\mathbb{R}^{k}$ (the remaining components are constrained to be zero). The parameters $(\\beta_{S}, b)$ are estimated by maximum likelihood, which coincides with minimizing the sum of absolute residuals with respect to $\\beta_{S}$ and estimating $b$ by maximizing the Laplace likelihood.\n\nDefine the training deviance by $D_{\\mathrm{train}}(S) = -2\\,\\ell\\big(\\hat{\\beta}_{S}, \\hat{b}; y\\big)$, where $\\ell(\\beta, b; y)$ is the Laplace log-likelihood for the observed sample under the model indexed by $S$, and $(\\hat{\\beta}_{S}, \\hat{b})$ are the maximum likelihood estimates under that model. Define the predictive deviance for a fresh, independent sample $y^{\\star}$ drawn from the same data-generating process, conditional on the training design $X$, by $D_{\\mathrm{pred}}(S) = -2\\,\\mathbb{E}_{y^{\\star}|X}\\!\\big[\\ell\\big(\\hat{\\beta}_{S}, \\hat{b}; y^{\\star}\\big)\\big]$, where the expectation is with respect to the sampling distribution of $y^{\\star}$ and conditioning is on $X$ and the training sample used to compute $(\\hat{\\beta}_{S}, \\hat{b})$.\n\nStarting from the definitions above and using asymptotic properties of maximum likelihood estimators under correctly specified parametric models, formulate an information criterion of the form $\\mathrm{IC}(S) = D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)$ that yields an asymptotically unbiased estimator of the predictive deviance, in the sense that $\\mathbb{E}\\!\\big[\\mathrm{IC}(S)\\big] = \\mathbb{E}\\!\\big[D_{\\mathrm{pred}}(S)\\big] + o(1)$ as $n \\to \\infty$. Derive the explicit closed-form expression for the penalty $\\mathrm{pen}(k,n)$ in terms of $k$ and any other necessary quantities. Your final answer must be a single closed-form analytic expression. If you need to present a numerical constant, retain it exactly; do not approximate it numerically.",
            "solution": "The problem requires the derivation of a penalty term, $\\mathrm{pen}(k,n)$, for an information criterion of the form $\\mathrm{IC}(S) = D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)$. This criterion must provide an asymptotically unbiased estimator of the predictive deviance, $D_{\\mathrm{pred}}(S)$, in the sense that $\\mathbb{E}[D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)] = \\mathbb{E}[D_{\\mathrm{pred}}(S)] + o(1)$ as the sample size $n \\to \\infty$. The expectation is taken over the distribution of the training data $y$.\n\nFrom the condition $\\mathbb{E}[D_{\\mathrm{train}}(S) + \\mathrm{pen}(k,n)] = \\mathbb{E}[D_{\\mathrm{pred}}(S)] + o(1)$, and assuming the penalty $\\mathrm{pen}(k,n)$ is non-random, we must derive an expression for the asymptotic value of the expected optimism, which is defined as the difference between the expected predictive deviance and the expected training deviance:\n$$\n\\mathrm{pen}(k,n) \\approx \\mathbb{E}[D_{\\mathrm{pred}}(S)] - \\mathbb{E}[D_{\\mathrm{train}}(S)]\n$$\nThis derivation follows the general logic used for Akaike's Information Criterion (AIC), relying on the asymptotic properties of Maximum Likelihood Estimators (MLEs). The problem statement specifies that a model indexed by a set $S$ with $|S|=k$ involves estimating a parameter vector $\\beta_S \\in \\mathbb{R}^k$ and the Laplace distribution's scale parameter $b$. Thus, the total number of parameters to be estimated for this model is $d = k+1$. Let $\\theta = (\\beta_S, b)$ be the $(k+1)$-dimensional parameter vector. Let $\\hat{\\theta} = (\\hat{\\beta}_S, \\hat{b})$ denote the MLE of $\\theta$ obtained from the training data $y$, and let $\\theta_0$ be the true, unknown parameter vector that generated the data. The assumption of a \"correctly specified\" model implies that such a $\\theta_0$ exists within the considered family of models.\n\nThe training deviance is $D_{\\mathrm{train}}(S) = -2\\ell(\\hat{\\theta}; y)$, where $\\ell(\\theta;y)$ is the log-likelihood function for the training data $y$. The predictive deviance is $D_{\\mathrm{pred}}(S) = -2\\mathbb{E}_{y^\\star|X}[\\ell(\\hat{\\theta}; y^\\star)]$, where the expectation is over a new, independent dataset $y^\\star$ from the same generating process.\n\nLet us analyze the relationship between the training deviance and the log-likelihood evaluated at the true parameter $\\theta_0$. We perform a second-order Taylor series expansion of $\\ell(\\theta_0; y)$ around the MLE $\\hat{\\theta}$:\n$$\n\\ell(\\theta_0; y) \\approx \\ell(\\hat{\\theta}; y) + (\\theta_0 - \\hat{\\theta})^{\\top} \\nabla\\ell(\\hat{\\theta}; y) + \\frac{1}{2}(\\theta_0 - \\hat{\\theta})^{\\top} \\nabla^2\\ell(\\hat{\\theta}; y) (\\theta_0 - \\hat{\\theta})\n$$\nBy definition of the MLE, the score (gradient) at $\\hat{\\theta}$ is zero, i.e., $\\nabla\\ell(\\hat{\\theta}; y) = 0$. Multiplying by $-2$, we get:\n$$\n-2\\ell(\\theta_0; y) \\approx -2\\ell(\\hat{\\theta}; y) - (\\theta_0 - \\hat{\\theta})^{\\top} \\nabla^2\\ell(\\hat{\\theta}; y) (\\theta_0 - \\hat{\\theta}) = D_{\\mathrm{train}}(S) + (\\hat{\\theta} - \\theta_0)^{\\top} [-\\nabla^2\\ell(\\hat{\\theta}; y)] (\\hat{\\theta} - \\theta_0)\n$$\nUnder standard asymptotic theory for MLEs, for large $n$, the MLE $\\hat{\\theta}$ is consistent, i.e., $\\hat{\\theta} \\to \\theta_0$ in probability. Also, the observed Fisher information, $-\\frac{1}{n}\\nabla^2\\ell(\\hat{\\theta}; y)$, converges to the Fisher information matrix per observation, $I(\\theta_0)$. Thus, $-\\nabla^2\\ell(\\hat{\\theta}; y) \\approx n I(\\theta_0)$. The asymptotic distribution of the MLE is given by $\\sqrt{n}(\\hat{\\theta} - \\theta_0) \\xrightarrow{d} N(0, I(\\theta_0)^{-1})$.\nTaking the expectation over the training data $y$:\n$$\n\\mathbb{E}[-2\\ell(\\theta_0; y)] \\approx \\mathbb{E}[D_{\\mathrm{train}}(S)] + \\mathbb{E}\\left[(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\\right]\n$$\nThe quadratic form follows, asymptotically, the trace of the product of the matrices: $\\mathbb{E}[Z^\\top A Z] = \\mathbb{E}[Z]^\\top A \\mathbb{E}[Z] + \\mathrm{tr}(A \\cdot \\mathrm{Cov}(Z))$. Here $Z = (\\hat{\\theta} - \\theta_0)$ and $A = nI(\\theta_0)$. Asymptotically, $\\mathbb{E}[Z] \\approx 0$ and $\\mathrm{Cov}(Z) \\approx (nI(\\theta_0))^{-1}$. The expectation of the quadratic term is asymptotically $\\mathrm{tr}(nI(\\theta_0) \\cdot (nI(\\theta_0))^{-1}) = \\mathrm{tr}(I_d) = d$, where $d=k+1$ is the dimension of $\\theta$.\nSo, we have the first relation:\n$$\n\\mathbb{E}[-2\\ell(\\theta_0; y)] \\approx \\mathbb{E}[D_{\\mathrm{train}}(S)] + d\n$$\n\nNext, we analyze the predictive deviance. We expand the log-likelihood for the new data $y^\\star$, $\\ell(\\hat{\\theta}; y^\\star)$, around the true parameter $\\theta_0$:\n$$\n\\ell(\\hat{\\theta}; y^\\star) \\approx \\ell(\\theta_0; y^\\star) + (\\hat{\\theta} - \\theta_0)^{\\top} \\nabla\\ell(\\theta_0; y^\\star) + \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} \\nabla^2\\ell(\\theta_0; y^\\star) (\\hat{\\theta} - \\theta_0)\n$$\nThe predictive deviance involves an expectation over $y^\\star$. Noting that $\\hat{\\theta}$ is fixed with respect to $y^\\star$:\n$$\n\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx \\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] + (\\hat{\\theta} - \\theta_0)^{\\top} \\mathbb{E}_{y^\\star}[\\nabla\\ell(\\theta_0; y^\\star)] + \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} \\mathbb{E}_{y^\\star}[\\nabla^2\\ell(\\theta_0; y^\\star)] (\\hat{\\theta} - \\theta_0)\n$$\nAt the true parameter $\\theta_0$, we have $\\mathbb{E}_{y^\\star}[\\nabla\\ell(\\theta_0; y^\\star)] = 0$ and $\\mathbb{E}_{y^\\star}[\\nabla^2\\ell(\\theta_0; y^\\star)] = -n I(\\theta_0)$. Thus:\n$$\n\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx \\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] - \\frac{1}{2}(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\n$$\nMultiplying by $-2$ gives:\n$$\nD_{\\mathrm{pred}}(S) = -2\\mathbb{E}_{y^\\star}[\\ell(\\hat{\\theta}; y^\\star)] \\approx -2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)] + (\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\n$$\nNow, we take the expectation over the training data $y$:\n$$\n\\mathbb{E}_y[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}_y[-2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)]] + \\mathbb{E}_y\\left[(\\hat{\\theta} - \\theta_0)^{\\top} (n I(\\theta_0)) (\\hat{\\theta} - \\theta_0)\\right]\n$$\nSince $y$ and $y^\\star$ are i.i.d. samples, $\\mathbb{E}_y[-2\\mathbb{E}_{y^\\star}[\\ell(\\theta_0; y^\\star)]] = \\mathbb{E}_y[-2\\ell(\\theta_0; y)]$. The second term, as calculated before, has an expectation that is asymptotically $d$.\nThis gives the second relation:\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] + d\n$$\n\nCombining the two asymptotic relations:\n$$\n\\mathbb{E}[D_{\\mathrm{train}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] - d\n$$\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] \\approx \\mathbb{E}[-2\\ell(\\theta_0; y)] + d\n$$\nSubtracting the first from the second, we find the expected optimism:\n$$\n\\mathbb{E}[D_{\\mathrm{pred}}(S)] - \\mathbb{E}[D_{\\mathrm{train}}(S)] \\approx (\\mathbb{E}[-2\\ell(\\theta_0; y)] + d) - (\\mathbb{E}[-2\\ell(\\theta_0; y)] - d) = 2d\n$$\nTherefore, the penalty that corrects for the optimistic bias of the training deviance is asymptotically $2d$. The problem specifies that a candidate model with $|S|=k$ estimates $k$ regression coefficients in $\\beta_S$ and the scale parameter $b$. The total number of estimated parameters is $d = k+1$.\nThe penalty term is:\n$$\n\\mathrm{pen}(k,n) = 2d = 2(k+1)\n$$\nThis is the penalty for the Akaike Information Criterion (AIC) generalized to this modeling context. Although the Laplace likelihood is not everywhere differentiable, which violates classical regularity conditions, the AIC result is known to hold under more general conditions that cover this case. The problem's direction to use standard asymptotic properties confirms this is the intended path. The resulting penalty depends on $k$ but not on $n$, which is consistent with the standard AIC formulation.",
            "answer": "$$ \\boxed{2(k+1)} $$"
        },
        {
            "introduction": "Information criteria are not just for model comparison; they provide powerful tools for tuning algorithm parameters. This advanced practice explores Stein's Unbiased Risk Estimate (SURE), which serves as a data-driven proxy for the true prediction risk. By deriving the SURE for a soft-thresholding estimator, you will see how it yields an optimal, data-dependent choice of the threshold parameter, connecting abstract statistical principles to concrete algorithmic design.",
            "id": "3452918",
            "problem": "Consider the orthogonal design setting in compressed sensing, where the observed response vector is modeled as $Y = \\theta + \\epsilon$ in $\\mathbb{R}^{n}$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ and unknown mean vector $\\theta \\in \\mathbb{R}^{n}$. Define the soft-thresholding estimator at threshold $\\lambda \\ge 0$ coordinatewise by\n$$\n\\big(\\eta_{\\lambda}(y)\\big)_{i} = \\operatorname{sign}(y_{i}) \\cdot \\big(|y_{i}| - \\lambda\\big)_{+}, \\quad i = 1, \\dots, n,\n$$\nwhere $(t)_{+} = \\max\\{t, 0\\}$.\n\nUsing only first principles grounded in Gaussian calculus (specifically Stein’s identity for Gaussian expectations) and the definition of degrees of freedom\n$$\n\\mathrm{df}(g) = \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} \\mathrm{Cov}\\big(g_{i}(Y), Y_{i}\\big),\n$$\nperform the following tasks:\n\n1. Derive the exact degrees of freedom of the soft-thresholding estimator $\\eta_{\\lambda}$ and show that it equals the expected number of nonzero coordinates under the sampling distribution of $Y$. Express this expectation explicitly in terms of $\\theta$, $\\sigma$, and $\\lambda$.\n\n2. Based on Stein’s Unbiased Risk Estimate (SURE), derive an explicit pathwise unbiased estimator of the prediction risk $\\mathbb{E}\\big[\\|\\eta_{\\lambda}(Y) - \\theta\\|_{2}^{2}\\big]$ in terms of the observed data vector $y$, the threshold $\\lambda$, and the noise level $\\sigma$.\n\n3. Use the structure you obtain to argue that the SURE as a function of $\\lambda$ is piecewise smooth with changes only at the absolute values of the coordinates of $y$. Conclude that any global minimizer of SURE over $\\lambda \\ge 0$ must lie in the finite set $\\{0, |y|_{(1)}, |y|_{(2)}, \\dots, |y|_{(n)}\\}$, where $|y|_{(1)} \\le |y|_{(2)} \\le \\dots \\le |y|_{(n)}$ are the order statistics of $\\{|y_{i}|\\}_{i=1}^{n}$.\n\nYour final answer must be a single closed-form analytic expression for the SURE-minimizing threshold $\\hat{\\lambda}$ as a function of $y$ and $\\sigma$. No numerical evaluation is required.",
            "solution": "### Part 1: Degrees of Freedom of the Soft-Thresholding Estimator\n\nThe degrees of freedom of an estimator $g(Y)$ is defined as $\\mathrm{df}(g) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\mathrm{Cov}(g_i(Y), Y_i)$. For the soft-thresholding estimator $\\eta_{\\lambda}$, the coordinates are processed independently, so $g_i(Y) = \\eta_{\\lambda, i}(Y_i)$. The sum can be analyzed term by term.\n\nWe use Stein's identity for a Gaussian random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ and a weakly differentiable function $h$. The identity states $\\mathrm{Cov}(h(Z), Z) = \\sigma^2 \\mathbb{E}[h'(Z)]$. Applying this to each coordinate $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$ with the function $h(y_i) = \\eta_{\\lambda, i}(y_i)$, we get:\n$$\n\\mathrm{Cov}(\\eta_{\\lambda, i}(Y_i), Y_i) = \\sigma^2 \\mathbb{E}\\left[\\frac{d}{dY_i}\\eta_{\\lambda, i}(Y_i)\\right]\n$$\nThe function $\\eta_{\\lambda, i}(y_i)$ can be written piecewise:\n$$\n\\eta_{\\lambda, i}(y_i) =\n\\begin{cases}\ny_i - \\lambda & \\text{if } y_i > \\lambda \\\\\n0 & \\text{if } -\\lambda \\le y_i \\le \\lambda \\\\\ny_i + \\lambda & \\text{if } y_i < -\\lambda\n\\end{cases}\n$$\nThis function is continuous. Its weak derivative with respect to $y_i$ is given by:\n$$\n\\frac{d}{dy_i}\\eta_{\\lambda, i}(y_i) =\n\\begin{cases}\n1 & \\text{if } y_i > \\lambda \\\\\n0 & \\text{if } -\\lambda < y_i < \\lambda \\\\\n1 & \\text{if } y_i < -\\lambda\n\\end{cases}\n$$\nThis derivative can be written compactly using an indicator function as $\\mathbf{1}_{\\{|y_i| > \\lambda\\}}$.\nSubstituting this into the expectation, we have:\n$$\n\\mathrm{Cov}(\\eta_{\\lambda, i}(Y_i), Y_i) = \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i| > \\lambda\\}}] = \\sigma^2 P(|Y_i| > \\lambda)\n$$\nSumming over all coordinates $i=1, \\dots, n$, the total degrees of freedom are:\n$$\n\\mathrm{df}(\\eta_{\\lambda}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\sigma^2 P(|Y_i| > \\lambda) = \\sum_{i=1}^n P(|Y_i| > \\lambda)\n$$\nNow, we show this equals the expected number of nonzero coordinates of $\\eta_{\\lambda}(Y)$. A coordinate $\\eta_{\\lambda, i}(Y_i)$ is nonzero if and only if $|Y_i| > \\lambda$. Let $Z_i = \\mathbf{1}_{\\{\\eta_{\\lambda, i}(Y_i) \\neq 0\\}} = \\mathbf{1}_{\\{|Y_i| > \\lambda\\}}$ be an indicator variable for the $i$-th coordinate being nonzero. The total number of nonzero coordinates is $\\sum_{i=1}^n Z_i$. Its expectation is:\n$$\n\\mathbb{E}\\left[\\sum_{i=1}^n Z_i\\right] = \\sum_{i=1}^n \\mathbb{E}[Z_i] = \\sum_{i=1}^n P(\\eta_{\\lambda, i}(Y_i) \\neq 0) = \\sum_{i=1}^n P(|Y_i| > \\lambda)\n$$\nThis is identical to the expression for $\\mathrm{df}(\\eta_{\\lambda})$.\nTo express this explicitly, we use the fact that $Y_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$. Let $\\Phi(\\cdot)$ be the cumulative distribution function of the standard normal distribution $\\mathcal{N}(0, 1)$.\n$$\nP(|Y_i| > \\lambda) = P(Y_i > \\lambda) + P(Y_i < -\\lambda) = P\\left(\\frac{Y_i - \\theta_i}{\\sigma} > \\frac{\\lambda - \\theta_i}{\\sigma}\\right) + P\\left(\\frac{Y_i - \\theta_i}{\\sigma} < \\frac{-\\lambda - \\theta_i}{\\sigma}\\right)\n$$\n$$\nP(|Y_i| > \\lambda) = \\left(1 - \\Phi\\left(\\frac{\\lambda - \\theta_i}{\\sigma}\\right)\\right) + \\Phi\\left(\\frac{-\\lambda - \\theta_i}{\\sigma}\\right) = \\Phi\\left(\\frac{\\theta_i - \\lambda}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\theta_i - \\lambda}{\\sigma}\\right)\n$$\nSo, the degrees of freedom are $\\mathrm{df}(\\eta_{\\lambda}) = \\sum_{i=1}^n \\left[ \\Phi\\left(\\frac{\\theta_i - \\lambda}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\theta_i - \\lambda}{\\sigma}\\right) \\right]$.\n\n### Part 2: Stein's Unbiased Risk Estimate (SURE)\n\nThe prediction risk is $R(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - \\theta\\|_2^2]$. We can decompose the risk as:\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y + Y - \\theta\\|_2^2] = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + \\mathbb{E}[\\|Y - \\theta\\|_2^2] + 2\\mathbb{E}[\\langle \\eta_{\\lambda}(Y) - Y, Y - \\theta \\rangle]\n$$\nThe middle term is $\\mathbb{E}[\\|\\epsilon\\|_2^2] = n\\sigma^2$. The cross-term can be analyzed using Stein's identity.\n$$\n\\mathbb{E}[\\langle \\eta_{\\lambda}(Y) - Y, Y - \\theta \\rangle] = \\sum_{i=1}^n \\mathbb{E}[(\\eta_{\\lambda, i}(Y_i) - Y_i)(Y_i - \\theta_i)]\n$$\nLet $h_i(y_i) = \\eta_{\\lambda, i}(y_i) - y_i$. Using Stein's identity $\\mathbb{E}[h_i(Y_i)(Y_i - \\theta_i)] = \\sigma^2 \\mathbb{E}[h_i'(Y_i)]$:\n$$\n\\mathbb{E}[h_i(Y_i)(Y_i - \\theta_i)] = \\sigma^2 \\mathbb{E}\\left[\\frac{d}{dY_i}(\\eta_{\\lambda, i}(Y_i) - Y_i)\\right] = \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i| > \\lambda\\}} - 1]\n$$\nSubstituting this back into the risk expansion:\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + n\\sigma^2 + 2\\sum_{i=1}^n \\sigma^2 \\mathbb{E}[\\mathbf{1}_{\\{|Y_i| > \\lambda\\}} - 1]\n$$\n$$\nR(\\lambda) = \\mathbb{E}[\\|\\eta_{\\lambda}(Y) - Y\\|_2^2] + n\\sigma^2 + 2\\sigma^2\\mathbb{E}\\left[\\sum_{i=1}^n \\mathbf{1}_{\\{|Y_i| > \\lambda\\}}\\right] - 2n\\sigma^2\n$$\n$$\nR(\\lambda) = \\mathbb{E}\\left[ \\|\\eta_{\\lambda}(Y) - Y\\|_2^2 - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|Y_i| > \\lambda\\}} \\right]\n$$\nBy the law of total expectation, the expression inside the expectation is an unbiased estimator of the risk. This is SURE. For an observed data vector $y$, the estimate is:\n$$\n\\mathrm{SURE}(y, \\lambda) = \\|\\eta_{\\lambda}(y) - y\\|_2^2 - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i| > \\lambda\\}}\n$$\nWe can write the first term more explicitly. For each coordinate $i$:\n$$\n(\\eta_{\\lambda, i}(y_i) - y_i)^2 =\n\\begin{cases}\n(y_i - \\lambda - y_i)^2 = \\lambda^2 & \\text{if } y_i > \\lambda \\\\\n(0 - y_i)^2 = y_i^2 & \\text{if } |y_i| \\le \\lambda \\\\\n(y_i + \\lambda - y_i)^2 = \\lambda^2 & \\text{if } y_i < -\\lambda\n\\end{cases}\n$$\nThis is equivalent to $\\min(y_i^2, \\lambda^2)$. Thus, the final pathwise unbiased risk estimator is:\n$$\n\\mathrm{SURE}(y, \\lambda) = \\sum_{i=1}^n \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i| > \\lambda\\}}\n$$\n\n### Part 3: Minimization of SURE\n\nWe seek to find $\\hat{\\lambda} = \\operatorname{argmin}_{\\lambda \\ge 0} \\mathrm{SURE}(y, \\lambda)$. This is equivalent to minimizing the function $g(\\lambda) = \\mathrm{SURE}(y, \\lambda) + n\\sigma^2$, given by:\n$$\ng(\\lambda) = \\sum_{i=1}^n \\left( \\min(y_i^2, \\lambda^2) + 2\\sigma^2 \\mathbf{1}_{\\{|y_i| > \\lambda\\}} \\right)\n$$\nThe function $g(\\lambda)$ is a sum of functions $g_i(\\lambda) = \\min(y_i^2, \\lambda^2) + 2\\sigma^2 \\mathbf{1}_{\\{|y_i| > \\lambda\\}}$. For a fixed $i$, $g_i(\\lambda)$ is continuous for $\\lambda \\ge 0$ except at $\\lambda = |y_i|$. The derivative of $\\min(y_i^2, \\lambda^2)$ with respect to $\\lambda$ is $2\\lambda$ for $\\lambda < |y_i|$ and $0$ for $\\lambda > |y_i|$. The indicator function $\\mathbf{1}_{\\{|y_i| > \\lambda\\}}$ is piecewise constant.\nLet $|y|_{(0)} \\equiv 0$ and $|y|_{(1)} \\le |y|_{(2)} \\le \\dots \\le |y|_{(n)}$ be the order statistics of the absolute values of the coordinates of $y$. These values partition the domain $\\lambda \\ge 0$ into intervals $[|y|_{(k)}, |y|_{(k+1)})$ for $k=0, \\dots, n-1$, and $[|y|_{(n)}, \\infty)$.\n\nFor any $\\lambda$ in an open interval $(|y|_{(k)}, |y|_{(k+1)})$:\n- There are $k$ coordinates with $|y_i| \\le |y|_{(k)} < \\lambda$. For these, $\\min(y_i^2, \\lambda^2) = y_i^2$ and $\\mathbf{1}_{\\{|y_i|>\\lambda\\}} = 0$.\n- There are $n-k$ coordinates with $|y_i| \\ge |y|_{(k+1)} > \\lambda$. For these, $\\min(y_i^2, \\lambda^2) = \\lambda^2$ and $\\mathbf{1}_{\\{|y_i|>\\lambda\\}} = 1$.\nSo, for $\\lambda \\in (|y|_{(k)}, |y|_{(k+1)})$, the function is:\n$$\ng(\\lambda) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(\\lambda^2 + 2\\sigma^2)\n$$\nThe derivative with respect to $\\lambda$ on this interval is $\\frac{dg}{d\\lambda} = 2(n-k)\\lambda$. Since $\\lambda > 0$ and $n-k \\ge 1$ (for $k < n$), this derivative is strictly positive. This implies that $g(\\lambda)$ is strictly increasing on each open interval $(|y|_{(k)}, |y|_{(k+1)})$.\nFor $\\lambda > |y|_{(n)}$, all $|y_i| \\le \\lambda$, so $g(\\lambda) = \\sum_{i=1}^n y_i^2$, which is constant.\nSince the function is increasing on the intervals between the points $\\{0, |y|_{(1)}, \\dots, |y|_{(n)}\\}$ and constant beyond the last point, the global minimum of $g(\\lambda)$ for $\\lambda \\ge 0$ must be attained at one of these points.\n\nWe therefore need to find the value of $k \\in \\{0, 1, \\dots, n\\}$ that minimizes $g(|y|_{(k)})$. Let's define the criterion $S(k) = g(|y|_{(k)})$.\nAt $\\lambda = |y|_{(k)}$:\n- $\\sum_{i=1}^n \\min(y_i^2, |y|_{(k)}^2) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)|y|_{(k)}^2$.\n- The term $\\sum_{i=1}^n \\mathbf{1}_{\\{|y_i| > |y|_{(k)}\\}}$ is the number of coordinates with absolute value strictly greater than $|y|_{(k)}$. This depends on whether the values $|y|_{(j)}$ are unique. However, the form of $g(\\lambda)$ on the interval $[|y|_{(k)}, |y|_{(k+1)})$ is $g(\\lambda) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(\\lambda^2+2\\sigma^2)$, which is continuous and increasing. Its minimum on this interval is at $\\lambda = |y|_{(k)}$. The value is $g(|y|_{(k)}) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)(|y|_{(k)}^2+2\\sigma^2)$. This expression correctly handles ties and serves as the criterion to minimize over $k$.\nSo, we need to find $\\hat{k} = \\operatorname{argmin}_{k \\in \\{0, 1, \\dots, n\\}} S(k)$, where\n$$\nS(k) = \\sum_{j=1}^k |y|_{(j)}^2 + (n-k)\\left(|y|_{(k)}^2 + 2\\sigma^2\\right)\n$$\nwith the conventions $|y|_{(0)} = 0$ and $\\sum_{j=1}^0 (\\cdot) = 0$. The minimizing threshold is then $\\hat{\\lambda} = |y|_{(\\hat{k})}$. This constitutes a closed-form analytic expression as it specifies a direct computational procedure involving sorting and a finite number of comparisons, not an iterative optimization.\n\nThe final expression is therefore $\\hat{\\lambda}=|y|_{(\\hat{k})}$, where $\\hat{k}$ is the index that minimizes the explicit function $S(k)$.",
            "answer": "$$\n\\boxed{\\hat{\\lambda} = |y|_{(\\hat{k})}, \\quad \\text{where } \\hat{k} = \\underset{k \\in \\{0, 1, \\dots, n\\}}{\\operatorname{argmin}} \\left\\{ \\sum_{j=1}^{k} |y|_{(j)}^{2} + (n-k)\\left(|y|_{(k)}^{2} + 2\\sigma^{2}\\right) \\right\\}}\n$$"
        }
    ]
}