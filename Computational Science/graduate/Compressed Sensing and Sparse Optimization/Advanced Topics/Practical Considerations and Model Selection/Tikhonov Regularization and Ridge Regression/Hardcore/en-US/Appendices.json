{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Tikhonov regularization, we must first build its solution from the ground up. This practice guides you through deriving the ridge estimator's normal equations and then solving them in a simplified setting where the problem decouples into a set of scalar equations. By analyzing the solution in this eigenbasis , you will gain a clear, intuitive understanding of how ridge regression acts as a spectral filter, selectively damping components of the solution to ensure stability.",
            "id": "3490519",
            "problem": "Consider a linear inverse problem in compressed sensing, where an unknown signal vector $x \\in \\mathbb{R}^{p}$ is estimated from measurements $y \\in \\mathbb{R}^{m}$ via a measurement matrix $X \\in \\mathbb{R}^{m \\times p}$. The estimate $x_{\\lambda}$ is obtained by minimizing a Tikhonov-regularized least squares objective\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2},\n$$\nwhere $L \\in \\mathbb{R}^{p \\times p}$ is a fixed linear regularization operator and $\\lambda  0$ is the regularization parameter. Assume $X^{\\top} X$ and $L^{\\top} L$ are symmetric positive semidefinite, commute, and are simultaneously diagonalizable by an orthogonal matrix $Q \\in \\mathbb{R}^{p \\times p}$. Let the diagonalizations be\n$$\nQ^{\\top}(X^{\\top} X)Q = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p}), \\quad Q^{\\top}(L^{\\top} L)Q = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p}),\n$$\nand define $w := Q^{\\top} X^{\\top} y \\in \\mathbb{R}^{p}$.\n\nWork from first principles to derive the normal equations for the minimizer $x_{\\lambda}$ and use the simultaneous diagonalization to obtain a closed-form representation for $x_{\\lambda}$ in the eigenbasis of $X^{\\top} X$ and $L^{\\top} L$. Then, for the concrete case $p = 4$, with eigenvalues and transformed data\n$$\n(a_{1}, a_{2}, a_{3}, a_{4}) = (9, 4, 1, 0), \\quad (b_{1}, b_{2}, b_{3}, b_{4}) = (1, 4, 9, 16), \\quad (w_{1}, w_{2}, w_{3}, w_{4}) = (3, 2, 1, 0),\n$$\nand regularization level $\\lambda = 2$, compute the exact value of the squared Euclidean norm $\\|x_{\\lambda}\\|_{2}^{2}$ as a single closed-form expression. No rounding is required.",
            "solution": "The user wants me to solve a Tikhonov regularization problem. The problem is well-defined, scientifically sound, and provides all necessary information. It is a standard problem in linear algebra and optimization, with direct applications in compressed sensing and machine learning. There are no contradictions, ambiguities, or factual errors. Therefore, the problem is valid. I will proceed with the solution.\n\nThe objective is to find the minimizer $x_{\\lambda}$ of the Tikhonov-regularized least squares cost function:\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2}\n$$\nTo find the minimizer, we must compute the gradient of $J(x)$ with respect to $x$ and set it to zero.\n\nFirst, we expand the terms in the objective function. The squared Euclidean norm is defined as $\\|v\\|_2^2 = v^{\\top}v$.\nThe first term is the least-squares data fidelity term:\n$$\n\\frac{1}{2} \\|X x - y\\|_{2}^{2} = \\frac{1}{2} (X x - y)^{\\top}(X x - y) = \\frac{1}{2} (x^{\\top}X^{\\top} - y^{\\top})(X x - y) = \\frac{1}{2} (x^{\\top}X^{\\top}X x - x^{\\top}X^{\\top}y - y^{\\top}X x + y^{\\top}y)\n$$\nSince $y^{\\top}X x$ is a scalar, it is equal to its transpose $(y^{\\top}X x)^{\\top} = x^{\\top}X^{\\top}y$. Thus, we can write this term as:\n$$\n\\frac{1}{2} (x^{\\top}X^{\\top}X x - 2 x^{\\top}X^{\\top}y + y^{\\top}y)\n$$\nThe second term is the regularization term:\n$$\n\\frac{\\lambda}{2} \\|L x\\|_{2}^{2} = \\frac{\\lambda}{2} (L x)^{\\top}(L x) = \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\nCombining these, the full objective function is:\n$$\nJ(x) = \\frac{1}{2} x^{\\top}X^{\\top}X x - x^{\\top}X^{\\top}y + \\frac{1}{2} y^{\\top}y + \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\nNow, we compute the gradient $\\nabla_x J(x)$. Using standard matrix calculus identities, $\\nabla_x (x^{\\top}Ax) = 2Ax$ for a symmetric matrix $A$, and $\\nabla_x (b^{\\top}x) = b$, we have:\n$$\n\\nabla_x J(x) = \\frac{1}{2} (2 X^{\\top}X x) - X^{\\top}y + 0 + \\frac{\\lambda}{2} (2 L^{\\top}L x)\n$$\n$$\n\\nabla_x J(x) = X^{\\top}X x - X^{\\top}y + \\lambda L^{\\top}L x\n$$\nSetting the gradient to zero, $\\nabla_x J(x) = 0$, gives the normal equations for the minimizer $x_{\\lambda}$:\n$$\nX^{\\top}X x_{\\lambda} + \\lambda L^{\\top}L x_{\\lambda} = X^{\\top}y\n$$\n$$\n(X^{\\top}X + \\lambda L^{\\top}L) x_{\\lambda} = X^{\\top}y\n$$\nThis is the general form of the normal equations.\n\nNext, we use the provided simultaneous diagonalization to solve for $x_{\\lambda}$. We are given an orthogonal matrix $Q$ such that $Q^{\\top}Q = QQ^{\\top} = I$ and:\n$$\nQ^{\\top}(X^{\\top} X)Q = A = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p})\n$$\n$$\nQ^{\\top}(L^{\\top} L)Q = B = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p})\n$$\nFrom these, we can express $X^{\\top}X$ and $L^{\\top}L$ as $X^{\\top}X = QAQ^{\\top}$ and $L^{\\top}L = QBQ^{\\top}$. Substituting these into the normal equations:\n$$\n(QAQ^{\\top} + \\lambda QBQ^{\\top}) x_{\\lambda} = X^{\\top}y\n$$\n$$\nQ(A + \\lambda B)Q^{\\top} x_{\\lambda} = X^{\\top}y\n$$\nMultiplying both sides from the left by $Q^{\\top}$:\n$$\nQ^{\\top}Q(A + \\lambda B)Q^{\\top} x_{\\lambda} = Q^{\\top}X^{\\top}y\n$$\n$$\n(A + \\lambda B)Q^{\\top} x_{\\lambda} = Q^{\\top}X^{\\top}y\n$$\nLet's define the solution vector in the eigenbasis as $\\tilde{x}_{\\lambda} = Q^{\\top}x_{\\lambda}$ and use the given definition $w = Q^{\\top}X^{\\top}y$. The equation becomes:\n$$\n(A + \\lambda B)\\tilde{x}_{\\lambda} = w\n$$\nSince $A$ and $B$ are diagonal matrices, their sum is also a diagonal matrix:\n$$\nA + \\lambda B = \\operatorname{diag}(a_1 + \\lambda b_1, a_2 + \\lambda b_2, \\dots, a_p + \\lambda b_p)\n$$\nThe system of equations decouples into $p$ independent scalar equations:\n$$\n(a_i + \\lambda b_i) (\\tilde{x}_{\\lambda})_i = w_i \\quad \\text{for } i = 1, \\dots, p\n$$\nThe components of the solution in the eigenbasis are thus:\n$$\n(\\tilde{x}_{\\lambda})_i = \\frac{w_i}{a_i + \\lambda b_i}\n$$\nThe problem asks for the squared Euclidean norm of the solution, $\\|x_{\\lambda}\\|_{2}^{2}$. Since $Q$ is an orthogonal matrix, it preserves the Euclidean norm. We can show this as follows:\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = x_{\\lambda}^{\\top}x_{\\lambda} = (Q\\tilde{x}_{\\lambda})^{\\top}(Q\\tilde{x}_{\\lambda}) = \\tilde{x}_{\\lambda}^{\\top}Q^{\\top}Q\\tilde{x}_{\\lambda} = \\tilde{x}_{\\lambda}^{\\top}I\\tilde{x}_{\\lambda} = \\tilde{x}_{\\lambda}^{\\top}\\tilde{x}_{\\lambda} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}\n$$\nTherefore, we can compute the norm by summing the squares of the components of $\\tilde{x}_{\\lambda}$:\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{p} ((\\tilde{x}_{\\lambda})_i)^2 = \\sum_{i=1}^{p} \\left( \\frac{w_i}{a_i + \\lambda b_i} \\right)^2\n$$\nNow, we substitute the specific values given in the problem: $p=4$, $\\lambda=2$, $(a_1, a_2, a_3, a_4) = (9, 4, 1, 0)$, $(b_1, b_2, b_3, b_4) = (1, 4, 9, 16)$, and $(w_1, w_2, w_3, w_4) = (3, 2, 1, 0)$.\n\nWe compute the components of $\\tilde{x}_{\\lambda}$:\nFor $i=1$:\n$$\n(\\tilde{x}_{\\lambda})_1 = \\frac{w_1}{a_1 + \\lambda b_1} = \\frac{3}{9 + 2(1)} = \\frac{3}{11}\n$$\nFor $i=2$:\n$$\n(\\tilde{x}_{\\lambda})_2 = \\frac{w_2}{a_2 + \\lambda b_2} = \\frac{2}{4 + 2(4)} = \\frac{2}{12} = \\frac{1}{6}\n$$\nFor $i=3$:\n$$\n(\\tilde{x}_{\\lambda})_3 = \\frac{w_3}{a_3 + \\lambda b_3} = \\frac{1}{1 + 2(9)} = \\frac{1}{19}\n$$\nFor $i=4$:\n$$\n(\\tilde{x}_{\\lambda})_4 = \\frac{w_4}{a_4 + \\lambda b_4} = \\frac{0}{0 + 2(16)} = \\frac{0}{32} = 0\n$$\nNow, we compute the squared norm $\\|x_{\\lambda}\\|_{2}^{2} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}$:\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\left(\\frac{3}{11}\\right)^2 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{19}\\right)^2 + 0^2\n$$\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{9}{121} + \\frac{1}{36} + \\frac{1}{361}\n$$\nTo sum these fractions, we find a common denominator. The denominators are $121=11^2$, $36=6^2$, and $361=19^2$. Since $11$, $6$, and $19$ are pairwise coprime, the least common denominator is their product: $121 \\times 36 \\times 361 = 4356 \\times 361 = 1572516$.\nWe convert each fraction to this common denominator:\n$$\n\\frac{9}{121} = \\frac{9 \\times 36 \\times 361}{121 \\times 36 \\times 361} = \\frac{9 \\times 12996}{1572516} = \\frac{116964}{1572516}\n$$\n$$\n\\frac{1}{36} = \\frac{1 \\times 121 \\times 361}{36 \\times 121 \\times 361} = \\frac{43681}{1572516}\n$$\n$$\n\\frac{1}{361} = \\frac{1 \\times 121 \\times 36}{361 \\times 121 \\times 36} = \\frac{4356}{1572516}\n$$\nNow we sum the numerators:\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{116964 + 43681 + 4356}{1572516} = \\frac{165001}{1572516}\n$$\nThe prime factors of the denominator are $2$, $3$, $11$, and $19$. A check shows that the numerator $165001$ is not divisible by any of these primes, so the fraction is in its simplest form.",
            "answer": "$$\\boxed{\\frac{165001}{1572516}}$$"
        },
        {
            "introduction": "A theoretical model is only as good as our ability to assess its performance and select its parameters. This comprehensive exercise  challenges you to derive and implement the essential machinery for model evaluation in the context of Tikhonov regularization, including the hat matrix, effective degrees of freedom, and efficient formulas for leave-one-out cross-validation. Bridging the gap between algebraic derivation and practical coding, this practice illuminates the computational shortcuts that make sophisticated model diagnostics feasible.",
            "id": "3490562",
            "problem": "You are given a linear inverse problem of estimating a coefficient vector $ \\beta \\in \\mathbb{R}^p $ from measurements $ y \\in \\mathbb{R}^n $ and a design matrix $ X \\in \\mathbb{R}^{n \\times p} $ via Tikhonov regularization. Consider the penalized least squares estimator defined as the minimizer of the functional\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\| y - X \\beta \\|_2^2 + \\lambda \\, \\| L \\beta \\|_2^2,\n$$\nwhere $ \\lambda \\ge 0 $ is a regularization parameter and $ L \\in \\mathbb{R}^{m \\times p} $ is a prescribed linear operator. Start from the fundamental base of linear algebra and optimization, including the normal equations for unconstrained least squares, properties of orthogonal projections and linear smoothers, and the Sherman–Morrison–Woodbury identity. Do not assume pre-derived formulas for the hat matrix, effective degrees of freedom, or cross-validation shortcuts.\n\nTasks:\n1. Derive the estimator $ \\hat{\\beta}_\\lambda $ by setting the gradient to zero and solving the resulting normal equations. Then, express the fitted response $ \\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda $ in the form $ \\hat{y}_\\lambda = S_\\lambda \\, y $, identifying the linear smoother (hat matrix) $ S_\\lambda \\in \\mathbb{R}^{n \\times n} $ explicitly in terms of $ X $, $ L $, and $ \\lambda $.\n2. Using the definition of effective degrees of freedom for a linear smoother as $ \\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right) $, derive a computable expression for $ \\mathrm{df}_\\lambda $ from $ S_\\lambda $. Justify all steps from first principles.\n3. For Leave-One-Out Cross-Validation (LOOCV), define for each index $ i \\in \\{1,\\dots,n\\} $ the leave-one-out fitted value $ \\hat{y}^{(-i)}_{\\lambda, i} $ obtained by refitting the model on the dataset with the $ i $-th observation removed. Derive a formula for the leave-one-out residuals $ e^{\\mathrm{LOO}}_i = y_i - \\hat{y}^{(-i)}_{\\lambda, i} $ expressed in terms of the full-sample residuals and diagonal entries of $ S_\\lambda $. Use this to obtain a closed-form expression for the LOOCV mean squared error $ \\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e^{\\mathrm{LOO}}_i)^2 $ without explicitly refitting $ n $ times.\n4. Define the Generalized Cross-Validation (GCV) mean squared error $ \\mathrm{GCV}_\\lambda $ by replacing the pointwise leverages in LOOCV by their average derived from $ \\mathrm{df}_\\lambda $. Derive a computable expression for $ \\mathrm{GCV}_\\lambda $.\n\nThen implement a program that, for the following test suite, computes for each case the triple $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $ and prints a single line containing the list of these triples. All floating-point outputs must be rounded to exactly $ 8 $ decimal places, using standard rounding to nearest with ties to even. The output format must be a single line containing a comma-separated list of the results enclosed in square brackets, where each result itself is a list, for example $ [[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots] $.\n\nTest suite:\n- Case A (happy path, ridge regression): $ n = 5 $, $ p = 3 $, with\n$$\nX^{(A)} = \\begin{bmatrix}\n1  -1  2 \\\\\n0  2  -1 \\\\\n3  0  1 \\\\\n-2  1  0 \\\\\n1  1  1\n\\end{bmatrix}, \\quad\ny^{(A)} = \\begin{bmatrix}\n1 \\\\ -1 \\\\ 3 \\\\ 0 \\\\ 2\n\\end{bmatrix}, \\quad\nL^{(A)} = I_3, \\quad\n\\lambda^{(A)} = 0.5.\n$$\n- Case B (boundary, ordinary least squares): same $ X^{(A)} $, $ y^{(A)} $, and $ L^{(A)} $, but\n$$\n\\lambda^{(B)} = 0.\n$$\n- Case C (edge, extreme regularization): same $ X^{(A)} $, $ y^{(A)} $, and $ L^{(A)} $, but\n$$\n\\lambda^{(C)} = 10^6.\n$$\n- Case D (general Tikhonov with first-difference operator): same $ X^{(A)} $ and $ y^{(A)} $, but with the first-difference operator\n$$\nL^{(D)} = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}, \\quad\n\\lambda^{(D)} = 1.\n$$\n- Case E (underdetermined system, ridge stabilization): $ n = 3 $, $ p = 5 $, with\n$$\nX^{(E)} = \\begin{bmatrix}\n1  0  -1  2  0 \\\\\n0  1  1  -1  2 \\\\\n2  -1  0  1  1\n\\end{bmatrix}, \\quad\ny^{(E)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ -1\n\\end{bmatrix}, \\quad\nL^{(E)} = I_5, \\quad\n\\lambda^{(E)} = 0.3.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the list $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $ for the corresponding test case, in the order A, B, C, D, E. Each real number must be rounded to exactly $ 8 $ decimal places. No units are involved in this problem, and no angles are used.",
            "solution": "The problem statement is found to be valid as it is scientifically grounded in the theory of regularized linear models, well-posed, objective, and provides a complete and consistent setup for both the theoretical derivations and the numerical computations.\n\nThe task is to derive and compute key quantities related to Tikhonov regularization. The objective is to find the estimator $ \\beta \\in \\mathbb{R}^p $ that minimizes the penalized least squares functional:\n$$\nJ(\\beta) = \\| y - X \\beta \\|_2^2 + \\lambda \\| L \\beta \\|_2^2\n$$\nwhere $ y \\in \\mathbb{R}^n $ is the response vector, $ X \\in \\mathbb{R}^{n \\times p} $ is the design matrix, $ L \\in \\mathbb{R}^{m \\times p} $ is a penalty operator, and $ \\lambda \\ge 0 $ is the regularization parameter.\n\n**1. Derivation of the Estimator and the Smoother Matrix**\nThe objective function can be written using matrix-vector products as:\n$$\nJ(\\beta) = (y - X \\beta)^T (y - X \\beta) + \\lambda (L \\beta)^T (L \\beta)\n$$\nExpanding the terms, we have:\n$$\nJ(\\beta) = (y^T - \\beta^T X^T)(y - X \\beta) + \\lambda \\beta^T L^T L \\beta\n$$\n$$\nJ(\\beta) = y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda \\beta^T L^T L \\beta\n$$\nSince $ \\beta^T X^T y $ is a scalar, it is equal to its transpose $ y^T X \\beta $. Therefore, we can combine the cross-terms:\n$$\nJ(\\beta) = y^T y - 2 \\beta^T X^T y + \\beta^T (X^T X + \\lambda L^T L) \\beta\n$$\nTo find the minimizer $ \\hat{\\beta}_\\lambda $, we compute the gradient of $ J(\\beta) $ with respect to $ \\beta $ and set it to the zero vector. Using standard rules of matrix calculus ($ \\frac{\\partial b^T A c}{\\partial c} = A^T b $ and $ \\frac{\\partial c^T A c}{\\partial c} = (A+A^T)c $), and noting that $ X^T X + \\lambda L^T L $ is symmetric, we get:\n$$\n\\nabla_\\beta J(\\beta) = -2 X^T y + 2 (X^T X + \\lambda L^T L) \\beta\n$$\nSetting the gradient to zero yields the normal equations for the penalized problem:\n$$\n(X^T X + \\lambda L^T L) \\hat{\\beta}_\\lambda = X^T y\n$$\nThe matrix $ M = X^T X + \\lambda L^T L $ is invertible if $ \\lambda  0 $ and the null spaces of $ X $ and $ L $ intersect only at the origin (i.e., $ \\mathrm{ker}(X) \\cap \\mathrm{ker}(L) = \\{0\\} $), or if $ \\lambda = 0 $ and $ X^T X $ is invertible (i.e., $ X $ has full column rank). Assuming $ M $ is invertible, we can solve for the estimator $ \\hat{\\beta}_\\lambda $:\n$$\n\\hat{\\beta}_\\lambda = (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\nThe fitted response vector $ \\hat{y}_\\lambda $ is obtained by applying the design matrix to the estimated coefficients:\n$$\n\\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\nBy definition, the fitted response is a linear transformation of the observed response, $ \\hat{y}_\\lambda = S_\\lambda y $. By comparing the two expressions, we can identify the linear smoother, or hat matrix, $ S_\\lambda $:\n$$\nS_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T\n$$\n$ S_\\lambda $ is an $ n \\times n $ matrix that maps the observed data $ y $ to the fitted values $ \\hat{y}_\\lambda $.\n\n**2. Derivation of Effective Degrees of Freedom**\nThe effective degrees of freedom, $ \\mathrm{df}_\\lambda $, are defined as the trace of the Jacobian of the fitted values with respect to the observed values:\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right)\n$$\nWe have the linear relationship $ \\hat{y}_\\lambda = S_\\lambda y $. Let's write this component-wise. The $ i $-th element of $ \\hat{y}_\\lambda $ is given by:\n$$\n\\hat{y}_{\\lambda, i} = \\sum_{j=1}^n (S_\\lambda)_{ij} y_j\n$$\nThe partial derivative of the $ i $-th fitted value with respect to the $ j $-th observed value is:\n$$\n\\frac{\\partial \\hat{y}_{\\lambda, i}}{\\partial y_j} = (S_\\lambda)_{ij}\n$$\nThis is precisely the element in the $ i $-th row and $ j $-th column of the matrix $ S_\\lambda $. Therefore, the Jacobian matrix is the smoother matrix itself:\n$$\n\\frac{\\partial \\hat{y}_\\lambda}{\\partial y} = S_\\lambda\n$$\nSubstituting this back into the definition of effective degrees of freedom, we obtain the computable expression:\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}(S_\\lambda)\n$$\n\n**3. Derivation of the Leave-One-Out Cross-Validation (LOOCV) Formula**\nFor each observation $ i \\in \\{1,\\dots,n\\} $, we define a leave-one-out (LOO) model trained on all data except the $ i $-th pair $(x_i^T, y_i)$. Let $ X^{(-i)} $ be the matrix $ X $ with its $ i $-th row $ x_i^T $ removed, and $ y^{(-i)} $ be the vector $ y $ with its $ i $-th element $ y_i $ removed. The LOO estimator $ \\hat{\\beta}_\\lambda^{(-i)} $ is:\n$$\n\\hat{\\beta}_\\lambda^{(-i)} = \\left( (X^{(-i)})^T X^{(-i)} + \\lambda L^T L \\right)^{-1} (X^{(-i)})^T y^{(-i)}\n$$\nThe terms involving the reduced dataset can be expressed as rank-1 updates to the full-dataset matrices:\n$$\n(X^{(-i)})^T X^{(-i)} = \\sum_{j \\neq i} x_j x_j^T = X^T X - x_i x_i^T\n$$\n$$\n(X^{(-i)})^T y^{(-i)} = \\sum_{j \\neq i} x_j y_j = X^T y - x_i y_i\n$$\nLet $ A = X^T X + \\lambda L^T L $. Substituting these into the LOO estimator expression gives:\n$$\n\\hat{\\beta}_\\lambda^{(-i)} = (A - x_i x_i^T)^{-1} (X^T y - x_i y_i)\n$$\nWe use the Sherman-Morrison formula for the inverse of a rank-1 update: $(B - uv^T)^{-1} = B^{-1} + \\frac{B^{-1}uv^TB^{-1}}{1 - v^TB^{-1}u}$. With $ B=A $ and $ u=v=x_i $, we have:\n$$\n(A - x_i x_i^T)^{-1} = A^{-1} + \\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1} x_i}\n$$\nThe LOO predicted value for the $ i $-th observation is $ \\hat{y}_{\\lambda, i}^{(-i)} = x_i^T \\hat{\\beta}_\\lambda^{(-i)} $. Substituting the above:\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = x_i^T \\left( A^{-1} + \\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1} x_i} \\right) (X^T y - x_i y_i)\n$$\nLet's recognize two key quantities. First, the full-data fitted value at $ i $: $ \\hat{y}_{\\lambda, i} = x_i^T \\hat{\\beta}_\\lambda = x_i^T A^{-1} X^T y $. Second, the $ i $-th diagonal element of the smoother matrix $ S_\\lambda = X A^{-1} X^T $: $ S_{\\lambda, ii} = x_i^T A^{-1} x_i $.\nExpanding the expression for $ \\hat{y}_{\\lambda, i}^{(-i)} $:\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = \\left( x_i^T A^{-1} + \\frac{(x_i^T A^{-1} x_i) x_i^T A^{-1}}{1 - S_{\\lambda, ii}} \\right) (X^T y - x_i y_i)\n$$\n$$\n= \\left( 1 + \\frac{S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} \\right) x_i^T A^{-1} (X^T y - x_i y_i) = \\frac{1}{1 - S_{\\lambda, ii}} (x_i^T A^{-1} X^T y - y_i x_i^T A^{-1} x_i)\n$$\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = \\frac{\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}}\n$$\nNow we compute the LOO residual $ e_i^{\\mathrm{LOO}} = y_i - \\hat{y}_{\\lambda, i}^{(-i)} $:\n$$\ne_i^{\\mathrm{LOO}} = y_i - \\frac{\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} = \\frac{y_i(1 - S_{\\lambda, ii}) - (\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii})}{1 - S_{\\lambda, ii}}\n$$\n$$\ne_i^{\\mathrm{LOO}} = \\frac{y_i - y_i S_{\\lambda, ii} - \\hat{y}_{\\lambda, i} + y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} = \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}}\n$$\nThis crucial result shows that the LOO residual can be calculated from the ordinary full-sample residual $ e_i = y_i - \\hat{y}_{\\lambda, i} $ and the corresponding diagonal element of the hat matrix $ S_\\lambda $, without refitting the model.\nThe LOOCV mean squared error is then:\n$$\n\\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e_i^{\\mathrm{LOO}})^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}} \\right)^2\n$$\n\n**4. Derivation of the Generalized Cross-Validation (GCV) Formula**\nGeneralized Cross-Validation approximates LOOCV by replacing each individual leverage score $ S_{\\lambda, ii} $ in the denominator with their average value. The average leverage is:\n$$\n\\bar{S}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n S_{\\lambda, ii} = \\frac{1}{n} \\mathrm{trace}(S_\\lambda)\n$$\nFrom part 2, we know that $ \\mathrm{trace}(S_\\lambda) = \\mathrm{df}_\\lambda $. Therefore, the average leverage is simply $ \\frac{\\mathrm{df}_\\lambda}{n} $. Substituting this average for each $ S_{\\lambda, ii} $ in the LOOCV formula gives the GCV statistic:\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{trace}(S_\\lambda)/n} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{df}_\\lambda/n} \\right)^2\n$$\nThe denominator is constant with respect to the summation index $ i $, so it can be factored out:\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{(1 - \\mathrm{df}_\\lambda/n)^2} \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_{\\lambda, i})^2 \\right)\n$$\nRecognizing that the term in the parentheses is the training mean squared error, $ \\mathrm{MSE}_\\lambda = \\frac{1}{n} \\|y - \\hat{y}_\\lambda\\|_2^2 $, we arrive at the standard computable expression for GCV:\n$$\n\\mathrm{GCV}_\\lambda = \\frac{\\mathrm{MSE}_\\lambda}{\\left(1 - \\mathrm{df}_\\lambda/n\\right)^2} = \\frac{\\frac{1}{n} \\|y - \\hat{y}_\\lambda\\|_2^2}{\\left(1 - \\frac{\\mathrm{df}_\\lambda}{n}\\right)^2}\n$$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes effective degrees of freedom, LOOCV score, and GCV score\n    for Tikhonov regularization for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    X_A = np.array([\n        [1, -1, 2],\n        [0, 2, -1],\n        [3, 0, 1],\n        [-2, 1, 0],\n        [1, 1, 1]\n    ])\n    y_A = np.array([[1], [-1], [3], [0], [2]])\n    L_A = np.identity(3)\n    lambda_A = 0.5\n    \n    lambda_B = 0.0\n    \n    lambda_C = 1e6\n    \n    L_D = np.array([\n        [-1, 1, 0],\n        [0, -1, 1]\n    ])\n    lambda_D = 1.0\n    \n    X_E = np.array([\n        [1, 0, -1, 2, 0],\n        [0, 1, 1, -1, 2],\n        [2, -1, 0, 1, 1]\n    ])\n    y_E = np.array([[1], [0], [-1]])\n    L_E = np.identity(5)\n    lambda_E = 0.3\n\n    test_cases = [\n        # Case A\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_A},\n        # Case B\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_B},\n        # Case C\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_C},\n        # Case D\n        {\"X\": X_A, \"y\": y_A, \"L\": L_D, \"lam\": lambda_D},\n        # Case E\n        {\"X\": X_E, \"y\": y_E, \"L\": L_E, \"lam\": lambda_E},\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, L, lam = case[\"X\"], case[\"y\"], case[\"L\"], case[\"lam\"]\n        n, p = X.shape\n\n        # 1. Compute the smoother matrix S_lambda\n        XTX = X.T @ X\n        # L may not be square, so L.T @ L is required\n        LTL = L.T @ L\n        A = XTX + lam * LTL\n        \n        A_inv = np.linalg.inv(A)\n        \n        S_lambda = X @ A_inv @ X.T\n        \n        # 2. Compute effective degrees of freedom df_lambda\n        df_lambda = np.trace(S_lambda)\n       \n        # 3. Compute LOOCV_lambda\n        y_hat = S_lambda @ y\n        residuals = y - y_hat\n        S_ii = np.diag(S_lambda)\n        \n        # Reshape S_ii to (n,1) for element-wise division with residuals (n,1)\n        loo_residuals = residuals / (1 - S_ii.reshape(-1, 1))\n        \n        loocv_lambda = np.mean(loo_residuals**2)\n        \n        # 4. Compute GCV_lambda\n        mse = np.mean(residuals**2)\n        \n        denominator = 1 - df_lambda / n\n        if np.isclose(denominator, 0):\n             # Handle potential division by zero if df_lambda is very close to n\n             gcv_lambda = np.inf\n        else:\n             gcv_lambda = mse / (denominator**2)\n\n        # Round results to 8 decimal places (ties to even) and append\n        # np.round uses \"round half to even\", which is the specified method.\n        results.append([\n            np.round(df_lambda, 8),\n            np.round(loocv_lambda, 8),\n            np.round(gcv_lambda, 8)\n        ])\n\n    # Format the output string to match the required format\n    output_parts = []\n    for res in results:\n        # Format each rounded number to a string with 8 decimal places\n        formatted_res = [f\"{num:.8f}\" for num in res]\n        output_parts.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Tikhonov regularization is one of many tools for solving ill-posed inverse problems. This exercise  places ridge regression in a broader context by contrasting its mathematical properties with those of the popular lasso estimator. By applying principles of convex analysis, you will discover why the ridge solution path is smooth and analytic, while the lasso path is piecewise linear, a fundamental distinction that explains their differing abilities to perform variable selection and dictates the design of algorithms to compute them.",
            "id": "3490569",
            "problem": "Consider a linear inverse problem with design matrix $A \\in \\mathbb{R}^{m \\times n}$ and data vector $b \\in \\mathbb{R}^m$. For a parameter $\\lambda  0$, define the Tikhonov (ridge) estimator $\\beta_{\\mathrm{ridge}}(\\lambda,b)$ as any minimizer of the objective $J_{\\mathrm{ridge}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$, and the least absolute shrinkage and selection operator (lasso) estimator $\\beta_{\\mathrm{lasso}}(\\lambda,b)$ as any minimizer of $J_{\\mathrm{lasso}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$. Assume throughout that $\\lambda  0$. When uniqueness is needed for lasso, assume a genericity condition on $A$ (for example, columns in general position) so that for almost every $b$ there is a unique lasso solution for each fixed $\\lambda  0$.\n\nFrom first principles of convex analysis and differentiability, determine which statements about the dependence of these estimators on $b$ and $\\lambda$ are necessarily true. Select all that apply.\n\nA. For every fixed $A$ and every $\\lambda  0$, the map $(b,\\lambda) \\mapsto \\beta_{\\mathrm{ridge}}(\\lambda,b)$ is real-analytic in a neighborhood of any $(b,\\lambda)$, hence depends smoothly on both $b$ and $\\lambda$.\n\nB. If $A$ has full column rank, then for every $\\lambda  0$ the map $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ is everywhere continuously differentiable in $(b,\\lambda)$.\n\nC. Under the stated uniqueness/genericity conditions, for any fixed $b$ the lasso regularization path $\\lambda \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ is piecewise affine in $\\lambda$ on any compact interval, with a finite number of breakpoints at which the active set changes; at these breakpoints the path is generally non-differentiable.\n\nD. Regularization path algorithms such as Least Angle Regression (LARS) and homotopy methods rely on the piecewise linearity of ridge regression to compute the entire ridge path in finitely many linear segments.\n\nE. For ridge, the first-order optimality system can be written as a smooth equation in $(\\beta,b,\\lambda)$ with an invertible Jacobian in $\\beta$ for every $\\lambda  0$, so by the Implicit Function Theorem the solution depends locally smoothly on $(b,\\lambda)$; in particular, continuation in $\\lambda$ can be performed with a well-defined tangent direction at each $\\lambda  0$.\n\nChoose the correct option(s).",
            "solution": "The user wants me to validate and solve a problem concerning the properties of Tikhonov (ridge) and lasso estimators.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A linear inverse problem with a design matrix $A \\in \\mathbb{R}^{m \\times n}$ and a data vector $b \\in \\mathbb{R}^m$.\n-   A regularization parameter $\\lambda  0$.\n-   The Tikhonov (ridge) estimator $\\beta_{\\mathrm{ridge}}(\\lambda,b)$ is a minimizer of $J_{\\mathrm{ridge}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$.\n-   The lasso estimator $\\beta_{\\mathrm{lasso}}(\\lambda,b)$ is a minimizer of $J_{\\mathrm{lasso}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$.\n-   An assumption for lasso uniqueness: a genericity condition on $A$ ensures that for almost every $b$, there is a unique lasso solution for each fixed $\\lambda  0$.\n-   The task is to determine which of the provided statements about the dependence of $\\beta_{\\mathrm{ridge}}$ and $\\beta_{\\mathrm{lasso}}$ on $b$ and $\\lambda$ are necessarily true, based on first principles of convex analysis and differentiability.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is firmly based on the mathematical principles of convex optimization as applied to ridge regression and lasso, two fundamental methods in statistics and machine learning. All definitions and concepts are standard.\n-   **Well-Posed:** The objective functions are well-defined. The ridge objective is strictly convex for $\\lambda  0$, guaranteeing a unique minimizer. The lasso objective is convex, and the problem explicitly addresses the uniqueness of its solution with a standard genericity assumption. The question asks for properties that are \"necessarily true,\" which is a precise and answerable query.\n-   **Objective:** The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n-   **Completeness and Consistency:** The problem is self-contained. All variables and functions are defined, and the necessary assumptions ($\\lambda  0$, uniqueness for lasso) are clearly stated. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is a standard, well-posed problem in optimization and statistical theory. I will proceed to derive the solution.\n\n### Solution Derivation\n\nWe shall analyze the properties of the ridge and lasso estimators based on their respective first-order optimality conditions.\n\n**Ridge Regression Analysis**\nThe ridge objective function is $J_{\\mathrm{ridge}}(\\beta) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$. Since $\\lambda  0$, this function is strictly convex and continuously differentiable ($C^\\infty$) in $\\beta$. A unique minimizer exists and is found by setting the gradient with respect to $\\beta$ to zero:\n$$ \\nabla_\\beta J_{\\mathrm{ridge}} = A^T(A\\beta - b) + \\lambda\\beta = 0 $$\nRearranging this gives the linear system:\n$$ (A^T A + \\lambda I) \\beta = A^T b $$\nThe matrix $A^T A$ is positive semi-definite. For $\\lambda  0$, the matrix $A^T A + \\lambda I$ is positive definite and therefore invertible. This allows us to write the unique solution explicitly:\n$$ \\beta_{\\mathrm{ridge}}(\\lambda,b) = (A^T A + \\lambda I)^{-1} A^T b $$\n\n**Lasso Regression Analysis**\nThe lasso objective function is $J_{\\mathrm{lasso}}(\\beta) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$. This function is convex but not differentiable everywhere due to the $\\ell_1$-norm term. The optimality condition is given by subgradient calculus:\n$$ 0 \\in \\partial_\\beta J_{\\mathrm{lasso}}(\\beta) $$\nThis is equivalent to:\n$$ 0 \\in A^T(A\\beta - b) + \\lambda \\partial\\|\\beta\\|_1 $$\nwhere $\\partial\\|\\beta\\|_1$ is the subdifferential of the $\\ell_1$-norm. This set of conditions, known as the Karush-Kuhn-Tucker (KKT) conditions for the lasso, states that there must exist a subgradient vector $s \\in \\partial\\|\\beta\\|_1$ such that:\n$$ A^T(b - A\\beta) = \\lambda s $$\nThe components of the subgradient vector $s$ satisfy $s_i = \\mathrm{sgn}(\\beta_i)$ if $\\beta_i \\neq 0$ and $s_i \\in [-1, 1]$ if $\\beta_i = 0$.\n\nWith these fundamentals, we now evaluate each option.\n\n---\n**Option A. For every fixed $A$ and every $\\lambda  0$, the map $(b,\\lambda) \\mapsto \\beta_{\\mathrm{ridge}}(\\lambda,b)$ is real-analytic in a neighborhood of any $(b,\\lambda)$, hence depends smoothly on both $b$ and $\\lambda$.**\n\nThe explicit solution is $\\beta_{\\mathrm{ridge}}(\\lambda,b) = (A^T A + \\lambda I)^{-1} A^T b$. Let's examine the dependence on $b$ and $\\lambda$.\n1.  The term $A^T b$ is linear in $b$. A linear map is real-analytic.\n2.  The term $(A^T A + \\lambda I)^{-1}$ depends on $\\lambda$. The entries of the inverse of a matrix $M$ can be expressed using Cramer's rule as the ratio of the adjugate of $M$ to the determinant of $M$.\n    -   Here, $M(\\lambda) = A^T A + \\lambda I$.\n    -   The determinant $\\det(A^T A + \\lambda I)$ is a polynomial in $\\lambda$. For $\\lambda  0$, this determinant is non-zero because the matrix is positive definite.\n    -   The entries of the adjugate matrix $\\text{adj}(A^T A + \\lambda I)$ are also polynomials in $\\lambda$.\n    -   Therefore, each entry of the inverse matrix $(A^T A + \\lambda I)^{-1}$ is a rational function of $\\lambda$ whose denominator is non-zero for all $\\lambda  0$.\nA function that is a rational function in one variable ($\\lambda$) and a linear function in another ($b$) is real-analytic in both variables on its domain of definition. Since a real-analytic function is infinitely differentiable ($C^\\infty$), it is smooth. The statement is therefore correct.\n\n**Verdict: Correct.**\n\n---\n**Option B. If $A$ has full column rank, then for every $\\lambda  0$ the map $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ is everywhere continuously differentiable in $(b,\\lambda)$.**\n\nThe lasso solution is characterized by its KKT conditions. The solution path $\\beta_{\\mathrm{lasso}}(b, \\lambda)$ is known to be non-differentiable at points where the set of active variables (those with non-zero coefficients) changes. For a fixed $b$, as $\\lambda$ varies, coefficients can become zero or non-zero. Similarly, for a fixed $\\lambda$, as $b$ varies, the active set can change.\nLet's consider a point $(\\beta, b, \\lambda)$ satisfying the KKT conditions. If we are in a region where the active set $\\mathcal{A}=\\{i \\mid \\beta_i \\neq 0\\}$ and the signs of the active coefficients are constant, the solution restricted to $\\mathcal{A}$ behaves like an unpenalized least squares problem on the columns $A_{\\mathcal{A}}$ with an offset: $\\beta_{\\mathcal{A}} = (A_{\\mathcal{A}}^T A_{\\mathcal{A}})^{-1} (A_{\\mathcal{A}}^T b - \\lambda s_{\\mathcal{A}})$. This local expression is smooth in $(b, \\lambda)$. However, at the boundary of such a region, where a coefficient $\\beta_i$ hits zero or a subgradient condition $|(A^T(b - A\\beta))_j|$ for an inactive coefficient $j$ hits its bound $\\lambda$, the active set changes. At these \"breakpoints,\" the functional form of the solution changes abruptly. This leads to a \"kink\" in the solution path, where the derivative is discontinuous. Therefore, the map $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ is continuous but not everywhere differentiable. The full column rank of $A$ does not alter this fundamental property, which stems from the non-differentiability of the $\\ell_1$-norm.\n\n**Verdict: Incorrect.**\n\n---\n**Option C. Under the stated uniqueness/genericity conditions, for any fixed $b$ the lasso regularization path $\\lambda \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ is piecewise affine in $\\lambda$ on any compact interval, with a finite number of breakpoints at which the active set changes; at these breakpoints the path is generally non-differentiable.**\n\nThis statement accurately describes a fundamental property of the lasso solution path. As established in the analysis for Option B, between breakpoints (values of $\\lambda$ where the active set changes), the active coefficients are given by $\\beta_{\\mathcal{A}}(\\lambda) = (A_{\\mathcal{A}}^T A_{\\mathcal{A}})^{-1} (A_{\\mathcal{A}}^T b - \\lambda s_{\\mathcal{A}})$. Here, $s_{\\mathcal{A}}$ is the vector of signs of the active coefficients, which is constant between breakpoints.\nThis expression shows that each component $\\beta_i(\\lambda)$ for $i \\in \\mathcal{A}$ is an affine function of $\\lambda$ (of the form $c_1 - c_2\\lambda$). The inactive coefficients $\\beta_j(\\lambda)$ for $j \\notin \\mathcal{A}$ are constant at $0$. Therefore, each component of the solution vector $\\beta(\\lambda)$ is a piecewise affine function of $\\lambda$. The path itself, which is the vector function $\\lambda \\mapsto \\beta(\\lambda)$, is consequently piecewise linear. It can be shown that on any compact interval of $\\lambda  0$, there are only a finite number of such breakpoints. At these points, the derivative $d\\beta/d\\lambda$ generally does not exist, as the active set and the KKT conditions change their structure.\n\n**Verdict: Correct.**\n\n---\n**Option D. Regularization path algorithms such as Least Angle Regression (LARS) and homotopy methods rely on the piecewise linearity of ridge regression to compute the entire ridge path in finitely many linear segments.**\n\nThis statement contains two fundamental errors.\n1.  LARS and related homotopy algorithms are specifically designed to compute the solution path for the *lasso* problem (and its variants), not ridge regression. They exploit the piecewise linear nature of the lasso path described in Option C.\n2.  The ridge regression path, $\\beta_{\\mathrm{ridge}}(\\lambda) = (A^T A + \\lambda I)^{-1} A^T b$, is not piecewise linear. As established in the analysis of Option A, it is a smooth, real-analytic curve for $\\lambda  0$. It is not composed of a finite number of linear segments.\nThe premise of the statement is incorrect on both counts.\n\n**Verdict: Incorrect.**\n\n---\n**Option E. For ridge, the first-order optimality system can be written as a smooth equation in $(\\beta,b,\\lambda)$ with an invertible Jacobian in $\\beta$ for every $\\lambda  0$, so by the Implicit Function Theorem the solution depends locally smoothly on $(b,\\lambda)$; in particular, continuation in $\\lambda$ can be performed with a well-defined tangent direction at each $\\lambda  0$.**\n\nLet us formalize this using the Implicit Function Theorem (IFT). The first-order optimality condition is $F(\\beta, b, \\lambda) = (A^T A + \\lambda I)\\beta - A^T b = 0$.\n1.  The function $F: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}_{0} \\to \\mathbb{R}^n$ is a smooth (in fact, real-analytic) function of its arguments $(\\beta, b, \\lambda)$.\n2.  The IFT requires that the Jacobian of $F$ with respect to $\\beta$ be invertible. Let's compute this Jacobian:\n    $$ D_\\beta F(\\beta, b, \\lambda) = \\frac{\\partial}{\\partial \\beta^T} \\left[ (A^T A + \\lambda I)\\beta - A^T b \\right] = A^T A + \\lambda I $$\n3.  For any $\\lambda  0$, this Jacobian matrix $A^T A + \\lambda I$ is positive definite, and thus invertible.\nThe conditions of the IFT are satisfied at any solution point. The theorem then guarantees the existence of a unique, locally defined smooth function $\\beta(b, \\lambda)$ that solves the equation. The smoothness is of the same order as $F$, which is $C^\\infty$. This confirms that the ridge solution depends smoothly on its parameters.\nThe existence of a smooth path $\\lambda \\mapsto \\beta(\\lambda)$ implies that its derivative, the tangent direction $d\\beta/d\\lambda$, is well-defined for all $\\lambda  0$. Differentiating the identity $(A^T A + \\lambda I)\\beta(\\lambda) = A^T b$ with respect to $\\lambda$ yields:\n$$ I \\cdot \\beta(\\lambda) + (A^T A + \\lambda I) \\frac{d\\beta(\\lambda)}{d\\lambda} = 0 \\implies \\frac{d\\beta(\\lambda)}{d\\lambda} = - (A^T A + \\lambda I)^{-1} \\beta(\\lambda) $$\nThis expression for the tangent vector is well-defined since the inverse exists for all $\\lambda  0$. The statement is a correct application of the IFT.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}