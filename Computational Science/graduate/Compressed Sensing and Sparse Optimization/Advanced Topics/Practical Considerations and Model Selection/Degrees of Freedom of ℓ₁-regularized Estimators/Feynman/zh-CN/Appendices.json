{
    "hands_on_practices": [
        {
            "introduction": "第一个练习是一个基础性训练。我们将手动追踪一个简单Lasso问题的解路径，计算有效预测变量集合如何随着正则化参数 $\\lambda$ 的变化而改变。通过这个动手计算，你将巩固对Karush-Kuhn-Tucker (KKT) 条件的理解，并看到在表现良好的情况下，自由度如何直接对应于有效变量的数量。",
            "id": "3443323",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中有 $n=3$ 个观测值和 $p=2$ 个预测变量，设计矩阵 $X \\in \\mathbb{R}^{3 \\times 2}$ 的列 $x_{1}, x_{2}$ 由下式给出\n$$\nx_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \n\\qquad\nx_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix},\n$$\n响应向量为\n$$\ny = \\begin{pmatrix} 3 \\\\ -\\frac{1}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}.\n$$\n$X$ 的所有列都具有单位欧几里得范数，且 $x_{1}^{\\top} x_{2} = \\frac{1}{2}$。令正则化参数 $\\lambda \\ge 0$ 从 $+\\infty$ 变化到 $0$，并考虑完整的 LASSO 解路径。仅使用凸优化中的第一性原理（特别是 Karush–Kuhn–Tucker (KKT) 条件）和自由度定义为拟合值映射的散度（雅可比矩阵的迹），执行以下操作：\n\n1. 确定活动集首次变为非空的第一个节点 $\\lambda^{(1)}$，以及活动集在 LASSO 路径上接下来发生变化的第二个节点 $\\lambda^{(2)}$。\n\n2. 在每个 $\\lambda$ 的分段上，当活动集及其符号模式保持固定时，计算 LASSO 拟合的自由度 (DoF)，即拟合值关于 $y$ 的导数的迹。确定这些分段的自由度值\n$$\nS_{0}: \\lambda \\ge \\lambda^{(1)}, \n\\qquad\nS_{1}: \\lambda^{(2)}  \\lambda  \\lambda^{(1)}, \n\\qquad\nS_{2}: 0 \\le \\lambda \\le \\lambda^{(2)}.\n$$\n\n将您的最终答案报告为单行矩阵\n$$\n\\big(\\lambda^{(1)}, \\ \\lambda^{(2)}, \\ d_{0}, \\ d_{1}, \\ d_{2}\\big),\n$$\n其中 $d_{i}$ 表示分段 $S_{i}$ 上 $i \\in \\{0,1,2\\}$ 的自由度。提供精确值；无需四舍五入。",
            "solution": "用户想要为一个特定的 LASSO 问题找到节点和自由度。\n\nLASSO 优化问题由下式给出\n$$L(\\beta) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\n其中 $\\beta \\in \\mathbb{R}^{p}$。最优性的 Karush–Kuhn–Tucker (KKT) 条件表明，在解 $\\hat{\\beta}$ 处，零向量必须位于 $L(\\beta)$ 的次梯度中。最小二乘项的梯度是 $\\nabla_{\\beta} \\left(\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2}\\right) = -X^T(y - X\\beta)$。$\\ell_1$-范数惩罚项的次梯度是 $\\lambda \\partial \\|\\beta\\|_{1}$，其中 $\\partial \\|\\beta\\|_{1}$ 的第 $j$ 个分量是一个向量 $s$，满足如果 $\\beta_j \\neq 0$，则 $s_j = \\text{sign}(\\beta_j)$；如果 $\\beta_j = 0$，则 $s_j \\in [-1, 1]$。\n\n因此，KKT 条件为：\n$$-X^T(y - X\\hat{\\beta}) + \\lambda s = 0, \\quad s \\in \\partial\\|\\hat{\\beta}\\|_1$$\n这可以对每个分量 $j \\in \\{1, \\dots, p\\}$ 写成：\n$$x_j^T(y - X\\hat{\\beta}) = \\lambda s_j$$\n其中如果 $\\hat{\\beta}_j \\neq 0$，则 $s_j = \\text{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j=0$，则 $|s_j| \\le 1$。\n这等价于：\n1. 如果 $\\hat{\\beta}_j \\neq 0$，那么 $x_j^T(y - X\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$。\n2. 如果 $\\hat{\\beta}_j = 0$，那么 $|x_j^T(y - X\\hat{\\beta})| \\le \\lambda$。\n\n让我们用给定的数据计算必要的矩阵乘积：\n$X = \\begin{pmatrix} 1  1/2 \\\\ 0  \\sqrt{3}/2 \\\\ 0  0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix}$。\n相关向量 $X^T y$ 是：\n$$X^T y = \\begin{pmatrix} 1  0  0 \\\\ 1/2  \\sqrt{3}/2  0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{3}{2} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$$\n所以，$x_1^T y = 3$ 且 $x_2^T y = 1$。Gram 矩阵 $X^T X$ 是：\n$$X^T X = \\begin{pmatrix} x_1^T x_1  x_1^T x_2 \\\\ x_2^T x_1  x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}$$\n\n**第一部分：确定节点 $\\lambda^{(1)}$ 和 $\\lambda^{(2)}$**\n\n**寻找第一个节点 $\\lambda^{(1)}$：**\n对于非常大的 $\\lambda$，惩罚项占主导地位，迫使解为 $\\hat{\\beta}=0$。对于 $\\hat{\\beta}=0$ 的 KKT 条件简化为对所有 $j \\in \\{1, 2\\}$ 都有 $|x_j^T y| \\le \\lambda$。\n仅当 $\\lambda$ 减小到违反此条件的值时，系数才能变为非零。第一个系数在 $\\lambda = \\max_j |x_j^T y|$ 时变为非零。\n$$\\lambda^{(1)} = \\max(|x_1^T y|, |x_2^T y|) = \\max(3, 1) = 3$$\n第一个进入活动集的变量是对应于最大相关性的变量，即预测变量 1。因此，对于 $\\lambda  \\lambda^{(1)}$，我们有 $\\hat{\\beta}_1 \\neq 0$。\n\n**寻找第二个节点 $\\lambda^{(2)}$：**\n对于分段 $\\lambda  \\lambda^{(1)}$，活动集为 $\\mathcal{A}=\\{1\\}$，意味着 $\\hat{\\beta}_1 \\neq 0$ 且 $\\hat{\\beta}_2=0$。KKT 条件为：\n1. $x_1^T(y - x_1\\hat{\\beta}_1) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$\n2. $|x_2^T(y - x_1\\hat{\\beta}_1)| \\le \\lambda$\n\n根据第一个条件：$x_1^T y - (x_1^T x_1)\\hat{\\beta}_1 = \\lambda s_1$。由于 $\\lambda$ 从 $\\lambda^{(1)}=3$ 开始减小，$\\hat{\\beta}_1$ 将从 $0$ 开始出现。其符号将与 $x_1^T y = 3$ 的符号相同，即正号。所以，$s_1 = \\text{sign}(\\hat{\\beta}_1)=1$。\n$$3 - (1)\\hat{\\beta}_1 = \\lambda \\cdot 1 \\implies \\hat{\\beta}_1 = 3 - \\lambda$$\n这是相符的，因为对于 $\\lambda  3$ 有 $\\hat{\\beta}_1  0$。\n\n第二个节点 $\\lambda^{(2)}$ 发生在第二个 KKT 条件达到其边界时，即预测变量 2 进入模型时。我们必须在 $|x_2^T(y - x_1\\hat{\\beta}_1)| = \\lambda$ 中求解 $\\lambda$。\n令 $C_2(\\lambda) = x_2^T(y - x_1\\hat{\\beta}_1) = x_2^T y - (x_2^T x_1)\\hat{\\beta}_1$。\n使用我们预先计算的值，$x_2^T y = 1$ 和 $x_2^T x_1 = 1/2$：\n$$C_2(\\lambda) = 1 - \\frac{1}{2}(3-\\lambda) = 1 - \\frac{3}{2} + \\frac{\\lambda}{2} = \\frac{\\lambda}{2} - \\frac{1}{2}$$\n条件变为 $|\\frac{\\lambda}{2} - \\frac{1}{2}| = \\lambda$。这给出了两种可能性：\na) $\\frac{\\lambda}{2} - \\frac{1}{2} = \\lambda \\implies -\\frac{\\lambda}{2} = \\frac{1}{2} \\implies \\lambda = -1$，这是无效的，因为 $\\lambda \\ge 0$。\nb) $-(\\frac{\\lambda}{2} - \\frac{1}{2}) = \\lambda \\implies \\frac{1}{2} - \\frac{\\lambda}{2} = \\lambda \\implies \\frac{1}{2} = \\frac{3\\lambda}{2} \\implies \\lambda = \\frac{1}{3}$。\n这个值在区间 $(0, \\lambda^{(1)})$ 内。因此，第二个节点是 $\\lambda^{(2)} = \\frac{1}{3}$。\n\n**解路径分段总结：**\n- 对于 $\\lambda \\ge \\lambda^{(1)} = 3$ ($S_0$)：$\\hat{\\beta}=(0, 0)^T$。活动集 $\\mathcal{A}_0 = \\emptyset$。\n- 对于 $\\lambda^{(2)}  \\lambda  \\lambda^{(1)}$，即 $1/3  \\lambda  3$ ($S_1$)：$\\hat{\\beta}=(3-\\lambda, 0)^T$。活动集 $\\mathcal{A}_1 = \\{1\\}$。\n- 对于 $0 \\le \\lambda \\le \\lambda^{(2)} = 1/3$ ($S_2$)：两个预测变量都在模型中，所以 $\\mathcal{A}_2=\\{1, 2\\}$。KKT 条件构成一个线性系统：$X^T(y - X\\hat{\\beta}) = \\lambda s$。\n$$\\hat{\\beta} = (X^T X)^{-1} (X^T y - \\lambda s)$$\n需要确定符号向量 $s=(s_1, s_2)^T$。对于 $\\beta_1$，和之前一样 $s_1=1$。对于 $\\beta_2$，它进入模型时的符号是相关性 $C_2(\\lambda)$ 在节点 $\\lambda^{(2)}=1/3$ 处的符号。\n$C_2(1/3) = \\frac{1/3}{2} - \\frac{1}{2} = \\frac{1}{6} - \\frac{1}{2} = -\\frac{2}{6} = -\\frac{1}{3}$。符号为负，所以 $s_2=-1$。\n所以，$s=(1, -1)^T$。\n让我们计算 $(X^T X)^{-1}$：\n$$(X^T X)^{-1} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}^{-1} = \\frac{1}{1 - 1/4} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix}$$\n现在我们为这个分段找到 $\\hat{\\beta}$：\n$$\\hat{\\beta} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\left( \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\begin{pmatrix} 3-\\lambda \\\\ 1+\\lambda \\end{pmatrix}$$\n$$\\hat{\\beta}_1 = \\frac{4}{3}(3-\\lambda) - \\frac{2}{3}(1+\\lambda) = \\frac{12-4\\lambda-2-2\\lambda}{3} = \\frac{10-6\\lambda}{3} = \\frac{10}{3} - 2\\lambda$$\n$$\\hat{\\beta}_2 = -\\frac{2}{3}(3-\\lambda) + \\frac{4}{3}(1+\\lambda) = \\frac{-6+2\\lambda+4+4\\lambda}{3} = \\frac{-2+6\\lambda}{3} = 2\\lambda - \\frac{2}{3}$$\n对于 $\\lambda \\in [0, 1/3]$，$\\hat{\\beta}_1 > 0$ 且 $\\hat{\\beta}_2 \\le 0$，这与我们假设的符号 $s=(1, -1)^T$ 一致。\n\n**第二部分：自由度 (DoF) 的计算**\n\n自由度 (DoF) 定义为 $d = \\text{Tr}\\left(\\frac{\\partial \\hat{y}}{\\partial y}\\right)$，其中 $\\hat{y} = X\\hat{\\beta}_\\lambda(y)$ 是拟合值。在节点之间的任何分段上，活动集 $\\mathcal{A}$ 和符号 $s_\\mathcal{A}$ 是固定的。令 $X_\\mathcal{A}$ 是 $X$ 中列属于 $\\mathcal{A}$ 的子矩阵。解由下式给出：\n$$\\hat{\\beta}_\\mathcal{A} = (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$$\n拟合值为 $\\hat{y} = X_\\mathcal{A}\\hat{\\beta}_\\mathcal{A} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$。\n对向量 $y$ 求导：\n$$\\frac{\\partial \\hat{y}}{\\partial y} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T$$\n这是到 $X_\\mathcal{A}$ 列空间上的投影矩阵。DoF 是其迹：\n$$d = \\text{Tr}\\left(X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T\\right) = \\text{Tr}\\left((X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T X_\\mathcal{A}\\right) = \\text{Tr}(I_{|\\mathcal{A}|}) = |\\mathcal{A}|$$\n所以，每个分段上的自由度就是活动集的大小。\n\n-   **分段 $S_0: \\lambda \\ge 3$**：活动集为空，$\\mathcal{A}_0 = \\emptyset$。所以 $|\\mathcal{A}_0|=0$。\n    $$d_0 = 0$$\n    严格来说，$\\hat{\\beta}=0$，所以 $\\hat{y}=0$，而 $\\frac{\\partial \\hat{y}}{\\partial y}$ 是零矩阵，其迹为 $0$。\n\n-   **分段 $S_1: 1/3  \\lambda  3$**：活动集为 $\\mathcal{A}_1 = \\{1\\}$。所以 $|\\mathcal{A}_1|=1$。\n    $$d_1 = 1$$\n    严格来说，$\\frac{\\partial \\hat{y}}{\\partial y} = x_1 (x_1^T x_1)^{-1} x_1^T = x_1 (1)^{-1} x_1^T = x_1 x_1^T$。\n    $d_1 = \\text{Tr}(x_1 x_1^T) = \\text{Tr}(x_1^T x_1) = \\text{Tr}(1) = 1$。\n\n-   **分段 $S_2: 0 \\le \\lambda \\le 1/3$**：活动集为 $\\mathcal{A}_2=\\{1, 2\\}$。所以 $|\\mathcal{A}_2|=2$。\n    $$d_2 = 2$$\n    严格来说，$\\frac{\\partial \\hat{y}}{\\partial y} = X (X^T X)^{-1} X^T$。\n    $d_2 = \\text{Tr}(X (X^T X)^{-1} X^T) = \\text{Tr}((X^T X)^{-1} X^T X) = \\text{Tr}(I_2) = 2$。\n\n最终结果为 $\\lambda^{(1)}=3$，$\\lambda^{(2)}=1/3$，$d_0=0$，$d_1=1$ 和 $d_2=2$。这些结果需要以单行矩阵的形式报告。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3  \\frac{1}{3}  0  1  2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在了解了标准情况下的自由度行为之后，我们现在来探讨一个关键的边界情况：完全共线性。这个练习将揭示一个常见的启发式认知——将自由度等同于有效预测变量的数量——有时是会误导人的。通过分析一个包含相同预测变量的场景，你将发现一个更根本的原则：自由度是由有效预测变量矩阵的秩决定的，而不仅仅是其包含的变量个数。",
            "id": "3443335",
            "problem": "考虑最小绝对收缩和选择算子（LASSO）估计量，形式上即 $\\ell_1$-正则化最小二乘估计量，在线性模型 $y = \\mu + \\varepsilon$ 中，其中 $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ 且 $I_{n}$ 是 $n \\times n$ 的单位矩阵。设 $A \\in \\mathbb{R}^{3 \\times 2}$ 是一个设计矩阵，其两列相同且已归一化，具体为 $a_{1} = a_{2} = a$ 且 $a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$，因此 $\\|a\\|_{2} = 1$。LASSO 估计量定义为\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\},\n$$\n拟合值为 $\\hat{y}(y) = A \\hat{x}(y)$。假设观测数据向量为 $y = 2 a$，正则化参数为 $\\lambda = \\frac{1}{2}$。\n\n仅使用基本定义和最优性条件，完成以下任务：\n- 证明存在一个最优解 $\\hat{x}(y)$，其支撑集 $S = \\{ j : \\hat{x}_{j}(y) \\neq 0 \\}$ 满足 $|S| = 2$，尽管两个活动列是相同的。\n- 使用 Stein 无偏风险框架下的自由度定义，即拟合值映射 $y \\mapsto \\hat{y}(y)$ 在观测值 $y$ 处的散度，计算此设置下 $\\operatorname{df}(\\hat{y})$ 的值。\n\n将最终答案表示为一个实数。无需四舍五入。",
            "solution": "用户希望我解决一个关于 LASSO 估计量的问题，其中设计矩阵包含两个相同且已归一化的列。\n\n### 问题验证\n我将首先验证问题的陈述。\n\n**第一步：提取已知条件**\n-   线性模型：$y = \\mu + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ 且 $n=3$。\n-   设计矩阵：$A \\in \\mathbb{R}^{3 \\times 2}$，其列为 $a_{1} = a_{2} = a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$，且 $\\|a\\|_{2} = 1$。\n-   LASSO 估计量定义：$\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}$。\n-   拟合值定义：$\\hat{y}(y) = A \\hat{x}(y)$。\n-   观测数据向量：$y = 2 a$。\n-   正则化参数：$\\lambda = \\frac{1}{2}$。\n-   自由度定义：$\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}$。\n-   任务：\n    1.  证明存在一个最优解 $\\hat{x}(y)$，其支撑集大小 $|S|=2$。\n    2.  计算给定设置下的 $\\operatorname{df}(\\hat{y})$。\n\n**第二步：使用提取的已知条件进行验证**\n-   **科学依据**：该问题在统计学习和优化理论领域是公认的。LASSO 估计量、其在共线性下的性质，以及通过 Stein 无偏风险估计（SURE）定义的自由度概念都是标准课题。使用相同列的场景是一个合法但退化的情况，用于深入探究该方法的理解。\n-   **适定性**：该问题是适定的。LASSO 目标函数是凸的，保证了最小值的存在。虽然由于 $A$ 的列的共线性，最小化器 $\\hat{x}$ 不是唯一的，但拟合值向量 $\\hat{y} = A\\hat{x}$ 是唯一的。映射 $y \\mapsto \\hat{y}(y)$ 的散度几乎处处有定义，而问题要求计算其在有定义的特定点上的值。\n-   **客观性**：问题使用精确的数学语言陈述，没有主观或模糊的术语。\n-   **缺陷清单**：问题没有违反任何无效性标准。它在科学上是合理的，可形式化的，完整的，并且结构良好。\n\n**第三步：结论与行动**\n问题是**有效的**。我将继续解答。\n\n### 解答\n需要最小化的 LASSO 目标函数是\n$$\nL(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\n其中 $x = (x_1, x_2)^\\top$。已知 $A = [a, a]$，项 $Ax$ 可以写作 $Ax = x_1 a + x_2 a = (x_1+x_2)a$。\n令 $s = x_1+x_2$。目标函数变为\n$$\nL(x) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda (|x_1| + |x_2|)\n$$\n我们需要关于 $x_1$ 和 $x_2$ 最小化此函数。这可以分两步完成。首先，对于固定的和 $s = x_1+x_2$，我们最小化惩罚项 $\\lambda(|x_1| + |x_2|)$。根据三角不等式，有 $|x_1|+|x_2| \\ge |x_1+x_2| = |s|$，当且仅当 $x_1$ 和 $x_2$ 同号（或其中一个或两个为零）时等式成立。因此，对于固定的 $s$，惩罚项的最小值为 $\\lambda|s|$。\n\n问题于是简化为最小化以下关于单变量 $s$ 的函数：\n$$\ng(s) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda|s|\n$$\n展开平方范数，我们得到：\n$$\ng(s) = \\frac{1}{2}(y - sa)^\\top(y - sa) + \\lambda|s| = \\frac{1}{2}(y^\\top y - 2s a^\\top y + s^2 a^\\top a) + \\lambda|s|\n$$\n因为 $\\|a\\|_2 = 1$，我们有 $a^\\top a = 1$。需要最小化的函数是\n$$\ng(s) = \\frac{1}{2}s^2 - (a^\\top y)s + \\frac{1}{2}y^\\top y + \\lambda|s|\n$$\n这是一个一维 LASSO 问题。最小化 $g(s)$ 的解 $\\hat{s}$ 由应用于最小二乘估计 $a^\\top y$ 的软阈值算子给出：\n$$\n\\hat{s}(y) = S_{\\lambda}(a^\\top y) = \\operatorname{sign}(a^\\top y) \\max(|a^\\top y| - \\lambda, 0)\n$$\n对于给定的数据 $y=2a$ 和 $\\lambda = 1/2$，我们首先计算 $a^\\top y$：\n$$\na^\\top y = a^\\top (2a) = 2 a^\\top a = 2(1) = 2\n$$\n将此代入软阈值公式：\n$$\n\\hat{s} = S_{1/2}(2) = \\operatorname{sign}(2) \\max(|2| - 1/2, 0) = 1 \\cdot \\max(3/2, 0) = \\frac{3}{2}\n$$\n原问题的最优解集合 $\\hat{x} = (\\hat{x}_1, \\hat{x}_2)^\\top$ 必须满足两个条件：\n1.  $\\hat{x}_1 + \\hat{x}_2 = \\hat{s} = \\frac{3}{2}$\n2.  $|\\hat{x}_1| + |\\hat{x}_2| = |\\hat{s}| = \\frac{3}{2}$\n\n第二个条件意味着 $\\hat{x}_1$ 和 $\\hat{x}_2$ 必须为非负数，因为它们的和是正数。因此，最优解的集合是 $\\mathbb{R}^2$ 中的线段，由 $\\{(\\hat{x}_1, \\hat{x}_2) \\mid \\hat{x}_1 + \\hat{x}_2 = 3/2, \\hat{x}_1 \\ge 0, \\hat{x}_2 \\ge 0\\}$ 给出。\n\n**第一部分：证明存在一个支撑集大小为 2 的解**\n问题要求证明存在一个支撑集大小 $|S|=2$ 的最优解 $\\hat{x}(y)$。我们可以选择解线段内部的任意一点。例如，令 $\\hat{x}_1 = \\hat{x}_2 = 3/4$。这个选择满足条件：$\\hat{x}_1 + \\hat{x}_2 = 3/4 + 3/4 = 3/2$，并且 $\\hat{x}_1, \\hat{x}_2  0$。此解的支撑集为 $S=\\{1, 2\\}$，其大小为 $|S|=2$。这证明了问题的第一部分。\n\n**第二部分：计算自由度**\n拟合值向量 $\\hat{y}$ 是唯一的，与从解集中选择哪个 $\\hat{x}$ 无关：\n$$\n\\hat{y}(y) = A\\hat{x}(y) = (\\hat{x}_1(y) + \\hat{x}_2(y))a = \\hat{s}(y) a\n$$\n代入 $\\hat{s}(y)$ 的表达式，我们得到映射 $y \\mapsto \\hat{y}(y)$：\n$$\n\\hat{y}(y) = S_{\\lambda}(a^\\top y) a\n$$\n自由度定义为该向量场的散度：\n$$\n\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i}\n$$\n$\\hat{y}$ 的第 $i$ 个分量是 $\\hat{y}_i(y) = S_{\\lambda}(a^\\top y) a_i$。我们计算关于 $y_j$ 的偏导数：\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y) a_i] = a_i \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y)]\n$$\n使用链式法则，令 $u(y) = a^\\top y = \\sum_{k=1}^3 a_k y_k$。则 $\\frac{\\partial u}{\\partial y_j} = a_j$。\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = a_i \\frac{d S_{\\lambda}(u)}{du} \\frac{\\partial u}{\\partial y_j} = a_i a_j \\frac{d S_{\\lambda}(u)}{du}\n$$\n这是映射 $\\hat{y}$ 的雅可比矩阵的第 $(i,j)$ 个元素。散度是该雅可比矩阵的迹：\n$$\n\\operatorname{df}(\\hat{y}) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i} = \\sum_{i=1}^3 a_i^2 \\frac{d S_{\\lambda}(u)}{du} = \\left(\\sum_{i=1}^3 a_i^2\\right) \\frac{d S_{\\lambda}(u)}{du}\n$$\n因为 $\\sum_{i=1}^3 a_i^2 = \\|a\\|_2^2 = 1$，这简化为：\n$$\n\\operatorname{df}(\\hat{y}) = \\frac{d S_{\\lambda}(u)}{du}\n$$\n软阈值函数 $S_{\\lambda}(u)$ 的导数在 $|u| > \\lambda$ 时为 $1$，在 $|u|  \\lambda$ 时为 $0$。在 $|u|=\\lambda$ 时无定义。因此，对于几乎所有的 $y$，导数是示性函数 $\\mathbb{I}(|u|  \\lambda)$。\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(|a^\\top y|  \\lambda)\n$$\n现在我们为问题中给定的特定值计算这个表达式：$y=2a$ 和 $\\lambda=1/2$。\n我们有 $a^\\top y = 2$。我们需要检查是否 $|2|  1/2$。这个条件成立。\n因此，自由度的值为：\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(2  1/2) = 1\n$$\n这个结果值得注意。虽然存在一个支撑集大小为 2 的解 $\\hat{x}$，但拟合 $\\hat{y}$ 的自由度却是 1。这说明了常见的启发式规则 $\\operatorname{df} \\approx |S|$ 在像完全共线性这样的退化情况下可能会产生误导。自由度对应于估计量所选择的子空间的维度，在本例中是由向量 $a$ 张成的一维子空间。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "基于自由度等于有效子矩阵秩的理论洞见，这最后一个练习将理论与实践联系起来。我们将解决在浮点运算和近似共线性存在的情况下计算矩阵秩的挑战。这个问题要求你实现并比较数值稳定的秩估计算法，从而揭示在实践中，自由度的计算是如何依赖于所选择的数值公差的。",
            "id": "3443334",
            "problem": "给定一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和一个与 $\\ell_1$-正则化最小二乘估计量 (Lasso) 相关的活动索引集 $A \\subset \\{0,1,\\dots,p-1\\}$。考虑拟合值映射 $f: \\mathbb{R}^n \\to \\mathbb{R}^n$，其定义为 $f(y) = X \\widehat{\\beta}(y)$，其中 $y \\in \\mathbb{R}^n$ 是数据向量，$\\widehat{\\beta}(y)$ 是以下问题的任意解：\n$$\n\\widehat{\\beta}(y) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\tfrac{1}{2}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\},\n$$\n其中正则化水平 $\\lambda > 0$ 是固定的。$f$ 在点 $y$ 处的自由度可以通过源自 Stein 无偏风险估计 (SURE) 的基于散度的概念来定义：当 $f$ 在 $y$ 处（弱）可微时，自由度由散度给出，\n$$\n\\mathrm{df}(y) \\equiv \\sum_{i=1}^n \\frac{\\partial f_i(y)}{\\partial y_i}.\n$$\n在数据空间的任何区域上，只要活动集 $A$ 和活动系数的符号保持固定，并且定义 $\\widehat{\\beta}(y)$ 的代数约束是正则的，则映射 $f$ 在该区域上是光滑的。你的任务是：\n\n1) 仅从自由度作为拟合值映射的散度的定义以及 $\\ell_1$-正则化最小二乘的基本最优性条件出发，推导为何在活动集 $A$ 和活动系数的符号保持固定且代数正则性成立的任何区域上，自由度等于活动设计子矩阵 $X_A \\in \\mathbb{R}^{n \\times |A|}$ 的代数秩。明确指出需要对 $X_A$ 施加的条件以及 Karush–Kuhn–Tucker (KKT) 系统的结构，以将 $f$ 的雅可比矩阵与到 $X_A$ 列空间的投影联系起来。\n\n2) 提出并实现两种数值稳定的方法来计算代数秩 $\\operatorname{rank}(X_A)$：\n   - 一种基于奇异值分解 (SVD) 的方法。给定一个相对阈值 $\\tau \\in (0,1)$，将数值秩定义为 $X_A$ 的奇异值中严格大于 $\\tau$ 乘以 $X_A$ 最大奇异值的数量。\n   - 一种基于列主元 $QR$ 分解的方法。给定相同的相对阈值 $\\tau$，将数值秩定义为 $R$ 因子对角线元素中绝对值严格大于 $\\tau$ 乘以首个对角线元素大小的数量。\n\n3) 讨论并说明数值阈值 $\\tau$ 的选择如何在实践中影响自由度估计 $\\widehat{\\mathrm{df}} = \\operatorname{rank}(X_A)$，特别是在存在精确或近似多重共线性的情况下。然后，实现一个程序，针对下面的测试套件，为每种情况和每个阈值计算基于 SVD 和基于主元 $QR$ 的数值秩，并按指定格式输出所有结果。\n\n测试套件规范。对于每个测试用例，给定 $X$ 和 $A$。你必须通过提取由 $A$ 索引的 $X$ 的列（使用零基索引）来形成 $X_A$。对所有情况使用阈值集合 $\\mathcal{T} = \\{10^{-12}, 10^{-6}, 10^{-3}, 10^{-1}\\}$。\n\n- 情况 1 (良态满列秩): $n=5$, $p=3$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0 \\\\\n  0  1  0 \\\\\n  0  0  1 \\\\\n  0  0  0 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,2\\}.\n  $$\n- 情况 2 (活动列近似相关): $n=5$, $p=4$, 小参数 $\\varepsilon = 10^{-7}$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0  1 \\\\\n  0  1  0  0 \\\\\n  0  0  1  \\varepsilon \\\\\n  0  0  0  0 \\\\\n  0  0  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,3\\}.\n  $$\n- 情况 3 (活动集中存在精确的重复列): $n=4$, $p=3$,\n  $$\n  X = \\begin{bmatrix}\n  1  1  0 \\\\\n  0  0  1 \\\\\n  0  0  0 \\\\\n  0  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,2\\}.\n  $$\n- 情况 4 (宽活动集，内在秩受 $n$ 限制): $n=3$, $p=5$,\n  $$\n  X = \\begin{bmatrix}\n  1  0  0  1  0 \\\\\n  0  1  0  1  0 \\\\\n  0  0  1  0  0\n  \\end{bmatrix}, \\quad A = \\{0,1,2,3\\}.\n  $$\n\n最终输出格式。对于情况 1、2、3、4 中的每一种，并对于每个按升序排列的阈值 $\\tau \\in \\{10^{-12}, 10^{-6}, 10^{-3}, 10^{-1}\\}$，计算：\n- 基于 SVD 的 $X_A$ 的数值秩。\n- 基于主元 $QR$ 的 $X_A$ 的数值秩。\n\n将所有这些整数按以下确切顺序聚合到一个扁平列表中：\n$$\n\\big[\\; r_{\\mathrm{svd}}(1,10^{-12}),\\; r_{\\mathrm{qr}}(1,10^{-12}),\\; r_{\\mathrm{svd}}(1,10^{-6}),\\; r_{\\mathrm{qr}}(1,10^{-6}),\\; \\dots,\\; r_{\\mathrm{svd}}(4,10^{-1}),\\; r_{\\mathrm{qr}}(4,10^{-1}) \\;\\big].\n$$\n你的程序应生成包含此列表的单行输出，格式为用方括号括起来的逗号分隔序列，例如 $[3,3,3,3,\\dots]$。所有答案都是无单位的整数。不允许用户输入或外部文件；所有数据均如上文测试套件中所规定，嵌入在你的程序中。",
            "solution": "该问题要求推导在特定正则性条件下 Lasso 估计量的自由度，实现两种计算矩阵秩的数值方法，并讨论和应用这些方法到一个给定的测试套件。\n\n### 第 1 部分：自由度的推导\n\n拟合值映射 $f(y)$ 的自由度定义为其散度，$\\mathrm{df}(y) = \\nabla \\cdot f(y) = \\mathrm{tr}(J_f(y))$，其中 $J_f(y)$ 是 $f$ 关于 $y$ 的雅可比矩阵。该映射由 $f(y) = X\\widehat{\\beta}(y)$ 给出，其中 $\\widehat{\\beta}(y)$ 是 Lasso 优化问题的解：\n$$\n\\widehat{\\beta}(y) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\tfrac{1}{2}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\}\n$$\n对于固定的正则化参数 $\\lambda > 0$。\n\n此问题的 Karush-Kuhn-Tucker (KKT) 最优性条件是必要且充分的。目标函数关于 $\\beta$ 的次梯度必须包含零向量：\n$$\n-X^T(y - X\\widehat{\\beta}) + \\lambda z = 0\n$$\n其中 $z$ 是 $\\ell_1$-范数 $\\partial \\lVert \\widehat{\\beta} \\rVert_1$ 次微分中的一个向量。$z$ 的分量由以下公式给出：\n$$\nz_j = \\begin{cases} \\mathrm{sign}(\\widehat{\\beta}_j)  \\text{如果 } \\widehat{\\beta}_j \\neq 0 \\\\ v_j, \\text{ 其中 } |v_j| \\le 1  \\text{如果 } \\widehat{\\beta}_j = 0 \\end{cases}\n$$\n令活动集 $A = \\{j \\in \\{0, \\dots, p-1\\} \\mid \\widehat{\\beta}_j \\neq 0\\}$ 为对应于非零系数的索引集。令 $A^c$ 为其补集，即非活动集。令 $X_A$ 为 $X$ 中由 $A$ 索引的列构成的子矩阵，$\\widehat{\\beta}_A$ 为非零系数向量。根据定义，$\\widehat{\\beta}_{A^c} = 0$。KKT 条件可以根据活动集进行划分：\n1.  对于活动集 $A$：$X_A^T(y - X_A\\widehat{\\beta}_A) = \\lambda s_A$，其中 $s_A$ 是 $\\widehat{\\beta}_A$ 中系数的符号向量。\n2.  对于非活动集 $A^c$：$|X_{A^c}^T(y - X_A\\widehat{\\beta}_A)| \\le \\lambda$（逐元素）。\n\n问题陈述我们处于数据空间的一个区域，其中活动集 $A$ 和符号 $s_A$ 是固定的。第二个 KKT 条件（不等式）被假定为严格成立，意味着没有非活动预测变量即将进入模型。“代数正则性”一词意味着第一个 KKT 条件（即 $\\widehat{\\beta}_A$ 的线性方程组）将 $\\widehat{\\beta}_A$ 定义为 $y$ 的光滑函数。在这些假设下，我们可以分析第一个条件：\n$$\nX_A^T X_A \\widehat{\\beta}_A = X_A^T y - \\lambda s_A\n$$\n拟合值映射为 $f(y) = X\\widehat{\\beta}(y) = X_A\\widehat{\\beta}_A(y)$。为了求其雅可比矩阵，我们对该表达式关于 $y^T$ 求导。$f$ 的雅可比矩阵由 $J_f(y) = \\frac{\\partial f(y)}{\\partial y^T} = X_A \\frac{\\partial \\widehat{\\beta}_A(y)}{\\partial y^T}$ 给出。\n\n为了找到 $\\widehat{\\beta}_A$ 的雅可比矩阵，我们对活动集 KKT 方程关于 $y^T$ 求导。项 $\\lambda s_A$ 在该区域是常数，因此其导数为零。\n$$\n\\frac{\\partial}{\\partial y^T} \\left( X_A^T X_A \\widehat{\\beta}_A \\right) = \\frac{\\partial}{\\partial y^T} \\left( X_A^T y - \\lambda s_A \\right)\n$$\n应用链式法则，我们得到：\n$$\n(X_A^T X_A) \\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = X_A^T \\frac{\\partial y}{\\partial y^T} = X_A^T I_n = X_A^T\n$$\n这是一个关于雅可比矩阵 $\\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T}$ 的线性方程组。形如 $B Z = C$ 的方程的解由 $Z = B^+ C$ 给出，其中 $B^+$ 表示 $B$ 的 Moore-Penrose 伪逆。因此，我们可以写出：\n$$\n\\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = (X_A^T X_A)^+ X_A^T\n$$\n在这里，使用伪逆至关重要，因为它即使在矩阵 $X_A^T X_A$ 是奇异的情况下（即 $X_A$ 的列线性相关时）也能提供解。\n\n现在，我们将其代入拟合值映射的雅可比矩阵表达式中：\n$$\nJ_f(y) = X_A \\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = X_A (X_A^T X_A)^+ X_A^T\n$$\n矩阵 $P_A = X_A (X_A^T X_A)^+ X_A^T$ 是到 $X_A$ 的列空间（表示为 $\\mathcal{C}(X_A)$）上的正交投影矩阵。这是线性代数中的一个标准结果。\n\n自由度被定义为该雅可比矩阵的迹：\n$$\n\\mathrm{df}(y) = \\mathrm{tr}(J_f(y)) = \\mathrm{tr}(P_A)\n$$\n任何投影矩阵的一个基本性质是其迹等于它所投影到的子空间的维度。在这种情况下，子空间 $\\mathcal{C}(X_A)$ 的维度是矩阵 $X_A$ 的秩。因此，\n$$\n\\mathrm{df}(y) = \\operatorname{rank}(P_A) = \\dim(\\mathcal{C}(X_A)) = \\operatorname{rank}(X_A)\n$$\n此推导表明，在活动集和符号固定（意味着估计量的局部线性）的所述条件下，Lasso 拟合的自由度就是活动设计子矩阵 $X_A$ 中线性无关列的数量。所需的关键条件是 KKT 系统的可微性（由“代数正则性”假设保证），以及将拟合映射的雅可比矩阵识别为到 $\\mathcal{C}(X_A)$ 上的投影算子。\n\n### 第 2 部分：数值秩的计算\n\n在浮点运算中，代数秩通常是一个不稳定的量。近似共线性可以使一个矩阵在代数上是满秩的，但在实际的数值意义上是秩亏的。我们被要求实现两种计算数值秩的方法。\n\n1.  **基于奇异值分解 (SVD) 的秩**：\n    $X_A \\in \\mathbb{R}^{n \\times |A|}$ 的 SVD 是 $X_A = U \\Sigma V^T$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{|A| \\times |A|}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times |A|}$ 是一个对角线上为非负实数的矩形对角矩阵，这些数被称为奇异值，按非递增顺序排列：$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$，其中 $k=\\min(n, |A|)$。代数秩是非零奇异值的数量。给定一个相对阈值 $\\tau \\in (0,1)$，数值秩被定义为显著大于零的奇异值的数量，相对于最大奇异值而言：\n    $$\n    \\operatorname{rank}_{\\mathrm{svd}}(X_A, \\tau) = \\left| \\left\\{ i \\mid \\sigma_i  \\tau \\cdot \\sigma_1 \\right\\} \\right|\n    $$\n    这是确定数值秩最可靠的方法。\n\n2.  **基于列主元 QR 分解的秩**：\n    $X_A$ 的列主元 QR 分解是 $X_A P = Q R$，其中 $P$ 是一个置换矩阵，$Q$ 是一个正交矩阵，$R$ 是一个上梯形矩阵，其对角线元素的绝对值递减：$|R_{11}| \\ge |R_{22}| \\ge \\dots \\ge |R_{kk}| \\ge 0$。置换矩阵 $P$ 是通过贪心策略选择的，以在每一步最大化对角线元素的绝对值。如果 $X_A$ 是秩亏的，$R$ 的某些对角线元素将为零。与 SVD 方法类似，我们可以通过对 $R$ 的对角线元素相对于最大值进行阈值处理来定义数值秩：\n    $$\n    \\operatorname{rank}_{\\mathrm{qr}}(X_A, \\tau) = \\left| \\left\\{ i \\mid |R_{ii}|  \\tau \\cdot |R_{11}| \\right\\} \\right|\n    $$\n    这种方法通常比 SVD 快，但对于某些病态矩阵可能不太可靠，尽管在大多数实际情况中表现良好。\n\n### 第 3 部分：阈值的影响与应用\n\n阈值 $\\tau$ 的选择至关重要且依赖于具体情况。它定义了什么被认为是“数值上为零”。\n\n-   一个非常小的阈值（例如 $\\tau = 10^{-12}$ 或接近机器精度）会使秩的计算变得敏感。它可能会计入那些近似但非精确线性相关的列，从而可能高估矩阵的“有效”秩。这在情况 2 中可以看到，其中两列由一个范数为 $\\varepsilon = 10^{-7}$ 的向量分隔。阈值 $\\tau = 10^{-12}$ 远小于这个差值，因此 SVD 和 QR 方法将正确地识别出 3 个线性无关的列。\n\n-   一个较大的阈值（例如 $\\tau = 10^{-6}$）则充当一个更粗略的过滤器。对于情况 2，由于 $\\varepsilon=10^{-7}$ 小于 $\\tau=10^{-6}$（相对于最大奇异值/对角元素），两列之间的近似相关性将被检测到，数值秩将被报告为 2。这说明了 $\\tau$ 如何将近似共线性的概念操作化。\n\n-   在精确共线性的情况下（情况 3，重复列）或良态矩阵（情况 1），秩是明确的。SVD 将产生一个真正为零（或在机器精度量级）的奇异值，而主元 QR 将在 $R$ 的对角线上产生一个零。任何合理的正阈值 $\\tau$ 都能正确识别秩亏，得出秩为 2。\n\n-   在情况 4 中，列数 $|A|=4$ 大于行数 $n=3$，因此代数秩不能超过 3。由于前三列是线性无关的，所以秩恰好为 3。这是一个结构性质，而非数值性质，两种方法对于所有指定的阈值都应稳健地报告秩为 3。\n\n$\\tau$ 的选择反映了一种权衡：小的 $\\tau$ 紧密遵循秩的代数定义，而大的 $\\tau$ 提供了一个更稳定、更“有效”的秩，它忽略了近似共线数列的影响，这在统计建模中通常是可取的，以避免方差膨胀。下面的程序为给定的测试套件计算这些数值秩。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Computes numerical ranks for a suite of test cases using SVD and pivoted QR methods.\n    \"\"\"\n    # Define the set of relative thresholds for rank determination.\n    thresholds = [1e-12, 1e-6, 1e-3, 1e-1]\n\n    # Test cases specification: (matrix X, active set A)\n    # Case 1: Well-conditioned, full column rank\n    X1 = np.array([\n        [1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]\n    ])\n    A1 = [0, 1, 2]\n\n    # Case 2: Nearly dependent active columns\n    eps = 1e-7\n    X2 = np.array([\n        [1., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., eps],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]\n    ])\n    A2 = [0, 1, 3]\n\n    # Case 3: Exact duplicate column in the active set\n    X3 = np.array([\n        [1., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]\n    ])\n    A3 = [0, 1, 2]\n\n    # Case 4: Wide active set with rank limited by n\n    X4 = np.array([\n        [1., 0., 0., 1., 0.],\n        [0., 1., 0., 1., 0.],\n        [0., 0., 1., 0., 0.]\n    ])\n    A4 = [0, 1, 2, 3]\n    \n    test_cases = [\n        (X1, A1),\n        (X2, A2),\n        (X3, A3),\n        (X4, A4)\n    ]\n\n    # List to store the final sequence of results.\n    results = []\n\n    # Iterate over each test case.\n    for X, A in test_cases:\n        # Form the active submatrix X_A.\n        # Ensure that A is a list or tuple of indices, not a set, for correct slicing.\n        X_A = X[:, list(A)]\n\n        # Handle the case where X_A has no columns.\n        if X_A.shape[1] == 0:\n            for _ in thresholds:\n                results.extend([0, 0])\n            continue\n            \n        # Iterate over each threshold.\n        for tau in thresholds:\n            # --- SVD-based numerical rank ---\n            try:\n                s = np.linalg.svd(X_A, compute_uv=False)\n                # If s is empty, rank is 0. Otherwise, compute rank.\n                if s.size == 0:\n                    rank_svd = 0\n                else:\n                    s_max = s[0]\n                    # Check for s_max == 0 to avoid division by zero if X_A is a zero matrix.\n                    if s_max  0:\n                        rank_svd = np.sum(s  tau * s_max)\n                    else:\n                        rank_svd = 0\n            except np.linalg.LinAlgError:\n                rank_svd = 0 # Should not happen with compute_uv=False\n\n            results.append(int(rank_svd))\n\n            # --- Pivoted QR-based numerical rank ---\n            try:\n                # Use scipy.linalg.qr with column pivoting.\n                Q, R, P = qr(X_A, pivoting=True)\n                r_diag_abs = np.abs(np.diag(R))\n                \n                # If r_diag_abs is empty, rank is 0. Otherwise compute rank.\n                if r_diag_abs.size == 0:\n                    rank_qr = 0\n                else:\n                    r_max = r_diag_abs[0]\n                    # Check for r_max == 0 to avoid division by zero.\n                    if r_max  0:\n                        rank_qr = np.sum(r_diag_abs  tau * r_max)\n                    else:\n                        rank_qr = 0\n            except np.linalg.LinAlgError:\n                rank_qr = 0\n\n            results.append(int(rank_qr))\n\n    # Print the final aggregated list in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}