{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, our first practice involves manually tracing the solution path of a simple Lasso problem. By applying the Karush-Kuhn-Tucker (KKT) optimality conditions, you will determine the precise values of the regularization parameter $\\lambda$ where variables enter the model. This exercise provides a concrete understanding of how the Lasso performs variable selection and establishes the baseline intuition that, in well-behaved scenarios, the degrees of freedom equal the number of active predictors in the model .",
            "id": "3443323",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwith $n=3$ observations and $p=2$ predictors, where the design matrix $X \\in \\mathbb{R}^{3 \\times 2}$ has columns $x_{1}, x_{2}$ given by\n$$\nx_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \n\\qquad\nx_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix},\n$$\nand the response vector is\n$$\ny = \\begin{pmatrix} 3 \\\\ -\\frac{1}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}.\n$$\nAll columns of $X$ have unit Euclidean norm and $x_{1}^{\\top} x_{2} = \\frac{1}{2}$. Let the regularization parameter $\\lambda \\ge 0$ vary from $+\\infty$ down to $0$, and consider the full LASSO solution path. Using only first principles in convex optimization (specifically the Karush–Kuhn–Tucker (KKT) conditions) and the definition of degrees of freedom as the divergence (trace of the Jacobian) of the fitted values mapping, perform the following:\n\n1. Determine the first knot $\\lambda^{(1)}$ at which the active set becomes nonempty, and the second knot $\\lambda^{(2)}$ at which the active set changes next along the LASSO path.\n\n2. On each segment of $\\lambda$ where the active set and its sign pattern remain fixed, compute the degrees of freedom (DoF) of the LASSO fit as the trace of the derivative of the fitted values with respect to $y$. Identify these values for the segments \n$$\nS_{0}: \\ \\lambda \\ge \\lambda^{(1)}, \n\\qquad\nS_{1}: \\ \\lambda^{(2)} < \\lambda < \\lambda^{(1)}, \n\\qquad\nS_{2}: \\ 0 \\le \\lambda < \\lambda^{(2)}.\n$$\n\nReport your final answer as a single row matrix \n$$\n\\big(\\lambda^{(1)}, \\ \\lambda^{(2)}, \\ d_{0}, \\ d_{1}, \\ d_{2}\\big),\n$$\nwhere $d_{i}$ denotes the degrees of freedom on segment $S_{i}$ for $i \\in \\{0,1,2\\}$. Provide exact values; no rounding is required.",
            "solution": "The user wants to find the knots and degrees of freedom for a specific LASSO problem.\n\nThe LASSO optimization problem is given by\n$$L(\\beta) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\nwhere $\\beta \\in \\mathbb{R}^{p}$. The Karush–Kuhn–Tucker (KKT) conditions for optimality state that the zero vector must be in the subgradient of $L(\\beta)$ at the solution $\\hat{\\beta}$. The gradient of the least-squares term is $\\nabla_{\\beta} \\left(\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2}\\right) = -X^T(y - X\\beta)$. The subgradient of the $\\ell_1$-norm penalty term is $\\lambda \\partial \\|\\beta\\|_{1}$, where the $j$-th component of $\\partial \\|\\beta\\|_{1}$ is a vector $s$ such that $s_j = \\text{sign}(\\beta_j)$ if $\\beta_j \\neq 0$ and $s_j \\in [-1, 1]$ if $\\beta_j = 0$.\n\nThe KKT conditions are therefore:\n$$-X^T(y - X\\hat{\\beta}) + \\lambda s = 0, \\quad s \\in \\partial\\|\\hat{\\beta}\\|_1$$\nThis can be written for each component $j \\in \\{1, \\dots, p\\}$:\n$$x_j^T(y - X\\hat{\\beta}) = \\lambda s_j$$\nwhere $s_j = \\text{sign}(\\hat{\\beta}_j)$ if $\\hat{\\beta}_j \\neq 0$ and $|s_j| \\le 1$ if $\\hat{\\beta}_j=0$.\nThis is equivalent to:\n1. If $\\hat{\\beta}_j \\neq 0$, then $x_j^T(y - X\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$.\n2. If $\\hat{\\beta}_j = 0$, then $|x_j^T(y - X\\hat{\\beta})| \\le \\lambda$.\n\nLet's compute the necessary matrix products with the given data:\n$X = \\begin{pmatrix} 1 & 1/2 \\\\ 0 & \\sqrt{3}/2 \\\\ 0 & 0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix}$.\nThe correlation vector $X^T y$ is:\n$$X^T y = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1/2 & \\sqrt{3}/2 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -1/\\sqrt{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{3}{2} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$$\nSo, $x_1^T y = 3$ and $x_2^T y = 1$. The Gram matrix $X^T X$ is:\n$$X^T X = \\begin{pmatrix} x_1^T x_1 & x_1^T x_2 \\\\ x_2^T x_1 & x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix}$$\n\n**Part 1: Determination of the knots $\\lambda^{(1)}$ and $\\lambda^{(2)}$**\n\n**Finding the first knot $\\lambda^{(1)}$:**\nFor very large $\\lambda$, the penalty term dominates, forcing the solution to be $\\hat{\\beta}=0$. The KKT conditions for $\\hat{\\beta}=0$ simplify to $|x_j^T y| \\le \\lambda$ for all $j \\in \\{1, 2\\}$.\nA coefficient can become non-zero only when $\\lambda$ decreases to a value where this condition is violated. The first coefficient becomes non-zero at $\\lambda = \\max_j |x_j^T y|$.\n$$\\lambda^{(1)} = \\max(|x_1^T y|, |x_2^T y|) = \\max(3, 1) = 3$$\nThe first variable to enter the active set is the one corresponding to the maximum correlation, which is predictor $1$. So, for $\\lambda < \\lambda^{(1)}$, we have $\\hat{\\beta}_1 \\neq 0$.\n\n**Finding the second knot $\\lambda^{(2)}$:**\nFor the segment $\\lambda < \\lambda^{(1)}$, the active set is $\\mathcal{A}=\\{1\\}$, meaning $\\hat{\\beta}_1 \\neq 0$ and $\\hat{\\beta}_2=0$. The KKT conditions are:\n1. $x_1^T(y - x_1\\hat{\\beta}_1) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$\n2. $|x_2^T(y - x_1\\hat{\\beta}_1)| \\le \\lambda$\n\nFrom the first condition: $x_1^T y - (x_1^T x_1)\\hat{\\beta}_1 = \\lambda s_1$. Since $\\lambda$ decreases from $\\lambda^{(1)}=3$, $\\hat{\\beta}_1$ will emerge from $0$. Its sign will be that of $x_1^T y = 3$, which is positive. So, $s_1 = \\text{sign}(\\hat{\\beta}_1)=1$.\n$$3 - (1)\\hat{\\beta}_1 = \\lambda \\cdot 1 \\implies \\hat{\\beta}_1 = 3 - \\lambda$$\nThis is consistent, as $\\hat{\\beta}_1 > 0$ for $\\lambda < 3$.\n\nThe second knot $\\lambda^{(2)}$ occurs when the second KKT condition meets its boundary, i.e., when predictor $2$ enters the model. We must solve for $\\lambda$ in $|x_2^T(y - x_1\\hat{\\beta}_1)| = \\lambda$.\nLet $C_2(\\lambda) = x_2^T(y - x_1\\hat{\\beta}_1) = x_2^T y - (x_2^T x_1)\\hat{\\beta}_1$.\nUsing our pre-computed values, $x_2^T y = 1$ and $x_2^T x_1 = 1/2$:\n$$C_2(\\lambda) = 1 - \\frac{1}{2}(3-\\lambda) = 1 - \\frac{3}{2} + \\frac{\\lambda}{2} = \\frac{\\lambda}{2} - \\frac{1}{2}$$\nThe condition becomes $|\\frac{\\lambda}{2} - \\frac{1}{2}| = \\lambda$. This gives two possibilities:\na) $\\frac{\\lambda}{2} - \\frac{1}{2} = \\lambda \\implies -\\frac{\\lambda}{2} = \\frac{1}{2} \\implies \\lambda = -1$, which is not valid as $\\lambda \\ge 0$.\nb) $-(\\frac{\\lambda}{2} - \\frac{1}{2}) = \\lambda \\implies \\frac{1}{2} - \\frac{\\lambda}{2} = \\lambda \\implies \\frac{1}{2} = \\frac{3\\lambda}{2} \\implies \\lambda = \\frac{1}{3}$.\nThis value is in the interval $(0, \\lambda^{(1)})$. Thus, the second knot is $\\lambda^{(2)} = \\frac{1}{3}$.\n\n**Summary of solution path segments:**\n- For $\\lambda \\ge \\lambda^{(1)} = 3$ ($S_0$): $\\hat{\\beta}=(0, 0)^T$. Active set $\\mathcal{A}_0 = \\emptyset$.\n- For $\\lambda^{(2)} < \\lambda < \\lambda^{(1)}$, i.e., $1/3 < \\lambda < 3$ ($S_1$): $\\hat{\\beta}=(3-\\lambda, 0)^T$. Active set $\\mathcal{A}_1 = \\{1\\}$.\n- For $0 \\le \\lambda < \\lambda^{(2)} = 1/3$ ($S_2$): Both predictors are in the model, so $\\mathcal{A}_2=\\{1, 2\\}$. The KKT conditions form a linear system: $X^T(y - X\\hat{\\beta}) = \\lambda s$.\n$$\\hat{\\beta} = (X^T X)^{-1} (X^T y - \\lambda s)$$\nThe sign vector $s=(s_1, s_2)^T$ needs to be determined. For $\\beta_1$, $s_1=1$ as before. For $\\beta_2$, its sign upon entry is the sign of the correlation $C_2(\\lambda)$ at the knot $\\lambda^{(2)}=1/3$.\n$C_2(1/3) = \\frac{1/3}{2} - \\frac{1}{2} = \\frac{1}{6} - \\frac{1}{2} = -\\frac{2}{6} = -\\frac{1}{3}$. The sign is negative, so $s_2=-1$.\nSo, $s=(1, -1)^T$.\nLet's compute $(X^T X)^{-1}$:\n$$(X^T X)^{-1} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix}^{-1} = \\frac{1}{1 - 1/4} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix}$$\nNow we find $\\hat{\\beta}$ for this segment:\n$$\\hat{\\beta} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix} \\left( \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right) = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix} \\begin{pmatrix} 3-\\lambda \\\\ 1+\\lambda \\end{pmatrix}$$\n$$\\hat{\\beta}_1 = \\frac{4}{3}(3-\\lambda) - \\frac{2}{3}(1+\\lambda) = \\frac{12-4\\lambda-2-2\\lambda}{3} = \\frac{10-6\\lambda}{3} = \\frac{10}{3} - 2\\lambda$$\n$$\\hat{\\beta}_2 = -\\frac{2}{3}(3-\\lambda) + \\frac{4}{3}(1+\\lambda) = \\frac{-6+2\\lambda+4+4\\lambda}{3} = \\frac{-2+6\\lambda}{3} = 2\\lambda - \\frac{2}{3}$$\nFor $\\lambda \\in [0, 1/3)$, $\\hat{\\beta}_1 > 0$ and $\\hat{\\beta}_2 < 0$, which is consistent with our assumed signs $s=(1, -1)^T$.\n\n**Part 2: Computation of the Degrees of Freedom (DoF)**\n\nThe Degrees of Freedom (DoF) is defined as $d = \\text{Tr}\\left(\\frac{\\partial \\hat{y}}{\\partial y}\\right)$, where $\\hat{y} = X\\hat{\\beta}_\\lambda(y)$ are the fitted values. On any segment between knots, the active set $\\mathcal{A}$ and the signs $s_\\mathcal{A}$ are fixed. Let $X_\\mathcal{A}$ be the submatrix of $X$ with columns in $\\mathcal{A}$. The solution is given by:\n$$\\hat{\\beta}_\\mathcal{A} = (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$$\nThe fitted values are $\\hat{y} = X_\\mathcal{A}\\hat{\\beta}_\\mathcal{A} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} (X_\\mathcal{A}^T y - \\lambda s_\\mathcal{A})$.\nDifferentiating with respect to the vector $y$:\n$$\\frac{\\partial \\hat{y}}{\\partial y} = X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T$$\nThis is the projection matrix onto the column space of $X_\\mathcal{A}$. The DoF is its trace:\n$$d = \\text{Tr}\\left(X_\\mathcal{A} (X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T\\right) = \\text{Tr}\\left((X_\\mathcal{A}^T X_\\mathcal{A})^{-1} X_\\mathcal{A}^T X_\\mathcal{A}\\right) = \\text{Tr}(I_{|\\mathcal{A}|}) = |\\mathcal{A}|$$\nSo, the degrees of freedom on each segment is simply the size of the active set.\n\n-   **Segment $S_0: \\lambda \\ge 3$**: The active set is empty, $\\mathcal{A}_0 = \\emptyset$. So $|\\mathcal{A}_0|=0$.\n    $$d_0 = 0$$\n    Rigorously, $\\hat{\\beta}=0$, so $\\hat{y}=0$, and $\\frac{\\partial \\hat{y}}{\\partial y}$ is the zero matrix, whose trace is $0$.\n\n-   **Segment $S_1: 1/3 < \\lambda < 3$**: The active set is $\\mathcal{A}_1 = \\{1\\}$. So $|\\mathcal{A}_1|=1$.\n    $$d_1 = 1$$\n    Rigorously, $\\frac{\\partial \\hat{y}}{\\partial y} = x_1 (x_1^T x_1)^{-1} x_1^T = x_1 (1)^{-1} x_1^T = x_1 x_1^T$.\n    $d_1 = \\text{Tr}(x_1 x_1^T) = \\text{Tr}(x_1^T x_1) = \\text{Tr}(1) = 1$.\n\n-   **Segment $S_2: 0 \\le \\lambda < 1/3$**: The active set is $\\mathcal{A}_2=\\{1, 2\\}$. So $|\\mathcal{A}_2|=2$.\n    $$d_2 = 2$$\n    Rigorously, $\\frac{\\partial \\hat{y}}{\\partial y} = X (X^T X)^{-1} X^T$.\n    $d_2 = \\text{Tr}(X (X^T X)^{-1} X^T) = \\text{Tr}((X^T X)^{-1} X^T X) = \\text{Tr}(I_2) = 2$.\n\nThe final results are $\\lambda^{(1)}=3$, $\\lambda^{(2)}=1/3$, $d_0=0$, $d_1=1$, and $d_2=2$. These are to be reported in a single row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & \\frac{1}{3} & 0 & 1 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Next, we investigate a critical edge case that reveals a deeper truth about degrees of freedom. This problem explores a scenario with perfectly collinear predictors, where the common heuristic that degrees of freedom equal the active set size breaks down. By analyzing this degenerate case, you will discover that the degrees of freedom are more fundamentally defined by the rank of the active design submatrix, $\\operatorname{rank}(A_S)$, which measures the dimensionality of the subspace spanned by the active predictors .",
            "id": "3443335",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator, formally the $\\ell_1$-regularized least squares estimator, in the linear model $y = \\mu + \\varepsilon$ where $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ and $I_{n}$ is the $n \\times n$ identity matrix. Let $A \\in \\mathbb{R}^{3 \\times 2}$ be a design matrix whose two columns are identical and normalized, specifically $a_{1} = a_{2} = a$ with $a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$ so that $\\|a\\|_{2} = 1$. The LASSO estimator is defined by\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\},\n$$\nand the fitted values are $\\hat{y}(y) = A \\hat{x}(y)$. Assume the observed data vector is $y = 2 a$ and the regularization parameter is $\\lambda = \\frac{1}{2}$.\n\nUsing only fundamental definitions and optimality conditions, do the following:\n- Establish that there exists an optimal solution $\\hat{x}(y)$ whose support $S = \\{ j : \\hat{x}_{j}(y) \\neq 0 \\}$ satisfies $|S| = 2$ even though the two active columns are identical.\n- Using the definition of degrees of freedom via Stein’s unbiased risk framework, namely the divergence of the fitted value mapping $y \\mapsto \\hat{y}(y)$ at the observed $y$, compute the value of $\\operatorname{df}(\\hat{y})$ for this setup.\n\nExpress the final answer as a single real number. No rounding is required.",
            "solution": "The user wants me to solve a problem about the LASSO estimator with two identical, normalized columns in the design matrix.\n\n### Problem Validation\nI will first validate the problem statement.\n\n**Step 1: Extract Givens**\n-   Linear model: $y = \\mu + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, I_{n})$ and $n=3$.\n-   Design matrix: $A \\in \\mathbb{R}^{3 \\times 2}$ with columns $a_{1} = a_{2} = a = \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$, and $\\|a\\|_{2} = 1$.\n-   LASSO estimator definition: $\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}$.\n-   Fitted values definition: $\\hat{y}(y) = A \\hat{x}(y)$.\n-   Observed data vector: $y = 2 a$.\n-   Regularization parameter: $\\lambda = \\frac{1}{2}$.\n-   Degrees of freedom definition: $\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^n \\frac{\\partial \\hat{y}_i}{\\partial y_i}$.\n-   Tasks:\n    1.  Show that an optimal solution $\\hat{x}(y)$ exists with support size $|S|=2$.\n    2.  Compute $\\operatorname{df}(\\hat{y})$ for the given setup.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-established in the field of statistical learning and optimization theory. The LASSO estimator, its properties under collinearity, and the concept of degrees of freedom via Stein's Unbiased Risk Estimate (SURE) are standard topics. The scenario with identical columns is a legitimate, albeit degenerate, case used to probe a deeper understanding of the method.\n-   **Well-Posed**: The problem is well-posed. The LASSO objective function is convex, guaranteeing the existence of a minimum. While the minimizer $\\hat{x}$ is not unique due to the collinearity of the columns of $A$, the fitted value vector $\\hat{y} = A\\hat{x}$ is unique. The divergence of the mapping $y \\mapsto \\hat{y}(y)$ is well-defined almost everywhere, and the problem asks for its value at a specific point where it is defined.\n-   **Objective**: The problem is stated using precise mathematical language, with no subjective or ambiguous terms.\n-   **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, and well-structured.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. I will proceed with the solution.\n\n### Solution\nThe LASSO objective function to be minimized is\n$$\nL(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nwhere $x = (x_1, x_2)^\\top$. Given that $A = [a, a]$, the term $Ax$ can be written as $Ax = x_1 a + x_2 a = (x_1+x_2)a$.\nLet $s = x_1+x_2$. The objective function becomes\n$$\nL(x) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda (|x_1| + |x_2|)\n$$\nWe need to minimize this function with respect to $x_1$ and $x_2$. This can be done in two stages. First, for a fixed sum $s = x_1+x_2$, we minimize the penalty term $\\lambda(|x_1| + |x_2|)$. By the triangle inequality, $|x_1|+|x_2| \\ge |x_1+x_2| = |s|$, with equality holding if and only if $x_1$ and $x_2$ have the same sign (or one or both are zero). Thus, the minimum value of the penalty term for a fixed $s$ is $\\lambda|s|$.\n\nThe problem then reduces to minimizing the following function of a single variable $s$:\n$$\ng(s) = \\frac{1}{2}\\|y - s a\\|_{2}^{2} + \\lambda|s|\n$$\nExpanding the squared norm, we get:\n$$\ng(s) = \\frac{1}{2}(y - sa)^\\top(y - sa) + \\lambda|s| = \\frac{1}{2}(y^\\top y - 2s a^\\top y + s^2 a^\\top a) + \\lambda|s|\n$$\nSince $\\|a\\|_2 = 1$, we have $a^\\top a = 1$. The function to minimize is\n$$\ng(s) = \\frac{1}{2}s^2 - (a^\\top y)s + \\frac{1}{2}y^\\top y + \\lambda|s|\n$$\nThis is a one-dimensional LASSO problem. The solution $\\hat{s}$ that minimizes $g(s)$ is given by the soft-thresholding operator applied to the least-squares estimate $a^\\top y$:\n$$\n\\hat{s}(y) = S_{\\lambda}(a^\\top y) = \\operatorname{sign}(a^\\top y) \\max(|a^\\top y| - \\lambda, 0)\n$$\nFor the given data $y=2a$ and $\\lambda = 1/2$, we first compute $a^\\top y$:\n$$\na^\\top y = a^\\top (2a) = 2 a^\\top a = 2(1) = 2\n$$\nPlugging this into the soft-thresholding formula:\n$$\n\\hat{s} = S_{1/2}(2) = \\operatorname{sign}(2) \\max(|2| - 1/2, 0) = 1 \\cdot \\max(3/2, 0) = \\frac{3}{2}\n$$\nThe set of optimal solutions $\\hat{x} = (\\hat{x}_1, \\hat{x}_2)^\\top$ for the original problem must satisfy two conditions:\n1.  $\\hat{x}_1 + \\hat{x}_2 = \\hat{s} = \\frac{3}{2}$\n2.  $|\\hat{x}_1| + |\\hat{x}_2| = |\\hat{s}| = \\frac{3}{2}$\n\nThe second condition implies that $\\hat{x}_1$ and $\\hat{x}_2$ must be non-negative, since their sum is positive. Thus, the set of optimal solutions is the line segment in $\\mathbb{R}^2$ given by $\\{(\\hat{x}_1, \\hat{x}_2) \\mid \\hat{x}_1 + \\hat{x}_2 = 3/2, \\hat{x}_1 \\ge 0, \\hat{x}_2 \\ge 0\\}$.\n\n**Part 1: Existence of a solution with support size 2**\nThe problem asks to show that an optimal solution $\\hat{x}(y)$ exists with support size $|S|=2$. We can choose any point in the interior of the solution segment. For example, let $\\hat{x}_1 = \\hat{x}_2 = 3/4$. This choice satisfies the conditions: $\\hat{x}_1 + \\hat{x}_2 = 3/4 + 3/4 = 3/2$, and $\\hat{x}_1, \\hat{x}_2 > 0$. The support for this solution is $S=\\{1, 2\\}$, and its size is $|S|=2$. This establishes the first part of the problem.\n\n**Part 2: Computation of degrees of freedom**\nThe fitted value vector $\\hat{y}$ is unique, regardless of the choice of $\\hat{x}$ from the solution set:\n$$\n\\hat{y}(y) = A\\hat{x}(y) = (\\hat{x}_1(y) + \\hat{x}_2(y))a = \\hat{s}(y) a\n$$\nSubstituting the expression for $\\hat{s}(y)$, we have the mapping $y \\mapsto \\hat{y}(y)$:\n$$\n\\hat{y}(y) = S_{\\lambda}(a^\\top y) a\n$$\nThe degrees of freedom are defined as the divergence of this vector field:\n$$\n\\operatorname{df}(\\hat{y}) = \\nabla \\cdot \\hat{y}(y) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i}\n$$\nThe $i$-th component of $\\hat{y}$ is $\\hat{y}_i(y) = S_{\\lambda}(a^\\top y) a_i$. We compute the partial derivative with respect to $y_j$:\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y) a_i] = a_i \\frac{\\partial}{\\partial y_j} [S_{\\lambda}(a^\\top y)]\n$$\nUsing the chain rule, let $u(y) = a^\\top y = \\sum_{k=1}^3 a_k y_k$. Then $\\frac{\\partial u}{\\partial y_j} = a_j$.\n$$\n\\frac{\\partial \\hat{y}_i}{\\partial y_j} = a_i \\frac{d S_{\\lambda}(u)}{du} \\frac{\\partial u}{\\partial y_j} = a_i a_j \\frac{d S_{\\lambda}(u)}{du}\n$$\nThis is the $(i,j)$-th element of the Jacobian matrix of the map $\\hat{y}$. The divergence is the trace of this Jacobian:\n$$\n\\operatorname{df}(\\hat{y}) = \\sum_{i=1}^3 \\frac{\\partial \\hat{y}_i}{\\partial y_i} = \\sum_{i=1}^3 a_i^2 \\frac{d S_{\\lambda}(u)}{du} = \\left(\\sum_{i=1}^3 a_i^2\\right) \\frac{d S_{\\lambda}(u)}{du}\n$$\nSince $\\sum_{i=1}^3 a_i^2 = \\|a\\|_2^2 = 1$, this simplifies to:\n$$\n\\operatorname{df}(\\hat{y}) = \\frac{d S_{\\lambda}(u)}{du}\n$$\nThe derivative of the soft-thresholding function $S_{\\lambda}(u)$ is $1$ if $|u| > \\lambda$ and $0$ if $|u| < \\lambda$. It is undefined at $|u|=\\lambda$. Thus, for almost all $y$, the derivative is the indicator function $\\mathbb{I}(|u| > \\lambda)$.\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(|a^\\top y| > \\lambda)\n$$\nWe now evaluate this for the specific values given in the problem: $y=2a$ and $\\lambda=1/2$.\nWe have $a^\\top y = 2$. We need to check if $|2| > 1/2$. This condition is true.\nTherefore, the value of the degrees of freedom is:\n$$\n\\operatorname{df}(\\hat{y}) = \\mathbb{I}(2 > 1/2) = 1\n$$\nThis result is notable. While there exists a solution $\\hat{x}$ with support size $2$, the degrees of freedom of the fit $\\hat{y}$ is $1$. This illustrates that the common heuristic $\\operatorname{df} \\approx |S|$ can be misleading in degenerate cases like perfect collinearity. The degrees of freedom correspond to the dimensionality of the subspace selected by the estimator, which in this case is the one-dimensional subspace spanned by the vector $a$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Finally, we bridge the gap between algebraic theory and computational practice. This exercise tasks you with implementing numerically stable methods, using Singular Value Decomposition (SVD) and pivoted QR factorization, to compute the matrix rank needed for the degrees of freedom calculation. You will explore how the choice of a numerical threshold affects the rank estimate in the presence of near-collinearity, providing essential skills for robustly estimating model complexity in real-world data analysis .",
            "id": "3443334",
            "problem": "You are given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ and an active index set $A \\subset \\{0,1,\\dots,p-1\\}$ associated with an $\\ell_1$-regularized least-squares estimator (Lasso). Consider the fitted values map $f: \\mathbb{R}^n \\to \\mathbb{R}^n$ defined by $f(y) = X \\widehat{\\beta}(y)$, where $y \\in \\mathbb{R}^n$ is the data vector and $\\widehat{\\beta}(y)$ is any solution of\n$$\n\\widehat{\\beta}(y) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\tfrac{1}{2}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\},\n$$\nwith a fixed regularization level $\\lambda > 0$. The degrees of freedom of $f$ at a point $y$ can be defined through the divergence-based notion arising from Stein’s Unbiased Risk Estimation (SURE): when $f$ is (weakly) differentiable at $y$, the degrees of freedom is given by the divergence,\n$$\n\\mathrm{df}(y) \\equiv \\sum_{i=1}^n \\frac{\\partial f_i(y)}{\\partial y_i}.\n$$\nOn any region of the data space where the active set $A$ and the signs of the active coefficients remain fixed and the algebraic constraints defining $\\widehat{\\beta}(y)$ are regular, the mapping $f$ is smooth on that region. Your tasks are:\n\n1) Starting only from the definition of the degrees of freedom as the divergence of the fitted values map and the basic optimality conditions for $\\ell_1$-regularized least squares, derive why, on any region where the active set $A$ and the signs of the active coefficients are fixed and algebraic regularity holds, the degrees of freedom equals the algebraic rank of the active-design submatrix $X_A \\in \\mathbb{R}^{n \\times |A|}$. Make explicit the conditions needed on $X_A$ and the structure of the Karush–Kuhn–Tucker (KKT) system to connect the Jacobian of $f$ with a projection onto the column space of $X_A$.\n\n2) Propose and implement two numerically stable methods to compute the algebraic rank $\\operatorname{rank}(X_A)$:\n   - A method based on the Singular Value Decomposition (SVD). Given a relative threshold $\\tau \\in (0,1)$, define the numerical rank as the number of singular values of $X_A$ strictly larger than $\\tau$ times the largest singular value of $X_A$.\n   - A method based on column-pivoted $QR$ factorization. Given the same relative threshold $\\tau$, define the numerical rank as the number of diagonal entries of the $R$ factor whose absolute values are strictly larger than $\\tau$ times the leading diagonal entry magnitude.\n\n3) Discuss and illustrate how the choice of the numerical threshold $\\tau$ influences the degrees-of-freedom estimate $\\widehat{\\mathrm{df}} = \\operatorname{rank}(X_A)$ in practice, especially in the presence of exact or near multicollinearity. Then, implement a program that, for the test suite below, computes both the SVD-based and the pivoted-$QR$-based numerical ranks for each case and each threshold, and outputs all results in the specified format.\n\nTest suite specification. For each test case, you are given $X$ and $A$. You must form $X_A$ by extracting the columns of $X$ indexed by $A$ (using zero-based indexing). Use the threshold set $\\mathcal{T} = \\{10^{-12}, 10^{-6}, 10^{-3}, 10^{-1}\\}$ for all cases.\n\n- Case 1 (well-conditioned full column rank): $n=5$, $p=3$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix}, \\quad A = \\{0,1,2\\}.\n  $$\n- Case 2 (nearly dependent active columns): $n=5$, $p=4$, with a small parameter $\\varepsilon = 10^{-7}$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 0 & 0 & 1 \\\\\n  0 & 1 & 0 & 0 \\\\\n  0 & 0 & 1 & \\varepsilon \\\\\n  0 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0\n  \\end{bmatrix}, \\quad A = \\{0,1,3\\}.\n  $$\n- Case 3 (exact duplicate column in the active set): $n=4$, $p=3$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix}, \\quad A = \\{0,1,2\\}.\n  $$\n- Case 4 (wide active set with intrinsic rank limited by $n$): $n=3$, $p=5$,\n  $$\n  X = \\begin{bmatrix}\n  1 & 0 & 0 & 1 & 0 \\\\\n  0 & 1 & 0 & 1 & 0 \\\\\n  0 & 0 & 1 & 0 & 0\n  \\end{bmatrix}, \\quad A = \\{0,1,2,3\\}.\n  $$\n\nFinal output format. For each case in the order 1, 2, 3, 4, and for each threshold in ascending order $\\tau \\in \\{10^{-12}, 10^{-6}, 10^{-3}, 10^{-1}\\}$, compute:\n- The SVD-based numerical rank of $X_A$.\n- The pivoted-$QR$-based numerical rank of $X_A$.\n\nAggregate all these integers into a single flat list in the exact order\n$$\n\\big[\\; r_{\\mathrm{svd}}(1,10^{-12}),\\; r_{\\mathrm{qr}}(1,10^{-12}),\\; r_{\\mathrm{svd}}(1,10^{-6}),\\; r_{\\mathrm{qr}}(1,10^{-6}),\\; \\dots,\\; r_{\\mathrm{svd}}(4,10^{-1}),\\; r_{\\mathrm{qr}}(4,10^{-1}) \\;\\big].\n$$\nYour program should produce a single line of output containing this list as a comma-separated sequence enclosed in square brackets, for example, $[3,3,3,3,\\dots]$. All answers are unitless integers. No user input or external files are permitted; all data are fixed as specified above in the test suite embedded in your program.",
            "solution": "The problem asks for a derivation of the degrees of freedom for the Lasso estimator under specific regularity conditions, the implementation of two numerical methods for computing matrix rank, and a discussion and application of these methods to a given test suite.\n\n### Part 1: Derivation of the Degrees of Freedom\n\nThe degrees of freedom of the fitted values map $f(y)$ is defined as its divergence, $\\mathrm{df}(y) = \\nabla \\cdot f(y) = \\mathrm{tr}(J_f(y))$, where $J_f(y)$ is the Jacobian matrix of $f$ with respect to $y$. The map is given by $f(y) = X\\widehat{\\beta}(y)$, where $\\widehat{\\beta}(y)$ is a solution to the Lasso optimization problem:\n$$\n\\widehat{\\beta}(y) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\tfrac{1}{2}\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\}\n$$\nfor a fixed regularization parameter $\\lambda > 0$.\n\nThe Karush-Kuhn-Tucker (KKT) optimality conditions for this problem are necessary and sufficient. The subgradient of the objective function with respect to $\\beta$ must contain the zero vector:\n$$\n-X^T(y - X\\widehat{\\beta}) + \\lambda z = 0\n$$\nwhere $z$ is a vector in the subdifferential of the $\\ell_1$-norm, $\\partial \\lVert \\widehat{\\beta} \\rVert_1$. The components of $z$ are given by:\n$$\nz_j = \\begin{cases} \\mathrm{sign}(\\widehat{\\beta}_j) & \\text{if } \\widehat{\\beta}_j \\neq 0 \\\\ v_j, \\text{ with } |v_j| \\le 1 & \\text{if } \\widehat{\\beta}_j = 0 \\end{cases}\n$$\nLet the active set $A = \\{j \\in \\{0, \\dots, p-1\\} \\mid \\widehat{\\beta}_j \\neq 0\\}$ be the set of indices corresponding to non-zero coefficients. Let $A^c$ be its complement, the inactive set. Let $X_A$ be the submatrix of $X$ containing columns indexed by $A$, and $\\widehat{\\beta}_A$ be the vector of non-zero coefficients. By definition, $\\widehat{\\beta}_{A^c} = 0$. The KKT conditions can be split based on the active set:\n1.  For the active set $A$: $X_A^T(y - X_A\\widehat{\\beta}_A) = \\lambda s_A$, where $s_A$ is the vector of signs of the coefficients in $\\widehat{\\beta}_A$.\n2.  For the inactive set $A^c$: $|X_{A^c}^T(y - X_A\\widehat{\\beta}_A)| \\le \\lambda$ (element-wise).\n\nThe problem states that we are on a region of the data space where the active set $A$ and the signs $s_A$ are fixed. The second KKT condition (the inequality) is assumed to hold strictly, meaning no inactive predictor is on the verge of entering the model. The term \"algebraic regularity\" implies that the first KKT condition, which is a system of linear equations for $\\widehat{\\beta}_A$, defines $\\widehat{\\beta}_A$ as a smooth function of $y$. With these assumptions, we can analyze the first condition:\n$$\nX_A^T X_A \\widehat{\\beta}_A = X_A^T y - \\lambda s_A\n$$\nThe fitted values map is $f(y) = X\\widehat{\\beta}(y) = X_A\\widehat{\\beta}_A(y)$. To find its Jacobian, we differentiate this expression with respect to $y^T$. The Jacobian of $f$ is given by $J_f(y) = \\frac{\\partial f(y)}{\\partial y^T} = X_A \\frac{\\partial \\widehat{\\beta}_A(y)}{\\partial y^T}$.\n\nTo find the Jacobian of $\\widehat{\\beta}_A$, we differentiate the active set KKT equation with respect to $y^T$. The term $\\lambda s_A$ is constant in this region, so its derivative is zero.\n$$\n\\frac{\\partial}{\\partial y^T} \\left( X_A^T X_A \\widehat{\\beta}_A \\right) = \\frac{\\partial}{\\partial y^T} \\left( X_A^T y - \\lambda s_A \\right)\n$$\nApplying the chain rule, we get:\n$$\n(X_A^T X_A) \\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = X_A^T \\frac{\\partial y}{\\partial y^T} = X_A^T I_n = X_A^T\n$$\nThis is a system of linear equations for the Jacobian matrix $\\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T}$. A solution to an equation of the form $B Z = C$ is given by $Z = B^+ C$, where $B^+$ denotes the Moore-Penrose pseudoinverse of $B$. Thus, we can write:\n$$\n\\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = (X_A^T X_A)^+ X_A^T\n$$\nHere, the use of the pseudoinverse is crucial as it provides a solution even when the matrix $X_A^T X_A$ is singular, which occurs if the columns of $X_A$ are linearly dependent.\n\nNow, we substitute this back into the expression for the Jacobian of the fitted values map:\n$$\nJ_f(y) = X_A \\frac{\\partial \\widehat{\\beta}_A}{\\partial y^T} = X_A (X_A^T X_A)^+ X_A^T\n$$\nThe matrix $P_A = X_A (X_A^T X_A)^+ X_A^T$ is the orthogonal projection matrix onto the column space of $X_A$, denoted $\\mathcal{C}(X_A)$. This is a standard result in linear algebra.\n\nThe degrees of freedom are defined as the trace of this Jacobian matrix:\n$$\n\\mathrm{df}(y) = \\mathrm{tr}(J_f(y)) = \\mathrm{tr}(P_A)\n$$\nA fundamental property of any projection matrix is that its trace equals the dimension of the subspace onto which it projects. In this case, the dimension of the subspace $\\mathcal{C}(X_A)$ is the rank of the matrix $X_A$. Therefore,\n$$\n\\mathrm{df}(y) = \\operatorname{rank}(P_A) = \\dim(\\mathcal{C}(X_A)) = \\operatorname{rank}(X_A)\n$$\nThis derivation shows that under the stated conditions of a fixed active set and signs (implying local linearity of the estimator), the degrees of freedom of the Lasso fit is simply the number of linearly independent columns in the active design submatrix $X_A$. The key conditions required were the ability to differentiate the KKT system, which is ensured by the \"algebraic regularity\" assumption, and the identification of the Jacobian of the fit map with the projection operator onto $\\mathcal{C}(X_A)$.\n\n### Part 2: Numerical Rank Computation\n\nThe algebraic rank is often an unstable quantity in floating-point arithmetic. Near-collinearities can make a matrix full-rank algebraically but rank-deficient in a practical, numerical sense. We are asked to implement two methods to compute the numerical rank.\n\n1.  **Singular Value Decomposition (SVD)-based Rank**:\n    The SVD of $X_A \\in \\mathbb{R}^{n \\times |A|}$ is $X_A = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{|A| \\times |A|}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times |A|}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values, sorted in non-increasing order: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_k \\ge 0$, where $k=\\min(n, |A|)$. The algebraic rank is the number of non-zero singular values. The numerical rank, given a relative threshold $\\tau \\in (0,1)$, is defined as the number of singular values that are significantly larger than zero, relative to the largest singular value:\n    $$\n    \\operatorname{rank}_{\\mathrm{svd}}(X_A, \\tau) = \\left| \\left\\{ i \\mid \\sigma_i > \\tau \\cdot \\sigma_1 \\right\\} \\right|\n    $$\n    This is the most reliable method for determining numerical rank.\n\n2.  **Column-pivoted QR Factorization-based Rank**:\n    The column-pivoted QR factorization of $X_A$ is $X_A P = Q R$, where $P$ is a permutation matrix, $Q$ is an orthogonal matrix, and $R$ is an upper-trapezoidal matrix whose diagonal entries have decreasing magnitudes: $|R_{11}| \\ge |R_{22}| \\ge \\dots \\ge |R_{kk}| \\ge 0$. The permutation $P$ is chosen greedily to maximize the magnitude of the diagonal entries at each step. If $X_A$ is rank-deficient, some diagonal entries of $R$ will be zero. Similar to the SVD approach, we can define a numerical rank by thresholding the diagonal entries of $R$ relative to the largest one:\n    $$\n    \\operatorname{rank}_{\\mathrm{qr}}(X_A, \\tau) = \\left| \\left\\{ i \\mid |R_{ii}| > \\tau \\cdot |R_{11}| \\right\\} \\right|\n    $$\n    This method is generally faster than SVD but can be less reliable for certain pathological matrices, although it performs well in most practical situations.\n\n### Part 3: Influence of Threshold and Application\n\nThe choice of the threshold $\\tau$ is critical and context-dependent. It defines what is considered \"numerically zero\".\n\n-   A very small threshold (e.g., $\\tau = 10^{-12}$ or near machine epsilon) makes the rank computation sensitive. It may count columns that are nearly, but not exactly, linearly dependent, potentially overestimating the \"effective\" rank of the matrix. This is seen in Case 2, where two columns are separated by a vector of norm $\\varepsilon = 10^{-7}$. A threshold of $\\tau = 10^{-12}$ is much smaller than this separation, so the SVD and QR methods will correctly identify $3$ linearly independent columns.\n\n-   A larger threshold (e.g., $\\tau = 10^{-6}$) acts as a coarser filter. For Case 2, since $\\varepsilon=10^{-7}$ is smaller than $\\tau=10^{-6}$ (relative to the largest singular value/diagonal element), the near-dependency between the two columns will be detected, and the numerical rank will be reported as $2$. This illustrates how $\\tau$ operationalizes the concept of near-collinearity.\n\n-   In cases of exact collinearity (Case 3, duplicate columns) or a well-conditioned matrix (Case 1), the rank is unambiguous. The SVD will produce a singular value that is truly zero (or on the order of machine epsilon), and the pivoted QR will produce a zero on the diagonal of $R$. Any reasonable positive threshold $\\tau$ will correctly identify the rank deficiency, yielding a rank of $2$.\n\n-   In Case 4, the number of columns $|A|=4$ is greater than the number of rows $n=3$, so the algebraic rank cannot exceed $3$. Since the first three columns are linearly independent, the rank is exactly $3$. This is a structural property, not a numerical one, and both methods should robustly report a rank of $3$ for all specified thresholds.\n\nThe choice of $\\tau$ reflects a trade-off: a small $\\tau$ adheres closely to the algebraic definition of rank, while a larger $\\tau$ provides a more stable, \"effective\" rank that discounts the influence of near-collinear columns, which is often desirable in statistical modeling to avoid variance inflation. The following program computes these numerical ranks for the given test suite.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Computes numerical ranks for a suite of test cases using SVD and pivoted QR methods.\n    \"\"\"\n    # Define the set of relative thresholds for rank determination.\n    thresholds = [1e-12, 1e-6, 1e-3, 1e-1]\n\n    # Test cases specification: (matrix X, active set A)\n    # Case 1: Well-conditioned, full column rank\n    X1 = np.array([\n        [1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]\n    ])\n    A1 = [0, 1, 2]\n\n    # Case 2: Nearly dependent active columns\n    eps = 1e-7\n    X2 = np.array([\n        [1., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., eps],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]\n    ])\n    A2 = [0, 1, 3]\n\n    # Case 3: Exact duplicate column in the active set\n    X3 = np.array([\n        [1., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.],\n        [0., 0., 0.]\n    ])\n    A3 = [0, 1, 2]\n\n    # Case 4: Wide active set with rank limited by n\n    X4 = np.array([\n        [1., 0., 0., 1., 0.],\n        [0., 1., 0., 1., 0.],\n        [0., 0., 1., 0., 0.]\n    ])\n    A4 = [0, 1, 2, 3]\n    \n    test_cases = [\n        (X1, A1),\n        (X2, A2),\n        (X3, A3),\n        (X4, A4)\n    ]\n\n    # List to store the final sequence of results.\n    results = []\n\n    # Iterate over each test case.\n    for X, A in test_cases:\n        # Form the active submatrix X_A.\n        # Ensure that A is a list or tuple of indices, not a set, for correct slicing.\n        X_A = X[:, list(A)]\n\n        # Handle the case where X_A has no columns.\n        if X_A.shape[1] == 0:\n            for _ in thresholds:\n                results.extend([0, 0])\n            continue\n            \n        # Iterate over each threshold.\n        for tau in thresholds:\n            # --- SVD-based numerical rank ---\n            try:\n                s = np.linalg.svd(X_A, compute_uv=False)\n                # If s is empty, rank is 0. Otherwise, compute rank.\n                if s.size == 0:\n                    rank_svd = 0\n                else:\n                    s_max = s[0]\n                    # Check for s_max == 0 to avoid division by zero if X_A is a zero matrix.\n                    if s_max > 0:\n                        rank_svd = np.sum(s > tau * s_max)\n                    else:\n                        rank_svd = 0\n            except np.linalg.LinAlgError:\n                rank_svd = 0 # Should not happen with compute_uv=False\n\n            results.append(int(rank_svd))\n\n            # --- Pivoted QR-based numerical rank ---\n            try:\n                # Use scipy.linalg.qr with column pivoting.\n                Q, R, P = qr(X_A, pivoting=True)\n                r_diag_abs = np.abs(np.diag(R))\n                \n                # If r_diag_abs is empty, rank is 0. Otherwise compute rank.\n                if r_diag_abs.size == 0:\n                    rank_qr = 0\n                else:\n                    r_max = r_diag_abs[0]\n                    # Check for r_max == 0 to avoid division by zero.\n                    if r_max > 0:\n                        rank_qr = np.sum(r_diag_abs > tau * r_max)\n                    else:\n                        rank_qr = 0\n            except np.linalg.LinAlgError:\n                rank_qr = 0\n\n            results.append(int(rank_qr))\n\n    # Print the final aggregated list in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}