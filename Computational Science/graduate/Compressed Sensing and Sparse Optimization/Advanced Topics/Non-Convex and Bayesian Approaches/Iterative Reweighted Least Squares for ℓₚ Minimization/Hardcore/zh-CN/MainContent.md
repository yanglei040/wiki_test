## 引言
在[稀疏信号恢复](@entry_id:755127)、[压缩感知](@entry_id:197903)和机器学习等众多领域，从[欠定线性系统](@entry_id:756304)中恢复[稀疏解](@entry_id:187463)是一个核心挑战。虽然凸的ℓ₁最小化为此提供了一个强大的框架，但非凸的ℓₚ（$p < 1$）正则化因其能诱导出更稀疏、更精确的解而备受关注。然而，这种优越性是以牺牲[凸性](@entry_id:138568)为代价的，这使得优化过程变得异常困难，充满了局部最优解的陷阱。本文旨在系统性地攻克这一难题，深入剖析一种强大而优雅的算法——迭代重加权最小二乘 (Iterative Reweighted Least Squares, IRLS)，它为求解非凸ℓₚ最小化问题提供了一条行之有效的路径。

本文将分为三个核心部分，旨在带领读者从理论基础走向实际应用。在第一章“原理与机制”中，我们将奠定理论基石，从ℓₚ问题的数学构建出发，解释其促进稀疏性的几何与分析原理，并深入探讨作为IRLS灵魂的“主化-最小化”（MM）框架，最终推导出算法的具体形式并分析其收敛保证。随后的第二章“应用与[交叉](@entry_id:147634)学科联系”将视野拓宽，通过比较IRLS与其他算法、探讨其在结构化[稀疏模型](@entry_id:755136)中的扩展，以及介绍连续化等稳健实现策略，展示IRLS作为一个灵活框架的强大威力，并辅以超分辨率成像等交叉学科案例。最后，在“动手实践”部分，我们将通过一系列精心设计的编程练习，引导读者从手动计算单次迭代到实现完整的连续化[IRLS算法](@entry_id:750839)，真正将理论知识内化为解决实际问题的能力。

## 原理与机制

本章旨在深入阐述通过迭代重加权最小二乘（Iterative Reweighted Least Squares, IRLS）方法求解 $\ell_p$ 最小化问题的核心原理与算法机制。我们将从问题的数学构建出发，探讨其促进稀疏性的根本原因，随后详细剖析作为 IRLS 理论基础的 Majorization-Minimization (MM) 原理，并最终推导出 IRLS 算法的具体形式、解释其关键技术细节，并概述其收敛性和解的理论保证。

### $\ell_p$ 最小化问题的构建

在[稀疏信号恢复](@entry_id:755127)和[压缩感知](@entry_id:197903)的领域中，我们的核心目标是从一个可能欠定的[线性系统](@entry_id:147850)中恢复出[稀疏解](@entry_id:187463)。这一目标可以通过[优化问题](@entry_id:266749)的形式进行[数学建模](@entry_id:262517)。对于一个[线性模型](@entry_id:178302)，其中[设计矩阵](@entry_id:165826)为 $A \in \mathbb{R}^{m \times n}$，未知向量为 $x \in \mathbb{R}^{n}$，观测数据为 $y \in \mathbb{R}^{m}$，我们通常采用两种[标准形式](@entry_id:153058)来寻找稀疏解 $x$。

第一种是**[等式约束](@entry_id:175290)问题**，适用于无噪声的场景 ($Ax=y$)：
$$ \min_{x \in \mathbb{R}^{n}} \|x\|_p^p \quad \text{subject to} \quad Ax = y $$
其中，$\|x\|_p^p = \sum_{i=1}^{n} |x_i|^p$ 是 **$\ell_p$ 惩罚项**，参数 $p \in (0, 1]$。当 $p=1$ 时，这是一个凸[优化问题](@entry_id:266749)，即[基追踪](@entry_id:200728)（Basis Pursuit）。当 $p \in (0,1)$ 时，目标函数变为非凸的，这使得问题更难求解，但也带来了更强的稀疏促进能力。

第二种是**无约束正则化问题**，它更具[一般性](@entry_id:161765)，能够处理带噪声的观测数据：
$$ \min_{x \in \mathbb{R}^{n}} J_{p,\lambda}(x) = \frac{1}{2}\|Ax - y\|_2^2 + \lambda \|x\|_p^p $$
此形式在数据保真项 $\frac{1}{2}\|Ax - y\|_2^2$ 和稀疏促进项 $\lambda \|x\|_p^p$ 之间取得平衡。**正则化参数** $\lambda > 0$ 控制了这种平衡：较小的 $\lambda$ 侧重于拟[合数](@entry_id:263553)据（即最小化残差 $\|Ax - y\|_2$），而较大的 $\lambda$ 则更强调解的[稀疏性](@entry_id:136793)（即最小化 $\|x\|_p^p$），这通常以牺牲部分[数据拟合](@entry_id:149007)度为代价。沿着由不同 $\lambda$ 值定义的解的路径，即**正则化路径**，$\|x_\lambda\|_p^p$ 通常随 $\lambda$ 的增加而单调不增，而数据残差 $\|Ax_\lambda - y\|_2$ 则单调不减，这精确地体现了[稀疏性](@entry_id:136793)与数据保真度之间的权衡。

在理论上，当约束问题可行（即存在 $x$ 使得 $Ax=y$）且观测无噪声时，正则化问题是约束问题的一种近似。可以证明，对于 $p \ge 1$ 的凸情况，当 $\lambda \to 0$ 时，正则化问题的全局最优解序列 $x_\lambda$ 的任何聚点都将是约束问题的最优解。即使对于 $p \in (0,1)$ 的非凸情况，尽管严格的[拉格朗日对偶](@entry_id:638042)等价性可能失效，但对于[全局最优解](@entry_id:175747)，当 $\lambda \to 0$ 时，数据残差 $\|Ax_\lambda - y\|_2$ 依然会收敛到 0。

在处理这些[优化问题](@entry_id:266749)时，解的存在性和唯一性是首要关注的问题。对于正则化问题，当 $\lambda > 0$ 且 $p \in (0,1)$ 时，尽管目标函数 $J_{p,\lambda}(x)$ 是非凸的，但我们可以证明其具有**强制性（coercivity）**。这是因为惩罚项 $\|x\|_p^p$ 会随着 $\|x\|_2 \to \infty$ 而趋于无穷。具体来说，$\|x\|_p^p \ge m_p \|x\|_2^p$，其中 $m_p$ 是一个大于零的常数。因此，[目标函数](@entry_id:267263) $J_{p,\lambda}(x)$ 也是强制的。根据魏尔斯特拉斯[极值定理](@entry_id:142794)，一个在 $\mathbb{R}^n$ 上的连续且强制的函数必存在[全局最小值](@entry_id:165977)。因此，对于任何 $\lambda > 0$ 和任何矩阵 $A$，[全局最优解](@entry_id:175747)总是存在的，并且所有[全局最优解](@entry_id:175747)构成的集合是有界的。然而，由于 $p \in (0,1)$ 时目标函数的非凸性，全局最优解的唯一性无法得到保证，可能存在多个不同的[全局最优解](@entry_id:175747)。

### $\ell_p$ 惩罚项 ($p \lt 1$) 的稀疏促进能力

选择 $p \in (0,1)$ 而非更常用的 $\ell_1$ 范数 ($p=1$) 的核心动机在于其更强的稀疏促进能力。这种优越性可以从几何、分析和算法三个层面来理解。

**几何直观**
考察不同 $p$ 值对应的“[单位球](@entry_id:142558)” $\mathcal{B}_p = \{x \in \mathbb{R}^n : \|x\|_p \le 1\}$ 的形状，可以获得深刻的直观理解。
*   当 $p=2$ 时，$\mathcal{B}_2$ 是一个标准的欧几里得球体，其边界光滑且处处严格凸。它没有任何特定的方向偏好，因此 $\ell_2$ 最小化通常产生非稀疏（稠密）的解。
*   当 $p=1$ 时，$\mathcal{B}_1$ 是一个称为**[交叉多胞体](@entry_id:748072)**（cross-polytope）的[凸多面体](@entry_id:170947)。它的极点（顶点）是坐标轴上的 $2n$ 个点 $\{\pm e_i\}$，这些点是 1-稀疏的。优化过程相当于寻找一个扩张的 $\ell_1$ 球与[解空间](@entry_id:200470)超平面 $Ax=y$ 的第一个接触点，这个接触点很可能发生在这些稀疏的顶点上，从而产生稀疏解。
*   当 $p \in (0,1)$ 时（例如 $p=1/2$），单位“球” $\mathcal{B}_p$ 变为一个非凸的**[星形集](@entry_id:154094)**。其边界在坐标轴之间向内凹陷，形成比 $\ell_1$ 球的顶点更尖锐的“[尖点](@entry_id:636792)”（cusps）。这种几何形态使得 $\mathcal{B}_p$ 在与超平面 $Ax=y$ 相切时，极大概率会接触到坐标轴上的点，这意味着解的某些分量为零。这种更“尖”的几何结构是 $\ell_p$ ($p \lt 1$) 惩罚项比 $\ell_1$ 惩罚项能产生更稀疏[解的几何解释](@entry_id:155287)。 

**分析视角**
从分析的角度看，我们可以考察标量惩[罚函数](@entry_id:638029) $\phi_p(t) = |t|^p$。对于 $p \in (0,1)$，函数 $\phi_p(t)$ 在 $t>0$ 时的导数为 $\phi_p'(t) = pt^{p-1}$。由于指数 $p-1$ 为负，当 $t \to 0^+$ 时，导数 $\phi_p'(t) \to \infty$。这意味着在原点附近，$\ell_p$ 惩罚对一个微小的非零值施加了几乎无穷大的“边际惩罚”，强烈地阻止系数从零变为非零。与之对比，$\ell_1$ 惩罚的边际惩罚在各处（除原点外）都是常数 1。另一方面，对于较大的系数值（$|t|>1$），$|t|^p \lt |t|$，这意味着 $\ell_p$ 惩罚对大系数的增长比 $\ell_1$ 惩罚更为宽容。这种“惩罚小值、宽容大值”的策略，有效地将解的系数推向两极：要么精确为零，要么相对较大，从而实现比 $\ell_1$ 范数更强的稀疏性。

### Majorization-Minimization 原理：IRLS 的理论基础

直接最小化非凸的 $\ell_p$ 正则化目标函数是困难的。IRLS 算法为此提供了一个巧妙且强大的框架，其理论基础是 **Majorization-Minimization (MM) 原理**。MM 算法是一种迭代策略，它将一个困难的原始[优化问题](@entry_id:266749) $\min_x f(x)$ 替换为一系列更容易求解的子问题。

在每次迭代中，给定当前解 $x^{(k)}$，我们构建一个**代理函数（surrogate function）** $Q(x; x^{(k)})$，它满足以下两个关键性质：
1.  **Majorization（主化）**: 对所有 $x$，代理函数始终位于原始函数之上，即 $Q(x; x^{(k)}) \ge f(x)$。
2.  **Tangency（相切）**: 在当前点 $x^{(k)}$，代理函数与原始函数值相等，即 $Q(x^{(k)}; x^{(k)}) = f(x^{(k)})$。

然后，我们通过最小化（或至少充分减小）这个代理函数来获得下一个迭代点 $x^{(k+1)}$：
$$ x^{(k+1)} \in \arg\min_x Q(x; x^{(k)}) $$
这一过程保证了原始目标函数值的**单调下降**。其证明非常简洁且不依赖于 $f(x)$ 的[凸性](@entry_id:138568)：
$$ f(x^{(k+1)}) \stackrel{(1)}{\le} Q(x^{(k+1)}; x^{(k)}) \stackrel{(2)}{\le} Q(x^{(k)}; x^{(k)}) \stackrel{(3)}{=} f(x^{(k)}) $$
不等式 (1) 来自主化性质，不等式 (2) 来自对代理函数的最小化步骤，等式 (3) 来自相切性质。MM 算法的威力在于，只要我们能为复杂的非凸函数 $f(x)$ 找到一个易于优化的（例如凸的、二次的）代理函数 $Q(x; x^{(k)})$，就能保证算法在[目标函数](@entry_id:267263)值上稳定下降，从而收敛到一个（可能是局部的）最优点。

### IRLS 算法：机制与实现

IRLS 正是为 $\ell_p$ 最小化问题量身定制的一种 MM 算法。其核心思想是为主化非凸的惩罚项 $\sum_i |x_i|^p$ 构建一个可分的二次代理函数。

**构建二次代理函数**
我们可以通过变量替换来简化惩罚项的结构。令 $t = x^2$，则 $|x|^p = (x^2)^{p/2} = t^{p/2}$。考虑标量函数 $\varphi(t) = t^{p/2}$，其中 $t \ge 0$。对于 $p \in (0, 2)$，指数 $p/2 \in (0,1)$，这使得 $\varphi(t)$ 成为一个严格的**[凹函数](@entry_id:274100)**。

根据[凹函数](@entry_id:274100)的基本性质，其图形位于其任意[切线](@entry_id:268870)的下方。在任意点 $t_0 > 0$，我们有以下一阶不等式：
$$ \varphi(t) \le \varphi(t_0) + \varphi'(t_0)(t - t_0) $$
其中 $\varphi'(t) = \frac{p}{2}t^{p/2-1}$ 是其导数。将 $t=x^2$ 和 $t_0 = x_0^2$（其中 $x_0 \ne 0$ 是当前迭代点）代入，我们得到 $|x|^p$ 的一个二次代理：
$$ |x|^p \le |x_0|^p + \frac{p}{2}|x_0|^{p-2}(x^2 - x_0^2) $$
整理后，我们得到一个形如 $Q(x; x_0) = \alpha(x_0)x^2 + \beta(x_0)$ 的二次代理函数：
$$ Q(x; x_0) = \left(\frac{p}{2} |x_0|^{p-2}\right) x^{2} + \left(1 - \frac{p}{2}\right) |x_0|^{p} $$
这个二次函数在全局范围内主化了 $|x|^p$，并在 $x=x_0$ 处与之相切。

**IRLS 子问题与权重**
将这个思想应用于 $\ell_p$ 正则化问题的每一项，我们在第 $k$ 次迭代中，用一个可分的二次函数来主化整个惩罚项 $\lambda \sum_i |x_i|^p$。这引导我们求解如下的**加权最小二乘子问题**来得到 $x^{(k+1)}$：
$$ x^{(k+1)} = \arg\min_x \left( \frac{1}{2}\|Ax - y\|_2^2 + \lambda \sum_{i=1}^{n} w_i^{(k)} x_i^2 \right) $$
其中的**权重** $w_i^{(k)}$ 直接来自于上述代理函数的推导，其形式为：
$$ w_i^{(k)} \propto |x_i^{(k)}|^{p-2} $$
这个子问题是一个凸的二次[优化问题](@entry_id:266749)，其解可以通过求解一个[线性方程组](@entry_id:148943)得到：$(A^T A + 2\lambda W^{(k)})x = A^T y$，其中 $W^{(k)}$ 是对角元素为 $w_i^{(k)}$ 的对角矩阵。

**平滑参数 $\varepsilon$ 的作用**
在实际应用中，上述权重公式存在一个严重问题：当某个迭代分量 $x_i^{(k)}$ 恰好为零时，由于 $p-2 < 0$，权重 $w_i^{(k)}$ 会变为无穷大。为了解决这个问题并提高算法的[数值稳定性](@entry_id:146550)，我们引入一个小的**平滑参数** $\varepsilon > 0$，将权重修正为：
$$ w_i^{(k)} = \left( (x_i^{(k)})^2 + \varepsilon \right)^{p/2 - 1} $$
这个小小的改动至关重要，其作用体现在两个方面：
1.  **避免权重无限大**：由于 $\varepsilon > 0$，分母 $(x_i^{(k)})^2 + \varepsilon$ 始终为正，确保了权重 $w_i^{(k)}$ 始终是有限的正数，使得代理函数和子问题始终是良定义的。
2.  **保证子问题的良定性**：权重 $w_i^{(k)}$ 始终为正，意味着对角矩阵 $W^{(k)}$ 是正定的。因此，子问题的海森矩阵 $A^T A + 2\lambda W^{(k)}$ 是一个[半正定矩阵](@entry_id:155134)（$A^TA$）与一个[正定矩阵](@entry_id:155546)之和，结果必然是正定的。这保证了即使在 $A$ [秩亏](@entry_id:754065)的情况下，IRLS 的每一步迭代都有唯一的、稳定的解。

通过迭代更新权重并求解加权[最小二乘问题](@entry_id:164198)，IRLS 算法形成了一个强大的反馈循环。如果一个系数 $|x_i^{(k)}|$ 很小，对应的权重 $w_i^{(k)}$ 就会变得非常大，从而在下一次迭代中对 $x_i^2$ 施加巨大的惩罚，迫使 $x_i^{(k+1)}$ 更接近于零。这正是 IRLS 算法将 $\ell_p$ 惩罚的稀疏促进原理转化为具体计算步骤的机制。

### 理论保证与[收敛性分析](@entry_id:151547)

一个有效的算法不仅需要能够执行，还需要有坚实的理论支持。IRLS 的理论保证主要涉及其收敛点、[稀疏恢复](@entry_id:199430)能力和迭代[序列的收敛](@entry_id:140648)性。

**[不动点](@entry_id:156394)与[最优性条件](@entry_id:634091)**
IRLS 算法生成一个序列 $\{x^{(k)}\}$。如果这个序列收敛到一个[不动点](@entry_id:156394) $x^\star$，即 $x^{(k)} \to x^\star$，那么这个[不动点](@entry_id:156394)与原始非凸问题 $J_{p,\lambda}(x)$ 的解有什么关系？我们可以通过分析 IRLS 子问题的[一阶最优性条件](@entry_id:634945)来回答这个问题。在第 $k+1$ 次迭代，我们有：
$$ A^T(Ax^{(k+1)} - y) + 2\lambda W^{(k)} x^{(k+1)} = 0 $$
假设权重 $W^{(k)}$ 的定义与原始问题的梯度相关联，并且在收敛时是一致的。那么在极限情况下（$k \to \infty$），上述方程会转化为原始非凸[目标函数](@entry_id:267263) $J_{p,\lambda}(x)$ 的[一阶最优性条件](@entry_id:634945)（即梯度为零）：
$$ A^T(Ax^\star - y) + \lambda \nabla_x \left( \sum_{i=1}^n |x_i|^p \right)\Big|_{x=x^\star} = 0 $$
这表明，IRLS 算法收敛到的任何[不动点](@entry_id:156394)都是原始非凸问题的一个**[临界点](@entry_id:144653)**（critical point）。

**[稀疏恢复保证](@entry_id:755121)**
在[压缩感知](@entry_id:197903)的理论框架下，$\ell_p$ 最小化能否成功恢复稀疏信号，取决于传感矩阵 $A$ 的性质。一个关键的性质是**受限等距性质 (Restricted Isometry Property, RIP)**。RIP 刻画了矩阵 $A$ 在作用于稀疏向量时近似保持其欧几里得长度的能力。已有理论证明，如果矩阵 $A$ 满足特定阶数和常数的 RIP 条件（例如，对于 $s$-[稀疏信号](@entry_id:755125)，一个充分条件是 $2s$ 阶的 RIP 常数 $\delta_{2s}  \sqrt{2}-1$），那么对于任意 $s$-[稀疏信号](@entry_id:755125) $x_\star$，通过求解 $\ell_p$ 最小化问题（对于所有 $p \in (0,1]$），可以唯一地、精确地恢复出 $x_\star$。IRLS 算法正是实现这一（非凸）最小化过程的有效工具。

**迭代[序列的收敛](@entry_id:140648)性**
MM 原理保证了[目标函数](@entry_id:267263)值 $J_{p,\lambda}(x^{(k)})$ 的单调收敛。然而，这并不直接保证迭代序列 $\{x^{(k)}\}$ 本身会收敛到一个点（它可能在多个最优解之间[振荡](@entry_id:267781)）。对于[非凸优化](@entry_id:634396)，这是一个长期存在的难题。现代[非凸优化](@entry_id:634396)理论为此提供了强大的工具，其中最重要的是**Kurdyka-Łojasiewicz (KL) 性质**。

幸运的是，我们所研究的 $\ell_p$ 正则化目标函数 $J_{p,\lambda}(x)$ 属于一类具有良好几何性质的半[代数函数](@entry_id:187534)，而所有半[代数函数](@entry_id:187534)都满足 KL 性质。可以证明，在一些相当标准的正则性假设下（例如，保证算法有充分下降和梯度信息与迭代步长相关的条件），KL 性质能够保证 IRLS 算法产生的**整个序列 $\{x^{(k)}\}$ 收敛到一个唯一的[临界点](@entry_id:144653) $x^\star$**。这意味着算法不会在[解空间](@entry_id:200470)中永久[振荡](@entry_id:267781)，而是稳定地趋向于一个确定的解。这是对[非凸优化](@entry_id:634396)算法能够获得的最强的收敛性保证之一。

综上所述，IRLS 算法不仅在实践中表现出色，而且其背后有深刻的数学原理和坚实的理论保证，使其成为解决[非凸稀疏恢复](@entry_id:752556)问题的基石方法之一。