{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握迭代重加权最小二乘（IRLS）算法，没有什么比亲手执行一次迭代更好的方法了。本练习将引导您完成一个平滑 $\\ell_1$ 问题的单步 IRLS 过程，从一个零向量初始化开始。通过计算初始权重并求解由此产生的加权正规方程，您将对驱动该算法走向稀疏解的核心机制有一个具体而深刻的理解。",
            "id": "3454747",
            "problem": "考虑稀疏恢复的惩罚最小二乘公式，它通过使用可微代理函数平滑绝对值来近似 $p=1$ 时的 $\\ell_{p}$ 最小化。设目标函数为\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{i=1}^{n} \\phi_{\\varepsilon}(x_{i}),\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$\\lambda0$，且 $\\phi_{\\varepsilon}(t)$ 是一个光滑、严格凸的函数，当 $\\varepsilon \\to 0^{+}$ 时，它近似于 $|t|$，具体为 $\\phi_{\\varepsilon}(t) = \\sqrt{t^{2} + \\varepsilon^{2}}$。迭代重加权最小二乘（IRLS）方法在第 $k$ 次迭代时，围绕 $x^{(k)}$ 构建惩罚项的一个二次上界（majorizer），并通过最小化该二次代理函数与数据保真项之和来更新 $x^{(k+1)}$。从上述基本定义和凹平方根函数的不等式性质出发，推导关于对角权重矩阵 $W^{(k)}$ 的 $x^{(k+1)}$ 的加权正规方程，其中 $W^{(k)}$ 的对角线元素取决于 $x^{(k)}$ 和 $\\varepsilon$，并证明权重对应于代理函数在 $x^{(k)}$ 处的曲率。\n\n然后，使用具体数据\n$$\nA \\;=\\; \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}, \\qquad y \\;=\\; \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\qquad p \\;=\\; 1, \\qquad \\lambda \\;=\\; 0.1, \\qquad \\varepsilon \\;=\\; 10^{-3},\n$$\n和初始化 $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，计算一次 IRLS 迭代，如下所示：\n- 评估 $w^{(0)}$ 以构成 $W^{(0)}$，即迭代 $k=0$ 时的权重矩阵的对角线。\n- 求解线性系统 $(A^{\\top} A + \\lambda W^{(0)})\\, x^{(1)} = A^{\\top} y$ 以得到 $x^{(1)}$。\n- 从 $x^{(1)}$ 评估 $w^{(1)}$。\n\n将你的最终答案表示为一个单行矩阵，按顺序包含 $x^{(1)}$ 的三个分量，后跟 $w^{(1)}$ 的三个分量。提供精确值；不需要四舍五入。最终答案中不要包含任何单位。",
            "solution": "该问题定义明确，要求推导针对特定平滑 $\\ell_1$ 惩罚的迭代重加权最小二乘（IRLS）更新，讨论权重的解释，并进行单次迭代的数值计算。\n\n### 第一步：提取已知条件\n已知条件如下：\n- 目标函数：$J(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\phi_{\\varepsilon}(x_{i})$。\n- 矩阵和向量维度：$A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。\n- 正则化参数：$\\lambda  0$。\n- 平滑函数：$\\phi_{\\varepsilon}(t) = \\sqrt{t^{2} + \\varepsilon^{2}}$，对于小的 $\\varepsilon  0$，它近似于 $|t|$。\n- 数值数据：\n  - $A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n  - $p = 1$（选择惩罚项的背景）\n  - $\\lambda = 0.1$\n  - $\\varepsilon = 10^{-3}$\n- 初始化：$x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 任务：推导 $x^{(k+1)}$ 的加权正规方程，展示权重与曲率之间的联系，并计算一次 IRLS 迭代以求得 $x^{(1)}$ 和随后的权重 $w^{(1)}$。\n\n### 第二步：使用提取的已知条件进行验证\n根据指定标准对问题进行验证：\n- **科学依据**：该问题设置在凸优化，特别是主化-最小化和用于稀疏信号恢复的 IRLS 的成熟数学框架内。选择 $\\phi_{\\varepsilon}(t)$ 作为绝对值函数的光滑、严格凸代理是一种标准技术。\n- **适定性**：目标函数 $J(x)$ 是严格凸的，因为它是凸函数（最小二乘项）和严格凸函数之和（当 $\\varepsilon  0$ 时 $\\phi_{\\varepsilon}(t)$ 是严格凸的）的和。因此，存在唯一的最小化子。所描述的 IRLS 算法是解决此类问题的标准程序。\n- **客观性**：问题使用精确的数学语言和定义陈述。\n- **完整性与一致性**：推导和数值计算所需的所有必要数据和定义均已提供。没有矛盾之处。\n- **其他缺陷**：问题并非无关紧要、隐喻性或伪深刻。它涉及稀疏问题计算优化中的一个核心概念。\n\n### 第三步：结论与行动\n该问题有效。我将继续进行解答。\n\n### IRLS 更新的推导\n需要最小化的目标函数是：\n$$\nJ(x) = \\frac{1}{2}\\|Ax - y\\|_2^2 + \\lambda \\sum_{i=1}^n \\phi_\\varepsilon(x_i)\n$$\n其中 $\\phi_\\varepsilon(t) = \\sqrt{t^2 + \\varepsilon^2}$。IRLS 方法是主化-最小化（MM）算法的一个实例。我们需要为非二次惩罚项 $\\psi(x) = \\sum_{i=1}^n \\phi_\\varepsilon(x_i)$ 找到一个二次 majorizer。\n\n让我们关注单个分量 $\\phi_\\varepsilon(t)$。令 $g(u) = \\sqrt{u}$，对于 $u \\ge 0$。函数 $g(u)$ 是凹函数。对于任何凹函数，其在任意点的切线都位于函数图像之上。这给出了不等式：\n$$\ng(u) \\le g(u_k) + g'(u_k)(u - u_k)\n$$\n其中 $u_k$ 是切点。导数为 $g'(u) = \\frac{1}{2\\sqrt{u}}$。\n\n我们将此不等式应用于 $\\phi_\\varepsilon(x_i) = \\sqrt{x_i^2 + \\varepsilon^2}$。我们设 $u = x_i^2 + \\varepsilon^2$。在当前迭代点 $x^{(k)}$，我们定义 $u_k = (x_i^{(k)})^2 + \\varepsilon^2$。\n代入不等式，我们得到 $\\phi_\\varepsilon(x_i)$ 的一个 majorizer：\n\\begin{align*}\n\\phi_\\varepsilon(x_i) = \\sqrt{x_i^2 + \\varepsilon^2} \\le \\sqrt{(x_i^{(k)})^2 + \\varepsilon^2} + \\frac{1}{2\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}} \\left( (x_i^2 + \\varepsilon^2) - ((x_i^{(k)})^2 + \\varepsilon^2) \\right) \\\\\n\\le \\phi_\\varepsilon(x_i^{(k)}) + \\frac{1}{2\\phi_\\varepsilon(x_i^{(k)})} (x_i^2 - (x_i^{(k)})^2)\n\\end{align*}\n这个不等式对所有 $x_i$ 成立，并在 $x_i = x_i^{(k)}$ 时取等号。右侧是 $x_i$ 的二次函数，并作为 majorizer。\n\n对所有分量 $i=1, \\dots, n$求和并乘以 $\\lambda$，我们得到整个惩罚项的一个 majorizer：\n$$\n\\lambda \\sum_{i=1}^n \\phi_\\varepsilon(x_i) \\le \\lambda \\sum_{i=1}^n \\left( \\phi_\\varepsilon(x_i^{(k)}) + \\frac{1}{2\\phi_\\varepsilon(x_i^{(k)})} (x_i^2 - (x_i^{(k)})^2) \\right)\n$$\n因此，完整的目标函数 $J(x)$ 被一个二次代理函数 $Q(x, x^{(k)})$ 所主化：\n$$\nJ(x) \\le Q(x, x^{(k)}) = \\frac{1}{2}\\|Ax - y\\|_2^2 + \\frac{\\lambda}{2} \\sum_{i=1}^n \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} x_i^2 + C(x^{(k)})\n$$\n其中 $C(x^{(k)})$ 汇集了所有相对于 $x$ 是常数的项。\n下一个迭代点 $x^{(k+1)}$ 通过最小化这个代理函数得到：\n$$\nx^{(k+1)} = \\arg\\min_x Q(x, x^{(k)}) = \\arg\\min_x \\left( \\frac{1}{2}\\|Ax - y\\|_2^2 + \\frac{\\lambda}{2} \\sum_{i=1}^n w_i^{(k)} x_i^2 \\right)\n$$\n其中我们定义权重 $w_i^{(k)}$ 为：\n$$\nw_i^{(k)} = \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} = \\frac{1}{\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}}\n$$\n令 $W^{(k)}$ 为对角线元素为 $w_i^{(k)}$ 的对角矩阵。项 $\\sum_{i=1}^n w_i^{(k)} x_i^2$可以写成 $x^T W^{(k)} x$。最小化问题变为：\n$$\nx^{(k+1)} = \\arg\\min_x \\left( \\frac{1}{2}(Ax - y)^T(Ax-y) + \\frac{\\lambda}{2} x^T W^{(k)} x \\right)\n$$\n这是一个关于 $x$ 的二次函数。为了找到最小值，我们将其关于 $x$ 的梯度设为零：\n$$\n\\nabla_x \\left( \\frac{1}{2}(x^T A^T A x - 2y^T A x + y^T y) + \\frac{\\lambda}{2} x^T W^{(k)} x \\right) = 0\n$$\n$$\nA^T A x - A^T y + \\lambda W^{(k)} x = 0\n$$\n整理后得到 $x = x^{(k+1)}$ 的加权正规方程：\n$$\n(A^T A + \\lambda W^{(k)}) x^{(k+1)} = A^T y\n$$\n\n### 将权重解释为曲率\n问题要求证明权重对应于代理函数的曲率。这里的“代理函数”指的是主化函数。\n对于惩罚项的第 $i$ 个分量 $\\lambda \\phi_\\varepsilon(x_i)$，我们构建了二次 majorizer（代理函数）：\n$$\nM_i(x_i; x_i^{(k)}) = \\text{const}(x_i^{(k)}) + \\frac{\\lambda}{2} \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} x_i^2 = \\text{const}(x_i^{(k)}) + \\frac{\\lambda}{2} w_i^{(k)} x_i^2\n$$\n一维函数的曲率与其二阶导数有关。这个二次代理函数关于 $x_i$ 的二阶导数是：\n$$\n\\frac{d^2}{dx_i^2} M_i(x_i; x_i^{(k)}) = \\lambda w_i^{(k)}\n$$\n因此，权重 $w_i^{(k)}$ 与第 $k$ 次迭代中用于第 $i$ 个惩罚项的二次代理函数的曲率（二阶导数）成正比（比例常数为 $\\lambda$）。\n\n### 数值计算：一次 IRLS 迭代\n给定：\n$A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，$\\lambda = 0.1$，$\\varepsilon = 10^{-3}$，以及 $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n\n**1. 计算权重 $w^{(0)}$ 和矩阵 $W^{(0)}$：**\n权重为 $w_i^{(k)} = 1/\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}$。对于 $k=0$ 和 $x^{(0)} = \\begin{bmatrix} 0  0  0 \\end{bmatrix}^T$：\n$$\nw_1^{(0)} = w_2^{(0)} = w_3^{(0)} = \\frac{1}{\\sqrt{0^2 + (10^{-3})^2}} = \\frac{1}{10^{-3}} = 1000\n$$\n所以，$w^{(0)} = \\begin{bmatrix} 1000 \\\\ 1000 \\\\ 1000 \\end{bmatrix}$，且 $W^{(0)} = \\text{diag}(1000, 1000, 1000) = 1000 I_3$。\n\n**2. 求解 $x^{(1)}$：**\n我们必须求解方程组 $(A^T A + \\lambda W^{(0)}) x^{(1)} = A^T y$。\n首先，计算各个部分：\n$$\nA^T = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}\n$$\n$$\nA^T A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix}\n$$\n$$\nA^T y = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\n$$\n$$\n\\lambda W^{(0)} = 0.1 \\times 1000 I_3 = 100 I_3 = \\begin{bmatrix} 100  0  0 \\\\ 0  100  0 \\\\ 0  0  100 \\end{bmatrix}\n$$\n方程组的矩阵是：\n$$\nA^T A + \\lambda W^{(0)} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix} + \\begin{bmatrix} 100  0  0 \\\\ 0  100  0 \\\\ 0  0  100 \\end{bmatrix} = \\begin{bmatrix} 101  0  1 \\\\ 0  101  1 \\\\ 1  1  102 \\end{bmatrix}\n$$\n线性系统是：\n$$\n\\begin{bmatrix} 101  0  1 \\\\ 0  101  1 \\\\ 1  1  102 \\end{bmatrix} \\begin{bmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\\\ x_3^{(1)} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\n$$\n从前两行得到：\n$101 x_1^{(1)} + x_3^{(1)} = 1 \\implies x_1^{(1)} = (1 - x_3^{(1)}) / 101$\n$101 x_2^{(1)} + x_3^{(1)} = 1 \\implies x_2^{(1)} = (1 - x_3^{(1)}) / 101$\n这表明 $x_1^{(1)} = x_2^{(1)}$。\n将这些代入第三个方程：\n$x_1^{(1)} + x_2^{(1)} + 102 x_3^{(1)} = 2$\n$2 \\left( \\frac{1 - x_3^{(1)}}{101} \\right) + 102 x_3^{(1)} = 2$\n$2(1 - x_3^{(1)}) + 101 \\times 102 x_3^{(1)} = 2 \\times 101$\n$2 - 2x_3^{(1)} + 10302 x_3^{(1)} = 202$\n$10300 x_3^{(1)} = 200 \\implies x_3^{(1)} = \\frac{200}{10300} = \\frac{2}{103}$\n现在，我们求解 $x_1^{(1)}$ 和 $x_2^{(1)}$：\n$x_1^{(1)} = x_2^{(1)} = \\frac{1 - 2/103}{101} = \\frac{(103-2)/103}{101} = \\frac{101/103}{101} = \\frac{1}{103}$\n所以，$x^{(1)} = \\begin{bmatrix} 1/103 \\\\ 1/103 \\\\ 2/103 \\end{bmatrix}$。\n\n**3. 从 $x^{(1)}$ 评估 $w^{(1)}$：**\n使用 $x^{(1)}$ 和 $\\varepsilon = 10^{-3} = 1/1000$：\n$w_1^{(1)} = \\frac{1}{\\sqrt{(x_1^{(1)})^2 + \\varepsilon^2}} = \\frac{1}{\\sqrt{(1/103)^2 + (1/1000)^2}} = \\frac{1}{\\sqrt{\\frac{1}{103^2} + \\frac{1}{1000^2}}} = \\frac{1}{\\sqrt{\\frac{1000^2 + 103^2}{103^2 \\cdot 1000^2}}} = \\frac{103 \\cdot 1000}{\\sqrt{1000^2 + 103^2}}$\n$103^2 = 10609$，$1000^2 = 1000000$。\n$w_1^{(1)} = \\frac{103000}{\\sqrt{1000000 + 10609}} = \\frac{103000}{\\sqrt{1010609}}$。\n\n由于 $x_2^{(1)} = x_1^{(1)}$，所以 $w_2^{(1)} = w_1^{(1)} = \\frac{103000}{\\sqrt{1010609}}$。\n\n对于 $w_3^{(1)}$：\n$w_3^{(1)} = \\frac{1}{\\sqrt{(x_3^{(1)})^2 + \\varepsilon^2}} = \\frac{1}{\\sqrt{(2/103)^2 + (1/1000)^2}} = \\frac{1}{\\sqrt{\\frac{4}{103^2} + \\frac{1}{1000^2}}} = \\frac{103 \\cdot 1000}{\\sqrt{4 \\cdot 1000^2 + 103^2}}$\n$w_3^{(1)} = \\frac{103000}{\\sqrt{4000000 + 10609}} = \\frac{103000}{\\sqrt{4010609}}$。\n\n最终答案所需的各分量为：\n$x_1^{(1)} = \\frac{1}{103}$，$x_2^{(1)} = \\frac{1}{103}$，$x_3^{(1)} = \\frac{2}{103}$\n$w_1^{(1)} = \\frac{103000}{\\sqrt{1010609}}$，$w_2^{(1)} = \\frac{103000}{\\sqrt{1010609}}$，$w_3^{(1)} = \\frac{103000}{\\sqrt{4010609}}$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{103}  \\frac{1}{103}  \\frac{2}{103}  \\frac{103000}{\\sqrt{1010609}}  \\frac{103000}{\\sqrt{1010609}}  \\frac{103000}{\\sqrt{4010609}} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "IRLS 算法在求解非凸 $\\ell_p$ 问题时威力强大，但它也伴随着一个重要的警示：并不能保证收敛到全局最优（即最稀疏）的解。本练习展示了一个精心构建的反例，其中由于对称的初始化，IRLS 算法会收敛到一个非稀疏的稳定点，尽管存在一个更稀疏的解。通过分析这种失效模式，您将更深刻地体会到问题的非凸性以及初始化所扮演的关键角色。",
            "id": "3454768",
            "problem": "考虑一个压缩感知中的线性传感模型，其测量矩阵为 $A \\in \\mathbb{R}^{1 \\times 2}$，未知信号为 $x \\in \\mathbb{R}^{2}$，测量值为 $y \\in \\mathbb{R}$，由 $y = A x$ 给出。设 $A = [\\,1 \\;\\; 1\\,]$，并假设数据是无噪声的，其真实值为 $x^{\\mathrm{true}} = (1,0)^{\\top}$，因此 $y = A x^{\\mathrm{true}} = 1$。对于阶数 $k$ 定义的限制等距性质 (Restricted Isometry Property, RIP)，即存在一个常数 $\\delta_{k} \\in [0,1)$，使得对于每个 $k$-稀疏的 $x$ 都满足：\n$$(1 - \\delta_{k}) \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_{k}) \\|x\\|_{2}^{2},$$\n对于此处的矩阵 $A$，当 $k=2$ 时该性质不成立。我们考虑用于 $\\ell_{p}$ 最小化的迭代重加权最小二乘 (Iterative Reweighted Least Squares, IRLS) 算法，其中 $p \\in (0,1)$ 且平滑参数 $\\epsilon  0$。定义平滑 $\\ell_{p}$ 目标函数为\n$$J_{p,\\epsilon}(x) = \\sum_{i=1}^{2} \\big(x_{i}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2}}。$$\n在第 $k$ 次迭代中，IRLS 方案根据 $x^{(k)}$ 选择权重 $w_{i}^{(k)}$，然后将 $x^{(k+1)}$ 计算为约束加权最小二乘子问题的解，\n$$\\min_{x \\in \\mathbb{R}^{2}} \\sum_{i=1}^{2} w_{i}^{(k)} x_{i}^{2} \\quad \\text{满足} \\quad A x = y,$$\n其中正权重遵循关于 $|x_{i}^{(k)}|$ 的标准单调性，例如 $w_{i}^{(k)} = \\big(x_{i}^{(k)}{}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2} - 1}$。在对称点 $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 初始化 IRLS 算法。\n\n仅使用基本定义和平滑约束优化的一阶最优性条件，证明 IRLS 迭代保持在对称点，因此收敛到平滑目标函数 $J_{p,\\epsilon}$ 的一个稳定点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$，尽管存在一个更稀疏的可行点 $x^{\\mathrm{true}} = (1,0)^{\\top}$。构建上述显式反例，并为量化其次优性，计算成本差距的精确解析表达式\n$$\\Delta(p,\\epsilon) = J_{p,\\epsilon}\\!\\big(x^{\\star}\\big) - J_{p,\\epsilon}\\!\\big(x^{\\mathrm{true}}\\big),$$\n作为 $p \\in (0,1)$ 和 $\\epsilon  0$ 的闭式函数。你的最终答案必须是这个单一的表达式。不需要四舍五入，也没有物理单位。",
            "solution": "该问题被验证为自洽、有科学依据且定义明确。所有必要信息都已提供，没有内部矛盾。我们开始求解。\n\n问题要求我们分析迭代重加权最小二乘 (IRLS) 算法在特定 $\\ell_{p}$ 最小化实例中的行为。我们给定一个线性系统 $y = Ax$，其中 $A = [\\,1 \\;\\; 1\\,]$，$x \\in \\mathbb{R}^{2}$，且 $y = 1$。IRLS 算法试图通过最小化平滑 $\\ell_{p}$ 目标函数 $J_{p,\\epsilon}(x) = \\sum_{i=1}^{2} \\big(x_{i}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2}}$ 来找到一个稀疏解，其中 $p \\in (0,1)$ 且平滑参数 $\\epsilon  0$，并满足约束条件 $Ax = y$。\n\n算法在可行点 $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 初始化，因为 $A x^{(0)} = 1 \\cdot \\tfrac{1}{2} + 1 \\cdot \\tfrac{1}{2} = 1 = y$。在每次迭代 $k$ 中，通过求解加权最小二乘问题找到下一个迭代点 $x^{(k+1)}$：\n$$x^{(k+1)} = \\arg\\min_{x \\in \\mathbb{R}^{2}} \\sum_{i=1}^{2} w_{i}^{(k)} x_{i}^{2} \\quad \\text{满足} \\quad A x = y$$\n其中权重由 $w_{i}^{(k)} = \\big((x_{i}^{(k)})^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2} - 1}$ 给出。\n\n首先，我们用数学归纳法证明迭代点保持在对称点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n基础情况已给出：$x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n\n对于归纳步骤，假设对于某个 $k \\ge 0$，我们有 $x^{(k)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。我们必须计算 $x^{(k+1)}$。\n首先，我们根据 $x^{(k)}$ 计算权重 $w_i^{(k)}$：\n$$x_{1}^{(k)} = x_{2}^{(k)} = \\frac{1}{2}$$\n因此，权重是相同的：\n$$w_{1}^{(k)} = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1} = \\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1}$$\n$$w_{2}^{(k)} = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1} = \\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1}$$\n令 $w^{(k)} = w_{1}^{(k)} = w_{2}^{(k)}$。由于 $p \\in (0,1)$ 且 $\\epsilon  0$，幂的底数为正，因此权重 $w^{(k)}$ 是良定义且为正的。\n\n现在，我们求解关于 $x^{(k+1)} = (x_1, x_2)^\\top$ 的子问题：\n$$\\min_{x_1, x_2} w^{(k)} x_{1}^{2} + w^{(k)} x_{2}^{2} \\quad \\text{满足} \\quad x_{1} + x_{2} = 1$$\n由于 $w^{(k)}  0$，这等价于最小化 $x_{1}^{2} + x_{2}^{2}$。这是一个经典问题，即在线 $x_1+x_2=1$ 上寻找欧几里得范数最小的点。我们可以使用拉格朗日乘数法。拉格朗日函数是：\n$$\\mathcal{L}(x_1, x_2, \\lambda) = x_{1}^{2} + x_{2}^{2} + \\lambda(x_{1} + x_{2} - 1)$$\n求偏导数并令其为零，得到一阶最优性条件：\n$$\\frac{\\partial \\mathcal{L}}{\\partial x_1} = 2x_{1} + \\lambda = 0 \\implies x_1 = -\\frac{\\lambda}{2}$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial x_2} = 2x_{2} + \\lambda = 0 \\implies x_2 = -\\frac{\\lambda}{2}$$\n这意味着 $x_1 = x_2$。将此代入约束条件 $x_1 + x_2 = 1$，我们得到：\n$$x_1 + x_1 = 2x_1 = 1 \\implies x_1 = \\frac{1}{2}$$\n因此，唯一解是 $x_1 = \\frac{1}{2}$ 和 $x_2 = \\frac{1}{2}$。\n所以，$x^{(k+1)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n根据数学归纳法，对所有 $k \\ge 0$，$x^{(k)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。迭代序列是常数序列，因此收敛于 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n\n接下来，我们必须证明 $x^{\\star}$ 是带约束的平滑目标函数 $J_{p,\\epsilon}(x)$ 的一个稳定点。如果一个点满足 Karush-Kuhn-Tucker (KKT) 条件，那么它就是稳定点。对于等式约束问题 $\\min f(x)$ s.t. $g(x)=0$，这意味着存在一个拉格朗日乘数 $\\nu$，使得 $\\nabla f(x^{\\star}) + \\nu \\nabla g(x^{\\star}) = 0$。\n这里，$f(x) = J_{p,\\epsilon}(x)$ 且 $g(x) = Ax-y = x_1+x_2-1$。\n目标函数的梯度是 $\\nabla J_{p,\\epsilon}(x) = \\begin{pmatrix} \\frac{\\partial J_{p,\\epsilon}}{\\partial x_1} \\\\ \\frac{\\partial J_{p,\\epsilon}}{\\partial x_2} \\end{pmatrix}$，其中\n$$\\frac{\\partial J_{p,\\epsilon}}{\\partial x_i} = p x_i \\left(x_i^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1}$$\n在点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$，梯度的两个分量相等：\n$$\\frac{\\partial J_{p,\\epsilon}}{\\partial x_1}\\bigg|_{x^{\\star}} = \\frac{\\partial J_{p,\\epsilon}}{\\partial x_2}\\bigg|_{x^{\\star}} = p \\left(\\frac{1}{2}\\right) \\left(\\left(\\frac{1}{2}\\right)^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1} = \\frac{p}{2} \\left(\\frac{1}{4} + \\epsilon^2\\right)^{\\frac{p}{2}-1}$$\n设这个共同的值为 $C$。那么，$\\nabla J_{p,\\epsilon}(x^{\\star}) = (C, C)^{\\top}$。\n约束的梯度是 $\\nabla g(x) = A^{\\top} = (1, 1)^{\\top}$。\nKKT 条件为 $\\nabla J_{p,\\epsilon}(x^{\\star}) + \\nu A^{\\top} = 0$：\n$$\\begin{pmatrix} C \\\\ C \\end{pmatrix} + \\nu \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n通过选择 $\\nu = -C$ 可以满足该方程组。由于存在这样的拉格朗日乘数，$x^{\\star}$ 是该约束问题的一个稳定点。这证实了 IRLS 算法收敛到一个稳定点，但在本例中，这个稳定点不是最稀疏的可能解 $x^{\\mathrm{true}} = (1,0)^{\\top}$。\n\n最后，我们计算成本差距 $\\Delta(p,\\epsilon) = J_{p,\\epsilon}(x^{\\star}) - J_{p,\\epsilon}(x^{\\mathrm{true}})$。\n我们在稳定点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 处计算目标函数的值：\n$$J_{p,\\epsilon}(x^{\\star}) = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = 2\\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2}}$$\n我们可以简化此项：\n$$2\\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = 2\\left(\\frac{1+4\\epsilon^{2}}{4}\\right)^{\\frac{p}{2}} = 2 \\frac{\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}}{4^{\\frac{p}{2}}} = 2 \\frac{\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}}{2^{p}} = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}$$\n接下来，我们在稀疏的真实值点 $x^{\\mathrm{true}} = (1,0)^{\\top}$ 处计算目标函数的值：\n$$J_{p,\\epsilon}(x^{\\mathrm{true}}) = \\left(1^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(0^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(\\epsilon^{2}\\right)^{\\frac{p}{2}}$$\n由于 $\\epsilon  0$，$(\\epsilon^2)^{p/2} = (\\epsilon^p \\epsilon^p)^{1/2} = \\sqrt{\\epsilon^{2p}} = |\\epsilon^p| = \\epsilon^p$ (因为 $\\epsilon  0$）。\n所以，$J_{p,\\epsilon}(x^{\\mathrm{true}}) = \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\epsilon^{p}$。\n\n成本差距 $\\Delta(p,\\epsilon)$ 是两者之差：\n$$\\Delta(p,\\epsilon) = J_{p,\\epsilon}(x^{\\star}) - J_{p,\\epsilon}(x^{\\mathrm{true}}) = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}} - \\left( \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\epsilon^{p} \\right)$$\n$$\\Delta(p,\\epsilon) = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}} - \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} - \\epsilon^{p}$$\n这就是次优性差距的最终解析表达式。",
            "answer": "$$\\boxed{2^{1-p}(1+4\\epsilon^2)^{\\frac{p}{2}} - (1+\\epsilon^2)^{\\frac{p}{2}} - \\epsilon^{p}}$$"
        },
        {
            "introduction": "从理论理解到实际应用是掌握任何数值方法的关键一步。本练习将挑战您实现完整的 IRLS 算法，并研究“连续化”（continuation）策略——这是一种在 $\\ell_p$ 最小化这类复杂的非凸问题中寻找更优解的强大技术。通过比较对稀疏度参数 $p$ 和正则化参数 $\\lambda$ 进行连续化的效果，您将通过实验发现，哪种方法在面对具有挑战性的传感矩阵时，能更可靠地避开我们之前讨论过的不良局部最小值。",
            "id": "3454798",
            "problem": "考虑一个线性逆问题，其中有一个欠定传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个未知的 $k$-稀疏信号 $x^{\\star} \\in \\mathbb{R}^{n}$。带噪观测由 $b = A x^{\\star} + \\eta$ 给出，其中 $\\eta \\in \\mathbb{R}^{m}$ 模拟加性噪声。目标是通过求解一个非凸正则化最小二乘问题来恢复 $x^{\\star}$，该问题通过一个 $0  p  1$ 的 $\\ell_{p}$ 拟范数来利用 $x^{\\star}$ 的稀疏性。具体来说，考虑以下目标函数\n$$\nF(x; p, \\lambda, \\varepsilon) = \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\left(|x_{i}|^{2} + \\varepsilon\\right)^{\\frac{p}{2}},\n$$\n其中 $p \\in (0, 1]$ 控制诱导稀疏性的非凸性，$\\lambda  0$ 是一个正则化参数，$\\varepsilon  0$ 是一个小的平滑常数，以确保可微性和数值稳定性。\n\n要求您设计、推导并实现一个迭代重加权最小二乘（IRLS）方法，该方法基于一个原则性的主化-最小化构造来最小化 $F(x; p, \\lambda, \\varepsilon)$，并经验性地比较两种连续化策略：\n- $p$ 值的连续化：从 $p = 1$ 开始，在保持 $\\lambda$ 固定的情况下单调递减至目标值 $p_{\\min}$，并在各阶段之间使用热启动。\n- $\\lambda$ 值的连续化：从一个大的 $\\lambda_{\\max}$ 开始，在保持 $p$ 固定于目标值 $p_{\\min}$ 的情况下单调递减至目标值 $\\lambda_{\\min}$，并在各阶段之间使用热启动。\n\n您的比较必须在一套传感矩阵测试集上进行，这些矩阵具有经验性计算的限制等距性质（RIP）常数。对于给定的 $A$ 和整数 $k \\geq 1$，限制等距常数 $\\delta_{k}(A)$ 定义为满足以下条件的最小 $\\delta \\geq 0$：对于所有满足 $|S| \\leq k$ 的支撑集 $S \\subset \\{1,\\dots,n\\}$ 和所有 $z \\in \\mathbb{R}^{|S|}$，\n$$\n(1 - \\delta) \\|z\\|_{2}^{2} \\leq \\|A_{S} z\\|_{2}^{2} \\leq (1 + \\delta) \\|z\\|_{2}^{2},\n$$\n其中 $A_{S} \\in \\mathbb{R}^{m \\times |S|}$ 表示由 $S$ 索引的列构成的 $A$ 的子矩阵。对于小的 $n$ 和中等的 $k$，可以通过枚举所有大小为 $k$ 的支撑集 $S$ 并评估 $A_{S}^{\\top} A_{S}$ 的极端特征值来精确计算 $\\delta_{k}(A)$。\n\n您必须推导一个基于凹罚项的主化-最小化原则的 IRLS 方案，该方案在每次迭代中用一个二次代理函数替换非凸项，从而得到一系列形如下式的加权最小二乘问题\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{\\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_{i}^{(t)} x_{i}^{2}\\right\\},\n$$\n其中权重 $w_{i}^{(t)}$ 依赖于迭代，由前一次的迭代结果 $x^{(t)}$ 决定。推导必须从 $\\ell_{p}$ 拟范数和函数 $u \\mapsto (u^{2} + \\varepsilon)^{\\frac{p}{2}}$（对于 $0  p \\leq 1$）的凹性的基本定义开始，并逻辑地构建一个有效的主化-最小化代理函数以及相应的加权最小二乘更新的法方程。\n\n定义一个相对于 $x^{\\star}$ 的“差的局部最小值”概念：如果一个计算出的解 $\\hat{x}$ 的重构误差 $\\|\\hat{x} - x^{\\star}\\|_{2}$ 比竞争路径的重构误差严格大出一个边际量 $\\eta  0$（选择为一个小的数值容差），则称该解为“差的”。您必须通过计算两种策略产生的最终解的重构误差来比较这两条连续化路径，并声明哪条路径在每个测试案例中更可靠地避免了差的局部最小值。具体来说，对于每个测试案例，输出：\n- $1$，如果 $p$ 值的连续化产生的重构误差比 $\\lambda$ 值的连续化至少小 $\\eta$，\n- $-1$，如果 $\\lambda$ 值的连续化产生的重构误差比 $p$ 值的连续化至少小 $\\eta$，\n- $0$，如果两者均未比对方严格优越至少 $\\eta$。\n\n构建以下四个案例的测试套件，通过为任何伪随机数生成设置种子来确保科学真实性和可复现性。对于每个案例，将 $A$ 的列归一化为单位 $\\ell_{2}$ 范数，生成一个 $k$-稀疏向量 $x^{\\star}$，其非零项从零均值高斯分布中抽取，并设置 $b = A x^{\\star} + \\eta$，其中 $\\eta$ 从指定方差的零均值高斯分布中抽取。对于每个测试案例，通过枚举所有大小为 $k$ 的支撑集 $S$ 来精确计算 $\\delta_{k}(A)$。\n\n- 案例 1（理想情况，良好的限制等距性质（RIP））：\n  - $m = 10$, $n = 16$, $k = 3$，矩阵类型：独立同分布的高斯条目，经过缩放和列归一化，噪声标准差 $\\sigma = 10^{-3}$，种子 $= 1$，$p_{\\min} = 0.5$，$\\lambda_{\\min} = 0.05$，$\\lambda_{\\max} = 1.0$。\n- 案例 2（中等列相关性，中等的 RIP 常数）：\n  - $m = 10$, $n = 16$, $k = 3$，矩阵类型：通过在归一化前将前一列的一部分添加到当前列来构造相关列（相关性水平 $c = 0.5$），噪声标准差 $\\sigma = 10^{-3}$，种子 $= 2$，$p_{\\min} = 0.5$，$\\lambda_{\\min} = 0.05$，$\\lambda_{\\max} = 1.0$。\n- 案例 3（近乎重复的列，接近边界的 RIP 常数）：\n  - $m = 8$, $n = 16$, $k = 3$，矩阵类型：其中一列通过对另一列添加小扰动来构造成其近乎重复，噪声标准差 $\\sigma = 10^{-3}$，种子 $= 3$，$p_{\\min} = 0.5$，$\\lambda_{\\min} = 0.05$，$\\lambda_{\\max} = 1.0$。\n- 案例 4（更欠定的情况，更低的 $p$ 值）：\n  - $m = 6$, $n = 16$, $k = 2$，矩阵类型：独立同分布的高斯条目，经过缩放和列归一化，噪声标准差 $\\sigma = 10^{-3}$，种子 $= 4$，$p_{\\min} = 0.3$，$\\lambda_{\\min} = 0.02$，$\\lambda_{\\max} = 1.0$。\n\n实现要求：\n- 在目标函数中使用固定的平滑参数 $\\varepsilon = 10^{-8}$。\n- 在连续化阶段之间使用热启动，即将前一阶段的最终迭代结果作为下一阶段的初始点。\n- 对 IRLS 内循环使用停止准则，基于迭代值的相对变化，容差为 $\\tau = 10^{-8}$，且每个阶段最多 100 次迭代。\n- 对每条路径使用包含 8 个阶段的连续化方案。对于 $p$ 值的连续化，使用从 $1$ 到 $p_{\\min}$ 的单调递减序列。对于 $\\lambda$ 值的连续化，使用从 $\\lambda_{\\max}$到 $\\lambda_{\\min}$ 的单调递减的几何序列。\n\n角度单位不适用。不涉及物理单位。所有数值输出必须是无量纲实数。用于声明严格改进的“边际量”必须设置为 $\\eta = 10^{-6}$。\n\n最终输出规范：\n您的程序应生成单行输出，其中包含一个逗号分隔的整数列表（每个整数在 $\\{-1, 0, 1\\}$ 中），并用方括号括起来，按顺序对应于四个测试案例（案例 1，案例 2，案例 3，案例 4）。例如，输出行可能看起来像 $\\texttt{[1,0,-1,1]}$，但实际值必须由您的实现计算得出。",
            "solution": "该问题要求设计、推导和实现一个迭代重加权最小二乘（IRLS）算法，以解决用于稀疏信号恢复的非凸正则化最小二乘问题。推导必须基于主化-最小化（MM）原则。随后，必须在一个定义的测试套件上实现并比较两种连续化策略，一种针对诱导稀疏性的参数 $p$，另一种针对正则化参数 $\\lambda$。\n\n需要最小化的目标函数是：\n$$\nF(x; p, \\lambda, \\varepsilon) = \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\left(|x_{i}|^{2} + \\varepsilon\\right)^{\\frac{p}{2}}\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 是待恢复的信号，$A \\in \\mathbb{R}^{m \\times n}$ 是传感矩阵，$b \\in \\mathbb{R}^{m}$ 是观测值，$\\lambda  0$ 是正则化参数，$p \\in (0, 1]$ 控制正则化项的非凸性，$\\varepsilon  0$ 是一个平滑参数。\n\n函数 $F(x)$ 由两部分组成：一个凸的数据保真项 $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ 和一个非凸的正则化项 $\\Phi(x) = \\lambda \\sum_{i=1}^{n} \\phi(x_i)$，其中 $\\phi(x_i) = (|x_i|^2 + \\varepsilon)^{p/2}$。IRLS 算法是通过将 MM 原则应用于非凸项 $\\Phi(x)$ 推导出来的。\n\nMM 的核心思想是用一系列更易于最小化的简单代理函数来替代一个难以最小化的目标函数。在每次迭代 $t$ 时，我们构建一个代理函数 $G(x | x^{(t)})$，它在当前迭代点 $x^{(t)}$ 上主化了原始函数 $F(x)$，即对于所有 $x$ 都有 $G(x | x^{(t)}) \\ge F(x)$，并且 $G(x^{(t)} | x^{(t)}) = F(x^{(t)})$。然后通过最小化这个代理函数来找到下一个迭代点：$x^{(t+1)} = \\arg\\min_x G(x | x^{(t)})$。\n\n我们将为非凸惩罚项 $\\Phi(x)$ 构建一个代理函数。数据保真项 $f(x)$ 已经是凸和二次的，所以我们可以保持它不变。我们关注单个惩罚函数 $\\phi(x_i) = (|x_i|^2 + \\varepsilon)^{p/2}$。我们引入变量代换 $u_i = x_i^2$。每个分量上的惩罚成为 $u_i \\ge 0$ 的函数：\n$$g(u_i) = (u_i + \\varepsilon)^{p/2}$$\n为了找到 $\\phi(x_i)$ 的二次主化函数，我们首先找到 $g(u_i)$ 的线性主化函数。我们研究当 $u_i \\ge 0$ 时 $g(u_i)$ 的凹性。$g$ 关于 $u_i$ 的一阶和二阶导数是：\n$$\ng'(u_i) = \\frac{p}{2} (u_i + \\varepsilon)^{\\frac{p}{2} - 1}\n$$\n$$\ng''(u_i) = \\frac{p}{2} \\left(\\frac{p}{2} - 1\\right) (u_i + \\varepsilon)^{\\frac{p}{2} - 2}\n$$\n对于 $p \\in (0, 1]$，我们有 $p/2  0$ 且 $(p/2 - 1) \\le -1/2$。由于 $u_i \\ge 0$ 且 $\\varepsilon  0$，项 $(u_i + \\varepsilon)$ 总是正的。因此，$g''(u_i) \\le 0$，这证明了 $g(u_i)$ 是 $u_i$ 的凹函数。\n\n对于任何凹且可微的函数，其在任意点的一阶泰勒展开（即切线）提供了该函数的一个上界。令 $u_i^{(t)} = (x_i^{(t)})^2$ 为当前迭代点的值。那么，对于所有 $u_i \\ge 0$：\n$$g(u_i) \\le g(u_i^{(t)}) + g'(u_i^{(t)}) (u_i - u_i^{(t)})$$\n将 $u_i = x_i^2$ 代回，我们得到 $\\phi(x_i)$ 的一个主化函数：\n$$\n(|x_i|^2 + \\varepsilon)^{p/2} \\le ((x_i^{(t)})^2 + \\varepsilon)^{p/2} + \\frac{p}{2} ((x_i^{(t)})^2 + \\varepsilon)^{\\frac{p}{2} - 1} (x_i^2 - (x_i^{(t)})^2)\n$$\n这个主化函数是 $x_i$ 的二次函数，外加一些相对于 $x_i$ 是常数的项。我们定义权重 $w_i^{(t)}$ 为：\n$$\nw_i^{(t)} = \\frac{p}{2} \\left((x_i^{(t)})^2 + \\varepsilon\\right)^{\\frac{p}{2} - 1}\n$$\n$\\phi(x_i)$ 的主化不等式可以写成：\n$$\n\\phi(x_i) \\le w_i^{(t)} x_i^2 + C_i^{(t)}\n$$\n其中 $C_i^{(t)} = \\phi(x_i^{(t)}) - w_i^{(t)} (x_i^{(t)})^2$ 是相对于 $x$ 的常数。\n\n对所有分量求和并加上数据保真项，我们得到完整目标函数 $F(x)$ 的主化代理函数：\n$$\nG(x | x^{(t)}) = \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\left( w_i^{(t)} x_i^2 + C_i^{(t)} \\right)\n$$\n为了找到下一个迭代点 $x^{(t+1)}$，我们最小化这个关于 $x$ 的代理函数：\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\mathbb{R}^{n}} G(x | x^{(t)}) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_i^{(t)} x_i^2 \\right\\}\n$$\n这是一个加权最小二乘问题，也称为 Tikhonov 正则化或岭回归，其权重依赖于前一次的迭代结果 $x^{(t)}$。目标函数是 $x$ 的二次函数，可以写成：\n$$\nJ(x) = \\frac{1}{2}(Ax - b)^T(Ax - b) + \\lambda x^T W^{(t)} x\n$$\n其中 $W^{(t)}$ 是一个对角矩阵，其对角线上的元素是权重 $w_i^{(t)}$。为了找到最小值，我们将关于 $x$ 的梯度设为零：\n$$\n\\nabla_x J(x) = A^T(Ax - b) + 2\\lambda W^{(t)} x = 0\n$$\n这就得出了更新的法方程：\n$$\n\\left(A^T A + 2\\lambda W^{(t)}\\right) x = A^T b\n$$\n下一个迭代点 $x^{(t+1)}$ 是这个线性系统的解：\n$$\nx^{(t+1)} = \\left(A^T A + 2\\lambda W^{(t)}\\right)^{-1} A^T b\n$$\n这构成了 IRLS 算法的核心。该过程从一个初始猜测 $x^{(0)}$（例如，$x^{(0)}=0$）开始，并通过计算权重和解决加权最小二乘问题来迭代地优化解，直到满足收敛准则。\n\n需要比较的两种连续化策略是：\n1.  **$p$ 值的连续化**：算法从 $p=1$（凸 $\\ell_1$ 情况）和固定的目标正则化参数 $\\lambda = \\lambda_{\\min}$ 开始。$p$ 的值在一系列阶段中逐渐减小，趋向目标值 $p_{\\min}$。每个阶段的解都用作下一阶段的热启动。该策略旨在通过从凸环境中开始并逐渐引入非凸性，来引导迭代避开差的局部最小值。\n2.  **$\\lambda$ 值的连续化**：算法从一个大的正则化参数 $\\lambda = \\lambda_{\\max}$ 和固定的目标非凸性参数 $p = p_{\\min}$ 开始。$\\lambda$ 的值逐渐减小，趋向目标值 $\\lambda_{\\min}$。一个大的初始 $\\lambda$ 会促进解的量级非常小，通常接近于零，并且可以有效地在早期选出正确的稀疏支撑集。随着 $\\lambda$ 的减小，非零分量的量级会得到优化。\n\n比较在四个具有不同传感矩阵属性的测试案例上进行，这些属性部分由它们的限制等距性质（RIP）常数 $\\delta_k$ 来表征。每条路径的最终解都与真实值 $x^{\\star}$ 进行比较，并且在该案例中，产生至少小 $\\eta=10^{-6}$ 的重构误差的路径被认为是更优的。",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main solver function to perform the comparison of continuation strategies.\n    \"\"\"\n\n    def compute_rip_constant(A, k):\n        \"\"\"\n        Computes the exact RIP constant delta_k(A) by enumerating all C(n,k) submatrices.\n        This function is for characterization and not strictly required for the final output,\n        but is included as per the problem's instruction to compute it.\n        \"\"\"\n        n = A.shape[1]\n        all_supports = combinations(range(n), k)\n        \n        min_lambda_min = float('inf')\n        max_lambda_max = float('-inf')\n        \n        for support in all_supports:\n            S = list(support)\n            A_S = A[:, S]\n            A_S_T_A_S = A_S.T @ A_S\n            \n            try:\n                # Use eigh for symmetric matrices\n                eigvals = scipy.linalg.eigh(A_S_T_A_S, eigvals_only=True)\n                min_lambda_min = min(min_lambda_min, eigvals[0])\n                max_lambda_max = max(max_lambda_max, eigvals[-1])\n            except np.linalg.LinAlgError:\n                # This should not happen for A_S^T A_S if A_S has full column rank.\n                continue\n\n        delta_k = max(1 - min_lambda_min, max_lambda_max - 1)\n        return delta_k\n\n    def generate_matrix(m, n, matrix_type, params, rng):\n        \"\"\"Generates the sensing matrix A.\"\"\"\n        if matrix_type == 'gaussian':\n            A_raw = rng.standard_normal(size=(m, n))\n        elif matrix_type == 'correlated':\n            c = params['c']\n            A_raw = rng.standard_normal(size=(m, n))\n            for i in range(1, n):\n                A_raw[:, i] += c * A_raw[:, i - 1]\n        elif matrix_type == 'near_duplicate':\n            A_raw = rng.standard_normal(size=(m, n))\n            # Make column 1 a perturbed version of column 0\n            perturbation = rng.standard_normal(size=m) * 1e-5\n            A_raw[:, 1] = A_raw[:, 0] + perturbation\n        else:\n            raise ValueError(\"Unknown matrix type\")\n        \n        # Normalize columns\n        A = A_raw / np.linalg.norm(A_raw, axis=0, keepdims=True)\n        return A\n\n    def irls(A, b, p, lamb, epsilon, tau, max_iter, x_init):\n        \"\"\"\n        Performs Iterative Reweighted Least Squares to minimize the l_p objective.\n        \"\"\"\n        x = np.copy(x_init)\n        n = A.shape[1]\n        AtA = A.T @ A\n        Atb = A.T @ b\n\n        for _ in range(max_iter):\n            x_old = np.copy(x)\n            \n            # Calculate weights\n            weights = (p / 2) * ((x**2 + epsilon)**(p / 2 - 1))\n            W = np.diag(weights)\n            \n            # Solve the normal equations\n            try:\n                matrix_to_invert = AtA + 2 * lamb * W\n                x = np.linalg.solve(matrix_to_invert, Atb)\n            except np.linalg.LinAlgError:\n                # If matrix is singular, use a pseudo-inverse (more robust)\n                x = np.linalg.pinv(matrix_to_invert) @ Atb\n\n            # Check for convergence\n            norm_x_old = np.linalg.norm(x_old)\n            if norm_x_old > 1e-9: # Avoid division by zero\n                rel_change = np.linalg.norm(x - x_old) / norm_x_old\n                if rel_change  tau:\n                    break\n            elif np.linalg.norm(x - x_old)  tau:\n                break\n                \n        return x\n\n    def run_p_continuation(A, b, p_min, lambda_min, params):\n        \"\"\"Runs IRLS with continuation in p.\"\"\"\n        num_stages = params['num_stages']\n        p_schedule = np.linspace(1.0, p_min, num_stages)\n        x = np.zeros(A.shape[1])\n        \n        for p_val in p_schedule:\n            x = irls(A, b, p_val, lambda_min, params['epsilon'], params['tau'], params['max_iter'], x)\n            \n        return x\n\n    def run_lambda_continuation(A, b, p_min, lambda_min, lambda_max, params):\n        \"\"\"Runs IRLS with continuation in lambda.\"\"\"\n        num_stages = params['num_stages']\n        lambda_schedule = np.geomspace(lambda_max, lambda_min, num_stages)\n        x = np.zeros(A.shape[1])\n        \n        for lambda_val in lambda_schedule:\n            x = irls(A, b, p_min, lambda_val, params['epsilon'], params['tau'], params['max_iter'], x)\n            \n        return x\n\n    test_cases = [\n        {'m': 10, 'n': 16, 'k': 3, 'sigma': 1e-3, 'seed': 1, 'matrix_type': 'gaussian', 'matrix_params': {}, 'p_min': 0.5, 'lambda_min': 0.05, 'lambda_max': 1.0},\n        {'m': 10, 'n': 16, 'k': 3, 'sigma': 1e-3, 'seed': 2, 'matrix_type': 'correlated', 'matrix_params': {'c': 0.5}, 'p_min': 0.5, 'lambda_min': 0.05, 'lambda_max': 1.0},\n        {'m': 8, 'n': 16, 'k': 3, 'sigma': 1e-3, 'seed': 3, 'matrix_type': 'near_duplicate', 'matrix_params': {}, 'p_min': 0.5, 'lambda_min': 0.05, 'lambda_max': 1.0},\n        {'m': 6, 'n': 16, 'k': 2, 'sigma': 1e-3, 'seed': 4, 'matrix_type': 'gaussian', 'matrix_params': {}, 'p_min': 0.3, 'lambda_min': 0.02, 'lambda_max': 1.0}\n    ]\n\n    irls_params = {\n        'epsilon': 1e-8,\n        'tau': 1e-8,\n        'max_iter': 100,\n        'num_stages': 8,\n        'eta': 1e-6\n    }\n    \n    results = []\n\n    for case in test_cases:\n        m, n, k = case['m'], case['n'], case['k']\n        rng = np.random.default_rng(case['seed'])\n        \n        # Generate matrix A\n        A = generate_matrix(m, n, case['matrix_type'], case['matrix_params'], rng)\n        \n        # This is computed as specified, but not used in the final result.\n        # It characterizes the problem instance.\n        # delta_k = compute_rip_constant(A, k)\n        \n        # Generate sparse signal x_star\n        x_star = np.zeros(n)\n        support = rng.choice(n, k, replace=False)\n        x_star[support] = rng.standard_normal(size=k)\n        \n        # Generate noisy measurements b\n        noise = rng.normal(loc=0.0, scale=case['sigma'], size=m)\n        b = A @ x_star + noise\n        \n        # Run p-continuation\n        x_p = run_p_continuation(A, b, case['p_min'], case['lambda_min'], irls_params)\n        err_p = np.linalg.norm(x_p - x_star)\n        \n        # Run lambda-continuation\n        x_lambda = run_lambda_continuation(A, b, case['p_min'], case['lambda_min'], case['lambda_max'], irls_params)\n        err_lambda = np.linalg.norm(x_lambda - x_star)\n        \n        # Compare results\n        eta = irls_params['eta']\n        if err_p  err_lambda - eta:\n            result = 1\n        elif err_lambda  err_p - eta:\n            result = -1\n        else:\n            result = 0\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}