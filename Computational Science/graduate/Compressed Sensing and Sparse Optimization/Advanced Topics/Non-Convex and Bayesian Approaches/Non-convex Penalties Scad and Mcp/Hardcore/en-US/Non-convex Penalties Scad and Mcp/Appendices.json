{
    "hands_on_practices": [
        {
            "introduction": "At the heart of penalized regression methods like SCAD and MCP are their corresponding thresholding operators, which act as the solution to a simple one-dimensional optimization problem. These operators are defined by piecewise functions designed to apply different amounts of shrinkage depending on the magnitude of the input. This initial practice provides direct, hands-on experience with applying these definitions to a concrete numerical example, building essential intuition for their behavior and comparative performance .",
            "id": "3462713",
            "problem": "Consider a single-coordinate proximal update in penalized least squares for sparse estimation, where the estimator $T(z)$ minimizes the univariate objective $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$ with respect to $b \\in \\mathbb{R}$. The penalty $P$ is either the Smoothly Clipped Absolute Deviation (SCAD) penalty or the Minimax Concave Penalty (MCP), each parameterized by a regularization strength $\\lambda > 0$ and a shape parameter. Work in the standard coordinate-descent framework where the optimality condition for a differentiable branch is $b - z + \\partial P(b) = 0$, interpreting $\\partial P(b)$ as the derivative of $P$ with respect to $b$ on $b \\ge 0$ and using symmetry in the sign of $z$.\n\nUse the following fundamental definitions for the penalty derivatives on the nonnegative half-line:\n- For SCAD with parameters $(\\lambda,a)$ where $a > 2$, define for $t \\ge 0$ the derivative $p'_{\\text{SCAD}}(t)$ of $P_{\\text{SCAD}}(t)$ by\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda, & 0 \\le t \\le \\lambda, \\\\\n\\dfrac{a\\lambda - t}{a - 1}, & \\lambda < t \\le a\\lambda, \\\\\n0, & t > a\\lambda,\n\\end{cases}\n$$\nand extend it to $b < 0$ by odd symmetry of the subgradient in $b$.\n- For MCP with parameters $(\\lambda,\\gamma)$ where $\\gamma > 1$, define for $t \\ge 0$ the derivative $p'_{\\text{MCP}}(t)$ of $P_{\\text{MCP}}(t)$ by\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\dfrac{t}{\\gamma\\lambda}\\right), & 0 \\le t \\le \\gamma\\lambda, \\\\\n0, & t > \\gamma\\lambda,\n\\end{cases}\n$$\nand extend it to $b < 0$ by odd symmetry of the subgradient in $b$.\n\nTake the numerical instance $z = 3.2$, $\\lambda = 1.0$, $a = 3.7$, and $\\gamma = 2.5$. Compute explicitly the SCAD threshold $T_{\\text{SCAD}}(z)$ and the MCP threshold $T_{\\text{MCP}}(z)$, understood as the minimizers of $\\frac{1}{2}(b - z)^{2} + P(|b|)$ under the respective penalties. Then, assuming the true underlying coefficient is zero, compare the two estimators under squared loss by evaluating $\\left(T_{\\text{SCAD}}(z) - 0\\right)^{2}$ and $\\left(T_{\\text{MCP}}(z) - 0\\right)^{2}$. Provide the value of the difference\n$$\n\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}.\n$$\n\nYour final answer must be the row matrix containing the three quantities $T_{\\text{SCAD}}(z)$, $T_{\\text{MCP}}(z)$, and $\\Delta$, in that order, written as exact values without rounding.",
            "solution": "The problem requires the computation of thresholding functions for the Smoothly Clipped Absolute Deviation (SCAD) and Minimax Concave Penalty (MCP) penalties, and the subsequent evaluation of these functions and their difference in squared loss for a specific case.\n\nThe estimator $T(z)$ is defined as the minimizer of the univariate objective function:\n$$F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$$\nThe minimizer $b$ is the proximal operator of the penalty $P(|\\cdot|)$ evaluated at $z$. Due to the symmetry of the penalty term $P(|b|)$ and the quadratic term $(b-z)^2$, the solution $b = T(z)$ will have the same sign as $z$. Given $z = 3.2 > 0$, we can restrict our search for the minimizer to $b \\ge 0$. For $b \\ge 0$, the objective function becomes $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(b)$ (since $|b|=b$).\n\nThe first-order necessary condition for a minimum at $b > 0$ is that the derivative of $F(b;z)$ with respect to $b$ is zero. This condition is given as $b-z+p'(b) = 0$, which can be rearranged to $z = b + p'(b)$, where $p'(b)$ is the derivative of the penalty function $P(t)$ evaluated at $t=b$. For $b=0$ to be the solution, we must satisfy the subgradient optimality condition $0 \\in \\partial F(0;z)$, which is $z \\in \\partial P(|b|)|_{b=0}$. For both SCAD and MCP, the subgradient at the origin is the interval $[-\\lambda, \\lambda]$. Thus, for $z \\in [0, \\lambda]$, the solution is $b=0$.\n\nWe are given the numerical values $z = 3.2$, $\\lambda = 1.0$, $a = 3.7$ for SCAD, and $\\gamma = 2.5$ for MCP.\n\nFirst, we calculate the SCAD threshold $T_{\\text{SCAD}}(z)$.\nThe derivative of the SCAD penalty for $t \\ge 0$ is given as:\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda, & 0 \\le t \\le \\lambda, \\\\\n\\dfrac{a\\lambda - t}{a - 1}, & \\lambda < t \\le a\\lambda, \\\\\n0, & t > a\\lambda.\n\\end{cases}\n$$\nWe analyze the solution $b$ based on the equation $z = b + p'_{\\text{SCAD}}(b)$ for $z > \\lambda$.\n1.  If $b \\in (\\lambda, 2\\lambda]$, this corresponds to the soft-thresholding solution $b = z-\\lambda$. This is derived from the first case of $p'_{\\text{SCAD}}(t)$ for $z \\in (\\lambda, 2\\lambda]$.\n2.  If $b \\in (\\lambda, a\\lambda]$, we use the second case for $p'_{\\text{SCAD}}(b)$:\n    $z = b + \\dfrac{a\\lambda - b}{a - 1} = \\dfrac{b(a-1) + a\\lambda - b}{a - 1} = \\dfrac{b(a-2) + a\\lambda}{a-1}$.\n    Solving for $b$: $z(a-1) = b(a-2) + a\\lambda$, which gives $b = \\dfrac{z(a-1) - a\\lambda}{a-2}$. This solution is valid when the resulting $b$ is in $(\\lambda, a\\lambda]$, which corresponds to $z \\in (2\\lambda, a\\lambda]$.\n3.  If $b > a\\lambda$, we have $p'_{\\text{SCAD}}(b) = 0$, so $z=b$. This is valid for $z > a\\lambda$.\n\nWith the given values, $\\lambda=1.0$ and $a=3.7$, the critical points for $z$ are $2\\lambda = 2.0$ and $a\\lambda = 3.7$. The given value $z=3.2$ satisfies $2\\lambda < z \\le a\\lambda$ (i.e., $2.0 < 3.2 \\le 3.7$). Therefore, we use the formula from case 2:\n$$T_{\\text{SCAD}}(z) = b = \\frac{z(a-1) - a\\lambda}{a-2}$$\nSubstituting the numerical values:\n$$T_{\\text{SCAD}}(3.2) = \\frac{3.2(3.7-1) - 3.7(1.0)}{3.7-2} = \\frac{3.2(2.7) - 3.7}{1.7} = \\frac{8.64 - 3.7}{1.7} = \\frac{4.94}{1.7}$$\nTo express this as an exact fraction:\n$$T_{\\text{SCAD}}(3.2) = \\frac{4.94}{1.7} = \\frac{494}{170} = \\frac{247}{85}$$\n\nNext, we calculate the MCP threshold $T_{\\text{MCP}}(z)$.\nThe derivative of the MCP penalty for $t \\ge 0$ is:\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\dfrac{t}{\\gamma\\lambda}\\right), & 0 \\le t \\le \\gamma\\lambda, \\\\\n0, & t > \\gamma\\lambda.\n\\end{cases}\n$$\nWe analyze the solution $b$ from $z = b + p'_{\\text{MCP}}(b)$ for $z > \\lambda$.\n1.  If $b \\in (0, \\gamma\\lambda]$, we use the first case for $p'_{\\text{MCP}}(b)$:\n    $z = b + \\lambda\\left(1 - \\dfrac{b}{\\gamma\\lambda}\\right) = b + \\lambda - \\dfrac{b}{\\gamma} = b\\left(1 - \\dfrac{1}{\\gamma}\\right) + \\lambda$.\n    Solving for $b$: $b\\left(\\dfrac{\\gamma-1}{\\gamma}\\right) = z-\\lambda$, which gives $b = \\dfrac{\\gamma}{\\gamma-1}(z-\\lambda)$. This solution is valid when the resulting $b$ is in $(0, \\gamma\\lambda]$, which corresponds to $z \\in (\\lambda, \\gamma\\lambda]$.\n2.  If $b > \\gamma\\lambda$, we have $p'_{\\text{MCP}}(b) = 0$, so $z=b$. This is valid for $z > \\gamma\\lambda$.\n\nWith the given values, $\\lambda=1.0$ and $\\gamma=2.5$, the critical point for $z$ is $\\gamma\\lambda = 2.5$. The given value $z=3.2$ satisfies $z > \\gamma\\lambda$ (i.e., $3.2 > 2.5$). Therefore, we use the formula from case 2:\n$$T_{\\text{MCP}}(z) = z$$\nSubstituting the numerical value $z=3.2$:\n$$T_{\\text{MCP}}(3.2) = 3.2 = \\frac{32}{10} = \\frac{16}{5}$$\n\nFinally, we compute the difference in squared estimators, $\\Delta$:\n$$\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}$$\nSubstituting the calculated values:\n$$\\Delta = \\left(\\frac{16}{5}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2}$$\nTo simplify the calculation, we find a common denominator. Since $85 = 5 \\times 17$, we can write $\\frac{16}{5} = \\frac{16 \\times 17}{5 \\times 17} = \\frac{272}{85}$.\n$$\\Delta = \\left(\\frac{272}{85}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2} = \\frac{272^{2} - 247^{2}}{85^{2}}$$\nUsing the difference of squares formula, $x^2 - y^2 = (x-y)(x+y)$:\n$$\\Delta = \\frac{(272 - 247)(272 + 247)}{85^{2}} = \\frac{(25)(519)}{85^{2}}$$\nSince $85^2 = (5 \\times 17)^2 = 25 \\times 17^2 = 25 \\times 289$:\n$$\\Delta = \\frac{25 \\times 519}{25 \\times 289} = \\frac{519}{289}$$\nThe fraction $\\frac{519}{289}$ is irreducible because $289 = 17^2$ and $519 = 17 \\times 30 + 9$, so $519$ is not divisible by $17$.\n\nThe three requested quantities are $T_{\\text{SCAD}}(z) = \\frac{247}{85}$, $T_{\\text{MCP}}(z) = \\frac{16}{5}$, and $\\Delta = \\frac{519}{289}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{247}{85} & \\frac{16}{5} & \\frac{519}{289}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A key motivation for using non-convex penalties is their ability to mitigate the systematic bias that convex penalties like the LASSO introduce for large coefficients. Ideally, a penalty should shrink small, noisy coefficients to zero while leaving large, true signals untouched, a property known as unbiasedness. This exercise challenges you to formalize this concept by deriving the precise conditions on the MCP parameters, $\\lambda$ and $\\gamma$, that guarantee both unbiased estimation of true signals and correct thresholding of noise under a worst-case scenario .",
            "id": "3462708",
            "problem": "Consider the following sparse estimation model. Let $x^{\\star} \\in \\mathbb{R}^{n}$ be $s$-sparse with support $S \\subset \\{1,\\dots,n\\}$ and smallest nonzero magnitude $x_{\\min} := \\min_{i \\in S} |x^{\\star}_{i}| > 0$. Let $y = x^{\\star} + w$ be observed with $\\|w\\|_{\\infty} \\le \\sigma$ for a known noise bound $\\sigma \\ge 0$. Consider the estimator\n$$\n\\hat{x} \\in \\arg\\min_{b \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|y - b\\|_{2}^{2} + \\sum_{i=1}^{n} p_{\\lambda,\\gamma}(|b_{i}|) \\right\\},\n$$\nwhere $p_{\\lambda,\\gamma}$ is the Minimax Concave Penalty (MCP), defined for $\\lambda > 0$ and $\\gamma > 1$ by\n$$\np_{\\lambda,\\gamma}(t) \\;:=\\; \\lambda \\int_{0}^{t} \\left(1 - \\frac{s}{\\gamma \\lambda}\\right)_{+} \\, ds, \\quad t \\ge 0,\n$$\nand $(u)_{+} := \\max\\{u,0\\}$. Throughout, interpret unbiasedness for a coordinate as the absence of regularization-induced shrinkage, namely the condition that the coordinate-wise solution equals the corresponding data value $y_{i}$ (which coincides with $x^{\\star}_{i}$ in the noiseless limit).\n\nStarting from the given definitions and the first-order optimality conditions for the above separable problem, derive a safe region in the $(\\lambda,\\gamma)$-plane that guarantees both of the following simultaneously:\n- for every $i \\in S$ with $|x^{\\star}_{i}| = x_{\\min}$, the unique coordinate-wise minimizer is unbiased in the above sense under the worst-case admissible noise;\n- for every $j \\notin S$, the unique coordinate-wise minimizer is exactly zero under the worst-case admissible noise.\n\nThen, analyze a near-threshold nonzero coefficient by considering a hypothetical entry of magnitude $x_{\\mathrm{nt}} := \\gamma \\lambda + \\varepsilon$ with $\\varepsilon > 0$, and determine, under the same worst-case noise model, the necessary and sufficient condition on $\\varepsilon$ for this entry to remain unbiased.\n\nFinally, express in closed form the supremum value of $\\gamma$ (as a function of $x_{\\min}$ and $\\sigma$) for which the derived safe region in $(\\lambda,\\gamma)$ is nonempty. Provide this supremum as your final answer in a single analytic expression.",
            "solution": "The problem asks for an analysis of a sparse estimator based on the Minimax Concave Penalty (MCP). The process involves deriving conditions for reliable estimation, analyzing a near-threshold scenario, and finding a supremum value for a penalty parameter.\n\nFirst, we must characterize the estimator. The optimization problem is\n$$\n\\hat{x} \\in \\arg\\min_{b \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|y - b\\|_{2}^{2} + \\sum_{i=1}^{n} p_{\\lambda,\\gamma}(|b_{i}|) \\right\\}\n$$\nThis problem is separable, meaning we can solve for each coordinate $\\hat{x}_i$ independently by minimizing\n$$\nL_i(b_i) = \\frac{1}{2} (y_i - b_i)^2 + p_{\\lambda,\\gamma}(|b_i|)\n$$\nThe MCP penalty for $t \\ge 0$ is given by $p_{\\lambda,\\gamma}(t) = \\lambda \\int_{0}^{t} \\left(1 - \\frac{s}{\\gamma \\lambda}\\right)_{+} ds$. Its derivative, which is needed for the first-order optimality conditions, is $p'_{\\lambda,\\gamma}(t) = \\lambda \\left(1 - \\frac{t}{\\gamma \\lambda}\\right)_{+}$ for $t \\ge 0$. Explicitly,\n$$\np'_{\\lambda,\\gamma}(t) = \\begin{cases} \\lambda - \\frac{t}{\\gamma} & \\text{if } 0 \\le t \\le \\gamma\\lambda \\\\ 0 & \\text{if } t > \\gamma\\lambda \\end{cases}\n$$\nThe first-order conditions for a minimizer $\\hat{b}_i$ of $L_i(b_i)$ are given by the subgradient inclusion $0 \\in \\hat{b}_i - y_i + \\partial_t [p_{\\lambda,\\gamma}(|t|)]|_{t=\\hat{b}_i}$. Solving these conditions yields the MCP thresholding function, $\\hat{b}_i = T_{\\lambda,\\gamma}(y_i)$, which is\n$$\nT_{\\lambda,\\gamma}(y_i) = \\begin{cases} 0 & \\text{if } |y_i| \\le \\lambda \\\\ \\mathrm{sgn}(y_i) \\frac{\\gamma(|y_i|-\\lambda)}{\\gamma-1} & \\text{if } \\lambda < |y_i| \\le \\gamma\\lambda \\\\ y_i & \\text{if } |y_i| > \\gamma\\lambda \\end{cases}\n$$\nThis function characterizes the unique coordinate-wise minimizer for $\\gamma > 1$.\n\nNow, we derive the safe region in the $(\\lambda, \\gamma)$-plane. This region must simultaneously satisfy two conditions under the worst-case noise model $\\|w\\|_{\\infty} \\le \\sigma$.\n\n1.  **Unbiasedness on the support set**: For any $i \\in S$ with $|x^{\\star}_i| = x_{\\min}$, the estimator must be unbiased, i.e., $\\hat{x}_i = y_i$. According to the thresholding function, this occurs if and only if $|y_i| > \\gamma\\lambda$. The term \"worst-case admissible noise\" implies that this condition must hold even for the noise realization that makes it hardest to satisfy. The challenge here is to keep $|y_i|$ large. The value of $|y_i| = |x^{\\star}_i + w_i|$ is minimized when the noise $w_i$ counteracts the signal $x^{\\star}_i$. The minimum value is $\\min_{|w_i|\\le\\sigma} |x^{\\star}_i + w_i| = \\max(0, |x^{\\star}_i|-\\sigma)$. For our case, this is $\\max(0, x_{\\min}-\\sigma)$. Therefore, we require $\\max(0, x_{\\min}-\\sigma) > \\gamma\\lambda$. This single inequality implies two conditions: first, $x_{\\min} > \\sigma$, and second, $x_{\\min} - \\sigma > \\gamma\\lambda$. So the first condition for the safe region is:\n    $$\n    \\gamma\\lambda < x_{\\min} - \\sigma\n    $$\n\n2.  **Sparsity off the support set**: For any $j \\notin S$, the estimator must be zero, i.e., $\\hat{x}_j=0$. According to the thresholding function, this occurs if and only if $|y_j| \\le \\lambda$. For such an index $j$, $x^{\\star}_j=0$, so $y_j = w_j$. The \"worst-case admissible noise\" here is the noise that makes $|y_j|$ as large as possible, challenging the condition $|y_j| \\le \\lambda$. This worst case is any $w_j$ with $|w_j|=\\sigma$. Thus, we must have $\\max_{|w_j|\\le\\sigma} |w_j| \\le \\lambda$, which simplifies to:\n    $$\n    \\lambda \\ge \\sigma\n    $$\n\nCombining these two conditions with the parameter constraints $\\lambda > 0$ and $\\gamma > 1$, the safe region in the $(\\lambda, \\gamma)$-plane is the set of points satisfying\n$$\n\\sigma \\le \\lambda < \\frac{x_{\\min}-\\sigma}{\\gamma} \\quad \\text{and} \\quad \\gamma > 1\n$$\nNote that for this region to exist, we must have $x_{\\min}-\\sigma > 0$, or $x_{\\min} > \\sigma$.\n\nNext, we analyze the near-threshold coefficient. Consider a hypothetical entry with magnitude $|x^{\\star}_{\\mathrm{nt}}| = \\gamma\\lambda + \\varepsilon$ for $\\varepsilon > 0$. For this entry to be unbiased ($\\hat{x}_{\\mathrm{nt}} = y_{\\mathrm{nt}}$), we require $|y_{\\mathrm{nt}}| > \\gamma\\lambda$. This must hold under the worst-case noise, so we need $\\min_{|w|\\le\\sigma} |x^{\\star}_{\\mathrm{nt}} + w| > \\gamma\\lambda$. This minimum is $| |x^{\\star}_{\\mathrm{nt}}| - \\sigma |$. Substituting the value of $|x^{\\star}_{\\mathrm{nt}}|$, the condition becomes\n$$\n|(\\gamma\\lambda + \\varepsilon) - \\sigma| > \\gamma\\lambda\n$$\nAssuming the parameters $(\\lambda, \\gamma)$ are chosen from the safe region, we have $\\lambda \\ge \\sigma$ and $\\gamma > 1$. Let's examine the sign of the term inside the absolute value: $\\gamma\\lambda + \\varepsilon - \\sigma$. Since $\\lambda \\ge \\sigma$, we have $\\gamma\\lambda \\ge \\gamma\\sigma$. Thus, $\\gamma\\lambda + \\varepsilon - \\sigma \\ge \\gamma\\sigma + \\varepsilon - \\sigma = (\\gamma-1)\\sigma + \\varepsilon$. As $\\gamma > 1$, $\\sigma \\ge 0$, and $\\varepsilon > 0$, this term is strictly positive. We can therefore drop the absolute value:\n$$\n\\gamma\\lambda + \\varepsilon - \\sigma > \\gamma\\lambda\n$$\nThis simplifies to $\\varepsilon > \\sigma$. This is the necessary and sufficient condition on $\\varepsilon$ for the entry to remain unbiased.\n\nFinally, we find the supremum value of $\\gamma$ for which the safe region is nonempty. A nonempty safe region requires the existence of a $\\lambda$ such that $\\sigma \\le \\lambda < \\frac{x_{\\min}-\\sigma}{\\gamma}$. This is only possible if the lower bound is strictly less than the upper bound:\n$$\n\\sigma < \\frac{x_{\\min}-\\sigma}{\\gamma}\n$$\nAssuming $\\sigma > 0$, we can rearrange this inequality to solve for $\\gamma$. Since $\\gamma > 1$ is positive, we multiply by $\\gamma$:\n$$\n\\gamma\\sigma < x_{\\min} - \\sigma\n$$\n$$\n\\gamma\\sigma + \\sigma < x_{\\min}\n$$\n$$\n(\\gamma+1)\\sigma < x_{\\min}\n$$\n$$\n\\gamma + 1 < \\frac{x_{\\min}}{\\sigma}\n$$\n$$\n\\gamma < \\frac{x_{\\min}}{\\sigma} - 1\n$$\nThe set of all $\\gamma$ values for which the safe region is nonempty is therefore given by the intersection of the conditions $\\gamma > 1$ and $\\gamma < \\frac{x_{\\min}}{\\sigma} - 1$. This defines the interval $\\left(1, \\frac{x_{\\min}}{\\sigma} - 1\\right)$. For this interval to be non-empty, we need $1 < \\frac{x_{\\min}}{\\sigma} - 1$, which means $2 < \\frac{x_{\\min}}{\\sigma}$, or $x_{\\min} > 2\\sigma$, a standard condition in sparse recovery theory.\n\nThe question asks for the supremum of the set of possible $\\gamma$ values. The supremum of the interval $\\left(1, \\frac{x_{\\min}}{\\sigma} - 1\\right)$ is its upper endpoint.\nIf $\\sigma = 0$, the conditions for the safe region become $\\lambda \\ge 0$ and $\\gamma\\lambda < x_{\\min}$. For any $\\gamma > 1$, we can choose $\\lambda \\in (0, x_{\\min}/\\gamma)$, so the region is non-empty. In this case, the set of valid $\\gamma$ is $(1, \\infty)$, and the supremum is $\\infty$. The derived expression $\\frac{x_{\\min}}{\\sigma} - 1$ correctly diverges to $\\infty$ as $\\sigma \\to 0^+$. Thus, it serves as the general closed-form answer.",
            "answer": "$$\n\\boxed{\\frac{x_{\\min}}{\\sigma} - 1}\n$$"
        },
        {
            "introduction": "Moving from theory to practice requires adapting idealized models to more general settings. While thresholding rules are often derived assuming a design matrix with unit-norm columns, real-world problems do not always fit this structure. This exercise bridges that gap by guiding you to derive and implement a \"renormalized\" thresholding rule for SCAD and MCP that correctly accounts for varying column norms, a crucial step for building a robust coordinate descent solver for general linear models .",
            "id": "3462665",
            "problem": "You are given a linear inverse model in compressed sensing with a design matrix $A \\in \\mathbb{R}^{m \\times n}$, a sparse ground-truth vector $x_0 \\in \\mathbb{R}^n$, and noiseless measurements $y = A x_0$. In practice, columns of $A$ are often normalized, but manufacturing or preprocessing errors can introduce per-column scaling factors so that the actual design matrix used is $A D$, where $D = \\mathrm{diag}(s_1,\\dots,s_n)$ with $s_j > 0$. This implies that the column $A_j$ is scaled as $A_j \\gets s_j A_j$, and the per-column squared norm $c_j = \\lVert A_j \\rVert_2^2$ deviates from its target. You will examine the impact of such column normalization errors on the thresholding behavior of non-convex penalties and derive a renormalized thresholding rule that correctly accounts for $c_j$ when performing coordinate descent updates under the Smoothly Clipped Absolute Deviation (SCAD) and the Minimax Concave Penalty (MCP).\n\nStarting from the objective\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\lVert y - A x \\rVert_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta),\n$$\nwhere $p(\\cdot;\\lambda,\\theta)$ is either the Smoothly Clipped Absolute Deviation (SCAD) penalty or the Minimax Concave Penalty (MCP), both parameterized by a regularization level $\\lambda > 0$ and a shape parameter $\\theta$ (for SCAD use $a > 2$, for MCP use $\\gamma > 1$), derive, from first principles, the coordinate-wise one-dimensional subproblem for the $j$-th coordinate and obtain a renormalized thresholding rule that explicitly depends on the per-column squared norm $c_j = \\lVert A_j \\rVert_2^2$ and the scalar $z_j = A_j^\\top r + c_j x_j$, where $r = y - A x$ is the current residual and $x_j$ is the current iterate for the $j$-th coefficient. Your derivation must start from the coordinate descent perspective on the least-squares term and the subgradient stationarity condition for the non-convex penalty, and it must not invoke any shortcut formulas directly.\n\nThe Smoothly Clipped Absolute Deviation (SCAD) penalty uses parameter $a > 2$. The Minimax Concave Penalty (MCP) uses parameter $\\gamma > 1$. For each penalty, derive the piecewise thresholding function for the one-dimensional subproblem that maps $(z_j, c_j, \\lambda, \\theta)$ to an updated coefficient $t_j^\\star$. The renormalized rule must reduce to the known thresholding behavior when $c_j = 1$, and it must correctly adjust thresholds and shrinkage for $c_j \\neq 1$.\n\nImplement a proximal coordinate descent solver that:\n- Initializes $x$ to the zero vector.\n- Maintains the residual $r = y - A x$.\n- Iterates cyclically over coordinates $j = 1,\\dots,n$, computing $z_j = A_j^\\top r + c_j x_j$, then updating $x_j \\gets t_j^\\star$ via your derived renormalized thresholding rule for the chosen penalty, and updating the residual accordingly.\n- Stops when the maximum absolute change across coordinates falls below a tolerance or when a maximum number of iterations is reached.\n\nSimulate recovery under random per-column scaling noise as follows:\n- Generate $A_{\\text{base}}$ with independent and identically distributed entries from a standard normal distribution, then normalize each column to unit Euclidean norm.\n- Draw $s_j$ independently and uniformly from $[1 - \\delta, 1 + \\delta]$ for a given $\\delta \\in [0,1)$, and set $A = A_{\\text{base}} \\cdot \\mathrm{diag}(s)$ (column-wise scaling).\n- Choose a support set of size $k$ uniformly at random, and populate $x_0$ on that support with random signs and magnitudes uniformly drawn from $[0.5, 1.0]$. Set $y = A x_0$.\n- Run your solver with either SCAD or MCP penalty, using given hyperparameters $(\\lambda, a)$ for SCAD or $(\\lambda, \\gamma)$ for MCP.\n\nDefine exact support recovery as the event that the set of indices of the $k$ largest absolute entries of the recovered $x$ equals the true support of $x_0$.\n\nYour program must evaluate the following test suite, each test case specified as $(\\text{penalty}, \\delta, \\lambda, \\theta, m, n, k, \\text{seed})$:\n- Case $1$: MCP, $\\delta = 0.0$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 1$.\n- Case $2$: MCP, $\\delta = 0.1$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 2$.\n- Case $3$: MCP, $\\delta = 0.3$, $\\lambda = 0.08$, $\\gamma = 1.2$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 3$.\n- Case $4$: SCAD, $\\delta = 0.0$, $\\lambda = 0.08$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 4$.\n- Case $5$: SCAD, $\\delta = 0.3$, $\\lambda = 0.08$, $a = 2.1$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 5$.\n- Case $6$: SCAD, $\\delta = 0.2$, $\\lambda = 0.25$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 6$.\n\nFor each case, your program must produce a boolean indicating whether exact support recovery was achieved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5,r_6]$).",
            "solution": "The objective is to derive and implement a renormalized coordinate-wise update rule for the SCAD and MCP penalties in a linear regression context where the columns of the design matrix are not necessarily unit norm.\n\n### 1. Coordinate Descent Subproblem\n\nWe start with the objective function:\n$$\nF(x) = \\frac{1}{2} \\lVert y - A x \\rVert_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta)\n$$\nIn a coordinate descent algorithm, we optimize $F(x)$ with respect to a single coordinate $x_j$ at a time, holding all other coordinates $x_k$ ($k \\neq j$) fixed. Let the current iterate be $x$, and let $t$ be the new value for the $j$-th coordinate. We can express the vector being updated as $x - x_j e_j + t e_j$, where $e_j$ is the $j$-th standard basis vector.\n\nThe least-squares term can be written as:\n$$\n\\frac{1}{2} \\lVert y - A(x - x_j e_j + t e_j) \\rVert_2^2 = \\frac{1}{2} \\lVert (y - A x) + (x_j-t)A_j \\rVert_2^2\n$$\nLet $r = y - Ax$ be the current residual and $A_j$ be the $j$-th column of $A$. Expanding the squared norm, we get:\n$$\n\\frac{1}{2} ( \\lVert r \\rVert_2^2 + 2(x_j-t) A_j^\\top r + (x_j-t)^2 \\lVert A_j \\rVert_2^2 )\n$$\nThe one-dimensional subproblem for updating $x_j$ to $t$ is to minimize:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}\\lVert A_j \\rVert_2^2 (t - x_j)^2 - (t - x_j)A_j^\\top r + p(t;\\lambda,\\theta) \\right\\}\n$$\nLet $c_j = \\lVert A_j \\rVert_2^2$. We collect terms involving $t$:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} (t^2 - 2tx_j) - t A_j^\\top r + p(t;\\lambda,\\theta) + \\text{const} \\right\\}\n$$\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - t(c_j x_j + A_j^\\top r) + p(t;\\lambda,\\theta) \\right\\}\n$$\nAs defined in the problem, let $z_j = c_j x_j + A_j^\\top r$. The subproblem becomes:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - z_j t + p(t;\\lambda,\\theta) \\right\\}\n$$\nBy completing the square, this is equivalent to minimizing:\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} \\left( t - \\frac{z_j}{c_j} \\right)^2 + p(t;\\lambda,\\theta) \\right\\}\n$$\nThis is the proximal operator of the function $\\frac{1}{c_j}p(\\cdot)$ evaluated at $\\frac{z_j}{c_j}$. The first-order necessary condition for a minimum $t^\\star_j$ is given by the subgradient stationarity condition:\n$$\n0 \\in \\frac{\\partial}{\\partial t} \\left[ \\frac{c_j}{2} t^2 - z_j t \\right] + \\partial p(t;\\lambda,\\theta) \\bigg|_{t=t^\\star_j}\n$$\n$$\n0 \\in c_j t^\\star_j - z_j + \\partial p(t^\\star_j;\\lambda,\\theta) \\quad \\implies \\quad z_j - c_j t^\\star_j \\in \\partial p(t^\\star_j;\\lambda,\\theta)\n$$\nWe now solve this for the SCAD and MCP penalties. The penalties are symmetric, so we derive the rules for $z_j > 0$ (which implies $t^\\star_j \\ge 0$) and extend by symmetry.\n\n### 2. Renormalized SCAD Thresholding\n\nThe SCAD penalty's derivative for $t>0$ with parameter $a>2$ is:\n$$\np'(t) = \\begin{cases} \\lambda & \\text{if } 0 < t \\le \\lambda \\\\ \\frac{a\\lambda - t}{a-1} & \\text{if } \\lambda < t \\le a\\lambda \\\\ 0 & \\text{if } t > a\\lambda \\end{cases}\n$$\nThe one-dimensional subproblem is convex if its second derivative is non-negative everywhere. The second derivative of the objective is $c_j + p''(|t|)$. For SCAD, $p''(|t|)$ is $-1/(a-1)$ for $|t| \\in (\\lambda, a\\lambda)$ and 0 otherwise. Thus, the subproblem is convex if $c_j - 1/(a-1) \\ge 0$, i.e., $c_j(a-1) \\ge 1$.\n\n**Case 1: $c_j(a-1) \\ge 1$ (Convex Subproblem)**\nWe solve $z_j = c_j t + p'(t)$ for $t>0$.\n1. If $t^\\star_j=0$, we need $0 < z_j \\le \\lambda$.\n2. If $0 < t^\\star_j \\le \\lambda$, we have $p'(t)=\\lambda$. So $z_j = c_j t + \\lambda \\implies t^\\star_j = (z_j-\\lambda)/c_j$. This is valid for $\\lambda < z_j \\le (c_j+1)\\lambda$.\n3. If $\\lambda < t^\\star_j \\le a\\lambda$, we have $p'(t)=(a\\lambda-t)/(a-1)$. So $z_j = c_j t + (a\\lambda-t)/(a-1)$, which gives $t^\\star_j = \\frac{z_j(a-1)-a\\lambda}{c_j(a-1)-1}$. This is valid for $(c_j+1)\\lambda < z_j \\le a c_j\\lambda$.\n4. If $t^\\star_j > a\\lambda$, we have $p'(t)=0$. So $z_j = c_j t \\implies t^\\star_j = z_j/c_j$. This is valid for $z_j > a c_j \\lambda$.\n\n**Case 2: $c_j(a-1) < 1$ (Non-Convex Subproblem)**\nThe objective function can have multiple local minima. The global minimizer must be one of the local minima, which occur at $t=0$ and in the region where the penalty derivative is zero (i.e., $|t| > a\\lambda$). In that region, the minimizer is $t=z_j/c_j$. We find the global minimum by comparing the objective function value at these two points.\n$J(t) = \\frac{c_j}{2} t^2 - z_j t + p(|t|)$.\n- $J(0) = p(0) = 0$.\n- $J(z_j/c_j) = \\frac{c_j}{2}(\\frac{z_j}{c_j})^2 - z_j(\\frac{z_j}{c_j}) + p(|z_j/c_j|) = -\\frac{z_j^2}{2c_j} + p(|z_j/c_j|)$.\nFor $|z_j/c_j| > a\\lambda$, $p(|z_j/c_j|) = \\frac{(a+1)\\lambda^2}{2}$. So $J(z_j/c_j) = -\\frac{z_j^2}{2c_j} + \\frac{(a+1)\\lambda^2}{2}$.\nComparing $J(0)=0$ and $J(z_j/c_j)$, we choose $t=z_j/c_j$ if $J(z_j/c_j)  0$, which means $\\frac{(a+1)\\lambda^2}{2}  \\frac{z_j^2}{2c_j}$, or $|z_j| > \\lambda\\sqrt{c_j(a+1)}$. This results in a hard-thresholding rule.\n\n### 3. Renormalized MCP Thresholding\n\nThe MCP penalty's derivative for $t0$ with parameter $\\gamma1$ is $p'(t) = (\\lambda - t/\\gamma)_+$. The subproblem is convex if $c_j+p''(|t|) \\ge 0$. Here $p''(|t|) = -1/\\gamma$ for $|t| \\in (0, \\gamma\\lambda)$. So convexity holds if $c_j - 1/\\gamma \\ge 0$, or $\\gamma c_j \\ge 1$.\n\n**Case 1: $\\gamma c_j \\ge 1$ (Convex Subproblem)**\nSolving $z_j = c_j t + p'(t)$:\n1. If $t^\\star_j=0$, we need $0  z_j \\le \\lambda$.\n2. If $0  t^\\star_j \\le \\gamma\\lambda$, we have $z_j = c_j t + (\\lambda-t/\\gamma)$, giving $t^\\star_j = \\frac{\\gamma(z_j-\\lambda)}{\\gamma c_j - 1}$. This is valid for $\\lambda  z_j \\le \\gamma c_j \\lambda$.\n3. If $t^\\star_j  \\gamma\\lambda$, we have $p'(t)=0$, giving $t^\\star_j = z_j/c_j$. This is valid for $z_j > \\gamma c_j \\lambda$.\n\n**Case 2: $\\gamma c_j  1$ (Non-Convex Subproblem)**\nSimilar to SCAD, we compare the objective at $t=0$ and $t=z_j/c_j$.\n- $J(0) = 0$.\n- $J(z_j/c_j) = -\\frac{z_j^2}{2c_j} + p(|z_j/c_j|)$. For $|z_j/c_j| > \\gamma\\lambda$, $p(|z_j/c_j|) = \\frac{\\gamma\\lambda^2}{2}$.\nWe choose $t=z_j/c_j$ if $J(z_j/c_j)  0 \\implies \\frac{\\gamma\\lambda^2}{2}  \\frac{z_j^2}{2c_j} \\implies |z_j| > \\lambda\\sqrt{\\gamma c_j}$. This is a hard-thresholding rule.\n\n### 4. Proximal Coordinate Descent Algorithm\nThe solver implements the coordinate descent algorithm as described in the problem, using the correctly derived renormalized thresholding rules for SCAD and MCP, including the separate logic for convex and non-convex subproblems. The implementation details are in the provided Python code.",
            "answer": "```python\nimport numpy as np\n\ndef scad_thresh(z, c, lam, a):\n    \"\"\"Renormalized SCAD thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    sign_z = np.sign(z) if z != 0 else 1\n\n    if abs_z = lam:\n        return 0.0\n\n    # Convex subproblem if c*(a-1) >= 1\n    if c * (a - 1.0) = 1.0:\n        if abs_z = (c + 1.0) * lam:\n            return (abs_z - lam) * sign_z / c\n        elif abs_z = a * c * lam:\n            return ((a - 1.0) * abs_z - a * lam) * sign_z / (c * (a - 1.0) - 1.0)\n        else:\n            return z / c\n    else:  # Non-convex subproblem\n        # Compare objective at t=0 and local minimum in unbiased region (t=z/c)\n        # J(t) = 0.5*c*t^2 - z*t + p(t). J(0)=0.\n        # J_unbiased_sol = 0.5*c*(z/c)^2 - z*(z/c) + p_sat = -z^2/(2c) + (a+1)*lam^2/2\n        # Choose z/c if J_unbiased_sol  0 => z^2 > c*(a+1)*lam^2\n        threshold = lam * np.sqrt(c * (a + 1.0))\n        if abs_z  threshold:\n            # Check if z/c is in the unbiased region for this rule to apply\n            if abs_z / c  a * lam:\n                return z / c\n            else:\n                # This case is complex. The minimizer might be elsewhere.\n                # A simple practical choice is soft-thresholding\n                return (abs_z - lam) * sign_z / c\n        else:\n            return 0.0\n\n\ndef mcp_thresh(z, c, lam, gamma):\n    \"\"\"Renormalized MCP thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    sign_z = np.sign(z) if z != 0 else 1\n\n    # Convex subproblem if gamma*c >= 1\n    if gamma * c = 1.0:\n        if abs_z = lam:\n            return 0.0\n        elif abs_z = gamma * c * lam:\n            return (gamma * (abs_z - lam)) * sign_z / (gamma * c - 1.0)\n        else:\n            return z / c\n    else:  # Non-convex subproblem\n        # Hard-thresholding rule by comparing J(0) and J(z/c)\n        threshold = lam * np.sqrt(gamma * c)\n        if abs_z = threshold:\n            return 0.0\n        else:\n            return z / c\n\n\ndef pcd_solver(A, y, penalty, lam, theta, tol=1e-6, max_iter=1000):\n    \"\"\"Proximal Coordinate Descent solver.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    r = y.copy()\n    \n    c = np.sum(A * A, axis=0)\n    \n    if penalty.lower() == 'scad':\n        a = theta\n        thresh_op = scad_thresh\n        params = (lam, a)\n    elif penalty.lower() == 'mcp':\n        gamma = theta\n        thresh_op = mcp_thresh\n        params = (lam, gamma)\n    else:\n        raise ValueError(\"Unknown penalty type\")\n\n    for _ in range(max_iter):\n        max_change = 0.0\n        for j in range(n):\n            if c[j]  1e-9: continue\n            \n            x_old_j = x[j]\n            z_j = A[:, j].T @ r + c[j] * x_old_j\n            \n            x_new_j = thresh_op(z_j, c[j], *params)\n            \n            delta_x_j = x_new_j - x_old_j\n            \n            if delta_x_j != 0.0:\n                r -= delta_x_j * A[:, j]\n                x[j] = x_new_j\n            \n            max_change = max(max_change, np.abs(delta_x_j))\n            \n        if max_change  tol:\n            break\n            \n    return x\n\ndef solve():\n    test_cases = [\n        ('MCP', 0.0, 0.08, 3.0, 120, 200, 10, 1),\n        ('MCP', 0.1, 0.08, 3.0, 120, 200, 10, 2),\n        ('MCP', 0.3, 0.08, 1.2, 120, 200, 10, 3),\n        ('SCAD', 0.0, 0.08, 3.7, 120, 200, 10, 4),\n        ('SCAD', 0.3, 0.08, 2.1, 120, 200, 10, 5),\n        ('SCAD', 0.2, 0.25, 3.7, 120, 200, 10, 6)\n    ]\n    \n    results = []\n\n    for penalty, delta, lam, theta, m, n, k, seed in test_cases:\n        np.random.seed(seed)\n        \n        # 1. Generate data\n        A_base = np.random.randn(m, n)\n        A_base /= np.linalg.norm(A_base, axis=0) # Normalize columns\n        \n        s = np.random.uniform(1 - delta, 1 + delta, n)\n        A = A_base * s # Apply scaling noise\n\n        support = np.random.choice(n, k, replace=False)\n        x0 = np.zeros(n)\n        magnitudes = np.random.uniform(0.5, 1.0, k)\n        signs = np.random.choice([-1, 1], k)\n        x0[support] = magnitudes * signs\n        \n        y = A @ x0\n        \n        # 2. Run solver\n        x_hat = pcd_solver(A, y, penalty=penalty, lam=lam, theta=theta, max_iter=200)\n        \n        # 3. Evaluate support recovery\n        true_support = set(np.where(x0 != 0)[0])\n        est_support = set(np.argsort(np.abs(x_hat))[-k:])\n        \n        recovery = (true_support == est_support)\n        results.append(recovery)\n        \n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n\n```"
        }
    ]
}