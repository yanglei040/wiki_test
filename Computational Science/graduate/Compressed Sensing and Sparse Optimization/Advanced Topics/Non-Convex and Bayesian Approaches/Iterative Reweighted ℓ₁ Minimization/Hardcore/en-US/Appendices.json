{
    "hands_on_practices": [
        {
            "introduction": "The power of iterative algorithms lies in the repeated application of a core update step. This exercise  demystifies the engine of iterative reweighted $\\ell_1$ minimization by having you compute one explicit step of the proximal-gradient method (PGM). By breaking down the process into a gradient update and a weighted soft-thresholding operation, you will gain a concrete understanding of the algorithm's fundamental mechanics.",
            "id": "3454424",
            "problem": "Consider the composite optimization model used in iterative reweighted $\\ell_{1}$ minimization for compressed sensing and sparse optimization:\n$$F(x) \\equiv f(x) + g(x), \\quad f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}, \\quad g(x) = \\lambda \\sum_{i=1}^{n} w_{i} |x_{i}|,$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda  0$, and $w \\in \\mathbb{R}^{n}_{+}$ are given data. The proximal-gradient method (PGM) performs an iteration of the form\n$$x^{(k+1)} = \\operatorname{prox}_{t g}\\Big(x^{(k)} - t \\nabla f\\big(x^{(k)}\\big)\\Big),$$\nwhere $t  0$ is a step size and $\\operatorname{prox}_{t g}$ is the proximity operator of $t g$.\n\nUsing only the definition of the gradient as the linear functional that maps a direction to the directional derivative, the linearity of $A$, and the definition of the proximity operator as the unique minimizer of a strongly convex separable function, compute one explicit proximal-gradient step at the point\n$$x^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix},$$\nfor the data\n$$A = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad w = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}, \\quad \\lambda = \\frac{1}{2}, \\quad t = \\frac{1}{5}.$$\n\nYour task is to carry out this single iteration by:\n1. Computing the gradient $\\nabla f\\big(x^{(0)}\\big)$ from first principles.\n2. Forming the forward (gradient) step $v = x^{(0)} - t \\nabla f\\big(x^{(0)}\\big)$.\n3. Applying the weighted soft-thresholding induced by the proximity operator of $t g$, which acts componentwise with thresholds $t \\lambda w_{i}$.\n\nExpress the final iterate $x^{(1)}$ as a single row vector in exact form. No rounding is required.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Optimization Model: $F(x) \\equiv f(x) + g(x)$\n- Differentiable part: $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$\n- Non-smooth part: $g(x) = \\lambda \\sum_{i=1}^{n} w_{i} |x_{i}|$\n- Data types: $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda  0$, $w \\in \\mathbb{R}^{n}_{+}$\n- Iteration formula: $x^{(k+1)} = \\operatorname{prox}_{t g}\\Big(x^{(k)} - t \\nabla f\\big(x^{(k)}\\big)\\Big)$\n- Step size: $t  0$\n- Initial point: $x^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$\n- Matrix $A$: $A = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix}$\n- Vector $y$: $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$\n- Weight vector $w$: $w = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}$\n- Regularization parameter: $\\lambda = \\frac{1}{2}$\n- Step size: $t = \\frac{1}{5}$\n\nThe problem requires a single proximal-gradient step, comprising three sub-tasks: computing the gradient, performing the forward step, and applying the proximity operator.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is set in the well-defined mathematical framework of convex optimization and sparse signal recovery. The composite model, the proximal gradient method, and the specific functions ($f(x)$ as a least-squares loss and $g(x)$ as a weighted $\\ell_1$-norm penalty) are canonical in this field. The problem adheres strictly to established mathematical principles.\n- **Well-Posed:** The problem asks for the computation of a single, deterministic iteration. The function $f(x)$ is convex and continuously differentiable, and $g(x)$ is convex. The proximity operator of a scaled $\\ell_1$-norm is uniquely defined and has a well-known closed-form solution (the soft-thresholding operator). Thus, a unique solution exists.\n- **Objective:** The problem is formulated using precise mathematical definitions and numerical values, devoid of any subjective or ambiguous language.\n- **Completeness and Consistency:** All necessary numerical data ($A$, $y$, $x^{(0)}$, $w$, $\\lambda$, $t$) for performing one iteration are provided. The dimensions of the matrices and vectors are consistent: $A$ is $2 \\times 3$, $x^{(0)}$ is $3 \\times 1$, so $Ax^{(0)}$ is $2 \\times 1$. This is compatible with the dimension of $y$, which is $2 \\times 1$. The dimensions of $w$ ($3 \\times 1$) also match that of $x^{(0)}$.\n- **No other flaws:** The problem is a standard, non-trivial exercise in applying a fundamental optimization algorithm. It is not metaphorical, circular, or based on false premises.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe task is to compute one explicit proximal-gradient step, $x^{(1)}$, starting from $x^{(0)}$. The process involves three sequential calculations.\n\n**1. Compute the gradient $\\nabla f\\big(x^{(0)}\\big)$ from first principles.**\n\nThe function is $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$. The gradient $\\nabla f(x)$ is the unique vector satisfying the relation for the directional derivative $D f(x)[h]$ in any direction $h \\in \\mathbb{R}^n$:\n$$D f(x)[h] = \\lim_{\\alpha \\to 0} \\frac{f(x+\\alpha h) - f(x)}{\\alpha} = \\langle \\nabla f(x), h \\rangle = (\\nabla f(x))^{T} h.$$\nWe expand $f(x+\\alpha h)$:\n\\begin{align*}\nf(x+\\alpha h) = \\frac{1}{2} \\|A(x+\\alpha h) - y\\|_{2}^{2} \\\\\n= \\frac{1}{2} \\|(Ax - y) + \\alpha Ah\\|_{2}^{2} \\\\\n= \\frac{1}{2} \\langle (Ax - y) + \\alpha Ah, (Ax - y) + \\alpha Ah \\rangle \\\\\n= \\frac{1}{2} \\left( \\|Ax - y\\|_{2}^{2} + 2\\alpha \\langle Ax - y, Ah \\rangle + \\alpha^2 \\|Ah\\|_{2}^{2} \\right) \\\\\n= f(x) + \\alpha (Ax - y)^{T} (Ah) + \\frac{\\alpha^2}{2} \\|Ah\\|_{2}^{2}.\n\\end{align*}\nUsing the property of the transpose, $(Ax - y)^{T} (Ah) = (A^{T}(Ax-y))^{T} h$.\nThus, the directional derivative is:\n$$D f(x)[h] = \\lim_{\\alpha \\to 0} \\frac{f(x+\\alpha h) - f(x)}{\\alpha} = \\lim_{\\alpha \\to 0} \\left( (A^{T}(Ax-y))^{T} h + \\frac{\\alpha}{2} \\|Ah\\|_{2}^{2} \\right) = (A^{T}(Ax-y))^{T} h.$$\nBy identification, the gradient is $\\nabla f(x) = A^{T}(Ax-y)$.\n\nNow we compute this for the given data at $x^{(0)}$:\nFirst, calculate the residual $r = Ax^{(0)} - y$:\n$$Ax^{(0)} = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (-1)(-1) + (2)(1) \\\\ (0)(0) + (2)(-1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}.$$\n$$r = Ax^{(0)} - y = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}.$$\nNext, compute the gradient $\\nabla f(x^{(0)}) = A^{T}r$:\n$$A^{T} = \\begin{pmatrix} 1  0 \\\\ -1  2 \\\\ 2  1 \\end{pmatrix}.$$\n$$\\nabla f(x^{(0)}) = \\begin{pmatrix} 1  0 \\\\ -1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(-2) \\\\ (-1)(0) + (2)(-2) \\\\ (2)(0) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -4 \\\\ -2 \\end{pmatrix}.$$\n\n**2. Form the forward (gradient) step $v = x^{(0)} - t \\nabla f\\big(x^{(0)}\\big)$.**\n\nWith $t = \\frac{1}{5}$ and the computed gradient, we find the intermediate vector $v$:\n$$v = x^{(0)} - t \\nabla f(x^{(0)}) = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -4 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -\\frac{4}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} 0 - 0 \\\\ -1 - (-\\frac{4}{5}) \\\\ 1 - (-\\frac{2}{5}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{5} \\\\ \\frac{7}{5} \\end{pmatrix}.$$\n\n**3. Apply the weighted soft-thresholding to compute $x^{(1)} = \\operatorname{prox}_{t g}(v)$.**\n\nThe proximity operator is defined as:\n$$x^{(1)} = \\operatorname{prox}_{t g}(v) = \\arg\\min_{z \\in \\mathbb{R}^n} \\left( t g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right).$$\nSubstituting $g(z) = \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}|$, the objective becomes:\n$$\\arg\\min_{z \\in \\mathbb{R}^n} \\left( t \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}| + \\frac{1}{2}\\sum_{i=1}^{n} (z_i - v_i)^{2} \\right).$$\nThis optimization problem is separable, meaning we can solve for each component $z_i$ independently:\n$$x^{(1)}_{i} = \\arg\\min_{z_i \\in \\mathbb{R}} \\left( (t \\lambda w_{i}) |z_{i}| + \\frac{1}{2}(z_{i} - v_{i})^{2} \\right).$$\nThe solution is given by the soft-thresholding operator $S_{\\tau}(u) = \\operatorname{sign}(u) \\max(|u| - \\tau, 0)$, where the component-wise threshold is $\\tau_i = t \\lambda w_{i}$.\n\nLet's calculate the thresholds $\\tau_i$:\nThe product $t \\lambda = \\frac{1}{5} \\times \\frac{1}{2} = \\frac{1}{10}$.\nThe weight vector is $w = (2, 1, 3)^T$.\n$$\\tau_1 = t \\lambda w_1 = \\frac{1}{10} \\times 2 = \\frac{2}{10} = \\frac{1}{5}.$$\n$$\\tau_2 = t \\lambda w_2 = \\frac{1}{10} \\times 1 = \\frac{1}{10}.$$\n$$\\tau_3 = t \\lambda w_3 = \\frac{1}{10} \\times 3 = \\frac{3}{10}.$$\n\nNow we apply the soft-thresholding operator to each component of $v = (0, -1/5, 7/5)^T$:\nFor $i=1$: $v_1 = 0$, $\\tau_1 = \\frac{1}{5}$.\n$x^{(1)}_{1} = S_{1/5}(0) = \\operatorname{sign}(0) \\max(|0| - \\frac{1}{5}, 0) = 0$.\n\nFor $i=2$: $v_2 = -\\frac{1}{5}$, $\\tau_2 = \\frac{1}{10}$.\n$|v_2| = \\frac{1}{5} = \\frac{2}{10}$, which is greater than $\\tau_2 = \\frac{1}{10}$.\n$x^{(1)}_{2} = S_{1/10}(-\\frac{1}{5}) = \\operatorname{sign}(-\\frac{1}{5}) \\left( |-\\frac{1}{5}| - \\frac{1}{10} \\right) = (-1) \\left( \\frac{1}{5} - \\frac{1}{10} \\right) = (-1) \\left( \\frac{2}{10} - \\frac{1}{10} \\right) = -\\frac{1}{10}$.\n\nFor $i=3$: $v_3 = \\frac{7}{5}$, $\\tau_3 = \\frac{3}{10}$.\n$|v_3| = \\frac{7}{5} = \\frac{14}{10}$, which is greater than $\\tau_3 = \\frac{3}{10}$.\n$x^{(1)}_{3} = S_{3/10}(\\frac{7}{5}) = \\operatorname{sign}(\\frac{7}{5}) \\left( |\\frac{7}{5}| - \\frac{3}{10} \\right) = (+1) \\left( \\frac{7}{5} - \\frac{3}{10} \\right) = \\frac{14}{10} - \\frac{3}{10} = \\frac{11}{10}$.\n\nCombining the components, the next iterate $x^{(1)}$ is:\n$$x^{(1)} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{10} \\\\ \\frac{11}{10} \\end{pmatrix}.$$\nThe problem requests the answer as a single row vector.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0  -\\frac{1}{10}  \\frac{11}{10} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Effective reweighting schemes tailor the sparsity penalty to match the known structure of the signal. This problem  presents a thought experiment to quantify the significant performance gain achieved by using block-informed weights for a block-sparse signal, as opposed to a mismatched scalar approach. By deriving the conditions for exact recovery, you will appreciate how prior knowledge can be encoded into the weights to lower the threshold for successful signal reconstruction.",
            "id": "3454474",
            "problem": "Consider a single iteration of Iterative Reweighted $\\ell_{1}$ minimization (IRL1) within the setting of compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix whose columns are unit-norm and whose mutual coherence is bounded by $\\mu$, where mutual coherence is defined as $\\mu \\triangleq \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$, with $a_{i}$ denoting the $i$-th column of $A$. Index the coordinates $\\{1,\\dots,n\\}$ by a partition into $G$ disjoint, equal-sized blocks of size $d$, and suppose the true signal $x^{\\star} \\in \\mathbb{R}^{n}$ is block-sparse: exactly $K$ blocks are active, and within each active block every entry is nonzero and has equal magnitude $a0$. Measurements are noiseless, $y = A x^{\\star}$, and $m \\geq K d$ so that $A_{S}$ has full column rank for the support set $S \\subset \\{1,\\dots,n\\}$ with $|S| = K d$.\n\nAssume the following worst-case but coherent and self-consistent geometric conditions hold: $A_{S}^{\\top} A_{S} = I_{Kd}$ and, for any $i \\in S^{c}$ and $j \\in S$, $|a_{i}^{\\top} a_{j}| = \\mu$. Consider a one-shot weighted $\\ell_{1}$ recovery step\n$\\min_{x \\in \\mathbb{R}^{n}} \\|W x\\|_{1} \\ \\text{subject to} \\ A x = y$,\nwhere $W = \\mathrm{diag}(w_{1},\\dots,w_{n}) \\succ 0$ are weights computed from a previous iterate $x^{\\mathrm{prev}}$. Model $x^{\\mathrm{prev}}$ as follows: for $i \\in S$, $x^{\\mathrm{prev}}_{i} = a$, and for $i \\in S^{c}$, $x^{\\mathrm{prev}}_{i} = 0$. Weights are clipped to avoid degeneracy: for all $i$, $w_{i} = \\min\\{w_{\\max}, 1/(u_{i}+\\varepsilon)\\}$ for some $\\varepsilon  0$ and a nonnegative proxy $u_{i}$.\n\nTwo weight designs are considered:\n1) Scalar IRL1 (model-mismatched): $u_{i} = |x^{\\mathrm{prev}}_{i}|$, so that for $i \\in S$, $w^{(s)}_{\\mathrm{in}} = 1/(a+\\varepsilon)$, and for $i \\in S^{c}$, $w^{(s)}_{0} = \\min\\{w_{\\max}, 1/\\varepsilon\\}$. Assume the same weight is used for all $i \\in S^{c}$, and denote $w_{0} \\triangleq w^{(s)}_{0}$.\n2) Block-informed reweighting: for $i$ in block $g$, $u_{i} = \\|x^{\\mathrm{prev}}_{g}\\|_{2}$. Then for $i \\in S$, $w^{(b)}_{\\mathrm{in}} = 1/(\\sqrt{d}\\, a + \\varepsilon)$, and for $i \\in S^{c}$, the same clipping yields $w^{(b)}_{0} = \\min\\{w_{\\max}, 1/\\varepsilon\\}$. Assume the same $w_{0}$ is used for all $i \\in S^{c}$.\n\nStarting from the definitions of subgradients and the Karush–Kuhn–Tucker (KKT) optimality conditions for weighted $\\ell_{1}$ minimization, together with the mutual coherence framework, derive a sufficient condition for exact support recovery for the weighted $\\ell_{1}$ program in terms of $A$, $W$, and the support $S$. Then, under the structural simplifications above, specialize this condition to obtain the smallest amplitudes $a^{(s)}_{\\min}$ and $a^{(b)}_{\\min}$ that guarantee exact support recovery for the scalar and block-informed weights, respectively, expressed in terms of $\\mu$, $K$, $d$, $\\varepsilon$, and $w_{0}$. Assume parameters satisfy $\\mu K d / w_{0}  \\varepsilon$ so that both thresholds are positive and nontrivial.\n\nFinally, quantify the model-mismatch penalty by deriving the closed-form ratio $R \\triangleq a^{(s)}_{\\min} / a^{(b)}_{\\min}$. Provide your final answer as a single simplified analytical expression for $R$. Do not include units. Do not round.",
            "solution": "The problem asks for the ratio of minimum signal amplitudes required for exact support recovery in a single iteration of two different Iterative Reweighted $\\ell_{1}$ (IRL1) schemes. We begin by establishing the general condition for exact support recovery for the weighted $\\ell_{1}$ minimization problem.\n\nThe optimization problem is\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|W x\\|_{1} \\ \\text{subject to} \\ A x = y $$\nwhere $W = \\mathrm{diag}(w_{1},\\dots,w_{n})$ is a diagonal matrix of positive weights, $A \\in \\mathbb{R}^{m \\times n}$ is the sensing matrix, and $y = A x^{\\star}$ are the noiseless measurements of a true signal $x^{\\star}$.\n\nThe true signal $x^{\\star}$ is a feasible point. For it to be the unique solution, it must satisfy the Karush-Kuhn-Tucker (KKT) conditions. The Lagrangian for this problem is $L(x, \\nu) = \\|W x\\|_{1} + \\nu^{\\top}(A x - y)$. The KKT conditions state that there must exist a dual vector $\\nu \\in \\mathbb{R}^{m}$ such that the subgradient of the Lagrangian with respect to $x$ at $x=x^{\\star}$ contains the zero vector.\n$$ 0 \\in \\partial \\left( \\|W x\\|_{1} \\right)|_{x=x^{\\star}} + A^{\\top}\\nu $$\nThis is equivalent to $-A^{\\top}\\nu \\in \\partial \\left( \\|W x\\|_{1} \\right)|_{x=x^{\\star}}$. The subgradient of the weighted $\\ell_1$ norm is given by $(\\partial \\|Wx\\|_1)_i = w_i \\partial |x_i|$. Let $S$ be the support of $x^{\\star}$, i.e., $S = \\{i \\mid x^{\\star}_i \\neq 0\\}$. The KKT conditions can be written component-wise:\n1. For $i \\in S$, $x^{\\star}_i \\neq 0$, so $\\partial |x_i||_{x=x^{\\star}_i} = \\{\\mathrm{sgn}(x^{\\star}_i)\\}$. The condition is $(A^{\\top}\\nu)_i = -w_i \\mathrm{sgn}(x^{\\star}_i)$.\n2. For $i \\in S^c$, $x^{\\star}_i = 0$, so $\\partial |x_i||_{x=x^{\\star}_i} = [-1, 1]$. The condition is $|(A^{\\top}\\nu)_i| \\leq w_i$.\n\nFor the solution with support $S$ to be unique, the condition on the non-support set $S^c$ must be strict: $|(A^{\\top}\\nu)_i|  w_i$ for all $i \\in S^c$.\n\nLet $A_S$ denote the matrix composed of columns of $A$ indexed by $S$. The first condition can be written in matrix form as $A_S^{\\top}\\nu = -W_S \\mathrm{sgn}(x^{\\star}_S)$, where $W_S$ is the diagonal matrix of weights for indices in $S$. Since the problem states $A_S$ has full column rank, $A_S^{\\top}A_S$ is invertible. We can find a dual vector $\\nu$ that satisfies this. A particular solution for $\\nu$ is given by $\\nu = -A_S(A_S^{\\top}A_S)^{-1}W_S \\mathrm{sgn}(x^{\\star}_S)$.\n\nSubstituting this into the strict inequality for $i \\in S^c$:\n$$ |a_i^{\\top} \\left( -A_S(A_S^{\\top}A_S)^{-1}W_S \\mathrm{sgn}(x^{\\star}_S) \\right) |  w_i \\quad \\forall i \\in S^c $$\nThis is the general sufficient condition for exact support recovery, often called a dual certificate condition.\n\nNow, we apply the structural simplifications given in the problem. We are given $A_S^{\\top}A_S = I_{Kd}$, where $|S| = Kd$. The condition simplifies to:\n$$ |a_i^{\\top} A_S W_S \\mathrm{sgn}(x^{\\star}_S)|  w_i \\quad \\forall i \\in S^c $$\nThe term inside the absolute value is a scalar product: $a_i^{\\top} \\sum_{j \\in S} a_j (W_S \\mathrm{sgn}(x^{\\star}_S))_j = \\sum_{j \\in S} (a_i^{\\top} a_j) w_j \\mathrm{sgn}(x^{\\star}_j)$.\nSo, for every $i \\in S^c$, we need:\n$$ \\left| \\sum_{j \\in S} (a_i^{\\top} a_j) w_j \\mathrm{sgn}(x^{\\star}_j) \\right|  w_i $$\nTo obtain a sufficient condition that is independent of the signs of $x^\\star_j$ and the specific inner products, we use the triangle inequality and the worst-case coherence assumption. For any $i \\in S^c$ and $j \\in S$, we are given $|a_i^{\\top} a_j| = \\mu$.\n$$ \\left| \\sum_{j \\in S} (a_i^{\\top} a_j) w_j \\mathrm{sgn}(x^{\\star}_j) \\right| \\leq \\sum_{j \\in S} |a_i^{\\top} a_j| w_j |\\mathrm{sgn}(x^{\\star}_j)| = \\sum_{j \\in S} \\mu w_j = \\mu \\sum_{j \\in S} w_j $$\nThus, a sufficient condition for recovery is $\\mu \\sum_{j \\in S} w_j  w_i$ for all $i \\in S^c$.\n\nNext, we specialize this condition for the two weighting schemes.\n\nCase 1: Scalar IRL1 (model-mismatched)\nThe weights are given by $w_j = w^{(s)}_{\\mathrm{in}} = 1/(a+\\varepsilon)$ for $j \\in S$, and $w_i = w_0$ for $i \\in S^c$. The support size is $|S|=Kd$.\nThe sum of weights on the support is $\\sum_{j \\in S} w_j = Kd \\cdot w^{(s)}_{\\mathrm{in}} = \\frac{Kd}{a+\\varepsilon}$.\nThe recovery condition becomes:\n$$ \\mu \\frac{Kd}{a+\\varepsilon}  w_0 $$\nWe solve for the signal amplitude $a$:\n$$ \\frac{\\mu Kd}{w_0}  a + \\varepsilon \\implies a  \\frac{\\mu Kd}{w_0} - \\varepsilon $$\nThe smallest amplitude that guarantees recovery under this condition is the threshold:\n$$ a^{(s)}_{\\min} = \\frac{\\mu Kd}{w_0} - \\varepsilon $$\n\nCase 2: Block-informed reweighting\nThe weights are $w_j = w^{(b)}_{\\mathrm{in}} = 1/(\\sqrt{d} a + \\varepsilon)$ for $j \\in S$, and $w_i = w_0$ for $i \\in S^c$.\nThe sum of weights on the support is $\\sum_{j \\in S} w_j = Kd \\cdot w^{(b)}_{\\mathrm{in}} = \\frac{Kd}{\\sqrt{d} a + \\varepsilon}$.\nThe recovery condition becomes:\n$$ \\mu \\frac{Kd}{\\sqrt{d} a + \\varepsilon}  w_0 $$\nSolving for $a$:\n$$ \\frac{\\mu Kd}{w_0}  \\sqrt{d} a + \\varepsilon \\implies \\sqrt{d} a  \\frac{\\mu Kd}{w_0} - \\varepsilon \\implies a  \\frac{1}{\\sqrt{d}}\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right) $$\nThe minimum amplitude is:\n$$ a^{(b)}_{\\min} = \\frac{1}{\\sqrt{d}}\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right) $$\nThe problem states that $\\mu K d / w_{0}  \\varepsilon$, which ensures that both $a^{(s)}_{\\min}$ and $a^{(b)}_{\\min}$ are positive.\n\nFinally, we compute the model-mismatch penalty ratio $R = a^{(s)}_{\\min} / a^{(b)}_{\\min}$.\n$$ R = \\frac{\\frac{\\mu Kd}{w_0} - \\varepsilon}{\\frac{1}{\\sqrt{d}}\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right)} $$\nThe term $\\left(\\frac{\\mu Kd}{w_0} - \\varepsilon\\right)$ is a positive common factor in the numerator and denominator, so it cancels.\n$$ R = \\frac{1}{1/\\sqrt{d}} = \\sqrt{d} $$\nThis result quantifies the benefit of using a block-informed weighting strategy. The required signal magnitude for guaranteed recovery is reduced by a factor of $\\sqrt{d}$ compared to the standard scalar reweighting approach, demonstrating a significant performance gain when the block structure is known and exploited.",
            "answer": "$$\n\\boxed{\\sqrt{d}}\n$$"
        },
        {
            "introduction": "The principle of reweighting extends far beyond simple scalar sparsity, finding power in various structured signal models. This practice  guides you through generalizing the iterative reweighting framework to the group-sparse setting, a common scenario in applications like genomics and neuroimaging. By deriving the group-specific weights via Majorization-Minimization and applying the corresponding group soft-thresholding operator, you will learn how to adapt this powerful technique to more complex structural priors.",
            "id": "3454450",
            "problem": "Consider a block-sparse signal model with a fixed partition of indices into disjoint groups $\\mathcal{G} = \\{g_{1}, g_{2}, \\dots, g_{M}\\}$, where each group $g \\in \\mathcal{G}$ is a subset of $\\{1,2,\\dots,N\\}$. For a vector $\\mathbf{x} \\in \\mathbb{R}^{N}$ and a subset $g$, denote by $\\mathbf{x}_{g}$ the subvector of $\\mathbf{x}$ restricted to indices in $g$. The group $\\ell_{1,2}$ norm of $\\mathbf{x}$ is defined as $\\sum_{g \\in \\mathcal{G}} \\|\\mathbf{x}_{g}\\|_{2}$. To promote block-sparsity, consider a concave, nondecreasing function $\\psi:[0,\\infty)\\to\\mathbb{R}$ (e.g., a logarithmic surrogate) applied to group norms, leading to the objective $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$.\n\nStarting from the standard principles of convex analysis and majorization-minimization (MM), perform the following:\n\n1. Formulate an iterative reweighted group $\\ell_{1,2}$ scheme as a sequence of convex subproblems obtained by majorizing $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ at the current iterate. Your formulation must explicitly show how the surrogate becomes a weighted group $\\ell_{1,2}$ penalty and identify the group weights in terms of the current iterate and $\\psi$.\n\n2. For a single convex subproblem of the form\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{N}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2},\n$$\nderive, from first principles using optimality conditions, the explicit closed-form expression of the group-weighted soft-thresholding operator that maps $\\mathbf{v}$ to the minimizer. Your derivation must start from the definition of the subdifferential of the Euclidean norm and the separability across groups, and proceed logically to the final operator without invoking pre-memorized formulas.\n\n3. Provide a complete numerical example of one MM iteration for a six-dimensional signal with two groups $g_{1}=\\{1,2,3\\}$ and $g_{2}=\\{4,5,6\\}$. Use the data fidelity model\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{6}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{b}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}^{(k)}\\|\\mathbf{x}_{g}\\|_{2},\n$$\nwhere the weights at iteration $k$ are obtained from $\\mathbf{x}^{(k)}$ via the MM linearization with $\\psi(s)=\\ln(s+\\epsilon)$ and a fixed $\\epsilon0$. Let the initial iterate be $\\mathbf{x}^{(0)}=\\mathbf{b}$, with $\\mathbf{b}=(3,4,0,0,2,0)$, the parameter $\\lambda=1$, and the stabilization constant $\\epsilon=\\tfrac{1}{2}$. Compute the weights $w_{g}^{(0)}$ and then compute the next iterate $\\mathbf{x}^{(1)}$ by applying the derived group-weighted soft-thresholding operator to $\\mathbf{b}$. Express the final answer for $\\mathbf{x}^{(1)}$ exactly; no rounding is required.\n\nYour final answer must be the single row vector $\\mathbf{x}^{(1)}$.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. We proceed with a detailed solution.\n\nThe problem is divided into three parts. We will address them sequentially.\n\n**Part 1: Formulation of the Iterative Reweighted Scheme**\n\nThe goal is to minimize an objective function of the form $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$, where $\\psi$ is a concave, nondecreasing function. We employ the Majorization-Minimization (MM) algorithm. The core idea of an MM algorithm is to replace a difficult optimization problem with a sequence of simpler ones. At each iteration $k$, we minimize a surrogate function $Q(\\mathbf{x} | \\mathbf{x}^{(k)})$ that majorizes the original objective $\\Phi(\\mathbf{x}) = \\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ at the current iterate $\\mathbf{x}^{(k)}$. The surrogate must satisfy:\n$1$. $Q(\\mathbf{x} | \\mathbf{x}^{(k)}) \\ge \\Phi(\\mathbf{x})$ for all $\\mathbf{x}$.\n$2$. $Q(\\mathbf{x}^{(k)} | \\mathbf{x}^{(k)}) = \\Phi(\\mathbf{x})$.\nThe next iterate is then found by $\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} Q(\\mathbf{x} | \\mathbf{x}^{(k)})$.\n\nSince $\\psi: [0,\\infty)\\to\\mathbb{R}$ is a concave and differentiable function, we can construct a majorizer using its first-order Taylor approximation. For any concave function $f$, we have the property $f(z) \\le f(z_0) + f'(z_0)(z - z_0)$ for all $z, z_0$ in its domain. This inequality provides a tight upper bound at $z=z_0$.\n\nWe apply this property to each term $\\psi(\\|\\mathbf{x}_{g}\\|_{2})$ in the sum. Let $z_g = \\|\\mathbf{x}_g\\|_2$ and $z_g^{(k)} = \\|\\mathbf{x}_g^{(k)}\\|_2$. The majorization for the $g$-th term is:\n$$\n\\psi(\\|\\mathbf{x}_{g}\\|_{2}) \\le \\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) + \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\cdot (\\|\\mathbf{x}_{g}\\|_{2} - \\|\\mathbf{x}_{g}^{(k)}\\|_{2})\n$$\nSumming over all groups $g \\in \\mathcal{G}$, we obtain a surrogate for the entire objective function:\n$$\nQ(\\mathbf{x} | \\mathbf{x}^{(k)}) = \\sum_{g \\in \\mathcal{G}} \\left[ \\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) + \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\cdot (\\|\\mathbf{x}_{g}\\|_{2} - \\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\right]\n$$\nTo find the next iterate $\\mathbf{x}^{(k+1)}$, we minimize this surrogate function. When minimizing with respect to $\\mathbf{x}$, we can discard terms that are constant with respect to $\\mathbf{x}$, which are $\\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})$ and $-\\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})\\|\\mathbf{x}_{g}^{(k)}\\|_{2}$. The minimization problem for the $(k+1)$-th iteration becomes:\n$$\n\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} \\sum_{g \\in \\mathcal{G}} \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\|\\mathbf{x}_{g}\\|_{2}\n$$\nThis is a weighted group $\\ell_{1,2}$ minimization problem. The weight for each group $g$ at iteration $k$ is given by the derivative of $\\psi$ evaluated at the norm of the group's current iterate $\\mathbf{x}_g^{(k)}$:\n$$\nw_g^{(k)} = \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})\n$$\nThe full iterative reweighted scheme, when combined with a data fidelity term like $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2$, involves solving a sequence of convex subproblems of the form:\n$$\n\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\lambda \\sum_{g \\in \\mathcal{G}} w_g^{(k)} \\|\\mathbf{x}_g\\|_2\n$$\n\n**Part 2: Derivation of the Group-Weighted Soft-Thresholding Operator**\n\nWe are asked to find the closed-form solution to the convex subproblem:\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{N}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2}\n$$\nLet the objective function be $F(\\mathbf{x}) = f(\\mathbf{x}) + h(\\mathbf{x})$, where $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2}$ is a smooth, strongly convex function, and $h(\\mathbf{x}) = \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2}$ is a convex, non-smooth regularizer. A vector $\\mathbf{x}^{*}$ is the unique minimizer if and only if it satisfies the first-order optimality condition from convex analysis:\n$$\n\\mathbf{0} \\in \\partial F(\\mathbf{x}^{*}) = \\nabla f(\\mathbf{x}^{*}) + \\partial h(\\mathbf{x}^{*})\n$$\nThe gradient of $f(\\mathbf{x})$ is $\\nabla f(\\mathbf{x}) = \\mathbf{x} - \\mathbf{v}$. The condition becomes:\n$$\n\\mathbf{0} \\in (\\mathbf{x}^{*} - \\mathbf{v}) + \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}^{*}\\|_{2}\\right) \\iff \\mathbf{v} - \\mathbf{x}^{*} \\in \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}^{*}\\|_{2}\\right)\n$$\nSince the groups $\\{g\\}$ form a partition of the indices, the regularizer $h(\\mathbf{x})$ is separable with respect to the subvectors $\\mathbf{x}_g$. That is, $h(\\mathbf{x}) = \\sum_{g \\in \\mathcal{G}} h_g(\\mathbf{x}_g)$ where $h_g(\\mathbf{x}_g) = \\lambda w_g \\|\\mathbf{x}_g\\|_2$. The subdifferential $\\partial h(\\mathbf{x})$ is the Cartesian product of the subdifferentials $\\partial h_g(\\mathbf{x}_g)$. This separability means the optimization problem decouples into $M$ independent problems, one for each group $g \\in \\mathcal{G}$:\n$$\n\\min_{\\mathbf{x}_g} \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{v}_g\\|_2^2 + \\lambda w_g \\|\\mathbf{x}_g\\|_2\n$$\nLet us solve for a single group $g$. Let $\\mathbf{y} = \\mathbf{x}_g$, $\\mathbf{u} = \\mathbf{v}_g$, and $\\mu = \\lambda w_g \\ge 0$. The problem is $\\min_{\\mathbf{y}} \\frac{1}{2} \\|\\mathbf{y}-\\mathbf{u}\\|_2^2 + \\mu\\|\\mathbf{y}\\|_2$. The optimality condition for the minimizer $\\mathbf{y}^{*}$ is:\n$$\n\\mathbf{u} - \\mathbf{y}^{*} \\in \\partial(\\mu \\|\\mathbf{y}^{*}\\|_2) = \\mu \\partial\\|\\mathbf{y}^{*}\\|_2\n$$\nThe subdifferential of the Euclidean norm is:\n$$\n\\partial\\|\\mathbf{y}\\|_2 = \n\\begin{cases} \n\\{\\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|_2}\\}  \\text{if } \\mathbf{y} \\neq \\mathbf{0} \\\\\n\\{\\mathbf{z} \\mid \\|\\mathbf{z}\\|_2 \\le 1\\}  \\text{if } \\mathbf{y} = \\mathbf{0} \n\\end{cases}\n$$\nWe analyze two cases for the solution $\\mathbf{y}^{*}$:\nCase 1: $\\mathbf{y}^{*} \\neq \\mathbf{0}$. The optimality condition becomes $\\mathbf{u} - \\mathbf{y}^{*} = \\mu \\frac{\\mathbf{y}^{*}}{\\|\\mathbf{y}^{*}\\|_2}$. Rearranging for $\\mathbf{u}$, we get $\\mathbf{u} = \\mathbf{y}^{*} + \\mu \\frac{\\mathbf{y}^{*}}{\\|\\mathbf{y}^{*}\\|_2} = \\mathbf{y}^{*} \\left(1 + \\frac{\\mu}{\\|\\mathbf{y}^{*}\\|_2}\\right)$. This implies that $\\mathbf{u}$ and $\\mathbf{y}^{*}$ are collinear, so $\\mathbf{y}^{*} = c \\mathbf{u}$ for some scalar $c  0$. Taking the norm, we have $\\|\\mathbf{y}^{*}\\|_2 = c\\|\\mathbf{u}\\|_2$. Substituting back into the main expression for $\\mathbf{u}$:\n$\\|\\mathbf{u}\\|_2 = \\|\\mathbf{y}^{*}\\|_2 \\left(1 + \\frac{\\mu}{\\|\\mathbf{y}^{*}\\|_2}\\right) = \\|\\mathbf{y}^{*}\\|_2 + \\mu$.\nThis gives $\\|\\mathbf{y}^{*}\\|_2 = \\|\\mathbf{u}\\|_2 - \\mu$. Since $\\|\\mathbf{y}^{*}\\|_2  0$ in this case, we must have $\\|\\mathbf{u}\\|_2  \\mu$. The solution is then:\n$$\n\\mathbf{y}^{*} = \\frac{\\|\\mathbf{y}^{*}\\|_2}{\\|\\mathbf{u}\\|_2}\\mathbf{u} = \\frac{\\|\\mathbf{u}\\|_2 - \\mu}{\\|\\mathbf{u}\\|_2}\\mathbf{u} = \\left(1 - \\frac{\\mu}{\\|\\mathbf{u}\\|_2}\\right)\\mathbf{u}\n$$\nCase 2: $\\mathbf{y}^{*} = \\mathbf{0}$. The optimality condition becomes $\\mathbf{u} - \\mathbf{0} \\in \\mu \\partial\\|\\mathbf{0}\\|_2$, which is $\\mathbf{u} \\in \\{\\mathbf{z} \\mid \\|\\mathbf{z}\\|_2 \\le \\mu\\}$. This is equivalent to the condition $\\|\\mathbf{u}\\|_2 \\le \\mu$.\n\nCombining both cases, we can write the solution for $\\mathbf{y}^{*}$ compactly using the positive part function $(a)_+ = \\max(a, 0)$:\n$$\n\\mathbf{y}^{*} = \\left(1 - \\frac{\\mu}{\\|\\mathbf{u}\\|_2}\\right)_+ \\mathbf{u}\n$$\nThis expression is the group soft-thresholding operator. If $\\|\\mathbf{u}\\|_2=0$, the fraction is undefined, but the condition $\\|\\mathbf{u}\\|_2 \\le \\mu$ holds, so $\\mathbf{y}^*=\\mathbf{0}$, which is the correct limit.\nReverting to the original notation for group $g$, the minimizer $\\mathbf{x}_g^*$ is:\n$$\n\\mathbf{x}_g^{*} = \\left(1 - \\frac{\\lambda w_g}{\\|\\mathbf{v}_g\\|_2}\\right)_+ \\mathbf{v}_g\n$$\nThe final solution $\\mathbf{x}^*$ is obtained by concatenating the solutions $\\mathbf{x}_g^*$ for all groups.\n\n**Part 3: Numerical Example**\n\nWe perform one MM iteration. The problem is to find $\\mathbf{x}^{(1)}$ by solving\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{6}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{b}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}^{(0)}\\|\\mathbf{x}_{g}\\|_{2}\n$$\nGiven:\n- Signal dimension $N=6$.\n- Groups: $g_1 = \\{1, 2, 3\\}$, $g_2 = \\{4, 5, 6\\}$.\n- Initial iterate $\\mathbf{x}^{(0)} = \\mathbf{b} = (3, 4, 0, 0, 2, 0)^T$.\n- Concave function: $\\psi(s) = \\ln(s+\\epsilon)$ with $\\epsilon=\\frac{1}{2}$.\n- Parameter: $\\lambda = 1$.\n\nFirst, we compute the weights $w_g^{(0)} = \\psi'(\\|\\mathbf{x}_g^{(0)}\\|_2)$.\nThe derivative of $\\psi(s)$ is $\\psi'(s) = \\frac{1}{s+\\epsilon}$.\nThe subvectors of the initial iterate are:\n$\\mathbf{x}_{g_1}^{(0)} = (3, 4, 0)^T$\n$\\mathbf{x}_{g_2}^{(0)} = (0, 2, 0)^T$\nNext, we compute their Euclidean norms:\n$\\|\\mathbf{x}_{g_1}^{(0)}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\n$\\|\\mathbf{x}_{g_2}^{(0)}\\|_2 = \\sqrt{0^2 + 2^2 + 0^2} = \\sqrt{4} = 2$.\nNow we can compute the weights for iteration $k=0$:\n$w_{g_1}^{(0)} = \\psi'(\\|\\mathbf{x}_{g_1}^{(0)}\\|_2) = \\frac{1}{5 + \\epsilon} = \\frac{1}{5 + \\frac{1}{2}} = \\frac{1}{\\frac{11}{2}} = \\frac{2}{11}$.\n$w_{g_2}^{(0)} = \\psi'(\\|\\mathbf{x}_{g_2}^{(0)}\\|_2) = \\frac{1}{2 + \\epsilon} = \\frac{1}{2 + \\frac{1}{2}} = \\frac{1}{\\frac{5}{2}} = \\frac{2}{5}$.\n\nSecond, we compute the next iterate $\\mathbf{x}^{(1)}$ by applying the group-weighted soft-thresholding operator derived in Part 2, with $\\mathbf{v}=\\mathbf{b}$. The solution for each group $\\mathbf{x}_g^{(1)}$ is:\n$$\n\\mathbf{x}_g^{(1)} = \\left(1 - \\frac{\\lambda w_g^{(0)}}{\\|\\mathbf{b}_g\\|_2}\\right)_+ \\mathbf{b}_g\n$$\nThe subvectors of $\\mathbf{b}$ are $\\mathbf{b}_{g_1} = (3, 4, 0)^T$ and $\\mathbf{b}_{g_2} = (0, 2, 0)^T$. Their norms are $\\|\\mathbf{b}_{g_1}\\|_2 = 5$ and $\\|\\mathbf{b}_{g_2}\\|_2 = 2$.\n\nFor group $g_1$:\nThe threshold is $\\mu_1 = \\lambda w_{g_1}^{(0)} = 1 \\cdot \\frac{2}{11} = \\frac{2}{11}$.\nSince $\\|\\mathbf{b}_{g_1}\\|_2 = 5  \\frac{2}{11}$, the group is not set to zero. The scaling factor is:\n$$\n\\left(1 - \\frac{\\mu_1}{\\|\\mathbf{b}_{g_1}\\|_2}\\right) = 1 - \\frac{\\frac{2}{11}}{5} = 1 - \\frac{2}{55} = \\frac{53}{55}\n$$\nSo, the updated subvector is:\n$$\n\\mathbf{x}_{g_1}^{(1)} = \\frac{53}{55} \\mathbf{b}_{g_1} = \\frac{53}{55} \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{159}{55} \\\\ \\frac{212}{55} \\\\ 0 \\end{pmatrix}\n$$\n\nFor group $g_2$:\nThe threshold is $\\mu_2 = \\lambda w_{g_2}^{(0)} = 1 \\cdot \\frac{2}{5} = \\frac{2}{5}$.\nSince $\\|\\mathbf{b}_{g_2}\\|_2 = 2  \\frac{2}{5}$, this group is also not set to zero. The scaling factor is:\n$$\n\\left(1 - \\frac{\\mu_2}{\\|\\mathbf{b}_{g_2}\\|_2}\\right) = 1 - \\frac{\\frac{2}{5}}{2} = 1 - \\frac{1}{5} = \\frac{4}{5}\n$$\nSo, the updated subvector is:\n$$\n\\mathbf{x}_{g_2}^{(1)} = \\frac{4}{5} \\mathbf{b}_{g_2} = \\frac{4}{5} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{8}{5} \\\\ 0 \\end{pmatrix}\n$$\n\nFinally, we assemble the full vector $\\mathbf{x}^{(1)}$ by concatenating $\\mathbf{x}_{g_1}^{(1)}$ and $\\mathbf{x}_{g_2}^{(1)}$:\n$$\n\\mathbf{x}^{(1)} = \\left( \\frac{159}{55}, \\frac{212}{55}, 0, 0, \\frac{8}{5}, 0 \\right)^T\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{159}{55}  \\frac{212}{55}  0  0  \\frac{8}{5}  0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}