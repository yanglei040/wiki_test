{
    "hands_on_practices": [
        {
            "introduction": "迭代重加权 $\\ell_1$ 最小化算法的核心思想是将一个复杂的非凸问题分解为一系列凸的加权 $\\ell_1$ 子问题。这些子问题通常通过近端梯度法等迭代算法求解。本练习  旨在让你亲手实践这一核心计算步骤，通过一个具体的数值例子，你将执行一步近端梯度迭代，从而对算法引擎的内部工作机制有更深刻的理解。",
            "id": "3454424",
            "problem": "考虑在压缩感知和稀疏优化的迭代重加权 $\\ell_1$ 最小化中使用的复合优化模型：\n$$F(x) \\equiv f(x) + g(x), \\quad f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}, \\quad g(x) = \\lambda \\sum_{i=1}^{n} w_{i} |x_{i}|,$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$\\lambda > 0$ 和 $w \\in \\mathbb{R}^{n}_{+}$ 是给定数据。近端梯度法（也称为 Proximal Gradient Method (PGM)）执行以下形式的迭代：\n$$x^{(k+1)} = \\operatorname{prox}_{t g}\\Big(x^{(k)} - t \\nabla f\\big(x^{(k)}\\big)\\Big),$$\n其中 $t > 0$ 是步长，$\\operatorname{prox}_{t g}$ 是 $t g$ 的近端算子。\n\n仅使用梯度的定义（即：将方向映射到方向导数的线性泛函）、$A$ 的线性性质以及近端算子的定义（即：强凸可分函数的唯一最小化子），计算在点\n$$x^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix},$$\n的以下数据的一个显式的近端梯度步骤\n$$A = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad w = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}, \\quad \\lambda = \\frac{1}{2}, \\quad t = \\frac{1}{5}.$$\n\n你的任务是通过以下步骤执行这单次迭代：\n1. 从第一性原理计算梯度 $\\nabla f\\big(x^{(0)}\\big)$。\n2. 形成前向（梯度）步骤 $v = x^{(0)} - t \\nabla f\\big(x^{(0)}\\big)$。\n3. 应用由 $t g$ 的近端算子导出的加权软阈值，该算子以阈值 $t \\lambda w_{i}$ 逐分量作用。\n\n将最终迭代结果 $x^{(1)}$ 表示为精确形式的单个行向量。无需四舍五入。",
            "solution": "首先根据指定标准验证问题。\n\n### 问题验证\n\n**步骤 1：提取给定条件**\n- 优化模型: $F(x) \\equiv f(x) + g(x)$\n- 可微部分: $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$\n- 非光滑部分: $g(x) = \\lambda \\sum_{i=1}^{n} w_{i} |x_{i}|$\n- 数据类型: $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda  0$, $w \\in \\mathbb{R}^{n}_{+}$\n- 迭代公式: $x^{(k+1)} = \\operatorname{prox}_{t g}\\Big(x^{(k)} - t \\nabla f\\big(x^{(k)}\\big)\\Big)$\n- 步长: $t  0$\n- 初始点: $x^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix}$\n- 矩阵 $A$: $A = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix}$\n- 向量 $y$: $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$\n- 权重向量 $w$: $w = \\begin{pmatrix} 2 \\\\ 1 \\\\ 3 \\end{pmatrix}$\n- 正则化参数: $\\lambda = \\frac{1}{2}$\n- 步长: $t = \\frac{1}{5}$\n\n该问题要求执行单个近端梯度步骤，包括三个子任务：计算梯度、执行前向步骤和应用近端算子。\n\n**步骤 2：使用提取的给定条件进行验证**\n- **科学依据：** 该问题设置在凸优化和稀疏信号恢复的明确定义的数学框架中。复合模型、近端梯度法以及特定函数（$f(x)$作为最小二乘损失，$g(x)$作为加权 $\\ell_1$ 范数惩罚）在该领域是典型的。该问题严格遵守已建立的数学原理。\n- **适定性：** 该问题要求计算一个单一的、确定性的迭代。函数 $f(x)$ 是凸且连续可微的，而 $g(x)$ 是凸的。缩放$\\ell_1$范数的近端算子是唯一定义的，并具有众所周知的闭式解（软阈值算子）。因此，存在唯一解。\n- **客观性：** 问题使用精确的数学定义和数值进行表述，没有任何主观或模棱两可的语言。\n- **完整性和一致性：** 提供了执行一次迭代所需的所有数值数据（$A$，$y$，$x^{(0)}$，$w$，$\\lambda$，$t$）。矩阵和向量的维度是一致的：$A$ 是 $2 \\times 3$，$x^{(0)}$ 是 $3 \\times 1$，因此 $Ax^{(0)}$ 是 $2 \\times 1$。这与 $y$ 的维度（$2 \\times 1$）兼容。$w$ 的维度（$3 \\times 1$）也与 $x^{(0)}$ 的维度匹配。\n- **无其他缺陷：** 该问题是应用一种基本优化算法的标准、非平凡的练习。它不是比喻性的、循环的，也不是基于错误的前提。\n\n**步骤 3：结论与行动**\n问题有效。将提供完整解答。\n\n### 解答\n\n任务是从 $x^{(0)}$ 开始，计算一个显式的近端梯度步骤，得到 $x^{(1)}$。该过程涉及三个顺序计算。\n\n**1. 从第一性原理计算梯度 $\\nabla f\\big(x^{(0)}\\big)$。**\n\n函数为 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$。梯度 $\\nabla f(x)$ 是满足在任何方向 $h \\in \\mathbb{R}^n$ 上方向导数 $D f(x)[h]$ 关系的唯一向量：\n$$D f(x)[h] = \\lim_{\\alpha \\to 0} \\frac{f(x+\\alpha h) - f(x)}{\\alpha} = \\langle \\nabla f(x), h \\rangle = (\\nabla f(x))^{T} h.$$\n我们展开 $f(x+\\alpha h)$:\n\\begin{align*}\nf(x+\\alpha h) = \\frac{1}{2} \\|A(x+\\alpha h) - y\\|_{2}^{2} \\\\\n= \\frac{1}{2} \\|(Ax - y) + \\alpha Ah\\|_{2}^{2} \\\\\n= \\frac{1}{2} \\langle (Ax - y) + \\alpha Ah, (Ax - y) + \\alpha Ah \\rangle \\\\\n= \\frac{1}{2} \\left( \\|Ax - y\\|_{2}^{2} + 2\\alpha \\langle Ax - y, Ah \\rangle + \\alpha^2 \\|Ah\\|_{2}^{2} \\right) \\\\\n= f(x) + \\alpha (Ax - y)^{T} (Ah) + \\frac{\\alpha^2}{2} \\|Ah\\|_{2}^{2}.\n\\end{align*}\n使用转置的性质，$(Ax - y)^{T} (Ah) = (A^{T}(Ax-y))^{T} h$。\n因此，方向导数为：\n$$D f(x)[h] = \\lim_{\\alpha \\to 0} \\frac{f(x+\\alpha h) - f(x)}{\\alpha} = \\lim_{\\alpha \\to 0} \\left( (A^{T}(Ax-y))^{T} h + \\frac{\\alpha}{2} \\|Ah\\|_{2}^{2} \\right) = (A^{T}(Ax-y))^{T} h.$$\n通过辨识，梯度为 $\\nabla f(x) = A^{T}(Ax-y)$。\n\n现在我们根据给定数据在 $x^{(0)}$ 处计算这个值：\n首先，计算残差 $r = Ax^{(0)} - y$：\n$$Ax^{(0)} = \\begin{pmatrix} 1  -1  2 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (-1)(-1) + (2)(1) \\\\ (0)(0) + (2)(-1) + (1)(1) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix}.$$\n$$r = Ax^{(0)} - y = \\begin{pmatrix} 3 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix}.$$\n接下来，计算梯度 $\\nabla f(x^{(0)}) = A^{T}r$：\n$$A^{T} = \\begin{pmatrix} 1  0 \\\\ -1  2 \\\\ 2  1 \\end{pmatrix}.$$\n$$\\nabla f(x^{(0)}) = \\begin{pmatrix} 1  0 \\\\ -1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(-2) \\\\ (-1)(0) + (2)(-2) \\\\ (2)(0) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -4 \\\\ -2 \\end{pmatrix}.$$\n\n**2. 形成前向（梯度）步骤 $v = x^{(0)} - t \\nabla f\\big(x^{(0)}\\big)$。**\n\n使用 $t = \\frac{1}{5}$ 和已计算的梯度，我们求得中间向量 $v$：\n$$v = x^{(0)} - t \\nabla f(x^{(0)}) = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 0 \\\\ -4 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -\\frac{4}{5} \\\\ -\\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} 0 - 0 \\\\ -1 - (-\\frac{4}{5}) \\\\ 1 - (-\\frac{2}{5}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{5} \\\\ \\frac{7}{5} \\end{pmatrix}.$$\n\n**3. 应用加权软阈值计算 $x^{(1)} = \\operatorname{prox}_{t g}(v)$。**\n\n近端算子定义为：\n$$x^{(1)} = \\operatorname{prox}_{t g}(v) = \\arg\\min_{z \\in \\mathbb{R}^n} \\left( t g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right).$$\n代入 $g(z) = \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}|$，目标函数变为：\n$$\\arg\\min_{z \\in \\mathbb{R}^n} \\left( t \\lambda \\sum_{i=1}^{n} w_{i} |z_{i}| + \\frac{1}{2}\\sum_{i=1}^{n} (z_i - v_i)^{2} \\right).$$\n这个优化问题是可分的，意味着我们可以独立求解每个分量 $z_i$：\n$$x^{(1)}_{i} = \\arg\\min_{z_i \\in \\mathbb{R}} \\left( (t \\lambda w_{i}) |z_{i}| + \\frac{1}{2}(z_{i} - v_{i})^{2} \\right).$$\n解由软阈值算子 $S_{\\tau}(u) = \\operatorname{sign}(u) \\max(|u| - \\tau, 0)$ 给出，其中逐分量的阈值为 $\\tau_i = t \\lambda w_{i}$。\n\n我们来计算阈值 $\\tau_i$：\n乘积 $t \\lambda = \\frac{1}{5} \\times \\frac{1}{2} = \\frac{1}{10}$。\n权重向量为 $w = (2, 1, 3)^T$。\n$$\\tau_1 = t \\lambda w_1 = \\frac{1}{10} \\times 2 = \\frac{2}{10} = \\frac{1}{5}.$$\n$$\\tau_2 = t \\lambda w_2 = \\frac{1}{10} \\times 1 = \\frac{1}{10}.$$\n$$\\tau_3 = t \\lambda w_3 = \\frac{1}{10} \\times 3 = \\frac{3}{10}.$$\n\n现在我们将软阈值算子应用于 $v = (0, -1/5, 7/5)^T$ 的每个分量：\n对于 $i=1$： $v_1 = 0$，$\\tau_1 = \\frac{1}{5}$。\n$x^{(1)}_{1} = S_{1/5}(0) = \\operatorname{sign}(0) \\max(|0| - \\frac{1}{5}, 0) = 0$。\n\n对于 $i=2$： $v_2 = -\\frac{1}{5}$，$\\tau_2 = \\frac{1}{10}$。\n$|v_2| = \\frac{1}{5} = \\frac{2}{10}$，大于 $\\tau_2 = \\frac{1}{10}$。\n$x^{(1)}_{2} = S_{1/10}(-\\frac{1}{5}) = \\operatorname{sign}(-\\frac{1}{5}) \\left( |-\\frac{1}{5}| - \\frac{1}{10} \\right) = (-1) \\left( \\frac{1}{5} - \\frac{1}{10} \\right) = (-1) \\left( \\frac{2}{10} - \\frac{1}{10} \\right) = -\\frac{1}{10}$。\n\n对于 $i=3$： $v_3 = \\frac{7}{5}$，$\\tau_3 = \\frac{3}{10}$。\n$|v_3| = \\frac{7}{5} = \\frac{14}{10}$，大于 $\\tau_3 = \\frac{3}{10}$。\n$x^{(1)}_{3} = S_{3/10}(\\frac{7}{5}) = \\operatorname{sign}(\\frac{7}{5}) \\left( |\\frac{7}{5}| - \\frac{3}{10} \\right) = (+1) \\left( \\frac{7}{5} - \\frac{3}{10} \\right) = \\frac{14}{10} - \\frac{3}{10} = \\frac{11}{10}$。\n\n组合各分量，下一个迭代结果 $x^{(1)}$ 是：\n$$x^{(1)} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{10} \\\\ \\frac{11}{10} \\end{pmatrix}.$$\n问题要求答案以单个行向量的形式给出。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0  -\\frac{1}{10}  \\frac{11}{10} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在实践中，标准的重加权方案可能会导致数值不稳定性，特别是当某个分量接近零时，其对应的权重会趋于无穷大。本练习  探讨了一种常见的解决方案——权重裁剪，并要求你从理论上分析这种修正带来的近似误差。通过这个练习，你将把算法实现的实际考量与其背后严谨的数学论证联系起来。",
            "id": "3454440",
            "problem": "考虑通过对一个可分离非凸稀疏性惩罚项进行上界最小化（majorization-minimization）构建的迭代重加权 $\\ell_1$ 最小化。设目标惩罚项为 $\\phi(t) = \\lambda t^{p}$（$t \\geq 0$），其中参数 $\\lambda  0$ 且 $p \\in (0,1)$，该惩罚项按分量应用于 $|x_{i}|$。在 $t_{0} \\geq 0$ 处通过切线线性上界进行的常规上界化，为下一个加权 $\\ell_1$ 子问题生成权重 $w(t_{0}) = \\phi'(t_{0})$。对于所选的 $\\phi$，有 $\\phi'(t) = \\lambda p t^{p-1}$，当 $t \\to 0^{+}$ 时该式发散，并可能导致停滞：接近零的分量会获得近乎无穷大的权重，从而无法重新变为非零值。\n\n一种解决方法是在一个有限水平 $c  0$ 处对权重进行截断（等效于对斜率进行Huber化处理），即将 $w(t)$ 替换为 $\\min\\{\\phi'(t), c\\}$。定义相应的截断斜率惩罚项 $\\psi_{c}$，通过从零开始对截断后的导数进行积分，并设 $\\psi_{c}(0)=0$：\n$$\n\\psi_{c}(t) \\triangleq \\int_{0}^{t} \\min\\{\\lambda p s^{p-1},\\, c\\} \\, ds, \\quad t \\geq 0.\n$$\n这种修改在近似目标 $\\phi$ 的同时，防止了在较小的 $t$ 值处出现无穷大或近乎无穷大的权重。\n\n从上述定义以及凹函数和积分的基本性质出发，推导 $\\phi$ 和 $\\psi_{c}$ 之间的最坏情况下的均匀近似误差，\n$$\nE(c) \\triangleq \\sup_{t \\geq 0} \\big(\\phi(t) - \\psi_{c}(t)\\big),\n$$\n得到一个用 $\\lambda$、$p$ 和 $c$ 表示的闭式解析表达式。最终答案必须是单个简化的表达式。无需进行数值计算。",
            "solution": "问题要求解目标惩罚项 $\\phi(t)$ 与截断斜率惩罚项 $\\psi_c(t)$ 之间的最坏情况下的均匀近似误差 $E(c)$。该误差定义为\n$$\nE(c) \\triangleq \\sup_{t \\geq 0} D(t)\n$$\n其中 $D(t) \\triangleq \\phi(t) - \\psi_c(t)$。给定 $\\phi(t) = \\lambda t^p$（$t \\geq 0$），参数 $\\lambda  0$ 且 $p \\in (0,1)$，以及截断惩罚项\n$$\n\\psi_{c}(t) \\triangleq \\int_{0}^{t} \\min\\{\\lambda p s^{p-1},\\, c\\} \\, ds, \\quad t \\geq 0,\n$$\n其中截断水平 $c  0$。\n\n我们的第一步是分析被积函数 $\\min\\{\\lambda p s^{p-1}, c\\}$。函数 $\\phi'(s) = \\lambda p s^{p-1}$ 对于 $s0$ 是 $s$ 的严格递减函数，因为指数 $p-1$ 是负数。$\\phi'(s)$ 的值从 $s \\to 0^+$ 时的 $+\\infty$ 递减到 $s \\to \\infty$ 时的 $0$。因此，存在一个唯一的交叉点，我们记为 $t_c$，在该点导数等于截断水平 $c$。\n$$\n\\lambda p t_c^{p-1} = c\n$$\n求解 $t_c$，我们得到：\n$$\nt_c^{p-1} = \\frac{c}{\\lambda p}\n$$\n由于 $p \\in (0,1)$，指数 $1-p$ 是正数。将两边取 $\\frac{1}{p-1} = -\\frac{1}{1-p}$ 次幂，得到：\n$$\nt_c = \\left(\\frac{c}{\\lambda p}\\right)^{\\frac{1}{p-1}} = \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{1}{1-p}}\n$$\n鉴于 $\\lambda, p, c$ 均为正数，$t_c$ 是一个良定义的正实数。\n\n现在可以明确被积函数的行为：\n$$\n\\min\\{\\lambda p s^{p-1}, c\\} = \\begin{cases} c  \\text{if } 0 \\le s \\leq t_c \\\\ \\lambda p s^{p-1}  \\text{if } s  t_c \\end{cases}\n$$\n这是因为当 $s \\leq t_c$ 时，有 $s^{p-1} \\geq t_c^{p-1}$，这意味着 $\\lambda p s^{p-1} \\geq \\lambda p t_c^{p-1} = c$。\n\n接下来，我们计算定义 $\\psi_c(t)$ 的积分。计算取决于 $t$ 是小于还是大于 $t_c$。\n\n情况1：$0 \\leq t \\leq t_c$。\n在此区间内，被积函数为常数 $c$。\n$$\n\\psi_c(t) = \\int_0^t c \\, ds = ct\n$$\n\n情况2：$t  t_c$。\n我们必须在交叉点 $t_c$ 处拆分积分。\n$$\n\\psi_c(t) = \\int_0^{t_c} c \\, ds + \\int_{t_c}^t \\lambda p s^{p-1} \\, ds\n$$\n第一个积分是 $ct_c$。第二个积分是：\n$$\n\\int_{t_c}^t \\lambda p s^{p-1} \\, ds = \\lambda p \\left[ \\frac{s^p}{p} \\right]_{s=t_c}^{s=t} = \\lambda (t^p - t_c^p) = \\phi(t) - \\phi(t_c)\n$$\n综合这些，对于 $t  t_c$：\n$$\n\\psi_c(t) = ct_c + \\phi(t) - \\phi(t_c)\n$$\n\n现在我们可以将误差函数 $D(t) = \\phi(t) - \\psi_c(t)$ 写成分段形式。\n\n对于 $0 \\leq t \\leq t_c$：\n$$\nD(t) = \\phi(t) - \\psi_c(t) = \\lambda t^p - ct\n$$\n\n对于 $t  t_c$：\n$$\nD(t) = \\phi(t) - \\psi_c(t) = \\phi(t) - \\big(ct_c + \\phi(t) - \\phi(t_c)\\big) = \\phi(t_c) - ct_c\n$$\n注意，当 $t  t_c$ 时，误差 $D(t)$ 是一个常数，等于其在 $t=t_c$ 处的值。函数 $D(t)$ 在 $t=t_c$ 处是连续的，因为 $\\lim_{t \\to t_c^-} (\\lambda t^p - ct) = \\lambda t_c^p - ct_c$。\n\n为了找到 $D(t)$ 在 $t \\geq 0$ 上的上确界，我们需要找到 $D(t)$ 在区间 $[0, t_c]$ 上的最大值，因为此后函数为常数。让我们通过考察其导数来分析 $D(t) = \\lambda t^p - ct$ 在 $[0, t_c]$ 上的行为：\n$$\nD'(t) = \\frac{d}{dt}(\\lambda t^p - ct) = \\lambda p t^{p-1} - c\n$$\n$D'(t)$ 的符号由项 $\\lambda p t^{p-1}$ 决定。如前所述，对于 $t \\in (0, t_c)$，我们有 $\\lambda p t^{p-1}  c$。因此，对于 $t \\in (0, t_c)$，有 $D'(t)  0$。这表明 $D(t)$ 在区间 $[0, t_c]$ 上是一个严格递增函数。\n\n递增函数在闭区间上的最大值出现在右边界。因此，$D(t)$ 在 $[0, t_c]$ 上的最大值在 $t = t_c$ 处取得。\n$$\n\\max_{t \\in [0, t_c]} D(t) = D(t_c) = \\lambda t_c^p - ct_c\n$$\n由于当 $t  t_c$ 时 $D(t)$ 是常数，因此这也是在所有 $t \\geq 0$ 上的上确界。\n$$\nE(c) = \\sup_{t \\geq 0} D(t) = \\lambda t_c^p - ct_c\n$$\n\n最后一步是通过代入 $t_c$ 的表达式，将此结果用给定的参数 $\\lambda$、$p$ 和 $c$ 表示出来：\n$$\nt_c = \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{1}{1-p}}\n$$\n我们可以将 $E(c)$ 写为：\n$$\nE(c) = \\lambda \\left[ \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{1}{1-p}} \\right]^p - c \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{1}{1-p}}\n$$\n$$\nE(c) = \\lambda \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}} - c \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{1}{1-p}}\n$$\n为了简化，我们提出公因式 $(\\frac{\\lambda p}{c})^{\\frac{p}{1-p}}$。我们可以通过注意到 $\\frac{1}{1-p} = 1 + \\frac{p}{1-p}$ 来重写第二项。\n$$\nc \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{1}{1-p}} = c \\left(\\frac{\\lambda p}{c}\\right)^{1} \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}} = (\\lambda p) \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}}\n$$\n将此代回 $E(c)$ 的表达式中：\n$$\nE(c) = \\lambda \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}} - \\lambda p \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}}\n$$\n提出公因式得到：\n$$\nE(c) = (\\lambda - \\lambda p) \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}}\n$$\n$$\nE(c) = \\lambda (1-p) \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}}\n$$\n这就是最坏情况下的均匀近似误差的闭式解析表达式。",
            "answer": "$$\\boxed{\\lambda (1-p) \\left(\\frac{\\lambda p}{c}\\right)^{\\frac{p}{1-p}}}$$"
        },
        {
            "introduction": "掌握了标量稀疏性的基本原理后，我们可以将迭代重加权的思想推广到更复杂的结构化稀疏模型中。本练习  将引导你探索块稀疏信号的迭代重加权组 $\\ell_{1,2}$ 最小化。你将从第一性原理出发，推导重加权方案和求解算子，并完成一次完整的迭代计算，从而巩固你对IRL1核心逻辑如何适应不同信号模型的理解。",
            "id": "3454450",
            "problem": "考虑一个块稀疏信号模型，其索引被固定地划分为不相交的组 $\\mathcal{G} = \\{g_{1}, g_{2}, \\dots, g_{M}\\}$，其中每个组 $g \\in \\mathcal{G}$ 是 $\\{1,2,\\dots,N\\}$ 的一个子集。对于向量 $\\mathbf{x} \\in \\mathbb{R}^{N}$ 和子集 $g$，用 $\\mathbf{x}_{g}$ 表示 $\\mathbf{x}$ 限制在索引 $g$ 内的子向量。$\\mathbf{x}$ 的组 $\\ell_{1,2}$ 范数定义为 $\\sum_{g \\in \\mathcal{G}} \\|\\mathbf{x}_{g}\\|_{2}$。为了促进块稀疏性，考虑将一个凹、非减函数 $\\psi:[0,\\infty)\\to\\mathbb{R}$（例如，对数代理函数）应用于组范数，从而得到目标函数 $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$。\n\n从凸分析和多数化-最小化 (MM) 算法的标准原理出发，完成以下任务：\n\n1. 通过在当前迭代点对 $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ 进行多数化，将一个迭代重加权组 $\\ell_{1,2}$ 方案构建为一系列凸子问题。您的构建必须明确说明代理函数如何成为一个加权组 $\\ell_{1,2}$ 惩罚项，并根据当前迭代点和 $\\psi$ 确定组权重。\n\n2. 对于一个形如\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{N}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2},\n$$\n的凸子问题，使用最优性条件从第一性原理出发，推导将 $\\mathbf{v}$ 映射到最小化点的组加权软阈值算子的显式闭式表达式。您的推导必须从欧几里得范数的次微分的定义和跨组的可分性开始，并逻辑地推导出最终的算子，而不能直接使用预先记忆的公式。\n\n3. 为一个六维信号和两个组 $g_{1}=\\{1,2,3\\}$ 和 $g_{2}=\\{4,5,6\\}$，提供一个完整的单次 MM 迭代的数值示例。使用数据保真模型\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{6}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{b}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}^{(k)}\\|\\mathbf{x}_{g}\\|_{2},\n$$\n其中第 $k$ 次迭代的权重 $w_{g}^{(k)}$ 是通过对 $\\psi(s)=\\ln(s+\\epsilon)$（其中 $\\epsilon0$ 为固定值）进行 MM 线性化从 $\\mathbf{x}^{(k)}$ 获得的。设初始迭代点为 $\\mathbf{x}^{(0)}=\\mathbf{b}$，其中 $\\mathbf{b}=(3,4,0,0,2,0)$，参数 $\\lambda=1$，稳定化常数 $\\epsilon=\\tfrac{1}{2}$。计算权重 $w_{g}^{(0)}$，然后通过将推导出的组加权软阈值算子应用于 $\\mathbf{b}$ 来计算下一个迭代点 $\\mathbf{x}^{(1)}$。请给出 $\\mathbf{x}^{(1)}$ 的精确最终答案，无需四舍五入。\n\n你的最终答案必须是单个行向量 $\\mathbf{x}^{(1)}$。",
            "solution": "该问题被验证为具有科学依据、适定、客观和完整。我们接下来给出详细解答。\n\n问题分为三个部分。我们将依次解决。\n\n**第 1 部分：迭代重加权方案的构建**\n\n目标是最小化形如 $\\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ 的目标函数，其中 $\\psi$ 是一个凹、非减函数。我们采用多数化-最小化（MM）算法。MM 算法的核心思想是用一系列更简单的问题来替代一个困难的优化问题。在每次迭代 $k$ 中，我们最小化一个代理函数 $Q(\\mathbf{x} | \\mathbf{x}^{(k)})$，该函数在当前迭代点 $\\mathbf{x}^{(k)}$ 处对原始目标函数 $\\Phi(\\mathbf{x}) = \\sum_{g \\in \\mathcal{G}} \\psi(\\|\\mathbf{x}_{g}\\|_{2})$ 进行多数化。该代理函数必须满足：\n1. 对所有 $\\mathbf{x}$，$Q(\\mathbf{x} | \\mathbf{x}^{(k)}) \\ge \\Phi(\\mathbf{x})$。\n2. $Q(\\mathbf{x}^{(k)} | \\mathbf{x}^{(k)}) = \\Phi(\\mathbf{x})$。\n下一个迭代点通过 $\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} Q(\\mathbf{x} | \\mathbf{x}^{(k)})$ 求得。\n\n由于 $\\psi: [0,\\infty)\\to\\mathbb{R}$ 是一个凹且可微的函数，我们可以使用其一阶泰勒近似来构造一个上界函数。对于任何凹函数 $f$，我们有性质 $f(z) \\le f(z_0) + f'(z_0)(z - z_0)$，对定义域中的所有 $z, z_0$ 均成立。这个不等式在 $z=z_0$ 处提供了一个紧上界。\n\n我们将此性质应用于和中的每一项 $\\psi(\\|\\mathbf{x}_{g}\\|_{2})$。令 $z_g = \\|\\mathbf{x}_g\\|_2$ 和 $z_g^{(k)} = \\|\\mathbf{x}_g^{(k)}\\|_2$。第 $g$ 项的上界为：\n$$\n\\psi(\\|\\mathbf{x}_{g}\\|_{2}) \\le \\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) + \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\cdot (\\|\\mathbf{x}_{g}\\|_{2} - \\|\\mathbf{x}_{g}^{(k)}\\|_{2})\n$$\n对所有组 $g \\in \\mathcal{G}$ 求和，我们得到整个目标函数的代理函数：\n$$\nQ(\\mathbf{x} | \\mathbf{x}^{(k)}) = \\sum_{g \\in \\mathcal{G}} \\left[ \\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) + \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\cdot (\\|\\mathbf{x}_{g}\\|_{2} - \\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\right]\n$$\n为了找到下一个迭代点 $\\mathbf{x}^{(k+1)}$，我们最小化这个代理函数。在对 $\\mathbf{x}$ 进行最小化时，我们可以舍弃相对于 $\\mathbf{x}$ 为常数的项，即 $\\psi(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})$ 和 $-\\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})\\|\\mathbf{x}_{g}^{(k)}\\|_{2}$。第 $(k+1)$ 次迭代的最小化问题变为：\n$$\n\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} \\sum_{g \\in \\mathcal{G}} \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2}) \\|\\mathbf{x}_{g}\\|_{2}\n$$\n这是一个加权组 $\\ell_{1,2}$ 最小化问题。在第 $k$ 次迭代中，每个组 $g$ 的权重由 $\\psi$ 在该组当前迭代点 $\\mathbf{x}_g^{(k)}$ 的范数处的导数给出：\n$$\nw_g^{(k)} = \\psi'(\\|\\mathbf{x}_{g}^{(k)}\\|_{2})\n$$\n当与像 $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2$ 这样的数据保真项结合时，完整的迭代重加权方案涉及求解一系列形如\n$$\n\\mathbf{x}^{(k+1)} = \\arg\\min_{\\mathbf{x}} \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\lambda \\sum_{g \\in \\mathcal{G}} w_g^{(k)} \\|\\mathbf{x}_g\\|_2\n$$\n的凸子问题。\n\n**第 2 部分：组加权软阈值算子的推导**\n\n我们需要求解以下凸子问题的闭式解：\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{N}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2}\n$$\n设目标函数为 $F(\\mathbf{x}) = f(\\mathbf{x}) + h(\\mathbf{x})$，其中 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{v}\\|_{2}^{2}$ 是一个光滑的强凸函数，而 $h(\\mathbf{x}) = \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}\\|_{2}$ 是一个凸的非光滑正则化项。一个向量 $\\mathbf{x}^{*}$ 是唯一最小化点的充要条件是它满足凸分析中的一阶最优性条件：\n$$\n\\mathbf{0} \\in \\partial F(\\mathbf{x}^{*}) = \\nabla f(\\mathbf{x}^{*}) + \\partial h(\\mathbf{x}^{*})\n$$\n$f(\\mathbf{x})$ 的梯度是 $\\nabla f(\\mathbf{x}) = \\mathbf{x} - \\mathbf{v}$。条件变为：\n$$\n\\mathbf{0} \\in (\\mathbf{x}^{*} - \\mathbf{v}) + \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}^{*}\\|_{2}\\right) \\iff \\mathbf{v} - \\mathbf{x}^{*} \\in \\partial \\left(\\lambda \\sum_{g \\in \\mathcal{G}} w_{g}\\|\\mathbf{x}_{g}^{*}\\|_{2}\\right)\n$$\n由于组 $\\{g\\}$ 构成了索引的一个划分，正则化项 $h(\\mathbf{x})$ 相对于子向量 $\\mathbf{x}_g$ 是可分的。也就是说，$h(\\mathbf{x}) = \\sum_{g \\in \\mathcal{G}} h_g(\\mathbf{x}_g)$，其中 $h_g(\\mathbf{x}_g) = \\lambda w_g \\|\\mathbf{x}_g\\|_2$。次微分 $\\partial h(\\mathbf{x})$ 是各次微分 $\\partial h_g(\\mathbf{x}_g)$ 的笛卡尔积。这种可分性意味着优化问题分解为 $M$ 个独立的子问题，每个组 $g \\in \\mathcal{G}$ 对应一个：\n$$\n\\min_{\\mathbf{x}_g} \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{v}_g\\|_2^2 + \\lambda w_g \\|\\mathbf{x}_g\\|_2\n$$\n让我们求解单个组 $g$。令 $\\mathbf{y} = \\mathbf{x}_g$，$\\mathbf{u} = \\mathbf{v}_g$，以及 $\\mu = \\lambda w_g \\ge 0$。问题是 $\\min_{\\mathbf{y}} \\frac{1}{2} \\|\\mathbf{y}-\\mathbf{u}\\|_2^2 + \\mu\\|\\mathbf{y}\\|_2$。最小化点 $\\mathbf{y}^{*}$ 的最优性条件是：\n$$\n\\mathbf{u} - \\mathbf{y}^{*} \\in \\partial(\\mu \\|\\mathbf{y}^{*}\\|_2) = \\mu \\partial\\|\\mathbf{y}^{*}\\|_2\n$$\n欧几里得范数的次微分是：\n$$\n\\partial\\|\\mathbf{y}\\|_2 = \n\\begin{cases} \n\\{\\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|_2}\\}  \\text{if } \\mathbf{y} \\neq \\mathbf{0} \\\\\n\\{\\mathbf{z} \\mid \\|\\mathbf{z}\\|_2 \\le 1\\}  \\text{if } \\mathbf{y} = \\mathbf{0} \n\\end{cases}\n$$\n我们分析解 $\\mathbf{y}^{*}$ 的两种情况：\n情况 1：$\\mathbf{y}^{*} \\neq \\mathbf{0}$。最优性条件变为 $\\mathbf{u} - \\mathbf{y}^{*} = \\mu \\frac{\\mathbf{y}^{*}}{\\|\\mathbf{y}^{*}\\|_2}$。整理关于 $\\mathbf{u}$ 的表达式，我们得到 $\\mathbf{u} = \\mathbf{y}^{*} + \\mu \\frac{\\mathbf{y}^{*}}{\\|\\mathbf{y}^{*}\\|_2} = \\mathbf{y}^{*} \\left(1 + \\frac{\\mu}{\\|\\mathbf{y}^{*}\\|_2}\\right)$。这意味着 $\\mathbf{u}$ 和 $\\mathbf{y}^{*}$ 共线，因此存在某个标量 $c  0$ 使得 $\\mathbf{y}^{*} = c \\mathbf{u}$。取范数，我们有 $\\|\\mathbf{y}^{*}\\|_2 = c\\|\\mathbf{u}\\|_2$。代回到 $\\mathbf{u}$ 的主表达式中：\n$\\|\\mathbf{u}\\|_2 = \\|\\mathbf{y}^{*}\\|_2 \\left(1 + \\frac{\\mu}{\\|\\mathbf{y}^{*}\\|_2}\\right) = \\|\\mathbf{y}^{*}\\|_2 + \\mu$。\n这得到 $\\|\\mathbf{y}^{*}\\|_2 = \\|\\mathbf{u}\\|_2 - \\mu$。因为在这种情况下 $\\|\\mathbf{y}^{*}\\|_2  0$，我们必须有 $\\|\\mathbf{u}\\|_2  \\mu$。那么解是：\n$$\n\\mathbf{y}^{*} = \\frac{\\|\\mathbf{y}^{*}\\|_2}{\\|\\mathbf{u}\\|_2}\\mathbf{u} = \\frac{\\|\\mathbf{u}\\|_2 - \\mu}{\\|\\mathbf{u}\\|_2}\\mathbf{u} = \\left(1 - \\frac{\\mu}{\\|\\mathbf{u}\\|_2}\\right)\\mathbf{u}\n$$\n情况 2：$\\mathbf{y}^{*} = \\mathbf{0}$。最优性条件变为 $\\mathbf{u} - \\mathbf{0} \\in \\mu \\partial\\|\\mathbf{0}\\|_2$，即 $\\mathbf{u} \\in \\{\\mathbf{z} \\mid \\|\\mathbf{z}\\|_2 \\le \\mu\\}$。这等价于条件 $\\|\\mathbf{u}\\|_2 \\le \\mu$。\n\n结合这两种情况，我们可以使用正部函数 $(a)_+ = \\max(a, 0)$ 紧凑地写出 $\\mathbf{y}^{*}$ 的解：\n$$\n\\mathbf{y}^{*} = \\left(1 - \\frac{\\mu}{\\|\\mathbf{u}\\|_2}\\right)_+ \\mathbf{u}\n$$\n这个表达式就是组软阈值算子。如果 $\\|\\mathbf{u}\\|_2=0$，分数未定义，但条件 $\\|\\mathbf{u}\\|_2 \\le \\mu$ 成立，所以 $\\mathbf{y}^*=\\mathbf{0}$，这是正确的极限情况。\n恢复到组 $g$ 的原始符号，最小化点 $\\mathbf{x}_g^*$ 是：\n$$\n\\mathbf{x}_g^{*} = \\left(1 - \\frac{\\lambda w_g}{\\|\\mathbf{v}_g\\|_2}\\right)_+ \\mathbf{v}_g\n$$\n最终解 $\\mathbf{x}^*$ 是通过拼接所有组的解 $\\mathbf{x}_g^*$ 得到的。\n\n**第 3 部分：数值示例**\n\n我们执行一次 MM 迭代。问题是通过求解\n$$\n\\min_{\\mathbf{x}\\in\\mathbb{R}^{6}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{b}\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g}^{(0)}\\|\\mathbf{x}_{g}\\|_{2}\n$$\n来找到 $\\mathbf{x}^{(1)}$。\n已知：\n- 信号维度 $N=6$。\n- 组：$g_1 = \\{1, 2, 3\\}$, $g_2 = \\{4, 5, 6\\}$。\n- 初始迭代点 $\\mathbf{x}^{(0)} = \\mathbf{b} = (3, 4, 0, 0, 2, 0)^T$。\n- 凹函数：$\\psi(s) = \\ln(s+\\epsilon)$，其中 $\\epsilon=\\frac{1}{2}$。\n- 参数：$\\lambda = 1$。\n\n首先，我们计算权重 $w_g^{(0)} = \\psi'(\\|\\mathbf{x}_g^{(0)}\\|_2)$。\n$\\psi(s)$ 的导数是 $\\psi'(s) = \\frac{1}{s+\\epsilon}$。\n初始迭代点的子向量是：\n$\\mathbf{x}_{g_1}^{(0)} = (3, 4, 0)^T$\n$\\mathbf{x}_{g_2}^{(0)} = (0, 2, 0)^T$\n接下来，我们计算它们的欧几里得范数：\n$\\|\\mathbf{x}_{g_1}^{(0)}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2} = \\sqrt{9+16} = \\sqrt{25} = 5$。\n$\\|\\mathbf{x}_{g_2}^{(0)}\\|_2 = \\sqrt{0^2 + 2^2 + 0^2} = \\sqrt{4} = 2$。\n现在我们可以计算第 $k=0$ 次迭代的权重：\n$w_{g_1}^{(0)} = \\psi'(\\|\\mathbf{x}_{g_1}^{(0)}\\|_2) = \\frac{1}{5 + \\epsilon} = \\frac{1}{5 + \\frac{1}{2}} = \\frac{1}{\\frac{11}{2}} = \\frac{2}{11}$。\n$w_{g_2}^{(0)} = \\psi'(\\|\\mathbf{x}_{g_2}^{(0)}\\|_2) = \\frac{1}{2 + \\epsilon} = \\frac{1}{2 + \\frac{1}{2}} = \\frac{1}{\\frac{5}{2}} = \\frac{2}{5}$。\n\n其次，我们通过应用第 2 部分中推导出的组加权软阈值算子来计算下一个迭代点 $\\mathbf{x}^{(1)}$，其中 $\\mathbf{v}=\\mathbf{b}$。每个组 $\\mathbf{x}_g^{(1)}$ 的解是：\n$$\n\\mathbf{x}_g^{(1)} = \\left(1 - \\frac{\\lambda w_g^{(0)}}{\\|\\mathbf{b}_g\\|_2}\\right)_+ \\mathbf{b}_g\n$$\n$\\mathbf{b}$ 的子向量是 $\\mathbf{b}_{g_1} = (3, 4, 0)^T$ 和 $\\mathbf{b}_{g_2} = (0, 2, 0)^T$。它们的范数是 $\\|\\mathbf{b}_{g_1}\\|_2 = 5$ 和 $\\|\\mathbf{b}_{g_2}\\|_2 = 2$。\n\n对于组 $g_1$：\n阈值是 $\\mu_1 = \\lambda w_{g_1}^{(0)} = 1 \\cdot \\frac{2}{11} = \\frac{2}{11}$。\n由于 $\\|\\mathbf{b}_{g_1}\\|_2 = 5  \\frac{2}{11}$，该组不被置为零。缩放因子是：\n$$\n\\left(1 - \\frac{\\mu_1}{\\|\\mathbf{b}_{g_1}\\|_2}\\right) = 1 - \\frac{\\frac{2}{11}}{5} = 1 - \\frac{2}{55} = \\frac{53}{55}\n$$\n所以，更新后的子向量是：\n$$\n\\mathbf{x}_{g_1}^{(1)} = \\frac{53}{55} \\mathbf{b}_{g_1} = \\frac{53}{55} \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{159}{55} \\\\ \\frac{212}{55} \\\\ 0 \\end{pmatrix}\n$$\n\n对于组 $g_2$：\n阈值是 $\\mu_2 = \\lambda w_{g_2}^{(0)} = 1 \\cdot \\frac{2}{5} = \\frac{2}{5}$。\n由于 $\\|\\mathbf{b}_{g_2}\\|_2 = 2  \\frac{2}{5}$，该组也不被置为零。缩放因子是：\n$$\n\\left(1 - \\frac{\\mu_2}{\\|\\mathbf{b}_{g_2}\\|_2}\\right) = 1 - \\frac{\\frac{2}{5}}{2} = 1 - \\frac{1}{5} = \\frac{4}{5}\n$$\n所以，更新后的子向量是：\n$$\n\\mathbf{x}_{g_2}^{(1)} = \\frac{4}{5} \\mathbf{b}_{g_2} = \\frac{4}{5} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{8}{5} \\\\ 0 \\end{pmatrix}\n$$\n\n最后，我们通过拼接 $\\mathbf{x}_{g_1}^{(1)}$ 和 $\\mathbf{x}_{g_2}^{(1)}$ 来组装完整的向量 $\\mathbf{x}^{(1)}$：\n$$\n\\mathbf{x}^{(1)} = \\left( \\frac{159}{55}, \\frac{212}{55}, 0, 0, \\frac{8}{5}, 0 \\right)^T\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{159}{55}  \\frac{212}{55}  0  0  \\frac{8}{5}  0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}