{
    "hands_on_practices": [
        {
            "introduction": "这项实践将引导你深入稀疏贝叶斯学习（SBL）的核心机制：自动相关性确定（ARD）原理。你将通过数学推导，得出增加一个基向量反而会降低模型证据（marginal likelihood）的精确条件，从而为“剪枝”提供依据。这项练习有助于你从根本上理解SBL是如何通过自动剪除不相关的模型分量来获得稀疏性的。",
            "id": "3433874",
            "problem": "考虑在稀疏贝叶斯学习 (Sparse Bayesian Learning, SBL) 和相关向量机 (Relevance Vector Machine, RVM) 中使用的贝叶斯线性回归模型。令 $y \\in \\mathbb{R}^{N}$ 由 $y = \\Phi w + \\varepsilon$ 生成，其中 $\\Phi \\in \\mathbb{R}^{N \\times M}$ 是一个固定的设计矩阵，$w \\in \\mathbb{R}^{M}$ 是一个权重向量，其具有自动相关性确定 (Automatic Relevance Determination, ARD) 高斯先验 $p(w \\mid \\alpha) = \\prod_{j=1}^{M} \\mathcal{N}(w_{j} \\mid 0, \\alpha_{j}^{-1})$，精度为 $\\alpha_{j} > 0$，并且 $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{N})$，噪声精度为 $\\beta > 0$。边缘似然（证据）具有高斯形式 $p(y \\mid \\alpha, \\beta) = \\mathcal{N}(y \\mid 0, C)$，其中 $C = \\beta^{-1} I_{N} + \\Phi A^{-1} \\Phi^{\\top}$ 且 $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{M})$。假设对于索引 $i$，第 $i$ 个基（列）当前被排除，即 $\\alpha_{i} = \\infty$，并定义 $C_{-i} = \\beta^{-1} I_{N} + \\sum_{j \\neq i} \\alpha_{j}^{-1} \\phi_{j} \\phi_{j}^{\\top}$，其中 $\\phi_{j}$ 是 $\\Phi$ 的第 $j$ 列。\n\n定义稀疏度因子 $s_{i}$ 和质量因子 $q_{i}$ 为\n$$\ns_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i}, \n\\qquad\nq_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} y.\n$$\n您将分析当先前被排除的第 $i$ 个基被（假设性地）赋予一个有限的精度 $\\alpha_{i} \\in (0, \\infty)$，同时保持所有其他超参数固定时，对数边缘似然（对数证据）的变化。\n\n任务：\n1. 从高斯证据 $p(y \\mid \\alpha, \\beta) = \\mathcal{N}(y \\mid 0, C)$ 和秩一更新的基本线性代数恒等式出发，推导对数证据变化的精确表达式\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) \\triangleq \\ln p(y \\mid \\alpha_{-i}, \\alpha_{i}, \\beta) - \\ln p(y \\mid \\alpha_{-i}, \\alpha_{i} = \\infty, \\beta),\n$$\n该表达式仅用 $s_{i}$、$q_{i}$ 和 $\\alpha_{i}$ 表示。然后，通过分析 $\\Delta \\mathcal{L}(\\alpha_{i})$ 在 $\\alpha_{i} \\in (0, \\infty)$ 上的驻点，推导出一个关于 $s_{i}$ 和 $q_{i}$ 的充分必要条件，在该条件下，相对于保持排除状态，以任何有限的 $\\alpha_{i}$ 添加第 $i$ 个基都会严格减小对数证据。\n\n2. 通过一个合成测试进行验证：设 $N = 3$，当前活动集为空，因此 $C_{-i} = \\beta^{-1} I_{N}$，并取 $\\beta = 1$。考虑一个候选基向量 $\\phi_{i} = [1, 0, 0]^{\\top}$ 和一个观测数据向量 $y = [0.5, 0.2, -0.1]^{\\top}$。使用您的公式并将试验精度设置为自稀疏度值 $\\alpha_{i} = s_{i}$，计算 $\\Delta \\mathcal{L}(\\alpha_{i})$ 的数值。将您的答案四舍五入到四位有效数字。\n\n你的最终答案应该仅为任务2中合成测试的 $\\Delta \\mathcal{L}(\\alpha_{i})$ 的数值（无单位，以自然对数为单位），并按要求四舍五入。最终答案中不要包含任何单位或附加文本。",
            "solution": "我们从带有自动相关性确定 (ARD) 先验的贝叶斯线性回归证据开始。边缘似然是高斯分布，其协方差为\n$$\nC = \\beta^{-1} I_{N} + \\Phi A^{-1} \\Phi^{\\top},\n$$\n其中 $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{M})$。对数边缘似然（对数证据）为\n$$\n\\mathcal{L}(\\alpha, \\beta) = \\ln p(y \\mid \\alpha, \\beta) = -\\frac{1}{2}\\left( N \\ln (2\\pi) + \\ln |C| + y^{\\top} C^{-1} y \\right).\n$$\n我们考虑包含或不包含第 $i$ 个基函数的影响。令 $\\alpha_{i} = \\infty$ 表示排除，并考虑一个有限的 $\\alpha_{i} \\in (0, \\infty)$ 用于假设性地包含，同时保持所有其他超参数固定。定义\n$$\nC_{-i} \\triangleq \\beta^{-1} I_{N} + \\sum_{j \\neq i} \\alpha_{j}^{-1} \\phi_{j} \\phi_{j}^{\\top}.\n$$\n然后，当以有限的 $\\alpha_{i}$ 包含第 $i$ 个基时，我们得到一个秩一更新\n$$\nC = C_{-i} + \\alpha_{i}^{-1} \\phi_{i} \\phi_{i}^{\\top}.\n$$\n定义稀疏度因子和质量因子\n$$\ns_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i}, \n\\qquad\nq_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} y.\n$$\n\n为获得对数证据的变化，我们使用两个经过充分检验的关于秩一更新的线性代数恒等式：矩阵行列式引理和 Sherman–Morrison–Woodbury (SMW) 恒等式。矩阵行列式引理给出\n$$\n|C| = |C_{-i} + \\alpha_{i}^{-1} \\phi_{i} \\phi_{i}^{\\top}| \n= |C_{-i}| \\left( 1 + \\alpha_{i}^{-1} \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i} \\right) \n= |C_{-i}| \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right),\n$$\n所以\n$$\n\\ln |C| - \\ln |C_{-i}| = \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right).\n$$\n对于逆矩阵，Sherman–Morrison–Woodbury 恒等式得出\n$$\nC^{-1} = C_{-i}^{-1} - \\frac{C_{-i}^{-1} \\phi_{i} \\phi_{i}^{\\top} C_{-i}^{-1}}{\\alpha_{i} + s_{i}}.\n$$\n因此，二次型满足\n$$\ny^{\\top} C^{-1} y = y^{\\top} C_{-i}^{-1} y - \\frac{(y^{\\top} C_{-i}^{-1} \\phi_{i})^{2}}{\\alpha_{i} + s_{i}}\n= y^{\\top} C_{-i}^{-1} y - \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}}.\n$$\n因此，在保持其他所有参数不变的情况下，将 $\\alpha_{i}$ 从 $\\infty$（排除）变为有限值（包含）时，对数证据的变化为\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) \\triangleq \\mathcal{L}(\\alpha_{-i}, \\alpha_{i}, \\beta) - \\mathcal{L}(\\alpha_{-i}, \\alpha_{i} = \\infty, \\beta) \n= -\\frac{1}{2} \\left[ \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) - \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} \\right].\n$$\n等价地，\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) = \\frac{1}{2} \\left( \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} - \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) \\right).\n$$\n\n我们现在分析在何种情况下，对于所有有限的 $\\alpha_{i}$，添加该基会减小对数证据。通过对 $\\alpha_{i}$ 求导来考虑驻点：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha_{i}} \\Delta \\mathcal{L}(\\alpha_{i}) \n= \\frac{1}{2} \\left( - \\frac{q_{i}^{2}}{(\\alpha_{i} + s_{i})^{2}} + \\frac{s_{i}}{\\alpha_{i}(\\alpha_{i} + s_{i})} \\right).\n$$\n将此导数设为零并求解 $\\alpha_{i}$，得到\n$$\n- \\frac{q_{i}^{2}}{(\\alpha_{i} + s_{i})^{2}} + \\frac{s_{i}}{\\alpha_{i}(\\alpha_{i} + s_{i})} = 0\n\\quad \\Longleftrightarrow \\quad\n\\frac{s_{i}}{\\alpha_{i}} = \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}}\n\\quad \\Longleftrightarrow \\quad\n\\alpha_{i}^{\\star} = \\frac{s_{i}^{2}}{q_{i}^{2} - s_{i}}.\n$$\n当且仅当 $q_{i}^{2} > s_{i}$ 时，存在一个有限的正驻点。在这种情况下，$\\alpha_{i}^{\\star} > 0$ 会得到 $\\Delta \\mathcal{L}(\\alpha_{i})$ 的一个局部最大值。如果 $q_{i}^{2} \\leq s_{i}$，则不存在正驻点，并且对于 $\\alpha_{i} \\in (0, \\infty)$，$\\Delta \\mathcal{L}(\\alpha_{i})$ 在 $\\alpha_{i}$ 上是严格递增的，直至极限值\n$$\n\\lim_{\\alpha_{i} \\to \\infty} \\Delta \\mathcal{L}(\\alpha_{i}) \n= \\frac{1}{2} \\left( 0 - \\ln(1 + 0) \\right) = 0,\n$$\n这意味着对于任何有限的 $\\alpha_{i}$，我们都有 $\\Delta \\mathcal{L}(\\alpha_{i})  0$。因此，对于所有有限的 $\\alpha_{i}$，添加第 $i$ 个基会严格减小对数证据的充分必要条件是\n$$\nq_{i}^{2} \\leq s_{i}.\n$$\n\n我们现在在这个合成测试上验证这个条件。取 $N = 3$，$\\beta = 1$，$C_{-i} = \\beta^{-1} I_{N} = I_{N}$，$\\phi_{i} = [1, 0, 0]^{\\top}$，以及 $y = [0.5, 0.2, -0.1]^{\\top}$。则\n$$\nC_{-i}^{-1} = I_{N}, \\quad s_{i} = \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i} = \\phi_{i}^{\\top} \\phi_{i} = 1,\n$$\n并且\n$$\nq_{i} = \\phi_{i}^{\\top} C_{-i}^{-1} y = \\phi_{i}^{\\top} y = 0.5.\n$$\n因此 $q_{i}^{2} = 0.25 \\leq s_{i} = 1$，这预测了任何有限的包含都会减小对数证据。按照指令，取试验精度 $\\alpha_{i} = s_{i}$，我们计算\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) \n= \\frac{1}{2} \\left( \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} - \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) \\right)\n= \\frac{1}{2} \\left( \\frac{0.25}{1 + 1} - \\ln(1 + 1) \\right)\n= \\frac{1}{2} \\left( \\frac{0.25}{2} - \\ln 2 \\right).\n$$\n这可以简化为\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) = 0.0625 - \\frac{1}{2} \\ln 2.\n$$\n数值上，使用 $\\ln 2 \\approx 0.6931471805599453$，我们得到\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) \\approx 0.0625 - 0.34657359027997264 \\approx -0.28407359027997264.\n$$\n四舍五入到四位有效数字，该值为 $-0.2841$。",
            "answer": "$$\\boxed{-0.2841}$$"
        },
        {
            "introduction": "一个完整的SBL算法需要通过迭代更新所有超参数来最大化模型证据。本练习聚焦于该过程中的一个关键环节：噪声方差 $\\sigma^2$ 的更新法则。通过推导该更新公式，你不仅能掌握算法的重要组成部分，还能深入理解“有效自由度”这一概念，它在贝叶斯框架下巧妙地量化了模型的复杂度。",
            "id": "3433924",
            "problem": "考虑稀疏贝叶斯学习（SBL）和相关向量机（RVM）中使用的线性高斯模型。设 $y \\in \\mathbb{R}^{n}$ 为测量向量，$A \\in \\mathbb{R}^{n \\times m}$ 为设计矩阵，其列为 $\\{a_{i}\\}_{i=1}^{m}$。未知系数向量 $x \\in \\mathbb{R}^{m}$ 服从零均值高斯先验 $x \\sim \\mathcal{N}(0,\\Gamma)$，其协方差矩阵为对角阵 $\\Gamma=\\mathrm{diag}(\\gamma_{1},\\ldots,\\gamma_{m})$。似然函数为 $y \\mid x,\\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{n})$，其中 $\\sigma^{2}0$ 是噪声方差。用 $\\mu \\in \\mathbb{R}^{m}$ 和 $\\Sigma \\in \\mathbb{R}^{m \\times m}$ 分别表示在 $\\Gamma$ 和 $\\sigma^{2}$ 固定的条件下，给定 $y$ 时 $x$ 的后验均值和协方差。设边际协方差为 $C=\\sigma^{2} I_{n}+A \\Gamma A^{\\top}$，并定义影响项 $s_{i}=a_{i}^{\\top} C^{-1} a_{i}$。\n\n从高斯似然、高斯先验、后验条件恒等式以及高斯积分下的边际似然的基本定义出发，推导在 $\\Gamma$ 保持固定的情况下，通过证据最大化（II 型最大似然）得到的噪声方差 $\\sigma^{2}$ 的不动点方程。将此更新完全用残差范数 $\\|y-A \\mu\\|_{2}^{2}$、样本大小 $n$ 以及由 $\\{\\gamma_{i}\\}$ 加权的基特定影响项 $\\{s_{i}\\}$ 的和来表示。通过构建一个由 $(A,\\Sigma,\\sigma^{2})$ 构成的合适的“帽子”矩阵，并将其迹与 $\\sum_{i} \\gamma_{i} s_{i}$ 关联起来，建立自由度解释。\n\n你的最终答案必须是关于 $y$、$A$、$\\mu$、$\\{\\gamma_{i}\\}$ 和 $\\{s_{i}\\}$ 的 $\\sigma^{2}$ 的单个闭式解析表达式。无需数值四舍五入，也无需单位。",
            "solution": "本题要求在稀疏贝叶斯学习 (SBL) 框架中，通过最大化边际似然（证据）来推导噪声方差 $\\sigma^2$ 的不动点更新方程。\n\n首先，我们建立问题陈述中定义的概率模型。\n给定系数 $x \\in \\mathbb{R}^m$ 和噪声方差 $\\sigma^2$，测量值 $y \\in \\mathbb{R}^n$ 的似然是高斯分布：\n$$p(y|x, \\sigma^2) = \\mathcal{N}(y | Ax, \\sigma^2 I_n) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y-Ax\\|_2^2\\right)$$\n系数 $x$ 的先验是一个零均值高斯分布，其协方差矩阵为对角阵 $\\Gamma = \\mathrm{diag}(\\gamma_1, \\ldots, \\gamma_m)$：\n$$p(x|\\Gamma) = \\mathcal{N}(x | 0, \\Gamma) = (2\\pi)^{-m/2} |\\Gamma|^{-1/2} \\exp\\left(-\\frac{1}{2}x^\\top \\Gamma^{-1} x\\right)$$\n目标是找到使边际似然（即证据）$p(y|\\Gamma, \\sigma^2)$ 最大化的 $\\sigma^2$ 值，同时保持 $\\Gamma$ 固定。这被称为 II 型最大似然或证据最大化。\n\n边际似然通过对系数 $x$ 积分得到：\n$$p(y|\\Gamma, \\sigma^2) = \\int p(y|x, \\sigma^2) p(x|\\Gamma) dx$$\n由于似然和先验都是高斯分布，边际分布也是高斯分布。其均值为 $E[y] = E[Ax] = A E[x] = 0$。协方差为 $\\mathrm{Cov}(y) = \\mathrm{Cov}(Ax+\\epsilon) = A \\mathrm{Cov}(x) A^\\top + \\mathrm{Cov}(\\epsilon) = A\\Gamma A^\\top + \\sigma^2 I_n$。这就是问题中定义的矩阵 $C$。\n因此，$y \\sim \\mathcal{N}(0, C)$，其中 $C = \\sigma^2 I_n + A\\Gamma A^\\top$。\n\n对数边际似然为：\n$$\\mathcal{L}(\\sigma^2, \\Gamma) = \\ln p(y|\\Gamma, \\sigma^2) = -\\frac{1}{2} y^\\top C^{-1} y - \\frac{1}{2} \\ln|C| - \\frac{n}{2}\\ln(2\\pi)$$\n可以通过将其关于 $\\sigma^2$ 的导数设为零来最大化该式。然而，一种更结构化的方法是期望最大化 (EM) 算法，它能得到相同的结果。在 SBL 的背景下，我们将系数 $x$ 视为隐变量。更新 $\\sigma^2$ 的 M 步涉及最大化完全数据对数似然在当前参数估计下计算的 $x$ 的后验分布下的期望。\n\n设噪声方差的当前估计为 $\\sigma^2_{old}$。$x$ 的后验分布为 $p(x|y, \\Gamma, \\sigma^2_{old}) = \\mathcal{N}(x|\\mu, \\Sigma)$，其中 $\\mu$ 和 $\\Sigma$ 分别是后验均值和协方差。M 步需要最大化 Q 函数：\n$$Q(\\sigma^2) = E_{p(x|y, \\Gamma, \\sigma^2_{old})} [\\ln p(y,x | \\sigma^2, \\Gamma)]$$\n$$p(y,x | \\sigma^2, \\Gamma) = p(y|x, \\sigma^2) p(x|\\Gamma)$$\n由于 $p(x|\\Gamma)$ 不依赖于 $\\sigma^2$，我们只需考虑似然项：\n$$Q(\\sigma^2) = E[\\ln p(y|x, \\sigma^2)] + \\text{const} = E\\left[-\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\|y-Ax\\|_2^2\\right] + \\text{const}$$\n$$Q(\\sigma^2) = -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} E\\left[\\|y-Ax\\|_2^2\\right] + \\text{const}$$\n期望是关于 $p(x|y, \\Gamma, \\sigma^2_{old})$ 计算的。我们有 $E[x]=\\mu$ 和 $E[(x-\\mu)(x-\\mu)^\\top] = \\Sigma$。\n$$E\\left[\\|y-Ax\\|_2^2\\right] = E\\left[(y-A\\mu-A(x-\\mu))^\\top(y-A\\mu-A(x-\\mu))\\right]$$\n$$= \\|y-A\\mu\\|_2^2 - 2(y-A\\mu)^\\top A E[x-\\mu] + E\\left[(x-\\mu)^\\top A^\\top A (x-\\mu)\\right]$$\n由于 $E[x-\\mu]=0$，交叉项消失。对二次型使用迹恒等式：\n$$E\\left[\\mathrm{Tr}((x-\\mu)^\\top A^\\top A (x-\\mu))\\right] = \\mathrm{Tr}\\left(A^\\top A E[(x-\\mu)(x-\\mu)^\\top]\\right) = \\mathrm{Tr}(A^\\top A \\Sigma)$$\n所以，$E\\left[\\|y-Ax\\|_2^2\\right] = \\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)$。\n\n将此代入 Q 函数：\n$$Q(\\sigma^2) = -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2}\\left(\\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)\\right)$$\n为了最大化 $Q(\\sigma^2)$，我们对 $\\sigma^2$ 求导并令结果为零：\n$$\\frac{\\partial Q}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2}\\left(\\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)\\right) = 0$$\n乘以 $2(\\sigma^2)^2$ 得：\n$$-n\\sigma^2 + \\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma) = 0$$\n解出 $\\sigma^2$ 得到更新规则：\n$$\\sigma^2 = \\frac{1}{n}\\left(\\|y-A\\mu\\|_2^2 + \\mathrm{Tr}(A^\\top A \\Sigma)\\right)$$\n此表达式必须与影响项 $s_i = a_i^\\top C^{-1} a_i$ 相关联。为此，我们建立自由度解释。预测输出为 $\\hat{y} = A\\mu$。后验均值由 $\\mu = \\sigma^{-2}\\Sigma A^\\top y$ 给出。因此，我们可以写出 $\\hat{y} = A(\\sigma^{-2}\\Sigma A^\\top)y$。我们可以定义一个“帽子”矩阵 $H = \\sigma^{-2}A\\Sigma A^\\top$。模型消耗的有效参数数量或自由度为 $\\delta = \\mathrm{Tr}(H)$。\n$$\\delta = \\mathrm{Tr}(\\sigma^{-2}A\\Sigma A^\\top) = \\sigma^{-2}\\mathrm{Tr}(A\\Sigma A^\\top) = \\sigma^{-2}\\mathrm{Tr}(\\Sigma A^\\top A)$$\n这意味着 $\\mathrm{Tr}(A^\\top A \\Sigma) = \\sigma^2 \\delta$。\n将此代入我们的 $\\sigma^2$ 方程中：\n$$n\\sigma^2 = \\|y-A\\mu\\|_2^2 + \\sigma^2 \\delta \\quad\\implies\\quad \\sigma^2 = \\frac{\\|y-A\\mu\\|_2^2}{n-\\delta}$$\n现在我们必须将 $\\delta$ 与和式 $\\sum_i \\gamma_i s_i$ 联系起来。\n$$\\delta = \\mathrm{Tr}(H) = \\mathrm{Tr}(\\sigma^{-2}A\\Sigma A^\\top)$$\n我们使用 Woodbury 矩阵恒等式，它将边际协方差的逆 $C^{-1}$ 与后验协方差 $\\Sigma$ 联系起来：\n$$C^{-1} = (\\sigma^2 I_n + A\\Gamma A^\\top)^{-1} = \\frac{1}{\\sigma^2}I_n - \\frac{1}{\\sigma^4}A(\\Gamma^{-1}+\\frac{1}{\\sigma^2}A^\\top A)^{-1}A^\\top = \\frac{1}{\\sigma^2}I_n - \\frac{1}{\\sigma^4}A\\Sigma A^\\top$$\n整理这个恒等式，我们得到 $A\\Sigma A^\\top = \\sigma^2 I_n - \\sigma^4 C^{-1}$。\n现在我们可以计算帽子矩阵的迹：\n$$\\delta = \\mathrm{Tr}(H) = \\mathrm{Tr}(\\sigma^{-2}A\\Sigma A^\\top) = \\mathrm{Tr}(\\sigma^{-2}(\\sigma^2 I_n - \\sigma^4 C^{-1}))$$\n$$\\delta = \\mathrm{Tr}(I_n - \\sigma^2 C^{-1}) = \\mathrm{Tr}(I_n) - \\sigma^2 \\mathrm{Tr}(C^{-1}) = n - \\sigma^2 \\mathrm{Tr}(C^{-1})$$\n接下来，我们计算项 $\\sum_i \\gamma_i s_i$：\n$$\\sum_{i=1}^m \\gamma_i s_i = \\sum_{i=1}^m \\gamma_i (a_i^\\top C^{-1} a_i) = \\sum_{i=1}^m \\mathrm{Tr}(\\gamma_i a_i^\\top C^{-1} a_i) = \\sum_{i=1}^m \\mathrm{Tr}(C^{-1} a_i \\gamma_i a_i^\\top)$$\n利用迹算子的线性性质：\n$$\\sum_{i=1}^m \\gamma_i s_i = \\mathrm{Tr}\\left(C^{-1} \\sum_{i=1}^m \\gamma_i a_i a_i^\\top\\right)$$\n括号中的和式恰好是 $A\\Gamma A^\\top$。\n$$\\sum_{i=1}^m \\gamma_i s_i = \\mathrm{Tr}(C^{-1} (A\\Gamma A^\\top))$$\n使用定义 $C = \\sigma^2 I_n + A\\Gamma A^\\top$，我们有 $A\\Gamma A^\\top = C - \\sigma^2 I_n$。\n$$\\sum_{i=1}^m \\gamma_i s_i = \\mathrm{Tr}(C^{-1} (C - \\sigma^2 I_n)) = \\mathrm{Tr}(I_n - \\sigma^2 C^{-1}) = n - \\sigma^2 \\mathrm{Tr}(C^{-1})$$\n比较这两个结果，我们发现恒等式：\n$$\\delta = n - \\sigma^2 \\mathrm{Tr}(C^{-1}) = \\sum_{i=1}^m \\gamma_i s_i$$\n所以，有效参数数量为 $\\delta = \\sum_{i=1}^m \\gamma_i s_i$。将此代入我们的表达式 $\\sigma^2 = \\frac{\\|y-A\\mu\\|_2^2}{n-\\delta}$：\n$$\\sigma^2 = \\frac{\\|y-A\\mu\\|_2^2}{n - \\sum_{i=1}^{m} \\gamma_i s_i}$$\n这就是所求的 $\\sigma^2$ 的不动点方程。它是一个不动点方程，因为 $\\sigma^2$ 隐式地出现在右侧，因为影响项 $s_i = a_i^\\top C^{-1} a_i$ 依赖于 $C = \\sigma^2 I_n + A\\Gamma A^\\top$。该表达式完全由所要求的量表示。",
            "answer": "$$\\boxed{\\sigma^2 = \\frac{\\|y - A\\mu\\|_2^2}{n - \\sum_{i=1}^{m} \\gamma_i s_i}}$$"
        },
        {
            "introduction": "理论的优美必须与实践的可行性相匹配。最后的这项实践旨在解决计算效率和数值稳定性这两个关键挑战，尤其是在特征数量远超样本数量（$p \\gg n$）的常见高维场景中。你将运用Woodbury矩阵恒等式推导出一个高效的算法，该算法巧妙地避免了大规模的矩阵求逆运算，并利用Cholesky分解来保证计算过程的数值鲁棒性。",
            "id": "3433942",
            "problem": "考虑一个用于稀疏恢复的标准线性高斯模型，该模型被用于稀疏贝叶斯学习 (SBL) 和相关向量机 (RVM)。设 $A \\in \\mathbb{R}^{n \\times p}$ 满足 $n \\ll p$，未知系数向量为 $x \\in \\mathbb{R}^{p}$，观测值 $y \\in \\mathbb{R}^{n}$ 由 $y = A x + \\varepsilon$ 生成，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{n})$，噪声精度 $\\beta  0$。假设先验 $x \\sim \\mathcal{N}(0, \\Gamma)$，其中 $\\Gamma = \\operatorname{diag}(\\tau_{1},\\dots,\\tau_{p}) \\succ 0$。后验分布是高斯分布，$x \\mid y \\sim \\mathcal{N}(\\mu, \\Sigma)$，边际似然（证据）的协方差为 $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$。\n\n任务：\n1) 仅从多元高斯条件化的基本定义和矩阵求逆引理（Woodbury 恒等式）出发，推导适用于 $n \\ll p$ 情况的低秩表达式：\n- 推导用 $\\Gamma$、$A$ 和 $C^{-1}$ 表示的 $\\Sigma$，且不直接对任何 $p \\times p$ 矩阵求逆。\n- 推导用 $\\beta$、$A$ 和 $\\Sigma$ 或 $\\Gamma$ 表示的 $C^{-1}$，并在可能的情况下避免 $p \\times p$ 矩阵的求逆运算。\n提供明确使用 Woodbury 恒等式的表达式，并指出哪个矩阵的 Cholesky 分解可用于避免显式求逆。\n\n2) 仅使用对称正定矩阵的 Cholesky 分解的基本性质，推导用于计算证据对数密度\n$$\n\\log p(y \\mid \\beta, \\Gamma) = -\\tfrac{1}{2}\\left( n \\log(2\\pi) + \\log \\lvert C \\rvert + y^{\\top} C^{-1} y \\right),\n$$\n的数值稳定公式，该公式不显式构造 $C^{-1}$。你的推导应使用三角求解来表示 $\\log \\lvert C \\rvert$ 和 $y^{\\top} C^{-1} y$。\n\n3) 设计一个算法，在给定 $(A, y, \\tau, \\beta)$ 的情况下，计算：\n- 使用一个仅需要对 $C$ 进行求解的公式计算 $\\mu$，\n- 使用一个基于对 $C$ 求解的低秩表达式计算 $\\Sigma$，\n- 使用 $C$ 的 Cholesky 分解计算 $\\log p(y \\mid \\beta, \\Gamma)$，\n并通过计算以下误差度量，在小规模问题上与直接的稠密计算进行一致性验证：\n- $e_{\\Sigma} = \\max\\limits_{i,j} \\lvert \\Sigma_{\\text{low-rank}}(i,j) - \\Sigma_{\\text{direct}}(i,j) \\rvert$,\n- $e_{C^{-1}} = \\max\\limits_{i,j} \\lvert C_{\\text{low-rank}}^{-1}(i,j) - C_{\\text{direct}}^{-1}(i,j) \\rvert$,\n- $e_{\\log \\text{ev}} = \\big\\lvert \\log p_{\\text{Cholesky}}(y \\mid \\beta, \\Gamma) - \\log p_{\\text{direct}}(y \\mid \\beta, \\Gamma) \\big\\rvert$,\n- $e_{\\mu} = \\lVert \\mu_{\\Gamma A^{\\top} C^{-1} y} - \\mu_{\\beta \\Sigma A^{\\top} y} \\rVert_{2}$。\n此处，$\\Sigma_{\\text{direct}}$ 表示在小的 $p$ 值下，对 $p \\times p$ 系统直接计算 $\\Gamma^{-1} + \\beta A^{\\top} A$ 的逆；$C_{\\text{direct}}^{-1}$ 表示对 $n \\times n$ 系统直接计算 $C$ 的逆；证据的比较则将基于 Cholesky 的计算与直接稠密方法进行对比。\n\n你的程序需要实现的测试套件：\n使用确定性的、由公式定义的数据构建四个 $n \\ll p$ 的测试用例。对于每个用例，$A$、$y$、$\\tau$ 和 $\\beta$ 必须严格按照如下方式定义。\n\n- 用例 1：$n = 3$, $p = 8$。\n  - 对于 $i \\in \\{0,1,2\\}$ 和 $j \\in \\{0,\\dots,7\\}$，\n    $$\n    A_{ij} = \\sin\\big((i+1)(j+1)\\big) + 0.1 \\cos\\big((i+1) + 2(j+1)\\big).\n    $$\n  - 对于 $i \\in \\{0,1,2\\}$，\n    $$\n    y_{i} = \\cos(i+1) + 0.5 \\sin\\big(2(i+1)\\big).\n    $$\n  - 对于 $j \\in \\{0,\\dots,7\\}$，\n    $$\n    \\tau_{j} = 0.5 + 0.1(j+1).\n    $$\n  - $\\beta = 25.0$。\n\n- 用例 2：$n = 3$, $p = 8$。\n  - 设 $v = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 3.0 \\end{bmatrix}$，$\\epsilon = 10^{-3}$，且对于每个 $j \\in \\{0,\\dots,7\\}$ 定义\n    $$\n    w_{j} = \\begin{bmatrix} \\sin(j+1) \\\\ \\cos(j+1) \\\\ \\sin\\big(2(j+1)\\big) \\end{bmatrix}, \\quad A_{:,j} = v + \\epsilon w_{j}.\n    $$\n  - $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n  - 对于 $j \\in \\{0,\\dots,7\\}$，\n    $$\n    \\tau_{j} = 10^{-4 + 6 \\cdot \\frac{j}{7}}.\n    $$\n  - $\\beta = 1.0$。\n\n- 用例 3：$n = 4$, $p = 10$。\n  - 对于 $i \\in \\{0,1,2,3\\}$ 和 $j \\in \\{0,\\dots,9\\}$，\n    $$\n    A_{ij} = \\sin\\big(0.3(i+1) + 0.7(j+1)\\big) + 0.05\\big((i+1) - (j+1)\\big).\n    $$\n  - 对于 $i \\in \\{0,1,2,3\\}$，\n    $$\n    y_{i} = \\sin\\big(1.5(i+1)\\big).\n    $$\n  - 对于 $j \\in \\{0,\\dots,9\\}$，\n    $$\n    \\tau_{j} = 0.2 + 0.05 j.\n    $$\n  - $\\beta = 1000.0$。\n\n- 用例 4：$n = 2$, $p = 6$。\n  - 对于 $i \\in \\{0,1\\}$ 和 $j \\in \\{0,\\dots,5\\}$，\n    $$\n    A_{ij} = \\cos\\big( (i+1)(j+2) \\big) + 0.2 \\sin\\big( (i+2) + (j+1) \\big).\n    $$\n  - $y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$。\n  - 对于 $j \\in \\{0,\\dots,5\\}$，\n    $$\n    \\tau_{j} = \\begin{cases}\n    10^{6},  \\text{若 } j+1 \\in \\{1,2\\},\\\\\n    10^{-6},  \\text{若 } j+1 = 3,\\\\\n    1.0,  \\text{其他情况。}\n    \\end{cases}\n    $$\n  - $\\beta = 10.0$。\n\n你的程序的必需输出：\n- 对于每个用例，计算如上定义的四元组 $\\big(e_{\\Sigma}, e_{C^{-1}}, e_{\\log \\text{ev}}, e_{\\mu}\\big)$。\n- 将所有结果按用例 1 到 4 的顺序汇总到一个列表中，并按此顺序连接这些四元组。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_{1}, r_{2}, \\dots, r_{16}]$，其中每个 $r_{k}$ 都是一个浮点数。\n\n所有计算都是纯数学的，不需要物理单位。按照惯例，三角函数内的角度以弧度为单位。请在任何适用的地方使用 Cholesky 分解和三角求解，并避免使用显式的矩阵逆（除非在本测试中用于直接比较的小规模基准所严格要求），以确保数值稳定性。",
            "solution": "我们从稀疏贝叶斯学习 (SBL) 和相关向量机 (RVM) 中使用的线性高斯模型开始：$y = A x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{n})$，以及高斯先验 $x \\sim \\mathcal{N}(0, \\Gamma)$，其中 $\\Gamma = \\operatorname{diag}(\\tau_{1},\\dots,\\tau_{p}) \\succ 0$。$y$ 的边际似然（证据）是高斯的，$y \\sim \\mathcal{N}(0, C)$，其中 $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$，后验分布为 $x \\mid y \\sim \\mathcal{N}(\\mu, \\Sigma)$。\n\n使用矩阵求逆引理推导低秩表达式：\n从高斯条件化得到的后验协方差可以写为 $\\Sigma = \\left(\\Gamma^{-1} + \\beta A^{\\top} A\\right)^{-1}$。为避免在 $p$ 很大时直接对 $p \\times p$ 矩阵求逆，我们使用矩阵求逆引理（Woodbury 恒等式）。Woodbury 恒等式指出，对于具有适当逆的可共形矩阵，\n$$\n\\left(U + B R B^{\\top}\\right)^{-1} = U^{-1} - U^{-1} B \\left( R^{-1} + B^{\\top} U^{-1} B \\right)^{-1} B^{\\top} U^{-1}.\n$$\n选择 $U = \\Gamma^{-1}$，$B = \\sqrt{\\beta} A^{\\top}$ 和 $R = I_{n}$。那么\n$$\n\\Sigma = \\left(\\Gamma^{-1} + \\beta A^{\\top} A\\right)^{-1} = \\Gamma - \\Gamma A^{\\top} \\left( \\beta^{-1} I_{n} + A \\Gamma A^{\\top} \\right)^{-1} A \\Gamma.\n$$\n定义 $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$，可得到低秩形式\n$$\n\\Sigma = \\Gamma - \\Gamma A^{\\top} C^{-1} A \\Gamma,\n$$\n这只需要求解与 $C$ 相关的 $n \\times n$ 系统。\n\n类似地，从 $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$ 出发，应用 Woodbury 恒等式，令 $U = \\beta^{-1} I_{n}$、$B = A$、$R = \\Gamma$，可得\n$$\nC^{-1} = \\beta I_{n} - \\beta^{2} A \\left( \\Gamma^{-1} + \\beta A^{\\top} A \\right)^{-1} A^{\\top} = \\beta I_{n} - \\beta^{2} A \\Sigma A^{\\top}.\n$$\n因此，$C^{-1}$ 可以用 $\\Sigma$ 表示，而无需显式地对任何 $n \\times n$ 矩阵求逆，除了 Cholesky 求解所需的那些。\n\n后验均值的等价形式：\n根据高斯条件化，后验均值满足\n$$\n\\mu = \\beta \\Sigma A^{\\top} y.\n$$\n使用 $\\Sigma$ 的低秩表达式，我们也可以写成\n$$\n\\mu = \\Gamma A^{\\top} C^{-1} y,\n$$\n这只用到了与 $C$ 相关的求解和与 $\\Gamma$ 的矩阵乘法。\n\n使用 Cholesky 分解对证据进行数值稳定的评估：\n对于一个对称正定矩阵 $C$，其 Cholesky 分解 $C = L L^{\\top}$（其中 $L$ 是下三角矩阵）保证了：\n- 对数行列式可以计算为\n$$\n\\log \\lvert C \\rvert = 2 \\sum_{i=1}^{n} \\log L_{ii}.\n$$\n- 二次型可以通过求解 $L z = y$ 然后求解 $L^{\\top} w = z$ 来计算，得到 $w = C^{-1} y$，因此\n$$\ny^{\\top} C^{-1} y = \\lVert z \\rVert_{2}^{2}.\n$$\n综合这些，对数证据为\n$$\n\\log p(y \\mid \\beta, \\Gamma) = -\\tfrac{1}{2} \\left( n \\log(2\\pi) + 2 \\sum_{i=1}^{n} \\log L_{ii} + \\lVert z \\rVert_{2}^{2} \\right).\n$$\n这些表达式避免了显式构造 $C^{-1}$，并且是数值稳定的。\n\n针对 $n \\ll p$ 的算法设计：\n给定 $A$、$y$、$\\tau$ 和 $\\beta$：\n- 构造 $\\Gamma = \\operatorname{diag}(\\tau)$ 和 $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$。\n- 计算 Cholesky 分解 $C = L L^{\\top}$，如有必要，可添加最小的对角抖动以确保数值上的正定性。\n- 稳定地计算 $\\mu$：\n  - 求解 $L z = y$ 和 $L^{\\top} w = z$，从而 $w = C^{-1} y$。\n  - 计算 $\\mu = \\Gamma A^{\\top} w$。\n  - 为了验证，一旦 $\\Sigma$ 可用（见下文），也计算 $\\mu' = \\beta \\Sigma A^{\\top} y$，并比较 $\\lVert \\mu - \\mu' \\rVert_{2}$。\n- 以低秩形式计算 $\\Sigma$：\n  - 构造 $AG = A \\Gamma$ 并求解 $L Y = AG$ 和 $L^{\\top} Z = Y$，从而 $Z = C^{-1} A \\Gamma$。\n  - 计算 $\\Sigma = \\Gamma - (\\Gamma A^{\\top}) Z$。\n- 通过 $\\Sigma$ 以低秩形式计算 $C^{-1}$：\n  - 计算 $C^{-1}_{\\text{low-rank}} = \\beta I_{n} - \\beta^{2} A \\Sigma A^{\\top}$。\n- 如上所述，使用 $L$ 计算对数证据。\n- 对于小维度的直接基准：\n  - 通过 $K = \\Gamma^{-1} + \\beta A^{\\top} A$ 的 Cholesky 分解来求逆，从而计算 $\\Sigma_{\\text{direct}}$。\n  - 通过 $C$ 的 Cholesky 分解来求逆，从而计算 $C^{-1}_{\\text{direct}}$。\n  - 使用符号对数行列式计算和线性求解，或等效地再次通过 Cholesky 因子，直接计算对数证据。\n- 报告误差度量：\n  - $e_{\\Sigma} = \\max\\limits_{i,j} \\lvert \\Sigma - \\Sigma_{\\text{direct}} \\rvert$,\n  - $e_{C^{-1}} = \\max\\limits_{i,j} \\lvert C^{-1}_{\\text{low-rank}} - C^{-1}_{\\text{direct}} \\rvert$,\n  - $e_{\\log \\text{ev}} = \\lvert \\log p_{\\text{Cholesky}} - \\log p_{\\text{direct}} \\rvert$,\n  - $e_{\\mu} = \\lVert \\mu - \\beta \\Sigma_{\\text{direct}} A^{\\top} y \\rVert_{2}$。\n因为基于 Cholesky 的计算和直接计算在数学上是等价的，对于良态情况，误差应接近机器精度（在双精度下约为 $10^{-12}$ 到 $10^{-9}$ 的量级），而病态测试用例由于数值舍入误差，可能会表现出稍大但仍然很小的差异。\n\n复杂度考量：\n- 如果通过 $A \\Gamma A^{\\top}$ 计算，构造 $C$ 的成本为 $\\mathcal{O}(n^{2} p)$。在低秩策略中，主要成本是 $C$ 的 Cholesky 分解，成本为 $\\mathcal{O}(n^{3})$，以及三角求解，成本为 $\\mathcal{O}(n^{2} p)$。\n- 在低秩路径中不需要对 $p \\times p$ 矩阵求逆；直接的 $p \\times p$ 求逆仅用于测试套件强制要求的小规模验证。\n\n该程序为指定的四个确定性测试用例实现了上述算法，并为每个用例输出四个误差度量的串联列表，形式为单行 $[r_{1}, r_{2}, \\dots, r_{16}]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef chol_with_jitter(M, lower=True, max_tries=5, initial_jitter=0.0):\n    \"\"\"Compute a Cholesky factorization with increasing diagonal jitter if needed.\"\"\"\n    jitter = initial_jitter\n    for _ in range(max_tries):\n        try:\n            L = cholesky(M + jitter * np.eye(M.shape[0]), lower=lower, check_finite=False)\n            return L, jitter\n        except Exception:\n            # Increase jitter geometrically\n            jitter = 1e-12 if jitter == 0.0 else jitter * 10.0\n    # Final attempt, let it raise\n    L = cholesky(M + jitter * np.eye(M.shape[0]), lower=lower, check_finite=False)\n    return L, jitter\n\ndef build_case(case_id):\n    if case_id == 1:\n        n, p = 3, 8\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.sin((i+1)*(j+1)) + 0.1*np.cos((i+1) + 2*(j+1))\n        y = np.array([np.cos(i+1) + 0.5*np.sin(2*(i+1)) for i in range(n)], dtype=float)\n        tau = np.array([0.5 + 0.1*(j+1) for j in range(p)], dtype=float)\n        beta = 25.0\n        return A, y, tau, beta\n    elif case_id == 2:\n        n, p = 3, 8\n        v = np.array([1.0, -2.0, 3.0], dtype=float)\n        eps = 1e-3\n        A = np.zeros((n, p), dtype=float)\n        for j in range(p):\n            wj = np.array([np.sin(j+1), np.cos(j+1), np.sin(2*(j+1))], dtype=float)\n            A[:, j] = v + eps * wj\n        y = np.zeros(n, dtype=float)\n        tau = np.array([10.0 ** (-4.0 + 6.0 * (j/7.0)) for j in range(p)], dtype=float)\n        beta = 1.0\n        return A, y, tau, beta\n    elif case_id == 3:\n        n, p = 4, 10\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.sin(0.3*(i+1) + 0.7*(j+1)) + 0.05*((i+1) - (j+1))\n        y = np.array([np.sin(1.5*(i+1)) for i in range(n)], dtype=float)\n        tau = np.array([0.2 + 0.05*j for j in range(p)], dtype=float)\n        beta = 1000.0\n        return A, y, tau, beta\n    elif case_id == 4:\n        n, p = 2, 6\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.cos((i+1)*(j+2)) + 0.2*np.sin((i+2) + (j+1))\n        y = np.array([1.0, -1.0], dtype=float)\n        tau = np.zeros(p, dtype=float)\n        for j in range(p):\n            if (j+1) in (1, 2):\n                tau[j] = 1e6\n            elif (j+1) == 3:\n                tau[j] = 1e-6\n            else:\n                tau[j] = 1.0\n        beta = 10.0\n        return A, y, tau, beta\n    else:\n        raise ValueError(\"Unknown case id\")\n\ndef stable_evidence_from_cholesky(L, y):\n    # C = L L^T, y^T C^{-1} y = ||L^{-1} y||^2\n    z = solve_triangular(L, y, lower=True, check_finite=False)\n    quad = float(np.dot(z, z))\n    logdet = 2.0 * np.sum(np.log(np.diag(L)))\n    n = L.shape[0]\n    log_ev = -0.5 * (n * np.log(2.0*np.pi) + logdet + quad)\n    return log_ev, quad, logdet\n\ndef invert_via_cholesky(L):\n    # Given C = L L^T, compute C^{-1} without forming C^{-1} explicitly via solving for I\n    n = L.shape[0]\n    I = np.eye(n)\n    # Solve L Z = I - Z = L^{-1}\n    Z = solve_triangular(L, I, lower=True, check_finite=False)\n    # Then C^{-1} = (L^{-T}) (L^{-1}) = Z^T Z\n    Cinv = Z.T @ Z\n    return Cinv\n\ndef sigma_direct(Gamma, A, beta):\n    # Sigma_direct = (Gamma^{-1} + beta A^T A)^{-1} via Cholesky\n    p = Gamma.shape[0]\n    Ginv = np.diag(1.0 / np.diag(Gamma))\n    K = Ginv + beta * (A.T @ A)\n    Lk, _ = chol_with_jitter(K, lower=True)\n    # Invert K via Cholesky\n    pI = np.eye(p)\n    Z = solve_triangular(Lk, pI, lower=True, check_finite=False)\n    Kinv = Z.T @ Z\n    return Kinv\n\ndef compute_errors_for_case(A, y, tau, beta):\n    n, p = A.shape\n    Gamma = np.diag(tau)\n    # Build C\n    C = (1.0/beta) * np.eye(n) + A @ Gamma @ A.T\n    # Cholesky of C\n    Lc, jitter = chol_with_jitter(C, lower=True)\n    # Low-rank mu: mu = Gamma A^T C^{-1} y\n    z = solve_triangular(Lc, y, lower=True, check_finite=False)\n    w = solve_triangular(Lc.T, z, lower=False, check_finite=False)\n    mu_lowrank = Gamma @ (A.T @ w)\n    # Low-rank Sigma: Sigma = Gamma - Gamma A^T C^{-1} A Gamma\n    AG = A @ Gamma\n    Y = solve_triangular(Lc, AG, lower=True, check_finite=False)\n    Z = solve_triangular(Lc.T, Y, lower=False, check_finite=False)  # Z = C^{-1} A Gamma\n    Sigma_lowrank = Gamma - (Gamma @ A.T) @ Z\n    # Low-rank Cinv via Sigma: C^{-1} = beta I - beta^2 A Sigma A^T\n    Cinv_lowrank = beta * np.eye(n) - (beta**2) * (A @ Sigma_lowrank @ A.T)\n    # Evidence via Cholesky\n    log_ev_chol, quad_chol, logdet_chol = stable_evidence_from_cholesky(Lc, y)\n    # Direct baselines\n    Sigma_dir = sigma_direct(Gamma, A, beta)\n    # Direct Cinv via Cholesky inversion (reuse Lc to be consistent)\n    Cinv_dir = invert_via_cholesky(Lc)\n    # Direct evidence using C (via slogdet and solve)\n    sign, logdet_direct = np.linalg.slogdet(C)\n    if sign = 0:\n        # Fallback to Cholesky logdet if numerical issue\n        logdet_direct = logdet_chol\n    sol = np.linalg.solve(C, y)\n    quad_direct = float(y @ sol)\n    log_ev_direct = -0.5 * (n * np.log(2.0*np.pi) + logdet_direct + quad_direct)\n    # Posterior mean via beta Sigma A^T y\n    mu_via_sigma = beta * (Sigma_dir @ (A.T @ y))\n    # Errors\n    e_sigma = float(np.max(np.abs(Sigma_lowrank - Sigma_dir)))\n    e_cinv = float(np.max(np.abs(Cinv_lowrank - Cinv_dir)))\n    e_logev = float(abs(log_ev_chol - log_ev_direct))\n    e_mu = float(np.linalg.norm(mu_lowrank - mu_via_sigma))\n    return e_sigma, e_cinv, e_logev, e_mu\n\ndef solve():\n    test_cases = [1, 2, 3, 4]\n    results = []\n    for cid in test_cases:\n        A, y, tau, beta = build_case(cid)\n        e_sigma, e_cinv, e_logev, e_mu = compute_errors_for_case(A, y, tau, beta)\n        results.extend([e_sigma, e_cinv, e_logev, e_mu])\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}