{
    "hands_on_practices": [
        {
            "introduction": "This exercise demonstrates how a specific Gaussian Scale Mixture (GSM) construction—mixing a Gaussian with an Inverse-Gamma distribution—naturally leads to the Student-$t$ prior. This practice is valuable for understanding why such priors generate non-convex penalties and how their properties, like logarithmic growth and lack of an exact threshold, differ fundamentally from the standard $\\ell_1$ penalty. Grasping this connection is key to appreciating how heavy-tailed priors can preserve large signal coefficients while still promoting sparsity. ",
            "id": "3451059",
            "problem": "Consider a single real coefficient $x \\in \\mathbb{R}$ endowed with a hierarchical sparsity prior constructed as a Gaussian scale mixture (GSM). Specifically, conditionally on a latent variance $v > 0$, let $x \\,|\\, v \\sim \\mathcal{N}(0, v)$, and let the mixing density be an inverse-gamma distribution $v \\sim \\mathrm{InvGamma}(\\alpha, \\beta)$ with shape $\\alpha = \\nu/2$ and scale $\\beta = \\nu \\tau^{2}/2$, where $\\nu > 0$ is the degrees-of-freedom parameter and $\\tau > 0$ is a scale parameter. Define the negative log-prior penalty $\\phi(r)$ for $r = |x|$ (up to an additive constant) by $\\phi(r) := -\\ln p(x)$ with the additive constant chosen so that $\\phi(0) = 0$.\n\nTasks:\n- Starting only from the definitions of the Gaussian density, the inverse-gamma density, and the GSM construction, derive the explicit closed-form expression of the penalty $\\phi(r)$ as a function of $r$, $\\nu$, and $\\tau$.\n- Using first and second derivatives with respect to $r$, determine on which intervals of $r \\ge 0$ the penalty is convex or concave, and state whether $\\phi(r)$ is strictly increasing on $r \\ge 0$.\n- Compare the asymptotic growth of $\\phi(r)$ as $r \\to \\infty$ to that of the $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(r) = r$.\n- Consider scalar Gaussian denoising under a Maximum A Posteriori (MAP) estimator: for a given observation $y \\in \\mathbb{R}$ and noise variance $\\sigma^{2} > 0$, define\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|) \\right\\},\n$$\nwhere $\\lambda > 0$ is a regularization weight. Determine whether the associated proximal operator exhibits an exact threshold $T > 0$ such that $\\hat{x}(y) = 0$ for all $|y| \\le T$. If there is such a threshold, provide $T$ in closed form; if there is no such threshold, state $T = 0$ and justify this fact. Contrast this with the $\\ell_{1}$ case $\\phi_{\\ell_{1}}(r) = r$, and determine the corresponding threshold in that case.\n\nYour final reported answer must be the explicit expression you derived for $\\phi(r)$ (with the additive constant chosen so that $\\phi(0) = 0$). No numerical approximation is required.",
            "solution": "We begin from the Gaussian scale mixture (GSM) construction. By definition, the conditional prior of $x$ given $v$ is Gaussian,\n$$\np(x \\,|\\, v) \\;=\\; \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right),\n$$\nand the mixing density is inverse-gamma with shape $\\alpha = \\nu/2$ and scale $\\beta = \\nu \\tau^{2}/2$:\n$$\np(v) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right), \\quad v > 0.\n$$\nThe marginal prior of $x$ is obtained by integrating out $v$:\n$$\np(x) \\;=\\; \\int_{0}^{\\infty} p(x \\,|\\, v) \\, p(v) \\, \\mathrm{d}v \n\\;=\\; \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right) \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\nCollecting powers of $v$ and exponents yields\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\int_{0}^{\\infty} v^{-(\\alpha + \\tfrac{3}{2})} \\exp\\!\\left(-\\frac{\\tfrac{x^{2}}{2} + \\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\nDefine $C := \\tfrac{x^{2}}{2} + \\beta$. Using the standard integral identity for $p > 0$,\n$$\n\\int_{0}^{\\infty} v^{-p-1} \\exp\\!\\left(-\\frac{C}{v}\\right) \\, \\mathrm{d}v \\;=\\; C^{-p} \\, \\Gamma(p),\n$$\nand setting $p = \\alpha + \\tfrac{1}{2}$, we obtain\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\alpha + \\tfrac{1}{2}\\right) \\, C^{-(\\alpha + \\tfrac{1}{2})}.\n$$\nSubstituting $\\alpha = \\nu/2$ and $\\beta = \\nu \\tau^{2}/2$, we have $C = \\tfrac{1}{2}(x^{2} + \\nu \\tau^{2})$ and\n$$\np(x) \\;=\\; \\frac{\\left(\\tfrac{\\nu \\tau^{2}}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right) \\left(\\tfrac{x^{2} + \\nu \\tau^{2}}{2}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\nRearranging constants gives the standard Student-$t$ density with $\\nu$ degrees of freedom and scale $\\tau$,\n$$\np(x) \\;=\\; \\frac{\\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right) \\sqrt{\\pi \\nu} \\, \\tau} \\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\nDefine the penalty $\\phi(r)$ for $r = |x|$ by $\\phi(r) := -\\ln p(x)$, up to an additive constant chosen so that $\\phi(0) = 0$. Using the expression above,\n$$\n-\\ln p(x) \\;=\\; \\text{const} + \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right).\n$$\nImposing $\\phi(0) = 0$ fixes the additive constant, yielding the explicit penalty\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right).\n$$\n\nWe now analyze its curvature and monotonicity on $r \\ge 0$. Differentiate with respect to $r$:\n$$\n\\phi'(r) \\;=\\; \\frac{\\nu + 1}{2} \\cdot \\frac{2 r}{\\nu \\tau^{2} + r^{2}} \\;=\\; \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}}.\n$$\nFor $r \\ge 0$, we have $\\phi'(r) \\ge 0$, with $\\phi'(r) = 0$ if and only if $r = 0$, hence $\\phi$ is strictly increasing on $r > 0$. The second derivative is\n$$\n\\phi''(r) \\;=\\; (\\nu + 1) \\cdot \\frac{(\\nu \\tau^{2} + r^{2}) - 2 r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}\n\\;=\\; (\\nu + 1) \\cdot \\frac{\\nu \\tau^{2} - r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}.\n$$\nThus, $\\phi''(r) > 0$ for $0 \\le r < \\sqrt{\\nu}\\,\\tau$, $\\phi''(r) = 0$ at $r = \\sqrt{\\nu}\\,\\tau$, and $\\phi''(r) < 0$ for $r > \\sqrt{\\nu}\\,\\tau$. Therefore, the penalty is convex near the origin, has an inflection point at $r = \\sqrt{\\nu}\\,\\tau$, and is concave for sufficiently large $r$.\n\nFor asymptotic growth, as $r \\to \\infty$,\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(\\frac{r^{2}}{\\nu \\tau^{2}}\\left(1 + o(1)\\right)\\right)\n\\;=\\; (\\nu + 1) \\, \\ln r \\;-\\; \\frac{\\nu + 1}{2} \\, \\ln(\\nu \\tau^{2}) \\;+\\; o(1),\n$$\nwhich grows logarithmically. In contrast, the $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(r) = r$ grows linearly. Hence, the Student-$t$ penalty is substantially less punitive to large coefficients than $\\ell_{1}$, reflecting heavy tails.\n\nWe next analyze thresholding for scalar Gaussian denoising under a Maximum A Posteriori (MAP) estimator with objective\n$$\nJ(x) \\;=\\; \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|).\n$$\nWe check whether there exists $T > 0$ such that $\\hat{x}(y) = 0$ for all $|y| \\le T$. Because $\\phi$ is differentiable at $r = 0$ with\n$$\n\\phi'(0^{+}) \\;=\\; \\lim_{r \\downarrow 0} \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}} \\;=\\; 0,\n$$\nthe one-sided derivatives of $J$ at $x = 0$ are\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\downarrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} + \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}, \n\\quad\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\uparrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} - \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}.\n$$\nFor $x = 0$ to be a minimizer, we require that zero belongs to the subdifferential of $J$ at $0$. Here the subdifferential reduces to the singleton $\\{-y/\\sigma^{2}\\}$ because $J$ is differentiable at $0$, and this contains $0$ if and only if $y = 0$. Therefore, there is no nonzero threshold region: the only case where the minimizer is exactly zero is $y = 0$. Hence the threshold is $T = 0$ for the Student-$t$ penalty. This stands in contrast to the $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(r) = r$, whose subgradient at $0$ is the interval $[-1, 1]$, yielding the well-known soft-thresholding condition $|y|/\\sigma^{2} \\le \\lambda$ for $\\hat{x}(y) = 0$, i.e., threshold $T = \\lambda \\, \\sigma^{2}$.\n\nIn summary:\n- The explicit penalty induced by the Student-$t$ GSM prior is $\\phi(r) = \\tfrac{\\nu + 1}{2} \\ln\\!\\left(1 + \\tfrac{r^{2}}{\\nu \\tau^{2}}\\right)$ with $\\phi(0) = 0$.\n- $\\phi$ is strictly increasing on $r \\ge 0$, convex for $0 \\le r < \\sqrt{\\nu}\\,\\tau$, has an inflection point at $r = \\sqrt{\\nu}\\,\\tau$, and is concave for $r > \\sqrt{\\nu}\\,\\tau$.\n- $\\phi(r)$ grows logarithmically as $r \\to \\infty$, slower than the linear growth of $\\ell_{1}$.\n- The scalar MAP denoiser with this penalty has no nonzero threshold ($T = 0$), unlike the $\\ell_{1}$ case where $T = \\lambda \\sigma^{2}$.",
            "answer": "$$\\boxed{\\frac{\\nu + 1}{2} \\,\\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right)}$$"
        },
        {
            "introduction": "This practice explores the subtle yet crucial differences between the Maximum A Posteriori (MAP) and the Minimum Mean-Squared Error (MMSE) estimators, using the classic Laplace prior as a case study. By leveraging the GSM representation of the Laplace distribution, you will analyze why the fully Bayesian MMSE estimator yields a smooth, continuous shrinkage function. This behavior contrasts sharply with the \"kink\" and \"dead zone\" of the familiar soft-thresholding operator derived from the MAP objective, highlighting a key trade-off between optimization-based and fully Bayesian approaches to sparse estimation. ",
            "id": "3451024",
            "problem": "Consider a single-coefficient denoising problem under Additive White Gaussian Noise (AWGN), where an unknown scalar $x \\in \\mathbb{R}$ is observed as $y = x + n$, with $n \\sim \\mathcal{N}(0,\\sigma^2)$ independent of $x$. Assume a Laplace prior on $x$ with parameter $\\lambda > 0$, i.e., $p(x) = (\\lambda/2) \\exp(-\\lambda |x|)$. It is known that the Laplace law can be represented as a Gaussian Scale Mixture (GSM), namely a hierarchical prior of the form $x \\, | \\, s \\sim \\mathcal{N}(0,s)$ and $s \\sim \\operatorname{Exp}(\\lambda^2/2)$, where $\\operatorname{Exp}(\\rho)$ denotes the exponential distribution with rate $\\rho$. The Minimum Mean-Squared Error (MMSE) Bayes estimator (also called the MMSE denoiser) is defined as $f_{\\mathrm{MMSE}}(y) = \\mathbb{E}[x \\, | \\, y]$. The Maximum A Posteriori (MAP) estimator under the Laplace prior coincides with the soft-thresholding denoiser $f_{\\mathrm{MAP}}(y) = \\operatorname{soft}(y; \\tau)$ with threshold $\\tau = \\sigma^2 \\lambda$, i.e., $f_{\\mathrm{MAP}}(y) = \\operatorname{sign}(y) \\max(|y| - \\sigma^2 \\lambda, 0)$.\n\nStarting from first principles for hierarchical models and standard Gaussian conditioning, and without assuming any pre-derived shrinkage formulas, perform the following:\n\n$1.$ Specialize the MMSE estimator $f_{\\mathrm{MMSE}}(y)$ to the Laplace-as-GSM model by expressing it as an integral over the mixing variable $s$, and reduce it to a one-dimensional expectation of a Gaussian linear estimator under the posterior $p(s \\, | \\, y)$.\n\n$2.$ Establish qualitative properties of $f_{\\mathrm{MMSE}}(y)$: oddness, continuity, strict monotonicity in $y$, and the fact that its slope at $y=0$ lies strictly between $0$ and $1$. Use the hierarchical structure and the monotone likelihood ratio of $p(y \\, | \\, s)$ in $s$ to justify the monotonicity of the posterior-weighted shrinkage factor.\n\n$3.$ Derive the asymptotic behavior of $f_{\\mathrm{MMSE}}(y)$ as $|y| \\to \\infty$ (for fixed $\\sigma^2$ and $\\lambda$) by analyzing the posterior $p(x \\, | \\, y)$ split over $x \\ge 0$ and $x < 0$, and show that it matches the leading-order bias of the MAP soft-threshold when $|y|$ is large.\n\n$4.$ Compare $f_{\\mathrm{MMSE}}(y)$ to soft-thresholding across Signal-to-Noise Ratio (SNR) regimes. Here $\\mathrm{SNR}$ can be taken as $\\operatorname{Var}(x)/\\sigma^2$ with $\\operatorname{Var}(x) = 2/\\lambda^2$ for the Laplace prior. Discuss the behavior as $\\sigma \\to 0$ (high SNR) and as $\\sigma \\to \\infty$ (low SNR), focusing on whether $f_{\\mathrm{MMSE}}(y)$ exhibits a dead zone, how the transition around $|y| \\approx \\sigma^2 \\lambda$ differs from soft-thresholding, and pointwise limits for fixed $y$.\n\nBased on your derivations and qualitative analysis, choose all correct statements:\n\nA. The MMSE denoiser under the Laplace-as-GSM prior can be written as $y$ multiplied by a posterior-average shrinkage factor $\\mathbb{E}\\!\\left[ \\frac{s}{s+\\sigma^2} \\, \\big| \\, y \\right]$, which lies strictly between $0$ and $1$ and increases with $|y|$; it is continuous and has no dead zone, unlike soft-thresholding, which sets the output to $0$ for $|y| \\le \\sigma^2 \\lambda$.\n\nB. In the high signal-to-noise ratio regime $\\sigma \\to 0$, both the MMSE denoiser and soft-thresholding approach the identity mapping for all $y$, including small $|y|$; hence the MMSE denoiser has a dead zone of width $\\sigma^2 \\lambda$ that matches soft-thresholding.\n\nC. For fixed $\\lambda$ and $\\sigma^2$, the MMSE denoiser asymptotically matches soft-thresholding for $|y| \\to \\infty$ in the sense that $f_{\\mathrm{MMSE}}(y) = \\operatorname{sign}(y)\\big(|y| - \\sigma^2 \\lambda\\big) + o(1)$; yet the two differ around $|y| \\approx \\sigma^2 \\lambda$, where the MMSE transition is smooth while soft-thresholding has a kink.\n\nD. In the extreme low signal-to-noise ratio regime $\\sigma \\to \\infty$, the MMSE denoiser converges pointwise to $0$ for any finite $y$, whereas soft-thresholding converges to $\\operatorname{sign}(y)\\,|y|$, i.e., it becomes hard-thresholding with infinite threshold.\n\nE. The slope of the MMSE denoiser at $y=0$ is exactly $1 - \\sigma^2 \\lambda$; therefore, at sufficiently large $\\sigma$ the denoiser is decreasing near $0$, violating monotonicity.\n\nSelect all options that are correct.",
            "solution": "The problem statement is found to be valid. It is scientifically grounded, well-posed, objective, and contains sufficient information for a rigorous analysis. We proceed with the derivation.\n\nThe problem considers a denoising task with the model $y = x + n$, where $n \\sim \\mathcal{N}(0,\\sigma^2)$. The prior on the unknown scalar $x$ is a Laplace distribution, $p(x) = (\\lambda/2) \\exp(-\\lambda |x|)$. This prior is represented as a Gaussian Scale Mixture (GSM), a hierarchical model where $x \\, | \\, s \\sim \\mathcal{N}(0,s)$ and the mixing variable $s$ follows an exponential distribution $s \\sim \\operatorname{Exp}(\\lambda^2/2)$, whose probability density function is $p(s) = (\\lambda^2/2) \\exp(-(\\lambda^2/2)s)$ for $s \\ge 0$.\n\n### 1. Derivation of the MMSE Estimator in the GSM Framework\n\nThe Minimum Mean-Squared Error (MMSE) estimator is the posterior mean of $x$ given the observation $y$:\n$$f_{\\mathrm{MMSE}}(y) = \\mathbb{E}[x \\, | \\, y]$$\nUsing the law of total expectation over the hierarchical structure, we can write this as:\n$$f_{\\mathrm{MMSE}}(y) = \\mathbb{E}[\\mathbb{E}[x \\, | \\, y, s] \\, | \\, y]$$\nThe inner expectation, $\\mathbb{E}[x \\, | \\, y, s]$, is the posterior mean of $x$ given both the observation $y$ and the variance parameter $s$. This corresponds to a standard Gaussian-Gaussian estimation problem:\n- Prior: $x \\sim \\mathcal{N}(0, s)$\n- Likelihood: $y \\, | \\, x \\sim \\mathcal{N}(x, \\sigma^2)$\n\nFor this model, the posterior $p(x \\, | \\, y, s)$ is also Gaussian. Its mean, which is the linear MMSE estimator (Wiener filter) for a fixed $s$, is given by:\n$$\\mathbb{E}[x \\, | \\, y, s] = \\frac{s}{s + \\sigma^2} y$$\nThis is a standard result from Bayesian inference with Gaussian distributions. The term $\\frac{s}{s+\\sigma^2}$ is a shrinkage factor that depends on the signal variance $s$ and noise variance $\\sigma^2$.\n\nSubstituting this back into the outer expectation, we get the expression for the MMSE denoiser:\n$$f_{\\mathrm{MMSE}}(y) = \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} y \\, \\Big| \\, y \\right]$$\nSince $y$ is a given constant with respect to the expectation over the posterior distribution of $s$, we can factor it out:\n$$f_{\\mathrm{MMSE}}(y) = y \\, \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\Big| \\, y \\right]$$\nThis expresses the MMSE estimator as the observation $y$ multiplied by a shrinkage factor, which is itself an average of the Gaussian shrinkage factor $\\frac{s}{s+\\sigma^2}$ weighted by the posterior probability density $p(s \\, | \\, y)$. This completes the first task.\n\n### 2. Qualitative Properties of the MMSE Estimator\n\n**Oddness:** To show $f_{\\mathrm{MMSE}}(y)$ is an odd function, we examine the posterior $p(s \\, | \\, y)$:\n$$p(s \\, | \\, y) = \\frac{p(y \\, | \\, s) p(s)}{p(y)}$$\nThe marginal likelihood $p(y \\, | \\, s)$ is obtained by integrating out $x$:\n$$p(y \\, | \\, s) = \\int_{-\\infty}^{\\infty} p(y \\, | \\, x) p(x \\, | \\, s) \\, dx = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; x, \\sigma^2) \\mathcal{N}(x; 0, s) \\, dx$$\nThis is a convolution of two zero-mean Gaussian densities, which results in another zero-mean Gaussian density with variance equal to the sum of variances: $y \\, | \\, s \\sim \\mathcal{N}(0, s + \\sigma^2)$.\n$$p(y \\, | \\, s) = \\frac{1}{\\sqrt{2\\pi(s+\\sigma^2)}} \\exp\\left( -\\frac{y^2}{2(s+\\sigma^2)} \\right)$$\nSince $p(y \\, | \\, s)$ depends on $y^2$, it is an even function of $y$. Consequently, $p(s \\, | \\, y)$, which is proportional to $p(y \\, | \\, s)p(s)$, is also an even function of $y$, i.e., $p(s \\, | \\, y) = p(s \\, | \\, -y)$. Therefore, the posterior expectation is also even in $y$:\n$$\\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\Big| \\, y \\right] = \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\Big| \\, -y \\right]$$\nFinally, $f_{\\mathrm{MMSE}}(-y) = (-y) \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\Big| \\, -y \\right] = -y \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\Big| \\, y \\right] = -f_{\\mathrm{MMSE}}(y)$. The function is odd.\n\n**Continuity:** The marginal likelihood $p(y \\, | \\, s)$ is a continuous function of $y$ for any $s \\ge 0, \\sigma^2 > 0$. The marginal $p(y) = \\int p(y|s)p(s) ds$ is also continuous. The posterior $p(s|y)$ is therefore continuous in $y$ (away from where $p(y)=0$, which is nowhere). The expectation $\\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, | \\, y \\right]$ is an integral of a continuous function of $y$, and is thus continuous. Multiplication by $y$ preserves continuity. Thus, $f_{\\mathrm{MMSE}}(y)$ is continuous for all $y \\in \\mathbb{R}$. In fact, it can be shown to be infinitely differentiable ($C^\\infty$).\n\n**Strict Monotonicity:** Let $W(y) = \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\big| \\, y \\right]$. Then $f_{\\mathrm{MMSE}}(y) = y W(y)$. The family of likelihoods $\\{p(y|s)\\}_{s\\ge 0}$ has a Monotone Likelihood Ratio (MLR) in $|y|$. For $s_2 > s_1$, the ratio\n$$\\frac{p(y|s_2)}{p(y|s_1)} = \\sqrt{\\frac{s_1+\\sigma^2}{s_2+\\sigma^2}} \\exp\\left( \\frac{y^2}{2} \\left( \\frac{1}{s_1+\\sigma^2} - \\frac{1}{s_2+\\sigma^2} \\right) \\right)$$\nis a strictly increasing function of $|y|$ because a larger $s_2$ gives a smaller denominator in the exponent's coefficient. By a standard result, if the likelihood has MLR in a statistic $|y|$, then the posterior distribution $p(s|y)$ is stochastically increasing with $|y|$. This means that for any increasing function $g(s)$, $\\mathbb{E}[g(s) \\, | \\, y]$ is a non-decreasing function of $|y|$. The shrinkage function $g(s) = \\frac{s}{s+\\sigma^2} = 1 - \\frac{\\sigma^2}{s+\\sigma^2}$ is strictly increasing for $s \\ge 0$. Therefore, $W(y)$ is a strictly increasing function of $|y|$ for $y \\ne 0$.\nThe derivative is $f'_{\\mathrm{MMSE}}(y) = W(y) + yW'(y)$. For $y > 0$, $W(y) > 0$ and $W'(y) > 0$, so $f'_{\\mathrm{MMSE}}(y) > 0$. By oddness, the derivative is also positive for $y<0$. At $y=0$, the slope is $f'_{\\mathrm{MMSE}}(0)=W(0)$.\n\n**Slope at $y=0$:** The slope at the origin is $f'_{\\mathrm{MMSE}}(0) = \\lim_{y\\to 0} \\frac{f_{\\mathrm{MMSE}}(y)}{y} = \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\big| \\, y=0 \\right]$. The function $\\frac{s}{s+\\sigma^2}$ is strictly between $0$ and $1$ for $s>0$ and $\\sigma^2 > 0$. The posterior $p(s|y=0)$ is a valid probability distribution over $s>0$. Therefore, its expectation must also lie strictly between $0$ and $1$. This proves $0 < f'_{\\mathrm{MMSE}}(0) < 1$. Because the slope is positive everywhere, the function is strictly monotonic.\n\n### 3. Asymptotic Behavior for $|y| \\to \\infty$\n\nThe posterior density is $p(x|y) \\propto p(y|x)p(x) = \\mathcal{N}(y; x, \\sigma^2) \\cdot (\\lambda/2)\\exp(-\\lambda|x|)$.\n$$p(x|y) \\propto \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2} - \\lambda|x|\\right)$$\nLet's analyze this for $y \\to \\infty$. We expect the posterior mass to concentrate where $x$ is positive and large. For $x > 0$, the negative log-posterior is $\\frac{(y-x)^2}{2\\sigma^2} + \\lambda x$ (+ constant). Minimizing this with respect to $x$ gives the mode (MAP estimate):\n$$\\frac{d}{dx}\\left(\\frac{(y-x)^2}{2\\sigma^2} + \\lambda x\\right) = -\\frac{y-x}{\\sigma^2} + \\lambda = 0 \\implies x_{\\mathrm{MAP}} = y - \\sigma^2\\lambda$$\nFor large $y$, $x_{\\mathrm{MAP}} > 0$ as required. The posterior distribution $p(x|y)$ will be concentrated around this mode. A Laplace approximation shows that for large $y$, $p(x|y)$ is well-approximated by a Gaussian centered at $y - \\sigma^2\\lambda$ with variance $\\sigma^2$. The mean of this approximating Gaussian is $y - \\sigma^2\\lambda$. The contribution to the posterior mean from the $x < 0$ region becomes negligible.\nThus, as $y \\to \\infty$,\n$$f_{\\mathrm{MMSE}}(y) = \\mathbb{E}[x|y] \\to y - \\sigma^2\\lambda$$\nMore formally, $f_{\\mathrm{MMSE}}(y) = y - \\sigma^2\\lambda + o(1)$. By oddness, for $y \\to -\\infty$, $f_{\\mathrm{MMSE}}(y) \\to y + \\sigma^2\\lambda$. We can combine these as:\n$$f_{\\mathrm{MMSE}}(y) = \\operatorname{sign}(y)\\left(|y| - \\sigma^2\\lambda\\right) + o(1) \\quad \\text{as } |y| \\to \\infty$$\nThis matches the asymptotic behavior of the soft-thresholding function $f_{\\mathrm{MAP}}(y) = \\operatorname{soft}(y; \\sigma^2\\lambda)$, which for large $|y|$ is exactly $\\operatorname{sign}(y)(|y| - \\sigma^2\\lambda)$.\n\n### 4. Comparison Across SNR Regimes\n\n**High SNR ($\\sigma \\to 0$):**\n-   $f_{\\mathrm{MAP}}(y) = \\operatorname{soft}(y; \\sigma^2\\lambda)$. The threshold $\\tau = \\sigma^2\\lambda \\to 0$. Thus, $f_{\\mathrm{MAP}}(y) \\to \\operatorname{soft}(y; 0) = y$.\n-   For $f_{\\mathrm{MMSE}}(y)$, as $\\sigma \\to 0$, the likelihood $p(y|x) = \\mathcal{N}(y; x, \\sigma^2)$ approaches a Dirac delta function $\\delta(x-y)$. The posterior $p(x|y) \\propto p(y|x)p(x)$ converges to $\\delta(x-y)$. Therefore, $\\mathbb{E}[x|y] \\to y$.\n-   Both estimators approach the identity function. Soft-thresholding has a dead zone of width $2\\sigma^2\\lambda$, which vanishes as $\\sigma \\to 0$. The MMSE estimator has no dead zone for any $\\sigma^2 > 0$, since $f_{\\mathrm{MMSE}}(y)=0$ only if $y=0$.\n\n**Low SNR ($\\sigma \\to \\infty$):**\n-   $f_{\\mathrm{MAP}}(y) = \\operatorname{soft}(y; \\sigma^2\\lambda)$. The threshold $\\tau = \\sigma^2\\lambda \\to \\infty$. For any fixed $y$, eventually $|y| < \\tau$, which means $f_{\\mathrm{MAP}}(y) = 0$. So, $f_{\\mathrm{MAP}}(y) \\to 0$ pointwise for all $y$.\n-   For $f_{\\mathrm{MMSE}}(y) = y \\, \\mathbb{E}\\left[ \\frac{s}{s + \\sigma^2} \\, \\big| \\, y \\right]$, we examine the posterior $p(s|y)$. As $\\sigma \\to \\infty$, the likelihood $p(y|s) \\approx 1/\\sqrt{2\\pi\\sigma^2}$ becomes independent of $s$. Thus, the data $y$ provides no information about $s$, and the posterior $p(s|y)$ reverts to the prior $p(s) = \\operatorname{Exp}(\\lambda^2/2)$.\n$$f_{\\mathrm{MMSE}}(y) \\approx y \\, \\mathbb{E}_{s \\sim p(s)}\\left[ \\frac{s}{s + \\sigma^2} \\right]$$\nThe expectation term behaves as $O(1/\\sigma^2)$ for large $\\sigma$. Specifically, $\\mathbb{E}\\left[\\frac{s}{s+\\sigma^2}\\right] < \\mathbb{E}\\left[\\frac{s}{\\sigma^2}\\right] = \\frac{\\mathbb{E}[s]}{\\sigma^2} = \\frac{2/\\lambda^2}{\\sigma^2}$. Therefore, for any fixed $y$, $f_{\\mathrm{MMSE}}(y) \\to 0$ as $\\sigma \\to \\infty$.\n-   Both estimators converge to the zero function.\n\n### Evaluation of Options\n\n**A. The MMSE denoiser under the Laplace-as-GSM prior can be written as $y$ multiplied by a posterior-average shrinkage factor $\\mathbb{E}\\!\\left[ \\frac{s}{s+\\sigma^2} \\, \\big| \\, y \\right]$, which lies strictly between $0$ and $1$ and increases with $|y|$; it is continuous and has no dead zone, unlike soft-thresholding, which sets the output to $0$ for $|y| \\le \\sigma^2 \\lambda$.**\n-   The formula $f_{\\mathrm{MMSE}}(y) = y \\mathbb{E}[ \\frac{s}{s+\\sigma^2} | y ]$ was derived in step 1. **Correct.**\n-   The shrinkage factor $\\frac{s}{s+\\sigma^2}$ is in $(0,1)$ for $s>0, \\sigma^2>0$, so its posterior expectation is also in $(0,1)$. Monotonicity with $|y|$ was established in step 2. **Correct.**\n-   Continuity was established in step 2. The absence of a dead zone is also correct, as $f_{\\mathrm{MMSE}}(y) \\ne 0$ for $y \\ne 0$. The description of soft-thresholding's dead zone is accurate. **Correct.**\n-   This statement is entirely **Correct**.\n\n**B. In the high signal-to-noise ratio regime $\\sigma \\to 0$, both the MMSE denoiser and soft-thresholding approach the identity mapping for all $y$, including small $|y|$; hence the MMSE denoiser has a dead zone of width $\\sigma^2 \\lambda$ that matches soft-thresholding.**\n-   The first part, that both estimators approach the identity, is correct as shown in step 4.\n-   The conclusion \"hence the MMSE denoiser has a dead zone...\" is a non-sequitur and is factually incorrect. The MMSE estimator does not have a dead zone for any $\\sigma^2 > 0$.\n-   This statement is **Incorrect**.\n\n**C. For fixed $\\lambda$ and $\\sigma^2$, the MMSE denoiser asymptotically matches soft-thresholding for $|y| \\to \\infty$ in the sense that $f_{\\mathrm{MMSE}}(y) = \\operatorname{sign}(y)\\big(|y| - \\sigma^2 \\lambda\\big) + o(1)$; yet the two differ around $|y| \\approx \\sigma^2 \\lambda$, where the MMSE transition is smooth while soft-thresholding has a kink.**\n-   The asymptotic matching expression was derived in step 3. **Correct.**\n-   The MAP estimator (soft-thresholding) has non-differentiable points (kinks) at $|y|=\\sigma^2\\lambda$. The MMSE estimator, as argued in step 2, is a smooth function. The difference in their behavior near the threshold is accurately described. **Correct.**\n-   This statement is entirely **Correct**.\n\n**D. In the extreme low signal-to-noise ratio regime $\\sigma \\to \\infty$, the MMSE denoiser converges pointwise to $0$ for any finite $y$, whereas soft-thresholding converges to $\\operatorname{sign}(y)\\,|y|$, i.e., it becomes hard-thresholding with infinite threshold.**\n-   The claim that $f_{\\mathrm{MMSE}}(y) \\to 0$ as $\\sigma \\to \\infty$ is correct, as shown in step 4.\n-   The claim that soft-thresholding converges to $\\operatorname{sign}(y)|y|=y$ is incorrect. As shown in step 4, $f_{\\mathrm{MAP}}(y) = \\operatorname{soft}(y; \\sigma^2\\lambda)$ also converges to $0$ as the threshold $\\sigma^2\\lambda \\to \\infty$.\n-   This statement is **Incorrect**.\n\n**E. The slope of the MMSE denoiser at $y=0$ is exactly $1 - \\sigma^2 \\lambda$; therefore, at sufficiently large $\\sigma$ the denoiser is decreasing near $0$, violating monotonicity.**\n-   The claim for the slope $f'_{\\mathrm{MMSE}}(0) = 1 - \\sigma^2\\lambda$ is dimensionally inconsistent. The slope is dimensionless, whereas $\\sigma^2 \\lambda$ has units of length (if $x$ has units of length). The correct slope is $f'_{\\mathrm{MMSE}}(0) = \\mathbb{E}[\\frac{s}{s+\\sigma^2} \\,| \\, y=0]$, which we proved in step 2 is strictly between $0$ and $1$. The premise is false.\n-   The conclusion that monotonicity is violated for large $\\sigma$ is also false. We proved the function is strictly monotonic for all $\\sigma^2>0$.\n-   This statement is **Incorrect**.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Moving from individual coefficients to groups, this exercise tackles structured sparsity through the lens of the group lasso penalty, which itself can be motivated by a hierarchical GSM model. You will derive the core algorithmic building block—the block-soft-thresholding operator—which is the proximal operator essential for solving the group lasso problem with modern optimization methods like the proximal gradient algorithm. This hands-on derivation provides a concrete understanding of how sparsity-inducing priors are operationalized in practical, large-scale computational frameworks. ",
            "id": "3451068",
            "problem": "Consider a linear inverse problem in compressed sensing with grouped structure. The measurement model is $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and the noise $\\varepsilon$ is independent Gaussian with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$. Suppose the coefficient vector $x$ is partitioned into disjoint groups $\\{G_{g}\\}_{g=1}^{G}$, with $x_{G_{g}} \\in \\mathbb{R}^{|G_{g}|}$ denoting the subvector corresponding to the indices in $G_{g}$. Assume a hierarchical sparsity prior constructed via a Gaussian Scale Mixture (GSM): for each group $g$, the conditional prior is $p(x_{G_{g}} \\mid \\tau_{g}) = \\mathcal{N}(0, \\tau_{g} I_{|G_{g}|})$, and the mixing distribution over scales $\\tau_{g}$ is chosen so that the marginal prior over $x_{G_{g}}$ is rotationally invariant and heavy-tailed. Using this GSM construction and Maximum A Posteriori (MAP) estimation under the Gaussian likelihood above, one arrives at a penalty of the form $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$ for some $\\lambda > 0$.\n\nStarting from the foundational definitions of the Gaussian likelihood, the GSM hierarchical prior, and the definition of the proximal operator for a convex penalty, derive the block-soft-thresholding operator that appears as the proximal map of the group-lasso penalty $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$. Then apply one proximal-gradient step for this penalized least-squares objective starting from $x^{(0)} = 0$ with step size $t = 1$ when the design matrix and data are specified as\n$$\nA = I_{3 \\times 3}, \\quad y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix},\n$$\nwith two groups $G_{1} = \\{1,2\\}$ and $G_{2} = \\{3\\}$, and regularization parameter $\\lambda = 2$. Let $z = x^{(0)} + t A^{\\top}(y - A x^{(0)})$ denote the gradient step input to the proximal operator. Use the block-soft-thresholding operator you derived to compute $x^{(1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{2}}(z)$ under the given grouping.\n\nFinally, provide the Euclidean norm $\\|x^{(1)}\\|_{2}$ as your single final answer. No rounding is required.",
            "solution": "The problem asks for the derivation of the block-soft-thresholding operator and its application in a single step of a proximal gradient algorithm for a group-lasso penalized least-squares problem. Finally, we must compute the Euclidean norm of the resulting vector.\n\nThe underlying optimization problem arises from Maximum A Posteriori (MAP) estimation. Given the Gaussian likelihood $p(y|x) \\propto \\exp(-\\frac{1}{2\\sigma^2}\\|y - Ax\\|_2^2)$ and a a hierarchical prior on $x$ that leads to a penalty of the form $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$, the MAP estimate is found by minimizing the negative log-posterior, which is equivalent to solving the group-lasso problem:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2} \\|y - Ax\\|_2^2 + \\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_2 \\right)\n$$\nThis is an objective function of the form $J(x) = f(x) + h(x)$, where $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$ is a smooth convex function and $h(x) = \\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_2$ is a non-smooth convex function. Such problems are efficiently solved using proximal gradient methods. A single step of the proximal gradient algorithm is given by:\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla f(x^{(k)}))\n$$\nwhere $t > 0$ is the step size. The gradient of $f(x)$ is $\\nabla f(x) = A^\\top(Ax - y)$. Thus, the update can be written as:\n$$\nx^{(k+1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{G_g, 2}}(x^{(k)} + t A^\\top(y - Ax^{(k)}))\n$$\nwhere we use the notation $\\|\\cdot\\|_{G_g, 2}$ to mean applying the $\\ell_2$-norm to the subvector $x_{G_g}$. Let $z = x^{(k)} + t A^\\top(y - Ax^{(k)})$. The core of the update is the evaluation of the proximal operator $\\operatorname{prox}_{t h}(z)$.\n\nFirst, we derive the form of this proximal operator, which is the block-soft-thresholding operator.\nThe proximal operator of a function $\\phi(x)$ is defined as:\n$$\n\\operatorname{prox}_{\\phi}(z) = \\arg \\min_{x} \\left( \\phi(x) + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\nIn our case, the function is $\\phi(x) = \\alpha \\sum_{g=1}^{G} \\|x_{G_g}\\|_2$, where we set $\\alpha = t \\lambda$ for notational simplicity. The optimization problem for the proximal operator becomes:\n$$\n\\arg \\min_{x} \\left( \\alpha \\sum_{g=1}^{G} \\|x_{G_g}\\|_2 + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\nSince the groups $\\{G_g\\}$ form a partition of the indices, the squared Euclidean norm term is separable across these groups: $\\|x - z\\|_2^2 = \\sum_{g=1}^{G} \\|x_{G_g} - z_{G_g}\\|_2^2$. This separability allows us to decompose the minimization problem into $G$ independent subproblems, one for each group:\n$$\n\\min_{x} \\sum_{g=1}^{G} \\left( \\alpha \\|x_{G_g}\\|_2 + \\frac{1}{2} \\|x_{G_g} - z_{G_g}\\|_2^2 \\right)\n$$\nThe solution for the full vector $x$ is obtained by concatenating the solutions $x_{G_g}^*$ of the individual subproblems:\n$$\nx_{G_g}^* = \\arg \\min_{u \\in \\mathbb{R}^{|G_g|}} \\left( \\alpha \\|u\\|_2 + \\frac{1}{2} \\|u - v\\|_2^2 \\right)\n$$\nwhere we let $u = x_{G_g}$ and $v = z_{G_g}$.\nTo solve this subproblem, we use subdifferential calculus. The first-order optimality condition is that $0$ must be in the subdifferential of the objective function at the minimum $u^*$:\n$$\n0 \\in \\partial \\left( \\alpha \\|u^*\\|_2 + \\frac{1}{2} \\|u^* - v\\|_2^2 \\right) = \\alpha \\partial(\\|u^*\\|_2) + (u^* - v)\n$$\nThe subdifferential of the $\\ell_2$-norm is given by:\n$$\n\\partial(\\|u\\|_2) = \\begin{cases} \\{ u / \\|u\\|_2 \\} & \\text{if } u \\neq 0 \\\\ \\{ w \\in \\mathbb{R}^{|G_g|} : \\|w\\|_2 \\le 1 \\} & \\text{if } u = 0 \\end{cases}\n$$\nWe analyze two cases for the solution $u^*$:\n\nCase 1: $u^* = 0$.\nThe optimality condition becomes $v \\in \\alpha \\partial(\\|0\\|_2) = \\{w: \\|w\\|_2 \\le \\alpha \\}$. This means if $\\|v\\|_2 \\le \\alpha$, then $u^*=0$ is a valid solution.\n\nCase 2: $u^* \\neq 0$.\nThe optimality condition becomes $0 = \\alpha \\frac{u^*}{\\|u^*\\|_2} + u^* - v$.\nRearranging gives $v = u^* + \\alpha \\frac{u^*}{\\|u^*\\|_2} = u^* \\left(1 + \\frac{\\alpha}{\\|u^*\\|_2}\\right)$.\nThis equation implies that $v$ must be a positive scaling of $u^*$, so $u^*$ and $v$ are collinear and point in the same direction. We can write $u^* = c v$ for some scalar $c > 0$. Taking the norm of both sides of $v = u^*(1 + \\alpha/\\|u^*\\|_2)$, we get $\\|v\\|_2 = \\|u^*\\|_2(1 + \\alpha/\\|u^*\\|_2) = \\|u^*\\|_2 + \\alpha$. Thus, $\\|u^*\\|_2 = \\|v\\|_2 - \\alpha$. Since $\\|u^*\\|_2 > 0$, this case is only possible if $\\|v\\|_2 > \\alpha$.\nSubstituting $\\|u^*\\|_2$ back into the expression for $v$:\n$v = u^* \\left(1 + \\frac{\\alpha}{\\|v\\|_2 - \\alpha}\\right) = u^* \\left(\\frac{\\|v\\|_2 - \\alpha + \\alpha}{\\|v\\|_2 - \\alpha}\\right) = u^* \\frac{\\|v\\|_2}{\\|v\\|_2 - \\alpha}$.\nSolving for $u^*$ gives $u^* = v \\frac{\\|v\\|_2 - \\alpha}{\\|v\\|_2} = \\left(1 - \\frac{\\alpha}{\\|v\\|_2}\\right) v$.\n\nCombining both cases, the solution for the subproblem is:\n$$\nx_{G_g}^* = \\begin{cases} \\left(1 - \\frac{\\alpha}{\\|z_{G_g}\\|_2}\\right) z_{G_g} & \\text{if } \\|z_{G_g}\\|_2 > \\alpha \\\\ 0 & \\text{if } \\|z_{G_g}\\|_2 \\le \\alpha \\end{cases}\n$$\nThis can be written compactly as $x_{G_g}^* = \\left(1 - \\frac{\\alpha}{\\|z_{G_g}\\|_2}\\right)_+ z_{G_g}$, where $(c)_+ = \\max(0, c)$. This is the block-soft-thresholding operator.\n\nNow we apply this to the specific problem. We need to compute $x^{(1)}$ starting from $x^{(0)} = 0$.\nThe update step is $x^{(1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{2}}(z)$, with $z = x^{(0)} + t A^{\\top}(y - A x^{(0)})$.\nThe given parameters are:\n$$\nA = I_{3 \\times 3}, \\quad y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix}, \\quad x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad t = 1, \\quad \\lambda = 2\n$$\nFirst, we compute the argument $z$ of the proximal operator:\n$$\nz = x^{(0)} + t A^{\\top}(y - A x^{(0)}) = 0 + 1 \\cdot I_{3 \\times 3}^\\top(y - I_{3 \\times 3} \\cdot 0) = y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix}\n$$\nThe parameter for block-soft-thresholding is $\\alpha = t \\lambda = 1 \\cdot 2 = 2$.\nThe groups are $G_1 = \\{1,2\\}$ and $G_2 = \\{3\\}$. We partition $z$ accordingly:\n$$\nz_{G_1} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, \\quad z_{G_2} = \\begin{pmatrix} 0.5 \\end{pmatrix}\n$$\nNow we apply the derived operator to each group subvector.\n\nFor group $G_1$:\nFirst, compute the $\\ell_2$-norm:\n$$\n\\|z_{G_1}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n$$\nWe compare this norm to $\\alpha$. Since $\\|z_{G_1}\\|_2 = 5 > \\alpha = 2$, we are in the \"shrink\" case:\n$$\nx_{G_1}^{(1)} = \\left(1 - \\frac{\\alpha}{\\|z_{G_1}\\|_2}\\right) z_{G_1} = \\left(1 - \\frac{2}{5}\\right) \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\frac{3}{5} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 9/5 \\\\ 12/5 \\end{pmatrix} = \\begin{pmatrix} 1.8 \\\\ 2.4 \\end{pmatrix}\n$$\n\nFor group $G_2$:\nFirst, compute the $\\ell_2$-norm:\n$$\n\\|z_{G_2}\\|_2 = |0.5| = 0.5\n$$\nWe compare this norm to $\\alpha$. Since $\\|z_{G_2}\\|_2 = 0.5 \\le \\alpha = 2$, we are in the \"threshold\" case:\n$$\nx_{G_2}^{(1)} = 0\n$$\n\nCombining the results for the two groups, we get the updated vector $x^{(1)}$:\n$$\nx^{(1)} = \\begin{pmatrix} x_{G_1}^{(1)} \\\\ x_{G_2}^{(1)} \\end{pmatrix} = \\begin{pmatrix} 1.8 \\\\ 2.4 \\\\ 0 \\end{pmatrix}\n$$\n\nFinally, the problem asks for the Euclidean norm of this resulting vector, $\\|x^{(1)}\\|_2$.\n$$\n\\|x^{(1)}\\|_2 = \\sqrt{(1.8)^2 + (2.4)^2 + 0^2} = \\sqrt{\\left(\\frac{9}{5}\\right)^2 + \\left(\\frac{12}{5}\\right)^2 + 0}\n$$\n$$\n\\|x^{(1)}\\|_2 = \\sqrt{\\frac{81}{25} + \\frac{144}{25}} = \\sqrt{\\frac{81 + 144}{25}} = \\sqrt{\\frac{225}{25}} = \\sqrt{9} = 3\n$$\nAlternatively, since $x_{G_2}^{(1)}=0$, the norm is just the norm of the first block component:\n$$\n\\|x^{(1)}\\|_2 = \\|x_{G_1}^{(1)}\\|_2 = \\left\\| \\frac{3}{5} z_{G_1} \\right\\|_2 = \\frac{3}{5} \\|z_{G_1}\\|_2 = \\frac{3}{5} \\cdot 5 = 3\n$$\nThe Euclidean norm of $x^{(1)}$ is $3$.",
            "answer": "$$\\boxed{3}$$"
        }
    ]
}