{
    "hands_on_practices": [
        {
            "introduction": "The foundation of inference in structured models is understanding how to leverage the model's structure for efficient computation. This exercise provides a core \"pen-and-paper\" demonstration of Maximum A Posteriori (MAP) estimation on a one-dimensional chain, a scenario where exact inference is possible using dynamic programming. By working through the Viterbi algorithm, you will gain direct insight into how the Markov Random Field (MRF) prior balances local, data-driven evidence ($\\theta_i$) against a penalty for breaking structural contiguity, revealing why this approach can produce more coherent solutions than simple independent thresholding .",
            "id": "3480135",
            "problem": "Consider a compressed sensing model with a Spike-and-Slab prior over coefficients. Let $y \\in \\mathbb{R}^{n}$ be measurements obeying $y = A x + \\epsilon$, where $A \\in \\mathbb{R}^{n \\times p}$ is a known sensing matrix, $x \\in \\mathbb{R}^{p}$ are unknown coefficients, and $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$ is Gaussian noise. Introduce binary latent variables $z_{i} \\in \\{0,1\\}$ encoding sparsity via the Spike-and-Slab prior $p(x_{i} \\mid z_{i}) = (1 - z_{i}) \\delta(x_{i}) + z_{i} \\,\\mathcal{N}(x_{i} \\mid 0, \\tau^{2})$. Assume a one-dimensional chain Markov Random Field (MRF) prior on $z_{1},\\dots,z_{p}$ with pairwise potential encouraging contiguity,\n$$\n\\phi(z_{i}, z_{i+1}) \\triangleq w \\, |z_{i} - z_{i+1}|,\\quad w > 0,\n$$\nand unary potentials $\\psi_{i}(z_{i})$ derived from the local evidence of $y$ under the likelihood and prior. Up to an additive constant independent of $z$, the posterior energy takes the form\n$$\nE(z_{1},\\dots,z_{p}) \\triangleq \\sum_{i=1}^{p} \\theta_{i} z_{i} \\;+\\; w \\sum_{i=1}^{p-1} |z_{i} - z_{i+1}|,\n$$\nwhere $\\theta_{i} \\in \\mathbb{R}$ are fixed unary costs summarizing local evidence (e.g., negative log Bayes factors favoring $z_{i}=1$ over $z_{i}=0$).\n\nFor a one-dimensional chain of length $p=5$ with homogeneous pairwise weight $w=1$ and unary costs\n$$\n\\theta_{1} = -0.6,\\quad \\theta_{2} = -0.4,\\quad \\theta_{3} = 0.2,\\quad \\theta_{4} = -0.5,\\quad \\theta_{5} = -0.3,\n$$\nperform the following:\n\n1. Derive, from first principles of dynamic programming (DP) on a chain Markov Random Field (MRF), a recurrence that computes the minimum energy to index $i$ when $z_{i}$ is fixed to $0$ or $1$. Use this to compute the Maximum A Posteriori (MAP) configuration $z^{\\star} \\in \\{0,1\\}^{5}$ that minimizes $E(z)$, and show the backtracking steps that recover $z^{\\star}$.\n\n2. Define independent thresholding that ignores pairwise interactions by minimizing $\\sum_{i=1}^{5} \\theta_{i} z_{i}$ site-wise. Compute the thresholded configuration $z^{\\mathrm{thr}}$ and compare its energy $E(z^{\\mathrm{thr}})$ to $E(z^{\\star})$, explaining any differences in terms of the contiguity penalty.\n\nExpress your final MAP configuration as a binary row matrix with five entries using LaTeX. No rounding is needed.",
            "solution": "The problem is valid. It presents a well-posed MAP inference task on a chain Markov Random Field, a standard problem in statistical signal processing and machine learning. All parameters are provided, and the objective is clearly defined. We will proceed with the solution in two parts as requested.\n\nThe energy function to be minimized is given by:\n$$\nE(z_{1},\\dots,z_{p}) = \\sum_{i=1}^{p} \\theta_{i} z_{i} + w \\sum_{i=1}^{p-1} |z_{i} - z_{i+1}|\n$$\nWith the given parameters $p=5$, $w=1$, and $\\theta = (-0.6, -0.4, 0.2, -0.5, -0.3)$, the energy for a configuration $z = (z_1, z_2, z_3, z_4, z_5)$ is:\n$$\nE(z) = \\sum_{i=1}^{5} \\theta_{i} z_{i} + \\sum_{i=1}^{4} |z_{i} - z_{i+1}|\n$$\nThe pairwise cost $|z_i - z_{i+1}|$ is $1$ if $z_i \\neq z_{i+1}$ (a change in state) and $0$ if $z_i = z_{i+1}$ (state remains the same).\n\n### Part 1: Dynamic Programming for MAP Inference\n\nWe seek the Maximum A Posteriori (MAP) configuration $z^{\\star} = \\arg\\min_{z \\in \\{0,1\\}^5} E(z)$. Since the underlying graphical model is a chain, this can be solved efficiently using dynamic programming (specifically, the min-sum or Viterbi algorithm).\n\nLet $M_i(k)$ be the minimum energy of a partial configuration $(z_1, \\dots, z_i)$ ending with $z_i = k$, where $k \\in \\{0, 1\\}$. The total energy is additive, so we can establish a recurrence. The energy up to site $i$ with $z_i=k$ is the unary cost at $i$ ($\\theta_i k$) plus the minimum possible energy of a path ending at $i-1$ plus the transition cost from $z_{i-1}$ to $z_i$.\n\nThe recurrence relation is:\n$$\nM_i(k) = \\theta_i k + \\min_{j \\in \\{0,1\\}} \\left( M_{i-1}(j) + w|k-j| \\right)\n$$\nSince $w=1$, we can write the recurrences for $k=0$ and $k=1$ explicitly:\n$$\nM_i(0) = \\theta_i \\cdot 0 + \\min \\left( M_{i-1}(0) + |0-0|, M_{i-1}(1) + |0-1| \\right) = \\min(M_{i-1}(0), M_{i-1}(1) + 1)\n$$\n$$\nM_i(1) = \\theta_i \\cdot 1 + \\min \\left( M_{i-1}(0) + |1-0|, M_{i-1}(1) + |1-1| \\right) = \\theta_i + \\min(M_{i-1}(0) + 1, M_{i-1}(1))\n$$\nTo recover the optimal path, we also store backpointers $\\psi_i(k)$ which record the state $j \\in \\{0,1\\}$ at step $i-1$ that yielded the minimum cost for $M_i(k)$:\n$$\n\\psi_i(k) = \\arg\\min_{j \\in \\{0,1\\}} \\left( M_{i-1}(j) + w|k-j| \\right)\n$$\n\n**Forward Pass:**\n\n**Initialization ($i=1$):**\nThe energy of a path of length $1$ is just the unary cost.\n- $M_1(0) = \\theta_1 \\cdot 0 = 0$\n- $M_1(1) = \\theta_1 \\cdot 1 = -0.6$\n\n**Step $i=2$ ($\\theta_2 = -0.4$):**\n- $M_2(0) = \\min(M_1(0), M_1(1) + 1) = \\min(0, -0.6 + 1) = \\min(0, 0.4) = 0$. The minimum came from $M_1(0)$, so $\\psi_2(0) = 0$.\n- $M_2(1) = \\theta_2 + \\min(M_1(0) + 1, M_1(1)) = -0.4 + \\min(0+1, -0.6) = -0.4 - 0.6 = -1.0$. The minimum came from $M_1(1)$, so $\\psi_2(1) = 1$.\n\n**Step $i=3$ ($\\theta_3 = 0.2$):**\n- $M_3(0) = \\min(M_2(0), M_2(1) + 1) = \\min(0, -1.0 + 1) = \\min(0, 0) = 0$. The minimum is achieved by both paths. We can choose $\\psi_3(0) = 0$ by convention.\n- $M_3(1) = \\theta_3 + \\min(M_2(0) + 1, M_2(1)) = 0.2 + \\min(0+1, -1.0) = 0.2 - 1.0 = -0.8$. The minimum came from $M_2(1)$, so $\\psi_3(1) = 1$.\n\n**Step $i=4$ ($\\theta_4 = -0.5$):**\n- $M_4(0) = \\min(M_3(0), M_3(1) + 1) = \\min(0, -0.8 + 1) = \\min(0, 0.2) = 0$. The minimum came from $M_3(0)$, so $\\psi_4(0) = 0$.\n- $M_4(1) = \\theta_4 + \\min(M_3(0) + 1, M_3(1)) = -0.5 + \\min(0+1, -0.8) = -0.5 - 0.8 = -1.3$. The minimum came from $M_3(1)$, so $\\psi_4(1) = 1$.\n\n**Step $i=5$ ($\\theta_5 = -0.3$):**\n- $M_5(0) = \\min(M_4(0), M_4(1) + 1) = \\min(0, -1.3 + 1) = \\min(0, -0.3) = -0.3$. The minimum came from $M_4(1)$, so $\\psi_5(0) = 1$.\n- $M_5(1) = \\theta_5 + \\min(M_4(0) + 1, M_4(1)) = -0.3 + \\min(0+1, -1.3) = -0.3 - 1.3 = -1.6$. The minimum came from $M_4(1)$, so $\\psi_5(1) = 1$.\n\n**Backtracking:**\n\nThe minimum total energy for the entire chain is $E(z^{\\star}) = \\min(M_5(0), M_5(1)) = \\min(-0.3, -1.6) = -1.6$.\nThe optimal state at the final position is $z^{\\star}_5 = \\arg\\min_{k \\in \\{0,1\\}} M_5(k) = 1$.\n\nNow we trace back using the stored pointers $\\psi$ to find the optimal path:\n- $z^{\\star}_5 = 1$\n- $z^{\\star}_4 = \\psi_5(z^{\\star}_5) = \\psi_5(1) = 1$\n- $z^{\\star}_3 = \\psi_4(z^{\\star}_4) = \\psi_4(1) = 1$\n- $z^{\\star}_2 = \\psi_3(z^{\\star}_3) = \\psi_3(1) = 1$\n- $z^{\\star}_1 = \\psi_2(z^{\\star}_2) = \\psi_2(1) = 1$\n\nThus, the MAP configuration is $z^{\\star} = (1, 1, 1, 1, 1)$.\n\n### Part 2: Independent Thresholding and Comparison\n\nIndependent thresholding ignores the pairwise interaction term $w \\sum |z_i - z_{i+1}|$ and minimizes only the sum of unary terms, $E_{\\mathrm{thr}}(z) = \\sum_{i=1}^5 \\theta_i z_i$. This sum is an uncoupled optimization, so we can minimize each term $\\theta_i z_i$ independently. For each site $i$, the optimal choice $z^{\\mathrm{thr}}_i$ is:\n$$\nz^{\\mathrm{thr}}_i =\n\\begin{cases}\n1  \\text{if } \\theta_i  0 \\\\\n0  \\text{if } \\theta_i \\ge 0\n\\end{cases}\n$$\nApplying this rule to the given unary costs:\n- $\\theta_1 = -0.6  0 \\implies z^{\\mathrm{thr}}_1 = 1$\n- $\\theta_2 = -0.4  0 \\implies z^{\\mathrm{thr}}_2 = 1$\n- $\\theta_3 = 0.2 > 0 \\implies z^{\\mathrm{thr}}_3 = 0$\n- $\\theta_4 = -0.5  0 \\implies z^{\\mathrm{thr}}_4 = 1$\n- $\\theta_5 = -0.3  0 \\implies z^{\\mathrm{thr}}_5 = 1$\n\nThe thresholded configuration is $z^{\\mathrm{thr}} = (1, 1, 0, 1, 1)$.\n\nNow, we compute the energy of this configuration using the *full* energy function $E(z)$:\n$$\nE(z^{\\mathrm{thr}}) = \\sum_{i=1}^{5} \\theta_{i} z^{\\mathrm{thr}}_{i} + \\sum_{i=1}^{4} |z^{\\mathrm{thr}}_{i} - z^{\\mathrm{thr}}_{i+1}|\n$$\nThe unary term contribution is:\n$$\n\\sum \\theta_i z^{\\mathrm{thr}}_i = (-0.6)\\cdot 1 + (-0.4)\\cdot 1 + (0.2)\\cdot 0 + (-0.5)\\cdot 1 + (-0.3)\\cdot 1 = -0.6 - 0.4 - 0.5 - 0.3 = -1.8\n$$\nThe pairwise term contribution (contiguity penalty) is:\n$$\n\\sum |z^{\\mathrm{thr}}_i - z^{\\mathrm{thr}}_{i+1}| = |1-1| + |1-0| + |0-1| + |1-1| = 0 + 1 + 1 + 0 = 2\n$$\nThe total energy is $E(z^{\\mathrm{thr}}) = -1.8 + 2 = 0.2$.\n\n**Comparison and Explanation:**\n- MAP energy: $E(z^{\\star}) = E(1,1,1,1,1) = (-0.6 - 0.4 + 0.2 - 0.5 - 0.3) + 0 = -1.6$.\n- Thresholding energy: $E(z^{\\mathrm{thr}}) = E(1,1,0,1,1) = 0.2$.\n\nAs expected, $E(z^{\\star})  E(z^{\\mathrm{thr}})$ since $z^{\\star}$ is the global minimizer. The independent thresholding solution $z^{\\mathrm{thr}}$ achieves a lower unary energy ($-1.8$) compared to the MAP solution's unary energy ($-1.6$). However, $z^{\\mathrm{thr}}$ pays a significant penalty ($2$) from the pairwise term for breaking contiguity at index $i=3$. The MAP solution $z^{\\star}$ pays a small price in the unary term (by choosing $z_3=1$ despite $\\theta_3=0.2 > 0$) in order to achieve a zero pairwise penalty. The trade-off is clear at $i=3$: setting $z_3=0$ (with neighbors being $1$) adds $(\\theta_3\\cdot 0) + |z_2-z_3| + |z_3-z_4| = 0 + |1-0|+|0-1| = 2$ to the energy, while setting $z_3=1$ adds $(\\theta_3\\cdot 1) + |z_2-z_3|+|z_3-z_4| = 0.2 + |1-1|+|1-1| = 0.2$. Since $0.2  2$, the MAP solution correctly chooses $z_3=1$, demonstrating how the pairwise potential enforces structural preferences (contiguity) over local, greedy decisions.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1  1  1  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While exact inference is feasible for chains, most real-world graphical models, such as those on 2D grids, contain cycles, rendering exact methods computationally intractable. This practice transitions us to the realm of approximate inference, a cornerstone of modern machine learning. You will implement and compare two of the most fundamental algorithms: mean-field variational inference, which uses a fully factorized approximation, and the Bethe approximation, implemented via loopy belief propagation, which offers a more refined estimate by capturing local dependencies. This hands-on coding exercise will illuminate the practical trade-offs between algorithmic complexity and the accuracy of posterior marginal estimates .",
            "id": "3480139",
            "problem": "Consider a binary latent field $\\{z_i\\}_{i=1}^n$ on a two-dimensional grid graph with four-neighbor connectivity, where each $z_i \\in \\{0,1\\}$. The prior over $\\boldsymbol{z} = (z_1,\\dots,z_n)$ is an Ising-type Markov Random Field (MRF) with pairwise interactions and a local bias. The prior takes the form\n$$\np(\\boldsymbol{z}) \\propto \\exp\\left( \\sum_{i=1}^n b_i z_i + \\sum_{(i,j)\\in E} w\\, z_i z_j \\right),\n$$\nwhere $E$ denotes the set of undirected edges corresponding to nearest-neighbor pairs on the grid, $w \\in \\mathbb{R}$ is a uniform interaction strength, and $b_i \\in \\mathbb{R}$ is a local bias obtained from a global sparsity parameter $\\rho \\in (0,1)$ via $b_i = \\log\\left(\\frac{\\rho}{1-\\rho}\\right)$.\n\nThe spike-and-slab model links $\\boldsymbol{z}$, $\\boldsymbol{x}$, and observations $\\boldsymbol{y}$ through the following components:\n- Spike-and-slab prior on coefficients: for each index $i$, the conditional distribution of $x_i$ given $z_i$ is\n$$\nx_i \\mid z_i \\sim \n\\begin{cases}\n\\delta_0  \\text{if } z_i = 0, \\\\\n\\mathcal{N}(0,\\tau^2)  \\text{if } z_i = 1,\n\\end{cases}\n$$\nwhere $\\delta_0$ denotes a point mass at $0$, and $\\tau^2  0$.\n- Gaussian observation model: for each index $i$,\n$$\ny_i \\mid x_i \\sim \\mathcal{N}(x_i,\\sigma^2),\n$$\nwith $\\sigma^2  0$.\n\nThe observations are conditionally independent across indices given $\\boldsymbol{x}$. The two-dimensional grids are specified in row-major order so that index $i$ corresponds to position $(r,c)$ with $i = r \\cdot C + c + 1$, where $C$ is the number of columns and $r,c$ are zero-based row and column indices; neighbors are up to four cardinal directions that remain within bounds.\n\nTask:\n1. Starting from the definition of the variational mean-field family $q(\\boldsymbol{z}) = \\prod_{i=1}^n \\text{Bernoulli}(\\pi_i)$, derive fixed-point equations that a stationary point of the Evidence Lower Bound (ELBO) must satisfy for each $\\pi_i$, explicitly accounting for the Ising prior and the marginalized local likelihood from the spike-and-slab and Gaussian observation model. Your derivation must be from first principles of variational inference (optimization of the ELBO) and properties of exponential family distributions.\n2. Define the Bethe approximation via loopy sum-product belief propagation on the binary pairwise model obtained after analytically marginalizing $\\boldsymbol{x}$. Derive the message update equations for the directed messages $m_{i\\to j}(z_j)$ and the node beliefs $b_i(z_i)$.\n3. Implement a program that:\n   - Constructs the grid graph neighbors for each test case.\n   - Computes the mean-field fixed point for $\\boldsymbol{\\pi}$ by iterating the fixed-point equations with damping until convergence to a prescribed tolerance.\n   - Computes Bethe node beliefs $b_i(1)$ via loopy sum-product with damping until convergence to a prescribed tolerance.\n   - Outputs, for each test case, the mean absolute difference between the mean-field probabilities $\\pi_i$ and the Bethe beliefs $b_i(1)$, aggregated over all nodes as a single scalar per test case.\n\nYou must use the following test suite with explicit parameter values:\n- Test case $1$ (happy path):\n  - Grid size: $2 \\times 2$.\n  - Interaction strength: $w = 0.6$.\n  - Sparsity parameter: $\\rho = 0.3$ (so $b_i = \\log(\\rho/(1-\\rho))$).\n  - Slab variance: $\\tau^2 = 1.0$.\n  - Observation noise variance: $\\sigma^2 = 0.25$.\n  - Observations: $\\boldsymbol{y} = (1.2, -0.3, 0.7, 2.0)$ in row-major order.\n- Test case $2$ (boundary condition: no coupling):\n  - Grid size: $3 \\times 3$.\n  - Interaction strength: $w = 0.0$.\n  - Sparsity parameter: $\\rho = 0.2$ (so $b_i = \\log(\\rho/(1-\\rho))$).\n  - Slab variance: $\\tau^2 = 1.5$.\n  - Observation noise variance: $\\sigma^2 = 0.16$.\n  - Observations: $\\boldsymbol{y} = (-0.1, 0.2, 0.0, 0.5, -1.0, 0.3, 0.7, -0.4, 0.1)$ in row-major order.\n- Test case $3$ (edge case: strong coupling and high noise):\n  - Grid size: $3 \\times 3$.\n  - Interaction strength: $w = 1.2$.\n  - Sparsity parameter: $\\rho = 0.5$ (so $b_i = 0$).\n  - Slab variance: $\\tau^2 = 0.5$.\n  - Observation noise variance: $\\sigma^2 = 1.0$.\n  - Observations: $\\boldsymbol{y} = (0.0, 0.5, -0.5, 1.5, -1.2, 0.2, -0.8, 0.9, -0.1)$ in row-major order.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$, $2$, and $3$, for example, $[r_1,r_2,r_3]$, where each $r_k$ is a real number (a float) equal to the mean absolute difference between the two sets of node marginals for the $k$-th test case.",
            "solution": "The problem requires the derivation of variational inference update equations for a hierarchical Bayesian model and their implementation for numerical comparison. The model combines a spike-and-slab prior with an Ising-type Markov Random Field (MRF) prior on latent binary variables indicating sparsity. We will perform inference for these latent variables using two common approximate inference methods: mean-field variational inference and the Bethe approximation via loopy belief propagation.\n\n### 1. Model Specification and Marginalization\n\nThe full probabilistic model is defined over observations $\\boldsymbol{y}$, sparse coefficients $\\boldsymbol{x}$, and binary latent variables $\\boldsymbol{z}$. The joint probability is $p(\\boldsymbol{y}, \\boldsymbol{x}, \\boldsymbol{z}) = p(\\boldsymbol{y} \\mid \\boldsymbol{x}) p(\\boldsymbol{x} \\mid \\boldsymbol{z}) p(\\boldsymbol{z})$. Our goal is to infer the posterior over the latent variables, $p(\\boldsymbol{z} \\mid \\boldsymbol{y})$. To do this, we first marginalize out the continuous variables $\\boldsymbol{x}$.\n\nGiven the conditional independence assumptions, the marginal likelihood of the observations given the latent variables, $p(\\boldsymbol{y} \\mid \\boldsymbol{z})$, factorizes over the indices $i$:\n$$\np(\\boldsymbol{y} \\mid \\boldsymbol{z}) = \\prod_{i=1}^n p(y_i \\mid z_i)\n$$\nwhere each term $p(y_i \\mid z_i)$ is obtained by integrating over $x_i$:\n$$\np(y_i \\mid z_i) = \\int p(y_i \\mid x_i) p(x_i \\mid z_i) \\,dx_i.\n$$\nWe evaluate this integral for the two cases of $z_i$:\n- If $z_i=0$, the prior on $x_i$ is a Dirac delta function $p(x_i \\mid z_i=0) = \\delta_0(x_i)$. The integral becomes an evaluation at $x_i=0$:\n$$\np(y_i \\mid z_i=0) = p(y_i \\mid x_i=0) = \\mathcal{N}(y_i; 0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{y_i^2}{2\\sigma^2}\\right).\n$$\n- If $z_i=1$, the prior on $x_i$ is Gaussian, $p(x_i \\mid z_i=1) = \\mathcal{N}(x_i; 0, \\tau^2)$. The observation model is $p(y_i \\mid x_i) = \\mathcal{N}(y_i; x_i, \\sigma^2)$. The marginal $p(y_i \\mid z_i=1)$ is the result of a convolution of two Gaussian distributions, which is another Gaussian distribution whose mean is the sum of the means ($0+0=0$) and whose variance is the sum of the variances ($\\tau^2+\\sigma^2$):\n$$\np(y_i \\mid z_i=1) = \\mathcal{N}(y_i; 0, \\tau^2+\\sigma^2) = \\frac{1}{\\sqrt{2\\pi(\\tau^2+\\sigma^2)}} \\exp\\left(-\\frac{y_i^2}{2(\\tau^2+\\sigma^2)}\\right).\n$$\nThe full posterior over $\\boldsymbol{z}$ is then proportional to the product of the marginal likelihood and the prior on $\\boldsymbol{z}$:\n$$\np(\\boldsymbol{z} \\mid \\boldsymbol{y}) \\propto p(\\boldsymbol{y} \\mid \\boldsymbol{z}) p(\\boldsymbol{z}) = \\left(\\prod_{i=1}^n p(y_i \\mid z_i)\\right) \\exp\\left( \\sum_{i=1}^n b_i z_i + \\sum_{(i,j)\\in E} w\\, z_i z_j \\right).\n$$\n\n### 2. Mean-Field Variational Inference\n\nIn mean-field variational inference, we approximate the true posterior $p(\\boldsymbol{z} \\mid \\boldsymbol{y})$ with a fully factorized distribution $q(\\boldsymbol{z}) = \\prod_{i=1}^n q_i(z_i)$. For a binary variable $z_i$, we parameterize $q_i(z_i)$ as a Bernoulli distribution: $q_i(z_i) = \\text{Bernoulli}(z_i; \\pi_i) = \\pi_i^{z_i} (1-\\pi_i)^{1-z_i}$, where $\\pi_i = \\mathbb{E}_q[z_i]$.\n\nThe optimal distribution $q_k^*(z_k)$ for a single variable $z_k$ that maximizes the Evidence Lower Bound (ELBO) is given by the coordinate-ascent update rule:\n$$\n\\log q_k^*(z_k) = \\mathbb{E}_{q_{\\setminus k}}[\\log p(\\boldsymbol{z}, \\boldsymbol{y})] + \\text{constant},\n$$\nwhere $q_{\\setminus k} = \\prod_{j \\neq k} q_j(z_j)$ and $\\log p(\\boldsymbol{z}, \\boldsymbol{y}) = \\log p(\\boldsymbol{z} \\mid \\boldsymbol{y}) + \\log p(\\boldsymbol{y})$.\nWe have:\n$$\n\\log p(\\boldsymbol{z}, \\boldsymbol{y}) = \\sum_{i=1}^n \\log p(y_i \\mid z_i) + \\sum_{i=1}^n b_i z_i + \\sum_{(i,j)\\in E} w\\, z_i z_j + \\text{constant}.\n$$\nTaking the expectation with respect to $q_{\\setminus k}$ and isolating terms involving $z_k$:\n$$\n\\log q_k^*(z_k) \\propto \\log p(y_k \\mid z_k) + b_k z_k + \\mathbb{E}_{q_{\\setminus k}}\\left[\\sum_{j \\in \\mathcal{N}(k)} w z_k z_j\\right],\n$$\nwhere $\\mathcal{N}(k)$ is the set of neighbors of node $k$.\n$$\n\\log q_k^*(z_k) \\propto \\log p(y_k \\mid z_k) + b_k z_k + z_k \\sum_{j \\in \\mathcal{N}(k)} w \\, \\mathbb{E}_{q_j}[z_j].\n$$\nSince $\\mathbb{E}_{q_j}[z_j] = \\pi_j$, we get:\n$$\n\\log q_k^*(z_k) \\propto \\log p(y_k \\mid z_k) + z_k \\left( b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\pi_j \\right).\n$$\nThe distribution $q_k^*$ is Bernoulli. The log-odds of $\\pi_k = q_k^*(1)$ is:\n$$\n\\text{logit}(\\pi_k) = \\log\\left(\\frac{\\pi_k}{1-\\pi_k}\\right) = \\log\\left(\\frac{q_k^*(1)}{q_k^*(0)}\\right).\n$$\nThis is obtained by taking the difference of the expression for $\\log q_k^*(z_k)$ evaluated at $z_k=1$ and $z_k=0$:\n$$\n\\text{logit}(\\pi_k) = \\left( \\log p(y_k \\mid z_k=1) + b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\pi_j \\right) - \\left( \\log p(y_k \\mid z_k=0) \\right).\n$$\nLet's define the log-likelihood ratio term $\\Delta L_k$:\n$$\n\\Delta L_k = \\log p(y_k \\mid z_k=1) - \\log p(y_k \\mid z_k=0).\n$$\nSubstituting the Gaussian densities:\n$$\n\\Delta L_k = \\left(-\\frac{1}{2} \\log(2\\pi(\\tau^2+\\sigma^2)) - \\frac{y_k^2}{2(\\tau^2+\\sigma^2)}\\right) - \\left(-\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{y_k^2}{2\\sigma^2}\\right)\n$$\n$$\n\\Delta L_k = -\\frac{1}{2} \\log\\left(\\frac{\\tau^2+\\sigma^2}{\\sigma^2}\\right) + \\frac{y_k^2}{2}\\left(\\frac{1}{\\sigma^2} - \\frac{1}{\\tau^2+\\sigma^2}\\right) = -\\frac{1}{2}\\log\\left(1+\\frac{\\tau^2}{\\sigma^2}\\right) + \\frac{y_k^2 \\tau^2}{2\\sigma^2(\\tau^2+\\sigma^2)}.\n$$\nThe fixed-point equation for $\\pi_k$ is then:\n$$\n\\pi_k = \\text{sigmoid}\\left( b_k + w \\sum_{j \\in \\mathcal{N}(k)} \\pi_j + \\Delta L_k \\right),\n$$\nwhere $b_k = \\log(\\rho/(1-\\rho))$. These equations are iterated for all $k$ until convergence.\n\n### 3. Bethe Approximation (Loopy Belief Propagation)\n\nThe Bethe approximation provides a more accurate estimate of marginals than mean-field by accounting for pairwise correlations. It can be implemented using the sum-product algorithm (belief propagation), even on graphs with cycles (loopy BP). The algorithm works by passing messages between neighboring nodes.\n\nThe posterior $p(\\boldsymbol{z} \\mid \\boldsymbol{y})$ can be expressed as a product of node and edge potentials:\n$$\np(\\boldsymbol{z} \\mid \\boldsymbol{y}) \\propto \\prod_{i=1}^n \\phi_i(z_i) \\prod_{(i,j)\\in E} \\psi_{ij}(z_i, z_j),\n$$\nwhere the node potential $\\phi_i(z_i)$ incorporates the local evidence and bias:\n$$\n\\phi_i(z_i) = p(y_i \\mid z_i) \\exp(b_i z_i),\n$$\nand the edge potential $\\psi_{ij}(z_i, z_j)$ captures the interaction:\n$$\n\\psi_{ij}(z_i, z_j) = \\exp(w z_i z_j).\n$$\nA message $m_{i \\to j}(z_j)$ is sent from node $i$ to its neighbor $j$. It represents the belief that $i$ has about the state of $j$. The update rule is:\n$$\nm_{i \\to j}(z_j) \\propto \\sum_{z_i \\in \\{0,1\\}} \\phi_i(z_i) \\psi_{ij}(z_i, z_j) \\prod_{k \\in \\mathcal{N}(i) \\setminus \\{j\\}} m_{k \\to i}(z_i).\n$$\nFor binary variables, it is numerically more stable and efficient to work with log-ratio messages, $\\mu_{i \\to j} = \\log\\frac{m_{i \\to j}(1)}{m_{i \\to j}(0)}$.\nThe update for $\\mu_{i \\to j}$ is derived as:\n$$\n\\mu_{i \\to j} = \\log\\left( \\frac{\\sum_{z_i} \\phi_i(z_i) \\psi_{ij}(z_i, 1) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i)}{\\sum_{z_i} \\phi_i(z_i) \\psi_{ij}(z_i, 0) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i)} \\right).\n$$\nLet's define the log-ratio of the node potential term as $\\lambda_i = \\log(\\phi_i(1)/\\phi_i(0)) = \\Delta L_i + b_i$.\nThe sum of incoming log-ratio messages to node $i$ from its neighbors except $j$ is $\\sum_{k \\in \\mathcal{N}(i) \\setminus j} \\mu_{k \\to i}$.\nLet $S_{i \\setminus j} = \\lambda_i + \\sum_{k \\in \\mathcal{N}(i) \\setminus j} \\mu_{k \\to i}$. This represents the log-odds of $z_i=1$ based on local evidence and incoming messages from neighbors other than $j$.\nThe sum in the numerator of the $\\mu_{i \\to j}$ update becomes:\n$$\n\\sum_{z_i \\in \\{0,1\\}} \\exp(w z_i) \\left( \\phi_i(z_i) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i) \\right) \\propto 1 \\cdot \\exp(S_{i \\setminus j} \\cdot 0) + \\exp(w) \\exp(S_{i \\setminus j} \\cdot 1) = 1 + \\exp(w+S_{i \\setminus j}).\n$$\nThe sum in the denominator, where $\\psi_{ij}(z_i, 0)=1$, becomes:\n$$\n\\sum_{z_i \\in \\{0,1\\}} 1 \\cdot \\left( \\phi_i(z_i) \\prod_{k \\in \\mathcal{N}(i) \\setminus j} m_{k \\to i}(z_i) \\right) \\propto 1 + \\exp(S_{i \\setminus j}).\n$$\nThus, the message update equation in log-ratio form is:\n$$\n\\mu_{i \\to j} = \\log\\left(\\frac{1 + \\exp(w+S_{i \\setminus j})}{1 + \\exp(S_{i \\setminus j})}\\right) = \\text{logaddexp}(0, w+S_{i \\setminus j}) - \\text{logaddexp}(0, S_{i \\setminus j}).\n$$\nThese message updates are iterated for all directed edges until convergence.\n\nOnce the messages have converged, the (unnormalized) marginal belief at each node $i$, $b_i(z_i)$, is computed by multiplying the node potential with all incoming messages:\n$$\nb_i(z_i) \\propto \\phi_i(z_i) \\prod_{k \\in \\mathcal{N}(i)} m_{k \\to i}(z_i).\n$$\nThe log-odds of the belief is the sum of the log-ratios of the potentials and messages:\n$$\n\\text{logit}(b_i(1)) = \\log\\left(\\frac{b_i(1)}{b_i(0)}\\right) = \\lambda_i + \\sum_{k \\in \\mathcal{N}(i)} \\mu_{k \\to i}.\n$$\nThe final belief probability is:\n$$\nb_i(1) = \\text{sigmoid}\\left( \\lambda_i + \\sum_{k \\in \\mathcal{N}(i)} \\mu_{k \\to i} \\right).\n$$\n\n### 4. Algorithmic Summary\n\nBoth methods are implemented as iterative algorithms.\n- **Mean-Field**: Initialize $\\pi_i$ for all $i$ (e.g., to $0.5$). Repeatedly update all $\\pi_i$ using the derived fixed-point equation, applying damping to aid convergence, until the maximum change in any $\\pi_i$ falls below a tolerance.\n- **Loopy BP**: Initialize all messages $\\mu_{i \\to j}$ to $0$. Repeatedly update all messages using the derived fixed-point equation, applying damping, until the maximum change in any message falls below a tolerance. Finally, compute the beliefs $b_i(1)$ using the converged messages.\n\nFor numerical stability, the `sigmoid` function should handle large arguments, and `logaddexp` should be used for the LBP update. For both algorithms, a damping factor $\\alpha \\in (0,1)$ is used: $x_{\\text{new}} = (1-\\alpha) x_{\\text{old}} + \\alpha \\cdot \\text{update}(x_{\\text{old}})$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"grid_size\": (2, 2),\n            \"w\": 0.6,\n            \"rho\": 0.3,\n            \"tau_sq\": 1.0,\n            \"sigma_sq\": 0.25,\n            \"y\": np.array([1.2, -0.3, 0.7, 2.0]),\n        },\n        {\n            \"grid_size\": (3, 3),\n            \"w\": 0.0,\n            \"rho\": 0.2,\n            \"tau_sq\": 1.5,\n            \"sigma_sq\": 0.16,\n            \"y\": np.array([-0.1, 0.2, 0.0, 0.5, -1.0, 0.3, 0.7, -0.4, 0.1]),\n        },\n        {\n            \"grid_size\": (3, 3),\n            \"w\": 1.2,\n            \"rho\": 0.5,\n            \"tau_sq\": 0.5,\n            \"sigma_sq\": 1.0,\n            \"y\": np.array([0.0, 0.5, -0.5, 1.5, -1.2, 0.2, -0.8, 0.9, -0.1]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        R, C = params[\"grid_size\"]\n        n = R * C\n        adj =_build_graph(R, C)\n        \n        # Precompute constants\n        b = np.log(params[\"rho\"] / (1 - params[\"rho\"])) if params[\"rho\"] not in [0, 1] else (-np.inf if params[\"rho\"]==0 else np.inf)\n        if params[\"rho\"] == 0.5: b = 0.0 # Handle log(1) case exactly\n        \n        tau_sq = params[\"tau_sq\"]\n        sigma_sq = params[\"sigma_sq\"]\n        \n        term1 = -0.5 * np.log(1 + tau_sq / sigma_sq)\n        term2_coeff = (tau_sq) / (2 * sigma_sq * (tau_sq + sigma_sq))\n        delta_L = term1 + term2_coeff * params[\"y\"]**2\n        \n        # Run Mean-Field\n        pi = _run_mean_field(n, adj, b, params[\"w\"], delta_L)\n        \n        # Run Loopy BP\n        beliefs = _run_loopy_bp(n, adj, b, params[\"w\"], delta_L)\n        \n        # Compute Mean Absolute Difference\n        mad = np.mean(np.abs(pi - beliefs))\n        results.append(mad)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _build_graph(R, C):\n    \"\"\"Constructs the adjacency list for a grid graph.\"\"\"\n    n = R * C\n    adj = [[] for _ in range(n)]\n    for r in range(R):\n        for c in range(C):\n            idx = r * C + c\n            if r > 0:\n                adj[idx].append((r - 1) * C + c)\n            if r  R - 1:\n                adj[idx].append((r + 1) * C + c)\n            if c > 0:\n                adj[idx].append(r * C + (c - 1))\n            if c  C - 1:\n                adj[idx].append(r * C + (c + 1))\n    return adj\n\ndef _run_mean_field(n, adj, b, w, delta_L, tol=1e-8, max_iter=1000, damping=0.5):\n    \"\"\"Computes mean-field fixed point probabilities.\"\"\"\n    pi = np.full(n, 0.5)\n    \n    for _ in range(max_iter):\n        pi_old = pi.copy()\n        pi_new = np.zeros(n)\n        \n        for i in range(n):\n            neighbor_sum = np.sum(pi_old[adj[i]])\n            logit_arg = b + w * neighbor_sum + delta_L[i]\n            pi_new[i] = expit(logit_arg)\n        \n        pi = (1 - damping) * pi_old + damping * pi_new\n        \n        if np.max(np.abs(pi - pi_old))  tol:\n            break\n            \n    return pi\n\ndef _run_loopy_bp(n, adj, b, w, delta_L, tol=1e-8, max_iter=1000, damping=0.5):\n    \"\"\"Computes Bethe beliefs via loopy sum-product.\"\"\"\n    lambda_nodes = b + delta_L\n    \n    # Messages mu_{i->j} stored in a dictionary\n    messages = {}\n    for i in range(n):\n        for j in adj[i]:\n            messages[(i, j)] = 0.0\n\n    for _ in range(max_iter):\n        messages_old = messages.copy()\n        \n        max_diff = 0.0\n        \n        # Iterate over all directed edges\n        for i in range(n):\n            for j in adj[i]:\n                # Sum of incoming messages to i, excluding from j\n                incoming_sum = sum(messages_old[(k, i)] for k in adj[i] if k != j)\n                \n                S_ij = lambda_nodes[i] + incoming_sum\n                \n                # Update message mu_{i->j} using numerically stable logaddexp\n                # np.logaddexp(0, x) computes log(1 + exp(x))\n                raw_update = np.logaddexp(0, w + S_ij) - np.logaddexp(0, S_ij)\n                \n                new_msg = (1 - damping) * messages_old[(i, j)] + damping * raw_update\n                messages[(i, j)] = new_msg\n                \n                max_diff = max(max_diff, np.abs(new_msg - messages_old[(i, j)]))\n\n        if max_diff  tol:\n            break\n\n    # Compute final beliefs\n    beliefs = np.zeros(n)\n    for i in range(n):\n        total_incoming = sum(messages[(k, i)] for k in adj[i])\n        logit_arg = lambda_nodes[i] + total_incoming\n        beliefs[i] = expit(logit_arg)\n        \n    return beliefs\n\nsolve()\n```"
        },
        {
            "introduction": "Finding the MAP configuration in a spike-and-slab model with an MRF prior is a non-convex optimization problem, meaning simple local search algorithms can easily become trapped in suboptimal solutions. This advanced practice addresses this challenge by implementing a continuation, or homotopy, method. This powerful strategy guides a local search algorithm (Iterated Conditional Modes) toward the global optimum by starting with a simplified, smoother version of the optimization landscape and gradually annealing parameters to their target values. By comparing the locally tracked solution to the global optimum, you will analyze the stability of the homotopy path and quantify the \"niceness\" of the final energy landscape by measuring the basin of attraction of the true MAP solution .",
            "id": "3480144",
            "problem": "Consider the linear observation model in compressed sensing with a spike-and-slab prior and a Markov Random Field (MRF) on the support indicators. Let $y \\in \\mathbb{R}^{m}$ be generated as $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known design matrix, $x \\in \\mathbb{R}^{n}$ is the unknown sparse signal, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is Gaussian noise with variance parameter $\\sigma^2 > 0$. The spike-and-slab prior introduces binary support indicators $z \\in \\{0,1\\}^{n}$ and continuous slab weights $w \\in \\mathbb{R}^{n}$ such that $x = z \\odot w$, where $\\odot$ denotes element-wise multiplication. Given $z$, the slab weights follow a Gaussian prior $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$ with slab variance $\\tau^2 > 0$. The support vector $z$ follows an Ising-type Markov Random Field (MRF) prior on a chain graph with pairwise interactions:\n$$\np(z) \\propto \\exp\\left( \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right),\n$$\nwhere $\\beta \\in \\mathbb{R}$ is the coupling strength and $h \\in \\mathbb{R}$ is the unary field that biases sparsity.\n\nFor a fixed parameter tuple $(\\sigma^2, \\tau^2, \\beta, h)$, the Maximum A Posteriori (MAP) estimate solves\n$$\n\\max_{z \\in \\{0,1\\}^{n},\\, w \\in \\mathbb{R}^{n}} \\left\\{ -\\frac{1}{2\\sigma^2}\\|y - A(z \\odot w)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right\\}.\n$$\nFor a fixed $z$, the optimal $w$ is the unique minimizer of a strictly convex quadratic, and thus can be obtained in closed form by solving a ridge regression restricted to the active support.\n\nYou must propose and implement a continuation (homotopy) strategy that starts from weak MRF coupling and large slab variance, and anneals to target values. Specifically, define an annealing schedule of $K$ steps that linearly interpolates\n$$\n\\beta_k = \\beta_{\\mathrm{init}} + \\frac{k}{K}(\\beta_{\\mathrm{tgt}} - \\beta_{\\mathrm{init}}), \\quad \\tau_k^2 = \\tau_{\\mathrm{init}}^2 - \\frac{k}{K}(\\tau_{\\mathrm{init}}^2 - \\tau_{\\mathrm{tgt}}^2),\n$$\nfor $k \\in \\{0,1,\\dots,K\\}$, with initial parameters $\\beta_{\\mathrm{init}} = 0$ and $\\tau_{\\mathrm{init}}^2 = 10$. At each step, compute:\n- The global MAP support by exact enumeration over all $z \\in \\{0,1\\}^{n}$, optimizing $w$ in closed form for each $z$.\n- A locally tracked MAP support using Iterated Conditional Modes (ICM), initialized from the previous stepâ€™s locally tracked support, where ICM alternates discrete updates of individual $z_i$ with exact re-optimization of $w$ for the updated support.\n\nHomotopy path stability is defined as the event that, for all steps $k \\in \\{1,\\dots,K\\}$, the locally tracked MAP support equals the exact global MAP support at that step. The basin of attraction at the target parameters is quantified as the fraction of random initial supports that converge under ICM to the exact global MAP support at the target parameters.\n\nStarting from first principles, your program must:\n1. Construct $A \\in \\mathbb{R}^{m \\times n}$ by drawing independent standard normal entries using a fixed pseudorandom seed $0$, and then normalizing each column to unit Euclidean norm.\n2. Fix $n = 8$ and $m = 5$.\n3. Define a deterministic ground-truth sparse signal $x^{\\star} \\in \\mathbb{R}^{n}$ with active indices at positions $2$, $4$, and $7$ (one-based indexing), i.e., $x^{\\star}_{2} = 1.5$, $x^{\\star}_{4} = -1.0$, $x^{\\star}_{7} = 0.8$, and all other entries equal to $0$.\n4. Generate the observation $y = A x^{\\star} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{gen}}^2 I_m)$ using a fixed pseudorandom seed $1$ and $\\sigma_{\\mathrm{gen}}^2 = 0.01$.\n5. Use the chain graph MRF prior $p(z)$ with edges $(i, i+1)$ for $i \\in \\{1,\\dots,n-1\\}$.\n\nDefine the Iterated Conditional Modes (ICM) local optimization as follows: given $(\\sigma^2, \\tau^2, \\beta, h)$ and an initialization $z^{(0)}$, repeat coordinate ascent sweeps over $i \\in \\{1,\\dots,n\\}$; at each coordinate, evaluate the posterior objective for $z_i = 0$ and $z_i = 1$ while keeping other $z_j$ fixed, each time re-optimizing $w$ in closed form for the resulting support; update $z_i$ to the choice yielding the higher posterior objective. Stop when a full sweep makes no changes or after a maximum of $50$ sweeps.\n\nYour program must implement the above and compute, for each test case, two outputs:\n- A stability flag, equal to $1$ if the locally tracked path matches the exact global path at all steps, and $0$ otherwise.\n- The basin fraction at the target parameters, defined as the fraction (rounded to three decimal places) of $R$ random initial supports (each sampled i.i.d. Bernoulli with parameter $0.5$ using a fixed pseudorandom seed $2$) that converge under ICM to the exact global MAP support at the target parameters.\n\nTest Suite. Use three test cases with parameters:\n- Case $1$: $\\beta_{\\mathrm{tgt}} = 0.4$, $\\tau_{\\mathrm{tgt}}^2 = 1.0$, $h = -0.3$, $\\sigma^2 = 0.01$, $K = 6$, $R = 25$.\n- Case $2$: $\\beta_{\\mathrm{tgt}} = 0.0$, $\\tau_{\\mathrm{tgt}}^2 = 1.0$, $h = -0.3$, $\\sigma^2 = 0.01$, $K = 6$, $R = 25$.\n- Case $3$: $\\beta_{\\mathrm{tgt}} = 1.2$, $\\tau_{\\mathrm{tgt}}^2 = 0.3$, $h = -0.15$, $\\sigma^2 = 0.015$, $K = 6$, $R = 25$.\n\nAlgorithmic Requirements:\n- The exact global MAP support must be obtained at each step by enumerating all $z \\in \\{0,1\\}^{n}$, and solving the ridge regression for $w$ restricted to the active support:\n$$\nw_S = \\left( \\frac{1}{\\sigma^2} A_S^{\\top} A_S + \\frac{1}{\\tau^2} I_{|S|} \\right)^{-1} \\left( \\frac{1}{\\sigma^2} A_S^{\\top} y \\right),\n$$\nwhere $S = \\{ i \\mid z_i = 1 \\}$ and $A_S$ is the submatrix of $A$ with columns in $S$.\n- The posterior objective to be maximized for a given $z$ is\n$$\n\\mathcal{L}(z) = -\\frac{1}{2\\sigma^2}\\left\\| y - A(z \\odot w_S) \\right\\|_2^2 - \\frac{1}{2\\tau^2}\\left\\| w_S \\right\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i,\n$$\nwith $w_S$ as above and $w_{i} = 0$ for $i \\notin S$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated Python-style list of three inner lists, each inner list containing two entries: the stability flag (an integer $0$ or $1$) and the basin fraction (a float rounded to three decimals). For example, the output should look like $[[1,0.920],[0,0.480],[1,0.840]]$, but with the actual values computed by your program.\n\nNo physical units or angles are involved in this problem. All numeric values must be treated as dimensionless. Ensure deterministic behavior by using the prescribed pseudorandom seeds where specified, and do not read any external input.",
            "solution": "The problem requires an analysis of Maximum A Posteriori (MAP) estimation for a sparse signal in a Bayesian framework. This involves a spike-and-slab prior on the signal combined with a Markov Random Field (MRF) prior on its support. The analysis will compare a global optimization method (exact enumeration) with a local one (Iterated Conditional Modes, ICM) within a homotopy or continuation scheme.\n\n### Principle-Based Design\n\n#### 1. Bayesian Model Formulation\nThe problem begins with a standard linear observation model from compressed sensing:\n$$\ny = A x + \\varepsilon\n$$\nwhere $y \\in \\mathbb{R}^{m}$ is the measurement vector, $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix, $x \\in \\mathbb{R}^{n}$ is the sparse signal to be recovered, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is additive Gaussian noise with variance $\\sigma^2$.\n\nThe sparsity of $x$ is modeled using a spike-and-slab prior. This prior introduces a binary support vector $z \\in \\{0, 1\\}^n$, where $z_i=1$ indicates that the $i$-th component of the signal, $x_i$, is non-zero, and $z_i=0$ indicates it is zero. The signal is then represented as $x = z \\odot w$, where $\\odot$ is the element-wise product and $w \\in \\mathbb{R}^n$ are the \"slab\" coefficients. The prior distributions are:\n-   For the slab weights $w$, given the support $z$, a Gaussian prior is assumed for the active coefficients: $p(w_i|z_i=1) \\sim \\mathcal{N}(0, \\tau^2)$. This is equivalent to a prior on the full vector $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$, as the posterior for $w_i$ with $z_i=0$ will be irrelevant since $x_i = 0$ anyway.\n-   For the support vector $z$, an Ising-type Markov Random Field (MRF) prior is defined on a chain graph. This prior encourages structured sparsity, where neighboring coefficients in the signal are more likely to be simultaneously active or inactive. The probability mass function is:\n    $$\n    p(z|\\beta, h) \\propto \\exp\\left( \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i \\right)\n    $$\n    The parameter $\\beta$ controls the coupling strength between adjacent support variables, and $h$ is a global parameter controlling the overall sparsity level.\n\n#### 2. MAP Estimation and Profiled Objective\nThe goal is to find the MAP estimate of both the support $z$ and the weights $w$ by maximizing the posterior distribution $p(z, w | y)$. This is equivalent to maximizing the log-posterior, which, by Bayes' theorem, is proportional to the sum of the log-likelihood and the log-priors:\n$$\n\\log p(z, w | y) \\propto \\log p(y | w, z) + \\log p(w | z) + \\log p(z)\n$$\nSubstituting the Gaussian likelihood and the specified priors, the objective function to maximize is:\n$$\n\\mathcal{J}(z, w) = -\\frac{1}{2\\sigma^2}\\|y - A(z \\odot w)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i\n$$\nA key insight is that for a fixed support vector $z$, the optimization problem for $w$ is quadratic and strictly convex. Let $S = \\{i | z_i=1\\}$ be the set of active indices and $|S|$ its cardinality. Let $w_S$ be the vector of weights corresponding to these indices and $A_S$ be the submatrix of $A$ with columns indexed by $S$. The objective for $w_S$ is:\n$$\n\\mathcal{J}(w_S | z) = -\\frac{1}{2\\sigma^2}\\|y - A_S w_S\\|_2^2 - \\frac{1}{2\\tau^2}\\|w_S\\|_2^2\n$$\nThe unique maximizer $w_S^*$ is found by setting the gradient to zero, which yields the solution to a ridge regression problem:\n$$\nw_S^*(z) = \\left( A_S^{\\top} A_S + \\frac{\\sigma^2}{\\tau^2} I_{|S|} \\right)^{-1} A_S^{\\top} y\n$$\nBy substituting this optimal $w_S^*(z)$ back into the full objective function, we obtain a profiled objective $\\mathcal{L}(z)$ that depends only on the discrete support vector $z$:\n$$\n\\mathcal{L}(z) = \\mathcal{J}(z, w^*(z)) = -\\frac{1}{2\\sigma^2}\\|y - A_S w_S^*(z)\\|_2^2 - \\frac{1}{2\\tau^2}\\|w_S^*(z)\\|_2^2 + \\beta \\sum_{i=1}^{n-1} z_i z_{i+1} + h \\sum_{i=1}^{n} z_i\n$$\nThe MAP estimation problem is now reduced to a combinatorial optimization over $z \\in \\{0, 1\\}^n$:\n$$\nz_{\\text{MAP}} = \\arg\\max_{z \\in \\{0,1\\}^n} \\mathcal{L}(z)\n$$\n\n#### 3. Optimization and Analysis Strategy\nThe problem requires comparing two approaches to find $z_{\\text{MAP}}$ within a continuation framework.\n\n-   **Global Optimization**: Since the signal dimension $n=8$ is small, the search space for $z$ has a manageable size of $2^8 = 256$. We can find the true global MAP support $z_{\\text{global}}$ by computing $\\mathcal{L}(z)$ for every possible $z$ and selecting the one that yields the maximum value. This provides a gold standard for comparison.\n\n-   **Local Optimization (ICM)**: For larger $n$, enumeration is infeasible. Iterated Conditional Modes (ICM) is a greedy local search algorithm. It iteratively optimizes the objective one coordinate at a time. For each $z_i$, holding all other $z_j$ ($j \\neq i$) fixed, ICM computes $\\mathcal{L}(z)$ for the two cases $z_i=0$ and $z_i=1$ and updates $z_i$ to the value that yields a higher objective. A full sweep consists of iterating through all $i \\in \\{1, \\dots, n\\}$. The algorithm terminates when a sweep produces no changes, guaranteeing convergence to a local maximum of $\\mathcal{L}(z)$.\n\n-   **Continuation (Homotopy) Method**: Local search methods like ICM are susceptible to getting trapped in poor local optima. A continuation strategy can mitigate this. The idea is to start with a \"simpler\" version of the problem and gradually anneal the parameters towards their target values. Here, we start with weak MRF coupling ($\\beta_{\\mathrm{init}}=0$) and a large slab variance ($\\tau_{\\mathrm{init}}^2=10$, which down-weights the prior on $w$). The parameters are linearly interpolated over $K$ steps to their target values $(\\beta_{\\mathrm{tgt}}, \\tau_{\\mathrm{tgt}}^2)$. The locally-optimal solution from step $k-1$ is used as the starting point for the ICM search at step $k$. This path-following procedure helps the local search stay near the basin of attraction of the global optimum.\n\n#### 4. Evaluation Metrics\n\n-   **Homotopy Path Stability**: This metric assesses the effectiveness of the continuation strategy. Stability is declared if the locally-tracked ICM solution matches the globally optimal solution at every step of the annealing schedule. A stability flag of $1$ indicates success, while $0$ indicates failure.\n\n-   **Basin of Attraction**: This quantifies the \"niceness\" of the final optimization landscape at the target parameters. It is estimated by the fraction of random initial supports that converge to the true global MAP support when ICM is run. A larger basin fraction suggests that the global optimum is easier to find from a random starting point.\n\nThe implementation will proceed by first generating the fixed synthetic data ($A, y$). Then, for each test case, it will execute the homotopy analysis to determine path stability and subsequently compute the basin fraction at the final parameters. All random processes are governed by fixed seeds to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the three test cases.\n    It orchestrates data generation, and for each case, runs the homotopy\n    analysis and basin of attraction calculation.\n    \"\"\"\n    \n    # Problem constants\n    n, m = 8, 5\n    sigma_gen_sq = 0.01\n\n    # Generate synthetic data A and y (fixed for all test cases)\n    rng_A = np.random.default_rng(0)\n    A = rng_A.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0)\n\n    x_star = np.zeros(n)\n    # One-based indexing in problem: 2, 4, 7\n    # Zero-based indexing in python: 1, 3, 6\n    x_star[1] = 1.5\n    x_star[3] = -1.0\n    x_star[6] = 0.8\n\n    rng_eps = np.random.default_rng(1)\n    epsilon = rng_eps.normal(0, np.sqrt(sigma_gen_sq), m)\n    y = A @ x_star + epsilon\n\n    # Test cases\n    test_cases = [\n        # (beta_tgt, tau_tgt^2, h, sigma^2, K, R)\n        (0.4, 1.0, -0.3, 0.01, 6, 25),\n        (0.0, 1.0, -0.3, 0.01, 6, 25),\n        (1.2, 0.3, -0.15, 0.015, 6, 25),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = process_case(params, A, y, n)\n        all_results.append(result)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef compute_log_posterior(z, sigma2, tau2, beta, h, y, A):\n    \"\"\"\n    Computes the profiled log-posterior objective L(z) for a given support z.\n    \"\"\"\n    z = np.array(z)\n    active_set = np.where(z == 1)[0]\n    k = len(active_set)\n    \n    mrf_term = beta * np.sum(z[:-1] * z[1:]) + h * np.sum(z)\n\n    if k == 0:\n        log_posterior = -0.5 / sigma2 * np.dot(y, y)\n        return log_posterior + mrf_term\n\n    A_S = A[:, active_set]\n    \n    lambda_reg = sigma2 / tau2\n    \n    try:\n        # Ridge regression solution for w_S\n        mat_inv = np.linalg.inv(A_S.T @ A_S + lambda_reg * np.eye(k))\n        w_S = mat_inv @ (A_S.T @ y)\n\n        residual = y - A_S @ w_S\n        log_lik = -0.5 / sigma2 * np.dot(residual, residual)\n        w_prior = -0.5 / tau2 * np.dot(w_S, w_S)\n        \n        log_posterior = log_lik + w_prior\n        return log_posterior + mrf_term\n    except np.linalg.LinAlgError:\n        return -np.inf\n\ndef find_global_map_support(params, y, A, n):\n    \"\"\"\n    Finds the global MAP support by enumerating all 2^n possibilities.\n    \"\"\"\n    sigma2, tau2, beta, h = params\n    best_z = None\n    max_log_post = -np.inf\n\n    for z_tuple in itertools.product([0, 1], repeat=n):\n        z = np.array(z_tuple)\n        log_post = compute_log_posterior(z, sigma2, tau2, beta, h, y, A)\n        if log_post > max_log_post:\n            max_log_post = log_post\n            best_z = z\n            \n    return best_z\n\ndef run_icm(z_init, params, y, A, n, max_sweeps=50):\n    \"\"\"\n    Performs Iterated Conditional Modes (ICM) local search.\n    \"\"\"\n    sigma2, tau2, beta, h = params\n    z_current = np.array(z_init, dtype=int)\n\n    for _ in range(max_sweeps):\n        changed = False\n        for i in range(n):\n            z_off = z_current.copy()\n            z_off[i] = 0\n            log_post_off = compute_log_posterior(z_off, sigma2, tau2, beta, h, y, A)\n\n            z_on = z_current.copy()\n            z_on[i] = 1\n            log_post_on = compute_log_posterior(z_on, sigma2, tau2, beta, h, y, A)\n\n            current_val = z_current[i]\n            if log_post_on > log_post_off:\n                if current_val == 0:\n                    z_current[i] = 1\n                    changed = True\n            else:\n                if current_val == 1:\n                    z_current[i] = 0\n                    changed = True\n        \n        if not changed:\n            break\n            \n    return z_current\n\ndef process_case(case_params, A, y, n):\n    \"\"\"\n    Executes the full analysis for a single test case.\n    \"\"\"\n    beta_tgt, tau2_tgt, h, sigma2, K, R = case_params\n    \n    # Homotopy parameters\n    beta_init = 0.0\n    tau2_init = 10.0\n    \n    betas = np.linspace(beta_init, beta_tgt, K + 1)\n    # The problem formula is equivalent to linspace.\n    # tau2_k = tau2_init - (k/K) * (tau2_init - tau2_tgt)\n    # This is (1 - k/K) * tau2_init + (k/K) * tau2_tgt\n    tau2s = np.linspace(tau2_init, tau2_tgt, K + 1)\n\n    # --- Homotopy Path Stability Analysis ---\n    stability_flag = 1\n    \n    # Step k=0\n    params_k0 = (sigma2, tau2s[0], betas[0], h)\n    z_global_k0 = find_global_map_support(params_k0, y, A, n)\n    z_local_prev = z_global_k0\n    z_global_target = None # Will store the final global MAP\n\n    for k in range(1, K + 1):\n        params_k = (sigma2, tau2s[k], betas[k], h)\n        z_global_k = find_global_map_support(params_k, y, A, n)\n        \n        # Track local MAP from previous step's solution\n        z_local_k = run_icm(z_local_prev, params_k, y, A, n)\n        \n        if not np.array_equal(z_local_k, z_global_k):\n            stability_flag = 0\n            \n        z_local_prev = z_local_k\n        if k == K:\n            z_global_target = z_global_k\n            \n    if K == 0: # Handle edge case where no annealing steps\n        z_global_target = z_global_k0\n\n    # --- Basin of Attraction Analysis ---\n    target_params = (sigma2, tau2s[K], betas[K], h)\n    convergence_count = 0\n    rng_basin = np.random.default_rng(2)\n\n    for _ in range(R):\n        z_init = rng_basin.integers(0, 2, size=n, dtype=int)\n        z_converged = run_icm(z_init, target_params, y, A, n)\n        if np.array_equal(z_converged, z_global_target):\n            convergence_count += 1\n            \n    basin_fraction = round(convergence_count / R, 3)\n\n    return [stability_flag, basin_fraction]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}