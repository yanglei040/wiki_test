{
    "hands_on_practices": [
        {
            "introduction": "要有效地进行网络剪枝，核心问题在于如何衡量和比较单个参数的重要性。本练习将通过一个简化的二次损失函数模型，让您亲手计算和比较三种主流的剪枝准则：基于参数幅度的剪枝、基于一阶梯度的邻近算子剪枝以及基于二阶信息的Hessian矩阵对角线剪枝。通过这个实践，您将直观地理解不同剪枝策略如何基于不同的数学原理来识别并移除“不重要”的参数，并看到它们的选择可能并不一致。",
            "id": "3461726",
            "problem": "考虑一个单层参数模型，其参数向量为 $w \\in \\mathbb{R}^{6}$，在二次损失 $L(w) = \\tfrac{1}{2} w^{\\top} H w$ 下进行训练，其中 $H \\in \\mathbb{R}^{6 \\times 6}$ 是一个对称正定矩阵，表示损失函数在 $w$ 附近的局部曲率。本着稀疏优化和压缩感知的精神，假设我们希望剪枝掉固定数量的 $q=3$ 个参数以得到一个稀疏子网络，其动机源于彩票假设，即在原始的稠密网络中可以找到一个高性能的子网络（一张“中奖彩票”）。我们将计算并比较由三种基于一阶和二阶信息以及$\\ell_{1}$正则化的近端观点的原则性准则所产生的剪枝索引集。\n\n设当前参数和海森矩阵由下式给出\n$$\nw = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix}, \\quad H = \\operatorname{diag}\\!\\big(12,\\ 1.5,\\ 9,\\ 25,\\ 0.8,\\ 4\\big).\n$$\n定义以下剪枝规则：\n1. 幅度剪枝：移除$q$个$|w_{i}|$值最小的索引。\n2. 近端$\\ell_{1}$范数剪枝：考虑复合目标 $F(w) = L(w) + \\lambda \\|w\\|_{1}$，其中正则化参数 $\\lambda > 0$，$\\|w\\|_{1} = \\sum_{i=1}^{6} |w_{i}|$。从$w$开始执行一步近端梯度下降，步长为 $\\eta > 0$：\n$$\nw^{+} = \\operatorname{prox}_{\\eta \\lambda \\|\\cdot\\|_{1}}\\!\\big(w - \\eta \\nabla L(w)\\big),\n$$\n并剪掉满足 $(w^{+})_{i} = 0$ 的索引 $i$。此处 $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{1}}(z)$ 表示$\\ell_{1}$范数在$z$点、参数为$\\tau$的近端算子，即逐元素的软阈值函数 $\\operatorname{sign}(z_{i}) \\max(|z_{i}| - \\tau, 0)$。使用 $\\eta = 0.1$ 和 $\\lambda = 0.3$。\n3. 海森对角显著性剪枝：使用带有对角曲率的二阶泰勒展开来近似将坐标 $i$ 置零所带来的损失增加量，并剪掉$q$个使显著性 $s_{i} = \\tfrac{1}{2} H_{ii} w_{i}^{2}$ 最小的索引。这是一种被称为 Optimal Brain Damage (OBD) 的二阶剪枝启发式方法的对角变体。\n\n令 $S_{\\mathrm{mag}}$、$S_{\\mathrm{prox}}$ 和 $S_{\\mathrm{hess}}$ 分别表示由规则1、2和3剪枝的索引集合，每个集合的基数均为 $q=3$。请明确计算每个集合，然后计算三重交集的基数 $|S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}}|$。将最终答案表示为单个实数。无需四舍五入，也不涉及物理单位。",
            "solution": "首先对问题进行严格的验证过程。\n\n### 第1步：提取已知条件\n问题陈述中明确给出的数据、变量和定义如下：\n- 模型参数向量：$w \\in \\mathbb{R}^{6}$\n- 损失函数：$L(w) = \\tfrac{1}{2} w^{\\top} H w$\n- 海森矩阵：$H \\in \\mathbb{R}^{6 \\times 6}$ 是一个对称正定矩阵。\n- 剪枝数量：$q=3$ 个参数。\n- 参数向量值：$w = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix}$\n- 海森矩阵值：$H = \\operatorname{diag}\\!\\big(12,\\ 1.5,\\ 9,\\ 25,\\ 0.8,\\ 4\\big)$\n- 剪枝规则1（幅度剪枝）：剪掉$q=3$个$|w_{i}|$值最小的索引。结果集为 $S_{\\mathrm{mag}}$。\n- 剪枝规则2（近端$\\ell_1$范数剪枝）：在一步近端梯度步 $w^{+} = \\operatorname{prox}_{\\eta \\lambda \\|\\cdot\\|_{1}}\\!\\big(w - \\eta \\nabla L(w)\\big)$ 后，剪掉满足 $(w^{+})_{i} = 0$ 的索引 $i$。该规则的参数为 $\\eta = 0.1$ 和 $\\lambda = 0.3$。近端算子是逐元素软阈值函数：$\\operatorname{prox}_{\\tau \\|\\cdot\\|_{1}}(z)_{i} = \\operatorname{sign}(z_{i}) \\max(|z_{i}| - \\tau, 0)$。结果集为 $S_{\\mathrm{prox}}$。\n- 剪枝规则3（海森对角显著性剪枝）：剪掉$q=3$个使显著性 $s_{i} = \\tfrac{1}{2} H_{ii} w_{i}^{2}$ 最小的索引。结果集为 $S_{\\mathrm{hess}}$。\n- 最终目标是计算三重交集的基数 $|S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}}|$。\n\n### 第2步：使用提取的已知条件进行验证\n对问题进行有效性评估。\n- **科学依据**：问题基于数值优化和机器学习中已建立的标准概念，特别是网络剪枝和稀疏优化。损失函数是标准的二次近似，海森矩阵表示局部曲率，而剪枝方法（幅度剪枝、用于$\\ell_1$的近端梯度以及Optimal Brain Damage的一种变体）都是该领域众所周知的启发式方法。\n- **良构的**：问题提供了所有必要的数值（$w$、$H$、$\\eta$、$\\lambda$、$q$）以及针对三种剪枝准则的清晰、明确的定义。目标是一个具体、可计算的量。\n- **客观性**：问题以精确的数学语言陈述，不含主观或基于观点的论断。\n\n经检查，该问题是自洽、一致的，并且没有验证清单中列出的任何缺陷。这是一个良构的数学练习。\n\n### 第3步：结论与行动\n问题是**有效的**。将提供完整的解答。\n\n任务是计算剪枝索引集 $S_{\\mathrm{mag}}$、$S_{\\mathrm{prox}}$ 和 $S_{\\mathrm{hess}}$，每个集合的基数均为 $q=3$，然后求出它们交集的基数。索引范围为1到6。\n\n**1. 计算 $S_{\\mathrm{mag}}$ (幅度剪枝)**\n该规则要求剪掉$q=3$个绝对幅度最小的参数。我们首先计算$w$各分量的绝对值：\n$$\nw = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix} \\implies |w| = \\begin{pmatrix} |w_1| \\\\ |w_2| \\\\ |w_3| \\\\ |w_4| \\\\ |w_5| \\\\ |w_6| \\end{pmatrix} = \\begin{pmatrix} 0.08 \\\\ 0.30 \\\\ 0.15 \\\\ 0.02 \\\\ 0.24 \\\\ 0.12 \\end{pmatrix}\n$$\n我们将这些幅度按升序排列，以找出最小的三个：\n$|w_4|=0.02, |w_1|=0.08, |w_6|=0.12, |w_3|=0.15, |w_5|=0.24, |w_2|=0.30$。\n对应最小三个幅度的索引是 $4$、$1$ 和 $6$。\n因此，剪枝索引集为 $S_{\\mathrm{mag}} = \\{1, 4, 6\\}$。\n\n**2. 计算 $S_{\\mathrm{prox}}$ (近端$\\ell_1$范数剪枝)**\n该规则涉及单步近端梯度下降。首先，我们计算损失函数的梯度 $\\nabla L(w)$。给定 $L(w) = \\tfrac{1}{2} w^{\\top} H w$，梯度为 $\\nabla L(w) = Hw$。\n$$\n\\nabla L(w) = H w = \\begin{pmatrix} 12 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 1.5 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 9 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 25 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0.8 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix} = \\begin{pmatrix} 12 \\times 0.08 \\\\ 1.5 \\times (-0.30) \\\\ 9 \\times 0.15 \\\\ 25 \\times (-0.02) \\\\ 0.8 \\times 0.24 \\\\ 4 \\times (-0.12) \\end{pmatrix} = \\begin{pmatrix} 0.96 \\\\ -0.45 \\\\ 1.35 \\\\ -0.50 \\\\ 0.192 \\\\ -0.48 \\end{pmatrix}\n$$\n接下来，我们计算近端算子的输入参数 $z = w - \\eta \\nabla L(w)$，步长 $\\eta=0.1$：\n$$\nz = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix} - 0.1 \\begin{pmatrix} 0.96 \\\\ -0.45 \\\\ 1.35 \\\\ -0.50 \\\\ 0.192 \\\\ -0.48 \\end{pmatrix} = \\begin{pmatrix} 0.08 - 0.096 \\\\ -0.30 + 0.045 \\\\ 0.15 - 0.135 \\\\ -0.02 + 0.05 \\\\ 0.24 - 0.0192 \\\\ -0.12 + 0.048 \\end{pmatrix} = \\begin{pmatrix} -0.016 \\\\ -0.255 \\\\ 0.015 \\\\ 0.030 \\\\ 0.2208 \\\\ -0.072 \\end{pmatrix}\n$$\n近端算子是软阈值函数，如果一个分量的幅度小于或等于阈值 $\\tau$，则将其设置为零。阈值为 $\\tau = \\eta \\lambda = 0.1 \\times 0.3 = 0.03$。我们剪掉满足 $|z_i| \\le \\tau$ 的索引 $i$。\n- $|z_1| = |-0.016| = 0.016 \\le 0.03$。剪掉索引 $1$。\n- $|z_2| = |-0.255| = 0.255 > 0.03$。不剪掉索引 $2$。\n- $|z_3| = |0.015| = 0.015 \\le 0.03$。剪掉索引 $3$。\n- $|z_4| = |0.030| = 0.03 \\le 0.03$。剪掉索引 $4$。\n- $|z_5| = |0.2208| = 0.2208 > 0.03$。不剪掉索引 $5$。\n- $|z_6| = |-0.072| = 0.072 > 0.03$。不剪掉索引 $6$。\n被置零的索引是 $1$、$3$ 和 $4$。\n因此，剪枝索引集为 $S_{\\mathrm{prox}} = \\{1, 3, 4\\}$。\n\n**3. 计算 $S_{\\mathrm{hess}}$ (海森对角显著性剪枝)**\n该规则剪掉$q=3$个显著性值 $s_i = \\tfrac{1}{2} H_{ii} w_i^2$ 最小的索引。我们可以等价地比较缩放后的显著性 $2s_i = H_{ii} w_i^2$：\n- $2s_1 = H_{11} w_1^2 = 12 \\times (0.08)^2 = 12 \\times 0.0064 = 0.0768$\n- $2s_2 = H_{22} w_2^2 = 1.5 \\times (-0.30)^2 = 1.5 \\times 0.09 = 0.135$\n- $2s_3 = H_{33} w_3^2 = 9 \\times (0.15)^2 = 9 \\times 0.0225 = 0.2025$\n- $2s_4 = H_{44} w_4^2 = 25 \\times (-0.02)^2 = 25 \\times 0.0004 = 0.01$\n- $2s_5 = H_{55} w_5^2 = 0.8 \\times (0.24)^2 = 0.8 \\times 0.0576 = 0.04608$\n- $2s_6 = H_{66} w_6^2 = 4 \\times (-0.12)^2 = 4 \\times 0.0144 = 0.0576$\n将这些显著性值按升序排列：\n$2s_4=0.01, 2s_5=0.04608, 2s_6=0.0576, 2s_1=0.0768, 2s_2=0.135, 2s_3=0.2025$。\n对应最小三个显著性的索引是 $4$、$5$ 和 $6$。\n因此，剪枝索引集为 $S_{\\mathrm{hess}} = \\{4, 5, 6\\}$。\n\n**4. 计算交集的基数**\n最后，我们计算这三个集合交集的基数：\n- $S_{\\mathrm{mag}} = \\{1, 4, 6\\}$\n- $S_{\\mathrm{prox}} = \\{1, 3, 4\\}$\n- $S_{\\mathrm{hess}} = \\{4, 5, 6\\}$\n\n所有三个集合的交集是：\n$$\nS_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}} = \\{1, 4, 6\\} \\cap \\{1, 3, 4\\} \\cap \\{4, 5, 6\\}\n$$\n首先，我们求前两个集合的交集：$S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} = \\{1, 4\\}$。\n然后，将此结果与第三个集合求交集：$\\{1, 4\\} \\cap \\{4, 5, 6\\} = \\{4\\}$。\n结果集为 $\\{4\\}$。该集合的基数为 $1$。\n$$\n|S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}}| = |\\{4\\}| = 1\n$$\n唯一被所有三个标准剪掉的参数是索引为 $4$ 的参数。",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "幅度剪枝因其简单性而被广泛使用，但它是否总是最优选择？这个练习旨在通过一个精心设计的反例，揭示幅度剪枝的潜在缺陷。您将使用损失函数的二阶泰勒展开来精确量化剪枝单个参数对模型性能的影响，即所谓的“显著性”，并计算采用简单幅度剪枝所带来的“遗憾值”。这个过程将深刻地揭示，参数的绝对值大小与其对模型性能的真实重要性之间可能存在巨大差异，从而阐明了发展更复杂剪枝方法的必要性。",
            "id": "3461747",
            "problem": "考虑一个神经网络层中的参数向量 $w \\in \\mathbb{R}^{3}$，其当前值为 $w = (0.01,\\,0.08,\\,0.12)$。训练损失 $L(w)$ 在 $w$ 处是二阶连续可微的，其梯度为 $\\nabla L(w) = g = (0.5,\\,0.1,\\,0.05)$，Hessian矩阵为 $H$，其主对角线元素为 $(2000,\\,10,\\,5)$。假设对于 $w$ 附近的一个无穷小扰动 $\\Delta w$，损失满足二阶泰勒展开 $L(w+\\Delta w) \\approx L(w) + g^{\\top}\\Delta w + \\tfrac{1}{2}\\Delta w^{\\top}H\\Delta w$。一个移除坐标 $i$ 的剪枝操作会将 $w$ 的第 $i$ 个分量设为零，即应用 $\\Delta w = -w_{i} e_{i}$，其中 $e_{i}$ 是 $\\mathbb{R}^{3}$ 中的第 $i$ 个标准基向量。在幅度剪枝中，选择具有最小 $|w_{i}|$ 的索引 $i$ 进行移除；在显著性感知剪枝中，选择最小化由剪枝引起的损失增加的二阶泰勒近似值的索引 $i$。\n\n从二阶泰勒展开以及梯度和Hessian矩阵的定义出发，推导由剪枝单个坐标 $i$ 引起的损失变化的解析近似，并用它计算每个坐标的近似损失增加量。确定幅度剪枝将移除的坐标以及最小化近似损失增加量的坐标。然后计算幅度剪枝的遗憾值（regret），其定义为幅度剪枝下的近似损失增加量与显著性感知剪枝下的最小近似损失增加量之间的差值。将遗憾值以单个实数的形式给出。无需四舍五入，最终答案中不应包含任何单位。",
            "solution": "问题要求计算幅度剪枝相对于最优显著性感知剪枝方法的遗憾值。遗憾值定义为由幅度剪枝引起的近似损失增加量与可能的最小近似损失增加量之间的差值。\n\n首先，我们必须推导当单个权重坐标 $w_i$ 被剪枝（设为0）时，损失变化 $\\Delta L$ 的解析近似。问题陈述，剪枝坐标 $i$ 对应于一个扰动 $\\Delta w = -w_i e_i$，其中 $e_i$ 是 $\\mathbb{R}^{3}$ 中的第 $i$ 个标准基向量。新的权重向量为 $w' = w + \\Delta w$。\n\n损失的变化由围绕 $w$ 的二阶泰勒展开给出：\n$$L(w + \\Delta w) - L(w) \\approx g^{\\top}\\Delta w + \\frac{1}{2}\\Delta w^{\\top}H\\Delta w$$\n令 $\\Delta L_i$ 表示剪枝坐标 $i$ 时的近似损失变化。我们将 $\\Delta w = -w_i e_i$ 代入展开式。\n\n第一项是梯度 $g$ 和扰动 $\\Delta w$ 的内积：\n$$g^{\\top}\\Delta w = g^{\\top}(-w_i e_i) = -w_i (g^{\\top}e_i)$$\n由于 $g^{\\top}e_i$ 是梯度向量 $g$ 的第 $i$ 个分量 $g_i$，该项简化为 $-w_i g_i$。\n\n第二项涉及 Hessian 矩阵 $H$：\n$$\\frac{1}{2}\\Delta w^{\\top}H\\Delta w = \\frac{1}{2}(-w_i e_i)^{\\top}H(-w_i e_i) = \\frac{1}{2}w_i^2 (e_i^{\\top}H e_i)$$\n二次型 $e_i^{\\top}H e_i$ 选择了 Hessian 矩阵的第 $i$ 个对角元素 $H_{ii}$。因此，第二项简化为 $\\frac{1}{2}w_i^2 H_{ii}$。\n\n结合这两项，因剪枝坐标 $i$ 引起的损失增加的解析近似为：\n$$\\Delta L_i \\approx -w_i g_i + \\frac{1}{2}w_i^2 H_{ii}$$\n这个量通常被称为权重 $w_i$ 的显著性（saliency）。\n\n给定的值为：\n参数向量：$w = (w_1, w_2, w_3) = (0.01, 0.08, 0.12)$\n梯度向量：$g = (g_1, g_2, g_3) = (0.5, 0.1, 0.05)$\nHessian 矩阵对角线元素：$(H_{11}, H_{22}, H_{33}) = (2000, 10, 5)$\n\n现在，我们为每个坐标 $i \\in \\{1, 2, 3\\}$ 计算近似损失增加量 $\\Delta L_i$。\n\n对于坐标 $i=1$：\n$w_1 = 0.01$, $g_1 = 0.5$, $H_{11} = 2000$。\n$$\\Delta L_1 \\approx -(0.01)(0.5) + \\frac{1}{2}(0.01)^2(2000) = -0.005 + \\frac{1}{2}(0.0001)(2000) = -0.005 + 0.1 = 0.095$$\n\n对于坐标 $i=2$：\n$w_2 = 0.08$, $g_2 = 0.1$, $H_{22} = 10$。\n$$\\Delta L_2 \\approx -(0.08)(0.1) + \\frac{1}{2}(0.08)^2(10) = -0.008 + \\frac{1}{2}(0.0064)(10) = -0.008 + 0.032 = 0.024$$\n\n对于坐标 $i=3$：\n$w_3 = 0.12$, $g_3 = 0.05$, $H_{33} = 5$。\n$$\\Delta L_3 \\approx -(0.12)(0.05) + \\frac{1}{2}(0.12)^2(5) = -0.006 + \\frac{1}{2}(0.0144)(5) = -0.006 + 0.036 = 0.030$$\n\n接下来，我们确定每种剪枝方法将移除的坐标。\n\n幅度剪枝移除具有最小绝对值 $|w_i|$ 的坐标。\n$|w_1| = 0.01$\n$|w_2| = 0.08$\n$|w_3| = 0.12$\n最小值为 $|w_1| = 0.01$。因此，幅度剪枝移除坐标1。相关的损失增加量为 $\\Delta L_{\\text{magnitude}} = \\Delta L_1 \\approx 0.095$。\n\n显著性感知剪枝移除最小化近似损失增加量 $\\Delta L_i$ 的坐标。\n$\\Delta L_1 \\approx 0.095$\n$\\Delta L_2 \\approx 0.024$\n$\\Delta L_3 \\approx 0.030$\n最小值为 $\\Delta L_2 \\approx 0.024$。因此，显著性感知剪枝移除坐标2。最小的近似损失增加量为 $\\Delta L_{\\text{min}} = \\Delta L_2 \\approx 0.024$。\n\n最后，我们计算幅度剪枝的遗憾值，即幅度剪枝造成的损失增加量与可能的最小损失增加量之间的差值。\n$$\\text{Regret} = \\Delta L_{\\text{magnitude}} - \\Delta L_{\\text{min}} = \\Delta L_1 - \\Delta L_2$$\n$$\\text{Regret} \\approx 0.095 - 0.024 = 0.071$$\n遗憾值为 $0.071$。",
            "answer": "$$\n\\boxed{0.071}\n$$"
        },
        {
            "introduction": "彩票假设（Lottery Ticket Hypothesis）的核心思想是在一个大型网络中存在一个“中奖彩票”——一个稀疏子网络，它可以在隔离训练时达到与完整网络相媲美的性能。这个综合性的编程实践将指导您构建一个完整的实验流程，以检验迭代幅度剪枝（Iterative Magnitude Pruning, IMP）这一关键算法是否能在一个合成的“教师-学生”网络设置中成功找到预设的稀疏结构。您将把网络剪枝问题与压缩感知中的稀疏恢复理论联系起来，通过控制数据相关性（即互相关性）和噪声水平，探索该算法在不同条件下的性能边界。",
            "id": "3461714",
            "problem": "您将构建并评估一个合成的稀疏教师线性网络，以测试带有权重回溯的迭代幅度剪枝（IMP）过程是否能识别真实的支撑集，这借鉴了压缩感知和稀疏优化中可识别性测试的精神。核心对象是一个单层线性网络（即线性回归器），它是神经网络的一个特例，并为连接网络剪枝、彩票假设（LTH）与稀疏回归可识别性提供了最简单的场景。\n\n基本和核心定义：\n\n- 考虑一个稀疏教师下的标准线性模型，\n$$\ny = X w^\\star + \\varepsilon,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，$w^\\star \\in \\mathbb{R}^d$ 是教师参数向量，其稀疏支撑集为 $S^\\star \\subset \\{1,\\dots,d\\}$，大小为 $\\lvert S^\\star \\rvert = s$，而 $\\varepsilon \\in \\mathbb{R}^n$ 是噪声。\n\n- 列归一化设计矩阵 $X$ 的互相关性定义为\n$$\n\\mu(X) = \\max_{i \\neq j} \\left\\lvert \\langle x_i, x_j \\rangle \\right\\rvert,\n$$\n其中 $x_i$ 表示 $X$ 的第 $i$ 列，并已缩放至单位 $\\ell_2$-范数。互相关性是压缩感知中用于衡量稀疏支撑集可识别性的经典指标。\n\n- 模型使用平方误差损失进行训练，也称为均方误差（MSE），\n$$\n\\mathcal{L}(w) = \\frac{1}{2n} \\left\\| X w - y \\right\\|_2^2,\n$$\n其梯度是优化领域中一个众所周知的基本对象。\n\n- 彩票假设（LTH）指出，在一个更大的随机初始化网络中，存在一些子网络（即所谓的“中奖彩票”），当这些子网络从其原始初始化状态被隔离训练时，可以达到与完整网络相当的性能。迭代幅度剪枝（IMP）是寻找此类子网络的标准过程，通过重复训练、剪枝掉幅度最小的参数，并将剩余参数回溯到其初始值。\n\n任务：\n\n- 您将实现一个完整且确定性的程序，该程序能够：\n  1. 合成具有预设近似互相关性水平和已知稀疏支撑集的数据。\n  2. 在一个强制剪枝的二元掩码下，通过全批量梯度下降训练一个线性网络。\n  3. 执行带有权重回溯的迭代幅度剪枝（IMP），在每个剪枝轮次将权重回溯到初始随机初始化值，直到恰好剩下 $s$ 个非零权重。\n  4. 评估精确支撑集恢复，即最终恢复的支撑集 $\\widehat{S}$ 是否等于真实的 $S^\\star$。\n\n数据生成协议：\n\n- 固定整数 $n$、$d$ 和 $s$，其中 $s \\ll d \\leq n$。生成一个协方差模型，其目标非对角线相关参数为 $\\rho \\in [0,1)$：\n$$\n\\Sigma(\\rho) = (1 - \\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵，$\\mathbf{1}\\mathbf{1}^\\top$ 是全一矩阵。从均值为零、协方差为 $\\Sigma(\\rho)$ 的多元正态分布中抽取 $n$ 个独立样本，构成 $X$ 的行，然后将 $X$ 的每一列归一化为单位 $\\ell_2$-范数。这将生成一个经验互相关性接近目标相关性 $\\rho$ 的设计矩阵。\n\n- 构建教师向量 $w^\\star$，使其在已知的支撑集 $S^\\star$ 上恰好有 $s$ 个非零项；非零值应设置为一个固定的非零幅度。使用以下公式生成标签：\n$$\ny = X w^\\star + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n),\n$$\n其中 $\\sigma \\ge 0$ 是指定的噪声标准差。\n\n训练与剪枝协议：\n\n- 从一个固定的种子随机初始化可训练权重 $w^{(0)}$。使用一个二元掩码 $m \\in \\{0,1\\}^d$ 来强制剪枝，通过将有效权重限制为 $w \\odot m$，其中 $\\odot$ 表示哈达玛积。\n\n- 在每个 IMP 轮次中：\n  1. 在 MSE 损失上，使用全批量梯度下降法训练被掩码的模型 $w \\odot m$，步数固定，步长选择小于梯度的利普希茨常数。该常数已知为\n  $$\n  L = \\frac{\\|X\\|_2^2}{n},\n  $$\n  其中 $\\|X\\|_2$ 是谱范数。\n  2. 通过将除最大幅度的坐标之外的所有坐标设置为零来进行剪枝，保留当前未剪枝权重的一定比例；相应地更新掩码 $m$。\n  3. 将幸存的权重回溯到它们的初始值：$w \\leftarrow w^{(0)} \\odot m$。\n  4. 重复此过程，直到恰好剩下 $s$ 个坐标未被剪枝。在最后一次剪枝后，在最终掩码下再次训练固定步数。\n\n- 恢复的支撑集 $\\widehat{S}$ 是最终掩码中值为 1 的索引集合。\n\n评估：\n\n- 对于每个测试用例，返回一个布尔值，指示是否实现了精确支撑集恢复，即 $\\widehat{S} = S^\\star$ 是否成立。\n\n测试套件和覆盖范围：\n\n- 使用固定的 $n = 256$，$d = 64$，$s = 6$。对 $w^\\star$ 的非零项使用恒定的非零幅度。\n\n- 使用以下五个测试用例 $(\\rho, \\sigma)$：\n  1. $(0.0, 0.0)$：理想情况，正交设计，无噪声。\n  2. $(0.3, 0.0)$：中等相关性，无噪声。\n  3. $(0.6, 0.0)$：高相关性，无噪声。\n  4. $(0.3, 0.1)$：中等相关性，低噪声。\n  5. $(0.6, 0.5)$：高相关性，较高噪声（压力测试）。\n\n- 程序必须是确定性的，使用固定的随机种子，并且必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，`[True,False,...]`）。每个元素必须是一个布尔值，表示对应测试用例（按上述顺序列出）的精确支撑集恢复情况。\n\n角度单位和物理单位不适用。所有数值答案均无单位。最终输出格式必须严格为指定列表格式的一行。",
            "solution": "用户希望验证带有权重回溯的迭代幅度剪枝（IMP）是否能识别教师线性网络的真实稀疏支撑集。此任务将深度学习中的彩票假设（LTH）与压缩感知中的稀疏恢复和可识别性原理联系起来。解决方案涉及合成具有受控属性的数据，实现IMP算法，并在一组测试用例上评估其性能。\n\n首先，我们对问题进行形式化。底层模型是一个稀疏线性回归设置：\n$$\ny = X w^\\star + \\varepsilon\n$$\n这里，$X \\in \\mathbb{R}^{n \\times d}$ 是数据或设计矩阵，$w^\\star \\in \\mathbb{R}^d$ 是真实参数矢量，它是 $s$-稀疏的，意味着它只有 $s$ 个非零项。这些非零项的索引集合是真实支撑集，记为 $S^\\star$，其基数为 $|S^\\star| = s$。项 $\\varepsilon \\in \\mathbb{R}^n$ 代表加性噪声，通常假定为高斯噪声。维度固定为 $n=256$，$d=64$，以及 $s=6$。\n\n学习算法的目标是在给定 $X$ 和 $y$ 的情况下恢复 $S^\\star$。该算法通过最小化均方误差（MSE）损失函数来训练一个稠密参数矢量 $w \\in \\mathbb{R}^d$：\n$$\n\\mathcal{L}(w) = \\frac{1}{2n} \\| Xw - y \\|_2^2\n$$\n此损失函数关于 $w$ 的梯度是：\n$$\n\\nabla_w \\mathcal{L}(w) = \\frac{1}{n} X^\\top (Xw - y)\n$$\n这个梯度是训练过程的核心。\n\n数据合成协议旨在创建一个难度可调的问题。设计矩阵 $X$ 是从一个零均值的多元正态分布 $X_{ij} \\sim \\mathcal{N}(0, \\Sigma(\\rho))$ 生成的，其中协方差矩阵 $\\Sigma(\\rho)$ 由下式给出：\n$$\n\\Sigma(\\rho) = (1 - \\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵，$\\mathbf{1}\\mathbf{1}^\\top$ 是全一矩阵。参数 $\\rho \\in [0,1)$ 控制特征之间的相关性。在生成初始数据后，矩阵 $X$ 的每一列都被归一化为单位 $\\ell_2$-范数。此过程产生的设计矩阵的经验互相关性 $\\mu(X)$ 近似于 $\\rho$。高相关性使得区分相关特征的影响更加困难，从而对支撑集恢复构成挑战。教师向量 $w^\\star$ 的构建方式是将其支撑集 $S^\\star$ 设置为前 $s=6$ 个索引，非零项的幅度设为 $1.0$。然后，观测向量 $y$ 通过加性高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 计算得出。\n\n任务的核心是实现带有权重回溯的迭代幅度剪枝（IMP）。该算法按轮次进行：\n1.  **初始化**：从 $\\mathcal{N}(0, I_d)$ 中随机抽取一个权重向量 $w^{(0)}$ 并存储。这是“彩票”的初始化。初始的二元掩码 $m^{(0)}$ 是一个全为一的向量，$m^{(0)} = \\mathbf{1} \\in \\{0,1\\}^d$。\n\n2.  **迭代周期（训练、剪枝、回溯）**：算法进行固定数量的轮次，每一轮都增加模型的稀疏度。对于第 $k$ 轮：\n    *   **训练**：首先将当前活动支撑集上的权重重置为其初始值：$w_{start} = w^{(0)} \\odot m^{(k-1)}$。然后，模型使用全批量梯度下降进行固定次数的迭代训练。权重更新被限制在由掩码 $m^{(k-1)}$ 定义的子网络上：\n      $$\n      w_{t+1} = w_t - \\eta \\cdot \\left( \\nabla_w \\mathcal{L}(w_t) \\odot m^{(k-1)} \\right)\n      $$\n      学习率 $\\eta$ 设置为低于梯度利普希茨常数的倒数，$\\eta = \\alpha / L$，其中 $\\alpha \\in (0,1)$ 且 $L = \\|X\\|_2^2/n$，以确保稳定收敛。训练后，我们得到这一轮的最终权重 $w^{(k)}_{final}$。\n    *   **剪枝**：通过识别 $w^{(k)}_{final}$ 中绝对值最大的参数的索引，生成一个新的、更稀疏的掩码 $m^{(k)}$。保留的权重数量 $s_k$ 由预定义的计划决定。在本实现中，该计划逐步减小支撑集的大小：$d=64 \\to 32 \\to 16 \\to 8 \\to s=6$。新的支撑集 $S^{(k)}$ 是：\n      $$\n      S^{(k)} = \\underset{I \\subset \\{1,\\dots,d\\}, |I|=s_k}{\\text{argmax}} \\sum_{i \\in I} \\left| \\left(w^{(k)}_{final}\\right)_i \\right|\n      $$\n      新掩码 $m^{(k)}$ 在 $S^{(k)}$ 中的索引处为1，其他位置为0。\n    *   **回溯**：通过将新支撑集 $S^{(k)}$ 上的值回溯到它们来自 $w^{(0)}$ 的初始值，来准备*下一*轮训练的权重。这通过设置 $w = w^{(0)} \\odot m^{(k)}$ 来实现。\n\n3.  **最终评估**：在最后一轮剪枝之后，得到的掩码 $\\widehat{m} = m^{(K)}$ 恰好有 $s$ 个非零项，定义了恢复的支撑集 $\\widehat{S} = \\{i \\mid \\widehat{m}_i = 1\\}$。成功与否由精确支撑集恢复的条件决定，即 $\\widehat{S} = S^\\star$ 是否成立。该算法在 $s$-稀疏网络上进行最终的训练运行，但支撑集恢复的结果仅由最终掩码确定。\n\n整个过程使用固定的随机种子以确定性方式实现，用于测试五个指定的 $(\\rho, \\sigma)$ 情况，这些情况系统地改变了数据相关性和噪声水平，以探究IMP算法恢复能力的极限。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the IMP experiment and print results, adhering to the specified problem.\n    \"\"\"\n\n    def train(X, y, w_initial, mask, num_steps, learning_rate):\n        \"\"\"\n        Trains a linear model with a given mask using full-batch gradient descent.\n        The weights 'w' start sparse (as per w_initial) and updates are restricted\n        to the mask's support, so 'w' remains sparse throughout training.\n        \n        Args:\n            X (np.ndarray): Design matrix of shape (n, d).\n            y (np.ndarray): Target vector of shape (n, 1).\n            w_initial (np.ndarray): Initial weights for the training round, shape (d, 1).\n            mask (np.ndarray): Binary mask of shape (d, 1).\n            num_steps (int): Number of gradient descent steps.\n            learning_rate (float): Learning rate for the optimizer.\n\n        Returns:\n            np.ndarray: Trained weights of shape (d, 1).\n        \"\"\"\n        w = w_initial.copy()\n        n = X.shape[0]\n\n        for _ in range(num_steps):\n            # Gradient of MSE loss: (1/n) * X.T @ (X @ w - y)\n            error = X @ w - y\n            grad = (1 / n) * X.T @ error\n            \n            # Project the gradient onto the subspace of active weights defined by the mask.\n            grad_masked = grad * mask\n            w -= learning_rate * grad_masked\n            \n        return w\n\n    def run_experiment(rho, sigma, n, d, s, seed):\n        \"\"\"\n        Synthesizes data, runs the Iterative Magnitude Pruning (IMP) procedure,\n        and evaluates for exact support recovery.\n\n        Args:\n            rho (float): Correlation parameter for data generation.\n            sigma (float): Standard deviation of the noise.\n            n (int): Number of samples.\n            d (int): Number of features/dimensions.\n            s (int): Sparsity level of the teacher model.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            bool: True if the true support was exactly recovered, False otherwise.\n        \"\"\"\n        # Master Random Number Generator for the experiment\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        true_support = set(range(s))\n        cov_matrix = (1 - rho) * np.eye(d) + rho * np.ones((d, d))\n        X_pre = rng.multivariate_normal(np.zeros(d), cov_matrix, size=n, check_valid='warn', tol=1e-8)\n        col_norms = np.linalg.norm(X_pre, axis=0)\n        col_norms[col_norms == 0] = 1.0 # Avoid division by zero\n        X = X_pre / col_norms\n\n        w_star = np.zeros(d)\n        w_star[list(true_support)] = 1.0\n        noise = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = (X @ w_star + noise).reshape(-1, 1)\n\n        # 2. Training and Pruning Protocol Setup\n        num_train_steps = 1000\n        sparsity_schedule = [32, 16, 8, s]\n\n        try:\n            spectral_norm_X = np.linalg.svd(X, compute_uv=False)[0]\n            L = spectral_norm_X**2 / n\n            learning_rate = 0.5 / L if L > 1e-9 else 0.01\n        except np.linalg.LinAlgError:\n            learning_rate = 0.01\n\n        # Use a distinct, but fixed, seed for weight initialization\n        init_rng = np.random.default_rng(seed + 1)\n        w_init = init_rng.normal(loc=0.0, scale=1.0, size=(d, 1))\n        \n        mask = np.ones((d, 1))\n        w = w_init.copy()\n\n        # Core IMP loop: Train, Prune, Rewind\n        for target_sparsity in sparsity_schedule:\n            # Train the model with the current mask.\n            w_trained = train(X, y, w, mask, num_train_steps, learning_rate)\n\n            # Prune based on the magnitudes of the trained weights.\n            magnitudes = np.abs(w_trained.flatten())\n            indices_to_keep = np.argsort(magnitudes)[-target_sparsity:]\n\n            # Update the mask for the next round.\n            mask.fill(0)\n            mask[indices_to_keep] = 1\n            \n            # Rewind the weights to their initial values, applying the new mask.\n            w = w_init * mask\n            \n        # 3. Evaluation\n        recovered_support = set(np.where(mask.flatten() == 1)[0])\n        return recovered_support == true_support\n\n    # Fixed parameters and test cases from the problem statement\n    n, d, s = 256, 64, 6\n    global_seed = 42\n\n    test_cases = [\n        (0.0, 0.0),    # Case 1: Orthogonal-like, noiseless\n        (0.3, 0.0),    # Case 2: Moderate coherence, noiseless\n        (0.6, 0.0),    # Case 3: High coherence, noiseless\n        (0.3, 0.1),    # Case 4: Moderate coherence, low noise\n        (0.6, 0.5),    # Case 5: High coherence, high noise\n    ]\n\n    results = []\n    for rho, sigma in test_cases:\n        # Each experiment run uses the same global seed for reproducibility\n        # This means the same data generation process (for a given rho) and\n        # the same weight initialization are used across all test cases.\n        result = run_experiment(rho, sigma, n, d, s, seed=global_seed)\n        results.append(result)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}