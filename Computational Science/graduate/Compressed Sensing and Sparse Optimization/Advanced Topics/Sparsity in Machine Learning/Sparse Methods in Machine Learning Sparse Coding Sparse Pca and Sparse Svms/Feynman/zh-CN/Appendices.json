{
    "hands_on_practices": [
        {
            "introduction": "在稀疏优化中，许多算法的核心是一种被称为“软阈值”的基本操作。这个练习将带你深入了解这一关键构件的数学原理，即与 LASSO 惩罚相关的近端算子。通过从第一性原理（次梯度最优性条件）出发，你将推导出这个算子，并将其应用于一个具体向量，亲手体验它如何通过收缩和平移分量来实现稀疏性。",
            "id": "3477691",
            "problem": "考虑与最小绝对收缩和选择算子 (LASSO) 惩罚项相关的近端映射。对于给定的向量 $v \\in \\mathbb{R}^{n}$ 和正则化参数 $\\lambda \\geq 0$，将 $\\ell_{1}$-范数的近端算子定义为\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v) \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\n从凸优化的次梯度最优性条件和 $\\ell_{1}$-范数的定义出发，推导上述问题关于 $v$ 和 $\\lambda$ 的坐标级最小化器 $x^{\\star}$。然后，对于具体数据 $v = (\\,3,\\,-0.5,\\,-2,\\,0.1\\,)$ 和 $\\lambda = 1$，计算所得向量 $x^{\\star}$ 并确定其支撑集，支撑集理解为在基于 1 的索引下其非零元素的索引集合。\n\n最后，计算该最小化器 $x^{\\star}$ 的欧几里得范数平方 $\\|x^{\\star}\\|_{2}^{2}$，并将此值作为您的最终答案。无需四舍五入；请提供作为实数的精确值。",
            "solution": "用户希望我解决这个问题。\n### 第 1 步：提取给定信息\n- **近端算子定义**：$\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v) \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}$\n- **输入向量**：$v \\in \\mathbb{R}^{n}$\n- **正则化参数**：$\\lambda \\geq 0$\n- **具体数据**：$v = (\\,3,\\,-0.5,\\,-2,\\,0.1\\,)$ 和 $\\lambda = 1$。\n- **目标 1**：推导坐标级最小化器 $x^{\\star}$。\n- **目标 2**：根据给定数据计算 $x^{\\star}$ 并确定其支撑集（基于 1 的索引）。\n- **目标 3**：计算最小化器的欧几里得范数平方 $\\|x^{\\star}\\|_{2}^{2}$。\n- **最终答案**：报告 $\\|x^{\\star}\\|_{2}^{2}$ 的精确值。\n\n### 第 2 步：使用提取的给定信息进行验证\n- **科学依据**：该问题是凸优化、统计学和机器学习中的一个标准基础问题，特别与 LASSO 方法相关。近端算子、$\\ell_{1}$ 和 $\\ell_{2}$ 范数以及次梯度微积分的概念是公认的数学原理。该问题在科学上是合理的。\n- **适定性**：目标函数 $F(x) = \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 是一个严格凸函数（欧几里得范数平方项）和一个凸函数（$\\ell_{1}$-范数项）之和。因此，该和是严格凸的。定义在凸集（$\\mathbb{R}^{n}$）上的严格凸函数有唯一的最小化器。因此，该问题是适定的。\n- **目标**：该问题使用精确、无歧义的数学术语表述。\n- **完整性和一致性**：所有必要的定义、变量和数据都已提供。没有矛盾之处。\n\n### 第 3 步：结论和行动\n该问题是**有效的**。我将继续推导解决方案。\n\n要最小化的目标函数是\n$$\nF(x) = \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\n其中 $x, v \\in \\mathbb{R}^{n}$ 且 $\\lambda \\geq 0$。我们可以通过将范数展开为各分量之和来重写目标函数：\n$$\nF(x) = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - v_i)^2 + \\lambda \\sum_{i=1}^{n} |x_i| = \\sum_{i=1}^{n} \\left( \\frac{1}{2} (x_i - v_i)^2 + \\lambda |x_i| \\right)\n$$\n由于目标函数是可分的，我们可以通过独立地为每个分量 $x_i$ 找到最小化器来最小化它。对于每个 $i \\in \\{1, \\dots, n\\}$，我们需要解决以下一维优化问题：\n$$\nx_i^{\\star} = \\arg\\min_{x_i \\in \\mathbb{R}} \\left\\{ f_i(x_i) \\right\\} \\quad \\text{其中} \\quad f_i(x_i) = \\frac{1}{2} (x_i - v_i)^2 + \\lambda |x_i|\n$$\n函数 $f_i(x_i)$ 是凸函数。项 $|x_i|$ 在 $x_i = 0$ 处不可微，因此我们必须使用次梯度微积分。最小化器 $x_i^{\\star}$ 的最优性条件是零必须在 $f_i$ 于 $x_i^{\\star}$ 处的次梯度中：\n$$\n0 \\in \\partial f_i(x_i^{\\star})\n$$\n$f_i(x_i)$ 的次梯度由下式给出：\n$$\n\\partial f_i(x_i) = (x_i - v_i) + \\lambda \\, \\partial|x_i|\n$$\n其中 $\\partial|x_i|$ 是绝对值函数的次梯度：\n$$\n\\partial|x_i| = \\begin{cases} \\{1\\}  \\text{若 } x_i  0 \\\\ \\{-1\\}  \\text{若 } x_i  0 \\\\ [-1, 1]  \\text{若 } x_i = 0 \\end{cases}\n$$\n我们根据 $x_i^{\\star}$ 的值，分三种情况分析最优性条件 $0 \\in (x_i^{\\star} - v_i) + \\lambda \\, \\partial|x_i^{\\star}|$：\n\n情况 1：$x_i^{\\star}  0$。\n次梯度是一个单点：$\\partial f_i(x_i^{\\star}) = (x_i^{\\star} - v_i) + \\lambda(1)$。令其为 $0$ 得：\n$x_i^{\\star} - v_i + \\lambda = 0 \\implies x_i^{\\star} = v_i - \\lambda$。\n为了使该解与假设 $x_i^{\\star}  0$ 一致，我们必须有 $v_i - \\lambda  0$，即 $v_i  \\lambda$。\n\n情况 2：$x_i^{\\star}  0$。\n次梯度是一个单点：$\\partial f_i(x_i^{\\star}) = (x_i^{\\star} - v_i) + \\lambda(-1)$。令其为 $0$ 得：\n$x_i^{\\star} - v_i - \\lambda = 0 \\implies x_i^{\\star} = v_i + \\lambda$。\n为了使该解与假设 $x_i^{\\star}  0$ 一致，我们必须有 $v_i + \\lambda  0$，即 $v_i  -\\lambda$。\n\n情况 3：$x_i^{\\star} = 0$。\n最优性条件变为 $0 \\in (0 - v_i) + \\lambda [-1, 1]$，可简化为 $0 \\in -v_i + [-\\lambda, \\lambda]$。这等价于说 $v_i$ 必须在区间 $[-\\lambda, \\lambda]$ 内，即 $|v_i| \\le \\lambda$。\n\n总结坐标级最小化器 $x_i^{\\star}$ 的结果：\n$$\nx_i^{\\star} = \\begin{cases} v_i - \\lambda  \\text{若 } v_i  \\lambda \\\\ v_i + \\lambda  \\text{若 } v_i  -\\lambda \\\\ 0  \\text{若 } |v_i| \\le \\lambda \\end{cases}\n$$\n此操作称为软阈值（soft-thresholding），可以紧凑地写为 $x_i^{\\star} = \\operatorname{sgn}(v_i) \\max(0, |v_i| - \\lambda)$。\n\n现在，我们将此规则应用于提供的具体数据：$v = (\\,3,\\,-0.5,\\,-2,\\,0.1\\,)$ 和 $\\lambda = 1$。\n\n对于 $i=1$：$v_1 = 3$。由于 $v_1 = 3  \\lambda = 1$，我们处于第一种情况。\n$x_1^{\\star} = v_1 - \\lambda = 3 - 1 = 2$。\n\n对于 $i=2$：$v_2 = -0.5$。由于 $|v_2| = 0.5 \\le \\lambda = 1$，我们处于第三种情况。\n$x_2^{\\star} = 0$。\n\n对于 $i=3$：$v_3 = -2$。由于 $v_3 = -2  -\\lambda = -1$，我们处于第二种情况。\n$x_3^{\\star} = v_3 + \\lambda = -2 + 1 = -1$。\n\n对于 $i=4$：$v_4 = 0.1$。由于 $|v_4| = 0.1 \\le \\lambda = 1$，我们处于第三种情况。\n$x_4^{\\star} = 0$。\n\n所得向量为 $x^{\\star} = (\\,2,\\,0,\\,-1,\\,0\\,)$。\n\n$x^{\\star}$ 的支撑集是其非零元素的索引集合。使用基于 1 的索引，非零元素位于索引 1 和索引 3 处。因此，支撑集为 $\\{1, 3\\}$。\n\n最后，我们计算最小化器 $x^{\\star}$ 的欧几里得范数平方：\n$$\n\\|x^{\\star}\\|_{2}^{2} = \\sum_{i=1}^{4} (x_i^{\\star})^2 = (2)^2 + (0)^2 + (-1)^2 + (0)^2\n$$\n$$\n\\|x^{\\star}\\|_{2}^{2} = 4 + 0 + 1 + 0 = 5\n$$\n欧几里得范数平方为 $5$。",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "在掌握了基本的稀疏化操作之后，我们来探索一种构建稀疏解的经典贪心算法——正交匹配追踪（OMP）。OMP 的策略是迭代地选择与当前信号残差最相关的“原子”，然后将信号投影到已选原子的空间上以更新表示。通过手动执行 OMP 的前两步，你将直观地理解这种逐步构建稀疏解的过程，并体会其在稀疏编码中的应用。",
            "id": "3477688",
            "problem": "考虑稀疏编码任务，其中一个信号被近似为字典中列向量的稀疏线性组合。设字典矩阵 $A \\in \\mathbb{R}^{3 \\times 4}$ 具有单位范数的列向量（原子），如下所示：\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  0  \\frac{1}{\\sqrt{2}} \\\\\n0  1  0  -\\frac{1}{\\sqrt{2}} \\\\\n0  0  1  0\n\\end{pmatrix},\n$$\n因此，原子为 $a_{1} = (1,0,0)^{\\top}$，$a_{2} = (0,1,0)^{\\top}$，$a_{3} = (0,0,1)^{\\top}$，以及 $a_{4} = \\left(\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}, 0\\right)^{\\top}$。设测量值为\n$$\ny \\;=\\; \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\n从欧几里得内积和到线性子空间的正交投影的基本定义出发，使用正交匹配追踪（OMP）算法：在每次迭代中，选择与当前残差具有最大绝对内积的原子，然后通过将测量值正交投影到所选原子的张成空间上来更新系数。将残差初始化为 $r_{0} = y$。请手动执行正好两次迭代，展示：每次迭代中选择的原子，每一步之后的残差，以及通过在所选支撑集上求解最小二乘问题获得的系数更新。\n\n完成两次迭代后，计算残差的欧几里得范数的平方，即 $\\|r_{2}\\|_{2}^{2}$。请将最终答案以单个实数的形式给出。无需四舍五入。",
            "solution": "我们按照正交匹配追踪 (OMP) 算法的步骤执行两次迭代。\n\n**初始化**\n- 迭代计数器: $k=0$\n- 初始残差: $r_0 = y = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix}$\n- 初始支撑集 (已选原子的索引): $S_0 = \\emptyset$\n\n---\n\n**第 1 次迭代**\n\n1.  **原子选择**: 找到与当前残差 $r_0$ 最相关的原子，即最大化 $|a_j^\\top r_0|$。\n    - $|a_1^\\top r_0| = |(1 \\cdot 3) + (0 \\cdot 1) + (0 \\cdot 2)| = 3$\n    - $|a_2^\\top r_0| = |(0 \\cdot 3) + (1 \\cdot 1) + (0 \\cdot 2)| = 1$\n    - $|a_3^\\top r_0| = |(0 \\cdot 3) + (0 \\cdot 1) + (1 \\cdot 2)| = 2$\n    - $|a_4^\\top r_0| = |(\\frac{1}{\\sqrt{2}} \\cdot 3) + (-\\frac{1}{\\sqrt{2}} \\cdot 1) + (0 \\cdot 2)| = |\\frac{2}{\\sqrt{2}}| = \\sqrt{2} \\approx 1.414$\n    \n    最大相关值为 3，对应于原子 $a_1$。因此，我们选择原子 $a_1$。\n\n2.  **更新支撑集**: $S_1 = S_0 \\cup \\{1\\} = \\{1\\}$。\n\n3.  **系数更新**: 求解最小二乘问题 $\\arg\\min_{x} \\|y - A_{S_1}x\\|_2^2$，其中 $A_{S_1} = [a_1] = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n    系数 $\\hat{x}_{S_1} = (A_{S_1}^\\top A_{S_1})^{-1} A_{S_1}^\\top y = (a_1^\\top a_1)^{-1} (a_1^\\top y) = (1)^{-1} (3) = 3$。\n    \n4.  **残差更新**: $r_1 = y - A_{S_1}\\hat{x}_{S_1} = y - a_1 \\cdot 3 = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} - 3 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$。\n\n---\n\n**第 2 次迭代**\n\n1.  **原子选择**: 找到与当前残差 $r_1$ 最相关的原子。\n    - $|a_1^\\top r_1| = |(1 \\cdot 0) + (0 \\cdot 1) + (0 \\cdot 2)| = 0$\n    - $|a_2^\\top r_1| = |(0 \\cdot 0) + (1 \\cdot 1) + (0 \\cdot 2)| = 1$\n    - $|a_3^\\top r_1| = |(0 \\cdot 0) + (0 \\cdot 1) + (1 \\cdot 2)| = 2$\n    - $|a_4^\\top r_1| = |(\\frac{1}{\\sqrt{2}} \\cdot 0) + (-\\frac{1}{\\sqrt{2}} \\cdot 1) + (0 \\cdot 2)| = |-\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}} \\approx 0.707$\n    \n    最大相关值为 2，对应于原子 $a_3$。因此，我们选择原子 $a_3$。\n\n2.  **更新支撑集**: $S_2 = S_1 \\cup \\{3\\} = \\{1, 3\\}$。\n\n3.  **系数更新**: 求解最小二乘问题 $\\arg\\min_{x} \\|y - A_{S_2}x\\|_2^2$，其中 $A_{S_2} = [a_1, a_3] = \\begin{pmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{pmatrix}$。\n    系数向量 $\\hat{x}_{S_2} = (A_{S_2}^\\top A_{S_2})^{-1} A_{S_2}^\\top y$。\n    - $A_{S_2}^\\top A_{S_2} = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2$。\n    - $A_{S_2}^\\top y = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}$。\n    - $\\hat{x}_{S_2} = I_2^{-1} \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 2 \\end{pmatrix}$。\n    所以，与原子 $a_1$ 和 $a_3$ 对应的系数分别为 3 和 2。\n    \n4.  **残差更新**: $r_2 = y - A_{S_2}\\hat{x}_{S_2} = y - (a_1 \\cdot 3 + a_3 \\cdot 2) = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} - \\left( \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix} \\right) = \\begin{pmatrix} 3 \\\\ 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n\n---\n\n**最终计算**\n\n两次迭代后得到的残差是 $r_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n其欧几里得范数的平方为：\n$$\n\\|r_2\\|_2^2 = 0^2 + 1^2 + 0^2 = 1\n$$",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "稀疏性的威力在于它能为传统的机器学习模型带来更强的可解释性。本练习将稀疏思想应用于一个核心的无监督学习方法：主成分分析（PCA）。你将通过执行一步阈值幂法（Thresholded Power Method），来为一个人为构造的协方差矩阵寻找稀疏主成分。这个过程清晰地展示了如何在经典的幂法迭代中嵌入稀疏性约束，从而从数据中提取出简洁且易于理解的模式。",
            "id": "3477667",
            "problem": "考虑一个稀疏主成分分析 (PCA) 的场景，其中总体协方差矩阵是根据一个单尖峰模型构建的。设协方差矩阵定义为\n$$\n\\Sigma \\;=\\; \\mu I_{3} \\;+\\; \\lambda\\, v v^{\\top},\n$$\n其中 $I_{3}$ 是 $3\\times 3$ 的单位矩阵，$\\mu = 1$，$\\lambda = 3$，植入的稀疏主特征向量为\n$$\nv \\;=\\; \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}.\n$$\n该构造产生了一个对称半正定协方差矩阵，适用于PCA的尖峰模型。在用于稀疏PCA的阈值幂法的一步中，执行以下操作：\n- 乘以协方差矩阵：给定当前迭代向量 $u \\in \\mathbb{R}^{3}$，计算 $y \\;=\\; \\Sigma u$。\n- 应用硬阈值处理以强制稀疏性：对于指定的稀疏度 $k$，应用算子 $H_{k}$，它保留 $y$ 中绝对值最大的 $k$ 个分量，并将其余分量置为 $0$。\n- 重新归一化到单位欧几里得范数：将下一个迭代向量设置为 $z \\;=\\; \\dfrac{H_{k}(y)}{\\|H_{k}(y)\\|_{2}}$。\n\n从归一化的各向同性向量开始迭代\n$$\nu_{0} \\;=\\; \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix},\n$$\n使用稀疏度 $k = 2$，并如上所述执行恰好一次迭代。仅使用提供的基本定义和标准线性代数运算，确定在这次单次迭代后得到的重新归一化向量 $z$ 的第一个分量。请用精确的解析表达式表示你的答案。不要近似。不需要单位。",
            "solution": "问题要求在稀疏PCA的阈值幂法迭代一次后得到的向量的第一个分量。该过程包括三个步骤：乘以协方差矩阵、硬阈值处理和重新归一化。\n\n给定参数如下：\n- 协方差矩阵结构：$\\Sigma = \\mu I_{3} + \\lambda v v^{\\top}$\n- 单位矩阵：$I_{3}$ 是 $3 \\times 3$ 的单位矩阵。\n- 标量参数：$\\mu = 1$ 和 $\\lambda = 3$。\n- 植入的稀疏特征向量：$v = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$。\n- 初始迭代向量：$u_{0} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix}$。\n- 硬阈值处理的稀疏度：$k=2$。\n\n我们按照规定进行单次迭代。\n\n**第1步：乘以协方差矩阵**\n我们计算向量 $y = \\Sigma u_{0}$。利用 $\\Sigma$ 的结构，可得：\n$$\ny = (\\mu I_{3} + \\lambda v v^{\\top}) u_{0} = \\mu u_{0} + \\lambda v (v^{\\top} u_{0})\n$$\n首先，我们计算标量积 $v^{\\top} u_{0}$：\n$$\nv^{\\top} u_{0} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{3}}\\right) + \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{3}}\\right) + (0)\\left(\\frac{1}{\\sqrt{3}}\\right) = \\frac{1}{\\sqrt{6}} + \\frac{1}{\\sqrt{6}} = \\frac{2}{\\sqrt{6}}\n$$\n现在我们可以将给定值 $\\mu=1$ 和 $\\lambda=3$ 代入 $y$ 的表达式中：\n$$\ny = 1 \\cdot u_{0} + 3 \\cdot v \\left(\\frac{2}{\\sqrt{6}}\\right) = u_{0} + \\frac{6}{\\sqrt{6}} v = u_{0} + \\sqrt{6} v\n$$\n代入向量 $u_0$ 和 $v$：\n$$\ny = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} + \\sqrt{6} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} + \\begin{pmatrix} \\frac{\\sqrt{6}}{\\sqrt{2}} \\\\ \\frac{\\sqrt{6}}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} + \\begin{pmatrix} \\sqrt{3} \\\\ \\sqrt{3} \\\\ 0 \\end{pmatrix}\n$$\n合并各项得到 $y$ 的分量：\n$$\ny = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} + \\sqrt{3} \\\\ \\frac{1}{\\sqrt{3}} + \\sqrt{3} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1+3}{\\sqrt{3}} \\\\ \\frac{1+3}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix}\n$$\n\n**第2步：应用硬阈值处理**\n硬阈值算子 $H_{k}(y)$ 保留 $y$ 中绝对值最大的 $k$ 个分量，并将其余分量置为 $0$。这里 $k=2$。\n$y$ 的分量为 $y_1 = \\frac{4}{\\sqrt{3}}$，$y_2 = \\frac{4}{\\sqrt{3}}$ 和 $y_3 = \\frac{1}{\\sqrt{3}}$。\n绝对值为 $|y_1| = \\frac{4}{\\sqrt{3}}$，$|y_2| = \\frac{4}{\\sqrt{3}}$ 和 $|y_3| = \\frac{1}{\\sqrt{3}}$。\n显然，$|y_1|$ 和 $|y_2|$ 是两个最大的值。因此，我们保留前两个分量，并将第三个分量置为 $0$。我们用 $w$ 表示阈值处理后的向量：\n$$\nw = H_{2}(y) = \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}\n$$\n\n**第3步：重新归一化到单位欧几里得范数**\n最后一步是通过归一化 $w$ 来计算下一个迭代向量 $z$：\n$$\nz = \\frac{w}{\\|w\\|_{2}}\n$$\n首先，我们计算欧几里得范数的平方 $\\|w\\|_{2}^{2}$：\n$$\n\\|w\\|_{2}^{2} = \\left(\\frac{4}{\\sqrt{3}}\\right)^{2} + \\left(\\frac{4}{\\sqrt{3}}\\right)^{2} + 0^{2} = \\frac{16}{3} + \\frac{16}{3} = \\frac{32}{3}\n$$\n因此，欧几里得范数为：\n$$\n\\|w\\|_{2} = \\sqrt{\\frac{32}{3}} = \\frac{\\sqrt{32}}{\\sqrt{3}} = \\frac{4\\sqrt{2}}{\\sqrt{3}}\n$$\n现在我们计算 $z$：\n$$\nz = \\frac{1}{\\|w\\|_{2}} w = \\frac{1}{\\frac{4\\sqrt{2}}{\\sqrt{3}}} \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ 0 \\end{pmatrix} = \\frac{\\sqrt{3}}{4\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}\n$$\n执行标量乘法：\n$$\nz = \\begin{pmatrix} \\frac{\\sqrt{3}}{4\\sqrt{2}} \\cdot \\frac{4}{\\sqrt{3}} \\\\ \\frac{\\sqrt{3}}{4\\sqrt{2}} \\cdot \\frac{4}{\\sqrt{3}} \\\\ \\frac{\\sqrt{3}}{4\\sqrt{2}} \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\n问题要求的是重新归一化向量 $z$ 的第一个分量。这是向量 $z$ 的第一个元素。\n$$\nz_1 = \\frac{1}{\\sqrt{2}}\n$$",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2}}}\n$$"
        }
    ]
}