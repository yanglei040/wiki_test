{
    "hands_on_practices": [
        {
            "introduction": "Many advanced algorithms for sparse optimization, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA), rely on a fundamental operation known as the proximal operator of the $\\ell_1$-norm. This operator, more commonly called the \"soft-thresholding\" function, is the key to inducing sparsity during optimization. This exercise guides you through the process of deriving this operator from first principles of convex optimization and applying it to a concrete vector, providing a foundational understanding of how sparsity is computationally achieved .",
            "id": "3477691",
            "problem": "Consider the proximal mapping associated with the Least Absolute Shrinkage and Selection Operator (LASSO) penalty. For a given vector $v \\in \\mathbb{R}^{n}$ and a regularization parameter $\\lambda \\geq 0$, define the proximal operator of the $\\ell_{1}$-norm as\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v) \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\nStarting from the subgradient optimality condition for convex optimization and the definition of the $\\ell_{1}$-norm, derive the coordinate-wise minimizer $x^{\\star}$ of the above problem in terms of $v$ and $\\lambda$. Then, for the specific data $v = (\\,3,\\,-0.5,\\,-2,\\,0.1\\,)$ and $\\lambda = 1$, compute the resulting vector $x^{\\star}$ and identify its support, understood as the index set of its nonzero entries under $1$-based indexing.\n\nFinally, compute the squared Euclidean norm $\\|x^{\\star}\\|_{2}^{2}$ of the minimizer and report this value as your final answer. No rounding is required; provide the exact value as a real number.",
            "solution": "The user wants me to solve this problem.\n### Step 1: Extract Givens\n- **Proximal operator definition**: $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{1}}(v) \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}$\n- **Input vector**: $v \\in \\mathbb{R}^{n}$\n- **Regularization parameter**: $\\lambda \\geq 0$\n- **Specific data**: $v = (\\,3,\\,-0.5,\\,-2,\\,0.1\\,)$ and $\\lambda = 1$.\n- **Objective 1**: Derive the coordinate-wise minimizer $x^{\\star}$.\n- **Objective 2**: Compute $x^{\\star}$ for the given data and identify its support (1-based indexing).\n- **Objective 3**: Compute the squared Euclidean norm $\\|x^{\\star}\\|_{2}^{2}$ of the minimizer.\n- **Final Answer**: Report the exact value of $\\|x^{\\star}\\|_{2}^{2}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a standard, fundamental problem in convex optimization, statistics, and machine learning, specifically related to the LASSO method. The concepts of proximal operators, $\\ell_{1}$ and $\\ell_{2}$ norms, and subgradient calculus are well-established mathematical principles. The problem is scientifically sound.\n- **Well-Posed**: The objective function $F(x) = \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ is the sum of a strictly convex function (the squared Euclidean norm term) and a convex function (the $\\ell_{1}$-norm term). The sum is therefore strictly convex. A strictly convex function defined over a convex set ($\\mathbb{R}^{n}$) has a unique minimizer. Thus, the problem is well-posed.\n- **Objective**: The problem is stated using precise, unambiguous mathematical terminology.\n- **Completeness and Consistency**: All necessary definitions, variables, and data are provided. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to derive the solution.\n\nThe objective function to minimize is\n$$\nF(x) = \\frac{1}{2} \\|x - v\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nwhere $x, v \\in \\mathbb{R}^{n}$ and $\\lambda \\geq 0$. We can rewrite the objective function by expanding the norms as a sum over the components:\n$$\nF(x) = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - v_i)^2 + \\lambda \\sum_{i=1}^{n} |x_i| = \\sum_{i=1}^{n} \\left( \\frac{1}{2} (x_i - v_i)^2 + \\lambda |x_i| \\right)\n$$\nSince the objective function is separable, we can minimize it by finding the minimizer for each component $x_i$ independently. For each $i \\in \\{1, \\dots, n\\}$, we need to solve the one-dimensional optimization problem:\n$$\nx_i^{\\star} = \\arg\\min_{x_i \\in \\mathbb{R}} \\left\\{ f_i(x_i) \\right\\} \\quad \\text{where} \\quad f_i(x_i) = \\frac{1}{2} (x_i - v_i)^2 + \\lambda |x_i|\n$$\nThe function $f_i(x_i)$ is convex. The term $|x_i|$ is non-differentiable at $x_i = 0$, so we must use subgradient calculus. The optimality condition for a minimizer $x_i^{\\star}$ is that zero must be in the subgradient of $f_i$ at $x_i^{\\star}$:\n$$\n0 \\in \\partial f_i(x_i^{\\star})\n$$\nThe subgradient of $f_i(x_i)$ is given by:\n$$\n\\partial f_i(x_i) = (x_i - v_i) + \\lambda \\, \\partial|x_i|\n$$\nwhere $\\partial|x_i|$ is the subgradient of the absolute value function:\n$$\n\\partial|x_i| = \\begin{cases} \\{1\\}  \\text{if } x_i  0 \\\\ \\{-1\\}  \\text{if } x_i  0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\nWe analyze the optimality condition $0 \\in (x_i^{\\star} - v_i) + \\lambda \\, \\partial|x_i^{\\star}|$ for three cases based on the value of $x_i^{\\star}$:\n\nCase 1: $x_i^{\\star}  0$.\nThe subgradient is a single point: $\\partial f_i(x_i^{\\star}) = (x_i^{\\star} - v_i) + \\lambda(1)$. Setting this to $0$ gives:\n$x_i^{\\star} - v_i + \\lambda = 0 \\implies x_i^{\\star} = v_i - \\lambda$.\nFor this solution to be consistent with the assumption $x_i^{\\star}  0$, we must have $v_i - \\lambda  0$, which means $v_i  \\lambda$.\n\nCase 2: $x_i^{\\star}  0$.\nThe subgradient is a single point: $\\partial f_i(x_i^{\\star}) = (x_i^{\\star} - v_i) + \\lambda(-1)$. Setting this to $0$ gives:\n$x_i^{\\star} - v_i - \\lambda = 0 \\implies x_i^{\\star} = v_i + \\lambda$.\nFor this solution to be consistent with the assumption $x_i^{\\star}  0$, we must have $v_i + \\lambda  0$, which means $v_i  -\\lambda$.\n\nCase 3: $x_i^{\\star} = 0$.\nThe optimality condition becomes $0 \\in (0 - v_i) + \\lambda [-1, 1]$, which simplifies to $0 \\in -v_i + [-\\lambda, \\lambda]$. This is equivalent to saying that $v_i$ must be in the interval $[-\\lambda, \\lambda]$, or $|v_i| \\le \\lambda$.\n\nSummarizing the results for the coordinate-wise minimizer $x_i^{\\star}$:\n$$\nx_i^{\\star} = \\begin{cases} v_i - \\lambda  \\text{if } v_i  \\lambda \\\\ v_i + \\lambda  \\text{if } v_i  -\\lambda \\\\ 0  \\text{if } |v_i| \\le \\lambda \\end{cases}\n$$\nThis operation is known as soft-thresholding and can be written compactly as $x_i^{\\star} = \\operatorname{sgn}(v_i) \\max(0, |v_i| - \\lambda)$.\n\nNow, we apply this rule to the specific data provided: $v = (\\,3,\\,-0.5,\\,-2,\\,0.1\\,)$ and $\\lambda = 1$.\n\nFor $i=1$: $v_1 = 3$. Since $v_1 = 3  \\lambda = 1$, we are in the first case.\n$x_1^{\\star} = v_1 - \\lambda = 3 - 1 = 2$.\n\nFor $i=2$: $v_2 = -0.5$. Since $|v_2| = 0.5 \\le \\lambda = 1$, we are in the third case.\n$x_2^{\\star} = 0$.\n\nFor $i=3$: $v_3 = -2$. Since $v_3 = -2  -\\lambda = -1$, we are in the second case.\n$x_3^{\\star} = v_3 + \\lambda = -2 + 1 = -1$.\n\nFor $i=4$: $v_4 = 0.1$. Since $|v_4| = 0.1 \\le \\lambda = 1$, we are in the third case.\n$x_4^{\\star} = 0$.\n\nThe resulting vector is $x^{\\star} = (\\,2,\\,0,\\,-1,\\,0\\,)$.\n\nThe support of $x^{\\star}$ is the set of indices of its non-zero entries. Using $1$-based indexing, the non-zero entries are at index $1$ and index $3$. Thus, the support is $\\{1, 3\\}$.\n\nFinally, we compute the squared Euclidean norm of the minimizer $x^{\\star}$:\n$$\n\\|x^{\\star}\\|_{2}^{2} = \\sum_{i=1}^{4} (x_i^{\\star})^2 = (2)^2 + (0)^2 + (-1)^2 + (0)^2\n$$\n$$\n\\|x^{\\star}\\|_{2}^{2} = 4 + 0 + 1 + 0 = 5\n$$\nThe squared Euclidean norm is $5$.",
            "answer": "$$\n\\boxed{5}\n$$"
        },
        {
            "introduction": "Beyond regression, sparsity is a powerful concept in unsupervised learning, particularly for enhancing the interpretability of dimensionality reduction techniques. Standard Principal Component Analysis (PCA) often produces dense principal components that involve all original features, making them difficult to interpret. This exercise introduces Sparse PCA, where the goal is to find sparse components that are easier to understand. You will perform one iteration of the thresholded power method, a practical algorithm for Sparse PCA, to see how sparsity can be integrated into classic matrix factorization techniques .",
            "id": "3477667",
            "problem": "Consider a sparse Principal Component Analysis (PCA) setting in which the population covariance matrix is constructed according to a single-spike model. Let the covariance matrix be defined by\n$$\n\\Sigma \\;=\\; \\mu I_{3} \\;+\\; \\lambda\\, v v^{\\top},\n$$\nwhere $I_{3}$ is the $3\\times 3$ identity matrix, $\\mu = 1$, $\\lambda = 3$, and the planted sparse leading eigenvector is\n$$\nv \\;=\\; \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\[4pt] \\frac{1}{\\sqrt{2}} \\\\[4pt] 0 \\end{pmatrix}.\n$$\nThis construction yields a symmetric positive semidefinite covariance matrix appropriate for a spiked model of PCA. In one step of the thresholded power method used for sparse PCA, the following operations are performed:\n- Multiply by the covariance matrix: given a current iterate $u \\in \\mathbb{R}^{3}$, compute $y \\;=\\; \\Sigma u$.\n- Apply hard-thresholding to enforce sparsity: for a specified sparsity level $k$, apply the operator $H_{k}$ that retains the $k$ entries of $y$ with largest absolute values and sets the remaining entries to $0$.\n- Renormalize to unit Euclidean norm: set the next iterate to $z \\;=\\; \\dfrac{H_{k}(y)}{\\|H_{k}(y)\\|_{2}}$.\n\nStart the iteration from the normalized isotropic vector\n$$\nu_{0} \\;=\\; \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\[4pt] \\frac{1}{\\sqrt{3}} \\\\[4pt] \\frac{1}{\\sqrt{3}} \\end{pmatrix},\n$$\nuse sparsity level $k = 2$, and perform exactly one iteration as specified above. Using only the fundamental definitions provided and standard linear algebra operations, determine the first component of the renormalized vector $z$ obtained after this single iteration. Express your answer as an exact analytic expression. Do not approximate. No units are required.",
            "solution": "The problem asks for the first component of the vector obtained after one iteration of the thresholded power method for sparse PCA. The process consists of three steps: multiplication by the covariance matrix, hard-thresholding, and renormalization.\n\nThe given parameters are:\n- The covariance matrix structure: $\\Sigma = \\mu I_{3} + \\lambda v v^{\\top}$\n- Identity matrix: $I_{3}$ is the $3 \\times 3$ identity matrix.\n- Scalar parameters: $\\mu = 1$ and $\\lambda = 3$.\n- The planted sparse eigenvector: $v = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$.\n- The initial iterate: $u_{0} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix}$.\n- The sparsity level for hard-thresholding: $k=2$.\n\nWe proceed with the single iteration as specified.\n\n**Step 1: Multiply by the covariance matrix**\nWe compute the vector $y = \\Sigma u_{0}$. Using the structure of $\\Sigma$, this is given by:\n$$\ny = (\\mu I_{3} + \\lambda v v^{\\top}) u_{0} = \\mu u_{0} + \\lambda v (v^{\\top} u_{0})\n$$\nFirst, we compute the scalar product $v^{\\top} u_{0}$:\n$$\nv^{\\top} u_{0} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{3}}\\right) + \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{3}}\\right) + (0)\\left(\\frac{1}{\\sqrt{3}}\\right) = \\frac{1}{\\sqrt{6}} + \\frac{1}{\\sqrt{6}} = \\frac{2}{\\sqrt{6}}\n$$\nNow we can substitute the given values $\\mu=1$ and $\\lambda=3$ into the expression for $y$:\n$$\ny = 1 \\cdot u_{0} + 3 \\cdot v \\left(\\frac{2}{\\sqrt{6}}\\right) = u_{0} + \\frac{6}{\\sqrt{6}} v = u_{0} + \\sqrt{6} v\n$$\nSubstituting the vectors $u_0$ and $v$:\n$$\ny = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} + \\sqrt{6} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} + \\begin{pmatrix} \\frac{\\sqrt{6}}{\\sqrt{2}} \\\\ \\frac{\\sqrt{6}}{\\sqrt{2}} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} + \\begin{pmatrix} \\sqrt{3} \\\\ \\sqrt{3} \\\\ 0 \\end{pmatrix}\n$$\nCombining the terms gives the components of $y$:\n$$\ny = \\begin{pmatrix} \\frac{1}{\\sqrt{3}} + \\sqrt{3} \\\\ \\frac{1}{\\sqrt{3}} + \\sqrt{3} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1+3}{\\sqrt{3}} \\\\ \\frac{1+3}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{pmatrix}\n$$\n\n**Step 2: Apply hard-thresholding**\nThe hard-thresholding operator $H_{k}(y)$ retains the $k$ entries of $y$ with the largest absolute values and sets the others to $0$. Here, $k=2$.\nThe components of $y$ are $y_1 = \\frac{4}{\\sqrt{3}}$, $y_2 = \\frac{4}{\\sqrt{3}}$, and $y_3 = \\frac{1}{\\sqrt{3}}$.\nThe absolute values are $|y_1| = \\frac{4}{\\sqrt{3}}$, $|y_2| = \\frac{4}{\\sqrt{3}}$, and $|y_3| = \\frac{1}{\\sqrt{3}}$.\nClearly, $|y_1|$ and $|y_2|$ are the two largest values. Therefore, we keep the first two components and set the third to $0$. Let's denote the thresholded vector by $w$:\n$$\nw = H_{2}(y) = \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}\n$$\n\n**Step 3: Renormalize to unit Euclidean norm**\nThe final step is to compute the next iterate $z$ by normalizing $w$:\n$$\nz = \\frac{w}{\\|w\\|_{2}}\n$$\nFirst, we calculate the squared Euclidean norm $\\|w\\|_{2}^{2}$:\n$$\n\\|w\\|_{2}^{2} = \\left(\\frac{4}{\\sqrt{3}}\\right)^{2} + \\left(\\frac{4}{\\sqrt{3}}\\right)^{2} + 0^{2} = \\frac{16}{3} + \\frac{16}{3} = \\frac{32}{3}\n$$\nThe Euclidean norm is therefore:\n$$\n\\|w\\|_{2} = \\sqrt{\\frac{32}{3}} = \\frac{\\sqrt{32}}{\\sqrt{3}} = \\frac{4\\sqrt{2}}{\\sqrt{3}}\n$$\nNow we compute $z$:\n$$\nz = \\frac{1}{\\|w\\|_{2}} w = \\frac{1}{\\frac{4\\sqrt{2}}{\\sqrt{3}}} \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ 0 \\end{pmatrix} = \\frac{\\sqrt{3}}{4\\sqrt{2}} \\begin{pmatrix} \\frac{4}{\\sqrt{3}} \\\\ \\frac{4}{\\sqrt{3}} \\\\ 0 \\end{pmatrix}\n$$\nPerforming the scalar multiplication:\n$$\nz = \\begin{pmatrix} \\frac{\\sqrt{3}}{4\\sqrt{2}} \\cdot \\frac{4}{\\sqrt{3}} \\\\ \\frac{\\sqrt{3}}{4\\sqrt{2}} \\cdot \\frac{4}{\\sqrt{3}} \\\\ \\frac{\\sqrt{3}}{4\\sqrt{2}} \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}\n$$\nThe problem asks for the first component of the renormalized vector $z$. This is the first entry of the vector $z$.\n$$\nz_1 = \\frac{1}{\\sqrt{2}}\n$$",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{2}}}\n$$"
        },
        {
            "introduction": "In the realm of supervised learning, sparsity provides a powerful mechanism for automatic feature selection. This practice problem demonstrates this by incorporating $\\ell_1$-regularization into the classic Support Vector Machine (SVM) framework to create a \"sparse SVM\". By encouraging a sparse weight vector, the model simultaneously learns a classification boundary and identifies the most relevant features. Solving this problem for a small, illustrative dataset will provide hands-on experience with the optimization principles, such as the KKT conditions, that govern the trade-off between margin maximization and feature selection .",
            "id": "3477686",
            "problem": "Consider a binary classification task with two features. You are given four training examples $\\{(x_i, y_i)\\}_{i=1}^{4}$ with labels $y_i \\in \\{-1, +1\\}$:\n$$\nx_1 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix},\\quad y_1 = +1;\\qquad\nx_2 = \\begin{pmatrix} 3 \\\\ -\\frac{1}{2} \\end{pmatrix},\\quad y_2 = +1;\n$$\n$$\nx_3 = \\begin{pmatrix} -1 \\\\ \\frac{1}{5} \\end{pmatrix},\\quad y_3 = -1;\\qquad\nx_4 = \\begin{pmatrix} -2 \\\\ \\frac{1}{10} \\end{pmatrix},\\quad y_4 = -1.\n$$\nAssume these data are linearly separable. Consider the sparse Support Vector Machine (SVM) with $\\ell_1$-regularization on the weights, defined by the primal optimization problem\n$$\n\\min_{w \\in \\mathbb{R}^2,\\, b \\in \\mathbb{R},\\, \\xi \\in \\mathbb{R}^4} \\ \\|w\\|_{1} + C \\sum_{i=1}^{4} \\xi_i\n$$\nsubject to the margin constraints\n$$\ny_i \\left( w^{\\top} x_i + b \\right) \\geq 1 - \\xi_i,\\quad \\xi_i \\geq 0,\\quad i=1,\\dots,4,\n$$\nwhere $\\|w\\|_{1} = |w_1| + |w_2|$ is the $\\ell_1$-norm and $C = 10$ is a given regularization parameter. Start from the fundamental definitions of the hinge loss, margin constraints, and $\\ell_1$-regularization, and using first-principles reasoning, solve this optimization problem by hand. Derive the sparse separating hyperplane and provide the optimal parameters $(w^\\star, b^\\star)$. Express your final answer as a single row matrix containing $(w_1^\\star, w_2^\\star, b^\\star)$ with exact rational values. No rounding is required.",
            "solution": "The problem asks for the optimal parameters $(w^\\star, b^\\star)$ of a sparse Support Vector Machine (SVM) with $\\ell_1$-regularization for a given binary classification dataset. The solution is found by solving the specified primal optimization problem.\n\nThe primal optimization problem is given by:\n$$\n\\min_{w \\in \\mathbb{R}^2,\\, b \\in \\mathbb{R},\\, \\xi \\in \\mathbb{R}^4} \\ \\|w\\|_{1} + C \\sum_{i=1}^{4} \\xi_i\n$$\nsubject to the constraints:\n$$\ny_i \\left( w^{\\top} x_i + b \\right) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\text{for } i=1, \\dots, 4.\n$$\nHere, $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ is the weight vector, $b$ is the bias, $\\xi_i$ are slack variables measuring the degree of misclassification, $\\|w\\|_1 = |w_1| + |w_2|$ is the $\\ell_1$-norm which promotes sparsity in $w$, and $C=10$ is the regularization parameter that balances the trade-off between the norm of the weights and the classification error. The constraints $y_i(w^\\top x_i + b) \\ge 1 - \\xi_i$ define the soft margin. The term $\\sum \\xi_i$ is related to the hinge loss.\n\nThis is a convex optimization problem, as the objective function is a sum of convex functions ($\\ell_1$-norm and a linear term in $\\xi_i$), and the constraints define a convex feasible set. Therefore, any point that satisfies the Karush-Kuhn-Tucker (KKT) conditions is a global minimum.\n\nWe introduce Lagrange multipliers $\\alpha_i \\ge 0$ for the margin constraints and $\\mu_i \\ge 0$ for the non-negativity constraints on $\\xi_i$. The Lagrangian is:\n$$\nL(w, b, \\xi, \\alpha, \\mu) = \\|w\\|_1 + C \\sum_{i=1}^{4} \\xi_i + \\sum_{i=1}^{4} \\alpha_i (1 - \\xi_i - y_i(w^{\\top} x_i + b)) - \\sum_{i=1}^{4} \\mu_i \\xi_i\n$$\n\nThe KKT conditions for an optimal solution $(w^\\star, b^\\star, \\xi^\\star, \\alpha^\\star, \\mu^\\star)$ are:\n1.  **Stationarity:** The gradient of the Lagrangian with respect to the primal variables must be zero (or contain zero for non-differentiable parts).\n    -   $\\nabla_w L = 0 \\implies \\mathbf{0} \\in \\partial_w \\|w^\\star\\|_1 - \\sum_{i=1}^{4} \\alpha_i^\\star y_i x_i$\n    -   $\\frac{\\partial L}{\\partial b} = 0 \\implies \\sum_{i=1}^{4} \\alpha_i^\\star y_i = 0$\n    -   $\\frac{\\partial L}{\\partial \\xi_i} = 0 \\implies C - \\alpha_i^\\star - \\mu_i^\\star = 0$ for $i=1,\\dots,4$\n\n2.  **Primal Feasibility:**\n    -   $y_i(w^{\\star\\top}x_i + b^\\star) \\geq 1 - \\xi_i^\\star$\n    -   $\\xi_i^\\star \\geq 0$\n\n3.  **Dual Feasibility:**\n    -   $\\alpha_i^\\star \\geq 0$\n    -   $\\mu_i^\\star \\geq 0$\n\n4.  **Complementary Slackness:**\n    -   $\\alpha_i^\\star (1 - \\xi_i^\\star - y_i(w^{\\star\\top} x_i + b^\\star)) = 0$\n    -   $\\mu_i^\\star \\xi_i^\\star = 0$\n\nFrom $C - \\alpha_i^\\star - \\mu_i^\\star = 0$ and $\\mu_i^\\star \\ge 0$, we get $0 \\le \\alpha_i^\\star \\le C$.\n\nThe data points are:\n$x_1 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, y_1 = +1$; $x_2 = \\begin{pmatrix} 3 \\\\ -1/2 \\end{pmatrix}, y_2 = +1$; $x_3 = \\begin{pmatrix} -1 \\\\ 1/5 \\end{pmatrix}, y_3 = -1$; $x_4 = \\begin{pmatrix} -2 \\\\ 1/10 \\end{pmatrix}, y_4 = -1$.\nA visual inspection of the data reveals that the first feature, $x^{(1)}$, seems to be highly discriminative. Points with $x^{(1)}  0$ have label $y=+1$, while points with $x^{(1)}  0$ have label $y=-1$. This suggests a sparse solution where $w_2^\\star = 0$. We will proceed with this hypothesis.\n\nLet's assume $w_2^\\star = 0$ and, because the data is linearly separable, let's seek a solution with no margin violations, i.e., $\\xi_i^\\star = 0$ for all $i$. The problem simplifies to finding $w_1, b$ that minimize $|w_1|$ subject to:\n$y_i(w_1 x_{i1} + b) \\ge 1$ for all $i$.\nFrom the data, we assume $w_1  0$. The objective is to minimize $w_1$.\nThe constraints become:\n-   $i=1$: $1(w_1 \\cdot 2 + b) \\ge 1 \\implies 2w_1 + b \\ge 1 \\implies b \\ge 1 - 2w_1$.\n-   $i=2$: $1(w_1 \\cdot 3 + b) \\ge 1 \\implies 3w_1 + b \\ge 1$. Since $w_1  0$, this is less restrictive on $b$ than the first constraint.\n-   $i=3$: $-1(w_1 \\cdot (-1) + b) \\ge 1 \\implies w_1 - b \\ge 1 \\implies b \\le w_1 - 1$.\n-   $i=4$: $-1(w_1 \\cdot (-2) + b) \\ge 1 \\implies 2w_1 - b \\ge 1$. Since $w_1  0$, this is less restrictive on $b$ than the third constraint.\n\nTo find the minimum $w_1  0$, we need the feasible region for $b$ to be non-empty:\n$1 - 2w_1 \\le b \\le w_1 - 1$\nThis requires $1 - 2w_1 \\le w_1 - 1$, which implies $2 \\le 3w_1$, so $w_1 \\ge \\frac{2}{3}$.\nThe minimum value for $w_1$ is $\\frac{2}{3}$. At this minimum, the lower and upper bounds for $b$ are equal:\n$b = 1 - 2(\\frac{2}{3}) = 1 - \\frac{4}{3} = -\\frac{1}{3}$.\n$b = \\frac{2}{3} - 1 = -\\frac{1}{3}$.\n\nThis gives a candidate solution: $w^\\star = \\begin{pmatrix} 2/3 \\\\ 0 \\end{pmatrix}$ and $b^\\star = -1/3$, with all $\\xi_i^\\star = 0$.\nNow, we must verify if this solution satisfies the KKT conditions for the original problem.\nThe data points for which the margin constraint $y_i(w^{\\star\\top}x_i+b^\\star) = 1$ holds are the support vectors.\n-   $i=1$: $y_1(w^{\\star\\top}x_1+b^\\star) = 1(\\frac{2}{3} \\cdot 2 + 0 - \\frac{1}{3}) = \\frac{4}{3} - \\frac{1}{3} = 1$. This is a support vector.\n-   $i=2$: $y_2(w^{\\star\\top}x_2+b^\\star) = 1(\\frac{2}{3} \\cdot 3 + 0 - \\frac{1}{3}) = 2 - \\frac{1}{3} = \\frac{5}{3}  1$. Not a support vector.\n-   $i=3$: $y_3(w^{\\star\\top}x_3+b^\\star) = -1(\\frac{2}{3} \\cdot (-1) + 0 - \\frac{1}{3}) = -1(-\\frac{2}{3} - \\frac{1}{3}) = 1$. This is a support vector.\n-   $i=4$: $y_4(w^{\\star\\top}x_4+b^\\star) = -1(\\frac{2}{3} \\cdot (-2) + 0 - \\frac{1}{3}) = -1(-\\frac{4}{3} - \\frac{1}{3}) = \\frac{5}{3}  1$. Not a support vector.\n\nPoints $x_2$ and $x_4$ are correctly classified and outside the margin. By complementary slackness ($\\alpha_i(1-\\xi_i-y_i(\\dots))=0$), since $(1-\\xi_i-y_i(\\dots)) \\ne 0$ for $i=2,4$, we must have $\\alpha_2^\\star=0$ and $\\alpha_4^\\star=0$.\nThe support vectors are $x_1$ and $x_3$, so $\\alpha_1^\\star$ and $\\alpha_3^\\star$ can be non-zero. They must satisfy the stationarity conditions.\nFrom $\\sum \\alpha_i^\\star y_i = 0$:\n$\\alpha_1^\\star y_1 + \\alpha_2^\\star y_2 + \\alpha_3^\\star y_3 + \\alpha_4^\\star y_4 = 0$\n$\\alpha_1^\\star(1) + 0 + \\alpha_3^\\star(-1) + 0 = 0 \\implies \\alpha_1^\\star = \\alpha_3^\\star$.\n\nThe subgradient of the $\\ell_1$-norm at $w^\\star = (2/3, 0)$ is $\\partial \\|w^\\star\\|_1 = \\begin{pmatrix} \\text{sgn}(2/3) \\\\ [-1, 1] \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ [-1, 1] \\end{pmatrix}$.\nThe stationarity condition for $w$ is $\\sum_{i=1}^4 \\alpha_i^\\star y_i x_i \\in \\partial \\|w^\\star\\|_1$.\n$\\alpha_1^\\star y_1 x_1 + \\alpha_3^\\star y_3 x_3 = \\alpha_1^\\star(1)\\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\alpha_3^\\star(-1)\\begin{pmatrix} -1 \\\\ 1/5 \\end{pmatrix} = \\begin{pmatrix} 2\\alpha_1^\\star + \\alpha_3^\\star \\\\ -\\alpha_3^\\star/5 \\end{pmatrix}$.\n\nThis vector must be in $\\partial \\|w^\\star\\|_1$.\n1.  For the first component: $2\\alpha_1^\\star + \\alpha_3^\\star = 1$.\n2.  For the second component: $-\\alpha_3^\\star/5 \\in [-1, 1]$.\n\nWe have a system of two equations for $\\alpha_1^\\star$ and $\\alpha_3^\\star$:\n1.  $\\alpha_1^\\star = \\alpha_3^\\star$\n2.  $2\\alpha_1^\\star + \\alpha_3^\\star = 1$\n\nSubstituting (1) into (2): $2\\alpha_1^\\star + \\alpha_1^\\star = 1 \\implies 3\\alpha_1^\\star = 1 \\implies \\alpha_1^\\star = 1/3$.\nThus, $\\alpha_1^\\star = 1/3$ and $\\alpha_3^\\star = 1/3$.\n\nWe verify all conditions with $\\alpha^\\star = (1/3, 0, 1/3, 0)$:\n-   Dual feasibility: $0 \\le \\alpha_i^\\star \\le C=10$. This is true, as $0 \\le 1/3 \\le 10$.\n-   Stationarity for $w_2$: We check if $-\\alpha_3^\\star/5 \\in [-1, 1]$.\n    $-\\frac{1/3}{5} = -1/15$. Since $-1 \\le -1/15 \\le 1$, this condition is satisfied.\n-   All other KKT conditions (primal feasibility, complementary slackness for $\\xi_i$ and $\\mu_i$) were used to derive this solution or are satisfied by construction. For instance, with $\\xi_i^\\star=0$, $\\mu_i^\\star \\xi_i^\\star = 0$ is satisfied. And $\\mu_i^\\star = C-\\alpha_i^\\star \\ge 0$ is satisfied since $C=10$ is greater than all $\\alpha_i^\\star$.\n\nSince the problem is convex and we have found a point $(w^\\star, b^\\star, \\xi^\\star)$ and dual variables $(\\alpha^\\star, \\mu^\\star)$ that satisfy all KKT conditions, this point is the global optimum.\nThe optimal parameters are $w_1^\\star = 2/3$, $w_2^\\star = 0$, and $b^\\star = -1/3$.\n\nThe sparse separating hyperplane is given by $w^{\\star\\top}x+b^\\star=0$, which is $\\frac{2}{3}x_1 - \\frac{1}{3} = 0$, or $x_1 = 1/2$. This is a vertical line, consistent with our sparsity assumption.\n\nThe final answer is required as a row matrix of the optimal parameters $(w_1^\\star, w_2^\\star, b^\\star)$.\n$w_1^\\star = \\frac{2}{3}$\n$w_2^\\star = 0$\n$b^\\star = -\\frac{1}{3}$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3}  0  -\\frac{1}{3} \\end{pmatrix}}\n$$"
        }
    ]
}