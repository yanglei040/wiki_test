{
    "hands_on_practices": [
        {
            "introduction": "稀疏逻辑回归是在高维数据中进行特征选择和分类的关键工具。要深刻理解并开发相关的优化算法，掌握其数学结构至关重要。本练习将引导你推导逻辑损失函数的梯度和海森矩阵，并构建 $\\ell_1$ 正则化问题的KKT（Karush-Kuhn-Tucker）最优性条件，这是分析和解决这类问题的基础。",
            "id": "3476956",
            "problem": "给定训练数据 $\\{(x_{i},y_{i})\\}_{i=1}^{n}$，其中 $x_{i} \\in \\mathbb{R}^{p}$ 且 $y_{i} \\in \\{-1,+1\\}$。考虑用于二元分类的逻辑斯蒂损失，定义为\n$$\n\\ell(\\beta) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\log\\!\\big(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta)\\big), \\quad \\beta \\in \\mathbb{R}^{p}.\n$$\n仅从向量微积分中的链式法则、梯度和海森矩阵的定义以及$\\ell_{1}$范数的次微分出发，完成以下任务：\n\n1.  显式推导梯度 $\\nabla \\ell(\\beta)$，其形式为关于数据点的和，并用 $x_{i}$、$y_{i}$ 和 $x_{i}^{\\top}\\beta$ 表示。\n2.  显式推导海森矩阵 $\\nabla^{2}\\ell(\\beta)$，并使用数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$（其第$i$行为$x_{i}^{\\top}$）和一个依赖于$\\beta$的对角权重矩阵，将其紧凑地表示为矩阵形式。简要证明为何该海森矩阵是半正定的。\n3.  考虑$\\ell_{1}$正则化的逻辑斯蒂回归问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}, \\quad \\lambda > 0.\n$$\n使用$\\ell_{1}$次微分写出Karush–Kuhn–Tucker (KKT) 最优性条件，要求同时给出向量形式和分量形式。\n\n作为最终答案，提供海森矩阵 $\\nabla^{2}\\ell(\\beta)$ 的单一紧凑矩阵表达式，用 $X$ 和一个对角矩阵表示，该对角矩阵的元素是 $y_{i} x_{i}^{\\top}\\beta$ 的函数。不需要数值近似，也不涉及单位。最终答案必须是单一的封闭形式解析表达式。",
            "solution": "在进行解答之前，首先对问题进行验证。\n\n### 步骤 1：提取已知条件\n- 训练数据：$\\{(x_{i}, y_{i})\\}_{i=1}^{n}$\n- 输入向量：$x_{i} \\in \\mathbb{R}^{p}$，其中 $i=1, \\dots, n$。\n- 二元标签：$y_{i} \\in \\{-1, +1\\}$，其中 $i=1, \\dots, n$。\n- 逻辑斯蒂损失函数：$\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\log(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta))$，其中 $\\beta \\in \\mathbb{R}^{p}$。\n- 数据矩阵：$X \\in \\mathbb{R}^{n \\times p}$，其第$i$行为$x_{i}^{\\top}$。\n- 正则化参数：$\\lambda > 0$。\n- 优化问题：$\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n基于科学基础、良态性（well-posedness）和客观性对问题进行有效性评估。\n\n1.  **科学或事实不健全**：该问题没有任何科学或事实错误。它描述了正则化逻辑斯蒂回归的标准设置，这是现代统计学和机器学习的基石。所有定义都是标准的并且在数学上是合理的。\n2.  **无法形式化或不相关**：该问题与稀疏优化和机器学习领域高度相关。这是一个形式化的数学问题，要求基于既定原则进行推导。\n3.  **不完整或矛盾的设置**：该问题是自洽的，提供了所有必要的信息和定义。所提供的数据或约束中没有矛盾。\n4.  **不切实际或不可行**：该问题是一个理论练习，不包含不切实际的物理约束或数据。\n5.  **病态（Ill-Posed）或结构不良**：任务定义明确。损失函数的可微性以及凸函数的性质确保了所要求的推导（梯度、海森矩阵）和最优性条件是良态的（well-posed）。\n6.  **伪深刻、琐碎或同义反复**：该问题是一个标准的、不琐碎的练习，旨在检验应用于机器学习的向量微积分和凸优化理论的基础知识。\n7.  **超出科学可验证性**：所有推导和结果都可以通过数学证明进行严格验证。\n\n### 步骤 3：结论和行动\n问题被判定为**有效**。下面提供完整的解答。\n\n解答按顺序处理问题陈述的三个部分。\n\n### 第 1 部分：逻辑斯蒂损失的梯度\n\n逻辑斯蒂损失函数由下式给出\n$$\n\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_i(\\beta), \\quad \\text{其中} \\quad \\ell_i(\\beta) = \\log(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta)).\n$$\n梯度算子是线性的，因此我们可以计算每一项 $\\ell_i(\\beta)$ 的梯度，然后将结果相加。\n$$\n\\nabla \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell_i(\\beta).\n$$\n为了求梯度 $\\nabla \\ell_i(\\beta)$，我们应用链式法则。令 $f(u) = \\log(1 + \\exp(u))$ 并且 $u_i(\\beta) = -y_i x_i^\\top \\beta$。那么 $\\ell_i(\\beta) = f(u_i(\\beta))$。\n\n$f(u)$ 对 $u$ 的导数是\n$$\n\\frac{d f}{d u} = \\frac{1}{1 + \\exp(u)} \\cdot \\exp(u) = \\frac{\\exp(u)}{1 + \\exp(u)}.\n$$\n$u_i(\\beta)$ 对 $\\beta$ 的梯度是 $\\mathbb{R}^p$ 中的一个向量：\n$$\n\\nabla u_i(\\beta) = \\nabla_\\beta(-y_i x_i^\\top \\beta) = -y_i x_i.\n$$\n根据向量微积分的链式法则，$\\ell_i(\\beta)$ 的梯度是\n$$\n\\nabla \\ell_i(\\beta) = \\frac{d f}{d u}\\bigg|_{u=u_i(\\beta)} \\cdot \\nabla u_i(\\beta).\n$$\n代入这些表达式，我们得到\n$$\n\\nabla \\ell_i(\\beta) = \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} \\cdot (-y_i x_i).\n$$\n对所有数据点 $i=1, \\dots, n$ 求和并除以 $n$，得到完整的梯度：\n$$\n\\nabla \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} (-y_i x_i) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\left( \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} \\right) x_i.\n$$\n这就是以数据点之和形式表示的梯度的显式表达式。\n\n### 第 2 部分：逻辑斯蒂损失的海森矩阵\n\n海森矩阵是梯度的雅可比矩阵。由于微分算子的线性性，我们有\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla^2 \\ell_i(\\beta).\n$$\n我们计算单项 $\\ell_i(\\beta) = f(u_i(\\beta))$ 的海森矩阵，其中 $f(u) = \\log(1+\\exp(u))$ 且 $u_i(\\beta) = -y_i x_i^\\top \\beta$。我们使用海森矩阵的链式法则：\n$$\n\\nabla^2 \\ell_i(\\beta) = (\\nabla u_i(\\beta)) \\frac{d^2 f}{du^2}\\bigg|_{u=u_i(\\beta)} (\\nabla u_i(\\beta))^\\top + \\frac{d f}{du}\\bigg|_{u=u_i(\\beta)} \\nabla^2 u_i(\\beta).\n$$\n项 $u_i(\\beta)$ 是 $\\beta$ 的线性函数，所以其海森矩阵为零矩阵：$\\nabla^2 u_i(\\beta) = \\mathbf{0}$。\n$f(u)$ 的二阶导数是\n$$\n\\frac{d^2 f}{du^2} = \\frac{d}{du} \\left( \\frac{\\exp(u)}{1 + \\exp(u)} \\right) = \\frac{\\exp(u)(1+\\exp(u)) - \\exp(u)\\exp(u)}{(1+\\exp(u))^2} = \\frac{\\exp(u)}{(1+\\exp(u))^2}.\n$$\n我们定义 sigmoid 函数 $\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$。其导数为 $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$。注意到 $\\frac{d f}{du} = \\frac{\\exp(u)}{1+\\exp(u)} = \\frac{1}{\\exp(-u)+1} = \\sigma(u)$，所以 $\\frac{d^2 f}{du^2} = \\sigma'(u) = \\sigma(u)(1-\\sigma(u))$。并且，$1-\\sigma(u) = \\sigma(-u)$，因此 $\\frac{d^2 f}{du^2} = \\sigma(u)\\sigma(-u)$。\n\n代入海森矩阵链式法则公式：\n$$\n\\nabla^2 \\ell_i(\\beta) = (\\nabla u_i(\\beta)) \\left( \\sigma(u_i(\\beta))\\sigma(-u_i(\\beta)) \\right) (\\nabla u_i(\\beta))^\\top.\n$$\n使用 $\\nabla u_i(\\beta) = -y_i x_i$ 和 $u_i(\\beta) = -y_i x_i^\\top \\beta$：\n$$\n\\nabla^2 \\ell_i(\\beta) = (-y_i x_i) \\left( \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta) \\right) (-y_i x_i)^\\top = y_i^2 \\left( \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta) \\right) x_i x_i^\\top.\n$$\n由于 $y_i \\in \\{-1, +1\\}$，我们有 $y_i^2 = 1$。令 $w_i(\\beta) = \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta)$。第 $i$ 项的海森矩阵是\n$$\n\\nabla^2 \\ell_i(\\beta) = w_i(\\beta) x_i x_i^\\top.\n$$\n完整的海森矩阵是这些矩阵的平均值：\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) x_i x_i^\\top.\n$$\n这可以紧凑地表示为矩阵形式。令 $W$ 为一个对角矩阵，其对角线元素为 $W_{ii} = w_i(\\beta)$。外积之和可以写成\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} X^\\top W X.\n$$\n对角矩阵 $W$ 的元素是\n$$\nw_i(\\beta) = \\sigma(y_i x_i^\\top \\beta) \\sigma(-y_i x_i^\\top \\beta) = \\frac{1}{1+\\exp(-y_i x_i^\\top \\beta)} \\cdot \\frac{1}{1+\\exp(y_i x_i^\\top \\beta)} = \\frac{1}{2 + \\exp(y_i x_i^\\top \\beta) + \\exp(-y_i x_i^\\top \\beta)}.\n$$\n由于 sigmoid 函数的值域是 $(0,1)$，对于所有 $\\beta$，权重 $w_i(\\beta)$ 都是严格为正的。\n\n**半正定性的证明**：要证明 $\\nabla^2 \\ell(\\beta)$ 是半正定的（PSD），我们必须证明对于任何向量 $v \\in \\mathbb{R}^p$，都有 $v^\\top (\\nabla^2 \\ell(\\beta)) v \\ge 0$。\n$$\nv^\\top (\\nabla^2 \\ell(\\beta)) v = v^\\top \\left( \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) x_i x_i^\\top \\right) v = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) v^\\top (x_i x_i^\\top) v = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) (x_i^\\top v)^2.\n$$\n如前所述，$w_i(\\beta) > 0$。项 $(x_i^\\top v)^2$ 总是非负的。因此，这个和是非负项之和，其本身也是非负的。\n$$\nv^\\top (\\nabla^2 \\ell(\\beta)) v \\ge 0.\n$$\n这对所有 $v \\in \\mathbb{R}^p$ 都成立，从而证实了海森矩阵 $\\nabla^2 \\ell(\\beta)$ 是半正定的。这也证明了逻辑斯蒂损失 $\\ell(\\beta)$ 是一个凸函数。\n\n### 第 3 部分：KKT 最优性条件\n\n我们考虑 $\\ell_1$ 正则化的逻辑斯蒂回归问题：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; F(\\beta), \\quad \\text{其中} \\quad F(\\beta) = \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}.\n$$\n这是一个凸优化问题，因为它是两个凸函数之和：$\\ell(\\beta)$（在第 2 部分中已证明是凸的）和 $\\lambda\\|\\beta\\|_1$（因为 $\\|\\cdot\\|_1$ 是一个范数且 $\\lambda>0$）。\n对于凸函数 $F$，在点 $\\beta^*$ 处的一阶充要最优性条件是，零向量必须属于 $F$ 在 $\\beta^*$ 处的次微分：\n$$\n\\mathbf{0} \\in \\partial F(\\beta^*).\n$$\n对于两个凸函数之和，其中一个可微（如 $\\ell(\\beta)$），和的次微分等于梯度和次微分的和：\n$$\n\\partial F(\\beta) = \\nabla \\ell(\\beta) + \\partial (\\lambda \\|\\beta\\|_1) = \\nabla \\ell(\\beta) + \\lambda \\partial \\|\\beta\\|_1.\n$$\n因此，解 $\\beta^*$ 的最优性条件（在此背景下也称为 Karush-Kuhn-Tucker 或 KKT 条件）是\n$$\n\\mathbf{0} \\in \\nabla \\ell(\\beta^*) + \\lambda \\partial \\|\\beta^*\\|_1,\n$$\n可以改写为\n$$\n-\\nabla \\ell(\\beta^*) \\in \\lambda \\partial \\|\\beta^*\\|_1.\n$$\n这就是向量形式的 KKT 条件。\n\n为了得到分量形式，我们考虑 $\\ell_1$ 范数 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 的次微分。次微分 $\\partial \\|\\beta\\|_1$ 是每个分量 $|\\beta_j|$ 次微分的笛卡尔积。对于单个变量 $z$，绝对值函数 $|z|$ 的次微分是：\n$$\n\\partial |z| =\n\\begin{cases}\n\\{\\text{sign}(z)\\},  \\text{若 } z \\neq 0 \\\\\n[-1, 1],  \\text{若 } z = 0\n\\end{cases}\n$$\n向量形式的 KKT 条件必须对每个坐标 $j \\in \\{1, \\dots, p\\}$ 成立。令 $g_j = (\\nabla \\ell(\\beta^*))_j$ 为梯度的第 $j$ 个分量。该条件变为 $-g_j \\in \\lambda \\partial |\\beta^*_j|$。\n\n我们根据 $\\beta^*_j$ 的值来分析这个条件：\n1.  如果 $\\beta^*_j > 0$，那么 $\\partial |\\beta^*_j| = \\{1\\}$，所以 $-g_j = \\lambda$，这意味着 $g_j = -\\lambda$。\n2.  如果 $\\beta^*_j  < 0$，那么 $\\partial |\\beta^*_j| = \\{-1\\}$，所以 $-g_j = -\\lambda$，这意味着 $g_j = \\lambda$。\n    当 $\\beta^*_j \\neq 0$ 时，这两种情况可以合并为 $g_j = -\\lambda \\text{sign}(\\beta^*_j)$。\n3.  如果 $\\beta^*_j = 0$，那么 $\\partial |\\beta^*_j| = [-1, 1]$，所以 $-g_j \\in [-\\lambda, \\lambda]$，这等价于 $|g_j| \\le \\lambda$。\n\n所以，分量形式的 KKT 条件是：\n对于每个 $j \\in \\{1, \\dots, p\\}$，\n$$\n\\begin{cases}\n(\\nabla \\ell(\\beta^*))_j = -\\lambda \\text{sign}(\\beta^*_j),  \\text{若 } \\beta^*_j \\neq 0 \\\\\n|(\\nabla \\ell(\\beta^*))_j| \\le \\lambda,  \\text{若 } \\beta^*_j = 0\n\\end{cases}\n$$\n其中 $(\\nabla \\ell(\\beta^*))_j = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\left( \\frac{\\exp(-y_i x_i^\\top \\beta^*)}{1 + \\exp(-y_i x_i^\\top \\beta^*)} \\right) x_{ij}$。",
            "answer": "$$\\boxed{\\frac{1}{n} X^\\top \\mathrm{diag}\\left(\\left\\{ \\frac{1}{2 + \\exp(y_i x_i^\\top \\beta) + \\exp(-y_i x_i^\\top \\beta)} \\right\\}_{i=1,\\ldots,n} \\right) X}$$"
        },
        {
            "introduction": "在建立稀疏优化问题之后，一个核心问题是：我们何时能保证LASSO找到正确的非零特征集？不可分条件（Irrepresentable Condition）是在总体层面上回答这一问题的基础理论工具。本练习提供了一个具体的计算，通过检验给定协方差矩阵的不可分条件，将抽象的理论与可行的验证联系起来。",
            "id": "3477011",
            "problem": "考虑一个线性回归模型，其随机设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列被标准化为零均值和单位方差。令总体格拉姆 (协方差) 矩阵定义为 $\\Sigma = \\mathbb{E}\\!\\left[X^{\\top}X/n\\right] \\in \\mathbb{R}^{p \\times p}$。假设使用最小绝对值收敛和选择算子 (LASSO) 估计量，其通过求解以下问题定义\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n用于在无噪声模型 $y = X\\beta^{\\star}$ 下进行稀疏恢复，其中给定了支撑集 $S \\subset \\{1,\\dots,p\\}$ 和符号模式 $s = \\operatorname{sgn}(\\beta^{\\star}_{S}) \\in \\{-1,+1\\}^{|S|}$。从 LASSO 的 Karush–Kuhn–Tucker (KKT) 条件出发，在总体水平上进行分析，推导与 $S$ 相关的不可表示向量的表达式（用 $\\Sigma$ 的分块和 $s$ 表示），然后对以下具体设定评估相应的 $\\ell_{\\infty}$ 不可表示量：\n$$\n\\Sigma = \\begin{pmatrix}\n1  0.1  0.2  0.05 \\\\\n0.1  1  0.05  0.1 \\\\\n0.2  0.05  1  0.15 \\\\\n0.05  0.1  0.15  1\n\\end{pmatrix}, \\quad\nS = \\{1,3\\}, \\quad\nS^{c} = \\{2,4\\}, \\quad\ns = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}.\n$$\n令容差参数为 $\\eta = 0.875$。在推导出不可表示向量后，计算其 $\\ell_{\\infty}$ 范数，并用它来判断总体水平上的支撑集恢复的不可表示条件是否成立，该条件被解释为计算出的量小于或等于 $1 - \\eta$。将 $\\ell_{\\infty}$ 不可表示量的数值作为最终答案报告，四舍五入到四位有效数字。不涉及物理单位。",
            "solution": "用户希望解决问题陈述。该问题是使用 LASSO 进行稀疏恢复理论中的一个标准练习，问题提法是适定的，具有科学依据，并包含了所有必要信息。因此，该问题是有效的，将提供一个解答。\n\n问题要求推导和评估 LASSO 估计量在总体水平上的 $\\ell_{\\infty}$ 不可表示量。\n\nLASSO 估计量是以下优化问题的解：\n$$ \\min_{\\beta \\in \\mathbb{R}^{p}} \\ L(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\left\\{ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\} $$\n最优性的 Karush–Kuhn–Tucker (KKT) 条件指出，在解 $\\hat{\\beta}$ 处，零向量必须是目标函数 $L(\\beta)$ 的次梯度的一个元素。最小二乘项的梯度是 $-\\frac{1}{n}X^{\\top}(y - X\\beta)$。$\\ell_1$ 范数的次梯度 $\\partial\\|\\beta\\|_1$ 是一个向量 $g$，其中如果 $\\beta_j \\neq 0$，则 $g_j = \\operatorname{sgn}(\\beta_j)$；如果 $\\beta_j=0$，则 $g_j \\in [-1, 1]$。因此，KKT 条件为：\n$$ -\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta}) + \\lambda g = 0, \\quad \\text{其中 } g \\in \\partial\\|\\hat{\\beta}\\|_{1} $$\n这可以重写为 $\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta}) = \\lambda g$。\n\n问题要求在总体水平上进行分析。我们用总体期望来代替样本量。给定无噪声模型 $y = X\\beta^{\\star}$，梯度项的期望值为：\n$$ \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta})\\right] = \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}(X\\beta^{\\star} - X\\hat{\\beta})\\right] = \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}X\\right](\\beta^{\\star} - \\hat{\\beta}) = \\Sigma(\\beta^{\\star} - \\hat{\\beta}) $$\n其中 $\\Sigma = \\mathbb{E}[X^{\\top}X/n]$ 是总体格拉姆矩阵。总体水平上的 KKT 条件变为：\n$$ \\Sigma(\\beta^{\\star} - \\hat{\\beta}) = \\lambda g, \\quad g \\in \\partial\\|\\hat{\\beta}\\|_{1} $$\n我们关心的是正确支撑集恢复的条件，即 LASSO 估计量 $\\hat{\\beta}$ 与真实系数向量 $\\beta^{\\star}$ 具有相同的支撑集 $S = \\operatorname{supp}(\\beta^{\\star})$ 和符号模式 $s = \\operatorname{sgn}(\\beta^{\\star}_S)$。这意味着 $\\hat{\\beta}_{S^c} = 0$（其中 $S^c$ 是 $S$ 的补集）且 $\\operatorname{sgn}(\\hat{\\beta}_S) = s$。\n\n我们根据支撑集 $S$ 及其补集 $S^c$ 对问题进行分块。矩阵 $\\Sigma$ 和向量 $\\beta^{\\star}$, $\\hat{\\beta}$ 被分块如下：\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{SS}  \\Sigma_{SS^c} \\\\ \\Sigma_{S^cS}  \\Sigma_{S^cS^c} \\end{pmatrix}, \\quad \\beta^{\\star} = \\begin{pmatrix} \\beta^{\\star}_S \\\\ 0 \\end{pmatrix}, \\quad \\hat{\\beta} = \\begin{pmatrix} \\hat{\\beta}_S \\\\ 0 \\end{pmatrix} $$\nKKT 条件对每个分块写出如下：\n对于索引 $j \\in S$，我们有 $\\hat{\\beta}_j \\neq 0$，所以 $g_S = \\operatorname{sgn}(\\hat{\\beta}_S) = s$。KKT 方程的相应分块为：\n$$ (\\Sigma(\\beta^{\\star} - \\hat{\\beta}))_S = \\lambda s $$\n$$ \\Sigma_{SS}(\\beta^{\\star}_S - \\hat{\\beta}_S) + \\Sigma_{SS^c}(\\beta^{\\star}_{S^c} - \\hat{\\beta}_{S^c}) = \\lambda s $$\n由于 $\\beta^{\\star}_{S^c}=0$ 并且我们假设 $\\hat{\\beta}_{S^c}=0$，这可以简化为：\n$$ \\Sigma_{SS}(\\beta^{\\star}_S - \\hat{\\beta}_S) = \\lambda s $$\n假设 $\\Sigma_{SS}$ 是可逆的，我们可以写出：\n$$ \\beta^{\\star}_S - \\hat{\\beta}_S = \\lambda \\Sigma_{SS}^{-1} s $$\n对于索引 $j \\in S^c$，我们有 $\\hat{\\beta}_j = 0$，所以 $|g_j| \\le 1$。此分块的 KKT 条件是：\n$$ |(\\Sigma(\\beta^{\\star} - \\hat{\\beta}))_{S^c}| \\le \\lambda \\quad (\\text{逐元素的}) $$\n$$ |\\Sigma_{S^cS}(\\beta^{\\star}_S - \\hat{\\beta}_S) + \\Sigma_{S^cS^c}(\\beta^{\\star}_{S^c} - \\hat{\\beta}_{S^c})| \\le \\lambda $$\n这可以简化为：\n$$ |\\Sigma_{S^cS}(\\beta^{\\star}_S - \\hat{\\beta}_S)| \\le \\lambda $$\n将来自活跃集方程的 $(\\beta^{\\star}_S - \\hat{\\beta}_S)$ 表达式代入，得到：\n$$ |\\Sigma_{S^cS}(\\lambda \\Sigma_{SS}^{-1} s)| \\le \\lambda $$\n对于 $\\lambda > 0$，我们可以除以 $\\lambda$ 得到不可表示条件：\n$$ |\\Sigma_{S^cS} \\Sigma_{SS}^{-1} s| \\le 1 $$\n向量 $\\gamma = \\Sigma_{S^cS} \\Sigma_{SS}^{-1} s$ 是不可表示向量。支撑集恢复的条件是该向量的 $\\ell_{\\infty}$ 范数（我们称之为 $\\ell_{\\infty}$ 不可表示量）小于或等于 $1$：\n$$ \\|\\gamma\\|_{\\infty} = \\|\\Sigma_{S^cS} \\Sigma_{SS}^{-1} s\\|_{\\infty} \\le 1 $$\n现在，我们对给定的数值规范评估此量。\n提供的数据是：\n$$ \\Sigma = \\begin{pmatrix} 1  0.1  0.2  0.05 \\\\ 0.1  1  0.05  0.1 \\\\ 0.2  0.05  1  0.15 \\\\ 0.05  0.1  0.15  1 \\end{pmatrix}, \\quad S = \\{1,3\\}, \\quad S^{c} = \\{2,4\\}, \\quad s = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix} $$\n我们根据 $S$ 和 $S^c$ 中的索引提取子矩阵 $\\Sigma_{SS}$ 和 $\\Sigma_{S^cS}$：\n$$ \\Sigma_{SS} = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{13} \\\\ \\Sigma_{31}  \\Sigma_{33} \\end{pmatrix} = \\begin{pmatrix} 1  0.2 \\\\ 0.2  1 \\end{pmatrix} $$\n$$ \\Sigma_{S^cS} = \\begin{pmatrix} \\Sigma_{21}  \\Sigma_{23} \\\\ \\Sigma_{41}  \\Sigma_{43} \\end{pmatrix} = \\begin{pmatrix} 0.1  0.05 \\\\ 0.05  0.15 \\end{pmatrix} $$\n首先，我们计算 $\\Sigma_{SS}$ 的逆矩阵：\n$$ \\det(\\Sigma_{SS}) = (1)(1) - (0.2)(0.2) = 1 - 0.04 = 0.96 $$\n$$ \\Sigma_{SS}^{-1} = \\frac{1}{0.96} \\begin{pmatrix} 1  -0.2 \\\\ -0.2  1 \\end{pmatrix} = \\frac{100}{96} \\begin{pmatrix} 1  -0.2 \\\\ -0.2  1 \\end{pmatrix} = \\frac{25}{24} \\begin{pmatrix} 1  -0.2 \\\\ -0.2  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{24}  -\\frac{5}{24} \\\\ -\\frac{5}{24}  \\frac{25}{24} \\end{pmatrix} $$\n接下来，我们计算向量 $\\Sigma_{SS}^{-1} s$：\n$$ \\Sigma_{SS}^{-1} s = \\begin{pmatrix} \\frac{25}{24}  -\\frac{5}{24} \\\\ -\\frac{5}{24}  \\frac{25}{24} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{24} + \\frac{5}{24} \\\\ -\\frac{5}{24} - \\frac{25}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{24} \\\\ -\\frac{30}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\n现在，我们计算不可表示向量 $\\gamma = \\Sigma_{S^cS} (\\Sigma_{SS}^{-1} s)$：\n$$ \\gamma = \\begin{pmatrix} 0.1  0.05 \\\\ 0.05  0.15 \\end{pmatrix} \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10}  \\frac{1}{20} \\\\ \\frac{1}{20}  \\frac{3}{20} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\n$\\gamma$ 的分量计算如下：\n$$ \\gamma_1 = \\left(\\frac{1}{10}\\right)\\left(\\frac{5}{4}\\right) + \\left(\\frac{1}{20}\\right)\\left(-\\frac{5}{4}\\right) = \\frac{5}{40} - \\frac{5}{80} = \\frac{10}{80} - \\frac{5}{80} = \\frac{5}{80} = \\frac{1}{16} $$\n$$ \\gamma_2 = \\left(\\frac{1}{20}\\right)\\left(\\frac{5}{4}\\right) + \\left(\\frac{3}{20}\\right)\\left(-\\frac{5}{4}\\right) = \\frac{5}{80} - \\frac{15}{80} = -\\frac{10}{80} = -\\frac{1}{8} $$\n所以不可表示向量是 $\\gamma = \\begin{pmatrix} 1/16 \\\\ -1/8 \\end{pmatrix}$。\n$\\ell_{\\infty}$ 不可表示量是 $\\gamma$ 各分量绝对值的最大值：\n$$ \\|\\gamma\\|_{\\infty} = \\max\\left(\\left|\\frac{1}{16}\\right|, \\left|-\\frac{1}{8}\\right|\\right) = \\max\\left(\\frac{1}{16}, \\frac{1}{8}\\right) = \\frac{1}{8} $$\n数值是 $0.125$。问题要求将此值四舍五入到四位有效数字，即 $0.1250$。\n问题还要求判断不可表示条件是否成立，该条件是计算出的量小于或等于 $1 - \\eta$，其中 $\\eta = 0.875$。我们有 $1 - \\eta = 1 - 0.875 = 0.125$。由于我们计算出的量是 $0.125$，条件 $0.125 \\le 0.125$ 是满足的。最终答案是 $\\ell_{\\infty}$ 不可表示量本身的数值。",
            "answer": "$$\n\\boxed{0.1250}\n$$"
        },
        {
            "introduction": "求解LASSO目标函数需要专门的迭代算法。迭代软阈值算法（ISTA）是一种基本的近端梯度方法，而快速迭代软阈值算法（FISTA）是其加速版本。本练习将通过一个数值算例，带你完成这两种算法的单步迭代，让你对它们如何具体操作和更新解的估计有一个直观的感受。",
            "id": "3476942",
            "problem": "考虑用于稀疏线性回归的最小绝对收缩和选择算子 (LASSO) 的目标函数，\n$$\nf(\\beta) \\;=\\; \\frac{1}{2}\\,\\|X\\beta - y\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{2\\times 2}$，$y \\in \\mathbb{R}^{2}$，$\\lambda > 0$，以及 $\\beta \\in \\mathbb{R}^{2}$。给定测量矩阵、响应向量、正则化参数和步长\n$$\nX \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad\ny \\;=\\; \\begin{pmatrix} 0.1 \\\\ -0.6 \\end{pmatrix}, \\quad\n\\lambda \\;=\\; \\frac{1}{2}, \\quad\nt \\;=\\; \\frac{1}{4}.\n$$\n假设初始迭代点为\n$$\n\\beta^{0} \\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}.\n$$\n使用这些数据，执行：\n- 一次迭代收缩阈值算法 (ISTA) 的迭代，即从 $\\beta^{0}$ 开始对 $f(\\beta)$ 进行单个近端梯度步。\n- 一次快速迭代收缩阈值算法 (FISTA) 的迭代；初始化 $x_{0} = \\beta^{0}$，$y_{0} = x_{0}$，加速参数 $\\alpha_{0} = 1$，并执行第一次更新。\n\n计算这两个更新后的迭代点，然后在每个更新后的迭代点上评估 LASSO 目标函数 $f(\\beta)$ 的值。最后，计算差值\n$$\n\\Delta \\;=\\; f\\!\\big(\\beta^{1}_{\\mathrm{ISTA}}\\big) \\;-\\; f\\!\\big(\\beta^{1}_{\\mathrm{FISTA}}\\big).\n$$\n将最终差值 $\\Delta$ 表示为精确值；无需四舍五入。",
            "solution": "我们从复合凸目标函数开始\n$$\nf(\\beta) \\;=\\; g(\\beta) \\;+\\; h(\\beta),\n$$\n其中光滑部分为 $g(\\beta) = \\frac{1}{2}\\|X\\beta - y\\|_{2}^{2}$，非光滑部分为 $h(\\beta) = \\lambda\\|\\beta\\|_{1}$。$g$ 的梯度是\n$$\n\\nabla g(\\beta) \\;=\\; X^{\\top}(X\\beta - y).\n$$\n对于给定的 $X = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$，我们有 $X^{\\top}X = \\operatorname{diag}(1,4)$，其谱范数（最大特征值）为 $L = 4$。所选的步长 $t = \\frac{1}{4}$ 满足 $t = \\frac{1}{L}$，这是一个标准选择，可以确保近端梯度步对于光滑部分是非扩张的。\n\n$h(\\beta) = \\lambda\\|\\beta\\|_{1}$ 的近端映射是逐坐标应用的软阈值算子。对于一个向量 $v \\in \\mathbb{R}^{2}$ 和一个阈值 $\\tau > 0$，软阈值算子 $S_{\\tau}$ 定义为\n$$\n\\big(S_{\\tau}(v)\\big)_{i} \\;=\\; \\operatorname{sign}(v_{i})\\cdot\\max\\{|v_{i}| - \\tau,\\, 0\\}.\n$$\n在我们的情境中，近端步中使用的阈值为 $\\tau = \\lambda t = \\frac{1}{2}\\cdot\\frac{1}{4} = \\frac{1}{8} = 0.125$。\n\n迭代收缩阈值算法 (ISTA) 更新：\nISTA 执行\n$$\n\\beta^{1}_{\\mathrm{ISTA}} \\;=\\; S_{\\lambda t}\\!\\left(\\beta^{0} - t\\,\\nabla g(\\beta^{0})\\right).\n$$\n我们首先计算 $\\nabla g(\\beta^{0})$。当 $\\beta^{0} = \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}$ 时，\n$$\nX\\beta^{0} \\;=\\; \\begin{pmatrix} 1\\cdot 0.3 \\\\ 2\\cdot(-0.4) \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.8 \\end{pmatrix},\n\\quad\nX\\beta^{0} - y \\;=\\; \\begin{pmatrix} 0.3 - 0.1 \\\\ -0.8 - (-0.6) \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.2 \\\\ -0.2 \\end{pmatrix}.\n$$\n因此，\n$$\n\\nabla g(\\beta^{0}) \\;=\\; X^{\\top}(X\\beta^{0} - y)\n\\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\n\\begin{pmatrix} 0.2 \\\\ -0.2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.2 \\\\ -0.4 \\end{pmatrix}.\n$$\n梯度步为\n$$\n\\beta^{0} - t\\,\\nabla g(\\beta^{0})\n\\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}\n- \\frac{1}{4}\\begin{pmatrix} 0.2 \\\\ -0.4 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.3 - 0.05 \\\\ -0.4 + 0.1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.25 \\\\ -0.3 \\end{pmatrix}.\n$$\n使用 $\\tau = 0.125$ 逐坐标地应用软阈值，\n$$\nS_{0.125}(0.25) \\;=\\; 0.25 - 0.125 \\;=\\; 0.125, \n\\qquad\nS_{0.125}(-0.3) \\;=\\; -\\big(0.3 - 0.125\\big) \\;=\\; -0.175,\n$$\n所以\n$$\n\\beta^{1}_{\\mathrm{ISTA}} \\;=\\; \\begin{pmatrix} 0.125 \\\\ -0.175 \\end{pmatrix}.\n$$\n\n快速迭代收缩阈值算法 (FISTA) 更新：\n快速迭代收缩阈值算法 (FISTA) 使用一个外推点 $y_{k}$ 来评估梯度，并进行动量更新。在标准初始化 $x_{0} = \\beta^{0}$，$y_{0} = x_{0}$ 和 $\\alpha_{0} = 1$ 的情况下，第一个近端梯度更新是\n$$\nx_{1} \\;=\\; S_{\\lambda t}\\!\\left(y_{0} - t\\,\\nabla g(y_{0})\\right),\n$$\n随后是动量设置\n$$\n\\alpha_{1} \\;=\\; \\frac{1 + \\sqrt{1 + 4\\alpha_{0}^{2}}}{2}, \n\\qquad\ny_{1} \\;=\\; x_{1} + \\frac{\\alpha_{0} - 1}{\\alpha_{1}}\\,(x_{1} - x_{0}).\n$$\n因为 $y_{0} = x_{0} = \\beta^{0}$，所以第一次更新 $x_{1}$ 与在 $\\beta^{0}$ 处计算的 ISTA 更新结果一致：\n$$\n\\beta^{1}_{\\mathrm{FISTA}} \\;=\\; x_{1} \\;=\\; \\begin{pmatrix} 0.125 \\\\ -0.175 \\end{pmatrix}.\n$$\n因此，在标准初始化下进行一次迭代后，ISTA 和 FISTA 产生相同的更新后迭代点。\n\n目标函数值及其差值：\n我们计算在 $\\beta = \\begin{pmatrix} 0.125 \\\\ -0.175 \\end{pmatrix}$ 处 $f(\\beta)$ 的值。计算残差\n$$\nX\\beta - y \n\\;=\\; \\begin{pmatrix} 1\\cdot 0.125 \\\\ 2\\cdot(-0.175) \\end{pmatrix}\n- \\begin{pmatrix} 0.1 \\\\ -0.6 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.125 - 0.1 \\\\ -0.35 + 0.6 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.025 \\\\ 0.25 \\end{pmatrix}.\n$$\n欧几里得范数的平方是\n$$\n\\|X\\beta - y\\|_{2}^{2} \\;=\\; (0.025)^{2} + (0.25)^{2} \\;=\\; \\frac{1}{1600} + \\frac{1}{16} \\;=\\; \\frac{101}{1600}.\n$$\n因此，\n$$\n\\frac{1}{2}\\|X\\beta - y\\|_{2}^{2} \\;=\\; \\frac{101}{3200}.\n$$\n$\\ell_{1}$ 范数是\n$$\n\\|\\beta\\|_{1} \\;=\\; |0.125| + |-0.175| \\;=\\; 0.125 + 0.175 \\;=\\; 0.3 \\;=\\; \\frac{3}{10},\n$$\n正则化项是\n$$\n\\lambda\\|\\beta\\|_{1} \\;=\\; \\frac{1}{2}\\cdot \\frac{3}{10} \\;=\\; \\frac{3}{20} \\;=\\; \\frac{480}{3200}.\n$$\n因此，\n$$\nf\\!\\big(\\beta^{1}_{\\mathrm{ISTA}}\\big) \\;=\\; f\\!\\big(\\beta^{1}_{\\mathrm{FISTA}}\\big) \n\\;=\\; \\frac{101}{3200} + \\frac{480}{3200} \n\\;=\\; \\frac{581}{3200}.\n$$\n所求的差值为\n$$\n\\Delta \\;=\\; f\\!\\big(\\beta^{1}_{\\mathrm{ISTA}}\\big) - f\\!\\big(\\beta^{1}_{\\mathrm{FISTA}}\\big) \\;=\\; \\frac{581}{3200} - \\frac{581}{3200} \\;=\\; 0.\n$$\n因此，在指定的标准初始化下，ISTA 和 FISTA 的第一次迭代产生相同的目标函数值，差值 $\\Delta$ 恰好为零。",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}