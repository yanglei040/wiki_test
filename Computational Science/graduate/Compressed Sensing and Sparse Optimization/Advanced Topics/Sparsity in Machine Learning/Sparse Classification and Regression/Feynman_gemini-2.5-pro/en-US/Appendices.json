{
    "hands_on_practices": [
        {
            "introduction": "Solving the LASSO objective requires specialized algorithms that can handle the non-differentiable $\\ell_1$ norm. This exercise provides a hands-on walk-through of two fundamental methods, the Iterative Shrinkage-Thresholding Algorithm (ISTA) and its accelerated variant (FISTA). By performing a single iteration numerically, you will gain a concrete understanding of the proximal gradient descent mechanism, which combines a standard gradient step with a soft-thresholding operation to induce sparsity. ",
            "id": "3476942",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective for sparse linear regression,\n$$\nf(\\beta) \\;=\\; \\frac{1}{2}\\,\\|X\\beta - y\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\nwhere $X \\in \\mathbb{R}^{2\\times 2}$, $y \\in \\mathbb{R}^{2}$, $\\lambda > 0$, and $\\beta \\in \\mathbb{R}^{2}$. You are given the measurement matrix, response vector, regularization parameter, and step size\n$$\nX \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad\ny \\;=\\; \\begin{pmatrix} 0.1 \\\\ -0.6 \\end{pmatrix}, \\quad\n\\lambda \\;=\\; \\frac{1}{2}, \\quad\nt \\;=\\; \\frac{1}{4}.\n$$\nAssume the initial iterate is\n$$\n\\beta^{0} \\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}.\n$$\nUsing these data, perform:\n- One iteration of the Iterative Shrinkage-Thresholding Algorithm (ISTA), i.e., a single proximal gradient step for $f(\\beta)$ starting from $\\beta^{0}$.\n- One iteration of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA); initialize with $x_{0} = \\beta^{0}$, $y_{0} = x_{0}$, and acceleration parameter $\\alpha_{0} = 1$, and perform the first update.\n\nCompute the two updated iterates and then evaluate the LASSO objective $f(\\beta)$ at each updated iterate. Finally, compute the difference\n$$\n\\Delta \\;=\\; f\\!\\big(\\beta^{1}_{\\mathrm{ISTA}}\\big) \\;-\\; f\\!\\big(\\beta^{1}_{\\mathrm{FISTA}}\\big).\n$$\nExpress the final difference $\\Delta$ as an exact value; no rounding is required.",
            "solution": "We begin from the composite convex objective\n$$\nf(\\beta) \\;=\\; g(\\beta) \\;+\\; h(\\beta),\n$$\nwith smooth part $g(\\beta) = \\frac{1}{2}\\|X\\beta - y\\|_{2}^{2}$ and nonsmooth part $h(\\beta) = \\lambda\\|\\beta\\|_{1}$. The gradient of $g$ is\n$$\n\\nabla g(\\beta) \\;=\\; X^{\\top}(X\\beta - y).\n$$\nFor the given $X = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$, we have $X^{\\top}X = \\operatorname{diag}(1,4)$, whose spectral norm (largest eigenvalue) is $L = 4$. The chosen step size $t = \\frac{1}{4}$ satisfies $t = \\frac{1}{L}$, which is a standard choice ensuring that the proximal gradient step is nonexpansive for the smooth part.\n\nThe proximal mapping for $h(\\beta) = \\lambda\\|\\beta\\|_{1}$ is the soft-thresholding operator applied coordinatewise. For a vector $v \\in \\mathbb{R}^{2}$ and threshold $\\tau > 0$, the soft-thresholding operator $S_{\\tau}$ is defined by\n$$\n\\big(S_{\\tau}(v)\\big)_{i} \\;=\\; \\operatorname{sign}(v_{i})\\cdot\\max\\{|v_{i}| - \\tau,\\, 0\\}.\n$$\nIn our context, the threshold used in the proximal step is $\\tau = \\lambda t = \\frac{1}{2}\\cdot\\frac{1}{4} = \\frac{1}{8} = 0.125$.\n\nIterative Shrinkage-Thresholding Algorithm (ISTA) update:\nISTA performs\n$$\n\\beta^{1}_{\\mathrm{ISTA}} \\;=\\; S_{\\lambda t}\\!\\left(\\beta^{0} - t\\,\\nabla g(\\beta^{0})\\right).\n$$\nWe first compute $\\nabla g(\\beta^{0})$. With $\\beta^{0} = \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}$,\n$$\nX\\beta^{0} \\;=\\; \\begin{pmatrix} 1\\cdot 0.3 \\\\ 2\\cdot(-0.4) \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.8 \\end{pmatrix},\n\\quad\nX\\beta^{0} - y \\;=\\; \\begin{pmatrix} 0.3 - 0.1 \\\\ -0.8 - (-0.6) \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.2 \\\\ -0.2 \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\nabla g(\\beta^{0}) \\;=\\; X^{\\top}(X\\beta^{0} - y)\n\\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\n\\begin{pmatrix} 0.2 \\\\ -0.2 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.2 \\\\ -0.4 \\end{pmatrix}.\n$$\nThe gradient step is\n$$\n\\beta^{0} - t\\,\\nabla g(\\beta^{0})\n\\;=\\; \\begin{pmatrix} 0.3 \\\\ -0.4 \\end{pmatrix}\n- \\frac{1}{4}\\begin{pmatrix} 0.2 \\\\ -0.4 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.3 - 0.05 \\\\ -0.4 + 0.1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.25 \\\\ -0.3 \\end{pmatrix}.\n$$\nApplying soft-thresholding with $\\tau = 0.125$ coordinatewise,\n$$\nS_{0.125}(0.25) \\;=\\; 0.25 - 0.125 \\;=\\; 0.125, \n\\qquad\nS_{0.125}(-0.3) \\;=\\; -\\big(0.3 - 0.125\\big) \\;=\\; -0.175,\n$$\nso\n$$\n\\beta^{1}_{\\mathrm{ISTA}} \\;=\\; \\begin{pmatrix} 0.125 \\\\ -0.175 \\end{pmatrix}.\n$$\n\nFast Iterative Shrinkage-Thresholding Algorithm (FISTA) update:\nFast Iterative Shrinkage-Thresholding Algorithm (FISTA) uses an extrapolated point $y_{k}$ for the gradient evaluation and a momentum update. With the standard initialization $x_{0} = \\beta^{0}$, $y_{0} = x_{0}$, and $\\alpha_{0} = 1$, the first proximal gradient update is\n$$\nx_{1} \\;=\\; S_{\\lambda t}\\!\\left(y_{0} - t\\,\\nabla g(y_{0})\\right),\n$$\nfollowed by momentum setting\n$$\n\\alpha_{1} \\;=\\; \\frac{1 + \\sqrt{1 + 4\\alpha_{0}^{2}}}{2}, \n\\qquad\ny_{1} \\;=\\; x_{1} + \\frac{\\alpha_{0} - 1}{\\alpha_{1}}\\,(x_{1} - x_{0}).\n$$\nBecause $y_{0} = x_{0} = \\beta^{0}$, the first update $x_{1}$ coincides with the ISTA update computed at $\\beta^{0}$:\n$$\n\\beta^{1}_{\\mathrm{FISTA}} \\;=\\; x_{1} \\;=\\; \\begin{pmatrix} 0.125 \\\\ -0.175 \\end{pmatrix}.\n$$\nThus, after one iteration with standard initialization, ISTA and FISTA produce the same updated iterate.\n\nObjective values and their difference:\nWe evaluate $f(\\beta)$ at $\\beta = \\begin{pmatrix} 0.125 \\\\ -0.175 \\end{pmatrix}$. Compute the residual\n$$\nX\\beta - y \n\\;=\\; \\begin{pmatrix} 1\\cdot 0.125 \\\\ 2\\cdot(-0.175) \\end{pmatrix}\n- \\begin{pmatrix} 0.1 \\\\ -0.6 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.125 - 0.1 \\\\ -0.35 + 0.6 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.025 \\\\ 0.25 \\end{pmatrix}.\n$$\nThe squared Euclidean norm is\n$$\n\\|X\\beta - y\\|_{2}^{2} \\;=\\; (0.025)^{2} + (0.25)^{2} \\;=\\; \\frac{1}{1600} + \\frac{1}{16} \\;=\\; \\frac{101}{1600}.\n$$\nTherefore,\n$$\n\\frac{1}{2}\\|X\\beta - y\\|_{2}^{2} \\;=\\; \\frac{101}{3200}.\n$$\nThe $\\ell_{1}$ norm is\n$$\n\\|\\beta\\|_{1} \\;=\\; |0.125| + |-0.175| \\;=\\; 0.125 + 0.175 \\;=\\; 0.3 \\;=\\; \\frac{3}{10},\n$$\nand the regularization term is\n$$\n\\lambda\\|\\beta\\|_{1} \\;=\\; \\frac{1}{2}\\cdot \\frac{3}{10} \\;=\\; \\frac{3}{20} \\;=\\; \\frac{480}{3200}.\n$$\nHence,\n$$\nf\\!\\big(\\beta^{1}_{\\mathrm{ISTA}}\\big) \\;=\\; f\\!\\big(\\beta^{1}_{\\mathrm{FISTA}}\\big) \n\\;=\\; \\frac{101}{3200} + \\frac{480}{3200} \n\\;=\\; \\frac{581}{3200}.\n$$\nThe requested difference is\n$$\n\\Delta \\;=\\; f\\!\\big(\\beta^{1}_{\\mathrm{ISTA}}\\big) - f\\!\\big(\\beta^{1}_{\\mathrm{FISTA}}\\big) \\;=\\; \\frac{581}{3200} - \\frac{581}{3200} \\;=\\; 0.\n$$\nTherefore, with the specified standard initialization, the first ISTA and FISTA iterates yield identical objective values, and the difference $\\Delta$ is exactly zero.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "The principles of sparse modeling extend naturally from regression to classification by replacing the squared error with a suitable classification loss, such as the logistic loss. This practice focuses on the analytical foundations of $\\ell_1$-regularized logistic regression, a cornerstone of sparse classification. Deriving the gradient, Hessian, and the Karush–Kuhn–Tucker (KKT) optimality conditions will illuminate the mathematical structure that optimization algorithms exploit to find sparse solutions in a high-dimensional classification context. ",
            "id": "3476956",
            "problem": "You are given training data $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ with $x_{i} \\in \\mathbb{R}^{p}$ and $y_{i} \\in \\{-1,+1\\}$. Consider the logistic loss for binary classification defined by\n$$\n\\ell(\\beta) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\log\\!\\big(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta)\\big), \\quad \\beta \\in \\mathbb{R}^{p}.\n$$\nStarting only from the chain rule in vector calculus, the definition of the gradient and Hessian, and the subdifferential of the $\\ell_{1}$ norm, do the following:\n\n1. Derive the gradient $\\nabla \\ell(\\beta)$ explicitly as a sum over data points in terms of $x_{i}$, $y_{i}$, and $x_{i}^{\\top}\\beta$.\n2. Derive the Hessian $\\nabla^{2}\\ell(\\beta)$ explicitly and express it compactly in matrix form using the data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose $i$-th row is $x_{i}^{\\top}$ and a diagonal weight matrix depending on $\\beta$. Briefly justify why this Hessian is positive semidefinite.\n3. Consider the $\\ell_{1}$-regularized logistic regression problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}, \\quad \\lambda > 0.\n$$\nWrite the Karush–Kuhn–Tucker (KKT) optimality conditions using the $\\ell_{1}$ subdifferential, both in vector form and coordinate-wise form.\n\nProvide, as your final answer, the single compact matrix expression for the Hessian $\\nabla^{2}\\ell(\\beta)$ in terms of $X$ and a diagonal matrix whose entries are functions of $y_{i} x_{i}^{\\top}\\beta$. No numerical approximation is required and no units apply. The final answer must be a single closed-form analytic expression.",
            "solution": "The solution proceeds by addressing the three parts of the problem statement in order.\n\n### Part 1: Gradient of the Logistic Loss\n\nThe logistic loss function is given by\n$$\n\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_i(\\beta), \\quad \\text{where} \\quad \\ell_i(\\beta) = \\log(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta)).\n$$\nThe gradient operator is linear, so we can compute the gradient of each term $\\ell_i(\\beta)$ and then sum the results.\n$$\n\\nabla \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell_i(\\beta).\n$$\nTo find the gradient $\\nabla \\ell_i(\\beta)$, we apply the chain rule. Let $f(u) = \\log(1 + \\exp(u))$ and let $u_i(\\beta) = -y_i x_i^\\top \\beta$. Then $\\ell_i(\\beta) = f(u_i(\\beta))$.\n\nThe derivative of $f(u)$ with respect to $u$ is\n$$\n\\frac{d f}{d u} = \\frac{1}{1 + \\exp(u)} \\cdot \\exp(u) = \\frac{\\exp(u)}{1 + \\exp(u)}.\n$$\nThe gradient of $u_i(\\beta)$ with respect to $\\beta$ is a vector in $\\mathbb{R}^p$:\n$$\n\\nabla u_i(\\beta) = \\nabla_\\beta(-y_i x_i^\\top \\beta) = -y_i x_i.\n$$\nBy the chain rule for vector calculus, the gradient of $\\ell_i(\\beta)$ is\n$$\n\\nabla \\ell_i(\\beta) = \\frac{d f}{d u}\\bigg|_{u=u_i(\\beta)} \\cdot \\nabla u_i(\\beta).\n$$\nSubstituting the expressions, we get\n$$\n\\nabla \\ell_i(\\beta) = \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} \\cdot (-y_i x_i).\n$$\nSumming over all data points $i=1, \\dots, n$ and dividing by $n$ gives the full gradient:\n$$\n\\nabla \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} (-y_i x_i) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\left( \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} \\right) x_i.\n$$\nThis is the explicit expression for the gradient as a sum over data points.\n\n### Part 2: Hessian of the Logistic Loss\n\nThe Hessian matrix is the Jacobian of the gradient. Due to the linearity of the differentiation operator, we have\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla^2 \\ell_i(\\beta).\n$$\nWe compute the Hessian for a single term $\\ell_i(\\beta) = f(u_i(\\beta))$, where $f(u) = \\log(1+\\exp(u))$ and $u_i(\\beta) = -y_i x_i^\\top \\beta$. We use the chain rule for Hessians:\n$$\n\\nabla^2 \\ell_i(\\beta) = (\\nabla u_i(\\beta)) \\frac{d^2 f}{du^2}\\bigg|_{u=u_i(\\beta)} (\\nabla u_i(\\beta))^\\top + \\frac{d f}{du}\\bigg|_{u=u_i(\\beta)} \\nabla^2 u_i(\\beta).\n$$\nThe term $u_i(\\beta)$ is a linear function of $\\beta$, so its Hessian is the zero matrix: $\\nabla^2 u_i(\\beta) = \\mathbf{0}$.\nThe second derivative of $f(u)$ is\n$$\n\\frac{d^2 f}{du^2} = \\frac{d}{du} \\left( \\frac{\\exp(u)}{1 + \\exp(u)} \\right) = \\frac{\\exp(u)(1+\\exp(u)) - \\exp(u)\\exp(u)}{(1+\\exp(u))^2} = \\frac{\\exp(u)}{(1+\\exp(u))^2}.\n$$\nLet's define the sigmoid function $\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$. Its derivative is $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$. Notice that $\\frac{d f}{du} = \\frac{\\exp(u)}{1+\\exp(u)} = \\frac{1}{\\exp(-u)+1} = \\sigma(u)$, so $\\frac{d^2 f}{du^2} = \\sigma'(u) = \\sigma(u)(1-\\sigma(u))$. Also, $1-\\sigma(u) = \\sigma(-u)$, thus $\\frac{d^2 f}{du^2} = \\sigma(u)\\sigma(-u)$.\n\nSubstituting into the Hessian chain rule formula:\n$$\n\\nabla^2 \\ell_i(\\beta) = (\\nabla u_i(\\beta)) \\left( \\sigma(u_i(\\beta))\\sigma(-u_i(\\beta)) \\right) (\\nabla u_i(\\beta))^\\top.\n$$\nWith $\\nabla u_i(\\beta) = -y_i x_i$ and $u_i(\\beta) = -y_i x_i^\\top \\beta$:\n$$\n\\nabla^2 \\ell_i(\\beta) = (-y_i x_i) \\left( \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta) \\right) (-y_i x_i)^\\top = y_i^2 \\left( \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta) \\right) x_i x_i^\\top.\n$$\nSince $y_i \\in \\{-1, +1\\}$, we have $y_i^2 = 1$. Let $w_i(\\beta) = \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta)$. The Hessian for the $i$-th term is\n$$\n\\nabla^2 \\ell_i(\\beta) = w_i(\\beta) x_i x_i^\\top.\n$$\nThe full Hessian is the average of these matrices:\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) x_i x_i^\\top.\n$$\nThis can be expressed compactly in matrix form. Let $W$ be a diagonal matrix with diagonal entries $W_{ii} = w_i(\\beta)$. The sum of outer products can be written as\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} X^\\top W X.\n$$\nThe entries of the diagonal matrix $W$ are\n$$\nw_i(\\beta) = \\sigma(y_i x_i^\\top \\beta) \\sigma(-y_i x_i^\\top \\beta) = \\frac{1}{1+\\exp(-y_i x_i^\\top \\beta)} \\cdot \\frac{1}{1+\\exp(y_i x_i^\\top \\beta)} = \\frac{1}{2 + \\exp(y_i x_i^\\top \\beta) + \\exp(-y_i x_i^\\top \\beta)}.\n$$\nSince the range of the sigmoid function is $(0,1)$, the weights $w_i(\\beta)$ are strictly positive for all $\\beta$.\n\n**Justification for Positive Semidefiniteness**: To show $\\nabla^2 \\ell(\\beta)$ is positive semidefinite (PSD), we must show that for any vector $v \\in \\mathbb{R}^p$, $v^\\top (\\nabla^2 \\ell(\\beta)) v \\ge 0$.\n$$\nv^\\top (\\nabla^2 \\ell(\\beta)) v = v^\\top \\left( \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) x_i x_i^\\top \\right) v = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) v^\\top (x_i x_i^\\top) v = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) (x_i^\\top v)^2.\n$$\nAs established, $w_i(\\beta) > 0$. The term $(x_i^\\top v)^2$ is always non-negative. Therefore, the sum is a sum of non-negative terms, which is itself non-negative.\n$$\nv^\\top (\\nabla^2 \\ell(\\beta)) v \\ge 0.\n$$\nThis holds for all $v \\in \\mathbb{R}^p$, confirming that the Hessian $\\nabla^2 \\ell(\\beta)$ is positive semidefinite. This also proves that the logistic loss $\\ell(\\beta)$ is a convex function.\n\n### Part 3: KKT Optimality Conditions\n\nWe consider the $\\ell_1$-regularized logistic regression problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; F(\\beta), \\quad \\text{where} \\quad F(\\beta) = \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}.\n$$\nThis is a convex optimization problem, as it is the sum of two convex functions: $\\ell(\\beta)$ (shown to be convex in Part 2) and $\\lambda\\|\\beta\\|_1$ (since $\\|\\cdot\\|_1$ is a norm and $\\lambda>0$).\nThe first-order necessary and sufficient optimality condition for a convex function $F$ at a point $\\beta^*$ is that the zero vector must be in the subdifferential of $F$ at $\\beta^*$:\n$$\n\\mathbf{0} \\in \\partial F(\\beta^*).\n$$\nFor a sum of two convex functions, where one is differentiable (like $\\ell(\\beta)$), the subdifferential of the sum is the sum of the gradient and the subdifferential:\n$$\n\\partial F(\\beta) = \\nabla \\ell(\\beta) + \\partial (\\lambda \\|\\beta\\|_1) = \\nabla \\ell(\\beta) + \\lambda \\partial \\|\\beta\\|_1.\n$$\nThus, the optimality condition (also known as the Karush-Kuhn-Tucker or KKT condition in this context) for a solution $\\beta^*$ is\n$$\n\\mathbf{0} \\in \\nabla \\ell(\\beta^*) + \\lambda \\partial \\|\\beta^*\\|_1,\n$$\nwhich can be rewritten as\n$$\n-\\nabla \\ell(\\beta^*) \\in \\lambda \\partial \\|\\beta^*\\|_1.\n$$\nThis is the KKT condition in vector form.\n\nTo obtain the coordinate-wise form, we consider the subdifferential of the $\\ell_1$-norm, $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$. The subdifferential $\\partial \\|\\beta\\|_1$ is the Cartesian product of the subdifferentials of each component $|\\beta_j|$. For a single variable $z$, the subdifferential of the absolute value function $|z|$ is:\n$$\n\\partial |z| =\n\\begin{cases}\n\\{\\text{sign}(z)\\}, & \\text{if } z \\neq 0 \\\\\n[-1, 1], & \\text{if } z = 0\n\\end{cases}\n$$\nThe vector-form KKT condition must hold for each coordinate $j \\in \\{1, \\dots, p\\}$. Let $g_j = (\\nabla \\ell(\\beta^*))_j$ be the $j$-th component of the gradient. The condition becomes $-g_j \\in \\lambda \\partial |\\beta^*_j|$.\n\nWe analyze this based on the value of $\\beta^*_j$:\n1.  If $\\beta^*_j > 0$, then $\\partial |\\beta^*_j| = \\{1\\}$, so $-g_j = \\lambda$, which implies $g_j = -\\lambda$.\n2.  If $\\beta^*_j < 0$, then $\\partial |\\beta^*_j| = \\{-1\\}$, so $-g_j = -\\lambda$, which implies $g_j = \\lambda$.\n    These two cases combine to $g_j = -\\lambda \\text{sign}(\\beta^*_j)$ if $\\beta^*_j \\neq 0$.\n3.  If $\\beta^*_j = 0$, then $\\partial |\\beta^*_j| = [-1, 1]$, so $-g_j \\in [-\\lambda, \\lambda]$, which is equivalent to $|g_j| \\le \\lambda$.\n\nSo, the coordinate-wise KKT conditions are:\nFor each $j \\in \\{1, \\dots, p\\}$,\n$$\n\\begin{cases}\n(\\nabla \\ell(\\beta^*))_j = -\\lambda \\text{sign}(\\beta^*_j), & \\text{if } \\beta^*_j \\neq 0 \\\\\n|(\\nabla \\ell(\\beta^*))_j| \\le \\lambda, & \\text{if } \\beta^*_j = 0\n\\end{cases}\n$$\nwhere $(\\nabla \\ell(\\beta^*))_j = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\left( \\frac{\\exp(-y_i x_i^\\top \\beta^*)}{1 + \\exp(-y_i x_i^\\top \\beta^*)} \\right) x_{ij}$.",
            "answer": "$$\\boxed{\\frac{1}{n} X^\\top \\mathrm{diag}\\left(\\left\\{ \\frac{1}{2 + \\exp(y_i x_i^\\top \\beta) + \\exp(-y_i x_i^\\top \\beta)} \\right\\}_{i=1,\\dots,n} \\right) X}$$"
        },
        {
            "introduction": "A crucial question in sparse regression is whether the LASSO estimator can reliably identify the correct set of non-zero coefficients, a property known as variable selection consistency. The \"Irrepresentable Condition\" provides a powerful theoretical answer, linking the success of LASSO to the correlation structure of the design matrix. This exercise challenges you to compute this condition for a given covariance matrix, offering a tangible glimpse into the deep theory that guarantees when sparse recovery is possible. ",
            "id": "3477011",
            "problem": "Consider a linear regression model with a random design matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns are standardized to have zero mean and unit variance. Let the population Gram (covariance) matrix be defined by $\\Sigma = \\mathbb{E}\\!\\left[X^{\\top}X/n\\right] \\in \\mathbb{R}^{p \\times p}$. Assume the least absolute shrinkage and selection operator (LASSO) estimator, defined by solving\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nis used for sparse recovery under a noiseless model $y = X\\beta^{\\star}$ with a proposed support $S \\subset \\{1,\\dots,p\\}$ and sign pattern $s = \\operatorname{sgn}(\\beta^{\\star}_{S}) \\in \\{-1,+1\\}^{|S|}$. Starting from the Karush–Kuhn–Tucker (KKT) conditions for the LASSO and working at the population level, derive the expression for the irrepresentable vector associated with $S$ in terms of block partitions of $\\Sigma$ and $s$, and then evaluate the corresponding $\\ell_{\\infty}$ irrepresentable quantity for the following concrete specification:\n$$\n\\Sigma = \\begin{pmatrix}\n1 & 0.1 & 0.2 & 0.05 \\\\\n0.1 & 1 & 0.05 & 0.1 \\\\\n0.2 & 0.05 & 1 & 0.15 \\\\\n0.05 & 0.1 & 0.15 & 1\n\\end{pmatrix}, \\quad\nS = \\{1,3\\}, \\quad\nS^{c} = \\{2,4\\}, \\quad\ns = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}.\n$$\nLet the tolerance parameter be $\\eta = 0.875$. After deriving the irrepresentable vector, compute its $\\ell_{\\infty}$ norm and use it to decide whether the irrepresentable condition for support recovery at the population level holds, interpreted as the requirement that the computed quantity is less than or equal to $1 - \\eta$. Report the numerical value of the $\\ell_{\\infty}$ irrepresentable quantity as your final answer, rounded to four significant figures. No physical units are involved.",
            "solution": "The problem asks for the derivation and evaluation of the $\\ell_{\\infty}$ irrepresentable quantity for the LASSO estimator at the population level.\n\nThe LASSO estimator is the solution to the optimization problem:\n$$ \\min_{\\beta \\in \\mathbb{R}^{p}} \\ L(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\left\\{ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\} $$\nThe Karush–Kuhn–Tucker (KKT) conditions for optimality state that at a solution $\\hat{\\beta}$, the zero vector must be an element of the subgradient of the objective function $L(\\beta)$. The gradient of the least-squares term is $-\\frac{1}{n}X^{\\top}(y - X\\beta)$. The subgradient of the $\\ell_1$-norm, $\\partial\\|\\beta\\|_1$, is a vector $g$ where $g_j = \\operatorname{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$ and $g_j \\in [-1, 1]$ if $\\beta_j=0$. The KKT conditions are thus:\n$$ -\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta}) + \\lambda g = 0, \\quad \\text{where } g \\in \\partial\\|\\hat{\\beta}\\|_{1} $$\nThis can be rewritten as $\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta}) = \\lambda g$.\n\nThe problem requires an analysis at the population level. We replace sample quantities with their population expectations. Given the noiseless model $y = X\\beta^{\\star}$, the expected value of the gradient term is:\n$$ \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta})\\right] = \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}(X\\beta^{\\star} - X\\hat{\\beta})\\right] = \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}X\\right](\\beta^{\\star} - \\hat{\\beta}) = \\Sigma(\\beta^{\\star} - \\hat{\\beta}) $$\nwhere $\\Sigma = \\mathbb{E}[X^{\\top}X/n]$ is the population Gram matrix. The KKT conditions at the population level become:\n$$ \\Sigma(\\beta^{\\star} - \\hat{\\beta}) = \\lambda g, \\quad g \\in \\partial\\|\\hat{\\beta}\\|_{1} $$\nWe are interested in the condition for correct support recovery, where the LASSO estimator $\\hat{\\beta}$ has the same support $S = \\operatorname{supp}(\\beta^{\\star})$ and sign pattern $s = \\operatorname{sgn}(\\beta^{\\star}_S)$ as the true coefficient vector $\\beta^{\\star}$. This implies that $\\hat{\\beta}_{S^c} = 0$, where $S^c$ is the complement of $S$, and $\\operatorname{sgn}(\\hat{\\beta}_S) = s$.\n\nWe partition the problem based on the support $S$ and its complement $S^c$. The matrix $\\Sigma$ and vectors $\\beta^{\\star}$, $\\hat{\\beta}$ are partitioned as:\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{SS} & \\Sigma_{SS^c} \\\\ \\Sigma_{S^cS} & \\Sigma_{S^cS^c} \\end{pmatrix}, \\quad \\beta^{\\star} = \\begin{pmatrix} \\beta^{\\star}_S \\\\ 0 \\end{pmatrix}, \\quad \\hat{\\beta} = \\begin{pmatrix} \\hat{\\beta}_S \\\\ 0 \\end{pmatrix} $$\nThe KKT conditions are written for each block:\nFor indices $j \\in S$, we have $\\hat{\\beta}_j \\neq 0$, so $g_S = \\operatorname{sgn}(\\hat{\\beta}_S) = s$. The corresponding block of the KKT equations is:\n$$ (\\Sigma(\\beta^{\\star} - \\hat{\\beta}))_S = \\lambda s $$\n$$ \\Sigma_{SS}(\\beta^{\\star}_S - \\hat{\\beta}_S) + \\Sigma_{SS^c}(\\beta^{\\star}_{S^c} - \\hat{\\beta}_{S^c}) = \\lambda s $$\nSince $\\beta^{\\star}_{S^c}=0$ and we assume $\\hat{\\beta}_{S^c}=0$, this simplifies to:\n$$ \\Sigma_{SS}(\\beta^{\\star}_S - \\hat{\\beta}_S) = \\lambda s $$\nAssuming $\\Sigma_{SS}$ is invertible, we can write:\n$$ \\beta^{\\star}_S - \\hat{\\beta}_S = \\lambda \\Sigma_{SS}^{-1} s $$\nFor indices $j \\in S^c$, we have $\\hat{\\beta}_j = 0$, so $|g_j| \\le 1$. The KKT conditions for this block are:\n$$ |(\\Sigma(\\beta^{\\star} - \\hat{\\beta}))_{S^c}| \\le \\lambda \\quad (\\text{element-wise}) $$\n$$ |\\Sigma_{S^cS}(\\beta^{\\star}_S - \\hat{\\beta}_S) + \\Sigma_{S^cS^c}(\\beta^{\\star}_{S^c} - \\hat{\\beta}_{S^c})| \\le \\lambda $$\nThis simplifies to:\n$$ |\\Sigma_{S^cS}(\\beta^{\\star}_S - \\hat{\\beta}_S)| \\le \\lambda $$\nSubstituting the expression for $(\\beta^{\\star}_S - \\hat{\\beta}_S)$ from the active set equations gives:\n$$ |\\Sigma_{S^cS}(\\lambda \\Sigma_{SS}^{-1} s)| \\le \\lambda $$\nFor $\\lambda > 0$, we can divide by $\\lambda$ to obtain the irrepresentable condition:\n$$ |\\Sigma_{S^cS} \\Sigma_{SS}^{-1} s| \\le 1 $$\nThe vector $\\gamma = \\Sigma_{S^cS} \\Sigma_{SS}^{-1} s$ is the irrepresentable vector. The condition for support recovery is that the $\\ell_{\\infty}$ norm of this vector, which we denote as the $\\ell_{\\infty}$ irrepresentable quantity, is less than or equal to $1$:\n$$ \\|\\gamma\\|_{\\infty} = \\|\\Sigma_{S^cS} \\Sigma_{SS}^{-1} s\\|_{\\infty} \\le 1 $$\nNow, we evaluate this quantity for the given numerical specification.\nThe provided data are:\n$$ \\Sigma = \\begin{pmatrix} 1 & 0.1 & 0.2 & 0.05 \\\\ 0.1 & 1 & 0.05 & 0.1 \\\\ 0.2 & 0.05 & 1 & 0.15 \\\\ 0.05 & 0.1 & 0.15 & 1 \\end{pmatrix}, \\quad S = \\{1,3\\}, \\quad S^{c} = \\{2,4\\}, \\quad s = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix} $$\nWe extract the submatrices $\\Sigma_{SS}$ and $\\Sigma_{S^cS}$ according to the indices in $S$ and $S^c$:\n$$ \\Sigma_{SS} = \\begin{pmatrix} \\Sigma_{11} & \\Sigma_{13} \\\\ \\Sigma_{31} & \\Sigma_{33} \\end{pmatrix} = \\begin{pmatrix} 1 & 0.2 \\\\ 0.2 & 1 \\end{pmatrix} $$\n$$ \\Sigma_{S^cS} = \\begin{pmatrix} \\Sigma_{21} & \\Sigma_{23} \\\\ \\Sigma_{41} & \\Sigma_{43} \\end{pmatrix} = \\begin{pmatrix} 0.1 & 0.05 \\\\ 0.05 & 0.15 \\end{pmatrix} $$\nFirst, we compute the inverse of $\\Sigma_{SS}$:\n$$ \\det(\\Sigma_{SS}) = (1)(1) - (0.2)(0.2) = 1 - 0.04 = 0.96 $$\n$$ \\Sigma_{SS}^{-1} = \\frac{1}{0.96} \\begin{pmatrix} 1 & -0.2 \\\\ -0.2 & 1 \\end{pmatrix} = \\frac{100}{96} \\begin{pmatrix} 1 & -0.2 \\\\ -0.2 & 1 \\end{pmatrix} = \\frac{25}{24} \\begin{pmatrix} 1 & -0.2 \\\\ -0.2 & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{24} & -\\frac{5}{24} \\\\ -\\frac{5}{24} & \\frac{25}{24} \\end{pmatrix} $$\nNext, we compute the vector $\\Sigma_{SS}^{-1} s$:\n$$ \\Sigma_{SS}^{-1} s = \\begin{pmatrix} \\frac{25}{24} & -\\frac{5}{24} \\\\ -\\frac{5}{24} & \\frac{25}{24} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{24} + \\frac{5}{24} \\\\ -\\frac{5}{24} - \\frac{25}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{24} \\\\ -\\frac{30}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nNow, we compute the irrepresentable vector $\\gamma = \\Sigma_{S^cS} (\\Sigma_{SS}^{-1} s)$:\n$$ \\gamma = \\begin{pmatrix} 0.1 & 0.05 \\\\ 0.05 & 0.15 \\end{pmatrix} \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} & \\frac{1}{20} \\\\ \\frac{1}{20} & \\frac{3}{20} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nThe components of $\\gamma$ are calculated as:\n$$ \\gamma_1 = \\left(\\frac{1}{10}\\right)\\left(\\frac{5}{4}\\right) + \\left(\\frac{1}{20}\\right)\\left(-\\frac{5}{4}\\right) = \\frac{5}{40} - \\frac{5}{80} = \\frac{10}{80} - \\frac{5}{80} = \\frac{5}{80} = \\frac{1}{16} $$\n$$ \\gamma_2 = \\left(\\frac{1}{20}\\right)\\left(\\frac{5}{4}\\right) + \\left(\\frac{3}{20}\\right)\\left(-\\frac{5}{4}\\right) = \\frac{5}{80} - \\frac{15}{80} = -\\frac{10}{80} = -\\frac{1}{8} $$\nSo the irrepresentable vector is $\\gamma = \\begin{pmatrix} 1/16 \\\\ -1/8 \\end{pmatrix}$.\nThe $\\ell_{\\infty}$ irrepresentable quantity is the maximum absolute value of the components of $\\gamma$:\n$$ \\|\\gamma\\|_{\\infty} = \\max\\left(\\left|\\frac{1}{16}\\right|, \\left|-\\frac{1}{8}\\right|\\right) = \\max\\left(\\frac{1}{16}, \\frac{1}{8}\\right) = \\frac{1}{8} $$\nThe numerical value is $0.125$. The problem requests this value rounded to four significant figures, which is $0.1250$.\nThe problem also asks to decide if the irrepresentable condition holds, where the condition is given as the computed quantity being less than or equal to $1 - \\eta$ with $\\eta = 0.875$. We have $1 - \\eta = 1 - 0.875 = 0.125$. Since our computed quantity is $0.125$, the condition $0.125 \\le 0.125$ is satisfied. The final answer is the numerical value of the $\\ell_{\\infty}$ irrepresentable quantity itself.",
            "answer": "$$\n\\boxed{0.1250}\n$$"
        }
    ]
}