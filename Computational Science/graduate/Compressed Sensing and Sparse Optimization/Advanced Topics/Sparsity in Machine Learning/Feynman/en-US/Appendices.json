{
    "hands_on_practices": [
        {
            "introduction": "At the heart of every dictionary learning algorithm lies the sparse coding step: given a fixed dictionary, we must find a sparse representation for each signal. This exercise focuses on deriving the workhorse solvers for this task, the Iterative Shrinkage-Thresholding Algorithm (ISTA) and its popular accelerated version, FISTA. Starting from the definition of the proximal operator, you will build these powerful iterative methods from the ground up, providing you with a foundational tool for sparse optimization .",
            "id": "3444184",
            "problem": "Consider the sparse coding subproblem that arises in dictionary learning methods such as the Method of Optimal Directions (MOD) and the K-Singular Value Decomposition (K-SVD), as well as in online formulations of dictionary learning. Let $D \\in \\mathbb{R}^{m \\times p}$ be a fixed dictionary, $y \\in \\mathbb{R}^{m}$ be a given signal, and $\\lambda  0$. The sparse code $x \\in \\mathbb{R}^{p}$ is obtained by solving the composite convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2}\\|y - D x\\|_{2}^{2} + \\lambda \\|x\\|_{1}.\n$$\nStarting from the definitions of the proximal operator for a proper, closed, convex function and the characterization of the Lipschitz continuity of the gradient of a smooth function, derive from first principles:\n1. The proximal operator of the function $x \\mapsto \\lambda \\|x\\|_{1}$ in closed form.\n2. The Iterative Shrinkage-Thresholding Algorithm (ISTA) update for the above problem using a constant step size chosen from the Lipschitz constant of the gradient of the smooth term.\n3. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) accelerated updates for the same problem, including the momentum parameter update and the extrapolated iterate, with the same step size choice.\n\nYour derivations must begin with the fundamental definition of the proximal operator and the Lipschitz gradient condition. Explicitly identify the Lipschitz constant of the gradient of the smooth term and use it to determine a valid fixed step size. Express the proximal operator and both ISTA and FISTA updates in closed form without omitting intermediate logical steps. Present your final expressions compactly as a single row matrix whose entries, in order, are the proximal operator, the ISTA update, and the FISTA updates. No numerical evaluation is required.",
            "solution": "The problem presented is a standard composite convex optimization problem known as the Lasso or Basis Pursuit Denoising. The objective function is the sum of a smooth, convex, differentiable loss function and a convex, non-smooth regularization term. We are asked to derive the proximal operator for the regularization term, and the update rules for the Iterative Shrinkage-Thresholding Algorithm (ISTA) and its accelerated version, FISTA, from first principles.\n\nThe optimization problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\;\\; F(x) \\equiv f(x) + g(x)\n$$\nwhere the smooth term is $f(x) = \\frac{1}{2}\\|y - D x\\|_{2}^{2}$ and the non-smooth term is $g(x) = \\lambda \\|x\\|_{1}$.\n\n### 1. Derivation of the Proximal Operator for $g(x) = \\lambda \\|x\\|_{1}$\n\nThe proximal operator of a proper, closed, convex function $h:\\mathbb{R}^p \\to \\mathbb{R} \\cup \\{+\\infty\\}$ with parameter $\\tau  0$ is defined as:\n$$\n\\text{prox}_{\\tau h}(z) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ h(x) + \\frac{1}{2\\tau} \\|x - z\\|_2^2 \\right\\}\n$$\nFor the function $g(x) = \\lambda \\|x\\|_{1}$, its proximal operator is given by:\n$$\n\\text{prox}_{\\tau\\lambda\\|\\cdot\\|_1}(z) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ \\lambda \\|x\\|_{1} + \\frac{1}{2\\tau} \\|x - z\\|_2^2 \\right\\}\n$$\nThe objective function is separable with respect to the components of $x$, since $\\|x\\|_1 = \\sum_{i=1}^p |x_i|$ and $\\|x-z\\|_2^2 = \\sum_{i=1}^p (x_i-z_i)^2$. Thus, we can solve for each component $x_i$ independently by minimizing:\n$$\n\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\lambda |x_i| + \\frac{1}{2\\tau} (x_i - z_i)^2 \\right\\}\n$$\nThis is a one-dimensional convex optimization problem. A point $x_i^*$ is a minimizer if and only if $0$ is in the subdifferential of the objective function at $x_i^*$. The subdifferential of the objective function with respect to $x_i$ is:\n$$\n\\partial \\left( \\lambda |x_i| + \\frac{1}{2\\tau} (x_i - z_i)^2 \\right) = \\lambda \\cdot \\partial|x_i| + \\frac{1}{\\tau}(x_i - z_i)\n$$\nwhere $\\partial|x_i|$ is the subdifferential of the absolute value function:\n$$\n\\partial|x_i| = \\text{sgn}(x_i) = \\begin{cases} \\{1\\}  \\text{if } x_i  0 \\\\ \\{-1\\}  \\text{if } x_i  0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\nThe optimality condition $0 \\in \\lambda \\cdot \\text{sgn}(x_i^*) + \\frac{1}{\\tau}(x_i^* - z_i)$ can be rewritten as $z_i - x_i^* \\in \\tau\\lambda \\cdot \\text{sgn}(x_i^*)$. We analyze this condition in three cases for $x_i^*$:\n\n-   **Case 1: $x_i^*  0$.** Then $\\text{sgn}(x_i^*) = \\{1\\}$, so $z_i - x_i^* = \\tau\\lambda$, which implies $x_i^* = z_i - \\tau\\lambda$. This is consistent with $x_i^*0$ only if $z_i  \\tau\\lambda$.\n-   **Case 2: $x_i^*  0$.** Then $\\text{sgn}(x_i^*) = \\{-1\\}$, so $z_i - x_i^* = -\\tau\\lambda$, which implies $x_i^* = z_i + \\tau\\lambda$. This is consistent with $x_i^*0$ only if $z_i  -\\tau\\lambda$.\n-   **Case 3: $x_i^* = 0$.** Then $\\text{sgn}(x_i^*) = [-1, 1]$, so $z_i - 0 \\in [-\\tau\\lambda, \\tau\\lambda]$, which means $|z_i| \\le \\tau\\lambda$.\n\nCombining these cases, the solution for each component $x_i^*$ is:\n$$\nx_i^* = \\begin{cases} z_i - \\tau\\lambda  \\text{if } z_i  \\tau\\lambda \\\\ 0  \\text{if } |z_i| \\le \\tau\\lambda \\\\ z_i + \\tau\\lambda  \\text{if } z_i  -\\tau\\lambda \\end{cases}\n$$\nThis operation is known as the soft-thresholding operator, which we denote by $S_{\\alpha}(\\cdot)$. For a vector $v$ and scalar $\\alpha0$, it is defined element-wise as $(S_{\\alpha}(v))_i = \\text{sgn}(v_i) \\max(|v_i|-\\alpha, 0)$.\nTherefore, the proximal operator of $g(x) = \\lambda \\|x\\|_1$ is the soft-thresholding operator with threshold $\\tau\\lambda$:\n$$\n\\text{prox}_{\\tau\\lambda\\|\\cdot\\|_1}(z) = S_{\\tau\\lambda}(z)\n$$\n\n### 2. Derivation of the ISTA Update\n\nISTA is an instance of the proximal gradient method, designed for problems of the form $\\min_x f(x) + g(x)$. The iterative update rule is given by:\n$$\nx^{k+1} = \\text{prox}_{\\tau_k g}(x^k - \\tau_k \\nabla f(x^k))\n$$\nwhere $\\tau_k  0$ is the step size at iteration $k$. For convergence, the step size for a constant step scheme is typically chosen based on the Lipschitz constant of $\\nabla f$. A gradient $\\nabla f$ is $L$-Lipschitz continuous if $\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 \\le L \\|x_1 - x_2\\|_2$ for all $x_1, x_2$.\n\nFirst, we compute the gradient of $f(x) = \\frac{1}{2}\\|y - Dx\\|_2^2$:\n$$\nf(x) = \\frac{1}{2}(y - Dx)^T(y - Dx) = \\frac{1}{2}(y^T y - 2y^T Dx + x^T D^T D x)\n$$\nTaking the gradient with respect to $x$ yields:\n$$\n\\nabla f(x) = D^T(Dx - y) = -D^T(y-Dx)\n$$\nNext, we find the Lipschitz constant $L$ of $\\nabla f(x)$. Consider two points $x_1, x_2 \\in \\mathbb{R}^p$:\n$$\n\\nabla f(x_1) - \\nabla f(x_2) = (D^T(Dx_1-y)) - (D^T(Dx_2-y)) = D^T D(x_1 - x_2)\n$$\nTaking the Euclidean norm:\n$$\n\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 = \\|D^T D(x_1 - x_2)\\|_2 \\le \\|D^T D\\|_2 \\|x_1 - x_2\\|_2\n$$\nwhere $\\|D^T D\\|_2$ is the spectral norm of the matrix $D^T D$, which is its largest singular value, or equivalently, the maximum eigenvalue $\\lambda_{\\max}(D^T D)$ since $D^T D$ is positive semi-definite. Thus, the smallest possible Lipschitz constant for $\\nabla f$ is $L = \\|D^T D\\|_2 = \\sigma_{\\max}^2(D)$.\n\nFor a constant step size, convergence is guaranteed if $\\tau \\in (0, 2/L)$. A standard choice is $\\tau = 1/L$. Using this step size, the ISTA update becomes:\n$$\nx^{k+1} = \\text{prox}_{(1/L)g}(x^k - (1/L)\\nabla f(x^k))\n$$\nSubstituting our expressions for $g$, $\\nabla f$, and $L$:\n$$\nx^{k+1} = S_{(\\lambda/L)}(x^k - (1/L)D^T(Dx^k-y))\n$$\nwhere $L = \\|D^T D\\|_2$.\n\n### 3. Derivation of the FISTA Updates\n\nFISTA is an accelerated version of ISTA. It achieves a faster convergence rate by incorporating a momentum term. It maintains a primary sequence of iterates, which we denote as $\\{x_k\\}$, and an auxiliary extrapolated sequence $\\{y_k\\}$. The update rules are derived from Nesterov's accelerated gradient methods. Let $t_k$ be a sequence of momentum parameters. The algorithm, starting from an initial guess $x_0$, is typically initialized as $y_1 = x_0$ and $t_1 = 1$.\n\nFor each iteration $k = 1, 2, \\ldots$:\n1.  The next iterate $x_k$ is computed by applying the proximal gradient step at the extrapolated point $y_k$, using the same step size $\\tau = 1/L$:\n    $$\n    x_k = \\text{prox}_{(1/L)g}(y_k - (1/L)\\nabla f(y_k)) = S_{\\lambda/L}(y_k - (1/L)D^T(Dy_k-y))\n    $$\n2.  The momentum parameter is updated:\n    $$\n    t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}\n    $$\n3.  The next extrapolated point $y_{k+1}$ is formed as a linear combination of the previous two iterates, $x_{k-1}$ and $x_k$:\n    $$\n    y_{k+1} = x_k + \\left(\\frac{t_k - 1}{t_{k+1}}\\right)(x_k - x_{k-1})\n    $$\nThis set of three equations constitutes the FISTA updates for the given problem.\n\nThe final expressions are collected in the answer below.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_{\\tau \\lambda}(z)  x^{k+1} = S_{\\lambda/L}\\left(x^k - \\frac{1}{L} D^T(Dx^k - y)\\right)  \\begin{aligned} x_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L} D^T(Dy_k - y)\\right) \\\\ t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2} \\\\ y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1}) \\end{aligned}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "With a solver for the sparse codes in hand, we now turn our attention to updating the dictionary itself. This practice explores a critical question: what happens if we minimize the learning objective without any constraints on the dictionary atoms? Through a carefully constructed thought experiment, you will discover a fundamental pathology of the unconstrained problem, revealing why practical algorithms like MOD and K-SVD must enforce constraints to prevent divergence and learn meaningful features .",
            "id": "3444131",
            "problem": "Consider the unconstrained sparsity-driven dictionary learning objective for a single training vector in the absence of any normalization or projection of the dictionary:\n$$\n\\min_{D,X}\\; \\frac{1}{2}\\|Y-DX\\|_{F}^{2}+\\lambda\\|X\\|_{1},\n$$\nwhere $Y\\in\\mathbb{R}^{n\\times 1}$ is the data matrix (a single column), $D\\in\\mathbb{R}^{n\\times 1}$ is the dictionary (a single atom), $X\\in\\mathbb{R}^{1\\times 1}$ is the scalar code, $\\|\\cdot\\|_{F}$ denotes the Frobenius norm, $\\|\\cdot\\|_{1}$ denotes the entrywise $\\ell_{1}$ norm, and $\\lambda0$ is a fixed regularization parameter. This formulation, or variants of it, underpins the Method of Optimal Directions (MOD), the K-Singular Value Decomposition (K-SVD), and Online Dictionary Learning (ODL), which in practice enforce unit-norm constraints or projections on dictionary columns to prevent ill-posed scaling.\n\nLet $y\\in\\mathbb{R}^{n}$ be a fixed unit-norm data vector satisfying $\\|y\\|_{2}=1$, and interpret $Y=y$, $D=d\\in\\mathbb{R}^{n}$, and $X=x\\in\\mathbb{R}$. Consider the following naive alternating update that does not normalize or project $d$:\n- For iteration $t=0,1,2,\\dots$, given $d_{t}$ and $x_{t}$, update the dictionary by a single gradient step on the objective holding $x_{t}$ fixed:\n$$\nd_{t+1}=d_{t}-\\eta\\,\\nabla_{d}\\left(\\frac{1}{2}\\|y-dx_{t}\\|_{2}^{2}+\\lambda|x_{t}|\\right)\\Bigg|_{d=d_{t}},\n$$\nwith a fixed step size $\\eta0$.\n- Then update the code exactly by solving the one-dimensional sparse coding subproblem with the updated $d_{t+1}$:\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\mathbb{R}}\\;\\frac{1}{2}\\|y-d_{t+1}x\\|_{2}^{2}+\\lambda|x|.\n$$\n\nAssume the initialization $d_{0}=a_{0}y$ with $a_{0}\\lambda$ and $x_{0}$ chosen as the exact minimizer for $d_{0}$. Using only first principles (gradient calculus and subgradient/Karush–Kuhn–Tucker optimality for the one-dimensional sparse coding), derive the dynamics of the scalar amplitude $a_{t}$ defined by $d_{t}=a_{t}y$, and the resulting objective value\n$$\nf_{t}=\\frac{1}{2}\\|y-d_{t}x_{t}\\|_{2}^{2}+\\lambda|x_{t}|.\n$$\nShow that under this constructed case, the dictionary norm $\\|d_{t}\\|_{2}=|a_{t}|$ grows without bound while the objective value $f_{t}$ decreases monotonically. Compute the exact limit $\\lim_{t\\to\\infty}f_{t}$.\n\nYour final answer must be a single real number. No rounding is required.",
            "solution": "The problem is first validated and found to be well-posed, scientifically grounded, and objective. It presents a simplified but canonical case study of dictionary learning dynamics without dictionary atom normalization, which is a standard topic in the field.\n\nLet the objective function be $f(d, x) = \\frac{1}{2}\\|y-dx\\|_{2}^{2}+\\lambda|x|$, where $y \\in \\mathbb{R}^{n}$ is a fixed vector with $\\|y\\|_{2}=1$, $d \\in \\mathbb{R}^{n}$ is a dictionary atom, $x \\in \\mathbb{R}$ is its corresponding code, and $\\lambda  0$ is a regularization parameter. The optimization proceeds via alternating updates for $d$ and $x$.\n\nFirst, we analyze the dictionary update step. The update for $d_{t+1}$ is a gradient descent step on the objective with $x_t$ held fixed. The term $\\lambda|x_t|$ is constant with respect to $d$.\nThe gradient of the reconstruction error term with respect to $d$ is:\n$$ \\nabla_{d}\\left(\\frac{1}{2}\\|y-dx_{t}\\|_{2}^{2}\\right) = \\nabla_{d}\\left(\\frac{1}{2}(y - dx_t)^T(y - dx_t)\\right) = \\nabla_{d}\\left(\\frac{1}{2}(y^T y - 2x_t d^T y + x_t^2 d^T d)\\right) = -x_t y + x_t^2 d = -x_t(y - dx_t) $$\nThe dictionary update rule is therefore:\n$$ d_{t+1} = d_{t} - \\eta \\nabla_{d} f(d, x_t)|_{d=d_t} = d_{t} - \\eta(-x_{t}(y - d_{t}x_{t})) = d_{t} + \\eta x_{t}(y - d_{t}x_{t}) $$\n\nNext, we analyze the structure of $d_t$. The problem states the initialization $d_0 = a_0 y$ for some scalar $a_0  \\lambda$. We show by induction that $d_t$ remains proportional to $y$ for all iterations $t=0, 1, 2, \\dots$. Assume $d_t = a_t y$ for some scalar $a_t$. Substituting this into the update rule for $d_{t+1}$:\n$$ d_{t+1} = a_t y + \\eta x_t(y - (a_t y) x_t) = a_t y + \\eta x_t(1 - a_t x_t)y = \\left(a_t + \\eta x_t(1 - a_t x_t)\\right)y $$\nThis shows that $d_{t+1}$ is also proportional to $y$. We can thus write $d_{t+1} = a_{t+1}y$, where the scalar amplitude follows the recurrence:\n$$ a_{t+1} = a_t + \\eta x_t(1 - a_t x_t) $$\n\nNow, we derive the expression for the code $x_{t+1}$. It is found by exactly minimizing the objective with $d_{t+1}$ fixed:\n$$ x_{t+1} = \\arg\\min_{x \\in \\mathbb{R}}\\; h(x) \\quad \\text{where} \\quad h(x) = \\frac{1}{2}\\|y-d_{t+1}x\\|_{2}^{2}+\\lambda|x| $$\nThis is a one-dimensional Lasso problem. The objective can be expanded as:\n$$ h(x) = \\frac{1}{2}(y^T y - 2x y^T d_{t+1} + x^2 \\|d_{t+1}\\|_2^2) + \\lambda|x| $$\nThe solution is given by the soft-thresholding operator. The first-order optimality condition from subgradient calculus is $0 \\in \\partial_x h(x)$, where:\n$$ \\partial_x h(x) = \\|d_{t+1}\\|_2^2 x - y^T d_{t+1} + \\lambda \\cdot \\mathrm{sgn}(x) $$\nwhere $\\mathrm{sgn}(x)$ is the subgradient of $|x|$. The solution is:\n$$ x_{t+1} = \\frac{1}{\\|d_{t+1}\\|_2^2} \\text{soft-thresh}(y^T d_{t+1}, \\lambda) = \\frac{1}{\\|d_{t+1}\\|_2^2} \\mathrm{sgn}(y^T d_{t+1}) \\max(|y^T d_{t+1}| - \\lambda, 0) $$\nSince $d_{t+1} = a_{t+1}y$ and $\\|y\\|_2=1$, we have $\\|d_{t+1}\\|_2^2 = a_{t+1}^2 \\|y\\|_2^2 = a_{t+1}^2$ and $y^T d_{t+1} = y^T(a_{t+1}y) = a_{t+1} \\|y\\|_2^2 = a_{t+1}$. The expression for $x_{t+1}$ simplifies to:\n$$ x_{t+1} = \\frac{1}{a_{t+1}^2} \\text{soft-thresh}(a_{t+1}, \\lambda) $$\nThis applies for any iteration, so $x_t = \\frac{1}{a_t^2} \\text{soft-thresh}(a_t, \\lambda)$.\n\nNow we establish the dynamics of $a_t$. We start with $a_0  \\lambda  0$. Thus, $\\text{soft-thresh}(a_0, \\lambda) = a_0 - \\lambda  0$. So, $x_0 = \\frac{a_0-\\lambda}{a_0^2}  0$.\nLet's assume $a_t  \\lambda$. Then $x_t = \\frac{a_t-\\lambda}{a_t^2}$. We substitute this into the recurrence for $a_t$:\n$$ a_{t+1} = a_t + \\eta x_t(1 - a_t x_t) = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(1 - a_t \\frac{a_t-\\lambda}{a_t^2}\\right) $$\n$$ a_{t+1} = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(1 - \\frac{a_t-\\lambda}{a_t}\\right) = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(\\frac{a_t - (a_t-\\lambda)}{a_t}\\right) = a_t + \\eta \\lambda \\frac{a_t-\\lambda}{a_t^3} $$\nSince $a_t  \\lambda$, $\\eta  0$, and $\\lambda  0$, the term $\\eta \\lambda \\frac{a_t-\\lambda}{a_t^3}$ is strictly positive.\nThus, $a_{t+1}  a_t$. Since $a_0  \\lambda$, it follows by induction that the sequence $(a_t)_{t \\ge 0}$ is strictly increasing and $a_t  \\lambda$ for all $t \\ge 0$. This confirms our assumption for using the simplified expression for $x_t$.\nAn increasing sequence either converges to a finite limit or diverges to infinity. If $a_t \\to L$ for some finite $L$, then $L$ must be a fixed point of the recurrence:\n$$ L = L + \\eta \\lambda \\frac{L-\\lambda}{L^3} \\quad \\implies \\quad \\eta \\lambda \\frac{L-\\lambda}{L^3} = 0 $$\nThis implies $L=\\lambda$. However, since $(a_t)$ is strictly increasing and $a_0  \\lambda$, the limit must satisfy $L \\ge a_0  \\lambda$. This is a contradiction. Therefore, the sequence $(a_t)$ cannot converge to a finite limit and must diverge: $\\lim_{t\\to\\infty} a_t = \\infty$.\nThe dictionary norm is $\\|d_t\\|_2 = \\|a_t y\\|_2 = |a_t| \\|y\\|_2 = |a_t|$. Since $a_t$ is an increasing sequence starting from $a_0  0$, $a_t  0$ for all $t$, so $\\|d_t\\|_2 = a_t$. Thus, $\\|d_t\\|_2$ grows without bound.\n\nNext, we analyze the objective value $f_t = \\frac{1}{2}\\|y-d_t x_t\\|_2^2 + \\lambda |x_t|$. We express $f_t$ in terms of $a_t$.\nThe reconstruction residual is $y - d_t x_t = y - (a_t y) \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) = y\\left(1 - \\frac{a_t-\\lambda}{a_t}\\right) = y\\left(\\frac{\\lambda}{a_t}\\right)$.\nThe squared norm of the residual is $\\|y-d_t x_t\\|_2^2 = \\|\\frac{\\lambda}{a_t}y\\|_2^2 = \\frac{\\lambda^2}{a_t^2}\\|y\\|_2^2 = \\frac{\\lambda^2}{a_t^2}$.\nThe regularization term is $\\lambda|x_t| = \\lambda x_t = \\lambda\\frac{a_t-\\lambda}{a_t^2}$ since $a_t\\lambda0$ implies $x_t0$.\nSo, the objective value is:\n$$ f_t = \\frac{1}{2}\\left(\\frac{\\lambda^2}{a_t^2}\\right) + \\lambda\\left(\\frac{a_t-\\lambda}{a_t^2}\\right) = \\frac{\\lambda^2 + 2\\lambda a_t - 2\\lambda^2}{2a_t^2} = \\frac{2\\lambda a_t - \\lambda^2}{2a_t^2} = \\frac{\\lambda}{a_t} - \\frac{\\lambda^2}{2a_t^2} $$\nTo show that $f_t$ decreases monotonically, consider the function $g(a) = \\frac{\\lambda}{a} - \\frac{\\lambda^2}{2a^2}$. Then $f_t=g(a_t)$. The derivative of $g(a)$ is:\n$$ g'(a) = \\frac{d}{da}\\left(\\lambda a^{-1} - \\frac{\\lambda^2}{2} a^{-2}\\right) = -\\lambda a^{-2} + \\lambda^2 a^{-3} = \\frac{-\\lambda a + \\lambda^2}{a^3} = \\frac{\\lambda(\\lambda-a)}{a^3} $$\nSince we established that $a_t  \\lambda$ for all $t$, the term $\\lambda - a_t$ is negative, while $\\lambda$ and $a_t^3$ are positive. Thus, $g'(a_t)  0$.\nThe sequence $(a_t)$ is strictly increasing. Since $f_t = g(a_t)$ and $g(a)$ is a strictly decreasing function for $a  \\lambda$, it follows that the sequence $(f_t)$ is strictly decreasing. This demonstrates monotonic decrease of the objective value.\n\nFinally, we compute the limit of $f_t$ as $t \\to \\infty$. We have previously shown that $\\lim_{t\\to\\infty} a_t = \\infty$.\n$$ \\lim_{t\\to\\infty} f_t = \\lim_{t\\to\\infty} \\left(\\frac{\\lambda}{a_t} - \\frac{\\lambda^2}{2a_t^2}\\right) $$\nAs $a_t \\to \\infty$, both terms in the expression go to $0$:\n$$ \\lim_{a_t\\to\\infty} \\frac{\\lambda}{a_t} = 0 \\quad \\text{and} \\quad \\lim_{a_t\\to\\infty} \\frac{\\lambda^2}{2a_t^2} = 0 $$\nTherefore, the limit of the objective function is:\n$$ \\lim_{t\\to\\infty} f_t = 0 - 0 = 0 $$\nThis analysis highlights a key pathology of unconstrained dictionary learning: the dictionary atom norm can grow infinitely large, while the sparse code magnitude shrinks to zero, allowing the reconstruction error and the objective function to approach zero without providing a meaningful sparse representation. This is the primary motivation for imposing constraints (e.g., unit norm) on the dictionary atoms in practical algorithms like MOD and K-SVD.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "Having established the necessity of dictionary constraints, this final practice provides a constructive solution. We will implement a dictionary update step that respects these constraints using the projected gradient descent method, a technique central to the Method of Optimal Directions (MOD). You will derive the gradient of an incoherence-promoting penalty term and combine it with a projection step that forces the dictionary atoms to maintain unit norm, ensuring the learning process is stable and well-behaved .",
            "id": "3444142",
            "problem": "In dictionary learning for compressed sensing and sparse optimization, the Method of Optimal Directions (MOD) and K-Singular Value Decomposition (K-SVD) often incorporate incoherence-promoting terms to discourage highly correlated atoms. Consider a dictionary matrix $D \\in \\mathbb{R}^{m \\times k}$ whose columns are intended to have unit Euclidean norm and the incoherence penalty\n$$\\phi(D) = \\frac{\\eta}{2}\\left\\|D^{\\top} D - I\\right\\|_{F}^{2},$$\nwhere $\\eta  0$ is a regularization parameter, $I \\in \\mathbb{R}^{k \\times k}$ is the identity, and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. Starting only from the following foundational facts:\n- the Frobenius norm identity $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$ for any matrix $A$,\n- the cyclic property of the trace $\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$ for conformable matrices,\n- the differential of the Gram operator $G(D) = D^{\\top}D$ given by $\\mathrm{d}(D^{\\top}D) = \\mathrm{d}D^{\\top}\\,D + D^{\\top}\\,\\mathrm{d}D$,\n- and the definition of the gradient via the first-order variation $\\mathrm{d}\\phi(D) = \\operatorname{tr}\\!\\big((\\nabla \\phi(D))^{\\top}\\mathrm{d}D\\big)$,\nderive the gradient $\\nabla \\phi(D)$ and then construct a projected gradient update with step size $\\tau  0$ that preserves the unit-norm constraint on each column of $D$ by projecting onto the set $\\mathcal{C} = \\{D \\in \\mathbb{R}^{m \\times k} : \\|D_{:,j}\\|_{2} = 1 \\text{ for all } j \\in \\{1,\\dots,k\\}\\}$. Express the projection as a right multiplication by a diagonal rescaling matrix whose diagonal entries are the inverses of the column norms of the pre-projected iterate.\n\nYour final answer must be a single closed-form analytic expression for one such projected gradient update in terms of $D$, $\\eta$, $\\tau$, and $I$. No numerical evaluation is required.",
            "solution": "The problem statement is well-posed, scientifically grounded, and self-contained. It provides all necessary definitions and foundational facts to derive the requested gradient update rule. The context is standard in the field of sparse optimization and dictionary learning. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe objective is to first find the gradient of the incoherence penalty function $\\phi(D)$ and then use it to construct a projected gradient descent update step.\n\nLet the penalty function be\n$$ \\phi(D) = \\frac{\\eta}{2}\\left\\|D^{\\top} D - I\\right\\|_{F}^{2} $$\nwhere $D \\in \\mathbb{R}^{m \\times k}$, $I \\in \\mathbb{R}^{k \\times k}$ is the identity matrix, and $\\eta  0$.\n\nFirst, we derive the gradient $\\nabla \\phi(D)$. We start by expressing the squared Frobenius norm using the provided trace identity, $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$.\nLet $A = D^{\\top}D - I$. Then $A^{\\top} = (D^{\\top}D - I)^{\\top} = (D^{\\top}D)^{\\top} - I^{\\top} = D^{\\top}(D^{\\top})^{\\top} - I = D^{\\top}D - I$, since the identity matrix $I$ is symmetric. Thus, $A$ is a symmetric matrix.\nThe penalty function can be written as:\n$$ \\phi(D) = \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)^{\\top}(D^{\\top}D - I)\\right) = \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)(D^{\\top}D - I)\\right) $$\nTo find the gradient, we first compute the first-order variation, or differential, $\\mathrm{d}\\phi(D)$. Using the linearity of the differential and trace operators, and the product rule for differentials, we have:\n$$ \\mathrm{d}\\phi(D) = \\mathrm{d}\\left( \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)(D^{\\top}D - I)\\right) \\right) $$\n$$ \\mathrm{d}\\phi(D) = \\frac{\\eta}{2} \\operatorname{tr}\\left( \\mathrm{d}(D^{\\top}D - I)(D^{\\top}D - I) + (D^{\\top}D - I)\\mathrm{d}(D^{\\top}D - I) \\right) $$\nSince the two terms inside the trace are transposes of each other and the trace of a matrix is equal to the trace of its transpose, they are equal. A simpler way is to note that they are identical products, so we can combine them:\n$$ \\mathrm{d}\\phi(D) = \\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}(D^{\\top}D - I) \\right) $$\nThe differential of a constant matrix ($I$) is zero, so $\\mathrm{d}(D^{\\top}D - I) = \\mathrm{d}(D^{\\top}D)$. The problem provides the differential of the Gramian matrix $G(D) = D^{\\top}D$ as $\\mathrm{d}(D^{\\top}D) = \\mathrm{d}D^{\\top}D + D^{\\top}\\mathrm{d}D$. Substituting this into our expression for $\\mathrm{d}\\phi(D)$:\n$$ \\mathrm{d}\\phi(D) = \\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) (\\mathrm{d}D^{\\top}D + D^{\\top}\\mathrm{d}D) \\right) $$\n$$ \\mathrm{d}\\phi(D) = \\eta \\left( \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right) + \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) \\right) $$\nWe need to manipulate this expression into the form $\\mathrm{d}\\phi(D) = \\operatorname{tr}((\\nabla \\phi(D))^{\\top}\\mathrm{d}D)$. Let's analyze the first term, $\\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right)$. Using the property $\\operatorname{tr}(X) = \\operatorname{tr}(X^{\\top})$:\n$$ \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right) = \\operatorname{tr}\\left( \\left((D^{\\top}D - I) \\mathrm{d}D^{\\top}D\\right)^{\\top} \\right) = \\operatorname{tr}\\left( D^{\\top}(\\mathrm{d}D^{\\top})^{\\top}(D^{\\top}D - I)^{\\top} \\right) $$\nSince $(D^{\\top}D - I)$ is symmetric and $(\\mathrm{d}D^{\\top})^{\\top} = \\mathrm{d}D$, this becomes:\n$$ \\operatorname{tr}\\left( D^{\\top}\\mathrm{d}D (D^{\\top}D - I) \\right) $$\nNow, using the cyclic property of the trace, $\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$, with $A=D^{\\top}$, $B=\\mathrm{d}D$, and $C=(D^{\\top}D - I)$:\n$$ \\operatorname{tr}\\left( D^{\\top}\\mathrm{d}D (D^{\\top}D - I) \\right) = \\operatorname{tr}\\left( \\mathrm{d}D (D^{\\top}D - I)D^{\\top} \\right) $$\nLet's examine the second term in the expression for $\\mathrm{d}\\phi(D)$, which is $\\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right)$. Using the cyclic property with $A=(D^{\\top}D - I)D^{\\top}$ and $B=\\mathrm{d}D$, we have $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$:\n$$ \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) = \\operatorname{tr}\\left( \\mathrm{d}D (D^{\\top}D - I) D^{\\top} \\right) $$\nWe observe that the two terms are identical. Therefore, we can combine them:\n$$ \\mathrm{d}\\phi(D) = \\eta \\left( \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) + \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) \\right) $$\n$$ \\mathrm{d}\\phi(D) = 2\\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) $$\nTo match this to the definition $\\mathrm{d}\\phi(D) = \\operatorname{tr}((\\nabla \\phi(D))^{\\top}\\mathrm{d}D)$, we identify $(\\nabla \\phi(D))^{\\top}$ with the matrix multiplying $\\mathrm{d}D$ from the left inside the trace. However, the standard inner product for real matrices is $\\langle X, Y \\rangle = \\operatorname{tr}(X^{\\top}Y)$. The definition of the gradient is based on this. We have an expression of the form $\\operatorname{tr}(A B)$, where we want it to be $\\operatorname{tr}(G^{\\top}B)$. We must identify $G^{\\top} = A$.\nSo, we must have $(\\nabla \\phi(D))^{\\top} = 2\\eta (D^{\\top}D - I) D^{\\top}$.\nThe gradient $\\nabla \\phi(D)$ is the transpose of this expression:\n$$ \\nabla \\phi(D) = \\left( 2\\eta (D^{\\top}D - I) D^{\\top} \\right)^{\\top} = 2\\eta (D^{\\top})^{\\top} (D^{\\top}D - I)^{\\top} $$\n$$ \\nabla \\phi(D) = 2\\eta D (D^{\\top}D - I) $$\nThis expression for the gradient has dimensions $(m \\times k) \\times (k \\times k) = m \\times k$, which matches the dimensions of $D$, as expected.\n\nNext, we construct the projected gradient update. The standard gradient descent update for $D$ with a step size $\\tau  0$ is:\n$$ D_{\\text{temp}} = D - \\tau \\nabla \\phi(D) $$\nSubstituting our derived gradient:\n$$ D_{\\text{temp}} = D - 2\\eta\\tau D(D^{\\top}D - I) $$\nWe can factor out $D$ to write this more compactly:\n$$ D_{\\text{temp}} = D(I - 2\\eta\\tau(D^{\\top}D - I)) $$\nThis temporary iterate $D_{\\text{temp}}$ does not, in general, have unit-norm columns. We must project it onto the constraint set $\\mathcal{C} = \\{X \\in \\mathbb{R}^{m \\times k} : \\|X_{:,j}\\|_{2} = 1 \\text{ for all } j\\}$. The projection involves normalizing each column of $D_{\\text{temp}}$. Let $D_{\\text{next}}$ be the updated matrix. The $j$-th column of $D_{\\text{next}}$ is:\n$$ (D_{\\text{next}})_{:,j} = \\frac{(D_{\\text{temp}})_{:,j}}{\\|(D_{\\text{temp}})_{:,j}\\|_{2}} $$\nThis operation can be expressed as a right-multiplication by a diagonal matrix $S \\in \\mathbb{R}^{k \\times k}$, where the diagonal entries are the inverses of the column norms of $D_{\\text{temp}}$:\n$$ D_{\\text{next}} = D_{\\text{temp}}S, \\quad \\text{where} \\quad S = \\operatorname{diag}\\left(\\frac{1}{\\|(D_{\\text{temp}})_{:,1}\\|_{2}}, \\dots, \\frac{1}{\\|(D_{\\text{temp}})_{:,k}\\|_{2}}\\right) $$\nThe squared norm of the $j$-th column of $D_{\\text{temp}}$ is the $j$-th diagonal entry of the matrix $D_{\\text{temp}}^{\\top}D_{\\text{temp}}$. Let $N = \\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}})$. This is a diagonal matrix whose entries are $N_{jj} = \\|(D_{\\text{temp}})_{:,j}\\|_{2}^{2}$.\nTherefore, the diagonal rescaling matrix $S$ can be written as $S = N^{-1/2} = (\\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}}))^{-1/2}$.\n\nCombining these pieces, the complete projected gradient update is:\n$$ D_{\\text{next}} = D_{\\text{temp}} (\\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}}))^{-1/2} $$\nSubstituting the expression for $D_{\\text{temp}} = D(I - 2\\eta\\tau(D^{\\top}D - I))$, we obtain the final closed-form expression for the update rule.\nLet $U = D(I - 2\\eta\\tau(D^{\\top}D - I))$. The update is given by:\n$$ D_{\\text{next}} = U \\left( \\operatorname{diag}(U^{\\top} U) \\right)^{-1/2} $$\nThis is a single analytic expression for the projected gradient update in terms of $D$, $\\eta$, $\\tau$, and $I$.",
            "answer": "$$ \\boxed{\\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right) \\left( \\operatorname{diag}\\left( \\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right)^{\\top} \\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right) \\right) \\right)^{-1/2}} $$"
        }
    ]
}