{
    "hands_on_practices": [
        {
            "introduction": "在深入研究求解算法之前，理解字典学习目标函数的潜在陷阱至关重要。这个练习通过一个精心设计的思想实验，揭示了在没有对字典原子施加约束（如范数归一化）的情况下，朴素的交替优化会如何导致模型参数发散，最终得到毫无意义的解。通过这个实践，你将深刻体会到对字典施加约束（例如，单位范数约束）的根本必要性 。",
            "id": "3444131",
            "problem": "考虑在字典没有任何归一化或投影的情况下，针对单个训练向量的无约束稀疏性驱动字典学习目标：\n$$\n\\min_{D,X}\\; \\frac{1}{2}\\|Y-DX\\|_{F}^{2}+\\lambda\\|X\\|_{1},\n$$\n其中 $Y\\in\\mathbb{R}^{n\\times 1}$ 是数据矩阵（单列），$D\\in\\mathbb{R}^{n\\times 1}$ 是字典（单个原子），$X\\in\\mathbb{R}^{1\\times 1}$ 是标量编码，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数，$\\|\\cdot\\|_{1}$ 表示逐元素的 $\\ell_{1}$ 范数，$\\lambda0$ 是一个固定的正则化参数。这种形式或其变体是“最优方向法”(MOD)、“K-奇异值分解”(K-SVD)和“在线字典学习”(ODL)等算法的基础，这些算法在实践中对字典列强制施加单位范数约束或投影，以防止病态缩放。\n\n设 $y\\in\\mathbb{R}^{n}$ 是一个满足 $\\|y\\|_{2}=1$ 的固定单位范数数据向量，并将 $Y=y$，$D=d\\in\\mathbb{R}^{n}$ 和 $X=x\\in\\mathbb{R}$ 进行解释。考虑以下不对 $d$ 进行归一化或投影的朴素交替更新方法：\n- 对于迭代 $t=0,1,2,\\dots$，给定 $d_{t}$ 和 $x_{t}$，通过在目标函数上执行单个梯度步长来更新字典，同时保持 $x_{t}$ 固定：\n$$\nd_{t+1}=d_{t}-\\eta\\,\\nabla_{d}\\left(\\frac{1}{2}\\|y-dx_{t}\\|_{2}^{2}+\\lambda|x_{t}|\\right)\\Bigg|_{d=d_{t}},\n$$\n其中 $\\eta0$ 是一个固定的步长。\n- 然后通过求解使用更新后的 $d_{t+1}$ 的一维稀疏编码子问题来精确更新编码：\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\mathbb{R}}\\;\\frac{1}{2}\\|y-d_{t+1}x\\|_{2}^{2}+\\lambda|x|.\n$$\n\n假设初始化为 $d_{0}=a_{0}y$，其中 $a_{0}\\lambda$，且 $x_{0}$ 被选为对于 $d_{0}$ 的精确最小化解。仅使用第一性原理（梯度微积分和一维稀疏编码的次梯度/Karush–Kuhn–Tucker最优性条件），推导由 $d_{t}=a_{t}y$ 定义的标量振幅 $a_{t}$ 的动态变化，以及由此产生的目标值\n$$\nf_{t}=\\frac{1}{2}\\|y-d_{t}x_{t}\\|_{2}^{2}+\\lambda|x_{t}|.\n$$\n证明在这个构造的情况下，字典范数 $\\|d_{t}\\|_{2}=|a_{t}|$ 无界增长，而目标值 $f_{t}$ 单调递减。计算精确极限 $\\lim_{t\\to\\infty}f_{t}$。\n\n你的最终答案必须是一个实数。不需要四舍五入。",
            "solution": "该问题首先经过验证，确认其是适定的、有科学依据且客观的。它提出了一个关于无字典原子归一化的字典学习动态的简化但经典的案例研究，这是该领域的一个标准课题。\n\n设目标函数为 $f(d, x) = \\frac{1}{2}\\|y-dx\\|_{2}^{2}+\\lambda|x|$，其中 $y \\in \\mathbb{R}^{n}$ 是一个范数为 $\\|y\\|_{2}=1$ 的固定向量，$d \\in \\mathbb{R}^{n}$ 是一个字典原子，$x \\in \\mathbb{R}$ 是其对应的编码，$\\lambda  0$ 是一个正则化参数。优化过程通过对 $d$ 和 $x$ 的交替更新进行。\n\n首先，我们分析字典更新步骤。$d_{t+1}$ 的更新是在目标函数上关于 $d$ 的梯度下降步，同时保持 $x_t$ 固定。项 $\\lambda|x_t|$ 相对于 $d$ 是一个常数。\n重建误差项关于 $d$ 的梯度是：\n$$ \\nabla_{d}\\left(\\frac{1}{2}\\|y-dx_{t}\\|_{2}^{2}\\right) = \\nabla_{d}\\left(\\frac{1}{2}(y - dx_t)^T(y - dx_t)\\right) = \\nabla_{d}\\left(\\frac{1}{2}(y^T y - 2x_t d^T y + x_t^2 d^T d)\\right) = -x_t y + x_t^2 d = -x_t(y - dx_t) $$\n因此，字典的更新规则是：\n$$ d_{t+1} = d_{t} - \\eta \\nabla_{d} f(d, x_t)|_{d=d_t} = d_{t} - \\eta(-x_{t}(y - d_{t}x_{t})) = d_{t} + \\eta x_{t}(y - d_{t}x_{t}) $$\n\n接下来，我们分析 $d_t$ 的结构。问题陈述了初始化为 $d_0 = a_0 y$，其中标量 $a_0  \\lambda$。我们通过归纳法证明，对于所有迭代 $t=0, 1, 2, \\dots$，$d_t$ 始终与 $y$ 成比例。假设对于某个标量 $a_t$，有 $d_t = a_t y$。将此代入 $d_{t+1}$ 的更新规则：\n$$ d_{t+1} = a_t y + \\eta x_t(y - (a_t y) x_t) = a_t y + \\eta x_t(1 - a_t x_t)y = \\left(a_t + \\eta x_t(1 - a_t x_t)\\right)y $$\n这表明 $d_{t+1}$ 也与 $y$ 成比例。因此我们可以写成 $d_{t+1} = a_{t+1}y$，其中标量振幅遵循以下递推关系：\n$$ a_{t+1} = a_t + \\eta x_t(1 - a_t x_t) $$\n\n现在，我们推导编码 $x_{t+1}$ 的表达式。它是通过在固定 $d_{t+1}$ 的情况下精确最小化目标函数得到的：\n$$ x_{t+1} = \\arg\\min_{x \\in \\mathbb{R}}\\; h(x) \\quad \\text{其中} \\quad h(x) = \\frac{1}{2}\\|y-d_{t+1}x\\|_{2}^{2}+\\lambda|x| $$\n这是一个一维Lasso问题。目标函数可以展开为：\n$$ h(x) = \\frac{1}{2}(y^T y - 2x y^T d_{t+1} + x^2 \\|d_{t+1}\\|_2^2) + \\lambda|x| $$\n解由软阈值算子给出。根据次梯度微积分，一阶最优性条件是 $0 \\in \\partial_x h(x)$，其中：\n$$ \\partial_x h(x) = \\|d_{t+1}\\|_2^2 x - y^T d_{t+1} + \\lambda \\cdot \\mathrm{sgn}(x) $$\n其中 $\\mathrm{sgn}(x)$ 是 $|x|$ 的次梯度。解是：\n$$ x_{t+1} = \\frac{1}{\\|d_{t+1}\\|_2^2} \\mathrm{soft-thresh}(y^T d_{t+1}, \\lambda) = \\frac{1}{\\|d_{t+1}\\|_2^2} \\mathrm{sgn}(y^T d_{t+1}) \\max(|y^T d_{t+1}| - \\lambda, 0) $$\n由于 $d_{t+1} = a_{t+1}y$ 且 $\\|y\\|_2=1$，我们有 $\\|d_{t+1}\\|_2^2 = a_{t+1}^2 \\|y\\|_2^2 = a_{t+1}^2$ 以及 $y^T d_{t+1} = y^T(a_{t+1}y) = a_{t+1} \\|y\\|_2^2 = a_{t+1}$。因此，$x_{t+1}$ 的表达式简化为：\n$$ x_{t+1} = \\frac{1}{a_{t+1}^2} \\mathrm{soft-thresh}(a_{t+1}, \\lambda) $$\n这适用于任何一次迭代，因此 $x_t = \\frac{1}{a_t^2} \\mathrm{soft-thresh}(a_t, \\lambda)$。\n\n现在我们来确定 $a_t$ 的动态。我们从 $a_0  \\lambda  0$ 开始。因此，$\\mathrm{soft-thresh}(a_0, \\lambda) = a_0 - \\lambda  0$。所以，$x_0 = \\frac{a_0-\\lambda}{a_0^2}  0$。\n假设 $a_t  \\lambda$。那么 $x_t = \\frac{a_t-\\lambda}{a_t^2}$。我们将此代入 $a_t$ 的递推关系中：\n$$ a_{t+1} = a_t + \\eta x_t(1 - a_t x_t) = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(1 - a_t \\frac{a_t-\\lambda}{a_t^2}\\right) $$\n$$ a_{t+1} = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(1 - \\frac{a_t-\\lambda}{a_t}\\right) = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(\\frac{a_t - (a_t-\\lambda)}{a_t}\\right) = a_t + \\eta \\lambda \\frac{a_t-\\lambda}{a_t^3} $$\n由于 $a_t  \\lambda$，$\\eta  0$ 且 $\\lambda  0$，项 $\\eta \\lambda \\frac{a_t-\\lambda}{a_t^3}$ 是严格为正的。\n因此，$a_{t+1}  a_t$。由于 $a_0  \\lambda$，通过归纳法可以得出，序列 $(a_t)_{t \\ge 0}$ 是严格递增的，并且对于所有 $t \\ge 0$ 都有 $a_t  \\lambda$。这证实了我们使用 $x_t$ 的简化表达式的假设。\n一个递增序列要么收敛到一个有限极限，要么发散到无穷大。如果 $a_t \\to L$（其中 $L$ 是某个有限值），那么 $L$ 必须是该递推关系的一个不动点：\n$$ L = L + \\eta \\lambda \\frac{L-\\lambda}{L^3} \\quad \\implies \\quad \\eta \\lambda \\frac{L-\\lambda}{L^3} = 0 $$\n这意味着 $L=\\lambda$。然而，由于 $(a_t)$ 是严格递增的且 $a_0  \\lambda$，其极限必须满足 $L \\ge a_0  \\lambda$。这是一个矛盾。因此，序列 $(a_t)$ 不能收敛到有限极限，必须发散：$\\lim_{t\\to\\infty} a_t = \\infty$。\n字典范数为 $\\|d_t\\|_2 = \\|a_t y\\|_2 = |a_t| \\|y\\|_2 = |a_t|$。由于 $a_t$ 是一个从 $a_0  0$ 开始的递增序列，因此对所有 $t$ 都有 $a_t  0$，所以 $\\|d_t\\|_2 = a_t$。因此，$\\|d_t\\|_2$ 无界增长。\n\n接下来，我们分析目标值 $f_t = \\frac{1}{2}\\|y-d_t x_t\\|_2^2 + \\lambda |x_t|$。我们将 $f_t$ 用 $a_t$ 来表示。\n重建残差为 $y - d_t x_t = y - (a_t y) \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) = y\\left(1 - \\frac{a_t-\\lambda}{a_t}\\right) = y\\left(\\frac{\\lambda}{a_t}\\right)$。\n残差的平方范数为 $\\|y-d_t x_t\\|_2^2 = \\|\\frac{\\lambda}{a_t}y\\|_2^2 = \\frac{\\lambda^2}{a_t^2}\\|y\\|_2^2 = \\frac{\\lambda^2}{a_t^2}$。\n正则化项为 $\\lambda|x_t| = \\lambda x_t = \\lambda\\frac{a_t-\\lambda}{a_t^2}$，因为 $a_t\\lambda0$ 意味着 $x_t0$。\n所以，目标值为：\n$$ f_t = \\frac{1}{2}\\left(\\frac{\\lambda^2}{a_t^2}\\right) + \\lambda\\left(\\frac{a_t-\\lambda}{a_t^2}\\right) = \\frac{\\lambda^2 + 2\\lambda a_t - 2\\lambda^2}{2a_t^2} = \\frac{2\\lambda a_t - \\lambda^2}{2a_t^2} = \\frac{\\lambda}{a_t} - \\frac{\\lambda^2}{2a_t^2} $$\n为了证明 $f_t$ 单调递减，我们考虑函数 $g(a) = \\frac{\\lambda}{a} - \\frac{\\lambda^2}{2a^2}$。那么有 $f_t=g(a_t)$。$g(a)$ 的导数是：\n$$ g'(a) = \\frac{d}{da}\\left(\\lambda a^{-1} - \\frac{\\lambda^2}{2} a^{-2}\\right) = -\\lambda a^{-2} + \\lambda^2 a^{-3} = \\frac{-\\lambda a + \\lambda^2}{a^3} = \\frac{\\lambda(\\lambda-a)}{a^3} $$\n由于我们已经证明了对所有 $t$ 都有 $a_t  \\lambda$，所以项 $\\lambda - a_t$ 是负的，而 $\\lambda$ 和 $a_t^3$ 是正的。因此，$g'(a_t)  0$。\n序列 $(a_t)$ 是严格递增的。由于 $f_t = g(a_t)$ 且当 $a  \\lambda$ 时 $g(a)$ 是一个严格递减函数，因此序列 $(f_t)$ 是严格递减的。这证明了目标值是单调递减的。\n\n最后，我们计算当 $t \\to \\infty$ 时 $f_t$ 的极限。我们之前已经证明 $\\lim_{t\\to\\infty} a_t = \\infty$。\n$$ \\lim_{t\\to\\infty} f_t = \\lim_{t\\to\\infty} \\left(\\frac{\\lambda}{a_t} - \\frac{\\lambda^2}{2a_t^2}\\right) $$\n当 $a_t \\to \\infty$ 时，表达式中的两项都趋于 $0$：\n$$ \\lim_{a_t\\to\\infty} \\frac{\\lambda}{a_t} = 0 \\quad \\text{和} \\quad \\lim_{a_t\\to\\infty} \\frac{\\lambda^2}{2a_t^2} = 0 $$\n因此，目标函数的极限是：\n$$ \\lim_{t\\to\\infty} f_t = 0 - 0 = 0 $$\n这一分析突显了无约束字典学习的一个关键病态问题：字典原子的范数可以无限增大，而稀疏编码的幅度则收缩到零，这使得重建误差和目标函数能够趋近于零，却没有提供有意义的稀疏表示。这就是在诸如MOD和K-SVD等实际算法中对字典原子施加约束（例如单位范数约束）的主要动机。",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "字典学习通常采用交替优化的策略，其中稀疏编码是其核心环节，即在给定字典的情况下求解稀疏系数。本练习要求你从第一性原理出发，推导出求解此 $L_1$ 正则化最小二乘问题的两个关键算法——迭代软阈值算法（ISTA）及其快速版本（FISTA）的更新法则 。掌握这些推导过程是理解现代稀疏优化的基础，并为你实现高效的字典学习算法奠定坚实的理论基础。",
            "id": "3444184",
            "problem": "考虑在字典学习方法（如最优方向法 (MOD) 和 K-奇异值分解 (K-SVD)）以及字典学习的在线形式中出现的稀疏编码子问题。令 $D \\in \\mathbb{R}^{m \\times p}$ 为一个固定字典，$y \\in \\mathbb{R}^{m}$ 为一个给定信号，且 $\\lambda  0$。稀疏码 $x \\in \\mathbb{R}^{p}$ 通过求解以下复合凸优化问题得到：\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2}\\|y - D x\\|_{2}^{2} + \\lambda \\|x\\|_{1}.\n$$\n从正常、闭、凸函数的近端算子的定义以及光滑函数梯度的Lipschitz连续性的刻画出发，从第一性原理推导：\n1. 函数 $x \\mapsto \\lambda \\|x\\|_{1}$ 的近端算子的闭式解。\n2. 针对上述问题，使用根据光滑项梯度的Lipschitz常数选择的恒定步长，推导迭代收缩阈值算法（ISTA）的更新规则。\n3. 针对同一问题，使用相同的步长选择，推导快速迭代收缩阈值算法（FISTA）的加速更新规则，包括动量参数更新和外推迭代点。\n\n您的推导必须从近端算子的基本定义和Lipschitz梯度条件开始。明确指出光滑项梯度的Lipschitz常数，并用它来确定一个有效的固定步长。以闭式解形式表示近端算子以及ISTA和FISTA的更新规则，不要省略中间的逻辑步骤。将您的最终表达式紧凑地表示为一个单行矩阵，其条目依次为近端算子、ISTA更新规则和FISTA更新规则。无需进行数值计算。",
            "solution": "所给出的问题是一个标准的复合凸优化问题，称为Lasso或基追踪降噪（Basis Pursuit Denoising）。目标函数是一个光滑、凸、可微的损失函数与一个凸、非光滑的正则化项之和。我们需要从第一性原理出发，推导正则化项的近端算子，以及迭代收缩阈值算法（ISTA）及其加速版本FISTA的更新规则。\n\n该优化问题为：\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\;\\; F(x) \\equiv f(x) + g(x)\n$$\n其中光滑项是 $f(x) = \\frac{1}{2}\\|y - D x\\|_{2}^{2}$，非光滑项是 $g(x) = \\lambda \\|x\\|_{1}$。\n\n### 1. $g(x) = \\lambda \\|x\\|_{1}$ 的近端算子推导\n\n一个正常、闭、凸函数 $h:\\mathbb{R}^p \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 在参数 $\\tau  0$ 下的近端算子定义为：\n$$\n\\text{prox}_{\\tau h}(z) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ h(x) + \\frac{1}{2\\tau} \\|x - z\\|_2^2 \\right\\}\n$$\n对于函数 $g(x) = \\lambda \\|x\\|_{1}$，其近端算子由下式给出：\n$$\n\\text{prox}_{\\tau\\lambda\\|\\cdot\\|_1}(z) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ \\lambda \\|x\\|_{1} + \\frac{1}{2\\tau} \\|x - z\\|_2^2 \\right\\}\n$$\n目标函数相对于 $x$ 的分量是可分的，因为 $\\|x\\|_1 = \\sum_{i=1}^p |x_i|$ 且 $\\|x-z\\|_2^2 = \\sum_{i=1}^p (x_i-z_i)^2$。因此，我们可以通过最小化以下式子来独立求解每个分量 $x_i$：\n$$\n\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\lambda |x_i| + \\frac{1}{2\\tau} (x_i - z_i)^2 \\right\\}\n$$\n这是一个一维凸优化问题。一个点 $x_i^*$ 是最小值点，当且仅当 $0$ 位于目标函数在 $x_i^*$ 处的次微分中。目标函数关于 $x_i$ 的次微分是：\n$$\n\\partial \\left( \\lambda |x_i| + \\frac{1}{2\\tau} (x_i - z_i)^2 \\right) = \\lambda \\cdot \\partial|x_i| + \\frac{1}{\\tau}(x_i - z_i)\n$$\n其中 $\\partial|x_i|$ 是绝对值函数的次微分：\n$$\n\\partial|x_i| = \\text{sgn}(x_i) = \\begin{cases} \\{1\\}  \\text{if } x_i  0 \\\\ \\{-1\\}  \\text{if } x_i  0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\n最优性条件 $0 \\in \\lambda \\cdot \\text{sgn}(x_i^*) + \\frac{1}{\\tau}(x_i^* - z_i)$ 可以重写为 $z_i - x_i^* \\in \\tau\\lambda \\cdot \\text{sgn}(x_i^*)$。我们分三种情况对 $x_i^*$ 分析此条件：\n\n-   **情况1：$x_i^*  0$。** 则 $\\text{sgn}(x_i^*) = \\{1\\}$，所以 $z_i - x_i^* = \\tau\\lambda$，这意味着 $x_i^* = z_i - \\tau\\lambda$。这仅在 $z_i  \\tau\\lambda$ 时与 $x_i^*0$ 一致。\n-   **情况2：$x_i^*  0$。** 则 $\\text{sgn}(x_i^*) = \\{-1\\}$，所以 $z_i - x_i^* = -\\tau\\lambda$，这意味着 $x_i^* = z_i + \\tau\\lambda$。这仅在 $z_i  -\\tau\\lambda$ 时与 $x_i^*0$ 一致。\n-   **情况3：$x_i^* = 0$。** 则 $\\text{sgn}(x_i^*) = [-1, 1]$，所以 $z_i - 0 \\in [-\\tau\\lambda, \\tau\\lambda]$，这意味着 $|z_i| \\le \\tau\\lambda$。\n\n综合这些情况，每个分量 $x_i^*$ 的解为：\n$$\nx_i^* = \\begin{cases} z_i - \\tau\\lambda  \\text{if } z_i  \\tau\\lambda \\\\ 0  \\text{if } |z_i| \\le \\tau\\lambda \\\\ z_i + \\tau\\lambda  \\text{if } z_i  -\\tau\\lambda \\end{cases}\n$$\n这个操作被称为软阈值算子，我们记为 $S_{\\alpha}(\\cdot)$。对于一个向量 $v$ 和一个标量 $\\alpha0$，它被逐元素地定义为 $(S_{\\alpha}(v))_i = \\text{sgn}(v_i) \\max(|v_i|-\\alpha, 0)$。\n因此，$g(x) = \\lambda \\|x\\|_1$ 的近端算子是阈值为 $\\tau\\lambda$ 的软阈值算子：\n$$\n\\text{prox}_{\\tau\\lambda\\|\\cdot\\|_1}(z) = S_{\\tau\\lambda}(z)\n$$\n\n### 2. ISTA 更新规则的推导\n\nISTA 是近端梯度法的一个实例，专为形如 $\\min_x f(x) + g(x)$ 的问题设计。其迭代更新规则由下式给出：\n$$\nx^{k+1} = \\text{prox}_{\\tau_k g}(x^k - \\tau_k \\nabla f(x^k))\n$$\n其中 $\\tau_k  0$ 是第 $k$ 次迭代的步长。为保证收敛，对于恒定步长方案，步长通常根据 $\\nabla f$ 的Lipschitz常数来选择。如果对于所有的 $x_1, x_2$ 都有 $\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 \\le L \\|x_1 - x_2\\|_2$，则称梯度 $\\nabla f$ 是 L-Lipschitz 连续的。\n\n首先，我们计算 $f(x) = \\frac{1}{2}\\|y - Dx\\|_2^2$ 的梯度：\n$$\nf(x) = \\frac{1}{2}(y - Dx)^T(y - Dx) = \\frac{1}{2}(y^T y - 2y^T Dx + x^T D^T D x)\n$$\n对 $x$ 求梯度得到：\n$$\n\\nabla f(x) = D^T(Dx - y) = -D^T(y-Dx)\n$$\n接下来，我们求 $\\nabla f(x)$ 的Lipschitz常数 $L$。考虑两个点 $x_1, x_2 \\in \\mathbb{R}^p$：\n$$\n\\nabla f(x_1) - \\nabla f(x_2) = (D^T(Dx_1-y)) - (D^T(Dx_2-y)) = D^T D(x_1 - x_2)\n$$\n取欧几里得范数：\n$$\n\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 = \\|D^T D(x_1 - x_2)\\|_2 \\le \\|D^T D\\|_2 \\|x_1 - x_2\\|_2\n$$\n其中 $\\|D^T D\\|_2$ 是矩阵 $D^T D$ 的谱范数，即其最大奇异值，或者等价地，由于 $D^T D$ 是半正定的，为其最大特征值 $\\lambda_{\\max}(D^T D)$。因此，$\\nabla f$ 的最小可能Lipschitz常数是 $L = \\|D^T D\\|_2 = \\sigma_{\\max}^2(D)$。\n\n对于恒定步长，如果 $\\tau \\in (0, 2/L)$，则收敛性得到保证。一个标准的选择是 $\\tau = 1/L$。使用这个步长，ISTA的更新规则变为：\n$$\nx^{k+1} = \\text{prox}_{(1/L)g}(x^k - (1/L)\\nabla f(x^k))\n$$\n代入我们得到的 $g$、$\\nabla f$ 和 $L$ 的表达式：\n$$\nx^{k+1} = S_{(\\lambda/L)}(x^k - (1/L)D^T(Dx^k-y))\n$$\n其中 $L = \\|D^T D\\|_2$。\n\n### 3. FISTA 更新规则的推导\n\nFISTA 是 ISTA 的一个加速版本。它通过引入一个动量项来获得更快的收敛速度。该算法维护一个主迭代序列（我们记为 $\\{x_k\\}$）和一个辅助外推序列 $\\{y_k\\}$。其更新规则源自Nesterov的加速梯度法。令 $t_k$ 为一个动量参数序列。该算法从一个初始猜测值 $x_0$ 开始，通常初始化为 $y_1 = x_0$ 和 $t_1 = 1$。\n\n对于每次迭代 $k = 1, 2, \\ldots$：\n1.  通过在外推点 $y_k$ 处应用近端梯度步骤来计算下一个迭代点 $x_k$，使用相同的步长 $\\tau = 1/L$：\n    $$\n    x_k = \\text{prox}_{(1/L)g}(y_k - (1/L)\\nabla f(y_k)) = S_{\\lambda/L}(y_k - (1/L)D^T(Dy_k-y))\n    $$\n2.  更新动量参数：\n    $$\n    t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}\n    $$\n3.  下一个外推点 $y_{k+1}$ 由前两个迭代点 $x_{k-1}$ 和 $x_k$ 的线性组合构成：\n    $$\n    y_{k+1} = x_k + \\left(\\frac{t_k - 1}{t_{k+1}}\\right)(x_k - x_{k-1})\n    $$\n这三个方程组构成了针对该问题的FISTA更新规则。\n\n最终表达式汇总在下面的答案中。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_{\\tau \\lambda}(z)  x^{k+1} = S_{\\lambda/L}\\left(x^k - \\frac{1}{L} D^T(Dx^k - y)\\right)  \\begin{aligned} x_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L} D^T(Dy_k - y)\\right) \\\\ t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2} \\\\ y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1}) \\end{aligned}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "解决了稀疏编码子问题后，我们转向交替优化中的另一步：字典更新。正如第一个练习所揭示的，对字典的约束是必不可少的，本练习将展示如何具体实施这些约束。你将推导出一个投影梯度更新步骤，它能确保字典原子在迭代过程中始终保持单位范数，同时学习如何引入一个非相干性惩罚项来提升字典质量 。",
            "id": "3444142",
            "problem": "在压缩感知和稀疏优化的字典学习中，最优方向法（Method of Optimal Directions, MOD）和 K-奇异值分解（K-Singular Value Decomposition, K-SVD）通常会引入非相干性促进项，以抑制高度相关的原子。考虑一个字典矩阵 $D \\in \\mathbb{R}^{m \\times k}$，其列向量被设计为具有单位欧几里得范数，并考虑非相干性惩罚项\n$$\\phi(D) = \\frac{\\eta}{2}\\left\\|D^{\\top} D - I\\right\\|_{F}^{2},$$\n其中 $\\eta  0$ 是一个正则化参数，$I \\in \\mathbb{R}^{k \\times k}$ 是单位矩阵，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数。仅从以下基本事实出发：\n- 对任意矩阵 $A$，弗罗贝尼乌斯范数恒等式 $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$，\n- 对可乘矩阵，迹的循环性质 $\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$，\n- 格拉姆算子 $G(D) = D^{\\top}D$ 的微分由 $\\mathrm{d}(D^{\\top}D) = \\mathrm{d}D^{\\top}\\,D + D^{\\top}\\,\\mathrm{d}D$ 给出，\n- 以及通过一阶变分 $\\mathrm{d}\\phi(D) = \\operatorname{tr}\\!\\big((\\nabla \\phi(D))^{\\top}\\mathrm{d}D\\big)$ 定义的梯度，\n推导梯度 $\\nabla \\phi(D)$，然后构建一个步长为 $\\tau  0$ 的投影梯度更新，通过投影到集合 $\\mathcal{C} = \\{D \\in \\mathbb{R}^{m \\times k} : \\|D_{:,j}\\|_{2} = 1 \\text{ for all } j \\in \\{1,\\dots,k\\}\\}$ 上，以保持 $D$ 的每一列的单位范数约束。将该投影表示为一个对角缩放矩阵的右乘形式，该对角矩阵的对角线元素是投影前迭代值的列范数的倒数。\n\n你的最终答案必须是关于 $D$、$\\eta$、$\\tau$ 和 $I$ 的单个闭式解析表达式，表示这样一次投影梯度更新。无需进行数值计算。",
            "solution": "问题陈述是适定的、有科学依据且自洽的。它提供了推导所要求的梯度更新规则所需的所有必要定义和基本事实。其背景在稀疏优化和字典学习领域是标准的。因此，该问题是有效的，我们可以着手求解。\n\n目标是首先求出非相干性惩罚函数 $\\phi(D)$ 的梯度，然后用它来构建一个投影梯度下降更新步骤。\n\n设惩罚函数为\n$$ \\phi(D) = \\frac{\\eta}{2}\\left\\|D^{\\top} D - I\\right\\|_{F}^{2} $$\n其中 $D \\in \\mathbb{R}^{m \\times k}$，$I \\in \\mathbb{R}^{k \\times k}$ 是单位矩阵，且 $\\eta  0$。\n\n首先，我们推导梯度 $\\nabla \\phi(D)$。我们从使用给定的迹恒等式 $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$ 来表示弗罗贝尼乌斯范数的平方开始。\n令 $A = D^{\\top}D - I$。那么 $A^{\\top} = (D^{\\top}D - I)^{\\top} = (D^{\\top}D)^{\\top} - I^{\\top} = D^{\\top}(D^{\\top})^{\\top} - I = D^{\\top}D - I$，因为单位矩阵 $I$ 是对称的。因此，$A$ 是一个对称矩阵。\n惩罚函数可以写成：\n$$ \\phi(D) = \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)^{\\top}(D^{\\top}D - I)\\right) = \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)(D^{\\top}D - I)\\right) $$\n为了求梯度，我们首先计算一阶变分，即微分 $\\mathrm{d}\\phi(D)$。利用微分和迹算子的线性性质，以及微分的乘法法则，我们有：\n$$ \\mathrm{d}\\phi(D) = \\mathrm{d}\\left( \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)(D^{\\top}D - I)\\right) \\right) $$\n$$ \\mathrm{d}\\phi(D) = \\frac{\\eta}{2} \\operatorname{tr}\\left( \\mathrm{d}(D^{\\top}D - I)(D^{\\top}D - I) + (D^{\\top}D - I)\\mathrm{d}(D^{\\top}D - I) \\right) $$\n由于迹内的两项互为转置，且矩阵的迹等于其转置的迹，因此它们是相等的。一个更简单的方法是注意到它们是相同的乘积，所以我们可以将它们合并：\n$$ \\mathrm{d}\\phi(D) = \\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}(D^{\\top}D - I) \\right) $$\n常数矩阵 ($I$) 的微分为零，所以 $\\mathrm{d}(D^{\\top}D - I) = \\mathrm{d}(D^{\\top}D)$。题目给出了格拉姆矩阵 $G(D) = D^{\\top}D$ 的微分为 $\\mathrm{d}(D^{\\top}D) = \\mathrm{d}D^{\\top}D + D^{\\top}\\mathrm{d}D$。将此代入 $\\mathrm{d}\\phi(D)$ 的表达式中：\n$$ \\mathrm{d}\\phi(D) = \\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) (\\mathrm{d}D^{\\top}D + D^{\\top}\\mathrm{d}D) \\right) $$\n$$ \\mathrm{d}\\phi(D) = \\eta \\left( \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right) + \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrmdD \\right) \\right) $$\n我们需要将此表达式整理成 $\\mathrm{d}\\phi(D) = \\operatorname{tr}((\\nabla \\phi(D))^{\\top}\\mathrm{d}D)$ 的形式。让我们分析第一项 $\\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right)$。利用性质 $\\operatorname{tr}(X) = \\operatorname{tr}(X^{\\top})$：\n$$ \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right) = \\operatorname{tr}\\left( \\left((D^{\\top}D - I) \\mathrm{d}D^{\\top}D\\right)^{\\top} \\right) = \\operatorname{tr}\\left( D^{\\top}(\\mathrm{d}D^{\\top})^{\\top}(D^{\\top}D - I)^{\\top} \\right) $$\n由于 $(D^{\\top}D - I)$ 是对称的且 $(\\mathrm{d}D^{\\top})^{\\top} = \\mathrmdD$，上式变为：\n$$ \\operatorname{tr}\\left( D^{\\top}\\mathrmdD (D^{\\top}D - I) \\right) $$\n现在，使用迹的循环性质 $\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$，令 $A=D^{\\top}$，$B=\\mathrmdD$，$C=(D^{\\top}D - I)$：\n$$ \\operatorname{tr}\\left( D^{\\top}\\mathrmdD (D^{\\top}D - I) \\right) = \\operatorname{tr}\\left( \\mathrmdD (D^{\\top}D - I)D^{\\top} \\right) $$\n我们来考察 $\\mathrm{d}\\phi(D)$ 表达式中的第二项，即 $\\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrmdD \\right)$。使用循环性质，令 $A=(D^{\\top}D - I)D^{\\top}$ 和 $B=\\mathrmdD$，我们有 $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$：\n$$ \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrmdD \\right) = \\operatorname{tr}\\left( \\mathrmdD (D^{\\top}D - I) D^{\\top} \\right) $$\n我们观察到这两项是相同的。因此，我们可以将它们合并：\n$$ \\mathrm{d}\\phi(D) = \\eta \\left( \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrmdD \\right) + \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrmdD \\right) \\right) $$\n$$ \\mathrm{d}\\phi(D) = 2\\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrmdD \\right) $$\n为了将其与定义 $\\mathrm{d}\\phi(D) = \\operatorname{tr}((\\nabla \\phi(D))^{\\top}\\mathrm{d}D)$ 相匹配，我们将迹内部从左侧乘以 $\\mathrmdD$ 的矩阵等同于 $(\\nabla \\phi(D))^{\\top}$。然而，实矩阵的标准内积是 $\\langle X, Y \\rangle = \\operatorname{tr}(X^{\\top}Y)$。梯度的定义基于此。我们有一个形如 $\\operatorname{tr}(A B)$ 的表达式，而我们希望它是 $\\operatorname{tr}(G^{\\top}B)$ 的形式。我们必须确定 $G^{\\top} = A$。\n所以，我们必须有 $(\\nabla \\phi(D))^{\\top} = 2\\eta (D^{\\top}D - I) D^{\\top}$。\n梯度 $\\nabla \\phi(D)$ 是这个表达式的转置：\n$$ \\nabla \\phi(D) = \\left( 2\\eta (D^{\\top}D - I) D^{\\top} \\right)^{\\top} = 2\\eta (D^{\\top})^{\\top} (D^{\\top}D - I)^{\\top} $$\n$$ \\nabla \\phi(D) = 2\\eta D (D^{\\top}D - I) $$\n这个梯度的表达式维度为 $(m \\times k) \\times (k \\times k) = m \\times k$，与 $D$ 的维度相匹配，符合预期。\n\n接下来，我们构建投影梯度更新。对于步长为 $\\tau  0$ 的 $D$，标准的梯度下降更新为：\n$$ D_{\\text{temp}} = D - \\tau \\nabla \\phi(D) $$\n代入我们推导出的梯度：\n$$ D_{\\text{temp}} = D - 2\\eta\\tau D(D^{\\top}D - I) $$\n我们可以提出因子 $D$ 以更紧凑地书写：\n$$ D_{\\text{temp}} = D(I - 2\\eta\\tau(D^{\\top}D - I)) $$\n这个临时迭代值 $D_{\\text{temp}}$ 通常不具有单位范数的列。我们必须将其投影到约束集 $\\mathcal{C} = \\{X \\in \\mathbb{R}^{m \\times k} : \\|X_{:,j}\\|_{2} = 1 \\text{ for all } j\\}$ 上。投影操作涉及对 $D_{\\text{temp}}$ 的每一列进行归一化。设 $D_{\\text{next}}$ 为更新后的矩阵。$D_{\\text{next}}$ 的第 $j$ 列是：\n$$ (D_{\\text{next}})_{:,j} = \\frac{(D_{\\text{temp}})_{:,j}}{\\|(D_{\\text{temp}})_{:,j}\\|_{2}} $$\n这个操作可以表示为与一个对角矩阵 $S \\in \\mathbb{R}^{k \\times k}$ 的右乘，其中对角线元素是 $D_{\\text{temp}}$ 列范数的倒数：\n$$ D_{\\text{next}} = D_{\\text{temp}}S, \\quad \\text{其中} \\quad S = \\operatorname{diag}\\left(\\frac{1}{\\|(D_{\\text{temp}})_{:,1}\\|_{2}}, \\dots, \\frac{1}{\\|(D_{\\text{temp}})_{:,k}\\|_{2}}\\right) $$\n$D_{\\text{temp}}$ 第 $j$ 列的范数平方是矩阵 $D_{\\text{temp}}^{\\top}D_{\\text{temp}}$ 的第 $j$ 个对角元素。令 $N = \\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}})$。这是一个对角矩阵，其元素为 $N_{jj} = \\|(D_{\\text{temp}})_{:,j}\\|_{2}^{2}$。\n因此，对角缩放矩阵 $S$ 可以写为 $S = N^{-1/2} = (\\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}}))^{-1/2}$。\n\n综合这些部分，完整的投影梯度更新为：\n$$ D_{\\text{next}} = D_{\\text{temp}} (\\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}}))^{-1/2} $$\n代入 $D_{\\text{temp}} = D(I - 2\\eta\\tau(D^{\\top}D - I))$ 的表达式，我们得到更新规则的最终闭式表达式。\n令 $U = D(I - 2\\eta\\tau(D^{\\top}D - I))$。更新由下式给出：\n$$ D_{\\text{next}} = U \\left( \\operatorname{diag}(U^{\\top} U) \\right)^{-1/2} $$\n这是一个用 $D$、$\\eta$、$\\tau$ 和 $I$ 表示的投影梯度更新的单个解析表达式。",
            "answer": "$$ \\boxed{\\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right) \\left( \\operatorname{diag}\\left( \\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right)^{\\top} \\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right) \\right) \\right)^{-1/2}} $$"
        }
    ]
}