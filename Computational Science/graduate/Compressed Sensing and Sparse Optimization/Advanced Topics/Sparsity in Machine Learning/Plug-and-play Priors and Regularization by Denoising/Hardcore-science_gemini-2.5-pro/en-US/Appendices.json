{
    "hands_on_practices": [
        {
            "introduction": "To build intuition for modern Plug-and-Play (PnP) and Regularization by Denoising (RED) frameworks, it is invaluable to connect them to classical methods. This exercise demystifies these approaches by demonstrating that for a simple quadratic prior, they are equivalent to the well-known Tikhonov regularization. By deriving the effective regularization matrix from first principles, you will gain a concrete understanding of how algorithmic parameters like $\\rho$, $\\tau$, and $\\beta$ directly control the strength and form of regularization .",
            "id": "3466500",
            "problem": "Consider the linear inverse problem with measurements modeled as $y \\in \\mathbb{R}^{m}$ and a forward operator $A \\in \\mathbb{R}^{m \\times n}$ under additive white Gaussian noise of variance $\\sigma^{2}$. The data-fidelity term is $f(x) = \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2}$ with $x \\in \\mathbb{R}^{n}$. Let $L \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive semidefinite matrix and define the quadratic potential $\\phi(x) = \\frac{1}{2} x^{\\top} L x$. Let $\\tau > 0$, $\\rho > 0$, and $\\beta > 0$ be given scalars.\n\n- In the Plug-and-Play (PnP) method implemented via the Alternating Direction Method of Multipliers (ADMM), the $z$-update uses a denoiser that is the proximal operator of $\\tau \\phi$, that is, for any $v \\in \\mathbb{R}^{n}$,\n$$\nD_{\\tau}(v) = \\operatorname{prox}_{\\tau \\phi}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - v\\|_{2}^{2} + \\tau \\phi(z) \\right\\}.\n$$\nAssume the ADMM penalty parameter is $\\rho$ and consider the standard variable splitting $x=z$.\n\n- In Regularization by Denoising (RED), consider the variational formulation\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + \\beta R(x),\n$$\nwhere the RED regularizer associated with the same denoiser $D_{\\tau}$ is\n$$\nR(x) = \\frac{1}{2} x^{\\top} \\left( x - D_{\\tau}(x) \\right),\n$$\nand assume the denoiser is linear, symmetric, and nonexpansive.\n\nStarting from first principles and core definitions only (proximal operator for convex functions, the ADMM splitting for $x=z$ with penalty parameter $\\rho$, and the RED construction), derive the explicit closed-form expression for the effective Tikhonov regularization matrix in each case. Concretely, show that both PnP-ADMM with the proximal denoiser of $\\tau \\phi$ and RED with the same denoiser reduce to classical Tikhonov regularization of the form\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma x,\n$$\nand compute each method’s effective regularization matrix $\\Gamma$ explicitly in terms of $\\rho$, $\\tau$, $\\beta$, and $L$. \n\nYour final answer must be the pair consisting of the two effective matrices in a single row matrix, in the order $\\left(\\Gamma_{\\mathrm{PnP}}, \\Gamma_{\\mathrm{RED}}\\right)$, as a closed-form analytic expression. No numerical approximation is required, and no units are involved.",
            "solution": "The supplied problem is valid. It is scientifically grounded in the principles of computational imaging and optimization theory, well-posed, objective, and contains all necessary information for a unique solution. We proceed with the derivation.\n\nThe objective is to demonstrate that both Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM) and Regularization by Denoising (RED), under the specified conditions, are equivalent to a Tikhonov regularization problem of the form\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma x\n$$\nand to determine the explicit expressions for the effective regularization matrix $\\Gamma$ in each case.\n\nFirst, we derive the explicit form of the denoiser $D_{\\tau}(v)$. The denoiser is defined as the proximal operator of the function $\\tau \\phi(z)$, where $\\phi(z) = \\frac{1}{2} z^{\\top} L z$. By definition:\n$$\nD_{\\tau}(v) = \\operatorname{prox}_{\\tau \\phi}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - v\\|_{2}^{2} + \\tau \\phi(z) \\right\\} = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|z - v\\|_{2}^{2} + \\frac{\\tau}{2} z^{\\top} L z \\right\\}.\n$$\nThe objective function is quadratic and strictly convex, since $L$ is positive semidefinite and $\\tau > 0$, ensuring the Hessian $I + \\tau L$ is positive definite. The unique minimizer is found by setting the gradient with respect to $z$ to zero:\n$$\n\\nabla_{z} \\left( \\frac{1}{2} (z - v)^{\\top}(z - v) + \\frac{\\tau}{2} z^{\\top} L z \\right) = (z - v) + \\tau L z = 0.\n$$\nRearranging the terms, we get $(I + \\tau L)z = v$. Since $I + \\tau L$ is invertible, we can solve for $z$:\n$$\nz = (I + \\tau L)^{-1} v.\n$$\nThus, the denoiser is a linear operator represented by the matrix $(I + \\tau L)^{-1}$:\n$$\nD_{\\tau}(v) = (I + \\tau L)^{-1} v.\n$$\n\nWe will now analyze each method separately.\n\n**Part 1: Plug-and-Play ADMM (PnP-ADMM)**\n\nThe PnP-ADMM algorithm is applied to solve an inverse problem of the form $\\min_x f(x) + g(x)$ by using variable splitting $x=z$. The standard ADMM algorithm involves iterations on $x$, $z$, and a scaled dual variable $u$. The problem states that the $z$-update, which would typically be $\\operatorname{prox}_{g/\\rho}$, is replaced by a denoiser $D_{\\tau}$. The fixed point $(x^*, z^*, u^*)$ of this algorithm, to which the iterates are presumed to converge, must satisfy the following conditions:\n1. $x$-update: $x^* = \\arg\\min_x \\left\\{ f(x) + \\frac{\\rho}{2} \\|x - (z^* - u^*)\\|_2^2 \\right\\}$. The first-order optimality condition for this step is $\\nabla f(x^*) + \\rho(x^* - z^* + u^*) = 0$.\n2. $z$-update (PnP step): $z^* = D_{\\tau}(x^* + u^*)$.\n3. Dual update: $u^* = u^* + x^* - z^*$, which implies the fixed-point constraint $x^* = z^*$.\n\nWe use these fixed-point relations to find the implicit optimization problem that the algorithm solves.\nSubstituting $x^* = z^*$ into the first-order condition from the $x$-update:\n$$\n\\nabla f(x^*) + \\rho(x^* - x^* + u^*) = 0 \\implies \\nabla f(x^*) + \\rho u^* = 0.\n$$\nThis gives an expression for the dual variable at the fixed point:\n$$\nu^* = -\\frac{1}{\\rho} \\nabla f(x^*).\n$$\nNow, substitute $z^* = x^*$ and the expression for $u^*$ into the $z$-update equation:\n$$\nx^* = D_{\\tau}\\left(x^* - \\frac{1}{\\rho} \\nabla f(x^*)\\right).\n$$\nThis is the fixed-point equation that characterizes the solution $x^*$. We now substitute the derived expression for the denoiser $D_{\\tau}(v) = (I + \\tau L)^{-1} v$:\n$$\nx^* = (I + \\tau L)^{-1} \\left(x^* - \\frac{1}{\\rho} \\nabla f(x^*)\\right).\n$$\nMultiplying both sides by the invertible matrix $(I + \\tau L)$:\n$$\n(I + \\tau L)x^* = x^* - \\frac{1}{\\rho} \\nabla f(x^*).\n$$\nExpanding the left side yields $x^* + \\tau L x^* = x^* - \\frac{1}{\\rho} \\nabla f(x^*)$.\nSimplifying, we have $\\tau L x^* = -\\frac{1}{\\rho} \\nabla f(x^*)$, which can be rewritten as:\n$$\n\\nabla f(x^*) + \\rho \\tau L x^* = 0.\n$$\nThis equation is the first-order necessary and sufficient optimality condition for a convex minimization problem. We can identify this problem by finding a function whose gradient corresponds to the left-hand side. Let this function be $J_{\\mathrm{PnP}}(x)$:\n$$\n\\nabla J_{\\mathrm{PnP}}(x) = \\nabla f(x) + \\rho \\tau L x.\n$$\nIntegrating with respect to $x$, we find the objective function (up to a constant):\n$$\nJ_{\\mathrm{PnP}}(x) = f(x) + \\frac{1}{2} x^{\\top} (\\rho \\tau L) x = \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{\\rho \\tau}{2} x^{\\top} L x.\n$$\nThis is a Tikhonov regularization problem. By comparing it with the target form $\\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma_{\\mathrm{PnP}} x$, we can directly identify the effective regularization matrix for PnP-ADMM:\n$$\n\\Gamma_{\\mathrm{PnP}} = \\rho \\tau L.\n$$\n\n**Part 2: Regularization by Denoising (RED)**\n\nThe RED framework posits a variational problem explicitly. The problem is given as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + \\beta R(x),\n$$\nwhere $f(x) = \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2}$ and the RED regularizer is $R(x) = \\frac{1}{2} x^{\\top} (x - D_{\\tau}(x))$.\nTo find the effective Tikhonov matrix, we substitute the expressions for $f(x)$ and $R(x)$ into the minimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{\\beta}{2} x^{\\top} (x - D_{\\tau}(x)).\n$$\nWe use the same denoiser expression as before: $D_{\\tau}(x) = (I + \\tau L)^{-1} x$. Substituting this into the regularization term gives:\n$$\n\\frac{\\beta}{2} x^{\\top} (x - (I + \\tau L)^{-1} x) = \\frac{\\beta}{2} x^{\\top} (I - (I + \\tau L)^{-1}) x.\n$$\nWe simplify the matrix expression $I - (I + \\tau L)^{-1}$:\n$$\nI - (I + \\tau L)^{-1} = (I + \\tau L)(I + \\tau L)^{-1} - (I + \\tau L)^{-1} = ((I + \\tau L) - I)(I + \\tau L)^{-1} = \\tau L (I + \\tau L)^{-1}.\n$$\nSubstituting this simplified matrix back into the regularization term, we get:\n$$\n\\frac{\\beta}{2} x^{\\top} (\\tau L (I + \\tau L)^{-1}) x = \\frac{\\beta \\tau}{2} x^{\\top} L (I + \\tau L)^{-1} x.\n$$\nThus, the full RED optimization problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\left( \\beta \\tau L (I + \\tau L)^{-1} \\right) x.\n$$\nThis is also in the form of Tikhonov regularization. By comparing it with the target form $\\frac{1}{2 \\sigma^{2}} \\|A x - y\\|_{2}^{2} + \\frac{1}{2} x^{\\top} \\Gamma_{\\mathrm{RED}} x$, we identify the effective regularization matrix for RED:\n$$\n\\Gamma_{\\mathrm{RED}} = \\beta \\tau L (I + \\tau L)^{-1}.\n$$\nSince $L$ is a symmetric matrix, both derived regularization matrices $\\Gamma_{\\mathrm{PnP}}$ and $\\Gamma_{\\mathrm{RED}}$ are symmetric, consistent with the quadratic form of Tikhonov regularization.\n\n**Conclusion**\n\nWe have shown that both PnP-ADMM and RED, with the specified quadratic potential, reduce to Tikhonov regularization. The effective regularization matrices are $\\Gamma_{\\mathrm{PnP}} = \\rho \\tau L$ and $\\Gamma_{\\mathrm{RED}} = \\beta \\tau L (I + \\tau L)^{-1}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\rho \\tau L & \\beta \\tau L(I + \\tau L)^{-1}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The practical success of PnP algorithms hinges on their convergence, which is guaranteed only when the chosen denoiser satisfies certain mathematical properties. This practice explores what happens when these conditions are violated, specifically when the denoiser is expansive—meaning it can amplify its input. Through a combination of theoretical analysis and a coding implementation, you will construct a concrete counterexample showing how PnP-ADMM can diverge, providing critical insight into why properties like non-expansiveness are a cornerstone of convergence theory .",
            "id": "3466533",
            "problem": "Consider Alternating Direction Method of Multipliers (ADMM), which solves problems of the form $\\min_{\\mathbf{x}} \\, g(\\mathbf{x}) + h(\\mathbf{x})$ by splitting variables and introducing the constraint $\\mathbf{x} = \\mathbf{v}$. The scaled Alternating Direction Method of Multipliers (ADMM) iterates over three variables: the primal variable $\\mathbf{x}$, the auxiliary variable $\\mathbf{v}$, and the scaled dual variable $\\mathbf{u}$. Plug-and-Play (PnP) methods, where PnP stands for Plug-and-Play, replace the proximal operator of $h$ with a denoiser $D$, producing PnP-ADMM. In one dimension, assume the data-fidelity term is quadratic, the forward operator is identity, and the denoiser is linear.\n\nStarting from the following fundamental base:\n\n- The variable-splitting form of ADMM for $\\min_{\\mathbf{x}} \\, g(\\mathbf{x}) + h(\\mathbf{v})$ subject to $\\mathbf{x} = \\mathbf{v}$ uses the augmented Lagrangian with penalty parameter $\\rho > 0$ and the scaled dual variable $\\mathbf{u}$.\n- In PnP-ADMM, the update corresponding to the proximal operator of $h$ is replaced by a denoising operation $D$ evaluated at the appropriate argument.\n- For a one-dimensional quadratic data fidelity $g(x) = \\frac{\\gamma}{2} (x - y)^2$ with $\\gamma > 0$, identity forward operator, and linear denoiser $D(z) = \\alpha z$ with gain $\\alpha > 0$, each iteration is well-defined and yields a deterministic linear-affine recurrence.\n\nYour task is to construct a counterexample showing that an expansive denoiser (i.e., a denoiser with gain $\\alpha > 1$) can cause divergence in PnP-ADMM and, moreover, may produce oscillatory behavior. Proceed as follows, strictly deriving from the base above:\n\n1. Derive, from first principles, the one-dimensional PnP-ADMM iteration for the triple $(x^{k+1}, v^{k+1}, u^{k+1})$ using the quadratic $g(x)$, linear denoiser $D(z)$, and penalty parameter $\\rho$. Express the iteration in closed form in terms of $x^k$, $v^k$, $u^k$, $\\gamma$, $\\rho$, $y$, and $\\alpha$.\n2. Show that the iteration can be written as a linear-affine dynamical system on an appropriate state, and derive the matrix that governs the linear part. Compute the spectral radius of this matrix and provide the precise divergence condition in terms of $\\alpha$, $\\gamma$, and $\\rho$ using only this derivation.\n3. Explain, in terms of the sign of the dominant eigenvalue, when oscillatory behavior (alternating signs across iterations) occurs.\n4. Implement a complete, runnable program that:\n   - Simulates the PnP-ADMM iteration in one dimension using the derived closed-form updates.\n   - For each test case, computes the spectral radius of the linear part, determines if the sequence diverges (based on the derived necessary and sufficient condition), and detects oscillatory behavior by counting sign alternations in the scaled dual variable over the simulated iterations.\n   - Returns, for each test case, a list of three items: the spectral radius as a float rounded to six decimal places, a boolean indicating divergence, and a boolean indicating oscillatory behavior.\n\nUse the following test suite with one-dimensional parameters $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K)$, where $K$ is the number of iterations:\n\n- Test case $1$ (expansive, monotone divergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (1.2, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50)$.\n- Test case $2$ (expansive, oscillatory divergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (5.0, 1.0, 0.1, 1.0, 0.0, 0.0, 0.0, 50)$.\n- Test case $3$ (borderline expansive, linear-growth divergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = \\left(\\frac{1}{1 - 0.1}, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50\\right)$.\n- Test case $4$ (contractive, convergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (0.8, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50)$.\n- Test case $5$ (nonexpansive, convergence): $(\\alpha, \\gamma, \\rho, y, x^0, v^0, u^0, K) = (1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50)$.\n\nThe divergence decision must be purely based on the spectral radius and constant term of the affine system that you derived, not merely on numerical blow-up. The oscillation decision must be based on sign alternations observed in the simulated scaled dual variable across $K$ iterations with a reasonable threshold for declaring oscillation.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list formatted as $[\\text{spectral\\_radius}, \\text{diverges}, \\text{oscillatory}]$. For example, the output should look exactly like $[[r_1,b_1,o_1],[r_2,b_2,o_2],\\dots]$ with no spaces. Angles are not involved in this problem. No physical units are involved. All values are dimensionless. The only numerical values to report are the spectral radii, which should be rounded to six decimal places, and the booleans.",
            "solution": "The problem requires an analysis of the one-dimensional Plug-and-Play Alternating Direction Method of Multipliers (PnP-ADMM) algorithm for a specific quadratic data-fidelity term and a linear denoiser. The task is to derive the iterative update equations, analyze their stability as a linear-affine dynamical system, and implement a simulation to demonstrate divergence and oscillatory behavior for an expansive denoiser.\n\nThe problem is first validated.\n\n**Step 1: Extraction of Givens**\n- Optimization Problem: $\\min_{\\mathbf{x}} \\, g(\\mathbf{x}) + h(\\mathbf{x})$.\n- ADMM Split Form: $\\min_{\\mathbf{x}, \\mathbf{v}} \\, g(\\mathbf{x}) + h(\\mathbf{v})$ subject to $\\mathbf{x} = \\mathbf{v}$.\n- Algorithm: Scaled PnP-ADMM with primal variable $\\mathbf{x}$, auxiliary variable $\\mathbf{v}$, and scaled dual variable $\\mathbf{u}$.\n- Augmented Lagrangian: Utilizes penalty parameter $\\rho > 0$.\n- One-Dimensional Specialization:\n  - Data-fidelity term: $g(x) = \\frac{\\gamma}{2} (x - y)^2$ with $\\gamma > 0$.\n  - Denoiser (replacing prox of $h$): $D(z) = \\alpha z$ with gain $\\alpha > 0$.\n- Task:\n  1. Derive the PnP-ADMM iteration $(x^{k+1}, v^{k+1}, u^{k+1})$ in closed form.\n  2. Express the iteration as a linear-affine system, derive the governing matrix, its spectral radius, and the divergence condition.\n  3. Explain the condition for oscillatory behavior.\n  4. Implement a simulation to validate the theory on specified test cases.\n\n**Step 2: Validation**\nThe problem is well-defined within the mathematical framework of optimization theory and signal processing. It is scientifically sound, objective, and self-contained. The use of a simplified one-dimensional model with a linear denoiser is a standard technique for performing analytical stability analysis of iterative algorithms. The problem is free of any scientific flaws, ambiguities, or contradictions.\n\n**Step 3: Verdict**\nThe problem is valid. The solution proceeds as follows.\n\n**1. Derivation of the PnP-ADMM Iteration**\n\nThe scaled ADMM algorithm addresses the problem $\\min_{x,v} g(x) + h(v)$ subject to $x = v$ by iterating on the augmented Lagrangian:\n$$ \\mathcal{L}_{\\rho}(x, v, u) = g(x) + h(v) + \\frac{\\rho}{2} \\|x - v + u\\|^2_2 - \\frac{\\rho}{2} \\|u\\|^2_2 $$\nThe one-dimensional ADMM iteration consists of three steps:\n\n**x-update:** The variable $x$ is updated by minimizing $\\mathcal{L}_{\\rho}$ with respect to $x$:\n$$ x^{k+1} = \\arg\\min_x \\left( g(x) + \\frac{\\rho}{2} (x - v^k + u^k)^2 \\right) $$\nSubstituting $g(x) = \\frac{\\gamma}{2} (x - y)^2$:\n$$ x^{k+1} = \\arg\\min_x \\left( \\frac{\\gamma}{2} (x - y)^2 + \\frac{\\rho}{2} (x - v^k + u^k)^2 \\right) $$\nTo find the minimum, we set the derivative with respect to $x$ to zero:\n$$ \\frac{\\partial}{\\partial x} \\left[ \\dots \\right] = \\gamma(x-y) + \\rho(x - v^k + u^k) = 0 $$\n$$ (\\gamma + \\rho)x = \\gamma y + \\rho(v^k - u^k) $$\nSolving for $x$ gives the closed-form update:\n$$ x^{k+1} = \\frac{\\gamma y + \\rho(v^k - u^k)}{\\gamma + \\rho} $$\n\n**v-update:** The standard $v$-update is $v^{k+1} = \\text{prox}_{h/\\rho}(x^{k+1} + u^k)$. In PnP-ADMM, this step is replaced by applying a denoiser $D$ to the argument of the proximal operator:\n$$ v^{k+1} = D(x^{k+1} + u^k) $$\nUsing the specified linear denoiser $D(z) = \\alpha z$, the update becomes:\n$$ v^{k+1} = \\alpha (x^{k+1} + u^k) $$\n\n**u-update:** The scaled dual variable is updated as:\n$$ u^{k+1} = u^k + x^{k+1} - v^{k+1} $$\n\nThese three equations define the PnP-ADMM iteration in closed form.\n\n**2. Linear-Affine System and Stability Analysis**\n\nTo analyze the convergence, we formulate the iteration as a linear-affine dynamical system. The state of the system can be defined by the variables that are carried over between iterations, which are $v^k$ and $u^k$. We express $(v^{k+1}, u^{k+1})$ in terms of $(v^k, u^k)$.\n\nFirst, substitute the expression for $v^{k+1}$ into the $u$-update:\n$$ u^{k+1} = u^k + x^{k+1} - \\alpha (x^{k+1} + u^k) = (1 - \\alpha)u^k + (1 - \\alpha)x^{k+1} $$\nNow, substitute the expression for $x^{k+1}$ into the updates for $v^{k+1}$ and $u^{k+1}$. Let $c_x = \\frac{\\gamma y}{\\gamma + \\rho}$.\n$$ x^{k+1} = \\frac{\\rho}{\\gamma + \\rho}v^k - \\frac{\\rho}{\\gamma + \\rho}u^k + c_x $$\nThe update for $v^{k+1}$ becomes:\n$$ v^{k+1} = \\alpha \\left( \\left[ \\frac{\\rho}{\\gamma + \\rho}v^k - \\frac{\\rho}{\\gamma + \\rho}u^k + c_x \\right] + u^k \\right) $$\n$$ v^{k+1} = \\alpha \\left( \\frac{\\rho}{\\gamma + \\rho}v^k + \\left(1 - \\frac{\\rho}{\\gamma + \\rho}\\right)u^k + c_x \\right) $$\n$$ v^{k+1} = \\frac{\\alpha\\rho}{\\gamma + \\rho}v^k + \\frac{\\alpha\\gamma}{\\gamma + \\rho}u^k + \\alpha c_x $$\nThe update for $u^{k+1}$ becomes:\n$$ u^{k+1} = (1 - \\alpha)u^k + (1 - \\alpha)\\left( \\frac{\\rho}{\\gamma + \\rho}v^k - \\frac{\\rho}{\\gamma + \\rho}u^k + c_x \\right) $$\n$$ u^{k+1} = \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho}v^k + \\left( (1 - \\alpha) - \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho} \\right)u^k + (1 - \\alpha)c_x $$\n$$ u^{k+1} = \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho}v^k + (1 - \\alpha)\\left( \\frac{\\gamma + \\rho - \\rho}{\\gamma + \\rho} \\right)u^k + (1 - \\alpha)c_x $$\n$$ u^{k+1} = \\frac{\\rho(1 - \\alpha)}{\\gamma + \\rho}v^k + \\frac{\\gamma(1 - \\alpha)}{\\gamma + \\rho}u^k + (1 - \\alpha)c_x $$\nLet the state vector be $z^k = [v^k, u^k]^T$. The iteration is a linear-affine system $z^{k+1} = M z^k + b$, where the matrix $M$ and vector $b$ are:\n$$ M = \\frac{1}{\\gamma + \\rho} \\begin{pmatrix} \\alpha\\rho & \\alpha\\gamma \\\\ \\rho(1-\\alpha) & \\gamma(1-\\alpha) \\end{pmatrix}, \\quad b = c_x \\begin{pmatrix} \\alpha \\\\ 1-\\alpha \\end{pmatrix} = \\frac{\\gamma y}{\\gamma + \\rho} \\begin{pmatrix} \\alpha \\\\ 1-\\alpha \\end{pmatrix} $$\nThe stability of this system is determined by the spectral radius of $M$, $\\rho(M)$, which is the maximum absolute value of its eigenvalues. We find the eigenvalues $\\lambda$ by solving $\\det(M - \\lambda I) = 0$. Let $\\lambda' = (\\gamma+\\rho)\\lambda$ be the eigenvalues of $(\\gamma+\\rho)M$.\n$$ \\det \\begin{pmatrix} \\alpha\\rho - \\lambda' & \\alpha\\gamma \\\\ \\rho(1-\\alpha) & \\gamma(1-\\alpha) - \\lambda' \\end{pmatrix} = 0 $$\n$$ (\\alpha\\rho - \\lambda')(\\gamma(1-\\alpha) - \\lambda') - \\alpha\\gamma\\rho(1-\\alpha) = 0 $$\n$$ \\lambda'^2 - (\\alpha\\rho + \\gamma(1-\\alpha))\\lambda' + \\alpha\\rho\\gamma(1-\\alpha) - \\alpha\\gamma\\rho(1-\\alpha) = 0 $$\n$$ \\lambda'^2 - (\\alpha\\rho + \\gamma - \\alpha\\gamma)\\lambda' = 0 $$\n$$ \\lambda'(\\lambda' - (\\alpha\\rho + \\gamma - \\alpha\\gamma)) = 0 $$\nThe eigenvalues of $(\\gamma+\\rho)M$ are $\\lambda'_1 = 0$ and $\\lambda'_2 = \\alpha(\\rho - \\gamma) + \\gamma$. The eigenvalues of $M$ are therefore:\n$$ \\lambda_1 = 0, \\quad \\lambda_2 = \\frac{\\alpha(\\rho - \\gamma) + \\gamma}{\\gamma + \\rho} $$\nThe spectral radius is $\\rho(M) = \\max(|\\lambda_1|, |\\lambda_2|) = |\\lambda_2|$.\n$$ \\rho(M) = \\left| \\frac{\\alpha(\\rho - \\gamma) + \\gamma}{\\gamma + \\rho} \\right| $$\nThe PnP-ADMM iteration converges if $\\rho(M) < 1$ and diverges if $\\rho(M) > 1$. In the borderline case $\\rho(M) = 1$, the iteration may exhibit linear growth and thus diverges if the constant term $b$ is not in the image of $(I-M)$, which is the case for the parameters given. Therefore, the precise condition for divergence is $\\rho(M) \\ge 1$.\n\n**3. Condition for Oscillatory Behavior**\n\nOscillatory behavior, characterized by the alternating signs of the an error term across iterations, occurs when the dominant eigenvalue is real and negative. Since $\\lambda_1 = 0$, the dynamics are governed by $\\lambda_2$. Oscillation occurs when $\\lambda_2 < 0$.\n$$ \\frac{\\alpha(\\rho - \\gamma) + \\gamma}{\\gamma + \\rho} < 0 $$\nAs $\\gamma > 0$ and $\\rho > 0$, the denominator $\\gamma + \\rho$ is positive. The condition simplifies to:\n$$ \\alpha(\\rho - \\gamma) + \\gamma < 0 \\implies \\alpha(\\rho - \\gamma) < -\\gamma $$\nThis inequality can only be satisfied if $\\rho - \\gamma < 0$ (i.e., $\\gamma > \\rho$), because if $\\rho - \\gamma \\ge 0$, the left side is non-negative for $\\alpha > 0$ and cannot be less than the negative value $-\\gamma$. If $\\gamma > \\rho$, the condition for oscillation becomes:\n$$ \\alpha > \\frac{-\\gamma}{\\rho - \\gamma} \\implies \\alpha > \\frac{\\gamma}{\\gamma - \\rho} $$\nThus, oscillatory behavior is expected when $\\gamma > \\rho$ and the denoiser gain $\\alpha$ is sufficiently large, specifically $\\alpha > \\gamma / (\\gamma - \\rho)$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and simulates PnP-ADMM for several test cases.\n\n    For each case, it computes the spectral radius of the iteration matrix,\n    determines divergence based on the spectral radius, and detects oscillatory\n    behavior by simulating the sequence and counting sign changes.\n    \"\"\"\n    # Test cases: (alpha, gamma, rho, y, x0, v0, u0, K)\n    test_cases = [\n        (1.2, 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n        (5.0, 1.0, 0.1, 1.0, 0.0, 0.0, 0.0, 50),\n        (1.0 / (1.0 - 0.1), 0.1, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n        (0.8, 2.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n        (1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 50),\n    ]\n\n    results = []\n    for alpha, gamma, rho, y, x0, v0, u0, K in test_cases:\n        # 1. Theoretical Analysis\n        # Calculate the non-zero eigenvalue of the iteration matrix.\n        lambda2 = (alpha * (rho - gamma) + gamma) / (gamma + rho)\n        \n        # The spectral radius is the absolute value of the dominant eigenvalue.\n        spectral_radius = abs(lambda2)\n        \n        # Divergence occurs if the spectral radius is greater than or equal to 1.\n        diverges = spectral_radius >= 1.0\n\n        # 2. Simulation\n        x, v, u = float(x0), float(v0), float(u0)\n        u_history = [u]\n        \n        for _ in range(K):\n            # One-dimensional PnP-ADMM updates\n            x_next = (gamma * y + rho * (v - u)) / (gamma + rho)\n            v_next = alpha * (x_next + u)\n            u_next = u + x_next - v_next\n            \n            x, v, u = x_next, v_next, u_next\n            u_history.append(u)\n        \n        # 3. Oscillation Check\n        # Oscillation is detected by counting sign alternations in the dual variable u.\n        sign_changes = 0\n        for i in range(1, len(u_history)):\n            # A sign change occurs if u_k and u_{k-1} have opposite signs.\n            # np.sign handles the zero case correctly.\n            if np.sign(u_history[i]) * np.sign(u_history[i-1]) == -1:\n                sign_changes += 1\n        \n        # A reasonable threshold for oscillation: sign changes in more than a quarter of steps.\n        oscillatory = sign_changes > K / 4\n\n        results.append([spectral_radius, diverges, oscillatory])\n\n    # 4. Format Output\n    # The output must be a single line, formatted as a list of lists with no spaces.\n    formatted_results = []\n    for r, b, o in results:\n        r_str = f\"{r:.6f}\"\n        b_str = str(b).lower()\n        o_str = str(o).lower()\n        formatted_results.append(f\"[{r_str},{b_str},{o_str}]\")\n    \n    # Final print statement must be in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many inverse problems, particularly in compressed sensing, the forward operator $A$ has a non-trivial null space, meaning some signal components are invisible to the measurements. A powerful denoiser might inadvertently introduce or \"hallucinate\" artificial structures within this null space, degrading reconstruction quality. This advanced practice  tackles this subtle but critical issue by teaching you to first quantify this null-space drift and then design a principled penalization method to suppress it, enhancing the faithfulness of the final solution.",
            "id": "3466556",
            "problem": "Consider the linear inverse problem model in compressed sensing: a signal $\\mathbf{x} \\in \\mathbb{R}^n$ is observed through measurements $\\mathbf{y} = A \\mathbf{x} + \\mathbf{w}$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known sensing matrix with $m \\leq n$, and $\\mathbf{w}$ is measurement noise. Let $D_\\sigma: \\mathbb{R}^n \\to \\mathbb{R}^n$ be a denoiser parameterized by noise level $\\sigma > 0$, as used in Plug-and-Play Alternating Direction Method of Multipliers (ADMM) and Regularization by Denoising (RED). In underdetermined settings, the null space $\\mathcal{N}(A) = \\{\\mathbf{z} \\in \\mathbb{R}^n : A \\mathbf{z} = \\mathbf{0}\\}$ is nontrivial, and changes to the component of $\\mathbf{x}$ in $\\mathcal{N}(A)$ do not affect the data consistency term $\\|A \\mathbf{x} - \\mathbf{y}\\|_2$. This can lead to failure modes where the denoiser $D_\\sigma$ hallucinates content in the null space, i.e., modifies $\\mathbf{x}$ along directions invisible to the measurements.\n\nUsing only fundamental definitions and well-tested facts from linear algebra and optimization, you must:\n\n1. Define the orthogonal projector $P_{\\mathcal{N}(A)}$ onto the null space $\\mathcal{N}(A)$, and compute the null-space drift induced by the denoiser as\n   $$\\Delta_{\\text{ns}}(\\mathbf{x}; A, D_\\sigma) = \\left\\| P_{\\mathcal{N}(A)} \\left( D_\\sigma(\\mathbf{x}) \\right) - P_{\\mathcal{N}(A)} \\left( \\mathbf{x} \\right) \\right\\|_2.$$\n   Your program should compute this quantity numerically for each test case.\n\n2. Design a penalization that suppresses null-space changes by solving, for a given $\\lambda_{\\text{ns}} \\geq 0$,\n   $$\\mathbf{z}^\\star = \\arg\\min_{\\mathbf{z} \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\left\\| \\mathbf{z} - D_\\sigma(\\mathbf{x}) \\right\\|_2^2 \\;+\\; \\frac{\\lambda_{\\text{ns}}}{2} \\left\\| P_{\\mathcal{N}(A)} \\left( \\mathbf{z} - \\mathbf{x} \\right) \\right\\|_2^2.$$\n   Derive from first principles an explicit expression for $\\mathbf{z}^\\star$ in terms of $D_\\sigma(\\mathbf{x})$, $P_{\\mathcal{N}(A)}$, $\\mathbf{x}$, and $\\lambda_{\\text{ns}}$, using only the properties that $P_{\\mathcal{N}(A)}$ is an orthogonal projector (symmetric and idempotent) and basic rules of quadratic minimization.\n\n3. Implement the derived expression for $\\mathbf{z}^\\star$ and measure the penalized null-space drift\n   $$\\Delta_{\\text{ns}}^{\\text{pen}}(\\mathbf{x}; A, D_\\sigma, \\lambda_{\\text{ns}}) = \\left\\| P_{\\mathcal{N}(A)} \\left( \\mathbf{z}^\\star \\right) - P_{\\mathcal{N}(A)} \\left( \\mathbf{x} \\right) \\right\\|_2.$$\n   Your program must verify whether null-space drift decreases when $\\lambda_{\\text{ns}} > 0$, in comparison to the unpenalized drift.\n\nFundamental base to be used:\n- The null space definition $\\mathcal{N}(A) = \\{\\mathbf{z} : A \\mathbf{z} = \\mathbf{0}\\}$.\n- The orthogonal projector properties: $P_{\\mathcal{N}(A)}^\\top = P_{\\mathcal{N}(A)}$ and $P_{\\mathcal{N}(A)}^2 = P_{\\mathcal{N}(A)}$.\n- Singular Value Decomposition (SVD) for computing an orthonormal basis for $\\mathcal{N}(A)$ and hence $P_{\\mathcal{N}(A)}$.\n- Quadratic minimization with symmetric positive semidefinite matrices.\n\nDenoiser definition to be implemented:\n- Use the elementwise soft-thresholding denoiser for $\\sigma \\geq 0$:\n  $$\\left[ D_\\sigma(\\mathbf{x}) \\right]_i = \\operatorname{sign}(x_i) \\cdot \\max\\left( |x_i| - \\sigma, \\; 0 \\right), \\quad i = 1, \\dots, n.$$\n\nNumerical computation of $P_{\\mathcal{N}(A)}$:\n- Compute the Singular Value Decomposition $A = U \\Sigma V^\\top$ with $V \\in \\mathbb{R}^{n \\times n}$ orthogonal. Let $r$ be the numerical rank of $A$ (number of singular values greater than a tolerance). Then an orthonormal basis for $\\mathcal{N}(A)$ is given by the last $n - r$ columns of $V$. If $V_{\\text{null}} \\in \\mathbb{R}^{n \\times (n-r)}$ collects these columns, set\n  $$P_{\\mathcal{N}(A)} = V_{\\text{null}} V_{\\text{null}}^\\top.$$\n- If $\\mathcal{N}(A) = \\{\\mathbf{0}\\}$, then $P_{\\mathcal{N}(A)}$ is the zero matrix in $\\mathbb{R}^{n \\times n}$.\n\nTest suite:\nImplement the computations for the following five test cases. Matrices and vectors are specified explicitly and must be used exactly as given.\n\n- Case 1 (underdetermined, moderate penalization):\n  $$A_1 = \\begin{bmatrix}\n  1 & 0 & -1 & 2 & 0 & 1 & 0 & 0 \\\\\n  0 & 1 & 0 & -1 & 2 & 0 & 1 & 0 \\\\\n  1 & 1 & 0 & 0 & 1 & -1 & 0 & 1 \\\\\n  0 & 0 & 1 & 1 & -1 & 0 & 2 & 0 \\\\\n  2 & -1 & 0 & 0 & 1 & 1 & 0 & -1\n  \\end{bmatrix}, \\quad \\mathbf{x}_1 = \\begin{bmatrix} 0.3 \\\\ -0.7 \\\\ 1.2 \\\\ -0.1 \\\\ 0.0 \\\\ 0.5 \\\\ -0.4 \\\\ 0.9 \\end{bmatrix}, \\quad \\sigma_1 = 0.2, \\quad \\lambda_{\\text{ns},1} = 0.5.$$\n\n- Case 2 (same as Case 1, no penalization):\n  $$A_1 \\text{ as above}, \\quad \\mathbf{x}_1 \\text{ as above}, \\quad \\sigma_2 = 0.2, \\quad \\lambda_{\\text{ns},2} = 0.$$\n\n- Case 3 (same as Case 1, strong penalization):\n  $$A_1 \\text{ as above}, \\quad \\mathbf{x}_1 \\text{ as above}, \\quad \\sigma_3 = 0.2, \\quad \\lambda_{\\text{ns},3} = 10.$$\n\n- Case 4 (square, full-rank matrix, null space trivial):\n  $$A_2 = \\begin{bmatrix}\n  2 & 0 & 0 & 0 & 0 & 0 \\\\\n  1 & 3 & 0 & 0 & 0 & 0 \\\\\n  0 & -1 & 1 & 0 & 0 & 0 \\\\\n  0 & 0 & 2 & 4 & 0 & 0 \\\\\n  0 & 0 & 0 & -2 & 5 & 0 \\\\\n  0 & 0 & 0 & 0 & 1 & 6\n  \\end{bmatrix}, \\quad \\mathbf{x}_2 = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\\\ -1.5 \\\\ 0.75 \\\\ 0.0 \\end{bmatrix}, \\quad \\sigma_4 = 0.3, \\quad \\lambda_{\\text{ns},4} = 0.7.$$\n\n- Case 5 (underdetermined, identity denoiser):\n  $$A_1 \\text{ as above}, \\quad \\mathbf{x}_3 = \\begin{bmatrix} -0.05 \\\\ 0.04 \\\\ -0.8 \\\\ 0.12 \\\\ -0.33 \\\\ 0.55 \\\\ 0.0 \\\\ -0.1 \\end{bmatrix}, \\quad \\sigma_5 = 0.0, \\quad \\lambda_{\\text{ns},5} = 5.0.$$\n\nTasks for each case:\n- Compute the projector $P_{\\mathcal{N}(A)}$.\n- Compute the unpenalized denoiser output $D_\\sigma(\\mathbf{x})$.\n- Compute the null-space drift $\\Delta_{\\text{ns}}(\\mathbf{x}; A, D_\\sigma)$.\n- Derive and implement the penalized output $\\mathbf{z}^\\star$.\n- Compute the penalized null-space drift $\\Delta_{\\text{ns}}^{\\text{pen}}(\\mathbf{x}; A, D_\\sigma, \\lambda_{\\text{ns}})$.\n- Determine whether drift decreased, i.e., check if $\\Delta_{\\text{ns}}^{\\text{pen}} \\le \\Delta_{\\text{ns}}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be represented as a list containing four entries in the order:\n$$\\left[ \\Delta_{\\text{ns}}, \\; \\Delta_{\\text{ns}}^{\\text{pen}}, \\; r, \\; b \\right],$$\nwhere $r$ is the floating-point reduction ratio $\\Delta_{\\text{ns}}^{\\text{pen}} / \\Delta_{\\text{ns}}$ (use $r = 1.0$ if $\\Delta_{\\text{ns}} = 0$), and $b$ is a boolean indicating whether null-space drift decreased. For example,\n$$\\left[ [d_1, d_1^{\\text{pen}}, r_1, b_1], [d_2, d_2^{\\text{pen}}, r_2, b_2], \\dots \\right].$$",
            "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n\n- **Model**: Linear inverse problem $\\mathbf{y} = A \\mathbf{x} + \\mathbf{w}$, with $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{y} \\in \\mathbb{R}^m$, $A \\in \\mathbb{R}^{m \\times n}$ ($m \\le n$), and noise $\\mathbf{w}$.\n- **Denoiser**: A function $D_\\sigma: \\mathbb{R}^n \\to \\mathbb{R}^n$ parameterized by noise level $\\sigma > 0$. The specific implementation is the elementwise soft-thresholding operator: $\\left[ D_\\sigma(\\mathbf{x}) \\right]_i = \\operatorname{sign}(x_i) \\cdot \\max\\left( |x_i| - \\sigma, \\; 0 \\right)$.\n- **Null Space**: $\\mathcal{N}(A) = \\{\\mathbf{z} \\in \\mathbb{R}^n : A \\mathbf{z} = \\mathbf{0}\\}$.\n- **Projector**: $P_{\\mathcal{N}(A)}$ is the orthogonal projector onto $\\mathcal{N}(A)$.\n- **Projector Computation**: Given the SVD $A = U \\Sigma V^\\top$, where $V = [\\mathbf{v}_1, \\dots, \\mathbf{v}_n]$ is orthogonal, and rank of $A$ is $r$. The null space basis is $V_{\\text{null}} = [\\mathbf{v}_{r+1}, \\dots, \\mathbf{v}_n]$. The projector is $P_{\\mathcal{N}(A)} = V_{\\text{null}} V_{\\text{null}}^\\top$.\n- **Projector Properties**: $P_{\\mathcal{N}(A)}^\\top = P_{\\mathcal{N}(A)}$ (symmetry) and $P_{\\mathcal{N}(A)}^2 = P_{\\mathcal{N}(A)}$ (idempotence).\n- **Null-Space Drift**: $\\Delta_{\\text{ns}}(\\mathbf{x}; A, D_\\sigma) = \\left\\| P_{\\mathcal{N}(A)} \\left( D_\\sigma(\\mathbf{x}) \\right) - P_{\\mathcal{N}(A)} \\left( \\mathbf{x} \\right) \\right\\|_2$.\n- **Penalized Optimization Problem**:\n  $$\\mathbf{z}^\\star = \\arg\\min_{\\mathbf{z} \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\left\\| \\mathbf{z} - D_\\sigma(\\mathbf{x}) \\right\\|_2^2 \\;+\\; \\frac{\\lambda_{\\text{ns}}}{2} \\left\\| P_{\\mathcal{N}(A)} \\left( \\mathbf{z} - \\mathbf{x} \\right) \\right\\|_2^2, \\quad \\text{for a given } \\lambda_{\\text{ns}} \\ge 0.$$\n- **Penalized Null-Space Drift**: $\\Delta_{\\text{ns}}^{\\text{pen}}(\\mathbf{x}; A, D_\\sigma, \\lambda_{\\text{ns}}) = \\left\\| P_{\\mathcal{N}(A)} \\left( \\mathbf{z}^\\star \\right) - P_{\\mathcal{N}(A)} \\left( \\mathbf{x} \\right) \\right\\|_2$.\n- **Test Cases**: Five specific cases are provided with matrices $A_1, A_2$, vectors $\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3$, and parameters $\\sigma_k, \\lambda_{\\text{ns},k}$.\n- **Required Outputs**: For each case, compute $\\Delta_{\\text{ns}}$, $\\Delta_{\\text{ns}}^{\\text{pen}}$, the reduction ratio $r$, and a boolean $b$ indicating if drift decreased.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is firmly rooted in linear algebra (null spaces, orthogonal projectors, SVD) and convex optimization (quadratic minimization). The application context is compressed sensing and image reconstruction, where plug-and-play methods and null-space behavior are established areas of research. The model and definitions are standard.\n- **Well-Posed**: The optimization problem for $\\mathbf{z}^\\star$ involves minimizing a sum of two squared $\\ell_2$-norms. This objective function is strictly convex, guaranteeing a unique solution exists. The steps for computation are clearly specified.\n- **Objective**: The problem is stated with mathematical precision. All variables, matrices, and tasks are defined formally. There is no subjective or ambiguous language.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is scientifically sound, well-posed, and objective.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivations and Method\n\nThe problem requires the derivation of the penalized estimate $\\mathbf{z}^\\star$ and the analysis of its effect on null-space drift.\n\n**1. Null-Space Projector and Drift Computation**\n\nThe orthogonal projector onto the null space $\\mathcal{N}(A)$ is a symmetric and idempotent matrix $P_{\\mathcal{N}(A)}$. It can be constructed from an orthonormal basis for $\\mathcal{N}(A)$. As specified, such a basis can be found from the Singular Value Decomposition (SVD) of $A$. Let $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix of singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq 0$. If $A$ has numerical rank $r$, then the last $n-r$ columns of $V$, which correspond to the zero (or numerically negligible) singular values, form an orthonormal basis for $\\mathcal{N}(A)$. Let this basis be collected in the matrix $V_{\\text{null}} \\in \\mathbb{R}^{n \\times (n-r)}$. The projector is then given by $P_{\\mathcal{N}(A)} = V_{\\text{null}}V_{\\text{null}}^\\top$. If $n=r$, the null space is trivial, $\\mathcal{N}(A) = \\{\\mathbf{0}\\}$, and $P_{\\mathcal{N}(A)}$ is the $n \\times n$ zero matrix.\n\nThe unpenalized null-space drift is defined as $\\Delta_{\\text{ns}} = \\| P_{\\mathcal{N}(A)}(D_\\sigma(\\mathbf{x})) - P_{\\mathcal{N}(A)}(\\mathbf{x}) \\|_2$. Using the linearity of the projection operator, this simplifies to:\n$$ \\Delta_{\\text{ns}} = \\left\\| P_{\\mathcal{N}(A)} \\left( D_\\sigma(\\mathbf{x}) - \\mathbf{x} \\right) \\right\\|_2 $$\n\n**2. Derivation of the Penalized Solution $\\mathbf{z}^\\star$**\n\nThe solution $\\mathbf{z}^\\star$ minimizes the objective function:\n$$ J(\\mathbf{z}) = \\frac{1}{2} \\left\\| \\mathbf{z} - D_\\sigma(\\mathbf{x}) \\right\\|_2^2 \\;+\\; \\frac{\\lambda_{\\text{ns}}}{2} \\left\\| P_{\\mathcal{N}(A)} \\left( \\mathbf{z} - \\mathbf{x} \\right) \\right\\|_2^2 $$\nThis function is quadratic and convex. The minimum is found by setting its gradient with respect to $\\mathbf{z}$ to zero. First, we expand the squared norms using inner products:\n$$ J(\\mathbf{z}) = \\frac{1}{2} (\\mathbf{z} - D_\\sigma(\\mathbf{x}))^\\top (\\mathbf{z} - D_\\sigma(\\mathbf{x})) + \\frac{\\lambda_{\\text{ns}}}{2} (\\mathbf{z} - \\mathbf{x})^\\top P_{\\mathcal{N}(A)}^\\top P_{\\mathcal{N}(A)} (\\mathbf{z} - \\mathbf{x}) $$\nUsing the properties $P_{\\mathcal{N}(A)}^\\top = P_{\\mathcal{N}(A)}$ and $P_{\\mathcal{N}(A)}^2 = P_{\\mathcal{N}(A)}$, this becomes:\n$$ J(\\mathbf{z}) = \\frac{1}{2} (\\mathbf{z} - D_\\sigma(\\mathbf{x}))^\\top (\\mathbf{z} - D_\\sigma(\\mathbf{x})) + \\frac{\\lambda_{\\text{ns}}}{2} (\\mathbf{z} - \\mathbf{x})^\\top P_{\\mathcal{N}(A)} (\\mathbf{z} - \\mathbf{x}) $$\nThe gradient with respect to $\\mathbf{z}$ is:\n$$ \\nabla_{\\mathbf{z}} J(\\mathbf{z}) = (\\mathbf{z} - D_\\sigma(\\mathbf{x})) + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)} (\\mathbf{z} - \\mathbf{x}) $$\nSetting $\\nabla_{\\mathbf{z}} J(\\mathbf{z}^\\star) = \\mathbf{0}$:\n$$ \\mathbf{z}^\\star - D_\\sigma(\\mathbf{x}) + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)} \\mathbf{z}^\\star - \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)} \\mathbf{x} = \\mathbf{0} $$\nGrouping terms involving $\\mathbf{z}^\\star$:\n$$ (I + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)}) \\mathbf{z}^\\star = D_\\sigma(\\mathbf{x}) + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)} \\mathbf{x} $$\nwhere $I$ is the identity matrix. To solve for $\\mathbf{z}^\\star$, we need to find the inverse of the matrix $(I + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)})$. We can posit an inverse of the form $(I + \\alpha P_{\\mathcal{N}(A)})$ and solve for $\\alpha$.\n$$ (I + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)})(I + \\alpha P_{\\mathcal{N}(A)}) = I + \\alpha P_{\\mathcal{N}(A)} + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)} + \\alpha \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)}^2 \\\\ = I + (\\alpha + \\lambda_{\\text{ns}} + \\alpha \\lambda_{\\text{ns}}) P_{\\mathcal{N}(A)} $$\nFor this to equal $I$, the coefficient of $P_{\\mathcal{N}(A)}$ must be zero:\n$$ \\alpha(1 + \\lambda_{\\text{ns}}) + \\lambda_{\\text{ns}} = 0 \\implies \\alpha = -\\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} $$\nThis is valid since $\\lambda_{\\text{ns}} \\ge 0$. Thus, $(I + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)})^{-1} = I - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)}$.\nApplying this inverse:\n$$ \\mathbf{z}^\\star = \\left(I - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)}\\right) \\left( D_\\sigma(\\mathbf{x}) + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)} \\mathbf{x} \\right) $$\nExpanding and simplifying (using $P_{\\mathcal{N}(A)}^2=P_{\\mathcal{N}(A)}$):\n$$ \\mathbf{z}^\\star = D_\\sigma(\\mathbf{x}) + \\lambda_{\\text{ns}} P_{\\mathcal{N}(A)}\\mathbf{x} - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)}D_\\sigma(\\mathbf{x}) - \\frac{\\lambda_{\\text{ns}}^2}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)}\\mathbf{x} $$\n$$ \\mathbf{z}^\\star = D_\\sigma(\\mathbf{x}) - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)}D_\\sigma(\\mathbf{x}) + \\left(\\lambda_{\\text{ns}} - \\frac{\\lambda_{\\text{ns}}^2}{1+\\lambda_{\\text{ns}}}\\right) P_{\\mathcal{N}(A)}\\mathbf{x} $$\nThe term in parentheses simplifies to $\\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}}$. So, we obtain the final expression:\n$$ \\mathbf{z}^\\star = D_\\sigma(\\mathbf{x}) - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)} \\left( D_\\sigma(\\mathbf{x}) - \\mathbf{x} \\right) $$\nThis expression is computationally efficient and will be used in the implementation.\n\n**3. Penalized Null-Space Drift Analysis**\n\nThe penalized null-space drift is $\\Delta_{\\text{ns}}^{\\text{pen}} = \\| P_{\\mathcal{N}(A)}(\\mathbf{z}^\\star) - P_{\\mathcal{N}(A)}(\\mathbf{x}) \\|_2 = \\| P_{\\mathcal{N}(A)}(\\mathbf{z}^\\star - \\mathbf{x}) \\|_2$.\nLet's find the projected difference $P_{\\mathcal{N}(A)}(\\mathbf{z}^\\star - \\mathbf{x})$:\n$$ \\mathbf{z}^\\star - \\mathbf{x} = (D_\\sigma(\\mathbf{x}) - \\mathbf{x}) - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)} ( D_\\sigma(\\mathbf{x}) - \\mathbf{x} ) $$\nApplying $P_{\\mathcal{N}(A)}$ from the left and using $P_{\\mathcal{N}(A)}^2 = P_{\\mathcal{N}(A)}$:\n$$ P_{\\mathcal{N}(A)}(\\mathbf{z}^\\star - \\mathbf{x}) = P_{\\mathcal{N}(A)}(D_\\sigma(\\mathbf{x}) - \\mathbf{x}) - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)}^2 ( D_\\sigma(\\mathbf{x}) - \\mathbf{x} ) $$\n$$ P_{\\mathcal{N}(A)}(\\mathbf{z}^\\star - \\mathbf{x}) = \\left(1 - \\frac{\\lambda_{\\text{ns}}}{1+\\lambda_{\\text{ns}}}\\right) P_{\\mathcal{N}(A)} ( D_\\sigma(\\mathbf{x}) - \\mathbf{x} ) = \\frac{1}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)} ( D_\\sigma(\\mathbf{x}) - \\mathbf{x} ) $$\nTaking the $\\ell_2$-norm gives the penalized drift:\n$$ \\Delta_{\\text{ns}}^{\\text{pen}} = \\left\\| \\frac{1}{1+\\lambda_{\\text{ns}}} P_{\\mathcal{N}(A)} ( D_\\sigma(\\mathbf{x}) - \\mathbf{x} ) \\right\\|_2 = \\frac{1}{1+\\lambda_{\\text{ns}}} \\left\\| P_{\\mathcal{N}(A)} ( D_\\sigma(\\mathbf{x}) - \\mathbf{x} ) \\right\\|_2 $$\nThis leads to the remarkable analytical relationship:\n$$ \\Delta_{\\text{ns}}^{\\text{pen}} = \\frac{1}{1+\\lambda_{\\text{ns}}} \\Delta_{\\text{ns}} $$\nThe reduction ratio is $r = \\Delta_{\\text{ns}}^{\\text{pen}} / \\Delta_{\\text{ns}} = \\frac{1}{1+\\lambda_{\\text{ns}}}$, provided $\\Delta_{\\text{ns}} \\neq 0$. As required, if $\\Delta_{\\text{ns}} = 0$, we set $r=1.0$.\nSince $\\lambda_{\\text{ns}} \\ge 0$, the factor $\\frac{1}{1+\\lambda_{\\text{ns}}}$ is always less than or equal to $1$. Therefore, $\\Delta_{\\text{ns}}^{\\text{pen}} \\le \\Delta_{\\text{ns}}$ is always true, meaning the penalization never increases the null-space drift. For $\\lambda_{\\text{ns}} > 0$ and $\\Delta_{\\text{ns}} > 0$, the drift is strictly reduced.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    # --- Test Case Definitions ---\n    A1 = np.array([\n        [1, 0, -1, 2, 0, 1, 0, 0],\n        [0, 1, 0, -1, 2, 0, 1, 0],\n        [1, 1, 0, 0, 1, -1, 0, 1],\n        [0, 0, 1, 1, -1, 0, 2, 0],\n        [2, -1, 0, 0, 1, 1, 0, -1]\n    ], dtype=np.float64)\n\n    x1 = np.array([0.3, -0.7, 1.2, -0.1, 0.0, 0.5, -0.4, 0.9], dtype=np.float64)\n\n    A2 = np.array([\n        [2, 0, 0, 0, 0, 0],\n        [1, 3, 0, 0, 0, 0],\n        [0, -1, 1, 0, 0, 0],\n        [0, 0, 2, 4, 0, 0],\n        [0, 0, 0, -2, 5, 0],\n        [0, 0, 0, 0, 1, 6]\n    ], dtype=np.float64)\n\n    x2 = np.array([1.0, -0.5, 0.25, -1.5, 0.75, 0.0], dtype=np.float64)\n\n    x3 = np.array([-0.05, 0.04, -0.8, 0.12, -0.33, 0.55, 0.0, -0.1], dtype=np.float64)\n\n    test_cases = [\n        # Case 1: underdetermined, moderate penalization\n        {'A': A1, 'x': x1, 'sigma': 0.2, 'lambda_ns': 0.5},\n        # Case 2: same as Case 1, no penalization\n        {'A': A1, 'x': x1, 'sigma': 0.2, 'lambda_ns': 0.0},\n        # Case 3: same as Case 1, strong penalization\n        {'A': A1, 'x': x1, 'sigma': 0.2, 'lambda_ns': 10.0},\n        # Case 4: square, full-rank matrix\n        {'A': A2, 'x': x2, 'sigma': 0.3, 'lambda_ns': 0.7},\n        # Case 5: underdetermined, identity denoiser\n        {'A': A1, 'x': x3, 'sigma': 0.0, 'lambda_ns': 5.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = compute_drift_metrics(**case)\n        results.append(list(res))\n    \n    # Custom string representation to match the required format\n    # \"[[d1,d1_pen,r1,b1],[d2,d2_pen,r2,b2],...]\"\n    outer_list = []\n    for res_list in results:\n        # Format each item in the inner list\n        d_ns_str = f\"{res_list[0]:.16g}\"\n        d_pen_str = f\"{res_list[1]:.16g}\"\n        r_str = f\"{res_list[2]:.16g}\"\n        b_str = str(res_list[3]).lower()\n        inner_str = f\"[{d_ns_str},{d_pen_str},{r_str},{b_str}]\"\n        outer_list.append(inner_str)\n    \n    final_output = f\"[{','.join(outer_list)}]\"\n    print(final_output)\n\n\ndef soft_thresholding_denoiser(x, sigma):\n    \"\"\"\n    Computes the elementwise soft-thresholding denoiser.\n    [D_sigma(x)]_i = sign(x_i) * max(|x_i| - sigma, 0)\n    \"\"\"\n    if sigma  0:\n        raise ValueError(\"Sigma must be non-negative.\")\n    return np.sign(x) * np.maximum(np.abs(x) - sigma, 0)\n\ndef get_null_space_projector(A, tol=1e-12):\n    \"\"\"\n    Computes the orthogonal projector onto the null space of A.\n    \"\"\"\n    n = A.shape[1]\n    # U, s, Vt are standard names for SVD components. V is Vt.T\n    _, s, Vt = np.linalg.svd(A)\n    \n    # Determine numerical rank\n    rank = np.sum(s > tol)\n    \n    # Get the basis for the null space (last n-r columns of V)\n    V_null = Vt[rank:].T\n    \n    # The projector P = V_null @ V_null.T\n    if V_null.shape[1] == 0: # Trivial null space\n        return np.zeros((n, n), dtype=A.dtype)\n    else:\n        return V_null @ V_null.T\n\ndef compute_drift_metrics(A, x, sigma, lambda_ns):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \n    Returns:\n        tuple: (delta_ns, delta_ns_pen, r, b)\n    \"\"\"\n    # 1. Compute the projector\n    P_ns = get_null_space_projector(A)\n    \n    # 2. Compute the denoiser output\n    Dx = soft_thresholding_denoiser(x, sigma)\n    \n    # 3. Compute the unpenalized null-space drift\n    diff_unpenalized = Dx - x\n    projected_diff_unpenalized = P_ns @ diff_unpenalized\n    delta_ns = np.linalg.norm(projected_diff_unpenalized)\n    \n    # 4. Compute the penalized solution z_star from the derived expression\n    # z_star = Dx - (lambda_ns / (1 + lambda_ns)) * P_ns @ (Dx - x)\n    if 1 + lambda_ns == 0: # Should not happen as lambda_ns >= 0\n        factor = np.inf\n    else:\n        factor = lambda_ns / (1 + lambda_ns)\n        \n    z_star = Dx - factor * projected_diff_unpenalized\n    \n    # 5. Compute the penalized null-space drift\n    diff_penalized = z_star - x\n    projected_diff_penalized = P_ns @ diff_penalized\n    delta_ns_pen = np.linalg.norm(projected_diff_penalized)\n    \n    # 6. Compute reduction ratio r\n    if delta_ns  1e-15: # Treat numerically zero drift as a special case\n        r = 1.0\n    else:\n        r = delta_ns_pen / delta_ns\n        \n    # 7. Check if drift decreased\n    # Use a small tolerance for floating point comparison\n    b = (delta_ns_pen = delta_ns + 1e-15)\n    \n    return delta_ns, delta_ns_pen, r, b\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}