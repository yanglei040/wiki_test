## 引言
在[计算成像](@entry_id:170703)、信号处理和机器学习等众多领域，从不完整或含噪的观测数据中恢复高质量的原始信号，即所谓的“[逆问题](@entry_id:143129)”，是一个核心且充满挑战的任务。传统方法通常依赖于手工设计的正则化项（如[稀疏性](@entry_id:136793)或总变分）来约束解空间，但这些简单的先验模型往往难以捕捉真实世界信号（如自然图像）的复杂内在结构。

近年来，即插即用先验（Plug-and-Play Priors, PnP）与通过去噪进行正则化（Regularization by Denoising, RED）作为一种革命性的[范式](@entry_id:161181)应运而生，极大地改变了我们解决逆问题的方式。这些方法巧妙地提出，与其局限于具有明确数学形式的正则化函数，不如直接利用[图像去噪](@entry_id:750522)领域最先进的算法——包括强大的[深度神经网络](@entry_id:636170)——作为隐式或显式的先验知识。这种解耦思想不仅极大地提升了重建性能，也为融合数据驱动模型与经典优化框架提供了坚实的桥梁。然而，这种灵活性也带来了新的理论挑战：我们如何保证算法的收敛，其解又对应着何种优化目标？

本文旨在为读者系统性地梳理 PnP 与 RED 的核心思想、理论基础与前沿应用。在“**原理与机制**”一章中，我们将从[贝叶斯推断](@entry_id:146958)的视角出发，揭示正则化与概率先验的深刻联系，并深入探讨 PnP 与 RED 的算法机制、收敛性条件以及与现代去噪器（尤其是[神经网](@entry_id:276355)络）结合时的关键考量。接下来，在“**应用与[交叉](@entry_id:147634)学科联系**”一章中，我们将展示这些方法如何在[计算成像](@entry_id:170703)、[网络科学](@entry_id:139925)、科学计算等多个领域大放异彩，解决实际问题。最后，通过“**动手实践**”部分提供的练习，读者将有机会亲手实现和分析这些算法，加深对关键概念的理解。

## 原理与机制

本章深入探讨了即插即用先验（Plug-and-Play, PnP）与通过去噪进行正则化（Regularization by Denoising, RED）这两种现代[计算成像](@entry_id:170703)与逆问题求解方法的核心科学原理。我们将从[贝叶斯推断](@entry_id:146958)的基础出发，揭示正则化优化与概率模型之间的深刻联系，进而详细阐述 PnP 和 RED 的算法机制、理论保证以及在实际应用（特别是使用深度神经网络）中的关键考量。

### [贝叶斯推断](@entry_id:146958)：正则化优化的概率视角

许多科学与工程领域的[逆问题](@entry_id:143129)都可以抽象为一个线性模型：

$y = A x + w$

其中，$x \in \mathbb{R}^n$ 是我们希望恢复的未知信号或图像；$y \in \mathbb{R}^m$ 是我们获得的观测数据；$A \in \mathbb{R}^{m \times n}$ 是一个描述测量过程的已知线性算子（或称传感矩阵）；$w \in \mathbb{R}^m$ 则代表测量过程中不可避免的噪声。

一个强大而系统的求解这类问题的方法是贝叶斯推断。在该框架下，我们不将 $x$ 视为一个固定的未知量，而是将其看作一个随机向量，其固有的统计特性由**[先验概率](@entry_id:275634)[分布](@entry_id:182848)** $p(x)$ 描述。这个[先验分布](@entry_id:141376) $p(x)$ 封装了我们对于“自然”或“有意义”的信号 $x$ 应具有何种结构的信念。例如，我们可能相信自然图像在某些变换域（如[小波](@entry_id:636492)域）中是稀疏的，或者其梯度[分布](@entry_id:182848)是[重尾](@entry_id:274276)的。

噪声 $w$ 也被建模为随机向量，其[分布](@entry_id:182848)描述了[测量误差](@entry_id:270998)的特性。一个常见且在数学上易于处理的模型是[加性高斯白噪声](@entry_id:269320)，即 $w \sim \mathcal{N}(0, \sigma_w^2 I)$，其中 $I$ 是单位矩阵。根据线性模型，给定一个确定的信号 $x$，观测 $y$ 的[条件概率分布](@entry_id:163069)（即**[似然函数](@entry_id:141927)**）为 $p(y|x) \sim \mathcal{N}(Ax, \sigma_w^2 I)$。其概率密度函数为：

$p(y|x) = \frac{1}{(2\pi \sigma_w^2)^{m/2}} \exp\left(-\frac{1}{2\sigma_w^2} \|y - Ax\|_2^2\right)$

[贝叶斯推断](@entry_id:146958)的核心目标是利用观测数据 $y$ 来更新我们对 $x$ 的认识，即计算**[后验概率](@entry_id:153467)[分布](@entry_id:182848)** $p(x|y)$。根据贝叶斯定理，[后验分布](@entry_id:145605)正比于似然与先验的乘积：

$p(x|y) \propto p(y|x) p(x)$

在获得[后验分布](@entry_id:145605)后，我们可以通过多种方式提取对 $x$ 的[点估计](@entry_id:174544)。其中最流行的一种是**[最大后验概率](@entry_id:268939)（Maximum a Posteriori, MAP）**估计，它寻找[后验分布](@entry_id:145605)中概率密度最高的点，即：

$\hat{x}_{\text{MAP}} = \arg \max_{x} p(x|y)$

为了简化计算，我们通常转而最小化负对数[后验概率](@entry_id:153467) $-\ln p(x|y)$。结合似然和先验，这一问题变为：

$\hat{x}_{\text{MAP}} = \arg \min_{x} \{ -\ln p(y|x) - \ln p(x) \}$

对于[高斯噪声](@entry_id:260752)模型，[负对数似然](@entry_id:637801)项（忽略与 $x$ 无关的常数）为 $\frac{1}{2\sigma_w^2} \|y - Ax\|_2^2$。这是一个**数据保真项**，它惩罚与观测数据 $y$ 不符的解。

对于先验分布，在信号处理和机器学习中，常采用吉布斯[分布](@entry_id:182848)（Gibbs distribution）形式：$p(x) \propto \exp(-\lambda \phi(x))$，其中 $\phi(x)$ 是一个**正则化函数**（或称能量函数），它对“不希望”出现的信号结构赋予较高的值；$\lambda > 0$ 是一个控制先验强度（或正则化强度）的参数。这种形式的负对数先验就是 $\lambda \phi(x)$。

综上所述，MAP 估计问题最终转化为一个正则化[优化问题](@entry_id:266749) ：

$\hat{x}_{\text{MAP}} = \arg \min_{x} \left\{ \frac{1}{2\sigma_w^2} \|y - Ax\|_2^2 + \lambda \phi(x) \right\}$

这个目标函数清晰地体现了数据保真与先验知识之间的权衡。整个[目标函数](@entry_id:267263)可以乘以一个正常数而不改变其最小化解的位置。例如，乘以 $2\sigma_w^2$，等价的[优化问题](@entry_id:266749)变为：

$\hat{x}_{\text{MAP}} = \arg \min_{x} \left\{ \|y - Ax\|_2^2 + (2\lambda \sigma_w^2) \phi(x) \right\}$

这表明，最终的解取决于数据保真项和正则化项的相对权重，这个相对权重由有效[正则化参数](@entry_id:162917) $\lambda' = 2\lambda\sigma_w^2$ 决定。噪声水平较高（$\sigma_w^2$ 增大）或我们对先验的信念更强（$\lambda$ 增大），都会导致更强的正则化，从而使解更偏向于满足先验所描述的结构。 

### 即插即用（PnP）[范式](@entry_id:161181)：解耦数据与先验

形如 $\min_x \{f(x) + g(x)\}$ 的[复合优化](@entry_id:165215)问题在现代信号处理中无处不在，其中 $f(x)$ 通常是数据保真项，而 $g(x)$ 是正则化项。直接最小化这个和函数可能很困难，特别是当 $g(x)$ 是非光滑的（例如 $\ell_1$ 范数）或者形式复杂时。

诸如**交替方向乘子法（Alternating Direction Method of Multipliers, [ADMM](@entry_id:163024)）**之类的[算子分裂](@entry_id:634210)算法为此类问题提供了强大的求解框架。ADMM 的核心思想是引入一个辅助变量 $v$，将问题等价地重写为：

$\min_{x,v} f(x) + g(v) \quad \text{subject to} \quad x = v$

然后，通过最小化其增广[拉格朗日函数](@entry_id:174593)，[ADMM](@entry_id:163024) 将原[问题分解](@entry_id:272624)为一系列更易于处理的子问题。使用缩放形式的[对偶变量](@entry_id:143282) $u$，其迭代格式如下 ：

1.  **$x$-更新**：$x^{k+1} = \arg\min_x \left( f(x) + \frac{\rho}{2} \|x - v^k + u^k\|_2^2 \right)$
2.  **$v$-更新**：$v^{k+1} = \arg\min_v \left( g(v) + \frac{\rho}{2} \|x^{k+1} - v + u^k\|_2^2 \right)$
3.  **$u$-更新**：$u^{k+1} = u^k + x^{k+1} - v^{k+1}$

其中 $\rho > 0$ 是一个罚参数。

让我们仔细审视这些更新步骤。$x$-更新仅涉及数据保真项 $f(x)$。对于我们的[线性逆问题](@entry_id:751313)，$f(x) = \frac{1}{2}\|y-Ax\|_2^2$（为方便起见，我们暂时忽略了 $1/\sigma_w^2$ 缩放），这是一个二次函数。$x$-更新步骤的[目标函数](@entry_id:267263)也是二次的，其解可以通过设置梯度为零得到一个闭式解（或至少是一个易于求解的线性系统）：

$x^{k+1} = (A^\top A + \rho I)^{-1} (A^\top y + \rho(v^k - u^k))$

$v$-更新则仅涉及正则化项 $g(v)$。这个更新步骤可以被改写为：

$v^{k+1} = \arg\min_v \left( g(v) + \frac{\rho}{2} \|v - (x^{k+1} + u^k)\|_2^2 \right)$

这正是**邻近算子（proximal operator）**的定义。具体来说，$v^{k+1} = \text{prox}_{g/\rho}(x^{k+1} + u^k)$。邻近算子可以被看作是一种广义的投影，它对输入向量 $z = x^{k+1} + u^k$ 进行“去噪”，使其在满足正则化项 $g(v)$ 的同时，与 $z$ 保持接近。

**即插即用（PnP）**[范式](@entry_id:161181)的革命性思想正在于此：如果 $v$-更新本质上是一个[去噪](@entry_id:165626)步骤，我们何必局限于那些具有易于计算的邻近算子的简单正则化函数 $g(v)$ 呢？我们可以直接用一个先进的、通用的**[去噪](@entry_id:165626)器（denoiser）** $D_\sigma(\cdot)$ 来替换这个邻近算子步骤。这个去噪器可以是任何算法——例如 BM3D、[字典学习](@entry_id:748389)，甚至是训练好的[卷积神经网络](@entry_id:178973)（CNN）——只要它能有效地从含噪图像中恢复出干净图像。

于是，[PnP-ADMM](@entry_id:753534) 算法的 $v$-更新步骤就变成了 ：

$v^{k+1} = D_\sigma(x^{k+1} + u^k)$

其中 $\sigma$ 是去噪器对应的噪声水平参数，它通常与 [ADMM](@entry_id:163024) 的参数 $\rho$ 相关联（通常 $\sigma^2 \propto 1/\rho$）。这种“即插即用”的方式极大地扩展了我们可以在[逆问题](@entry_id:143129)中使用的先验模型的范围，使我们能够利用[图像去噪](@entry_id:750522)领域的最新进展，而无需为每个新的[去噪](@entry_id:165626)器推导一个明确的正则化函数 $\phi(x)$ 和对应的邻近算子。

### [可解释性](@entry_id:637759)问题：[去噪](@entry_id:165626)器何时是邻近算子？

PnP 的灵活性也带来了深刻的理论问题：当我们用一个任意的去噪器 $D$ 替换邻近算子时，整个迭代过程还在求解一个明确的 MAP [优化问题](@entry_id:266749)吗？答案是：仅在非常特殊的情况下成立。

[PnP-ADMM](@entry_id:753534) 算法收敛到某个 MAP 问题的解，当且仅当所使用的去噪器 $D$ 本身就是一个对应于某个正则化函数 $\phi$ 的邻近算子 。一个算子 $T: \mathbb{R}^n \to \mathbb{R}^n$ 是一个正常、下半连续、凸函数 $\phi$ 的邻近算子，即 $T = \text{prox}_\phi$，需要满足严格的数学条件。根据 Moreau 和 Rockafellar 的经典理论，一个算子是邻近算子的一个必要条件是它是**紧非扩张的（firmly nonexpansive）**。对于可微算子，这进一步意味着其雅可比矩阵必须是对称的，且所有[特征值](@entry_id:154894)都在 $[0, 1]$ 区间内 。

**紧非扩张**算子 $T$ 定义为满足以下不等式的算子：
$\|Tx - Ty\|_2^2 \le \langle Tx - Ty, x - y \rangle, \quad \forall x, y$

然而，大多数先进的[去噪](@entry_id:165626)器，无论是传统的还是基于深度学习的，通常都不满足这些苛刻的条件。例如，考虑一个由非对称卷积核实现的线性[去噪](@entry_id:165626)器 $D(x) = Kx$。由于其[矩阵表示](@entry_id:146025) $K$ 不是对称的，它的雅可比矩阵（就是 $K$ 本身）也不是对称的。因此，它不可能是任何凸函数的邻近算子 。

这意味着，当使用这样一个非对称的线性去噪器或一个典型的 CNN 去噪器时，[PnP-ADMM](@entry_id:753534) 算法的收敛点一般**不能**被解释为任何一个具有显式凸先验的 MAP 估计。在这种情况下，算法的[不动点](@entry_id:156394)被理解为一个**共识均衡（consensus equilibrium）**，即一个在数据保真约束和[去噪](@entry_id:165626)器施加的[隐式正则化](@entry_id:187599)之间达到平衡的点，但这个[平衡点](@entry_id:272705)背后并没有一个统一的能量函数可以最小化 。

### 通过去噪进行正则化（RED）：构建显式正则化项

与 PnP 试图保持算法结构、替换其中一个模块的思路不同，**通过去噪进行正则化（Regularization by Denoising, RED）** 提出了一种更具建设性的方法：直接从一个给定的去噪器 $D$ 出发，来定义一个显式的正则化项 $R(x)$。

#### 基于梯度的正则化

RED 的一个核心思想是，正则化项的**梯度** $\nabla R(x)$ 应该将信号 $x$ 推向更符合先验的方向。一个自然的定义是，这个梯度应该与**[去噪](@entry_id:165626)残差** $x - D(x)$ 成正比。我们不妨设：

$\nabla R(x) = x - D(x)$

这个定义非常直观：在优化过程中，梯度下降步骤 $x \leftarrow x - \gamma \nabla R(x)$ 会将 $x$ 向其[去噪](@entry_id:165626)版本 $D(x)$ 移动。然而，一个关键的数学问题是：一个任意的向量场（这里是 $x - D(x)$）是否总能成为某个标量函数 $R(x)$ 的梯度？

根据向量微积分中的[庞加莱引理](@entry_id:160150)，在 $\mathbb{R}^n$ 这样的单连通域上，一个可微向量场是某个[标量势](@entry_id:276177)函数梯度的充要条件是该向量场是**无旋的（irrotational）**，即其[雅可比矩阵](@entry_id:264467)是对称的。设向量场为 $f(x) = x - D(x)$，其[雅可比矩阵](@entry_id:264467)为 $J_f(x) = I - J_D(x)$。$J_f(x)$ 对称的条件等价于去噪器 $D(x)$ 的[雅可比矩阵](@entry_id:264467) $J_D(x)$ 是对称的 。

这个**可积性（integrability）**或**雅可比对称**条件，是 RED 梯度方法有效性的核心。虽然对于任意[去噪](@entry_id:165626)器（如标准 CNN）通常不成立，但它启发了一种有原则地设计去噪器的方法：我们可以从一个标量势函数 $s(x)$ 出发，定义去噪器为 $D(x) = x - \nabla s(x)$。根据定义，[去噪](@entry_id:165626)残差 $x - D(x) = \nabla s(x)$ 自然是一个梯度场，并且该去噪器的雅可比矩阵 $J_D(x) = I - \nabla^2 s(x)$ 也必然是对称的（因为海森矩阵 $\nabla^2 s(x)$ 是对称的） 。

#### 显式能量函数

RED 框架还提出了一个显式的能量函数：

$g_{\text{RED}}(x) = \frac{1}{2} x^\top (x - D(x))$

这个二次形式的表达式看起来非常简洁，但它的梯度是否就是我们所期望的[去噪](@entry_id:165626)残差 $x-D(x)$ 呢？通过直接求导，我们可以得到其梯度的一般表达式 ：

$\nabla g_{\text{RED}}(x) = x - \frac{1}{2} (D(x) + J_D(x)^\top x)$

显然，这个梯度通常不等于 $x - D(x)$。只有在满足特定条件时，两者才会相等。这些条件是：$J_D(x)$ 是对称的，并且 $D(x)$ 是 1 次齐次的（即 $D(x) = J_D(x)x$）。这在实践中是相当强的限制 。因此，在使用 RED 框架时，必须仔细区分是采用梯度形式的正则化 $\nabla R(x) = x-D(x)$（需要[雅可比](@entry_id:264467)对称），还是采用能量形式的正则化 $g_{\text{RED}}(x)$（其梯度有更复杂的形式）。

### 去噪器的理论基础与收敛性

PnP 和 RED 算法的性能和收敛性严重依赖于所用去噪器的数学性质。[算子理论](@entry_id:139990)为此提供了坚实的分析工具。

#### 算子性质及其对收敛的影响

在分析迭代算法时，以下几类算子性质至关重要 ：

-   **收缩（Contractive）**：算子 $T$ 是收缩的，如果存在常数 $q \in [0, 1)$，使得 $\|Tx - Ty\| \le q \|x-y\|$。根据[巴拿赫不动点定理](@entry_id:146620)，若 PnP 算法的迭代算子是收缩的，则算法将以线性速率[全局收敛](@entry_id:635436)到唯一的[不动点](@entry_id:156394)。
-   **非扩张（Nonexpansive）**：算子 $T$ 是非扩张的，如果其[利普希茨常数](@entry_id:146583)为 1，即 $\|Tx - Ty\| \le \|x-y\|$。这是保证许多迭代算法（如基于 Krasnosel'skii-Mann 迭代的 PnP 方案）收敛的一个关键条件，但通常只能保证[次线性收敛速率](@entry_id:755607)。
-   **平均（Averaged）**：算子 $T$ 是 $\alpha$-平均的（$\alpha \in (0,1)$），如果它可以写成 $T = (1-\alpha)I + \alpha S$ 的形式，其中 $S$ 是一个非扩张算子。[平均算子](@entry_id:746605)是非扩张算子的一类重要[子集](@entry_id:261956)，同样能保证收敛性。邻近[梯度算子](@entry_id:275922)就是一个典型的[平均算子](@entry_id:746605)。
-   **紧非扩张（Firmly Nonexpansive）**：如前所述，这类算子满足 $\|Tx-Ty\|^2 \le \langle Tx-Ty, x-y \rangle$。它们是 1/2-[平均算子](@entry_id:746605)，因此也是非扩张的。任何凸函数的邻近算子都是紧非扩张的。
-   **余强制（Cocoercive）**：算子 $B$ 是 $\beta$-余强制的（$\beta>0$），如果 $\langle Bx-By, x-y \rangle \ge \beta \|Bx-By\|^2$。一个重要的例子是，一个具有 $L$-利普希茨连续梯度的[凸函数](@entry_id:143075)的梯度是 $1/L$-余强制的。

在 PnP 框架中，如果[去噪](@entry_id:165626)器 $D$ 是非扩张的，那么在合适的参数设置下，整个 [PnP-ADMM](@entry_id:753534) 的迭代算子通常可以被证明是平均的，从而保证算法的收敛性。

#### MMSE [去噪](@entry_id:165626)器：一个理论基石

一个在理论上极为重要的去噪器是**最小均方误差（Minimum Mean Squared Error, MMSE）**[去噪](@entry_id:165626)器。对于观测模型 $Z = X + N$（其中 $X$ 是具有先验 $p_X$ 的信号，N 是[高斯噪声](@entry_id:260752) $N \sim \mathcal{N}(0, \sigma^2 I)$），MMSE 去噪器定义为[后验均值](@entry_id:173826)：

$D_\sigma(z) = \mathbb{E}[X | Z=z]$

MMSE 去噪器拥有非凡的性质。其中最著名的是 **Tweedie 公式** ：

$D_\sigma(z) = z + \sigma^2 \nabla_z \ln p_Z(z)$

其中 $p_Z(z)$ 是观测数据 $Z$ 的边缘概率密度。这个公式揭示了一个惊人的联系：[去噪](@entry_id:165626)残差 $z - D_\sigma(z) = -\sigma^2 \nabla_z \ln p_Z(z)$。这意味着对于 MMSE 去噪器，其去噪残差**总是**一个[保守场](@entry_id:137555)（一个标量势函数的梯度），其[势函数](@entry_id:176105)为 $-\sigma^2 \ln p_Z(z)$。因此，MMSE [去噪](@entry_id:165626)器自然满足 RED 梯度方法的可积性条件，其雅可比矩阵总是对称的  。

MMSE [去噪](@entry_id:165626)器的利普希茨性质则与先验分布 $p_X$ 的对数[凹性](@entry_id:139843)密切相关 ：
-   如果先验 $p_X$ 是**对数凹**的，则 MMSE 去噪器 $D_\sigma$ 是**非扩张**的。
-   如果先验 $p_X$ 是**强对数凹**的，则 MMSE 去噪器 $D_\sigma$ 是一个**收缩**算子。

然而，对于非对数凹的先验（例如，描述多模态[分布](@entry_id:182848)的先验），MMSE 去噪器可能是扩张的（[利普希茨常数](@entry_id:146583)大于1），此时在 PnP 算法中直接使用它可能会导致发散。

### 现代[神经网](@entry_id:276355)络去噪器的实用考量

在实践中，最强大的去噪器通常是基于[卷积神经网络](@entry_id:178973)（CNN）的。然而，一个标准的 CNN 架构并不能保证其作为算子是稳定的（例如，非扩张的）。一个 CNN 是多层算子的复合，其全局[利普希茨常数](@entry_id:146583)是各层[利普希茨常数](@entry_id:146583)的乘积的上界。为了确保整个网络非扩张，我们需要仔细控制每一层的性质 。

-   **卷积层**：其[利普希茨常数](@entry_id:146583)是其[谱范数](@entry_id:143091)。可以通过**[谱归一化](@entry_id:637347)**等技术来约束它，例如通过[幂迭代](@entry_id:141327)来估计并归一化最大奇异值。对于使用循环填充的卷积，其[谱范数](@entry_id:143091)可以通过[快速傅里叶变换](@entry_id:143432)（FFT）精确计算和控制 。
-   **激活函数**：常用的[激活函数](@entry_id:141784)，如 ReLU 和 [Leaky ReLU](@entry_id:634000)，其[利普希茨常数](@entry_id:146583)是 1 或略大于 1。
-   **[归一化层](@entry_id:636850)**：这是最棘手的部分。**[批量归一化](@entry_id:634986)（Batch Normalization）**在推理时是一个[仿射变换](@entry_id:144885)，其缩放因子可能导致层的[利普希茨常数](@entry_id:146583)大于 1。**[实例归一化](@entry_id:638027)（Instance Normalization）**或**[组归一化](@entry_id:634207)（Group Normalization）**由于其归一化统计量依赖于输入，是[非线性](@entry_id:637147)操作，同样不保证非扩[张性](@entry_id:141857)。
-   **[残差连接](@entry_id:637548)**：标准的[残差连接](@entry_id:637548)（$F(x) = x + G(x)$）会破坏非扩[张性](@entry_id:141857)，因为其[利普希茨常数](@entry_id:146583)[上界](@entry_id:274738)为 $1+L_G$。

因此，为了在 PnP/RED 算法中稳定地使用 CNN [去噪](@entry_id:165626)器，需要采用特殊设计的网络架构，例如，通过[谱归一化](@entry_id:637347)约束所有卷积层，并避免使用标准的[批量归一化](@entry_id:634986)和[残差连接](@entry_id:637548)，或者对它们进行特殊处理。即使一个 CNN 被设计为非扩张的，它的雅可比矩阵通常也不是对称的，这意味着它仍然不能被解释为 MAP 优化中的邻近算子，而 PnP 算法的收敛点应被理解为共识均衡点。