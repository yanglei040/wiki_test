{
    "hands_on_practices": [
        {
            "introduction": "The first step in solving an inverse problem is to translate our physical and statistical assumptions into a mathematical objective function. A critical component of this is the data-fidelity term, which quantifies how well a potential solution explains the observed measurements. This exercise  provides a foundational practice in deriving this term from first principles, showing how different statistical models for measurement noise—specifically Gaussian and Laplace distributions—lead directly to the widely used squared $L_2$-norm and the $L_1$-norm objectives, respectively.",
            "id": "3442929",
            "problem": "Consider a linear inverse problem with a deep generative prior. Let the unknown signal be constrained to the range of a generator $G$, specifically $x = G(z) \\in \\mathbb{R}^{n}$ with a one-dimensional latent code $z \\in \\mathbb{R}$ and a linear generator $G(z) = b z$, where $b \\in \\mathbb{R}^{n}$ is fixed and known. Measurements are acquired via a known forward operator $A \\in \\mathbb{R}^{m \\times n}$, yielding $y = A x + \\varepsilon \\in \\mathbb{R}^{m}$, where $\\varepsilon$ models random measurement noise. Assume that the latent code $z$ is the only unknown (that is, there is no explicit prior distribution on $z$ beyond the constraint $x \\in \\mathrm{range}(G)$), and we seek the maximum likelihood estimator (MLE) of $z$ given observed $y$.\n\nStart from the fundamental definition of the likelihood $p(y \\mid x)$ under two different noise models and derive the corresponding negative log-likelihood (data-fidelity) terms as functions of $z$ (up to additive constants that do not depend on $z$), and the corresponding MLE problems in $z$:\n- Independent and identically distributed (i.i.d.) Gaussian noise: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$.\n- Independent and identically distributed (i.i.d.) Laplace noise with scale $\\beta$: each entry of $\\varepsilon$ has density $(2 \\beta)^{-1} \\exp(-|e|/\\beta)$.\n\nThen, using the specific numerical instance with $m = 4$ and $n = 3$,\n- $$A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  0 \\\\ 2  0  1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 5 \\\\ 7 \\\\ 9 \\\\ 6 \\end{pmatrix},$$\nperform the following:\n- Express the Gaussian-noise MLE of $z$ in closed form in terms of $A$, $b$, and $y$, and evaluate it numerically for the given $A$, $b$, and $y$.\n- Express the Laplace-noise MLE of $z$ as the solution of a univariate convex optimization problem and characterize it using subdifferential calculus. Then evaluate it explicitly for the given $A$, $b$, and $y$.\n\nFinally, compute the difference between the Gaussian-noise MLE and the Laplace-noise MLE,\n$$z_{\\mathrm{G}} - z_{\\mathrm{L}},$$\nand report this final quantity as a single simplified exact fraction. Do not round. No units are required.",
            "solution": "*   Start with the problem validation step. List givens. State it's a valid problem.\n*   Proceed to the solution.\n*   **General Derivation**:\n    *   Define the measurement model with the generative prior: $y = A(bz) + \\varepsilon = (Ab)z + \\varepsilon$.\n    *   Introduce $v = Ab$.\n    *   **Gaussian Case**: Derive the likelihood $p(y|z)$, the negative log-likelihood $\\mathcal{L}_G(z)$, and state the resulting optimization problem $\\min_z \\|y - vz\\|_2^2$.\n    *   **Laplace Case**: Derive the likelihood $p(y|z)$, the negative log-likelihood $\\mathcal{L}_L(z)$, and state the resulting optimization problem $\\min_z \\|y - vz\\|_1$.\n*   **Specific Solution**:\n    *   Compute the vector $v = Ab$ using the given numerical values for $A$ and $b$.\n    *   **Gaussian MLE, $z_G$**:\n        *   State the objective function $f_G(z) = \\|y - vz\\|_2^2$.\n        *   Find its minimum by setting the derivative to zero. Show the closed-form solution $z_G = \\frac{v^T y}{v^T v}$.\n        *   Calculate $v^T y$ and $v^T v$.\n        *   Calculate $z_G$.\n    *   **Laplace MLE, $z_L$**:\n        *   State the objective function $f_L(z) = \\|y - vz\\|_1$.\n        *   Explain that this is a univariate convex optimization problem, specifically a weighted median problem.\n        *   Characterize the solution using subdifferential calculus. The minimizer $z_L$ must satisfy $0 \\in \\partial f_L(z_L)$.\n        *   Write out the subdifferential: $\\partial f_L(z) = \\sum_{i=1}^m \\partial|y_i - v_i z| = \\sum_{i=1}^m -v_i \\cdot \\partial|z - y_i/v_i|$.\n        *   The optimality condition is $0 \\in \\sum_{i=1}^m -v_i \\text{sign}(y_i - v_i z_L)$.\n        *   Identify the \"knots\" $z_i^* = y_i / v_i$ and the weights $w_i = |v_i|$.\n        *   Find the weighted median of the knots $\\{z_i^*\\}$ with weights $\\{w_i\\}$. This gives $z_L$.\n        *   Confirm the solution by checking the subdifferential condition interval.\n*   **Final Calculation**:\n    *   Compute the difference $z_G - z_L$.\n    *   Show the arithmetic to get the final fraction.",
            "answer": "*   Boxed final answer `$$\\boxed{\\frac{11}{93}}$$`."
        },
        {
            "introduction": "Using a generative model as a prior is powerful, but it introduces a fundamental tradeoff. By constraining the solution to the generator's range, we reduce the influence of noise (decreasing variance), but we also risk introducing a systematic error if the true signal lies outside this range (increasing bias). This exercise  makes this abstract concept concrete by guiding you through a first-principles derivation of the mean squared error. You will explicitly calculate how the total error decomposes into a variance term, which depends on the measurement noise $\\sigma^2$ and model dimension $k$, and a bias term, which depends on the model mismatch variance $\\tau^2$.",
            "id": "3442952",
            "problem": "Consider a deep generative model prior represented by a deterministic mapping $G : \\mathbb{R}^{k} \\to \\mathbb{R}^{n}$ whose image is the set $S = \\mathrm{Range}(G) \\subset \\mathbb{R}^{n}$ of dimension $k$. You observe noisy measurements in the denoising setting given by $y = x^{\\star} + w$, where the unknown ground-truth signal $x^{\\star} \\in \\mathbb{R}^{n}$ and the noise $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ is a zero-mean Gaussian with covariance $\\sigma^{2} I_{n}$, independent of $x^{\\star}$. You constrain the reconstruction to $S$ by choosing the minimum-distance estimator $x^{\\wedge} = \\arg\\min_{x \\in S} \\|x - y\\|_{2}^{2}$, which equals the orthogonal projection $x^{\\wedge} = P_{S} y$ for the orthogonal projector $P_{S}$ onto $S$.\n\nAssume the true signals are not exactly in $S$ and admit the decomposition $x^{\\star} = G(z^{\\star}) + x_{\\perp}$ with $G(z^{\\star}) \\in S$ and $x_{\\perp} \\in S^{\\perp}$, where $x_{\\perp}$ is independent of $w$ and has distribution $x_{\\perp} \\sim \\mathcal{N}(0, \\tau^{2} I_{n-k})$ supported on the $n-k$ dimensional orthogonal complement $S^{\\perp}$. Using only fundamental properties of orthogonal projections, Gaussian expectations, and the definition of mean squared error, derive from first principles how bias–variance tradeoffs arise when constraining to $S$, and compute the expected squared reconstruction error $\\mathbb{E}\\big[\\|x^{\\wedge} - x^{\\star}\\|_{2}^{2}\\big]$ under the joint randomness of $w$ and $x_{\\perp}$.\n\nProvide your final answer as a single closed-form analytic expression in terms of $n$, $k$, $\\sigma$, and $\\tau$. No rounding is required.",
            "solution": "The problem is subjected to validation and is deemed valid. We now proceed with a formal derivation of the expected squared reconstruction error.\n\nOur goal is to compute the mean squared error (MSE), which is defined as $\\mathbb{E}\\big[\\|x^{\\wedge} - x^{\\star}\\|_{2}^{2}\\big]$. The expectation is taken over the joint distribution of the noise $w$ and the out-of-subspace component $x_{\\perp}$.\n\nThe estimator $x^{\\wedge}$ is the orthogonal projection of the measurement $y$ onto the signal subspace $S$:\n$$x^{\\wedge} = P_{S} y$$\nWe are given the measurement model $y = x^{\\star} + w$. Substituting this into the expression for $x^{\\wedge}$ gives:\n$$x^{\\wedge} = P_{S}(x^{\\star} + w)$$\nThe true signal $x^{\\star}$ is decomposed into a component in $S$ and a component in its orthogonal complement $S^{\\perp}$: $x^{\\star} = x_{\\parallel} + x_{\\perp}$, where for notational clarity we define $x_{\\parallel} = G(z^{\\star}) \\in S$ and $x_{\\perp} \\in S^{\\perp}$. Substituting this decomposition yields:\n$$x^{\\wedge} = P_{S}(x_{\\parallel} + x_{\\perp} + w)$$\nBy the linearity of the projection operator $P_{S}$, we have:\n$$x^{\\wedge} = P_{S}x_{\\parallel} + P_{S}x_{\\perp} + P_{S}w$$\nWe now use the fundamental properties of orthogonal projections. Since $x_{\\parallel}$ is in $S$, its projection onto $S$ is itself, $P_{S}x_{\\parallel} = x_{\\parallel}$. Since $x_{\\perp}$ is in the orthogonal complement $S^{\\perp}$, its projection onto $S$ is the zero vector, $P_{S}x_{\\perp} = 0$. This simplifies the expression for the estimator to:\n$$x^{\\wedge} = x_{\\parallel} + P_{S}w$$\nNow we form the reconstruction error vector, $x^{\\wedge} - x^{\\star}$:\n$$x^{\\wedge} - x^{\\star} = (x_{\\parallel} + P_{S}w) - (x_{\\parallel} + x_{\\perp}) = P_{S}w - x_{\\perp}$$\nThe squared reconstruction error is the squared Euclidean norm of this vector:\n$$\\|x^{\\wedge} - x^{\\star}\\|_{2}^{2} = \\|P_{S}w - x_{\\perp}\\|_{2}^{2}$$\nThe vector $P_{S}w$ lies in the subspace $S$, by definition of the projection operator. The vector $x_{\\perp}$ lies in the orthogonal complement $S^{\\perp}$. Therefore, the vectors $P_{S}w$ and $-x_{\\perp}$ are orthogonal. By the Pythagorean theorem, the squared norm of their sum (or difference) is the sum of their squared norms:\n$$\\|x^{\\wedge} - x^{\\star}\\|_{2}^{2} = \\|P_{S}w\\|_{2}^{2} + \\|-x_{\\perp}\\|_{2}^{2} = \\|P_{S}w\\|_{2}^{2} + \\|x_{\\perp}\\|_{2}^{2}$$\nTo find the expected squared error, we take the expectation of this expression. By the linearity of expectation:\n$$\\mathbb{E}\\big[\\|x^{\\wedge} - x^{\\star}\\|_{2}^{2}\\big] = \\mathbb{E}\\big[\\|P_{S}w\\|_{2}^{2}\\big] + \\mathbb{E}\\big[\\|x_{\\perp}\\|_{2}^{2}\\big]$$\nThese two terms correspond to the variance and the squared bias of the estimator, respectively. We compute each term separately.\n\nFirst term (Variance): $\\mathbb{E}\\big[\\|P_{S}w\\|_{2}^{2}\\big]$.\nLet $\\{u_1, \\dots, u_k\\}$ be an orthonormal basis for the $k$-dimensional subspace $S$. The projection of $w$ onto $S$ can be written as $P_{S}w = \\sum_{i=1}^{k} \\langle w, u_i \\rangle u_i$. The squared norm is then:\n$$\\|P_{S}w\\|_{2}^{2} = \\sum_{i=1}^{k} \\langle w, u_i \\rangle^2$$\nThe expectation is $\\mathbb{E}\\big[\\|P_{S}w\\|_{2}^{2}\\big] = \\sum_{i=1}^{k} \\mathbb{E}\\big[\\langle w, u_i \\rangle^2\\big]$. The noise vector $w$ is distributed as $\\mathcal{N}(0, \\sigma^{2}I_n)$. The random variable $\\langle w, u_i \\rangle$ is a linear transformation of a Gaussian vector, and is thus Gaussian. Its mean is $\\mathbb{E}[\\langle w, u_i \\rangle] = \\langle \\mathbb{E}[w], u_i \\rangle = \\langle 0, u_i \\rangle = 0$. Its variance is $\\mathrm{Var}(\\langle w, u_i \\rangle) = u_i^T \\mathrm{Cov}(w) u_i = u_i^T (\\sigma^2 I_n) u_i = \\sigma^2 u_i^T u_i = \\sigma^2 \\|u_i\\|_2^2 = \\sigma^2$. For a zero-mean random variable, the expectation of its square is equal to its variance. Thus, $\\mathbb{E}[\\langle w, u_i \\rangle^2] = \\sigma^2$.\nSumming over the $k$ basis vectors, we get:\n$$\\mathbb{E}\\big[\\|P_{S}w\\|_{2}^{2}\\big] = \\sum_{i=1}^{k} \\sigma^2 = k \\sigma^2$$\nThis term represents the contribution to the error from the measurement noise $w$. A higher-dimensional signal model (larger $k$) is more flexible and \"fits\" more of the noise, leading to higher variance in the reconstruction.\n\nSecond term (Squared Bias): $\\mathbb{E}\\big[\\|x_{\\perp}\\|_{2}^{2}\\big]$.\nThis term represents the systematic error due to the model's inability to represent the component of the true signal that lies outside of the subspace $S$. We are given that $x_{\\perp}$ is distributed as a Gaussian on the $(n-k)$-dimensional subspace $S^{\\perp}$, specified as $x_{\\perp} \\sim \\mathcal{N}(0, \\tau^{2}I_{n-k})$.\nLet $\\{v_1, \\dots, v_{n-k}\\}$ be an orthonormal basis for $S^{\\perp}$. We can write $x_{\\perp} = \\sum_{j=1}^{n-k} c_j v_j$, where the coefficients $c = (c_1, \\dots, c_{n-k})^T$ are distributed as $\\mathcal{N}(0, \\tau^2 I_{n-k})$. The squared norm is:\n$$\\|x_{\\perp}\\|_{2}^{2} = \\sum_{j=1}^{n-k} c_j^2$$\nThe expectation is $\\mathbb{E}\\big[\\|x_{\\perp}\\|_{2}^{2}\\big] = \\sum_{j=1}^{n-k} \\mathbb{E}[c_j^2]$. Each coefficient $c_j$ has a $\\mathcal{N}(0, \\tau^2)$ distribution. As before, $\\mathbb{E}[c_j^2] = \\mathrm{Var}(c_j) + (\\mathbb{E}[c_j])^2 = \\tau^2 + 0^2 = \\tau^2$.\nSumming over the $n-k$ basis vectors, we get:\n$$\\mathbb{E}\\big[\\|x_{\\perp}\\|_{2}^{2}\\big] = \\sum_{j=1}^{n-k} \\tau^2 = (n-k)\\tau^2$$\nThis term represents the error from model mismatch. A higher-dimensional model (larger $k$) leaves a lower-dimensional orthogonal complement, reducing this bias term.\n\nCombining the two terms, the total expected squared reconstruction error is:\n$$\\mathbb{E}\\big[\\|x^{\\wedge} - x^{\\star}\\|_{2}^{2}\\big] = k \\sigma^2 + (n-k)\\tau^2$$\nThis expression clearly illustrates the bias-variance tradeoff. Increasing the model complexity, represented by the dimension $k$, increases the variance term $k \\sigma^2$ while decreasing the bias term $(n-k)\\tau^2$. The optimal model complexity would depend on the relative magnitudes of the noise variance $\\sigma^2$ and the model-mismatch variance $\\tau^2$.",
            "answer": "$$\n\\boxed{k \\sigma^{2} + (n-k) \\tau^{2}}\n$$"
        },
        {
            "introduction": "Formulating the correct objective function is only half the battle; we also need to solve the resulting optimization problem efficiently. This can be challenging when the problem is poorly conditioned, leading to slow convergence for standard gradient-based methods. This final practice  delves into a powerful technique for accelerating optimization: preconditioning. You will analyze how incorporating curvature information from both the forward model and the latent-space prior into a preconditioner reshapes the optimization landscape, and you will quantify the resulting speed-up by directly comparing the convergence rates of the unpreconditioned and preconditioned systems.",
            "id": "3442832",
            "problem": "Consider a linear inverse problem with a deep generative prior linearized in the latent space. Let the latent variable be $z \\in \\mathbb{R}^{2}$ and the generator be locally linear with Jacobian $J_{G} = W \\in \\mathbb{R}^{2 \\times 2}$. Measurements follow $y \\approx A x$ with $x = G(z)$ and $A \\in \\mathbb{R}^{2 \\times 2}$. Assume a Gaussian prior on $z$ with density $p(z) \\propto \\exp\\!\\left(-\\tfrac{1}{2} z^{\\top} \\Sigma^{-1} z\\right)$, so that $-\\log p(z) = \\tfrac{1}{2} z^{\\top} \\Sigma^{-1} z + \\text{const}$ and $\\nabla^{2}(-\\log p) = \\Sigma^{-1}$. The Maximum A Posteriori (MAP) objective in latent space is\n$$\nf(z) = \\tfrac{1}{2} \\|A G(z) - y\\|_{2}^{2} + \\tfrac{\\lambda}{2} z^{\\top} \\Sigma^{-1} z,\n$$\nwith $\\lambda  0$ and, under the local linearization $G(z) \\approx W z$, has (Gauss–Newton) curvature\n$$\nB \\;=\\; J_{G}^{\\top} A^{\\top} A J_{G} \\;+\\; \\lambda \\,\\nabla^{2}(-\\log p) \\;=\\; W^{\\top} A^{\\top} A W \\;+\\; \\lambda \\,\\Sigma^{-1}.\n$$\nWe consider first-order optimization in $z$ and a curvature-aware preconditioner given by a metric $P \\succ 0$ applied as a right-preconditioned gradient step $\\Delta z = - \\alpha \\, P^{-1} \\nabla f(z)$.\n\nAssume the following specific, numerically consistent setting:\n- $W = \\mathrm{diag}(2,\\,1)$,\n- $A = \\mathrm{diag}(3,\\,1)$,\n- $\\Sigma = \\mathrm{diag}\\!\\left(\\tfrac{1}{4},\\,1\\right)$, hence $\\Sigma^{-1} = \\mathrm{diag}(4,\\,1)$,\n- $\\lambda = 1$.\n\n1. Using the foundations of linear least squares, Gaussian priors, and Gauss–Newton curvature, form $B$ explicitly and compute its spectral condition number $\\kappa_{0} = \\lambda_{\\max}(B)/\\lambda_{\\min}(B)$.\n2. Using the prior-preconditioner $P = \\lambda \\,\\Sigma^{-1}$, analyze the preconditioned quadratic model and determine the effective curvature. Compute the corresponding condition number $\\kappa_{1}$ of the effective curvature.\n3. Using the standard strongly convex quadratic convergence bound for gradient descent with the minimax-optimal constant step size $\\alpha^{\\star} = \\tfrac{2}{L + \\mu}$, where $L$ and $\\mu$ are the largest and smallest eigenvalues of the curvature respectively, the worst-case linear contraction factor is $q = \\tfrac{\\kappa - 1}{\\kappa + 1}$. Let $q_{0}$ be the contraction factor without preconditioning (using $\\kappa_{0}$) and $q_{1}$ be that with prior-preconditioning (using $\\kappa_{1}$). Compute the exact ratio\n$$\nr \\;=\\; \\frac{q_{0}}{q_{1}}.\n$$\n\nProvide $r$ as a single simplified exact rational number. No rounding is required. There are no physical units to report.",
            "solution": "The problem asks for the ratio of convergence rates for a specific latent-space optimization problem, comparing an unpreconditioned approach to one using a prior-based preconditioner. The solution proceeds in three steps as outlined in the problem statement.\n\nFirst, we define the matrices and parameters provided in the problem.\nThe generator Jacobian is $W = \\mathrm{diag}(2, 1)$, which can be written as:\n$$\nW = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe measurement matrix is $A = \\mathrm{diag}(3, 1)$:\n$$\nA = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe inverse covariance matrix of the Gaussian prior on the latent variable $z$ is $\\Sigma^{-1} = \\mathrm{diag}(4, 1)$:\n$$\n\\Sigma^{-1} = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe regularization parameter is $\\lambda = 1$.\n\n**1. Unpreconditioned Curvature and Condition Number**\n\nThe (Gauss–Newton) curvature matrix $B$ is given by the formula:\n$$\nB = W^{\\top} A^{\\top} A W + \\lambda \\Sigma^{-1}\n$$\nSince $W$ and $A$ are diagonal, they are symmetric, so $W^{\\top} = W$ and $A^{\\top} = A$.\nFirst, we compute the data-fidelity term $W^{\\top} A^{\\top} A W$:\n$$\nW^{\\top} A^{\\top} A W = W A A W = W A^2 W\n$$\n$$\nA^2 = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 3^2  0 \\\\ 0  1^2 \\end{pmatrix} = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nW A^2 W = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 9 \\cdot 2  0 \\\\ 0  1 \\cdot 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 36  0 \\\\ 0  1 \\end{pmatrix}\n$$\nNext, we compute the prior term $\\lambda \\Sigma^{-1}$:\n$$\n\\lambda \\Sigma^{-1} = 1 \\cdot \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n$$\nNow, we sum these two matrices to find $B$:\n$$\nB = \\begin{pmatrix} 36  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 36+4  0 \\\\ 0  1+1 \\end{pmatrix} = \\begin{pmatrix} 40  0 \\\\ 0  2 \\end{pmatrix}\n$$\nFor a diagonal matrix, the eigenvalues are the diagonal entries. Thus, the eigenvalues of $B$ are $\\lambda_{\\max}(B) = 40$ and $\\lambda_{\\min}(B) = 2$.\nThe spectral condition number $\\kappa_0$ is the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa_0 = \\frac{\\lambda_{\\max}(B)}{\\lambda_{\\min}(B)} = \\frac{40}{2} = 20\n$$\n\n**2. Preconditioned Curvature and Condition Number**\n\nThe problem defines a prior-preconditioner $P = \\lambda \\Sigma^{-1}$. For the given values:\n$$\nP = 1 \\cdot \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe right-preconditioned gradient step involves the matrix $P^{-1} B$. This is the effective curvature of the preconditioned system. First, we find $P^{-1}$:\n$$\nP^{-1} = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}^{-1} = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ 0  1 \\end{pmatrix}\n$$\nNow we compute the effective curvature matrix, which we denote as $H_{\\text{eff}}$:\n$$\nH_{\\text{eff}} = P^{-1} B = \\begin{pmatrix} \\frac{1}{4}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 40  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\cdot 40  0 \\\\ 0  1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 10  0 \\\\ 0  2 \\end{pmatrix}\n$$\nThe eigenvalues of the effective curvature matrix $H_{\\text{eff}}$ are its diagonal entries, which are $10$ and $2$.\nLet $L$ and $\\mu$ be the largest and smallest eigenvalues of $H_{\\text{eff}}$, so $L = 10$ and $\\mu = 2$. The condition number of the preconditioned system, $\\kappa_1$, is:\n$$\n\\kappa_1 = \\frac{L}{\\mu} = \\frac{10}{2} = 5\n$$\n\n**3. Ratio of Contraction Factors**\n\nThe worst-case linear contraction factor $q$ for gradient descent on a strongly convex quadratic objective with condition number $\\kappa$ is given by:\n$$\nq = \\frac{\\kappa - 1}{\\kappa + 1}\n$$\nFor the unpreconditioned system, the contraction factor $q_0$ is based on $\\kappa_0 = 20$:\n$$\nq_0 = \\frac{\\kappa_0 - 1}{\\kappa_0 + 1} = \\frac{20 - 1}{20 + 1} = \\frac{19}{21}\n$$\nFor the prior-preconditioned system, the contraction factor $q_1$ is based on $\\kappa_1 = 5$:\n$$\nq_1 = \\frac{\\kappa_1 - 1}{\\kappa_1 + 1} = \\frac{5 - 1}{5 + 1} = \\frac{4}{6} = \\frac{2}{3}\n$$\nFinally, we compute the required ratio $r$:\n$$\nr = \\frac{q_0}{q_1} = \\frac{\\frac{19}{21}}{\\frac{2}{3}} = \\frac{19}{21} \\cdot \\frac{3}{2} = \\frac{19 \\cdot 3}{21 \\cdot 2}\n$$\nSince $21 = 7 \\cdot 3$, we can simplify the expression:\n$$\nr = \\frac{19 \\cdot 3}{7 \\cdot 3 \\cdot 2} = \\frac{19}{7 \\cdot 2} = \\frac{19}{14}\n$$\nThe final result is an exact rational number.",
            "answer": "$$\n\\boxed{\\frac{19}{14}}\n$$"
        }
    ]
}