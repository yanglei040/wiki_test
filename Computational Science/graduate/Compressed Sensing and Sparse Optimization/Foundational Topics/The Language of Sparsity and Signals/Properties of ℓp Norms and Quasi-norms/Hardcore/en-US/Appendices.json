{
    "hands_on_practices": [
        {
            "introduction": "The ideal measure of sparsity, the $\\ell_{0}$ pseudo-norm, is computationally intractable for optimization problems. This has led to the use of continuous surrogates, with the $\\ell_{p}$ quasi-norms for $p \\in (0,1]$ being prominent examples. This practice explores the fundamental connection between these penalties by asking you to derive the precise asymptotic relationship between $\\|x\\|_{p}^{p}$ and $\\|x\\|_{0}$ as $p$ approaches zero . Mastering this connection provides the theoretical justification for using $\\ell_{p}$ penalties and offers insight into how to properly scale regularization parameters to emulate true sparsity promotion.",
            "id": "3469680",
            "problem": "Let $x \\in \\mathbb{R}^{n}$ be fixed and let $p \\in (0,1]$. Define the $\\ell_{p}$ quasi-norm by $\\|x\\|_{p} = \\left(\\sum_{i=1}^{n} |x_{i}|^{p}\\right)^{1/p}$ and the $\\ell_{0}$ pseudo-norm by $\\|x\\|_{0} = \\#\\{i : x_{i} \\neq 0\\}$. Consider the following two tasks grounded in the basic definitions of $\\ell_{p}$ quasi-norms, the expansion $\\exp(z) = 1 + z + o(z)$ as $z \\to 0$, and the Maximum A Posteriori (MAP) formulation for Gaussian likelihoods:\n\n1) Starting from first principles, derive the exact first-order asymptotic expansion of $\\|x\\|_{p}^{p}$ as $p \\to 0^{+}$, in the form $\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\, S(x) + o(p)$, and determine the closed-form expression of the coefficient $S(x)$ in terms of $x$.\n\n2) Consider the MAP estimation problem with Gaussian likelihood and $\\ell_{p}$ penalty\n$$\nF_{p}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\lambda(p) \\, \\|u\\|_{p}^{p},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\sigma > 0$ are fixed and independent of $p$, and $\\lambda(p) > 0$ is a $p$-dependent regularization parameter. Let $\\gamma > 0$ be a fixed target sparsity penalty for the $\\ell_{0}$-regularized objective $\\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}$. Determine, in terms of limits with respect to $p \\to 0^{+}$, the necessary and sufficient asymptotic conditions on $\\lambda(p)$ so that $F_{p}(u)$ converges pointwise in $u$ to $\\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}$ up to $u$-independent additive terms. Express these conditions as the two limits $L_{1} = \\lim_{p \\to 0^{+}} \\lambda(p)$ and $L_{2} = \\lim_{p \\to 0^{+}} p \\, \\lambda(p)$.\n\nProvide your final answer as a single row vector containing, in order, the expression for $S(x)$, then $L_{1}$, then $L_{2}$. The final answer must be a closed-form analytic expression. No rounding is required and no units are involved.",
            "solution": "This problem consists of two related parts. The first part requires deriving the first-order asymptotic expansion of the $\\ell_p$ quasi-norm squared, $\\|x\\|_{p}^{p}$, as $p \\to 0^{+}$. The second part applies this result to a maximum a posteriori (MAP) estimation problem to find conditions for the convergence of an $\\ell_p$-regularized objective function to an $\\ell_0$-regularized one.\n\nPart 1: Asymptotic Expansion of $\\|x\\|_{p}^{p}$\n\nLet $x \\in \\mathbb{R}^{n}$ be a fixed vector. The term to be expanded is $\\|x\\|_{p}^{p}$, which is defined as:\n$$\n\\|x\\|_{p}^{p} = \\sum_{i=1}^{n} |x_{i}|^{p}\n$$\nWe are interested in the behavior of this expression as $p \\to 0^{+}$.\n\nLet us partition the set of indices $\\{1, 2, \\dots, n\\}$ into two disjoint sets:\n- $I_{0} = \\{i \\mid x_{i} = 0\\}$, the set of indices corresponding to zero components of $x$.\n- $I_{+} = \\{i \\mid x_{i} \\neq 0\\}$, the set of indices corresponding to non-zero components of $x$.\n\nThe sum can be split over these two sets:\n$$\n\\|x\\|_{p}^{p} = \\sum_{i \\in I_{0}} |x_{i}|^{p} + \\sum_{i \\in I_{+}} |x_{i}|^{p}\n$$\nFor any index $i \\in I_{0}$, we have $|x_{i}| = 0$. For any $p > 0$, $|x_{i}|^{p} = 0^{p} = 0$. Thus, the first sum is zero:\n$$\n\\sum_{i \\in I_{0}} |x_{i}|^{p} = 0\n$$\nThe problem reduces to analyzing the second sum over the non-zero components. The number of non-zero components is, by definition, the $\\ell_0$ pseudo-norm of $x$, denoted by $\\|x\\|_{0} = |I_{+}|$.\n\nFor each index $i \\in I_{+}$, we have $|x_{i}| > 0$. We can use the identity $a^b = \\exp(b \\ln a)$ for $a>0$. Let $a = |x_i|$ and $b = p$.\n$$\n|x_{i}|^{p} = \\exp(p \\ln|x_{i}|)\n$$\nThe problem provides the first-order Taylor expansion for the exponential function around $z=0$: $\\exp(z) = 1 + z + o(z)$. As $p \\to 0^{+}$, the argument of the exponential, $z = p \\ln|x_{i}|$, also approaches $0$ for any fixed $x_i \\neq 0$. Applying this expansion, we get:\n$$\n|x_{i}|^{p} = 1 + p \\ln|x_{i}| + o(p)\n$$\nNow, we can substitute this expansion back into the sum over $I_{+}$:\n$$\n\\sum_{i \\in I_{+}} |x_{i}|^{p} = \\sum_{i \\in I_{+}} \\left(1 + p \\ln|x_{i}| + o(p)\\right)\n$$\nBy the properties of summation, we can separate the terms:\n$$\n\\sum_{i \\in I_{+}} 1 + \\sum_{i \\in I_{+}} p \\ln|x_{i}| + \\sum_{i \\in I_{+}} o(p)\n$$\nLet's evaluate each term:\n1. The first term is the sum of $1$ over the set $I_{+}$. The number of elements in $I_{+}$ is $|I_{+}| = \\|x\\|_{0}$. So, $\\sum_{i \\in I_{+}} 1 = \\|x\\|_{0}$.\n2. The second term is $\\sum_{i \\in I_{+}} p \\ln|x_{i}| = p \\left(\\sum_{i \\in I_{+}} \\ln|x_{i}|\\right)$.\n3. The third term is a sum of $\\|x\\|_{0}$ terms, each of which is $o(p)$. Since $\\|x\\|_{0}$ is a finite constant (at most $n$), the sum is also $o(p)$: $\\sum_{i \\in I_{+}} o(p) = \\|x\\|_{0} \\cdot o(p) = o(p)$.\n\nCombining these results, we obtain the asymptotic expansion for $\\|x\\|_{p}^{p}$:\n$$\n\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\left(\\sum_{i \\in I_{+}} \\ln|x_{i}|\\right) + o(p)\n$$\nThis expression is in the required form $\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\, S(x) + o(p)$. By direct comparison, we identify the coefficient $S(x)$:\n$$\nS(x) = \\sum_{i \\in I_{+}} \\ln|x_{i}| = \\sum_{i: x_{i} \\neq 0} \\ln|x_{i}|\n$$\n\nPart 2: Asymptotic Conditions on $\\lambda(p)$\n\nWe are given the $p$-dependent objective function:\n$$\nF_{p}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\lambda(p) \\, \\|u\\|_{p}^{p}\n$$\nand the target $\\ell_0$-regularized objective function:\n$$\nF_{0}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}\n$$\nWe seek the necessary and sufficient conditions on $\\lambda(p)$ such that $F_{p}(u)$ converges pointwise in $u$ to $F_{0}(u)$ up to a $u$-independent additive term as $p \\to 0^{+}$. This means that for any fixed $u \\in \\mathbb{R}^n$, the limit\n$$\n\\lim_{p \\to 0^{+}} \\left( F_{p}(u) - F_{0}(u) \\right) = C\n$$\nmust exist and be a constant $C$ independent of $u$.\n\nLet's analyze the difference $F_{p}(u) - F_{0}(u)$:\n$$\nF_{p}(u) - F_{0}(u) = \\lambda(p) \\, \\|u\\|_{p}^{p} - \\gamma \\, \\|u\\|_{0}\n$$\nUsing the expansion for $\\|u\\|_{p}^{p}$ derived in Part 1 with $x=u$:\n$$\n\\|u\\|_{p}^{p} = \\|u\\|_{0} + p \\, S(u) + o(p) \\quad \\text{where} \\quad S(u) = \\sum_{i: u_{i} \\neq 0} \\ln|u_{i}|\n$$\nSubstituting this into the difference gives:\n$$\nF_{p}(u) - F_{0}(u) = \\lambda(p) \\left( \\|u\\|_{0} + p \\, S(u) + o(p) \\right) - \\gamma \\, \\|u\\|_{0}\n$$\n$$\n= (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p)\n$$\nFor the limit of this expression as $p \\to 0^{+}$ to be a constant $C$ independent of $u$, we examine its behavior for different choices of $u$.\n\nFirst, consider $u = 0$, the zero vector. In this case, $\\|u\\|_0 = 0$ and $S(u)$ is an empty sum, which is $0$. The expression becomes $0$, so its limit is $0$. Therefore, the constant $C$ must be $0$. The condition simplifies to:\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p) \\right) = 0 \\quad \\text{for all } u \\in \\mathbb{R}^n.\n$$\nThe last term, $\\lambda(p)o(p)$, can be written as $\\lambda(p) p \\frac{o(p)}{p}$. As we will show, $\\lim_{p\\to 0^+} \\lambda(p)$ must be finite, so $\\lambda(p)$ is bounded near $p=0$. As $\\lim_{p\\to 0^+} \\frac{o(p)}{p}=0$, the term $\\lambda(p) \\, o(p)$ approaches $0$ and can be ignored in the subsequent analysis of the necessary conditions.\n\nTo find the necessary conditions, we test with specific non-zero vectors $u$:\n1. Let $u$ be a vector with a single non-zero component, $u_k = 1$, and all other components zero. For this $u$, $\\|u\\|_{0} = 1$ and $S(u) = \\ln|1| = 0$. The limit condition becomes:\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\cdot 1 + p \\, \\lambda(p) \\cdot 0 \\right) = 0 \\implies \\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) = 0\n$$\nThis implies $\\lim_{p \\to 0^{+}} \\lambda(p) = \\gamma$. This is the condition for $L_1$. So, $L_1 = \\gamma$.\n\n2. Let $u$ be a vector with a single non-zero component, $u_k = c$, where $|c| \\neq 1$ (e.g., $c=e$). For this $u$, $\\|u\\|_{0} = 1$ and $S(u) = \\ln|c| \\neq 0$. The limit condition becomes:\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\cdot 1 + p \\, \\lambda(p) \\cdot \\ln|c| \\right) = 0\n$$\nSince we have already established from the first case that $\\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) = 0$, the condition simplifies to:\n$$\n\\lim_{p \\to 0^{+}} \\left( p \\, \\lambda(p) \\cdot \\ln|c| \\right) = 0 \\implies (\\ln|c|) \\lim_{p \\to 0^{+}} (p \\, \\lambda(p)) = 0\n$$\nSince we chose $c$ such that $\\ln|c| \\neq 0$, it is necessary that $\\lim_{p \\to 0^{+}} p \\, \\lambda(p) = 0$. This is the condition for $L_2$. So, $L_2 = 0$.\n\nThus, the necessary conditions are $L_{1} = \\lim_{p \\to 0^{+}} \\lambda(p) = \\gamma$ and $L_{2} = \\lim_{p \\to 0^{+}} p \\, \\lambda(p) = 0$.\n\nNow, we show these conditions are also sufficient. Assume $L_1 = \\gamma$ and $L_2 = 0$. Then for any fixed $u$:\n$$\n\\lim_{p \\to 0^{+}} (F_{p}(u) - F_{0}(u)) = \\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p) \\right)\n$$\nEvaluating the limit of each term:\n- $\\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) \\|u\\|_{0} = (\\lim_{p \\to 0^{+}} \\lambda(p) - \\gamma) \\|u\\|_{0} = (\\gamma - \\gamma) \\|u\\|_{0} = 0$.\n- $\\lim_{p \\to 0^{+}} p \\, \\lambda(p) \\, S(u) = (\\lim_{p \\to 0^{+}} p \\, \\lambda(p)) S(u) = 0 \\cdot S(u) = 0$.\n- $\\lim_{p \\to 0^{+}} \\lambda(p) \\, o(p) = \\lim_{p \\to 0^{+}} \\lambda(p) \\cdot \\lim_{p \\to 0^{+}} o(p) = \\gamma \\cdot 0 = 0$.\n\nThe sum of these limits is $0$. Thus, $\\lim_{p \\to 0^{+}} (F_{p}(u) - F_{0}(u)) = 0$. This limit is a constant ($0$) independent of $u$, so the conditions are sufficient.\n\nIn summary, the results are:\n1. $S(x) = \\sum_{i: x_{i} \\neq 0} \\ln|x_{i}|$\n2. $L_{1} = \\gamma$\n3. $L_{2} = 0$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{i: x_i \\neq 0} \\ln|x_i| & \\gamma & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While $\\ell_{p}$ penalties are effective at enforcing sparsity, they can introduce a systematic bias, shrinking the magnitude of the estimated non-zero coefficients toward zero. This exercise provides a concrete analysis of this phenomenon in a simple scalar denoising setting, which serves as a building block for understanding more complex high-dimensional models . By deriving the asymptotic shrinkage factor and quantifying the impact on mean squared error, you will develop a hands-on appreciation for the bias-variance trade-off inherent in penalized estimation methods.",
            "id": "3469670",
            "problem": "Consider the scalar denoising subproblem that arises from separability under an orthonormal design in compressed sensing and sparse optimization: for given observation $y \\in \\mathbb{R}$, regularization exponent $p \\in (0,1]$, and penalty weight $\\lambda > 0$, define the $\\ell_{p}$-penalized estimator $x^{\\star}(y)$ as any minimizer of\n$$\n\\min_{x \\in \\mathbb{R}} \\;\\; \\frac{1}{2}\\,(y - x)^{2} + \\lambda\\,|x|^{p}.\n$$\nAssume $y > 0$ and that the minimizer is strictly positive, so that first-order stationarity applies. Let the true coefficient be $a > 0$ and consider the large-coefficient regime $a \\to \\infty$.\n\n(a) In the noiseless case $y = a$, use the stationarity condition (without invoking any pre-derived proximal formulas) to derive the leading-order asymptotic shrinkage factor $s(a)$, defined by $x^{\\star}(a) = s(a)\\,a$, to its first nontrivial order in the small parameter $\\lambda\\,a^{p-2}$. Your answer must be a closed-form analytic expression in $a$, $p$, and $\\lambda$.\n\n(b) In the noisy case $y = a + \\varepsilon$ with $\\varepsilon$ a zero-mean Gaussian random variable of variance $\\sigma^{2} > 0$, suppose that a separate support-selection step has correctly retained the index of $a$, and that subsequent least squares (LS) debiasing on the selected support sets $x^{\\mathrm{db}} := y$. Using a first-order expansion in $\\varepsilon$ about $a$ and working to the first non-vanishing orders in $\\lambda$ and $\\sigma$, compute the leading-order difference in mean squared error\n$$\n\\Delta \\;:=\\; \\mathbb{E}\\!\\left[(x^{\\star}(y) - a)^{2}\\right] \\;-\\; \\mathbb{E}\\!\\left[(x^{\\mathrm{db}} - a)^{2}\\right],\n$$\nexpressed in closed form as a function of $a$, $p$, $\\lambda$, and $\\sigma$, keeping terms up to order $\\lambda\\,\\sigma^{2}$ and $\\lambda^{2}$.\n\nProvide your final answer as a single $1 \\times 2$ row matrix containing, in order, the expressions for $s(a)$ from part (a) and $\\Delta$ from part (b). No rounding is required and no units are involved. Use radians if any angles appear (no angles are expected here).",
            "solution": "This problem has two parts. Part (a) asks for the asymptotic shrinkage factor of the $\\ell_p$ estimator in a noiseless setting. Part (b) asks for the difference in mean squared error (MSE) between the $\\ell_p$ estimator and a debiased least squares estimator in a noisy setting.\n\n**Part (a): Noiseless Case and Shrinkage Factor**\nThe objective function to minimize is $f(x) = \\frac{1}{2}(y - x)^2 + \\lambda |x|^p$.\nIn the noiseless case, $y=a$. We are given that $a > 0$ and the minimizer $x^{\\star}(a)$ is strictly positive. Therefore, the objective function is $f(x) = \\frac{1}{2}(a - x)^2 + \\lambda x^p$. The first-order stationarity condition $\\frac{df}{dx} = 0$ gives:\n$$-(a - x) + \\lambda p x^{p-1} = 0$$\nLet $x^\\star = x^\\star(a)$ be the minimizer. The stationarity condition is $x^{\\star} - a + \\lambda p (x^{\\star})^{p-1} = 0$.\nIn the large-coefficient regime ($a \\to \\infty$), we expect $x^{\\star}$ to be close to $a$. We can thus approximate $(x^{\\star})^{p-1} \\approx a^{p-1}$ in the penalty term.\n$$x^{\\star} \\approx a - \\lambda p a^{p-1}$$\nThe shrinkage factor $s(a)$ is defined by $x^{\\star} = s(a)a$. Dividing the approximation by $a$ gives:\n$$s(a) = \\frac{x^{\\star}}{a} \\approx \\frac{a - \\lambda p a^{p-1}}{a} = 1 - \\lambda p a^{p-2}$$\nThis is the leading-order asymptotic shrinkage factor.\n\n**Part (b): MSE Difference in the Noisy Case**\nWe need to compute $\\Delta = \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}]$, where $y = a + \\varepsilon$ and $\\mathbb{E}[\\varepsilon] = 0, \\mathbb{E}[\\varepsilon^2] = \\sigma^2$.\nThe MSE of the debiased estimator $x^{\\mathrm{db}} = y$ is:\n$$\\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}] = \\mathbb{E}[(y - a)^{2}] = \\mathbb{E}[\\varepsilon^{2}] = \\sigma^{2}$$\nFor the $\\ell_p$ estimator $x^{\\star}(y)$, we use a first-order Taylor expansion around $y=a$:\n$$x^{\\star}(y) = x^{\\star}(a + \\varepsilon) \\approx x^{\\star}(a) + \\varepsilon \\frac{dx^{\\star}}{dy}\\bigg|_{y=a}$$\nThe estimation error is $x^{\\star}(y) - a \\approx (x^{\\star}(a) - a) + \\varepsilon \\frac{dx^{\\star}}{dy}(a)$. The MSE is the expected squared error:\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\mathbb{E}\\left[\\left( (x^{\\star}(a) - a) + \\varepsilon \\frac{dx^{\\star}}{dy}(a) \\right)^2\\right] = (x^{\\star}(a) - a)^2 + \\sigma^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2$$\nThe first term is the squared bias. From Part (a), the bias is $x^{\\star}(a) - a \\approx -\\lambda p a^{p-1}$. The squared bias is approximately $\\lambda^2 p^2 a^{2p-2}$.\nTo find the derivative $\\frac{dx^{\\star}}{dy}$, we use implicit differentiation on the stationarity condition $x^{\\star}(y) - y + \\lambda p (x^{\\star}(y))^{p-1} = 0$:\n$$\\frac{dx^{\\star}}{dy} - 1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2} \\frac{dx^{\\star}}{dy} = 0 \\implies \\frac{dx^{\\star}}{dy} = \\frac{1}{1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2}}$$\nEvaluating at $y=a$, using $x^{\\star}(a) \\approx a$, and expanding for small $\\lambda a^{p-2}$:\n$$\\frac{dx^{\\star}}{dy}(a) \\approx \\frac{1}{1 + \\lambda p (p-1) a^{p-2}} \\approx 1 - \\lambda p (p-1) a^{p-2}$$\nThe squared derivative is approximately $1 - 2\\lambda p (p-1) a^{p-2}$.\nThe MSE of $x^\\star(y)$ is then:\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\lambda^2 p^2 a^{2p-2} + \\sigma^2 (1 - 2\\lambda p(p-1)a^{p-2})$$\nFinally, the difference in MSE is:\n$$\\Delta \\approx (\\lambda^2 p^2 a^{2p-2} + \\sigma^2 - 2\\lambda p(p-1)\\sigma^2 a^{p-2}) - \\sigma^2 = \\lambda^2 p^2 a^{2p-2} - 2\\lambda p(p-1)\\sigma^2 a^{p-2}$$\nThis is the final result.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\lambda p a^{p-2} & \\lambda^{2} p^{2} a^{2p-2} - 2\\lambda p(p-1)\\sigma^{2} a^{p-2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "We have explored why $\\ell_{p}$ quasi-norms promote sparsity and the effects they have on coefficient estimates. A crucial remaining question is: under what conditions on the measurement matrix $A$ can we guarantee that $\\ell_{p}$ minimization will uniquely recover the correct sparse signal? This exercise guides you through the derivation of the celebrated Null Space Property (NSP), which provides exactly such a guarantee . By determining the sharpest constant for the $\\ell_p$ NSP, you will connect the properties of the quasi-norm to the geometric conditions required for robust sparse recovery, solidifying the theoretical foundation of compressed sensing.",
            "id": "3469687",
            "problem": "Consider a linear measurement model $y = A x$ with $A \\in \\mathbb{R}^{m \\times n}$ and an unknown signal $x \\in \\mathbb{R}^{n}$ that is $k$-sparse, meaning $|\\operatorname{supp}(x)| \\le k$, where $\\operatorname{supp}(x)$ denotes the index set of nonzero entries of $x$. For a parameter $p \\in (0,1]$, define the $\\ell_{p}$ quasi-norm by $\\|x\\|_{p} = \\left(\\sum_{i=1}^{n} |x_{i}|^{p} \\right)^{1/p}$, with the understanding that for $p=1$ this is the usual $\\ell_{1}$ norm.\n\nWe consider recovery by $\\ell_{p}$ minimization: given $y$, solve $\\min \\|z\\|_{p}$ subject to $A z = y$. A fundamental property used to ensure uniqueness of such recovery is the Null Space Property (NSP): there exists a constant $\\theta_{p} \\ge 0$ such that for every index set $S \\subset \\{1,\\dots,n\\}$ with $|S| \\le k$ and for every $h \\in \\ker(A)$, one has $\\|h_{S}\\|_{p} \\le \\theta_{p} \\|h_{S^{c}}\\|_{p}$, where $S^{c}$ is the complement of $S$ and $h_{S}$ denotes the vector equal to $h$ on $S$ and zero on $S^{c}$.\n\nStarting from the core definitions above and the elementary properties of the $\\ell_{p}$ quasi-norm for $p \\in (0,1]$, determine the smallest constant $\\theta_{p}$ (as a closed-form analytic expression in $p$) such that the following implication holds: if a matrix $A$ satisfies $\\|h_{S}\\|_{p} \\le \\theta_{p} \\|h_{S^{c}}\\|_{p}$ for all $h \\in \\ker(A)$ and all $S$ with $|S| \\le k$, then every $k$-sparse vector $x$ is the unique solution to $\\min \\|z\\|_{p}$ subject to $A z = A x$. Your final answer must be a single analytic expression in $p$. No numerical rounding is required.",
            "solution": "The problem asks for the smallest constant $\\theta_p$ in the Null Space Property (NSP) that guarantees a $k$-sparse signal $x$ is the unique solution to the $\\ell_p$-minimization problem.\n\n**1. Uniqueness Condition**\nLet $x$ be a $k$-sparse signal with support $S = \\operatorname{supp}(x)$, where $|S| \\le k$. We are recovering $x$ from measurements $y = Ax$ by solving:\n$$ \\min_{z \\in \\mathbb{R}^n} \\|z\\|_{p} \\quad \\text{subject to} \\quad Az = y $$\nFor $x$ to be the unique solution, for any other feasible vector $z \\neq x$, we must have $\\|z\\|_{p} > \\|x\\|_{p}$.\nLet $z = x+h$, where $h \\neq 0$. Feasibility ($Az=Ax$) implies $A(x+h) = Ax$, so $Ah=0$. Thus, $h$ is a non-zero vector in the null space of $A$, $h \\in \\ker(A) \\setminus \\{0\\}$. The uniqueness condition is $\\|x+h\\|_{p} > \\|x\\|_{p}$ for all such $h$. Since $t \\mapsto t^p$ for $t \\ge 0$ and $p>0$ is strictly increasing, this is equivalent to $\\|x+h\\|_{p}^{p} > \\|x\\|_{p}^{p}$.\n\n**2. Applying Properties of the $\\ell_p$ Quasi-norm**\nLet $h$ be decomposed as $h = h_S + h_{S^c}$, where $h_S$ is supported on $S$ and $h_{S^c}$ is supported on its complement $S^c$. The vector $x+h$ can be written as $(x+h_S) + h_{S^c}$. The supports of these two parts are disjoint. For any two vectors $u, v$ with disjoint support and $p \\in (0,1]$, the $\\ell_p^p$ \"norm\" is additive: $\\|u+v\\|_{p}^{p} = \\|u\\|_{p}^{p} + \\|v\\|_{p}^{p}$.\nApplying this, we get:\n$$ \\|x+h\\|_{p}^{p} = \\|x+h_S\\|_{p}^{p} + \\|h_{S^c}\\|_{p}^{p} $$\nThe uniqueness condition becomes:\n$$ \\|x+h_S\\|_{p}^{p} + \\|h_{S^c}\\|_{p}^{p} > \\|x\\|_{p}^{p} $$\n\n**3. Bounding the Term on the Support**\nFor $p \\in (0,1]$, it is a known property that $\\|a\\|_p^p \\le \\|a-b\\|_p^p + \\|b\\|_p^p$. Letting $a=x$ and $b=-h_S$ (both supported on S), we get $\\|x\\|_p^p \\le \\|x+h_S\\|_p^p + \\|-h_S\\|_p^p$. This provides a lower bound:\n$$ \\|x+h_S\\|_{p}^{p} \\ge \\|x\\|_{p}^{p} - \\|h_S\\|_{p}^{p} $$\nNote that $\\|x\\|_p = \\|x_S\\|_p$. Substituting this bound into the uniqueness condition gives a sufficient condition for uniqueness:\n$$ (\\|x\\|_{p}^{p} - \\|h_S\\|_{p}^{p}) + \\|h_{S^c}\\|_{p}^{p} > \\|x\\|_{p}^{p} $$\nThis simplifies to:\n$$ \\|h_{S^c}\\|_{p}^{p} > \\|h_S\\|_{p}^{p} \\quad \\text{or equivalently} \\quad \\|h_{S^c}\\|_{p} > \\|h_S\\|_{p} $$\n\n**4. Connecting to the Null Space Property**\nThe NSP states that for a constant $\\theta_p$, we have $\\|h_S\\|_{p} \\le \\theta_{p} \\|h_{S^c}\\|_{p}$ for any $h \\in \\ker(A)$ and any set $S$ with $|S| \\le k$.\nTo ensure the sufficient condition $\\|h_S\\|_{p} < \\|h_{S^c}\\|_{p}$ holds, we must have:\n$$ \\theta_{p} \\|h_{S^c}\\|_{p} < \\|h_{S^c}\\|_{p} $$\nSince $h \\neq 0$, if $h_{S^c}=0$, the NSP would imply $h_S=0$, so $h=0$, which is excluded. Thus we can assume $\\|h_{S^c}\\|_p > 0$ and divide by it, which yields the condition $\\theta_p < 1$.\n\n**5. Determining the Sharpest Constant**\nIf the NSP holds with a constant $\\theta_p < 1$, unique recovery is guaranteed. The question asks for the boundary value. If $\\theta_p = 1$, the NSP is $\\|h_S\\|_p \\le \\|h_{S^c}\\|_p$. This does not guarantee strict inequality, so uniqueness is not guaranteed. A non-unique solution could exist if $\\|h_S\\|_p = \\|h_{S^c}\\|_p$. Therefore, to guarantee uniqueness for any $k$-sparse signal $x$ and for any matrix satisfying the property, we must require its NSP constant $\\theta_p$ to be strictly less than 1. The threshold value that separates guaranteed uniqueness from potential failure is 1. This result is independent of $p \\in (0,1]$.\nThe question asks for the value of this constant, which is 1.",
            "answer": "$$\n\\boxed{1}\n$$"
        }
    ]
}