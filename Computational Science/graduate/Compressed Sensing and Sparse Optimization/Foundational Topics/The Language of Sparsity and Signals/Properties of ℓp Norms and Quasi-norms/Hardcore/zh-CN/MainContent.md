## 引言
在现代数据科学、信号处理和机器学习领域，从海量甚至不完整的数据中提取有意义信息的能力至关重要。稀疏性，即信号或模型的大部分参数为零的假设，已成为应对这一挑战的基石。作为量化与促进稀疏性的核心工具，ℓp范数与拟范数家族扮演着不可或缺的角色。然而，对于不同的p值，这些惩罚项的性质差异巨大，直接影响着[算法设计](@entry_id:634229)、理论保证和最终的应用性能。

本文旨在系统性地解决一个核心问题：不同的ℓp惩罚项究竟是如何以及为何能诱导稀疏性的，它们各自的优缺点是什么？我们将超越表面化的应用，深入探讨其背后的数学原理，填补从理论到实践的认知鸿沟。读者将通过本文学习到，[p值](@entry_id:136498)的选择并非一个简单的技术调整，而是一个涉及几何直观、分析特性和统计权衡的深刻决策。

为构建一个全面的认知框架，本文将分三个层次展开。在“原理与机制”一章中，我们将从几何、分析和统计等角度，剖析ℓp（拟）范数的内在属性，揭示其促进稀疏性的机理、与ℓ0范数的深刻联系，以及对恢复理论保证的影响。接下来，在“应用与跨学科联系”一章中，我们将展示这些抽象原理如何在压缩感知、[统计建模](@entry_id:272466)和[鲁棒机器学习](@entry_id:635133)等实际问题中转化为具体的算法设计和性能优势。最后，“动手实践”部分将提供一系列精心设计的练习，帮助读者巩固理论知识，将抽象概念转化为可操作的分析技能。通过这一结构化的学习路径，您将对ℓp范数这一强大工具有一个更加深刻和实用的理解。

## 原理与机制

本章深入探讨在[稀疏优化](@entry_id:166698)和压缩感知领域中起核心作用的 $\ell_p$ 范数与拟范数的性质。继前一章对稀疏性基本概念的介绍之后，我们将从几何、分析和统计等多个角度，系统地剖析这些惩罚项如何以及为何能有效地促进稀疏性，并阐明它们在[算法设计](@entry_id:634229)和理论保证中的关键角色。

### $\ell_p$ (拟)范数家族的定义

我们首先定义在 $\mathbb{R}^n$ 空间中的 $\ell_p$ (拟)范数。对于一个向量 $x = (x_1, \dots, x_n)^\top \in \mathbb{R}^n$，其 $\ell_p$ (拟)范数定义为：

$$
\|x\|_p = \left( \sum_{i=1}^n |x_i|^p \right)^{1/p}
$$

根据参数 $p$ 的取值范围，该定义涵盖了一系列具有不同性质的函数：

-   当 $p \ge 1$ 时，$\| \cdot \|_p$ 是一个**范数 (norm)**。它满足正定性、齐次性和三角不等式 $\|x+y\|_p \le \|x\|_p + \|y\|_p$。其中最著名的是 $p=1$ 时的 **$\ell_1$ 范数** ([曼哈顿范数](@entry_id:143036)) 和 $p=2$ 时的 **$\ell_2$ 范数** (欧几里得范数)。

-   当 $0  p  1$ 时，$\| \cdot \|_p$ 不再满足[三角不等式](@entry_id:143750)，因此不是一个真正的范数。它被称为**拟范数 (quasi-norm)**。尽管如此，它仍然满足其他性质，如正定性和齐次性。这个范围内的拟范数由于其强大的稀疏促进能力，在理论研究和前沿应用中备受关注。

-   作为一种极限情况，我们定义 **$\ell_0$ “范数”**，它计算向量中非零元素的个数：$\|x\|_0 = \#\{i : x_i \neq 0\}$。严格来说，$\ell_0$ 甚至不是一个拟范数（例如，它不满足齐次性），但它直接度量了向量的[稀疏性](@entry_id:136793)，是许多[稀疏优化](@entry_id:166698)问题的理想目标。

在[优化问题](@entry_id:266749)中，我们通常使用惩罚项 $\|x\|_p^p = \sum_{i=1}^n |x_i|^p$ 来代替 $\|x\|_p$，因为前者在分析上更为便捷，尤其是在求导时。这种替换不改变最小化问题的最优解位置。

### 非凸性 ($0  p  1$) 的稀疏[诱导能](@entry_id:190820)力

当 $p  1$ 时，$\ell_p$ 拟范数及其幂次形式 $\|x\|_p^p$ 均为非[凸函数](@entry_id:143075)。正是这种非[凸性](@entry_id:138568)赋予了它们相较于凸范数 ($\ell_1$) 更强的稀疏促进能力。我们可以从几何和分析两个角度来理解这一现象。

#### 几何直观：星形[单位球](@entry_id:142558)与暴[露点](@entry_id:153435)

$\ell_p$ (拟)范数的[单位球](@entry_id:142558) $\mathcal{B}_p = \{x \in \mathbb{R}^n : \|x\|_p \le 1\}$ 的形状直观地揭示了其性质。

-   对于 $p \ge 1$，$\mathcal{B}_p$ 是一个凸集。例如，$\mathcal{B}_1$ 是一个超菱形 (cross-polytope)，$\mathcal{B}_2$ 是一个超球体。
-   对于 $0  p  1$，$\mathcal{B}_p$ 是一个非凸的**[星形集](@entry_id:154094) (star-shaped set)**。它的表面向内凹陷，并在坐标轴方向上形成尖锐的“尖角”或“cusp”。

在求解一个典型的[稀疏恢复](@entry_id:199430)问题，如 $\min \|x\|_p$ s.t. $Ax=y$ 时，我们实际上是在寻找 $\ell_p$ 惩罚项的水平集（即一个缩放的 $\ell_p$ [单位球](@entry_id:142558)）与数据约束所定义的仿射[子空间](@entry_id:150286) $H = \{x : Ax=y\}$ 的第一个接触点。由于 $p1$ 时 $\mathcal{B}_p$ 的尖角正对着坐标轴，这种接触极有可能发生在坐标轴上或非常靠近坐标轴的稀疏向量处。

为了使这一几何直观更加严谨，我们可以考察[单位球](@entry_id:142558)的**暴[露点](@entry_id:153435) (exposed points)**。一个点 $x^\star$ 被称为集合 $\mathcal{S}$ 的暴[露点](@entry_id:153435)，如果存在一个[超平面](@entry_id:268044)（由向量 $v$ 定义）使得 $x^\star$ 在该超平面上的投影值严格大于集合中所有其他点的投影值。对于 $\ell_p$ 单位球 $\mathcal{B}_p$ 而言，这等价于寻找线性泛函 $\langle v, x \rangle$ 在 $x \in \mathcal{B}_p$ 上的唯一最大化子。

可以证明，对于任意 $p \in (0,1)$，$\mathcal{B}_p$ 的[支撑函数](@entry_id:755667)为 $h_{\mathcal{B}_p}(v) = \sup_{x \in \mathcal{B}_p} \langle v, x \rangle = \|v\|_\infty$。如果向量 $v$ 的[无穷范数](@entry_id:637586)（即最大[绝对值](@entry_id:147688)分量）在某个坐标 $j$ 上是唯一的，那么 $\langle v, x \rangle$ 在 $\mathcal{B}_p$ 上的唯一最大化子就是 $1$-稀疏向量 $x^\star = \mathrm{sign}(v_j) e_j$，其中 $e_j$ 是第 $j$ 个[标准基向量](@entry_id:152417)。这意味着，对于 $p \in (0,1)$，$\mathcal{B}_p$ 唯一能够被超平面“暴露”出来的点只有坐标轴上的稀疏向量 $\{\pm e_i\}_{i=1}^n$。这为 $\ell_p$ 最小化倾向于产生[稀疏解](@entry_id:187463)提供了强有力的几何证据 。

#### 分析视角：原点处的无限梯度

从分析的角度看，$\ell_p$ ($0  p  1$) 惩罚项的稀疏促进能力源于其在原点处的导数行为。考虑一维惩罚函数 $\phi(t) = |t|^p$。对于 $t \neq 0$，其导数为 $\phi'(t) = p \cdot \mathrm{sign}(t) |t|^{p-1}$。

由于 $p-1  0$，当 $t \to 0$ 时，导数的[绝对值](@entry_id:147688) $|\phi'(t)| = p|t|^{p-1}$ 会趋向于无穷大。这意味着，对于一个接近于零的系数，$\ell_p$ 惩罚会施加一个近乎无限大的梯度，强力地将其“推”回零。相比之下，$\ell_1$ 范数的惩罚导数在非零处的[绝对值](@entry_id:147688)恒为 $1$，提供的收缩力是恒定的。

这种无限梯度效应导致了**硬阈值 (hard-thresholding)** 行为。在考虑一个简单的标量去噪子问题 $\min_{t \in \mathbb{R}} \frac{1}{2}(t-y)^2 + \lambda|t|^p$ 时，由于惩罚项在原点处的无限梯度，解会表现出一种“全有或全无”的特性：存在一个阈值，当观测值 $|y|$ 小于该阈值时，最优解 $t^\star$ 精确为零；而当 $|y|$ 大于该阈值时，$t^\star$ 则跳跃到一个非零值。这种稳定性使得 $\ell_p$ 正则化能够有效地区分真实信号与小噪声，从而在解中保持稀疏的支撑集 。

我们可以通过比较不同惩罚项在原点附近的**局部曲率 (local curvature)** $\phi''(x)$ 来深化理解。将 $\ell_p$ ($p \in (0,1)$) 与其他著名的非凸稀疏促进惩罚项，如 SCAD (Smoothly Clipped Absolute Deviation) 和 MCP (Minimax Concave Penalty) 进行比较，可以发现：

-   SCAD 在原点附近的行为类似 $\ell_1$ 范数，$\phi_{\mathrm{SCAD}}''(x) = 0$。
-   MCP 在原点附近具有恒定的[负曲率](@entry_id:159335)，$\phi_{\mathrm{MCP}}''(x) = -1/\gamma$。
-   $\ell_p$ 惩罚项的曲率为 $\phi_p''(x) = \lambda_p p(p-1)|x|^{p-2}$，当 $x \to 0$ 时，它趋向于 $-\infty$。

这意味着在足够小的原点邻域内，$\ell_p$ 惩罚项具有比 SCAD 和 MCP 更负的曲率。这对应于其边际惩罚 $\phi_p'(x)$ 在 $x \to 0$ 时增长得最快。因此，在处理含有许多微小（可能由噪声引起）系数的问题时，$\ell_p$ 惩罚项能施加最强的收缩效应，从而更有效地减少“[假阳性](@entry_id:197064)”或错误包含的系数。

### 通往稀疏之桥：$p \to 0^+$ 的极限

$\ell_p$ 拟范数与 $\ell_0$ “范数”之间的深刻联系，可以通过考察 $p \to 0^+$ 时的极限行为来建立。这为使用连续的 $\ell_p$ 最小化来逼近组合式的 $\ell_0$ 最小化问题提供了理论依据。

让我们推导 $\|x\|_p^p$ 在 $p \to 0^+$ 时的一阶[渐近展开](@entry_id:173196)。对于一个固定的向量 $x \in \mathbb{R}^n$，我们有：
$$
\|x\|_p^p = \sum_{i=1}^n |x_i|^p
$$
我们将求和分解到 $x$ 的支撑集 $I_+ = \{i : x_i \neq 0\}$ 和零集 $I_0 = \{i : x_i = 0\}$ 上。对于 $i \in I_0$，项为 $0$。对于 $i \in I_+$，我们利用恒等式 $a^b = \exp(b \ln a)$：
$$
|x_i|^p = \exp(p \ln|x_i|)
$$
当 $p \to 0^+$ 时，指数的参数 $z = p \ln|x_i|$ 也趋向于 $0$。利用[指数函数](@entry_id:161417)的泰勒展开 $\exp(z) = 1 + z + o(z)$，我们得到：
$$
|x_i|^p = 1 + p \ln|x_i| + o(p)
$$
将此展开代入求和式中：
$$
\|x\|_p^p = \sum_{i \in I_+} (1 + p \ln|x_i| + o(p)) = \sum_{i \in I_+} 1 + p \sum_{i \in I_+} \ln|x_i| + \sum_{i \in I_+} o(p)
$$
识别各项：第一项是支撑集的大小，即 $\|x\|_0$；第二项是 $p$ 乘以对数和；第三项是有限个 $o(p)$ 项之和，仍然是 $o(p)$。因此，我们得到如下重要的渐近关系 ：
$$
\|x\|_p^p = \|x\|_0 + p \left( \sum_{i: x_i \neq 0} \ln|x_i| \right) + o(p)
$$
这个公式清晰地表明，当 $p$ 足够小时，$\|x\|_p^p$ 的值主要由 $\|x\|_0$ 决定，而 $p$ 乘以一个与 $x$ 的非零元素幅度相关的项作为[一阶修正](@entry_id:155896)。这启发我们，通过最小化 $\|x\|_p^p$ 来近似最小化 $\|x\|_0$ 是一个合理的策略。

例如，在最大后验 (MAP) 估计问题中，我们可以考察 $\ell_p$ 正则化目标函数 $F_p(u) = \frac{1}{2\sigma^2}\|y - Au\|_2^2 + \lambda(p) \|u\|_p^p$ 如何收敛到 $\ell_0$ 正则化形式 $F_0(u) = \frac{1}{2\sigma^2}\|y - Au\|_2^2 + \gamma \|u\|_0$。通过使用上述[渐近展开](@entry_id:173196)，可以推导出，为了使 $F_p(u)$ 点态收敛于 $F_0(u)$（在相差一个与 $u$ 无关的常数意义下），[正则化参数](@entry_id:162917) $\lambda(p)$ 必须满足以下渐近条件：
$$
\lim_{p \to 0^+} \lambda(p) = \gamma \quad \text{和} \quad \lim_{p \to 0^+} p \lambda(p) = 0
$$
这为在实践中如何选择依赖于 $p$ 的正则化强度以模拟 $\ell_0$ 惩罚提供了精确的指导 。

### 算法后果：阈值算子的动态

$\ell_p$ 惩罚项的性质直接决定了相关优化算法的行为。许多现代[稀疏恢复算法](@entry_id:189308)，如迭代收缩-阈值算法 (ISTA) 及其变体，都依赖于一个核心步骤：**邻近算子 (proximal operator)** 的应用。对于一个惩[罚函数](@entry_id:638029) $\phi$，其邻近算子定义为：
$$
\mathrm{prox}_{\lambda \phi}(y) = \arg\min_{z \in \mathbb{R}} \left\{ \frac{1}{2}(z-y)^2 + \lambda \phi(z) \right\}
$$
这个算子可以看作是在二次近似数据保真项和正则化惩罚之间进行权衡。对于可分离的惩罚项，如 $\|x\|_p^p$，多维邻近算子可以分解为一系列独立的标量（一维）邻近算子。分析这些标量算子（也称为收缩或[阈值函数](@entry_id:272436)）的行为，对于理解算法的收敛性和解的特性至关重要。

我们来比较不同 $p$ 值对应的阈值算子 $T_p(y) = \mathrm{prox}_{\lambda \phi_p}(y)$ 及其在迭代 $x_{k+1} = T_p(x_k)$ 下的动力学：

-   **$p=1$ ([软阈值](@entry_id:635249))**：惩罚项为 $\lambda|z|$。邻近算子是著名的**[软阈值算子](@entry_id:755010) (soft-thresholding operator)**，$T_1(y) = \mathrm{sign}(y) \max(0, |y|-\lambda)$。它将小的输入值设为零，并将大的输入值向零收缩一个固定的量 $\lambda$。对于迭代映射 $x_{k+1} = T_1(x_k)$，其唯一的**[不动点](@entry_id:156394) (fixed point)** 是 $x=0$。从任何初始点 $x_0$ 出发，迭代序列都会在有限步内精确地收敛到 $0$。

-   **$p \to 0^+$ (硬阈值)**：惩罚项为 $\lambda \mathbf{1}_{\{z \neq 0\}}$。邻近算子是**硬阈值算子 (hard-thresholding operator)**，$T_0(y) = y$ 如果 $|y|  \sqrt{2\lambda}$，否则为 $0$。它保留大于某个阈值的输入，而将所有其他输入截断为零，不产生收缩。这个映射的[不动点](@entry_id:156394)集合是 $\{0\} \cup \{x : |x| \ge \sqrt{2\lambda}\}$。初始点 $|x_0|  \sqrt{2\lambda}$ 会在一步内收敛到 $0$，而其他点则保持不变。

-   **$0  p  1$ (非凸收缩)**：惩罚项为 $\lambda |z|^p$。这是一个[非凸优化](@entry_id:634396)问题。尽管问题非凸，但可以证明其迭代映射 $x_{k+1}=T_p(x_k)$ 仍然具有良好行为。其唯一的[不动点](@entry_id:156394)是 $x=0$。与[软阈值](@entry_id:635249)类似，从任何初始点 $x_0$ 出发，迭代序列也将在有限步内收敛到 $0$。然而，其动态过程与[软阈值](@entry_id:635249)不同，它表现出更接近硬阈值的行为，即对小信号的截断作用更强。

这些分析揭示了，尽管 $\ell_p$ ($p \in (0,1)$) 惩罚是非凸的，但其对应的迭代收缩映射表现出稳定的收敛行为，并且其阈值特性结合了 $\ell_1$ 和 $\ell_0$ 惩罚的某些方面，这为设计有效的[非凸优化](@entry_id:634396)算法提供了基础。

### 恢复的统计性质与理论保证

除了几何与分析特性，$\ell_p$ (拟)范数的选择也深刻影响着恢复信号的统计性质和理论保证的强度。

#### 估计中的偏差

所有[正则化方法](@entry_id:150559)都会在估计中引入**偏差 (bias)**，以换取[方差](@entry_id:200758)的减小和[模型复杂度](@entry_id:145563)的控制。对于 $\ell_p$ 正则化，偏差的大小与 $p$ 的值密切相关。

考虑一个简单的标量去噪问题，真实系数为 $a0$，观测值为 $y=a+\varepsilon$，其中 $\varepsilon$ 是噪声。$\ell_p$ 惩罚估计器 $x^\star(y)$ 在无噪声情况下（$y=a$），当真实系数 $a$ 很大时，会产生一个收缩效应。其渐近**收缩因子 (shrinkage factor)** $s(a) = x^\star(a)/a$ 可以被推导出来 ：
$$
s(a) \approx 1 - \lambda p a^{p-2}
$$
这个表达式告诉我们：
-   对于 $\ell_1$ 正则化 ($p=1$)，收缩因子约为 $1 - \lambda/a$，偏差 $a - x^\star(a) \approx \lambda$。对于大系数，这是一种不可忽略的恒定偏差。
-   对于 $\ell_2$ 正则化 ([岭回归](@entry_id:140984)，$p=2$)，收缩因子约为 $1 - 2\lambda$，偏差与系数大小成正比。
-   对于 $0  p  1$，由于 $p-2  -1$，偏差 $\lambda p a^{p-1}$ 会随着 $a$ 的增大而迅速减小。

这意味着，虽然 $\ell_p$ ($p1$) 在原点附近提供了强烈的稀疏促进力，但它对大系数引入的偏差比 $\ell_1$ 更小，这是一个理想的性质。尽管如此，偏差依然存在。通过对 $\ell_p$ 估计器的[均方误差 (MSE)](@entry_id:165831)进行分析，并与一个简单的**去偏 (debiased)** 估计器（如在选定的支撑集上进行普通最小二乘）进行比较，可以量化这种[偏差-方差权衡](@entry_id:138822)。分析表明，$\ell_p$ 估计器的偏差平方项 $\lambda^2 p^2 a^{2p-2}$ 可能导致MSE增大，而去偏步骤则可以显著降低这种误差 。

#### 唯一性恢复的代数条件：[零空间性质](@entry_id:752758)

为了保证 $\ell_p$ 最小化能够唯一地恢复出真实的稀疏信号，测量矩阵 $A$ 必须满足一定的条件。其中一个核心条件是**[零空间性质](@entry_id:752758) (Null Space Property, NSP)**。NSP 要求矩阵 $A$ 的零空间 $\ker(A)$ 中不能包含任何“过于集中”在少数几个坐标上的向量。

对于任意 $p \in (0,1]$，$\ell_p$-NSP 可以表述为：对于任意支撑集 $S$（$|S| \le k$）和任意 $h \in \ker(A)$，必须满足：
$$
\|h_S\|_p  \|h_{S^c}\|_p
$$
其中 $h_S$ 和 $h_{S^c}$ 分别是 $h$ 在支撑集 $S$ 和其[补集](@entry_id:161099) $S^c$ 上的部分。这个条件保证了任何非零的扰动 $h \in \ker(A)$ 添加到真实的 $k$-[稀疏解](@entry_id:187463) $x$ 上（即 $z=x+h$）都会导致 $\ell_p$ 范数值的增加，从而确保 $x$ 是唯一的最小解。

一个更量化的版本是，存在一个常数 $\theta_p$，使得 $\|h_S\|_p \le \theta_p \|h_{S^c}\|_p$。通过对恢复条件的推导可以证明，为了保证唯一恢复，该常数必须满足 $\theta_p  1$。令人惊讶的是，这个保证唯一性的临界常数值 **$1$** 对于所有 $p \in (0,1]$ 都是相同的。这表明，从代数条件上看，$\ell_1$ 范数和 $\ell_p$ 拟范数对测量矩阵的要求具有相同的基本结构。

#### 恢复的几何条件：锥与统计维度

除了代数条件，压缩感知理论还从[高维几何](@entry_id:144192)的角度为恢复成功提供了深刻的见解。这套理论的核心思想是分析在真实稀疏解 $x_0$ 处的**[切锥](@entry_id:191609) (tangent cone)** 或**[下降锥](@entry_id:748320) (descent cone)** 的几何尺寸。

[下降锥](@entry_id:748320) $D(f, x_0)$ 包含了所有使得目标函数 $f(x) = \|x\|_p$ 从 $x_0$ 开始下降的方向。[切锥](@entry_id:191609) $T_{B_p(\tau)}(x_0)$ 描述了在 $x_0$ 点可以离开 $\ell_p$ 球 $B_p(\tau)$ 的所有方向。对于凸情况 ($p=1$)，这两个概念紧密相关。

-   对于 $p \in (0,1)$，在 $k$-稀疏点 $x_0$ 处，[下降锥](@entry_id:748320)和[切锥](@entry_id:191609)的结构异常简单：它们就是 $x_0$ 的支撑集所张成的 $k$ 维[子空间](@entry_id:150286)。
-   对于 $p=1$，[下降锥](@entry_id:748320)和[切锥](@entry_id:191609)的结构则复杂得多，它是一个跨越整个 $\mathbb{R}^n$ [空间的锥](@entry_id:274207)，其边界由向量在支撑集内外的 $\ell_1$ 范数共同决定。

这些锥的“尺寸”可以通过**统计维度 (statistical dimension)** $\delta(C)$ 来衡量，它定义为高斯[向量投影](@entry_id:147046)到锥 $C$ 上的平均平方范数。根据锥形[积分几何](@entry_id:273587)理论，成功恢复一个稀疏信号所需的测量次数 $m$ 与相应锥的统计维度密切相关，即 $m > \delta(C)$。

计算表明：
-   对于 $p \in (0,1)$，$\delta(T_{B_p(\tau)}(x_0)) = k$。
-   对于 $p=1$，$\delta(T_{B_1(\tau)}(x_0))$ 是一个远大于 $k$ 的值，它近似为 $k \ln(n/k)$。

这一结果戏剧性地揭示了[非凸正则化](@entry_id:636532)的威力：理论上，使用 $\ell_p$ ($p1$) 最小化进行[稀疏恢复](@entry_id:199430)所需的测量次数可能只与稀疏度 $k$ 成正比，而与信号的整体维度 $n$ 无关。而 $\ell_1$ 最小化则需要 $m \sim k \ln(n/k)$ 次测量。这为非凸方法在极度[欠采样](@entry_id:272871)情况下的优越性能提供了根本的几何解释。

### 关于鲁棒性的思考：最优范数的选择

迄今为止的讨论似乎表明，选择尽可能小的 $p0$ 是有益的。然而，在面对有噪声或[对抗性扰动](@entry_id:746324)的现实世界问题时，情况变得更加微妙。

我们可以从**鲁棒性 (robustness)** 的角度来重新审视范数的选择。考虑一个测量模型 $y = Ax_0 + \Delta$，其中扰动 $\Delta$ 是未知的，但其大小受 $\ell_q$ 范数约束，即 $\|\Delta\|_q \le \eta$。在设计一个基于 $\ell_p$ 正则化的估计器时，一个自然的设计准则是选择 $p$ 和正则化参数 $\lambda$ 来最小化最坏情况下的恢复误差。

[一阶最优性条件](@entry_id:634945)表明，$\lambda$ 的选择与项 $\|A^\top \Delta\|_{p'}$ 直接相关，其中 $p'$ 是 $p$ 的[共轭指数](@entry_id:138847) ($1/p + 1/p' = 1$)。为了保证解的鲁棒性，我们应该选择一个能有效抑制这个最坏情况扰动项的 $p$。这引导我们去最小化如下定义的量：
$$
\Lambda(p;q,\eta,A) = \sup_{\|\Delta\|_q \le \eta} \|A^\top \Delta\|_{p'}
$$
通过分析范数不等式，可以证明 $\Lambda(p; q, \eta, A)$ 是关于 $p$ 的一个非减函数。这意味着，要最小化这个最坏情况下的扰动影响，我们应该选择最小的可能的 $p$ 值。在 $p \ge 1$ 的范数范围内，最优选择是 $p=1$。

这个结论为 $\ell_1$ 范数的广泛应用提供了另一个强有力的理由：在所有 $\ell_p$ **范数** ($p \ge 1$) 中，$\ell_1$ 正则化对于有界扰动提供了最强的鲁棒性保证。这与之前关于稀疏促进能力的讨论形成了一种有趣的权衡：虽然 $p1$ 的拟范数在理论上能用更少的样本恢复信号，但 $p=1$ 的 $\ell_1$ 范数在面对噪声时表现出更优的鲁棒性，这解释了它为何在实践中如此成功和持久。