{
    "hands_on_practices": [
        {
            "introduction": "将向量投影到 $\\ell_1$ 球上是现代稀疏优化中的一个基石操作，经常作为求解Lasso等问题的算法中的关键子程序。本练习将引导您从第一性原理（即KKT条件）出发，完整地推导出高效的排序阈值算法，并将其付诸实践 。通过完成此练习，您将巩固对约束优化的理解，并为您的计算工具箱增添一个实用工具。",
            "id": "3448189",
            "problem": "考虑$\\mathbb{R}^n$中的单位$\\ell_1$球，定义为 $B_1^n = \\{ x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1 \\}$。一个向量 $v \\in \\mathbb{R}^n$ 到 $B_1^n$ 上的欧几里得投影是凸优化问题 $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|x - v\\|_2^2$ subject to $\\|x\\|_1 \\le 1$ 的唯一解 $x^\\star$。您的任务是，从适用于压缩感知和稀疏优化的基本原理出发，推导出用于计算 $x^\\star$ 的排序阈值算法，证明该算法的正确性，分析其计算复杂性，并加以实现。\n\n推导要求：\n- 从核心定义和原理开始：凸投影；约束凸优化的Karush–Kuhn–Tucker (KKT) 最优性条件；$\\ell_1$ 范数的次微分；以及范数与其对偶范数（$\\ell_1$ 范数的对偶范数是 $\\ell_\\infty$ 范数）之间的基本关系。\n- 不要假设任何关于阈值或排序值的目标公式。从平稳性、可行性、互补松弛性和对偶可行性条件出发，逻辑地推导出完整的算法。\n- 最后得出一个可实现的算法，该算法使用 $v$ 的排序幅度来确定一个全局阈值，并逐坐标应用软阈值化。\n\n正确性要求：\n- 使用严格凸目标函数和凸闭可行集的性质，证明投影 $x^\\star$ 的存在性和唯一性。\n- 建立解的精确软阈值形式，以及通过排序过程计算出的阈值的一致性。\n- 通过对偶范数将构造与$\\ell_p$球的几何形状联系起来，展示对偶$\\ell_\\infty$范数如何界定解处的次梯度。\n\n复杂性要求：\n- 根据 $n$ 为算法的渐近复杂性提供一个紧界，并突出排序和线性遍历的贡献。\n- 讨论可能的改进以及所涉及的权衡（例如，部分选择与完全排序）。\n\n实现要求：\n- 实现一个函数，给定 $v \\in \\mathbb{R}^n$，返回其到 $B_1^n$ 上的投影 $x^\\star$ 以及计算出的阈值。该实现必须是纯数值的，没有外部输入，并且在比较中必须遵守数值公差。\n\n测试套件：\n- 使用以下固定的测试向量，每个向量都是一个实数列表，其中 $n$ 可变：\n    1. $v_1 = [0.3, -0.2, 0.5, 0.0]$ (边界情况，$\\|v_1\\|_1 = 1$)。\n    2. $v_2 = [0.0, 0.0, 0.0]$ (在球内部)。\n    3. $v_3 = [3.0, -1.0, 0.0, 2.0]$ (在球外部，有混合符号)。\n    4. $v_4 = [0.5, 0.5, 0.5, 0.5]$ (在外部，幅度相等)。\n    5. $v_5 = [-0.7, -0.7, -0.7]$ (在外部，负幅度相等)。\n    6. $v_6 = [10^{-12}, -10^{-12}, 10^{-12}, -10^{-12}]$ (幅度极小，远在内部)。\n    7. $v_7 = [2.0]$ (一维，在外部)。\n    8. $v_8 = [0.4, -0.6]$ (边界情况，$\\|v_8\\|_1 = 1$)。\n- 对于每个测试向量 $v$，计算 $x^\\star$ 和一个数值阈值 $\\theta$。令 $\\|\\cdot\\|_p$ 表示 $\\ell_p$ 范数，并对所有不等式检查使用公差 $\\varepsilon = 10^{-10}$。\n- 对于每个测试用例，您必须按顺序计算并输出以下三个量：\n    1. $\\|x^\\star\\|_1$，浮点数。\n    2. 一个布尔值，表示 $v$ 是否在单位 $\\ell_1$ 球内，即 $\\|v\\|_1 \\le 1$ 是否在公差范围内成立。\n    3. 一个布尔值，表示数值上检查的KKT条件对于计算出的 $x^\\star$ 和 $\\theta$ 是否成立：\n        - 如果 $\\|v\\|_1 \\le 1$，验证在公差范围内 $x^\\star = v$。\n        - 如果 $\\|v\\|_1 > 1$，验证在公差范围内 $\\|x^\\star\\|_1$ 等于 $1$；对于所有 $x^\\star_i \\ne 0$ 的索引 $i$，检查在公差范围内 $x^\\star_i = \\operatorname{sign}(v_i)\\max(|v_i| - \\theta, 0)$；对于所有 $x^\\star_i = 0$ 的索引 $i$，检查 $|v_i| \\le \\theta + \\varepsilon$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔结果列表。该列表必须按顺序连接每个测试用例的三个输出，总共产生 $24$ 个条目：\n  $[\\|x^\\star(v_1)\\|_1,\\ \\text{inside}(v_1),\\ \\text{KKT\\_ok}(v_1),\\ \\ldots,\\ \\|x^\\star(v_8)\\|_1,\\ \\text{inside}(v_8),\\ \\text{KKT\\_ok}(v_8)]$。",
            "solution": "对用户提供的问题进行有效性评估。\n\n### 步骤1：提取已知条件\n- **问题领域**：在$\\mathbb{R}^n$中投影到单位$\\ell_1$球上。\n- **可行集**：$B_1^n = \\{ x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1 \\}$。\n- **优化问题**：$\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|x - v\\|_2^2$ subject to $\\|x\\|_1 \\le 1$。解记为 $x^\\star$。\n- **任务**：推导、证明正确性、分析复杂性并实现用于计算 $x^\\star$ 的排序阈值算法。\n- **推导要求**：使用基本原理，包括凸投影、KKT条件、次微分和对偶范数。不假设公式。\n- **正确性要求**：证明存在性/唯一性，建立软阈值形式，并与对偶范数建立联系。\n- **复杂性要求**：提供一个紧的渐近界，并讨论改进方法。\n- **实现要求**：实现一个函数，输入 $v$ 并返回 $x^\\star$ 和阈值 $\\theta$。\n- **测试套件**：提供了一个包含8个特定测试向量 $v_1, \\dots, v_8$ 的列表。\n- **输出指标**：对于每个测试向量 $v$，计算并报告：\n    1. $\\|x^\\star\\|_1$ (浮点数)。\n    2. 一个布尔值，表示是否 $\\|v\\|_1 \\le 1$ (在公差 $\\varepsilon = 10^{-10}$ 内)。\n    3. 一个布尔值，表示计算出的 $x^\\star$ 和 $\\theta$ 是否在数值上满足KKT条件 (在公差 $\\varepsilon = 10^{-10}$ 内)。\n- **最终输出格式**：一个单行的、用方括号括起来的、包含24个结果的逗号分隔列表。\n\n### 步骤2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题是凸优化中的一个标准练习，在信号处理、统计学和机器学习（例如，作为ADMM求解Lasso问题的子步骤）中有重要应用。所有涉及的概念——凸集、$\\ell_p$范数、KKT条件、次微分、对偶范数——都是数学中基础且完善的概念。该问题在科学和数学上是合理的。\n2.  **良态性**：目标函数 $f(x) = \\frac{1}{2}\\|x - v\\|_2^2$ 是严格凸的。可行集 $B_1^n$ 是一个非空、闭合的凸集。在希尔伯特空间中，一个点到一个非空、闭合、凸集上的投影存在且唯一。因此，该问题是良态的，对任何 $v$ 都存在唯一的解 $x^\\star$。\n3.  **客观性**：问题以精确的数学语言陈述，没有歧义、主观性或观点。\n4.  **完整性**：问题陈述是自包含的。它提供了目标函数、约束集，以及对推导、证明、分析和实现的明确要求，包括具体的测试用例和输出格式。\n5.  **平凡性**：从KKT条件推导算法是一个非平凡的练习，需要仔细处理不可微的 $\\ell_1$ 范数及其次微分。这是一个典型但有实质内容的问题。\n\n### 步骤3：结论与行动\n该问题是**有效的**。这是一个定义明确的标准凸优化问题，具有一套清晰的要求。可以继续进行求解。\n\n### 解法\n问题是求向量 $v \\in \\mathbb{R}^n$ 到闭合单位 $\\ell_1$ 球 $B_1^n$ 上的欧几里得投影。这等价于求解以下约束凸优化问题：\n$$\nx^\\star = \\arg\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x - v\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le 1\n$$\n令目标函数为 $f(x) = \\frac{1}{2}\\|x - v\\|_2^2$，不等式约束为 $g(x) = \\|x\\|_1 - 1 \\le 0$。\n\n**1. 解的存在性与唯一性**\n目标函数 $f(x)$ 是一个二次函数，其Hessian矩阵是单位矩阵 $\\nabla^2 f(x) = I$。由于单位矩阵是正定的，所以 $f(x)$ 是严格凸的。可行集 $B_1^n = \\{x \\in \\mathbb{R}^n : g(x) \\le 0\\}$ 是一个凸集（因为所有范数球都是凸的）、闭集且非空（例如 $0 \\in B_1^n$）。在一个非空、闭合、凸集上最小化一个严格凸函数，其解是唯一的。因此，对于任何 $v \\in \\mathbb{R}^n$，都保证存在唯一的投影 $x^\\star$。\n\n**2. 从Karush–Kuhn–Tucker (KKT) 条件推导**\n由于该问题是凸的并且满足Slater条件（例如，$x=0$ 是严格可行的，因为 $\\|0\\|_1  1$），KKT条件是最优性的充要条件。我们构建拉格朗日函数：\n$$\nL(x, \\lambda) = f(x) + \\lambda g(x) = \\frac{1}{2} \\|x - v\\|_2^2 + \\lambda (\\|x\\|_1 - 1)\n$$\n其中 $\\lambda \\in \\mathbb{R}$ 是与不等式约束相关的拉格朗日乘子。对于最优解 $x^\\star$ 的KKT条件是：\n1.  **平稳性 (Stationarity)**：$0 \\in \\nabla_x L(x^\\star, \\lambda) = \\nabla f(x^\\star) + \\lambda \\partial_x g(x^\\star)$。由于 $g(x)$ 涉及不可微的 $\\ell_1$ 范数，我们使用次微分。\n2.  **原始可行性 (Primal Feasibility)**：$\\|x^\\star\\|_1 - 1 \\le 0$。\n3.  **对偶可行性 (Dual Feasibility)**：$\\lambda \\ge 0$。\n4.  **互补松弛性 (Complementary Slackness)**：$\\lambda (\\|x^\\star\\|_1 - 1) = 0$。\n\n$f(x)$ 的梯度是 $\\nabla f(x) = x - v$。$g(x)$ 的次微分是 $\\partial g(x) = \\partial \\|x\\|_1$。因此，平稳性条件变为：\n$$\n0 \\in x^\\star - v + \\lambda \\partial \\|x^\\star\\|_1 \\quad \\iff \\quad v - x^\\star \\in \\lambda \\partial \\|x^\\star\\|_1\n$$\n我们根据 $v$ 的位置分析两种情况。\n\n**情况1：$v$ 在 $\\ell_1$ 球内部或边界上 ($ \\|v\\|_1 \\le 1 $)**\n我们检验假设 $x^\\star = v$。\n- **原始可行性**：$\\|x^\\star\\|_1 = \\|v\\|_1 \\le 1$。此条件满足。\n- 为了满足**互补松弛性**，如果 $\\|v\\|_1  1$，我们必须有 $\\lambda = 0$。如果 $\\|v\\|_1 = 1$，$\\lambda$ 可以为非负数。我们尝试 $\\lambda=0$。\n- **对偶可行性**：$\\lambda = 0 \\ge 0$。满足。\n- **平稳性**：当 $x^\\star = v$ 和 $\\lambda = 0$ 时，条件变为 $v - v \\in 0 \\cdot \\partial \\|v\\|_1$，简化为 $0 \\in \\{0\\}$。这是成立的。\n由于所有KKT条件在 $x^\\star = v$ 和 $\\lambda = 0$ 时都满足，这是唯一的最优解。\n\n**情况2：$v$ 在 $\\ell_1$ 球外部 ($ \\|v\\|_1 > 1 $)**\n在这种情况下，$x^\\star \\ne v$。为了使解尽可能接近 $v$，它必须位于可行集的边界上。\n- **原始可行性**：我们必须有 $\\|x^\\star\\|_1 = 1$。\n- **互补松弛性**：$\\lambda (\\|x^\\star\\|_1 - 1) = \\lambda (1-1) = 0$。此条件对任何 $\\lambda$ 都满足。\n- **对偶可行性**：我们需要证明 $\\lambda > 0$。如果我们假设 $\\lambda=0$，平稳性会给出 $x^\\star - v = 0$，所以 $x^\\star = v$。但这与原始可行性矛盾，因为 $\\|v\\|_1 > 1$。因此，我们必须有 $\\lambda > 0$。\n- **平稳性**：$v - x^\\star \\in \\lambda \\partial \\|x^\\star\\|_1$。我们按分量分析。$\\ell_1$ 范数的次微分由 $(\\partial \\|x\\|_1)_i = \\{\\operatorname{sign}(x_i)\\}$ (如果 $x_i \\ne 0$) 和 $(\\partial \\|x\\|_1)_i = [-1, 1]$ (如果 $x_i = 0$) 给出。\n\n平稳性条件 $v_i - x_i^\\star \\in \\lambda (\\partial \\|x^\\star\\|_1)_i$ 对于每个分量 $i=1, \\dots, n$ 意味着：\n- 如果 $x_i^\\star \\ne 0$：$v_i - x_i^\\star = \\lambda \\operatorname{sign}(x_i^\\star)$。直观上，为了最小化距离，投影 $x_i^\\star$ 的非零分量的符号必须与 $v_i$ 的符号相匹配。因此，$\\operatorname{sign}(x_i^\\star) = \\operatorname{sign}(v_i)$。\n  这给出 $x_i^\\star = v_i - \\lambda \\operatorname{sign}(v_i) = \\operatorname{sign}(v_i)(|v_i| - \\lambda)$。因为 $|x_i^\\star| > 0$，我们必须有 $|v_i| > \\lambda$。\n- 如果 $x_i^\\star = 0$：$v_i - 0 \\in \\lambda [-1, 1]$，这意味着 $|v_i| \\le \\lambda$。\n\n综合这些，我们可以用一个涉及正阈值的单一公式来表示每个分量的解，我们记此阈值为 $\\theta = \\lambda$：\n$$\nx_i^\\star = \\operatorname{sign}(v_i) \\max(0, |v_i| - \\theta)\n$$\n这就是著名的软阈值算子 $S_\\theta(v_i)$。\n为了找到阈值 $\\theta > 0$ 的值，我们使用唯一剩下的条件 $\\|x^\\star\\|_1 = 1$：\n$$\n\\sum_{i=1}^n |x_i^\\star| = \\sum_{i=1}^n \\max(0, |v_i| - \\theta) = 1\n$$\n我们定义一个函数 $h(\\theta) = \\sum_{i=1}^n \\max(0, |v_i| - \\theta)$。我们需要找到 $h(\\theta) = 1$ 的根 $\\theta > 0$。函数 $h(\\theta)$ 是连续的、非增的、分段线性的。在 $\\theta=0$ 时，$h(0) = \\sum |v_i| = \\|v\\|_1 > 1$。当 $\\theta \\to \\max_i |v_i|$ 及更大时，$h(\\theta) \\to 0$。根据介值定理，存在唯一的解 $\\theta > 0$。\n\n为了系统地找到这个 $\\theta$，我们将 $v$ 的绝对值按降序排序：$|v|_{(1)} \\ge |v|_{(2)} \\ge \\dots \\ge |v|_{(n)}$。分段线性函数 $h(\\theta)$ 的断点出现在这些值上。假设解 $\\theta$ 位于区间 $(|v|_{(\\rho+1)}, |v|_{(\\rho)}]$ 内，对于某个索引 $\\rho \\in \\{1, \\dots, n\\}$ (我们定义 $|v|_{(n+1)} = 0$）。对于这样的 $\\theta$， $x^\\star$ 中恰好有 $\\rho$ 个分量是非零的。方程 $h(\\theta)=1$ 变为：\n$$\n\\sum_{i=1}^{\\rho} (|v|_{(i)} - \\theta) + \\sum_{i=\\rho+1}^{n} 0 = 1\n$$\n$$\n\\left(\\sum_{i=1}^{\\rho} |v|_{(i)}\\right) - \\rho\\theta = 1 \\quad \\implies \\quad \\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} |v|_{(i)} - 1\\right)\n$$\n这个 $\\theta$ 的值只有当它落在假设的区间内，即 $|v|_{(\\rho+1)}  \\theta \\le |v|_{(\\rho)}$ 时才有效，这等价于 $\\theta > |v|_{(\\rho+1)}$ 和 $\\theta- |v|_{(\\rho)} \\le 0$。第二部分可以改写为\n$ \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} |v|_{(i)} - 1\\right) \\le |v|_{(\\rho)} \\iff \\sum_{i=1}^{\\rho} |v|_{(i)} - 1 \\le \\rho |v|_{(\\rho)} \\iff \\sum_{i=1}^{\\rho} (|v|_{(i)} - |v|_{(\\rho)}) \\ge 1$。对于 $\\rho > 1$ 这总是成立的。关键是第一个条件。我们需要找到 $\\rho$，使得计算出的 $\\theta$ 大于 $|v|_{(\\rho+1)}$。函数 $(\\sum_{i=1}^k |v|_{(i)} - 1)/k$ 在 $k$ 上不是单调的，所以我们必须通过检查条件来找到正确的 $\\rho$。有效的 $\\rho$是满足 $|v|_{(k)} > \\frac{1}{k}(\\sum_{i=1}^k |v|_{(i)} - 1)$ 的最大整数 $k \\in \\{1, \\dots, n\\}$。\n\n**3. 与对偶范数的联系及几何解释**\n平稳性条件 $v - x^\\star = z$ 其中 $z \\in \\lambda \\partial \\|x^\\star\\|_1$ 是关键。一个向量 $s$ 是 $\\ell_1$ 范数在 $x$ 处的次梯度，即 $s \\in \\partial\\|x\\|_1$，当且仅当 $\\|s\\|_\\infty \\le 1$ 且 $s^T x = \\|x\\|_1$。$\\ell_\\infty$ 范数是 $\\ell_1$ 范数的对偶范数。\n所以，向量 $z/\\lambda$ 必须是 $\\|x^\\star\\|_1$ 的一个次梯度。这意味着 $\\|(v - x^\\star)/\\lambda\\|_\\infty \\le 1$，即 $\\|v - x^\\star\\|_\\infty \\le \\lambda = \\theta$。\n这提供了一个几何视角：从解 $x^\\star$ 到原始点 $v$ 的向量，其最大分量（绝对值）受拉格朗日乘子 $\\theta$ 的限制。对于 $x_i^\\star \\ne 0$ 的分量 $i$，我们有 $|v_i - x_i^\\star| = \\theta$。对于 $x_i^\\star = 0$ 的分量 $i$，我们有 $|v_i| \\le \\theta$。这正是软阈值算子所实现的。\n\n**4. 算法总结与复杂性分析**\n完整的算法如下：\n1.  给定 $v \\in \\mathbb{R}^n$。\n2.  计算 $\\|v\\|_1$。如果 $\\|v\\|_1 \\le 1$，解为 $x^\\star = v$，阈值为 $\\theta = 0$。\n3.  如果 $\\|v\\|_1 > 1$：\n    a. 创建一个包含 $v$ 的绝对值的向量 $u$：$u_i = |v_i|$。\n    b. 将 $u$ 按降序排序，得到 $u_{(1)} \\ge u_{(2)} \\ge \\dots \\ge u_{(n)}$。\n    c. 找到满足 $u_{(\\rho)} - \\frac{1}{\\rho}(\\sum_{i=1}^{\\rho} u_{(i)} - 1) > 0$ 的最大整数 $\\rho \\in \\{1, \\dots, n\\}$。\n    d. 计算阈值 $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^{\\rho} u_{(i)} - 1)$。\n    e. 解由 $x_i^\\star = \\operatorname{sign}(v_i) \\max(0, |v_i| - \\theta)$ 对所有 $i$ 给出。\n\n**复杂性分析**：\n- 主要操作是对 $v$ 的绝对值进行排序。这具有 $O(n \\log n)$ 的时间复杂度。\n- 计算累积和需要 $O(n)$ 时间。\n- 找到索引 $\\rho$ 需要一次遍历，即 $O(n)$。\n- 最终的软阈值步骤也是 $O(n)$。\n- 因此，总计算复杂度受排序限制：**$O(n \\log n)$**。\n\n**可能的改进**：完全排序并非严格必要。问题在于找到一个值 $\\theta$，使得索引集合 $\\{i : |v_i| > \\theta\\}$ 的基数为 $\\rho$，且 $\\theta = \\frac{1}{\\rho}(\\sum_{|v_i|>\\theta} |v_i| - 1)$。这种结构允许使用中位数的中位数选择算法在 $O(n)$ 时间内找到正确的枢轴值，从而得到 $O(n)$ 的解法。然而，这类算法实现起来更复杂，且常数因子较大，因此对于中等大小的 $n$，基于排序的 $O(n \\log n)$ 方法因其简单性和实际性能而常常被优先选择。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the projection algorithm on a suite of test cases.\n    \"\"\"\n    \n    # Tolerance for floating-point comparisons\n    EPS = 1e-10\n\n    def project_l1_ball(v: np.ndarray) -> tuple[np.ndarray, float]:\n        \"\"\"\n        Projects a vector v onto the L1 unit ball.\n\n        Args:\n            v: A numpy array representing the vector to project.\n\n        Returns:\n            A tuple containing:\n            - The projected vector x_star.\n            - The computed threshold theta.\n        \"\"\"\n        # Trivial case: if v is already in the ball, the projection is v itself.\n        if np.sum(np.abs(v)) = 1.0 + EPS:\n            return v.copy(), 0.0\n\n        # Non-trivial case: v is outside the ball.\n        # Sort the absolute values of v in descending order.\n        u = np.sort(np.abs(v))[::-1]\n        \n        # Compute cumulative sums of the sorted absolute values.\n        cssv = np.cumsum(u)\n        \n        # Find the correct rho (number of non-zero elements in the projection).\n        # We need to find the largest k such that u_k > (S_k - 1) / k.\n        # This is equivalent to k * u_k - S_k > -1.\n        # The following vectorized operation finds all k satisfying this and takes the last one.\n        rho_candidates = np.arange(1, len(v) + 1)\n        \n        # The condition from the derivation is u_k - (cssv_k - 1)/k > 0\n        # We find all indices k that satisfy the condition and rho is the largest one.\n        # In NumPy, indices are 0-based, so rho will be the index + 1.\n        indices = np.where(u > (cssv - 1) / rho_candidates)[0]\n        \n        # If indices is empty, it's a floating point edge case. rho is n.\n        if len(indices) == 0:\n            rho = len(v)\n        else:\n            rho = indices[-1] + 1\n        \n        # Compute the threshold theta.\n        theta = (cssv[rho - 1] - 1) / rho\n        \n        # Apply the soft-thresholding operator.\n        x_star = np.sign(v) * np.maximum(0, np.abs(v) - theta)\n        \n        return x_star, theta\n\n    def check_kkt(v: np.ndarray, x_star: np.ndarray, theta: float, is_inside: bool) -> bool:\n        \"\"\"\n        Numerically verifies the KKT conditions for the projection.\n\n        Args:\n            v: The original vector.\n            x_star: The computed projection.\n            theta: The computed threshold (Lagrange multiplier).\n            is_inside: Boolean indicating if ||v||_1 = 1.\n\n        Returns:\n            True if KKT conditions hold within tolerance, False otherwise.\n        \"\"\"\n        if is_inside:\n            # If v was inside, x_star must be equal to v.\n            return np.allclose(x_star, v, atol=EPS)\n        else:\n            # If v was outside, check boundary, soft-thresholding, and subgradient conditions.\n            \n            # 1. Primal Feasibility on the boundary: ||x_star||_1 == 1\n            l1_norm_check = np.isclose(np.sum(np.abs(x_star)), 1.0, atol=EPS)\n            if not l1_norm_check:\n                return False\n\n            # 2. Stationarity check\n            soft_thresh_ok = True\n            subgrad_ok = True\n            \n            for i in range(len(v)):\n                # Check condition for components that are non-zero in the solution\n                if not np.isclose(x_star[i], 0.0, atol=EPS):\n                    expected_x_i = np.sign(v[i]) * max(abs(v[i]) - theta, 0)\n                    if not np.isclose(x_star[i], expected_x_i, atol=EPS):\n                        soft_thresh_ok = False\n                        break\n                # Check condition for components that are zero in the solution\n                else:\n                    if not (abs(v[i]) = theta + EPS):\n                        subgrad_ok = False\n                        break\n            \n            return l1_norm_check and soft_thresh_ok and subgrad_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0.3, -0.2, 0.5, 0.0],\n        [0.0, 0.0, 0.0],\n        [3.0, -1.0, 0.0, 2.0],\n        [0.5, 0.5, 0.5, 0.5],\n        [-0.7, -0.7, -0.7],\n        [1e-12, -1e-12, 1e-12, -1e-12],\n        [2.0],\n        [0.4, -0.6],\n    ]\n\n    results = []\n    for v_list in test_cases:\n        v_np = np.array(v_list, dtype=float)\n        \n        # Compute projection\n        x_star, theta = project_l1_ball(v_np)\n        \n        # 1. ||x*||_1\n        l1_norm_x_star = np.sum(np.abs(x_star))\n        \n        # 2. Is v inside the ball?\n        is_inside = (np.sum(np.abs(v_np)) = 1.0 + EPS)\n\n        # 3. Do KKT conditions hold?\n        kkt_ok = check_kkt(v_np, x_star, theta, is_inside)\n        \n        results.extend([l1_norm_x_star, str(kkt_ok).lower()])\n        # Inserting is_inside boolean between the two\n        results.insert(-1, str(is_inside).lower())\n\n\n    # Final print statement in the exact required format.\n    # Convert all items to string for join\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "对偶性是凸分析的核心概念，而 $\\ell_1$ 和 $\\ell_\\infty$ 范数之间的关系便是一个绝佳范例。本练习将利用凸优化中的一个强大恒等式——Moreau分解，来阐明这种联系 。您将推导 $\\ell_\\infty$ 范数的邻近算子如何通过其对偶范数（$\\ell_1$ 范数）的球投影来表达，从而具体展示对偶的几何结构在计算上是如何关联的。",
            "id": "3448183",
            "problem": "设 $n \\in \\mathbb{N}$，考虑 $\\mathbb{R}^{n}$ 上的 $\\ell_{p}$ 范数，其对偶范数关系为 $\\|\\cdot\\|_{\\infty}$ 的对偶是 $\\|\\cdot\\|_{1}$。将半径为 $\\alpha$ 的闭 $\\ell_{\\infty}$ 球记为 $B_{\\infty}^{n}(\\alpha) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\leq \\alpha\\}$，将半径为 $\\lambda$ 的闭 $\\ell_{1}$ 球记为 $B_{1}^{n}(\\lambda) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq \\lambda\\}$。一个真、闭、凸函数 $f$ 的近端算子为 $\\operatorname{prox}_{f}(v) \\coloneqq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ f(x) + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right\\}$，其凸共轭为 $f^{*}(y) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - f(x)\\}$。\n\n从这些定义以及凸函数及其凸共轭的一般 Moreau 分解出发，完成以下任务：\n\n1. 仅使用欧几里得投影的定义和可分性，推导出到 $\\ell_{\\infty}$ 球上的欧几里得投影是坐标级的限幅操作：对于任意 $v \\in \\mathbb{R}^{n}$ 和任意 $\\alpha > 0$，投影 $\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)$ 按分量满足 $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\min\\{\\max\\{v_{i}, -\\alpha\\}, \\alpha\\}$。\n\n2. 使用对偶范数关系和 $\\ell_{\\infty}$ 范数的凸共轭，特化 Moreau 分解，以获得一个用 $\\ell_{1}$ 球上的欧几里得投影表示的 $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ 的表达式。\n\n3. 应用你的表达式来计算 $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$，其中\n$$\nv = \\begin{pmatrix} 4 \\\\ -3 \\\\ 2 \\\\ -1 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{6}, \\quad \\lambda = 4.\n$$\n\n将最终答案表示为单个行向量，其元素为精确的有理数。无需四舍五入。仅提供近端向量作为最终答案。",
            "solution": "该问题是有效的，因为它科学地基于凸分析和优化的原理，定义齐全、问题适定，并且陈述客观。我们按要求分三部分进行解答。\n\n第一部分：推导到 $\\ell_{\\infty}$ 球上的欧几里得投影。\n\n向量 $v \\in \\mathbb{R}^{n}$ 到闭凸集 $C \\subset \\mathbb{R}^{n}$ 上的欧几里得投影被定义为 $C$ 中在欧几里得范数意义下距离 $v$ 最近的唯一一个点。它是以下优化问题的解：\n$$\n\\Pi_{C}(v) \\coloneqq \\arg\\min_{x \\in C} \\frac{1}{2}\\|x - v\\|_{2}^{2}\n$$\n在此问题中，集合 $C$ 是半径为 $\\alpha > 0$ 的闭 $\\ell_{\\infty}$ 球，记为 $B_{\\infty}^{n}(\\alpha)$。该集合定义为：\n$$\nB_{\\infty}^{n}(\\alpha) = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\leq \\alpha\\} = \\{x \\in \\mathbb{R}^{n} : \\max_{i=1,\\dots,n} |x_{i}| \\leq \\alpha\\}\n$$\n约束 $\\|x\\|_{\\infty} \\leq \\alpha$ 等价于一组分量级约束：对所有 $i \\in \\{1, \\dots, n\\}$，有 $|x_{i}| \\leq \\alpha$。其中每一个都可以写成 $-\\alpha \\leq x_{i} \\leq \\alpha$。\n\n目标函数是欧几里得范数的平方，它关于 $x$ 的分量是可分的：\n$$\n\\frac{1}{2}\\|x - v\\|_{2}^{2} = \\frac{1}{2}\\sum_{i=1}^{n} (x_{i} - v_{i})^{2}\n$$\n由于目标函数和约束都是可分的，这个 $n$ 维优化问题可以分解为 $n$ 个独立的一维优化问题。对每个分量 $i$，我们求解：\n$$\n\\min_{x_i} \\frac{1}{2}(x_{i} - v_{i})^{2} \\quad \\text{subject to} \\quad -\\alpha \\leq x_{i} \\leq \\alpha\n$$\n这是一个简单的在区间 $[-\\alpha, \\alpha]$ 中找到最接近 $v_{i}$ 的点的问题。其解（我们记作 $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i}$）由以下情况给出：\n1. 如果 $v_{i}$ 已经在此区间内，即 $-\\alpha \\leq v_{i} \\leq \\alpha$，则最近的点是 $v_{i}$ 本身。所以，$[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = v_{i}$。\n2. 如果 $v_{i}$ 大于上界，即 $v_{i} > \\alpha$，则最近的点是上界。所以，$[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\alpha$。\n3. 如果 $v_{i}$ 小于下界，即 $v_{i}  -\\alpha$，则最近的点是下界。所以，$[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = -\\alpha$。\n\n这三种情况可以使用 min 和 max 函数合并成一个单一的表达式：\n$$\n[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\min\\{\\alpha, \\max\\{-\\alpha, v_{i}\\}\\}\n$$\n这个操作被称为限幅或裁剪，推导至此完成。\n\n第二部分：使用 Moreau 分解得到 $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ 的表达式。\n\nMoreau 分解指出，对于任意真、闭、凸函数 $f: \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{\\infty\\}$ 及其凸共轭 $f^{*}$，任何向量 $v \\in \\mathbb{R}^{n}$ 都可以唯一地分解为：\n$$\nv = \\operatorname{prox}_{f}(v) + \\operatorname{prox}_{f^{*}}(v)\n$$\n我们感兴趣的是函数 $f(x) = \\lambda \\|x\\|_{\\infty}$ 的近端算子，其中 $\\lambda > 0$。首先，我们计算凸共轭 $f^{*}(y)$：\n$$\nf^{*}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - f(x)\\} = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - \\lambda \\|x\\|_{\\infty}\\}\n$$\n我们根据 $y$ 的 $\\ell_{1}$ 范数分析两种情况：\n\n情况 1: $\\|y\\|_{1} > \\lambda$。我们选择一个向量 $x_c = c \\cdot \\text{sign}(y)$，其中 $c > 0$ 是一个标量，$\\text{sign}(y)$ 是分量级符号向量。那么 $\\langle y, x_c \\rangle = c \\sum_{i} y_i \\text{sign}(y_i) = c \\sum_{i} |y_i| = c \\|y\\|_{1}$。$\\ell_{\\infty}$ 范数为 $\\|x_c\\|_{\\infty} = \\max_i |c \\cdot \\text{sign}(y_i)| = c$。该表达式变为 $c\\|y\\|_{1} - \\lambda c = c(\\|y\\|_{1} - \\lambda)$。由于 $\\|y\\|_{1} > \\lambda$，这个量是正的，并且当 $c \\to \\infty$ 时趋近于 $\\infty$。因此，$f^{*}(y) = \\infty$。\n\n情况 2: $\\|y\\|_{1} \\leq \\lambda$。根据 Hölder 不等式，有 $|\\langle y, x \\rangle| \\leq \\|y\\|_{1} \\|x\\|_{\\infty}$。这给出 $\\langle y, x \\rangle \\leq \\|y\\|_{1} \\|x\\|_{\\infty}$。因此，上确界的表达式是有界的：\n$$\n\\langle y, x \\rangle - \\lambda \\|x\\|_{\\infty} \\leq \\|y\\|_{1} \\|x\\|_{\\infty} - \\lambda \\|x\\|_{\\infty} = (\\|y\\|_{1} - \\lambda)\\|x\\|_{\\infty}\n$$\n由于 $\\|y\\|_{1} \\leq \\lambda$，项 $(\\|y\\|_{1} - \\lambda) \\leq 0$。由于 $\\|x\\|_{\\infty} \\geq 0$，整个表达式是非正的。当 $x = 0$ 时达到最大值 $0$。因此，$f^{*}(y) = 0$。\n\n结合这些结果，凸共轭 $f^{*}(y)$ 是半径为 $\\lambda$ 的闭 $\\ell_{1}$ 球的指示函数：\n$$\nf^{*}(y) = I_{B_{1}^{n}(\\lambda)}(y) \\coloneqq \\begin{cases} 0  \\text{if } \\|y\\|_{1} \\leq \\lambda \\\\ \\infty  \\text{if } \\|y\\|_{1} > \\lambda \\end{cases}\n$$\n现在，我们求 $f^{*}$ 的近端算子：\n$$\n\\operatorname{prox}_{f^{*}}(v) = \\operatorname{prox}_{I_{B_{1}^{n}(\\lambda)}}(v) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ I_{B_{1}^{n}(\\lambda)}(x) + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right\\}\n$$\n指示函数将最小化域限制在集合 $B_{1}^{n}(\\lambda)$ 内。该问题等价于：\n$$\n\\arg\\min_{x \\in B_{1}^{n}(\\lambda)} \\frac{1}{2}\\|x - v\\|_{2}^{2}\n$$\n这正是 $v$ 到 $\\ell_{1}$ 球 $B_{1}^{n}(\\lambda)$ 上的欧几里得投影的定义，记为 $\\Pi_{B_{1}^{n}(\\lambda)}(v)$。\n\n将此结果代入 Moreau 分解中：\n$$\nv = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v) + \\Pi_{B_{1}^{n}(\\lambda)}(v)\n$$\n对所需项进行整理，我们得到表达式：\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v) = v - \\Pi_{B_{1}^{n}(\\lambda)}(v)\n$$\n\n第三部分：对特定向量的应用。\n\n我们必须计算 $v = \\begin{pmatrix} 4  -3  2  -1  \\frac{1}{2}  0 \\end{pmatrix}^{T}$ 和 $\\lambda=4$ 时的 $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$。设 $n=6$。\n使用第二部分的公式，结果是 $x = v - \\Pi_{B_{1}^{6}(4)}(v)$。\n\n首先，我们计算投影 $u = \\Pi_{B_{1}^{6}(4)}(v)$。我们检查 $v$ 的 $\\ell_{1}$ 范数：\n$$\n\\|v\\|_{1} = |4| + |-3| + |2| + |-1| + |\\frac{1}{2}| + |0| = 4 + 3 + 2 + 1 + 0.5 + 0 = 10.5\n$$\n由于 $\\|v\\|_{1} > 4$，投影 $u$ 不是 $v$ 本身，而是 $\\ell_{1}$ 球边界上的一个点，这意味着 $\\|u\\|_{1} = 4$。\n到 $\\ell_{1}$ 球上的投影由软阈值算子给出，$u_i = \\text{sign}(v_i) \\max\\{0, |v_i| - \\theta\\}$，其中阈值 $\\theta \\ge 0$ 的选择要满足 $\\|u\\|_{1} = \\lambda$。我们需要找到一个 $\\theta > 0$ 来解此方程：\n$$\n\\sum_{i=1}^{6} \\max\\{0, |v_i| - \\theta\\} = 4\n$$\n$v$ 的各分量的绝对值为 $\\{4, 3, 2, 1, 0.5, 0\\}$。令 $g(\\theta) = \\sum_{i} \\max\\{0, |v_i| - \\theta\\}$。\n$g(\\theta)$ 是一个关于 $\\theta$ 的连续、非增、分段线性函数。我们在由 $v_i$ 的排序后的大小所定义的区间内测试 $\\theta$ 的值。\n对于 $\\theta \\in [1, 2]$，方程变为：\n$$\n(4 - \\theta) + (3 - \\theta) + (2 - \\theta) + \\max\\{0, 1-\\theta\\} + \\dots = (4 - \\theta) + (3 - \\theta) + (2 - \\theta) = 9 - 3\\theta\n$$\n我们令其等于 $4$：\n$$\n9 - 3\\theta = 4 \\implies 3\\theta = 5 \\implies \\theta = \\frac{5}{3}\n$$\n由于 $1 \\le \\frac{5}{3} \\approx 1.67 \\le 2$，这是 $\\theta$ 的正确值。\n\n现在我们计算投影 $u$ 的分量：\n$u_{1} = \\text{sign}(4) \\max\\{0, 4 - \\frac{5}{3}\\} = 1 \\cdot (\\frac{12}{3} - \\frac{5}{3}) = \\frac{7}{3}$。\n$u_{2} = \\text{sign}(-3) \\max\\{0, 3 - \\frac{5}{3}\\} = -1 \\cdot (\\frac{9}{3} - \\frac{5}{3}) = -\\frac{4}{3}$。\n$u_{3} = \\text{sign}(2) \\max\\{0, 2 - \\frac{5}{3}\\} = 1 \\cdot (\\frac{6}{3} - \\frac{5}{3}) = \\frac{1}{3}$。\n$u_{4} = \\text{sign}(-1) \\max\\{0, 1 - \\frac{5}{3}\\} = -1 \\cdot 0 = 0$。\n$u_{5} = \\text{sign}(\\frac{1}{2}) \\max\\{0, \\frac{1}{2} - \\frac{5}{3}\\} = 1 \\cdot 0 = 0$。\n$u_{6} = \\text{sign}(0) \\max\\{0, 0 - \\frac{5}{3}\\} = 0$。\n所以，$u = \\Pi_{B_{1}^{6}(4)}(v) = \\begin{pmatrix} \\frac{7}{3}  -\\frac{4}{3}  \\frac{1}{3}  0  0  0 \\end{pmatrix}^{T}$。\n\n最后，我们计算近端向量 $x = v - u$：\n$x_{1} = 4 - \\frac{7}{3} = \\frac{12 - 7}{3} = \\frac{5}{3}$。\n$x_{2} = -3 - (-\\frac{4}{3}) = -\\frac{9}{3} + \\frac{4}{3} = -\\frac{5}{3}$。\n$x_{3} = 2 - \\frac{1}{3} = \\frac{6 - 1}{3} = \\frac{5}{3}$。\n$x_{4} = -1 - 0 = -1$。\n$x_{5} = \\frac{1}{2} - 0 = \\frac{1}{2}$。\n$x_{6} = 0 - 0 = 0$。\n\n得到的近端向量是 $x = \\begin{pmatrix} \\frac{5}{3}  -\\frac{5}{3}  \\frac{5}{3}  -1  \\frac{1}{2}  0 \\end{pmatrix}^{T}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{3}  -\\frac{5}{3}  \\frac{5}{3}  -1  \\frac{1}{2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "$\\ell_1$ 球的几何特性对稀疏恢复方法的性能有着深远的影响。本项高级练习将通过分析 $\\ell_1$ 范数在稀疏向量处的下降锥，来探索其局部几何结构 。您将推导该锥的统计维度，这是一个复杂的量，它量化了锥的“大小”，并惊人地预测了高斯测量下压缩感知成功与失败之间的急剧相变。",
            "id": "3448241",
            "problem": "考虑一个向量 $x^\\star \\in \\mathbb{R}^{n}$，其支撑集为 $S \\subset \\{1,2,\\dots,n\\}$，基数为 $|S|=k$，符号模式为 $\\operatorname{sign}(x^\\star_{S}) \\in \\{-1,+1\\}^{k}$，其中 $x^\\star_{S^c}=\\mathbf{0}$。令 $f(x)=\\|x\\|_{1}$ 为 $\\ell_{1}$ 范数。凸函数 $f$ 在点 $x^\\star$ 处的下降锥 $\\mathcal{D}(x^\\star)$ 定义为方向集合 $d \\in \\mathbb{R}^{n}$，使得存在 $t>0$ 满足 $f(x^\\star + t d) \\leq f(x^\\star)$。一个闭凸锥 $\\mathcal{D} \\subset \\mathbb{R}^{n}$ 的统计维度 $\\delta(\\mathcal{D})$ 是一个标准高斯向量在 $\\mathcal{D}$ 上投影的平均平方长度，等价地可以通过其极锥，利用锥积分几何来表示。\n\n从 $f$ 在 $x^\\star$ 处的次微分、下降锥和锥的统计维度的基本定义出发，完成以下任务：\n\n- 导出 $f(x)=\\|x\\|_{1}$ 在 $k$-稀疏点 $x^\\star$ 处的下降锥 $\\mathcal{D}(x^\\star)$ 的一个显式刻画，用支撑集 $S$ 和符号模式 $\\operatorname{sign}(x^\\star_{S})$ 表示。\n- 使用锥积分几何，计算该下降锥的统计维度 $\\delta(\\mathcal{D})$，给出一个包含 $n$、$k$ 以及标准正态分布的高斯概率密度函数 $\\varphi$ 和累积分布函数 $\\Phi$ 的闭式解析表达式。您的推导应从次微分的定义和下降锥与法锥之间的极关系开始，并应显式地对任何必要的辅助参数进行最小化。\n- 最后，使用 $\\delta(\\mathcal{D})$ 的表达式来预测在具有独立同分布标准高斯测量的基追踪恢复中，相变的高斯测量阈值 $m \\approx \\delta(\\mathcal{D})$。\n\n请以 $\\delta(\\mathcal{D})$ 关于 $n$、$k$ 和最优解 $\\tau^\\star$ 的精确解析表达式的形式提供最终答案。不需要数值近似或四舍五入，也不涉及物理单位。在表达式中使用标准正态概率密度函数 $\\varphi(t)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\big(-\\frac{t^{2}}{2}\\big)$ 和累积分布函数 $\\Phi(t)=\\int_{-\\infty}^{t}\\varphi(u)\\,\\mathrm{d}u$。",
            "solution": "该问题要求刻画 $\\ell_1$-范数在稀疏向量处的下降锥，计算其统计维度，并建立其与压缩感知中相变的联系。\n\n### 步骤 1：下降锥 $\\mathcal{D}(x^\\star)$ 的刻画\n\n该问题涉及函数 $f(x) = \\|x\\|_1$ 和一个 $k$-稀疏向量 $x^\\star \\in \\mathbb{R}^n$，其支撑集为 $S$（其中 $|S|=k$），符号模式为 $z = \\operatorname{sign}(x^\\star_S) \\in \\{-1, +1\\}^k$。根据定义，$x^\\star_i \\neq 0$ 对 $i \\in S$ 成立，且 $x^\\star_j = 0$ 对 $j \\in S^c$ 成立。\n\n凸函数 $f$ 在点 $x^\\star$ 处的下降锥由 $\\mathcal{D}(x^\\star) = \\{d \\in \\mathbb{R}^n \\mid f'(x^\\star; d) \\le 0\\}$ 给出，其中 $f'(x^\\star; d)$ 是 $f$ 在 $x^\\star$ 沿方向 $d$ 的方向导数。对于凸函数，该方向导数由次微分 $\\partial f(x^\\star)$ 的支撑函数给出：\n$$ f'(x^\\star; d) = \\sup_{v \\in \\partial f(x^\\star)} \\langle v, d \\rangle $$\n$\\ell_1$-范数 $f(x) = \\|x\\|_1 = \\sum_i |x_i|$ 在点 $x$ 的次微分是集合：\n$$ \\partial f(x) = \\{ v \\in \\mathbb{R}^n \\mid v_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0, \\text{ and } v_i \\in [-1, 1] \\text{ if } x_i = 0 \\} $$\n对于给定的点 $x^\\star$，其在支撑集 $S$ 上的分量非零，在补集 $S^c$ 上的分量为零。因此，其次微分是：\n$$ \\partial f(x^\\star) = \\{ v \\in \\mathbb{R}^n \\mid v_S = \\operatorname{sign}(x^\\star_S) = z, \\text{ and } \\|v_{S^c}\\|_{\\infty} \\le 1 \\} $$\n其中 $v_S$ 表示 $v$ 中索引在 $S$ 内的子向量，$\\| \\cdot \\|_\\infty$ 是最大绝对值范数。\n\n现在我们可以计算方向导数：\n$$ f'(x^\\star; d) = \\sup_{v_S = z, \\|v_{S^c}\\|_{\\infty} \\le 1} \\langle v, d \\rangle = \\sup_{v_S = z, \\|v_{S^c}\\|_{\\infty} \\le 1} (\\langle v_S, d_S \\rangle + \\langle v_{S^c}, d_{S^c} \\rangle) $$\n上确界可分为两部分：\n$$ f'(x^\\star; d) = \\langle z, d_S \\rangle + \\sup_{\\|v_{S^c}\\|_{\\infty} \\le 1} \\langle v_{S^c}, d_{S^c} \\rangle $$\n第二项是 $\\ell_\\infty$-范数的对偶范数的定义，即 $\\ell_1$-范数。因此：\n$$ \\sup_{\\|v_{S^c}\\|_{\\infty} \\le 1} \\langle v_{S^c}, d_{S^c} \\rangle = \\|d_{S^c}\\|_1 $$\n所以，方向导数为 $f'(x^\\star; d) = \\langle z, d_S \\rangle + \\|d_{S^c}\\|_1$。\n下降锥 $\\mathcal{D}(x^\\star)$ 是使该量为非正的方向 $d$ 的集合。\n$$ \\mathcal{D}(x^\\star) = \\{ d \\in \\mathbb{R}^n \\mid \\langle z, d_S \\rangle + \\|d_{S^c}\\|_1 \\le 0 \\} $$\n这就给出了下降锥的显式刻画。\n\n### 步骤 2：统计维度 $\\delta(\\mathcal{D})$ 的计算\n\n锥 $\\mathcal{D}(x^\\star)$ 的统计维度是压缩感知理论中的一个核心量。该领域的一个强大结果指出，对于形如 $\\mathcal{D} = \\{x \\in \\mathbb{R}^n : F(x) \\le 0\\}$ 的锥，其中 $F$ 是一个凸的、正齐次函数（一个规范函数），其统计维度可以通过一个涉及 $F$ 的近端算子的优化问题来计算。\n$$ \\delta(\\mathcal{D}) = \\min_{\\tau \\ge 0} \\mathbb{E}_{g \\sim \\mathcal{N}(0, I_n)} \\left[ \\|g - \\operatorname{prox}_{\\tau F}(g)\\|^2 \\right] $$\n其中近端算子定义为：\n$$ \\operatorname{prox}_{\\tau F}(g) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\|u-g\\|^2 + \\tau F(u) \\right\\} $$\n在我们的例子中，$F(d) = \\langle z, d_S \\rangle + \\|d_{S^c}\\|_1$，它是凸且正齐次的。我们通过计算近端算子来继续。该最小化问题在索引集 $S$ 和 $S^c$ 上是可分的：\n$$ \\min_{d_S, d_{S^c}} \\left\\{ \\frac{1}{2}\\|d_S-g_S\\|^2 + \\frac{1}{2}\\|d_{S^c}-g_{S^c}\\|^2 + \\tau(\\langle z, d_S \\rangle + \\|d_{S^c}\\|_1) \\right\\} $$\n这等价于求解两个独立的问题。\n\n1.  **对于支撑集 $S$ 上的分量：**\n    $$ \\min_{d_S \\in \\mathbb{R}^k} \\left\\{ \\frac{1}{2}\\|d_S - g_S\\|^2 + \\tau \\langle z, d_S \\rangle \\right\\} $$\n    这是一个无约束二次最小化问题。将关于 $d_S$ 的梯度设为零，得到：\n    $$ (d_S - g_S) + \\tau z = 0 \\implies d_S^* = g_S - \\tau z $$\n\n2.  **对于补集 $S^c$ 上的分量：**\n    $$ \\min_{d_{S^c} \\in \\mathbb{R}^{n-k}} \\left\\{ \\frac{1}{2}\\|d_{S^c} - g_{S^c}\\|^2 + \\tau \\|d_{S^c}\\|_1 \\right\\} $$\n    这是 $\\ell_1$-范数的近端算子的定义，即软阈值算子 $S_\\tau(\\cdot)$。解是逐分量应用的：\n    $$ (d_{S^c}^*)_j = S_\\tau(g_j) = \\operatorname{sign}(g_j) \\max(|g_j| - \\tau, 0) $$\n\n令 $u^* = \\operatorname{prox}_{\\tau F}(g)$。我们有 $u^*_S = g_S - \\tau z$ 和 $u^*_{S^c} = S_\\tau(g_{S^c})$。现在我们计算残差 $g - u^*$ 的平方范数：\n$$ \\|g - u^*\\|^2 = \\|g_S - u_S^*\\|^2 + \\|g_{S^c} - u_{S^c}^*\\|^2 $$\n第一项是：\n$$ \\|g_S - (g_S - \\tau z)\\|^2 = \\|\\tau z\\|^2 = \\tau^2 \\sum_{i \\in S} z_i^2 = \\tau^2 k $$\n第二项是一个向量与其软阈值处理后的版本之差的平方范数。这个量是众所周知的：\n$$ \\|g_{S^c} - S_\\tau(g_{S^c})\\|^2 = \\sum_{j \\in S^c} \\|g_j - S_\\tau(g_j)\\|^2 = \\sum_{j \\in S^c} \\left( g_j^2 \\cdot \\mathbb{I}(|g_j| \\le \\tau) + \\tau^2 \\cdot \\mathbb{I}(|g_j| > \\tau) \\right) $$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n统计维度是该量期望值关于 $\\tau \\ge 0$ 的最小值。令 $L(\\tau) = \\mathbb{E}[\\|g - u^*\\|^2]$。\n$$ L(\\tau) = \\mathbb{E}[k\\tau^2] + \\mathbb{E}\\left[\\sum_{j \\in S^c} \\left( g_j^2 \\cdot \\mathbb{I}(|g_j| \\le \\tau) + \\tau^2 \\cdot \\mathbb{I}(|g_j| > \\tau) \\right)\\right] $$\n由于 $g$ 的分量是独立同分布的标准正态变量 $\\gamma \\sim \\mathcal{N}(0, 1)$，和的期望是单个分量期望的 $(n-k)$ 倍：\n$$ L(\\tau) = k\\tau^2 + (n-k) \\mathbb{E}_{\\gamma \\sim \\mathcal{N}(0,1)} \\left[ \\gamma^2 \\cdot \\mathbb{I}(|\\gamma| \\le \\tau) + \\tau^2 \\cdot \\mathbb{I}(|\\gamma| > \\tau) \\right] $$\n我们来计算这个期望。令 $\\varphi(t) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2)$ 和 $\\Phi(t) = \\int_{-\\infty}^t \\varphi(u) du$。\n$$ \\mathbb{E}\\left[ \\tau^2 \\cdot \\mathbb{I}(|\\gamma| > \\tau) \\right] = \\tau^2 \\mathbb{P}(|\\gamma| > \\tau) = \\tau^2 (2(1 - \\Phi(\\tau))) $$\n$$ \\mathbb{E}\\left[ \\gamma^2 \\cdot \\mathbb{I}(|\\gamma| \\le \\tau) \\right] = \\int_{-\\tau}^{\\tau} t^2 \\varphi(t) dt $$\n使用分部积分法 $\\int t^2 \\varphi(t) dt = -t\\varphi(t) + \\Phi(t)$，我们得到：\n$$ \\int_{-\\tau}^{\\tau} t^2 \\varphi(t) dt = [-t\\varphi(t) + \\Phi(t)]_{-\\tau}^{\\tau} = (-\\tau\\varphi(\\tau) + \\Phi(\\tau)) - (\\tau\\varphi(-\\tau) + \\Phi(-\\tau)) $$\n因为 $\\varphi(t)$ 是偶函数，所以 $\\varphi(-\\tau) = \\varphi(\\tau)$，且 $\\Phi(-\\tau) = 1-\\Phi(\\tau)$：\n$$ \\int_{-\\tau}^{\\tau} t^2 \\varphi(t) dt = -2\\tau\\varphi(\\tau) + \\Phi(\\tau) - (1-\\Phi(\\tau)) = 2\\Phi(\\tau) - 1 - 2\\tau\\varphi(\\tau) $$\n合并这些项，期望为：\n$$ (2\\Phi(\\tau) - 1 - 2\\tau\\varphi(\\tau)) + 2\\tau^2(1-\\Phi(\\tau)) $$\n因此，统计维度是以下函数在 $\\tau \\ge 0$ 上的最小值：\n$$ \\delta(\\mathcal{D}) = \\min_{\\tau \\ge 0} \\left\\{ k\\tau^2 + (n-k)\\left[ 2\\Phi(\\tau) - 1 - 2\\tau\\varphi(\\tau) + 2\\tau^2(1-\\Phi(\\tau)) \\right] \\right\\} $$\n设 $\\tau^\\star$ 是实现此最小值的 $\\tau \\ge 0$。问题要求用这个最优解 $\\tau^\\star$ 来表示 $\\delta(\\mathcal{D})$ 的表达式。\n\n### 步骤 3：高斯测量阈值\n\n下降锥的统计维度 $\\delta(\\mathcal{D})$ 与用于信号恢复的凸优化方法的性能有深刻的联系。对于基追踪问题（通过求解 $\\min \\|x\\|_1$ s.t. $Ax=y$ 从线性测量 $y=Ax^\\star$ 中恢复稀疏信号 $x^\\star$），当测量矩阵 $A$ 具有独立同分布的标准高斯元素时，会发生相变。如果测量数量 $m$ 超过某个阈值，信号 $x^\\star$ 将以高概率被恢复；如果 $m$ 低于该阈值，则以高概率恢复失败。这个阈值恰好由下降锥的统计维度预测。\n$$ m \\approx \\delta(\\mathcal{D}(x^\\star)) $$\n因此，为 $\\delta(\\mathcal{D})$ 推导出的表达式为成功恢复所需的高斯测量数量提供了理论预测。\n\n最终答案是目标函数在最优解 $\\tau^\\star$ 处的值。\n$$ \\delta(\\mathcal{D}) = k(\\tau^\\star)^2 + (n-k)\\left[ 2\\Phi(\\tau^\\star) - 1 - 2\\tau^\\star\\varphi(\\tau^\\star) + 2(\\tau^\\star)^2(1-\\Phi(\\tau^\\star)) \\right] $$",
            "answer": "$$\n\\boxed{k(\\tau^\\star)^{2} + (n-k)\\left(2\\Phi(\\tau^\\star) - 1 - 2\\tau^\\star\\varphi(\\tau^\\star) + 2(\\tau^\\star)^{2}(1-\\Phi(\\tau^\\star))\\right)}\n$$"
        }
    ]
}