{
    "hands_on_practices": [
        {
            "introduction": "To begin, we establish the sheer scale of the problem that motivates compressed sensing. This first exercise provides a direct, quantitative comparison between the sampling requirements of the classical Nyquist–Shannon theorem and a standard result from compressed sensing theory . By calculating and comparing the number of samples needed for a moderately high-dimensional signal, you will gain a tangible appreciation for the 'curse of dimensionality' and see the dramatic efficiency gains promised by leveraging signal sparsity.",
            "id": "3434276",
            "problem": "Consider a real-valued function $f : [0,1]^{d} \\to \\mathbb{R}$ that is bandlimited in the sense that its $d$-dimensional Fourier transform has support contained in the Cartesian product $[-W, W]^{d}$. Classical Nyquist–Shannon sampling theory asserts that avoiding aliasing in one dimension requires a uniform sampling interval $\\Delta$ satisfying $\\Delta \\leq \\frac{1}{2W}$. In $d$ dimensions on a uniform grid, this requirement applies independently along each coordinate axis. Therefore, the minimum integer number of grid points per axis is $N = \\lceil \\frac{1}{\\Delta} \\rceil$, and the total number of grid samples on $[0,1]^{d}$ is $N^{d}$.\n\nUnder the Restricted Isometry Property (RIP) regime from compressed sensing, a standard sufficient measurement bound for recovery of an $s$-sparse vector in ambient dimension $n$ is $m = C s \\ln\\left(\\frac{n}{s}\\right)$, where $\\ln$ denotes the natural logarithm.\n\nUsing $d = 10$, $W = 50$, $[0,1]^{10}$, $n = 10^{6}$, $s = 100$, and $C = 5$, compute the ratio $R$ of the minimum number of uniform grid samples required by classical bandlimited sampling to the compressed sensing bound $m$. Express your final answer for $R$ in scientific notation and round to three significant figures. No units are required.",
            "solution": "The user wants to compare the number of samples required by two different signal acquisition frameworks: classical Nyquist-Shannon sampling and compressed sensing. The problem statement provides all necessary formulas and parameters for this comparison.\n\nFirst, we calculate the minimum number of uniform grid samples required by classical bandlimited sampling theory, which we denote as $N_{\\text{classical}}$.\nThe problem specifies a function $f : [0,1]^{d} \\to \\mathbb{R}$ whose Fourier transform has support within $[-W, W]^{d}$. According to the Nyquist-Shannon sampling theorem, the sampling interval $\\Delta$ along each dimension must satisfy the condition $\\Delta \\leq \\frac{1}{2W}$.\nTo obtain the minimum number of samples, we must use the largest permissible sampling interval, which is $\\Delta = \\frac{1}{2W}$.\nThe problem provides the bandwidth parameter $W = 50$. Substituting this value, we find the sampling interval:\n$$ \\Delta = \\frac{1}{2 \\times 50} = \\frac{1}{100} $$\nThe problem defines the minimum integer number of grid points per axis as $N = \\lceil \\frac{1}{\\Delta} \\rceil$. Using our calculated value of $\\Delta$:\n$$ N = \\left\\lceil \\frac{1}{1/100} \\right\\rceil = \\lceil 100 \\rceil = 100 $$\nThe total number of grid samples for a $d$-dimensional function on a uniform grid is given by $N_{\\text{classical}} = N^{d}$. The problem specifies the dimension as $d = 10$. Therefore, the total number of classical samples is:\n$$ N_{\\text{classical}} = 100^{10} = (10^{2})^{10} = 10^{20} $$\n\nNext, we calculate the number of measurements required under the compressed sensing (CS) regime, denoted by $m$. The problem provides the standard sufficient measurement bound from the Restricted Isometry Property (RIP):\n$$ m = C s \\ln\\left(\\frac{n}{s}\\right) $$\nThe given parameters are the constant $C = 5$, the sparsity level $s = 100$, and the ambient dimension $n = 10^{6}$. We substitute these values into the formula for $m$:\n$$ m = 5 \\times 100 \\times \\ln\\left(\\frac{10^{6}}{100}\\right) $$\nFirst, we simplify the argument of the natural logarithm:\n$$ \\frac{n}{s} = \\frac{10^{6}}{10^{2}} = 10^{4} $$\nNow, we substitute this back into the expression for $m$:\n$$ m = 500 \\times \\ln(10^{4}) $$\nUsing the property of logarithms $\\ln(a^{b}) = b \\ln(a)$, we get:\n$$ m = 500 \\times 4 \\ln(10) = 2000 \\ln(10) $$\n\nFinally, we compute the ratio $R$ of the number of classical samples to the number of CS measurements:\n$$ R = \\frac{N_{\\text{classical}}}{m} = \\frac{10^{20}}{2000 \\ln(10)} $$\nTo express this in scientific notation, we can rewrite the denominator:\n$$ R = \\frac{10^{20}}{2 \\times 10^{3} \\ln(10)} = \\frac{10^{17}}{2 \\ln(10)} $$\nTo obtain a numerical value, we use the approximation for the natural logarithm of $10$, which is $\\ln(10) \\approx 2.302585$.\n$$ R \\approx \\frac{10^{17}}{2 \\times 2.302585} = \\frac{10^{17}}{4.60517} $$\n$$ R \\approx 0.217147 \\times 10^{17} $$\nTo write this in standard scientific notation, we adjust the mantissa and the exponent:\n$$ R \\approx 2.17147 \\times 10^{16} $$\nThe problem requires the final answer to be rounded to three significant figures.\n$$ R \\approx 2.17 \\times 10^{16} $$\nThis ratio demonstrates the substantial reduction in the required number of measurements offered by compressed sensing compared to classical Nyquist-rate sampling for high-dimensional sparse signals, a phenomenon that circumvents the \"curse of dimensionality\".",
            "answer": "$$\\boxed{2.17 \\times 10^{16}}$$"
        },
        {
            "introduction": "Having appreciated the numerical scale of the challenge, we now delve into a conceptual failure mode of classical sampling. This exercise guides you to construct an explicit high-dimensional signal that, due to phase-aligned aliasing, is rendered completely invisible to a standard, separable sampling grid, yielding only zero measurements . This powerful counterexample illustrates a fundamental vulnerability of deterministic grid sampling and highlights why new measurement paradigms are necessary for high-dimensional sparse signals.",
            "id": "3434218",
            "problem": "Consider a $d$-dimensional continuous signal $x:\\mathbb{R}^{d}\\to\\mathbb{R}$ and a classical separable sampling operator that samples $x$ on the tensor-product grid $\\{(n_{1}\\Delta_{1},\\dots,n_{d}\\Delta_{d}) : n_{j}\\in\\mathbb{Z}\\}$, where $\\Delta_{j}>0$ are fixed sampling spacings along each axis. Throughout, assume that reconstruction is attempted from these point samples only. Begin from the fundamental facts that for any integer $k$ and any integer $n$, $\\exp(\\mathrm{i}2\\pi k n)=1$ and $\\cos(2\\pi k n)=1$, and that uniform sampling at spacings $\\Delta_{j}$ cannot distinguish frequency components that differ by integer multiples of $1/\\Delta_{j}$ along axis $j$.\n\nConstruct an explicit $d$-dimensional separable signal of the form\n$$\nx(\\mathbf{t}) \\;=\\; a\\prod_{j=1}^{d} \\cos\\!\\big(2\\pi f_{j} t_{j}\\big) \\;-\\; a\\prod_{j=1}^{d} \\cos\\!\\big(2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) t_{j}\\big),\n$$\nwith amplitude $a>0$, real base frequencies $f_{j}\\in\\mathbb{R}$, and nonzero integers $s_{j}\\in\\mathbb{Z}\\setminus\\{0\\}$, that is $2$-sparse in the dictionary of separable rank-one cosine atoms. Using only the facts above, show that on the separable Nyquist grid $(n_{1}\\Delta_{1},\\dots,n_{d}\\Delta_{d})$ the two separable components produce identical samples and hence $x$ is mapped to the zero data array, despite $x$ being nonzero as a continuous function. This demonstrates a breakdown of classical sampling in high dimensions under separability due to phase-aligned aliasing across axes.\n\nNow consider the following randomized, separability-breaking measurement: draw a single random point $\\mathbf{U}=(U_{1},\\dots,U_{d})$ with independent coordinates $U_{j}\\sim \\mathrm{Uniform}(0,\\Delta_{j})$ (independent and identically distributed (i.i.d.) across $j$ except for their ranges), and measure the scalar projection $y=x(\\mathbf{U})$. For analytical tractability, set $f_{j}=0$ for all $j$ while keeping $s_{j}\\neq 0$. Compute, in closed form, the expectation of the squared measurement,\n$$\n\\mathbb{E}\\big[y^{2}\\big] \\;=\\; \\mathbb{E}\\!\\left[\\,x(\\mathbf{U})^{2}\\right],\n$$\nas a function of $a$ and $d$. Your final answer must be a single closed-form analytic expression. No rounding is required, and no physical units are involved. Express your final answer as a simplified symbolic expression.",
            "solution": "The problem is valid. It is scientifically grounded in the principles of sampling theory and Fourier analysis, well-posed, objective, and internally consistent.\n\nThe problem is addressed in two parts. First, we demonstrate that the given signal $x(\\mathbf{t})$ evaluates to zero on the specified separable sampling grid. Second, we compute the expected squared value of the signal at a randomized point for a special case.\n\nPart 1: Analysis on the Separable Grid\n\nThe signal is defined as\n$$\nx(\\mathbf{t}) \\;=\\; a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} t_{j}) \\;-\\; a\\prod_{j=1}^{d} \\cos\\big(2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) t_{j}\\big)\n$$\nwhere $\\mathbf{t} = (t_1, \\dots, t_d) \\in \\mathbb{R}^d$. The classical sampling operator measures the signal at points on the tensor-product grid $\\mathbf{t}_{\\mathbf{n}} = (n_{1}\\Delta_{1}, \\dots, n_{d}\\Delta_{d})$, where $\\mathbf{n} = (n_1, \\dots, n_d)$ is a vector of integers, $n_j \\in \\mathbb{Z}$.\n\nLet's evaluate the signal $x(\\mathbf{t})$ at a grid point $\\mathbf{t}_{\\mathbf{n}}$. The first term is\n$$\na\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_{j}\\Delta_{j})\n$$\nFor the second term, we analyze the argument of the cosine for each component $j \\in \\{1, \\dots, d\\}$ at $t_j = n_j\\Delta_j$:\n$$\n2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) t_j = 2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) (n_j\\Delta_j) = 2\\pi f_{j} n_j\\Delta_j + 2\\pi s_{j} n_j\n$$\nGiven that $s_j \\in \\mathbb{Z}\\setminus\\{0\\}$ and $n_j \\in \\mathbb{Z}$, their product $s_j n_j$ is an integer. Let $k_j = s_j n_j \\in \\mathbb{Z}$. The argument becomes $2\\pi f_{j} n_j\\Delta_j + 2\\pi k_j$.\nThe cosine function is $2\\pi$-periodic, meaning $\\cos(\\theta + 2\\pi k) = \\cos(\\theta)$ for any integer $k$. Therefore, for each $j$, we have:\n$$\n\\cos\\big(2\\pi f_{j} n_j\\Delta_j + 2\\pi k_j\\big) = \\cos(2\\pi f_{j} n_j\\Delta_j)\n$$\nThis is a direct consequence of the aliasing phenomenon mentioned in the problem: a frequency component at $f_j' = f_j + s_j\\Delta_j^{-1}$ is indistinguishable from one at $f_j$ when sampled uniformly with spacing $\\Delta_j$, as their difference is an integer multiple of the sampling rate $1/\\Delta_j$.\n\nSubstituting this result back into the second term of $x(\\mathbf{t}_{\\mathbf{n}})$ gives:\n$$\na\\prod_{j=1}^{d} \\cos\\big(2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) n_j\\Delta_j\\big) = a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_j\\Delta_j)\n$$\nThis expression is identical to the first term. Thus, the signal evaluated at any point on the grid is:\n$$\nx(\\mathbf{t}_{\\mathbf{n}}) = a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_j\\Delta_j) - a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_j\\Delta_j) = 0\n$$\nThis demonstrates that although $x(\\mathbf{t})$ is a nonzero function in the $d$-dimensional continuous domain, all its samples on the separable grid are zero. The sampling scheme is completely blind to this signal.\n\nPart 2: Expectation of the Randomized Measurement\n\nWe now consider the simplified case where $f_j=0$ for all $j=1, \\dots, d$. The signal $x(\\mathbf{t})$ becomes:\n$$\nx(\\mathbf{t}) = a\\prod_{j=1}^{d} \\cos(0) - a\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} t_{j}) = a - a\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} t_{j})\n$$\nThe measurement $y$ is the value of the signal at a random point $\\mathbf{U}=(U_1, \\dots, U_d)$, where each $U_j$ is drawn independently from a uniform distribution $\\mathrm{Uniform}(0, \\Delta_j)$.\n$$\ny = x(\\mathbf{U}) = a - a\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) = a\\left(1 - \\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right)\n$$\nWe need to compute the expectation of the squared measurement, $\\mathbb{E}[y^2]$.\n$$\ny^2 = a^{2} \\left( 1 - \\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) \\right)^2\n$$\nExpanding the square:\n$$\ny^2 = a^{2} \\left( 1 - 2\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) + \\left(\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right)^2 \\right)\n$$\n$$\ny^2 = a^{2} \\left( 1 - 2\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) + \\prod_{j=1}^{d} \\cos^{2}(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) \\right)\n$$\nBy the linearity of expectation,\n$$\n\\mathbb{E}[y^2] = a^{2} \\left( \\mathbb{E}[1] - 2\\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right] + \\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right] \\right)\n$$\nSince the random variables $U_j$ are independent, any functions of them, such as $\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})$, are also independent. Thus, the expectation of a product is the product of expectations:\n$$\n\\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos(\\dots)\\right] = \\prod_{j=1}^{d} \\mathbb{E}[\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})]\n$$\n$$\n\\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos^2(\\dots)\\right] = \\prod_{j=1}^{d} \\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})]\n$$\nLet's compute the individual expectations. The probability density function of $U_j$ is $p(u) = 1/\\Delta_j$ for $u \\in (0, \\Delta_j)$.\nFor the first expectation term:\n$$\n\\mathbb{E}[\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\int_{0}^{\\Delta_j} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} u) \\frac{1}{\\Delta_j} du = \\frac{1}{\\Delta_j} \\left[ \\frac{\\sin(2\\pi s_{j}\\Delta_{j}^{-1} u)}{2\\pi s_{j}\\Delta_{j}^{-1}} \\right]_{0}^{\\Delta_j}\n$$\n$$\n= \\frac{1}{2\\pi s_j} [\\sin(2\\pi s_{j}\\Delta_{j}^{-1} u)]_{0}^{\\Delta_j} = \\frac{1}{2\\pi s_j} (\\sin(2\\pi s_j) - \\sin(0))\n$$\nSince $s_j$ is a non-zero integer, $\\sin(2\\pi s_j) = 0$. Thus, $\\mathbb{E}[\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = 0$.\nThe product of these expectations is therefore $0$ for any $d \\ge 1$.\n\nFor the second expectation term, we use the identity $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\n\\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\mathbb{E}\\left[\\frac{1 + \\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})}{2}\\right] = \\frac{1}{2} \\left(1 + \\mathbb{E}[\\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})] \\right)\n$$\nThe expectation $\\mathbb{E}[\\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})]$ is calculated similarly:\n$$\n\\mathbb{E}[\\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\int_{0}^{\\Delta_j} \\cos(4\\pi s_{j}\\Delta_{j}^{-1} u) \\frac{1}{\\Delta_j} du = \\frac{1}{4\\pi s_j}[\\sin(4\\pi s_{j}) - \\sin(0)] = 0\n$$\nsince $s_j$ is an integer.\nTherefore,\n$$\n\\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\frac{1}{2}(1 + 0) = \\frac{1}{2}\n$$\nThis result holds for each $j$. The product of these expectations is:\n$$\n\\prod_{j=1}^{d} \\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\prod_{j=1}^{d} \\frac{1}{2} = \\left(\\frac{1}{2}\\right)^d = 2^{-d}\n$$\nSubstituting these results back into the expression for $\\mathbb{E}[y^2]$:\n$$\n\\mathbb{E}[y^2] = a^{2} \\left( 1 - 2(0) + 2^{-d} \\right) = a^{2}(1 + 2^{-d})\n$$\nThis is the final closed-form expression for the expected squared measurement, which is strictly positive and demonstrates that the randomized measurement, unlike separable grid sampling, is not blind to the signal $x(\\mathbf{t})$.",
            "answer": "$$\n\\boxed{a^{2}(1 + 2^{-d})}\n$$"
        },
        {
            "introduction": "Our final practice bridges the gap between theoretical principles and computational reality, exploring how different sampling strategies perform in a high-dimensional Fourier setting. You will implement estimators based on the concept of mutual coherence to quantitatively compare the sparse recovery quality of uniform random sampling versus structured, variable-density sampling . This exercise demonstrates how theoretical metrics can inform practical decisions in measurement design and reveals important trade-offs between different sampling philosophies.",
            "id": "3434286",
            "problem": "Consider a discrete $d$-dimensional signal of size $N$ per axis, so the ambient dimension is $n=N^d$. Classical uniform-grid sampling for exact Fourier-domain reconstruction requires $n$ samples, which becomes infeasible in high dimensions when $n$ is extremely large. Compressed sensing proposes exact recovery of $s$-sparse signals from $m \\ll n$ measurements under appropriate measurement design and sparsity assumptions. Your objective is to quantify, via principled estimates grounded in core definitions and well-tested results, the breakdown of classical sampling and to compare the expected sparse recovery quality for two Fourier sampling strategies in $d=4$ dimensions: uniform random sampling and separable variable-density sampling.\n\nFundamental base:\n- A partial discrete Fourier transform measurement model selects $m$ rows (frequencies) from the $n \\times n$ unitary discrete Fourier transform matrix and observes the corresponding linear measurements of an $s$-sparse vector $x \\in \\mathbb{C}^n$.\n- Mutual coherence $\\mu$ of a measurement matrix (with columns normalized) is defined as the maximum absolute normalized inner product between any two distinct columns, and is a well-tested proxy for sparse recovery guarantees.\n- A classical sufficient condition for exact recovery of $s$-sparse vectors (in the absence of noise) via convex programs such as Basis Pursuit or greedy methods such as Orthogonal Matching Pursuit is the inequality $s < \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$, which connects recovery to the mutual coherence.\n\nSet-up:\n- Dimension is $d=4$ with side length $N=128$, so the ambient dimension is $n=128^4$.\n- Two sampling strategies are considered:\n  1. Uniform random sampling: Each Fourier frequency (row) is chosen uniformly and independently at random from the $4$-dimensional frequency grid.\n  2. Variable-density sampling: The selection probability across the $4$-dimensional frequency grid is separable and proportional to a power law per axis. Specifically, define the one-dimensional frequency index set $\\mathcal{K}=\\{-\\frac{N}{2},\\ldots,-1,0,1,\\ldots,\\frac{N}{2}-1\\}$. The one-dimensional weights are $w(k)=(1+|k|)^{-\\alpha}$ for $k \\in \\mathcal{K}$, normalized to a probability mass function $p_1(k)=w(k)/\\sum_{k'\\in\\mathcal{K}} w(k')$. The $4$-dimensional sampling probability is $p(k_1,k_2,k_3,k_4)=\\prod_{r=1}^{4} p_1(k_r)$.\n\nEstimation task:\n- The mutual coherence for uniform sampling is driven by random correlations between columns (normalized $m$-length vectors of complex exponentials restricted to the selected rows). For uniform independent sampling, the maximum absolute normalized inner product across distinct column differences behaves like the maximum of $M$ sub-Gaussian random sums of unit-modulus complex phases, where $M$ is the number of distinct column-difference patterns. A conservative and well-tested union-bound-based estimate yields the order-of-magnitude proxy\n  $$\\mu_{\\text{unif}} \\approx \\sqrt{\\frac{2 \\ln n}{m}},$$\n  using natural logarithms and $M \\approx n$. This captures the breakdown that, for fixed $m$, the coherence degrades with growing $n$.\n- For variable-density separable sampling, the expected normalized inner product between two columns whose spatial indices differ by the $4$-vector $\\Delta=(\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4)$ equals the product across dimensions of the one-dimensional discrete Fourier transform of $p_1$ evaluated at the offsets:\n  $$\\mathbb{E}\\left[\\langle a_i, a_j \\rangle\\right] = \\prod_{r=1}^{4} \\phi(\\Delta_r), \\quad \\text{where} \\quad \\phi(t)=\\sum_{k \\in \\mathcal{K}} p_1(k)\\, e^{-2\\pi i k t / N}.$$\n  Because $\\phi(0)=1$ and $|\\phi(t)|$ typically decreases with $|t|$, the dominant nonzero correlation occurs for differences with one coordinate equal to $1$ and the others zero, so a principled proxy for the variable-density mean coherence is $|\\phi(1)|$. Finite-sample randomness adds fluctuations of the same scale as the uniform case. Thus an operational estimate is\n  $$\\mu_{\\text{var}} \\approx |\\phi(1)| + \\sqrt{\\frac{2 \\ln n}{m}},$$\n  clipped to $1$ if necessary.\n- From mutual coherence $\\mu$, define the coherence-based sparsity threshold\n  $$s_{\\max}(\\mu) = \\left\\lfloor \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right) \\right\\rfloor.$$\n- Define a dimensionless recovery-quality score for a given $(n,s,m)$ and sampling strategy as the ratio\n  $$Q = \\frac{s_{\\max}(\\mu)}{s}.$$\n  Values $Q \\geq 1$ indicate that the sufficient coherence-based guarantee admits exact recovery at the target sparsity $s$; values $Q < 1$ indicate that the guarantee fails at $s$, revealing the breakdown in high dimensions for given $m$.\n\nProgramming task:\n- Implement the above estimators for $\\mu_{\\text{unif}}$ and $\\mu_{\\text{var}}$ and compute $Q$ for the provided test suite. Use $d=4$ and $N=128$ throughout. For the variable-density case, use $\\alpha=2$ (dimensionless).\n- Test suite (all with $d=4$, $N=128$, and $\\alpha=2$):\n  1. General high-dimensional case illustrating breakdown: $n=128^4$, $s=2 \\cdot 10^4$, $m=5 \\cdot 10^4$.\n  2. Boundary-condition case calibrated near the uniform coherence threshold: $s=20$, $m$ chosen so that $\\mu_{\\text{unif}}\\approx \\frac{1}{2s-1}$; specifically use $m=\\left\\lfloor \\frac{2 \\ln n}{(1/(2s-1))^2} \\right\\rfloor$.\n  3. Happy-path case for uniform sampling: $s=10$, $m=5 \\cdot 10^6$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list $[Q_{\\text{unif}}, Q_{\\text{var}}]$ with each float rounded to six decimal places. For example, the format is like\n  $$[[Q_{\\text{unif,1}},Q_{\\text{var,1}}],[Q_{\\text{unif,2}},Q_{\\text{var,2}}],[Q_{\\text{unif,3}},Q_{\\text{var,3}}]].$$\nNo physical units or angles are involved, and all values must be reported as dimensionless floats in the specified format.",
            "solution": "The problem posed is to quantify the breakdown of classical sampling in high dimensions and compare the sparse recovery capabilities of two different Fourier sampling strategies within the framework of compressed sensing. This requires a multi-step calculation based on a set of provided theoretical models and estimators. The problem is scientifically grounded in the established principles of compressed sensing and sparse signal recovery, it is mathematically well-posed, and all necessary parameters and formulae are provided. Therefore, the problem is valid.\n\nThe core of the problem lies in the tension between the ambient dimension of a signal, $n$, the number of measurements, $m$, and the signal's sparsity, $s$. Classical signal processing, governed by the Nyquist-Shannon sampling theorem, requires a number of samples on the order of the signal's total size, $n$. For a $d$-dimensional signal on a grid of size $N$ per axis, the ambient dimension is $n=N^d$. For the given parameters of $d=4$ and $N=128$, the total number of signal elements is $n=128^4 = 268,435,456$, which is an astronomically large number, making classical sampling infeasible.\n\nCompressed sensing (CS) offers a path to sub-Nyquist sampling, positing that if a signal $x \\in \\mathbb{C}^n$ is $s$-sparse (i.e., has only $s$ non-zero entries in some basis), it can be perfectly recovered from $m$ linear measurements, where $m$ can be dramatically smaller than $n$ ($m \\ll n$). The measurement process is modeled by the linear system $y = Ax$, where $y \\in \\mathbb{C}^m$ is the vector of measurements and $A \\in \\mathbb{C}^{m \\times n}$ is the measurement matrix. For this problem, $A$ is formed by selecting $m$ rows from the $n \\times n$ discrete Fourier transform (DFT) matrix.\n\nThe success of recovery algorithms, such as Basis Pursuit or Orthogonal Matching Pursuit, depends crucially on the properties of the measurement matrix $A$. A key property is the mutual coherence, $\\mu$, defined as the maximum absolute inner product between any two distinct, normalized columns of $A$. A small coherence is desirable. A well-known sufficient condition for the exact recovery of any $s$-sparse signal is given by $s < \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$. This inequality connects the geometry of the measurement matrix (via $\\mu$) to the maximum recoverable sparsity level. To quantify recovery quality, we first define the theoretical maximum recoverable sparsity for a given matrix as $s_{\\max}(\\mu_A) = \\left\\lfloor \\frac{1}{2}\\left(1 + \\frac{1}{\\mu_A}\\right) \\right\\rfloor$. We then define a dimensionless recovery quality score, $Q = \\frac{s_{\\max}}{s}$. A score of $Q \\ge 1$ indicates that the recovery guarantee is met for a signal of sparsity $s$.\n\nWe will now analyze the two proposed sampling strategies using the estimators provided. The fixed parameters are the dimension $d=4$, side length $N=128$, and power-law exponent $\\alpha=2$. This gives an ambient dimension of $n=128^4$ and a natural logarithm of $\\ln n = 4 \\ln 128 = 28 \\ln 2 \\approx 19.4081$.\n\n**1. Uniform Random Sampling**\nIn this strategy, $m$ Fourier frequencies are selected uniformly and independently at random. The columns of the resulting measurement matrix are pseudo-random vectors. The coherence is limited by random correlations, and a well-tested estimate is provided:\n$$\n\\mu_{\\text{unif}} \\approx \\sqrt{\\frac{2 \\ln n}{m}}\n$$\nThis estimate reveals a fundamental challenge of high-dimensional problems: for a fixed number of measurements $m$, the coherence degrades (increases) as the ambient dimension $n$ grows, making recovery harder.\n\n**2. Variable-Density Sampling (VDS)**\nThis strategy samples frequencies non-uniformly, with a higher probability for low frequencies. The sampling probability is separable and follows a power law. The $d$-dimensional probability $p(k_1, k_2, k_3, k_4)$ is the product of one-dimensional probability mass functions $p_1(k_r)$, where $p_1(k) \\propto (1+|k|)^{-\\alpha}$ for $k$ in the one-dimensional frequency set $\\mathcal{K}=\\{-\\frac{N}{2}, \\dots, \\frac{N}{2}-1\\}$. This non-uniformity introduces a systematic correlation between the columns of the measurement matrix. The provided proxy for the mutual coherence is:\n$$\n\\mu_{\\text{var}} \\approx |\\phi(1)| + \\sqrt{\\frac{2 \\ln n}{m}}\n$$\nwhere the additional term $|\\phi(1)|$ represents this systematic correlation. It is derived from the discrete Fourier transform of the $1D$ sampling probability distribution, $\\phi(t)=\\sum_{k \\in \\mathcal{K}} p_1(k)\\, e^{-2\\pi i k t / N}$. The term $\\sqrt{\\frac{2 \\ln n}{m}}$ represents the same random fluctuations as in the uniform case. For the given parameters $N=128$ and $\\alpha=2$, the value of $|\\phi(1)|$ is calculated as\n$$\n|\\phi(1)| = \\left| \\frac{\\sum_{k=-64}^{63} (1+|k|)^{-2} e^{-2\\pi i k / 128}}{\\sum_{k'=-64}^{63} (1+|k'|)^{-2}} \\right| \\approx 0.957597\n$$\nWithin this model, VDS is expected to have a higher coherence than uniform sampling, because $|\\phi(1)| > 0$. This suggests poorer recovery performance as judged by this specific general-purpose metric. This result highlights that while VDS is powerful for signals that are compressible in the Fourier domain (e.g., natural images), it may not be optimal under a strict sparsity model in the spatial domain when judged by the mutual coherence criterion, which is sensitive to worst-case column correlations.\n\nWe now proceed to compute the quality score $Q$ for each test case.\n\n**Test Case 1: High-dimensional breakdown**\nGiven: $s = 20,000$, $m = 50,000$.\n- Uniform: $\\mu_{\\text{unif}} \\approx \\sqrt{\\frac{2 \\ln(128^4)}{50,000}} \\approx 0.027862$.\n  $s_{\\max}(\\mu_{\\text{unif}}) = \\lfloor \\frac{1}{2}(1 + 1/0.027862) \\rfloor = 18$.\n  $Q_{\\text{unif}} = 18 / 20,000 = 0.0009$.\n- VDS: $\\mu_{\\text{var}} \\approx 0.957597 + 0.027862 \\approx 0.985459$.\n  $s_{\\max}(\\mu_{\\text{var}}) = \\lfloor \\frac{1}{2}(1 + 1/0.985459) \\rfloor = 1$.\n  $Q_{\\text{var}} = 1 / 20,000 = 0.00005$.\nBoth scores are far below $1$, illustrating the failure of the recovery guarantee when $m$ is too small for the given sparsity $s$ in such a high dimension.\n\n**Test Case 2: Boundary condition**\nGiven: $s = 20$. The number of measurements $m$ is chosen to place uniform sampling at the theoretical recovery threshold: $m = \\lfloor (2 \\ln n)(2s-1)^2 \\rfloor = \\lfloor (2\\ln(128^4))(39)^2 \\rfloor = 59,039$.\n- Uniform: $\\mu_{\\text{unif}} \\approx \\sqrt{\\frac{2 \\ln(128^4)}{59,039}} \\approx 0.025641 \\approx 1/39$.\n  $s_{\\max}(\\mu_{\\text{unif}}) = \\lfloor \\frac{1}{2}(1 + 1/0.025641) \\rfloor = \\lfloor \\frac{1}{2}(1+39) \\rfloor = 20$.\n  $Q_{\\text{unif}} = 20 / 20 = 1.0$.\n- VDS: $\\mu_{\\text{var}} \\approx 0.957597 + 0.025641 \\approx 0.983238$.\n  $s_{\\max}(\\mu_{\\text{var}}) = \\lfloor \\frac{1}{2}(1 + 1/0.983238) \\rfloor = 1$.\n  $Q_{\\text{var}} = 1 / 20 = 0.05$.\nAs designed, uniform sampling is exactly at the recovery threshold ($Q=1$), while VDS fails significantly.\n\n**Test Case 3: Happy path**\nGiven: $s = 10$, $m = 5,000,000$.\n- Uniform: $\\mu_{\\text{unif}} \\approx \\sqrt{\\frac{2 \\ln(128^4)}{5,000,000}} \\approx 0.002786$.\n  $s_{\\max}(\\mu_{\\text{unif}}) = \\lfloor \\frac{1}{2}(1 + 1/0.002786) \\rfloor = 179$.\n  $Q_{\\text{unif}} = 179 / 10 = 17.9$.\n- VDS: $\\mu_{\\text{var}} \\approx 0.957597 + 0.002786 \\approx 0.960383$.\n  $s_{\\max}(\\mu_{\\text{var}}) = \\lfloor \\frac{1}{2}(1 + 1/0.960383) \\rfloor = 1$.\n  $Q_{\\text{var}} = 1 / 10 = 0.1$.\nWith a large number of measurements, the uniform sampling strategy provides a strong guarantee ($Q \\gg 1$), whereas the VDS strategy's performance remains poor due to its high intrinsic coherence, according to this model.\nThe final computed values are collated in the program below.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the estimators for sparse recovery quality scores as defined\n    in the problem statement and computes them for the given test suite.\n    \"\"\"\n\n    #\n    # Define fixed parameters from the problem statement.\n    #\n    d = 4                 # Dimension\n    N = 128               # Side length\n    alpha = 2.0           # Power-law exponent for VDS\n    n = float(N**d)       # Ambient dimension\n    log_n = np.log(n)     # Natural logarithm of n\n\n    #\n    # Pre-compute the constant term |phi(1)| for variable-density sampling.\n    # phi(t) is the characteristic function of the 1D sampling probability distribution.\n    #\n    \n    # 1D frequency index set K = {-N/2, ..., N/2-1}\n    k_values = np.arange(-N / 2, N / 2, dtype=np.float64)\n    \n    # 1D weights w(k) = (1+|k|)^-alpha\n    weights = (1 + np.abs(k_values))**(-alpha)\n    \n    # 1D probability mass function p1(k) = w(k) / sum(w)\n    sum_weights = np.sum(weights)\n    p1_k = weights / sum_weights\n    \n    # phi(1) = sum_{k in K} p1(k) * exp(-2*pi*i*k/N)\n    phi1 = np.sum(p1_k * np.exp(-2j * np.pi * k_values / N))\n    phi1_abs = np.abs(phi1)\n\n    #\n    # Define the test suite based on the problem statement.\n    #\n    s_case2 = 20.0\n    # For case 2, m is calibrated to meet the uniform coherence threshold.\n    m_case2 = np.floor(2 * log_n * (2 * s_case2 - 1)**2)\n    \n    test_cases = [\n        (2.0e4, 5.0e4),      # Case 1: General high-dimensional breakdown\n        (s_case2, m_case2),  # Case 2: Boundary condition\n        (10.0, 5.0e6)        # Case 3: Happy-path\n    ]\n\n    #\n    # Define helper functions for the core calculations.\n    #\n    def calculate_s_max(mu):\n        \"\"\"Computes the coherence-based sparsity threshold.\"\"\"\n        if mu <= 1e-12: # mu is non-negative. Avoid division by zero/small numbers.\n            return float('inf')\n        return np.floor(0.5 * (1.0 + 1.0 / mu))\n\n    def calculate_Q(s_max_val, s_val):\n        \"\"\"Computes the dimensionless recovery-quality score.\"\"\"\n        return s_max_val / s_val\n\n    #\n    # Process each test case and store the results.\n    #\n    results = []\n    for s, m in test_cases:\n        # Common random fluctuation term in coherence estimates\n        rand_term = np.sqrt(2 * log_n / m)\n\n        # 1. Uniform random sampling calculations\n        mu_unif = rand_term\n        s_max_unif = calculate_s_max(mu_unif)\n        Q_unif = calculate_Q(s_max_unif, s)\n\n        # 2. Variable-density sampling calculations\n        mu_var_est = phi1_abs + rand_term\n        mu_var = min(mu_var_est, 1.0) # Coherence is clipped at 1\n        s_max_var = calculate_s_max(mu_var)\n        Q_var = calculate_Q(s_max_var, s)\n\n        # Append the pair of rounded Q-scores to the results list\n        results.append([round(Q_unif, 6), round(Q_var, 6)])\n\n    #\n    # Final print statement in the exact required format.\n    # The output is a list of lists, formatted as a string.\n    #\n    print(f\"[[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}