{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on exploration with two foundational techniques for stabilizing ill-posed inverse problems: Tikhonov regularization and Truncated Singular Value Decomposition (TSVD). Both methods operate by mitigating the noise amplification caused by small singular values, but they do so in distinct ways: Tikhonov applies a smooth filter that dampens noisy components, whereas TSVD imposes a sharp cutoff. This practice provides a direct, computational comparison of their behaviors, guiding you to construct a numerical experiment where a 'spectral gap' in the data reveals the practical consequences of these different filtering strategies .",
            "id": "3599470",
            "problem": "Consider solving a linear inverse problem in the sense of numerical linear algebra: given a matrix $A \\in \\mathbb{R}^{n \\times n}$, an unknown vector $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, deterministic data noise $\\eta \\in \\mathbb{R}^{n}$, and observed data $b = A x_{\\mathrm{true}} + \\eta$, compare two regularization strategies: Tikhonov regularization and truncated singular value decomposition. The goal is to compute the relative solution errors for both strategies over a set of test cases and to highlight a design where truncated singular value decomposition can outperform Tikhonov regularization due to spectral gaps.\n\nBase definitions and requirements:\n- For Tikhonov regularization with regularization parameter $\\lambda > 0$, the solution is the minimizer of the strictly convex objective $\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$, which is the unique solution to the normal equations $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$.\n- For truncated singular value decomposition (TSVD), define the singular value decomposition $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$. For a truncation index $k \\in \\{0,1,\\dots,n\\}$, the TSVD solution is $x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}$. In this problem, the truncation index $k$ is determined by the rule $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$, with the convention that if the set is empty, then $k = 0$.\n- The relative error of a candidate solution $\\widehat{x}$ with respect to $x_{\\mathrm{true}}$ is $\\|\\widehat{x} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n\nImplementation constraints and specialization for testability:\n- In all test cases, take $n = 10$ and choose $A$ to be diagonal with descending positive diagonal entries, so that $U = I$, $V = I$, and the singular values are the diagonal entries of $A$. This makes $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ and $A^{\\top} A = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{n}^{2})$.\n- For each test case, define the noise vector deterministically by $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$ for $i = 1,2,\\dots,n$.\n- For Tikhonov, the explicit componentwise expression derived from the normal equations in this diagonal setting is $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$ for $i = 1,2,\\dots,n$.\n- For TSVD, given $k$ by the above selection rule, the explicit componentwise expression in this diagonal setting is $x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}, & i \\le k, \\\\ 0, & i > k. \\end{cases}$\n\nTest suite:\n- Test case $\\#1$ (spectral gap; designed to favor truncated singular value decomposition): \n  - $n = 10$.\n  - Singular values $\\sigma = [1.0, 0.6, 0.36, 0.216, 0.1296, 10^{-3}, 5 \\cdot 10^{-4}, 2 \\cdot 10^{-4}, 10^{-4}, 5 \\cdot 10^{-5}]$.\n  - True solution $x_{\\mathrm{true}} = [1, -\\tfrac{1}{2}, \\tfrac{1}{4}, -\\tfrac{1}{8}, \\tfrac{1}{16}, 0, 0, 0, 0, 0]$.\n  - Regularization parameter $\\lambda = 10^{-5}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#2$ (smooth spectrum; no pronounced gap):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{-4}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#3$ (boundary condition with very small regularization):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{-12}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#4$ (boundary condition with very large regularization):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{1}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n\nTasks to implement for each test case:\n- Construct $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$, $x_{\\mathrm{true}}$, $\\eta$ with $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$, and $b = A x_{\\mathrm{true}} + \\eta$.\n- Compute the Tikhonov solution $x_{\\mathrm{tik}}$ using $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$.\n- Compute the truncation index $k = \\max\\{ i : \\sigma_{i}^{2} \\ge \\lambda \\}$ with the convention $k = 0$ if the set is empty.\n- Compute the truncated singular value decomposition solution $x_{\\mathrm{tsvd}}^{(k)}$ using $x_{\\mathrm{tsvd},i}^{(k)} = b_{i}/\\sigma_{i}$ for $i \\le k$ and $x_{\\mathrm{tsvd},i}^{(k)} = 0$ for $i > k$.\n- Compute the relative $2$-norm errors $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ and $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n- Also compute the boolean $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}} < e_{\\mathrm{tik}})$ to indicate whether truncated singular value decomposition strictly outperforms Tikhonov in that test.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a list of the form $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$. The final output should therefore be a list with four inner lists, one per test case, in order of test cases $\\#1$ through $\\#4$, for example, $[[k_{1}, e_{\\mathrm{tsvd},1}, e_{\\mathrm{tik},1}, b_{\\mathrm{adv},1}], [k_{2}, e_{\\mathrm{tsvd},2}, e_{\\mathrm{tik},2}, b_{\\mathrm{adv},2}], [k_{3}, e_{\\mathrm{tsvd},3}, e_{\\mathrm{tik},3}, b_{\\mathrm{adv},3}], [k_{4}, e_{\\mathrm{tsvd},4}, e_{\\mathrm{tik},4}, b_{\\mathrm{adv},4}]]$.\n\nNotes:\n- There are no physical units; all computations are in dimensionless floating-point arithmetic.\n- Angles are not used.\n- Express all booleans as language-native boolean values and all errors as floating-point numbers.",
            "solution": "The problem presented requires the comparison of two standard regularization techniques for solving ill-posed linear inverse problems: Tikhonov regularization and Truncated Singular Value Decomposition (TSVD). The validation confirms that the problem is scientifically sound, well-posed, and provides a clear, self-contained set of instructions and data for a numerical experiment. All definitions and formulas are consistent with the established literature in numerical linear algebra. We may therefore proceed with a solution.\n\nThe core of the problem lies in solving the linear system $A x = b$ where the matrix $A$ is ill-conditioned and the data vector $b$ is corrupted by noise $\\eta$. The model is given by $b = A x_{\\mathrm{true}} + \\eta$, where $x_{\\mathrm{true}}$ is the ground truth solution we seek to approximate. A naive attempt to solve for $x$ via $x = A^{-1} b$ would result in $x = x_{\\mathrm{true}} + A^{-1} \\eta$. Since $A$ is ill-conditioned, its inverse $A^{-1}$ has a very large norm, leading to extreme amplification of the noise term $\\eta$. Regularization methods are designed to counteract this by introducing a controlled bias to the solution in exchange for a dramatic reduction in variance due to noise.\n\nThe problem simplifies the analysis by considering a diagonal matrix $A = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$, where the diagonal entries $\\sigma_i > 0$ are the singular values of $A$. In the general case, any matrix $A$ has a Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$. The choice of a diagonal $A$ is equivalent to working in a basis where the SVD is trivial ($U=V=I$), allowing us to focus on how the singular values themselves are handled by each regularization method.\n\n**Tikhonov Regularization**\n\nTikhonov regularization recasts the problem as an optimization problem, seeking a solution $x$ that minimizes a combination of the data fidelity term and a penalty on the solution norm:\n$$ x_{\\mathrm{tik}} = \\arg\\min_{x} \\left( \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2} \\right) $$\nThe parameter $\\lambda > 0$ controls the trade-off. The unique minimizer $x_{\\mathrm{tik}}$ is found by solving the associated normal equations: $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$. For our diagonal matrix $A=\\mathrm{diag}(\\sigma_i)$, this system decouples into $n$ scalar equations:\n$$ (\\sigma_{i}^{2} + \\lambda) x_{\\mathrm{tik},i} = \\sigma_{i} b_{i} \\quad \\text{for } i \\in \\{1, \\dots, n\\} $$\nThis gives the explicit component-wise formula for the Tikhonov solution:\n$$ x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i} $$\nThe term $f_{i}^{\\mathrm{tik}} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$ can be viewed as a \"filter factor\". It smoothly attenuates components associated with small singular values. If $\\sigma_i^2 \\gg \\lambda$, then $f_{i}^{\\mathrm{tik}} \\approx 1$, and the component is largely unchanged. If $\\sigma_i^2 \\ll \\lambda$, then $f_{i}^{\\mathrm{tik}} \\approx 0$, and the component is suppressed.\n\n**Truncated Singular Value Decomposition (TSVD)**\n\nTSVD takes a more direct approach by constructing the solution using only the \"significant\" singular components. The general TSVD solution is given by a truncated sum:\n$$ x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\nwhere $k$ is the truncation index, determining how many components are included. In our diagonal case ($U=I, V=I$), this simplifies to:\n$$ x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i} & \\text{if } i \\le k \\\\ 0 & \\text{if } i > k \\end{cases} $$\nThis corresponds to filter factors $f_{i}^{\\mathrm{tsvd}}$ that form a step function: $f_{i}^{\\mathrm{tsvd}} = 1$ for $i \\le k$ and $f_{i}^{\\mathrm{tsvd}} = 0$ for $i > k$. The problem links the choice of $k$ to the Tikhonov parameter $\\lambda$ via the rule $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$, with $k=0$ if the set is empty. This rule essentially keeps all components for which the Tikhonov filter factor would be at least $1/2$.\n\n**Comparison and the Role of Spectral Gaps**\n\nThe fundamental difference lies in their filter functions: Tikhonov's is smooth, while TSVD's is sharp. Test case $\\#1$ is specifically designed to highlight a scenario where TSVD's sharp cutoff is advantageous. It features a \"spectral gap,\" where the singular values show a large drop between $\\sigma_5$ and $\\sigma_6$. The regularization parameter $\\lambda=10^{-5}$ is chosen to lie within this gap (i.e., $\\sigma_5^2 \\gg \\lambda \\gg \\sigma_6^2$). Furthermore, the true solution $x_{\\mathrm{true}}$ has its information content restricted to the first $5$ components.\n\nUnder these conditions, the truncation rule for TSVD yields $k=5$. TSVD therefore retains the first $5$ components (where the signal lies) and completely discards the remaining components (which contain only noise, as $x_{\\mathrm{true},i}=0$ for $i>5$). This acts as a perfect filter for this specific problem structure. In contrast, Tikhonov regularization applies its smooth filter to all components. While it heavily suppresses components $i > 5$, it still allows a small, filtered amount of noise to pass through. More importantly, it also slightly damps components $i \\le 5$, introducing a regularization error that TSVD does not have forthese components. This leads to TSVD outperforming Tikhonov.\n\nFor the other test cases with smoother spectral decay, the sharp cutoff of TSVD can be detrimental. If $x_{\\mathrm{true}}$ contains significant energy in components that TSVD truncates (because their $\\sigma_i$ are small), it will incur a large regularization error. Tikhonov's gentle damping of these components can result in a better overall approximation.\n\n**Computational Steps**\n\nFor each of the four test cases, the following procedure is implemented:\n1.  Initialize the parameters: $n=10$, singular values $\\sigma$, true solution $x_{\\mathrm{true}}$, regularization parameter $\\lambda$, and noise level.\n2.  Construct the noise vector $\\eta$ where $\\eta_{j} = \\mathrm{noise\\_level} \\cdot (-1)^{j+1}$ for the $0$-based index $j \\in \\{0, \\dots, 9\\}$.\n3.  Compute the data vector $b = \\sigma \\odot x_{\\mathrm{true}} + \\eta$, where $\\odot$ denotes element-wise multiplication.\n4.  Calculate the Tikhonov solution vector $x_{\\mathrm{tik}}$ using its component-wise formula.\n5.  Determine the TSVD truncation index $k$ based on the provided rule.\n6.  Calculate the TSVD solution vector $x_{\\mathrm{tsvd}}^{(k)}$.\n7.  Compute the relative $2$-norm errors for both solutions: $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ and $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n8.  Evaluate the boolean condition $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}} < e_{\\mathrm{tik}})$.\nThe collected results $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$ for each case are then reported.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(sigma_vals, xtrue_vals, lam, noise_level):\n    \"\"\"\n    Runs a single test case for comparing Tikhonov and TSVD regularization.\n    \"\"\"\n    n = 10\n    sigma = np.array(sigma_vals, dtype=float)\n    xtrue = np.array(xtrue_vals, dtype=float)\n\n    # Construct the noise vector eta and observed data b\n    # The problem uses 1-based indexing i=1,...,n. Python uses 0-based j=0,...,n-1.\n    # eta_i = noise_level * (-1)^i translates to eta[j] = noise_level * (-1)**(j+1)\n    indices_1_based = np.arange(1, n + 1)\n    eta = noise_level * ((-1) ** indices_1_based)\n    b = sigma * xtrue + eta\n\n    # Compute the Tikhonov solution\n    xtik = (sigma / (sigma**2 + lam)) * b\n\n    # Determine the TSVD truncation index k\n    # k = max{ i in {1..n} : sigma_i^2 >= lam }\n    # np.where returns 0-based indices. k needs to be a 1-based count.\n    valid_indices = np.where(sigma**2 >= lam)[0]\n    if len(valid_indices) == 0:\n        k = 0\n    else:\n        k = int(np.max(valid_indices) + 1)\n\n    # Compute the TSVD solution\n    xtsvd = np.zeros(n)\n    if k > 0:\n        # Slicing with :k works correctly for 0-based index up to k-1.\n        xtsvd[:k] = b[:k] / sigma[:k]\n\n    # Compute the relative 2-norm errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    # This problem guarantees norm_xtrue > 0, so no division-by-zero check is needed.\n    e_tsvd = np.linalg.norm(xtsvd - xtrue) / norm_xtrue\n    e_tik = np.linalg.norm(xtik - xtrue) / norm_xtrue\n\n    # Determine if TSVD has a strictly smaller error\n    b_adv = bool(e_tsvd < e_tik)\n    \n    return [k, e_tsvd, e_tik, b_adv]\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    # Test case #1: Spectral gap\n    case1 = {\n        \"sigma_vals\": [1.0, 0.6, 0.36, 0.216, 0.1296, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5],\n        \"xtrue_vals\": [1.0, -0.5, 0.25, -0.125, 0.0625, 0, 0, 0, 0, 0],\n        \"lam\": 1e-5,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #2: Smooth spectrum\n    n = 10\n    j_indices = np.arange(n)\n    sigma_smooth = 10**(-j_indices / 3.0)\n    xtrue_smooth = 0.8**j_indices\n    case2 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-4,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #3: Small regularization parameter\n    case3 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-12,\n        \"noise_level\": 1e-6\n    }\n\n    # Test case #4: Large regularization parameter\n    case4 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e1,\n        \"noise_level\": 1e-6\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            case[\"sigma_vals\"],\n            case[\"xtrue_vals\"],\n            case[\"lam\"],\n            case[\"noise_level\"]\n        )\n        results.append(result)\n\n    # Print in the specified format: [[k1, e_tsvd1, e_tik1, b_adv1], [k2, ...]]\n    # Python's default string representation for a list of lists matches the required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from classical $\\ell_2$ methods to the modern paradigm of sparse recovery, we now focus on the celebrated Lasso estimator, which employs an $\\ell_1$-norm penalty. Understanding how the $\\ell_1$ norm promotes sparsity requires a shift from the filtering perspective to the language of convex optimization. This exercise centers on deriving and interpreting the Karush-Kuhn-Tucker (KKT) optimality conditions for the Lasso objective, a task that requires the use of subgradient calculus to handle the non-differentiable penalty term . Mastering these conditions is essential, as they provide the fundamental theoretical explanation for why and how Lasso selects a sparse set of active predictors.",
            "id": "3452160",
            "problem": "Consider an ill-posed linear inverse problem in which $y \\in \\mathbb{R}^m$ is modeled as $y \\approx A x$ with $A \\in \\mathbb{R}^{m \\times n}$ and $m<n$. To stabilize the inversion and promote sparsity, consider the least absolute shrinkage and selection operator (Lasso) estimator\n$$\nx^* \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\,\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwith $\\lambda>0$. Using only the convexity of the objective, the subgradient definition for the $\\ell_1$ norm, and the first-order necessary and sufficient optimality condition for convex problems, identify which of the following statements correctly express the Karush-Kuhn-Tucker (KKT) optimality conditions and interpret them in terms of the support and sign pattern of $x^*$. Let $S \\triangleq \\{ i \\in \\{1,\\dots,n\\} : x_i^* \\neq 0 \\}$ and $S^c$ denote its complement.\n\nA. There exists $z \\in \\partial \\|x^*\\|_1$ such that $A^\\top (A x^* - y) + \\lambda z = 0$, equivalently $A^\\top (y - A x^*) = \\lambda z$. With $z_i = \\operatorname{sign}(x_i^*)$ for $i \\in S$ and $z_i \\in [-1,1]$ for $i \\in S^c$, it follows that for $i \\in S$,\n$$\nA_i^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_i^*),\n$$\nand for $i \\in S^c$,\n$$\n|A_i^\\top (y - A x^*)| \\le \\lambda.\n$$\n\nB. For any optimal $x^*$ and any $i \\in S^c$, the KKT conditions imply\n$$\n|A_i^\\top (y - A x^*)| \\ge \\lambda.\n$$\n\nC. The subgradient optimality enforces\n$$\nA_S^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_S^*)\n\\quad \\text{and} \\quad\nA_{S^c}^\\top (y - A x^*) = 0.\n$$\n\nD. If $A_S$ has full column rank and $x_{S^c}^* = 0$, then stationarity on $S$ is equivalent to\n$$\n(A_S^\\top A_S)\\, x_S^* = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*).\n$$\nMoreover, if in addition the strict dual feasibility condition $\\|A_{S^c}^\\top (y - A x^*)\\|_\\infty < \\lambda$ holds, then $x^*$ is the unique minimizer with support $S$ and sign pattern $\\operatorname{sign}(x_S^*)$.\n\nE. Any subgradient $z \\in \\partial \\|x^*\\|_1$ must satisfy $\\|z\\|_2 \\le 1$, and for all $i \\in S^c$ one must have $z_i = 0$.\n\nSelect all options that are correct.",
            "solution": "The user wants me to validate the problem statement concerning the optimality conditions of the Lasso estimator and then identify the correct statements from the given options.\n\n### Step 1: Extract Givens\n- **Problem Setup**: An ill-posed linear inverse problem modeled as $y \\approx A x$.\n- **Data and Dimensions**: $y \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, with $m < n$.\n- **Estimator**: The Lasso estimator is defined as the solution to the minimization problem:\n$$\nx^* \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\,\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1\n$$\n- **Parameter**: The regularization parameter $\\lambda > 0$.\n- **Definitions**:\n    - The support of the solution is $S \\triangleq \\{ i \\in \\{1,\\dots,n\\} : x_i^* \\neq 0 \\}$.\n    - $S^c$ is the complement of the support, i.e., $S^c \\triangleq \\{ i \\in \\{1,\\dots,n\\} : x_i^* = 0 \\}$.\n- **Constraints on Method**: Use only convexity, the subgradient definition for the $\\ell_1$ norm, and the first-order necessary and sufficient optimality condition.\n- **Goal**: Identify the correct statements about the Karush-Kuhn-Tucker (KKT) conditions and their interpretation.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is a cornerstone of modern statistics, machine learning, and signal processing, specifically in compressed sensing and sparse optimization. The Lasso formulation is a standard and well-studied topic. The problem is scientifically sound.\n- **Well-Posed**: The problem defines a convex optimization problem. The objective function is the sum of a differentiable convex function and a non-differentiable convex function. Since the function is convex and coercive (it grows to infinity as $\\|x\\| \\to \\infty$), a minimizer $x^*$ is guaranteed to exist. The question asks for properties of any such minimizer, which is a well-defined task.\n- **Objective**: The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n- **Completeness**: The problem provides all necessary information to derive the first-order optimality conditions.\n- **Consistency and Feasibility**: The setup is standard, with no contradictory or physically impossible conditions. The condition $m<n$ correctly frames it as an underdetermined problem where regularization is essential.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will proceed with deriving the solution and evaluating the options.\n\n### Derivation of Optimality Conditions\n\nThe objective function to minimize is $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1$.\nThis function is convex, being the sum of a convex quadratic function and the convex $\\ell_1$-norm (scaled by $\\lambda > 0$). For a convex function, a point $x^*$ is a global minimum if and only if the zero vector is contained in the subdifferential of the function at that point.\n$$ 0 \\in \\partial f(x^*) $$\nThe objective function is a sum of a differentiable function $g(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ and a non-differentiable function $h(x) = \\lambda \\|x\\|_1$. By the sum rule for subdifferentials (which applies here since $g(x)$ is differentiable), we have:\n$$ \\partial f(x^*) = \\nabla g(x^*) + \\partial h(x^*) $$\nThe gradient of the least-squares term $g(x)$ is:\n$$ \\nabla g(x) = A^\\top(Ax - y) $$\nThe subdifferential of the $\\ell_1$-norm term $h(x)$ is:\n$$ \\partial h(x) = \\lambda \\, \\partial \\|x\\|_1 $$\nThe subdifferential of the $\\ell_1$-norm, $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$, at a point $x^*$ is the set of all vectors $z \\in \\mathbb{R}^n$ with components $z_i$ satisfying:\n$$\nz_i =\n\\begin{cases}\n\\operatorname{sign}(x_i^*) & \\text{if } x_i^* \\neq 0 \\\\\nv_i \\in [-1, 1] & \\text{if } x_i^* = 0\n\\end{cases}\n$$\nUsing the notation for the support $S$ and its complement $S^c$, a vector $z$ is in $\\partial \\|x^*\\|_1$ if and only if:\n- For $i \\in S$, $z_i = \\operatorname{sign}(x_i^*)$.\n- For $i \\in S^c$, $z_i \\in [-1, 1]$.\n\nThe first-order optimality condition $0 \\in \\nabla g(x^*) + \\partial h(x^*)$ means that there must exist a subgradient vector $z \\in \\partial \\|x^*\\|_1$ such that:\n$$ A^\\top(Ax^* - y) + \\lambda z = 0 $$\nThis can be rearranged to:\n$$ A^\\top(y - Ax^*) = \\lambda z $$\nLet $A_i$ denote the $i$-th column of the matrix $A$. Writing the above vector equation component-wise gives:\n$$ A_i^\\top(y - Ax^*) = \\lambda z_i \\quad \\text{for each } i \\in \\{1, \\dots, n\\} $$\nWe now analyze this condition based on whether the index $i$ is in the support $S$:\n\n1.  **For $i \\in S$ (active set):** Since $x_i^* \\neq 0$, the subgradient component is uniquely determined as $z_i = \\operatorname{sign}(x_i^*)$. The condition becomes:\n    $$ A_i^\\top(y - Ax^*) = \\lambda \\operatorname{sign}(x_i^*) $$\n\n2.  **For $i \\in S^c$ (inactive set):** Since $x_i^* = 0$, the subgradient component $z_i$ can be any value in the interval $[-1, 1]$. The condition is $A_i^\\top(y - Ax^*) = \\lambda z_i$. Since $z_i \\in [-1, 1]$ and $\\lambda>0$, this is equivalent to:\n    $$ \\left| \\frac{A_i^\\top(y - Ax^*)}{\\lambda} \\right| \\le 1 \\implies |A_i^\\top(y - Ax^*)| \\le \\lambda $$\n\nThese are the KKT conditions for the Lasso problem.\n\n### Option-by-Option Analysis\n\n**A. There exists $z \\in \\partial \\|x^*\\|_1$ such that $A^\\top (A x^* - y) + \\lambda z = 0$, equivalently $A^\\top (y - A x^*) = \\lambda z$. With $z_i = \\operatorname{sign}(x_i^*)$ for $i \\in S$ and $z_i \\in [-1,1]$ for $i \\in S^c$, it follows that for $i \\in S$,\n$$\nA_i^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_i^*),\n$$\nand for $i \\in S^c$,\n$$\n|A_i^\\top (y - A x^*)| \\le \\lambda.\n$$**\nThis option presents the derivation of the KKT conditions exactly as performed above. It correctly states the subgradient optimality condition, correctly defines the subgradient of the $\\ell_1$-norm, and correctly interprets the conditions for indices in the support $S$ and its complement $S^c$.\n**Verdict: Correct.**\n\n**B. For any optimal $x^*$ and any $i \\in S^c$, the KKT conditions imply\n$$\n|A_i^\\top (y - A x^*)| \\ge \\lambda.\n$$**\nAs derived above, for $i \\in S^c$, the condition is $|A_i^\\top (y - A x^*)| \\le \\lambda$. The statement in this option presents the reverse inequality (except at the boundary where equality holds). It is possible for the inequality to be strict, i.e., $|A_i^\\top (y - A x^*)| < \\lambda$, which would violate the condition stated in this option. Therefore, this statement is false.\n**Verdict: Incorrect.**\n\n**C. The subgradient optimality enforces\n$$\nA_S^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_S^*)\n\\quad \\text{and} \\quad\nA_{S^c}^\\top (y - A x^*) = 0.\n$$**\nThe first part, $A_S^\\top (y - A x^*) = \\lambda \\operatorname{sign}(x_S^*)$, is a correct matrix notation for the condition $A_i^\\top(y - Ax^*) = \\lambda \\operatorname{sign}(x_i^*)$ for all $i \\in S$. The second part, $A_{S^c}^\\top (y - A x^*) = 0$, implies that $A_i^\\top(y - Ax^*) = 0$ for all $i \\in S^c$. This corresponds to the case where the subgradient components $z_i$ are all zero for $i \\in S^c$. However, the general condition is $|A_i^\\top(y - Ax^*)| \\le \\lambda$, which allows for non-zero values. The condition $A_{S^c}^\\top (y - A x^*) = 0$ is overly restrictive and not generally true.\n**Verdict: Incorrect.**\n\n**D. If $A_S$ has full column rank and $x_{S^c}^* = 0$, then stationarity on $S$ is equivalent to\n$$\n(A_S^\\top A_S)\\, x_S^* = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*).\n$$\nMoreover, if in addition the strict dual feasibility condition $\\|A_{S^c}^\\top (y - A x^*)\\|_\\infty < \\lambda$ holds, then $x^*$ is the unique minimizer with support $S$ and sign pattern $\\operatorname{sign}(x_S^*)$.**\n\nLet's analyze the first part. The stationarity condition on the support $S$ is $A_S^\\top(y - Ax^*) = \\lambda \\operatorname{sign}(x_S^*)$. We can decompose $Ax^*$ as $Ax^* = A_S x_S^* + A_{S^c} x_{S^c}^*$. Given that $x_{S^c}^* = 0$, this simplifies to $Ax^* = A_S x_S^*$. Substituting this into the stationarity condition yields $A_S^\\top(y - A_S x_S^*) = \\lambda \\operatorname{sign}(x_S^*)$. Expanding this gives $A_S^\\top y - A_S^\\top A_S x_S^* = \\lambda \\operatorname{sign}(x_S^*)$, which rearranges to $(A_S^\\top A_S)\\, x_S^* = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*)$. This is a correct derivation.\n\nNow for the second part. It makes a claim about uniqueness for a fixed support and sign pattern. Let's assume there is another minimizer $\\tilde{x}$ with the same support $S$ and sign pattern $\\operatorname{sign}(x_S^*)$. Then $\\tilde{x}_{S^c}=0$ and $\\operatorname{sign}(\\tilde{x}_S) = \\operatorname{sign}(x_S^*)$. It must also satisfy the equation derived above: $(A_S^\\top A_S)\\, \\tilde{x}_S = A_S^\\top y - \\lambda \\operatorname{sign}(\\tilde{x}_S) = A_S^\\top y - \\lambda \\operatorname{sign}(x_S^*)$. Subtracting the equations for $x_S^*$ and $\\tilde{x}_S$ gives $(A_S^\\top A_S)(x_S^* - \\tilde{x}_S) = 0$. Since we assumed $A_S$ has full column rank, the Gram matrix $A_S^\\top A_S$ is invertible. Therefore, $x_S^* - \\tilde{x}_S = 0$, which implies $x_S^* = \\tilde{x}_S$. As both solutions are zero outside the support $S$, we have $x^* = \\tilde{x}$. This proves uniqueness for the given support and sign pattern using only the full rank of $A_S$. The additional condition, $\\|A_{S^c}^\\top (y - A x^*)\\|_\\infty < \\lambda$, known as strict dual feasibility, is a standard condition (along with the full rank of $A_S$) for proving that $x^*$ is the *unique global minimizer* of the Lasso problem. Since this stronger set of conditions implies global uniqueness, it certainly implies the weaker conclusion stated in the option (uniqueness within a class). The statement is logically sound.\n**Verdict: Correct.**\n\n**E. Any subgradient $z \\in \\partial \\|x^*\\|_1$ must satisfy $\\|z\\|_2 \\le 1$, and for all $i \\in S^c$ one must have $z_i = 0$.**\nThe first part claims $\\|z\\|_2 \\le 1$. A subgradient $z \\in \\partial \\|x^*\\|_1$ has components that satisfy $|z_i| \\le 1$ for all $i=1,\\dots,n$. The squared $\\ell_2$-norm is $\\|z\\|_2^2 = \\sum_{i=1}^n z_i^2$. Since $z_i^2 \\le 1$, we have $\\|z\\|_2^2 \\le \\sum_{i=1}^n 1 = n$. Thus, $\\|z\\|_2 \\le \\sqrt{n}$. For $n>1$, it is not necessary that $\\|z\\|_2 \\le 1$. For example, if $n=2$ and $x^*=(1,1)^\\top$, then $z=(1,1)^\\top$ is the unique subgradient, and $\\|z\\|_2=\\sqrt{2}>1$. The correct norm bound is $\\|z\\|_\\infty \\le 1$. The first part of the statement is false.\nThe second part claims that for $i \\in S^c$, we must have $z_i=0$. As shown in the derivation, the condition for $i \\in S^c$ is that $z_i = \\lambda^{-1} A_i^\\top(y - Ax^*)$ and $|z_i| \\le 1$. There is no reason for $z_i$ to be zero in general. This part of the statement is also false.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{AD}$$"
        },
        {
            "introduction": "After building a solid foundation in $\\ell_1$ regularization, it is natural to ask whether its performance can be improved. This practice explores advanced, nonconvex penalties such as the Smoothly Clipped Absolute Deviation (SCAD) and the Minimax Concave Penalty (MCP), which were designed to overcome a key limitation of Lasso: the systematic bias it introduces in the estimates of large coefficients. By deriving the thresholding functions associated with these penalties, you will discover how they are able to shrink small coefficients while leaving large ones nearly untouched, thus providing asymptotically unbiased estimates. This exercise illuminates the trade-offs between the algorithmic simplicity of convex regularizers and the superior statistical properties of their nonconvex counterparts .",
            "id": "3452161",
            "problem": "Consider the one-dimensional penalized least-squares subproblem that arises in iterative methods for solving ill-posed inverse problems in compressed sensing:\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|),\n$$\nwhere $y \\in \\mathbb{R}$ is a scalar sufficient statistic (for example, the current back-projection of data in a proximal-gradient iteration when $A^{\\top}A \\approx I$), $\\lambda > 0$ is a regularization parameter, and $\\phi$ is a sparsity-promoting penalty applied elementwise. Use as your fundamental base the first-order optimality condition for a minimizer $x^{\\star}$ of the above objective, namely the subgradient equation for a separable one-dimensional penalty:\n$$\n0 \\in x^{\\star} - y + \\lambda \\, \\partial\\big(\\phi(|x^{\\star}|)\\big),\n$$\nand the facts that the penalties below are even functions of $x$, differentiable for $x \\neq 0$, and have piecewise-linear derivatives in $|x|$.\n\nDerive the explicit thresholding functions (closed-form proximal mappings) $T_{\\ell_{1}}(y)$, $T_{\\mathrm{SCAD}}(y)$, and $T_{\\mathrm{MCP}}(y)$ corresponding to the following penalties:\n1. The $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(|x|) = |x|$.\n2. The Smoothly Clipped Absolute Deviation (SCAD) penalty $\\phi_{\\mathrm{SCAD}}(|x|)$ with parameter $a > 2$, defined by its derivative with respect to $|x|$:\n$$\n\\phi_{\\mathrm{SCAD}}'(|x|) = \n\\begin{cases}\n1, & 0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a - |x|/\\lambda}{a - 1}, & \\lambda < |x| \\leq a\\lambda, \\\\\n0, & |x| > a\\lambda,\n\\end{cases}\n$$\nand extended to $x < 0$ by odd symmetry in $x$.\n3. The Minimax Concave Penalty (MCP) $\\phi_{\\mathrm{MCP}}(|x|)$ with parameter $\\gamma > 1$, defined by its derivative with respect to $|x|$:\n$$\n\\phi_{\\mathrm{MCP}}'(|x|) =\n\\begin{cases}\n1 - \\dfrac{|x|}{\\gamma \\lambda}, & 0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0, & |x| > \\gamma \\lambda,\n\\end{cases}\n$$\nand extended to $x < 0$ by odd symmetry in $x$.\n\nYour derivations must start from the stated optimality principle and proceed by case analysis on $|x|$ induced by the piecewise-linear derivatives. Clearly state the resulting thresholding functions in terms of $y$, $\\lambda$, $a$, and $\\gamma$, emphasizing their odd symmetry.\n\nThen, within the context of ill-posed inverse problems and regularization principles in compressed sensing, discuss—based on your derived thresholding functions—why nonconvex penalties such as SCAD and MCP may reduce bias relative to $\\ell_{1}$ for large signal coefficients, and articulate potential pitfalls including algorithmic nonconvexity, sensitivity to parameter choices, and implications for solution stability and uniqueness.\n\nFinally, evaluate all three thresholding functions at the parameter values $\\lambda = 1$, $a = 3.7$, $\\gamma = 3$, and the observation $y_{0} = 2.2$. Express the three values $T_{\\ell_{1}}(y_{0})$, $T_{\\mathrm{MCP}}(y_{0})$, and $T_{\\mathrm{SCAD}}(y_{0})$ as exact rational numbers when possible. If any value does not simplify to a rational number in closed form with the given parameters, leave it in exact symbolic form. No rounding is required.",
            "solution": "The problem asks for the derivation of three thresholding functions, a discussion of their properties, and a numerical evaluation. The problem is approached by solving the one-dimensional optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}} \\; f(x) = \\frac{1}{2}(x - y)^{2} + P(x),\n$$\nwhere $P(x) = \\lambda \\phi(|x|)$ is the respective penalty term ($\\ell_1$, SCAD, or MCP). The solution $x^{\\star}$ is the value that minimizes $f(x)$.\n\nThe first-order necessary condition for a minimum $x^{\\star}$ is given by the subgradient inclusion:\n$$\n0 \\in \\partial f(x^{\\star}) = x^{\\star} - y + \\partial P(x^{\\star}).\n$$\nThis can be rewritten as $y - x^{\\star} \\in \\partial P(x^{\\star})$. The penalty functions $P(x)$ are even, so $P(x) = \\tilde{P}(|x|)$ for some function $\\tilde{P}$ on $\\mathbb{R}_{\\ge 0}$. The subgradient $\\partial P(x)$ for $x \\neq 0$ is a singleton set containing the derivative, $\\partial P(x) = \\{ \\tilde{P}'(|x|) \\mathrm{sgn}(x) \\}$. At $x=0$, the subgradient is the interval $\\partial P(0) = [-\\tilde{P}'(0^+), \\tilde{P}'(0^+)]$.\n\nAn important property of the solution $x^\\star = T(y)$ is that it is an odd function of $y$, i.e., $T(-y) = -T(y)$. This is because replacing $y$ with $-y$ and $x$ with $-x$ in the objective function yields $\\frac{1}{2}(-x - (-y))^2 + P(-x) = \\frac{1}{2}(y-x)^2 + P(x)$, which is the original objective. Thus, if $x^\\star$ minimizes the objective for $y$, then $-x^\\star$ must minimize it for $-y$. This means we can derive the solution for $y > 0$ (which implies $x^{\\star} \\geq 0$) and then extend it to all $y \\in \\mathbb{R}$ by odd symmetry.\n\nFor $y > 0$, we have $x^{\\star} \\geq 0$. The derivative of the penalty term is $P'_{|x|}(|x|) = \\lambda \\phi'(|x|)$. The optimality condition simplifies:\n- If $x^{\\star} > 0$, then $y - x^{\\star} = P'_{|x|}(x^{\\star})$.\n- If $x^{\\star} = 0$, then $y \\in [ -P'_{|x|}(0^+), P'_{|x|}(0^+) ]$. Since $y > 0$, this becomes $0 < y \\leq P'_{|x|}(0^+)$.\n\nWe now apply this framework to each penalty.\n\n### 1. The $\\ell_{1}$ Penalty: $T_{\\ell_{1}}(y)$\nThe penalty is $P(x) = \\lambda |x|$. So $\\phi(|x|) = |x|$ and $\\phi'(|x|) = 1$ for $|x|>0$. The penalty derivative is $P'_{|x|}(|x|) = \\lambda$. The right-derivative at $0$ is $P'_{|x|}(0^+) = \\lambda$.\nLet $y > 0$.\n- If $x^{\\star}=0$, the condition is $0 < y \\leq \\lambda$.\n- If $x^{\\star} > 0$, the condition is $y - x^{\\star} = \\lambda$, which implies $x^{\\star} = y - \\lambda$. This is valid only if $x^{\\star} > 0$, meaning $y - \\lambda > 0$, or $y > \\lambda$.\nCombining these cases for $y>0$: $x^{\\star} = \\max(0, y-\\lambda)$.\nExtending by odd symmetry for any $y \\in \\mathbb{R}$:\n$$\nT_{\\ell_{1}}(y) = \\mathrm{sgn}(y) \\max(0, |y|-\\lambda).\n$$\nThis is the well-known soft-thresholding function.\n\n### 2. The SCAD Penalty: $T_{\\mathrm{SCAD}}(y)$\nThe derivative of the penalty is $P'_{|x|}(|x|) = \\lambda \\phi'_{\\mathrm{SCAD}}(|x|)$. With the given $\\phi'$, this is:\n$$\nP'_{|x|}(|x|) = \n\\begin{cases}\n\\lambda, & 0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{a - 1}, & \\lambda < |x| \\leq a\\lambda, \\\\\n0, & |x| > a\\lambda.\n\\end{cases}\n$$\nThe right-derivative at $0$ is $P'_{|x|}(0^+) = \\lambda$.\nLet $y > 0$, so $x^{\\star} \\geq 0$.\n- If $x^{\\star}=0$, the condition is $0 < y \\leq \\lambda$.\n- If $x^{\\star} > 0$, we have $y - x^{\\star} = P'_{|x|}(x^{\\star})$. We analyze this by cases on $x^{\\star}$:\n    - Case $0 < x^{\\star} \\leq \\lambda$: $y - x^{\\star} = \\lambda \\implies x^{\\star} = y - \\lambda$. This solution is valid for $0 < y - \\lambda \\leq \\lambda$, which corresponds to $\\lambda < y \\leq 2\\lambda$.\n    - Case $\\lambda < x^{\\star} \\leq a\\lambda$: $y - x^{\\star} = \\frac{a\\lambda - x^{\\star}}{a-1}$. Solving for $x^{\\star}$:\n      $(a-1)y - (a-1)x^{\\star} = a\\lambda - x^{\\star} \\implies (a-1)y - a\\lambda = (a-2)x^{\\star} \\implies x^{\\star} = \\frac{(a-1)y - a\\lambda}{a-2}$.\n      This solution is valid if $\\lambda < x^{\\star} \\leq a\\lambda$.\n      $\\lambda < \\frac{(a-1)y - a\\lambda}{a-2} \\implies (a-2)\\lambda < (a-1)y - a\\lambda \\implies (2a-2)\\lambda < (a-1)y \\implies y > 2\\lambda$.\n      $\\frac{(a-1)y - a\\lambda}{a-2} \\leq a\\lambda \\implies (a-1)y - a\\lambda \\leq a\\lambda(a-2) \\implies (a-1)y \\leq a^2\\lambda-a\\lambda \\implies y \\leq a\\lambda$.\n      So, this case holds for $2\\lambda < y \\leq a\\lambda$.\n    - Case $x^{\\star} > a\\lambda$: $y - x^{\\star} = 0 \\implies x^{\\star} = y$. This is valid for $y > a\\lambda$.\n\nCombining all cases for $y>0$ and extending by odd symmetry:\n$$\nT_{\\mathrm{SCAD}}(y) = \n\\begin{cases}\n\\mathrm{sgn}(y)\\max(0, |y|-\\lambda), & |y| \\leq 2\\lambda, \\\\\n\\dfrac{(a-1)y - a\\lambda\\,\\mathrm{sgn}(y)}{a-2}, & 2\\lambda < |y| \\leq a\\lambda, \\\\\ny, & |y| > a\\lambda.\n\\end{cases}\n$$\nThe first part, for $|y| \\leq 2\\lambda$, is identical to the soft-thresholding function $T_{\\ell_{1}}(y)$ evaluated with parameter $\\lambda$.\n\n### 3. The MCP Penalty: $T_{\\mathrm{MCP}}(y)$\nThe derivative of the penalty is $P'_{|x|}(|x|) = \\lambda \\phi'_{\\mathrm{MCP}}(|x|)$. With the given $\\phi'$, this is:\n$$\nP'_{|x|}(|x|) =\n\\begin{cases}\n\\lambda \\left(1 - \\dfrac{|x|}{\\gamma \\lambda}\\right) = \\lambda - \\frac{|x|}{\\gamma}, & 0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0, & |x| > \\gamma \\lambda.\n\\end{cases}\n$$\nThe right-derivative at $0$ is $P'_{|x|}(0^+) = \\lambda$.\nLet $y > 0$, so $x^{\\star} \\geq 0$.\n- If $x^{\\star}=0$, the condition is $0 < y \\leq \\lambda$.\n- If $x^{\\star} > 0$, we have $y - x^{\\star} = P'_{|x|}(x^{\\star})$.\n    - Case $0 < x^{\\star} \\leq \\gamma\\lambda$: $y - x^{\\star} = \\lambda - \\frac{x^{\\star}}{\\gamma}$. Solving for $x^{\\star}$:\n      $y - \\lambda = x^{\\star}(1 - 1/\\gamma) \\implies x^{\\star} = \\frac{y-\\lambda}{1-1/\\gamma} = \\frac{\\gamma(y-\\lambda)}{\\gamma-1}$.\n      This solution is valid if $0 < x^{\\star} \\leq \\gamma\\lambda$.\n      $x^{\\star} > 0$ requires $y > \\lambda$.\n      $x^{\\star} \\leq \\gamma\\lambda$ requires $\\frac{\\gamma(y-\\lambda)}{\\gamma-1} \\leq \\gamma\\lambda \\implies y-\\lambda \\leq \\lambda(\\gamma-1) \\implies y \\leq \\gamma\\lambda$.\n      So, this case holds for $\\lambda < y \\leq \\gamma\\lambda$.\n    - Case $x^{\\star} > \\gamma\\lambda$: $y - x^{\\star} = 0 \\implies x^{\\star} = y$. This is valid for $y > \\gamma\\lambda$.\n\nCombining cases for $y>0$ and extending by odd symmetry:\n$$\nT_{\\mathrm{MCP}}(y) = \n\\begin{cases}\n0, & |y| \\leq \\lambda, \\\\\n\\dfrac{\\gamma(y-\\lambda\\,\\mathrm{sgn}(y))}{\\gamma-1}, & \\lambda < |y| \\leq \\gamma\\lambda, \\\\\ny, & |y| > \\gamma\\lambda.\n\\end{cases}\n$$\n\n### Discussion\nThe derived thresholding functions reveal key properties of the penalties in the context of ill-posed inverse problems.\n- **Bias Reduction with Nonconvex Penalties:**\nThe $\\ell_1$ thresholding function, $T_{\\ell_1}(y)=\\mathrm{sgn}(y)\\max(0, |y|-\\lambda)$, always shrinks large coefficients towards zero by a constant amount $\\lambda$. If $y$ is an observation of a true large coefficient $x_{true}$, the estimate $x^\\star$ will be approximately $|x_{true}|-\\lambda$ (in magnitude), introducing a systematic bias.\nIn contrast, both SCAD and MCP have thresholding functions that become the identity function, $T(y)=y$, for large values of $|y|$ (specifically, $|y| > a\\lambda$ for SCAD and $|y| > \\gamma\\lambda$ for MCP). This means that they do not penalize large coefficients, leading to asymptotically unbiased estimates for large signals. This property, often termed \"unbiasedness\", is a significant advantage of nonconvex penalties over $\\ell_1$.\n\n- **Potential Pitfalls of Nonconvex Penalties:**\n    - **Algorithmic Nonconvexity:** The primary drawback is that the overall objective function, e.g., $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\sum_i P(x_i)$, becomes nonconvex. Iterative algorithms like proximal gradient descent, which solve a sequence of one-dimensional subproblems of the form analyzed here, are only guaranteed to converge to a stationary point, which may be a local, not global, minimum. The final solution can be sensitive to the algorithm's initialization.\n    - **Sensitivity to Parameter Choices:** The performance of SCAD and MCP is highly dependent on the choice of their shape parameters ($a$ and $\\gamma$, respectively) in addition to the regularization parameter $\\lambda$. These extra parameters control the degree of concavity and the threshold for unbiasedness. Finding optimal values for these parameters is a difficult model selection problem, often more complex and computationally intensive than for the convex $\\ell_1$ penalty.\n    - **Solution Stability and Uniqueness:** For convex problems like LASSO, the solution is often unique and stable under perturbations of the data. For nonconvex penalties, the existence of multiple local minima can lead to solution instability. Small changes in the data can cause an algorithm to converge to a different local minimum, resulting in a significantly different solution. While these estimators have strong asymptotic guarantees (e.g., oracle properties), their finite-sample stability can be less robust than their convex counterparts.\n\n### Numerical Evaluation\nWe evaluate the three thresholding functions for the given parameters $\\lambda = 1$, $a = 3.7$, $\\gamma = 3$, and the observation $y_0 = 2.2$.\n\n- **$T_{\\ell_{1}}(y_0)$:**\nWith $\\lambda=1$ and $y_0=2.2$, we have $|y_0| > \\lambda$.\n$T_{\\ell_{1}}(2.2) = \\mathrm{sgn}(2.2)(|2.2| - 1) = 1(2.2 - 1) = 1.2 = \\frac{12}{10} = \\frac{6}{5}$.\n\n- **$T_{\\mathrm{MCP}}(y_0)$:**\nWith $\\lambda=1$, $\\gamma=3$, and $y_0=2.2$. The intervals are defined by $\\lambda=1$ and $\\gamma\\lambda = 3$. Since $1 < |2.2| \\leq 3$, we use the second case for $T_{\\mathrm{MCP}}(y)$.\n$T_{\\mathrm{MCP}}(2.2) = \\frac{3(2.2 - 1 \\cdot \\mathrm{sgn}(2.2))}{3-1} = \\frac{3(1.2)}{2} = \\frac{3.6}{2} = 1.8 = \\frac{18}{10} = \\frac{9}{5}$.\n\n- **$T_{\\mathrm{SCAD}}(y_0)$:**\nWith $\\lambda=1$, $a=3.7$, and $y_0=2.2$. The intervals are defined by $2\\lambda=2$ and $a\\lambda=3.7$. Since $2 < |2.2| \\leq 3.7$, we use the second case for $T_{\\mathrm{SCAD}}(y)$.\n$T_{\\mathrm{SCAD}}(2.2) = \\frac{(3.7 - 1) \\cdot 2.2 - 3.7 \\cdot 1 \\cdot \\mathrm{sgn}(2.2)}{3.7 - 2} = \\frac{2.7 \\cdot 2.2 - 3.7}{1.7} = \\frac{5.94 - 3.7}{1.7} = \\frac{2.24}{1.7}$.\nAs an exact rational number:\n$\\frac{2.24}{1.7} = \\frac{224/100}{17/10} = \\frac{224}{100} \\cdot \\frac{10}{17} = \\frac{224}{170} = \\frac{112}{85}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{5} & \\frac{9}{5} & \\frac{112}{85} \\end{pmatrix}}\n$$"
        }
    ]
}