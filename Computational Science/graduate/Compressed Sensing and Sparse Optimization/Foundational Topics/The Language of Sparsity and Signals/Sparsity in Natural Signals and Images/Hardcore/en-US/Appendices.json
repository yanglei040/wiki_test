{
    "hands_on_practices": [
        {
            "introduction": "Sparsity can be modeled in two primary ways: the synthesis model, where a signal is constructed from a few atoms of a dictionary, and the analysis model, where a signal becomes sparse after applying an analysis operator. This exercise provides a hands-on comparison of these two viewpoints . By calculating the sparsity of a simple, localized image feature under both a wavelet synthesis basis and a gradient analysis operator, you will gain a concrete understanding of how these models capture structure differently and why the analysis model can be more efficient for representing piecewise-constant signals.",
            "id": "3478943",
            "problem": "Consider the analysis model for images, where an analysis operator $\\Omega$ maps an image, represented as a vector $x \\in \\mathbb{R}^{n}$, to analysis coefficients $\\Omega x \\in \\mathbb{R}^{m}$. The analysis cosparsity is defined as the number of zero entries in $\\Omega x$, that is $\\kappa := m - \\|\\Omega x\\|_{0}$, where $\\|\\cdot\\|_{0}$ counts the number of nonzero entries. In contrast, the synthesis model represents an image as $x = D \\alpha$, where $D$ is a dictionary and $\\alpha$ is a coefficient vector; its synthesis sparsity is $s := \\|\\alpha\\|_{0}$.\n\nConstruct an explicit example showing how analysis cosparsity with a discrete image gradient operator encodes piecewise-constant structure that synthesis sparsity in a wavelet dictionary may fail to capture as efficiently:\n\n- Let the image be of size $4 \\times 4$, vectorized column-wise into $x \\in \\mathbb{R}^{16}$. Define the forward-difference discrete gradient analysis operator $\\Omega \\in \\mathbb{R}^{24 \\times 16}$ by including all horizontal and vertical nearest-neighbor differences on interior edges: for each row index $i \\in \\{1,2,3,4\\}$ and column index $j \\in \\{1,2,3\\}$ include a horizontal difference $x_{i,j+1} - x_{i,j}$, and for each $i \\in \\{1,2,3\\}$ and $j \\in \\{1,2,3,4\\}$ include a vertical difference $x_{i+1,j} - x_{i,j}$. Stack these $24$ differences to form $\\Omega x$.\n\n- Consider the piecewise-constant image $X \\in \\mathbb{R}^{4 \\times 4}$ with entries $X_{i,j} = 1$ if $(i,j) = (3,3)$ and $X_{i,j} = 0$ otherwise, and let $x \\in \\mathbb{R}^{16}$ be its column-wise vectorization.\n\n- For the synthesis model, take $D$ to be the separable, orthonormal two-dimensional Haar dictionary on $4 \\times 4$ images, implemented by applying the one-dimensional orthonormal Haar transform twice along rows and then along columns. The one-dimensional orthonormal Haar transform of a length-$4$ vector $[a, b, c, d]$ is computed as follows: form first-level averages $s_{1} = (a+b)/\\sqrt{2}$, $s_{2} = (c+d)/\\sqrt{2}$, and details $w_{1} = (a-b)/\\sqrt{2}$, $w_{2} = (c-d)/\\sqrt{2}$, then form second-level averages and details $S = (s_{1} + s_{2})/\\sqrt{2}$ and $W = (s_{1} - s_{2})/\\sqrt{2}$, yielding $[S, W, w_{1}, w_{2}]$.\n\nCompute the analysis cosparsity $\\kappa$ for $x$ with respect to $\\Omega$, and compute the synthesis sparsity $s$ of $x$ in $D$ by counting the nonzero two-dimensional Discrete Wavelet Transform (DWT) coefficients. Report the single quantity $\\kappa - s$ as your final answer. Express the final answer as an integer; no rounding is required.",
            "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a standard task within the field of sparse signal representation and compressed sensing, providing all necessary definitions and data to arrive at a unique, verifiable solution. We can therefore proceed with the calculation.\n\nThe problem asks for the computation of two quantities: the analysis cosparsity $\\kappa$ and the synthesis sparsity $s$ for a given image, and then to report their difference $\\kappa-s$.\n\nFirst, we compute the analysis cosparsity $\\kappa$.\nThe image is a $4 \\times 4$ matrix $X$ with a single non-zero entry at position $(3,3)$:\n$$\nX = \\begin{pmatrix}\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\nThe image vector $x \\in \\mathbb{R}^{16}$ is the column-wise vectorization of $X$.\nThe analysis operator $\\Omega \\in \\mathbb{R}^{m \\times n}$ computes nearest-neighbor differences. Here, the image dimension is $n=16$. The operator $\\Omega$ is defined by stacking all horizontal and vertical differences.\nThe number of horizontal differences is specified as $4$ rows $\\times$ $3$ differences per row, totaling $12$.\nThe number of vertical differences is specified as $3$ differences per row of edges $\\times$ $4$ columns, totaling $12$.\nThe total number of analysis coefficients is $m = 12 + 12 = 24$.\nThe analysis cosparsity is defined as $\\kappa = m - \\|\\Omega x\\|_0$, where $\\|\\Omega x\\|_0$ is the number of non-zero entries in the vector of differences $\\Omega x$.\n\nAn entry in $\\Omega x$ is a difference of the form $x_{i',j'} - x_{i,j}$ for two adjacent pixels. Since the image $X$ (and its vectorization $x$) has only one non-zero entry at position $(3,3)$, a difference can be non-zero only if it is taken across an edge connected to this pixel. The pixel at $(3,3)$ is an interior pixel and has four neighbors: $(3,2)$, $(3,4)$, $(2,3)$, and $(4,3)$. All neighboring pixels have a value of $0$.\nThe four differences involving the pixel $X_{3,3}=1$ are:\n1.  Horizontal difference $X_{3,3} - X_{3,2} = 1 - 0 = 1$. The problem defines horizontal differences as $x_{i,j+1} - x_{i,j}$. So for $i=3, j=2$, we have $X_{3,3}-X_{3,2}=1$.\n2.  Horizontal difference $X_{3,4} - X_{3,3} = 0 - 1 = -1$. This is for $i=3, j=3$.\n3.  Vertical difference $X_{3,3} - X_{2,3} = 1 - 0 = 1$. The problem defines vertical differences as $x_{i+1,j} - x_{i,j}$. So for $i=2, j=3$, we have $X_{3,3}-X_{2,3}=1$.\n4.  Vertical difference $X_{4,3} - X_{3,3} = 0 - 1 = -1$. This is for $i=3, j=3$.\n\nAll other differences are between two pixels with value $0$, and are therefore $0$.\nThus, the number of non-zero entries in $\\Omega x$ is $\\|\\Omega x\\|_{0} = 4$.\nThe analysis cosparsity is $\\kappa = m - \\|\\Omega x\\|_{0} = 24 - 4 = 20$.\n\nSecond, we compute the synthesis sparsity $s$.\nThe synthesis representation is $x = D\\alpha$, where $D$ is the $2$D orthonormal Haar dictionary. The coefficient vector $\\alpha$ is found by applying the $2$D Haar transform to the image $x$. Since $D$ is orthonormal, $\\alpha = D^T x$. The sparsity $s$ is the number of non-zero coefficients in $\\alpha$, i.e., $s = \\|\\alpha\\|_0$.\nThe $2$D transform is computed by applying the $1$D transform to each row, and then to each column of the resulting matrix.\nThe $1$D orthonormal Haar transform for a length-$4$ vector $[a, b, c, d]$ is given. Let's find the corresponding transform matrix $W$.\nThe coefficients are $c_1 = S = \\frac{1}{2}(a+b+c+d)$, $c_2 = W = \\frac{1}{2}(a+b-c-d)$, $c_3 = w_1 = \\frac{1}{\\sqrt{2}}(a-b)$, and $c_4 = w_2 = \\frac{1}{\\sqrt{2}}(c-d)$.\nThe $1$D transform matrix $W$ is therefore:\n$$\nW = \\begin{pmatrix}\n1/2  1/2  1/2  1/2 \\\\\n1/2  1/2  -1/2  -1/2 \\\\\n1/\\sqrt{2}  -1/\\sqrt{2}  0  0 \\\\\n0  0  1/\\sqrt{2}  -1/\\sqrt{2}\n\\end{pmatrix}\n$$\nThe $2$D Haar coefficients are the entries of the matrix $C = W X W^T$.\n\nStep 1: Transform the rows of $X$. Let's call the result $X'$. The $i$-th row of $X'$ is the transform of the $i$-th row of $X$.\nRows $1$, $2$, and $4$ of $X$ are all-zero vectors, so their transforms are also all-zero vectors.\nRow $3$ of $X$ is $[0, 0, 1, 0]$. Its transform is $W [0, 0, 1, 0]^T$, which is the third column of $W$.\n$$ \\text{Transformed Row 3} = [1/2, -1/2, 0, 1/\\sqrt{2}] $$\nSo, the intermediate matrix is:\n$$\nX' = \\begin{pmatrix}\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n1/2  -1/2  0  1/\\sqrt{2} \\\\\n0  0  0  0\n\\end{pmatrix}\n$$\nStep 2: Transform the columns of $X'$. The final coefficient matrix $C$ is obtained by transforming the columns of $X'$.\nColumn $1$ of $X'$ is $[0, 0, 1/2, 0]^T$. Its transform is $W [0, 0, 1/2, 0]^T = \\frac{1}{2} W [0, 0, 1, 0]^T = \\frac{1}{2} \\times (\\text{3rd column of } W) = [1/4, -1/4, 0, 1/(2\\sqrt{2})]^T$.\nColumn $2$ of $X'$ is $[0, 0, -1/2, 0]^T$. Its transform is $-\\frac{1}{2} W [0, 0, 1, 0]^T = [-1/4, 1/4, 0, -1/(2\\sqrt{2})]^T$.\nColumn $3$ of $X'$ is $[0, 0, 0, 0]^T$. Its transform is $[0, 0, 0, 0]^T$.\nColumn $4$ of $X'$ is $[0, 0, 1/\\sqrt{2}, 0]^T$. Its transform is $\\frac{1}{\\sqrt{2}} W [0, 0, 1, 0]^T = [1/(2\\sqrt{2}), -1/(2\\sqrt{2}), 0, 1/2]^T$.\n\nThe final coefficient matrix $C$ (whose entries are the coefficients of $\\alpha$ arranged in a $4 \\times 4$ grid) is:\n$$\nC = \\begin{pmatrix}\n1/4  -1/4  0  1/(2\\sqrt{2}) \\\\\n-1/4  1/4  0  -1/(2\\sqrt{2}) \\\\\n0  0  0  0 \\\\\n1/(2\\sqrt{2})  -1/(2\\sqrt{2})  0  1/2\n\\end{pmatrix}\n$$\nThe synthesis sparsity $s$ is the number of non-zero entries in this matrix. By inspection, we count the non-zero entries:\n- Row 1: $3$ non-zero entries.\n- Row 2: $3$ non-zero entries.\n- Row 3: $0$ non-zero entries.\n- Row 4: $3$ non-zero entries.\nThe total number of non-zero coefficients is $s = 3 + 3 + 0 + 3 = 9$.\n\nFinally, we compute the required quantity $\\kappa - s$:\n$$ \\kappa - s = 20 - 9 = 11 $$",
            "answer": "$$\\boxed{11}$$"
        },
        {
            "introduction": "The choice of a transform or basis is critical for effective sparse representation, as different bases are tailored to different signal structures. This practice moves from theory to empirical validation by asking you to compare the energy compaction performance of the Discrete Cosine Transform (DCT) and the Haar wavelet transform . By implementing these transforms and evaluating them on a curated test suite of image patches—including edges, textures, and smooth blobs—you will develop a practical intuition for why DCT is the cornerstone of JPEG compression and why wavelets are fundamental to modern standards like JPEG 2000.",
            "id": "3479024",
            "problem": "Let $x \\in \\mathbb{R}^{n \\times n}$ denote a square image patch of side length $n$ sampled on a uniform grid. Consider two orthonormal transforms on $\\mathbb{R}^{n \\times n}$: the two-dimensional Discrete Cosine Transform (DCT) of type II with orthonormal scaling, and the two-dimensional orthonormal Haar wavelet transform with dyadic multiresolution decomposition. For any orthonormal transform $\\mathcal{T}:\\mathbb{R}^{n \\times n} \\to \\mathbb{R}^{n \\times n}$, define the transform coefficients $c = \\mathcal{T}(x)$ and the fraction-of-energy captured by the largest $k$-magnitude coefficients as\n$$\nF_{\\mathcal{T}}(k; x) = \\frac{\\sum_{i \\in S_k} c_i^2}{\\sum_{i=1}^{n^2} c_i^2},\n$$\nwhere $S_k$ indexes the $k$ largest entries of $|c_i|$ and $c_i$ enumerates the entries of $c$ in any fixed linearization order. Because $\\mathcal{T}$ is orthonormal, Parseval's identity implies $\\sum_{i=1}^{n^2} c_i^2 = \\sum_{u,v=1}^{n} x_{uv}^2$.\n\nStarting from the definitions of orthonormal transforms, energy, and Parseval's identity, analyze the energy compaction and sparsity properties of natural image patches under the two-dimensional DCT versus the two-dimensional Haar wavelet transform. Use the fraction-of-energy metric $F_{\\mathcal{T}}(k;x)$ and study its dependence on $k$.\n\nConstruct a deterministic test suite of image patches intended to reflect common structures in natural images (piecewise smooth edges, textures, and smooth blobs), using $n=32$. Each patch must be mean-centered and scaled to unit $\\ell_2$ norm, so that energy fractions are dimensionless and comparable. Explicitly define the three categories as follows:\n\n1. Edge patches (cartoon-like step edges): For angles $\\theta \\in \\{0, \\pi/6, \\pi/3, \\pi/2\\}$, define\n$$\nx_{\\text{edge}}(u,v;\\theta) = \\begin{cases}\n1,  \\cos(\\theta)\\left(\\frac{u}{n} - \\frac{1}{2}\\right) + \\sin(\\theta)\\left(\\frac{v}{n} - \\frac{1}{2}\\right) \\ge 0, \\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nfor $u,v \\in \\{0,1,\\dots,n-1\\}$, then subtract the mean and scale to unit $\\ell_2$ norm.\n\n2. Texture patches (sum of cosine gratings): For four predefined lists of spatial frequencies and phases,\n- Frequencies $\\{(1,2),(2,1),(3,0),(0,3)\\}$ with phases $\\{0, \\pi/4, \\pi/2, 3\\pi/4\\}$,\n- Frequencies $\\{(1,1),(2,0),(0,2),(3,3)\\}$ with phases $\\{\\pi/6, \\pi/3, \\pi/2, 2\\pi/3\\}$,\n- Frequencies $\\{(4,1),(1,4),(2,2),(0,1)\\}$ with phases $\\{\\pi/5, 2\\pi/5, 3\\pi/5, 4\\pi/5\\}$,\n- Frequencies $\\{(3,2),(2,3),(1,0),(0,1)\\}$ with phases $\\{0.1, 0.7, 1.2, 2.0\\}$,\ndefine\n$$\nx_{\\text{tex}}(u,v) = \\sum_{j=1}^{4} \\frac{1}{1 + \\sqrt{f_{x,j}^2 + f_{y,j}^2}} \\cos\\!\\left(2\\pi\\left(f_{x,j}\\frac{u}{n} + f_{y,j}\\frac{v}{n}\\right) + \\phi_j\\right),\n$$\nthen subtract the mean and scale to unit $\\ell_2$ norm.\n\n3. Smooth blob patches (sum of Gaussians): For each of four sets of centers, standard deviations, and amplitudes, define\n$$\nx_{\\text{blob}}(u,v) = \\sum_{j=1}^{3} a_j \\exp\\!\\left(-\\frac{\\left(\\frac{u}{n} - c_{x,j}\\right)^2 + \\left(\\frac{v}{n} - c_{y,j}\\right)^2}{2\\sigma_j^2}\\right),\n$$\nwith the following deterministic parameters:\n- Centers $\\{(0.3,0.3),(0.7,0.5),(0.5,0.8)\\}$, $\\sigma \\in \\{0.08,0.12,0.10\\}$, $a \\in \\{1.0,0.6,0.8\\}$;\n- Centers $\\{(0.2,0.7),(0.6,0.2),(0.8,0.8)\\}$, $\\sigma \\in \\{0.10,0.09,0.11\\}$, $a \\in \\{0.9,0.7,0.5\\}$;\n- Centers $\\{(0.4,0.4),(0.5,0.6),(0.7,0.3)\\}$, $\\sigma \\in \\{0.07,0.13,0.09\\}$, $a \\in \\{1.1,0.5,0.6\\}$;\n- Centers $\\{(0.25,0.25),(0.75,0.75),(0.5,0.5)\\}$, $\\sigma \\in \\{0.12,0.12,0.08\\}$, $a \\in \\{0.8,0.8,1.0\\}$;\nthen subtract the mean and scale to unit $\\ell_2$ norm.\n\nFor the DCT, use the two-dimensional type-II DCT with orthonormal scaling applied separably along rows and columns. For the wavelet, use the two-dimensional orthonormal Haar transform with dyadic decomposition implemented by repeatedly applying the one-dimensional orthonormal Haar transform along rows and columns on the current low-low subband, until the subband is of size $1 \\times 1$.\n\nLet the set of $k$ values be $K = \\{0,1,4,16,64,256,n^2\\}$ with $n=32$ so $n^2=1024$. For each $k \\in K$, compute the average fraction-of-energy over the full test suite under the two transforms, denoted by $\\overline{F}_{\\text{DCT}}(k)$ and $\\overline{F}_{\\text{Haar}}(k)$. Define the comparison metric for each $k$ as the difference\n$$\nD(k) = \\overline{F}_{\\text{DCT}}(k) - \\overline{F}_{\\text{Haar}}(k),\n$$\nwhich is a real number. The answers are dimensionless fractions expressed as decimals in $[0,1]$ for each transform, and real numbers for $D(k)$.\n\nYour program must:\n- Construct the $12$ patches ($4$ edge, $4$ texture, $4$ blob) as specified, mean-center and unit-normalize each.\n- Implement the two-dimensional orthonormal DCT and orthonormal Haar transforms.\n- For each $k \\in K$, compute $\\overline{F}_{\\text{DCT}}(k)$, $\\overline{F}_{\\text{Haar}}(k)$, and then $D(k)$ over the $12$ patches.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of $k$ in $K$, i.e., $[D(0),D(1),D(4),D(16),D(64),D(256),D(1024)]$.\n\nThe test suite covers boundary conditions ($k=0$ and $k=n^2$) and varying sparsity scales. The final outputs are real numbers (floats) without physical units.",
            "solution": "The user has provided a problem that requires a comparative analysis of two prominent orthonormal transforms in signal processing—the two-dimensional Discrete Cosine Transform (DCT) and the two-dimensional Haar wavelet transform—with respect to their ability to sparsely represent different types of image structures. The comparison is to be performed on a well-defined, synthetic test suite of image patches.\n\nThe analysis is based on the concept of **energy compaction**. In signal processing, a transform is said to provide a sparse representation for a class of signals if most of the signal's energy can be captured by a small number of transform coefficients. The metric for this is the fraction-of-energy, $F_{\\mathcal{T}}(k; x)$, defined as:\n$$\nF_{\\mathcal{T}}(k; x) = \\frac{\\sum_{i \\in S_k} c_i^2}{\\sum_{i=1}^{n^2} c_i^2}\n$$\nHere, $c = \\mathcal{T}(x)$ are the coefficients of the image patch $x \\in \\mathbb{R}^{n \\times n}$ under an orthonormal transform $\\mathcal{T}$. The set $S_k$ contains the indices of the $k$ coefficients with the largest magnitudes, $|c_i|$. Since the transform $\\mathcal{T}$ is orthonormal, Parseval's identity states that the total energy is conserved between the signal domain and the transform domain: $\\sum_{i=1}^{n^2} c_i^2 = \\sum_{u,v=1}^{n} x_{uv}^2 = \\|x\\|_F^2$, where $\\|x\\|_F$ is the Frobenius norm. The problem specifies that each patch $x$ is normalized to have unit $\\ell_2$ norm (which is equivalent to the Frobenius norm for a vectorized matrix), meaning $\\|x\\|_F^2 = 1$. Consequently, the denominator in the energy fraction formula is always $1$, simplifying the metric to the sum of the squared magnitudes of the top $k$ coefficients:\n$$\nF_{\\mathcal{T}}(k; x) = \\sum_{i \\in S_k} c_i^2\n$$\n\nThe two transforms under consideration have basis functions with starkly different properties, leading to different strengths in signal representation.\n\n1.  **Two-Dimensional Orthonormal Discrete Cosine Transform (DCT-II)**: The basis functions of the DCT are cosines of varying frequencies. These functions are smooth and non-local (their support covers the entire image patch). As a result, the DCT excels at representing signals that are smooth and highly correlated, which is a common characteristic of the smoothly varying regions in natural images. The 2D DCT is separable and is computed by applying the 1D DCT to the rows and then to the columns of the image patch. For an orthonormal transform, specific scaling factors are applied: $\\sqrt{1/n}$ for the zero-frequency coefficient and $\\sqrt{2/n}$ for all other frequencies.\n\n2.  **Two-Dimensional Orthonormal Haar Wavelet Transform**: The Haar wavelet system is constructed from a piecewise constant scaling function (a box function) and a piecewise constant wavelet function (a square wave). Its basis functions are localized in both space and frequency (though with poor frequency localization). This spatial localization makes the Haar wavelet transform highly effective at representing signals with sharp discontinuities, such as edges, which are compactly represented by a few wavelet coefficients at the corresponding locations and scales. The specified transform is a dyadic multiresolution decomposition, where the 1D Haar transform is separably applied, yielding four subbands (LL, LH, HL, HH), and this process is recursively applied to the low-low (LL) subband until a $1 \\times 1$ LL subband remains.\n\nThe problem requires the creation of a deterministic test suite of $12$ image patches of size $n=32$, designed to model three fundamental types of image structures:\n*   **Edge Patches**: These are cartoon-like images with a single, sharp step edge at various orientations. Due to the sharp, localized discontinuity, the Haar transform is expected to provide a more compact representation than the DCT.\n*   **Texture Patches**: These are generated by summing cosine gratings. As they are composed of sinusoidal components, the DCT, whose basis functions are sinusoids, is expected to provide an exceptionally compact representation.\n*   **Smooth Blob Patches**: These are generated by summing smooth Gaussian functions. The smoothness and high spatial correlation of these patches make them ideal candidates for sparse representation by the DCT.\n\nFor each of the $12$ generated patches, we will perform the following steps:\n1.  Mean-center the patch $x$ by subtracting its mean value, $x' = x - \\bar{x}$.\n2.  Normalize the patch to have unit $\\ell_2$ norm, $x_{\\text{norm}} = x' / \\|x'\\|_2$.\n3.  Compute the transform coefficients $c_{\\text{DCT}} = \\mathcal{T}_{\\text{DCT}}(x_{\\text{norm}})$ and $c_{\\text{Haar}} = \\mathcal{T}_{\\text{Haar}}(x_{\\text{norm}})$.\n4.  For each value of $k$ in the set $K = \\{0, 1, 4, 16, 64, 256, 1024\\}$, calculate the energy fractions $F_{\\text{DCT}}(k; x_{\\text{norm}})$ and $F_{\\text{Haar}}(k; x_{\\text{norm}})$. This involves sorting the absolute values of the coefficients in descending order and summing the squares of the top $k$ values.\n5.  After processing all $12$ patches, compute the average energy fractions $\\overline{F}_{\\text{DCT}}(k)$ and $\\overline{F}_{\\text{Haar}}(k)$ for each $k \\in K$.\n6.  Finally, compute the difference metric $D(k) = \\overline{F}_{\\text{DCT}}(k) - \\overline{F}_{\\text{Haar}}(k)$.\n\nA positive value of $D(k)$ indicates that, on average, the DCT achieves better energy compaction for the top $k$ coefficients, while a negative value indicates the same for the Haar transform. The values for $k=0$ and $k=n^2=1024$ serve as boundary checks. For $k=0$, no energy is captured, so $F(0)=0$ and $D(0)=0$. For $k=n^2$, all energy is captured, so $F(n^2)=1$ (due to Parseval's identity and unit norm), and thus $D(n^2)=0$. The intermediate values of $D(k)$ will reveal the relative performance of the two transforms across different sparsity levels. Based on the properties of the transforms and the test patches, we anticipate that $D(k)$ will be positive for most $k$, as $8$ of the $12$ patches (textures and blobs) are inherently smooth or sinusoidal, favoring the DCT. The edge patches will contribute negatively to the sum, but their effect might be outweighed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.fft import dct\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the patch generation, transformation, and analysis.\n    \"\"\"\n    n = 32\n    K = [0, 1, 4, 16, 64, 256, 1024]\n    \n    patches = generate_patches(n)\n    \n    num_patches = len(patches)\n    num_k = len(K)\n    \n    F_dct_all = np.zeros((num_patches, num_k))\n    F_haar_all = np.zeros((num_patches, num_k))\n    \n    for i, patch in enumerate(patches):\n        # Apply transforms\n        c_dct = ortho_dct2(patch)\n        c_haar = ortho_haar2(patch)\n        \n        # Calculate energy fractions for each k\n        for j, k in enumerate(K):\n            F_dct_all[i, j] = calculate_energy_fraction(c_dct, k)\n            F_haar_all[i, j] = calculate_energy_fraction(c_haar, k)\n            \n    # Average over all patches\n    F_dct_avg = np.mean(F_dct_all, axis=0)\n    F_haar_avg = np.mean(F_haar_all, axis=0)\n    \n    # Compute the difference metric\n    D = F_dct_avg - F_haar_avg\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{d:.12f}' for d in D)}]\")\n\ndef _normalize_patch(patch):\n    \"\"\"Mean-centers and scales a patch to unit l2 norm.\"\"\"\n    p = patch - np.mean(patch)\n    norm = np.linalg.norm(p)\n    if norm > 1e-9:\n        return p / norm\n    return p\n\ndef generate_patches(n):\n    \"\"\"\n    Generates the test suite of 12 image patches.\n    \"\"\"\n    patches = []\n    \n    # Coordinate grids\n    u = np.arange(n)\n    v = np.arange(n)\n    uu, vv = np.meshgrid(u, v, indexing='ij')\n    \n    # 1. Edge Patches\n    x_coords_centered = uu / n - 0.5\n    y_coords_centered = vv / n - 0.5\n    thetas = [0, np.pi/6, np.pi/3, np.pi/2]\n    for theta in thetas:\n        patch = (np.cos(theta) * x_coords_centered + np.sin(theta) * y_coords_centered >= 0).astype(float)\n        patches.append(_normalize_patch(patch))\n\n    u_norm = uu / n\n    v_norm = vv / n\n\n    # 2. Texture Patches\n    tex_params = [\n        ({'freqs': [(1,2),(2,1),(3,0),(0,3)], 'phases': [0, np.pi/4, np.pi/2, 3*np.pi/4]}),\n        ({'freqs': [(1,1),(2,0),(0,2),(3,3)], 'phases': [np.pi/6, np.pi/3, np.pi/2, 2*np.pi/3]}),\n        ({'freqs': [(4,1),(1,4),(2,2),(0,1)], 'phases': [np.pi/5, 2*np.pi/5, 3*np.pi/5, 4*np.pi/5]}),\n        ({'freqs': [(3,2),(2,3),(1,0),(0,1)], 'phases': [0.1, 0.7, 1.2, 2.0]}),\n    ]\n    for params in tex_params:\n        patch = np.zeros((n, n))\n        for j in range(4):\n            fx, fy = params['freqs'][j]\n            phi = params['phases'][j]\n            weight = 1.0 / (1.0 + np.sqrt(fx**2 + fy**2))\n            patch += weight * np.cos(2 * np.pi * (fx * u_norm + fy * v_norm) + phi)\n        patches.append(_normalize_patch(patch))\n\n    # 3. Smooth Blob Patches\n    blob_params = [\n        ({'centers': [(0.3,0.3),(0.7,0.5),(0.5,0.8)], 'sigmas': [0.08,0.12,0.10], 'amps': [1.0,0.6,0.8]}),\n        ({'centers': [(0.2,0.7),(0.6,0.2),(0.8,0.8)], 'sigmas': [0.10,0.09,0.11], 'amps': [0.9,0.7,0.5]}),\n        ({'centers': [(0.4,0.4),(0.5,0.6),(0.7,0.3)], 'sigmas': [0.07,0.13,0.09], 'amps': [1.1,0.5,0.6]}),\n        ({'centers': [(0.25,0.25),(0.75,0.75),(0.5,0.5)], 'sigmas': [0.12,0.12,0.08], 'amps': [0.8,0.8,1.0]}),\n    ]\n    for params in blob_params:\n        patch = np.zeros((n, n))\n        for j in range(3):\n            cx, cy = params['centers'][j]\n            sigma = params['sigmas'][j]\n            a = params['amps'][j]\n            patch += a * np.exp(-((u_norm - cx)**2 + (v_norm - cy)**2) / (2 * sigma**2))\n        patches.append(_normalize_patch(patch))\n        \n    return patches\n\n\ndef ortho_dct2(x):\n    \"\"\"\n    Computes the 2D orthonormal DCT-II.\n    \"\"\"\n    return dct(dct(x, type=2, norm='ortho', axis=1), type=2, norm='ortho', axis=0)\n\ndef ortho_haar2(x):\n    \"\"\"\n    Computes the 2D orthonormal Haar wavelet transform with dyadic decomposition.\n    \"\"\"\n    n = x.shape[0]\n    h = x.copy()\n    L = n\n    sqrt2 = np.sqrt(2.0)\n    \n    while L > 1:\n        L_half = L // 2\n        # Operate on the current LL subband\n        subband = h[:L, :L]\n        \n        # 1D Haar on rows\n        rows_avg = (subband[:, 0::2] + subband[:, 1::2]) / sqrt2\n        rows_diff = (subband[:, 0::2] - subband[:, 1::2]) / sqrt2\n        transformed_rows = np.hstack((rows_avg, rows_diff))\n        \n        # 1D Haar on columns of the row-transformed matrix\n        cols_avg = (transformed_rows[0::2, :] + transformed_rows[1::2, :]) / sqrt2\n        cols_diff = (transformed_rows[0::2, :] - transformed_rows[1::2, :]) / sqrt2\n        transformed_cols = np.vstack((cols_avg, cols_diff))\n        \n        # Place the transformed result back into the main matrix\n        h[:L, :L] = transformed_cols\n        \n        L = L_half\n        \n    return h\n\n\ndef calculate_energy_fraction(c, k):\n    \"\"\"\n    Calculates the fraction of energy in the top-k magnitude coefficients.\n    Assumes total energy (denominator) is 1.\n    \"\"\"\n    if k == 0:\n        return 0.0\n    \n    c_flat = c.flatten()\n    # The problem implies that total energy is 1, so the fraction is simply\n    # the sum of squares of the k largest-magnitude coefficients.\n    \n    # Sort absolute values in descending order\n    sorted_abs_coeffs_sq = np.sort(np.abs(c_flat)**2)[::-1]\n    \n    # Sum the k largest squared coefficients\n    energy = np.sum(sorted_abs_coeffs_sq[:k])\n    \n    return energy\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While Total Variation (TV) regularization is a powerful tool for promoting piecewise-constant structure, it famously suffers from the \"staircasing\" artifact, where smooth gradients are reconstructed as a series of flat steps. This hands-on exercise introduces a more advanced model, second-order Total Generalized Variation (TGV), which remedies this issue by promoting piecewise-linear structure . By implementing a primal-dual algorithm from first principles, you will directly compare the performance of TV and TGV on ramp signals and gain practical experience with the state-of-the-art in variational image processing.",
            "id": "3478968",
            "problem": "You are asked to implement a primal-dual algorithm to denoise a small one-dimensional signal using second-order Total Generalized Variation (TGV) regularization and to compare its ramp bias against first-order Total Variation (TV) denoising. The task must be completed by building on the following fundamental base: convex optimization of a sum of a data fidelity term and a sparsity-inducing regularizer, the definition of the discrete derivative operator on one-dimensional signals, and the proximal operator for a squared error data fidelity. You must not assume any result that is not directly derivable from these bases, and you should derive your algorithmic updates from first principles of convex analysis, duality, and proximal splitting.\n\nConsider a signal with $n$ samples represented as a vector $u \\in \\mathbb{R}^n$. Let $f \\in \\mathbb{R}^n$ denote the observed noisy data. The discrete forward difference operator $D : \\mathbb{R}^n \\to \\mathbb{R}^n$ is defined using a one-sided difference with homogeneous Neumann boundary conditions. The data fidelity term is the squared error $F(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$. The first-order Total Variation (TV) regularizer is the anisotropic $\\ell_1$ norm of $D u$. The second-order Total Generalized Variation of order $2$ (TGV$^2$) introduces an auxiliary field $w \\in \\mathbb{R}^n$ and penalizes the anisotropic $\\ell_1$ norms of $D u - w$ and $D w$, with positive weights. Your implementation must derive and use a primal-dual scheme based on these definitions to compute both TV- and TGV$^2$-regularized denoised solutions.\n\nDefine the bias at ramps to be the difference between the estimated slope of the reconstructed signal and the true slope of the underlying noiseless ramp. For each reconstruction, estimate the slope by least squares linear regression over all samples of the reconstructed signal. Compute the absolute ramp bias for TV and TGV$^2$ reconstructions separately and report the difference between these absolute biases, defined as $\\Delta = \\lvert b_{\\mathrm{TV}} \\rvert - \\lvert b_{\\mathrm{TGV}} \\rvert$, where $b_{\\mathrm{TV}}$ and $b_{\\mathrm{TGV}}$ are the signed slope errors with respect to the true slope. A positive value of $\\Delta$ indicates that TGV$^2$ exhibits lower absolute bias than TV.\n\nImplement the algorithms in a single self-contained program that produces results for the following test suite. In all cases, use fixed parameters for the algorithms: number of samples $n = 32$, primal step size $\\tau = 0.1$, dual step size $\\sigma = 0.1$, over-relaxation parameter $\\theta = 1$, number of iterations $N_{\\mathrm{iter}} = 1000$, TV regularization weight $\\lambda_{\\mathrm{TV}} = 0.12$, and TGV$^2$ weights $\\alpha_1 = 0.8$ and $\\alpha_2 = 1.6$. Use a fixed random seed set to $42$ for reproducibility of the noise. All quantities are dimensionless. Angles are not involved. There are no physical units.\n\nTest suite:\n- Case $1$ (happy path): A linear ramp signal $u^\\star[i] = i/(n-1)$ for $i \\in \\{0, \\dots, n-1\\}$, observed with additive independent and identically distributed Gaussian noise of zero mean and standard deviation $\\sigma_{\\mathrm{noise}} = 0.05$. Compute $\\Delta$ as specified.\n- Case $2$ (boundary condition coverage): A constant signal $u^\\star[i] = 0.5$, observed with additive independent and identically distributed Gaussian noise of zero mean and standard deviation $\\sigma_{\\mathrm{noise}} = 0.05$. Compute $\\Delta$ as specified.\n- Case $3$ (edge case): A noise-free linear ramp signal $u^\\star[i] = i/(n-1)$ with $\\sigma_{\\mathrm{noise}} = 0$. Compute $\\Delta$ as specified.\n\nYour program should produce a single line of output containing the results as a comma-separated list of floating-point numbers enclosed in square brackets, ordered as $[\\Delta_{\\text{Case }1}, \\Delta_{\\text{Case }2}, \\Delta_{\\text{Case }3}]$, with each number rounded to six decimal places (e.g., $[0.123456,0.000001,-0.045000]$).",
            "solution": "The problem requires the implementation and comparison of first-order Total Variation (TV) and second-order Total Generalized Variation (TGV) regularization for one-dimensional signal denoising. The comparison is based on the ramp bias, which quantifies how well each method preserves linear slopes. The solution is derived from first principles of convex optimization using a primal-dual algorithm.\n\nThe general form of the denoising problem is to find a signal $u \\in \\mathbb{R}^n$ that minimizes a composite objective function:\n$$ u^* = \\arg\\min_{u \\in \\mathbb{R}^n} \\left( F(u) + R(u) \\right) $$\nwhere $F(u)$ is a data fidelity term and $R(u)$ is a regularizer. The problem specifies the data fidelity term as the squared $\\ell_2$-norm distance to the observed noisy signal $f \\in \\mathbb{R}^n$, given by $F(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$.\n\nThe problem is solved using the Primal-Dual Hybrid Gradient (PDHG) method, also known as the Chambolle-Pock algorithm, which is well-suited for problems of the form:\n$$ \\min_{x} \\mathcal{F}(x) + \\mathcal{G}(Kx) $$\nwhere $\\mathcal{F}$ and $\\mathcal{G}$ are proper, convex, lower semi-continuous functions, and $K$ is a continuous linear operator. The iterative scheme for this problem is defined by:\n$$\n\\begin{cases}\ny^{k+1} = \\mathrm{prox}_{\\sigma \\mathcal{G}^*}(y^k + \\sigma K \\bar{x}^k) \\\\\nx^{k+1} = \\mathrm{prox}_{\\tau \\mathcal{F}}(x^k - \\tau K^* y^{k+1}) \\\\\n\\bar{x}^{k+1} = x^{k+1} + \\theta(x^{k+1} - x^k)\n\\end{cases}\n$$\nHere, $x$ is the primal variable, $y$ is the dual variable, $K^*$ is the adjoint of $K$, $\\mathcal{G}^*$ is the convex conjugate of $\\mathcal{G}$, and $\\mathrm{prox}_{\\gamma H}(z) = \\arg\\min_v ( H(v) + \\frac{1}{2\\gamma} \\lVert v-z \\rVert_2^2 )$ is the proximal operator. The algorithm parameters are the primal step size $\\tau  0$, the dual step size $\\sigma  0$, and the over-relaxation parameter $\\theta \\in [0, 1]$. The step sizes must satisfy $\\tau \\sigma \\lVert K \\rVert^2  1$. The problem specifies $\\tau=0.1$, $\\sigma=0.1$, and $\\theta=1$.\n\nThe discrete forward difference operator $D: \\mathbb{R}^n \\to \\mathbb{R}^n$ with homogeneous Neumann boundary conditions is defined as $(Du)_i = u_{i+1} - u_i$ for $i \\in \\{0, \\dots, n-2\\}$, and $(Du)_{n-1} = 0$. Its adjoint operator $D^*: \\mathbb{R}^n \\to \\mathbb{R}^n$ must satisfy $\\langle Du, p \\rangle = \\langle u, D^*p \\rangle$ for all $u, p \\in \\mathbb{R}^n$. This yields $(D^*p)_0 = -p_0$, $(D^*p)_i = p_{i-1} - p_i$ for $i \\in \\{1, \\dots, n-2\\}$, and $(D^*p)_{n-1} = p_{n-2}$.\n\nFor both TV and TGV models, the data fidelity component $\\mathcal{F}$ and its proximal operator are common. Let $\\mathcal{F}(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$. Its proximal operator is:\n$$ \\mathrm{prox}_{\\tau \\mathcal{F}}(v) = \\arg\\min_u \\left( \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert u - v \\rVert_2^2 \\right) = \\frac{v + \\tau f}{1 + \\tau} $$\n\n**First-Order Total Variation (TV) Denoising**\nThe TV-regularized problem is:\n$$ \\min_{u \\in \\mathbb{R}^n} \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\lambda_{\\mathrm{TV}} \\lVert Du \\rVert_1 $$\nThis fits the PDHG framework with the following assignments:\n- Primal variable $x = u \\in \\mathbb{R}^n$.\n- $\\mathcal{F}(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$.\n- $\\mathcal{G}(y) = \\lambda_{\\mathrm{TV}} \\lVert y \\rVert_1$, where $y \\in \\mathbb{R}^n$ is a generic variable.\n- Linear operator $K = D$.\n\nThe convex conjugate of $\\mathcal{G}$ is $\\mathcal{G}^*(p) = I_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(p)$, which is the indicator function of the $\\ell_\\infty$-ball of radius $\\lambda_{\\mathrm{TV}}$. The proximal operator of $\\mathcal{G}^*$ is the projection onto this ball:\n$$ \\mathrm{prox}_{\\sigma \\mathcal{G}^*}(v) = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(v) $$\nThis projection is computed element-wise as $(v_i)_{\\text{proj}} = v_i / \\max(1, |v_i|/\\lambda_{\\mathrm{TV}})$.\nThe complete TV denoising algorithm is:\n1. Initialize $u^0$, $\\bar{u}^0 = u^0$, and the dual variable $y^0$.\n2. For $k=0, 1, \\dots, N_{\\mathrm{iter}}-1$:\n   $y^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(y^k + \\sigma D \\bar{u}^k)$\n   $u^{k+1} = \\frac{(u^k - \\tau D^* y^{k+1}) + \\tau f}{1 + \\tau}$\n   $\\bar{u}^{k+1} = u^{k+1} + \\theta(u^{k+1} - u^k)$\n\n**Second-Order Total Generalized Variation (TGV$^2$) Denoising**\nThe TGV$^2$-regularized problem is a joint minimization over the signal $u$ and an auxiliary field $w \\in \\mathbb{R}^n$:\n$$ \\min_{u, w} \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\alpha_1 \\lVert Du - w \\rVert_1 + \\alpha_2 \\lVert Dw \\rVert_1 $$\nTo fit this into the PDHG framework, we define a composite primal variable $x = (u, w) \\in \\mathbb{R}^{2n}$.\n- $\\mathcal{F}(u, w) = \\frac{1}{2}\\lVert u - f \\rVert_2^2 + 0 \\cdot \\lVert w \\rVert_2^2$.\n- $\\mathcal{G}(z_1, z_2) = \\alpha_1 \\lVert z_1 \\rVert_1 + \\alpha_2 \\lVert z_2 \\rVert_1$, for $(z_1, z_2) \\in \\mathbb{R}^{2n}$.\n- Linear operator $K(u,w) = \\begin{pmatrix} Du - w \\\\ Dw \\end{pmatrix} = \\begin{pmatrix} D  -I \\\\ 0  D \\end{pmatrix} \\begin{pmatrix} u \\\\ w \\end{pmatrix}$.\n\nThe adjoint operator is $K^* = \\begin{pmatrix} D^*  0 \\\\ -I  D^* \\end{pmatrix}$. The dual variable is $y=(p,q) \\in \\mathbb{R}^{2n}$.\nThe proximal operator of $\\mathcal{F}$ separates for $u$ and $w$:\n$$ \\mathrm{prox}_{\\tau\\mathcal{F}}((v_u, v_w)) = \\left(\\frac{v_u + \\tau f}{1+\\tau}, v_w\\right) $$\nThe conjugate $\\mathcal{G}^*$ is the indicator function of the set $\\{ (p,q) \\mid \\lVert p \\rVert_\\infty \\le \\alpha_1, \\lVert q \\rVert_\\infty \\le \\alpha_2 \\}$. Its proximal operator is the projection onto this set, which separates for $p$ and $q$.\nThe TGV$^2$ denoising algorithm is:\n1. Initialize $u^0, w^0$, $\\bar{u}^0=u^0, \\bar{w}^0=w^0$, and dual variables $p^0, q^0$.\n2. For $k=0, 1, \\dots, N_{\\mathrm{iter}}-1$:\n   $p^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\alpha_1}(p^k + \\sigma(D\\bar{u}^k - \\bar{w}^k))$\n   $q^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\alpha_2}(q^k + \\sigma D\\bar{w}^k)$\n   $u_{v} = u^k - \\tau D^*p^{k+1}$\n   $w_{v} = w^k - \\tau(-p^{k+1} + D^*q^{k+1})$\n   $u^{k+1} = \\frac{u_v + \\tau f}{1+\\tau}$\n   $w^{k+1} = w_v$\n   $\\bar{u}^{k+1} = u^{k+1} + \\theta(u^{k+1} - u^k)$\n   $\\bar{w}^{k+1} = w^{k+1} + \\theta(w^{k+1} - w^k)$\n\n**Ramp Bias Calculation**\nFor each reconstructed signal $u_{\\text{recon}}$, the slope is estimated by finding the coefficient $m$ that minimizes $\\sum_{i=0}^{n-1} ( (m \\cdot i + c) - u_{\\text{recon}}[i] )^2$ via least squares regression. The signed slope error, or bias, is $b = m - m^\\star$, where $m^\\star$ is the slope of the true underlying signal. For a linear ramp $u^\\star[i] = i/(n-1)$, $m^\\star = 1/(n-1)$. For a constant signal, $m^\\star = 0$. The final metric is the difference in absolute biases: $\\Delta = |b_{\\mathrm{TV}}| - |b_{\\mathrm{TGV}}|$. A positive $\\Delta$ indicates that TGV$^2$ has a smaller absolute ramp bias, and thus better preserves linear structures, a known theoretical advantage of TGV$^2$ over TV. The implementation will execute these derived algorithms for the specified test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the denoising experiments and compute the ramp bias difference.\n    Implements primal-dual algorithms for TV and TGV denoising from first principles.\n    \"\"\"\n    # --- Global Problem Parameters ---\n    N = 32\n    TAU = 0.1\n    SIGMA = 0.1\n    THETA = 1.0\n    N_ITER = 1000\n    LAMBDA_TV = 0.12\n    ALPHA1 = 0.8\n    ALPHA2 = 1.6\n    SEED = 42\n\n    # --- Operator Definitions ---\n    def D_op(u):\n        \"\"\"Discrete forward difference operator with homogeneous Neumann boundary.\"\"\"\n        n = len(u)\n        res = np.zeros(n, dtype=np.float64)\n        res[:-1] = u[1:] - u[:-1]\n        # res[-1] is 0 by initialization\n        return res\n\n    def D_adj_op(p):\n        \"\"\"Adjoint of the discrete forward difference operator.\"\"\"\n        n = len(p)\n        res = np.zeros(n, dtype=np.float64)\n        res[0] = -p[0]\n        if n > 2:\n            res[1:-1] = p[:-2] - p[1:-1]\n        if n > 1:\n            res[-1] = p[-2]\n        return res\n\n    # --- Denoising Algorithms ---\n    def denoise_tv(f):\n        \"\"\"\n        TV denoising using a primal-dual algorithm.\n        min_u 0.5 * ||u - f||^2 + LAMBDA_TV * ||Du||_1\n        \"\"\"\n        u = np.copy(f)\n        u_bar = np.copy(u)\n        y = np.zeros_like(f)\n\n        for _ in range(N_ITER):\n            u_old = np.copy(u)\n            \n            # Dual update using prox of the conjugate\n            y_update_arg = y + SIGMA * D_op(u_bar)\n            denom = np.maximum(1.0, np.abs(y_update_arg) / LAMBDA_TV)\n            y = y_update_arg / denom\n\n            # Primal update using prox of the data fidelity term\n            prox_arg = u_old - TAU * D_adj_op(y)\n            u = (prox_arg + TAU * f) / (1.0 + TAU)\n            \n            # Over-relaxation step\n            u_bar = u + THETA * (u - u_old)\n        \n        return u\n\n    def denoise_tgv(f):\n        \"\"\"\n        TGV denoising using a primal-dual algorithm.\n        min_{u,w} 0.5*||u-f||^2 + ALPHA1*||Du-w||_1 + ALPHA2*||Dw||_1\n        \"\"\"\n        u = np.copy(f)\n        u_bar = np.copy(u)\n        w = np.zeros_like(f)\n        w_bar = np.zeros_like(f)\n        p = np.zeros_like(f)\n        q = np.zeros_like(f)\n        \n        for _ in range(N_ITER):\n            u_old = np.copy(u)\n            w_old = np.copy(w)\n            \n            # Dual update for (p, q)\n            v_p = p + SIGMA * (D_op(u_bar) - w_bar)\n            v_q = q + SIGMA * D_op(w_bar)\n            \n            p = v_p / np.maximum(1.0, np.abs(v_p) / ALPHA1)\n            q = v_q / np.maximum(1.0, np.abs(q) / ALPHA2)\n            \n            # Primal update for (u, w)\n            v_u = u_old - TAU * D_adj_op(p)\n            v_w = w_old - TAU * (-p + D_adj_op(q))\n            \n            u = (v_u + TAU * f) / (1.0 + TAU)\n            w = v_w\n            \n            # Over-relaxation step\n            u_bar = u + THETA * (u - u_old)\n            w_bar = w + THETA * (w - w_old)\n            \n        return u\n\n    # --- Bias Calculation ---\n    def get_slope(u):\n        \"\"\"Estimate the slope of a signal using linear least squares.\"\"\"\n        n = len(u)\n        x = np.arange(n, dtype=np.float64)\n        A = np.vstack([x, np.ones(n)]).T\n        m, _ = np.linalg.lstsq(A, u, rcond=None)[0]\n        return m\n\n    # --- Test Suite Execution ---\n    rng = np.random.default_rng(SEED)\n    test_specs = [\n        {'type': 'ramp', 'noise_std': 0.05},\n        {'type': 'const', 'noise_std': 0.05},\n        {'type': 'ramp', 'noise_std': 0.0},\n    ]\n    \n    results = []\n    for spec in test_specs:\n        if spec['type'] == 'ramp':\n            u_star = np.arange(N, dtype=np.float64) / (N - 1)\n            true_slope = 1.0 / (N - 1)\n        else: # 'const'\n            u_star = np.full(N, 0.5, dtype=np.float64)\n            true_slope = 0.0\n\n        if spec['noise_std'] > 0:\n            noise = rng.normal(0, spec['noise_std'], N)\n            f = u_star + noise\n        else:\n            f = u_star\n            \n        # Perform denoising\n        u_tv = denoise_tv(f)\n        u_tgv = denoise_tgv(f)\n        \n        # Calculate slope errors (bias)\n        slope_tv = get_slope(u_tv)\n        slope_tgv = get_slope(u_tgv)\n        \n        bias_tv = slope_tv - true_slope\n        bias_tgv = slope_tgv - true_slope\n        \n        # Calculate delta of absolute biases\n        delta = np.abs(bias_tv) - np.abs(bias_tgv)\n        results.append(delta)\n\n    # --- Format and Print Output according to problem specification ---\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}