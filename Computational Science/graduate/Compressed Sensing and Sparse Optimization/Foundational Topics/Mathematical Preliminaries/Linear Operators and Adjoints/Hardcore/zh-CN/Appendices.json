{
    "hands_on_practices": [
        {
            "introduction": "在压缩感知中，测量过程通常可以用一个线性算子来建模，其中最基本的一种是子采样算子。本练习将引导你通过伴随算子的基本定义，推导出子采样算子的伴随算子。这个过程将清晰地揭示，采样操作的伴随是“补零”操作，这是理解和设计重建算法的关键一步。",
            "id": "3457713",
            "problem": "在压缩感知和稀疏优化的背景下，考虑一个线性算子 $M : \\mathbb{C}^{n} \\to \\mathbb{C}^{m}$，该算子通过对一个索引集所索引的坐标进行子采样来定义。令 $\\Omega = \\{\\omega_1, \\omega_2, \\ldots, \\omega_m\\} \\subset \\{1,2,\\ldots,n\\}$，并定义 $M$ 为\n$$(Mx)_{k} = x_{\\omega_{k}} \\quad \\text{对于所有 } k \\in \\{1,\\ldots,m\\},$$\n对于任意 $x \\in \\mathbb{C}^{n}$。为 $\\mathbb{C}^{n}$ 和 $\\mathbb{C}^{m}$ 配备标准的复欧几里得内积，即对于维度匹配的向量 $u$ 和 $v$，有 $\\langle u, v \\rangle = u^{*} v$，其中 $u^{*}$ 表示 $u$ 的共轭转置。\n\n仅使用伴随算子 $M^{*}$ 的定义，即满足\n$$\\langle Mx, y \\rangle = \\langle x, M^{*} y \\rangle \\quad \\text{对于所有 } x \\in \\mathbb{C}^{n}, \\ y \\in \\mathbb{C}^{m},$$\n的唯一线性算子 $M^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$，推导出 $M^{*} y$ 的一个以 $\\Omega$ 和 $y \\in \\mathbb{C}^{m}$ 表示的闭式显式表达式。将最终答案表示为单个解析表达式。不需要数值近似。",
            "solution": "在尝试求解之前，对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **算子：** 线性算子 $M : \\mathbb{C}^{n} \\to \\mathbb{C}^{m}$。\n- **索引集：** $\\Omega = \\{\\omega_1, \\omega_2, \\ldots, \\omega_m\\} \\subset \\{1,2,\\ldots,n\\}$。\n- **算子定义：** 对所有 $k \\in \\{1,\\ldots,m\\}$ 和任意 $x \\in \\mathbb{C}^{n}$，有 $(Mx)_{k} = x_{\\omega_{k}}$。\n- **向量空间：** 定义域为 $\\mathbb{C}^{n}$，到达域为 $\\mathbb{C}^{m}$。\n- **内积：** 对于向量 $u, v$，内积为 $\\langle u, v \\rangle = u^{*} v$，其中 $u^{*}$ 是 $u$ 的共轭转置。这是标准的复欧几里得内积。\n- **伴随算子定义：** 伴随算子 $M^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$ 是对所有 $x \\in \\mathbb{C}^{n}$ 和 $y \\in \\mathbb{C}^{m}$ 满足 $\\langle Mx, y \\rangle = \\langle x, M^{*} y \\rangle$ 的唯一线性算子。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在数学上是良定的且内部一致。\n- **科学基础：** 线性算子、向量空间、内积和伴随算子的概念是线性代数和泛函分析中的基本概念。所描述的算子 $M$ 是一个标准的子采样或限制算子，常用于信号处理和压缩感知等领域。该问题基于已建立的数学原理。\n- **良定性：** 对于有限维希尔伯特空间（如此处给定的内积下的 $\\mathbb{C}^{n}$ 和 $\\mathbb{C}^{m}$）之间的线性映射，其伴随算子的存在性和唯一性是一个标准定理。该问题要求推导这个唯一的算子，这是一个良定的任务。\n- **客观性：** 问题以精确、形式化的数学语言陈述，没有主观或模棱两可的术语。所有必要信息都以符号形式提供，适合进行一般性推导。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将推导解答。\n\n### 解答\n伴随算子 $M^{*} : \\mathbb{C}^{m} \\to \\mathbb{C}^{n}$ 由关系式\n$$ \\langle Mx, y \\rangle = \\langle x, M^{*} y \\rangle $$\n定义，对于所有向量 $x \\in \\mathbb{C}^{n}$ 和 $y \\in \\mathbb{C}^{m}$。等式左侧的内积是在 $\\mathbb{C}^{m}$ 中，而右侧的内积是在 $\\mathbb{C}^{n}$ 中。\n\n我们首先使用内积的定义 $\\langle u, v \\rangle = u^{*}v = \\sum_{k} \\overline{u_{k}} v_{k}$ 来展开左侧（LHS）。\n$$ \\langle Mx, y \\rangle = (Mx)^{*} y $$\n向量 $Mx \\in \\mathbb{C}^{m}$ 的分量为 $(Mx)_{k} = x_{\\omega_{k}}$，其中 $k \\in \\{1, \\ldots, m\\}$。因此，\n$$ \\langle Mx, y \\rangle = \\sum_{k=1}^{m} \\overline{(Mx)_{k}} y_{k} = \\sum_{k=1}^{m} \\overline{x_{\\omega_{k}}} y_{k}. $$\n\n接下来，我们展开右侧（RHS）。令 $z = M^{*}y$。由于 $M^{*}$ 从 $\\mathbb{C}^{m}$ 映射到 $\\mathbb{C}^{n}$，向量 $z$ 属于 $\\mathbb{C}^{n}$。\n$$ \\langle x, M^{*} y \\rangle = \\langle x, z \\rangle = x^{*} z = \\sum_{j=1}^{n} \\overline{x_{j}} z_{j}. $$\n$z$ 的分量正是我们想要确定的，即 $z_{j} = (M^{*}y)_{j}$。\n\n令展开后的左侧和右侧相等，我们得到\n$$ \\sum_{k=1}^{m} \\overline{x_{\\omega_{k}}} y_{k} = \\sum_{j=1}^{n} \\overline{x_{j}} (M^{*}y)_{j}. $$\n此等式对于向量 $x \\in \\mathbb{C}^{n}$ 的任意选择都必须成立。这使我们能够通过比较等式两边每个 $\\overline{x_{j}}$ 的系数来确定分量 $(M^{*}y)_{j}$。\n\n让我们将左侧的求和重排为对索引集 $\\{1, \\ldots, n\\}$ 的求和。左侧出现的索引是 $\\omega_{1}, \\ldots, \\omega_{m}$，它们构成了集合 $\\Omega$。对于任何索引 $j \\in \\{1, \\ldots, n\\}$，我们可以分析其贡献。\n\n如果 $j \\notin \\Omega$，那么 $\\overline{x_{j}}$ 不会出现在左侧。这等价于其系数为 $0$。\n如果 $j \\in \\Omega$，那么对于某个唯一的 $k \\in \\{1, \\ldots, m\\}$，有 $j = \\omega_{k}$，因为 $\\Omega$ 中的元素是互不相同的。在这种情况下，项 $\\overline{x_{\\omega_{k}}}y_{k}$ 出现在左侧，即 $\\overline{x_{j}}y_{k}$。\n\n通过对每个 $j \\in \\{1, \\ldots, n\\}$ 比较 $\\overline{x_{j}}$ 的系数，我们可以推断出 $(M^{*}y)_{j}$ 的值。\n对于一个特定的 $j \\in \\{1, \\ldots, n\\}$：\n$1$. 如果 $j \\notin \\Omega$，左侧 $\\overline{x_{j}}$ 的系数是 $0$。因此，$(M^{*}y)_{j} = 0$。\n$2$. 如果 $j \\in \\Omega$，存在唯一的 $k$ 使得 $j = \\omega_{k}$。左侧 $\\overline{x_{j}}$（即 $\\overline{x_{\\omega_k}}$）的系数是 $y_{k}$。因此，$(M^{*}y)_{j} = y_{k}$。\n\n结合这两种情况，我们得到了向量 $M^{*}y$ 的完整分量形式描述。算子 $M^{*}$ 接收一个向量 $y \\in \\mathbb{C}^{m}$，并生成一个 $\\mathbb{C}^{n}$ 中的向量，该向量的元素除了在 $\\Omega$ 指定的索引处外都为零。在索引 $\\omega_{k} \\in \\Omega$ 处，其值为 $y_{k}$。这个操作通常被称为补零或嵌入。\n\n我们可以将这个结果写成紧凑的向量形式。令 $e_{j}$ 表示 $\\mathbb{C}^{n}$ 中的第 $j$ 个标准基向量，它在第 $j$ 个位置为 $1$，其他位置为零。向量 $y_{k} e_{\\omega_{k}}$ 是 $\\mathbb{C}^{n}$ 中的一个向量，它在索引 $\\omega_{k}$ 处的值为 $y_{k}$，在其他所有位置都为零。向量 $M^{*}y$ 是对 $y$ 的所有分量，这类向量的和：\n$$ M^{*}y = y_{1} e_{\\omega_{1}} + y_{2} e_{\\omega_{2}} + \\cdots + y_{m} e_{\\omega_{m}}. $$\n这可以写成求和形式，它为作用于向量 $y$ 的伴随算子提供了最终的闭式表达式。\n$$ M^{*}y = \\sum_{k=1}^{m} y_{k} e_{\\omega_{k}}. $$\n此处，$e_{\\omega_k}$ 代表 $\\mathbb{C}^n$ 中对应于索引 $\\omega_k \\in \\Omega$ 的标准基向量。",
            "answer": "$$\\boxed{\\sum_{k=1}^{m} y_{k} e_{\\omega_{k}}}$$"
        },
        {
            "introduction": "除了简单的子采样，卷积是另一个在信号与图像处理中至关重要的线性算子，常用于模拟模糊或滤波效果。本练习探讨了一个在实践中非常重要的问题：边界条件如何影响伴随算子的结构。通过推导和比较零填充与循环边界下的卷积伴随算子，你将具体地理解到算子的代数结构（如托普利茨与循环矩阵）是如何在其伴随算子中得以体现的。",
            "id": "3457699",
            "problem": "考虑$\\mathbb{C}^{N}$中的有限长度信号，其中$N=5$。设 $h \\in \\mathbb{C}^{3}$ 是一个模糊核，其分量为 $h_{0}=1+i$，$h_{1}=-2$ 和 $h_{2}=1-i$。定义线性模糊算子 $C_{h}:\\mathbb{C}^{5}\\to\\mathbb{C}^{5}$ 为与 $h$ 的离散卷积，在两种不同的边界条件下，使用从零开始的索引 $k\\in\\{0,1,2,3,4\\}$：\n1) 补零边界条件：$(C_{h}^{\\mathrm{z}} x)_{k}=\\sum_{j=0}^{2} h_{j}\\,x_{k-j}$，其中当 $m\\notin\\{0,1,2,3,4\\}$ 时，$x_{m}=0$。\n2) 循环（周期性）边界条件：$(C_{h}^{\\mathrm{c}} x)_{k}=\\sum_{j=0}^{2} h_{j}\\,x_{(k-j)\\bmod 5}$。\n\n为 $\\mathbb{C}^{5}$ 配备标准复内积 $\\langle u,v\\rangle=\\sum_{k=0}^{4}\\overline{u_{k}}\\,v_{k}$。线性算子 $C_{h}$ 的伴随算子 $C_{h}^{*}$ 由以下要求定义：对所有 $x,y\\in\\mathbb{C}^{5}$，都有 $\\langle C_{h}x,y\\rangle=\\langle x,C_{h}^{*}y\\rangle$。\n\n任务：\n- 仅使用伴随算子的定义和上述算子定义，推导 $(C_{h}^{\\mathrm{z}})^{*}$ 和 $(C_{h}^{\\mathrm{c}})^{*}$ 关于 $h$ 和 $y\\in\\mathbb{C}^{5}$ 的显式表达式，并解释边界条件如何改变这些表达式。\n- 然后，对于给定的特定向量 $y\\in\\mathbb{C}^{5}$，$y=\\begin{pmatrix}1 & 1-i & 0 & 2 & -i\\end{pmatrix}^{\\top}$，计算其欧几里得范数的平方 $\\|(C_{h}^{\\mathrm{c}})^{*}y-(C_{h}^{\\mathrm{z}})^{*}y\\|_{2}^{2}$。\n\n$\\|(C_{h}^{\\mathrm{c}})^{*}y-(C_{h}^{\\mathrm{z}})^{*}y\\|_{2}^{2}$ 的精确值是多少？",
            "solution": "该问题要求得到两个主要结果。首先，推导在补零和循环边界条件下，对 $\\mathbb{C}^5$ 中的信号使用核 $h \\in \\mathbb{C}^3$ 进行卷积所对应的伴随算子 $(C_{h}^{\\mathrm{z}})^{*}$ 和 $(C_{h}^{\\mathrm{c}})^{*}$ 的显式表达式。其次，对一个特定的向量 $y \\in \\mathbb{C}^5$，计算这两个伴随算子输出之差的欧几里得范数的平方。\n\n我们首先推导一个类卷积算子 $C_h$ 的伴随算子 $C_h^{*}$ 的一般形式。伴随算子由关系 $\\langle C_h x, y \\rangle = \\langle x, C_h^{*} y \\rangle$ 定义，对所有 $x, y \\in \\mathbb{C}^5$ 成立。内积由 $\\langle u, v \\rangle = \\sum_{k=0}^{4} \\overline{u_k} v_k$ 给出。\n\n伴随算子定义的左侧是：\n$$ \\langle C_h x, y \\rangle = \\sum_{k=0}^{4} \\overline{(C_h x)_k} y_k $$\n右侧是：\n$$ \\langle x, C_h^{*} y \\rangle = \\sum_{m=0}^{4} \\overline{x_m} (C_h^{*} y)_m $$\n我们的目标是操作 $\\langle C_h x, y \\rangle$ 的表达式，使其与一个关于 $m$ 的、包含 $\\overline{x_m}$ 项的求和形式相匹配，从而确定 $(C_h^{*} y)$ 的分量。\n\n我们来考虑一个通用的卷积表达式 $(C_h x)_k = \\sum_{j=0}^{2} h_j x_{\\text{index}(k-j)}$，其中索引函数 `index()` 取决于边界条件。\n$$ \\langle C_h x, y \\rangle = \\sum_{k=0}^{4} \\overline{\\left( \\sum_{j=0}^{2} h_j x_{\\text{index}(k-j)} \\right)} y_k = \\sum_{k=0}^{4} \\sum_{j=0}^{2} \\overline{h_j} \\overline{x_{\\text{index}(k-j)}} y_k $$\n通过交换求和顺序，我们得到：\n$$ \\sum_{j=0}^{2} \\overline{h_j} \\sum_{k=0}^{4} \\overline{x_{\\text{index}(k-j)}} y_k $$\n为了分离出 $\\overline{x_m}$ 项，我们需要在内层求和中进行变量替换。这种替换的具体方式取决于边界条件。\n\n**1. 补零算子 $(C_{h}^{\\mathrm{z}})^{*}$ 的伴随算子。**\n对于补零算子 $C_{h}^{\\mathrm{z}}$，卷积为 $(C_{h}^{\\mathrm{z}} x)_k = \\sum_{j=0}^{2} h_j x_{k-j}$，其中我们定义当 $m \\notin \\{0, 1, 2, 3, 4\\}$ 时，$x_m = 0$。\n内积变为：\n$$ \\langle C_{h}^{\\mathrm{z}} x, y \\rangle = \\sum_{k=0}^{4} \\sum_{j=0}^{2} \\overline{h_j} \\overline{x_{k-j}} y_k $$\n索引 $k-j$ 的范围可以从 $0-2=-2$ 到 $4-0=4$。由于当 $m \\notin \\{0, 1, 2, 3, 4\\}$ 时 $x_m=0$，这些项为零。因此，我们是对 $m=k-j$ 进行求和，其中 $m \\in \\{0, 1, 2, 3, 4\\}$。\n我们按 $x$ 的索引重新组合各项：\n$$ \\sum_{m=0}^{4} \\overline{x_m} \\left( \\sum_{k,j : k-j=m, k\\in[0,4], j\\in[0,2]} \\overline{h_j} y_k \\right) $$\n将 $k = m+j$ 代入内层求和，得到：\n$$ \\sum_{m=0}^{4} \\overline{x_m} \\left( \\sum_{j=0}^{2} \\overline{h_j} y_{m+j} \\right) $$\n在内层求和中，索引 $m+j$ 可能超过 $4$。例如，如果 $m=3$ 且 $j=2$，则 $m+j=5$。由于 $y \\in \\mathbb{C}^5$，其分量仅为索引 $0, \\dots, 4$ 定义。为了与 $x$ 上的补零保持一致，我们也必须认为 $y$ 是补零的。我们定义当 $p \\notin \\{0, 1, 2, 3, 4\\}$ 时，$y_p=0$。\n通过与 $\\sum_{m=0}^{4} \\overline{x_m} ((C_{h}^{\\mathrm{z}})^{*} y)_m$ 比较，我们确定伴随算子的表达式：\n$$ ((C_{h}^{\\mathrm{z}})^{*} y)_m = \\sum_{j=0}^{2} \\overline{h_j} y_{m+j} \\quad \\text{其中当 } p \\notin \\{0, 1, 2, 3, 4\\} \\text{ 时 } y_p = 0 $$\n此操作是 $y$ 与 $h$ 的互相关，其中对 $y$ 进行了补零。\n\n**2. 循环算子 $(C_{h}^{\\mathrm{c}})^{*}$ 的伴随算子。**\n对于循环算子 $C_{h}^{\\mathrm{c}}$，卷积为 $(C_{h}^{\\mathrm{c}} x)_k = \\sum_{j=0}^{2} h_j x_{(k-j)\\bmod 5}$。\n内积为：\n$$ \\langle C_{h}^{\\mathrm{c}} x, y \\rangle = \\sum_{k=0}^{4} \\sum_{j=0}^{2} \\overline{h_j} \\overline{x_{(k-j)\\bmod 5}} y_k $$\n我们交换求和顺序：$\\sum_{j=0}^{2} \\overline{h_j} \\sum_{k=0}^{4} \\overline{x_{(k-j)\\bmod 5}} y_k$。\n在内层求和中，令 $m = (k-j) \\bmod 5$。当 $k$ 从 $0$ 到 $4$ 遍历时，$m$ 也会在 $0, \\dots, 4$ 之间循环。该映射是一个双射，因此我们可以将求和变量从 $k$ 更改为 $m$。逆映射是 $k = (m+j) \\bmod 5$。\n内层求和变为 $\\sum_{m=0}^{4} \\overline{x_m} y_{(m+j)\\bmod 5}$。\n将其代回并交换求和顺序：\n$$ \\sum_{j=0}^{2} \\overline{h_j} \\sum_{m=0}^{4} \\overline{x_m} y_{(m+j)\\bmod 5} = \\sum_{m=0}^{4} \\overline{x_m} \\left( \\sum_{j=0}^{2} \\overline{h_j} y_{(m+j)\\bmod 5} \\right) $$\n与 $\\langle x, (C_{h}^{\\mathrm{c}})^{*} y \\rangle$ 比较，我们推导出循环伴随算子的表达式：\n$$ ((C_{h}^{\\mathrm{c}})^{*} y)_m = \\sum_{j=0}^{2} \\overline{h_j} y_{(m+j)\\bmod 5} $$\n这是 $y$ 与 $h$ 的循环互相关。原算子（循环）的边界条件被其伴随算子继承。\n\n**计算 $\\|(C_{h}^{\\mathrm{c}})^{*}y-(C_{h}^{\\mathrm{z}})^{*}y\\|_{2}^{2}$。**\n给定 $h = (1+i, -2, 1-i)^{\\top}$ 和 $y = (1, 1-i, 0, 2, -i)^{\\top}$。\n核系数的复共轭是 $\\overline{h_0} = 1-i$，$\\overline{h_1} = -2$ 和 $\\overline{h_2} = 1+i$。\n$y$ 的分量是 $y_0=1$，$y_1=1-i$，$y_2=0$，$y_3=2$，$y_4=-i$。\n\n令 $\\delta = (C_{h}^{\\mathrm{c}})^{*}y-(C_{h}^{\\mathrm{z}})^{*}y$。第 $m$ 个分量是：\n$$ \\delta_m = ((C_{h}^{\\mathrm{c}})^{*} y)_m - ((C_{h}^{\\mathrm{z}})^{*} y)_m = \\left(\\sum_{j=0}^{2} \\overline{h_j} y_{(m+j)\\bmod 5}\\right) - \\left(\\sum_{j=0}^{2} \\overline{h_j} y_{m+j}\\right) $$\n其中第二项中的 $y_{m+j}$ 意味着补零。只有当某个 $j \\in \\{0, 1, 2\\}$ 使得索引 $m+j$ 落在范围 $\\{0, 1, 2, 3, 4\\}$ 之外时，差值 $\\delta_m$ 才非零。\n这在 $m+j \\ge 5$ 时发生。\n- 对于 $m \\in \\{0, 1, 2\\}$，$m+j \\le 2+2=4$，所以索引总是在范围内。因此，$\\delta_0 = \\delta_1 = \\delta_2 = 0$。\n- 对于 $m=3$：索引 $m+j$ 是 $3, 4, 5$。$j=2$ 的项导致了差异。\n$$ \\delta_3 = (\\overline{h_0}y_3 + \\overline{h_1}y_4 + \\overline{h_2}y_0) - (\\overline{h_0}y_3 + \\overline{h_1}y_4 + \\overline{h_2} \\cdot 0) = \\overline{h_2} y_0 $$\n$$ \\delta_3 = (1+i)(1) = 1+i $$\n- 对于 $m=4$：索引 $m+j$ 是 $4, 5, 6$。$j=1$ 和 $j=2$ 的项导致了差异。\n$$ \\delta_4 = (\\overline{h_0}y_4 + \\overline{h_1}y_{(5)\\bmod 5} + \\overline{h_2}y_{(6)\\bmod 5}) - (\\overline{h_0}y_4 + \\overline{h_1} \\cdot 0 + \\overline{h_2} \\cdot 0) $$\n$$ \\delta_4 = \\overline{h_1}y_0 + \\overline{h_2}y_1 $$\n代入数值：\n$$ \\delta_4 = (-2)(1) + (1+i)(1-i) = -2 + (1^2 - i^2) = -2 + (1 - (-1)) = -2 + 2 = 0 $$\n所以，差向量是 $\\delta = (0, 0, 0, 1+i, 0)^{\\top}$。\n\n最后，我们计算 $\\delta$ 的欧几里得范数的平方：\n$$ \\|\\delta\\|_{2}^{2} = \\sum_{m=0}^{4} |\\delta_m|^2 = |0|^2 + |0|^2 + |0|^2 + |1+i|^2 + |0|^2 $$\n$$ \\|\\delta\\|_{2}^{2} = |1+i|^2 = (1+i)\\overline{(1+i)} = (1+i)(1-i) = 1 - i^2 = 1 - (-1) = 2 $$\n或者，$|1+i|^2 = (\\sqrt{1^2+1^2})^2 = (\\sqrt{2})^2 = 2$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "我们已经学习了如何推导伴随算子，现在是时候将这一概念应用于解决实际的计算问题了。在处理大规模问题时，我们往往无法显式地构造和存储算子矩阵，而只能通过函数来计算算子及其伴随算子对向量的作用。本练习要求你设计并实现一个仅使用算子$A$和其伴随$A^*$的“无矩阵”算法——幂迭代法，来估计算子的谱范数，这在许多迭代优化算法的参数设置中至关重要。",
            "id": "3457681",
            "problem": "设 $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ 是一个定义在赋有标准内积和诱导欧几里得范数的有限维欧几里得空间之间的线性算子。对于所有 $x \\in \\mathbb{R}^n$ 和 $y \\in \\mathbb{R}^m$，伴随算子 $A^* : \\mathbb{R}^m \\to \\mathbb{R}^n$ 由关系式 $\\langle A x, y \\rangle = \\langle x, A^* y \\rangle$ 定义。$A$ 的谱范数为 $\\|A\\|_2 = \\sup_{x \\neq 0} \\frac{\\|A x\\|_2}{\\|x\\|_2}$。\n\n您的任务是设计并实现一个幂迭代估计器，该估计器仅通过重复应用 $A$ 和 $A^*$ 来近似 $\\|A\\|_2$，而不显式构造 $A^* A$ 也不使用 $A$ 的任何分解。该设计必须从第一性原理出发进行论证：从伴随算子和谱范数的定义开始，并且只使用诸如奇异值分解 (SVD) 的存在性与性质以及对称半正定算子的谱定理等公认事实。您的算法必须：\n- 基于实现算子及其伴随算子对向量作用的 $A$ 和 $A^*$ 的函数句柄进行操作。\n- 使用随机非零初始化以避免与零空间发生对抗性对齐，并为保证可复现性使用固定的种子。\n- 包含一个基于相对变化小于容差 $\\varepsilon$ 的停止准则，以及一个最大迭代次数上限 $K$。\n- 如果首次将 $A$ 应用于单位范数向量得到零向量，则稳健地检测出零算子并返回 $0$。\n\n在压缩感知和稀疏优化的背景下，许多传感算子仅通过其自身的作用及其伴随算子的作用来使用（例如，子采样正交变换和随机高斯矩阵）。您将使用以下测试套件来验证您的估计器。在每种情况下，将 $A$ 和 $A^*$ 实现为函数，并使用有数学依据的方法计算独立的谱范数基准真值。在所有情况下，估计器均使用以下固定参数：容差 $\\varepsilon = 10^{-12}$，最大迭代次数 $K = 1000$，以及按指定情况设置的初始化种子。所有输出均为不带物理单位的实数。\n\n测试套件：\n- 情况 1（稠密高斯，矩形）：设 $m = 8$, $n = 5$，并使用伪随机种子 $12345$ 构造一个矩阵 $G \\in \\mathbb{R}^{m \\times n}$，其元素为独立的标准正态分布。定义 $A(x) = G x$ 和 $A^*(y) = G^\\top y$。谱范数基准真值计算为矩阵 $G$ 的 2-范数。\n- 情况 2（零算子）：设 $m = 3$, $n = 7$，并定义 $A(x) = 0 \\in \\mathbb{R}^m$ 和 $A^*(y) = 0 \\in \\mathbb{R}^n$。谱范数基准真值为 $0$。\n- 情况 3（子采样正交变换）：设 $n = 64$ 并考虑具有正交归一化的正交 2 型离散余弦变换 (DCT) $F : \\mathbb{R}^n \\to \\mathbb{R}^n$。设 $S \\subset \\{0,1,\\dots,n-1\\}$ 为偶数索引 $S = \\{0,2,4,\\dots,62\\}$，并定义 $A(x) = (F x)_S$（选取 $S$ 中的坐标），$A^*(y)$ 的伴随作用则对应于在索引 $S$ 处补零回到长度 $n$，然后进行逆变换。对于此子采样正交算子，谱范数基准真值为 $1$。\n- 情况 4（对角算子）：设 $n = m = 4$ 并定义 $A$ 为对角元素为 $\\{0.1, 2.5, -3.7, 1.2\\}$ 的对角算子。那么 $A^* = A$，谱范数基准真值为对角元素的最大绝对值，即 $3.7$。\n- 情况 5（缩放的稠密高斯）：设 $m = 10$, $n = 4$。使用伪随机种子 $2023$ 构造一个矩阵 $H \\in \\mathbb{R}^{m \\times n}$，其元素为独立的标准正态分布，并设 $c = 7.3$。定义 $A(x) = (c H) x$ 和 $A^*(y) = (c H)^\\top y$。谱范数基准真值是矩阵 $c H$ 的 2-范数。\n\n对于每种情况，使用以下初始化种子运行您的估计器以生成随机起始向量：情况 1 使用种子 $0$，情况 2 使用种子 $1$，情况 3 使用种子 $2$，情况 4 使用种子 $3$，以及情况 5 使用种子 $4$。使用绝对容差 $\\delta = 10^{-8}$ 将您的估计值 $\\widehat{\\|A\\|_2}$ 与基准真值 $\\|A\\|_2$ 进行比较，并生成一个布尔结果，当且仅当 $|\\widehat{\\|A\\|_2} - \\|A\\|_2| \\le \\delta$ 时，该结果为真。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表的结果（例如，“[result1,result2,result3,result4,result5]”），其中每个结果是按情况 1 到 5 顺序排列的相应情况的布尔值输出。",
            "solution": "任务是设计并实现一个基于幂迭代的估计器，用于计算线性算子 $A : \\mathbb{R}^n \\to \\mathbb{R}^m$ 的谱范数 $\\|A\\|_2$。该估计器使用 $A$ 及其伴随算子 $A^*$ 的函数句柄，而无需访问它们的矩阵表示。设计必须从第一性原理出发进行论证。\n\n**1. 理论基础**\n\n在有限维欧几里得空间之间，线性算子 $A$ 的谱范数定义为：\n$$\n\\|A\\|_2 = \\sup_{x \\in \\mathbb{R}^n, x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}\n$$\n根据定义，谱范数等于 $A$ 的最大奇异值，记为 $\\sigma_{\\max}(A)$。算子 $A$ 的奇异值 $\\sigma_i(A)$ 是算子 $A^*A$ 特征值的非负平方根，其中 $A^*: \\mathbb{R}^m \\to \\mathbb{R}^n$ 是 $A$ 的伴随算子，对所有 $x \\in \\mathbb{R}^n$ 和 $y \\in \\mathbb{R}^m$ 满足关系式 $\\langle Ax, y \\rangle = \\langle x, A^*y \\rangle$。\n\n算子 $B = A^*A : \\mathbb{R}^n \\to \\mathbb{R}^n$ 是自伴随且半正定的：\n- **自伴随：** 对于任意 $x, z \\in \\mathbb{R}^n$，我们有 $\\langle Bx, z \\rangle = \\langle (A^*A)x, z \\rangle = \\langle Ax, Az \\rangle = \\langle x, A^*(Az) \\rangle = \\langle x, (A^*A)z \\rangle = \\langle x, Bz \\rangle$。\n- **半正定：** 对于任意 $x \\in \\mathbb{R}^n$，我们有 $\\langle Bx, x \\rangle = \\langle (A^*A)x, x \\rangle = \\langle Ax, Ax \\rangle = \\|Ax\\|_2^2 \\ge 0$。\n\n根据谱定理，有限维空间上的自伴随算子拥有一组完备的、由特征向量组成的正交基，且其对应的特征值均为实数。对于像 $A^*A$ 这样的半正定算子，这些特征值都是非负的。设 $A^*A$ 的特征值为 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$。$A$ 的奇异值与 $A^*A$ 的特征值之间的关系是 $\\sigma_i(A)^2 = \\lambda_i(A^*A)$。\n\n因此，$A$ 的谱范数的平方就是 $A^*A$ 的最大特征值：\n$$\n\\|A\\|_2^2 = (\\sigma_{\\max}(A))^2 = \\lambda_{\\max}(A^*A)\n$$\n这就将计算 $\\|A\\|_2$ 的问题简化为计算算子 $A^*A$ 最大特征值的平方根。\n\n**2. 算法设计：幂迭代法**\n\n幂迭代法是一种用于寻找算子最大模特征值的标准算法。当应用于像 $B = A^*A$ 这样的自伴随半正定算子时，它会收敛到最大特征值 $\\lambda_{\\max}(B)$。\n\n该算法过程如下：\n1. 从一个非零初始向量 $v_0 \\in \\mathbb{R}^n$ 开始。为避免与对应较小特征值的特征空间发生对抗性对齐，$v_0$ 是随机选择的。然后将其归一化为单位范数，即 $\\|v_0\\|_2 = 1$。\n2. 使用以下关系式迭代生成一个向量序列：\n   $$\n   v_{k+1} = \\frac{B v_k}{\\|B v_k\\|_2} = \\frac{(A^*A) v_k}{\\| (A^*A) v_k \\|_2}\n   $$\n当 $k \\to \\infty$ 时，向量 $v_k$ 会收敛到 $B$ 对应于 $\\lambda_{\\max}(B)$ 的单位范数特征向量，前提是 $v_0$ 在该特征向量方向上有一个非零分量（对于随机的 $v_0$ 来说，这种情况的概率为 1）。\n\n关键的洞察在于，将 $B = A^*A$ 应用于向量 $v$ 的操作可以分两个阶段执行，仅使用算子 $A$ 和 $A^*$：\n- 首先，计算 $u = A v$。\n- 其次，计算 $w = A^* u$。\n结果为 $w = A^*(Av) = (A^*A)v = Bv$。这样就避免了显式构造算子 $B$ 的需要，因为在大规模场景中，这种构造通常计算成本高昂或不可行。\n\n相应的最大特征值 $\\lambda_{\\max}(B)$ 可以从迭代过程中估计出来。使用瑞利商，我们有：\n$$\n\\lambda_k = \\langle (A^*A) v_k, v_k \\rangle = \\langle A v_k, A v_k \\rangle = \\|A v_k\\|_2^2\n$$\n随着 $v_k$ 收敛到主特征向量，$\\lambda_k$ 收敛到 $\\lambda_{\\max}(A^*A)$。因此，标量序列 $s_k = \\|A v_k\\|_2$ 收敛到 $\\sqrt{\\lambda_{\\max}(A^*A)} = \\|A\\|_2$。这为谱范数提供了一个直接的估计。\n\n**3. 最终算法规范**\n\n基于上述原理，该估计器设计如下：\n\n1.  **初始化**：\n    - 使用一个带种子的伪随机数生成器选择一个随机向量 $v \\in \\mathbb{R}^n$ 以保证可复现性。\n    - 归一化向量：$v \\leftarrow v / \\|v\\|_2$。\n    - 初始化前一次的范数估计值 $\\widehat{\\sigma}_{\\text{old}} = 0.0$。\n2.  **迭代**：对于 $k = 1, \\dots, K$（其中 $K$ 是最大迭代次数）：\n    a. 应用算子 $A$：$u = A(v)$。\n    b. 计算新的范数估计值：$\\widehat{\\sigma}_{\\text{new}} = \\|u\\|_2$。\n    c. **零算子检测**：如果 $k=1$ 且 $\\widehat{\\sigma}_{\\text{new}} = 0$，则算子 $A$ 是零算子。终止并返回 $0.0$。\n    d. **收敛性检查**：如果 $\\widehat{\\sigma}_{\\text{new}} > 0$ 且相对变化小于容差 $\\varepsilon$，即 $|\\widehat{\\sigma}_{\\text{new}} - \\widehat{\\sigma}_{\\text{old}}| < \\varepsilon \\cdot \\widehat{\\sigma}_{\\text{new}}$，则估计已收敛。终止并返回 $\\widehat{\\sigma}_{\\text{new}}$。\n    e. 更新前一次的估计值：$\\widehat{\\sigma}_{\\text{old}} = \\widehat{\\sigma}_{\\text{new}}$。\n    f. 应用伴随算子 $A^*$：$w = A^*(u)$。\n    g. **稳定性检查**：如果 $\\|w\\|_2 = 0$，则迭代已收敛到 $A^*A$ 的零空间。终止并返回当前估计值 $\\widehat{\\sigma}_{\\text{new}}$。\n    h. 归一化以获得下一次迭代的向量：$v \\leftarrow w / \\|w\\|_2$。\n3.  **终止**：如果循环在 $K$ 次迭代后仍未收敛，则返回最终的估计值 $\\widehat{\\sigma}_{\\text{new}}$。\n\n此设计满足所有问题要求，提供了一种仅使用算子及其伴随算子的应用来估计谱范数的稳健方法。",
            "answer": "```python\nimport numpy as np\nfrom scipy.fft import dct, idct\n\ndef power_iteration_estimator(A, A_star, n, K, epsilon, seed):\n    \"\"\"\n    Estimates the spectral norm of a linear operator A using power iteration.\n\n    Args:\n        A (callable): A function that applies the operator A to a vector.\n        A_star (callable): A function that applies the adjoint operator A* to a vector.\n        n (int): The dimension of the domain of A.\n        K (int): The maximum number of iterations.\n        epsilon (float): The tolerance for the stopping criterion.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        float: The estimated spectral norm of A.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    v = rng.standard_normal(n)\n    v_norm = np.linalg.norm(v)\n    if v_norm == 0.0:\n        # This is extremely unlikely but good practice to handle.\n        v = np.ones(n)\n        v_norm = np.linalg.norm(v)\n    v = v / v_norm\n\n    norm_est = 0.0\n    for k in range(K):\n        # Apply the operator A\n        u = A(v)\n        \n        # Calculate the new estimate for the spectral norm\n        norm_est_new = np.linalg.norm(u)\n\n        # Robustly detect the zero operator on the first iteration\n        if k == 0 and norm_est_new == 0.0:\n            return 0.0\n\n        # Check for convergence based on relative change\n        if norm_est_new > 0.0 and abs(norm_est_new - norm_est) < epsilon * norm_est_new:\n            return norm_est_new\n        \n        norm_est = norm_est_new\n\n        # Apply the adjoint operator A*\n        w = A_star(u)\n        norm_w = np.linalg.norm(w)\n\n        # If w is the zero vector, the algorithm has converged or hit a null space\n        if norm_w == 0.0:\n            return norm_est\n            \n        # Normalize to get the next iterate\n        v = w / norm_w\n    \n    return norm_est\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the power iteration estimator.\n    \"\"\"\n    # Fixed parameters for the estimator\n    EPSILON = 1e-12\n    K = 1000\n    # Absolute tolerance for comparison with ground truth\n    DELTA = 1e-8\n\n    results = []\n\n    # --- Case 1: Dense Gaussian, rectangular ---\n    m1, n1, seed_G, seed_est_1 = 8, 5, 12345, 0\n    rng1 = np.random.default_rng(seed_G)\n    G = rng1.standard_normal((m1, n1))\n    A1 = lambda x: G @ x\n    A1_star = lambda y: G.T @ y\n    gt_1 = np.linalg.norm(G, 2)\n    est_1 = power_iteration_estimator(A1, A1_star, n1, K, EPSILON, seed_est_1)\n    results.append(abs(est_1 - gt_1) <= DELTA)\n\n    # --- Case 2: Zero operator ---\n    m2, n2, seed_est_2 = 3, 7, 1\n    A2 = lambda x: np.zeros(m2)\n    A2_star = lambda y: np.zeros(n2)\n    gt_2 = 0.0\n    est_2 = power_iteration_estimator(A2, A2_star, n2, K, EPSILON, seed_est_2)\n    results.append(abs(est_2 - gt_2) <= DELTA)\n\n    # --- Case 3: Subsampled orthonormal transform (DCT) ---\n    n3, seed_est_3 = 64, 2\n    S = np.arange(0, n3, 2, dtype=int)\n    A3 = lambda x: dct(x, type=2, norm='ortho')[S]\n    def A3_star(y):\n        z = np.zeros(n3)\n        z[S] = y\n        return idct(z, type=2, norm='ortho')\n    gt_3 = 1.0\n    est_3 = power_iteration_estimator(A3, A3_star, n3, K, EPSILON, seed_est_3)\n    results.append(abs(est_3 - gt_3) <= DELTA)\n\n    # --- Case 4: Diagonal operator ---\n    n4, seed_est_4 = 4, 3\n    d = np.array([0.1, 2.5, -3.7, 1.2])\n    A4 = lambda x: d * x\n    A4_star = A4  # Real diagonal operator is self-adjoint\n    gt_4 = np.max(np.abs(d))\n    est_4 = power_iteration_estimator(A4, A4_star, n4, K, EPSILON, seed_est_4)\n    results.append(abs(est_4 - gt_4) <= DELTA)\n\n    # --- Case 5: Scaled dense Gaussian ---\n    m5, n5, seed_H, seed_est_5 = 10, 4, 2023, 4\n    c = 7.3\n    rng5 = np.random.default_rng(seed_H)\n    H = rng5.standard_normal((m5, n5))\n    cH = c * H\n    A5 = lambda x: cH @ x\n    A5_star = lambda y: cH.T @ y\n    gt_5 = np.linalg.norm(cH, 2)\n    est_5 = power_iteration_estimator(A5, A5_star, n5, K, EPSILON, seed_est_5)\n    results.append(abs(est_5 - gt_5) <= DELTA)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}