{
    "hands_on_practices": [
        {
            "introduction": "Understanding the stability of a linear system begins with its condition number. This first exercise provides a concrete, hands-on calculation of the 2-norm condition number for the pseudoinverse of a non-square matrix, a common scenario in overdetermined systems. By working through the steps , you will solidify the connection between singular values and conditioning and verify the important identity that a matrix and its pseudoinverse share the same condition number.",
            "id": "959945",
            "problem": "Compute the 2-norm condition number $\\kappa_2$ of the pseudoinverse of the matrix  \n$$  \nA = \\begin{bmatrix}  \n2 & 1 \\\\  \n1 & 2 \\\\  \n1 & 1  \n\\end{bmatrix}.  \n$$  \nThe pseudoinverse of a matrix $B$ is denoted $B^\\dagger$. The 2-norm condition number is defined as $\\kappa_2(B) = \\|B\\|_2 \\|B^\\dagger\\|_2$. For the pseudoinverse of $A$, compute $\\kappa_2(A^\\dagger)$.",
            "solution": "To compute the 2-norm condition number $\\kappa_2(A^\\dagger)$ of the pseudoinverse of matrix $A$, we use the following key steps:\n\n\n### Step 1: Relationship between $\\kappa_2(A^\\dagger)$ and $\\kappa_2(A)$  \nThe 2-norm condition number of a matrix $B$ is defined as $\\kappa_2(B) = \\|B\\|_2 \\|B^\\dagger\\|_2$. For $B = A^\\dagger$, this becomes:  \n\n$$\n\\kappa_2(A^\\dagger) = \\|A^\\dagger\\|_2 \\|(A^\\dagger)^\\dagger\\|_2\n$$\n  \nBy the property of the Moore-Penrose pseudoinverse, $(A^\\dagger)^\\dagger = A$. Thus:  \n\n$$\n\\kappa_2(A^\\dagger) = \\|A^\\dagger\\|_2 \\|A\\|_2\n$$\n  \nThis is exactly the 2-norm condition number of $A$, so $\\kappa_2(A^\\dagger) = \\kappa_2(A)$.\n\n\n### Step 2: Singular values of $A$  \nThe 2-norm condition number of $A$ is $\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_r}$, where $\\sigma_1$ is the largest singular value of $A$ and $\\sigma_r$ is the smallest non-zero singular value. Singular values of $A$ are the square roots of the eigenvalues of $A^\\top A$.\n\nFor $A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{bmatrix}$, compute $A^\\top A$:  \n\n$$\nA^\\top A = \\begin{bmatrix} 2 & 1 & 1 \\\\ 1 & 2 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 6 & 5 \\\\ 5 & 6 \\end{bmatrix}\n$$\n\n\n\n### Step 3: Eigenvalues of $A^\\top A$  \nThe characteristic equation of $A^\\top A$ is $\\det(A^\\top A - \\lambda I) = 0$:  \n\n$$\n\\det\\left(\\begin{bmatrix} 6-\\lambda & 5 \\\\ 5 & 6-\\lambda \\end{bmatrix}\\right) = (6-\\lambda)^2 - 5^2 = (\\lambda - 11)(\\lambda - 1) = 0\n$$\n  \nThus, the eigenvalues are $\\lambda_1 = 11$ and $\\lambda_2 = 1$.\n\n\n### Step 4: Singular values and condition number  \nThe singular values of $A$ are $\\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{11}$ (largest) and $\\sigma_2 = \\sqrt{\\lambda_2} = 1$ (smallest non-zero). Thus:  \n\n$$\n\\kappa_2(A) = \\frac{\\sigma_1}{\\sigma_2} = \\frac{\\sqrt{11}}{1} = \\sqrt{11}\n$$\n  \n\n\nSince $\\kappa_2(A^\\dagger) = \\kappa_2(A)$, we conclude $\\kappa_2(A^\\dagger) = \\sqrt{11}$.",
            "answer": "$$\\boxed{\\sqrt{11}}$$"
        },
        {
            "introduction": "With a grasp of the condition number, we now explore its direct impact on the accuracy of solutions in the presence of noise. This practice  guides you to derive the precise worst-case noise amplification factor for a least-squares estimate, revealing that it is governed by the smallest singular value of the system matrix. You will identify the \"adversarial\" noise direction that maximizes estimation error, providing a sharp, quantitative understanding of how ill-conditioning compromises a solution.",
            "id": "3471152",
            "problem": "Consider a linear measurement model in compressed sensing, $y = A x + e$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix with $m \\geq n$, $x \\in \\mathbb{R}^{n}$ is $k$-sparse with support $S \\subset \\{1,\\dots,n\\}$ of size $|S| = k$, and $e \\in \\mathbb{R}^{m}$ is an additive noise vector. Assume that an oracle provides the correct support $S$, and the coefficients on $S$ are estimated by the least squares (LS) method constrained to the support, that is, $\\hat{x}_{S} = \\arg\\min_{z \\in \\mathbb{R}^{n}:\\,\\mathrm{supp}(z)\\subseteq S} \\|y - A z\\|_{2}^{2}$, and $\\hat{x}_{S^{c}} = 0$. Let $A_{S} \\in \\mathbb{R}^{m \\times k}$ denote the submatrix of $A$ with columns indexed by $S$, and suppose $A_{S}$ has full column rank. The Moore–Penrose pseudoinverse of $A_{S}$ is denoted $A_{S}^{\\dagger}$.\n\nStarting only from the definitions of the Moore–Penrose pseudoinverse, the operator norm induced by the Euclidean norm, and the singular value decomposition (SVD), perform the following:\n\n1. Derive the exact worst-case noise amplification factor for the support-constrained LS estimate, namely, the value of\n$$\n\\sup_{e \\neq 0} \\frac{\\|\\hat{x} - x\\|_{2}}{\\|e\\|_{2}},\n$$\nexpressed purely in terms of the singular values of $A_{S}$.\n\n2. Identify a noise direction $e$ (as a function of $A_{S}$) that achieves this supremum, and explain why this direction is adversarial in the sense of maximizing the estimation error on the support.\n\n3. Now specialize to the following concrete instance: let $m = 6$, $k = 3$, and suppose the nonzero singular values of $A_{S}$ are $\\sigma_{1} = 5$, $\\sigma_{2} = 3$, and $\\sigma_{3} = \\tfrac{1}{5}$, listed in nonincreasing order. If the noise has Euclidean norm $\\|e\\|_{2} = \\tfrac{1}{50}$, and the noise is chosen adversarially as in item $2$, compute the maximal possible value of the estimation error norm $\\|\\hat{x} - x\\|_{2}$.\n\nProvide your final answer as a single real number. No rounding is required, and no units are involved.",
            "solution": "### Solution\n\nThe analysis begins with the formulation of the support-constrained least squares estimate. Let $x_S \\in \\mathbb{R}^k$ be the vector of non-zero coefficients of $x$ on the support $S$. The measurement model can be written as $y = A_S x_S + e$. The LS estimator for $x_S$ is denoted $\\hat{x}_S$ and is the solution to $\\min_{z_S \\in \\mathbb{R}^k} \\|y - A_S z_S\\|_2^2$.\n\nThe normal equations for this LS problem are given by $(A_S^\\top A_S) \\hat{x}_S = A_S^\\top y$. Since $A_S$ is an $m \\times k$ matrix with full column rank (given $m \\geq k$), the $k \\times k$ matrix $A_S^\\top A_S$ is invertible. Therefore, the unique solution is $\\hat{x}_S = (A_S^\\top A_S)^{-1} A_S^\\top y$. This expression is precisely the definition of the Moore-Penrose pseudoinverse for a matrix with full column rank, so we have $\\hat{x}_S = A_S^\\dagger y$.\n\nThe estimation error vector is $\\hat{x} - x$. Since both $\\hat{x}$ and $x$ have support contained in $S$, their difference is also supported on $S$. The non-zero part of the error vector is $\\hat{x}_S - x_S$. We can express this error in terms of the noise vector $e$:\n$$\n\\hat{x}_S - x_S = A_S^\\dagger y - x_S = A_S^\\dagger(A_S x_S + e) - x_S = (A_S^\\dagger A_S) x_S + A_S^\\dagger e - x_S\n$$\nFor a matrix $A_S$ with full column rank, $A_S^\\dagger A_S = I_k$, where $I_k$ is the $k \\times k$ identity matrix. Thus, the equation simplifies to:\n$$\n\\hat{x}_S - x_S = I_k x_S + A_S^\\dagger e - x_S = A_S^\\dagger e\n$$\nThe Euclidean norm of the total estimation error is $\\|\\hat{x} - x\\|_2 = \\|\\hat{x}_S - x_S\\|_2 = \\|A_S^\\dagger e\\|_2$.\n\n**1. Worst-Case Noise Amplification Factor**\n\nWe are asked to find the quantity $\\sup_{e \\neq 0} \\frac{\\|\\hat{x} - x\\|_{2}}{\\|e\\|_{2}}$. Substituting the expression for the error norm, we get:\n$$\n\\sup_{e \\neq 0} \\frac{\\|A_S^\\dagger e\\|_{2}}{\\|e\\|_{2}}\n$$\nThis is the definition of the operator norm (or spectral norm) of the matrix $A_S^\\dagger$, denoted $\\|A_S^\\dagger\\|_2$. To express this in terms of the singular values of $A_S$, we use the singular value decomposition (SVD). Let the SVD of $A_S$ be $A_S = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times k}$ is a matrix with orthonormal columns ($U^\\top U = I_k$), $V \\in \\mathbb{R}^{k \\times k}$ is an orthogonal matrix ($V^\\top V = V V^\\top = I_k$), and $\\Sigma \\in \\mathbb{R}^{k \\times k}$ is a diagonal matrix containing the singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_k > 0$. The singular values are positive because $A_S$ has full column rank.\n\nThe pseudoinverse $A_S^\\dagger$ has the SVD form $A_S^\\dagger = V \\Sigma^{-1} U^\\top$. The singular values of $A_S^\\dagger$ are the diagonal entries of $\\Sigma^{-1}$, which are $\\frac{1}{\\sigma_1}, \\frac{1}{\\sigma_2}, \\dots, \\frac{1}{\\sigma_k}$.\nSince a matrix's operator norm is its largest singular value, we have:\n$$\n\\|A_S^\\dagger\\|_2 = \\max\\left(\\frac{1}{\\sigma_1}, \\frac{1}{\\sigma_2}, \\dots, \\frac{1}{\\sigma_k}\\right)\n$$\nGiven the ordering $\\sigma_1 \\geq \\dots \\geq \\sigma_k > 0$, the maximum of their reciprocals is $\\frac{1}{\\sigma_k}$. Therefore, the worst-case noise amplification factor is:\n$$\n\\sup_{e \\neq 0} \\frac{\\|\\hat{x} - x\\|_{2}}{\\|e\\|_{2}} = \\|A_S^\\dagger\\|_2 = \\frac{1}{\\sigma_k} = \\frac{1}{\\sigma_{\\min}(A_S)}\n$$\nwhere $\\sigma_{\\min}(A_S)$ is the smallest singular value of $A_S$.\n\n**2. Adversarial Noise Direction**\n\nThe supremum of the ratio $\\frac{\\|A_S^\\dagger e\\|_2}{\\|e\\|_2}$ is achieved when the vector $e$ is aligned with the right singular vector of $A_S^\\dagger$ corresponding to its largest singular value. From the SVD of $A_S^\\dagger = V \\Sigma^{-1} U^\\top$, the right singular vectors are the columns of $U$ and the singular values are $\\{1/\\sigma_i\\}_{i=1}^k$. The largest singular value is $1/\\sigma_k$, which corresponds to the $k$-th column of $U$, denoted $u_k$.\n\nTherefore, the adversarial noise direction is any vector $e$ proportional to $u_k$, i.e., $e = c \\cdot u_k$ for any non-zero scalar $c \\in \\mathbb{R}$. The vector $u_k$ is the left singular vector of the original matrix $A_S$ corresponding to its smallest singular value $\\sigma_k$. This direction is adversarial because it lies in the subspace where the operator $A_S$ has the minimum gain ($\\sigma_k$), meaning its inverse (the pseudoinverse $A_S^\\dagger$) has the maximum gain ($1/\\sigma_k$). Noise in this direction is amplified the most when estimating the signal coefficients.\n\n**3. Concrete Instance Calculation**\n\nWe are given $m=6$, $k=3$, and the singular values of $A_S$ as $\\sigma_1 = 5$, $\\sigma_2 = 3$, and $\\sigma_3 = \\frac{1}{5}$. The smallest singular value is $\\sigma_k = \\sigma_3 = \\frac{1}{5}$. The noise has norm $\\|e\\|_2 = \\frac{1}{50}$ and is chosen adversarially.\n\nThe maximal possible estimation error norm $\\|\\hat{x} - x\\|_2$ occurs when the noise is adversarial. In this case, the error norm is given by:\n$$\n\\|\\hat{x} - x\\|_2 = \\|A_S^\\dagger e\\|_2 = \\|A_S^\\dagger\\|_2 \\|e\\|_2\n$$\nFrom part 1, we know that $\\|A_S^\\dagger\\|_2 = \\frac{1}{\\sigma_k} = \\frac{1}{\\sigma_3}$.\nSubstituting the given values:\n$$\n\\|A_S^\\dagger\\|_2 = \\frac{1}{\\frac{1}{5}} = 5\n$$\nNow, we can compute the maximal estimation error norm:\n$$\n\\|\\hat{x} - x\\|_2 = (5) \\cdot \\left(\\frac{1}{50}\\right) = \\frac{5}{50} = \\frac{1}{10}\n$$",
            "answer": "$$\\boxed{\\frac{1}{10}}$$"
        },
        {
            "introduction": "Having established the theoretical link between ill-conditioning and noise amplification, we turn to a practical, algorithmic solution. This hands-on coding exercise  challenges you to implement a principled early-stopping rule for the LSQR iterative solver, a technique known as iterative regularization. By designing a criterion that balances data fidelity against solution stability using estimates of the pseudoinverse norm, you will see firsthand how theoretical insights are translated into robust numerical methods for solving real-world inverse problems.",
            "id": "3471119",
            "problem": "You are given a linear inverse problem in the setting of compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, and noisy data $y \\in \\mathbb{R}^{m}$ generated by $y = A x_{\\mathrm{true}} + e$, where the entries of the noise vector $e$ are independent and identically distributed zero-mean Gaussian random variables with variance $\\sigma^{2}$. Consider solving the least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|y - A x\\|_{2}$ using Least Squares QR (LSQR), an iterative Krylov subspace method.\n\nThe core challenge is to design a principled early-stopping rule for LSQR that balances the amplification of noise by ill-conditioning against the approximation error. The rule must use only quantities available from the LSQR iterates: the residual norms and an estimate of the spectral norm of the Moore–Penrose pseudoinverse (MPP) of $A$, denoted $\\|A^{\\dagger}\\|_{2}$. The spectral norm of the pseudoinverse is defined by $\\|A^{\\dagger}\\|_{2} = 1/\\sigma_{\\min}(A)$, and the two-norm condition number of $A$ is defined by $\\kappa_{2}(A) = \\|A\\|_{2} \\,\\|A^{\\dagger}\\|_{2}$. LSQR provides reliable estimates of $\\|A\\|_{2}$ and $\\kappa_{2}(A)$ at each iteration, from which you may estimate $\\|A^{\\dagger}\\|_{2}$ by $\\|A^{\\dagger}\\|_{2} \\approx \\kappa_{2}(A) / \\|A\\|_{2}$.\n\nLet $x_{k}$ denote the LSQR iterate after $k$ iterations, and $r_{k} = y - A x_{k}$ its residual. Let $\\delta = \\sqrt{m}\\,\\sigma$ denote the typical scale of the noise norm $\\|e\\|_{2}$. You must implement the following stopping rule:\n\n- Choose fixed constants $\\tau \\ge 1$ and $\\alpha \\in (0,1]$. For this problem, use $\\tau = 1.1$ and $\\alpha = 1.0$.\n- Define the estimated pseudoinverse norm at iteration $k$ by $\\widehat{\\|A^{\\dagger}\\|}_{2,k} = \\widehat{\\kappa_{2}(A)}_{k} / \\widehat{\\|A\\|}_{2,k}$, using LSQR’s internal estimates at iteration $k$.\n- Stop at the smallest iteration $k \\ge 2$ such that both\n  $$\n  \\|r_{k}\\|_{2} \\le \\tau\\,\\delta\n  \\quad\\text{and}\\quad\n  \\|x_{k} - x_{k-1}\\|_{2} \\le \\alpha\\,\\widehat{\\|A^{\\dagger}\\|}_{2,k}\\,\\delta.\n  $$\nIf no such $k$ is found up to a maximum iteration budget $k_{\\max}$, return $k = k_{\\max}$ and the corresponding quantities.\n\nYour program must implement LSQR with the above stopping rule and produce results for the following test suite. For reproducibility, use a fixed pseudorandom generator seed as specified. All matrices and vectors are purely dimensionless; there are no physical units in this problem.\n\nTest suite (four cases), each case specified by $(m,n,\\text{profile},\\sigma,s,\\text{seed})$:\n- Case $1$: $(40,60,\\text{mild\\_ill},10^{-2},5,0)$.\n- Case $2$: $(60,60,\\text{well},5 \\cdot 10^{-3},10,1)$.\n- Case $3$: $(40,60,\\text{severe\\_ill},10^{-2},5,2)$.\n- Case $4$: $(50,200,\\text{severe\\_ill},5 \\cdot 10^{-3},10,3)$.\n\nMatrix construction:\n- For $\\text{well}$: let $A$ have independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0,1/m)$, with no column scaling.\n- For $\\text{mild\\_ill}$: let $A$ be as above, but scale the last $\\lfloor n/2 \\rfloor$ columns by $10^{-2}$.\n- For $\\text{severe\\_ill}$: let $A$ be as above, but scale column $j$ by $10^{-\\gamma}$ where $\\gamma = 8 \\cdot \\frac{j-1}{n-1}$, producing singular values spanning approximately $[1,10^{-8}]$.\n\nSignal construction:\n- Let $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ be $s$-sparse with nonzero entries chosen uniformly at random positions and values sampled i.i.d. from $\\mathcal{N}(0,1)$.\n\nNoise:\n- Let $e \\in \\mathbb{R}^{m}$ have entries sampled i.i.d. from $\\mathcal{N}(0,\\sigma^{2})$.\n\nFor each case, compute $y = A x_{\\mathrm{true}} + e$, set $\\delta = \\sqrt{m}\\,\\sigma$, run LSQR with the stopping rule above, and cap the maximum iteration count by $k_{\\max} = \\min\\{m,n\\}$. For LSQR, use zero absolute and relative tolerances and no condition-limit stopping, i.e., $\\text{atol} = 0$, $\\text{btol} = 0$, and $\\text{conlim} = +\\infty$, and enforce the iteration limit exactly at $k$ when extracting $x_{k}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, the result is the list $[k^{\\star}, \\widehat{\\|A^{\\dagger}\\|}_{2,k^{\\star}}, \\|r_{k^{\\star}}\\|_{2}]$, where $k^{\\star}$ is the selected stopping iteration, $\\widehat{\\|A^{\\dagger}\\|}_{2,k^{\\star}}$ is the estimated pseudoinverse norm at stop, and $\\|r_{k^{\\star}}\\|_{2}$ is the residual norm at stop. Thus the final output must be of the form\n$\n\\big[\n[k_{1}, \\widehat{\\|A^{\\dagger}\\|}_{2,1}, \\|r_{1}\\|_{2}],\n[k_{2}, \\widehat{\\|A^{\\dagger}\\|}_{2,2}, \\|r_{2}\\|_{2}],\n[k_{3}, \\widehat{\\|A^{\\dagger}\\|}_{2,3}, \\|r_{3}\\|_{2}],\n[k_{4}, \\widehat{\\|A^{\\dagger}\\|}_{2,4}, \\|r_{4}\\|_{2}]\n\\big]\n$ and must be printed as a single line.",
            "solution": "The problem requires the implementation of a specific early-stopping rule for the Least Squares QR (LSQR) algorithm, a Krylov subspace method used to solve the linear least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|y - A x\\|_{2}$. The system arises from a linear inverse problem $y = A x_{\\mathrm{true}} + e$, where $A \\in \\mathbb{R}^{m \\times n}$ is a given matrix, $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is the unknown sparse signal, and $e \\in \\mathbb{R}^{m}$ is a noise vector with independent and identically distributed entries from $\\mathcal{N}(0, \\sigma^2)$.\n\nThe fundamental challenge in solving such inverse problems, especially when the matrix $A$ is ill-conditioned, is to prevent the amplification of noise in the solution. Iterative methods like LSQR offer a form of regularization through early stopping: terminating the iteration before it starts to excessively fit the noise. The problem specifies a principled stopping rule designed to balance the trade-off between approximation error (how well $A x_k$ fits $y$) and solution stability (preventing noise amplification).\n\nThe stopping rule is defined as follows: for given constants $\\tau \\ge 1$ and $\\alpha \\in (0,1]$, the iteration is stopped at the smallest integer $k \\ge 2$ that satisfies two conditions simultaneously:\n1.  $\\|r_{k}\\|_{2} \\le \\tau\\,\\delta$\n2.  $\\|x_{k} - x_{k-1}\\|_{2} \\le \\alpha\\,\\widehat{\\|A^{\\dagger}\\|}_{2,k}\\,\\delta$\n\nHere, $x_k$ is the solution estimate at iteration $k$, and $r_k = y - A x_k$ is the corresponding residual. The parameter $\\delta = \\sqrt{m}\\,\\sigma$ represents the expected Euclidean norm of the noise vector $e$. The first condition is a form of the discrepancy principle; it dictates that the iteration should stop when the residual's norm is on the order of the noise norm. Further reduction of the residual would likely correspond to fitting the noise $e$ rather than the underlying signal structure in $A x_{\\mathrm{true}}$. For this problem, $\\tau = 1.1$.\n\nThe second condition introduces a solution stabilization check. The term $\\widehat{\\|A^{\\dagger}\\|}_{2,k}$ is an estimate of the spectral norm of the Moore-Penrose pseudoinverse of $A$, computed at iteration $k$ from LSQR's internal estimates of the condition number $\\widehat{\\kappa_{2}(A)}_{k}$ and spectral norm $\\widehat{\\|A\\|}_{2,k}$ as $\\widehat{\\|A^{\\dagger}\\|}_{2,k} = \\widehat{\\kappa_{2}(A)}_{k} / \\widehat{\\|A\\|}_{2,k}$. The quantity $\\widehat{\\|A^{\\dagger}\\|}_{2,k}\\,\\delta$ can be interpreted as an estimate of the magnitude of noise propagated into the solution space. The condition thus stops the iteration when the change in the solution vector, $\\|x_k - x_{k-1}\\|_2$, becomes smaller than this noise amplification estimate, suggesting that subsequent updates are unlikely to yield significant improvements to the true signal. For this problem, $\\alpha = 1.0$.\n\nIf these conditions are not met by the maximum allowed iteration count, $k_{\\max} = \\min\\{m,n\\}$, the process terminates and returns the results from iteration $k_{\\max}$.\n\nThe implementation proceeds by first generating the test data for each case as specified. A matrix $A$ of size $m \\times n$ is constructed with entries from $\\mathcal{N}(0, 1/m)$, and its columns are scaled according to the specified ill-conditioning profile ('well', 'mild_ill', or 'severe_ill'). An $s$-sparse signal vector $x_{\\mathrm{true}}$ is generated with random support and normally distributed non-zero values. The measurement vector $y$ is then computed as $y = A x_{\\mathrm{true}} + e$, where $e$ is a realization of the Gaussian noise.\n\nTo apply the custom stopping rule, we must access quantities from each iteration of LSQR. A direct approach is to run the standard `scipy.sparse.linalg.lsqr` function within a loop. For each iteration $k$ from $2$ to $k_{\\max}$, we execute `lsqr` with `iter_lim=k` to obtain $x_k$ and the estimates $\\widehat{\\|A\\|}_{2,k}$ and $\\widehat{\\kappa_{2}(A)}_{k}$. We also require $x_{k-1}$, which is retained from the previous step in the loop. With these quantities, both stopping conditions can be evaluated at each $k$. The first $k$ that satisfies both is chosen as the stopping iteration $k^\\star$. This iterative procedure, while computationally more intensive than a single `lsqr` call, correctly implements the specified logic by providing the necessary history of iterates.\n\nThe specified parameters for the `lsqr` solver are an absolute tolerance `atol` of $0$, a relative tolerance `btol` of $0$, and an unlimited condition number `conlim` of $+\\infty$, ensuring that the solver's termination is controlled solely by our external loop and the iteration limit `iter_lim`. For each test case, the final output consists of the stopping iteration $k^{\\star}$, the estimated pseudoinverse norm $\\widehat{\\|A^{\\dagger}\\|}_{2,k^{\\star}}$, and the residual norm $\\|r_{k^{\\star}}\\|_2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse.linalg import lsqr\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (m, n, profile, sigma, s, seed)\n        (40, 60, 'mild_ill', 1e-2, 5, 0),\n        (60, 60, 'well', 5e-3, 10, 1),\n        (40, 60, 'severe_ill', 1e-2, 5, 2),\n        (50, 200, 'severe_ill', 5e-3, 10, 3)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(*case)\n        results.append(result)\n\n    # Format the final output string as specified, removing whitespace.\n    formatted_results = [str(res).replace(' ', '') for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef process_case(m, n, profile, sigma, s, seed):\n    \"\"\"\n    Generates data for a single test case and runs the LSQR with the custom stopping rule.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate matrix A\n    A = rng.normal(0, np.sqrt(1/m), size=(m, n))\n    if profile == 'mild_ill':\n        num_cols_to_scale = int(np.floor(n / 2))\n        scaling_factor = 1e-2\n        A[:, -num_cols_to_scale:] *= scaling_factor\n    elif profile == 'severe_ill':\n        for j in range(n):\n            gamma = 8.0 * j / (n - 1)\n            scaling_factor = 10**(-gamma)\n            A[:, j] *= scaling_factor\n\n    # 2. Generate true signal x_true\n    x_true = np.zeros(n)\n    support = rng.choice(n, s, replace=False)\n    x_true[support] = rng.normal(0, 1, size=s)\n\n    # 3. Generate noise e and measurement y\n    e = rng.normal(0, sigma, size=m)\n    y = A @ x_true + e\n\n    # 4. Implement LSQR with the specified stopping rule\n    tau = 1.1\n    alpha = 1.0\n    delta = np.sqrt(m) * sigma\n    k_max = min(m, n)\n\n    k_star = k_max\n    final_report = None\n\n    # Get x_{k-1} for the first iteration of the loop (k=2)\n    # This corresponds to x_1\n    res_prev = lsqr(A, y, damp=0.0, atol=0, btol=0, conlim=float('inf'), iter_lim=1)\n    x_prev = res_prev[0]\n\n    for k in range(2, k_max + 1):\n        res_k = lsqr(A, y, damp=0.0, atol=0, btol=0, conlim=float('inf'), iter_lim=k)\n        x_k = res_k[0]\n        r_norm_k = res_k[3]\n        anorm_k = res_k[5]\n        acond_k = res_k[6]\n        \n        if anorm_k > 0:\n            est_A_dagger_norm_k = acond_k / anorm_k\n        else:\n            est_A_dagger_norm_k = float('inf')\n\n        x_diff_norm_k = np.linalg.norm(x_k - x_prev)\n\n        cond1 = r_norm_k = tau * delta\n        cond2 = x_diff_norm_k = alpha * est_A_dagger_norm_k * delta\n        \n        if cond1 and cond2:\n            k_star = k\n            final_report = [k_star, est_A_dagger_norm_k, r_norm_k]\n            break\n\n        # Update x_prev for the next iteration step\n        x_prev = x_k\n\n    # If the loop completes without stopping, use results from k_max\n    if final_report is None:\n        # The last iteration of the loop was k = k_max. res_k holds its results.\n        res_kmax = lsqr(A, y, damp=0.0, atol=0, btol=0, conlim=float('inf'), iter_lim=k_max)\n        r_norm_kmax = res_kmax[3]\n        anorm_kmax = res_kmax[5]\n        acond_kmax = res_kmax[6]\n        if anorm_kmax > 0:\n            est_A_dagger_norm_kmax = acond_kmax / anorm_kmax\n        else:\n            est_A_dagger_norm_kmax = float('inf')\n        \n        final_report = [k_max, est_A_dagger_norm_kmax, r_norm_kmax]\n\n    return final_report\n\nsolve()\n```"
        }
    ]
}