{
    "hands_on_practices": [
        {
            "introduction": "在许多高维问题（如图像处理和遥感）中，算子通常以可分离的方式作用于不同的维度。本练习将引导您推导一个关键恒等式，该恒等式将此类算子的矩阵形式（$Y = \\Phi_1 X \\Phi_2^T$）与其使用克罗内克积的向量化形式（$y = (\\Phi_2 \\otimes \\Phi_1)\\operatorname{vec}(X)$）联系起来。理解并验证这种关系是利用这些结构进行高效计算的第一步，也是后续更复杂应用的基础。",
            "id": "3493467",
            "problem": "给定一个由矩阵 $X \\in \\mathbb{R}^{n_1 \\times n_2}$ 表示的二维实值信号，以及两个实值感知矩阵 $\\Phi_1 \\in \\mathbb{R}^{m_1 \\times n_1}$ 和 $\\Phi_2 \\in \\mathbb{R}^{m_2 \\times n_2}$。在用于压缩感知和稀疏优化的可分离感知中，$X$ 的测量是通过沿行和列独立混合来构建的。从向量化算子和克罗内克积的核心定义出发，推导可分离的二维测量模型，该模型以向量化的 $X$ 和感知矩阵的克罗内克积来表示测量值 $y \\in \\mathbb{R}^{m_1 m_2}$。然后，推导相应的伴随映射，该映射通过在适当重塑的数组上进行两次标准矩阵乘法，将可分离算子的转置应用于任何测量向量，而无需显式地构建克罗内克积。\n\n您的推导必须基于以下基本依据：\n- 向量化算子的定义，该算子将矩阵的列堆叠成单个列向量。\n- 两个矩阵之间克罗内克积的定义。\n- 矩阵乘法和向量化的线性性。\n\n您不得引用任何预先给出的快捷恒等式。相反，应从上述基础出发进行推理，以建立测量模型及其伴随模型，并展示如何通过在重塑操作前后进行两次矩阵乘法来实现伴随。实现必须采用列主序约定进行向量化，这意味着 $\\operatorname{vec}(X)$ 按顺序堆叠 $X$ 的列。\n\n完成推导后，实现一个程序，通过以下测试套件对推导出的恒等式进行数值验证。对于每个测试用例，您必须：\n1. 使用应用于 $X$ 的列主序向量化的显式克罗内克积来计算测量值。\n2. 使用两次矩阵乘法来计算测量值，首先用 $\\Phi_1$ 沿行和用 $\\Phi_2$ 沿列混合 $X$，然后进行列主序向量化。\n3. 使用应用于测量向量的显式克罗内克积来计算测量的伴随。\n4. 使用两次矩阵乘法来计算伴随，方法是将测量向量按列主序重塑为 $m_1 \\times m_2$ 数组，然后在左侧乘以 $\\Phi_1^T$，在右侧乘以 $\\Phi_2$，并按列主序重新向量化。\n\n对于每个测试用例，返回两个浮点值：\n- 步骤 1 和 2 中测量值之间差值的欧几里得范数。\n- 步骤 3 和 4 中伴随值之间差值的欧几里得范数。\n\n使用以下测试套件，所有矩阵均明确给出：\n- 测试用例 1:\n  - $\\Phi_1 \\in \\mathbb{R}^{2 \\times 3}$: $\\begin{bmatrix}0.6  -0.3  0.1 \\\\ 0.2  0.5  -0.4\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{3 \\times 4}$: $\\begin{bmatrix}0.5  -0.2  0.0  0.3 \\\\ -0.1  0.4  0.6  -0.2 \\\\ 0.0  0.1  -0.3  0.7\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{3 \\times 4}$: $\\begin{bmatrix}0.0  1.0  -1.5  0.0 \\\\ 0.0  0.0  0.0  -2.0 \\\\ 0.5  0.0  0.0  0.0\\end{bmatrix}$\n- 测试用例 2 (边界维度):\n  - $\\Phi_1 \\in \\mathbb{R}^{1 \\times 2}$: $\\begin{bmatrix}1.0  -1.0\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}2.0 \\\\ -3.0\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}4.0 \\\\ -5.0\\end{bmatrix}$\n- 测试用例 3 (一个因子为单位矩阵):\n  - $\\Phi_1 \\in \\mathbb{R}^{3 \\times 3}$: 单位矩阵 $\\begin{bmatrix}1  0  0 \\\\ 0  1  0 \\\\ 0  0  1\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{2 \\times 2}$: $\\begin{bmatrix}0.8  -0.6 \\\\ 0.3  0.9\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{3 \\times 2}$: $\\begin{bmatrix}1.0  0.0 \\\\ 0.0  -1.0 \\\\ 2.0  1.5\\end{bmatrix}$\n- 测试用例 4 (标量信号):\n  - $\\Phi_1 \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}1.2 \\\\ -0.7\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}0.5 \\\\ -1.1\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{1 \\times 1}$: $\\begin{bmatrix}3.0\\end{bmatrix}$\n\n您的程序应生成单行输出，其中包含八个浮点结果，格式为逗号分隔的列表，并用方括号括起来，顺序为 $[\\text{error\\_measure\\_TC1}, \\text{error\\_adjoint\\_TC1}, \\text{error\\_measure\\_TC2}, \\text{error\\_adjoint\\_TC2}, \\text{error\\_measure\\_TC3}, \\text{error\\_adjoint\\_TC3}, \\text{error\\_measure\\_TC4}, \\text{error\\_adjoint\\_TC4}]$。此问题不涉及单位，也不使用角度。所有值都必须在整个计算过程中使用列主序约定进行向量化和重塑。",
            "solution": "该问题要求推导与二维可分离感知相关的两个基本恒等式，这是压缩感知和稀疏优化中的一个常用模型。具体来说，我们必须首先建立一个使用显式克罗内克积的向量化测量模型与一个使用顺序矩阵乘法的更高效模型之间的等价关系。其次，我们必须推导相应的伴随映射，并证明它也可以在不形成大的克罗内克积矩阵的情况下高效实现。推导将基于指定的第一性原理：向量化算子的定义、克罗内克积的定义以及矩阵运算的线性性。我们在整个过程中假设向量化采用列主序约定。\n\n令信号为矩阵 $X \\in \\mathbb{R}^{n_1 \\times n_2}$。感知矩阵为 $\\Phi_1 \\in \\mathbb{R}^{m_1 \\times n_1}$ 和 $\\Phi_2 \\in \\mathbb{R}^{m_2 \\times n_2}$。可分离感知操作定义为 $Y = \\Phi_1 X \\Phi_2^T$，其中 $Y \\in \\mathbb{R}^{m_1 \\times m_2}$ 是测量矩阵。最终的测量向量是 $y = \\operatorname{vec}(Y) \\in \\mathbb{R}^{m_1 m_2}$。\n\n首先，我们建立必要的定义。\n\n**向量化算子 ($\\operatorname{vec}$)**\n对于一个具有列 $a_1, a_2, \\ldots, a_n \\in \\mathbb{R}^m$ 的矩阵 $A \\in \\mathbb{R}^{m \\times n}$，向量化算子 $\\operatorname{vec}(A)$ 将 $A$ 的列堆叠成一个单一的列向量：\n$$\n\\operatorname{vec}(A) = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\in \\mathbb{R}^{mn \\times 1}\n$$\n\n**克罗内克积 ($\\otimes$)**\n对于两个矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和 $B \\in \\mathbb{R}^{p \\times q}$，它们的克罗内克积 $A \\otimes B$ 是一个 $(mp) \\times (nq)$ 的分块矩阵，定义为：\n$$\nA \\otimes B = \\begin{bmatrix}\na_{11}B  a_{12}B  \\cdots  a_{1n}B \\\\\na_{21}B  a_{22}B  \\cdots  a_{2n}B \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\na_{m1}B  a_{m2}B  \\cdots  a_{mn}B\n\\end{bmatrix}\n$$\n\n**第一部分：前向测量模型的推导**\n\n我们的目标是证明恒等式 $\\operatorname{vec}(\\Phi_1 X \\Phi_2^T) = (\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)$。\n\n令 $Y = \\Phi_1 X \\Phi_2^T$。我们将分析 $Y$ 的列结构。设 $X$ 由其列 $x_1, x_2, \\ldots, x_{n_2}$ 表示，其中每个 $x_k \\in \\mathbb{R}^{n_1}$。所以，$X = [x_1, x_2, \\ldots, x_{n_2}]$。\n$Y$ 的第 $j$ 列，记为 $y_j \\in \\mathbb{R}^{m_1}$，由 $y_j = Y e_j$ 给出，其中 $e_j$ 是 $\\mathbb{R}^{m_2}$ 中的第 $j$ 个标准基向量。\n代入 $Y$ 的表达式，我们得到：\n$$\ny_j = (\\Phi_1 X \\Phi_2^T) e_j = \\Phi_1 X (\\Phi_2^T e_j)\n$$\n项 $\\Phi_2^T e_j$ 只是矩阵 $\\Phi_2^T$ 的第 $j$ 列。让我们将 $\\Phi_2 \\in \\mathbb{R}^{m_2 \\times n_2}$ 的元素表示为 $(\\Phi_2)_{ik}$。那么 $\\Phi_2^T \\in \\mathbb{R}^{n_2 \\times m_2}$ 的元素是 $(\\Phi_2^T)_{ki} = (\\Phi_2)_{ik}$。$\\Phi_2^T$ 的第 $j$ 列是一个在 $\\mathbb{R}^{n_2}$ 中的向量，其第 $k$ 个元素是 $(\\Phi_2^T)_{kj} = (\\Phi_2)_{jk}$。\n\n现在我们可以将矩阵-向量积 $X (\\Phi_2^T e_j)$ 表示为 $X$ 列的线性组合：\n$$\nX (\\Phi_2^T e_j) = \\sum_{k=1}^{n_2} x_k (\\Phi_2^T)_{kj} = \\sum_{k=1}^{n_2} x_k (\\Phi_2)_{jk}\n$$\n将此代回 $y_j$ 的表达式，并利用矩阵乘法的线性性：\n$$\ny_j = \\Phi_1 \\left( \\sum_{k=1}^{n_2} (\\Phi_2)_{jk} x_k \\right) = \\sum_{k=1}^{n_2} (\\Phi_2)_{jk} (\\Phi_1 x_k)\n$$\n这个表达式定义了 $Y$ 的第 $j$ 列。向量化的测量值 $y = \\operatorname{vec}(Y)$ 是通过堆叠这些列（$j = 1, 2, \\ldots, m_2$）形成的：\n$$\ny = \\operatorname{vec}(Y) = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{m_2} \\end{bmatrix} = \\begin{bmatrix} \\sum_{k=1}^{n_2} (\\Phi_2)_{1k} (\\Phi_1 x_k) \\\\ \\sum_{k=1}^{n_2} (\\Phi_2)_{2k} (\\Phi_1 x_k) \\\\ \\vdots \\\\ \\sum_{k=1}^{n_2} (\\Phi_2)_{m_2,k} (\\Phi_1 x_k) \\end{bmatrix}\n$$\n现在，让我们分析表达式 $(\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)$。使用克罗内克积和向量化的定义：\n$$\n(\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X) =\n\\begin{bmatrix}\n(\\Phi_2)_{11}\\Phi_1  (\\Phi_2)_{12}\\Phi_1  \\cdots  (\\Phi_2)_{1,n_2}\\Phi_1 \\\\\n(\\Phi_2)_{21}\\Phi_1  (\\Phi_2)_{22}\\Phi_1  \\cdots  (\\Phi_2)_{2,n_2}\\Phi_1 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n(\\Phi_2)_{m_2,1}\\Phi_1  (\\Phi_2)_{m_2,2}\\Phi_1  \\cdots  (\\Phi_2)_{m_2,n_2}\\Phi_1\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{n_2} \\end{bmatrix}\n$$\n执行分块矩阵-向量乘法，结果向量的第 $j$ 个块（这是一个大小为 $m_1 \\times 1$ 的向量）是：\n$$\nj\\text{-th block} = \\sum_{k=1}^{n_2} ((\\Phi_2)_{jk}\\Phi_1) x_k = \\sum_{k=1}^{n_2} (\\Phi_2)_{jk} (\\Phi_1 x_k)\n$$\n这正是我们为 $y_j$（$Y$ 的第 $j$ 列）找到的表达式。由于 $(\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)$ 的第 $j$ 个块对于所有 $j=1, \\ldots, m_2$ 都等于 $Y$ 的第 $j$ 列，因此可以得出结论，完整的堆叠向量是相同的。因此，我们证明了：\n$$\ny = \\operatorname{vec}(\\Phi_1 X \\Phi_2^T) = (\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)\n$$\n这个恒等式表明，可分离感知操作（涉及两次矩阵乘法后跟向量化）等价于将一个由克罗内克积形成的单个大矩阵应用于向量化信号。\n\n**第二部分：伴随映射的推导**\n\n前向算子是线性映射 $A = \\Phi_2 \\otimes \\Phi_1$。伴随算子是其转置 $A^T$。我们的目标是推导一种高效的实现方法，用于将 $A^T$ 应用于测量向量 $y \\in \\mathbb{R}^{m_1 m_2}$，如问题中所述：首先将 $y$ 重塑为矩阵 $Y_{meas} = \\operatorname{unvec}(y) \\in \\mathbb{R}^{m_1 \\times m_2}$，然后计算 $\\Phi_1^T Y_{meas} \\Phi_2$，最后将结果向量化。\n\n首先，让我们建立性质 $(A \\otimes B)^T = A^T \\otimes B^T$。令 $A \\in \\mathbb{R}^{m \\times n}$ 和 $B \\in \\mathbb{R}^{p \\times q}$。克罗内克积 $C = A \\otimes B$ 是一个分块矩阵，其中第 $(i,j)$ 个块是 $C_{ij} = a_{ij}B$。转置 $C^T$ 是一个分块矩阵，其中第 $(i,j)$ 个块是 $(C_{ji})^T = (a_{ji}B)^T = a_{ji}B^T$。\n现在考虑矩阵 $D = A^T \\otimes B^T$。$A^T$ 的元素是 $(A^T)_{ij} = a_{ji}$。$D$ 的第 $(i,j)$ 个块是 $D_{ij} = (A^T)_{ij}B^T = a_{ji}B^T$。\n由于对应的块是相同的，我们验证了 $(A \\otimes B)^T = A^T \\otimes B^T$。\n\n将此性质应用于我们的前向算子 $A = \\Phi_2 \\otimes \\Phi_1$，伴随算子是：\n$$\nA^T = (\\Phi_2 \\otimes \\Phi_1)^T = \\Phi_2^T \\otimes \\Phi_1^T\n$$\n因此，应用于向量 $y$ 的伴随操作是 $(\\Phi_2^T \\otimes \\Phi_1^T)y$。\n\n现在让我们分析问题中提出的步骤。令 $y \\in \\mathbb{R}^{m_1 m_2}$ 为一个测量向量。\n1. 将 $y$ 重塑为矩阵 $Y_{meas} = \\operatorname{unvec}(y) \\in \\mathbb{R}^{m_1 \\times m_2}$。根据定义，$\\operatorname{vec}(Y_{meas}) = y$。\n2. 计算矩阵 $Z = \\Phi_1^T Y_{meas} \\Phi_2$。该矩阵的维度为 $(n_1 \\times m_1)(m_1 \\times m_2)(m_2 \\times n_2) \\to n_1 \\times n_2$。\n3. 将结果向量化以获得 $\\operatorname{vec}(Z)$。\n\n我们可以使用我们推导出的前向模型恒等式 $\\operatorname{vec}(ABC) = (C^T \\otimes A)\\operatorname{vec}(B)$，这是我们早期结果的一个推广。让我们将其应用于 $Z = \\Phi_1^T Y_{meas} \\Phi_2$。这里，$A = \\Phi_1^T$, $B = Y_{meas}$, 并且 $C=\\Phi_2$。\n$$\n\\operatorname{vec}(Z) = \\operatorname{vec}(\\Phi_1^T Y_{meas} \\Phi_2) = (\\Phi_2^T \\otimes \\Phi_1^T) \\operatorname{vec}(Y_{meas})\n$$\n由于 $\\operatorname{vec}(Y_{meas}) = y$，我们有：\n$$\n\\operatorname{vec}(Z) = (\\Phi_2^T \\otimes \\Phi_1^T) y\n$$\n这正是伴随操作 $A^T y$ 的表达式。因此，我们已经证明，伴随映射可以通过将测量向量 $y$ 重塑为 $m_1 \\times m_2$ 矩阵，应用与 $\\Phi_1^T$ 和 $\\Phi_2$ 的矩阵乘法，然后将结果向量化来计算：\n$$\n(\\Phi_2 \\otimes \\Phi_1)^T y = \\operatorname{vec}(\\Phi_1^T (\\operatorname{unvec}(y)) \\Phi_2)\n$$\n这个推导证实了可分离感知算子的伴随可以使用标准的矩阵乘法和重塑来高效实现，避免了显式转置克罗内克积矩阵 $A^T \\in \\mathbb{R}^{n_1 n_2 \\times m_1 m_2}$ 的计算成本高昂的形成和应用。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies identities for separable sensing in 2D.\n    The primary identity is vec(Phi1 * X * Phi2.T) = (Phi2 kron Phi1) * vec(X).\n    The adjoint identity is (Phi2 kron Phi1).T * y = vec(Phi1.T * unvec(y) * Phi2).\n    All vectorization and reshaping operations use column-major ('F') order.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"Phi1\": np.array([[0.6, -0.3, 0.1], [0.2, 0.5, -0.4]]),\n            \"Phi2\": np.array([[0.5, -0.2, 0.0, 0.3], [-0.1, 0.4, 0.6, -0.2], [0.0, 0.1, -0.3, 0.7]]),\n            \"X\": np.array([[0.0, 1.0, -1.5, 0.0], [0.0, 0.0, 0.0, -2.0], [0.5, 0.0, 0.0, 0.0]]),\n        },\n        # Test Case 2 (boundary dimensions)\n        {\n            \"Phi1\": np.array([[1.0, -1.0]]),\n            \"Phi2\": np.array([[2.0], [-3.0]]),\n            \"X\": np.array([[4.0], [-5.0]]),\n        },\n        # Test Case 3 (identity on one factor)\n        {\n            \"Phi1\": np.array([[1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]),\n            \"Phi2\": np.array([[0.8, -0.6], [0.3, 0.9]]),\n            \"X\": np.array([[1.0, 0.0], [0.0, -1.0], [2.0, 1.5]]),\n        },\n        # Test Case 4 (scalar signal)\n        {\n            \"Phi1\": np.array([[1.2], [-0.7]]),\n            \"Phi2\": np.array([[0.5], [-1.1]]),\n            \"X\": np.array([[3.0]]),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        Phi1 = case[\"Phi1\"]\n        Phi2 = case[\"Phi2\"]\n        X = case[\"X\"]\n        \n        m1, n1 = Phi1.shape\n        m2, n2 = Phi2.shape\n\n        # Column-major vectorization of X\n        vec_X = X.flatten(order='F')\n\n        # --- Forward Model Verification ---\n\n        # 1. Compute measurement using explicit Kronecker product\n        A = np.kron(Phi2, Phi1)\n        y1 = A @ vec_X\n        \n        # 2. Compute measurement using two matrix multiplies\n        Y = Phi1 @ X @ Phi2.T\n        y2 = Y.flatten(order='F')\n\n        # --- Adjoint Mapping Verification ---\n        \n        # 3. Compute adjoint using explicit Kronecker product transpose\n        At = A.T\n        x_adj1 = At @ y1\n        \n        # 4. Compute adjoint using two matrix multiplies on reshaped measurement\n        # Reshape measurement vector y1 into an m1 x m2 matrix using column-major order\n        Y_meas = y1.reshape((m1, m2), order='F')\n        \n        # Multiply on the left by Phi1.T and on the right by Phi2\n        X_adj_mat = Phi1.T @ Y_meas @ Phi2\n        \n        # Re-vectorize the resulting n1 x n2 matrix using column-major order\n        x_adj2 = X_adj_mat.flatten(order='F')\n        \n        # --- Calculate Errors ---\n        \n        error_measure = np.linalg.norm(y1 - y2)\n        error_adjoint = np.linalg.norm(x_adj1 - x_adj2)\n        \n        results.append(error_measure)\n        results.append(error_adjoint)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "建立了基本恒等式后，我们自然会问：如何利用它来求解问题？本练习将探讨如何高效地求解包含克罗内克积的线性系统，特别是如何计算形如 $(I + \\tau A \\otimes B)^{-1}$ 的算子。直接求逆在计算上是不可行的，而本练习将展示一种利用矩阵 $A$ 和 $B$ 的特征分解的优雅高效的解决方案。该方法通过在联合特征基中进行对角缩放，从而在不显式构造巨大的克罗内克积矩阵的情况下，实现对它的对角化，这真正展示了矩阵形式和向量化形式之间相互作用的威力。",
            "id": "3493470",
            "problem": "考虑矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和 $B \\in \\mathbb{R}^{m \\times m}$，它们均为实对称且可对角线正交对角化的矩阵，以及一个向量 $v \\in \\mathbb{R}^{mn}$。在压缩感知（CS）和稀疏优化中出现的结构化二次问题中，例如对矩阵型变量的可分离二次惩罚项进行近端映射时，会频繁出现线性算子 $I + \\tau A \\otimes B$。其中 $I$ 是 $mn \\times mn$ 的单位矩阵，$\\tau \\in \\mathbb{R}$ 是一个非负标量，$\\otimes$ 表示克罗内克积，$\\operatorname{vec}(\\cdot)$ 表示将矩阵的列按列主序堆叠成单个向量的向量化操作。对于大规模问题，在不显式构造克罗内克积的情况下，高效地将 $(I + \\tau A \\otimes B)^{-1}$ 应用于向量 $v$ 至关重要。\n\n仅从以下基本定义和经过充分检验的性质出发：\n- 对于任何满足乘法条件的矩阵 $M$、$X$ 和 $N$，向量化恒等式为 $ \\operatorname{vec}(M X N^{\\top}) = (N \\otimes M)\\,\\operatorname{vec}(X)$。\n- 实对称矩阵允许正交特征分解：如果 $A$ 是实对称矩阵，则存在一个正交矩阵 $U$ 和一个实对角矩阵 $\\Lambda$，使得 $A = U \\Lambda U^{\\top}$；类似地，对于 $B = V \\Sigma V^{\\top}$。\n- 克罗内克积满足 $(A \\otimes B)(u \\otimes v) = (A u) \\otimes (B v)$，适用于任何维度兼容的向量 $u$ 和 $v$。\n\n您的任务：\n1. 从第一性原理出发，根据 $A$ 和 $B$ 的特征分解推导出 $A \\otimes B$ 的谱结构。利用此结构设计一个算法，用于计算 $w = (I + \\tau A \\otimes B)^{-1} v$，而无需显式构造 $A \\otimes B$。该算法必须通过将 $v$ 重塑为矩阵 $Y \\in \\mathbb{R}^{m \\times n}$（使用列主序），变换到联合特征基，执行对角缩放，然后变换回来，且与向量化恒等式保持一致的方式来操作。解释为何该算法是正确的，并且在数值上比直接求逆更高效。\n2. 将此算法实现为一个程序。对于给定的一组测试用例，该程序通过推导出的方法计算 $w$，并通过与显式构造 $I + \\tau A \\otimes B$ 并求解相应线性系统的直接法进行比较来验证结果。对于每个测试用例，将两个结果向量之间的最大绝对差报告为单个浮点数。\n\n重要的实现约定：\n- 对所有 $\\operatorname{vec}(\\cdot)$ 操作使用列主序向量化：对于 $X \\in \\mathbb{R}^{m \\times n}$，$\\operatorname{vec}(X)$ 按顺序堆叠 $X$ 的列；在代码中，这对应于在重塑时使用顺序 \"F\"。\n- 将 $(I + \\tau A \\otimes B)$ 应用于 $\\operatorname{vec}(X)$ 时，通过向量化恒等式将其解释为 $\\operatorname{vec}(X + \\tau B X A)$。\n\n测试套件：\n让程序处理以下五个测试用例。每个用例包括 $(A, B, \\tau, v)$，其中 $A$ 和 $B$ 被指定为实对称矩阵，$\\tau$ 是一个实标量，$v$ 是一个长度为 $mn$ 的实向量，并按列主序解释。所有数字都是无量纲的实标量。\n\n- 用例 1 (通用，非平凡维度):\n  - $A = \\begin{bmatrix} 3  1  0 \\\\ 1  2  1 \\\\ 0  1  1 \\end{bmatrix}$, $B = \\begin{bmatrix} 2  0.5 \\\\ 0.5  1.5 \\end{bmatrix}$,\n  - $\\tau = 0.5$,\n  - $v = [1.0,\\,-0.5,\\,0.3,\\,2.0,\\,-1.0,\\,0.7]$ in $\\mathbb{R}^{6}$，按列主序解释为 $\\operatorname{vec}(Y)$，其中 $Y \\in \\mathbb{R}^{2 \\times 3}$。\n- 用例 2 (边界条件 $\\tau=0$):\n  - $A = \\begin{bmatrix} 1  0.2 \\\\ 0.2  1.3 \\end{bmatrix}$, $B = \\begin{bmatrix} 1.1  0.4 \\\\ 0.4  0.9 \\end{bmatrix}$,\n  - $\\tau = 0$,\n  - $v = [0.2,\\,-0.1,\\,0.4,\\,0.9]$ in $\\mathbb{R}^{4}$，按列主序解释为 $\\operatorname{vec}(Y)$，其中 $Y \\in \\mathbb{R}^{2 \\times 2}$。\n- 用例 3 (重复特征值):\n  - $A = I_2$, $B = I_3$,\n  - $\\tau = 1.2$,\n  - $v = [0.1,\\,0.2,\\,0.3,\\,0.4,\\,0.5,\\,0.6]$ in $\\mathbb{R}^{6}$，按列主序解释为 $\\operatorname{vec}(Y)$，其中 $Y \\in \\mathbb{R}^{3 \\times 2}$。\n- 用例 4 (零向量输入):\n  - $A = \\begin{bmatrix} 2  0.3 \\\\ 0.3  1 \\end{bmatrix}$, $B = \\begin{bmatrix} 1.5  0.2 \\\\ 0.2  1.1 \\end{bmatrix}$,\n  - $\\tau = 0.8$,\n  - $v = [0.0,\\,0.0,\\,0.0,\\,0.0]$ in $\\mathbb{R}^{4}$，按列主序解释为 $\\operatorname{vec}(Y)$，其中 $Y \\in \\mathbb{R}^{2 \\times 2}$。\n- 用例 5 (病态缩放):\n  - $A = \\begin{bmatrix} 10^{-3}  0 \\\\ 0  10 \\end{bmatrix}$, $B = \\begin{bmatrix} 5  1 \\\\ 1  0.5 \\end{bmatrix}$,\n  - $\\tau = 50$,\n  - $v = [1.0,\\,2.0,\\,3.0,\\,4.0]$ in $\\mathbb{R}^{4}$，按列主序解释为 $\\operatorname{vec}(Y)$，其中 $Y \\in \\mathbb{R}^{2 \\times 2}$。\n\n输出规范：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。每个条目必须是通过推导的特征基算法计算的向量与通过显式构造 $I + \\tau A \\otimes B$ 并求解相应线性系统的直接法计算的向量之间的最大绝对差，顺序与上面列出的测试用例相同。例如：\"[result1,result2,result3,result4,result5]\"。所有结果必须是浮点数。",
            "solution": "问题陈述已经过验证，被认为是有效的。这是一个线性代数中定义明确的问题，在计算科学，特别是稀疏优化和压缩感知领域有直接应用。其前提在科学上是合理的，定义是一致的，目标是明确的。\n\n我们的任务是高效计算 $w = (I + \\tau A \\otimes B)^{-1} v$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 和 $B \\in \\mathbb{R}^{m \\times m}$ 是实对称矩阵，$\\tau \\ge 0$ 是一个标量，$v \\in \\mathbb{R}^{mn}$ 是一个向量。解决方案需要推导一种算法，以避免显式构造 $mn \\times mn$ 的克罗内克积矩阵 $A \\otimes B$。\n\n设向量 $v$ 是矩阵 $Y \\in \\mathbb{R}^{m \\times n}$ 的列主序向量化，即 $v = \\operatorname{vec}(Y)$。类似地，设解向量 $w$ 是矩阵 $X \\in \\mathbb{R}^{m \\times n}$ 的向量化，因此 $w = \\operatorname{vec}(X)$。待求解的方程是：\n$$ (I + \\tau A \\otimes B) w = v $$\n$$ (I + \\tau A \\otimes B) \\operatorname{vec}(X) = \\operatorname{vec}(Y) $$\n利用 $\\operatorname{vec}(\\cdot)$ 算子的线性性质以及给定的恒等式 $\\operatorname{vec}(M X N^{\\top}) = (N \\otimes M)\\operatorname{vec}(X)$，我们可以重写左侧。鉴于 $A$ 是对称的（$A = A^{\\top}$），我们可以在恒等式中识别出 $N=A$ 和 $M=B$：\n$$ (A \\otimes B)\\operatorname{vec}(X) = \\operatorname{vec}(B X A^{\\top}) = \\operatorname{vec}(B X A) $$\n因此，方程变为：\n$$ \\operatorname{vec}(X) + \\tau \\operatorname{vec}(B X A) = \\operatorname{vec}(Y) $$\n$$ \\operatorname{vec}(X + \\tau B X A) = \\operatorname{vec}(Y) $$\n这等价于矩阵方程：\n$$ X + \\tau B X A = Y $$\n我们的目标是高效地求解这个线性矩阵方程以得到 $X$。\n\n**1. 算子的谱分解**\n\n矩阵 $A$ 和 $B$ 是实对称的，因此它们允许正交特征分解。\n设 $A = U \\Lambda U^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 是一个正交矩阵（$U U^{\\top} = U^{\\top} U = I_n$），其列是 $A$ 的特征向量，$\\Lambda \\in \\mathbb{R}^{n \\times n}$ 是相应实特征值的对角矩阵，$\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$。\n设 $B = V \\Sigma V^{\\top}$，其中 $V \\in \\mathbb{R}^{m \\times m}$ 是一个正交矩阵（$V V^{\\top} = V^{\\top} V = I_m$），其列是 $B$ 的特征向量，$\\Sigma \\in \\mathbb{R}^{m \\times m}$ 是相应实特征值的对角矩阵，$\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_m)$。\n\n将这些分解代入矩阵方程：\n$$ X + \\tau (V \\Sigma V^{\\top}) X (U \\Lambda U^{\\top}) = Y $$\n我们可以在方程左边预乘 $V^{\\top}$，右边后乘 $U$：\n$$ V^{\\top} X U + \\tau (V^{\\top} V \\Sigma V^{\\top}) X (U \\Lambda U^{\\top} U) = V^{\\top} Y U $$\n因为 $V^{\\top} V = I_m$ 和 $U^{\\top} U = I_n$：\n$$ V^{\\top} X U + \\tau \\Sigma (V^{\\top} X U) \\Lambda = V^{\\top} Y U $$\n我们定义变换后的矩阵 $\\hat{X} = V^{\\top} X U$ 和 $\\hat{Y} = V^{\\top} Y U$。方程简化为：\n$$ \\hat{X} + \\tau \\Sigma \\hat{X} \\Lambda = \\hat{Y} $$\n该方程表示在 $A$ 和 $B$ 的联合特征基中的问题。由于 $\\Sigma$ 和 $\\Lambda$ 是对角矩阵，乘积 $\\Sigma \\hat{X} \\Lambda$ 的计算非常直接。设 $\\hat{X}_{ji}$ 是 $\\hat{X}$ 在第 $j$ 行第 $i$ 列的元素。$\\Sigma \\hat{X} \\Lambda$ 的对应元素是：\n$$ (\\Sigma \\hat{X} \\Lambda)_{ji} = \\sum_{k=1}^{m} \\sum_{l=1}^{n} \\Sigma_{jk} \\hat{X}_{kl} \\Lambda_{li} = \\Sigma_{jj} \\hat{X}_{ji} \\Lambda_{ii} = \\sigma_j \\lambda_i \\hat{X}_{ji} $$\n因此，矩阵方程解耦为 $mn$ 个独立的标量方程：\n$$ \\hat{X}_{ji} + \\tau \\sigma_j \\lambda_i \\hat{X}_{ji} = \\hat{Y}_{ji} $$\n$$ (1 + \\tau \\sigma_j \\lambda_i) \\hat{X}_{ji} = \\hat{Y}_{ji} $$\n由于 $\\tau \\ge 0$ 且所提供测试用例中矩阵的特征值使得 $1 + \\tau \\sigma_j \\lambda_i \\neq 0$，我们可以求解每个 $\\hat{X}_{ji}$：\n$$ \\hat{X}_{ji} = \\frac{\\hat{Y}_{ji}}{1 + \\tau \\sigma_j \\lambda_i} $$\n此操作是矩阵 $\\hat{Y}$ 与缩放矩阵 $D$ 的逐元素相除，其中 $D_{ji} = 1 + \\tau \\sigma_j \\lambda_i$。该缩放矩阵可以通过特征值向量 $\\boldsymbol{\\sigma} = [\\sigma_1, \\dots, \\sigma_m]^{\\top}$ 和 $\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_n]^{\\top}$ 的外积构造，即 $D = 1 + \\tau (\\boldsymbol{\\sigma} \\boldsymbol{\\lambda}^{\\top})$。\n\n**2. 快速算法的推导**\n\n上述推导产生了一个清晰、高效的求解 $X$ 以及随后的 $w = \\operatorname{vec}(X)$ 的流程。\n\n总体算法如下：\n1.  给定 $v \\in \\mathbb{R}^{mn}$，使用列主序将其重塑为 $m \\times n$ 矩阵 $Y$。\n2.  计算 $A = U \\Lambda U^{\\top}$ 和 $B = V \\Sigma V^{\\top}$ 的特征分解，以获得正交矩阵 $U, V$ 和特征值向量 $\\boldsymbol{\\lambda}, \\boldsymbol{\\sigma}$。\n3.  **变换到联合特征基**：计算 $\\hat{Y} = V^{\\top} Y U$。\n4.  **执行对角缩放**：通过将 $\\hat{Y}$ 逐元素除以缩放矩阵 $D = 1 + \\tau(\\boldsymbol{\\sigma} \\boldsymbol{\\lambda}^{\\top})$ 来计算 $\\hat{X}$。即 $\\hat{X} = \\hat{Y} \\oslash D$。\n5.  **变换回标准基**：计算 $X = V \\hat{X} U^{\\top}$。\n6.  最终解向量是 $w = \\operatorname{vec}(X)$，通过按列主序扁平化 $X$ 获得。\n\n**3. 正确性与效率分析**\n\n**正确性**：该算法是通过基变换直接从原始矩阵方程推导出来的。每一步都是等价变换，确保最终解 $X$ 满足原始方程 $X + \\tau BXA = Y$，因此 $w = \\operatorname{vec}(X)$ 是 $(I+\\tau A \\otimes B)w=v$ 的正确解。\n\n**效率**：\n-   **直接法**：这涉及显式构造 $mn \\times mn$ 矩阵 $M = I + \\tau (A \\otimes B)$ 并求解线性系统 $Mw = v$。\n    -   构造 $A \\otimes B$：$O(m^2 n^2)$ 的运算量和内存。\n    -   求解系统（例如，使用 LU 分解）：$O((mn)^3) = O(m^3 n^3)$ 的运算量。\n    对于中等规模的矩阵（例如，$m,n \\approx 100$），这种方法在计算上是不可行的。\n\n-   **推导的算法**：该方法避免了大型矩阵，并在大小为 $n \\times n$、$m \\times m$ 和 $m \\times n$ 的矩阵上操作。\n    -   $A$ 和 $B$ 的特征分解：分别为 $O(n^3)$ 和 $O(m^3)$ 的运算量。\n    -   矩阵乘法（$V^{\\top}YU$、$V\\hat{X}U^{\\top}$）：每个乘法涉及类似 $(V^{\\top}Y)U$ 的运算，成本为 $O(m^2n + mn^2) = O(mn(m+n))$。\n    -   缩放步骤：$O(mn)$ 的运算量。\n    总时间复杂度由特征分解和矩阵乘法主导，得到 $O(n^3 + m^3 + mn(m+n))$。空间复杂度由存储矩阵 $A, B, U, V$ 主导，为 $O(n^2 + m^2)$。\n\n此分析证实，与直接法相比，推导的算法在计算和内存需求方面都有显著降低，使其适用于大规模问题。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a predefined set of test cases.\n    For each case, it computes w = (I + tau*A kron B)^-1 v efficiently\n    and compares it to the direct (brute-force) computation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[3.0, 1.0, 0.0], [1.0, 2.0, 1.0], [0.0, 1.0, 1.0]]),\n            \"B\": np.array([[2.0, 0.5], [0.5, 1.5]]),\n            \"tau\": 0.5,\n            \"v\": np.array([1.0, -0.5, 0.3, 2.0, -1.0, 0.7]),\n        },\n        {\n            \"A\": np.array([[1.0, 0.2], [0.2, 1.3]]),\n            \"B\": np.array([[1.1, 0.4], [0.4, 0.9]]),\n            \"tau\": 0.0,\n            \"v\": np.array([0.2, -0.1, 0.4, 0.9]),\n        },\n        {\n            \"A\": np.eye(2),\n            \"B\": np.eye(3),\n            \"tau\": 1.2,\n            \"v\": np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]),\n        },\n        {\n            \"A\": np.array([[2.0, 0.3], [0.3, 1.0]]),\n            \"B\": np.array([[1.5, 0.2], [0.2, 1.1]]),\n            \"tau\": 0.8,\n            \"v\": np.zeros(4),\n        },\n        {\n            \"A\": np.array([[1e-3, 0.0], [0.0, 10.0]]),\n            \"B\": np.array([[5.0, 1.0], [1.0, 0.5]]),\n            \"tau\": 50.0,\n            \"v\": np.array([1.0, 2.0, 3.0, 4.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        B = case[\"B\"]\n        tau = case[\"tau\"]\n        v = case[\"v\"]\n        \n        n = A.shape[0]\n        m = B.shape[0]\n\n        # --- Fast Eigenbasis Algorithm ---\n        \n        # 1. Eigendecompositions of A and B\n        evals_A, U = np.linalg.eigh(A)\n        evals_B, V = np.linalg.eigh(B)\n        \n        # 2. Reshape v into matrix Y (column-major)\n        Y = v.reshape((m, n), order='F')\n        \n        # 3. Transform to joint eigenbasis\n        Y_hat = V.T @ Y @ U\n        \n        # 4. Perform diagonal scaling\n        # Construct the scaling matrix from outer product of eigenvalues\n        scaling_denominators = 1.0 + tau * np.outer(evals_B, evals_A)\n        X_hat = Y_hat / scaling_denominators\n        \n        # 5. Transform back to standard basis\n        X = V @ X_hat @ U.T\n        \n        # 6. Vectorize result\n        w_fast = X.flatten(order='F')\n\n        # --- Direct Method for Validation ---\n        \n        # 1. Form the full Kronecker product matrix\n        I_mn = np.eye(m * n)\n        A_kron_B = np.kron(A, B)\n        \n        # 2. Form the linear system matrix\n        M = I_mn + tau * A_kron_B\n        \n        # 3. Solve the linear system\n        w_direct = np.linalg.solve(M, v)\n        \n        # --- Compare results ---\n        max_abs_diff = np.max(np.abs(w_fast - w_direct))\n        results.append(max_abs_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们将这些概念应用于一个前沿领域：稀疏优化。许多现代优化算法，如迭代软阈值算法 (ISTA) 及其加速版本 (FISTA)，都依赖于近端算子 (proximal operator)。本练习专注于推导混合$\\ell_{2,1}$范数的近端算子，该范数用于促进组稀疏性。您的任务是将这个本质上是逐列进行的操作，统一表达为作用于向量化变量的、包含克罗内克积结构的单一算子，从而为解决多任务学习或联合稀疏恢复问题提供关键的算法构件。",
            "id": "3493466",
            "problem": "考虑一个多任务压缩感知设置，其中信号矩阵 $S \\in \\mathbb{R}^{m \\times n}$ 通过最小化一个平滑数据保真项与一个混合 $\\ell_{2,1}$ 范数之和来估计，该混合范数能够引导列之间的联合稀疏性，即 $\\sum_{j=1}^{n} \\|S_{:,j}\\|_{2}$。对于任意 $Z \\in \\mathbb{R}^{m \\times n}$，设一个真、闭、凸函数 $f$ 的近端算子定义为 $\\operatorname{prox}_{f}(Z) = \\arg\\min_{S} \\left\\{ \\frac{1}{2} \\|S - Z\\|_{F}^{2} + f(S) \\right\\}$，其中 $\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数。对于一个固定的 $\\lambda > 0$，请从基本原理出发，利用近端算子、弗罗贝尼乌斯范数和向量化的核心定义，推导出 $f(S) = \\lambda \\sum_{j=1}^{n} \\|S_{:,j}\\|_{2}$ 的矩阵形式的近端算子。然后，通过显式构造以下内容，将此近端算子表示为一个作用于 $\\operatorname{vec}(S) \\in \\mathbb{R}^{mn}$ 的组稀疏性近端算子，其中 $\\operatorname{vec}(\\cdot)$ 是按列堆叠的向量化算子：\n- 一个置换矩阵 $\\Pi \\in \\mathbb{R}^{mn \\times mn}$，它将 $\\operatorname{vec}(S)$ 重排为 $n$ 个大小为 $m$ 的连续块，对应于 $S$ 的各列（对于按列堆叠，可取 $\\Pi = I_{mn}$，或者如果采用不同的堆叠约定，$\\Pi$ 可等效地作为规范的vec-置换矩阵），以及\n- 一个克罗内克积结构的块算子，它在每个组中应用按列的近端收缩。\n\n您的最终表达式必须将作用于 $\\operatorname{vec}(Z)$ 的近端算子呈现为包含 $\\Pi$、克罗内克积和标准基向量 $\\{e_{j}\\}_{j=1}^{n}$ 的单一闭式算子，其中 $e_{j} \\in \\mathbb{R}^{n}$ 是第 $j$ 个标准基向量。最终答案必须是单一的闭式解析表达式。无需进行数值计算。",
            "solution": "函数 $f(S) = \\lambda \\sum_{j=1}^{n} \\|S_{:,j}\\|_{2}$ 的近端算子定义为以下优化问题的解：\n$$\nS^* = \\operatorname{prox}_{f}(Z) = \\arg\\min_{S \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\frac{1}{2} \\|S - Z\\|_{F}^{2} + \\lambda \\sum_{j=1}^{n} \\|S_{:,j}\\|_{2} \\right\\}\n$$\n\n矩阵的弗罗贝尼乌斯范数是其所有元素平方和的平方根。弗罗贝尼乌斯范数的平方可以分解为各列的欧几里得范数平方之和：\n$$\n\\|S - Z\\|_{F}^{2} = \\sum_{i=1}^{m} \\sum_{j=1}^{n} (S_{ij} - Z_{ij})^2 = \\sum_{j=1}^{n} \\left( \\sum_{i=1}^{m} (S_{ij} - Z_{ij})^2 \\right) = \\sum_{j=1}^{n} \\|S_{:,j} - Z_{:,j}\\|_{2}^{2}\n$$\n将此代入目标函数，我们得到：\n$$\nJ(S) = \\frac{1}{2} \\sum_{j=1}^{n} \\|S_{:,j} - Z_{:,j}\\|_{2}^{2} + \\lambda \\sum_{j=1}^{n} \\|S_{:,j}\\|_{2} = \\sum_{j=1}^{n} \\left( \\frac{1}{2} \\|S_{:,j} - Z_{:,j}\\|_{2}^{2} + \\lambda \\|S_{:,j}\\|_{2} \\right)\n$$\n目标函数是各项之和，其中每一项仅依赖于单个列 $S_{:,j}$。因此，该最小化问题是可分的，可以对每一列独立求解：\n$$\nS_{:,j}^* = \\arg\\min_{s_j \\in \\mathbb{R}^{m}} \\left\\{ \\frac{1}{2} \\|s_j - Z_{:,j}\\|_{2}^{2} + \\lambda \\|s_j\\|_{2} \\right\\} \\quad \\text{for } j=1, \\dots, n\n$$\n这就是 $\\ell_2$ 范数的近端算子的定义，通常称为组或块软阈值算子。令 $s_j = S_{:,j}$ 和 $z_j = Z_{:,j}$。此凸问题的一阶最优性条件表明，零向量必须位于最小化子 $s_j^*$ 处目标函数的次微分中：\n$$\n0 \\in \\partial \\left( \\frac{1}{2} \\|s_j^* - z_j\\|_{2}^{2} + \\lambda \\|s_j^*\\|_{2} \\right) = (s_j^* - z_j) + \\lambda \\partial \\|s_j^*\\|_{2}\n$$\n这等价于 $z_j - s_j^* \\in \\lambda \\partial \\|s_j^*\\|_{2}$。$\\ell_2$ 范数的次微分是：\n$$\n\\partial \\|s_j^*\\|_{2} = \\begin{cases} \\{ \\frac{s_j^*}{\\|s_j^*\\|_{2}} \\}  \\text{if } s_j^* \\neq 0 \\\\ \\{ u \\in \\mathbb{R}^m : \\|u\\|_2 \\le 1 \\}  \\text{if } s_j^* = 0 \\end{cases}\n$$\n我们考虑两种情况：\n情况1：$\\|z_j\\|_2 > \\lambda$。\n我们期望一个非零解，$s_j^* \\neq 0$。最优性条件变为 $z_j - s_j^* = \\lambda \\frac{s_j^*}{\\|s_j^*\\|_{2}}$。这意味着 $z_j = s_j^* \\left(1 + \\frac{\\lambda}{\\|s_j^*\\|_{2}}\\right)$。因此，$s_j^*$ 必须是 $z_j$ 的一个正常数倍缩放，即对于某个标量 $c > 0$，有 $s_j^* = c z_j$。取范数，我们得到 $\\|s_j^*\\|_2 = c\\|z_j\\|_2$。代入回去：$z_j = c z_j \\left(1 + \\frac{\\lambda}{c\\|z_j\\|_2}\\right)$，这给出 $1 = c + \\frac{\\lambda}{\\|z_j\\|_2}$。解出 $c$ 得到 $c = 1 - \\frac{\\lambda}{\\|z_j\\|_2}$。由于 $\\|z_j\\|_2 > \\lambda$，我们有 $c > 0$，这与我们的假设一致。解是 $s_j^* = \\left(1 - \\frac{\\lambda}{\\|z_j\\|_2}\\right) z_j$。\n\n情况2：$\\|z_j\\|_2 \\le \\lambda$。\n我们检验 $s_j^* = 0$ 是否为解。最优性条件变为 $z_j - 0 \\in \\lambda \\{ u : \\|u\\|_2 \\le 1 \\}$，简化为 $\\|z_j\\|_2 \\le \\lambda$。这与我们的情况假设一致。因此，如果 $\\|z_j\\|_2 \\le \\lambda$，解就是 $s_j^*=0$。\n\n综合这两种情况，我们得到块软阈值公式：\n$$\nS_{:,j}^* = \\left(1 - \\frac{\\lambda}{\\|Z_{:,j}\\|_{2}}\\right)_{+} Z_{:,j}\n$$\n其中 $(x)_+ = \\max(0, x)$。这个公式即使在 $Z_{:,j}=0$ 时也是良定义的，因为 $\\|Z_{:,j}\\|_2 = 0 \\le \\lambda$，这属于第二种情况，得出 $S_{:,j}^*=0$。\n\n通过对 $Z$ 的每一列应用此操作，形成结果矩阵 $S^*$：\n$$\nS^* = \\left[ \\left(1 - \\frac{\\lambda}{\\|Z_{:,1}\\|_{2}}\\right)_{+} Z_{:,1} \\quad \\dots \\quad \\left(1 - \\frac{\\lambda}{\\|Z_{:,n}\\|_{2}}\\right)_{+} Z_{:,n} \\right]\n$$\n\n接下来，我们在向量化域中表示此操作。令 $v = \\operatorname{vec}(Z) \\in \\mathbb{R}^{mn}$。按列堆叠算子 `vec` 将 $Z$ 的各列排列成一个单一向量：\n$$\nv = \\operatorname{vec}(Z) = \\begin{pmatrix} Z_{:,1} \\\\ Z_{:,2} \\\\ \\vdots \\\\ Z_{:,n} \\end{pmatrix}\n$$\n$Z$ 的各列已经构成了 $n$ 个大小为 $m$ 的连续块。问题要求使用一个置换矩阵 $\\Pi$ 来将向量元素重排成这样的组。由于按列堆叠的 `vec` 操作已经产生了这种结构，因此不需要置换。我们可以形式上将置换矩阵设为单位矩阵，$\\Pi = I_{mn} \\in \\mathbb{R}^{mn \\times mn}$。置换后的向量是 $\\Pi v = v$。\n\n近端算子的作用是缩放这 $n$ 个块中的每一个。第 $j$ 个块是向量 $Z_{:,j}$。我们可以使用一个选择矩阵从 $v$ 中提取这个块。这个选择矩阵可以用克罗内克积来构造。令 $e_j \\in \\mathbb{R}^n$ 为第 $j$ 个标准基向量。第 $j$ 个块的提取算子是 $e_j^T \\otimes I_m \\in \\mathbb{R}^{m \\times mn}$。将其应用于 $v$：\n$$\n(e_j^T \\otimes I_m) v = Z_{:,j}\n$$\n因此，第 $j$ 个块的范数是 $\\|(e_j^T \\otimes I_m) v\\|_2$。第 $j$ 个块的收缩系数是：\n$$\nc_j(v) = \\left(1 - \\frac{\\lambda}{\\|(e_j^T \\otimes I_m) v\\|_2}\\right)_{+}\n$$\n整体算子将标量 $c_j(v)$ 应用于 $v$ 的第 $j$ 个块。这可以由一个块对角矩阵表示，该矩阵可以构造为秩为 $m$ 的变换之和：\n$$\n\\sum_{j=1}^{n} c_j(v) \\begin{pmatrix} 0   \\\\  \\ddots   \\\\   I_m  \\\\    \\ddots  \\\\     0 \\end{pmatrix}\n$$\n其中 $I_m$ 位于第 $j$ 个块对角位置。这个块对角矩阵可以用克罗内克积优雅地写出：$e_j e_j^T \\otimes I_m$ 是一个在第 $(j,j)$ 块位置为 $I_m$、其余位置为零的矩阵。\n作用于 $v$ 的完整算子是与矩阵 $\\sum_{j=1}^{n} c_j(v) (e_j e_j^T \\otimes I_m)$ 相乘。\n\n将所有组件（包括置换矩阵 $\\Pi$）组合在一起，我们可以写出向量化的近端算子，我们将其表示为 $\\mathcal{P}_{f}(v)$，如下所示。首先，对输入向量进行置换：$v' = \\Pi v$。然后，应用块级收缩。第 $j$ 个块的标量是 $c_j(v') = \\left(1 - \\frac{\\lambda}{\\|(e_j^T \\otimes I_m) v'\\|_2}\\right)_{+}$。收缩后的新向量是 $v'' = \\left( \\sum_{j=1}^{n} c_j(v') (e_j e_j^T \\otimes I_m) \\right)v'$。最后，应用逆置换：$\\mathcal{P}_{f}(v) = \\Pi^T v''$。代入表达式，我们得到：\n$$\n\\mathcal{P}_{f}(v) = \\Pi^T \\left( \\sum_{j=1}^{n} \\left(1 - \\frac{\\lambda}{\\|(e_j^T \\otimes I_m) \\Pi v\\|_2}\\right)_{+} (e_j e_j^T \\otimes I_m) \\right) \\Pi v\n$$\n这是作用于向量化输入 $v = \\operatorname{vec}(Z)$ 的近端算子的最终闭式表达式。它按要求显式地使用了置换矩阵 $\\Pi$（在本例中为 $I_{mn}$）、克罗内克积和标准基向量 $e_j$。",
            "answer": "$$\n\\boxed{\\Pi^T \\left( \\sum_{j=1}^{n} \\left(1 - \\frac{\\lambda}{\\|(e_j^T \\otimes I_m) \\Pi v\\|_2}\\right)_{+} (e_j e_j^T \\otimes I_m) \\right) \\Pi v}\n$$"
        }
    ]
}