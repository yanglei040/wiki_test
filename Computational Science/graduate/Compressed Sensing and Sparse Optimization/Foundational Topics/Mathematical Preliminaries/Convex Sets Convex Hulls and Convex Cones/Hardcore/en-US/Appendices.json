{
    "hands_on_practices": [
        {
            "introduction": "The $\\ell_1$ unit ball is a cornerstone of sparse optimization, primarily because it is the convex hull of the signed canonical basis vectors—the sparsest unit vectors. This exercise challenges you to formalize this relationship by constructively representing any point within this ball as a convex combination of these sparse \"atoms\". By engaging with this task , you will develop a concrete understanding of the ball's geometric composition and explore its combinatorial complexity through the lens of Carathéodory's theorem.",
            "id": "3440599",
            "problem": "In the analysis of sparse recovery via convex relaxation, one frequently encounters the atomic norm defined by the set of atoms given by the signed canonical basis in dimension $n$, namely $\\mathcal{A} = \\{\\pm e_{i} : i \\in \\{1,\\dots,n\\}\\}$, where $e_{i}$ denotes the $i$-th canonical basis vector in $\\mathbb{R}^{n}$. The corresponding atomic norm ball is the $\\ell_{1}$ unit ball $B_{1} = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq 1\\}$, which is a centrally symmetric convex polytope.\n\nStarting only from the definitions of convex combination, convex hull, and the $\\ell_{1}$ norm, carry out the following:\n\n1. Provide a constructive representation that, for any $x \\in B_{1}$, expresses $x$ as a convex combination of signed canonical basis vectors $v_{j} \\in \\mathcal{A}$ with nonnegative weights $\\lambda_{j}$ that sum to $1$, i.e., $x = \\sum_{j=1}^{m} \\lambda_{j} v_{j}$ with $\\lambda_{j} \\geq 0$ and $\\sum_{j=1}^{m} \\lambda_{j} = 1$. Your construction must give explicit weights in terms of $x$ and must be valid for all $x$ with $\\|x\\|_{1} \\leq 1$.\n\n2. Quantify, in terms of the ambient dimension $n$, the smallest integer $M(n)$ such that every point $x \\in B_{1}$ can be represented as above using no more than $M(n)$ signed canonical basis vectors. Provide your final answer for $M(n)$ as a closed-form analytic expression in $n$.\n\nThe final answer must be a single analytic expression. No rounding is required and no physical units are involved.",
            "solution": "The problem statement has been validated and is deemed valid. It is a well-posed mathematical problem in the field of convex geometry, specifically concerning the properties of the $\\ell_1$ unit ball in $\\mathbb{R}^n$. All terms are standard and the premises are factually correct within the mathematical context.\n\nThe problem is divided into two parts. First, we must provide a constructive method to express any point $x$ in the $\\ell_1$ unit ball, $B_1 = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq 1\\}$, as a convex combination of the atoms $\\mathcal{A} = \\{\\pm e_{i} : i \\in \\{1,\\dots,n\\}\\}$. Second, we must determine the smallest integer $M(n)$ which is the maximum number of atoms required for any such representation.\n\n**Part 1: Constructive Representation**\n\nLet $x = (x_1, x_2, \\dots, x_n)$ be an arbitrary point in the $\\ell_1$ unit ball $B_1$. By definition, its $\\ell_1$ norm is $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i| \\leq 1$. We seek to find coefficients $\\lambda_j \\ge 0$ and atoms $v_j \\in \\mathcal{A}$ such that $x = \\sum_j \\lambda_j v_j$ and $\\sum_j \\lambda_j = 1$.\n\nFor each component $x_i$ of the vector $x$, we define its positive and negative parts:\n$x_i^+ = \\max(x_i, 0)$\n$x_i^- = \\max(-x_i, 0)$\nThese definitions ensure that $x_i = x_i^+ - x_i^-$ and $|x_i| = x_i^+ + x_i^-$. Also, for any given $i$, at least one of $x_i^+$ or $x_i^-$ must be zero.\n\nWe can express the vector $x$ as a linear combination of the canonical basis vectors $e_i$:\n$$x = \\sum_{i=1}^{n} x_i e_i = \\sum_{i=1}^{n} (x_i^+ - x_i^-) e_i$$\nThis can be rewritten using the atoms from $\\mathcal{A}$ by distributing the sum:\n$$x = \\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)$$\nThe vectors in this sum, $e_i$ and $-e_i$, are all members of the atomic set $\\mathcal{A}$. The coefficients, $x_i^+$ and $x_i^-$, are all non-negative by definition.\n\nLet's compute the sum of these coefficients:\n$$S_c = \\sum_{i=1}^{n} (x_i^+ + x_i^-) = \\sum_{i=1}^{n} |x_i| = \\|x\\|_1$$\nBy the problem definition, we have $S_c = \\|x\\|_1 \\le 1$.\n\nCase 1: $\\|x\\|_1 = 1$.\nIf $\\|x\\|_1 = 1$, the sum of the non-negative coefficients $x_i^+$ and $x_i^-$ is exactly $1$. In this case, the representation\n$$x = \\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)$$\nis a valid convex combination of atoms from $\\mathcal{A}$. The weights are explicitly $\\lambda_{e_i} = x_i^+$ and $\\lambda_{-e_i} = x_i^-$.\n\nCase 2: $\\|x\\|_1  1$.\nIf $\\|x\\|_1 = S_c  1$, the sum of coefficients is less than $1$, so it is not a convex combination. To form a valid convex combination, the sum of weights must be $1$. We can achieve this by adding a term that sums to $x$ while ensuring the total weights sum to $1$. This is possible because the origin, $0 \\in \\mathbb{R}^n$, is in the convex hull of $\\mathcal{A}$. We can introduce the origin with a weight of $(1 - S_c)$:\n$$x = \\left(\\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)\\right) + (1 - S_c) \\cdot 0$$\nThe coefficients are now $\\{x_i^+\\}_{i=1}^n$, $\\{x_i^-\\}_{i=1}^n$, and $(1-S_c)$. They are all non-negative and their sum is $S_c + (1-S_c) = 1$. This expresses $x$ as a convex combination of atoms from $\\mathcal{A}$ and the origin. To complete the construction, we must express the origin as a convex combination of atoms from $\\mathcal{A}$. A simple way is to use any atom and its negative, for instance $e_1$ and $-e_1$:\n$$0 = \\frac{1}{2} e_1 + \\frac{1}{2} (-e_1)$$\nSubstituting this into our expression for $x$:\n$$x = \\left(\\sum_{i=1}^{n} x_i^+ e_i + \\sum_{i=1}^{n} x_i^- (-e_i)\\right) + (1 - S_c) \\left(\\frac{1}{2} e_1 + \\frac{1}{2} (-e_1)\\right)$$\nWe can group the terms by the atoms:\n$$x = \\left(x_1^+ + \\frac{1 - S_c}{2}\\right) e_1 + \\left(x_1^- + \\frac{1 - S_c}{2}\\right) (-e_1) + \\sum_{i=2}^{n} x_i^+ e_i + \\sum_{i=2}^{n} x_i^- (-e_i)$$\nThis is the required constructive representation. The weights are explicitly given in terms of the components of $x$ and its $\\ell_1$ norm $S_c = \\|x\\|_1$.\nThe weights are:\n- For atom $e_1$: $\\lambda_{e_1} = x_1^+ + \\frac{1-\\|x\\|_1}{2}$\n- For atom $-e_1$: $\\lambda_{-e_1} = x_1^- + \\frac{1-\\|x\\|_1}{2}$\n- For atoms $e_i$, $i \\ge 2$: $\\lambda_{e_i} = x_i^+$\n- For atoms $-e_i$, $i \\ge 2$: $\\lambda_{-e_i} = x_i^-$\nAll other weights are zero. Since $\\|x\\|_1 \\le 1$ and $x_i^\\pm \\ge 0$, all these weights are non-negative. Their sum is $(x_1^+ + x_1^-) + (1-\\|x\\|_1) + \\sum_{i=2}^n(x_i^+ + x_i^-) = |x_1| + 1 - \\|x\\|_1 + \\sum_{i=2}^n |x_i| = \\sum_{i=1}^n |x_i| + 1 - \\|x\\|_1 = \\|x\\|_1 + 1 - \\|x\\|_1 = 1$. The construction is valid for all $x \\in B_1$.\n\n**Part 2: Quantification of M(n)**\n\nWe need to find the smallest integer $M(n)$ such that any point $x \\in B_1 = \\text{conv}(\\mathcal{A})$ can be written as a convex combination of at most $M(n)$ points from $\\mathcal{A}$. This number is known as the Carathéodory number for the set $\\mathcal{A}$.\n\nCarathéodory's theorem states that if a point lies in the convex hull of a set $P$ in an affine space of dimension $d$, it can be expressed as a convex combination of at most $d+1$ points from $P$. Here, our points $\\mathcal{A}$ are in $\\mathbb{R}^n$, which has dimension $n$. Therefore, any point $x \\in B_1$ can be represented as a convex combination of at most $n+1$ atoms from $\\mathcal{A}$. This provides an upper bound: $M(n) \\le n+1$.\n\nTo determine if this bound is tight, we must check if there exists a point in $B_1$ that requires exactly $n+1$ atoms for its representation. Let's construct such a point.\nConsider the point $x = (c, c, \\dots, c) \\in \\mathbb{R}^n$ for some constant $c  0$. For this point to be in $B_1$, we must have $\\|x\\|_1 = \\sum_{i=1}^n |c| = nc \\le 1$. Let's choose $c$ such that $0  nc  1$, for example, $c = \\frac{1}{2n}$. The point is $x = (\\frac{1}{2n}, \\frac{1}{2n}, \\dots, \\frac{1}{2n})$. Its $\\ell_1$ norm is $\\|x\\|_1 = n(\\frac{1}{2n}) = \\frac{1}{2}$, so it is in $B_1$.\n\nSuppose $x$ can be expressed as a convex combination of $m$ atoms, $\\{v_1, \\dots, v_m\\} \\subset \\mathcal{A}$:\n$$x = \\sum_{j=1}^{m} \\lambda_j v_j, \\quad \\text{with } \\lambda_j \\ge 0 \\text{ and } \\sum_{j=1}^{m} \\lambda_j = 1.$$\nLooking at the $k$-th component of this vector equation for any $k \\in \\{1, \\dots, n\\}$:\n$$x_k = \\frac{1}{2n} = \\sum_{j=1}^{m} \\lambda_j (v_j)_k$$\nwhere $(v_j)_k$ is the $k$-th component of the atom $v_j$. Since $\\lambda_j \\ge 0$ and the sum is positive, for each $k$, there must be at least one atom $v_j$ in the sum for which $(v_j)_k  0$. The only atom in $\\mathcal{A}$ with a positive $k$-th component is $e_k$. Thus, for each $k \\in \\{1, \\dots, n\\}$, the atom $e_k$ must be included in the set $\\{v_1, \\dots, v_m\\}$.\nThis implies that the set of atoms used must contain the set $\\{e_1, e_2, \\dots, e_n\\}$. Therefore, the number of atoms $m$ must be at least $n$.\n\nNow, let's test if $m=n$ is possible. If $m=n$, the set of atoms must be exactly $\\{e_1, e_2, \\dots, e_n\\}$. The convex combination representation would be:\n$$x = \\sum_{i=1}^{n} \\lambda_i e_i$$\nThis implies that for each $i$, $x_i = \\lambda_i$. So, $\\lambda_i = \\frac{1}{2n}$ for all $i=1, \\dots, n$.\nThe sum of these coefficients is:\n$$\\sum_{i=1}^{n} \\lambda_i = \\sum_{i=1}^{n} \\frac{1}{2n} = n \\cdot \\frac{1}{2n} = \\frac{1}{2}$$\nFor a convex combination, the sum of coefficients must be $1$. Since $\\frac{1}{2} \\ne 1$, the point $x=(\\frac{1}{2n}, \\dots, \\frac{1}{2n})$ cannot be represented as a convex combination of $n$ atoms.\n\nTherefore, this point requires more than $n$ atoms. Since Carathéodory's theorem guarantees a representation with at most $n+1$ atoms, the minimum number of atoms required for this particular point must be exactly $n+1$.\nSince we have found a point in $B_1$ that requires $n+1$ atoms, the smallest integer $M(n)$ that works for all points in $B_1$ cannot be smaller than $n+1$.\nThis establishes the lower bound $M(n) \\ge n+1$.\n\nCombining the upper bound from Carathéodory's theorem ($M(n) \\le n+1$) and the lower bound derived from our specific example ($M(n) \\ge n+1$), we conclude that $M(n)$ must be exactly $n+1$.",
            "answer": "$$\n\\boxed{n+1}\n$$"
        },
        {
            "introduction": "The effectiveness of $\\ell_1$ minimization in promoting sparsity is not accidental; it is a direct consequence of the geometry of the $\\ell_1$ ball. This exercise  explores this connection by asking you to identify the \"exposed face\" where a linear functional attains its maximum value. By working through this problem, you will see how the extremal points of this interaction correspond to the sparse vertices of the $\\ell_1$ ball, providing a clear geometric intuition for the success of methods like Basis Pursuit.",
            "id": "3440587",
            "problem": "Consider the $\\ell_{1}$ unit ball $B_{1} \\subset \\mathbb{R}^{n}$ defined by $B_{1} = \\{ y \\in \\mathbb{R}^{n} : \\|y\\|_{1} \\leq 1 \\}$ and a linear functional $f(y) = \\langle y, c \\rangle$ for a fixed vector $c \\in \\mathbb{R}^{n}$. In compressed sensing and sparse optimization, the $\\ell_{1}$ norm plays a central role in promoting sparsity, as in Basis Pursuit (BP), and the geometry of $B_{1}$ determines which sparse patterns are favored by linear objectives. An exposed face of a convex set $C$ with respect to a functional $f$ is defined as $F = \\{ x \\in C : f(x) = \\sup_{z \\in C} f(z) \\}$.\n\nStarting from core definitions of norms, dual norms, and exposed faces in convex analysis, derive the supremum value $\\sup_{y \\in B_{1}} \\langle y, c \\rangle$ and characterize the exposed face $F_{c} = \\{ y \\in B_{1} : \\langle y, c \\rangle = \\sup_{z \\in B_{1}} \\langle z, c \\rangle \\}$ in terms of the index set where $|c_{i}|$ attains its maximum and the corresponding sign pattern. Then, specialize to the concrete case with $n = 7$ and\n$$\nc = (2,\\,-2,\\,2,\\,-1,\\,0,\\,\\tfrac{1}{2},\\,-2).\n$$\nDetermine the dimension of the exposed face $F_{c}$ of $B_{1}$ induced by $f(y) = \\langle y, c \\rangle$ for this specific $c$. Provide your final answer as a single integer.",
            "solution": "The problem is well-posed and consistent with standard definitions in convex analysis and optimization. We may proceed with the solution.\n\nThe problem asks for the dimension of an exposed face of the $\\ell_1$ unit ball $B_1 = \\{ y \\in \\mathbb{R}^{n} : \\|y\\|_{1} \\leq 1 \\}$, where $\\|y\\|_1 = \\sum_{i=1}^n |y_i|$. The face is induced by the linear functional $f(y) = \\langle y, c \\rangle$ for a given vector $c \\in \\mathbb{R}^n$. The exposed face $F_c$ is defined as the set of points in $B_1$ where the functional attains its maximum value.\n$$F_{c} = \\{ y \\in B_{1} : \\langle y, c \\rangle = \\sup_{z \\in B_{1}} \\langle z, c \\rangle \\}$$\n\nFirst, we must determine the supremum value $\\sup_{z \\in B_{1}} \\langle z, c \\rangle$. This value is given by the dual norm of the $\\ell_1$ norm, which is the $\\ell_\\infty$ norm, evaluated at $c$. The $\\ell_\\infty$ norm is defined as $\\|c\\|_\\infty = \\max_{1 \\le i \\le n} |c_i|$. We can prove this directly.\n\nFor any $y \\in B_1$, the inner product $\\langle y, c \\rangle$ can be written as $\\sum_{i=1}^n y_i c_i$. We can bound this term:\n$$ \\langle y, c \\rangle = \\sum_{i=1}^n y_i c_i \\le \\sum_{i=1}^n |y_i c_i| = \\sum_{i=1}^n |y_i| |c_i| $$\nSince $|c_i| \\le \\|c\\|_\\infty$ for all $i$, we have:\n$$ \\sum_{i=1}^n |y_i| |c_i| \\le \\sum_{i=1}^n |y_i| \\|c\\|_\\infty = \\|c\\|_\\infty \\sum_{i=1}^n |y_i| = \\|c\\|_\\infty \\|y\\|_1 $$\nAs $y \\in B_1$, we know $\\|y\\|_1 \\le 1$. Therefore, we have the upper bound:\n$$ \\langle y, c \\rangle \\le \\|c\\|_\\infty \\|y\\|_1 \\le \\|c\\|_\\infty $$\nThis shows that $\\sup_{y \\in B_{1}} \\langle y, c \\rangle \\le \\|c\\|_\\infty$. To show that this supremum is indeed $\\|c\\|_\\infty$, we must demonstrate that this bound is attained for some $y \\in B_1$. Let $k$ be an index for which $|c_k| = \\|c\\|_\\infty$. Consider the vector $y^* \\in \\mathbb{R}^n$ with components defined as:\n$$ y^*_i = \\begin{cases} \\operatorname{sign}(c_k)  \\text{if } i=k \\\\ 0  \\text{if } i \\neq k \\end{cases} $$\nwhere $\\operatorname{sign}(x)$ is $1$ if $x0$, $-1$ if $x0$, and can be taken as $0$ if $x=0$. If $\\|c\\|_\\infty = 0$, then $c=0$ and the problem is trivial. We assume $c \\neq 0$, so $\\|c\\|_\\infty  0$ and $c_k \\neq 0$.\nThe $\\ell_1$ norm of this vector is $\\|y^*\\|_1 = |\\operatorname{sign}(c_k)| = 1$, so $y^* \\in B_1$.\nThe value of the functional at $y^*$ is:\n$$ \\langle y^*, c \\rangle = \\sum_{i=1}^n y^*_i c_i = y^*_k c_k = \\operatorname{sign}(c_k) c_k = |c_k| = \\|c\\|_\\infty $$\nThus, the supremum is attained, and $\\sup_{z \\in B_{1}} \\langle z, c \\rangle = \\|c\\|_\\infty$.\n\nNow, we characterize the exposed face $F_c = \\{ y \\in B_1 : \\langle y, c \\rangle = \\|c\\|_\\infty \\}$. The equality $\\langle y, c \\rangle = \\|c\\|_\\infty$ holds if and only if all inequalities in the derivation above are equalities.\nLet's analyze the conditions for equality:\n1.  $\\|c\\|_\\infty \\|y\\|_1 = \\|c\\|_\\infty$: Since we assume $c \\neq 0$, this requires $\\|y\\|_1 = 1$.\n2.  $\\sum_{i=1}^n |y_i| |c_i| = \\|c\\|_\\infty \\|y\\|_1$: With $\\|y\\|_1=1$, this becomes $\\sum_{i=1}^n |y_i| |c_i| = \\|c\\|_\\infty$. This can be rewritten as $\\sum_{i=1}^n |y_i| |c_i| = \\|c\\|_\\infty \\sum_{i=1}^n |y_i|$, or $\\sum_{i=1}^n |y_i| (\\|c\\|_\\infty - |c_i|) = 0$. Since each term $|y_i| (\\|c\\|_\\infty - |c_i|)$ is non-negative, the sum is zero if and only if each term is zero. This implies that if $|c_i|  \\|c\\|_\\infty$, then $|y_i|$ must be $0$. Let $I_{\\max} = \\{i : |c_i| = \\|c\\|_\\infty \\}$. This condition states that $y_i = 0$ for all $i \\notin I_{\\max}$.\n3.  $\\sum_{i=1}^n y_i c_i = \\sum_{i=1}^n |y_i||c_i|$: This equality holds if and only if $y_i c_i \\ge 0$ for all $i$. For $i \\in I_{\\max}$, this means $y_i$ must have the same sign as $c_i$ (or be zero). That is, $\\operatorname{sign}(y_i) = \\operatorname{sign}(c_i)$ if $y_i \\ne 0$.\n\nCombining these conditions, a vector $y$ lies in the face $F_c$ if and only if:\na) $y_i = 0$ for all $i \\notin I_{\\max}$.\nb) For all $i \\in I_{\\max}$, $y_i$ has the same sign as $c_i$ (i.e., $y_i c_i \\ge 0$).\nc) $\\sum_{i \\in I_{\\max}} |y_i| = 1$.\n\nLet $k = |I_{\\max}|$. The vectors in $F_c$ are of the form $y = \\sum_{i \\in I_{\\max}} y_i e_i$, where $e_i$ is the $i$-th standard basis vector. Let $\\alpha_i = |y_i|$ for $i \\in I_{\\max}$. The conditions become $\\alpha_i \\ge 0$ and $\\sum_{i \\in I_{\\max}} \\alpha_i = 1$. The vector $y$ can be written as $y = \\sum_{i \\in I_{\\max}} \\alpha_i (\\operatorname{sign}(c_i) e_i)$.\nThis shows that $F_c$ is the convex hull of the $k$ vertices $\\{ \\operatorname{sign}(c_i)e_i \\}_{i \\in I_{\\max}}$. These $k$ vectors are vertices of the $\\ell_1$ ball. They are linearly independent since they involve different standard basis vectors. Therefore, they are also affinely independent. The dimension of the convex hull of $k$ affinely independent points is $k-1$.\nThus, the dimension of the exposed face $F_c$ is $\\dim(F_c) = |I_{\\max}| - 1$.\n\nWe are given the specific case $n=7$ and $c = (2, -2, 2, -1, 0, \\frac{1}{2}, -2)$.\nFirst, we find the $\\ell_\\infty$ norm of $c$. We compute the absolute values of its components:\n$|c_1|=2$, $|c_2|=2$, $|c_3|=2$, $|c_4|=1$, $|c_5|=0$, $|c_6|=\\frac{1}{2}$, $|c_7|=2$.\nThe maximum of these values is $\\|c\\|_\\infty = 2$.\n\nNext, we identify the index set $I_{\\max}$ where the absolute value of the component equals $\\|c\\|_\\infty$:\n$$I_{\\max} = \\{ i \\in \\{1, \\dots, 7\\} : |c_i| = 2 \\}$$\nBy inspection, the indices are $1$, $2$, $3$, and $7$.\nSo, $I_{\\max} = \\{1, 2, 3, 7\\}$.\n\nThe number of elements in this set is $|I_{\\max}| = 4$.\nThe dimension of the exposed face $F_c$ is given by $|I_{\\max}| - 1$.\n$$\\dim(F_c) = 4 - 1 = 3$$\nThe face $F_c$ is the convex hull of the four vertices $\\{e_1, -e_2, e_3, -e_7\\}$. A point $y$ on this face has $y_4=y_5=y_6=0$, with components $y_1, y_2, y_3, y_7$ satisfying $y_1 \\ge 0$, $y_2 \\le 0$, $y_3 \\ge 0$, $y_7 \\le 0$ and $y_1 - y_2 + y_3 - y_7 = 1$. This set is a $3$-dimensional simplex.",
            "answer": "$$\\boxed{3}$$"
        },
        {
            "introduction": "To fully understand optimality conditions in constrained optimization, we must move from the global structure of a convex set to its local geometry. The normal cone is the fundamental concept for this, characterizing the set of all outward-pointing directions from a point on the boundary. In this practice , you will derive the complete polyhedral description of the normal cone to the $\\ell_1$ ball at a sparse point, solidifying your understanding of the geometric constraints that underpin recovery guarantees in compressed sensing.",
            "id": "3440629",
            "problem": "Let $n \\in \\mathbb{N}$ and let $k \\in \\{1,\\dots,n\\}$. Consider the $\\ell_{1}$ ball of radius $\\tau0$ in $\\mathbb{R}^{n}$, defined as $B_{1}(\\tau) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\le \\tau\\}$. Let $x^{\\star} \\in \\mathbb{R}^{n}$ be a boundary point of $B_{1}(\\tau)$ with sparsity level $k$, meaning $\\|x^{\\star}\\|_{1}=\\tau$ and $|\\operatorname{supp}(x^{\\star})|=k$. Denote the support by $S \\coloneqq \\operatorname{supp}(x^{\\star}) \\subset \\{1,\\dots,n\\}$ and the sign pattern on $S$ by $s \\in \\{-1,1\\}^{S}$, so that for each $i \\in S$ we have $\\operatorname{sign}(x^{\\star}_{i}) = s_{i}$.\n\nUsing only core definitions from convex analysis, namely: (i) the normal cone $N_{C}(x) \\coloneqq \\{v \\in \\mathbb{R}^{n} : \\langle v, y-x \\rangle \\le 0 \\ \\text{for all} \\ y \\in C\\}$ for a nonempty closed convex set $C \\subset \\mathbb{R}^{n}$, (ii) the subdifferential $\\partial f(x) \\coloneqq \\{g \\in \\mathbb{R}^{n} : f(y) \\ge f(x) + \\langle g, y-x \\rangle \\ \\text{for all} \\ y \\in \\mathbb{R}^{n}\\}$ for a proper closed convex function $f$, and (iii) separability of the $\\ell_{1}$ norm across coordinates, derive the extreme rays of the normal cone $N_{B_{1}(\\tau)}(x^{\\star})$ at $x^{\\star}$, and express this cone in both its halfspace representation (H-representation, meaning a finite system of linear equalities and inequalities) and its generator representation (V-representation, meaning a conic hull of finitely many rays). Your derivation should explicitly exhibit how the support $S$ and sign vector $s$ determine these representations.\n\nFinally, report as your answer a single closed-form analytic expression for the total number of extreme rays of $N_{B_{1}(\\tau)}(x^{\\star})$ as a function of $n$ and $k$. No rounding is required, and no units apply. The final boxed answer must be only that expression.",
            "solution": "The problem asks for the characterization of the normal cone to the $\\ell_1$ ball $B_1(\\tau) = \\{x \\in \\mathbb{R}^n : \\|x\\|_1 \\le \\tau\\}$ at a specific boundary point $x^\\star$. The point $x^\\star$ is defined by its norm $\\|x^\\star\\|_1 = \\tau$ and its sparsity level $|\\operatorname{supp}(x^\\star)| = k$. Let $S \\coloneqq \\operatorname{supp}(x^\\star)$ be the support of $x^\\star$, with $|S|=k$, and let $s \\in \\{-1, 1\\}^S$ be the vector of signs of the non-zero components of $x^\\star$, i.e., $s_i = \\operatorname{sign}(x^\\star_i)$ for all $i \\in S$. The problem requires deriving the H-representation and V-representation of the normal cone $N_{B_1(\\tau)}(x^\\star)$ and then finding the total number of its extreme rays.\n\nThe derivation will proceed by first relating the normal cone to the subdifferential of the $\\ell_1$ norm, then characterizing this subdifferential, and finally using this characterization to derive the cone's representations and count its extreme rays.\n\nThe set $B_1(\\tau)$ is the $\\tau$-sublevel set of the convex function $f(x) = \\|x\\|_1$. For a convex function $f$, the normal cone to its $\\tau$-sublevel set $C = \\{y : f(y) \\le \\tau\\}$ at a point $x$ on the boundary (where $f(x) = \\tau$) is given by the conic hull of the subdifferential of $f$ at $x$. This is a standard result from convex analysis.\n$$N_{B_1(\\tau)}(x^\\star) = \\operatorname{cone}(\\partial f(x^\\star)) = \\operatorname{cone}(\\partial \\|x^\\star\\|_1)$$\nwhere $\\operatorname{cone}(A) \\coloneqq \\{\\lambda a : \\lambda \\ge 0, a \\in A\\}$. Our task thus reduces to first finding the subdifferential of the $\\ell_1$ norm at $x^\\star$.\n\nThe $\\ell_1$ norm is separable, $f(x) = \\|x\\|_1 = \\sum_{i=1}^n |x_i|$. The subdifferential of a sum of independent convex functions is the Cartesian product of their individual subdifferentials. Therefore,\n$$\\partial \\|x^\\star\\|_1 = \\partial |x^\\star_1| \\times \\partial |x^\\star_2| \\times \\dots \\times \\partial |x^\\star_n|$$\nThe subdifferential of the scalar absolute value function $g(t)=|t|$ is:\n$$\\partial g(t) = \\begin{cases} \\{1\\}  \\text{if } t  0 \\\\ \\{-1\\}  \\text{if } t  0 \\\\ [-1, 1]  \\text{if } t = 0 \\end{cases}$$\nWe apply this to each component of $x^\\star$. Let $S^c \\coloneqq \\{1, \\dots, n\\} \\setminus S$ be the complement of the support.\nFor indices $i \\in S$, we have $x^\\star_i \\neq 0$, and $\\operatorname{sign}(x^\\star_i) = s_i$. Thus, $\\partial |x^\\star_i| = \\{s_i\\}$.\nFor indices $i \\in S^c$, we have $x^\\star_i = 0$. Thus, $\\partial |x^\\star_i| = [-1, 1]$.\n\nCombining these results, the subdifferential $\\partial \\|x^\\star\\|_1$ is the set of all vectors $g \\in \\mathbb{R}^n$ such that:\n$$g_i = s_i \\quad \\text{for all } i \\in S$$\n$$g_i \\in [-1, 1] \\quad \\text{for all } i \\in S^c$$\nThis set is a convex polytope in $\\mathbb{R}^n$.\n\nNow we can characterize the normal cone $N_{B_1(\\tau)}(x^\\star) = \\operatorname{cone}(\\partial \\|x^\\star\\|_1)$. A vector $v \\in \\mathbb{R}^n$ belongs to this cone if and only if $v = \\lambda g$ for some scalar $\\lambda \\ge 0$ and some vector $g \\in \\partial \\|x^\\star\\|_1$.\nThis implies:\n$1.$ For $i \\in S$, $v_i = \\lambda s_i$.\n$2.$ For $i \\in S^c$, $v_i = \\lambda g_i$ where $g_i \\in [-1, 1]$.\n\nFrom the first condition, if $S$ is non-empty (which it is, as $k \\ge 1$), we can express $\\lambda$ in terms of the components of $v$. For any $j \\in S$, we have $\\lambda = v_j / s_j$. Since $s_j^2 = 1$, this is equivalent to $\\lambda = s_j v_j$. The condition $\\lambda \\ge 0$ becomes $s_j v_j \\ge 0$. Since this must hold for any choice of $j \\in S$, it implies that $s_i v_i = s_j v_j$ for all $i, j \\in S$, and this common value must be non-negative.\n\nFrom the second condition, for $i \\in S^c$, we have $|v_i| = |\\lambda g_i| = \\lambda |g_i|$. Since $|g_i| \\le 1$, we get $|v_i| \\le \\lambda$. Substituting $\\lambda = s_j v_j$ for any $j \\in S$, we have $|v_i| \\le s_j v_j$ for all $i \\in S^c$.\n\nThis leads to the halfspace representation (H-representation) of the normal cone. A vector $v \\in \\mathbb{R}^n$ is in $N_{B_1(\\tau)}(x^\\star)$ if and only if it satisfies the following system of linear equalities and inequalities. Let us fix an arbitrary index $j_0 \\in S$:\n\\begin{itemize}\n    \\item $s_i v_i - s_{j_0} v_{j_0} = 0$ for all $i \\in S \\setminus \\{j_0\\}$. These $k-1$ equalities ensure that $s_i v_i$ is constant across the support $S$.\n    \\item $s_{j_0} v_{j_0} - v_i \\ge 0$ for all $i \\in S^c$.\n    \\item $s_{j_0} v_{j_0} + v_i \\ge 0$ for all $i \\in S^c$. These $2(n-k)$ inequalities correspond to $|v_i| \\le s_{j_0} v_{j_0}$.\n\\end{itemize}\nThe condition $s_{j_0} v_{j_0} \\ge 0$ is also necessary. If $kn$ (i.e., $S^c$ is not empty), this condition is redundant as it can be obtained by summing the two inequalities $s_{j_0} v_{j_0} - v_i \\ge 0$ and $s_{j_0} v_{j_0} + v_i \\ge 0$. If $k=n$ (i.e., $S^c$ is empty), the cone is defined by $s_i v_i = s_j v_j$ for all $i,j$ and $s_{j_0} v_{j_0} \\ge 0$.\n\nNext, we derive the generator representation (V-representation). The normal cone $N_{B_1(\\tau)}(x^\\star)$ is a polyhedral cone, and its extreme rays are the non-negative multiples of the vertices of its base polytope, $\\partial \\|x^\\star\\|_1$. We therefore need to find the vertices of $\\partial \\|x^\\star\\|_1$.\nThe set $\\partial \\|x^\\star\\|_1$ is a Cartesian product of simpler sets:\n$$\\partial \\|x^\\star\\|_1 = \\left( \\prod_{i \\in S} \\{s_i\\} \\right) \\times \\left( \\prod_{j \\in S^c} [-1, 1] \\right)$$\nThe vertices of a Cartesian product of polytopes are the elements formed by taking the Cartesian product of the vertices of the individual polytopes.\nFor each $i \\in S$, the set is a point $\\{s_i\\}$, which has only one vertex: $s_i$.\nFor each $j \\in S^c$, the set is the interval $[-1, 1]$, which has two vertices: $-1$ and $1$.\n\nA vector $g$ is a vertex of $\\partial \\|x^\\star\\|_1$ if and only if its components are chosen from the vertices of the corresponding component sets. This means:\n$$g_i = s_i \\quad \\text{for all } i \\in S$$\n$$g_i \\in \\{-1, 1\\} \\quad \\text{for all } i \\in S^c$$\nThe set of these vertex vectors, let's call it $G_{ext}$, generates all the extreme rays. The V-representation of the normal cone is $N_{B_1(\\tau)}(x^\\star) = \\operatorname{cone}(G_{ext})$.\n\nFinally, we must find the total number of extreme rays. This is equal to the number of vertices of $\\partial \\|x^\\star\\|_1$, which is the cardinality of the set $G_{ext}$.\nTo define a vector in $G_{ext}$, we have no choice for the components $g_i$ with $i \\in S$; they are fixed to $s_i$. For the components $g_i$ with $i \\in S^c$, we have two choices for each component: either $-1$ or $1$.\nThe size of the support $S$ is $k$. The size of its complement $S^c$ is $n-k$.\nThere are $n-k$ indices for which we can make a choice. For each of these $n-k$ indices, there are $2$ independent choices. By the rule of product in combinatorics, the total number of distinct vertex vectors is:\n$$ \\underbrace{2 \\times 2 \\times \\dots \\times 2}_{n-k \\text{ times}} = 2^{n-k} $$\nThis is the total number of extreme rays of the normal cone $N_{B_1(\\tau)}(x^\\star)$.\nThis formula holds for all allowed values of $k$. If $k=n$, then $S^c$ is empty, and the number of rays is $2^{n-n} = 2^0 = 1$. This is correct, as in this case $\\partial \\|x^\\star\\|_1$ is the single point $s$, and the normal cone is the single ray $\\{\\lambda s : \\lambda \\ge 0\\}$. If $k=1$, the number is $2^{n-1}$.\n\nThe number of extreme rays is a function of $n$ and $k$, and is independent of $\\tau$ and the specific sign pattern $s$.",
            "answer": "$$\\boxed{2^{n-k}}$$"
        }
    ]
}