{
    "hands_on_practices": [
        {
            "introduction": "A crucial step in many sparse recovery algorithms is the final debiasing, where a least-squares problem is solved on the identified support set. This practice explores how the geometric correlation between selected columns directly impacts the algebraic properties of the underlying system. By analytically calculating the eigenvalues and condition number of an equi-correlated Gram matrix, you will forge a direct link between the matrix structure and the numerical stability and noise sensitivity of the recovery process, providing a foundational understanding of how column coherence affects performance .",
            "id": "3445779",
            "problem": "In sparse recovery pipelines, one often performs support-restricted debiasing after an initial support selection (for example, by the Least Absolute Shrinkage and Selection Operator (LASSO)). Consider a sensing matrix with unit-norm columns, and let $S$ be a selected support of size $s=4$. Suppose the columns indexed by $S$ are equi-correlated: every off-diagonal inner product equals a common coherence level $\\mu \\in [0,1)$. Consequently, the Gram matrix is\n$$\nG \\equiv A_{S}^{\\top}A_{S} = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top},\n$$\nwhere $I_{4}$ is the $4 \\times 4$ identity and $\\mathbf{1}$ denotes the $4$-dimensional all-ones vector. Using only core definitions from linear algebra and properties of positive semidefinite matrices, compute the spectral condition number $\\kappa_{2}(G)$, defined as the ratio of the largest to the smallest eigenvalue of $G$, in closed form as a function of $\\mu$. Then, starting from first principles, justify what the computed $\\kappa_{2}(G)$ implies about the numerical accuracy and noise sensitivity of support-restricted least-squares debiasing on $S$ under additive white Gaussian noise (AWGN), compared to the orthonormal case. The final answer must be your closed-form expression for $\\kappa_{2}(G)$ in terms of $\\mu$.",
            "solution": "The problem asks for the spectral condition number of a specific Gram matrix and an interpretation of its meaning in the context of sparse recovery. I will first validate the problem statement. The problem provides a well-defined mathematical object, the matrix $G = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top}$ for $\\mu \\in [0,1)$, and asks for a standard, well-defined quantity, the spectral condition number $\\kappa_{2}(G)$. The context of sparse recovery and least-squares debiasing is scientifically sound and standard in the field. All terms are clearly defined, and no contradictions or missing information are present. The problem is therefore valid.\n\nTo compute the spectral condition number $\\kappa_{2}(G)$, we must find the ratio of the largest to the smallest eigenvalue of $G$. We proceed by finding the eigenvalues of $G$ directly from the core definition $G\\mathbf{v} = \\lambda\\mathbf{v}$, where $\\lambda$ is an eigenvalue and $\\mathbf{v}$ is its corresponding non-zero eigenvector.\n\nThe given matrix is $G = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top}$, where $I_{4}$ is the $4 \\times 4$ identity matrix and $\\mathbf{1}$ is the $4 \\times 1$ vector of all ones. The eigenvalue equation is:\n$$\n\\left( (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top} \\right) \\mathbf{v} = \\lambda \\mathbf{v}\n$$\nApplying the terms to the vector $\\mathbf{v}$:\n$$\n(1-\\mu)\\mathbf{v} + \\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = \\lambda\\mathbf{v}\n$$\nRearranging the terms, we get:\n$$\n\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - (1-\\mu))\\mathbf{v}\n$$\n$$\n\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - 1 + \\mu)\\mathbf{v}\n$$\nWe analyze this equation for two distinct cases for the eigenvector $\\mathbf{v}$.\n\nCase 1: The eigenvector $\\mathbf{v}$ is orthogonal to the all-ones vector $\\mathbf{1}$.\nIn this case, the inner product $\\mathbf{1}^{\\top}\\mathbf{v} = 0$. The left-hand side of the equation becomes the zero vector:\n$$\n\\mathbf{0} = (\\lambda - 1 + \\mu)\\mathbf{v}\n$$\nSince $\\mathbf{v}$ is an eigenvector, it must be non-zero ($\\mathbf{v} \\neq \\mathbf{0}$). Therefore, the scalar multiple must be zero:\n$$\n\\lambda - 1 + \\mu = 0 \\implies \\lambda = 1 - \\mu\n$$\nThe space of vectors in $\\mathbb{R}^4$ orthogonal to $\\mathbf{1}$ is a subspace of dimension $4-1=3$. Thus, there are $3$ linearly independent eigenvectors associated with the eigenvalue $\\lambda = 1 - \\mu$. The multiplicity of this eigenvalue is $3$.\n\nCase 2: The eigenvector $\\mathbf{v}$ is not orthogonal to $\\mathbf{1}$.\nFrom the equation $\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - 1 + \\mu)\\mathbf{v}$, we can see that if the quantities on both sides are non-zero, then $\\mathbf{v}$ must be a scalar multiple of $\\mathbf{1}$. Let's test the vector $\\mathbf{v} = \\mathbf{1}$ itself.\nThe inner product is $\\mathbf{1}^{\\top}\\mathbf{v} = \\mathbf{1}^{\\top}\\mathbf{1} = 1^2 + 1^2 + 1^2 + 1^2 = 4$.\nSubstituting $\\mathbf{v}=\\mathbf{1}$ into the eigenvalue equation:\n$$\nG\\mathbf{1} = \\left( (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top} \\right) \\mathbf{1} = (1-\\mu)\\mathbf{1} + \\mu\\mathbf{1}(\\mathbf{1}^{\\top}\\mathbf{1}) = (1-\\mu)\\mathbf{1} + \\mu\\mathbf{1}(4)\n$$\n$$\nG\\mathbf{1} = (1-\\mu+4\\mu)\\mathbf{1} = (1+3\\mu)\\mathbf{1}\n$$\nThis shows that $\\mathbf{1}$ is an eigenvector with the corresponding eigenvalue $\\lambda = 1+3\\mu$.\n\nThe matrix $G$ is a $4 \\times 4$ matrix, so it has $4$ eigenvalues. We have found all of them:\n$$\n\\lambda_{1} = 1+3\\mu \\quad (\\text{multiplicity } 1)\n$$\n$$\n\\lambda_{2,3,4} = 1-\\mu \\quad (\\text{multiplicity } 3)\n$$\nThe problem specifies that $\\mu \\in [0, 1)$. For any $\\mu$ in this interval, $1+3\\mu \\geq 1$ and $1-\\mu > 0$. Thus, all eigenvalues are strictly positive, which confirms that $G$ is a positive definite matrix.\n\nTo find the spectral condition number $\\kappa_2(G)$, we need to identify the maximum and minimum eigenvalues, $\\lambda_{\\max}(G)$ and $\\lambda_{\\min}(G)$. We compare the two distinct eigenvalue expressions: $1+3\\mu$ and $1-\\mu$.\nFor $\\mu \\in [0,1)$, we have $3\\mu \\geq 0$ and $-\\mu \\leq 0$. Therefore, $1+3\\mu \\geq 1$ and $1-\\mu \\leq 1$.\nMore directly, $3\\mu \\geq -\\mu$ for $\\mu \\geq 0$, which implies $1+3\\mu \\geq 1-\\mu$. Equality holds only if $\\mu=0$.\nSo, for $\\mu \\in [0, 1)$:\n$$\n\\lambda_{\\max}(G) = 1+3\\mu\n$$\n$$\n\\lambda_{\\min}(G) = 1-\\mu\n$$\nThe spectral condition number is defined as their ratio:\n$$\n\\kappa_{2}(G) = \\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)} = \\frac{1+3\\mu}{1-\\mu}\n$$\n\nNow, we must justify what this computed $\\kappa_{2}(G)$ implies about the support-restricted least-squares debiasing problem. The debiasing step involves solving the standard least-squares problem restricted to the support $S$:\n$$\n\\min_{\\mathbf{x}_S} \\|\\mathbf{y} - A_S \\mathbf{x}_S\\|_2^2\n$$\nwhere $A_S$ is the matrix of columns of $A$ indexed by $S$. The solution is given by the normal equations:\n$$\n(A_S^{\\top}A_S)\\mathbf{x}_S = A_S^{\\top}\\mathbf{y} \\implies G\\mathbf{x}_S = A_S^{\\top}\\mathbf{y}\n$$\nThe properties of this linear system are governed by the condition number $\\kappa_{2}(G)$.\n\nThe orthonormal case corresponds to the columns of $A_S$ being orthonormal, meaning $G = A_S^{\\top}A_S = I_4$. This is equivalent to setting $\\mu=0$ in our model. In this case, $\\lambda_{\\max}(G) = 1$ and $\\lambda_{\\min}(G)=1$, so $\\kappa_{2}(G) = \\frac{1+0}{1-0} = 1$. A condition number of $1$ is the optimal, lowest possible value, signifying a perfectly well-conditioned problem.\n\nAs the coherence $\\mu$ increases from $0$ towards $1$, the condition number $\\kappa_{2}(G) = \\frac{1+3\\mu}{1-\\mu}$ increases monotonically. As $\\mu \\to 1$, $\\kappa_{2}(G) \\to \\infty$. A large condition number indicates that the matrix $G$ is ill-conditioned, meaning it is close to being singular. This has two critical implications compared to the orthonormal case:\n\n1.  **Numerical Accuracy:** When solving the system $G\\mathbf{x}_S = A_S^{\\top}\\mathbf{y}$ using finite-precision arithmetic (e.g., on a computer), the relative error in the computed solution is bounded by $\\kappa_2(G)$. A larger $\\kappa_{2}(G)$ means that small floating-point representation errors get amplified, leading to a numerically computed solution that can be significantly different from the true mathematical solution. As $\\mu$ increases, the debiasing step becomes numerically unstable and the results less reliable.\n\n2.  **Noise Sensitivity:** In a model with additive white Gaussian noise (AWGN), $\\mathbf{y} = A_S\\mathbf{x}_{S,\\text{true}} + \\mathbf{n}$, the least-squares estimate is $\\mathbf{x}_S = \\mathbf{x}_{S,\\text{true}} + G^{-1}A_S^{\\top}\\mathbf{n}$. The error term, $\\mathbf{e} = G^{-1}A_S^{\\top}\\mathbf{n}$, is the result of propagating the measurement noise $\\mathbf{n}$ through the system. The amplification of this noise is determined by the matrix $G^{-1}$. The eigenvalues of $G^{-1}$ are the reciprocals of the eigenvalues of $G$, specifically $1/\\lambda_{\\min}(G) = 1/(1-\\mu)$ and $1/\\lambda_{\\max}(G) = 1/(1+3\\mu)$. A large $\\kappa_2(G)$ implies that $\\lambda_{\\min}(G)$ is very small. Consequently, the largest eigenvalue of $G^{-1}$ is very large. This means that noise components aligned with the eigenvector corresponding to $\\lambda_{\\min}(G)$ are drastically amplified. Therefore, a high coherence $\\mu$ leads to a high condition number, which in turn causes extreme sensitivity to measurement noise, rendering the debiased estimate $\\mathbf{x}_S$ highly inaccurate and unstable compared to the robust, noise-resilient estimate obtained in the orthonormal ($\\mu=0$) case.\n\nIn summary, the value of $\\kappa_{2}(G)$ serves as a precise measure of the degradation in both numerical stability and statistical efficiency (noise amplification) of the least-squares debiasing procedure as the columns of the sensing matrix on the selected support become more correlated.",
            "answer": "$$\n\\boxed{\\frac{1+3\\mu}{1-\\mu}}\n$$"
        },
        {
            "introduction": "While mutual coherence is a widely used heuristic for evaluating sensing matrices, it provides an incomplete picture by only considering pairwise column correlations. This exercise demonstrates the limitations of such simple proxies and highlights the superior descriptive power of a full spectral analysis. By comparing two Gram matrices with identical mutual coherence but different sign patterns, you will see firsthand how the collective interplay of columns, captured by the matrix's minimal eigenvalue, is the true determinant of stability . This sharpens the intuition that the properties of a positive semidefinite matrix depend on its global structure, not just its largest off-diagonal entries.",
            "id": "3445795",
            "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{3 \\times 3}$ with unit-norm columns, and let the mutual coherence be defined by $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product and $a_i$ is the $i$-th column of $A$. For a support set $S \\subseteq \\{1,2,3\\}$, the Gram matrix of the restricted sensing matrix is $G = A_S^{\\top} A_S$, which is symmetric and positive semidefinite by construction. Starting from these definitions, analyze how the sign pattern of the off-diagonal entries in $G$ affects the smallest eigenvalue of $G$.\n\nConstruct two sensing matrices $A^{(1)}$ and $A^{(2)}$ in $\\mathbb{R}^{3 \\times 3}$ with identical mutual coherence $\\mu \\in (0, \\tfrac{1}{2})$, unit-norm columns, and support $S = \\{1,2,3\\}$, such that:\n- For $A^{(1)}$, the Gram matrix $G^{(1)} = A^{(1)}_S{}^{\\top} A^{(1)}_S$ has entries $G^{(1)}_{ii} = 1$ and $G^{(1)}_{ij} = \\mu$ for $i \\neq j$.\n- For $A^{(2)}$, the Gram matrix $G^{(2)} = A^{(2)}_S{}^{\\top} A^{(2)}_S$ has entries $G^{(2)}_{ii} = 1$, $G^{(2)}_{12} = G^{(2)}_{21} = \\mu$, $G^{(2)}_{13} = G^{(2)}_{31} = \\mu$, and $G^{(2)}_{23} = G^{(2)}_{32} = -\\mu$.\n\nYou may use the fundamental fact that any symmetric positive semidefinite matrix is a Gram matrix of some set of vectors in a Euclidean space. Both $A^{(1)}$ and $A^{(2)}$ are required to have the same mutual coherence $\\mu$, and both $G^{(1)}$ and $G^{(2)}$ must be positive semidefinite.\n\nCompute the exact closed-form expression, as a function of $\\mu$, for the ratio\n$$\nr(\\mu) = \\frac{\\lambda_{\\min}\\!\\left(A^{(1)}_S{}^{\\top} A^{(1)}_S\\right)}{\\lambda_{\\min}\\!\\left(A^{(2)}_S{}^{\\top} A^{(2)}_S\\right)}.\n$$\nProvide your final answer as a single simplified analytic expression in $\\mu$. No rounding is required. The answer must not include any units.",
            "solution": "The problem asks for the ratio of the smallest eigenvalues of two specified Gram matrices, $G^{(1)}$ and $G^{(2)}$, which are derived from sensing matrices $A^{(1)}$ and $A^{(2)}$ with certain properties. The support set is given as $S = \\{1,2,3\\}$, which for a $3 \\times 3$ matrix means the restricted matrix $A_S$ is the full matrix $A$. The Gram matrices are thus $G = A^{\\top}A$.\n\nBased on the problem description, the two Gram matrices are defined as:\n$$\nG^{(1)} = \\begin{pmatrix} 1 & \\mu & \\mu \\\\ \\mu & 1 & \\mu \\\\ \\mu & \\mu & 1 \\end{pmatrix}\n$$\nand\n$$\nG^{(2)} = \\begin{pmatrix} 1 & \\mu & \\mu \\\\ \\mu & 1 & -\\mu \\\\ \\mu & -\\mu & 1 \\end{pmatrix}\n$$\nwhere the mutual coherence $\\mu$ is in the range $\\mu \\in (0, \\frac{1}{2})$. Our goal is to compute $r(\\mu) = \\frac{\\lambda_{\\min}(G^{(1)})}{\\lambda_{\\min}(G^{(2)})}$. We proceed by finding the eigenvalues for each matrix.\n\nFirst, we analyze $G^{(1)}$. This matrix has a special structure and can be written as a linear combination of the identity matrix $I$ and the all-ones matrix $J$:\n$$\nG^{(1)} = (1-\\mu)I + \\mu J\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix and $J$ is the $3 \\times 3$ matrix with all entries equal to $1$. The eigenvalues of an $n \\times n$ matrix of the form $aI + bJ$ are $a+nb$ (with multiplicity $1$) and $a$ (with multiplicity $n-1$). For our case, $n=3$, $a=1-\\mu$, and $b=\\mu$.\nThus, the eigenvalues of $G^{(1)}$ are:\n$\\lambda_1 = (1-\\mu) + 3\\mu = 1+2\\mu$ (multiplicity $1$)\n$\\lambda_2, \\lambda_3 = 1-\\mu$ (multiplicity $2$)\n\nTo find the smallest eigenvalue, $\\lambda_{\\min}(G^{(1)})$, we compare $1+2\\mu$ and $1-\\mu$. Given that $\\mu \\in (0, \\frac{1}{2})$, $\\mu$ is positive. The difference is $(1+2\\mu) - (1-\\mu) = 3\\mu$. Since $\\mu > 0$, this difference is positive, which implies $1+2\\mu > 1-\\mu$. Therefore, the smallest eigenvalue of $G^{(1)}$ is:\n$$\n\\lambda_{\\min}(G^{(1)}) = 1-\\mu\n$$\nThe problem statement asserts that $G^{(1)}$ must be positive semidefinite, which requires all eigenvalues to be non-negative. Since $\\mu  \\frac{1}{2}$, we have $1-\\mu > \\frac{1}{2} > 0$, and $1+2\\mu > 1 > 0$, so this condition is satisfied.\n\nNext, we analyze $G^{(2)}$. To find its eigenvalues, we solve the characteristic equation $\\det(G^{(2)} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda  \\mu  \\mu \\\\ \\mu  1-\\lambda  -\\mu \\\\ \\mu  -\\mu  1-\\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant, we get:\n$$\n(1-\\lambda)((1-\\lambda)^2 - (-\\mu)^2) - \\mu(\\mu(1-\\lambda) - \\mu(-\\mu)) + \\mu(\\mu(-\\mu) - \\mu(1-\\lambda)) = 0\n$$\n$$\n(1-\\lambda)((1-\\lambda)^2 - \\mu^2) - \\mu(\\mu(1-\\lambda) + \\mu^2) + \\mu(-\\mu^2 - \\mu(1-\\lambda)) = 0\n$$\n$$\n(1-\\lambda)^3 - \\mu^2(1-\\lambda) - \\mu^2(1-\\lambda) - \\mu^3 - \\mu^3 - \\mu^2(1-\\lambda) = 0\n$$\n$$\n(1-\\lambda)^3 - 3\\mu^2(1-\\lambda) - 2\\mu^3 = 0\n$$\nLet's make a substitution $x = 1-\\lambda$. The equation becomes a depressed cubic in $x$:\n$$\nx^3 - 3\\mu^2 x - 2\\mu^3 = 0\n$$\nWe can find the roots of this polynomial by factorization. We test for simple roots related to $\\mu$.\nFor $x=2\\mu$: $(2\\mu)^3 - 3\\mu^2(2\\mu) - 2\\mu^3 = 8\\mu^3 - 6\\mu^3 - 2\\mu^3 = 0$. So, $x=2\\mu$ is a root.\nFor $x=-\\mu$: $(-\\mu)^3 - 3\\mu^2(-\\mu) - 2\\mu^3 = -\\mu^3 + 3\\mu^3 - 2\\mu^3 = 0$. So, $x=-\\mu$ is a root.\nSince the sum of the roots of $x^3+px^2+qx+r=0$ is $-p$, and the coefficient of $x^2$ in our equation is $0$, the sum of the three roots must be $0$. If we have roots $2\\mu$ and $-\\mu$, the third root must be $-(2\\mu - \\mu) = -\\mu$.\nSo, the roots for $x$ are $2\\mu$, $-\\mu$, and $-\\mu$.\nThe eigenvalues $\\lambda$ are found from $\\lambda = 1-x$. Thus, the eigenvalues of $G^{(2)}$ are:\n$\\lambda_1 = 1 - 2\\mu$ (multiplicity $1$)\n$\\lambda_2, \\lambda_3 = 1 - (-\\mu) = 1+\\mu$ (multiplicity $2$)\n\nTo find the smallest eigenvalue, $\\lambda_{\\min}(G^{(2)})$, we compare $1-2\\mu$ and $1+\\mu$. The difference is $(1+\\mu) - (1-2\\mu) = 3\\mu$. Since $\\mu > 0$, this difference is positive, which implies $1+\\mu > 1-2\\mu$. Therefore, the smallest eigenvalue of $G^{(2)}$ is:\n$$\n\\lambda_{\\min}(G^{(2)}) = 1-2\\mu\n$$\n$G^{(2)}$ must also be positive semidefinite. For $\\mu \\in (0, \\frac{1}{2})$, we have $1-2\\mu > 0$ and $1+\\mu > 1 > 0$, so the eigenvalues are indeed positive, confirming the matrix is positive definite.\n\nFinally, we compute the required ratio $r(\\mu)$:\n$$\nr(\\mu) = \\frac{\\lambda_{\\min}(G^{(1)})}{\\lambda_{\\min}(G^{(2)})} = \\frac{1-\\mu}{1-2\\mu}\n$$\nThis expression is simplified and is the final answer.",
            "answer": "$$\\boxed{\\frac{1-\\mu}{1-2\\mu}}$$"
        },
        {
            "introduction": "The previous exercises demonstrated the theoretical importance of the Gram matrix's minimal eigenvalue, $\\lambda_{\\min}$. This practice transitions from analytical derivation to numerical computation, challenging you to implement an iterative algorithm to approximate this critical quantity from first principles. You will use Rayleigh Quotient Iteration to find $\\lambda_{\\min}$ and then apply this result to assess concrete conditions for sparse recovery, such as solution uniqueness and noise stability . This exercise bridges the gap between abstract spectral theory and the practical diagnostics used to validate and understand the performance of sensing matrices in real-world scenarios.",
            "id": "3445866",
            "problem": "You are given a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and a support set $S \\subseteq \\{0,1,\\dots,n-1\\}$ selecting a subset of columns. For the submatrix $A_S \\in \\mathbb{R}^{m \\times |S|}$ formed by the columns of $A$ indexed by $S$, consider the Gram matrix $G_S = A_S^{\\top} A_S$, which is symmetric and positive semidefinite. The smallest eigenvalue $\\lambda_{\\min}(G_S)$ equals the square of the smallest singular value of $A_S$. In compressed sensing and sparse optimization, properties of $\\lambda_{\\min}(G_S)$ are crucial for uniqueness and stability of recovery when the support is known or hypothesized.\n\nStarting from the fundamental base:\n- The Rayleigh quotient is defined for any nonzero vector $x \\in \\mathbb{R}^{|S|}$ by $R(x) = \\dfrac{x^{\\top} G_S x}{x^{\\top} x}$.\n- For a symmetric matrix $G_S$, the minimum of $R(x)$ over $x \\neq 0$ is exactly $\\lambda_{\\min}(G_S)$, achieved at an eigenvector of $G_S$ associated with $\\lambda_{\\min}(G_S)$.\n- A symmetric positive semidefinite matrix has all eigenvalues nonnegative, and $G_S = A_S^{\\top} A_S$ is positive semidefinite by construction.\n\nTask: Implement a numerical approximation of $\\lambda_{\\min}(G_S)$ using an iterative method based on the Rayleigh quotient. Use an inverse iteration with Rayleigh quotient updates to approximate the minimal eigenvalue and eigenvector, and include safeguards suitable for positive semidefinite matrices that may be singular or nearly singular. Then relate the approximation to recovery conditions and compute the following quantities:\n1. The numerical approximation of $\\lambda_{\\min}(G_S)$ obtained by your Rayleigh quotient iterations.\n2. The exact $\\lambda_{\\min}(G_S)$ computed via a direct symmetric eigendecomposition to serve as ground truth.\n3. A boolean indicating whether recovery on the support $S$ is unique in the noiseless case, defined as $A_S$ having full column rank. Use the criterion $\\lambda_{\\min}(G_S)  \\tau$, with tolerance $\\tau = 10^{-8}$.\n4. A stability factor $c_S$ defined as $c_S = 1 / \\sigma_{\\min}(A_S) = 1 / \\sqrt{\\lambda_{\\min}(G_S)}$, where a larger $c_S$ indicates greater noise amplification in least squares recovery constrained to $S$.\n5. The subset restricted isometry deviation $\\delta_S$ defined by $\\delta_S = \\max_{i} \\left| \\lambda_i(G_S) - 1 \\right|$ when all columns of $A$ are normalized to unit $\\ell_2$ norm. This quantifies how far $A_S$ deviates from being an isometry on the subspace spanned by its columns.\n6. A boolean indicating whether the classical restricted isometry-like check passes on the subset, defined as $\\delta_S  \\sqrt{2} - 1$.\n\nConstruct and evaluate the following test suite. In all cases, normalize every column of $A$ to unit $\\ell_2$ norm before forming $A_S$ and $G_S$.\n\n- Test case $1$ (general, random, well-conditioned):\n  - Dimensions: $m = 8$, $n = 10$.\n  - Random seed: $0$.\n  - Construction: draw $A$ with independent standard normal entries and then normalize each column to unit $\\ell_2$ norm.\n  - Support: $S = [0, 3, 7]$.\n\n- Test case $2$ (near-collinearity causing near-singularity):\n  - Dimensions: $m = 6$, $n = 8$.\n  - Random seed: $1$.\n  - Construction:\n    - Generate a random base matrix with independent standard normal entries.\n    - Form a unit vector $u \\in \\mathbb{R}^{m}$ by normalizing a random column.\n    - Generate a random vector $v \\in \\mathbb{R}^{m}$, orthogonalize it against $u$ by $v \\leftarrow v - (u^{\\top} v) u$, then normalize $v$.\n    - Set column $0$ of $A$ to $u$, set column $1$ of $A$ to the normalization of $u + \\varepsilon v$ with $\\varepsilon = 10^{-3}$, set column $2$ of $A$ to the normalization of a random vector, and leave other columns as random. Finally normalize all columns of $A$ to unit $\\ell_2$ norm.\n  - Support: $S = [0, 1, 2]$.\n\n- Test case $3$ (single-column subset):\n  - Dimensions: $m = 5$, $n = 7$.\n  - Random seed: $123$.\n  - Construction: draw $A$ with independent standard normal entries and then normalize each column to unit $\\ell_2$ norm.\n  - Support: $S = [5]$.\n\n- Test case $4$ (ill-conditioned but full rank):\n  - Dimensions: $m = 6$, $n = 8$.\n  - Random seed: $42$.\n  - Construction:\n    - Generate a random base matrix with independent standard normal entries.\n    - Normalize all columns of $A$.\n    - Modify column $7$ to be the normalization of $A[:, 1] + \\eta r$, where $r$ is a fresh random vector and $\\eta = 10^{-2}$, then renormalize all columns.\n  - Support: $S = [1, 4, 6, 7]$.\n\nYour program should output, for each test case, a list containing:\n- the approximated $\\lambda_{\\min}(G_S)$ as a float,\n- the exact $\\lambda_{\\min}(G_S)$ as a float,\n- the uniqueness boolean,\n- the stability factor $c_S$ as a float,\n- the deviation $\\delta_S$ as a float,\n- the restricted isometry check boolean.\n\nYour program should produce a single line of output containing the list of results for all test cases as a comma-separated list enclosed in square brackets, for example, $$[ [r_1], [r_2], [r_3], [r_4] ]$$ where each $[r_i]$ is itself a list of the six items described above.",
            "solution": "The user wants me to solve a problem related to compressed sensing and sparse optimization.\n\nHere's a breakdown of the task:\n\n1.  **Role**: I need to act as a meticulous STEM professor, emphasizing correctness and rigor.\n2.  **Core Task**: Validate the problem statement first. If valid, solve it. If invalid, explain why.\n3.  **Problem Domain**: The problem is about analyzing the Gram matrix $G_S = A_S^\\top A_S$ derived from a submatrix $A_S$ of a sensing matrix $A$. The analysis focuses on the smallest eigenvalue $\\lambda_{\\min}(G_S)$ and related properties like uniqueness of recovery, stability, and restricted isometry.\n4.  **Methodology**: I need to implement an iterative method to approximate $\\lambda_{\\min}(G_S)$. The suggested method is *inverse iteration with Rayleigh quotient updates*. This is a standard numerical linear algebra technique. It's essentially the power method applied to the inverse of a matrix.\n5.  **Safeguards**: The problem mentions safeguards for singular or nearly singular matrices. This is important for inverse iteration, as it involves solving a system of linear equations with a matrix that could be ill-conditioned. A common safeguard is to use a \"shifted\" inverse iteration, solving $(G_S - \\sigma I)x_{k+1} = x_k$ where $\\sigma$ is an estimate of the eigenvalue. Here, since we are looking for the smallest eigenvalue and the matrix is positive semidefinite, the shift would be close to 0. A simple approach is to solve $G_S x_{k+1} = x_k$, but if $G_S$ is singular, this is problematic. A better approach is to solve $(G_S - \\sigma I)y_{k+1} = x_k$, where $\\sigma$ is our current best guess for $\\lambda_{min}$. This is exactly the Rayleigh quotient iteration. The system to solve is $(G_S - R(x_k)I)y = x_k$. However, this is even more likely to be singular.\n    A more robust variant for finding the smallest eigenvalue of a positive semidefinite matrix is just inverse power iteration without a shift, or with a very small positive shift if singularity is a concern. The problem states to use inverse iteration with Rayleigh quotient updates. Let's formalize this.\n    - Start with a random vector $x_0$.\n    - For $k=0, 1, 2, \\dots$:\n        1.  Compute the Rayleigh quotient: $\\mu_k = \\frac{x_k^\\top G_S x_k}{x_k^\\top x_k}$.\n        2.  Solve $(G_S - \\mu_k I) y_{k+1} = x_k$ for $y_{k+1}$.\n        3.  Normalize: $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|_2$.\n    The sequence $\\mu_k$ converges cubically to an eigenvalue. To find the *smallest* eigenvalue, we need a good starting vector $x_0$ whose Rayleigh quotient is closer to $\\lambda_{\\min}$ than any other eigenvalue. A random vector is not guaranteed to do this.\n    \n    A simpler and more common method is the standard inverse power method.\n    - Start with a random vector $x_0$.\n    - For $k=0, 1, 2, \\dots$:\n        1.  Solve $G_S y_{k+1} = x_k$ for $y_{k+1}$.\n        2.  Normalize: $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|_2$.\n    The Rayleigh quotient $R(x_k)$ will converge to $1 / \\lambda_{\\max}(G_S^{-1}) = \\lambda_{\\min}(G_S)$.\n    This method has a problem if $G_S$ is singular (i.e., $\\lambda_{\\min}(G_S) = 0$). The system $G_S y_{k+1} = x_k$ may not have a solution.\n    A safeguard is to use a shifted inverse method:\n    - Choose a shift $\\sigma$ that is *smaller* than the desired eigenvalue. For $\\lambda_{\\min}(G_S) \\ge 0$, any $\\sigma  0$ would work, but this doesn't target $\\lambda_{\\min}$ specifically.\n    - A better approach is to use a small positive shift, effectively regularizing the matrix: solve $(G_S + \\alpha I) y_{k+1} = x_k$, where $\\alpha  0$ is small. This finds the eigenvalue of $G_S$ closest to $-\\alpha$, which is $\\lambda_{\\min}(G_S)$. The eigenvalue of $(G_S + \\alpha I)^{-1}$ we find is $1/(\\lambda_{min}(G_S) + \\alpha)$. So we can recover $\\lambda_{min}(G_S)$.\n    \n    The problem statement specifically says \"inverse iteration with Rayleigh quotient updates\". This is a specific algorithm, also known as Rayleigh Quotient Iteration (RQI). I must implement that. The issue of singularity of $(G_S - \\mu_k I)$ is real. When $\\mu_k$ gets close to an eigenvalue, the matrix becomes nearly singular. This is paradoxically what makes the method fast (cubic convergence), but also numerically tricky. The solution $y_{k+1}$ will be very large and point in the direction of the eigenvector. We can use `numpy.linalg.solve` which should handle nearly singular systems reasonably well, perhaps throwing a `LinAlgError` if it's too bad. We can catch this error. If it happens, it means our estimate $\\mu_k$ is extremely close to the true eigenvalue, so we can probably stop and use $\\mu_k$ as the result.\n    \n6.  **Quantities to Compute**: For each test case, I need to calculate a list of 6 items:\n    1.  `approx_lambda_min`: From my RQI implementation.\n    2.  `exact_lambda_min`: From a direct eigendecomposition (`numpy.linalg.eigh`).\n    3.  `is_unique`: `exact_lambda_min  1e-8`.\n    4.  `stability_factor_c_S`: $1 / \\sqrt{\\lambda_{\\min}(G_S)}$. Handle $\\lambda_{\\min} \\le 0$ (due to numerical error) or $\\lambda_{\\min}=0$. If $\\lambda_{\\min} \\le 10^{-8}$, it's effectively zero. In this case, the stability factor is infinite. I will return `np.inf`.\n    5.  `rip_deviation_delta_S`: $\\max_{i} |\\lambda_i(G_S) - 1|$. This requires computing *all* eigenvalues of $G_S$.\n    6.  `rip_check`: `rip_deviation_delta_S  sqrt(2) - 1`.\n\n7.  **Test Cases**: Four specific test cases are provided, with details on matrix generation, dimensions, random seeds, and support sets. An important instruction is to normalize every column of $A$ to unit $\\ell_2$ norm *before* forming $A_S$.\n\nNow, let's proceed with the formal validation process.\n\n---\n\n### Step 1: Extract Givens\n\n-   **Sensing Matrix**: $A \\in \\mathbb{R}^{m \\times n}$.\n-   **Support Set**: $S \\subseteq \\{0,1,\\dots,n-1\\}$.\n-   **Submatrix**: $A_S \\in \\mathbb{R}^{m \\times |S|}$ (columns of $A$ indexed by $S$).\n-   **Gram Matrix**: $G_S = A_S^{\\top} A_S$. It is symmetric and positive semidefinite.\n-   **Smallest Eigenvalue**: $\\lambda_{\\min}(G_S)$.\n-   **Rayleigh Quotient**: $R(x) = \\dfrac{x^{\\top} G_S x}{x^{\\top} x}$ for $x \\in \\mathbb{R}^{|S|}, x \\neq 0$.\n-   **Minimization Property**: $\\min_{x \\neq 0} R(x) = \\lambda_{\\min}(G_S)$.\n-   **Calculation Task**: Implement an approximation of $\\lambda_{\\min}(G_S)$ using inverse iteration with Rayleigh quotient updates.\n-   **Tolerance for Uniqueness**: $\\tau = 10^{-8}$.\n-   **Uniqueness Condition**: $\\lambda_{\\min}(G_S)  \\tau$.\n-   **Stability Factor**: $c_S = 1 / \\sigma_{\\min}(A_S) = 1 / \\sqrt{\\lambda_{\\min}(G_S)}$.\n-   **Restricted Isometry Deviation**: $\\delta_S = \\max_{i} \\left| \\lambda_i(G_S) - 1 \\right|$, where all columns of $A$ are normalized to unit $\\ell_2$ norm.\n-   **Restricted Isometry Check**: $\\delta_S  \\sqrt{2} - 1$.\n-   **Normalization Pre-step**: Before forming $A_S$ and $G_S$, normalize every column of $A$ to unit $\\ell_2$ norm.\n\n-   **Test Case 1**:\n    -   $m = 8$, $n = 10$.\n    -   Random seed: $0$.\n    -   Construction: $A$ has i.i.d. standard normal entries, then columns are normalized.\n    -   Support: $S = [0, 3, 7]$.\n\n-   **Test Case 2**:\n    -   $m = 6$, $n = 8$.\n    -   Random seed: $1$.\n    -   Construction:\n        -   Generate a random base matrix with i.i.d. standard normal entries.\n        -   Form unit vector $u$ from a random column.\n        -   Generate random vector $v$, orthogonalize against $u$, normalize $v$.\n        -   Column 0 of $A$ is $u$.\n        -   Column 1 of $A$ is normalization of $u + \\varepsilon v$ with $\\varepsilon = 10^{-3}$.\n        -   Column 2 of $A$ is normalization of a random vector.\n        -   Other columns are random.\n        -   Finally normalize all columns of $A$.\n    -   Support: $S = [0, 1, 2]$.\n\n-   **Test Case 3**:\n    -   $m = 5$, $n = 7$.\n    -   Random seed: $123$.\n    -   Construction: $A$ has i.i.d. standard normal entries, then columns are normalized.\n    -   Support: $S = [5]$.\n\n-   **Test Case 4**:\n    -   $m = 6$, $n = 8$.\n    -   Random seed: $42$.\n    -   Construction:\n        -   Generate a random base matrix with independent standard normal entries.\n        -   Normalize all columns of $A$.\n        -   Modify column 7 to be normalization of $A[:, 1] + \\eta r$, where $r$ is a fresh random vector and $\\eta = 10^{-2}$.\n        -   Renormalize all columns.\n    -   Support: $S = [1, 4, 6, 7]$.\n\n-   **Output Format**: A single line `[ [r_1], [r_2], [r_3], [r_4] ]` where each `[r_i]` is a list of six computed values for the corresponding test case.\n\n---\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded**: The problem is rooted in numerical linear algebra and compressed sensing theory. All concepts—eigenvalues, eigenvectors, Rayleigh quotient, Gram matrix, restricted isometry property (RIP), positive semidefiniteness, inverse iteration—are standard and well-established in these fields. The relationships described, such as $\\lambda_{\\min}(A_S^\\top A_S) = \\sigma_{\\min}(A_S)^2$, are fundamental facts. The problem is scientifically sound.\n\n2.  **Well-Posed**: The problem asks for the implementation of a specific numerical algorithm and the computation of several well-defined quantities. For each test case, the matrix construction is specified algorithmically, including random seeds, ensuring reproducibility. The tasks are clearly defined. A unique, stable, and meaningful solution (in the sense of a correct program producing the correct numerical results) exists.\n\n3.  **Objective**: The problem is stated in precise mathematical and algorithmic terms. It is free from subjective language or opinion.\n\nUpon reviewing the invalidity flaws checklist:\n1.  **Scientific or Factual Unsoundness**: No violations are found. The problem statements and definitions align with established theory.\n2.  **Non-Formalizable or Irrelevant**: The problem is formalizable and directly relevant to the specified topic.\n3.  **Incomplete or Contradictory Setup**: The problem specification is comprehensive. While some construction descriptions require careful reading (e.g., test cases 2 and 4), they are procedural and not contradictory. For instance, the instruction to \"renormalize all columns\" after modifying one is interpreted as a distinct, sequential step applied to the entire matrix, a specific instruction rather than a contradiction.\n4.  **Unrealistic or Infeasible**: The problem is computationally feasible, with small matrix dimensions.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-structured, with a clear goal and path to solution.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires non-trivial numerical implementation and understanding of key concepts, including edge cases like a single-column support set, which serves as a valid and useful test.\n7.  **Outside Scientific Verifiability**: The results are numerically reproducible and verifiable given the specified random seeds.\n\nThe problem is deemed valid.\n\n---\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design\nThe solution is structured around the principles of numerical linear algebra and their application to compressed sensing. The core of the problem is to analyze the Gram matrix $G_S = A_S^\\top A_S$ associated with a submatrix $A_S$. The smallest eigenvalue, $\\lambda_{\\min}(G_S)$, is of paramount importance as it governs the conditioning of the sparse recovery problem on the known support set $S$.\n\n**Approximation of $\\lambda_{\\min}(G_S)$:**\nThe problem mandates the use of \"inverse iteration with Rayleigh quotient updates,\" a powerful algorithm more commonly known as Rayleigh Quotient Iteration (RQI). For a symmetric matrix $G_S$ and an approximate eigenvector $x_k$, RQI consists of two steps:\n$1$. Compute the Rayleigh quotient: $\\mu_k = \\frac{x_k^\\top G_S x_k}{x_k^\\top x_k}$. This is the best eigenvalue estimate for the given $x_k$.\n$2$. Update the eigenvector estimate by solving $(G_S - \\mu_k I) y_{k+1} = x_k$ and normalizing: $x_{k+1} = y_{k+1} / \\|y_{k+1}\\|_2$. This step is a form of inverse iteration, where the shift $\\mu_k$ is dynamically updated.\n\nRQI exhibits cubic convergence, but its success in finding a specific eigenvalue (the smallest one, in this case) depends on the initial vector $x_0$. A naive random start may lead to convergence to an arbitrary eigenvalue. To reliably target $\\lambda_{\\min}(G_S)$, a more robust initialization is required. We employ a pre-processing step using simple inverse power iteration on a slightly shifted matrix, $(G_S + \\epsilon I)$, where $\\epsilon$ is a small positive number. Applying a few iterations of solving $(G_S + \\epsilon I)y = x$ will guide the vector $x$ toward the eigenvector corresponding to the eigenvalue of $G_S$ closest to $-\\epsilon$, which is precisely $\\lambda_{\\min}(G_S)$. This refined vector then serves as a high-quality starting point for RQI, ensuring convergence to the desired eigenvalue.\n\nA critical aspect of RQI is handling the linear system solve. As $\\mu_k$ approaches a true eigenvalue, the matrix $(G_S - \\mu_k I)$ becomes ill-conditioned or singular. This is a feature, not a bug, of the algorithm; the large magnitude of the solution vector $y_{k+1}$ is what accelerates convergence. Our implementation will trap the `numpy.linalg.LinAlgError` that arises from this singularity, correctly interpreting it as a sign of convergence and terminating the iteration.\n\n**Computation of Related Metrics:**\nOnce $\\lambda_{\\min}(G_S)$ is determined, we compute several related quantities central to compressed sensing theory:\n\n$1$. **Ground Truth Eigenvalue:** We use a standard, highly-optimized symmetric eigensolver (`numpy.linalg.eigh`) to compute all eigenvalues of $G_S$ for an exact reference value of $\\lambda_{\\min}(G_S)$ and for calculating $\\delta_S$.\n$2$. **Uniqueness of Recovery:** For a noiseless measurement $y=Ax$, if the true sparse vector $x$ is supported on $S$, the unique recoverability of $x$ (given knowledge of $S$) is equivalent to being able to uniquely solve $y = A_S x_S$. This requires $A_S$ to have full column rank, which is true if and only if $G_S=A_S^\\top A_S$ is invertible, i.e., $\\lambda_{\\min}(G_S)  0$. We test this condition against a numerical tolerance $\\tau = 10^{-8}$.\n$3$. **Stability Factor ($c_S$):** In the presence of noise, the stability of the least-squares solution depends on the conditioning of $A_S$. A key measure is $c_S = 1/\\sigma_{\\min}(A_S)$, where $\\sigma_{\\min}(A_S)$ is the smallest singular value of $A_S$. Using the identity $\\lambda_{\\min}(G_S) = \\sigma_{\\min}(A_S)^2$, we have $c_S = 1/\\sqrt{\\lambda_{\\min}(G_S)}$. A large $c_S$ indicates high sensitivity to noise. If $\\lambda_{\\min}(G_S)=0$, the factor is infinite.\n$4$. **Subset Restricted Isometry Deviation ($\\delta_S$):** The Restricted Isometry Property (RIP) is a cornerstone of compressed sensing theory. While computing the true RIP constant is NP-hard, we can check a related property for a specific subset $S$. If all columns of $A$ are unit-normalized, the operator $A_S^\\top A_S$ acts as an identity on its domain if $A_S$ is an isometry. The deviation $\\delta_S = \\max_i |\\lambda_i(G_S) - 1|$ quantifies how much $G_S$ differs from the identity matrix $I$.\n$5$. **Classical RIP-like Check:** A sufficient (but not necessary) condition for many recovery guarantees is that $\\delta_{2k}  \\sqrt{2}-1$ for $2k$-sparse signals. We apply this check to our specific subset $S$, evaluating if $\\delta_S  \\sqrt{2}-1$.\n\nThe entire process is automated for a suite of test cases, each designed to probe a different aspect of the problem, from a well-conditioned random matrix to deliberately constructed ill-conditioned cases. The use of fixed random seeds ensures the reproducibility of all constructions and results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve\n\ndef approximate_lambda_min(G, rng, num_init_iter=10, num_rqi_iter=15):\n    \"\"\"\n    Approximates the smallest eigenvalue of a symmetric positive semidefinite matrix G\n    using inverse iteration followed by Rayleigh Quotient Iteration (RQI).\n    \n    Args:\n        G (np.ndarray): The symmetric positive semidefinite matrix.\n        rng (np.random.Generator): A random number generator.\n        num_init_iter (int): Number of iterations for the initial inverse power method.\n        num_rqi_iter (int): Number of iterations for RQI.\n\n    Returns:\n        float: The approximated smallest eigenvalue.\n    \"\"\"\n    k = G.shape[0]\n    if k == 0:\n        return np.nan\n    if k == 1:\n        return G[0, 0]\n\n    # Initialize a random vector\n    x = rng.standard_normal(size=k)\n    x /= np.linalg.norm(x)\n\n    # 1. Initial phase: Shifted inverse iteration to find a good starting vector.\n    # This guides the vector towards the eigenvector of the smallest eigenvalue.\n    # The shift makes the system well-conditioned.\n    shift = 1e-12\n    G_shifted = G + shift * np.identity(k)\n    try:\n        # Pre-compute LU factorization for efficiency in the loop\n        lu, piv = lu_factor(G_shifted)\n        for _ in range(num_init_iter):\n            y = lu_solve((lu, piv), x)\n            norm_y = np.linalg.norm(y)\n            if norm_y  1e-20:\n                # Vector collapsed; restart with new random vector\n                x = rng.standard_normal(size=k)\n                x /= np.linalg.norm(x)\n                lu, piv = lu_factor(G + shift * np.identity(k)) # Recalculate LU\n                continue\n            x = y / norm_y\n    except np.linalg.LinAlgError:\n        # Fallback to pseudo-inverse if LU fails (extremely unlikely with the shift)\n        G_pinv = np.linalg.pinv(G_shifted)\n        for _ in range(num_init_iter):\n            y = G_pinv @ x\n            norm_y = np.linalg.norm(y)\n            if norm_y  1e-20:\n                x = rng.standard_normal(size=k)\n                x /= np.linalg.norm(x)\n                continue\n            x = y / norm_y\n\n    # 2. Refinement phase: Rayleigh Quotient Iteration\n    lambda_approx = np.dot(x, G @ x)\n\n    for _ in range(num_rqi_iter):\n        M = G - lambda_approx * np.identity(k)\n        try:\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # Expected condition: M is singular, so lambda_approx is close to an eigenvalue.\n            return lambda_approx\n        \n        norm_y = np.linalg.norm(y)\n        if norm_y  1e-20:\n            return lambda_approx\n        x = y / norm_y\n        lambda_approx = np.dot(x, G @ x)\n        \n    return lambda_approx\n\ndef run_test_case(m, n, seed, S, A_construction_func):\n    \"\"\"\n    Sets up and runs a single test case, computing all required metrics.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = A_construction_func(m, n, rng)\n\n    A_S = A[:, S]\n    G_S = A_S.T @ A_S\n\n    # 1. Approximate lambda_min using RQI\n    approx_lambda_min = approximate_lambda_min(G_S, rng)\n\n    # 2. Exact eigenvalues for ground truth and other metrics\n    if G_S.shape[0] == 0:\n        all_eigenvalues = np.array([])\n        exact_lambda_min = np.nan\n    else:\n\n        all_eigenvalues = np.linalg.eigh(G_S)[0]\n        exact_lambda_min = all_eigenvalues[0] if len(all_eigenvalues) > 0 else np.nan\n\n\n    # 3. Uniqueness boolean\n    tau = 1e-8\n    is_unique = exact_lambda_min > tau if not np.isnan(exact_lambda_min) else False\n\n    # 4. Stability factor\n    if not np.isnan(exact_lambda_min) and exact_lambda_min > 1e-15:\n        stability_factor_c_S = 1.0 / np.sqrt(exact_lambda_min)\n    else:\n        stability_factor_c_S = np.inf\n        \n    # 5. Restricted Isometry Deviation\n    if G_S.shape[0] > 0:\n        rip_deviation_delta_S = np.max(np.abs(all_eigenvalues - 1.0))\n    else:\n        rip_deviation_delta_S = 0.0\n\n    # 6. Restricted Isometry Check\n    rip_check = rip_deviation_delta_S  (np.sqrt(2.0) - 1.0)\n\n    return [\n        approx_lambda_min,\n        exact_lambda_min,\n        is_unique,\n        stability_factor_c_S,\n        rip_deviation_delta_S,\n        rip_check,\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and print results for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"m\": 8, \"n\": 10, \"seed\": 0, \"S\": [0, 3, 7],\n            \"construct\": lambda m, n, rng: (\n                lambda A: A / np.linalg.norm(A, axis=0)\n            )(rng.standard_normal(size=(m, n)))\n        },\n        {\n            \"m\": 6, \"n\": 8, \"seed\": 1, \"S\": [0, 1, 2],\n            \"construct\": lambda m, n, rng: (\n                lambda A: (\n                    A.__setitem__((slice(None), 0), (lambda u_rand: u_rand / np.linalg.norm(u_rand))(rng.standard_normal(size=m))),\n                    A.__setitem__((slice(None), 1), (lambda u, v, eps: (u + eps * v) / np.linalg.norm(u + eps * v))(\n                        A[:, 0], \n                        (lambda u, v_rand: (v_rand - np.dot(u, v_rand) * u) / np.linalg.norm(v_rand - np.dot(u, v_rand) * u))(A[:, 0], rng.standard_normal(size=m)),\n                        1e-3\n                    )),\n                    A.__setitem__((slice(None), 2), (lambda r: r / np.linalg.norm(r))(rng.standard_normal(size=m))),\n                    A / np.linalg.norm(A, axis=0)\n                )[-1]\n            )(rng.standard_normal(size=(m, n)))\n        },\n        {\n            \"m\": 5, \"n\": 7, \"seed\": 123, \"S\": [5],\n            \"construct\": lambda m, n, rng: (\n                lambda A: A / np.linalg.norm(A, axis=0)\n            )(rng.standard_normal(size=(m, n)))\n        },\n        {\n            \"m\": 6, \"n\": 8, \"seed\": 42, \"S\": [1, 4, 6, 7],\n            \"construct\": lambda m, n, rng: (\n                lambda A: (\n                    A.__setitem__((slice(None), 7), (lambda col: col / np.linalg.norm(col))(A[:, 1] + 1e-2 * rng.standard_normal(size=m))),\n                    A / np.linalg.norm(A, axis=0)\n                )[-1]\n            )((lambda B: B / np.linalg.norm(B, axis=0))(rng.standard_normal(size=(m, n))))\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case[\"m\"], case[\"n\"], case[\"seed\"], case[\"S\"], case[\"construct\"])\n        results.append(result)\n\n    # Format output as a string representing a list of lists.\n    # The template `f\"[{','.join(map(str, results))}]\"` handles this structure correctly.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}