{
    "hands_on_practices": [
        {
            "introduction": "Understanding convexity begins with geometry. This exercise provides a tangible example of a set that seems simple but violates the definition of convexity, a scenario that is fundamental to understanding why optimizing for true sparsity is so challenging . By computing the Jensen gap, you will quantitatively diagnose the non-convexity of the sparsity-counting \"norm,\" a key insight in sparse optimization.",
            "id": "3455578",
            "problem": "In sparse modeling and compressed sensing, the set of $k$-sparse vectors is a central but nonconvex model set. In the toy ambient space $\\mathbb{R}^2$, an analogue of the $k$-sparse model when $k=1$ is the union of the coordinate axes. Using only fundamental definitions of star-shapedness, convexity, and Jensen’s inequality, construct and analyze a concrete example that captures this geometry and its implications for nonconvex penalties.\n\nDefine the subset $C \\subset \\mathbb{R}^2$ by\n$$\nC \\triangleq \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_2=0,\\ |x_1|\\leq 1\\}\\ \\cup\\ \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1=0,\\ |x_2|\\leq 1\\}.\n$$\nThis set represents the union of the two coordinate axis segments of length $2$ centered at the origin, and is a $1$-sparse model proxy frequently used in sparse optimization thought experiments.\n\nTasks:\n- Using only the definitions of star-shapedness and convexity, verify that $C$ is star-shaped about the origin but not convex. In particular, explicitly choose $x \\in C$, $y \\in C$, and $t \\in (0,1)$ such that $t x + (1-t) y \\notin C$.\n- Let $\\|\\cdot\\|_0$ denote the zero “norm” that counts the number of nonzero entries of a vector. Recall that for a convex function $f$, Jensen’s inequality states $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$ for all $x,y$ and all $t \\in [0,1]$. For your explicit $x,y$ and with $t=\\tfrac{1}{2}$, compute the Jensen gap\n$$\n\\Delta \\triangleq \\|t x + (1-t) y\\|_0 - \\big(t \\|x\\|_0 + (1-t) \\|y\\|_0\\big),\n$$\nand explain how its sign diagnoses the failure of convexity of $\\|\\cdot\\|_0$ on $C$.\n\nProvide as your final answer the numerical value of $\\Delta$ for your explicit choice with $t=\\tfrac{1}{2}$. No rounding is required.",
            "solution": "The problem requires a three-part analysis of a set $C \\subset \\mathbb{R}^2$ and the zero \"norm\" $\\|\\cdot\\|_0$, based on fundamental definitions. The set is defined as the union of two line segments along the coordinate axes:\n$$\nC \\triangleq \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_2=0,\\ |x_1|\\leq 1\\}\\ \\cup\\ \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1=0,\\ |x_2|\\leq 1\\}\n$$\n\nFirst, we verify the geometric properties of $C$. We use the formal definitions of star-shapedness and convexity.\nA set $S \\subseteq \\mathbb{R}^n$ is **star-shaped about the origin** if for every point $x \\in S$ and every scalar $t \\in [0,1]$, the point $tx$ is also in $S$.\nA set $S \\subseteq \\mathbb{R}^n$ is **convex** if for every pair of points $x, y \\in S$ and every scalar $t \\in [0,1]$, the point representing their convex combination, $tx + (1-t)y$, is also in $S$.\n\nTo verify that $C$ is star-shaped about the origin, we take an arbitrary point $x \\in C$ and a scalar $t \\in [0,1]$. According to the definition of $C$, $x$ must be of the form $(x_1, 0)$ with $|x_1| \\leq 1$, or of the form $(0, x_2)$ with $|x_2| \\leq 1$.\nCase 1: $x = (x_1, 0)$ with $|x_1| \\leq 1$. The scaled point is $tx = (tx_1, 0)$. The second component is $0$. For the first component, we have $|tx_1| = |t||x_1| = t|x_1|$ since $t \\geq 0$. As $t \\in [0,1]$ and $|x_1| \\leq 1$, it follows that $t|x_1| \\leq 1 \\cdot 1 = 1$. Thus, $tx \\in C$.\nCase 2: $x = (0, x_2)$ with $|x_2| \\leq 1$. The scaled point is $tx = (0, tx_2)$. The first component is $0$. For the second component, we have $|tx_2| = t|x_2| \\leq 1$. Thus, $tx \\in C$.\nIn both possible cases, $tx$ is in $C$. Therefore, the set $C$ is star-shaped about the origin.\n\nTo show that $C$ is not convex, we must provide a counterexample: a pair of points $x, y \\in C$ and a scalar $t \\in (0,1)$ such that their convex combination $z \\triangleq t x + (1-t) y$ falls outside of $C$. Let us select one point from each axis segment. A canonical choice is $x \\triangleq (1,0)$ and $y \\triangleq (0,1)$. Both points are in $C$ since for $x$, we have $x_2=0$ and $|x_1|=1 \\leq 1$, and for $y$, we have $y_1=0$ and $|y_2|=1 \\leq 1$.\nLet us choose the scalar $t = \\frac{1}{2}$, which is in $(0,1)$. The convex combination is:\n$$\nz \\triangleq \\frac{1}{2}x + \\left(1-\\frac{1}{2}\\right)y = \\frac{1}{2}(1,0) + \\frac{1}{2}(0,1) = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)\n$$\nFor the point $z = (z_1, z_2) = (\\frac{1}{2}, \\frac{1}{2})$ to be in $C$, it would need to have either $z_1=0$ or $z_2=0$. However, both coordinates are non-zero. Therefore, $z \\notin C$. This counterexample proves that the set $C$ is not convex.\n\nSecond, we compute the Jensen gap $\\Delta$ for the zero \"norm\" $f(\\cdot) \\triangleq \\|\\cdot\\|_0$. This function counts the number of non-zero entries in a vector. The Jensen gap is defined as:\n$$\n\\Delta \\triangleq f(t x + (1-t) y) - \\big(t f(x) + (1-t) f(y)\\big)\n$$\nWe use the same points $x = (1,0)$, $y = (0,1)$, and scalar $t = \\frac{1}{2}$ from our non-convexity argument.\nWe first evaluate $\\|\\cdot\\|_0$ for $x$ and $y$:\n$\\|x\\|_0 = \\|(1,0)\\|_0 = 1$ (since only the first entry is non-zero).\n$\\|y\\|_0 = \\|(0,1)\\|_0 = 1$ (since only the second entry is non-zero).\nThe convex combination of the function values is:\n$$\nt \\|x\\|_0 + (1-t) \\|y\\|_0 = \\frac{1}{2}(1) + \\frac{1}{2}(1) = 1\n$$\nNext, we evaluate $\\|\\cdot\\|_0$ for the convex combination of the vectors, $z = (\\frac{1}{2}, \\frac{1}{2})$:\n$$\n\\|t x + (1-t) y\\|_0 = \\left\\|\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\right\\|_0 = 2\n$$\n(since both entries are non-zero).\nNow we can compute the Jensen gap $\\Delta$:\n$$\n\\Delta = 2 - 1 = 1\n$$\n\nThird, we explain the significance of the sign of $\\Delta$. A function $f$ is convex over a convex set $S$ if and only if Jensen's inequality, $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$, holds for all $x,y \\in S$ and all $t \\in [0,1]$. This inequality is equivalent to the Jensen gap $\\Delta$ being non-positive, i.e., $\\Delta \\leq 0$.\nOur calculation for the function $f(\\cdot) = \\|\\cdot\\|_0$ with points from the set $C$ yielded a Jensen gap of $\\Delta = 1$. Since $\\Delta > 0$, Jensen's inequality is violated:\n$$\n\\|t x + (1-t) y\\|_0 = 2 > 1 = t \\|x\\|_0 + (1-t) \\|y\\|_0\n$$\nThe positive sign of the Jensen gap provides a direct and concrete diagnosis of the failure of convexity. The existence of this single counterexample is sufficient to prove that the zero \"norm\" $\\|\\cdot\\|_0$ is not a convex function on the set $C$ (or on $\\mathbb{R}^2$ in general). This non-convexity is a fundamental reason why sparse optimization problems involving $\\|\\cdot\\|_0$ are computationally hard.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Jensen's inequality is not just for convex functions; a parallel and equally important version exists for concave functions. This practice explores this dual relationship by modeling a simple measurement process, a concept critical in fields from economics to signal processing . You will directly compute the \"reversed\" inequality, building intuition for how concave mappings affect expectations.",
            "id": "3455580",
            "problem": "Consider a random measurement residual energy in compressed sensing defined as follows. Let a sensing matrix act on a sparse signal and suppose an algorithm produces a residual vector whose squared Euclidean norm is modeled by a nonnegative random variable $X$. The mapping $f(x) = \\sqrt{x}$ is used to translate squared energy to energy (Euclidean norm), and is known to be concave on $x \\ge 0$.\n\nStarting from the definition of concavity, derive the inequality relating $f(\\mathbb{E}[X])$ and $\\mathbb{E}[f(X)]$ for a two-point distribution. Then, to concretely illustrate how the inequality is reversed relative to the convex case, consider a two-point model in which the residual energy $X$ takes the value $x_{1} = 0$ with probability $\\theta = \\frac{1}{4}$ (an exact fit event) and the value $x_{2} = 1$ with probability $1 - \\theta = \\frac{3}{4}$ (a mismatch event). Compute the quantities $\\mathbb{E}[f(X)]$ and $f(\\mathbb{E}[X])$, and report the difference\n$$\\Delta = f(\\mathbb{E}[X]) - \\mathbb{E}[f(X)].$$\nProvide your final answer as a single closed-form analytical expression. No rounding is required, and no units are to be included in the final expression.",
            "solution": "The problem requires a two-part solution: first, a derivation of Jensen's inequality for a concave function with a two-point distribution, and second, a calculation of a specific quantity for a given example.\n\n**Part 1: Derivation of the Inequality**\n\nThe definition of a concave function $f$ on an interval $I$ is that for any two points $x_1, x_2 \\in I$ and for any $\\lambda \\in [0, 1]$, the following inequality holds:\n$$f(\\lambda x_1 + (1-\\lambda) x_2) \\ge \\lambda f(x_1) + (1-\\lambda) f(x_2)$$\nThis states that the function's value at a weighted average of two points is greater than or equal to the weighted average of the function's values at those points.\n\nNow, consider a discrete random variable $X$ that takes on two values, $x_1$ and $x_2$, with respective probabilities $P(X=x_1) = \\theta$ and $P(X=x_2) = 1-\\theta$, where $\\theta \\in [0, 1]$.\nThe expected value of $X$ is defined as:\n$$\\mathbb{E}[X] = \\theta x_1 + (1-\\theta) x_2$$\nThe expected value of the transformed random variable $f(X)$ is:\n$$\\mathbb{E}[f(X)] = \\theta f(x_1) + (1-\\theta) f(x_2)$$\nBy setting $\\lambda = \\theta$ in the definition of concavity, we can directly relate these two expectations. The term $\\lambda x_1 + (1-\\lambda) x_2$ becomes $\\theta x_1 + (1-\\theta) x_2$, which is exactly $\\mathbb{E}[X]$. The term $\\lambda f(x_1) + (1-\\lambda) f(x_2)$ becomes $\\theta f(x_1) + (1-\\theta) f(x_2)$, which is exactly $\\mathbb{E}[f(X)]$.\n\nSubstituting these into the concavity definition yields Jensen's inequality for a concave function and a two-point distribution:\n$$f(\\mathbb{E}[X]) \\ge \\mathbb{E}[f(X)]$$\nThis inequality is reversed compared to the one for convex functions, as correctly noted in the problem statement.\n\n**Part 2: Concrete Calculation**\n\nWe are given the concave function $f(x) = \\sqrt{x}$ and a random variable $X$ with the following two-point distribution:\n- $X$ takes the value $x_1 = 0$ with probability $\\theta = \\frac{1}{4}$.\n- $X$ takes the value $x_2 = 1$ with probability $1 - \\theta = 1 - \\frac{1}{4} = \\frac{3}{4}$.\n\nFirst, we compute the expected value of $X$, $\\mathbb{E}[X]$.\n$$\\mathbb{E}[X] = \\theta x_1 + (1-\\theta) x_2 = \\left(\\frac{1}{4}\\right)(0) + \\left(\\frac{3}{4}\\right)(1) = 0 + \\frac{3}{4} = \\frac{3}{4}$$\n\nNext, we calculate $f(\\mathbb{E}[X])$. This is the function $f$ evaluated at the expected value of $X$.\n$$f(\\mathbb{E}[X]) = f\\left(\\frac{3}{4}\\right) = \\sqrt{\\frac{3}{4}} = \\frac{\\sqrt{3}}{\\sqrt{4}} = \\frac{\\sqrt{3}}{2}$$\n\nThen, we compute the expected value of $f(X)$, $\\mathbb{E}[f(X)]$.\n$$\\mathbb{E}[f(X)] = \\theta f(x_1) + (1-\\theta) f(x_2)$$\nWe evaluate the function at each point:\n$$f(x_1) = f(0) = \\sqrt{0} = 0$$\n$$f(x_2) = f(1) = \\sqrt{1} = 1$$\nNow, we can compute the expectation:\n$$\\mathbb{E}[f(X)] = \\left(\\frac{1}{4}\\right)(0) + \\left(\\frac{3}{4}\\right)(1) = 0 + \\frac{3}{4} = \\frac{3}{4}$$\n\nFinally, we compute the required difference, $\\Delta$.\n$$\\Delta = f(\\mathbb{E}[X]) - \\mathbb{E}[f(X)]$$\nSubstituting the computed values:\n$$\\Delta = \\frac{\\sqrt{3}}{2} - \\frac{3}{4}$$\nThis is the final closed-form analytical expression for the difference. As expected from Jensen's inequality for a concave function, this difference is non-negative since $\\frac{\\sqrt{3}}{2} \\approx \\frac{1.732}{2} = 0.866$ and $\\frac{3}{4} = 0.75$.",
            "answer": "$$\\boxed{\\frac{\\sqrt{3}}{2} - \\frac{3}{4}}$$"
        },
        {
            "introduction": "The properties of convex and concave functions are not merely for analysis; they are powerful tools for designing algorithms. This exercise demonstrates how the concavity of a logarithmic penalty function can be used to construct a simpler, solvable surrogate problem at each step of an optimization . This technique, known as majorization-minimization, is a cornerstone of modern computational science for tackling otherwise intractable non-convex problems.",
            "id": "3455581",
            "problem": "Consider a linear inverse problem with measurements modeled as $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $y \\in \\mathbb{R}^{m}$. A common nonconvex sparsity-promoting penalty in compressed sensing is built from the function $g(t)$ defined by $g(t) = \\ln(1 + t)$ for $t \\ge 0$. Suppose we aim to minimize the regularized objective $\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} g(|x_{i}|)$ with $\\lambda > 0$ using a majorization-minimization strategy. At a current iterate $x^{(k)}$, let $t_{0} = |x_{i}^{(k)}|$ for each coordinate $i$. Starting only from the definition of concavity and Jensen’s inequality, construct an affine upper bound of the one-dimensional penalty $g(t)$ that is exact at $t = t_{0}$. Then use this bound to derive the surrogate regularization at iteration $k$ in the form of a weighted $\\ell_{1}$ penalty $\\sum_{i=1}^{n} w_{i}^{(k)} |x_{i}|$.\n\nYour task is to determine the explicit closed-form analytic expression for the scalar weight $w(t_{0})$ that arises from this construction, expressed solely in terms of $t_{0}$. Provide your final answer as a single analytic expression. No rounding is required, and no units are involved.",
            "solution": "We start from the function $g(t) = \\ln(1 + t)$ defined for $t \\ge 0$. Our goal is to construct an affine upper bound that is exact at a given expansion point $t_{0} \\ge 0$, relying exclusively on the fundamental properties of concavity and Jensen’s inequality.\n\nFirst, we establish concavity of $g$. Compute the first and second derivatives:\n$$\ng'(t) = \\frac{1}{1 + t}, \\quad g''(t) = -\\frac{1}{(1 + t)^{2}}.\n$$\nFor all $t \\ge 0$, we have $g''(t) \\le 0$, which implies that $g$ is concave on $[0, \\infty)$. A twice continuously differentiable concave function in one dimension admits a global supporting hyperplane at every point, meaning its first-order Taylor expansion is an affine upper bound. This follows from Taylor’s theorem with Lagrange remainder: for any $s$ and a concave $g$,\n$$\ng(s) = g(t_{0}) + g'(t_{0})(s - t_{0}) + \\frac{1}{2} g''(\\xi) (s - t_{0})^{2}\n$$\nfor some $\\xi$ between $s$ and $t_{0}$. Because $g''(\\xi) \\le 0$, the remainder term is nonpositive, yielding\n$$\ng(s) \\le g(t_{0}) + g'(t_{0})(s - t_{0}).\n$$\nThus, the affine function\n$$\nM(s; t_{0}) = g(t_{0}) + g'(t_{0})(s - t_{0})\n$$\nis a global majorizer of $g$ that is exact at $s = t_{0}$.\n\nTo connect this construction to Jensen’s inequality and the compressed sensing context, recall that for any concave function $g$ and any random variable $T$, Jensen’s inequality states $g(\\mathbb{E}[T]) \\ge \\mathbb{E}[g(T)]$. The tangent-affine form $M(\\cdot; t_{0})$ is linear and hence satisfies $\\mathbb{E}[M(T; t_{0})] = M(\\mathbb{E}[T]; t_{0})$. Since $g(T) \\le M(T; t_{0})$ pointwise, taking expectations gives $\\mathbb{E}[g(T)] \\le \\mathbb{E}[M(T; t_{0})] = M(\\mathbb{E}[T]; t_{0})$, which is consistent with the concavity and supports the use of linear majorizers in expectation-based or deterministic surrogate constructions.\n\nIn the majorization-minimization procedure for the regularized objective $\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} g(|x_{i}|)$, we majorize each $g(|x_{i}|)$ at $t_{0} = |x_{i}^{(k)}|$ by $M(|x_{i}|; t_{0})$. Summing over $i$, the regularization term becomes\n$$\n\\sum_{i=1}^{n} g(|x_{i}|) \\le \\sum_{i=1}^{n} \\left[g(t_{0}) + g'(t_{0})(|x_{i}| - t_{0})\\right] = \\sum_{i=1}^{n} g(t_{0}) - \\sum_{i=1}^{n} g'(t_{0}) t_{0} + \\sum_{i=1}^{n} g'(t_{0}) |x_{i}|.\n$$\nThe first two sums are constants with respect to $x$ at iteration $k$, so the surrogate objective to minimize with respect to $x$ at iteration $k$ reduces to\n$$\n\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_{i}^{(k)} |x_{i}|,\n$$\nwhere the weights are given by\n$$\nw_{i}^{(k)} = g'\\big(|x_{i}^{(k)}|\\big).\n$$\nSince $g'(t) = \\frac{1}{1 + t}$, we obtain the explicit closed-form expression\n$$\nw(t_{0}) = \\frac{1}{1 + t_{0}}.\n$$\nThis is the weight as a function of the local expansion point $t_{0} = |x_{i}^{(k)}|$, yielding a reweighted $\\ell_{1}$ step where coordinates with larger $|x_{i}^{(k)}|$ receive smaller weights, in line with nonconvex sparsity promotion via concavity of $g$. The expression is closed-form and directly derived from the concavity-based majorization without invoking any shortcuts beyond the fundamental properties established above.",
            "answer": "$$\\boxed{\\frac{1}{1+t_{0}}}$$"
        }
    ]
}