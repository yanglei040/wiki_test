{
    "hands_on_practices": [
        {
            "introduction": "This practice explores the fundamental relationship between the $\\ell_1$-norm and the Euclidean $\\ell_2$-norm, which is central to the theory of compressed sensing . By deriving the constants that bound the ratio of these norms, both on the entire space and on the subspace of sparse vectors, you will gain insight into the geometric underpinnings of sparsity. Understanding this connection is the first step toward appreciating why minimizing the $\\ell_1$-norm is an effective proxy for finding the sparsest solution to a system of linear equations.",
            "id": "3493063",
            "problem": "Let $V=\\mathbb{R}^{n}$ be equipped with the canonical basis $\\{e_{1},\\dots,e_{n}\\}$ and the standard inner product. For $x=(x_{1},\\dots,x_{n})\\in\\mathbb{R}^{n}$, define the norms $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ and $\\|x\\|_{2}=\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{1/2}$. Consider the identity maps between the normed spaces $(\\mathbb{R}^{n},\\|\\cdot\\|_{2})$ and $(\\mathbb{R}^{n},\\|\\cdot\\|_{1})$. Starting only from the definitions of $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{2}$, the Cauchy–Schwarz inequality, and properties of the canonical basis, derive the sharp constants $a_{n}$ and $b_{n}$ such that for all $x\\in\\mathbb{R}^{n}\\setminus\\{0\\}$,\n$$\na_{n}\\leq \\frac{\\|x\\|_{1}}{\\|x\\|_{2}}\\leq b_{n}.\n$$\nThen, for a fixed sparsity level $k$ with $1\\leq k\\leq n$, define the set of $k$-sparse vectors\n$$\n\\Sigma_{k}=\\{x\\in\\mathbb{R}^{n}:\\#\\{i:x_{i}\\neq 0\\}\\leq k\\},\n$$\nand derive the sharp constant $b_{n,k}$ such that for all $x\\in\\Sigma_{k}\\setminus\\{0\\}$,\n$$\n\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}\\leq b_{n,k}.\n$$\nExplain the geometric meaning of these inequalities in terms of the shapes and relative containments of the unit balls of $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{2}$, and articulate how this geometry promotes sparsity when using $\\ell_{1}$-regularization in Compressed Sensing (CS). Your derivation must begin from the stated foundational definitions and inequalities and should connect the role of the canonical basis and coordinate subspaces to the behavior of $\\|\\cdot\\|_{1}$ on sparse vectors.\n\nReport your final answer as the row vector $(\\alpha_{n},\\beta_{n},\\beta_{n,k})$, where $\\alpha_{n}=\\inf_{x\\neq 0}\\|x\\|_{1}/\\|x\\|_{2}$, $\\beta_{n}=\\sup_{x\\neq 0}\\|x\\|_{1}/\\|x\\|_{2}$, and $\\beta_{n,k}=\\sup_{x\\in\\Sigma_{k}\\setminus\\{0\\}}\\|x\\|_{1}/\\|x\\|_{2}$. No numerical approximation is required.",
            "solution": "The problem statement is a well-posed mathematical problem in linear algebra and analysis. It is scientifically grounded, objective, and self-contained, providing all necessary definitions and constraints. The problem is valid.\n\nWe are tasked with finding the sharp constants $a_n$, $b_n$, and $b_{n,k}$ for inequalities relating the $\\ell_1$ and $\\ell_2$ norms in $\\mathbb{R}^n$, and explaining the geometric and practical implications of these relationships in the context of compressed sensing. The ratio $\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}$ is homogeneous of degree zero, meaning its value is constant for any non-zero scalar multiple of $x$. Therefore, finding the infimum and supremum over all $x \\in \\mathbb{R}^n \\setminus \\{0\\}$ is equivalent to finding the minimum and maximum over the set of vectors with unit $\\ell_2$-norm, i.e., $\\|x\\|_2 = 1$.\n\nLet $x = (x_1, \\dots, x_n) \\in \\mathbb{R}^n$. The norms are defined as $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$ and $\\|x\\|_{2}=\\left(\\sum_{i=1}^{n}x_{i}^{2}\\right)^{1/2}$.\n\n**Derivation of the lower bound $a_n$**\n\nWe seek to find the infimum of $\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}$. Let's consider the square of the $\\ell_1$-norm:\n$$\n\\|x\\|_1^2 = \\left(\\sum_{i=1}^{n}|x_i|\\right)^2 = \\sum_{i=1}^{n}|x_i|^2 + \\sum_{i \\neq j} |x_i||x_j|\n$$\nThe first term is exactly the square of the $\\ell_2$-norm: $\\sum_{i=1}^{n}|x_i|^2 = \\sum_{i=1}^{n}x_i^2 = \\|x\\|_2^2$. The second term, representing the sum of all cross-products, is non-negative, since $|x_i| \\ge 0$ for all $i$. Therefore, we have the inequality:\n$$\n\\|x\\|_1^2 \\ge \\|x\\|_2^2\n$$\nSince norms are non-negative, we can take the square root of both sides to obtain $\\|x\\|_1 \\ge \\|x\\|_2$. For any $x \\neq 0$, this implies:\n$$\n\\frac{\\|x\\|_1}{\\|x\\|_2} \\ge 1\n$$\nThis establishes $a_n \\ge 1$. To show that this bound is sharp, we must find a vector $x$ for which the equality holds. Equality holds if and only if the sum of cross-products is zero, i.e., $\\sum_{i \\neq j} |x_i||x_j| = 0$. This condition is met if and only if at most one component $x_i$ is non-zero. Such vectors are scalar multiples of the canonical basis vectors $\\{e_1, \\dots, e_n\\}$.\nLet's choose $x = e_j$ for some $j \\in \\{1, \\dots, n\\}$.\nThen $\\|x\\|_1 = |1| = 1$ and $\\|x\\|_2 = \\sqrt{1^2} = 1$. For this vector, the ratio is $\\frac{\\|x\\|_1}{\\|x\\|_2} = 1$.\nThus, the infimum is $1$, and the constant is sharp. Let $\\alpha_n = a_n$.\n$$\n\\alpha_n = \\inf_{x \\neq 0} \\frac{\\|x\\|_1}{\\|x\\|_2} = 1\n$$\n\n**Derivation of the upper bound $b_n$**\n\nWe seek the supremum of $\\frac{\\|x\\|_{1}}{\\|x\\|_{2}}$. We use the Cauchy–Schwarz inequality, which states that for any two vectors $u, v \\in \\mathbb{R}^n$, $|\\langle u, v \\rangle| \\le \\|u\\|_2 \\|v\\|_2$.\nLet us define two vectors: $u = (|x_1|, |x_2|, \\dots, |x_n|)$ and $v = (1, 1, \\dots, 1)$.\nTheir inner product is:\n$$\n\\langle u, v \\rangle = \\sum_{i=1}^n |x_i| \\cdot 1 = \\|x\\|_1\n$$\nThe $\\ell_2$-norms of these vectors are:\n$$\n\\|u\\|_2 = \\left(\\sum_{i=1}^n |x_i|^2\\right)^{1/2} = \\left(\\sum_{i=1}^n x_i^2\\right)^{1/2} = \\|x\\|_2\n$$\n$$\n\\|v\\|_2 = \\left(\\sum_{i=1}^n 1^2\\right)^{1/2} = \\sqrt{n}\n$$\nApplying the Cauchy–Schwarz inequality:\n$$\n\\|x\\|_1 = \\langle u, v \\rangle \\le \\|u\\|_2 \\|v\\|_2 = \\|x\\|_2 \\sqrt{n}\n$$\nFor any $x \\neq 0$, this gives the upper bound:\n$$\n\\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{n}\n$$\nThis establishes $b_n \\le \\sqrt{n}$. To show this bound is sharp, we must find a vector for which equality holds. Equality in the Cauchy–Schwarz inequality holds if and only if $u$ is a scalar multiple of $v$, i.e., $u = \\lambda v$ for some $\\lambda \\in \\mathbb{R}$. This means $(|x_1|, \\dots, |x_n|) = \\lambda (1, \\dots, 1)$, which implies $|x_1| = |x_2| = \\dots = |x_n| = \\lambda$.\nLet's choose $x = (1, 1, \\dots, 1)$.\nThen $\\|x\\|_1 = \\sum_{i=1}^n |1| = n$ and $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^n 1^2} = \\sqrt{n}$. The ratio for this vector is $\\frac{\\|x\\|_1}{\\|x\\|_2} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$.\nThus, the supremum is $\\sqrt{n}$, and the constant is sharp. Let $\\beta_n = b_n$.\n$$\n\\beta_n = \\sup_{x \\neq 0} \\frac{\\|x\\|_1}{\\|x\\|_2} = \\sqrt{n}\n$$\n\n**Derivation of the upper bound $b_{n,k}$ for sparse vectors**\n\nNow we restrict our attention to the set of $k$-sparse vectors, $\\Sigma_k = \\{x \\in \\mathbb{R}^n : \\#\\{i : x_i \\neq 0\\} \\le k\\}$. We want to find the supremum of $\\frac{\\|x\\|_1}{\\|x\\|_2}$ for $x \\in \\Sigma_k \\setminus \\{0\\}$.\nLet $x \\in \\Sigma_k$, and let $S$ be the support of $x$, i.e., the set of indices where $x_i \\neq 0$. Let $m = |S| = \\#\\{i : x_i \\neq 0\\}$. By definition of $\\Sigma_k$, we have $m \\le k$.\nThe $\\ell_1$ and $\\ell_2$ norms of $x$ only depend on its non-zero components. Let $x_S \\in \\mathbb{R}^m$ be the vector consisting of the non-zero entries of $x$.\n$$\n\\|x\\|_1 = \\sum_{i \\in S} |x_i| = \\|x_S\\|_1\n$$\n$$\n\\|x\\|_2 = \\left(\\sum_{i \\in S} x_i^2\\right)^{1/2} = \\|x_S\\|_2\n$$\nThe ratio is $\\frac{\\|x\\|_1}{\\|x\\|_2} = \\frac{\\|x_S\\|_1}{\\|x_S\\|_2}$. Since $x_S$ is a vector in $\\mathbb{R}^m$, we can apply the upper bound derived previously to this vector.\n$$\n\\frac{\\|x_S\\|_1}{\\|x_S\\|_2} \\le \\sqrt{m}\n$$\nSince $m = |S| \\le k$, we have $\\sqrt{m} \\le \\sqrt{k}$. Therefore, for any $x \\in \\Sigma_k \\setminus \\{0\\}$:\n$$\n\\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{k}\n$$\nThis establishes $b_{n,k} \\le \\sqrt{k}$. To show this bound is sharp, we must find a vector in $\\Sigma_k$ that achieves it. The maximum value of $\\sqrt{m}$ for $m \\le k$ is $\\sqrt{k}$. Equality is achieved for a vector $x$ that has exactly $k$ non-zero components, all of which have the same absolute value.\nLet's construct such a vector. For example, let $x$ be the vector with its first $k$ components equal to $1$ and the remaining $n-k$ components equal to $0$.\n$x = (1, \\dots, 1, 0, \\dots, 0)$. This vector has support size $k$, so $x \\in \\Sigma_k$.\n$\\|x\\|_1 = \\sum_{i=1}^k |1| = k$.\n$\\|x\\|_2 = \\sqrt{\\sum_{i=1}^k 1^2} = \\sqrt{k}$.\nThe ratio is $\\frac{\\|x\\|_1}{\\|x\\|_2} = \\frac{k}{\\sqrt{k}} = \\sqrt{k}$.\nThus, the supremum over $\\Sigma_k$ is $\\sqrt{k}$, and the constant is sharp. Let $\\beta_{n,k} = b_{n,k}$.\n$$\n\\beta_{n,k} = \\sup_{x \\in \\Sigma_k \\setminus \\{0\\}} \\frac{\\|x\\|_1}{\\|x\\|_2} = \\sqrt{k}\n$$\n\n**Geometric Meaning and Connection to Compressed Sensing (CS)**\n\nThe inequalities $1 \\le \\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{n}$ can be rewritten as $\\|x\\|_2 \\le \\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$. These describe the geometric relationship between the unit balls of the respective norms. Let $B_p = \\{x \\in \\mathbb{R}^n : \\|x\\|_p \\le 1\\}$ for $p=1,2$.\nThe inequality $\\|x\\|_2 \\le \\|x\\|_1$ is equivalent to the set inclusion $B_1 \\subset B_2$. The unit $\\ell_1$-ball (a cross-polytope, e.g., a square rotated by $45^\\circ$ in $\\mathbb{R}^2$ or an octahedron in $\\mathbb{R}^3$) is contained within the unit $\\ell_2$-ball (the standard Euclidean ball). They touch at the points where equality holds, which are the vectors $\\pm e_j$. These points are precisely the vertices of $B_1$ and lie on the surface of $B_2$.\nThe inequality $\\|x\\|_1 \\le \\sqrt{n}\\|x\\|_2$ is equivalent to $B_2 \\subset \\sqrt{n}B_1$. The unit $\\ell_2$-ball is contained within the $\\ell_1$-ball scaled by a factor of $\\sqrt{n}$. They touch at points where $|x_1| = \\dots = |x_n|$, which are the \"corners\" of the Euclidean ball in the $\\ell_1$ sense.\n\nIn Compressed Sensing, we aim to recover a sparse signal $x_0 \\in \\mathbb{R}^n$ from a small number of linear measurements $y=Ax_0$, where $A$ is an $m \\times n$ matrix with $m \\ll n$. Since the system is underdetermined, there is an entire affine subspace of solutions $S_y = \\{x \\in \\mathbb{R}^n : Ax=y\\}$. The challenge is to select $x_0$ from this subspace.\nA natural approach is to find the sparsest vector in $S_y$, which is the solution to $\\min \\|x\\|_0$ subject to $Ax=y$. This is computationally intractable (NP-hard).\nThe key insight of CS is to relax this by solving for the vector with the minimum $\\ell_1$-norm instead:\n$$\n\\min \\|x\\|_1 \\quad \\text{subject to} \\quad Ax=y\n$$\nThis is a convex optimization problem and can be solved efficiently. The geometric reason for its success lies in the shape of the $\\ell_1$-ball. The problem is equivalent to finding the smallest $\\ell_1$-ball that intersects the affine subspace $S_y$. Because the $\\ell_1$-ball has \"spikes\" or vertices pointing along the coordinate axes (the canonical basis vectors), an expanding $\\ell_1$-ball is most likely to first touch the affine subspace $S_y$ at one of these vertices or a low-dimensional edge/face. A point on a vertex is $1$-sparse; a point on an edge is $2$-sparse, and so on. This geometry inherently promotes sparse solutions.\nIn contrast, minimizing the $\\ell_2$-norm (the classical least-norm solution) corresponds to finding the point in $S_y$ closest to the origin. This is equivalent to finding the first point of contact between $S_y$ and an expanding $\\ell_2$-ball. Due to the perfect roundness of the $\\ell_2$-ball, the point of contact is generally not on a coordinate axis and the resulting solution is dense.\n\nThe inequality $\\frac{\\|x\\|_1}{\\|x\\|_2} \\le \\sqrt{k}$ for $k$-sparse vectors is fundamental to the theory of CS. It shows that on the set of sparse vectors, the $\\ell_1$ and $\\ell_2$ norms are much more closely related (by a factor of $\\sqrt{k}$) than on the whole space (factor of $\\sqrt{n}$). This property, often discussed in the context of the Restricted Isometry Property (RIP), ensures that $\\ell_1$-minimization can successfully recover sparse signals under certain conditions on the measurement matrix $A$.\n\nThe final answer is composed of the derived sharp constants $(\\alpha_n, \\beta_n, \\beta_{n,k})$.\n$\\alpha_n = 1$\n$\\beta_n = \\sqrt{n}$\n$\\beta_{n,k} = \\sqrt{k}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  \\sqrt{n}  \\sqrt{k}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Successful sparse signal recovery depends critically on the measurement operator not being \"aligned\" with the sparse signal structure. This exercise provides a concrete setting to quantify this alignment by calculating the principal angles between signal subspaces and the null space of the measurement matrix . By constructing a scenario where this alignment can be precisely controlled, you will develop a geometric intuition for coherence, a key concept that explains why certain combinations of signals and measurements can be problematic for recovery algorithms.",
            "id": "3493059",
            "problem": "Consider a measurement operator $A \\in \\mathbb{R}^{2 \\times 4}$ acting on $\\mathbb{R}^{4}$, defined by\n$$\nA = \\begin{pmatrix}\n0  0  1  0 \\\\\n0  0  0  1\n\\end{pmatrix}.\n$$\nBy construction, the null space $\\mathcal{N}(A)$ is the subspace\n$$\n\\mathcal{N}(A) = \\mathrm{span}\\{e_1, e_2\\},\n$$\nwhere $e_i$ denotes the $i$-th canonical basis vector of $\\mathbb{R}^4$.\n\nDefine a dictionary $D = \\{d_1, d_2, d_3, d_4\\}$ whose atoms form a basis of $\\mathbb{R}^4$ via a rotation in the $e_2$–$e_3$ plane parameterized by an angle $\\epsilon \\in (0, \\pi/4]$:\n$$\nd_1 = e_1, \\quad\nd_2 = \\cos(\\epsilon)\\, e_2 + \\sin(\\epsilon)\\, e_3, \\quad\nd_3 = -\\sin(\\epsilon)\\, e_2 + \\cos(\\epsilon)\\, e_3, \\quad\nd_4 = e_4.\n$$\nFor any support $S \\subset \\{1,2,3,4\\}$ with $|S| = 2$, define the $2$-sparse subspace\n$$\n\\mathcal{U}_S = \\mathrm{span}\\{d_i : i \\in S\\}.\n$$\nLet the Friedrichs angle $\\varphi(\\mathcal{U}_S, \\mathcal{N}(A))$ denote the smallest positive principal angle between $\\mathcal{U}_S$ and $\\mathcal{N}(A)$, i.e., the smallest principal angle after removing any common intersection subspace $\\mathcal{U}_S \\cap \\mathcal{N}(A)$. The principal angles are defined in terms of the singular values of the product of orthonormal basis matrices: if $Q_{\\mathcal{U}}$ and $Q_{\\mathcal{V}}$ have orthonormal columns spanning $\\mathcal{U}$ and $\\mathcal{V}$, respectively, then the cosines of the principal angles are the singular values of $Q_{\\mathcal{U}}^{\\top} Q_{\\mathcal{V}}$.\n\nStarting from the core definitions of vector subspaces, bases, null spaces, and principal angles, determine the exact closed-form expression (as a function of $\\epsilon$) for\n$$\n\\min_{S \\subset \\{1,2,3,4\\},\\, |S|=2} \\ \\varphi(\\mathcal{U}_S, \\mathcal{N}(A)).\n$$\nExpress your final answer in radians. No numerical rounding is required.",
            "solution": "We begin with the fundamental definitions. The null space $\\mathcal{N}(A)$ is the set of vectors $x \\in \\mathbb{R}^4$ with $A x = 0$. From the explicit form of $A$, $A e_1 = 0$ and $A e_2 = 0$, while $A e_3 = e_1$ and $A e_4 = e_2$. Hence\n$$\n\\mathcal{N}(A) = \\mathrm{span}\\{e_1, e_2\\}.\n$$\nThe dictionary atoms are defined by\n$$\nd_1 = e_1, \\quad\nd_2 = \\cos(\\epsilon)\\, e_2 + \\sin(\\epsilon)\\, e_3, \\quad\nd_3 = -\\sin(\\epsilon)\\, e_2 + \\cos(\\epsilon)\\, e_3, \\quad\nd_4 = e_4,\n$$\nwith $\\epsilon \\in (0, \\pi/4]$. Because $(d_2, d_3)$ is obtained from $(e_2, e_3)$ by a rotation, and $d_1 = e_1$, $d_4 = e_4$, the set $\\{d_1, d_2, d_3, d_4\\}$ is an orthonormal basis of $\\mathbb{R}^4$.\n\nWe will compute the Friedrichs angle between each $2$-sparse subspace $\\mathcal{U}_S$ and $\\mathcal{N}(A)$, and then take the minimum over all supports $S$ of cardinality $2$. The Friedrichs angle is the smallest positive principal angle (after removing any shared intersection). Principal angles between two subspaces spanned by orthonormal columns $Q_{\\mathcal{U}}$ and $Q_{\\mathcal{V}}$ are obtained from the singular values of $Q_{\\mathcal{U}}^{\\top} Q_{\\mathcal{V}}$; specifically, if $\\sigma_1 \\geq \\sigma_2$ are the singular values of $Q_{\\mathcal{U}}^{\\top} Q_{\\mathcal{V}}$, then the principal angles $\\theta_1 \\leq \\theta_2$ satisfy $\\cos(\\theta_i) = \\sigma_i$.\n\nLet\n$$\nQ_{\\mathcal{N}} = \\begin{pmatrix} e_1  e_2 \\end{pmatrix}\n$$\nbe an orthonormal basis matrix for $\\mathcal{N}(A)$, and let $Q_S$ be the $4 \\times 2$ matrix whose columns are the two atoms $\\{d_i : i \\in S\\}$ (ordered arbitrarily). We analyze each support $S$:\n\n1. Case $S = \\{1,2\\}$. Then $Q_S = \\begin{pmatrix} d_1  d_2 \\end{pmatrix}$. The matrix of inner products is\n$$\nQ_{\\mathcal{N}}^{\\top} Q_S = \n\\begin{pmatrix}\n\\langle e_1, d_1 \\rangle  \\langle e_1, d_2 \\rangle \\\\\n\\langle e_2, d_1 \\rangle  \\langle e_2, d_2 \\rangle\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  0 \\\\\n0  \\cos(\\epsilon)\n\\end{pmatrix}.\n$$\nIts singular values are $1$ and $\\cos(\\epsilon)$, so the principal angles are $\\theta_1 = 0$ and $\\theta_2 = \\arccos(\\cos(\\epsilon)) = \\epsilon$. Because the subspaces intersect (they share $e_1$), the Friedrichs angle is the smallest positive principal angle, which is\n$$\n\\varphi(\\mathcal{U}_{\\{1,2\\}}, \\mathcal{N}(A)) = \\epsilon.\n$$\n\n2. Case $S = \\{1,3\\}$. Then\n$$\nQ_{\\mathcal{N}}^{\\top} Q_S =\n\\begin{pmatrix}\n\\langle e_1, d_1 \\rangle  \\langle e_1, d_3 \\rangle \\\\\n\\langle e_2, d_1 \\rangle  \\langle e_2, d_3 \\rangle\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  0 \\\\\n0  -\\sin(\\epsilon)\n\\end{pmatrix}.\n$$\nThe singular values are $1$ and $\\sin(\\epsilon)$, so the principal angles are $\\theta_1 = 0$ and $\\theta_2 = \\arccos(\\sin(\\epsilon))$. The Friedrichs angle is\n$$\n\\varphi(\\mathcal{U}_{\\{1,3\\}}, \\mathcal{N}(A)) = \\arccos(\\sin(\\epsilon)).\n$$\n\n3. Case $S = \\{1,4\\}$. Then\n$$\nQ_{\\mathcal{N}}^{\\top} Q_S =\n\\begin{pmatrix}\n\\langle e_1, d_1 \\rangle  \\langle e_1, d_4 \\rangle \\\\\n\\langle e_2, d_1 \\rangle  \\langle e_2, d_4 \\rangle\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  0 \\\\\n0  0\n\\end{pmatrix}.\n$$\nThe singular values are $1$ and $0$, so the principal angles are $\\theta_1 = 0$ and $\\theta_2 = \\arccos(0) = \\frac{\\pi}{2}$. The Friedrichs angle is\n$$\n\\varphi(\\mathcal{U}_{\\{1,4\\}}, \\mathcal{N}(A)) = \\frac{\\pi}{2}.\n$$\n\n4. Case $S = \\{2,3\\}$. Then\n$$\nQ_{\\mathcal{N}}^{\\top} Q_S =\n\\begin{pmatrix}\n\\langle e_1, d_2 \\rangle  \\langle e_1, d_3 \\rangle \\\\\n\\langle e_2, d_2 \\rangle  \\langle e_2, d_3 \\rangle\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0 \\\\\n\\cos(\\epsilon)  -\\sin(\\epsilon)\n\\end{pmatrix}.\n$$\nThe singular values of this matrix are $1$ and $0$ (since the second row is a unit vector in $\\mathbb{R}^2$). Therefore, the principal angles are $\\theta_1 = 0$ and $\\theta_2 = \\frac{\\pi}{2}$. The Friedrichs angle is\n$$\n\\varphi(\\mathcal{U}_{\\{2,3\\}}, \\mathcal{N}(A)) = \\frac{\\pi}{2}.\n$$\n\n5. Case $S = \\{2,4\\}$. Then\n$$\nQ_{\\mathcal{N}}^{\\top} Q_S =\n\\begin{pmatrix}\n\\langle e_1, d_2 \\rangle  \\langle e_1, d_4 \\rangle \\\\\n\\langle e_2, d_2 \\rangle  \\langle e_2, d_4 \\rangle\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0 \\\\\n\\cos(\\epsilon)  0\n\\end{pmatrix}.\n$$\nThe singular values are $\\cos(\\epsilon)$ and $0$, so the principal angles are $\\theta_1 = \\arccos(\\cos(\\epsilon)) = \\epsilon$ and $\\theta_2 = \\frac{\\pi}{2}$. Here the intersection is trivial, so the Friedrichs angle equals the smallest principal angle:\n$$\n\\varphi(\\mathcal{U}_{\\{2,4\\}}, \\mathcal{N}(A)) = \\epsilon.\n$$\n\n6. Case $S = \\{3,4\\}$. Then\n$$\nQ_{\\mathcal{N}}^{\\top} Q_S =\n\\begin{pmatrix}\n\\langle e_1, d_3 \\rangle  \\langle e_1, d_4 \\rangle \\\\\n\\langle e_2, d_3 \\rangle  \\langle e_2, d_4 \\rangle\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0  0 \\\\\n-\\sin(\\epsilon)  0\n\\end{pmatrix}.\n$$\nThe singular values are $\\sin(\\epsilon)$ and $0$, so the principal angles are $\\theta_1 = \\arccos(\\sin(\\epsilon))$ and $\\theta_2 = \\frac{\\pi}{2}$. Thus\n$$\n\\varphi(\\mathcal{U}_{\\{3,4\\}}, \\mathcal{N}(A)) = \\arccos(\\sin(\\epsilon)).\n$$\n\nWe now take the minimum over all supports $S$ of cardinality $2$:\n$$\n\\min_{|S|=2} \\ \\varphi(\\mathcal{U}_S, \\mathcal{N}(A)) = \\min\\left\\{ \\epsilon, \\arccos(\\sin(\\epsilon)), \\frac{\\pi}{2} \\right\\}.\n$$\nSince $\\epsilon \\in (0, \\pi/4]$, we have $\\sin(\\epsilon) \\leq \\sin(\\pi/4) = \\frac{\\sqrt{2}}{2}$, which implies\n$$\n\\arccos(\\sin(\\epsilon)) \\geq \\arccos\\!\\left(\\frac{\\sqrt{2}}{2}\\right) = \\frac{\\pi}{4} \\geq \\epsilon.\n$$\nTherefore, for $\\epsilon \\in (0, \\pi/4]$,\n$$\n\\min_{|S|=2} \\ \\varphi(\\mathcal{U}_S, \\mathcal{N}(A)) = \\epsilon.\n$$\nHence the pathological dictionary produces a $2$-sparse subspace whose Friedrichs angle to the null space of $A$ is exactly $\\epsilon$, in radians.",
            "answer": "$$\\boxed{\\epsilon}$$"
        },
        {
            "introduction": "The Restricted Isometry Property (RIP) is a powerful condition that provides performance guarantees for sparse recovery, ensuring that a measurement matrix approximately preserves the length of all sparse vectors. This computational exercise challenges you to translate the abstract definition of the RIP and its associated constant, $\\delta_s$, into a concrete algorithm . By implementing a program to calculate $\\delta_s$ for different matrices, you will gain a practical understanding of how to certify a measurement operator and appreciate the computational complexity involved in verifying these theoretical guarantees.",
            "id": "3493111",
            "problem": "You are asked to construct, analyze, and numerically certify a linear measurement operator within linear algebraic foundations tailored to sparse signal models. Let $A \\in \\mathbb{R}^{m \\times n}$ be a real matrix. The set of all $s$-sparse vectors is the union of all coordinate subspaces spanned by subsets of the standard basis of $\\mathbb{R}^{n}$ of size at most $s$. Formally, define the family of $s$-sparse subspaces as \n$$\n\\mathcal{U}_s \\triangleq \\bigcup_{\\substack{S \\subseteq [n] \\\\ |S| \\le s}} \\left\\{ x \\in \\mathbb{R}^{n} : \\operatorname{supp}(x) \\subseteq S \\right\\}.\n$$\nA matrix $A$ is a subspace embedding for all $s$-sparse subspaces if there exists a smallest constant $\\delta_s \\in [0,1)$ such that, for every $x \\in \\mathcal{U}_s$, the restricted isometry inequality holds:\n$$\n(1 - \\delta_s) \\, \\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_s) \\, \\|x\\|_2^2.\n$$\nThe quantity $\\delta_s$ is the restricted isometry constant (RIC) of order $s$. The rows of $A$ must be linearly independent so that they form a basis (not necessarily orthonormal) for the row space, that is, $\\operatorname{rank}(A) = m$.\n\nYour task is to:\n- Design a procedure to construct $A$ so that its rows form a basis and $A$ acts as a subspace embedding for all $s$-sparse subspaces. Use a construction that is justified from first principles. One canonical choice is to take $A$ with independent and identically distributed Gaussian entries with mean $0$ and variance $1/m$, which implies $\\mathbb{E}[A^\\top A] = I_n$, and with probability $1$ the rows are linearly independent for $m \\le n$. As a boundary case, you may also consider the identity matrix $A = I_n$ when $m = n$, which yields an exact embedding with $\\delta_s = 0$ for all $s \\le n$.\n- Exactly quantify the optimal restricted isometry constant $\\delta_s$ for a given $A$ by computing the smallest $\\delta \\ge 0$ that satisfies the above inequality for all $x \\in \\mathcal{U}_s$. Equivalently, for every support set $S \\subseteq [n]$ with $|S| \\le s$, let $A_S \\in \\mathbb{R}^{m \\times |S|}$ denote the restriction of $A$ to the columns indexed by $S$, and let $G_S \\triangleq A_S^\\top A_S \\in \\mathbb{R}^{|S| \\times |S|}$. Then \n$$\n\\delta_s \\;=\\; \\max_{\\substack{S \\subseteq [n] \\\\ |S| \\le s}} \\left\\| G_S - I_{|S|} \\right\\|_2 \\;=\\; \\max_{\\substack{S \\subseteq [n] \\\\ |S| \\le s}} \\max_{1 \\le i \\le |S|} \\left| \\lambda_i(G_S) - 1 \\right|,\n$$\nwhere $\\lambda_i(\\cdot)$ are the eigenvalues and $\\|\\cdot\\|_2$ denotes the spectral norm.\n\nProgram requirements:\n- Implement a program that constructs $A$ according to the following test suite, verifies $\\operatorname{rank}(A) = m$, and computes the exact $\\delta_s$ by exhaustive enumeration of all supports $S \\subseteq [n]$ with $|S| = s$ and evaluation of the spectral norm $\\|A_S^\\top A_S - I\\|_2$. If $\\operatorname{rank}(A) \\ne m$ for a random construction, resample until $\\operatorname{rank}(A) = m$.\n- The construction protocols for $A$ are:\n  - Identity protocol: set $A = I_n$ when $m = n$, which yields $\\delta_s = 0$ for all $s \\le n$.\n  - Gaussian protocol: draw $A_{ij} \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0, 1/m)$, with a fixed pseudorandom seed for reproducibility.\n\nUse the following test suite:\n- Test $1$: $n = 8$, $m = 8$, $s = 3$, protocol $=$ identity.\n- Test $2$: $n = 12$, $m = 9$, $s = 2$, protocol $=$ Gaussian with seed $= 7$.\n- Test $3$: $n = 12$, $m = 6$, $s = 3$, protocol $=$ Gaussian with seed $= 13$.\n- Test $4$: $n = 10$, $m = 6$, $s = 1$, protocol $=$ Gaussian with seed $= 42$.\n\nOutput format:\n- Your program should produce a single line of output containing the $4$ results (one per test, in the listed order) as a comma-separated list enclosed in square brackets, with each number rounded to $6$ decimal places, for example, $[a,b,c,d]$.\n\nAll quantities are purely mathematical and unitless. Angles are not involved.\n\nYour program must be self-contained and accept no input. It must compute and report the exact values of $\\delta_s$ for the given tests by exhaustive enumeration over all supports of size $s$ and spectral norm evaluation.",
            "solution": "The problem requires the construction of a linear measurement operator, represented by a matrix $A \\in \\mathbb{R}^{m \\times n}$, and the subsequent computation of its order-$s$ Restricted Isometry Constant (RIC), denoted $\\delta_s$. The operator must satisfy specific structural properties, namely that its rows are linearly independent, and it acts as a good subspace embedding for sparse vectors.\n\nFirst, we formalize the definition of the RIC. A matrix $A$ satisfies the Restricted Isometry Property (RIP) of order $s$ if there exists a constant $\\delta_s \\in [0, 1)$ such that for all $s$-sparse vectors $x$ (i.e., vectors with at most $s$ non-zero entries), the following inequality holds:\n$$\n(1 - \\delta_s) \\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_s) \\|x\\|_2^2\n$$\nThe smallest such $\\delta_s$ is the RIC. An equivalent and computationally tractable definition for $\\delta_s$ is given in terms of the spectral properties of submatrices of $A$. Let $S \\subseteq \\{1, 2, \\dots, n\\}$ be a set of column indices, and let $A_S$ be the submatrix of $A$ formed by the columns indexed by $S$. Let $G_S = A_S^\\top A_S$ be the Gram matrix associated with these columns. The RIC $\\delta_s$ is then the maximum possible deviation from isometry over all subspaces of dimension up to $s$:\n$$\n\\delta_s = \\max_{\\substack{S \\subseteq \\{1, \\dots, n\\} \\\\ |S| \\le s}} \\|A_S^\\top A_S - I_{|S|}\\|_2\n$$\nwhere $I_{|S|}$ is the identity matrix of size $|S| \\times |S|$, and $\\|\\cdot\\|_2$ denotes the spectral norm. The spectral norm of a symmetric matrix is the maximum absolute value of its eigenvalues. Therefore, we can write:\n$$\n\\delta_s = \\max_{\\substack{S \\subseteq \\{1, \\dots, n\\} \\\\ |S| \\le s}} \\max\\left( \\lambda_{\\max}(G_S) - 1, 1 - \\lambda_{\\min}(G_S) \\right)\n$$\nwhere $\\lambda_{\\max}(G_S)$ and $\\lambda_{\\min}(G_S)$ are the maximum and minimum eigenvalues of $G_S$, respectively.\n\nThe problem statement directs us to compute $\\delta_s$ by enumerating over all supports $S$ of size exactly $s$. This is a valid simplification. The property $\\delta_k \\le \\delta_{k+1}$ holds for RICs. This monotonicity arises from the eigenvalue interlacing theorem. If $S' \\subset S$ with $|S'|=|S|-1$, then $G_{S'}$ is a principal submatrix of $G_S$. By Cauchy's interlacing theorem, the eigenvalues of $G_{S'}$ interlace those of $G_S$. This implies $\\lambda_{\\min}(G_S) \\le \\lambda_{\\min}(G_{S'})$ and $\\lambda_{\\max}(G_{S'}) \\le \\lambda_{\\max}(G_S)$. Consequently, the deviation from $1$, as captured by $\\max(\\lambda_{\\max}-1, 1-\\lambda_{\\min})$, tends to be maximized for larger support sets. Thus, the maximum over all $|S| \\le s$ will be achieved for a support of size $s$. We can therefore compute $\\delta_s$ as:\n$$\n\\delta_s = \\max_{\\substack{S \\subseteq \\{1, \\dots, n\\} \\\\ |S| = s}} \\|A_S^\\top A_S - I_s\\|_2\n$$\n\nThe overall computational procedure is as follows:\n1.  For each test case, construct the matrix $A \\in \\mathbb{R}^{m \\times n}$ according to the specified protocol ('identity' or 'Gaussian').\n    -  For the 'identity' protocol, $A$ is the identity matrix $I_n$.\n    -  For the 'Gaussian' protocol, the entries $A_{ij}$ are drawn independently from a normal distribution $\\mathcal{N}(0, 1/m)$. A fixed random seed ensures reproducibility.\n2.  Verify that the constructed matrix $A$ has full row rank, i.e., $\\operatorname{rank}(A) = m$. For a matrix with entries drawn from a continuous distribution (like Gaussian), this condition holds with probability $1$ as long as $m \\le n$.\n3.  Initialize a variable $\\delta_{\\text{max}} = 0$.\n4.  Generate all unique combinations of column indices $S$ of size $s$ from the set $\\{0, 1, \\dots, n-1\\}$.\n5.  For each index set $S$:\n    a.  Extract the submatrix $A_S \\in \\mathbb{R}^{m \\times s}$.\n    b.  Compute the Gram matrix $G_S = A_S^\\top A_S$.\n    c.  Since $G_S$ is a symmetric matrix, calculate its eigenvalues. Let them be $\\lambda_1, \\dots, \\lambda_s$.\n    d.  Determine the spectral norm $\\|G_S - I_s\\|_2 = \\max_{i} |\\lambda_i - 1|$. This is equivalent to $\\max(\\lambda_{\\max} - 1, 1 - \\lambda_{\\min})$.\n    e.  Update $\\delta_{\\text{max}} = \\max(\\delta_{\\text{max}}, \\|G_S - I_s\\|_2)$.\n6.  The final value of $\\delta_{\\text{max}}$ is the desired RIC, $\\delta_s$.\n\nThis procedure will be applied to the four specified test cases.\n\n- **Test 1:** $n = 8, m = 8, s = 3$, protocol $=$ identity.\n  The matrix is $A = I_8$. For any submatrix $A_S$ formed by $s=3$ columns of the identity matrix, $A_S^\\top A_S = I_3$. The eigenvalues of $I_3$ are all $1$. Thus, $\\|A_S^\\top A_S - I_3\\|_2 = \\max|1 - 1| = 0$. This is true for all $S$, so $\\delta_3=0$.\n\n- **Test 2:** $n = 12, m = 9, s = 2$, protocol $=$ Gaussian with seed $= 7$.\n  We construct an $A \\in \\mathbb{R}^{9 \\times 12}$ with entries from $\\mathcal{N}(0, 1/9)$ using seed $7$. We then iterate through all $\\binom{12}{2} = 66$ subsets of columns of size $2$. For each $A_S \\in \\mathbb{R}^{9 \\times 2}$, we compute the two eigenvalues of the $2 \\times 2$ matrix $A_S^\\top A_S$ and find the maximal deviation from $1$. The RIC $\\delta_2$ is the maximum of these deviations over all $66$ subsets.\n\n- **Test 3:** $n = 12, m = 6, s = 3$, protocol $=$ Gaussian with seed $= 13$.\n  We construct an $A \\in \\mathbb{R}^{6 \\times 12}$ with entries from $\\mathcal{N}(0, 1/6)$ using seed $13$. We iterate through all $\\binom{12}{3} = 220$ subsets of columns of size $3$. For each $A_S \\in \\mathbb{R}^{6 \\times 3}$, we find the eigenvalues of the $3 \\times 3$ matrix $A_S^\\top A_S$ and compute the corresponding deviation. The maximum of these deviations is $\\delta_3$.\n\n- **Test 4:** $n = 10, m = 6, s = 1$, protocol $=$ Gaussian with seed $= 42$.\n  We construct an $A \\in \\mathbb{R}^{6 \\times 10}$ with entries from $\\mathcal{N}(0, 1/6)$ using seed $42$. For $s=1$, each subset $S$ contains a single column, say $a_j$. Then $A_S = a_j$, and $G_S = a_j^\\top a_j = \\|a_j\\|_2^2$, which is a $1 \\times 1$ matrix. Its single eigenvalue is $\\|a_j\\|_2^2$. The RIC is $\\delta_1 = \\max_{j} |\\|a_j\\|_2^2 - 1|$. We compute this by finding the squared norm of each of the $10$ columns and determining the maximum deviation from $1$.",
            "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef calculate_ric(n, m, s, protocol, seed=None):\n    \"\"\"\n    Constructs a matrix A and computes its Restricted Isometry Constant (RIC) delta_s.\n\n    Args:\n        n (int): Number of columns of A.\n        m (int): Number of rows of A.\n        s (int): Sparsity level.\n        protocol (str): Construction protocol ('identity' or 'Gaussian').\n        seed (int, optional): Seed for the random number generator.\n\n    Returns:\n        float: The computed RIC delta_s.\n    \"\"\"\n    \n    # Step 1: Construct matrix A\n    if protocol == 'identity':\n        if m != n:\n            raise ValueError(\"Identity protocol requires m == n.\")\n        A = np.eye(n)\n    elif protocol == 'Gaussian':\n        if seed is None:\n            raise ValueError(\"Gaussian protocol requires a seed.\")\n        rng = np.random.default_rng(seed)\n        # Resample until rank condition is met. For a continuous distribution,\n        # rank  m is a zero-probability event, so a loop is not practically needed.\n        # A single generation and check is sufficient.\n        A = rng.normal(loc=0.0, scale=1.0 / np.sqrt(m), size=(m, n))\n    else:\n        raise ValueError(f\"Unknown protocol: {protocol}\")\n\n    # Step 2: Verify rank(A) = m\n    # This is a mandatory condition specified in the problem statement.\n    rank_A = np.linalg.matrix_rank(A)\n    if rank_A != m:\n        # This case is highly improbable for the Gaussian protocol with m = n.\n        # If it happens, it might indicate an issue with the seed or extreme parameters.\n        raise RuntimeError(f\"Matrix rank is {rank_A}, but expected {m}. Resampling would be needed.\")\n\n    # Step 3  4: Initialize max delta and iterate through all subsets\n    max_delta = 0.0\n    column_indices = range(n)\n    \n    # As justified in the solution text, we only need to check subsets of size exactly s.\n    for s_indices in combinations(column_indices, s):\n        # Step 5a: Extract submatrix A_S\n        A_S = A[:, s_indices]\n        \n        # Step 5b: Compute Gram matrix G_S\n        G_S = A_S.T @ A_S\n        \n        # Step 5c: Compute eigenvalues of G_S\n        # G_S is symmetric, so we use eigvalsh which is efficient and returns sorted eigenvalues.\n        eigenvalues = np.linalg.eigvalsh(G_S)\n        \n        # Step 5d: Compute spectral norm of G_S - I\n        # This is max(|lambda_max - 1|, |lambda_min - 1|)\n        delta_S = max(np.abs(eigenvalues[-1] - 1.0), np.abs(eigenvalues[0] - 1.0))\n        \n        # Step 5e: Update max delta\n        if delta_S  max_delta:\n            max_delta = delta_S\n            \n    # Step 6: Return the final RIC\n    return max_delta\n\ndef solve():\n    \"\"\"\n    Executes the test suite and prints the results.\n    \"\"\"\n    test_cases = [\n        {'n': 8, 'm': 8, 's': 3, 'protocol': 'identity', 'seed': None},\n        {'n': 12, 'm': 9, 's': 2, 'protocol': 'Gaussian', 'seed': 7},\n        {'n': 12, 'm': 6, 's': 3, 'protocol': 'Gaussian', 'seed': 13},\n        {'n': 10, 'm': 6, 's': 1, 'protocol': 'Gaussian', 'seed': 42},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_ric(\n            n=case['n'],\n            m=case['m'],\n            s=case['s'],\n            protocol=case['protocol'],\n            seed=case['seed']\n        )\n        results.append(result)\n\n    # Format the final output as specified\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}