{
    "hands_on_practices": [
        {
            "introduction": "我们如何能从一个欠定线性方程组中唯一地确定一个稀疏信号？这个问题是压缩感知的核心。本练习将引导你探索一个关键的线性代数概念——矩阵的“spark”，它为稀疏解的唯一性提供了一个直接而优雅的保证。通过证明这一基本唯一性条件并构建一个边界情况下的反例，你将深入理解保证稀疏恢复的根本属性。 ",
            "id": "3460579",
            "problem": "给定一个压缩感知中的线性测量模型 $y = A x_{\\star}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $x_{\\star} \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的，意即 $x_{\\star}$ 的非零项数量至多为 $k$。矩阵 $A$ 的 $\\operatorname{spark}$，记作 $\\operatorname{spark}(A)$，定义为 $A$ 的列向量中构成线性相关集的最小数量。仅使用线性代数和稀疏建模中的基本定义来解决以下问题。\n\n- 仅使用线性无关、矩阵的零空间、$k$-稀疏性和 $\\operatorname{spark}(A)$ 的定义，证明如果 $\\operatorname{spark}(A) > 2k$，那么向量 $x_{\\star}$ 是 $A x = y$ 的唯一 $k$-稀疏解。\n\n- 解释为什么边界情况 $\\operatorname{spark}(A) = 2k$ 允许非唯一解，并构造一个最小维度反例，其中存在两个不同的 $k$-稀疏向量 $x^{(1)} \\neq x^{(2)}$ 满足 $A x^{(1)} = A x^{(2)}$。这里的“最小维度”是指您应使用 $m = 2k - 1$ 以及能使 $\\operatorname{spark}(A) = 2k$ 成立的最小的 $n$。然后，为 $k = 2$ 的情况具体实例化这个构造，取 $m = 3$、$n = 4$，并令\n  - $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$、$a_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$、$a_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$、$a_{4} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}$ 作为 $A$ 的列向量，\n  - $x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 和 $x^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n\n在此实例中验证 $\\operatorname{spark}(A) = 4$ 和 $A x^{(1)} = A x^{(2)}$，并计算 $\\|x^{(1)} - x^{(2)}\\|_{2}^{2}$ 的标量值。将此标量作为您的最终答案。最终答案是精确值，无需四舍五入。",
            "solution": "我们从基本定义开始。一个向量 $x \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的，如果它至多有 $k$ 个非零项。矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的 $\\operatorname{spark}$，记作 $\\operatorname{spark}(A)$，是 $A$ 中构成线性相关集的最小列数。等价地，任何数量少于 $\\operatorname{spark}(A)$ 的列向量都是线性无关的。零空间 $\\mathcal{N}(A)$ 是集合 $\\{h \\in \\mathbb{R}^{n} : A h = 0\\}$。\n\n当 $\\operatorname{spark}(A) > 2k$ 时的唯一性。假设 $y = A x_{\\star}$ 且 $x_{\\star}$ 是 $k$-稀疏的。假设存在另一个 $k$-稀疏解 $x$ 满足 $A x = y$ 且 $x \\neq x_{\\star}$。定义 $h = x - x_{\\star} \\neq 0$。则 $A h = A x - A x_{\\star} = 0$，所以 $h \\in \\mathcal{N}(A) \\setminus \\{0\\}$。令 $S = \\operatorname{supp}(x)$ 和 $T = \\operatorname{supp}(x_{\\star})$ 表示支撑集。那么 $\\operatorname{supp}(h) \\subseteq S \\cup T$，所以 $|\\operatorname{supp}(h)| \\leq |S| + |T| \\leq k + k = 2k$。因为 $h \\in \\mathcal{N}(A) \\setminus \\{0\\}$，由 $\\operatorname{supp}(h)$ 索引的 $A$ 的列向量必定线性相关。根据 $\\operatorname{spark}(A)$ 的定义，任何线性相关的列向量集合的大小至少为 $\\operatorname{spark}(A)$。因此 $|\\operatorname{supp}(h)| \\geq \\operatorname{spark}(A)$。结合起来得到 $\\operatorname{spark}(A) \\leq |\\operatorname{supp}(h)| \\leq 2k$。如果 $\\operatorname{spark}(A) > 2k$，这是不可能的。因此，不存在这样的 $x \\neq x_{\\star}$，故 $x_{\\star}$ 是唯一的 $k$-稀疏解。\n\n边界 $\\operatorname{spark}(A) = 2k$ 时的非唯一性及最小反例。如果 $\\operatorname{spark}(A) = 2k$，那么根据定义，存在一个非零向量 $z \\in \\mathcal{N}(A)$，其支撑集的基数恰好为 $2k$，并且这 $2k$ 个列的任何真子集都是线性无关的。将 $z$ 的支撑集划分为两个不相交的集合 $S$ 和 $T$，使得 $|S| = |T| = k$，并写出 $z = z_{S} - z_{T}$，其中 $z_{S}$ 和 $z_{T}$ 分别支撑在 $S$ 和 $T$ 上，并且其符号与 $z$ 在各自支撑集上的符号相匹配。那么 $A z = 0$ 意味着 $A z_{S} = A z_{T}$。$z_{S}$ 和 $z_{T}$ 都是 $k$-稀疏且不相同的，因此它们为同一个测量值 $y = A z_{S} = A z_{T}$ 提供了两个不同的 $k$-稀疏解。最小维度源于一般性界限 $\\operatorname{spark}(A) \\leq m + 1$。为了达到 $\\operatorname{spark}(A) = 2k$，需要 $m \\geq 2k - 1$，最小的选择是 $m = 2k - 1$。要使 $\\operatorname{spark}(A) = 2k$ 成为可能，最小的 $n$ 必须满足 $n \\ge 2k$，这样，一个包含 $2k$ 个列的集合才可能线性相关，而任意 $2k - 1$ 个列都保持线性无关，从而实现 $\\operatorname{spark}(A) = 2k$。\n\n对于 $k = 2$ 的具体实例。取 $m = 3$，$n = 4$，并令 $A$ 的列向量为\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix},\\quad\na_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix},\\quad\na_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix},\\quad\na_{4} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\n我们验证 $\\operatorname{spark}(A) = 4$。要使 $\\operatorname{spark}(A)$ 等于 $4$，任何少于 $4$ 个列的集合都必须是线性无关的。包含三个列的子矩阵有：\n- 列 $\\{a_{1}, a_{2}, a_{3}\\}$ 构成 $\\mathbb{R}^{3}$ 中的单位矩阵，因此是线性无关的。\n- 列 $\\{a_{1}, a_{2}, a_{4}\\}$ 给出矩阵\n$$\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n0  0  -1\n\\end{pmatrix},\n$$\n其行列式为 $-1$，因此是线性无关的。\n- 列 $\\{a_{1}, a_{3}, a_{4}\\}$ 给出矩阵\n$$\n\\begin{pmatrix}\n1  0  1 \\\\\n0  0  1 \\\\\n0  1  -1\n\\end{pmatrix},\n$$\n其行列式等于 $-1$，因此是线性无关的。\n- 列 $\\{a_{2}, a_{3}, a_{4}\\}$ 给出矩阵\n$$\n\\begin{pmatrix}\n0  0  1 \\\\\n1  0  1 \\\\\n0  1  -1\n\\end{pmatrix},\n$$\n其行列式等于 $1$，因此是线性无关的。\n由于任意 3 个列的集合都是线性无关的，而这 4 个列组成的集合在 $\\mathbb{R}^3$ 中必然是线性相关的，我们得到 $\\operatorname{spark}(A) = 4 = 2k$。\n\n接着，定义\n$$\nx^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nx^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n$x^{(1)}$ 和 $x^{(2)}$ 都是 $2$-稀疏的。计算测量值：\n$$\nA x^{(1)} = 1 \\cdot a_{1} + 1 \\cdot a_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix},\n$$\n以及\n$$\nA x^{(2)} = 1 \\cdot a_{3} + 1 \\cdot a_{4} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\n因此 $A x^{(1)} = A x^{(2)}$，这证明了在 $\\operatorname{spark}(A) = 2k$ 时解的非唯一性。\n\n最后，计算所要求的标量值：\n$$\n\\|x^{(1)} - x^{(2)}\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right\\|_{2}^{2} = \\left\\| \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{pmatrix} \\right\\|_{2}^{2} = 1^{2} + 1^{2} + (-1)^{2} + (-1)^{2} = 4.\n$$\n这个精确值就是最终答案。",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "既然我们知道唯一的稀疏解可能存在，那么我们该如何找到它呢？“基追踪”（Basis Pursuit）作为一种凸优化问题，为我们提供了寻找最稀疏解的主要方法。本练习将深入探讨拉格朗日对偶这一强大工具，用以分析基追踪问题，这是理解其理论性质和开发高效算法的基石。 ",
            "id": "3460592",
            "problem": "考虑压缩感知中的基础追踪（BP）公式：在满足线性等式约束的条件下最小化 $\\ell_{1}$ 范数，即 $ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} $，约束条件为 $ A x = y $，其中 $ A \\in \\mathbb{R}^{m \\times n} $ 且 $ y \\in \\mathbb{R}^{m} $。从拉格朗日对偶性的基本框架和正常闭凸函数 $ \\varphi $ 的 Fenchel 共轭定义 $ \\varphi^{*}(u) = \\sup_{x} \\{ u^{\\top} x - \\varphi(x) \\} $ 出发，推导出与 BP 对应的对偶问题，并用关于 $\\ell_{1}$ 范数的次梯度最优性来表述互补松弛条件。\n\n然后，对于给定的感知矩阵和数据\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}, \n\\quad\ny = \\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}.\n$$\n将推导过程实例化。\n使用您的对偶公式和推导出的互补松弛条件，计算此 $(A,y)$ 的 BP 最优目标值，结果表示为一个实数。无需四舍五入；请报告精确值。",
            "solution": "用户提供了一个来自压缩感知和凸优化领域的有效问题陈述。我们将进行完整的推导和解答。\n\n该问题要求推导基础追踪（BP）的对偶问题，陈述相应的互补松弛条件，并应用这些工具计算一个特定实例的最优目标值。\n\n原问题，即基础追踪，由下式给出\n$$\n(P) \\quad \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 是优化变量，给定的数据是矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和向量 $y \\in \\mathbb{R}^{m}$。目标函数为 $\\varphi(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$。\n\n为推导对偶问题，我们引入拉格朗日函数。线性等式约束 $Ax=y$ 通过拉格朗日乘子向量 $\\nu \\in \\mathbb{R}^{m}$ 被引入。拉格朗日函数为：\n$$\nL(x, \\nu) = \\|x\\|_{1} + \\nu^{\\top}(Ax - y)\n$$\n拉格朗日对偶函数 $g(\\nu)$ 定义为拉格朗日函数关于原变量 $x$ 的下确界：\n$$\ng(\\nu) = \\inf_{x \\in \\mathbb{R}^{n}} L(x, \\nu) = \\inf_{x \\in \\mathbb{R}^{n}} \\left( \\|x\\|_{1} + \\nu^{\\top}Ax - \\nu^{\\top}y \\right)\n$$\n我们可以重新排列各项，以分离出依赖于 $x$ 的部分：\n$$\ng(\\nu) = \\inf_{x \\in \\mathbb{R}^{n}} \\left( (A^{\\top}\\nu)^{\\top}x + \\|x\\|_{1} \\right) - \\nu^{\\top}y\n$$\n下确界项可以与 $\\ell_{1}$-范数的 Fenchel 共轭联系起来。问题中给出了函数 $\\varphi$ 的 Fenchel 共轭定义为 $\\varphi^{*}(u) = \\sup_{x} \\{ u^{\\top}x - \\varphi(x) \\}$。\n使用此定义，我们可以写出：\n$$\n\\inf_{x \\in \\mathbb{R}^{n}} \\left( (A^{\\top}\\nu)^{\\top}x + \\|x\\|_{1} \\right) = - \\sup_{x \\in \\mathbb{R}^{n}} \\left( (-A^{\\top}\\nu)^{\\top}x - \\|x\\|_{1} \\right) = -\\varphi^{*}(-A^{\\top}\\nu)\n$$\n其中 $\\varphi(x) = \\|x\\|_{1}$。\n\n现在我们必须计算 $\\ell_{1}$-范数的 Fenchel 共轭。令 $u \\in \\mathbb{R}^{n}$。\n$$\n\\varphi^{*}(u) = \\sup_{x \\in \\mathbb{R}^{n}} \\left( u^{\\top}x - \\|x\\|_{1} \\right) = \\sup_{x \\in \\mathbb{R}^{n}} \\sum_{i=1}^{n} (u_i x_i - |x_i|)\n$$\n由于和是可分的，我们可以对每一项独立地求最大值：\n$$\n\\varphi^{*}(u) = \\sum_{i=1}^{n} \\sup_{x_i \\in \\mathbb{R}} (u_i x_i - |x_i|)\n$$\n考虑单项 $\\sup_{x_i} (u_i x_i - |x_i|)$。\n如果 $|u_i| \\le 1$，那么 $u_i x_i \\le |u_i||x_i| \\le |x_i|$。因此，$u_i x_i - |x_i| \\le 0$。上确界为 $0$，在 $x_i=0$ 处取得。\n如果 $|u_i| > 1$，比如说 $u_i > 1$，我们可以选择 $x_i > 0$。那么 $u_i x_i - |x_i| = (u_i - 1)x_i$。当 $x_i \\to \\infty$ 时，该项趋向于 $\\infty$。如果 $u_i  -1$，我们可以选择 $x_i  0$。那么 $u_i x_i - |x_i| = u_i x_i - (-x_i) = (u_i+1)x_i$。当 $x_i \\to -\\infty$ 时，该项也趋向于 $\\infty$。\n因此，上确界是有限的当且仅当对所有 $i=1, \\dots, n$ 都有 $|u_i| \\le 1$。这等价于条件 $\\|u\\|_{\\infty} \\le 1$。共轭函数为：\n$$\n\\varphi^{*}(u) = \\begin{cases} 0  \\text{if } \\|u\\|_{\\infty} \\le 1 \\\\ \\infty  \\text{otherwise} \\end{cases}\n$$\n这是 $\\ell_{\\infty}$-范数单位球的指示函数。\n\n将此结果代回对偶函数 $g(\\nu)$ 的表达式中：\n$$\ng(\\nu) = -\\varphi^{*}(-A^{\\top}\\nu) - \\nu^{\\top}y = \\begin{cases} -y^{\\top}\\nu  \\text{if } \\|-A^{\\top}\\nu\\|_{\\infty} \\le 1 \\\\ -\\infty  \\text{otherwise} \\end{cases}\n$$\n由于 $\\|-\\mathbf{v}\\|_{\\infty} = \\|\\mathbf{v}\\|_{\\infty}$，约束条件为 $\\|A^{\\top}\\nu\\|_{\\infty} \\le 1$。对偶问题是最大化对偶函数 $g(\\nu)$：\n$$\n(D) \\quad \\max_{\\nu \\in \\mathbb{R}^{m}} -y^{\\top}\\nu \\quad \\text{subject to} \\quad \\|A^{\\top}\\nu\\|_{\\infty} \\le 1\n$$\n这就是与基础追踪对应的对偶问题。\n\n接下来，我们推导互补松弛条件。对于像 BP 这样的凸问题，假设原问题是可行的（我们确实如此假设），那么强对偶性成立。设 $x^*$ 是 $(P)$ 的一个最优解，$\\nu^*$ 是 $(D)$ 的一个最优解。强对偶性意味着原问题和对偶问题的最优值相等，即对偶间隙为零：\n$$\n\\|x^*\\|_{1} = -y^{\\top}\\nu^*\n$$\n这个等式源于 $L(x^*, \\nu^*) = g(\\nu^*)$ 这一事实。\n原问题的最优性条件（卡罗需-库恩-塔克，或 KKT）指出，在最优点 $x^*$ 处，必须存在一个对偶变量 $\\nu^*$，使得拉格朗日函数关于 $x$ 的梯度包含零。由于 $\\|x\\|_1$ 不可微，我们使用次梯度的概念：\n$$\n0 \\in \\partial_{x} L(x, \\nu) \\rvert_{x=x^*, \\nu=\\nu^*} = \\partial \\|x^*\\|_{1} + A^{\\top}\\nu^*\n$$\n这意味着 $-A^{\\top}\\nu^* \\in \\partial \\|x^*\\|_{1}$。\n在点 $x$ 处 $\\ell_1$-范数的次梯度是向量 $s \\in \\mathbb{R}^{n}$ 的集合，其分量为：\n$$\ns_i \\in \\begin{cases} \\{\\text{sgn}(x_i)\\}  \\text{if } x_i \\neq 0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\n因此，包含互补松弛的最优性条件通过以下关系将原问题最优解 $x^*$ 和对偶问题最优解 $\\nu^*$ 联系起来：\n1.  原问题可行性：$Ax^* = y$。\n2.  对偶可行性：$\\|A^{\\top}\\nu^*\\|_{\\infty} \\le 1$。\n3.  次梯度条件：令 $s^* = -A^{\\top}\\nu^*$。那么对于每个 $i \\in \\{1, \\dots, n\\}$：\n    - 如果 $x_i^* \\neq 0$，则 $s_i^* = \\text{sgn}(x_i^*)$。这意味着对于 $x^*$ 支撑集中的所有 $i$，有 $|(A^{\\top}\\nu^*)_i| = 1$。\n    - 如果 $x_i^* = 0$，则 $|s_i^*| \\le 1$。由于对偶可行性，此条件自动满足。\n\n现在，我们将问题针对给定的感知矩阵和数据进行实例化：\n$$\nA = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n此处，$n=3$ 且 $m=2$。原变量为 $x = (x_1, x_2, x_3)^{\\top} \\in \\mathbb{R}^3$，对偶变量为 $\\nu = (\\nu_1, \\nu_2)^{\\top} \\in \\mathbb{R}^2$。\n原问题是 $\\min |x_1|+|x_2|+|x_3|$，约束条件为 $x_1+x_3=1$ 和 $x_2+x_3=1$。\n\n将 $A$ 和 $y$ 代入一般对偶形式 $(D)$，得到对偶问题：\n$$\n\\max_{\\nu_1, \\nu_2} -(\\nu_1 + \\nu_2) \\quad \\text{subject to} \\quad \\|A^{\\top}\\nu\\|_{\\infty} \\le 1\n$$\n我们来计算 $A^{\\top}\\nu$：\n$$\nA^{\\top}\\nu = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} \\nu_1 \\\\ \\nu_2 \\end{pmatrix} = \\begin{pmatrix} \\nu_1 \\\\ \\nu_2 \\\\ \\nu_1 + \\nu_2 \\end{pmatrix}\n$$\n约束 $\\|A^{\\top}\\nu\\|_{\\infty} \\le 1$ 转化为以下不等式组：\n1.  $|\\nu_1| \\le 1$\n2.  $|\\nu_2| \\le 1$\n3.  $|\\nu_1 + \\nu_2| \\le 1$\n\n具体的对偶问题是以下线性规划：\n$$\n\\max_{\\nu_1, \\nu_2} -(\\nu_1 + \\nu_2) \\quad \\text{subject to} \\quad -1 \\le \\nu_1 \\le 1, \\quad -1 \\le \\nu_2 \\le 1, \\quad -1 \\le \\nu_1 + \\nu_2 \\le 1\n$$\n根据强对偶性，此问题的最优值等于原 BP 问题的最优目标值。我们想要最大化 $-(\\nu_1+\\nu_2)$，这等价于在可行集上最小化 $\\nu_1+\\nu_2$。在 $(\\nu_1, \\nu_2)$ 平面上的可行域是一个六边形。线性函数在多胞体上的最小值出现在其某个顶点上。该六边形的顶点为 $(1,0)$, $(1,-1)$, $(0,-1)$, $(-1,0)$, $(-1,1)$ 和 $(0,1)$。在这些顶点上计算目标函数 $\\nu_1+\\nu_2$ 的值：\n- 在 $(1,0)$ 处：$\\nu_1+\\nu_2 = 1$\n- 在 $(1,-1)$ 处：$\\nu_1+\\nu_2 = 0$\n- 在 $(0,-1)$ 处：$\\nu_1+\\nu_2 = -1$\n- 在 $(-1,0)$ 处：$\\nu_1+\\nu_2 = -1$\n- 在 $(-1,1)$ 处：$\\nu_1+\\nu_2 = 0$\n- 在 $(0,1)$ 处：$\\nu_1+\\nu_2 = 1$\n\n$\\nu_1+\\nu_2$ 的最小值为 $-1$。因此，$-(\\nu_1+\\nu_2)$ 的最大值为 $1$。BP 问题的最优目标值为 $1$。\n\n为验证这一点并演示互补松弛条件的使用，我们来求解原问题解 $x^*$。对偶最优值在连接 $(0,-1)$ 和 $(-1,0)$ 的线段上的任何点 $(\\nu_1, \\nu_2)$ 处都能达到。我们选择对偶最优解 $\\nu^* = (-1, 0)^{\\top}$。\n现在，我们计算相应的向量 $s^* = -A^{\\top}\\nu^*$：\n$$\ns^* = - \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = - \\begin{pmatrix} -1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n我们对 $x^*$ 应用次梯度最优性条件：\n- $s_1^* = 1$：如果 $x_1^* \\ne 0$，那么 $\\text{sgn}(x_1^*) = 1$，所以 $x_1^* > 0$。如果 $x_1^*=0$，条件 $|s_1^*| \\le 1$ 满足。然而， $|s_1^*|=1$ 表明 $x_1^*$ 可能非零。我们继续。\n- $s_2^* = 0$：由于 $\\text{sgn}(x_2^*)$ 不可能为 $0$，这强制 $x_2^* = 0$。对于 $x_2^*=0$ 的条件是 $|s_2^*|\\le 1$，对于 $s_2^*=0$ 这是成立的。\n- $s_3^* = 1$：如果 $x_3^* \\ne 0$，那么 $\\text{sgn}(x_3^*) = 1$，所以 $x_3^* > 0$。\n\n从 $s_2^*=0$，我们推断出 $x_2^*=0$。现在我们使用原问题可行性约束 $Ax^*=y$：\n1. $x_1^* + x_3^* = 1$\n2. $x_2^* + x_3^* = 1$\n\n将 $x_2^*=0$ 代入第二个方程得到 $0 + x_3^* = 1 \\implies x_3^*=1$。\n将 $x_3^*=1$ 代入第一个方程得到 $x_1^* + 1 = 1 \\implies x_1^*=0$。\n所以，原问题最优解的候选解是 $x^* = (0, 0, 1)^{\\top}$。\n\n我们来检查这个 $x^*$ 和我们选择的 $\\nu^*=(-1,0)^{\\top}$ (以及对应的 $s^*=(1,0,1)^{\\top}$) 是否满足所有最优性条件：\n- 原问题可行性 $Ax^*=y$：$\\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，这正是 $y$。条件成立。\n- 对偶可行性 $\\|A^{\\top}\\nu^*\\|_{\\infty}\\le 1$：$\\|(-1,0,-1)^{\\top}\\|_{\\infty} = 1 \\le 1$。条件成立。\n- 次梯度条件 $-A^{\\top}\\nu^* \\in \\partial\\|x^*\\|_1$：我们有 $s^* = (1,0,1)^{\\top}$ 和 $x^*=(0,0,1)^{\\top}$。我们逐分量检查次梯度条件：\n    - $x_1^*=0$: $|s_1^*| = |1| \\le 1$。成立。\n    - $x_2^*=0$: $|s_2^*| = |0| \\le 1$。成立。\n    - $x_3^*=1 \\ne 0$: $s_3^* = \\text{sgn}(x_3^*) = \\text{sgn}(1) = 1$。成立。\n所有条件都满足，因此 $x^* = (0, 0, 1)^{\\top}$ 确实是最优解。\n\n最后，我们计算最优目标值，即 $x^*$ 的 $\\ell_1$-范数：\n$$\n\\|x^*\\|_{1} = |x_1^*| + |x_2^*| + |x_3^*| = |0| + |0| + |1| = 1\n$$\n这证实了通过求解对偶问题得到的数值 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "现实世界中的测量几乎总是被噪声污染，这使得基追踪中的严格等式约束变得不切实际。LASSO（Least Absolute Shrinkage and Selection Operator）通过在数据保真度和解的稀疏性之间取得平衡，有效地解决了这个问题。本练习将重点推导LASSO问题的基本最优性条件（KKT条件），这些条件对于设计和分析求解LASSO的实用迭代算法至关重要。 ",
            "id": "3460541",
            "problem": "考虑压缩感知背景下的最小绝对收缩和选择算子 (LASSO) 问题，该问题通过平衡数据保真度和一个逐项促进稀疏性的正则化项来寻求稀疏解。设 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，且 $\\lambda  0$。\n\n(a) 形式化地定义LASSO优化问题，即最小化一个二次数据保真度项和一个$\\ell_{1}$范数正则化项的和，并用 $A$、$y$ 和 $\\lambda$ 明确写出其目标函数。\n\n(b) 仅利用以下两个基本事实：(i) 一个真、闭、凸函数的次微分通过费马法则来刻画其一阶最优性，以及 (ii) $\\ell_{1}$范数的次微分是各坐标上绝对值函数次微分的笛卡尔积，来推导LASSO问题的必要且充分的Karush–Kuhn–Tucker (KKT) 最优性条件，该条件应以$\\ell_{1}$范数的次梯度表示。你的推导应该是分量式的，并应明确区分坐标为零和非零的情况。\n\n(c) 将你的结果应用于候选解 $x^{\\star} = 0 \\in \\mathbb{R}^{n}$。推导出一个关于$\\lambda$的必要且充分条件，使得 $x^{\\star} = 0$ 是最优解，该条件仅用 $A$、$y$ 和 $\\lambda$ 表示。然后，对于给定的具体数据\n$$\nA \\,=\\, \\begin{pmatrix}\n1  -1  2  0 \\\\\n0  2  -1  1 \\\\\n1  0  1  -2\n\\end{pmatrix}, \n\\qquad\ny \\,=\\, \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{pmatrix},\n$$\n计算使 $x^{\\star} = 0$ 满足你的KKT条件的最小$\\lambda$值。将你的最终答案表示为一个精确的实数，无需四舍五入。",
            "solution": "(a) 最小绝对收缩和选择算子 (LASSO) 问题旨在寻找一个向量 $x \\in \\mathbb{R}^{n}$，以最小化一个由数据保真度项和正则化项组成的目标函数。数据保真度项衡量模型预测 $Ax$ 与观测数据 $y$ 之间的不匹配程度，通常由残差的平方$\\ell_{2}$范数 $\\|Ax - y\\|_{2}^{2}$ 给出。正则化项促进解向量 $x$ 的稀疏性，由 $x$ 的$\\ell_{1}$范数 $\\|x\\|_{1}$ 给出。参数 $\\lambda  0$ 控制这两项之间的权衡。为了在求导时方便数学计算，通常在数据保真度项前加上一个因子 $\\frac{1}{2}$。\n\n一个向量 $x \\in \\mathbb{R}^{n}$ 的$\\ell_{1}$范数定义为 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。一个向量 $v \\in \\mathbb{R}^{m}$ 的平方$\\ell_{2}$范数是 $\\|v\\|_{2}^{2} = \\sum_{j=1}^{m} v_{j}^{2}$。\n\n因此，我们记为 $J(x)$ 的LASSO目标函数是：\n$$\nJ(x) = \\frac{1}{2} \\|Ax - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nLASSO优化问题被形式化地表述为找到一个向量 $x^{\\star}$ 来求解：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2} \\|Ax - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right)\n$$\n\n(b) 为了推导Karush-Kuhn-Tucker (KKT) 最优性条件，我们使用次微分分析。目标函数 $J(x)$ 是两个凸函数之和：$f(x) = \\frac{1}{2} \\|Ax - y\\|_{2}^{2}$ 和 $g(x) = \\lambda \\|x\\|_{1}$。函数 $f(x)$ 是可微的，而当 $x$ 的任何分量为零时，$g(x)$ 是不可微的。由于 $J(x)$ 是一个真、闭、凸函数，一个点 $x^{\\star}$ 成为最小化点的必要且充分的一阶最优性条件由费马法则给出：\n$$\n0 \\in \\partial J(x^{\\star})\n$$\n其中 $\\partial J(x^{\\star})$ 是 $J$ 在 $x^{\\star}$ 处的次微分。\n\n根据次微分的和法则，当其中一个函数可微时，我们有 $\\partial J(x) = \\nabla f(x) + \\partial g(x)$。\n$f(x)$ 的梯度是：\n$$\n\\nabla f(x) = A^{\\intercal}(Ax - y)\n$$\n$g(x) = \\lambda \\|x\\|_{1}$ 的次微分是 $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$。我们已知$\\ell_{1}$范数的次微分是各坐标上绝对值函数次微分的笛卡尔积。令 $\\phi(t) = |t|$。它的次微分是：\n$$\n\\partial \\phi(t) = \n\\begin{cases} \n\\{ \\text{sgn}(t) \\}  \\text{if } t \\neq 0 \\\\\n[-1, 1]  \\text{if } t = 0 \n\\end{cases}\n$$\n其中 $\\text{sgn}(t)$ 是符号函数，当 $t>0$ 时取值为1，当 $t0$ 时取值为-1。\n所以，次微分 $\\partial \\|x\\|_{1}$ 是向量 $v \\in \\mathbb{R}^{n}$ 的集合，其中每个分量 $v_{i}$ 满足 $v_{i} \\in \\partial |x_{i}|$。\n\n最优性条件 $0 \\in \\partial J(x^{\\star})$ 变为 $0 \\in \\nabla f(x^{\\star}) + \\lambda \\partial \\|x^{\\star}\\|_{1}$。这意味着必须存在一个向量 $s \\in \\partial \\|x^{\\star}\\|_{1}$ 使得：\n$$\n\\nabla f(x^{\\star}) + \\lambda s = 0 \\implies A^{\\intercal}(Ax^{\\star} - y) = -\\lambda s\n$$\n我们现在对 $i = 1, \\dots, n$ 分量式地写出此条件。令 $(v)_{i}$ 表示向量 $v$ 的第 $i$ 个分量。\n$$\n(A^{\\intercal}(Ax^{\\star} - y))_{i} = -\\lambda s_{i}, \\quad \\text{where } s_{i} \\in \\partial |x^{\\star}_{i}|\n$$\n我们对每个分量 $x^{\\star}_{i}$ 区分两种情况：\n1.  如果 $x^{\\star}_{i} \\neq 0$，那么 $\\partial |x^{\\star}_{i}| = \\{ \\text{sgn}(x^{\\star}_{i}) \\}$。条件变为：\n    $$\n    (A^{\\intercal}(Ax^{\\star} - y))_{i} = -\\lambda \\, \\text{sgn}(x^{\\star}_{i})\n    $$\n2.  如果 $x^{\\star}_{i} = 0$，那么 $\\partial |x^{\\star}_{i}| = [-1, 1]$。条件是存在某个 $s_{i} \\in [-1, 1]$ 使得 $(A^{\\intercal}(Ax^{\\star} - y))_{i} = -\\lambda s_{i}$。这等价于：\n    $$\n    \\left| (A^{\\intercal}(Ax^{\\star} - y))_{i} \\right| \\leq \\lambda\n    $$\n这两组条件是LASSO问题的必要且充分的KKT条件。\n\n(c) 我们现在将这些条件应用于候选解 $x^{\\star} = 0 \\in \\mathbb{R}^{n}$。对于这个解，每个分量都为零，因此对所有 $i = 1, \\dots, n$ 都有 $x^{\\star}_{i} = 0$。所以，对所有分量我们都处于第二种情况。\n将 $x^{\\star} = 0$ 代入条件 $| (A^{\\intercal}(Ax^{\\star} - y))_{i} | \\leq \\lambda$ 中：\n$$\n\\left| (A^{\\intercal}(A \\cdot 0 - y))_{i} \\right| \\leq \\lambda\n$$\n$$\n\\left| (A^{\\intercal}(-y))_{i} \\right| \\leq \\lambda\n$$\n$$\n\\left| -(A^{\\intercal}y)_{i} \\right| \\leq \\lambda\n$$\n$$\n\\left| (A^{\\intercal}y)_{i} \\right| \\leq \\lambda\n$$\n这个条件必须对所有 $i = 1, \\dots, n$ 成立。这可以用无穷范数（$\\ell_{\\infty}$-范数）紧凑地表示，即向量各分量绝对值的最大值。$x^{\\star} = 0$ 是最优解的必要且充分条件是：\n$$\n\\|A^{\\intercal}y\\|_{\\infty} \\leq \\lambda\n$$\n由于题目说明 $\\lambda  0$，使 $x^{\\star} = 0$ 成为最优解的最小 $\\lambda$ 值是 $\\lambda_{\\min} = \\|A^{\\intercal}y\\|_{\\infty}$，假设 $A^{\\intercal}y \\neq 0$。\n\n现在，我们为给定的数据计算这个值：\n$$\nA = \\begin{pmatrix}\n1  -1  2  0 \\\\\n0  2  -1  1 \\\\\n1  0  1  -2\n\\end{pmatrix}, \n\\qquad\ny = \\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{pmatrix}\n$$\n首先，我们求 $A$ 的转置：\n$$\nA^{\\intercal} = \\begin{pmatrix}\n1  0  1 \\\\\n-1  2  0 \\\\\n2  -1  1 \\\\\n0  1  -2\n\\end{pmatrix}\n$$\n接下来，我们计算乘积 $A^{\\intercal}y$：\n$$\nA^{\\intercal}y = \\begin{pmatrix}\n1  0  1 \\\\\n-1  2  0 \\\\\n2  -1  1 \\\\\n0  1  -2\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\\n-1 \\\\\n3\n\\end{pmatrix} = \\begin{pmatrix}\n(1)(2) + (0)(-1) + (1)(3) \\\\\n(-1)(2) + (2)(-1) + (0)(3) \\\\\n(2)(2) + (-1)(-1) + (1)(3) \\\\\n(0)(2) + (1)(-1) + (-2)(3)\n\\end{pmatrix} = \\begin{pmatrix}\n2 + 0 + 3 \\\\\n-2 - 2 + 0 \\\\\n4 + 1 + 3 \\\\\n0 - 1 - 6\n\\end{pmatrix} = \\begin{pmatrix}\n5 \\\\\n-4 \\\\\n8 \\\\\n-7\n\\end{pmatrix}\n$$\n最后，我们计算这个向量的无穷范数：\n$$\n\\|A^{\\intercal}y\\|_{\\infty} = \\max \\{ |5|, |-4|, |8|, |-7| \\} = \\max \\{ 5, 4, 8, 7 \\} = 8\n$$\n因此，使 $x^{\\star} = 0$ 成为最优解的最小 $\\lambda$ 值是 $8$。",
            "answer": "$$\n\\boxed{8}\n$$"
        }
    ]
}