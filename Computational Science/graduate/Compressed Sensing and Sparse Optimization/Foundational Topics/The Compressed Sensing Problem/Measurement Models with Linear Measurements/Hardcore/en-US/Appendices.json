{
    "hands_on_practices": [
        {
            "introduction": "Understanding the conditions under which iterative algorithms converge to the correct sparse solution is a cornerstone of compressed sensing theory. This exercise provides a foundational analysis of the Iterative Hard Thresholding (IHT) algorithm, a classic method for sparse recovery. By working through this problem , you will derive a critical parameter—the step size—that guarantees convergence, linking the algorithm's behavior directly to the spectral properties of the measurement matrix.",
            "id": "3459927",
            "problem": "Consider a linear measurement model in compressed sensing of the form $y = A x^{\\star}$, where $A \\in \\mathbb{R}^{m \\times n}$, $x^{\\star} \\in \\mathbb{R}^{n}$ is an unknown $s$-sparse signal, and $y \\in \\mathbb{R}^{m}$ is the measurement vector. Let the objective be the least-squares loss $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, and consider the Iterative Hard Thresholding (IHT) algorithm, defined as $x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$, where $H_{s}(\\cdot)$ denotes hard thresholding to $s$ entries by magnitude, and $\\eta  0$ is a constant step size. Assume that $A$ satisfies a Restricted Isometry Property (RIP) of appropriate order, with restricted isometry constant $\\delta_{k}$ defined by $(1 - \\delta_{k})\\|u\\|_{2}^{2} \\leq \\|A u\\|_{2}^{2} \\leq (1 + \\delta_{k})\\|u\\|_{2}^{2}$ for all vectors $u$ with support size at most $k$. Under suitable RIP conditions ensuring contraction of the IHT error recurrence on the union of supports encountered by the iterates, derive from first principles an explicit upper bound on the allowable constant step size, expressed solely in terms of the spectral norm $\\|A\\|_{2 \\to 2}$, that guarantees convergence of the IHT iterates to the $s$-sparse solution. State your final bound as a single closed-form analytic expression. No numerical rounding is required.",
            "solution": "The Iterative Hard Thresholding (IHT) algorithm aims to find a sparse solution $x$ to the linear system $y = Ax$. The update rule is given by:\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right)$$\nwhere $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ is the least-squares loss function, $\\eta$ is the step size, and $H_s(\\cdot)$ is the hard-thresholding operator. The gradient is $\\nabla f(x) = A^{\\top}(A x - y)$. Substituting this and the fact that $y=Ax^\\star$ into the update gives:\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}A(x^{t} - x^{\\star})\\right)$$\nLet $e^{t} = x^{t} - x^{\\star}$ be the error. The update can be expressed in terms of the error:\n$$x^{t+1} = H_{s}\\!\\left(x^{\\star} + (I - \\eta A^{\\top}A)e^{t}\\right)$$\nConvergence proofs for IHT typically analyze the contraction of the error $e^t$. The error vector $e^t = x^t - x^\\star$ is supported on the union of the supports of $x^t$ and $x^\\star$, which has a size of at most $2s$. Convergence requires that the operator $(I - \\eta A^{\\top}A)$ is a strict contraction when restricted to these sparse subspaces.\nLet $T$ be any index set with $|T| \\leq 2s$. The condition for strict contraction of the operator on vectors supported on $T$ is that the spectral norm of the corresponding matrix block is less than 1:\n$$\\|I_T - \\eta A_T^{\\top}A_T\\|_{2 \\to 2}  1$$\nwhere $A_T$ is the submatrix of $A$ with columns indexed by $T$. The eigenvalues of the symmetric matrix $I_T - \\eta A_T^{\\top}A_T$ are $1 - \\eta \\lambda_i(A_T^{\\top}A_T)$. For the spectral norm to be less than 1, all these eigenvalues must lie in $(-1, 1)$. This requires:\n$$-1  1 - \\eta \\lambda_{\\max}(A_T^{\\top}A_T)$$\nwhich implies $\\eta \\lambda_{\\max}(A_T^{\\top}A_T)  2$. The largest eigenvalue is the squared spectral norm of the submatrix, $\\lambda_{\\max}(A_T^{\\top}A_T) = \\|A_T\\|_{2 \\to 2}^2$. Thus, the condition is:\n$$\\eta  \\frac{2}{\\|A_T\\|_{2 \\to 2}^2}$$\nThis must hold for the worst-case submatrix, i.e., for all $|T| \\le 2s$. The problem asks for a bound in terms of the global spectral norm, $\\|A\\|_{2 \\to 2}$. Since the spectral norm of any submatrix is bounded by the norm of the full matrix, $\\|A_T\\|_{2 \\to 2} \\le \\|A\\|_{2 \\to 2}$, we have $\\|A_T\\|_{2 \\to 2}^2 \\le \\|A\\|_{2 \\to 2}^2$.\nTherefore, a more restrictive but sufficient condition on the step size is:\n$$\\eta  \\frac{2}{\\|A\\|_{2 \\to 2}^2}$$\nThis guarantees contraction on all relevant sparse subspaces, leading to convergence. The upper bound on the allowable constant step size is therefore the value on the right-hand side of this inequality.",
            "answer": "$$\n\\boxed{\\frac{2}{\\|A\\|_{2 \\to 2}^{2}}}\n$$"
        },
        {
            "introduction": "Beyond simply solving for a sparse signal given a set of measurements, a crucial aspect of compressed sensing involves designing the measurement process itself. This practice shifts the focus to optimal experiment design, where the goal is to choose measurement vectors to maximize the information captured about an unknown sparse state. In this hands-on coding exercise , you will implement a greedy sensor selection algorithm based on the principle of D-optimality, providing a practical demonstration of how to strategically design a linear measurement system for a state sparse in a wavelet basis.",
            "id": "3459917",
            "problem": "Consider a one-dimensional discretized state associated with a Partial Differential Equation (PDE) on a uniform spatial grid. Let the discretized state be represented by the vector $x \\in \\mathbb{R}^n$. Assume a linear measurement model with pointwise sensors defined by $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensor placement matrix with exactly one nonzero per row selecting $m$ distinct entries of $x$. Assume that $x$ admits a sparse representation in an orthonormal Haar wavelet basis $W \\in \\mathbb{R}^{n \\times n}$, with $x = W^\\top \\alpha$, and $\\alpha \\in \\mathbb{R}^n$ being $K$-sparse with support $S \\subset \\{0,1,\\dots,n-1\\}$, where $|S| = K$. The Haar wavelet coefficient ordering is defined as follows: index $0$ corresponds to the final scaling coefficient, followed by detail coefficients from the coarsest scale to the finest scale, i.e., $[c_{\\text{final}}, d_{\\text{coarsest}}, \\dots, d_{\\text{finest}}]$; for $n = 2^L$, there are $L$ detail levels with lengths $1,2,4,\\dots,2^{L-1}$.\n\nStarting from the measurement model and the sparsity model, formulate a principled greedy sensor selection strategy grounded in the information-theoretic optimal design for linear models. In particular, for a fixed support $S$, let $M_S = A W^\\top[:, S] \\in \\mathbb{R}^{m \\times K}$ denote the submatrix mapping $\\alpha_S \\in \\mathbb{R}^K$ to $y$. Consider additive white Gaussian noise (AWGN) with variance $\\sigma^2$ and recall that under a linear Gaussian model the Fisher information about $\\alpha_S$ is proportional to $M_S^\\top M_S$. To ensure numerical stability, use a regularized information matrix $G = M_S^\\top M_S + \\lambda I_K$ with $\\lambda  0$ and $I_K$ the $K \\times K$ identity matrix. The greedy selection should build the sensor index set row by row to maximize the determinant of $G$ (determinant optimality, also called $D$-optimality), which reduces the estimator covariance volume for $\\alpha_S$ under AWGN.\n\nYour task is to implement the following components in a single program:\n- Construct the orthonormal Haar wavelet transform matrix $W \\in \\mathbb{R}^{n \\times n}$ consistent with the coefficient ordering described above, by applying the transform to each standard basis vector to form the columns of $W$.\n- For a given support set $S$, implement a greedy $D$-optimal sensor selection algorithm that chooses $m$ distinct sensor locations (row indices of $A$) to maximize $\\log \\det(G)$, where $G = M_S^\\top M_S + \\lambda I_K$ after selecting $m$ sensors.\n- After selecting sensors, compute the minimum singular value $\\sigma_{\\min}(M_S)$ using the Singular Value Decomposition (SVD) and report it.\n- Simulate a $K$-sparse coefficient vector $\\alpha$ supported on $S$ with nonzero entries drawn from a standard normal distribution, form $x = W^\\top \\alpha$, compute measurements $y = A x$, and estimate $\\alpha_S$ via Least Squares (LS) by solving $\\min_{\\hat{\\alpha}_S} \\|M_S \\hat{\\alpha}_S - y\\|_2$. Report the relative reconstruction error $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$.\n- Report also the final value of $\\log \\det(G)$ after greedy selection.\n\nDesign choices must be justified from first principles related to compressed sensing and linear measurement models. No shortcut formulas may be introduced in the problem statement; the objective definitions above are the only permitted explicit targets.\n\nUse the following test suite of parameter values. In each case, $A$ selects rows specified by the greedy algorithm, $W$ is the Haar matrix of size $n$, and the support $S$ is given explicitly:\n- Case $1$: $n = 16$, $K = 3$, $m = 6$, $S = \\{1,5,10\\}$, $\\lambda = 10^{-6}$, random seed $123$.\n- Case $2$: $n = 16$, $K = 4$, $m = 4$ (boundary where $m = K$), $S = \\{2,4,9,12\\}$, $\\lambda = 10^{-6}$, random seed $7$.\n- Case $3$: $n = 32$, $K = 4$, $m = 8$, $S = \\{1,3,5,20\\}$, $\\lambda = 10^{-6}$, random seed $42$.\n\nFor each case, your program must output a list containing:\n- The selected sensor indices as an integer list in ascending order.\n- The minimum singular value $\\sigma_{\\min}(M_S)$ as a float.\n- The relative reconstruction error as a float.\n- The final $\\log \\det(G)$ value as a float.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-case result is formatted as its own bracketed comma-separated list with no spaces. For example, the output must be of the form $[ [\\text{case1}], [\\text{case2}], [\\text{case3}] ]$, specifically as $[[\\text{indices},\\sigma_{\\min},\\text{error},\\log\\det],\\dots]$ with no spaces anywhere in the line.",
            "solution": "The solution involves three main steps: constructing the Haar wavelet basis, implementing the greedy sensor selection algorithm, and finally, performing reconstruction to evaluate performance.\n\nFirst, the orthonormal Haar wavelet transform matrix $W \\in \\mathbb{R}^{n \\times n}$ is constructed. The problem specifies that the columns of $W$ are the Haar transforms of the standard basis vectors, which is equivalent to saying $W$ is the matrix representation of the transform itself. For an orthonormal transform, $W^{-1}=W^\\top$, so the synthesis model $x = W^\\top \\alpha$ corresponds to the analysis transform $\\alpha = Wx$. The Haar transform is implemented by iteratively computing scaled averages and differences of vector pairs, starting from the finest level and proceeding to the coarsest.\n\nSecond, a greedy sensor selection algorithm based on D-optimality is implemented. The measurement model is $y=Ax$, which becomes $y = (A W^\\top) \\alpha$. For a known $K$-sparse support $S$, this simplifies to $y = (A W^\\top[:,S]) \\alpha_S = M_S \\alpha_S$. The objective is to select $m$ rows for the matrix $A$ (corresponding to sensor locations) to maximize the determinant of the regularized Fisher information matrix, $G = M_S^\\top M_S + \\lambda I_K$. The algorithm proceeds iteratively: starting with an empty set of sensors, at each step it adds the sensor location that provides the largest marginal increase in $\\log \\det(G)$. For each candidate sensor location $p$, a new row vector $v_p = (W^\\top[:,S])_{p,:}$ is considered. The new information matrix would be $G_{new} = G_{current} + v_p^\\top v_p$. The location $p$ that maximizes $\\log\\det(G_{new})$ is chosen and added to the sensor set. This process is repeated $m$ times.\n\nThird, after the $m$ sensors are selected, the final matrix $M_S \\in \\mathbb{R}^{m \\times K}$ is formed. A ground truth signal is synthesized by creating a sparse coefficient vector $\\alpha_S$ with entries drawn from a standard normal distribution, and forming the physical state $x=W^\\top\\alpha$. Measurements are taken as $y=Ax$. The sparse coefficients are then estimated by solving the least-squares problem $\\hat{\\alpha}_S = \\arg\\min_{\\beta} \\| M_S \\beta - y \\|_2$. The solution is found using a standard LS solver. Performance is evaluated by computing the relative reconstruction error, $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$. The minimum singular value of $M_S$ and the final $\\log \\det(G)$ value are also reported as measures of the quality of the selected measurement design.",
            "answer": "```python\nimport numpy as np\n\ndef _haar_1d_transform(v):\n    \"\"\"\n    Computes the 1D orthonormal Haar wavelet transform of a vector.\n    The coefficient ordering is: final scaling coefficient, followed by\n    detail coefficients from the coarsest to the finest scale.\n    \"\"\"\n    n = len(v)\n    if n == 1:\n        return v.astype(np.float64)\n    \n    if n  (n - 1) != 0 or n == 0:\n        raise ValueError(\"Input vector length must be a power of 2.\")\n\n    temp_v = v.astype(np.float64)\n    coeffs = np.zeros(n, dtype=np.float64)\n    \n    current_len = n\n    while current_len  1:\n        next_len = current_len // 2\n        averages = (temp_v[0:current_len:2] + temp_v[1:current_len:2]) / np.sqrt(2)\n        details = (temp_v[0:current_len:2] - temp_v[1:current_len:2]) / np.sqrt(2)\n        \n        # Place details in the second half of the current segment in the output array.\n        # This naturally orders them from finest to coarsest as current_len decreases.\n        # Here we place them to match the problem statement.\n        coeffs[next_len:current_len] = details\n        temp_v[:next_len] = averages\n        current_len = next_len\n\n    coeffs[0] = temp_v[0]\n    return coeffs\n\ndef construct_haar_matrix(n):\n    \"\"\"\n    Constructs the orthonormal Haar wavelet transform matrix W of size n x n.\n    The columns of W are the transforms of the standard basis vectors.\n    \"\"\"\n    W = np.zeros((n, n), dtype=np.float64)\n    for i in range(n):\n        e_i = np.zeros(n)\n        e_i[i] = 1.0\n        W[:, i] = _haar_1d_transform(e_i)\n    return W\n\ndef solve():\n    \"\"\"\n    Main function to solve the sensor selection problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 16, 'K': 3, 'm': 6, 'S': {1, 5, 10}, 'lambda': 1e-6, 'seed': 123},\n        {'n': 16, 'K': 4, 'm': 4, 'S': {2, 4, 9, 12}, 'lambda': 1e-6, 'seed': 7},\n        {'n': 32, 'K': 4, 'm': 8, 'S': {1, 3, 5, 20}, 'lambda': 1e-6, 'seed': 42},\n    ]\n\n    results_for_all_cases = []\n\n    for case in test_cases:\n        n, K, m, S_set, lambda_reg, seed = case['n'], case['K'], case['m'], case['S'], case['lambda'], case['seed']\n        S = list(S_set)\n\n        # 1. Construct Haar wavelet matrix W\n        W = construct_haar_matrix(n)\n        Psi_S = W.T[:, S]\n\n        # 2. Greedy D-optimal sensor selection\n        selected_indices = []\n        available_indices = list(range(n))\n        \n        # Initialize regularized information matrix G\n        G = lambda_reg * np.eye(K)\n        \n        current_log_det = np.linalg.slogdet(G)[1]\n\n        for _ in range(m):\n            best_p = -1\n            best_log_det = -np.inf\n            \n            for p in available_indices:\n                v_p = Psi_S[p, :]\n                \n                # Rank-1 update to G\n                G_candidate = G + np.outer(v_p, v_p)\n                \n                # Using slogdet for numerical stability\n                sign, log_det_candidate = np.linalg.slogdet(G_candidate)\n                \n                if sign  0 and log_det_candidate  best_log_det:\n                    best_log_det = log_det_candidate\n                    best_p = p\n\n            selected_indices.append(best_p)\n            available_indices.remove(best_p)\n            \n            # Update G and log_det for the next iteration\n            v_best = Psi_S[best_p, :]\n            G += np.outer(v_best, v_best)\n            current_log_det = best_log_det\n        \n        final_log_det = current_log_det\n        selected_indices.sort()\n\n        # 3. Construct final M_S and compute minimum singular value\n        M_S = Psi_S[selected_indices, :]\n        if M_S.shape[0]  0:\n            singular_values = np.linalg.svd(M_S, compute_uv=False)\n            sigma_min = singular_values.min()\n        else:\n            sigma_min = 0.0\n\n        # 4. Simulate and reconstruct\n        np.random.seed(seed)\n        alpha = np.zeros(n)\n        alpha_S_true = np.random.randn(K)\n        alpha[S] = alpha_S_true\n        \n        x = W.T @ alpha\n        \n        # Measurement matrix A is implicit\n        y = x[selected_indices]\n        \n        # Estimate alpha_S via Least Squares\n        alpha_S_hat, _, _, _ = np.linalg.lstsq(M_S, y, rcond=None)\n        \n        # Compute relative reconstruction error\n        norm_true = np.linalg.norm(alpha_S_true)\n        if norm_true == 0:\n            error = 0.0 if np.linalg.norm(alpha_S_hat) == 0 else np.inf\n        else:\n            error = np.linalg.norm(alpha_S_hat - alpha_S_true) / norm_true\n\n        # Format results for the current case\n        indices_str = f\"[{','.join(map(str, selected_indices))}]\"\n        case_result_str = f\"[{indices_str},{sigma_min},{error},{final_log_det}]\"\n        results_for_all_cases.append(case_result_str)\n\n    # Final print statement in the exact required format with no spaces\n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many real-world problems involve recovering multiple signals that are not only individually sparse but also share a common underlying structure. This exercise introduces the multi-task learning framework, where the goal is to leverage this shared structure to improve recovery accuracy. By implementing and comparing two different estimators , you will explore the power of mixed-norm regularization (specifically the $\\ell_{2,1}$ norm) to enforce joint sparsity, and you will gain practical experience with the proximal gradient methods used to solve such structured optimization problems.",
            "id": "3459923",
            "problem": "Consider the multi-task linear measurement model in compressed sensing and sparse optimization with joint sparsity across tasks. Let $A \\in \\mathbb{R}^{m \\times n}$ denote the measurement matrix, let $X \\in \\mathbb{R}^{n \\times T}$ denote the unknown coefficient matrix whose columns correspond to $T$ tasks, and let $Y \\in \\mathbb{R}^{m \\times T}$ denote the observed measurements satisfying the linear model $Y = A X + W$. Assume that the columns of $X$ share a common support: the set of indices of rows of $X$ that are nonzero is the same across all columns. The noise matrix $W$ has columns $w_t \\in \\mathbb{R}^{m}$, $t \\in \\{1,\\dots,T\\}$, where each $w_t$ is zero-mean Gaussian with column-dependent covariance $\\Sigma_t \\in \\mathbb{R}^{m \\times m}$ that is positive definite, i.e., $w_t \\sim \\mathcal{N}(0, \\Sigma_t)$, and $\\Sigma_t \\neq \\Sigma_{t'}$ in general.\n\nStart from the following foundational base:\n- The negative log-likelihood for independent Gaussian noise with covariance $\\Sigma_t$ is, up to additive constants independent of $X$, proportional to $\\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2$, where $x_t$ is the $t$-th column of $X$ and $y_t$ is the $t$-th column of $Y$.\n- Joint sparsity across tasks is promoted by mixed norms that aggregate row-wise magnitudes, such as the mixed $\\ell_{p,q}$ norms.\n\nYou must derive, implement, and compare two estimators for $X$ that differ in the mixed-norm regularizer:\n- Estimator $1$ uses the mixed $\\ell_{2,1}$ norm, defined as $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2$, where $X_{i,:}$ denotes the $i$-th row of $X$.\n- Estimator $2$ uses the mixed $\\ell_{2,2}$ norm, defined as $\\|X\\|_{2,2} = \\left( \\sum_{i=1}^{n} \\|X_{i,:}\\|_2^2 \\right)^{1/2}$, which is proportional to the Frobenius norm and does not impose sparsity.\n\nYour program must:\n- Derive algorithmic steps from the base principles above to compute an $\\ell_{2,1}$-regularized estimate by solving $\\min_{X \\in \\mathbb{R}^{n \\times T}} \\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\lambda_{2,1} \\|X\\|_{2,1}$, using a principled first-order method with a correct proximal operator for the mixed $\\ell_{2,1}$ norm and a step size justified by the Lipschitz continuity of the gradient of the smooth term.\n- Derive algorithmic steps from the base principles above to compute an $\\ell_{2,2}$-regularized estimate by solving $\\min_{X \\in \\mathbb{R}^{n \\times T}} \\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\frac{\\lambda_{2,2}}{2} \\|X\\|_{F}^2$, where $\\|X\\|_{F}$ denotes the Frobenius norm, using a correct linear algebraic solution justified by optimality conditions.\n\nJoint support recovery evaluation must be performed by identifying the support as the set of row indices whose row-wise $\\ell_2$ norm across tasks is among the largest $s$ values, where $s$ is the true sparsity level (the number of nonzero rows in the ground-truth $X$). For $s = 0$, the true support is empty and the recovered support is empty if all row norms are below any positive threshold; in this case, define the recovered support as the empty set. For each estimator, declare support recovery success if and only if the recovered support set matches the true support set exactly.\n\nThe test suite consists of three synthetic cases with scientifically consistent and reproducible parameters. In all cases, construct $A$ with independent standard normal entries and normalize its columns to unit $\\ell_2$ norm. Construct $X$ with a common random support of cardinality $s$ and nonzero rows drawn independently across tasks with amplitude specified below. For each task $t$, generate $\\Sigma_t$ as diagonal with entries sampled uniformly from the specified range, and generate $w_t$ as Gaussian with covariance $\\Sigma_t$. Let $Y = A X + W$.\n\nTest case $1$ (happy path):\n- Dimensions: $m = 40$, $n = 80$, $T = 3$.\n- Sparsity: $s = 6$.\n- Amplitude of nonzero rows: $1.5$.\n- Noise covariance ranges per task: diagonal entries in $[0.3, 1.2]$.\n- Regularization parameters: $\\lambda_{2,1} = 0.25$, $\\lambda_{2,2} = 0.05$.\n- Random seed: $123456$.\n\nTest case $2$ (boundary case $s = 0$):\n- Dimensions: $m = 50$, $n = 70$, $T = 2$.\n- Sparsity: $s = 0$.\n- Amplitude of nonzero rows: $0.0$.\n- Noise covariance ranges per task: diagonal entries in $[0.5, 1.0]$.\n- Regularization parameters: $\\lambda_{2,1} = 0.30$, $\\lambda_{2,2} = 0.10$.\n- Random seed: $123457$.\n\nTest case $3$ (edge case with strongly anisotropic, column-dependent noise):\n- Dimensions: $m = 30$, $n = 90$, $T = 4$.\n- Sparsity: $s = 5$.\n- Amplitude of nonzero rows: $1.2$.\n- Noise covariance ranges per task: diagonal entries in $[0.1, 3.0]$.\n- Regularization parameters: $\\lambda_{2,1} = 0.20$, $\\lambda_{2,2} = 0.05$.\n- Random seed: $123458$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to a test case and must be a two-element list of booleans $[b_{2,1}, b_{2,2}]$, where $b_{2,1}$ is $true$ if the $\\ell_{2,1}$ estimator recovers the exact support and $false$ otherwise, and $b_{2,2}$ is $true$ if the $\\ell_{2,2}$ estimator recovers the exact support and $false$ otherwise. For example, an output might look like $[[true,false],[true,true],[false,false]]$. Note that your program must not print any additional text beyond this single line.",
            "solution": "The problem requires deriving and implementing two estimators for the multi-task linear model $Y = AX + W$, where the noise $W$ has task-dependent covariance.\n\n**Estimator 1: $\\ell_{2,1}$-Regularized Estimation**\nThe first estimator solves the optimization problem:\n$$ \\min_{X} \\frac{1}{2} \\sum_{t=1}^{T} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\lambda_{2,1} \\|X\\|_{2,1} $$\nThis problem has the composite structure of a smooth, convex data fidelity term, $f(X)$, and a non-smooth, convex regularizer, $g(X) = \\lambda_{2,1} \\|X\\|_{2,1}$. It is solved using a proximal gradient method (ISTA), which follows the update rule $X^{(k+1)} = \\text{prox}_{\\alpha g}(X^{(k)} - \\alpha \\nabla f(X^{(k)}))$.\nThe gradient of the smooth term is a matrix whose $t$-th column is $\\nabla_{x_t} f(X) = A^T \\Sigma_t^{-1} (A x_t - y_t)$.\nFor convergence, the step size $\\alpha$ must be less than or equal to the reciprocal of the Lipschitz constant $L$ of the gradient. $L$ is given by the maximum spectral norm of the Hessians for each task: $L = \\max_t \\lambda_{\\max}(A^T \\Sigma_t^{-1} A) = \\max_t \\sigma_{\\max}(\\Sigma_t^{-1/2} A)^2$.\nThe proximal operator for the $\\ell_{2,1}$ norm, $\\text{prox}_{\\gamma \\|\\cdot\\|_{2,1}}(Z)$ with $\\gamma = \\alpha \\lambda_{2,1}$, is known as block soft-thresholding. It operates row-wise on the input matrix $Z$:\n$$ (\\text{prox}_{\\gamma \\|\\cdot\\|_{2,1}}(Z))_{i,:} = \\max\\left(0, 1 - \\frac{\\gamma}{\\|Z_{i,:}\\|_2}\\right) Z_{i,:} $$\nThis operation shrinks rows towards zero and sets rows with a small $\\ell_2$ norm to zero, thus enforcing the desired joint sparsity.\n\n**Estimator 2: $\\ell_{2,2}$-Regularized Estimation**\nThe second estimator solves the multi-task ridge regression problem:\n$$ \\min_{X} \\frac{1}{2} \\sum_{t=1}^{T} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\frac{\\lambda_{2,2}}{2} \\|X\\|_{F}^2 $$\nThis objective is separable across tasks. For each task $t$, we solve an independent ridge regression problem by setting the gradient with respect to $x_t$ to zero:\n$$ A^T \\Sigma_t^{-1} (A x_t - y_t) + \\lambda_{2,2} x_t = 0 $$\nThis yields a closed-form solution for each column of $X$ by solving the linear system:\n$$ \\hat{x}_t = (A^T \\Sigma_t^{-1} A + \\lambda_{2,2} I_n)^{-1} (A^T \\Sigma_t^{-1} y_t) $$\nThe final estimate $\\hat{X}_{2,2}$ is formed by concatenating these column vectors. This estimator does not promote sparsity.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(m, n, T, s, amplitude, cov_range, seed):\n    \"\"\"Generates synthetic data for one test case.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate measurement matrix A\n    A = rng.standard_normal((m, n))\n    A_norms = np.linalg.norm(A, axis=0)\n    A /= A_norms[np.newaxis, :]\n    \n    # Generate true sparse matrix X\n    X_true = np.zeros((n, T))\n    true_support = set()\n    if s  0:\n        support_indices = rng.choice(n, s, replace=False)\n        true_support = set(support_indices)\n        for i in support_indices:\n            # Nonzero entries are amplitude * random sign, independent across tasks\n            X_true[i, :] = amplitude * rng.choice([-1, 1], size=T)\n            \n    # Generate noise W and observations Y\n    Y = np.zeros((m, T))\n    all_Sigma = []\n    for t in range(T):\n        # Diagonal covariance matrix for task t\n        sigma_diag = rng.uniform(cov_range[0], cov_range[1], size=m)\n        Sigma_t = np.diag(sigma_diag)\n        all_Sigma.append(Sigma_t)\n        \n        # Generate noise w_t ~ N(0, Sigma_t)\n        # w_t = Sigma_t^{1/2} * z, where z ~ N(0, I)\n        z = rng.standard_normal(m)\n        w_t = np.sqrt(sigma_diag) * z\n        Y[:, t] = A @ X_true[:, t] + w_t\n        \n    return A, X_true, Y, all_Sigma, true_support\n\ndef estimate_l21(A, Y, all_Sigma, lambda_21, num_iter=2000):\n    \"\"\"Solves the l2,1-regularized problem using ISTA.\"\"\"\n    m, n = A.shape\n    _, T = Y.shape\n    \n    # Calculate Lipschitz constant L\n    L = 0.0\n    for t in range(T):\n        Sigma_t = all_Sigma[t]\n        sigma_inv_sqrt_diag = 1.0 / np.sqrt(np.diag(Sigma_t))\n        A_tilde_t = sigma_inv_sqrt_diag[:, np.newaxis] * A\n        # spectral norm squared of A_tilde_t\n        s_vals_sq = np.linalg.svd(A_tilde_t, compute_uv=False)**2\n        L = max(L, np.max(s_vals_sq))\n    \n    alpha = 1.0 / L\n    \n    X_est = np.zeros((n, T))\n    \n    for _ in range(num_iter):\n        # Gradient computation\n        grad_f = np.zeros((n, T))\n        for t in range(T):\n            Sigma_inv_diag = 1.0 / np.diag(all_Sigma[t])\n            residual_t = A @ X_est[:, t] - Y[:, t]\n            grad_f[:, t] = A.T @ (Sigma_inv_diag * residual_t)\n        \n        # Gradient descent step\n        Z = X_est - alpha * grad_f\n        \n        # Proximal step (block soft-thresholding)\n        row_norms = np.linalg.norm(Z, axis=1)\n        threshold = alpha * lambda_21\n        \n        # Avoid division by zero\n        shrinkage_factors = np.zeros_like(row_norms)\n        non_zero_norm_indices = row_norms  0\n        \n        shrinkage_factors[non_zero_norm_indices] = np.maximum(\n            0, 1 - threshold / row_norms[non_zero_norm_indices]\n        )\n        \n        X_est = Z * shrinkage_factors[:, np.newaxis]\n        \n    return X_est\n\ndef estimate_l22(A, Y, all_Sigma, lambda_22):\n    \"\"\"Solves the l2,2-regularized problem (ridge).\"\"\"\n    n, T = Y.shape[1], Y.shape[1]\n    m, n = A.shape\n\n    X_est = np.zeros((n, T))\n    I_n = np.eye(n)\n    \n    for t in range(T):\n        Sigma_inv_diag = 1.0 / np.diag(all_Sigma[t])\n        A_T_Sigma_inv = A.T * Sigma_inv_diag[np.newaxis, :]\n        \n        M_t = A_T_Sigma_inv @ A + lambda_22 * I_n\n        b_t = A_T_Sigma_inv @ Y[:, t]\n        \n        X_est[:, t] = np.linalg.solve(M_t, b_t)\n        \n    return X_est\n\ndef evaluate_support(X_est, true_support, s):\n    \"\"\"Evaluates support recovery success.\"\"\"\n    if s == 0:\n        # Per problem spec, recovered support for s=0 is the empty set\n        recovered_support = set()\n    else:\n        row_norms = np.linalg.norm(X_est, axis=1)\n        # Indices of s largest norms\n        recovered_support = set(np.argsort(row_norms)[-s:])\n        \n    return recovered_support == true_support\n\ndef solve():\n    test_cases = [\n        {\n            \"m\": 40, \"n\": 80, \"T\": 3, \"s\": 6, \"amp\": 1.5,\n            \"cov_range\": [0.3, 1.2], \"lambda_21\": 0.25, \"lambda_22\": 0.05,\n            \"seed\": 123456\n        },\n        {\n            \"m\": 50, \"n\": 70, \"T\": 2, \"s\": 0, \"amp\": 0.0,\n            \"cov_range\": [0.5, 1.0], \"lambda_21\": 0.30, \"lambda_22\": 0.10,\n            \"seed\": 123457\n        },\n        {\n            \"m\": 30, \"n\": 90, \"T\": 4, \"s\": 5, \"amp\": 1.2,\n            \"cov_range\": [0.1, 3.0], \"lambda_21\": 0.20, \"lambda_22\": 0.05,\n            \"seed\": 123458\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, X_true, Y, all_Sigma, true_support = generate_data(\n            case[\"m\"], case[\"n\"], case[\"T\"], case[\"s\"], case[\"amp\"],\n            case[\"cov_range\"], case[\"seed\"]\n        )\n        \n        # Estimator 1: l2,1 regularization\n        X_est_21 = estimate_l21(A, Y, all_Sigma, case[\"lambda_21\"])\n        success_21 = evaluate_support(X_est_21, true_support, case[\"s\"])\n        \n        # Estimator 2: l2,2 regularization\n        X_est_22 = estimate_l22(A, Y, all_Sigma, case[\"lambda_22\"])\n        success_22 = evaluate_support(X_est_22, true_support, case[\"s\"])\n        \n        results.append([success_21, success_22])\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"[{str(r[0]).lower()},{str(r[1]).lower()}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}