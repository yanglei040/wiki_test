{
    "hands_on_practices": [
        {
            "introduction": "对于像迭代硬阈值（IHT）这样的迭代算法，保证其收敛性至关重要。本练习将引导你通过理论推导，找出一个确保算法收敛的步长上限。这个过程不仅能加深你对算法收敛条件的理解，还能揭示算法行为与测量矩阵属性（特别是其谱范数 $\\|A\\|_{2 \\to 2}$）之间的深刻联系。",
            "id": "3459927",
            "problem": "考虑压缩感知中的一个线性测量模型，其形式为 $y = A x^{\\star}$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x^{\\star} \\in \\mathbb{R}^{n}$ 是一个未知的 $s$-稀疏信号，$y \\in \\mathbb{R}^{m}$ 是测量向量。设目标函数为最小二乘损失 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$，并考虑迭代硬阈值 (IHT) 算法，其定义为 $x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$，其中 $H_{s}(\\cdot)$ 表示按幅值大小对 $s$ 个元素进行硬阈值处理，$\\eta  0$ 是一个恒定步长。假设矩阵 $A$ 满足适当阶数的限制等距性质 (RIP)，其限制等距常数 $\\delta_{k}$ 定义为 $(1 - \\delta_{k})\\|u\\|_{2}^{2} \\leq \\|A u\\|_{2}^{2} \\leq (1 + \\delta_{k})\\|u\\|_{2}^{2}$ 对所有支撑集大小至多为 $k$ 的向量 $u$ 成立。在确保 IHT 误差递推在迭代过程中所遇到的支撑集并集上收缩的适当 RIP 条件下，从第一性原理出发，推导允许的恒定步长的一个显式上界，该上界仅用谱范数 $\\|A\\|_{2 \\to 2}$ 表示，并保证 IHT 迭代收敛到 $s$-稀疏解。请将你的最终界限表述为单个闭式解析表达式。无需进行数值取整。",
            "solution": "迭代硬阈值 (IHT) 算法旨在找到线性系统 $y = Ax$ 的一个稀疏解 $x$。其更新规则如下：\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right)$$\n其中 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 是最小二乘损失函数，$\\eta$ 是步长，$H_s(\\cdot)$ 是将向量投影到 $s$-稀疏向量集合上的硬阈值算子。\n\n损失函数的梯度是 $\\nabla f(x) = A^{\\top}(A x - y)$。将此代入 IHT 更新规则中得到：\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$$\n我们已知 $y = A x^{\\star}$，其中 $x^{\\star}$ 是我们希望恢复的真实 $s$-稀疏信号。将此代入更新规则：\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - A x^{\\star})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}A(x^{t} - x^{\\star})\\right)$$\n令 $e^{t} = x^{t} - x^{\\star}$ 为第 $t$ 次迭代时的误差。我们可以用误差和真实信号 $x^{\\star}$ 来重写硬阈值算子的参数：\n$$x^{t+1} = H_{s}\\!\\left(x^{\\star} + e^{t} - \\eta A^{\\top}A e^{t}\\right) = H_{s}\\!\\left(x^{\\star} + (I - \\eta A^{\\top}A)e^{t}\\right)$$\nIHT 的收敛性证明分析了误差范数 $\\|e^{t+1}\\|_2 = \\|x^{t+1}-x^\\star\\|_2$ 的演化。这些证明的核心是迭代在受限子空间上的行为。误差向量 $e^t = x^t - x^\\star$ 的支撑集是 $x^t$ 和 $x^\\star$ 支撑集的并集，即 $\\text{supp}(x^t) \\cup \\text{supp}(x^\\star)$。由于 $x^t$（根据构造）和 $x^\\star$（根据假设）都是 $s$-稀疏的，因此 $e^t$ 的支撑集大小至多为 $2s$。\n\n问题陈述指示我们假设适当的 RIP 条件成立以确保收缩。这使我们能够专注于收敛的主要驱动力，即使得步长中的梯度下降部分，即 $z \\mapsto (I - \\eta A^{\\top}A)z$，在相关的稀疏子空间上是收缩的。\n令 $T$ 是一个索引集，其大小 $|T| \\leq 2s$。令 $z$ 是一个支撑在 $T$ 上的向量。梯度更新对 $z$ 的作用对应于乘以矩阵 $(I - \\eta A_T^{\\top}A_T)$，其中 $A_T$ 是 $A$ 中仅包含由 $T$ 索引的列所构成的子矩阵。为了使整个算法收敛，一个关键要求是该算子在支撑于 $T$ 上的向量空间上是一个严格收缩。\n严格收缩的条件是该矩阵的谱范数（算子 $2$-范数）小于 $1$：\n$\\|I - \\eta A_T^{\\top}A_T\\|_{2 \\to 2}  1$\n矩阵 $A_T^{\\top}A_T$ 是对称半正定的。设其特征值为 $\\lambda_i(A_T^{\\top}A_T)$。那么 $I - \\eta A_T^{\\top}A_T$ 的特征值为 $1 - \\eta \\lambda_i(A_T^{\\top}A_T)$。谱范数是这些特征值绝对值的最大值。\n$\\max_{i} |1 - \\eta \\lambda_i(A_T^{\\top}A_T)|  1$\n这个不等式等价于以下两个条件对所有特征值 $\\lambda_i = \\lambda_i(A_T^{\\top}A_T)$ 都成立：\n$-1  1 - \\eta \\lambda_i  1$\n右侧不等式 $1 - \\eta \\lambda_i  1$ 意味着 $-\\eta \\lambda_i  0$，这总是成立的，因为 $\\eta > 0$ 且 $\\lambda_i \\ge 0$。\n左侧不等式 $-1  1 - \\eta \\lambda_i$ 意味着 $\\eta \\lambda_i  2$。\n这个条件必须对所有特征值成立，包括最大的特征值 $\\lambda_{\\max}(A_T^{\\top}A_T)$。因此，我们需要：\n$\\eta \\lambda_{\\max}(A_T^{\\top}A_T)  2 \\implies \\eta  \\frac{2}{\\lambda_{\\max}(A_T^{\\top}A_T)}$\n$A_T^{\\top}A_T$ 的最大特征值等于 $A_T$ 的谱范数的平方：$\\lambda_{\\max}(A_T^{\\top}A_T) = \\|A_T\\|_{2 \\to 2}^2$。所以条件变为：\n$\\eta  \\frac{2}{\\|A_T\\|_{2 \\to 2}^2}$\n这必须对迭代过程中遇到的任何支撑集 $T$ 都成立，如前所述，其大小至多为 $2s$。因此，我们必须为“最坏情况”的子矩阵，即具有最大谱范数的子矩阵，满足此条件：\n$\\eta  \\frac{2}{\\max_{|T| \\leq 2s} \\|A_T\\|_{2 \\to 2}^2}$\n问题要求一个仅用整个矩阵 $A$ 的全局谱范数（表示为 $\\|A\\|_{2 \\to 2}$）表示的上界。任何子矩阵 $A_T$ 的谱范数都受限于完整矩阵 $A$ 的谱范数：\n$$\\|A_T\\|_{2 \\to 2} = \\sup_{\\|z\\|_2=1, \\text{supp}(z) \\subseteq T} \\|Az\\|_2 \\leq \\sup_{\\|x\\|_2=1, x \\in \\mathbb{R}^n} \\|Ax\\|_2 = \\|A\\|_{2 \\to 2}$$\n这意味着：\n$$\\max_{|T| \\leq 2s} \\|A_T\\|_{2 \\to 2}^2 \\leq \\|A\\|_{2 \\to 2}^2$$\n因此，如果我们选择一个满足更严格条件的步长 $\\eta$：\n$\\eta  \\frac{2}{\\|A\\|_{2 \\to 2}^2}$\n那么在所有相关子空间上的收缩条件 $\\eta  2 / (\\max_{|T| \\leq 2s} \\|A_T\\|_{2 \\to 2}^2)$ 也将得到满足。\n这为步长 $\\eta$ 提供了一个充分条件，以确保在问题假设下收敛。因此，允许的恒定步长的显式上界是此不等式右侧的值。",
            "answer": "$$\n\\boxed{\\frac{2}{\\|A\\|_{2 \\to 2}^{2}}}\n$$"
        },
        {
            "introduction": "在分析了算法的内在属性后，我们转向一个更实际的挑战：如何设计最优的测量过程。本练习  模拟了一个工程情景，即恢复一个在小波基下稀疏的偏微分方程（PDE）状态。你的任务是编写代码，基于 $D$-最优性准则，通过贪心算法选择最佳的传感器位置，从而最大化所收集到的信息，这将压缩感知理论与最优实验设计紧密地联系起来。",
            "id": "3459917",
            "problem": "考虑与均匀空间网格上的偏微分方程（PDE）相关的一维离散化状态。令离散化状态由向量 $x \\in \\mathbb{R}^n$ 表示。假设一个线性测量模型，其点态传感器定义为 $y = A x$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个传感器放置矩阵，每行只有一个非零元素，用于选择 $x$ 的 $m$ 个不同条目。假设 $x$ 在一个标准正交哈尔小波基 $W \\in \\mathbb{R}^{n \\times n}$ 中具有稀疏表示，$x = W^\\top \\alpha$，且 $\\alpha \\in \\mathbb{R}^n$ 是 $K$-稀疏的，其支撑集为 $S \\subset \\{0,1,\\dots,n-1\\}$，其中 $|S| = K$。哈尔小波系数的排序定义如下：索引 $0$ 对应于最终的尺度系数，其后是细节系数，从最粗尺度到最细尺度排列，即 $[c_{\\text{final}}, d_{\\text{coarsest}}, \\dots, d_{\\text{finest}}]$；对于 $n = 2^L$，有 $L$ 个细节层级，其长度分别为 $1,2,4,\\dots,2^{L-1}$。\n\n从测量模型和稀疏性模型出发，为线性模型构建一个基于信息论最优设计的、有原则的贪婪传感器选择策略。具体来说，对于一个固定的支撑集 $S$，令 $M_S = A W^\\top[:, S] \\in \\mathbb{R}^{m \\times K}$ 表示将 $\\alpha_S \\in \\mathbb{R}^K$ 映射到 $y$ 的子矩阵。考虑方差为 $\\sigma^2$ 的加性高斯白噪声（AWGN），并回顾在线性高斯模型下，关于 $\\alpha_S$ 的费雪信息与 $M_S^\\top M_S$ 成正比。为确保数值稳定性，使用一个正则化的信息矩阵 $G = M_S^\\top M_S + \\lambda I_K$，其中 $\\lambda > 0$ 且 $I_K$ 是 $K \\times K$ 的单位矩阵。贪婪选择应逐行构建传感器索引集，以最大化 $\\log \\det(G)$，这在 AWGN 条件下减小了 $\\alpha_S$ 估计量的协方差体积。\n\n您的任务是在一个程序中实现以下组件：\n- 构建与上述系数排序一致的标准正交哈尔小波变换矩阵 $W \\in \\mathbb{R}^{n \\times n}$，方法是对每个标准基向量应用变换以形成 $W$ 的列。\n- 对于给定的支撑集 $S$，实现一个贪婪的 $D$-最优传感器选择算法，该算法选择 $m$ 个不同的传感器位置（$A$ 的行索引）以最大化 $\\log \\det(G)$，其中 $G = M_S^\\top M_S + \\lambda I_K$ 是选择 $m$ 个传感器后的结果。\n- 选择传感器后，使用奇异值分解（SVD）计算最小奇异值 $\\sigma_{\\min}(M_S)$ 并报告它。\n- 模拟一个支撑在 $S$ 上的 $K$-稀疏系数向量 $\\alpha$，其非零项从标准正态分布中抽取，然后形成 $x = W^\\top \\alpha$，计算测量值 $y = A x$，并通过求解最小二乘（LS）问题 $\\min_{\\hat{\\alpha}_S} \\|M_S \\hat{\\alpha}_S - y\\|_2$ 来估计 $\\alpha_S$。报告相对重构误差 $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$。\n- 同时报告贪婪选择后 $\\log \\det(G)$ 的最终值。\n\n设计选择必须基于与压缩感知和线性测量模型相关的基本原理进行论证。问题陈述中不得引入任何快捷公式；上述目标定义是唯一允许的明确目标。\n\n使用以下参数值测试套件。在每种情况下，$A$ 选择由贪婪算法指定的行，$W$ 是大小为 $n$ 的哈尔矩阵，支撑集 $S$ 被明确给出：\n- 案例 1：$n = 16$， $K = 3$， $m = 6$， $S = \\{1,5,10\\}$， $\\lambda = 10^{-6}$，随机种子 $123$。\n- 案例 2：$n = 16$， $K = 4$， $m = 4$（边界情况 $m = K$）， $S = \\{2,4,9,12\\}$， $\\lambda = 10^{-6}$，随机种子 $7$。\n- 案例 3：$n = 32$， $K = 4$， $m = 8$， $S = \\{1,3,5,20\\}$， $\\lambda = 10^{-6}$，随机种子 $42$。\n\n对于每个案例，您的程序必须输出一个包含以下内容的列表：\n- 所选传感器索引，以升序整数列表形式。\n- 最小奇异值 $\\sigma_{\\min}(M_S)$，以浮点数形式。\n- 相对重构误差，以浮点数形式。\n- 最终的 $\\log \\det(G)$ 值，以浮点数形式。\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个案例的结果格式化为其自身的、无空格的、用方括号括起来的逗号分隔列表。例如，输出必须是 $[ [\\text{case1}], [\\text{case2}], [\\text{case3}] ]$ 的形式，具体为 $[[\\text{indices},\\sigma_{\\min},\\text{error},\\log\\det],\\dots]$，行内任何地方都不能有空格。",
            "solution": "问题陈述在稀疏恢复的最优实验设计领域提出了一个明确定义的任务，该领域是压缩感知的一个子领域。它要求实现一个基于 D-最优性原理的贪婪传感器选择算法，并将其应用于一个在哈尔小波域中稀疏的状态的线性测量模型。该问题在科学上是合理的、自洽的，并且在算法上是明确的。所有参数都已提供，目标也是可量化的。因此，该问题被认为是有效的，并且可以构建解决方案。\n\n解决方案将以有原则的、循序渐进的方式进行开发。\n\n**1. 标准正交哈尔小波变换矩阵 ($W$)**\n问题指明状态向量 $x \\in \\mathbb{R}^n$ 在一个标准正交哈尔小波基中具有稀疏表示。其关系由 $x = W^\\top \\alpha$ 给出，其中 $\\alpha \\in \\mathbb{R}^n$ 是 $K$-稀疏系数向量，$W \\in \\mathbb{R}^{n \\times n}$ 是标准正交小波变换矩阵。由此可得，逆关系为 $\\alpha = W x$，因为 $W$ 是标准正交的，这意味着 $W^{-1} = W^\\top$。\n\n矩阵 $W$ 的构造方式使其第 $j$ 行 $W_{j,:}$ 是第 $j$ 个哈尔系数的合成基向量，其第 $i$ 列 $W_{:,i}$ 是第 $i$ 个标准基向量 $e_i$ 的哈尔系数向量。问题要求通过对每个 $e_i$ 应用变换来构造 $W$ 的列来构建 $W$。\n\n指定的系数排序是：索引 $0$ 用于最终的尺度系数，其后是细节系数，从最粗尺度到最细尺度。对于 $n=2^L$，这对应于一个结构为 $[c_0, d_0, (d_{1,0}, d_{1,1}), \\dots, (d_{L-1,0}, \\dots, d_{L-1, 2^{L-1}-1})]$ 的系数向量。这是一种标准的“金字塔”排序。\n\n一维标准正交哈尔小波变换是通过迭代计算均值和差值来实现的。对于长度为 $2p$ 的向量段，我们计算 $p$ 个均值和 $p$ 个差值。一对 $(a, b)$ 的均值为 $(a+b)/\\sqrt{2}$，差值为 $(a-b)/\\sqrt{2}$。因子 $1/\\sqrt{2}$ 确保了标准正交性。这个过程被递归地应用于均值向量，直到只剩下一个尺度系数。\n\n**2. 通过 D-最优性的贪婪传感器选择**\n问题的核心是从 $n$ 个可能的位置中选择 $m$ 个传感器位置。每个传感器对应一个点态测量，因此在位置 $p$ 选择一个传感器等同于选择单位矩阵的第 $p$ 行来构成测量矩阵 $A$ 的一行。测量模型为 $y=Ax$。代入稀疏模型，我们得到 $y = A W^\\top \\alpha$。由于 $\\alpha$ 在已知支撑集 $S$ 上是 $K$-稀疏的，我们可以写成 $y = (A W^\\top[:,S]) \\alpha_S = M_S \\alpha_S$，其中 $W^\\top[:,S]$ 是 $W^\\top$ 的子矩阵，其列由 $S$ 索引，而 $\\alpha_S \\in \\mathbb{R}^K$ 包含非零系数。矩阵 $M_S \\in \\mathbb{R}^{m \\times K}$ 将非零系数映射到测量值。\n\n传感器放置的质量通过其促进 $\\alpha_S$ 精确估计的能力来评估。对于带有加性高斯白噪声的线性模型，关于 $\\alpha_S$ 的费雪信息矩阵与 $M_S^\\top M_S$ 成正比。D-最优性旨在最大化该信息矩阵的行列式，这在几何上等同于最小化 $\\alpha_S$ 最小二乘估计量的置信椭球体积。\n\n我们使用一个正则化的信息矩阵 $G = M_S^\\top M_S + \\lambda I_K$，其中 $\\lambda  0$ 是一个正则化参数，确保 $G$ 始终是良态且可逆的。目标是选择 $A$ 的 $m$ 行（即传感器位置），以最大化 $\\log \\det(G)$。\n\n我们采用贪婪算法来解决这个组合优化问题。从一个空的传感器集合开始，我们迭代地添加能使 $\\log \\det(G)$ 边际增益最大的传感器。设 $\\mathcal{P}_{k-1}$ 是已选择的 $k-1$ 个传感器索引的集合。相应的信息矩阵是 $G_{k-1}$。为了选择第 $k$ 个传感器，我们评估每个候选位置 $p \\notin \\mathcal{P}_{k-1}$。添加传感器 $p$ 对应于将行向量 $v_p = (W^\\top[:,S])_{p,:}$ 附加到当前的测量子矩阵。新的信息矩阵变为 $G_k = G_{k-1} + v_p^\\top v_p$。我们选择使 $\\log\\det(G_k)$ 最大化的传感器 $p^*$。这个过程重复 $m$ 次。使用 $\\log \\det$ 可以提高数值稳定性。\n\n**3. 重构与性能评估**\n在选择了 $m$ 个传感器位置后，我们形成最终的测量矩阵 $M_S \\in \\mathbb{R}^{m \\times K}$。\n这个矩阵的质量部分由其奇异值来表征。最小奇异值 $\\sigma_{\\min}(M_S)$ 是噪声最差情况放大率的度量，并与矩阵的条件数有关。一个小的 $\\sigma_{\\min}(M_S)$ 表明 $M_S$ 接近秩亏，这会损害估计的稳定性。\n\n为了测试重构性能，我们首先模拟一个基准真相。创建一个 $K$-稀疏系数向量 $\\alpha$，其在给定支撑集 $S$ 上的非零项从标准正态分布中抽取。物理状态被合成为 $x=W^\\top\\alpha$。然后计算无噪声的测量值 $y = Ax$。\n\n系数向量 $\\alpha_S$ 是通过求解线性最小二乘问题从测量值 $y$ 中估计出来的：\n$$ \\hat{\\alpha}_S = \\arg\\min_{\\beta \\in \\mathbb{R}^K} \\| M_S \\beta - y \\|_2^2 $$\n其解由 $\\hat{\\alpha}_S = (M_S^\\top M_S)^{-1} M_S^\\top y$ 给出。在数值上，最好使用稳健的方法来求解，例如基于 QR 分解或 SVD 的方法，正如在 `numpy.linalg.lstsq` 中实现的那样。\n\n性能由相对重构误差 $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$ 来量化。\n\n最终算法的流程是：首先构造哈尔矩阵 $W$，然后运行 $m$ 步的贪婪选择循环。在每一步中，测试每个可用的传感器位置，并选择使更新后的正则化信息矩阵 $G$ 的对数行列式最大化的位置。最后，在选择完成后，计算并报告所需的度量指标——所选传感器索引列表、$\\sigma_{\\min}(M_S)$、重构误差以及最终的 $\\log\\det(G)$。",
            "answer": "```python\nimport numpy as np\n\ndef _haar_1d_transform(v):\n    \"\"\"\n    Computes the 1D orthonormal Haar wavelet transform of a vector.\n    The coefficient ordering is: final scaling coefficient, followed by\n    detail coefficients from the coarsest to the finest scale.\n    \"\"\"\n    n = len(v)\n    if n == 1:\n        return v.astype(np.float64)\n    \n    if n  (n - 1) != 0 or n == 0:\n        raise ValueError(\"Input vector length must be a power of 2.\")\n\n    temp_v = v.astype(np.float64)\n    coeffs = np.zeros(n, dtype=np.float64)\n    \n    current_len = n\n    while current_len  1:\n        next_len = current_len // 2\n        averages = (temp_v[0:current_len:2] + temp_v[1:current_len:2]) / np.sqrt(2)\n        details = (temp_v[0:current_len:2] - temp_v[1:current_len:2]) / np.sqrt(2)\n        \n        # Place details in the second half of the current segment in the output array.\n        # This naturally orders them from finest to coarsest as current_len decreases.\n        # Here we place them to match the problem statement.\n        coeffs[next_len:current_len] = details\n        temp_v[:next_len] = averages\n        current_len = next_len\n\n    coeffs[0] = temp_v[0]\n    return coeffs\n\ndef construct_haar_matrix(n):\n    \"\"\"\n    Constructs the orthonormal Haar wavelet transform matrix W of size n x n.\n    The columns of W are the transforms of the standard basis vectors.\n    \"\"\"\n    W = np.zeros((n, n), dtype=np.float64)\n    for i in range(n):\n        e_i = np.zeros(n)\n        e_i[i] = 1.0\n        W[:, i] = _haar_1d_transform(e_i)\n    return W\n\ndef solve():\n    \"\"\"\n    Main function to solve the sensor selection problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 16, 'K': 3, 'm': 6, 'S': {1, 5, 10}, 'lambda': 1e-6, 'seed': 123},\n        {'n': 16, 'K': 4, 'm': 4, 'S': {2, 4, 9, 12}, 'lambda': 1e-6, 'seed': 7},\n        {'n': 32, 'K': 4, 'm': 8, 'S': {1, 3, 5, 20}, 'lambda': 1e-6, 'seed': 42},\n    ]\n\n    results_for_all_cases = []\n\n    for case in test_cases:\n        n, K, m, S_set, lambda_reg, seed = case['n'], case['K'], case['m'], case['S'], case['lambda'], case['seed']\n        S = list(S_set)\n\n        # 1. Construct Haar wavelet matrix W\n        W = construct_haar_matrix(n)\n        Psi_S = W.T[:, S]\n\n        # 2. Greedy D-optimal sensor selection\n        selected_indices = []\n        available_indices = list(range(n))\n        \n        # Initialize regularized information matrix G\n        G = lambda_reg * np.eye(K)\n        \n        current_log_det = np.linalg.slogdet(G)[1]\n\n        for _ in range(m):\n            best_p = -1\n            best_log_det = -np.inf\n            \n            for p in available_indices:\n                v_p = Psi_S[p, :]\n                \n                # Rank-1 update to G\n                G_candidate = G + np.outer(v_p, v_p)\n                \n                # Using slogdet for numerical stability\n                sign, log_det_candidate = np.linalg.slogdet(G_candidate)\n                \n                if sign  0 and log_det_candidate  best_log_det:\n                    best_log_det = log_det_candidate\n                    best_p = p\n\n            selected_indices.append(best_p)\n            available_indices.remove(best_p)\n            \n            # Update G and log_det for the next iteration\n            v_best = Psi_S[best_p, :]\n            G += np.outer(v_best, v_best)\n            current_log_det = best_log_det\n        \n        final_log_det = current_log_det\n        selected_indices.sort()\n\n        # 3. Construct final M_S and compute minimum singular value\n        M_S = Psi_S[selected_indices, :]\n        if M_S.shape[0]  0:\n            singular_values = np.linalg.svd(M_S, compute_uv=False)\n            sigma_min = singular_values.min()\n        else:\n            sigma_min = 0.0\n\n        # 4. Simulate and reconstruct\n        np.random.seed(seed)\n        alpha = np.zeros(n)\n        alpha_S_true = np.random.randn(K)\n        alpha[S] = alpha_S_true\n        \n        x = W.T @ alpha\n        \n        # Measurement matrix A is implicit\n        y = x[selected_indices]\n        \n        # Estimate alpha_S via Least Squares\n        alpha_S_hat, _, _, _ = np.linalg.lstsq(M_S, y, rcond=None)\n        \n        # Compute relative reconstruction error\n        norm_true = np.linalg.norm(alpha_S_true)\n        if norm_true == 0:\n            error = 0.0 if np.linalg.norm(alpha_S_hat) == 0 else np.inf\n        else:\n            error = np.linalg.norm(alpha_S_hat - alpha_S_true) / norm_true\n\n        # Format results for the current case\n        indices_str = f\"[{','.join(map(str, selected_indices))}]\"\n        case_result_str = f\"[{indices_str},{sigma_min},{error},{final_log_det}]\"\n        results_for_all_cases.append(case_result_str)\n\n    # Final print statement in the exact required format with no spaces\n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后，我们探索一个更高级的模型：多任务或联合稀疏恢复。在许多应用中，我们需要同时恢复多个共享相同稀疏支撑集的信号。本练习  将指导你实现并比较两种不同的估计器，一种利用 $\\ell_{2,1}$ 范数促进联合稀疏性，另一种则不具备此功能，让你亲身体验更复杂的结构化稀疏模型及其求解算法，例如近端梯度法。",
            "id": "3459923",
            "problem": "考虑压缩感知和稀疏优化中的多任务线性测量模型，该模型具有跨任务的联合稀疏性。令 $A \\in \\mathbb{R}^{m \\times n}$ 表示测量矩阵，令 $X \\in \\mathbb{R}^{n \\times T}$ 表示未知的系数矩阵，其列对应于 $T$ 个任务，令 $Y \\in \\mathbb{R}^{m \\times T}$ 表示满足线性模型 $Y = A X + W$ 的观测测量值。假设 $X$ 的各列共享一个共同的支撑集：$X$ 的非零行的索引集合在所有列中都是相同的。噪声矩阵 $W$ 的列为 $w_t \\in \\mathbb{R}^{m}$，$t \\in \\{1,\\dots,T\\}$，其中每个 $w_t$ 是零均值高斯噪声，具有与列相关的、正定的协方差矩阵 $\\Sigma_t \\in \\mathbb{R}^{m \\times m}$，即 $w_t \\sim \\mathcal{N}(0, \\Sigma_t)$，并且通常情况下 $\\Sigma_t \\neq \\Sigma_{t'}$。\n\n从以下基础出发：\n- 对于协方差为 $\\Sigma_t$ 的独立高斯噪声，其负对数似然（在不考虑与 $X$ 无关的加性常数的情况下）与 $\\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2$ 成正比，其中 $x_t$ 是 $X$ 的第 $t$ 列，$y_t$ 是 $Y$ 的第 $t$ 列。\n- 跨任务的联合稀疏性可通过混合范数来促进，这些范数聚合了行向量的模长，例如混合 $\\ell_{p,q}$ 范数。\n\n您必须推导、实现并比较两种用于求解 $X$ 的估计器，它们在混合范数正则化项上有所不同：\n- 估计器 1 使用混合 $\\ell_{2,1}$ 范数，定义为 $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2$，其中 $X_{i,:}$ 表示 $X$ 的第 $i$ 行。\n- 估计器 2 使用混合 $\\ell_{2,2}$ 范数，定义为 $\\|X\\|_{2,2} = \\left( \\sum_{i=1}^{n} \\|X_{i,:}\\|_2^2 \\right)^{1/2}$，该范数与 Frobenius 范数成正比，并且不施加稀疏性。\n\n您的程序必须：\n- 基于上述基本原理推导算法步骤，通过求解 $\\min_{X \\in \\mathbb{R}^{n \\times T}} \\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\lambda_{2,1} \\|X\\|_{2,1}$ 来计算 $\\ell_{2,1}$ 正则化估计。该算法应使用一种有原则的一阶方法，该方法包含针对混合 $\\ell_{2,1}$ 范数的正确近端算子，并根据平滑项梯度的 Lipschitz 连续性来确定步长。\n- 基于上述基本原理推导算法步骤，通过求解 $\\min_{X \\in \\mathbb{R}^{n \\times T}} \\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\frac{\\lambda_{2,2}}{2} \\|X\\|_{F}^2$ 来计算 $\\ell_{2,2}$ 正则化估计，其中 $\\|X\\|_{F}$ 表示 Frobenius 范数。该算法应使用基于最优性条件的正确线性代数解法。\n\n联合支撑集恢复的评估必须通过以下方式进行：将支撑集识别为行索引的集合，这些行的跨任务行范数（$\\ell_2$ 范数）是最大的 $s$ 个值之一，其中 $s$ 是真实的稀疏度（即真实 $X$ 中的非零行数）。对于 $s = 0$ 的情况，真实支撑集为空集。如果所有行范数都低于任何正阈值，则恢复的支撑集也为空集；在这种情况下，将恢复的支撑集定义为空集。对于每个估计器，当且仅当恢复的支撑集与真实的支撑集完全匹配时，才声明支撑集恢复成功。\n\n测试套件包含三个合成案例，其参数具有科学一致性和可复现性。在所有案例中，用独立的标准正态分布项构造 $A$，并将其列归一化为单位 $\\ell_2$ 范数。构造一个具有基数为 $s$ 的共同随机支撑集的 $X$，其非零行在各任务间独立抽取，振幅如下所述。对于每个任务 $t$，生成对角矩阵 $\\Sigma_t$，其对角线元素在指定范围内均匀采样，并生成协方差为 $\\Sigma_t$ 的高斯噪声 $w_t$。令 $Y = A X + W$。\n\n测试案例 1（理想路径）：\n- 维度：$m = 40$, $n = 80$, $T = 3$。\n- 稀疏度：$s = 6$。\n- 非零行振幅：$1.5$。\n- 每个任务的噪声协方差范围：对角线元素在 $[0.3, 1.2]$。\n- 正则化参数：$\\lambda_{2,1} = 0.25$, $\\lambda_{2,2} = 0.05$。\n- 随机种子：$123456$。\n\n测试案例 2（边界情况 $s = 0$）：\n- 维度：$m = 50$, $n = 70$, $T = 2$。\n- 稀疏度：$s = 0$。\n- 非零行振幅：$0.0$。\n- 每个任务的噪声协方差范围：对角线元素在 $[0.5, 1.0]$。\n- 正则化参数：$\\lambda_{2,1} = 0.30$, $\\lambda_{2,2} = 0.10$。\n- 随机种子：$123457$。\n\n测试案例 3（具有强各向异性、列相关噪声的边缘情况）：\n- 维度：$m = 30$, $n = 90$, $T = 4$。\n- 稀疏度：$s = 5$。\n- 非零行振幅：$1.2$。\n- 每个任务的噪声协方差范围：对角线元素在 $[0.1, 3.0]$。\n- 正则化参数：$\\lambda_{2,1} = 0.20$, $\\lambda_{2,2} = 0.05$。\n- 随机种子：$123458$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。此列表中的每个元素对应一个测试案例，并且必须是一个包含两个布尔值的列表 $[b_{2,1}, b_{2,2}]$，其中如果 $\\ell_{2,1}$ 估计器恢复了精确的支撑集，则 $b_{2,1}$ 为 $true$，否则为 $false$；如果 $\\ell_{2,2}$ 估计器恢复了精确的支撑集，则 $b_{2,2}$ 为 $true$，否则为 $false$。例如，输出可能看起来像 $[[true,false],[true,true],[false,false]]$。请注意，除了这单行文本外，您的程序不得打印任何其他内容。",
            "solution": "用户要求推导、实现和比较两种用于具有联合稀疏性的多任务线性模型的估计器，并根据它们恢复正确稀疏支撑集的能力进行评估。该模型为 $Y = AX + W$，其中 $X \\in \\mathbb{R}^{n \\times T}$ 是待恢复的信号矩阵，$A \\in \\mathbb{R}^{m \\times n}$ 是测量矩阵，$W$ 是一个与任务相关的（task-dependent）高斯噪声矩阵，其每列 $w_t \\sim \\mathcal{N}(0, \\Sigma_t)$。\n\n问题的核心是求解两个不同的凸优化问题。第一个估计器使用 $\\ell_{2,1}$ 范数正则化器来促进联合（逐行）稀疏性，而第二个估计器使用 $\\ell_{2,2}$ 范数（Frobenius 范数）正则化器，这对应于多任务岭回归（ridge regression），并且不强制稀疏性。\n\n### 估计器 1：$\\ell_{2,1}$ 正则化估计（多任务组 LASSO）\n\n第一个估计器 $\\hat{X}_{2,1}$ 是以下优化问题的解：\n$$ \\min_{X \\in \\mathbb{R}^{n \\times T}} F(X) = \\underbrace{\\frac{1}{2} \\sum_{t=1}^{T} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2}_{f(X)} + \\underbrace{\\lambda_{2,1} \\|X\\|_{2,1}}_{g(X)} $$\n其中 $x_t$ 和 $y_t$ 分别是 $X$ 和 $Y$ 的第 $t$ 列，$\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2$ 是混合 $\\ell_{2,1}$ 范数。\n\n该目标函数是一个平滑、可微、凸的函数 $f(X)$（负对数似然）和一个非平滑、凸的正则化项 $g(X)$ 的和。这种结构非常适合使用近端梯度算法，例如迭代收缩阈值算法（ISTA）。ISTA 的更新规则由下式给出：\n$$ X^{(k+1)} = \\text{prox}_{\\alpha g}\\left(X^{(k)} - \\alpha \\nabla f(X^{(k)})\\right) $$\n其中 $\\alpha$ 是步长。\n\n**1. 平滑项的梯度, $\\nabla f(X)$**\n平滑项 $f(X)$ 可以重写为 $f(X) = \\frac{1}{2} \\sum_{t=1}^{T} (A x_t - y_t)^T \\Sigma_t^{-1} (A x_t - y_t)$。在平滑部分，目标函数对于 $X$ 的各列是可分的。我们可以独立地求出关于每列 $x_t$ 的梯度：\n$$ \\nabla_{x_t} f(X) = A^T \\Sigma_t^{-1} (A x_t - y_t) $$\n完整的梯度 $\\nabla f(X) \\in \\mathbb{R}^{n \\times T}$ 是一个矩阵，其第 $t$ 列为 $\\nabla_{x_t} f(X)$。\n\n**2. 步长 $\\alpha$ 和 Lipschitz 常数**\n为使 ISTA 收敛，步长 $\\alpha$ 必须满足 $0  \\alpha \\le 1/L$，其中 $L$ 是梯度 $\\nabla f(X)$ 的 Lipschitz 常数。Lipschitz 常数是 $f(X)$ 的 Hessian 矩阵的最大特征值（谱范数）。$f(X)$ 关于向量化变量 $\\text{vec}(X)$ 的 Hessian 矩阵是一个块对角矩阵：\n$$ \\nabla^2 f(\\text{vec}(X)) = \\text{diag}(A^T \\Sigma_1^{-1} A, \\dots, A^T \\Sigma_T^{-1} A) $$\nLipschitz 常数 $L$ 是这些块的最大特征值的最大值：\n$$ L = \\max_{t=1, \\dots, T} \\lambda_{\\max}(A^T \\Sigma_t^{-1} A) $$\n由于 $A^T \\Sigma_t^{-1} A = (\\Sigma_t^{-1/2} A)^T (\\Sigma_t^{-1/2} A)$，其中 $\\Sigma_t^{-1/2}$ 是逆协方差的矩阵平方根，我们有 $\\lambda_{\\max}(A^T \\Sigma_t^{-1} A) = \\sigma_{\\max}(\\Sigma_t^{-1/2} A)^2$，其中 $\\sigma_{\\max}(\\cdot)$ 表示最大奇异值（谱范数）。因此，\n$$ L = \\max_{t=1, \\dots, T} \\sigma_{\\max}(\\Sigma_t^{-1/2} A)^2 $$\n我们为算法选择步长 $\\alpha = 1/L$。\n\n**3. $\\ell_{2,1}$ 范数的近端算子**\n$g(X) = \\lambda_{2,1} \\|X\\|_{2,1}$ 的近端算子（参数为 $\\gamma = \\alpha \\lambda_{2,1}$）定义为：\n$$ \\text{prox}_{\\gamma \\|\\cdot\\|_{2,1}}(Z) = \\arg\\min_{X} \\left( \\gamma \\sum_{i=1}^{n} \\|X_{i,:}\\|_2 + \\frac{1}{2} \\|X - Z\\|_F^2 \\right) $$\n这个问题在 $X$ 的各行上是解耦的。对于每一行 $i$，我们求解：\n$$ \\arg\\min_{u \\in \\mathbb{R}^T} \\left( \\gamma \\|u\\|_2 + \\frac{1}{2} \\|u - z_{i,:}\\|_2^2 \\right) $$\n其中 $u = X_{i,:}$ 且 $z_{i,:} = Z_{i,:}$。这是欧几里得范数的近端算子，其解被称为块软阈值（block soft-thresholding）：\n$$ (\\text{prox}_{\\gamma \\|\\cdot\\|_{2,1}}(Z))_{i,:} = \\left(1 - \\frac{\\gamma}{\\|Z_{i,:}\\|_2}\\right)_+ Z_{i,:} = \\max\\left(0, 1 - \\frac{\\gamma}{\\|Z_{i,:}\\|_2}\\right) Z_{i,:} $$\n该算子将 $Z$ 的行向原点收缩，将 $\\ell_2$ 范数小于或等于 $\\gamma$ 的行设置为零，从而引导行稀疏性。\n\n### 估计器 2：$\\ell_{2,2}$ 正则化估计（多任务岭回归）\n\n第二个估计器 $\\hat{X}_{2,2}$ 是岭回归问题的解：\n$$ \\min_{X \\in \\mathbb{R}^{n \\times T}} J(X) = \\frac{1}{2} \\sum_{t=1}^{T} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\frac{\\lambda_{2,2}}{2} \\|X\\|_{F}^2 $$\nFrobenius 范数的平方为 $\\|X\\|_{F}^2 = \\sum_{t=1}^{T} \\|x_t\\|_2^2$。目标函数 $J(X)$ 在各个任务（列）上是完全可分的：\n$$ J(X) = \\sum_{t=1}^{T} \\underbrace{\\left( \\frac{1}{2} (A x_t - y_t)^T \\Sigma_t^{-1} (A x_t - y_t) + \\frac{\\lambda_{2,2}}{2} x_t^T x_t \\right)}_{J_t(x_t)} $$\n我们可以通过最小化 $J_t(x_t)$ 来独立地求解 $X$ 的每一列 $x_t$。由于 $J_t(x_t)$ 是一个严格凸的二次函数，其唯一的最小值点可以通过将其梯度设为零来找到。\n$$ \\nabla_{x_t} J_t(x_t) = A^T \\Sigma_t^{-1} (A x_t - y_t) + \\lambda_{2,2} x_t = 0 $$\n重新整理各项以求解 $x_t$：\n$$ (A^T \\Sigma_t^{-1} A) x_t + \\lambda_{2,2} I_n x_t = A^T \\Sigma_t^{-1} y_t $$\n$$ (A^T \\Sigma_t^{-1} A + \\lambda_{2,2} I_n) x_t = A^T \\Sigma_t^{-1} y_t $$\n其中 $I_n$ 是 $n \\times n$ 的单位矩阵。这通过求解一个线性系统，为每个任务的系数向量提供了一个闭式解：\n$$ \\hat{x}_t = (A^T \\Sigma_t^{-1} A + \\lambda_{2,2} I_n)^{-1} (A^T \\Sigma_t^{-1} y_t) $$\n最终的估计 $\\hat{X}_{2,2}$ 是通过连接这些列向量构造的：$\\hat{X}_{2,2} = [\\hat{x}_1, \\dots, \\hat{x}_T]$。与 $\\ell_{2,1}$ 正则化器不同，岭惩罚项不促进稀疏性，因此得到的 $\\hat{X}_{2,2}$ 通常是稠密的。\n\n### 支撑集恢复评估\n\n对于这两个估计器，支撑集都按照给定的规则进行恢复。首先，我们计算估计矩阵 $\\hat{X}$ 的每一行的 $\\ell_2$ 范数。令这些范数为 $r_i = \\|\\hat{X}_{i,:}\\|_2$，其中 $i=1, \\dots, n$。恢复的支撑集 $S_{rec}$ 是对应于 $s$ 个最大行范数的索引集合，其中 $s$ 是真实的稀疏度。真实支撑集 $S_{true}$ 是基准真相矩阵 $X$ 中非零行的索引集合。当且仅当 $S_{rec} = S_{true}$ 时，恢复被视为成功。对于 $s=0$ 的特殊情况，真实支撑集是空集。恢复规则要求选择 0 个最大的范数，这也导致一个空集。因此，根据此定义，对于 $s=0$ 的情况，支撑集恢复总是成功的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(m, n, T, s, amplitude, cov_range, seed):\n    \"\"\"Generates synthetic data for one test case.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate measurement matrix A\n    A = rng.standard_normal((m, n))\n    A_norms = np.linalg.norm(A, axis=0)\n    A /= A_norms[np.newaxis, :]\n    \n    # Generate true sparse matrix X\n    X_true = np.zeros((n, T))\n    true_support = set()\n    if s  0:\n        support_indices = rng.choice(n, s, replace=False)\n        true_support = set(support_indices)\n        for i in support_indices:\n            # Nonzero entries are amplitude * random sign, independent across tasks\n            X_true[i, :] = amplitude * rng.choice([-1, 1], size=T)\n            \n    # Generate noise W and observations Y\n    Y = np.zeros((m, T))\n    all_Sigma = []\n    for t in range(T):\n        # Diagonal covariance matrix for task t\n        sigma_diag = rng.uniform(cov_range[0], cov_range[1], size=m)\n        Sigma_t = np.diag(sigma_diag)\n        all_Sigma.append(Sigma_t)\n        \n        # Generate noise w_t ~ N(0, Sigma_t)\n        # w_t = Sigma_t^{1/2} * z, where z ~ N(0, I)\n        z = rng.standard_normal(m)\n        w_t = np.sqrt(sigma_diag) * z\n        Y[:, t] = A @ X_true[:, t] + w_t\n        \n    return A, X_true, Y, all_Sigma, true_support\n\ndef estimate_l21(A, Y, all_Sigma, lambda_21, num_iter=2000):\n    \"\"\"Solves the l2,1-regularized problem using ISTA.\"\"\"\n    m, n = A.shape\n    _, T = Y.shape\n    \n    # Calculate Lipschitz constant L\n    L = 0.0\n    for t in range(T):\n        Sigma_t = all_Sigma[t]\n        sigma_inv_sqrt_diag = 1.0 / np.sqrt(np.diag(Sigma_t))\n        A_tilde_t = sigma_inv_sqrt_diag[:, np.newaxis] * A\n        # spectral norm squared of A_tilde_t\n        s_vals_sq = np.linalg.svd(A_tilde_t, compute_uv=False)**2\n        L = max(L, np.max(s_vals_sq))\n    \n    alpha = 1.0 / L\n    \n    X_est = np.zeros((n, T))\n    \n    for _ in range(num_iter):\n        # Gradient computation\n        grad_f = np.zeros((n, T))\n        for t in range(T):\n            Sigma_inv_diag = 1.0 / np.diag(all_Sigma[t])\n            residual_t = A @ X_est[:, t] - Y[:, t]\n            grad_f[:, t] = A.T @ (Sigma_inv_diag * residual_t)\n        \n        # Gradient descent step\n        Z = X_est - alpha * grad_f\n        \n        # Proximal step (block soft-thresholding)\n        row_norms = np.linalg.norm(Z, axis=1)\n        threshold = alpha * lambda_21\n        \n        # Avoid division by zero\n        shrinkage_factors = np.zeros_like(row_norms)\n        non_zero_norm_indices = row_norms  0\n        \n        shrinkage_factors[non_zero_norm_indices] = np.maximum(\n            0, 1 - threshold / row_norms[non_zero_norm_indices]\n        )\n        \n        X_est = Z * shrinkage_factors[:, np.newaxis]\n        \n    return X_est\n\ndef estimate_l22(A, Y, all_Sigma, lambda_22):\n    \"\"\"Solves the l2,2-regularized problem (ridge).\"\"\"\n    n, T = Y.shape[1], Y.shape[1]\n    m, n = A.shape\n\n    X_est = np.zeros((n, T))\n    I_n = np.eye(n)\n    \n    for t in range(T):\n        Sigma_inv_diag = 1.0 / np.diag(all_Sigma[t])\n        A_T_Sigma_inv = A.T * Sigma_inv_diag[np.newaxis, :]\n        \n        M_t = A_T_Sigma_inv @ A + lambda_22 * I_n\n        b_t = A_T_Sigma_inv @ Y[:, t]\n        \n        X_est[:, t] = np.linalg.solve(M_t, b_t)\n        \n    return X_est\n\ndef evaluate_support(X_est, true_support, s):\n    \"\"\"Evaluates support recovery success.\"\"\"\n    if s == 0:\n        # Per problem spec, recovered support for s=0 is the empty set\n        recovered_support = set()\n    else:\n        row_norms = np.linalg.norm(X_est, axis=1)\n        # Indices of s largest norms\n        recovered_support = set(np.argsort(row_norms)[-s:])\n        \n    return recovered_support == true_support\n\ndef solve():\n    test_cases = [\n        {\n            \"m\": 40, \"n\": 80, \"T\": 3, \"s\": 6, \"amp\": 1.5,\n            \"cov_range\": [0.3, 1.2], \"lambda_21\": 0.25, \"lambda_22\": 0.05,\n            \"seed\": 123456\n        },\n        {\n            \"m\": 50, \"n\": 70, \"T\": 2, \"s\": 0, \"amp\": 0.0,\n            \"cov_range\": [0.5, 1.0], \"lambda_21\": 0.30, \"lambda_22\": 0.10,\n            \"seed\": 123457\n        },\n        {\n            \"m\": 30, \"n\": 90, \"T\": 4, \"s\": 5, \"amp\": 1.2,\n            \"cov_range\": [0.1, 3.0], \"lambda_21\": 0.20, \"lambda_22\": 0.05,\n            \"seed\": 123458\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, X_true, Y, all_Sigma, true_support = generate_data(\n            case[\"m\"], case[\"n\"], case[\"T\"], case[\"s\"], case[\"amp\"],\n            case[\"cov_range\"], case[\"seed\"]\n        )\n        \n        # Estimator 1: l2,1 regularization\n        X_est_21 = estimate_l21(A, Y, all_Sigma, case[\"lambda_21\"])\n        success_21 = evaluate_support(X_est_21, true_support, case[\"s\"])\n        \n        # Estimator 2: l2,2 regularization\n        X_est_22 = estimate_l22(A, Y, all_Sigma, case[\"lambda_22\"])\n        success_22 = evaluate_support(X_est_22, true_support, case[\"s\"])\n        \n        results.append([success_21, success_22])\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"[{str(r[0]).lower()},{str(r[1]).lower()}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}