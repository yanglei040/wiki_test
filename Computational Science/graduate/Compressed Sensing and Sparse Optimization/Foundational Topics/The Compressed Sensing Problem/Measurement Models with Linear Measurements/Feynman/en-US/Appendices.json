{
    "hands_on_practices": [
        {
            "introduction": "To ensure that our sparse recovery algorithms work reliably, we must understand their convergence properties. This practice  focuses on the Iterative Hard Thresholding (IHT) algorithm, a cornerstone of sparse recovery. You will derive a condition on the algorithm's step size $\\eta$ that guarantees convergence, linking it directly to the spectral properties of the measurement matrix $A$. This theoretical exercise is fundamental for anyone implementing or analyzing first-order optimization methods in compressed sensing.",
            "id": "3459927",
            "problem": "Consider a linear measurement model in compressed sensing of the form $y = A x^{\\star}$, where $A \\in \\mathbb{R}^{m \\times n}$, $x^{\\star} \\in \\mathbb{R}^{n}$ is an unknown $s$-sparse signal, and $y \\in \\mathbb{R}^{m}$ is the measurement vector. Let the objective be the least-squares loss $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, and consider the Iterative Hard Thresholding (IHT) algorithm, defined as $x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$, where $H_{s}(\\cdot)$ denotes hard thresholding to $s$ entries by magnitude, and $\\eta  0$ is a constant step size. Assume that $A$ satisfies a Restricted Isometry Property (RIP) of appropriate order, with restricted isometry constant $\\delta_{k}$ defined by $(1 - \\delta_{k})\\|u\\|_{2}^{2} \\leq \\|A u\\|_{2}^{2} \\leq (1 + \\delta_{k})\\|u\\|_{2}^{2}$ for all vectors $u$ with support size at most $k$. Under suitable RIP conditions ensuring contraction of the IHT error recurrence on the union of supports encountered by the iterates, derive from first principles an explicit upper bound on the allowable constant step size, expressed solely in terms of the spectral norm $\\|A\\|_{2 \\to 2}$, that guarantees convergence of the IHT iterates to the $s$-sparse solution. State your final bound as a single closed-form analytic expression. No numerical rounding is required.",
            "solution": "The Iterative Hard Thresholding (IHT) algorithm aims to find a sparse solution $x$ to the linear system $y = Ax$. The update rule is given by:\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right)$$\nwhere $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ is the least-squares loss function, $\\eta$ is the step size, and $H_s(\\cdot)$ is the hard-thresholding operator that projects a vector onto the set of $s$-sparse vectors.\n\nThe gradient of the loss function is $\\nabla f(x) = A^{\\top}(A x - y)$. Substituting this into the IHT update rule gives:\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$$\nWe are given that $y = A x^{\\star}$, where $x^{\\star}$ is the true $s$-sparse signal we wish to recover. Substituting this into the update rule:\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - A x^{\\star})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}A(x^{t} - x^{\\star})\\right)$$\nLet $e^{t} = x^{t} - x^{\\star}$ be the error at iteration $t$. We can rewrite the argument of the hard-thresholding operator in terms of the error and the true signal $x^{\\star}$:\n$$x^{t+1} = H_{s}\\!\\left(x^{\\star} + e^{t} - \\eta A^{\\top}A e^{t}\\right) = H_{s}\\!\\left(x^{\\star} + (I - \\eta A^{\\top}A)e^{t}\\right)$$\nConvergence proofs for IHT analyze the evolution of the error norm $\\|e^{t+1}\\|_2 = \\|x^{t+1}-x^\\star\\|_2$. The error vector $e^t = x^t - x^\\star$ is supported on the union of the supports of $x^t$ and $x^\\star$, which has size at most $2s$. A crucial requirement for convergence is that the gradient descent operator, $z \\mapsto (I - \\eta A^{\\top}A)z$, is a strict contraction on the space of $2s$-sparse vectors.\nLet $T$ be any set of indices with $|T| \\leq 2s$, and let $A_T$ be the submatrix of $A$ with columns indexed by $T$. The condition for strict contraction is that the spectral norm of the operator's matrix representation on this subspace is less than 1:\n$$\\|I - \\eta A_T^{\\top}A_T\\|_{2 \\to 2}  1$$\nThe matrix $A_T^{\\top}A_T$ is symmetric positive semi-definite. Let its eigenvalues be $\\lambda_i(A_T^{\\top}A_T)$. The spectral norm condition is equivalent to requiring $|1 - \\eta \\lambda_i|  1$ for all eigenvalues $\\lambda_i$. This simplifies to requiring $\\eta \\lambda_i  2$. To satisfy this for all eigenvalues, we must have:\n$$\\eta \\lambda_{\\max}(A_T^{\\top}A_T)  2 \\implies \\eta  \\frac{2}{\\lambda_{\\max}(A_T^{\\top}A_T)}$$\nThe largest eigenvalue of $A_T^{\\top}A_T$ is equal to the squared spectral norm of $A_T$, so the condition is $\\eta  2 / \\|A_T\\|_{2 \\to 2}^2$. This must hold for any submatrix $A_T$ corresponding to a support of size at most $2s$.\n\nThe problem asks for an upper bound solely in terms of the global spectral norm of the entire matrix, $\\|A\\|_{2 \\to 2}$. The spectral norm of any submatrix $A_T$ is bounded by the spectral norm of the full matrix $A$:\n$$\\|A_T\\|_{2 \\to 2} \\leq \\|A\\|_{2 \\to 2}$$\nTherefore, a sufficient, though potentially looser, condition is derived by replacing the submatrix norm with the full matrix norm:\n$$\\eta  \\frac{2}{\\|A\\|_{2 \\to 2}^2}$$\nThis provides a sufficient condition on the step size $\\eta$ to guarantee convergence. The explicit upper bound on the allowable constant step size is the value on the right-hand side of this inequality.",
            "answer": "$$\n\\boxed{\\frac{2}{\\|A\\|_{2 \\to 2}^{2}}}\n$$"
        },
        {
            "introduction": "The linear measurement model finds powerful applications in system design, where the measurement matrix is not arbitrary but a choice to be optimized. This coding practice  frames the problem of sensor placement as an optimal experiment design task. You will implement a greedy algorithm to select the most informative sensor locations for recovering a signal that is sparse in the Haar wavelet basis, grounding the abstract concept of D-optimality in a tangible engineering scenario.",
            "id": "3459917",
            "problem": "Consider a one-dimensional discretized state associated with a Partial Differential Equation (PDE) on a uniform spatial grid. Let the discretized state be represented by the vector $x \\in \\mathbb{R}^n$. Assume a linear measurement model with pointwise sensors defined by $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensor placement matrix with exactly one nonzero per row selecting $m$ distinct entries of $x$. Assume that $x$ admits a sparse representation in an orthonormal Haar wavelet basis $W \\in \\mathbb{R}^{n \\times n}$, with $x = W^\\top \\alpha$, and $\\alpha \\in \\mathbb{R}^n$ being $K$-sparse with support $S \\subset \\{0,1,\\dots,n-1\\}$, where $|S| = K$. The Haar wavelet coefficient ordering is defined as follows: index $0$ corresponds to the final scaling coefficient, followed by detail coefficients from the coarsest scale to the finest scale, i.e., $[c_{\\text{final}}, d_{\\text{coarsest}}, \\dots, d_{\\text{finest}}]$; for $n = 2^L$, there are $L$ detail levels with lengths $1,2,4,\\dots,2^{L-1}$.\n\nStarting from the measurement model and the sparsity model, formulate a principled greedy sensor selection strategy grounded in the information-theoretic optimal design for linear models. In particular, for a fixed support $S$, let $M_S = A W^\\top[:, S] \\in \\mathbb{R}^{m \\times K}$ denote the submatrix mapping $\\alpha_S \\in \\mathbb{R}^K$ to $y$. Consider additive white Gaussian noise (AWGN) with variance $\\sigma^2$ and recall that under a linear Gaussian model the Fisher information about $\\alpha_S$ is proportional to $M_S^\\top M_S$. To ensure numerical stability, use a regularized information matrix $G = M_S^\\top M_S + \\lambda I_K$ with $\\lambda  0$ and $I_K$ the $K \\times K$ identity matrix. The greedy selection should build the sensor index set row by row to maximize the determinant of $G$ (determinant optimality, also called $D$-optimality), which reduces the estimator covariance volume for $\\alpha_S$ under AWGN.\n\nYour task is to implement the following components in a single program:\n- Construct the orthonormal Haar wavelet transform matrix $W \\in \\mathbb{R}^{n \\times n}$ consistent with the coefficient ordering described above, by applying the transform to each standard basis vector to form the columns of $W$.\n- For a given support set $S$, implement a greedy $D$-optimal sensor selection algorithm that chooses $m$ distinct sensor locations (row indices of $A$) to maximize $\\log \\det(G)$, where $G = M_S^\\top M_S + \\lambda I_K$ after selecting $m$ sensors.\n- After selecting sensors, compute the minimum singular value $\\sigma_{\\min}(M_S)$ using the Singular Value Decomposition (SVD) and report it.\n- Simulate a $K$-sparse coefficient vector $\\alpha$ supported on $S$ with nonzero entries drawn from a standard normal distribution, form $x = W^\\top \\alpha$, compute measurements $y = A x$, and estimate $\\alpha_S$ via Least Squares (LS) by solving $\\min_{\\hat{\\alpha}_S} \\|M_S \\hat{\\alpha}_S - y\\|_2^2$. Report the relative reconstruction error $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$.\n- Report also the final value of $\\log \\det(G)$ after greedy selection.\n\nDesign choices must be justified from first principles related to compressed sensing and linear measurement models. No shortcut formulas may be introduced in the problem statement; the objective definitions above are the only permitted explicit targets.\n\nUse the following test suite of parameter values. In each case, $A$ selects rows specified by the greedy algorithm, $W$ is the Haar matrix of size $n$, and the support $S$ is given explicitly:\n- Case $1$: $n = 16$, $K = 3$, $m = 6$, $S = \\{1,5,10\\}$, $\\lambda = 10^{-6}$, random seed $123$.\n- Case $2$: $n = 16$, $K = 4$, $m = 4$ (boundary where $m = K$), $S = \\{2,4,9,12\\}$, $\\lambda = 10^{-6}$, random seed $7$.\n- Case $3$: $n = 32$, $K = 4$, $m = 8$, $S = \\{1,3,5,20\\}$, $\\lambda = 10^{-6}$, random seed $42$.\n\nFor each case, your program must output a list containing:\n- The selected sensor indices as an integer list in ascending order.\n- The minimum singular value $\\sigma_{\\min}(M_S)$ as a float.\n- The relative reconstruction error as a float.\n- The final $\\log \\det(G)$ value as a float.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-case result is formatted as its own bracketed comma-separated list with no spaces. For example, the output must be of the form $[ [\\text{case1}], [\\text{case2}], [\\text{case3}] ]$, specifically as $[[\\text{indices},\\sigma_{\\min},\\text{error},\\log\\det],\\dots]$ with no spaces anywhere in the line.",
            "solution": "The problem statement poses a well-defined task in the domain of optimal experiment design for sparse recovery, a subfield of compressed sensing. It asks for the implementation of a greedy sensor selection algorithm based on the principle of D-optimality, applied to a linear measurement model of a state that is sparse in the Haar wavelet domain. The problem is scientifically sound, self-contained, and algorithmically specified. All parameters are provided, and the objectives are quantifiable. Therefore, the problem is deemed valid and a solution can be constructed.\n\nThe solution will be developed in a principled, step-by-step manner.\n\n**1. Orthonormal Haar Wavelet Transform Matrix ($W$)**\nThe problem specifies that the state vector $x \\in \\mathbb{R}^n$ has a sparse representation in an orthonormal Haar wavelet basis. The relationship is given by $x = W^\\top \\alpha$, where $\\alpha \\in \\mathbb{R}^n$ is the $K$-sparse coefficient vector and $W \\in \\mathbb{R}^{n \\times n}$ is the orthonormal wavelet transform matrix. From this, the inverse relationship is $\\alpha = W x$, as $W$ being orthonormal implies $W^{-1} = W^\\top$.\n\nThe matrix $W$ is constructed such that its $j$-th row, $W_{j,:}$, is the synthesis basis vector for the $j$-th Haar coefficient, and its $i$-th column, $W_{:,i}$, is the vector of Haar coefficients for the $i$-th standard basis vector $e_i$. The problem requires constructing $W$ by applying the transform to each $e_i$ to form the columns of $W$.\n\nThe specified coefficient ordering is: index $0$ for the final scaling coefficient, followed by detail coefficients from the coarsest to the finest scale. For $n=2^L$, this corresponds to a coefficient vector structured as $[c_0, d_0, (d_{1,0}, d_{1,1}), \\dots, (d_{L-1,0}, \\dots, d_{L-1, 2^{L-1}-1})]$. This is a standard \"pyramid\" ordering.\n\nThe 1D orthonormal Haar wavelet transform is implemented by iteratively computing averages and differences. For a vector segment of length $2p$, we compute $p$ averages and $p$ differences.\nThe average of a pair $(a, b)$ is $(a+b)/\\sqrt{2}$, and the difference is $(a-b)/\\sqrt{2}$. The factor of $1/\\sqrt{2}$ ensures orthonormality. This process is applied recursively to the vector of averages until only one scaling coefficient remains.\n\n**2. Greedy Sensor Selection via D-Optimality**\nThe core of the problem is to select $m$ sensor locations out of a possible $n$. Each sensor corresponds to a pointwise measurement, so selecting a sensor at location $p$ is equivalent to selecting the $p$-th row of the identity matrix to form a row of the measurement matrix $A$. The measurement model is $y=Ax$. Substituting the sparsity model, we get $y = A W^\\top \\alpha$.\nSince $\\alpha$ is $K$-sparse on a known support set $S$, we can write $y = (A W^\\top[:,S]) \\alpha_S = M_S \\alpha_S$, where $W^\\top[:,S]$ is the submatrix of $W^\\top$ with columns indexed by $S$, and $\\alpha_S \\in \\mathbb{R}^K$ contains the non-zero coefficients. The matrix $M_S \\in \\mathbb{R}^{m \\times K}$ maps the non-zero coefficients to the measurements.\n\nThe quality of the sensor placement is evaluated by its ability to facilitate an accurate estimation of $\\alpha_S$. For a linear model with additive white Gaussian noise, the Fisher information matrix for $\\alpha_S$ is proportional to $M_S^\\top M_S$. D-optimality seeks to maximize the determinant of this information matrix, which is geometrically equivalent to minimizing the volume of the confidence ellipsoid of the least-squares estimator for $\\alpha_S$.\n\nWe use a regularized information matrix, $G = M_S^\\top M_S + \\lambda I_K$, where $\\lambda  0$ is a regularization parameter ensuring that $G$ is always well-conditioned and invertible. The objective is to select the $m$ rows of $A$ (i.e., sensor locations) that maximize $\\log \\det(G)$.\n\nA greedy algorithm is employed to solve this combinatorial optimization problem. Starting with an empty set of sensors, we iteratively add the sensor that provides the maximum marginal increase in $\\log \\det(G)$. Let $\\mathcal{P}_{k-1}$ be the set of $k-1$ selected sensor indices. The corresponding information matrix is $G_{k-1}$. To select the $k$-th sensor, we evaluate each candidate location $p \\notin \\mathcal{P}_{k-1}$. Adding sensor $p$ corresponds to appending the row vector $v_p = (W^\\top[:,S])_{p,:}$ to the current measurement sub-matrix. The new information matrix becomes $G_k = G_{k-1} + v_p^\\top v_p$. We select the sensor $p^*$ that maximizes $\\log\\det(G_k)$. This process repeats $m$ times. The use of $\\log \\det$ improves numerical stability.\n\n**3. Reconstruction and Performance Evaluation**\nAfter selecting the $m$ sensor locations, we form the final measurement matrix $M_S \\in \\mathbb{R}^{m \\times K}$.\nThe quality of this matrix is partially characterized by its singular values. The minimum singular value, $\\sigma_{\\min}(M_S)$, is a measure of the worst-case amplification of noise and is related to the condition number of the matrix. A small $\\sigma_{\\min}(M_S)$ indicates that $M_S$ is close to being rank-deficient, which compromises estimation stability.\n\nTo test the reconstruction performance, we first simulate a ground truth. A $K$-sparse coefficient vector $\\alpha$ is created with its non-zero entries on the given support $S$ drawn from a standard normal distribution. The physical state is synthesized as $x=W^\\top\\alpha$. Noiseless measurements are then computed as $y = Ax$.\n\nThe coefficient vector $\\alpha_S$ is estimated from the measurements $y$ by solving the linear least-squares problem:\n$$ \\hat{\\alpha}_S = \\arg\\min_{\\beta \\in \\mathbb{R}^K} \\| M_S \\beta - y \\|_2^2 $$\nThe solution is given by $\\hat{\\alpha}_S = (M_S^\\top M_S)^{-1} M_S^\\top y$. Numerically, this is best solved using robust methods like those based on QR decomposition or SVD, as implemented in `numpy.linalg.lstsq`.\n\nThe performance is quantified by the relative reconstruction error, $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$.\n\nThe final algorithm proceeds by first constructing the Haar matrix $W$, then running the greedy selection loop for $m$ steps. In each step, every available sensor location is tested, and the one maximizing the log-determinant of the updated regularized information matrix $G$ is chosen. Finally, after the selection is complete, the required metrics—the list of selected sensor indices, $\\sigma_{\\min}(M_S)$, the reconstruction error, and the final $\\log\\det(G)$—are computed and reported.",
            "answer": "```python\nimport numpy as np\n\ndef _haar_1d_transform(v):\n    \"\"\"\n    Computes the 1D orthonormal Haar wavelet transform of a vector.\n    The coefficient ordering is: final scaling coefficient, followed by\n    detail coefficients from the coarsest to the finest scale.\n    \"\"\"\n    n = len(v)\n    if n == 1:\n        return v.astype(np.float64)\n    \n    if n  (n - 1) != 0 or n == 0:\n        raise ValueError(\"Input vector length must be a power of 2.\")\n\n    temp_v = v.astype(np.float64)\n    coeffs = np.zeros(n, dtype=np.float64)\n    \n    current_len = n\n    while current_len  1:\n        next_len = current_len // 2\n        averages = (temp_v[0:current_len:2] + temp_v[1:current_len:2]) / np.sqrt(2)\n        details = (temp_v[0:current_len:2] - temp_v[1:current_len:2]) / np.sqrt(2)\n        \n        # Place details in the second half of the current segment in the output array.\n        # This naturally orders them from finest to coarsest as current_len decreases.\n        # Here we place them to match the problem statement.\n        coeffs[next_len:current_len] = details\n        temp_v[:next_len] = averages\n        current_len = next_len\n\n    coeffs[0] = temp_v[0]\n    return coeffs\n\ndef construct_haar_matrix(n):\n    \"\"\"\n    Constructs the orthonormal Haar wavelet transform matrix W of size n x n.\n    The columns of W are the transforms of the standard basis vectors.\n    \"\"\"\n    W = np.zeros((n, n), dtype=np.float64)\n    for i in range(n):\n        e_i = np.zeros(n)\n        e_i[i] = 1.0\n        W[:, i] = _haar_1d_transform(e_i)\n    return W\n\ndef solve():\n    \"\"\"\n    Main function to solve the sensor selection problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 16, 'K': 3, 'm': 6, 'S': {1, 5, 10}, 'lambda': 1e-6, 'seed': 123},\n        {'n': 16, 'K': 4, 'm': 4, 'S': {2, 4, 9, 12}, 'lambda': 1e-6, 'seed': 7},\n        {'n': 32, 'K': 4, 'm': 8, 'S': {1, 3, 5, 20}, 'lambda': 1e-6, 'seed': 42},\n    ]\n\n    results_for_all_cases = []\n\n    for case in test_cases:\n        n, K, m, S_set, lambda_reg, seed = case['n'], case['K'], case['m'], case['S'], case['lambda'], case['seed']\n        S = list(S_set)\n\n        # 1. Construct Haar wavelet matrix W\n        W = construct_haar_matrix(n)\n        Psi_S = W.T[:, S]\n\n        # 2. Greedy D-optimal sensor selection\n        selected_indices = []\n        available_indices = list(range(n))\n        \n        # Initialize regularized information matrix G\n        G = lambda_reg * np.eye(K)\n        \n        current_log_det = np.linalg.slogdet(G)[1]\n\n        for _ in range(m):\n            best_p = -1\n            best_log_det = -np.inf\n            \n            for p in available_indices:\n                v_p = Psi_S[p, :]\n                \n                # Rank-1 update to G\n                G_candidate = G + np.outer(v_p, v_p)\n                \n                # Using slogdet for numerical stability\n                sign, log_det_candidate = np.linalg.slogdet(G_candidate)\n                \n                if sign  0 and log_det_candidate  best_log_det:\n                    best_log_det = log_det_candidate\n                    best_p = p\n\n            selected_indices.append(best_p)\n            available_indices.remove(best_p)\n            \n            # Update G and log_det for the next iteration\n            v_best = Psi_S[best_p, :]\n            G += np.outer(v_best, v_best)\n            current_log_det = best_log_det\n        \n        final_log_det = current_log_det\n        selected_indices.sort()\n\n        # 3. Construct final M_S and compute minimum singular value\n        M_S = Psi_S[selected_indices, :]\n        if M_S.shape[0]  0:\n            singular_values = np.linalg.svd(M_S, compute_uv=False)\n            sigma_min = singular_values.min()\n        else:\n            sigma_min = 0.0\n\n        # 4. Simulate and reconstruct\n        np.random.seed(seed)\n        alpha = np.zeros(n)\n        alpha_S_true = np.random.randn(K)\n        alpha[S] = alpha_S_true\n        \n        x = W.T @ alpha\n        \n        # Measurement matrix A is implicit\n        y = x[selected_indices]\n        \n        # Estimate alpha_S via Least Squares\n        alpha_S_hat, _, _, _ = np.linalg.lstsq(M_S, y, rcond=None)\n        \n        # Compute relative reconstruction error\n        norm_true = np.linalg.norm(alpha_S_true)\n        if norm_true == 0:\n            error = 0.0 if np.linalg.norm(alpha_S_hat) == 0 else np.inf\n        else:\n            error = np.linalg.norm(alpha_S_hat - alpha_S_true) / norm_true\n\n        # Format results for the current case\n        indices_str = f\"[{','.join(map(str, selected_indices))}]\"\n        case_result_str = f\"[{indices_str},{sigma_min},{error},{final_log_det}]\"\n        results_for_all_cases.append(case_result_str)\n\n    # Final print statement in the exact required format with no spaces\n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond recovering a single sparse vector, many modern applications require estimating multiple signals that share a common sparsity pattern. This coding practice  explores this multi-task learning scenario, demonstrating how structured sparsity can be promoted using mixed-norm regularization. By implementing and comparing an $\\ell_{2,1}$-regularized estimator with a standard $\\ell_{2,2}$ (ridge) regression, you will gain hands-on experience with proximal gradient methods and see firsthand the power of exploiting joint sparsity to improve recovery.",
            "id": "3459923",
            "problem": "Consider the multi-task linear measurement model in compressed sensing and sparse optimization with joint sparsity across tasks. Let $A \\in \\mathbb{R}^{m \\times n}$ denote the measurement matrix, let $X \\in \\mathbb{R}^{n \\times T}$ denote the unknown coefficient matrix whose columns correspond to $T$ tasks, and let $Y \\in \\mathbb{R}^{m \\times T}$ denote the observed measurements satisfying the linear model $Y = A X + W$. Assume that the columns of $X$ share a common support: the set of indices of rows of $X$ that are nonzero is the same across all columns. The noise matrix $W$ has columns $w_t \\in \\mathbb{R}^{m}$, $t \\in \\{1,\\dots,T\\}$, where each $w_t$ is zero-mean Gaussian with column-dependent covariance $\\Sigma_t \\in \\mathbb{R}^{m \\times m}$ that is positive definite, i.e., $w_t \\sim \\mathcal{N}(0, \\Sigma_t)$, and $\\Sigma_t \\neq \\Sigma_{t'}$ in general.\n\nStart from the following foundational base:\n- The negative log-likelihood for independent Gaussian noise with covariance $\\Sigma_t$ is, up to additive constants independent of $X$, proportional to $\\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2$, where $x_t$ is the $t$-th column of $X$ and $y_t$ is the $t$-th column of $Y$.\n- Joint sparsity across tasks is promoted by mixed norms that aggregate row-wise magnitudes, such as the mixed $\\ell_{p,q}$ norms.\n\nYou must derive, implement, and compare two estimators for $X$ that differ in the mixed-norm regularizer:\n- Estimator $1$ uses the mixed $\\ell_{2,1}$ norm, defined as $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2$, where $X_{i,:}$ denotes the $i$-th row of $X$.\n- Estimator $2$ uses the mixed $\\ell_{2,2}$ norm, defined as $\\|X\\|_{2,2} = \\left( \\sum_{i=1}^{n} \\|X_{i,:}\\|_2^2 \\right)^{1/2}$, which is proportional to the Frobenius norm and does not impose sparsity.\n\nYour program must:\n- Derive algorithmic steps from the base principles above to compute an $\\ell_{2,1}$-regularized estimate by solving\n$$ \\min_{X \\in \\mathbb{R}^{n \\times T}} \\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\lambda_{2,1} \\|X\\|_{2,1} $$\nusing a principled first-order method with a correct proximal operator for the mixed $\\ell_{2,1}$ norm and a step size justified by the Lipschitz continuity of the gradient of the smooth term.\n- Derive algorithmic steps from the base principles above to compute an $\\ell_{2,2}$-regularized estimate by solving\n$$ \\min_{X \\in \\mathbb{R}^{n \\times T}} \\sum_{t=1}^{T} \\frac{1}{2} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\frac{\\lambda_{2,2}}{2} \\|X\\|_{F}^2 $$\nwhere $\\|X\\|_{F}$ denotes the Frobenius norm, using a correct linear algebraic solution justified by optimality conditions.\n\nJoint support recovery evaluation must be performed by identifying the support as the set of row indices whose row-wise $\\ell_2$ norm across tasks is among the largest $s$ values, where $s$ is the true sparsity level (the number of nonzero rows in the ground-truth $X$). For $s = 0$, the true support is empty and the recovered support is empty if all row norms are below any positive threshold; in this case, define the recovered support as the empty set. For each estimator, declare support recovery success if and only if the recovered support set matches the true support set exactly.\n\nThe test suite consists of three synthetic cases with scientifically consistent and reproducible parameters. In all cases, construct $A$ with independent standard normal entries and normalize its columns to unit $\\ell_2$ norm. Construct $X$ with a common random support of cardinality $s$ and nonzero rows drawn independently across tasks with amplitude specified below. For each task $t$, generate $\\Sigma_t$ as diagonal with entries sampled uniformly from the specified range, and generate $w_t$ as Gaussian with covariance $\\Sigma_t$. Let $Y = A X + W$.\n\nTest case $1$ (happy path):\n- Dimensions: $m = 40$, $n = 80$, $T = 3$.\n- Sparsity: $s = 6$.\n- Amplitude of nonzero rows: $1.5$.\n- Noise covariance ranges per task: diagonal entries in $[0.3, 1.2]$.\n- Regularization parameters: $\\lambda_{2,1} = 0.25$, $\\lambda_{2,2} = 0.05$.\n- Random seed: $123456$.\n\nTest case $2$ (boundary case $s = 0$):\n- Dimensions: $m = 50$, $n = 70$, $T = 2$.\n- Sparsity: $s = 0$.\n- Amplitude of nonzero rows: $0.0$.\n- Noise covariance ranges per task: diagonal entries in $[0.5, 1.0]$.\n- Regularization parameters: $\\lambda_{2,1} = 0.30$, $\\lambda_{2,2} = 0.10$.\n- Random seed: $123457$.\n\nTest case $3$ (edge case with strongly anisotropic, column-dependent noise):\n- Dimensions: $m = 30$, $n = 90$, $T = 4$.\n- Sparsity: $s = 5$.\n- Amplitude of nonzero rows: $1.2$.\n- Noise covariance ranges per task: diagonal entries in $[0.1, 3.0]$.\n- Regularization parameters: $\\lambda_{2,1} = 0.20$, $\\lambda_{2,2} = 0.05$.\n- Random seed: $123458$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to a test case and must be a two-element list of booleans $[b_{2,1}, b_{2,2}]$, where $b_{2,1}$ is $true$ if the $\\ell_{2,1}$ estimator recovers the exact support and $false$ otherwise, and $b_{2,2}$ is $true$ if the $\\ell_{2,2}$ estimator recovers the exact support and $false$ otherwise. For example, an output might look like $[[true,false],[true,true],[false,false]]$. Note that your program must not print any additional text beyond this single line.",
            "solution": "The user requests the derivation, implementation, and comparison of two estimators for a multi-task linear model with joint sparsity, evaluated on their ability to recover the correct sparse support. The model is $Y = AX + W$, where $X \\in \\mathbb{R}^{n \\times T}$ is the signal matrix to be recovered, $A \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, and $W$ is a matrix of task-dependent Gaussian noise, with each column $w_t \\sim \\mathcal{N}(0, \\Sigma_t)$.\n\nThe core of the problem is to solve two distinct convex optimization problems. The first estimator uses an $\\ell_{2,1}$-norm regularizer to promote joint (row-wise) sparsity, while the second uses an $\\ell_{2,2}$-norm (Frobenius norm) regularizer, which corresponds to multi-task ridge regression and does not enforce sparsity.\n\n### Estimator 1: $\\ell_{2,1}$-Regularized Estimation (Multi-Task Group LASSO)\n\nThe first estimator, $\\hat{X}_{2,1}$, is the solution to the following optimization problem:\n$$ \\min_{X \\in \\mathbb{R}^{n \\times T}} F(X) = \\underbrace{\\frac{1}{2} \\sum_{t=1}^{T} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2}_{f(X)} + \\underbrace{\\lambda_{2,1} \\|X\\|_{2,1}}_{g(X)} $$\nwhere $x_t$ and $y_t$ are the $t$-th columns of $X$ and $Y$ respectively, and $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_2$ is the mixed $\\ell_{2,1}$ norm.\n\nThis objective function is a sum of a smooth, differentiable, convex function $f(X)$ (the negative log-likelihood) and a non-smooth, convex regularizer $g(X)$. This structure is ideal for a proximal gradient algorithm, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). The ISTA update rule is given by:\n$$ X^{(k+1)} = \\text{prox}_{\\alpha g}\\left(X^{(k)} - \\alpha \\nabla f(X^{(k)})\\right) $$\nwhere $\\alpha$ is the step size.\n\n**1. Gradient of the Smooth Term, $\\nabla f(X)$**\nThe smooth term $f(X)$ can be rewritten as $f(X) = \\frac{1}{2} \\sum_{t=1}^{T} (A x_t - y_t)^T \\Sigma_t^{-1} (A x_t - y_t)$. The objective function is separable with respect to the columns of $X$ in the smooth part. We can find the gradient with respect to each column $x_t$ independently:\n$$ \\nabla_{x_t} f(X) = A^T \\Sigma_t^{-1} (A x_t - y_t) $$\nThe full gradient $\\nabla f(X) \\in \\mathbb{R}^{n \\times T}$ is the matrix whose $t$-th column is $\\nabla_{x_t} f(X)$.\n\n**2. Step Size $\\alpha$ and Lipschitz Constant**\nFor ISTA to converge, the step size $\\alpha$ must satisfy $0  \\alpha \\le 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(X)$. The Lipschitz constant is the largest eigenvalue (spectral norm) of the Hessian of $f(X)$. The Hessian of $f(X)$ with respect to the vectorized variable $\\text{vec}(X)$ is a block-diagonal matrix:\n$$ \\nabla^2 f(\\text{vec}(X)) = \\text{diag}(A^T \\Sigma_1^{-1} A, \\dots, A^T \\Sigma_T^{-1} A) $$\nThe Lipschitz constant $L$ is the maximum of the largest eigenvalues of these blocks:\n$$ L = \\max_{t=1, \\dots, T} \\lambda_{\\max}(A^T \\Sigma_t^{-1} A) $$\nSince $A^T \\Sigma_t^{-1} A = (\\Sigma_t^{-1/2} A)^T (\\Sigma_t^{-1/2} A)$, where $\\Sigma_t^{-1/2}$ is the matrix square root of the inverse covariance, we have $\\lambda_{\\max}(A^T \\Sigma_t^{-1} A) = \\sigma_{\\max}(\\Sigma_t^{-1/2} A)^2$, where $\\sigma_{\\max}(\\cdot)$ denotes the largest singular value (spectral norm). Thus,\n$$ L = \\max_{t=1, \\dots, T} \\sigma_{\\max}(\\Sigma_t^{-1/2} A)^2 $$\nWe choose the step size $\\alpha = 1/L$ for the algorithm.\n\n**3. Proximal Operator for the $\\ell_{2,1}$-norm**\nThe proximal operator of $g(X) = \\lambda_{2,1} \\|X\\|_{2,1}$ with parameter $\\gamma = \\alpha \\lambda_{2,1}$ is defined as:\n$$ \\text{prox}_{\\gamma \\|\\cdot\\|_{2,1}}(Z) = \\arg\\min_{X} \\left( \\gamma \\sum_{i=1}^{n} \\|X_{i,:}\\|_2 + \\frac{1}{2} \\|X - Z\\|_F^2 \\right) $$\nThis problem decouples across the rows of $X$. For each row $i$, we solve:\n$$ \\arg\\min_{u \\in \\mathbb{R}^T} \\left( \\gamma \\|u\\|_2 + \\frac{1}{2} \\|u - z_{i,:}\\|_2^2 \\right) $$\nwhere $u = X_{i,:}$ and $z_{i,:} = Z_{i,:}$. This is the proximal operator of the Euclidean norm, whose solution is known as block soft-thresholding:\n$$ (\\text{prox}_{\\gamma \\|\\cdot\\|_{2,1}}(Z))_{i,:} = \\left(1 - \\frac{\\gamma}{\\|Z_{i,:}\\|_2}\\right)_+ Z_{i,:} = \\max\\left(0, 1 - \\frac{\\gamma}{\\|Z_{i,:}\\|_2}\\right) Z_{i,:} $$\nThis operator shrinks the rows of $Z$ towards the origin, setting rows with an $\\ell_2$ norm less than or equal to $\\gamma$ to zero, thereby inducing row-sparsity.\n\n### Estimator 2: $\\ell_{2,2}$-Regularized Estimation (Multi-Task Ridge Regression)\n\nThe second estimator, $\\hat{X}_{2,2}$, is the solution to the ridge regression problem:\n$$ \\min_{X \\in \\mathbb{R}^{n \\times T}} J(X) = \\frac{1}{2} \\sum_{t=1}^{T} \\left\\| \\Sigma_t^{-1/2} (A x_t - y_t) \\right\\|_2^2 + \\frac{\\lambda_{2,2}}{2} \\|X\\|_{F}^2 $$\nThe Frobenius norm squared is $\\|X\\|_{F}^2 = \\sum_{t=1}^{T} \\|x_t\\|_2^2$. The objective function $J(X)$ is completely separable across the tasks (columns):\n$$ J(X) = \\sum_{t=1}^{T} \\underbrace{\\left( \\frac{1}{2} (A x_t - y_t)^T \\Sigma_t^{-1} (A x_t - y_t) + \\frac{\\lambda_{2,2}}{2} x_t^T x_t \\right)}_{J_t(x_t)} $$\nWe can solve for each column $x_t$ of $X$ independently by minimizing $J_t(x_t)$. Since $J_t(x_t)$ is a strictly convex quadratic function, its unique minimizer is found by setting its gradient to zero.\n$$ \\nabla_{x_t} J_t(x_t) = A^T \\Sigma_t^{-1} (A x_t - y_t) + \\lambda_{2,2} x_t = 0 $$\nRearranging the terms to solve for $x_t$:\n$$ (A^T \\Sigma_t^{-1} A) x_t + \\lambda_{2,2} I_n x_t = A^T \\Sigma_t^{-1} y_t $$\n$$ (A^T \\Sigma_t^{-1} A + \\lambda_{2,2} I_n) x_t = A^T \\Sigma_t^{-1} y_t $$\nwhere $I_n$ is the $n \\times n$ identity matrix. This yields a closed-form solution for each task's coefficient vector by solving a linear system:\n$$ \\hat{x}_t = (A^T \\Sigma_t^{-1} A + \\lambda_{2,2} I_n)^{-1} (A^T \\Sigma_t^{-1} y_t) $$\nThe final estimate $\\hat{X}_{2,2}$ is constructed by concatenating these column vectors: $\\hat{X}_{2,2} = [\\hat{x}_1, \\dots, \\hat{x}_T]$. Unlike the $\\ell_{2,1}$ regularizer, the ridge penalty does not promote sparsity, so the resulting $\\hat{X}_{2,2}$ will generally be dense.\n\n### Support Recovery Evaluation\n\nFor both estimators, the support is recovered following the rule provided. First, we compute the $\\ell_2$ norm of each row of the estimated matrix $\\hat{X}$. Let these norms be $r_i = \\|\\hat{X}_{i,:}\\|_2$ for $i=1, \\dots, n$. The recovered support, $S_{rec}$, is the set of indices corresponding to the $s$ largest row norms, where $s$ is the true sparsity level. The true support, $S_{true}$, is the set of indices of the non-zero rows in the ground-truth matrix $X$. Recovery is deemed a success if and only if $S_{rec} = S_{true}$. For the special case $s=0$, the true support is the empty set. The recovery rule requires selecting the $0$ largest norms, which also results in an empty set. Thus, for $s=0$, support recovery is always successful by this definition.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(m, n, T, s, amplitude, cov_range, seed):\n    \"\"\"Generates synthetic data for one test case.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate measurement matrix A\n    A = rng.standard_normal((m, n))\n    A_norms = np.linalg.norm(A, axis=0)\n    A /= A_norms[np.newaxis, :]\n    \n    # Generate true sparse matrix X\n    X_true = np.zeros((n, T))\n    true_support = set()\n    if s  0:\n        support_indices = rng.choice(n, s, replace=False)\n        true_support = set(support_indices)\n        for i in support_indices:\n            # Nonzero entries are amplitude * random sign, independent across tasks\n            X_true[i, :] = amplitude * rng.choice([-1, 1], size=T)\n            \n    # Generate noise W and observations Y\n    Y = np.zeros((m, T))\n    all_Sigma = []\n    for t in range(T):\n        # Diagonal covariance matrix for task t\n        sigma_diag = rng.uniform(cov_range[0], cov_range[1], size=m)\n        Sigma_t = np.diag(sigma_diag)\n        all_Sigma.append(Sigma_t)\n        \n        # Generate noise w_t ~ N(0, Sigma_t)\n        # w_t = Sigma_t^{1/2} * z, where z ~ N(0, I)\n        z = rng.standard_normal(m)\n        w_t = np.sqrt(sigma_diag) * z\n        Y[:, t] = A @ X_true[:, t] + w_t\n        \n    return A, X_true, Y, all_Sigma, true_support\n\ndef estimate_l21(A, Y, all_Sigma, lambda_21, num_iter=2000):\n    \"\"\"Solves the l2,1-regularized problem using ISTA.\"\"\"\n    m, n = A.shape\n    _, T = Y.shape\n    \n    # Calculate Lipschitz constant L\n    L = 0.0\n    for t in range(T):\n        Sigma_t = all_Sigma[t]\n        sigma_inv_sqrt_diag = 1.0 / np.sqrt(np.diag(Sigma_t))\n        A_tilde_t = sigma_inv_sqrt_diag[:, np.newaxis] * A\n        # spectral norm squared of A_tilde_t\n        s_vals_sq = np.linalg.svd(A_tilde_t, compute_uv=False)**2\n        L = max(L, np.max(s_vals_sq))\n    \n    alpha = 1.0 / L\n    \n    X_est = np.zeros((n, T))\n    \n    for _ in range(num_iter):\n        # Gradient computation\n        grad_f = np.zeros((n, T))\n        for t in range(T):\n            Sigma_inv_diag = 1.0 / np.diag(all_Sigma[t])\n            residual_t = A @ X_est[:, t] - Y[:, t]\n            grad_f[:, t] = A.T @ (Sigma_inv_diag * residual_t)\n        \n        # Gradient descent step\n        Z = X_est - alpha * grad_f\n        \n        # Proximal step (block soft-thresholding)\n        row_norms = np.linalg.norm(Z, axis=1)\n        threshold = alpha * lambda_21\n        \n        # Avoid division by zero\n        shrinkage_factors = np.zeros_like(row_norms)\n        non_zero_norm_indices = row_norms  0\n        \n        shrinkage_factors[non_zero_norm_indices] = np.maximum(\n            0, 1 - threshold / row_norms[non_zero_norm_indices]\n        )\n        \n        X_est = Z * shrinkage_factors[:, np.newaxis]\n        \n    return X_est\n\ndef estimate_l22(A, Y, all_Sigma, lambda_22):\n    \"\"\"Solves the l2,2-regularized problem (ridge).\"\"\"\n    n, T = Y.shape[1], Y.shape[1]\n    m, n = A.shape\n\n    X_est = np.zeros((n, T))\n    I_n = np.eye(n)\n    \n    for t in range(T):\n        Sigma_inv_diag = 1.0 / np.diag(all_Sigma[t])\n        A_T_Sigma_inv = A.T * Sigma_inv_diag[np.newaxis, :]\n        \n        M_t = A_T_Sigma_inv @ A + lambda_22 * I_n\n        b_t = A_T_Sigma_inv @ Y[:, t]\n        \n        X_est[:, t] = np.linalg.solve(M_t, b_t)\n        \n    return X_est\n\ndef evaluate_support(X_est, true_support, s):\n    \"\"\"Evaluates support recovery success.\"\"\"\n    if s == 0:\n        # Per problem spec, recovered support for s=0 is the empty set\n        recovered_support = set()\n    else:\n        row_norms = np.linalg.norm(X_est, axis=1)\n        # Indices of s largest norms\n        recovered_support = set(np.argsort(row_norms)[-s:])\n        \n    return recovered_support == true_support\n\ndef solve():\n    test_cases = [\n        {\n            \"m\": 40, \"n\": 80, \"T\": 3, \"s\": 6, \"amp\": 1.5,\n            \"cov_range\": [0.3, 1.2], \"lambda_21\": 0.25, \"lambda_22\": 0.05,\n            \"seed\": 123456\n        },\n        {\n            \"m\": 50, \"n\": 70, \"T\": 2, \"s\": 0, \"amp\": 0.0,\n            \"cov_range\": [0.5, 1.0], \"lambda_21\": 0.30, \"lambda_22\": 0.10,\n            \"seed\": 123457\n        },\n        {\n            \"m\": 30, \"n\": 90, \"T\": 4, \"s\": 5, \"amp\": 1.2,\n            \"cov_range\": [0.1, 3.0], \"lambda_21\": 0.20, \"lambda_22\": 0.05,\n            \"seed\": 123458\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, X_true, Y, all_Sigma, true_support = generate_data(\n            case[\"m\"], case[\"n\"], case[\"T\"], case[\"s\"], case[\"amp\"],\n            case[\"cov_range\"], case[\"seed\"]\n        )\n        \n        # Estimator 1: l2,1 regularization\n        X_est_21 = estimate_l21(A, Y, all_Sigma, case[\"lambda_21\"])\n        success_21 = evaluate_support(X_est_21, true_support, case[\"s\"])\n        \n        # Estimator 2: l2,2 regularization\n        X_est_22 = estimate_l22(A, Y, all_Sigma, case[\"lambda_22\"])\n        success_22 = evaluate_support(X_est_22, true_support, case[\"s\"])\n        \n        results.append([success_21, success_22])\n\n    # Format the final output string exactly as required\n    formatted_results = [f\"[{str(r[0]).lower()},{str(r[1]).lower()}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}