{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of sparse representation is the idea that a signal can be uniquely and sparsely described. This exercise explores the most fundamental algebraic condition for such uniqueness: the **spark** of a dictionary. By calculating the **spark** and investigating its relationship with the **nullspace**, you will derive from first principles why some sparse representations are guaranteed to be unique while others are not .",
            "id": "3434598",
            "problem": "Let $\\Phi \\in \\mathbb{R}^{2 \\times 4}$ be the dictionary matrix\n$$\n\\Phi=\\begin{bmatrix}\n1  0  1  1\\\\\n0  1  1  -1\n\\end{bmatrix},\n$$\nwhose columns $\\phi_{1},\\phi_{2},\\phi_{3},\\phi_{4} \\in \\mathbb{R}^{2}$ are used to form sparse representations $y=\\Phi x$ with coefficient vectors $x \\in \\mathbb{R}^{4}$. The number of nonzero entries of a vector $x$ is its $\\ell_{0}$ pseudo-norm (cardinality), and a vector $x$ is called $k$-sparse if it has at most $k$ nonzero entries. The spark of a matrix, denoted $\\mathrm{spark}(\\Phi)$, is defined as the smallest number of columns of $\\Phi$ that are linearly dependent.\n\nUsing only foundational linear-algebraic definitions and facts, do the following:\n- Compute $\\mathrm{spark}(\\Phi)$ by reasoning from the definition and the ambient dimension constraint.\n- Based on the definition of spark and the notion of $k$-sparse representations, determine whether every $1$-sparse representation $y=\\Phi x$ is unique, and whether non-unique $2$-sparse representations can exist. Justify your conclusions purely from first principles and, if non-uniqueness is possible for $k=2$, exhibit explicit distinct $2$-sparse representations of the same $y$.\n\nProvide the value of $\\mathrm{spark}(\\Phi)$ as your final answer. No rounding is required.",
            "solution": "The problem is validated as scientifically sound, well-posed, and objective. It is a standard exercise in the fundamentals of sparse representation theory.\n\nThe problem asks for three things: the computation of the spark of a given matrix $\\Phi$, an analysis of the uniqueness of $1$-sparse representations, and an analysis of the uniqueness of $2$-sparse representations, all justified from first principles.\n\nThe given dictionary matrix is $\\Phi \\in \\mathbb{R}^{2 \\times 4}$, with columns $\\phi_1, \\phi_2, \\phi_3, \\phi_4 \\in \\mathbb{R}^{2}$:\n$$\n\\Phi = \\begin{bmatrix} \\phi_1  \\phi_2  \\phi_3  \\phi_4 \\end{bmatrix} = \\begin{bmatrix}\n1  0  1  1\\\\\n0  1  1  -1\n\\end{bmatrix}\n$$\n\n**Part 1: Computation of $\\mathrm{spark}(\\Phi)$**\n\nThe spark of a matrix $\\Phi$, denoted $\\mathrm{spark}(\\Phi)$, is defined as the smallest number of columns of $\\Phi$ that are linearly dependent. We will examine subsets of columns of increasing size.\n\n1.  **Subsets of size $1$**: A single column vector $\\phi_i$ is linearly dependent if and only if it is the zero vector. The columns of $\\Phi$ are:\n    $$\n    \\phi_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\n    \\phi_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\n    \\phi_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad\n    \\phi_4 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n    $$\n    None of these vectors is the zero vector. Therefore, no subset of size $1$ is linearly dependent. This implies $\\mathrm{spark}(\\Phi) > 1$.\n\n2.  **Subsets of size $2$**: A set of two vectors $\\{\\phi_i, \\phi_j\\}$ in $\\mathbb{R}^2$ with $i \\neq j$ is linearly dependent if and only if one is a scalar multiple of the other, or equivalently, if the determinant of the matrix formed by these two columns is zero. We check all $\\binom{4}{2}=6$ pairs:\n    -   $\\det(\\begin{bmatrix} \\phi_1  \\phi_2 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}) = 1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_1  \\phi_3 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}) = 1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_1  \\phi_4 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  1 \\\\ 0  -1 \\end{bmatrix}) = -1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_2  \\phi_3 \\end{bmatrix}) = \\det(\\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix}) = -1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_2  \\phi_4 \\end{bmatrix}) = \\det(\\begin{bmatrix} 0  1 \\\\ 1  -1 \\end{bmatrix}) = -1 \\neq 0$.\n    -   $\\det(\\begin{bmatrix} \\phi_3  \\phi_4 \\end{bmatrix}) = \\det(\\begin{bmatrix} 1  1 \\\\ 1  -1 \\end{bmatrix}) = -1 - 1 = -2 \\neq 0$.\n    Since no determinant is zero, every pair of columns is linearly independent. Thus, $\\mathrm{spark}(\\Phi) > 2$.\n\n3.  **Subsets of size $3$**: The columns of $\\Phi$ are vectors in the ambient space $\\mathbb{R}^2$. A fundamental theorem of linear algebra states that any set of $p$ vectors in an $m$-dimensional vector space is linearly dependent if $p > m$. Here, the dimension of the space is $m=2$. For any subset of $p=3$ columns, we have $3 > 2$. Therefore, any set of $3$ columns of $\\Phi$ must be linearly dependent.\n\nSince we have established that $\\mathrm{spark}(\\Phi) > 2$ and that any set of $3$ columns is linearly dependent, the smallest number of linearly dependent columns must be exactly $3$.\nTherefore, $\\mathrm{spark}(\\Phi) = 3$.\n\n**Part 2: Uniqueness of Sparse Representations**\n\nA representation $y = \\Phi x$ is non-unique if there exist two distinct coefficient vectors $x_1, x_2$ such that $\\Phi x_1 = \\Phi x_2 = y$. This is equivalent to $\\Phi(x_1 - x_2) = 0$, where $z = x_1 - x_2$ is a non-zero vector in the null space of $\\Phi$. The vector $z$ represents a linear dependency among the columns of $\\Phi$ corresponding to its non-zero entries. By the definition of spark, the minimum number of non-zero entries in any such vector $z$ is precisely $\\mathrm{spark}(\\Phi)$. Therefore, for any $z \\in \\mathrm{null}(\\Phi), z \\neq 0$, we must have $\\|z\\|_0 \\ge \\mathrm{spark}(\\Phi) = 3$.\n\n**Uniqueness for $1$-sparse representations ($k=1$):**\nA vector $x$ is $1$-sparse if it has at most one non-zero entry, i.e., $\\|x\\|_0 \\le 1$.\nAssume, for the sake of contradiction, that there exist two distinct $1$-sparse vectors, $x_1$ and $x_2$, such that $\\Phi x_1 = \\Phi x_2 = y$.\nSince $x_1$ and $x_2$ are distinct, their difference $z = x_1 - x_2$ is a non-zero vector in the null space of $\\Phi$.\nThe number of non-zero entries in $z$ is bounded by the sum of the non-zero entries in $x_1$ and $x_2$:\n$$\n\\|z\\|_0 = \\|x_1 - x_2\\|_0 \\le \\|x_1\\|_0 + \\|x_2\\|_0\n$$\nSince $x_1$ and $x_2$ are $1$-sparse, $\\|x_1\\|_0 \\le 1$ and $\\|x_2\\|_0 \\le 1$. Thus,\n$$\n\\|z\\|_0 \\le 1 + 1 = 2\n$$\nHowever, as established earlier, any non-zero vector $z$ in the null space of $\\Phi$ must satisfy $\\|z\\|_0 \\ge \\mathrm{spark}(\\Phi) = 3$. This gives the contradiction $3 \\le \\|z\\|_0 \\le 2$.\nTherefore, our initial assumption must be false. No two distinct $1$-sparse vectors can produce the same representation $y$. Every $1$-sparse representation is unique.\n\n**Non-uniqueness for $2$-sparse representations ($k=2$):**\nA vector $x$ is $2$-sparse if $\\|x\\|_0 \\le 2$. We investigate whether non-unique $2$-sparse representations can exist.\nLet us again consider two distinct $2$-sparse vectors $x_1, x_2$ giving the same $y$. Their difference $z = x_1 - x_2$ must be a non-zero vector in $\\mathrm{null}(\\Phi)$.\nThe bound on the sparsity of $z$ is now:\n$$\n\\|z\\|_0 \\le \\|x_1\\|_0 + \\|x_2\\|_0 \\le 2 + 2 = 4\n$$\nCombining with the spark condition, we require a null space vector $z \\neq 0$ such that $3 \\le \\|z\\|_0 \\le 4$. This is not a contradiction, so non-unique $2$-sparse representations may exist. To confirm this, we must exhibit an explicit example.\n\nWe seek a non-zero vector $z = (z_1, z_2, z_3, z_4)^T$ such that $\\Phi z = 0$. This gives the system:\n$$\nz_1 + z_3 + z_4 = 0 \\\\\nz_2 + z_3 - z_4 = 0\n$$\nA simple non-zero solution can be found by observing a linear dependency among the columns. By inspection, $\\phi_3 = \\phi_1 + \\phi_2$. This can be rewritten as $1 \\cdot \\phi_1 + 1 \\cdot \\phi_2 - 1 \\cdot \\phi_3 + 0 \\cdot \\phi_4 = 0$. This corresponds to a vector $z = (1, 1, -1, 0)^T$ in the null space of $\\Phi$. Note that $\\|z\\|_0=3$, which is consistent with $\\mathrm{spark}(\\Phi)=3$.\n\nNow we must decompose $z$ as $z = x_1 - x_2$ where $x_1$ and $x_2$ are distinct $2$-sparse vectors.\nLet us define $x_1$ and $x_2$ based on the positive and negative entries of $z$.\nLet $x_1 = (1, 1, 0, 0)^T$. This vector is $2$-sparse since $\\|x_1\\|_0 = 2$.\nLet $x_2 = (0, 0, 1, 0)^T$. This vector is $1$-sparse since $\\|x_2\\|_0 = 1$. Since $1 \\le 2$, $x_2$ is also a $2$-sparse vector.\nThe vectors $x_1$ and $x_2$ are distinct.\nWe check that they generate the same signal $y$:\n$$\ny_1 = \\Phi x_1 = 1 \\cdot \\phi_1 + 1 \\cdot \\phi_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\ny_2 = \\Phi x_2 = 1 \\cdot \\phi_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nSince $y_1 = y_2$, we have found two distinct $2$-sparse vectors, $x_1$ and $x_2$, that produce the same representation $y = (1, 1)^T$.\nThis demonstrates that non-unique $2$-sparse representations can and do exist for this dictionary $\\Phi$.",
            "answer": "$$\n\\boxed{3}\n$$"
        },
        {
            "introduction": "Sparsity is not an absolute property of a signal; it is critically dependent on the chosen basis or dictionary. This practice provides a powerful counterexample to demonstrate that a signal with a very sparse representation in one basis can be completely dense in another . This exercise highlights the crucial role the dictionary plays in transforming sparsity and why theoretical guarantees cannot be carelessly transferred between different domains.",
            "id": "3434623",
            "problem": "Let $\\Phi \\in \\mathbb{R}^{n \\times n}$ denote an invertible dictionary, and let a signal be represented as $x = \\Phi \\alpha$ with coefficient vector $\\alpha \\in \\mathbb{R}^{n}$. The standard notion of sparsity in the canonical domain refers to the number of nonzero entries in $x$, quantified by the $\\ell_{0}$ pseudo-norm $\\|x\\|_{0}$. In contrast, sparsity in the dictionary domain refers to the number of nonzero entries in $\\alpha$ for a given $\\Phi$. Consider a sensing matrix $A \\in \\mathbb{R}^{n \\times n}$ that is claimed to satisfy the Restricted Isometry Property (RIP) of order $k$ in the canonical domain, meaning there exists $\\delta_{k} \\in (0,1)$ such that for all $k$-sparse $x \\in \\mathbb{R}^{n}$,\n$$\n(1 - \\delta_{k}) \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_{k}) \\|x\\|_{2}^{2}.\n$$\nStarting from these definitions, construct a concrete counterexample that demonstrates how right multiplication by a dense, ill-conditioned, invertible matrix $\\Phi$ can map a $k$-sparse coefficient vector $\\alpha$ to a dense signal $x = \\Phi \\alpha$, thereby invalidating any RIP-based guarantee stated purely in the canonical domain.\n\nTo that end, work with the following explicit and scientifically consistent setup:\n- Take $n = 4$.\n- Let $A = I_{4}$, the $4 \\times 4$ identity matrix.\n- Let $\\Phi$ be the $4 \\times 4$ Hilbert matrix with entries $\\Phi_{ij} = \\frac{1}{i + j - 1}$ for $i,j \\in \\{1,2,3,4\\}$. This $\\Phi$ is dense, invertible, and ill-conditioned.\n- Let $\\alpha = e_{1} = (1,0,0,0)^{\\top}$, which is $1$-sparse in the dictionary domain.\n\nCompute $x = \\Phi \\alpha$ exactly from these specifications, apply the fundamental definition of support and the $\\ell_{0}$ pseudo-norm, and report the value of $\\|x\\|_{0}$. No rounding is required. Your answer must be the single number $\\|x\\|_{0}$.",
            "solution": "The problem is valid. It presents a well-defined mathematical task based on established principles in linear algebra and compressed sensing. All necessary parameters are provided, and the objective is unambiguous.\n\nThe problem asks for the construction of a specific counterexample to illustrate a key concept in sparse signal processing. The goal is to show that a signal sparse in a dictionary basis may not be sparse in the canonical basis, which has implications for guarantees based on the Restricted Isometry Property (RIP). We are asked to compute the canonical sparsity of a signal $x$ generated from a sparse coefficient vector $\\alpha$ via a dense dictionary $\\Phi$.\n\nThe problem provides the following specifications:\nThe dimension of the space is $n=4$.\nThe dictionary $\\Phi \\in \\mathbb{R}^{4 \\times 4}$ is the $4 \\times 4$ Hilbert matrix, whose entries are given by the formula $\\Phi_{ij} = \\frac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, 3, 4\\}$.\nThe coefficient vector $\\alpha \\in \\mathbb{R}^{4}$ is the first standard basis vector, $\\alpha = e_1 = (1, 0, 0, 0)^{\\top}$.\nThe signal in the canonical basis is given by the transformation $x = \\Phi \\alpha$.\nWe are tasked with computing the sparsity of $x$ in the canonical domain, which is given by the $\\ell_0$ pseudo-norm, $\\|x\\|_0$.\n\nFirst, let us explicitly construct the Hilbert matrix $\\Phi$ for $n=4$:\n$$\n\\Phi = \\begin{pmatrix}\n\\frac{1}{1+1-1}  \\frac{1}{1+2-1}  \\frac{1}{1+3-1}  \\frac{1}{1+4-1} \\\\\n\\frac{1}{2+1-1}  \\frac{1}{2+2-1}  \\frac{1}{2+3-1}  \\frac{1}{2+4-1} \\\\\n\\frac{1}{3+1-1}  \\frac{1}{3+2-1}  \\frac{1}{3+3-1}  \\frac{1}{3+4-1} \\\\\n\\frac{1}{4+1-1}  \\frac{1}{4+2-1}  \\frac{1}{4+3-1}  \\frac{1}{4+4-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1  \\frac{1}{2}  \\frac{1}{3}  \\frac{1}{4} \\\\\n\\frac{1}{2}  \\frac{1}{3}  \\frac{1}{4}  \\frac{1}{5} \\\\\n\\frac{1}{3}  \\frac{1}{4}  \\frac{1}{5}  \\frac{1}{6} \\\\\n\\frac{1}{4}  \\frac{1}{5}  \\frac{1}{6}  \\frac{1}{7}\n\\end{pmatrix}\n$$\nThe coefficient vector $\\alpha$ is given as:\n$$\n\\alpha = e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe sparsity of this vector in its own domain (the dictionary domain) is $\\|\\alpha\\|_0 = 1$, as it has only one non-zero entry.\n\nNext, we compute the signal vector $x$ using the given transformation $x = \\Phi \\alpha$:\n$$\nx = \\Phi \\alpha = \\begin{pmatrix}\n1  \\frac{1}{2}  \\frac{1}{3}  \\frac{1}{4} \\\\\n\\frac{1}{2}  \\frac{1}{3}  \\frac{1}{4}  \\frac{1}{5} \\\\\n\\frac{1}{3}  \\frac{1}{4}  \\frac{1}{5}  \\frac{1}{6} \\\\\n\\frac{1}{4}  \\frac{1}{5}  \\frac{1}{6}  \\frac{1}{7}\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe multiplication of a matrix by the first standard basis vector $e_1$ yields the first column of that matrix. Therefore, the resulting vector $x$ is:\n$$\nx = \\begin{pmatrix}\n1 \\\\\n\\frac{1}{2} \\\\\n\\frac{1}{3} \\\\\n\\frac{1}{4}\n\\end{pmatrix}\n$$\nThe final step is to compute the $\\ell_0$ pseudo-norm of $x$, denoted $\\|x\\|_0$. The $\\ell_0$ pseudo-norm is defined as the number of non-zero elements in the vector. The components of $x$ are $x_1=1$, $x_2=\\frac{1}{2}$, $x_3=\\frac{1}{3}$, and $x_4=\\frac{1}{4}$. All four of these components are non-zero.\nTherefore, the number of non-zero entries in $x$ is $4$.\n$$\n\\|x\\|_0 = 4\n$$\nThis result demonstrates the core of the counterexample. A vector $\\alpha$ that is $1$-sparse in the dictionary domain is mapped to a vector $x$ that is $4$-sparse (i.e., fully dense) in the canonical domain. Any RIP-based guarantee that relies on the canonical sparsity of the signal would not apply in the manner one might naively expect from the sparsity of $\\alpha$. For instance, an RIP of order $k=1$ for the sensing matrix $A=I_4$ is not applicable to the signal $x$, because $x$ is not $1$-sparse. This highlights the fact that RIP guarantees are basis-dependent.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "While **spark** provides a combinatorial condition for uniqueness, the Restricted Isometry Property (RIP) offers a more robust, geometric guarantee for the stable recovery of sparse signals. This exercise provides a concrete, hands-on opportunity to compute the Restricted Isometry Constant (RIC) $\\delta_k$ directly from its definition . By systematically analyzing all submatrices, you will gain a practical understanding of how the RIC quantifies how well a matrix preserves the geometry of sparse vectors.",
            "id": "3434608",
            "problem": "Consider the linear measurements model in compressed sensing, where a measurement matrix $A \\in \\mathbb{R}^{3 \\times 4}$ and a representation basis $\\Phi \\in \\mathbb{R}^{4 \\times 4}$ act on coefficient vectors $x \\in \\mathbb{R}^{4}$. Let the matrix $A$ and the basis $\\Phi$ be given by\n$$\nA=\\frac{1}{2}\\begin{bmatrix}\n1  0  1  -1 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{bmatrix}, \\qquad \\Phi=I_4,\n$$\nwhere $I_4$ denotes the $4 \\times 4$ identity matrix. The Restricted Isometry Property (RIP) of order $s$ quantifies how well $A\\Phi$ preserves the Euclidean norm of $s$-sparse vectors, via the Restricted Isometry Constant $\\delta_s$ defined by the smallest nonnegative number such that\n$$\n(1-\\delta_s)\\|x\\|_2^2 \\le \\|A\\Phi x\\|_2^2 \\le (1+\\delta_s)\\|x\\|_2^2\n$$\nfor all $s$-sparse vectors $x$.\n\nFocus on the case $s=2$. Enumerate all $\\binom{4}{2}$ two-column submatrices of $A\\Phi$ induced by supports $T \\subset \\{1,2,3,4\\}$ with $|T|=2$, compute the smallest and largest singular values of each such $3 \\times 2$ submatrix, and, using the fundamental definition of the Restricted Isometry Constant, determine an exact, closed-form expression for $\\delta_2$ of $A\\Phi$. Express the final answer as an exact analytic expression with no rounding.",
            "solution": "The problem statement is parsed and validated. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard exercise in compressed sensing theory. Therefore, we proceed with the solution.\n\nThe problem asks for the calculation of the Restricted Isometry Constant (RIC) $\\delta_2$ for a given matrix $A\\Phi$.\nLet the matrix of interest be $M = A\\Phi$. The input matrices are given by\n$$\nA=\\frac{1}{2}\\begin{bmatrix}\n1  0  1  -1 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{bmatrix}, \\qquad \\Phi=I_4\n$$\nwhere $I_4$ is the $4 \\times 4$ identity matrix. Thus, the matrix $M$ is simply $A$:\n$$\nM = A = \\frac{1}{2}\\begin{bmatrix}\n1  0  1  -1 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n$$\nThe Restricted Isometry Constant $\\delta_s$ is the smallest non-negative number satisfying the inequality\n$$\n(1-\\delta_s)\\|x\\|_2^2 \\le \\|M x\\|_2^2 \\le (1+\\delta_s)\\|x\\|_2^2\n$$\nfor all $s$-sparse vectors $x \\in \\mathbb{R}^4$. An $s$-sparse vector $x$ has at most $s$ non-zero entries. Let $T = \\text{supp}(x)$ be the set of indices of the non-zero entries of $x$, with $|T| \\le s$. Let $x_T$ be the subvector of $x$ containing only these non-zero entries, and let $M_T$ be the submatrix of $M$ formed by the columns indexed by $T$. Then $Mx = M_T x_T$. The condition becomes\n$$\n(1-\\delta_s)\\|x_T\\|_2^2 \\le \\|M_T x_T\\|_2^2 \\le (1+\\delta_s)\\|x_T\\|_2^2\n$$\nfor all $x_T \\in \\mathbb{R}^{|T|}$. This is equivalent to stating that all eigenvalues of the Gram matrices $M_T^T M_T$ must lie in the interval $[1-\\delta_s, 1+\\delta_s]$ for all index sets $T$ with $|T| = s$.\nLet $\\lambda_{\\min}(M_T^T M_T)$ and $\\lambda_{\\max}(M_T^T M_T)$ be the minimum and maximum eigenvalues of $M_T^T M_T$, respectively. These eigenvalues are equal to the squared singular values of $M_T$. To satisfy the condition for all $T$ with $|T|=s$, $\\delta_s$ must be chosen such that:\n$$\n\\max_{T:|T|=s} \\lambda_{\\max}(M_T^T M_T) \\le 1 + \\delta_s \\quad \\text{and} \\quad \\min_{T:|T|=s} \\lambda_{\\min}(M_T^T M_T) \\ge 1 - \\delta_s\n$$\nSince we seek the smallest such $\\delta_s \\ge 0$, we have:\n$$\n\\delta_s = \\max \\left( \\max_{T:|T|=s} \\lambda_{\\max}(M_T^T M_T) - 1, 1 - \\min_{T:|T|=s} \\lambda_{\\min}(M_T^T M_T) \\right)\n$$\nWe are interested in the case $s=2$. There are $\\binom{4}{2} = 6$ possible supports $T$ of size $2$. We must form the corresponding $3 \\times 2$ submatrices $M_T$ and compute the eigenvalues of the $2 \\times 2$ Gram matrices $M_T^T M_T$.\n\nLet the columns of $M$ be $m_1, m_2, m_3, m_4$.\n$$\nm_1 = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad m_2 = \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad m_3 = \\frac{1}{2}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad m_4 = \\frac{1}{2}\\begin{pmatrix} -1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe entries of the Gram matrices $M_T^T M_T$ are the inner products $m_i^T m_j$. Let us pre-compute all of them. The full Gram matrix $M^T M$ is:\n$$\nM^T M = \\begin{pmatrix} m_1^T m_1  m_1^T m_2  m_1^T m_3  m_1^T m_4 \\\\ m_2^T m_1  m_2^T m_2  m_2^T m_3  m_2^T m_4 \\\\ m_3^T m_1  m_3^T m_2  m_3^T m_3  m_3^T m_4 \\\\ m_4^T m_1  m_4^T m_2  m_4^T m_3  m_4^T m_4 \\end{pmatrix}\n$$\nThe individual inner products are:\n$m_1^T m_1 = \\frac{1}{4}(1^2+0^2+1^2) = \\frac{2}{4} = \\frac{1}{2}$\n$m_2^T m_2 = \\frac{1}{4}(0^2+1^2+1^2) = \\frac{2}{4} = \\frac{1}{2}$\n$m_3^T m_3 = \\frac{1}{4}(1^2+1^2+0^2) = \\frac{2}{4} = \\frac{1}{2}$\n$m_4^T m_4 = \\frac{1}{4}((-1)^2+1^2+1^2) = \\frac{3}{4}$\n$m_1^T m_2 = \\frac{1}{4}(1\\cdot 0 + 0\\cdot 1 + 1\\cdot 1) = \\frac{1}{4}$\n$m_1^T m_3 = \\frac{1}{4}(1\\cdot 1 + 0\\cdot 1 + 1\\cdot 0) = \\frac{1}{4}$\n$m_1^T m_4 = \\frac{1}{4}(1\\cdot(-1) + 0\\cdot 1 + 1\\cdot 1) = 0$\n$m_2^T m_3 = \\frac{1}{4}(0\\cdot 1 + 1\\cdot 1 + 1\\cdot 0) = \\frac{1}{4}$\n$m_2^T m_4 = \\frac{1}{4}(0\\cdot(-1) + 1\\cdot 1 + 1\\cdot 1) = \\frac{2}{4} = \\frac{1}{2}$\n$m_3^T m_4 = \\frac{1}{4}(1\\cdot(-1) + 1\\cdot 1 + 0\\cdot 1) = 0$\n\nNow, we analyze the $6$ submatrices $M_T^T M_T$.\n\nCase 1: $T = \\{1, 2\\}$\n$M_{\\{1,2\\}}^T M_{\\{1,2\\}} = \\begin{pmatrix} m_1^T m_1  m_1^T m_2 \\\\ m_2^T m_1  m_2^T m_2 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/4 \\\\ 1/4  1/2 \\end{pmatrix}$.\nThe eigenvalues $\\lambda$ satisfy $\\det(\\begin{pmatrix} 1/2-\\lambda  1/4 \\\\ 1/4  1/2-\\lambda \\end{pmatrix}) = 0$, so $(1/2-\\lambda)^2 - (1/4)^2 = 0$. This gives $\\lambda - 1/2 = \\pm 1/4$.\n$\\lambda_{\\max} = 1/2 + 1/4 = 3/4$.\n$\\lambda_{\\min} = 1/2 - 1/4 = 1/4$.\n\nCase 2: $T = \\{1, 3\\}$\n$M_{\\{1,3\\}}^T M_{\\{1,3\\}} = \\begin{pmatrix} m_1^T m_1  m_1^T m_3 \\\\ m_3^T m_1  m_3^T m_3 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/4 \\\\ 1/4  1/2 \\end{pmatrix}$.\nThis is identical to the previous case.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/4$.\n\nCase 3: $T = \\{1, 4\\}$\n$M_{\\{1,4\\}}^T M_{\\{1,4\\}} = \\begin{pmatrix} m_1^T m_1  m_1^T m_4 \\\\ m_4^T m_1  m_4^T m_4 \\end{pmatrix} = \\begin{pmatrix} 1/2  0 \\\\ 0  3/4 \\end{pmatrix}$.\nThis matrix is diagonal, so its eigenvalues are its diagonal entries.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/2$.\n\nCase 4: $T = \\{2, 3\\}$\n$M_{\\{2,3\\}}^T M_{\\{2,3\\}} = \\begin{pmatrix} m_2^T m_2  m_2^T m_3 \\\\ m_3^T m_2  m_3^T m_3 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/4 \\\\ 1/4  1/2 \\end{pmatrix}$.\nThis is identical to the first case.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/4$.\n\nCase 5: $T = \\{2, 4\\}$\n$M_{\\{2,4\\}}^T M_{\\{2,4\\}} = \\begin{pmatrix} m_2^T m_2  m_2^T m_4 \\\\ m_4^T m_2  m_4^T m_4 \\end{pmatrix} = \\begin{pmatrix} 1/2  1/2 \\\\ 1/2  3/4 \\end{pmatrix}$.\nThe characteristic equation is $\\lambda^2 - \\text{tr}(M_T^T M_T)\\lambda + \\det(M_T^T M_T) = 0$.\n$\\text{tr} = 1/2 + 3/4 = 5/4$.\n$\\det = (1/2)(3/4) - (1/2)^2 = 3/8 - 1/4 = 1/8$.\n$\\lambda^2 - \\frac{5}{4}\\lambda + \\frac{1}{8} = 0$.\n$\\lambda = \\frac{5/4 \\pm \\sqrt{(5/4)^2 - 4(1/8)}}{2} = \\frac{5/4 \\pm \\sqrt{25/16 - 1/2}}{2} = \\frac{5/4 \\pm \\sqrt{17/16}}{2} = \\frac{5/4 \\pm \\sqrt{17}/4}{2} = \\frac{5 \\pm \\sqrt{17}}{8}$.\n$\\lambda_{\\max} = \\frac{5 + \\sqrt{17}}{8}$.\n$\\lambda_{\\min} = \\frac{5 - \\sqrt{17}}{8}$.\n\nCase 6: $T = \\{3, 4\\}$\n$M_{\\{3,4\\}}^T M_{\\{3,4\\}} = \\begin{pmatrix} m_3^T m_3  m_3^T m_4 \\\\ m_4^T m_3  m_4^T m_4 \\end{pmatrix} = \\begin{pmatrix} 1/2  0 \\\\ 0  3/4 \\end{pmatrix}$.\nThis is identical to case $3$.\n$\\lambda_{\\max} = 3/4$, $\\lambda_{\\min} = 1/2$.\n\nWe summarize the eigenvalues for all $s=2$ submatrices:\n- $T = \\{1,2\\}: \\lambda_{\\min}=1/4$, $\\lambda_{\\max}=3/4$.\n- $T = \\{1,3\\}: \\lambda_{\\min}=1/4$, $\\lambda_{\\max}=3/4$.\n- $T = \\{1,4\\}: \\lambda_{\\min}=1/2$, $\\lambda_{\\max}=3/4$.\n- $T = \\{2,3\\}: \\lambda_{\\min}=1/4$, $\\lambda_{\\max}=3/4$.\n- $T = \\{2,4\\}: \\lambda_{\\min}=\\frac{5-\\sqrt{17}}{8}$, $\\lambda_{\\max}=\\frac{5+\\sqrt{17}}{8}$.\n- $T = \\{3,4\\}: \\lambda_{\\min}=1/2$, $\\lambda_{\\max}=3/4$.\n\nNow we find the overall minimum and maximum eigenvalues.\n$\\min_{T:|T|=2} \\lambda_{\\min}(M_T^T M_T) = \\min\\{1/4, 1/2, \\frac{5-\\sqrt{17}}{8}\\}$.\nSince $4  \\sqrt{17}  5$, specifically $4.12  \\sqrt{17}  4.13$, we have $5-\\sqrt{17}  1$. So $\\frac{5-\\sqrt{17}}{8}  1/8$. Also $1/4=2/8$. Thus $\\frac{5-\\sqrt{17}}{8}$ is the smallest value.\n$\\min_{T:|T|=2} \\lambda_{\\min}(M_T^T M_T) = \\frac{5-\\sqrt{17}}{8}$.\n\n$\\max_{T:|T|=2} \\lambda_{\\max}(M_T^T M_T) = \\max\\{3/4, \\frac{5+\\sqrt{17}}{8}\\}$.\n$3/4 = 6/8$. Since $\\sqrt{17}  1$, we have $5+\\sqrt{17}  6$. So $\\frac{5+\\sqrt{17}}{8}  6/8$.\n$\\max_{T:|T|=2} \\lambda_{\\max}(M_T^T M_T) = \\frac{5+\\sqrt{17}}{8}$.\n\nFinally, we compute $\\delta_2$:\n$$\n\\delta_2 = \\max \\left( \\frac{5+\\sqrt{17}}{8} - 1, 1 - \\frac{5-\\sqrt{17}}{8} \\right)\n$$\n$$\n\\delta_2 = \\max \\left( \\frac{5+\\sqrt{17}-8}{8}, \\frac{8-(5-\\sqrt{17})}{8} \\right)\n$$\n$$\n\\delta_2 = \\max \\left( \\frac{\\sqrt{17}-3}{8}, \\frac{3+\\sqrt{17}}{8} \\right)\n$$\nSince $\\sqrt{17}>0$, it is evident that $3+\\sqrt{17} > \\sqrt{17}-3$.\nTherefore, the restricted isometry constant is:\n$$\n\\delta_2 = \\frac{3+\\sqrt{17}}{8}\n$$",
            "answer": "$$\\boxed{\\frac{3+\\sqrt{17}}{8}}$$"
        }
    ]
}