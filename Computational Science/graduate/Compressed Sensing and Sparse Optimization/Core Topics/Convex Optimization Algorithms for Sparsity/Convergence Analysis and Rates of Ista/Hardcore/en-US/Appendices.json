{
    "hands_on_practices": [
        {
            "introduction": "Before analyzing convergence rates, it is essential to master the mechanics of the Iterative Shrinkage-Thresholding Algorithm (ISTA). This first practice provides a concrete, hands-on calculation of a single ISTA step for the LASSO problem . By working through this exercise, you will solidify your understanding of how the gradient of the smooth part, the step-size derived from the Lipschitz constant, and the soft-thresholding operator combine to produce the next iterate.",
            "id": "3438561",
            "problem": "Consider the composite optimization problem in compressed sensing and sparse optimization that seeks to minimize the function $F(x) = f(x) + \\lambda \\|x\\|_{1}$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$, and $\\|\\cdot\\|_{1}$ and $\\|\\cdot\\|_{2}$ denote the $\\ell_{1}$-norm and the Euclidean norm, respectively. The Iterative Shrinkage-Thresholding Algorithm (ISTA) is defined by the proximal-gradient iteration $x^{k+1} = S_{\\alpha \\lambda}\\!\\left(x^{k} - \\alpha \\nabla f(x^{k})\\right)$, where $S_{\\tau}$ is the soft-thresholding operator with threshold $\\tau$, and $\\alpha$ is a constant step-size chosen to satisfy theoretical convergence guarantees based on the Lipschitz continuity of $\\nabla f$.\n\nStart from the fundamental bases:\n- The gradient of a least-squares function $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is given by $\\nabla f(x) = A^{\\top}(A x - b)$.\n- The gradient $\\nabla f$ is Lipschitz continuous with constant $L = \\|A^{\\top} A\\|_{2}$, the spectral norm (largest eigenvalue) of $A^{\\top} A$.\n- The soft-thresholding operator $S_{\\tau}$ is defined componentwise by $S_{\\tau}(z)_{i} = \\operatorname{sign}(z_{i})\\max\\{|z_{i}| - \\tau, 0\\}$.\n\nLet the data be\n$$\nA = \\begin{pmatrix}\n2  0 \\\\\n0  3 \\\\\n0  0\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\\n-6 \\\\\n0\n\\end{pmatrix}, \\quad\n\\lambda = 1, \\quad\nx^{0} = \\begin{pmatrix}\n\\frac{1}{3} \\\\\n-\\frac{1}{2}\n\\end{pmatrix}.\n$$\nChoose the constant step-size $\\alpha$ to be $\\alpha = \\frac{1}{L}$, where $L$ is the Lipschitz constant of $\\nabla f$ determined from $A$.\n\nCompute one ISTA iteration explicitly,\n$$\nx^{1} = S_{\\alpha \\lambda}\\!\\left(x^{0} - \\alpha A^{\\top}(A x^{0} - b)\\right),\n$$\nand provide the resulting $x^{1}$ as a single row vector. No rounding is required; provide the exact rational values.",
            "solution": "The objective is to compute one iteration of the Iterative Shrinkage-Thresholding Algorithm (ISTA) to find $x^{1}$, starting from a given initial point $x^{0}$. The ISTA update rule is given by:\n$$\nx^{k+1} = S_{\\alpha \\lambda}\\!\\left(x^{k} - \\alpha \\nabla f(x^{k})\\right)\n$$\nFor our specific problem, we need to compute $x^{1}$ using $k=0$:\n$$\nx^{1} = S_{\\alpha \\lambda}\\!\\left(x^{0} - \\alpha \\nabla f(x^{0})\\right)\n$$\nThe function $f(x)$ is the least-squares term $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$, and its gradient is $\\nabla f(x) = A^{\\top}(A x - b)$.\n\nFirst, we must determine the step-size $\\alpha$. The problem specifies that $\\alpha = \\frac{1}{L}$, where $L$ is the Lipschitz constant of $\\nabla f(x)$. The Lipschitz constant is given by the spectral norm of $A^{\\top} A$, denoted as $L = \\|A^{\\top} A\\|_{2}$.\n\nThe given matrix is $A = \\begin{pmatrix} 2  0 \\\\ 0  3 \\\\ 0  0 \\end{pmatrix}$. Its transpose is $A^{\\top} = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\end{pmatrix}$.\nWe compute the product $A^{\\top} A$:\n$$\nA^{\\top} A = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  3 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  0 \\\\ 0  9 \\end{pmatrix}\n$$\nThe spectral norm of a symmetric (or, in this case, diagonal) matrix is the magnitude of its largest eigenvalue. The eigenvalues of the diagonal matrix $\\begin{pmatrix} 4  0 \\\\ 0  9 \\end{pmatrix}$ are its diagonal entries, $4$ and $9$.\nTherefore, the Lipschitz constant is $L = \\max\\{4, 9\\} = 9$.\nThe step-size is $\\alpha = \\frac{1}{L} = \\frac{1}{9}$.\n\nNext, we compute the gradient of $f$ at the initial point $x^{0} = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix}$.\nFirst, calculate $A x^{0} - b$:\n$$\nA x^{0} = \\begin{pmatrix} 2  0 \\\\ 0  3 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2(\\frac{1}{3}) \\\\ 3(-\\frac{1}{2}) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{3}{2} \\\\ 0 \\end{pmatrix}\n$$\nWith $b = \\begin{pmatrix} 1 \\\\ -6 \\\\ 0 \\end{pmatrix}$, we have:\n$$\nA x^{0} - b = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{3}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -6 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} - 1 \\\\ -\\frac{3}{2} + 6 \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{3} \\\\ \\frac{9}{2} \\\\ 0 \\end{pmatrix}\n$$\nNow, we compute the gradient $\\nabla f(x^{0}) = A^{\\top}(A x^{0} - b)$:\n$$\n\\nabla f(x^{0}) = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{3} \\\\ \\frac{9}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(-\\frac{1}{3}) \\\\ 3(\\frac{9}{2}) \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{3} \\\\ \\frac{27}{2} \\end{pmatrix}\n$$\nThe next step is to compute the argument of the soft-thresholding operator, which we denote as $z$:\n$$\nz = x^{0} - \\alpha \\nabla f(x^{0}) = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix} - \\frac{1}{9} \\begin{pmatrix} -\\frac{2}{3} \\\\ \\frac{27}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} \\\\ -\\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} -\\frac{2}{27} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} + \\frac{2}{27} \\\\ -\\frac{1}{2} - \\frac{3}{2} \\end{pmatrix}\n$$\n$$\nz = \\begin{pmatrix} \\frac{9}{27} + \\frac{2}{27} \\\\ -\\frac{4}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{27} \\\\ -2 \\end{pmatrix}\n$$\nFinally, we apply the soft-thresholding operator $S_{\\tau}(z)$ with threshold $\\tau = \\alpha \\lambda$. Given $\\lambda = 1$ and $\\alpha = \\frac{1}{9}$, the threshold is $\\tau = \\frac{1}{9} \\cdot 1 = \\frac{1}{9}$.\nThe soft-thresholding operator is applied component-wise: $S_{\\tau}(z)_{i} = \\operatorname{sign}(z_{i})\\max\\{|z_{i}| - \\tau, 0\\}$.\n\nFor the first component, $z_1 = \\frac{11}{27}$:\n$$\nx^{1}_{1} = S_{1/9}\\left(\\frac{11}{27}\\right) = \\operatorname{sign}\\left(\\frac{11}{27}\\right) \\max\\left\\{\\left|\\frac{11}{27}\\right| - \\frac{1}{9}, 0\\right\\}\n$$\nSince $\\frac{11}{27}  \\frac{1}{9}$ (as $\\frac{11}{27}  \\frac{3}{27}$), the operation is:\n$$\nx^{1}_{1} = 1 \\cdot \\left(\\frac{11}{27} - \\frac{1}{9}\\right) = \\frac{11}{27} - \\frac{3}{27} = \\frac{8}{27}\n$$\nFor the second component, $z_2 = -2$:\n$$\nx^{1}_{2} = S_{1/9}(-2) = \\operatorname{sign}(-2) \\max\\left\\{|-2| - \\frac{1}{9}, 0\\right\\}\n$$\nSince $|-2|=2  \\frac{1}{9}$, the operation is:\n$$\nx^{1}_{2} = -1 \\cdot \\left(2 - \\frac{1}{9}\\right) = -\\left(\\frac{18}{9} - \\frac{1}{9}\\right) = -\\frac{17}{9}\n$$\nThus, the result of one ISTA iteration is the vector $x^{1} = \\begin{pmatrix} \\frac{8}{27} \\\\ -\\frac{17}{9} \\end{pmatrix}$. The problem asks for the answer as a single row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{8}{27}  -\\frac{17}{9} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "While ISTA is guaranteed to converge, its rate of convergence can vary dramatically, and a key factor is whether the algorithm can identify the set of non-zero components of the solution (the support) in finite time. This practice explores a scenario where a critical condition, known as strict complementarity, fails to hold at the solution . Deriving the closed-form iterates for this case reveals that the algorithm converges to the correct support only asymptotically, never identifying it exactly in a finite number of steps.",
            "id": "3438522",
            "problem": "Consider the one-dimensional Least Absolute Shrinkage and Selection Operator (LASSO) problem\n$$\\min_{x \\in \\mathbb{R}} \\; F(x) := \\frac{1}{2}\\,(x - b)^{2} + \\lambda\\,|x|,$$\nwhere $b \\in \\mathbb{R}$ is a given data point and $\\lambda  0$ is the regularization parameter. The Iterative Shrinkage-Thresholding Algorithm (ISTA, Iterative Shrinkage-Thresholding Algorithm) with step size $\\tau \\in (0,1)$ generates the iterates\n$$x^{k+1} \\in \\operatorname{prox}_{\\tau \\lambda |\\cdot|}\\big(x^{k} - \\tau \\nabla f(x^{k})\\big), \\quad f(x) := \\frac{1}{2}\\,(x - b)^{2},$$\nwhere the proximity operator of the scaled absolute value is the soft-thresholding map\n$$S_{\\alpha}(z) := \\operatorname{sign}(z)\\,\\max\\{|z| - \\alpha, 0\\}.$$\nYou are asked to construct a counterexample to finite-time support identification by ISTA when strict complementarity fails. Proceed as follows.\n\n1. Specialize the data to the regime $b = \\lambda$ and show that the unique minimizer is $x^{\\star} = 0$. Verify that strict complementarity at $x^{\\star}$ fails, namely $|\\nabla f(x^{\\star})| = \\lambda$.\n2. Write the ISTA update for this one-dimensional instance explicitly in terms of $\\tau$, $\\lambda$, $b$, and $x^{k}$, and simplify it under the specialization $b = \\lambda$.\n3. Assume an initial point $x^{0}  0$. Derive the exact closed-form expression for the ISTA iterates $x^{k}$ as a function of $k$, $\\tau$, and $x^{0}$. Use only the definitions above and standard optimality conditions for convex functions.\n\nYour final answer must be the closed-form analytic expression for $x^{k}$ derived in step $3$. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the derivation of a closed-form expression for the iterates of the Iterative Shrinkage-Thresholding Algorithm (ISTA) for a specific one-dimensional LASSO problem, which serves as a counterexample to finite-time support identification when strict complementarity fails. We will proceed by following the three specified steps.\n\nThe LASSO problem is to minimize $F(x) := \\frac{1}{2}(x - b)^{2} + \\lambda|x|$ for $x \\in \\mathbb{R}$, where $b \\in \\mathbb{R}$ and $\\lambda  0$. The function $F(x)$ is a sum of a strictly convex function $f(x) = \\frac{1}{2}(x-b)^2$ and a convex function $g(x) = \\lambda|x|$. Thus, $F(x)$ is strictly convex and has a unique minimizer $x^{\\star}$.\n\n**Step 1: Analysis of the minimizer and strict complementarity for $b = \\lambda$**\n\nThe unique minimizer $x^{\\star}$ is characterized by the first-order optimality condition $0 \\in \\partial F(x^{\\star})$, where $\\partial F(x)$ is the subdifferential of $F(x)$.\nThe subdifferential is given by $\\partial F(x) = \\nabla f(x) + \\partial g(x) = (x - b) + \\lambda \\partial|x|$.\nThe subdifferential of the absolute value function is:\n$$\n\\partial|x| =\n\\begin{cases}\n    \\{1\\}  \\text{if } x  0 \\\\\n    [-1, 1]  \\text{if } x = 0 \\\\\n    \\{-1\\}  \\text{if } x  0\n\\end{cases}\n$$\nThe optimality condition $0 \\in (x^{\\star} - b) + \\lambda \\partial|x^{\\star}|$ can be rewritten as $b - x^{\\star} \\in \\lambda \\partial|x^{\\star}|$. We analyze three cases for $x^{\\star}$:\n1. If $x^{\\star}  0$: The condition becomes $b - x^{\\star} = \\lambda$, which implies $x^{\\star} = b - \\lambda$. This case is self-consistent only if $x^{\\star}  0$, i.e., $b  \\lambda$.\n2. If $x^{\\star}  0$: The condition becomes $b - x^{\\star} = -\\lambda$, which implies $x^{\\star} = b + \\lambda$. This case is self-consistent only if $x^{\\star}  0$, i.e., $b  -\\lambda$.\n3. If $x^{\\star} = 0$: The condition becomes $b - 0 \\in \\lambda[-1, 1]$, which is equivalent to $|b| \\le \\lambda$.\n\nThe problem specifies the regime $b = \\lambda$. Since $\\lambda  0$, we have $|b| = |\\lambda| = \\lambda$, which satisfies the condition $|b| \\le \\lambda$. Therefore, we are in the third case, and the unique minimizer is $x^{\\star} = 0$.\n\nNext, we verify the failure of strict complementarity. For a solution $x^\\star$ with $x^\\star_i=0$, strict complementarity holds if the corresponding component of the smooth gradient, $\\nabla_i f(x^\\star)$, lies strictly in the interior of the subdifferential of the nonsmooth part at $x^\\star$. In our one-dimensional case with $x^\\star=0$, this condition is $|\\nabla f(x^{\\star})|  \\lambda$.\nThe gradient of the smooth part is $\\nabla f(x) = x - b$.\nAt the minimizer $x^{\\star} = 0$, the gradient is $\\nabla f(0) = 0 - b = -b$.\nWith the specialization $b = \\lambda$, we have $\\nabla f(x^{\\star}) = -\\lambda$.\nThe magnitude is $|\\nabla f(x^{\\star})| = |-\\lambda| = \\lambda$.\nThe strict complementarity condition would require $\\lambda  \\lambda$, which is false. We have $|\\nabla f(x^{\\star})| = \\lambda$, which is the boundary case where strict complementarity fails.\n\n**Step 2: ISTA update rule for $b = \\lambda$**\n\nThe general ISTA update rule is given by\n$$x^{k+1} = \\operatorname{prox}_{\\tau \\lambda |\\cdot|}\\left(x^{k} - \\tau \\nabla f(x^{k})\\right)$$\nwhere $\\tau \\in (0,1)$ is the step size. The proximity operator of $g(x) = \\lambda|x|$ scaled by $\\tau$ is the soft-thresholding operator $S_{\\tau\\lambda}(z)$.\nThe argument to the operator is $x^{k} - \\tau \\nabla f(x^{k}) = x^{k} - \\tau(x^{k}-b) = (1-\\tau)x^{k} + \\tau b$.\nSo, the update is $x^{k+1} = S_{\\tau\\lambda}((1-\\tau)x^{k} + \\tau b)$.\nSpecializing to the case $b=\\lambda$, the update rule becomes:\n$$x^{k+1} = S_{\\tau\\lambda}((1-\\tau)x^{k} + \\tau\\lambda)$$\n\n**Step 3: Closed-form expression for the iterates $x^{k}$**\n\nWe are asked to derive the expression for $x^{k}$ assuming an initial point $x^{0}  0$. We use the recurrence relation from Step 2.\nThe soft-thresholding operator is defined as $S_{\\alpha}(z) = \\operatorname{sign}(z)\\max\\{|z| - \\alpha, 0\\}$.\nWe proceed by induction. The base case is $x^{0}  0$.\nLet's assume that for some $k \\ge 0$, we have $x^{k}  0$. We analyze the next iterate $x^{k+1}$.\nThe argument of the soft-thresholding function is $z^{k} = (1-\\tau)x^{k} + \\tau\\lambda$.\nGiven $\\tau \\in (0,1)$, we have $1-\\tau  0$. Also, $\\lambda  0$. From our inductive hypothesis, $x^{k}  0$.\nThus, every term in the expression for $z^{k}$ is positive, which implies $z^{k}  0$.\nMore specifically, since $(1-\\tau)x^k  0$, we have $z^k = (1-\\tau)x^k + \\tau\\lambda  \\tau\\lambda$.\nFor a positive argument $z  0$, the soft-thresholding operator simplifies to $S_{\\alpha}(z) = \\max\\{z - \\alpha, 0\\}$.\nIn our case, $\\alpha = \\tau\\lambda$ and the argument is $z^k$.\n$$x^{k+1} = \\max\\{z^{k} - \\tau\\lambda, 0\\}$$\nSubstituting the expression for $z^k$:\n$$x^{k+1} = \\max\\{( (1-\\tau)x^{k} + \\tau\\lambda ) - \\tau\\lambda, 0\\} = \\max\\{(1-\\tau)x^{k}, 0\\}$$\nSince $1-\\tau  0$ and we assumed $x^{k}  0$, the term $(1-\\tau)x^{k}$ is strictly positive.\nTherefore, the maximum is simply the term itself:\n$$x^{k+1} = (1-\\tau)x^{k}$$\nThis confirms our inductive step: if $x^{k}  0$, then $x^{k+1}  0$. Since we start with $x^{0}  0$, it follows that $x^{k}  0$ for all $k \\ge 0$.\nThe recurrence relation $x^{k+1} = (1-\\tau)x^{k}$ is a simple geometric progression. We can unroll it to find the closed-form solution:\n$x^{1} = (1-\\tau)x^{0}$\n$x^{2} = (1-\\tau)x^{1} = (1-\\tau)^2 x^{0}$\nand so on. For any integer $k \\ge 0$, the iterate is given by:\n$$x^{k} = (1-\\tau)^{k} x^{0}$$\nThis is the required closed-form expression. As $\\tau \\in (0,1)$, we have $0  1-\\tau  1$, so $x^k \\to 0 = x^\\star$ as $k \\to \\infty$. However, for any finite $k$, $x^k$ is never exactly $0$, demonstrating that the support of the solution is not identified in finite time.",
            "answer": "$$\\boxed{(1-\\tau)^{k} x^{0}}$$"
        },
        {
            "introduction": "Having seen how the lack of strict complementarity can slow convergence, we now explore the opposite and more common scenario. This exercise demonstrates the powerful acceleration that occurs when the active support is identified correctly and in finite time, a property guaranteed under conditions like strict complementarity . You will analyze how the algorithm transitions from its slow global sublinear rate to a much faster local linear rate, a crucial aspect of understanding ISTA's performance in practice.",
            "id": "3438565",
            "problem": "Consider the Iterative Shrinkage-Thresholding Algorithm (ISTA) applied to the sparse optimization problem in compressed sensing that minimizes the objective function\n$$\nF(x) \\triangleq g(x) + h(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$, $y \\in \\mathbb{R}^{3}$, and $\\lambda  0$. The smooth part is $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ with gradient $\\nabla g(x) = A^{\\top}(A x - y)$, which is Lipschitz continuous with constant $L = \\|A^{\\top}A\\|_{2}$. The nonsmooth part is $h(x) = \\lambda \\|x\\|_{1}$ whose proximal operator is the soft-thresholding operator. The Iterative Shrinkage-Thresholding Algorithm (ISTA) iterates\n$$\nx^{k+1} = \\operatorname{prox}_{t_{k} h}\\big(x^{k} - t_{k} \\nabla g(x^{k})\\big),\n$$\nwhere $t_{k}  0$ is chosen by a standard backtracking procedure to ensure sufficient decrease based on local Lipschitz continuity of $\\nabla g$. A known phenomenon in such composite optimization is finite identification of the active manifold (support and signs) under strict complementarity, after which the iteration evolves linearly on the identified manifold.\n\nConstruct the following explicit instance. Let\n$$\nA = \\begin{pmatrix}\n2  0  0\\\\\n0  2  0\\\\\n0  0  100\n\\end{pmatrix}, \\qquad y = \\begin{pmatrix} 2 \\\\ 2 \\\\ 0.005 \\end{pmatrix}, \\qquad \\lambda = 1.\n$$\nThis choice is scientifically realistic and internally consistent: columns corresponding to the first two variables are moderately scaled and mutually orthogonal, while the third column is very large, producing a large global Lipschitz constant. Consider ISTA with backtracking that, after identification, uses the local Lipschitz constant associated with the identified manifold.\n\nTasks:\n- Using only fundamental definitions of convexity, proximal operators, and first-order optimality (Karush–Kuhn–Tucker conditions), show that the unique minimizer $x^{\\star}$ has support $S = \\{1,2\\}$ with strictly positive entries and $x_{3}^{\\star} = 0$.\n- Verify strict complementarity on the inactive coordinate $3$.\n- Argue from first principles why ISTA identifies $S$ and the signs in finite iterations under these conditions.\n- Once identification occurs, characterize the local linear iteration of ISTA restricted to $S$ and express its rate constant in terms of the spectrum of $A_{S}^{\\top}A_{S}$ and the accepted step-size $t$ that backtracking converges to on the manifold.\n- For this instance, compute the exact numerical value of the local linear convergence rate constant $\\rho_{\\mathrm{local}}$ after identification. Provide your answer as a single real number. No rounding is required.",
            "solution": "The objective function to minimize is $F(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|Ax - y\\|_{2}^{2}$ and $h(x) = \\lambda \\|x\\|_{1}$. The given parameter values are $\\lambda = 1$ and\n$$\nA = \\begin{pmatrix}\n2  0  0\\\\\n0  2  0\\\\\n0  0  100\n\\end{pmatrix}, \\qquad y = \\begin{pmatrix} 2 \\\\ 2 \\\\ 0.005 \\end{pmatrix}.\n$$\nThe gradient of the smooth part $g(x)$ is $\\nabla g(x) = A^{\\top}(Ax - y)$. Since $A$ is a diagonal matrix, $A^{\\top} = A$, and thus $\\nabla g(x) = A^2 x - Ay$.\n$$\nA^2 = \\begin{pmatrix}\n4  0  0\\\\\n0  4  0\\\\\n0  0  10000\n\\end{pmatrix}, \\qquad Ay = \\begin{pmatrix}\n4 \\\\\n4 \\\\\n0.5\n\\end{pmatrix}.\n$$\nThe components of the gradient are:\n$(\\nabla g(x))_1 = 4x_1 - 4$\n$(\\nabla g(x))_2 = 4x_2 - 4$\n$(\\nabla g(x))_3 = 10000x_3 - 0.5$\n\n### Task 1: Find the Unique Minimizer $x^{\\star}$\n\nThe function $F(x)$ is strictly convex because $g(x)$ is strictly convex ($A$ is invertible) and $h(x)$ is convex. Therefore, a unique minimizer $x^{\\star}$ exists. The first-order optimality conditions (or Karush-Kuhn-Tucker conditions) for a point $x^{\\star}$ to be the minimizer state that $0 \\in \\nabla g(x^{\\star}) + \\partial h(x^{\\star})$, where $\\partial h(x^{\\star})$ is the subdifferential of $h(x)$ at $x^{\\star}$. This is equivalent to $-\\nabla g(x^{\\star}) \\in \\partial (\\lambda \\|x\\|_1)|_{x=x^{\\star}}$.\n\nThis condition can be expressed component-wise:\n1.  If $x_i^{\\star} \\ne 0$, then $(\\nabla g(x^{\\star}))_i + \\lambda \\operatorname{sign}(x_i^{\\star}) = 0$.\n2.  If $x_i^{\\star} = 0$, then $|(\\nabla g(x^{\\star}))_i| \\le \\lambda$.\n\nWe hypothesize that the support of the solution is $S = \\{1,2\\}$, meaning $x_1^{\\star} \\neq 0$, $x_2^{\\star} \\neq 0$, and $x_3^{\\star} = 0$. We further hypothesize that $x_1^{\\star}  0$ and $x_2^{\\star}  0$.\n\nFor $i=1$: We apply condition (1) with $\\operatorname{sign}(x_1^{\\star}) = 1$ and $\\lambda=1$.\n$(\\nabla g(x^{\\star}))_1 + 1 = 0 \\implies (4x_1^{\\star} - 4) + 1 = 0 \\implies 4x_1^{\\star} = 3 \\implies x_1^{\\star} = \\frac{3}{4}$.\nSince $x_1^{\\star} = \\frac{3}{4}  0$, our sign assumption is consistent.\n\nFor $i=2$: Similarly, we apply condition (1) with $\\operatorname{sign}(x_2^{\\star}) = 1$.\n$(\\nabla g(x^{\\star}))_2 + 1 = 0 \\implies (4x_2^{\\star} - 4) + 1 = 0 \\implies 4x_2^{\\star} = 3 \\implies x_2^{\\star} = \\frac{3}{4}$.\nSince $x_2^{\\star} = \\frac{3}{4}  0$, this sign assumption is also consistent.\n\nFor $i=3$: We test our hypothesis that $x_3^{\\star} = 0$ using condition (2). We must check if $|(\\nabla g(x^{\\star}))_3| \\le \\lambda = 1$.\nWe substitute $x_3^{\\star}=0$ into the expression for the gradient component:\n$(\\nabla g(x^{\\star}))_3 = 10000x_3^{\\star} - 0.5 = 10000(0) - 0.5 = -0.5$.\nWe check the condition: $|-0.5| = 0.5 \\le 1$. The condition holds.\n\nAll conditions are met for the candidate solution $x^{\\star} = (\\frac{3}{4}, \\frac{3}{4}, 0)^{\\top}$. The support is indeed $S=\\{1,2\\}$, the entries on the support are strictly positive, and $x_3^{\\star} = 0$. Due to strict convexity, this is the unique minimizer.\n\n### Task 2: Verify Strict Complementarity\n\nStrict complementarity for an inactive coordinate $j$ (where $x_j^{\\star}=0$) requires the subgradient condition for that coordinate to hold with strict inequality. For $j=3$, we must verify that $|(\\nabla g(x^{\\star}))_3|  \\lambda$.\nFrom the previous step, we calculated $(\\nabla g(x^{\\star}))_3 = -0.5$ and we are given $\\lambda = 1$.\nThe condition is $|-0.5|  1$, which simplifies to $0.5  1$. This is true.\nThus, strict complementarity holds on the inactive coordinate $j=3$.\n\n### Task 3: Finite Identification of the Active Manifold\n\nThe ISTA iteration is given by $x^{k+1} = \\operatorname{prox}_{t_k h}(x^{k} - t_k \\nabla g(x^{k}))$. The proximal operator for $h(x) = \\lambda \\|x\\|_1$ is the component-wise soft-thresholding function, $S_{t_k\\lambda}(z)_i = \\operatorname{sign}(z_i)\\max(|z_i|-t_k\\lambda, 0)$.\nA coordinate $x_i^{k+1}$ is set to zero if and only if $|(x^k - t_k \\nabla g(x^k))_i| \\le t_k\\lambda$.\n\nAs ISTA is a convergent algorithm, $x^k \\to x^{\\star}$ as $k \\to \\infty$. We also assume the step sizes $t_k$ used in the backtracking line search converge to a positive value $t  0$. Let $z^k = x^k - t_k \\nabla g(x^k)$. Then $z^k \\to z^{\\star} = x^{\\star} - t \\nabla g(x^{\\star})$.\n\nLet's analyze the components of $z^{\\star}$:\nFor the inactive coordinate $i=3$:\n$z_3^{\\star} = x_3^{\\star} - t (\\nabla g(x^{\\star}))_3 = 0 - t(-0.5) = 0.5t$.\nThe condition for the $3$-rd coordinate to be zeroed out is $|z_3^k| \\le t_k \\lambda$. In the limit, this is $|z_3^{\\star}| \\le t\\lambda$, which is $|0.5t| \\le t(1)$, or $0.5t \\le t$. This inequality is strict for $t0$.\nBecause of the strict inequality $0.5  1$ (which is the strict complementarity condition), by continuity, for any $k$ large enough such that $x^k$ is in a sufficiently small neighborhood of $x^{\\star}$ and $t_k$ is close to $t$, we will have $|(x^k - t_k \\nabla g(x^k))_3|  t_k \\lambda$. Once this occurs, $x_3^{k+1}$ becomes exactly $0$. It will remain zero in all subsequent iterations, as the iteration for the third component will be $x_3^{j+1} = S_{t_j\\lambda}(-t_j (\\nabla g(x^j))_3)$ for $j  k$, and the argument will remain within the thresholding interval $[ -t_j\\lambda, t_j\\lambda ]$.\n\nFor the active coordinates $i \\in S = \\{1,2\\}$:\n$z_i^{\\star} = x_i^{\\star} - t(\\nabla g(x^{\\star}))_i$. From the KKT conditions, $(\\nabla g(x^{\\star}))_i = -\\lambda \\operatorname{sign}(x_i^{\\star}) = -1$ for $i=1,2$.\n$z_1^{\\star} = x_1^{\\star} - t(-1) = \\frac{3}{4} + t$.\n$z_2^{\\star} = x_2^{\\star} - t(-1) = \\frac{3}{4} + t$.\nSince $t  0$, we have $z_{1,2}^{\\star}  t = t\\lambda$. By continuity, for $k$ large enough, $|z_{1,2}^k|  t_k\\lambda$. Thus, $x_{1,2}^{k+1}$ will be non-zero. Furthermore, since $z_{1,2}^k$ will be positive, the signs of $x_{1,2}^{k+1}$ will be positive, matching the signs of $x_{1,2}^{\\star}$.\n\nTherefore, due to the strict complementarity at the solution, the algorithm will identify the correct support $S=\\{1,2\\}$ and signs in a finite number of iterations.\n\n### Task 4: Local Linear Iteration and Convergence Rate\n\nOnce the algorithm has identified the active manifold for $k \\ge K$ (i.e., $x_i^k = 0$ for $i \\notin S$ and $\\operatorname{sign}(x_i^k) = \\operatorname{sign}(x_i^{\\star})$ for $i \\in S$), the iteration simplifies. Let $x_S$ denote the vector of components in the support $S=\\{1,2\\}$. For $k \\ge K$, we have $x_{S^c}^k = 0$ and $\\operatorname{sign}(x_S^k) = (1, 1)^{\\top}$.\nThe ISTA update for $x_S$ is:\n$$\nx_S^{k+1} = \\operatorname{prox}_{t h_S}(x_S^k - t \\nabla_S g(x^k))\n$$\nwhere $h_S(x_S) = \\lambda \\|x_S\\|_1 = \\lambda \\mathbf{1}^{\\top}x_S$ for positive $x_S$, and $t$ is the step size. The gradient restricted to the manifold is $\\nabla_S g(x^k) = A_S^{\\top}(A_S x_S^k - y)$. The prox operation becomes a simple shift:\n$$\nx_S^{k+1} = (x_S^k - t(A_S^{\\top}(A_S x_S^k - y))) - t\\lambda \\mathbf{1}.\n$$\nThe solution on the manifold, $x_S^{\\star}$, is a fixed point of this iteration, satisfying:\n$$\nx_S^{\\star} = (x_S^{\\star} - t(A_S^{\\top}(A_S x_S^{\\star} - y))) - t\\lambda \\mathbf{1},\n$$\nwhich implies $A_S^{\\top}(A_S x_S^{\\star} - y) + \\lambda \\mathbf{1} = 0$, consistent with the KKT conditions.\nLet the error be $e^k = x_S^k - x_S^{\\star}$. Subtracting the fixed-point equation from the iteration equation gives:\n$$\ne^{k+1} = e^k - t \\left( A_S^{\\top}A_S x_S^k - A_S^{\\top}A_S x_S^{\\star} \\right) = e^k - t A_S^{\\top}A_S e^k = (I - t A_S^{\\top}A_S) e^k.\n$$\nThis is a linear iteration. The rate of convergence is determined by the spectral radius of the iteration matrix $M = I - t A_S^{\\top}A_S$. The local linear convergence rate constant is $\\rho_{\\mathrm{local}} = \\rho(M) = \\|I - t A_S^{\\top}A_S\\|_2$.\nThe eigenvalues of $M$ are $1 - t\\mu_j$, where $\\mu_j$ are the eigenvalues of $A_S^{\\top}A_S$. Thus, the rate constant is given by:\n$$\n\\rho_{\\mathrm{local}} = \\max_j |1 - t \\mu_j(A_S^{\\top}A_S)| = \\max\\left( |1 - t \\mu_{\\min}(A_S^{\\top}A_S)|, |1 - t \\mu_{\\max}(A_S^{\\top}A_S)| \\right).\n$$\n\n### Task 5: Compute the Numerical Value of the Local Convergence Rate\n\nWe first compute the matrix $A_S^{\\top}A_S$. The submatrix $A_S$ consists of the first two columns of $A$:\n$$\nA_S = \\begin{pmatrix} 2  0 \\\\ 0  2 \\\\ 0  0 \\end{pmatrix}.\n$$\nThen,\n$$\nA_S^{\\top}A_S = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  2 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  0 \\\\ 0  4 \\end{pmatrix} = 4I.\n$$\nThe eigenvalues of $A_S^{\\top}A_S$ are $\\mu_1 = 4$ and $\\mu_2 = 4$. So, $\\mu_{\\min}(A_S^{\\top}A_S) = \\mu_{\\max}(A_S^{\\top}A_S) = 4$.\nThe local Lipschitz constant for the gradient on the manifold is $L_S = \\|A_S^{\\top}A_S\\|_2 = \\mu_{\\max}(A_S^{\\top}A_S) = 4$.\nThe problem states that backtracking is used. A standard backtracking line search for ISTA will accept any step size $t_k \\in (0, 1/L_S]$. The procedure typically tries a large step first and reduces it, so it is expected to converge to the largest possible step size that satisfies the descent condition, which is $t = 1/L_S$.\nUsing $t = 1/L_S = 1/4$, we compute the rate constant:\n$$\n\\rho_{\\mathrm{local}} = \\max(|1 - t \\mu_1|, |1 - t \\mu_2|) = \\max\\left(\\left|1 - \\frac{1}{4} \\cdot 4\\right|, \\left|1 - \\frac{1}{4} \\cdot 4\\right|\\right) = \\max(|1-1|, |1-1|) = 0.\n$$\nA rate constant of $0$ indicates that the algorithm converges in a single step once it has fully identified the active manifold and adapted its step-size to the local constant $L_S$.",
            "answer": "$$\n\\boxed{0}\n$$"
        }
    ]
}