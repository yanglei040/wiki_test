{
    "hands_on_practices": [
        {
            "introduction": "Nesterov's accelerated methods elegantly combine an extrapolation step with a proximal gradient update. A natural question arises: is the proximal part truly necessary, or could one simply apply extrapolation to a standard gradient descent step? This exercise  provides a definitive answer by analyzing a simple, one-dimensional scenario. Through a hands-on calculation, you will discover that this naive approach can lead to divergence, reinforcing the fundamental principle that the proximal operator is essential for ensuring stability and convergence in nonsmooth optimization.",
            "id": "3461182",
            "problem": "Consider the composite convex optimization problem central to compressed sensing and sparse optimization: minimize the function $F(x) = f(x) + g(x)$, where $f$ has an $L$-Lipschitz continuous gradient and $g$ is a proper, closed, convex but generally nonsmooth regularizer. A canonical instance is the one-dimensional Lasso objective with $f(x) = \\tfrac{1}{2} L x^{2}$ (so that $\\nabla f(x) = L x$) and $g(x) = \\lambda |x|$. The minimizer in this example is $x^{\\star} = 0$ for any $\\lambda > 0$. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) uses an extrapolation step followed by a composite proximal step that accounts for $g$. In contrast, a naive acceleration that applies extrapolation but omits the proximal step treats the objective as if $g$ were absent and performs an accelerated gradient step only on $f$.\n\nAnalyze the following naive extrapolated scheme that ignores $g$ entirely:\n- Fix $L = 1$, $\\lambda > 0$, a constant extrapolation parameter $\\beta = 2$, and a step size $t = 0.1$ (which satisfies $t \\leq 1/L$).\n- Define the extrapolated point $y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$.\n- Update via the smooth-gradient-only rule $x_{k+1} = y_{k} - t \\nabla f(y_{k})$.\n- Initialize with $x_{-1} = 0$ and $x_{0} = 1$.\n\nStarting from the foundational definitions of $L$-Lipschitz gradients, convexity, and the proximal operator, derive the linear recurrence induced by this naive scheme and compute the spectral radius of the associated $2 \\times 2$ companion matrix governing the state evolution $(x_{k}, x_{k-1})$. Use this calculation to demonstrate that the naive extrapolation can diverge even with a step size $t \\leq 1/L$, thereby justifying the necessity of the composite proximal structure in accelerated methods for nonsmooth regularizers such as the $\\ell_{1}$ norm. Your final answer must be the spectral radius as an exact real number. Do not round your answer.",
            "solution": "The problem asks for an analysis of a naive accelerated gradient method applied to a composite optimization problem. The validity of the problem statement has been confirmed as it is scientifically grounded, well-posed, and objective. We proceed to derive the solution.\n\nThe objective function to minimize is $F(x) = f(x) + g(x)$, where $f(x)$ has an $L$-Lipschitz continuous gradient and $g(x)$ is a nonsmooth regularizer. The specific instance under consideration is the one-dimensional Lasso problem with $f(x) = \\frac{1}{2} L x^2$ and $g(x) = \\lambda |x|$. The problem specifies the following constants: $L=1$, $\\lambda > 0$, a constant extrapolation parameter $\\beta = 2$, and a step size $t = 0.1$. The step size condition $t \\leq \\frac{1}{L}$ is satisfied, as $0.1 \\leq \\frac{1}{1}$.\n\nThe naive scheme ignores the nonsmooth term $g(x)$ and consists of the following steps:\n1. Extrapolation: $y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$\n2. Gradient update on $f$ only: $x_{k+1} = y_{k} - t \\nabla f(y_{k})$\n\nFor the given function $f(x) = \\frac{1}{2} L x^2$ with $L=1$, we have $f(x) = \\frac{1}{2} x^2$. The gradient of $f$ is $\\nabla f(x) = x$. Therefore, the gradient evaluated at the extrapolated point $y_k$ is $\\nabla f(y_k) = y_k$.\n\nWe can now substitute the given parameters and expressions into the iterative scheme.\nThe extrapolation step with $\\beta=2$ is:\n$$y_{k} = x_{k} + 2(x_{k} - x_{k-1}) = 3x_k - 2x_{k-1}$$\n\nThe update step with $t=0.1$ and $\\nabla f(y_k) = y_k$ is:\n$$x_{k+1} = y_{k} - 0.1 y_{k} = (1 - 0.1) y_{k} = 0.9 y_{k}$$\n\nCombining these two equations, we can establish a linear recurrence relation for the sequence $\\{x_k\\}$:\n$$x_{k+1} = 0.9 (3x_k - 2x_{k-1})$$\n$$x_{k+1} = 2.7 x_k - 1.8 x_{k-1}$$\n\nThis is a second-order homogeneous linear recurrence relation with constant coefficients. To analyze its dynamics, we can represent it in a state-space form. Let the state vector at iteration $k$ be $v_k = \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}$. The state transition is given by $v_{k+1} = M v_k$, where $M$ is the companion matrix.\nThe next state vector is $v_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix}$. We can write the system as:\n$$\n\\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix} = \\begin{pmatrix} 2.7 & -1.8 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}\n$$\nThe companion matrix governing the state evolution is therefore:\n$$M = \\begin{pmatrix} 2.7 & -1.8 \\\\ 1 & 0 \\end{pmatrix}$$\n\nThe stability of the system is determined by the spectral radius of $M$, denoted $\\rho(M)$, which is the maximum of the absolute values of its eigenvalues. To find the eigenvalues, we solve the characteristic equation $\\det(M - \\zeta I) = 0$, where $I$ is the identity matrix and $\\zeta$ represents an eigenvalue.\n$$\\det \\begin{pmatrix} 2.7 - \\zeta & -1.8 \\\\ 1 & 0 - \\zeta \\end{pmatrix} = 0$$\n$$(2.7 - \\zeta)(-\\zeta) - (-1.8)(1) = 0$$\n$$-2.7\\zeta + \\zeta^2 + 1.8 = 0$$\n$$\\zeta^2 - 2.7\\zeta + 1.8 = 0$$\n\nWe solve this quadratic equation for $\\zeta$ using the quadratic formula, $\\zeta = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=-2.7$, and $c=1.8$.\n$$\\zeta = \\frac{-(-2.7) \\pm \\sqrt{(-2.7)^2 - 4(1)(1.8)}}{2(1)}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{7.29 - 7.2}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{0.09}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm 0.3}{2}$$\n\nThe two eigenvalues are:\n$$\\zeta_1 = \\frac{2.7 + 0.3}{2} = \\frac{3.0}{2} = 1.5$$\n$$\\zeta_2 = \\frac{2.7 - 0.3}{2} = \\frac{2.4}{2} = 1.2$$\n\nThe spectral radius $\\rho(M)$ is the maximum of the magnitudes of the eigenvalues:\n$$\\rho(M) = \\max(|\\zeta_1|, |\\zeta_2|) = \\max(|1.5|, |1.2|) = 1.5$$\n\nSince the spectral radius $\\rho(M) = 1.5$ is greater than $1$, the iterative scheme is unstable. For the given non-zero initialization ($x_{-1}=0, x_0=1$), the norm of the state vector $\\|v_k\\|$ will grow exponentially, and thus the sequence of iterates $\\{x_k\\}$ will diverge. This demonstrates that applying extrapolation without the corresponding proximal step for the nonsmooth term $g(x)$ can lead to divergence, even when the step size $t$ is chosen small enough to grant convergence for standard gradient descent on the smooth part $f(x)$. This justifies the necessity of the composite proximal structure inherent to algorithms like FISTA, which are designed to handle nonsmooth regularizers and ensure convergence.",
            "answer": "$$\\boxed{1.5}$$"
        },
        {
            "introduction": "The convergence guarantees for accelerated proximal gradient methods are not unconditional; they rest upon key mathematical assumptions about the objective function. A cornerstone of the theory is the requirement that the smooth component, $f(x)$, must have a Lipschitz-continuous gradient, which ensures a global quadratic upper bound on the function. This practice  invites you to probe this theoretical boundary by constructing a counterexample where this condition is violated, revealing why the algorithm's stability and convergence are no longer guaranteed.",
            "id": "3461164",
            "problem": "Consider the composite convex optimization problem typical in compressed sensing,\n$$\\min_{x \\in \\mathbb{R}^n} F(x) := f(x) + g(x),$$\nwhere $f$ is convex and differentiable and $g$ is proper, closed, convex with an efficiently computable proximal operator, such as the sparsity-inducing penalty $g(x) = \\lambda \\lVert x \\rVert_1$ with $\\lambda > 0$. Nesterov acceleration for nonsmooth optimization (for example, the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)) applies an inertial step followed by a proximal gradient step. Its standard convergence guarantees rely on the assumption that $f$ has an $L$-Lipschitz continuous gradient; that is, there exists $L \\in (0,\\infty)$ such that\n$$\\lVert \\nabla f(x) - \\nabla f(y) \\rVert \\le L \\lVert x - y \\rVert \\quad \\text{for all } x,y \\in \\mathbb{R}^n.$$\nEquivalently, the so-called descent lemma (or quadratic upper model) holds:\n$$f(u) \\le f(v) + \\langle \\nabla f(v), u - v \\rangle + \\frac{L}{2} \\lVert u - v \\rVert^2 \\quad \\text{for all } u,v \\in \\mathbb{R}^n.$$\nYour task is to identify a choice of $f$ relevant to compressed sensing for which $\\nabla f$ is not Lipschitz, and to explain from first principles why, in that setting, the usual accelerated proximal method is not guaranteed to converge. Reason from the above definitions and the composite structure; do not assume any properties that were not stated.\n\nWhich option correctly constructs such an $f$ and provides a correct justification?\n\nA. Let $f(x) = \\frac{1}{p} \\lVert A x - b \\rVert_p^p$ with $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $p \\in (1,2)$, and take $g(x) = \\lambda \\lVert x \\rVert_1$. Then $\\nabla f(x) = A^\\top \\left( \\lvert A x - b \\rvert^{p-2} \\odot (A x - b) \\right)$ (componentwise power), which is not Lipschitz at residuals near $0$ because $\\lvert r \\rvert^{p-2} \\to \\infty$ as $r \\to 0$. Since accelerated proximal methods require a global quadratic upper bound with a finite curvature parameter to establish descent and stability of the inertial step, this assumption fails and convergence is not guaranteed.\n\nB. Let $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ and $g(x) = \\lambda \\lVert x \\rVert_1$. Here $\\nabla f(x) = A^\\top (A x - b)$ is $L$-Lipschitz with $L = \\lVert A^\\top A \\rVert$, but accelerated proximal methods are not guaranteed to converge because $f$ is not strongly convex.\n\nC. Let $f(x) = \\lVert A x - b \\rVert_1$ and $g(x) = \\lambda \\lVert x \\rVert_1$. The gradient of $f$ is not Lipschitz, so accelerated proximal methods are not guaranteed to converge.\n\nD. Let $f(x) = \\frac{1}{p} \\lVert A x - b \\rVert_p^p$ with $p \\in (1,2)$ and $g(x) = \\lambda \\lVert x \\rVert_1$. Although $\\nabla f$ is not Lipschitz, convexity of $f$ alone guarantees convergence of accelerated proximal methods, so non-Lipschitzness of $\\nabla f$ is irrelevant.\n\nE. Let $f(x) = \\frac{1}{3} \\lVert A x - b \\rVert_2^3$ and $g(x) = \\lambda \\lVert x \\rVert_1$. Since $\\nabla f$ is not globally Lipschitz (it grows without bound as $\\lVert x \\rVert \\to \\infty$), accelerated proximal methods are not guaranteed to converge from arbitrary initializations.",
            "solution": "The user wants me to act as a meticulous and exacting professor in a STEM field to solve a problem related to Nesterov acceleration in convex optimization.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n- **Problem Formulation:** We consider the composite convex optimization problem $\\min_{x \\in \\mathbb{R}^n} F(x) := f(x) + g(x)$.\n- **Function Properties:**\n    - $f(x)$ is convex and differentiable.\n    - $g(x)$ is a proper, closed, convex function with an efficiently computable proximal operator. A specific example given is the sparsity-inducing penalty $g(x) = \\lambda \\lVert x \\rVert_1$ for $\\lambda > 0$.\n- **Algorithm Context:** The problem concerns Nesterov-accelerated proximal methods (like FISTA) for nonsmooth optimization.\n- **Key Assumption for Convergence:** The standard convergence guarantee for such methods relies on the assumption that $\\nabla f$ is $L$-Lipschitz continuous for some finite $L > 0$. This is defined by:\n    - Condition 1: $\\lVert \\nabla f(x) - \\nabla f(y) \\rVert \\le L \\lVert x - y \\rVert$ for all $x,y \\in \\mathbb{R}^n$.\n    - Condition 2 (equivalent): The descent lemma, $f(u) \\le f(v) + \\langle \\nabla f(v), u - v \\rangle + \\frac{L}{2} \\lVert u - v \\rVert^2$ for all $u,v \\in \\mathbb{R}^n$.\n- **Task:** Identify a function $f(x)$, relevant to compressed sensing, for which $\\nabla f$ is *not* Lipschitz continuous, and explain from first principles why this violates the convergence guarantee of the usual accelerated proximal method.\n\n**1.2. Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is set firmly within the well-established mathematical framework of convex optimization and its application to compressed sensing. The concepts of Lipschitz continuity, proximal operators, FISTA, and composite objective functions are standard and correctly defined. The relationship between the Lipschitz continuity of the gradient and the convergence of first-order methods is a fundamental topic in optimization theory. The problem is scientifically and mathematically sound.\n- **Well-Posedness:** The task is to construct a specific example and provide a theoretical explanation based on given principles. This is a well-defined conceptual problem that admits a clear and verifiable answer.\n- **Objectivity:** The problem statement uses precise, unambiguous mathematical language and definitions. It is free from subjective content.\n- **Completeness and Consistency:** All necessary definitions and context are provided. The premises are consistent with one another.\n\n**1.3. Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n### Step 2: Derivation and Option Analysis\n\nThe core of the problem lies in understanding the role of the $L$-Lipschitz continuity of $\\nabla f$ in the convergence proofs of accelerated proximal gradient methods. This property, through the equivalent descent lemma, provides a global quadratic upper bound on the function $f$. An accelerated method like FISTA performs an update of the form:\n$$ y_k = x_k + \\theta_k (x_k - x_{k-1}) $$\n$$ x_{k+1} = \\mathrm{prox}_{t g}(y_k - t \\nabla f(y_k)) $$\nwhere $\\theta_k$ is an inertial parameter and $t$ is the step size. The convergence analysis critically relies on the inequality derived from the descent lemma, which holds for a step size $t \\le 1/L$:\n$$ F(x_{k+1}) \\le F(y_k) - \\frac{1}{2t} \\lVert x_{k+1} - y_k \\rVert^2 + \\frac{t}{2} \\lVert \\nabla f(y_k) \\rVert^2 - \\frac{t}{2} \\lVert \\nabla f(y_k) + \\frac{1}{t}(x_{k+1} - y_k) \\rVert^2 $$\n(This is one form of the key one-step progress inequality). This bound on the objective value at the new iterate $x_{k+1}$ is essential. When combined with the analysis of the inertial step, it allows for the construction of a Lyapunov function that proves the $O(1/k^2)$ convergence rate of the objective value. If no such global, finite $L$ exists, a fixed step size $t$ cannot be chosen to guarantee this descent property for all possible iterates $y_k$. The stability of the method, which must control the potential \"overshoot\" from the inertial step, is compromised, and the convergence proof collapses.\n\nWe must now find an $f$ with a non-Lipschitz gradient and evaluate the options.\n\n**Analysis of Option A:**\n- **Function Choice:** $f(x) = \\frac{1}{p} \\lVert A x - b \\rVert_p^p$ with $p \\in (1,2)$. This defines a data fidelity term based on an $\\ell_p$-norm, which is a relevant generalization of least squares in robust statistics and signal processing, especially for handling non-Gaussian noise.\n- **Differentiability and Convexity:** For $p \\ge 1$, the function $t \\mapsto |t|^p$ is convex. The composition with an affine map $x \\mapsto Ax-b$ preserves convexity. Thus, $f(x)$ is convex. The gradient is given by $\\nabla f(x) = A^\\top \\psi(Ax-b)$, where the $i$-th component of $\\psi(r)$ is $\\psi_i(r_i) = |r_i|^{p-1}\\mathrm{sgn}(r_i)$. For $p \\in (1,2)$, the exponent $p-1$ is in $(0,1)$, making $\\psi_i(r_i)$ a continuous function of $r_i$. Therefore, $f(x)$ is continuously differentiable ($C^1$).\n- **Lipschitz Continuity of $\\nabla f$:** To check for Lipschitz continuity of the gradient, we examine its derivative, which relates to the Hessian of $f$. The derivative of $\\psi_i(r_i)$ is $(p-1)|r_i|^{p-2}$. Since $p \\in (1,2)$, the exponent $p-2$ is in $(-1,0)$. Consequently, as $r_i \\to 0$, this derivative tends to infinity. This implies that the Hessian of $f$ is unbounded in any neighborhood of a point $x_0$ for which a component of the residual, $(Ax_0 - b)_i$, is zero. Therefore, $\\nabla f$ is not locally Lipschitz at such points, and thus cannot be globally Lipschitz.\n- **Justification:** The option correctly states that $\\lvert r \\rvert^{p-2} \\to \\infty$ as $r \\to 0$, causing the non-Lipschitz behavior. It then correctly concludes that because accelerated methods require a \"global quadratic upper bound with a finite curvature parameter\" (which is exactly the descent lemma with a finite $L$), the convergence guarantee fails. This explanation is precise and gets to the heart of the mathematical failure in the proof.\n- **Verdict:** **Correct**.\n\n**Analysis of Option B:**\n- **Function Choice:** $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$. This is the standard least-squares objective.\n- **Lipschitz Continuity of $\\nabla f$:** The gradient is $\\nabla f(x) = A^\\top(Ax-b)$. This is an affine function of $x$. Its derivative (the Hessian of $f$) is the constant matrix $A^\\top A$. Therefore, $\\nabla f$ is globally Lipschitz with the Lipschitz constant $L = \\lVert A^\\top A \\rVert_2$ (the spectral norm). The option correctly states that $\\nabla f$ is $L$-Lipschitz, but this contradicts the task requirement of finding a function where it is *not* Lipschitz.\n- **Justification:** The claim that convergence is not guaranteed because $f$ is not strongly convex is false. Lack of strong convexity affects the *rate* of convergence (from linear to sublinear, $O(1/k^2)$), but FISTA is proven to converge to the minimum objective value even for non-strongly convex functions.\n- **Verdict:** **Incorrect**.\n\n**Analysis of Option C:**\n- **Function Choice:** $f(x) = \\lVert A x - b \\rVert_1$.\n- **Differentiability:** The $\\ell_1$-norm is not differentiable at points where its argument is zero. In this case, $f(x)$ is not differentiable whenever any component of the residual $Ax-b$ is zero. This violates the problem's explicit assumption that \"$f$ is convex and differentiable\". The problem structure is designed for a smooth part $f$ and a non-smooth part $g$. This option incorrectly places a non-smooth function in the role of $f$.\n- **Verdict:** **Incorrect**.\n\n**Analysis of Option D:**\n- **Function Choice:** $f(x) = \\frac{1}{p} \\lVert A x - b \\rVert_p^p$ with $p \\in (1,2)$. This choice is valid, as established in the analysis of Option A.\n- **Justification:** The justification states that \"convexity of $f$ alone guarantees convergence of accelerated proximal methods, so non-Lipschitzness of $\\nabla f$ is irrelevant.\" This is fundamentally false. The convergence proofs for standard accelerated methods like FISTA with fixed step sizes explicitly and non-negotiably require the Lipschitz continuity of $\\nabla f$. Removing this assumption invalidates the entire argument for convergence.\n- **Verdict:** **Incorrect**.\n\n**Analysis of Option E:**\n- **Function Choice:** $f(x) = \\frac{1}{3} \\lVert A x - b \\rVert_2^3$. This function is convex and differentiable.\n- **Lipschitz Continuity of $\\nabla f$:** The gradient is $\\nabla f(x) = \\lVert Ax - b \\rVert_2 A^\\top (Ax-b)$. Let's test the Lipschitz condition. Consider $A=I$ and $b=0$, so $f(x) = \\frac{1}{3}\\lVert x \\rVert_2^3$ and $\\nabla f(x) = \\lVert x \\rVert_2 x$. The condition $\\lVert \\nabla f(x) - \\nabla f(y) \\rVert \\le L \\lVert x - y \\rVert$ must hold for all $x,y$. If we take $y=0$, this becomes $\\lVert \\lVert x \\rVert_2 x \\rVert \\le L \\lVert x \\rVert_2$, which simplifies to $\\lVert x \\rVert_2^2 \\le L \\lVert x \\rVert_2$, or $\\lVert x \\rVert_2 \\le L$. This cannot hold for all $x \\in \\mathbb{R}^n$, as $\\lVert x \\rVert_2$ can be arbitrarily large. Thus, $\\nabla f$ is not globally Lipschitz.\n- **Justification:** The reasoning is that because $\\nabla f$ is not globally Lipschitz, convergence is not guaranteed \"from arbitrary initializations\". This is a correct statement. An initialization or subsequent iterate far from the origin would experience a very large local \"curvature\", causing a fixed-step algorithm to diverge. However, this example and reasoning are arguably weaker than in Option A. The failure occurs as $\\lVert x \\rVert \\to \\infty$, which might be considered less fundamental than the local failure in Option A, which can occur inside any bounded set. Furthermore, the explanation in A is more fundamental as it directly references the \"quadratic upper bound\" which is the specific mechanism in the proof that fails.\n- **Verdict:** While technically correct, Option A provides a more canonical example and a more precise, first-principles explanation, making it the superior choice.\n\n**Conclusion:**\nOption A provides a valid example of a function $f$ relevant to compressed sensing and robust optimization whose gradient is not Lipschitz continuous. Its explanation for why this breaks the convergence guarantee of accelerated methods is the most accurate and fundamental among the choices, correctly identifying the failure to satisfy the descent lemma (global quadratic upper bound) as the root cause.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Having explored the theoretical foundations, the final step is to translate these concepts into a functioning, robust algorithm. This hands-on programming challenge  guides you through the implementation of a full-featured FISTA solver for the canonical LASSO problem. You will move beyond the basic algorithm by incorporating practical enhancements such as backtracking line search for the step size and an adaptive restart strategy to improve performance, culminating in a powerful tool for sparse optimization.",
            "id": "3461192",
            "problem": "You are asked to derive and implement an accelerated proximal gradient method with line search and restart for a composite convex objective that is canonical in compressed sensing and sparse optimization. Consider the problem of minimizing a composite function of the form $F(x) = f(x) + g(x)$, where $f(x)$ is a smooth convex function with Lipschitz-continuous gradient and $g(x)$ is a proper closed convex function for which the proximal operator is efficiently computable. In this assignment, you will focus on the least-squares data fidelity with an $\\ell_{1}$-norm regularizer, namely\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda \\ge 0$.\n\nStarting from the following fundamental bases:\n- The gradient of $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is $\\nabla f(x) = A^{\\top} (A x - b)$, and is Lipschitz-continuous with a Lipschitz constant $L_{f}$ equal to the squared spectral norm of $A$, that is $L_{f} = \\|A\\|_{2}^{2}$.\n- The proximal operator of the scaled $\\ell_{1}$-norm $g(x)=\\lambda \\|x\\|_{1}$ at a point $z$ with stepsize $\\alpha>0$ is given by componentwise soft-thresholding, that is $\\operatorname{prox}_{\\alpha g}(z) = \\mathcal{S}_{\\alpha \\lambda}(z)$ where $\\left(\\mathcal{S}_{\\tau}(z)\\right)_{i} = \\operatorname{sign}(z_{i}) \\max\\{|z_{i}| - \\tau, 0\\}$.\n- Nesterov acceleration can be used to extrapolate iterates in the proximal gradient method through a momentum sequence defined by a scalar sequence $\\{t_{k}\\}_{k \\ge 0}$ and an extrapolation step $y_{k}$ built from iterates $x_{k}$.\n- A backtracking line search can enforce the quadratic upper bound for $f(x)$ given a current point $y$, a trial $x$, and a candidate Lipschitz constant $L$: \n$$\nf(x) \\le f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{L}{2}\\|x - y\\|_{2}^{2}.\n$$\n- An adaptive restart condition can be used to reset the acceleration when the objective value increases.\n\nYour tasks are:\n- Derive, from the bases above, a complete set of updates for an accelerated proximal gradient method with Nesterov momentum, backtracking line search on the local Lipschitz constant, and an objective-based adaptive restart rule.\n- Implement the derived algorithm as a program that solves the sparse least-squares problem for a provided test suite. Your implementation must compute the gradient, the proximal operator, enforce the backtracking inequality above, update the Nesterov momentum, and apply a restart rule when it improves convergence stability.\n\nProgram input and randomness:\n- The program must be self-contained and must not read any user input. When randomness is needed, use a fixed seed so that results are deterministic.\n\nStopping rule:\n- Use a stopping rule that combines both a relative iterate change threshold and a proximal-gradient mapping condition suitable for composite optimization. Let $\\epsilon$ denote a small tolerance. The iterate change test can be based on $\\|x_{k+1} - x_{k}\\|_{2}/\\max\\{1,\\|x_{k}\\|_{2}\\} \\le \\epsilon$. The proximal-gradient mapping for stepsize $L$ at extrapolated point $y$ is $G_{L}(y) = L\\left(y - \\operatorname{prox}_{g/L}\\left(y - \\nabla f(y)/L\\right)\\right)$. Use the infinity norm $\\|G_{L}(y)\\|_{\\infty} \\le \\epsilon'$ with a choice of $\\epsilon'$ commensurate with $\\epsilon$.\n\nTest suite:\nImplement your method and run it on the following four cases. All matrices and vectors must be generated exactly as specified. For each case, the final result must be a single basic type (a boolean, an integer, or a float). Use the Euclidean norm $\\|\\cdot\\|_{2}$ and the infinity norm $\\|\\cdot\\|_{\\infty}$ as appropriate.\n\n- Case A (compressed sensing “happy path”):\n  - Dimensions: $m = 40$, $n = 120$.\n  - Sparsity: $k = 8$ nonzeros in the ground-truth $x_{\\star}$.\n  - Random generation: set the random seed to $12345$. Generate $A$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Generate a $k$-sparse $x_{\\star}$ by choosing $k$ indices uniformly without replacement and assigning independent standard normal values to those entries, with zeros elsewhere. Generate noise $e$ with independent normal entries of variance $\\sigma^{2}$ where $\\sigma = 10^{-3}$, and set $b = A x_{\\star} + e$.\n  - Regularization: $\\lambda = 0.01 \\cdot \\|A^{\\top} b\\|_{\\infty}$.\n  - Output for this case: the relative solution error $\\|x_{\\text{alg}} - x_{\\star}\\|_{2} / \\max\\{1, \\|x_{\\star}\\|_{2}\\}$ as a float.\n\n- Case B (boundary condition $\\lambda = 0$):\n  - Dimensions: $m = 50$, $n = 20$.\n  - Random generation: set the random seed to $54321$. Generate $A$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Generate $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 0$.\n  - Baseline: compute the least-squares solution $x_{\\text{LS}}$ as the minimum-norm solution of $\\min_{x} \\|A x - b\\|_{2}^{2}$.\n  - Output for this case: the relative discrepancy $\\|x_{\\text{alg}} - x_{\\text{LS}}\\|_{2} / \\max\\{1, \\|x_{\\text{LS}}\\|_{2}\\}$ as a float.\n\n- Case C (edge case with very strong regularization):\n  - Dimensions: $m = 30$, $n = 50$.\n  - Random generation: set the random seed to $11111$. Generate $A$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Generate $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 100 \\cdot \\|A^{\\top} b\\|_{\\infty}$.\n  - Output for this case: the integer cardinality of the estimated support, i.e., the number of indices $i$ such that $|x_{\\text{alg}, i}| > 10^{-8}$.\n\n- Case D (rank-deficient design matrix):\n  - Dimensions: $m = 30$, $n = 60$ constructed as follows. Set the random seed to $22222$. First generate $A_{0} \\in \\mathbb{R}^{m \\times 30}$ with independent standard normal entries and scale columns by $1/\\sqrt{m}$. Then set $A = [A_{0} \\;\\; A_{0}]$ by concatenating $A_{0}$ with itself. Generate $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 0.05 \\cdot \\|A^{\\top} b\\|_{\\infty}$.\n  - Output for this case: the Karush–Kuhn–Tucker (KKT) stationarity residual at $x_{\\text{alg}}$, defined as the infinity norm of a smallest subgradient residual\n    $$\n    r(x) \\equiv \\left\\|\\nabla f(x) + \\lambda v \\right\\|_{\\infty},\n    $$\n    where $v \\in \\partial \\|x\\|_{1}$ is any subgradient. Compute $r(x)$ via the componentwise formula where\n    $$\n    \\left(\\nabla f(x)\\right)_{i} = \\left(A^{\\top}(A x - b)\\right)_{i},\n    $$\n    and\n    $$\n    r(x) = \\max\\left\\{ \\max_{i: x_{i} \\neq 0} \\left| \\left(\\nabla f(x)\\right)_{i} + \\lambda \\operatorname{sign}(x_{i}) \\right|, \\; \\max_{i: x_{i} = 0} \\max\\left\\{0, \\left|\\left(\\nabla f(x)\\right)_{i}\\right| - \\lambda\\right\\} \\right\\}.\n    $$\n    Use the convention that “$x_{i} = 0$” is tested numerically by $|x_{i}| \\le 10^{-12}$. Return $r(x)$ as a float.\n\nImplementation constraints:\n- Use Nesterov acceleration and a restart rule that resets the momentum when the objective increases, i.e., if $F(x_{k+1}) > F(x_{k})$, set the momentum scalar to $t_{k+1} = 1$ and the extrapolated point to $y_{k+1} = x_{k+1}$.\n- Use a backtracking line search that starts from a current estimate of a local Lipschitz constant $L$ and multiplies it by a factor $\\eta > 1$ until the quadratic upper bound condition for $f$ is satisfied.\n- Initialize the stepsize using $L_{0} = \\|A\\|_{2}^{2}$ obtained by a singular-value computation or an equivalent spectral norm evaluation.\n\nNumerical tolerances:\n- Use a maximum of $N_{\\max} = 10000$ iterations and a tolerance $\\epsilon = 10^{-8}$ for iterate change. Choose the proximal-gradient mapping tolerance $\\epsilon'$ to be of the same order as $\\epsilon$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results of the four cases as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, Case D. For example, the output format must be exactly like\n$[r_{A}, r_{B}, r_{C}, r_{D}]$\nwhere $r_{A}$, $r_{B}$, and $r_{D}$ are floats and $r_{C}$ is an integer. No additional text should be printed.",
            "solution": "The user requires the derivation and implementation of an accelerated proximal gradient method tailored for the sparse least-squares problem, commonly known as LASSO. The algorithm must incorporate Nesterov-style momentum, a backtracking line search for the step size, and an adaptive restart mechanism based on the objective function's behavior.\n\n### 1. Problem Formulation\n\nThe optimization problem is to minimize a composite objective function $F(x)$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv f(x) + g(x)\n$$\nwhere\n- $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is the smooth, convex data fidelity term. Its gradient is $\\nabla f(x) = A^{\\top} (A x - b)$, which is Lipschitz-continuous with constant $L_f = \\|A\\|_{2}^{2}$.\n- $g(x) = \\lambda \\|x\\|_{1}$ is the convex, non-smooth regularization term. Its proximal operator, $\\operatorname{prox}_{\\alpha g}(z)$, is the soft-thresholding operator $\\mathcal{S}_{\\alpha\\lambda}(z)$, defined component-wise as $(\\mathcal{S}_{\\tau}(z))_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\tau, 0\\}$.\n\n### 2. Algorithm Derivation\n\nThe algorithm is a variant of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA). We will construct the update rules step-by-step, combining the required components. Let the sequence of iterates be $\\{x_k\\}$.\n\n#### 2.1. Core Proximal Gradient Step\n\nThe basic iterative step for minimizing $F(x)$ is the proximal gradient update. At an iterate $y$, and with a step size $\\alpha > 0$, the next iterate $x^{+}$ is found by minimizing a quadratic approximation of $f(x)$ around $y$ plus the non-smooth term $g(x)$:\n$$\nx^{+} = \\arg\\min_{x} \\left( f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{1}{2\\alpha}\\|x - y\\|_{2}^{2} + g(x) \\right)\n$$\nCompleting the square, this is equivalent to:\n$$\nx^{+} = \\arg\\min_{x} \\left( \\frac{1}{2\\alpha}\\|x - (y - \\alpha \\nabla f(y))\\|_{2}^{2} + g(x) \\right)\n$$\nThis is the definition of the proximal operator of $g$ scaled by $\\alpha$:\n$$\nx^{+} = \\operatorname{prox}_{\\alpha g}(y - \\alpha \\nabla f(y))\n$$\nIn our case, with step size $\\alpha = 1/L$ where $L$ is a local Lipschitz estimate, the update is:\n$$\nx^{+} = \\mathcal{S}_{\\lambda/L}(y - \\frac{1}{L}A^{\\top}(Ay-b))\n$$\n\n#### 2.2. Nesterov Acceleration\n\nNesterov's acceleration introduces a \"momentum\" term by computing the proximal gradient step not at the current iterate $x_k$, but at an extrapolated point $y_k$. The update sequence for the iterates $\\{x_k\\}$, extrapolated points $\\{y_k\\}$, and momentum scalars $\\{t_k\\}$ is as follows:\n1.  **Compute next iterate**: $x_{k+1} = \\operatorname{prox}_{g/L_k}(y_k - \\frac{1}{L_k}\\nabla f(y_k))$\n2.  **Update momentum scalar**: $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n3.  **Extrapolate for next step**: $y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$\n\nThe initial conditions are $x_0 = 0$, $y_0 = x_0$, and $t_0 = 1$. Note that some variants exist; we choose a formulation that aligns well with the restart rule.\n\n#### 2.3. Backtracking Line Search\n\nThe global Lipschitz constant $L_f = \\|A\\|_2^2$ can be a pessimistic over-estimate of the local curvature, leading to slow convergence. A backtracking line search adaptively finds a suitable local constant $L_k$ at each iteration. Starting with an initial guess for $L$ (e.g., the one from the previous step, $L_{k-1}$), we check if the following quadratic upper bound on $f$ is satisfied for the candidate point $x_{k+1}$ computed from $y_k$:\n$$\nf(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L_k}{2}\\|x_{k+1} - y_k\\|_{2}^{2}\n$$\nIf the condition fails, we increase $L_k$ by a factor $\\eta > 1$ (e.g., $\\eta=2$) and re-compute $x_{k+1}$ until the condition is met. This ensures sufficient decrease in the smooth part of the objective.\n\n#### 2.4. Adaptive Restart\n\nNesterov acceleration is not a descent method; the objective function $F(x_k)$ is not guaranteed to decrease monotonically. When the objective value increases (i.e., $F(x_{k+1}) > F(x_k)$), it's a sign that the momentum is too aggressive and is overshooting the minimum. The adaptive restart rule handles this by resetting the momentum. We implement the rule as specified: if $F(x_{k+1}) > F(x_k)$:\n- Reset the momentum scalar for the next step: $t_{k+1} = 1$.\n- Reset the extrapolation point for the next step: $y_{k+1} = x_{k+1}$.\n\n### 3. Complete Algorithm\n\nCombining these components, we arrive at the following algorithm.\n\n**Initialization**:\n- Set $k=0$, $x_0 = 0 \\in \\mathbb{R}^n$, $y_0 = x_0$, $t_0=1$.\n- Set max iterations $N_{\\max}$, tolerances $\\epsilon, \\epsilon'$.\n- Set backtracking factor $\\eta > 1$.\n- Compute initial Lipschitz estimate $L_0 = \\|A\\|_2^2$.\n- Set $F_{-1} = \\infty$.\n\n**Main Loop (for $k = 0, 1, \\dots, N_{\\max}-1$)**:\n1.  Let $x_{\\text{prev}} = x_k$, $t_{\\text{prev}} = t_k$.\n2.  **Backtracking Line Search**:\n    a. Initialize trial $L = L_k / \\eta$.\n    b. Repeatedly update $L \\leftarrow \\eta L$ until the condition\n       $f(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L}{2}\\|x_{k+1} - y_k\\|_{2}^{2}$\n       is satisfied, where $x_{k+1} = \\mathcal{S}_{\\lambda/L}(y_k - \\frac{1}{L}\\nabla f(y_k))$.\n    c. Set $L_{k+1} = L$.\n3.  **Check Stopping Criteria**:\n    a. Relative change: $\\delta_x = \\|x_{k+1} - x_k\\|_{2} / \\max\\{1, \\|x_k\\|_{2}\\}$.\n    b. Proximal-gradient stationarity: $\\|G_L(y_k)\\|_\\infty = \\|L(y_k - x_{k+1})\\|_\\infty$.\n    c. If $\\delta_x \\le \\epsilon$ and $\\|G_L(y_k)\\|_\\infty \\le \\epsilon'$, terminate and return $x_{k+1}$.\n4.  **Update for Next Iteration (with Restart)**:\n    a. Compute objective values $F_k = F(x_k)$ and $F_{k+1} = F(x_{k+1})$.\n    b. **If** $F_{k+1} > F_k$:\n        i.  $t_{k+1} = 1$.\n        ii. $y_{k+1} = x_{k+1}$.\n    c. **Else** (no restart):\n        i.  $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$.\n        ii. $y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$.\n5.  Increment $k \\leftarrow k+1$.\n\nThis defines the complete set of updates for the required algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to derive, implement, and test an accelerated proximal gradient method\n    with line search and restart for sparse least-squares problems.\n    \"\"\"\n\n    def fista_with_restart(A, b, lambda_val, max_iter, tol, tol_prox_grad):\n        \"\"\"\n        Implements the FISTA algorithm with backtracking line search and adaptive restart\n        for the LASSO problem: min 0.5*||Ax-b||^2 + lambda*||x||_1.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_val (float): The regularization parameter.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Tolerance for relative iterate change.\n            tol_prox_grad (float): Tolerance for the proximal gradient norm.\n\n        Returns:\n            np.ndarray: The optimized solution vector x.\n        \"\"\"\n        m, n = A.shape\n        eta = 2.0\n\n        def f(x_vec, A_mat, b_vec):\n            return 0.5 * np.linalg.norm(A_mat @ x_vec - b_vec)**2\n\n        def g(x_vec, lam):\n            return lam * np.linalg.norm(x_vec, 1)\n\n        def soft_threshold(z, tau):\n            return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n        # Initialization\n        x_curr = np.zeros(n)\n        y_curr = np.zeros(n)\n        t_curr = 1.0\n        \n        # Initial Lipschitz constant estimate from the spectral norm of A.\n        L = np.linalg.norm(A, 2)**2\n\n        # Storing the objective value of the previous iterate for the restart condition.\n        F_x_prev = f(x_curr, A, b) + g(x_curr, lambda_val)\n\n        for k in range(max_iter):\n            x_prev = x_curr\n            t_prev = t_curr\n\n            # Backtracking line search\n            L_trial = L / eta  # Start with a smaller L for efficiency\n            while True:\n                grad_y = A.T @ (A @ y_curr - b)\n                z = y_curr - grad_y / L_trial\n                x_curr = soft_threshold(z, lambda_val / L_trial)\n                \n                f_x = f(x_curr, A, b)\n                f_y = f(y_curr, A, b)\n                \n                quadratic_approx = f_y + np.dot(grad_y, x_curr - y_curr) + (L_trial / 2.0) * np.linalg.norm(x_curr - y_curr)**2\n                \n                if f_x <= quadratic_approx:\n                    L = L_trial\n                    break\n                else:\n                    L_trial *= eta\n            \n            # Stopping conditions\n            rel_change = np.linalg.norm(x_curr - x_prev) / max(1.0, np.linalg.norm(x_prev))\n            prox_grad_norm = L * np.linalg.norm(y_curr - x_curr, np.inf)\n\n            if k > 0 and rel_change <= tol and prox_grad_norm <= tol_prox_grad:\n                break\n            \n            F_x_curr = f_x + g(x_curr, lambda_val)\n            \n            # Restart check and momentum update for the next iteration\n            if k > 0 and F_x_curr > F_x_prev:\n                t_curr = 1.0\n                y_next = x_curr\n            else:\n                t_curr = (1.0 + np.sqrt(1.0 + 4.0 * t_prev**2)) / 2.0\n                y_next = x_curr + ((t_prev - 1.0) / t_curr) * (x_curr - x_prev)\n\n            # Update state for next iteration\n            y_curr = y_next\n            F_x_prev = F_x_curr\n            \n        return x_curr\n\n    # General parameters\n    MAX_ITER = 10000\n    TOL = 1e-8\n    results = []\n    \n    # --- Case A: Compressed sensing \"happy path\" ---\n    m_A, n_A, k_sparse_A = 40, 120, 8\n    rng_A = np.random.default_rng(12345)\n    A_A = rng_A.standard_normal((m_A, n_A)) / np.sqrt(m_A)\n    x_star_A = np.zeros(n_A)\n    support_A = rng_A.choice(n_A, k_sparse_A, replace=False)\n    x_star_A[support_A] = rng_A.standard_normal(k_sparse_A)\n    e_A = rng_A.normal(0, 1e-3, m_A)\n    b_A = A_A @ x_star_A + e_A\n    lambda_A = 0.01 * np.linalg.norm(A_A.T @ b_A, np.inf)\n    \n    x_alg_A = fista_with_restart(A_A, b_A, lambda_A, MAX_ITER, TOL, TOL)\n    rel_err_A = np.linalg.norm(x_alg_A - x_star_A) / max(1.0, np.linalg.norm(x_star_A))\n    results.append(rel_err_A)\n\n    # --- Case B: Boundary condition lambda = 0 ---\n    m_B, n_B = 50, 20\n    rng_B = np.random.default_rng(54321)\n    A_B = rng_B.standard_normal((m_B, n_B)) / np.sqrt(m_B)\n    b_B = rng_B.standard_normal(m_B)\n    lambda_B = 0.0\n    \n    x_alg_B = fista_with_restart(A_B, b_B, lambda_B, MAX_ITER, TOL, TOL)\n    x_ls_B, _, _, _ = np.linalg.lstsq(A_B, b_B, rcond=None)\n    rel_discrepancy_B = np.linalg.norm(x_alg_B - x_ls_B) / max(1.0, np.linalg.norm(x_ls_B))\n    results.append(rel_discrepancy_B)\n\n    # --- Case C: Very strong regularization ---\n    m_C, n_C = 30, 50\n    rng_C = np.random.default_rng(11111)\n    A_C = rng_C.standard_normal((m_C, n_C)) / np.sqrt(m_C)\n    b_C = rng_C.standard_normal(m_C)\n    lambda_C = 100 * np.linalg.norm(A_C.T @ b_C, np.inf)\n    \n    x_alg_C = fista_with_restart(A_C, b_C, lambda_C, MAX_ITER, TOL, TOL)\n    cardinality_C = np.sum(np.abs(x_alg_C) > 1e-8)\n    results.append(cardinality_C)\n\n    # --- Case D: Rank-deficient design matrix ---\n    m_D, n_half_D = 30, 30\n    rng_D = np.random.default_rng(22222)\n    A0_D = rng_D.standard_normal((m_D, n_half_D)) / np.sqrt(m_D)\n    A_D = np.hstack([A0_D, A0_D])\n    b_D = rng_D.standard_normal(m_D)\n    lambda_D = 0.05 * np.linalg.norm(A_D.T @ b_D, np.inf)\n\n    x_alg_D = fista_with_restart(A_D, b_D, lambda_D, MAX_ITER, TOL, TOL)\n    grad_f_D = A_D.T @ (A_D @ x_alg_D - b_D)\n    \n    tol_kkt_zero = 1e-12\n    is_zero = np.abs(x_alg_D) <= tol_kkt_zero\n    is_nonzero = ~is_zero\n    \n    res_nonzero = np.abs(grad_f_D[is_nonzero] + lambda_D * np.sign(x_alg_D[is_nonzero]))\n    res_zero = np.maximum(0, np.abs(grad_f_D[is_zero]) - lambda_D)\n    \n    max_res_nonzero = np.max(res_nonzero) if res_nonzero.size > 0 else 0.0\n    max_res_zero = np.max(res_zero) if res_zero.size > 0 else 0.0\n    \n    kkt_residual_D = np.max([max_res_nonzero, max_res_zero])\n    results.append(kkt_residual_D)\n\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```"
        }
    ]
}