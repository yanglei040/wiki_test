{
    "hands_on_practices": [
        {
            "introduction": "掌握任何迭代算法的第一步都是彻底理解单次迭代的过程。这个练习  将分解投影次梯度法的一个步骤。它要求你为 $\\ell_1$ 范数计算一个特定的次梯度，然后执行到概率单纯形上的投影，这是许多优化问题中的一个基本构建模块。",
            "id": "3483166",
            "problem": "考虑在标准概率单纯形 $\\Delta := \\{x \\in \\mathbb{R}^{5} : x \\ge 0,\\ \\mathbf{1}^{\\top}x = 1\\}$ 上最小化 $\\ell_{1}$ 范数 $f(x) = \\|x\\|_{1}$ 的非光滑凸优化问题。从初始点 $x^{(0)} \\in \\Delta$ 开始，给定步长 $\\alpha  0$，一次投影次梯度下降迭代的形式为 $x^{+} = \\Pi_{\\Delta}\\big(x^{(0)} - \\alpha\\,g^{(0)}\\big)$，其中 $g^{(0)} \\in \\partial \\|x^{(0)}\\|_{1}$ 是 $f$ 在 $x^{(0)}$ 处的任意一个次梯度，$\\partial \\|x^{(0)}\\|_{1}$ 表示次微分，$\\Pi_{\\Delta}$ 是到 $\\Delta$ 上的欧几里得投影，即对于给定的 $y$，$\\frac{1}{2}\\|z - y\\|_{2}^{2}$ 在 $z \\in \\Delta$ 上的唯一最小化子。\n\n在推导中仅使用以下基本事实：\n- 对于一个凸函数 $f$，在 $x$ 处的次梯度 $g$ 是对所有 $y$ 都满足 $f(y) \\ge f(x) + g^{\\top}(y - x)$ 的任意向量。\n- 对于 $f(x)=\\|x\\|_{1}$，一个有效的次梯度是当 $x_{i} \\ne 0$ 时，$g_{i} = \\operatorname{sign}(x_{i})$；当 $x_{i} = 0$ 时，$g_{i}$ 是 $[-1,1]$ 中的任意值。\n- 到一个闭凸集上的欧几里得投影是相应二次规划的唯一解，该解可由 Karush–Kuhn–Tucker (KKT) 条件刻画。\n\n设初始点为\n$$\nx^{(0)} = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{1}{4} \\\\ \\frac{1}{10} \\\\ \\frac{1}{20} \\\\ 0 \\end{pmatrix},\n$$\n步长为 $\\alpha = \\frac{3}{10}$。按分量选择次梯度 $g^{(0)} \\in \\partial \\|x^{(0)}\\|_{1}$，规则为：当 $x^{(0)}_{i} > 0$ 时，$g^{(0)}_{i} = 1$；当 $x^{(0)}_{i} = 0$ 时，$g^{(0)}_{i} = 0$。\n\n通过推导欧几里得投影子问题的 KKT 条件并求解有效集和阈值，精确计算下一个迭代点 $x^{+} = \\Pi_{\\Delta}\\big(x^{(0)} - \\alpha\\,g^{(0)}\\big)$。将最终答案表示为单个行向量。无需四舍五入，不涉及单位。",
            "solution": "该问题被评估为有效。这是一个凸优化领域内定义明确的数学问题，所有必要的数据和条件都已一致地提供。不存在科学或事实上的不健全、模糊或矛盾之处。\n\n任务是计算一次投影次梯度法迭代，用于在概率单纯形 $\\Delta$ 上最小化函数 $f(x) = \\|x\\|_{1}$。更新公式为 $x^{+} = \\Pi_{\\Delta}\\big(x^{(0)} - \\alpha\\,g^{(0)}\\big)$。\n\n该过程包括三个主要步骤：\n1.  确定在初始点 $x^{(0)}$ 处的次梯度 $g^{(0)}$。\n2.  计算中间向量 $y = x^{(0)} - \\alpha\\,g^{(0)}$。\n3.  计算 $y$ 到单纯形 $\\Delta$ 上的欧几里得投影，即 $x^{+} = \\Pi_{\\Delta}(y)$。\n\n**步骤 1：计算次梯度 $g^{(0)}$**\n\n初始点给定为\n$$x^{(0)} = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{1}{4} \\\\ \\frac{1}{10} \\\\ \\frac{1}{20} \\\\ 0 \\end{pmatrix}$$\n问题指定了选择次梯度 $g^{(0)} \\in \\partial \\|x^{(0)}\\|_{1}$ 的规则为：当 $x^{(0)}_{i} > 0$ 时，$g^{(0)}_{i} = 1$；当 $x^{(0)}_{i} = 0$ 时，$g^{(0)}_{i} = 0$。\n$x^{(0)}$ 的前四个分量为正，第五个分量为零。应用此规则得到：\n$g^{(0)}_{1} = 1$ 因为 $x^{(0)}_{1} = \\frac{3}{5} > 0$。\n$g^{(0)}_{2} = 1$ 因为 $x^{(0)}_{2} = \\frac{1}{4} > 0$。\n$g^{(0)}_{3} = 1$ 因为 $x^{(0)}_{3} = \\frac{1}{10} > 0$。\n$g^{(0)}_{4} = 1$ 因为 $x^{(0)}_{4} = \\frac{1}{20} > 0$。\n$g^{(0)}_{5} = 0$ 因为 $x^{(0)}_{5} = 0$。\n因此，次梯度向量是\n$$g^{(0)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n\n**步骤 2：计算中间向量 $y$**\n\n中间向量是 $y = x^{(0)} - \\alpha\\,g^{(0)}$，步长 $\\alpha = \\frac{3}{10}$。\n$$y = \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{1}{4} \\\\ \\frac{1}{10} \\\\ \\frac{1}{20} \\\\ 0 \\end{pmatrix} - \\frac{3}{10} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} - \\frac{3}{10} \\\\ \\frac{1}{4} - \\frac{3}{10} \\\\ \\frac{1}{10} - \\frac{3}{10} \\\\ \\frac{1}{20} - \\frac{3}{10} \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{6-3}{10} \\\\ \\frac{5-6}{20} \\\\ \\frac{1-3}{10} \\\\ \\frac{1-6}{20} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{10} \\\\ -\\frac{1}{20} \\\\ -\\frac{2}{10} \\\\ -\\frac{5}{20} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{10} \\\\ -\\frac{1}{20} \\\\ -\\frac{1}{5} \\\\ -\\frac{1}{4} \\\\ 0 \\end{pmatrix}$$\n所以，要投影的向量是 $y = \\begin{pmatrix} \\frac{3}{10},  -\\frac{1}{20},  -\\frac{1}{5},  -\\frac{1}{4},  0 \\end{pmatrix}^{\\top}$。\n\n**步骤 3：计算投影 $x^{+} = \\Pi_{\\Delta}(y)$**\n\n投影 $x^{+} = \\Pi_{\\Delta}(y)$ 是以下凸优化问题的唯一解：\n$$ \\min_{z \\in \\mathbb{R}^{5}} \\frac{1}{2}\\|z - y\\|_{2}^{2} \\quad \\text{subject to} \\quad z \\ge 0, \\quad \\mathbf{1}^{\\top}z = 1 $$\n我们构建拉格朗日函数，其中拉格朗日乘子 $\\lambda \\in \\mathbb{R}^{5}$ 对应非负约束 ($z_i \\ge 0$)，乘子 $\\nu \\in \\mathbb{R}$ 对应等式约束 ($\\sum_{i=1}^{5} z_i = 1$)。\n$$ L(z, \\lambda, \\nu) = \\frac{1}{2}\\sum_{i=1}^{5}(z_i - y_i)^2 - \\sum_{i=1}^{5}\\lambda_i z_i + \\nu\\left(\\sum_{i=1}^{5}z_i - 1\\right) $$\n最优解 $x^{+}$ 的 Karush-Kuhn-Tucker (KKT) 条件是：\n1.  **平稳性 (Stationarity)**：$\\frac{\\partial L}{\\partial z_i} = (x^{+}_i - y_i) - \\lambda_i + \\nu = 0$，对于 $i=1, \\dots, 5$。\n2.  **原始可行性 (Primal Feasibility)**：$x^{+}_i \\ge 0$ 对所有 $i$ 成立，且 $\\sum_{i=1}^{5}x^{+}_i = 1$。\n3.  **对偶可行性 (Dual Feasibility)**：$\\lambda_i \\ge 0$ 对所有 $i$ 成立。\n4.  **互补松弛性 (Complementary Slackness)**：$\\lambda_i x^{+}_i = 0$ 对所有 $i$ 成立。\n\n从平稳性条件，我们有 $x^{+}_i = y_i - \\nu + \\lambda_i$。\n从互补松弛性，如果 $x^{+}_i > 0$，则 $\\lambda_i = 0$，这意味着 $x^{+}_i = y_i - \\nu$。为了使该值为正，我们必须有 $y_i > \\nu$。\n如果 $x^{+}_i = 0$，则 $\\lambda_i \\ge 0$。平稳性条件给出 $\\lambda_i = \\nu - y_i$，所以我们必须有 $\\nu - y_i \\ge 0$，即 $y_i \\le \\nu$。\n综合这些情况，每个分量的解由 $x^{+}_i = \\max(0, y_i - \\nu)$ 给出。令 $\\theta = \\nu$，解的形式为 $x^{+}_i = (y_i - \\theta)_+$，其中 $(v)_+ = \\max(0,v)$。\n\n阈值 $\\theta$ 由等式约束 $\\sum_{i=1}^{5} x^{+}_i = 1$ 确定：\n$$ \\sum_{i=1}^{5} (y_i - \\theta)_+ = 1 $$\n为了找到 $\\theta$，我们使用一个标准算法。首先，将 $y$ 的分量按降序排序：$y_{(1)} \\ge y_{(2)} \\ge y_{(3)} \\ge y_{(4)} \\ge y_{(5)}$。\n$y$ 的分量是：$\\{ \\frac{3}{10}, -\\frac{1}{20}, -\\frac{1}{5}, -\\frac{1}{4}, 0 \\}$。用小数形式表示：$\\{ 0.3, -0.05, -0.2, -0.25, 0 \\}$。\n排序后的分量是：\n$y_{(1)} = \\frac{3}{10}$ (来自 $y_1$)\n$y_{(2)} = 0$ (来自 $y_5$)\n$y_{(3)} = -\\frac{1}{20}$ (来自 $y_2$)\n$y_{(4)} = -\\frac{1}{5}$ (来自 $y_3$)\n$y_{(5)} = -\\frac{1}{4}$ (来自 $y_4$)\n\n我们寻找一个整数 $\\rho \\in \\{1, \\dots, 5\\}$ 和一个值 $\\theta$，使得 $\\sum_{i=1}^{\\rho} (y_{(i)} - \\theta) = 1$ 且 $y_{(\\rho)} > \\theta \\ge y_{(\\rho+1)}$（其中 $y_{(6)} = -\\infty$）。这意味着 $x^{+}$ 恰好有 $\\rho$ 个分量是正的。阈值则为 $\\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} y_{(i)} - 1\\right)$。\n\n我们来测试 $\\rho$ 的值：\n- 对于 $\\rho=1$：$\\theta = y_{(1)} - 1 = \\frac{3}{10} - 1 = -\\frac{7}{10}$。条件检查：$y_{(1)} > \\theta$ 成立，但 $y_{(2)} \\ge \\theta$ ($0 \\ge -0.7$) 也成立。条件 $y_{(\\rho)} > \\theta \\ge y_{(\\rho+1)}$ 不满足。\n- 对于 $\\rho=2$：$\\theta = \\frac{y_{(1)} + y_{(2)} - 1}{2} = \\frac{\\frac{3}{10} + 0 - 1}{2} = -\\frac{7}{20}$。条件检查：$y_{(2)} > \\theta$ 成立，但 $y_{(3)} \\ge \\theta$ ($-\\frac{1}{20} \\ge -\\frac{7}{20}$) 也成立。不满足。\n- 对于 $\\rho=3$：$\\theta = \\frac{y_{(1)} + y_{(2)} + y_{(3)} - 1}{3} = \\frac{\\frac{3}{10} + 0 - \\frac{1}{20} - 1}{3} = \\frac{\\frac{6}{20}-\\frac{1}{20}-\\frac{20}{20}}{3} = \\frac{-15/20}{3} = -\\frac{1}{4}$。条件检查：$y_{(3)} > \\theta$ ($-\\frac{1}{20} > -\\frac{1}{4}$) 成立，但 $y_{(4)} \\ge \\theta$ ($-\\frac{1}{5} \\ge -\\frac{1}{4}$) 也成立。不满足。\n- 对于 $\\rho=4$：$\\theta = \\frac{y_{(1)} + y_{(2)} + y_{(3)} + y_{(4)} - 1}{4} = \\frac{\\frac{3}{10} + 0 - \\frac{1}{20} - \\frac{1}{5} - 1}{4} = \\frac{\\frac{6}{20}-\\frac{1}{20}-\\frac{4}{20}-\\frac{20}{20}}{4} = \\frac{-19/20}{4} = -\\frac{19}{80}$。\n条件检查：$y_{(4)} > \\theta \\ge y_{(5)}$？\n$y_{(4)} = -\\frac{1}{5} = -\\frac{16}{80}$。$-\\frac{16}{80} > -\\frac{19}{80}$ 吗？是的。\n$y_{(5)} = -\\frac{1}{4} = -\\frac{20}{80}$。$-\\frac{19}{80} \\ge -\\frac{20}{80}$ 吗？是的。\n该条件成立。因此，有效的（非零的）分量的正确数量是 $\\rho=4$，阈值是 $\\theta = -\\frac{19}{80}$。\n\n有效集由对应于 $y_{(1)}, y_{(2)}, y_{(3)}, y_{(4)}$ 的分量组成，即 $\\{y_1, y_5, y_2, y_3\\}$。分量 $x^{+}_4$ 将为零。\n\n我们现在使用 $x^{+}_i = (y_i - \\theta)_+$ 和 $\\theta = -\\frac{19}{80}$ 来计算 $x^{+}$ 的分量：\n$x^{+}_1 = y_1 - \\theta = \\frac{3}{10} - (-\\frac{19}{80}) = \\frac{24}{80} + \\frac{19}{80} = \\frac{43}{80}$。\n$x^{+}_2 = y_2 - \\theta = -\\frac{1}{20} - (-\\frac{19}{80}) = -\\frac{4}{80} + \\frac{19}{80} = \\frac{15}{80} = \\frac{3}{16}$。\n$x^{+}_3 = y_3 - \\theta = -\\frac{1}{5} - (-\\frac{19}{80}) = -\\frac{16}{80} + \\frac{19}{80} = \\frac{3}{80}$。\n$x^{+}_4 = (y_4 - \\theta)_+ = (-\\frac{1}{4} - (-\\frac{19}{80}))_+ = (-\\frac{20}{80} + \\frac{19}{80})_+ = (-\\frac{1}{80})_+ = 0$。\n$x^{+}_5 = y_5 - \\theta = 0 - (-\\frac{19}{80}) = \\frac{19}{80}$。\n\n得到的向量是 $x^{+} = \\begin{pmatrix} \\frac{43}{80} \\\\ \\frac{3}{16} \\\\ \\frac{3}{80} \\\\ 0 \\\\ \\frac{19}{80} \\end{pmatrix}$。\n作为检验，我们确认这些分量是非负的且和为 1：\n$\\frac{43}{80} + \\frac{3}{16} + \\frac{3}{80} + 0 + \\frac{19}{80} = \\frac{43}{80} + \\frac{15}{80} + \\frac{3}{80} + \\frac{19}{80} = \\frac{43+15+3+19}{80} = \\frac{80}{80} = 1$。\n向量 $x^{+}$ 位于单纯形 $\\Delta$ 中。计算正确。最终答案应表示为单个行向量。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{43}{80}  \\frac{3}{16}  \\frac{3}{80}  0  \\frac{19}{80}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在理解了如何执行单步迭代之后，一个关键问题随之而来：我们距离解有多近？这个练习  引入了 Karush–Kuhn–Tucker (KKT) 残差，它是衡量像 LASSO 这样的非光滑问题次优性的一个实用指标。通过计算这个残差并应用一个理论误差界，你将学会如何估计到最优解集的距离，这是设计算法有效停止准则的关键概念。",
            "id": "3483136",
            "problem": "考虑被称为最小绝对收缩和选择算子 (LASSO) 的凸优化问题：最小化函数 $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{3 \\times 3}$，$b \\in \\mathbb{R}^{3}$，且 $\\lambda > 0$。最优性的 Karush–Kuhn–Tucker (KKT) 平稳性条件是 $0 \\in A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}$，其中 $\\partial \\|x\\|_{1}$ 是 $\\ell_{1}$ 范数的次微分。在迭代点 $x$ 处的 KKT 残差定义为原点到集合 $A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}$ 的欧几里得距离：\n$$\n\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big) \\;=\\; \\min_{s \\in \\partial \\|x\\|_{1}} \\big\\|A^{\\top}(A x - b) + \\lambda s\\big\\|_{2}.\n$$\n假设误差界条件成立：存在 $\\gamma > 0$ 使得对于所有 $x$，$\\operatorname{dist}(x, X^{\\star}) \\leq \\gamma \\,\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)$，其中 $X^{\\star}$ 是 $F$ 的最小化子集合。对于具体数据\n$$\nA = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 0.5, \\quad x = \\begin{pmatrix} 0 \\\\ 1 \\\\ -0.2 \\end{pmatrix},\n$$\n并假设误差界常数由 $\\gamma = \\frac{1}{\\lambda_{\\min}(A^{\\top}A)}$ 给出，其中 $\\lambda_{\\min}(A^{\\top}A)$ 表示 $A^{\\top}A$ 的最小特征值，计算 KKT 残差 $\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)$，然后使用误差界得到 $\\operatorname{dist}(x, X^{\\star})$ 的数值上界。将您的最终数值界四舍五入到四位有效数字。",
            "solution": "我们从具有非光滑项的凸优化的基本 KKT 平稳性条件开始。对于函数 $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，其次梯度集合为\n$$\n\\partial F(x) = A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1},\n$$\n其中对于每个坐标 $i$，$\\ell_{1}$ 范数的次梯度满足\n$$\n\\big(\\partial \\|x\\|_{1}\\big)_{i} = \n\\begin{cases}\n\\{\\operatorname{sign}(x_{i})\\},  \\text{if } x_{i} \\neq 0, \\\\\n\\left[-1,\\, 1\\right],  \\text{if } x_{i} = 0.\n\\end{cases}\n$$\nKKT 残差是 $0$ 到集合 $\\partial F(x)$ 的欧几里得距离：\n$$\n\\operatorname{dist}\\big(0,\\, \\partial F(x)\\big) = \\min_{s \\in \\partial \\|x\\|_{1}} \\left\\|A^{\\top}(A x - b) + \\lambda s\\right\\|_{2}.\n$$\n\n我们为给定数据计算 $q := A^{\\top}(A x - b)$。首先，\n$$\nA x = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\\\ -0.2 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 2 \\\\ -0.6 \\end{pmatrix},\n\\quad\nA x - b = \\begin{pmatrix} 0 \\\\ 2 \\\\ -0.6 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} -1 \\\\ 4 \\\\ -0.6 \\end{pmatrix}.\n$$\n由于 $A$ 是对角矩阵，$A^{\\top} = A$，因此\n$$\nq = A^{\\top}(A x - b) = A(A x - b)\n= \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}\n\\begin{pmatrix} -1 \\\\ 4 \\\\ -0.6 \\end{pmatrix}\n= \\begin{pmatrix} -2 \\\\ 8 \\\\ -1.8 \\end{pmatrix}.\n$$\n\n为了在 $s \\in \\partial \\|x\\|_{1}$ 上最小化 $\\left\\|q + \\lambda s\\right\\|_{2}$，我们根据次梯度的规则处理各个坐标。令 $q_{i}$ 表示 $q$ 的第 $i$ 个分量。\n\n- 对于 $i = 1$，$x_{1} = 0$，所以 $s_{1} \\in [-1,1]$。我们通过选择 $s_{1}$ 作为 $-\\frac{q_{1}}{\\lambda}$ 在 $[-1,1]$ 上的投影来最小化 $\\left|q_{1} + \\lambda s_{1}\\right|$。这里，$-\\frac{q_{1}}{\\lambda} = -\\frac{-2}{0.5} = 4$，所以投影是 $s_{1} = 1$。因此 $r_{1} := q_{1} + \\lambda s_{1} = -2 + 0.5 \\cdot 1 = -1.5$ 且 $|r_{1}| = 1.5$。等价地，当 $x_{1} = 0$ 时，可达到的最小幅值为 $\\max\\{|q_{1}| - \\lambda, 0\\} = \\max\\{2 - 0.5, 0\\} = 1.5$。\n\n- 对于 $i = 2$，$x_{2} = 1 \\neq 0$，所以 $s_{2} = \\operatorname{sign}(x_{2}) = 1$。那么 $r_{2} := q_{2} + \\lambda s_{2} = 8 + 0.5 \\cdot 1 = 8.5$ 且 $|r_{2}| = 8.5$。\n\n- 对于 $i = 3$，$x_{3} = -0.2 \\neq 0$，所以 $s_{3} = \\operatorname{sign}(x_{3}) = -1$。那么 $r_{3} := q_{3} + \\lambda s_{3} = -1.8 + 0.5 \\cdot (-1) = -2.3$ 且 $|r_{3}| = 2.3$。\n\n综合这些，最小化选择的 $s$ 得到残差向量 $r = q + \\lambda s = \\begin{pmatrix} -1.5 \\\\ 8.5 \\\\ -2.3 \\end{pmatrix}$，所以\n$$\n\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)\n= \\|r\\|_{2}\n= \\sqrt{(-1.5)^{2} + (8.5)^{2} + (-2.3)^{2}}\n= \\sqrt{2.25 + 72.25 + 5.29}\n= \\sqrt{79.79}.\n$$\n\n接下来，我们在源于强凸性的误差界条件下，将 KKT 残差与到最优解集的距离联系起来。函数 $g(x) := \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 的海森矩阵为 $A^{\\top}A$。如果 $A^{\\top}A$ 是正定的，其最小特征值为 $\\lambda_{\\min}(A^{\\top}A) = m > 0$，则 $g$ 是 $m$-强凸的，并且 $F(x) = g(x) + \\lambda \\|x\\|_{1}$ 也是 $m$-强凸的。对于一个 $m$-强凸函数 $F$，其次微分映射 $\\partial F$ 是 $m$-强单调的，这意味着误差界\n$$\n\\operatorname{dist}(x, X^{\\star}) \\leq \\frac{1}{m} \\,\\operatorname{dist}\\big(0,\\, \\partial F(x)\\big).\n$$\n在我们的例子中，$A^{\\top}A = \\operatorname{diag}(4,4,9)$，所以 $\\lambda_{\\min}(A^{\\top}A) = 4$，因此 $\\gamma = \\frac{1}{\\lambda_{\\min}(A^{\\top}A)} = \\frac{1}{4}$。\n\n因此，\n$$\n\\operatorname{dist}(x, X^{\\star}) \\leq \\gamma \\,\\operatorname{dist}\\big(0,\\, A^{\\top}(A x - b) + \\lambda \\partial \\|x\\|_{1}\\big)\n= \\frac{1}{4} \\sqrt{79.79}.\n$$\n数值上，\n$$\n\\sqrt{79.79} \\approx 8.93253,\n\\quad\n\\frac{1}{4}\\sqrt{79.79} \\approx 2.23313.\n$$\n四舍五入到四位有效数字，$\\operatorname{dist}(x, X^{\\star})$ 的数值上界是 $2.233$。",
            "answer": "$$\\boxed{2.233}$$"
        },
        {
            "introduction": "现在我们将这些概念应用于一个真实的信号处理任务。这个实践  探讨了用于识别时间序列数据中突变的融合 LASSO 模型。你不仅要为这个目标函数实现一个次梯度方法，还要设计一个步长启发式策略来加速这些变化点的发现，从而深入了解如何根据特定问题的结构来定制算法设计。",
            "id": "3483144",
            "problem": "考虑一维时间序列的融合最小绝对收缩和选择算子 (Fused LASSO) 正则化重构。给定一个数据向量 $y \\in \\mathbb{R}^n$，定义离散差分算子 $D \\in \\mathbb{R}^{(n-1)\\times n}$ 为 $(Dx)_i = x_{i+1} - x_i$，其中 $i \\in \\{0,\\dots,n-2\\}$。设目标函数为\n$$\nF(x) = \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda \\|x\\|_1 + \\gamma \\|D x\\|_1,\n$$\n该函数结合了二次数据保真项、元素级稀疏性以及全变分 (TV) 正则化。全变分 (TV) 定义为离散差分的 $\\ell_1$ 范数。\n\n从非光滑凸函数的次梯度核心定义以及差分算子的线性性质出发，构建一个次梯度法来最小化 $F(x)$（对于 $x \\in \\mathbb{R}^n$），并检验其次梯度轨迹（即迭代点序列 $\\{x^k\\}$）。设计一种步长启发式策略，该策略应明确优先考虑在重构时间序列中早期检测到变化点，且仅使用迭代过程中可获得的量。变化点的检测应基于一个阈值规则，该规则应用于当前迭代点 $x^k$ 的连续差分：\n$$\n\\mathcal{C}(x^k) = \\{ i \\in \\{0,\\dots,n-2\\} : |x^k_{i+1} - x^k_{i}| \\ge \\tau \\}.\n$$\n\n程序必须：\n- 根据第一性原理实现次梯度法，在每次迭代中计算 $F(x)$ 的一个有效次梯度，该次梯度遵循 $\\ell_1$ 范数次梯度的定义以及带有线性算子的次梯度链式法则。\n- 提出并实现一种优先考虑早期变化点检测的步长启发式策略。该启发式策略必须有原则性，可从迭代期间观察到的量计算得出，并且不得依赖任何关于未知最小化子的“神谕”信息。当 TV 贡献相对较强时，该启发式策略应提供较大的早期步长，以加速迭代点中显著离散差分的出现。\n- 对于每个测试用例，记录使 $\\mathcal{C}(x^k)$ 至少包含一个基准真相信号 $x^\\star$ 的真实变化点的最早迭代索引 $k \\in \\{1,\\dots,K\\}$。如果在迭代预算 $K$ 内未检测到任何真实变化点，则该测试用例输出 $-1$。\n\n您的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3]”）。测试用例的答案必须是整数或布尔值，与下面每个测试用例的描述完全一致。\n\n您必须为每个测试用例使用以下确定性数据生成方法。对于一个整数长度 $n$ 和一个由具有恒定值的段定义的 piecewise-constant ground truth（分段常数基准真相）$x^\\star$，设置\n$$\ny_i = x^\\star_i + 0.05 \\sin(0.3 i), \\quad i \\in \\{0,\\dots,n-1\\},\n$$\n除非另有规定。真实变化点是索引 $i$，其中 $x^\\star_{i+1} - x^\\star_i \\neq 0$。\n\n实现次梯度法时，需遵循以下约束和可测量量：\n- 初始化 $x^0 = 0$。\n- 固定的迭代预算 $K$。\n- 检测阈值 $\\tau > 0$。\n- 正则化参数 $\\lambda > 0$ 和 $\\gamma > 0$。\n- 步长上限 $\\alpha_{\\max} > 0$。\n- 优先增益 $\\rho \\ge 0$，用于调节步长启发式策略中对 TV 分量的强调程度。\n\n测试套件包含五个案例：\n\n- 测试案例 1（单个早期变化点，理想路径）：\n  - $n = 40$。\n  - $x^\\star$ 的分段：$(0,10,0.0)$ 然后 $(10,40,1.0)$。\n  - $\\lambda = 0.2$, $\\gamma = 1.0$, $\\tau = 0.3$, $K = 200$, $\\alpha_{\\max} = 2.0$, $\\rho = 1.0$。\n  - 输出：检测到任何真实变化点的最早迭代索引 $k$；如果在 $K$ 次迭代内没有检测到，则输出 $-1$。\n\n- 测试案例 2（无变化点，避免误报的边界情况）：\n  - $n = 40$。\n  - $x^\\star$ 的分段：$(0,40,0.0)$。\n  - $\\lambda = 0.2$, $\\gamma = 1.0$, $\\tau = 0.6$, $K = 200$, $\\alpha_{\\max} = 2.0$, $\\rho = 1.0$。\n  - 输出：如果在 $K$ 次迭代内没有检测到，则输出 $-1$；否则输出最早的迭代索引。\n\n- 测试案例 3（多个变化点，幅度不同）：\n  - $n = 60$。\n  - $x^\\star$ 的分段：$(0,15,0.0)$ 然后 $(15,30,2.0)$ 然后 $(30,60,-1.0)$。\n  - $\\lambda = 0.25$, $\\gamma = 1.5$, $\\tau = 0.5$, $K = 300$, $\\alpha_{\\max} = 2.0$, $\\rho = 1.2$。\n  - 输出：最早的迭代索引 $k$（如果没有则为 $-1$）。\n\n- 测试案例 4（短信号中的边界变化点）：\n  - $n = 5$。\n  - $x^\\star$ 的分段：$(0,1,1.0)$ 然后 $(1,5,0.0)$。\n  - 使用 $y = x^\\star$（本案例无正弦扰动）。\n  - $\\lambda = 0.2$, $\\gamma = 1.0$, $\\tau = 0.5$, $K = 200$, $\\alpha_{\\max} = 2.0$, $\\rho = 1.5$。\n  - 输出：最早的迭代索引 $k$（如果没有则为 $-1$）。\n\n- 测试案例 5（与基准步长规则的比较）：\n  - 使用与测试案例 3 相同的 $n$、分段和参数。\n  - 基准步长规则：$\\alpha_k = c / \\sqrt{k+1}$，其中 $c = 1.0$。\n  - 输出：一个布尔值，指示所提出的启发式策略是否严格早于基准策略检测到真实变化点（即，如果 $k_{\\text{heuristic}}  k_{\\text{baseline}}$，则输出 true，否则输出 false；如果其中一种方法在 $K$ 次迭代内未能检测到，则将其失败视为迭代索引为 $+\\infty$）。\n\n您的程序应生成单行输出，其中包含按上述顺序排列的五个测试案例的结果，格式为逗号分隔的列表并用方括号括起来。此问题不涉及任何物理单位或角度；结果为纯整数或最后一个案例的布尔值。",
            "solution": "该问题要求最小化一维时间序列 $x \\in \\mathbb{R}^n$ 的融合LASSO目标函数，给定一个数据向量 $y \\in \\mathbb{R}^n$。目标函数为：\n$$\nF(x) = \\frac{1}{2}\\|x - y\\|_2^2 + \\lambda \\|x\\|_1 + \\gamma \\|D x\\|_1\n$$\n此处，$\\lambda > 0$ 和 $\\gamma > 0$ 是正则化参数。算子 $D \\in \\mathbb{R}^{(n-1)\\times n}$ 是离散差分算子，定义为 $(Dx)_i = x_{i+1} - x_i$，其中 $i \\in \\{0, \\dots, n-2\\}$。$\\|x\\|_1$ 项促进解的元素级稀疏性，而 $\\|Dx\\|_1$ 项，即全变分 (TV) 半范数，则通过惩罚相邻元素之间的差异来促进分段常数解。\n\n函数 $F(x)$ 是三个凸函数的和。第一项 $f_1(x) = \\frac{1}{2}\\|x - y\\|_2^2$ 是严格凸且光滑（可微）的。另外两项 $f_2(x) = \\lambda \\|x\\|_1$ 和 $f_3(x) = \\gamma \\|Dx\\|_1$ 是凸但非光滑的。总体函数 $F(x)$ 是严格凸的，这保证了其存在唯一的最小化子。为了找到这个最小化子，我们可以采用次梯度法，这是一种用于非光滑凸优化的迭代算法。\n\n一个凸函数 $f$ 在点 $x$ 的次梯度是一个向量 $g$，使得对于定义域中的所有 $z$，都有 $f(z) \\ge f(x) + g^T(z-x)$。在点 $x$ 的所有次梯度的集合称为次微分，记为 $\\partial f(x)$。对于像 $F(x)$ 这样的凸函数之和，其次微分是其各组分次微分的和（根据次微分的和法则）：\n$$\n\\partial F(x) = \\partial f_1(x) + \\partial f_2(x) + \\partial f_3(x)\n$$\n要实现次梯度法，我们需要在每次迭代中找到一个有效的次梯度 $g \\in \\partial F(x)$。\n\n1.  **数据保真项的次梯度**：由于 $f_1(x) = \\frac{1}{2}\\|x - y\\|_2^2$ 是可微的，其​​次微分只包含其梯度：\n    $$\n    \\partial f_1(x) = \\{ \\nabla f_1(x) \\} = \\{ x - y \\}\n    $$\n\n2.  **$\\ell_1$ 范数项的次梯度**：对于 $f_2(x) = \\lambda \\|x\\|_1$，我们使用 $\\ell_1$ 范数的次微分。一个次梯度 $g_2 \\in \\partial f_2(x)$ 的分量 $(g_2)_i$ 由下式给出：\n    $$\n    (g_2)_i \\in \\lambda \\cdot \\begin{cases} \\{ \\text{sign}(x_i) \\}  \\text{if } x_i \\neq 0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n    $$\n    对于具体的实现，我们可以选择当 $x_i=0$ 时分量为 $0$ 的次梯度。这给出了具体的次梯度向量 $g_2(x) = \\lambda \\cdot \\text{sign}(x)$，其中 $\\text{sign}(0)=0$。\n\n3.  **全变分项的次梯度**：对于 $f_3(x) = \\gamma \\|Dx\\|_1$，我们应用次微分的链式法则。令 $h(z) = \\gamma \\|z\\|_1$。则 $f_3(x) = h(Dx)$，其​​次微分为 $\\partial f_3(x) = D^T \\partial h(Dx)$。一个次梯度 $g_z \\in \\partial h(z)$ 的分量为 $(g_z)_i = \\gamma \\cdot \\text{sign}(z_i)$（同样，取 $\\text{sign}(0)=0$）。令 $z=Dx$。因此，$f_3(x)$ 的一个具体次梯度是：\n    $$\n    g_3(x) = \\gamma D^T \\text{sign}(Dx)\n    $$\n    算子 $D^T$ 是差分算子的转置。对于一个向量 $u \\in \\mathbb{R}^{n-1}$，向量 $v = D^T u \\in \\mathbb{R}^n$ 由 $v_0 = -u_0$，$v_{n-1}=u_{n-2}$，以及 $v_j = u_{j-1} - u_j$（对于 $j \\in \\{1,\\dots,n-2\\}$）给出。\n\n综合这些，在迭代点 $x^k$ 处的一个有效次梯度 $g^k \\in \\partial F(x^k)$ 是：\n$$\ng^k = (x^k - y) + \\lambda \\cdot \\text{sign}(x^k) + \\gamma D^T \\text{sign}(Dx^k)\n$$\n\n次梯度法的更新规则是 $x^{k+1} = x^k - \\alpha_k g^k$，其中 $\\alpha_k > 0$ 是第 $k$ 次迭代的步长。问题要求设计一种优先考虑早期变化点检测的步长启发式策略。其直觉是，TV项 $\\gamma \\|Dx\\|_1$ 负责产生这些变化点（跳跃）。因此，当下降方向受TV次梯度分量强烈影响时，步长规则应更加激进。\n\n令 $g_{TV}^k = \\gamma D^T \\text{sign}(Dx^k)$ 为次梯度的TV部分。我们可以通过其范数与总次梯度范数的比值来量化其相对强度：$\\beta_k = \\|g_{TV}^k\\|_2 / \\|g^k\\|_2$。一个较大的 $\\beta_k$ 表明更新将主要作用于减少全变分，可能形成或锐化变化点。\n\n基于此原则，我们提出以下启发式步长规则：\n$$\n\\alpha_k = \\min\\left(\\alpha_{\\max}, \\frac{1}{\\sqrt{k+1}} (1 + \\rho \\cdot \\beta_k)\\right)\n$$\n该规则有几个关键特性：\n-   它包含一个递减因子 $1/\\sqrt{k+1}$，以确保步长最终变小，这是次梯度法收敛的常见条件。\n-   项 $(1 + \\rho \\cdot \\beta_k)$ 充当自适应增益。当TV分量强时（$\\beta_k$ 较大），步长会增加，其增幅由增益参数 $\\rho \\ge 0$ 调节。\n-   步长上限为 $\\alpha_{\\max}$，以防止可能导致不稳定的过大步长。\n-   它完全可以从第 $k$ 次迭代中可用的量计算得出。\n\n作为比较，一个基准步长规则被指定为 $\\alpha_k^{\\text{base}} = c / \\sqrt{k+1}$，其中 $c=1.0$。\n\n总体算法流程如下：\n1.  初始化 $x^0 = \\mathbf{0}$。\n2.  对于 $k = 0, 1, \\dots, K-1$：\n    a. 计算次梯度 $g^k = (x^k - y) + \\lambda \\cdot \\text{sign}(x^k) + \\gamma D^T \\text{sign}(Dx^k)$。\n    b. 使用所提出的启发式策略或基准规则计算步长 $\\alpha_k$。\n    c. 更新解：$x^{k+1} = x^k - \\alpha_k g^k$。\n    d. 检查检测情况：找到检测到的变化点集合 $\\mathcal{C}(x^{k+1}) = \\{ i : |x^{k+1}_{i+1} - x^{k+1}_i| \\ge \\tau \\}$。\n    e. 如果 $\\mathcal{C}(x^{k+1})$ 包含基准真相信号 $x^\\star$ 的真实变化点集合中的任何索引，算法终止并返回当前迭代次数 $k+1$。\n3.  如果循环完成而未检测到，则返回 $-1$。\n\n这种有原则的构建方法为解决该问题和评估所提出的步长启发式策略的性能提供了一个完整的方法。",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(n, segments, sinusoidal_noise=True):\n    \"\"\"\n    Generates the ground-truth signal x_star, the measurement vector y,\n    and the set of true change point indices.\n    Segments are defined as (start_inclusive, end_exclusive, value).\n    \"\"\"\n    x_star = np.zeros(n)\n    true_changepoints = set()\n    \n    last_end = 0\n    last_val = 0.0\n    \n    # Sort segments by start index to handle them in order\n    segments.sort(key=lambda s: s[0])\n\n    for i in range(len(segments)):\n        start, end, value = segments[i]\n        \n        # Check for change point at the beginning of the segment\n        if start > 0 and x_star[start - 1] != value:\n            true_changepoints.add(start - 1)\n        \n        x_star[start:end] = value\n        last_end = end\n        last_val = value\n\n    y = np.copy(x_star)\n    if sinusoidal_noise:\n        noise = 0.05 * np.sin(0.3 * np.arange(n))\n        y += noise\n        \n    # Re-calculate true change points from final signal for robustness\n    true_changepoints = set(np.where(np.abs(x_star[1:] - x_star[:-1]) > 1e-9)[0])\n\n    return x_star, y, true_changepoints\n\ndef d_transpose_times_vec(u, n):\n    \"\"\"\n    Computes D^T @ u where D is the discrete difference operator and u is in R^(n-1).\n    \"\"\"\n    if n = 1:\n        return np.zeros(n)\n    v = np.zeros(n)\n    v[0] = -u[0]\n    v[-1] = u[-1]\n    if n > 2:\n        v[1:-1] = u[:-1] - u[1:]\n    return v\n\ndef run_subgradient_descent(y, lam, gamma, tau, K, alpha_max, rho, true_changepoints, step_size_rule='heuristic'):\n    \"\"\"\n    Runs the subgradient method to minimize the fused LASSO objective.\n    Returns the first iteration index k where a true change point is detected, or -1.\n    \"\"\"\n    n = len(y)\n    x = np.zeros(n)\n\n    for k in range(K):\n        # Step 1: Compute a subgradient g of F(x) at x\n        # g1 for 0.5 * ||x - y||_2^2\n        g1 = x - y\n        \n        # g2 for lam * ||x||_1\n        g2 = lam * np.sign(x)\n        \n        # g3 for gamma * ||Dx||_1\n        if n > 1:\n            Dx = x[1:] - x[:-1]\n            sz = np.sign(Dx)\n            g3 = gamma * d_transpose_times_vec(sz, n)\n        else:\n            g3 = np.zeros(n)\n        \n        g = g1 + g2 + g3\n        \n        # Step 2: Compute step size alpha_k\n        if step_size_rule == 'heuristic':\n            g_tv_norm = np.linalg.norm(g3)\n            g_total_norm = np.linalg.norm(g)\n            \n            if g_total_norm  1e-9: # Converged\n                break\n                \n            beta = g_tv_norm / g_total_norm\n            alpha = min(alpha_max, (1.0 / np.sqrt(k + 1)) * (1.0 + rho * beta))\n        \n        elif step_size_rule == 'baseline':\n            c = 1.0\n            alpha = c / np.sqrt(k + 1)\n        else:\n            raise ValueError(f\"Unknown step_size_rule: {step_size_rule}\")\n\n        # Step 3: Update x\n        x_next = x - alpha * g\n        x = x_next\n        \n        # Step 4: Check for change point detection\n        if n > 1 and len(true_changepoints) > 0:\n            diffs = np.abs(x[1:] - x[:-1])\n            detected_indices = np.where(diffs >= tau)[0]\n            \n            if set(detected_indices).intersection(true_changepoints):\n                return k + 1  # Iteration count is k+1\n                \n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases_spec = [\n        # Case 1\n        {'n': 40, 'segments': [(0, 10, 0.0), (10, 40, 1.0)], 'sinusoidal_noise': True,\n         'params': {'lam': 0.2, 'gamma': 1.0, 'tau': 0.3, 'K': 200, 'alpha_max': 2.0, 'rho': 1.0}},\n        # Case 2\n        {'n': 40, 'segments': [(0, 40, 0.0)], 'sinusoidal_noise': True,\n         'params': {'lam': 0.2, 'gamma': 1.0, 'tau': 0.6, 'K': 200, 'alpha_max': 2.0, 'rho': 1.0}},\n        # Case 3\n        {'n': 60, 'segments': [(0, 15, 0.0), (15, 30, 2.0), (30, 60, -1.0)], 'sinusoidal_noise': True,\n         'params': {'lam': 0.25, 'gamma': 1.5, 'tau': 0.5, 'K': 300, 'alpha_max': 2.0, 'rho': 1.2}},\n        # Case 4\n        {'n': 5, 'segments': [(0, 1, 1.0), (1, 5, 0.0)], 'sinusoidal_noise': False,\n         'params': {'lam': 0.2, 'gamma': 1.0, 'tau': 0.5, 'K': 200, 'alpha_max': 2.0, 'rho': 1.5}},\n    ]\n    \n    results = []\n\n    for case_spec in test_cases_spec:\n        x_star, y, tcp = generate_data(case_spec['n'], case_spec['segments'], case_spec['sinusoidal_noise'])\n        result = run_subgradient_descent(y, **case_spec['params'], true_changepoints=tcp)\n        results.append(result)\n\n    # Test Case 5\n    case3_spec = test_cases_spec[2]\n    x_star3, y3, tcp3 = generate_data(case3_spec['n'], case3_spec['segments'], case3_spec['sinusoidal_noise'])\n    params3 = case3_spec['params']\n\n    # Run with heuristic\n    k_h = run_subgradient_descent(y3, **params3, true_changepoints=tcp3, step_size_rule='heuristic')\n    # Run with baseline\n    k_b = run_subgradient_descent(y3, **params3, true_changepoints=tcp3, step_size_rule='baseline')\n\n    k_h_inf = float('inf') if k_h == -1 else k_h\n    k_b_inf = float('inf') if k_b == -1 else k_b\n    \n    res5 = k_h_inf  k_b_inf\n\n    results.append(str(res5).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}