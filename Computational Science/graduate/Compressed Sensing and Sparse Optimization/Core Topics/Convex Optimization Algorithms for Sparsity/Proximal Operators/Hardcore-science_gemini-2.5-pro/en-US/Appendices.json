{
    "hands_on_practices": [
        {
            "introduction": "This exercise provides a foundational experience in deriving a proximal operator directly from its definition. We will tackle the negative entropy regularizer, a function crucial in statistical modeling and machine learning for enforcing positivity and modeling distributions. By applying first-order optimality conditions, you will derive a closed-form solution involving the Lambert $W$ function, a special function that frequently appears in such problems .",
            "id": "3470855",
            "problem": "Let $\\lambda  0$ and define the separable convex function $f : \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ by\n$$\nf(x) \\triangleq \\lambda \\sum_{i=1}^{n} \\big(x_{i} \\ln x_{i} - x_{i}\\big) + \\iota_{\\{x \\in \\mathbb{R}^{n} : x \\ge 0\\}}(x),\n$$\nwhere $\\iota_{\\{x \\ge 0\\}}$ is the indicator of the nonnegative orthant and $x \\ln x$ is extended by continuity at $x=0$ so that $x \\ln x \\to 0$ as $x \\downarrow 0$. Consider the proximal operator of $f$ at a given $y \\in \\mathbb{R}^{n}$,\n$$\n\\operatorname{prox}_{f}(y) \\triangleq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - y\\|_{2}^{2} + f(x) \\right\\}.\n$$\nStarting only from the definition of the proximal operator, elementary properties of convex functions, and standard first-order optimality conditions, do the following:\n\n- Prove that the minimization is separable across coordinates and that the unique minimizer satisfies $x_{i}^{\\star}  0$ for all $i \\in \\{1,\\dots,n\\}$.\n\n- Derive the optimality condition for each coordinate and solve it in closed form using a standard special function as needed, yielding an explicit analytic formula for $\\operatorname{prox}_{f}(y)$ as a function of $y$ and $\\lambda$.\n\n- Show how the scalar optimality condition can be rearranged into a fixed-point form of the type $x_{i} = c_{i} \\exp(-x_{i}/\\lambda)$ with $c_{i}  0$, and explain how this gives a multiplicative update iteration $x_{i}^{(t+1)} = c_{i} \\exp\\!\\big(-x_{i}^{(t)}/\\lambda\\big)$ that converges to the proximal point. Briefly discuss the relation of this multiplicative structure to algorithms based on negative-entropy regularization and Kullback–Leibler divergence (KLD).\n\n- Analyze numerical stability of evaluating your closed-form expression when the resulting entries are very small (i.e., when $x_{i}^{\\star} \\ll 1$). Provide the leading-order asymptotic expansion of $x_{i}^{\\star}$ in terms of $y_{i}$ and $\\lambda$ in the regime producing $x_{i}^{\\star} \\to 0^{+}$, and discuss a numerically stable evaluation strategy in that regime.\n\nYour final answer must be a single closed-form analytic expression for $\\operatorname{prox}_{f}(y)$ in terms of $y$ and $\\lambda$ (you may use standard special functions). Do not include any units. Do not provide an inequality or an equation as the final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in convex optimization concerning the proximal operator of the negative entropy regularizer. All definitions and conditions are provided, and there are no contradictions or ambiguities. The problem is solvable using established mathematical principles.\n\nThe objective is to compute the proximal operator of the function $f : \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{+\\infty\\}$ defined by\n$$\nf(x) \\triangleq \\lambda \\sum_{i=1}^{n} \\big(x_{i} \\ln x_{i} - x_{i}\\big) + \\iota_{\\{x \\in \\mathbb{R}^{n} : x \\ge 0\\}}(x)\n$$\nfor a parameter $\\lambda  0$. The proximal operator at a point $y \\in \\mathbb{R}^{n}$ is defined as\n$$\n\\operatorname{prox}_{f}(y) \\triangleq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|x - y\\|_{2}^{2} + f(x) \\right\\}.\n$$\nLet the objective function of this minimization problem be denoted by $G(x)$. Substituting the definition of $f(x)$, we have\n$$\nG(x) = \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\big(x_{i} \\ln x_{i} - x_{i}\\big) + \\iota_{\\{x \\ge 0\\}}(x).\n$$\nThe indicator function $\\iota_{\\{x \\ge 0\\}}(x)$ restricts the domain of the minimization to the non-negative orthant, i.e., $x_i \\ge 0$ for all $i=1, \\dots, n$.\n\n**Separability and Positivity of the Solution**\n\nThe objective function $G(x)$ can be written in a separable form by expanding the squared Euclidean norm:\n$$\nG(x) = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - y_i)^2 + \\lambda \\sum_{i=1}^{n} (x_i \\ln x_i - x_i)\n$$\nsubject to the constraint $x_i \\ge 0$ for all $i$. The sums can be combined:\n$$\nG(x) = \\sum_{i=1}^{n} \\left( \\frac{1}{2} (x_i - y_i)^2 + \\lambda (x_i \\ln x_i - x_i) \\right).\n$$\nLet $g_i(x_i) = \\frac{1}{2} (x_i - y_i)^2 + \\lambda (x_i \\ln x_i - x_i)$. The total minimization problem $\\min_{x \\ge 0} G(x)$ decouples into $n$ independent scalar minimization problems:\n$$\n\\min_{x_i \\ge 0} g_i(x_i) \\quad \\text{for each } i \\in \\{1, \\dots, n\\}.\n$$\nThis proves that the minimization is separable across coordinates.\n\nTo prove that the unique minimizer $x_i^\\star$ is strictly positive, we analyze the scalar function $g_i(x_i)$ for $x_i \\ge 0$. The function $\\phi(t) = t \\ln t - t$ is strictly convex for $t0$. Its second derivative is $\\phi''(t) = 1/t  0$. The function $\\frac{1}{2}(x_i-y_i)^2$ is also strictly convex. Since $\\lambda  0$, $g_i(x_i)$ is a sum of a strictly convex function and a convex function, hence it is strictly convex on its domain $\\{x_i : x_i \\ge 0\\}$. A strictly convex function on a closed convex set has a unique minimum.\n\nLet's examine the derivative of $g_i(x_i)$ for $x_i  0$:\n$$\ng_i'(x_i) = \\frac{d}{dx_i} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda(x_i \\ln x_i - x_i) \\right) = (x_i - y_i) + \\lambda(\\ln x_i + 1 - 1) = x_i - y_i + \\lambda \\ln x_i.\n$$\nWe consider the limit of the derivative as $x_i$ approaches $0$ from the right:\n$$\n\\lim_{x_i \\to 0^+} g_i'(x_i) = \\lim_{x_i \\to 0^+} (x_i - y_i + \\lambda \\ln x_i) = -\\infty.\n$$\nSince the derivative is negative for values of $x_i$ close to $0$, the function $g_i(x_i)$ is decreasing as $x_i$ approaches $0$. Therefore, the minimum cannot occur at the boundary point $x_i = 0$. The unique minimizer $x_i^\\star$ must lie in the interior of the domain, i.e., $x_i^\\star  0$.\n\n**Derivation of the Closed-Form Solution**\n\nSince the minimizer $x_i^\\star$ is positive, it must satisfy the first-order optimality condition $g_i'(x_i^\\star) = 0$:\n$$\nx_i^\\star - y_i + \\lambda \\ln x_i^\\star = 0.\n$$\nThis transcendental equation can be solved for $x_i^\\star$. Rearranging the terms, we get:\n$$\n\\lambda \\ln x_i^\\star + x_i^\\star = y_i.\n$$\nTo solve this, we will put it into a form compatible with the Lambert W function, which is defined by the relation $W(z) \\exp(W(z)) = z$.\nDivide the equation by $\\lambda$:\n$$\n\\ln x_i^\\star + \\frac{x_i^\\star}{\\lambda} = \\frac{y_i}{\\lambda}.\n$$\nExponentiating both sides gives:\n$$\n\\exp\\left(\\ln x_i^\\star + \\frac{x_i^\\star}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right)\n$$\n$$\n\\exp(\\ln x_i^\\star) \\exp\\left(\\frac{x_i^\\star}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right)\n$$\n$$\nx_i^\\star \\exp\\left(\\frac{x_i^\\star}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right).\n$$\nTo match the form $Z \\exp(Z) = C$, let's multiply by $1/\\lambda$:\n$$\n\\frac{x_i^\\star}{\\lambda} \\exp\\left(\\frac{x_i^\\star}{\\lambda}\\right) = \\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right).\n$$\nLet $Z_i = \\frac{x_i^\\star}{\\lambda}$. The equation becomes $Z_i \\exp(Z_i) = \\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)$. The solution for $Z_i$ is given by the Lambert W function:\n$$\nZ_i = W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right).\n$$\nSubstituting back $Z_i = \\frac{x_i^\\star}{\\lambda}$, we find the solution for $x_i^\\star$:\n$$\nx_i^\\star = \\lambda W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right).\n$$\nThe argument of the Lambert W function is always positive, so we use the principal branch, denoted by $W$. This formula gives the unique positive minimizer for each component. The full proximal operator result is a vector where each component is given by this expression.\n\n**Fixed-Point Form and Multiplicative Update**\n\nThe optimality condition $x_{i} - y_{i} + \\lambda \\ln x_{i} = 0$ can be rearranged into a fixed-point equation.\n$$\n\\lambda \\ln x_i = y_i - x_i\n$$\n$$\n\\ln x_i = \\frac{y_i - x_i}{\\lambda}\n$$\nExponentiating both sides yields:\n$$\nx_i = \\exp\\left(\\frac{y_i - x_i}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right) \\exp\\left(-\\frac{x_i}{\\lambda}\\right).\n$$\nThis equation is of the form $x_i = c_i \\exp(-x_i/\\lambda)$, where $c_i = \\exp(y_i/\\lambda)$. Since $y_i \\in \\mathbb{R}$, the constant $c_i$ is always positive. This fixed-point form naturally suggests an iterative update rule:\n$$\nx_{i}^{(t+1)} = c_{i} \\exp\\left(-\\frac{x_{i}^{(t)}}{\\lambda}\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right) \\exp\\left(-\\frac{x_{i}^{(t)}}{\\lambda}\\right).\n$$\nThis is a multiplicative update, as the new value is obtained by scaling a factor dependent on the old value. Such updates intrinsically preserve the non-negativity of $x_i$, provided the initial value $x_i^{(0)}$ is positive.\nThis structure is common in algorithms for optimization problems regularized by negative entropy. The function $f(x)$ is, up to scaling and a linear term, the negative entropy. The Bregman divergence associated with the convex generator $\\sum_i (x_i \\ln x_i - x_i)$ is the unnormalized Kullback-Leibler divergence (KLD). Algorithms such as mirror descent or exponentiated gradient for KLD geometry lead to multiplicative updates. The derived iteration can be seen as a special case of such methods for solving this specific proximal problem.\n\n**Numerical Stability Analysis**\n\nThe closed-form solution is $x_{i}^{\\star} = \\lambda W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right)$. We analyze its numerical behavior when $x_i^\\star$ is very small, i.e., $x_i^\\star \\to 0^+$. This corresponds to the argument of the Lambert W function becoming very small. Let $z_i = \\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)$. For $z_i \\to 0$, we must have $y_i \\to -\\infty$.\nThe Taylor series expansion of the Lambert W function around $z=0$ is $W(z) = z - z^2 + O(z^3)$. Using this, we can find the leading-order asymptotic expansion for $x_i^\\star$:\n$$\nx_i^\\star = \\lambda W(z_i) \\approx \\lambda z_i = \\lambda \\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y_i}{\\lambda}\\right)\\right) = \\exp\\left(\\frac{y_i}{\\lambda}\\right).\n$$\nThis approximation is valid for $y_i \\ll -\\lambda$.\nNumerically, if $y_i/\\lambda$ is a large negative number (e.g., $y_i/\\lambda  -100$), the term $\\exp(y_i/\\lambda)$ can underflow to zero in standard floating-point arithmetic. A direct evaluation of the Lambert W function might then return $W(0)=0$, leading to an incorrect result $x_i^\\star=0$.\nA numerically stable evaluation strategy should switch to the asymptotic approximation when $z_i$ is small. Thus, for sufficiently negative $y_i/\\lambda$, one should compute $x_i^\\star$ as $\\exp(y_i/\\lambda)$. This expression is numerically stable; if it underflows to zero, it means the true value is smaller than the machine epsilon, and zero is the best floating-point representation.\nFor example, a practical a strategy would be:\nIf $y_i/\\lambda  T$ for some threshold $T$ (e.g., $T=-30$):\n  $x_i^\\star = \\exp(y_i/\\lambda)$\nElse:\n  $x_i^\\star = \\lambda W(\\frac{1}{\\lambda} \\exp(y_i/\\lambda))$ (using a library implementation of $W$).\nThe fixed-point iteration $x_{i}^{(t+1)} = \\exp(y_i/\\lambda) \\exp(-x_{i}^{(t)}/\\lambda)$ is also a stable way to compute small values of $x_i^\\star$. Starting with $x_i^{(0)}=0$, the first step gives $x_i^{(1)} = \\exp(y_i/\\lambda)$, which is already the leading-order approximation.\n\nThe final answer requested is the single closed-form expression for the proximal operator. Interpreting this as the element-wise operation on the input vector $y$, the solution is given below.",
            "answer": "$$\n\\boxed{\\lambda W\\left(\\frac{1}{\\lambda} \\exp\\left(\\frac{y}{\\lambda}\\right)\\right)}\n$$"
        },
        {
            "introduction": "Proximal operators obey several powerful identities, with Moreau’s decomposition being one of the most fundamental. This identity beautifully connects a function's proximal operator with that of its convex conjugate, providing a deep link to duality. In this practice, you will analytically and numerically verify Moreau’s identity for the $\\ell_{\\infty}$ norm, which requires you to compute its conjugate and implement algorithms for both projection onto an $\\ell_1$ ball and the proximal operator of the $\\ell_{\\infty}$ norm .",
            "id": "3470868",
            "problem": "Consider the real finite-dimensional vector space with the standard Euclidean inner product. Let $f : \\mathbb{R}^n \\to \\mathbb{R}$ be a proper, lower semicontinuous convex function. The proximal operator of $f$ with parameter $\\lambda \\ge 0$ is defined by\n$$\n\\operatorname{prox}_{\\lambda f}(x) := \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2} \\|u - x\\|_2^2 + \\lambda f(u) \\right\\},\n$$\nand the convex conjugate (Fenchel conjugate) of $f$ is defined by\n$$\nf^*(y) := \\sup_{u \\in \\mathbb{R}^n} \\left\\{ \\langle y, u \\rangle - f(u) \\right\\}.\n$$\nDenote by $\\|\\cdot\\|_p$ the usual $\\ell_p$ norm on $\\mathbb{R}^n$, and let $\\mathbb{B}_1 := \\{ y \\in \\mathbb{R}^n : \\|y\\|_1 \\le 1 \\}$ be the unit $\\ell_1$ ball. The indicator function of a set $C \\subseteq \\mathbb{R}^n$ is defined by $I_C(x) = 0$ if $x \\in C$ and $I_C(x) = +\\infty$ otherwise. The exact projection of a vector $v \\in \\mathbb{R}^n$ onto $\\mathbb{B}_1$ is the unique minimizer of $\\tfrac{1}{2} \\|z - v\\|_2^2$ subject to $\\|z\\|_1 \\le 1$.\n\nThe task is to verify Moreau’s identity for a nontrivial function. Take $f(x) = \\|x\\|_\\infty$, the $\\ell_\\infty$ norm, and prove the identity\n$$\nx = \\operatorname{prox}_{\\lambda \\| \\cdot \\|_\\infty}(x) + \\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right),\n$$\nfor all $x \\in \\mathbb{R}^n$ and $\\lambda \\ge 0$, where $f^*$ is the convex conjugate of $f$. Then, implement both sides numerically in a single program using two independent algorithms: one to compute $\\operatorname{prox}_{\\lambda \\| \\cdot \\|_\\infty}(x)$ that does not invoke Moreau’s identity, and one to compute $\\operatorname{prox}_{f^*/\\lambda}(x/\\lambda)$ via exact projection onto the unit $\\ell_1$ ball. Your implementation must be robust for arbitrary finite vectors.\n\nYour program must use the following test suite of $(x,\\lambda)$ pairs:\n- Test case $1$: $x = [3.2, -1.1, 0.0, 5.5, -2.2]$, $\\lambda = 1.7$.\n- Test case $2$: $x = [0.0, 0.0, 0.0, 0.0]$, $\\lambda = 2.5$.\n- Test case $3$: $x = [10.0, -10.0]$, $\\lambda = 0.0$.\n- Test case $4$: $x = [-0.3, 0.3, -0.3, 0.3, -0.3, 0.3]$, $\\lambda = 0.5$.\n- Test case $5$: For $n = 20$, define $x \\in \\mathbb{R}^{20}$ componentwise by $x_i = \\sin(i) + \\tfrac{1}{2} \\cos(2 i)$ for $i = 1,2,\\dots,20$, with the arguments of the sine and cosine functions in radians, and set $\\lambda = 3.0$.\n\nFor each test case, check numerically whether\n$$\nx \\stackrel{?}{=} \\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x) + \\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right),\n$$\nwithin a numerical tolerance of absolute error $10^{-9}$ and relative error $10^{-10}$ in each coordinate (use the same tolerance across all tests). The output for each test case must be a boolean indicating whether the identity holds to the specified tolerance.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1, result_2, result_3, result_4, result_5]$). No external input is permitted. All angles must be in radians. No physical units are involved. The program must be fully self-contained and executable in a modern language, without requiring any external files or network access. The use of only standard numerical linear algebra operations is permitted.",
            "solution": "The problem requires the verification of Moreau’s identity for the function $f(x) = \\|x\\|_\\infty$, the $\\ell_\\infty$ norm on $\\mathbb{R}^n$. The identity states that for any $x \\in \\mathbb{R}^n$ and $\\lambda \\ge 0$,\n$$\nx = \\operatorname{prox}_{\\lambda f}(x) + \\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right)\n$$\nwhere $f^*$ is the convex conjugate of $f$. We will develop independent algorithms for the two proximal terms on the right-hand side and numerically verify the identity for a set of test cases.\n\nThe case $\\lambda=0$ is trivial. The proximal operator $\\operatorname{prox}_{0 \\cdot f}(x)$ reduces to $\\arg\\min_u \\frac{1}{2}\\|u-x\\|_2^2$, whose unique solution is $u=x$. Moreau's identity becomes $x = x + 0 \\cdot \\operatorname{prox}_{f^*/0}(x/0) = x$, which is always true. We will handle this case separately. For the remainder of the derivation, we assume $\\lambda  0$.\n\n### Term 1: $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x)$\n\nBy definition, the first term is the solution to the optimization problem:\n$$\nu = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x) := \\arg\\min_{v \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2} \\|v - x\\|_2^2 + \\lambda \\|v\\|_\\infty \\right\\}\n$$\nLet $t = \\|v\\|_\\infty$. The problem is equivalent to finding the solution to\n$$\n\\min_{t \\ge 0} \\left( \\min_{\\|v\\|_\\infty \\le t} \\tfrac{1}{2} \\|v - x\\|_2^2 + \\lambda t \\right)\n$$\nThe inner minimization is a projection of $x$ onto the $\\ell_\\infty$-ball of radius $t$ (a hypercube), which can be solved component-wise: $v_i = \\operatorname{sign}(x_i) \\min(|x_i|, t)$. Substituting this back, we obtain a problem in the single variable $t$:\n$$\n\\min_{t \\ge 0} g(t) \\quad \\text{where} \\quad g(t) = \\tfrac{1}{2} \\sum_{i=1}^n (\\operatorname{sign}(x_i) \\min(|x_i|, t) - x_i)^2 + \\lambda t\n$$\nSimplifying the squared term, we get $(\\min(|x_i|, t) - |x_i|)^2 = (\\max(0, |x_i|-t))^2$. So,\n$$\ng(t) = \\tfrac{1}{2} \\sum_{i=1}^n (\\max(0, |x_i|-t))^2 + \\lambda t\n$$\nThe function $g(t)$ is convex and its derivative is $g'(t) = -\\sum_{i=1}^n \\max(0, |x_i|-t) + \\lambda$. Setting the derivative to zero gives the optimality condition for the threshold $t^*$:\n$$\n\\sum_{i=1}^n \\max(0, |x_i|-t^*) = \\lambda\n$$\nThe function $\\phi(t) = \\sum_{i=1}^n \\max(0, |x_i|-t)$ is continuous and monotonically decreasing. If $\\phi(0) = \\sum |x_i| = \\|x\\|_1 \\le \\lambda$, then the minimum of $g(t)$ is at $t^*=0$, which implies $u=0$. Otherwise, a unique root $t^* \\in (0, \\max_i|x_i|]$ exists. We can find this root using a numerical method like bisection or Brent's method. Once $t^*$ is found, the solution is given by $u_i = \\operatorname{sign}(x_i) \\min(|x_i|, t^*)$. This provides an algorithm for computing the first term without invoking Moreau's identity.\n\n### Term 2: $\\lambda \\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right)$\n\nFirst, we must find the convex conjugate $f^*$ of $f(x) = \\|x\\|_\\infty$:\n$$\nf^*(y) = \\sup_{u \\in \\mathbb{R}^n} \\{ \\langle y, u \\rangle - \\|u\\|_\\infty \\}\n$$\nBy Hölder's inequality, $|\\langle y, u \\rangle| \\le \\|y\\|_1 \\|u\\|_\\infty$.\nIf $\\|y\\|_1 \\le 1$, then $\\langle y, u \\rangle \\le \\|y\\|_1 \\|u\\|_\\infty \\le \\|u\\|_\\infty$, which implies $\\langle y, u \\rangle - \\|u\\|_\\infty \\le 0$. The supremum is $0$.\nIf $\\|y\\|_1  1$, we can choose $u$ with components $u_i = K \\cdot \\operatorname{sign}(y_i)$ for some $K0$. Then $\\langle y, u \\rangle - \\|u\\|_\\infty = K\\|y\\|_1 - K = K(\\|y\\|_1 - 1)$. As $K \\to \\infty$, this goes to $+\\infty$.\nThus, $f^*$ is the indicator function of the unit $\\ell_1$ ball, $\\mathbb{B}_1 = \\{y \\in \\mathbb{R}^n : \\|y\\|_1 \\le 1\\}$:\n$$\nf^*(y) = I_{\\mathbb{B}_1}(y) = \\begin{cases} 0  \\text{if } \\|y\\|_1 \\le 1 \\\\ +\\infty  \\text{if } \\|y\\|_1  1 \\end{cases}\n$$\nNow we analyze the proximal operator $\\operatorname{prox}_{f^*/\\lambda}\\left(\\frac{x}{\\lambda}\\right)$. Let $g(v) = \\frac{1}{\\lambda}f^*(v)$. For $\\lambda  0$, $g(v) = \\frac{1}{\\lambda}I_{\\mathbb{B}_1}(v) = I_{\\mathbb{B}_1}(v)$, since the scaling of an indicator function taking values $\\{0, \\infty\\}$ does not change it. The proximal operator is:\n$$\n\\operatorname{prox}_{g}\\left(\\tfrac{x}{\\lambda}\\right) = \\arg\\min_{v \\in \\mathbb{R}^n} \\left\\{ \\tfrac{1}{2}\\left\\|v - \\tfrac{x}{\\lambda}\\right\\|_2^2 + I_{\\mathbb{B}_1}(v) \\right\\}\n$$\nThe indicator function $I_{\\mathbb{B}_1}(v)$ restricts the domain of minimization to the set $\\mathbb{B}_1$. The problem is therefore equivalent to finding the Euclidean projection of the vector $z=x/\\lambda$ onto the unit $\\ell_1$ ball:\n$$\n\\operatorname{prox}_{f^*/\\lambda}\\left(\\tfrac{x}{\\lambda}\\right) = \\operatorname{proj}_{\\mathbb{B}_1}\\left(\\tfrac{x}{\\lambda}\\right)\n$$\nThis projection can be computed efficiently. If $\\|z\\|_1 \\le 1$, the projection is $z$ itself. Otherwise, the projection lies on the boundary of the $\\ell_1$ ball. An efficient algorithm exists for this, which involves sorting the absolute values of the components of $z$ and finding a threshold $\\theta$ such that the solution is given by component-wise soft-thresholding: $v_i = \\operatorname{sign}(z_i)\\max(0, |z_i| - \\theta)$. This provides the required independent algorithm for the second term.\n\n### Summary of the Method\nThe program will implement two distinct functions:\n1.  A function to compute $u = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_\\infty}(x)$ by numerically finding the root $t^*$ of $\\sum_{i=1}^n \\max(0, |x_i|-t) = \\lambda$.\n2.  A function to compute $v = \\operatorname{prox}_{f^*/\\lambda}(x/\\lambda)$ by implementing the efficient projection algorithm onto the unit $\\ell_1$ ball for the vector $x/\\lambda$.\n\nFor each test case $(x, \\lambda)$, we compute $u$ and $v$ using these independent methods and verify if $x$ is equal to $u + \\lambda v$ within the specified numerical tolerance for each coordinate.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef compute_prox_linf(x: np.ndarray, lambda_val: float) - np.ndarray:\n    \"\"\"\n    Computes prox_{lambda * ||.||_inf}(x) using a root-finding method.\n    This is the first term in Moreau's identity.\n    \n    The proximal operator is defined as the solution u to:\n    argmin_u { 1/2 ||u - x||_2^2 + lambda * ||u||_inf }\n\n    The solution is a soft-thresholding of x by a value t_star, where t_star is the root of\n    sum(max(0, |x_i| - t)) = lambda.\n    \"\"\"\n    if lambda_val == 0.0:\n        return x.copy()\n    \n    # Use np.float64 for precision in sums and comparisons\n    x_abs = np.abs(x, dtype=np.float64)\n    norm1_x = np.sum(x_abs)\n\n    if lambda_val = norm1_x:\n        return np.zeros_like(x, dtype=np.float64)\n\n    # Define the function whose root we need to find\n    def phi(t: float, x_abs_in: np.ndarray, lambda_in: float) - float:\n        return np.sum(np.maximum(0., x_abs_in - t)) - lambda_in\n\n    t_max = np.max(x_abs)\n    if t_max == 0:  # Happens if x is the zero vector\n        return np.zeros_like(x, dtype=np.float64)\n    \n    # The root must lie in [0, t_max].\n    # phi(0) = norm1_x - lambda_val  0.\n    # phi(t_max) = -lambda_val  0.\n    # The interval [0, t_max] is guaranteed to bracket the root.\n    try:\n        t_star = brentq(phi, 0.0, t_max, args=(x_abs, lambda_val))\n    except ValueError:\n        # This may occur if t_max is very close to 0 and phi(t_max) evaluates to a non-negative\n        # number due to floating point inaccuracies. In this case, t_star is effectively t_max.\n        if phi(t_max) = 0:\n            t_star = t_max\n        else:\n            raise\n\n    # The solution u is found by component-wise operations\n    u = np.sign(x) * np.minimum(x_abs, t_star)\n    return u\n\ndef project_l1_ball(z: np.ndarray) - np.ndarray:\n    \"\"\"\n    Computes the Euclidean projection of vector z onto the unit L1 ball.\n    Implements the algorithm by Duchi et al. (2008).\n    \"\"\"\n    z = np.asarray(z, dtype=np.float64)\n    if np.sum(np.abs(z)) = 1.0:\n        return z\n\n    n = len(z)\n    mu = np.sort(np.abs(z))[::-1]\n    S_mu = np.cumsum(mu)\n    \n    j_vals = np.arange(1, n + 1)\n    test_vals = mu - (S_mu - 1.0) / j_vals\n    \n    try:\n        # Find the index of the last positive value in test_vals.\n        # This corresponds to rho.\n        rho_idx = np.flatnonzero(test_vals  0)[-1]\n        rho = rho_idx + 1\n    except IndexError:\n        # This block should be unreachable if ||z||_1  1.\n        # It's included for robustness.\n        return z\n\n    theta = (S_mu[rho - 1] - 1.0) / rho\n    \n    # The projection w is a soft-thresholding of z by theta\n    w = np.sign(z) * np.maximum(0., np.abs(z) - theta)\n    return w\n\ndef compute_prox_conjugate_linf(x: np.ndarray, lambda_val: float) - np.ndarray:\n    \"\"\"\n    Computes prox_{f*/lambda}(x/lambda), where f = ||.||_inf.\n    This is equivalent to projecting x/lambda onto the L1 ball.\n    This is the second term (without the lambda multiplier) in Moreau's identity.\n    \"\"\"\n    if lambda_val == 0.0:\n        # The function becomes infinity * I_B1(v). The prox operator projects\n        # onto the domain which is the L1 ball. The problem is argmin{1/2||v-z||^2 + inf*I_B1(v)}.\n        # For this to be finite, v must be in the L1 ball. The formal definition of prox_{inf*g}\n        # can be interpreted as projection onto dom(g) a {0}, which gives {0}.\n        return np.zeros_like(x, dtype=np.float64)\n\n    z = x / lambda_val\n    return project_l1_ball(z)\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and verify Moreau's identity.\n    \"\"\"\n    test_cases = [\n        (np.array([3.2, -1.1, 0.0, 5.5, -2.2]), 1.7),\n        (np.array([0.0, 0.0, 0.0, 0.0]), 2.5),\n        (np.array([10.0, -10.0]), 0.0),\n        (np.array([-0.3, 0.3, -0.3, 0.3, -0.3, 0.3]), 0.5),\n        (np.array([np.sin(i) + 0.5 * np.cos(2*i) for i in range(1, 21)]), 3.0)\n    ]\n\n    results = []\n    atol = 1e-9\n    rtol = 1e-10\n\n    for x_orig, lambda_val in test_cases:\n        # Ensure data types are consistent for high precision\n        x = np.asarray(x_orig, dtype=np.float64)\n\n        # First term of Moreau's identity\n        prox1 = compute_prox_linf(x, lambda_val)\n\n        # Second term (the proximal part only)\n        prox2 = compute_prox_conjugate_linf(x, lambda_val)\n\n        # Right-hand side of Moreau's identity\n        rhs = prox1 + lambda_val * prox2\n        \n        # Numerical verification per coordinate\n        # |x_i - rhs_i| = atol + rtol * |x_i| for all i\n        is_close = np.all(np.abs(x - rhs) = atol + rtol * np.abs(x))\n        results.append(is_close)\n    \n    # Format the boolean results as lowercase strings\n    print(f\"[{','.join(map(lambda b: str(b).lower(), results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of proximal operators shines when they are used as building blocks within larger iterative algorithms to solve complex optimization problems. This advanced practice challenges you to implement Dykstra’s projection method to compute the proximal operator for a sum of two functions: the $\\ell_1$ norm and an indicator function for linear constraints. This exercise demonstrates how repeated application of simple proximal maps (soft-thresholding and affine projection) can solve a sophisticated constrained optimization problem .",
            "id": "3470842",
            "problem": "Implement a program that computes the proximal operator of the convex function $x \\mapsto \\lambda \\lVert x \\rVert_1 + \\iota_{\\{Bx=c\\}}(x)$ at a given point $y \\in \\mathbb{R}^n$ using Dykstra’s method for the sum of two proximable functions, where $\\lambda \\ge 0$ is a scalar, $\\lVert x \\rVert_1$ is the $\\ell_1$ norm, and $\\iota_{\\{Bx=c\\}}$ is the indicator function of the affine set $\\{x \\in \\mathbb{R}^n : Bx = c\\}$. The proximal operator to be computed is\n$$\n\\operatorname{prox}_{\\lambda \\lVert \\cdot \\rVert_1 + \\iota_{\\{Bx=c\\}}}(y)\n\\;\\;=\\;\\;\\arg\\min_{x \\in \\mathbb{R}^n} \\;\\; \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 \\quad \\text{subject to} \\quad Bx = c.\n$$\nYou must implement Dykstra’s method specialized to the case of two functions with known proximal mappings:\n- $f(x) = \\lambda \\lVert x \\rVert_1$, whose proximal operator is the soft-thresholding map, and\n- $g(x) = \\iota_{\\{Bx=c\\}}(x)$, whose proximal operator is the Euclidean projection onto the affine set $\\{x : Bx=c\\}$.\n\nYour derivation should be based solely on the following fundamental definitions and facts:\n- The definition of the proximal operator of a proper, closed, convex function $h$: for any $y \\in \\mathbb{R}^n$,\n$$\n\\operatorname{prox}_h(y) \\;=\\; \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ h(x) + \\tfrac{1}{2} \\lVert x - y \\rVert_2^2 \\right\\}.\n$$\n- The soft-thresholding formula for the proximal operator of $\\lambda \\lVert \\cdot \\rVert_1$.\n- The Euclidean projection formula onto an affine set $\\{x: Bx=c\\}$, expressible via the Moore–Penrose pseudoinverse.\n- The Dykstra iteration for the sum of two proximable functions to compute $\\operatorname{prox}_{f+g}(y)$, initialized from $y$.\n\nAdditionally, you must provide sufficient conditions for linear convergence of Dykstra’s method using Hoffman bounds on constraint sets. Use the formal Hoffman error bound for linear systems: there exists a constant $\\kappa_H \\in (0,\\infty)$ such that for every $x \\in \\mathbb{R}^n$, the distance to the affine set $\\mathcal{A} = \\{x: Bx=c\\}$ satisfies\n$$\n\\operatorname{dist}(x,\\mathcal{A}) \\;\\le\\; \\kappa_H \\,\\lVert Bx - c \\rVert_2,\n$$\nwith $\\kappa_H = \\lVert B^\\dagger \\rVert_2$ when $B$ has full row rank and $\\dagger$ denotes the Moore–Penrose pseudoinverse. Based on this, state explicit assumptions on $B$, $\\lambda$, and feasibility that imply global linear convergence of Dykstra’s method to the unique solution $x^\\star$.\n\nYour program must:\n- Implement Dykstra’s method for computing $x^\\star = \\operatorname{prox}_{\\lambda \\lVert \\cdot \\rVert_1 + \\iota_{\\{Bx=c\\}}}(y)$.\n- Implement the auxiliary proximal mappings:\n  - Soft-thresholding for $f(x) = \\lambda \\lVert x \\rVert_1$.\n  - Projection onto $\\{x: Bx=c\\}$ using the pseudoinverse formula $P_{\\{Bx=c\\}}(v) = v - B^\\top (B B^\\top)^\\dagger (Bv - c)$, which is valid for any $B$ and any $c$ when the pseudoinverse is used; in the special case with no constraints, i.e., $B$ has zero rows, the projection equals the identity map.\n- Implement an Alternating Direction Method of Multipliers (ADMM) solver as an independent check in the general case to solve the same problem by splitting the $\\ell_1$ term from the quadratic term under the linear constraint $Bx=c$. This will be used only to produce a numerical cross-check in the final test case.\n\nTest suite:\nUse the following four test cases. In all cases, vectors are in $\\mathbb{R}^n$, matrices $B \\in \\mathbb{R}^{m \\times n}$, and all entries are real numbers.\n- Test case $1$ (no constraints): $n=6$, $B \\in \\mathbb{R}^{0 \\times 6}$ (zero rows), $c \\in \\mathbb{R}^0$ (empty), $\\lambda = 0.7$, $y = [1.2,-0.5,0.1,2.0,-3.1,0.0]$. The ground truth is $\\operatorname{prox}_{\\lambda \\lVert \\cdot \\rVert_1}(y)$ via soft-thresholding.\n- Test case $2$ (pure projection): $n=5$, $m=2$, $\\lambda = 0$, \n$$\nB = \\begin{bmatrix}\n1  2  0  0  1 \\\\\n0  1  1  0  -1\n\\end{bmatrix}, \\quad\nc = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 2.0 \\\\ 0.3 \\\\ -0.7 \\end{bmatrix}.\n$$\nThe ground truth is the Euclidean projection of $y$ onto $\\{x: Bx=c\\}$.\n- Test case $3$ (singleton affine set): $n=4$, $m=4$, $B = I_4$ (the $4 \\times 4$ identity), $c = [0.2,-0.1,0.3,-0.4]^\\top$, $\\lambda = 0.8$, $y = [3.0,-2.0,0.5,1.5]^\\top$. The ground truth is $c$, since the affine set is a singleton.\n- Test case $4$ (general case cross-check with Alternating Direction Method of Multipliers): $n=8$, $m=3$, $\\lambda = 0.6$,\n$$\nB = \\begin{bmatrix}\n1  0  -1  0  2  0  0  0 \\\\\n0  1  0  -1  0  2  0  0 \\\\\n1  1  0  0  0  0  1  -1\n\\end{bmatrix}, \\quad\nc = \\begin{bmatrix} 0.5 \\\\ -1.2 \\\\ 0.3 \\end{bmatrix}, \\quad\ny = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.7 \\\\ -1.0 \\\\ 2.0 \\\\ -0.3 \\\\ 0.4 \\\\ -0.8 \\end{bmatrix}.\n$$\nCompute the Dykstra solution $x_{\\mathrm{Dyk}}$ and an Alternating Direction Method of Multipliers solution $x_{\\mathrm{ADMM}}$ with a well-chosen penalty parameter and stopping tolerance. Report the Euclidean norm $\\lVert x_{\\mathrm{Dyk}} - x_{\\mathrm{ADMM}} \\rVert_2$, the primal feasibility $\\lVert B x_{\\mathrm{Dyk}} - c \\rVert_2$, and the primal feasibility $\\lVert B x_{\\mathrm{ADMM}} - c \\rVert_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with the following six floating-point numbers in the order given:\n  - For test case $1$: $\\lVert x_{\\mathrm{Dyk}} - x_{\\mathrm{soft}} \\rVert_2$, where $x_{\\mathrm{soft}}$ is the soft-thresholding result.\n  - For test case $2$: $\\lVert x_{\\mathrm{Dyk}} - x_{\\mathrm{proj}} \\rVert_2$, where $x_{\\mathrm{proj}}$ is the Euclidean projection of $y$ onto $\\{x: Bx=c\\}$.\n  - For test case $3$: $\\lVert x_{\\mathrm{Dyk}} - c \\rVert_2$.\n  - For test case $4$: $\\lVert x_{\\mathrm{Dyk}} - x_{\\mathrm{ADMM}} \\rVert_2$, $\\lVert B x_{\\mathrm{Dyk}} - c \\rVert_2$, and $\\lVert B x_{\\mathrm{ADMM}} - c \\rVert_2$.\n- The output must be exactly one line, printed as a Python list literal, for example, $[a,b,c,d,e,f]$ with no additional text.\n\nAngle units and physical units are not applicable. All numerical answers are real-valued scalars. Ensure your program is deterministic and self-contained, using only the specified libraries. Convergence tolerances should be sufficiently stringent to expose numerical correctness to at least $10^{-7}$ relative accuracy in the reported quantities.",
            "solution": "We derive and implement a method to compute $x^\\star = \\operatorname{prox}_{\\lambda \\lVert \\cdot \\rVert_1 + \\iota_{\\{Bx=c\\}}}(y)$ using Dykstra’s method, starting from core definitions in convex analysis. We also state conditions for linear convergence using the Hoffman bound.\n\n1. Foundational definitions and proximal mappings.\nLet $h:\\mathbb{R}^n \\to (-\\infty,+\\infty]$ be proper, closed, convex. The proximal operator at $y \\in \\mathbb{R}^n$ is\n$$\n\\operatorname{prox}_h(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ h(x) + \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 \\right\\}.\n$$\nWe consider two functions $f(x) = \\lambda \\lVert x \\rVert_1$ and $g(x) = \\iota_{\\mathcal{A}}(x)$ where $\\mathcal{A} = \\{x: Bx=c\\}$ is an affine set. Then $\\operatorname{prox}_f$ is the soft-threshold map applied componentwise:\n$$\n\\left[\\operatorname{prox}_{\\lambda \\lVert \\cdot \\rVert_1}(v)\\right]_i \\;=\\; \\operatorname{sign}(v_i)\\,\\max\\{|v_i| - \\lambda, 0\\}.\n$$\nFor $g(x) = \\iota_{\\mathcal{A}}(x)$, the proximal operator $\\operatorname{prox}_g$ is the Euclidean projection onto $\\mathcal{A}$:\n$$\n\\operatorname{prox}_{\\iota_{\\mathcal{A}}}(v) \\;=\\; \\arg\\min_{x \\in \\mathcal{A}} \\tfrac{1}{2}\\lVert x - v \\rVert_2^2 \\;=\\; v - B^\\top (B B^\\top)^\\dagger (Bv - c),\n$$\nwhere $(\\cdot)^\\dagger$ denotes the Moore–Penrose pseudoinverse. When $B$ has zero rows, $\\mathcal{A} = \\mathbb{R}^n$ and the projection equals the identity.\n\n2. Dykstra’s method for $\\operatorname{prox}_{f+g}(y)$.\nWe seek the minimizer of\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) + g(x) + \\tfrac{1}{2}\\lVert x - y \\rVert_2^2,\n$$\nwhich equals $\\operatorname{prox}_{f+g}(y)$ by definition. Dykstra’s method (two-function case) generates sequences $\\{x^k\\}$, $\\{p^k\\}$, $\\{q^k\\}$ initialized at $x^0 = y$, $p^0 = 0$, $q^0 = 0$, and iterates\n$$\n\\begin{aligned}\nx^{k+\\tfrac{1}{2}} = \\operatorname{prox}_f(x^k + p^k), \\\\\np^{k+1} = x^k + p^k - x^{k+\\tfrac{1}{2}}, \\\\\nx^{k+1} = \\operatorname{prox}_g(x^{k+\\tfrac{1}{2}} + q^k), \\\\\nq^{k+1} = x^{k+\\tfrac{1}{2}} + q^k - x^{k+1}.\n\\end{aligned}\n$$\nUnder standard assumptions (proper, closed, convex $f$ and $g$), $x^k$ converges to $\\operatorname{prox}_{f+g}(y)$. In our case, $\\operatorname{prox}_f$ is soft-thresholding and $\\operatorname{prox}_g$ is projection onto $\\mathcal{A}$.\n\nStopping criterion is chosen via fixed-point stabilization and feasibility: terminate when $\\lVert x^{k+1} - x^k \\rVert_2 \\le \\varepsilon \\max\\{1,\\lVert x^k \\rVert_2\\}$ and $\\lVert Bx^{k+1} - c \\rVert_2 \\le \\varepsilon_{\\mathrm{feas}}$, for tolerances $\\varepsilon$, $\\varepsilon_{\\mathrm{feas}}$.\n\n3. Alternating Direction Method of Multipliers (ADMM) cross-check.\nWe provide an independent solver using splitting of the $\\ell_1$ term. Introduce $z$ with constraint $x - z = 0$ and enforce $Bx=c$:\n$$\n\\min_{x,z} \\; \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\lambda \\lVert z \\rVert_1 \\quad \\text{subject to} \\quad x - z = 0, \\; Bx = c.\n$$\nScaled ADMM with penalty parameter $\\rho  0$ updates\n$$\n\\begin{aligned}\nx^{k+1} = \\arg\\min_{Bx=c} \\; \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\tfrac{\\rho}{2}\\lVert x - z^k + u^k \\rVert_2^2, \\\\\nz^{k+1} = \\operatorname{prox}_{(\\lambda/\\rho)\\lVert \\cdot \\rVert_1}(x^{k+1} + u^k), \\\\\nu^{k+1} = u^k + x^{k+1} - z^{k+1}.\n\\end{aligned}\n$$\nThe $x$-update is a strictly convex quadratic program with linear equality $Bx=c$ and closed form via Karush–Kuhn–Tucker (KKT) conditions. Let $\\alpha = 1 + \\rho$, then\n$$\n\\alpha x - (y + \\rho(z^k - u^k)) + B^\\top \\nu = 0, \\quad Bx = c.\n$$\nEliminate $x$:\n$$\nx = \\alpha^{-1}\\left(y + \\rho(z^k - u^k) - B^\\top \\nu \\right), \\quad\nB B^\\top \\nu = (B y + \\rho B(z^k - u^k)) - \\alpha c.\n$$\nWe compute $\\nu = (B B^\\top)^\\dagger \\left( (B y + \\rho B(z^k - u^k)) - \\alpha c \\right)$ and then $x$. This yields a numerically stable update with the pseudoinverse.\n\n4. Conditions for linear convergence using the Hoffman bound.\nDefine $\\mathcal{A} = \\{x \\in \\mathbb{R}^n : Bx = c\\}$. Assume:\n- $B \\in \\mathbb{R}^{m \\times n}$ has full row rank (so $B B^\\top$ is invertible), and the system is consistent, hence $\\mathcal{A} \\neq \\emptyset$.\n- The Hoffman bound holds for the affine set: there exists $\\kappa_H \\in (0,\\infty)$ such that for all $x \\in \\mathbb{R}^n$,\n$$\n\\operatorname{dist}(x,\\mathcal{A}) \\le \\kappa_H \\lVert Bx - c \\rVert_2,\n$$\nwith $\\kappa_H = \\lVert B^\\dagger \\rVert_2$ when $B$ has full row rank.\n- The function $f(x) = \\lambda \\lVert x \\rVert_1$ is polyhedral (its epigraph is a polyhedron), and $g(x) = \\iota_{\\mathcal{A}}(x)$ is also polyhedral. The composite objective for the proximal point,\n$$\nF_y(x) = f(x) + g(x) + \\tfrac{1}{2}\\lVert x - y \\rVert_2^2,\n$$\nis piecewise linear–quadratic and strongly convex with modulus $1$ due to the quadratic term. By standard error bound results for piecewise linear–quadratic convex functions, there exists $\\mu \\in (0,\\infty)$ such that for all $x$ in a neighborhood of the minimizer $x^\\star$,\n$$\n\\operatorname{dist}(x, X^\\star) \\le \\mu \\, \\operatorname{dist}\\left(0, \\partial F_y(x)\\right),\n$$\nwhere $X^\\star = \\{x^\\star\\}$ is the singleton minimizer set and $\\partial F_y$ is the subdifferential of $F_y$. The constant $\\mu$ can be chosen proportional to $\\kappa_H$ because the only source of non-smoothness and constraint infeasibility is polyhedral, and the Hoffman bound controls the distance to feasibility via residuals $Bx-c$.\n\nUnder these assumptions (polyhedrality of $f$ and $g$, strong convexity of the quadratic term, nonempty $\\mathcal{A}$, and a Hoffman bound), the subdifferential $\\partial F_y$ is metrically subregular at $(x^\\star,0)$ with modulus depending on $\\kappa_H$ and $\\lambda$. Consequently, the cyclic proximal map associated with Dykstra’s method is locally contractive with a linear rate. Therefore, there exists a constant $\\theta \\in (0,1)$, depending only on $\\kappa_H$, $\\lambda$, and the geometry of the active faces at $x^\\star$, such that for all sufficiently large $k$,\n$$\n\\lVert x^{k+1} - x^\\star \\rVert_2 \\le \\theta \\, \\lVert x^k - x^\\star \\rVert_2.\n$$\nIn particular, when $B$ has full row rank (hence finite $\\kappa_H$) and the problem is feasible, the iterates of Dykstra’s method converge linearly to $x^\\star$. If $B$ lacks full row rank but the affine set $\\mathcal{A}$ is nonempty, the Hoffman bound still holds with $\\kappa_H = \\lVert B^\\dagger \\rVert_2$ using the Moore–Penrose pseudoinverse, and the same conclusion follows.\n\n5. Test suite and outputs.\nWe implement the algorithm and apply it to four fixed test cases specified in the problem. For test case $1$ (no constraints), the result must equal soft-thresholding; we report the Euclidean discrepancy. For test case $2$ (pure projection), we report the discrepancy to the analytic projection. For test case $3$ (singleton affine set), we report the discrepancy to $c$. For test case $4$ (general case), we compute both Dykstra and Alternating Direction Method of Multipliers solutions and report the Euclidean difference and the primal feasibility of both. The program outputs a single line list $[r_1, r_2, r_3, r_4, r_5, r_6]$ containing these six floats.\n\nAll numerical linear algebra is performed with the Moore–Penrose pseudoinverse to ensure correctness without additional assumptions beyond feasibility. Tolerances are chosen to typically yield residuals below $10^{-8}$ to $10^{-10}$ on the provided tests.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(v: np.ndarray, lam: float) - np.ndarray:\n    \"\"\"Soft-thresholding operator for lambda * ||x||_1.\"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - lam, 0.0)\n\n\ndef proj_affine(v: np.ndarray, B: np.ndarray, c: np.ndarray) - np.ndarray:\n    \"\"\"\n    Euclidean projection of v onto the affine set {x: B x = c}.\n    Uses pseudoinverse: x = v - B^T (B B^T)^+ (B v - c).\n    Handles the zero-rows case by returning v.\n    \"\"\"\n    m, n = B.shape\n    if m == 0:\n        return v.copy()\n    M = B @ B.T  # shape (m, m)\n    # Use pseudoinverse for robustness in all cases.\n    M_pinv = np.linalg.pinv(M)\n    delta = B @ v - c\n    w = M_pinv @ delta\n    x = v - B.T @ w\n    return x\n\n\ndef dykstra_prox_l1_affine(y: np.ndarray, lam: float, B: np.ndarray, c: np.ndarray,\n                           max_iter: int = 20000, tol: float = 1e-12, feas_tol: float = 1e-12) - np.ndarray:\n    \"\"\"\n    Dykstra's method for prox_{lam ||.||_1 + iota_{Bx=c}}(y).\n    Returns the computed proximal point.\n    \"\"\"\n    x = y.copy()\n    p = np.zeros_like(y)\n    q = np.zeros_like(y)\n    for k in range(max_iter):\n        x_f = soft_threshold(x + p, lam)\n        p = x + p - x_f\n        x_g = proj_affine(x_f + q, B, c)\n        q = x_f + q - x_g\n        x_new = x_g\n        # Stopping criteria: small change and feasible.\n        if np.linalg.norm(x_new - x) = tol * max(1.0, np.linalg.norm(x)) and \\\n           (B.shape[0] == 0 or np.linalg.norm(B @ x_new - c) = feas_tol):\n            x = x_new\n            break\n        x = x_new\n    return x\n\n\ndef admm_prox_l1_affine(y: np.ndarray, lam: float, B: np.ndarray, c: np.ndarray,\n                        rho: float = 1.0, max_iter: int = 10000, tol: float = 1e-8) - np.ndarray:\n    \"\"\"\n    ADMM to solve min 0.5||x - y||^2 + lam ||z||_1 s.t. x - z = 0, Bx = c.\n    Returns x.\n    \"\"\"\n    n = y.size\n    x = y.copy()\n    z = soft_threshold(x, lam)  # initial z\n    u = np.zeros_like(y)\n    m = B.shape[0]\n    # Precompute pseudoinverse of B B^T\n    if m  0:\n        M = B @ B.T\n        M_pinv = np.linalg.pinv(M)\n    alpha = 1.0 + rho\n    for k in range(max_iter):\n        v = y + rho * (z - u)\n        if m  0:\n            rhs = (B @ v) - alpha * c\n            nu = M_pinv @ rhs\n            x_new = (v - B.T @ nu) / alpha\n        else:\n            x_new = v / alpha\n        z_old = z\n        # z-update: soft-threshold on x + u\n        z = soft_threshold(x_new + u, lam / rho)\n        # dual update\n        u = u + x_new - z\n        # Residuals\n        r_norm = np.linalg.norm(x_new - z)\n        s_norm = rho * np.linalg.norm(z - z_old)\n        feas = np.linalg.norm(B @ x_new - c) if m  0 else 0.0\n        x = x_new\n        if r_norm = tol * max(1.0, np.linalg.norm(x), np.linalg.norm(z)) and \\\n           s_norm = tol * max(1.0, np.linalg.norm(u)) and \\\n           (feas = tol * max(1.0, np.linalg.norm(c)) if m  0 else True):\n            break\n    return x\n\n\ndef solve():\n    results = []\n\n    # Test case 1: No constraints, prox reduces to soft-thresholding\n    y1 = np.array([1.2, -0.5, 0.1, 2.0, -3.1, 0.0], dtype=float)\n    lam1 = 0.7\n    B1 = np.zeros((0, 6), dtype=float)  # zero rows\n    c1 = np.zeros((0,), dtype=float)\n    x_dyk_1 = dykstra_prox_l1_affine(y1, lam1, B1, c1)\n    x_soft_1 = soft_threshold(y1, lam1)\n    res1 = float(np.linalg.norm(x_dyk_1 - x_soft_1))\n    results.append(res1)\n\n    # Test case 2: Pure projection (lambda = 0)\n    y2 = np.array([0.5, -1.0, 2.0, 0.3, -0.7], dtype=float)\n    lam2 = 0.0\n    B2 = np.array([[1.0, 2.0, 0.0, 0.0, 1.0],\n                   [0.0, 1.0, 1.0, 0.0, -1.0]], dtype=float)\n    c2 = np.array([1.0, -0.5], dtype=float)\n    x_dyk_2 = dykstra_prox_l1_affine(y2, lam2, B2, c2)\n    x_proj_2 = proj_affine(y2, B2, c2)\n    res2 = float(np.linalg.norm(x_dyk_2 - x_proj_2))\n    results.append(res2)\n\n    # Test case 3: Singleton affine set (B = I), solution is c\n    y3 = np.array([3.0, -2.0, 0.5, 1.5], dtype=float)\n    lam3 = 0.8\n    B3 = np.eye(4, dtype=float)\n    c3 = np.array([0.2, -0.1, 0.3, -0.4], dtype=float)\n    x_dyk_3 = dykstra_prox_l1_affine(y3, lam3, B3, c3)\n    res3 = float(np.linalg.norm(x_dyk_3 - c3))\n    results.append(res3)\n\n    # Test case 4: General case cross-check with ADMM\n    y4 = np.array([1.0, -0.5, 0.7, -1.0, 2.0, -0.3, 0.4, -0.8], dtype=float)\n    lam4 = 0.6\n    B4 = np.array([\n        [1.0, 0.0, -1.0, 0.0, 2.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0,  0.0,-1.0, 0.0, 2.0, 0.0, 0.0],\n        [1.0, 1.0,  0.0, 0.0, 0.0, 0.0, 1.0,-1.0]\n    ], dtype=float)\n    c4 = np.array([0.5, -1.2, 0.3], dtype=float)\n\n    x_dyk_4 = dykstra_prox_l1_affine(y4, lam4, B4, c4, max_iter=30000, tol=1e-12, feas_tol=1e-12)\n    x_admm_4 = admm_prox_l1_affine(y4, lam4, B4, c4, rho=1.0, max_iter=20000, tol=1e-8)\n    diff4 = float(np.linalg.norm(x_dyk_4 - x_admm_4))\n    feas_dyk_4 = float(np.linalg.norm(B4 @ x_dyk_4 - c4))\n    feas_admm_4 = float(np.linalg.norm(B4 @ x_admm_4 - c4))\n    results.extend([diff4, feas_dyk_4, feas_admm_4])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(repr, results))}]\")\n\nsolve()\n```"
        }
    ]
}