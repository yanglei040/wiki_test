{
    "hands_on_practices": [
        {
            "introduction": "Before deploying any iterative algorithm, it is crucial to understand its computational complexity. This first practice invites you to look under the hood of a single FISTA iteration applied to the LASSO problem. By meticulously counting the floating-point operations (flops) involved in each component step—from matrix-vector products to the soft-thresholding operator—you will develop a concrete understanding of the algorithm's workload and how it scales with problem dimensions $m$ and $n$ . This analysis is fundamental for predicting performance and identifying computational bottlenecks in large-scale applications.",
            "id": "3446947",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem with a dense matrix $A \\in \\mathbb{R}^{m \\times n}$ and a data vector $b \\in \\mathbb{R}^{m}$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda > 0$. The Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is a proximal-gradient method that, at each iteration, forms a momentum point as a linear combination of the two most recent iterates, computes the gradient of the smooth quadratic term at that momentum point, and then applies the proximal operator of the $\\ell_{1}$ norm (soft-thresholding).\n\nUse only the following fundamental base:\n- The smooth part is $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$, whose gradient is $\\nabla g(x) = A^{\\mathsf{T}}(A x - b)$.\n- The nonsmooth part is $h(x) = \\lambda \\|x\\|_{1}$, whose proximal operator at step size $t > 0$ is the soft-thresholding map $S_{\\tau}$ with threshold $\\tau = \\lambda t$, applied componentwise.\n\nAssume the following computational and counting model:\n- The matrix $A$ is dense, and matrix-vector products are computed by naive row-wise and column-wise dot products: each dot product of length $n$ uses $n$ multiplications and $n - 1$ additions; each dot product of length $m$ uses $m$ multiplications and $m - 1$ additions.\n- Count one floating-point addition or multiplication as exactly one floating-point operation (flop). Precomputed scalars (such as $t$, $1/t$, or $\\tau$) are available without additional cost.\n- Comparisons, branching, and absolute value or sign determinations are not counted as flops. Any division by a scalar is implemented as a multiplication by its reciprocal.\n- Treat the soft-thresholding step in a worst-case arithmetic sense: for each component, if it is active (not mapped to zero), it incurs exactly one addition with $\\tau$; if it is mapped to zero, it incurs no arithmetic flops. For the purpose of this estimate, consider the worst case that all $n$ components are active.\n- Ignore any line-search or step-size update beyond a fixed step size $t = 1/L$, where $L$ is a Lipschitz constant of $\\nabla g$; ignore any stopping checks.\n\nUnder these assumptions, one FISTA iteration for LASSO can be decomposed as:\n- A momentum formation as a linear combination of two $n$-vectors.\n- Two matrix-vector multiplications, one with $A$ and one with $A^{\\mathsf{T}}$.\n- One subtraction of the $m$-vector $b$ from an $m$-vector.\n- A gradient step involving scaling an $n$-vector and subtracting from an $n$-vector.\n- A soft-thresholding map applied componentwise to an $n$-vector at threshold $\\tau$.\n\nDerive, from first principles and the above counting model, the exact total flop count per FISTA iteration as a closed-form expression in terms of $m$ and $n$. Your final answer must be a single analytic expression. No rounding is required.",
            "solution": "The objective is to determine the total number of floating-point operations (flops) required for a single iteration of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) applied to the LASSO problem, based on a specified computational model. The FISTA iteration is decomposed into five distinct steps, and we will calculate the flop count for each step before summing them to find the total.\n\nThe problem is defined as $\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, with a dense matrix $A \\in \\mathbb{R}^{m \\times n}$, and vectors $b \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$. The smooth part is $g(x)$ and the non-smooth part is $h(x)$.\n\nWe will analyze each step of the provided decomposition of a FISTA iteration.\n\n1.  **Momentum formation as a linear combination of two $n$-vectors:**\n    Let the two most recent iterates be $x_k \\in \\mathbb{R}^n$ and $x_{k-1} \\in \\mathbb{R}^n$. The momentum point $y_k$ is typically formed as $y_k = x_k + \\alpha_k (x_k - x_{k-1})$, where $\\alpha_k$ is a precomputed scalar. This can be analyzed in three parts:\n    -   Vector subtraction: $d = x_k - x_{k-1}$. This involves $n$ subtractions, costing $n$ flops.\n    -   Scalar-vector multiplication: $\\alpha_k d$. This involves $n$ multiplications, costing $n$ flops.\n    -   Vector addition: $x_k + (\\alpha_k d)$. This involves $n$ additions, costing $n$ flops.\n    Both interpretations lead to a total of $3n$ flops for this step.\n\n2.  **Two matrix-vector multiplications and one vector subtraction:**\n    These operations are performed to compute the gradient of the smooth term, $\\nabla g(y_k) = A^{\\mathsf{T}}(A y_k - b)$. The computation proceeds as follows:\n    -   First matrix-vector product, $v = A y_k$: The matrix $A$ is of size $m \\times n$ and $y_k$ is an $n$-vector. This requires computing $m$ dot products of length $n$. According to the specified model, a dot product of length $n$ costs $n$ multiplications and $n-1$ additions, for a total of $2n-1$ flops. The total cost for this operation is $m(2n-1) = 2mn - m$ flops.\n    -   Vector subtraction, $u = v - b$: Here, $v$ and $b$ are both $m$-vectors. This operation requires $m$ subtractions, costing $m$ flops.\n    -   Second matrix-vector product, $\\nabla g = A^{\\mathsf{T}} u$: The matrix $A^{\\mathsf{T}}$ is of size $n \\times m$ and $u$ is an $m$-vector. This requires computing $n$ dot products of length $m$. A dot product of length $m$ costs $m$ multiplications and $m-1$ additions, for a total of $2m-1$ flops. The total cost is $n(2m-1) = 2mn - n$ flops.\n    The combined cost for these three related operations is $(2mn - m) + m + (2mn - n) = 4mn - n$ flops.\n\n3.  **A gradient step involving scaling an $n$-vector and subtracting from an $n$-vector:**\n    This step corresponds to updating the momentum point with the gradient information: $z_k = y_k - t \\nabla g(y_k)$, where $t$ is the step size (a precomputed scalar) and $\\nabla g(y_k)$ is the $n$-vector computed previously.\n    -   Scalar-vector multiplication: $t \\nabla g(y_k)$. This involves scaling an $n$-vector by a scalar, which costs $n$ multiplications.\n    -   Vector subtraction: $y_k - (t \\nabla g(y_k))$. This involves subtracting two $n$-vectors, which costs $n$ subtractions.\n    The total cost for this step is $n + n = 2n$ flops.\n\n4.  **A soft-thresholding map applied componentwise to an $n$-vector:**\n    This is the proximal step, $x_{k+1} = S_{\\tau}(z_k)$, where $\\tau = \\lambda t$. The problem statement provides a specific counting model for this operation: in the worst-case scenario where all $n$ components are active (i.e., not mapped to zero), the cost is exactly one addition/subtraction per component. Any other operations like comparisons, absolute values, or sign determinations are not counted.\n    Therefore, the total cost for this step, under the worst-case assumption, is $n \\times 1 = n$ flops.\n\n**Total Flop Count:**\nThe total flop count per FISTA iteration is the sum of the flops from each of these steps.\nTotal Flops = (Momentum) + (Gradient Calculation) + (Gradient Step) + (Soft-Thresholding)\nTotal Flops = $(3n) + (4mn - n) + (2n) + (n)$\nTotal Flops = $4mn + (3 - 1 + 2 + 1)n$\nTotal Flops = $4mn + 5n$\nThus, the exact total flop count per FISTA iteration, under the given model and assumptions, is $4mn + 5n$.",
            "answer": "$$\n\\boxed{4mn + 5n}\n$$"
        },
        {
            "introduction": "The convergence of proximal gradient methods like ISTA and FISTA hinges on the correct choice of step size, which is inversely related to the Lipschitz constant $L$ of the smooth part of the objective. This exercise provides a crucial, hands-on demonstration of this theoretical requirement. You will first compute the true Lipschitz constant for a given problem and then simulate the behavior of both ISTA and FISTA using step sizes that are correct, too conservative (overestimating $L$), and too aggressive (underestimating $L$) . Observing the resulting stability or divergence will build a strong intuition for why the condition $t \\le 1/L$ is not merely a theoretical footnote, but a critical practical guarantee.",
            "id": "3446919",
            "problem": "Consider the sparse optimization problem in compressed sensing with the composite objective consisting of a smooth quadratic data-fit term and a nonsmooth sparsity-promoting term. Let the smooth part be defined by the matrix-vector map and its gradient, and let the nonsmooth part be the entry-wise absolute-value penalty. The Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) are proximal gradient methods whose step-size choice depends on the Lipschitz constant of the gradient of the smooth part. Your task is to compute the Lipschitz constant from specified singular values and then simulate the effect of underestimating or overestimating this constant on the stability of ISTA and FISTA steps.\n\nStart from the following fundamental base:\n- The smooth part is the quadratic function $g(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2$ and its gradient $\\nabla g(x)$ exists for all $x$.\n- The composite objective is $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$, where $\\lambda > 0$ is a scalar.\n- The proximal gradient methodology ensures that a step-size $t$ at most the reciprocal of the Lipschitz constant of $\\nabla g$ guarantees descent properties for ISTA on $F(x)$.\n\nYou must:\n- Compute the Lipschitz constant $L$ of $\\nabla g$ using only the provided singular values of $A$, based on foundational facts about the gradient of a quadratic form and the relationship between singular values and operator norms.\n- Construct a matrix $A$ of size $m \\times n$ with the specified singular values using orthonormal factors, and generate a sparse ground truth $x^\\star$ and observation vector $b$ with small additive noise.\n- Implement the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) from first principles as proximal gradient methods.\n- For a given estimated Lipschitz constant $\\widehat{L}$, use the step-size $t = 1 / \\widehat{L}$ in both ISTA and FISTA.\n- Define a stability indicator for each algorithm as follows: the sequence of objective values $\\{F(x_k)\\}_{k=0}^{K}$ is stable if it is nonincreasing up to a numerical tolerance $\\tau$, meaning $F(x_{k+1}) \\leq F(x_k) + \\tau$ for all $k \\in \\{0, 1, \\dots, K-1\\}$, where $\\tau = 10^{-12}$.\n- Analyze the impact of step-size choices corresponding to exact, overestimated, and underestimated $\\widehat{L}$ values on stability for ISTA and FISTA.\n\nMatrix construction and data generation details:\n- Use $m = 10$ and $n = 10$ for all test cases. Construct $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with the specified singular values on its diagonal. Generate $U$ and $V$ by orthonormalizing pseudorandom Gaussian matrices with fixed seeds.\n- Use the sparse ground truth $x^\\star \\in \\mathbb{R}^{n}$ with nonzero entries at indices $0$, $3$, and $7$, with values $3.0$, $-2.0$, and $1.5$, respectively, and zeros elsewhere. That is, $x^\\star = [3.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0]$.\n- Generate $b = A x^\\star + \\eta$, where $\\eta \\in \\mathbb{R}^{m}$ is Gaussian noise with mean $0$ and standard deviation $\\sigma = 0.01$.\n- Use the regularization parameter $\\lambda = 0.1$.\n\nAlgorithms to implement:\n- Implement ISTA and FISTA for the objective $F(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$, with initialization $x_0 = 0$ for ISTA and $x_0 = y_0 = 0$ for FISTA, and a maximum number of iterations $K = 200$ for both. For FISTA, use the standard acceleration sequence for the momentum parameter.\n\nTest suite:\nProvide the following three test cases, each with a specified list of singular values for $A$, a pseudorandom seed for reproducibility, and evaluate four choices of $\\widehat{L}$: exact $\\widehat{L} = L$, overestimate $\\widehat{L} = 2 L$, and underestimates $\\widehat{L} = 0.5 L$ and $\\widehat{L} = 0.2 L$.\n\n- Test Case $1$ (well-conditioned):\n  - Singular values: $[1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7]$.\n  - Seed for matrix construction and noise: $0$.\n- Test Case $2$ (ill-conditioned):\n  - Singular values: $[3.0, 1.0, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]$.\n  - Seed for matrix construction and noise: $1$.\n- Test Case $3$ (rank-deficient):\n  - Singular values: $[2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Seed for matrix construction and noise: $2$.\n\nFor each test case, compute $L$ from the singular values and, for each choice of $\\widehat{L} \\in \\{L, 2 L, 0.5 L, 0.2 L\\}$, run ISTA and FISTA with $t = 1 / \\widehat{L}$. For each run, compute the objective sequence $\\{F(x_k)\\}_{k=0}^{K}$ and the stability indicator defined above.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for the three test cases. Each test case result must be a list of nine entries:\n$[L,\\ \\text{ISTA\\_stable\\_exact},\\ \\text{FISTA\\_stable\\_exact},\\ \\text{ISTA\\_stable\\_over},\\ \\text{FISTA\\_stable\\_over},\\ \\text{ISTA\\_stable\\_under0.5},\\ \\text{FISTA\\_stable\\_under0.5},\\ \\text{ISTA\\_stable\\_under0.2},\\ \\text{FISTA\\_stable\\_under0.2}]$,\nwhere $L$ is a floating-point number and each stability indicator is a boolean. The program must print the list of the three per-test-case lists as a comma-separated list enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The task is to analyze the stability of two proximal gradient methods, the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), when applied to a sparse optimization problem. The stability is evaluated with respect to the choice of step-size, which is determined by an estimate $\\widehat{L}$ of the Lipschitz constant $L$ of the gradient of the problem's smooth component.\n\nThe composite objective function is given by $F(x) = g(x) + h(x)$, where $g(x)$ is the smooth data-fidelity term and $h(x)$ is the nonsmooth regularization term.\n- The smooth part is the least-squares cost function: $g(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2$, for a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector of observations $b \\in \\mathbb{R}^{m}$, and the variable $x \\in \\mathbb{R}^{n}$.\n- The nonsmooth part is the $\\ell_1$-norm, which promotes sparsity: $h(x) = \\lambda \\lVert x \\rVert_1$, with a regularization parameter $\\lambda > 0$.\n\nThe solution proceeds in three stages: first, we derive the formula for the Lipschitz constant $L$; second, we specify the ISTA and FISTA algorithms; and third, we outline the numerical experiment to test stability.\n\n**1. Derivation of the Lipschitz Constant**\n\nThe convergence of proximal gradient methods like ISTA and FISTA relies on a proper step-size selection, which is bounded by the reciprocal of the Lipschitz constant of $\\nabla g$. A function $\\nabla g: \\mathbb{R}^n \\to \\mathbb{R}^n$ is Lipschitz continuous with constant $L$ if, for all $x, y \\in \\mathbb{R}^n$:\n$$ \\lVert \\nabla g(x) - \\nabla g(y) \\rVert_2 \\leq L \\lVert x - y \\rVert_2 $$\nThe smallest such $L$ is the Lipschitz constant.\n\nFirst, we compute the gradient of $g(x)$:\n$$ g(x) = \\frac{1}{2}(Ax-b)^\\top(Ax-b) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) $$\nThe gradient with respect to $x$ is:\n$$ \\nabla g(x) = A^\\top A x - A^\\top b = A^\\top(Ax-b) $$\nNext, we evaluate the difference $\\nabla g(x) - \\nabla g(y)$:\n$$ \\nabla g(x) - \\nabla g(y) = (A^\\top A x - A^\\top b) - (A^\\top A y - A^\\top b) = A^\\top A (x - y) $$\nSubstituting this into the Lipschitz condition:\n$$ \\lVert A^\\top A (x - y) \\rVert_2 \\leq L \\lVert x - y \\rVert_2 $$\nThis inequality must hold for all $x, y$. This is equivalent to finding the operator norm (or spectral norm) of the matrix $A^\\top A$:\n$$ L = \\lVert A^\\top A \\rVert_2 = \\sup_{z \\neq 0} \\frac{\\lVert A^\\top A z \\rVert_2}{\\lVert z \\rVert_2} $$\nThe operator norm of a symmetric positive semi-definite matrix like $A^\\top A$ is its largest eigenvalue. Let the singular values of $A$ be $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_{\\min(m,n)} \\geq 0$. The eigenvalues of $A^\\top A$ are the squares of the singular values of $A$. Thus, the largest eigenvalue of $A^\\top A$ is the square of the largest singular value of $A$, denoted $\\sigma_{\\max}(A)$.\nTherefore, the Lipschitz constant is:\n$$ L = \\sigma_{\\max}(A)^2 $$\nFor each test case, $L$ will be computed by taking the maximum value from the provided list of singular values and squaring it.\n\n**2. ISTA and FISTA Algorithms**\n\nBoth ISTA and FISTA are instances of the proximal gradient method, which generates a sequence of iterates $\\{x_k\\}$ that converges to a minimizer of $F(x)$. The core of the method is the proximal operator. For $h(x) = \\lambda \\lVert x \\rVert_1$, the proximal operator is the soft-thresholding function, $S_{\\gamma}(z)$:\n$$ \\text{prox}_{\\gamma h}(z) = S_{\\gamma}(z) $$\nwhere $(S_{\\gamma}(z))_i = \\text{sign}(z_i) \\max(|z_i| - \\gamma, 0)$ for each component $i$ of the vector $z$.\n\nThe general update step involves a gradient descent step on the smooth part $g(x)$ followed by a proximal step on the nonsmooth part $h(x)$. Both algorithms use a fixed step-size $t = 1/\\widehat{L}$, where $\\widehat{L}$ is an estimate of the true Lipschitz constant $L$.\n\n**Iterative Shrinkage-Thresholding Algorithm (ISTA)**\nWith an initial guess $x_0$, the ISTA update rule is:\n$$ x_{k+1} = S_{t\\lambda}(x_k - t \\nabla g(x_k)) = S_{t\\lambda}(x_k - t A^\\top(Ax_k - b)) $$\nFor a step-size $t \\leq 1/L$, ISTA is guaranteed to be a descent algorithm, i.e., $F(x_{k+1}) \\leq F(x_k)$.\n\n**Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)**\nFISTA accelerates ISTA by incorporating a momentum term. It uses an auxiliary sequence $\\{y_k\\}$ and a momentum parameter sequence $\\{\\theta_k\\}$.\nWith initializations $x_0 = y_0$ and $\\theta_0=1$:\n\\begin{align*}\nx_{k+1} &= S_{t\\lambda}(y_k - t \\nabla g(y_k)) = S_{t\\lambda}(y_k - t A^\\top(Ay_k - b)) \\\\\n\\theta_{k+1} &= \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2} \\\\\ny_{k+1} &= x_{k+1} + \\frac{\\theta_k - 1}{\\theta_{k+1}}(x_{k+1} - x_k)\n\\end{align*}\nFISTA achieves a faster convergence rate of $O(1/k^2)$ for the objective value error, provided that $t \\leq 1/L$. However, unlike ISTA, FISTA is not a descent method; the sequence of objective values $\\{F(x_k)\\}$ is not guaranteed to be non-increasing, even with a valid step-size. Oscillations can occur, though the overall trend is descent towards the minimum.\n\n**3. Simulation and Stability Analysis**\n\nThe problem requires constructing a matrix $A \\in \\mathbb{R}^{10 \\times 10}$ with specified singular values. This is done by forming the Singular Value Decomposition $A = U \\Sigma V^\\top$, where $\\Sigma$ is the diagonal matrix of specified singular values, and $U, V$ are random orthonormal matrices generated via QR decomposition of Gaussian random matrices. A sparse ground truth vector $x^\\star$ is defined, and the measurement vector $b$ is generated as $b = A x^\\star + \\eta$, where $\\eta$ is a small Gaussian noise vector.\n\nFor each of the three test cases (well-conditioned, ill-conditioned, and rank-deficient), we perform the following analysis:\n1. Compute the true Lipschitz constant $L = \\sigma_{\\max}(A)^2$.\n2. For each estimate $\\widehat{L} \\in \\{L, 2L, 0.5L, 0.2L\\}$, set the step-size $t = 1/\\widehat{L}$.\n3. Run ISTA and FISTA for $K=200$ iterations starting from $x_0 = 0$ (and $y_0 = 0$ for FISTA).\n4. For each run, record the sequence of objective function values $\\{F(x_k)\\}_{k=0}^{K}$.\n5. Evaluate the stability of this sequence using the provided criterion: the sequence is stable if $F(x_{k+1}) \\leq F(x_k) + \\tau$ for all $k \\in \\{0, 1, \\dots, K-1\\}$, where $\\tau = 10^{-12}$.\n\nThe expected outcome is that for $\\widehat{L} \\geq L$ (i.e., $t \\leq 1/L$), the algorithms will be stable, as this satisfies the theoretical condition for convergence. For $\\widehat{L} < L$ (i.e., $t > 1/L$), the step-size is too large, violating the convergence condition, and the algorithms are expected to become unstable, exhibiting either oscillatory or divergent behavior in the objective function values. As noted, FISTA may fail the strict stability test even for $t \\le 1/L$ due to its non-monotonic nature.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing ISTA and FISTA stability for different step-sizes.\n    \"\"\"\n\n    def soft_threshold(z, gamma):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - gamma, 0)\n\n    def objective_function(x, A, b, lambda_reg):\n        \"\"\"Computes the objective function value F(x).\"\"\"\n        return 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_reg * np.linalg.norm(x, 1)\n\n    def ista(A, b, lambda_reg, t, K):\n        \"\"\"Iterative Shrinkage-Thresholding Algorithm.\"\"\"\n        n = A.shape[1]\n        x_k = np.zeros(n)\n        obj_vals = [objective_function(x_k, A, b, lambda_reg)]\n        for _ in range(K):\n            grad_x = A.T @ (A @ x_k - b)\n            x_k = soft_threshold(x_k - t * grad_x, t * lambda_reg)\n            obj_vals.append(objective_function(x_k, A, b, lambda_reg))\n        return obj_vals\n\n    def fista(A, b, lambda_reg, t, K):\n        \"\"\"Fast Iterative Shrinkage-Thresholding Algorithm.\"\"\"\n        n = A.shape[1]\n        x_k = np.zeros(n)\n        y_k = np.zeros(n)\n        theta_k = 1.0\n        obj_vals = [objective_function(x_k, A, b, lambda_reg)]\n        x_prev = x_k\n        for _ in range(K):\n            grad_y = A.T @ (A @ y_k - b)\n            x_k_new = soft_threshold(y_k - t * grad_y, t * lambda_reg)\n            \n            theta_k_plus_1 = (1.0 + np.sqrt(1.0 + 4.0 * theta_k**2)) / 2.0\n            y_k_new = x_k_new + ((theta_k - 1.0) / theta_k_plus_1) * (x_k_new - x_k)\n            \n            x_k = x_k_new\n            y_k = y_k_new\n            theta_k = theta_k_plus_1\n            obj_vals.append(objective_function(x_k, A, b, lambda_reg))\n        return obj_vals\n\n    def check_stability(obj_vals, K, tau):\n        \"\"\"Checks if the objective function sequence is non-increasing.\"\"\"\n        for k in range(K):\n            if obj_vals[k+1] > obj_vals[k] + tau:\n                return False\n        return True\n\n    m, n = 10, 10\n    x_star = np.zeros(n)\n    x_star[0] = 3.0\n    x_star[3] = -2.0\n    x_star[7] = 1.5\n    sigma = 0.01\n    lambda_reg = 0.1\n    K = 200\n    tau = 1e-12\n\n    test_cases = [\n        # Test Case 1: well-conditioned\n        ([1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7], 0),\n        # Test Case 2: ill-conditioned\n        ([3.0, 1.0, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001], 1),\n        # Test Case 3: rank-deficient\n        ([2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2)\n    ]\n\n    all_results = []\n\n    for s_vals, seed in test_cases:\n        np.random.seed(seed)\n        \n        # Matrix Construction from specified singular values\n        s_vals_np = np.array(s_vals)\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, s_vals_np)\n        \n        U, _ = np.linalg.qr(np.random.randn(m, m))\n        V, _ = np.linalg.qr(np.random.randn(n, n))\n        A = U @ Sigma @ V.T\n\n        # Data Generation\n        eta = np.random.normal(0, sigma, size=m)\n        b = A @ x_star + eta\n\n        # Compute true Lipschitz constant\n        L = np.max(s_vals_np)**2 if len(s_vals_np) > 0 else 0.0\n\n        # Define estimated Lipschitz constants to test\n        L_hat_factors = [1.0, 2.0, 0.5, 0.2]\n        \n        case_results = [L]\n\n        for factor in L_hat_factors:\n            L_hat = factor * L\n            \n            # A zero Lipschitz constant implies a zero matrix A (if b=0), or constant gradient\n            # step-size would be infinite. We consider this case unstable.\n            if L_hat < 1e-15:\n                ista_stable = False\n                fista_stable = False\n            else:\n                t = 1.0 / L_hat\n\n                # Run ISTA and check stability\n                ista_obj_vals = ista(A, b, lambda_reg, t, K)\n                ista_stable = check_stability(ista_obj_vals, K, tau)\n\n                # Run FISTA and check stability\n                fista_obj_vals = fista(A, b, lambda_reg, t, K)\n                fista_stable = check_stability(fista_obj_vals, K, tau)\n            \n            case_results.extend([ista_stable, fista_stable])\n\n        all_results.append(case_results)\n\n    # Format and print the final output as a single-line string representation of the list\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "While the previous exercise  demonstrated the importance of knowing the Lipschitz constant $L$, in many practical scenarios, computing it is prohibitively expensive or even impossible. This final practice addresses this challenge by implementing FISTA with a backtracking line search. This adaptive technique allows the algorithm to automatically find a suitable step size at each iteration by testing a condition derived from the Descent Lemma, ensuring convergence without prior knowledge of $L$ . Mastering this robust variant of FISTA is essential for applying it to a wide range of real-world problems where system parameters are not perfectly known.",
            "id": "3446933",
            "problem": "You are asked to implement a numerical method grounded in convex analysis to solve a sequence of synthetic sparse reconstruction instances. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\equiv g(x) + h(x),\n$$\nwhere the smooth part is\n$$\ng(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2\n$$\nwith gradient\n$$\n\\nabla g(x) = A^\\top (A x - b),\n$$\nand the nonsmooth regularizer is the scaled one-norm\n$$\nh(x) \\equiv \\lambda \\|x\\|_1.\n$$\nThe gradient of $g$ is globally Lipschitz continuous with some constant $L^\\star \\in (0,\\infty)$ satisfying\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L^\\star \\|x - y\\|_2\n$$\nfor all $x,y \\in \\mathbb{R}^n$. In this setting, the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with backtracking uses the following fundamental base:\n- The quadratic upper bound implied by Lipschitz continuity of $\\nabla g$ that justifies a sufficient decrease condition for accepting a candidate step size.\n- The proximal operator of the scaled one-norm,\n$$\n\\operatorname{prox}_{\\tau \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^n}\\left\\{\\tfrac{1}{2}\\|x - v\\|_2^2 + \\tau \\|x\\|_1\\right\\},\n$$\nwhich is the soft-thresholding map.\n- Nesterov’s acceleration sequence with $t_1 = 1$ and extrapolation built from $t_k$.\n\nYour task is to:\n- Start from the above base and derive the sufficient decrease condition for backtracking from the Lipschitz property of $\\nabla g$ to validate a local quadratic upper model of $g$ about the current extrapolated point.\n- Implement the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with backtracking for the LASSO objective $F(x)$, assuming the Lipschitz constant $L^\\star$ is unknown. Initialize with $x_0 = 0$, $y_1 = x_0$, and $t_1 = 1$. At each outer iteration $k \\in \\{1,2,\\dots\\}$, perform a backtracking line search that starts from a current estimate $L_k$ and multiplies $L_k$ by a factor $\\eta > 1$ until the sufficient decrease condition holds. Use the proximal map associated with $h$ to form the proximal-gradient candidate from the extrapolated point and the current $L_k$. Maintain the standard FISTA acceleration update using $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_k^2}}{2}$ and the usual extrapolation.\n- Let an “increase of $L_k$” be counted each time the backtracking line search multiplies $L_k$ by $\\eta$ before an iteration $k$ is accepted. Record the total number of such increases that occur during the first $10$ outer iterations (i.e., for $k \\in \\{1,\\dots,10\\}$).\n\nNo physical units are involved. All angles, if any, are to be assumed in radians. All answers must be real-valued numbers.\n\nTest suite and required output:\nImplement your program to run the following three synthetic test cases. In each case, form $b$ from a planted vector $x_{\\mathrm{true}}$ via $b = A x_{\\mathrm{true}}$ (no noise). For each case, run FISTA with backtracking for exactly $10$ outer iterations and report the total count of increases of $L_k$ that occurred across these $10$ iterations.\n\n- Case $1$ (happy path, moderate backtracking):\n  - $A \\in \\mathbb{R}^{6 \\times 10}$:\n    $$\n    A = \\begin{bmatrix}\n    1 & 0 & 0 & 0 & 0 & 0 & 0.5 & -0.2 & 0.3 & 0.1 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & -0.1 & 0.4 & 0.2 & -0.3 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0.3 & -0.4 & 0.1 & 0.2 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & -0.2 & 0.1 & 0.5 & -0.1 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0.4 & 0.3 & -0.2 & 0.2 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & -0.3 & 0.2 & 0.1 & 0.4\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{10}$ with entries\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0 & 1.2 & 0 & -0.7 & 0 & 0 & 0 & 0 & 2.0 & 0 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$, initial $L_1 = 0.1$, and backtracking factor $\\eta = 2.0$.\n\n- Case $2$ (boundary case, no backtracking needed):\n  - Same $A$ and $x_{\\mathrm{true}}$ as in Case $1$, with $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$, initial $L_1 = 100.0$, and backtracking factor $\\eta = 2.0$.\n\n- Case $3$ (edge case, many small increases due to a small $\\eta$ and larger operator norm):\n  - $A \\in \\mathbb{R}^{8 \\times 12}$:\n    $$\n    A = \\begin{bmatrix}\n    3.0 & -2.0 & 1.0 & 0.0 & 1.0 & -1.0 & 2.0 & -3.0 & 1.5 & -0.5 & 0.5 & -1.5 \\\\\n    0.0 & 1.0 & -1.0 & 2.0 & -2.0 & 1.0 & -1.5 & 2.5 & -1.0 & 0.5 & -0.5 & 1.0 \\\\\n    1.5 & -1.0 & 0.5 & -0.5 & 1.0 & -1.5 & 2.0 & -2.0 & 1.0 & -1.0 & 0.5 & -0.5 \\\\\n    -1.0 & 2.0 & -2.0 & 1.0 & 0.0 & 1.0 & -2.5 & 3.0 & -1.5 & 1.0 & -1.0 & 0.5 \\\\\n    2.0 & -1.0 & 1.5 & -1.0 & 2.0 & -2.0 & 1.0 & -1.0 & 0.5 & -0.5 & 1.0 & -1.5 \\\\\n    -2.0 & 1.0 & -0.5 & 1.5 & -1.0 & 2.0 & -1.0 & 1.0 & -0.5 & 0.5 & -1.0 & 1.0 \\\\\n    1.0 & 0.5 & -1.5 & 2.0 & -2.5 & 1.5 & -1.0 & 2.0 & -1.0 & 0.5 & -0.5 & 1.0 \\\\\n    -1.5 & 2.5 & -1.0 & 0.5 & 1.0 & -1.0 & 2.0 & -3.0 & 1.5 & -1.0 & 1.0 & -0.5\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{12}$ with entries\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0 & 0 & 2.0 & -1.0 & 0 & 0 & 0 & 3.0 & 0 & 0 & 0 & 0.5 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.1$, initial $L_1 = 1.0$, and backtracking factor $\\eta = 1.1$.\n\nFor each case, run exactly $10$ outer iterations of FISTA with backtracking and count the total number of times that $L_k$ is multiplied by $\\eta$ across these $10$ iterations. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$). Each result must be an integer representing the count for its corresponding test case. No additional text should be printed.",
            "solution": "The problem requires the implementation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with a backtracking line search to solve the LASSO optimization problem. The primary task is to count the number of times the Lipschitz constant estimate is increased during the backtracking procedure over a fixed number of iterations.\n\nThe LASSO objective function is given by $F(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ is the smooth data fidelity term and $h(x) = \\lambda \\|x\\|_1$ is the non-smooth sparsity-inducing regularizer.\n\nFirst, we derive the sufficient decrease condition used in the backtracking line search. The gradient of the smooth term is $\\nabla g(x) = A^\\top (A x - b)$. It is given that $\\nabla g$ is Lipschitz continuous with constant $L^\\star$, which implies the following inequality for any $L \\ge L^\\star$:\n$$\ng(z) \\le g(y) + \\langle \\nabla g(y), z - y \\rangle + \\frac{L}{2} \\|z - y\\|_2^2\n$$\nThis inequality, known as the Descent Lemma, provides a quadratic upper bound for the function $g$ at a point $z$ around a point $y$.\n\nFISTA is a proximal gradient method that combines a standard proximal gradient step with a Nesterov-style acceleration. At each iteration $k$, given an extrapolated point $y_k$, the algorithm seeks to find a new point $x_k$ by minimizing a quadratic approximation of $F(x)$ around $y_k$. This minimization leads to the proximal gradient update:\n$$\nx_k = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))\n$$\nwhere $L$ is the Lipschitz constant estimate and $\\operatorname{prox}_{\\tau h}(\\cdot)$ is the proximal operator of the function $\\tau h$. For $h(x) = \\lambda \\|x\\|_1$, the proximal step involves the soft-thresholding operator, $S_{\\tau}(v)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$, such that:\n$$\nx_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b)\\right)\n$$\nSince the true Lipschitz constant $L^\\star$ is unknown, a backtracking line search is employed to find a suitable step size $1/L_k$ at each iteration $k$. Starting with an estimate for $L_k$ (e.g., the value from the previous iteration, $L_{k-1}$), we check if the quadratic upper bound holds for our candidate point $x_k$. The sufficient decrease condition is derived directly from the Descent Lemma by substituting $z=x_k$ and $y=y_k$. We require our chosen $L_k$ to satisfy:\n$$\ng(x_k) \\le g(y_k) + \\langle \\nabla g(y_k), x_k - y_k \\rangle + \\frac{L_k}{2} \\|x_k - y_k\\|_2^2\n$$\nIf this condition is not met, the estimate $L_k$ is too small. We increase it by a factor $\\eta > 1$ (i.e., $L_k \\leftarrow \\eta L_k$), recompute the candidate $x_k$ with the new $L_k$, and check the condition again. This process is repeated until the condition is satisfied. Each multiplication by $\\eta$ constitutes an \"increase\" that we must count.\n\nThe complete FISTA with backtracking algorithm proceeds as follows:\n\n1.  **Initialization**: Given an initial guess for the Lipschitz constant $L_0$, backtracking factor $\\eta > 1$. Set $x_0 = 0$, $y_1 = x_0$, and $t_1 = 1$. Let the total count of increases be $C = 0$.\n\n2.  **Iteration**: For $k = 1, 2, \\ldots, 10$:\n    a. **Backtracking Line Search**:\n        i. Start with a trial Lipschitz constant, e.g., $L_{trial} = L_{k-1}$ (or $L_0$ for $k=1$).\n        ii. Compute the candidate point $x_{k, trial} = S_{\\lambda/L_{trial}}\\left(y_k - \\frac{1}{L_{trial}}\\nabla g(y_k)\\right)$.\n        iii. Check the sufficient decrease condition: if $g(x_{k, trial}) > g(y_k) + \\langle \\nabla g(y_k), x_{k, trial} - y_k \\rangle + \\frac{L_{trial}}{2} \\|x_{k, trial} - y_k\\|_2^2$, then update $L_{trial} \\leftarrow \\eta L_{trial}$, increment the counter $C \\leftarrow C+1$, and go back to step (ii).\n        iv. Once the condition is satisfied, set $L_k = L_{trial}$ and $x_k = x_{k, trial}$.\n\n    b. **Acceleration Step**: Update the momentum terms:\n        i. $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$.\n        ii. $y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1})$.\n\n3.  After $10$ iterations, the final value of the counter $C$ is the result for the given test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs FISTA with backtracking on a suite of test cases and reports the\n    total number of backtracking steps (increases of L).\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 0.1,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 100.0,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A\": np.array([\n                [3.0, -2.0, 1.0, 0.0, 1.0, -1.0, 2.0, -3.0, 1.5, -0.5, 0.5, -1.5],\n                [0.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.5, 2.5, -1.0, 0.5, -0.5, 1.0],\n                [1.5, -1.0, 0.5, -0.5, 1.0, -1.5, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5],\n                [-1.0, 2.0, -2.0, 1.0, 0.0, 1.0, -2.5, 3.0, -1.5, 1.0, -1.0, 0.5],\n                [2.0, -1.0, 1.5, -1.0, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5, 1.0, -1.5],\n                [-2.0, 1.0, -0.5, 1.5, -1.0, 2.0, -1.0, 1.0, -0.5, 0.5, -1.0, 1.0],\n                [1.0, 0.5, -1.5, 2.0, -2.5, 1.5, -1.0, 2.0, -1.0, 0.5, -0.5, 1.0],\n                [-1.5, 2.5, -1.0, 0.5, 1.0, -1.0, 2.0, -3.0, 1.5, -1.0, 1.0, -0.5]\n            ]),\n            \"x_true\": np.array([0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.5]),\n            \"lam\": 0.1,\n            \"L_initial\": 1.0,\n            \"eta\": 1.1\n        }\n    ]\n\n    results = []\n\n    def soft_threshold(v, tau):\n        return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        x_true = case[\"x_true\"]\n        lam = case[\"lam\"]\n        L_k = case[\"L_initial\"]\n        eta = case[\"eta\"]\n        n_iters = 10\n\n        b = A @ x_true\n        n = A.shape[1]\n        \n        x_k_prev = np.zeros(n)\n        x_k = np.zeros(n)\n        t_k = 1.0\n        \n        total_increases = 0\n\n        for k in range(1, n_iters + 1):\n            y_k = x_k + ((t_k - 1) / ( (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0 )) * (x_k - x_k_prev)\n\n            L_trial = L_k \n            while True:\n                grad_g_y = A.T @ (A @ y_k - b)\n                x_candidate = soft_threshold(y_k - (1.0 / L_trial) * grad_g_y, lam / L_trial)\n\n                g_x = 0.5 * np.linalg.norm(A @ x_candidate - b)**2\n                g_y = 0.5 * np.linalg.norm(A @ y_k - b)**2\n                rhs = g_y + np.dot(grad_g_y, x_candidate - y_k) + \\\n                      (L_trial / 2.0) * np.linalg.norm(x_candidate - y_k)**2\n\n                if g_x = rhs:\n                    L_k = L_trial\n                    break\n                else:\n                    L_trial *= eta\n                    total_increases += 1\n            \n            x_k_prev = x_k\n            x_k = x_candidate\n            \n            t_k_next = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n            t_k = t_k_next\n\n        results.append(total_increases)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}