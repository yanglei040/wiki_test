{
    "hands_on_practices": [
        {
            "introduction": "The convergence of proximal gradient methods like FISTA is critically dependent on the choice of step-size. Theory dictates that for the objective $F(x) = f(x) + g(x)$, the step-size $t$ must satisfy $t \\le 1/L$, where $L$ is the Lipschitz constant of the gradient of the smooth component, $\\nabla f$. This exercise provides a foundational, hands-on exploration of this principle. You will first calculate the exact Lipschitz constant for a quadratic loss function from the singular values of the data matrix, and then simulate both ISTA and FISTA to observe the consequences of adhering to, or violating, this fundamental step-size constraint . This practice is essential for building an intuition about algorithmic stability and the practical importance of parameter tuning.",
            "id": "3446919",
            "problem": "Consider the sparse optimization problem in compressed sensing with the composite objective consisting of a smooth quadratic data-fit term and a non-smooth sparsity-promoting term. Let the smooth part be defined by the matrix-vector map and its gradient, and let the non-smooth part be the entry-wise absolute-value penalty. The Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) are proximal gradient methods whose step-size choice depends on the Lipschitz constant of the gradient of the smooth part. Your task is to compute the Lipschitz constant from specified singular values and then simulate the effect of underestimating or overestimating this constant on the stability of ISTA and FISTA steps.\n\nStart from the following fundamental base:\n- The smooth part is the quadratic function $f(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2$ and its gradient $\\nabla f(x)$ exists for all $x$.\n- The composite objective is $F(x) = f(x) + \\lambda \\lVert x \\rVert_1$, where $\\lambda > 0$ is a scalar.\n- The proximal gradient methodology ensures that a step-size $t$ at most the reciprocal of the Lipschitz constant of $\\nabla f$ guarantees descent properties for ISTA on $F(x)$.\n\nYou must:\n- Compute the Lipschitz constant $L$ of $\\nabla f$ using only the provided singular values of $A$, based on foundational facts about the gradient of a quadratic form and the relationship between singular values and operator norms.\n- Construct a matrix $A$ of size $m \\times n$ with the specified singular values using orthonormal factors, and generate a sparse ground truth $x^\\star$ and observation vector $b$ with small additive noise.\n- Implement the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) from first principles as proximal gradient methods.\n- For a given estimated Lipschitz constant $\\widehat{L}$, use the step-size $t = 1 / \\widehat{L}$ in both ISTA and FISTA.\n- Define a stability indicator for each algorithm as follows: the sequence of objective values $\\{F(x_k)\\}_{k=0}^{K}$ is stable if it is non-increasing up to a numerical tolerance $\\tau$, meaning $F(x_{k+1}) \\leq F(x_k) + \\tau$ for all $k \\in \\{0, 1, \\dots, K-1\\}$, where $\\tau = 10^{-12}$.\n- Analyze the impact of step-size choices corresponding to exact, overestimated, and underestimated $\\widehat{L}$ values on stability for ISTA and FISTA.\n\nMatrix construction and data generation details:\n- Use $m = 10$ and $n = 10$ for all test cases. Construct $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with the specified singular values on its diagonal. Generate $U$ and $V$ by orthonormalizing pseudorandom Gaussian matrices with fixed seeds.\n- Use the sparse ground truth $x^\\star \\in \\mathbb{R}^{n}$ with nonzero entries at indices $0$, $3$, and $7$, with values $3.0$, $-2.0$, and $1.5$, respectively, and zeros elsewhere. That is, $x^\\star = [3.0, 0.0, 0.0, -2.0, 0.0, 0.0, 0.0, 1.5, 0.0, 0.0]$.\n- Generate $b = A x^\\star + \\eta$, where $\\eta \\in \\mathbb{R}^{m}$ is Gaussian noise with mean $0$ and standard deviation $\\sigma = 0.01$.\n- Use the regularization parameter $\\lambda = 0.1$.\n\nAlgorithms to implement:\n- Implement ISTA and FISTA for the objective $F(x) = \\tfrac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$, with initialization $x_0 = 0$ for ISTA and $x_0 = y_0 = 0$ for FISTA, and a maximum number of iterations $K = 200$ for both. For FISTA, use the standard acceleration sequence for the momentum parameter.\n\nTest suite:\nProvide the following three test cases, each with a specified list of singular values for $A$, a pseudorandom seed for reproducibility, and evaluate four choices of $\\widehat{L}$: exact $\\widehat{L} = L$, overestimate $\\widehat{L} = 2 L$, and underestimates $\\widehat{L} = 0.5 L$ and $\\widehat{L} = 0.2 L$.\n\n- Test Case $1$ (well-conditioned):\n  - Singular values: $[1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7]$.\n  - Seed for matrix construction and noise: $0$.\n- Test Case $2$ (ill-conditioned):\n  - Singular values: $[3.0, 1.0, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001]$.\n  - Seed for matrix construction and noise: $1$.\n- Test Case $3$ (rank-deficient):\n  - Singular values: $[2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]$.\n  - Seed for matrix construction and noise: $2$.\n\nFor each test case, compute $L$ from the singular values and, for each choice of $\\widehat{L} \\in \\{L, 2 L, 0.5 L, 0.2 L\\}$, run ISTA and FISTA with $t = 1 / \\widehat{L}$. For each run, compute the objective sequence $\\{F(x_k)\\}_{k=0}^{K}$ and the stability indicator defined above.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for the three test cases. Each test case result must be a list of nine entries:\n$[L,\\ \\text{ISTA\\_stable\\_exact},\\ \\text{FISTA\\_stable\\_exact},\\ \\text{ISTA\\_stable\\_over},\\ \\text{FISTA\\_stable\\_over},\\ \\text{ISTA\\_stable\\_under0.5},\\ \\text{FISTA\\_stable\\_under0.5},\\ \\text{ISTA\\_stable\\_under0.2},\\ \\text{FISTA\\_stable\\_under0.2}]$,\nwhere $L$ is a floating-point number and each stability indicator is a boolean. The program must print the list of the three per-test-case lists as a comma-separated list enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots]]$.",
            "solution": "The task is to analyze the stability of two proximal gradient methods, the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), when applied to a sparse optimization problem. The stability is evaluated with respect to the choice of step-size, which is determined by an estimate $\\widehat{L}$ of the Lipschitz constant $L$ of the gradient of the problem's smooth component.\n\nThe composite objective function is given by $F(x) = f(x) + g(x)$, where $f(x)$ is the smooth data-fidelity term and $g(x)$ is the non-smooth regularization term.\n- The smooth part is the least-squares cost function: $f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2$, for a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector of observations $b \\in \\mathbb{R}^{m}$, and the variable $x \\in \\mathbb{R}^{n}$.\n- The non-smooth part is the $\\ell_1$-norm, which promotes sparsity: $g(x) = \\lambda \\lVert x \\rVert_1$, with a regularization parameter $\\lambda > 0$.\n\nThe solution proceeds in three stages: first, we derive the formula for the Lipschitz constant $L$; second, we specify the ISTA and FISTA algorithms; and third, we outline the numerical experiment to test stability.\n\n**1. Derivation of the Lipschitz Constant**\n\nThe convergence of proximal gradient methods like ISTA and FISTA relies on a proper step-size selection, which is bounded by the reciprocal of the Lipschitz constant of $\\nabla f$. A function $\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n$ is Lipschitz continuous with constant $L$ if, for all $x, y \\in \\mathbb{R}^n$:\n$$ \\lVert \\nabla f(x) - \\nabla f(y) \\rVert_2 \\leq L \\lVert x - y \\rVert_2 $$\nThe smallest such $L$ is the Lipschitz constant.\n\nFirst, we compute the gradient of $f(x)$:\n$$ f(x) = \\frac{1}{2}(Ax-b)^\\top(Ax-b) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b) $$\nThe gradient with respect to $x$ is:\n$$ \\nabla f(x) = A^\\top A x - A^\\top b = A^\\top(Ax-b) $$\nNext, we evaluate the difference $\\nabla f(x) - \\nabla f(y)$:\n$$ \\nabla f(x) - \\nabla f(y) = (A^\\top A x - A^\\top b) - (A^\\top A y - A^\\top b) = A^\\top A (x - y) $$\nSubstituting this into the Lipschitz condition:\n$$ \\lVert A^\\top A (x - y) \\rVert_2 \\leq L \\lVert x - y \\rVert_2 $$\nThis inequality must hold for all $x, y$. This is equivalent to finding the operator norm (or spectral norm) of the matrix $A^\\top A$:\n$$ L = \\lVert A^\\top A \\rVert_2 = \\sup_{z \\neq 0} \\frac{\\lVert A^\\top A z \\rVert_2}{\\lVert z \\rVert_2} $$\nThe operator norm of a symmetric positive semi-definite matrix like $A^\\top A$ is its largest eigenvalue. Let the singular values of $A$ be $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_{\\min(m,n)} \\geq 0$. The eigenvalues of $A^\\top A$ are the squares of the singular values of $A$. Thus, the largest eigenvalue of $A^\\top A$ is the square of the largest singular value of $A$, denoted $\\sigma_{\\max}(A)$.\nTherefore, the Lipschitz constant is:\n$$ L = \\sigma_{\\max}(A)^2 $$\nFor each test case, $L$ will be computed by taking the maximum value from the provided list of singular values and squaring it.\n\n**2. ISTA and FISTA Algorithms**\n\nBoth ISTA and FISTA are instances of the proximal gradient method, which generates a sequence of iterates $\\{x_k\\}$ that converges to a minimizer of $F(x)$. The core of the method is the proximal operator. For $g(x) = \\lambda \\lVert x \\rVert_1$, the proximal operator is the soft-thresholding function, $S_{\\gamma}(z)$:\n$$ \\text{prox}_{\\gamma g}(z) = S_{\\gamma}(z) $$\nwhere $(S_{\\gamma}(z))_i = \\text{sign}(z_i) \\max(|z_i| - \\gamma, 0)$ for each component $i$ of the vector $z$.\n\nThe general update step involves a gradient descent step on the smooth part $f(x)$ followed by a proximal step on the non-smooth part $g(x)$. Both algorithms use a fixed step-size $t = 1/\\widehat{L}$, where $\\widehat{L}$ is an estimate of the true Lipschitz constant $L$.\n\n**Iterative Shrinkage-Thresholding Algorithm (ISTA)**\nWith an initial guess $x_0$, the ISTA update rule is:\n$$ x_{k+1} = S_{t\\lambda}(x_k - t \\nabla f(x_k)) = S_{t\\lambda}(x_k - t A^\\top(Ax_k - b)) $$\nFor a step-size $t \\leq 1/L$, ISTA is guaranteed to be a descent algorithm, i.e., $F(x_{k+1}) \\leq F(x_k)$.\n\n**Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)**\nFISTA accelerates ISTA by incorporating a momentum term. It uses an auxiliary sequence $\\{y_k\\}$ and a momentum parameter sequence $\\{\\theta_k\\}$.\nWith initializations $x_0 = y_0$ and $\\theta_0=1$:\n\\begin{align*}\nx_{k+1} &= S_{t\\lambda}(y_k - t \\nabla f(y_k)) = S_{t\\lambda}(y_k - t A^\\top(Ay_k - b)) \\\\\n\\theta_{k+1} &= \\frac{1 + \\sqrt{1 + 4\\theta_k^2}}{2} \\\\\ny_{k+1} &= x_{k+1} + \\frac{\\theta_k - 1}{\\theta_{k+1}}(x_{k+1} - x_k)\n\\end{align*}\nFISTA achieves a faster convergence rate of $O(1/k^2)$ for the objective value error, provided that $t \\leq 1/L$. However, unlike ISTA, FISTA is not a descent method; the sequence of objective values $\\{F(x_k)\\}$ is not guaranteed to be non-increasing, even with a valid step-size. Oscillations can occur, though the overall trend is descent towards the minimum.\n\n**3. Simulation and Stability Analysis**\n\nThe problem requires constructing a matrix $A \\in \\mathbb{R}^{10 \\times 10}$ with specified singular values. This is done by forming the Singular Value Decomposition $A = U \\Sigma V^\\top$, where $\\Sigma$ is the diagonal matrix of specified singular values, and $U, V$ are random orthonormal matrices generated via QR decomposition of Gaussian random matrices. A sparse ground truth vector $x^\\star$ is defined, and the measurement vector $b$ is generated as $b = A x^\\star + \\eta$, where $\\eta$ is a small Gaussian noise vector.\n\nFor each of the three test cases (well-conditioned, ill-conditioned, and rank-deficient), we perform the following analysis:\n1. Compute the true Lipschitz constant $L = \\sigma_{\\max}(A)^2$.\n2. For each estimate $\\widehat{L} \\in \\{L, 2L, 0.5L, 0.2L\\}$, set the step-size $t = 1/\\widehat{L}$.\n3. Run ISTA and FISTA for $K=200$ iterations starting from $x_0 = 0$ (and $y_0 = 0$ for FISTA).\n4. For each run, record the sequence of objective function values $\\{F(x_k)\\}_{k=0}^{K}$.\n5. Evaluate the stability of this sequence using the provided criterion: the sequence is stable if $F(x_{k+1}) \\leq F(x_k) + \\tau$ for all $k \\in \\{0, 1, \\dots, K-1\\}$, where $\\tau = 10^{-12}$.\n\nThe expected outcome is that for $\\widehat{L} \\geq L$ (i.e., $t \\leq 1/L$), the algorithms will be stable, as this satisfies the theoretical condition for convergence. For $\\widehat{L} < L$ (i.e., $t > 1/L$), the step-size is too large, violating the convergence condition, and the algorithms are expected to become unstable, exhibiting either oscillatory or divergent behavior in the objective function values. As noted, FISTA may fail the strict stability test even for $t \\le 1/L$ due to its non-monotonic nature.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing ISTA and FISTA stability for different step-sizes.\n    \"\"\"\n\n    def soft_threshold(z, gamma):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - gamma, 0)\n\n    def objective_function(x, A, b, lambda_reg):\n        \"\"\"Computes the objective function value F(x).\"\"\"\n        return 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_reg * np.linalg.norm(x, 1)\n\n    def ista(A, b, lambda_reg, t, K):\n        \"\"\"Iterative Shrinkage-Thresholding Algorithm.\"\"\"\n        n = A.shape[1]\n        x_k = np.zeros(n)\n        obj_vals = [objective_function(x_k, A, b, lambda_reg)]\n        for _ in range(K):\n            grad_x = A.T @ (A @ x_k - b)\n            x_k = soft_threshold(x_k - t * grad_x, t * lambda_reg)\n            obj_vals.append(objective_function(x_k, A, b, lambda_reg))\n        return obj_vals\n\n    def fista(A, b, lambda_reg, t, K):\n        \"\"\"Fast Iterative Shrinkage-Thresholding Algorithm.\"\"\"\n        n = A.shape[1]\n        x_k = np.zeros(n)\n        y_k = np.zeros(n)\n        theta_k = 1.0\n        obj_vals = [objective_function(x_k, A, b, lambda_reg)]\n        for _ in range(K):\n            x_prev = x_k\n            grad_y = A.T @ (A @ y_k - b)\n            x_k = soft_threshold(y_k - t * grad_y, t * lambda_reg)\n            \n            theta_k_plus_1 = (1.0 + np.sqrt(1.0 + 4.0 * theta_k**2)) / 2.0\n            y_k = x_k + ((theta_k - 1.0) / theta_k_plus_1) * (x_k - x_prev)\n            \n            theta_k = theta_k_plus_1\n            obj_vals.append(objective_function(x_k, A, b, lambda_reg))\n        return obj_vals\n\n    def check_stability(obj_vals, K, tau):\n        \"\"\"Checks if the objective function sequence is non-increasing.\"\"\"\n        for k in range(K):\n            if obj_vals[k+1] > obj_vals[k] + tau:\n                return False\n        return True\n\n    m, n = 10, 10\n    x_star = np.zeros(n)\n    x_star[0] = 3.0\n    x_star[3] = -2.0\n    x_star[7] = 1.5\n    sigma = 0.01\n    lambda_reg = 0.1\n    K = 200\n    tau = 1e-12\n\n    test_cases = [\n        # Test Case 1: well-conditioned\n        ([1.6, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7], 0),\n        # Test Case 2: ill-conditioned\n        ([3.0, 1.0, 0.3, 0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001], 1),\n        # Test Case 3: rank-deficient\n        ([2.0, 1.0, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2)\n    ]\n\n    all_results = []\n\n    for s_vals, seed in test_cases:\n        np.random.seed(seed)\n        \n        # Matrix Construction from specified singular values\n        s_vals_np = np.array(s_vals)\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, s_vals_np)\n        \n        U, _ = np.linalg.qr(np.random.randn(m, m))\n        V, _ = np.linalg.qr(np.random.randn(n, n))\n        A = U @ Sigma @ V.T\n\n        # Data Generation\n        eta = np.random.normal(0, sigma, size=m)\n        b = A @ x_star + eta\n\n        # Compute true Lipschitz constant\n        L = np.max(s_vals_np)**2 if len(s_vals_np) > 0 else 0.0\n\n        # Define estimated Lipschitz constants to test\n        L_hat_factors = [1.0, 2.0, 0.5, 0.2]\n        \n        case_results = [L]\n\n        for factor in L_hat_factors:\n            L_hat = factor * L\n            \n            # A zero Lipschitz constant implies a zero matrix A (if b=0), or constant gradient\n            # step-size would be infinite. We consider this case unstable.\n            if L_hat < 1e-15:\n                ista_stable = False\n                fista_stable = False\n            else:\n                t = 1.0 / L_hat\n\n                # Run ISTA and check stability\n                ista_obj_vals = ista(A, b, lambda_reg, t, K)\n                ista_stable = check_stability(ista_obj_vals, K, tau)\n\n                # Run FISTA and check stability\n                fista_obj_vals = fista(A, b, lambda_reg, t, K)\n                fista_stable = check_stability(fista_obj_vals, K, tau)\n            \n            case_results.extend([ista_stable, fista_stable])\n\n        all_results.append(case_results)\n\n    # Format and print the final output as a single-line string representation of the list\n    print(str(all_results))\n\nsolve()\n```"
        },
        {
            "introduction": "While knowing the Lipschitz constant $L$ is crucial for setting a valid step-size, computing it directly via the spectral norm can be prohibitively expensive for large-scale problems. A more practical and robust approach is to estimate the step-size adaptively using a backtracking line search. This exercise guides you through the implementation of FISTA with backtracking, a technique that starts with an optimistic step-size and iteratively refines it until a sufficient decrease condition, derived from the properties of Lipschitz gradients, is met . By tracking how the algorithm adjusts its internal estimate of $L$, you will gain a deeper understanding of how FISTA can be made both efficient and reliable without prior knowledge of the problem's global properties.",
            "id": "3446933",
            "problem": "You are asked to implement a numerical method grounded in convex analysis to solve a sequence of synthetic sparse reconstruction instances. Consider the Least Absolute Shrinkage and Selection Operator (LASSO) objective\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\equiv g(x) + h(x),\n$$\nwhere the smooth part is\n$$\ng(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2\n$$\nwith gradient\n$$\n\\nabla g(x) = A^\\top (A x - b),\n$$\nand the nonsmooth regularizer is the scaled one-norm\n$$\nh(x) \\equiv \\lambda \\|x\\|_1.\n$$\nThe gradient of $g$ is globally Lipschitz continuous with some constant $L^\\star \\in (0,\\infty)$ satisfying\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L^\\star \\|x - y\\|_2\n$$\nfor all $x,y \\in \\mathbb{R}^n$. In this setting, the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with backtracking uses the following fundamental base:\n- The quadratic upper bound implied by Lipschitz continuity of $\\nabla g$ that justifies a sufficient decrease condition for accepting a candidate step size.\n- The proximal operator of the scaled one-norm,\n$$\n\\operatorname{prox}_{\\tau \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^n}\\left\\{\\tfrac{1}{2}\\|x - v\\|_2^2 + \\tau \\|x\\|_1\\right\\},\n$$\nwhich is the soft-thresholding map.\n- Nesterov’s acceleration sequence with $t_1 = 1$ and extrapolation built from $t_k$.\n\nYour task is to:\n- Start from the above base and derive the sufficient decrease condition for backtracking from the Lipschitz property of $\\nabla g$ to validate a local quadratic upper model of $g$ about the current extrapolated point.\n- Implement the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with backtracking for the LASSO objective $F(x)$, assuming the Lipschitz constant $L^\\star$ is unknown. Initialize with $x_1 = 0$, $y_1 = x_1$, and $t_1 = 1$. At each outer iteration $k \\in \\{1,2,\\dots\\}$, perform a backtracking line search that starts from a current estimate $L_k$ and multiplies $L_k$ by a factor $\\eta > 1$ until the sufficient decrease condition holds. Use the proximal map associated with $h$ to form the proximal-gradient candidate from the extrapolated point and the current $L_k$. Maintain the standard FISTA acceleration update using $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_k^2}}{2}$ and the usual extrapolation.\n- Let an “increase of $L_k$” be counted each time the backtracking line search multiplies $L_k$ by $\\eta$ before an iteration $k$ is accepted. Record the total number of such increases that occur during the first $10$ outer iterations (i.e., for $k \\in \\{1,\\dots,10\\}$).\n\nNo physical units are involved. All angles, if any, are to be assumed in radians. All answers must be real-valued numbers.\n\nTest suite and required output:\nImplement your program to run the following three synthetic test cases. In each case, form $b$ from a planted vector $x_{\\mathrm{true}}$ via $b = A x_{\\mathrm{true}}$ (no noise). For each case, run FISTA with backtracking for exactly $10$ outer iterations and report the total count of increases of $L_k$ that occurred across these $10$ iterations.\n\n- Case $1$ (happy path, moderate backtracking):\n  - $A \\in \\mathbb{R}^{6 \\times 10}$:\n    $$\n    A = \\begin{bmatrix}\n    1 & 0 & 0 & 0 & 0 & 0 & 0.5 & -0.2 & 0.3 & 0.1 \\\\\n    0 & 1 & 0 & 0 & 0 & 0 & -0.1 & 0.4 & 0.2 & -0.3 \\\\\n    0 & 0 & 1 & 0 & 0 & 0 & 0.3 & -0.4 & 0.1 & 0.2 \\\\\n    0 & 0 & 0 & 1 & 0 & 0 & -0.2 & 0.1 & 0.5 & -0.1 \\\\\n    0 & 0 & 0 & 0 & 1 & 0 & 0.4 & 0.3 & -0.2 & 0.2 \\\\\n    0 & 0 & 0 & 0 & 0 & 1 & -0.3 & 0.2 & 0.1 & 0.4\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{10}$ with entries\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0 & 1.2 & 0 & -0.7 & 0 & 0 & 0 & 0 & 2.0 & 0 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$, initial $L_1 = 0.1$, and backtracking factor $\\eta = 2.0$.\n\n- Case $2$ (boundary case, no backtracking needed):\n  - Same $A$ and $x_{\\mathrm{true}}$ as in Case $1$, with $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.05$, initial $L_1 = 100.0$, and backtracking factor $\\eta = 2.0$.\n\n- Case $3$ (edge case, many small increases due to a small $\\eta$ and larger operator norm):\n  - $A \\in \\mathbb{R}^{8 \\times 12}$:\n    $$\n    A = \\begin{bmatrix}\n    3.0 & -2.0 & 1.0 & 0.0 & 1.0 & -1.0 & 2.0 & -3.0 & 1.5 & -0.5 & 0.5 & -1.5 \\\\\n    0.0 & 1.0 & -1.0 & 2.0 & -2.0 & 1.0 & -1.5 & 2.5 & -1.0 & 0.5 & -0.5 & 1.0 \\\\\n    1.5 & -1.0 & 0.5 & -0.5 & 1.0 & -1.5 & 2.0 & -2.0 & 1.0 & -1.0 & 0.5 & -0.5 \\\\\n    -1.0 & 2.0 & -2.0 & 1.0 & 0.0 & 1.0 & -2.5 & 3.0 & -1.5 & 1.0 & -1.0 & 0.5 \\\\\n    2.0 & -1.0 & 1.5 & -1.0 & 2.0 & -2.0 & 1.0 & -1.0 & 0.5 & -0.5 & 1.0 & -1.5 \\\\\n    -2.0 & 1.0 & -0.5 & 1.5 & -1.0 & 2.0 & -1.0 & 1.0 & -0.5 & 0.5 & -1.0 & 1.0 \\\\\n    1.0 & 0.5 & -1.5 & 2.0 & -2.5 & 1.5 & -1.0 & 2.0 & -1.0 & 0.5 & -0.5 & 1.0 \\\\\n    -1.5 & 2.5 & -1.0 & 0.5 & 1.0 & -1.0 & 2.0 & -3.0 & 1.5 & -1.0 & 1.0 & -0.5\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{12}$ with entries\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0 & 0 & 2.0 & -1.0 & 0 & 0 & 0 & 3.0 & 0 & 0 & 0 & 0.5 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$.\n  - $\\lambda = 0.1$, initial $L_1 = 1.0$, and backtracking factor $\\eta = 1.1$.\n\nFor each case, run exactly $10$ outer iterations of FISTA with backtracking and count the total number of times that $L_k$ is multiplied by $\\eta$ across these $10$ iterations. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$). Each result must be an integer representing the count for its corresponding test case. No additional text should be printed.",
            "solution": "The problem requires the implementation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with a backtracking line search to solve the LASSO optimization problem. The primary task is to count the number of times the Lipschitz constant estimate is increased during the backtracking procedure over a fixed number of iterations.\n\nThe LASSO objective function is given by $F(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ is the smooth data fidelity term and $h(x) = \\lambda \\|x\\|_1$ is the non-smooth sparsity-inducing regularizer.\n\nFirst, we derive the sufficient decrease condition used in the backtracking line search. The gradient of the smooth term is $\\nabla g(x) = A^\\top (A x - b)$. It is given that $\\nabla g$ is Lipschitz continuous with constant $L^\\star$, which implies the following inequality for any $L \\ge L^\\star$:\n$$\ng(z) \\le g(y) + \\langle \\nabla g(y), z - y \\rangle + \\frac{L}{2} \\|z - y\\|_2^2\n$$\nThis inequality, known as the Descent Lemma, provides a quadratic upper bound for the function $g$ at a point $z$ around a point $y$.\n\nFISTA is a proximal gradient method that combines a standard proximal gradient step with a Nesterov-style acceleration. At each iteration $k$, given an extrapolated point $y_k$, the algorithm seeks to find a new point $x_k$ by minimizing a quadratic approximation of $F(x)$ around $y_k$. This minimization leads to the proximal gradient update:\n$$\nx_k = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))\n$$\nwhere $L$ is the Lipschitz constant estimate and $\\operatorname{prox}_{\\tau h}(\\cdot)$ is the proximal operator of the function $\\tau h$. For $h(x) = \\lambda \\|x\\|_1$, the proximal step involves the soft-thresholding operator, $S_{\\tau}(v)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$, such that:\n$$\nx_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b)\\right)\n$$\nSince the true Lipschitz constant $L^\\star$ is unknown, a backtracking line search is employed to find a suitable step size $1/L_k$ at each iteration $k$. Starting with an estimate for $L_k$ (e.g., the value from the previous iteration, $L_{k-1}$), we check if the quadratic upper bound holds for our candidate point $x_k$. The sufficient decrease condition is derived directly from the Descent Lemma by substituting $z=x_k$ and $y=y_k$. We require our chosen $L_k$ to satisfy:\n$$\ng(x_k) \\le g(y_k) + \\langle \\nabla g(y_k), x_k - y_k \\rangle + \\frac{L_k}{2} \\|x_k - y_k\\|_2^2\n$$\nIf this condition is not met, the estimate $L_k$ is too small. We increase it by a factor $\\eta > 1$ (i.e., $L_k \\leftarrow \\eta L_k$), recompute the candidate $x_k$ with the new $L_k$, and check the condition again. This process is repeated until the condition is satisfied. Each multiplication by $\\eta$ constitutes an \"increase\" that we must count.\n\nThe complete FISTA with backtracking algorithm proceeds as follows:\n\n1.  **Initialization**: Given an initial guess for the Lipschitz constant $L_0$, backtracking factor $\\eta > 1$. Set $x_0 = 0$, $y_1 = x_0$, and $t_1 = 1$. Let the total count of increases be $C = 0$.\n\n2.  **Iteration**: For $k = 1, 2, \\ldots, 10$:\n    a. **Backtracking Line Search**:\n        i. Start with a trial Lipschitz constant, e.g., $L_{trial} = L_{k-1}$.\n        ii. Compute the candidate point $x_{k, trial} = S_{\\lambda/L_{trial}}\\left(y_k - \\frac{1}{L_{trial}}\\nabla g(y_k)\\right)$.\n        iii. Check the sufficient decrease condition: if $g(x_{k, trial}) > g(y_k) + \\langle \\nabla g(y_k), x_{k, trial} - y_k \\rangle + \\frac{L_{trial}}{2} \\|x_{k, trial} - y_k\\|_2^2$, then update $L_{trial} \\leftarrow \\eta L_{trial}$, increment the counter $C \\leftarrow C+1$, and go back to step (ii).\n        iv. Once the condition is satisfied, set $L_k = L_{trial}$ and $x_k = x_{k, trial}$.\n\n    b. **Acceleration Step**: Update the momentum terms:\n        i. $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$.\n        ii. $y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1})$.\n\n3.  After $10$ iterations, the final value of the counter $C$ is the result for the given test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs FISTA with backtracking on a suite of test cases and reports the\n    total number of backtracking steps (increases of L).\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 0.1,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 100.0,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A\": np.array([\n                [3.0, -2.0, 1.0, 0.0, 1.0, -1.0, 2.0, -3.0, 1.5, -0.5, 0.5, -1.5],\n                [0.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.5, 2.5, -1.0, 0.5, -0.5, 1.0],\n                [1.5, -1.0, 0.5, -0.5, 1.0, -1.5, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5],\n                [-1.0, 2.0, -2.0, 1.0, 0.0, 1.0, -2.5, 3.0, -1.5, 1.0, -1.0, 0.5],\n                [2.0, -1.0, 1.5, -1.0, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5, 1.0, -1.5],\n                [-2.0, 1.0, -0.5, 1.5, -1.0, 2.0, -1.0, 1.0, -0.5, 0.5, -1.0, 1.0],\n                [1.0, 0.5, -1.5, 2.0, -2.5, 1.5, -1.0, 2.0, -1.0, 0.5, -0.5, 1.0],\n                [-1.5, 2.5, -1.0, 0.5, 1.0, -1.0, 2.0, -3.0, 1.5, -1.0, 1.0, -0.5]\n            ]),\n            \"x_true\": np.array([0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.5]),\n            \"lam\": 0.1,\n            \"L_initial\": 1.0,\n            \"eta\": 1.1\n        }\n    ]\n\n    results = []\n\n    def soft_threshold(v, tau):\n        return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        x_true = case[\"x_true\"]\n        lam = case[\"lam\"]\n        L_initial = case[\"L_initial\"]\n        eta = case[\"eta\"]\n        n_iters = 10\n\n        b = A @ x_true\n        n = A.shape[1]\n        \n        # Following standard FISTA notation: x_0, y_1, t_1\n        x_k = np.zeros(n)      # Corresponds to x_{k-1} in iteration k\n        x_km1 = np.zeros(n)    # Corresponds to x_{k-2} in iteration k\n        y_k = x_k              # y_1 = x_0\n        t_k = 1.0              # t_1\n        L = L_initial          # L_0\n        total_increases = 0\n\n        # Loop for k = 1, ..., 10\n        for _ in range(1, n_iters + 1):\n            L_inner = L\n            while True:\n                grad_g_y = A.T @ (A @ y_k - b)\n                x_k_candidate = soft_threshold(y_k - (1.0 / L_inner) * grad_g_y, lam / L_inner)\n\n                g_x = 0.5 * np.linalg.norm(A @ x_k_candidate - b)**2\n                g_y = 0.5 * np.linalg.norm(A @ y_k - b)**2\n                rhs = g_y + np.dot(grad_g_y, x_k_candidate - y_k) + \\\n                      (L_inner / 2.0) * np.linalg.norm(x_k_candidate - y_k)**2\n\n                if g_x <= rhs:\n                    L = L_inner\n                    break\n                else:\n                    L_inner *= eta\n                    total_increases += 1\n            \n            # Found a valid L and x_k_candidate (which is now x_k)\n            x_km1 = x_k\n            x_k = x_k_candidate\n            \n            # Update momentum terms\n            t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n            y_kp1 = x_k + ((t_k - 1.0) / t_kp1) * (x_k - x_km1)\n            \n            # Update state for next iteration\n            t_k = t_kp1\n            y_k = y_kp1\n\n        results.append(total_increases)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond adaptive step-sizes, performance in sparse optimization can often be significantly improved using heuristics like continuation, or \"warm-starting.\" The core idea is to approach a difficult optimization problem by first solving a related, but simpler, one. In the context of LASSO, this involves starting with a high value of the regularization parameter $\\lambda$, which yields a very sparse solution, and then using this solution as an initial guess for the target problem with a smaller $\\lambda$ . This practice implements a two-stage continuation scheme, allowing you to observe how this strategy can effectively identify the correct sparse support in the first stage and then accurately refine the coefficient values in the second, often leading to faster overall convergence.",
            "id": "3446918",
            "problem": "You are asked to implement a two-stage continuation procedure for the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), a method for solving sparse optimization problems in compressed sensing. The target problem is the convex composite minimization of the form minimizing $F_{\\lambda}(\\boldsymbol{x}) = f(\\boldsymbol{x}) + g(\\boldsymbol{x})$ where $f(\\boldsymbol{x})$ is differentiable with a Lipschitz-continuous gradient and $g(\\boldsymbol{x})$ is a proper closed convex function. The foundational base is the proximal gradient framework for convex composite minimization and the mathematical definition of the proximity operator. Specifically, consider the Lasso-type objective $F_{\\lambda}(\\boldsymbol{x}) = \\tfrac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{y}\\|_{2}^{2} + \\lambda \\|\\boldsymbol{x}\\|_{1}$, where $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$, $\\boldsymbol{y} \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. You must use the following base facts, but you must not include any shortcut formulas or update rules in the problem statement: (i) the smooth part is $f(\\boldsymbol{x}) = \\tfrac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{y}\\|_{2}^{2}$ with gradient $\\nabla f(\\boldsymbol{x})$, (ii) a Lipschitz constant $L$ for $\\nabla f$ is given by the square of the spectral norm of $\\boldsymbol{A}$, i.e., $L = \\|\\boldsymbol{A}\\|_{2}^{2}$, and (iii) the proximity operator of the $\\ell_1$ norm is defined by $\\operatorname{prox}_{\\alpha \\|\\cdot\\|_{1}}(\\boldsymbol{v}) = \\arg\\min_{\\boldsymbol{x}} \\tfrac{1}{2}\\|\\boldsymbol{x} - \\boldsymbol{v}\\|_{2}^{2} + \\alpha \\|\\boldsymbol{x}\\|_{1}$.\n\nYour program must implement the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) as an accelerated proximal gradient method to minimize $F_{\\lambda}(\\boldsymbol{x})$ for two regularization parameters $\\lambda^{(1)}$ and $\\lambda^{(2)}$ with $\\lambda^{(2)} < \\lambda^{(1)}$. The two-stage continuation protocol is: Stage $1$ solves the problem for $\\lambda^{(1)}$ starting from the zero vector, and Stage $2$ solves the problem for $\\lambda^{(2)}$ initialized with the minimizer obtained at the end of Stage $1$.\n\nSynthetic data generation for each test case must follow these steps:\n- Draw a measurement matrix $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ with entries i.i.d. from a standard normal distribution scaled by $1/\\sqrt{m}$, i.e., each entry is distributed as $\\mathcal{N}(0, 1/m)$.\n- Normalize each column of $\\boldsymbol{A}$ to have unit $\\ell_{2}$ norm.\n- Choose a support set $\\mathcal{S} \\subset \\{0,1,\\dots,n-1\\}$ of size $k$ uniformly at random and set nonzero entries of the ground-truth vector $\\boldsymbol{x}^{\\star} \\in \\mathbb{R}^{n}$ on $\\mathcal{S}$ to i.i.d. standard normal variables, with zeros elsewhere.\n- Generate noise $\\boldsymbol{w} \\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma^{2}\\boldsymbol{I}_{m})$ and measurements $\\boldsymbol{y} = \\boldsymbol{A}\\boldsymbol{x}^{\\star} + \\boldsymbol{w}$.\n\nAlgorithmic requirements:\n- Compute a valid Lipschitz constant $L$ as $L = \\|\\boldsymbol{A}\\|_{2}^{2}$ (the square of the spectral norm).\n- Use a fixed step size $t = 1/L$.\n- Use the accelerated proximal gradient scheme (FISTA) with a maximum of $N_{\\text{iter}}$ iterations per stage and a stopping rule based on relative change in iterates: stop if $\\|\\boldsymbol{x}^{(k)} - \\boldsymbol{x}^{(k-1)}\\|_{2}/\\max\\{1,\\|\\boldsymbol{x}^{(k-1)}\\|_{2}\\} \\le \\varepsilon_{\\text{stop}}$.\n- After each stage $s \\in \\{1,2\\}$, report the objective value $F_{\\lambda^{(s)}}(\\widehat{\\boldsymbol{x}}^{(s)})$ evaluated at the final iterate and the support set $\\operatorname{supp}(\\widehat{\\boldsymbol{x}}^{(s)}) = \\{i : | \\widehat{\\boldsymbol{x}}^{(s)}_{i} | > \\tau_{\\text{supp}}\\}$ where $\\tau_{\\text{supp}}$ is a fixed threshold.\n\nTest suite:\nImplement the following three test cases, each fully specified by $(m,n,k,\\sigma,\\lambda^{(1)},\\lambda^{(2)},N_{\\text{iter}},\\varepsilon_{\\text{stop}},\\tau_{\\text{supp}},\\text{seed})$:\n- Case $1$: $(m,n,k,\\sigma,\\lambda^{(1)},\\lambda^{(2)},N_{\\text{iter}},\\varepsilon_{\\text{stop}},\\tau_{\\text{supp}},\\text{seed}) = (40, 100, 5, 0.01, 0.12, 0.03, 400, 10^{-8}, 10^{-6}, 7)$.\n- Case $2$: $(m,n,k,\\sigma,\\lambda^{(1)},\\lambda^{(2)},N_{\\text{iter}},\\varepsilon_{\\text{stop}},\\tau_{\\text{supp}},\\text{seed}) = (64, 128, 10, 0, 0.20, 0.05, 400, 10^{-8}, 10^{-6}, 11)$.\n- Case $3$: $(m,n,k,\\sigma,\\lambda^{(1)},\\lambda^{(2)},N_{\\text{iter}},\\varepsilon_{\\text{stop}},\\tau_{\\text{supp}},\\text{seed}) = (32, 200, 6, 0.02, 0.18, 0.04, 400, 10^{-8}, 10^{-6}, 23)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Encode the per-case results sequentially and contiguously in the following order, using only numbers:\n- For each case in the order $1,2,3$, append:\n  - $F_{\\lambda^{(1)}}(\\widehat{\\boldsymbol{x}}^{(1)})$ (a float),\n  - the cardinality of $\\operatorname{supp}(\\widehat{\\boldsymbol{x}}^{(1)})$ (an integer),\n  - the sorted indices of $\\operatorname{supp}(\\widehat{\\boldsymbol{x}}^{(1)})$ (integers),\n  - $F_{\\lambda^{(2)}}(\\widehat{\\boldsymbol{x}}^{(2)})$ (a float),\n  - the cardinality of $\\operatorname{supp}(\\widehat{\\boldsymbol{x}}^{(2)})$ (an integer),\n  - the sorted indices of $\\operatorname{supp}(\\widehat{\\boldsymbol{x}}^{(2)})$ (integers).\nNo other text or delimiters are allowed. The single output line must be exactly one Python list literal, for example $[a_{1},a_{2},\\dots]$, where all entries are numbers. Because each stage’s support size is given immediately before its support indices, the sequence is self-delimiting and parseable without additional markers.\n\nAngle units do not apply. There are no physical units in this problem. Percentages are not used. All numbers in the output must be numeric values.",
            "solution": "The problem is valid. It is a well-defined computational task based on established principles of convex optimization and compressed sensing. It asks for the implementation of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with a two-stage continuation scheme to solve the Lasso optimization problem. All parameters and procedures are specified without ambiguity.\n\nThe objective is to find a vector $\\boldsymbol{x} \\in \\mathbb{R}^{n}$ that minimizes the composite objective function:\n$$\nF_{\\lambda}(\\boldsymbol{x}) = f(\\boldsymbol{x}) + g(\\boldsymbol{x})\n$$\nwhere $f(\\boldsymbol{x})$ is a smooth convex function and $g(\\boldsymbol{x})$ is a convex (possibly non-smooth) function. For the Lasso problem specified, these components are:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{y}\\|_{2}^{2}\n$$\n$$\ng(\\boldsymbol{x}) = \\lambda \\|\\boldsymbol{x}\\|_{1}\n$$\nHere, $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, $\\boldsymbol{y} \\in \\mathbb{R}^{m}$ is the vector of observations, and $\\lambda > 0$ is the regularization parameter that controls the sparsity of the solution $\\boldsymbol{x}$.\n\nFISTA is an accelerated version of the proximal gradient method. The core idea of proximal gradient methods is to iteratively take a gradient descent step on the smooth part $f(\\boldsymbol{x})$ and then apply a proximal operator corresponding to the non-smooth part $g(\\boldsymbol{x})$ to promote the desired structure (sparsity in this case).\n\nThe gradient of the smooth component $f(\\boldsymbol{x})$ is:\n$$\n\\nabla f(\\boldsymbol{x}) = \\boldsymbol{A}^{T}(\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{y})\n$$\nFor the algorithm to converge, the step size must be chosen carefully. A standard choice is $1/L$, where $L$ is a Lipschitz constant of the gradient $\\nabla f(\\boldsymbol{x})$. A valid Lipschitz constant is the maximal eigenvalue of the Hessian $\\nabla^2 f(\\boldsymbol{x}) = \\boldsymbol{A}^T \\boldsymbol{A}$, which is equivalent to the squared spectral norm of $\\boldsymbol{A}$:\n$$\nL = \\|\\boldsymbol{A}^{T}\\boldsymbol{A}\\|_{2} = \\|\\boldsymbol{A}\\|_{2}^{2}\n$$\n\nThe proximal operator of a function $h$ is defined as:\n$$\n\\operatorname{prox}_{\\alpha h}(\\boldsymbol{v}) = \\arg\\min_{\\boldsymbol{x}} \\left\\{ \\frac{1}{2}\\|\\boldsymbol{x} - \\boldsymbol{v}\\|_{2}^{2} + \\alpha h(\\boldsymbol{x}) \\right\\}\n$$\nFor our problem, with $g(\\boldsymbol{x}) = \\lambda \\|\\boldsymbol{x}\\|_{1}$, the proximal step becomes:\n$$\n\\operatorname{prox}_{\\frac{1}{L}g}(\\boldsymbol{v}) = \\operatorname{prox}_{\\frac{\\lambda}{L} \\|\\cdot\\|_{1}}(\\boldsymbol{v}) = \\arg\\min_{\\boldsymbol{x}} \\left\\{ \\frac{1}{2}\\|\\boldsymbol{x} - \\boldsymbol{v}\\|_{2}^{2} + \\frac{\\lambda}{L} \\|\\boldsymbol{x}\\|_{1} \\right\\}\n$$\nThis operation has a well-known closed-form solution called the soft-thresholding operator, denoted by $S_{\\kappa}(\\cdot)$. Applied element-wise, it is defined as:\n$$\n(S_{\\kappa}(\\boldsymbol{v}))_i = \\operatorname{sgn}(v_i) \\max(|v_i| - \\kappa, 0)\n$$\nIn our case, the threshold is $\\kappa = \\lambda/L$.\n\nFISTA accelerates convergence by using an auxiliary sequence $\\boldsymbol{z}^{(k)}$ which represents an extrapolated point from which the proximal gradient step is taken. The algorithm proceeds as follows:\n\n1.  **Initialization**:\n    -   Set an initial guess $\\boldsymbol{x}^{(0)}$.\n    -   Initialize auxiliary sequence $\\boldsymbol{z}^{(1)} = \\boldsymbol{x}^{(0)}$.\n    -   Initialize momentum terms $t_1 = 1$.\n\n2.  **Iteration $k=1, 2, \\dots, N_{\\text{iter}}$**:\n    -   Store the previous iterate: $\\boldsymbol{x}^{(k-1)}_{\\text{old}} = \\boldsymbol{x}^{(k-1)}$.\n    -   Perform the proximal gradient step from the extrapolated point $\\boldsymbol{z}^{(k)}$:\n        $$\n        \\boldsymbol{x}^{(k)} = S_{\\lambda/L}\\left(\\boldsymbol{z}^{(k)} - \\frac{1}{L} \\nabla f(\\boldsymbol{z}^{(k)})\\right) = S_{\\lambda/L}\\left(\\boldsymbol{z}^{(k)} - \\frac{1}{L} \\boldsymbol{A}^{T}(\\boldsymbol{A}\\boldsymbol{z}^{(k)} - \\boldsymbol{y})\\right)\n        $$\n    -   Update the momentum term:\n        $$\n        t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_{k}^2}}{2}\n        $$\n    -   Update the auxiliary sequence with the new momentum:\n        $$\n        \\boldsymbol{z}^{(k+1)} = \\boldsymbol{x}^{(k)} + \\left(\\frac{t_k - 1}{t_{k+1}}\\right)(\\boldsymbol{x}^{(k)} - \\boldsymbol{x}^{(k-1)}_{\\text{old}})\n        $$\n    -   Update $t_{k+1}$ to $t_k$ for the next iteration.\n    -   Check for convergence using the specified stopping criterion:\n        $$\n        \\frac{\\|\\boldsymbol{x}^{(k)} - \\boldsymbol{x}^{(k-1)}_{\\text{old}}\\|_{2}}{\\max\\{1, \\|\\boldsymbol{x}^{(k-1)}_{\\text{old}}\\|_{2}\\}} \\le \\varepsilon_{\\text{stop}}\n        $$\n        If the condition is met, terminate the iteration.\n\nThe two-stage continuation procedure is implemented as follows:\n-   **Stage 1**: The FISTA algorithm is run to solve the optimization problem for a relatively large regularization parameter $\\lambda^{(1)}$. The algorithm is initialized with the zero vector, $\\boldsymbol{x}^{(0)} = \\boldsymbol{0}$. The resulting minimizer is denoted $\\widehat{\\boldsymbol{x}}^{(1)}$.\n-   **Stage 2**: The FISTA algorithm is then run again for a smaller regularization parameter $\\lambda^{(2)} < \\lambda^{(1)}$. Critically, this stage is warm-started, meaning its initial guess is the solution from the previous stage, i.e., $\\boldsymbol{x}^{(0)} = \\widehat{\\boldsymbol{x}}^{(1)}$. This often leads to faster convergence as the support of the solution is often found during the first stage, and the second stage primarily refines the coefficient values.\n\nThe implementation will follow these mathematical steps. For each test case, we first perform the specified synthetic data generation. The Lipschitz constant $L$ is computed as the squared spectral norm of the generated matrix $\\boldsymbol{A}$. Then, the two-stage FISTA procedure is executed. After each stage, the final objective value $F_{\\lambda}(\\widehat{\\boldsymbol{x}})$ is calculated, and the support of the final iterate $\\widehat{\\boldsymbol{x}}$ is identified by thresholding its absolute values with $\\tau_{\\text{supp}}$. The results are then collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef soft_thresholding(v, kappa):\n    \"\"\"\n    Soft-thresholding operator S_kappa(v).\n    \"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n\ndef run_fista(A, y, lambd, L, N_iter, eps_stop, x_init):\n    \"\"\"\n    Implements the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).\n    \n    Args:\n        A (np.ndarray): Measurement matrix.\n        y (np.ndarray): Measurement vector.\n        lambd (float): Regularization parameter.\n        L (float): Lipschitz constant of the gradient of the smooth term.\n        N_iter (int): Maximum number of iterations.\n        eps_stop (float): Tolerance for the stopping criterion.\n        x_init (np.ndarray): Initial guess for the solution vector.\n        \n    Returns:\n        np.ndarray: The estimated sparse solution vector.\n    \"\"\"\n    x_k = x_init.copy()\n    z_k = x_init.copy()\n    t_k = 1.0\n    \n    A_T = A.T  # Pre-calculate transpose for efficiency\n    \n    for _ in range(N_iter):\n        x_prev = x_k.copy()\n        \n        # Proximal gradient step, evaluated at the extrapolated point z_k\n        grad_f_z = A_T @ (A @ z_k - y)\n        x_k = soft_thresholding(z_k - (1.0 / L) * grad_f_z, lambd / L)\n        \n        # Acceleration step\n        t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n        z_k_next = x_k + ((t_k - 1.0) / t_kp1) * (x_k - x_prev)\n        \n        # Update for next iteration\n        z_k = z_k_next\n        t_k = t_kp1\n        \n        # Stopping criterion based on relative change in iterates\n        rel_change = np.linalg.norm(x_k - x_prev) / max(1.0, np.linalg.norm(x_prev))\n        if rel_change <= eps_stop:\n            break\n            \n    return x_k\n\ndef solve():\n    \"\"\"\n    Main function to run the two-stage FISTA for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k, sigma, lambda1, lambda2, N_iter, eps_stop, tau_supp, seed)\n        (40, 100, 5, 0.01, 0.12, 0.03, 400, 1e-8, 1e-6, 7),\n        (64, 128, 10, 0, 0.20, 0.05, 400, 1e-8, 1e-6, 11),\n        (32, 200, 6, 0.02, 0.18, 0.04, 400, 1e-8, 1e-6, 23),\n    ]\n\n    results = []\n\n    for params in test_cases:\n        m, n, k, sigma, lambda1, lambda2, N_iter, eps_stop, tau_supp, seed = params\n        \n        # --- Synthetic Data Generation ---\n        rng = np.random.default_rng(seed)\n        \n        # Create measurement matrix A\n        A = rng.normal(loc=0.0, scale=1.0/np.sqrt(m), size=(m, n))\n        A /= np.linalg.norm(A, axis=0)  # Normalize columns to unit l2-norm\n        \n        # Create sparse ground-truth vector x_star\n        x_star = np.zeros(n)\n        support_indices_true = rng.choice(n, k, replace=False)\n        x_star[support_indices_true] = rng.standard_normal(k)\n        \n        # Create measurement vector y with additive noise\n        noise = rng.normal(loc=0.0, scale=sigma, size=m)\n        y = A @ x_star + noise\n        \n        # --- Algorithm Setup ---\n        # Lipschitz constant is the squared spectral norm of A\n        L = np.linalg.norm(A, 2)**2\n        \n        # --- Stage 1: Solve for lambda1 ---\n        x_hat_1 = run_fista(A, y, lambda1, L, N_iter, eps_stop, np.zeros(n))\n        \n        # --- Stage 2: Solve for lambda2, warm-started with result from Stage 1 ---\n        x_hat_2 = run_fista(A, y, lambda2, L, N_iter, eps_stop, x_hat_1)\n        \n        # --- Result Collection for Stage 1 ---\n        obj_val_1 = 0.5 * np.linalg.norm(A @ x_hat_1 - y)**2 + lambda1 * np.linalg.norm(x_hat_1, 1)\n        support_1 = np.where(np.abs(x_hat_1) > tau_supp)[0]\n        \n        results.append(obj_val_1)\n        results.append(len(support_1))\n        results.extend(sorted(support_1.tolist()))\n\n        # --- Result Collection for Stage 2 ---\n        obj_val_2 = 0.5 * np.linalg.norm(A @ x_hat_2 - y)**2 + lambda2 * np.linalg.norm(x_hat_2, 1)\n        support_2 = np.where(np.abs(x_hat_2) > tau_supp)[0]\n        \n        results.append(obj_val_2)\n        results.append(len(support_2))\n        results.extend(sorted(support_2.tolist()))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}