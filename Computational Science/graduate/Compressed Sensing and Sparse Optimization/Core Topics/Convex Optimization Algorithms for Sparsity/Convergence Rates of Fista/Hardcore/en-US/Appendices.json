{
    "hands_on_practices": [
        {
            "introduction": "The celebrated $O(1/k^2)$ convergence rate of FISTA is more than a theoretical guarantee; it's a practical tool for planning and resource allocation in computational tasks. This exercise bridges the gap between theory and application by asking you to compute a concrete iteration budget. By working through this problem (), you will see how to use the algorithm's parameters and the convergence formula to predict the effort required to achieve a specified level of accuracy for a LASSO problem.",
            "id": "3439140",
            "problem": "Consider the composite convex optimization problem in compressed sensing\n$$\nF(x) \\equiv f(x) + g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$, $b \\in \\mathbb{R}^{3}$, $\\lambda  0$, $f$ is convex with a gradient that is Lipschitz continuous, and $g$ is a proper, closed, convex function with a computable proximal operator. Suppose the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is applied to this problem with the standard constant step size choice $1/L$ and exact proximal evaluations, where $L$ is the Lipschitz constant of $\\nabla f$.\n\nUse the following data:\n- $A = \\operatorname{diag}(5, 3, 1)$,\n- $b = (0, 0, 0)^{\\top}$,\n- $\\lambda = 1$,\n- initial point $x_{0} = (3, 4, 0)^{\\top}$.\n\nAssuming the standard FISTA rate for composite convex minimization under $L$-smoothness of $f$ and convexity of $g$, derive an explicit iteration budget that guarantees the objective gap satisfies $F(x_{k}) - F(x^{\\star}) \\le \\epsilon$ for a prescribed tolerance $\\epsilon$, and then compute the minimal integer $k$ for the tolerance $\\epsilon = 2 \\times 10^{-2}$.\n\nProvide your final answer as a single integer. No rounding is required beyond taking the minimal integer that satisfies the guarantee.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **Optimization Problem**: Minimize $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$.\n- **Domain**: $A \\in \\mathbb{R}^{3 \\times 3}$, $b \\in \\mathbb{R}^{3}$.\n- **Algorithm**: Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with step size $1/L$.\n- **Data**:\n    - $A = \\operatorname{diag}(5, 3, 1)$\n    - $b = (0, 0, 0)^{\\top}$\n    - $\\lambda = 1$\n    - Initial point: $x_{0} = (3, 4, 0)^{\\top}$\n- **Properties**: $f(x)$ is convex with an $L$-Lipschitz continuous gradient $\\nabla f$. $g(x)$ is a proper, closed, convex function.\n- **Task**:\n    1.  Derive an explicit iteration budget $k$ that guarantees $F(x_{k}) - F(x^{\\star}) \\le \\epsilon$.\n    2.  Compute the minimal integer $k$ required for a tolerance of $\\epsilon = 2 \\times 10^{-2}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem describes the LASSO objective function, a cornerstone of compressed sensing and sparse regression. FISTA is a standard, provably convergent algorithm for this class of problems. The convergence rate for FISTA is a classic result in convex optimization theory. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The problem provides all necessary information to determine the quantities in the FISTA convergence rate formula. The Lipschitz constant $L$ can be derived from $A$. The optimal solution $x^\\star$ can be uniquely determined from the given data. The task is to apply a known theoretical bound, which is a well-defined mathematical exercise.\n- **Objectivity**: The problem is stated in precise mathematical terms, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is a standard application of a well-established theoretical result in optimization. We proceed with the solution.\n\n### Detailed Solution\n\nThe problem is to find the minimum number of iterations $k$ for FISTA to guarantee an objective value gap of at most $\\epsilon$. The objective function is $F(x) = f(x) + g(x)$, with $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$.\n\nThe standard convergence rate for FISTA applied to this composite problem states that the sequence of iterates $\\{x_k\\}$ satisfies:\n$$\nF(x_{k}) - F(x^{\\star}) \\le \\frac{2L \\|x_0 - x^{\\star}\\|_2^2}{(k+1)^2}\n$$\nwhere $x^{\\star}$ is a minimizer of $F(x)$, $x_0$ is the initial point, and $L$ is the Lipschitz constant of the gradient of the smooth part, $\\nabla f(x)$.\n\nTo find the iteration budget, we need to compute $L$, $x^{\\star}$, and $\\|x_0 - x^{\\star}\\|_2^2$.\n\n**1. Compute the Lipschitz constant $L$**\nThe gradient of $f(x)$ is $\\nabla f(x) = A^{\\top}(Ax - b)$. The Hessian is $\\nabla^2 f(x) = A^{\\top}A$. The Lipschitz constant $L$ of $\\nabla f(x)$ is the largest eigenvalue of the Hessian matrix, $L = \\lambda_{\\max}(A^{\\top}A)$.\nGiven $A = \\operatorname{diag}(5, 3, 1)$, $A$ is a symmetric matrix, so $A^{\\top} = A$.\n$$\nA^{\\top}A = A^2 = (\\operatorname{diag}(5, 3, 1))^2 = \\operatorname{diag}(5^2, 3^2, 1^2) = \\operatorname{diag}(25, 9, 1)\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. The maximum eigenvalue is therefore $\\lambda_{\\max}(A^{\\top}A) = 25$.\nThus, the Lipschitz constant is $L = 25$.\n\n**2. Find the optimal solution $x^{\\star}$**\nThe objective function is $F(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$.\nWith the given data, $b = (0, 0, 0)^{\\top}$ and $\\lambda=1$, the function becomes:\n$$\nF(x) = \\frac{1}{2}\\|Ax\\|_{2}^{2} + \\|x\\|_{1}\n$$\nThe term $f(x) = \\frac{1}{2}\\|Ax\\|_{2}^{2}$ is non-negative. Since $A = \\operatorname{diag}(5, 3, 1)$ is invertible, $Ax=0$ if and only if $x=0$. Thus, $f(x) \\ge 0$, with equality only at $x=0$.\nThe term $g(x) = \\|x\\|_{1}$ is also non-negative, and $g(x)=0$ if and only if $x=0$.\nThe sum $F(x) = f(x) + g(x)$ is therefore non-negative, and it equals $0$ if and only if both terms are zero, which occurs uniquely at $x=0$.\nSo, the minimizer is $x^{\\star} = (0, 0, 0)^{\\top}$, and the minimum objective value is $F(x^{\\star}) = 0$.\n\n**3. Compute the initial distance $\\|x_0 - x^{\\star}\\|_2^2$**\nThe initial point is given as $x_0 = (3, 4, 0)^{\\top}$, and we found $x^{\\star} = (0, 0, 0)^{\\top}$.\nThe squared Euclidean distance is:\n$$\n\\|x_0 - x^{\\star}\\|_2^2 = \\|(3, 4, 0)^{\\top} - (0, 0, 0)^{\\top}\\|_2^2 = \\|(3, 4, 0)^{\\top}\\|_2^2 = 3^2 + 4^2 + 0^2 = 9 + 16 = 25\n$$\n\n**4. Derive the explicit iteration budget and compute $k$**\nWe require the number of iterations $k$ such that $F(x_k) - F(x^\\star) \\le \\epsilon$. Using the FISTA convergence guarantee, we need to satisfy:\n$$\n\\frac{2L \\|x_0 - x^{\\star}\\|_2^2}{(k+1)^2} \\le \\epsilon\n$$\nRearranging this inequality to solve for $k$ gives the explicit iteration budget:\n$$\n(k+1)^2 \\ge \\frac{2L \\|x_0 - x^{\\star}\\|_2^2}{\\epsilon}\n$$\n$$\nk+1 \\ge \\sqrt{\\frac{2L \\|x_0 - x^{\\star}\\|_2^2}{\\epsilon}}\n$$\n$$\nk \\ge \\sqrt{\\frac{2L \\|x_0 - x^{\\star}\\|_2^2}{\\epsilon}} - 1\n$$\nThis is the general expression for the iteration budget. Now we substitute the specific values for the problem: $L=25$, $\\|x_0 - x^{\\star}\\|_2^2=25$, and $\\epsilon = 2 \\times 10^{-2}$.\n$$\nk \\ge \\sqrt{\\frac{2 \\times 25 \\times 25}{2 \\times 10^{-2}}} - 1\n$$\n$$\nk \\ge \\sqrt{\\frac{25^2}{10^{-2}}} - 1\n$$\n$$\nk \\ge \\sqrt{625 \\times 100} - 1\n$$\n$$\nk \\ge \\sqrt{62500} - 1\n$$\n$$\nk \\ge 250 - 1\n$$\n$$\nk \\ge 249\n$$\nThe problem asks for the minimal integer $k$ that satisfies this condition. The smallest integer $k$ that is greater than or equal to $249$ is $249$.\nTherefore, a minimum of $k=249$ iterations are required to guarantee the objective gap is less than or equal to $2 \\times 10^{-2}$.",
            "answer": "$$\\boxed{249}$$"
        },
        {
            "introduction": "To truly appreciate why FISTA is considered an 'accelerated' method, it is essential to compare it directly with its predecessor, the Iterative Shrinkage-Thresholding Algorithm (ISTA). This practice () guides you through the derivation of the explicit convergence bounds for both algorithms, starting from first principles. By quantifying the constants in ISTA's $O(1/k)$ rate and FISTA's $O(1/k^2)$ rate, you will gain a deeper insight into the mechanics of Nesterov's momentum and its powerful effect on performance.",
            "id": "3439144",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (Lasso) objective written as the composite convex function $F(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $h(x) = \\lambda \\|x\\|_{1}$ with $\\lambda  0$. Let the gradient $\\nabla g$ be $L$-Lipschitz with $L = \\|A\\|_{2}^{2}$, where $\\|A\\|_{2}$ denotes the spectral norm (largest singular value) of $A$. The Iterative Shrinkage-Thresholding Algorithm (ISTA) uses the proximal gradient update with stepsize $1/L$, and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) accelerates ISTA using Nesterov-type extrapolation with the canonical sequence of inertial parameters.\n\nStarting only from fundamental definitions of convexity, Lipschitz continuity of the gradient, and properties of the proximal operator, derive explicit constants for the bounds\n$$F(x_{k}) - F(x^{\\star}) \\leq \\frac{C_{\\mathrm{ISTA}}}{k} \\quad \\text{and} \\quad F(x_{k}) - F(x^{\\star}) \\leq \\frac{C_{\\mathrm{FISTA}}}{(k+1)^{2}},$$\nfor ISTA and FISTA respectively, when both use stepsize $1/L$ on the same Lasso instance. Your derivation must explicitly express the constants $C_{\\mathrm{ISTA}}$ and $C_{\\mathrm{FISTA}}$ in terms of $L$ and $\\|x_{0} - x^{\\star}\\|_{2}^{2}$, where $x_{0}$ is the initial iterate and $x^{\\star}$ is any minimizer of $F$.\n\nThen, evaluate these constants on the specific instance with\n$$A = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix}, \\quad b = 0, \\quad \\lambda  0, \\quad x_{0} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}.$$\nCompute $L = \\|A\\|_{2}^{2}$ and the ratio $C_{\\mathrm{FISTA}} / C_{\\mathrm{ISTA}}$ for this instance. Provide the ratio as the final answer. If you need to approximate any intermediate quantities, do not do so; use exact arithmetic throughout. The final answer must be a single real number.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the well-established theory of convex optimization, specifically first-order methods for composite functions. The problem is well-posed, objective, and contains all necessary information to derive the constants and compute their ratio.\n\nThe overall objective function is $F(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is convex and has an $L$-Lipschitz continuous gradient $\\nabla g(x) = A^T(Ax-b)$, and $h(x) = \\lambda \\|x\\|_{1}$ is convex but non-differentiable. The Lipschitz constant of the gradient is $L = \\|A^T A\\|_2 = \\|A\\|_2^2$. The minimizers of $F$ are denoted by $x^\\star$.\n\n**Part 1: Derivation of the Convergence Rate Constant for ISTA**\n\nThe Iterative Shrinkage-Thresholding Algorithm (ISTA) is a proximal gradient method. The update rule for a stepsize $\\alpha$ is given by:\n$$x_{k+1} = \\mathrm{prox}_{\\alpha h}(x_k - \\alpha \\nabla g(x_k))$$\nThe problem specifies a stepsize $\\alpha = 1/L$.\nThe derivation of the convergence rate relies on a key inequality that we first establish.\nBy definition, a function $g$ with an $L$-Lipschitz gradient satisfies the descent lemma: for any $x, y$,\n$$g(y) \\leq g(x) + \\langle \\nabla g(x), y-x \\rangle + \\frac{L}{2} \\|y-x\\|_{2}^{2}$$\nLet $x_k$ be the current iterate and $x_{k+1}$ be the next iterate generated by ISTA with $\\alpha = 1/L$.\nSetting $x=x_k$ and $y=x_{k+1}$ in the descent lemma, we have:\n$$g(x_{k+1}) \\leq g(x_k) + \\langle \\nabla g(x_k), x_{k+1}-x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|_{2}^{2}$$\nThe proximal operator update $x_{k+1} = \\mathrm{prox}_{h/L}(x_k - \\frac{1}{L} \\nabla g(x_k))$ is the unique minimizer of the function $u \\mapsto \\frac{L}{2} \\|u - (x_k - \\frac{1}{L} \\nabla g(x_k))\\|_2^2 + h(u)$.\nThe optimality condition for $x_{k+1}$ is $0 \\in \\partial h(x_{k+1}) + L(x_{k+1} - (x_k - \\frac{1}{L} \\nabla g(x_k)))$, which simplifies to\n$$L(x_k - x_{k+1}) - \\nabla g(x_k) \\in \\partial h(x_{k+1})$$\nLet $\\xi_{k+1} = L(x_k - x_{k+1}) - \\nabla g(x_k)$ be this subgradient in $\\partial h(x_{k+1})$. By the definition of subgradient for the convex function $h$, for any point $y$:\n$$h(y) \\geq h(x_{k+1}) + \\langle \\xi_{k+1}, y-x_{k+1} \\rangle = h(x_{k+1}) + \\langle L(x_k - x_{k+1}) - \\nabla g(x_k), y-x_{k+1} \\rangle$$\nRearranging gives:\n$$h(x_{k+1}) \\leq h(y) - \\langle L(x_k - x_{k+1}) - \\nabla g(x_k), y-x_{k+1} \\rangle$$\nNow, consider the total function value $F(x_{k+1}) = g(x_{k+1}) + h(x_{k+1})$.\n$$F(x_{k+1}) \\leq \\left( g(x_k) + \\langle \\nabla g(x_k), x_{k+1}-x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|_{2}^{2} \\right) + \\left( h(y) - \\langle L(x_k - x_{k+1}) - \\nabla g(x_k), y-x_{k+1} \\rangle \\right)$$\n$$F(x_{k+1}) \\leq g(x_k) + h(y) + \\langle \\nabla g(x_k), y-x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|_{2}^{2} - L\\langle x_k - x_{k+1}, y-x_{k+1} \\rangle$$\nThe term $\\langle x_k - x_{k+1}, y-x_{k+1} \\rangle$ can be expanded using the identity $2\\langle a,b \\rangle = \\|a\\|^2+\\|b\\|^2-\\|a-b\\|^2$ with $a=x_k-x_{k+1}$ and $b=y-x_{k+1}$. We get $2\\langle x_k - x_{k+1}, y-x_{k+1} \\rangle = \\|x_k-x_{k+1}\\|^2 + \\|y-x_{k+1}\\|^2 - \\|x_k-y\\|^2$.\nSubstituting this in:\n$$F(x_{k+1}) \\leq g(x_k) + h(y) + \\langle \\nabla g(x_k), y-x_k \\rangle + \\frac{L}{2} \\|x_{k+1}-x_k\\|_{2}^{2} - \\frac{L}{2} \\left( \\|x_k-x_{k+1}\\|^2 + \\|y-x_{k+1}\\|^2 - \\|x_k-y\\|^2 \\right)$$\n$$F(x_{k+1}) \\leq g(x_k) + \\langle \\nabla g(x_k), y-x_k \\rangle + h(y) + \\frac{L}{2} \\left( \\|x_k-y\\|^2 - \\|x_{k+1}-y\\|^2 \\right)$$\nBy convexity of $g$, we have $g(y) \\geq g(x_k) + \\langle \\nabla g(x_k), y-x_k \\rangle$. Thus, $g(x_k) + \\langle \\nabla g(x_k), y-x_k \\rangle \\leq g(y)$.\n$$F(x_{k+1}) \\leq g(y) + h(y) + \\frac{L}{2} \\left( \\|x_k-y\\|^2 - \\|x_{k+1}-y\\|^2 \\right)$$\nThis inequality holds for any $y$. We choose $y=x^\\star$, a minimizer of $F$.\n$$F(x_{k+1}) - F(x^\\star) \\leq \\frac{L}{2} \\left( \\|x_k-x^\\star\\|_2^2 - \\|x_{k+1}-x^\\star\\|_2^2 \\right)$$\nLet $\\delta_k = F(x_k) - F(x^\\star)$. The above inequality is $\\delta_{k+1} \\le \\frac{L}{2} (\\|x_k-x^\\star\\|_2^2 - \\|x_{k+1}-x^\\star\\|_2^2)$.\nSumming this from $k=0$ to $K-1$:\n$$\\sum_{k=1}^{K} \\delta_k \\leq \\frac{L}{2} \\sum_{k=0}^{K-1} \\left( \\|x_k-x^\\star\\|_2^2 - \\|x_{k+1}-x^\\star\\|_2^2 \\right) = \\frac{L}{2} \\left( \\|x_0-x^\\star\\|_2^2 - \\|x_K-x^\\star\\|_2^2 \\right)$$\nSince $\\|x_K-x^\\star\\|_2^2 \\geq 0$, we have $\\sum_{k=1}^{K} \\delta_k \\leq \\frac{L}{2} \\|x_0-x^\\star\\|_2^2$.\nThe sequence of function values $\\{F(x_k)\\}$ is non-increasing for ISTA with $\\alpha \\le 1/L$. Thus, $\\{\\delta_k\\}$ is a non-increasing sequence of non-negative numbers.\nThis implies $\\delta_K \\leq \\delta_k$ for all $k \\leq K$.\nTherefore, $K \\delta_K \\leq \\sum_{k=1}^{K} \\delta_k$.\nCombining these results:\n$$K \\delta_K \\leq \\frac{L}{2} \\|x_0-x^\\star\\|_2^2 \\implies \\delta_K \\leq \\frac{L \\|x_0-x^\\star\\|_2^2}{2K}$$\nReplacing $K$ with $k$, we get the bound for the $k$-th iterate:\n$$F(x_k) - F(x^\\star) \\leq \\frac{L \\|x_0-x^\\star\\|_2^2}{2k}$$\nComparing this to the specified form $F(x_k) - F(x^\\star) \\leq \\frac{C_{\\mathrm{ISTA}}}{k}$, we identify the constant:\n$$C_{\\mathrm{ISTA}} = \\frac{L}{2} \\|x_0-x^\\star\\|_2^2$$\n\n**Part 2: Derivation of the Convergence Rate Constant for FISTA**\n\nThe Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) uses a Nesterov-type acceleration scheme. The derivation of its $O(1/k^2)$ rate is considerably more involved. It relies on a carefully constructed potential or Lyapunov function and a specific sequence of momentum parameters. We sketch the main ideas.\n\nThe FISTA updates are:\n1. $x_k = \\mathrm{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))$\n2. $t_{k+1} = \\frac{1+\\sqrt{1+4t_k^2}}{2}$\n3. $y_{k+1} = x_k + \\frac{t_k-1}{t_{k+1}}(x_k-x_{k-1})$\nwith initialization $t_1=1$, $y_1=x_0$, and $x_{-1}=x_0$.\n\nThe proof strategy involves manipulating the same fundamental inequalities used for ISTA, but applied to a mix of iterates $x_k$ and extrapolated points $y_k$. One establishes a recursive relationship. The key inequality from the proof (e.g., Theorem 4.4 in Beck  Teboulle, 2009) is of the form:\n$$t_k^2(F(x_k)-F(x^\\star)) - t_{k-1}^2(F(x_{k-1})-F(x^\\star)) \\le \\frac{L}{2}(\\|u_{k-1}\\|^2 - \\|u_k\\|^2)$$\nwhere $u_k$ is an auxiliary sequence of vectors related to the iterates. Summing this inequality from $k=1$ to $K$ yields a telescoping sum on the right-hand side. This leads to an inequality of the form:\n$$t_K^2(F(x_K)-F(x^\\star)) \\le \\frac{L}{2}\\|x_0-x^\\star\\|_2^2$$\nThe parameter sequence satisfies $t_K \\ge \\frac{K+1}{2}$. Substituting this gives:\n$$(\\frac{K+1}{2})^2 (F(x_K)-F(x^\\star)) \\le \\frac{L}{2}\\|x_0-x^\\star\\|_2^2$$\n$$F(x_K)-F(x^\\star) \\le \\frac{2L\\|x_0-x^\\star\\|_2^2}{(K+1)^2}$$\nSome proofs yield a slightly different constant or dependence on $k^2$ versus $(k+1)^2$. The form specified in the problem, $\\frac{C_{\\mathrm{FISTA}}}{(k+1)^2}$, is a standard result. Comparing with our derived bound, we identify the constant:\n$$C_{\\mathrm{FISTA}} = 2L \\|x_0-x^\\star\\|_2^2$$\n\n**Part 3: Evaluation for the Specific Instance**\n\nWe are given the specific instance:\n$$A = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  2 \\end{pmatrix}, \\quad b = 0, \\quad \\lambda  0, \\quad x_{0} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}$$\nFirst, we compute the necessary quantities.\n1.  **Lipschitz constant $L$**:\n    $L = \\|A\\|_2^2$. The spectral norm $\\|A\\|_2$ is the largest singular value of $A$. Since $A$ is a diagonal matrix, its singular values are the absolute values of the diagonal entries: $\\{|3|, |1|, |2|\\}$. The largest singular value is $3$.\n    Therefore, $L = 3^2 = 9$.\n\n2.  **Optimal solution $x^\\star$**:\n    We must find the minimizer of $F(x) = \\frac{1}{2}\\|Ax\\|_2^2 + \\lambda\\|x\\|_1$.\n    The objective function is $F(x) = \\frac{1}{2}( (3x_1)^2 + (1x_2)^2 + (2x_3)^2 ) + \\lambda(|x_1|+|x_2|+|x_3|)$.\n    $F(x) = \\frac{1}{2}(9x_1^2 + x_2^2 + 4x_3^2) + \\lambda(|x_1|+|x_2|+|x_3|)$.\n    Both terms are non-negative for any $x$. The first term is zero if and only if $x=0$. The second term is zero if and only if $x=0$ (since $\\lambda  0$).\n    Thus, $F(x)$ achieves its minimum value of $0$ only at $x=0$. The unique minimizer is $x^\\star = 0$.\n\n3.  **Initial distance to optimum $\\|x_0 - x^\\star\\|_2^2$**:\n    With $x^\\star = 0$, we have $x_0 - x^\\star = x_0 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n    $\\|x_0 - x^\\star\\|_2^2 = 1^2 + 2^2 + 2^2 = 1 + 4 + 4 = 9$.\n\nNow we can express the constants for this instance:\n$$C_{\\mathrm{ISTA}} = \\frac{L}{2} \\|x_0 - x^\\star\\|_2^2 = \\frac{9}{2} \\times 9 = \\frac{81}{2}$$\n$$C_{\\mathrm{FISTA}} = 2L \\|x_0 - x^\\star\\|_2^2 = 2 \\times 9 \\times 9 = 162$$\n\n**Part 4: Compute the Ratio**\n\nThe final step is to compute the ratio $C_{\\mathrm{FISTA}} / C_{\\mathrm{ISTA}}$.\n$$\\frac{C_{\\mathrm{FISTA}}}{C_{\\mathrm{ISTA}}} = \\frac{2L \\|x_0 - x^\\star\\|_2^2}{\\frac{L}{2} \\|x_0 - x^\\star\\|_2^2} = \\frac{2}{1/2} = 4$$\nThe ratio is independent of the specific values of $L$ and $\\|x_0-x^\\star\\|_2^2$, as long as they are non-zero. It reflects the fundamental difference in the derived upper bounds for the two algorithms.",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "In real-world applications, computing the exact Lipschitz constant $L$ can be computationally expensive or even infeasible, leading practitioners to use a more easily calculated, conservative upper bound $L' \\gt L$. This choice, while guaranteeing convergence, comes at a cost. This exercise () explores the precise trade-offs involved, asking you to derive how overestimating $L$ by a factor of $c$ impacts both the theoretical error bound at a fixed iteration and the total number of iterations needed to reach a target accuracy.",
            "id": "3439173",
            "problem": "Consider the composite convex optimization problem in compressed sensing, where one seeks to minimize the objective $$F(x) = f(x) + g(x),$$ with $$f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} \\quad \\text{and} \\quad g(x) = \\lambda \\|x\\|_{1},$$ where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $b \\in \\mathbb{R}^{m}$ is a given measurement vector, and $\\lambda  0$ is a regularization parameter. The gradient $\\nabla f$ is Lipschitz continuous with a smallest valid Lipschitz constant $L = \\sigma_{\\max}(A)^{2}$, where $\\sigma_{\\max}(A)$ denotes the largest singular value of $A$. Assume one applies the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) with a constant step size $t = 1/L'$ and standard Nesterov-type acceleration parameters, where $L' = c L$ for some factor $c  1$ (i.e., the Lipschitz constant is overestimated by a multiplicative factor $c$).\n\nUsing only the fundamental smoothness majorization of $f$ and the convexity of $g$, start from the inequality\n$$f(y) \\leq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L}{2}\\|y - x\\|_{2}^{2},$$\nand the property that a surrogate built with any $L' \\geq L$ is a valid upper model for $f$. From these principles, derive how the function value accuracy bound of FISTA scales with $L'$ (do not assume or quote any explicit convergence rate formula a priori; instead, reason from the homogeneity of the quadratic upper model and the acceleration potential construction). Then:\n\n1. At a fixed iteration index $k$, determine the exact multiplicative slowdown factor in the function value error bound when $L'$ is replaced by $c L$.\n2. For a fixed target accuracy level $\\varepsilon  0$, determine the exact multiplicative slowdown factor in the iteration complexity (the number of iterations required to guarantee $F(x_{k}) - F(x^{\\star}) \\leq \\varepsilon$), again when $L'$ is replaced by $c L$.\n\nFinally, illustrate these slowdowns in the setting of ill-conditioned sensing matrices by expressing the overestimation factor $c$ that arises when one uses the Frobenius norm bound $L' = \\|A\\|_{F}^{2}$ in place of the spectral norm bound $L = \\sigma_{\\max}(A)^{2}$, and explain how the singular value distribution affects the slowdowns. Your final answer must be the pair of slowdown factors for parts 1 and 2, expressed as a row matrix in terms of $c$. No rounding is required, and no physical units apply.",
            "solution": "The problem asks for an analysis of the convergence rate of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) when the Lipschitz constant of the gradient of the smooth part of the objective function is overestimated. We are to determine the slowdown in terms of the error bound and iteration complexity.\n\nThe optimization problem is to minimize $F(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. The function $f(x)$ is convex and continuously differentiable, and its gradient $\\nabla f(x) = A^T(Ax-b)$ is Lipschitz continuous with constant $L = \\sigma_{\\max}(A)^2$. The function $g(x)$ is convex but non-smooth.\n\nFISTA is an accelerated proximal gradient method. A key element of its analysis is the quadratic upper bound on $f(x)$ which is guaranteed by the Lipschitz continuity of its gradient. For any $L' \\geq L$, we have the majorization inequality:\n$$\nf(y) \\leq f(x) + \\langle \\nabla f(x), y - x \\rangle + \\frac{L'}{2}\\|y - x\\|_{2}^{2}, \\quad \\forall x, y \\in \\mathbb{R}^n.\n$$\nThis inequality is the starting point of our derivation.\n\nThe FISTA algorithm, at iteration $k$, generates a new iterate $x_{k+1}$ from a search point $y_k$ by performing a proximal gradient step. This step can be viewed as minimizing a surrogate function for $F(x)$ around $y_k$:\n$$\nx_{k+1} = \\underset{x \\in \\mathbb{R}^n}{\\arg\\min} \\left\\{ f(y_k) + \\langle \\nabla f(y_k), x - y_k \\rangle + \\frac{L'}{2}\\|x - y_k\\|_{2}^{2} + g(x) \\right\\}.\n$$\nThe step-size used is $t = 1/L'$. Let the minimized surrogate be $Q_{L'}(x, y_k)$. Because $L' \\geq L$, this is an upper bound for $F(x)$: $F(x) \\le Q_{L'}(x, y_k)$ for all $x$. Thus, $F(x_{k+1}) \\le Q_{L'}(x_{k+1}, y_k)$. As $x_{k+1}$ is the minimizer of $Q_{L'}(x, y_k)$, we have $Q_{L'}(x_{k+1}, y_k) \\le Q_{L'}(x, y_k)$ for any $x \\in \\mathbb{R}^n$.\n\nCombining these facts with the convexity of $f$ and $g$ allows for the derivation of a fundamental inequality that governs the progress in one iteration. For any $x \\in \\mathbb{R}^n$:\n\\begin{align*}\nF(x_{k+1}) - F(x)  \\le Q_{L'}(x_{k+1}, y_k) - F(x) \\\\\n \\le Q_{L'}(x, y_k) - F(x) \\\\\n = \\left( f(y_k) + \\langle \\nabla f(y_k), x - y_k \\rangle + \\frac{L'}{2}\\|x - y_k\\|_{2}^{2} + g(x) \\right) - (f(x) + g(x)) \\\\\n = (f(y_k) - f(x) + \\langle \\nabla f(y_k), x - y_k \\rangle) + \\frac{L'}{2}\\|x - y_k\\|_{2}^{2}.\n\\end{align*}\nBy the convexity of $f$, we have $f(y_k) - f(x) + \\langle \\nabla f(y_k), x - y_k \\rangle \\le 0$. This leads to $F(x_{k+1}) - F(x) \\le \\frac{L'}{2}\\|x-y_k\\|_2^2$. While correct, this is not the tightest bound. A more careful derivation, as requested by the prompt, involves using the optimality condition of the proximal step and the definitions of convexity for both $f$ and $g$. This leads to the well-known inequality:\n$$\nF(x_{k+1}) - F(x) \\le \\frac{L'}{2} \\left[ \\|x - y_k\\|_{2}^{2} - \\|x - x_{k+1}\\|_{2}^{2} \\right].\n$$\nThis inequality is central to the convergence analysis of FISTA. The \"acceleration potential construction\" refers to combining this inequality (evaluated at $x = x_k$ and $x = x^{\\star}$, where $x^{\\star}$ is a minimizer of $F$) with the specific Nesterov momentum rule for $y_k$. This process constructs a Lyapunov function that proves the convergence rate. The standard analysis shows that for a proper choice of momentum parameters, the function value error $F(x_k) - F(x^{\\star})$ is bounded as:\n$$\nF(x_k) - F(x^{\\star}) \\leq \\frac{\\alpha L' \\|x_0 - x^{\\star}\\|_{2}^{2}}{k^2},\n$$\nwhere $\\alpha$ is a constant (typically $\\alpha=2$ for the common FISTA variant with $t_k \\ge k/2$). The crucial point is that the bound is directly proportional to the Lipschitz constant $L'$ used in the algorithm's step size. The \"homogeneity of the quadratic upper model\" means that the coefficient of the quadratic term, $\\frac{L'}{2}$, scales linearly with $L'$, and this linear scaling propagates directly to the final convergence rate bound.\n\nLet's denote the error bound by $B(k, L') = \\frac{\\alpha L' \\|x_0 - x^{\\star}\\|_{2}^{2}}{k^2}$. We are interested in what happens when we replace the optimal constant $L = \\sigma_{\\max}(A)^2$ with an overestimated constant $L' = cL$ for some $c  1$.\n\n**1. Multiplicative slowdown factor in the function value error bound**\n\nAt a fixed iteration $k$, we compare the error bound obtained using $L$ with the one obtained using $L' = cL$.\nThe bound with the optimal constant is $B(k, L) = \\frac{\\alpha L \\|x_0 - x^{\\star}\\|_{2}^{2}}{k^2}$.\nThe bound with the overestimated constant is $B(k, cL) = \\frac{\\alpha (cL) \\|x_0 - x^{\\star}\\|_{2}^{2}}{k^2}$.\n\nThe multiplicative factor increase in the error bound is the ratio:\n$$\n\\frac{B(k, cL)}{B(k, L)} = \\frac{\\frac{\\alpha c L \\|x_0 - x^{\\star}\\|_{2}^{2}}{k^2}}{\\frac{\\alpha L \\|x_0 - x^{\\star}\\|_{2}^{2}}{k^2}} = c.\n$$\nThus, overestimating the Lipschitz constant by a factor of $c$ leads to a theoretical error bound that is $c$ times larger at any given iteration $k$.\n\n**2. Multiplicative slowdown factor in the iteration complexity**\n\nIteration complexity refers to the number of iterations required to achieve a desired accuracy $\\varepsilon  0$. Let $K(L', \\varepsilon)$ be the minimum number of iterations $k$ needed to guarantee $F(x_k) - F(x^{\\star}) \\leq \\varepsilon$.\n\nUsing the constant $L$, we need to find $k_1 = K(L, \\varepsilon)$ such that:\n$$\n\\frac{\\alpha L \\|x_0 - x^{\\star}\\|_{2}^{2}}{k_1^2} \\leq \\varepsilon \\implies k_1^2 \\geq \\frac{\\alpha L \\|x_0 - x^{\\star}\\|_{2}^{2}}{\\varepsilon} \\implies k_1 \\geq \\sqrt{\\frac{\\alpha L}{\\varepsilon}} \\|x_0 - x^{\\star}\\|_{2}.\n$$\nUsing the constant $L' = cL$, we need to find $k_2 = K(cL, \\varepsilon)$ such that:\n$$\n\\frac{\\alpha (cL) \\|x_0 - x^{\\star}\\|_{2}^{2}}{k_2^2} \\leq \\varepsilon \\implies k_2^2 \\geq \\frac{\\alpha cL \\|x_0 - x^{\\star}\\|_{2}^{2}}{\\varepsilon} \\implies k_2 \\geq \\sqrt{\\frac{\\alpha cL}{\\varepsilon}} \\|x_0 - x^{\\star}\\|_{2}.\n$$\nThe multiplicative slowdown factor in iteration complexity is the ratio of the required number of iterations:\n$$\n\\frac{k_2}{k_1} \\approx \\frac{\\sqrt{\\frac{\\alpha cL}{\\varepsilon}} \\|x_0 - x^{\\star}\\|_{2}}{\\sqrt{\\frac{\\alpha L}{\\varepsilon}} \\|x_0 - x^{\\star}\\|_{2}} = \\sqrt{c}.\n$$\nThus, overestimating the Lipschitz constant by a factor of $c$ increases the number of iterations required to reach a given accuracy by a factor of $\\sqrt{c}$.\n\n**Illustration with Frobenius Norm**\n\nThe problem suggests an illustration where the tight Lipschitz constant $L = \\sigma_{\\max}(A)^2$ is replaced by the more easily computable upper bound $L' = \\|A\\|_F^2$. The Frobenius norm of a matrix $A$ is related to its singular values $\\sigma_i(A)$ by $\\|A\\|_F^2 = \\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2$. The largest singular value is $\\sigma_{\\max}(A) = \\sigma_1(A)$ (assuming sorted singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots$).\nThe overestimation factor $c$ is:\n$$\nc = \\frac{L'}{L} = \\frac{\\|A\\|_{F}^{2}}{\\sigma_{\\max}(A)^2} = \\frac{\\sum_{i=1}^{\\text{rank}(A)} \\sigma_i(A)^2}{\\sigma_1(A)^2} = 1 + \\frac{\\sum_{i=2}^{\\text{rank}(A)} \\sigma_i(A)^2}{\\sigma_1(A)^2}.\n$$\nSince $\\sigma_i(A)^2 \\geq 0$, we have $c \\geq 1$. The magnitude of $c$ depends on the distribution of the singular values.\n- If the matrix is numerically rank-$1$ or has a very steep decay in its singular values ($\\sigma_1 \\gg \\sigma_i$ for $i  1$), then the sum $\\sum_{i=2}^{\\text{rank}(A)} \\sigma_i(A)^2$ will be small compared to $\\sigma_1(A)^2$, and $c$ will be close to $1$. In this case, the slowdowns are negligible.\n- If the singular values are relatively flat (many $\\sigma_i$ are of similar magnitude to $\\sigma_1$), then $c$ can be large. In the extreme case where all non-zero singular values are equal, $c = \\text{rank}(A)$.\n- An \"ill-conditioned\" matrix (large $\\sigma_1/\\sigma_{\\text{rank}(A)}$) does not necessarily imply a large $c$. As seen above, a large $c$ is caused by a flat spectrum of singular values, not necessarily a large ratio between the largest and smallest. A well-conditioned matrix where all $\\sigma_i$ are equal results in the largest possible $c$ for a given rank.\n\nTherefore, using the Frobenius norm bound is most detrimental, leading to significant slowdowns of factors $c$ and $\\sqrt{c}$, when the sensing matrix $A$ has its \"energy\" (sum of squared singular values) distributed across many singular values rather than concentrated in a few dominant ones.\n\nThe two slowdown factors are $c$ for the error bound and $\\sqrt{c}$ for the iteration complexity.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nc  \\sqrt{c}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}