{
    "hands_on_practices": [
        {
            "introduction": "Before we can devise an algorithm to solve the Lasso problem, we must first understand what characterizes a solution. This first practice delves into the heart of convex optimization by deriving the optimality conditions for the Lasso problem. By understanding how the subgradient of the $\\ell_{1}$-norm dictates the properties of a solution, you will gain a foundational insight into why Lasso produces sparse models and how to verify if a candidate solution is truly optimal. The exercise culminates in a practical application: determining the regularization strength needed to make the simplest possible model—the zero vector—the correct answer. ",
            "id": "3470505",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (Lasso) problem, which is the composite convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. Starting from first principles of convex analysis for composite objectives, derive the optimality conditions by invoking the Karush-Kuhn-Tucker (KKT) conditions for this unconstrained composite problem. Explicitly characterize the subdifferential of the $\\ell_{1}$ norm and explain how the signs on the support of an optimal solution are determined by the subgradient. Then, apply your characterization to the specific instance with\n$$\nA=\\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 1 & 0 \\\\\n2 & -1 & 1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix},\n$$\nand determine the smallest value of $\\lambda$ (expressed as a real number) for which $x^{\\star}=\\mathbf{0}$ satisfies the KKT conditions and is therefore optimal. Provide the final answer as a single real number. No rounding is required.",
            "solution": "The problem asks for the derivation of the optimality conditions for the Lasso problem and then to find the smallest regularization parameter $\\lambda$ for which the zero vector is the optimal solution for a specific instance.\n\nThe Lasso objective function is a composite function of the form $F(x) = f(x) + g(x)$, where $x \\in \\mathbb{R}^n$. The two components are:\n1.  A smooth, convex, differentiable data fidelity term: $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$.\n2.  A non-smooth, convex, non-differentiable regularization term: $g(x) = \\lambda \\|x\\|_{1}$, with $\\lambda > 0$.\n\nFor a convex function $F(x)$, a point $x^{\\star}$ is a global minimizer if and only if the zero vector is contained in the subdifferential of $F$ at $x^{\\star}$. This is the first-order optimality condition and is a generalization of the Karush-Kuhn-Tucker (KKT) conditions to non-smooth unconstrained convex problems. The condition is:\n$$\n\\mathbf{0} \\in \\partial F(x^{\\star})\n$$\nSince $f(x)$ is convex and continuously differentiable and $g(x)$ is convex, the subdifferential of their sum is the sum of their subdifferentials:\n$$\n\\partial F(x) = \\partial f(x) + \\partial g(x)\n$$\nThe subdifferential of a differentiable function is the set containing only its gradient. The gradient of $f(x)$ is:\n$$\n\\nabla f(x) = A^T(A x - b)\n$$\nThus, $\\partial f(x) = \\{\\nabla f(x)\\}$. The subdifferential of $g(x) = \\lambda \\|x\\|_{1}$ is $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$.\n\nCombining these, the optimality condition for $x^{\\star}$ becomes:\n$$\n\\mathbf{0} \\in A^T(A x^{\\star} - b) + \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\nThis can be rewritten as:\n$$\nA^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\n\nNext, we characterize the subdifferential of the $\\ell_1$-norm, $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. Since this function is separable, its subdifferential is the Cartesian product of the subdifferentials of its individual components, $|x_i|$. The subdifferential of the absolute value function $h(z) = |z|$ at a point $z \\in \\mathbb{R}$ is:\n$$\n\\partial |z| = \\begin{cases} \\{\\text{sgn}(z)\\} & \\text{if } z \\neq 0 \\\\ [-1, 1] & \\text{if } z = 0 \\end{cases}\n$$\nTherefore, the subdifferential $\\partial \\|x\\|_{1}$ is the set of all vectors $s \\in \\mathbb{R}^n$ (called subgradients) whose components $s_i$ satisfy:\n$$\ns_i = \\begin{cases} \\text{sgn}(x_i) & \\text{if } x_i \\neq 0 \\\\ v_i \\in [-1, 1] & \\text{if } x_i = 0 \\end{cases}\n$$\n\nThe optimality condition $A^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$ means there must exist a subgradient $s^{\\star} \\in \\partial \\|x^{\\star}\\|_{1}$ such that $A^T(b - A x^{\\star}) = \\lambda s^{\\star}$. Analyzing this component-wise for $i=1, \\ldots, n$:\n\\begin{enumerate}\n    \\item If $x^{\\star}_i \\neq 0$, then $s^{\\star}_i = \\text{sgn}(x^{\\star}_i)$. The condition becomes $(A^T(b - A x^{\\star}))_i = \\lambda \\cdot \\text{sgn}(x^{\\star}_i)$. This implies that $|(A^T(b - A x^{\\star}))_i| = \\lambda$, and the sign of the non-zero component $x^{\\star}_i$ is determined by the sign of the corresponding component of the vector $A^T(b - A x^{\\star})$. Specifically, $\\text{sgn}(x^{\\star}_i) = \\frac{1}{\\lambda} (A^T(b-Ax^{\\star}))_i$.\n    \\item If $x^{\\star}_i = 0$, then $s^{\\star}_i \\in [-1, 1]$. The condition becomes $(A^T(b - A x^{\\star}))_i = \\lambda s^{\\star}_i$, which implies $|(A^T(b - A x^{\\star}))_i| \\leq \\lambda$.\n\\end{enumerate}\n\nNow, we apply this framework to determine the smallest value of $\\lambda > 0$ for which $x^{\\star} = \\mathbf{0}$ is an optimal solution. We substitute $x^{\\star} = \\mathbf{0}$ into the optimality condition. For every component $i$, we are in case 2, since $x^{\\star}_i = 0$. The condition becomes:\n$$\n| (A^T(b - A\\mathbf{0}))_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\nThis simplifies to:\n$$\n| (A^T b)_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\nThis set of inequalities must hold for all components $i$. This is equivalent to requiring that $\\lambda$ be greater than or equal to the maximum absolute value among all components of the vector $A^T b$. This maximum value is the $\\ell_{\\infty}$-norm of $A^T b$.\n$$\n\\lambda \\geq \\max_{i} |(A^T b)_i| = \\|A^T b\\|_{\\infty}\n$$\nThe smallest value of $\\lambda$ for which $x^{\\star} = \\mathbf{0}$ is optimal is therefore $\\lambda = \\|A^T b\\|_{\\infty}$.\n\nWe are given the specific instance:\n$$\nA=\\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 1 & 0 \\\\\n2 & -1 & 1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n$$\nFirst, we compute the transpose of $A$:\n$$\nA^T = \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 1 & -1 \\\\\n2 & -1 & 0 & 1\n\\end{pmatrix}\n$$\nNext, we compute the product $A^T b$:\n$$\nA^T b = \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 1 & -1 \\\\\n2 & -1 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n= \\begin{pmatrix}\n1(3) + 0(-2) + 1(1) + 2(4) \\\\\n0(3) + 1(-2) + 1(1) + (-1)(4) \\\\\n2(3) + (-1)(-2) + 0(1) + 1(4)\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 + 0 + 1 + 8 \\\\\n0 - 2 + 1 - 4 \\\\\n6 + 2 + 0 + 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\\n-5 \\\\\n12\n\\end{pmatrix}\n$$\nFinally, we calculate the $\\ell_{\\infty}$-norm of this vector to find the smallest $\\lambda$:\n$$\n\\lambda = \\|A^T b\\|_{\\infty} = \\max(|12|, |-5|, |12|) = \\max(12, 5, 12) = 12\n$$\nThus, the smallest value of $\\lambda$ for which $x^{\\star} = \\mathbf{0}$ is the optimal solution is $12$.",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "Proximal gradient methods rely on local information to make progress, and a crucial piece of that information is the curvature of the smooth part of our objective. This exercise guides you through the derivation of coordinate-wise Lipschitz constants, which quantify this curvature along each dimension. Understanding these constants is key to designing efficient algorithms with adaptive step sizes, such as coordinate descent, ensuring stable and rapid convergence. ",
            "id": "3470519",
            "problem": "Consider the composite objective in sparse estimation, where the smooth data fidelity term is quadratic and the nonsmooth penalty promotes sparsity. Let $g(x)$ denote the smooth term and $h(x)$ denote the nonsmooth term, with\n$$\ng(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}, \\quad h(x) = \\lambda \\|x\\|_{1},\n$$\nfor a matrix $A \\in \\mathbb{R}^{m \\times n}$, a vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda > 0$. Proximal gradient descent for the composite objective $g(x) + h(x)$ uses gradient information from $g(x)$ and the proximity operator of $h(x)$.\n\nUsing only the following foundational elements:\n- the definition of the gradient for a smooth function and its coordinate representation,\n- the definition of Lipschitz continuity of a scalar function and of the gradient mapping,\n- the proximity operator definition for the scaled absolute value,\nderive the coordinate-wise Lipschitz constants $L_{i}$ for the gradient of $g(x)$, defined as the smallest constant satisfying\n$$\n\\left|\\frac{\\partial g}{\\partial x_{i}}(x + t e_{i}) - \\frac{\\partial g}{\\partial x_{i}}(x)\\right| \\leq L_{i} |t| \\quad \\text{for all } x \\in \\mathbb{R}^{n}, \\; t \\in \\mathbb{R},\n$$\nwhere $e_{i}$ is the $i$-th standard basis vector in $\\mathbb{R}^{n}$. Explain how these constants determine adaptive coordinate-wise step sizes in proximal gradient descent for the Least Absolute Shrinkage and Selection Operator (LASSO), where $h(x) = \\lambda \\|x\\|_{1}$.\n\nThen, for the specific matrix\n$$\nA = \\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 2 & -1 \\\\\n1 & 2 & 1 \\\\\n1 & -1 & 0\n\\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3},\n$$\ncompute the numerical values of the coordinate-wise Lipschitz constants $L_{1}$, $L_{2}$, and $L_{3}$. Provide your final answer as a single row matrix containing $L_{1}$, $L_{2}$, and $L_{3}$. No rounding is necessary.",
            "solution": "**Derivation of Coordinate-wise Lipschitz Constants $L_i$**\n\nThe smooth part of the objective function is given by $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$. We can expand this expression as a quadratic form:\n$$\ng(x) = \\frac{1}{2} (Ax - b)^{T}(Ax - b) = \\frac{1}{2} (x^{T}A^{T} - b^{T})(Ax - b) = \\frac{1}{2} (x^{T}A^{T}Ax - 2b^{T}Ax + b^{T}b).\n$$\nThe gradient of $g(x)$ with respect to the vector $x$ is:\n$$\n\\nabla g(x) = \\frac{1}{2} (2A^{T}Ax - 2A^{T}b) = A^{T}(Ax - b).\n$$\nThe $i$-th partial derivative of $g(x)$, denoted $\\frac{\\partial g}{\\partial x_i}(x)$, is the $i$-th component of the gradient vector $\\nabla g(x)$. Let $A_i$ represent the $i$-th column of the matrix $A$. The $i$-th row of $A^T$ is then $A_i^T$. Thus, we can write the $i$-th partial derivative as:\n$$\n\\frac{\\partial g}{\\partial x_i}(x) = [\\nabla g(x)]_i = A_i^{T}(Ax - b).\n$$\nTo find the coordinate-wise Lipschitz constant $L_i$, we must analyze the change in this partial derivative when only the $i$-th coordinate of $x$ is perturbed. We evaluate $\\frac{\\partial g}{\\partial x_i}$ at the point $x + t e_i$, where $e_i$ is the $i$-th standard basis vector and $t \\in \\mathbb{R}$:\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) = A_i^{T}(A(x + t e_i) - b).\n$$\nUsing the linearity of the matrix-vector product, we get:\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) = A_i^{T}(Ax + t A e_i - b).\n$$\nBy definition, the product $A e_i$ extracts the $i$-th column of $A$, which is $A_i$. So,\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) = A_i^{T}(Ax - b + t A_i) = A_i^{T}(Ax - b) + t A_i^{T}A_i.\n$$\nNow, we compute the difference required by the definition of the Lipschitz constant:\n$$\n\\frac{\\partial g}{\\partial x_i}(x + t e_i) - \\frac{\\partial g}{\\partial x_i}(x) = \\left( A_i^{T}(Ax - b) + t A_i^{T}A_i \\right) - A_i^{T}(Ax - b) = t A_i^{T}A_i.\n$$\nThe term $A_i^{T}A_i$ is the dot product of the column vector $A_i$ with itself, which is equal to the squared Euclidean norm of $A_i$, i.e., $A_i^{T}A_i = \\|A_i\\|_2^2$. The difference is therefore $t \\|A_i\\|_2^2$.\nThe coordinate-wise Lipschitz condition is:\n$$\n\\left| \\frac{\\partial g}{\\partial x_i}(x + t e_i) - \\frac{\\partial g}{\\partial x_i}(x) \\right| \\leq L_i |t|.\n$$\nSubstituting our result for the difference:\n$$\n|t \\|A_i\\|_2^2| \\leq L_i |t|.\n$$\nSince $\\|A_i\\|_2^2 \\geq 0$, this simplifies to:\n$$\n|t| \\|A_i\\|_2^2 \\leq L_i |t|.\n$$\nFor any $t \\neq 0$, we can divide by $|t|$ to obtain $\\|A_i\\|_2^2 \\leq L_i$. The problem asks for the smallest constant $L_i$ that satisfies this inequality. The smallest such value is precisely the lower bound itself. Therefore, the coordinate-wise Lipschitz constant for the $i$-th partial derivative of $g(x)$ is:\n$$\nL_i = \\|A_i\\|_2^2.\n$$\n\n**Role of $L_i$ in Adaptive Step Sizes for LASSO**\n\nIn proximal coordinate descent for the LASSO objective $F(x) = g(x) + h(x) = \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1$, we iteratively update one coordinate at a time while keeping the others fixed. To update the $i$-th coordinate at iteration $k$, we seek to solve the one-dimensional subproblem:\n$$\nx_i^{(k+1)} = \\arg\\min_{u \\in \\mathbb{R}} F(x_1^{(k)}, \\dots, x_{i-1}^{(k)}, u, x_{i+1}^{(k)}, \\dots, x_n^{(k)}).\n$$\nThis subproblem is equivalent to minimizing an objective in terms of the change $d = u - x_i^{(k)}$:\n$$\n\\min_{d} g(x^{(k)} + de_i) + \\lambda|x_i^{(k)} + d| + \\lambda\\sum_{j \\neq i}|x_j^{(k)}|.\n$$\nThe $L_i$-Lipschitz continuity of the partial derivative $\\frac{\\partial g}{\\partial x_i}$ allows us to form a quadratic upper bound on $g(x)$ along the $i$-th coordinate direction (a consequence of the Descent Lemma):\n$$\ng(x^{(k)} + de_i) \\leq g(x^{(k)}) + d \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\frac{L_i}{2}d^2.\n$$\nSubstituting this upper bound into the subproblem, we minimize a majorizing function:\n$$\n\\min_d \\left( g(x^{(k)}) + d \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\frac{L_i}{2}d^2 + \\lambda|x_i^{(k)} + d| \\right).\n$$\nDropping constant terms and letting $u = x_i^{(k)} + d$, we are solving:\n$$\n\\min_u \\left( (u - x_i^{(k)}) \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\frac{L_i}{2}(u - x_i^{(k)})^2 + \\lambda|u| \\right).\n$$\nRearranging terms to match the form of a proximity operator problem:\n$$\n\\min_u \\left( \\frac{L_i}{2} \\left( u^2 - 2u x_i^{(k)} + (x_i^{(k)})^2 \\right) + u \\frac{\\partial g}{\\partial x_i}(x^{(k)}) + \\lambda|u| + \\dots \\right).\n$$\nThis is equivalent to minimizing:\n$$\n\\min_u \\left( \\frac{L_i}{2} \\left(u - \\left(x_i^{(k)} - \\frac{1}{L_i}\\frac{\\partial g}{\\partial x_i}(x^{(k)})\\right)\\right)^2 + \\lambda|u| \\right).\n$$\nThis is precisely the definition of the scaled proximity operator of $h_i(u) = \\lambda|u|$:\n$$\nx_i^{(k+1)} = \\text{prox}_{\\frac{\\lambda}{L_i}|\\cdot|}\\left(x_i^{(k)} - \\frac{1}{L_i}\\frac{\\partial g}{\\partial x_i}(x^{(k)}) \\right).\n$$\nThe proximity operator for the scaled L1-norm is the soft-thresholding operator, $S_{\\alpha}(z) = \\text{sgn}(z)\\max(|z|-\\alpha, 0)$. Thus, the update rule is:\n$$\nx_i^{(k+1)} = S_{\\lambda/L_i}\\left(x_i^{(k)} - \\frac{1}{L_i}\\frac{\\partial g}{\\partial x_i}(x^{(k)})\\right).\n$$\nThis is a proximal gradient update for the $i$-th coordinate using a step size of $\\gamma_i = 1/L_i$. The coordinate-wise Lipschitz constant $L_i$ determines the inverse of the adaptive step size for each coordinate. A larger $L_i$ indicates a greater curvature of the objective function along the $i$-th coordinate axis, necessitating a smaller, more cautious step size $\\gamma_i$ to ensure convergence.\n\n**Numerical Computation for the given Matrix A**\n\nThe given matrix $A \\in \\mathbb{R}^{4 \\times 3}$ is:\n$$\nA = \\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 2 & -1 \\\\ 1 & 2 & 1 \\\\ 1 & -1 & 0 \\end{pmatrix}.\n$$\nThe columns of $A$ are:\n$$\nA_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad A_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad A_3 = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nWe compute the coordinate-wise Lipschitz constants $L_i = \\|A_i\\|_2^2$ for $i=1, 2, 3$.\n\nFor $i=1$:\n$$\nL_1 = \\|A_1\\|_2^2 = 1^2 + 0^2 + 1^2 + 1^2 = 1 + 0 + 1 + 1 = 3.\n$$\n\nFor $i=2$:\n$$\nL_2 = \\|A_2\\|_2^2 = 0^2 + 2^2 + 2^2 + (-1)^2 = 0 + 4 + 4 + 1 = 9.\n$$\n\nFor $i=3$:\n$$\nL_3 = \\|A_3\\|_2^2 = 2^2 + (-1)^2 + 1^2 + 0^2 = 4 + 1 + 1 + 0 = 6.\n$$\nThe numerical values of the coordinate-wise Lipschitz constants are $L_1=3$, $L_2=9$, and $L_3=6.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & 9 & 6\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In practice, iterative algorithms need a clear stopping criterion. This exercise introduces the proximal gradient mapping, a vector that acts as a \"residual\" for the optimality conditions. You will see that this mapping becomes zero precisely at the optimal solution, making its norm an excellent measure of convergence. By computing this mapping for a concrete example, you will develop a practical feel for how violations of optimality are quantified and monitored. ",
            "id": "3470531",
            "problem": "Consider the composite optimization problem of the Least Absolute Shrinkage and Selection Operator (Lasso), which minimizes $F(x) = f(x) + g(x)$ over $x \\in \\mathbb{R}^{n}$, where $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$. Let the proximal operator of a proper, closed, convex function $g$ be defined by\n$$\n\\operatorname{prox}_{\\tau g}(z) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ g(u) + \\frac{1}{2 \\tau}\\|u - z\\|_{2}^{2} \\right\\},\n$$\nand let the proximal gradient mapping for a step size $\\alpha > 0$ be defined by\n$$\nG_{\\alpha}(x) \\triangleq \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big)\\right).\n$$\nYou are given the Lasso instance with\n$$\nA = \\begin{pmatrix}\n1 & 2 & -1 \\\\\n-2 & 1 & 1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}, \\quad\n\\lambda = 1, \\quad\n\\alpha = \\frac{1}{2}, \\quad\nx = \\begin{pmatrix}\n2 \\\\ -1 \\\\ 0\n\\end{pmatrix}.\n$$\n(a) Using only the definitions above and fundamental properties of convex analysis, derive an explicit expression for $G_{\\alpha}(x)$ for the Lasso in terms of the coordinate-wise soft-thresholding of $x - \\alpha \\nabla f(x)$, and identify the threshold parameter in terms of $\\alpha$ and $\\lambda$.\n\n(b) For the data given, compute the value of $\\|G_{\\alpha}(x)\\|_{\\infty}$ exactly.\n\n(c) Explain briefly how the magnitude of the components of $G_{\\alpha}(x)$ quantifies violations of the subgradient optimality condition $0 \\in A^{\\top}(A x - b) + \\lambda \\,\\partial \\|x\\|_{1}$ at $x$. Your final reported answer must be only the value requested in part (b). No rounding is required; report the exact value without units.",
            "solution": "(a) Derivation of the explicit expression for $G_{\\alpha}(x)$ for the Lasso problem.\n\nThe proximal gradient mapping is defined as $G_{\\alpha}(x) \\triangleq \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big)\\right)$. To derive its explicit form for the Lasso, we must first determine expressions for $\\nabla f(x)$ and $\\operatorname{prox}_{\\alpha g}(z)$.\n\nFirst, we find the gradient of the smooth part, $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$.\n$f(x) = \\frac{1}{2}(A x - b)^{\\top}(A x - b) = \\frac{1}{2}(x^{\\top}A^{\\top} - b^{\\top})(A x - b) = \\frac{1}{2}(x^{\\top}A^{\\top}A x - 2 x^{\\top}A^{\\top}b + b^{\\top}b)$.\nThe gradient with respect to $x$ is:\n$$\n\\nabla f(x) = \\frac{1}{2}(2 A^{\\top}A x - 2 A^{\\top}b) = A^{\\top}(A x - b).\n$$\nNext, we derive the proximal operator for the function $g(x) = \\lambda \\|x\\|_1$, with a parameter $\\tau = \\alpha$. By definition:\n$$\n\\operatorname{prox}_{\\alpha g}(z) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\alpha g(u) + \\frac{1}{2}\\|u - z\\|_{2}^{2} \\right\\} = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\alpha \\lambda \\|u\\|_{1} + \\frac{1}{2}\\|u - z\\|_{2}^{2} \\right\\}.\n$$\nThe objective function is separable with respect to the components of $u$. We can write $\\|u\\|_1 = \\sum_{i=1}^n |u_i|$ and $\\|u-z\\|_2^2 = \\sum_{i=1}^n (u_i-z_i)^2$. Thus, we can minimize for each component $u_i$ independently:\n$$\n\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\alpha \\lambda |u_i| + \\frac{1}{2}(u_i - z_i)^2 \\right\\}.\n$$\nThe first-order optimality condition is found using subgradient calculus. The subgradient of the objective with respect to $u_i$ is $\\alpha \\lambda \\partial|u_i| + (u_i - z_i)$. Setting this to contain $0$ gives $z_i - u_i \\in \\alpha \\lambda \\partial|u_i|$. The subdifferential of the absolute value function is $\\partial|u_i| = \\operatorname{sign}(u_i)$ if $u_i \\ne 0$ and $\\partial|u_i| = [-1, 1]$ if $u_i = 0$.\n- If $u_i > 0$, the condition becomes $z_i - u_i = \\alpha\\lambda$, so $u_i = z_i - \\alpha\\lambda$. This is consistent only if $z_i - \\alpha\\lambda > 0$, i.e., $z_i > \\alpha\\lambda$.\n- If $u_i < 0$, the condition becomes $z_i - u_i = -\\alpha\\lambda$, so $u_i = z_i + \\alpha\\lambda$. This is consistent only if $z_i + \\alpha\\lambda < 0$, i.e., $z_i < -\\alpha\\lambda$.\n- If $u_i = 0$, the condition is $z_i \\in \\alpha\\lambda [-1, 1]$, i.e., $|z_i| \\le \\alpha\\lambda$.\n\nCombining these cases, the solution $u_i$ is given by:\n$$\nu_i = \\begin{cases} z_i - \\alpha \\lambda & \\text{if } z_i > \\alpha \\lambda \\\\ 0 & \\text{if } |z_i| \\le \\alpha \\lambda \\\\ z_i + \\alpha \\lambda & \\text{if } z_i < -\\alpha \\lambda \\end{cases}.\n$$\nThis is the coordinate-wise soft-thresholding operator, commonly denoted as $S_{\\kappa}(z)$, where the threshold parameter is $\\kappa = \\alpha\\lambda$. A compact expression is $S_{\\kappa}(z_i) = \\operatorname{sign}(z_i) \\max(0, |z_i| - \\kappa)$. So, $\\operatorname{prox}_{\\alpha g}(z) = S_{\\alpha\\lambda}(z)$.\n\nSubstituting these expressions back into the definition of $G_{\\alpha}(x)$, we get:\n$$\nG_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - S_{\\alpha\\lambda}\\big(x - \\alpha A^{\\top}(Ax - b)\\big)\\right).\n$$\nThis is the explicit expression for the proximal gradient mapping for Lasso in terms of coordinate-wise soft-thresholding. The threshold parameter is $\\alpha\\lambda$.\n\n(b) Computation of $\\|G_{\\alpha}(x)\\|_{\\infty}$ for the given data.\n\nWe are given the data:\n$A = \\begin{pmatrix} 1 & 2 & -1 \\\\ -2 & 1 & 1 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$, $\\lambda = 1$, $\\alpha = \\frac{1}{2}$, and $x = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n\nStep 1: Compute the gradient $\\nabla f(x) = A^{\\top}(Ax - b)$.\nFirst, compute the residual $r = Ax - b$:\n$$\nAx = \\begin{pmatrix} 1 & 2 & -1 \\\\ -2 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (2)(-1) + (-1)(0) \\\\ (-2)(2) + (1)(-1) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -5 \\end{pmatrix}.\n$$\n$$\nr = Ax - b = \\begin{pmatrix} 0 \\\\ -5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix}.\n$$\nNow, compute the gradient:\n$$\n\\nabla f(x) = A^{\\top}r = \\begin{pmatrix} 1 & -2 \\\\ 2 & 1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (-2)(-2) \\\\ (2)(-1) + (1)(-2) \\\\ (-1)(-1) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -4 \\\\ -1 \\end{pmatrix}.\n$$\n\nStep 2: Compute the argument of the proximal operator, $z = x - \\alpha \\nabla f(x)$.\nWith $\\alpha = \\frac{1}{2}$:\n$$\nz = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 3 \\\\ -4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{3}{2} \\\\ -1 + 2 \\\\ 0 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\n\nStep 3: Apply the soft-thresholding operator $\\operatorname{prox}_{\\alpha g}(z) = S_{\\alpha\\lambda}(z)$.\nThe threshold parameter is $\\kappa = \\alpha\\lambda = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}$.\nWe apply $S_{1/2}(z)$ component-wise:\n$$\n(S_{1/2}(z))_1 = \\operatorname{sign}(z_1) \\max(0, |z_1| - \\kappa) = \\operatorname{sign}(\\frac{1}{2}) \\max(0, |\\frac{1}{2}| - \\frac{1}{2}) = 1 \\cdot 0 = 0.\n$$\n$$\n(S_{1/2}(z))_2 = \\operatorname{sign}(z_2) \\max(0, |z_2| - \\kappa) = \\operatorname{sign}(1) \\max(0, |1| - \\frac{1}{2}) = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}.\n$$\n$$\n(S_{1/2}(z))_3 = \\operatorname{sign}(z_3) \\max(0, |z_3| - \\kappa) = \\operatorname{sign}(\\frac{1}{2}) \\max(0, |\\frac{1}{2}| - \\frac{1}{2}) = 1 \\cdot 0 = 0.\n$$\nSo, $\\operatorname{prox}_{\\alpha g}(z) = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}$.\n\nStep 4: Compute $G_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}(z)\\right)$.\n$$\nG_{\\alpha}(x) = \\frac{1}{1/2} \\left( \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\right) = 2 \\begin{pmatrix} 2 \\\\ -1 - \\frac{1}{2} \\\\ 0 \\end{pmatrix} = 2 \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -3 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep 5: Compute the infinity norm $\\|G_{\\alpha}(x)\\|_{\\infty}$.\n$$\n\\|G_{\\alpha}(x)\\|_{\\infty} = \\max(|4|, |-3|, |0|) = \\max(4, 3, 0) = 4.\n$$\n\n(c) Explanation of how $G_{\\alpha}(x)$ quantifies violations of the subgradient optimality condition.\n\nThe first-order necessary and sufficient optimality condition for the convex problem $\\min_x F(x) = f(x) + g(x)$ is that $x^*$ is a minimizer if and only if $0 \\in \\partial F(x^*)$, where $\\partial F(x^*)$ is the subdifferential of $F$ at $x^*$. For the Lasso problem, this condition is $0 \\in \\nabla f(x^*) + \\partial g(x^*)$, which translates to:\n$$\n0 \\in A^{\\top}(A x^* - b) + \\lambda \\,\\partial \\|x^*\\|_{1}.\n$$\nThe proximal gradient method generates a sequence $x^{k+1} = \\operatorname{prox}_{\\alpha g}(x^k - \\alpha \\nabla f(x^k))$. A point $x^*$ is a minimizer if and only if it is a fixed point of this iteration, i.e., $x^* = \\operatorname{prox}_{\\alpha g}(x^* - \\alpha \\nabla f(x^*))$. This fixed-point equation is equivalent to the optimality condition. To see this, recall from part (a) that $u = \\operatorname{prox}_{\\tau g}(z)$ is equivalent to $z - u \\in \\tau \\partial g(u)$. Applying this to the fixed-point equation with $u=x^*$, $z=x^*-\\alpha \\nabla f(x^*)$, and $\\tau=\\alpha$, we get:\n$$\n(x^* - \\alpha \\nabla f(x^*)) - x^* \\in \\alpha \\, \\partial g(x^*),\n$$\nwhich simplifies to $-\\alpha \\nabla f(x^*) \\in \\alpha \\, \\partial g(x^*)$. Since $\\alpha > 0$, this is equivalent to $-\\nabla f(x^*) \\in \\partial g(x^*)$, or $0 \\in \\nabla f(x^*) + \\partial g(x^*)$.\n\nThe proximal gradient mapping is $G_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x))\\right)$. A point $x$ is a fixed point of the proximal gradient iteration if and only if $G_{\\alpha}(x) = 0$. Therefore, the condition $G_{\\alpha}(x) = 0$ is equivalent to the subgradient optimality condition being satisfied at $x$.\n\nConsequently, $G_{\\alpha}(x)$ serves as a residual or a measure of how far the point $x$ is from satisfying the optimality condition. The magnitude of $G_{\\alpha}(x)$ (in any norm, such as the infinity norm) quantifies the violation of this condition. A non-zero component $(G_{\\alpha}(x))_i$ indicates that the $i$-th coordinate of $x$ does not satisfy the fixed-point condition, and thus the optimality condition is violated with respect to that coordinate. In iterative algorithms, $\\|G_{\\alpha}(x)\\|$ is often used as a termination criterion; the algorithm stops when this value is below a small tolerance, indicating that the current iterate is sufficiently close to an optimal point.",
            "answer": "$$\n\\boxed{4}\n$$"
        }
    ]
}