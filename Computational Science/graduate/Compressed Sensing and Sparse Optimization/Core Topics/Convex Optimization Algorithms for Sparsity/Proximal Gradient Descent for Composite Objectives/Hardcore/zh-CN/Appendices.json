{
    "hands_on_practices": [
        {
            "introduction": "在尝试用算法求解一个优化问题之前，我们必须首先精确地理解解的特征。本练习将指导你推导 LASSO 问题的 Karush-Kuhn-Tucker (KKT) 条件，这是对最优解的严格数学刻画 。通过完成这一实践，你将能够理解近端梯度下降算法所要达成的最终目标，并学会如何通过分析 KKT 条件来确定问题参数（如正则化系数 $\\lambda$）如何影响解的稀疏性。",
            "id": "3470505",
            "problem": "考虑最小绝对值收敛和选择算子（Lasso）问题，它是一个复合凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda  0$。从复合目标凸分析的基本原理出发，通过引用此无约束复合问题的 Karush-Kuhn-Tucker (KKT) 条件，推导出最优性条件。明确地刻画 $\\ell_{1}$ 范数的次微分，并解释最优解的非零分量（支撑集上）的符号是如何由次梯度决定的。然后，将你的刻画应用于以下具体实例：\n$$\nA=\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0 \\\\\n2  -1  1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix},\n$$\n并确定使得 $x^{\\star}=\\mathbf{0}$ 满足 KKT 条件并因此成为最优解的最小 $\\lambda$ 值（表示为一个实数）。将最终答案以单个实数的形式给出。无需四舍五入。",
            "solution": "该问题要求推导 Lasso 问题的最优性条件，然后找出一个具体实例中使得零向量成为最优解的最小正则化参数 $\\lambda$。\n\nLasso 目标函数是一个形如 $F(x) = f(x) + g(x)$ 的复合函数，其中 $x \\in \\mathbb{R}^n$。其两个组成部分是：\n1.  一个光滑、凸、可微的数据保真项：$f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$。\n2.  一个非光滑、凸、不可微的正则化项：$g(x) = \\lambda \\|x\\|_{1}$，其中 $\\lambda  0$。\n\n对于一个凸函数 $F(x)$，一个点 $x^{\\star}$ 是全局极小值点当且仅当零向量包含在 $F$ 于 $x^{\\star}$ 处的次微分中。这是一阶最优性条件，也是 Karush-Kuhn-Tucker (KKT) 条件对非光滑无约束凸问题的推广。该条件是：\n$$\n\\mathbf{0} \\in \\partial F(x^{\\star})\n$$\n由于 $f(x)$ 是凸且连续可微的，而 $g(x)$ 是凸的，它们的和的次微分等于它们次微分的和：\n$$\n\\partial F(x) = \\partial f(x) + \\partial g(x)\n$$\n一个可微函数的次微分是只包含其梯度的集合。$f(x)$ 的梯度是：\n$$\n\\nabla f(x) = A^T(A x - b)\n$$\n因此，$\\partial f(x) = \\{\\nabla f(x)\\}$。$g(x) = \\lambda \\|x\\|_{1}$ 的次微分是 $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$。\n\n综合这些，$x^{\\star}$ 的最优性条件变为：\n$$\n\\mathbf{0} \\in A^T(A x^{\\star} - b) + \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\n这可以重写为：\n$$\nA^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\n\n接下来，我们刻画 $\\ell_1$ 范数 $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$ 的次微分。由于这个函数是可分离的，其次微分是其各个分量 $|x_i|$ 的次微分的笛卡尔积。绝对值函数 $h(z) = |z|$ 在点 $z \\in \\mathbb{R}$ 处的次微分是：\n$$\n\\partial |z| = \\begin{cases} \\{\\text{sgn}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\n因此，次微分 $\\partial \\|x\\|_{1}$ 是所有向量 $s \\in \\mathbb{R}^n$（称为次梯度）的集合，其中分量 $s_i$ 满足：\n$$\ns_i = \\begin{cases} \\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\ v_i \\in [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\n\n最优性条件 $A^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$ 意味着必须存在一个次梯度 $s^{\\star} \\in \\partial \\|x^{\\star}\\|_{1}$ 使得 $A^T(b - A x^{\\star}) = \\lambda s^{\\star}$。对 $i=1, \\ldots, n$ 逐分量分析此条件：\n\\begin{enumerate}\n    \\item 如果 $x^{\\star}_i \\neq 0$，那么 $s^{\\star}_i = \\text{sgn}(x^{\\star}_i)$。条件变为 $(A^T(b - A x^{\\star}))_i = \\lambda \\cdot \\text{sgn}(x^{\\star}_i)$。这意味着 $|(A^T(b - A x^{\\star}))_i| = \\lambda$，并且非零分量 $x^{\\star}_i$ 的符号由向量 $A^T(b - A x^{\\star})$ 相应分量的符号决定。具体来说，$\\text{sgn}(x^{\\star}_i) = \\frac{1}{\\lambda} (A^T(b-Ax^{\\star}))_i$。\n    \\item 如果 $x^{\\star}_i = 0$，那么 $s^{\\star}_i \\in [-1, 1]$。条件变为 $(A^T(b - A x^{\\star}))_i = \\lambda s^{\\star}_i$，这意味着 $|(A^T(b - A x^{\\star}))_i| \\leq \\lambda$。\n\\end{enumerate}\n\n现在，我们应用这个框架来确定使得 $x^{\\star} = \\mathbf{0}$ 成为最优解的最小 $\\lambda  0$ 值。我们将 $x^{\\star} = \\mathbf{0}$ 代入最优性条件。对于每个分量 $i$，我们都属于情况 2，因为 $x^{\\star}_i = 0$。条件变为：\n$$\n| (A^T(b - A\\mathbf{0}))_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\n这可以简化为：\n$$\n| (A^T b)_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\n这组不等式必须对所有分量 $i$ 成立。这等价于要求 $\\lambda$ 大于或等于向量 $A^T b$ 所有分量中的最大绝对值。这个最大值就是 $A^T b$ 的 $\\ell_{\\infty}$ 范数。\n$$\n\\lambda \\geq \\max_{i} |(A^T b)_i| = \\|A^T b\\|_{\\infty}\n$$\n因此，使得 $x^{\\star} = \\mathbf{0}$ 为最优解的最小 $\\lambda$ 值是 $\\lambda = \\|A^T b\\|_{\\infty}$。\n\n我们得到具体实例：\n$$\nA=\\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0 \\\\\n2  -1  1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n$$\n首先，我们计算 $A$ 的转置：\n$$\nA^T = \\begin{pmatrix}\n1  0  1  2 \\\\\n0  1  1  -1 \\\\\n2  -1  0  1\n\\end{pmatrix}\n$$\n接下来，我们计算乘积 $A^T b$：\n$$\nA^T b = \\begin{pmatrix}\n1  0  1  2 \\\\\n0  1  1  -1 \\\\\n2  -1  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n= \\begin{pmatrix}\n1(3) + 0(-2) + 1(1) + 2(4) \\\\\n0(3) + 1(-2) + 1(1) + (-1)(4) \\\\\n2(3) + (-1)(-2) + 0(1) + 1(4)\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 + 0 + 1 + 8 \\\\\n0 - 2 + 1 - 4 \\\\\n6 + 2 + 0 + 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\\n-5 \\\\\n12\n\\end{pmatrix}\n$$\n最后，我们计算该向量的 $\\ell_{\\infty}$ 范数以找到最小的 $\\lambda$：\n$$\n\\lambda = \\|A^T b\\|_{\\infty} = \\max(|12|, |-5|, |12|) = \\max(12, 5, 12) = 12\n$$\n因此，使得 $x^{\\star} = \\mathbf{0}$ 成为最优解的最小 $\\lambda$ 值是 $12$。",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "近端梯度映射 $G_{\\alpha}(x)$ 是一个核心概念，它衡量了当前点 $x$ 在多大程度上违反了最优性条件。本练习要求你为一个具体的 LASSO 实例计算这个映射，从而让你对这一概念有一个定量的认识 。理解 $G_{\\alpha}(x)$ 的大小如何量化与最优解的“距离”，对于设计算法的终止准则和诊断收敛性至关重要。",
            "id": "3470531",
            "problem": "考虑最小绝对收缩和选择算子 (Lasso) 的复合优化问题，该问题在 $x \\in \\mathbb{R}^{n}$ 上最小化 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ 且 $g(x) = \\lambda \\|x\\|_{1}$。设一个正常、闭、凸函数 $g$ 的临近算子定义为\n$$\n\\operatorname{prox}_{\\tau g}(z) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ g(u) + \\frac{1}{2 \\tau}\\|u - z\\|_{2}^{2} \\right\\},\n$$\n并设步长为 $\\alpha  0$ 的临近梯度映射定义为\n$$\nG_{\\alpha}(x) \\triangleq \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big)\\right).\n$$\n给定以下 Lasso 实例\n$$\nA = \\begin{pmatrix}\n1  2  -1 \\\\\n-2  1  1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ -3\n\\end{pmatrix}, \\quad\n\\lambda = 1, \\quad\n\\alpha = \\frac{1}{2}, \\quad\nx = \\begin{pmatrix}\n2 \\\\ -1 \\\\ 0\n\\end{pmatrix}.\n$$\n(a) 仅使用上述定义和凸分析的基本性质，推导 Lasso 问题的 $G_{\\alpha}(x)$ 的显式表达式，该表达式用 $x - \\alpha \\nabla f(x)$ 的坐标级软阈值表示，并用 $\\alpha$ 和 $\\lambda$ 确定阈值参数。\n\n(b) 对于给定的数据，精确计算 $\\|G_{\\alpha}(x)\\|_{\\infty}$ 的值。\n\n(c) 简要解释 $G_{\\alpha}(x)$ 各分量的大小如何量化在点 $x$ 处对次梯度最优性条件 $0 \\in A^{\\top}(A x - b) + \\lambda \\,\\partial \\|x\\|_{1}$ 的违反程度。您最终报告的答案必须仅为 (b) 部分要求的值。无需四舍五入；报告精确值，不带单位。",
            "solution": "该问题要求对一个特定 Lasso 实例的临近梯度映射进行三部分分析。我们首先验证问题陈述，发现其是适定的、有科学依据且自洽的。我们继续进行求解。\n\n(a) 推导 Lasso 问题的 $G_{\\alpha}(x)$ 的显式表达式。\n\n临近梯度映射定义为 $G_{\\alpha}(x) \\triangleq \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big)\\right)$。为了推导其对 Lasso 问题的显式形式，我们必须首先确定 $\\nabla f(x)$ 和 $\\operatorname{prox}_{\\alpha g}(z)$ 的表达式。\n\n首先，我们求光滑部分 $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 的梯度。\n$f(x) = \\frac{1}{2}(A x - b)^{\\top}(A x - b) = \\frac{1}{2}(x^{\\top}A^{\\top} - b^{\\top})(A x - b) = \\frac{1}{2}(x^{\\top}A^{\\top}A x - 2 x^{\\top}A^{\\top}b + b^{\\top}b)$。\n关于 $x$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{2}(2 A^{\\top}A x - 2 A^{\\top}b) = A^{\\top}(A x - b).\n$$\n接下来，我们推导函数 $g(x) = \\lambda \\|x\\|_1$ 的临近算子，其中参数为 $\\tau = \\alpha$。根据定义：\n$$\n\\operatorname{prox}_{\\alpha g}(z) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\alpha g(u) + \\frac{1}{2}\\|u - z\\|_{2}^{2} \\right\\} = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\alpha \\lambda \\|u\\|_{1} + \\frac{1}{2}\\|u - z\\|_{2}^{2} \\right\\}.\n$$\n目标函数关于 $u$ 的分量是可分的。我们可以写出 $\\|u\\|_1 = \\sum_{i=1}^n |u_i|$ 和 $\\|u-z\\|_2^2 = \\sum_{i=1}^n (u_i-z_i)^2$。因此，我们可以对每个分量 $u_i$ 独立地进行最小化：\n$$\n\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\alpha \\lambda |u_i| + \\frac{1}{2}(u_i - z_i)^2 \\right\\}.\n$$\n一阶最优性条件通过次梯度微积分找到。目标函数关于 $u_i$ 的次梯度是 $\\alpha \\lambda \\partial|u_i| + (u_i - z_i)$。令其包含 $0$ 可得 $z_i - u_i \\in \\alpha \\lambda \\partial|u_i|$。绝对值函数的次微分是 $\\partial|u_i| = \\operatorname{sign}(u_i)$ 如果 $u_i \\ne 0$，以及 $\\partial|u_i| = [-1, 1]$ 如果 $u_i = 0$。\n- 如果 $u_i > 0$，条件变为 $z_i - u_i = \\alpha\\lambda$，所以 $u_i = z_i - \\alpha\\lambda$。这仅当 $z_i - \\alpha\\lambda > 0$，即 $z_i > \\alpha\\lambda$ 时才成立。\n- 如果 $u_i  0$，条件变为 $z_i - u_i = -\\alpha\\lambda$，所以 $u_i = z_i + \\alpha\\lambda$。这仅当 $z_i + \\alpha\\lambda  0$，即 $z_i  -\\alpha\\lambda$ 时才成立。\n- 如果 $u_i = 0$，条件是 $z_i \\in \\alpha\\lambda [-1, 1]$，即 $|z_i| \\le \\alpha\\lambda$。\n\n综合这些情况，解 $u_i$ 由下式给出：\n$$\nu_i = \\begin{cases} z_i - \\alpha \\lambda  \\text{若 } z_i  \\alpha \\lambda \\\\ 0  \\text{若 } |z_i| \\le \\alpha \\lambda \\\\ z_i + \\alpha \\lambda  \\text{若 } z_i  -\\alpha \\lambda \\end{cases}.\n$$\n这是坐标级软阈值算子，通常记作 $S_{\\kappa}(z)$，其中阈值参数为 $\\kappa = \\alpha\\lambda$。一个紧凑的表达式是 $S_{\\kappa}(z_i) = \\operatorname{sign}(z_i) \\max(0, |z_i| - \\kappa)$。所以，$\\operatorname{prox}_{\\alpha g}(z) = S_{\\alpha\\lambda}(z)$。\n\n将这些表达式代回 $G_{\\alpha}(x)$ 的定义中，我们得到：\n$$\nG_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - S_{\\alpha\\lambda}\\big(x - \\alpha A^{\\top}(Ax - b)\\big)\\right).\n$$\n这就是 Lasso 问题的临近梯度映射以坐标级软阈值表示的显式表达式。阈值参数是 $\\alpha\\lambda$。\n\n(b) 对于给定的数据，计算 $\\|G_{\\alpha}(x)\\|_{\\infty}$。\n\n给定数据为：\n$A = \\begin{pmatrix} 1  2  -1 \\\\ -2  1  1 \\end{pmatrix}$，$b = \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix}$，$\\lambda = 1$，$\\alpha = \\frac{1}{2}$，以及 $x = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$。\n\n步骤 1：计算梯度 $\\nabla f(x) = A^{\\top}(Ax - b)$。\n首先，计算残差 $r = Ax - b$：\n$$\nAx = \\begin{pmatrix} 1  2  -1 \\\\ -2  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (2)(-1) + (-1)(0) \\\\ (-2)(2) + (1)(-1) + (1)(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -5 \\end{pmatrix}.\n$$\n$$\nr = Ax - b = \\begin{pmatrix} 0 \\\\ -5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix}.\n$$\n现在，计算梯度：\n$$\n\\nabla f(x) = A^{\\top}r = \\begin{pmatrix} 1  -2 \\\\ 2  1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} (1)(-1) + (-2)(-2) \\\\ (2)(-1) + (1)(-2) \\\\ (-1)(-1) + (1)(-2) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -4 \\\\ -1 \\end{pmatrix}.\n$$\n\n步骤 2：计算临近算子的自变量 $z = x - \\alpha \\nabla f(x)$。\n当 $\\alpha = \\frac{1}{2}$ 时：\n$$\nz = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 3 \\\\ -4 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{3}{2} \\\\ -1 + 2 \\\\ 0 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{1}{2} \\end{pmatrix}.\n$$\n\n步骤 3：应用软阈值算子 $\\operatorname{prox}_{\\alpha g}(z) = S_{\\alpha\\lambda}(z)$。\n阈值参数为 $\\kappa = \\alpha\\lambda = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}$。\n我们逐分量地应用 $S_{1/2}(z)$：\n$$\n(S_{1/2}(z))_1 = \\operatorname{sign}(z_1) \\max(0, |z_1| - \\kappa) = \\operatorname{sign}(\\frac{1}{2}) \\max(0, |\\frac{1}{2}| - \\frac{1}{2}) = 1 \\cdot 0 = 0.\n$$\n$$\n(S_{1/2}(z))_2 = \\operatorname{sign}(z_2) \\max(0, |z_2| - \\kappa) = \\operatorname{sign}(1) \\max(0, |1| - \\frac{1}{2}) = 1 \\cdot \\frac{1}{2} = \\frac{1}{2}.\n$$\n$$\n(S_{1/2}(z))_3 = \\operatorname{sign}(z_3) \\max(0, |z_3| - \\kappa) = \\operatorname{sign}(\\frac{1}{2}) \\max(0, |\\frac{1}{2}| - \\frac{1}{2}) = 1 \\cdot 0 = 0.\n$$\n所以，$\\operatorname{prox}_{\\alpha g}(z) = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix}$。\n\n步骤 4：计算 $G_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}(z)\\right)$。\n$$\nG_{\\alpha}(x) = \\frac{1}{1/2} \\left( \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\right) = 2 \\begin{pmatrix} 2 \\\\ -1 - \\frac{1}{2} \\\\ 0 \\end{pmatrix} = 2 \\begin{pmatrix} 2 \\\\ -\\frac{3}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -3 \\\\ 0 \\end{pmatrix}.\n$$\n\n步骤 5：计算无穷范数 $\\|G_{\\alpha}(x)\\|_{\\infty}$。\n$$\n\\|G_{\\alpha}(x)\\|_{\\infty} = \\max(|4|, |-3|, |0|) = \\max(4, 3, 0) = 4.\n$$\n\n(c) 解释 $G_{\\alpha}(x)$ 如何量化对次梯度最优性条件的违反程度。\n\n对于凸问题 $\\min_x F(x) = f(x) + g(x)$，一阶充分必要最优性条件是 $x^*$ 是一个最小化子当且仅当 $0 \\in \\partial F(x^*)$，其中 $\\partial F(x^*)$ 是 $F$ 在 $x^*$ 处的次微分。对于 Lasso 问题，此条件是 $0 \\in \\nabla f(x^*) + \\partial g(x^*)$，这等价于：\n$$\n0 \\in A^{\\top}(A x^* - b) + \\lambda \\,\\partial \\|x^*\\|_{1}.\n$$\n临近梯度法生成序列 $x^{k+1} = \\operatorname{prox}_{\\alpha g}(x^k - \\alpha \\nabla f(x^k))$。一个点 $x^*$ 是最小化子当且仅当它是此迭代的不动点，即 $x^* = \\operatorname{prox}_{\\alpha g}(x^* - \\alpha \\nabla f(x^*))$。这个不动点方程等价于最优性条件。为了证明这一点，回顾 (a) 部分，$u = \\operatorname{prox}_{\\tau g}(z)$ 等价于 $z - u \\in \\tau \\partial g(u)$。将此应用于不动点方程，其中 $u=x^*$，$z=x^*-\\alpha \\nabla f(x^*)$，以及 $\\tau=\\alpha$，我们得到：\n$$\n(x^* - \\alpha \\nabla f(x^*)) - x^* \\in \\alpha \\, \\partial g(x^*),\n$$\n化简为 $-\\alpha \\nabla f(x^*) \\in \\alpha \\, \\partial g(x^*)$。因为 $\\alpha  0$，这等价于 $-\\nabla f(x^*) \\in \\partial g(x^*)$，或 $0 \\in \\nabla f(x^*) + \\partial g(x^*)$。\n\n临近梯度映射是 $G_{\\alpha}(x) = \\frac{1}{\\alpha}\\left(x - \\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x))\\right)$。一个点 $x$ 是临近梯度迭代的不动点当且仅当 $G_{\\alpha}(x) = 0$。因此，条件 $G_{\\alpha}(x) = 0$ 等价于次梯度最优性条件在 $x$ 点成立。\n\n因此，$G_{\\alpha}(x)$ 可作为一种残差，或衡量点 $x$ 距离满足最优性条件有多远的度量。$G_{\\alpha}(x)$ 的大小（在任何范数下，如无穷范数）量化了对该条件的违反程度。一个非零分量 $(G_{\\alpha}(x))_i$ 表明 $x$ 的第 $i$ 个坐标不满足不动点条件，因此最优性条件在该坐标上被违反。在迭代算法中，$\\|G_{\\alpha}(x)\\|$ 常被用作终止准则；当该值低于一个小的容差时，算法停止，表明当前迭代点已足够接近最优点。",
            "answer": "$$\n\\boxed{4}\n$$"
        },
        {
            "introduction": "最后的这项实践将所有理论知识融会贯通，并付诸于代码实现。你将需要推导用于设定算法步长的全局 Lipschitz 常数，设计一个实用的数值方法（幂迭代法）来估计它，并最终实现完整的近端梯度下降算法 。这项练习旨在填平数学概念与解决实际稀疏优化问题的有效代码之间的鸿沟，是掌握该算法的关键一步。",
            "id": "3470569",
            "problem": "给定压缩感知和稀疏优化中的复合目标函数，其定义为 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2} \\lVert A x - b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，并且 $\\lambda \\ge 0$。您的任务如下。\n\n首先，从梯度利普希茨连续性和欧几里得算子范数的基本定义出发。仅使用这些基础，推导出一个关于 $f(x)$ 梯度的紧的利普希茨常数 $L$，并用 $A$ 的矩阵范数明确表示 $L$。\n\n其次，从矩阵最大奇异值的定义（即 $A^\\top A$ 最大特征值的平方根）出发，推导并概述一个用于估计 $\\lVert A \\rVert_2$ 的鲁棒幂迭代程序。您的概述必须论证：\n- 使用形如 $v_{k+1} = \\frac{A^\\top A v_k}{\\lVert A^\\top A v_k \\rVert_2}$ 的迭代；\n- 通过瑞利商 $r_k = v_k^\\top (A^\\top A) v_k$ 和 $\\widehat{\\sigma}_k = \\sqrt{r_k}$ 计算迭代中的估计值；\n- 一个基于 $\\widehat{\\sigma}_k$ 相对变化的终止条件；\n- 鲁棒化措施，例如多次随机重启和对 $A = 0$ 情况的安全处理。\n\n第三，为复合目标 $F(x)$ 实现一个近端梯度下降方法。该方法使用您的谱范数估计来设置一个固定步长 $t = \\tfrac{1}{\\alpha L}$（其中有一个小的安全膨胀系数 $\\alpha \\gt 1$），并可选地使用回溯线搜索来确保 $F(x)$ 的单调递减。$g(x) = \\lambda \\lVert x \\rVert_1$ 的近端算子必须使用软阈值规则精确实现。在利普希茨常数 $L$ 为 $0$ 的边缘情况下（例如，当 $A$ 是零矩阵时），您的实现必须是数值鲁棒的。\n\n测试套件和要求输出。您的程序必须实现上述程序，并为以下测试用例生成结果。所有随机矩阵和向量必须使用指定的伪随机数生成器种子生成，以保证可复现性。\n\n- 测试 S$1$（对角，精确）：$A = \\mathrm{diag}(3, 1, 0.5)$。通过您的幂迭代估计 $\\lVert A \\rVert_2$。返回一个布尔值，指示谱范数估计的相对误差是否至多为 $10^{-10}$。\n- 测试 S$2$（零矩阵）：$A \\in \\mathbb{R}^{4 \\times 2}$，所有元素均为 $0$。估计 $\\lVert A \\rVert_2$。返回一个布尔值，指示绝对误差是否至多为 $10^{-12}$。\n- 测试 S$3$（病态，对角）：$A = \\mathrm{diag}(1000, 2, 10^{-3}, 10)$。估计 $\\lVert A \\rVert_2$。返回一个布尔值，指示相对误差是否至多为 $10^{-8}$。\n- 测试 P$1$（在已知解的正交列上使用近端梯度）：\n  - 如下构造具有正交列的 $A \\in \\mathbb{R}^{6 \\times 3}$：使用种子 $999$ 从独立标准正态分布中抽取 $M \\in \\mathbb{R}^{6 \\times 3}$，计算简约 $QR$ 分解 $M = Q R$，并设置 $A = Q$。\n  - 设 $b \\in \\mathbb{R}^6$ 使用相同的种子 $999$ 进行另一次独立抽取，并设置 $\\lambda = 0.2$。\n  - 唯一极小值点是 $x^\\star = \\mathrm{soft}(A^\\top b, \\lambda)$，其中 $\\mathrm{soft}$ 表示逐坐标应用的软阈值算子。\n  - 从 $x_0 = 0$ 开始，使用固定步长 $t = \\tfrac{1}{\\alpha L}$（其中 $\\alpha = 1.01$ 且 L 取自您的谱范数估计值）运行近端梯度下降 $300$ 次迭代。以浮点数形式返回欧几里得范数 $\\lVert x_{300} - x^\\star \\rVert_2$。\n- 测试 P$2$（使用回溯法实现单调递减）：\n  - 使用种子 $123$ 构造 $A \\in \\mathbb{R}^{5 \\times 4}$（其元素为独立标准正态分布），使用相同的种子进行另一次独立抽取构造 $b \\in \\mathbb{R}^5$，并设置 $\\lambda = 0.1$。\n  - 从 $x_0 = 0$ 开始，使用回溯线搜索运行近端梯度下降 $200$ 次迭代，确保 $F(x_k)$ 是非增的。使用您的谱范数估计将步长初始化为 $t_0 = \\tfrac{1}{L}$，并在需要时使用 $\\beta = \\tfrac{1}{2}$ 的回溯收缩因子。\n  - 返回一个布尔值，指示整个目标值序列 $\\{F(x_k)\\}_{k=0}^{200}$ 是否在 $10^{-12}$ 的数值容差内非增，即 $F(x_{k+1}) \\le F(x_k) + 10^{-12}$ 是否对所有 $k \\in \\{0, \\dotsc, 199\\}$ 成立。\n\n最终输出格式。您的程序应生成单行输出，其中包含五个测试的结果，格式为方括号括起来的逗号分隔列表，顺序如下：$\\left[\\text{S}1,\\text{S}2,\\text{S}3,\\text{P}1,\\text{P}2\\right]$。条目 $\\text{S}1$、$\\text{S}2$、$\\text{S}3$ 和 $\\text{P}2$ 必须是布尔值，$\\text{P}1$ 必须是浮点数。不应打印任何额外文本。",
            "solution": "该问题是良定的、有科学依据的，并包含了完整推导和实现所需的所有信息。我们开始进行求解。\n\n该问题涉及最小化一个复合目标函数 $F(x) = f(x) + g(x)$，这在稀疏优化和压缩感知中很常见。其中 $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ 是一个光滑的数据保真项，而 $g(x) = \\lambda \\lVert x \\rVert_1$ 是一个促进稀疏性的非光滑正则化项。这里，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda \\ge 0$。我们将处理所要求的三个部分：推导 $f(x)$ 梯度的利普希茨常数，概述一个幂迭代方法来估计该常数，以及描述用于最小化 $F(x)$ 的近端梯度算法。\n\n首先，我们推导 $f(x)$ 梯度的紧的利普希茨常数 $L$。如果存在一个常数 $L \\ge 0$，使得对于所有 $x, y \\in \\mathbb{R}^n$，以下不等式成立，则函数 $\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n$ 是 $L$-利普希茨连续的：\n$$\n\\lVert \\nabla f(x) - \\nabla f(y) \\rVert_2 \\le L \\lVert x - y \\rVert_2\n$$\n为了找到 $L$，我们首先计算 $f(x)$ 的梯度。函数为 $f(x) = \\frac{1}{2} (Ax - b)^\\top(Ax - b) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)$。对 $x$求梯度得到：\n$$\n\\nabla f(x) = \\frac{1}{2}(2 A^\\top A x - 2 A^\\top b) = A^\\top(Ax - b)\n$$\n现在，我们将其代入利普希茨定义中：\n$$\n\\lVert \\nabla f(x) - \\nabla f(y) \\rVert_2 = \\lVert (A^\\top(Ax - b)) - (A^\\top(Ay - b)) \\rVert_2 = \\lVert A^\\top A x - A^\\top A y \\rVert_2 = \\lVert A^\\top A (x - y) \\rVert_2\n$$\n根据矩阵 $M$ 的诱导 $L_2$-范数（或谱范数）的定义，对任意向量 $z$，我们有 $\\lVert M z \\rVert_2 \\le \\lVert M \\rVert_2 \\lVert z \\rVert_2$。将此应用于 $M = A^\\top A$ 和 $z = x - y$：\n$$\n\\lVert A^\\top A (x - y) \\rVert_2 \\le \\lVert A^\\top A \\rVert_2 \\lVert x - y \\rVert_2\n$$\n这个不等式表明，利普希茨常数 $L$ 可以取为 $L = \\lVert A^\\top A \\rVert_2$。$A^\\top A$ 的谱范数等于其最大特征值 $\\lambda_{\\max}(A^\\top A)$，因为 $A^\\top A$ 是对称半正定的。此外，$A^\\top A$ 的特征值是 $A$ 的奇异值的平方。因此，$\\lambda_{\\max}(A^\\top A) = (\\sigma_{\\max}(A))^2$。$A$ 的谱范数是其最大奇异值，$\\lVert A \\rVert_2 = \\sigma_{\\max}(A)$。所以，我们可以将利普希茨常数表示为：\n$$\nL = \\lVert A^\\top A \\rVert_2 = \\lambda_{\\max}(A^\\top A) = (\\sigma_{\\max}(A))^2 = \\lVert A \\rVert_2^2\n$$\n这个常数是紧的，因为如果选择 $x-y$ 为 $A^\\top A$ 对应于其最大特征值的特征向量，则该不等式变为等式。\n\n其次，我们设计一个基于幂迭代法的程序来估计 $\\lVert A \\rVert_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)}$。幂迭代法旨在寻找给定矩阵的具有最大模的特征值所对应的特征向量。这里，我们将其应用于矩阵 $B = A^\\top A$。\n该方法的论证如下：\n设 $v_0$ 是一个初始向量，通常是单位范数，且不与和 $\\lambda_{\\max}(B)$ 相关联的特征向量 $u_1$ 正交。设 $B$ 的特征向量为 $\\{u_i\\}$，对应的实非负特征值为 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$。我们可以在特征基中表示 $v_0$：$v_0 = \\sum_{i=1}^n c_i u_i$。\n幂方法的核心是矩阵 $B$ 的迭代应用：\n$$\nB^k v_0 = B^k \\sum_{i=1}^n c_i u_i = \\sum_{i=1}^n c_i B^k u_i = \\sum_{i=1}^n c_i \\lambda_i^k u_i = \\lambda_1^k \\left( c_1 u_1 + \\sum_{i=2}^n c_i \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k u_i \\right)\n$$\n假设最大特征值是唯一的（$\\lambda_1  \\lambda_2$），当 $k \\to \\infty$ 时，对于 $i  1$ 的项 $(\\lambda_i/\\lambda_1)^k$ 趋近于零。因此，向量 $B^k v_0$ 与主特征向量 $u_1$ 的方向对齐。为防止迭代向量的模增长或缩小至零，在每次迭代中引入一个归一化步骤。这导出了更新规则：\n$$\nv_{k+1} = \\frac{B v_k}{\\lVert B v_k \\rVert_2} = \\frac{A^\\top A v_k}{\\lVert A^\\top A v_k \\rVert_2}\n$$\n序列 $\\{v_k\\}$ 收敛到主特征向量 $u_1$。\n一旦我们有了近似 $v_k \\approx u_1$，我们就可以使用瑞利商来估计对应的特征值 $\\lambda_1$。对于一个向量 $v$ 且 $\\lVert v \\rVert_2 = 1$，瑞利商为 $r(v) = v^\\top B v$。当 $v_k \\to u_1$ 时，估计值 $r_k = v_k^\\top B v_k = v_k^\\top (A^\\top A) v_k$ 收敛到 $\\lambda_1$。因此，在第 k 次迭代时 A 的最大奇异值的估计值为 $\\widehat{\\sigma}_k = \\sqrt{r_k}$。\n一个合适的终止条件是监控奇异值估计的收敛情况。当连续估计值之间的相对变化小于预设容差 $\\epsilon$ 时，迭代停止：$\\frac{|\\widehat{\\sigma}_k - \\widehat{\\sigma}_{k-1}|}{\\widehat{\\sigma}_k}  \\epsilon$。\n为保证鲁棒性，需要采取几项措施。如果初始向量 $v_0$ 与 $u_1$ 正交（即 $c_1=0$），该方法将收敛到次大特征值，或失败。为缓解此问题，可以多次运行该算法，每次从不同的随机向量 $v_0$ 开始，并取找到的最大奇异值。此外，如果 $A$ 是零矩阵，则 $A^\\top A$ 也是零矩阵，对于任何 $v_k$，$A^\\top A v_k$ 都是零向量。归一化将涉及除以零。一个鲁棒的实现必须检测到这种情况（例如，如果 $\\lVert A^\\top A v_k \\rVert_2 = 0$）并正确地得出最大特征值为 $0$，因此 $\\lVert A \\rVert_2 = 0$。\n\n第三，我们描述最小化 $F(x) = f(x) + g(x)$ 的近端梯度下降法的实现。迭代更新是光滑部分 $f(x)$ 上的标准梯度下降步骤和与非光滑部分 $g(x)$ 对应的近端映射的组合。更新规则是：\n$$\nx_{k+1} = \\mathrm{prox}_{t_k g}(x_k - t_k \\nabla f(x_k))\n$$\n其中 $t_k  0$ 是第 k 次迭代的步长。梯度为 $\\nabla f(x_k) = A^\\top(A x_k - b)$。$h(x)$ 的近端算子定义为 $\\mathrm{prox}_h(v) = \\arg\\min_x (h(x) + \\frac{1}{2}\\lVert x - v \\rVert_2^2)$。对于 $g(x) = \\lambda \\lVert x \\rVert_1$，其缩放后的近端算子 $\\mathrm{prox}_{t g}$ 是逐元素的软阈值算子 $S_{t\\lambda}(\\cdot)$：\n$$\n(\\mathrm{prox}_{t g}(v))_i = S_{t\\lambda}(v_i) = \\mathrm{sgn}(v_i) \\max(|v_i| - t\\lambda, 0)\n$$\n因此，完整的更新是：\n$$\nx_{k+1} = S_{t_k \\lambda}(x_k - t_k A^\\top(A x_k - b))\n$$\n步长 $t_k$ 可以用两种方式选择。可以使用一个固定的步长 $t$，如果 $0  t  2/L$，则保证收敛。一个安全且常见的选择是 $t = 1/L = 1/\\lVert A \\rVert_2^2$。问题指定了一个膨胀的步长分母，$t = 1/(\\alpha L)$ 且 $\\alpha  1$，这对应于一个更小、更保守的步长，同样能保证收敛。如果 $L=0$（即 $A=0$），则 $\\nabla f(x) = 0$。更新简化为 $x_{k+1} = S_{t\\lambda}(x_k)$。对于任何 $t  0$，此迭代收敛到 $g(x)$ 的极小值点，即 $x=0$。在数值上，当 $L=0$ 时计算 $t=1/L$ 需要小心。一个鲁棒的实现可以处理由此产生的浮点无穷大，在阈值化步骤中正确地将任何非零值映射到零。\n或者，可以使用回溯线搜索在每次迭代中确定 $t_k$。从一个初始猜测 $t_k^{(0)}$（例如，$1/L$）开始，检查一个充分下降条件。问题仅要求目标函数单调递减，$F(x_{k+1}) \\le F(x_k)$。如果不满足此条件，则减小步长，$t_k^{(j+1)} = \\beta t_k^{(j)}$（其中收缩因子 $\\beta \\in (0, 1)$），并重新计算候选点 $x_{k+1}$。重复此过程直到条件满足。即使不精确知道 $L$，这也确保了稳定性和收敛性，尽管 $L$ 的估计值为步长提供了一个很好的初始猜测。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all specified tests and print the results.\n    \"\"\"\n\n    def power_iteration(A, num_restarts=10, max_iter=1000, tol=1e-12):\n        \"\"\"\n        Estimates the spectral norm of A using power iteration on A.T @ A.\n\n        The spectral norm ||A||_2 is the largest singular value of A, which is the\n        square root of the largest eigenvalue of the symmetric matrix B = A.T @ A.\n        This function finds the largest eigenvalue of B using power iteration.\n\n        Justification:\n        1. Update Rule: v_{k+1} = (A.T @ A @ v_k) / ||A.T @ A @ v_k||_2. This\n           iteratively aligns v with the dominant eigenvector of A.T @ A.\n        2. Estimate: The Rayleigh quotient r_k = v_k.T @ (A.T @ A) @ v_k gives an\n           estimate of the dominant eigenvalue. The singular value is sqrt(r_k).\n        3. Robustness:\n           - Multiple random restarts mitigate the risk of the initial vector being\n             orthogonal to the dominant eigenvector.\n           - If A=0, A.T @ A @ v_k will be a zero vector. The code handles this\n             by checking the norm before division, correctly returning 0.\n        \"\"\"\n        if A.shape[1] == 0:\n            return 0.0\n\n        rng = np.random.default_rng(0)  # Fixed seed for reproducibility within the function\n        max_sigma = 0.0\n\n        for _ in range(num_restarts):\n            v = rng.standard_normal(A.shape[1])\n            if np.linalg.norm(v) == 0:  # Highly unlikely but possible\n                v = np.ones(A.shape[1])\n            v = v / np.linalg.norm(v)\n\n            sigma_prev = 0.0\n            for _ in range(max_iter):\n                Av = A @ v\n                AtAv = A.T @ Av\n                norm_AtAv = np.linalg.norm(AtAv)\n\n                if norm_AtAv == 0:\n                    sigma_curr = 0.0\n                    break\n\n                v = AtAv / norm_AtAv\n                \n                # Rayleigh quotient for eigenvalue estimation\n                AtAv_v = A.T @ (A @ v)\n                lambda_max_est = v.T @ AtAv_v\n                \n                # Eigenvalue can't be negative for A.T @ A\n                if lambda_max_est  0:\n                    lambda_max_est = 0\n\n                sigma_curr = np.sqrt(lambda_max_est)\n\n                if sigma_curr > 0 and abs(sigma_curr - sigma_prev) / sigma_curr  tol:\n                    break\n                \n                sigma_prev = sigma_curr\n            \n            if sigma_curr > max_sigma:\n                max_sigma = sigma_curr\n        \n        return max_sigma\n\n    def soft_thresholding(v, t):\n        \"\"\"\n        Implements the element-wise soft-thresholding operator.\n        prox_{t*g}(v) = sgn(v) * max(|v| - t, 0) for g(x) = ||x||_1\n        \"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - t, 0)\n\n    def objective_function(A, b, lambda_, x):\n        \"\"\"Computes F(x) = 0.5 * ||Ax - b||^2_2 + lambda * ||x||_1\"\"\"\n        f_x = 0.5 * np.linalg.norm(A @ x - b)**2\n        g_x = lambda_ * np.linalg.norm(x, 1)\n        return f_x + g_x\n\n    def proximal_gradient_descent(A, b, lambda_, x0, num_iter, mode='fixed', **kwargs):\n        \"\"\"\n        Performs proximal gradient descent to minimize the LASSO objective.\n        \n        Update Rule: x_{k+1} = soft_thresh(x_k - t*grad_f(x_k), t*lambda)\n        \n        Step Size (t):\n        - Fixed: t = 1 / (alpha * L), where L is the Lipschitz constant. This is a\n          safe, constant step size guaranteeing convergence.\n        - Backtracking: Adjusts t at each step to ensure monotonic decrease of F(x),\n          making it robust.\n        \"\"\"\n        x = x0.copy()\n        \n        L = power_iteration(A)**2\n        \n        F_values = []\n        if mode == 'backtracking':\n             F_values.append(objective_function(A, b, lambda_, x))\n\n        # Handle L=0 case for step size calculation\n        if L  1e-15: # L is effectively zero\n            if mode == 'fixed':\n                # Division by zero yields np.inf, which is handled correctly\n                # by soft-thresholding to send x to 0 if lambda > 0.\n                t = np.inf\n            else: # backtracking\n                t_init = 1.0 # Use a default initial step for backtracking\n        else:\n            if mode == 'fixed':\n                alpha = kwargs.get('alpha', 1.01)\n                t = 1.0 / (alpha * L)\n            else: # backtracking\n                t_init = 1.0 / L\n\n        beta = kwargs.get('beta', 0.5)\n\n        for _ in range(num_iter):\n            grad = A.T @ (A @ x - b)\n            \n            if mode == 'fixed':\n                step_size = t\n                x = soft_thresholding(x - step_size * grad, step_size * lambda_)\n            \n            elif mode == 'backtracking':\n                step_size = t_init\n                F_old = F_values[-1]\n                \n                while True:\n                    x_new = soft_thresholding(x - step_size * grad, step_size * lambda_)\n                    F_new = objective_function(A, b, lambda_, x_new)\n                    \n                    if F_new = F_old + 1e-12:  # Allow for small numerical tolerance\n                        x = x_new\n                        F_values.append(F_new)\n                        break\n                    \n                    step_size *= beta\n                    if step_size  1e-20: # Failsafe\n                        x = x_new\n                        F_values.append(F_new)\n                        break\n            \n        if mode == 'backtracking':\n            return x, F_values\n        return x\n\n    results = []\n\n    # Test S1 (diagonal, exact)\n    A1 = np.diag([3.0, 1.0, 0.5])\n    norm_A1_est = power_iteration(A1)\n    norm_A1_true = 3.0\n    s1_res = abs(norm_A1_est - norm_A1_true) / norm_A1_true = 1e-10\n    results.append(s1_res)\n\n    # Test S2 (zero matrix)\n    A2 = np.zeros((4, 2))\n    norm_A2_est = power_iteration(A2)\n    norm_A2_true = 0.0\n    s2_res = abs(norm_A2_est - norm_A2_true) = 1e-12\n    results.append(s2_res)\n    \n    # Test S3 (ill-conditioned, diagonal)\n    A3 = np.diag([1000.0, 2.0, 1e-3, 10.0])\n    norm_A3_est = power_iteration(A3, num_restarts=20) # More restarts for robustness\n    norm_A3_true = 1000.0\n    s3_res = abs(norm_A3_est - norm_A3_true) / norm_A3_true = 1e-8\n    results.append(s3_res)\n\n    # Test P1 (proximal gradient on orthonormal columns)\n    rng_p1 = np.random.default_rng(999)\n    M = rng_p1.standard_normal((6, 3))\n    A4, _ = np.linalg.qr(M)\n    b4 = rng_p1.standard_normal(6)\n    lambda_p1 = 0.2\n    alpha_p1 = 1.01\n    \n    # The known solution for orthonormal A is soft(A^T b, lambda)\n    x_star = soft_thresholding(A4.T @ b4, lambda_p1)\n    \n    x0_p1 = np.zeros(3)\n    x_300 = proximal_gradient_descent(A4, b4, lambda_p1, x0_p1, 300, mode='fixed', alpha=alpha_p1)\n    \n    p1_res = np.linalg.norm(x_300 - x_star)\n    results.append(p1_res)\n\n    # Test P2 (monotonic decrease with backtracking)\n    rng_p2 = np.random.default_rng(123)\n    A5 = rng_p2.standard_normal((5, 4))\n    b5 = rng_p2.standard_normal(5)\n    lambda_p2 = 0.1\n    \n    x0_p2 = np.zeros(4)\n    _, F_values_p2 = proximal_gradient_descent(A5, b5, lambda_p2, x0_p2, 200, mode='backtracking', beta=0.5)\n    \n    is_nonincreasing = True\n    for k in range(len(F_values_p2) - 1):\n        if F_values_p2[k+1] > F_values_p2[k] + 1e-12:\n            is_nonincreasing = False\n            break\n    p2_res = is_nonincreasing\n    results.append(p2_res)\n    \n    # Final output formatting\n    output_str = f\"[{str(results[0]).lower()},{str(results[1]).lower()},{str(results[2]).lower()},{str(results[3])},{str(results[4]).lower()}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}