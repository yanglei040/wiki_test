{
    "hands_on_practices": [
        {
            "introduction": "To build robust optimization algorithms for non-smooth problems, we must first master the calculus of subdifferentials. This first practice focuses on a cornerstone calculation: finding the subdifferential of a function composed of the $\\ell_{1}$-norm and an affine map . By applying the chain rule, you will learn to characterize the complete set of subgradients at a point where the argument of the norm has zero entries, revealing the rich geometric structure of the subdifferential.",
            "id": "3188856",
            "problem": "Consider the convex function $f:\\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $f(x) = \\|B(Ax - c)\\|_{1}$, where $A \\in \\mathbb{R}^{3 \\times 2}$, $B \\in \\mathbb{R}^{3 \\times 3}$, and $c \\in \\mathbb{R}^{3}$ are given by\n$$\nA = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & -1\n\\end{pmatrix}, \n\\quad\nB = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & -1\n\\end{pmatrix},\n\\quad\nc = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nLet $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Using only the definition of a convex subgradient and the subdifferential, the fact that the one-norm (also called the $\\ell_{1}$ norm) satisfies $\\partial \\|z\\|_{1} = \\{s \\in \\mathbb{R}^{m} : s_{i} = \\operatorname{sign}(z_{i}) \\text{ if } z_{i} \\neq 0,\\ \\ s_{i} \\in [-1,1] \\text{ if } z_{i} = 0\\}$, and first principles for composing convex functions with affine maps, derive a valid expression for the subdifferential $\\partial f(x_{0})$. In particular, identify all possible subgradients at $x_{0}$ by characterizing the set $\\partial f(x_{0})$ explicitly as a set in $\\mathbb{R}^{2}$.\n\nExpress your final answer as a single closed-form set expression. No rounding is required. No physical units are involved.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the established principles of convex analysis, well-posed with a unique solution, and expressed in objective, unambiguous mathematical language. All necessary data and definitions are provided, and there are no internal contradictions. We may therefore proceed with the derivation.\n\nThe function is given by $f(x) = \\|B(Ax - c)\\|_{1}$, where $x \\in \\mathbb{R}^{2}$. This is a composition of the $\\ell_{1}$-norm, which is a convex function, with an affine transformation. Let us denote $h(z) = \\|z\\|_{1}$ and the affine map as $g(x) = B(Ax - c) = (BA)x - Bc$. Thus, $f(x) = h(g(x))$.\n\nThe chain rule for subdifferentials states that for a function of the form $f(x) = h(Lx + d)$, where $h$ is a convex function and $x \\mapsto Lx+d$ is an affine map, the subdifferential of $f$ is given by:\n$$\n\\partial f(x) = L^{T} \\partial h(Lx + d)\n$$\nIn our case, the linear part of the affine map is $L = BA$. Therefore, the subdifferential of $f$ at a point $x$ is:\n$$\n\\partial f(x) = (BA)^{T} \\partial h(B(Ax - c)) = A^{T}B^{T} \\partial \\|B(Ax-c)\\|_{1}\n$$\nWe need to compute the subdifferential at the specific point $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The process involves three steps:\n1.  Compute the vector argument of the norm, $z_{0} = B(Ax_{0} - c)$.\n2.  Determine the subdifferential of the $\\ell_{1}$-norm at $z_{0}$, denoted $\\partial \\|z_{0}\\|_{1}$.\n3.  Apply the linear transformation $A^{T}B^{T}$ to the set $\\partial \\|z_{0}\\|_{1}$.\n\nStep 1: Compute $z_{0} = B(Ax_{0} - c)$.\nFirst, we evaluate the affine part $Ax_{0} - c$:\n$$\nA x_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\\\ 1 \\cdot 1 - 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\nAx_{0} - c = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nNow, we apply the matrix $B$:\n$$\nz_{0} = B(Ax_{0} - c) = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 0 \\\\ 1 \\cdot 1 \\\\ -1 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\nStep 2: Determine $\\partial \\|z_{0}\\|_{1}$.\nThe problem provides the rule for the subdifferential of the $\\ell_{1}$-norm:\n$\\partial \\|z\\|_{1} = \\{s \\in \\mathbb{R}^{m} : s_{i} = \\operatorname{sign}(z_{i}) \\text{ if } z_{i} \\neq 0, \\text{ and } s_{i} \\in [-1,1] \\text{ if } z_{i} = 0\\}$.\nOur vector is $z_{0} = (z_{0,1}, z_{0,2}, z_{0,3})^{T} = (0, 1, 0)^{T}$.\nFor a subgradient vector $s = (s_{1}, s_{2}, s_{3})^{T} \\in \\partial \\|z_{0}\\|_{1}$:\n- For the first component, $z_{0,1} = 0$, so $s_{1}$ can be any value in the interval $[-1, 1]$.\n- For the second component, $z_{0,2} = 1 \\neq 0$, so $s_{2} = \\operatorname{sign}(1) = 1$.\n- For the third component, $z_{0,3} = 0$, so $s_{3}$ can be any value in the interval $[-1, 1]$.\nLet us introduce parameters $\\alpha = s_{1}$ and $\\beta = s_{3}$. The set of subgradients of the norm at $z_{0}$ is:\n$$\n\\partial \\|z_{0}\\|_{1} = \\left\\{ \\begin{pmatrix} \\alpha \\\\ 1 \\\\ \\beta \\end{pmatrix} \\, \\middle| \\, \\alpha \\in [-1, 1], \\, \\beta \\in [-1, 1] \\right\\}\n$$\n\nStep 3: Apply the transformation $A^{T}B^{T}$.\nFirst, we compute the matrix $A^{T}B^{T}$:\n$$\nA^{T} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix}, \\quad B^{T} = B = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix}\n$$\n$$\nA^{T}B^{T} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nNow, we find the set $\\partial f(x_{0}) = A^{T}B^{T} \\partial \\|z_{0}\\|_{1}$. Any element $g \\in \\partial f(x_{0})$ is obtained by multiplying $A^{T}B^{T}$ by an element $s \\in \\partial \\|z_{0}\\|_{1}$:\n$$\ng = \\begin{pmatrix} 2 & 0 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ 1 \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} 2\\alpha - \\beta \\\\ 1 + \\beta \\end{pmatrix}\n$$\nwhere $\\alpha \\in [-1, 1]$ and $\\beta \\in [-1, 1]$.\nThis provides the explicit characterization of the subdifferential set $\\partial f(x_{0})$. Each subgradient at $x_{0}$ is a vector in $\\mathbb{R}^{2}$ that can be generated by choosing values for $\\alpha$ and $\\beta$ from their respective intervals.\n\nThe set of all subgradients of $f$ at $x_{0}$ is therefore:\n$$\n\\partial f(x_{0}) = \\left\\{ \\begin{pmatrix} 2\\alpha - \\beta \\\\ 1 + \\beta \\end{pmatrix} \\, \\middle| \\, \\alpha \\in [-1, 1], \\, \\beta \\in [-1, 1] \\right\\}\n$$\nThis set is a parallelogram in $\\mathbb{R}^{2}$ defined by the linear mapping of the square $[-1,1] \\times [-1,1]$ in the $(\\alpha, \\beta)$-plane.",
            "answer": "$$\n\\boxed{\\left\\{ \\begin{pmatrix} 2\\alpha - \\beta \\\\ 1 + \\beta \\end{pmatrix} \\, \\middle| \\, \\alpha \\in [-1, 1], \\, \\beta \\in [-1, 1] \\right\\}}\n$$"
        },
        {
            "introduction": "Many practical optimization problems leverage separable objective functions, where the function decomposes into a sum of terms that each depend on a single variable. This structure is computationally convenient and allows for highly efficient algorithms. This exercise  challenges you to compute the subdifferential for such a separable penalty, reinforcing the rule that the subdifferential of the sum is the Cartesian product of the individual subdifferentials. You will also practice selecting the minimum-norm element, a common and crucial step in defining a deterministic descent direction.",
            "id": "3483573",
            "problem": "Consider a separable convex penalty used in compressed sensing and sparse optimization, defined for a vector $x \\in \\mathbb{R}^{n}$ by\n$$\nf(x) \\;=\\; \\sum_{i=1}^{n} \\phi_i(x_i),\n$$\nwhere each coordinate function $\\phi_i : \\mathbb{R} \\to \\mathbb{R}$ is given by\n$$\n\\phi_i(t) \\;=\\; w_i\\,|t| \\;+\\; u_i\\,\\max(0,\\, t - a_i) \\;+\\; v_i\\,\\max(0,\\,-t - b_i).\n$$\nThis penalty combines a weighted $\\ell_1$ regularizer and asymmetric saturation hinges to reduce bias while preserving sparsity. Use the fundamental definition of the subgradient and properties of separable convex functions to determine, at the given point, the element of the subdifferential with minimum Euclidean norm.\n\nLet $n = 5$, with parameters\n$$\nw \\;=\\; (\\,3,\\, 1,\\, 2,\\, 1,\\, 2\\,),\\quad\nu \\;=\\; (\\,2,\\, 4,\\, 1,\\, 2,\\, 5\\,),\\quad\nv \\;=\\; (\\,5,\\, 3,\\, 2,\\, 1,\\, 1\\,),\n$$\n$$\na \\;=\\; (\\,1,\\, 2,\\, 3,\\, 0.2,\\, 0.5\\,),\\quad\nb \\;=\\; (\\,0.5,\\, 1,\\, 2,\\, 3,\\, 0.1\\,),\n$$\nand\n$$\nx \\;=\\; (\\,1,\\, 0,\\, 1,\\, -4,\\, -0.05\\,).\n$$\nCompute the minimum Euclidean-norm element of the subdifferential $\\partial f(x)$. Your final answer must be a single row vector written as a LaTeX $\\pmatrix$ expression. No rounding is required.",
            "solution": "We start from the definition of the subgradient of a convex function. For a convex function $g : \\mathbb{R}^{m} \\to \\mathbb{R}$, a vector $s \\in \\mathbb{R}^{m}$ is a subgradient at $z$ if\n$$\ng(y) \\;\\ge\\; g(z) \\;+\\; s^{\\top}(y - z)\\quad \\text{for all } y \\in \\mathbb{R}^{m}.\n$$\nThe subdifferential $\\partial g(z)$ is the set of all such $s$. For a separable convex function $f(x) = \\sum_{i=1}^{n} \\phi_i(x_i)$ with $\\phi_i : \\mathbb{R} \\to \\mathbb{R}$ convex, we have\n$$\n\\partial f(x) \\;=\\; \\prod_{i=1}^{n} \\partial \\phi_i(x_i),\n$$\nthat is, the Cartesian product of the coordinatewise subdifferentials (this follows from additivity of subgradients under separability and the Minkowski sum structure of subdifferentials of sums of convex functions defined on disjoint coordinates).\n\nNext, we use well-known subdifferentials of basic convex building blocks:\n\n- For the absolute value, $\\psi(t) = |t|$, the subdifferential is\n$$\n\\partial |t| \\;=\\; \n\\begin{cases}\n\\{1\\}, & t > 0,\\\\\n[-1,\\,1], & t = 0,\\\\\n\\{-1\\}, & t  0.\n\\end{cases}\n$$\nWith a positive weight $w_i$, we scale the set: $\\partial\\big(w_i|t|\\big) = w_i \\,\\partial|t|$.\n\n- For the shifted hinge on the positive side, $\\eta_a(t) = \\max(0,\\, t - a)$,\n$$\n\\partial \\eta_a(t) \\;=\\;\n\\begin{cases}\n\\{0\\},  t  a,\\\\\n[0,\\,1],  t = a,\\\\\n\\{1\\},  t > a.\n\\end{cases}\n$$\nWith a positive weight $u_i$, we get $\\partial\\big(u_i \\eta_{a_i}(t)\\big) = u_i \\,\\partial \\eta_{a_i}(t)$.\n\n- For the shifted hinge on the negative side, $\\zeta_b(t) = \\max(0,\\, -t - b)$,\n$$\n\\partial \\zeta_b(t) \\;=\\;\n\\begin{cases}\n\\{-1\\},  t  -b,\\\\\n[-1,\\,0],  t = -b,\\\\\n\\{0\\},  t > -b.\n\\end{cases}\n$$\nWith a positive weight $v_i$, we have $\\partial\\big(v_i \\zeta_{b_i}(t)\\big) = v_i \\,\\partial \\zeta_{b_i}(t)$.\n\nBecause each $\\phi_i$ is a sum of convex functions of a single variable, its subdifferential at $t$ is the Minkowski sum of the corresponding sets:\n$$\n\\partial \\phi_i(t) \\;=\\; \\partial\\big(w_i|t|\\big) \\;+\\; \\partial\\big(u_i \\max(0, t - a_i)\\big) \\;+\\; \\partial\\big(v_i \\max(0, -t - b_i)\\big).\n$$\n\nWe now evaluate $\\partial \\phi_i(x_i)$ for each coordinate $i = 1,\\dots,5$ using the given parameters and $x$.\n\n- Coordinate $i = 1$: $x_1 = 1$, $a_1 = 1$, $b_1 = 0.5$, $w_1 = 3$, $u_1 = 2$, $v_1 = 5$.\nWe have $x_1 = a_1$, so\n$$\n\\partial\\big(w_1|x_1|\\big) = \\{\\,3\\,\\},\\quad\n\\partial\\big(u_1 \\max(0, x_1 - a_1)\\big) = u_1 \\,[0,1] = [\\,0,\\,2\\,],\\quad\n\\partial\\big(v_1 \\max(0, -x_1 - b_1)\\big) = \\{\\,0\\,\\},\n$$\nsince $-x_1 - b_1 = -1 - 0.5 = -1.5  0$ yields zero for the negative hinge at non-kink points. Therefore,\n$$\n\\partial \\phi_1(x_1) \\;=\\; \\{\\,3\\,\\} + [\\,0,\\,2\\,] + \\{\\,0\\,\\} \\;=\\; [\\,3,\\,5\\,].\n$$\n\n- Coordinate $i = 2$: $x_2 = 0$, $a_2 = 2$, $b_2 = 1$, $w_2 = 1$, $u_2 = 4$, $v_2 = 3$.\nWe have $x_2 = 0  a_2$ and $x_2 > -b_2$, so\n$$\n\\partial\\big(w_2|x_2|\\big) = [-1,\\,1],\\quad\n\\partial\\big(u_2 \\max(0, x_2 - a_2)\\big) = \\{\\,0\\,\\},\\quad\n\\partial\\big(v_2 \\max(0, -x_2 - b_2)\\big) = \\{\\,0\\,\\}.\n$$\nThus,\n$$\n\\partial \\phi_2(x_2) \\;=\\; [-1,\\,1].\n$$\n\n- Coordinate $i = 3$: $x_3 = 1$, $a_3 = 3$, $b_3 = 2$, $w_3 = 2$, $u_3 = 1$, $v_3 = 2$.\nWe have $x_3 > 0$, $x_3  a_3$, and $x_3 > -b_3$, so\n$$\n\\partial\\big(w_3|x_3|\\big) = \\{\\,2\\,\\},\\quad\n\\partial\\big(u_3 \\max(0, x_3 - a_3)\\big) = \\{\\,0\\,\\},\\quad\n\\partial\\big(v_3 \\max(0, -x_3 - b_3)\\big) = \\{\\,0\\,\\}.\n$$\nHence,\n$$\n\\partial \\phi_3(x_3) \\;=\\; \\{\\,2\\,\\}.\n$$\n\n- Coordinate $i = 4$: $x_4 = -4$, $a_4 = 0.2$, $b_4 = 3$, $w_4 = 1$, $u_4 = 2$, $v_4 = 1$.\nWe have $x_4  0$, $x_4  -b_4$, so\n$$\n\\partial\\big(w_4|x_4|\\big) = \\{-1\\},\\quad\n\\partial\\big(u_4 \\max(0, x_4 - a_4)\\big) = \\{\\,0\\,\\},\\quad\n\\partial\\big(v_4 \\max(0, -x_4 - b_4)\\big) = \\{-1\\}.\n$$\nTherefore,\n$$\n\\partial \\phi_4(x_4) \\;=\\; \\{-1\\} + \\{\\,0\\,\\} + \\{-1\\} \\;=\\; \\{-2\\}.\n$$\n\n- Coordinate $i = 5$: $x_5 = -0.05$, $a_5 = 0.5$, $b_5 = 0.1$, $w_5 = 2$, $u_5 = 5$, $v_5 = 1$.\nWe have $x_5  0$, $x_5 > -b_5$ (since $-0.05 > -0.1$), and $x_5  a_5$, so\n$$\n\\partial\\big(w_5|x_5|\\big) = \\{-2\\},\\quad\n\\partial\\big(u_5 \\max(0, x_5 - a_5)\\big) = \\{\\,0\\,\\},\\quad\n\\partial\\big(v_5 \\max(0, -x_5 - b_5)\\big) = \\{\\,0\\,\\}.\n$$\nThus,\n$$\n\\partial \\phi_5(x_5) \\;=\\; \\{-2\\}.\n$$\n\nCollecting these, the subdifferential of $f$ at $x$ is the Cartesian product\n$$\n\\partial f(x) \\;=\\; \\partial \\phi_1(x_1) \\times \\partial \\phi_2(x_2) \\times \\partial \\phi_3(x_3) \\times \\partial \\phi_4(x_4) \\times \\partial \\phi_5(x_5)\n\\;=\\; [\\,3,\\,5\\,] \\times [\\,-1,\\,1\\,] \\times \\{\\,2\\,\\} \\times \\{\\,-2\\,\\} \\times \\{\\,-2\\,\\}.\n$$\n\nWe now find the element of $\\partial f(x)$ with minimum Euclidean norm. Because $\\partial f(x)$ is a product of intervals (or singleton sets) and the squared Euclidean norm $\\|g\\|_2^{2} = \\sum_{i=1}^{5} g_i^{2}$ decomposes additively across coordinates, the minimization\n$$\n\\min_{g \\in \\partial f(x)} \\|g\\|_2\n$$\ndecouples into coordinatewise minimizations\n$$\ng_i^{\\star} \\;\\in\\; \\arg\\min_{g_i \\in \\partial \\phi_i(x_i)} g_i^{2}.\n$$\nFor an interval $[L, U]$ containing $0$, the minimizer is $g_i^{\\star} = 0$. If the interval does not contain $0$, the minimizer is the endpoint of the interval with smallest magnitude (closest to zero). For singleton sets, the choice is the unique element.\n\nApplying this:\n\n- For $i = 1$, $\\partial \\phi_1(x_1) = [\\,3,\\,5\\,]$, which does not contain $0$. The closest point to $0$ is $g_1^{\\star} = 3$.\n\n- For $i = 2$, $\\partial \\phi_2(x_2) = [\\,-1,\\,1\\,]$, which contains $0$. The minimizer is $g_2^{\\star} = 0$.\n\n- For $i = 3$, $\\partial \\phi_3(x_3) = \\{\\,2\\,\\}$, so $g_3^{\\star} = 2$.\n\n- For $i = 4$, $\\partial \\phi_4(x_4) = \\{\\,-2\\,\\}$, so $g_4^{\\star} = -2$.\n\n- For $i = 5$, $\\partial \\phi_5(x_5) = \\{\\,-2\\,\\}$, so $g_5^{\\star} = -2$.\n\nTherefore, the minimum Euclidean-norm element of $\\partial f(x)$ is\n$$\ng^{\\star} \\;=\\; \\begin{pmatrix} 3  0  2  -2  -2 \\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} 3  0  2  -2  -2 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Real-world regularization often involves combining multiple, non-separable penalty terms to enforce complex structural priors. This final practice presents a challenging objective function that is the sum of an $\\ell_1$-norm composition and a scaled $\\ell_{\\infty}$-norm . Successfully characterizing its subdifferential requires synthesizing the entire toolkit you have developed: the sum rule (via Minkowski sum), the chain rule, and the characterization of a norm's subgradient through its dual norm. This exercise serves as a capstone, testing your ability to navigate the interplay of multiple fundamental rules in a complex, integrated scenario.",
            "id": "3483561",
            "problem": "Consider the sparse-regularization objective used in compressed sensing and sparse optimization, defined for $x \\in \\mathbb{R}^{3}$ by the function $F(x) = \\|A x\\|_{1} + \\alpha \\|x\\|_{\\infty}$, where $\\alpha  0$ is a given scalar and $A \\in \\mathbb{R}^{3 \\times 3}$ is the linear operator\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nLet $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix}$. Using only the foundational definition of a subgradient of a convex function and the basic characterization of subgradients for norms via their dual norms, determine the element $g^{\\star} \\in \\partial F(x^{\\star})$ with minimal Euclidean norm $\\|g\\|_{2}$. Express your final answer as a row vector in exact symbolic form. No rounding is required.",
            "solution": "The problem asks for the subgradient $g^{\\star}$ with the minimum Euclidean norm within the subdifferential $\\partial F(x^{\\star})$ of the function $F(x) = \\|A x\\|_{1} + \\alpha \\|x\\|_{\\infty}$ at the point $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix}$. The matrix $A$ is given as $A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$ and $\\alpha  0$.\n\nThe function $F(x)$ is a sum of two convex functions, $f_1(x) = \\|Ax\\|_{1}$ and $f_2(x) = \\alpha \\|x\\|_{\\infty}$. The function $f_1(x)$ is a composition of the convex $\\ell_1$-norm and a linear transformation, which is convex. The function $f_2(x)$ is the convex $\\ell_\\infty$-norm scaled by a positive constant $\\alpha$, which is also convex. The sum of two convex functions is convex, so $F(x)$ is convex.\n\nFor a convex function $F$, the subdifferential at a point $x$ is a non-empty, compact, and convex set. We can use the sum rule for subdifferentials (a consequence of the Fenchel-Moreau theorem), which states that if the functions are convex and one is continuous, then the subdifferential of the sum is the Minkowski sum of the subdifferentials:\n$$\n\\partial F(x^{\\star}) = \\partial f_1(x^{\\star}) + \\partial f_2(x^{\\star}) = \\partial (\\|Ax\\|_{1})|_{x=x^{\\star}} + \\partial (\\alpha \\|x\\|_{\\infty})|_{x=x^{\\star}}\n$$\n\nFirst, we characterize the subdifferential of $f_1(x) = \\|Ax\\|_{1}$ at $x^{\\star}$. We apply the chain rule for subdifferentials with a linear map:\n$$\n\\partial f_1(x^{\\star}) = A^T \\partial(\\|\\cdot\\|_{1})|_{y=Ax^{\\star}}\n$$\nLet's compute $y^{\\star} = Ax^{\\star}$:\n$$\ny^{\\star} = A x^{\\star} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) + 1(-2) \\\\ 0(1) + 1(0) + 0(-2) \\\\ 0(1) + 0(0) + 1(-2) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -2 \\end{pmatrix}\n$$\nThe subdifferential of the $\\ell_1$-norm, $\\|y\\|_{1}$, at a point $y$ is the set of vectors $v$ whose components $v_i$ satisfy:\n$$\nv_i = \\begin{cases} \\mathrm{sgn}(y_i)  \\text{if } y_i \\neq 0 \\\\ c_i \\in [-1, 1]  \\text{if } y_i = 0 \\end{cases}\n$$\nFor $y^{\\star} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -2 \\end{pmatrix}$:\n\\begin{itemize}\n    \\item $y^{\\star}_1 = -1$, so $v_1 = \\mathrm{sgn}(-1) = -1$.\n    \\item $y^{\\star}_2 = 0$, so $v_2$ can be any value in $[-1, 1]$. Let's denote it by a parameter $s_1$, where $s_1 \\in [-1, 1]$.\n    \\item $y^{\\star}_3 = -2$, so $v_3 = \\mathrm{sgn}(-2) = -1$.\n\\end{itemize}\nThus, the subdifferential of the $\\ell_1$-norm at $y^{\\star}$ is the set:\n$$\n\\partial (\\|y^{\\star}\\|_{1}) = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -1 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\}\n$$\nNow, we compute $\\partial f_1(x^{\\star})$ by multiplying by $A^T$:\n$$\nA^T = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix}\n$$\n$$\n\\partial f_1(x^{\\star}) = A^T \\partial(\\|y^{\\star}\\|_{1}) = \\left\\{ \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ s_1 \\\\ -1 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\} = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\}\n$$\n\nNext, we characterize the subdifferential of $f_2(x) = \\alpha \\|x\\|_{\\infty}$ at $x^{\\star}$. Using the scaling rule for subdifferentials ($\\alpha  0$):\n$$\n\\partial f_2(x^{\\star}) = \\alpha \\, \\partial (\\|x\\|_{\\infty})|_{x=x^{\\star}}\n$$\nThe problem requests using the characterization of norm subgradients via dual norms. The dual norm of the $\\ell_\\infty$-norm is the $\\ell_1$-norm. The subdifferential $\\partial \\|x\\|_{\\infty}$ is the set of vectors $v$ such that $\\|v\\|_{1} \\le 1$ and $v^T x = \\|x\\|_{\\infty}$.\nFor $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix}$, we have $\\|x^{\\star}\\|_{\\infty} = \\max(|1|, |0|, |-2|) = 2$.\nWe seek vectors $v = (v_1, v_2, v_3)^T$ satisfying:\n\\begin{enumerate}\n    \\item $|v_1| + |v_2| + |v_3| \\le 1$\n    \\item $v^T x^{\\star} = v_1(1) + v_2(0) + v_3(-2) = v_1 - 2v_3 = 2$\n\\end{enumerate}\nFrom condition 2, by the triangle inequality, $2 = |v_1 - 2v_3| \\le |v_1| + 2|v_3|$.\nFrom condition 1, we have $|v_1| + |v_3| \\le |v_1| + |v_2| + |v_3| \\le 1$.\nSo, $|v_1| \\le 1$ and $|v_3| \\le 1$.\nThe maximum value of $|v_1| + 2|v_3|$ is $1 + 2(1) = 3$. The condition $2 \\le |v_1| + 2|v_3|$ is possible.\nLet's combine $|v_1| + |v_3| \\le 1$ with $2 \\le |v_1| + 2|v_3|$.\n$2 \\le |v_1| + 2|v_3| = |v_1| + |v_3| + |v_3| \\le 1 + |v_3|$. This implies $|v_3| \\ge 1$.\nSince we must have $|v_3| \\le 1$, the only possibility is $|v_3| = 1$.\nIf $|v_3|=1$, then $1+|v_3| = 2$. For the inequality $2 \\le 1+|v_3|$ to hold, all intermediate inequalities must be equalities. Specifically, $|v_1|+|v_2|+|v_3|=1$. Since $|v_3|=1$, this requires $|v_1|=0$ and $|v_2|=0$, so $v_1=0$ and $v_2=0$.\nLet's check if $v_1=0, v_2=0, |v_3|=1$ is a solution.\nSubstitute $v_1=0$ into $v_1 - 2v_3 = 2$, which gives $-2v_3=2$, so $v_3=-1$. This satisfies $|v_3|=1$.\nSo the only possible vector is $v = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix}$. This vector satisfies $\\|v\\|_1 = 1 \\le 1$ and $v^T x^{\\star} = 0(1) + 0(0) + (-1)(-2) = 2 = \\|x^{\\star}\\|_\\infty$.\nThus, the subdifferential is a singleton set:\n$$\n\\partial (\\|x^{\\star}\\|_{\\infty}) = \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right\\}\n$$\nAnd\n$$\n\\partial f_2(x^{\\star}) = \\alpha \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right\\} = \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\alpha \\end{pmatrix} \\right\\}\n$$\n\nNow we find the total subdifferential $\\partial F(x^{\\star})$ by the Minkowski sum:\n$$\n\\partial F(x^{\\star}) = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\} + \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\alpha \\end{pmatrix} \\right\\} = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2-\\alpha \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\}\n$$\nThis set describes a line segment in $\\mathbb{R}^3$. We need to find the element $g \\in \\partial F(x^{\\star})$ that has the minimum Euclidean norm $\\|g\\|_{2}$. This is equivalent to minimizing $\\|g\\|_{2}^2$.\nLet $g(s_1) = \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2-\\alpha \\end{pmatrix}$.\n$$\n\\|g(s_1)\\|_{2}^2 = (-1)^2 + s_1^2 + (-2-\\alpha)^2 = 1 + s_1^2 + (2+\\alpha)^2\n$$\nTo minimize this quantity with respect to $s_1 \\in [-1, 1]$, we need to minimize $s_1^2$. The minimum value of $s_1^2$ on the interval $[-1, 1]$ is $0$, which occurs at $s_1 = 0$.\nTherefore, the subgradient $g^{\\star}$ with the minimum Euclidean norm corresponds to $s_1=0$.\n$$\ng^{\\star} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -2-\\alpha \\end{pmatrix}\n$$\nThe problem requests the answer as a row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix}-1  0  -2-\\alpha\\end{pmatrix}}\n$$"
        }
    ]
}