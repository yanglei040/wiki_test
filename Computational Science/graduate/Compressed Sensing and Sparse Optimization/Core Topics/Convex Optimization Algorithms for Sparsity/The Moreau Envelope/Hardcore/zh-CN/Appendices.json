{
    "hands_on_practices": [
        {
            "introduction": "理解一个数学算子的最佳方式往往是从一个具体的、可手动计算的例子开始。本练习正为此提供了契机。通过对一个简单但不可微的分段线性函数进行分析，你将从第一性原理出发，推导出其Moreau包络的解析表达式。这个过程将使“平滑”这一抽象概念变得具体可感，因为你将直接观察到Moreau包络是如何将原函数的尖锐“扭结”转变为平滑曲线的。",
            "id": "3167906",
            "problem": "考虑由下式定义的凸分段仿射函数 $f:\\mathbb{R}\\to\\mathbb{R}$\n$$\nf(y)=|y|+2\\max\\{0,y-1\\}=\n\\begin{cases}\n-y,  y\\leq 0,\\\\\ny,  0\\leq y\\leq 1,\\\\\n3y-2,  y\\geq 1.\n\\end{cases}\n$$\n设 $\\lambda0$ 为一个固定值。对于每个 $x\\in\\mathbb{R}$，Moreau 包络 $e_{\\lambda}f:\\mathbb{R}\\to\\mathbb{R}$ 定义为\n$$\ne_{\\lambda}f(x)=\\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}.\n$$\n从 Moreau 包络的定义和 $f$ 的分段仿射结构出发，在由区域 $y\\leq 0$、$0\\leq y\\leq 1$ 和 $y\\geq 1$ 中的无约束驻点以及扭折点 $y=0$ 和 $y=1$ 所导出的每个 $x$ 区间上，解析地推导 $e_{\\lambda}f(x)$。证明下确界是唯一达到的，并通过每个区域的一阶最优性条件确定最小值点，必要时在 $y=0$ 和 $y=1$ 处进行边界检查。然后，在每个得到的区间上，用 $x$ 和 $\\lambda$ 显式地计算 $e_{\\lambda}f(x)$，并解释 $f$ 在 $y=0$ 和 $y=1$ 处的不可微扭折点是如何在 $e_{\\lambda}f$ 中被平滑的。\n\n使用标准数学符号，将您的最终答案表示为关于 $x$ 的分段形式的单一闭式解析表达式。不需要四舍五入，也不涉及物理单位。",
            "solution": "该问题是凸分析中的一个有效练习，特别是关于给定凸分段仿射函数的 Moreau 包络的计算。所有术语都有明确定义，且前提在数学上是合理的。\n\n对于参数 $\\lambda  0$，函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 的 Moreau 包络定义为：\n$$\ne_{\\lambda}f(x) = \\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}\n$$\n令 $g(y; x) = f(y)+\\frac{1}{2\\lambda}(y-x)^{2}$。函数 $f(y)$ 是凸的。对于固定的 $x$ 和 $\\lambda  0$，项 $\\frac{1}{2\\lambda}(y-x)^{2}$ 关于 $y$ 是强凸的。一个凸函数与一个强凸函数之和是强凸的。因此，对于任意给定的 $x \\in \\mathbb{R}$，$g(y;x)$ 是关于 $y$ 的强凸函数，这保证了下确界在唯一的点 $y^*$ 处达到。这个唯一的最小值点是 $\\lambda f$ 在 $x$ 处的邻近算子，记为 $y^* = \\text{prox}_{\\lambda f}(x)$。\n\n对于凸函数，一阶最优性条件指出，最小值点 $y^*$ 是目标函数的次梯度包含零的点。$g(y;x)$ 关于 $y$ 的次梯度为：\n$$\n\\partial_y g(y;x) = \\partial f(y) + \\frac{1}{\\lambda}(y-x)\n$$\n最优性条件是 $0 \\in \\partial g(y^*;x)$，可以重写为：\n$$\nx - y^* \\in \\lambda \\partial f(y^*)\n$$\n我们必须首先计算给定函数的次微分 $\\partial f(y)$：\n$$\nf(y)=\n\\begin{cases}\n-y,  y\\leq 0,\\\\\ny,  0\\leq y\\leq 1,\\\\\n3y-2,  y\\geq 1.\n\\end{cases}\n$$\n次微分是：\n- 对于 $y  0$，$f'(y)=-1$，所以 $\\partial f(y) = \\{-1\\}$。\n- 对于 $0  y  1$，$f'(y)=1$，所以 $\\partial f(y) = \\{1\\}$。\n- 对于 $y > 1$，$f'(y)=3$，所以 $\\partial f(y) = \\{3\\}$。\n- 在扭折点 $y=0$ 处，次微分是左右导数之间的区间，$\\partial f(0) = [-1, 1]$。\n- 在扭折点 $y=1$ 处，次微分是 $\\partial f(1) = [1, 3]$。\n\n我们现在通过考虑 $y^*$ 位置的五个详尽且互斥的情况来确定最小值点 $y^*$ 和 $e_{\\lambda}f(x)$ 的相应值，这些情况划分了 $x$ 的定义域。\n\n**情况 1：最小值点位于区域 $y^*  0$ 中。**\n这里，$\\partial f(y^*) = \\{-1\\}$。最优性条件变为 $x - y^* = \\lambda(-1) = -\\lambda$，这意味着 $y^* = x + \\lambda$。这种情况成立当且仅当 $y^*  0$，即 $x + \\lambda  0$，或 $x  -\\lambda$。\n对于 $x  -\\lambda$，最小值点是 $y^*=x+\\lambda$。Moreau 包络为：\n$$\ne_{\\lambda}f(x) = f(x+\\lambda) + \\frac{1}{2\\lambda}((x+\\lambda)-x)^2 = -(x+\\lambda) + \\frac{\\lambda^2}{2\\lambda} = -x - \\lambda + \\frac{\\lambda}{2} = -x - \\frac{\\lambda}{2}\n$$\n\n**情况 2：最小值点位于扭折点 $y^* = 0$ 处。**\n这里，$\\partial f(0) = [-1, 1]$。最优性条件变为 $x - 0 \\in \\lambda[-1, 1]$，简化为 $x \\in [-\\lambda, \\lambda]$。\n对于 $x \\in [-\\lambda, \\lambda]$，最小值点是 $y^*=0$。Moreau 包络为：\n$$\ne_{\\lambda}f(x) = f(0) + \\frac{1}{2\\lambda}(0-x)^2 = 0 + \\frac{x^2}{2\\lambda} = \\frac{x^2}{2\\lambda}\n$$\n\n**情况 3：最小值点位于区域 $0  y^*  1$ 中。**\n这里，$\\partial f(y^*) = \\{1\\}$。最优性条件是 $x - y^* = \\lambda(1) = \\lambda$，这意味着 $y^* = x - \\lambda$。这种情况成立当且仅当 $0  y^*  1$，即 $0  x - \\lambda  1$，或 $\\lambda  x  1+\\lambda$。\n对于 $\\lambda  x  1+\\lambda$，最小值点是 $y^*=x-\\lambda$。Moreau 包络为：\n$$\ne_{\\lambda}f(x) = f(x-\\lambda) + \\frac{1}{2\\lambda}((x-\\lambda)-x)^2 = (x-\\lambda) + \\frac{\\lambda^2}{2\\lambda} = x - \\lambda + \\frac{\\lambda}{2} = x - \\frac{\\lambda}{2}\n$$\n\n**情况 4：最小值点位于扭折点 $y^* = 1$ 处。**\n这里，$\\partial f(1) = [1, 3]$。最优性条件是 $x - 1 \\in \\lambda[1, 3]$，这意味着 $\\lambda \\leq x - 1 \\leq 3\\lambda$，或 $1+\\lambda \\leq x \\leq 1+3\\lambda$。\n对于 $x \\in [1+\\lambda, 1+3\\lambda]$，最小值点是 $y^*=1$。Moreau 包络为：\n$$\ne_{\\lambda}f(x) = f(1) + \\frac{1}{2\\lambda}(1-x)^2 = (3(1)-2) + \\frac{(x-1)^2}{2\\lambda} = 1 + \\frac{(x-1)^2}{2\\lambda}\n$$\n\n**情况 5：最小值点位于区域 $y^* > 1$ 中。**\n这里，$\\partial f(y^*) = \\{3\\}$。最优性条件是 $x - y^* = \\lambda(3) = 3\\lambda$，这意味着 $y^* = x - 3\\lambda$。这种情况成立当且仅当 $y^* > 1$，即 $x - 3\\lambda > 1$，或 $x > 1+3\\lambda$。\n对于 $x > 1+3\\lambda$，最小值点是 $y^*=x-3\\lambda$。Moreau 包络为：\n$$\ne_{\\lambda}f(x) = f(x-3\\lambda) + \\frac{1}{2\\lambda}((x-3\\lambda)-x)^2 = (3(x-3\\lambda)-2) + \\frac{(3\\lambda)^2}{2\\lambda} = 3x - 9\\lambda - 2 + \\frac{9\\lambda}{2} = 3x - 2 - \\frac{9\\lambda}{2}\n$$\n\n综合这五种情况，我们得到 Moreau 包络 $e_{\\lambda}f(x)$ 的分段解析表达式：\n$$\ne_{\\lambda}f(x) =\n\\begin{cases}\n-x - \\frac{\\lambda}{2},  x  -\\lambda \\\\\n\\frac{x^2}{2\\lambda},  -\\lambda \\leq x \\leq \\lambda \\\\\nx - \\frac{\\lambda}{2},  \\lambda  x  1+\\lambda \\\\\n1 + \\frac{(x-1)^2}{2\\lambda},  1+\\lambda \\leq x \\leq 1+3\\lambda \\\\\n3x - 2 - \\frac{9\\lambda}{2},  x > 1+3\\lambda\n\\end{cases}\n$$\n函数 $e_{\\lambda}f(x)$ 是连续可微的（$C^1$）。其导数由通用公式 $e'_{\\lambda}f(x) = \\frac{1}{\\lambda}(x - \\text{prox}_{\\lambda f}(x))$ 给出。对每一段计算这个导数：\n- 对于 $x  -\\lambda$，$e'_{\\lambda}f(x) = -1$。\n- 对于 $-\\lambda  x  \\lambda$，$e'_{\\lambda}f(x) = \\frac{x}{\\lambda}$。\n- 对于 $\\lambda  x  1+\\lambda$，$e'_{\\lambda}f(x) = 1$。\n- 对于 $1+\\lambda  x  1+3\\lambda$，$e'_{\\lambda}f(x) = \\frac{x-1}{\\lambda}$。\n- 对于 $x > 1+3\\lambda$，$e'_{\\lambda}f(x) = 3$。\n很容易验证，这个导数在所有边界点 $x=-\\lambda, \\lambda, 1+\\lambda, 1+3\\lambda$ 处都是连续的。这个 $C^1$ 属性展示了 Moreau 包络的平滑效应。$f(y)$ 在 $y=0$ 和 $y=1$ 处的不可微扭折点被平滑了。$f(y)$ 在 $y=0$ 处的导数从 $-1$ 到 $1$ 的跳跃，被 $e'_{\\lambda}f(x)$ 在区间 $x \\in [-\\lambda, \\lambda]$ 上从 $-1$ 到 $1$ 的平滑线性过渡所取代。类似地，$f(y)$ 在 $y=1$ 处的导数从 $1$ 到 $3$ 的跳跃，被 $e'_{\\lambda}f(x)$ 在区间 $x \\in [1+\\lambda, 1+3\\lambda]$ 上从 $1$ 到 $3$ 的平滑线性过渡所取代。$e_{\\lambda}f(x)$ 中的二次分段起到了“圆滑”原始函数 $f(y)$ 尖角的作用。",
            "answer": "$$\n\\boxed{\ne_{\\lambda}f(x) =\n\\begin{cases}\n-x - \\frac{\\lambda}{2},  x  -\\lambda \\\\\n\\frac{x^2}{2\\lambda},  -\\lambda \\leq x \\leq \\lambda \\\\\nx - \\frac{\\lambda}{2},  \\lambda  x  1+\\lambda \\\\\n1 + \\frac{(x-1)^2}{2\\lambda},  1+\\lambda \\leq x \\leq 1+3\\lambda \\\\\n3x - 2 - \\frac{9\\lambda}{2},  x > 1+3\\lambda\n\\end{cases}\n}\n$$"
        },
        {
            "introduction": "$L_1$范数是稀疏优化的基石，但其在原点的不可微性给标准梯度方法的应用带来了挑战。本练习将演示Moreau包络如何通过构造一个平滑的代理函数来巧妙地解决此问题。你不仅将推导出该平滑函数的梯度，还将揭示它与软阈值算子之间的深刻联系，最终得出一个重要的见解：对Moreau包络进行梯度下降，等价于应用著名的近端点算法。",
            "id": "3126039",
            "problem": "考虑一个真、下半连续的凸函数 $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 以及在参数 $\\lambda  0$ 下定义的 Moreau 包络\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\n您的任务是使用 Moreau 包络来平滑一个非光滑凸函数，并从第一性原理推导其梯度，然后在这个平滑代理上实现梯度下降。\n\n使用的基本原理：\n- 凸性和次梯度的定义：对于一个凸函数 $f$，在 $u$ 处的次微分，记作 $\\partial f(u)$，是所有满足 $f(v) \\ge f(u) + g^\\top (v - u)$（对于所有 $v$）的次梯度 $g$ 的集合。\n- 凸最小化的最优性条件：如果 $u^\\star$ 是凸函数 $g(u)$ 的最小值点，则 $0 \\in \\partial g(u^\\star)$。\n- 函数 $f$ 在参数 $\\lambda$ 下的近端算子，定义为\n$$\n\\operatorname{prox}_{\\lambda f}(w) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\n- Danskin 定理（参数优化灵敏度）：如果 $\\phi(u,w)$ 对 $u$ 是凸的，对 $w$ 是可微的，并且对于所有 $w$，最小化子 $u^\\star(w)$ 是唯一的，那么 $g(w) \\triangleq \\min_{u} \\phi(u,w)$ 是可微的且 $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$。\n\n问题要求：\n1. 仅从上述定义和凸最小化的最优性条件出发，推导梯度 $\\nabla M_{\\lambda} f(w)$ 关于近端算子 $\\operatorname{prox}_{\\lambda f}(w)$ 的表达式。证明所有步骤，确保每一步都源于所提供的基本原理。\n2. 专门针对非光滑凸函数 $f(w) = \\alpha \\|w\\|_1$（其中 $\\alpha \\ge 0$ 且 $w \\in \\mathbb{R}^d$）。推导该 $f$ 的闭式近端算子，并得到 $\\nabla M_{\\lambda} f(w)$ 的显式逐坐标公式，确保推导过程与近端算子和 Moreau 包络的定义一致。\n3. 针对专门化的 $f(w) = \\alpha \\|w\\|_1$，实现一个梯度下降算法以最小化 $M_{\\lambda} f(w)$。使用固定步长 $\\eta = \\lambda$。基于 $M_{\\lambda} f$ 梯度的 Lipschitz 连续性，解释为什么 $\\eta = \\lambda$ 是一个有效的选择。\n4. 在您的程序中，实现用于计算专门化的 $f$ 的近端算子、Moreau 包络值及其梯度的函数。然后从给定的初始点开始运行梯度下降，进行固定次数的迭代，并报告最终的 Moreau 包络值 $M_{\\lambda} f(w_T)$，其中 $w_T$ 是 $T$ 步后的最终迭代点。\n\n测试套件：\n对以下参数集运行您的程序。在每种情况下，$w_0$ 是初始向量，$d$ 是维度（等于 $w_0$ 的长度），$\\alpha$ 和 $\\lambda$ 是标量，其中 $\\alpha \\ge 0$ 且 $\\lambda  0$，$T$ 是梯度下降的迭代次数。\n- 情况 1：$\\alpha = 0.5$，$\\lambda = 0.2$，$d = 3$，$w_0 = \\left[3, -1, 0.05\\right]$，$T = 100$。\n- 情况 2：$\\alpha = 0.5$，$\\lambda = 1.0$，$d = 3$，$w_0 = \\left[-2, 2, 0\\right]$，$T = 50$。\n- 情况 3（边界条件）：$\\alpha = 0.0$，$\\lambda = 0.3$，$d = 4$，$w_0 = \\left[1, -1, 2, -2\\right]$，$T = 30$。\n- 情况 4（小 $\\lambda$）：$\\alpha = 1.0$，$\\lambda = 0.05$，$d = 5$，$w_0 = \\left[1, -0.5, 0.2, -3, 4\\right]$，$T = 200$。\n- 情况 5（大 $\\lambda$）：$\\alpha = 0.2$，$\\lambda = 2.0$，$d = 2$，$w_0 = \\left[10, -10\\right]$，$T = 50$。\n\n答案规格：\n- 对于每个测试用例，计算经过 $T$ 次步长为 $\\eta = \\lambda$ 的梯度下降迭代后的最终标量值 $M_{\\lambda} f(w_T)$。\n- 您的程序应生成单行输出，其中包含这 5 个最终值，以逗号分隔的列表形式并用方括号括起来，顺序与给定的情况一致：例如，$\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$。\n- 每个输出元素必须是实数（浮点数）。此问题不涉及物理单位，也不需要角度或百分比。\n\n该场景纯属数学性质，并与统计学习一致。上述陈述中的所有符号、变量、函数、算子和数字都必须用 LaTeX 表示。",
            "solution": "该问题是有效的，因为它是数学上良定义的、自洽的，并且基于凸优化和统计学习的标准原理。我们按要求进行推导和实现。\n\n### 1. Moreau 包络梯度的推导\n\n一个真、下半连续的凸函数 $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 的 Moreau 包络定义为\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\n我们定义函数 $\\phi(u, w) = f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2$。那么 $M_{\\lambda} f(w) = \\inf_{u} \\phi(u, w)$。\n近端算子被定义为该表达式的唯一最小化子：\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\phi(u, w).\n$$\n最小化子的唯一性得到保证，因为 $f(u)$ 是凸的，而二次项 $\\frac{1}{2\\lambda} \\|u - w\\|_2^2$ 关于 $u$ 是强凸的（对于 $\\lambda  0$），使得它们的和 $\\phi(u, w)$ 关于 $u$ 是强凸的。\n\n我们已知 Danskin 定理，该定理指出，如果最小化子 $u^\\star(w)$ 是唯一的，那么 $g(w) \\triangleq \\min_{u} \\phi(u,w)$ 的梯度由 $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$ 给出。\n在我们的例子中，$g(w) = M_{\\lambda} f(w)$ 且唯一的最小化子是 $u^\\star(w) = \\operatorname{prox}_{\\lambda f}(w)$。函数 $\\phi(u, w)$ 关于 $w$ 是可微的。我们计算它关于 $w$ 的梯度 $\\nabla_w \\phi(u, w)$：\n$$\n\\nabla_w \\phi(u, w) = \\nabla_w \\left( f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right).\n$$\n由于 $f(u)$ 不依赖于 $w$，其关于 $w$ 的梯度为零。二次项的梯度是：\n$$\n\\nabla_w \\left( \\frac{1}{2\\lambda} (u - w)^\\top(u - w) \\right) = \\frac{1}{2\\lambda} \\cdot 2(u-w) \\cdot (-1) = -\\frac{1}{\\lambda}(u - w) = \\frac{1}{\\lambda}(w - u).\n$$\n应用 Danskin 定理，我们将最小化子 $u = \\operatorname{prox}_{\\lambda f}(w)$ 代入该梯度表达式中：\n$$\n\\nabla M_{\\lambda} f(w) = \\nabla_w \\phi(\\operatorname{prox}_{\\lambda f}(w), w) = \\frac{1}{\\lambda} (w - \\operatorname{prox}_{\\lambda f}(w)).\n$$\n这就是 Moreau 包络梯度的所求表达式。\n\n### 2. 专门针对 $f(w) = \\alpha \\|w\\|_1$\n\n现在我们考虑特定的非光滑凸函数 $f(w) = \\alpha \\|w\\|_1 = \\alpha \\sum_{i=1}^d |w_i|$，其中 $\\alpha \\ge 0$。\n为了找到近端算子 $\\operatorname{prox}_{\\lambda f}(w)$，我们必须解决以下最小化问题：\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\alpha \\sum_{i=1}^d |u_i| + \\frac{1}{2\\lambda} \\sum_{i=1}^d (u_i - w_i)^2 \\right\\}.\n$$\n该目标函数是可分离的，这意味着它可以对每个坐标 $u_i$ 独立地进行最小化：\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2 \\right\\}.\n$$\n令 $g(u_i) = \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2$。根据凸最小化的最优性条件，最小化子 $u_i^\\star$ 必须满足 $0 \\in \\partial g(u_i^\\star)$。次微分是 $\\partial g(u_i) = \\alpha \\partial|u_i| + \\frac{1}{\\lambda}(u_i - w_i)$。\n绝对值函数的次微分是 $\\partial|x| = \\operatorname{sgn}(x)$ 如果 $x \\ne 0$，以及 $\\partial|x| = [-1, 1]$ 如果 $x = 0$。\n条件 $0 \\in \\partial g(u_i^\\star)$ 意味着 $w_i - u_i^\\star \\in \\lambda\\alpha \\partial|u_i^\\star|$。我们分析 $u_i^\\star$ 的三种情况：\n1. 如果 $u_i^\\star > 0$，那么 $\\partial|u_i^\\star| = \\{1\\}$。条件变为 $w_i - u_i^\\star = \\lambda\\alpha$，所以 $u_i^\\star = w_i - \\lambda\\alpha$。这仅在 $w_i - \\lambda\\alpha > 0$，即 $w_i > \\lambda\\alpha$ 时成立。\n2. 如果 $u_i^\\star  0$，那么 $\\partial|u_i^\\star| = \\{-1\\}$。条件变为 $w_i - u_i^\\star = -\\lambda\\alpha$，所以 $u_i^\\star = w_i + \\lambda\\alpha$。这仅在 $w_i + \\lambda\\alpha  0$，即 $w_i  -\\lambda\\alpha$ 时成立。\n3. 如果 $u_i^\\star = 0$，那么 $\\partial|u_i^\\star| = [-1, 1]$。条件变为 $w_i \\in \\lambda\\alpha[-1, 1]$，这等价于 $|w_i| \\le \\lambda\\alpha$。\n\n综合这些情况，我们得到软阈值算子 $S_{\\lambda\\alpha}$：\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = S_{\\lambda\\alpha}(w_i) \\triangleq \\begin{cases} w_i - \\lambda\\alpha  \\text{若 } w_i  \\lambda\\alpha \\\\ w_i + \\lambda\\alpha  \\text{若 } w_i  -\\lambda\\alpha \\\\ 0  \\text{若 } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\n这可以紧凑地写为 $(\\operatorname{prox}_{\\lambda f}(w))_i = \\operatorname{sgn}(w_i) \\max(|w_i| - \\lambda\\alpha, 0)$。\n\n使用这个闭式近端算子，我们可以从第 1 部分写出 Moreau 包络梯度的显式公式：\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\frac{1}{\\lambda}(w_i - (\\operatorname{prox}_{\\lambda f}(w))_i) = \\frac{1}{\\lambda}(w_i - S_{\\lambda\\alpha}(w_i)).\n$$\n这得到一个逐坐标的梯度公式：\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\begin{cases} \\alpha  \\text{若 } w_i  \\lambda\\alpha \\\\ -\\alpha  \\text{若 } w_i  -\\lambda\\alpha \\\\ w_i/\\lambda  \\text{若 } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\n\n### 3. 梯度下降算法和步长合理性证明\n\n用于最小化 $M_{\\lambda} f(w)$ 的梯度下降更新法则是 $w_{k+1} = w_k - \\eta \\nabla M_{\\lambda} f(w_k)$。我们需要使用步长 $\\eta = \\lambda$。\n为了证明这个选择的合理性，我们必须表明梯度 $\\nabla M_{\\lambda} f$ 是 Lipschitz 连续的。已知 Moreau 包络的梯度是 $1/\\lambda$-Lipschitz 连续的（这是 Baillon-Haddad 定理的一个结果）。对于一个凸函数 $f$，其 Moreau 包络 $M_\\lambda f$ 是连续可微的，其梯度是 Lipschitz 连续的，常数为 $L = 1/\\lambda$。\n对于具有 $L$-Lipschitz 连续梯度的函数，梯度下降的一个标准结果是，对于任何固定的步长 $\\eta \\in (0, 2/L)$，收敛性是有保证的。在我们的例子中，$L=1/\\lambda$，所以收敛条件是 $\\eta \\in (0, 2\\lambda)$。选择 $\\eta = \\lambda$ 显然落在这个区间内，使其成为一个有效的选择。\n\n此外，这个特定的选择带来了一个显著的简化。将 $\\eta = \\lambda$ 和梯度表达式代入更新法则：\n$$\nw_{k+1} = w_k - \\lambda \\cdot \\nabla M_{\\lambda} f(w_k) = w_k - \\lambda \\cdot \\left(\\frac{1}{\\lambda}(w_k - \\operatorname{prox}_{\\lambda f}(w_k))\\right) = w_k - (w_k - \\operatorname{prox}_{\\lambda f}(w_k)).\n$$\n这简化为：\n$$\nw_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k).\n$$\n因此，在 Moreau 包络 $M_{\\lambda} f(w)$ 上执行步长为 $\\eta=\\lambda$ 的梯度下降，等同于对原始函数 $f(w)$ 应用近端点算法。这是平滑技术和近端方法之间的一个强大联系。\n\n### 4. 实现策略\n\n基于以上分析，实现将包括以下内容：\n1.  一个函数 `prox_l1(w, alpha, lambda_val)`，它实现软阈值算子 $S_{\\lambda\\alpha}(w)$。\n2.  一个函数 `moreau_envelope_l1(w, alpha, lambda_val)`，它通过首先找到 $u = \\operatorname{prox}_{\\lambda f}(w)$ 然后计算 $f(u) + \\frac{1}{2\\lambda}\\|u-w\\|_2^2$ 来计算 $M_{\\lambda} f(w)$。\n3.  主循环将迭代 $T$ 步。在每一步中，它将使用简化的近端点更新来更新权重向量 $w$：$w_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k)$。\n4.  经过 $T$ 次迭代后，为最终的迭代点 $w_T$ 计算最终的 Moreau 包络值 $M_{\\lambda} f(w_T)$ 并存储。对每个测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing gradient descent on the Moreau envelope\n    for the L1 norm, which simplifies to the proximal point algorithm.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # alpha, lambda, w0, T\n        (0.5, 0.2, np.array([3.0, -1.0, 0.05]), 100),\n        (0.5, 1.0, np.array([-2.0, 2.0, 0.0]), 50),\n        (0.0, 0.3, np.array([1.0, -1.0, 2.0, -2.0]), 30),\n        (1.0, 0.05, np.array([1.0, -0.5, 0.2, -3.0, 4.0]), 200),\n        (0.2, 2.0, np.array([10.0, -10.0]), 50),\n    ]\n\n    def prox_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the proximal operator for f(w) = alpha * ||w||_1.\n        This is the soft-thresholding operator.\n        \"\"\"\n        threshold = lambda_val * alpha\n        # Component-wise operation\n        return np.sign(w) * np.maximum(np.abs(w) - threshold, 0.0)\n\n    def moreau_envelope_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the value of the Moreau envelope for f(w) = alpha * ||w||_1.\n        M_lambda_f(w) = f(prox(w)) + (1/(2*lambda)) * ||prox(w) - w||^2\n        \"\"\"\n        u_prox = prox_l1(w, alpha, lambda_val)\n        \n        # f(u_prox) = alpha * ||u_prox||_1\n        f_u_prox = alpha * np.linalg.norm(u_prox, 1)\n        \n        # (1/(2*lambda)) * ||u_prox - w||_2^2\n        quadratic_term = (1.0 / (2.0 * lambda_val)) * np.linalg.norm(u_prox - w, 2)**2\n        \n        return f_u_prox + quadratic_term\n\n    results = []\n    for case in test_cases:\n        alpha, lambda_val, w0, T = case\n        \n        w = w0.copy() # Start with the initial vector\n        \n        # Gradient descent with step size eta = lambda simplifies to the proximal point algorithm\n        # w_{k+1} = prox(w_k)\n        for _ in range(T):\n            w = prox_l1(w, alpha, lambda_val)\n        \n        # After T iterations, we have w_T. Compute the final Moreau envelope value.\n        final_value = moreau_envelope_l1(w, alpha, lambda_val)\n        results.append(final_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在标准框架的基础上，我们可以通过将欧几里得度量替换为由正定矩阵$M$诱导的度量，来推广Moreau包络。这种更高级的构造允许我们根据优化问题的特定几何结构来定制平滑过程，这也是预处理技术的核心思想。在本练习中，你将分析这个广义包络，并实现一个算法来探究其作为预处理器的有效性，从而展示一个理论工具如何直接转化为解决实际问题时计算效率的显著提升。",
            "id": "3489010",
            "problem": "您的任务是在压缩感知和稀疏优化的背景下分析广义 Moreau 包络，其中的度量由一个对称正定矩阵定义。令 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 是一个真、闭、凸函数。对于一个标量 $\\lambda0$ 和一个对称正定矩阵 $M\\succ 0$，广义 Moreau 包络定义为\n$$\ne_{\\lambda,M} f(x) \\;\\triangleq\\; \\inf_{y\\in\\mathbb{R}^n}\\Big\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_M^2 \\Big\\},\n$$\n其中 $\\|u\\|_M \\triangleq \\sqrt{u^\\top M u}$。在本问题中，我们特指 $f(x) = \\|x\\|_1$。\n\n仅从凸分析中的基本定义和最优性条件出发，完成以下任务：\n\n1. 根据获得下确界的唯一极小化子 $y^\\star(x)$，推导广义 Moreau 包络 $e_{\\lambda,M}\\|x\\|_1$ 在点 $x\\in\\mathbb{R}^n$ 处的梯度。您的推导必须从包络定义中内部最小化问题的一阶最优性条件开始，并且必须明确证明在 $M\\succ 0$ 条件下包络的可微性。\n\n2. 证明 $e_{\\lambda,M}\\|x\\|_1$ 的梯度是全局 Lipschitz 连续的，并根据矩阵 $M$ 和参数 $\\lambda$ 给出一个有效的 Lipschitz 常数上界。您的论证必须依赖于邻近映射的非扩张性质和基本的谱范数界，不得引用任何关于包络梯度 Lipschitz 常数的现成公式。\n\n3. 展示如何使用基于保证收敛的显式步长的一阶分裂方法来计算邻近点\n$$\n\\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x) \\;\\triangleq\\; \\arg\\min_{y\\in\\mathbb{R}^n}\\Big\\{\\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2\\Big\\}\n$$\n。具体来说，利用光滑的二次部分和非光滑的 $\\ell_1$ 部分推导出一个迭代格式，并解释如何利用 $M$ 的最大特征值和 $\\lambda$ 来选择步长。\n\n4. 在光滑函数 $x\\mapsto e_{\\lambda,M}\\|x\\|_1$ 上实现梯度下降，使用第 1 项中推导的梯度，步长选择为第 2 项中确立的 Lipschitz 常数的倒数。在每次迭代中使用第 3 项的邻近计算来评估梯度。停止准则必须是梯度的欧几里得范数小于或等于容差 $t_{\\text{grad}}$，或者达到了预设的最大迭代次数。报告满足停止准则所需的迭代次数。\n\n5. 通过选择与传感矩阵 $A\\in\\mathbb{R}^{m\\times n}$ 通过 $A^\\top A$ 相关的 $M$ 来研究预处理的效果。构建以下测试套件，其中 $A$ 是一个实数矩阵，其条目是使用指定种子生成的独立同分布高斯分布， $M$ 的选择为：\n    - $M=\\mathrm{I}_n$ (单位矩阵),\n    - $M=\\mathrm{diag}(A^\\top A)$ (对角预处理器),\n    - $M=A^\\top A$ (完整预处理器).\n\n对所有测试使用相同的初始点 $x_0\\in\\mathbb{R}^n$，其条目为独立同分布的高斯分布。\n\n对于下面的每个测试用例，从 $x_0$ 开始在 $e_{\\lambda,M}\\|x\\|_1$ 上运行梯度下降，并给出达到 $\\|\\nabla e_{\\lambda,M}\\|x\\|_1(x_k)\\|_2 \\leq t_{\\text{grad}}$ 所需的迭代次数，如果未达到容差，则为最大迭代次数：\n\n- 测试用例 1：$n=50$，$m=80$，使用种子 $42$ 生成 $A$，$\\lambda=0.1$，$M=\\mathrm{I}_n$，$t_{\\text{grad}}=10^{-3}$，最大迭代次数 $500$。\n- 测试用例 2：$n=50$，$m=80$，使用种子 $42$ 生成 $A$，$\\lambda=0.1$，$M=\\mathrm{diag}(A^\\top A)$，$t_{\\text{grad}}=10^{-3}$，最大迭代次数 $500$。\n- 测试用例 3：$n=50$，$m=80$，使用种子 $42$ 生成 $A$，$\\lambda=0.1$，$M=A^\\top A$，$t_{\\text{grad}}=10^{-3}$，最大迭代次数 $500$。\n- 测试用例 4：$n=50$，$m=80$，使用种子 $42$ 生成 $A$，$\\lambda=0.01$，$M=\\mathrm{I}_n$，$t_{\\text{grad}}=10^{-3}$，最大迭代次数 $800$。\n- 测试用例 5：$n=50$，$m=80$，使用种子 $7$ 生成 $A$，$\\lambda=0.5$，$M=\\mathrm{diag}(A^\\top A)$，$t_{\\text{grad}}=10^{-3}$，最大迭代次数 $300$。\n\n所有引用的随机变量必须是具有指定种子和维度的标准正态分布的实现，矩阵 $A$ 的条目必须是服从 $\\mathcal{N}(0,1)$ 分布的。初始点 $x_0$ 必须使用独立于 $A$ 的种子 $123$ 生成。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔的整数列表形式的结果，顺序与测试用例相同（例如，$[\\text{result1},\\text{result2},\\text{result3},\\text{result4},\\text{result5}]$）。如果未达到容差，则为该用例输出最大迭代次数。此问题不涉及任何物理单位、角度或百分比。",
            "solution": "所给问题是凸优化领域一个适定且有科学依据的练习，具体涉及广义 Moreau 包络。我们将按要求进行系统的推导和分析。\n\n令 $f(x) = \\|x\\|_1$，这是一个真、闭、凸函数。对于一个标量 $\\lambda0$ 和一个对称正定矩阵 $M\\succ 0$，广义 Moreau 包络定义为\n$$\ne_{\\lambda,M} f(x) \\triangleq \\inf_{y\\in\\mathbb{R}^n}\\Big\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_M^2 \\Big\\},\n$$\n其中 $\\|u\\|_M = \\sqrt{u^\\top M u}$。\n\n### 1. 梯度的推导\n\n令 $J(x, y) = \\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2$。函数 $y \\mapsto J(x,y)$ 是一个凸函数（$\\|y\\|_1$）和一个严格凸函数（$\\frac{1}{2\\lambda}\\|y-x\\|_M^2$，因为 $M \\succ 0$ 且 $\\lambda  0$）的和。因此，$J(x,y)$ 在 $y$ 上是严格凸的。此外，当 $\\|y\\|_2 \\to \\infty$ 时，$J(x,y) \\to \\infty$，这意味着该函数是强制的。在 $\\mathbb{R}^n$ 上的严格凸且强制的函数有唯一的极小化子。令这个唯一的极小化子表示为 $y^\\star(x)$：\n$$\ny^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x) \\triangleq \\arg\\min_{y\\in\\mathbb{R}^n} J(x,y).\n$$\nMoreau 包络可以写为 $e_{\\lambda,M}\\|x\\|_1 = J(x, y^\\star(x))$。\n关于 $y$ 的最小化问题的一阶充要最优性条件是，零向量必须在 $J(x,y)$于$y=y^\\star(x)$ 处的次梯度中：\n$$\n0 \\in \\partial_y J(x, y)\\Big|_{y=y^\\star(x)}.\n$$\n$\\|y\\|_1$ 的次梯度是 $\\partial\\|y\\|_1$。光滑二次项的梯度是 $\\nabla_y \\left(\\frac{1}{2\\lambda}(y-x)^\\top M (y-x)\\right) = \\frac{1}{\\lambda}M(y-x)$。\n因此，最优性条件是：\n$$\n0 \\in \\partial\\|y^\\star(x)\\|_1 + \\frac{1}{\\lambda}M(y^\\star(x)-x).\n$$\n这可以重写为：\n$$\n\\frac{1}{\\lambda} M(x - y^\\star(x)) \\in \\partial\\|y^\\star(x)\\|_1.\n$$\n函数 $(x,y) \\mapsto J(x,y)$ 在 $(x,y)$ 上是联合凸的（这不是必需的，但很有用）。因为对于每个 $x$，极小化子 $y^\\star(x)$ 是唯一的，所以值函数 $e_{\\lambda,M}\\|x\\|_1$ 是连续可微的。这是 Danskin 定理或包络定理的一个推论。包络的梯度可以通过求 $J(x,y)$ 关于 $x$ 的偏导数并在 $y=y^\\star(x)$ 处求值得到：\n$$\n\\nabla e_{\\lambda,M}\\|x\\|_1(x) = \\nabla_x J(x, y)\\Big|_{y=y^\\star(x)}.\n$$\n我们计算 $J(x,y)$ 关于 $x$ 的梯度：\n$$\n\\nabla_x J(x, y) = \\nabla_x \\left( \\|y\\|_1 + \\frac{1}{2\\lambda}(y-x)^\\top M (y-x) \\right) = \\frac{1}{2\\lambda} \\nabla_x (x^\\top M x - 2x^\\top M y + y^\\top M y) = \\frac{1}{2\\lambda}(2Mx - 2My) = \\frac{1}{\\lambda}M(x-y).\n$$\n在 $y=y^\\star(x)$ 处求值，得到 Moreau 包络的梯度：\n$$\n\\nabla e_{\\lambda,M}\\|x\\|_1(x) = \\frac{1}{\\lambda}M(x-y^\\star(x)).\n$$\n\n### 2. 梯度的 Lipschitz 连续性\n\n令 $p(x) = y^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x)$。梯度为 $\\nabla e(x) = \\frac{1}{\\lambda}M(x-p(x))$。我们希望找到这个映射的一个 Lipschitz 常数。\n根据一阶最优性条件，对于任意 $x_1, x_2 \\in \\mathbb{R}^n$，我们有：\n$$\n\\frac{1}{\\lambda} M(x_1 - p(x_1)) \\in \\partial\\|p(x_1)\\|_1\n$$\n$$\n\\frac{1}{\\lambda} M(x_2 - p(x_2)) \\in \\partial\\|p(x_2)\\|_1\n$$\n根据凸函数 $\\|\\cdot\\|_1$ 的次梯度的单调性，我们有：\n$$\n\\left(\\frac{1}{\\lambda} M(x_1 - p(x_1)) - \\frac{1}{\\lambda} M(x_2 - p(x_2))\\right)^\\top (p(x_1) - p(x_2)) \\ge 0.\n$$\n令 $z = x_1 - x_2$ 且 $w = p(x_1) - p(x_2)$。整理该不等式可得：\n$$\n(M(z-w))^\\top w \\ge 0 \\implies (z-w)^\\top M w \\ge 0 \\implies z^\\top Mw \\ge w^\\top Mw = \\|w\\|_M^2.\n$$\n该性质表明算子 $p(x)$ 在 M-范数下是稳定非扩张的，如下所示：\n$$\n\\|z-w\\|_M^2 = (z-w)^\\top M (z-w) = \\|z\\|_M^2 - 2 z^\\top M w + \\|w\\|_M^2 \\le \\|z\\|_M^2 - 2\\|w\\|_M^2 + \\|w\\|_M^2 = \\|z\\|_M^2 - \\|w\\|_M^2.\n$$\n这意味着 $\\|p(x_1)-p(x_2)\\|_M^2 \\le \\|x_1-x_2\\|_M^2$，所以 $p(x)$ 在 M-范数下是非扩张的。\n令 $T(x) = x-p(x)$。我们有 $\\|T(x_1)-T(x_2)\\|_M^2 = \\|z-w\\|_M^2 \\le \\|z\\|_M^2 - \\|w\\|_M^2 \\le \\|z\\|_M^2$。所以 $T(x)$ 在 M-范数下也是非扩张的。\n我们需要对梯度之差的欧几里得范数进行界定：\n$$\n\\|\\nabla e(x_1) - \\nabla e(x_2)\\|_2 = \\left\\| \\frac{1}{\\lambda}M(T(x_1)-T(x_2)) \\right\\|_2.\n$$\n令 $u = T(x_1)-T(x_2)$。由于 $M \\succ 0$，其对称平方根 $M^{1/2}$ 存在且可逆。\n$$\n\\|Mu\\|_2 = \\|M^{1/2}M^{1/2}u\\|_2 \\le \\|M^{1/2}\\|_2 \\|M^{1/2}u\\|_2.\n$$\n谱范数 $\\|M^{1/2}\\|_2$ 是 $\\lambda_{\\max}(M^{1/2}) = \\sqrt{\\lambda_{\\max}(M)}$。\n项 $\\|M^{1/2}u\\|_2$ 等于 $\\|u\\|_M$。\n所以，$\\|Mu\\|_2 \\le \\sqrt{\\lambda_{\\max}(M)} \\|u\\|_M$。\n由于 $T$ 在 M-范数下是非扩张的，我们有 $\\|u\\|_M = \\|T(x_1)-T(x_2)\\|_M \\le \\|x_1-x_2\\|_M$。\n此外，$\\|x_1-x_2\\|_M = \\|M^{1/2}(x_1-x_2)\\|_2 \\le \\|M^{1/2}\\|_2 \\|x_1-x_2\\|_2 = \\sqrt{\\lambda_{\\max}(M)}\\|x_1-x_2\\|_2$。\n结合这些不等式：\n$$\n\\|Mu\\|_2 \\le \\sqrt{\\lambda_{\\max}(M)} \\|u\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\|x_1-x_2\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\left( \\sqrt{\\lambda_{\\max}(M)} \\|x_1-x_2\\|_2 \\right) = \\lambda_{\\max}(M) \\|x_1-x_2\\|_2.\n$$\n代回到梯度差的表达式中：\n$$\n\\|\\nabla e(x_1) - \\nabla e(x_2)\\|_2 \\le \\frac{\\lambda_{\\max}(M)}{\\lambda} \\|x_1-x_2\\|_2.\n$$\n因此，梯度 $\\nabla e_{\\lambda,M}\\|x\\|_1$ 是全局 Lipschitz 连续的，其 Lipschitz 常数 $L \\le \\frac{\\lambda_{\\max}(M)}{\\lambda}$。由于对于对称正定矩阵有 $\\|M\\|_2 = \\lambda_{\\max}(M)$，我们可以写成 $L = \\frac{\\|M\\|_2}{\\lambda}$。\n\n### 3. 邻近点的计算\n\n邻近点 $y^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x)$ 是以下最小化问题的解：\n$$\n\\min_{y\\in\\mathbb{R}^n} F(y), \\quad \\text{其中 } F(y) = \\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2.\n$$\n这是一个复合凸优化问题。我们可以将目标函数分解为一个非光滑部分 $h(y) = \\|y\\|_1$ 和一个光滑、凸的部分 $g(y) = \\frac{1}{2\\lambda}\\|y-x\\|_M^2$。该问题可以使用一阶分裂方法（如邻近梯度法，也称为 ISTA）高效求解。\n光滑部分的梯度是：\n$$\n\\nabla g(y) = \\frac{1}{\\lambda}M(y-x).\n$$\n$\\nabla g(y)$ 的 Lipschitz 常数是 $L_g = \\frac{1}{\\lambda}\\|M\\|_2 = \\frac{\\lambda_{\\max}(M)}{\\lambda}$。\n邻近梯度迭代由 $y_{k+1} = \\operatorname{prox}_{\\tau h}(y_k - \\tau \\nabla g(y_k))$ 给出，其中 $\\tau$ 是步长。为保证收敛，必须有 $0  \\tau  2/L_g$。保证收敛的标准选择是 $\\tau = 1/L_g = \\lambda/\\lambda_{\\max}(M)$。\n$h(y)=\\|y\\|_1$ 的邻近算子是软阈值算子，$\\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(z)_i = \\operatorname{sgn}(z_i)\\max(|z_i|-\\tau, 0)$，我们将其表示为 $S_\\tau(z)$。\n代入所有部分，计算 $y^\\star(x)$ 的迭代格式是：\n$$\ny_{k+1} = S_{\\tau}\\left(y_k - \\tau \\left(\\frac{1}{\\lambda}M(y_k-x)\\right)\\right)\n$$\n当 $\\tau = \\lambda/\\lambda_{\\max}(M)$ 时，这变为：\n$$\ny_{k+1} = S_{\\lambda/\\lambda_{\\max}(M)}\\left(y_k - \\frac{\\lambda}{\\lambda_{\\max}(M)}\\frac{1}{\\lambda}M(y_k-x)\\right) = S_{\\lambda/\\lambda_{\\max}(M)}\\left(y_k - \\frac{1}{\\lambda_{\\max}(M)}M(y_k-x)\\right).\n$$\n从一个初始点（例如 $y_0=x$）开始，此迭代收敛到唯一的极小化子 $y^\\star(x)$。\n\n### 4. 梯度下降算法\n\n为了最小化关于 $x$ 的 $e_{\\lambda,M}\\|x\\|_1$，我们采用梯度下降法。更新规则是：\n$$\nx_{k+1} = x_k - \\eta \\nabla e_{\\lambda,M}\\|x\\|_1(x_k),\n$$\n其中 $\\eta$ 是步长。\n根据第 1 部分，梯度为 $\\nabla e_{\\lambda,M}\\|x\\|_1(x_k) = \\frac{1}{\\lambda}M(x_k-y^\\star(x_k))$。\n根据第 2 部分，梯度是 Lipschitz 连续的，其常数为 $L = \\frac{\\lambda_{\\max}(M)}{\\lambda}$。对于具有固定步长的梯度下降法，如果 $\\eta  2/L$，则保证收敛。问题指定使用步长 $\\eta = 1/L$。\n因此，步长为 $\\eta = \\frac{\\lambda}{\\lambda_{\\max}(M)}$。\n完整的算法如下：\n1. 初始化 $x_0$。\n2. 对于 $k=0, 1, \\dots, \\text{max\\_iterations}-1$：\n   a. 使用第 3 部分的迭代格式计算 $y^\\star(x_k)$。\n   b. 计算梯度 $g_k = \\frac{1}{\\lambda}M(x_k-y^\\star(x_k))$。\n   c. 如果 $\\|g_k\\|_2 \\le t_{\\text{grad}}$，则终止。\n   d. 更新 $x_{k+1} = x_k - \\left(\\frac{\\lambda}{\\lambda_{\\max}(M)}\\right) g_k$。\n如果在最大迭代次数内未满足停止准则，则过程终止并报告最大次数。\n\n实现将遵循此逻辑来处理指定的测试用例。请注意，当 $M=\\mathrm{I}_n$（单位矩阵）时，$\\lambda_{\\max}(M)=1$。邻近算子简化为标准的软阈值算子：$y^\\star(x) = S_\\lambda(x)$。梯度变为 $\\nabla e_{\\lambda, \\mathrm{I}_n}\\|x\\|_1(x) = \\frac{1}{\\lambda}(x - S_\\lambda(x))$，步长为 $\\eta = \\lambda$。更新简化为 $x_{k+1} = x_k - \\lambda \\frac{1}{\\lambda}(x_k-S_\\lambda(x_k)) = S_\\lambda(x_k)$。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Component-wise soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef compute_prox_M(x, M, lamb, lambda_max_M, max_prox_iter=500, prox_tol=1e-8):\n    \"\"\"\n    Computes prox_{lambda*||.||_1}^M(x).\n    Solves min_y { ||y||_1 + 1/(2*lambda) * ||y-x||_M^2 } using ISTA.\n    \"\"\"\n    n = x.shape[0]\n\n    # The problem is min_y { h(y) + g(y) } where h(y)=||y||_1 and g(y)=1/(2*lambda)||y-x||_M^2.\n    # The gradient of the smooth part g(y) is grad_g(y) = (1/lambda) * M @ (y - x).\n    # The Lipschitz constant of grad_g is L_g = lambda_max(M) / lambda.\n    # ISTA step-size is chosen as tau = 1/L_g = lambda / lambda_max(M).\n    # The ISTA update is y_{k+1} = prox_{tau*h}(y_k - tau * grad_g(y_k)).\n    # prox_{tau*h}(z) = S_tau(z).\n    # y_{k+1} = S_tau(y_k - tau * (1/lambda) * M @ (y_k - x))\n    # y_{k+1} = S_{lambda/lambda_max_M}(y_k - (lambda/lambda_max_M) * (1/lambda) * M @ (y_k - x))\n    # y_{k+1} = S_{lambda/lambda_max_M}(y_k - (1/lambda_max_M) * M @ (y_k - x))\n\n    y = np.copy(x)  # Initialization\n    tau = lamb / lambda_max_M # Threshold for soft-thresholding\n    inv_lambda_max_M = 1.0 / lambda_max_M\n\n    for _ in range(max_prox_iter):\n        y_prev = y\n        y = soft_threshold(y - inv_lambda_max_M * (M @ (y - x)), tau)\n        if np.linalg.norm(y - y_prev)  prox_tol:\n            break\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for gradient descent on the Moreau envelope.\n    \"\"\"\n    test_cases = [\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'I', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'diag', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'full', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.01, 'M_type': 'I', 't_grad': 1e-3, 'max_iter': 800},\n        {'n': 50, 'm': 80, 'seed_A': 7, 'lamb': 0.5, 'M_type': 'diag', 't_grad': 1e-3, 'max_iter': 300},\n    ]\n\n    results = []\n    \n    # Generate the common initial point x0\n    rng_x0 = np.random.default_rng(123)\n    x0 = rng_x0.standard_normal(test_cases[0]['n'])\n\n    for case in test_cases:\n        n, m, seed_A, lamb, M_type, t_grad, max_iter = \\\n            case['n'], case['m'], case['seed_A'], case['lamb'], case['M_type'], case['t_grad'], case['max_iter']\n\n        # Generate sensing matrix A\n        rng_A = np.random.default_rng(seed_A)\n        A = rng_A.standard_normal((m, n))\n\n        # Construct preconditioning matrix M\n        if M_type == 'I':\n            M = np.eye(n)\n        elif M_type == 'diag':\n            M = np.diag(np.diag(A.T @ A))\n        elif M_type == 'full':\n            M = A.T @ A\n        \n        # Calculate max eigenvalue of M\n        lambda_max_M = np.linalg.norm(M, 2)\n        \n        # Gradient descent step-size\n        eta = lamb / lambda_max_M\n\n        x_k = np.copy(x0)\n        \n        # Gradient descent loop\n        iter_count = max_iter\n        for k in range(1, max_iter + 1):\n            if M_type == 'I':\n                # Closed-form for prox with M=I\n                y_star = soft_threshold(x_k, lamb)\n            else:\n                # Use ISTA for general M\n                y_star = compute_prox_M(x_k, M, lamb, lambda_max_M)\n            \n            # Compute gradient of the Moreau envelope\n            grad = (1.0 / lamb) * (M @ (x_k - y_star))\n            \n            # Check stopping criterion\n            grad_norm = np.linalg.norm(grad)\n            if grad_norm = t_grad:\n                iter_count = k\n                break\n            \n            # Update x\n            x_k = x_k - eta * grad\n        \n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}