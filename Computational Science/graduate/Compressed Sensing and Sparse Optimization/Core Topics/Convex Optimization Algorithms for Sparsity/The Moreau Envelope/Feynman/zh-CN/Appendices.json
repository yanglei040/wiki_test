{
    "hands_on_practices": [
        {
            "introduction": "莫罗包络通过创建一个平滑的近似，为将基于梯度的方法应用于非光滑问题提供了一个强大的框架。第一个练习是基础：你将从第一性原理出发，推导莫罗包络的梯度。通过这个过程，你将揭示在包络上进行梯度下降与著名的邻近点算法之间的一个优雅联系，从而在平滑技术和经典的分裂方法之间架起一座桥梁。",
            "id": "3126039",
            "problem": "考虑一个正常、下半连续的凸函数 $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 以及在参数 $\\lambda > 0$ 下的莫罗包络，其定义为\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}。\n$$\n你的任务是使用莫罗包络来平滑一个非光滑的凸函数，并从基本定义出发推导其梯度，然后在这个平滑的代理函数上实现梯度下降。\n\n需要使用的基本原理：\n- 凸性和次梯度的定义：对于一个凸函数 $f$，在点 $u$ 的次微分 $\\partial f(u)$ 是所有满足 $f(v) \\ge f(u) + g^\\top (v - u)$（对于所有 $v$）的次梯度 $g$ 的集合。\n- 凸最小化的最优性条件：如果 $u^\\star$ 是凸函数 $g(u)$ 的一个最小化子，则 $0 \\in \\partial g(u^\\star)$。\n- 函数 $f$ 在参数 $\\lambda$ 下的近端算子，定义为\n$$\n\\operatorname{prox}_{\\lambda f}(w) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}。\n$$\n- Danskin 定理（参数优化灵敏度）：如果 $\\phi(u,w)$ 在 $u$ 上是凸的，在 $w$ 上是可微的，并且对于所有 $w$，最小化子 $u^\\star(w)$ 是唯一的，那么 $g(w) \\triangleq \\min_{u} \\phi(u,w)$ 是可微的，且 $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$。\n\n问题要求：\n1. 仅从上述定义和凸最小化的最优性条件出发，推导梯度 $\\nabla M_{\\lambda} f(w)$ 关于近端算子 $\\operatorname{prox}_{\\lambda f}(w)$ 的表达式。证明所有步骤，使每一步都源自所提供的基本原理。\n2. 将问题特化到非光滑凸函数 $f(w) = \\alpha \\|w\\|_1$，其中 $\\alpha \\ge 0$ 且 $w \\in \\mathbb{R}^d$。推导此 $f$ 的闭式近端算子，并获得 $\\nabla M_{\\lambda} f(w)$ 的显式逐坐标公式，确保推导过程与近端算子和莫罗包络的定义一致。\n3. 实现一个梯度下降算法，以最小化特化函数 $f(w) = \\alpha \\|w\\|_1$ 的莫罗包络 $M_{\\lambda} f(w)$。使用固定的步长 $\\eta = \\lambda$。解释为什么基于 $M_{\\lambda} f$ 梯度的 Lipschitz 连续性，$\\eta = \\lambda$ 是一个有效的选择。\n4. 在你的程序中，为特化的函数 $f$ 实现计算近端算子、莫罗包络值及其梯度的函数。然后，从给定的初始点开始运行梯度下降固定次数的迭代，并报告最终的莫罗包络值 $M_{\\lambda} f(w_T)$，其中 $w_T$ 是经过 $T$ 步后的最终迭代向量。\n\n测试套件：\n在以下参数集上运行你的程序。在每种情况下，$w_0$ 是初始向量，$d$ 是维度（等于 $w_0$ 的长度），$\\alpha$ 和 $\\lambda$ 是标量，其中 $\\alpha \\ge 0$ 且 $\\lambda > 0$，$T$ 是梯度下降的迭代次数。\n- 情况 1：$\\alpha = 0.5$, $\\lambda = 0.2$, $d = 3$, $w_0 = \\left[3, -1, 0.05\\right]$, $T = 100$。\n- 情况 2：$\\alpha = 0.5$, $\\lambda = 1.0$, $d = 3$, $w_0 = \\left[-2, 2, 0\\right]$, $T = 50$。\n- 情况 3（边界条件）：$\\alpha = 0.0$, $\\lambda = 0.3$, $d = 4$, $w_0 = \\left[1, -1, 2, -2\\right]$, $T = 30$。\n- 情况 4（小 $\\lambda$）：$\\alpha = 1.0$, $\\lambda = 0.05$, $d = 5$, $w_0 = \\left[1, -0.5, 0.2, -3, 4\\right]$, $T = 200$。\n- 情况 5（大 $\\lambda$）：$\\alpha = 0.2$, $\\lambda = 2.0$, $d = 2$, $w_0 = \\left[10, -10\\right]$, $T = 50$。\n\n答案规格：\n- 对于每个测试用例，使用步长 $\\eta = \\lambda$ 进行 $T$ 次梯度下降迭代后，计算最终的标量值 $M_{\\lambda} f(w_T)$。\n- 你的程序应生成单行输出，其中包含这 5 个最终值，以逗号分隔列表的形式并用方括号括起来，顺序与给定的情况一致：例如，$\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$。\n- 每个输出元素必须是实数（浮点数）。此问题不涉及物理单位，也不需要角度或百分比。\n\n该场景纯粹是数学性的，并与统计学习一致。上述陈述中的所有符号、变量、函数、算子和数字都必须用 LaTeX 表示。",
            "solution": "该问题是有效的，因为它在数学上是良定义的、自洽的，并且基于凸优化和统计学习的标准原理。我们按要求进行推导和实现。\n\n### 1. Moreau 包络梯度的推导\n\n一个正常、下半连续的凸函数 $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 的莫罗包络定义为\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}。\n$$\n我们定义函数 $\\phi(u, w) = f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2$。于是 $M_{\\lambda} f(w) = \\inf_{u} \\phi(u, w)$。\n近端算子被定义为该表达式的唯一最小化子：\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\phi(u, w)。\n$$\n最小化子的唯一性得到保证，因为 $f(u)$ 是凸的，而二次项 $\\frac{1}{2\\lambda} \\|u - w\\|_2^2$ 在 $u$ 上是强凸的（对于 $\\lambda > 0$），这使得它们的和 $\\phi(u, w)$ 在 $u$ 上是强凸的。\n\n我们已知 Danskin 定理，该定理指出如果最小化子 $u^\\star(w)$ 是唯一的，那么 $g(w) \\triangleq \\min_{u} \\phi(u,w)$ 的梯度由 $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$ 给出。\n在我们的情况下，$g(w) = M_{\\lambda} f(w)$，唯一的最小化子是 $u^\\star(w) = \\operatorname{prox}_{\\lambda f}(w)$。函数 $\\phi(u, w)$ 关于 $w$ 是可微的。我们计算其梯度 $\\nabla_w \\phi(u, w)$：\n$$\n\\nabla_w \\phi(u, w) = \\nabla_w \\left( f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right)。\n$$\n由于 $f(u)$ 不依赖于 $w$，其关于 $w$ 的梯度为零。二次项的梯度是：\n$$\n\\nabla_w \\left( \\frac{1}{2\\lambda} (u - w)^\\top(u - w) \\right) = \\frac{1}{2\\lambda} \\cdot 2(u-w) \\cdot (-1) = -\\frac{1}{\\lambda}(u - w) = \\frac{1}{\\lambda}(w - u)。\n$$\n应用 Danskin 定理，我们将最小化子 $u = \\operatorname{prox}_{\\lambda f}(w)$ 代入这个梯度表达式中：\n$$\n\\nabla M_{\\lambda} f(w) = \\nabla_w \\phi(\\operatorname{prox}_{\\lambda f}(w), w) = \\frac{1}{\\lambda} (w - \\operatorname{prox}_{\\lambda f}(w))。\n$$\n这就是莫罗包络梯度的所求表达式。\n\n### 2. 特化到 $f(w) = \\alpha \\|w\\|_1$\n\n现在我们考虑特定的非光滑凸函数 $f(w) = \\alpha \\|w\\|_1 = \\alpha \\sum_{i=1}^d |w_i|$，其中 $\\alpha \\ge 0$。\n为了找到近端算子 $\\operatorname{prox}_{\\lambda f}(w)$，我们必须解决以下最小化问题：\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\alpha \\sum_{i=1}^d |u_i| + \\frac{1}{2\\lambda} \\sum_{i=1}^d (u_i - w_i)^2 \\right\\}。\n$$\n这个目标函数是可分离的，意味着可以对每个坐标 $u_i$ 独立地进行最小化：\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2 \\right\\}。\n$$\n令 $g(u_i) = \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2$。根据凸最小化的最优性条件，最小化子 $u_i^\\star$ 必须满足 $0 \\in \\partial g(u_i^\\star)$。次微分是 $\\partial g(u_i) = \\alpha \\partial|u_i| + \\frac{1}{\\lambda}(u_i - w_i)$。\n绝对值函数的次微分是 $\\partial|x| = \\operatorname{sgn}(x)$ 如果 $x \\ne 0$，以及 $\\partial|x| = [-1, 1]$ 如果 $x = 0$。\n条件 $0 \\in \\partial g(u_i^\\star)$ 意味着 $w_i - u_i^\\star \\in \\lambda\\alpha \\partial|u_i^\\star|$。我们分析 $u_i^\\star$ 的三种情况：\n1. 如果 $u_i^\\star > 0$，那么 $\\partial|u_i^\\star| = \\{1\\}$。条件变为 $w_i - u_i^\\star = \\lambda\\alpha$，所以 $u_i^\\star = w_i - \\lambda\\alpha$。这仅在 $w_i - \\lambda\\alpha > 0$，即 $w_i > \\lambda\\alpha$ 时成立。\n2. 如果 $u_i^\\star < 0$，那么 $\\partial|u_i^\\star| = \\{-1\\}$。条件变为 $w_i - u_i^\\star = -\\lambda\\alpha$，所以 $u_i^\\star = w_i + \\lambda\\alpha$。这仅在 $w_i + \\lambda\\alpha < 0$，即 $w_i < -\\lambda\\alpha$ 时成立。\n3. 如果 $u_i^\\star = 0$，那么 $\\partial|u_i^\\star| = [-1, 1]$。条件变为 $w_i \\in \\lambda\\alpha[-1, 1]$，这等价于 $|w_i| \\le \\lambda\\alpha$。\n\n结合这些情况，我们得到软阈值算子 $S_{\\lambda\\alpha}$：\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = S_{\\lambda\\alpha}(w_i) \\triangleq \\begin{cases} w_i - \\lambda\\alpha & \\text{if } w_i > \\lambda\\alpha \\\\ w_i + \\lambda\\alpha & \\text{if } w_i < -\\lambda\\alpha \\\\ 0 & \\text{if } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\n这可以紧凑地写为 $(\\operatorname{prox}_{\\lambda f}(w))_i = \\operatorname{sgn}(w_i) \\max(|w_i| - \\lambda\\alpha, 0)$。\n\n利用这个闭式近端算子，我们可以从第 1 部分写出莫罗包络梯度的显式公式：\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\frac{1}{\\lambda}(w_i - (\\operatorname{prox}_{\\lambda f}(w))_i) = \\frac{1}{\\lambda}(w_i - S_{\\lambda\\alpha}(w_i))。\n$$\n这得到了一个逐坐标的梯度公式：\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\begin{cases} \\alpha & \\text{if } w_i > \\lambda\\alpha \\\\ -\\alpha & \\text{if } w_i < -\\lambda\\alpha \\\\ w_i/\\lambda & \\text{if } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\n\n### 3. 梯度下降算法及步长选择的合理性\n\n最小化 $M_{\\lambda} f(w)$ 的梯度下降更新法则是 $w_{k+1} = w_k - \\eta \\nabla M_{\\lambda} f(w_k)$。我们需要使用步长 $\\eta = \\lambda$。\n为了证明这一选择的合理性，我们必须表明梯度 $\\nabla M_{\\lambda} f$ 是 Lipschitz 连续的。已知莫罗包络的梯度是 $1/\\lambda$-Lipschitz 连续的（这是 Baillon-Haddad 定理的一个结果）。对于一个凸函数 $f$，其莫罗包络 $M_\\lambda f$ 是连续可微的，其梯度是 Lipschitz 连续的，常数为 $L = 1/\\lambda$。\n对于具有 $L$-Lipschitz 连续梯度的函数的梯度下降，一个标准结果是对于任何固定的步长 $\\eta \\in (0, 2/L)$，收敛性是有保证的。在我们的情况下，$L=1/\\lambda$，因此收敛条件是 $\\eta \\in (0, 2\\lambda)$。选择 $\\eta = \\lambda$ 显然落在这个区间内，使其成为一个有效的选择。\n\n此外，这个特定的选择带来了一个显著的简化。将 $\\eta = \\lambda$ 和梯度表达式代入更新法则：\n$$\nw_{k+1} = w_k - \\lambda \\cdot \\nabla M_{\\lambda} f(w_k) = w_k - \\lambda \\cdot \\left(\\frac{1}{\\lambda}(w_k - \\operatorname{prox}_{\\lambda f}(w_k))\\right) = w_k - (w_k - \\operatorname{prox}_{\\lambda f}(w_k))。\n$$\n这可以简化为：\n$$\nw_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k)。\n$$\n因此，在莫罗包络 $M_{\\lambda} f(w)$ 上以步长 $\\eta=\\lambda$ 执行梯度下降，等价于对原始函数 $f(w)$ 应用近端点算法。这是平滑技术和近端方法之间一个强有力的联系。\n\n### 4. 实现策略\n\n基于以上分析，实现将包括以下部分：\n1.  一个函数 `prox_l1(w, alpha, lambda_val)`，实现软阈值算子 $S_{\\lambda\\alpha}(w)$。\n2.  一个函数 `moreau_envelope_l1(w, alpha, lambda_val)`，通过先找到 $u = \\operatorname{prox}_{\\lambda f}(w)$，然后计算 $f(u) + \\frac{1}{2\\lambda}\\|u-w\\|_2^2$ 来计算 $M_{\\lambda} f(w)$。\n3.  主循环将迭代 $T$ 步。在每一步中，它将使用简化的近端点更新法则更新权重向量 $w$：$w_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k)$。\n4.  在 $T$ 次迭代后，为最终的迭代向量 $w_T$ 计算最终的莫罗包络值 $M_{\\lambda} f(w_T)$ 并存储。对每个测试用例重复此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing gradient descent on the Moreau envelope\n    for the L1 norm, which simplifies to the proximal point algorithm.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # alpha, lambda, w0, T\n        (0.5, 0.2, np.array([3.0, -1.0, 0.05]), 100),\n        (0.5, 1.0, np.array([-2.0, 2.0, 0.0]), 50),\n        (0.0, 0.3, np.array([1.0, -1.0, 2.0, -2.0]), 30),\n        (1.0, 0.05, np.array([1.0, -0.5, 0.2, -3.0, 4.0]), 200),\n        (0.2, 2.0, np.array([10.0, -10.0]), 50),\n    ]\n\n    def prox_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the proximal operator for f(w) = alpha * ||w||_1.\n        This is the soft-thresholding operator.\n        \"\"\"\n        threshold = lambda_val * alpha\n        # Component-wise operation\n        return np.sign(w) * np.maximum(np.abs(w) - threshold, 0.0)\n\n    def moreau_envelope_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the value of the Moreau envelope for f(w) = alpha * ||w||_1.\n        M_lambda_f(w) = f(prox(w)) + (1/(2*lambda)) * ||prox(w) - w||^2\n        \"\"\"\n        u_prox = prox_l1(w, alpha, lambda_val)\n        \n        # f(u_prox) = alpha * ||u_prox||_1\n        f_u_prox = alpha * np.linalg.norm(u_prox, 1)\n        \n        # (1/(2*lambda)) * ||u_prox - w||_2^2\n        quadratic_term = (1.0 / (2.0 * lambda_val)) * np.linalg.norm(u_prox - w, 2)**2\n        \n        return f_u_prox + quadratic_term\n\n    results = []\n    for case in test_cases:\n        alpha, lambda_val, w0, T = case\n        \n        w = w0.copy() # Start with the initial vector\n        \n        # Gradient descent with step size eta = lambda simplifies to the proximal point algorithm\n        # w_{k+1} = prox(w_k)\n        for _ in range(T):\n            w = prox_l1(w, alpha, lambda_val)\n        \n        # After T iterations, we have w_T. Compute the final Moreau envelope value.\n        final_value = moreau_envelope_l1(w, alpha, lambda_val)\n        results.append(final_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为了建立强大的直觉，亲眼见证抽象概念在具体实例中的作用是无价的。本练习将超越一般公式，对莫罗包络的平滑效应进行解析性探索。通过为一个简单的分段仿射函数显式计算其包络，你将亲眼目睹这一数学工具如何优雅地“磨圆”原始函数中尖锐的、不可微的“拐点”。",
            "id": "3167906",
            "problem": "考虑由下式定义的凸分段仿射函数 $f:\\mathbb{R}\\to\\mathbb{R}$\n$$\nf(y)=|y|+2\\max\\{0,y-1\\}=\n\\begin{cases}\n-y, & y\\leq 0,\\\\\ny, & 0\\leq y\\leq 1,\\\\\n3y-2, & y\\geq 1.\n\\end{cases}\n$$\n设 $\\lambda>0$ 为一个固定的常数。莫罗包络 $e_{\\lambda}f:\\mathbb{R}\\to\\mathbb{R}$ 对每个 $x\\in\\mathbb{R}$ 定义为\n$$\ne_{\\lambda}f(x)=\\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}。\n$$\n从莫罗包络的定义和 $f$ 的分段仿射结构出发，在由区域 $y\\leq 0$、$0\\leq y\\leq 1$ 和 $y\\geq 1$ 中的无约束驻点以及扭结点 $y=0$ 和 $y=1$ 所导出的每个 $x$ 的区间上，解析地推导出 $e_{\\lambda}f(x)$。证明其下确界是唯一达到的，并通过在每个区域内使用一阶最优性条件来确定最小化点，必要时在 $y=0$ 和 $y=1$ 处进行边界检查。然后，在每个得到的区间上，用 $x$ 和 $\\lambda$ 显式地计算 $e_{\\lambda}f(x)$，并解释 $f$ 在 $y=0$ 和 $y=1$ 处的不可微扭结点在 $e_{\\lambda}f$ 中是如何被平滑的。\n\n使用标准数学符号，将你的最终答案表示为 $e_{\\lambda}f(x)$ 的一个分段书写的、闭合形式的解析表达式。无需四舍五入，也不涉及物理单位。",
            "solution": "该问题是凸分析中的一个有效练习，具体来说是计算给定凸分段仿射函数的莫罗包络。所有术语都有明确定义，且前提在数学上是合理的。\n\n参数为 $\\lambda > 0$ 的函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 的莫罗包络定义为：\n$$\ne_{\\lambda}f(x) = \\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}\n$$\n令 $g(y; x) = f(y)+\\frac{1}{2\\lambda}(y-x)^{2}$。函数 $f(y)$ 是凸的。对于固定的 $x$ 和 $\\lambda > 0$，项 $\\frac{1}{2\\lambda}(y-x)^{2}$ 是关于 $y$ 的强凸函数。一个凸函数与一个强凸函数之和是强凸的。因此，对于任意给定的 $x \\in \\mathbb{R}$，$g(y;x)$ 是关于 $y$ 的强凸函数，这保证了其下确界在唯一的点 $y^*$ 处达到。这个唯一的最小化点是 $\\lambda f$ 在 $x$ 处的邻近算子，记作 $y^* = \\text{prox}_{\\lambda f}(x)$。\n\n对于凸函数，一阶最优性条件表明，最小化点 $y^*$ 是目标函数的次梯度包含零的点。$g(y;x)$ 关于 $y$ 的次梯度为：\n$$\n\\partial_y g(y;x) = \\partial f(y) + \\frac{1}{\\lambda}(y-x)\n$$\n最优性条件是 $0 \\in \\partial g(y^*;x)$，可以重写为：\n$$\nx - y^* \\in \\lambda \\partial f(y^*)\n$$\n我们必须首先计算给定函数的次微分 $\\partial f(y)$：\n$$\nf(y)=\n\\begin{cases}\n-y, & y\\leq 0,\\\\\ny, & 0\\leq y\\leq 1,\\\\\n3y-2, & y\\geq 1.\n\\end{cases}\n$$\n次微分是：\n- 对于 $y < 0$，$f'(y)=-1$，所以 $\\partial f(y) = \\{-1\\}$。\n- 对于 $0 < y < 1$，$f'(y)=1$，所以 $\\partial f(y) = \\{1\\}$。\n- 对于 $y > 1$，$f'(y)=3$，所以 $\\partial f(y) = \\{3\\}$。\n- 在扭结点 $y=0$ 处，次微分是左右导数之间的区间，即 $\\partial f(0) = [-1, 1]$。\n- 在扭结点 $y=1$ 处，次微分是 $\\partial f(1) = [1, 3]$。\n\n现在我们通过考虑 $y^*$ 位置的五个穷尽且互斥的情况来确定最小化点 $y^*$ 和对应的 $e_{\\lambda}f(x)$ 的值，这五个情况划分了 $x$ 的定义域。\n\n**情况 1：最小化点在区域 $y^* < 0$ 内。**\n在此情况下，$\\partial f(y^*) = \\{-1\\}$。最优性条件变为 $x - y^* = \\lambda(-1) = -\\lambda$，这意味着 $y^* = x + \\lambda$。这种情况成立当且仅当 $y^* < 0$，即 $x + \\lambda < 0$，或 $x < -\\lambda$。\n对于 $x < -\\lambda$，最小化点是 $y^*=x+\\lambda$。莫罗包络为：\n$$\ne_{\\lambda}f(x) = f(x+\\lambda) + \\frac{1}{2\\lambda}((x+\\lambda)-x)^2 = -(x+\\lambda) + \\frac{(-\\lambda)^2}{2\\lambda} = -x - \\lambda + \\frac{\\lambda}{2} = -x - \\frac{\\lambda}{2}\n$$\n\n**情况 2：最小化点在扭结点 $y^* = 0$ 处。**\n在此情况下，$\\partial f(0) = [-1, 1]$。最优性条件变为 $x - 0 \\in \\lambda[-1, 1]$，简化为 $x \\in [-\\lambda, \\lambda]$。\n对于 $x \\in [-\\lambda, \\lambda]$，最小化点是 $y^*=0$。莫罗包络为：\n$$\ne_{\\lambda}f(x) = f(0) + \\frac{1}{2\\lambda}(0-x)^2 = 0 + \\frac{x^2}{2\\lambda} = \\frac{x^2}{2\\lambda}\n$$\n\n**情况 3：最小化点在区域 $0 < y^* < 1$ 内。**\n在此情况下，$\\partial f(y^*) = \\{1\\}$。最优性条件为 $x - y^* = \\lambda(1) = \\lambda$，这意味着 $y^* = x - \\lambda$。这种情况成立当且仅当 $0 < y^* < 1$，即 $0 < x - \\lambda < 1$，或 $\\lambda < x < 1+\\lambda$。\n对于 $\\lambda < x < 1+\\lambda$，最小化点是 $y^*=x-\\lambda$。莫罗包络为：\n$$\ne_{\\lambda}f(x) = f(x-\\lambda) + \\frac{1}{2\\lambda}((x-\\lambda)-x)^2 = (x-\\lambda) + \\frac{\\lambda^2}{2\\lambda} = x - \\lambda + \\frac{\\lambda}{2} = x - \\frac{\\lambda}{2}\n$$\n\n**情况 4：最小化点在扭结点 $y^* = 1$ 处。**\n在此情况下，$\\partial f(1) = [1, 3]$。最优性条件为 $x - 1 \\in \\lambda[1, 3]$，这意味着 $\\lambda \\leq x - 1 \\leq 3\\lambda$，或 $1+\\lambda \\leq x \\leq 1+3\\lambda$。\n对于 $x \\in [1+\\lambda, 1+3\\lambda]$，最小化点是 $y^*=1$。莫罗包络为：\n$$\ne_{\\lambda}f(x) = f(1) + \\frac{1}{2\\lambda}(1-x)^2 = (3(1)-2) + \\frac{(x-1)^2}{2\\lambda} = 1 + \\frac{(x-1)^2}{2\\lambda}\n$$\n\n**情况 5：最小化点在区域 $y^* > 1$ 内。**\n在此情况下，$\\partial f(y^*) = \\{3\\}$。最优性条件为 $x - y^* = \\lambda(3) = 3\\lambda$，这意味着 $y^* = x - 3\\lambda$。这种情况成立当且仅当 $y^* > 1$，即 $x - 3\\lambda > 1$，或 $x > 1+3\\lambda$。\n对于 $x > 1+3\\lambda$，最小化点是 $y^*=x-3\\lambda$。莫罗包络为：\n$$\ne_{\\lambda}f(x) = f(x-3\\lambda) + \\frac{1}{2\\lambda}((x-3\\lambda)-x)^2 = (3(x-3\\lambda)-2) + \\frac{(-3\\lambda)^2}{2\\lambda} = 3x - 9\\lambda - 2 + \\frac{9\\lambda}{2} = 3x - 2 - \\frac{9\\lambda}{2}\n$$\n\n综合这五种情况，我们得到莫罗包络 $e_{\\lambda}f(x)$ 的分段解析表达式：\n$$\ne_{\\lambda}f(x) =\n\\begin{cases}\n-x - \\frac{\\lambda}{2}, & x < -\\lambda \\\\\n\\frac{x^2}{2\\lambda}, & -\\lambda \\leq x \\leq \\lambda \\\\\nx - \\frac{\\lambda}{2}, & \\lambda < x < 1+\\lambda \\\\\n1 + \\frac{(x-1)^2}{2\\lambda}, & 1+\\lambda \\leq x \\leq 1+3\\lambda \\\\\n3x - 2 - \\frac{9\\lambda}{2}, & x > 1+3\\lambda\n\\end{cases}\n$$\n函数 $e_{\\lambda}f(x)$ 是连续可微的 ($C^1$)。其导数由通用公式 $e'_{\\lambda}f(x) = \\frac{1}{\\lambda}(x - \\text{prox}_{\\lambda f}(x))$ 给出。对每一段计算该导数：\n- 对于 $x < -\\lambda$，$e'_{\\lambda}f(x) = -1$。\n- 对于 $-\\lambda < x < \\lambda$，$e'_{\\lambda}f(x) = \\frac{x}{\\lambda}$。\n- 对于 $\\lambda < x < 1+\\lambda$，$e'_{\\lambda}f(x) = 1$。\n- 对于 $1+\\lambda < x < 1+3\\lambda$，$e'_{\\lambda f}(x) = \\frac{x-1}{\\lambda}$。\n- 对于 $x > 1+3\\lambda$，$e'_{\\lambda}f(x) = 3$。\n可以很容易地验证，该导数在所有边界点 $x=-\\lambda, \\lambda, 1+\\lambda, 1+3\\lambda$ 处是连续的。这个 $C^1$ 性质展示了莫罗包络的平滑效应。$f(y)$ 在 $y=0$ 和 $y=1$ 处的不可微扭结点被平滑了。$f(y)$ 的导数在 $y=0$ 处从 $-1$ 到 $1$ 的跳跃，被 $e'_{\\lambda}f(x)$ 在区间 $x \\in [-\\lambda, \\lambda]$ 上从 $-1$ 到 $1$ 的平滑线性过渡所取代。类似地，$f(y)$ 的导数在 $y=1$ 处从 $1$ 到 $3$ 的跳跃，被 $e'_{\\lambda}f(x)$ 在区间 $x \\in [1+\\lambda, 1+3\\lambda]$ 上从 $1$ 到 $3$ 的平滑线性过渡所取代。$e_{\\lambda}f(x)$ 中的二次项部分起到了“磨圆”原始函数 $f(y)$ 尖角的作用。",
            "answer": "$$\n\\boxed{\ne_{\\lambda}f(x) =\n\\begin{cases}\n-x - \\frac{\\lambda}{2}, & x \\le -\\lambda \\\\\n\\frac{x^2}{2\\lambda}, & -\\lambda \\leq x \\leq \\lambda \\\\\nx - \\frac{\\lambda}{2}, & \\lambda \\le x \\le 1+\\lambda \\\\\n1 + \\frac{(x-1)^2}{2\\lambda}, & 1+\\lambda \\leq x \\leq 1+3\\lambda \\\\\n3x - 2 - \\frac{9\\lambda}{2}, & x \\ge 1+3\\lambda\n\\end{cases}\n}\n$$"
        },
        {
            "introduction": "莫罗包络的强大功能和通用性远不止于标准的欧几里得范数。这个高级练习将引入一个使用正定矩阵定义的度量的广义包络，这是大规模优化中预处理的一个关键概念。你将实现并测试一个深思熟虑的度量选择（根据问题结构进行设计），如何能够显著加速基于梯度的算法的收敛速度。",
            "id": "3489010",
            "problem": "您的任务是在压缩感知和稀疏优化的背景下分析广义莫罗包络，其中度量由一个对称正定矩阵定义。设 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 为一个正常闭凸函数。对于一个标量 $\\lambda>0$ 和一个对称正定矩阵 $M\\succ 0$，定义广义莫罗包络为\n$$\ne_{\\lambda,M} f(x) \\;\\triangleq\\; \\inf_{y\\in\\mathbb{R}^n}\\Big\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_M^2 \\Big\\},\n$$\n其中 $\\|u\\|_M \\triangleq \\sqrt{u^\\top M u}$。在本问题中，我们特化为 $f(x) = \\|x\\|_1$。\n\n仅从凸分析中的基本定义和最优性条件出发，完成以下任务：\n\n1. 推导广义莫罗包络 $e_{\\lambda,M}\\|x\\|_1$ 在点 $x\\in\\mathbb{R}^n$ 处的梯度，用达到下确界的唯一最小化子 $y^\\star(x)$ 来表示。您的推导必须从包络定义中内部最小化问题的一阶最优性条件开始，并且必须明确证明在 $M\\succ 0$ 条件下包络的可微性。\n\n2. 证明 $e_{\\lambda,M}\\|x\\|_1$ 的梯度是全局 Lipschitz 连续的，并用矩阵 $M$ 和参数 $\\lambda$ 给出一个有效的 Lipschitz 常数上界。您的论证必须依赖于邻近映射的非扩张性质和基本的谱范数界，而不能引用任何关于包络梯度 Lipschitz 常数的预先给出的公式。\n\n3. 展示如何使用一种基于保证收敛的显式步长的一阶分裂方法来计算邻近点\n$$\n\\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x) \\;\\triangleq\\; \\arg\\min_{y\\in\\mathbb{R}^n}\\Big\\{\\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2\\Big\\}\n$$\n。具体来说，利用一个光滑二次部分和一个非光滑 $\\ell_1$ 部分推导一个迭代格式，并解释如何利用 $M$ 的最大特征值和 $\\lambda$ 来选择步长。\n\n4. 对光滑函数 $x\\mapsto e_{\\lambda,M}\\|x\\|_1$ 实现梯度下降，使用第 1 项中推导的梯度，并选择第 2 项中建立的 Lipschitz 常数的倒数作为步长。在每次迭代中，使用第 3 项中的邻近计算来评估梯度。停止准则必须是梯度的欧几里得范数小于或等于一个容差 $t_{\\text{grad}}$，或者达到了预设的最大迭代次数。报告满足停止准则所需的迭代次数。\n\n5. 通过选择与传感矩阵 $A\\in\\mathbb{R}^{m\\times n}$ 相关的 $M$ (通过 $A^\\top A$) 来研究预处理的效果。构建以下测试套件，其中 $A$ 是一个实矩阵，其条目是使用指定种子生成的独立同分布高斯随机变量，并且 $M$ 的选择为：\n    - $M=\\mathrm{I}_n$ (单位矩阵),\n    - $M=\\mathrm{diag}(A^\\top A)$ (对角预处理器),\n    - $M=A^\\top A$ (完整预处理器)。\n\n对所有测试使用相同的初始点 $x_0\\in\\mathbb{R}^n$，其条目为独立同分布的高斯随机变量。\n\n对于下方的每个测试用例，从 $x_0$ 开始对 $e_{\\lambda,M}\\|x\\|_1$ 运行梯度下降，并输出达到 $\\|\\nabla e_{\\lambda,M}\\|x\\|_1(x_k)\\|_2 \\leq t_{\\text{grad}}$ 所需的迭代次数，如果未达到容差，则输出最大迭代次数：\n\n- 测试用例 1：$n=50$, $m=80$, $A$ 使用种子 $42$ 生成, $\\lambda=0.1$, $M=\\mathrm{I}_n$, $t_{\\text{grad}}=10^{-3}$, 最大迭代次数 $500$。\n- 测试用例 2：$n=50$, $m=80$, $A$ 使用种子 $42$ 生成, $\\lambda=0.1$, $M=\\mathrm{diag}(A^\\top A)$, $t_{\\text{grad}}=10^{-3}$, 最大迭代次数 $500$。\n- 测试用例 3：$n=50$, $m=80$, $A$ 使用种子 $42$ 生成, $\\lambda=0.1$, $M=A^\\top A$, $t_{\\text{grad}}=10^{-3}$, 最大迭代次数 $500$。\n- 测试用例 4：$n=50$, $m=80$, $A$ 使用种子 $42$ 生成, $\\lambda=0.01$, $M=\\mathrm{I}_n$, $t_{\\text{grad}}=10^{-3}$, 最大迭代次数 $800$。\n- 测试用例 5：$n=50$, $m=80$, $A$ 使用种子 $7$ 生成, $\\lambda=0.5$, $M=\\mathrm{diag}(A^\\top A)$, $t_{\\text{grad}}=10^{-3}$, 最大迭代次数 $300$。\n\n所有引用的随机变量必须是具有指定种子和维度的标准正态分布的实现，并且矩阵 $A$ 的条目必须服从 $\\mathcal{N}(0,1)$ 分布。初始点 $x_0$ 必须使用种子 123 生成，且独立于 $A$。\n\n您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔的整数列表，顺序与测试用例相同（例如，$[\\text{结果1},\\text{结果2},\\text{结果3},\\text{结果4},\\text{结果5}]$）。如果未达到容差，则输出该用例的最大迭代次数。本问题不涉及任何物理单位、角度或百分比。",
            "solution": "所给问题是凸优化领域中一个关于广义莫罗包络的良定且有科学依据的练习。我们将按要求进行系统的推导和分析。\n\n设 $f(x) = \\|x\\|_1$，这是一个正常闭凸函数。对于标量 $\\lambda>0$ 和对称正定矩阵 $M\\succ 0$，广义莫罗包络定义为\n$$\ne_{\\lambda,M} f(x) \\triangleq \\inf_{y\\in\\mathbb{R}^n}\\Big\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_M^2 \\Big\\},\n$$\n其中 $\\|u\\|_M = \\sqrt{u^\\top M u}$。\n\n### 1. 梯度的推导\n\n令 $J(x, y) = \\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2$。函数 $y \\mapsto J(x,y)$ 是一个凸函数（$\\|y\\|_1$）和一个严格凸函数（$\\frac{1}{2\\lambda}\\|y-x\\|_M^2$，因为 $M \\succ 0$ 且 $\\lambda > 0$）的和。因此，$J(x,y)$ 在 $y$ 上是严格凸的。此外，当 $\\|y\\|_2 \\to \\infty$ 时，$J(x,y) \\to \\infty$，意味着该函数是强制的。$\\mathbb{R}^n$ 上的严格凸且强制的函数有唯一的最小化子。记这个唯一的最小化子为 $y^\\star(x)$：\n$$\ny^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x) \\triangleq \\arg\\min_{y\\in\\mathbb{R}^n} J(x,y).\n$$\n莫罗包络可以写为 $e_{\\lambda,M}\\|x\\|_1 = J(x, y^\\star(x))$。\n关于 $y$ 的最小化问题的一阶充要最优性条件是，零向量必须在 $J(x,y)$ 于 $y=y^\\star(x)$ 处的次梯度中：\n$$\n0 \\in \\partial_y J(x, y)\\Big|_{y=y^\\star(x)}.\n$$\n$\\|y\\|_1$ 的次梯度是 $\\partial\\|y\\|_1$。光滑二次项的梯度是 $\\nabla_y \\left(\\frac{1}{2\\lambda}(y-x)^\\top M (y-x)\\right) = \\frac{1}{\\lambda}M(y-x)$。\n因此，最优性条件是：\n$$\n0 \\in \\partial\\|y^\\star(x)\\|_1 + \\frac{1}{\\lambda}M(y^\\star(x)-x).\n$$\n这可以重写为：\n$$\n\\frac{1}{\\lambda} M(x - y^\\star(x)) \\in \\partial\\|y^\\star(x)\\|_1.\n$$\n函数 $(x,y) \\mapsto J(x,y)$ 在 $(x,y)$ 上是联合凸的（这不是必需的但很有用）。因为对于每个 $x$，最小化子 $y^\\star(x)$ 是唯一的，所以值函数 $e_{\\lambda,M}\\|x\\|_1$ 是连续可微的。这是 Danskin 定理或包络定理的一个推论。包络的梯度可以通过求 $J(x,y)$ 关于 $x$ 的偏导数并在 $y=y^\\star(x)$ 处求值得到：\n$$\n\\nabla e_{\\lambda,M}\\|x\\|_1(x) = \\nabla_x J(x, y)\\Big|_{y=y^\\star(x)}.\n$$\n我们计算 $J(x,y)$ 关于 $x$ 的梯度：\n$$\n\\nabla_x J(x, y) = \\nabla_x \\left( \\|y\\|_1 + \\frac{1}{2\\lambda}(y-x)^\\top M (y-x) \\right) = \\frac{1}{2\\lambda} \\nabla_x (x^\\top M x - 2x^\\top M y + y^\\top M y) = \\frac{1}{2\\lambda}(2Mx - 2My) = \\frac{1}{\\lambda}M(x-y).\n$$\n在 $y=y^\\star(x)$ 处求值，得到莫罗包络的梯度：\n$$\n\\nabla e_{\\lambda,M}\\|x\\|_1(x) = \\frac{1}{\\lambda}M(x-y^\\star(x)).\n$$\n\n### 2. 梯度的 Lipschitz 连续性\n\n令 $p(x) = y^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x)$。梯度为 $\\nabla e(x) = \\frac{1}{\\lambda}M(x-p(x))$。我们想找到这个映射的 Lipschitz 常数。\n从一阶最优性条件，对于任意 $x_1, x_2 \\in \\mathbb{R}^n$，我们有：\n$$\n\\frac{1}{\\lambda} M(x_1 - p(x_1)) \\in \\partial\\|p(x_1)\\|_1\n$$\n$$\n\\frac{1}{\\lambda} M(x_2 - p(x_2)) \\in \\partial\\|p(x_2)\\|_1\n$$\n根据凸函数 $\\|\\cdot\\|_1$ 次梯度的单调性，我们有：\n$$\n\\left(\\frac{1}{\\lambda} M(x_1 - p(x_1)) - \\frac{1}{\\lambda} M(x_2 - p(x_2))\\right)^\\top (p(x_1) - p(x_2)) \\ge 0.\n$$\n令 $z = x_1 - x_2$ 且 $w = p(x_1) - p(x_2)$。整理不等式得到：\n$$\n(M(z-w))^\\top w \\ge 0 \\implies (z-w)^\\top M w \\ge 0 \\implies z^\\top Mw \\ge w^\\top Mw = \\|w\\|_M^2.\n$$\n这个性质表明算子 $p(x)$ 在 M-范数下是稳固非扩张的，如下所示：\n$$\n\\|z-w\\|_M^2 = (z-w)^\\top M (z-w) = \\|z\\|_M^2 - 2 z^\\top M w + \\|w\\|_M^2 \\le \\|z\\|_M^2 - 2\\|w\\|_M^2 + \\|w\\|_M^2 = \\|z\\|_M^2 - \\|w\\|_M^2.\n$$\n这意味着 $\\|p(x_1)-p(x_2)\\|_M^2 \\le \\|x_1-x_2\\|_M^2$，所以 $p(x)$ 在 M-范数下是非扩张的。\n令 $T(x) = x-p(x)$。我们有 $\\|T(x_1)-T(x_2)\\|_M^2 = \\|z-w\\|_M^2 \\le \\|z\\|_M^2 - \\|w\\|_M^2 \\le \\|z\\|_M^2$。所以 $T(x)$ 在 M-范数下也是非扩张的。\n我们需要界定梯度差的欧几里得范数：\n$$\n\\|\\nabla e(x_1) - \\nabla e(x_2)\\|_2 = \\left\\| \\frac{1}{\\lambda}M(T(x_1)-T(x_2)) \\right\\|_2.\n$$\n令 $u = T(x_1)-T(x_2)$。因为 $M \\succ 0$，其对称平方根 $M^{1/2}$ 存在且可逆。\n$$\n\\|Mu\\|_2 = \\|M^{1/2}M^{1/2}u\\|_2 \\le \\|M^{1/2}\\|_2 \\|M^{1/2}u\\|_2.\n$$\n谱范数 $\\|M^{1/2}\\|_2$ 是 $\\lambda_{\\max}(M^{1/2}) = \\sqrt{\\lambda_{\\max}(M)}$。\n项 $\\|M^{1/2}u\\|_2$ 等于 $\\|u\\|_M$。\n所以，$\\|Mu\\|_2 \\le \\sqrt{\\lambda_{\\max}(M)} \\|u\\|_M$。\n由于 $T$ 在 M-范数下是非扩张的，我们有 $\\|u\\|_M = \\|T(x_1)-T(x_2)\\|_M \\le \\|x_1-x_2\\|_M$。\n进一步，$\\|x_1-x_2\\|_M = \\|M^{1/2}(x_1-x_2)\\|_2 \\le \\|M^{1/2}\\|_2 \\|x_1-x_2\\|_2 = \\sqrt{\\lambda_{\\max}(M)}\\|x_1-x_2\\|_2$。\n结合这些不等式：\n$$\n\\|Mu\\|_2 \\le \\sqrt{\\lambda_{\\max}(M)} \\|u\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\|x_1-x_2\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\left( \\sqrt{\\lambda_{\\max}(M)} \\|x_1-x_2\\|_2 \\right) = \\lambda_{\\max}(M) \\|x_1-x_2\\|_2.\n$$\n代入梯度差表达式中：\n$$\n\\|\\nabla e(x_1) - \\nabla e(x_2)\\|_2 \\le \\frac{\\lambda_{\\max}(M)}{\\lambda} \\|x_1-x_2\\|_2.\n$$\n因此，梯度 $\\nabla e_{\\lambda,M}\\|x\\|_1$ 是全局 Lipschitz 连续的，其 Lipschitz 常数 $L \\le \\frac{\\lambda_{\\max}(M)}{\\lambda}$。对于对称正定矩阵，$\\|M\\|_2 = \\lambda_{\\max}(M)$，所以我们可以写成 $L = \\frac{\\|M\\|_2}{\\lambda}$。\n\n### 3. 邻近点的计算\n\n邻近点 $y^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x)$ 是以下最小化问题的解：\n$$\n\\min_{y\\in\\mathbb{R}^n} F(y), \\quad \\text{其中 } F(y) = \\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2.\n$$\n这是一个复合凸优化问题。我们可以将目标函数分解为一个非光滑部分 $h(y) = \\|y\\|_1$ 和一个光滑凸部分 $g(y) = \\frac{1}{2\\lambda}\\|y-x\\|_M^2$。该问题可以使用一阶分裂方法如邻近梯度法（也称为 ISTA）高效求解。\n光滑部分的梯度是：\n$$\n\\nabla g(y) = \\frac{1}{\\lambda}M(y-x).\n$$\n$\\nabla g(y)$ 的 Lipschitz 常数是 $L_g = \\frac{1}{\\lambda}\\|M\\|_2 = \\frac{\\lambda_{\\max}(M)}{\\lambda}$。\n邻近梯度迭代由 $y_{k+1} = \\operatorname{prox}_{\\tau h}(y_k - \\tau \\nabla g(y_k))$ 给出，其中 $\\tau$ 是步长。为保证收敛，必须有 $0 < \\tau < 2/L_g$。保证收敛的一个标准选择是 $\\tau = 1/L_g = \\lambda/\\lambda_{\\max}(M)$。\n$h(y)=\\|y\\|_1$ 的邻近算子是软阈值算子，$\\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(z)_i = \\operatorname{sgn}(z_i)\\max(|z_i|-\\tau, 0)$，我们记作 $S_\\tau(z)$。\n将所有部分代入，计算 $y^\\star(x)$ 的迭代格式为：\n$$\ny_{k+1} = S_{\\tau}\\left(y_k - \\tau \\left(\\frac{1}{\\lambda}M(y_k-x)\\right)\\right)\n$$\n当 $\\tau = \\lambda/\\lambda_{\\max}(M)$ 时，这变为：\n$$\ny_{k+1} = S_{\\lambda/\\lambda_{\\max}(M)}\\left(y_k - \\frac{\\lambda}{\\lambda_{\\max}(M)}\\frac{1}{\\lambda}M(y_k-x)\\right) = S_{\\lambda/\\lambda_{\\max}(M)}\\left(y_k - \\frac{1}{\\lambda_{\\max}(M)}M(y_k-x)\\right).\n$$\n从一个初始点（例如 $y_0=x$）开始，这个迭代会收敛到唯一的最小化子 $y^\\star(x)$。\n\n### 4. 梯度下降算法\n\n为了最小化关于 $x$ 的 $e_{\\lambda,M}\\|x\\|_1$，我们采用梯度下降法。更新规则是：\n$$\nx_{k+1} = x_k - \\eta \\nabla e_{\\lambda,M}\\|x\\|_1(x_k),\n$$\n其中 $\\eta$ 是步长。\n根据第 1 部分，梯度是 $\\nabla e_{\\lambda,M}\\|x\\|_1(x_k) = \\frac{1}{\\lambda}M(x_k-y^\\star(x_k))$。\n根据第 2 部分，梯度是 Lipschitz 连续的，其常数为 $L = \\frac{\\lambda_{\\max}(M)}{\\lambda}$。对于固定步长的梯度下降，如果 $\\eta < 2/L$，则收敛得到保证。问题指定使用步长 $\\eta = 1/L$。\n因此，步长为 $\\eta = \\frac{\\lambda}{\\lambda_{\\max}(M)}$。\n完整的算法如下：\n1. 初始化 $x_0$。\n2. 对于 $k=0, 1, \\dots, \\text{max\\_iterations}-1$：\n   a. 使用第 3 部分的迭代格式计算 $y^\\star(x_k)$。\n   b. 计算梯度 $g_k = \\frac{1}{\\lambda}M(x_k-y^\\star(x_k))$。\n   c. 如果 $\\|g_k\\|_2 \\le t_{\\text{grad}}$，则终止。\n   d. 更新 $x_{k+1} = x_k - \\left(\\frac{\\lambda}{\\lambda_{\\max}(M)}\\right) g_k$。\n如果在最大迭代次数内未满足停止准则，则过程终止并报告最大次数。\n\n实现将遵循此逻辑来处理指定的测试用例。注意，当 $M=\\mathrm{I}_n$（单位矩阵）时，$\\lambda_{\\max}(M)=1$。邻近算子简化为标准的软阈值算子：$y^\\star(x) = S_\\lambda(x)$。梯度变为 $\\nabla e_{\\lambda, \\mathrm{I}_n}\\|x\\|_1(x) = \\frac{1}{\\lambda}(x - S_\\lambda(x))$，步长为 $\\eta = \\lambda$。更新简化为 $x_{k+1} = x_k - \\lambda \\frac{1}{\\lambda}(x_k-S_\\lambda(x_k)) = S_\\lambda(x_k)$。",
            "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Component-wise soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef compute_prox_M(x, M, lamb, lambda_max_M, max_prox_iter=500, prox_tol=1e-8):\n    \"\"\"\n    Computes prox_{lambda*||.||_1}^M(x).\n    Solves min_y { ||y||_1 + 1/(2*lambda) * ||y-x||_M^2 } using ISTA.\n    \"\"\"\n    n = x.shape[0]\n\n    # The problem is min_y { h(y) + g(y) } where h(y)=||y||_1 and g(y)=1/(2*lambda)||y-x||_M^2.\n    # The gradient of the smooth part g(y) is grad_g(y) = (1/lambda) * M @ (y - x).\n    # The Lipschitz constant of grad_g is L_g = lambda_max(M) / lambda.\n    # ISTA step-size is chosen as tau = 1/L_g = lambda / lambda_max(M).\n    # The ISTA update is y_{k+1} = prox_{tau*h}(y_k - tau * grad_g(y_k)).\n    # prox_{tau*h}(z) = S_tau(z).\n    # y_{k+1} = S_tau(y_k - tau * (1/lambda) * M @ (y_k - x))\n    # y_{k+1} = S_{lambda/lambda_max_M}(y_k - (lambda/lambda_max_M) * (1/lambda) * M @ (y_k - x))\n    # y_{k+1} = S_{lambda/lambda_max_M}(y_k - (1/lambda_max_M) * M @ (y_k - x))\n\n    y = np.copy(x)  # Initialization\n    tau = lamb / lambda_max_M # Threshold for soft-thresholding\n    inv_lambda_max_M = 1.0 / lambda_max_M\n\n    for _ in range(max_prox_iter):\n        y_prev = y\n        y = soft_threshold(y - inv_lambda_max_M * (M @ (y - x)), tau)\n        if np.linalg.norm(y - y_prev) < prox_tol:\n            break\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for gradient descent on the Moreau envelope.\n    \"\"\"\n    test_cases = [\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'I', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'diag', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'full', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.01, 'M_type': 'I', 't_grad': 1e-3, 'max_iter': 800},\n        {'n': 50, 'm': 80, 'seed_A': 7, 'lamb': 0.5, 'M_type': 'diag', 't_grad': 1e-3, 'max_iter': 300},\n    ]\n\n    results = []\n    \n    # Generate the common initial point x0\n    rng_x0 = np.random.default_rng(123)\n    x0 = rng_x0.standard_normal(test_cases[0]['n'])\n\n    for case in test_cases:\n        n, m, seed_A, lamb, M_type, t_grad, max_iter = \\\n            case['n'], case['m'], case['seed_A'], case['lamb'], case['M_type'], case['t_grad'], case['max_iter']\n\n        # Generate sensing matrix A\n        rng_A = np.random.default_rng(seed_A)\n        A = rng_A.standard_normal((m, n))\n\n        # Construct preconditioning matrix M\n        if M_type == 'I':\n            M = np.eye(n)\n        elif M_type == 'diag':\n            M = np.diag(np.diag(A.T @ A))\n        elif M_type == 'full':\n            M = A.T @ A\n        \n        # Calculate max eigenvalue of M\n        lambda_max_M = np.linalg.norm(M, 2)\n        \n        # Gradient descent step-size\n        eta = lamb / lambda_max_M\n\n        x_k = np.copy(x0)\n        \n        # Gradient descent loop\n        iter_count = max_iter\n        for k in range(1, max_iter + 1):\n            if M_type == 'I':\n                # Closed-form for prox with M=I\n                y_star = soft_threshold(x_k, lamb)\n            else:\n                # Use ISTA for general M\n                y_star = compute_prox_M(x_k, M, lamb, lambda_max_M)\n            \n            # Compute gradient of the Moreau envelope\n            grad = (1.0 / lamb) * (M @ (x_k - y_star))\n            \n            # Check stopping criterion\n            grad_norm = np.linalg.norm(grad)\n            if grad_norm <= t_grad:\n                iter_count = k\n                break\n            \n            # Update x\n            x_k = x_k - eta * grad\n        \n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}