{
    "hands_on_practices": [
        {
            "introduction": "交替方向乘子法（ADMM）的核心优势在于其能够将复杂问题分解为一系列更易于处理的子问题。在图像处理等领域，一个子问题常常涉及到一个非光滑的正则项，例如在图像去噪和复原中至关重要的全变分（Total Variation, TV）正则项。本练习将聚焦于 ADMM 中的 $z$-更新步骤，该步骤在形式上等价于求解一个邻近算子。通过从第一性原理推导并计算各向同性 TV 的邻近算子——即分块向量软阈值算子——您将深刻理解 ADMM 是如何通过迭代施加复杂的结构性先验（例如，促使图像形成分片常数区域）的。",
            "id": "3430656",
            "problem": "考虑压缩感知中的各向同性总变分 (TV) 正则化最小二乘问题，\n$$\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|D x\\|_{2,1},$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个传感矩阵，$b \\in \\mathbb{R}^{m}$ 是测量向量，$D \\in \\mathbb{R}^{2n \\times n}$ 是离散梯度算子，它将每个像素映射到一个二维梯度（水平和垂直差分），$\\|\\cdot\\|_{2,1}$ 表示混合范数，定义为 $\\|z\\|_{2,1} = \\sum_{i=1}^{n} \\|z_{i}\\|_{2}$，其中 $z_{i} \\in \\mathbb{R}^{2}$ 是像素 $i$ 处的梯度块。\n\n使用交替方向乘子法 (ADMM)，引入分裂变量 $z = D x$ 和缩放对偶变量 $u$。$z$-更新步骤求解关于混合范数 $\\|\\cdot\\|_{2,1}$ 的近端子问题：\n$$z^{k+1} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(v),$$\n其中惩罚参数 $\\rho$ 为适当选择的正数，标量 $\\tau = \\lambda / \\rho$，且 $v = D x^{k+1} + u^{k}$。\n\n从正常、闭、凸函数 $g$ 的近端算子定义出发，\n$$\\operatorname{prox}_{\\tau g}(v) = \\arg\\min_{z} \\left\\{ \\tau g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right\\},$$\n请从第一性原理推导 $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(v)$ 对于一个被划分为 $n$ 个与各向同性梯度相关的二维块 $v_{i} \\in \\mathbb{R}^{2}$ 的向量 $v$ 的显式分块映射。然后，考虑一个具体实例，其中有 $n = 3$ 个像素，正则化参数 $\\lambda = 1$，惩罚参数 $\\rho = 1$，因此 $\\tau = 1$，块向量为\n$$v = \\big( v_{1}, v_{2}, v_{3} \\big), \\quad v_{1} = (3, 0), \\quad v_{2} = (\\sqrt{3}, 1), \\quad v_{3} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right)。$$\n通过将推导出的分块近端映射应用于每个块 $v_{i}$，计算 $z$-更新 $z^{k+1} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(v)$，并将连接后的结果报告为一个包含六个分量 $(z_{1}^{(1)}, z_{1}^{(2)}, z_{2}^{(1)}, z_{2}^{(2)}, z_{3}^{(1)}, z_{3}^{(2)})$ 的单行向量。\n\n最后，从概念上解释这个 $z$-更新在交替方向乘子法 (ADMM) 中为强制实现各向同性 TV 正则化所起的作用。\n\n你最终报告的向量必须是精确的；不要四舍五入。",
            "solution": "我们从近端算子的定义开始。对于一个正常、闭、凸函数 $g$，其参数为 $\\tau > 0$ 的近端算子定义为\n$$\\operatorname{prox}_{\\tau g}(v) = \\arg\\min_{z} \\left\\{ \\tau g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right\\}。$$\n在我们的例子中，$g(z) = \\|z\\|_{2,1} = \\sum_{i=1}^{n} \\|z_{i}\\|_{2}$，其中 $z_{i} \\in \\mathbb{R}^{2}$ 是每个像素的二维梯度块。由于 $g$ 是关于块的求和，并且二次项在各个块之间是可分的，因此最小化问题可以分解为 $n$ 个独立的子问题，每个块一个：\n$$z_{i}^{\\star} = \\arg\\min_{z_{i} \\in \\mathbb{R}^{2}} \\left\\{ \\tau \\|z_{i}\\|_{2} + \\frac{1}{2} \\|z_{i} - v_{i}\\|_{2}^{2} \\right\\}。$$\n我们从第一性原理分析每个块的问题。目标函数仅通过 $z_{i}$ 的欧几里得范数及其到 $v_{i}$ 的欧几里得距离来依赖于 $z_{i}$。这种旋转对称性意味着最优解 $z_{i}^{\\star}$ 位于 $v_{i}$ 的方向上（或者是零向量）。当 $v_{i} \\neq 0$ 时，我们写作 $z_{i} = \\alpha v_{i}$，其中标量 $\\alpha \\geq 0$（符号为非负，因为范数的正倍数项会惩罚大小，并且二次项沿 $v_{i}$ 的方向最小化）。代入后得到关于 $\\alpha$ 的一维问题：\n$$\\min_{\\alpha \\geq 0} \\ \\tau \\|\\alpha v_{i}\\|_{2} + \\frac{1}{2} \\|\\alpha v_{i} - v_{i}\\|_{2}^{2}\n= \\min_{\\alpha \\geq 0} \\ \\tau \\alpha \\|v_{i}\\|_{2} + \\frac{1}{2} (\\alpha - 1)^{2} \\|v_{i}\\|_{2}^{2}。$$\n定义 $r_{i} = \\|v_{i}\\|_{2} > 0$。标量目标函数变为\n$$\\phi(\\alpha) = \\tau \\alpha r_{i} + \\frac{1}{2} (\\alpha - 1)^{2} r_{i}^{2}。$$\n对 $\\alpha$ 求导并令其等于零以求驻点，\n$$\\phi'(\\alpha) = \\tau r_{i} + (\\alpha - 1) r_{i}^{2} = 0 \\quad \\Rightarrow \\quad \\alpha^{\\star} = 1 - \\frac{\\tau}{r_{i}}。$$\n我们必须强制 $\\alpha^{\\star} \\geq 0$。如果 $r_{i} \\leq \\tau$，则 $1 - \\frac{\\tau}{r_{i}} \\leq 0$，最小值点在 $\\alpha^{\\star} = 0$ 处取得。如果 $r_{i} > \\tau$，则 $\\alpha^{\\star} > 0$ 且有效。因此，最优的块解是\n$$z_{i}^{\\star} =\n\\begin{cases}\n\\left(1 - \\frac{\\tau}{\\|v_{i}\\|_{2}}\\right) v_{i},  \\text{若 } \\|v_{i}\\|_{2} > \\tau, \\\\\n0,  \\text{若 } \\|v_{i}\\|_{2} \\leq \\tau.\n\\end{cases}$$\n这就是分块向量收缩映射，等价地表示为 $z_{i}^{\\star} = \\max\\!\\left(1 - \\frac{\\tau}{\\|v_{i}\\|_{2}}, 0\\right) v_{i}$，适用于每个块 $i$，并约定如果 $v_{i} = 0$ 则 $z_{i}^{\\star} = 0$。\n\n我们现在将此映射应用于给定的实例。参数为 $\\lambda = 1$，$\\rho = 1$，因此 $\\tau = \\lambda / \\rho = 1$。块为\n$$v_{1} = (3, 0), \\quad v_{2} = (\\sqrt{3}, 1), \\quad v_{3} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right)。$$\n计算范数：\n- 对于 $v_{1}$，$\\|v_{1}\\|_{2} = \\sqrt{3^{2} + 0^{2}} = 3$。由于 $3 > \\tau$，收缩因子为 $1 - \\frac{\\tau}{\\|v_{1}\\|_{2}} = 1 - \\frac{1}{3} = \\frac{2}{3}$，所以\n$$z_{1}^{\\star} = \\frac{2}{3} (3, 0) = (2, 0)。$$\n- 对于 $v_{2}$，$\\|v_{2}\\|_{2} = \\sqrt{(\\sqrt{3})^{2} + 1^{2}} = \\sqrt{3 + 1} = 2$。由于 $2 > \\tau$，收缩因子为 $1 - \\frac{1}{2} = \\frac{1}{2}$，所以\n$$z_{2}^{\\star} = \\frac{1}{2} (\\sqrt{3}, 1) = \\left(\\frac{\\sqrt{3}}{2}, \\frac{1}{2}\\right)。$$\n- 对于 $v_{3}$，$\\|v_{3}\\|_{2} = \\sqrt{\\left(\\frac{3}{5}\\right)^{2} + \\left(\\frac{4}{5}\\right)^{2}} = \\sqrt{\\frac{9}{25} + \\frac{16}{25}} = \\sqrt{\\frac{25}{25}} = 1$。由于 $1 \\leq \\tau$，收缩因子为 $0$，所以\n$$z_{3}^{\\star} = (0, 0)。$$\n将分量 $(z_{1}^{(1)}, z_{1}^{(2)}, z_{2}^{(1)}, z_{2}^{(2)}, z_{3}^{(1)}, z_{3}^{(2)})$ 连接起来，得到行向量\n$$(2, 0, \\frac{\\sqrt{3}}{2}, \\frac{1}{2}, 0, 0)。$$\n\n最后，我们解释此更新在交替方向乘子法 (ADMM) 中的作用。在针对各向同性 TV 问题的 ADMM 分裂中，$z$-更新通过求解关于 $z$ 的近端子问题来分离出非光滑的正则化项 $\\|D x\\|_{2,1}$，其输入为 $v = D x^{k+1} + u^{k}$，参数为 $\\tau = \\lambda / \\rho$。分块向量收缩映射通过减小每个像素梯度向量的模长来强制实现各向同性 TV 正则化：梯度模长较小（小于或等于 $\\tau$）的块被精确地设置为零，从而促进分段常数结构；而较大的梯度则在不改变其方向的情况下向零收缩，保留了边缘方向。这个近端步骤由于在像素间的可分性而计算高效，并提供了 ADMM 将 TV 正则化融入迭代更新的机制，从而在图像梯度的数据保真度和稀疏性之间取得平衡。",
            "answer": "$$\\boxed{\\begin{pmatrix} 2  0  \\frac{\\sqrt{3}}{2}  \\frac{1}{2}  0  0 \\end{pmatrix}}$$"
        },
        {
            "introduction": "ADMM 的收敛速度对增广拉格朗日惩罚参数 $\\rho$ 的选择极为敏感，一个固定的 $\\rho$ 值往往难以在所有问题上都表现出色。一种有效的策略是在迭代过程中动态调整 $\\rho$，旨在平衡原始残差（primal residual）与对偶残差（dual residual）的量级。这种启发式方法有助于算法在迭代的每一步都能同时朝向原始可行性和最优性稳步前进。本练习将引导您实现一种基于残差平衡的自适应 $\\rho$ 更新方案。通过将其与固定 $\\rho$ 值的求解器进行性能对比，您将体会到自适应参数调优对于提升 ADMM 效率的实际效果，尤其是在处理病态问题时。",
            "id": "3364422",
            "problem": "考虑将交替方向乘子法 (ADMM) 应用于二维凸、二次连续可微二次目标函数的一致性分裂问题。目标是设计一个残差平衡规则，该规则能自适应调整增广拉格朗日惩罚参数，以平衡原始残差和对偶残差的范数，并分析其在二维二次测试问题上对收敛性的影响。您将编写的程序必须是完全自包含的，并能计算多个测试用例收敛所需的迭代次数。\n\n从以下基础开始。设优化问题表述为最小化一个可分离凸函数之和，并带有一致性形式的等式约束：\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\n其中\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}, \\quad g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z},\n$$\n并且矩阵 $\\boldsymbol{Q}$ 和 $\\boldsymbol{R}$ 是对称正定的，向量 $\\boldsymbol{q}$ 和 $\\boldsymbol{r}$ 属于 $\\mathbb{R}^2$。对于惩罚参数 $\\rho > 0$ 和缩放对偶变量 $\\boldsymbol{u}^k \\in \\mathbb{R}^2$，定义缩放形式的 ADMM 迭代 $(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1})$。原始残差为\n$$\n\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1},\n$$\n对偶残差为\n$$\n\\boldsymbol{s}^{k+1} = \\rho\\left(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k}\\right).\n$$\n\n您的任务是：\n- 当 $f$ 和 $g$ 是所述的凸二次函数且约束为 $\\boldsymbol{x} = \\boldsymbol{z}$ 时，从第一性原理和子问题的一阶最优性条件出发，推导出 $\\boldsymbol{x}^{k+1}$ 和 $\\boldsymbol{z}^{k+1}$ 的显式闭式更新公式。\n- 实现具有两种模式的 ADMM：固定惩罚参数模式和自适应残差平衡模式。在自适应模式中，使用以下带有参数 $\\mu > 0$ 和 $\\kappa > 1$ 的残差平衡规则，在迭代过程中调整 $\\rho$：\n$$\n\\text{若 } \\|\\boldsymbol{r}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{s}^{k+1}\\|_2 \\text{ 则令 } \\rho \\leftarrow \\kappa \\rho \\text{ 且 } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}/\\kappa;\n$$\n$$\n\\text{否则若 } \\|\\boldsymbol{s}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{r}^{k+1}\\|_2 \\text{ 则令 } \\rho \\leftarrow \\rho/\\kappa \\text{ 且 } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}\\kappa;\n$$\n否则保持 $\\rho$ 不变。在 $\\rho$ 发生变化时，对 $\\boldsymbol{u}^{k+1}$ 的缩放必须保持未缩放的拉格朗日乘子 $\\boldsymbol{y}^{k+1} = \\rho \\boldsymbol{u}^{k+1}$ 不变。\n- 使用基于绝对和相对容差的停止准则。令 $n = 2$。定义\n$$\n\\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right),\n$$\n$$\n\\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\rho\\,\\boldsymbol{u}^{k+1}\\|_2,\n$$\n当 $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ 和 $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$ 同时满足时终止。仅在检查这些停止准则之后再应用任何惩罚参数自适应调整。\n\n使用以下固定输入和测试套件实现算法。所有向量均为 $\\mathbb{R}^2$ 中的列向量，所有矩阵均为 $2 \\times 2$。\n\n通用初始条件：\n- $\\boldsymbol{x}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\boldsymbol{z}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\boldsymbol{u}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 容差：$\\varepsilon_{\\mathrm{abs}} = 10^{-6}$，$\\varepsilon_{\\mathrm{rel}} = 10^{-6}$。\n- 最大迭代次数：$50000$。\n- 所有残差和阈值计算必须使用欧几里得范数。\n\n测试套件参数集：\n- 用例 1（良态，固定 $\\rho$）：\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 4  1 \\\\ 1  2 \\end{bmatrix}$，$\\boldsymbol{R} = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$，\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，$\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$，\n  - 固定 $\\rho = 1$，禁用自适应模式。\n- 用例 2（良态，自适应，初始 $\\rho$ 较小）：\n  - $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$ 与用例 1 相同，\n  - 初始 $\\rho = 10^{-4}$，启用自适应模式，参数为 $\\mu = 10$，$\\kappa = 2$。\n- 用例 3（良态，自适应，初始 $\\rho$ 较大）：\n  - $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$ 与用例 1 相同，\n  - 初始 $\\rho = 10^{2}$，启用自适应模式，参数为 $\\mu = 2$，$\\kappa = 2$。\n- 用例 4（病态，固定 $\\rho$）：\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 1000  0 \\\\ 0  1 \\end{bmatrix}$，$\\boldsymbol{R} = \\begin{bmatrix} 1  0 \\\\ 0  100 \\end{bmatrix}$，\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$，$\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$，\n  - 固定 $\\rho = 1$，禁用自适应模式。\n- 用例 5（病态，自适应，初始 $\\rho$ 非常小）：\n  - $\\boldsymbol{Q}$、$\\boldsymbol{R}$、$\\boldsymbol{q}$、$\\boldsymbol{r}$ 与用例 4 相同，\n  - 初始 $\\rho = 10^{-6}$，启用自适应模式，参数为 $\\mu = 3$，$\\kappa = 2$。\n\n对于每个用例，计算并返回满足停止准则所需的总迭代次数。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如 $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$，其中每个 $\\text{result}_i$ 是用例 $i$ 的整数迭代次数。",
            "solution": "用户提供的问题是有效的。这是一个基于交替方向乘子法 (ADMM) 的既定原则的适定数值优化任务。所有数据、参数和条件都已提供，使得问题自成体系，没有矛盾或歧义。\n\n该问题要求实现 ADMM 来解决一个一致性形式下的可分离凸二次优化问题：\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\n其中 $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}$ 且 $g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z}$。矩阵 $\\boldsymbol{Q}$ 和 $\\boldsymbol{R}$ 是对称正定的。\n\n解决方案首先推导 ADMM 算法的显式更新方程，然后描述算法实现，包括自适应惩罚参数方案。\n\n**ADMM 公式化与更新推导**\n\n对于此问题，使用缩放对偶变量 $\\boldsymbol{u}$ 的缩放形式增广拉格朗日量 $L_\\rho$ 为：\n$$\nL_\\rho(\\boldsymbol{x}, \\boldsymbol{z}, \\boldsymbol{u}) = f(\\boldsymbol{x}) + g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z} + \\boldsymbol{u}\\|_2^2 - \\frac{\\rho}{2} \\|\\boldsymbol{u}\\|_2^2\n$$\n对于给定的惩罚参数 $\\rho > 0$，ADMM 算法在每次迭代 $k$ 中包含三个顺序更新步骤：\n$$\n\\begin{align*}\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} L_\\rho(\\boldsymbol{x}, \\boldsymbol{z}^k, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} L_\\rho(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n\\end{align*}\n$$\n我们推导 $\\boldsymbol{x}$ 和 $\\boldsymbol{z}$ 子问题的闭式解。\n\n**1. $\\boldsymbol{x}$-更新推导**\n$\\boldsymbol{x}$ 的最小化子问题是：\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( f(\\boldsymbol{x}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\n代入 $f(\\boldsymbol{x})$ 的二次形式：\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x} + \\frac{\\rho}{2} \\|\\boldsymbol{x} - (\\boldsymbol{z}^k - \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\n这是一个无约束的凸二次最小化问题。通过将关于 $\\boldsymbol{x}$ 的梯度设为零来找到最小化点。梯度是：\n$$\n\\nabla_{\\boldsymbol{x}} (\\cdot) = \\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q} + \\rho(\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k)\n$$\n将梯度设为零并求解 $\\boldsymbol{x}$：\n$$\n\\boldsymbol{Q}\\boldsymbol{x}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n$$\n(\\boldsymbol{Q} + \\rho\\boldsymbol{I})\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n由于 $\\boldsymbol{Q}$ 是正定的且 $\\rho > 0$，矩阵 $(\\boldsymbol{Q} + \\rho\\boldsymbol{I})$ 是对称正定且因此是可逆的。显式更新公式是：\n$$\n\\boldsymbol{x}^{k+1} = (\\boldsymbol{Q} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q} \\right)\n$$\n\n**2. $\\boldsymbol{z}$-更新推导**\n$\\boldsymbol{z}$ 的最小化子问题使用新计算出的 $\\boldsymbol{x}^{k+1}$：\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\n代入 $g(\\boldsymbol{z})$ 的二次形式：\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z} + \\frac{\\rho}{2} \\|\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\n同样，我们将关于 $\\boldsymbol{z}$ 的梯度设为零。梯度是：\n$$\n\\nabla_{\\boldsymbol{z}} (\\cdot) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} + \\rho(\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} - \\rho(\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k)\n$$\n将梯度设为零并求解 $\\boldsymbol{z}$：\n$$\n\\boldsymbol{R}\\boldsymbol{z}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{R} + \\rho\\boldsymbol{I})\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n由于 $\\boldsymbol{R}$ 是正定的，$(\\boldsymbol{R} + \\rho\\boldsymbol{I})$ 是可逆的。显式更新公式是：\n$$\n\\boldsymbol{z}^{k+1} = (\\boldsymbol{R} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r} \\right)\n$$\n\n**3. $\\boldsymbol{u}$-更新**\n对偶变量的更新由下式给出：\n$$\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n$$\n\n**算法实现**\n该算法迭代这些更新直至收敛。迭代 $k$（以计算迭代 $k+1$ 的值）的过程如下：\n1.  初始化 $\\boldsymbol{x}^0, \\boldsymbol{z}^0, \\boldsymbol{u}^0$ 为 $\\boldsymbol{0}$，并将 $\\rho_0$ 设为其初始值。\n2.  对于 $k=0, 1, \\dots, \\text{max\\_iter}-1$：\n    a.  使用 $\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k$ 和推导出的公式计算 $\\boldsymbol{x}^{k+1}$。\n    b.  使用 $\\boldsymbol{x}^{k+1}, \\boldsymbol{u}^k, \\rho_k$ 和推导出的公式计算 $\\boldsymbol{z}^{k+1}$。\n    c.  计算缩放前的对偶更新：$\\boldsymbol{u}^{k+1}_{\\text{pre}} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$。\n    d.  计算残差：\n        -   原始残差：$\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$。\n        -   对偶残差：$\\boldsymbol{s}^{k+1} = \\rho_k(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k})$。\n    e.  基于绝对容差 $\\varepsilon_{\\mathrm{abs}}$ 和相对容差 $\\varepsilon_{\\mathrm{rel}}$ 计算停止阈值：\n        $$\n        \\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right)\n        $$\n        未缩放的拉格朗日乘子为 $\\boldsymbol{y}^{k+1} = \\rho_k \\boldsymbol{u}^{k+1}_{\\text{pre}}$。对偶阈值为：\n        $$\n        \\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\boldsymbol{y}^{k+1}\\|_2\n        $$\n    f. 检查收敛性：如果 $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ 且 $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$，则终止并返回 $k+1$。\n    g. 如果启用了自适应模式，则为下一次迭代更新 $\\rho$ 为 $\\rho_{k+1}$，并相应地缩放 $\\boldsymbol{u}^{k+1}$。令 $\\boldsymbol{u}^{k+1}_{\\text{post}}$ 为下一次迭代更新后的对偶变量。\n        - 默认设置 $\\rho_{k+1} = \\rho_k$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} = \\boldsymbol{u}^{k+1}_{\\text{pre}}$。\n        - 如果 $\\|\\boldsymbol{r}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{s}^{k+1}\\|_2$：\n            $\\rho_{k+1} \\leftarrow \\kappa \\rho_k$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} / \\kappa$。\n        - 否则如果 $\\|\\boldsymbol{s}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{r}^{k+1}\\|_2$：\n            $\\rho_{k+1} \\leftarrow \\rho_k / \\kappa$ 且 $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} \\times \\kappa$。\n    h. 为下一次迭代做准备：更新状态变量 $(\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k)$ 为 $(\\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1}_{\\text{post}}, \\rho_{k+1})$。\n3.  如果循环完成但未收敛，则返回最大迭代次数。\n最终答案中的 Python 代码实现了此逻辑。",
            "answer": "```python\nimport numpy as np\n\ndef admm_solver(Q, R, q, r, rho_init, adaptive_params, max_iter, eps_abs, eps_rel):\n    \"\"\"\n    Solves a 2D quadratic consensus ADMM problem.\n\n    Args:\n        Q (np.array): 2x2 symmetric positive definite matrix for f(x).\n        R (np.array): 2x2 symmetric positive definite matrix for g(z).\n        q (np.array): 2x1 vector for f(x).\n        r (np.array): 2x1 vector for g(z).\n        rho_init (float): Initial penalty parameter.\n        adaptive_params (tuple): (is_adaptive, mu, kappa) for rho adaptation.\n        max_iter (int): Maximum number of iterations.\n        eps_abs (float): Absolute tolerance for stopping criteria.\n        eps_rel (float): Relative tolerance for stopping criteria.\n\n    Returns:\n        int: Number of iterations to converge, or max_iter.\n    \"\"\"\n    is_adaptive, mu, kappa = adaptive_params\n\n    # Initialization\n    n = 2  # Dimension\n    x_k = np.zeros((n, 1))\n    z_k = np.zeros((n, 1))\n    u_k = np.zeros((n, 1))\n    rho_k = float(rho_init)\n\n    I = np.identity(n)\n\n    for k in range(max_iter):\n        # Precompute matrix inverses, which depend on rho\n        try:\n            inv_Q_rhoI = np.linalg.inv(Q + rho_k * I)\n            inv_R_rhoI = np.linalg.inv(R + rho_k * I)\n        except np.linalg.LinAlgError:\n            # In case of numerical issues, though unlikely with SPD matrices\n            return max_iter\n\n        # x-update\n        x_k1 = inv_Q_rhoI @ (rho_k * (z_k - u_k) - q)\n\n        # z-update\n        z_k1 = inv_R_rhoI @ (rho_k * (x_k1 + u_k) - r)\n\n        # u-update (before potential scaling)\n        u_k1_prescale = u_k + x_k1 - z_k1\n\n        # Calculate residuals\n        r_k1 = x_k1 - z_k1\n        s_k1 = rho_k * (z_k1 - z_k)\n\n        # Calculate residual norms\n        norm_r = np.linalg.norm(r_k1)\n        norm_s = np.linalg.norm(s_k1)\n\n        # Calculate stopping thresholds\n        eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x_k1), np.linalg.norm(z_k1))\n        \n        # y^{k+1} = rho_k * u_{k+1}, where u_{k+1} = u_k + r_{k+1}\n        y_k1 = rho_k * u_k1_prescale\n        eps_dual = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(y_k1)\n\n        # Check for convergence\n        if norm_r = eps_pri and norm_s = eps_dual:\n            return k + 1\n\n        # Penalty parameter and dual variable adaptation for the next iteration\n        rho_k1 = rho_k\n        u_k1 = u_k1_prescale\n        if is_adaptive:\n            if norm_r > mu * norm_s:\n                rho_k1 = kappa * rho_k\n                u_k1 = u_k1_prescale / kappa\n            elif norm_s > mu * norm_r:\n                rho_k1 = rho_k / kappa\n                u_k1 = u_k1_prescale * kappa\n        \n        # Update states for next iteration\n        # x_k is not needed, will be recomputed\n        z_k = z_k1\n        u_k = u_k1\n        rho_k = rho_k1\n    \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Common parameters\n    eps_abs = 1e-6\n    eps_rel = 1e-6\n    max_iter = 50000\n    \n    # Common vectors for specified cases\n    q_vec = np.array([[-1.0], [2.0]])\n    r_vec = np.array([[0.5], [-1.0]])\n\n    # Case 1 (well-conditioned, fixed rho)\n    Q1 = np.array([[4.0, 1.0], [1.0, 2.0]])\n    R1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    rho1 = 1.0\n    params1 = (False, 0, 0)\n    \n    # Case 2 (well-conditioned, adaptive small rho)\n    rho2 = 1e-4\n    params2 = (True, 10.0, 2.0)\n\n    # Case 3 (well-conditioned, adaptive large rho)\n    rho3 = 1e2\n    params3 = (True, 2.0, 2.0)\n    \n    # Case 4 (ill-conditioned, fixed rho)\n    Q4 = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    R4 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    rho4 = 1.0\n    params4 = (False, 0, 0)\n    \n    # Case 5 (ill-conditioned, adaptive very small rho)\n    rho5 = 1e-6\n    params5 = (True, 3.0, 2.0)\n\n    test_cases = [\n        (Q1, R1, q_vec, r_vec, rho1, params1),\n        (Q1, R1, q_vec, r_vec, rho2, params2),\n        (Q1, R1, q_vec, r_vec, rho3, params3),\n        (Q4, R4, q_vec, r_vec, rho4, params4),\n        (Q4, R4, q_vec, r_vec, rho5, params5),\n    ]\n\n    results = []\n    for case in test_cases:\n        iter_count = admm_solver(*case, max_iter, eps_abs, eps_rel)\n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "尽管 ADMM 对于两个块的凸问题具有收敛性保证，但这种保证并不能直接推广到三个或更多块的情况。事实上，简单地将两块 ADMM 的高斯-赛德尔（Gauss-Seidel）迭代模式应用于多块问题可能导致算法发散。线性迭代方法的收敛性可以通过分析其迭代矩阵的谱半径来判断；若谱半径大于1，则迭代过程是不稳定的。本练习提供了一个极具启发性的反例。通过构建一个简单的三块二次规划问题并分析其迭代矩阵，您将亲眼见证并从根本上理解其发散的内在机制，这对于将 ADMM 应用于多块优化场景是一个至关重要的教训。",
            "id": "3364446",
            "problem": "考虑一个每块为一维的线性约束三块二次优化问题，由变量 $x_1 \\in \\mathbb{R}$、$x_2 \\in \\mathbb{R}$、$x_3 \\in \\mathbb{R}$ 和约束系数标量 $A_1, A_2, A_3 \\in \\mathbb{R}$ 定义：\n$$\n\\min_{x_1,x_2,x_3} \\;\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\quad \\text{subject to} \\quad A_1 x_1 + A_2 x_2 + A_3 x_3 = c,\n$$\n其中 $q_1, q_2, q_3 \\ge 0$，$c \\in \\mathbb{R}$ 是一个给定的常数。\n\n众所周知，将交替方向乘子法（ADMM, Alternating Direction Method of Multipliers）以高斯-赛德尔 (Gauss-Seidel) 方式朴素地应用于超过两个块的问题时，算法可能会发散。在上述三块问题中，考虑使用惩罚参数 $\\rho  0$ 和缩放对偶变量 $u$ 的缩放对偶 ADMM 迭代：\n1. 固定 $x_2$、$x_3$、$u$，通过关于 $x_1$ 最小化增广拉格朗日函数来更新 $x_1$。\n2. 固定 $x_1$ (刚刚更新的)、$x_3$、$u$，通过关于 $x_2$ 最小化增广拉格朗日函数来更新 $x_2$。\n3. 固定 $x_1$、$x_2$ (刚刚更新的)、$u$，通过关于 $x_3$ 最小化增广拉格朗日函数来更新 $x_3$。\n4. 用一个单步的高斯-赛德尔 (Gauss-Seidel) 步骤更新缩放对偶变量 $u$。\n\n从增广拉格朗日函数的定义出发\n$$\n\\mathcal{L}_\\rho(x_1,x_2,x_3,u) \\;=\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\;+\\; \\frac{\\rho}{2}\\left(A_1 x_1 + A_2 x_2 + A_3 x_3 - c + u\\right)^2,\n$$\n每个块更新的一阶最优性条件可以导出闭式线性更新。记\n$$\n\\theta_i \\;=\\; \\frac{\\rho A_i}{q_i + \\rho A_i^2}, \\quad i \\in \\{1,2,3\\},\n$$\n假设 $A_i \\neq 0$ 或 $q_i  0$，因此所有分母均为正。然后，对于 $c=0$（这分离出了齐次线性迭代），朴素的 Gauss-Seidel ADMM 块更新为：\n$$\nx_1^{k+1} = - \\theta_1\\left(A_2 x_2^k + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_2^{k+1} = - \\theta_2\\left(A_1 x_1^{k+1} + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_3^{k+1} = - \\theta_3\\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k\\right),\n$$\n$$\nu^{k+1} = u^k + \\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}\\right).\n$$\n\n这四个方程构成了对状态向量 $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top$ 的线性迭代，可以写成\n$$\nz^{k+1} = J z^k,\n$$\n其中迭代矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 仅由 $(q_1,q_2,q_3)$、$(A_1,A_2,A_3)$ 和 $\\rho$ 确定。发散机制由 $J$ 的谱半径决定，即\n$$\n\\varrho(J) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } J\\}.\n$$\n如果 $\\varrho(J)  1$，迭代是线性不稳定的，并会沿着与不稳定特征值相关联的特征向量发散；如果 $\\varrho(J)  1$，迭代会收缩并线性收敛到不动点（最优解）；如果 $\\varrho(J) = 1$，迭代至多是临界稳定的，并且可能无法收敛。\n\n任务：\n- 对于齐次情况 $c=0$，推导与上述 Gauss-Seidel ADMM 步骤一致的显式迭代矩阵 $J$。\n- 根据 $\\varrho(J)$ 解释发散机制。\n- 实现一个程序，该程序能为给定的参数构造 $J$，计算 $\\varrho(J)$，并为一组能够检验收敛、临界稳定和发散情况的参数集报告其谱半径。\n\n你的程序必须：\n1. 通过将线性更新映射应用于 $\\mathbb{R}^4$ 的每个规范基向量来构建迭代矩阵 $J$。\n2. 将谱半径计算为 $J$ 的特征值的最大模。\n3. 对于每个测试用例，返回谱半径，该值应为浮点数并四舍五入到六位小数。\n\n用于覆盖不同情况的测试套件：\n- 情况 1（强凸、良态，预期收敛）：$(q_1,q_2,q_3) = (10.0,10.0,10.0)$, $(A_1,A_2,A_3) = (1.0,1.0,1.0)$, $\\rho = 1.0$, $c = 0.0$。\n- 情况 2（无目标函数二次项，单位系数，临界稳定）：$(q_1,q_2,q_3) = (0.0,0.0,0.0)$, $(A_1,A_2,A_3) = (1.0,1.0,1.0)$, $\\rho = 1.0$, $c = 0.0$。\n- 情况 3（低幅值且符号不定的系数产生大的块增益，预期发散）：$(q_1,q_2,q_3) = (0.01,0.01,0.01)$, $(A_1,A_2,A_3) = (0.2,-0.2,0.2)$, $\\rho = 1.0$, $c = 0.0$。\n- 情况 4（混合符号与中等缩放，压力测试）：$(q_1,q_2,q_3) = (0.0,0.0,0.0)$, $(A_1,A_2,A_3) = (0.5,0.5,-0.5)$, $\\rho = 1.0$, $c = 0.0$。\n\n最终输出格式：\n你的程序应该生成单行输出，其中包含四个测试用例的谱半径，格式为用方括号括起来的逗号分隔列表，每个值都四舍五入到六位小数（例如，“[0.732101,1.000000,1.284557,0.998340]”）。\n此问题不涉及物理单位，也没有角度；所有量都是无量纲的实数。",
            "solution": "问题陈述已经过验证，被认为具有科学依据、适定、客观和完整。所有参数和更新规则都已提供，可以直接进行无歧义的推导和分析。该问题探讨了数值优化中的一个标准且重要的课题——多于两个块的交替方向乘子法（ADMM）的收敛性——并被构建为一个可解的数学和计算任务。\n\n对三块 ADMM 迭代的分析始于将给定的 Gauss-Seidel 更新步骤表述为作用于状态向量的单个线性变换。第 $k$ 次迭代的状态由向量 $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top \\in \\mathbb{R}^4$ 表示。迭代过程通过一个由迭代矩阵 $J \\in \\mathbb{R}^{4 \\times 4}$ 表示的线性算子，将该状态映射到下一个状态 $z^{k+1}$，使得 $z^{k+1} = J z^k$。\n\n为了推导矩阵 $J$，我们首先写出齐次情况（$c=0$）下的更新方程：\n$$x_1^{k+1} = - \\theta_1(A_2 x_2^k + A_3 x_3^k + u^k)$$\n$$x_2^{k+1} = - \\theta_2(A_1 x_1^{k+1} + A_3 x_3^k + u^k)$$\n$$x_3^{k+1} = - \\theta_3(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k)$$\n$$u^{k+1} = u^k + A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}$$\n\n这些方程构成一个耦合系统。上标为 $k+1$ 的变量是未知数，它们依赖于上标为 $k$ 的变量（来自上一次迭代的已知数）以及在当前迭代中已经更新的其他变量（高斯-赛德尔依赖关系）。为了形成一个标准的矩阵方程，我们重新整理这些方程，将所有上标为 $k+1$ 的项移到左侧，所有上标为 $k$ 的项移到右侧。\n\n1. $1 \\cdot x_1^{k+1} = 0 \\cdot x_1^k - \\theta_1 A_2 x_2^k - \\theta_1 A_3 x_3^k - \\theta_1 u^k$\n2. $\\theta_2 A_1 x_1^{k+1} + 1 \\cdot x_2^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k - \\theta_2 A_3 x_3^k - \\theta_2 u^k$\n3. $\\theta_3 A_1 x_1^{k+1} + \\theta_3 A_2 x_2^{k+1} + 1 \\cdot x_3^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k - \\theta_3 u^k$\n4. $-A_1 x_1^{k+1} - A_2 x_2^{k+1} - A_3 x_3^{k+1} + 1 \\cdot u^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k + 1 \\cdot u^k$\n\n这个线性方程组可以写成矩阵形式 $L z^{k+1} = R z^k$，其中 $L$ 和 $R$ 是 $4 \\times 4$ 矩阵。矩阵 $L$ 包含 $z^{k+1}$ 项的系数，矩阵 $R$ 包含 $z^k$ 项的系数。\n\n根据上述方程，我们可以显式地构造 $L$ 和 $R$：\n$$\nL = \\begin{pmatrix}\n1  0  0  0 \\\\\n\\theta_2 A_1  1  0  0 \\\\\n\\theta_3 A_1  \\theta_3 A_2  1  0 \\\\\n-A_1  -A_2  -A_3  1\n\\end{pmatrix}\n$$\n矩阵 $L$ 是一个对角线上元素为 1 的下三角矩阵，这反映了 Gauss-Seidel 更新的顺序、前馈性质。这种结构保证了 $L$ 是可逆的。\n\n$$\nR = \\begin{pmatrix}\n0  -\\theta_1 A_2  -\\theta_1 A_3  -\\theta_1 \\\\\n0  0  -\\theta_2 A_3  -\\theta_2 \\\\\n0  0  0  -\\theta_3 \\\\\n0  0  0  1\n\\end{pmatrix}\n$$\n矩阵 $R$ 是一个上三角矩阵。第一列为零，因为 $x_1^k$ 没有出现在任何更新方程的右侧。\n\n迭代矩阵 $J$ 接着通过求解 $z^{k+1}$ 得到：\n$$z^{k+1} = L^{-1} R z^k \\implies J = L^{-1} R$$\n在数值计算上，这是通过求解矩阵 $J$ 的线性系统 $LJ = R$ 来计算的，这比直接计算 $L$ 的逆矩阵更稳定、更高效。\n\n迭代 $z^{k+1} = J z^k$ 的收敛性由 $J$ 的谱半径确定，记作 $\\varrho(J)$。谱半径定义为 $J$ 的特征值的最大模：$\\varrho(J) = \\max \\{|\\lambda_i|\\}$，其中 $\\lambda_i$ 是 $J$ 的特征值。\n通过考虑迭代的长期行为可以理解其发散机制。通常，任何初始状态 $z^0$ 都可以表示为 $J$ 的特征向量的线性组合（为简化说明，假设 $J$ 是可对角化的）。设对于特征向量 $v_i$ 和特征值 $\\lambda_i$ 有 $J v_i = \\lambda_i v_i$。如果 $z^0 = \\sum_i c_i v_i$，那么经过 $k$ 次迭代后，状态为 $z^k = J^k z^0 = \\sum_i c_i \\lambda_i^k v_i$。\n- 如果 $\\varrho(J)  1$，那么对所有 $i$ 都有 $|\\lambda_i|  1$。因此，当 $k \\to \\infty$ 时，$\\lambda_i^k \\to 0$，迭代收敛到零向量，这是不动点（也是 $c=0$ 时的最优解）。\n- 如果 $\\varrho(J)  1$，则至少存在一个特征值 $\\lambda_j$ 满足 $|\\lambda_j|  1$。只要初始状态 $z^0$ 沿着 $v_j$ 有一个非零分量（即 $c_j \\neq 0$），对应的项 $c_j \\lambda_j^k v_j$ 的模就会增长，导致状态向量 $z^k$ 发散。这就是线性不稳定的条件。\n- 如果 $\\varrho(J) = 1$，迭代是临界稳定的。状态的某些分量可能不会衰减，导致振荡或缓慢的次线性收敛，甚至在模为 1 的特征值的几何重数小于其代数重数时发散。\n\n下面的程序将实现这一推导。对于每个测试用例，它将：\n1. 定义参数 $q_i, A_i, \\rho$。\n2. 计算中间变量 $\\theta_i = \\frac{\\rho A_i}{q_i + \\rho A_i^2}$。\n3. 根据上面推导的公式构造矩阵 $L$ 和 $R$。\n4. 求解线性矩阵方程 $LJ=R$ 以找到迭代矩阵 $J$。\n5. 计算 $J$ 的特征值。\n6. 通过找到特征值中的最大绝对值来确定谱半径 $\\varrho(J)$。\n7. 报告这个值，四舍五入到六位小数。\n对所有提供的测试用例重复此过程，以探索不同的收敛机制。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the 3-block ADMM iteration matrix\n    for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple: ((q1, q2, q3), (A1, A2, A3), rho)\n    test_cases = [\n        # Case 1: strongly convex, well-conditioned, expected contraction\n        ((10.0, 10.0, 10.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 2: no objective quadratics, identity coefficients, marginal stability\n        ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 3: low-magnitude coefficients producing large gains, expected divergence\n        ((0.01, 0.01, 0.01), (0.2, -0.2, 0.2), 1.0),\n        # Case 4: mixed signs with moderate scaling, stress test\n        ((0.0, 0.0, 0.0), (0.5, 0.5, -0.5), 1.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        (q1, q2, q3), (A1, A2, A3), rho = case\n        \n        # Calculate theta_i values\n        # theta_i = (rho * A_i) / (q_i + rho * A_i^2)\n        # Denominators are positive as per problem constraints.\n        theta1 = (rho * A1) / (q1 + rho * A1**2)\n        theta2 = (rho * A2) / (q2 + rho * A2**2)\n        theta3 = (rho * A3) / (q3 + rho * A3**2)\n        \n        # Construct the L matrix from the equation L * z^{k+1} = R * z^k\n        # L = [[1, 0, 0, 0],\n        #      [theta2*A1, 1, 0, 0],\n        #      [theta3*A1, theta3*A2, 1, 0],\n        #      [-A1, -A2, -A3, 1]]\n        L = np.array([\n            [1.0, 0.0, 0.0, 0.0],\n            [theta2 * A1, 1.0, 0.0, 0.0],\n            [theta3 * A1, theta3 * A2, 1.0, 0.0],\n            [-A1, -A2, -A3, 1.0]\n        ])\n        \n        # Construct the R matrix\n        # R = [[0, -theta1*A2, -theta1*A3, -theta1],\n        #      [0, 0, -theta2*A3, -theta2],\n        #      [0, 0, 0, -theta3],\n        #      [0, 0, 0, 1]]\n        R = np.array([\n            [0.0, -theta1 * A2, -theta1 * A3, -theta1],\n            [0.0, 0.0, -theta2 * A3, -theta2],\n            [0.0, 0.0, 0.0, -theta3],\n            [0.0, 0.0, 0.0, 1.0]\n        ])\n\n        # The iteration matrix J is defined by z^{k+1} = J * z^k,\n        # where J = inv(L) * R.\n        # This is best solved as L*J = R.\n        J = np.linalg.solve(L, R)\n        \n        # Compute the eigenvalues of the iteration matrix J.\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # The spectral radius is the maximum magnitude of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        results.append(spectral_radius)\n\n    # Format the final output as a comma-separated list in brackets,\n    # with each value rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}