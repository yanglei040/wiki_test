{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a foundational workout in deriving and interpreting the Karush-Kuhn-Tucker (KKT) conditions for a constrained sparse optimization problem. By adding a non-negativity constraint to the standard LASSO objective, a scenario common in signal processing and machine learning where features represent physical quantities, we must carefully handle the complete KKT system. This practice  will solidify your understanding of how Lagrange multipliers and complementary slackness conditions explicitly characterize the optimal solution.",
            "id": "3456248",
            "problem": "Consider the sparse optimization problem in compressed sensing with nonnegativity constraints,\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\quad \\text{subject to} \\quad x \\ge 0,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\lambda  0$, and the inequality $x \\ge 0$ holds elementwise. Because of the nonnegativity constraints, $\\|x\\|_{1} = \\mathbf{1}^{\\top}x$ on the feasible set, where $\\mathbf{1}$ denotes the vector of all ones in $\\mathbb{R}^{n}$. Starting from the definition of the Lagrangian and the Karush–Kuhn–Tucker (KKT) conditions (Karush–Kuhn–Tucker (KKT) conditions) for inequality-constrained convex programs, and from Fenchel–Rockafellar duality, perform the following:\n\n- Derive the KKT system (primal feasibility, dual feasibility, stationarity, and complementary slackness) for the above problem. Clearly identify the residual $r := A x - b$ and summarize the per-coordinate optimality conditions in terms of correlations $A^{\\top} r$.\n- Derive the Lagrange dual problem by treating the least-squares term via its convex conjugate, and characterize the dual feasibility region. Explain how nonnegativity changes the dual feasibility relative to the unconstrained Least Absolute Shrinkage and Selection Operator (LASSO), particularly how it alters the correlation thresholding conditions that govern support identifiability.\n- Using your KKT characterization, determine the smallest real value $\\lambda_{\\star}$ such that $x^{\\star} = 0$ is optimal for the given data\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1  0 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{pmatrix}, \n\\quad\nb \\;=\\; \\begin{pmatrix}\n3 \\\\ 2 \\\\ 4\n\\end{pmatrix}.\n$$\n\nYour final answer must be a single real number for $\\lambda_{\\star}$, expressed exactly (no rounding needed).",
            "solution": "The given optimization problem is\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\quad \\text{subject to} \\quad x \\ge 0,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda  0$. The constraint $x \\ge 0$ means $x_i \\ge 0$ for all $i \\in \\{1, \\dots, n\\}$. On the feasible set where $x \\ge 0$, the $\\ell_1$-norm simplifies to $\\|x\\|_{1} = \\sum_{i=1}^{n} x_i = \\mathbf{1}^{\\top}x$. The problem can thus be rewritten as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; f(x) := \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\mathbf{1}^{\\top}x \\quad \\text{subject to} \\quad -x_i \\le 0 \\text{ for } i=1, \\dots, n.\n$$\nThis is a convex optimization problem because the objective function $f(x)$ is a sum of a convex quadratic function and a linear function, hence convex, and the constraints are linear, defining a convex feasible set. Since Slater's condition holds (e.g., $x = \\mathbf{1}$ is a strictly feasible point), strong duality holds, and the Karush–Kuhn–Tucker (KKT) conditions are necessary and sufficient for optimality.\n\nFirst, we derive the KKT system for this problem. The Lagrangian is constructed by introducing Lagrange multipliers $\\mu_i \\ge 0$ for each constraint $-x_i \\le 0$:\n$$\nL(x, \\mu) = f(x) + \\sum_{i=1}^{n} \\mu_i(-x_i) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\mathbf{1}^{\\top}x - \\mu^{\\top}x.\n$$\nThe KKT conditions for an optimal point $x^{\\star}$ with corresponding dual variable $\\mu^{\\star}$ are:\n1.  **Primal Feasibility**: $x^{\\star} \\ge 0$.\n2.  **Dual Feasibility**: $\\mu^{\\star} \\ge 0$.\n3.  **Complementary Slackness**: $\\mu_i^{\\star} x_i^{\\star} = 0$ for all $i=1, \\dots, n$.\n4.  **Stationarity**: The gradient of the Lagrangian with respect to $x$ must be zero at $x^{\\star}$: $\\nabla_x L(x^{\\star}, \\mu^{\\star}) = 0$.\n\nLet's compute the gradient for the stationarity condition:\n$$\n\\nabla_x L(x, \\mu) = \\nabla_x \\left( \\frac{1}{2}(Ax-b)^{\\top}(Ax-b) + \\lambda \\mathbf{1}^{\\top}x - \\mu^{\\top}x \\right) = A^{\\top}(Ax-b) + \\lambda\\mathbf{1} - \\mu.\n$$\nLet the residual be denoted by $r = Ax-b$. The stationarity condition is $A^{\\top}r + \\lambda\\mathbf{1} - \\mu = 0$, or $\\mu = A^{\\top}r + \\lambda\\mathbf{1}$.\n\nWe can now summarize the KKT system in terms of the primal variable $x$ and the residual $r = Ax-b$:\n-   $x_i \\ge 0$ for all $i$.\n-   $[A^{\\top}r]_i + \\lambda \\ge 0$ for all $i$ (from $\\mu_i \\ge 0$).\n-   $([A^{\\top}r]_i + \\lambda)x_i = 0$ for all $i$.\n\nThese conditions can be broken down per coordinate $i$:\n-   If $x_i  0$ (the coordinate is in the active set), complementary slackness requires $[A^{\\top}r]_i + \\lambda = 0$, which implies the correlation $[A^{\\top}r]_i = -\\lambda$.\n-   If $x_i = 0$ (the coordinate is inactive), complementary slackness is satisfied. The remaining condition is dual feasibility, $[A^{\\top}r]_i + \\lambda \\ge 0$, which implies $[A^{\\top}r]_i \\ge -\\lambda$.\nThe vector $A^{\\top}r$ contains the correlations between the columns of $A$ and the residual.\n\nNext, we derive the Lagrange dual problem using the Fenchel-Rockafellar duality framework. We express the primal problem as $\\min_{x} h(Ax) + g(x)$, where $h(y) = \\frac{1}{2}\\|y-b\\|_2^2$ and $g(x) = \\lambda \\|x\\|_1 + I_+(x)$, with $I_+(x)$ being the indicator function for the non-negative orthant $\\{x \\mid x \\ge 0\\}$. The dual problem is $\\max_{\\nu} -h^{\\star}(\\nu) - g^{\\star}(-A^{\\top}\\nu)$.\nThe convex conjugate of $h(y)$ is:\n$$\nh^{\\star}(\\nu) = \\sup_{y} (\\nu^{\\top}y - h(y)) = \\sup_{y} \\left(\\nu^{\\top}y - \\frac{1}{2}\\|y-b\\|_2^2\\right).\n$$\nThe maximizer is found by setting the gradient to zero: $\\nu - (y-b) = 0 \\implies y = \\nu+b$. Substituting this back gives $h^{\\star}(\\nu) = \\nu^{\\top}(\\nu+b) - \\frac{1}{2}\\|\\nu\\|_2^2 = \\frac{1}{2}\\|\\nu\\|_2^2 + \\nu^{\\top}b$.\nThe convex conjugate of $g(x)$ is:\n$$\ng^{\\star}(z) = \\sup_{x} (z^{\\top}x - g(x)) = \\sup_{x \\ge 0} (z^{\\top}x - \\lambda\\mathbf{1}^{\\top}x) = \\sup_{x \\ge 0} (z - \\lambda\\mathbf{1})^{\\top}x.\n$$\nThis supremum is $0$ if $z - \\lambda\\mathbf{1} \\le 0$ (i.e., $z_i \\le \\lambda$ for all $i$), and it is $+\\infty$ otherwise. Thus, $g^{\\star}(z)$ is the indicator function for the set $\\{z \\mid z \\le \\lambda\\mathbf{1}\\}$.\nThen, $g^{\\star}(-A^{\\top}\\nu)$ is $0$ if $-A^{\\top}\\nu \\le \\lambda\\mathbf{1}$, which is equivalent to $A^{\\top}\\nu \\ge -\\lambda\\mathbf{1}$, and it is $+\\infty$ otherwise.\nThe dual problem is to maximize:\n$$\n\\max_{\\nu} \\quad -h^{\\star}(\\nu) - g^{\\star}(-A^{\\top}\\nu) = \\max_{\\nu} \\quad -\\left(\\frac{1}{2}\\|\\nu\\|_2^2 + \\nu^{\\top}b\\right) \\quad \\text{subject to} \\quad A^{\\top}\\nu \\ge -\\lambda\\mathbf{1}.\n$$\nThis is equivalent to minimizing the negative of the objective:\n$$\n\\min_{\\nu} \\quad \\frac{1}{2}\\|\\nu\\|_2^2 + \\nu^{\\top}b \\quad \\text{subject to} \\quad A^{\\top}\\nu \\ge -\\lambda\\mathbf{1}.\n$$\nBy completing the square, this is equivalent to $\\min_{\\nu} \\frac{1}{2}\\|\\nu+b\\|_2^2$ under the same constraints. The dual variable $\\nu$ can be identified with the primal residual $r = Ax-b$. The dual feasibility region is defined by $A^{\\top}\\nu \\ge -\\lambda\\mathbf{1}$.\n\nFor comparison, the dual of the unconstrained LASSO problem ($\\min_x \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda\\|x\\|_1$) has the feasibility condition $\\|A^{\\top}\\nu\\|_{\\infty} \\le \\lambda$, which is equivalent to $-\\lambda\\mathbf{1} \\le A^{\\top}\\nu \\le \\lambda\\mathbf{1}$.\nThe nonnegativity constraint $x \\ge 0$ relaxes the dual feasibility condition from a symmetric interval $[-\\lambda, \\lambda]$ for each correlation $[A^{\\top}\\nu]_i$ to a one-sided condition $[A^{\\top}\\nu]_i \\ge -\\lambda$. This alters the thresholding mechanism: for a variable $x_i$ to become active (i.e., $x_i0$), its corresponding correlation $[A^{\\top}r]_i$ must hit the lower bound $-\\lambda$. In the unconstrained LASSO, the *magnitude* of the correlation $|[A^{\\top}r]_i|$ must hit the threshold $\\lambda$.\n\nFinally, we determine the smallest value $\\lambda_{\\star}$ for which $x^{\\star}=0$ is an optimal solution for the given data.\nFor $x^{\\star}=0$ to be optimal, it must satisfy the KKT conditions. Setting $x=0$, the condition on inactive coordinates must hold for all $i=1, \\dots, n$:\n$$\n[A^{\\top}(A(0)-b)]_i \\ge -\\lambda \\quad \\implies \\quad [-A^{\\top}b]_i \\ge -\\lambda \\quad \\implies \\quad [A^{\\top}b]_i \\le \\lambda.\n$$\nThis inequality must hold for all $i=1, \\dots, n$. This requires $\\lambda$ to be greater than or equal to the maximum of these correlation values:\n$$\n\\lambda \\ge \\max_{i} \\left\\{ [A^{\\top}b]_i \\right\\}.\n$$\nThe smallest value $\\lambda_{\\star}$ that ensures $x^{\\star} = 0$ is an optimal solution is therefore the maximum of these correlations (provided this maximum is positive, which implies $\\lambda_{\\star}  0$ as required).\nThe given data are:\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1  0 \\\\\n0  1  1  1 \\\\\n1  1  0  1\n\\end{pmatrix}, \n\\quad\nb \\;=\\; \\begin{pmatrix}\n3 \\\\\n2 \\\\\n4\n\\end{pmatrix}.\n$$\nWe compute the vector of correlations $A^{\\top}b$:\n$$\nA^{\\top}b \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  0 \\\\\n0  1  1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n2 \\\\\n4\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1(3) + 0(2) + 1(4) \\\\\n0(3) + 1(2) + 1(4) \\\\\n1(3) + 1(2) + 0(4) \\\\\n0(3) + 1(2) + 1(4)\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n7 \\\\\n6 \\\\\n5 \\\\\n6\n\\end{pmatrix}.\n$$\nThe smallest value of $\\lambda$ must be the maximum component of this vector:\n$$\n\\lambda_{\\star} = \\max\\{7, 6, 5, 6\\} = 7.\n$$",
            "answer": "$$\\boxed{7}$$"
        },
        {
            "introduction": "Beyond verifying the optimality of a single point, the KKT conditions can be used to map the entire solution path of the LASSO as the regularization parameter $\\lambda$ changes. This exercise demonstrates that the solution vector $x^{\\star}(\\lambda)$ is a piecewise-linear function of $\\lambda$, with changes in its support occurring at specific breakpoints. By deriving this path from first principles , you will develop a powerful intuition for the mechanics of variable selection and the dynamic nature of the optimality conditions.",
            "id": "3456230",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem, which seeks $x \\in \\mathbb{R}^{n}$ minimizing the objective\n$$\n\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda \\geq 0$ is the regularization parameter. Assume $A$ has columns of unit Euclidean norm. Starting from the fundamental definitions of convex optimization and the Karush-Kuhn-Tucker (KKT) conditions, you will derive the structure of the solution path $x^{\\star}(\\lambda)$ and determine the conditions under which the support of $x^{\\star}(\\lambda)$ changes.\n\nLet $m = 2$ and $n = 2$ with\n$$\na_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad a_{2} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2}\\end{pmatrix}, \\quad A = \\begin{pmatrix}1  \\frac{1}{2} \\\\ 0  \\frac{\\sqrt{3}}{2}\\end{pmatrix}, \\quad y = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}.\n$$\nBoth columns of $A$ have unit norm and correlation $a_{1}^{\\top} a_{2} = \\frac{1}{2}$. Define the residual $r(\\lambda) = y - A x^{\\star}(\\lambda)$, and recall that subgradients of the $\\ell_{1}$ norm are given componentwise by $z_{j} \\in \\partial |x_{j}|$: $z_{j} = \\operatorname{sign}(x_{j})$ if $x_{j} \\neq 0$ and $z_{j} \\in [-1, 1]$ if $x_{j} = 0$.\n\nTasks:\n- Derive the KKT conditions that characterize optimality for this LASSO problem.\n- Using only these conditions and the basic properties of convex differentiable objectives and subgradients, analyze how the support and sign pattern of $x^{\\star}(\\lambda)$ evolve as $\\lambda$ decreases from a sufficiently large value to $0$. Identify the exact breakpoint values of $\\lambda$ at which the support changes and determine the corresponding sign pattern.\n- Compute the explicit piecewise-linear expression for the solution path $x^{\\star}(\\lambda)$ over the entire range $\\lambda \\in [0, \\infty)$ for the given $A$ and $y$.\n- Explain which KKT conditions become tight at each breakpoint and why these tightness events trigger changes in the support.\n\nYour final answer must be the complete analytic expression of the piecewise-defined vector $x^{\\star}(\\lambda)$ as a function of $\\lambda$ for all $\\lambda \\geq 0$. No numerical rounding is required; give exact expressions.",
            "solution": "The problem is to find the solution path $x^{\\star}(\\lambda)$ for the Least Absolute Shrinkage and Selection Operator (LASSO) problem, which is defined by the minimization of the objective function:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right)\n$$\nfor given data $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda \\ge 0$. The objective function is convex, being the sum of a differentiable convex function, $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, and a non-differentiable convex function, $g(x) = \\lambda \\|x\\|_{1}$.\n\nThe first-order necessary and sufficient condition for a point $x^{\\star}$ to be a minimizer is that the zero vector must be an element of the subdifferential of the objective function at $x^{\\star}$. The subdifferential of the sum is the sum of the subdifferentials:\n$$\n0 \\in \\partial \\left( \\frac{1}{2}\\|A x^{\\star} - y\\|_{2}^{2} + \\lambda \\|x^{\\star}\\|_{1} \\right) = \\nabla \\left( \\frac{1}{2}\\|A x^{\\star} - y\\|_{2}^{2} \\right) + \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\nThe gradient of the least-squares term is $\\nabla f(x^{\\star}) = A^{T}(A x^{\\star} - y)$. Let the residual be $r(x^{\\star}) = y - A x^{\\star}$. Then $\\nabla f(x^{\\star}) = -A^{T} r(x^{\\star})$. The subdifferential of the $\\ell_{1}$-norm, $\\partial \\|x^{\\star}\\|_{1}$, is the set of vectors $z \\in \\mathbb{R}^{n}$ with components $z_{j}$ such that $z_{j} = \\operatorname{sign}(x^{\\star}_{j})$ if $x^{\\star}_{j} \\neq 0$, and $z_{j} \\in [-1, 1]$ if $x^{\\star}_{j} = 0$.\n\nThe optimality condition is therefore $-A^{T} r(x^{\\star}) + \\lambda z = 0$ for some $z \\in \\partial \\|x^{\\star}\\|_{1}$, which can be rewritten as $A^{T} r(x^{\\star}) = \\lambda z$. This yields the Karush-Kuhn-Tucker (KKT) conditions for each component $j \\in \\{1, \\dots, n\\}$:\n1.  If $x^{\\star}_{j} \\neq 0$, then $a_{j}^{T} r(x^{\\star}) = \\lambda \\operatorname{sign}(x^{\\star}_{j})$. The correlation of column $a_j$ with the residual is maximal and aligned with the sign of $x^\\star_j$.\n2.  If $x^{\\star}_{j} = 0$, then $|a_{j}^{T} r(x^{\\star})| \\le \\lambda$. The correlation of column $a_j$ with the residual is sub-maximal.\n\nWe now apply these conditions to the specific problem with $n=2$, $m=2$, and\n$$\nA = \\begin{pmatrix}1  \\frac{1}{2} \\\\ 0  \\frac{\\sqrt{3}}{2}\\end{pmatrix}, \\quad y = \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}.\n$$\nThe columns are $a_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $a_{2} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2}\\end{pmatrix}$.\n\nWe trace the solution path $x^{\\star}(\\lambda)$ as $\\lambda$ decreases from $\\infty$ to $0$.\n\n**Segment 1: Large $\\lambda$  First Breakpoint $\\lambda_{1}$**\nFor sufficiently large $\\lambda$, the $\\ell_{1}$ penalty is so strong that the optimal solution is $x^{\\star} = 0$. In this case, the residual is $r(0) = y$. The KKT conditions for inactive variables ($x^{\\star}_{1}=0, x^{\\star}_{2}=0$) require $|a_{j}^{T} y| \\le \\lambda$ for $j=1, 2$.\nWe compute the correlations:\n$c_{1} = a_{1}^{T} y = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = 1$.\n$c_{2} = a_{2}^{T} y = \\begin{pmatrix}\\frac{1}{2}  \\frac{\\sqrt{3}}{2}\\end{pmatrix} \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} = \\frac{1+\\sqrt{3}}{2}$.\nThe solution $x^{\\star}=0$ is optimal as long as $\\lambda \\ge |c_1|$ and $\\lambda \\ge |c_2|$. Since $\\frac{1+\\sqrt{3}}{2} \\approx 1.366  1$, this holds for $\\lambda \\ge \\frac{1+\\sqrt{3}}{2}$.\nThe first breakpoint $\\lambda_1$ occurs when the solution transitions from $0$. This happens at the largest value of $\\lambda$ for which one of the inactive KKT conditions becomes an equality:\n$\\lambda_{1} = \\max_{j} |a_{j}^{T} y| = \\frac{1+\\sqrt{3}}{2}$.\nAt $\\lambda = \\lambda_{1}$, the condition $|a_{2}^{T} r| \\le \\lambda$ becomes tight. For $\\lambda  \\lambda_{1}$, $x_2$ must become non-zero to maintain optimality. The sign of $x_2$ will be $\\operatorname{sign}(a_{2}^{T} y) = +1$.\nThus, for $\\lambda \\in [\\frac{1+\\sqrt{3}}{2}, \\infty)$, $x^{\\star}(\\lambda) = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$.\n\n**Segment 2: $\\lambda \\in [\\lambda_{2}, \\lambda_{1})$  Second Breakpoint $\\lambda_{2}$**\nFor this range of $\\lambda$, the active set of indices is $\\mathcal{A}=\\{2\\}$, so $x_{1}=0$ and $x_{2}  0$. The KKT conditions are:\n-   $|a_{1}^{T} r(x^{\\star})| \\le \\lambda$ (for $x_{1}=0$)\n-   $a_{2}^{T} r(x^{\\star}) = \\lambda$ (for $x_{2}0$)\nHere $x^{\\star} = (0, x_{2})^{T}$, so $A x^{\\star} = a_{2} x_{2}$. The second condition gives $a_{2}^{T}(y - a_{2} x_{2}) = \\lambda$. Since $\\|a_{2}\\|_{2}=1$, $a_{2}^{T} a_{2}=1$. This simplifies to $a_{2}^{T} y - x_{2} = \\lambda$.\nSolving for $x_{2}$: $x_{2}(\\lambda) = a_{2}^{T} y - \\lambda = \\frac{1+\\sqrt{3}}{2} - \\lambda$. This is positive for $\\lambda  \\lambda_{1}$.\nNow we check the condition for the inactive variable $x_1$:\n$a_{1}^{T} r(x^{\\star}) = a_{1}^{T} (y - a_{2} x_{2}(\\lambda)) = a_{1}^{T} y - (a_{1}^{T} a_{2}) x_{2}(\\lambda)$.\nWe have $a_{1}^{T} y = 1$ and $a_{1}^{T} a_{2} = \\frac{1}{2}$.\n$a_{1}^{T} r(x^{\\star}) = 1 - \\frac{1}{2}\\left(\\frac{1+\\sqrt{3}}{2} - \\lambda\\right) = 1 - \\frac{1+\\sqrt{3}}{4} + \\frac{\\lambda}{2} = \\frac{3-\\sqrt{3}}{4} + \\frac{\\lambda}{2}$.\nThe condition is $|\\frac{3-\\sqrt{3}}{4} + \\frac{\\lambda}{2}| \\le \\lambda$. Since $3  \\sqrt{3}$ and $\\lambda0$, the argument is positive.\n$\\frac{3-\\sqrt{3}}{4} + \\frac{\\lambda}{2} \\le \\lambda \\implies \\frac{3-\\sqrt{3}}{4} \\le \\frac{\\lambda}{2} \\implies \\lambda \\ge \\frac{3-\\sqrt{3}}{2}$.\nThis solution is valid for $\\lambda \\in [\\frac{3-\\sqrt{3}}{2}, \\frac{1+\\sqrt{3}}{2})$. The next breakpoint $\\lambda_2$ occurs when this inequality becomes tight:\n$\\lambda_{2} = \\frac{3-\\sqrt{3}}{2}$.\nAt $\\lambda=\\lambda_2$, $|a_{1}^{T} r| = \\lambda_2$, signaling that $x_1$ enters the model. Its sign will be $\\operatorname{sign}(a_{1}^{T} r(\\lambda_2)) = \\operatorname{sign}(\\lambda_2) = +1$.\n\n**Segment 3: $\\lambda \\in [0, \\lambda_{2})$**\nFor $\\lambda  \\lambda_2$, the active set is $\\mathcal{A}=\\{1, 2\\}$, with $x_{1}0, x_{2}0$. Both KKT conditions are equalities:\n-   $a_{1}^{T} (y - A x^{\\star}) = \\lambda$\n-   $a_{2}^{T} (y - A x^{\\star}) = \\lambda$\nThis forms a linear system: $A^{T} (y - A x^{\\star}) = \\lambda \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$, or $A^{T} A x^{\\star} = A^{T} y - \\lambda \\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$.\nThe solution is $x^{\\star}(\\lambda) = (A^{T}A)^{-1} \\left( A^{T}y - \\lambda \\begin{pmatrix}1 \\\\ 1\\end{pmatrix} \\right)$.\nFirst, we compute the required matrices:\n$A^{T}A = \\begin{pmatrix}1  0 \\\\ \\frac{1}{2}  \\frac{\\sqrt{3}}{2}\\end{pmatrix} \\begin{pmatrix}1  \\frac{1}{2} \\\\ 0  \\frac{\\sqrt{3}}{2}\\end{pmatrix} = \\begin{pmatrix}1  \\frac{1}{2} \\\\ \\frac{1}{2}  1\\end{pmatrix}$.\n$(A^{T}A)^{-1} = \\frac{1}{1 - 1/4} \\begin{pmatrix}1  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1\\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix}1  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1\\end{pmatrix} = \\begin{pmatrix}\\frac{4}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3}\\end{pmatrix}$.\n$A^{T}y = \\begin{pmatrix}1 \\\\ \\frac{1+\\sqrt{3}}{2}\\end{pmatrix}$.\nNow we solve for $x^{\\star}(\\lambda)$:\n$x^{\\star}(\\lambda) = \\begin{pmatrix}\\frac{4}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3}\\end{pmatrix} \\left( \\begin{pmatrix}1 \\\\ \\frac{1+\\sqrt{3}}{2}\\end{pmatrix} - \\begin{pmatrix}\\lambda \\\\ \\lambda\\end{pmatrix} \\right) = \\begin{pmatrix}\\frac{4}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3}\\end{pmatrix} \\begin{pmatrix}1-\\lambda \\\\ \\frac{1+\\sqrt{3}}{2}-\\lambda\\end{pmatrix}$.\nThe components are:\n$x_{1}(\\lambda) = \\frac{4}{3}(1-\\lambda) - \\frac{2}{3}(\\frac{1+\\sqrt{3}}{2}-\\lambda) = \\frac{4}{3} - \\frac{4}{3}\\lambda - \\frac{1+\\sqrt{3}}{3} + \\frac{2}{3}\\lambda = \\frac{3-\\sqrt{3}}{3} - \\frac{2}{3}\\lambda = 1 - \\frac{\\sqrt{3}}{3} - \\frac{2}{3}\\lambda$.\n$x_{2}(\\lambda) = -\\frac{2}{3}(1-\\lambda) + \\frac{4}{3}(\\frac{1+\\sqrt{3}}{2}-\\lambda) = -\\frac{2}{3} + \\frac{2}{3}\\lambda + \\frac{2+2\\sqrt{3}}{3} - \\frac{4}{3}\\lambda = \\frac{2\\sqrt{3}}{3} - \\frac{2}{3}\\lambda$.\nThis solution is valid as long as $x_{1}(\\lambda)0$ and $x_{2}(\\lambda)0$.\n$x_{1}(\\lambda)  0 \\implies 1 - \\frac{\\sqrt{3}}{3}  \\frac{2}{3}\\lambda \\implies \\lambda  \\frac{3-\\sqrt{3}}{2} = \\lambda_{2}$.\n$x_{2}(\\lambda)  0 \\implies \\frac{2\\sqrt{3}}{3}  \\frac{2}{3}\\lambda \\implies \\lambda  \\sqrt{3}$.\nSince $\\lambda_2  \\sqrt{3}$, the expression for $x^{\\star}(\\lambda)$ is valid for $\\lambda \\in [0, \\lambda_2)$. As $\\lambda \\to 0$, $x^{\\star}(\\lambda)$ approaches the Ordinary Least Squares solution. No other breakpoints occur.\n\nThe complete solution path $x^{\\star}(\\lambda)$ for $\\lambda \\ge 0$ is a piecewise linear function defined over three intervals.\nLetting $\\lambda_{1} = \\frac{1+\\sqrt{3}}{2}$ and $\\lambda_{2} = \\frac{3-\\sqrt{3}}{2}$:\n- For $\\lambda \\ge \\lambda_{1}$, the support is $\\emptyset$.\n- For $\\lambda \\in [\\lambda_{2}, \\lambda_{1})$, the support is $\\{2\\}$.\n- For $\\lambda \\in [0, \\lambda_{2})$, the support is $\\{1, 2\\}$.\n\nThe expression for $x^{\\star}(\\lambda)$ is given by assembling these three segments.",
            "answer": "$$\n\\boxed{\nx^{\\star}(\\lambda) = \n\\begin{cases}\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}  \\text{if } \\lambda \\ge \\frac{1+\\sqrt{3}}{2} \\\\\n\\begin{pmatrix} 0 \\\\ \\frac{1+\\sqrt{3}}{2} - \\lambda \\end{pmatrix}  \\text{if } \\frac{3-\\sqrt{3}}{2} \\le \\lambda  \\frac{1+\\sqrt{3}}{2} \\\\\n\\begin{pmatrix} 1 - \\frac{\\sqrt{3}}{3} - \\frac{2}{3}\\lambda \\\\ \\frac{2\\sqrt{3}}{3} - \\frac{2}{3}\\lambda \\end{pmatrix}  \\text{if } 0 \\le \\lambda  \\frac{3-\\sqrt{3}}{2}\n\\end{cases}\n}\n$$"
        },
        {
            "introduction": "The power of Lagrange duality extends far beyond any single problem formulation. This practice explores how the KKT framework provides a unified lens for analyzing a family of $\\ell_1$-regularized regression models, each defined by a different loss function: squared, absolute, and Huber loss. By deriving the dual problem for each case , you will see how the convex conjugate of the loss function elegantly determines the dual constraints and the relationship between the primal residuals and dual variables, deepening your understanding of model design.",
            "id": "3456196",
            "problem": "Consider the family of $\\ell_1$-regularized regression problems of the form\n$$\\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\sum_{i=1}^n \\ell\\!\\left(y_i - x_i^\\top \\beta\\right) \\;+\\; \\lambda \\|\\beta\\|_1,$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ has rows $x_i^\\top$, $y \\in \\mathbb{R}^n$, $\\lambda  0$, and the loss $\\ell$ is one of the following convex functions applied componentwise to the residual $r = y - X\\beta$:\n- squared loss $\\ell(t) = \\tfrac{1}{2} t^2$,\n- absolute loss $\\ell(t) = |t|$,\n- Huber loss with parameter $\\delta  0$, given by $\\ell(t) = \\rho_\\delta(t)$ where $\\rho_\\delta(t) = \\tfrac{1}{2} t^2$ for $|t| \\le \\delta$ and $\\rho_\\delta(t) = \\delta |t| - \\tfrac{1}{2}\\delta^2$ for $|t|  \\delta$.\n\nUsing only fundamental definitions from convex analysis (Fenchel conjugate $f^*(u) = \\sup_{z} \\{ u^\\top z - f(z)\\}$, subdifferential $\\partial f$, and the Karush-Kuhn-Tucker (KKT) conditions), together with linear-algebraic manipulations, derive the Lagrange dual problems and dual feasible sets corresponding to each loss, and characterize the KKT relations linking the dual variable to the residual. Based on this principled derivation, which of the following statements correctly compare the dual feasible sets for the three losses and correctly explain how the loss conjugate bounds or clips the residual via the KKT relations?\n\nSelect all that apply.\n\n- A. For squared loss, the dual is $\\max_{\\theta \\in \\mathbb{R}^n} \\; y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2$ subject to $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$. The KKT condition gives $\\theta = r := y - X\\beta$. There is no pointwise bound on $\\theta$ from the loss conjugate, so residuals are not componentwise bounded by the loss; only the design-imposed constraint $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$ appears in the dual feasible set.\n\n- B. For absolute loss, the dual is $\\max_{\\theta \\in \\mathbb{R}^n} \\; y^\\top \\theta$ subject to $\\|\\theta\\|_\\infty \\le 1$ and $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$. The KKT condition gives $\\theta_i \\in \\partial |r_i|$, i.e., $\\theta_i = \\operatorname{sign}(r_i)$ when $r_i \\ne 0$ and $\\theta_i \\in [-1,1]$ when $r_i = 0$. Thus the loss conjugate imposes the componentwise bound $\\|\\theta\\|_\\infty \\le 1$, which forces dual variables to saturate at $\\pm 1$ whenever residuals are nonzero.\n\n- C. For Huber loss with parameter $\\delta  0$, the dual is $\\max_{\\theta \\in \\mathbb{R}^n} \\; y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2$ subject to $\\|\\theta\\|_\\infty \\le \\delta$ and $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$. The KKT relation is $\\theta_i = \\mathrm{clip}(r_i,[-\\delta,\\delta])$, i.e., $\\theta_i = r_i$ when $|r_i| \\le \\delta$ and $\\theta_i = \\delta \\,\\operatorname{sign}(r_i)$ when $|r_i|  \\delta$. Hence the loss conjugate imposes the bound $\\|\\theta\\|_\\infty \\le \\delta$, which clips the dual variable (and thereby the subgradient) at level $\\delta$.\n\n- D. For squared loss, the dual feasible set necessarily includes the constraint $\\|\\theta\\|_\\infty \\le 1$, and KKT implies $\\theta_i \\in \\partial |r_i|$, so residual components are bounded in magnitude by $1$.\n\n- E. For absolute loss, the dual objective is $y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2$ with the single constraint $\\|\\theta\\|_\\infty \\le 1$, and KKT gives $\\theta = r$, implying $\\|r\\|_\\infty \\le 1$.\n\n- F. For Huber loss, the dual feasible set is only $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$ (no constraint on $\\|\\theta\\|_\\infty$), because the Huber conjugate is finite everywhere, and KKT yields $\\theta = r$ exactly as in the squared-loss case.",
            "solution": "The user wants me to validate the problem statement and, if it is valid, solve the problem by deriving the Lagrange duals and KKT conditions for $\\ell_1$-regularized regression with three different loss functions.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem is to analyze the optimization problem:\n$$ \\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\sum_{i=1}^n \\ell\\!\\left(y_i - x_i^\\top \\beta\\right) \\;+\\; \\lambda \\|\\beta\\|_1 $$\nwhere:\n-   The optimization variable is $\\beta \\in \\mathbb{R}^p$.\n-   The data are a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with rows $x_i^\\top$, a response vector $y \\in \\mathbb{R}^n$.\n-   The regularization parameter is $\\lambda  0$.\n-   The loss function $\\ell(t)$ is one of three convex functions applied to the components of the residual vector $r = y - X\\beta$:\n    1.  Squared loss: $\\ell(t) = \\tfrac{1}{2} t^2$.\n    2.  Absolute loss: $\\ell(t) = |t|$.\n    3.  Huber loss: $\\ell(t) = \\rho_\\delta(t)$, defined as $\\rho_\\delta(t) = \\tfrac{1}{2} t^2$ for $|t| \\le \\delta$ and $\\rho_\\delta(t) = \\delta |t| - \\tfrac{1}{2}\\delta^2$ for $|t|  \\delta$, with a parameter $\\delta  0$.\n-   The analysis must use fundamental definitions: Fenchel conjugate $f^*(u) = \\sup_{z} \\{ u^\\top z - f(z)\\}$, subdifferential $\\partial f$, and the Karush-Kuhn-Tucker (KKT) conditions.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem is a cornerstone of modern statistics, machine learning, and signal processing, specifically in the areas of sparse modeling and high-dimensional data analysis (e.g., the LASSO). The formulation is based on well-established principles of convex optimization. This is a critical requirement and it is met.\n-   **Well-Posed:** The problem statement is structured as a clear request for mathematical derivation and comparison based on standard theory. The underlying optimization problems are convex and thus have well-defined duals and KKT conditions. A unique solution exists for the derivation task.\n-   **Objective:** The problem is stated in precise mathematical language, free from subjectivity or ambiguity. This is a critical requirement and it is met.\n-   **Other Flaws:** The problem does not violate any of the other criteria for validity. It is complete, consistent, formalizable, and non-trivial.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The solution process will now proceed.\n\n### Derivation of the Dual Problem and KKT Conditions\n\nWe start with the primal problem. To facilitate the derivation of the dual, we introduce an auxiliary variable for the residuals, $r = y - X\\beta$. The problem becomes:\n$$ \\min_{\\beta \\in \\mathbb{R}^p, r \\in \\mathbb{R}^n} \\;\\; \\sum_{i=1}^n \\ell(r_i) + \\lambda \\|\\beta\\|_1 \\quad \\text{subject to} \\quad r + X\\beta = y $$\nThe objective function is convex, and the constraint is affine. We form the Lagrangian by introducing a dual variable $\\theta \\in \\mathbb{R}^n$:\n$$ L(\\beta, r; \\theta) = \\sum_{i=1}^n \\ell(r_i) + \\lambda \\|\\beta\\|_1 - \\theta^\\top(r + X\\beta - y) $$\nThe Lagrange dual function $g(\\theta)$ is the infimum of the Lagrangian over the primal variables $\\beta$ and $r$:\n$$ g(\\theta) = \\inf_{\\beta, r} L(\\beta, r; \\theta) = y^\\top\\theta + \\inf_{r} \\left( \\sum_{i=1}^n \\ell(r_i) - \\theta^\\top r \\right) + \\inf_{\\beta} \\left( \\lambda \\|\\beta\\|_1 - (X^\\top\\theta)^\\top \\beta \\right) $$\nWe analyze the two infimum terms separately.\n\nThe infimum over $r$ is related to the Fenchel conjugate of the total loss $L_{total}(r) = \\sum_{i=1}^n \\ell(r_i)$. The conjugate is $L_{total}^*(\\theta) = \\sup_r(\\theta^\\top r - L_{total}(r))$. Due to the separable nature of the loss, $L_{total}^*(\\theta) = \\sum_{i=1}^n \\ell^*(\\theta_i)$, where $\\ell^*$ is the conjugate of the scalar loss function $\\ell$.\nThus, the first term is:\n$$ \\inf_{r} \\left( \\sum_{i=1}^n \\ell(r_i) - \\theta^\\top r \\right) = - \\sup_{r} \\left( \\theta^\\top r - \\sum_{i=1}^n \\ell(r_i) \\right) = - \\sum_{i=1}^n \\ell^*(\\theta_i) $$\n\nThe infimum over $\\beta$ is related to the conjugate of the $\\ell_1$-norm. Let $h(\\beta) = \\lambda \\|\\beta\\|_1$. Its conjugate is $h^*(v) = \\sup_\\beta(v^\\top\\beta - \\lambda\\|\\beta\\|_1)$. This conjugate is $0$ if $\\|v\\|_\\infty \\le \\lambda$ and $+\\infty$ otherwise. It can be written as an indicator function $I_{\\{\\|\\cdot\\|_\\infty \\le \\lambda\\}}(v)$.\nThe second term is:\n$$ \\inf_{\\beta} \\left( \\lambda \\|\\beta\\|_1 - (X^\\top\\theta)^\\top \\beta \\right) = - \\sup_{\\beta} \\left( (X^\\top\\theta)^\\top \\beta - \\lambda \\|\\beta\\|_1 \\right) = -h^*(X^\\top\\theta) $$\nThis expression evaluates to $0$ if $\\|X^\\top\\theta\\|_\\infty \\le \\lambda$, and $-\\infty$ otherwise.\n\nCombining these results, the dual function is:\n$$ g(\\theta) = y^\\top\\theta - \\sum_{i=1}^n \\ell^*(\\theta_i) \\quad \\text{if } \\|X^\\top\\theta\\|_\\infty \\le \\lambda $$\nand $g(\\theta) = -\\infty$ otherwise. The domain of the conjugate functions $\\ell^*(\\theta_i)$ may introduce additional constraints. The dual problem is to maximize $g(\\theta)$:\n$$ \\max_{\\theta \\in \\mathbb{R}^n} \\;\\; y^\\top\\theta - \\sum_{i=1}^n \\ell^*(\\theta_i) \\quad \\text{subject to} \\quad \\|X^\\top\\theta\\|_\\infty \\le \\lambda \\text{ and } \\theta \\in \\text{dom}(L_{total}^*) $$\nThe KKT stationarity condition with respect to $r$ is $\\partial_r L(\\beta^*, r^*; \\theta^*) \\ni 0$. This implies $\\partial_r (\\sum_i \\ell(r_i^*)) - \\theta^* \\ni 0$, which simplifies to $\\theta_i^* \\in \\partial \\ell(r_i^*)$ for each component $i=1, \\dots, n$.\n\nWe now apply this general framework to each of the three loss functions.\n\n**Case 1: Squared Loss**\n-   Loss: $\\ell(t) = \\tfrac{1}{2}t^2$. This is differentiable, so $\\partial\\ell(t) = \\{\\ell'(t)\\} = \\{t\\}$.\n-   Conjugate: $\\ell^*(u) = \\sup_t(ut - \\tfrac{1}{2}t^2)$. The supremum is found by setting the derivative to zero: $u - t = 0 \\implies t=u$. Thus, $\\ell^*(u) = u^2 - \\tfrac{1}{2}u^2 = \\tfrac{1}{2}u^2$. The domain of $\\ell^*$ is all of $\\mathbb{R}$.\n-   Dual problem: The term $\\sum \\ell^*(\\theta_i)$ becomes $\\tfrac{1}{2}\\sum \\theta_i^2 = \\tfrac{1}{2}\\|\\theta\\|_2^2$. Since $\\text{dom}(\\ell^*) = \\mathbb{R}$, no additional constraints are placed on $\\theta$. The dual problem is:\n    $$ \\max_{\\theta \\in \\mathbb{R}^n} \\;\\; y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2 \\quad \\text{subject to} \\quad \\|X^\\top \\theta\\|_\\infty \\le \\lambda $$\n-   KKT condition for $r$: The relation $\\theta_i \\in \\partial\\ell(r_i)$ becomes $\\theta_i = r_i$. Thus, at the optimum, the dual variable is equal to the residual vector: $\\theta = r = y-X\\beta$.\n\n**Case 2: Absolute Loss**\n-   Loss: $\\ell(t) = |t|$. The subdifferential is $\\partial|t| = \\{\\text{sign}(t)\\}$ for $t \\ne 0$ and $\\partial|0| = [-1,1]$.\n-   Conjugate: $\\ell^*(u) = \\sup_t(ut - |t|)$. This supremum is finite only if $|u| \\le 1$, in which case it is $0$. Therefore, $\\ell^*(u)$ is the indicator function of the interval $[-1,1]$, i.e., $\\ell^*(u)=0$ if $|u|\\le 1$ and $+\\infty$ otherwise.\n-   Dual problem: The term $\\sum \\ell^*(\\theta_i)$ is $0$ if $\\|\\theta\\|_\\infty \\le 1$ and $+\\infty$ otherwise. This imposes the constraint $\\|\\theta\\|_\\infty \\le 1$ on the dual feasible set. The dual problem becomes:\n    $$ \\max_{\\theta \\in \\mathbb{R}^n} \\;\\; y^\\top \\theta \\quad \\text{subject to} \\quad \\|X^\\top \\theta\\|_\\infty \\le \\lambda \\text{ and } \\|\\theta\\|_\\infty \\le 1 $$\n-   KKT condition for $r$: The relation $\\theta_i \\in \\partial|r_i|$ means that if $r_i \\ne 0$, then $\\theta_i = \\text{sign}(r_i)$, and if $r_i=0$, then $\\theta_i \\in [-1,1]$.\n\n**Case 3: Huber Loss**\n-   Loss: $\\ell(t) = \\rho_\\delta(t)$. This function is differentiable everywhere, with derivative $\\ell'(t) = \\text{clip}(t, [-\\delta, \\delta]) = \\max(-\\delta, \\min(\\delta, t))$.\n-   Conjugate: $\\ell^*(u) = \\sup_t(ut - \\rho_\\delta(t))$. The supremum is unbounded if $|u|  \\delta$. If $|u| \\le \\delta$, the supremum is attained at $t=u$, for which $\\rho_\\delta(u) = \\tfrac{1}{2}u^2$. So, $\\ell^*(u) = u^2 - \\tfrac{1}{2}u^2 = \\tfrac{1}{2}u^2$. Therefore, $\\ell^*(u) = \\tfrac{1}{2}u^2$ if $|u|\\le\\delta$ and $+\\infty$ otherwise.\n-   Dual problem: The term $\\sum \\ell^*(\\theta_i)$ is $\\tfrac{1}{2}\\|\\theta\\|_2^2$ if $\\|\\theta\\|_\\infty \\le \\delta$ and $+\\infty$ otherwise. This imposes the constraint $\\|\\theta\\|_\\infty \\le \\delta$ on the dual feasible set. The dual problem is:\n    $$ \\max_{\\theta \\in \\mathbb{R}^n} \\;\\; y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2 \\quad \\text{subject to} \\quad \\|X^\\top \\theta\\|_\\infty \\le \\lambda \\text{ and } \\|\\theta\\|_\\infty \\le \\delta $$\n-   KKT condition for $r$: The relation $\\theta_i \\in \\partial\\ell(r_i)$ becomes $\\theta_i = \\ell'(r_i) = \\text{clip}(r_i, [-\\delta, \\delta])$. This means $\\theta_i = r_i$ if $|r_i| \\le \\delta$, and $\\theta_i = \\delta \\cdot \\text{sign}(r_i)$ if $|r_i|  \\delta$.\n\n### Option-by-Option Analysis\n\n-   **A. For squared loss, the dual is $\\max_{\\theta \\in \\mathbb{R}^n} \\; y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2$ subject to $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$. The KKT condition gives $\\theta = r := y - X\\beta$. There is no pointwise bound on $\\theta$ from the loss conjugate, so residuals are not componentwise bounded by the loss; only the design-imposed constraint $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$ appears in the dual feasible set.**\n    Our derivation matches this statement perfectly. The dual problem, the KKT relation, and the characterization of the feasible set are all correct. The domain of the squared loss conjugate is $\\mathbb{R}$, so it imposes no constraint on $\\theta$.\n    **Verdict: Correct.**\n\n-   **B. For absolute loss, the dual is $\\max_{\\theta \\in \\mathbb{R}^n} \\; y^\\top \\theta$ subject to $\\|\\theta\\|_\\infty \\le 1$ and $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$. The KKT condition gives $\\theta_i \\in \\partial |r_i|$, i.e., $\\theta_i = \\operatorname{sign}(r_i)$ when $r_i \\ne 0$ and $\\theta_i \\in [-1,1]$ when $r_i = 0$. Thus the loss conjugate imposes the componentwise bound $\\|\\theta\\|_\\infty \\le 1$, which forces dual variables to saturate at $\\pm 1$ whenever residuals are nonzero.**\n    Our derivation matches this statement perfectly. The dual problem, the KKT relation, and the interpretation are correct. The conjugate of the absolute loss imposes the constraint $\\|\\theta\\|_\\infty \\le 1$. The KKT condition $\\theta_i = \\text{sign}(r_i)$ for non-zero residuals means $\\theta_i$ takes values in $\\{-1, 1\\}$, i.e., it saturates.\n    **Verdict: Correct.**\n\n-   **C. For Huber loss with parameter $\\delta  0$, the dual is $\\max_{\\theta \\in \\mathbb{R}^n} \\; y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2$ subject to $\\|\\theta\\|_\\infty \\le \\delta$ and $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$. The KKT relation is $\\theta_i = \\mathrm{clip}(r_i,[-\\delta,\\delta])$, i.e., $\\theta_i = r_i$ when $|r_i| \\le \\delta$ and $\\theta_i = \\delta \\,\\operatorname{sign}(r_i)$ when $|r_i|  \\delta$. Hence the loss conjugate imposes the bound $\\|\\theta\\|_\\infty \\le \\delta$, which clips the dual variable (and thereby the subgradient) at level $\\delta$.**\n    The derived dual problem and KKT relation are identical to those in the statement. The Huber conjugate indeed imposes the constraint $\\|\\theta\\|_\\infty \\le \\delta$. The KKT relation is correctly identified as a clipping function. The final phrase \"clips the dual variable... at level $\\delta$\" is slightly imprecise phrasing, as the dual variable $\\theta_i$ is the *result* of clipping the residual $r_i$, not the input to a clipping function. However, the core mathematical facts presented (the dual form, the KKT equation, and the resulting bound on $\\theta$) are all correct. In the context of comparing the losses, this statement accurately captures the behavior of the Huber loss.\n    **Verdict: Correct.**\n\n-   **D. For squared loss, the dual feasible set necessarily includes the constraint $\\|\\theta\\|_\\infty \\le 1$, and KKT implies $\\theta_i \\in \\partial |r_i|$, so residual components are bounded in magnitude by $1$.**\n    This statement is incorrect. The constraint $\\|\\theta\\|_\\infty \\le 1$ and the KKT relation $\\theta_i \\in \\partial|r_i|$ are associated with the absolute loss, not the squared loss.\n    **Verdict: Incorrect.**\n\n-   **E. For absolute loss, the dual objective is $y^\\top \\theta - \\tfrac{1}{2}\\|\\theta\\|_2^2$ with the single constraint $\\|\\theta\\|_\\infty \\le 1$, and KKT gives $\\theta = r$, implying $\\|r\\|_\\infty \\le 1$.**\n    This statement is incorrect. The dual objective for absolute loss is $y^\\top \\theta$. The quadratic term $-\\tfrac{1}{2}\\|\\theta\\|_2^2$ belongs to the squared and Huber losses. Also, the KKT relation $\\theta=r$ belongs to the squared loss.\n    **Verdict: Incorrect.**\n\n-   **F. For Huber loss, the dual feasible set is only $\\|X^\\top \\theta\\|_\\infty \\le \\lambda$ (no constraint on $\\|\\theta\\|_\\infty$), because the Huber conjugate is finite everywhere, and KKT yields $\\theta = r$ exactly as in the squared-loss case.**\n    This statement is incorrect. The conjugate of the Huber loss is not finite everywhere; its domain is $\\{u : |u| \\le \\delta\\}$, which adds the constraint $\\|\\theta\\|_\\infty \\le \\delta$ to the dual feasible set. The KKT relation is $\\theta_i = \\text{clip}(r_i, [-\\delta, \\delta])$, not $\\theta=r$.\n    **Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}