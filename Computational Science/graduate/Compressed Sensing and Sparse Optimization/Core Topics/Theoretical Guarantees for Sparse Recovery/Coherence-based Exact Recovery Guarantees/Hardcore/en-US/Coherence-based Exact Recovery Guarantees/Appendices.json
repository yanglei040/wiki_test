{
    "hands_on_practices": [
        {
            "introduction": "To begin our exploration of sparse recovery guarantees, we first derive the most fundamental result in the field. This exercise walks you through the process of establishing a sufficient condition for exact sparse signal recovery based solely on the mutual coherence, $\\mu(A)$, a simple geometric measure of how \"orthogonal\" the columns of a sensing matrix are. By starting from the Exact Recovery Condition (ERC) and using standard matrix analysis tools, you will derive the celebrated threshold that connects the maximum allowable sparsity, $s$, to the coherence of the dictionary . This practice is essential for building a solid theoretical foundation and understanding the interplay between dictionary design and recovery performance.",
            "id": "3435250",
            "problem": "You are given a real matrix with unit-norm columns and asked to analyze the classical mutual-coherence-based exact recovery threshold for sparse recovery, together with its limiting behavior as the mutual coherence vanishes. Work entirely in the setting of linear algebra over the real numbers with finite dimensions. All angles are in radians and no physical units apply.\n\nFoundational base:\n- Mutual coherence of a matrix with unit-norm columns is defined by $\\,\\mu(A) \\triangleq \\max_{i\\neq j}\\,|\\langle a_i,a_j\\rangle|\\,$, where $\\,a_i\\,$ denotes the $\\,i$-th column and $\\,\\langle\\cdot,\\cdot\\rangle\\,$ is the standard inner product.\n- The Gram matrix of a column submatrix $\\,A_S\\,$ is $\\,G_S \\triangleq A_S^\\top A_S\\,$.\n- The Exact Recovery Condition (ERC) from sparse optimization states that a sufficient condition for unique recovery of any $\\,s$-sparse vector via $\\ell_1$-minimization or Orthogonal Matching Pursuit is\n$$\n\\max_{j\\notin S}\\,\\|A_S^+ a_j\\|_1  1,\n$$\nfor every support set $\\,S\\subset\\{1,\\dots,n\\}\\,$ with $\\,|S|=s\\,$, where $\\,A_S^+ \\triangleq (A_S^\\top A_S)^{-1}A_S^\\top\\,$ denotes the Moore–Penrose pseudoinverse for full-column-rank $\\,A_S\\,$.\n- You may appeal to classical matrix norm inequalities (submultiplicativity of matrix norms, relationships between vector and matrix norms), the Neumann series bound for inverses $\\,\\|(I - E)^{-1}\\|\\leq 1/(1-\\|E\\|)\\,$ when $\\,\\|E\\|1\\,$, and the Gershgorin disc theorem to argue invertibility of $\\,G_S\\,$.\n\nTask A (derivation): Starting only from the above foundational base, derive a coherence-only sufficient threshold in terms of $\\,\\mu(A)\\,$ that guarantees the ERC for all supports of size $\\,s\\,$. Then analyze the limit $\\,\\mu(A)\\to 0\\,$ to show that the threshold scales like\n$$\ns \\approx \\tfrac{1}{2}\\!\\left(1+\\tfrac{1}{\\mu(A)}\\right),\n$$\nin the precise sense that the largest integer $\\,s\\,$ obeying the strict inequality implied by your bound satisfies\n$$\n\\lim_{\\mu\\to 0^+}\\; \\mu \\cdot s(\\mu) = \\tfrac{1}{2},\\qquad\n\\lim_{\\mu\\to 0^+}\\; \\frac{s(\\mu)}{\\tfrac{1}{2}(1+\\tfrac{1}{\\mu})} = 1.\n$$\nYour derivation must proceed from the definitions and the listed well-tested facts; do not invoke any result that presupposes the target threshold as an input.\n\nTask B (numerical illustration): Consider the explicit $\\,\\varepsilon$-parametrized family of dictionaries $\\,A(\\varepsilon)\\in\\mathbb{R}^{n\\times n}\\,$ with $\\,n\\geq 2\\,$, defined as follows. Let $\\,\\{e_1,\\dots,e_n\\}\\,$ be the standard basis of $\\,\\mathbb{R}^n\\,$. Define\n- $\\,a_1(\\varepsilon) \\triangleq e_1\\,$,\n- for every $\\,j\\in\\{2,\\dots,n\\}\\,$, $\\,a_j(\\varepsilon) \\triangleq \\varepsilon\\,e_1 + \\sqrt{1-\\varepsilon^2}\\,e_j\\,$,\nand assemble the columns $\\,a_j(\\varepsilon)\\,$ into $\\,A(\\varepsilon)\\,$. Show that for every $\\,\\varepsilon\\in(0,1]\\,$ the columns of $\\,A(\\varepsilon)\\,$ have unit norm and that\n$$\n\\mu\\big(A(\\varepsilon)\\big) = \\max\\{\\varepsilon,\\,\\varepsilon^2\\} = \\varepsilon.\n$$\n\nTask C (program specification): Implement a program that performs the following for each test case $\\,(\\varepsilon, n)\\,$ with $\\,n\\in\\mathbb{N}\\,$ and $\\,\\varepsilon\\in(0,1]\\,$:\n- Constructs $\\,A(\\varepsilon)\\in\\mathbb{R}^{n\\times n}\\,$ as above and computes $\\,\\mu\\big(A(\\varepsilon)\\big)\\,$.\n- Computes the real-valued coherence threshold\n$$\ns_{\\mathrm{cont}}(\\mu) \\triangleq \\tfrac{1}{2}\\!\\left(1+\\tfrac{1}{\\mu}\\right),\n$$\nand the largest integer satisfying the strict inequality implied by your derivation, encoded as\n$$\ns_{\\mathrm{int}}(\\mu) \\triangleq \\left\\lceil s_{\\mathrm{cont}}(\\mu)\\right\\rceil - 1.\n$$\n- Reports the scaled product $\\,\\mu\\cdot s_{\\mathrm{int}}(\\mu)\\,$ and the relative deviation\n$$\n\\mathrm{rel\\_dev} \\triangleq \\frac{\\big|s_{\\mathrm{cont}}(\\mu)-s_{\\mathrm{int}}(\\mu)\\big|}{s_{\\mathrm{cont}}(\\mu)}.\n$$\n- Optionally verifies the Exact Recovery Condition numerically at $\\,s_{\\mathrm{int}}(\\mu)\\,$ by exhaustive support search: define\n$$\nT_{A,s} \\triangleq \\max_{S\\subset\\{1,\\dots,n\\},\\,|S|=s}\\;\\max_{j\\notin S}\\; \\big\\|A_S^+\\,a_j\\big\\|_1.\n$$\nIf $\\,s_{\\mathrm{int}}(\\mu)\\leq n-1\\,$, compute $\\,T_{A,s_{\\mathrm{int}}}\\,$ exactly by enumeration and set an indicator\n$$\n\\mathrm{erc\\_ok} = \\begin{cases}\n1  \\text{if } T_{A,s_{\\mathrm{int}}}  1,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\nIf $\\,s_{\\mathrm{int}}(\\mu)  n-1\\,$, skip verification and output $\\,\\mathrm{erc\\_ok}=-1\\,$.\n- Round all floating-point outputs to $\\,6\\,$ decimal places.\n\nTest suite:\n- Use $\\,n=8\\,$ for all cases and the following $\\,\\varepsilon\\,$ values:\n  - $\\,\\varepsilon = 0.5\\,$,\n  - $\\,\\varepsilon = \\tfrac{1}{3}\\,$,\n  - $\\,\\varepsilon = 0.2\\,$,\n  - $\\,\\varepsilon = 0.15\\,$,\n  - $\\,\\varepsilon = 0.1\\,$,\n  - $\\,\\varepsilon = 0.05\\,$.\nFor each case, the program must output a list\n$$\n\\big[\\mu,\\, s_{\\mathrm{int}},\\, \\mu\\cdot s_{\\mathrm{int}},\\, \\mathrm{rel\\_dev},\\, \\mathrm{erc\\_ok}\\big],\n$$\nwith the real numbers rounded to $\\,6\\,$ decimals as specified.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the per-case lists, enclosed in square brackets, for example\n$$\n\\big[\\,[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\;[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\;\\dots\\big].\n$$\nNo other text should be printed.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in the field of compressed sensing and sparse optimization, with all necessary components defined and a clear set of tasks.\n\n### Task A: Derivation of the Coherence-Based Recovery Threshold\n\nThe primary objective is to derive a sufficient condition on the sparsity level $s$ in terms of the mutual coherence $\\mu(A)$ that guarantees the Exact Recovery Condition (ERC). The ERC is given by\n$$\n\\max_{S:|S|=s} \\max_{j\\notin S} \\|A_S^+ a_j\\|_1  1\n$$\nwhere $A$ is a matrix with unit-norm columns, $S$ is a support set of size $s$, $a_j$ is the $j$-th column of $A$, and $A_S^+$ is the Moore-Penrose pseudoinverse of the submatrix $A_S$ formed by columns with indices in $S$. We are given $A_S^+ = (A_S^\\top A_S)^{-1}A_S^\\top$.\n\nLet's fix a support set $S$ with $|S|=s$ and an index $j \\notin S$. We need to bound $\\|A_S^+ a_j\\|_1$.\nLet $v_j = A_S^+ a_j = (A_S^\\top A_S)^{-1} (A_S^\\top a_j)$.\nLet $G_S = A_S^\\top A_S$ be the Gram matrix of $A_S$. This is an $s \\times s$ matrix. Let $c_j = A_S^\\top a_j$ be an $s$-dimensional vector. Then $v_j = G_S^{-1} c_j$.\n\nUsing standard matrix and vector norm inequalities, we have:\n$$\n\\|v_j\\|_1 = \\|G_S^{-1} c_j\\|_1 \\le \\|G_S^{-1}\\|_1 \\|c_j\\|_1\n$$\nWe need to bound the two terms on the right-hand side.\n\n1.  **Bounding $\\|c_j\\|_1$**: The vector $c_j$ has components $(c_j)_i = \\langle a_i, a_j \\rangle$ for each $i \\in S$. The mutual coherence $\\mu(A)$ is defined as $\\mu(A) \\triangleq \\max_{i\\neq k}\\,|\\langle a_i, a_k\\rangle|$. Since $j \\notin S$, for every $i \\in S$, we have $i \\ne j$. Therefore, $|\\langle a_i, a_j \\rangle| \\le \\mu(A)$.\n    The $\\ell_1$-norm of $c_j$ is the sum of the absolute values of its $s$ components:\n    $$\n    \\|c_j\\|_1 = \\sum_{i \\in S} |\\langle a_i, a_j \\rangle| \\le \\sum_{i \\in S} \\mu(A) = s \\mu(A)\n    $$\n\n2.  **Bounding $\\|G_S^{-1}\\|_1$**: The Gram matrix $G_S$ can be written as $G_S = I + E$, where $I$ is the $s \\times s$ identity matrix and $E$ is a matrix containing the off-diagonal inner products. The diagonal entries of $G_S$ are $(G_S)_{ii} = \\langle a_i, a_i \\rangle = 1$ since all columns are unit-norm. The off-diagonal entries are $(G_S)_{ik} = \\langle a_i, a_k \\rangle$ for $i, k \\in S, i \\ne k$.\n    Thus, $E$ has zeros on its diagonal, and its off-diagonal entries are $(E)_{ik} = \\langle a_i, a_k \\rangle$, so $|(E)_{ik}| \\le \\mu(A)$ for $i \\ne k$.\n\n    To ensure $G_S$ is invertible and to bound its inverse, we can use the Gershgorin circle theorem or the Neumann series. Let's use the latter, which requires a bound on a norm of $E$. The matrix $\\ell_\\infty$-norm is given by the maximum absolute row sum:\n    $$\n    \\|E\\|_\\infty = \\max_{i \\in S} \\sum_{k \\in S} |E_{ik}| = \\max_{i \\in S} \\sum_{k \\in S, k \\ne i} |\\langle a_i, a_k \\rangle| \\le \\max_{i \\in S} \\sum_{k \\in S, k \\ne i} \\mu(A) = (s-1)\\mu(A)\n    $$\n    If $\\|E\\|_\\infty  1$, i.e., $(s-1)\\mu(A)  1$, the matrix $G_S = I+E$ is invertible, and we can bound its inverse norm:\n    $$\n    \\|G_S^{-1}\\|_\\infty = \\|(I+E)^{-1}\\|_\\infty \\le \\frac{1}{1 - \\|E\\|_\\infty} \\le \\frac{1}{1 - (s-1)\\mu(A)}\n    $$\n    Since $G_S$ is a Gram matrix, it is symmetric. Its inverse $G_S^{-1}$ is also symmetric. For any symmetric matrix $M$, $\\|M\\|_1 = \\|M\\|_\\infty$. Therefore,\n    $$\n    \\|G_S^{-1}\\|_1 = \\|G_S^{-1}\\|_\\infty \\le \\frac{1}{1 - (s-1)\\mu(A)}\n    $$\n\nCombining these bounds, we get:\n$$\n\\|A_S^+ a_j\\|_1 \\le \\|G_S^{-1}\\|_1 \\|c_j\\|_1 \\le \\frac{s \\mu(A)}{1 - (s-1)\\mu(A)}\n$$\nFor the ERC to hold, we require this upper bound to be strictly less than $1$:\n$$\n\\frac{s \\mu(A)}{1 - (s-1)\\mu(A)}  1\n$$\nThis inequality is contingent on the denominator being positive, which is the condition $(s-1)\\mu(A)  1$ we already imposed. Assuming this holds, we can multiply both sides by the denominator:\n$$\ns \\mu(A)  1 - (s-1)\\mu(A) \\implies s \\mu(A)  1 - s\\mu(A) + \\mu(A)\n$$\nRearranging the terms gives:\n$$\n2s\\mu(A) - \\mu(A)  1 \\implies (2s-1)\\mu(A)  1\n$$\nAssuming $\\mu(A)  0$, we can divide to find the condition on $s$:\n$$\n2s-1  \\frac{1}{\\mu(A)} \\implies 2s  1 + \\frac{1}{\\mu(A)} \\implies s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right)\n$$\nThis is the well-known coherence-based sufficient condition for exact sparse recovery.\n\nNow, we analyze the limiting behavior as $\\mu \\to 0^+$. Let $s(\\mu)$ be the largest integer satisfying the strict inequality $s  \\frac{1}{2}(1 + 1/\\mu)$. This is given by $s(\\mu) = \\lceil \\frac{1}{2}(1 + 1/\\mu) \\rceil - 1$.\nLet $f(\\mu) = \\frac{1}{2}(1 + 1/\\mu)$. By definition of the ceiling function, we have $f(\\mu) \\le \\lceil f(\\mu) \\rceil  f(\\mu)+1$. Subtracting $1$ gives $f(\\mu)-1 \\le s(\\mu)  f(\\mu)$.\n$\\frac{1}{2}(1 + 1/\\mu) - 1  s(\\mu)  \\frac{1}{2}(1 + 1/\\mu)$.\n$\\frac{1}{2\\mu} - \\frac{1}{2}  s(\\mu)  \\frac{1}{2\\mu} + \\frac{1}{2}$.\n\n1.  **Limit of $\\mu \\cdot s(\\mu)$**: Multiply the inequality by $\\mu  0$:\n    $$\n    \\mu\\left(\\frac{1}{2\\mu} - \\frac{1}{2}\\right)  \\mu \\cdot s(\\mu)  \\mu\\left(\\frac{1}{2\\mu} + \\frac{1}{2}\\right) \\implies \\frac{1}{2} - \\frac{\\mu}{2}  \\mu \\cdot s(\\mu)  \\frac{1}{2} + \\frac{\\mu}{2}\n    $$\n    As $\\mu \\to 0^+$, both the lower and upper bounds converge to $1/2$. By the Squeeze Theorem, $\\lim_{\\mu\\to 0^+} \\mu \\cdot s(\\mu) = \\frac{1}{2}$.\n\n2.  **Limit of the ratio**: Divide the inequality $f(\\mu)-1  s(\\mu)  f(\\mu)$ by $f(\\mu)$:\n    $$\n    \\frac{f(\\mu)-1}{f(\\mu)}  \\frac{s(\\mu)}{f(\\mu)}  \\frac{f(\\mu)}{f(\\mu)} \\implies 1 - \\frac{1}{f(\\mu)}  \\frac{s(\\mu)}{f(\\mu)}  1\n    $$\n    As $\\mu \\to 0^+$, $f(\\mu) = \\frac{1}{2}(1+1/\\mu) \\to \\infty$, so $1/f(\\mu) \\to 0$. The lower bound converges to $1$. By the Squeeze Theorem, $\\lim_{\\mu\\to 0^+} \\frac{s(\\mu)}{f(\\mu)} = 1$.\n\n### Task B: Analysis of the Matrix Family $A(\\varepsilon)$\n\nWe are given $A(\\varepsilon) \\in \\mathbb{R}^{n \\times n}$ for $n \\ge 2, \\varepsilon \\in (0,1]$, with columns $a_1(\\varepsilon) = e_1$ and $a_j(\\varepsilon) = \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j$ for $j \\ge 2$.\n\n1.  **Unit Column Norms**:\n    - For column $1$: $\\|a_1\\|^2 = \\|e_1\\|^2 = 1^2 = 1$.\n    - For columns $j \\in \\{2, \\dots, n\\}$: The vectors $e_1$ and $e_j$ are orthogonal. By the Pythagorean theorem:\n      $\\|a_j\\|^2 = \\|\\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j\\|^2 = (\\varepsilon)^2 + (\\sqrt{1-\\varepsilon^2})^2 = \\varepsilon^2 + 1 - \\varepsilon^2 = 1$.\n    All columns have unit $\\ell_2$-norm.\n\n2.  **Mutual Coherence**: We compute the inner products between distinct columns.\n    - For $j \\in \\{2, \\dots, n\\}$:\n      $\\langle a_1, a_j \\rangle = \\langle e_1, \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j \\rangle = \\varepsilon \\langle e_1, e_1 \\rangle + \\sqrt{1-\\varepsilon^2} \\langle e_1, e_j \\rangle = \\varepsilon \\cdot 1 + 0 = \\varepsilon$.\n    - For $i,j \\in \\{2, \\dots, n\\}$ with $i \\ne j$:\n      $\\langle a_i, a_j \\rangle = \\langle \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_i, \\varepsilon e_1 + \\sqrt{1-\\varepsilon^2} e_j \\rangle$.\n      Expanding this using orthogonality of $\\{e_k\\}$:\n      $\\langle a_i, a_j \\rangle = \\varepsilon^2 \\langle e_1, e_1 \\rangle + (1-\\varepsilon^2) \\langle e_i, e_j \\rangle = \\varepsilon^2 \\cdot 1 + (1-\\varepsilon^2) \\cdot 0 = \\varepsilon^2$.\n    The set of absolute values of off-diagonal inner products is $\\{\\varepsilon, \\varepsilon^2\\}$. The mutual coherence is the maximum of these:\n    $\\mu(A(\\varepsilon)) = \\max \\{|\\varepsilon|, |\\varepsilon^2|\\}$. Since $\\varepsilon \\in (0,1]$, we have $\\varepsilon  0$ and $\\varepsilon^2 \\le \\varepsilon$. Thus, $\\mu(A(\\varepsilon)) = \\varepsilon$.\n\n### Task C: Program Specification Logic\n\nThe program will implement the specified calculations for a test suite where $n=8$ and $\\varepsilon$ takes several values in $(0,1]$. For each test case $(\\varepsilon, n)$:\n1.  The mutual coherence is computed as $\\mu = \\varepsilon$, per the derivation in Task B.\n2.  The continuous and integer sparsity thresholds are calculated:\n    $s_{\\mathrm{cont}}(\\mu) = \\frac{1}{2}(1+1/\\mu)$ and $s_{\\mathrm{int}}(\\mu) = \\lceil s_{\\mathrm{cont}}(\\mu)\\rceil - 1$.\n3.  The scaled product $\\mu \\cdot s_{\\mathrm{int}}(\\mu)$ and relative deviation $\\mathrm{rel\\_dev} = |s_{\\mathrm{cont}}(\\mu)-s_{\\mathrm{int}}(\\mu)|/s_{\\mathrm{cont}}(\\mu)$ are computed.\n4.  The ERC is numerically verified for sparsity $s = s_{\\mathrm{int}}(\\mu)$.\n    - If $s_{\\mathrm{int}}(\\mu)  n-1$, verification is not possible, and $\\mathrm{erc\\_ok}$ is set to $-1$.\n    - Otherwise, the matrix $A(\\varepsilon)$ of size $n \\times n$ is constructed. The program then exhaustively iterates through all support sets $S \\subset \\{1, \\dots, n\\}$ of size $s_{\\mathrm{int}}(\\mu)$ using `itertools.combinations`.\n    - For each support $S$, the program computes the maximum of $\\|A_S^+ a_j\\|_1$ over all $j \\notin S$. The pseudoinverse $A_S^+$ is computed using `numpy.linalg.pinv`.\n    - The overall maximum value across all supports $S$, denoted $T_{A,s_{\\mathrm{int}}}$, is determined.\n    - The indicator $\\mathrm{erc\\_ok}$ is set to $1$ if $T_{A,s_{\\mathrm{int}}}  1$ and $0$ otherwise.\n5.  Finally, a list containing $[\\mu, s_{\\mathrm{int}}, \\mu \\cdot s_{\\mathrm{int}}, \\mathrm{rel\\_dev}, \\mathrm{erc\\_ok}]$, with all floating-point values rounded to $6$ decimal places, is generated for the test case. The results for all test cases are aggregated into a list of lists for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy  # Per library specification, though not strictly used.\nimport math\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing a parametrized matrix family A(eps)\n    and verifying the derived coherence-based exact recovery threshold.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (epsilon, n)\n        (0.5, 8),\n        (1/3, 8),\n        (0.2, 8),\n        (0.15, 8),\n        (0.1, 8),\n        (0.05, 8),\n    ]\n\n    all_results = []\n    for eps, n in test_cases:\n        # Per Task B, mu(A(eps)) = eps for eps in (0, 1].\n        mu = eps\n\n        # Compute coherence-based thresholds\n        s_cont = 0.5 * (1.0 + 1.0 / mu)\n        s_int = math.ceil(s_cont) - 1\n\n        # Compute derived quantities\n        mu_s_int = mu * s_int\n        if s_cont > 0:\n            rel_dev = abs(s_cont - s_int) / s_cont\n        else:\n            rel_dev = 0.0\n\n        # Numerically verify the Exact Recovery Condition (ERC)\n        erc_ok = -1\n        if s_int > n - 1:\n            erc_ok = -1\n        else:\n            # Construct the matrix A(eps)\n            A = np.zeros((n, n), dtype=float)\n            e1 = np.zeros(n, dtype=float)\n            e1[0] = 1.0\n            A[:, 0] = e1\n            for j in range(1, n):\n                ej = np.zeros(n, dtype=float)\n                ej[j] = 1.0\n                A[:, j] = eps * e1 + math.sqrt(1 - eps**2) * ej\n\n            # Exhaustively check all supports of size s_int\n            max_erc_val = 0.0\n            all_indices = set(range(n))\n            \n            # The number of combinations can be large, but for n=8 and s=7 it's feasible.\n            s = int(s_int)\n            # s=0 is trivial, ERC holds if defined. Let's assume s >= 1.\n            if s >= 1:\n                for s_indices in combinations(range(n), s):\n                    S = list(s_indices)\n                    J = list(all_indices - set(S))\n                    \n                    A_S = A[:, S]\n                    \n                    # Compute the Moore-Penrose pseudoinverse\n                    A_S_plus = np.linalg.pinv(A_S)\n                    \n                    for j in J:\n                        a_j = A[:, j]\n                        \n                        # Calculate the l1-norm of the projection\n                        norm_val = np.linalg.norm(A_S_plus @ a_j, ord=1)\n                        \n                        if norm_val > max_erc_val:\n                            max_erc_val = norm_val\n            \n            # The ERC is T_{A,s}  1\n            erc_ok = 1 if max_erc_val  1.0 else 0\n\n        # Assemble the list of results for this case, with rounding\n        case_result = [\n            round(mu, 6),\n            s_int,\n            round(mu_s_int, 6),\n            round(rel_dev, 6),\n            erc_ok\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists\n    # e.g., [[val1, val2], [val3, val4]]\n    # Using str() on a list of lists produces the desired format with spaces.\n    # The problem asks for comma-separated list of lists, without specifying space behavior.\n    # Using a join of stringified lists is safer.\n    final_output_str = f\"[{','.join([str(res) for res in all_results])}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_str.replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "After establishing a foundational recovery guarantee using mutual coherence, a natural question arises: is mutual coherence the whole story? This practice is designed to challenge that notion and reveal the limitations of relying on a single summary statistic. You will construct two distinct dictionaries that share the exact same mutual coherence but exhibit dramatically different recovery behaviors when used with Orthogonal Matching Pursuit (OMP) . The key lies in analyzing the spectral properties—specifically, the minimum eigenvalue—of their sub-Gram matrices, which provides a more nuanced view of the linear independence of subsets of atoms. This exercise is crucial for developing a deeper, more critical understanding of why recovery algorithms succeed or fail, pushing you to look beyond simple pairwise correlations.",
            "id": "3435245",
            "problem": "Construct two explicit real dictionaries $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{m \\times n}$ with identical mutual coherence but different spectral properties of carefully chosen sub-Gram matrices, and empirically compare exact recovery by Orthogonal Matching Pursuit (OMP) for several sparse signals. Work entirely in exact arithmetic over real numbers with no physical units. Angles need not be introduced. The task is to derive and implement, from first principles, the following pipeline:\n\n1. Definitions and core objects to be used.\n   - Mutual coherence $\\mu(D)$ of a dictionary $D$ with unit-norm columns is defined as $\\mu(D) \\triangleq \\max_{i \\neq j} \\left| \\langle d_i, d_j \\rangle \\right|$, where $d_i$ denotes the $i$-th column of $D$. For non-unit columns, the inner products in the Gram matrix must be computed after column-wise normalization.\n   - The sub-Gram matrix of a subset of columns indexed by a finite set $T \\subset \\{1,\\dots,n\\}$ is $G_T(D) \\triangleq D_T^\\top D_T$, where $D_T$ is the submatrix formed by the columns with indices in $T$.\n   - Orthogonal Matching Pursuit (OMP) is a greedy algorithm for sparse recovery. Given a dictionary $D$, a measurement vector $y$, and a target sparsity level $k$, OMP iteratively:\n     - Selects an index maximizing the absolute correlation with the current residual.\n     - Augments the active set.\n     - Recomputes the least-squares fit on the active set and updates the residual.\n     - Repeats exactly $k$ iterations and returns the selected support set.\n   - Exact recovery for a given instance means that the returned support set equals the true support set.\n\n2. Concrete dictionaries. Use $m = 4$ and $n = 5$. Define the columns of $A$ and $B$ as follows, where $\\mathbf{e}_1,\\mathbf{e}_2,\\mathbf{e}_3,\\mathbf{e}_4$ are the standard basis vectors in $\\mathbb{R}^4$:\n   - For $A$:\n     - $a_1 = \\mathbf{e}_1 = [1,0,0,0]^\\top$,\n     - $a_2 = \\mathbf{e}_2 = [0,1,0,0]^\\top$,\n     - $a_3 = \\mathbf{e}_3 = [0,0,1,0]^\\top$,\n     - $a_4 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, \\frac{1}{\\sqrt{2}}\\right]^\\top$,\n     - $a_5 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, -\\frac{1}{\\sqrt{2}}\\right]^\\top$.\n   - For $B$:\n     - $b_1 = \\mathbf{e}_1 = [1,0,0,0]^\\top$,\n     - $b_2 = \\mathbf{e}_2 = [0,1,0,0]^\\top$,\n     - $b_3 = \\mathbf{e}_3 = [0,0,1,0]^\\top$,\n     - $b_4 = \\left[\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}\\right]^\\top$,\n     - $b_5 = -\\mathbf{e}_4 = [0,0,0,-1]^\\top$.\n   All columns are unit-norm. The mutual coherence of both $A$ and $B$ is $\\mu = \\frac{1}{2}$, since the maximum absolute off-diagonal entry in the respective column-normalized Gram matrices equals $\\frac{1}{2}$.\n\n3. Sub-Gram matrices to compare spectrally. Let the target support set be $S = \\{1,2,3\\}$ and define $T = S \\cup \\{4\\} = \\{1,2,3,4\\}$. You must compute the minimum eigenvalue of $G_T(A)$ and $G_T(B)$. These capture how differently the collections of columns $A_T$ and $B_T$ are conditioned, despite $A$ and $B$ having identical mutual coherence.\n\n4. Test suite of sparse signals. For each test, create a coefficient vector $x \\in \\mathbb{R}^n$ and use $y = D x$ with the specified $D \\in \\{A,B\\}$. For all tests, the true support is $S = \\{1,2,3\\}$.\n   - Case $1$ (happy path versus failure mode): $k=3$, $x = [1, 1, 1, 0, 0]^\\top$.\n   - Case $2$ (boundary case from coherence-based guarantees): $k=1$, $x = [1, 0, 0, 0, 0]^\\top$.\n   - Case $3$ (small equal coefficients, aggregating correlation challenge): $k=3$, $x = [0.2, 0.2, 0.2, 0, 0]^\\top$.\n   For each case, run OMP for exactly $k$ iterations with $D=A$ and $D=B$, and report whether the recovered support equals $S$.\n\n5. Required computations and outputs.\n   - Compute $\\mu(A)$ and $\\mu(B)$ and verify whether they are identical within numerical tolerance $\\tau = 10^{-12}$, reported as a boolean per test case.\n   - Compute the minimum eigenvalues $\\lambda_{\\min}\\!\\left(G_T(A)\\right)$ and $\\lambda_{\\min}\\!\\left(G_T(B)\\right)$.\n   - For each case, run OMP as specified and report two booleans indicating exact support recovery using $A$ and using $B$.\n   - For each case, aggregate the results into a list:\n     - $\\left[\\text{mu\\_equal}, \\lambda_{\\min}\\!\\left(G_T(A)\\right), \\lambda_{\\min}\\!\\left(G_T(B)\\right), \\text{omp\\_success\\_A}, \\text{omp\\_success\\_B}\\right]$,\n     where $\\text{mu\\_equal}$, $\\text{omp\\_success\\_A}$, and $\\text{omp\\_success\\_B}$ are booleans, and the eigenvalues are floats rounded to $6$ decimal places.\n   - Final output format: Your program should produce a single line of output containing the list of the three per-case result lists, i.e., a single line that looks like $[\\text{case1\\_result},\\text{case2\\_result},\\text{case3\\_result}]$ with no extra whitespace or text. For example: $[[\\text{...}],[\\text{...}],[\\text{...}]]$.\n\nEnsure your implementation does not read input and uses deterministic linear algebra with a fixed tolerance for numerical comparisons. Implement Orthogonal Matching Pursuit (OMP) exactly as described above, stopping after exactly $k$ iterations in each case. All inner products and Gram matrices must be computed after column normalization when evaluating mutual coherence. Round floating-point eigenvalues to $6$ decimal places as specified.",
            "solution": "The problem requires the construction and analysis of two dictionaries, $A$ and $B$, to demonstrate how spectral properties of sub-Gram matrices, rather than just mutual coherence, influence the success of sparse recovery algorithms like Orthogonal Matching Pursuit (OMP). We will proceed by first defining the mathematical objects, then calculating the required metrics, and finally simulating OMP to verify the theoretical predictions.\n\nThe dictionaries $A, B \\in \\mathbb{R}^{m \\times n}$ are given for $m=4$ and $n=5$. Let $\\mathbf{e}_i$ be the $i$-th standard basis vector in $\\mathbb{R}^4$.\n\nThe columns of dictionary $A$ are:\n- $a_1 = [1, 0, 0, 0]^\\top$\n- $a_2 = [0, 1, 0, 0]^\\top$\n- $a_3 = [0, 0, 1, 0]^\\top$\n- $a_4 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, \\frac{1}{\\sqrt{2}}\\right]^\\top$\n- $a_5 = \\left[\\frac{1}{2}, -\\frac{1}{2}, 0, -\\frac{1}{\\sqrt{2}}\\right]^\\top$\n\nThe columns of dictionary $B$ are:\n- $b_1 = [1, 0, 0, 0]^\\top$\n- $b_2 = [0, 1, 0, 0]^\\top$\n- $b_3 = [0, 0, 1, 0]^\\top$\n- $b_4 = \\left[\\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}, \\frac{1}{2}\\right]^\\top$\n- $b_5 = [0, 0, 0, -1]^\\top$\n\nAll columns of both dictionaries are verified to have a Euclidean norm of $1$.\n\nThe mutual coherence $\\mu(D)$ of a dictionary $D$ with unit-norm columns $d_i$ is $\\mu(D) = \\max_{i \\neq j} |\\langle d_i, d_j \\rangle|$. This is the largest absolute off-diagonal element of the Gram matrix $G(D) = D^\\top D$.\n\nFor dictionary $A$, the Gram matrix $G(A) = A^\\top A$ is:\n$$ G(A) = \\begin{pmatrix}\n1  0  0  1/2  1/2 \\\\\n0  1  0  -1/2  -1/2 \\\\\n0  0  1  0  0 \\\\\n1/2  -1/2  0  1  0 \\\\\n1/2  -1/2  0  0  1\n\\end{pmatrix} $$\nThe maximum absolute off-diagonal value is $|\\pm 1/2| = 1/2$. Thus, $\\mu(A) = 1/2$.\n\nFor dictionary $B$, the Gram matrix $G(B) = B^\\top B$ is:\n$$ G(B) = \\begin{pmatrix}\n1  0  0  1/2  0 \\\\\n0  1  0  1/2  0 \\\\\n0  0  1  1/2  0 \\\\\n1/2  1/2  1/2  1  0 \\\\\n0  0  0  0  1\n\\end{pmatrix} $$\nThe maximum absolute off-diagonal value is $1/2$. Thus, $\\mu(B) = 1/2$.\nThe mutual coherences are identical: $\\mu(A) = \\mu(B) = 1/2$.\n\nNext, we analyze the spectral properties of sub-Gram matrices. The specified index set is $T = \\{1, 2, 3, 4\\}$, containing the true support $S = \\{1, 2, 3\\}$ and one distractor atom. The sub-Gram matrices are $G_T(A) = A_T^\\top A_T$ and $G_T(B) = B_T^\\top B_T$.\n\nThe submatrix $A_T$ consists of columns $a_1, a_2, a_3, a_4$. Its Gram matrix is:\n$$ G_T(A) = \\begin{pmatrix}\n1  0  0  1/2 \\\\\n0  1  0  -1/2 \\\\\n0  0  1  0 \\\\\n1/2  -1/2  0  1\n\\end{pmatrix} $$\nThe characteristic polynomial is $\\det(G_T(A) - \\lambda I) = (1-\\lambda)^2 ((1-\\lambda)^2 - 1/2)$. The eigenvalues are $\\lambda \\in \\{1, 1, 1 - 1/\\sqrt{2}, 1 + 1/\\sqrt{2}\\}$. The minimum eigenvalue is $\\lambda_{\\min}(G_T(A)) = 1 - 1/\\sqrt{2} \\approx 0.292893$.\n\nThe submatrix $B_T$ consists of columns $b_1, b_2, b_3, b_4$. Its Gram matrix is:\n$$ G_T(B) = \\begin{pmatrix}\n1  0  0  1/2 \\\\\n0  1  0  1/2 \\\\\n0  0  1  1/2 \\\\\n1/2  1/2  1/2  1\n\\end{pmatrix} $$\nThe characteristic polynomial is $\\det(G_T(B) - \\lambda I) = (1-\\lambda)^2 ((1-\\lambda)^2 - 3/4)$. The eigenvalues are $\\lambda \\in \\{1, 1, 1 - \\sqrt{3}/2, 1 + \\sqrt{3}/2\\}$. The minimum eigenvalue is $\\lambda_{\\min}(G_T(B)) = 1 - \\sqrt{3}/2 \\approx 0.133975$.\n\nThe minimum eigenvalue of a Gram matrix quantifies the linear independence of its columns. A smaller value indicates that the columns are \"closer\" to being linearly dependent. Here, $\\lambda_{\\min}(G_T(B))  \\lambda_{\\min}(G_T(A))$, suggesting that the set of atoms $\\{b_1, b_2, b_3, b_4\\}$ is more poorly conditioned than $\\{a_1, a_2, a_3, a_4\\}$. This is the key to explaining OMP's differing performance.\n\nOrthogonal Matching Pursuit (OMP) is an iterative greedy algorithm. For a given sparsity $k$, dictionary $D$, and measurement $y = Dx$, it operates as follows for $i=1, \\dots, k$:\n1. Initialization: Residual $r_0 = y$, support set $\\Lambda_0 = \\emptyset$.\n2. Atom selection: Find index $j_i = \\arg\\max_{j} |\\langle d_j, r_{i-1} \\rangle|$.\n3. Support update: $\\Lambda_i = \\Lambda_{i-1} \\cup \\{j_i\\}$.\n4. Signal update and residual calculation: Solve the least-squares problem $x_i = \\arg\\min_{z} \\|y - D_{\\Lambda_i} z\\|_2^2$. The new residual is $r_i = y - D_{\\Lambda_i} x_i$. This is equivalent to projecting $y$ onto the orthogonal complement of the subspace spanned by $D_{\\Lambda_i}$, i.e., $r_i = (I - P_{\\Lambda_i})y$ where $P_{\\Lambda_i}$ is the projection matrix.\nThe final recovered support is $\\Lambda_k$.\n\nWe analyze the test cases using this procedure. Note that indices are 0-based in implementation, so $S=\\{0,1,2\\}$.\n\nCase 1: $k=3$, $x=[1, 1, 1, 0, 0]^\\top$. True support is $S=\\{0,1,2\\}$.\n- For dictionary $A$: $y_A = A x = a_1 + a_2 + a_3 = [1, 1, 1, 0]^\\top$. OMP starts with residual $r_0 = y_A$. The correlations are $|\\langle a_1, r_0 \\rangle|=1$, $|\\langle a_2, r_0 \\rangle|=1$, $|\\langle a_3, r_0 \\rangle|=1$, and others are $0$. Assuming tie-breaking picks the smallest index, OMP will select atoms $0, 1, 2$ in sequence. The recovered support is $\\{0, 1, 2\\}$, which equals $S$. Thus, recovery is successful.\n- For dictionary $B$: $y_B = B x = b_1 + b_2 + b_3 = [1, 1, 1, 0]^\\top$. The initial residual is $r_0=y_B$. The correlations are $|\\langle b_1, r_0 \\rangle|=1$, $|\\langle b_2, r_0 \\rangle|=1$, $|\\langle b_3, r_0 \\rangle|=1$, but $|\\langle b_4, r_0 \\rangle| = |\\langle b_4, b_1+b_2+b_3 \\rangle| = |\\frac{1}{2}+\\frac{1}{2}+\\frac{1}{2}|=1.5$. OMP incorrectly selects atom $3$ (corresponding to $b_4$) in the first step because it has the highest correlation. The final support cannot be $S$. Recovery fails. This failure is a direct consequence of the poor conditioning of $B_T$ as indicated by its smaller $\\lambda_{\\min}$. Atom $b_4$ is highly coherent with the linear combination of the true atoms $b_1+b_2+b_3$.\n\nCase 2: $k=1$, $x=[1, 0, 0, 0, 0]^\\top$. The true support is $\\{0\\}$.\nThe problem specifies that the recovered support should be compared against $S=\\{0,1,2\\}$.\n- For dictionary $A$: $y_A = a_1 = [1, 0, 0, 0]^\\top$. For $k=1$, OMP selects the atom with the highest correlation with $y_A$. This is clearly $a_1$ (index $0$), with correlation $1$. The recovered support is $\\{0\\}$. Since $\\{0\\} \\neq \\{0, 1, 2\\}$, the test is considered a failure.\n- For dictionary $B$: $y_B = b_1 = [1, 0, 0, 0]^\\top$. Similarly, OMP selects atom $b_1$ (index $0$). The recovered support is $\\{0\\}$, which is not equal to $S=\\{0, 1, 2\\}$. The test fails.\nIn both sub-cases, OMP correctly identifies the true $1$-sparse support, but fails the test due to the specified comparison against a $3$-element set.\n\nCase 3: $k=3$, $x=[0.2, 0.2, 0.2, 0, 0]^\\top$. True support is $S=\\{0,1,2\\}$.\nThis case is a scaled version of Case 1. The measurement vector $y$ is simply $0.2$ times the vector from Case 1. The OMP selection criterion $\\arg\\max_{j} |\\langle d_j, r_{i-1} \\rangle|$ is invariant to a positive scaling of the measurement vector (and thus all subsequent residuals). Therefore, the sequence of selected atoms will be identical to Case 1 for both dictionaries.\n- For dictionary $A$: Recovery is successful, yielding support $\\{0, 1, 2\\}$.\n- For dictionary $B$: Recovery fails, as atom $3$ is selected first.\n\nThe results confirm that even with identical mutual coherence, the distribution of correlations among subsets of atoms, captured by the spectral properties of sub-Gram matrices, is a more decisive factor for sparse recovery.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the compressed sensing problem by defining two dictionaries,\n    calculating their properties, and running Orthogonal Matching Pursuit (OMP).\n    \"\"\"\n\n    # 1. Define Dictionaries A and B\n    sqrt2 = np.sqrt(2.0)\n    A = np.array([\n        [1.0, 0.0, 0.0, 0.5, 0.5],\n        [0.0, 1.0, 0.0, -0.5, -0.5],\n        [0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0 / sqrt2, -1.0 / sqrt2]\n    ])\n\n    B = np.array([\n        [1.0, 0.0, 0.0, 0.5, 0.0],\n        [0.0, 1.0, 0.0, 0.5, 0.0],\n        [0.0, 0.0, 1.0, 0.5, 0.0],\n        [0.0, 0.0, 0.0, 0.5, -1.0]\n    ])\n\n    def get_mutual_coherence(D):\n        \"\"\"Computes the mutual coherence of a dictionary.\"\"\"\n        # Normalize columns to be safe, as per problem description\n        D_norm = D / np.linalg.norm(D, axis=0)\n        gram_matrix = D_norm.T @ D_norm\n        np.fill_diagonal(gram_matrix, 0)\n        return np.max(np.abs(gram_matrix))\n\n    def get_min_eigenvalue_subgram(D, T_indices):\n        \"\"\"Computes the minimum eigenvalue of a sub-Gram matrix.\"\"\"\n        D_T = D[:, T_indices]\n        G_T = D_T.T @ D_T\n        # use eigvalsh for Hermitian matrices\n        eigenvalues = np.linalg.eigvalsh(G_T)\n        return np.min(eigenvalues)\n\n    def omp(D, y, k):\n        \"\"\"\n        Orthogonal Matching Pursuit algorithm.\n        Stops after exactly k iterations.\n        \"\"\"\n        num_atoms = D.shape[1]\n        residual = np.copy(y)\n        support = []\n        \n        for _ in range(k):\n            correlations = np.abs(D.T @ residual)\n            # Mask already selected atoms\n            if support:\n                correlations[support] = -1\n            \n            # Find the best atom (smallest index in case of a tie)\n            best_atom_idx = np.argmax(correlations)\n            support.append(best_atom_idx)\n\n            # Update residual using projection\n            D_support = D[:, support]\n            \n            # Solve least squares: x = (D_S^T D_S)^-1 D_S^T y\n            # Using pseudoinverse for stability, although solve would also work\n            # as submatrices are full rank here.\n            x_support = np.linalg.pinv(D_support) @ y\n            \n            # Update residual\n            residual = y - D_support @ x_support\n            \n        return set(support)\n\n    # 2. Define Test Cases and Fixed Parameters\n    T_indices = [0, 1, 2, 3] # Indices for sub-Gram matrix, 1-based {1,2,3,4}\n    S_target = {0, 1, 2}     # Target support set, 1-based {1,2,3}\n    \n    test_cases = [\n        # k, x_coeffs\n        (3, np.array([1.0, 1.0, 1.0, 0.0, 0.0])),\n        (1, np.array([1.0, 0.0, 0.0, 0.0, 0.0])),\n        (3, np.array([0.2, 0.2, 0.2, 0.0, 0.0])),\n    ]\n\n    # Pre-compute dictionary properties as they are the same for all cases\n    mu_A = get_mutual_coherence(A)\n    mu_B = get_mutual_coherence(B)\n    mu_equal = abs(mu_A - mu_B)  1e-12\n\n    lambda_min_A = get_min_eigenvalue_subgram(A, T_indices)\n    lambda_min_B = get_min_eigenvalue_subgram(B, T_indices)\n\n    results = []\n    for k, x in test_cases:\n        # Generate measurement vectors\n        y_A = A @ x\n        y_B = B @ x\n\n        # Run OMP\n        recovered_support_A = omp(A, y_A, k)\n        recovered_support_B = omp(B, y_B, k)\n\n        # Check for exact recovery against the specified target set S\n        omp_success_A = (recovered_support_A == S_target)\n        omp_success_B = (recovered_support_B == S_target)\n\n        # Aggregate results for the case\n        case_result = [\n            mu_equal,\n            round(lambda_min_A, 6),\n            round(lambda_min_B, 6),\n            omp_success_A,\n            omp_success_B\n        ]\n        results.append(case_result)\n        \n    # Final print statement in the exact required format.\n    # The string representation of list of lists is \"[...], [...]\"\n    # We join these with commas and wrap in an outer \"[]\"\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen that mutual coherence is not always a sufficient predictor of algorithm performance, we now turn to a more refined analytical tool: the cumulative coherence, $\\mu_1(s)$. This measure captures the aggregate interference from a group of atoms, rather than just the worst-case pairwise interaction, offering a sharper lens through which to analyze greedy algorithms like OMP and Compressive Sampling Matching Pursuit (CoSaMP) . In this exercise, you will construct adversarial scenarios where the standard coherence threshold is satisfied, yet recovery fails due to subtle effects like deterministic tie-breaking. This advanced practice illuminates the failure modes of greedy methods and demonstrates how more sophisticated metrics like cumulative coherence can predict and explain them.",
            "id": "3435266",
            "problem": "Consider a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ with unit-norm columns (a dictionary), a $k$-sparse signal $x \\in \\mathbb{R}^n$, and noiseless measurements $y = A x$. Two classical greedy algorithms for sparse recovery are Orthogonal Matching Pursuit (OMP) and Compressive Sampling Matching Pursuit (CoSaMP). The mutual coherence of $A$ is defined as $\\mu(A) = \\max_{i \\neq j} \\left| \\langle a_i, a_j \\rangle \\right|$, where $a_i$ denotes the $i$-th column of $A$. The cumulative coherence (also known as the Babel function) of order $s$ is defined as\n$$\n\\mu_1(s) = \\max_{j \\in \\{1,\\dots,n\\}} \\sum_{i \\in T_j(s)} \\left| \\langle a_j, a_i \\rangle \\right|,\n$$\nwhere $T_j(s)$ is the set of indices corresponding to the $s$ largest absolute inner products $\\left| \\langle a_j, a_i \\rangle \\right|$ with $i \\neq j$.\n\nIn the coherence-based framework, a well-tested sufficient condition for exact recovery by greedy selection (and for uniqueness of $k$-sparse representation) is that the mutual coherence satisfy a threshold of the form $\\mu(A)  \\frac{1}{2k-1}$, under unit-norm columns and noiseless measurements. However, cumulative coherence captures multi-index interactions and can be more informative about failure modes of greedy selection. Index selection errors can occur when a non-support column has correlation with the residual that is comparable to or larger than that of a true support column. These errors may be exacerbated by tie-breaking rules when correlations are equal in magnitude.\n\nTask:\n1. Starting from the core definitions of mutual coherence $\\mu(A)$ and cumulative coherence $\\mu_1(s)$, and the selection rules of Orthogonal Matching Pursuit (OMP) and Compressive Sampling Matching Pursuit (CoSaMP), analyze index selection behavior from first principles: the correlation between $y$ and any column $a_j$ is $c_j = \\langle a_j, y \\rangle = \\sum_{i \\in \\operatorname{supp}(x)} x_i \\langle a_j, a_i \\rangle$. Explain how bounds based on $\\mu(A)$ and $\\mu_1(s)$ constrain $|c_j|$ for $j$ in the support versus $j$ outside the support.\n2. Construct explicit dictionaries $A$ with prescribed mutual coherence $\\mu(A)$ and signals $x$ that:\n   - satisfy the coherence threshold $\\mu(A)  \\frac{1}{2k-1}$,\n   - yet can produce index selection ties that lead to adversarial failures in OMP or CoSaMP due to deterministic tie-breaking.\n3. Implement OMP (select one index per iteration via the largest absolute correlation, update via least squares on the selected support) and CoSaMP (identify $2k$ indices via largest absolute correlation with residual, merge, least squares on the merged set, prune to the $k$ largest coefficients, and update residual), both with deterministic tie-breaking by picking the smallest index on ties.\n4. For each constructed test case, compute:\n   - the mutual coherence $\\mu(A)$,\n   - the cumulative coherence $\\mu_1(k-1)$ and $\\mu_1(k)$,\n   - a boolean indicating whether OMP exactly recovers the true support,\n   - a boolean indicating whether CoSaMP exactly recovers the true support after a fixed small number of iterations (use two iterations).\n5. Provide a test suite comprising three cases that together exercise a general case, a boundary case with adversarial tie, and a near-boundary edge case:\n   - Case 1 (general, happy path): $k = 2$, $\\mu(A) = 0.25$, support indices are two columns with positive coefficients of equal magnitude.\n   - Case 2 (boundary, adversarial tie): $k = 2$, $\\mu(A) = \\frac{1}{3}$, support has coefficients with opposite signs causing equal-magnitude correlations for a true and a false index; dictionary columns are ordered to force deterministic tie-breaking to select a false index first.\n   - Case 3 (edge case near threshold): $k = 2$, $\\mu(A) = 0.32$, support has opposite signs but the strict mutual coherence condition is satisfied, preventing ties; both algorithms should recover.\n6. Your program must build the dictionaries explicitly with unit-norm columns to realize the prescribed pairwise inner products (including negative inner products where needed), run OMP and CoSaMP on the specified signals, compute $\\mu(A)$ and $\\mu_1(s)$, and evaluate success or failure. Angles are implicit in inner products; there are no physical units involved.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results of all test cases as a comma-separated list enclosed in square brackets, with no spaces. Each test case result must be a list of five entries in the order: \n- OMP success as a boolean,\n- CoSaMP success as a boolean,\n- $\\mu(A)$ as a float,\n- $\\mu_1(k-1)$ as a float,\n- $\\mu_1(k)$ as a float.\nFor example, the output must look like:\n\"[True,False,0.3333333333,0.3333333333,0.6666666667,True,True,0.25,0.25,0.5,True,True,0.32,0.32,0.64]\"\nbut with the actual values from your computations for the specified test suite.\n\nTest Suite Specification:\n- Case 1: $m = 4$, $n = 4$, $k = 2$, $\\mu(A) = 0.25$, signal $x$ has support on the first two columns with coefficients $x = [1, 1, 0, 0]^T$.\n- Case 2: $m = 4$, $n = 4$, $k = 2$, $\\mu(A) = \\frac{1}{3}$, signal $x$ has support on two columns with coefficients of opposite sign $x = [1, -1, 0, 0]^T$; dictionary columns are ordered to induce a deterministic tie in OMP and CoSaMP that selects a false index first.\n- Case 3: $m = 4$, $n = 4$, $k = 2$, $\\mu(A) = 0.32$, signal $x$ has support on two columns with coefficients of opposite sign $x = [1, -1, 0, 0]^T$.\n\nThe program must be self-contained and deterministic (no randomization), use the specified runtime environment, and print only the single line specified above as the final output.",
            "solution": "The problem is valid as it is a well-posed, scientifically grounded, and substantive exercise in the field of compressed sensing. It correctly references established concepts like mutual coherence, cumulative coherence, Orthogonal Matching Pursuit (OMP), and Compressive Sampling Matching Pursuit (CoSaMP), and asks for a rigorous analysis and implementation of these concepts, including the construction of specific test cases to probe the limits of theoretical recovery guarantees. All necessary parameters and definitions are provided.\n\nThe core of this problem lies in the analysis of index selection in greedy sparse recovery algorithms. The success of algorithms like OMP and CoSaMP depends on correctly identifying the support $S_0 = \\text{supp}(x)$ of a $k$-sparse signal $x \\in \\mathbb{R}^n$ from measurements $y = Ax \\in \\mathbb{R}^m$. The selection process is driven by the correlation of the dictionary columns $a_j$ with the current residual. In the first iteration, this is the correlation with the measurement vector $y$.\n\nLet $c_j$ be the correlation for column $j$:\n$$c_j = \\langle a_j, y \\rangle = \\left\\langle a_j, \\sum_{i \\in S_0} a_i x_i \\right\\rangle = \\sum_{i \\in S_0} x_i \\langle a_j, a_i \\rangle$$\nSince the columns $a_i$ of the dictionary $A$ have unit norm, $\\langle a_i, a_i \\rangle = 1$. We can analyze the magnitude of $c_j$ for columns inside and outside the true support $S_0$.\n\nFor a support column $j \\in S_0$:\n$$c_j = x_j \\langle a_j, a_j \\rangle + \\sum_{i \\in S_0, i \\neq j} x_i \\langle a_j, a_i \\rangle = x_j + \\sum_{i \\in S_0, i \\neq j} x_i \\langle a_j, a_i \\rangle$$\nThe second term represents interference from other support atoms. The magnitude $|c_j|$ is bounded by:\n$$|x_j| - \\left| \\sum_{i \\in S_0, i \\neq j} x_i \\langle a_j, a_i \\rangle \\right| \\leq |c_j| \\leq |x_j| + \\left| \\sum_{i \\in S_0, i \\neq j} x_i \\langle a_j, a_i \\rangle \\right|$$\nUsing the triangle inequality and the definitions of mutual coherence $\\mu(A)$ and cumulative coherence $\\mu_1(s)$, we can bound the interference term:\n$$\\left| \\sum_{i \\in S_0, i \\neq j} x_i \\langle a_j, a_i \\rangle \\right| \\leq \\sum_{i \\in S_0, i \\neq j} |x_i| |\\langle a_j, a_i \\rangle| \\leq x_{\\max} \\sum_{i \\in S_0, i \\neq j} |\\langle a_j, a_i \\rangle| \\leq x_{\\max} \\mu_1(k-1)$$\nwhere $x_{\\max} = \\max_{i \\in S_0} |x_i|$. A looser bound using mutual coherence is $(k-1) x_{\\max} \\mu(A)$. Thus, a lower bound on the correlation with a true support column is $|c_j| \\geq x_{\\min} - x_{\\max} \\mu_1(k-1)$, where $x_{\\min} = \\min_{i \\in S_0} |x_i|$.\n\nFor a non-support column $l \\notin S_0$:\n$$c_l = \\sum_{i \\in S_0} x_i \\langle a_l, a_i \\rangle$$\nThe magnitude is bounded by:\n$$|c_l| \\leq \\sum_{i \\in S_0} |x_i| |\\langle a_l, a_i \\rangle| \\leq x_{\\max} \\sum_{i \\in S_0} |\\langle a_l, a_i \\rangle| \\leq x_{\\max} \\mu_1(k)$$\nThe last inequality holds because $\\mu_1(k)$ is the maximum sum of $k$ absolute inner products over all columns. A looser bound is $k x_{\\max} \\mu(A)$.\n\nOMP succeeds in the first step if the largest correlation magnitude corresponds to a true support column. A sufficient condition is $\\min_{j \\in S_0} |c_j|  \\max_{l \\notin S_0} |c_l|$. Using the bounds above, this translates to $x_{\\min} - x_{\\max} \\mu_1(k-1)  x_{\\max} \\mu_1(k)$, or $\\frac{x_{\\min}}{x_{\\max}}  \\mu_1(k-1) + \\mu_1(k)$. If we simplify by assuming $|x_i|$ are equal and use the mutual coherence bound, we get the classic condition $1  (2k-1)\\mu(A)$, i.e., $\\mu(A)  \\frac{1}{2k-1}$.\n\nFailures can occur when this inequality is not strictly met. At the boundary $\\mu(A) = \\frac{1}{2k-1}$, it is possible to construct signals and dictionaries that cause ties, i.e., $|c_j| = |c_l|$ for some $j \\in S_0$ and $l \\notin S_0$. If a deterministic tie-breaking rule (e.g., choosing the smallest index) selects the incorrect index $l$, the algorithm may fail.\n\nThis insight guides the construction of the test cases. To create a dictionary $A$ with a prescribed Gram matrix $G = A^T A$, we ensure $G$ is symmetric positive semidefinite with ones on the diagonal. Then $A$ can be realized via Cholesky decomposition ($G=LL^T$, $A=L^T$), guaranteeing unit-norm columns and the desired inner products $\\langle a_i, a_j \\rangle = G_{ij}$.\n\n**Case 1 (General):** $k=2$, $\\mu(A)=0.25$. This satisfies $\\mu(A)  1/(2k-1) = 1/3$. With $x=[1,1,0,0]^T$, correlations for support columns are $|c_{0,1}| \\approx |1+0.25| = 1.25$, while for non-support columns $|c_{2,3}| \\approx |0.25+0.25|=0.5$. The support columns are clearly distinguished, and recovery is expected for both OMP and CoSaMP.\n\n**Case 2 (Adversarial Tie):** $k=2$, $\\mu(A)=1/3$. This is the boundary of the guarantee. We set support $S_0=\\{2,3\\}$ and signal $x=[0,0,1,-1]^T$. The dictionary is engineered so that $\\langle a_2, a_3 \\rangle=1/3$, $\\langle a_0, a_2 \\rangle=1/3$, and $\\langle a_0, a_3 \\rangle=-1/3$.\nCorrelation for support index $j=2$: $c_2 = \\langle a_2, a_2 \\rangle x_2 + \\langle a_2, a_3 \\rangle x_3 = 1\\cdot(1) + (1/3)\\cdot(-1) = 2/3$.\nCorrelation for non-support index $l=0$: $c_0 = \\langle a_0, a_2 \\rangle x_2 + \\langle a_0, a_3 \\rangle x_3 = (1/3)\\cdot(1) + (-1/3)\\cdot(-1) = 2/3$.\nA tie $|c_2|=|c_0|$ occurs. OMP's tie-breaking rule (smallest index) selects index $0$, which is outside the support, leading to failure. CoSaMP, by selecting a larger candidate set of $2k=4$ indices, includes the entire true support. Its least-squares and pruning steps are robust enough to correct the initial ambiguity, leading to successful recovery.\n\n**Case 3 (Near-Boundary):** $k=2$, $\\mu(A)=0.32$. This case is close to the boundary but satisfies the strict inequality $0.32  1/3$. With $x=[1,-1,0,0]^T$, the maximum correlation for a non-support column is bounded by $2\\mu = 0.64$, while the correlation for a support column is $|1-\\mu|=0.68$. Since $0.68  0.64$, no tie occurs, and both algorithms are expected to succeed.\n\nThe implementation will compute these outcomes, along with the specified coherence metrics. For the chosen Gram matrices, $\\mu_1(k-1) = \\mu_1(1)$, which is simply the largest single off-diagonal absolute entry, so $\\mu_1(1)=\\mu(A)$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky\n\ndef omp(A, y, k):\n    \"\"\"\n    Orthogonal Matching Pursuit (OMP) algorithm.\n    Selects one index per iteration and updates residual via least-squares projection.\n    \"\"\"\n    m, n = A.shape\n    support = []\n    residual = y.copy()\n    \n    for _ in range(k):\n        correlations = A.T @ residual\n        # np.argmax implements the tie-breaking rule: pick the smallest index\n        new_index = np.argmax(np.abs(correlations))\n        \n        # This check prevents adding duplicate indices, which can happen if the\n        # residual is in the span of the current support.\n        if new_index in support:\n            break\n            \n        support.append(new_index)\n        \n        # Solve the least squares problem on the current support\n        A_supp = A[:, sorted(support)]\n        x_hat = np.linalg.pinv(A_supp) @ y\n        \n        # Update the residual\n        residual = y - A_supp @ x_hat\n        \n    return sorted(support)\n\ndef cosamp(A, y, k, num_iter):\n    \"\"\"\n    Compressive Sampling Matching Pursuit (CoSaMP) algorithm.\n    Identifies 2k candidates, merges, prunes to k, and iterates.\n    \"\"\"\n    m, n = A.shape\n    support = []\n    x_k = np.zeros(n)\n    \n    for _ in range(num_iter):\n        residual = y - A @ x_k\n        correlations = A.T @ residual\n        \n        # 1. Identify 2k indices with largest correlation magnitudes\n        omega = np.argsort(-np.abs(correlations))[:2*k]\n        \n        # 2. Merge with previous support\n        T = sorted(list(set(support) | set(omega)))\n        \n        # 3. Estimate signal on the merged support via least squares\n        A_T = A[:, T]\n        b = np.linalg.pinv(A_T) @ y\n        \n        # Map coefficients 'b' back to the full n-dimensional space\n        x_T_full = np.zeros(n)\n        x_T_full[T] = b\n        \n        # 4. Prune to the k largest coefficients to get the new support\n        support = np.argsort(-np.abs(x_T_full))[:k]\n        \n        # 5. Update signal estimate for the next iteration\n        x_k = np.zeros(n)\n        x_k[support] = x_T_full[support]\n        \n    # After iterations, one final least-squares estimate on the final support\n    final_support = sorted([i for i, val in enumerate(x_k) if val != 0])\n    if not final_support:\n        return []\n\n    A_supp = A[:, final_support]\n    final_coeffs = np.linalg.pinv(A_supp) @ y\n    \n    # Return final support indices (filter out negligible coefficients)\n    final_indices = [idx for i, idx in enumerate(final_support) if np.abs(final_coeffs[i]) > 1e-9]\n    return sorted(final_indices)\n\ndef compute_coherence_metrics(G, k):\n    \"\"\"Computes mutual coherence mu and cumulative coherence mu_1(s) from a Gram matrix.\"\"\"\n    n = G.shape[0]\n    # Absolute values of off-diagonal elements\n    off_diag_abs = np.abs(G - np.identity(n))\n    \n    mu = np.max(off_diag_abs)\n    \n    mu1_sums_k_minus_1 = []\n    mu1_sums_k = []\n    for j in range(n):\n        # Correlations of column j with all other columns (i != j)\n        corrs_j = np.delete(off_diag_abs[j, :], j)\n        sorted_corrs = np.sort(corrs_j)[::-1]\n        \n        if k - 1 > 0 and k - 1  n:\n            mu1_sums_k_minus_1.append(np.sum(sorted_corrs[:k-1]))\n        if k > 0 and k  n:\n            mu1_sums_k.append(np.sum(sorted_corrs[:k]))\n\n    mu1_k_minus_1 = max(mu1_sums_k_minus_1) if mu1_sums_k_minus_1 else 0.0\n    mu1_k = max(mu1_sums_k) if mu1_sums_k else 0.0\n\n    return mu, mu1_k_minus_1, mu1_k\n\ndef solve():\n    # Case 1: General case, recovery expected\n    k1 = 2\n    G1 = np.full((4, 4), 0.25)\n    np.fill_diagonal(G1, 1)\n    x1 = np.array([1.0, 1.0, 0.0, 0.0])\n    true_support1 = {0, 1}\n    \n    # Case 2: Boundary case with adversarial tie\n    k2 = 2\n    G2 = np.array([\n        [1.0, 0.0, 1/3, -1/3],\n        [0.0, 1.0, 1/3,  1/3],\n        [1/3, 1/3, 1.0,  1/3],\n        [-1/3, 1/3, 1/3, 1.0]\n    ])\n    # Support is moved to {2, 3} and signal has opposite signs to create a tie\n    # with index 0, causing OMP failure due to smallest-index tie-breaking.\n    x2 = np.array([0.0, 0.0, 1.0, -1.0])\n    true_support2 = {2, 3}\n\n    # Case 3: Edge case near threshold, strict inequality holds\n    k3 = 2\n    G3 = np.full((4, 4), 0.32)\n    np.fill_diagonal(G3, 1)\n    x3 = np.array([1.0, -1.0, 0.0, 0.0])\n    true_support3 = {0, 1}\n\n    test_cases = [\n        (G1, k1, x1, true_support1),\n        (G2, k2, x2, true_support2),\n        (G3, k3, x3, true_support3),\n    ]\n\n    all_results = []\n    \n    for G, k, x, true_support in test_cases:\n        # Construct dictionary A from Gram matrix G\n        # The constructions ensure G is positive semidefinite\n        L = cholesky(G, lower=True)\n        A = L.T\n\n        # Compute measurements\n        y = A @ x\n        \n        # Run OMP and check for success\n        omp_support = omp(A, y, k)\n        omp_success = (set(omp_support) == true_support)\n        \n        # Run CoSaMP for 2 iterations and check for success\n        cosamp_support = cosamp(A, y, k, num_iter=2)\n        cosamp_success = (set(cosamp_support) == true_support)\n\n        # Compute coherence metrics\n        mu, mu1_km1, mu1_k = compute_coherence_metrics(G, k)\n        \n        # Store results for this case\n        all_results.extend([omp_success, cosamp_success, mu, mu1_km1, mu1_k])\n\n    # Format final output string as specified\n    formatted_results = []\n    for r in all_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        elif isinstance(r, (float, np.floating)):\n            formatted_results.append(f\"{r:.12f}\")\n        else:\n            formatted_results.append(str(r))\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}