{
    "hands_on_practices": [
        {
            "introduction": "在压缩感知领域，一个最核心的问题是：为了精确重建一个稀疏信号，我们最少需要多少次测量？这个问题探讨的不是某个特定算法的性能，而是由信息论决定的根本极限。本练习将引导你从第一性原理出发，通过严格的数学推导来回答这个问题。你将使用Fano不等式等信息论工具，为稀疏向量估计的极小极大风险（minimax risk）建立一个下界，从而揭示著名的 $k \\ln(p/k)$ 样本复杂度是如何产生的。",
            "id": "3474986",
            "problem": "考虑带有高斯噪声的高维线性回归模型。设 $X \\in \\mathbb{R}^{n \\times p}$ 为一个固定设计矩阵，其列被归一化，使得对于每个列索引 $j \\in \\{1,\\dots,p\\}$，都有 $(1/n)\\|X_{j}\\|_{2}^{2} = 1$。对于一个未知的参数向量 $\\theta^{\\star} \\in \\mathbb{R}^{p}$，观测值由 $Y = X \\theta^{\\star} + \\varepsilon$ 给出，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 且 $\\sigma > 0$ 是已知的。假设 $\\theta^{\\star}$ 是 $k$-稀疏的，即 $\\|\\theta^{\\star}\\|_{0} \\leq k$，其中 $k \\in \\{1,\\dots,p\\}$。\n\n定义参数空间 $\\Theta_{k} = \\{\\theta \\in \\mathbb{R}^{p} : \\|\\theta\\|_{0} \\leq k\\}$。考虑估计量 $\\widehat{\\theta}(Y)$ 和极小极大均方 $\\ell_{2}$ 估计风险\n$$\nR^{\\star} = \\inf_{\\widehat{\\theta}} \\sup_{\\theta^{\\star} \\in \\Theta_{k}} \\mathbb{E}\\big[\\|\\widehat{\\theta}(Y) - \\theta^{\\star}\\|_{2}^{2}\\big].\n$$\n仅使用信息论和高维统计学的基本原理，即 (i) 高斯移位模型的 Kullback–Leibler 散度，(ii) 信息论中的 Fano 不等式，(iii) 用于离散子集选择的标准 packing 论证，以及 (iv) 以下定义。\n\n对于稀疏度水平 $s \\in \\{1,\\dots,p\\}$，定义受限最小和最大特征值常数\n$$\n\\kappa_{s} = \\inf_{\\substack{u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u\\|_{0} \\leq s}} \\frac{1}{n} \\frac{\\|X u\\|_{2}^{2}}{\\|u\\|_{2}^{2}}, \\qquad \\Lambda_{s} = \\sup_{\\substack{u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u\\|_{0} \\leq s}} \\frac{1}{n} \\frac{\\|X u\\|_{2}^{2}}{\\|u\\|_{2}^{2}},\n$$\n以及相容性常数（在最小绝对收缩和选择算子 (LASSO) 的锥受限分析意义下）\n$$\n\\phi(s) = \\inf_{\\substack{S \\subset \\{1,\\dots,p\\},\\, |S| \\leq s \\\\ u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u_{S^{c}}\\|_{1} \\leq 3 \\|u_{S}\\|_{1}}} \\frac{\\sqrt{n}\\,\\|X u\\|_{2}}{\\|u_{S}\\|_{1}/\\sqrt{s}}.\n$$\n\n从上述基本事实出发（不引入任何快捷公式），推导在 $p \\gg k$ 和 $k \\ln(p/k) \\ll n$ 的情况下，$\\Theta_{k}$ 上的极小极大风险 $R^{\\star}$ 的主阶渐近表达式，并将此速率明确地与神谕风险（即估计量已知 $\\theta^{\\star}$ 的支撑集但不知其符号或大小的情况）以及相容性常数和受限特征值常数在可达上界中的作用联系起来。\n\n你的最终答案必须是关于 $n$, $p$, $k$, $\\sigma$ 和一个适当的受限设计常数的 $R^{\\star}$ 主阶极小极大速率的单一闭式解析表达式。最终答案中不允许出现不等式。不要包含任何单位。如果引入任何渐近近似，忽略绝对常数，并仅以闭式表达式的形式给出主导项。",
            "solution": "该问题要求在高维稀疏线性回归模型中，极小极大均方 $\\ell_2$ 估计风险的主阶渐近表达式。推导必须从第一性原理出发。极小极大风险定义为\n$$\nR^{\\star} = \\inf_{\\widehat{\\theta}} \\sup_{\\theta^{\\star} \\in \\Theta_{k}} \\mathbb{E}\\big[\\|\\widehat{\\theta}(Y) - \\theta^{\\star}\\|_{2}^{2}\\big],\n$$\n其中 $\\Theta_{k} = \\{\\theta \\in \\mathbb{R}^{p} : \\|\\theta\\|_{0} \\leq k\\}$。我们将通过使用信息论方法推导一个下界来确定 $R^\\star$ 的速率，然后论证该下界是可达的，从而确定极小极大速率。\n\n首先，我们使用 Fano 不等式推导 $R^{\\star}$ 的一个下界。该方法需要构造参数空间 $\\Theta_k$ 的一个有限子集，记为 $\\Theta_0 = \\{\\theta_1, \\dots, \\theta_M\\}$，使得其元素在估计度量（$\\ell_2$-范数）下是良好分离的，同时它们所诱导的统计模型 $P_i = \\mathcal{N}(X\\theta_i, \\sigma^2 I_n)$ 难以区分。\n\n令 $\\inf_{\\widehat{\\theta}} \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}[\\|\\widehat{\\theta} - \\theta\\|_2^2]$ 为子集 $\\Theta_0$ 上的极小极大风险。Fano 不等式的一个标准推论指出，如果我们能构造一个集合 $\\Theta_0 = \\{\\theta_1, \\dots, \\theta_M\\} \\subset \\Theta_k$ 满足：\n1. 对所有 $i \\neq j$，$\\|\\theta_i - \\theta_j\\|_2 \\ge \\delta$。\n2. 任意两个对应分布 $P_i$ 和 $P_j$ 之间的 Kullback-Leibler (KL) 散度有界，即 $D_{KL}(P_i \\| P_j) \\le \\alpha \\ln(M)$，其中常数 $\\alpha < 1$。\n\n那么，$\\Theta_k$ 上的极小极大风险有下界\n$$\nR^{\\star} \\ge \\inf_{\\widehat{\\theta}} \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}[\\|\\widehat{\\theta} - \\theta\\|_2^2] \\ge \\frac{\\delta^2}{2}\\left(1 - \\alpha - \\frac{\\ln 2}{\\ln M}\\right).\n$$\n\n我们现在构造这样一个集合 $\\Theta_0$。这是一个标准的 packing 论证。令 $\\mathcal{W}$ 为二元向量 $\\{0, 1\\}^p$ 的一个子集，其中每个 $\\omega \\in \\mathcal{W}$ 的汉明权重为 $\\|\\omega\\|_0=k$。利用编码理论中的 Varshamov-Gilbert 界，可以构造这样一个集合 $\\mathcal{W}$，其中任意两个不同元素 $\\omega_i, \\omega_j \\in \\mathcal{W}$ 之间的汉明距离至少为 $d_H(\\omega_i, \\omega_j) \\ge k$。在 $p \\gg k$ 的情况下，该集合的大小 $M=|\\mathcal{W}|$ 保证满足 $\\ln(M) \\ge c_1 k \\ln(\\frac{p}{k})$，其中某个常数 $c_1 > 0$。\n\n我们基于这个 packing 集合 $\\mathcal{W}$ 来定义我们的参数子集 $\\Theta_0$，即 $\\Theta_0 = \\{a \\cdot \\omega : \\omega \\in \\mathcal{W}\\}$，其中 $a$ 是一个待选的标量幅值。每个 $\\theta \\in \\Theta_0$ 都是 $k$-稀疏的，所以 $\\Theta_0 \\subset \\Theta_k$。\n\n接下来，我们确定分离度 $\\delta$。对于 $\\Theta_0$ 中任意不同的 $\\theta_i = a \\omega_i$ 和 $\\theta_j = a \\omega_j$：\n$$\n\\|\\theta_i - \\theta_j\\|_2^2 = a^2 \\|\\omega_i - \\omega_j\\|_2^2 = a^2 d_H(\\omega_i, \\omega_j) \\ge a^2 k.\n$$\n因此我们可以设置 $\\delta^2 = a^2 k$。\n\n现在，我们对 KL 散度进行界定。对于两个多元高斯分布 $P_i = \\mathcal{N}(\\mu_i, \\sigma^2 I_n)$ 和 $P_j = \\mathcal{N}(\\mu_j, \\sigma^2 I_n)$，KL 散度为 $D_{KL}(P_i \\| P_j) = \\frac{1}{2\\sigma^2} \\|\\mu_i - \\mu_j\\|_2^2$。在我们的情况下，$\\mu_i = X\\theta_i$，所以：\n$$\nD_{KL}(P_i \\| P_j) = \\frac{1}{2\\sigma^2} \\|X(\\theta_i - \\theta_j)\\|_2^2 = \\frac{a^2}{2\\sigma^2} \\|X(\\omega_i - \\omega_j)\\|_2^2.\n$$\n令 $u = \\omega_i - \\omega_j$。向量 $u$ 的元素来自 $\\{-1, 0, 1\\}$。其非零元素的数量为 $\\|u\\|_0 = d_H(\\omega_i, \\omega_j)$。由于 $\\|\\omega_i\\|_0 = \\|\\omega_j\\|_0 = k$，$u$ 的稀疏度在范围 $k \\le \\|u\\|_0 \\le 2k$ 内。我们可以使用受限最大特征值常数 $\\Lambda_s$ 来获得一个上界：\n$$\n\\|Xu\\|_2^2 = n \\cdot \\frac{\\|Xu\\|_2^2}{n \\|u\\|_2^2} \\cdot \\|u\\|_2^2 \\le n \\cdot \\Lambda_{\\|u\\|_0} \\cdot \\|u\\|_2^2 \\le n \\cdot \\Lambda_{2k} \\cdot \\|u\\|_2^2.\n$$\n由于 $\\|u\\|_2^2 = d_H(\\omega_i, \\omega_j) \\le 2k$，我们有：\n$$\nD_{KL}(P_i \\| P_j) \\le \\frac{a^2}{2\\sigma^2} n \\Lambda_{2k} (2k) = \\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2}.\n$$\n为了应用 Fano 不等式，我们需要这个 KL 散度是组合熵 $\\ln(M)$ 的一小部分。我们要求 $\\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2} \\le \\frac{1}{2} \\ln(M)$。使用大小界 $\\ln(M) \\ge c_1 k \\ln(p/k)$，我们需要：\n$$\n\\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2} \\le \\frac{c_1}{2} k \\ln(p/k).\n$$\n这约束了幅值 $a$。我们选择尽可能大的 $a$ 来满足这个条件，因此我们设定 $a^2$ 与上界成正比：\n$$\na^2 = c_2 \\frac{\\sigma^2 \\ln(p/k)}{n \\Lambda_{2k}},\n$$\n其中 $c_2$ 是某个足够小的常数。将其代回到 $\\delta^2 = a^2 k$ 的表达式中：\n$$\n\\delta^2 = c_2 \\frac{\\sigma^2 k \\ln(p/k)}{n \\Lambda_{2k}}.\n$$\n将此代入 Fano 不等式，对于合适的常数，项 $(1 - \\alpha - \\frac{\\ln 2}{\\ln M})$ 是一个正常数。因此，极小极大风险的下界为：\n$$\nR^{\\star} \\gtrsim \\delta^2 \\asymp \\frac{\\sigma^2 k \\ln(p/k)}{n \\Lambda_{2k}}.\n$$\n这就确立了极小极大速率的下界。\n\n论证的第二部分是证明这个速率是可达的。高维统计学中的一个著名结果断言，在设计矩阵 $X$ 的某些条件下，这个速率可以由一个实用的估计量达到，例如 LASSO (最小绝对收缩和选择算子) 或 Dantzig 选择器。对此类估计量的分析表明，它们的均方误差有一个同阶量的上界。例如，LASSO 估计量的性能由设计矩阵上的条件保证，例如相容性常数 $\\phi(s)$ 或像 $\\kappa_s > 0$ 这样的受限特征值条件。LASSO 风险的一个典型上界形式为：\n$$\n\\sup_{\\theta^{\\star} \\in \\Theta_k} \\mathbb{E}[\\|\\widehat{\\theta}_{LASSO} - \\theta^{\\star}\\|_2^2] \\lesssim \\frac{\\sigma^2 k \\ln(p/k)}{n \\kappa_{2k}^2}.\n$$\n对于一个“行为良好”的设计矩阵，其受限特征值必须有界于零和无穷大之外，即对于所有相关的 $s$，有 $0 < c_L \\le \\kappa_s \\le \\Lambda_s \\le c_U < \\infty$。在这种情况下，下界（与 $1/\\Lambda_{2k}$ 成比例）和上界（与 $1/\\kappa_{2k}^2$ 成比例）在关于 $n, p, k$ 和 $\\sigma$ 的尺度上是匹配的。忽略绝对数值常数并假设这种行为良好的设计，速率是匹配的。因此，推导出的下界是紧的，并代表了该估计问题的基本统计极限。\n\n这个极小极大速率可以与“神谕”风险相比较。一个神谕估计量，如果知道 $\\theta^\\star$ 的真实支撑集 $S$（其中 $|S|=k$），会对相应的列 $X_S$ 进行最小二乘拟合。其风险为 $\\sigma^2 \\text{Tr}((X_S^T X_S)^{-1})$，其阶为 $\\frac{\\sigma^2 k}{n \\kappa_k}$。极小极大速率包含一个额外的因子 $\\ln(p/k)$，这代表了因不知道支撑集而不得不在 $\\binom{p}{k}$ 种可能性中搜索所付出的不可避免的统计代价。\n\n下界是最基本的量，因为它为任何可能的估计量提供了一个最终极限。出现在此界中的常数 $\\Lambda_{2k}$ 表征了由固定设计矩阵 $X$ 施加的“最坏情况”下的难度。因此，极小极大速率的主阶表达式由这个下界确定。",
            "answer": "$$\\boxed{\\frac{\\sigma^{2} k \\ln(p/k)}{n \\Lambda_{2k}}}$$"
        },
        {
            "introduction": "理论分析通常假设测量是具有无限精度的实数，然而在实际系统中，所有测量结果都必须经过量化，用有限的比特来表示。这个过程引入了新的权衡和约束。本练习将带你跳出理想化的模型，思考在固定的总比特预算（total bit budget）$B$ 下如何优化传感策略。你将面对一个关键的决策问题：是进行多次粗略的测量（低比特深度 $b$，高测量次数 $m$），还是进行少数几次精确的测量（高比特深度 $b$，低测量次数 $m$）？通过分析不同量化方案的优劣，你将学会如何在信息论极限和实际工程约束之间进行权衡，这是设计真实压缩感知系统时的一项核心技能。",
            "id": "3474937",
            "problem": "考虑一个标准的压缩感知模型，其中有一个未知的 $k$-稀疏向量 $x \\in \\mathbb{R}^{n}$，一个测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$（其条目为独立同分布的标准正态分布），以及线性测量值 $y = A x \\in \\mathbb{R}^{m}$。每个测量值在传输/存储前都经过量化。在固定的总比特预算 $B = m b$ 下，比较三种标量量化方案：(i) 1比特量化，记录一个阈值 $\\tau_i$ 下的 $\\operatorname{sign}(y_{i} - \\tau_{i})$；(ii) 无抖动的 $b$-比特均匀标量量化；以及 (iii) $b$-比特减性抖动均匀标量量化，其中在量化前加入独立抖动 $d_{i} \\sim \\mathsf{U}([-\\Delta/2, \\Delta/2])$ 并在解码器处减去，从而产生一个与信号无关的有效加性量化噪声模型。\n\n全文假设动态范围受到控制，以至于量化器饱和可以忽略不计，并且解码使用与测量和量化模型一致的计算上可行的过程（例如，在无噪声或加性噪声机制下的凸规划或统计最优估计器）。所考虑的目标任务是：错误概率最多为一个固定常数的精确支撑集恢复，以及在加性量化噪声近似下的均方误差估计。所有的信息论极限都应从第一性原理（如互信息和Fano不等式）推导，而性能界则应从经过充分检验的高维估计事实中导出（例如，在次高斯设计和方差为 $\\sigma^{2}$ 的加性噪声下，合适的稀疏估计器的平方误差尺度约为 $(k \\log(n/k)/m)\\sigma^{2}$）。\n\n在这些假设和基础之上，当总预算 $B = m b$ 固定时，以下哪些陈述正确地描述了三种量化方案之间的样本复杂度和比特分配权衡？选择所有适用的选项。\n\nA. 对于错误概率有界且不为 $1$ 的精确支撑集恢复，存在一个信息论下界 $B \\geq C\\, k \\log(n/k)$（在绝对常数 $C$ 的意义下），该下界对1比特、多比特和抖动量化均成立。此外，使用随机高斯测量和合适的解码器，通过使用常数 $b$（包括 $b=1$），可以在 $m = \\Theta(k \\log(n/k))$ 的条件下实现支撑集恢复，因此在固定预算 $B$ 下，可行性在阶数上简化为 $B \\gtrsim k \\log(n/k)$，而与量化方案无关。\n\nB. 对于减性抖动 $b$-比特量化下的均方误差估计，一个量化噪声方差为 $\\sigma_{q}^{2} \\asymp 2^{-2b}$ 的加性噪声模型导致 $E\\|\\hat{x}-x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{m}\\sigma_{q}^{2}$。在固定预算 $B = m b$ 下，在约束 $m \\gtrsim k \\log(n/k)$ 的条件下对 $b$ 最小化此界，得到的选择是 $b^{\\star}$ 在 $b^{\\star} \\leq B/(c\\, k \\log(n/k))$ 允许的范围内取最大值。因此，在紧张预算机制 $B \\asymp k \\log(n/k)$ 下，倾向于粗量化（常数 $b$）和更多的测量；而在高预算机制 $B \\gg k \\log(n/k)$ 下，增加 $b$ 是有益的。\n\nC. 纯粹的1比特量化，若无抖动或阈值自适应，即使在固定动态范围内 $m \\to \\infty$，也无法在幅度估计中实现消失的归一化均方误差，因为符号信息丢弃了幅度信息；然而，在标准的随机设计下，它可以用 $m = \\Theta(k \\log(n/k))$ 实现精确的支撑集恢复。\n\nD. 与非抖动多比特量化相比，抖动量化严格地降低了支撑集恢复所需的比特预算阶数；也就是说，存在一个常数 $c < 1$，使得对于所有问题规模，为达到相同的支撑集恢复保证，都有 $B_{\\text{dither}} \\leq c\\, B_{\\text{nodither}}$。\n\nE. 在固定的 $B$ 下，为每次测量分配更多比特必然会减少任何恢复任务所需的最小测量次数，而与稀疏度 $k$ 和环境维度 $n$ 无关。",
            "solution": "我们使用信息论极限和高维估计原理来分析每个陈述。\n\n首先，我们为支撑集恢复建立信息论基础。设支撑集 $S \\subset [n]$ 是非零条目的索引集，且 $|S| = k$。共有 $N = \\binom{n}{k}$ 种可能的支撑集。设 $Z$ 表示量化后的测量值（长度为 $B$ 的比特串）。根据数据处理不等式，互信息 $I(S; Z)$ 捕捉了 $Z$ 中关于 $S$ 的信息。对于任何每个测量最多产生 $b$ 比特的方案，我们有 $H(Z) \\leq B$，因此 $I(S; Z) \\leq H(Z) \\leq B$。根据Fano不等式，对于任何错误概率为 $P_{e} = \\mathbb{P}(\\hat{S} \\neq S)$ 的估计器 $\\hat{S}(Z)$，\n$$\nP_{e} \\geq 1 - \\frac{I(S; Z) + \\log 2}{\\log N}.\n$$\n为确保 $P_{e} \\leq \\delta$（对于某个常数 $\\delta < 1$），只需 $I(S; Z) \\gtrsim \\log N$ 即可。由于 $\\log N \\asymp k \\log(n/k)$，因此任何方案都必须满足\n$$\nB \\geq C\\, k \\log(n/k)\n$$\n对于某个与 $b$ 无关的绝对常数 $C$。这个下界与具体方案无关。\n\n在支撑集恢复的可达性方面，在无噪声或有界噪声设置下，使用随机高斯矩阵 $A$，存在计算上可行的过程（例如，基于 $\\ell_{1}$ 的方法或阈值相关性检验），能够以\n$$\nm = \\Theta(k \\log(n/k))\n$$\n次测量（在常数因子内）恢复支撑集，前提是每次测量的信息是非退化的（常数 $b$ 足以）。因此，在固定预算 $B$ 下，可行性简化为确保 $m$ 的阶数为 $k \\log(n/k)$，这反过来要求 $B \\gtrsim k \\log(n/k)$，在阶数尺度上与具体的量化方案无关。\n\n接下来，我们在加性量化噪声近似下分析均方误差（MSE）估计。设 $\\sigma_{q}^{2}$ 表示每次测量的量化噪声方差。对于减性抖动均匀量化，有效噪声独立于 $y$ 并且在 $[-\\Delta/2, \\Delta/2]$ 上均匀分布，所以\n$$\n\\sigma_{q}^{2} = \\frac{\\Delta^{2}}{12}.\n$$\n假设动态范围控制意味着 $\\Delta$ 与 $2^{b}$ 成反比，我们有 $\\sigma_{q}^{2} \\asymp 2^{-2b}$。对于高维稀疏估计器（例如，约束最小二乘或最小绝对收缩和选择算子），一个经过充分检验的事实是\n$$\nE\\|\\hat{x} - x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{m}\\, \\sigma^{2}\n$$\n当每次测量的加性噪声方差为 $\\sigma^{2}$ 且设计是次高斯时。令 $\\sigma^{2} = \\sigma_{q}^{2}$ 可得\n$$\nE\\|\\hat{x} - x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{m}\\, \\sigma_{q}^{2} \\asymp \\frac{k \\log(n/k)}{m}\\, 2^{-2b}.\n$$\n在固定的比特预算 $B = m b$ 下，我们有 $m = B/b$，因此\n$$\nE\\|\\hat{x} - x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{B}\\, b\\, 2^{-2b}.\n$$\n函数 $f(b) = b\\, 2^{-2b}$ 对于整数 $b \\geq 1$ 是严格递减的（因为指数衰减主导了线性因子）。因此，在没有其他约束的情况下，增加 $b$ 会减小 MSE 的界。然而，压缩感知对可辨识性和稳定性施加了最小测量要求：例如，对于支撑集恢复或稳定估计，通常需要\n$$\nm \\gtrsim k \\log(n/k),\n$$\n在固定 $B$ 的情况下，这转化为\n$$\n\\frac{B}{b} \\gtrsim k \\log(n/k) \\quad \\Longleftrightarrow \\quad b \\lesssim \\frac{B}{k \\log(n/k)}.\n$$\n因此，在固定 $B$ 的情况下，为最小化 MSE 并满足可行性，最优的 $b$ 是在约束 $b \\leq B/(c\\, k \\log(n/k))$（对于某个合适的常数 $c$）内“尽可能大”。在紧张预算机制 $B \\asymp k \\log(n/k)$ 下，这迫使 $b$ 为一个常数（通常 $b \\approx 1$ 或 $b \\approx 2$），使得采用更多测量的粗量化成为最优选择；而在高预算机制 $B \\gg k \\log(n/k)$ 下，更大的 $b$ 是有益的，因为 $m$ 仍高于可辨识性阈值，并且 $2^{-2b}$ 迅速减小。\n\n我们现在评估每个选项：\n\nA. 该陈述反映了 Fano 下界（$B \\gtrsim k \\log(n/k)$），该下界与方案无关，以及通过随机设计和常数 $b$ 实现的可达性，这意味着 $m = \\Theta(k \\log(n/k))$。在固定 $B$ 的情况下，可行性取决于 $B$ 是否达到阶数阈值，而与量化是1比特、多比特还是抖动无关。结论：正确。\n\nB. 该陈述应用了加性噪声 MSE 界，其中减性抖动量化的 $\\sigma_{q}^{2} \\asymp 2^{-2b}$，然后在固定 $B$ 下施加压缩感知约束 $m \\gtrsim k \\log(n/k)$。它正确地得出结论，最优的 $b$ 是由 $b \\leq B/(c\\, k \\log(n/k))$ “允许的最大值”，这导致在紧张预算机制下采用粗量化，在高预算机制下采用更大的 $b$。结论：正确。\n\nC. 无抖动或阈值自适应的1比特量化仅记录符号，这从根本上丢失了幅度信息，即使在固定动态范围内 $m \\to \\infty$，也无法实现幅度估计的 MSE 趋于零。然而，标准的1比特压缩感知使用随机超平面，可以用 $m = \\Theta(k \\log(n/k))$ 实现精确的支撑集恢复。结论：正确。\n\nD. 抖动通过使量化噪声独立来提高建模精度，并且通常能改善常数，但它不改变支撑集恢复所需的比特预算的阶数，该阶数由所有方案都遵循的 $B \\gtrsim k \\log(n/k)$ 决定。声称对于所有问题规模都存在一个统一的严格缩减因子 $c < 1$ 的说法，并没有得到信息论极限的支持。结论：不正确。\n\nE. 在固定 $B$ 下增加 $b$ 会减少 $m = B/b$。对于任何恢复任务，都存在一个最小的 $m$（例如，对于支撑集恢复，$m \\gtrsim k \\log(n/k)$），这个最小值无法通过增加 $b$ 来绕过。因此，为每次测量分配更多比特并不“必然”减少所需的最小测量次数；实际上，它可能违反最小测量要求。结论：不正确。\n\n因此，正确的选项是 A、B 和 C。",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}