{
    "hands_on_practices": [
        {
            "introduction": "Understanding the LASSO solution path begins with its starting point. This exercise tackles a fundamental question: for a given dataset, what is the largest value of the regularization parameter $\\lambda$ that results in an all-zero coefficient vector? By working through the Karush-Kuhn-Tucker (KKT) optimality conditions, you will calculate this critical threshold, $\\lambda_{\\max}$, and see how it determines which variable is the first to enter the model as $\\lambda$ decreases. ",
            "id": "3484726",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator defined by the optimization problem\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\},\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $y \\in \\mathbb{R}^{n}$ is the response vector, $n$ is the sample size, $p$ is the number of features, and $\\lambda \\geq 0$ is the regularization parameter. Let $n = 6$ and $p = 4$, and suppose\n$$\nX = \\begin{pmatrix}\n1  0  2  1 \\\\\n0  1  2  -2 \\\\\n2  -1  0  1 \\\\\n0  2  -1  1 \\\\\n-1  0  1  -1 \\\\\n3  1  0  0\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n3 \\\\ -1 \\\\ 4 \\\\ 0 \\\\ 2 \\\\ 5\n\\end{pmatrix}.\n$$\nStarting from the first-order (Karush–Kuhn–Tucker) optimality conditions for convex optimization and the subgradient characterization of the $\\ell_{1}$ norm, determine the smallest regularization level $\\lambda$ for which the zero vector $\\hat{\\beta}(\\lambda) = 0$ satisfies the LASSO optimality conditions. Then, using this value, identify the equicorrelation set and explain how the LASSO path is initialized at $\\hat{\\beta} = 0$ with the equicorrelation set equal to the indices that achieve the largest absolute rescaled correlations, and with signs determined accordingly.\n\nCompute the requested $\\lambda$ using the given $X$ and $y$. Provide the equicorrelation set explanation in words, but report only the numerical value of $\\lambda$ as your final answer. No rounding is necessary.",
            "solution": "The problem asks for the smallest regularization parameter $\\lambda$ for which the zero vector, $\\hat{\\beta} = 0$, is a solution to the LASSO optimization problem. The LASSO estimator is defined as:\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ F(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\}\n$$\nThis is a convex optimization problem. The objective function $F(\\beta)$ is the sum of a differentiable convex function, $L(\\beta) = \\frac{1}{2n} \\| y - X \\beta \\|_{2}^{2}$, and a non-differentiable convex function, $R(\\beta) = \\lambda \\|\\beta\\|_{1}$. A vector $\\hat{\\beta}$ is a minimizer of $F(\\beta)$ if and only if the zero vector is in the subdifferential of $F$ at $\\hat{\\beta}$. The subdifferential of the sum is the sum of the subdifferentials:\n$$\n\\partial F(\\hat{\\beta}) = \\nabla L(\\hat{\\beta}) + \\partial R(\\hat{\\beta})\n$$\nThe first-order optimality condition, also known as the Karush–Kuhn–Tucker (KKT) condition for this problem, is $0 \\in \\partial F(\\hat{\\beta})$.\n\nThe gradient of the least-squares loss term $L(\\beta)$ is:\n$$\n\\nabla L(\\beta) = \\frac{1}{n} X^T (X \\beta - y) = -\\frac{1}{n} X^T (y - X \\beta)\n$$\nThe subdifferential of the $\\ell_1$-norm regularization term $R(\\beta)$ is $\\partial R(\\beta) = \\lambda \\, \\partial \\|\\beta\\|_{1}$. The subdifferential of the $\\ell_1$-norm at a point $\\beta$ is the set of all subgradients $g \\in \\mathbb{R}^p$ such that:\n$$\ng_j = \\begin{cases} \\text{sign}(\\beta_j)  \\text{if } \\beta_j \\neq 0 \\\\ \\gamma_j \\in [-1, 1]  \\text{if } \\beta_j = 0 \\end{cases}\n$$\nfor $j=1, \\dots, p$.\n\nThe optimality condition $0 \\in \\nabla L(\\hat{\\beta}) + \\lambda \\, \\partial \\|\\hat{\\beta}\\|_{1}$ thus implies the existence of a subgradient vector $g \\in \\partial \\|\\hat{\\beta}\\|_{1}$ such that:\n$$\n-\\frac{1}{n} X^T (y - X \\hat{\\beta}) + \\lambda g = 0\n$$\n$$\n\\frac{1}{n} X^T (y - X \\hat{\\beta}) = \\lambda g\n$$\nWe are interested in the case where the solution is the zero vector, $\\hat{\\beta} = 0$. Substituting this into the optimality condition yields:\n$$\n\\frac{1}{n} X^T (y - X \\cdot 0) = \\lambda g\n$$\n$$\n\\frac{1}{n} X^T y = \\lambda g\n$$\nFor $\\hat{\\beta} = 0$, all components are zero, so the condition on the subgradient $g$ is that $g_j \\in [-1, 1]$ for all $j=1, \\dots, p$. This is equivalent to stating that the infinity norm of the subgradient vector is bounded by one: $\\|g\\|_{\\infty} \\le 1$.\n\nLet $c = \\frac{1}{n} X^T y$. The condition is $c = \\lambda g$. This implies that for each component $j$:\n$$\nc_j = \\lambda g_j\n$$\nSince $|g_j| \\le 1$, this requires $|c_j| \\le \\lambda$ for all $j=1, \\dots, p$. This can be written compactly using the infinity norm:\n$$\n\\|c\\|_{\\infty} = \\max_{j} |c_j| \\le \\lambda\n$$\nThe problem asks for the smallest value of $\\lambda \\ge 0$ for which $\\hat{\\beta} = 0$ is a valid solution. This corresponds to the smallest $\\lambda$ satisfying the inequality $\\|c\\|_{\\infty} \\le \\lambda$. This minimum value is clearly:\n$$\n\\lambda_{\\text{min}} = \\|c\\|_{\\infty} = \\left\\| \\frac{1}{n} X^T y \\right\\|_{\\infty} = \\frac{1}{n} \\max_{j=1,\\dots,p} |(X^T y)_j|\n$$\nFor any $\\lambda  \\lambda_{\\text{min}}$, at least one component $|c_j|$ will be greater than $\\lambda$, making it impossible to find a valid subgradient $g$ (since $|g_j| = |c_j|/\\lambda  1$). Thus, for $\\lambda  \\lambda_{\\text{min}}$, the solution cannot be $\\hat{\\beta} = 0$.\n\nNow, we compute this value using the given data:\n$n = 6$, $p = 4$.\n$$\nX = \\begin{pmatrix}\n1  0  2  1 \\\\\n0  1  2  -2 \\\\\n2  -1  0  1 \\\\\n0  2  -1  1 \\\\\n-1  0  1  -1 \\\\\n3  1  0  0\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n3 \\\\ -1 \\\\ 4 \\\\ 0 \\\\ 2 \\\\ 5\n\\end{pmatrix}\n$$\nFirst, we compute the product $X^T y$:\n$$\nX^T y = \\begin{pmatrix}\n1  0  2  0  -1  3 \\\\\n0  1  -1  2  0  1 \\\\\n2  2  0  -1  1  0 \\\\\n1  -2  1  1  -1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\ -1 \\\\ 4 \\\\ 0 \\\\ 2 \\\\ 5\n\\end{pmatrix}\n= \\begin{pmatrix}\n1(3) + 0(-1) + 2(4) + 0(0) - 1(2) + 3(5) \\\\\n0(3) + 1(-1) - 1(4) + 2(0) + 0(2) + 1(5) \\\\\n2(3) + 2(-1) + 0(4) - 1(0) + 1(2) + 0(5) \\\\\n1(3) - 2(-1) + 1(4) + 1(0) - 1(2) + 0(5)\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 + 0 + 8 - 2 + 15 \\\\\n0 - 1 - 4 + 0 + 0 + 5 \\\\\n6 - 2 + 0 - 0 + 2 + 0 \\\\\n3 + 2 + 4 + 0 - 2 + 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n24 \\\\ 0 \\\\ 6 \\\\ 7\n\\end{pmatrix}\n$$\nNext, we compute the vector $c = \\frac{1}{n} X^T y$:\n$$\nc = \\frac{1}{6} \\begin{pmatrix} 24 \\\\ 0 \\\\ 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 0 \\\\ 1 \\\\ 7/6 \\end{pmatrix}\n$$\nFinally, we find the smallest $\\lambda$ by taking the infinity norm of $c$:\n$$\n\\lambda = \\|c\\|_{\\infty} = \\max\\left(|4|, |0|, |1|, \\left|\\frac{7}{6}\\right|\\right) = \\max\\left(4, 0, 1, \\frac{7}{6}\\right) = 4\n$$\nThe smallest regularization level is therefore $\\lambda = 4$.\n\nFor this value of $\\lambda=4$, the optimality conditions for $\\hat{\\beta}=0$ are satisfied. The equicorrelation set consists of the indices $j$ for which the correlation is maximal in absolute value, i.e., $|c_j| = \\lambda$. In this case, $|c_1| = 4 = \\lambda$, while $|c_j|  \\lambda$ for $j \\in \\{2,3,4\\}$. Thus, the equicorrelation set is $\\{1\\}$. This means that the first feature is the most correlated with the response vector $y$.\n\nThe LASSO path, which tracks the coefficients $\\hat{\\beta}_j(\\lambda)$ as $\\lambda$ varies, is initialized at any $\\lambda$ large enough such that $\\hat{\\beta}(\\lambda)=0$. The value $\\lambda=4$ we have found is the critical point where the solution transitions from being identically zero. As $\\lambda$ is decreased just below $4$, the KKT condition $|(X^T y)_j / n| \\le \\lambda$ is violated for the index $j=1$. To restore optimality, the coefficient $\\hat{\\beta}_1$ must become non-zero. The sign of $\\hat{\\beta}_1$ will be the sign of the correlation, $\\text{sign}((X^T y)_1/n) = \\text{sign}(4) = +1$. Therefore, as the LASSO path begins, $\\hat{\\beta}_1$ becomes positive while other coefficients remain zero. This is how the first variable enters the active set.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Once the LASSO algorithm proposes a sparse solution, how can we rigorously confirm that it has correctly identified the true support and signs? This practice moves beyond merely running an algorithm to formally verifying its output using a primal-dual witness certificate. You will apply the KKT conditions to construct this certificate for a candidate solution, confirming its optimality and gaining insight into the conditions for exact recovery. ",
            "id": "3484748",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem that seeks to estimate a sparse vector $\\beta \\in \\mathbb{R}^{3}$ from data $(X,y)$ by minimizing the convex objective\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}.\n$$\nYou are given the design matrix $X \\in \\mathbb{R}^{3 \\times 3}$ with columns $x_{1}, x_{2}, x_{3}$,\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0  \\frac{3}{5} \\\\\n0  1  -\\frac{1}{10} \\\\\n0  0  \\frac{\\sqrt{63}}{10}\n\\end{pmatrix},\n$$\nthe response vector\n$$\ny \\;=\\; \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix},\n$$\nthe regularization parameter $\\lambda = \\frac{1}{2}$, and the candidate support $S = \\{1,2\\}$ with the presumed sign pattern on $S$ given by $s = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nUsing the fundamental optimality principles of convex optimization (Karush-Kuhn-Tucker conditions), the subdifferential of the $\\ell_{1}$-norm, and the stationarity condition, construct a primal-dual witness certificate that certifies support recovery and sign consistency on $S$ by:\n- Building a primal candidate $\\hat{\\beta}$ with $\\hat{\\beta}_{S^{c}} = 0$,\n- Computing a dual vector $z \\in \\partial \\|\\hat{\\beta}\\|_{1}$ that satisfies the LASSO stationarity condition,\n- Verifying strict complementarity on $S^{c}$.\n\nExplicitly compute the strict complementarity slack\n$$\n\\alpha \\;=\\; 1 - \\|z_{S^{c}}\\|_{\\infty}.\n$$\nProvide your final answer as a single exact real number or a single closed-form analytic expression for $\\alpha$. No rounding is required.",
            "solution": "The objective is to find the unique solution $\\hat{\\beta}$ to the LASSO problem and verify that its support is indeed $S = \\{1,2\\}$ with the sign pattern $s = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. This is done by constructing a primal-dual witness pair $(\\hat{\\beta}, z)$ that satisfies the optimality conditions.\n\nThe stationarity (KKT) condition for the LASSO problem is given by:\n$$\n0 \\in -X^T(y - X\\hat{\\beta}) + \\lambda \\partial\\|\\hat{\\beta}\\|_1\n$$\nwhere $\\partial\\|\\hat{\\beta}\\|_1$ is the subdifferential of the $\\ell_1$-norm at $\\hat{\\beta}$. This condition is equivalent to the existence of a dual vector $z \\in \\partial\\|\\hat{\\beta}\\|_1$ such that:\n$$\nX^T(y - X\\hat{\\beta}) = \\lambda z\n$$\nThe subgradient $z$ must satisfy:\n- $z_i = \\text{sign}(\\hat{\\beta}_i)$ for $i \\in S$ (indices where $\\hat{\\beta}_i \\neq 0$).\n- $|z_i| \\le 1$ for $i \\in S^c$ (indices where $\\hat{\\beta}_i = 0$).\n\nFor support recovery and sign consistency to hold, we require the stricter condition of strict complementarity on the inactive set $S^c$: $|z_i|  1$ for all $i \\in S^c$.\n\n**Step 1: Construct the Primal Candidate $\\hat{\\beta}$**\n\nWe construct a candidate solution $\\hat{\\beta}$ that conforms to the given support $S=\\{1,2\\}$ and sign pattern $s$. This implies $\\hat{\\beta}_{S^c} = \\hat{\\beta}_3 = 0$. The non-zero components $\\hat{\\beta}_S = (\\hat{\\beta}_1, \\hat{\\beta}_2)^T$ are determined by the optimality conditions restricted to the support $S$. Let $X_S$ be the matrix containing the columns of $X$ indexed by $S$.\n$$\nX_S = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\nThe stationarity condition for the components in $S$ is:\n$$\nX_S^T(y - X_S \\hat{\\beta}_S) = \\lambda z_S\n$$\nGiven the presumed sign pattern, we have $z_S = s = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. This leads to a linear system for $\\hat{\\beta}_S$:\n$$\nX_S^T X_S \\hat{\\beta}_S = X_S^T y - \\lambda s\n$$\nWe compute the required matrices and vectors:\n$$\nX_S^T X_S = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2\n$$\n$$\nX_S^T y = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nSubstituting these into the system:\n$$\nI_2 \\hat{\\beta}_S = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\frac{1}{2} \\\\ -1 - (-\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}\n$$\nSo, $\\hat{\\beta}_S = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}$. We must verify that the signs of $\\hat{\\beta}_S$ match the presumed sign pattern $s$:\n$$\n\\text{sign}(\\hat{\\beta}_S) = \\text{sign}\\left(\\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}\\right) = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = s\n$$\nThe sign consistency is confirmed. The full primal candidate is $\\hat{\\beta} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}$.\n\n**Step 2: Compute the Dual Vector $z$**\n\nThe dual vector $z$ is computed from the full stationarity condition $z = \\frac{1}{\\lambda} X^T (y - X\\hat{\\beta})$. First, we compute the residual vector $r = y - X\\hat{\\beta}$:\n$$\nX\\hat{\\beta} = \\begin{pmatrix} 1  0  \\frac{3}{5} \\\\ 0  1  -\\frac{1}{10} \\\\ 0  0  \\frac{\\sqrt{63}}{10} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\frac{3}{2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\n$$\nr = y - X\\hat{\\beta} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}\n$$\nNow, we compute $X^T r$:\n$$\nX^T r = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ \\frac{3}{5}  -\\frac{1}{10}  \\frac{\\sqrt{63}}{10} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(\\frac{1}{2}) \\\\ (1)(-\\frac{1}{2}) \\\\ (\\frac{3}{5})(\\frac{1}{2}) + (-\\frac{1}{10})(-\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ \\frac{3}{10} + \\frac{1}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ \\frac{7}{20} \\end{pmatrix}\n$$\nFinally, we compute $z$:\n$$\nz = \\frac{1}{\\lambda} X^T r = \\frac{1}{1/2} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ \\frac{7}{20} \\end{pmatrix} = 2 \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\\\ \\frac{7}{20} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ \\frac{14}{20} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ \\frac{7}{10} \\end{pmatrix}\n$$\n\n**Step 3: Verify the Optimality Conditions**\n\nWe verify that the pair $(\\hat{\\beta}, z)$ satisfies all optimality conditions.\n1.  Primal feasibility: $\\hat{\\beta} = (\\frac{3}{2}, -\\frac{1}{2}, 0)^T$ with support $S=\\{1,2\\}$.\n2.  Dual feasibility (subgradient condition):\n    - For $i \\in S = \\{1,2\\}$, we must have $z_i = \\text{sign}(\\hat{\\beta}_i)$.\n        - $z_1 = 1 = \\text{sign}(\\frac{3}{2})$. Correct.\n        - $z_2 = -1 = \\text{sign}(-\\frac{1}{2})$. Correct.\n    - For $i \\in S^c = \\{3\\}$, we must have $|z_i| \\le 1$.\n        - $|z_3| = |\\frac{7}{10}| = \\frac{7}{10} \\le 1$. Correct.\n3.  Strict complementarity on $S^c$: We must have $|z_i|  1$ for $i \\in S^c$.\n    - $|z_3| = \\frac{7}{10}  1$. This condition is satisfied.\n\nThe primal-dual witness certificate $(\\hat{\\beta}, z)$ successfully certifies that $\\hat{\\beta}$ is the unique LASSO solution with support $S=\\{1,2\\}$ and sign pattern $s=(1, -1)^T$.\n\n**Step 4: Compute the Strict Complementarity Slack $\\alpha$**\n\nThe strict complementarity slack is defined as $\\alpha = 1 - \\|z_{S^c}\\|_{\\infty}$.\nThe inactive set is $S^c=\\{3\\}$, so $z_{S^c} = z_3 = \\frac{7}{10}$.\n$$\n\\|z_{S^c}\\|_{\\infty} = \\max_{i \\in S^c} |z_i| = |z_3| = \\left|\\frac{7}{10}\\right| = \\frac{7}{10}\n$$\nTherefore, the slack is:\n$$\n\\alpha = 1 - \\frac{7}{10} = \\frac{3}{10}\n$$",
            "answer": "$$\\boxed{\\frac{3}{10}}$$"
        },
        {
            "introduction": "The LASSO is celebrated for both its predictive power and its ability to perform variable selection, but are these guarantees always aligned? This advanced problem explores a crucial theoretical distinction by constructing a scenario where the LASSO yields good predictions but fails to recover the true underlying support. You will dissect this failure by analyzing the interplay between the Restricted Eigenvalue (RE) condition, which governs predictive accuracy, and the Irrepresentable Condition (IC), which is necessary for correct variable selection. ",
            "id": "3484719",
            "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$, response vector $y \\in \\mathbb{R}^{n}$, and a true parameter vector $\\beta^{\\star} \\in \\mathbb{R}^{p}$ whose support is $S = \\{1,2\\}$ and whose signs on $S$ are positive. Assume the rows of $X$ are independent and identically distributed Gaussian vectors with zero mean and covariance matrix $\\Sigma \\in \\mathbb{R}^{p \\times p}$. Let the covariance be specified by\n$$\n\\Sigma = \\begin{pmatrix}\n1  r  a \\\\\nr  1  a \\\\\na  a  1\n\\end{pmatrix},\n$$\nwhere $p=3$, $0r1$, and $a$ is chosen to satisfy $(1+r)/2  a  \\sqrt{(1+r)/2}$. The estimator is the Least Absolute Shrinkage and Selection Operator (LASSO), defined as\n$$\n\\widehat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\},\n$$\nwith regularization parameter $\\lambda  0$.\n\nStarting from first principles, including the definition of the Least Absolute Shrinkage and Selection Operator (LASSO), the Karush–Kuhn–Tucker (KKT) conditions for convex optimization, and the basic linear algebra of block-partitioned covariance matrices, analyze the following claims for the specified $\\Sigma$ and $S$:\n\n1. The restricted eigenvalue condition (in the sense of a strictly positive lower bound on the quadratic form over a cone of directions dominated by the coordinates in $S$) holds for some cone parameter and constant, thus implying strong convexity of the quadratic loss in relevant directions and enabling stable prediction error control.\n\n2. The irrepresentable condition (characterizing sign consistency and exact support recovery of the LASSO) fails because the correlation between inactive and active coordinates is too strong relative to the conditioning of the active block.\n\nDerive the exact irrepresentable quantity relevant to support recovery for this block-partitioned covariance and the given sign pattern on $S$, expressed in terms of $r$ and $a$. Conclude from its derived value that the irrepresentable condition fails for the specified range of $(r,a)$ while the restricted eigenvalue condition can still hold.\n\nYour final answer should be the irrepresentable quantity as a single closed-form expression in $r$ and $a$. No rounding is required. Do not include any units in your final answer.",
            "solution": "The analysis of the problem statement proceeds by first principles, beginning with the Karush-Kuhn-Tucker (KKT) conditions for the LASSO estimator, followed by an examination of the Restricted Eigenvalue (RE) and Irrepresentable Conditions (IC) in the context of the provided population covariance matrix $\\Sigma$.\n\nThe support of the true parameter vector $\\beta^{\\star}$ is $S = \\{1,2\\}$, and its complement is $S^c=\\{3\\}$. The signs of the non-zero parameters are positive, so the sign vector on the support is $z_S = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe population covariance matrix $\\Sigma$ is partitioned according to $S$ and $S^c$:\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_{SS}  \\Sigma_{SS^c} \\\\ \\Sigma_{S^c S}  \\Sigma_{S^c S^c} \\end{pmatrix}\n$$\nFrom the problem statement, we identify the blocks:\n$$\n\\Sigma_{SS} = \\begin{pmatrix} 1  r \\\\ r  1 \\end{pmatrix}, \\quad \\Sigma_{SS^c} = \\begin{pmatrix} a \\\\ a \\end{pmatrix}, \\quad \\Sigma_{S^c S} = \\begin{pmatrix} a  a \\end{pmatrix}, \\quad \\Sigma_{S^c S^c} = (1)\n$$\n\nFirst, we analyze the Irrepresentable Condition (IC), which is a necessary and sufficient condition (under certain regularity assumptions) for the LASSO to be sign-consistent and recover the correct support. The condition states that the covariates in the inactive set $S^c$ must not be too strongly correlated with the covariates in the active set $S$, after accounting for the correlations within the active set. Formally, for the given sign pattern $z_S$, the condition requires:\n$$\n\\| \\Sigma_{S^c S} \\Sigma_{SS}^{-1} z_S \\|_{\\infty}  1\n$$\nHere, the norm is the element-wise maximum absolute value, which is simply the absolute value since $S^c$ contains a single element. The \"irrepresentable quantity\" requested is the vector (in this case, scalar) $\\Sigma_{S^c S} \\Sigma_{SS}^{-1} z_S$.\n\nTo compute this quantity, we first find the inverse of $\\Sigma_{SS}$:\n$$\n\\det(\\Sigma_{SS}) = 1 - r^2\n$$\n$$\n\\Sigma_{SS}^{-1} = \\frac{1}{1-r^2} \\begin{pmatrix} 1  -r \\\\ -r  1 \\end{pmatrix}\n$$\nSince $0  r  1$, the inverse exists. Now, we compute the product $\\Sigma_{S^c S} \\Sigma_{SS}^{-1}$:\n$$\n\\Sigma_{S^c S} \\Sigma_{SS}^{-1} = \\begin{pmatrix} a  a \\end{pmatrix} \\frac{1}{1-r^2} \\begin{pmatrix} 1  -r \\\\ -r  1 \\end{pmatrix} = \\frac{a}{1-r^2} \\begin{pmatrix} 1-r  1-r \\end{pmatrix}\n$$\nSimplifying this expression by factoring out $(1-r)$ from the numerator and denominator $(1-r^2) = (1-r)(1+r)$:\n$$\n\\Sigma_{S^c S} \\Sigma_{SS}^{-1} = \\frac{a(1-r)}{(1-r)(1+r)} \\begin{pmatrix} 1  1 \\end{pmatrix} = \\frac{a}{1+r} \\begin{pmatrix} 1  1 \\end{pmatrix}\n$$\nFinally, we multiply by the sign vector $z_S = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$:\n$$\n\\Sigma_{S^c S} \\Sigma_{SS}^{-1} z_S = \\frac{a}{1+r} \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{a}{1+r} (1+1) = \\frac{2a}{1+r}\n$$\nThis is the irrepresentable quantity. The Irrepresentable Condition requires $|\\frac{2a}{1+r}|  1$. Since $a0$ and $r0$, this is equivalent to $\\frac{2a}{1+r}  1$, or $a  \\frac{1+r}{2}$.\n\nHowever, the problem specifies the condition $(1+r)/2  a$. This directly implies:\n$$\na  \\frac{1+r}{2} \\implies 2a  1+r \\implies \\frac{2a}{1+r}  1\n$$\nSince the irrepresentable quantity is greater than $1$, the Irrepresentable Condition fails. This confirms the second claim in the problem statement. The failure of this condition implies that for any $\\lambda  0$, the LASSO solution $\\widehat{\\beta}(\\lambda)$ will either fail to identify the correct support (i.e., $\\widehat{\\beta}_3 \\neq 0$) or have incorrect signs on the support $S$.\n\nNext, we analyze the Restricted Eigenvalue (RE) condition. The RE condition provides a lower bound on the curvature of the loss function over a cone of directions relevant to sparse estimation. A sufficient condition for the RE property to hold is that the covariance matrix $\\Sigma$ is positive definite. A matrix is positive definite if and only if all its eigenvalues are strictly positive. We proceed to find the eigenvalues of $\\Sigma$. The characteristic equation is $\\det(\\Sigma - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda  r  a \\\\ r  1-\\lambda  a \\\\ a  a  1-\\lambda \\end{pmatrix} = 0\n$$\nWe can spot one eigenvector by inspection: $v_1 = (1, -1, 0)^T$.\n$$\n\\Sigma v_1 = \\begin{pmatrix} 1  r  a \\\\ r  1  a \\\\ a  a  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1-r \\\\ r-1 \\\\ 0 \\end{pmatrix} = (1-r) \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThus, $\\lambda_1 = 1-r$ is an eigenvalue. Since $0r1$, we have $\\lambda_1  0$.\n\nTo find the other two eigenvalues, we can expand the determinant. Let $\\mu = 1-\\lambda$. The characteristic equation becomes:\n$$\n\\mu(\\mu^2 - a^2) - r(r\\mu - a^2) + a(ra - a\\mu) = 0\n$$\n$$\n\\mu^3 - a^2\\mu - r^2\\mu + ra^2 + ra^2 - a^2\\mu = 0\n$$\n$$\n\\mu^3 - (r^2+2a^2)\\mu + 2ra^2 = 0\n$$\nWe know that $\\mu = 1-\\lambda_1 = 1 - (1-r) = r$ is a root. We can factor out $(\\mu-r)$:\n$$\n(\\mu-r)(\\mu^2 + r\\mu - 2a^2) = 0\n$$\nThe remaining roots for $\\mu$ are given by the quadratic equation $\\mu^2 + r\\mu - 2a^2 = 0$:\n$$\n\\mu = \\frac{-r \\pm \\sqrt{r^2 - 4(1)(-2a^2)}}{2} = \\frac{-r \\pm \\sqrt{r^2 + 8a^2}}{2}\n$$\nThe other two eigenvalues are $\\lambda = 1-\\mu$:\n$$\n\\lambda_2 = 1 - \\left( \\frac{-r + \\sqrt{r^2 + 8a^2}}{2} \\right) = \\frac{2+r - \\sqrt{r^2 + 8a^2}}{2}\n$$\n$$\n\\lambda_3 = 1 - \\left( \\frac{-r - \\sqrt{r^2 + 8a^2}}{2} \\right) = \\frac{2+r + \\sqrt{r^2 + 8a^2}}{2}\n$$\nClearly, $\\lambda_3  0$. For $\\lambda_2$ to be positive, we need $2+r  \\sqrt{r^2 + 8a^2}$. Squaring both sides (which are positive):\n$$\n(2+r)^2  r^2 + 8a^2 \\implies 4 + 4r + r^2  r^2 + 8a^2 \\implies 4+4r  8a^2 \\implies \\frac{1+r}{2}  a^2\n$$\nThe problem specifies $a  \\sqrt{(1+r)/2}$, which implies $a^2  (1+r)/2$. This is precisely the condition required for $\\lambda_2  0$. Therefore, all three eigenvalues of $\\Sigma$ are strictly positive, and $\\Sigma$ is positive definite.\n\nThe positive definiteness of $\\Sigma$ means there exists a minimum eigenvalue $\\lambda_{\\min}(\\Sigma)  0$ such that for any vector $v \\in \\mathbb{R}^p$, $v \\neq 0$:\n$$\nv^T \\Sigma v \\ge \\lambda_{\\min}(\\Sigma) \\|v\\|_2^2\n$$\nSince $\\|v\\|_2^2 = \\|v_S\\|_2^2 + \\|v_{S^c}\\|_2^2 \\ge \\|v_S\\|_2^2$, we have $v^T \\Sigma v \\ge \\lambda_{\\min}(\\Sigma) \\|v_S\\|_2^2$. This satisfies the RE condition with a constant $\\kappa = \\lambda_{\\min}(\\Sigma)  0$ for any cone. This confirms the first claim.\n\nIn conclusion, the problem sets up a scenario where the RE condition holds, ensuring the stability of the LASSO estimator in terms of prediction error, while the Irrepresentable Condition fails, which prevents the LASSO from consistently recovering the true sparse model. The derived irrepresentable quantity, $\\frac{2a}{1+r}$, and its value being greater than $1$ under the given constraints, is the mathematical basis for this failure of support recovery.",
            "answer": "$$\\boxed{\\frac{2a}{1+r}}$$"
        }
    ]
}