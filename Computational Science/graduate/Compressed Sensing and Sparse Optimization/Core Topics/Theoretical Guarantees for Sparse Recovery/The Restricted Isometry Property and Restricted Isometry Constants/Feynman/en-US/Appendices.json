{
    "hands_on_practices": [
        {
            "introduction": "Calculating the Restricted Isometry Constant (RIC) for a given matrix is a computationally intractable problem, known to be NP-hard. This exercise moves the concept from a purely theoretical definition to a practical, computational one by guiding you to implement a Monte Carlo method for estimating the RIC, $\\delta_s(A)$. By empirically measuring $\\delta_s(A)$ for random matrices, you will develop an intuition for how this crucial parameter scales with the number of measurements, ambient dimension, and sparsity level, connecting abstract theory to observable phenomena. ",
            "id": "3489923",
            "problem": "You are given the task of empirically approximating the Restricted Isometry Constant (RIC) associated with the Restricted Isometry Property (RIP) in the context of compressed sensing and sparse optimization. The Restricted Isometry Property (RIP) of order $s$ for a matrix $A \\in \\mathbb{R}^{m \\times n}$ is defined as the existence of a smallest number $\\delta_s(A) \\in [0,\\infty)$ such that for every $s$-sparse vector $x \\in \\mathbb{R}^n$ (i.e., a vector with at most $s$ nonzero entries),\n$$(1 - \\delta_s(A)) \\, \\lVert x \\rVert_2^2 \\le \\lVert A x \\rVert_2^2 \\le (1 + \\delta_s(A)) \\, \\lVert x \\rVert_2^2.$$\nHere, $\\delta_s(A)$ is called the Restricted Isometry Constant (RIC). A standard approach to probe $\\delta_s(A)$ computationally relies on the spectral behavior of submatrices of $A$ indexed by supports of size $s$.\n\nYour task is to design and implement a computational experiment that approximates $\\delta_s(A)$ using randomized search over supports of size $s$, and to use it to study empirically how $\\delta_s(A)$ scales with the sampling ratio $m/n$ and sparsity $s$ for random Gaussian sensing matrices. You must obey the following specifications.\n\n- Use the following foundational bases:\n  - The definition of the Restricted Isometry Property (RIP) and the Restricted Isometry Constant (RIC).\n  - The spectral characterization that for each support set $S \\subset \\{1,\\dots,n\\}$ with $\\lvert S \\rvert = s$, if $A_S \\in \\mathbb{R}^{m \\times s}$ denotes the submatrix formed by the columns of $A$ indexed by $S$, then the eigenvalues of $A_S^\\top A_S$ control the inequalities in the RIP definition on that support.\n  - Well-tested facts about random Gaussian matrices, including that if $A$ has independent and identically distributed entries with variance $1/m$ and columns are normalized to unit $\\ell_2$ norm, then the eigenvalues of $A_S^\\top A_S$ concentrate near $1$ when $s \\ll m$.\n\n- Implement the following randomized estimator:\n  - For a given $A \\in \\mathbb{R}^{m \\times n}$ and sparsity $s$, perform $T$ independent trials. In each trial, draw a support set $S \\subset \\{1,\\dots,n\\}$ of size $s$ uniformly at random without replacement. Form the submatrix $A_S \\in \\mathbb{R}^{m \\times s}$ and compute the Gram matrix $G_S = A_S^\\top A_S \\in \\mathbb{R}^{s \\times s}$. Let $\\lambda_{\\min}(G_S)$ and $\\lambda_{\\max}(G_S)$ denote its smallest and largest eigenvalues. Define the trial deviation $d(S) = \\max\\{ \\lvert \\lambda_{\\min}(G_S) - 1 \\rvert, \\lvert \\lambda_{\\max}(G_S) - 1 \\rvert \\}$. The estimator is $\\widehat{\\delta}_s(A) = \\max_{t = 1,\\dots,T} d(S_t)$, which is a Monte Carlo lower bound on $\\delta_s(A)$.\n  - Before running the estimator, normalize the columns of $A$ to have unit $\\ell_2$ norm. This ensures that for $s = 1$, the estimator should return a value close to $0$.\n\n- Experimental design:\n  - For each test case, construct $A$ as an $m \\times n$ matrix with independent entries distributed as $\\mathcal{N}(0, 1/m)$, and then normalize the columns to unit $\\ell_2$ norm.\n  - Use a fixed pseudo-random seed equal to $2025$ for reproducibility across all trials and test cases.\n  - Use the same number of trials $T$ across all test cases.\n\n- Test suite:\n  - You must evaluate the estimator on the following six parameter settings $(m,n,s)$:\n    1. $(m,n,s) = (80, 200, 1)$.\n    2. $(m,n,s) = (80, 200, 10)$.\n    3. $(m,n,s) = (40, 200, 10)$.\n    4. $(m,n,s) = (120, 200, 20)$.\n    5. $(m,n,s) = (60, 200, 50)$.\n    6. $(m,n,s) = (40, 200, 50)$.\n  - Use $T = 250$ trials for each case.\n\n- Output specification:\n  - Your program must output a single line containing a Python-style list of floating-point numbers of length $6$, where the $i$-th entry is your approximation $\\widehat{\\delta}_s(A)$ for the $i$-th test case.\n  - Each floating-point number must be rounded to exactly $6$ decimal places.\n  - The line must have no additional text and be formatted exactly as, for example, $[0.000000,0.123456,0.234567,0.345678,0.456789,0.567890]$.\n\n- Additional interpretive task (to be addressed in your internal algorithmic design and explained in your solution write-up, not in the code output): based on the well-tested behavior of random Gaussian matrices and the monotonicity of the RIC with respect to $s$, explain how the empirical values should vary as $m/n$ increases and as $s$ increases, and relate your observations to the heuristic scaling suggested by concentration of the spectrum of $A_S^\\top A_S$.\n\nNo physical units, angle units, or percentage symbols are involved in this problem. Your final answer must be a complete program that runs without user input and prints exactly the specified single-line list of results.",
            "solution": "The task is to implement a randomized algorithm to empirically estimate the Restricted Isometry Constant (RIC), denoted $\\delta_s(A)$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and sparsity level $s$. We will then apply this algorithm to study the behavior of the RIC for random Gaussian matrices across various parameter settings.\n\nThe Restricted Isometry Property (RIP) is a central concept in compressed sensing. A matrix $A$ satisfies the RIP of order $s$ if there exists a small constant $\\delta_s(A) \\in [0, 1)$ such that for all $s$-sparse vectors $x \\in \\mathbb{R}^n$ (i.e., vectors with at most $s$ non-zero entries), the following inequality holds:\n$$ (1 - \\delta_s(A)) \\lVert x \\rVert_2^2 \\le \\lVert Ax \\rVert_2^2 \\le (1 + \\delta_s(A)) \\lVert x \\rVert_2^2 $$\nThe constant $\\delta_s(A)$ is the $s$-th Restricted Isometry Constant of $A$. This property ensures that $A$ acts as a near-isometry on the set of all $s$-sparse vectors, preserving their lengths.\n\nA key insight for computing $\\delta_s(A)$ is its connection to the eigenvalues of submatrices. Let $S \\subset \\{1, \\dots, n\\}$ be a support set of indices with cardinality $|S| \\le s$. Let $A_S \\in \\mathbb{R}^{m \\times |S|}$ be the submatrix of $A$ containing only the columns indexed by $S$. For any vector $x$ supported on $S$, we have $Ax = A_S x_S$, where $x_S$ contains the non-zero entries of $x$. The squared norm is $\\lVert Ax \\rVert_2^2 = x_S^\\top A_S^\\top A_S x_S$. By the Rayleigh-Ritz theorem, the ratio $\\lVert Ax \\rVert_2^2 / \\lVert x \\rVert_2^2$ is bounded by the smallest and largest eigenvalues of the Gram matrix $G_S = A_S^\\top A_S$. The RIP inequality can thus be rephrased in terms of these eigenvalues. The RIC $\\delta_s(A)$ is formally defined as:\n$$ \\delta_s(A) = \\max_{|S|=s} \\lVert A_S^\\top A_S - I \\rVert_{2 \\to 2} $$\nwhere $\\lVert \\cdot \\rVert_{2 \\to 2}$ is the spectral norm. This is equivalent to finding the maximum deviation of the eigenvalues of all possible $s \\times s$ Gram matrices $A_S^\\top A_S$ from $1$:\n$$ \\delta_s(A) = \\max_{|S|=s} \\left\\{ \\max(|\\lambda_{\\max}(A_S^\\top A_S) - 1|, |\\lambda_{\\min}(A_S^\\top A_S) - 1|) \\right\\} $$\nCalculating $\\delta_s(A)$ exactly is computationally intractable (NP-hard) because it requires searching over all $\\binom{n}{s}$ possible support sets.\n\nTo overcome this, we employ a Monte Carlo approximation. Instead of exhaustively checking all supports, we sample a large number, $T$, of support sets $S_1, S_2, \\dots, S_T$ uniformly at random without replacement. For each sampled support $S_t$, we compute the trial deviation $d(S_t) = \\max\\{|\\lambda_{\\max}(A_{S_t}^\\top A_{S_t}) - 1|, |\\lambda_{\\min}(A_{S_t}^\\top A_{S_t}) - 1|\\}$. The estimator for the RIC, $\\widehat{\\delta}_s(A)$, is then the maximum deviation found across all trials:\n$$ \\widehat{\\delta}_s(A) = \\max_{t \\in \\{1,\\dots,T\\}} d(S_t) $$\nThis estimator provides a lower bound on the true $\\delta_s(A)$.\n\nThe experimental design specifies the construction of the sensing matrix $A$. Its entries are drawn independently from a Gaussian distribution $\\mathcal{N}(0, 1/m)$. Subsequently, each of its $n$ columns is normalized to have a unit $\\ell_2$ norm. This normalization is a standard convention and ensures that for $s=1$, a submatrix $A_S$ is a single column $a_j$ with $\\lVert a_j \\rVert_2 = 1$. The corresponding Gram matrix is a $1 \\times 1$ matrix $A_S^\\top A_S = [a_j^\\top a_j] = [1]$, whose only eigenvalue is $1$. The deviation is thus $0$, serving as a valuable sanity check for our implementation.\n\nThe algorithm for each test case $(m, n, s)$ proceeds as follows:\n1. Initialize a pseudo-random number generator with a fixed seed of $2025$ for reproducibility.\n2. Generate the $m \\times n$ matrix $A$ with entries drawn from $\\mathcal{N}(0, 1/m)$.\n3. Normalize each column of $A$ to have unit $\\ell_2$ norm.\n4. Initialize a variable `max_deviation` to $0.0$.\n5. Perform $T=250$ trials. In each trial:\n    a. Randomly choose a support set $S \\subset \\{1, \\dots, n\\}$ of size $s$ without replacement.\n    b. Extract the corresponding submatrix $A_S$.\n    c. Compute the Gram matrix $G_S = A_S^\\top A_S$.\n    d. Calculate the eigenvalues of $G_S$. Since $G_S$ is symmetric and positive semi-definite, a specialized algorithm for Hermitian matrices is used for accuracy and efficiency.\n    e. Determine the minimum eigenvalue, $\\lambda_{\\min}$, and maximum eigenvalue, $\\lambda_{\\max}$.\n    f. Calculate the deviation for this trial, $d(S) = \\max\\{1 - \\lambda_{\\min}, \\lambda_{\\max} - 1\\}$. Note that for a positive semi-definite matrix, $\\lambda_{\\min} \\ge 0$, and theory predicts it should be near $1$, so $|1-\\lambda_{\\min}| \\approx 1-\\lambda_{\\min}$.\n    g. Update `max_deviation` with the current trial's deviation if it is larger.\n6. The final value of `max_deviation` is the estimated RIC, $\\widehat{\\delta}_s(A)$, for the given parameters.\n\nBased on compressed sensing theory, we expect the following trends in our results:\n- **Dependence on Sparsity ($s$)**: The RIC, $\\delta_s(A)$, is monotonically non-decreasing with $s$. As $s$ increases, the set of $s$-sparse vectors expands, and the combinatorial maximization is over a larger family of submatrices, making it more likely to find a \"bad\" submatrix whose Gramian has eigenvalues far from $1$. Our estimate $\\widehat{\\delta}_s(A)$ should exhibit a similar increasing trend.\n- **Dependence on Measurements ($m$)**: For fixed $n$ and $s$, increasing the number of measurements $m$ (and thus the sampling ratio $m/n$) provides more information and makes the linear system more overdetermined. This generally improves the RIP, causing the eigenvalues of $A_S^\\top A_S$ to concentrate more tightly around $1$. Consequently, we expect $\\delta_s(A)$ and our estimate $\\widehat{\\delta}_s(A)$ to decrease as $m$ increases.\nThe test suite is designed to probe these behaviors, for instance, by comparing cases with the same $(n,s)$ but different $m$, or the same $(m,n)$ but different $s$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the RIC estimation experiments and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, s)\n        (80, 200, 1),\n        (80, 200, 10),\n        (40, 200, 10),\n        (120, 200, 20),\n        (60, 200, 50),\n        (40, 200, 50),\n    ]\n    \n    # Number of Monte Carlo trials\n    T = 250\n    # Seed for reproducibility\n    seed = 2025\n\n    # A single random number generator for all experiments to ensure reproducibility\n    rng = np.random.default_rng(seed)\n\n    def estimate_ric(m, n, s, T, rng):\n        \"\"\"\n        Estimates the Restricted Isometry Constant (RIC) for a random Gaussian matrix.\n\n        Args:\n            m (int): Number of rows (measurements).\n            n (int): Number of columns (ambient dimension).\n            s (int): Sparsity level.\n            T (int): Number of Monte Carlo trials.\n            rng (np.random.Generator): Random number generator instance.\n\n        Returns:\n            float: The estimated RIC, delta_s.\n        \"\"\"\n        # Step 1: Construct the random matrix A with N(0, 1/m) entries.\n        A = rng.normal(loc=0.0, scale=np.sqrt(1.0 / m), size=(m, n))\n\n        # Step 2: Normalize the columns of A to have unit l2 norm.\n        col_norms = np.linalg.norm(A, axis=0)\n        # Avoid division by zero, though highly improbable with a continuous distribution.\n        col_norms[col_norms == 0] = 1.0\n        A = A / col_norms\n\n        max_deviation = 0.0\n\n        if s == 0:\n            return 0.0\n\n        # Step 3: Perform T independent trials.\n        for _ in range(T):\n            # a. Draw a random support set S of size s without replacement.\n            support_S = rng.choice(n, size=s, replace=False)\n\n            # b. Form the submatrix A_S.\n            A_S = A[:, support_S]\n\n            # c. Compute the Gram matrix G_S = A_S^T * A_S.\n            G_S = A_S.T @ A_S\n\n            # d. Compute the eigenvalues of G_S.\n            # G_S is a real symmetric matrix, so use eigvalsh for efficiency\n            # and numerical stability. It returns sorted eigenvalues.\n            eigenvalues = np.linalg.eigvalsh(G_S)\n\n            lambda_min = eigenvalues[0]\n            lambda_max = eigenvalues[-1]\n\n            # e. Compute the trial deviation.\n            deviation = max(abs(lambda_min - 1.0), abs(lambda_max - 1.0))\n\n            # f. Update the overall maximum deviation.\n            if deviation > max_deviation:\n                max_deviation = deviation\n        \n        return max_deviation\n\n    results = []\n    for m, n, s in test_cases:\n        result = estimate_ric(m, n, s, T, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Each floating-point number is rounded to exactly 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "While random matrices often exhibit good restricted isometry properties on average, the RIC is a worst-case measure determined by the most ill-conditioned submatrix. This practice shifts the focus from numerical estimation to precise analytical derivation by challenging you to find the exact RIC for a matrix with a known, structured-correlation pattern. By identifying the adversarial choice of columns that maximizes the isometry distortion, you will gain a deeper understanding of how specific structural features within a sensing matrix directly govern its performance guarantees. ",
            "id": "3489942",
            "problem": "Consider a real sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with unit-norm columns $\\{a_{j}\\}_{j=1}^{n}$, and recall the Restricted Isometry Property (RIP): the order-$s$ Restricted Isometry Constant (RIC) $\\delta_{s}$ is the smallest number $\\delta \\geq 0$ such that for all $s$-sparse vectors $x \\in \\mathbb{R}^{n}$,\n$$(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|Ax\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}.$$\nEquivalently, for every support set $S \\subset [n]$ of size $|S|=s$, the $s \\times s$ Gram matrix $G_{S} := A_{S}^{\\top} A_{S}$ satisfies $\\|G_{S}-I\\|_{2} \\leq \\delta$, where $I$ is the $s \\times s$ identity.\n\nNow suppose $A$ has the following structure. There exists an index set $C \\subset [n]$ with $|C|=r$ and a fixed correlation parameter $\\mu$ with $0\\mu1$ such that:\n- For all distinct $i,j \\in C$, one has $a_{i}^{\\top} a_{j}=\\mu$, and $a_{i}^{\\top} a_{i}=1$ for all $i \\in C$.\n- For all $k \\notin C$, the columns $\\{a_{k}\\}_{k \\notin C}$ are mutually orthonormal, and each $a_{k}$ with $k \\notin C$ is orthogonal to every $a_{i}$ with $i \\in C$.\n\nFix an integer $s$ with $2 \\leq s \\leq r$. Consider adversarial support selection: among all supports $S \\subset [n]$ with $|S|=s$, one seeks those $S$ that maximize the spectral norm deviation $\\|A_{S}^{\\top} A_{S}-I\\|_{2}$, i.e., the most ill-conditioned $s$-column submatrices in the RIP sense.\n\nStarting from the core definitions above and using only standard linear algebra, perform the following:\n- Determine the structure of supports $S$ that maximize $\\|A_{S}^{\\top} A_{S}-I\\|_{2}$.\n- Compute the exact value of $\\max_{|S|=s}\\|A_{S}^{\\top} A_{S}-I\\|_{2}$ in terms of $\\mu$ and $s$.\n- Conclude the exact analytic expression for the Restricted Isometry Constant $\\delta_{s}$ of $A$ for $2 \\leq s \\leq r$.\n\nYour final answer must be a single closed-form expression in terms of $\\mu$ and $s$. No rounding is required, and no units apply.",
            "solution": "### Step 1: Extract Givens\n-   Sensing matrix: $A \\in \\mathbb{R}^{m \\times n}$\n-   Columns of $A$: $\\{a_j\\}_{j=1}^{n}$ are unit-norm, i.e., $a_j^{\\top} a_j = 1$ for all $j \\in [n]$.\n-   Restricted Isometry Constant (RIC) $\\delta_s$: The smallest $\\delta \\geq 0$ such that for all $s$-sparse vectors $x \\in \\mathbb{R}^{n}$, $(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|Ax\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}$.\n-   Equivalent definition of RIC: $\\delta_s = \\max_{S \\subset [n], |S|=s} \\|A_{S}^{\\top} A_{S} - I\\|_{2}$, where $A_S$ is the submatrix of $A$ with columns indexed by $S$, and $I$ is the $s \\times s$ identity matrix.\n-   Index set $C \\subset [n]$ with $|C|=r$.\n-   Correlation parameter $\\mu$ with $0  \\mu  1$.\n-   Structure of $A$:\n    1.  For all distinct $i, j \\in C$, $a_i^{\\top} a_j = \\mu$.\n    2.  For all $k \\notin C$, the columns $\\{a_k\\}_{k \\notin C}$ are mutually orthonormal, i.e., $a_k^{\\top} a_l = \\delta_{kl}$ (Kronecker delta).\n    3.  For all $i \\in C$ and $k \\notin C$, $a_i^{\\top} a_k = 0$.\n-   Sparseness level $s$ is an integer with $2 \\leq s \\leq r$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n1.  **Scientific or Factual Unsoundness**: The problem is rooted in the established mathematical framework of compressed sensing and linear algebra. The Restricted Isometry Property is a standard concept. No scientific principles are violated.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formal mathematical exercise directly related to the topic of the Restricted Isometry Property.\n3.  **Incomplete or Contradictory Setup**: The problem provides all necessary definitions, constants, and conditions. The conditions on the matrix $A$ are specific and consistent. The constraint $s \\leq r$ ensures that it is possible to select $s$ columns from the correlated set $C$.\n4.  **Unrealistic or Infeasible**: The construction of such a matrix $A$ is mathematically feasible (e.g., via a block-diagonal Gram matrix and subsequent Cholesky decomposition or Gram-Schmidt process).\n5.  **Ill-Posed or Poorly Structured**: The objective is clear: to find the maximum of a well-defined quantity over a finite set of possibilities. A unique solution is expected. All terms are standard in the field.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires a structured analysis of how the choice of support set $S$ affects the spectral norm of $A_S^{\\top} A_S - I$, which involves understanding the eigenvalues of structured matrices. It is not trivial.\n7.  **Outside Scientific Verifiability**: The claims are entirely within the realm of mathematical proof.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with a full solution.\n\n### Solution\nThe primary goal is to compute the order-$s$ Restricted Isometry Constant (RIC) $\\delta_s$ for the given matrix $A$. By definition, this is given by:\n$$ \\delta_s = \\max_{S \\subset [n], |S|=s} \\|A_S^{\\top} A_S - I_s\\|_2 $$\nwhere $S$ is a support set of size $s$, $A_S$ is the submatrix of $A$ containing columns indexed by $S$, and $I_s$ is the $s \\times s$ identity matrix. The matrix $G_S = A_S^{\\top} A_S$ is the Gram matrix of the columns in $A_S$.\n\nLet's analyze the structure of $G_S$ based on the composition of the support set $S$. Any support $S$ of size $s$ can be partitioned into two disjoint subsets: $S_C = S \\cap C$ and $S_{C^c} = S \\cap ([n] \\setminus C)$. Let $k = |S_C|$, so $0 \\leq k \\leq s$. Consequently, $|S_{C^c}| = s-k$.\n\nThe columns indexed by $S_C$ are all from the correlated set $C$. The columns indexed by $S_{C^c}$ are from the orthonormal set outside $C$. By the problem statement, any column from $C$ is orthogonal to any column not in $C$. Therefore, for $i \\in S_C$ and $j \\in S_{C^c}$, we have $a_i^{\\top} a_j = 0$.\n\nThis orthogonality implies that after a suitable permutation of rows and columns (which does not change the spectral norm), the Gram matrix $G_S$ is block-diagonal:\n$$ G_S = A_S^{\\top} A_S = \\begin{pmatrix} A_{S_C}^{\\top} A_{S_C}  A_{S_C}^{\\top} A_{S_{C^c}} \\\\ A_{S_{C^c}}^{\\top} A_{S_C}  A_{S_{C^c}}^{\\top} A_{S_{C^c}} \\end{pmatrix} = \\begin{pmatrix} G_{S_C}  \\mathbf{0} \\\\ \\mathbf{0}  G_{S_{C^c}} \\end{pmatrix} $$\nwhere $G_{S_C}$ is the $k \\times k$ Gram matrix for columns in $S_C$ and $G_{S_{C^c}}$ is the $(s-k) \\times (s-k)$ Gram matrix for columns in $S_{C^c}$.\n\nLet's analyze the two blocks:\n1.  The columns indexed by $S_{C^c}$ are mutually orthonormal. Thus, their Gram matrix is the identity matrix: $G_{S_{C^c}} = I_{s-k}$.\n2.  The columns indexed by $S_C$ all have unit norm and their pairwise inner product is $\\mu$. Thus, $G_{S_C}$ is a $k \\times k$ matrix with $1$s on the diagonal and $\\mu$ on all off-diagonal entries. Let us denote this matrix by $M_k$.\n$$ M_k = \\begin{pmatrix} 1  \\mu  \\dots  \\mu \\\\ \\mu  1  \\dots  \\mu \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ \\mu  \\mu  \\dots  1 \\end{pmatrix}_{k \\times k} $$\n\nThe matrix $G_S - I_s$ is then:\n$$ G_S - I_s = \\begin{pmatrix} M_k  \\mathbf{0} \\\\ \\mathbf{0}  I_{s-k} \\end{pmatrix} - \\begin{pmatrix} I_k  \\mathbf{0} \\\\ \\mathbf{0}  I_{s-k} \\end{pmatrix} = \\begin{pmatrix} M_k - I_k  \\mathbf{0} \\\\ \\mathbf{0}  \\mathbf{0} \\end{pmatrix} $$\nThe spectral norm (maximum singular value) of a block-diagonal matrix is the maximum of the spectral norms of its blocks. Thus:\n$$ \\|G_S - I_s\\|_2 = \\|M_k - I_k\\|_2 $$\nOur task now is to find $\\max_{0 \\leq k \\leq s} \\|M_k - I_k\\|_2$.\n\nLet's analyze $\\|M_k - I_k\\|_2$. The matrix $M_k$ can be written as $M_k = (1-\\mu)I_k + \\mu J_k$, where $J_k$ is the $k \\times k$ matrix of all ones. The matrix $M_k - I_k$ is then:\n$$ M_k - I_k = (1-\\mu)I_k + \\mu J_k - I_k = \\mu J_k - \\mu I_k = \\mu(J_k - I_k) $$\nThis is a matrix with $0$s on the diagonal and $\\mu$s everywhere else. Since it is a symmetric matrix, its spectral norm is the maximum absolute value of its eigenvalues. Let's find the eigenvalues of $J_k - I_k$. The matrix $J_k$ has eigenvalues $k$ (with multiplicity $1$, eigenvector being the all-ones vector $\\mathbf{1}$) and $0$ (with multiplicity $k-1$). Therefore, the eigenvalues of $J_k - I_k$ are $k-1$ and $-1$ (with multiplicity $k-1$).\n\nThe eigenvalues of $M_k - I_k = \\mu(J_k - I_k)$ are $\\mu(k-1)$ (multiplicity $1$) and $-\\mu$ (multiplicity $k-1$). This derivation is valid for $k \\geq 2$, as for $k2$ there are not enough dimensions for the multiplicity of the $-1$ eigenvalue to be well-defined in this way.\n\nLet's consider the possible values of $k \\in \\{0, 1, \\dots, s\\}$:\n-   If $k=0$, the support $S$ is entirely in $C^c$. $S_C$ is empty, so $M_0$ is an empty matrix and $\\|M_0-I_0\\|_2=0$.\n-   If $k=1$, $S$ contains one column from $C$ and $s-1$ columns from $C^c$. These $s$ columns are mutually orthogonal, so $G_S=I_s$ and $\\|G_S-I_s\\|_2 = 0$. Alternatively, $M_1$ is the $1 \\times 1$ matrix $[1]$, so $M_1 - I_1 = [0]$, and its norm is $0$.\n-   If $k \\geq 2$, the eigenvalues of $M_k - I_k$ are $\\mu(k-1)$ and $-\\mu$. The spectral norm is the maximum of their absolute values:\n    $$ \\|M_k - I_k\\|_2 = \\max (|\\mu(k-1)|, |-\\mu|) $$\n    Since $0  \\mu  1$ and $k \\geq 2$, we have $\\mu(k-1)  0$. So,\n    $$ \\|M_k - I_k\\|_2 = \\max (\\mu(k-1), \\mu) $$\n    As $k \\geq 2$, $k-1 \\geq 1$. Therefore, $\\mu(k-1) \\geq \\mu$. This simplifies the norm to:\n    $$ \\|M_k - I_k\\|_2 = \\mu(k-1) \\quad \\text{for } k \\geq 2 $$\n\nTo find $\\delta_s$, we must maximize this norm over all possible values of $k$:\n$$ \\delta_s = \\max_{k \\in \\{0, 1, \\dots, s\\}} \\|M_k - I_k\\|_2 $$\nLet $f(k) = \\|M_k - I_k\\|_2$. We have:\n$$ f(k) = \\begin{cases} 0  \\text{if } k=0, 1 \\\\ \\mu(k-1)  \\text{if } 2 \\leq k \\leq s \\end{cases} $$\nThe function $f(k)$ is non-decreasing for $k \\in \\{0, 1, \\dots, s\\}$. The maximum value is achieved at the largest possible value of $k$, which is $k=s$. The problem states $s \\geq 2$, so this case is relevant. The condition $s \\leq r$ ensures that it is possible to choose $s$ columns from the set $C$, i.e., to have $k=s$.\n\nThe maximum value is $f(s)=\\mu(s-1)$.\n\nTherefore, we can conclude:\n1.  **Structure of adversarial supports**: The supports $S$ of size $s$ that maximize $\\|A_{S}^{\\top} A_{S}-I\\|_{2}$ are those chosen entirely from the correlated set $C$. That is, any $S \\subset C$ with $|S|=s$.\n2.  **Maximum norm value**: The maximum value is $\\max_{|S|=s}\\|A_{S}^{\\top} A_{S}-I\\|_{2} = \\mu(s-1)$.\n3.  **Restricted Isometry Constant $\\delta_s$**: By definition, $\\delta_s$ is this maximum value.\n    $$ \\delta_s = \\mu(s-1) $$\n\nThis expression is valid for the given range $2 \\leq s \\leq r$.",
            "answer": "$$\\boxed{\\mu(s-1)}$$"
        },
        {
            "introduction": "Theoretical guarantees in compressed sensing, like those based on the RIP, often rely on idealized assumptions about signal structure. This thought experiment explores a crucial limitation of the RIP by demonstrating that a perfect constant for exactly sparse signals does not automatically ensure robust recovery for nearly sparse, or compressible, signals. By constructing a simple counterexample, you will quantify the degradation in recovery performance due to this model mismatch, a vital lesson for the practical application of sparse recovery methods. ",
            "id": "3489943",
            "problem": "Let $A \\in \\mathbb{R}^{1 \\times 3}$ be a sensing matrix with columns $a_{1} = 1$, $a_{2} = 1$, and $a_{3} = 1$. Consider the task of recovering a signal $x \\in \\mathbb{R}^{3}$ from noiseless measurements $y = A x$ via convex optimization that minimizes the $\\ell_{1}$ norm subject to the measurement constraint. The restricted isometry property (RIP) with restricted isometry constant (RIC) $\\delta_{s}$ is defined by the smallest $\\delta \\geq 0$ such that, for all $s$-sparse $z \\in \\mathbb{R}^{3}$,\n$$\n(1 - \\delta) \\|z\\|_{2}^{2} \\leq \\|A z\\|_{2}^{2} \\leq (1 + \\delta) \\|z\\|_{2}^{2}.\n$$\nYour goal is to demonstrate that a small $\\delta_{s}$ does not guarantee robustness to model mismatch when the true signal is compressible but not exactly $s$-sparse, and to quantify the degradation in recovery error.\n\nSpecifically:\n\n1. Using only the definition of the restricted isometry constant, compute $\\delta_{1}$ for the given matrix $A$.\n\n2. Let the true signal be $x^{\\star} = (1, \\epsilon, 0)^{\\top}$ with $0  \\epsilon  1$, which is compressible but not exactly $1$-sparse. Denote $y = A x^{\\star}$. Construct a feasible point $z^{\\flat}$ that minimizes $\\|z\\|_{1}$ subject to $A z = y$ and is $1$-sparse.\n\n3. Compute the Euclidean error $\\|z^{\\flat} - x^{\\star}\\|_{2}$. Let $x_{1}$ denote the best $1$-sparse approximation to $x^{\\star}$ obtained by retaining the largest-magnitude entry of $x^{\\star}$ and zeroing the rest, and compute the model mismatch error $\\|x^{\\star} - x_{1}\\|_{2}$.\n\n4. Define the degradation factor\n$$\nR(\\epsilon) := \\frac{\\|z^{\\flat} - x^{\\star}\\|_{2}}{\\|x^{\\star} - x_{1}\\|_{2}}.\n$$\nCompute $R(\\epsilon)$ exactly and present it as a single closed-form value. No rounding is required and no physical units apply. Your final answer must be a single real-valued number or a single analytic expression.",
            "solution": "The problem asks for a four-part analysis concerning the recovery of a signal using $\\ell_{1}$-minimization with a given sensing matrix. The matrix is $A \\in \\mathbb{R}^{1 \\times 3}$, defined as $A = \\begin{pmatrix} 1  1  1 \\end{pmatrix}$.\n\n**Part 1: Compute the restricted isometry constant $\\delta_{1}$.**\n\nThe restricted isometry constant $\\delta_{s}$ is the smallest non-negative number $\\delta$ satisfying the inequality\n$$\n(1 - \\delta) \\|z\\|_{2}^{2} \\leq \\|A z\\|_{2}^{2} \\leq (1 + \\delta) \\|z\\|_{2}^{2}\n$$\nfor all $s$-sparse vectors $z$. We are asked to compute $\\delta_{1}$.\n\nA $1$-sparse vector $z \\in \\mathbb{R}^{3}$ is a vector with at most one non-zero entry. Let $z$ be a general $1$-sparse vector. It can be written in one of three forms:\n$z = \\begin{pmatrix} c \\\\ 0 \\\\ 0 \\end{pmatrix}$, $z = \\begin{pmatrix} 0 \\\\ c \\\\ 0 \\end{pmatrix}$, or $z = \\begin{pmatrix} 0 \\\\ 0 \\\\ c \\end{pmatrix}$ for some scalar $c \\in \\mathbb{R}$.\n\nIn all three cases, the squared Euclidean norm of $z$ is $\\|z\\|_{2}^{2} = c^{2}$.\n\nNow we compute $\\|A z\\|_{2}^{2}$ for each case:\n1.  For $z = \\begin{pmatrix} c \\\\ 0 \\\\ 0 \\end{pmatrix}$, $Az = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} c \\\\ 0 \\\\ 0 \\end{pmatrix} = c$. Thus, $\\|Az\\|_{2}^{2} = c^{2}$.\n2.  For $z = \\begin{pmatrix} 0 \\\\ c \\\\ 0 \\end{pmatrix}$, $Az = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ c \\\\ 0 \\end{pmatrix} = c$. Thus, $\\|Az\\|_{2}^{2} = c^{2}$.\n3.  For $z = \\begin{pmatrix} 0 \\\\ 0 \\\\ c \\end{pmatrix}$, $Az = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ c \\end{pmatrix} = c$. Thus, $\\|Az\\|_{2}^{2} = c^{2}$.\n\nIn every case for any $1$-sparse vector $z$, we have shown that $\\|A z\\|_{2}^{2} = \\|z\\|_{2}^{2}$.\n\nSubstituting this into the RIP definition for $s=1$:\n$$\n(1 - \\delta_{1}) \\|z\\|_{2}^{2} \\leq \\|z\\|_{2}^{2} \\leq (1 + \\delta_{1}) \\|z\\|_{2}^{2}\n$$\nAssuming $z$ is not the zero vector (for which the inequality is trivial), we can divide by $\\|z\\|_{2}^{2}$:\n$$\n1 - \\delta_{1} \\leq 1 \\quad \\text{and} \\quad 1 \\leq 1 + \\delta_{1}\n$$\nThe first inequality implies $\\delta_{1} \\geq 0$. The second inequality also implies $\\delta_{1} \\geq 0$. The smallest non-negative value for $\\delta_{1}$ that satisfies these conditions is $\\delta_{1} = 0$.\n\n**Part 2: Construct the recovered signal $z^{\\flat}$.**\n\nThe true signal is $x^{\\star} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}$, where $0  \\epsilon  1$.\nThe noiseless measurement is $y = A x^{\\star} = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} = 1 + \\epsilon$.\n\nThe task is to find a vector $z^{\\flat}$ that minimizes the $\\ell_{1}$-norm $\\|z\\|_{1}$ subject to the constraint $A z = y$, with the additional property that $z^{\\flat}$ must be $1$-sparse.\n\nThe constraint is $z_{1} + z_{2} + z_{3} = 1+\\epsilon$. The objective is to minimize $\\|z\\|_{1} = |z_{1}| + |z_{2}| + |z_{3}|$.\nBy the triangle inequality, for any feasible $z$:\n$$\n\\|z\\|_{1} = |z_{1}| + |z_{2}| + |z_{3}| \\geq |z_{1} + z_{2} + z_{3}| = |1+\\epsilon|\n$$\nSince $0  \\epsilon  1$, we have $1+\\epsilon  0$, so $|1+\\epsilon| = 1+\\epsilon$. Thus, the minimum possible value of $\\|z\\|_{1}$ is $1+\\epsilon$.\n\nWe must find a $1$-sparse vector $z^{\\flat}$ that satisfies $A z^{\\flat} = 1+\\epsilon$ and achieves this minimum norm $\\|z^{\\flat}\\|_{1} = 1+\\epsilon$. Let's examine the possible $1$-sparse vectors:\n1.  $z = \\begin{pmatrix} c \\\\ 0 \\\\ 0 \\end{pmatrix}$: $A z = c$. So, $c=1+\\epsilon$. This gives $z^{(1)} = \\begin{pmatrix} 1+\\epsilon \\\\ 0 \\\\ 0 \\end{pmatrix}$. Its $\\ell_{1}$-norm is $\\|z^{(1)}\\|_{1} = |1+\\epsilon| = 1+\\epsilon$.\n2.  $z = \\begin{pmatrix} 0 \\\\ c \\\\ 0 \\end{pmatrix}$: $A z = c$. So, $c=1+\\epsilon$. This gives $z^{(2)} = \\begin{pmatrix} 0 \\\\ 1+\\epsilon \\\\ 0 \\end{pmatrix}$. Its $\\ell_{1}$-norm is $\\|z^{(2)}\\|_{1} = |1+\\epsilon| = 1+\\epsilon$.\n3.  $z = \\begin{pmatrix} 0 \\\\ 0 \\\\ c \\end{pmatrix}$: $A z = c$. So, $c=1+\\epsilon$. This gives $z^{(3)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1+\\epsilon \\end{pmatrix}$. Its $\\ell_{1}$-norm is $\\|z^{(3)}\\|_{1} = |1+\\epsilon| = 1+\\epsilon$.\n\nAll three of these vectors are $1$-sparse, satisfy the measurement constraint, and achieve the minimum $\\ell_{1}$-norm. The set of solutions to the $\\ell_1$-minimization problem is actually the convex hull of these three points. The problem asks us to construct a single point $z^{\\flat}$ that is $1$-sparse. The most principled choice is the one whose sparsity pattern matches the best $1$-sparse approximation of the true signal $x^{\\star}$. The largest component of $x^{\\star}$ is its first component, so its best $1$-sparse approximation is supported on the first index. We therefore select the solution $z^{\\flat}$ which is also supported on the first index.\n\nThus, we construct $z^{\\flat} = z^{(1)} = \\begin{pmatrix} 1+\\epsilon \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Part 3: Compute Euclidean error and model mismatch error.**\n\nThe Euclidean error between the recovered signal $z^{\\flat}$ and the true signal $x^{\\star}$ is:\n$$\n\\|z^{\\flat} - x^{\\star}\\|_{2} = \\left\\| \\begin{pmatrix} 1+\\epsilon \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\left\\| \\begin{pmatrix} \\epsilon \\\\ -\\epsilon \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{\\epsilon^{2} + (-\\epsilon)^{2} + 0^{2}} = \\sqrt{2\\epsilon^{2}} = \\epsilon\\sqrt{2}\n$$\nsince $\\epsilon > 0$.\n\nNext, we find $x_{1}$, the best $1$-sparse approximation to $x^{\\star}$. This is formed by keeping the entry of $x^{\\star}$ with the largest absolute value and setting the others to zero.\nGiven $x^{\\star} = \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix}$ and $0  \\epsilon  1$, the largest-magnitude entry is the first one, which is $1$.\nSo, $x_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe model mismatch error is the Euclidean distance between $x^{\\star}$ and $x_{1}$:\n$$\n\\|x^{\\star} - x_{1}\\|_{2} = \\left\\| \\begin{pmatrix} 1 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\left\\| \\begin{pmatrix} 0 \\\\ \\epsilon \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{0^{2} + \\epsilon^{2} + 0^{2}} = \\epsilon\n$$\n\n**Part 4: Compute the degradation factor $R(\\epsilon)$.**\n\nThe degradation factor is defined as the ratio of the two errors calculated above:\n$$\nR(\\epsilon) := \\frac{\\|z^{\\flat} - x^{\\star}\\|_{2}}{\\|x^{\\star} - x_{1}\\|_{2}}\n$$\nSubstituting the computed values:\n$$\nR(\\epsilon) = \\frac{\\epsilon\\sqrt{2}}{\\epsilon} = \\sqrt{2}\n$$\nThis result is a constant, independent of $\\epsilon$, which is consistent with the problem's implicit constraint that the answer must be a single closed-form value. This confirms our choice of $z^{\\flat}$ was the intended one. The final result demonstrates that even with a perfect isometry for $1$-sparse signals ($\\delta_{1}=0$), the $\\ell_{1}$-norm recovery error for a compressible signal is a factor of $\\sqrt{2}$ larger than the best-case approximation error.",
            "answer": "$$\\boxed{\\sqrt{2}}$$"
        }
    ]
}