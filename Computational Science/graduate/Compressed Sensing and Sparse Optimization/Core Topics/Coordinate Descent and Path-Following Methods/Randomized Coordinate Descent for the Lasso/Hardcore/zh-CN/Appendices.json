{
    "hands_on_practices": [
        {
            "introduction": "坐标下降算法可以通过不同策略选择下一个要更新的坐标。虽然简单的循环顺序（$1, 2, \\dots, p, 1, 2, \\dots$）直观易行，但在特征高度相关时，其效率可能出奇地低下，导致收敛过程出现“之字形”振荡。这项实践练习  将指导您构建一个具有相关特征的“最坏情况”场景，通过实验直观地展示循环坐标下降的缓慢收敛，并将其与随机选择策略的稳健性能进行对比，从而为在坐标下降方法中采用随机化策略提供强有力的实践动机。",
            "id": "3441210",
            "problem": "考虑压缩感知和稀疏优化中的最小绝对收缩和选择算子 (Lasso) 目标函数，对于一个感知矩阵 $A \\in \\mathbb{R}^{m \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^{m}$，其定义如下\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。坐标下降算法通过逐次对 $x$ 的一个坐标进行最小化来优化 $f(x)$，同时保持其他坐标固定。考虑以下两种选择规则：\n- 循环坐标下降 (CCD)：按固定顺序 $1,2,\\dots,p$ 重复访问坐标。\n- 随机坐标下降 (RCD)：在每次更新时，有放回地从所有坐标中均匀随机选择一个。\n\n一个轮次 (epoch) 定义为进行整 $p$ 次单坐标更新。两种方法都从 $x^{(0)} = 0$ 开始，并在每次更新时沿所选坐标进行精确最小化。\n\n设计一个最坏情况下的感知矩阵 $A$ 和响应 $y$，通过构造高度相关的列来为循环规则诱发“之”字形下降 (zig-zagging)。具体来说，对于给定的 $m$、$p$ 和相关性参数 $\\rho \\in [0,1)$，构造 $A$ 的前两列，使其内积近似为 $\\rho$ 且范数为单位范数，并构造其余各列，使其与前两列及彼此之间近似不相关。设置 $y = A x^\\star$，其中 $x^\\star$ 是一个稀疏的真实向量，只有前两个元素非零且幅值相等。然后，对于每个测试用例，程序必须比较 CCD 和 RCD 在固定的 $E$ 个轮次上的每轮次平均目标函数下降值。随机选择必须使用固定的伪随机种子 $2025$，以确保结果可复现。\n\n您的程序必须在每次更新时，仅基于 $f(x)$ 的定义和逐坐标的一维优化子问题，实现这两种使用精确单坐标最小化的选择规则。对于每个测试用例，它必须计算并报告以下比率\n$$\nr \\;=\\; \\frac{\\text{average per-epoch objective decrease under RCD}}{\\text{average per-epoch objective decrease under CCD}},\n$$\n以浮点数形式表示。\n\n测试套件。使用以下四个测试案例，全部使用 $E = 50$ 轮次且无观测噪声：\n- 案例 1：$m = 200$, $p = 2$, $\\rho = 0.999$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 案例 2：$m = 200$, $p = 2$, $\\rho = 0.9$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 案例 3：$m = 200$, $p = 2$, $\\rho = 0.0$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 案例 4：$m = 200$, $p = 10$，其中第 1 列和第 2 列具有相关性 $\\rho = 0.999$ 和单位范数，第 3 到 10 列是具有单位范数的随机向量，与前两列及彼此之间近似不相关，$\\lambda = 0.05$, $x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$（长度为 $p$）。\n\n构造细节。对于 $p \\ge 2$，通过从独立同分布的标准正态分布中采样生成第一列 $a_1$ 的元素，并将其归一化至单位欧几里得范数。按如下方式生成 $a_2$\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\n其中 $w$ 具有独立同分布的标准正态分布元素，然后将 $a_2$ 归一化为单位范数。对于 $p > 2$，将 $j \\ge 3$ 的列 $a_j$ 生成为独立的标准正态向量，并归一化为单位范数。设置 $A = [a_1,\\dots,a_p]$ 和 $y = A x^\\star$。对于 CCD 和 RCD，均初始化 $x^{(0)} = 0$，将一个轮次定义为 $p$ 次更新，并在每个轮次后记录 $f(x)$。\n\n每轮次平均目标函数下降值。对每种方法，按如下方式计算每轮次的平均下降值\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\n其中 $x^{(t)}$ 是经过 $t$ 个轮次后的迭代点。\n\n最终输出格式。您的程序应生成单行输出，包含一个由四个浮点比率 $[r_1,r_2,r_3,r_4]$ 组成的逗号分隔列表，顺序与上述测试案例一致，并用方括号括起来。对所有随机抽样使用伪随机种子 $2025$。不涉及物理单位、角度单位或百分比；所有输出均为无量纲的浮点数。",
            "solution": "该问题经评估为**有效**。这是一个数值优化领域的适定 (well-posed) 且有科学依据的问题，具体关注坐标下降算法在 Lasso 目标函数上的性能特征。所有参数、流程和评估指标都得到了明确和形式化的定义。\n\n问题的核心是为 Lasso 目标函数实现并比较循环坐标下降 (CCD) 和随机坐标下降 (RCD)，该目标函数由下式给出：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\n在此，$x \\in \\mathbb{R}^p$ 是待优化的参数向量，$A \\in \\mathbb{R}^{m \\times p}$ 是感知矩阵，$y \\in \\mathbb{R}^m$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n坐标下降算法通过迭代地对单个坐标 $x_j$ 进行最小化来优化此函数，同时保持所有其他坐标 $x_k$（对于 $k \\neq j$）固定。坐标 $x_j$ 的一维子问题是最小化：\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\n展开目标函数，我们分离出依赖于 $x_j$ 的项：\n$$\n\\begin{aligned}\nf(x) = \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\n其中 $a_k$ 是 $A$ 的第 $k$ 列。为对 $x_j$ 进行最小化，我们可以忽略不依赖于它的项。子问题变为最小化：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\n展开范数平方项：\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\n问题规定所有列 $a_j$ 都被归一化为单位欧几里得范数，即 $\\|a_j\\|_2^2 = a_j^T a_j = 1$。这将子问题简化为：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\n这是一个关于 $x_j$ 的二次函数外加一个 $\\ell_1$ 范数惩罚项。$\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ 的解由软阈值算子给出，$z^* = S_\\lambda(c)$。在我们的例子中，$c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$。\n因此，坐标 $x_j$ 的更新规则是：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\n其中 $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$。为了提高计算效率，我们可以预先计算格拉姆矩阵 $A^T A$ 和向量 $A^T y$。令 $G = A^T A$ 和 $c_y = A^T y$。更新规则变为：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\n求和项可以写成 $(G x)_j - G_{jj} x_j$。由于 $G_{jj}=a_j^T a_j=1$，软阈值函数的参数为 $(c_y)_j - ((G x)_j - x_j)$。向量 $x$ 包含坐标的最近更新值。\n\n该算法按以下步骤进行：\n1.  **初始化**：为保证可复现性，将伪随机种子设为 $2025$。对每个测试案例，按规定构造矩阵 $A$ 和向量 $y$。列 $a_j$ 从独立同分布的标准正态分布中生成并进行归一化。前两列 $a_1$ 和 $a_2$ 被构造成具有指定的相关性结构。响应是无噪声的，$y=Ax^\\star$。预计算 $A^TA$ 和 $A^Ty$。将解的初始估计值设为 $x^{(0)} = 0$。\n\n2.  **循环坐标下降 (CCD)**：迭代 $E$ 个轮次。在每个轮次中，从 $j=1, \\dots, p$ 顺序更新坐标。\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    在更新 $x_j$ 时所使用的 $x_k$ 值是当前可用的最新值。\n\n3.  **随机坐标下降 (RCD)**：迭代 $E$ 个轮次。在每个轮次中，执行 $p$ 次更新。对于每次更新，有放回地从 $\\{1, \\dots, p\\}$ 中均匀随机选择一个坐标 $j$。应用与 CCD 相同的更新规则。\n\n4.  **评估**：对于 CCD 和 RCD，经过 $t$ 个轮次后的迭代向量记为 $x^{(t)}$。记录 $t=0, \\dots, E$ 的目标函数值 $f(x^{(t)})$。每轮次的平均目标函数下降值计算如下：\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    每个测试用例的最终报告值为比率 $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$。\n\n代码实现了这一逻辑，仔细遵循了 $A$ 的构造细节、CCD 和 RCD 的迭代更新方案，以及最终性能比率的计算。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p >= 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j >= 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd > 0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在观察到随机坐标下降（RCD）在经验上优于循环方法之后，下一步自然是对其收敛行为进行理论分析。这个思想实验  考虑了一个理想化的场景，其中数据矩阵 $A$ 的列是标准正交的（即 $A^\\top A = I$）。这种特殊结构使得 LASSO 目标函数可以按坐标完全分离，从而极大地简化了分析。通过解决这个问题，您会发现找到最优解的过程等价于概率论中经典的“赠券收集者问题”（Coupon Collector's Problem），从而为期望迭代次数提供一个优美而精确的解析表达式，加深对 RCD 工作机理的理解。",
            "id": "3472578",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO)，它在 $x \\in \\mathbb{R}^{n}$ 上最小化目标函数 $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列，使得 $A^{\\top}A = I_{n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$。令 $a_{j} \\in \\mathbb{R}^{m}$ 表示 $A$ 的第 $j$ 列。定义软阈值（收缩）算子 $S_{\\lambda}:\\mathbb{R}\\to\\mathbb{R}$ 为 $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\,\\max(|t| - \\lambda, 0)$。\n\n考虑以下随机坐标下降过程：从任意初始向量 $x^{(0)} \\in \\mathbb{R}^{n}$ 开始，在每次迭代 $t = 1,2,\\dots$ 中，独立且均匀随机地从 $\\{1,\\dots,n\\}$ 中选择一个坐标 $j_t$，并仅通过 $x_{j_{t}} \\leftarrow S_{\\lambda}(a_{j_{t}}^{\\top} b)$ 更新该坐标，同时在迭代 $t$ 中保持所有其他坐标不变。\n\n假设算法在第一次迭代 $T$ 时终止，届时 $x^{(T)}$ 等于 $F$ 的唯一最小值点。请计算直到终止所需迭代次数的精确期望值，并将其表示为关于 $n$ 的一个闭式解析表达式。无需四舍五入，也无物理单位。你的最终答案必须是单个表达式。",
            "solution": "该问题要求计算一个特定的随机坐标下降算法在矩阵 $A$ 的列是标准正交的条件下，找到 LASSO 目标函数 $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的唯一最小值点所需的期望迭代次数。\n\n首先，我们来分析目标函数 $F(x)$。平方 $\\ell_2$ 范数项可以展开为：\n$$ \\|A x - b\\|_{2}^{2} = (A x - b)^{\\top}(A x - b) = x^{\\top}A^{\\top}Ax - 2x^{\\top}A^{\\top}b + b^{\\top}b $$\n问题陈述矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列，这可以正式表示为 $A^{\\top}A = I_{n}$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。将此代入 $F(x)$ 的表达式中：\n$$ F(x) = \\frac{1}{2}(x^{\\top}I_n x - 2x^{\\top}A^{\\top}b + b^{\\top}b) + \\lambda \\|x\\|_{1} $$\n让我们用向量 $x$ 的分量 $x_j$ 来表示它。$x^{\\top}x = \\sum_{j=1}^{n} x_j^2$。$x^{\\top}A^{\\top}b = \\sum_{j=1}^{n} x_j (A^{\\top}b)_j$。向量 $A^{\\top}b$ 的第 $j$ 个分量是 $a_j^{\\top}b$，其中 $a_j$ 是 $A$ 的第 $j$ 列。$\\ell_1$ 范数是 $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$。\n将这些综合起来，目标函数变为：\n$$ F(x) = \\frac{1}{2}\\left(\\sum_{j=1}^{n} x_j^2 - 2\\sum_{j=1}^{n} x_j (a_j^{\\top}b) + \\|b\\|_2^2\\right) + \\lambda \\sum_{j=1}^{n} |x_j| $$\n我们可以重排求和项：\n$$ F(x) = \\sum_{j=1}^{n} \\left( \\frac{1}{2}x_j^2 - (a_j^{\\top}b)x_j + \\lambda|x_j| \\right) + \\frac{1}{2}\\|b\\|_2^2 $$\n标准正交条件 $A^{\\top}A=I_n$ 的一个关键推论是目标函数 $F(x)$ 是可分离的。它是一个关于每个坐标 $x_j$ 的 $n$ 个独立函数的和，外加一个不影响最小值点位置的常数项 $\\frac{1}{2}\\|b\\|_2^2$。\n\n为了找到 $F(x)$ 的唯一最小值点 $x^*$，我们可以独立地最小化和中的每一项。对于每个 $j \\in \\{1, \\dots, n\\}$，我们必须找到使一维函数 $f_j(z)$ 最小化的值 $x_j^*$：\n$$ f_j(z) = \\frac{1}{2}z^2 - (a_j^{\\top}b)z + \\lambda|z| $$\n函数 $f_j(z)$ 是凸函数。最优性条件是 $0$ 必须在 $f_j(z)$ 于 $z=x_j^*$ 处的次梯度中。次梯度为：\n$$ \\partial f_j(z) = z - a_j^{\\top}b + \\lambda \\partial|z| $$\n其中 $\\partial|z|$ 是绝对值函数的次梯度，当 $z \\neq 0$ 时为 $\\mathrm{sgn}(z)$，当 $z=0$ 时为 $[-1, 1]$。令次梯度包含 $0$：\n$$ a_j^{\\top}b - x_j^* \\in \\lambda \\partial|x_j^*| \\quad \\iff \\quad x_j^* = \\mathrm{prox}_{\\lambda|\\cdot|}(a_j^{\\top}b) $$\n这个近端算子对应于软阈值函数 $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\max(|t|-\\lambda, 0)$。因此，唯一全局最小值点 $x^*$ 的第 $j$ 个分量是：\n$$ x_j^* = S_{\\lambda}(a_j^{\\top}b) $$\n这对所有 $j=1, \\dots, n$ 都成立。\n\n现在，让我们分析给定的随机坐标下降算法。在每次迭代 $t=1, 2, \\dots$ 中，一个坐标 $j_t \\in \\{1, \\dots, n\\}$被均匀随机地选择。该坐标的更新规则指定为：\n$$ x_{j_t} \\leftarrow S_{\\lambda}(a_{j_t}^{\\top} b) $$\n根据我们上面的分析，这个更新规则将所选坐标 $x_{j_t}$ 设置为其全局最优值 $x_{j_t}^*$。任何给定坐标的更新都独立于所有其他坐标，也独立于其自身的先前值。\n\n算法被定义为在第一次迭代 $T$ 时终止，此时迭代值 $x^{(T)}$ 等于唯一最小值点 $x^*$。当且仅当集合 $\\{1, \\dots, n\\}$ 中的每个坐标 $j$ 都已被更新为其最优值时，$x^{(T)} = x^*$ 这个条件才满足。由于更新规则将所选坐标设置为其最终值，所以这个终止条件等价于集合 $\\{1, \\dots, n\\}$ 中的每个坐标索引都至少被选择过一次。初始向量 $x^{(0)}$ 是无关紧要的，因为任何坐标一旦被选中，就会立即被“修正”为其最终的最优值。\n\n因此，这个问题转化为了一个经典的概率谜题：赠券收集问题（Coupon Collector's Problem）。我们从一组 $n$ 个“赠券”（即坐标索引）中以均匀概率、有放回地重复抽取。我们要求集齐所有 $n$ 种不同赠券所需的期望抽取次数。\n\n令 $T$ 为所需的总迭代次数。我们可以将 $T$ 表示为随机变量之和：$T = T_1 + T_2 + \\dots + T_n$，其中 $T_k$ 是在已经选定了 $k-1$ 个不同坐标的情况下，选择第 $k$ 个新坐标所需的额外迭代次数。根据期望的线性性，$E[T] = \\sum_{k=1}^{n} E[T_k]$。\n\n让我们计算每个 $T_k$ 的期望值：\n- 对于 $k=1$：第一次迭代总是选择一个“新”坐标。所以，$T_1=1$ 且 $E[T_1]=1$。\n- 对于 $k=2$：在选定了 1 个不同坐标后，还剩下 $n-1$ 个未选中的坐标。在任何一次迭代中选中一个新坐标的概率是 $p_2 = \\frac{n-1}{n}$。为获得第一次成功所需的试验次数 $T_2$ 服从成功概率为 $p_2$ 的几何分布。其期望值为 $E[T_2] = \\frac{1}{p_2} = \\frac{n}{n-1}$。\n- 一般地，对于任意 $k \\in \\{1, \\dots, n\\}$，假设已经选定了 $k-1$ 个不同的坐标。剩余未选中的坐标数量是 $n-(k-1)$。在下一次迭代中选中一个新坐标的概率是 $p_k = \\frac{n-(k-1)}{n} = \\frac{n-k+1}{n}$。选中这第 $k$ 个新坐标所需的迭代次数 $T_k$ 服从成功概率为 $p_k$ 的几何分布。其期望值为 $E[T_k] = \\frac{1}{p_k} = \\frac{n}{n-k+1}$。\n\n总的期望迭代次数是这些期望值的和：\n$$ E[T] = \\sum_{k=1}^{n} E[T_k] = \\sum_{k=1}^{n} \\frac{n}{n-k+1} $$\n为了简化这个和式的外观，我们进行一次索引更换。令 $j = n-k+1$。当 $k=1$ 时，$j=n$。当 $k=n$ 时，$j=1$。和式变为：\n$$ E[T] = \\sum_{j=1}^{n} \\frac{n}{j} = n \\sum_{j=1}^{n} \\frac{1}{j} $$\n这就是期望迭代次数的最终表达式。和式 $\\sum_{j=1}^{n} \\frac{1}{j}$ 被称为第 $n$ 个调和数，记作 $H_n$。",
            "answer": "$$\n\\boxed{n \\sum_{k=1}^{n} \\frac{1}{k}}\n$$"
        },
        {
            "introduction": "标准的随机坐标下降算法通常依赖于基于最坏情况曲率估计（即 Lipschitz 常数）的保守步长，这可能会减慢收敛速度。为了在实践中获得更快的性能，我们可以采用能够更好地反映目标函数局部几何形状的自适应步长。这项实践  介绍了一种基于 Barzilai-Borwein (BB) 谱步长策略的强大自适应方法，并将其应用于随机坐标设置中。您将实现这一先进技术，并在标准方法难以处理的病态问题上测试其性能，从而探索在加速与稳定性之间的权衡。这个练习展示了如何通过随机地融入二阶信息来加速优化算法，这是现代大规模优化的一个核心主题。",
            "id": "3472576",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，该问题旨在最小化凸目标函数 $$F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\lVert x\\rVert_1$$ 其中 $x \\in \\mathbb{R}^n$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$ 且 $\\lambda > 0$。$F(x)$ 的平滑部分为 $$f(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2,$$ 其梯度为 $$\\nabla f(x) = A^\\top (A x - b).$$ 对于单个坐标 $j$，定义坐标级利普希茨常数 $$L_j = \\lVert A_{:,j} \\rVert_2^2,$$ 其中 $A_{:,j}$ 表示 $A$ 的第 $j$ 列。步长为 $\\alpha_j$ 的坐标级邻近梯度更新为 $$x_j^{\\text{new}} = S_{\\lambda \\alpha_j}\\!\\left(x_j - \\alpha_j\\, [\\nabla f(x)]_j\\right),$$ 其中 $S_{\\tau}(z) = \\operatorname{sign}(z)\\max\\{|z| - \\tau, 0\\}$ 是水平为 $\\tau$ 的软阈值算子。\n\n在随机坐标下降中，每次迭代都会均匀随机地选择一个坐标 $j$，并应用上述更新。基准选择 $\\alpha_j = 1/L_j$ 基于曲率提供了一个安全的步长。另一种选择是使用坐标级 Barzilai–Borwein (BB) 缩放，它通过随机坐标访问所引发的随机方式，利用连续差分来估计曲率。坐标 $j$ 的 BB 步长在概念上由下式给出 $$\\eta_j \\approx \\frac{\\Delta x_j}{\\Delta g_j},$$ 其中 $\\Delta x_j$ 是两次连续随机访问之间第 $j$ 个坐标的变化量，而 $\\Delta g_j$ 是在这些访问中 $f(x)$ 梯度第 $j$ 个分量的相应变化量。由于访问时间是随机的，这些差分是对局部曲率的随机测量。为确保稳定性，需将 BB 步长裁剪到一个由坐标曲率决定的窗口内，$$\\alpha_j \\in \\left[\\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}\\right],$$ 其中 $c_{\\min}$ 和 $c_{\\max}$ 是正常数。问题在于，对于病态矩阵，这种随机 BB 缩放是否比基准步长更能改善收敛性，如果是，在什么样的曲率窗口内它能保持稳定。\n\n从上述基本定义出发，实现一个程序，该程序：\n\n1. 通过组合随机正交因子和跨越一个几何级数的奇异值，构造具有指定条件数 $K$ 的合成矩阵 $A$。设 $m  n$。生成一个稀疏真实向量 $x^\\star \\in \\mathbb{R}^n$，其包含 $s$ 个从标准正态分布中抽取的非零项，其位置是随机选择的。测量向量 $b \\in \\mathbb{R}^m$ 被构造为 $b = A x^\\star + \\varepsilon$，其中 $\\varepsilon$ 是高斯噪声，其标准差被设定为 $\\lVert A x^\\star \\rVert_2 / \\sqrt{m}$ 的 1%。\n\n2. 对于给定的总迭代次数 $T$，您的程序必须对两种步长策略（基准和随机BB）运行随机坐标下降。两种方法必须使用相同的、预先生成的随机坐标序列，以进行公平比较。\n\n3. 对于每种测试情况，计算两个指标：\n   a. **相对改进 $I$**：$I = (F_{\\text{baseline}} - F_{\\text{BB}}) / \\max\\{F_{\\text{baseline}}, 10^{-12}\\}$，其中 $F_{\\text{baseline}}$ 和 $F_{\\text{BB}}$ 分别是基准方法和 BB 方法在 $T$ 次迭代后达到的最终目标函数值。\n   b. **稳定性 $S$**：一个布尔值，如果 BB 方法的目标函数值在至少 95% 的迭代中是单调非增的，则为 `true`。\n\n4. 最终输出应为一个包含四个 `[I, S]` 数据对的列表，对应于指定的四个测试用例。每个数据对中的 $I$ 应为浮点数，$S$ 应为布尔值 (`true` 或 `false`)。",
            "solution": "用户提供的问题是有效的。这是一个在数值优化领域中定义明确的计算任务，特别关注用于解决 LASSO 问题的随机坐标下降方法。该问题在既有的优化理论中有科学依据，其组成部分（目标函数、梯度、邻近算子、Barzilai-Borwein 步长）定义正确，并提供了实现所需的所有必要参数。任务是在不同条件下实现并比较两种算法变体，这是计算科学中一个标准且有意义的练习。\n\n在此，遵循数值优化和算法设计的原则，制定了一个详细的解决方案。\n\n### 1. 数学公式和预备知识\n\n问题的核心是 LASSO 目标函数：\n$$F(x) = \\frac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\lVert x\\rVert_1$$\n其中 $x \\in \\mathbb{R}^n$ 是优化变量，$A \\in \\mathbb{R}^{m \\times n}$ 是数据矩阵，$b \\in \\mathbb{R}^m$ 是观测向量，$\\lambda  0$ 是正则化参数。目标函数是一个光滑、可微的二次项 $f(x) = \\frac{1}{2}\\lVert A x - b\\rVert_2^2$ 和一个非光滑但凸的正则化项 $g(x) = \\lambda \\lVert x\\rVert_1$ 的和。\n\n坐标下降法一次只针对一个坐标 $j$ 迭代地最小化目标函数，同时保持所有其他坐标固定。坐标 $j$ 的更新规则源于应用于一维子问题的邻近梯度法。这产生了更新规则：\n$$x_j \\leftarrow S_{\\lambda \\alpha_j}\\!\\left(x_j - \\alpha_j\\, [\\nabla f(x)]_j\\right)$$\n其中 $S_{\\tau}(z)$ 是软阈值算子，$\\alpha_j$ 是步长，$[\\nabla f(x)]_j$ 是平滑部分梯度的第 $j$ 个分量：\n$$[\\nabla f(x)]_j = A_{:,j}^\\top (A x - b)$$\n其中 $A_{:,j}$ 是 $A$ 的第 $j$ 列。为了提高效率，我们维护残差 $r = Ax - b$。梯度分量则为 $[\\nabla f(x)]_j = A_{:,j}^\\top r$。当 $x_j$ 更新了 $\\Delta x_j$ 时，残差可以通过 $r \\leftarrow r + A_{:,j} \\Delta x_j$ 高效更新。\n\n### 2. 步长策略\n\n问题的核心在于步长 $\\alpha_j$ 的选择。\n\n**基准（利普希茨逆）步长：** 一个保证收敛的步长选择是梯度坐标级利普希茨常数 $L_j$ 的倒数。该常数是 $f(x)$ 沿着坐标 $j$ 的曲率的上界：\n$$L_j = \\lVert A_{:,j} \\rVert_2^2$$\n因此，基准步长为 $\\alpha_j = 1/L_j$。这个选择是保守的，但能确保目标函数的单调下降。\n\n**随机 Barzilai-Borwein (BB) 步长：** BB 方法使用梯度和变量的有限差分来近似 Hessian 矩阵，形成一个类似割线法的近似。在随机坐标下降的背景下，坐标 $j$ 的步长基于对该坐标连续访问所获得的信息。\n\n设 $k_1$ 和 $k_2$ 是坐标 $j$ 被选中的两个连续迭代索引。在这些访问之间，由于其他坐标的更新，系统状态（向量 $x$）会发生演变。BB 步长 $\\eta_j$ 在概念上是 $\\eta_j \\approx \\Delta x_j / \\Delta g_j$。根据问题描述，我们定义：\n-   $\\Delta x_j$：在第一次访问 $k_1$ *期间* 更新时 $x_j$ 发生的变化。设其为 $\\delta_j^{(k_1)} = x_j^{(k_1)} - x_j^{(k_1-1)}$。\n-   $\\Delta g_j$：在第一次访问（$k_1$）*开始*和第二次访问（$k_2$）*开始*之间，第 $j$ 个梯度分量的变化。即 $\\Delta g_j = [\\nabla f(x^{(k_2-1)})]_j - [\\nabla f(x^{(k_1-1)})]_j$。\n\n这个公式捕捉了两次访问之间对其他坐标的更新如何影响在坐标 $j$ 处感知到的曲率。由此产生的 BB 步长估计值为：\n$$\\eta_j = \\frac{\\delta_j^{(k_1)}}{[\\nabla f(x^{(k_2-1)})]_j - [\\nabla f(x^{(k_1-1)})]_j}$$\n为确保稳定性，仅当此步长为正（表示正确的曲率方向）时才使用，并将其裁剪到一个窗口内：\n$$\\alpha_j = \\operatorname{clip}\\left(\\eta_j, \\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}\\right)$$\n如果 $\\eta_j \\le 0$，我们回退到安全的基准步长 $\\alpha_j = 1/L_j$。对于任何坐标的第一次访问，我们也使用基准步长。\n\n### 3. 算法实现\n\n该实现将包括以下组成部分：\n\n-   **问题生成：** 一个函数，通过形成 $A = U \\Sigma V_m^\\top$ 来构造具有指定条件数 $K$ 的矩阵 $A$。其中，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是随机正交矩阵（来自随机矩阵的 QR 分解），而 $\\Sigma \\in \\mathbb{R}^{m \\times m}$ 是一个对角矩阵，其奇异值 $\\sigma_i$ 形成一个从 $1$ 到 $1/K$ 的几何级数。使用可复现的随机数生成器生成稀疏真实向量 $x^\\star$ 和测量向量 $b = A x^\\star + \\varepsilon$。\n\n-   **求解器函数：** 单个函数 `run_solver` 将同时实现随机坐标下降算法的基准版本和随机 BB 版本。它将接受一个 `method` 参数来切换两种行为。该函数将使用高效的残差更新策略。对于 ‘bb’ 方法，它将维护每个坐标的必要状态（上次访问时的梯度、上次访问时坐标的变化量），以计算 BB 步长。\n\n-   **主循环：** 主要的 `solve` 函数将遍历四个测试用例。对于每个用例，它将：\n    1.  使用固定种子生成特定的问题实例（$A, b, x^\\star$）以保证可复现性。\n    2.  生成一个单一、共享的随机坐标序列，供两个求解器使用，以进行公平比较。\n    3.  运行基准求解器以获得最终目标值 $F_{\\text{baseline}}$。\n    4.  运行随机 BB 求解器，该求解器返回其最终目标值 $F_{\\text{BB}}$ 和每次迭代的目标值历史记录。\n    5.  通过检查 BB 求解器目标历史中的单调下降频率是否至少为 $0.95$ 来计算稳定性指标 $S$。使用一个小的容差（$10^{-12}$）来考虑浮点不精确性。\n    6.  计算相对改进指标 $I = (F_{\\text{baseline}} - F_{\\text{BB}}) / \\max\\{F_{\\text{baseline}}, 10^{-12}\\}$。\n    7.  存储并格式化数据对 $[I, S]$ 以用于最终输出。\n\n所有随机过程都设置了种子，以确保按要求完全复现实验。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the full analysis as described in the problem.\n    It sets up test cases, generates data, runs solvers, and reports results.\n    \"\"\"\n\n    def soft_threshold(z, tau):\n        \"\"\"Soft-thresholding operator S_tau(z).\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def run_solver(A, b, lambda_, T, j_sequence, L, method, c_min=None, c_max=None):\n        \"\"\"\n        Runs Randomized Coordinate Descent for LASSO.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_ (float): The regularization parameter.\n            T (int): The number of iterations.\n            j_sequence (np.ndarray): The pre-generated sequence of coordinates.\n            L (np.ndarray): The coordinate-wise Lipschitz constants.\n            method (str): 'baseline' or 'bb'.\n            c_min (float, optional): Min clipping factor for BB step.\n            c_max (float, optional): Max clipping factor for BB step.\n\n        Returns:\n            A tuple containing:\n            - float: The final objective value.\n            - list or None: History of objective values (only for 'bb' method).\n        \"\"\"\n        m, n = A.shape\n        x = np.zeros(n)\n        r = -b.copy()\n\n        # Method-specific initializations for the BB variant\n        if method == 'bb':\n            obj_history = []\n            grad_last_visit = np.zeros(n)\n            delta_x_last_visit = np.zeros(n)\n            first_visit = np.ones(n, dtype=bool)\n\n        # Main RCD loop\n        for t in range(T):\n            j = j_sequence[t]\n            \n            # The gradient component is computed using the current residual\n            grad_j_current = A[:, j].T @ r\n\n            # --- Step size calculation ---\n            if method == 'baseline':\n                alpha_j = 1.0 / L[j]\n            elif method == 'bb':\n                if first_visit[j]:\n                    alpha_j = 1.0 / L[j]\n                else:\n                    delta_x_prev = delta_x_last_visit[j]\n                    grad_j_prev = grad_last_visit[j]\n                    delta_g = grad_j_current - grad_j_prev\n                    \n                    # Compute BB step, ensuring valid numerator and denominator\n                    if np.abs(delta_x_prev) > 1e-12 and np.abs(delta_g) > 1e-12:\n                        eta_j = delta_x_prev / delta_g\n                        # Enforce positivity and clipping as per problem description\n                        if eta_j > 0:\n                            alpha_j = np.clip(eta_j, c_min / L[j], c_max / L[j])\n                        else:\n                            alpha_j = 1.0 / L[j]  # Fallback to safe step\n                    else:\n                        alpha_j = 1.0 / L[j]  # Fallback if no information\n            \n            # --- Store information for the next BB step for this coordinate ---\n            if method == 'bb':\n                grad_last_visit[j] = grad_j_current\n                first_visit[j] = False\n                \n            # --- Proximal gradient update ---\n            x_j_old = x[j]\n            z = x_j_old - alpha_j * grad_j_current\n            x_j_new = soft_threshold(z, lambda_ * alpha_j)\n            \n            delta_x_j = x_j_new - x_j_old\n            \n            # --- Update state (vector x and residual r) ---\n            x[j] = x_j_new\n            r += A[:, j] * delta_x_j\n            \n            if method == 'bb':\n                delta_x_last_visit[j] = delta_x_j\n                # Track objective value for stability analysis\n                F_current = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n                obj_history.append(F_current)\n\n        # Calculate final objective value after T iterations\n        final_obj = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n        \n        if method == 'bb':\n            return final_obj, obj_history\n        else:  # baseline\n            return final_obj, None\n\n    # Test cases as defined in the problem statement\n    test_cases = [\n        {'m': 120, 'n': 300, 'K': 10, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n        {'m': 120, 'n': 300, 'K': 10**4, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n        {'m': 120, 'n': 300, 'K': 10**4, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.5},\n        {'m': 120, 'n': 300, 'K': 10**3, 's': 10, 'lambda': 0.5, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n    ]\n\n    results = []\n    master_seed = 42  # For full reproducibility\n\n    for case_idx, params in enumerate(test_cases):\n        # Use a reproducible RNG for each distinct test case\n        case_rng = np.random.default_rng(master_seed + case_idx)\n        \n        m, n, K = params['m'], params['n'], params['K']\n        s, lambda_ = params['s'], params['lambda']\n        T, c_min, c_max = params['T'], params['c_min'], params['c_max']\n\n        # 1. Construct matrix A with prescribed condition number K\n        U, _ = np.linalg.qr(case_rng.standard_normal(size=(m, m)))\n        V, _ = np.linalg.qr(case_rng.standard_normal(size=(n, n)))\n        s_vals = np.logspace(0, -np.log10(K), num=m)\n        Sigma = np.diag(s_vals)\n        A = U @ Sigma @ V[:, :m].T\n\n        # 2. Generate sparse ground-truth x_star and noisy measurements b\n        x_star = np.zeros(n)\n        indices = case_rng.choice(n, s, replace=False)\n        x_star[indices] = case_rng.standard_normal(s)\n        \n        b_clean = A @ x_star\n        noise = case_rng.standard_normal(m)\n        noise_std = 0.01 * np.linalg.norm(b_clean) / np.sqrt(m)\n        b = b_clean + noise_std * noise\n\n        # Precompute coordinate-wise Lipschitz constants L_j = ||A_j||^2\n        L = np.sum(A**2, axis=0)\n        # Add a small epsilon to avoid division by zero for null columns\n        L[L == 0] = 1e-12\n\n        # 3. Generate shared coordinate sequence for a fair comparison\n        j_sequence = case_rng.integers(0, n, size=T)\n\n        # Run baseline solver\n        F_baseline, _ = run_solver(A, b, lambda_, T, j_sequence, L, 'baseline')\n\n        # Run randomized BB solver\n        F_bb, obj_history = run_solver(A, b, lambda_, T, j_sequence, L, 'bb', c_min, c_max)\n        \n        # 4. Evaluate stability S of the BB solver\n        monotonic_decreases = 0\n        if T > 1:\n            for i in range(1, len(obj_history)):\n                if obj_history[i] = obj_history[i-1] + 1e-12:\n                    monotonic_decreases += 1\n            stability_fraction = monotonic_decreases / (T - 1)\n        else:\n            stability_fraction = 1.0\n        S = stability_fraction >= 0.95\n\n        # 5. Evaluate relative improvement I\n        I = (F_baseline - F_bb) / np.maximum(F_baseline, 1e-12)\n        \n        results.append([I, S])\n    \n    # Format the final output string exactly as required\n    formatted_results = [f\"[{i:.6f},{str(s).lower()}]\" for i, s in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}