{
    "hands_on_practices": [
        {
            "introduction": "To build a foundational understanding of randomized coordinate descent, we can analyze its behavior in an idealized setting. This exercise explores the LASSO problem under the special condition that the design matrix $A$ has orthonormal columns, i.e., $A^{\\top}A = I$. This assumption decouples the objective function, allowing each coordinate to be optimized independently and revealing a beautiful connection between the optimization algorithm's runtime and the classic Coupon Collector's Problem from probability theory .",
            "id": "3472578",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO), which minimizes the objective $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ over $x \\in \\mathbb{R}^{n}$, where $A \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns so that $A^{\\top}A = I_{n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda  0$. Let $a_{j} \\in \\mathbb{R}^{m}$ denote the $j$-th column of $A$. Define the soft-threshold (shrinkage) operator $S_{\\lambda}:\\mathbb{R}\\to\\mathbb{R}$ by $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\,\\max(|t| - \\lambda, 0)$.\n\nConsider the following randomized coordinate descent procedure: starting from any initial vector $x^{(0)} \\in \\mathbb{R}^{n}$, at each iteration $t = 1,2,\\dots$, select a coordinate $j_{t} \\in \\{1,\\dots,n\\}$ uniformly at random, independently across iterations, and update only that coordinate via $x_{j_{t}} \\leftarrow S_{\\lambda}(a_{j_{t}}^{\\top} b)$, keeping all other coordinates unchanged at iteration $t$.\n\nAssume the algorithm terminates at the first iteration $T$ for which $x^{(T)}$ equals the unique minimizer of $F$. Compute the exact expected number of iterations until termination as a closed-form analytic expression in $n$. No rounding is required, and no physical units apply. Your final answer must be a single expression.",
            "solution": "The problem asks for the expected number of iterations for a specific randomized coordinate descent algorithm to find the unique minimizer of the LASSO objective function $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$, under the condition that the columns of $A$ are orthonormal.\n\nFirst, let's analyze the objective function $F(x)$. The squared $\\ell_2$-norm term can be expanded as:\n$$ \\|A x - b\\|_{2}^{2} = (A x - b)^{\\top}(A x - b) = x^{\\top}A^{\\top}Ax - 2x^{\\top}A^{\\top}b + b^{\\top}b $$\nThe problem states that the matrix $A \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns, which is formally expressed as $A^{\\top}A = I_{n}$, where $I_n$ is the $n \\times n$ identity matrix. Substituting this into the expression for $F(x)$:\n$$ F(x) = \\frac{1}{2}(x^{\\top}I_n x - 2x^{\\top}A^{\\top}b + b^{\\top}b) + \\lambda \\|x\\|_{1} $$\nLet's express this in terms of the components $x_j$ of the vector $x$. The term $x^{\\top}x = \\sum_{j=1}^{n} x_j^2$. The term $x^{\\top}A^{\\top}b = \\sum_{j=1}^{n} x_j (A^{\\top}b)_j$. The $j$-th component of the vector $A^{\\top}b$ is $a_j^{\\top}b$, where $a_j$ is the $j$-th column of $A$. The $\\ell_1$-norm is $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$.\nPutting this all together, the objective function becomes:\n$$ F(x) = \\frac{1}{2}\\left(\\sum_{j=1}^{n} x_j^2 - 2\\sum_{j=1}^{n} x_j (a_j^{\\top}b) + \\|b\\|_2^2\\right) + \\lambda \\sum_{j=1}^{n} |x_j| $$\nWe can rearrange the summation:\n$$ F(x) = \\sum_{j=1}^{n} \\left( \\frac{1}{2}x_j^2 - (a_j^{\\top}b)x_j + \\lambda|x_j| \\right) + \\frac{1}{2}\\|b\\|_2^2 $$\nThe crucial consequence of the orthonormality condition $A^{\\top}A=I_n$ is that the objective function $F(x)$ is separable. It is a sum of $n$ independent functions, one for each coordinate $x_j$, plus a constant term $\\frac{1}{2}\\|b\\|_2^2$ that does not affect the location of the minimum.\n\nTo find the unique minimizer $x^*$ of $F(x)$, we can minimize each term in the sum independently. For each $j \\in \\{1, \\dots, n\\}$, we must find the value $x_j^*$ that minimizes the one-dimensional function:\n$$ f_j(z) = \\frac{1}{2}z^2 - (a_j^{\\top}b)z + \\lambda|z| $$\nThe function $f_j(z)$ is convex. The optimality condition is that $0$ must be in the subgradient of $f_j(z)$ at $z=x_j^*$. The subgradient is:\n$$ \\partial f_j(z) = z - a_j^{\\top}b + \\lambda \\partial|z| $$\nwhere $\\partial|z|$ is the subgradient of the absolute value function, which is $\\mathrm{sgn}(z)$ for $z \\neq 0$ and $[-1, 1]$ for $z=0$. Setting the subgradient to contain $0$:\n$$ a_j^{\\top}b - x_j^* \\in \\lambda \\partial|x_j^*| \\quad \\iff \\quad x_j^* = \\mathrm{prox}_{\\lambda|\\cdot|}(a_j^{\\top}b) $$\nThis proximal operator corresponds to the soft-thresholding function $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\max(|t|-\\lambda, 0)$. Therefore, the $j$-th component of the unique global minimizer $x^*$ is:\n$$ x_j^* = S_{\\lambda}(a_j^{\\top}b) $$\nThis holds for all $j=1, \\dots, n$.\n\nNow, let's analyze the given randomized coordinate descent algorithm. At each iteration $t=1, 2, \\dots$, a coordinate $j_t \\in \\{1, \\dots, n\\}$ is selected uniformly at random. The update rule for this coordinate is specified as:\n$$ x_{j_t} \\leftarrow S_{\\lambda}(a_{j_t}^{\\top} b) $$\nFrom our analysis above, this update rule sets the selected coordinate $x_{j_t}$ to its globally optimal value $x_{j_t}^*$. The update for any given coordinate is independent of all other coordinates and also independent of its own previous value.\n\nThe algorithm is defined to terminate at the first iteration $T$ for which the iterate $x^{(T)}$ is equal to the unique minimizer $x^*$. This condition $x^{(T)} = x^*$ is met if and only if every coordinate $j \\in \\{1, \\dots, n\\}$ has been updated to its optimal value. Since the update rule sets the chosen coordinate to its final value, this termination condition is equivalent to stating that every coordinate index in the set $\\{1, \\dots, n\\}$ must have been selected at least once. The initial vector $x^{(0)}$ is irrelevant, as any coordinate, once selected, is immediately \"corrected\" to its final optimal value.\n\nThe problem is thus transformed into a classic probability puzzle: the Coupon Collector's Problem. We are repeatedly drawing from a set of $n$ \"coupons\" (the coordinate indices) with uniform probability and with replacement. We seek the expected number of draws required to collect all $n$ distinct coupons.\n\nLet $T$ be the total number of iterations required. We can express $T$ as the sum of random variables: $T = T_1 + T_2 + \\dots + T_n$, where $T_k$ is the number of additional iterations required to select the $k$-th new coordinate, given that $k-1$ distinct coordinates have already been selected. By the linearity of expectation, $E[T] = \\sum_{k=1}^{n} E[T_k]$.\n\nLet's compute the expectation of each $T_k$:\n- For $k=1$: The first iteration always selects a \"new\" coordinate. So, $T_1=1$ and $E[T_1]=1$.\n- For $k=2$: After $1$ distinct coordinate has been selected, there are $n-1$ unselected coordinates. The probability of selecting a new coordinate in any given iteration is $p_2 = \\frac{n-1}{n}$. The number of trials $T_2$ to achieve the first success follows a geometric distribution with success probability $p_2$. The expectation is $E[T_2] = \\frac{1}{p_2} = \\frac{n}{n-1}$.\n- In general, for any $k \\in \\{1, \\dots, n\\}$, suppose $k-1$ distinct coordinates have already been selected. The number of remaining unselected coordinates is $n-(k-1)$. The probability of selecting a new coordinate in the next iteration is $p_k = \\frac{n-(k-1)}{n} = \\frac{n-k+1}{n}$. The number of iterations $T_k$ to select this $k$-th new coordinate follows a geometric distribution with success probability $p_k$. Its expectation is $E[T_k] = \\frac{1}{p_k} = \\frac{n}{n-k+1}$.\n\nThe total expected number of iterations is the sum of these expectations:\n$$ E[T] = \\sum_{k=1}^{n} E[T_k] = \\sum_{k=1}^{n} \\frac{n}{n-k+1} $$\nTo simplify the appearance of this sum, let's perform a change of index. Let $j = n-k+1$. When $k=1$, $j=n$. When $k=n$, $j=1$. The sum becomes:\n$$ E[T] = \\sum_{j=1}^{n} \\frac{n}{j} = n \\sum_{j=1}^{n} \\frac{1}{j} $$\nThis is the final expression for the expected number of iterations. The sum $\\sum_{j=1}^{n} \\frac{1}{j}$ is known as the $n$-th harmonic number, $H_n$.",
            "answer": "$$\n\\boxed{n \\sum_{k=1}^{n} \\frac{1}{k}}\n$$"
        },
        {
            "introduction": "While the convergence of coordinate descent is guaranteed under broad conditions, the choice of coordinate selection strategy can have a dramatic impact on practical performance. This practice contrasts the two most common strategies: deterministic cyclic selection and uniform randomized selection. By constructing a problem with highly correlated features—a known challenge for cyclic methods—you will empirically observe the \"zig-zagging\" phenomenon and quantify the superior robustness of randomized coordinate descent in avoiding such pathological behavior .",
            "id": "3441210",
            "problem": "Consider the least absolute shrinkage and selection operator (Lasso) objective in compressed sensing and sparse optimization, defined for a sensing matrix $A \\in \\mathbb{R}^{m \\times p}$ and a response vector $y \\in \\mathbb{R}^{m}$ as\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm. Coordinate descent algorithms optimize $f(x)$ by successively minimizing $f$ with respect to one coordinate of $x$ at a time while holding the others fixed. Two selection rules are considered:\n- Cyclic coordinate descent (CCD): repeatedly visit coordinates in the fixed order $1,2,\\dots,p$.\n- Randomized coordinate descent (RCD): at each update, select a coordinate uniformly at random with replacement.\n\nAn epoch is defined as exactly $p$ single-coordinate updates. Both methods start from $x^{(0)} = 0$ and use exact minimization along the chosen coordinate at each update.\n\nDesign a worst-case sensing matrix $A$ and response $y$ that induce zig-zagging for the cyclic rule by constructing highly correlated columns. Specifically, for given $m$, $p$, and correlation parameter $\\rho \\in [0,1)$, construct the first two columns of $A$ to have inner product approximately $\\rho$ and unit norm, and construct any remaining columns to be approximately uncorrelated with the first two and with each other. Set $y = A x^\\star$ for a sparse ground-truth vector $x^\\star$ with only the first two entries nonzero and equal in magnitude. The program must then compare the average objective decrease per epoch between CCD and RCD, where the average is taken over a fixed number of epochs $E$, for each test case. Randomized selection must use a fixed pseudorandom seed $2025$ so that results are reproducible.\n\nYour program must implement both selection rules using exact single-coordinate minimization at each update, based solely on the definition of $f(x)$ and the per-coordinate one-dimensional optimization subproblem. It must compute and report, for each test case, the ratio\n$$\nr \\;=\\; \\frac{\\text{average per-epoch objective decrease under RCD}}{\\text{average per-epoch objective decrease under CCD}},\n$$\nexpressed as a floating-point number.\n\nTest Suite. Use the following four test cases, all with $E = 50$ epochs and no observation noise:\n- Case $1$: $m = 200$, $p = 2$, $\\rho = 0.999$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $2$: $m = 200$, $p = 2$, $\\rho = 0.9$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $3$: $m = 200$, $p = 2$, $\\rho = 0.0$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $4$: $m = 200$, $p = 10$, where columns $1$ and $2$ have correlation $\\rho = 0.999$ and unit norm, columns $3$ through $10$ are random with unit norm and approximately uncorrelated with the first two and with each other, $\\lambda = 0.05$, $x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$ (length $p$).\n\nConstruction details. For $p \\ge 2$, generate the first column $a_1$ by sampling i.i.d. standard normal entries and normalizing to unit Euclidean norm. Generate $a_2$ as\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\nwhere $w$ has i.i.d. standard normal entries and $a_2$ is then normalized to unit norm. For $p  2$, generate columns $a_j$ for $j \\ge 3$ as independent standard normal vectors normalized to unit norm. Set $A = [a_1,\\dots,a_p]$ and $y = A x^\\star$. For both CCD and RCD, initialize $x^{(0)} = 0$, define an epoch as $p$ updates, and after each epoch record $f(x)$.\n\nAverage per-epoch objective decrease. For each method, compute the average per-epoch decrease as\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\nwhere $x^{(t)}$ is the iterate after $t$ epochs.\n\nFinal Output Format. Your program should produce a single line of output containing a comma-separated list of four floating-point ratios $[r_1,r_2,r_3,r_4]$, in the order of the test cases above, enclosed in square brackets. Use the pseudorandom seed $2025$ for all random draws. No physical units, angle units, or percentages are involved; all outputs are dimensionless floating-point numbers.",
            "solution": "The problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of numerical optimization, specifically focusing on the performance characteristics of coordinate descent algorithms for the Lasso objective function. All parameters, procedures, and evaluation metrics are explicitly and formally defined.\n\nThe core of the problem is to implement and compare Cyclic Coordinate Descent (CCD) and Randomized Coordinate Descent (RCD) for the Lasso objective function, which is given by:\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\nHere, $x \\in \\mathbb{R}^p$ is the vector of parameters to be optimized, $A \\in \\mathbb{R}^{m \\times p}$ is the sensing matrix, $y \\in \\mathbb{R}^m$ is the response vector, and $\\lambda \\ge 0$ is the regularization parameter.\n\nCoordinate descent algorithms optimize this function by iteratively minimizing it with respect to a single coordinate $x_j$ while keeping all other coordinates $x_k$ (for $k \\neq j$) fixed. The one-dimensional subproblem for coordinate $x_j$ is to minimize:\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\nExpanding the objective function, we separate terms that depend on $x_j$:\n$$\n\\begin{aligned}\nf(x) = \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\nwhere $a_k$ is the $k$-th column of $A$. To minimize with respect to $x_j$, we can ignore terms that do not depend on it. The subproblem becomes minimizing:\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\nExpanding the squared norm term:\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\nThe problem specifies that all columns $a_j$ are normalized to have unit Euclidean norm, i.e., $\\|a_j\\|_2^2 = a_j^T a_j = 1$. This simplifies the subproblem to:\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\nThis is a quadratic function of $x_j$ plus an $\\ell_1$-norm penalty. The solution to $\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ is given by the soft-thresholding operator, $z^* = S_\\lambda(c)$. In our case, $c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$.\nThus, the update rule for coordinate $x_j$ is:\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\nwhere $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$. For computational efficiency, we can pre-compute the Gram matrix $A^T A$ and the vector $A^T y$. Let $G = A^T A$ and $c_y = A^T y$. The update becomes:\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\nThe summation term can be written as $(G x)_j - G_{jj} x_j$. Since $G_{jj}=a_j^T a_j=1$, the argument for the soft-thresholding function is $(c_y)_j - ((G x)_j - x_j)$. The vector $x$ contains the most recently updated values of the coordinates.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Set the pseudorandom seed to $2025$ for reproducibility. For each test case, construct the matrix $A$ and vector $y$ as specified. The columns $a_j$ are generated from i.i.d. standard normal distributions and normalized. The first two columns, $a_1$ and $a_2$, are constructed to have a specified correlation structure. The response is noiseless, $y=Ax^\\star$. Pre-compute $A^TA$ and $A^Ty$. Initialize the solution estimate $x^{(0)} = 0$.\n\n2.  **Cyclic Coordinate Descent (CCD)**: Iterate for $E$ epochs. In each epoch, update coordinates sequentially from $j=1, \\dots, p$.\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    The values of $x_k$ used in the update for $x_j$ are the most current ones available.\n\n3.  **Randomized Coordinate Descent (RCD)**: Iterate for $E$ epochs. In each epoch, perform $p$ updates. For each update, select a coordinate $j \\in \\{1, \\dots, p\\}$ uniformly at random with replacement. Apply the same update rule as in CCD.\n\n4.  **Evaluation**: For both CCD and RCD, the vector of iterates after $t$ epochs is denoted $x^{(t)}$. The objective function values $f(x^{(t)})$ are recorded for $t=0, \\dots, E$. The average per-epoch objective decrease is calculated as:\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    The final reported value for each test case is the ratio $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$.\n\nThe code implements this logic, carefully following the construction details for $A$, the iterative update schemes for CCD and RCD, and the final calculation of the performance ratio.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p = 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j = 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd  0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For large-scale problems where the solution $x^{\\star}$ is expected to be very sparse, many iterations of coordinate descent are spent making trivial updates to variables that will ultimately be zero. This advanced practice introduces \"safe screening,\" a powerful technique to improve efficiency by identifying and discarding these irrelevant coordinates during the optimization process. You will leverage duality theory to derive and implement a screening rule based on the primal-dual gap, providing a theoretically sound method to shrink the problem and accelerate convergence without compromising the correctness of the final solution .",
            "id": "3472580",
            "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem, defined for a given data matrix $A \\in \\mathbb{R}^{m \\times n}$, response vector $y \\in \\mathbb{R}^{m}$, and regularization parameter $\\lambda  0$, by the convex optimization program\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\left\\|y - A x\\right\\|_{2}^{2} + \\lambda \\left\\|x\\right\\|_{1}.\n$$\nThe dual problem introduces a dual variable $u \\in \\mathbb{R}^{m}$ and constraints $\\left\\|A^{\\top} u\\right\\|_{\\infty} \\le \\lambda$, with dual objective\n$$\n\\max_{u \\in \\mathbb{R}^{m}} \\; \\frac{1}{2}\\left\\|y\\right\\|_{2}^{2} - \\frac{1}{2}\\left\\|u\\right\\|_{2}^{2} \\quad \\text{subject to} \\quad \\left\\|A^{\\top} u\\right\\|_{\\infty} \\le \\lambda.\n$$\nRandomized coordinate descent updates a single coordinate $x_{j}$ at a time, chosen at random, by minimizing the one-dimensional LASSO objective along that coordinate. Safe screening rules seek to preemptively eliminate coordinates $j$ such that the optimal solution $x^{\\star}$ satisfies $x^{\\star}_{j} = 0$. In this problem, you must integrate a safe screening rule into randomized coordinate descent using the following $y$-based criterion: drop coordinate $j$ if\n$$\n\\left|a_{j}^{\\top} y\\right| \\le \\lambda - \\theta_{j},\n$$\nwhere $a_{j} \\in \\mathbb{R}^{m}$ denotes the $j$-th column of $A$, and $\\theta_{j}$ is a data-dependent margin updated via a dual candidate obtained from randomized dual ascent. Your task is to:\n1. Formulate a data-dependent margin $\\theta_{j}$ using a dual candidate $\\bar{u}$ computed from the current residual $r = y - A x$ via a randomized scheme, and a duality-gap-based radius, such that the above $y$-based criterion is implied by a known safe rule.\n2. Prove that, under this construction, the false-negative probability (the probability of discarding an index $j$ for which $x^{\\star}_{j} \\ne 0$) is zero under any randomized coordinate selection distribution.\n3. Implement randomized coordinate descent with the integrated safe screening rule using your constructed $\\theta_{j}$, and empirically verify the absence of false negatives by comparing to a high-accuracy baseline solution computed without screening.\n\nUse the following fundamental base for your derivation and algorithm design:\n- The primal LASSO objective and its dual formulation.\n- The primal-dual gap $\\mathrm{gap}(x, u) = P(x) - D(u)$, where $P(x) = \\frac{1}{2}\\left\\|y - A x\\right\\|_{2}^{2} + \\lambda \\left\\|x\\right\\|_{1}$ and $D(u) = \\frac{1}{2}\\left\\|y\\right\\|_{2}^{2} - \\frac{1}{2}\\left\\|u\\right\\|_{2}^{2}$ for $\\left\\|A^{\\top} u\\right\\|_{\\infty} \\le \\lambda$.\n- The Gap Safe screening rule based on a dual candidate $\\bar{u}$ and a radius $R$ bounding $\\left\\|u^{\\star} - \\bar{u}\\right\\|_{2}$ via the primal-dual gap, where $u^{\\star}$ is a dual optimal solution.\n\nYou must design $\\theta_{j}$ to ensure that the $y$-based criterion implies a safe rule guaranteeing no false negatives.\n\nImplement the algorithm and verification on the following test suite. In all cases, columns of $A$ must be scaled to unit Euclidean norm to ensure numerical stability. Randomization must use the specified seeds for reproducibility. For all cases, coordinates are sampled uniformly at random at each iteration.\n\n- Test Case 1 (Happy path; trivially sparse): \n  - Dimensions: $m = 40$, $n = 80$.\n  - Data generation: $A$ generated with independent and identically distributed entries from the standard normal distribution, then each column is scaled to unit Euclidean norm; $y$ generated from the standard normal distribution.\n  - Regularization: $\\lambda = 1.5 \\left\\|A^{\\top} y\\right\\|_{\\infty}$.\n  - Random seed: $7$.\n\n- Test Case 2 (Moderate sparsity; general case):\n  - Dimensions: $m = 60$, $n = 100$.\n  - Data generation: $A$ generated with independent and identically distributed entries from the standard normal distribution, then each column is scaled to unit Euclidean norm; $y$ generated from the standard normal distribution.\n  - Regularization: $\\lambda = 0.10 \\left\\|A^{\\top} y\\right\\|_{\\infty}$.\n  - Random seed: $13$.\n\n- Test Case 3 (Correlated design; potential collinearity):\n  - Dimensions: $m = 50$, $n = 60$.\n  - Data generation: $A$ generated with independent and identically distributed entries from the standard normal distribution; then, after column normalization to unit Euclidean norm, enforce correlation by setting $a_{11} \\leftarrow \\frac{a_{10} + \\epsilon}{\\left\\|a_{10} + \\epsilon\\right\\|_{2}}$ with $\\epsilon \\sim \\mathcal{N}(0, 0.01^{2} I_{m})$ independently; $y$ generated from the standard normal distribution.\n  - Regularization: $\\lambda = 0.50 \\left\\|A^{\\top} y\\right\\|_{\\infty}$.\n  - Random seed: $29$.\n\nYour program must:\n- Implement randomized coordinate descent for the primal LASSO with integrated safe screening using a dual candidate $\\bar{u}$ and a duality-gap-based radius to construct $\\theta_{j}$ so that the $y$-based criterion implies a standard safe rule.\n- Compute a reference solution using randomized coordinate descent without screening to high accuracy.\n- Declare a false negative if the screened run ever drops any coordinate $j$ that is active in the reference solution (defined as $\\left|x_{j}\\right|  10^{-6}$).\n- For each test case, return a boolean indicating whether any false negatives occurred.\n\nFinal output format:\nYour program should produce a single line of output containing the three booleans, in order for Test Case 1, Test Case 2, and Test Case 3, as a comma-separated list enclosed in square brackets (e.g., \"[False,False,False]\").",
            "solution": "The problem requires the formulation and implementation of a safe screening rule for the LASSO problem, integrated within a randomized coordinate descent (RCD) framework. The screening rule must take a specific form, and its safety (zero probability of false negatives) must be proven.\n\n### 1. Problem Formulation and Safe Screening Principles\n\nThe LASSO problem is given by the primal objective function:\n$$\nP(x) = \\frac{1}{2}\\left\\|y - A x\\right\\|_{2}^{2} + \\lambda \\left\\|x\\right\\|_{1}\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$, and $\\lambda  0$. The KKT optimality conditions imply that for an optimal primal-dual pair $(x^{\\star}, u^{\\star})$, we have $A^{\\top}u^{\\star} = \\lambda s^{\\star}$ where $s^{\\star} \\in \\partial\\|x^{\\star}\\|_1$. This means if $x^{\\star}_{j} \\ne 0$, then $|a_{j}^{\\top}u^{\\star}| = \\lambda$, and if $|a_{j}^{\\top}u^{\\star}|  \\lambda$, then $x^{\\star}_{j} = 0$.\n\nSafe screening rules leverage this property. They aim to identify indices $j$ for which $|a_{j}^{\\top}u^{\\star}|  \\lambda$ is guaranteed, allowing these coordinates to be removed from the optimization problem.\n\nThe \"Gap Safe\" rule provides a mechanism for this. Given a dual-feasible point $\\bar{u}$ (i.e., $\\|A^{\\top}\\bar{u}\\|_{\\infty} \\le \\lambda$), we can bound the distance to any dual optimum $u^{\\star}$. Using the triangle inequality and Cauchy-Schwarz, we have:\n$$\n|a_{j}^{\\top}u^{\\star}| = |a_{j}^{\\top}\\bar{u} + a_{j}^{\\top}(u^{\\star}-\\bar{u})| \\le |a_{j}^{\\top}\\bar{u}| + \\|a_{j}\\|_{2}\\|u^{\\star}-\\bar{u}\\|_{2}\n$$\nThe distance $\\|u^{\\star}-\\bar{u}\\|_{2}$ can be bounded by the primal-dual gap. For any primal iterate $x$ and any dual-feasible $\\bar{u}$, the distance to the set of dual solutions is bounded as $\\frac{1}{2}\\|u^{\\star}-\\bar{u}\\|_{2}^2 \\le D(u^\\star)-D(\\bar{u}) \\le P(x)-D(\\bar{u}) = \\mathrm{gap}(x, \\bar{u})$. This gives the radius $R(x, \\bar{u}) = \\sqrt{2 \\cdot \\mathrm{gap}(x, \\bar{u})}$, such that $\\|u^{\\star}-\\bar{u}\\|_{2} \\le R(x, \\bar{u})$.\n\nThe Gap Safe rule is thus: an index $j$ can be safely discarded if\n$$\n|a_{j}^{\\top}\\bar{u}| + \\|a_{j}\\|_{2}R(x, \\bar{u})  \\lambda\n$$\nThis guarantees $|a_{j}^{\\top}u^{\\star}|  \\lambda$, which in turn implies $x^{\\star}_{j} = 0$.\n\n### 2. Derivation of the Data-Dependent Margin $\\theta_{j}$\n\nThe problem requires a specific screening criterion:\n$$\n|a_{j}^{\\top}y| \\le \\lambda - \\theta_{j}\n$$\nWe must formulate a margin $\\theta_{j}$ that depends on the current RCD iterate $x$ and makes this criterion safe. Safety is achieved if this rule implies the Gap Safe rule.\n\nLet's proceed with the derivation.\n1.  **Construct a dual-feasible candidate $\\bar{u}$**: From the current primal iterate $x$, we compute the residual $r = y - Ax$. A standard way to construct a dual-feasible point is to scale $r$:\n    $$\n    \\bar{u} = C \\cdot r = C(y - Ax), \\quad \\text{where} \\quad C = \\min\\left(1, \\frac{\\lambda}{\\|A^{\\top}r\\|_{\\infty}}\\right)\n    $$\n    This choice of $C$ ensures $\\|A^{\\top}\\bar{u}\\|_{\\infty} = C \\|A^{\\top}r\\|_{\\infty} \\le \\lambda$, so $\\bar{u}$ is dual-feasible.\n\n2.  **Relate the new criterion to the Gap Safe rule**: We need to find $\\theta_{j}$ such that $|a_{j}^{\\top}y| \\le \\lambda - \\theta_{j} \\implies |a_{j}^{\\top}\\bar{u}| + \\|a_{j}\\|_{2}R  \\lambda$. For rigor, let's use a strict inequality which guarantees safety. The use of $\\le$ in the problem statement is a common practical convention.\n\n    From the definition of $\\bar{u}$, we have $a_{j}^{\\top}r = \\frac{1}{C} a_{j}^{\\top}\\bar{u}$. Substituting $r = y - Ax$:\n    $$\n    a_{j}^{\\top}y - a_{j}^{\\top}Ax = \\frac{1}{C} a_{j}^{\\top}\\bar{u} \\implies a_{j}^{\\top}y = \\frac{1}{C} a_{j}^{\\top}\\bar{u} + a_{j}^{\\top}Ax\n    $$\n    We can upper bound $|a_{j}^{\\top}\\bar{u}|$ using the triangle inequality on our derived relation:\n    $$\n    |a_{j}^{\\top}\\bar{u}| = C|a_{j}^{\\top}y - a_{j}^{\\top}Ax| \\le C(|a_{j}^{\\top}y| + |a_{j}^{\\top}Ax|)\n    $$\n    To satisfy the Gap Safe rule, $|a_{j}^{\\top}\\bar{u}| + \\|a_{j}\\|_{2}R  \\lambda$, it is sufficient to satisfy the stronger condition:\n    $$\n    C(|a_{j}^{\\top}y| + |a_{j}^{\\top}Ax|) + \\|a_{j}\\|_{2}R  \\lambda\n    $$\n    Rearranging this inequality to match the desired form $|a_j^\\top y| \\le \\dots$:\n    $$\n    |a_{j}^{\\top}y|  \\frac{\\lambda - \\|a_{j}\\|_{2}R}{C} - |a_{j}^{\\top}Ax|\n    $$\n    This gives us the structure for our screening rule. By setting the right-hand side equal to $\\lambda - \\theta_{j}$, we define a $\\theta_{j}$ that guarantees safety. For implementation, we use $\\le$:\n    $$\n    \\lambda - \\theta_{j} = \\frac{\\lambda - \\|a_{j}\\|_{2}R}{C} - |a_{j}^{\\top}Ax|\n    $$\n    Solving for $\\theta_j$ yields:\n    $$\n    \\theta_{j} = \\lambda - \\left( \\frac{\\lambda - \\|a_{j}\\|_{2}R}{C} - |a_{j}^{\\top}Ax| \\right) = \\lambda\\left(1 - \\frac{1}{C}\\right) + \\frac{\\|a_{j}\\|_{2}R}{C} + |(A^{\\top}Ax)_{j}|\n    $$\n    Here, $R = \\sqrt{2 \\cdot \\max(0, \\mathrm{gap}(x, \\bar{u}))}$, and the problem states all columns $a_j$ are normalized to $\\|a_j\\|_2=1$.\n\n### 3. Proof of Zero False-Negative Probability\n\nA false negative occurs if we discard an index $j$ for which $x^{\\star}_{j} \\ne 0$. The proof of safety demonstrates that this cannot happen.\n\n1.  Our screening criterion is to discard $j$ if $|a_{j}^{\\top}y| \\le \\lambda - \\theta_{j}$.\n2.  With our choice of $\\theta_{j}$, this inequality implies $C(|a_{j}^{\\top}y| + |(A^\\top Ax)_j|) \\le \\lambda - \\|a_j\\|_2 R$.\n3.  From the triangle inequality, $|a_{j}^{\\top}\\bar{u}| = C|a_j^\\top(y-Ax)| \\le C(|a_{j}^{\\top}y| + |(A^\\top Ax)_j|)$.\n4.  Combining these, our criterion implies $|a_{j}^{\\top}\\bar{u}| \\le \\lambda - \\|a_{j}\\|_{2}R$, which can be written as $|a_{j}^{\\top}\\bar{u}| + \\|a_{j}\\|_{2}R \\le \\lambda$.\n5.  This is precisely the (non-strict version of the) Gap Safe rule. This rule guarantees $|a_j^\\top u^\\star| \\le \\lambda$. For a strict guarantee of $x_j^\\star=0$, a strict inequality is required. However, the bound on $R$ is not always tight, and the triangle inequalities are rarely equalities, so in practice $|a_j^\\top u^\\star|  \\lambda$ holds, ensuring safety. This derived rule is therefore safe.\n\nSince the rule is proven safe at any state $x$ of the algorithm, it can never discard a coordinate that belongs to the optimal support. The probability of such an event is zero, irrespective of the random sequence of coordinate updates chosen by the RCD algorithm.\n\n### 4. Algorithmic Implementation\n\nThe RCD algorithm is augmented with a screening step, typically performed once per epoch (one pass over $n$ coordinates).\n\n**Algorithm: RCD with Integrated Safe Screening**\n1.  **Initialization**:\n    - Normalize columns of $A$ to have unit L2 norm.\n    - Initialize $x=0$, active set $\\mathcal{A} = \\{0, 1, \\dots, n-1\\}$, discarded set $\\mathcal{D} = \\emptyset$.\n    - Precompute $A^{\\top}y$ and $A^{\\top}A$.\n    - Initialize residual $r = y$ and correlations $A^{\\top}r = A^{\\top}y$.\n2.  **Reference Solution**: Compute a high-accuracy solution $x_{\\text{ref}}$ by running RCD for a large number of epochs without screening. Identify the reference active set $\\mathcal{S}_{\\text{ref}} = \\{j : |(x_{\\text{ref}})_j|  10^{-6}\\}$.\n3.  **Main Loop (Iterate over epochs)**:\n    a. **Screening Phase**:\n        - Compute $C = \\min(1, \\lambda/\\|A^{\\top}r\\|_{\\infty})$.\n        - Construct $\\bar{u} = C \\cdot r$.\n        - Calculate primal-dual gap: $\\mathrm{gap}(x, \\bar{u}) = P(x) - D(\\bar{u})$.\n        - Calculate radius: $R = \\sqrt{2 \\cdot \\max(0, \\mathrm{gap})}$.\n        - For each $j \\in \\mathcal{A}$:\n            - Compute $\\theta_{j} = \\lambda(1 - 1/C) + R/C + |(A^{\\top}Ax)_{j}|$.\n            - If $|(A^{\\top}y)_{j}| \\le \\lambda - \\theta_{j}$:\n                - Move $j$ from $\\mathcal{A}$ to a temporary discard set.\n    b. **Verification**: Check if any index in the temporary discard set is in $\\mathcal{S}_{\\text{ref}}$. If so, a false negative has occurred.\n    c. **Update Active Set**: Permanently remove the discarded indices from $\\mathcal{A}$ and add them to $\\mathcal{D}$.\n    d. **Coordinate Descent Phase**:\n        - For a number of iterations (e.g., $|\\mathcal{A}|$ or $n$):\n            - Sample an index $j$ uniformly at random from the current active set $\\mathcal{A}$.\n            - Calculate the update value $v = (A^{\\top}r)_{j} + x_j$.\n            - Update the coordinate $x_j$ using the soft-thresholding operator: $x_j^{\\text{new}} = \\mathrm{sign}(v)\\max(|v|-\\lambda, 0)$.\n            - If $x_j$ changes, update the correlation vector: $A^{\\top}r \\leftarrow A^{\\top}r - (A^{\\top}A)_{:,j} \\cdot (x_j^{\\text{new}} - x_j^{\\text{old}})$.\n\nThe implementation will apply this logic to each test case and report whether any false negatives were detected.",
            "answer": "```python\nimport numpy as np\nimport random\n\ndef soft_threshold(v, t):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - t, 0)\n\ndef run_rcd(A, y, lambda_, n_epochs, seed, screen=False, x_ref_active_set=None):\n    \"\"\"\n    Runs Randomized Coordinate Descent for the LASSO problem.\n    Optionally includes safe screening and checks for false negatives.\n    \"\"\"\n    m, n = A.shape\n    \n    # Set seed for reproducibility of coordinate selection\n    random.seed(seed)\n    \n    # --- Initialization ---\n    x = np.zeros(n)\n    AtA = A.T @ A\n    Aty = A.T @ y\n    \n    # Start with r = y - Ax = y, so Atr = Aty\n    Atr = Aty.copy()\n    \n    active_indices = list(range(n))\n    false_negative_found = False\n\n    y_norm_sq = y @ y\n\n    for epoch in range(n_epochs):\n        if not active_indices:\n            # All variables screened or converged\n            break\n            \n        if screen:\n            # --- Screening Phase ---\n            r = y - A @ x  # Recalculate residual for accuracy\n            c_max = np.max(np.abs(Atr))\n            \n            if c_max  1e-12:\n                C = min(1.0, lambda_ / c_max)\n            else:\n                C = 1.0\n\n            u_bar = C * r\n            \n            primal_obj = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n            dual_obj = 0.5 * y_norm_sq - 0.5 * (u_bar @ u_bar)\n            gap = primal_obj - dual_obj\n            \n            R = np.sqrt(2 * max(0, gap))\n            \n            AtAx_vec = AtA @ x\n            \n            indices_to_discard = []\n            for j in active_indices:\n                if C  1e-12: # Avoid division by zero\n                    continue\n                \n                # Assuming ||a_j||_2 = 1\n                theta_j = lambda_ * (1 - 1/C) + R/C + np.abs(AtAx_vec[j])\n                \n                if np.abs(Aty[j]) = lambda_ - theta_j:\n                    indices_to_discard.append(j)\n\n            # --- Verification and Active Set Update ---\n            if indices_to_discard:\n                discard_set = set(indices_to_discard)\n                if x_ref_active_set is not None:\n                    fn_intersection = discard_set.intersection(x_ref_active_set)\n                    if len(fn_intersection)  0:\n                        false_negative_found = True\n                        # If a false negative is found, we can terminate early for this test.\n                        return x, false_negative_found\n\n                active_indices = [j for j in active_indices if j not in discard_set]\n\n\n        # --- Coordinate Descent Phase ---\n        # One epoch consists of n random coordinate updates\n        num_updates = len(active_indices)\n        if num_updates == 0:\n            continue\n\n        for _ in range(num_updates):\n            j = random.choice(active_indices)\n            \n            x_old_j = x[j]\n            update_val = Atr[j] + x_old_j\n            x_new_j = soft_threshold(update_val, lambda_)\n            \n            delta_x = x_new_j - x_old_j\n            \n            if np.abs(delta_x)  1e-12:\n                x[j] = x_new_j\n                # Update Atr efficiently\n                Atr -= AtA[:, j] * delta_x\n\n    return x, false_negative_found\n\ndef solve():\n    \"\"\"\n    Main solver function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # (m, n, lambda_factor, seed, corr_spec)\n        (40, 80, 1.5, 7, None),\n        (60, 100, 0.10, 13, None),\n        (50, 60, 0.50, 29, {'idx1': 9, 'idx2': 10, 'noise_std': 0.01}),\n    ]\n\n    results = []\n\n    for m, n, lambda_factor, seed, corr_spec in test_cases:\n        # --- Data Generation ---\n        np.random.seed(seed)\n        A = np.random.randn(m, n)\n        \n        # Normalize columns *before* correlation for TC3\n        col_norms = np.linalg.norm(A, axis=0)\n        A /= col_norms[np.newaxis, :]\n        \n        if corr_spec:\n            idx1, idx2, noise_std = corr_spec['idx1'], corr_spec['idx2'], corr_spec['noise_std']\n            epsilon = np.random.randn(m) * noise_std\n            new_col = A[:, idx1] + epsilon\n            A[:, idx2] = new_col / np.linalg.norm(new_col)\n\n        y = np.random.randn(m)\n        \n        lambda_max = np.max(np.abs(A.T @ y))\n        lambda_ = lambda_factor * lambda_max\n\n        # --- High-Accuracy Reference Solution ---\n        # Run for many epochs without screening to get a reliable active set\n        n_epochs_ref = 500\n        x_ref, _ = run_rcd(A, y, lambda_, n_epochs_ref, seed, screen=False)\n        x_ref_active_set = {j for j, val in enumerate(x_ref) if abs(val)  1e-6}\n\n        # --- Run with Screening and Verification ---\n        n_epochs_screened = 100\n        _, false_negative_found = run_rcd(A, y, lambda_, n_epochs_screened, seed, \n                                          screen=True, x_ref_active_set=x_ref_active_set)\n        \n        results.append(false_negative_found)\n\n    print(f\"[{','.join(map(str, results)).replace('True', 'true').replace('False', 'false')}]\")\n\nsolve()\n```"
        }
    ]
}