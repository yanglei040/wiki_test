{
    "hands_on_practices": [
        {
            "introduction": "理解随机坐标下降法的一个极佳起点是在一个理想化的场景下对其进行分析。 这道练习假设数据矩阵 $A$ 的列是正交的，这一简化使得 LASSO 目标函数可以分解为各个坐标上的独立子问题。通过这种设定，算法的收敛过程惊人地等价于经典的“赠券收集问题”(Coupon Collector's Problem)，从而可以精确计算出收敛所需的期望迭代次数，为我们提供了关于算法行为的清晰理论洞见。",
            "id": "3472578",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO)，它在 $x \\in \\mathbb{R}^{n}$ 上最小化目标函数 $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列，因此 $A^{\\top}A = I_{n}$，$b \\in \\mathbb{R}^{m}$ 且 $\\lambda > 0$。令 $a_{j} \\in \\mathbb{R}^{m}$ 表示 $A$ 的第 $j$ 列。定义软阈值（收缩）算子 $S_{\\lambda}:\\mathbb{R}\\to\\mathbb{R}$ 为 $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\,\\max(|t| - \\lambda, 0)$。\n\n考虑以下随机坐标下降过程：从任意初始向量 $x^{(0)} \\in \\mathbb{R}^{n}$ 开始，在每次迭代 $t = 1,2,\\dots$ 中，独立且均匀随机地从 $\\{1,\\dots,n\\}$ 中选择一个坐标 $j_{t}$，并仅通过 $x_{j_{t}} \\leftarrow S_{\\lambda}(a_{j_{t}}^{\\top} b)$ 更新该坐标，在迭代 $t$ 中保持所有其他坐标不变。\n\n假设算法在第一次迭代 $T$ 时终止，此时 $x^{(T)}$ 等于 $F$ 的唯一最小化子。计算直到终止所需的期望迭代次数，结果表示为关于 $n$ 的封闭形式解析表达式。不需要四舍五入，也没有物理单位。您的最终答案必须是单一表达式。",
            "solution": "该问题要求计算一个特定的随机坐标下降算法找到 LASSO 目标函数 $F(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的唯一最小化子所需的期望迭代次数，条件是 $A$ 的列是标准正交的。\n\n首先，我们来分析目标函数 $F(x)$。平方的$\\ell_2$范数项可以展开为：\n$$ \\|A x - b\\|_{2}^{2} = (A x - b)^{\\top}(A x - b) = x^{\\top}A^{\\top}Ax - 2x^{\\top}A^{\\top}b + b^{\\top}b $$\n问题陈述矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 具有标准正交列，这可以正式表示为 $A^{\\top}A = I_{n}$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。将此代入 $F(x)$ 的表达式中：\n$$ F(x) = \\frac{1}{2}(x^{\\top}I_n x - 2x^{\\top}A^{\\top}b + b^{\\top}b) + \\lambda \\|x\\|_{1} $$\n让我们用向量 $x$ 的分量 $x_j$ 来表示它。项 $x^{\\top}x = \\sum_{j=1}^{n} x_j^2$。项 $x^{\\top}A^{\\top}b = \\sum_{j=1}^{n} x_j (A^{\\top}b)_j$。向量 $A^{\\top}b$ 的第 $j$ 个分量是 $a_j^{\\top}b$，其中 $a_j$ 是 $A$ 的第 $j$ 列。$\\ell_1$范数是 $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$。将所有这些放在一起，目标函数变为：\n$$ F(x) = \\frac{1}{2}\\left(\\sum_{j=1}^{n} x_j^2 - 2\\sum_{j=1}^{n} x_j (a_j^{\\top}b) + \\|b\\|_2^2\\right) + \\lambda \\sum_{j=1}^{n} |x_j| $$\n我们可以重新排列求和：\n$$ F(x) = \\sum_{j=1}^{n} \\left( \\frac{1}{2}x_j^2 - (a_j^{\\top}b)x_j + \\lambda|x_j| \\right) + \\frac{1}{2}\\|b\\|_2^2 $$\n标准正交条件 $A^{\\top}A=I_n$ 的关键结果是目标函数 $F(x)$ 是可分的。它是 $n$ 个独立函数的和，每个函数对应一个坐标 $x_j$，外加一个不影响最小值位置的常数项 $\\frac{1}{2}\\|b\\|_2^2$。\n\n为了找到 $F(x)$ 的唯一最小化子 $x^*$，我们可以独立地最小化和中的每一项。对于每个 $j \\in \\{1, \\dots, n\\}$，我们必须找到使一维函数\n$$ f_j(z) = \\frac{1}{2}z^2 - (a_j^{\\top}b)z + \\lambda|z| $$\n最小化的值 $x_j^*$。\n函数 $f_j(z)$ 是凸函数。最优性条件是 $0$ 必须在 $z=x_j^*$ 处 $f_j(z)$ 的次梯度中。次梯度为：\n$$ \\partial f_j(z) = z - a_j^{\\top}b + \\lambda \\partial|z| $$\n其中 $\\partial|z|$ 是绝对值函数的次梯度，当 $z \\neq 0$ 时为 $\\mathrm{sgn}(z)$，当 $z=0$ 时为 $[-1, 1]$。将次梯度设置为包含 $0$：\n$$ a_j^{\\top}b - x_j^* \\in \\lambda \\partial|x_j^*| \\quad \\iff \\quad x_j^* = \\mathrm{prox}_{\\lambda|\\cdot|}(a_j^{\\top}b) $$\n这个邻近算子对应于软阈值函数 $S_{\\lambda}(t) = \\mathrm{sgn}(t)\\max(|t|-\\lambda, 0)$。因此，唯一全局最小化子 $x^*$ 的第 $j$ 个分量是：\n$$ x_j^* = S_{\\lambda}(a_j^{\\top}b) $$\n这对所有 $j=1, \\dots, n$ 都成立。\n\n现在，我们来分析给定的随机坐标下降算法。在每次迭代 $t=1, 2, \\dots$ 中，从 $\\{1, \\dots, n\\}$ 中均匀随机选择一个坐标 $j_t$。该坐标的更新规则指定为：\n$$ x_{j_t} \\leftarrow S_{\\lambda}(a_{j_t}^{\\top} b) $$\n根据我们上面的分析，这个更新规则将所选坐标 $x_{j_t}$ 设置为其全局最优值 $x_{j_t}^*$。任何给定坐标的更新独立于所有其他坐标，也独立于其自身的先前值。\n\n算法被定义为在第一次迭代 $T$ 时终止，此时迭代量 $x^{(T)}$ 等于唯一最小化子 $x^*$。当且仅当 $\\{1, \\dots, n\\}$ 中的每个坐标 $j$ 都已更新为其最优值时，条件 $x^{(T)} = x^*$ 才满足。由于更新规则将所选坐标设置为其最终值，此终止条件等价于说明集合 $\\{1, \\dots, n\\}$ 中的每个坐标索引都必须至少被选择过一次。初始向量 $x^{(0)}$ 是无关紧要的，因为任何坐标一旦被选中，就会立即被“修正”为其最终的最优值。\n\n因此，该问题转化为一个经典的概率难题：赠券收集者问题。我们从一组 $n$ 个“赠券”（坐标索引）中以均匀概率进行有放回的重复抽取。我们寻求收集所有 $n$ 种不同赠券所需的期望抽取次数。\n\n令 $T$ 为所需的总迭代次数。我们可以将 $T$ 表示为随机变量之和：$T = T_1 + T_2 + \\dots + T_n$，其中 $T_k$ 是在已经选定 $k-1$ 个不同坐标的情况下，选定第 $k$ 个新坐标所需的额外迭代次数。根据期望的线性性质，有 $E[T] = \\sum_{k=1}^{n} E[T_k]$。\n\n我们来计算每个 $T_k$ 的期望：\n- 对于 $k=1$：第一次迭代总是选择一个“新”坐标。所以，$T_1=1$ 且 $E[T_1]=1$。\n- 对于 $k=2$：选定 1 个不同坐标后，还剩下 $n-1$ 个未选定的坐标。在任何一次迭代中选定一个新坐标的概率是 $p_2 = \\frac{n-1}{n}$。为获得第一次成功所需的试验次数 $T_2$ 服从成功概率为 $p_2$ 的几何分布。其期望为 $E[T_2] = \\frac{1}{p_2} = \\frac{n}{n-1}$。\n- 一般地，对于任意 $k \\in \\{1, \\dots, n\\}$，假设已经选定了 $k-1$ 个不同的坐标。剩余未选定的坐标数量为 $n-(k-1)$。在下一次迭代中选定一个新坐标的概率为 $p_k = \\frac{n-(k-1)}{n} = \\frac{n-k+1}{n}$。选定这第 $k$ 个新坐标所需的迭代次数 $T_k$ 服从成功概率为 $p_k$ 的几何分布。其期望为 $E[T_k] = \\frac{1}{p_k} = \\frac{n}{n-k+1}$。\n\n总的期望迭代次数是这些期望的总和：\n$$ E[T] = \\sum_{k=1}^{n} E[T_k] = \\sum_{k=1}^{n} \\frac{n}{n-k+1} $$\n为了简化这个和式的外观，我们进行一次换元。令 $j = n-k+1$。当 $k=1$ 时，$j=n$。当 $k=n$ 时，$j=1$。和式变为：\n$$ E[T] = \\sum_{j=1}^{n} \\frac{n}{j} = n \\sum_{j=1}^{n} \\frac{1}{j} $$\n这是期望迭代次数的最终表达式。和式 $\\sum_{j=1}^{n} \\frac{1}{j}$ 被称为第 $n$ 个调和数，记为 $H_n$。",
            "answer": "$$\n\\boxed{n \\sum_{k=1}^{n} \\frac{1}{k}}\n$$"
        },
        {
            "introduction": "在实践中，为何我们常常偏爱随机坐标选择，而非确定性的循环选择？ 这个动手实践旨在通过构建一个“最坏情况”来回答这一问题，即设计一个具有高度相关特征的场景，该场景会诱导循环坐标下降法产生“锯齒状”收敛行为。通过亲手实现并比较两种方法，您将直观地体验到随机选择策略在面对现实世界中常见的相关特征时的鲁棒性，并量化其性能优势。",
            "id": "3441210",
            "problem": "考虑在压缩感知和稀疏优化中的最小绝对值收敛和选择算子 (Lasso) 目标函数，对于一个感知矩阵 $A \\in \\mathbb{R}^{m \\times p}$ 和一个响应向量 $y \\in \\mathbb{R}^{m}$，其定义为\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。坐标下降算法通过逐次最小化 $f$ 来优化 $f(x)$，即在每次迭代中，选择 $x$ 的一个坐标进行最小化，同时保持其他坐标固定。考虑两种选择规则：\n- 循环坐标下降 (CCD)：按照 $1,2,\\dots,p$ 的固定顺序重复访问坐标。\n- 随机坐标下降 (RCD)：在每次更新时，有放回地从所有坐标中均匀随机选择一个。\n\n一个轮次 (epoch) 定义为恰好 $p$ 次单坐标更新。两种方法都从 $x^{(0)} = 0$ 开始，并在每次更新时沿所选坐标方向进行精确最小化。\n\n设计一个最坏情况下的感知矩阵 $A$ 和响应 $y$，通过构造高度相关的列，以引发循环规则的“之”字形振荡（zig-zagging）行为。具体来说，对于给定的 $m$、$p$ 和相关性参数 $\\rho \\in [0,1)$，构造 $A$ 的前两列，使其内积近似为 $\\rho$ 且范数为单位长度，并构造其余列，使其与前两列及彼此之间近似不相关。对于一个稀疏的真实解向量 $x^\\star$（其中只有前两个元素非零且幅值相等），设置 $y = A x^\\star$。程序必须对每个测试用例，比较 CCD 和 RCD 在固定轮次 $E$ 内的每轮次目标函数平均下降量。随机选择必须使用固定的伪随机种子 $2025$，以确保结果是可复现的。\n\n您的程序必须仅基于 $f(x)$ 的定义和每个坐标的一维优化子问题，实现这两种选择规则，并在每次更新中使用精确的单坐标最小化。对于每个测试用例，程序必须计算并报告以下比率\n$$\nr \\;=\\; \\frac{\\text{RCD下每轮次的平均目标下降量}}{\\text{CCD下每轮次的平均目标下降量}},\n$$\n并以浮点数形式表示。\n\n测试套件。使用以下四个测试用例，所有用例均设置 $E = 50$ 轮次且无观测噪声：\n- 用例 1：$m = 200$, $p = 2$, $\\rho = 0.999$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 用例 2：$m = 200$, $p = 2$, $\\rho = 0.9$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 用例 3：$m = 200$, $p = 2$, $\\rho = 0.0$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$。\n- 用例 4：$m = 200$, $p = 10$，其中第 1 列和第 2 列的相关性为 $\\rho = 0.999$ 且具有单位范数，第 3 到第 10 列是随机生成的单位范数向量，并与前两列及彼此之间近似不相关，$\\lambda = 0.05$, $x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$（长度为 $p$）。\n\n构造细节。对于 $p \\ge 2$，通过采样独立同分布的标准正态分布条目并将其归一化至单位欧几里得范数来生成第一列 $a_1$。按如下方式生成 $a_2$\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\n其中 $w$ 具有独立同分布的标准正态条目，然后将 $a_2$ 归一化至单位范数。对于 $p > 2$，将第 $j \\ge 3$ 列 $a_j$ 生成为独立的、归一化至单位范数的标准正态向量。设置 $A = [a_1,\\dots,a_p]$ 和 $y = A x^\\star$。对于 CCD 和 RCD，均初始化 $x^{(0)} = 0$，将一个轮次定义为 $p$ 次更新，并在每轮次后记录 $f(x)$ 的值。\n\n每轮次目标函数的平均下降量。对每种方法，按如下公式计算每轮次的平均下降量\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\n其中 $x^{(t)}$ 是经过 $t$ 轮次后的迭代结果。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个由四个浮点数比率 $[r_1,r_2,r_3,r_4]$ 组成的逗号分隔列表，该列表按上述测试用例的顺序排列，并用方括号括起来。所有随机抽样均使用伪随机种子 $2025$。不涉及任何物理单位、角度单位或百分比；所有输出均为无量纲的浮点数。",
            "solution": "该问题被评估为**有效**。这是一个在数值优化领域中良构且有科学依据的问题，特别关注坐标下降算法在 Lasso 目标函数上的性能特征。所有参数、程序和评估指标都得到了明确且形式化的定义。\n\n问题的核心是为 Lasso 目标函数实现并比较循环坐标下降 (CCD) 和随机坐标下降 (RCD) 算法，该目标函数由下式给出：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\n此处，$x \\in \\mathbb{R}^p$ 是待优化的参数向量，$A \\in \\mathbb{R}^{m \\times p}$ 是感知矩阵，$y \\in \\mathbb{R}^m$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n坐标下降算法通过迭代地对函数进行最小化来优化此函数，即在每次迭代中，相对于单个坐标 $x_j$ 进行最小化，同时保持所有其他坐标 $x_k$ (对于 $k \\neq j$) 固定。坐标 $x_j$ 的一维子问题是最小化：\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\n展开目标函数，我们将依赖于 $x_j$ 的项分离出来：\n$$\n\\begin{aligned}\nf(x) = \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\n其中 $a_k$ 是 $A$ 的第 $k$ 列。为了对 $x_j$ 进行最小化，我们可以忽略不依赖于它的项。子问题变为最小化：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\n展开平方范数项：\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\n问题规定所有列 $a_j$ 都被归一化为单位欧几里得范数，即 $\\|a_j\\|_2^2 = a_j^T a_j = 1$。这可将子问题简化为：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\n这是一个关于 $x_j$ 的二次函数加上一个 $\\ell_1$ 范数惩罚项。$\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ 的解由软阈值算子给出，$z^* = S_\\lambda(c)$。在我们的问题中，$c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$。\n因此，坐标 $x_j$ 的更新规则是：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\n其中 $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$。为了提高计算效率，我们可以预先计算格拉姆矩阵 (Gram matrix) $A^T A$ 和向量 $A^T y$。令 $G = A^T A$ 和 $c_y = A^T y$。更新变为：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\n求和项可以写成 $(G x)_j - G_{jj} x_j$。由于 $G_{jj}=a_j^T a_j=1$，软阈值函数的参数为 $(c_y)_j - ((G x)_j - x_j)$。向量 $x$ 包含坐标的最新更新值。\n\n算法流程如下：\n1.  **初始化**：为保证可复现性，将伪随机种子设为 $2025$。对每个测试用例，按规定构造矩阵 $A$ 和向量 $y$。列向量 $a_j$ 通过从独立同分布的标准正态分布中采样并进行归一化生成。前两列 $a_1$ 和 $a_2$ 按指定的相关性结构构造。响应是无噪声的，$y=Ax^\\star$。预计算 $A^TA$ 和 $A^Ty$。初始化解的估计值 $x^{(0)} = 0$。\n\n2.  **循环坐标下降 (CCD)**：迭代 $E$ 轮。在每一轮中，从 $j=1, \\dots, p$ 顺序更新坐标。\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    用于更新 $x_j$ 的 $x_k$ 值是可获得的最新值。\n\n3.  **随机坐标下降 (RCD)**：迭代 $E$ 轮。在每一轮中，执行 $p$ 次更新。对于每次更新，有放回地从 $\\{1, \\dots, p\\}$ 中均匀随机选择一个坐标 $j$。应用与 CCD 相同的更新规则。\n\n4.  **评估**：对于 CCD 和 RCD，经过 $t$ 轮后的迭代向量记为 $x^{(t)}$。记录 $t=0, \\dots, E$ 时的目标函数值 $f(x^{(t)})$。每轮次目标函数的平均下降量计算如下：\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    每个测试用例最终报告的值是比率 $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$。\n\n代码实现了这一逻辑，仔细遵循了 $A$ 的构造细节、CCD 和 RCD 的迭代更新方案，以及性能比率的最终计算。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p >= 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j >= 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd > 0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准的随机坐标下降法使用基于 Lipschitz 常数的固定步长，这种选择虽然安全但可能过于保守。 本练习将引导您探索一种更高级的性能优化技术：自适应步长。您将实现一种坐标级别的 Barzilai-Borwein (BB) 方法，它利用最近的迭代信息来估计局部曲率，从而动态调整步长。这项实践将让您深入了解在病态问题中，这种更具适应性的策略如何能够加速收敛，并探索其稳定性条件。",
            "id": "3472576",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，该问题旨在最小化 $x \\in \\mathbb{R}^n$ 上的凸目标函数 $$F(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\lVert x\\rVert_1$$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda > 0$。$F(x)$ 的光滑部分为 $$f(x) = \\tfrac{1}{2}\\lVert A x - b\\rVert_2^2,$$ 其梯度为 $$\\nabla f(x) = A^\\top (A x - b).$$ 对于单个坐标 $j$，定义坐标级利普希茨常数 (coordinate-wise Lipschitz constant) $$L_j = \\lVert A_{:,j} \\rVert_2^2,$$ 其中 $A_{:,j}$ 表示 $A$ 的第 $j$ 列。步长为 $\\alpha_j$ 的坐标级近端梯度更新为 $$x_j^{\\text{new}} = S_{\\lambda \\alpha_j}\\!\\left(x_j - \\alpha_j\\, [\\nabla f(x)]_j\\right),$$ 其中 $S_{\\tau}(z) = \\operatorname{sign}(z)\\max\\{|z| - \\tau, 0\\}$ 是水平为 $\\tau$ 的软阈值算子。\n\n在随机坐标下降法中，每次迭代都会均匀随机地选择一个坐标 $j$，并应用上述更新。基准选择 $\\alpha_j = 1/L_j$ 基于曲率提供了一个安全的步长。另一种选择是使用坐标级 Barzilai–Borwein (BB) 缩放，它通过由随机坐标访问所引发的随机方式，利用逐次差分来估计曲率。坐标 $j$ 的 BB 步长在概念上由 $$\\eta_j \\approx \\frac{\\Delta x_j}{\\Delta g_j}$$ 给出，其中 $\\Delta x_j$ 是两次连续随机访问之间第 $j$ 个坐标的变化量，而 $\\Delta g_j$ 是在这些访问中 $f(x)$ 梯度第 $j$ 个分量的相应变化量。由于访问时间是随机的，这些差分是对局部曲率的随机测量。为确保稳定性，将 BB 步长裁剪到由坐标曲率确定的窗口内，即 $$\\alpha_j \\in \\left[\\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}\\right],$$ 其中 $c_{\\min}$ 和 $c_{\\max}$ 是正常数。问题在于，对于病态矩阵，这种随机 BB 缩放相比于基准步长是否能改善收敛性，如果可以，它在何种曲率窗口内能保持稳定。\n\n从上述基本定义出发，实现一个程序，该程序能够：\n\n1. 设 $m=120, n=300, T=15000$。构建一个具有预设条件数 $K$ 的矩阵 $A$。生成一个稀疏度为 $s$ 的真实解 $x^\\star$ 和带有噪声的观测值 $b$。\n2. 实现基准随机坐标下降法（步长 $\\alpha_j = 1/L_j$）和随机 Barzilai-Borwein (BB) 坐标下降法（步长裁剪至 $[\\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}]$ 窗口内）。\n3. 对于四个具有不同参数组合 ($K, s, \\lambda, c_{\\min}, c_{\\max}$) 的测试用例，在同一预生成坐标序列上运行两种算法 $T$ 次迭代。\n4. 计算并报告一个包含四个 `[I, S]` 对的列表，其中 $I$ 是 BB 方法相对于基准方法的最终目标函数值的相对改进量，而 $S$ 是一个布尔值，指示 BB 方法的收敛是否稳定（目标函数下降频率 $\\ge 95\\%$）。",
            "solution": "用户提供的问题是有效的。这是一个在数值优化领域中定义明确的计算任务，特别专注于使用随机坐标下降法来解决 LASSO 问题。该问题在成熟的优化理论中有科学依据，其组成部分（目标函数、梯度、近端算子、Barzilai-Borwein 步长）都得到了正确定义，并且提供了实现所需的所有必要参数。任务是在不同条件下实现并比较两种算法变体，这是计算科学中一项标准且有意义的练习。\n\n在此，我们遵循数值优化和算法设计的原则，制定一个详细的解决方案。\n\n### 1. 数学公式和预备知识\n\n问题围绕 LASSO 目标函数展开：\n$$F(x) = \\frac{1}{2}\\lVert A x - b\\rVert_2^2 + \\lambda \\lVert x\\rVert_1$$\n其中 $x \\in \\mathbb{R}^n$ 是优化变量，$A \\in \\mathbb{R}^{m \\times n}$ 是数据矩阵，$b \\in \\mathbb{R}^m$ 是观测向量，$\\lambda > 0$ 是正则化参数。目标函数是一个光滑、可微的二次项 $f(x) = \\frac{1}{2}\\lVert A x - b\\rVert_2^2$ 和一个非光滑但凸的正则化项 $g(x) = \\lambda \\lVert x\\rVert_1$ 的和。\n\n坐标下降法通过每次只针对单个坐标 $j$ 进行最小化来迭代求解，同时保持所有其他坐标固定。坐标 $j$ 的更新是从应用于一维子问题的近端梯度法推导出来的。这产生了更新规则：\n$$x_j \\leftarrow S_{\\lambda \\alpha_j}\\!\\left(x_j - \\alpha_j\\, [\\nabla f(x)]_j\\right)$$\n其中 $S_{\\tau}(z)$ 是软阈值算子，$\\alpha_j$ 是步长，$[\\nabla f(x)]_j$ 是光滑部分梯度的第 $j$ 个分量：\n$$[\\nabla f(x)]_j = A_{:,j}^\\top (A x - b)$$\n其中 $A_{:,j}$ 是 $A$ 的第 $j$ 列。为提高效率，我们维护残差 $r = Ax - b$。梯度分量则为 $[\\nabla f(x)]_j = A_{:,j}^\\top r$。当 $x_j$ 更新了 $\\Delta x_j$ 时，残差通过 $r \\leftarrow r + A_{:,j} \\Delta x_j$ 高效更新。\n\n### 2. 步长策略\n\n问题的核心在于步长 $\\alpha_j$ 的选择。\n\n**基准（利普希茨倒数）步长：** 一个保证收敛的步长选择是梯度坐标级利普希茨常数 $L_j$ 的倒数。该常数是 $f(x)$ 沿坐标 $j$ 的曲率的上界：\n$$L_j = \\lVert A_{:,j} \\rVert_2^2$$\n因此，基准步长为 $\\alpha_j = 1/L_j$。这个选择比较保守，但能确保目标函数的单调下降。\n\n**随机 Barzilai-Borwein (BB) 步长：** BB 方法使用梯度和变量的有限差分来近似 Hessian 矩阵，形成一种类割线近似。在随机坐标下降的背景下，坐标 $j$ 的步长基于对该坐标连续访问所获得的信息。\n\n设 $k_1$ 和 $k_2$ 是坐标 $j$ 被选中的两个连续迭代索引。在这两次访问之间，系统状态（向量 $x$）由于其他坐标的更新而演变。BB 步长 $\\eta_j$ 在概念上是 $\\eta_j \\approx \\Delta x_j / \\Delta g_j$。根据问题描述，我们定义：\n-   $\\Delta x_j$：在第一次访问 $k_1$ *期间* 更新 $x_j$ 所发生的变化。设其为 $\\delta_j^{(k_1)} = x_j^{(k_1)} - x_j^{(k_1-1)}$。\n-   $\\Delta g_j$：在第一次访问（$k_1$）*开始*时和第二次访问（$k_2$）*开始*时之间，第 $j$ 个梯度分量的变化。即 $\\Delta g_j = [\\nabla f(x^{(k_2-1)})]_j - [\\nabla f(x^{(k_1-1)})]_j$。\n\n这个公式捕捉了两次访问之间对其他坐标的更新如何影响在坐标 $j$ 处感知到的曲率。由此产生的 BB 步长估计值为：\n$$\\eta_j = \\frac{\\delta_j^{(k_1)}}{[\\nabla f(x^{(k_2-1)})]_j - [\\nabla f(x^{(k_1-1)})]_j}$$\n为确保稳定性，只有当此步长为正（表示正确的曲率方向）时才使用，并将其裁剪到一个窗口内：\n$$\\alpha_j = \\operatorname{clip}\\left(\\eta_j, \\frac{c_{\\min}}{L_j}, \\frac{c_{\\max}}{L_j}\\right)$$\n如果 $\\eta_j \\le 0$，我们则退回到安全的基准步长 $\\alpha_j = 1/L_j$。对于任何坐标的第一次访问，我们也使用基准步长。\n\n### 3. 算法实现\n\n实现将包括以下组件：\n\n-   **问题生成：** 一个函数，通过构造 $A = U \\Sigma V_m^\\top$ 来构建具有指定条件数 $K$ 的矩阵 $A$。其中，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是随机正交矩阵（来自随机矩阵的 QR 分解），$\\Sigma \\in \\mathbb{R}^{m \\times m}$ 是一个对角矩阵，其奇异值 $\\sigma_i$ 形成一个从 $1$到 $1/K$ 的等比数列。使用可复现的随机数生成器生成基准稀疏向量 $x^\\star$ 和测量向量 $b = A x^\\star + \\varepsilon$。\n\n-   **求解器函数：** 一个函数 `run_solver` 将同时实现随机坐标下降算法的基准版本和随机 BB 版本。它将接受一个 `method` 参数来切换两种行为。该函数将使用高效的残差更新策略。对于 'bb' 方法，它将为每个坐标维护必要的状态（上次访问时的梯度、上次访问时坐标的变化量）以计算 BB 步长。\n\n-   **主循环：** 主函数 `solve` 将遍历四个测试案例。对于每个案例，它将：\n    1.  使用固定种子生成具体的问题实例（$A, b, x^\\star$）以保证可复现性。\n    2.  生成一个单一、共享的随机坐标序列，供两个求解器使用，以进行公平比较。\n    3.  运行基准求解器以获得最终目标值 $F_{\\text{baseline}}$。\n    4.  运行随机 BB 求解器，它会返回其最终目标值 $F_{\\text{BB}}$ 以及每次迭代的目标值历史记录。\n    5.  通过检查 BB 求解器目标历史记录中的单调下降频率是否至少为 $0.95$ 来计算稳定性指标 $S$。使用一个小的容差（$10^{-12}$）来考虑浮点数不精确性。\n    6.  计算相对改进指标 $I = (F_{\\text{baseline}} - F_{\\text{BB}}) / \\max\\{F_{\\text{baseline}}, 10^{-12}\\}$。\n    7.  存储并格式化数据对 $[I, S]$ 以用于最终输出。\n\n所有的随机过程都设置了种子，以确保按要求完全复现实验。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the full analysis as described in the problem.\n    It sets up test cases, generates data, runs solvers, and reports results.\n    \"\"\"\n\n    def soft_threshold(z, tau):\n        \"\"\"Soft-thresholding operator S_tau(z).\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def run_solver(A, b, lambda_, T, j_sequence, L, method, c_min=None, c_max=None):\n        \"\"\"\n        Runs Randomized Coordinate Descent for LASSO.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_ (float): The regularization parameter.\n            T (int): The number of iterations.\n            j_sequence (np.ndarray): The pre-generated sequence of coordinates.\n            L (np.ndarray): The coordinate-wise Lipschitz constants.\n            method (str): 'baseline' or 'bb'.\n            c_min (float, optional): Min clipping factor for BB step.\n            c_max (float, optional): Max clipping factor for BB step.\n\n        Returns:\n            A tuple containing:\n            - float: The final objective value.\n            - list or None: History of objective values (only for 'bb' method).\n        \"\"\"\n        m, n = A.shape\n        x = np.zeros(n)\n        r = -b.copy()\n\n        # Method-specific initializations for the BB variant\n        if method == 'bb':\n            obj_history = []\n            grad_last_visit = np.zeros(n)\n            delta_x_last_visit = np.zeros(n)\n            first_visit = np.ones(n, dtype=bool)\n\n        # Main RCD loop\n        for t in range(T):\n            j = j_sequence[t]\n            \n            # The gradient component is computed using the current residual\n            grad_j_current = A[:, j].T @ r\n\n            # --- Step size calculation ---\n            if method == 'baseline':\n                alpha_j = 1.0 / L[j]\n            elif method == 'bb':\n                if first_visit[j]:\n                    alpha_j = 1.0 / L[j]\n                else:\n                    delta_x_prev = delta_x_last_visit[j]\n                    grad_j_prev = grad_last_visit[j]\n                    delta_g = grad_j_current - grad_j_prev\n                    \n                    # Compute BB step, ensuring valid numerator and denominator\n                    if np.abs(delta_x_prev) > 1e-12 and np.abs(delta_g) > 1e-12:\n                        eta_j = delta_x_prev / delta_g\n                        # Enforce positivity and clipping as per problem description\n                        if eta_j > 0:\n                            alpha_j = np.clip(eta_j, c_min / L[j], c_max / L[j])\n                        else:\n                            alpha_j = 1.0 / L[j]  # Fallback to safe step\n                    else:\n                        alpha_j = 1.0 / L[j]  # Fallback if no information\n            \n            # --- Store information for the next BB step for this coordinate ---\n            if method == 'bb':\n                grad_last_visit[j] = grad_j_current\n                first_visit[j] = False\n                \n            # --- Proximal gradient update ---\n            x_j_old = x[j]\n            z = x_j_old - alpha_j * grad_j_current\n            x_j_new = soft_threshold(z, lambda_ * alpha_j)\n            \n            delta_x_j = x_j_new - x_j_old\n            \n            # --- Update state (vector x and residual r) ---\n            x[j] = x_j_new\n            r += A[:, j] * delta_x_j\n            \n            if method == 'bb':\n                delta_x_last_visit[j] = delta_x_j\n                # Track objective value for stability analysis\n                F_current = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n                obj_history.append(F_current)\n\n        # Calculate final objective value after T iterations\n        final_obj = 0.5 * (r @ r) + lambda_ * np.sum(np.abs(x))\n        \n        if method == 'bb':\n            return final_obj, obj_history\n        else:  # baseline\n            return final_obj, None\n\n    # Test cases as defined in the problem statement\n    test_cases = [\n        {'m': 120, 'n': 300, 'K': 10, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n        {'m': 120, 'n': 300, 'K': 10**4, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n        {'m': 120, 'n': 300, 'K': 10**4, 's': 20, 'lambda': 0.05, 'T': 15000, 'c_min': 0.2, 'c_max': 1.5},\n        {'m': 120, 'n': 300, 'K': 10**3, 's': 10, 'lambda': 0.5, 'T': 15000, 'c_min': 0.2, 'c_max': 1.0},\n    ]\n\n    results = []\n    master_seed = 42  # For full reproducibility\n\n    for case_idx, params in enumerate(test_cases):\n        # Use a reproducible RNG for each distinct test case\n        case_rng = np.random.default_rng(master_seed + case_idx)\n        \n        m, n, K = params['m'], params['n'], params['K']\n        s, lambda_ = params['s'], params['lambda']\n        T, c_min, c_max = params['T'], params['c_min'], params['c_max']\n\n        # 1. Construct matrix A with prescribed condition number K\n        U, _ = np.linalg.qr(case_rng.standard_normal(size=(m, m)))\n        V, _ = np.linalg.qr(case_rng.standard_normal(size=(n, n)))\n        s_vals = np.logspace(0, -np.log10(K), num=m)\n        Sigma = np.diag(s_vals)\n        A = U @ Sigma @ V[:, :m].T\n\n        # 2. Generate sparse ground-truth x_star and noisy measurements b\n        x_star = np.zeros(n)\n        indices = case_rng.choice(n, s, replace=False)\n        x_star[indices] = case_rng.standard_normal(s)\n        \n        b_clean = A @ x_star\n        noise = case_rng.standard_normal(m)\n        noise_std = 0.01 * np.linalg.norm(b_clean) / np.sqrt(m)\n        b = b_clean + noise_std * noise\n\n        # Precompute coordinate-wise Lipschitz constants L_j = ||A_j||^2\n        L = np.sum(A**2, axis=0)\n        # Add a small epsilon to avoid division by zero for null columns\n        L[L == 0] = 1e-12\n\n        # 3. Generate shared coordinate sequence for a fair comparison\n        j_sequence = case_rng.integers(0, n, size=T)\n\n        # Run baseline solver\n        F_baseline, _ = run_solver(A, b, lambda_, T, j_sequence, L, 'baseline')\n\n        # Run randomized BB solver\n        F_bb, obj_history = run_solver(A, b, lambda_, T, j_sequence, L, 'bb', c_min, c_max)\n        \n        # 4. Evaluate stability S of the BB solver\n        monotonic_decreases = 0\n        if T > 1:\n            for i in range(1, len(obj_history)):\n                if obj_history[i] = obj_history[i-1] + 1e-12:\n                    monotonic_decreases += 1\n            stability_fraction = monotonic_decreases / (T - 1)\n        else:\n            stability_fraction = 1.0\n        S = stability_fraction >= 0.95\n\n        # 5. Evaluate relative improvement I\n        I = (F_baseline - F_bb) / np.maximum(F_baseline, 1e-12)\n        \n        results.append([I, S])\n    \n    # Format the final output string exactly as required\n    formatted_results = [f\"[{i:.6f},{str(s).lower()}]\" for i, s in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}