{
    "hands_on_practices": [
        {
            "introduction": "Understanding an algorithm's behavior begins with tracing its execution. This first practice provides a direct, hands-on calculation of the Least Angle Regression (LARS) algorithm. By working through the initial steps on a small, well-defined dataset, you will solidify your understanding of the core mechanics: identifying the most correlated predictors, defining the equiangular direction, and calculating the precise step size that brings a new predictor into the active set .",
            "id": "3456897",
            "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{3 \\times 3}$ and response vector $y \\in \\mathbb{R}^{3}$. Let\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n1 & 0 & 1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n3 \\\\ 1 \\\\ 0\n\\end{pmatrix}.\n$$\nAssume the columns of $X$ are normalized to have unit Euclidean norm. Using the classical Least Angle Regression (LAR) algorithm (no Least Absolute Shrinkage and Selection Operator (LASSO) modification), start from the zero coefficient vector and perform the first two steps of LAR. Work from the core definitions: at each step select the active set as those predictors with maximal absolute correlation with the current residual, move the fitted values in the equiangular direction with respect to the active predictors, and choose the step size so that a new predictor attains equal absolute correlation with the active set. Compute, for each of the first two steps:\n- the equiangular direction $u_{A}$,\n- the step size $\\gamma$ that reaches the next event,\n- and the coefficient updates $\\beta$ produced by taking that step.\n\nProvide exact analytic values for all intermediate quantities. For the final answer, report the exact value of the second step size, denoted $\\gamma_{2}$. Do not approximate or round; provide the exact closed-form expression for $\\gamma_{2}$.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a direct application of the Least Angle Regression (LAR) algorithm. We will now proceed with the solution.\n\nFirst, we must normalize the columns of the design matrix $X$ to have unit Euclidean norm, as stipulated. The given matrix is\n$$\nX_{unnormalized} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix}\n$$\nThe columns are $x_{un,1} = (1, 0, 1)^T$, $x_{un,2} = (1, 1, 0)^T$, and $x_{un,3} = (0, 1, 1)^T$. The Euclidean norm of each column is $\\|x_{un,j}\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$. We divide each column by $\\sqrt{2}$ to obtain the normalized design matrix $X$:\n$$\nX = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1 \\end{pmatrix}\n$$\nLet the normalized columns be denoted $x_1$, $x_2$, and $x_3$. The response vector is $y = (3, 1, 0)^T$.\n\nThe LAR algorithm starts with a zero coefficient vector $\\beta_0 = (0, 0, 0)^T$. The initial fitted values are $\\mu_0 = X\\beta_0 = (0, 0, 0)^T$, and the initial residual is $r_0 = y - \\mu_0 = y = (3, 1, 0)^T$.\n\n**Step 1: First LAR Step**\n\nFirst, we compute the initial correlations of the predictors with the residual $r_0$. The vector of correlations is $c = X^T r_0$:\n$$\nc = X^T r_0 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\\\ 1 \\end{pmatrix}\n$$\nThe absolute correlations are $|\\frac{3}{\\sqrt{2}}|$, $|\\frac{4}{\\sqrt{2}}|$, and $|\\frac{1}{\\sqrt{2}}|$. The maximum absolute correlation is $|c_2| = \\frac{4}{\\sqrt{2}}$. Therefore, the initial active set is $\\mathcal{A}_1 = \\{2\\}$.\n\nThe equiangular direction for the fitted values, $u_{\\mathcal{A}_1}$, is aligned with the most correlated predictor, signed by its correlation. Since $c_2 > 0$, the direction is simply the unit vector $x_2$.\n$$\nu_{\\mathcal{A}_1} = x_2 = \\frac{1}{\\sqrt{2}}(1, 1, 0)^T\n$$\nWe move the fitted values from $\\mu_0$ in this direction: $\\mu(\\gamma) = \\mu_0 + \\gamma u_{\\mathcal{A}_1} = \\gamma x_2$. The residual becomes $r(\\gamma) = r_0 - \\gamma x_2$.\nThe step size $\\gamma_1$ is the smallest positive $\\gamma$ for which some other predictor $j \\notin \\mathcal{A}_1$ has a correlation with $r(\\gamma)$ of the same magnitude as predictor $2$: $|x_j^T r(\\gamma)| = |x_2^T r(\\gamma)|$.\nThe correlations evolve as $c_j(\\gamma) = x_j^T (r_0 - \\gamma x_2) = c_j - \\gamma (x_j^T x_2)$.\nWe need to compute the inner products $x_j^T x_2$ for $j \\in \\{1, 3\\}$:\n$x_1^T x_2 = \\frac{1}{2}(1 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{2}$.\n$x_3^T x_2 = \\frac{1}{2}(0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{2}$.\nSince $x_2$ is normalized, $x_2^T x_2 = 1$.\nWe need to solve for $\\gamma > 0$ in the following equations:\n1. For $j=1$: $|c_1 - \\gamma (x_1^T x_2)| = |c_2 - \\gamma (x_2^T x_2)| \\implies |\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2}| = |\\frac{4}{\\sqrt{2}} - \\gamma|$.\n   - $\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2} = \\frac{4}{\\sqrt{2}} - \\gamma \\implies \\frac{\\gamma}{2} = \\frac{1}{\\sqrt{2}} \\implies \\gamma = \\sqrt{2}$.\n   - $\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2} = -(\\frac{4}{\\sqrt{2}} - \\gamma) \\implies \\frac{7}{\\sqrt{2}} = \\frac{3\\gamma}{2} \\implies \\gamma = \\frac{14}{3\\sqrt{2}} = \\frac{7\\sqrt{2}}{3}$.\n2. For $j=3$: $|c_3 - \\gamma (x_3^T x_2)| = |c_2 - \\gamma (x_2^T x_2)| \\implies |\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2}| = |\\frac{4}{\\sqrt{2}} - \\gamma|$.\n   - $\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2} = \\frac{4}{\\sqrt{2}} - \\gamma \\implies \\frac{\\gamma}{2} = \\frac{3}{\\sqrt{2}} \\implies \\gamma = 3\\sqrt{2}$.\n   - $\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2} = -(\\frac{4}{\\sqrt{2}} - \\gamma) \\implies \\frac{5}{\\sqrt{2}} = \\frac{3\\gamma}{2} \\implies \\gamma = \\frac{10}{3\\sqrt{2}} = \\frac{5\\sqrt{2}}{3}$.\nThe smallest positive value is $\\gamma_1 = \\sqrt{2}$.\n\nThe step size is $\\gamma_1 = \\sqrt{2}$. At the end of this step, the fitted value is $\\mu_1 = \\mu_0 + \\gamma_1 u_{\\mathcal{A}_1} = \\sqrt{2} x_2 = (1, 1, 0)^T$. The coefficient vector $\\beta_1$ must satisfy $X\\beta_1 = \\mu_1 = \\sqrt{2} x_2$. This implies $\\beta_1$ has a non-zero element only at index $2$, with value $\\sqrt{2}$.\n$$\n\\beta_1 = (0, \\sqrt{2}, 0)^T\n$$\n\n**Step 2: Second LAR Step**\n\nAt the start of the second step, the active set is $\\mathcal{A}_2 = \\{1, 2\\}$, as predictor $1$ has now achieved the same absolute correlation as predictor $2$.\nThe residual is $r_1 = y - \\mu_1 = (3, 1, 0)^T - (1, 1, 0)^T = (2, 0, 0)^T$.\nLet's verify the correlations with $r_1$:\n$c(r_1) = X^T r_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{2} \\\\ 0 \\end{pmatrix}$.\nAs expected, $|c_1| = |c_2| > |c_3|$. The signs of the correlations for the active set are $s_{\\mathcal{A}_2} = (\\text{sign}(c_1), \\text{sign}(c_2))^T = (1, 1)^T$.\n\nThe new equiangular direction $u_{\\mathcal{A}_2}$ is a unit vector that makes equal angles with $s_1 x_1$ and $s_2 x_2$. It is given by $u_{\\mathcal{A}_2} \\propto X_{\\mathcal{A}_2} (X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2}$. Let $X_{\\mathcal{A}_2} = [x_1, x_2]$.\n$X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2} = \\begin{pmatrix} x_1^T x_1 & x_1^T x_2 \\\\ x_2^T x_1 & x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix}$.\nThe inverse is $(X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2})^{-1} = \\frac{1}{1-(1/2)^2} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix}$.\nThe weights for the direction are $w = (X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}$.\nThe unnormalized direction is $u'_{\\mathcal{A}_2} = X_{\\mathcal{A}_2} w = \\frac{2}{3}x_1 + \\frac{2}{3}x_2 = \\frac{2}{3} (\\frac{1}{\\sqrt{2}}(1,0,1)^T + \\frac{1}{\\sqrt{2}}(1,1,0)^T) = \\frac{\\sqrt{2}}{3}(2,1,1)^T$.\nThe norm is $\\|u'_{\\mathcal{A}_2}\\| = \\frac{\\sqrt{2}}{3} \\sqrt{2^2+1^2+1^2} = \\frac{\\sqrt{2}\\sqrt{6}}{3} = \\frac{2\\sqrt{3}}{3}$.\nThe normalized equiangular direction is $u_{\\mathcal{A}_2} = \\frac{u'_{\\mathcal{A}_2}}{\\|u'_{\\mathcal{A}_2}\\|} = \\frac{\\frac{\\sqrt{2}}{3}(2,1,1)^T}{2\\sqrt{3}/3} = \\frac{\\sqrt{2}}{2\\sqrt{3}}(2,1,1)^T = \\frac{1}{\\sqrt{6}}(2,1,1)^T$.\n\nThe step size $\\gamma_2$ is found by determining when the inactive predictor $x_3$ attains the same absolute correlation as the active set. The fitted values move as $\\mu(\\gamma) = \\mu_1+\\gamma u_{\\mathcal{A}_2}$, and the residual as $r(\\gamma) = r_1 - \\gamma u_{\\mathcal{A}_2}$.\nCorrelations evolve as $c_j(\\gamma) = c_j(r_1) - \\gamma(x_j^T u_{\\mathcal{A}_2})$.\nWe need the inner products $x_j^T u_{\\mathcal{A}_2}$:\n$x_1^T u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(1,0,1)^T \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^T = \\frac{3}{\\sqrt{12}} = \\frac{\\sqrt{3}}{2}$.\n$x_2^T u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(1,1,0)^T \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^T = \\frac{3}{\\sqrt{12}} = \\frac{\\sqrt{3}}{2}$.\n$x_3^T u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(0,1,1)^T \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^T = \\frac{2}{\\sqrt{12}} = \\frac{1}{\\sqrt{3}}$.\nWe set $|c_3(\\gamma)| = |c_1(\\gamma)|$:\n$|c_3(r_1) - \\gamma(x_3^T u_{\\mathcal{A}_2})| = |c_1(r_1) - \\gamma(x_1^T u_{\\mathcal{A}_2})|$\n$|0 - \\gamma \\frac{1}{\\sqrt{3}}| = |\\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2}|$.\nAssuming $\\gamma$ is small enough that $\\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2} > 0$, we solve:\n$\\frac{\\gamma}{\\sqrt{3}} = \\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2} \\implies \\gamma (\\frac{1}{\\sqrt{3}} + \\frac{\\sqrt{3}}{2}) = \\sqrt{2} \\implies \\gamma (\\frac{2+3}{2\\sqrt{3}}) = \\sqrt{2} \\implies \\gamma \\frac{5}{2\\sqrt{3}} = \\sqrt{2}$.\nThis gives $\\gamma = \\frac{2\\sqrt{6}}{5}$.\nThe other possibility is $\\frac{\\gamma}{\\sqrt{3}} = -(\\sqrt{2} - \\gamma\\frac{\\sqrt{3}}{2})$, which yields $\\gamma=2\\sqrt{6}$. The step size $\\gamma_2$ is the minimum of these positive solutions.\nThus, the second step size is $\\gamma_2 = \\frac{2\\sqrt{6}}{5}$.\n\nFinally, we find the coefficient vector $\\beta_2$ after this step. The change in the active coefficients $(\\beta_1, \\beta_2)$ is proportional to the weights $w = (2/3, 2/3)^T$. The total coefficient update is $\\Delta\\beta_{\\mathcal{A}_2} = \\frac{\\gamma_2}{\\|u'_{\\mathcal{A}_2}\\|} w = \\frac{2\\sqrt{6}/5}{2\\sqrt{3}/3} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\frac{3\\sqrt{2}}{5} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2}/5 \\\\ 2\\sqrt{2}/5 \\end{pmatrix}$.\nThe new coefficient vector is $\\beta_2 = \\beta_1 + \\Delta\\beta = (0, \\sqrt{2}, 0)^T + (\\frac{2\\sqrt{2}}{5}, \\frac{2\\sqrt{2}}{5}, 0)^T$.\n$$\n\\beta_2 = \\left(\\frac{2\\sqrt{2}}{5}, \\sqrt{2}+\\frac{2\\sqrt{2}}{5}, 0\\right)^T = \\left(\\frac{2\\sqrt{2}}{5}, \\frac{7\\sqrt{2}}{5}, 0\\right)^T\n$$\nThe problem asks for the value of $\\gamma_2$.\n\nSummary of calculated quantities for the first two steps:\nStep 1:\n- Equiangular direction $u_{\\mathcal{A}_1} = \\frac{1}{\\sqrt{2}}(1, 1, 0)^T$\n- Step size $\\gamma_1 = \\sqrt{2}$\n- Coefficient vector $\\beta_1 = (0, \\sqrt{2}, 0)^T$\n\nStep 2:\n- Equiangular direction $u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{6}}(2, 1, 1)^T$\n- Step size $\\gamma_2 = \\frac{2\\sqrt{6}}{5}$\n- Coefficient vector $\\beta_2 = (\\frac{2\\sqrt{2}}{5}, \\frac{7\\sqrt{2}}{5}, 0)^T$",
            "answer": "$$\n\\boxed{\\frac{2\\sqrt{6}}{5}}\n$$"
        },
        {
            "introduction": "Having executed the LARS algorithm numerically, we now move to a more abstract level to develop a deeper geometric intuition. This exercise focuses on the heart of the LARS procedure: the equiangular direction. By deriving this direction and its associated correlations algebraically for a general two-predictor case, you will see precisely how the relationship between predictors, captured by their correlation $\\rho$, dictates the geometry of the solution path .",
            "id": "3456886",
            "problem": "Consider a linear model with a design matrix $X \\in \\mathbb{R}^{n \\times 2}$ whose columns $x_{1}$ and $x_{2}$ are standardized so that the Gram matrix $X^{\\top}X$ equals\n$$\nG \\equiv X^{\\top}X \\;=\\; \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix},\n$$\nwith $0 < \\rho < 1$. At a certain step of the Least Angle Regression (LARS) algorithm, the active set is $A = \\{1,2\\}$ with signs $s_{A} = (1,1)^{\\top}$. Define the equiangular direction $u_{A}$ to be the unique vector in the span of $X_{A} = [x_{1}, x_{2}]$ that has unit Euclidean norm and equal signed correlations with the active predictors, in the sense that $X_{A}^{\\top} u_{A} = A_{A} s_{A}$ for some scalar $A_{A} > 0$. Let $a \\equiv X^{\\top} u_{A}$ denote the vector of correlations of all predictors with the equiangular direction.\n\nStarting only from these definitions and basic linear algebra (Gram matrix, Euclidean norm, and linear independence), derive explicit expressions in terms of $\\rho$ for:\n- the coefficient vector $w_{A} \\in \\mathbb{R}^{2}$ such that $u_{A} = X_{A} w_{A}$,\n- the vector $a \\in \\mathbb{R}^{2}$.\n\nExpress your final answer as a single row matrix containing the two entries of $w_{A}$ followed by the two entries of $a$, in this order. No numerical rounding is required and no physical units are involved.",
            "solution": "The problem is checked for validity and found to be scientifically grounded, well-posed, objective, and internally consistent. We may proceed with the solution.\n\nThe problem asks for explicit expressions for the coefficient vector $w_A$ and the correlation vector $a$ associated with the equiangular direction $u_A$ at a specific step of the Least Angle Regression (LARS) algorithm.\n\nWe are given the following definitions and conditions:\n1. The design matrix is $X \\in \\mathbb{R}^{n \\times 2}$ with columns $x_1$ and $x_2$. The active set is $A = \\{1, 2\\}$, so the matrix of active predictors is $X_A = X$.\n2. The Gram matrix is $G = X_A^{\\top}X_A = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$, with $0 < \\rho < 1$.\n3. The equiangular direction $u_A$ is in the span of the columns of $X_A$, so $u_A = X_A w_A$ for some vector $w_A \\in \\mathbb{R}^2$.\n4. The vector $u_A$ has a unit Euclidean norm: $\\|u_A\\|_2^2 = 1$.\n5. The vector $u_A$ is equiangular with respect to the active predictors, with signs given by $s_A = (1, 1)^{\\top}$. This means $X_A^{\\top} u_A = A_A s_A$ for some scalar $A_A > 0$.\n6. The vector of correlations is defined as $a = X^{\\top} u_A$. Since $X_A = X$, this is $a = X_A^{\\top} u_A$.\n\nFrom definition $6$ and $5$, we can immediately relate $a$ to $A_A$ and $s_A$:\n$$a = X_A^{\\top} u_A = A_A s_A$$\nTo find $a$, we must first determine the value of the scalar $A_A$.\n\nLet us combine the given definitions. Substituting $u_A = X_A w_A$ into the equiangularity condition gives:\n$$X_A^{\\top} (X_A w_A) = A_A s_A$$\n$$(X_A^{\\top} X_A) w_A = A_A s_A$$\nUsing the definition of the Gram matrix $G$, this becomes a linear system for $w_A$:\n$$G w_A = A_A s_A$$\nThe determinant of $G$ is $\\det(G) = 1 \\cdot 1 - \\rho \\cdot \\rho = 1 - \\rho^2$. Since we are given $0 < \\rho < 1$, it follows that $0 < 1 - \\rho^2 < 1$, so $\\det(G) \\neq 0$. Thus, $G$ is invertible, and we can solve for $w_A$:\n$$w_A = G^{-1} (A_A s_A) = A_A (G^{-1} s_A)$$\nThis expresses $w_A$ in terms of the yet unknown scalar $A_A$.\n\nTo find $A_A$, we use the unit norm condition $\\|u_A\\|_2^2 = 1$. Let's express the norm in terms of $w_A$:\n$$\\|u_A\\|_2^2 = (X_A w_A)^{\\top} (X_A w_A) = w_A^{\\top} (X_A^{\\top} X_A) w_A = w_A^{\\top} G w_A = 1$$\nNow, substitute the expression $w_A = A_A G^{-1} s_A$ into this norm equation:\n$$(A_A G^{-1} s_A)^{\\top} G (A_A G^{-1} s_A) = 1$$\n$$A_A^2 (s_A^{\\top} (G^{-1})^{\\top}) G (G^{-1} s_A) = 1$$\nThe Gram matrix $G$ is symmetric, so its inverse $G^{-1}$ is also symmetric, meaning $(G^{-1})^{\\top} = G^{-1}$. The equation simplifies to:\n$$A_A^2 s_A^{\\top} G^{-1} G G^{-1} s_A = 1$$\n$$A_A^2 s_A^{\\top} G^{-1} s_A = 1$$\nWe can solve for $A_A^2$ as:\n$$A_A^2 = \\frac{1}{s_A^{\\top} G^{-1} s_A}$$\nTo proceed, we must compute $G^{-1}$ and the quadratic form $s_A^{\\top} G^{-1} s_A$. The inverse of the $2 \\times 2$ matrix $G$ is:\n$$G^{-1} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix}$$\nNow, we compute the vector $G^{-1}s_A$ with $s_A = (1, 1)^{\\top}$:\n$$G^{-1} s_A = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 - \\rho \\\\ 1 - \\rho \\end{pmatrix}$$\nFactoring $1 - \\rho^2 = (1-\\rho)(1+\\rho)$ and noting that $1-\\rho \\neq 0$, we simplify:\n$$G^{-1} s_A = \\frac{1-\\rho}{(1-\\rho)(1+\\rho)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nNext, we compute the scalar $s_A^{\\top} G^{-1} s_A$:\n$$s_A^{\\top} G^{-1} s_A = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\left( \\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{1+\\rho} (1 \\cdot 1 + 1 \\cdot 1) = \\frac{2}{1+\\rho}$$\nSubstituting this result back into the expression for $A_A^2$:\n$$A_A^2 = \\frac{1}{2/(1+\\rho)} = \\frac{1+\\rho}{2}$$\nSince we are given that $A_A > 0$, we take the positive square root:\n$$A_A = \\sqrt{\\frac{1+\\rho}{2}}$$\nNow, we have determined $A_A$ and can find the explicit expressions for $w_A$ and $a$.\n\nFor the coefficient vector $w_A$:\n$$w_A = A_A (G^{-1}s_A) = \\left(\\sqrt{\\frac{1+\\rho}{2}}\\right) \\left(\\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right)$$\n$$w_A = \\frac{\\sqrt{1+\\rho}}{\\sqrt{2}} \\frac{1}{(\\sqrt{1+\\rho})^2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}\\sqrt{1+\\rho}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2(1+\\rho)}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nSo, the components of $w_A$ are $w_1 = \\frac{1}{\\sqrt{2(1+\\rho)}}$ and $w_2 = \\frac{1}{\\sqrt{2(1+\\rho)}}$.\n\nFor the correlation vector $a$:\n$$a = A_A s_A = \\sqrt{\\frac{1+\\rho}{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nSo, the components of $a$ are $a_1 = \\sqrt{\\frac{1+\\rho}{2}}$ and $a_2 = \\sqrt{\\frac{1+\\rho}{2}}$.\n\nThe final answer consists of the four components $[w_1, w_2, a_1, a_2]$ arranged in a row matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2(1+\\rho)}} & \\frac{1}{\\sqrt{2(1+\\rho)}} & \\sqrt{\\frac{1+\\rho}{2}} & \\sqrt{\\frac{1+\\rho}{2}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Least Angle Regression is part of a family of sparse modeling techniques, and understanding its unique properties requires comparing it to its relatives. This advanced practice explores the subtle but crucial divergence between the LARS-lasso and infinitesimal forward stagewise regression paths. You will construct a scenario where the LARS-lasso path becomes non-monotone—a defining characteristic—and quantify the resulting deviation from the strictly incremental stagewise path, offering a sharp insight into their differing behaviors .",
            "id": "3456905",
            "problem": "Consider linear regression with design matrix $X \\in \\mathbb{R}^{n \\times 3}$ whose columns $x_{1}, x_{2}, x_{3}$ are centered and scaled so that $\\|x_{j}\\|_{2} = 1$ for $j \\in \\{1,2,3\\}$. The Gram matrix $G = X^{\\top} X \\in \\mathbb{R}^{3 \\times 3}$ is\n$$\nG \\;=\\; \\begin{pmatrix}\n1 & 0.5 & 0.5 \\\\\n0.5 & 1 & -0.4 \\\\\n0.5 & -0.4 & 1\n\\end{pmatrix}.\n$$\nAssume the initial correlations with the response are strictly positive and ordered as $X^{\\top} y = (c_{1}^{(0)}, c_{2}^{(0)}, c_{3}^{(0)})^{\\top}$ with $c_{1}^{(0)} > c_{2}^{(0)} > c_{3}^{(0)} > 0$, so that variables enter the active set in the order $1$, then $2$, then $3$, and all signs at entry are positive. Work in the infinitesimal-step limit for both Least Angle Regression with Least Absolute Shrinkage and Selection Operator modification (LARS-lasso) and infinitesimal forward stagewise regression, and parameterize coefficient paths by the $\\ell_{1}$-arc length $t$ so that each infinitesimal direction vector is normalized to unit $\\ell_{1}$-speed.\n\nAt the instant just after the third variable enters, the active set is $A = \\{1,2,3\\}$ with sign vector $s_{A} = (1,1,1)^{\\top}$. Let $d_{\\mathrm{LARS}} \\in \\mathbb{R}^{3}$ be the LARS-lasso equiangular direction in coefficient space, defined as the solution to $G d_{\\mathrm{LARS}} = s_{A}$, and let $d_{\\mathrm{FS}} \\in \\mathbb{R}^{3}$ be the infinitesimal forward stagewise direction, defined as the nonnegative least-squares direction that enforces $d_{\\mathrm{FS}} \\ge 0$ and, among all such directions, maintains equality of absolute correlations across the active set; equivalently, $d_{\\mathrm{FS}}$ is obtained by solving $G_{BB} d_{B} = s_{B}$ on a subset $B \\subseteq A$ with $d_{B} > 0$ and $d_{A \\setminus B} = 0$, chosen to satisfy the nonnegativity constraint.\n\nNormalize both directions to unit $\\ell_{1}$-speed,\n$$\nv_{\\mathrm{LARS}} \\;=\\; \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}}, \\qquad v_{\\mathrm{FS}} \\;=\\; \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}}.\n$$\nDefine the curvature-based discrepancy at this kink as the turning angle between these two unit-$\\ell_{1}$-speed tangent directions,\n$$\n\\theta \\;=\\; \\arccos\\!\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\, \\|v_{\\mathrm{FS}}\\|_{2}} \\right),\n$$\ninterpreted as the concentrated curvature of the coefficient path at the divergence event.\n\nCompute $\\theta$ exactly. Express your final answer as a closed-form analytic expression in radians (no numerical rounding).",
            "solution": "The problem is scientifically sound and well-posed. We will solve it by following the specified steps.\n\n**1. Calculate the LARS-lasso Direction ($d_{\\mathrm{LARS}}$)**\n\nThe LARS-lasso direction in coefficient space, $d_{\\mathrm{LARS}}$, is defined by the equiangular condition $G d_{\\mathrm{LARS}} = s_{A}$, where $G$ is the Gram matrix and $s_{A} = (1, 1, 1)^{\\top}$ is the sign vector for the active set $A=\\{1,2,3\\}$. We must solve this linear system. First, we compute the inverse of $G$.\n\nThe Gram matrix is $G = \\begin{pmatrix} 1 & 0.5 & 0.5 \\\\ 0.5 & 1 & -0.4 \\\\ 0.5 & -0.4 & 1 \\end{pmatrix}$.\nThe determinant is $\\det(G) = 1(1 - (-0.4)^2) - 0.5(0.5 - (0.5)(-0.4)) + 0.5(-0.2 - 0.5) = 1(0.84) - 0.5(0.7) - 0.5(0.7) = 0.84 - 0.7 = 0.14$.\nThe inverse is $G^{-1} = \\frac{1}{\\det(G)} \\text{adj}(G)$. The adjugate matrix is the transpose of the cofactor matrix.\n$$G^{-1} = \\frac{1}{0.14} \\begin{pmatrix} 1 - 0.16 & -(0.5 + 0.2) & -0.2 - 0.5 \\\\ -(0.5 + 0.2) & 1 - 0.25 & -(-0.4 - 0.25) \\\\ -0.2 - 0.5 & -(-0.4 - 0.25) & 1 - 0.25 \\end{pmatrix} = \\frac{1}{0.14} \\begin{pmatrix} 0.84 & -0.7 & -0.7 \\\\ -0.7 & 0.75 & 0.65 \\\\ -0.7 & 0.65 & 0.75 \\end{pmatrix}.$$\nNow we compute $d_{\\mathrm{LARS}} = G^{-1} s_{A}$:\n$$d_{\\mathrm{LARS}} = \\frac{1}{0.14} \\begin{pmatrix} 0.84 - 0.7 - 0.7 \\\\ -0.7 + 0.75 + 0.65 \\\\ -0.7 + 0.65 + 0.75 \\end{pmatrix} = \\frac{1}{0.14} \\begin{pmatrix} -0.56 \\\\ 0.70 \\\\ 0.70 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 5 \\\\ 5 \\end{pmatrix}.$$\n\n**2. Calculate the Infinitesimal Forward Stagewise Direction ($d_{\\mathrm{FS}}$)**\n\nInfinitesimal forward stagewise (FS) regression requires that all coefficients are non-decreasing. Since all variables entered with positive signs, their path coefficients must be non-negative. This implies the direction vector components must be non-negative, $d_{\\mathrm{FS}} \\ge 0$.\nOur calculated LARS direction $d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$ has a negative first component. This violates the FS constraint. Thus, for FS, the path must diverge. The first component of the direction vector is set to zero: $d_{\\mathrm{FS},1} = 0$. The active set for the FS update reduces to $B=\\{2,3\\}$. The direction for this subset, $d_{B}$, is found by solving the equiangular condition for the subset: $G_{BB} d_{B} = s_{B}$.\nHere, $G_{BB} = \\begin{pmatrix} 1 & -0.4 \\\\ -0.4 & 1 \\end{pmatrix}$ and $s_{B} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe inverse is $G_{BB}^{-1} = \\frac{1}{1 - (-0.4)^2} \\begin{pmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{pmatrix} = \\frac{1}{0.84} \\begin{pmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{pmatrix}$.\nSo, $d_{B} = \\frac{1}{0.84} \\begin{pmatrix} 1 & 0.4 \\\\ 0.4 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{0.84} \\begin{pmatrix} 1.4 \\\\ 1.4 \\end{pmatrix} = \\frac{1.4}{0.84} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{5}{3} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe components are positive, satisfying the constraint. The full FS direction vector is $d_{\\mathrm{FS}} = (0, 5/3, 5/3)^{\\top}$. For calculating the angle, we can use any positively scaled version, such as the proportional vector $d_{\\mathrm{FS}} \\propto (0, 1, 1)^{\\top}$.\n\n**3. Normalize Direction Vectors to Unit $\\ell_{1}$-Speed**\n\nWe normalize the raw direction vectors $d_{\\mathrm{LARS}}=(-4, 5, 5)^{\\top}$ and $d_{\\mathrm{FS}}=(0, 1, 1)^{\\top}$ to obtain the unit-$\\ell_{1}$-speed tangents, $v_{\\mathrm{LARS}}$ and $v_{\\mathrm{FS}}$.\n\nFor LARS-lasso:\n$\\|d_{\\mathrm{LARS}}\\|_{1} = |-4| + |5| + |5| = 14$.\n$$v_{\\mathrm{LARS}} = \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}} = \\frac{1}{14} (-4, 5, 5)^{\\top} = \\left(-\\frac{2}{7}, \\frac{5}{14}, \\frac{5}{14}\\right)^{\\top}.$$\n\nFor Forward Stagewise:\n$\\|d_{\\mathrm{FS}}\\|_{1} = |0| + |1| + |1| = 2$.\n$$v_{\\mathrm{FS}} = \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}} = \\frac{1}{2} (0, 1, 1)^{\\top} = \\left(0, \\frac{1}{2}, \\frac{1}{2}\\right)^{\\top}.$$\n\n**4. Compute the Angle $\\theta$**\n\nFinally, we compute the angle $\\theta$ between the two tangent vectors using the formula $\\theta = \\arccos\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} \\right)$.\n\nFirst, the inner product:\n$$\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle = \\left(-\\frac{2}{7}\\right)(0) + \\left(\\frac{5}{14}\\right)\\left(\\frac{1}{2}\\right) + \\left(\\frac{5}{14}\\right)\\left(\\frac{1}{2}\\right) = 0 + \\frac{5}{28} + \\frac{5}{28} = \\frac{10}{28} = \\frac{5}{14}.$$\n\nNext, the $\\ell_2$-norms:\n$$\\|v_{\\mathrm{LARS}}\\|_{2}^2 = \\left(-\\frac{2}{7}\\right)^2 + \\left(\\frac{5}{14}\\right)^2 + \\left(\\frac{5}{14}\\right)^2 = \\frac{4}{49} + \\frac{25}{196} + \\frac{25}{196} = \\frac{16}{196} + \\frac{50}{196} = \\frac{66}{196} = \\frac{33}{98}.$$\n$$\\|v_{\\mathrm{LARS}}\\|_{2} = \\sqrt{\\frac{33}{98}} = \\frac{\\sqrt{33}}{\\sqrt{49 \\cdot 2}} = \\frac{\\sqrt{33}}{7\\sqrt{2}}.$$\n$$\\|v_{\\mathrm{FS}}\\|_{2}^2 = 0^2 + \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}.$$\n$$\\|v_{\\mathrm{FS}}\\|_{2} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}.$$\n\nNow, we compute the cosine of the angle:\n$$\\cos(\\theta) = \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} = \\frac{\\frac{5}{14}}{\\left(\\frac{\\sqrt{33}}{7\\sqrt{2}}\\right) \\left(\\frac{1}{\\sqrt{2}}\\right)} = \\frac{\\frac{5}{14}}{\\frac{\\sqrt{33}}{7 \\cdot 2}} = \\frac{5/14}{\\sqrt{33}/14} = \\frac{5}{\\sqrt{33}}.$$\n\nThe angle $\\theta$ is the arccosine of this value:\n$$\\theta = \\arccos\\left(\\frac{5}{\\sqrt{33}}\\right).$$",
            "answer": "$$\n\\boxed{\\arccos\\left(\\frac{5}{\\sqrt{33}}\\right)}\n$$"
        }
    ]
}