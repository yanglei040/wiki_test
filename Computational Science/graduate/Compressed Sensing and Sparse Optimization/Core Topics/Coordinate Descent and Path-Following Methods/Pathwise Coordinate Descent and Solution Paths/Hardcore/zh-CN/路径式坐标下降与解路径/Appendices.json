{
    "hands_on_practices": [
        {
            "introduction": "坐标下降算法的核心是其一次更新一个变量的迭代过程。本练习提供了一个具体的机会，让您从第一性原理出发，为 LASSO 问题执行单坐标更新。通过手动计算更新量并使用 Karush-Kuhn-Tucker (KKT) 条件检验结果，您将对软阈值算子——许多稀疏优化算法的基本构件——获得深刻的、机械层面的理解。",
            "id": "3465818",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，即最小化目标函数 $$\\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1},$$ 其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$，且 $\\lambda  0$。路径坐标下降法每次更新一个坐标 $x_{j}$，同时保持其他坐标不变。设当前迭代值为 $x$，残差为 $r = y - A x$。对于一个固定的坐标索引 $j$，坐标下降更新将 $x_{j}$ 替换为通过在 LASSO 目标函数中仅改变 $x_{j}$ 而得到的一维子问题的唯一最小化子，然后相应地更新残差。\n\n从凸优化和可分离一维凸函数的次微分最优性的第一性原理出发，推导 $x_{j}$ 的坐标级最小化子的闭式解，并对以下具体实例执行单坐标更新：\n$$A = \\begin{pmatrix}\n1   0   2   -1 \\\\\n0   1   1   1 \\\\\n1   -1  0   2\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2 \\\\ -1 \\\\ 3\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix}\n1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2}\n\\end{pmatrix}, \\quad\n\\lambda = 1,$$\n更新索引为 $j = 2$ 的坐标（第二个坐标）。计算此单坐标更新后的更新残差 $r$ 以及索引 $j$ 的坐标级 Karush-Kuhn-Tucker (KKT) 残差。坐标级 KKT 残差定义如下：如果更新后的 $x_{j} \\neq 0$，则设 $$v_{j} = a_{j}^{\\top} r - \\lambda \\,\\mathrm{sign}(x_{j}),$$ 如果更新后的 $x_{j} = 0$，则设 $$v_{j} = \\max(|a_{j}^{\\top} r| - \\lambda,\\, 0),$$ 其中 $a_{j}$ 表示 $A$ 的第 $j$ 列。将最终答案以单个行矩阵的形式给出，其中包含更新后残差 $r$ 的分量（按顺序），后跟标量 $v_{j}$。所有值都需精确表示，无需四舍五入。",
            "solution": "用户要求推导 LASSO 问题的坐标级最小化子，并将其应用于一个具体实例。分析将分两个阶段进行：首先，从第一性原理出发对更新规则进行形式化推导；其次，将此规则应用于所提供的数据。\n\nLASSO 目标函数由下式给出：\n$$f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$$\n其中 $y \\in \\mathbb{R}^{m}$，$A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，且 $\\lambda  0$。路径坐标下降法每次更新一个坐标 $x_j$，同时保持所有其他坐标 $x_k$（其中 $k \\neq j$）固定。为了找到 $x_j$ 的更新，我们求解以下一维优化问题：\n$$ \\min_{z \\in \\mathbb{R}} f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_n) $$\n我们将目标表示为 $z$ 的函数。$L_1$ 范数是可分离的：$\\|x\\|_1 = \\sum_{k \\neq j} |x_k| + |z|$。二次项可以展开为：\n$$ \\|y - Ax\\|_2^2 = \\left\\|y - \\sum_{k=1}^{n} x_k a_k\\right\\|_2^2 = \\left\\|y - \\sum_{k \\neq j} x_k a_k - z a_j\\right\\|_2^2 $$\n其中 $a_k$ 是 $A$ 的第 $k$ 列。令部分残差为 $r^{(j)} = y - \\sum_{k \\neq j} x_k a_k$。关于 $z$ 的优化问题变为：\n$$ \\min_{z \\in \\mathbb{R}} \\left( \\frac{1}{2} \\|r^{(j)} - z a_j\\|_2^2 + \\lambda |z| \\right) $$\n其他项 $\\lambda \\sum_{k \\neq j} |x_k|$ 相对于 $z$ 是常数，在最小化过程中可以忽略。我们展开平方范数：\n$$ g(z) = \\frac{1}{2} (r^{(j)} - z a_j)^{\\top}(r^{(j)} - z a_j) + \\lambda |z| = \\frac{1}{2} \\left( \\|r^{(j)}\\|_2^2 - 2z (a_j)^{\\top}r^{(j)} + z^2 \\|a_j\\|_2^2 \\right) + \\lambda |z| $$\n该目标函数 $g(z)$ 是一个严格凸二次函数与一个凸绝对值函数之和，因此它是严格凸的，并拥有唯一的最小化子。通过将 $g(z)$ 的次梯度设为零来找到最小化子。次梯度为：\n$$ \\partial g(z) = z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} + \\lambda \\partial|z| $$\n其中 $\\partial|z|$ 是绝对值函数的次梯度：如果 $z \\neq 0$，则 $\\partial|z| = \\mathrm{sign}(z)$；如果 $z=0$，则 $\\partial|z| \\in [-1, 1]$。最优性条件是 $0 \\in \\partial g(z)$。\n\n情况 1: $z  0$。条件是 $z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} + \\lambda = 0$。这得出 $z = \\frac{(a_j)^{\\top}r^{(j)} - \\lambda}{\\|a_j\\|_2^2}$。该解在 $z0$ 时有效，这意味着 $(a_j)^{\\top}r^{(j)}  \\lambda$。\n\n情况 2: $z  0$。条件是 $z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} - \\lambda = 0$。这得出 $z = \\frac{(a_j)^{\\top}r^{(j)} + \\lambda}{\\|a_j\\|_2^2}$。该解在 $z0$ 时有效，这意味着 $(a_j)^{\\top}r^{(j)}  -\\lambda$。\n\n情况 3: $z = 0$。条件是 $0 \\in -(a_j)^{\\top}r^{(j)} + \\lambda [-1, 1]$。这等价于 $(a_j)^{\\top}r^{(j)} \\in [-\\lambda, \\lambda]$，或者 $|(a_j)^{\\top}r^{(j)}| \\le \\lambda$。\n\n这三种情况定义了软阈值算子 (soft-thresholding operator)，$S_{\\tau}(\\rho) = \\mathrm{sign}(\\rho)\\max(|\\rho|-\\tau, 0)$。最优的 $z$（我们记作 $x_j^{\\text{new}}$）由下式给出：\n$$ x_j^{\\text{new}} = \\frac{S_{\\lambda}((a_j)^{\\top}r^{(j)})}{\\|a_j\\|_2^2} $$\n为了高效计算，我们用完整残差 $r = y - Ax$ 来表示 $(a_j)^{\\top}r^{(j)}$。\n$r^{(j)} = y - \\sum_{k \\neq j} x_k a_k = (y - Ax) + x_j a_j = r + x_j a_j$。\n因此，$(a_j)^{\\top}r^{(j)} = (a_j)^{\\top}(r + x_j a_j) = (a_j)^{\\top}r + x_j \\|a_j\\|_2^2$。\n令 $\\rho_j = (a_j)^{\\top}r + x_j \\|a_j\\|_2^2$。更新规则为：\n$$ x_j^{\\text{new}} = \\frac{S_{\\lambda}(\\rho_j)}{\\|a_j\\|_2^2} $$\n这就是坐标级最小化子的闭式解。\n\n现在，我们对给定的实例执行一次单坐标更新。\n给定的数据是：\n$$ A = \\begin{pmatrix} 1   0   2   -1 \\\\ 0   1   1   1 \\\\ 1   -1  0   2 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad x = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad \\lambda = 1 $$\n我们要更新索引为 $j = 2$ 的坐标。\n\n首先，我们计算当前残差 $r = y - Ax$：\n$$ Ax = \\begin{pmatrix} 1   0   2   -1 \\\\ 0   1   1   1 \\\\ 1   -1  0   2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1(1) + 2(-1) - 1(\\frac{1}{2}) \\\\ 1(0) + 1(-1) + 1(\\frac{1}{2}) \\\\ 1(1) - 1(0) + 2(\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} 1 - 2 - \\frac{1}{2} \\\\ -1 + \\frac{1}{2} \\\\ 1 + 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 2 \\end{pmatrix} $$\n$$ r = y - Ax = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 + \\frac{3}{2} \\\\ -1 + \\frac{1}{2} \\\\ 3 - 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} $$\n接下来，我们计算 $x_2$ 的更新。当前值为 $x_2=0$。A 的第二列是 $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$。其 $L_2$ 范数的平方是 $\\|a_2\\|_2^2 = 0^2 + 1^2 + (-1)^2 = 2$。\n我们计算 $\\rho_2 = (a_2)^{\\top}r + x_2 \\|a_2\\|_2^2$：\n$$ (a_2)^{\\top}r = \\begin{pmatrix} 0   1   -1 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} = 0(\\frac{7}{2}) + 1(-\\frac{1}{2}) - 1(1) = -\\frac{1}{2} - 1 = -\\frac{3}{2} $$\n$$ \\rho_2 = -\\frac{3}{2} + (0)(2) = -\\frac{3}{2} $$\n现在我们应用软阈值算子，其中 $\\lambda=1$：\n$$ S_1(\\rho_2) = S_1(-\\frac{3}{2}) = \\mathrm{sign}(-\\frac{3}{2}) \\max\\left(\\left|-\\frac{3}{2}\\right| - 1, 0\\right) = (-1) \\max\\left(\\frac{3}{2} - 1, 0\\right) = (-1) \\max\\left(\\frac{1}{2}, 0\\right) = -\\frac{1}{2} $$\n第二个坐标的新值为：\n$$ x_2^{\\text{new}} = \\frac{S_1(\\rho_2)}{\\|a_2\\|_2^2} = \\frac{-\\frac{1}{2}}{2} = -\\frac{1}{4} $$\n坐标的变化量为 $\\Delta x_2 = x_2^{\\text{new}} - x_2 = -\\frac{1}{4} - 0 = -\\frac{1}{4}$。\n更新后的残差 $r^{\\text{new}}$ 可通过 $r^{\\text{new}} = r - \\Delta x_2 a_2$ 高效地计算：\n$$ r^{\\text{new}} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} - \\left(-\\frac{1}{4}\\right) \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{1}{4} \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{2}{4} + \\frac{1}{4} \\\\ \\frac{4}{4} - \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} $$\n最后，我们计算坐标级 Karush-Kuhn-Tucker (KKT) 残差 $v_2$。由于更新后的坐标 $x_2^{\\text{new}} = -\\frac{1}{4} \\neq 0$，我们使用公式 $v_j = (a_j)^{\\top} r^{\\text{new}} - \\lambda \\,\\mathrm{sign}(x_j^{\\text{new}})$。\n$$ (a_2)^{\\top} r^{\\text{new}} = \\begin{pmatrix} 0   1   -1 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} = 0 - \\frac{1}{4} - \\frac{3}{4} = -1 $$\n$$ \\lambda \\,\\mathrm{sign}(x_2^{\\text{new}}) = (1) \\mathrm{sign}(-\\frac{1}{4}) = (1)(-1) = -1 $$\n$$ v_2 = (-1) - (-1) = 0 $$\n正如预期，刚优化过的坐标的 KKT 残差为零，这证实了该坐标的一阶最优性条件已满足。\n要求的输出是一个行矩阵，其中包含更新后残差 $r^{\\text{new}}$ 的各分量，后跟标量 $v_2$。\n$r^{\\text{new}}$ 的分量为 $\\frac{7}{2}$、$-\\frac{1}{4}$、$\\frac{3}{4}$。标量为 $v_2=0$。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{7}{2}  -\\frac{1}{4}  \\frac{3}{4}  0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "在理解了单步更新之后，我们现在将视角放大，以观察整个解路径，它展示了最优系数如何随着正则化参数 $λ$ 的变化而改变。本练习要求您为一个简单问题计算 LASSO 路径的前几个“断点”，这些是变量进入或离开模型的关键 $λ$ 值。通过追踪这条路径，您将揭示其分段线性的本质，并深入了解路径算法如何顺序地构建解。",
            "id": "3465883",
            "problem": "考虑一个低维设定下的带有平方误差损失的最小绝对收缩和选择算子 (LASSO) 问题。设设计矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，响应向量为 $y \\in \\mathbb{R}^{2}$，由下式给出\n$$\nX = \\begin{pmatrix}\n1   1 \\\\\n0   1\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}.\n$$\n对于正则化参数 $ \\lambda \\geq 0 $，LASSO 估计量 $ \\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{2} $ 定义为下式的最小化子：\n$$\n\\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1}.\n$$\n从凸优化的基本次梯度最优性条件出发，并利用路径坐标下降和解路径的概念框架，确定当 $ \\lambda $ 从一个足够大的值减小时， $ \\hat{\\beta}(\\lambda) $ 的活跃集发生变化的前两个断点 $ \\lambda_{1} $ 和 $ \\lambda_{2} $。为给定的 $X$ 和 $y$ 显式地计算这两个断点值的精确形式。然后，通过推导路径段的显式表达式，验证在连续断点之间的每个区间上，LASSO 解 $ \\hat{\\beta}(\\lambda) $ 线性依赖于 $ \\lambda $。\n\n请使用 LaTeX 的 $ \\texttt{pmatrix} $ 环境，将你的最终答案表示为单行矩阵中的数对 $ (\\lambda_{1}, \\lambda_{2}) $。无需四舍五入，也不涉及单位。",
            "solution": "我们从 LASSO 公式开始\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1},\n$$\n其中 $ X = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} $ 且 $ y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $。将 $ X $ 的列记为 $ x_{1} $ 和 $ x_{2} $，所以 $ x_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $ 且 $ x_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $。定义格拉姆矩阵 $ G = X^{\\top} X $ 和相关向量 $ c = X^{\\top} y $。计算它们：\n$$\nG = X^{\\top} X = \\begin{pmatrix}\nx_{1}^{\\top} x_{1}  x_{1}^{\\top} x_{2} \\\\\nx_{2}^{\\top} x_{1}  x_{2}^{\\top} x_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n1  1 \\\\\n1  2\n\\end{pmatrix},\n\\qquad\nc = X^{\\top} y = \\begin{pmatrix}\nx_{1}^{\\top} y \\\\\nx_{2}^{\\top} y\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 \\\\\n3\n\\end{pmatrix}.\n$$\nLASSO 的次梯度最优性条件表明，在最优点 $ \\hat{\\beta}(\\lambda) $ 处，对于每个坐标 $ j \\in \\{1,2\\} $，\n$$\nx_{j}^{\\top} (y - X \\hat{\\beta}(\\lambda)) = \\lambda s_{j},\n$$\n其中，如果 $ \\hat{\\beta}_{j}(\\lambda) \\neq 0 $，则 $ s_{j} = \\operatorname{sign}(\\hat{\\beta}_{j}(\\lambda)) $；如果 $ \\hat{\\beta}_{j}(\\lambda) = 0 $，则 $ s_{j} \\in [-1,1] $。以向量形式，当活跃集 $ A \\subseteq \\{1,2\\} $ 和符号模式 $ s_{A} $ 固定时，Karush–Kuhn–Tucker (KKT) 条件得出\n$$\nG_{A A} \\hat{\\beta}_{A}(\\lambda) = c_{A} - \\lambda s_{A},\n$$\n并且对于 $ j \\notin A $ 有 $ |x_{j}^{\\top} (y - X \\hat{\\beta}(\\lambda))| \\leq \\lambda $。因此，对于固定的 $ A $ 和 $ s_{A} $，解段为\n$$\n\\hat{\\beta}_{A}(\\lambda) = G_{A A}^{-1} \\big( c_{A} - \\lambda s_{A} \\big),\n$$\n其是关于 $ \\lambda $ 的仿射（因此是线性）函数。\n\n我们现在计算断点。初始断点 $ \\lambda_{1} $ 是使零解不再为最优解的最小 $ \\lambda $ 值。当 $ \\hat{\\beta}(\\lambda) = 0 $ 时，最优性要求对所有 $ j $ 都有 $ |c_{j}| \\leq \\lambda $，因此最大的相关性决定了第一个进入模型的变量：\n$$\n\\lambda_{\\max} = \\max_{j} |c_{j}| = \\max \\{ |2|, |3| \\} = 3.\n$$\n因此，在 $ \\lambda = 3 $ 时，索引为 $ j = 2 $ 的变量准备以正号进入模型（因为 $ c_{2} = 3 > 0 $）。因此，第一个断点是 $ \\lambda_{1} = 3 $。\n\n对于略小于 $ \\lambda_{1} = 3 $ 的 $ \\lambda $，活跃集为 $ A = \\{2\\} $，且 $ s_{2} = +1 $。在此段上的 KKT 方程为\n$$\ng_{22} \\hat{\\beta}_{2}(\\lambda) = c_{2} - \\lambda,\n\\quad \\text{其中} \\quad g_{22} = 2, \\; c_{2} = 3.\n$$\n因此\n$$\n\\hat{\\beta}_{2}(\\lambda) = \\frac{3 - \\lambda}{2},\n\\quad \\hat{\\beta}_{1}(\\lambda) = 0,\n\\quad \\text{在 } \\lambda \\in [\\lambda_{2}, \\lambda_{1}] \\text{ 时有效}。\n$$\n下一个断点 $ \\lambda_{2} $ 发生在非活跃坐标 $ j = 1 $ 达到边界 $ |x_{1}^{\\top} r(\\lambda)| = \\lambda $ 时，其中 $ r(\\lambda) = y - X \\hat{\\beta}(\\lambda) $ 是残差。在当前段上，\n$$\nr(\\lambda) = y - x_{2} \\hat{\\beta}_{2}(\\lambda)\n= \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot \\frac{3 - \\lambda}{2}\n= \\begin{pmatrix} 2 - \\frac{3 - \\lambda}{2} \\\\ 1 - \\frac{3 - \\lambda}{2} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{2} + \\frac{\\lambda}{2} \\\\ -\\frac{1}{2} + \\frac{\\lambda}{2} \\end{pmatrix}.\n$$\n因此，\n$$\nx_{1}^{\\top} r(\\lambda) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} \\frac{1}{2} + \\frac{\\lambda}{2} \\\\ -\\frac{1}{2} + \\frac{\\lambda}{2} \\end{pmatrix}\n= \\frac{1}{2} + \\frac{\\lambda}{2}.\n$$\n进入条件 $ |x_{1}^{\\top} r(\\lambda)| = \\lambda $（预期为正号，因为在 $ \\lambda $ 略小于 3 附近 $ x_{1}^{\\top} r(\\lambda) $ 为正）给出\n$$\n\\frac{1}{2} + \\frac{\\lambda}{2} = \\lambda \\quad \\Rightarrow \\quad \\lambda = 1.\n$$\n因此第二个断点是 $ \\lambda_{2} = 1 $。\n\n对于 $ \\lambda \\in [0, 1] $，两个变量都以正号活跃，所以 $ A = \\{1,2\\} $ 且 $ s = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $。使用完整的 KKT 系统，\n$$\nG \\hat{\\beta}(\\lambda) = c - \\lambda s,\n\\quad \\text{所以} \\quad\n\\hat{\\beta}(\\lambda) = G^{-1} (c - \\lambda s).\n$$\n计算 $ G^{-1} $：\n$$\nG = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}, \\quad \\det(G) = 1 \\cdot 2 - 1 \\cdot 1 = 1, \\quad\nG^{-1} = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix}.\n$$\n然后\n$$\nc - \\lambda s = \\begin{pmatrix} 2 - \\lambda \\\\ 3 - \\lambda \\end{pmatrix},\n\\quad \\hat{\\beta}(\\lambda) = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 2 - \\lambda \\\\ 3 - \\lambda \\end{pmatrix}\n= \\begin{pmatrix}\n2(2 - \\lambda) - (3 - \\lambda) \\\\\n-(2 - \\lambda) + (3 - \\lambda)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 - \\lambda \\\\\n1\n\\end{pmatrix}.\n$$\n因此，在 $ \\lambda \\in [0, 1] $ 上，解路径是关于 $ \\lambda $ 的显式线性函数，其中 $ \\hat{\\beta}_{1}(\\lambda) = 1 - \\lambda $ 且 $ \\hat{\\beta}_{2}(\\lambda) = 1 $。在 $ \\lambda \\in [1, 3] $ 上，解路径也是关于 $ \\lambda $ 的线性函数，其中 $ \\hat{\\beta}_{2}(\\lambda) = \\frac{3 - \\lambda}{2} $ 且 $ \\hat{\\beta}_{1}(\\lambda) = 0 $。在 $ \\lambda = 1 $ 处连续性成立，因为两个表达式都得出 $ \\hat{\\beta}(1) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $，并且在 $ \\lambda = 3 $ 处我们有 $ \\hat{\\beta}(3) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $，符合要求。\n\n因此，LASSO 路径的前两个断点是 $ \\lambda_{1} = 3 $ 和 $ \\lambda_{2} = 1 $，且这些断点之间的路径段是关于 $ \\lambda $ 的线性函数。",
            "answer": "$$\\boxed{\\begin{pmatrix} 3  1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "在实践中，计算完整的解路径可能计算量巨大，尤其是在处理高维数据时。本练习介绍了一种称为筛选规则的强大技术，它通过安全地剔除那些保证系数为零的特征，来显著加速路径算法。您将基于路径上前一个点的信息推导出一个“强规则”，并验证其安全性，从而展示 LASSO 的理论性质如何带来重大的实用性加速。",
            "id": "3465831",
            "problem": "考虑具有平方误差损失的最小绝对收缩和选择算子 (LASSO) 问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列已中心化并缩放至单位欧几里得范数，且 $y \\in \\mathbb{R}^{n}$。给定一个正则化参数的路径序列 $\\{\\lambda_{k-1}, \\lambda_{k}\\}$，其中 $\\lambda_{k}  \\lambda_{k-1}$，以及一个先前计算出的解 $\\beta(\\lambda_{k-1})$。任务是推导并应用一个序列筛选规则（通常称为强规则），以利用 $\\beta(\\lambda_{k-1})$ 在 $\\lambda_{k}$ 处提出一个筛选集，然后通过检查在 $\\lambda_{k}$ 处针对最初被剔除的特征的 Karush-Kuhn-Tucker (KKT) 条件来验证安全性。\n\n从基本原理出发：\n- 使用在给定 $\\lambda$ 下 LASSO 最优解的 KKT 条件：对于每个坐标 $j \\in \\{1,\\dots,p\\}$，设残差为 $r(\\lambda) = y - X\\beta(\\lambda)$，相关性为 $c_{j}(\\lambda) = x_{j}^{\\top} r(\\lambda)$。KKT 条件要求，如果 $\\beta_{j}(\\lambda) \\neq 0$，则 $c_{j}(\\lambda) = \\lambda \\,\\mathrm{sign}(\\beta_{j}(\\lambda))$; 如果 $\\beta_{j}(\\lambda) = 0$，则 $|c_{j}(\\lambda)| \\le \\lambda$。\n- 使用单位范数列的平方误差损失的单位斜率界：相关性 $c_{j}(\\lambda)$ 作为 $\\lambda$ 的函数，在解路径上是 1-Lipschitz 的。\n- 基于这些要素，根据在 $\\lambda_{k-1}$ 处已计算的量，推导出在 $\\lambda_{k}$ 处剔除特征 $j$ 的一个充分条件。\n\n将此应用于下面的具体实例，然后执行安全性检查：\n- 设 $n=4$ 且 $p=5$。$X$ 的列向量为\n$$\nx_{1} = \\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}, \\quad\nx_{2} = \\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}, \\quad\nx_{3} = \\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}, \\quad\nx_{4} = \\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}, \\quad\nx_{5} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix}.\n$$\n- 设 $y = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix}$，$\\lambda_{k-1} = 2$，以及 $\\lambda_{k} = 1.3$。\n- 在 $\\lambda_{k-1}$ 处，假设唯一解为 $\\beta(\\lambda_{k-1}) = 0$。\n\n任务：\n- 使用 KKT 条件和单位斜率界，从 $\\lambda_{k-1}$ 处的残差和相关性推导出一个充分的剔除不等式，以提出在 $\\lambda_{k}$ 处的被筛选掉的集合，并列出被提议剔除的特征。\n- 求解在 $\\lambda_{k}$ 处限制于您确定的保留集上的 LASSO 子问题，并获得残差 $r(\\lambda_{k})$。\n- 通过检查在 $\\lambda_{k}$ 处对所有最初被剔除的特征的 KKT 条件来验证安全性。如果任何特征违反 KKT 条件，则必须将其加回；否则筛选是安全的。\n- 报告在 KKT 验证后，在 $\\lambda_{k}$ 处安全剔除的特征数量。您的答案必须是一个没有单位的单一数字。不需要四舍五入。",
            "solution": "该问题要求我们为 LASSO 问题推导并应用一个序列筛选规则，然后验证其安全性。LASSO 优化问题由下式给出\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\n其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda  0$ 是正则化参数。$X$ 的列向量，记为 $x_j$，被缩放以具有单位欧几里得范数，即对于所有 $j \\in \\{1,\\dots,p\\}$，$\\|x_j\\|_2 = 1$。\n\n首先，我们推导筛选规则。Karush-Kuhn-Tucker (KKT) 条件是解 $\\beta(\\lambda)$ 在给定 $\\lambda$ 下为最优的充分必要条件。令 $r(\\lambda) = y - X\\beta(\\lambda)$ 为残差，$c_j(\\lambda) = x_j^{\\top}r(\\lambda)$ 为第 $j$ 个特征与残差的相关性。KKT 条件规定，对于每个坐标 $j$：\n\\begin{itemize}\n    \\item 如果 $\\beta_j(\\lambda) \\neq 0$，则 $c_j(\\lambda) = \\lambda \\, \\mathrm{sign}(\\beta_j(\\lambda))$。\n    \\item 如果 $\\beta_j(\\lambda) = 0$，则 $|c_j(\\lambda)| \\le \\lambda$。\n\\end{itemize}\n如果我们能保证特征 $j$ 的最优系数 $\\beta_j(\\lambda_k)$ 为零，那么在正则化水平 $\\lambda_k$ 处就可以安全地剔除该特征。根据 KKT 条件，$\\beta_j(\\lambda_k) = 0$ 的一个充分条件是严格不等式 $|c_j(\\lambda_k)|  \\lambda_k$。\n\n我们给定一个在 $\\lambda_{k-1} > \\lambda_k$ 处先前计算出的解 $\\beta(\\lambda_{k-1})$，并且我们可以计算出相应的相关性 $c_j(\\lambda_{k-1})$。为了将这些与未知的相关性 $c_j(\\lambda_k)$ 联系起来，我们使用给定的单位斜率界，该界表明函数 $c_j(\\lambda)$ 是 1-Lipschitz 连续的：\n$$\n|c_j(\\lambda_{k}) - c_j(\\lambda_{k-1})| \\le |\\lambda_{k} - \\lambda_{k-1}|\n$$\n由于 $\\lambda_k  \\lambda_{k-1}$，这变为 $|c_j(\\lambda_{k}) - c_j(\\lambda_{k-1})| \\le \\lambda_{k-1} - \\lambda_k$。使用三角不等式，我们可以对 $|c_j(\\lambda_k)|$ 进行界定：\n$$\n|c_j(\\lambda_k)| = |c_j(\\lambda_{k-1}) + (c_j(\\lambda_k) - c_j(\\lambda_{k-1}))| \\le |c_j(\\lambda_{k-1})| + |c_j(\\lambda_k) - c_j(\\lambda_{k-1})|\n$$\n应用 Lipschitz 属性，我们得到在 $\\lambda_k$ 处相关性的一个上界：\n$$\n|c_j(\\lambda_k)| \\le |c_j(\\lambda_{k-1})| + \\lambda_{k-1} - \\lambda_k\n$$\n如果这个上界本身小于 $\\lambda_k$，那么我们就可以保证 $|c_j(\\lambda_k)|  \\lambda_k$，从而确保 $\\beta_j(\\lambda_k)=0$。这给了我们充分的剔除不等式：\n$$\n|c_j(\\lambda_{k-1})| + \\lambda_{k-1} - \\lambda_k  \\lambda_k\n$$\n重新整理这个不等式，得到筛选规则：如果\n$$\n|c_j(\\lambda_{k-1})|  2\\lambda_k - \\lambda_{k-1}\n$$\n则可以初步剔除特征 $j$。\n\n现在，我们将此规则应用于给定的实例。数据如下：\n$n=4$，$p=5$，$\\lambda_{k-1}=2$，$\\lambda_k=1.3$。\n$y = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix}$，以及 $X$ 的列向量 $x_1, \\dots, x_5$。\n问题陈述在 $\\lambda_{k-1}=2$ 时，解为 $\\beta(\\lambda_{k-1}) = 0$。\n残差为 $r(\\lambda_{k-1}) = y - X \\cdot 0 = y$。\n在 $\\lambda_{k-1}$ 处的相关性为 $c_j(\\lambda_{k-1})=x_j^{\\top}y$：\n$c_1(\\lambda_{k-1}) = \\begin{pmatrix}1  0  0  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = 2$。\n$c_2(\\lambda_{k-1}) = \\begin{pmatrix}0  1  0  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = 0$。\n$c_3(\\lambda_{k-1}) = \\begin{pmatrix}0  0  1  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = -1.5$。\n$c_4(\\lambda_{k-1}) = \\begin{pmatrix}0  0  0  1\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = -1$。\n$c_5(\\lambda_{k-1}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  0  1  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{2}}(2 - 1.5) = \\frac{0.5}{\\sqrt{2}} = \\frac{\\sqrt{2}}{4}$。\n\n筛选规则的阈值为 $2\\lambda_k - \\lambda_{k-1} = 2(1.3) - 2 = 2.6 - 2 = 0.6$。\n如果 $|c_j(\\lambda_{k-1})|  0.6$，我们剔除特征 $j$：\n\\begin{itemize}\n    \\item $j=1$: $|c_1(\\lambda_{k-1})| = |2| = 2 \\not 0.6$。保留。\n    \\item $j=2$: $|c_2(\\lambda_{k-1})| = |0| = 0  0.6$。剔除。\n    \\item $j=3$: $|c_3(\\lambda_{k-1})| = |-1.5| = 1.5 \\not 0.6$。保留。\n    \\item $j=4$: $|c_4(\\lambda_{k-1})| = |-1| = 1 \\not 0.6$。保留。\n    \\item $j=5$: $|c_5(\\lambda_{k-1})| = |\\frac{\\sqrt{2}}{4}| \\approx 0.354  0.6$。剔除。\n\\end{itemize}\n提议的剔除特征集是 $\\{2, 5\\}$，保留集是 $\\mathcal{A}=\\{1, 3, 4\\}$。\n\n接下来，我们求解在 $\\lambda_k=1.3$ 处限制于特征集 $\\mathcal{A}$ 上的 LASSO 子问题。子问题是：\n$$\n\\min_{\\beta_1, \\beta_3, \\beta_4} \\ \\frac{1}{2}\\|y - (x_1\\beta_1 + x_3\\beta_3 + x_4\\beta_4)\\|_{2}^{2} + 1.3 (|\\beta_1| + |\\beta_3| + |\\beta_4|)\n$$\n列向量 $x_1, x_3, x_4$ 是标准正交的。对于一个标准正交的设计矩阵 $X_{\\mathcal{A}}$，LASSO 解由相关性 $X_{\\mathcal{A}}^{\\top}y$ 的坐标级软阈值给出。对于 $j \\in \\mathcal{A}$，解为 $\\beta_j(\\lambda_k) = S_{\\lambda_k}(x_j^{\\top}y)$，其中 $S_{\\alpha}(z) = \\mathrm{sign}(z)\\max(|z|-\\alpha, 0)$。\n相关的相关性是 $c_j(\\lambda_{k-1}) = x_j^{\\top}y$。\n\\begin{itemize}\n    \\item $\\beta_1(1.3) = S_{1.3}(2) = \\mathrm{sign}(2)\\max(|2|-1.3, 0) = 0.7$。\n    \\item $\\beta_3(1.3) = S_{1.3}(-1.5) = \\mathrm{sign}(-1.5)\\max(|-1.5|-1.3, 0) = -0.2$。\n    \\item $\\beta_4(1.3) = S_{1.3}(-1) = \\mathrm{sign}(-1)\\max(|-1|-1.3, 0) = 0$。\n\\end{itemize}\n子问题的解给出了暂定的完整解向量 $\\beta(\\lambda_k) = (0.7, 0, -0.2, 0, 0)^{\\top}$。\n在 $\\lambda_k=1.3$ 处的残差为 $r(\\lambda_k) = y - X\\beta(\\lambda_k)$：\n$$\nr(1.3) = y - (0.7x_1 - 0.2x_3) = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} - \\left(0.7\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix} - 0.2\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}\\right) = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} - \\begin{pmatrix}0.7\\\\0\\\\-0.2\\\\0\\end{pmatrix} = \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix}\n$$\n\n最后，我们通过检查在 $\\lambda_k=1.3$ 处对被剔除特征 $j \\in \\{2, 5\\}$ 的 KKT 条件来验证筛选的安全性。对于这些特征，我们假设 $\\beta_j(1.3)=0$，因此我们必须检查 $|c_j(1.3)| = |x_j^{\\top}r(1.3)| \\le \\lambda_k = 1.3$ 是否成立。\n\\begin{itemize}\n    \\item 对于 $j=2$：$c_2(1.3) = x_2^{\\top}r(1.3) = \\begin{pmatrix}0  1  0  0\\end{pmatrix} \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix} = 0$。\n    条件是 $|0| \\le 1.3$，成立。\n    \\item 对于 $j=5$：$c_5(1.3) = x_5^{\\top}r(1.3) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  0  1  0\\end{pmatrix} \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1.3 - 1.3) = 0$。\n    条件是 $|0| \\le 1.3$，成立。\n\\end{itemize}\n两个被剔除的特征都满足 KKT 条件。因此，筛选规则是安全的，没有发生 KKT 违规。在缩减问题上的解是完整问题的正确解。特征 $\\{2, 5\\}$ 被安全地剔除了。\n\n在 $\\lambda_k=1.3$ 处安全剔除的特征数量为 2。",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}