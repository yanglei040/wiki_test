{
    "hands_on_practices": [
        {
            "introduction": "要掌握路径式算法，首先需要精通其核心的单步操作。本练习将通过一个具体的、分步的计算任务，帮助你揭开软阈值算子（soft-thresholding operator）的神秘面纱。你将亲手实践如何针对单个变量最小化 LASSO 目标函数，这是理解整个坐标下降法工作原理的基石。",
            "id": "3465818",
            "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，其目标是最小化目标函数：$$\\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1},$$ 其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$，且 $\\lambda > 0$。路径坐标下降法每次更新一个坐标 $x_{j}$，同时保持其他坐标不变。设当前迭代值为 $x$，残差为 $r = y - A x$。对于一个固定的坐标索引 $j$，坐标下降更新通过在 LASSO 目标函数中仅改变 $x_j$ 得到的一维子问题的唯一最小化子来替换 $x_j$，然后相应地更新残差。\n\n从凸优化和可分一维凸函数的次微分最优性的基本原理出发，推导 $x_j$ 的坐标级最小化子的闭式解，并对以下具体实例执行一次单坐标更新：\n$$A = \\begin{pmatrix}\n1  0  2  -1 \\\\\n0  1  1  1 \\\\\n1  -1  0  2\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2 \\\\ -1 \\\\ 3\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix}\n1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2}\n\\end{pmatrix}, \\quad\n\\lambda = 1,$$\n更新索引为 $j = 2$ 的坐标（第二个坐标）。计算此单坐标更新后的更新残差 $r$ 以及索引 $j$ 的坐标级 Karush-Kuhn-Tucker (KKT) 残差。坐标级 KKT 残差定义如下：如果更新后的 $x_{j} \\neq 0$，则设 $$v_{j} = a_{j}^{\\top} r - \\lambda \\,\\mathrm{sign}(x_{j}),$$ 如果更新后的 $x_{j} = 0$，则设 $$v_{j} = \\max\\!\\big(|a_{j}^{\\top} r| - \\lambda,\\, 0\\big),$$ 其中 $a_{j}$ 表示 $A$ 的第 $j$ 列。请将你的最终答案以一个单行矩阵的形式给出，其中包含更新后残差 $r$ 的分量（按顺序），后跟标量 $v_{j}$。所有值都需精确表示，无需四舍五入。",
            "solution": "用户要求推导 LASSO 问题的坐标级最小化子，并将其应用于一个具体实例。分析将分两个阶段进行：首先，从基本原理出发，对更新规则进行形式化推导；其次，将此规则应用于所提供的数据。\n\nLASSO 目标函数由下式给出：\n$$f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$$\n其中 $y \\in \\mathbb{R}^{m}$，$A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，且 $\\lambda > 0$。路径坐标下降法每次更新单个坐标 $x_j$，同时保持所有其他坐标 $x_k$（其中 $k \\neq j$）固定。为了找到 $x_j$ 的更新，我们求解一维优化问题：\n$$ \\min_{z \\in \\mathbb{R}} f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_n) $$\n让我们将目标表示为 $z$ 的函数。$L_1$ 范数是可分的：$\\|x\\|_1 = \\sum_{k \\neq j} |x_k| + |z|$。二次项可以展开为：\n$$ \\|y - Ax\\|_2^2 = \\left\\|y - \\sum_{k=1}^{n} x_k a_k\\right\\|_2^2 = \\left\\|y - \\sum_{k \\neq j} x_k a_k - z a_j\\right\\|_2^2 $$\n其中 $a_k$ 是 $A$ 的第 $k$ 列。令部分残差为 $r^{(j)} = y - \\sum_{k \\neq j} x_k a_k$。关于 $z$ 的优化问题变为：\n$$ \\min_{z \\in \\mathbb{R}} \\left( \\frac{1}{2} \\|r^{(j)} - z a_j\\|_2^2 + \\lambda |z| \\right) $$\n其他项 $\\lambda \\sum_{k \\neq j} |x_k|$ 相对于 $z$ 是常数，在最小化过程中可以忽略。我们展开平方范数：\n$$ g(z) = \\frac{1}{2} (r^{(j)} - z a_j)^{\\top}(r^{(j)} - z a_j) + \\lambda |z| = \\frac{1}{2} \\left( \\|r^{(j)}\\|_2^2 - 2z (a_j)^{\\top}r^{(j)} + z^2 \\|a_j\\|_2^2 \\right) + \\lambda |z| $$\n该目标函数 $g(z)$ 是一个严格凸的二次函数和一个凸的绝对值函数之和，因此它是严格凸的，并有唯一的最小化子。通过将 $g(z)$ 的次梯度设为零来找到最小化子。次梯度是：\n$$ \\partial g(z) = z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} + \\lambda \\partial|z| $$\n其中 $\\partial|z|$ 是绝对值函数的次梯度：如果 $z \\neq 0$，$\\partial|z| = \\mathrm{sign}(z)$；如果 $z=0$，$\\partial|z| \\in [-1, 1]$。最优性条件是 $0 \\in \\partial g(z)$。\n\n情况1：$z > 0$。条件是 $z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} + \\lambda = 0$。这给出 $z = \\frac{(a_j)^{\\top}r^{(j)} - \\lambda}{\\|a_j\\|_2^2}$。此解在 $z>0$ 时有效，即 $(a_j)^{\\top}r^{(j)} > \\lambda$。\n\n情况2：$z  0$。条件是 $z \\|a_j\\|_2^2 - (a_j)^{\\top}r^{(j)} - \\lambda = 0$。这给出 $z = \\frac{(a_j)^{\\top}r^{(j)} + \\lambda}{\\|a_j\\|_2^2}$。此解在 $z0$ 时有效，即 $(a_j)^{\\top}r^{(j)}  -\\lambda$。\n\n情况3：$z = 0$。条件是 $0 \\in -(a_j)^{\\top}r^{(j)} + \\lambda [-1, 1]$。这等价于 $(a_j)^{\\top}r^{(j)} \\in [-\\lambda, \\lambda]$，或 $|(a_j)^{\\top}r^{(j)}| \\le \\lambda$。\n\n这三种情况定义了软阈值算子 $S_{\\tau}(\\rho) = \\mathrm{sign}(\\rho)\\max(|\\rho|-\\tau, 0)$。最优的 $z$，我们记为 $x_j^{\\text{new}}$，由下式给出：\n$$ x_j^{\\text{new}} = \\frac{S_{\\lambda}((a_j)^{\\top}r^{(j)})}{\\|a_j\\|_2^2} $$\n为了高效计算，我们用完整残差 $r = y - Ax$ 来表示 $(a_j)^{\\top}r^{(j)}$。\n$r^{(j)} = y - \\sum_{k \\neq j} x_k a_k = (y - Ax) + x_j a_j = r + x_j a_j$。\n因此，$(a_j)^{\\top}r^{(j)} = (a_j)^{\\top}(r + x_j a_j) = (a_j)^{\\top}r + x_j \\|a_j\\|_2^2$。\n令 $\\rho_j = (a_j)^{\\top}r + x_j \\|a_j\\|_2^2$。更新规则为：\n$$ x_j^{\\text{new}} = \\frac{S_{\\lambda}(\\rho_j)}{\\|a_j\\|_2^2} $$\n这就是坐标级最小化子的闭式解。\n\n现在，我们对给定的实例执行一次单坐标更新。\n给定的数据是：\n$$ A = \\begin{pmatrix} 1  0  2  -1 \\\\ 0  1  1  1 \\\\ 1  -1  0  2 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad x = \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix}, \\quad \\lambda = 1 $$\n我们要更新索引为 $j=2$ 的坐标。当前值为 $x_2=0$。\n\n首先，我们计算当前残差 $r = y - Ax$：\n$$ Ax = \\begin{pmatrix} 1  0  2  -1 \\\\ 0  1  1  1 \\\\ 1  -1  0  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) + 2(-1) - 1(\\frac{1}{2}) \\\\ 0(1) + 1(0) + 1(-1) + 1(\\frac{1}{2}) \\\\ 1(1) - 1(0) + 0(-1) + 2(\\frac{1}{2}) \\end{pmatrix} = \\begin{pmatrix} 1 - 2 - \\frac{1}{2} \\\\ -1 + \\frac{1}{2} \\\\ 1 + 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 2 \\end{pmatrix} $$\n$$ r = y - Ax = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} -\\frac{3}{2} \\\\ -\\frac{1}{2} \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 + \\frac{3}{2} \\\\ -1 + \\frac{1}{2} \\\\ 3 - 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} $$\n接下来，我们计算 $x_2$ 的更新。$A$ 的第二列是 $a_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$。$L_2$ 范数的平方是 $\\|a_2\\|_2^2 = 0^2 + 1^2 + (-1)^2 = 2$。\n我们计算 $\\rho_2 = (a_2)^{\\top}r + x_2 \\|a_2\\|_2^2$：\n$$ (a_2)^{\\top}r = \\begin{pmatrix} 0  1  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} = 0(\\frac{7}{2}) + 1(-\\frac{1}{2}) - 1(1) = -\\frac{1}{2} - 1 = -\\frac{3}{2} $$\n$$ \\rho_2 = -\\frac{3}{2} + (0)(2) = -\\frac{3}{2} $$\n现在我们应用软阈值算子，其中 $\\lambda=1$：\n$$ S_1(\\rho_2) = S_1(-\\frac{3}{2}) = \\mathrm{sign}(-\\frac{3}{2}) \\max\\left(\\left|-\\frac{3}{2}\\right| - 1, 0\\right) = (-1) \\max\\left(\\frac{3}{2} - 1, 0\\right) = (-1) \\max\\left(\\frac{1}{2}, 0\\right) = -\\frac{1}{2} $$\n第二个坐标的新值为：\n$$ x_2^{\\text{new}} = \\frac{S_1(\\rho_2)}{\\|a_2\\|_2^2} = \\frac{-\\frac{1}{2}}{2} = -\\frac{1}{4} $$\n坐标的变化量是 $\\Delta x_2 = x_2^{\\text{new}} - x_2 = -\\frac{1}{4} - 0 = -\\frac{1}{4}$。\n更新后的残差 $r^{\\text{new}}$ 可以通过 $r^{\\text{new}} = r - \\Delta x_2 a_2$ 高效计算：\n$$ r^{\\text{new}} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} - \\left(-\\frac{1}{4}\\right) \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{1}{4} \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{2}{4} + \\frac{1}{4} \\\\ \\frac{4}{4} - \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} $$\n最后，我们计算坐标级的 Karush-Kuhn-Tucker (KKT) 残差 $v_2$。由于更新后的坐标 $x_2^{\\text{new}} = -\\frac{1}{4} \\neq 0$，我们使用公式 $v_j = (a_j)^{\\top} r^{\\text{new}} - \\lambda \\,\\mathrm{sign}(x_j^{\\text{new}})$。\n$$ (a_2)^{\\top} r^{\\text{new}} = \\begin{pmatrix} 0  1  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{7}{2} \\\\ -\\frac{1}{4} \\\\ \\frac{3}{4} \\end{pmatrix} = 0 - \\frac{1}{4} - \\frac{3}{4} = -1 $$\n$$ \\lambda \\,\\mathrm{sign}(x_2^{\\text{new}}) = (1) \\mathrm{sign}(-\\frac{1}{4}) = (1)(-1) = -1 $$\n$$ v_2 = (-1) - (-1) = 0 $$\n正如预期的那样，刚刚优化过的坐标的 KKT 残差为零，这证实了该坐标的一阶最优性条件已满足。\n要求的输出是一个行矩阵，包含更新后残差 $r^{\\text{new}}$ 的分量，后跟标量 $v_2$。\n$r^{\\text{new}}$ 的分量是 $\\frac{7}{2}$，$-\\frac{1}{4}$，$\\frac{3}{4}$。标量是 $v_2=0$。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{7}{2}  -\\frac{1}{4}  \\frac{3}{4}  0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "在掌握了单坐标更新的机制后，我们可以将视野从单一步骤扩展到整个解路径的宏观行为。本练习  将引导你探索当正则化参数 $\\lambda$ 变化时，LASSO 解是如何演变的。你将学会如何计算解路径上的“断点”（breakpoints），即非零系数集合发生改变的关键位置，从而亲身体验解路径的分段线性特性。",
            "id": "3465883",
            "problem": "考虑一个在低维设定下带有平方误差损失的最小绝对收缩和选择算子 (LASSO) 问题。设设计矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，响应向量为 $y \\in \\mathbb{R}^{2}$，由下式给出\n$$\nX = \\begin{pmatrix}\n1  1 \\\\\n0  1\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}.\n$$\n对于正则化参数 $ \\lambda \\geq 0 $，LASSO 估计量 $ \\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{2} $ 定义为以下表达式的最小化子\n$$\n\\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1}.\n$$\n从凸优化的基本次梯度最优性条件出发，并利用路径坐标下降和解路径的概念框架，确定当 $ \\lambda $ 从一个足够大的值减小时， $ \\hat{\\beta}(\\lambda) $ 的活跃集发生改变的前两个断点 $ \\lambda_{1} $ 和 $ \\lambda_{2} $。对指定的 $X$ 和 $y$ 精确计算这两个断点的值。然后，通过推导路径段的显式表达式，验证在每个连续断点之间的区间上，LASSO 解 $ \\hat{\\beta}(\\lambda) $ 线性依赖于 $ \\lambda $。\n\n使用 LaTeX 的 $ \\texttt{pmatrix} $ 环境，将你的最终答案以单行矩阵的形式表示为数对 $ (\\lambda_{1}, \\lambda_{2}) $。不需要四舍五入，也不涉及单位。",
            "solution": "我们从 LASSO 公式出发\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1},\n$$\n其中 $ X = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} $ 且 $ y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $。将 $X$ 的列表示为 $ x_{1} $ 和 $ x_{2} $，因此 $ x_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $ 且 $ x_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $。定义 Gram 矩阵 $ G = X^{\\top} X $ 和相关向量 $ c = X^{\\top} y $。计算它们：\n$$\nG = X^{\\top} X = \\begin{pmatrix}\nx_{1}^{\\top} x_{1}  x_{1}^{\\top} x_{2} \\\\\nx_{2}^{\\top} x_{1}  x_{2}^{\\top} x_{2}\n\\end{pmatrix}\n= \\begin{pmatrix}\n1  1 \\\\\n1  2\n\\end{pmatrix},\n\\qquad\nc = X^{\\top} y = \\begin{pmatrix}\nx_{1}^{\\top} y \\\\\nx_{2}^{\\top} y\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 \\\\\n3\n\\end{pmatrix}.\n$$\nLASSO 的次梯度最优性条件表明，在最优解 $ \\hat{\\beta}(\\lambda) $ 处，对于每个坐标 $ j \\in \\{1,2\\} $，\n$$\nx_{j}^{\\top} (y - X \\hat{\\beta}(\\lambda)) = \\lambda s_{j},\n$$\n其中，如果 $ \\hat{\\beta}_{j}(\\lambda) \\neq 0 $，则 $ s_{j} = \\operatorname{sign}(\\hat{\\beta}_{j}(\\lambda)) $；如果 $ \\hat{\\beta}_{j}(\\lambda) = 0 $，则 $ s_{j} \\in [-1,1] $。以向量形式表示，当活跃集 $ A \\subseteq \\{1,2\\} $ 和符号模式 $ s_{A} $ 固定时，Karush–Kuhn–Tucker (KKT) 条件得出\n$$\nG_{A A} \\hat{\\beta}_{A}(\\lambda) = c_{A} - \\lambda s_{A},\n$$\n并且对于 $ j \\notin A $，有 $ |x_{j}^{\\top} (y - X \\hat{\\beta}(\\lambda))| \\leq \\lambda $。因此，对于固定的 $ A $ 和 $ s_{A} $，解段为\n$$\n\\hat{\\beta}_{A}(\\lambda) = G_{A A}^{-1} \\big( c_{A} - \\lambda s_{A} \\big),\n$$\n它是关于 $ \\lambda $ 的仿射（因此也是线性）函数。\n\n我们现在计算断点。初始断点 $ \\lambda_{1} $ 是使零解不再是最优解的最小 $ \\lambda $。在 $ \\hat{\\beta}(\\lambda) = 0 $ 时，最优性要求对所有 $ j $ 都有 $ |c_{j}| \\leq \\lambda $，因此最大的相关性决定了第一个进入模型的变量：\n$$\n\\lambda_{\\max} = \\max_{j} |c_{j}| = \\max \\{ |2|, |3| \\} = 3.\n$$\n因此，在 $ \\lambda = 3 $ 时，索引为 $ j = 2 $ 的变量即将以正号进入模型（因为 $ c_{2} = 3  0 $）。因此，第一个断点是 $ \\lambda_{1} = 3 $。\n\n对于略小于 $ \\lambda_{1} = 3 $ 的 $ \\lambda $，活跃集为 $ A = \\{2\\} $，且 $ s_{2} = +1 $。在此段上的 KKT 方程为\n$$\ng_{22} \\hat{\\beta}_{2}(\\lambda) = c_{2} - \\lambda,\n\\quad \\text{其中} \\quad g_{22} = 2, \\; c_{2} = 3.\n$$\n因此\n$$\n\\hat{\\beta}_{2}(\\lambda) = \\frac{3 - \\lambda}{2},\n\\quad \\hat{\\beta}_{1}(\\lambda) = 0,\n\\quad \\text{在 } \\lambda \\in [\\lambda_{2}, \\lambda_{1}] \\text{ 时有效}。\n$$\n下一个断点 $ \\lambda_{2} $ 发生在非活跃坐标 $ j = 1 $ 达到边界 $ |x_{1}^{\\top} r(\\lambda)| = \\lambda $ 时，其中 $ r(\\lambda) = y - X \\hat{\\beta}(\\lambda) $ 是残差。在当前段上，\n$$\nr(\\lambda) = y - x_{2} \\hat{\\beta}_{2}(\\lambda)\n= \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot \\frac{3 - \\lambda}{2}\n= \\begin{pmatrix} 2 - \\frac{3 - \\lambda}{2} \\\\ 1 - \\frac{3 - \\lambda}{2} \\end{pmatrix}\n= \\begin{pmatrix} \\frac{1}{2} + \\frac{\\lambda}{2} \\\\ -\\frac{1}{2} + \\frac{\\lambda}{2} \\end{pmatrix}.\n$$\n因此，\n$$\nx_{1}^{\\top} r(\\lambda) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} \\frac{1}{2} + \\frac{\\lambda}{2} \\\\ -\\frac{1}{2} + \\frac{\\lambda}{2} \\end{pmatrix}\n= \\frac{1}{2} + \\frac{\\lambda}{2}.\n$$\n进入条件 $ |x_{1}^{\\top} r(\\lambda)| = \\lambda $（预计为正号，因为当 $ \\lambda $ 略小于 $3$ 时，$ x_{1}^{\\top} r(\\lambda) $ 是正的）给出\n$$\n\\frac{1}{2} + \\frac{\\lambda}{2} = \\lambda \\quad \\Rightarrow \\quad \\lambda = 1.\n$$\n因此第二个断点是 $ \\lambda_{2} = 1 $。\n\n对于 $ \\lambda \\in [0, 1] $，两个变量都以正号处于活跃状态，因此 $ A = \\{1,2\\} $ 且 $ s = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $。使用完整的 KKT 系统，\n$$\nG \\hat{\\beta}(\\lambda) = c - \\lambda s,\n\\quad \\text{所以} \\quad\n\\hat{\\beta}(\\lambda) = G^{-1} (c - \\lambda s).\n$$\n计算 $ G^{-1} $：\n$$\nG = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}, \\quad \\det(G) = 1 \\cdot 2 - 1 \\cdot 1 = 1, \\quad\nG^{-1} = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix}.\n$$\n那么\n$$\nc - \\lambda s = \\begin{pmatrix} 2 - \\lambda \\\\ 3 - \\lambda \\end{pmatrix},\n\\quad \\hat{\\beta}(\\lambda) = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 2 - \\lambda \\\\ 3 - \\lambda \\end{pmatrix}\n= \\begin{pmatrix}\n2(2 - \\lambda) - (3 - \\lambda) \\\\\n-(2 - \\lambda) + (3 - \\lambda)\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 - \\lambda \\\\\n1\n\\end{pmatrix}.\n$$\n因此，在 $ \\lambda \\in [0, 1] $ 上，解路径是关于 $ \\lambda $ 的显式线性函数，其中 $ \\hat{\\beta}_{1}(\\lambda) = 1 - \\lambda $ 且 $ \\hat{\\beta}_{2}(\\lambda) = 1 $。在 $ \\lambda \\in [1, 3] $ 上，解路径同样是关于 $ \\lambda $ 的线性函数，其中 $ \\hat{\\beta}_{2}(\\lambda) = \\frac{3 - \\lambda}{2} $ 且 $ \\hat{\\beta}_{1}(\\lambda) = 0 $。在 $ \\lambda = 1 $ 处连续性成立，因为两个表达式都得出 $ \\hat{\\beta}(1) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} $；在 $ \\lambda = 3 $ 处，我们得到所要求的 $ \\hat{\\beta}(3) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $。\n\n因此，LASSO 路径的前两个断点是 $ \\lambda_{1} = 3 $ 和 $ \\lambda_{2} = 1 $，并且这些断点之间的路径段是关于 $ \\lambda $ 的线性函数。",
            "answer": "$$\\boxed{\\begin{pmatrix} 3  1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "理解了解路径是如何构建的之后，我们便可以探索如何更高效地计算它。这个高级练习  介绍了一种重要的加速技术：“筛选规则”（screening rules）。你将学习如何利用 KKT 条件的性质，安全地剔除那些在当前步骤下解很可能为零的变量，从而显著降低求解路径上每一步的计算成本。",
            "id": "3465831",
            "problem": "考虑带有平方误差损失的最小绝对收缩和选择算子 (LASSO) 问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的列已中心化并缩放至单位欧几里得范数，$y \\in \\mathbb{R}^{n}$。给定一个正则化参数的路径序列 $\\{\\lambda_{k-1}, \\lambda_{k}\\}$，其中 $\\lambda_{k}  \\lambda_{k-1}$，以及一个先前计算出的解 $\\beta(\\lambda_{k-1})$。任务是推导并应用一个序列筛选规则（通常称为强规则），利用 $\\beta(\\lambda_{k-1})$ 在 $\\lambda_{k}$ 处提出一个筛选集，然后通过检查最初被剔除的特征在 $\\lambda_{k}$ 处的 Karush-Kuhn-Tucker (KKT) 条件来验证其安全性。\n\n从第一性原理出发：\n- 使用在给定 $\\lambda$ 下 LASSO 最优解的 KKT 条件：对于每个坐标 $j \\in \\{1,\\dots,p\\}$，残差为 $r(\\lambda) = y - X\\beta(\\lambda)$，相关性为 $c_{j}(\\lambda) = x_{j}^{\\top} r(\\lambda)$，KKT 条件要求，如果 $\\beta_{j}(\\lambda) \\neq 0$，则 $c_{j}(\\lambda) = \\lambda \\,\\mathrm{sign}(\\beta_{j}(\\lambda))$；如果 $\\beta_{j}(\\lambda) = 0$，则 $|c_{j}(\\lambda)| \\le \\lambda$。\n- 使用单位范数列的平方误差损失的单位斜率界：相关性 $c_{j}(\\lambda)$ 作为 $\\lambda$ 的函数，在解路径上是 1-Lipschitz 的。\n- 基于这些元素，根据在 $\\lambda_{k-1}$ 处先前计算出的量，推导出在 $\\lambda_{k}$ 处剔除特征 $j$ 的一个充分条件。\n\n将此方法应用于下面的具体实例，然后执行安全性检查：\n- 设 $n=4$ 且 $p=5$。$X$ 的列为\n$$\nx_{1} = \\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}, \\quad\nx_{2} = \\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}, \\quad\nx_{3} = \\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}, \\quad\nx_{4} = \\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}, \\quad\nx_{5} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\0\\\\1\\\\0\\end{pmatrix}.\n$$\n- 设 $y = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix}$，$\\lambda_{k-1} = 2$，且 $\\lambda_{k} = 1.3$。\n- 在 $\\lambda_{k-1}$ 处，假设唯一解为 $\\beta(\\lambda_{k-1}) = 0$。\n\n任务：\n- 使用 KKT 条件和单位斜率界，从 $\\lambda_{k-1}$ 处的残差和相关性推导出一个充分的剔除不等式，以在 $\\lambda_{k}$ 处提出一个筛选出的集合，并列出提议剔除的特征。\n- 在 $\\lambda_{k}$ 处求解仅限于你所识别的保留集的 LASSO 子问题，并获得残差 $r(\\lambda_{k})$。\n- 通过检查所有最初被剔除的特征在 $\\lambda_{k}$ 处的 KKT 条件来验证安全性。如果任何特征违反了 KKT 条件，则必须将其加回；否则筛选是安全的。\n- 报告在 KKT 验证后，在 $\\lambda_{k}$ 处被安全剔除的特征数量。你的答案必须是一个没有单位的数字。不需要四舍五入。",
            "solution": "问题要求我们为 LASSO 问题推导并应用一个序列筛选规则，然后验证其安全性。LASSO 优化问题由下式给出\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\n其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda  0$ 是正则化参数。$X$ 的列，记作 $x_j$，被缩放至单位欧几里得范数，即对所有 $j \\in \\{1,\\dots,p\\}$ 都有 $\\|x_j\\|_2 = 1$。\n\n首先，我们推导筛选规则。Karush-Kuhn-Tucker (KKT) 条件是解 $\\beta(\\lambda)$ 在给定 $\\lambda$ 下为最优解的充分必要条件。令 $r(\\lambda) = y - X\\beta(\\lambda)$ 为残差，$c_j(\\lambda) = x_j^{\\top}r(\\lambda)$ 为第 $j$ 个特征与残差的相关性。KKT 条件陈述对于每个坐标 $j$：\n\\begin{itemize}\n    \\item 如果 $\\beta_j(\\lambda) \\neq 0$，则 $c_j(\\lambda) = \\lambda \\, \\mathrm{sign}(\\beta_j(\\lambda))$。\n    \\item 如果 $\\beta_j(\\lambda) = 0$，则 $|c_j(\\lambda)| \\le \\lambda$。\n\\end{itemize}\n如果在正则化水平 $\\lambda_k$ 处，我们可以保证特征 $j$ 的最优系数 $\\beta_j(\\lambda_k)$ 为零，那么该特征就可以被安全地剔除。根据 KKT 条件，$\\beta_j(\\lambda_k) = 0$ 的一个充分条件是严格不等式 $|c_j(\\lambda_k)|  \\lambda_k$。\n\n我们已知在 $\\lambda_{k-1}  \\lambda_k$ 处的一个先前计算出的解 $\\beta(\\lambda_{k-1})$，并且可以计算出相应的相关性 $c_j(\\lambda_{k-1})$。为了将这些与未知的相关性 $c_j(\\lambda_k)$ 联系起来，我们使用所提供的单位斜率界，该界表明函数 $c_j(\\lambda)$ 是 1-Lipschitz 连续的：\n$$\n|c_j(\\lambda_{k}) - c_j(\\lambda_{k-1})| \\le |\\lambda_{k} - \\lambda_{k-1}|\n$$\n由于 $\\lambda_k  \\lambda_{k-1}$，这变为 $|c_j(\\lambda_{k}) - c_j(\\lambda_{k-1})| \\le \\lambda_{k-1} - \\lambda_k$。使用三角不等式，我们可以对 $|c_j(\\lambda_k)|$ 进行界定：\n$$\n|c_j(\\lambda_k)| = |c_j(\\lambda_{k-1}) + (c_j(\\lambda_k) - c_j(\\lambda_{k-1}))| \\le |c_j(\\lambda_{k-1})| + |c_j(\\lambda_k) - c_j(\\lambda_{k-1})|\n$$\n应用 Lipschitz 属性，我们得到在 $\\lambda_k$ 处相关性的一个上界：\n$$\n|c_j(\\lambda_k)| \\le |c_j(\\lambda_{k-1})| + \\lambda_{k-1} - \\lambda_k\n$$\n如果这个上界本身小于 $\\lambda_k$，那么我们就能保证 $|c_j(\\lambda_k)|  \\lambda_k$，从而确保 $\\beta_j(\\lambda_k)=0$。这就给出了充分的剔除不等式：\n$$\n|c_j(\\lambda_{k-1})| + \\lambda_{k-1} - \\lambda_k  \\lambda_k\n$$\n整理这个不等式，得到筛选规则：如果一个特征 $j$ 满足以下条件，则可以被初步剔除\n$$\n|c_j(\\lambda_{k-1})|  2\\lambda_k - \\lambda_{k-1}\n$$\n\n现在，我们将此规则应用于给定的实例。数据为：\n$n=4$，$p=5$，$\\lambda_{k-1}=2$，$\\lambda_k=1.3$。\n$y = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix}$，以及 $X$ 的列 $x_1, \\dots, x_5$。\n问题陈述，在 $\\lambda_{k-1}=2$ 处，解为 $\\beta(\\lambda_{k-1}) = 0$。\n残差为 $r(\\lambda_{k-1}) = y - X \\cdot 0 = y$。\n在 $\\lambda_{k-1}$ 处的相关性为 $c_j(\\lambda_{k-1})=x_j^{\\top}y$：\n$c_1(\\lambda_{k-1}) = \\begin{pmatrix}1  0  0  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = 2$。\n$c_2(\\lambda_{k-1}) = \\begin{pmatrix}0  1  0  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = 0$。\n$c_3(\\lambda_{k-1}) = \\begin{pmatrix}0  0  1  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = -1.5$。\n$c_4(\\lambda_{k-1}) = \\begin{pmatrix}0  0  0  1\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = -1$。\n$c_5(\\lambda_{k-1}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  0  1  0\\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{2}}(2 - 1.5) = \\frac{0.5}{\\sqrt{2}} = \\frac{\\sqrt{2}}{4}$。\n\n筛选规则的阈值为 $2\\lambda_k - \\lambda_{k-1} = 2(1.3) - 2 = 2.6 - 2 = 0.6$。\n如果 $|c_j(\\lambda_{k-1})|  0.6$，我们剔除特征 $j$：\n\\begin{itemize}\n    \\item $j=1$: $|c_1(\\lambda_{k-1})| = |2| = 2 \\ge 0.6$。保留。\n    \\item $j=2$: $|c_2(\\lambda_{k-1})| = |0| = 0  0.6$。剔除。\n    \\item $j=3$: $|c_3(\\lambda_{k-1})| = |-1.5| = 1.5 \\ge 0.6$。保留。\n    \\item $j=4$: $|c_4(\\lambda_{k-1})| = |-1| = 1 \\ge 0.6$。保留。\n    \\item $j=5$: $|c_5(\\lambda_{k-1})| = |\\frac{\\sqrt{2}}{4}| \\approx 0.354  0.6$。剔除。\n\\end{itemize}\n提议剔除的特征集是 $\\{2, 5\\}$，保留的特征集是 $\\mathcal{A}=\\{1, 3, 4\\}$。\n\n接下来，我们在 $\\lambda_k=1.3$ 处求解仅限于集合 $\\mathcal{A}$ 中特征的 LASSO 子问题。该子问题是：\n$$\n\\min_{\\beta_1, \\beta_3, \\beta_4} \\ \\frac{1}{2}\\|y - (x_1\\beta_1 + x_3\\beta_3 + x_4\\beta_4)\\|_{2}^{2} + 1.3 (|\\beta_1| + |\\beta_3| + |\\beta_4|)\n$$\n列 $x_1, x_3, x_4$ 是标准正交的。对于一个标准正交的设计矩阵 $X_{\\mathcal{A}}$，LASSO 解由相关性 $X_{\\mathcal{A}}^{\\top}y$ 的坐标级软阈值给出。对于 $j \\in \\mathcal{A}$ 的解是 $\\beta_j(\\lambda_k) = S_{\\lambda_k}(x_j^{\\top}y)$，其中 $S_{\\alpha}(z) = \\mathrm{sign}(z)\\max(|z|-\\alpha, 0)$。\n相关的相关性是 $c_j(\\lambda_{k-1}) = x_j^{\\top}y$。\n\\begin{itemize}\n    \\item $\\beta_1(1.3) = S_{1.3}(2) = \\mathrm{sign}(2)\\max(|2|-1.3, 0) = 0.7$。\n    \\item $\\beta_3(1.3) = S_{1.3}(-1.5) = \\mathrm{sign}(-1.5)\\max(|-1.5|-1.3, 0) = -0.2$。\n    \\item $\\beta_4(1.3) = S_{1.3}(-1) = \\mathrm{sign}(-1)\\max(|-1|-1.3, 0) = 0$。\n\\end{itemize}\n子问题的解给出了初步的完整解向量 $\\beta(\\lambda_k) = (0.7, 0, -0.2, 0, 0)^{\\top}$。\n在 $\\lambda_k=1.3$ 处的残差是 $r(\\lambda_k) = y - X\\beta(\\lambda_k)$：\n$$\nr(1.3) = y - (0.7x_1 - 0.2x_3) = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} - \\left(0.7\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix} - 0.2\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}\\right) = \\begin{pmatrix}2\\\\0\\\\-1.5\\\\-1\\end{pmatrix} - \\begin{pmatrix}0.7\\\\0\\\\-0.2\\\\0\\end{pmatrix} = \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix}\n$$\n\n最后，我们通过检查在 $\\lambda_k=1.3$ 处被剔除特征 $j \\in \\{2, 5\\}$ 的 KKT 条件来验证筛选的安全性。对于这些特征，我们假设 $\\beta_j(1.3)=0$，因此我们必须检查是否 $|c_j(1.3)| = |x_j^{\\top}r(1.3)| \\le \\lambda_k = 1.3$。\n\\begin{itemize}\n    \\item 对于 $j=2$：$c_2(1.3) = x_2^{\\top}r(1.3) = \\begin{pmatrix}0  1  0  0\\end{pmatrix} \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix} = 0$。\n    条件是 $|0| \\le 1.3$，成立。\n    \\item 对于 $j=5$：$c_5(1.3) = x_5^{\\top}r(1.3) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  0  1  0\\end{pmatrix} \\begin{pmatrix}1.3\\\\0\\\\-1.3\\\\-1\\end{pmatrix} = \\frac{1}{\\sqrt{2}}(1.3 - 1.3) = 0$。\n    条件是 $|0| \\le 1.3$，成立。\n\\end{itemize}\n两个被剔除的特征都满足 KKT 条件。因此，筛选规则是安全的，没有发生违反 KKT 条件的情况。在简化问题上的解是完整问题的正确解。特征 $\\{2, 5\\}$ 被安全地剔除了。\n\n在 $\\lambda_k=1.3$ 处被安全剔除的特征数量是 $2$。",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}