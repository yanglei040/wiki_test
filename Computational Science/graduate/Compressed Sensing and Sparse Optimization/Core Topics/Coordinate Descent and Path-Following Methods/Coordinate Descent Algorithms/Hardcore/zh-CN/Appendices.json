{
    "hands_on_practices": [
        {
            "introduction": "在掌握了如何高效实现单次坐标更新后，下一个关键问题是如何选择更新坐标的顺序。本练习旨在比较标准的循环坐标下降（CCD）与随机坐标下降（RCD）在求解Lasso问题 $f(x) = \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 + \\lambda\\,\\|x\\|_1$ 时的性能差异。您将通过构建一个具有高度相关列的“最坏情况”场景，来亲手复现循环下降中可能出现的“之”字形（zig-zagging）收敛现象 。这个实践将直观地展示为何随机化策略通常能带来更稳健、更快速的收敛，尤其是在处理现实世界数据中常见的相关特征时。",
            "id": "3441210",
            "problem": "考虑在压缩感知和稀疏优化中的最小绝对值收缩和选择算子 (Lasso) 目标函数，对于传感矩阵 $A \\in \\mathbb{R}^{m \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{m}$，其定义为\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。坐标下降算法通过每次只针对 $x$ 的一个坐标最小化 $f$，同时保持其他坐标不变，来优化 $f(x)$。考虑两种选择规则：\n- 循环坐标下降 (CCD)：按固定顺序 $1,2,\\dots,p$ 重复访问坐标。\n- 随机坐标下降 (RCD)：在每次更新时，有放回地均匀随机选择一个坐标。\n\n一个轮次 (epoch) 定义为恰好 $p$ 次单坐标更新。两种方法都从 $x^{(0)} = 0$ 开始，并在每次更新时沿所选坐标方向进行精确最小化。\n\n设计一个最坏情况下的传感矩阵 $A$ 和响应 $y$，通过构造高度相关的列，为循环规则引发“Z”字形振荡 (zig-zagging)。具体来说，对于给定的 $m$、$p$ 和相关性参数 $\\rho \\in [0,1)$，构造 $A$ 的前两列，使其内积近似为 $\\rho$ 且范数为单位长度，并构造其余列，使其与前两列及彼此之间近似不相关。对于一个只有前两个条目非零且大小相等的稀疏真实向量 $x^\\star$，设置 $y = A x^\\star$。然后，程序必须对每个测试用例，比较 CCD 和 RCD 在每个轮次的平均目标函数下降值，该平均值是在固定轮次 $E$ 上计算的。随机选择必须使用固定的伪随机种子 $2025$，以确保结果是可复现的。\n\n您的程序必须仅基于 $f(x)$ 的定义和每个坐标的一维优化子问题，实现这两种选择规则，并在每次更新时使用精确的单坐标最小化。对于每个测试用例，它必须计算并报告比率\n$$\nr \\;=\\; \\frac{\\text{RCD下每轮次的平均目标函数下降值}}{\\text{CCD下每轮次的平均目标函数下降值}},\n$$\n该比率表示为浮点数。\n\n测试套件。使用以下四个测试用例，所有用例均设置 $E = 50$ 轮次且无观测噪声：\n- 用例 1：$m = 200, p = 2, \\rho = 0.999, \\lambda = 0.05, x^\\star = [1,\\,1]^T$。\n- 用例 2：$m = 200, p = 2, \\rho = 0.9, \\lambda = 0.05, x^\\star = [1,\\,1]^T$。\n- 用例 3：$m = 200, p = 2, \\rho = 0.0, \\lambda = 0.05, x^\\star = [1,\\,1]^T$。\n- 用例 4：$m = 200, p = 10$，其中列 1 和 2 的相关性为 $\\rho = 0.999$ 且范数为单位长度，列 3 到 10 为单位范数的随机列，与前两列及彼此之间近似不相关，$\\lambda = 0.05, x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$（长度为 $p$）。\n\n构建细节。对于 $p \\ge 2$，通过抽样独立同分布的标准正态项并归一化到单位欧几里得范数来生成第一列 $a_1$。生成 $a_2$ 的方式为\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\n其中 $w$ 具有独立同分布的标准正态项，然后将 $a_2$ 归一化到单位范数。对于 $p > 2$，将列 $a_j$（$j \\ge 3$）生成为归一化到单位范数的独立标准正态向量。设置 $A = [a_1,\\dots,a_p]$ 和 $y = A x^\\star$。对于 CCD 和 RCD，均初始化 $x^{(0)} = 0$，将一个轮次定义为 $p$ 次更新，并在每个轮次后记录 $f(x)$。\n\n每轮次的平均目标函数下降值。对于每种方法，按以下方式计算每轮次的平均下降值\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\n其中 $x^{(t)}$ 是 $t$ 轮次后的迭代向量。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个由四个浮点数比率 $[r_1,r_2,r_3,r_4]$ 组成的逗号分隔列表，顺序与上述测试用例一致，并用方括号括起来。所有随机抽样均使用伪随机种子 $2025$。所有输出均为无量纲的浮点数，不涉及物理单位、角度单位或百分比。",
            "solution": "此问题被评估为**有效** (valid)。这是一个数值优化领域的适定 (well-posed) 且有科学依据的问题，特别关注于坐标下降算法在 Lasso 目标函数上的性能特征。所有参数、流程和评估指标都得到了明确且形式化的定义。\n\n问题的核心是实现和比较循环坐标下降 (CCD) 和随机坐标下降 (RCD) 在 Lasso 目标函数上的表现，该目标函数由以下公式给出：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\n在这里，$x \\in \\mathbb{R}^p$ 是待优化的参数向量，$A \\in \\mathbb{R}^{m \\times p}$ 是传感矩阵，$y \\in \\mathbb{R}^m$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n坐标下降算法通过迭代地对单个坐标 $x_j$ 最小化该函数，同时保持所有其他坐标 $x_k$（对于 $k \\neq j$）固定，来优化此函数。坐标 $x_j$ 的一维子问题是最小化：\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\n展开目标函数，我们分离出依赖于 $x_j$ 的项：\n$$\n\\begin{aligned}\nf(x) = \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\n其中 $a_k$ 是 $A$ 的第 $k$ 列。为了对 $x_j$ 进行最小化，我们可以忽略不依赖于它的项。子问题变为最小化：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\n展开平方范数项：\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\n问题规定所有列 $a_j$ 都被归一化为单位欧几里得范数，即 $\\|a_j\\|_2^2 = a_j^T a_j = 1$。这简化了子问题为：\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\n这是一个关于 $x_j$ 的二次函数加上一个 $\\ell_1$ 范数惩罚项。$\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ 的解由软阈值算子 (soft-thresholding operator) 给出，$z^* = S_\\lambda(c)$。在我们的例子中，$c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$。\n因此，坐标 $x_j$ 的更新规则是：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\n其中 $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$。为了计算效率，我们可以预先计算格拉姆矩阵 (Gram matrix) $A^T A$ 和向量 $A^T y$。令 $G = A^T A$ 且 $c_y = A^T y$。更新变为：\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\n求和项可以写为 $(G x)_j - G_{jj} x_j$。由于 $G_{jj}=a_j^T a_j=1$，软阈值函数的参数是 $(c_y)_j - ((G x)_j - x_j)$。向量 $x$ 包含坐标的最新更新值。\n\n算法流程如下：\n1.  **初始化**：为保证可复现性，将伪随机种子设置为 $2025$。对每个测试用例，按规定构造矩阵 $A$ 和向量 $y$。列 $a_j$ 从独立同分布的标准正态分布中生成并被归一化。前两列 $a_1$ 和 $a_2$ 被构造成具有指定的相关性结构。响应是无噪声的，$y=Ax^\\star$。预计算 $A^TA$ 和 $A^Ty$。初始化解的估计值 $x^{(0)} = 0$。\n\n2.  **循环坐标下降 (CCD)**：迭代 $E$ 个轮次。在每个轮次中，从 $j=1, \\dots, p$ 顺序更新坐标。\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    在更新 $x_j$ 时使用的 $x_k$ 值是当前可用的最新值。\n\n3.  **随机坐标下降 (RCD)**：迭代 $E$ 个轮次。在每个轮次中，执行 $p$ 次更新。对于每次更新，从 $\\{1, \\dots, p\\}$ 中有放回地均匀随机选择一个坐标 $j$。应用与 CCD 中相同的更新规则。\n\n4.  **评估**：对于 CCD 和 RCD，经过 $t$ 个轮次后的迭代向量记为 $x^{(t)}$。记录 $t=0, \\dots, E$ 的目标函数值 $f(x^{(t)})$。每轮次的平均目标函数下降值计算如下：\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    每个测试用例最终报告的值是比率 $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$。\n\n代码实现了这一逻辑，仔细遵循了 $A$ 的构造细节、CCD 和 RCD 的迭代更新方案，以及最终性能比率的计算。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p >= 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j >= 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd > 0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "随机坐标选择是一个强大的策略，但均匀随机是最佳选择吗？本高级练习将带您探索重要性采样（importance sampling）的世界，这是一种通过非均匀地选择坐标来加速收敛的先进技术。其核心思想是更频繁地选择那些有望带来更大目标函数下降的坐标。您需要在一个给定的计算预算下，推导出能最大化期望目标下降的最优采样分布 。这个练习将算法的理论性质（如每坐标的下降量）与实际约束（如每坐标的计算成本）联系起来，让您一窥现代随机优化算法设计的前沿思想。",
            "id": "3441225",
            "problem": "考虑一个来自压缩感知的稀疏恢复目标，该目标被建模为一个复合凸问题，由一个光滑的数据拟合项和一个稀疏性诱导正则化项组成。设 $A \\in \\mathbb{R}^{3 \\times 3}$，$b \\in \\mathbb{R}^{3}$，并定义目标函数 $F(x) = f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ 且 $g(x) = \\lambda \\|x\\|_{1}$，$\\lambda > 0$。使用近端坐标梯度步进行坐标下降更新，其中坐标级 Lipschitz 常数 $L_{i}$ 由光滑部分 $f$ 定义。假设更新坐标 $i$ 的成本是一个已知的正权重 $w_{i}$，并且每次迭代有一个固定的计算预算 $B > 0$ 来约束每次迭代的期望成本。\n\n在当前迭代点 $x \\in \\mathbb{R}^{3}$，给定以下具体实例：\n- $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$，$b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$，$\\lambda = 0.5$，以及 $x = \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix}$。\n- 坐标更新成本为 $w_{1} = 1$，$w_{2} = 3$，$w_{3} = 2$。\n- 每次迭代的计算预算为 $B = 2$。\n\n仅使用 $f$ 的坐标级 Lipschitz 常数、$\\ell_{1}$ 范数的近端算子以及由近端步诱导的坐标级广义梯度映射的基本定义，按以下步骤进行：\n\n1. 从第一性原理出发，计算 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ 的坐标级 Lipschitz 常数 $L_{i}$。\n2. 使用梯度 $\\nabla f(x)$ 和近端映射 $\\operatorname{prox}_{\\alpha |\\cdot|}(\\cdot)$ 的定义，推导单个坐标 $i$ 在步长为 $1/L_{i}$ 时的单步近端坐标更新，并用 $x$、$\\nabla f(x)$ 和 $L_{i}$ 写出相应的广义梯度映射 $G_{i}(x)$。\n3. 利用从 $f$ 的光滑性和 $g$ 的可分性推导出的近端坐标更新的标准下降性质，证明目标函数在每个坐标上的下降量由一个形式为 $\\Delta_{i} = c_{i} G_{i}(x)^{2}$ 的标量所下界，其中 $c_{i} > 0$ 依赖于 $L_{i}$。并对于给定的 $x$，显式地计算 $i \\in \\{1,2,3\\}$ 时的 $\\Delta_{i}$。\n4. 对于坐标的均匀采样，即对所有 $i \\in \\{1,2,3\\}$，概率为 $p_{i}^{\\text{unif}} = 1/3$，计算每次迭代的期望下降量 $\\mathbb{E}[\\Delta] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} \\Delta_{i}$，每次迭代的期望成本 $\\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$，以及定义为比率 $\\mathbb{E}[\\Delta] / \\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$ 的每单位成本的期望下降量。提供精确值（不进行四舍五入）。\n5. 现在，在每次迭代的固定计算预算 $B=2$ 的条件下，推导采样分布 $p = (p_{1}, p_{2}, p_{3})$，该分布最大化每次迭代的期望下降量 $\\sum_{i=1}^{3} p_{i} \\Delta_{i}$，并满足约束条件 $\\sum_{i=1}^{3} p_{i} = 1$，$\\sum_{i=1}^{3} p_{i} w_{i} \\le B$ 以及对所有 $i$ 都有 $p_{i} \\ge 0$。从第一性原理出发，论证为什么最优解必须集中在至多两个坐标上，并在此实例中显式地求解最优的 $p$。\n\n以行矩阵形式给出你的最终答案，即最优采样分布 $p^{\\star}$。无需四舍五入。最终答案中不要包含任何单位。如果计算任何中间数值，请尽可能将其保留为精确的有理数。",
            "solution": "首先根据指定标准对问题进行验证。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 目标函数：$F(x) = f(x) + g(x)$，其中 $x \\in \\mathbb{R}^{3}$。\n- 光滑项：$f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{3 \\times 3}$，$b \\in \\mathbb{R}^{3}$。\n- 非光滑项：$g(x) = \\lambda \\|x\\|_{1}$，其中 $\\lambda > 0$。\n- 问题实例：\n  - $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$\n  - $b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$\n  - $\\lambda = 0.5$\n  - 当前迭代点：$x = \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix}$\n- 坐标更新成本：$w_{1} = 1$，$w_{2} = 3$，$w_{3} = 2$。\n- 计算预算：$B = 2$。\n- 任务是进行一个与坐标下降相关的五部分分析，涉及 Lipschitz 常数、近端更新、下降保证、均匀采样以及预算下的最优采样。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学性**：该问题是近端坐标下降在 LASSO 问题上的一个标准应用，LASSO 问题是稀疏优化和压缩感知的基石。所使用的所有概念（Lipschitz 常数、近端算子、基于目标的采样）都是凸优化领域中基础且成熟的概念。该问题在科学上和数学上都是合理的。\n- **适定性**：该问题陈述清晰，并提供了所有必要的数据。目标函数是凸的，确保了优化过程是良定义的。最后一部分涉及求解一个线性规划，它有一个良定义的解。该问题存在唯一且有意义的解。\n- **客观性**：该问题使用精确的数学语言表述，没有任何主观或模棱两可的术语。\n- **完整性和一致性**：所有矩阵和向量的维度都是一致的。为所要求任务提供的数据是完整的。没有内部矛盾。\n- **真实性和可行性**：该问题是一个小规模但真实的稀疏恢复问题实例。所有计算在计算上都是可行的。\n\n**步骤3：结论与行动**\n该问题有效。下面提供了完整的解答。\n\n### 详细解答\n\n解答分为五个部分呈现，如问题陈述所要求。\n\n**1. 坐标级 Lipschitz 常数**\n\n目标函数的光滑部分是 $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$。$f$ 的梯度是 $\\nabla f(x) = A^T(Ax - b)$，其 Hessian 矩阵是 $\\nabla^2 f(x) = A^T A$。$\\nabla_i f(x)$ 的坐标级 Lipschitz 常数 $L_i$ 是映射 $t \\mapsto \\nabla_i f(x + t e_i)$ 的 Lipschitz 常数，由 Hessian 矩阵的第 i 个对角元素给出：$L_i = (\\nabla^2 f(x))_{ii} = (A^T A)_{ii}$。\n设 $a_i$ 是矩阵 $A$ 的第 i 列。则 $(A^T A)_{ii} = a_i^T a_i = \\|a_i\\|_2^2$。\n给定 $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$，列向量为 $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$，$a_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$ 和 $a_3 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n我们计算平方范数：\n$L_1 = \\|a_1\\|_2^2 = 1^2 + 0^2 + 1^2 = 2$。\n$L_2 = \\|a_2\\|_2^2 = 0^2 + 2^2 + 1^2 = 5$。\n$L_3 = \\|a_3\\|_2^2 = 2^2 + 1^2 + 0^2 = 5$。\n因此，坐标级 Lipschitz 常数是 $L_1 = 2$，$L_2 = 5$ 和 $L_3 = 5$。\n\n**2. 近端坐标更新与广义梯度映射**\n\n坐标 $x_i$ 的近端坐标梯度更新是通过最小化 $f$ 的二次近似加上非光滑项 $g_i(x_i) = \\lambda |x_i|$ 来找到的。$x_i$ 的更新 $x_i^+$ 是：\n$$x_i^+ = \\arg\\min_{y \\in \\mathbb{R}} \\left( f(x) + \\nabla_i f(x) (y - x_i) + \\frac{L_i}{2}(y - x_i)^2 + \\lambda|y| \\right)$$\n这等价于找到下式的最小化子：\n$$y \\mapsto \\frac{L_i}{2} \\left( y - \\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right) \\right)^2 + \\lambda|y|$$\n解由参数为 $\\lambda/L_i$ 的 $g_i$ 的近端算子给出：\n$$x_i^+ = \\operatorname{prox}_{\\frac{\\lambda}{L_i}|\\cdot|}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right)$$\n对于 $\\ell_1$ 范数，近端算子是软阈值函数 $S_\\alpha(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$。因此，更新为：\n$$x_i^+ = S_{\\lambda/L_i}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right)$$\n坐标 i 的广义梯度映射 $G_i(x)$ 定义为 $G_i(x) = L_i(x_i - x_i^+)$。这可以写成：\n$$G_i(x) = L_i \\left( x_i - S_{\\lambda/L_i}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right) \\right)$$\n这个量衡量了在当前点 $x$ 处，坐标 i 的最优性条件被违反的程度。\n\n**3. 每个坐标的目标函数下降量**\n\n根据光滑函数的下降引理（具体来说，是由 $\\nabla_i f$ 的 Lipschitz 常数 $L_i$ 提供的二次上界），将坐标 i 更新为 $x_i^+$（令 $x^+$ 为新的迭代点）后 F 的变化是有界的：\n$$F(x^+) \\le F(x) + \\nabla_i f(x)(x_i^+ - x_i) + \\frac{L_i}{2}(x_i^+ - x_i)^2 + g_i(x_i^+) - g_i(x_i)$$\n$x_i^+$ 的最优性条件是 0 属于被最小化函数在 $x_i^+$ 处的次微分。这意味着存在一个次梯度 $\\zeta_i \\in \\partial g_i(x_i^+)$ 使得 $\\nabla_i f(x) + L_i(x_i^+ - x_i) + \\zeta_i = 0$。\n将 $\\nabla_i f(x) = -L_i(x_i^+ - x_i) - \\zeta_i$ 代入下降不等式：\n$$F(x^+) - F(x) \\le (-L_i(x_i^+ - x_i) - \\zeta_i)(x_i^+ - x_i) + \\frac{L_i}{2}(x_i^+ - x_i)^2 + g_i(x_i^+) - g_i(x_i)$$\n$$F(x^+) - F(x) \\le -L_i(x_i^+ - x_i)^2 - \\left(\\zeta_i(x_i^+ - x_i) - (g_i(x_i^+) - g_i(x_i))\\right) + \\frac{L_i}{2}(x_i^+ - x_i)^2$$\n根据 $g_i$ 的凸性，我们有 $g_i(x_i) \\ge g_i(x_i^+) + \\zeta_i(x_i - x_i^+)$，这意味着项 $-(\\dots)$ 是非正的。因此：\n$$F(x^+) - F(x) \\le -L_i(x_i^+ - x_i)^2 + \\frac{L_i}{2}(x_i^+ - x_i)^2 = -\\frac{L_i}{2}(x_i^+ - x_i)^2$$\n因此，目标函数的下降量 $F(x) - F(x^+)$ 被下界为：\n$$F(x) - F(x^+) \\ge \\frac{L_i}{2}(x_i^+ - x_i)^2$$\n使用定义 $G_i(x) = L_i(x_i - x_i^+)$，我们有 $x_i^+ - x_i = -\\frac{1}{L_i}G_i(x)$。将此代入界中：\n$$F(x) - F(x^+) \\ge \\frac{L_i}{2}\\left(-\\frac{1}{L_i}G_i(x)\\right)^2 = \\frac{1}{2L_i}G_i(x)^2$$\n这给出了每个坐标的下降量下界 $\\Delta_i = c_i G_i(x)^2$，其中 $c_i = \\frac{1}{2L_i}$。\n\n现在，我们计算在给定 $x$ 下，对于 $i \\in \\{1,2,3\\}$ 的 $\\Delta_i$。\n首先，计算梯度 $\\nabla f(x) = A^T(Ax-b)$:\n$Ax = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.4 \\end{pmatrix}$\n$Ax - b = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 1.8 \\\\ 0.4 \\end{pmatrix}$\n$\\nabla f(x) = \\begin{pmatrix} 1  0  1 \\\\ 0  2  1 \\\\ 2  1  0 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ 1.8 \\\\ 0.4 \\end{pmatrix} = \\begin{pmatrix} -0.1 \\\\ 4.0 \\\\ 0.8 \\end{pmatrix}$\n\n对于 $i=1$: $L_1=2, x_1=0.5, \\nabla_1 f(x)=-0.1, \\lambda=0.5$。\n$x_1^+ = S_{0.5/2}(0.5 - \\frac{1}{2}(-0.1)) = S_{0.25}(0.55) = 0.55 - 0.25 = 0.3$。\n$G_1(x) = L_1(x_1-x_1^+) = 2(0.5-0.3) = 0.4$。\n$\\Delta_1 = \\frac{1}{2L_1}G_1(x)^2 = \\frac{1}{2(2)}(0.4)^2 = \\frac{0.16}{4} = 0.04 = \\frac{1}{25}$。\n\n对于 $i=2$: $L_2=5, x_2=-0.1, \\nabla_2 f(x)=4.0, \\lambda=0.5$。\n$x_2^+ = S_{0.5/5}(-0.1 - \\frac{1}{5}(4.0)) = S_{0.1}(-0.9) = -(|-0.9|-0.1) = -0.8$。\n$G_2(x) = L_2(x_2-x_2^+) = 5(-0.1 - (-0.8)) = 5(0.7) = 3.5$。\n$\\Delta_2 = \\frac{1}{2L_2}G_2(x)^2 = \\frac{1}{2(5)}(3.5)^2 = \\frac{12.25}{10} = 1.225 = \\frac{49}{40}$。\n\n对于 $i=3$: $L_3=5, x_3=0.0, \\nabla_3 f(x)=0.8, \\lambda=0.5$。\n$x_3^+ = S_{0.5/5}(0.0 - \\frac{1}{5}(0.8)) = S_{0.1}(-0.16) = -(|-0.16|-0.1) = -0.06$。\n$G_3(x) = L_3(x_3-x_3^+) = 5(0.0 - (-0.06)) = 5(0.06) = 0.3$。\n$\\Delta_3 = \\frac{1}{2L_3}G_3(x)^2 = \\frac{1}{2(5)}(0.3)^2 = \\frac{0.09}{10} = 0.009 = \\frac{9}{1000}$。\n\n**4. 均匀采样分析**\n\n使用均匀采样概率 $p_i^{\\text{unif}} = 1/3$（对所有 i），我们计算：\n每次迭代的期望下降量：$\\mathbb{E}[\\Delta] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} \\Delta_{i} = \\frac{1}{3}(\\Delta_1 + \\Delta_2 + \\Delta_3)$。\n$\\mathbb{E}[\\Delta] = \\frac{1}{3}\\left(\\frac{1}{25} + \\frac{49}{40} + \\frac{9}{1000}\\right) = \\frac{1}{3}\\left(\\frac{40}{1000} + \\frac{1225}{1000} + \\frac{9}{1000}\\right) = \\frac{1}{3}\\frac{1274}{1000} = \\frac{637}{1500}$。\n\n每次迭代的期望成本：$\\mathbb{E}[w] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$。给定成本 $w_1=1, w_2=3, w_3=2$。\n$\\mathbb{E}[w] = \\frac{1}{3}(1+3+2) = \\frac{6}{3} = 2$。\n\n每单位成本的期望下降量：$\\frac{\\mathbb{E}[\\Delta]}{\\mathbb{E}[w]} = \\frac{637/1500}{2} = \\frac{637}{3000}$。\n\n**5. 最优采样分布**\n\n我们想要找到概率分布 $p=(p_1, p_2, p_3)$，它在预算和概率约束下最大化期望下降量 $\\sum p_i \\Delta_i$。这是一个线性规划（LP）问题：\n$$\\text{最大化} \\quad \\sum_{i=1}^3 p_i \\Delta_i$$\n$$\\text{约束条件} \\quad \\sum_{i=1}^3 p_i = 1, \\quad \\sum_{i=1}^3 p_i w_i \\le B, \\quad p_i \\ge 0 \\text{ for } i=1,2,3.$$\n这里，$B=2$，$w=(1,3,2)^T$ 且 $\\Delta = (1/25, 49/40, 9/1000)^T$。\n\n关于至多两个非零坐标的论证：概率的可行集是一个凸多面体。目标函数是线性的。线性规划的一个基本定理指出，线性规划的最优值总是在可行集的一个顶点（极点）处达到。在 $\\mathbb{R}^n$ 中由 $k$ 个线性等式约束定义的集合的极点必须至少有 $n-k$ 个零分量。这里，问题是在 $\\mathbb{R}^3$ 中。可行集由约束 $\\sum p_i = 1$ 和 $\\sum p_i w_i \\le B$ 定义。如果预算约束是活动的（即取等号），我们就有两个等式约束。在 $\\mathbb{R}^3$ 中具有两个等式约束的基本可行解将至多有两个非零分量。如果预算约束不活动，顶点就是概率单纯形的顶点，即 $(1,0,0), (0,1,0), (0,0,1)$，每个都只有一个非零分量。更正式地说，任何具有三个非零分量的可行点 $p$ 都可以写成另外两个不同的可行点 $p \\pm \\epsilon d$ 的凸组合，其中 $d$ 是定义活动约束的矩阵的零空间中的非零向量。因此，$p$ 不可能是极点。由于最优值必须在极点处达到，所以最优解 $p^*$ 将至多有两个非零坐标。\n\n为了求解该线性规划，我们找到可行域的顶点，并计算每个顶点处的目标函数值。约束条件是：\n$p_1+p_2+p_3 = 1$\n$p_1+3p_2+2p_3 \\le 2$\n$p_1, p_2, p_3 \\ge 0$\n我们将 $p_3 = 1 - p_1 - p_2$ 代入预算约束中：\n$p_1+3p_2+2(1-p_1-p_2) \\le 2 \\implies p_1+3p_2+2-2p_1-2p_2 \\le 2 \\implies -p_1+p_2 \\le 0 \\implies p_2 \\le p_1$。\n在 $(p_1, p_2)$ 平面上的可行域由 $p_1 \\ge 0$，$p_2 \\ge 0$，$p_1+p_2 \\le 1$ 和 $p_2 \\le p_1$ 定义。该区域的顶点是：\n- $(p_1,p_2) = (0,0) \\implies p=(0,0,1)$。成本：$2$。有效。\n- $(p_1,p_2) = (1,0) \\implies p=(1,0,0)$。成本：$1$。有效。\n- $(p_1,p_2) = (1/2, 1/2) \\implies p=(1/2,1/2,0)$。成本：$1/2(1) + 1/2(3) = 2$。有效。\n这三个点是可行集的顶点。让我们在这些顶点上评估目标函数 $J(p) = p_1 \\Delta_1 + p_2 \\Delta_2 + p_3 \\Delta_3$。\n- $p=(0,0,1): J = \\Delta_3 = \\frac{9}{1000} = 0.009$。\n- $p=(1,0,0): J = \\Delta_1 = \\frac{1}{25} = \\frac{40}{1000} = 0.04$。\n- $p=(1/2,1/2,0): J = \\frac{1}{2}\\Delta_1 + \\frac{1}{2}\\Delta_2 = \\frac{1}{2}\\left(\\frac{1}{25} + \\frac{49}{40}\\right) = \\frac{1}{2}\\left(\\frac{8}{200} + \\frac{245}{200}\\right) = \\frac{253}{400} = 0.6325$。\n\n比较这些值，最大期望下降量在 $p^{\\star} = (1/2, 1/2, 0)$ 处达到。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} & 0 \\end{pmatrix}}\n$$"
        }
    ]
}