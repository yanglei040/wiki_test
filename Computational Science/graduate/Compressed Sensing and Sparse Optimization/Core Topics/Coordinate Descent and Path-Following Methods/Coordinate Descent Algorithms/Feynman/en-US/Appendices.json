{
    "hands_on_practices": [
        {
            "introduction": "Mastering coordinate descent begins with a solid implementation. This first exercise focuses on the fundamental least-squares problem, tasking you with not just applying the algorithm, but doing so efficiently. By deriving the update rule from first principles and using a residual maintenance strategy, you will learn a critical technique for achieving fast per-coordinate updates, a cornerstone of high-performance optimization for large-scale data. ",
            "id": "3441217",
            "problem": "Consider the unconstrained quadratic least-squares objective $f(x)=\\tfrac{1}{2}\\lVert Ax - y\\rVert_2^2$ where $A \\in \\mathbb{R}^{n \\times p}$, $x \\in \\mathbb{R}^{p}$, and $y \\in \\mathbb{R}^{n}$. A cyclic coordinate descent method iteratively selects a coordinate $i \\in \\{1,\\dots,p\\}$ and performs an exact minimization along the one-dimensional subspace spanned by the $i$-th canonical vector. In the context of compressed sensing and sparse optimization, it is computationally critical to exploit sparsity in the columns of $A$ to achieve per-coordinate updates that scale in the number of nonzero entries of the selected column. Define the residual $r = y - Ax$. Your task is to derive, from first principles (calculus of one-dimensional quadratic minimization), the exact coordinate update rule that achieves an $O(\\lVert a_i\\rVert_0)$ per-coordinate time bound by maintaining and updating the residual $r$ without recomputing $Ax$ from scratch, where $a_i$ denotes the $i$-th column of $A$ and $\\lVert a_i\\rVert_0$ denotes its number of nonzero entries. Then, implement a cyclic coordinate descent algorithm that uses this residual maintenance strategy and runs for a specified number of full passes through the coordinates.\n\nYou must implement the algorithm using Compressed Sparse Column (CSC) storage for $A$ so that accessing a column $a_i$ and its nonzero pattern is efficient, and so that updating only the residual entries corresponding to nonzero positions of $a_i$ yields $O(\\lVert a_i\\rVert_0)$ computational time per coordinate. The program should also compute, for a specified coordinate $i$, how many entries of the residual $r$ change when performing a single coordinate update starting from $x=0$ and $r=y$, which will reflect the effective sparsity-aware update count (this integer will be less than or equal to $\\lVert a_i\\rVert_0$, and equals $\\lVert a_i\\rVert_0$ when the update step is nonzero across all nonzero positions).\n\nStart your derivation from the fundamental definitions of $f(x)$ and residual $r$, and from basic rules of differential calculus for minimizing a univariate quadratic function. Do not use any shortcut formulas; derive the update by setting the derivative to zero along the selected coordinate direction and expressing the result in terms of $a_i$ and $r$.\n\nYour program must implement the cyclic coordinate descent with residual maintenance for the following test suite. For each test case, initialize $x=0$ and $r=y$, perform the specified number of passes through coordinates $i=1,\\dots,p$ (in increasing order), and return two quantities: the final objective value $f(x)$ as a floating-point number, and the integer count of changed residual entries when performing a single coordinate update on the specified coordinate starting from $x=0$ and $r=y$. Express the final objective values rounded to $6$ decimal places.\n\nTest suite:\n\n1. Dense case (small, well-conditioned):\n- Matrix $A \\in \\mathbb{R}^{5 \\times 3}$ given by\n$$\nA = \\begin{bmatrix}\n2  -1  0 \\\\\n0  1  3 \\\\\n1  0  -2 \\\\\n4  1  1 \\\\\n-1  2  0\n\\end{bmatrix}.\n$$\n- Vector $y \\in \\mathbb{R}^{5}$ given by $y = [1,\\,4,\\,-3,\\,2,\\,0]^\\top$.\n- Number of passes: $8$.\n- Coordinate for single-update change-count: $i=2$ (that is, the second column).\n\n2. Sparse case (typical compressed sensing structure):\n- Dimensions: $n=10$, $p=12$.\n- The columns of $A \\in \\mathbb{R}^{10 \\times 12}$ are specified by their nonzero rows and values in Compressed Sparse Column form:\n    - Column $1$: rows $[1,\\,6,\\,10]$, values $[3,\\,-1,\\,2]$.\n    - Column $2$: rows $[2,\\,3]$, values $[4,\\,-2]$.\n    - Column $3$: rows $[4,\\,5,\\,9]$, values $[1,\\,5,\\,-3]$.\n    - Column $4$: rows $[7]$, values $[7]$.\n    - Column $5$: rows $[1,\\,8]$, values $[-2,\\,4]$.\n    - Column $6$: rows $[3,\\,5,\\,10]$, values $[6,\\,-1,\\,1]$.\n    - Column $7$: rows $[2,\\,6,\\,7]$, values $[1,\\,-3,\\,2]$.\n    - Column $8$: rows $[4,\\,8,\\,9]$, values $[-4,\\,2,\\,2]$.\n    - Column $9$: rows $[1]$, values $[5]$.\n    - Column $10$: rows $[3,\\,10]$, values $[-1,\\,3]$.\n    - Column $11$: rows $[5,\\,6,\\,8]$, values $[2,\\,2,\\,-2]$.\n    - Column $12$: rows $[7,\\,9]$, values $[1,\\,-1]$.\n  Here, rows are indexed from $1$ to $n$ in the listing above; your implementation should convert these to $0$-based indices.\n- Vector $y \\in \\mathbb{R}^{10}$ given by $y = [2,\\,-1,\\,3,\\,0,\\,5,\\,-2,\\,1,\\,4,\\,-3,\\,6]^\\top$.\n- Number of passes: $50$.\n- Coordinate for single-update change-count: $i=6$ (that is, the sixth column).\n\n3. Edge case with a zero column:\n- Matrix $A \\in \\mathbb{R}^{6 \\times 4}$ given by columns\n    - Column $1$: rows $[1,\\,4]$, values $[1,\\,-2]$.\n    - Column $2$: rows $[]$ (zero column).\n    - Column $3$: rows $[2,\\,5,\\,6]$, values $[3,\\,-1,\\,2]$.\n    - Column $4$: rows $[1,\\,3,\\,4,\\,6]$, values $[2,\\,-2,\\,1,\\,1]$.\n  Again, rows are listed with $1$-based indices and must be converted to $0$-based indices in your implementation.\n- Vector $y \\in \\mathbb{R}^{6}$ given by $y = [1,\\,0,\\,-1,\\,2,\\,-2,\\,3]^\\top$.\n- Number of passes: $20$.\n- Coordinate for single-update change-count: $i=2$ (the zero column).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list of the form $[f,\\,c]$ where $f$ is the final objective value rounded to $6$ decimal places (a floating-point number) and $c$ is the integer count of changed residual entries for the specified coordinate’s single update starting from $x=0$ and $r=y$. Therefore, the final output must look like $[[f_1,c_1],[f_2,c_2],[f_3,c_3]]$ with no spaces.",
            "solution": "The problem requires the derivation and implementation of a cyclic coordinate descent algorithm for the unconstrained least-squares objective function $f(x) = \\frac{1}{2}\\lVert Ax - y \\rVert_2^2$. The derivation must start from first principles and result in an update rule that exploits column sparsity for an efficient implementation.\n\n### Step 1: Problem Validation\n\n**1. Extraction of Givens:**\n- **Objective Function:** $f(x)=\\tfrac{1}{2}\\lVert Ax - y\\rVert_2^2$.\n- **Variables and Dimensions:** $A \\in \\mathbb{R}^{n \\times p}$, $x \\in \\mathbb{R}^{p}$, $y \\in \\mathbb{R}^{n}$.\n- **Algorithm:** Cyclic coordinate descent with exact one-dimensional minimization.\n- **Residual:** $r = y - Ax$.\n- **Computational Goal:** Per-coordinate updates with time complexity $O(\\lVert a_i\\rVert_0)$, where $a_i$ is the $i$-th column of $A$ and $\\lVert a_i\\rVert_0$ is its number of non-zero entries. This is to be achieved by maintaining the residual $r$.\n- **Data Structure:** $A$ must be stored in Compressed Sparse Column (CSC) format.\n- **Initial Conditions:** Algorithm starts with $x=0$ and $r=y$.\n- **Tasks:**\n    1. Implement the algorithm for a specified number of passes.\n    2. For a given coordinate $i$, compute the number of entries of the residual $r$ that change during a single coordinate update starting from $x=0$ and $r=y$.\n- **Test Suite:** Three test cases are provided with specific matrices $A$, vectors $y$, number of passes, and a coordinate for the change-count calculation.\n\n**2. Validation using Extracted Givens:**\n- **Scientific Grounding:** The problem is firmly grounded in numerical linear algebra and optimization theory. Coordinate descent is a standard, well-analyzed algorithm for large-scale problems, particularly for convex objectives like least-squares. The use of residual maintenance and sparse data structures to achieve computational efficiency is a cornerstone technique in scientific computing and machine learning. The problem is scientifically and mathematically sound.\n- **Well-Posedness:** The objective function $f(x)$ is convex, guaranteeing that a minimum exists. The cyclic coordinate descent algorithm is known to converge to a minimum for this problem. The tasks are deterministic: run a specific algorithm for a fixed number of steps and compute a specific quantity. The problem is well-posed.\n- **Objectivity:** The problem is stated using precise, unambiguous mathematical language and definitions. It is free of any subjective or opinion-based statements.\n- **Completeness and Consistency:** All necessary data (matrices, vectors, dimensions, iteration counts) and conditions are explicitly provided for each test case. There are no contradictions.\n- **Realism and Feasibility:** The problem setup, including the use of sparse matrices and the efficiency goal, is highly realistic and relevant to applications in fields like compressed sensing and sparse optimization. The test cases include a standard dense case, a representative sparse case, and a critical edge case (a zero column), which makes for a robust validation of the algorithm.\n- **Other Criteria:** The problem is not trivial, metaphorical, or ill-structured. It requires a rigorous derivation from first principles and a careful implementation that respects computational complexity constraints.\n\n**3. Verdict and Action:**\nThe problem is valid. It is scientifically sound, well-posed, and complete. I will proceed to derive the solution and implement the algorithm.\n\n### Step 2: Derivation and Algorithmic Design\n\nThe core task is to find the update for a single coordinate $x_i$ that minimizes the objective function $f(x)$, assuming all other coordinates $x_j$ (for $j \\neq i$) are fixed.\n\n**Derivation of the Coordinate Update Rule:**\nWe express the objective function in terms of the single variable $x_i$. Let $a_j$ be the $j$-th column of $A$.\n$$f(x) = \\frac{1}{2}\\left\\lVert \\sum_{j=1}^p x_j a_j - y \\right\\rVert_2^2 = \\frac{1}{2}\\left\\lVert x_i a_i + \\sum_{j \\neq i} x_j a_j - y \\right\\rVert_2^2$$\nTo simplify, we define the partial residual $r_{\\text{partial}, i} = y - \\sum_{j \\neq i} x_j a_j$, which represents the residual computed without the contribution of the $i$-th coordinate. The objective, as a function of $x_i$, becomes a one-dimensional least-squares problem:\n$$g(x_i) = \\frac{1}{2}\\lVert x_i a_i - r_{\\text{partial}, i} \\rVert_2^2$$\nThis is a quadratic function of $x_i$. To find the minimum, we take the derivative with respect to $x_i$ and set it to zero.\n$$g(x_i) = \\frac{1}{2} \\left( (x_i a_i - r_{\\text{partial}, i})^\\top (x_i a_i - r_{\\text{partial}, i}) \\right) = \\frac{1}{2} \\left( x_i^2 (a_i^\\top a_i) - 2x_i (a_i^\\top r_{\\text{partial}, i}) + r_{\\text{partial}, i}^\\top r_{\\text{partial}, i} \\right)$$\nThe derivative is:\n$$\\frac{dg}{dx_i} = x_i (a_i^\\top a_i) - a_i^\\top r_{\\text{partial}, i}$$\nSetting the derivative to zero and solving for $x_i$ gives the optimal new value, $x_i^{\\text{new}}$:\n$$x_i^{\\text{new}} (a_i^\\top a_i) = a_i^\\top r_{\\text{partial}, i} \\implies x_i^{\\text{new}} = \\frac{a_i^\\top r_{\\text{partial}, i}}{a_i^\\top a_i} = \\frac{a_i^\\top r_{\\text{partial}, i}}{\\lVert a_i \\rVert_2^2}$$\nThis update is only defined if $a_i$ is not the zero vector, i.e., $\\lVert a_i \\rVert_2^2 \\neq 0$. If $a_i=0$, the objective is independent of $x_i$, so we can leave $x_i$ unchanged.\n\n**Efficient Update via Residual Maintenance:**\nCalculating $r_{\\text{partial}, i}$ directly for each coordinate is computationally expensive. We can make the update efficient by relating $r_{\\text{partial}, i}$ to the full residual $r = y - Ax$. Let $x_i^{\\text{old}}$ and $r^{\\text{old}}$ be the values before updating the $i$-th coordinate.\n$$r^{\\text{old}} = y - \\sum_{j=1}^p x_j^{\\text{old}} a_j = \\left( y - \\sum_{j \\neq i} x_j^{\\text{old}} a_j \\right) - x_i^{\\text{old}} a_i = r_{\\text{partial}, i} - x_i^{\\text{old}} a_i$$\nFrom this, we have $r_{\\text{partial}, i} = r^{\\text{old}} + x_i^{\\text{old}} a_i$. Substituting this into the expression for $x_i^{\\text{new}}$:\n$$x_i^{\\text{new}} = \\frac{a_i^\\top (r^{\\text{old}} + x_i^{\\text{old}} a_i)}{a_i^\\top a_i} = \\frac{a_i^\\top r^{\\text{old}} + x_i^{\\text{old}} (a_i^\\top a_i)}{a_i^\\top a_i}$$\nThis formula allows us to compute the optimal new coordinate value using only the current full residual $r^{\\text{old}}$, the current coordinate value $x_i^{\\text{old}}$, the column $a_i$, and its pre-computable squared norm $\\lVert a_i \\rVert_2^2$.\n\n**Sparsity-Aware Residual Update:**\nAfter updating $x_i$, we must update the residual for the next iteration. Let $\\Delta x_i = x_i^{\\text{new}} - x_i^{\\text{old}}$ be the change in the coordinate. The new state vector is $x^{\\text{new}} = x^{\\text{old}} + \\Delta x_i e_i$, where $e_i$ is the $i$-th standard basis vector. The new residual is:\n$$r^{\\text{new}} = y - A x^{\\text{new}} = y - A(x^{\\text{old}} + \\Delta x_i e_i) = (y - A x^{\\text{old}}) - \\Delta x_i (A e_i) = r^{\\text{old}} - \\Delta x_i a_i$$\n\n**Computational Complexity:**\nFor each coordinate $i$, the update consists of:\n1.  **Computing $x_i^{\\text{new}}$**: The dominant operation is the dot product $a_i^\\top r^{\\text{old}}$. Since $a_i$ is sparse with $\\lVert a_i \\rVert_0$ non-zeros, this takes $O(\\lVert a_i \\rVert_0)$ time. The squared norms $\\lVert a_i \\rVert_2^2$ can be pre-computed.\n2.  **Updating $x_i$**: This is an $O(1)$ operation.\n3.  **Updating $r$**: The update $r^{\\text{new}} = r^{\\text{old}} - \\Delta x_i a_i$ requires scaling the sparse vector $a_i$ and subtracting it from $r$. This modifies only $\\lVert a_i \\rVert_0$ entries of the residual and takes $O(\\lVert a_i \\rVert_0)$ time.\n\nThe total time for one coordinate update is therefore $O(\\lVert a_i \\rVert_0)$, as required. Using a CSC matrix format for $A$ allows for efficient access to the non-zero indices and values of column $a_i$.\n\n**Algorithm:**\n1.  **Initialize:** $x \\leftarrow 0 \\in \\mathbb{R}^p$, $r \\leftarrow y \\in \\mathbb{R}^n$.\n2.  **Pre-computation:** For $i=1, \\dots, p$, compute and store $c_i = \\lVert a_i \\rVert_2^2$.\n3.  **Iteration:** For a specified number of passes:\n    For $i = 1, \\dots, p$:\n    a. If $c_i = 0$, continue to the next coordinate.\n    b. Fetch the non-zero elements and their row indices for column $a_i$.\n    c. Compute $x_i^{\\text{new}} = (a_i^\\top r + x_i c_i) / c_i$.\n    d. Compute the change $\\Delta x_i = x_i^{\\text{new}} - x_i$.\n    e. Update the coordinate: $x_i \\leftarrow x_i^{\\text{new}}$.\n    f. Update the residual: $r \\leftarrow r - \\Delta x_i a_i$. This sparse update is done only on the non-zero indices of $a_i$.\n4.  **Termination:** After the final pass, the objective value is $f(x)=\\frac{1}{2}\\lVert r \\rVert_2^2$.\n\n**Single-Update Residual Change Count:**\nFor a specific coordinate $i$, starting from $x=0$ and $r=y$:\n- $x_i^{\\text{old}} = 0$.\n- $x_i^{\\text{new}} = \\frac{a_i^\\top y + 0 \\cdot (a_i^\\top a_i)}{a_i^\\top a_i} = \\frac{a_i^\\top y}{\\lVert a_i \\rVert_2^2}$.\n- The change is $\\Delta x_i = x_i^{\\text{new}}$.\n- The residual changes by $\\Delta r = -(\\Delta x_i) a_i$. The $k$-th entry changes if $(\\Delta r)_k = -(\\Delta x_i) (a_i)_k \\neq 0$. This occurs if and only if $\\Delta x_i \\neq 0$ and $(a_i)_k \\neq 0$.\n- The number of changed entries is thus $\\lVert a_i \\rVert_0$ if $\\Delta x_i \\neq 0$, and $0$ otherwise. $\\Delta x_i = 0$ if $a_i$ is a zero column or if $a_i$ is orthogonal to $y$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csc_matrix\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for coordinate descent and print results.\n    \"\"\"\n\n    def solve_case(A_data_spec, n, p, y_vec, passes, i_spec_1based):\n        \"\"\"\n        Solves one instance of the coordinate descent problem.\n\n        Args:\n            A_data_spec: A dictionary defining the sparse matrix A, or a NumPy array.\n                         If a dictionary, keys are 1-based column indices and\n                         values are tuples of (rows, values), with 1-based rows.\n            n: Number of rows in A.\n            p: Number of columns in A.\n            y_vec: The vector y.\n            passes: Number of full passes for coordinate descent.\n            i_spec_1based: The 1-based coordinate for the single-update change-count.\n        \"\"\"\n        # Convert i_spec to 0-based index\n        i_spec_0based = i_spec_1based - 1\n\n        # Construct the CSC matrix A\n        if isinstance(A_data_spec, np.ndarray):\n            A = csc_matrix(A_data_spec)\n        else:\n            data = []\n            indices = []\n            indptr = [0]\n            for j in range(1, p + 1):\n                if j in A_data_spec:\n                    rows, vals = A_data_spec[j]\n                    data.extend(vals)\n                    # Convert 1-based row indices to 0-based\n                    indices.extend([r - 1 for r in rows])\n                indptr.append(len(data))\n            A = csc_matrix((data, indices, indptr), shape=(n, p), dtype=float)\n        \n        y = np.array(y_vec, dtype=float)\n        \n        # --- Single-update residual change count calculation ---\n        # Starting from x=0, r=y\n        a_i_spec = A[:, i_spec_0based]\n        a_i_spec_norm_sq = a_i_spec.power(2).sum()\n        \n        change_count = 0\n        # Use a small tolerance for floating point comparisons\n        if a_i_spec_norm_sq  1e-12:\n            # For x=0, delta_x_i = (a_i.T @ y) / ||a_i||^2\n            delta_x_i = a_i_spec.T.dot(y) / a_i_spec_norm_sq\n            if abs(delta_x_i)  1e-12:\n                change_count = a_i_spec.getnnz()\n        # If column is zero or a_i.T @ y is zero, delta_x is effectively zero, so no change.\n\n        # --- Cyclic Coordinate Descent Algorithm ---\n        x = np.zeros(p)\n        r = y.copy()\n\n        # Pre-compute squared L2 norms of columns\n        col_sq_norms = np.array([A[:, j].power(2).sum() for j in range(p)])\n\n        for _ in range(passes):\n            for i in range(p):\n                if col_sq_norms[i]  1e-12: # Skip zero columns\n                    continue\n                \n                # Get sparse column data efficiently from CSC structure\n                start, end = A.indptr[i], A.indptr[i+1]\n                rows_i = A.indices[start:end]\n                vals_i = A.data[start:end]\n                \n                # Calculate optimal new value for x_i\n                # x_i_new = (a_i.T @ (r + x[i]*a_i)) / ||a_i||^2\n                # term (a_i.T @ r) is computed sparsely\n                a_i_T_r = np.dot(vals_i, r[rows_i])\n                \n                x_i_old = x[i]\n                # The term (a_i.T @ (x_i * a_i)) = x_i * ||a_i||^2\n                numerator = a_i_T_r + x_i_old * col_sq_norms[i]\n                x_i_new = numerator / col_sq_norms[i]\n                \n                delta_x_i = x_i_new - x_i_old\n                \n                if abs(delta_x_i)  1e-12:\n                    x[i] = x_i_new\n                    # Update residual efficiently: r_new = r_old - delta_x_i * a_i\n                    r[rows_i] -= delta_x_i * vals_i\n\n        # Final objective value: 0.5 * ||r||_2^2, where r is the final residual\n        final_obj = 0.5 * np.dot(r, r)\n        \n        return [final_obj, change_count]\n\n    # --- Test Cases ---\n\n    # Test Case 1: Dense matrix\n    test_case_1 = {\n        \"A_data_spec\": np.array([\n            [2., -1., 0.],\n            [0., 1., 3.],\n            [1., 0., -2.],\n            [4., 1., 1.],\n            [-1., 2., 0.]\n        ]),\n        \"n\": 5, \"p\": 3,\n        \"y_vec\": [1., 4., -3., 2., 0.],\n        \"passes\": 8,\n        \"i_spec_1based\": 2\n    }\n    res1 = solve_case(**test_case_1)\n\n    # Test Case 2: Sparse matrix\n    test_case_2 = {\n        \"A_data_spec\": {\n            1: ([1, 6, 10], [3, -1, 2]), 2: ([2, 3], [4, -2]),\n            3: ([4, 5, 9], [1, 5, -3]),  4: ([7], [7]),\n            5: ([1, 8], [-2, 4]),       6: ([3, 5, 10], [6, -1, 1]),\n            7: ([2, 6, 7], [1, -3, 2]), 8: ([4, 8, 9], [-4, 2, 2]),\n            9: ([1], [5]),              10: ([3, 10], [-1, 3]),\n            11: ([5, 6, 8], [2, 2, -2]),12: ([7, 9], [1, -1])\n        },\n        \"n\": 10, \"p\": 12,\n        \"y_vec\": [2., -1., 3., 0., 5., -2., 1., 4., -3., 6.],\n        \"passes\": 50,\n        \"i_spec_1based\": 6\n    }\n    res2 = solve_case(**test_case_2)\n\n    # Test Case 3: Edge case with a zero column\n    test_case_3 = {\n        \"A_data_spec\": {\n            1: ([1, 4], [1, -2]),\n            2: ([], []),\n            3: ([2, 5, 6], [3, -1, 2]),\n            4: ([1, 3, 4, 6], [2, -2, 1, 1])\n        },\n        \"n\": 6, \"p\": 4,\n        \"y_vec\": [1., 0., -1., 2., -2., 3.],\n        \"passes\": 20,\n        \"i_spec_1based\": 2\n    }\n    res3 = solve_case(**test_case_3)\n\n    results = [res1, res2, res3]\n    \n    # Format the final output string exactly as required (no spaces)\n    inner_parts = []\n    for f, c in results:\n        # Use string format to ensure 6 decimal places for the float\n        inner_parts.append(f\"[{f:.6f},{c}]\")\n    final_str = f\"[{','.join(inner_parts)}]\"\n    print(final_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the basics, we now turn to the celebrated Lasso objective, which introduces a non-smooth $\\ell_1$ regularizer. This requires moving from simple gradient steps to proximal updates. This practice highlights a subtle but powerful aspect of coordinate descent: the coordinate selection rule. By comparing a deterministic cyclic strategy against a randomized one on a carefully constructed problem with correlated features, you will gain practical insight into why and when randomization can significantly accelerate convergence. ",
            "id": "3441210",
            "problem": "Consider the least absolute shrinkage and selection operator (Lasso) objective in compressed sensing and sparse optimization, defined for a sensing matrix $A \\in \\mathbb{R}^{m \\times p}$ and a response vector $y \\in \\mathbb{R}^{m}$ as\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1,\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm. Coordinate descent algorithms optimize $f(x)$ by successively minimizing $f$ with respect to one coordinate of $x$ at a time while holding the others fixed. Two selection rules are considered:\n- Cyclic coordinate descent (CCD): repeatedly visit coordinates in the fixed order $1,2,\\dots,p$.\n- Randomized coordinate descent (RCD): at each update, select a coordinate uniformly at random with replacement.\n\nAn epoch is defined as exactly $p$ single-coordinate updates. Both methods start from $x^{(0)} = 0$ and use exact minimization along the chosen coordinate at each update.\n\nDesign a worst-case sensing matrix $A$ and response $y$ that induce zig-zagging for the cyclic rule by constructing highly correlated columns. Specifically, for given $m$, $p$, and correlation parameter $\\rho \\in [0,1)$, construct the first two columns of $A$ to have inner product approximately $\\rho$ and unit norm, and construct any remaining columns to be approximately uncorrelated with the first two and with each other. Set $y = A x^\\star$ for a sparse ground-truth vector $x^\\star$ with only the first two entries nonzero and equal in magnitude. The program must then compare the average objective decrease per epoch between CCD and RCD, where the average is taken over a fixed number of epochs $E$, for each test case. Randomized selection must use a fixed pseudorandom seed $2025$ so that results are reproducible.\n\nYour program must implement both selection rules using exact single-coordinate minimization at each update, based solely on the definition of $f(x)$ and the per-coordinate one-dimensional optimization subproblem. It must compute and report, for each test case, the ratio\n$$\nr \\;=\\; \\frac{\\text{average per-epoch objective decrease under RCD}}{\\text{average per-epoch objective decrease under CCD}},\n$$\nexpressed as a floating-point number.\n\nTest Suite. Use the following four test cases, all with $E = 50$ epochs and no observation noise:\n- Case $1$: $m = 200$, $p = 2$, $\\rho = 0.999$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $2$: $m = 200$, $p = 2$, $\\rho = 0.9$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $3$: $m = 200$, $p = 2$, $\\rho = 0.0$, $\\lambda = 0.05$, $x^\\star = [1,\\,1]^T$.\n- Case $4$: $m = 200$, $p = 10$, where columns $1$ and $2$ have correlation $\\rho = 0.999$ and unit norm, columns $3$ through $10$ are random with unit norm and approximately uncorrelated with the first two and with each other, $\\lambda = 0.05$, $x^\\star = [1,\\,1,\\,0,\\,\\dots,\\,0]^T$ (length $p$).\n\nConstruction details. For $p \\ge 2$, generate the first column $a_1$ by sampling i.i.d. standard normal entries and normalizing to unit Euclidean norm. Generate $a_2$ as\n$$\na_2 \\;=\\; \\rho\\,a_1 \\;+\\; \\sqrt{1-\\rho^2}\\,w,\n$$\nwhere $w$ has i.i.d. standard normal entries and $a_2$ is then normalized to unit norm. For $p  2$, generate columns $a_j$ for $j \\ge 3$ as independent standard normal vectors normalized to unit norm. Set $A = [a_1,\\dots,a_p]$ and $y = A x^\\star$. For both CCD and RCD, initialize $x^{(0)} = 0$, define an epoch as $p$ updates, and after each epoch record $f(x)$.\n\nAverage per-epoch objective decrease. For each method, compute the average per-epoch decrease as\n$$\n\\Delta_{\\mathrm{avg}} \\;=\\; \\frac{1}{E}\\,\\sum_{t=0}^{E-1} \\left(f\\left(x^{(t)}\\right) - f\\left(x^{(t+1)}\\right)\\right),\n$$\nwhere $x^{(t)}$ is the iterate after $t$ epochs.\n\nFinal Output Format. Your program should produce a single line of output containing a comma-separated list of four floating-point ratios $[r_1,r_2,r_3,r_4]$, in the order of the test cases above, enclosed in square brackets. Use the pseudorandom seed $2025$ for all random draws. No physical units, angle units, or percentages are involved; all outputs are dimensionless floating-point numbers.",
            "solution": "The problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of numerical optimization, specifically focusing on the performance characteristics of coordinate descent algorithms for the Lasso objective function. All parameters, procedures, and evaluation metrics are explicitly and formally defined.\n\nThe core of the problem is to implement and compare Cyclic Coordinate Descent (CCD) and Randomized Coordinate Descent (RCD) for the Lasso objective function, which is given by:\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - y\\|_2^2 \\;+\\; \\lambda\\,\\|x\\|_1\n$$\nHere, $x \\in \\mathbb{R}^p$ is the vector of parameters to be optimized, $A \\in \\mathbb{R}^{m \\times p}$ is the sensing matrix, $y \\in \\mathbb{R}^m$ is the response vector, and $\\lambda \\ge 0$ is the regularization parameter.\n\nCoordinate descent algorithms optimize this function by iteratively minimizing it with respect to a single coordinate $x_j$ while keeping all other coordinates $x_k$ (for $k \\neq j$) fixed. The one-dimensional subproblem for coordinate $x_j$ is to minimize:\n$$\ng(z) \\;=\\; f(x_1, \\dots, x_{j-1}, z, x_{j+1}, \\dots, x_p)\n$$\nExpanding the objective function, we separate terms that depend on $x_j$:\n$$\n\\begin{aligned}\nf(x) = \\tfrac{1}{2} \\left\\| \\sum_{k=1}^p a_k x_k - y \\right\\|_2^2 + \\lambda \\sum_{k=1}^p |x_k| \\\\\n= \\tfrac{1}{2} \\left\\| a_j x_j + \\sum_{k \\neq j} a_k x_k - y \\right\\|_2^2 + \\lambda |x_j| + \\lambda \\sum_{k \\neq j} |x_k|\n\\end{aligned}\n$$\nwhere $a_k$ is the $k$-th column of $A$. To minimize with respect to $x_j$, we can ignore terms that do not depend on it. The subproblem becomes minimizing:\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} \\|a_j x_j + \\sum_{k \\neq j} a_k x_k - y\\|_2^2 + \\lambda|x_j| \\right)\n$$\nExpanding the squared norm term:\n$$\n\\tfrac{1}{2} \\left( x_j^2 \\|a_j\\|_2^2 + 2x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) \\right) + \\lambda|x_j| + \\text{const}\n$$\nThe problem specifies that all columns $a_j$ are normalized to have unit Euclidean norm, i.e., $\\|a_j\\|_2^2 = a_j^T a_j = 1$. This simplifies the subproblem to:\n$$\n\\arg\\min_{x_j} \\left( \\tfrac{1}{2} x_j^2 + x_j a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) + \\lambda|x_j| \\right)\n$$\nThis is a quadratic function of $x_j$ plus an $\\ell_1$-norm penalty. The solution to $\\arg\\min_z (\\frac{1}{2}z^2 - c z + \\lambda|z|)$ is given by the soft-thresholding operator, $z^* = S_\\lambda(c)$. In our case, $c = -a_j^T\\left(\\sum_{k \\neq j} a_k x_k - y\\right) = a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k$.\nThus, the update rule for coordinate $x_j$ is:\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( a_j^T y - \\sum_{k \\neq j} (a_j^T a_k) x_k \\right)\n$$\nwhere $S_\\lambda(z) = \\text{sgn}(z) \\max(|z| - \\lambda, 0)$. For computational efficiency, we can pre-compute the Gram matrix $A^T A$ and the vector $A^T y$. Let $G = A^T A$ and $c_y = A^T y$. The update becomes:\n$$\nx_j^{\\text{new}} \\leftarrow S_\\lambda\\left( (c_y)_j - \\sum_{k \\neq j} G_{jk} x_k \\right)\n$$\nThe summation term can be written as $(G x)_j - G_{jj} x_j$. Since $G_{jj}=a_j^T a_j=1$, the argument for the soft-thresholding function is $(c_y)_j - ((G x)_j - x_j)$. The vector $x$ contains the most recently updated values of the coordinates.\n\nThe algorithm proceeds as follows:\n1.  **Initialization**: Set the pseudorandom seed to $2025$ for reproducibility. For each test case, construct the matrix $A$ and vector $y$ as specified. The columns $a_j$ are generated from i.i.d. standard normal distributions and normalized. The first two columns, $a_1$ and $a_2$, are constructed to have a specified correlation structure. The response is noiseless, $y=Ax^\\star$. Pre-compute $A^TA$ and $A^Ty$. Initialize the solution estimate $x^{(0)} = 0$.\n\n2.  **Cyclic Coordinate Descent (CCD)**: Iterate for $E$ epochs. In each epoch, update coordinates sequentially from $j=1, \\dots, p$.\n    $$\n    x_j \\leftarrow S_\\lambda\\left( (A^Ty)_j - \\left( \\sum_{k=1}^p (A^TA)_{jk} x_k - (A^TA)_{jj} x_j \\right) \\right)\n    $$\n    The values of $x_k$ used in the update for $x_j$ are the most current ones available.\n\n3.  **Randomized Coordinate Descent (RCD)**: Iterate for $E$ epochs. In each epoch, perform $p$ updates. For each update, select a coordinate $j \\in \\{1, \\dots, p\\}$ uniformly at random with replacement. Apply the same update rule as in CCD.\n\n4.  **Evaluation**: For both CCD and RCD, the vector of iterates after $t$ epochs is denoted $x^{(t)}$. The objective function values $f(x^{(t)})$ are recorded for $t=0, \\dots, E$. The average per-epoch objective decrease is calculated as:\n    $$\n    \\Delta_{\\mathrm{avg}} = \\frac{1}{E} \\sum_{t=0}^{E-1} \\left(f(x^{(t)}) - f(x^{(t+1)})\\right)\n    $$\n    The final reported value for each test case is the ratio $r = \\Delta_{\\mathrm{avg, RCD}} / \\Delta_{\\mathrm{avg, CCD}}$.\n\nThe code implements this logic, carefully following the construction details for $A$, the iterative update schemes for CCD and RCD, and the final calculation of the performance ratio.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coordinate descent comparison problem for Lasso.\n    It implements and compares Cyclic Coordinate Descent (CCD) and\n    Randomized Coordinate Descent (RCD) on constructed test cases,\n    reporting the ratio of their average per-epoch objective decrease.\n    \"\"\"\n    \n    # Per the problem statement, a single seed is used for all random draws.\n    np.random.seed(2025)\n\n    test_cases = [\n        # Case 1: High correlation\n        {'m': 200, 'p': 2, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 2: Moderate correlation\n        {'m': 200, 'p': 2, 'rho': 0.9, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 3: No correlation\n        {'m': 200, 'p': 2, 'rho': 0.0, 'lam': 0.05, 'x_star': [1.0, 1.0], 'E': 50},\n        # Case 4: High correlation in a larger-p setting\n        {'m': 200, 'p': 10, 'rho': 0.999, 'lam': 0.05, 'x_star': [1.0, 1.0] + [0.0] * 8, 'E': 50}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        m, p, rho, lam, x_star_list, E = case['m'], case['p'], case['rho'], case['lam'], case['x_star'], case['E']\n        x_star = np.array(x_star_list, dtype=float)\n\n        # 1. Construct sensing matrix A and response vector y\n        A = np.zeros((m, p))\n        \n        # First column a_1\n        a1 = np.random.randn(m)\n        a1 /= np.linalg.norm(a1)\n        A[:, 0] = a1\n\n        # Second column a_2, constructed to be correlated with a_1\n        if p = 2:\n            w = np.random.randn(m)\n            # The construction follows the problem statement verbatim.\n            # Adding max(0,...) ensures the argument to sqrt is non-negative.\n            a2_unnormalized = rho * a1 + np.sqrt(max(0, 1 - rho**2)) * w\n            A[:, 1] = a2_unnormalized / np.linalg.norm(a2_unnormalized)\n\n        # Remaining columns a_j for j = 3\n        for j in range(2, p):\n            aj = np.random.randn(m)\n            aj /= np.linalg.norm(aj)\n            A[:, j] = aj\n            \n        # Noiseless response vector\n        y = A @ x_star\n        \n        # Precompute matrices for efficiency\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        # Helper functions defined within the loop to capture A, y, lam etc.\n        def objective_function(x):\n            residual = A @ x - y\n            l2_term = 0.5 * np.sum(residual**2)\n            l1_term = lam * np.sum(np.abs(x))\n            return l2_term + l1_term\n\n        def soft_threshold(z, l):\n            return np.sign(z) * np.maximum(np.abs(z) - l, 0.0)\n\n        # 2. Cyclic Coordinate Descent (CCD)\n        x_ccd = np.zeros(p)\n        f_values_ccd = [objective_function(x_ccd)]\n        for _ in range(E):\n            for j in range(p):\n                # Update rule for coordinate j\n                val = Aty[j] - (np.dot(AtA[j, :], x_ccd) - AtA[j,j] * x_ccd[j])\n                x_ccd[j] = soft_threshold(val, lam)\n            f_values_ccd.append(objective_function(x_ccd))\n        \n        decreases_ccd = [f_values_ccd[t] - f_values_ccd[t+1] for t in range(E)]\n        delta_ccd = np.mean(decreases_ccd)\n\n        # 3. Randomized Coordinate Descent (RCD)\n        x_rcd = np.zeros(p)\n        f_values_rcd = [objective_function(x_rcd)]\n        for _ in range(E):\n            # An epoch consists of p updates at random coordinates\n            for _ in range(p):\n                j = np.random.randint(0, p)\n                val = Aty[j] - (np.dot(AtA[j, :], x_rcd) - AtA[j,j] * x_rcd[j])\n                x_rcd[j] = soft_threshold(val, lam)\n            f_values_rcd.append(objective_function(x_rcd))\n\n        decreases_rcd = [f_values_rcd[t] - f_values_rcd[t+1] for t in range(E)]\n        delta_rcd = np.mean(decreases_rcd)\n\n        # 4. Compute and store the ratio\n        if delta_ccd == 0.0:\n            ratio = np.inf if delta_rcd  0 else 1.0\n        else:\n            ratio = delta_rcd / delta_ccd\n            \n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen the power of randomized selection, a natural question arises: can we do better than uniform randomness? This advanced practice explores importance sampling, a powerful method for optimizing the selection strategy itself. You will derive the optimal sampling probabilities that maximize the expected progress per iteration, taking into account both the potential for objective decrease and the computational cost of each update, thereby connecting theoretical descent guarantees directly to practical, budget-aware algorithm design. ",
            "id": "3441225",
            "problem": "Consider a sparse recovery objective from compressed sensing, modeled as a composite convex problem of a smooth data-fit term and a sparsity-inducing regularizer. Let $A \\in \\mathbb{R}^{3 \\times 3}$, $b \\in \\mathbb{R}^{3}$, and define the objective $F(x) = f(x) + g(x)$ with $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ and $g(x) = \\lambda \\|x\\|_{1}$, where $\\lambda  0$. Coordinate descent updates are carried out using the proximal coordinate gradient step with the coordinate-wise Lipschitz constant $L_{i}$ defined by the smooth part $f$. Assume that the cost to update coordinate $i$ is a known positive weight $w_{i}$, and that there is a fixed computational budget per iteration $B  0$ that constrains the expected per-iteration cost.\n\nYou are given the following specific instance at the current iterate $x \\in \\mathbb{R}^{3}$:\n- $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$, $b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$, $\\lambda = 0.5$, and $x = \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix}$.\n- Coordinate update costs are $w_{1} = 1$, $w_{2} = 3$, $w_{3} = 2$.\n- The computational budget per iteration is $B = 2$.\n\nUsing only the fundamental definitions of coordinate-wise Lipschitz constants for $f$, the proximal operator for the $\\ell_{1}$ norm, and the coordinate-wise generalized gradient mapping induced by the proximal step, proceed as follows:\n\n1. Compute the coordinate-wise Lipschitz constants $L_{i}$ for $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$ from first principles.\n2. Using the definition of the gradient $\\nabla f(x)$ and the proximal map $\\operatorname{prox}_{\\alpha |\\cdot|}(\\cdot)$, derive the one-step proximal coordinate update for a single coordinate $i$ with stepsize $1/L_{i}$, and write the corresponding generalized gradient mapping $G_{i}(x)$ in terms of $x$, $\\nabla f(x)$, and $L_{i}$.\n3. Using the standard descent property of the proximal coordinate update derived from the smoothness of $f$ and separability of $g$, show that the per-coordinate decrease in the objective is lower bounded by a scalar quantity of the form $\\Delta_{i} = c_{i} G_{i}(x)^{2}$ with $c_{i}  0$ depending on $L_{i}$, and compute $\\Delta_{i}$ explicitly for $i \\in \\{1,2,3\\}$ at the given $x$.\n4. For uniform sampling across coordinates, that is, with probabilities $p_{i}^{\\text{unif}} = 1/3$ for all $i \\in \\{1,2,3\\}$, compute the expected per-iteration decrease $\\mathbb{E}[\\Delta] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} \\Delta_{i}$, the expected per-iteration cost $\\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$, and the expected decrease per unit cost, defined as the ratio $\\mathbb{E}[\\Delta] / \\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$. Provide exact values (no rounding).\n5. Now, under the fixed computational budget per iteration $B = 2$, derive the sampling distribution $p = (p_{1}, p_{2}, p_{3})$ that maximizes the expected per-iteration decrease $\\sum_{i=1}^{3} p_{i} \\Delta_{i}$ subject to the constraints $\\sum_{i=1}^{3} p_{i} = 1$, $\\sum_{i=1}^{3} p_{i} w_{i} \\le B$, and $p_{i} \\ge 0$ for all $i$. Argue from first principles why the optimal solution must concentrate on at most two coordinates, and solve explicitly for the optimal $p$ in this instance.\n\nGive your final answer as the optimal sampling distribution $p^{\\star}$ in row-matrix form. No rounding is required. Do not include any units in your final answer. If you compute any intermediate numerical quantities, keep them exact as rational numbers whenever possible.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Objective function: $F(x) = f(x) + g(x)$ for $x \\in \\mathbb{R}^{3}$.\n- Smooth term: $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$, with $A \\in \\mathbb{R}^{3 \\times 3}$ and $b \\in \\mathbb{R}^{3}$.\n- Non-smooth term: $g(x) = \\lambda \\|x\\|_{1}$, with $\\lambda  0$.\n- Problem instance:\n  - $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$\n  - $b = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}$\n  - $\\lambda = 0.5$\n  - Current iterate: $x = \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix}$\n- Coordinate update costs: $w_{1} = 1$, $w_{2} = 3$, $w_{3} = 2$.\n- Computational budget: $B = 2$.\n- Task is to perform a five-part analysis related to coordinate descent, involving Lipschitz constants, proximal updates, descent guarantees, uniform sampling, and optimal sampling under a budget.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard application of proximal coordinate descent to the LASSO problem, a cornerstone of sparse optimization and compressed sensing. All concepts used (Lipschitz constants, proximal operators, objective-based sampling) are fundamental and well-established in the field of convex optimization. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is clearly stated with all necessary data provided. The objective function is convex, ensuring that the optimization procedures are well-defined. The final part involves solving a linear program, which has a well-defined solution. The problem admits a unique and meaningful solution.\n- **Objective**: The problem is expressed using precise mathematical language and is devoid of any subjective or ambiguous terminology.\n- **Completeness and Consistency**: The dimensions of all matrices and vectors are consistent. The provided data is complete for the requested tasks. There are no internal contradictions.\n- **Realism and Feasibility**: The problem is a small-scale, but realistic, instance of a sparse recovery problem. All calculations are computationally feasible.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution is provided below.\n\n### Detailed Solution\n\nThe solution is presented in five parts, as requested by the problem statement.\n\n**1. Coordinate-wise Lipschitz Constants**\n\nThe smooth part of the objective is $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$. The gradient of $f$ is $\\nabla f(x) = A^T(Ax - b)$, and its Hessian is $\\nabla^2 f(x) = A^T A$. The coordinate-wise Lipschitz constant $L_i$ for $\\nabla_i f(x)$ is the Lipschitz constant of the map $t \\mapsto \\nabla_i f(x + t e_i)$, which is given by the $i$-th diagonal element of the Hessian: $L_i = (\\nabla^2 f(x))_{ii} = (A^T A)_{ii}$.\nLet $a_i$ be the $i$-th column of matrix $A$. Then $(A^T A)_{ii} = a_i^T a_i = \\|a_i\\|_2^2$.\nGiven $A = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix}$, the columns are $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$, $a_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\end{pmatrix}$, and $a_3 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nWe compute the squared norms:\n$L_1 = \\|a_1\\|_2^2 = 1^2 + 0^2 + 1^2 = 2$.\n$L_2 = \\|a_2\\|_2^2 = 0^2 + 2^2 + 1^2 = 5$.\n$L_3 = \\|a_3\\|_2^2 = 2^2 + 1^2 + 0^2 = 5$.\nThus, the coordinate-wise Lipschitz constants are $L_1 = 2$, $L_2 = 5$, and $L_3 = 5$.\n\n**2. Proximal Coordinate Update and Generalized Gradient Mapping**\n\nA proximal coordinate gradient update for coordinate $x_i$ is found by minimizing a quadratic approximation of $f$ plus the non-smooth term $g_i(x_i) = \\lambda |x_i|$. The update $x_i^+$ for $x_i$ is:\n$$x_i^+ = \\arg\\min_{y \\in \\mathbb{R}} \\left( f(x) + \\nabla_i f(x) (y - x_i) + \\frac{L_i}{2}(y - x_i)^2 + \\lambda|y| \\right)$$\nThis is equivalent to finding the minimizer of:\n$$y \\mapsto \\frac{L_i}{2} \\left( y - \\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right) \\right)^2 + \\lambda|y|$$\nThe solution is given by the proximal operator of $g_i$ with parameter $\\lambda/L_i$:\n$$x_i^+ = \\operatorname{prox}_{\\frac{\\lambda}{L_i}|\\cdot|}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right)$$\nFor the $\\ell_1$ norm, the proximal operator is the soft-thresholding function $S_\\alpha(z) = \\operatorname{sign}(z) \\max(|z|-\\alpha, 0)$. Thus, the update is:\n$$x_i^+ = S_{\\lambda/L_i}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right)$$\nThe generalized gradient mapping $G_i(x)$ for coordinate $i$ is defined as $G_i(x) = L_i(x_i - x_i^+)$. This can be written as:\n$$G_i(x) = L_i \\left( x_i - S_{\\lambda/L_i}\\left(x_i - \\frac{1}{L_i}\\nabla_i f(x)\\right) \\right)$$\nThis quantity measures the extent to which the optimality condition for coordinate $i$ is violated at the current point $x$.\n\n**3. Per-Coordinate Objective Decrease**\n\nBy the descent lemma for smooth functions (specifically, the quadratic upper bound provided by the Lipschitz constant $L_i$ for $\\nabla_i f$), the change in $F$ from updating coordinate $i$ to $x_i^+$ (letting $x^+$ be the new iterate) is bounded:\n$$F(x^+) \\le F(x) + \\nabla_i f(x)(x_i^+ - x_i) + \\frac{L_i}{2}(x_i^+ - x_i)^2 + g_i(x_i^+) - g_i(x_i)$$\nThe optimality condition for $x_i^+$ is that $0$ belongs to the subdifferential of the minimized function at $x_i^+$. This implies that there exists a subgradient $\\zeta_i \\in \\partial g_i(x_i^+)$ such that $\\nabla_i f(x) + L_i(x_i^+ - x_i) + \\zeta_i = 0$.\nSubstituting $\\nabla_i f(x) = -L_i(x_i^+ - x_i) - \\zeta_i$ into the descent inequality:\n$$F(x^+) - F(x) \\le (-L_i(x_i^+ - x_i) - \\zeta_i)(x_i^+ - x_i) + \\frac{L_i}{2}(x_i^+ - x_i)^2 + g_i(x_i^+) - g_i(x_i)$$\n$$F(x^+) - F(x) \\le -L_i(x_i^+ - x_i)^2 - \\left(\\zeta_i(x_i^+ - x_i) - (g_i(x_i^+) - g_i(x_i))\\right) + \\frac{L_i}{2}(x_i^+ - x_i)^2$$\nBy convexity of $g_i$, we have $g_i(x_i) \\ge g_i(x_i^+) + \\zeta_i(x_i - x_i^+)$, which implies that the term $-(\\dots)$ is non-positive. Thus:\n$$F(x^+) - F(x) \\le -L_i(x_i^+ - x_i)^2 + \\frac{L_i}{2}(x_i^+ - x_i)^2 = -\\frac{L_i}{2}(x_i^+ - x_i)^2$$\nThe decrease in the objective, $F(x) - F(x^+)$, is therefore lower-bounded:\n$$F(x) - F(x^+) \\ge \\frac{L_i}{2}(x_i^+ - x_i)^2$$\nUsing the definition $G_i(x) = L_i(x_i - x_i^+)$, we have $x_i^+ - x_i = -\\frac{1}{L_i}G_i(x)$. Substituting this into the bound:\n$$F(x) - F(x^+) \\ge \\frac{L_i}{2}\\left(-\\frac{1}{L_i}G_i(x)\\right)^2 = \\frac{1}{2L_i}G_i(x)^2$$\nThis gives the per-coordinate decrease lower bound $\\Delta_i = c_i G_i(x)^2$ with $c_i = \\frac{1}{2L_i}$.\n\nNow, we compute $\\Delta_i$ for $i \\in \\{1,2,3\\}$ at the given $x$.\nFirst, calculate the gradient $\\nabla f(x) = A^T(Ax-b)$:\n$Ax = \\begin{pmatrix} 1  0  2 \\\\ 0  2  1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ -0.1 \\\\ 0.0 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.4 \\end{pmatrix}$\n$Ax - b = \\begin{pmatrix} 0.5 \\\\ -0.2 \\\\ 0.4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ 1.8 \\\\ 0.4 \\end{pmatrix}$\n$\\nabla f(x) = \\begin{pmatrix} 1  0  1 \\\\ 0  2  1 \\\\ 2  1  0 \\end{pmatrix} \\begin{pmatrix} -0.5 \\\\ 1.8 \\\\ 0.4 \\end{pmatrix} = \\begin{pmatrix} -0.1 \\\\ 4.0 \\\\ 0.8 \\end{pmatrix}$\n\nFor $i=1$: $L_1=2, x_1=0.5, \\nabla_1 f(x)=-0.1, \\lambda=0.5$.\n$x_1^+ = S_{0.5/2}(0.5 - \\frac{1}{2}(-0.1)) = S_{0.25}(0.55) = 0.55 - 0.25 = 0.3$.\n$G_1(x) = L_1(x_1-x_1^+) = 2(0.5-0.3) = 0.4$.\n$\\Delta_1 = \\frac{1}{2L_1}G_1(x)^2 = \\frac{1}{2(2)}(0.4)^2 = \\frac{0.16}{4} = 0.04 = \\frac{1}{25}$.\n\nFor $i=2$: $L_2=5, x_2=-0.1, \\nabla_2 f(x)=4.0, \\lambda=0.5$.\n$x_2^+ = S_{0.5/5}(-0.1 - \\frac{1}{5}(4.0)) = S_{0.1}(-0.9) = -(|-0.9|-0.1) = -0.8$.\n$G_2(x) = L_2(x_2-x_2^+) = 5(-0.1 - (-0.8)) = 5(0.7) = 3.5$.\n$\\Delta_2 = \\frac{1}{2L_2}G_2(x)^2 = \\frac{1}{2(5)}(3.5)^2 = \\frac{12.25}{10} = 1.225 = \\frac{49}{40}$.\n\nFor $i=3$: $L_3=5, x_3=0.0, \\nabla_3 f(x)=0.8, \\lambda=0.5$.\n$x_3^+ = S_{0.5/5}(0.0 - \\frac{1}{5}(0.8)) = S_{0.1}(-0.16) = -(|-0.16|-0.1) = -0.06$.\n$G_3(x) = L_3(x_3-x_3^+) = 5(0.0 - (-0.06)) = 5(0.06) = 0.3$.\n$\\Delta_3 = \\frac{1}{2L_3}G_3(x)^2 = \\frac{1}{2(5)}(0.3)^2 = \\frac{0.09}{10} = 0.009 = \\frac{9}{1000}$.\n\n**4. Uniform Sampling Analysis**\n\nWith uniform sampling probabilities $p_i^{\\text{unif}} = 1/3$ for all $i$, we compute:\nExpected per-iteration decrease: $\\mathbb{E}[\\Delta] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} \\Delta_{i} = \\frac{1}{3}(\\Delta_1 + \\Delta_2 + \\Delta_3)$.\n$\\mathbb{E}[\\Delta] = \\frac{1}{3}\\left(\\frac{1}{25} + \\frac{49}{40} + \\frac{9}{1000}\\right) = \\frac{1}{3}\\left(\\frac{40}{1000} + \\frac{1225}{1000} + \\frac{9}{1000}\\right) = \\frac{1}{3}\\frac{1274}{1000} = \\frac{637}{1500}$.\n\nExpected per-iteration cost: $\\mathbb{E}[w] = \\sum_{i=1}^{3} p_{i}^{\\text{unif}} w_{i}$. Given costs $w_1=1, w_2=3, w_3=2$.\n$\\mathbb{E}[w] = \\frac{1}{3}(1+3+2) = \\frac{6}{3} = 2$.\n\nExpected decrease per unit cost: $\\frac{\\mathbb{E}[\\Delta]}{\\mathbb{E}[w]} = \\frac{637/1500}{2} = \\frac{637}{3000}$.\n\n**5. Optimal Sampling Distribution**\n\nWe want to find the probability distribution $p=(p_1, p_2, p_3)$ that maximizes the expected decrease $\\sum p_i \\Delta_i$ subject to budget and probability constraints. This is the linear program (LP):\n$$\\text{Maximize} \\quad \\sum_{i=1}^3 p_i \\Delta_i$$\n$$\\text{subject to} \\quad \\sum_{i=1}^3 p_i = 1, \\quad \\sum_{i=1}^3 p_i w_i \\le B, \\quad p_i \\ge 0 \\text{ for } i=1,2,3.$$\nHere, $B=2$, $w=(1,3,2)^T$, and $\\Delta = (1/25, 49/40, 9/1000)^T$.\n\nArgument for at most two non-zero coordinates: The feasible set of probabilities is a convex polyhedron. The objective function is linear. A fundamental theorem of linear programming states that the optimal value of an LP is always achieved at a vertex (an extreme point) of the feasible set. An extreme point of a set defined by $k$ linear equality constraints in $\\mathbb{R}^n$ must have at least $n-k$ zero components. Here, the problem is in $\\mathbb{R}^3$. The feasible set is defined by the constraints $\\sum p_i = 1$ and $\\sum p_i w_i \\le B$. If the budget constraint is active, we have two equality constraints. A basic feasible solution in $\\mathbb{R}^3$ with two equality constraints will have at most two non-zero components. If the budget constraint is inactive, the vertices are those of the probability simplex, which are $(1,0,0), (0,1,0), (0,0,1)$, each having only one non-zero component. More formally, any feasible point $p$ with three non-zero components can be written as a convex combination of two other distinct feasible points $p \\pm \\epsilon d$, where $d$ is a non-zero vector in the null space of the matrix defining the active constraints. Thus, $p$ cannot be an extreme point. Since the optimum must be at an extreme point, the optimal solution $p^*$ will have at most two non-zero coordinates.\n\nTo solve the LP, we find the vertices of the feasible region and evaluate the objective at each. The constraints are:\n$p_1+p_2+p_3 = 1$\n$p_1+3p_2+2p_3 \\le 2$\n$p_1, p_2, p_3 \\ge 0$\nWe substitute $p_3 = 1 - p_1 - p_2$ into the budget constraint:\n$p_1+3p_2+2(1-p_1-p_2) \\le 2 \\implies p_1+3p_2+2-2p_1-2p_2 \\le 2 \\implies -p_1+p_2 \\le 0 \\implies p_2 \\le p_1$.\nThe feasible region in the $(p_1, p_2)$ plane is defined by $p_1 \\ge 0$, $p_2 \\ge 0$, $p_1+p_2 \\le 1$, and $p_2 \\le p_1$. The vertices of this region are:\n- $(p_1,p_2) = (0,0) \\implies p=(0,0,1)$. Cost: $2$. Valid.\n- $(p_1,p_2) = (1,0) \\implies p=(1,0,0)$. Cost: $1$. Valid.\n- $(p_1,p_2) = (1/2, 1/2) \\implies p=(1/2,1/2,0)$. Cost: $1/2(1) + 1/2(3) = 2$. Valid.\nThese three points are the vertices of the feasible set. Let's evaluate the objective $J(p) = p_1 \\Delta_1 + p_2 \\Delta_2 + p_3 \\Delta_3$ at these vertices.\n- $p=(0,0,1): J = \\Delta_3 = \\frac{9}{1000} = 0.009$.\n- $p=(1,0,0): J = \\Delta_1 = \\frac{1}{25} = \\frac{40}{1000} = 0.04$.\n- $p=(1/2,1/2,0): J = \\frac{1}{2}\\Delta_1 + \\frac{1}{2}\\Delta_2 = \\frac{1}{2}\\left(\\frac{1}{25} + \\frac{49}{40}\\right) = \\frac{1}{2}\\left(\\frac{8}{200} + \\frac{245}{200}\\right) = \\frac{253}{400} = 0.6325$.\n\nComparing the values, the maximum expected decrease is achieved at $p^{\\star} = (1/2, 1/2, 0)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0 \\end{pmatrix}}\n$$"
        }
    ]
}