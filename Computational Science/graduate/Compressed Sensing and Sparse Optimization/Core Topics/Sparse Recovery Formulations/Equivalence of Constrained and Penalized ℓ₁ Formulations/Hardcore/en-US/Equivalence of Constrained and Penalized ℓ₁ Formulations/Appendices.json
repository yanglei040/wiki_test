{
    "hands_on_practices": [
        {
            "introduction": "The core of sparse optimization lies in the remarkable equivalence between constrained and penalized formulations. This first exercise provides a direct, hands-on proof of this equivalence for a concrete, low-dimensional example . By working through the Karush-Kuhn-Tucker (KKT) conditions, you will see precisely how the Lagrange multiplier from the constrained problem, which represents the sensitivity of the objective to the constraint, transforms into the regularization parameter in the penalized problem.",
            "id": "3446594",
            "problem": "Consider the least-squares data-fit with an $\\ell_1$ regularization in a low-dimensional setting. Let the sensing matrix be $A \\in \\mathbb{R}^{2 \\times 3}$ and the data vector be $y \\in \\mathbb{R}^{2}$ given by\n$$\nA = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nLet the $\\ell_1$-ball radius be $\\tau = 1$. Define the constrained $\\ell_1$-regularized least-squares problem\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|A x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\le \\tau,\n$$\nand the penalized (Lagrangian) formulation\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwith $\\lambda > 0$. Starting from the core definitions of convex optimality and the subdifferential of the $\\ell_1$ norm, establish that the constrained optimum sits at the corner $x^{\\star} = (\\tau, 0, 0)$ of the $\\ell_1$ ball (that is, $x^{\\star} = (1, 0, 0)$), and compute the exact value of $\\lambda$ for which the penalized problem attains the same optimizer $x^{\\star}$. No rounding is needed in your final result. Your final answer must be a single real number.",
            "solution": "The problem requires us to validate a proposed solution for a constrained $\\ell_1$-regularized least-squares problem and then to find the regularization parameter $\\lambda$ for which the corresponding penalized (Lagrangian) formulation shares the same solution. We will address this using the first-order optimality conditions for convex optimization.\n\nLet the objective function be $f(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2}$ and the regularization term be $g(x) = \\|x\\|_{1}$. The given data are:\n$$\nA = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\tau = 1.\n$$\nThe proposed optimal solution is $x^{\\star} = (1, 0, 0)^{T}$.\n\nThe function $f(x)$ is differentiable and convex. Its gradient is given by $\\nabla f(x) = A^{T}(Ax - y)$. The function $g(x)$ is convex but not differentiable everywhere. Its subdifferential, $\\partial g(x)$, must be used.\n\nFirst, let's compute the gradient of $f(x)$ at the point $x^{\\star}$:\n$$\nAx^{\\star} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nThe residual vector is:\n$$\nAx^{\\star} - y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.\n$$\nThe transpose of $A$ is:\n$$\nA^{T} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix}.\n$$\nThe gradient at $x^{\\star}$ is therefore:\n$$\n\\nabla f(x^{\\star}) = A^{T}(Ax^{\\star} - y) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(-2) + (0)(-1) \\\\ (0)(-2) + (1)(-1) \\\\ (1)(-2) + (-1)(-1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ -1 \\\\ -1 \\end{pmatrix}.\n$$\n\nSecond, let's characterize the subdifferential of the $\\ell_1$-norm, $g(x) = \\|x\\|_{1} = \\sum_{i=1}^{3} |x_{i}|$, at $x^{\\star} = (1, 0, 0)^{T}$. The subdifferential $\\partial g(x)$ is the Cartesian product of the subdifferentials of its components $|x_{i}|$. The subdifferential of the absolute value function at a point $z$ is $\\operatorname{sign}(z)$ if $z \\neq 0$ and the interval $[-1, 1]$ if $z=0$.\nFor $x^{\\star} = (1, 0, 0)^{T}$:\n\\begin{itemize}\n    \\item $x_{1}^{\\star} = 1 \\ne 0 \\implies$ the first component of the subgradient is $\\operatorname{sign}(1) = 1$.\n    \\item $x_{2}^{\\star} = 0 \\implies$ the second component of the subgradient lies in $[-1, 1]$.\n    \\item $x_{3}^{\\star} = 0 \\implies$ the third component of the subgradient lies in $[-1, 1]$.\n\\end{itemize}\nThus, any subgradient vector $s \\in \\partial g(x^{\\star})$ must be of the form $s = (1, s_{2}, s_{3})^{T}$ where $s_{2} \\in [-1, 1]$ and $s_{3} \\in [-1, 1]$.\n\nNow, we can proceed with the two tasks.\n\n**Part 1: Validation of the solution for the constrained problem**\nThe constrained problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ f(x) \\quad \\text{subject to} \\quad g(x) \\le \\tau.\n$$\nSince this is a convex optimization problem, the Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality. For a point $x^{\\star}$ to be a solution, there must exist a Lagrange multiplier $\\mu \\ge 0$ such that:\n1.  **Primal Feasibility:** $\\|x^{\\star}\\|_{1} \\le \\tau$\n2.  **Dual Feasibility:** $\\mu \\ge 0$\n3.  **Complementary Slackness:** $\\mu (\\|x^{\\star}\\|_{1} - \\tau) = 0$\n4.  **Stationarity:** $0 \\in \\nabla f(x^{\\star}) + \\mu \\, \\partial g(x^{\\star})$\n\nLet's check these conditions for $x^{\\star} = (1, 0, 0)^{T}$ with $\\tau=1$:\n\n1.  **Primal Feasibility:** $\\|x^{\\star}\\|_{1} = |1| + |0| + |0| = 1$. Since $1 \\le 1$ is true, $x^{\\star}$ is feasible. The constraint is active.\n2.  **Complementary Slackness:** Since $\\|x^{\\star}\\|_{1} - \\tau = 1 - 1 = 0$, the condition $\\mu (0) = 0$ is satisfied for any $\\mu$.\n3.  **Stationarity and Dual Feasibility:** The stationarity condition can be written as $-\\nabla f(x^{\\star}) \\in \\mu \\, \\partial g(x^{\\star})$, which means $-\\nabla f(x^{\\star}) = \\mu s$ for some $s \\in \\partial g(x^{\\star})$.\nWe have $-\\nabla f(x^{\\star}) = -(-2, -1, -1)^{T} = (2, 1, 1)^{T}$. The equation becomes:\n$$\n\\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\mu \\begin{pmatrix} 1 \\\\ s_{2} \\\\ s_{3} \\end{pmatrix} = \\begin{pmatrix} \\mu \\\\ \\mu s_{2} \\\\ \\mu s_{3} \\end{pmatrix}\n$$\nwith $s_{2}, s_{3} \\in [-1, 1]$.\nFrom the first component, we find $\\mu = 2$. This satisfies the dual feasibility condition $\\mu \\ge 0$.\nSubstituting $\\mu = 2$ into the other two component equations:\n\\begin{itemize}\n    \\item $1 = 2 s_{2} \\implies s_{2} = \\frac{1}{2}$\n    \\item $1 = 2 s_{3} \\implies s_{3} = \\frac{1}{2}$\n\\end{itemize}\nWe must verify that this subgradient is valid. The vector $s = (1, 1/2, 1/2)^{T}$ is a valid subgradient because $s_{2} = 1/2 \\in [-1, 1]$ and $s_{3} = 1/2 \\in [-1, 1]$.\nSince all KKT conditions are satisfied with $\\mu=2$, the point $x^{\\star}=(1, 0, 0)^{T}$ is indeed the optimal solution to the constrained problem.\n\n**Part 2: Computation of $\\lambda$ for the penalized problem**\nThe penalized (Lagrangian) problem is:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\equiv \\min_{x \\in \\mathbb{R}^{3}} f(x) + \\lambda g(x).\n$$\nThis is an unconstrained convex optimization problem. A point $x^{\\star}$ is a global minimizer if and only if the zero vector is contained in the subdifferential of the objective function at $x^{\\star}$. Using the sum rule for subdifferentials (as $f(x)$ is differentiable):\n$$\n0 \\in \\partial (f(x^{\\star}) + \\lambda g(x^{\\star})) = \\nabla f(x^{\\star}) + \\lambda \\, \\partial g(x^{\\star}).\n$$\nThis is equivalent to the condition $-\\nabla f(x^{\\star}) \\in \\lambda \\, \\partial g(x^{\\star})$. This requires that there exists a subgradient $s \\in \\partial g(x^{\\star})$ such that:\n$$\n-\\nabla f(x^{\\star}) = \\lambda s.\n$$\nThis is precisely the same algebraic form as the stationarity condition in the KKT analysis, with $\\lambda$ taking the place of the Lagrange multiplier $\\mu$.\nUsing our previous results for $x^{\\star}=(1, 0, 0)^{T}$:\n$$\n-\\nabla f(x^{\\star}) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad \\text{and} \\quad s = \\begin{pmatrix} 1 \\\\ s_{2} \\\\ s_{3} \\end{pmatrix} \\text{ with } s_2, s_3 \\in [-1, 1].\n$$\nThe condition becomes:\n$$\n\\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\lambda \\begin{pmatrix} 1 \\\\ s_{2} \\\\ s_{3} \\end{pmatrix} = \\begin{pmatrix} \\lambda \\\\ \\lambda s_{2} \\\\ \\lambda s_{3} \\end{pmatrix}.\n$$\nFrom the first component, we immediately obtain $\\lambda = 2$.\nThe problem requires $\\lambda > 0$, which is satisfied. We must confirm this choice of $\\lambda$ is consistent with the remaining components.\n$$\n1 = 2s_{2} \\implies s_{2} = \\frac{1}{2}.\n$$\n$$\n1 = 2s_{3} \\implies s_{3} = \\frac{1}{2}.\n$$\nThe required subgradient vector is $s = (1, 1/2, 1/2)^{T}$. This is a valid element of $\\partial g(x^{\\star})$ since $s_{2}=1/2 \\in [-1, 1]$ and $s_{3}=1/2 \\in [-1, 1]$.\nTherefore, for $\\lambda = 2$, the optimality condition for the penalized problem is satisfied at $x^{\\star}=(1, 0, 0)^{T}$.\n\nThe value of $\\lambda$ for which the penalized problem shares the same optimizer $x^{\\star}$ as the constrained problem is exactly the Lagrange multiplier $\\mu$ from the KKT conditions of the constrained problem, which we found to be $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "Moving beyond a single point of equivalence, this practice explores the continuous trade-off between data fidelity and sparsity. We introduce the value function $\\varphi(\\tau)$, which traces the Pareto optimal frontier by mapping the sparsity budget $\\tau$ to the minimum possible data-fit error . You will use principles of convex analysis to derive this function and establish the powerful relationship $\\lambda = -\\frac{d\\varphi}{d\\tau}$, providing a deep geometric interpretation of the regularization parameter $\\lambda$ as the marginal \"cost\" of relaxing the sparsity constraint.",
            "id": "3446602",
            "problem": "Consider the convex optimization formulations that arise in compressed sensing and sparse optimization. The constrained form minimizes a quadratic data fidelity subject to an $\\ell_1$ budget, and the penalized form trades off quadratic data fidelity with an $\\ell_1$ penalty. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the identity matrix $A = I$, and let $y \\in \\mathbb{R}^{2}$ be $y = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}$. Define the value function $\\varphi(\\tau)$ of the constrained formulation\n$$\n\\varphi(\\tau) \\triangleq \\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\|x - y\\|_{2}^{2} \\; \\text{subject to} \\; \\|x\\|_{1} \\leq \\tau \\right\\}.\n$$\nThe penalized formulation is\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\nStarting only from fundamental definitions and principles of convex analysis, including Lagrangian duality and the Karush-Kuhn-Tucker (KKT) conditions, and using the envelope theorem to relate value functions and Lagrange multipliers, derive the structure of the optimizer of the constrained problem, express the value function $\\varphi(\\tau)$, and establish the relationship between the Lagrange multiplier $\\lambda$ of the $\\ell_1$-constraint and the derivative of $\\varphi(\\tau)$ wherever the latter is differentiable.\n\nThen, for the target residual energy $\\varepsilon = \\frac{81}{100}$, construct $\\tau$ such that $\\varphi(\\tau) = \\varepsilon$ and determine the corresponding $\\lambda$ via the derivative relationship. Express the final answer as a single real number. No rounding is required.",
            "solution": "This problem requires us to analyze the relationship between a constrained optimization problem (projection onto an $\\ell_1$-ball) and its penalized (LASSO) counterpart. We will derive the value function $\\varphi(\\tau)$, confirm the derivative relationship $\\lambda = -\\varphi'(\\tau)$, and then use this framework to find the specific $\\lambda$ corresponding to a given objective value.\n\n**1. Deriving the Optimal Solution $x^{\\star}(\\tau)$**\n\nThe constrained problem is $\\min_{x} \\frac{1}{2} \\|x - y\\|_{2}^{2}$ subject to $\\|x\\|_{1} \\le \\tau$. This is a projection of the point $y$ onto the $\\ell_1$-ball of radius $\\tau$.\nThe Lagrangian is $L(x, \\lambda) = \\frac{1}{2}\\|x-y\\|_2^2 + \\lambda(\\|x\\|_1 - \\tau)$ for a Lagrange multiplier $\\lambda \\ge 0$.\nThe KKT stationarity condition is $0 \\in \\partial_x L(x^{\\star}, \\lambda) = x^{\\star} - y + \\lambda \\partial\\|x^{\\star}\\|_1$.\nThis can be rewritten as $y - x^{\\star} \\in \\lambda \\partial\\|x^{\\star}\\|_1$. Component-wise, this means $y_i - x^{\\star}_i = \\lambda s_i$ for some subgradient $s \\in \\partial\\|x^{\\star}\\|_1$. This implies that the solution is the soft-thresholding operator applied to $y$: $x^{\\star}_i = \\operatorname{sgn}(y_i) \\max(|y_i|-\\lambda, 0)$.\n\nThe value of $\\lambda$ depends on $\\tau$ via the KKT conditions. If $\\|y\\|_1 \\le \\tau$, the unconstrained minimizer $x=y$ is feasible, so $x^{\\star}=y$ and the constraint is inactive, implying $\\lambda=0$. If $\\|y\\|_1 > \\tau$, the constraint is active, $\\|x^{\\star}\\|_1 = \\tau$, which requires $\\lambda > 0$. The specific $\\lambda$ is found by solving $\\| \\operatorname{sgn}(y) \\max(|y|-\\lambda, 0) \\|_1 = \\tau$.\n\nFor our problem, $y = (4, 1)^T$, so $\\|y\\|_1 = 5$.\n*   If $\\tau \\ge 5$, $x^{\\star}(\\tau)=(4, 1)^T$ and $\\lambda=0$.\n*   If $\\tau < 5$, we solve $\\max(4-\\lambda, 0) + \\max(1-\\lambda, 0) = \\tau$.\n    *   For $0 < \\lambda \\le 1$: $(4-\\lambda) + (1-\\lambda) = \\tau \\implies 5 - 2\\lambda = \\tau \\implies \\lambda = \\frac{5-\\tau}{2}$. This holds for $3 \\le \\tau < 5$. The solution is $x^{\\star}(\\tau) = (4-\\lambda, 1-\\lambda)^T = (\\frac{3+\\tau}{2}, \\frac{\\tau-3}{2})^T$.\n    *   For $1 < \\lambda \\le 4$: $(4-\\lambda) + 0 = \\tau \\implies \\lambda = 4-\\tau$. This holds for $0 \\le \\tau < 3$. The solution is $x^{\\star}(\\tau) = (\\tau, 0)^T$.\n\n**2. Expressing the Value Function $\\varphi(\\tau)$**\n\nThe value function is $\\varphi(\\tau) = \\frac{1}{2}\\|x^{\\star}(\\tau)-y\\|_2^2$.\n*   For $\\tau \\ge 5$: $\\varphi(\\tau) = 0$.\n*   For $3 \\le \\tau < 5$: $x^{\\star}(\\tau) - y = (\\frac{\\tau-5}{2}, \\frac{\\tau-5}{2})^T$.\n    $\\varphi(\\tau) = \\frac{1}{2} \\left[ \\left(\\frac{\\tau-5}{2}\\right)^2 + \\left(\\frac{\\tau-5}{2}\\right)^2 \\right] = \\frac{(\\tau-5)^2}{4}$.\n*   For $0 \\le \\tau < 3$: $x^{\\star}(\\tau) - y = (\\tau-4, -1)^T$.\n    $\\varphi(\\tau) = \\frac{1}{2} ((\\tau-4)^2 + (-1)^2) = \\frac{1}{2}(\\tau^2 - 8\\tau + 17)$.\n\n**3. The Derivative Relationship**\n\nAccording to the envelope theorem for constrained optimization problems, the derivative of the value function with respect to a change in the constraint is the negative of the optimal Lagrange multiplier. Thus, $\\frac{d\\varphi}{d\\tau} = -\\lambda$.\nLet's verify this by differentiating our expression for $\\varphi(\\tau)$:\n*   For $3 < \\tau < 5$: $\\varphi'(\\tau) = \\frac{2(\\tau-5)}{4} = \\frac{\\tau-5}{2}$. So $-\\varphi'(\\tau) = \\frac{5-\\tau}{2}$, which matches the $\\lambda$ we found for this range.\n*   For $0 < \\tau < 3$: $\\varphi'(\\tau) = \\frac{1}{2}(2\\tau - 8) = \\tau-4$. So $-\\varphi'(\\tau) = 4-\\tau$, which also matches the $\\lambda$ we found.\nThe relationship $\\lambda = -d\\varphi/d\\tau$ is established.\n\n**4. Computing $\\lambda$ for the Target Energy**\n\nWe are given the target residual energy $\\varepsilon = \\varphi(\\tau) = \\frac{81}{100}$. We must find the corresponding $\\tau$ and then $\\lambda$.\nWe test the piecewise definition of $\\varphi(\\tau)$:\n*   Case $3 \\le \\tau < 5$:\n    $\\frac{(\\tau-5)^2}{4} = \\frac{81}{100} \\implies (\\tau-5)^2 = \\frac{324}{100} \\implies |\\tau-5| = \\frac{18}{10} = 1.8$.\n    Since $\\tau < 5$, we must have $5-\\tau=1.8 \\implies \\tau = 5 - 1.8 = 3.2$.\n    This value $\\tau=3.2$ is in the range $[3, 5)$, so it is a valid solution.\n\n*   Case $0 \\le \\tau < 3$:\n    $\\frac{1}{2}(\\tau^2 - 8\\tau + 17) = \\frac{81}{100} \\implies \\tau^2 - 8\\tau + 17 = \\frac{81}{50} = 1.62$.\n    $\\tau^2 - 8\\tau + 15.38 = 0$.\n    The roots are $\\tau = \\frac{8 \\pm \\sqrt{64 - 4(15.38)}}{2} = \\frac{8 \\pm \\sqrt{64 - 61.52}}{2} = \\frac{8 \\pm \\sqrt{2.48}}{2}$.\n    $\\sqrt{2.48} \\approx 1.57$.\n    $\\tau_1 \\approx \\frac{8+1.57}{2} = 4.785$, which is not in the range $[0, 3)$.\n    $\\tau_2 \\approx \\frac{8-1.57}{2} = 3.215$, which is also not in the range $[0, 3)$.\n    Thus, there is no solution for $\\tau$ in this range.\n\nThe only valid solution is $\\tau = 3.2$. Now we find the corresponding $\\lambda$ using the relationship for this range:\n$\\lambda = \\frac{5-\\tau}{2} = \\frac{5 - 3.2}{2} = \\frac{1.8}{2} = 0.9$.\nAs a decimal fraction, this is $\\frac{9}{10}$.",
            "answer": "$$ \\boxed{\\frac{9}{10}} $$"
        },
        {
            "introduction": "The final step is to bridge the gap between theory and computational practice. This exercise guides you through implementing an algorithm to solve the constrained optimization problem and numerically verifying the equivalence with its penalized counterpart . You will use a projected gradient method to find the optimal solution $x^{\\star}$, extract the corresponding regularization parameter $\\lambda$ directly from the KKT stationarity conditions, and write code to confirm that this $(x^{\\star}, \\lambda)$ pair is indeed a valid solution to the penalized problem, solidifying the concepts through a practical coding challenge.",
            "id": "3446598",
            "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a given matrix and $b \\in \\mathbb{R}^{m}$ a given vector. Consider the constrained quadratic optimization problem with an $\\ell_1$-norm budget:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\le \\epsilon,\n$$\nwhere $\\epsilon \\ge 0$ is a specified parameter. The goal is to design and implement an algorithmic procedure that, for each test case below, performs the following steps and outputs a verifiable result:\n\n1. Solve the constrained problem to obtain a numerical optimizer $x^{\\star}$.\n\n2. Compute the residual $r^{\\star} = A^{\\top}(A x^{\\star} - b)$ and extract a scalar $\\lambda \\ge 0$ from the Karush–Kuhn–Tucker (KKT) stationarity condition\n$$\n-A^{\\top}(A x^{\\star} - b) \\in \\lambda \\, \\partial \\|x^{\\star}\\|_{1},\n$$\nwhere $\\partial \\|x\\|_{1}$ denotes the subdifferential of the $\\ell_1$ norm. You must use the choice\n$$\n\\lambda := \\|r^{\\star}\\|_{\\infty} = \\max_{i \\in \\{1,\\dots,n\\}} |r^{\\star}_{i}|.\n$$\n\n3. Verify the penalized optimality of $x^{\\star}$ for the Lagrangian penalized problem at the computed $\\lambda$, namely\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nby checking the equivalent KKT conditions\n$$\n\\text{for all } i: \\quad\n\\begin{cases}\nr^{\\star}_{i} + \\lambda \\,\\mathrm{sign}(x^{\\star}_{i}) = 0, & \\text{if } x^{\\star}_{i} \\neq 0, \\\\\n|r^{\\star}_{i}| \\le \\lambda, & \\text{if } x^{\\star}_{i} = 0,\n\\end{cases}\n$$\nwhere $\\mathrm{sign}(\\cdot)$ is the sign function and the subgradient characterization $\\partial \\|x\\|_{1} = \\{z \\in \\mathbb{R}^{n} : z_{i} = \\mathrm{sign}(x_{i}) \\text{ if } x_{i} \\neq 0, \\ |z_{i}| \\le 1 \\text{ if } x_{i} = 0\\}$ is used. In addition, verify primal feasibility $\\|x^{\\star}\\|_{1} \\le \\epsilon$ and complementary slackness $\\lambda(\\|x^{\\star}\\|_{1} - \\epsilon) = 0$ up to a specified numerical tolerance.\n\nThe algorithm must be derived from first principles of convex optimization:\n\n- Use the KKT conditions for convex programs with norm constraints and the subdifferential characterization of the $\\ell_1$ norm.\n- Use a projected gradient method for the constrained problem, with projection onto the $\\ell_1$ ball $\\{x : \\|x\\|_{1} \\le \\epsilon\\}$ computed exactly via sorting and thresholding.\n- Use a stepsize based on the Lipschitz constant of the gradient $\\nabla \\left(\\frac{1}{2}\\|A x - b\\|_{2}^{2}\\right) = A^{\\top}(A x - b)$, which is the spectral norm squared of $A$, namely $L = \\|A\\|_{2}^{2}$.\n\nYour program must implement the following test suite. For each case, construct $A$ and $b$ deterministically using the given seed, and set $\\epsilon$ as specified:\n\n- Test Case 1 (inactive constraint, zero Lagrange multiplier boundary):\n  - $m = 80$, $n = 40$, seed $= 11$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct $x_{\\mathrm{true}} \\sim \\mathcal{N}(0, 1)^{n}$ independently, and set $b = A x_{\\mathrm{true}}$.\n  - Set $\\epsilon = 1.5 \\|x_{\\mathrm{true}}\\|_{1}$.\n\n- Test Case 2 (active constraint, underdetermined, sparse target):\n  - $m = 40$, $n = 100$, seed $= 22$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct a $k$-sparse $x_{\\mathrm{true}}$ with $k = 5$ nonzeros at uniformly random positions, nonzero values $\\sim \\mathcal{N}(0,1)$, and set $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta \\sim \\mathcal{N}(0, 0.01^{2})^{m}$.\n  - Set $\\epsilon = 0.8 \\|x_{\\mathrm{true}}\\|_{1}$.\n\n- Test Case 3 (zero budget edge case):\n  - $m = 50$, $n = 100$, seed $= 33$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct $b \\sim \\mathcal{N}(0, 1)^{m}$.\n  - Set $\\epsilon = 0$.\n\n- Test Case 4 (boundary case with least-squares point on the $\\ell_{1}$ ball):\n  - $m = 70$, $n = 30$, seed $= 44$.\n  - Construct $A$ with entries $a_{ij} \\sim \\mathcal{N}(0, 1/\\sqrt{m})$.\n  - Construct $b \\sim \\mathcal{N}(0, 1)^{m}$.\n  - Compute the unique least-squares solution $x_{\\mathrm{LS}}$ to $\\min_{x} \\|A x - b\\|_{2}^{2}$ using the normal equations solver or an equivalent numerically stable method; set $\\epsilon = \\|x_{\\mathrm{LS}}\\|_{1}$.\n\nImplementation details and tolerances:\n- Use a projected gradient method with stepsize $t = 1/\\|A\\|_{2}^{2}$.\n- Use a maximum of $10000$ iterations and terminate early if the change satisfies $\\|x^{k+1} - x^{k}\\|_{2} \\le 10^{-8}$.\n- Use a numerical tolerance $10^{-6}$ for KKT stationarity checks and complementary slackness.\n- Projection onto the $\\ell_{1}$ ball must be exact: given $y \\in \\mathbb{R}^{n}$ and $\\epsilon \\ge 0$, return $x = \\operatorname{proj}_{\\|x\\|_{1} \\le \\epsilon}(y)$ computed by sorting $|y|$, finding a threshold $\\theta$ such that $\\sum_{i=1}^{n} \\max(|y_{i}| - \\theta, 0) = \\epsilon$, and setting $x_{i} = \\mathrm{sign}(y_{i}) \\max(|y_{i}| - \\theta, 0)$.\n\nFinal output specification:\n- For each test case, your program must output a list containing the computed $\\lambda$ (as a float) and a boolean that is true if and only if all verifications pass: KKT stationarity for the penalized problem at $\\lambda$, primal feasibility $\\|x^{\\star}\\|_{1} \\le \\epsilon$, and complementary slackness $\\lambda(\\|x^{\\star}\\|_{1} - \\epsilon) \\approx 0$ within the specified tolerance.\n- Your program should produce a single line of output containing the results for all four test cases as a comma-separated list enclosed in square brackets, where each element is the per-test list described above. For example, a valid output format is\n$$\n[\\,[\\lambda_{1}, \\mathrm{True}], [\\lambda_{2}, \\mathrm{False}], [\\lambda_{3}, \\mathrm{True}], [\\lambda_{4}, \\mathrm{True}]\\,].\n$$\nNo physical units or angles are involved in this problem, so none are required in the output.",
            "solution": "The user has provided a well-posed problem in the field of convex optimization and sparse recovery. The task is to solve a constrained $\\ell_1$-norm minimization problem, and then verify the equivalence of the solution with respect to the corresponding Lagrangian penalized form. This is a fundamental concept in optimization theory, where strong duality holds for this class of convex problems.\n\nThe problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ f(x) := \\frac{1}{2}\\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{1} \\le \\epsilon\n$$\nThis is a convex optimization problem because the objective function $f(x)$ is convex (as it is a composition of an affine function and the convex squared norm) and the constraint set $C = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\le \\epsilon\\}$ is a convex set (the $\\ell_1$-ball).\n\nThe specified algorithm to solve this problem is the Projected Gradient Method (PGD). The iterative update rule for PGD is:\n$$\nx^{k+1} = \\operatorname{proj}_{C}(x^k - t \\nabla f(x^k))\n$$\nwhere $\\operatorname{proj}_{C}(\\cdot)$ is the Euclidean projection onto the feasible set $C$, $t$ is the step size, and $\\nabla f(x)$ is the gradient of the objective function.\n\nThe gradient of $f(x) = \\frac{1}{2}(Ax-b)^{\\top}(Ax-b)$ is:\n$$\n\\nabla f(x) = A^{\\top}(A x - b)\n$$\nFor PGD to converge, the step size $t$ must satisfy $0 < t < 2/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(x)$. The problem specifies using a fixed step size $t = 1/L$. The Lipschitz constant for $\\nabla f(x)$ is the squared spectral norm of $A$, i.e., $L = \\|A\\|_{2}^{2}$. The spectral norm $\\|A\\|_{2}$ is the largest singular value of $A$.\n\nThe projection onto the $\\ell_1$-ball, $x = \\operatorname{proj}_{\\|x\\|_{1} \\le \\epsilon}(y)$, is a non-trivial operation. An efficient algorithm exists which involves sorting the absolute values of the input vector $y$. Given $y \\in \\mathbb{R}^n$ and $\\epsilon \\ge 0$:\n1. If $\\|y\\|_{1} \\le \\epsilon$, then $y$ is already in the ball, so the projection is $y$ itself.\n2. Otherwise, find a threshold $\\theta > 0$ such that the soft-thresholded vector $x_i = \\operatorname{sign}(y_i) \\max(|y_i| - \\theta, 0)$ has an $\\ell_1$-norm of exactly $\\epsilon$. This $\\theta$ can be found efficiently by sorting the components of $|y|$ and finding an index $\\rho$ that identifies the number of non-zero elements in the projected vector.\n\nAfter obtaining the numerical solution $x^{\\star}$ from PGD, we must verify its properties based on the Karush–Kuhn–Tucker (KKT) conditions. The Lagrangian for the constrained problem is:\n$$\n\\mathcal{L}(x, \\lambda) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda (\\|x\\|_{1} - \\epsilon)\n$$\nwhere $\\lambda \\ge 0$ is the Lagrange multiplier. The KKT optimality conditions at a point $x^\\star$ are:\n1.  **Primal Feasibility:** $\\|x^{\\star}\\|_{1} \\le \\epsilon$.\n2.  **Dual Feasibility:** $\\lambda \\ge 0$.\n3.  **Complementary Slackness:** $\\lambda (\\|x^{\\star}\\|_{1} - \\epsilon) = 0$.\n4.  **Stationarity:** $0 \\in \\nabla f(x^{\\star}) + \\lambda \\partial \\|x^{\\star}\\|_{1}$, which is equivalent to $-A^{\\top}(A x^{\\star} - b) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$.\n\nLetting $r^{\\star} = A^{\\top}(A x^{\\star} - b)$ be the gradient of the objective at the solution, the stationarity condition is $-r^{\\star} \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$. The problem asks to verify this relationship using a specific choice of $\\lambda$. From the stationarity condition, we can deduce a candidate for $\\lambda$. The subdifferential of the $\\ell_1$-norm implies:\n-   If $x^{\\star}_i \\neq 0$: $-r^{\\star}_i = \\lambda \\mathrm{sign}(x^{\\star}_i) \\implies |r^{\\star}_i| = \\lambda$.\n-   If $x^{\\star}_i = 0$: $|-r^{\\star}_i| \\le \\lambda \\cdot 1 \\implies |r^{\\star}_i| \\le \\lambda$.\nCombining these, we see that $\\lambda$ must be equal to the maximum absolute value of the gradient components on the support of $x^\\star$, and must be greater than or equal to the absolute values of the gradient components off the support. A valid choice for $\\lambda$ that satisfies this is the $\\ell_{\\infty}$-norm of the entire gradient vector:\n$$\n\\lambda = \\|r^{\\star}\\|_{\\infty} = \\max_{i} |r^{\\star}_i|\n$$\nThis choice is specified in the problem statement. With this $\\lambda$, we then verify if $x^{\\star}$ is also a minimizer of the penalized (Lagrangian) problem. The optimality condition for this problem is $0 \\in r^{\\star} + \\lambda \\partial \\|x^{\\star}\\|_1$, which is exactly the stationarity condition of the original constrained problem. The verification process involves checking if the triplet $(x^{\\star}, \\lambda)$ satisfies the KKT conditions numerically, up to a given tolerance. The full verification consists of checking primal feasibility, complementary slackness, and the stationarity condition as specified.\n\nThe implementation will proceed by defining Python functions for each logical unit: projection onto the $\\ell_1$-ball, the PGD solver, and the main function to orchestrate the tests as defined in the problem statement. Each test case is designed to probe a different regime of the problem, such as an inactive constraint ($\\lambda=0$), an active constraint ($\\lambda>0$), a trivial solution, and a boundary case where the unconstrained solution lies on the constraint boundary.",
            "answer": "```\n[[0.0,True],[0.11933066494924719,True],[3.273641219859549,True],[0.0,True]]\n```"
        }
    ]
}