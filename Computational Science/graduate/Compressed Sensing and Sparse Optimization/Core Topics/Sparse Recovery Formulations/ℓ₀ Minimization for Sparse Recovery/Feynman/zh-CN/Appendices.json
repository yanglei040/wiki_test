{
    "hands_on_practices": [
        {
            "introduction": "理论的价值在于解释和预测。本练习旨在通过一个精心设计的思想实验，帮助您建立关于稀疏恢复核心挑战的直观理解。我们将构建一个场景，其中用于$\\ell_0$最小化的贪婪算法（正交匹配追踪，OMP）能够成功恢复真实的稀疏信号，而广泛使用的$\\ell_1$凸松弛方法却失败了。通过从第一性原理出发进行计算，您将亲手揭示信号的动态范围（dynamic range）如何与传感矩阵的几何结构（以互相关性$\\mu$为度量）相互作用，从而决定恢复算法的成败。",
            "id": "3455943",
            "problem": "考虑一个传感矩阵 $A \\in \\mathbb{R}^{2 \\times 3}$，其列 $a_{1}, a_{2}, a_{3}$ 为单位范数列，定义为 $a_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$a_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，以及 $a_{3} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$。$A$ 的互相关性定义为 $\\mu(A) \\triangleq \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$，对于此矩阵 $A$ 有 $\\mu(A) = \\frac{1}{\\sqrt{2}}$。设未知信号 $x^{\\star} \\in \\mathbb{R}^{3}$ 的支撑集为索引集 $S^{\\star} = \\{1,2\\}$，其系数为正，$x_{1}^{\\star} = \\alpha$，$x_{2}^{\\star} = \\beta$，以及 $x_{3}^{\\star} = 0$，其中 $\\alpha \\geq \\beta  0$。测量是无噪声的：$y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$。定义动态范围 $R \\triangleq \\alpha / \\beta$。\n\n根据基本原理，使用互相关性、正交匹配追踪（OMP；一种通过相关性选择原子并执行正交投影的贪心算法）以及 $\\ell_{1}$-基追踪（在 $A x = y$ 约束下最小化 $x$ 的 $\\ell_1$ 范数）的定义，完成以下任务：\n\n1. 证明 $\\ell_{1}$-基追踪的解无法恢复真实支撑集 $S^{\\star}$，而是倾向于一个包含错误原子 $a_{3}$ 的不同双项表示。您必须比较 $y$ 的两种精确双稀疏表示的 $\\ell_{1}$ 范数。\n\n2. 推导在 OMP 第一步和第二步中，正确原子和错误原子之间的精确相关性边际，用互相关性 $\\mu(A)$ 和动态范围 $R$ 表示。具体来说，令 $M_{1}$ 表示第一步中最大正确相关性与最大错误相关性之间的差值，令 $M_{2}$ 表示正交投影后第二步中的这个差值。将 $M_{1}$ 和 $M_{2}$ 表示为仅含 $\\mu(A)$、$R$ 和 $\\beta$ 的解析函数。\n\n3. 确定最小动态范围 $R_{\\mathrm{min}}$，使得 OMP 在第一步选择 $a_{1}$，然后在第二步选择 $a_{2}$，从而恢复真实支撑集 $S^{\\star}$。您的最终答案必须是此 $R_{\\mathrm{min}}$ 的精确解析形式。无需四舍五入，且不涉及单位。\n\n最终答案必须是单一的封闭形式解析表达式。",
            "solution": "问题陈述已经过验证，被认为是有效的。它是自洽的，科学上基于压缩感知的原理，并且是适定的。所有提供的定义和数据都是一致的。给定的互相关性值 $\\mu(A) = \\frac{1}{\\sqrt{2}}$ 是正确的，通过计算内积可以验证：$|a_{1}^{\\top}a_{2}| = |0| = 0$，$|a_{1}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$，以及 $|a_{2}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$。这些值中的最大值为 $\\frac{1}{\\sqrt{2}}$。\n\n我们现在开始解决问题的三个部分。测量向量由下式给出 $y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$。代入 $a_{1}$ 和 $a_{2}$ 的定义：\n$$y = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$$\n\n**1. $\\ell_{1}$-基追踪失败的证明**\n\n$\\ell_{1}$-基追踪问题旨在找到一个系数向量 $x$，该向量在满足测量约束的条件下最小化 $\\ell_{1}$ 范数：\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y $$\n真实的稀疏信号是 $x^{\\star} = (\\alpha, \\beta, 0)^{\\top}$，其支撑集为 $S^{\\star} = \\{1, 2\\}$。真实信号的 $\\ell_{1}$ 范数是：\n$$ \\|x^{\\star}\\|_{1} = |\\alpha| + |\\beta| + |0| = \\alpha + \\beta $$\n因为已知 $\\alpha \\geq \\beta  0$。\n\n为了证明 $\\ell_{1}$-基追踪未能恢复 $x^{\\star}$，我们必须证明存在另一个信号 $x^{\\text{alt}}$，使得 $A x^{\\text{alt}} = y$ 但 $\\|x^{\\text{alt}}\\|_{1}  \\|x^{\\star}\\|_{1}$。我们来找到一个使用原子 $\\{a_{1}, a_{3}\\}$ 的 $y$ 的 2-稀疏表示。设这个替代信号为 $x^{(2)} = (c_{1}, 0, c_{3})^{\\top}$。我们必须满足约束 $A x^{(2)} = y$：\n$$ c_{1} a_{1} + c_{3} a_{3} = y $$\n$$ c_{1} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_{3} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} $$\n这产生了一个线性方程组：\n$$ \\begin{cases} c_{1} + \\frac{c_{3}}{\\sqrt{2}} = \\alpha \\\\ \\frac{c_{3}}{\\sqrt{2}} = \\beta \\end{cases} $$\n从第二个方程，我们得到 $c_{3} = \\sqrt{2}\\beta$。将其代入第一个方程得到 $c_{1} + \\beta = \\alpha$，所以 $c_{1} = \\alpha - \\beta$。\n替代信号是 $x^{(2)} = (\\alpha - \\beta, 0, \\sqrt{2}\\beta)^{\\top}$。\n\n现在我们计算 $x^{(2)}$ 的 $\\ell_{1}$ 范数。由于 $\\alpha \\geq \\beta  0$，我们有 $\\alpha - \\beta \\geq 0$ 且 $\\sqrt{2}\\beta  0$。\n$$ \\|x^{(2)}\\|_{1} = |\\alpha - \\beta| + |0| + |\\sqrt{2}\\beta| = (\\alpha - \\beta) + \\sqrt{2}\\beta = \\alpha + (\\sqrt{2}-1)\\beta $$\n我们将其与真实信号的 $\\ell_{1}$ 范数进行比较：\n$$ \\|x^{\\star}\\|_{1} = \\alpha + \\beta $$\n差值为 $\\|x^{\\star}\\|_{1} - \\|x^{(2)}\\|_{1} = (\\alpha + \\beta) - (\\alpha + (\\sqrt{2}-1)\\beta) = \\beta - (\\sqrt{2}-1)\\beta = (2-\\sqrt{2})\\beta$。\n由于 $\\sqrt{2}  2$ 且 $\\beta > 0$，差值是严格为正的。\n$$ \\|x^{(2)}\\|_{1}  \\|x^{\\star}\\|_{1} $$\n因为存在一个 $y$ 的替代表示，其 $\\ell_{1}$ 范数严格更小，所以 $\\ell_{1}$-基追踪问题的解不是真实信号 $x^{\\star}$。$\\ell_{1}$ 解的支撑集将包含索引 3，因此未能恢复真实支撑集 $S^{\\star}=\\{1,2\\}$。\n\n**2. OMP 相关性边际**\n\n正交匹配追踪（OMP）算法迭代地选择与当前残差最相关的 $A$ 的列。\n\n**第一步 (OMP-1)：**\n初始残差为 $r_{0} = y = \\alpha a_{1} + \\beta a_{2}$。我们计算原子与此残差的相关性：\n$$ |a_{1}^{\\top} r_{0}| = |a_{1}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{1}^{\\top}a_{1}) + \\beta (a_{1}^{\\top}a_{2})| = |\\alpha \\cdot 1 + \\beta \\cdot 0| = \\alpha $$\n$$ |a_{2}^{\\top} r_{0}| = |a_{2}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{2}^{\\top}a_{1}) + \\beta (a_{2}^{\\top}a_{2})| = |\\alpha \\cdot 0 + \\beta \\cdot 1| = \\beta $$\n$$ |a_{3}^{\\top} r_{0}| = |a_{3}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{3}^{\\top}a_{1}) + \\beta (a_{3}^{\\top}a_{2})| = \\left|\\alpha \\frac{1}{\\sqrt{2}} + \\beta \\frac{1}{\\sqrt{2}}\\right| = \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\n正确的原子是 $a_{1}$ 和 $a_{2}$。错误的原子是 $a_{3}$。\n最大正确相关性是 $\\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|) = \\max(\\alpha, \\beta) = \\alpha$，因为 $\\alpha \\geq \\beta$。\n最大（也是唯一）的错误相关性是 $|a_{3}^{\\top} r_{0}| = \\frac{\\alpha + \\beta}{\\sqrt{2}}$。\n相关性边际 $M_{1}$ 是其差值：\n$$ M_{1} = \\alpha - \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\n使用 $\\mu(A) = \\frac{1}{\\sqrt{2}}$ 和 $R = \\frac{\\alpha}{\\beta}$，我们将 $M_{1}$ 表示为：\n$$ M_{1} = \\alpha - \\mu(A)(\\alpha + \\beta) = \\alpha(1-\\mu(A)) - \\beta\\mu(A) = \\beta R(1-\\mu(A)) - \\beta\\mu(A) = \\beta[R(1-\\mu(A)) - \\mu(A)] $$\n\n**第二步 (OMP-2)：**\n为了使 OMP 成功，它必须首先选择一个正确的原子。如第 3 部分所示，这要求选择 $a_{1}$，这意味着我们假设 $M_{1}0$。选择原子 $a_{1}$ 后，OMP 通过将 $y$ 正交投影到由 $a_{1}$ 张成的子空间上，并从 $y$ 中减去该投影来更新残差。新的残差 $r_{1}$ 为：\n$$ r_{1} = y - P_{a_{1}}(y) = y - (a_{1}^{\\top}y)a_{1} $$\n我们有 $a_{1}^{\\top}y = a_{1}^{\\top}(\\alpha a_{1} + \\beta a_{2}) = \\alpha$。因此：\n$$ r_{1} = (\\alpha a_{1} + \\beta a_{2}) - \\alpha a_{1} = \\beta a_{2} $$\n下一个原子是通过在剩余原子 $\\{a_{2}, a_{3}\\}$ 中最大化与 $r_{1}$ 的相关性来选择的：\n$$ |a_{2}^{\\top} r_{1}| = |a_{2}^{\\top} (\\beta a_{2})| = \\beta|a_{2}^{\\top}a_{2}| = \\beta $$\n$$ |a_{3}^{\\top} r_{1}| = |a_{3}^{\\top} (\\beta a_{2})| = \\beta|a_{3}^{\\top}a_{2}| = \\beta \\frac{1}{\\sqrt{2}} $$\n要选择的正确原子是 $a_{2}$，错误原子是 $a_{3}$。相关性边际 $M_{2}$ 是：\n$$ M_{2} = \\beta - \\frac{\\beta}{\\sqrt{2}} = \\beta \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\n使用 $\\mu(A) = \\frac{1}{\\sqrt{2}}$，这可以简化为：\n$$ M_{2} = \\beta(1 - \\mu(A)) $$\n这个边际不依赖于动态范围 $R$。\n\n**3. OMP 成功的最小动态范围**\n\n如果 OMP 在第一步选择原子 $a_{1}$ 并在第二步选择原子 $a_{2}$，它就能成功恢复支撑集 $S^{\\star} = \\{1,2\\}$。\n\n**第一步的条件：** OMP 必须从真实支撑集 $S^{\\star}=\\{1,2\\}$ 中选择一个原子。相关性分别为 $\\alpha$ 和 $\\beta$。来自真实支撑集的最大相关性是 $\\alpha$。这必须严格大于与错误原子 $a_{3}$ 的相关性。\n$$ \\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|)  |a_{3}^{\\top} r_{0}| $$\n$$ \\alpha  \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\n我们针对动态范围 $R = \\alpha/\\beta$ 求解这个不等式：\n$$ \\alpha\\sqrt{2}  \\alpha + \\beta $$\n$$ \\alpha(\\sqrt{2} - 1)  \\beta $$\n$$ \\frac{\\alpha}{\\beta}  \\frac{1}{\\sqrt{2} - 1} $$\n为了使分母有理化，我们将分子和分母同乘以 $(\\sqrt{2} + 1)$：\n$$ \\frac{1}{\\sqrt{2} - 1} = \\frac{\\sqrt{2} + 1}{(\\sqrt{2} - 1)(\\sqrt{2} + 1)} = \\frac{\\sqrt{2} + 1}{2 - 1} = \\sqrt{2} + 1 $$\n所以，第一步成功的条件是 $R  \\sqrt{2} + 1$。\n\n**第二步的条件：** 如果第一步成功（即 $R  \\sqrt{2} + 1$），则选择原子 $a_{1}$。残差为 $r_{1} = \\beta a_{2}$。OMP 接下来必须从剩余原子 $\\{a_{2}, a_{3}\\}$ 中选择 $a_{2}$。这要求：\n$$ |a_{2}^{\\top} r_{1}|  |a_{3}^{\\top} r_{1}| $$\n如前所计算，这是：\n$$ \\beta  \\frac{\\beta}{\\sqrt{2}} $$\n由于 $\\beta  0$，我们可以两边除以 $\\beta$ 得到 $1  \\frac{1}{\\sqrt{2}}$，这是成立的，因为 $\\sqrt{2}  1$。因此，如果第一步成功，第二步也保证会成功。\n\nOMP 通过先选 $a_1$ 再选 $a_2$ 来恢复真实支撑集 $S^{\\star}$ 的总体条件完全由第一步的条件决定：$R  \\sqrt{2} + 1$。问题要求出现这种情况的最小动态范围 $R_{\\mathrm{min}}$。这对应于保证成功的 $R$ 值集合的下确界。\n$$ R_{\\mathrm{min}} = \\sqrt{2} + 1 $$\n在这个边界值上，正确原子 $a_{1}$ 的相关性与错误原子 $a_{3}$ 的相关性相等，如果没有一个有利的平局决胜规则，就不能严格保证成功恢复。对于任何 $R  R_{\\mathrm{min}}$，OMP 的成功是有保证的。",
            "answer": "$$\\boxed{\\sqrt{2} + 1}$$"
        },
        {
            "introduction": "在理解稀疏恢复能力的边界时，理论保证是至关重要的。本练习将引导您对比两种核心的$\\ell_0$最小化唯一性条件：一种基于矩阵的“火花”（spark），它给出了最强的保证但难以计算；另一种基于互相关性（mutual coherence），它更容易计算但可能过于保守。通过分析一个具体例子，您将量化这种“悲观性”，并探索如何通过矩阵预处理（preconditioning）技术来改善相关性结构，从而获得更强的恢复保证。",
            "id": "3455953",
            "problem": "考虑无噪声线性测量模型 $y = A x$，其中 $A \\in \\mathbb{R}^{3 \\times 4}$，其列向量 $a_1, a_2, a_3, a_4 \\in \\mathbb{R}^3$ 由下式给出：\n$$\na_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\na_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix},\\quad\na_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\na_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n$$\n字典 $A$ 的互相关性 $\\mu(A)$ 定义为\n$$\n\\mu(A) \\triangleq \\max_{i \\neq j} \\frac{|\\langle a_i, a_j \\rangle|}{\\|a_i\\|_2 \\, \\|a_j\\|_2},\n$$\n且 $A$ 的 spark，记作 $\\mathrm{spark}(A)$，是 $A$ 中线性相关的列向量的最小数量。对于通过 $\\ell_0$ 最小化实现的稀疏恢复，两个广泛使用的唯一性条件是：一个依赖于 $\\mu(A)$ 的基于相关性的条件，和一个依赖于 $\\mathrm{spark}(A)$ 的基于 spark 的条件。在本问题中，您将分析这个特定的高相关性示例，量化基于相关性的界限相对于基于 spark 的界限的悲观程度，并评估列归一化、正交化和左预处理如何影响互相关性和隐含的唯一性阈值。\n\n定义左预处理器\n$$\nW \\triangleq \\big(A A^\\top\\big)^{-1/2},\n$$\n以及预处理后的字典 $B \\triangleq W A$。您可以假设 $A A^\\top$ 是对称正定的，并且 $W$ 是良定义的。\n\n选择所有正确的陈述：\n\nA. 互相关性满足 $\\mu(A) = \\dfrac{1}{\\sqrt{3}}$，且基于相关性的唯一性阈值 $k$ 严格低于基于 spark 的阈值 $k  \\dfrac{\\mathrm{spark}(A)}{2}$，这表明对于该矩阵 $A$，基于相关性的界限是悲观的。\n\nB. $A$ 的 spark 是 $\\mathrm{spark}(A) = 3$。\n\nC. 对于此矩阵 $A$，将其列归一化为单位 $\\ell_2$ 范数不会改变 $\\mu(A)$，因为互相关性是使用归一化内积定义的；因此，基于相关性的 $k$ 的阈值保持不变。\n\nD. 使用左预处理器 $W = \\big(A A^\\top\\big)^{-1/2}$，预处理后字典 $B = W A$ 的互相关性变为 $\\mu(B) = \\dfrac{1}{3}$，基于相关性的唯一性条件变为 $k  2$，与基于 spark 的阈值相匹配（尽管由于严格不等式，仍然排除了 $k = 2$）。\n\nE. 对列进行右正交化（例如，用一个正交矩阵 $Q \\in \\mathbb{R}^{4 \\times 4}$ 替换 $A$ 为 $A Q$ 以使其列向量标准正交）会保留 $x$ 的支撑集，并在不改变原始 $x$ 的稀疏恢复保证的情况下，有效地将互相关性降低到 $0$。",
            "solution": "首先，我们计算矩阵 $A$ 的几个关键属性。\n\n**计算列范数和内积：**\n- $\\|a_1\\|_2 = 1$\n- $\\|a_2\\|_2 = \\sqrt{1^2+1^2+1^2} = \\sqrt{3}$\n- $\\|a_3\\|_2 = 1$\n- $\\|a_4\\|_2 = 1$\n- $\\langle a_1, a_2 \\rangle = 1$, $\\langle a_2, a_3 \\rangle = 1$, $\\langle a_2, a_4 \\rangle = 1$\n- 其他所有不同列之间的内积均为 0。\n\n**评估选项 A：**\n该选项声称 $\\mu(A) = \\frac{1}{\\sqrt{3}}$ 并且基于相关性的界限是悲观的。\n- **互相关性 $\\mu(A)$**：\n  $\\mu(A) = \\max_{i \\neq j} \\frac{|\\langle a_i, a_j \\rangle|}{\\|a_i\\|_2 \\|a_j\\|_2}$。\n  非零的相关性值是：\n  $\\frac{|\\langle a_1, a_2 \\rangle|}{\\|a_1\\|_2 \\|a_2\\|_2} = \\frac{1}{1 \\cdot \\sqrt{3}} = \\frac{1}{\\sqrt{3}}$\n  $\\frac{|\\langle a_2, a_3 \\rangle|}{\\|a_2\\|_2 \\|a_3\\|_2} = \\frac{1}{\\sqrt{3} \\cdot 1} = \\frac{1}{\\sqrt{3}}$\n  $\\frac{|\\langle a_2, a_4 \\rangle|}{\\|a_2\\|_2 \\|a_4\\|_2} = \\frac{1}{\\sqrt{3} \\cdot 1} = \\frac{1}{\\sqrt{3}}$\n  因此，$\\mu(A) = \\frac{1}{\\sqrt{3}}$。陈述的第一部分正确。\n- **比较唯一性阈值**：\n  基于相关性的唯一性条件是 $k  \\frac{1}{2} (1 + \\frac{1}{\\mu(A)}) = \\frac{1}{2} (1 + \\sqrt{3}) \\approx 1.366$。这保证了对于 $k=1$ 的稀疏信号可以唯一恢复。\n  基于 spark 的唯一性条件是 $k  \\frac{\\mathrm{spark}(A)}{2}$。我们将在选项 B 中计算 $\\mathrm{spark}(A)$。现在我们先计算出它：任意 3 列都是线性无关的（例如，$\\det([a_1, a_2, a_3]) = -1 \\neq 0$），但所有 4 列都在 $\\mathbb{R}^3$ 中，因此必然是线性相关的（具体来说，$a_1 - a_2 + a_3 + a_4 = 0$）。所以 $\\mathrm{spark}(A) = 4$。\n  基于 spark 的条件是 $k  \\frac{4}{2} = 2$。这同样保证了对于 $k=1$ 的稀疏信号可以唯一恢复。\n  然而，spark 条件给出的上界 (2) 严格大于相关性条件给出的上界 ($\\approx 1.366$)。这意味着 spark 条件允许的稀疏度范围更广（尽管在这个例子中，对于整数 $k$ 来说没有差别），因此相关性条件被认为是更保守或“悲观”的。\n  **结论：选项 A 是正确的。**\n\n**评估选项 B：**\n该选项声称 $\\mathrm{spark}(A) = 3$。如上所述，我们已经证明了任意 3 列都是线性无关的，所以 $\\mathrm{spark}(A) > 3$。由于存在 4 列的线性相关组合，$\\mathrm{spark}(A)=4$。\n**结论：选项 B 是不正确的。**\n\n**评估选项 C：**\n该选项声称列归一化不会改变 $\\mu(A)$。互相关性的定义 $\\mu(A) = \\max_{i \\neq j} \\frac{|\\langle a_i, a_j \\rangle|}{\\|a_i\\|_2 \\|a_j\\|_2}$ 本身就包含了对列范数的归一化。因此，预先将列归一化为单位范数不会改变最终的 $\\mu(A)$ 值。\n**结论：选项 C 是正确的。**\n\n**评估选项 D：**\n该选项评估了预处理后的矩阵 $B=WA$。\n- **计算 $\\mu(B)$**：$B$ 的格拉姆矩阵是 $G = B^\\top B = A^\\top (AA^\\top)^{-1} A$。\n  $A A^\\top = \\begin{bmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{bmatrix}$。其逆矩阵是 $(AA^\\top)^{-1} = \\frac{1}{4} \\begin{bmatrix} 3  -1  -1 \\\\ -1  3  -1 \\\\ -1  -1  3 \\end{bmatrix}$。\n  $G$ 的对角元素，例如 $G_{11} = a_1^\\top (AA^\\top)^{-1} a_1 = \\frac{3}{4}$。所有对角元素都是 $3/4$。\n  $G$ 的非对角元素，例如 $G_{12} = a_1^\\top (AA^\\top)^{-1} a_2 = \\frac{1}{4}(3-1-1) = \\frac{1}{4}$。所有非对角元素的绝对值都是 $1/4$。\n  因此，$\\mu(B) = \\max_{i \\neq j} \\frac{|G_{ij}|}{\\sqrt{G_{ii}G_{jj}}} = \\frac{1/4}{3/4} = \\frac{1}{3}$。陈述的第一部分正确。\n- **比较唯一性阈值**：\n  对于矩阵 $B$，新的基于相关性的条件是 $k  \\frac{1}{2} (1 + \\frac{1}{\\mu(B)}) = \\frac{1}{2} (1 + 3) = 2$。\n  预处理是一个可逆的线性变换，它不改变 $A$ 的零空间，因此 $\\mathrm{spark}(B) = \\mathrm{spark}(A) = 4$。基于 spark 的条件仍然是 $k  \\frac{4}{2} = 2$。\n  新的相关性界限 ($k  2$) 现在与 spark 界限 ($k  2$) 完全匹配。\n  **结论：选项 D 是正确的。**\n\n**评估选项 E：**\n该选项建议通过右乘正交矩阵 $Q$ 来使列标准正交。\n首先，一个 $3 \\times 4$ 矩阵的 4 个列向量位于 $\\mathbb{R}^3$ 中，它们不可能是线性无关的，因此不可能被标准正交化。这个前提就是错误的。\n其次，变换 $x \\to z=Q^\\top x$ 会将一个稀疏向量 $x$ 映射到一个通常是稠密的向量 $z$，从而破坏了稀疏恢复问题的前提。\n**结论：选项 E 是不正确的。**",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "虽然找到$\\ell_0$范数最小解在计算上是困难的（NP-hard），但在某些条件下，验证一个给定的候选解是否为全局最优解却非常直接。本编程练习将理论与实践相结合，要求您将$\\ell_0$正则化最小二乘问题的最优性条件转化为一个实用的验证算法。您将为具有正交列这一特殊但重要的传感矩阵情形，实现一个“认证程序”，从而掌握一个验证解的全局最优性的强大工具。",
            "id": "3455948",
            "problem": "考虑一个稀疏惩罚最小二乘目标，它由矩阵 $A \\in \\mathbb{R}^{m \\times n}$、数据向量 $b \\in \\mathbb{R}^{m}$ 和正则化参数 $\\lambda \\in \\mathbb{R}_{+}$ 定义：\n$$\nF(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0,\n$$\n其中 $\\lVert x \\rVert_0$ 表示 $x$ 中非零项的计数。设 $S \\subseteq \\{0,1,\\dots,n-1\\}$ 是一个候选支撑集。将 $S$ 上的受限最小二乘解定义为 $x_S^\\star \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2$，其中 $A_S$ 表示由 $S$ 索引的列组成的 $A$ 的子矩阵。定义残差 $r_S \\triangleq b - A_S x_S^\\star$。\n\n仅使用关于欧几里得投影和最小二乘的基本事实，推导出当通过单个索引 $j \\notin S$ 扩充支撑集，然后在 $S \\cup \\{j\\}$ 上重新优化时 $\\lVert A x - b \\rVert_2^2$ 的精确变化量，以及当从 $S$ 中剪除单个索引 $i \\in S$，然后在 $S \\setminus \\{i\\}$ 上重新优化时的精确变化量。将这两种变化直接用 $r_S$、$A_S$ 和候选列 $a_j \\in \\mathbb{R}^m$（$A$ 的第 $j$ 列）或被剪除的列 $a_i$ 来表示。利用这些推导，构建认证测试，断言在 $A$ 的列是正交规范的特殊情况下（即 $A^\\top A = I_n$），$S$ 是全局最优的。在这种正交规范的情况下，证明检查所有支撑集外的不等式和支撑集上的剪除不等式对于全局最优性是充分且必要的。\n\n你的程序必须实现以下逻辑：\n\n- 步骤 1 (受限回归)：给定 $A$、$b$、$\\lambda$ 和 $S$，计算 $x_S^\\star$ 和 $r_S$。\n\n- 步骤 2 (支撑集外扩充不等式)：对于每个 $j \\notin S$，计算通过将 $j$ 加入 $S$ 并重新优化所能获得的 $\\lVert A x - b \\rVert_2^2$ 的最佳可能减少量。将此减少量表示为 $\\Delta_{\\mathrm{add}}(j \\mid S)$。该认证要求对于所有 $j \\notin S$ 都有 $\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$。\n\n- 步骤 3 (支撑集上剪除不等式)：对于每个 $i \\in S$，计算从 $S$ 中剪除 $i$ 并重新优化所导致的 $\\lVert A x - b \\rVert_2^2$ 的增加量。将此增加量表示为 $\\Delta_{\\mathrm{drop}}(i \\mid S)$。该认证要求对于所有 $i \\in S$ 都有 $\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$。\n\n- 步骤 4 (全局认证的正交规范列先决条件)：当且仅当 $A^\\top A = I_n$ (在数值指定的容差范围内) 并且步骤 2 和 3 成立时，该认证才声明 $S$ 是全局最优的。如果 $A^\\top A \\neq I_n$，即使步骤 2 和 3 成立，程序也必须返回一个负认证（即，它无法认证全局最优性），因为这些不等式不再是充分条件。\n\n设计你的解决方案，使其推导植根于最小二乘的投影定理：对于任何满列秩的 $A_S$，向量 $A_S x_S^\\star$ 是 $b$ 在 $\\mathrm{span}(A_S)$ 上的正交投影，即 $A_S^\\top r_S = 0$。将扩充和剪除解释为残差的一维回归（或移除一列后的重新拟合），并相应地计算确切的目标变化。\n\n此问题不涉及角度。没有物理单位。\n\n实现一个程序，评估以下测试套件并打印汇总结果。每个结果必须是一个布尔值，指示候选支撑集是否在步骤 4 中描述的正交规范列测试下被认证为全局最优：\n\n- 测试用例 1 (顺利路径，正交规范列)：\n    - 维度: $m = 6$, $n = 4$。\n    - 矩阵 $A \\in \\mathbb{R}^{6 \\times 4}$ 的列等于 $\\mathbb{R}^6$ 中的前四个标准基向量。具体来说，$A$ 是一个行如下的矩阵：\n      $$\n      \\begin{bmatrix}\n      1  0  0  0 \\\\\n      0  1  0  0 \\\\\n      0  0  1  0 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - 数据 $b = [\\,2.0,\\,-1.5,\\,0.2,\\,0.0,\\,3.0,\\,-2.0\\,]^\\top$。\n    - 正则化 $\\lambda = 0.5$。\n    - 候选支撑集 $S = \\{0, 1\\}$。\n\n- 测试用例 2 (不等式中的边界等式，正交规范列)：\n    - 维度: $m = 6$, $n = 4$。\n    - 矩阵 $A$ 与测试用例 1 相同。\n    - 数据 $b = [\\,1.0,\\,1.1,\\,0.9,\\,0.0,\\,-0.5,\\,0.3\\,]^\\top$。\n    - 正则化 $\\lambda = 1.0$。\n    - 候选支撑集 $S = \\{0, 1\\}$。\n\n- 测试用例 3 (非正交规范设计，无法通过这些测试认证为全局最优)：\n    - 维度: $m = 6$, $n = 4$。\n    - 矩阵 $A$ 的列为\n      $$\n      a_0 = [\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_1 = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_2 = [\\,0,\\,1,\\,1,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_3 = [\\,0,\\,0,\\,1,\\,1,\\,0,\\,0\\,]^\\top.\n      $$\n      因此，\n      $$\n      A =\n      \\begin{bmatrix}\n      1  1  0  0 \\\\\n      0  1  1  0 \\\\\n      0  0  1  1 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - 数据 $b = [\\,1.5,\\,-0.5,\\,2.0,\\,-1.0,\\,0.0,\\,0.0\\,]^\\top$。\n    - 正则化 $\\lambda = 0.5$。\n    - 候选支撑集 $S = \\{0, 2\\}$。\n\n你的程序应该生成一行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，“[result1,result2,result3]”）。预期的输出是按三个测试用例顺序排列的三个布尔值的列表。不允许有其他输出。",
            "solution": "该问题要求推导 $\\ell_0$ 惩罚最小二乘问题的最优性条件，并实现一个相应的程序，在设计矩阵 $A$ 为正交规范的特殊情况下，对候选支撑集 $S$ 的全局最优性进行认证。\n\n需要最小化的目标函数是 $F(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，$\\lambda  0$，而 $\\lVert x \\rVert_0$ 是 $x \\in \\mathbb{R}^n$ 中非零项的数量。\n\n如果对应的解 $x^{(S)}$ 达到了 $F(x)$ 的最小值，则支撑集 $S \\subseteq \\{0, 1, \\dots, n-1\\}$ 是全局最优的。解 $x^{(S)}$ 定义为仅在索引集合 $S$ 中的项为非零。这些非零值（表示为向量 $x_S^\\star \\in \\mathbb{R}^{|S|}$）通过求解受限最小二乘问题来确定：\n$$\nx_S^\\star = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2.\n$$\n这里，$A_S$ 是由 $A$ 中被 $S$ 索引的列构成的矩阵。假设 $A_S$ 具有满列秩，这个标准最小二乘问题的解由正规方程给出：$x_S^\\star = (A_S^\\top A_S)^{-1} A_S^\\top b$。残差向量是 $r_S \\triangleq b - A_S x_S^\\star$。该支撑集的最小二乘误差是 $\\lVert r_S \\rVert_2^2$。总目标值为 $F(x^{(S)}) = \\lVert r_S \\rVert_2^2 + \\lambda |S|$。\n\n一个支撑集 $S$ 是最优的，当且仅当对 $S$ 的任何单元素更改（无论是添加一个不在 $S$ 中的元素还是从 $S$ 中移除一个元素）都不能降低目标函数 $F(x)$。\n\n**1. 扩充支撑集 (添加一个元素 $j \\notin S$)**\n\n让我们考虑添加一个索引 $j \\notin S$ 来形成一个新的支撑集 $S' = S \\cup \\{j\\}$。新的目标值将是 $F(x^{(S')}) = \\lVert r_{S'} \\rVert_2^2 + \\lambda (|S|+1)$。目标函数的变化是：\n$$\n\\Delta F_{\\mathrm{add}} = F(x^{(S')}) - F(x^{(S)}) = (\\lVert r_{S'} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) + \\lambda.\n$$\n项 $\\lVert r_S \\rVert_2^2 - \\lVert r_{S'} \\rVert_2^2$ 是最小二乘误差的减少量，我们将其表示为 $\\Delta_{\\mathrm{add}}(j \\mid S)$。为了使 $S$ 是最优的，对于所有 $j \\notin S$，必须有 $\\Delta F_{\\mathrm{add}} \\ge 0$，这意味着条件：\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda \\quad \\forall j \\notin S.\n$$\n为了推导 $\\Delta_{\\mathrm{add}}(j \\mid S)$，我们利用一个性质：在 $S$ 上的最小二乘拟合将 $b$ 投影到 $A_S$ 的列空间上（表示为 $\\mathrm{span}(A_S)$），而 $r_S$ 是这个投影的误差，因此 $r_S$ 与 $\\mathrm{span}(A_S)$ 正交（$A_S^\\top r_S = 0$）。当我们用 $j$ 扩充支撑集时，我们试图用新的列 $a_j$ 来解释剩余的残差 $r_S$。在 $\\mathrm{span}(A_{S \\cup \\{j\\}})$ 中对 $r_S$ 的最佳逼近是其正交投影。由于 $r_S$ 已经与 $\\mathrm{span}(A_S)$ 正交，这简化为将 $r_S$ 投影到 $a_j$ 与 $\\mathrm{span}(A_S)$ 正交的分量上。设 $P_S = A_S (A_S^\\top A_S)^{-1} A_S^\\top$ 是到 $\\mathrm{span}(A_S)$ 上的投影矩阵。相关的新方向是 $\\tilde{a}_j = (I - P_S) a_j$。误差平方的减少量是 $r_S$ 在 $\\tilde{a}_j$ 上投影的长度平方：\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) = \\frac{\\langle r_S, \\tilde{a}_j \\rangle^2}{\\lVert \\tilde{a}_j \\rVert_2^2} = \\frac{(r_S^\\top (I - P_S) a_j)^2}{a_j^\\top (I - P_S)^\\top (I - P_S) a_j} = \\frac{(a_j^\\top r_S)^2}{a_j^\\top (I - P_S) a_j},\n$$\n这里我们使用了 $P_S r_S = 0$ 以及 $I-P_S$ 是一个投影矩阵（幂等且对称）。\n\n**2. 剪除支撑集 (移除一个元素 $i \\in S$)**\n\n现在，考虑移除一个索引 $i \\in S$ 来形成一个新的支撑集 $S'' = S \\setminus \\{i\\}$。目标的变化是：\n$$\n\\Delta F_{\\mathrm{drop}} = F(x^{(S'')}) - F(x^{(S)}) = (\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) - \\lambda.\n$$\n项 $\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2$ 是最小二乘误差的增加量，表示为 $\\Delta_{\\mathrm{drop}}(i \\mid S)$。为保证 $S$ 的最优性，对于所有 $i \\in S$，需要 $\\Delta F_{\\mathrm{drop}} \\ge 0$，这意味着：\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda \\quad \\forall i \\in S.\n$$\n在一般情况下，$\\Delta_{\\mathrm{drop}}(i \\mid S)$ 的推导更为复杂。线性回归的一个已知结果表明，当移除一个变量时，残差平方和的增加量与其系数和方差-协方差矩阵有关。具体来说，如果 $C_S = (A_S^\\top A_S)^{-1}$，则\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) = \\frac{((x_S^\\star)_i)^2}{(C_S)_{ii}},\n$$\n其中 $(x_S^\\star)_i$ 是受限解中对应于列 $a_i$ 的系数，而 $(C_S)_{ii}$ 是 $C_S$ 中对应于索引 $i$ 的对角线项。\n\n**3. 特殊情况：正交规范列 ($A^\\top A = I_n$)**\n\n当 $A$ 的列是正交规范的时，认证全局最优性的问题大大简化。在这种情况下，$A^\\top A = I_n$，并且对于任何子集 $S$，$A_S^\\top A_S = I_{|S|}$。\n\n- **$\\Delta_{\\mathrm{add}}(j \\mid S)$ 的简化**：投影矩阵为 $P_S = A_S A_S^\\top$。由于 $j \\notin S$，$a_j$ 与 $A_S$ 中的所有列正交，所以 $A_S^\\top a_j = 0$，这意味着 $P_S a_j = A_S(A_S^\\top a_j) = 0$。$\\Delta_{\\mathrm{add}}$ 公式中的分母变为 $a_j^\\top (I-P_S) a_j = a_j^\\top a_j = \\lVert a_j \\rVert_2^2 = 1$。分子是 $(a_j^\\top r_S)^2$。因此：\n  $$\n  \\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2.\n  $$\n\n- **$\\Delta_{\\mathrm{drop}}(i \\mid S)$ 的简化**：矩阵 $C_S = (A_S^\\top A_S)^{-1} = I_{|S|}$，所以其对角元素都为 1。$\\Delta_{\\mathrm{drop}}$ 的公式变为：\n  $$\n  \\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_i)^2.\n  $$\n\n**4. 正交规范性下的充分性和必要性**\n\n对于正交规范矩阵 $A$，目标函数 $F(x)$ 变得可分离。设 $c = A^\\top b$。\n$$\n\\lVert Ax - b \\rVert_2^2 = (Ax-b)^\\top(Ax-b) = x^\\top A^\\top Ax - 2x^\\top A^\\top b + b^\\top b = \\lVert x \\rVert_2^2 - 2x^\\top c + \\lVert b \\rVert_2^2.\n$$\n通过配方法，得到 $\\lVert x-c \\rVert_2^2 - \\lVert c \\rVert_2^2 + \\lVert b \\rVert_2^2$。最小化 $F(x)$ 等价于最小化 $\\lVert x-c \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$。这可以分解为 $n$ 个独立的标量问题：\n$$\n\\min_{x_k} (x_k - c_k)^2 + \\lambda I(x_k \\ne 0) \\quad \\text{对于 } k=0, 1, \\dots, n-1.\n$$\n对于每个 $k$，我们有两个选择：\n1. $x_k = 0$：成本为 $c_k^2$。\n2. $x_k \\ne 0$：为了最小化 $(x_k - c_k)^2$，我们必须选择 $x_k = c_k$。成本为 $\\lambda$。\n\n如果 $\\lambda  c_k^2$，最优选择是 $x_k=c_k$；如果 $\\lambda > c_k^2$，最优选择是 $x_k=0$。如果 $\\lambda=c_k^2$，两种选择得到相同的目标值。一个常见的约定是倾向于更稀疏的解，所以 $x_k=0$。\n最优支撑集是 $S^* = \\{k \\mid c_k^2 > \\lambda\\} = \\{k \\mid (a_k^\\top b)^2 > \\lambda\\}$。\n\n让我们根据这个基准真相来检查给定支撑集 $S$ 的认证条件：\n- **扩充条件**：$\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$。当 $A$ 是正交规范时，$r_S = b - A_S A_S^\\top b$。那么 $a_j^\\top r_S = a_j^\\top b - a_j^\\top A_S (A_S^\\top b) = a_j^\\top b$ 因为 $a_j^\\top A_S = 0$。所以条件是 $(a_j^\\top b)^2 \\le \\lambda$。这恰好是索引 $j \\notin S$ 被正确地从最优支撑集中排除的条件。\n- **剪除条件**：$\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$。当 $A$ 是正交规范时，$x_S^\\star = A_S^\\top b$，所以 $(x_S^\\star)_i = a_i^\\top b$。条件是 $(a_i^\\top b)^2 \\ge \\lambda$。这恰好是索引 $i \\in S$ 被正确地包含在最优支撑集中的条件。\n\n因此，这些条件对于支撑集 $S$ 的全局最优性是充分且必要的，当且仅当 $A$ 具有正交规范列。如果 $A$ 不是正交规范的，这些简化的测试不足以保证全局最优性。\n\n**算法设计**\n\n程序将为给定的测试用例 $(A, b, \\lambda, S)$ 实现以下步骤：\n1. **正交规范性检查**：验证 $A^\\top A$ 是否接近单位矩阵 $I_n$。如果不是，认证无效，结果为 `False`。\n2. **设置**：如果矩阵是正交规范的，使用 $A_S$ 和 $b$ 上的最小二乘法计算 $x_S^\\star$，然后计算残差 $r_S = b - A_S x_S^\\star$。\n3. **扩充测试**：对于每个不在 $S$ 中的索引 $j$，计算 $\\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2$ 并检查其是否小于或等于 $\\lambda$。如果对任何 $j$ 该条件不成立，结果为 `False`。\n4. **剪除测试**：对于 $S$ 中的每个索引 $i$，找到相应的系数 $(x_S^\\star)_k$。计算 $\\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_k)^2$ 并检查其是否大于或等于 $\\lambda$。如果对任何 $i$ 该条件不成立，结果为 `False`。\n5. **认证**：如果所有测试都通过，则支撑集 $S$ 被认证为全局最优，结果为 `True`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by evaluating the global optimality certificate for three test cases.\n    \"\"\"\n\n    def certify_global_optimality(A, b, lambda_reg, S):\n        \"\"\"\n        Certifies if a support set S is globally optimal for the l0-penalized\n        least squares problem, under the condition that matrix A has orthonormal columns.\n        \"\"\"\n        m, n = A.shape\n\n        # Step 4: Orthonormal-column prerequisite for global certification.\n        # The certificate is only sufficient if A has orthonormal columns.\n        if not np.allclose(A.T @ A, np.eye(n)):\n            return False\n\n        S_list = sorted(list(S))\n        off_support_indices = sorted(list(set(range(n)) - set(S)))\n\n        # Step 1: Restricted regression to compute x_S_star and r_S.\n        if not S_list:  # Support S is empty\n            x_S_star = np.array([])\n            r_S = b\n        else:\n            A_S = A[:, S_list]\n            # For orthonormal A, x_S_star = A_S.T @ b, but lstsq is more general and robust.\n            x_S_star = np.linalg.lstsq(A_S, b, rcond=None)[0]\n            r_S = b - A_S @ x_S_star\n\n        # Step 2: Off-support augmentation inequalities.\n        # Check if Delta_add(j|S) = lambda for all j not in S.\n        for j in off_support_indices:\n            a_j = A[:, j]\n            # For orthonormal A, Delta_add(j|S) = (a_j^T * r_S)^2\n            delta_add = (a_j.T @ r_S)**2\n            if delta_add > lambda_reg:\n                return False\n\n        # Step 3: On-support pruning inequalities.\n        # Check if Delta_drop(i|S) >= lambda for all i in S.\n        # x_S_star contains coefficients ordered according to S_list.\n        for coeff in x_S_star:\n            # For orthonormal A, Delta_drop(i|S) = (coefficient for a_i)^2\n            delta_drop = coeff**2\n            if delta_drop  lambda_reg:\n                return False\n\n        # If all checks pass, the support S is certified globally optimal.\n        return True\n\n    # Test case 1 (Happy path, orthonormal columns)\n    test_case_1 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([2.0, -1.5, 0.2, 0.0, 3.0, -2.0]),\n        0.5,\n        {0, 1}\n    )\n\n    # Test case 2 (Boundary equalities, orthonormal columns)\n    test_case_2 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.0, 1.1, 0.9, 0.0, -0.5, 0.3]),\n        1.0,\n        {0, 1}\n    )\n\n    # Test case 3 (Non-orthonormal design)\n    test_case_3 = (\n        np.array([\n            [1, 1, 0, 0],\n            [0, 1, 1, 0],\n            [0, 0, 1, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.5, -0.5, 2.0, -1.0, 0.0, 0.0]),\n        0.5,\n        {0, 2}\n    )\n    \n    test_cases = [test_case_1, test_case_2, test_case_3]\n    \n    results = []\n    for case in test_cases:\n        A, b, lambda_reg, S = case\n        result = certify_global_optimality(A, b, lambda_reg, S)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n```"
        }
    ]
}