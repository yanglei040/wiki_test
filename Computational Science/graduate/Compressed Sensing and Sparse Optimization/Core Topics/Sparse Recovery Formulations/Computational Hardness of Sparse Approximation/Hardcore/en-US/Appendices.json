{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in sparse approximation is that directly verifying solution uniqueness is often NP-hard. This exercise introduces mutual coherence, $\\mu(A)$, as a computationally tractable alternative for analyzing the properties of a sensing matrix $A$. By working through a direct calculation, you will gain hands-on familiarity with this key metric, which provides a valuable, though sometimes conservative, tool for establishing performance guarantees in sparse recovery.",
            "id": "3437357",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns $a_{1}, a_{2}, \\dots, a_{n}$. In compressed sensing and sparse optimization, one route to certifying uniqueness of a sparsest solution relies on quantities that are computationally tractable. The mutual coherence of $A$ is one such tractable quantity. Work from the core definitions of inner product and Euclidean norm: for vectors $x, y \\in \\mathbb{R}^{m}$, the inner product is defined by $\\langle x, y \\rangle = x^{\\top} y$, and the Euclidean norm is defined by $\\|x\\|_{2} = \\sqrt{\\langle x, x \\rangle}$. For each column $a_{j}$, define the normalized column $\\tilde{a}_{j} = a_{j} / \\|a_{j}\\|_{2}$. The mutual coherence is then defined as\n$$\n\\mu(A) = \\max_{1 \\leq i \\neq j \\leq n} \\left| \\langle \\tilde{a}_{i}, \\tilde{a}_{j} \\rangle \\right|.\n$$\nThis quantity is efficiently computable, in contrast to other sparsity-related invariants whose exact computation is known to be computationally intractable in general, such as the spark, which is tied to Non-deterministic Polynomial time (NP)-hard decision problems.\n\nLet\n$$\nA = \\begin{pmatrix}\n2  1  0  1 \\\\\n0  1  1  -1 \\\\\n2  0  1  1\n\\end{pmatrix}.\n$$\nUsing only the above fundamental definitions and the given $A$, compute the mutual coherence $\\mu(A)$ exactly. Do not round; provide a closed-form expression.",
            "solution": "The problem is valid as it is a well-defined mathematical problem grounded in the principles of linear algebra, with all necessary information provided and no internal contradictions. It requests the exact computation of the mutual coherence, $\\mu(A)$, for a given matrix $A$.\n\nThe provided matrix is\n$$\nA = \\begin{pmatrix}\n2  1  0  1 \\\\\n0  1  1  -1 \\\\\n2  0  1  1\n\\end{pmatrix}.\n$$\nThe columns of the matrix $A$ are denoted as $a_{1}, a_{2}, a_{3}, a_{4}$:\n$$\na_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 2 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\end{pmatrix}.\n$$\nThe mutual coherence $\\mu(A)$ is defined as $\\mu(A) = \\max_{1 \\leq i \\neq j \\leq n} |\\langle \\tilde{a}_{i}, \\tilde{a}_{j} \\rangle|$, where $\\tilde{a}_{j} = a_{j} / \\|a_{j}\\|_{2}$. The term $\\langle \\tilde{a}_{i}, \\tilde{a}_{j} \\rangle$ can be written as $\\frac{\\langle a_{i}, a_{j} \\rangle}{\\|a_{i}\\|_{2} \\|a_{j}\\|_{2}}$.\n\nThe computation proceeds in three steps:\n1. Calculate the Euclidean norm $\\|a_{j}\\|_{2}$ for each column $a_{j}$.\n2. Calculate the inner products $\\langle a_{i}, a_{j} \\rangle$ for all pairs with $i \\neq j$.\n3. Compute the absolute values of the normalized inner products and find their maximum.\n\nStep 1: Calculate the Euclidean norms.\nThe Euclidean norm is defined as $\\|x\\|_{2} = \\sqrt{\\langle x, x \\rangle} = \\sqrt{x^{\\top}x}$.\nFor $a_{1}$:\n$$\n\\|a_{1}\\|_{2} = \\sqrt{2^{2} + 0^{2} + 2^{2}} = \\sqrt{4 + 0 + 4} = \\sqrt{8} = 2\\sqrt{2}.\n$$\nFor $a_{2}$:\n$$\n\\|a_{2}\\|_{2} = \\sqrt{1^{2} + 1^{2} + 0^{2}} = \\sqrt{1 + 1 + 0} = \\sqrt{2}.\n$$\nFor $a_{3}$:\n$$\n\\|a_{3}\\|_{2} = \\sqrt{0^{2} + 1^{2} + 1^{2}} = \\sqrt{0 + 1 + 1} = \\sqrt{2}.\n$$\nFor $a_{4}$:\n$$\n\\|a_{4}\\|_{2} = \\sqrt{1^{2} + (-1)^{2} + 1^{2}} = \\sqrt{1 + 1 + 1} = \\sqrt{3}.\n$$\n\nStep 2: Calculate the pairwise inner products $\\langle a_{i}, a_{j} \\rangle$ for $i  j$.\nThe inner product is defined as $\\langle x, y \\rangle = x^{\\top}y$.\n$$\n\\langle a_{1}, a_{2} \\rangle = (2)(1) + (0)(1) + (2)(0) = 2.\n$$\n$$\n\\langle a_{1}, a_{3} \\rangle = (2)(0) + (0)(1) + (2)(1) = 2.\n$$\n$$\n\\langle a_{1}, a_{4} \\rangle = (2)(1) + (0)(-1) + (2)(1) = 2 + 0 + 2 = 4.\n$$\n$$\n\\langle a_{2}, a_{3} \\rangle = (1)(0) + (1)(1) + (0)(1) = 1.\n$$\n$$\n\\langle a_{2}, a_{4} \\rangle = (1)(1) + (1)(-1) + (0)(1) = 1 - 1 + 0 = 0.\n$$\n$$\n\\langle a_{3}, a_{4} \\rangle = (0)(1) + (1)(-1) + (1)(1) = 0 - 1 + 1 = 0.\n$$\n\nStep 3: Compute $|\\langle \\tilde{a}_{i}, \\tilde{a}_{j} \\rangle|$ and find the maximum.\nFor each pair $(i, j)$ with $i \\neq j$, we compute $|\\langle \\tilde{a}_{i}, \\tilde{a}_{j} \\rangle| = \\frac{|\\langle a_{i}, a_{j} \\rangle|}{\\|a_{i}\\|_{2} \\|a_{j}\\|_{2}}$. Since the absolute value function is symmetric, we only need to compute for $i  j$.\n$$\n|\\langle \\tilde{a}_{1}, \\tilde{a}_{2} \\rangle| = \\frac{|\\langle a_{1}, a_{2} \\rangle|}{\\|a_{1}\\|_{2} \\|a_{2}\\|_{2}} = \\frac{|2|}{(2\\sqrt{2})(\\sqrt{2})} = \\frac{2}{4} = \\frac{1}{2}.\n$$\n$$\n|\\langle \\tilde{a}_{1}, \\tilde{a}_{3} \\rangle| = \\frac{|\\langle a_{1}, a_{3} \\rangle|}{\\|a_{1}\\|_{2} \\|a_{3}\\|_{2}} = \\frac{|2|}{(2\\sqrt{2})(\\sqrt{2})} = \\frac{2}{4} = \\frac{1}{2}.\n$$\n$$\n|\\langle \\tilde{a}_{1}, \\tilde{a}_{4} \\rangle| = \\frac{|\\langle a_{1}, a_{4} \\rangle|}{\\|a_{1}\\|_{2} \\|a_{4}\\|_{2}} = \\frac{|4|}{(2\\sqrt{2})(\\sqrt{3})} = \\frac{4}{2\\sqrt{6}} = \\frac{2}{\\sqrt{6}} = \\frac{2\\sqrt{6}}{6} = \\frac{\\sqrt{6}}{3}.\n$$\n$$\n|\\langle \\tilde{a}_{2}, \\tilde{a}_{3} \\rangle| = \\frac{|\\langle a_{2}, a_{3} \\rangle|}{\\|a_{2}\\|_{2} \\|a_{3}\\|_{2}} = \\frac{|1|}{(\\sqrt{2})(\\sqrt{2})} = \\frac{1}{2}.\n$$\n$$\n|\\langle \\tilde{a}_{2}, \\tilde{a}_{4} \\rangle| = \\frac{|\\langle a_{2}, a_{4} \\rangle|}{\\|a_{2}\\|_{2} \\|a_{4}\\|_{2}} = \\frac{|0|}{(\\sqrt{2})(\\sqrt{3})} = 0.\n$$\n$$\n|\\langle \\tilde{a}_{3}, \\tilde{a}_{4} \\rangle| = \\frac{|\\langle a_{3}, a_{4} \\rangle|}{\\|a_{3}\\|_{2} \\|a_{4}\\|_{2}} = \\frac{|0|}{(\\sqrt{2})(\\sqrt{3})} = 0.\n$$\nThe set of values for the absolute normalized inner products is $\\{ \\frac{1}{2}, \\frac{1}{2}, \\frac{\\sqrt{6}}{3}, \\frac{1}{2}, 0, 0 \\}$.\nThe mutual coherence $\\mu(A)$ is the maximum of these values:\n$$\n\\mu(A) = \\max \\left\\{ \\frac{1}{2}, \\frac{\\sqrt{6}}{3}, 0 \\right\\}.\n$$\nTo determine the maximum, we compare $\\frac{1}{2}$ and $\\frac{\\sqrt{6}}{3}$. We can compare their squares:\n$$\n\\left(\\frac{1}{2}\\right)^{2} = \\frac{1}{4} \\quad \\text{and} \\quad \\left(\\frac{\\sqrt{6}}{3}\\right)^{2} = \\frac{6}{9} = \\frac{2}{3}.\n$$\nSince $\\frac{2}{3}  \\frac{1}{4}$, it follows that $\\frac{\\sqrt{6}}{3}  \\frac{1}{2}$.\nTherefore, the maximum value is $\\frac{\\sqrt{6}}{3}$.",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{6}}{3}}\n$$"
        },
        {
            "introduction": "The computational hardness of the $\\ell_0$-minimization problem motivates the use of its convex surrogate, $\\ell_1$-minimization, a technique known as Basis Pursuit. However, this relaxation is not always equivalent, and understanding its failure modes is critical. This practice asks you to construct a concrete counterexample where Basis Pursuit fails and to connect this failure to a violation of a recovery condition based on the mutual coherence you explored in .",
            "id": "3437359",
            "problem": "Consider the linear system with sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$ whose columns are unit-norm vectors\n$$\na_1 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad a_2 = \\begin{pmatrix}\\frac{3}{5} \\\\ \\frac{4}{5}\\end{pmatrix}, \\quad a_3 = \\begin{pmatrix}\\frac{3}{5} \\\\ -\\frac{4}{5}\\end{pmatrix},\n$$\nand the measurement vector $y \\in \\mathbb{R}^2$ defined by $y = A x^{\\star}$ with\n$$\nx^{\\star} = \\begin{pmatrix}1 \\\\ -1 \\\\ 0\\end{pmatrix}.\n$$\nTasks:\n- Using only fundamental definitions of the $\\ell_0$ and $\\ell_1$ minimization problems, verify explicitly that $y$ admits the $2$-sparse representation $x^{\\star}$ and construct a different feasible vector $\\tilde{x}$ with $A \\tilde{x} = y$ whose $\\ell_1$ norm is strictly smaller than $\\|x^{\\star}\\|_1$. Conclude that $\\ell_1$ minimization (Basis Pursuit) fails to recover the sparsest representation $x^{\\star}$ for this instance.\n- Compute the mutual coherence $\\mu(A)$ of the matrix $A$, defined by\n$$\n\\mu(A) \\triangleq \\max_{i \\neq j} \\left| a_i^{\\top} a_j \\right|,\n$$\nand let $k$ denote the sparsity of $x^{\\star}$. Starting from the Exact Recovery Condition for Basis Pursuit (BP) and standard matrix norm bounds, derive the mutual-coherence-based sufficient recovery condition for all $k$-sparse representations and compute the corresponding mutual coherence threshold for this instance. Express the final threshold value as a reduced fraction. Do not round.",
            "solution": "The user has presented a two-part problem concerning sparse representations and their recovery via $\\ell_1$ minimization. The analysis will proceed in two stages as requested.\n\nFirst, we address the task of verifying the failure of Basis Pursuit for the given instance. The sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$ is given by its columns:\n$$\na_1 = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad a_2 = \\begin{pmatrix}\\frac{3}{5} \\\\ \\frac{4}{5}\\end{pmatrix}, \\quad a_3 = \\begin{pmatrix}\\frac{3}{5} \\\\ -\\frac{4}{5}\\end{pmatrix}.\n$$\nThus, the matrix $A$ is\n$$\nA = \\begin{pmatrix} 1  \\frac{3}{5}  \\frac{3}{5} \\\\ 0  \\frac{4}{5}  -\\frac{4}{5} \\end{pmatrix}.\n$$\nThe ground-truth vector is $x^{\\star} = \\begin{pmatrix}1 \\\\ -1 \\\\ 0\\end{pmatrix}$. The measurement vector $y$ is computed as $y = Ax^{\\star}$:\n$$\ny = \\begin{pmatrix} 1  \\frac{3}{5}  \\frac{3}{5} \\\\ 0  \\frac{4}{5}  -\\frac{4}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = 1 \\cdot a_1 - 1 \\cdot a_2 + 0 \\cdot a_3 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{5} \\\\ \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{3}{5} \\\\ 0 - \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{4}{5} \\end{pmatrix}.\n$$\nThe sparsity of a vector is its $\\ell_0$ pseudo-norm, $\\|x\\|_0$, which counts the number of non-zero entries. For $x^{\\star}$, the non-zero entries are at indices $1$ and $2$. Thus, $\\|x^{\\star}\\|_0 = 2$. Since $A x^{\\star} = y$ and $\\|x^{\\star}\\|_0 = 2$, $x^{\\star}$ is indeed a $2$-sparse representation of $y$.\n\nThe $\\ell_1$ norm of $x^{\\star}$ is the sum of the absolute values of its components:\n$$\n\\|x^{\\star}\\|_1 = |1| + |-1| + |0| = 2.\n$$\nTo show that Basis Pursuit (the $\\ell_1$ minimization problem $\\min_x \\|x\\|_1$ subject to $Ax=y$) fails to recover $x^{\\star}$, we must find another vector $\\tilde{x} \\in \\mathbb{R}^3$ such that $A\\tilde{x} = y$ and $\\|\\tilde{x}\\|_1  \\|x^{\\star}\\|_1$.\nAny other solution to $Ax=y$ can be expressed as $\\tilde{x} = x^{\\star} + h$, where $h$ is a non-zero vector in the null space of $A$, $\\mathcal{N}(A)$. We find $\\mathcal{N}(A)$ by solving $Ah=0$:\n$$\n\\begin{pmatrix} 1  \\frac{3}{5}  \\frac{3}{5} \\\\ 0  \\frac{4}{5}  -\\frac{4}{5} \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe second equation, $\\frac{4}{5}h_2 - \\frac{4}{5}h_3 = 0$, implies $h_2 = h_3$. Substituting this into the first equation gives $h_1 + \\frac{3}{5}h_2 + \\frac{3}{5}h_2 = 0$, which simplifies to $h_1 + \\frac{6}{5}h_2 = 0$, or $h_1 = -\\frac{6}{5}h_2$.\nThe null space vectors are of the form $h = c \\begin{pmatrix} -\\frac{6}{5} \\\\ 1 \\\\ 1 \\end{pmatrix}$ for any scalar $c \\in \\mathbb{R}$.\nWe seek a constant $c \\neq 0$ such that for $\\tilde{x} = x^{\\star} + h = \\begin{pmatrix} 1 - \\frac{6}{5}c \\\\ -1 + c \\\\ c \\end{pmatrix}$, we have $\\|\\tilde{x}\\|_1  2$.\nLet's test a value of $c$ that simplifies the expression by making one component zero. For instance, setting the first component to zero gives $1 - \\frac{6}{5}c = 0$, so $c = \\frac{5}{6}$.\nFor $c = \\frac{5}{6}$, the vector $\\tilde{x}$ becomes:\n$$\n\\tilde{x} = \\begin{pmatrix} 1 - \\frac{6}{5}\\left(\\frac{5}{6}\\right) \\\\ -1 + \\frac{5}{6} \\\\ \\frac{5}{6} \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ -\\frac{1}{6} \\\\ \\frac{5}{6} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{6} \\\\ \\frac{5}{6} \\end{pmatrix}.\n$$\nWe verify that $A\\tilde{x} = y$:\n$$\nA\\tilde{x} = \\begin{pmatrix} 1  \\frac{3}{5}  \\frac{3}{5} \\\\ 0  \\frac{4}{5}  -\\frac{4}{5} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{6} \\\\ \\frac{5}{6} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5}(-\\frac{1}{6}) + \\frac{3}{5}(\\frac{5}{6}) \\\\ \\frac{4}{5}(-\\frac{1}{6}) - \\frac{4}{5}(\\frac{5}{6}) \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5}(\\frac{4}{6}) \\\\ \\frac{4}{5}(-\\frac{6}{6}) \\end{pmatrix} = \\begin{pmatrix} \\frac{12}{30} \\\\ -\\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{4}{5} \\end{pmatrix}.\n$$\nThis is equal to $y$, so $\\tilde{x}$ is a feasible solution. Now we compute its $\\ell_1$ norm:\n$$\n\\|\\tilde{x}\\|_1 = |0| + |-\\frac{1}{6}| + |\\frac{5}{6}| = \\frac{1}{6} + \\frac{5}{6} = 1.\n$$\nSince $\\|\\tilde{x}\\|_1 = 1  2 = \\|x^{\\star}\\|_1$, we have constructed a feasible vector $\\tilde{x}$ with a strictly smaller $\\ell_1$ norm than $x^{\\star}$. Therefore, $x^{\\star}$ is not the solution to the Basis Pursuit problem, and we conclude that $\\ell_1$ minimization fails to recover the specific sparsest representation $x^{\\star}$ in this case.\n\nNext, we address the second part of the problem. We compute the mutual coherence $\\mu(A)$, defined as $\\mu(A) \\triangleq \\max_{i \\neq j} | a_i^{\\top} a_j |$. The columns $a_i$ are unit-norm.\n$$\n|a_1^{\\top} a_2| = \\left| \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}\\frac{3}{5} \\\\ \\frac{4}{5}\\end{pmatrix} \\right| = \\left|\\frac{3}{5}\\right| = \\frac{3}{5}.\n$$\n$$\n|a_1^{\\top} a_3| = \\left| \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}\\frac{3}{5} \\\\ -\\frac{4}{5}\\end{pmatrix} \\right| = \\left|\\frac{3}{5}\\right| = \\frac{3}{5}.\n$$\n$$\n|a_2^{\\top} a_3| = \\left| \\begin{pmatrix}\\frac{3}{5}  \\frac{4}{5}\\end{pmatrix} \\begin{pmatrix}\\frac{3}{5} \\\\ -\\frac{4}{5}\\end{pmatrix} \\right| = \\left|\\left(\\frac{3}{5}\\right)^2 - \\left(\\frac{4}{5}\\right)^2\\right| = \\left|\\frac{9}{25} - \\frac{16}{25}\\right| = \\left|-\\frac{7}{25}\\right| = \\frac{7}{25}.\n$$\nThe mutual coherence is the maximum of these values:\n$$\n\\mu(A) = \\max\\left(\\frac{3}{5}, \\frac{3}{5}, \\frac{7}{25}\\right) = \\frac{3}{5}.\n$$\nThe sparsity of $x^{\\star}$ is $k = \\|x^{\\star}\\|_0 = 2$. We now derive the sufficient condition for exact recovery of any $k$-sparse vector. The Exact Recovery Condition (ERC) states that a $k$-sparse vector $x_0$ with support $S = \\text{supp}(x_0)$ is the unique minimizer of $\\|x\\|_1$ subject to $Ax=Ax_0$ if and only if there exists a vector $\\lambda \\in \\mathbb{R}^m$ such that $A_S^{\\top}\\lambda = \\text{sgn}((x_0)_S)$ and $\\|A_{S^c}^{\\top}\\lambda\\|_{\\infty}  1$, where $A_S$ and $A_{S^c}$ are submatrices of $A$ with columns indexed by $S$ and its complement $S^c$, respectively.\nAssuming the columns in $A_S$ are linearly independent, the Gram matrix $G_S = A_S^\\top A_S$ is invertible. A solution for $\\lambda$ is given by $\\lambda = A_S(A_S^\\top A_S)^{-1}\\text{sgn}((x_0)_S)$. Substituting this into the second condition gives:\n$$\n\\|A_{S^c}^{\\top}A_S(A_S^\\top A_S)^{-1}\\text{sgn}((x_0)_S)\\|_{\\infty}  1.\n$$\nSince $\\|\\text{sgn}((x_0)_S)\\|_{\\infty}=1$, a sufficient condition can be derived using the submultiplicativity of induced matrix norms:\n$$\n\\|A_{S^c}^{\\top}A_S(A_S^\\top A_S)^{-1}\\|_{\\infty} \\le \\|A_{S^c}^{\\top}A_S\\|_{\\infty} \\|(A_S^\\top A_S)^{-1}\\|_{\\infty}  1.\n$$\nWe bound each term. The matrix $A_{S^c}^{\\top}A_S$ has entries $a_j^\\top a_i$ for $j \\in S^c, i \\in S$. The $\\infty$-norm (maximum absolute row sum) is bounded by:\n$$\n\\|A_{S^c}^{\\top}A_S\\|_{\\infty} = \\max_{j \\in S^c} \\sum_{i \\in S} |a_j^\\top a_i| \\le \\max_{j \\in S^c} \\sum_{i \\in S} \\mu(A) = k \\mu(A).\n$$\nThe Gram matrix $G_S=A_S^\\top A_S$ can be written as $I+K$, where $I$ is the $k \\times k$ identity and $K$ is the matrix of off-diagonal inner products. The $\\infty$-norm of $K$ is bounded by $\\|K\\|_{\\infty} \\le (k-1)\\mu(A)$. If $\\|K\\|_{\\infty}  1$, i.e., $(k-1)\\mu(A)1$, then $G_S$ is invertible and by Neumann series, $\\|(G_S)^{-1}\\|_{\\infty} \\le \\frac{1}{1-\\|K\\|_{\\infty}}$. A looser but sufficient bound is:\n$$\n\\|(A_S^\\top A_S)^{-1}\\|_{\\infty} \\le \\frac{1}{1-(k-1)\\mu(A)}.\n$$\nCombining these bounds, the sufficient condition for recovery becomes:\n$$\nk \\mu(A) \\cdot \\frac{1}{1-(k-1)\\mu(A)}  1.\n$$\nRearranging the inequality gives $k\\mu(A)  1-(k-1)\\mu(A)$, which simplifies to $(2k-1)\\mu(A)  1$, or $\\mu(A)  \\frac{1}{2k-1}$.\nThis condition guarantees the unique recovery of any $k$-sparse vector.\nThe mutual coherence threshold is the value $\\frac{1}{2k-1}$. For the given instance, the sparsity is $k=2$. The threshold is:\n$$\n\\frac{1}{2k-1} = \\frac{1}{2(2)-1} = \\frac{1}{3}.\n$$\nThis is the required mutual coherence threshold. For recovery to be guaranteed for any $2$-sparse vector, we would need $\\mu(A)  1/3$. In our case, $\\mu(A) = 3/5$, which violates this condition ($3/5  1/3$). This is consistent with our finding that for the specific $2$-sparse vector $x^{\\star}$, recovery failed.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        },
        {
            "introduction": "In contrast to the convex relaxation methods examined in , greedy algorithms like Orthogonal Matching Pursuit (OMP) offer a different computationally efficient path to sparse solutions. This exercise demonstrates that even algorithms with strong theoretical backing can have vulnerabilities by constructing an adversarial instance where OMP fails. Quantifying the precise failure point will deepen your understanding of algorithmic performance limits and the subtle ways a problem's structure can mislead a greedy search.",
            "id": "3437368",
            "problem": "Consider a matrix $A \\in \\mathbb{R}^{(k+1) \\times (k+1)}$ with $k \\geq 2$ constructed as follows. Let $\\{e_{i}\\}_{i=1}^{k+1}$ denote the standard orthonormal basis of $\\mathbb{R}^{k+1}$. Define the first $k$ columns of $A$ by $a_{i} = e_{i}$ for $i \\in \\{1,\\dots,k\\}$, and define the $(k+1)$-th column by\n$$\na_{k+1} \\;=\\; \\mu \\sum_{i=1}^{k} e_{i} \\;+\\; \\sqrt{1 - k \\mu^{2}} \\, e_{k+1},\n$$\nwhere $\\mu \\in (0,1/\\sqrt{k})$ is a scalar. All columns of $A$ are unit norm. Let the signal $x \\in \\mathbb{R}^{k+1}$ be supported on the first $k$ entries with equal nonzero entries $x_{i} = \\alpha  0$ for $i \\in \\{1,\\dots,k\\}$ and $x_{k+1} = 0$. Let the observation be $b = A x = \\alpha \\sum_{i=1}^{k} e_{i}$.\n\nThe mutual coherence of $A$, denoted $\\mu(A)$, is defined as the maximum absolute inner product between distinct unit-norm columns of $A$:\n$$\n\\mu(A) \\;=\\; \\max_{i \\neq j} \\, \\big| \\langle a_{i}, a_{j} \\rangle \\big|.\n$$\nOrthogonal Matching Pursuit (OMP) is a greedy algorithm that, at the first iteration, selects the column index $j$ maximizing the absolute inner product $|\\langle a_{j}, b \\rangle|$.\n\nUsing only the above definitions as the fundamental base, construct the argument that this $(A,b)$ is an adversarial instance with small mutual coherence where Orthogonal Matching Pursuit fails to identify any correct support index at its first iteration by selecting the index $k+1$. Then, quantify this failure mechanism by determining, as an exact closed-form expression in $k$, the smallest value of $\\mu$ for which this first-iteration failure occurs. Your final answer must be a single closed-form analytic expression in $k$. No rounding is required. Express your final answer without units.",
            "solution": "The problem statement is critically evaluated and found to be valid. It is scientifically grounded within the mathematical field of sparse approximation, well-posed, objective, and internally consistent. We may therefore proceed with a full solution.\n\nThe problem requires us to determine the condition under which the Orthogonal Matching Pursuit (OMP) algorithm fails at its first iteration for a specific problem instance. Failure is defined as selecting an index not in the true support of the signal $x$. The true support of $x$ is the set $\\{1, 2, \\dots, k\\}$. Thus, failure at the first step occurs if OMP selects the index $k+1$.\n\nFirst, we compute the observation vector $b$. Given the signal $x$ where $x_i = \\alpha$ for $i \\in \\{1, \\dots, k\\}$ and $x_{k+1} = 0$, and the matrix $A$ with columns $a_i$, the observation $b$ is given by:\n$$\nb = Ax = \\sum_{j=1}^{k+1} a_j x_j = \\sum_{j=1}^{k} a_j x_j + a_{k+1} x_{k+1} = \\sum_{j=1}^{k} a_j (\\alpha) + a_{k+1} (0)\n$$\nUsing the definition $a_i = e_i$ for $i \\in \\{1, \\dots, k\\}$, we have:\n$$\nb = \\alpha \\sum_{j=1}^{k} e_j\n$$\nThis confirms the expression for $b$ provided in the problem statement.\n\nThe first iteration of OMP consists of finding the column index $j$ that maximizes the absolute value of the inner product with the observation vector $b$. That is, OMP selects the index $j^*$ such that:\n$$\nj^* = \\arg\\max_{j \\in \\{1, \\dots, k+1\\}} |\\langle a_j, b \\rangle|\n$$\nTo analyze the algorithm's behavior, we must compute these inner products for all possible indices $j$.\n\nLet us first consider the \"correct\" indices, which are those in the support of $x$, i.e., $j \\in \\{1, \\dots, k\\}$. For any such index $j$, the column is $a_j = e_j$. The inner product is:\n$$\n\\langle a_j, b \\rangle = \\left\\langle e_j, \\alpha \\sum_{i=1}^{k} e_i \\right\\rangle = \\alpha \\sum_{i=1}^{k} \\langle e_j, e_i \\rangle\n$$\nSince $\\{e_i\\}$ is an orthonormal basis, the inner product $\\langle e_j, e_i \\rangle = \\delta_{ji}$, where $\\delta_{ji}$ is the Kronecker delta. The sum therefore collapses to a single non-zero term when $i=j$:\n$$\n\\langle a_j, b \\rangle = \\alpha \\cdot 1 = \\alpha\n$$\nThe absolute value is $|\\langle a_j, b \\rangle| = |\\alpha| = \\alpha$, since it is given that $\\alpha  0$. Thus, for all correct indices $j \\in \\{1, \\dots, k\\}$, the correlation is a constant value $\\alpha$.\n\nNext, we consider the \"incorrect\" index, $j = k+1$. The corresponding column is defined as:\n$$\na_{k+1} = \\mu \\sum_{i=1}^{k} e_i + \\sqrt{1 - k \\mu^2} \\, e_{k+1}\n$$\nThe inner product with $b$ is:\n$$\n\\langle a_{k+1}, b \\rangle = \\left\\langle \\mu \\sum_{i=1}^{k} e_i + \\sqrt{1 - k \\mu^2} \\, e_{k+1}, \\alpha \\sum_{l=1}^{k} e_l \\right\\rangle\n$$\nBy linearity of the inner product, we can separate this into two terms:\n$$\n\\langle a_{k+1}, b \\rangle = \\alpha \\mu \\left\\langle \\sum_{i=1}^{k} e_i, \\sum_{l=1}^{k} e_l \\right\\rangle + \\alpha \\sqrt{1 - k \\mu^2} \\left\\langle e_{k+1}, \\sum_{l=1}^{k} e_l \\right\\rangle\n$$\nThe second term is zero because $e_{k+1}$ is orthogonal to every $e_l$ for $l \\in \\{1, \\dots, k\\}$. We are left with the first term:\n$$\n\\langle a_{k+1}, b \\rangle = \\alpha \\mu \\sum_{i=1}^{k} \\sum_{l=1}^{k} \\langle e_i, e_l \\rangle = \\alpha \\mu \\sum_{i=1}^{k} 1 = \\alpha \\mu k\n$$\nThe absolute value is $|\\langle a_{k+1}, b \\rangle| = |\\alpha \\mu k|$. Since $\\alpha  0$, $k \\ge 2$, and $\\mu \\in (0, 1/\\sqrt{k})$ implies $\\mu  0$, this is simply $\\alpha \\mu k$.\n\nOMP fails at the first iteration by selecting the incorrect index $k+1$ if the correlation for this index is greater than or equal to the correlation for any of the correct indices. The condition for failure is therefore:\n$$\n|\\langle a_{k+1}, b \\rangle| \\ge \\max_{j \\in \\{1, \\dots, k\\}} |\\langle a_j, b \\rangle|\n$$\nSubstituting the values we computed:\n$$\n\\alpha k \\mu \\ge \\alpha\n$$\nSince $\\alpha  0$, we can divide both sides by $\\alpha$ without changing the inequality direction:\n$$\nk \\mu \\ge 1\n$$\nThis gives the condition on $\\mu$ for failure:\n$$\n\\mu \\ge \\frac{1}{k}\n$$\nThe problem asks for the smallest value of $\\mu$ for which this failure occurs. This corresponds to the lower bound of the derived inequality, which is:\n$$\n\\mu = \\frac{1}{k}\n$$\nWe must verify that this value is within the allowed range for $\\mu$, which is $\\mu \\in (0, 1/\\sqrt{k})$. Since $k \\ge 2$, we have $k  0$ and thus $1/k  0$. We must also check if $1/k  1/\\sqrt{k}$. This inequality is equivalent to $\\sqrt{k}  k$. Squaring both sides (which is valid as both are positive for $k \\ge 2$) gives $k  k^2$, or $1  k$, which is true for $k \\ge 2$. Therefore, the value $\\mu = 1/k$ is a valid parameter.\n\nFinally, we analyze the mutual coherence $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$.\nFor $1 \\le i,j \\le k$ with $i \\neq j$, we have $|\\langle a_i, a_j \\rangle| = |\\langle e_i, e_j \\rangle| = 0$.\nFor $i \\in \\{1, \\dots, k\\}$ and $j=k+1$, we have:\n$$\n|\\langle a_i, a_{k+1} \\rangle| = \\left|\\left\\langle e_i, \\mu \\sum_{l=1}^{k} e_l + \\sqrt{1 - k\\mu^2} e_{k+1} \\right\\rangle\\right| = |\\mu \\langle e_i, e_i \\rangle| = \\mu\n$$\nThus, the mutual coherence is $\\mu(A) = \\mu$. The failure occurs when $\\mu(A) \\ge 1/k$. For large $k$, $1/k$ is small, which demonstrates that OMP can fail even for systems with low mutual coherence, validating the problem's framing of this as an adversarial instance.\n\nThe smallest value of $\\mu$ for which failure occurs is $1/k$.",
            "answer": "$$\n\\boxed{\\frac{1}{k}}\n$$"
        }
    ]
}