{
    "hands_on_practices": [
        {
            "introduction": "To build a solid understanding of the relationship between the LASSO and the Dantzig selector, it is instructive to begin with an idealized scenario. This exercise explores the behavior of both estimators under an orthonormal design, where the columns of the design matrix $X$ are mutually orthogonal and have unit norm ($X^{\\top} X = I_{p}$). By deriving the closed-form solutions in this setting, you will discover that both methods simplify to the elegant and widely-used soft-thresholding operator, providing a foundational insight into their shared mechanics before we explore their differences in more complex situations. ",
            "id": "3435559",
            "problem": "Consider the linear model $y = X \\beta^{\\star} + w$ with $X \\in \\mathbb{R}^{n \\times p}$ whose columns are orthonormal, meaning $X^{\\top} X = I_{p}$. You are comparing two convex estimators of $\\beta^{\\star}$ in the compressed sensing and sparse optimization setting: the Least Absolute Shrinkage and Selection Operator (LASSO) estimator and the Dantzig selector. The LASSO estimator $\\hat{\\beta}_{\\mathrm{LASSO}}$ is defined as the minimizer of the objective\n$$\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},$$\nwhere $\\lambda > 0$ is a regularization parameter. The Dantzig selector $\\hat{\\beta}_{\\mathrm{DS}}$ is defined as the minimizer of $\\|\\beta\\|_{1}$ subject to the feasibility constraint\n$$\\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t,$$\nwhere $t > 0$ is a tuning parameter.\n\nStarting from the fundamental base of convex optimality conditions for the LASSO and feasibility geometry for the Dantzig selector, derive the closed-form expressions of $\\hat{\\beta}_{\\mathrm{LASSO}}$ and $\\hat{\\beta}_{\\mathrm{DS}}$ under the orthonormal design condition $X^{\\top} X = I_{p}$. Then, for the specific instance $p = 5$ with the sufficient statistics $X^{\\top} y = z$ given by\n$$z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix},$$\nand parameters $\\lambda = 0.9$ and $t = 0.7$, compute the squared $\\ell_{2}$ distance between the two estimators,\n$$\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}.$$\n\nRound your final numerical answer to four significant figures. Express your final answer with no units.",
            "solution": "The problem asks for the squared $\\ell_2$ distance between the LASSO and Dantzig selector estimators under the specific condition of an orthonormal design matrix. We begin by deriving the closed-form solutions for both estimators under this condition.\n\nLet the sufficient statistic be defined as $z = X^{\\top}y$. The orthonormal design condition is $X^{\\top}X = I_p$, where $I_p$ is the $p \\times p$ identity matrix.\n\nFirst, we analyze the LASSO estimator, $\\hat{\\beta}_{\\mathrm{LASSO}}$, which is the solution to the minimization problem:\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right) $$\nThe quadratic term in the objective function can be expanded:\n$$ \\|y - X \\beta\\|_{2}^{2} = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta $$\nUsing the condition $X^{\\top}X = I_p$ and the definition $z = X^{\\top}y$, this simplifies to:\n$$ \\|y - X \\beta\\|_{2}^{2} = y^{\\top}y - 2z^{\\top}\\beta + \\beta^{\\top}\\beta $$\nThe term $y^{\\top}y$ is constant with respect to $\\beta$ and can be dropped from the minimization. We can also add the constant term $\\frac{1}{2}z^{\\top}z$ without changing the minimizer. The objective function is thus equivalent to minimizing:\n$$ L(\\beta) = \\frac{1}{2}(\\beta^{\\top}\\beta - 2z^{\\top}\\beta + z^{\\top}z) + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\|\\beta - z\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} $$\nThis objective function is separable, meaning it can be written as a sum of functions of individual components of $\\beta$:\n$$ L(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\nTherefore, we can find the optimal $\\hat{\\beta}_{\\mathrm{LASSO}}$ by minimizing each component-wise term independently:\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\arg \\min_{\\beta_j \\in \\mathbb{R}} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\nThis is the proximal operator of the $\\ell_1$ norm, which yields the soft-thresholding function. The solution for each component is:\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - \\lambda, 0) $$\nThis can be denoted as $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{\\lambda}(z)$, where $S_{\\lambda}$ is the element-wise soft-thresholding operator with threshold $\\lambda$.\n\nNext, we analyze the Dantzig selector, $\\hat{\\beta}_{\\mathrm{DS}}$, which is the solution to the constrained optimization problem:\n$$ \\hat{\\beta}_{\\mathrm{DS}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t $$\nLet's simplify the constraint using the orthonormal design property $X^{\\top}X = I_p$ and the definition $z = X^{\\top}y$:\n$$ X^{\\top}(y - X \\beta) = X^{\\top}y - X^{\\top}X\\beta = z - \\beta $$\nThe constraint thus becomes $\\|z - \\beta\\|_{\\infty} \\leq t$. This is equivalent to a set of component-wise constraints:\n$$ |z_j - \\beta_j| \\leq t \\quad \\text{for all } j \\in \\{1, \\dots, p\\} $$\nEach inequality can be rewritten as $z_j - t \\leq \\beta_j \\leq z_j + t$. The optimization problem is now to minimize $\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_j|$ subject to these box constraints. Since both the objective and the constraints are separable, we can solve for each component independently:\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\arg \\min_{\\beta_j} |\\beta_j| \\quad \\text{subject to} \\quad z_j - t \\leq \\beta_j \\leq z_j + t $$\nTo find the minimum, we consider the location of the interval $[z_j - t, z_j + t]$ relative to $0$:\n1. If the interval contains $0$, i.e., $z_j - t \\leq 0 \\leq z_j + t$, which is equivalent to $|z_j| \\leq t$, the minimum of $|\\beta_j|$ is achieved at $\\hat{\\beta}_{j, \\mathrm{DS}} = 0$.\n2. If the interval is entirely positive, i.e., $z_j - t > 0$ or $z_j > t$, the point in the interval closest to $0$ is the left endpoint. Thus, $\\hat{\\beta}_{j, \\mathrm{DS}} = z_j - t$.\n3. If the interval is entirely negative, i.e., $z_j + t  0$ or $z_j  -t$, the point in the interval closest to $0$ is the right endpoint. Thus, $\\hat{\\beta}_{j, \\mathrm{DS}} = z_j + t$.\nThese three cases can be summarized by the same soft-thresholding function:\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - t, 0) $$\nThis can be denoted as $\\hat{\\beta}_{\\mathrm{DS}} = S_{t}(z)$.\n\nNow we use the given numerical values:\n$$ z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix}, \\quad \\lambda = 0.9, \\quad t = 0.7 $$\nWe compute the LASSO estimate $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{0.9}(z)$:\n$$ \\hat{\\beta}_{1, \\mathrm{LASSO}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.9, 0) = 1.5 $$\n$$ \\hat{\\beta}_{2, \\mathrm{LASSO}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.9, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{LASSO}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.9, 0) = 0.2 $$\n$$ \\hat{\\beta}_{4, \\mathrm{LASSO}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.9, 0) = -0.9 $$\n$$ \\hat{\\beta}_{5, \\mathrm{LASSO}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.9, 0) = 0 $$\nSo, $\\hat{\\beta}_{\\mathrm{LASSO}} = \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0.2 \\\\ -0.9 \\\\ 0 \\end{pmatrix}$.\n\nNext, we compute the Dantzig selector estimate $\\hat{\\beta}_{\\mathrm{DS}} = S_{0.7}(z)$:\n$$ \\hat{\\beta}_{1, \\mathrm{DS}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.7, 0) = 1.7 $$\n$$ \\hat{\\beta}_{2, \\mathrm{DS}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.7, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{DS}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.7, 0) = 0.4 $$\n$$ \\hat{\\beta}_{4, \\mathrm{DS}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.7, 0) = -1.1 $$\n$$ \\hat{\\beta}_{5, \\mathrm{DS}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.7, 0) = 0 $$\nSo, $\\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.7 \\\\ 0 \\\\ 0.4 \\\\ -1.1 \\\\ 0 \\end{pmatrix}$.\n\nFinally, we compute the squared $\\ell_2$ distance between the two estimators, $\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}$:\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.5 - 1.7 \\\\ 0 - 0 \\\\ 0.2 - 0.4 \\\\ -0.9 - (-1.1) \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ 0 \\\\ -0.2 \\\\ 0.2 \\\\ 0 \\end{pmatrix} $$\nThe squared $\\ell_2$ norm is the sum of the squares of the components of this difference vector:\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = (-0.2)^2 + 0^2 + (-0.2)^2 + (0.2)^2 + 0^2 $$\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = 0.04 + 0 + 0.04 + 0.04 + 0 = 0.12 $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value is $0.12$, which can be written as $0.1200$ to reflect this precision.",
            "answer": "$$\\boxed{0.1200}$$"
        },
        {
            "introduction": "While the orthonormal case reveals a surprising similarity, the true distinctions between the LASSO and the Dantzig selector lie in their geometric formulations. This practice problem moves beyond the ideal and uses a simple two-dimensional example to build sharp geometric intuition. You will see firsthand how minimizing the squared error over an $\\ell_1$-norm ball (LASSO) versus minimizing the $\\ell_1$-norm within an $\\ell_\\infty$-norm constraint on correlations (Dantzig selector) leads to different solutions, even when the solution of one is feasible for the other. ",
            "id": "3435555",
            "problem": "Consider a linear inverse problem in finite dimensions with design matrix $A \\in \\mathbb{R}^{n \\times p}$ and observation vector $y \\in \\mathbb{R}^{n}$. The Least Absolute Shrinkage and Selection Operator (LASSO) is defined here in its constrained form as the solution set of the convex program\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ \\ \\text{subject to} \\ \\ \\|x\\|_{1} \\leq t,\n$$\nfor a given budget $t > 0$. The Dantzig selector (DS) is defined as the solution set of the convex program\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\ \\|x\\|_{1} \\ \\ \\text{subject to} \\ \\ \\|A^{\\top}(A x - y)\\|_{\\infty} \\leq \\lambda,\n$$\nfor a given tolerance $\\lambda > 0$.\n\nWork in the regime $n = p = 2$ and take $A = I_{2}$, the $2 \\times 2$ identity matrix. Let $y = (3,1)^{\\top}$, $t = 2.5$, and $\\lambda = 0.8$. Starting only from the definitions above and standard convex analysis principles, do the following:\n\n- Derive the LASSO minimizer $x_{\\mathrm{L}}$ and the Dantzig selector minimizer $x_{\\mathrm{D}}$ for this instance, verifying explicitly that the LASSO solution set and the Dantzig selector feasible set intersect but yield different minimizers.\n- Provide a geometric explanation, in terms of the shapes of the level sets and feasible regions in $\\mathbb{R}^{2}$, for why the minimizers differ even though the LASSO minimizer is Dantzig-feasible.\n\nFinally, compute the Euclidean distance $\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2}$ and give your final answer as a simplified exact expression. Do not round your answer.",
            "solution": "The problem is specified for the case where $n = p = 2$, the design matrix is the identity $A = I_2$, the observation vector is $y = (3, 1)^{\\top}$, the LASSO budget is $t = 2.5$, and the Dantzig selector tolerance is $\\lambda = 0.8$.\n\n**1. Derivation of the LASSO Minimizer $x_{\\mathrm{L}}$**\n\nThe LASSO problem is\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ \\ \\text{subject to} \\ \\ \\|x\\|_{1} \\leq t\n$$\nSubstituting the given values, with $x = (x_1, x_2)^{\\top}$:\n$$\n\\min_{x_1, x_2} \\ \\frac{1}{2}\\|x - y\\|_{2}^{2} = \\frac{1}{2}((x_1 - 3)^2 + (x_2 - 1)^2) \\ \\ \\text{subject to} \\ \\ |x_1| + |x_2| \\leq 2.5\n$$\nThis problem is equivalent to finding the Euclidean projection of the point $y=(3,1)$ onto the $\\ell_1$-ball of radius $t=2.5$ centered at the origin.\nFirst, we check if the unconstrained minimizer, $x_{unc} = y = (3,1)$, is feasible. We compute its $\\ell_1$-norm: $\\|y\\|_1 = |3| + |1| = 4$. Since $4 > 2.5$, the unconstrained solution is not feasible, and the LASSO solution $x_{\\mathrm{L}}$ must lie on the boundary of the feasible region, i.e., $\\|x_{\\mathrm{L}}\\|_1 = 2.5$.\n\nSince $y$ is in the first quadrant, we anticipate that the solution $x_{\\mathrm{L}}$ will also be in the first quadrant, so $x_1 \\geq 0$ and $x_2 \\geq 0$. The constraint becomes $x_1 + x_2 = 2.5$. Geometrically, the solution is the point on the line segment $x_1 + x_2 = 2.5$ (with $x_1, x_2 \\ge 0$) that is closest to $y=(3,1)$. The vector from the solution $x_{\\mathrm{L}}$ to $y$ must be orthogonal to the line segment. The direction vector of the line $x_1+x_2=2.5$ is $(1, -1)$. Thus, the vector $y - x_{\\mathrm{L}} = (3-x_1, 1-x_2)$ must be orthogonal to $(1, -1)$.\nThe orthogonality condition is given by the dot product:\n$$\n(3-x_1, 1-x_2) \\cdot (1, -1) = 0\n$$\n$$\n(3-x_1) - (1-x_2) = 0 \\implies 2 - x_1 + x_2 = 0 \\implies x_1 - x_2 = 2\n$$\nWe now solve the system of linear equations:\n\\begin{enumerate}\n    \\item $x_1 + x_2 = 2.5$\n    \\item $x_1 - x_2 = 2$\n\\end{enumerate}\nAdding the two equations yields $2x_1 = 4.5$, so $x_1 = 2.25$.\nSubstituting $x_1$ into the first equation gives $2.25 + x_2 = 2.5$, so $x_2 = 0.25$.\nSince $x_1 = 2.25 > 0$ and $x_2 = 0.25 > 0$, our assumption was correct. The LASSO minimizer is $x_{\\mathrm{L}} = (2.25, 0.25)^{\\top}$.\n\n**2. Derivation of the Dantzig Selector Minimizer $x_{\\mathrm{D}}$**\n\nThe Dantzig selector (DS) problem is\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\ \\|x\\|_{1} \\ \\ \\text{subject to} \\ \\ \\|A^{\\top}(A x - y)\\|_{\\infty} \\leq \\lambda\n$$\nSubstituting the given values, $A=I_2$:\n$$\n\\min_{x_1, x_2} \\ |x_1| + |x_2| \\ \\ \\text{subject to} \\ \\ \\|x - y\\|_{\\infty} \\leq \\lambda\n$$\nThe constraint $\\|x-y\\|_{\\infty} \\leq 0.8$ expands to $\\max(|x_1 - 3|, |x_2 - 1|) \\leq 0.8$. This is equivalent to the system of inequalities:\n$$\n|x_1 - 3| \\leq 0.8 \\implies 3 - 0.8 \\leq x_1 \\leq 3 + 0.8 \\implies 2.2 \\leq x_1 \\leq 3.8\n$$\n$$\n|x_2 - 1| \\leq 0.8 \\implies 1 - 0.8 \\leq x_2 \\leq 1 + 0.8 \\implies 0.2 \\leq x_2 \\leq 1.8\n$$\nThe feasible region is the rectangle $[2.2, 3.8] \\times [0.2, 1.8]$. Since this entire region is in the first quadrant, the objective function simplifies to minimizing $x_1 + x_2$. This is a linear program. The minimum of a linear function over a convex polytope (our rectangle) must occur at a vertex. The vertices are $(2.2, 0.2)$, $(3.8, 0.2)$, $(2.2, 1.8)$, and $(3.8, 1.8)$. Evaluating the objective $x_1+x_2$ at each vertex:\n\\begin{itemize}\n    \\item At $(2.2, 0.2)$: $x_1 + x_2 = 2.2 + 0.2 = 2.4$\n    \\item At $(3.8, 0.2)$: $x_1 + x_2 = 3.8 + 0.2 = 4.0$\n    \\item At $(2.2, 1.8)$: $x_1 + x_2 = 2.2 + 1.8 = 4.0$\n    \\item At $(3.8, 1.8)$: $x_1 + x_2 = 3.8 + 1.8 = 5.6$\n\\end{itemize}\nThe minimum value is $2.4$, achieved at the vertex $(2.2, 0.2)$.\nThus, the Dantzig selector minimizer is $x_{\\mathrm{D}} = (2.2, 0.2)^{\\top}$.\n\n**3. Verification and Geometric Explanation**\n\nThe minimizers are $x_{\\mathrm{L}} = (2.25, 0.25)^{\\top}$ and $x_{\\mathrm{D}} = (2.2, 0.2)^{\\top}$, which are different.\nWe verify that the LASSO minimizer $x_{\\mathrm{L}}$ is feasible for the Dantzig selector problem. We check if $\\|x_{\\mathrm{L}} - y\\|_{\\infty} \\leq \\lambda$:\n$$\n\\|x_{\\mathrm{L}} - y\\|_{\\infty} = \\|(2.25 - 3, 0.25 - 1)\\|_{\\infty} = \\|(-0.75, -0.75)\\|_{\\infty} = \\max(|-0.75|, |-0.75|) = 0.75\n$$\nSince $0.75 \\leq 0.8$, $x_{\\mathrm{L}}$ is indeed in the Dantzig selector feasible set. This confirms that the LASSO solution set and the Dantzig selector feasible set intersect. However, $\\|x_{\\mathrm{L}}\\|_1 = 2.25 + 0.25 = 2.5$, while $\\|x_{\\mathrm{D}}\\|_1 = 2.2+0.2=2.4$. Since $\\|x_{\\mathrm{D}}\\|_1  \\|x_{\\mathrm{L}}\\|_1$, $x_{\\mathrm{L}}$ is not the Dantzig minimizer.\n\nGeometric Explanation:\n- The LASSO solution $x_{\\mathrm{L}}$ is the point on the $\\ell_1$-ball $\\|x\\|_1 \\leq 2.5$ (a diamond centered at the origin) that is closest in Euclidean distance to $y=(3,1)$. The level sets of the LASSO objective are circles centered at $y$. The solution is found by expanding a circle from $y$ until it first touches the diamond. This tangency point is the Euclidean projection of $y$ onto the diamond.\n- The Dantzig selector solution $x_{\\mathrm{D}}$ is the point in the $\\ell_{\\infty}$-ball $\\|x-y\\|_{\\infty} \\leq 0.8$ (a square centered at $y$) that has the minimum $\\ell_1$-norm. The level sets of the DS objective are diamonds $\\|x\\|_1=c$ centered at the origin. The solution is found by expanding a diamond from the origin until it first touches the feasible square.\n- The two procedures yield different results because they optimize different objectives over different feasible regions. LASSO minimizes $\\ell_2$ distance over an $\\ell_1$ constraint set, while the Dantzig selector minimizes $\\ell_1$ norm over an $\\ell_{\\infty}$ constraint set. The point $x_{\\mathrm{L}}=(2.25, 0.25)$ is closer to $y$ in Euclidean distance, but $x_{\\mathrm{D}}=(2.2, 0.2)$ is \"closer\" to the origin in the $\\ell_1$ metric. The different geometries of the objectives and constraints lead to different optimality conditions and thus different solutions.\n\n**4. Final Calculation**\n\nWe compute the Euclidean distance $\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2}$.\n$$\nx_{\\mathrm{L}} = (2.25, 0.25)^{\\top} = \\left(\\frac{9}{4}, \\frac{1}{4}\\right)^{\\top}\n$$\n$$\nx_{\\mathrm{D}} = (2.2, 0.2)^{\\top} = \\left(\\frac{11}{5}, \\frac{1}{5}\\right)^{\\top}\n$$\nThe difference vector is:\n$$\nx_{\\mathrm{L}} - x_{\\mathrm{D}} = \\left(\\frac{9}{4} - \\frac{11}{5}, \\frac{1}{4} - \\frac{1}{5}\\right) = \\left(\\frac{45 - 44}{20}, \\frac{5 - 4}{20}\\right) = \\left(\\frac{1}{20}, \\frac{1}{20}\\right)^{\\top}\n$$\nThe Euclidean distance is:\n$$\n\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2} = \\sqrt{\\left(\\frac{1}{20}\\right)^2 + \\left(\\frac{1}{20}\\right)^2} = \\sqrt{2 \\cdot \\left(\\frac{1}{20}\\right)^2} = \\sqrt{\\frac{2}{400}} = \\frac{\\sqrt{2}}{20}\n$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{2}}{20}}$$"
        },
        {
            "introduction": "The theoretical and geometric differences between estimators often have profound practical consequences, especially when dealing with imperfect data. This exercise investigates a critical challenge in regression analysis: perfect multicollinearity, where one predictor can be expressed as a linear combination of others. By working through this concrete example, you will observe how the distinct optimization structures of the LASSO and the Dantzig selector compel them to resolve this ambiguity in fundamentally different ways, leading to divergent variable selection outcomes. ",
            "id": "3435595",
            "problem": "Consider a linear regression model in compressed sensing with a design matrix $X \\in \\mathbb{R}^{2 \\times 3}$ and response vector $y \\in \\mathbb{R}^{2}$ given by\n$$\nX = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}, \n\\quad\ny = \\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n$$\nNote that the third column satisfies the exact linear relation $x_{3} = x_{1} + x_{2}$, so the design exhibits perfect collinearity. Define the Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\beta}^{\\text{L}} \\in \\mathbb{R}^{3}$ as the solution to\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{\\text{L}}\\|\\beta\\|_{1},\n$$\nand the Dantzig selector estimator $\\hat{\\beta}^{\\text{D}} \\in \\mathbb{R}^{3}$ as the solution to\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X\\beta)\\|_{\\infty} \\leq \\lambda_{\\text{D}}.\n$$\nTake the regularization levels to be $\\lambda_{\\text{L}} = 1$ for the LASSO and $\\lambda_{\\text{D}} = \\frac{3}{4}$ for the Dantzig selector. Starting from the core definitions above, compute both estimators exactly and explain how the perfect collinearity $x_{3} = x_{1} + x_{2}$ leads to different support recovery outcomes under these two procedures. Report the concatenated estimator vector as a single row matrix: first the LASSO coefficients in the order $(\\beta_{1}, \\beta_{2}, \\beta_{3})$, then the Dantzig selector coefficients in the same order. No rounding is required.",
            "solution": "### LASSO Estimator Calculation\nThe LASSO objective function is $L(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{\\text{L}}\\|\\beta\\|_{1}$. With $\\lambda_{\\text{L}} = 1$, this is:\n$$\nL(\\beta) = \\frac{1}{2}\\left\\| \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} \\right\\|_{2}^{2} + \\sum_{j=1}^{3} |\\beta_j|\n$$\n$$\nL(\\beta) = \\frac{1}{2} \\left[ (1 - \\beta_1 - \\beta_3)^2 + (-1 - \\beta_2 - \\beta_3)^2 \\right] + |\\beta_1| + |\\beta_2| + |\\beta_3|\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions state that at a minimum $\\hat{\\beta}$, the zero vector must be in the subgradient of the objective function: $0 \\in \\partial L(\\hat{\\beta})$. This gives the condition:\n$$\n-X^{\\top}(y - X\\hat{\\beta}) + \\lambda_{\\text{L}} s = 0, \\quad \\text{where } s \\in \\partial\\|\\hat{\\beta}\\|_{1}\n$$\nHere, $s$ is a vector such that $s_j = \\text{sign}(\\hat{\\beta}_j)$ if $\\hat{\\beta}_j \\neq 0$, and $s_j \\in [-1, 1]$ if $\\hat{\\beta}_j = 0$. With $\\lambda_{\\text{L}}=1$, this is $X^{\\top}X\\hat{\\beta} - X^{\\top}y + s = 0$.\nWe compute the required matrices:\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{pmatrix}\n$$\n$$\nX^{\\top}y = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThe KKT conditions form a system of equations:\n1. $\\hat{\\beta}_1 + \\hat{\\beta}_3 - 1 + s_1 = 0$\n2. $\\hat{\\beta}_2 + \\hat{\\beta}_3 + 1 + s_2 = 0$\n3. $\\hat{\\beta}_1 + \\hat{\\beta}_2 + 2\\hat{\\beta}_3 + s_3 = 0$\n\nSumming the first two equations gives $\\hat{\\beta}_1 + \\hat{\\beta}_2 + 2\\hat{\\beta}_3 + s_1 + s_2 = 0$. Comparing this with the third equation reveals a consistency condition imposed by the collinearity: $s_3 = s_1 + s_2$.\n\nLet's test the hypothesis that $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$. If $\\hat{\\beta} = 0$, we can choose any $s_j \\in [-1, 1]$. The KKT conditions become:\n1. $0 + 0 - 1 + s_1 = 0 \\implies s_1 = 1$\n2. $0 + 0 + 1 + s_2 = 0 \\implies s_2 = -1$\n3. $0 + 0 + 0 + s_3 = 0 \\implies s_3 = 0$\nThe subgradient vector is $s = (1, -1, 0)$. All its components are in $[-1, 1]$, so $s \\in \\partial\\|0\\|_1$. Furthermore, the consistency condition $s_3 = s_1 + s_2$ is satisfied, as $0 = 1 + (-1)$. Thus, $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$ is a valid solution. Since the LASSO objective is convex, this is a global minimum. The objective value is $L(0,0,0) = \\frac{1}{2}(1^2 + (-1)^2) = 1$. The strict convexity argument shows that for any direction $v$ in the null space of $X$, specifically $v=(1,1,-1)^\\top$, the objective $L(\\alpha v) = 1 + 3|\\alpha|$ is strictly minimized at $\\alpha=0$. This confirms the uniqueness of the solution.\nSo, $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$.\n\n### Dantzig Selector Estimator Calculation\nThe Dantzig selector problem is:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X\\beta)\\|_{\\infty} \\leq \\lambda_{\\text{D}}\n$$\nwith $\\lambda_{\\text{D}} = \\frac{3}{4}$. The constraint is $|(X^{\\top}X\\beta - X^{\\top}y)_j| \\le \\lambda_{\\text{D}}$ for $j=1,2,3$.\nLet's define $\\gamma_1 = \\beta_1 + \\beta_3$ and $\\gamma_2 = \\beta_2 + \\beta_3$. The term $X\\beta$ can be reparameterized as:\n$$\nX\\beta = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 + x_2) = (\\beta_1 + \\beta_3)x_1 + (\\beta_2 + \\beta_3)x_2 = \\gamma_1 x_1 + \\gamma_2 x_2\n$$\nThe constraints on the residual correlations are:\n1. $|(\\gamma_1 x_1 + \\gamma_2 x_2)^{\\top}x_1 - y^{\\top}x_1| \\le \\lambda_{\\text{D}} \\implies |\\gamma_1 - 1| \\le \\frac{3}{4} \\implies \\frac{1}{4} \\le \\gamma_1 \\le \\frac{7}{4}$.\n2. |$(\\gamma_1 x_1 + \\gamma_2 x_2)^{\\top}x_2 - y^{\\top}x_2| \\le \\lambda_{\\text{D}} \\implies |\\gamma_2 - (-1)| \\le \\frac{3}{4} \\implies -\\frac{7}{4} \\le \\gamma_2 \\le -\\frac{1}{4}$.\n3. |$(\\gamma_1 x_1 + \\gamma_2 x_2)^{\\top}x_3 - y^{\\top}x_3| \\le \\lambda_{\\text{D}} \\implies |\\gamma_1 + \\gamma_2 - 0| \\le \\frac{3}{4} \\implies -\\frac{3}{4} \\le \\gamma_1 + \\gamma_2 \\le \\frac{3}{4}$.\n\nThe objective is to minimize $\\|\\beta\\|_1 = |\\beta_1| + |\\beta_2| + |\\beta_3| = |\\gamma_1 - \\beta_3| + |\\gamma_2 - \\beta_3| + |\\beta_3|$. For any fixed $(\\gamma_1, \\gamma_2)$, this term is minimized by choosing $\\beta_3$ to be the median of $\\{0, \\gamma_1, \\gamma_2\\}$.\nFrom the constraints, we know that any feasible $\\gamma_1$ is positive and any feasible $\\gamma_2$ is negative. Thus, the ordered set is always $\\{\\gamma_2, 0, \\gamma_1\\}$. The median is $0$, so the optimal choice is $\\hat{\\beta}_3 = 0$.\nWith $\\hat{\\beta}_3=0$, we have $\\beta_1 = \\gamma_1$ and $\\beta_2 = \\gamma_2$. The objective becomes minimizing $|\\gamma_1| + |\\gamma_2|$. Since $\\gamma_1 > 0$ and $\\gamma_2  0$, this is equivalent to minimizing $\\gamma_1 - \\gamma_2$.\n\nWe now have a linear program:\nMinimize $\\gamma_1 - \\gamma_2$ subject to:\n$$\n\\frac{1}{4} \\le \\gamma_1 \\le \\frac{7}{4}\n$$\n$$\n-\\frac{7}{4} \\le \\gamma_2 \\le -\\frac{1}{4}\n$$\n$$\n-\\frac{3}{4} \\le \\gamma_1 + \\gamma_2 \\le \\frac{3}{4}\n$$\nTo minimize $\\gamma_1 - \\gamma_2$, we must choose the smallest possible $\\gamma_1$ and the largest possible $\\gamma_2$. From the first two constraints, these are $\\gamma_1 = \\frac{1}{4}$ and $\\gamma_2 = -\\frac{1}{4}$.\nWe verify if this pair satisfies the third constraint: $\\gamma_1 + \\gamma_2 = \\frac{1}{4} + (-\\frac{1}{4}) = 0$.\nSince $-\\frac{3}{4} \\le 0 \\le \\frac{3}{4}$, this point is feasible.\nThus, the solution is $\\hat{\\gamma}_1 = \\frac{1}{4}$ and $\\hat{\\gamma}_2 = -\\frac{1}{4}$.\nThe Dantzig selector estimator coefficients are:\n$$\n\\hat{\\beta}_3^{\\text{D}} = 0\n$$\n$$\n\\hat{\\beta}_1^{\\text{D}} = \\hat{\\gamma}_1 - \\hat{\\beta}_3^{\\text{D}} = \\frac{1}{4}\n$$\n$$\n\\hat{\\beta}_2^{\\text{D}} = \\hat{\\gamma}_2 - \\hat{\\beta}_3^{\\text{D}} = -\\frac{1}{4}\n$$\nSo, $\\hat{\\beta}^{\\text{D}} = (\\frac{1}{4}, -\\frac{1}{4}, 0)$.\n\n### Explanation of Different Outcomes\nThe perfect collinearity $x_3 = x_1 + x_2$ implies that for any vector $\\beta$, the prediction $X\\beta$ is unchanged if we replace $\\beta$ with $\\beta' = (\\beta_1 + \\alpha, \\beta_2 + \\alpha, \\beta_3 - \\alpha)$ for any scalar $\\alpha$. The two methods resolve this ambiguity differently.\n\n- **LASSO**: The objective function combines the squared error loss with the $l_1$ penalty. The regularization level $\\lambda_{\\text{L}} = 1$ is sufficiently high to shrink the effective coefficients to zero, making the optimal fit $X\\beta = 0$. Given this fit, LASSO must choose from the infinite family of coefficient vectors $(\\alpha, \\alpha, -\\alpha)$ that produce it. It does so by minimizing the penalty term, $\\|\\beta\\|_1=|\\alpha|+|\\alpha|+|-\\alpha|=3|\\alpha|$. This term is minimized at $\\alpha=0$, yielding the unique solution $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$. LASSO is unable to distinguish between the collinear predictors and, under this level of regularization, eliminates all of them.\n\n- **Dantzig Selector**: This method separates the model fit from the coefficient regularization. The fit is handled by the constraints, which require the correlation between each feature and the residual to be small ($|\\cdot| \\le \\lambda_{\\text{D}}$). The objective is solely to find the vector $\\beta$ with the minimum $l_1$ norm within this feasible set. The collinearity leads to a dependency in the constraints. However, the regularization level $\\lambda_{\\text{D}} = \\frac{3}{4}$ defines a feasible set that allows for non-zero coefficients. The optimization process correctly identifies that to minimize $\\|\\beta\\|_1$ under the given constraints, the solution must have $\\beta_3=0$. It then finds the non-zero coefficients for $\\beta_1$ and $\\beta_2$ that satisfy the correlation bounds with minimal $l_1$ norm. The result is a non-zero, sparse solution $\\hat{\\beta}^{\\text{D}} = (\\frac{1}{4}, -\\frac{1}{4}, 0)$ that correctly identifies the two basis predictors $x_1$ and $x_2$.\n\nIn essence, the Dantzig selector's structure allowed it to tolerate a certain amount of residual correlation to find a sparse solution, while the LASSO's unified objective, with the chosen $\\lambda_{\\text{L}}$, forced the solution to have zero residual correlation at the expense of setting all coefficients to zero.\n\nFinal concatenation:\nThe LASSO estimator is $(\\hat{\\beta}^{\\text{L}}_1, \\hat{\\beta}^{\\text{L}}_2, \\hat{\\beta}^{\\text{L}}_3) = (0, 0, 0)$.\nThe Dantzig selector estimator is $(\\hat{\\beta}^{\\text{D}}_1, \\hat{\\beta}^{\\text{D}}_2, \\hat{\\beta}^{\\text{D}}_3) = (\\frac{1}{4}, -\\frac{1}{4}, 0)$.\nThe concatenated vector is $(0, 0, 0, \\frac{1}{4}, -\\frac{1}{4}, 0)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0  0  \\frac{1}{4}  -\\frac{1}{4}  0 \\end{pmatrix}}\n$$"
        }
    ]
}