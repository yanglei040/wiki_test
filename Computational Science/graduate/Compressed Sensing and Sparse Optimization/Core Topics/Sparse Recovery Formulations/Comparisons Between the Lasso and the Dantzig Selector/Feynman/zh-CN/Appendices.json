{
    "hands_on_practices": [
        {
            "introduction": "理解复杂的估计量通常始于在简化的理想环境中对其进行分析。正交设计（即 $X^\\top X = I_p$）就是这样一种环境。该练习旨在探究此假设如何将 LASSO 和 Dantzig 选择器都简化为一种常见的数学运算——软阈值（soft-thresholding）。通过推导这些闭式解，您将对两种方法之间的深层联系获得基础性的理解，并看到它们各自的调节参数 $\\lambda$ 和 $t$ 如何直接控制这一阈值运算。",
            "id": "3435559",
            "problem": "考虑线性模型 $y = X \\beta^{\\star} + w$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 的列是标准正交的，即 $X^{\\top} X = I_{p}$。在压缩感知和稀疏优化的背景下，您需要比较 $\\beta^{\\star}$ 的两种凸估计量：最小绝对值收敛和选择算子 (LASSO) 估计量和 Dantzig 选择器。LASSO 估计量 $\\hat{\\beta}_{\\mathrm{LASSO}}$ 定义为目标函数\n$$\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\n的最小化子，其中 $\\lambda > 0$ 是一个正则化参数。Dantzig 选择器 $\\hat{\\beta}_{\\mathrm{DS}}$ 定义为在可行性约束\n$$\\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t$$\n条件下 $\\|\\beta\\|_{1}$ 的最小化子，其中 $t > 0$ 是一个调节参数。\n\n从 LASSO 的凸最优性条件和 Dantzig 选择器的可行性几何出发，推导在正交设计条件 $X^{\\top} X = I_{p}$ 下 $\\hat{\\beta}_{\\mathrm{LASSO}}$ 和 $\\hat{\\beta}_{\\mathrm{DS}}$ 的闭式表达式。然后，对于 $p = 5$ 的特定实例，给定充分统计量 $X^{\\top} y = z$ 为\n$$z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix},$$\n以及参数 $\\lambda = 0.9$ 和 $t = 0.7$，计算两个估计量之间的平方 $\\ell_{2}$ 距离，\n$$\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}。$$\n\n将您的最终数值答案四舍五入到四位有效数字。您的最终答案不带单位。",
            "solution": "该问题要求在正交设计矩阵的特定条件下，计算 LASSO 和 Dantzig 选择器估计量之间的平方 $\\ell_2$ 距离。我们首先推导在此条件下两种估计量的闭式解。\n\n定义充分统计量为 $z = X^{\\top}y$。正交设计条件为 $X^{\\top}X = I_p$，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。\n\n首先，我们分析 LASSO 估计量 $\\hat{\\beta}_{\\mathrm{LASSO}}$，它是以下最小化问题的解：\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right) $$\n目标函数中的二次项可以展开为：\n$$ \\|y - X \\beta\\|_{2}^{2} = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta $$\n使用条件 $X^{\\top}X = I_p$ 和定义 $z = X^{\\top}y$，上式可简化为：\n$$ \\|y - X \\beta\\|_{2}^{2} = y^{\\top}y - 2z^{\\top}\\beta + \\beta^{\\top}\\beta $$\n$y^{\\top}y$ 项相对于 $\\beta$ 是一个常数，可以从最小化问题中去掉。我们也可以加上常数项 $\\frac{1}{2}z^{\\top}z$ 而不改变最小化子。因此，目标函数等价于最小化：\n$$ L(\\beta) = \\frac{1}{2}(\\beta^{\\top}\\beta - 2z^{\\top}\\beta + z^{\\top}z) + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\|\\beta - z\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} $$\n这个目标函数是可分离的，意味着它可以写成关于 $\\beta$ 各个分量的函数之和：\n$$ L(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\n因此，我们可以通过独立地最小化每个分量项来找到最优的 $\\hat{\\beta}_{\\mathrm{LASSO}}$：\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\arg \\min_{\\beta_j \\in \\mathbb{R}} \\left( \\frac{1}{2}(\\beta_j - z_j)^2 + \\lambda |\\beta_j| \\right) $$\n这是 $\\ell_1$ 范数的近端算子，其解为软阈值函数。每个分量的解是：\n$$ \\hat{\\beta}_{j, \\mathrm{LASSO}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - \\lambda, 0) $$\n这可以表示为 $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{\\lambda}(z)$，其中 $S_{\\lambda}$ 是阈值为 $\\lambda$ 的逐元素软阈值算子。\n\n接下来，我们分析 Dantzig 选择器 $\\hat{\\beta}_{\\mathrm{DS}}$，它是以下约束优化问题的解：\n$$ \\hat{\\beta}_{\\mathrm{DS}} = \\arg \\min_{\\beta \\in \\mathbb{R}^p} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X \\beta)\\|_{\\infty} \\leq t $$\n我们使用正交设计性质 $X^{\\top}X = I_p$ 和定义 $z = X^{\\top}y$ 来简化约束条件：\n$$ X^{\\top}(y - X \\beta) = X^{\\top}y - X^{\\top}X\\beta = z - \\beta $$\n因此，约束变为 $\\|z - \\beta\\|_{\\infty} \\leq t$。这等价于一组分量形式的约束：\n$$ |z_j - \\beta_j| \\leq t \\quad \\text{for all } j \\in \\{1, \\dots, p\\} $$\n每个不等式可以重写为 $z_j - t \\leq \\beta_j \\leq z_j + t$。优化问题现在是在这些箱式约束下最小化 $\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_j|$。由于目标函数和约束都是可分离的，我们可以独立地求解每个分量：\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\arg \\min_{\\beta_j} |\\beta_j| \\quad \\text{subject to} \\quad z_j - t \\leq \\beta_j \\leq z_j + t $$\n为了找到最小值，我们考虑区间 $[z_j - t, z_j + t]$ 相对于 $0$ 的位置：\n1. 如果区间包含 $0$，即 $z_j - t \\leq 0 \\leq z_j + t$，这等价于 $|z_j| \\leq t$，那么 $|\\beta_j|$ 的最小值在 $\\hat{\\beta}_{j, \\mathrm{DS}} = 0$ 处取得。\n2. 如果区间完全为正，即 $z_j - t > 0$ 或 $z_j > t$，区间中最接近 $0$ 的点是左端点。因此，$\\hat{\\beta}_{j, \\mathrm{DS}} = z_j - t$。\n3. 如果区间完全为负，即 $z_j + t  0$ 或 $z_j  -t$，区间中最接近 $0$ 的点是右端点。因此，$\\hat{\\beta}_{j, \\mathrm{DS}} = z_j + t$。\n这三种情况可以由相同的软阈值函数概括：\n$$ \\hat{\\beta}_{j, \\mathrm{DS}} = \\mathrm{sign}(z_j) \\cdot \\max(|z_j| - t, 0) $$\n这可以表示为 $\\hat{\\beta}_{\\mathrm{DS}} = S_{t}(z)$。\n\n现在我们使用给定的数值：\n$$ z = \\begin{pmatrix} 2.4 \\\\ -0.3 \\\\ 1.1 \\\\ -1.8 \\\\ 0.0 \\end{pmatrix}, \\quad \\lambda = 0.9, \\quad t = 0.7 $$\n我们计算 LASSO 估计 $\\hat{\\beta}_{\\mathrm{LASSO}} = S_{0.9}(z)$：\n$$ \\hat{\\beta}_{1, \\mathrm{LASSO}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.9, 0) = 1.5 $$\n$$ \\hat{\\beta}_{2, \\mathrm{LASSO}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.9, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{LASSO}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.9, 0) = 0.2 $$\n$$ \\hat{\\beta}_{4, \\mathrm{LASSO}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.9, 0) = -0.9 $$\n$$ \\hat{\\beta}_{5, \\mathrm{LASSO}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.9, 0) = 0 $$\n所以，$\\hat{\\beta}_{\\mathrm{LASSO}} = \\begin{pmatrix} 1.5 \\\\ 0 \\\\ 0.2 \\\\ -0.9 \\\\ 0 \\end{pmatrix}$。\n\n接下来，我们计算 Dantzig 选择器估计 $\\hat{\\beta}_{\\mathrm{DS}} = S_{0.7}(z)$：\n$$ \\hat{\\beta}_{1, \\mathrm{DS}} = \\mathrm{sign}(2.4) \\max(|2.4| - 0.7, 0) = 1.7 $$\n$$ \\hat{\\beta}_{2, \\mathrm{DS}} = \\mathrm{sign}(-0.3) \\max(|-0.3| - 0.7, 0) = 0 $$\n$$ \\hat{\\beta}_{3, \\mathrm{DS}} = \\mathrm{sign}(1.1) \\max(|1.1| - 0.7, 0) = 0.4 $$\n$$ \\hat{\\beta}_{4, \\mathrm{DS}} = \\mathrm{sign}(-1.8) \\max(|-1.8| - 0.7, 0) = -1.1 $$\n$$ \\hat{\\beta}_{5, \\mathrm{DS}} = \\mathrm{sign}(0.0) \\max(|0.0| - 0.7, 0) = 0 $$\n所以，$\\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.7 \\\\ 0 \\\\ 0.4 \\\\ -1.1 \\\\ 0 \\end{pmatrix}$。\n\n最后，我们计算两个估计量之间的平方 $\\ell_2$ 距离 $\\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2}$：\n$$ \\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}} = \\begin{pmatrix} 1.5 - 1.7 \\\\ 0 - 0 \\\\ 0.2 - 0.4 \\\\ -0.9 - (-1.1) \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} -0.2 \\\\ 0 \\\\ -0.2 \\\\ 0.2 \\\\ 0 \\end{pmatrix} $$\n平方 $\\ell_2$ 范数是该差向量各分量平方的和：\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = (-0.2)^2 + 0^2 + (-0.2)^2 + (0.2)^2 + 0^2 $$\n$$ \\|\\hat{\\beta}_{\\mathrm{LASSO}} - \\hat{\\beta}_{\\mathrm{DS}}\\|_{2}^{2} = 0.04 + 0 + 0.04 + 0.04 + 0 = 0.12 $$\n题目要求答案四舍五入到四位有效数字。计算出的值为 $0.12$，可以写成 $0.1200$ 以反映此精度。",
            "answer": "$$\\boxed{0.1200}$$"
        },
        {
            "introduction": "除了代数形式之外，对估计量进行几何解释能提供更深刻的见解。本练习在一个简单的二维空间中对比了约束形式的 LASSO 和 Dantzig 选择器，您将直观地看到 LASSO 如何在 $\\ell_1$ 球上寻找离数据最近的点，而 Dantzig 选择器则在相关性约束的区域内寻找 $\\ell_1$ 范数最小的点。通过完成一个解不相同的具体示例，您将为这两种方法（尽管它们都有促进稀疏性的相似目标）为何是根本不同的优化问题建立起强大的几何直觉。",
            "id": "3435555",
            "problem": "考虑一个有限维线性逆问题，其设计矩阵为 $A \\in \\mathbb{R}^{n \\times p}$，观测向量为 $y \\in \\mathbb{R}^{n}$。最小绝对值收敛和选择算子 (LASSO) 在此以其约束形式定义为以下凸规划的解集\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ \\ \\text{subject to} \\ \\ \\|x\\|_{1} \\leq t,\n$$\n其中 $t  0$ 是给定的预算。Dantzig 选择器 (DS) 定义为以下凸规划的解集\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\ \\|x\\|_{1} \\ \\ \\text{subject to} \\ \\ \\|A^{\\top}(A x - y)\\|_{\\infty} \\leq \\lambda,\n$$\n其中 $\\lambda  0$ 是给定的容差。\n\n在 $n = p = 2$ 的情况下，取 $A = I_{2}$，即 $2 \\times 2$ 单位矩阵。设 $y = (3,1)^{\\top}$，$t = 2.5$，以及 $\\lambda = 0.8$。仅从上述定义和标准的凸分析原理出发，完成以下任务：\n\n- 推导此实例的 LASSO 最小化子 $x_{\\mathrm{L}}$ 和 Dantzig 选择器最小化子 $x_{\\mathrm{D}}$，并明确验证 LASSO 解集与 Dantzig 选择器可行集相交，但产生不同的最小化子。\n- 从 $\\mathbb{R}^{2}$ 中水平集和可行域的形状角度，提供一个几何解释，说明为什么即使 LASSO 最小化子是 Dantzig 可行的，两者的最小化子也不同。\n\n最后，计算欧几里得距离 $\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2}$，并以简化的精确表达式给出最终答案。不要对答案进行四舍五入。",
            "solution": "问题指定了 $n = p = 2$ 的情况，设计矩阵为单位矩阵 $A = I_2$，观测向量为 $y = (3, 1)^{\\top}$，LASSO 预算为 $t = 2.5$，Dantzig 选择器容差为 $\\lambda = 0.8$。\n\n**1. 推导 LASSO 最小化子 $x_{\\mathrm{L}}$**\n\nLASSO 问题为\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\ \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\ \\ \\text{subject to} \\ \\ \\|x\\|_{1} \\leq t\n$$\n代入给定值，设 $x = (x_1, x_2)^{\\top}$：\n$$\n\\min_{x_1, x_2} \\ \\frac{1}{2}\\|x - y\\|_{2}^{2} = \\frac{1}{2}((x_1 - 3)^2 + (x_2 - 1)^2) \\ \\ \\text{subject to} \\ \\ |x_1| + |x_2| \\leq 2.5\n$$\n这个问题等价于求点 $y=(3,1)$ 到以原点为中心、半径为 $t=2.5$ 的 $\\ell_1$ 球上的欧几里得投影。\n首先，我们检查无约束最小化子 $x_{unc} = y = (3,1)$ 是否可行。我们计算其 $\\ell_1$ 范数：$\\|y\\|_1 = |3| + |1| = 4$。由于 $4  2.5$，无约束解是不可行的，因此 LASSO 解 $x_{\\mathrm{L}}$ 必须位于可行域的边界上，即 $\\|x_{\\mathrm{L}}\\|_1 = 2.5$。\n\n由于 $y$ 位于第一象限，我们预期解 $x_{\\mathrm{L}}$ 也将位于第一象限，因此 $x_1 \\geq 0$ 且 $x_2 \\geq 0$。约束变为 $x_1 + x_2 = 2.5$。从几何上看，解是线段 $x_1 + x_2 = 2.5$（其中 $x_1, x_2 \\ge 0$）上距离 $y=(3,1)$ 最近的点。从解 $x_{\\mathrm{L}}$ 到 $y$ 的向量必须与该线段正交。直线 $x_1+x_2=2.5$ 的方向向量是 $(1, -1)$。因此，向量 $y - x_{\\mathrm{L}} = (3-x_1, 1-x_2)$ 必须与 $(1, -1)$ 正交。\n正交条件由点积给出：\n$$\n(3-x_1, 1-x_2) \\cdot (1, -1) = 0\n$$\n$$\n(3-x_1) - (1-x_2) = 0 \\implies 2 - x_1 + x_2 = 0 \\implies x_1 - x_2 = 2\n$$\n现在我们求解线性方程组：\n\\begin{enumerate}\n    \\item $x_1 + x_2 = 2.5$\n    \\item $x_1 - x_2 = 2$\n\\end{enumerate}\n两方程相加得到 $2x_1 = 4.5$，因此 $x_1 = 2.25$。\n将 $x_1$ 代入第一个方程得到 $2.25 + x_2 = 2.5$，因此 $x_2 = 0.25$。\n由于 $x_1 = 2.25  0$ 且 $x_2 = 0.25  0$，我们的假设是正确的。LASSO 最小化子是 $x_{\\mathrm{L}} = (2.25, 0.25)^{\\top}$。\n\n**2. 推导 Dantzig 选择器最小化子 $x_{\\mathrm{D}}$**\n\nDantzig 选择器 (DS) 问题是\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\ \\|x\\|_{1} \\ \\ \\text{subject to} \\ \\ \\|A^{\\top}(A x - y)\\|_{\\infty} \\leq \\lambda\n$$\n代入给定值，$A=I_2$：\n$$\n\\min_{x_1, x_2} \\ |x_1| + |x_2| \\ \\ \\text{subject to} \\ \\ \\|x - y\\|_{\\infty} \\leq \\lambda\n$$\n约束 $\\|x-y\\|_{\\infty} \\leq 0.8$ 展开为 $\\max(|x_1 - 3|, |x_2 - 1|) \\leq 0.8$。这等价于以下不等式组：\n$$\n|x_1 - 3| \\leq 0.8 \\implies 3 - 0.8 \\leq x_1 \\leq 3 + 0.8 \\implies 2.2 \\leq x_1 \\leq 3.8\n$$\n$$\n|x_2 - 1| \\leq 0.8 \\implies 1 - 0.8 \\leq x_2 \\leq 1 + 0.8 \\implies 0.2 \\leq x_2 \\leq 1.8\n$$\n可行域是矩形 $[2.2, 3.8] \\times [0.2, 1.8]$。由于整个区域都在第一象限，目标函数简化为最小化 $x_1 + x_2$。这是一个线性规划问题。线性函数在凸多胞体（我们的矩形）上的最小值必然出现在一个顶点上。这些顶点是 $(2.2, 0.2)$, $(3.8, 0.2)$, $(2.2, 1.8)$ 和 $(3.8, 1.8)$。在每个顶点上计算目标函数 $x_1+x_2$ 的值：\n\\begin{itemize}\n    \\item 在 $(2.2, 0.2)$：$x_1 + x_2 = 2.2 + 0.2 = 2.4$\n    \\item 在 $(3.8, 0.2)$：$x_1 + x_2 = 3.8 + 0.2 = 4.0$\n    \\item 在 $(2.2, 1.8)$：$x_1 + x_2 = 2.2 + 1.8 = 4.0$\n    \\item 在 $(3.8, 1.8)$：$x_1 + x_2 = 3.8 + 1.8 = 5.6$\n\\end{itemize}\n最小值为 $2.4$，在顶点 $(2.2, 0.2)$ 处取得。\n因此，Dantzig 选择器最小化子是 $x_{\\mathrm{D}} = (2.2, 0.2)^{\\top}$。\n\n**3. 验证与几何解释**\n\n最小化子分别为 $x_{\\mathrm{L}} = (2.25, 0.25)^{\\top}$ 和 $x_{\\mathrm{D}} = (2.2, 0.2)^{\\top}$，它们是不同的。\n我们验证 LASSO 最小化子 $x_{\\mathrm{L}}$ 对于 Dantzig 选择器问题是可行的。我们检查是否满足 $\\|x_{\\mathrm{L}} - y\\|_{\\infty} \\leq \\lambda$：\n$$\n\\|x_{\\mathrm{L}} - y\\|_{\\infty} = \\|(2.25 - 3, 0.25 - 1)\\|_{\\infty} = \\|(-0.75, -0.75)\\|_{\\infty} = \\max(|-0.75|, |-0.75|) = 0.75\n$$\n由于 $0.75 \\leq 0.8$，$x_{\\mathrm{L}}$ 确实在 Dantzig 选择器的可行集中。这证实了 LASSO 解集和 Dantzig 选择器可行集是相交的。然而，$\\|x_{\\mathrm{L}}\\|_1 = 2.25 + 0.25 = 2.5$，而 $\\|x_{\\mathrm{D}}\\|_1 = 2.2+0.2=2.4$。由于 $\\|x_{\\mathrm{D}}\\|_1  \\|x_{\\mathrm{L}}\\|_1$，$x_{\\mathrm{L}}$ 不是 Dantzig 最小化子。\n\n几何解释：\n- LASSO 解 $x_{\\mathrm{L}}$ 是 $\\ell_1$ 球 $\\|x\\|_1 \\leq 2.5$（一个以原点为中心的菱形）上与 $y=(3,1)$ 欧几里得距离最近的点。LASSO 目标函数的水平集是以 $y$ 为中心的圆。通过从 $y$ 点开始扩大一个圆，直到它首次接触到该菱形，即可找到解。这个切点就是 $y$ 在该菱形上的欧几里得投影。\n- Dantzig 选择器解 $x_{\\mathrm{D}}$ 是 $\\ell_{\\infty}$ 球 $\\|x-y\\|_{\\infty} \\leq 0.8$（一个以 $y$ 为中心的正方形）内具有最小 $\\ell_1$ 范数的点。DS 目标函数的水平集是以原点为中心的菱形 $\\|x\\|_1=c$。通过从原点开始扩大一个菱形，直到它首次接触到该可行正方形，即可找到解。\n- 这两种方法产生不同的结果，因为它们在不同的可行域上优化不同的目标函数。LASSO 在一个 $\\ell_1$ 约束集上最小化 $\\ell_2$ 距离，而 Dantzig 选择器在一个 $\\ell_{\\infty}$ 约束集上最小化 $\\ell_1$ 范数。点 $x_{\\mathrm{L}}=(2.25, 0.25)$ 在欧几里得距离上更接近 $y$，但点 $x_{\\mathrm{D}}=(2.2, 0.2)$ 在 $\\ell_1$ 度量下“更接近”原点。目标函数和约束的不同几何形状导致了不同的最优性条件，从而产生不同的解。\n\n**4. 最终计算**\n\n我们计算欧几里得距离 $\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2}$。\n$$\nx_{\\mathrm{L}} = (2.25, 0.25)^{\\top} = \\left(\\frac{9}{4}, \\frac{1}{4}\\right)^{\\top}\n$$\n$$\nx_{\\mathrm{D}} = (2.2, 0.2)^{\\top} = \\left(\\frac{11}{5}, \\frac{1}{5}\\right)^{\\top}\n$$\n差向量为：\n$$\nx_{\\mathrm{L}} - x_{\\mathrm{D}} = \\left(\\frac{9}{4} - \\frac{11}{5}, \\frac{1}{4} - \\frac{1}{5}\\right) = \\left(\\frac{45 - 44}{20}, \\frac{5 - 4}{20}\\right) = \\left(\\frac{1}{20}, \\frac{1}{20}\\right)^{\\top}\n$$\n欧几里得距离为：\n$$\n\\|x_{\\mathrm{L}} - x_{\\mathrm{D}}\\|_{2} = \\sqrt{\\left(\\frac{1}{20}\\right)^2 + \\left(\\frac{1}{20}\\right)^2} = \\sqrt{2 \\cdot \\left(\\frac{1}{20}\\right)^2} = \\sqrt{\\frac{2}{400}} = \\frac{\\sqrt{2}}{20}\n$$",
            "answer": "$$\\boxed{\\frac{\\sqrt{2}}{20}}$$"
        },
        {
            "introduction": "当处理具有挑战性的数据结构（例如多重共线性）时，估计量之间的理论差异变得最为明显。本练习将研究一个存在完全共线性的场景，其中一个预测变量是其他预测变量的线性组合。您将计算 LASSO 和 Dantzig 选择器的估计值，并观察它们在处理这种冗余信息时所采取的不同方法。这个练习展示了这两种方法在变量选择方面的关键实践差异，揭示了当预测变量不独立时，它们潜在的数学公式如何导致支撑集恢复的不同结果。",
            "id": "3435595",
            "problem": "考虑一个在压缩感知中的线性回归模型，其设计矩阵 $X \\in \\mathbb{R}^{2 \\times 3}$ 和响应向量 $y \\in \\mathbb{R}^{2}$ 由下式给出\n$$\nX = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}, \n\\quad\ny = \\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n$$\n注意，第三列满足精确的线性关系 $x_{3} = x_{1} + x_{2}$，因此该设计表现出完全共线性。将最小绝对收缩和选择算子 (LASSO) 估计量 $\\hat{\\beta}^{\\text{L}} \\in \\mathbb{R}^{3}$ 定义为以下问题的解\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{\\text{L}}\\|\\beta\\|_{1},\n$$\n并将 Dantzig 选择器估计量 $\\hat{\\beta}^{\\text{D}} \\in \\mathbb{R}^{3}$ 定义为以下问题的解\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X\\beta)\\|_{\\infty} \\leq \\lambda_{\\text{D}}.\n$$\n对于 LASSO，取正则化水平为 $\\lambda_{\\text{L}} = 1$，对于 Dantzig 选择器，取正则化水平为 $\\lambda_{\\text{D}} = \\frac{3}{4}$。从上述核心定义出发，精确计算这两个估计量，并解释完全共线性 $x_{3} = x_{1} + x_{2}$ 在这两种方法下如何导致不同的支撑集恢复结果。将拼接后的估计量向量以单行矩阵的形式报告：首先是 LASSO 系数，顺序为 $(\\beta_{1}, \\beta_{2}, \\beta_{3})$，然后是同样顺序的 Dantzig 选择器系数。无需四舍五入。",
            "solution": "### LASSO 估计量计算\nLASSO 目标函数为 $L(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{\\text{L}}\\|\\beta\\|_{1}$。当 $\\lambda_{\\text{L}} = 1$ 时，该函数为：\n$$\nL(\\beta) = \\frac{1}{2}\\left\\| \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{pmatrix} \\right\\|_{2}^{2} + \\sum_{j=1}^{3} |\\beta_j|\n$$\n$$\nL(\\beta) = \\frac{1}{2} \\left[ (1 - \\beta_1 - \\beta_3)^2 + (-1 - \\beta_2 - \\beta_3)^2 \\right] + |\\beta_1| + |\\beta_2| + |\\beta_3|\n$$\nKarush-Kuhn-Tucker (KKT) 条件指出，在最小值 $\\hat{\\beta}$ 处，零向量必须位于目标函数的次梯度中：$0 \\in \\partial L(\\hat{\\beta})$。这给出了条件：\n$$\n-X^{\\top}(y - X\\hat{\\beta}) + \\lambda_{\\text{L}} s = 0, \\quad \\text{其中 } s \\in \\partial\\|\\hat{\\beta}\\|_{1}\n$$\n此处，$s$ 是一个向量，使得当 $\\hat{\\beta}_j \\neq 0$ 时 $s_j = \\text{sign}(\\hat{\\beta}_j)$，当 $\\hat{\\beta}_j = 0$ 时 $s_j \\in [-1, 1]$。当 $\\lambda_{\\text{L}}=1$ 时，条件为 $X^{\\top}X\\hat{\\beta} - X^{\\top}y + s = 0$。\n我们计算所需的矩阵：\n$$\nX^{\\top}X = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{pmatrix}\n$$\n$$\nX^{\\top}y = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nKKT 条件构成一个方程组：\n1. $\\hat{\\beta}_1 + \\hat{\\beta}_3 - 1 + s_1 = 0$\n2. $\\hat{\\beta}_2 + \\hat{\\beta}_3 + 1 + s_2 = 0$\n3. $\\hat{\\beta}_1 + \\hat{\\beta}_2 + 2\\hat{\\beta}_3 + s_3 = 0$\n\n将前两个方程相加得到 $\\hat{\\beta}_1 + \\hat{\\beta}_2 + 2\\hat{\\beta}_3 + s_1 + s_2 = 0$。将此与第三个方程比较，揭示了由共线性施加的一致性条件：$s_3 = s_1 + s_2$。\n\n我们来检验假设 $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$。如果 $\\hat{\\beta} = 0$，我们可以选择任何 $s_j \\in [-1, 1]$。KKT 条件变为：\n1. $0 + 0 - 1 + s_1 = 0 \\implies s_1 = 1$\n2. $0 + 0 + 1 + s_2 = 0 \\implies s_2 = -1$\n3. $0 + 0 + 0 + s_3 = 0 \\implies s_3 = 0$\n次梯度向量为 $s = (1, -1, 0)$。其所有分量都在 $[-1, 1]$ 内，因此 $s \\in \\partial\\|0\\|_1$。此外，一致性条件 $s_3 = s_1 + s_2$ 也得到满足，因为 $0 = 1 + (-1)$。因此，$\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$ 是一个有效解。由于 LASSO 目标函数是凸的，这是一个全局最小值。目标值为 $L(0,0,0) = \\frac{1}{2}(1^2 + (-1)^2) = 1$。严格凸性论证表明，对于 $X$ 零空间中的任何方向 $v$，特别是 $v=(1,1,-1)^\\top$，目标函数 $L(\\alpha v) = 1 + 3|\\alpha|$ 在 $\\alpha=0$ 处严格最小化。这证实了解的唯一性。\n所以，$\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$。\n\n### Dantzig 选择器估计量计算\nDantzig 选择器问题是：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^{\\top}(y - X\\beta)\\|_{\\infty} \\leq \\lambda_{\\text{D}}\n$$\n其中 $\\lambda_{\\text{D}} = \\frac{3}{4}$。约束条件为 $|(X^{\\top}X\\beta - X^{\\top}y)_j| \\le \\lambda_{\\text{D}}$，对于 $j=1,2,3$。\n我们定义 $\\gamma_1 = \\beta_1 + \\beta_3$ 和 $\\gamma_2 = \\beta_2 + \\beta_3$。项 $X\\beta$ 可以被重新参数化为：\n$$\nX\\beta = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 + x_2) = (\\beta_1 + \\beta_3)x_1 + (\\beta_2 + \\beta_3)x_2 = \\gamma_1 x_1 + \\gamma_2 x_2\n$$\n对残差相关性的约束是：\n1. $|(\\gamma_1 x_1 + \\gamma_2 x_2)^{\\top}x_1 - y^{\\top}x_1| \\le \\lambda_{\\text{D}} \\implies |\\gamma_1 - 1| \\le \\frac{3}{4} \\implies \\frac{1}{4} \\le \\gamma_1 \\le \\frac{7}{4}$。\n2. |$(\\gamma_1 x_1 + \\gamma_2 x_2)^{\\top}x_2 - y^{\\top}x_2| \\le \\lambda_{\\text{D}} \\implies |\\gamma_2 - (-1)| \\le \\frac{3}{4} \\implies -\\frac{7}{4} \\le \\gamma_2 \\le -\\frac{1}{4}$。\n3. |$(\\gamma_1 x_1 + \\gamma_2 x_2)^{\\top}x_3 - y^{\\top}x_3| \\le \\lambda_{\\text{D}} \\implies |\\gamma_1 + \\gamma_2 - 0| \\le \\frac{3}{4} \\implies -\\frac{3}{4} \\le \\gamma_1 + \\gamma_2 \\le \\frac{3}{4}$。\n\n目标是最小化 $\\|\\beta\\|_1 = |\\beta_1| + |\\beta_2| + |\\beta_3| = |\\gamma_1 - \\beta_3| + |\\gamma_2 - \\beta_3| + |\\beta_3|$。对于任何固定的 $(\\gamma_1, \\gamma_2)$，该项通过选择 $\\beta_3$ 为 $\\{0, \\gamma_1, \\gamma_2\\}$ 的中位数来最小化。\n从约束条件可知，任何可行的 $\\gamma_1$ 都是正的，任何可行的 $\\gamma_2$ 都是负的。因此，排序后的集合总是 $\\{\\gamma_2, 0, \\gamma_1\\}$。中位数为 0，所以最优选择是 $\\hat{\\beta}_3 = 0$。\n当 $\\hat{\\beta}_3=0$ 时，我们有 $\\beta_1 = \\gamma_1$ 和 $\\beta_2 = \\gamma_2$。目标变为最小化 $|\\gamma_1| + |\\gamma_2|$。由于 $\\gamma_1 > 0$ 且 $\\gamma_2  0$，这等价于最小化 $\\gamma_1 - \\gamma_2$。\n\n我们现在得到一个线性规划问题：\n最小化 $\\gamma_1 - \\gamma_2$，约束条件为：\n$$\n\\frac{1}{4} \\le \\gamma_1 \\le \\frac{7}{4}\n$$\n$$\n-\\frac{7}{4} \\le \\gamma_2 \\le -\\frac{1}{4}\n$$\n$$\n-\\frac{3}{4} \\le \\gamma_1 + \\gamma_2 \\le \\frac{3}{4}\n$$\n为了最小化 $\\gamma_1 - \\gamma_2$，我们必须选择尽可能小的 $\\gamma_1$ 和尽可能大的 $\\gamma_2$。从前两个约束条件可知，它们是 $\\gamma_1 = \\frac{1}{4}$ 和 $\\gamma_2 = -\\frac{1}{4}$。\n我们验证这对值是否满足第三个约束条件：$\\gamma_1 + \\gamma_2 = \\frac{1}{4} + (-\\frac{1}{4}) = 0$。\n由于 $-\\frac{3}{4} \\le 0 \\le \\frac{3}{4}$，该点是可行的。\n因此，解为 $\\hat{\\gamma}_1 = \\frac{1}{4}$ 和 $\\hat{\\gamma}_2 = -\\frac{1}{4}$。\nDantzig 选择器估计量的系数是：\n$$\n\\hat{\\beta}_3^{\\text{D}} = 0\n$$\n$$\n\\hat{\\beta}_1^{\\text{D}} = \\hat{\\gamma}_1 - \\hat{\\beta}_3^{\\text{D}} = \\frac{1}{4}\n$$\n$$\n\\hat{\\beta}_2^{\\text{D}} = \\hat{\\gamma}_2 - \\hat{\\beta}_3^{\\text{D}} = -\\frac{1}{4}\n$$\n所以，$\\hat{\\beta}^{\\text{D}} = (\\frac{1}{4}, -\\frac{1}{4}, 0)$。\n\n### 不同结果的解释\n完全共线性 $x_3 = x_1 + x_2$ 意味着对于任何向量 $\\beta$，如果我们将 $\\beta$ 替换为 $\\beta' = (\\beta_1 + \\alpha, \\beta_2 + \\alpha, \\beta_3 - \\alpha)$（其中 $\\alpha$ 为任意标量），预测值 $X\\beta$ 保持不变。这两种方法以不同的方式解决这种模糊性。\n\n- **LASSO**：其目标函数将平方误差损失与 $l_1$ 惩罚项相结合。正则化水平 $\\lambda_{\\text{L}} = 1$ 足够高，以致于将有效系数 $\\gamma_1 = \\beta_1 + \\beta_3$ 和 $\\gamma_2 = \\beta_2 + \\beta_3$ 收缩至零。这使得最优拟合为 $X\\beta = 0$。在此拟合下，LASSO 必须从产生该拟合的无限个系数向量族 $(\\alpha, \\alpha, -\\alpha)$ 中进行选择。它通过最小化惩罚项 $\\|\\beta\\|_1=|\\alpha|+|\\alpha|+|-\\alpha|=3|\\alpha|$ 来实现这一点。该项在 $\\alpha=0$ 时最小化，从而产生唯一解 $\\hat{\\beta}^{\\text{L}} = (0, 0, 0)$。LASSO 无法区分共线的预测变量，并且在这种正则化水平下，将它们全部消除。\n\n- **Dantzig 选择器**：此方法将模型拟合与系数正则化分开。拟合由约束条件处理，这些约束要求每个特征与残差之间的相关性要小 ($|\\cdot| \\le \\lambda_{\\text{D}}$)。其目标仅仅是在此可行集内找到具有最小 $l_1$ 范数的向量 $\\beta$。共线性导致了约束中的依赖关系 ($r_3 = r_1+r_2$)。然而，正则化水平 $\\lambda_{\\text{D}} = \\frac{3}{4}$ 定义了一个允许非零系数的可行集。优化过程正确地识别出，在给定约束下要最小化 $\\|\\beta\\|_1$，解必须有 $\\beta_3=0$。然后它找到 $\\beta_1$ 和 $\\beta_2$ 的非零系数，这些系数在满足相关性边界的同时具有最小的 $l_1$ 范数。结果是一个非零的稀疏解 $\\hat{\\beta}^{\\text{D}} = (\\frac{1}{4}, -\\frac{1}{4}, 0)$，它正确地识别了两个基预测变量 $x_1$ 和 $x_2$。\n\n本质上，Dantzig 选择器的结构允许它容忍一定量的残差相关性以找到稀疏解，而 LASSO 的统一目标函数，在所选的 $\\lambda_{\\text{L}}$ 下，以将所有系数设置为零为代价，迫使解具有零残差相关性。\n\n最终拼接：\nLASSO 估计量是 $(\\hat{\\beta}^{\\text{L}}_1, \\hat{\\beta}^{\\text{L}}_2, \\hat{\\beta}^{\\text{L}}_3) = (0, 0, 0)$。\nDantzig 选择器估计量是 $(\\hat{\\beta}^{\\text{D}}_1, \\hat{\\beta}^{\\text{D}}_2, \\hat{\\beta}^{\\text{D}}_3) = (\\frac{1}{4}, -\\frac{1}{4}, 0)$。\n拼接后的向量是 $(0, 0, 0, \\frac{1}{4}, -\\frac{1}{4}, 0)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0  0  0  \\frac{1}{4}  -\\frac{1}{4}  0 \\end{pmatrix}}\n$$"
        }
    ]
}