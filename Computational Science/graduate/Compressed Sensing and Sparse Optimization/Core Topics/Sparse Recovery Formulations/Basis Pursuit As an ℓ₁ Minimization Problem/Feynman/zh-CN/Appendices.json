{
    "hands_on_practices": [
        {
            "introduction": "尽管基追踪（Basis Pursuit）在概念上是最小化$\\ell_1$范数，但其目标函数的非光滑性给直接计算带来了挑战。本练习将指导您完成一个关键的实践步骤：将基追踪问题转化为等价的线性规划（LP）形式。这项技术 () 不仅是理论与可计算性之间的桥梁，也是在实践中应用标准优化求解器的基础。",
            "id": "3433090",
            "problem": "考虑标准基追踪问题，该问题通过在等式约束下最小化$\\ell_1$范数，来寻求与线性测量一致的最稀疏解。设 $A \\in \\mathbb{R}^{m \\times n}$ 且 $y \\in \\mathbb{R}^{m}$，考虑在先行测量约束 $A x = y$ 下，最小化未知向量 $x \\in \\mathbb{R}^{n}$ 的$\\ell_1$范数的优化问题。$\\ell_1$范数 $\\|x\\|_{1}$ 定义为 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。线性规划 (LP) 问题的标准形式为 $\\min c^{\\top} z$，约束为 $G z = h, z \\ge 0$，其中 $c$ 是代价向量，$G$ 是约束矩阵，$h$ 是约束右端向量，$z$ 是非负决策向量。\n\n从上述基本定义出发，通过引入适当的非负辅助变量，利用上镜图视角将$\\ell_1$范数线性化，通过松弛变量将不等式关系转换为等式，并在需要时通过符号分离强制实现非负性，将基追踪问题重构为 LP 标准形式 $\\min c^{\\top} z$，约束为 $G z = h, z \\ge 0$。你的构造必须仅使用线性等式约束和线性目标，并且最终决策向量中的每个变量都必须被约束为非负。请使用 $A$、$y$、标准单位矩阵和适当维度的零矩阵，以分块形式明确指定代价向量 $c$、约束矩阵 $G$ 和右端向量 $h$。\n\n你的最终答案应为一个单一的解析表达式，其中按顺序列出三元组 $(c, G, h)$ 作为单个行矩阵的条目。不需要进行数值计算或四舍五入，也不涉及任何单位。表达式必须是精确的。",
            "solution": "我们从基追踪问题开始，其公式为\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $y \\in \\mathbb{R}^{m}$。$\\ell_1$范数 $\\|x\\|_{1}$ 定义为 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。线性规划 (LP) 问题的标准形式为\n$$\n\\min_{z \\ge 0} c^{\\top} z \\quad \\text{subject to} \\quad G z = h,\n$$\n其中 $c$ 是代价向量，$G$ 是一个矩阵，$h$ 是一个右端向量，且决策向量 $z$ 被约束为逐分量非负。\n\n为了将基追踪问题转换为此 LP 标准形式，我们使用$\\ell_1$范数的上镜图表示法，并结合符号分离和松弛变量。首先，引入一个向量 $t \\in \\mathbb{R}^{n}$ 且 $t \\ge 0$，它逐分量地限定 $x$ 的绝对值，即 $-t \\le x \\le t$。最小化 $\\sum_{i=1}^{n} t_{i}$ 等价于最小化 $\\|x\\|_{1}$。这产生了等价的问题\n$$\n\\min_{x, t} \\ \\mathbf{1}^{\\top} t \\quad \\text{subject to} \\quad A x = y, \\quad -t \\le x \\le t, \\quad t \\ge 0,\n$$\n其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 表示全一向量。\n\n现在我们通过将变量 $x$ 进行符号分离，分解为非负部分，来强制决策向量的非负性。定义 $u, v \\in \\mathbb{R}^{n}$，其中 $u \\ge 0$ 且 $v \\ge 0$，使得 $x = u - v$。\n测量约束变为 $A x = A(u - v) = y$。\n逐分量边界 $-t \\le x \\le t$ 变为两个不等式：\n$$\nu - v \\le t \\quad \\text{和} \\quad v - u \\le t.\n$$\n为了在保持非负约束的同时将这些不等式转换为等式，引入松弛变量 $s^{(1)} \\in \\mathbb{R}^{n}$ 和 $s^{(2)} \\in \\mathbb{R}^{n}$，两者均满足 $s^{(1)} \\ge 0$ 和 $s^{(2)} \\ge 0$，使得\n$$\nt - (u - v) - s^{(1)} = 0 \\quad \\implies \\quad -u + v + t - s^{(1)} = 0,\n$$\n$$\nt - (v - u) - s^{(2)} = 0 \\quad \\implies \\quad u - v + t - s^{(2)} = 0.\n$$\n将所有非负决策变量收集到一个单一向量中\n$$\nz = \\begin{pmatrix} u \\\\ v \\\\ t \\\\ s^{(1)} \\\\ s^{(2)} \\end{pmatrix} \\in \\mathbb{R}^{5n}, \\qquad z \\ge 0.\n$$\n目标 $\\mathbf{1}^{\\top} t$ 是 $z$ 的线性函数，可以用一个代价向量 $c \\in \\mathbb{R}^{5n}$ 表示为\n$$\nc = \\begin{pmatrix} 0_{n} \\\\ 0_{n} \\\\ \\mathbf{1}_{n} \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix},\n$$\n其中 $0_{n}$ 是 $\\mathbb{R}^{n}$ 中的零向量，$\\mathbf{1}_{n}$ 是 $\\mathbb{R}^{n}$ 中的全一向量。等式约束由以下部分组成：\n1. 测量约束：$A u - A v = y$\n2. 松弛等式约束：$-u + v + t - s^{(1)} = 0$ 和 $u - v + t - s^{(2)} = 0$\n\n这些可以紧凑地写成 $G z = h$ 的形式，其中\n$$\nG = \\begin{pmatrix}\nA   -A   0_{m \\times n}   0_{m \\times n}   0_{m \\times n} \\\\\n- I_{n}   I_{n}   I_{n}   - I_{n}   0_{n \\times n} \\\\\nI_{n}   - I_{n}   I_{n}   0_{n \\times n}   - I_{n}\n\\end{pmatrix}, \\qquad\nh = \\begin{pmatrix} y \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix},\n$$\n$I_{n}$ 是 $n \\times n$ 单位矩阵，$0_{p \\times q}$ 表示 $p \\times q$ 的零矩阵。\n\n因此，基追踪问题已被重构为标准的线性规划形式：\n$$\n\\min_{z \\ge 0} \\ c^{\\top} z \\quad \\text{subject to} \\quad G z = h,\n$$\n其中显式的 $c$、$G$ 和 $h$ 如上文所述，用 $A$、$y$ 以及标准单位矩阵和零矩阵构造。该构造使用了一范数的上镜图线性化、用于强制非负性的符号分离以及用于将不等式转换为等式的松弛变量，从而得到了一个纯线性的目标函数和带有非负变量的等式约束。",
            "answer": "$$\\boxed{\\begin{pmatrix}\n\\begin{pmatrix} 0_{n} \\\\ 0_{n} \\\\ \\mathbf{1}_{n} \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix} \n\\begin{pmatrix}\nA   -A   0_{m \\times n}   0_{m \\times n}   0_{m \\times n} \\\\\n- I_{n}   I_{n}   I_{n}   - I_{n}   0_{n \\times n} \\\\\nI_{n}   - I_{n}   I_{n}   0_{n \\times n}   - I_{n}\n\\end{pmatrix} \n\\begin{pmatrix} y \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix}\n\\end{pmatrix}}$$"
        },
        {
            "introduction": "解决了$\\ell_1$最小化问题是否就一定能找到我们期望的最稀疏解？本练习通过一个精心设计的反例揭示，答案并非总是肯定的，传感矩阵 $A$ 的性质至关重要。通过分析这个基追踪失败的案例 ()，您将对零空间性质（Null Space Property, NSP）建立更深刻的直观理解，并认识到为何它是稀疏恢复成功的关键条件。",
            "id": "3433126",
            "problem": "考虑稀疏恢复的基追踪(Basis Pursuit)公式，该方法旨在寻找一个向量 $x \\in \\mathbb{R}^{n}$，在满足线性等式约束 $A x = y$ 的前提下，最小化$\\ell_1$范数。其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $y \\in \\mathbb{R}^{m}$。从稀疏性、$\\ell_1$范数以及由线性系统描述的可行集的核心定义出发。此外，使用零空间性质(Null Space Property, NSP)的定义：如果对于任意非零向量 $h \\in \\ker(A)$ 和任意索引集 $S \\subset \\{1,\\dots,n\\}$ (其中 $|S| \\leq s$)，都有 $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$ 成立，则称矩阵 $A$ 满足 $s$ 阶NSP。\n\n设\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1   0   \\frac{1}{100} \\\\\n0   1   \\frac{1}{100}\n\\end{pmatrix},\n\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n\\frac{1}{100} \\\\\n\\frac{1}{100}\n\\end{pmatrix}.\n$$\n\n你的任务是：\n- 确定 $A x = y$ 的最稀疏解 $x_{0}$ 及其支撑集的大小。\n- 通过从线性约束导出的显式参数化来刻画整个可行集 $\\{x \\in \\mathbb{R}^{3} : A x = y\\}$。\n- 仅使用$\\ell_1$范数的定义以及由绝对值引起的分段线性结构，确定基追踪问题 $\\min \\|x\\|_{1}$ subject to $A x = y$ 的精确最小化子。\n- 判断$\\ell_1$最小化子是否与最稀疏解一致，并证明你的结论。\n- 通过计算最稀疏解 $x_{0}$ 的支撑集 $S$ 的零空间性质比率\n$$\nR_{S} \\;=\\; \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}},\n$$\n来分析导致任何失败的矩阵性质，其中 $h$ 是 $\\ker(A)$ 的一个非零生成元。以精确值表示 $R_{S}$ 的最终结果，不要四舍五入。",
            "solution": "此问题已经过验证。\n\n### 步骤1：提取已知条件\n- 问题是找到一个向量 $x \\in \\mathbb{R}^{n}$，在满足线性约束 $A x = y$ 的前提下，最小化$\\ell_1$范数 $\\|x\\|_{1}$。\n- 矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 由下式给出\n$$A = \\begin{pmatrix} 1   0   \\frac{1}{100} \\\\ 0   1   \\frac{1}{100} \\end{pmatrix}$$\n其中 $m=2$ 且 $n=3$。\n- 向量 $y \\in \\mathbb{R}^{m}$ 由下式给出\n$$y = \\begin{pmatrix} \\frac{1}{100} \\\\ \\frac{1}{100} \\end{pmatrix}$$\n- $s$阶零空间性质(NSP)的定义：如果对于任意非零向量 $h \\in \\ker(A)$ 和任意索引集 $S \\subset \\{1,\\dots,n\\}$ (其中 $|S| \\leq s$)，不等式 $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$ 均成立，则称矩阵 $A$ 满足该性质。此处 $h_S$ 是将向量 $h$ 中不在 $S$ 内的元素置零后得到的向量。\n- 任务是：\n    1. 确定 $A x = y$ 的最稀疏解 $x_{0}$ 及其支撑集的大小。\n    2. 刻画可行集 $\\{x \\in \\mathbb{R}^{3} : A x = y\\}$。\n    3. 确定基追踪问题 $\\min \\|x\\|_{1}$ subject to $A x = y$ 的精确最小化子。\n    4. 判断$\\ell_1$最小化子是否与最稀疏解一致。\n    5. 对于 $x_0$ 的支撑集 $S$ 和 $\\ker(A)$ 的一个生成元 $h$，计算NSP比率 $R_{S} = \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}}$。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题是压缩感知和稀疏优化领域的标准练习。基追踪、$\\ell_1$最小化和零空间性质是该理论的基础概念。\n- **适定性**：问题陈述清晰，提供了所有必要数据。矩阵和向量的维度（$A$ 是 $2 \\times 3$，$x$ 是 $3 \\times 1$，$y$ 是 $2 \\times 1$）是一致的。每个任务都是一个明确定义的数学目标。\n- **客观性**：语言精确、数学化，没有任何主观论断。\n\n该问题是自洽、一致且科学合理的。未检测到任何缺陷。\n\n### 步骤3：结论与行动\n该问题是**有效的**。开始解答。\n\n***\n\n### 解答\n该问题要求对一个特定的基追踪实例进行多部分分析。我们将按顺序完成每个任务。\n\n**1. 确定最稀疏解 $x_{0}$ 及其支撑集大小。**\n$A x = y$ 的最稀疏解是具有最少非零分量的解，即最小化$\\ell_0$-“范数” $\\|x\\|_0 = |\\{i : x_i \\neq 0\\}|$ 的解。我们从稀疏度递增开始寻找解。\n\n- **稀疏度为1**：一个1-稀疏解只有一个非零项。我们来测试三种可能性。\n    - 若 $x = (x_1, 0, 0)^T$，则 $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$。这不可能等于 $y = (1/100, 1/100)^T$。\n    - 若 $x = (0, x_2, 0)^T$，则 $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ x_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ x_2 \\end{pmatrix}$。这不可能等于 $y$。\n    - 若 $x = (0, 0, x_3)^T$，则 $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} x_3/100 \\\\ x_3/100 \\end{pmatrix}$。令其等于 $y$ 得到 $x_3/100 = 1/100$，这意味着 $x_3 = 1$。\n因此，存在一个1-稀疏解：$x_0 = (0, 0, 1)^T$。由于没有解能比1-稀疏解更稀疏（因为 $y \\neq 0$，零向量不是解），所以这是最稀疏解。\n最稀疏解是 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$。其支撑集是 $S_0 = \\{3\\}$，其支撑集大小（稀疏度）是 $|S_0|=1$。\n\n**2. 刻画整个可行集。**\n$A x = y$ 的所有解的集合是一个仿射子空间，可以写成 $x = x_p + h$ 的形式，其中 $x_p$ 是 $A x = y$ 的任意一个特解，$h$ 是 $A$ 的零空间 $\\ker(A)$ 中的任意向量。我们可以使用 $x_p = x_0 = (0, 0, 1)^T$。\n\n为了找到 $\\ker(A)$，我们求解 $A h = 0$，其中 $h = (h_1, h_2, h_3)^T$：\n$$\n\\begin{pmatrix}\n1   0   \\frac{1}{100} \\\\\n0   1   \\frac{1}{100}\n\\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n这得到方程组：\n$h_1 + \\frac{1}{100} h_3 = 0 \\implies h_1 = -\\frac{1}{100} h_3$\n$h_2 + \\frac{1}{100} h_3 = 0 \\implies h_2 = -\\frac{1}{100} h_3$\n零空间由所有形如 $h = c \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$ 的向量组成，其中 $c \\in \\mathbb{R}$ 是任意标量。这个一维零空间的一个生成元是 $h_{\\text{gen}} = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$。\n\n可行集由 $t \\in \\mathbb{R}$ 参数化：\n$$x(t) = x_0 + t \\, h_{\\text{gen}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + t \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -t/100 \\\\ -t/100 \\\\ 1+t \\end{pmatrix}$$\n\n**3. 确定基追踪问题的精确最小化子。**\n我们需要找到最小化$\\ell_1$范数 $\\|x\\|_{1} = \\sum_i |x_i|$ 的解 $x$。利用可行集的参数化，我们对函数 $f(t) = \\|x(t)\\|_{1}$ 关于 $t$ 进行最小化：\n$$f(t) = \\left\\| \\begin{pmatrix} -t/100 \\\\ -t/100 \\\\ 1+t \\end{pmatrix} \\right\\|_1 = \\left|-\\frac{t}{100}\\right| + \\left|-\\frac{t}{100}\\right| + |1+t| = \\frac{2}{100}|t| + |1+t|$$\n这个函数是凸的、分段线性的，在 $t=0$ 和 $t=-1$ 处不可导。最小值必然出现在这些点之一。\n让我们根据 $t$ 的区间来分析该函数：\n- 当 $t \\ge 0$ 时：$f(t) = \\frac{2}{100}t + (1+t) = 1 + (1 + \\frac{2}{100})t = 1 + \\frac{102}{100}t$。斜率为正，因此函数是递增的。\n- 当 $-1 \\le t  0$ 时：$f(t) = \\frac{2}{100}(-t) + (1+t) = 1 + (1 - \\frac{2}{100})t = 1 + \\frac{98}{100}t$。斜率为正，因此函数是递增的。\n- 当 $t  -1$ 时：$f(t) = \\frac{2}{100}(-t) - (1+t) = -1 + (-\\frac{2}{100} - 1)t = -1 - \\frac{102}{100}t$。斜率为负，因此函数是递减的。\n\n该函数在 $t  -1$ 时递减，在 $t > -1$ 时递增。因此，全局最小值在 $t = -1$ 处取得。\n我们将 $t = -1$ 代入我们的参数化中，求得最小化子 $x_{\\ell_1}$：\n$$x_{\\ell_1} = x(-1) = \\begin{pmatrix} -(-1)/100 \\\\ -(-1)/100 \\\\ 1+(-1) \\end{pmatrix} = \\begin{pmatrix} 1/100 \\\\ 1/100 \\\\ 0 \\end{pmatrix}$$\n\n**4. 判断$\\ell_1$最小化子是否与最稀疏解一致。**\n最稀疏解是 $x_0 = (0, 0, 1)^T$。\n$\\ell_1$最小化子是 $x_{\\ell_1} = (1/100, 1/100, 0)^T$。\n这两个向量不相同。最稀疏解是1-稀疏的，而$\\ell_1$最小化子是2-稀疏的。在这种情况下，基追踪未能恢复最稀疏解。\n我们可以检查它们各自的$\\ell_1$范数：\n$\\|x_0\\|_1 = |0| + |0| + |1| = 1$。\n$\\|x_{\\ell_1}\\|_1 = |1/100| + |1/100| + |0| = 2/100 = 1/50$。\n确实，$\\|x_{\\ell_1}\\|_1  \\|x_0\\|_1$。\n\n**5. 分析零空间性质并计算比率 $R_S$。**\n基追踪的失败可以通过违反零空间性质(NSP)来解释。为保证能恢复任何 $s$-稀疏向量，矩阵 $A$ 必须满足 $s$ 阶NSP。这里，最稀疏解 $x_0$ 是1-稀疏的($s=1$)，所以我们检查1阶NSP。条件是：对于任意非零 $h \\in \\ker(A)$ 和任意索引集 $S \\subset \\{1, 2, 3\\}$ (其中 $|S| \\le 1$)，必须有 $\\|h_S\\|_1  \\|h_{S^c}\\|_1$。\n我们来检查最稀疏解的支撑集 $S = S_0 = \\{3\\}$ 和零空间生成元 $h = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$ 是否满足这个条件。支撑集的补集是 $S^c = \\{1, 2\\}$。\n\n我们计算受限范数：\n- $h$ 在支撑集 $S$ 上的部分是 $h_S = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$。其$\\ell_1$范数是 $\\|h_S\\|_1 = |1| = 1$。\n- $h$ 在支撑集 $S$ 之外的部分是 $h_{S^c} = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 0 \\end{pmatrix}$。其$\\ell_1$范数是 $\\|h_{S^c}\\|_1 = |-1/100| + |-1/100| = 2/100 = 1/50$。\n\n对于这个支撑集，NSP条件应为 $\\|h_S\\|_1  \\|h_{S^c}\\|_1$，即 $1  1/50$。这是不成立的。\n问题要求计算比率 $R_S$：\n$$ R_S = \\frac{\\|h_S\\|_1}{\\|h_{S^c}\\|_1} = \\frac{1}{1/50} = 50 $$\n由于 $R_S = 50 > 1$，对于支撑集 $S=\\{3\\}$，1阶零空间性质被违反了。这个违反正是$\\ell_1$最小化未能恢复最稀疏解 $x_0$ 的根本原因。在$\\ell_1$范数的意义上，沿着零空间向量 $h$ 的结构，将信号能量从 $x_0$ 的支撑集转移到其补集上是“更划算的”。",
            "answer": "$$\n\\boxed{50}\n$$"
        },
        {
            "introduction": "理论条件（如零空间性质）通常与传感矩阵更易于衡量的特性（如列相关性）相关联。这项计算实践 () 将引导您通过数值模拟来实证探索这种联系。通过编写程序比较不同矩阵设计下的恢复性能，您将亲眼观察到列相关性如何系统性地影响恢复成功率，并导致“相变”边界的移动，从而在矩阵设计与稀疏恢复的实际极限之间建立起切实的联系。",
            "id": "3433151",
            "problem": "考虑一个无噪声的压缩感知模型，其中信号 $x_0 \\in \\mathbb{R}^n$ 通过一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 进行测量，生成测量值 $y \\in \\mathbb{R}^m$，满足 $y = A x_0$。Basis Pursuit 程序通过求解一个凸优化问题来恢复 $x_0$，该问题在满足精确数据一致性的约束下，最小化重构信号的$\\ell_1$范数。核心的科学问题是，与独立同分布 (i.i.d.) 设计相比，$A$ 中的列相关性是否会降低 Basis Pursuit 的稀疏恢复性能。您的研究必须从第一性原理出发，使用稀疏性、凸优化和线性代数的核心定义。\n\n您必须实现一个完整的程序，该程序：\n- 构建多个感知矩阵 $A$ 的随机系综，包括一个 i.i.d. 高斯系综和一个具有受控块内列相关性的块相关高斯系综。\n- 生成随机的 $k$-稀疏信号 $x_0$ 及其对应的测量值 $y = A x_0$。\n- 对每个实例求解 Basis Pursuit 问题 $\\min \\|x\\|_1$ subject to $A x = y$，以评估是否精确恢复。\n- 对于每个矩阵系综，经验性地确定一个相变指数 $k^\\star$，该指数定义为在少量试验中恢复成功率至少为 0.5（表示为小数）的最大稀疏度 $k$。\n- 报告 i.i.d. 系综和块相关系综的相变指数，以揭示由列相关性引起的偏移。\n\n基础知识（直接使用，无需推导）：\n- 如果一个向量 $x_0 \\in \\mathbb{R}^n$ 最多有 $k$ 个非零项，则称其为 $k$-稀疏的。\n- Basis Pursuit 恢复程序是一个凸优化问题：在约束条件 $A x = y$ 下最小化 $\\|x\\|_1$。\n- 可以通过将 $x$ 分解为非负约束下的正部和负部，使用线性规划来求解$\\ell_1$最小化问题。\n- 标准高斯向量的各分量独立，且服从 $\\mathcal{N}(0,1)$ 分布。\n\n系综的设计：\n- I.i.d. 高斯系综：$A$ 的每一列的元素都独立地从 $\\mathcal{N}(0,1)$ 中抽取，然后归一化为单位 $\\ell_2$ 范数。\n- 块相关高斯系综：固定块大小 $b$ 和相关参数 $\\rho \\in [0,1)$。将 $n$ 列划分为大小为 $b$ 的连续块（最后一个块可能更小）。对于每个块，抽取一个潜在向量 $g \\in \\mathbb{R}^m$，其元素独立地从 $\\mathcal{N}(0,1)$ 中抽取。对于该块中的每一列，独立地抽取一个 $z \\in \\mathbb{R}^m$，其元素独立地从 $\\mathcal{N}(0,1)$ 中抽取，并将该列设置为 $\\sqrt{\\rho}\\, g + \\sqrt{1-\\rho}\\, z$。将每一列归一化为单位 $\\ell_2$ 范数。这种构造在保持每列方差的同时，引入了块内相关性。\n\n恢复成功准则：\n- 令 $\\hat{x}$ 表示对 $y = A x_0$ 的 Basis Pursuit 解。\n- 如果相对$\\ell_2$误差 $\\| \\hat{x} - x_0 \\|_2 / \\| x_0 \\|_2$ 小于 $10^{-3}$（表示为小数），则声明成功，确保该判定是无量纲的。\n\n相变指数：\n- 对于固定的 $m$ 和 $n$，定义一个稀疏度 $k$ 的网格 $k \\in \\{2,4,6,8,10,12,14\\}$。\n- 对于每个 $k$ 和每个系综，进行 $T = 6$ 次独立试验，每次试验都抽取一个新的 $k$-稀疏 $x_0$，其支撑集随机选择，非零振幅为从 $\\{-1, +1\\}$ 中随机抽取的符号与从 $[0.5, 1.5]$ 中均匀抽取的幅值的独立乘积。对于每次试验，计算 $y = A x_0$，求解 Basis Pursuit，并测试是否成功。\n- 稀疏度为 $k$ 时的经验成功率是成功试验次数占总试验次数 $T$ 的比例。\n- 对于一个系综，将 $k^\\star$ 定义为网格中使得经验成功率至少为 0.5（表示为小数）的最大 $k$。如果没有 $k$ 能达到至少 0.5 的成功率，则设 $k^\\star = 0$。\n\n测试套件：\n- 使用 $m = 48$ 和 $n = 128$。\n- 使用块大小 $b = 8$。\n- 评估三个系综：\n  1. I.i.d. 高斯设计。\n  2. 相关系数 $\\rho = 0.3$ 的块相关设计。\n  3. 相关系数 $\\rho = 0.6$ 的块相关设计。\n- 对于所有系综，将列归一化为单位 $\\ell_2$ 范数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个相变指数，按以下顺序以逗号分隔并用方括号括起：i.i.d.、$\\rho = 0.3$、$\\rho = 0.6$。例如，输出行可能看起来像 `[12,10,8]`。\n- 此问题不涉及物理单位；所有量均为无量纲。\n- 程序必须是自包含的，不需要用户输入或外部文件。它必须固定其随机种子以确保可复现的输出。\n\n您的实现必须遵循稍后指定的运行时环境，并且必须通过正确的线性规划形式求解 Basis Pursuit 程序，不得依赖指定之外的外部库。",
            "solution": "我们从核心定义开始。一个向量 $x_0 \\in \\mathbb{R}^n$ 如果其最多有 $k$ 个非零项，则称其为 $k$-稀疏的。无噪声测量模型为 $y = A x_0$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是感知矩阵，$y \\in \\mathbb{R}^m$ 是测量值。Basis Pursuit 是一个凸优化程序 $\\min \\|x\\|_1$ subject to $A x = y$，它在线性约束下促进稀疏性。\n\n算法方法的原理性论证：\n- $\\ell_1$最小化程序是在一个多面体可行集上的凸优化问题，可以通过变量分裂作为线性规划问题来求解。引入非负变量 $u \\in \\mathbb{R}^n$ 和 $v \\in \\mathbb{R}^n$，使得 $x = u - v$ 且 $u \\ge 0, v \\ge 0$。当 $u$ 和 $v$ 如上约束时，$\\ell_1$范数变为 $\\|x\\|_1 = \\sum_{i=1}^n |x_i| = \\sum_{i=1}^n (u_i + v_i)$。等式约束变为 $A(u - v) = y$。这就产生了一个线性规划问题：\n$$\n\\min_{u,v \\in \\mathbb{R}^n} \\;\\; \\sum_{i=1}^n (u_i + v_i) \\quad \\text{subject to} \\quad A u - A v = y, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\n一个解 $(u^\\star, v^\\star)$ 提供了 Basis Pursuit 解 $x^\\star = u^\\star - v^\\star$。这种形式符合线性规划理论，并允许通过标准算法求解。\n\n- 在无噪声设置下，Basis Pursuit 的恢复成功与否取决于 $A$ 的零空间和由$\\ell_1$球定义的多面体的几何特性，而这些特性受列相关性的影响。增加的块内相关性会增大互相关性，从而降低可恢复的稀疏度。虽然我们不假设或使用显式的相变公式，但我们可以通过评估不同稀疏度下的成功率来经验性地估计相变指数 $k^\\star$。\n\n系综的设计：\n- 对于 i.i.d. 高斯系综，我们独立地从 $\\mathcal{N}(0,1)$ 中抽取 $A$ 的每个元素，并将列归一化为单位 $\\ell_2$ 范数以固定尺度。这种归一化在标准化列范数的同时保持了各向同性，这有助于控制互相关性的测量并使线性规划在数值上更稳定。\n\n- 对于块相关系综，我们将列划分为大小为 $b$ 的连续块。对于每个块，我们抽取一个潜在向量 $g \\in \\mathbb{R}^m$，其元素独立地从 $\\mathcal{N}(0,1)$ 中抽取。对于该块中的每一列，我们独立地抽取一个 $z \\in \\mathbb{R}^m$，其元素也独立地从 $\\mathcal{N}(0,1)$ 中抽取，并将该列构造为 $\\sqrt{\\rho}\\, g + \\sqrt{1-\\rho}\\, z$。这种构造对于同一块中的两列，在归一化之前产生大约为 $\\rho I_m$ 的互协方差，从而嵌入了结构化的相关性。在生成所有列之后，我们将每一列归一化为单位 $\\ell_2$ 范数。较大的 $\\rho$ 会增加块内对齐程度，这往往会阻碍 Basis Pursuit 对较大稀疏度的恢复。\n\n稀疏信号的生成与成功准则：\n- 对于每个稀疏度 $k$，我们通过均匀抽取一个大小为 $k$ 的支撑集 $S \\subset \\{1,\\dots,n\\}$ 来生成随机的 $x_0$。对于 $i \\in S$，我们设置 $x_{0,i} = s_i \\cdot a_i$，其中 $s_i \\in \\{-1,+1\\}$ 是一个随机符号，$a_i$ 是从 $[0.5, 1.5]$ 中均匀抽取的随机幅值。对于 $i \\notin S$，我们设置 $x_{0,i} = 0$。这避免了数值上过小的项，并平衡了符号。\n\n- 我们计算 $y = A x_0$ 并如前所述求解 Basis Pursuit 的线性规划问题。令 $\\hat{x}$ 表示恢复的向量。如果 $\\|\\hat{x} - x_0\\|_2 / \\|x_0\\|_2  10^{-3}$，我们声明成功，这是一个严格的相对精度阈值。\n\n经验性相变估计：\n- 对于每个系综和每个 $k \\in \\{2,4,6,8,10,12,14\\}$，我们进行 $T = 6$ 次独立试验，并计算经验成功率，即成功恢复的比例。我们将相变指数 $k^\\star$ 定义为成功率至少为 0.5 的最大 $k$。如果没有 $k$ 满足此准则，我们设 $k^\\star = 0$。这个定义捕捉了随着稀疏度增加而开始出现失败的临界点，并提供了一个简单、可量化的边界。\n\n数值考虑：\n- 对 $A$ 进行列归一化可以减少列尺度的可变性，并使优化过程更加稳定。\n- 该线性规划问题涉及 $2n$ 个变量和 $m$ 个等式约束，对于 $m = 48$ 和 $n = 128$ 来说是可解的。\n- 固定的随机种子确保了经验结果的可复现性。\n\n预期的定性结果：\n- i.i.d. 系综通常比相关系综允许更高的 $k^\\star$，因为列相关性增加了互相关性，并减少了可用于稀疏支撑集分离的有效维度。\n- 随着 $\\rho$ 从 0.3 增加到 0.6，我们预期会得到一个非递增的相变指数序列，这反映了更强的相关性对 Basis Pursuit 恢复的不利影响。\n\n程序将计算指定测试套件的三个相变指数，并以要求的单行格式 $[k_{\\text{iid}}, k_{\\rho=0.3}, k_{\\rho=0.6}]$ 打印它们，其中每一项都是一个整数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef normalize_columns(A: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Normalize columns of A to have unit l2 norm.\n    \"\"\"\n    col_norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero by replacing zero norms with one (though unlikely with Gaussian draws)\n    col_norms[col_norms == 0] = 1.0\n    return A / col_norms\n\ndef generate_iid_gaussian_matrix(m: int, n: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generate an i.i.d. Gaussian matrix with unit-norm columns.\n    \"\"\"\n    A = rng.standard_normal((m, n))\n    return normalize_columns(A)\n\ndef generate_block_correlated_gaussian_matrix(m: int, n: int, block_size: int, rho: float, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generate a block-correlated Gaussian matrix with unit-norm columns.\n    Each block shares a latent vector g, and each column is sqrt(rho)*g + sqrt(1-rho)*z,\n    then columns are normalized to unit l2 norm.\n    \"\"\"\n    A = np.zeros((m, n), dtype=float)\n    num_blocks = (n + block_size - 1) // block_size\n    idx = 0\n    sqrt_rho = np.sqrt(max(0.0, min(1.0, rho)))\n    sqrt_1mr = np.sqrt(max(0.0, 1.0 - rho))\n    for b in range(num_blocks):\n        start = b * block_size\n        end = min((b + 1) * block_size, n)\n        size = end - start\n        g = rng.standard_normal(m)\n        for j in range(size):\n            z = rng.standard_normal(m)\n            col = sqrt_rho * g + sqrt_1mr * z\n            A[:, start + j] = col\n    return normalize_columns(A)\n\ndef generate_sparse_signal(n: int, k: int, rng: np.random.Generator) -> np.ndarray:\n    \"\"\"\n    Generate a k-sparse signal of dimension n.\n    Nonzero entries are random signs multiplied by magnitudes uniformly in [0.5, 1.5].\n    \"\"\"\n    x0 = np.zeros(n, dtype=float)\n    if k == 0:\n        return x0\n    support = rng.choice(n, size=k, replace=False)\n    signs = rng.choice([-1.0, 1.0], size=k)\n    magnitudes = rng.uniform(0.5, 1.5, size=k)\n    x0[support] = signs * magnitudes\n    return x0\n\ndef basis_pursuit_via_lp(A: np.ndarray, y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Solve Basis Pursuit: min ||x||_1 s.t. A x = y\n    via LP on variables (u, v) >= 0 with x = u - v.\n    Objective: minimize sum(u + v)\n    Constraints: A u - A v = y\n    \"\"\"\n    m, n = A.shape\n    # Objective: c = [1,...,1, 1,...,1] for u and v\n    c = np.ones(2 * n, dtype=float)\n    # Equality constraints: A_eq @ [u; v] = y, where A_eq = [A, -A]\n    A_eq = np.hstack((A, -A))\n    b_eq = y.copy()\n    bounds = [(0.0, None)] * (2 * n)\n\n    # Solve LP\n    res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n    if not res.success:\n        # If solver fails, return NaNs to signify failure.\n        return np.full(n, np.nan)\n    x_hat = res.x[:n] - res.x[n:]\n    return x_hat\n\ndef exact_recovery(x_hat: np.ndarray, x0: np.ndarray, tol: float = 1e-3) -> bool:\n    \"\"\"\n    Decide exact recovery by relative l2 error threshold.\n    \"\"\"\n    if np.any(np.isnan(x_hat)):\n        return False\n    norm_x0 = np.linalg.norm(x0)\n    if norm_x0 == 0.0:\n        # Treat zero signal as recovered if estimate is numerically zero\n        return np.linalg.norm(x_hat)  tol\n    rel_err = np.linalg.norm(x_hat - x0) / norm_x0\n    return rel_err  tol\n\ndef evaluate_phase_transition(m: int, n: int, block_size: int, ensemble_type: str, rho: float,\n                              k_grid: list, trials: int, rng: np.random.Generator) -> int:\n    \"\"\"\n    Evaluate empirical phase transition index k* as the largest k in k_grid\n    with success rate >= 0.5, for the specified ensemble.\n    \"\"\"\n    # Generate a new matrix A for each ensemble evaluation.\n    if ensemble_type == \"iid\":\n        A = generate_iid_gaussian_matrix(m, n, rng)\n    elif ensemble_type == \"corr\":\n        A = generate_block_correlated_gaussian_matrix(m, n, block_size, rho, rng)\n    else:\n        raise ValueError(\"Unknown ensemble type.\")\n\n    k_star = 0\n    for k in k_grid:\n        success_count = 0\n        for _ in range(trials):\n            x0 = generate_sparse_signal(n, k, rng)\n            y = A @ x0\n            x_hat = basis_pursuit_via_lp(A, y)\n            if exact_recovery(x_hat, x0):\n                success_count += 1\n        success_rate = success_count / trials\n        if success_rate >= 0.5:\n            # Update k_star to this k if it meets the criterion\n            k_star = k\n    return k_star\n\ndef solve():\n    # Define the test cases from the problem statement.\n    m = 48\n    n = 128\n    block_size = 8\n    k_grid = [2, 4, 6, 8, 10, 12, 14]\n    trials = 6\n\n    # Fixed random seed for reproducibility\n    rng = np.random.default_rng(0)\n\n    test_cases = [\n        (\"iid\", 0.0),\n        (\"corr\", 0.3),\n        (\"corr\", 0.6),\n    ]\n\n    results = []\n    for ensemble_type, rho in test_cases:\n        k_star = evaluate_phase_transition(m, n, block_size, ensemble_type, rho, k_grid, trials, rng)\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}