{
    "hands_on_practices": [
        {
            "introduction": "To build intuition for structured random matrices, we first examine what happens when the measurement system is highly structured and *not* random. This exercise explores the fundamental link between coherence and the Restricted Isometry Property (RIP) by analyzing a worst-case scenario . By using the identity matrix—a maximally coherent system—and a simple subsampling operator, you will see how easily the RIP can be violated, providing a concrete understanding of why the incoherence offered by random matrices is so crucial for compressed sensing.",
            "id": "3482545",
            "problem": "Let $n \\in \\mathbb{N}$ and let $U \\in \\mathbb{R}^{n \\times n}$ be an orthonormal system. The coherence of $U$ is defined by\n$$\n\\mu(U) \\equiv n \\cdot \\max_{1 \\leq i,j \\leq n} |U_{ij}|^{2}.\n$$\nGiven a subset $\\Omega \\subset \\{1,\\dots,n\\}$ of cardinality $m$ chosen uniformly without replacement, define the uniform row subsampling operator $P_{\\Omega} \\in \\mathbb{R}^{m \\times n}$ that selects the rows indexed by $\\Omega$, and the measurement matrix $A = P_{\\Omega} U \\in \\mathbb{R}^{m \\times n}$. For $s \\in \\mathbb{N}$, the $s$-restricted isometry constant $\\delta_{s}(A)$ is the smallest $\\delta \\geq 0$ such that for every $s$-sparse vector $x \\in \\mathbb{R}^{n}$,\n$$\n(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}.\n$$\n\nConsider the high-coherence orthonormal system $U = I_{n}$ (the identity), with $n = 4096$ and $m = 1024$ so that $\\Omega \\subset \\{1,\\dots,n\\}$ satisfies $|\\Omega| = m  n$. Using only the definitions above as the fundamental base, do the following:\n- Compute $\\mu(U)$.\n- Construct an explicit $1$-sparse vector $x^{\\star} \\in \\mathbb{R}^{n}$ that exhibits the worst-case lower-bound violation in the restricted isometry inequality for $A = P_{\\Omega} U$ under the given $m$ and $n$.\n- From first principles, determine the exact value of the $1$-restricted isometry constant $\\delta_{1}(A)$.\n\nProvide as your final answer the exact value of $\\delta_{1}(A)$ as a single real number. No rounding is required, and no units are involved.",
            "solution": "The problem statement is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- $n \\in \\mathbb{N}$\n- $U \\in \\mathbb{R}^{n \\times n}$ is an orthonormal system.\n- Coherence: $\\mu(U) \\equiv n \\cdot \\max_{1 \\leq i,j \\leq n} |U_{ij}|^{2}$.\n- $\\Omega \\subset \\{1,\\dots,n\\}$, $|\\Omega| = m$, chosen uniformly without replacement.\n- $P_{\\Omega} \\in \\mathbb{R}^{m \\times n}$ is the uniform row subsampling operator.\n- $A = P_{\\Omega} U \\in \\mathbb{R}^{m \\times n}$.\n- $s$-restricted isometry constant $\\delta_{s}(A)$ is the smallest $\\delta \\geq 0$ such that for every $s$-sparse vector $x \\in \\mathbb{R}^{n}$, $(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}$.\n- Specific instance: $U = I_{n}$ (the $n \\times n$ identity matrix).\n- Specific values: $n = 4096$, $m = 1024$.\n- Tasks:\n    1. Compute $\\mu(U)$.\n    2. Construct an explicit $1$-sparse vector $x^{\\star} \\in \\mathbb{R}^{n}$ that exhibits the worst-case lower-bound violation.\n    3. Determine the exact value of the $1$-restricted isometry constant $\\delta_{1}(A)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. All definitions and constants are standard within the mathematical field of compressed sensing. The problem is self-contained and provides all necessary information to derive a unique, meaningful solution for the quantities requested. The choice of $U=I_n$ with row subsampling is a canonical example used to illustrate the concepts of coherence and the Restricted Isometry Property (RIP). The problem is not trivial as it requires a careful application of the fundamental definitions. The problem is thus deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution is provided below.\n\nThe solution proceeds by addressing the three tasks specified in the problem statement.\n\n**Part 1: Computation of Coherence $\\mu(U)$**\n\nThe orthonormal system is given as the identity matrix, $U = I_n \\in \\mathbb{R}^{n \\times n}$. The entries of $U$ are given by the Kronecker delta, $U_{ij} = \\delta_{ij}$, where $\\delta_{ij} = 1$ if $i=j$ and $\\delta_{ij} = 0$ if $i \\neq j$.\n\nThe coherence $\\mu(U)$ is defined as:\n$$\n\\mu(U) \\equiv n \\cdot \\max_{1 \\leq i,j \\leq n} |U_{ij}|^{2}\n$$\nWe first evaluate the maximum squared entry of $U = I_n$:\n$$\n\\max_{1 \\leq i,j \\leq n} |(I_n)_{ij}|^{2} = \\max_{1 \\leq i,j \\leq n} |\\delta_{ij}|^{2}\n$$\nThe value of $|\\delta_{ij}|^2$ is $1^2=1$ when $i=j$ and $0^2=0$ when $i \\neq j$. The maximum of these values is clearly $1$.\n$$\n\\max_{1 \\leq i,j \\leq n} |\\delta_{ij}|^{2} = 1\n$$\nSubstituting this into the definition of coherence, with $n=4096$:\n$$\n\\mu(I_n) = n \\cdot 1 = n = 4096\n$$\nThis system has the maximum possible coherence, which is characteristic of sparse bases in their own domain.\n\n**Part 2: Construction of a Worst-Case $1$-Sparse Vector $x^{\\star}$**\n\nThe measurement matrix is $A = P_{\\Omega} U = P_{\\Omega} I_n = P_{\\Omega}$. $A$ is an $m \\times n$ matrix, where $m=1024$ and $n=4096$. The operator $P_\\Omega$ selects the rows of $I_n$ indexed by the set $\\Omega \\subset \\{1, \\dots, n\\}$ with $|\\Omega| = m$.\n\nLet's analyze the columns of $A$. Let $a_j \\in \\mathbb{R}^m$ be the $j$-th column of $A$.\n$A e_j = a_j$, where $e_j \\in \\mathbb{R}^n$ is the $j$-th standard basis vector.\n$A e_j = P_\\Omega e_j$. The vector $P_\\Omega e_j$ is formed by taking the $j$-th column of $I_n$ (which is $e_j$) and keeping only the rows with indices in $\\Omega$.\n\n- If the index $j$ is in the set of selected rows, $j \\in \\Omega$, then the $j$-th row is selected. The vector $e_j$ has a $1$ at position $j$ and zeros elsewhere. When subsampled by $P_\\Omega$, the resulting column $a_j$ will contain a single $1$ at the position corresponding to the index $j$ in the new enumeration of rows, and zeros elsewhere. Thus, $a_j$ is a standard basis vector in $\\mathbb{R}^m$. The squared Euclidean norm is $\\|a_j\\|_2^2 = 1$.\n\n- If the index $j$ is not in the set of selected rows, $j \\notin \\Omega$, then the $j$-th row is discarded. Since the $j$-th column $e_j$ only has a non-zero entry in the $j$-th row, and this row is not selected, the resulting column $a_j$ is the zero vector in $\\mathbb{R}^m$. The squared Euclidean norm is $\\|a_j\\|_2^2 = 0$.\n\nThe restricted isometry inequality is $(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}$. The worst-case lower-bound violation occurs for a vector $x$ that minimizes the ratio $\\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2}$.\n\nLet's consider a generic $1$-sparse vector $x \\in \\mathbb{R}^n$. It can be written as $x = c e_j$ for some scalar $c \\in \\mathbb{R} \\setminus \\{0\\}$ and index $j \\in \\{1,\\dots,n\\}$.\nFor such a vector, $\\|x\\|_2^2 = \\|c e_j\\|_2^2 = c^2 \\|e_j\\|_2^2 = c^2$.\nThe squared norm of its measurement is $\\|Ax\\|_2^2 = \\|A (c e_j)\\|_2^2 = c^2 \\|A e_j\\|_2^2 = c^2 \\|a_j\\|_2^2$.\nThe ratio is $\\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2} = \\|a_j\\|_2^2$.\n\nTo find the worst-case violation, we must find the index $j$ that minimizes $\\|a_j\\|_2^2$. Based on our analysis of the columns of $A$:\n$$\n\\min_{j=1,\\dots,n} \\|a_j\\|_2^2 = 0\n$$\nThis minimum is achieved for any index $j$ such that $j \\notin \\Omega$. Since $m=1024  n=4096$, the set $\\{1,\\dots,n\\} \\setminus \\Omega$ is non-empty and contains $n-m = 3072$ indices.\n\nWe can therefore construct an explicit worst-case vector $x^\\star$ by choosing any index $j_0 \\notin \\Omega$. Let $j_0$ be any integer in $\\{1, \\dots, 4096\\} \\setminus \\Omega$. An explicit vector exhibiting the worst-case lower-bound violation is:\n$$\nx^{\\star} = e_{j_0}\n$$\nFor this vector, $\\|x^{\\star}\\|_2^2 = 1$, and since $j_0 \\notin \\Omega$, $\\|A x^{\\star}\\|_2^2 = \\|a_{j_0}\\|_2^2 = 0$. The lower-bound inequality for $s=1$ becomes $(1-\\delta_1)\\|x^{\\star}\\|_2^2 \\leq \\|Ax^{\\star}\\|_2^2$, which is $(1-\\delta_1) \\cdot 1 \\leq 0$, implying $\\delta_1 \\geq 1$.\n\n**Part 3: Determination of the $1$-Restricted Isometry Constant $\\delta_1(A)$**\n\nThe $1$-restricted isometry constant $\\delta_1(A)$ is the smallest non-negative number $\\delta$ that satisfies the inequality for all $1$-sparse vectors $x$. As shown above, for any $1$-sparse vector $x=ce_j$, the inequality is equivalent to:\n$$\n1-\\delta \\leq \\|a_j\\|_2^2 \\leq 1+\\delta\n$$\nThis must hold true for all possible choices of the index $j \\in \\{1, \\dots, n\\}$. This is equivalent to satisfying the following two conditions simultaneously:\n$$\n1 - \\delta \\leq \\min_{j=1,\\dots,n} \\|a_j\\|_2^2\n$$\n$$\n\\max_{j=1,\\dots,n} \\|a_j\\|_2^2 \\leq 1 + \\delta\n$$\nFrom Part 2, we have determined the minimum and maximum squared norms of the columns of $A$:\n- Since $m  n$, there exists at least one column $a_j$ which is the zero vector, so $\\min_{j=1,\\dots,n} \\|a_j\\|_2^2 = 0$.\n- Since $m  0$, there exists at least one column $a_j$ which is a standard basis vector in $\\mathbb{R}^m$, so $\\max_{j=1,\\dots,n} \\|a_j\\|_2^2 = 1$.\n\nSubstituting these values into the inequalities:\n1. $1 - \\delta \\leq 0 \\implies \\delta \\geq 1$.\n2. $1 \\leq 1 + \\delta \\implies \\delta \\geq 0$.\n\nBoth conditions must be satisfied. The most restrictive condition is $\\delta \\geq 1$.\nThe constant $\\delta_1(A)$ is defined as the *smallest* non-negative $\\delta$ satisfying these conditions. Therefore, the exact value is:\n$$\n\\delta_1(A) = 1\n$$\nThis result is independent of the specific choice of the subset $\\Omega$, as long as it is a proper subset of $\\{1, \\dots, n\\}$, which is guaranteed by $m  n$.",
            "answer": "$$\n\\boxed{1}\n$$"
        },
        {
            "introduction": "Having seen how high coherence can prevent sparse recovery, we now turn to a constructive example of a well-designed structured random matrix. This practice focuses on the partial circulant matrix, which leverages random convolutions to create an efficient and effective measurement scheme . Calculating the mutual coherence for this ensemble will demonstrate how randomness produces near-ideal incoherence with the standard basis, a key design principle that ensures information from a sparse signal is captured robustly across all measurements.",
            "id": "3482574",
            "problem": "Consider a discrete circular convolution model of length $n \\in \\mathbb{N}$. Let $h \\in \\{-1,+1\\}^{n}$ be a generating vector with independent and identically distributed Rademacher entries, and let $C(h) \\in \\mathbb{R}^{n \\times n}$ be the circulant matrix implementing circular convolution by $h$, defined by $(C(h)x)_{k} = \\sum_{j=0}^{n-1} h_{(k-j) \\bmod n} x_{j}$ for any $x \\in \\mathbb{R}^{n}$. Let $\\Omega \\subset \\{0,1,\\ldots,n-1\\}$ be a uniformly random subset of cardinality $m$ with $1 \\leq m \\leq n$, and let $R_{\\Omega} \\in \\{0,1\\}^{m \\times n}$ be the row-selection operator that extracts the rows of $C(h)$ indexed by $\\Omega$. Define the partial circulant ensemble $A = R_{\\Omega} C(h) \\in \\mathbb{R}^{m \\times n}$. For $k \\in \\Omega$, denote the $k$-th row of $A$ by $a_{k} \\in \\mathbb{R}^{n}$ and its unit-norm version by $\\tilde{a}_{k} = a_{k}/\\|a_{k}\\|_{2}$. Let the canonical basis of $\\mathbb{R}^{n}$ be $\\{e_{j}\\}_{j=0}^{n-1}$, collected as the identity matrix $I_{n}$.\n\nThe mutual coherence between two collections of unit vectors $\\{\\tilde{a}_{k}\\}_{k \\in \\Omega}$ and $\\{e_{j}\\}_{j=0}^{n-1}$ is defined as\n$$\\mu(\\tilde{A}, I_{n}) = \\max_{k \\in \\Omega} \\max_{0 \\leq j \\leq n-1} \\left| \\langle \\tilde{a}_{k}, e_{j} \\rangle \\right|.$$\nCompute the exact value of $\\mu(\\tilde{A}, I_{n})$ as a function of $n$. Then, based on your computation, reason from first principles about how this coherence value influences compressed sensing guarantees for signals that are sparse in convolutional dictionaries (for example, shift-invariant collections of localized atoms), explicitly noting the role of $\\mu(\\tilde{A}, I_{n})$ in standard recovery analyses. Express the final answer for $\\mu(\\tilde{A}, I_{n})$ as a closed-form analytic expression. No rounding is required, and no units are involved.",
            "solution": "The user wants me to first validate a problem from the field of compressed sensing and then provide a solution. The problem involves computing the mutual coherence of a specific structured random matrix ensemble and interpreting the result.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   $n \\in \\mathbb{N}$: dimension of the space, length of circular convolution.\n*   $h \\in \\{-1,+1\\}^{n}$: a generating vector with independent and identically distributed (i.i.d.) Rademacher entries.\n*   $C(h) \\in \\mathbb{R}^{n \\times n}$: the circulant matrix such that for any $x \\in \\mathbb{R}^{n}$, $(C(h)x)_{k} = \\sum_{j=0}^{n-1} h_{(k-j) \\bmod n} x_{j}$.\n*   $\\Omega \\subset \\{0,1,\\ldots,n-1\\}$: a uniformly random subset of indices of cardinality $m$, with $1 \\leq m \\leq n$.\n*   $R_{\\Omega} \\in \\{0,1\\}^{m \\times n}$: the row-selection operator corresponding to $\\Omega$.\n*   $A = R_{\\Omega} C(h) \\in \\mathbb{R}^{m \\times n}$: the sensing matrix, a partial circulant matrix.\n*   $a_{k}$: the $k$-th row of $A$ for $k \\in \\Omega$.\n*   $\\tilde{a}_{k} = a_{k}/\\|a_{k}\\|_{2}$: the unit-norm version of $a_{k}$.\n*   $\\{e_{j}\\}_{j=0}^{n-1}$: the canonical basis vectors of $\\mathbb{R}^{n}$, which form the columns of the identity matrix $I_{n}$.\n*   Mutual Coherence Definition: $\\mu(\\tilde{A}, I_{n}) = \\max_{k \\in \\Omega} \\max_{0 \\leq j \\leq n-1} \\left| \\langle \\tilde{a}_{k}, e_{j} \\rangle \\right|$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded, situated within the standard mathematical framework of compressed sensing and linear algebra. All terms like \"circulant matrix,\" \"Rademacher entries,\" and \"mutual coherence\" are well-defined and standard in the field. The setup is formal and objective.\n\nTo check if the problem is well-posed, we must determine if the quantity $\\mu(\\tilde{A}, I_{n})$ can be computed as a deterministic value. The matrix $A$ depends on the random vector $h$ and the random set $\\Omega$. Let's analyze the term inside the maximization: $\\left| \\langle \\tilde{a}_{k}, e_{j} \\rangle \\right|$.\n\nFor an index $k \\in \\Omega$, the vector $a_k$ is the $k$-th row of the full circulant matrix $C(h)$. From the definition of the convolution, the $k$-th row of $C(h)$ is the vector $(h_{k \\bmod n}, h_{(k-1) \\bmod n}, \\ldots, h_{(k-(n-1)) \\bmod n})$. Let's denote this vector as $c_k^T$. Thus, $a_k = c_k$.\n\nFirst, we compute the Euclidean norm of $a_k$. The components of $a_k$ are a cyclic shift of the components of $h$.\n$$\n\\|a_{k}\\|_{2}^{2} = \\sum_{j=0}^{n-1} (h_{(k-j) \\bmod n})^{2}\n$$\nSince the indices $(k-j) \\bmod n$ for $j=0, \\ldots, n-1$ cover the set $\\{0, \\ldots, n-1\\}$ exactly once, this sum is equivalent to summing over the components of $h$.\n$$\n\\|a_{k}\\|_{2}^{2} = \\sum_{l=0}^{n-1} h_{l}^{2}\n$$\nGiven that $h_l \\in \\{-1, +1\\}$, we have $h_{l}^{2} = 1$ for all $l$.\n$$\n\\|a_{k}\\|_{2}^{2} = \\sum_{l=0}^{n-1} 1 = n\n$$\nTherefore, $\\|a_{k}\\|_{2} = \\sqrt{n}$. This norm is a deterministic value, independent of the specific realization of $h$ and the index $k$. The normalized row is $\\tilde{a}_{k} = a_{k} / \\sqrt{n}$.\n\nNext, we compute the inner product $\\langle a_{k}, e_{j} \\rangle$. This inner product extracts the $j$-th component of the vector $a_k$.\n$$\n\\langle a_{k}, e_{j} \\rangle = (a_k)_j = h_{(k-j) \\bmod n}\n$$\nNow, we can evaluate the full expression for the inner product involving the normalized vector.\n$$\n\\langle \\tilde{a}_{k}, e_{j} \\rangle = \\frac{\\langle a_{k}, e_{j} \\rangle}{\\|a_{k}\\|_{2}} = \\frac{h_{(k-j) \\bmod n}}{\\sqrt{n}}\n$$\nTaking the absolute value, we get:\n$$\n\\left| \\langle \\tilde{a}_{k}, e_{j} \\rangle \\right| = \\left| \\frac{h_{(k-j) \\bmod n}}{\\sqrt{n}} \\right| = \\frac{|h_{(k-j) \\bmod n}|}{\\sqrt{n}}\n$$\nSince $|h_l|=1$ for all $l$, we obtain:\n$$\n\\left| \\langle \\tilde{a}_{k}, e_{j} \\rangle \\right| = \\frac{1}{\\sqrt{n}}\n$$\nThis value is constant for all $k \\in \\Omega$ and all $j \\in \\{0, \\ldots, n-1\\}$, and it does not depend on the random choices of $h$ or $\\Omega$.\n\nFinally, we compute the mutual coherence by taking the maximum over all allowed indices:\n$$\n\\mu(\\tilde{A}, I_{n}) = \\max_{k \\in \\Omega} \\max_{0 \\leq j \\leq n-1} \\left( \\frac{1}{\\sqrt{n}} \\right) = \\frac{1}{\\sqrt{n}}\n$$\nThe result is a unique, deterministic function of $n$. The problem is therefore well-posed, self-contained, and scientifically sound.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. We will proceed to provide a complete solution.\n\n### Solution\n\nThe problem asks for the computation of the mutual coherence $\\mu(\\tilde{A}, I_{n})$ and a discussion of its implications for compressed sensing.\n\n**Part 1: Computation of Mutual Coherence**\n\nThe mutual coherence between the collection of normalized rows of $A$, $\\{\\tilde{a}_{k}\\}_{k \\in \\Omega}$, and the canonical basis vectors, $\\{e_{j}\\}_{j=0}^{n-1}$ (columns of $I_n$), is defined as:\n$$\n\\mu(\\tilde{A}, I_{n}) = \\max_{k \\in \\Omega} \\max_{0 \\leq j \\leq n-1} \\left| \\langle \\tilde{a}_{k}, e_{j} \\rangle \\right|\n$$\nThe calculation proceeds by analyzing the term $\\langle \\tilde{a}_{k}, e_{j} \\rangle$, which is the inner product between a normalized row of the sensing matrix and a vector from the canonical basis.\n\nFirst, identify the structure of a row vector $a_k$. For any $k \\in \\Omega$, $a_k$ is the $k$-th row of the circulant matrix $C(h)$. Based on the definition of circular convolution, the $i$-th row of $C(h)$ is a cyclically shifted version of the generating vector $h$. Specifically, the $j$-th element of the $k$-th row is $(a_k)_j = h_{(k-j) \\bmod n}$.\n\nSecond, we determine the norm of $a_k$. The squared $l_2$-norm is the sum of the squares of its components:\n$$\n\\|a_k\\|_2^2 = \\sum_{j=0}^{n-1} ((a_k)_j)^2 = \\sum_{j=0}^{n-1} (h_{(k-j) \\bmod n})^2\n$$\nThe set of indices $\\{(k-j) \\bmod n \\mid j=0, \\ldots, n-1\\}$ is a permutation of $\\{0, \\ldots, n-1\\}$. Thus, the sum is equivalent to the sum of squares of all elements in $h$:\n$$\n\\|a_k\\|_2^2 = \\sum_{l=0}^{n-1} h_l^2\n$$\nSince each entry $h_l$ is from the set $\\{-1, +1\\}$, we have $h_l^2=1$.\n$$\n\\|a_k\\|_2^2 = \\sum_{l=0}^{n-1} 1 = n\n$$\nThis gives the norm $\\|a_k\\|_2 = \\sqrt{n}$. Importantly, this value is identical for all rows $a_k$ and is independent of the specific realization of the Rademacher vector $h$.\nThe normalized row is $\\tilde{a}_k = a_k / \\|a_k\\|_2 = a_k / \\sqrt{n}$.\n\nThird, we compute the inner product $\\langle \\tilde{a}_k, e_j \\rangle$. The inner product $\\langle a_k, e_j \\rangle$ simply isolates the $j$-th component of $a_k$.\n$$\n\\langle a_k, e_j \\rangle = (a_k)_j = h_{(k-j) \\bmod n}\n$$\nSo, the inner product with the normalized vector is:\n$$\n\\langle \\tilde{a}_k, e_j \\rangle = \\frac{\\langle a_k, e_j \\rangle}{\\|a_k\\|_2} = \\frac{h_{(k-j) \\bmod n}}{\\sqrt{n}}\n$$\nFourth, we take the absolute value of this quantity.\n$$\n\\left| \\langle \\tilde{a}_k, e_j \\rangle \\right| = \\left| \\frac{h_{(k-j) \\bmod n}}{\\sqrt{n}} \\right| = \\frac{|h_{(k-j) \\bmod n}|}{\\sqrt{n}} = \\frac{1}{\\sqrt{n}}\n$$\nThe last step holds because $|h_l| = 1$ for all $l$. The result is a constant value, independent of both $k$ and $j$, as well as the random choices for $h$ and $\\Omega$.\n\nFinally, the mutual coherence is the maximum of this constant value over the specified ranges of $k$ and $j$.\n$$\n\\mu(\\tilde{A}, I_{n}) = \\max_{k \\in \\Omega} \\max_{0 \\leq j \\leq n-1} \\frac{1}{\\sqrt{n}} = \\frac{1}{\\sqrt{n}}\n$$\n\n**Part 2: Interpretation for Compressed Sensing**\n\nThe computed value $\\mu(\\tilde{A}, I_{n}) = 1/\\sqrt{n}$ represents the coherence between the measurement ensemble (rows of $A$) and the canonical basis $I_n$. In the context of compressed sensing (CS), this quantity is crucial for establishing recovery guarantees.\n\nIn the standard CS model, we aim to recover a sparse signal $x$ from limited measurements $y = \\Phi x$. The signal $x$ is often assumed to be sparse in some orthonormal basis $\\Psi$, so $x = \\Psi \\alpha$ where $\\alpha$ is a sparse vector of coefficients. The measurement matrix is $\\Phi$, and the \"dictionary\" for recovery is $D = \\Phi \\Psi$. The success of CS recovery algorithms, such as Basis Pursuit ($\\ell_1$-minimization), depends on properties of $D$, often characterized by coherence or the Restricted Isometry Property (RIP).\n\nThe quantity $\\mu(\\tilde{A}, I_{n})$ measures the coherence between the sensing matrix $A$ (our $\\Phi$) and the canonical basis $I_n$ (our $\\Psi$). This corresponds to the case where the signal $x$ is assumed to be sparse in the standard basis itself. Such signals are linear combinations of a few canonical basis vectors $e_j$, representing impulses or point sources.\n\nA fundamental principle in compressed sensing is that recovery is more robust and requires fewer measurements when the sensing vectors are as \"incoherent\" or \"uncorrelated\" as possible with the sparsity basis vectors. A low coherence value, such as the one we computed, is highly desirable. The value $1/\\sqrt{n}$ is the lowest possible coherence between any two orthonormal bases in $\\mathbb{R}^n$ (known as the Welch bound, which also applies more generally). Here, our sensing vectors $\\{\\tilde{a}_k\\}$ are not an orthonormal basis, but they achieve this ideal level of incoherence with the canonical basis.\n\nThis low coherence of $1/\\sqrt{n}$ implies that each measurement $y_k = \\langle a_k, x \\rangle$ captures information about all components of $x$ in a distributed, non-preferential manner. No single sparse element (a non-zero component at position $j$) can hide from the measurements, as its contribution to any measurement $y_k$, given by $x_j \\langle a_k, e_j \\rangle$, has a magnitude of $|x_j|$, which is well-distributed. This \"democracy\" of measurements is key to successful sparse recovery. Standard CS theory provides guarantees that an $s$-sparse signal can be recovered from $m$ measurements if, for instance, $m \\gtrsim C \\cdot \\mu^2 \\cdot s \\cdot \\log n$, where $\\mu$ is a measure of coherence. The minimal coherence we found suggests that this measurement scheme is highly efficient for signals sparse in the canonical basis.\n\nThe problem makes reference to signals \"sparse in convolutional dictionaries.\" The canonical basis $I_n = C(e_0)$ is the simplest such dictionary, generated by a single impulse atom $d=e_0$. For a more general atom $d \\in \\mathbb{R}^n$, the sparsity \"basis\" is the set of dictionary vectors $\\{\\psi_j\\}_{j=0}^{n-1}$ where $\\psi_j$ is the $j$-th column of the circulant matrix $C(d)$. The coherence would then be $\\max_{k \\in \\Omega, j} |\\langle \\tilde{a}_k, \\psi_j \\rangle|$. As shown in our validation scratchpad, this leads to analyzing the circular cross-correlation between the random filter $h$ and the atom $d$. Because $h$ is an i.i.d. random vector, it behaves like discrete white noise. Its cross-correlation with any fixed, structured atom $d$ is expected to be small, bounded by concentration of measure phenomena. Therefore, the measurement ensemble $A=R_\\Omega C(h)$ is expected to exhibit low coherence not just with the canonical basis, but with a wide range of convolutional dictionaries. The computed value $\\mu(\\tilde{A}, I_{n}) = 1/\\sqrt{n}$ serves as an important benchmark, representing the ideal case for maximally localized atoms (impulses) and confirming the excellent properties of this structured random matrix family for compressed sensing applications.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{n}}}\n$$"
        },
        {
            "introduction": "The performance of subsampled structured matrices hinges on the statistical properties of the sampling process itself. This advanced exercise delves into a finer point of analysis by comparing two fundamental models: sampling with and without replacement . By deriving and comparing the variance of the measurement energy under both models, you will gain a deeper appreciation for the statistical underpinnings of compressed sensing and quantify the precise benefit of sampling without replacement, a concept known as the finite population correction.",
            "id": "3482581",
            "problem": "Let $n$ be a power of two and let $H \\in \\mathbb{R}^{n \\times n}$ be the normalized Hadamard matrix with entries in $\\{\\pm n^{-1/2}\\}$, so that $H^{\\top} H = I_{n}$. Let $D \\in \\mathbb{R}^{n \\times n}$ be a diagonal matrix with independent Rademacher entries on the diagonal, and let $x \\in \\mathbb{R}^{n}$ be fixed. Define $z := H D x \\in \\mathbb{R}^{n}$, and note that $\\|z\\|_{2}^{2} = \\|x\\|_{2}^{2}$ by orthogonality.\n\nFor a given sampling budget $m \\in \\{1,2,\\dots,n\\}$, construct a selector matrix $P \\in \\mathbb{R}^{m \\times n}$ by choosing $m$ rows of the identity according to one of the following two models:\n\n1. Sampling with replacement: choose indices $I_{1},\\dots,I_{m}$ independently and uniformly from $\\{1,\\dots,n\\}$ and let the rows of $P$ be $e_{I_{1}}^{\\top},\\dots,e_{I_{m}}^{\\top}$.\n2. Sampling without replacement: choose a subset $S \\subset \\{1,\\dots,n\\}$ of size $m$ uniformly at random and let the rows of $P$ be $\\{e_{i}^{\\top} : i \\in S\\}$.\n\nDefine the structured random sensing matrix $A := \\sqrt{\\frac{n}{m}}\\, P H D$ and the random quadratic form $Y := \\|A x\\|_{2}^{2}$. Treat $H$ and $D$ as fixed and consider the randomness over $P$ only. Starting from the definitions of expectation and variance, orthogonality of $H$, and standard facts about sampling with and without replacement (which can be justified using classical concentration inequalities for bounded arrays, such as those of Hoeffding or Serfling), derive explicit expressions for $\\operatorname{Var}(Y \\mid z)$ under both sampling models in terms of the empirical moments of $\\{z_{i}^{2}\\}_{i=1}^{n}$. Then, simplify to obtain the exact ratio\n$$\nR := \\frac{\\operatorname{Var}_{\\text{without replacement}}(Y \\mid z)}{\\operatorname{Var}_{\\text{with replacement}}(Y \\mid z)}.\n$$\nProvide $R$ as a closed-form expression that depends only on $n$ and $m$. No numerical approximation is required, and no units are involved. Express your final answer as a single simplified expression.",
            "solution": "The problem is well-posed, scientifically grounded, and contains sufficient information for a unique solution. All parameters are clearly defined, and the task is a standard derivation in sampling theory and statistics, applied to a context in compressed sensing.\n\nLet us begin by formalizing the random quadratic form $Y$. The matrix $A$ is defined as $A = \\sqrt{\\frac{n}{m}} P H D$. The vector $x$ is fixed, and we are conditioning on $z = H D x$. This means $z \\in \\mathbb{R}^n$ is treated as a fixed vector of constants for the analysis of randomness over the sampling matrix $P$.\n\nThe quantity of interest is $Y = \\|A x\\|_{2}^{2}$. We can express this in terms of $z$:\n$$\nY = \\|A x\\|_{2}^{2} = (A x)^{\\top} (A x) = x^{\\top} A^{\\top} A x\n$$\nSubstituting the definition of $A$:\n$$\nA^{\\top} A = \\left(\\sqrt{\\frac{n}{m}} P H D\\right)^{\\top} \\left(\\sqrt{\\frac{n}{m}} P H D\\right) = \\frac{n}{m} (HD)^{\\top} P^{\\top} P (HD)\n$$\nTherefore,\n$$\nY = \\frac{n}{m} x^{\\top} (HD)^{\\top} P^{\\top} P (HD) x = \\frac{n}{m} (HDx)^{\\top} P^{\\top} P (HDx) = \\frac{n}{m} z^{\\top} P^{\\top} P z\n$$\nThe matrix $P \\in \\mathbb{R}^{m \\times n}$ has rows which are standard basis vectors $e_{I_k}^{\\top}$ for $k=1, \\dots, m$. The matrix product $P^{\\top} P$ is a diagonal matrix whose $j$-th diagonal entry counts how many times the index $j$ was selected. Let $I_1, \\dots, I_m$ be the selected indices. Then $P^{\\top}P = \\sum_{k=1}^m e_{I_k}e_{I_k}^{\\top}$.\nThe quadratic form simplifies to:\n$$\nY = \\frac{n}{m} z^{\\top} \\left(\\sum_{k=1}^{m} e_{I_k} e_{I_k}^{\\top}\\right) z = \\frac{n}{m} \\sum_{k=1}^{m} z^{\\top} e_{I_k} e_{I_k}^{\\top} z = \\frac{n}{m} \\sum_{k=1}^{m} (z^{\\top} e_{I_k})^2 = \\frac{n}{m} \\sum_{k=1}^{m} z_{I_k}^2\n$$\nLet us define a population of $n$ values $\\mathcal{W} = \\{w_1, w_2, \\dots, w_n\\}$, where $w_i = z_i^2$. The expression for $Y$ is $\\frac{n}{m}$ times the sum of $m$ samples drawn from $\\mathcal{W}$. We need to compute $\\operatorname{Var}(Y \\mid z)$ under the two specified sampling models. The randomness arises from the choice of indices $I_k \\in \\{1,\\dots,n\\}$.\n\nLet $\\mathcal{M}_1$ and $\\mathcal{M}_2$ be the first two empirical moments of the population $\\{w_i\\}$:\n$$\n\\mathcal{M}_1 = \\frac{1}{n} \\sum_{i=1}^{n} w_i = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2\n$$\n$$\n\\mathcal{M}_2 = \\frac{1}{n} \\sum_{i=1}^{n} w_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} z_i^4\n$$\nThe population variance (with $1/n$ scaling) is $\\sigma_{\\mathcal{W}}^2 = \\frac{1}{n}\\sum_{i=1}^n (w_i - \\mathcal{M}_1)^2 = \\mathcal{M}_2 - \\mathcal{M}_1^2$.\n\nFirst, we calculate the expectation of $Y$, which is the same for both models. Let $W_k = w_{I_k}$.\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{n}{m} \\sum_{k=1}^{m} W_k\\right] = \\frac{n}{m} \\sum_{k=1}^{m} \\mathbb{E}[W_k]\n$$\nFor any $k$, $\\mathbb{E}[W_k] = \\sum_{j=1}^n P(I_k=j) w_j$. In both models, $P(I_k=j) = 1/n$. Thus, $\\mathbb{E}[W_k] = \\frac{1}{n}\\sum_{j=1}^n w_j = \\mathcal{M}_1$.\n$$\n\\mathbb{E}[Y] = \\frac{n}{m} \\sum_{k=1}^{m} \\mathcal{M}_1 = \\frac{n}{m} \\cdot m \\mathcal{M}_1 = n \\mathcal{M}_1 = \\sum_{i=1}^n z_i^2 = \\|z\\|_2^2\n$$\n\nNow we compute the variance for each model. $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$.\n\n**Model 1: Sampling with replacement**\nThe indices $I_1, \\dots, I_m$ are independent and identically distributed (i.i.d.), uniform on $\\{1, \\dots, n\\}$. The random variables $W_k = w_{I_k}$ are therefore also i.i.d.\nThe variance of one such variable $W_k$ is:\n$$\n\\operatorname{Var}(W_k) = \\mathbb{E}[W_k^2] - (\\mathbb{E}[W_k])^2\n$$\n$\\mathbb{E}[W_k^2] = \\sum_{j=1}^n P(I_k=j) w_j^2 = \\frac{1}{n}\\sum_{j=1}^n w_j^2 = \\mathcal{M}_2$.\n$$\n\\operatorname{Var}(W_k) = \\mathcal{M}_2 - \\mathcal{M}_1^2 = \\sigma_{\\mathcal{W}}^2\n$$\nSince the $W_k$ are i.i.d., the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}_{\\text{with}}(Y) = \\operatorname{Var}\\left(\\frac{n}{m} \\sum_{k=1}^{m} W_k\\right) = \\left(\\frac{n}{m}\\right)^2 \\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right) = \\frac{n^2}{m^2} \\sum_{k=1}^{m} \\operatorname{Var}(W_k)\n$$\n$$\n\\operatorname{Var}_{\\text{with}}(Y \\mid z) = \\frac{n^2}{m^2} \\cdot m (\\mathcal{M}_2 - \\mathcal{M}_1^2) = \\frac{n^2}{m} (\\mathcal{M}_2 - \\mathcal{M}_1^2)\n$$\n\n**Model 2: Sampling without replacement**\nThe indices $I_1, \\dots, I_m$ form a set $S$ of $m$ distinct indices chosen uniformly at random. The variables $W_k = w_{I_k}$ are not independent.\n$$\n\\operatorname{Var}_{\\text{without}}(Y) = \\operatorname{Var}\\left(\\frac{n}{m} \\sum_{k=1}^{m} W_k\\right) = \\frac{n^2}{m^2} \\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right)\n$$\nThe variance of the sum is $\\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right) = \\sum_{k=1}^{m} \\operatorname{Var}(W_k) + \\sum_{k \\neq l} \\operatorname{Cov}(W_k, W_l)$.\nThere are $m$ variance terms and $m(m-1)$ covariance terms. Due to the symmetry of uniform sampling, $\\operatorname{Var}(W_k)$ is the same for all $k$, and $\\operatorname{Cov}(W_k, W_l)$ is the same for all $k \\neq l$.\n$\\operatorname{Var}(W_1) = \\mathcal{M}_2 - \\mathcal{M}_1^2$.\nFor $k \\neq l$, we compute the covariance:\n$$\n\\operatorname{Cov}(W_k, W_l) = \\mathbb{E}[W_k W_l] - \\mathbb{E}[W_k]\\mathbb{E}[W_l] = \\mathbb{E}[w_{I_k} w_{I_l}] - \\mathcal{M}_1^2\n$$\nThe joint probability of selecting indices $i$ and $j$ (for $i \\neq j$) for two distinct draws is $P(I_k=i, I_l=j) = P(I_l=j|I_k=i)P(I_k=i) = \\frac{1}{n-1} \\frac{1}{n}$.\n$$\n\\mathbb{E}[w_{I_k} w_{I_l}] = \\sum_{i=1}^n \\sum_{j \\neq i} P(I_k=i, I_l=j) w_i w_j = \\sum_{i=1}^n \\sum_{j \\neq i} \\frac{1}{n(n-1)} w_i w_j\n$$\nThe sum can be written as:\n$$\n\\frac{1}{n(n-1)} \\sum_{i \\neq j} w_i w_j = \\frac{1}{n(n-1)} \\left[ \\left(\\sum_i w_i\\right)^2 - \\sum_i w_i^2 \\right]\n$$\nIn terms of moments:\n$$\n\\mathbb{E}[w_{I_k} w_{I_l}] = \\frac{1}{n(n-1)} [ (n\\mathcal{M}_1)^2 - n\\mathcal{M}_2 ] = \\frac{n^2\\mathcal{M}_1^2 - n\\mathcal{M}_2}{n(n-1)} = \\frac{n\\mathcal{M}_1^2 - \\mathcal{M}_2}{n-1}\n$$\nSo, the covariance is:\n$$\n\\operatorname{Cov}(W_k, W_l) = \\frac{n\\mathcal{M}_1^2 - \\mathcal{M}_2}{n-1} - \\mathcal{M}_1^2 = \\frac{n\\mathcal{M}_1^2 - \\mathcal{M}_2 - (n-1)\\mathcal{M}_1^2}{n-1} = \\frac{\\mathcal{M}_1^2 - \\mathcal{M}_2}{n-1} = -\\frac{\\sigma_{\\mathcal{W}}^2}{n-1}\n$$\nNow we assemble the variance of the sum:\n$$\n\\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right) = m \\cdot \\sigma_{\\mathcal{W}}^2 + m(m-1) \\left(-\\frac{\\sigma_{\\mathcal{W}}^2}{n-1}\\right) = m \\sigma_{\\mathcal{W}}^2 \\left(1 - \\frac{m-1}{n-1}\\right)\n$$\n$$\n= m \\sigma_{\\mathcal{W}}^2 \\left(\\frac{n-1 - (m-1)}{n-1}\\right) = m \\sigma_{\\mathcal{W}}^2 \\frac{n-m}{n-1}\n$$\nFinally, we find the variance of $Y$:\n$$\n\\operatorname{Var}_{\\text{without}}(Y \\mid z) = \\frac{n^2}{m^2} \\left(m (\\mathcal{M}_2 - \\mathcal{M}_1^2) \\frac{n-m}{n-1}\\right) = \\frac{n^2}{m} (\\mathcal{M}_2 - \\mathcal{M}_1^2) \\frac{n-m}{n-1}\n$$\n\n**Ratio of Variances**\nWe are asked for the ratio $R = \\frac{\\operatorname{Var}_{\\text{without}}(Y \\mid z)}{\\operatorname{Var}_{\\text{with}}(Y \\mid z)}$.\n$$\nR = \\frac{\\frac{n^2}{m} (\\mathcal{M}_2 - \\mathcal{M}_1^2) \\frac{n-m}{n-1}}{\\frac{n^2}{m}(\\mathcal{M}_2 - \\mathcal{M}_1^2)}\n$$\nAssuming not all $z_i^2$ are equal, $\\mathcal{M}_2 - \\mathcal{M}_1^2 \\neq 0$, and we can cancel this term.\n$$\nR = \\frac{n-m}{n-1}\n$$\nThis factor is known as the finite population correction. It reflects the reduction in variance achieved by sampling without replacement compared to sampling with replacement from a finite population.",
            "answer": "$$ \\boxed{\\frac{n-m}{n-1}} $$"
        }
    ]
}