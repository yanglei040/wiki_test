{
    "hands_on_practices": [
        {
            "introduction": "The Johnson-Lindenstrauss lemma's power lies in its ability to preserve geometric structure, but this hinges on careful construction of the projection matrix. This exercise  invites you to explore the critical role of normalization by first analyzing an *unscaled* Gaussian projection. By calculating the expected distortion and examining the concentration bounds, you will build a first-principles understanding of why the standard $\\frac{1}{\\sqrt{m}}$ scaling is essential for creating an approximate isometry.",
            "id": "3488224",
            "problem": "Consider a finite set $S \\subset \\mathbb{R}^n$ of cardinality $N$, and an embedding defined by a random matrix $A \\in \\mathbb{R}^{m \\times n}$ with independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0,1)$, where $\\mathcal{N}(0,1)$ denotes a zero-mean, unit-variance Gaussian distribution. This is an unnormalized embedding often contrasted with the normalized choice $A_{ij} \\sim \\mathcal{N}(0,1/m)$ used in the Johnson-Lindenstrauss (JL) lemma, where JL stands for Johnson-Lindenstrauss. For a fixed vector $x \\in \\mathbb{R}^n$, analyze the distribution of $\\|A x\\|_2^2$ by starting from the following base facts: linear combinations of independent Gaussian random variables are Gaussian with variance equal to the sum of squared coefficients, and the sum of squares of $m$ independent standard Gaussian variables has a chi-square distribution with $m$ degrees of freedom. Then, reason about pairwise distances by taking $v = x - y$ for $x,y \\in S$. Finally, consider the scaled map $\\tilde{A} = \\frac{1}{\\sqrt{m}} A$ and compare its behavior to the unnormalized map with respect to bias and concentration of norms and pairwise distances.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. If $A_{ij} \\sim \\mathcal{N}(0,1)$ independently, then for any fixed $x \\in \\mathbb{R}^n$, $\\mathbb{E}\\,\\|A x\\|_2^2 = m \\,\\|x\\|_2^2$, and there exists a universal constant $c > 0$ such that for all $\\varepsilon \\in (0,1)$,\n$\\mathbb{P}\\!\\left(\\left|\\|A x\\|_2^2 - m \\|x\\|_2^2\\right| > \\varepsilon \\, m \\, \\|x\\|_2^2\\right) \\le 2 \\exp\\!\\left(- c \\, \\varepsilon^2 \\, m\\right)$.\n\nB. If $\\tilde{A} = \\frac{1}{\\sqrt{m}} A$ with $A_{ij} \\sim \\mathcal{N}(0,1)$ independently, then for any fixed $x \\in \\mathbb{R}^n$, $\\mathbb{E}\\,\\|\\tilde{A} x\\|_2^2 = \\|x\\|_2^2$, and there exists a universal constant $c > 0$ such that for all $\\varepsilon \\in (0,1)$,\n$\\mathbb{P}\\!\\left(\\left|\\|\\tilde{A} x\\|_2^2 - \\|x\\|_2^2\\right| > \\varepsilon \\, \\|x\\|_2^2\\right) \\le 2 \\exp\\!\\left(- c \\, \\varepsilon^2 \\, m\\right)$.\n\nC. Without any scaling, the expected pairwise Euclidean distances satisfy $\\mathbb{E}\\,\\|A x - A y\\|_2 = \\|x - y\\|_2$ for all $x,y \\in \\mathbb{R}^n$, so the unnormalized map is unbiased in expectation for distances and only its variance is inflated.\n\nD. To correct the bias in the expected squared norms, one must scale by $\\frac{1}{m}$, since then $\\mathbb{E}\\,\\left\\|\\frac{1}{m} A x\\right\\|_2^2 = \\|x\\|_2^2$.\n\nE. For random matrices $A$ with independent and identically distributed subgaussian entries with zero mean and unit variance, choosing $m \\ge C \\,\\varepsilon^{-2} \\log N$ for a sufficiently large absolute constant $C$ guarantees that, over a fixed set $S$ of size $N$, the unnormalized distances $\\|A x - A y\\|_2$ concentrate around $\\|x - y\\|_2$ without any normalization, so no scaling is necessary.\n\nF. For the unnormalized Gaussian map with $A_{ij} \\sim \\mathcal{N}(0,1)$, if $m \\ge C \\,\\varepsilon^{-2} \\log N$ for a sufficiently large absolute constant $C$, then with high probability simultaneously over all $x,y \\in S$,\n$(1 - \\varepsilon)\\, \\sqrt{m}\\, \\|x - y\\|_2 \\le \\|A x - A y\\|_2 \\le (1 + \\varepsilon)\\, \\sqrt{m}\\, \\|x - y\\|_2$,\nthat is, all pairwise distances are preserved up to a $(1 \\pm \\varepsilon)$ factor around the deterministic scale $\\sqrt{m}$.\n\nChoose all correct options.",
            "solution": "This is a thinking process that validates the problem and derives the correct answer.\n\n**1. Problem Validation**\n-   **Givens**: The problem defines two Gaussian embeddings: an unnormalized one, $A$, with entries $A_{ij} \\sim \\mathcal{N}(0,1)$, and a normalized one, $\\tilde{A} = \\frac{1}{\\sqrt{m}}A$, whose entries are effectively $\\tilde{A}_{ij} \\sim \\mathcal{N}(0, 1/m)$. It asks to evaluate several statements about the expectation and concentration of the squared norm $\\|Ax\\|_2^2$ and the norm $\\|Ax-Ay\\|_2$.\n-   **Scientific Soundness**: The problem is grounded in the core theory of the Johnson-Lindenstrauss lemma. It tests the understanding of isotropy (unbiasedness in expectation for squared norms) and concentration of measure (chi-square distribution). All premises are standard and correct.\n-   **Well-Posedness**: The question is well-posed, asking for the correctness of specific mathematical statements. Each statement is verifiable through direct calculation or application of standard theorems.\n-   **Verdict**: The problem is valid and serves as an excellent check of fundamental understanding.\n\n**2. Solution Derivation**\n\nFirst, let's analyze the properties of the unnormalized map $A$ where $A_{ij} \\sim \\mathcal{N}(0,1)$. Let $x \\in \\mathbb{R}^n$ be a fixed vector.\n-   Let $y = Ax$. Each component $y_i = (Ax)_i = \\sum_{j=1}^n A_{ij} x_j$ is a linear combination of independent, zero-mean, unit-variance Gaussian variables.\n-   The distribution of $y_i$ is Gaussian with mean $\\mathbb{E}[y_i] = 0$ and variance $\\text{Var}(y_i) = \\sum_{j=1}^n x_j^2 \\text{Var}(A_{ij}) = \\|x\\|_2^2$. So, $y_i \\sim \\mathcal{N}(0, \\|x\\|_2^2)$.\n-   The components $y_i$ are independent since the rows of $A$ are independent.\n-   The squared norm is $\\|Ax\\|_2^2 = \\sum_{i=1}^m y_i^2$. Its expectation is $\\mathbb{E}[\\|Ax\\|_2^2] = \\sum_{i=1}^m \\mathbb{E}[y_i^2] = \\sum_{i=1}^m \\text{Var}(y_i) = \\sum_{i=1}^m \\|x\\|_2^2 = m \\|x\\|_2^2$.\n-   To analyze concentration, we normalize. Let $Z_i = y_i / \\|x\\|_2$. Then $Z_i \\sim \\mathcal{N}(0,1)$ are i.i.d. standard normal variables.\n-   We can write $\\|Ax\\|_2^2 = \\|x\\|_2^2 \\sum_{i=1}^m Z_i^2$. The sum $W = \\sum_{i=1}^m Z_i^2$ follows a chi-square distribution with $m$ degrees of freedom, $\\chi_m^2$.\n-   A standard concentration inequality for chi-square variables (a specific case of Bernstein's inequality for sub-exponential variables) states that for $W \\sim \\chi_m^2$, $\\mathbb{P}(|W - m| > \\varepsilon m) \\le 2\\exp(-c \\varepsilon^2 m)$ for some constant $c > 0$.\n\nNow let's evaluate each option:\n\n**A. Correct.** The expectation $\\mathbb{E}\\,\\|A x\\|_2^2 = m \\,\\|x\\|_2^2$ was derived above. The concentration inequality $\\mathbb{P}\\!\\left(\\left|\\|A x\\|_2^2 - m \\|x\\|_2^2\\right| > \\varepsilon \\, m \\, \\|x\\|_2^2\\right)$ can be rewritten by dividing inside the absolute value by $\\|x\\|_2^2$ (assuming $x \\neq 0$) to get $\\mathbb{P}\\!\\left(\\left|\\frac{\\|A x\\|_2^2}{\\|x\\|_2^2} - m\\right| > \\varepsilon \\, m \\right)$. Since $\\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2}$ is a $\\chi_m^2$ variable, this is exactly the standard concentration bound for a chi-square variable around its mean $m$.\n\n**B. Correct.** For the scaled map $\\tilde{A} = \\frac{1}{\\sqrt{m}}A$, the entries are $\\tilde{A}_{ij} \\sim \\mathcal{N}(0, 1/m)$. The expectation is $\\mathbb{E}\\,\\|\\tilde{A} x\\|_2^2 = \\mathbb{E}\\,\\|\\frac{1}{\\sqrt{m}} A x\\|_2^2 = \\frac{1}{m}\\mathbb{E}\\,\\|A x\\|_2^2 = \\frac{1}{m}(m \\|x\\|_2^2) = \\|x\\|_2^2$. This map is an unbiased estimator for the squared norm (isotropic). The probability inequality $\\mathbb{P}\\!\\left(\\left|\\|\\tilde{A} x\\|_2^2 - \\|x\\|_2^2\\right| > \\varepsilon \\, \\|x\\|_2^2\\right)$ is equivalent to $\\mathbb{P}\\!\\left(\\left|\\frac{1}{m}\\|A x\\|_2^2 - \\|x\\|_2^2\\right| > \\varepsilon \\, \\|x\\|_2^2\\right)$, which simplifies to the inequality in statement A. Since A is correct, B is also correct.\n\n**C. Incorrect.** The statement claims that for the unscaled map, $\\mathbb{E}\\,\\|A x - A y\\|_2 = \\|x - y\\|_2$. Let $v=x-y$. We know $\\mathbb{E}\\|Av\\|_2^2 = m\\|v\\|_2^2$. By Jensen's inequality for the concave square-root function, $\\mathbb{E}\\|Av\\|_2  \\sqrt{\\mathbb{E}\\|Av\\|_2^2} = \\sqrt{m}\\|v\\|_2$. The expectation is not $\\|v\\|_2$, it is biased by a factor related to $\\sqrt{m}$.\n\n**D. Incorrect.** The statement suggests scaling the matrix $A$ by a factor of $1/m$. Let's check the expectation: $\\mathbb{E}\\,\\left\\|\\frac{1}{m} A x\\right\\|_2^2 = \\frac{1}{m^2} \\mathbb{E}\\,\\|A x\\|_2^2 = \\frac{1}{m^2} (m\\|x\\|_2^2) = \\frac{1}{m}\\|x\\|_2^2$. This is not equal to $\\|x\\|_2^2$. The correct scaling factor for the matrix entries is $1/\\sqrt{m}$.\n\n**E. Incorrect.** The statement claims the unnormalized distances concentrate around the original distances. As shown in the analysis for A and C, the unnormalized squared distance $\\|Ax-Ay\\|_2^2$ concentrates around $m\\|x-y\\|_2^2$. Therefore, the unnormalized distance $\\|Ax-Ay\\|_2$ concentrates around $\\sqrt{m}\\|x-y\\|_2$. Scaling is necessary to remove this factor of $\\sqrt{m}$.\n\n**F. Correct.** This statement accurately describes the behavior of the unnormalized map. It correctly identifies that the pairwise distances $\\|A x - A y\\|_2$ concentrate not around $\\|x-y\\|_2$, but around a scaled version $\\sqrt{m}\\|x-y\\|_2$. The inequality $(1 - \\varepsilon)\\, \\sqrt{m}\\, \\|x - y\\|_2 \\le \\|A x - A y\\|_2 \\le (1 + \\varepsilon)\\, \\sqrt{m}\\, \\|x - y\\|_2$ is just a restatement of the Johnson-Lindenstrauss lemma applied to the normalized map $\\tilde{A} = \\frac{1}{\\sqrt{m}}A$. This is because dividing the inequality by $\\sqrt{m}$ gives $(1-\\varepsilon)\\|x-y\\|_2 \\le \\|\\tilde{A}(x-y)\\|_2 \\le (1+\\varepsilon)\\|x-y\\|_2$, which is the standard JL guarantee. The union bound argument ensures this holds for all pairs simultaneously if $m \\ge C \\varepsilon^{-2} \\log N$.",
            "answer": "$$\\boxed{ABF}$$"
        },
        {
            "introduction": "While Gaussian matrices provide a canonical example of a Johnson-Lindenstrauss transform, the phenomenon is far more general. This practice  compares the classic Gaussian ensemble with the computationally efficient Rademacher ensemble, whose entries are simply $\\pm 1/\\sqrt{m}$. Through this comparison, you will see that the underlying principle is not normality but the more general property of subgaussianity, which ensures strong concentration and allows for similar, powerful dimensionality reduction guarantees.",
            "id": "3488220",
            "problem": "Consider two independent and identically distributed (i.i.d.) random matrix ensembles used as Johnson-Lindenstrauss (JL) transforms for dimensionality reduction in compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{m \\times d}$ have i.i.d. entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ (Gaussian ensemble), and let $B \\in \\mathbb{R}^{m \\times d}$ have i.i.d. entries $B_{ij} \\in \\{\\pm 1/\\sqrt{m}\\}$ with equal probability (Rademacher ensemble). For any fixed $x \\in \\mathbb{R}^{d}$, denote the Euclidean $2$-norm by $\\| \\cdot \\|_{2}$. A random linear map is said to be isotropic if $\\mathbb{E}\\|Mx\\|_{2}^{2} = \\|x\\|_{2}^{2}$ for all $x \\in \\mathbb{R}^{d}$. A random variable $X$ is subgaussian with parameter $\\sigma$ if its moment generating function satisfies $\\mathbb{E}[\\exp(tX)] \\le \\exp\\left(\\sigma^{2} t^{2} / 2\\right)$ for all $t \\in \\mathbb{R}$, and the square of a subgaussian is subexponential, enabling Bernstein-type concentration for sums of independent centered squares.\n\nUsing only these fundamental definitions and facts, analyze the concentration of the distortion $\\|Mx\\|_{2}^{2}$ around $\\|x\\|_{2}^{2}$ for the two ensembles, and deduce the scaling of the embedding dimension $m$ required to preserve all pairwise distances within a finite set of $N$ points with failure probability at most $\\delta \\in (0,1)$. Choose all statements that are correct.\n\nA. For any fixed $x \\in \\mathbb{R}^{d}$ and any $\\varepsilon \\in (0,1)$, both ensembles satisfy a tail bound of the form $\\mathbb{P}\\left(\\left|\\|Mx\\|_{2}^{2} - \\|x\\|_{2}^{2}\\right| > \\varepsilon \\|x\\|_{2}^{2}\\right) \\le 2 \\exp\\left(- c m \\varepsilon^{2}\\right)$ for some absolute constant $c  0$, with the value of $c$ depending on the ensemble.\n\nB. The minimal embedding dimension $m$ that ensures, with probability at least $1 - \\delta$, that all pairwise distances among $N$ points are preserved within a factor $(1 \\pm \\varepsilon)$ scales as $m \\ge C \\varepsilon^{-2} \\log(N/\\delta)$ with the same absolute constant $C$ for both the Gaussian and Rademacher ensembles.\n\nC. The Gaussian ensemble is rotationally invariant, implying that for fixed $x \\in \\mathbb{R}^{d}$ the normalized distortion $\\|Ax\\|_{2}^{2}/\\|x\\|_{2}^{2}$ has an exact chi-square law with $m$ degrees of freedom, whereas the Rademacher ensemble lacks rotational invariance and requires subgaussian-to-subexponential arguments for $\\|Bx\\|_{2}^{2}$; this yields similar concentration rates but different tail constants.\n\nD. The Rademacher ensemble does not admit subgaussian tails for $(Bx)_{i}$ and therefore cannot achieve JL-type concentration with $m$ scaling like $\\varepsilon^{-2}$; instead, it requires $m$ to scale like $\\varepsilon^{-4}$ (up to logarithmic factors).\n\nE. To achieve isotropy, entries in both ensembles must be scaled by $1/m$ rather than $1/\\sqrt{m}$; otherwise, $\\mathbb{E}\\|Mx\\|_{2}^{2}$ will not equal $\\|x\\|_{2}^{2}$ as $m \\to \\infty$.",
            "solution": "This is a thinking process that validates the problem and derives the correct answer.\n\n**1. Problem Validation**\n-   **Givens**: The problem defines two correctly normalized random matrix ensembles for JL transforms: a Gaussian one ($A$) and a Rademacher one ($B$). It provides standard definitions for isotropy and subgaussianity.\n-   **Scientific Soundness**: The problem setup is standard in high-dimensional probability. Both ensembles are known to satisfy the JL property. The key is to compare their properties. The statement that squares of subgaussians are subexponential is a crucial and correct fact for this analysis. The normalizations are correct to ensure isotropy: for both ensembles, the entries are zero-mean and have variance $1/m$, which ensures $\\mathbb{E}\\|Mx\\|_2^2 = \\|x\\|_2^2$.\n-   **Well-Posedness**: The question asks to compare the two ensembles by evaluating specific, verifiable statements about their concentration properties and the resulting dimension requirements.\n-   **Verdict**: The problem is valid, well-posed, and an excellent exercise in distinguishing the finer points of different random matrix ensembles.\n\n**2. Solution Derivation**\n\nLet's evaluate each option based on the provided facts and standard results in probability.\n\n**A. Correct.** Both the Gaussian and Rademacher ensembles, when appropriately scaled as given, produce a random projection that approximately preserves norms. This is because in both cases, $\\|Mx\\|_2^2$ is a sum of $m$ i.i.d. random variables whose expectation is $\\|x\\|_2^2$. For the Gaussian case, the sum follows a scaled chi-square distribution. For the Rademacher case, the sum is of squared sums of scaled Rademacher variables. Both are sums of i.i.d. subexponential random variables (after centering). A standard result for such sums (Bernstein's inequality) gives a concentration inequality of the form $\\mathbb{P}(|Y - \\mathbb{E}Y| > t) \\le 2 \\exp(-c' \\min(t^2/V, t/K))$, where $V$ is a variance-like term and $K$ is a scale parameter. For small distortions $\\varepsilon$, the $t^2$ term dominates, leading to the subgaussian-type tail $\\exp(-c m \\varepsilon^2)$. The constant $c$ depends on the specific subexponential parameters of the summands, which will be different for the Gaussian and Rademacher ensembles.\n\n**B. Incorrect.** The dimension $m$ required for the JL lemma is derived from the concentration inequality in statement A via a union bound. The required $m$ is inversely proportional to the constant $c$ from the tail bound: $m \\ge C \\varepsilon^{-2} \\log(N^2/\\delta)$ where $C \\propto 1/c$. Since the concentration constants $c$ are different for the Gaussian and Rademacher ensembles (Gaussian ensembles typically yield better, i.e., larger, constants), the overall constant $C$ in the requirement for $m$ will also be different.\n\n**C. Correct.** This statement highlights a key difference between the two ensembles. The distribution of a matrix with i.i.d. Gaussian entries is invariant under multiplication by orthogonal matrices (rotational invariance). This allows simplifying the analysis of $\\|Ax\\|_2^2$ by rotating $x$ to be along an axis, which shows that $\\|Ax\\|_2^2/\\|x\\|_2^2$ is distributed as $\\frac{1}{m} \\chi_m^2$ (a scaled chi-square variable, not exactly chi-square, but directly related). The Rademacher ensemble lacks this symmetry; its entries are discrete, and rotation would not preserve the distribution. Therefore, its analysis relies on more general tools like Hoeffding's or Bernstein's inequalities applied to sums of subgaussian random variables. This difference in analytical tools and underlying distributions leads to different constants in the final tail bounds, even though the asymptotic scaling (the \"rate\") is the same.\n\n**D. Incorrect.** The variable $(Bx)_i = \\sum_j B_{ij}x_j$ is a sum of independent, centered, and bounded random variables ($|B_{ij}x_j| \\le |x_j|/\\sqrt{m}$). By Hoeffding's inequality, such a sum is subgaussian. Therefore, its square is subexponential, and the sum of squares $\\|Bx\\|_2^2$ concentrates sharply, leading to the standard JL scaling of $m \\propto \\varepsilon^{-2}$. The claim of requiring $\\varepsilon^{-4}$ is false.\n\n**E. Incorrect.** Isotropy, $\\mathbb{E}\\|Mx\\|_2^2 = \\|x\\|_2^2$, is achieved when the variance of the matrix entries is $1/m$ (assuming zero mean and i.i.d. entries). For a value/standard deviation scaling of $1/\\sqrt{m}$, the variance is $(1/\\sqrt{m})^2 = 1/m$, which is correct. If the scaling were $1/m$, the variance would be $1/m^2$, leading to $\\mathbb{E}\\|Mx\\|_2^2 = m \\cdot (1/m^2)\\|x\\|_2^2 = (1/m)\\|x\\|_2^2 \\neq \\|x\\|_2^2$. The scaling provided in the problem, $1/\\sqrt{m}$, is the correct one to achieve isotropy.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "Beyond its theoretical elegance, the Johnson-Lindenstrauss lemma is a powerful tool for accelerating large-scale machine learning algorithms. This problem  places you at the intersection of random projections and sparse recovery, asking you to analyze what happens when the LASSO is solved on dimension-reduced data. Your task is to quantify how the geometric distortion introduced by a Fast Johnson-Lindenstrauss Transform (FJLT) impacts the Restricted Strong Convexity of the problem and ultimately propagates to the final estimation error, providing a sharp analysis of this practical speed-up technique.",
            "id": "3488241",
            "problem": "Consider a data matrix $X \\in \\mathbb{R}^{n \\times d}$ and a response vector $y \\in \\mathbb{R}^{n}$ obeying the linear model $y = X \\beta^{\\star} + w$, where the true parameter $\\beta^{\\star} \\in \\mathbb{R}^{d}$ is $s$-sparse with support $S \\subset [d]$, $|S| = s$, and $w \\in \\mathbb{R}^{n}$ is noise. The Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\widehat{\\beta}$ is defined as the minimizer of the objective\n$$\n\\frac{1}{2n}\\|X \\beta - y\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nfor a given regularization parameter $\\lambda > 0$. Assume $X$ satisfies restricted strong convexity (RSC) and restricted smoothness (RSS) on the cone $\\mathcal{C}(S) = \\{h \\in \\mathbb{R}^{d} : \\|h_{S^{c}}\\|_{1} \\leq 3 \\|h_{S}\\|_{1}\\}$, i.e., there exist constants $\\alpha > 0$ and $L > 0$ such that for all $h \\in \\mathcal{C}(S)$,\n$$\n\\alpha \\|h\\|_{2}^{2} \\leq \\frac{1}{n}\\|X h\\|_{2}^{2} \\leq L \\|h\\|_{2}^{2}.\n$$\nAssume the noise is such that the dual feasibility condition holds for the original design: $\\left\\|\\frac{1}{n} X^{\\top} w\\right\\|_{\\infty} \\leq \\frac{\\lambda}{4}$.\n\nYou apply a Fast Johnson-Lindenstrauss Transform (FJLT), that is, a structured random linear map $\\Phi \\in \\mathbb{R}^{m \\times n}$ which, by the Johnson-Lindenstrauss lemma, preserves all squared norms in the set $\\{X h : h \\in \\mathcal{C}(S)\\}$ up to a distortion $\\varepsilon \\in (0,1)$ with probability at least $1 - \\delta$, in the sense that for all $h \\in \\mathcal{C}(S)$,\n$$\n(1 - \\varepsilon)\\, \\frac{1}{n}\\|X h\\|_{2}^{2} \\leq \\frac{1}{m}\\|\\Phi X h\\|_{2}^{2} \\leq (1 + \\varepsilon)\\, \\frac{1}{n}\\|X h\\|_{2}^{2}.\n$$\nYou then solve the preconditioned LASSO\n$$\n\\widehat{\\beta}_{\\mathrm{FJLT}} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{d}} \\left\\{ \\frac{1}{2m}\\|\\Phi X \\beta - \\Phi y\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\}.\n$$\nAssume the projected noise obeys $\\left\\|\\frac{1}{m}(\\Phi X)^{\\top} \\Phi w\\right\\|_{\\infty} \\leq \\frac{\\lambda}{2}$.\n\nStarting only from the definitions of restricted strong convexity and smoothness, the convex optimality of the LASSO, and the Johnson-Lindenstrauss norm preservation guarantee stated above, derive how the Fast Johnson-Lindenstrauss Transform affects the restricted condition number and the restricted strong convexity constant of the preconditioned design. Then, using these effects, derive a sharp upper bound on the $\\ell_{2}$-error $\\|\\widehat{\\beta}_{\\mathrm{FJLT}} - \\beta^{\\star}\\|_{2}$ in terms of $\\alpha$, $\\varepsilon$, $s$, and $\\lambda$. Express your final answer as a single closed-form analytic expression for this bound. No numerical approximation is required, and no units are involved. The final answer must be a single expression in terms of $\\alpha$, $\\varepsilon$, $s$, and $\\lambda$ only.",
            "solution": "This is a thinking process that derives the requested error bound.\n\n**1. Analyze the Effect of FJLT on RSC**\n\nThe problem provides the original RSC/RSS condition for the design matrix $X$ and the cone $\\mathcal{C}(S)$:\n$$ \\alpha \\|h\\|_{2}^{2} \\leq \\frac{1}{n}\\|X h\\|_{2}^{2} \\leq L \\|h\\|_{2}^{2} $$\nIt also provides the guarantee of the FJLT matrix $\\Phi$:\n$$ (1 - \\varepsilon)\\, \\frac{1}{n}\\|X h\\|_{2}^{2} \\leq \\frac{1}{m}\\|\\Phi X h\\|_{2}^{2} \\leq (1 + \\varepsilon)\\, \\frac{1}{n}\\|X h\\|_{2}^{2} $$\nTo find the new RSC constant for the preconditioned design $\\Phi X$, we need a lower bound on $\\frac{1}{m}\\|\\Phi X h\\|_{2}^{2}$ in terms of $\\|h\\|_2^2$. We can combine the two inequalities. For any $h \\in \\mathcal{C}(S)$:\n$$ \\frac{1}{m}\\|\\Phi X h\\|_{2}^{2} \\geq (1 - \\varepsilon) \\frac{1}{n}\\|X h\\|_{2}^{2} $$\nUsing the original RSC lower bound, $\\frac{1}{n}\\|X h\\|_{2}^{2} \\geq \\alpha \\|h\\|_{2}^{2}$, we get:\n$$ \\frac{1}{m}\\|\\Phi X h\\|_{2}^{2} \\geq (1 - \\varepsilon) \\alpha \\|h\\|_{2}^{2} $$\nThis means the preconditioned design satisfies RSC with a new constant $\\tilde{\\alpha} = \\alpha(1 - \\varepsilon)$.\n\n**2. Derive the Basic Inequality for the Preconditioned LASSO**\n\nLet $\\widehat{\\beta} \\equiv \\widehat{\\beta}_{\\mathrm{FJLT}}$ be the minimizer of the preconditioned objective. Let the error be $h = \\widehat{\\beta} - \\beta^{\\star}$. By optimality, the objective value at $\\widehat{\\beta}$ is less than or equal to the value at $\\beta^{\\star}$:\n$$ \\frac{1}{2m}\\|\\Phi X \\widehat{\\beta} - \\Phi y\\|_{2}^{2} + \\lambda \\|\\widehat{\\beta}\\|_{1} \\leq \\frac{1}{2m}\\|\\Phi X \\beta^{\\star} - \\Phi y\\|_{2}^{2} + \\lambda \\|\\beta^{\\star}\\|_{1} $$\nSubstitute $y = X \\beta^{\\star} + w$ and $h = \\widehat{\\beta} - \\beta^{\\star}$:\n$$ \\frac{1}{2m}\\|\\Phi X h - \\Phi w\\|_{2}^{2} + \\lambda \\|\\widehat{\\beta}\\|_{1} \\leq \\frac{1}{2m}\\|-\\Phi w\\|_{2}^{2} + \\lambda \\|\\beta^{\\star}\\|_{1} $$\nExpanding the norm on the left and canceling terms gives the standard basic inequality:\n$$ \\frac{1}{2m}\\|\\Phi X h\\|_{2}^{2} \\leq \\frac{1}{m}\\langle \\Phi X h, \\Phi w \\rangle + \\lambda (\\|\\beta^{\\star}\\|_{1} - \\|\\widehat{\\beta}\\|_{1}) $$\n\n**3. Bound the Terms on the Right-Hand Side**\n\n*   **Noise Term**: The inner product can be written as $\\langle h, \\frac{1}{m}(\\Phi X)^{\\top}\\Phi w \\rangle$. Using HÃ¶lder's inequality and the problem's assumption on the projected noise, we have:\n    $$ \\left\\langle h, \\frac{1}{m}(\\Phi X)^{\\top}\\Phi w \\right\\rangle \\leq \\|h\\|_{1} \\left\\|\\frac{1}{m}(\\Phi X)^{\\top}\\Phi w\\right\\|_{\\infty} \\leq \\|h\\|_{1} \\frac{\\lambda}{2} $$\n*   **Sparsity Term**: This is a standard LASSO analysis step. Using the triangle inequality and the fact that $\\beta^{\\star}$ is supported on $S$:\n    $$ \\|\\beta^{\\star}\\|_{1} = \\|\\beta_S^{\\star}\\|_1 = \\|\\widehat{\\beta}_S - h_S\\|_1 \\le \\|\\widehat{\\beta}_S\\|_1 + \\|h_S\\|_1 $$\n    $$ \\|\\widehat{\\beta}\\|_{1} = \\|\\widehat{\\beta}_S\\|_1 + \\|\\widehat{\\beta}_{S^c}\\|_1 = \\|\\widehat{\\beta}_S\\|_1 + \\|h_{S^c}\\|_1 $$\n    Therefore, $\\|\\beta^{\\star}\\|_{1} - \\|\\widehat{\\beta}\\|_{1} \\le (\\|\\widehat{\\beta}_S\\|_1 + \\|h_S\\|_1) - (\\|\\widehat{\\beta}_S\\|_1 + \\|h_{S^c}\\|_1) = \\|h_{S}\\|_{1} - \\|h_{S^c}\\|_{1}$.\n\n**4. Combine and Simplify**\n\nSubstituting these bounds into the basic inequality:\n$$ \\frac{1}{2m}\\|\\Phi X h\\|_{2}^{2} \\leq \\frac{\\lambda}{2}\\|h\\|_{1} + \\lambda (\\|h_{S}\\|_{1} - \\|h_{S^c}\\|_{1}) $$\nUsing $\\|h\\|_{1} = \\|h_{S}\\|_{1} + \\|h_{S^c}\\|_{1}$:\n$$ \\frac{1}{2m}\\|\\Phi X h\\|_{2}^{2} \\leq \\frac{\\lambda}{2}(\\|h_{S}\\|_{1} + \\|h_{S^c}\\|_{1}) + \\lambda \\|h_{S}\\|_{1} - \\lambda \\|h_{S^c}\\|_{1} = \\frac{3\\lambda}{2}\\|h_{S}\\|_{1} - \\frac{\\lambda}{2}\\|h_{S^c}\\|_{1} $$\nSince the left side is non-negative, we must have $\\frac{3\\lambda}{2}\\|h_{S}\\|_{1} \\ge \\frac{\\lambda}{2}\\|h_{S^c}\\|_{1}$, which implies $\\|h_{S^c}\\|_{1} \\leq 3\\|h_{S}\\|_{1}$. This proves that the error vector $h$ lies in the cone $\\mathcal{C}(S)$, which is a critical step as it allows us to use the RSC condition.\n\n**5. Apply RSC and Derive the Final Bound**\n\nNow that we know $h \\in \\mathcal{C}(S)$, we can apply the new RSC condition from Part 1:\n$$ \\frac{1}{2m}\\|\\Phi X h\\|_{2}^{2} \\geq \\frac{\\tilde{\\alpha}}{2} \\|h\\|_{2}^{2} = \\frac{\\alpha(1-\\varepsilon)}{2} \\|h\\|_{2}^{2} $$\nSubstitute this into our combined inequality. We can also drop the negative term $-\\frac{\\lambda}{2}\\|h_{S^c}\\|_{1}$ to get a looser, but still valid, upper bound:\n$$ \\frac{\\alpha(1-\\varepsilon)}{2} \\|h\\|_{2}^{2} \\leq \\frac{3\\lambda}{2}\\|h_{S}\\|_{1} $$\nMultiply by 2:\n$$ \\alpha(1-\\varepsilon) \\|h\\|_{2}^{2} \\leq 3\\lambda \\|h_{S}\\|_{1} $$\nTo get a bound on $\\|h\\|_2$, we need to relate $\\|h_S\\|_1$ to $\\|h\\|_2$. Using the Cauchy-Schwarz inequality for the vector $h_S$ (which has at most $s$ non-zero entries):\n$$ \\|h_{S}\\|_{1} \\leq \\sqrt{s} \\|h_{S}\\|_{2} $$\nSince $\\|h_{S}\\|_{2} \\leq \\|h\\|_{2}$, we have:\n$$ \\alpha(1-\\varepsilon) \\|h\\|_{2}^{2} \\leq 3\\lambda \\sqrt{s} \\|h\\|_{2} $$\nAssuming $\\|h\\|_2 > 0$, we can divide by $\\|h\\|_2$:\n$$ \\|h\\|_{2} \\leq \\frac{3\\lambda\\sqrt{s}}{\\alpha(1-\\varepsilon)} $$\nThis gives the final upper bound on the $\\ell_2$-error of the estimator.\n$$ \\|\\widehat{\\beta}_{\\mathrm{FJLT}} - \\beta^{\\star}\\|_{2} \\leq \\frac{3\\lambda\\sqrt{s}}{\\alpha(1-\\varepsilon)} $$",
            "answer": "$$\n\\boxed{\\frac{3\\lambda\\sqrt{s}}{\\alpha(1-\\varepsilon)}}\n$$"
        }
    ]
}