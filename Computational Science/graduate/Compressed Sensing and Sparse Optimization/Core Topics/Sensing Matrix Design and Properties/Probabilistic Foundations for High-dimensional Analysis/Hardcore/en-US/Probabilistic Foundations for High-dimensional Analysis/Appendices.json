{
    "hands_on_practices": [
        {
            "introduction": "Many powerful results in high-dimensional analysis rely on the fact that certain properties of large random matrices are highly concentrated around their expected values. McDiarmid's bounded differences inequality is a fundamental tool for proving such concentration phenomena for functions of independent random variables. This exercise provides essential practice in applying this inequality by first deriving the coordinate Lipschitz constants for a function central to random matrix theory—the deviation of an empirical covariance matrix—and then using them to establish a high-probability bound .",
            "id": "3468743",
            "problem": "Consider a random matrix $A \\in \\mathbb{R}^{m \\times d}$ whose rows $a_1, \\dots, a_m \\in \\mathbb{R}^d$ are independent random vectors supported on the Euclidean ball of radius $R$, that is, $\\|a_i\\|_2 \\le R$ almost surely for each index $i \\in \\{1, \\dots, m\\}$. Define the normalized random projection matrix $\\Pi = \\frac{1}{\\sqrt{m}} A$. Let the function $f$ act on the independent inputs $(a_1, \\dots, a_m)$ by measuring the normalization error of the empirical covariance of $\\Pi$, namely\n$$\nf(a_1, \\dots, a_m) \\;=\\; \\left\\| \\Pi^{\\top} \\Pi - I_d \\right\\|_{F} \\;=\\; \\left\\| \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top} - I_d \\right\\|_{F},\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm and $I_d$ denotes the $d \\times d$ identity matrix. For the function $f$, define the coordinate Lipschitz constant $c_i$ as any number satisfying the bounded differences property\n$$\n\\sup_{(a_1, \\dots, a_m),\\,(a_i')} \\left| f(a_1, \\dots, a_i, \\dots, a_m) - f(a_1, \\dots, a_i', \\dots, a_m) \\right| \\;\\le\\; c_i,\n$$\nwhere the supremum is taken over all admissible inputs $(a_1, \\dots, a_m)$ and modified coordinate $a_i'$ that also satisfy $\\|a_i\\|_2 \\le R$ and $\\|a_i'\\|_2 \\le R$. Starting only from the foundational definitions of independence, the Frobenius norm, and the bounded differences property (and invoking McDiarmid’s bounded differences inequality without restating it explicitly), perform the following:\n\n- Derive a valid set of coordinate Lipschitz constants $(c_1, \\dots, c_m)$ for the function $f$ under the stated support constraints on $a_i$.\n- Use these constants to obtain a high-probability upper deviation bound for $f$ about its expectation $\\mathbb{E}[f]$ of the form $\\mathbb{P}\\big( f \\le \\mathbb{E}[f] + t_{\\delta} \\big) \\ge 1 - \\delta$ for a target failure probability $\\delta \\in (0,1)$.\n- Express, in closed form, the smallest additive deviation $t_{\\delta}$ as an analytic expression in terms of $m$, $R$, and $\\delta$ that certifies the bound $\\mathbb{P}\\big( f \\le \\mathbb{E}[f] + t_{\\delta} \\big) \\ge 1 - \\delta$.\n\nYour final answer must be a single closed-form analytic expression for $t_{\\delta}$. No rounding is required, and there are no physical units involved.",
            "solution": "The problem statement will first be validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- A random matrix $A \\in \\mathbb{R}^{m \\times d}$ has rows $a_1, \\dots, a_m \\in \\mathbb{R}^d$ which are independent random vectors.\n- The support of each random vector $a_i$ is the Euclidean ball of radius $R$: $\\|a_i\\|_2 \\le R$ almost surely for each $i \\in \\{1, \\dots, m\\}$.\n- A normalized random projection matrix is defined as $\\Pi = \\frac{1}{\\sqrt{m}} A$.\n- A function $f$ is defined on the inputs $(a_1, \\dots, a_m)$ as:\n$$\nf(a_1, \\dots, a_m) = \\left\\| \\Pi^{\\top} \\Pi - I_d \\right\\|_{F} = \\left\\| \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top} - I_d \\right\\|_{F}\n$$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm and $I_d$ is the $d \\times d$ identity matrix.\n- The coordinate Lipschitz constant $c_i$ for the function $f$ must satisfy the bounded differences property:\n$$\n\\sup_{(a_1, \\dots, a_m),\\,(a_i')} \\left| f(a_1, \\dots, a_i, \\dots, a_m) - f(a_1, \\dots, a_i', \\dots, a_m) \\right| \\le c_i\n$$\nwhere the supremum is over all inputs satisfying $\\|a_j\\|_2 \\le R$ for all $j$ and $\\|a_i'\\|_2 \\le R$.\n- The task is to derive the constants $c_i$ and use them with McDiarmid’s inequality to find the smallest additive deviation $t_{\\delta}$ that certifies the high-probability bound $\\mathbb{P}\\big( f \\le \\mathbb{E}[f] + t_{\\delta} \\big) \\ge 1 - \\delta$ for $\\delta \\in (0,1)$.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the fields of random matrix theory, high-dimensional probability, and linear algebra. The concepts of random projections, Frobenius norm, and concentration inequalities are standard and rigorously defined. The expression $\\left\\| \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top} - I_d \\right\\|_{F}$ represents the error of a sample covariance matrix relative to the identity, a central object of study in high-dimensional statistics and its applications, such as compressed sensing.\n- **Well-Posed**: The problem is well-posed. It asks for the derivation of a specific quantity ($t_{\\delta}$) based on a clearly defined function and set of constraints. The existence and uniqueness of the requested analytic expression are guaranteed by the structure of the problem.\n- **Objective**: The problem is stated using precise mathematical language and definitions. There are no subjective or ambiguous terms.\n- **Self-Contained and Consistent**: The problem provides all necessary definitions and constraints. The identity $\\Pi^{\\top} \\Pi = \\frac{1}{m} \\sum_{i=1}^{m} a_i a_i^{\\top}$ is a direct consequence of the definitions of $A$ and $\\Pi$, and is thus consistent.\n- **Other Flaws**: The problem does not violate any of the other invalidity criteria. It is not metaphorical, trivial, or unverifiable.\n\n#### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with a complete, reasoned solution.\n\n### Solution Derivation\n\nThe goal is to find an expression for $t_{\\delta}$ in terms of $m$, $R$, and $\\delta$. This involves two main steps: first, deriving a set of coordinate Lipschitz constants $c_i$ for the function $f$, and second, applying McDiarmid's bounded differences inequality.\n\n**Step 1: Derive the Coordinate Lipschitz Constants $(c_1, \\dots, c_m)$**\n\nLet the set of input vectors be denoted by $\\mathbf{a} = (a_1, \\dots, a_m)$. Let $\\mathbf{a}'$ be an identical set of inputs, except for the $i$-th vector, which is replaced by $a_i'$. Both $a_i$ and $a_i'$ must satisfy the given support constraint, i.e., $\\|a_i\\|_2 \\le R$ and $\\|a_i'\\|_2 \\le R$.\n\nLet $M(\\mathbf{a}) = \\frac{1}{m} \\sum_{j=1}^{m} a_j a_j^{\\top} - I_d$. The function is $f(\\mathbf{a}) = \\|M(\\mathbf{a})\\|_F$. We need to bound the difference $|f(\\mathbf{a}) - f(\\mathbf{a}')|$.\nBy the reverse triangle inequality for the Frobenius norm, we have:\n$$\n|f(\\mathbf{a}) - f(\\mathbf{a}')| = \\big| \\|M(\\mathbf{a})\\|_F - \\|M(\\mathbf{a}')\\|_F \\big| \\le \\|M(\\mathbf{a}) - M(\\mathbf{a}')\\|_F\n$$\nThe difference of the matrices is:\n$$\nM(\\mathbf{a}) - M(\\mathbf{a}') = \\left(\\frac{1}{m} \\sum_{j=1}^{m} a_j a_j^{\\top} - I_d\\right) - \\left(\\frac{1}{m} \\left(\\sum_{j \\ne i} a_j a_j^{\\top} + a_i' a_i'^{\\top}\\right) - I_d\\right) = \\frac{1}{m} (a_i a_i^{\\top} - a_i' a_i'^{\\top})\n$$\nSubstituting this into the inequality:\n$$\n|f(\\mathbf{a}) - f(\\mathbf{a}')| \\le \\left\\| \\frac{1}{m} (a_i a_i^{\\top} - a_i' a_i'^{\\top}) \\right\\|_F = \\frac{1}{m} \\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F\n$$\nUsing the triangle inequality for the Frobenius norm on the right-hand side:\n$$\n\\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F \\le \\|a_i a_i^{\\top}\\|_F + \\|a_i' a_i'^{\\top}\\|_F\n$$\nNow, we evaluate the Frobenius norm of a rank-one matrix of the form $v v^{\\top}$ for a vector $v \\in \\mathbb{R}^d$. The squared Frobenius norm is the sum of the squares of all matrix entries. The entries of $v v^{\\top}$ are $(v v^{\\top})_{jk} = v_j v_k$.\n$$\n\\|v v^{\\top}\\|_F^2 = \\sum_{j=1}^{d} \\sum_{k=1}^{d} ((v v^{\\top})_{jk})^2 = \\sum_{j=1}^{d} \\sum_{k=1}^{d} (v_j v_k)^2 = \\left(\\sum_{j=1}^{d} v_j^2\\right) \\left(\\sum_{k=1}^{d} v_k^2\\right) = (\\|v\\|_2^2) (\\|v\\|_2^2) = \\|v\\|_2^4\n$$\nTaking the square root gives the identity $\\|v v^{\\top}\\|_F = \\|v\\|_2^2$.\n\nApplying this identity to our inequality:\n$$\n\\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F \\le \\|a_i\\|_2^2 + \\|a_i'\\|_2^2\n$$\nUsing the given support constraints, $\\|a_i\\|_2 \\le R$ and $\\|a_i'\\|_2 \\le R$:\n$$\n\\|a_i a_i^{\\top} - a_i' a_i'^{\\top}\\|_F \\le R^2 + R^2 = 2R^2\n$$\nSubstituting this result back into the bound for the function difference:\n$$\n|f(\\mathbf{a}) - f(\\mathbf{a}')| \\le \\frac{1}{m} (2R^2) = \\frac{2R^2}{m}\n$$\nThis inequality holds for any choice of admissible inputs. The supremum of the left-hand side is therefore bounded by the constant on the right-hand side. This allows us to set the coordinate Lipschitz constants $c_i$ for all $i \\in \\{1, \\dots, m\\}$:\n$$\nc_i = \\frac{2R^2}{m}\n$$\n\n**Step 2: Obtain the High-Probability Bound using McDiarmid's Inequality**\n\nMcDiarmid's bounded differences inequality states that for a function $f$ of $m$ independent variables satisfying the bounded differences property with constants $c_i$, the following holds for any $t>0$:\n$$\n\\mathbb{P}(f(a_1, \\dots, a_m) - \\mathbb{E}[f] \\ge t) \\le \\exp\\left( \\frac{-2t^2}{\\sum_{i=1}^m c_i^2} \\right)\n$$\nWe need to calculate the sum of the squares of our constants:\n$$\n\\sum_{i=1}^m c_i^2 = \\sum_{i=1}^m \\left(\\frac{2R^2}{m}\\right)^2 = m \\cdot \\frac{4R^4}{m^2} = \\frac{4R^4}{m}\n$$\nThe problem requires finding $t_{\\delta}$ such that $\\mathbb{P}(f \\le \\mathbb{E}[f] + t_{\\delta}) \\ge 1 - \\delta$. This is equivalent to finding $t_{\\delta}$ that satisfies $\\mathbb{P}(f - \\mathbb{E}[f] > t_{\\delta}) \\le \\delta$. As $f$ is a continuous random variable, this is equivalent to $\\mathbb{P}(f - \\mathbb{E}[f] \\ge t_{\\delta}) \\le \\delta$.\n\nWe equate the upper bound from McDiarmid's inequality to the target failure probability $\\delta$, with $t = t_{\\delta}$:\n$$\n\\delta = \\exp\\left( \\frac{-2t_{\\delta}^2}{\\sum_{i=1}^m c_i^2} \\right) = \\exp\\left( \\frac{-2t_{\\delta}^2}{4R^4/m} \\right) = \\exp\\left( \\frac{-2mt_{\\delta}^2}{4R^4} \\right) = \\exp\\left( \\frac{-mt_{\\delta}^2}{2R^4} \\right)\n$$\nNow, we solve for $t_{\\delta}$. Taking the natural logarithm of both sides:\n$$\n\\ln(\\delta) = -\\frac{mt_{\\delta}^2}{2R^4}\n$$\nMultiplying by $-1$ gives:\n$$\n-\\ln(\\delta) = \\ln\\left(\\frac{1}{\\delta}\\right) = \\frac{mt_{\\delta}^2}{2R^4}\n$$\nIsolating $t_{\\delta}^2$:\n$$\nt_{\\delta}^2 = \\frac{2R^4}{m} \\ln\\left(\\frac{1}{\\delta}\\right)\n$$\nFinally, taking the square root yields the expression for the smallest additive deviation $t_{\\delta}$ that certifies the bound:\n$$\nt_{\\delta} = \\sqrt{\\frac{2R^4}{m} \\ln\\left(\\frac{1}{\\delta}\\right)} = R^2 \\sqrt{\\frac{2 \\ln(1/\\delta)}{m}}\n$$\nThis is the closed-form analytic expression for $t_{\\delta}$ in terms of $m$, $R$, and $\\delta$.",
            "answer": "$$\n\\boxed{R^2 \\sqrt{\\frac{2 \\ln(1/\\delta)}{m}}}\n$$"
        },
        {
            "introduction": "To understand when sparse recovery algorithms like Basis Pursuit succeed, we must analyze the geometry of the underlying optimization problem. The Gaussian width of a cone offers a sharp probabilistic measure of its \"size,\" which remarkably predicts the number of random measurements required for successful recovery. In this practice, you will connect concepts from convex analysis, such as subdifferentials and descent cones, with the modern tool of statistical dimension to derive the celebrated phase transition curve for sparse signal recovery using the $\\ell_1$ norm .",
            "id": "3468782",
            "problem": "Let $x^\\star \\in \\mathbb{R}^{n}$ be a vector with support $T \\subset \\{1,\\dots,n\\}$, $|T|=k$, whose nonzero entries have arbitrary fixed signs. Consider the convex optimization program known as Basis Pursuit (BP),\n$$\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y,$$\nwith a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ having independent and identically distributed standard normal entries and noiseless measurements $y = Ax^\\star$. The success of BP can be predicted by the probabilistic geometry of convex cones: exact recovery typically occurs when the number of measurements $m$ is on the order of the squared Gaussian width $w(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star))^{2}$ of the descent cone $\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star)$ at $x^\\star$.\n\nStarting from the definitions and fundamental facts below, derive an analytic expression for $w(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star))^{2}$ as a function of $n$ and $k$:\n- The descent cone of a proper convex function $f$ at a point $x$ is $\\mathcal{D}(f,x) := \\{d \\in \\mathbb{R}^{n} : \\exists t>0 \\text{ with } f(x+td) \\le f(x)\\}$.\n- The Gaussian width of a set $C \\subset \\mathbb{R}^{n}$ is $w(C) := \\mathbb{E}\\big[\\sup_{u \\in C \\cap S^{n-1}} \\langle g,u \\rangle \\big]$, where $g \\sim \\mathcal{N}(0,I_{n})$ and $S^{n-1}$ is the unit sphere in $\\mathbb{R}^{n}$.\n- For a proper convex function $f$ and a point $x$, the subdifferential $\\partial f(x)$ is the set of all $s \\in \\mathbb{R}^{n}$ such that $f(z) \\ge f(x) + \\langle s, z-x \\rangle$ for all $z \\in \\mathbb{R}^{n}$.\n\nUse these bases to:\n- Characterize $\\partial \\|\\cdot\\|_{1}(x^\\star)$ in terms of $T$ and the signs of $x^\\star$.\n- Express the statistical dimension of the descent cone $\\delta\\big(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star)\\big)$ via a minimization over a scalar $\\tau \\ge 0$ involving the expected squared distance of a standard Gaussian vector $g \\sim \\mathcal{N}(0,I_{n})$ to the scaled subdifferential $\\tau \\partial \\|\\cdot\\|_{1}(x^\\star)$.\n- Evaluate the resulting expectation using standard normal tail identities to obtain a closed-form integrand in terms of the standard normal density $\\phi(t)$ and complementary cumulative distribution function $Q(t)$.\n\nFinally, use the probabilistic phase transition heuristic $m \\approx w(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star))^{2}$ to state the measurement threshold for exact recovery in terms of $n$ and $k$. Express your final answer as a single closed-form analytic expression for $w(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star))^{2}$ involving $\\phi$, $Q$, and a scalar $\\tau^\\star \\ge 0$ that satisfies the first-order optimality condition. No numerical approximation or rounding is required.",
            "solution": "We begin from the definitions. The descent cone of the $\\ell_{1}$ norm at $x^\\star$ is\n$$\n\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star) = \\{d \\in \\mathbb{R}^{n} : \\exists t>0 \\text{ such that } \\|x^\\star + t d\\|_{1} \\le \\|x^\\star\\|_{1}\\}.\n$$\nA fundamental convex-analytic fact for norms is that the subdifferential at $x^\\star$ is determined by the support and signs of $x^\\star$. Let $T$ be the support of $x^\\star$, with $|T|=k$. Then\n$$\n\\partial \\| \\cdot \\|_{1}(x^\\star) = \\left\\{ s \\in \\mathbb{R}^{n} : s_{i} = \\operatorname{sgn}(x^\\star_{i}) \\text{ for } i \\in T,\\ \\text{and}\\ s_{j} \\in [-1,1] \\text{ for } j \\in T^{c} \\right\\}.\n$$\nWe connect geometric prediction of Basis Pursuit success with conic integral geometry. The statistical dimension $\\delta(C)$ of a closed convex cone $C \\subset \\mathbb{R}^{n}$ is defined as\n$$\n\\delta(C) := \\mathbb{E}\\big[\\|\\Pi_{C}(g)\\|^{2}\\big],\n$$\nwhere $g \\sim \\mathcal{N}(0,I_{n})$ and $\\Pi_{C}$ denotes the Euclidean projection onto $C$. For descent cones of norms, a well-tested identity expresses the statistical dimension as the minimum expected squared distance from a Gaussian vector to scaled subgradients:\n$$\n\\delta\\big(\\mathcal{D}( \\|\\cdot\\|_{1}, x^\\star )\\big) = \\inf_{\\tau \\ge 0}\\ \\mathbb{E}\\Big[ \\operatorname{dist}^{2}\\big(g, \\tau \\,\\partial \\| \\cdot \\|_{1}(x^\\star)\\big) \\Big],\n$$\nwhere $g \\sim \\mathcal{N}(0,I_{n})$ and $\\operatorname{dist}(u,S) := \\inf_{v \\in S} \\|u-v\\|$.\n\nWe now compute $\\mathbb{E}\\big[ \\operatorname{dist}^{2}(g, \\tau \\partial \\|\\cdot\\|_{1}(x^\\star)) \\big]$. Because the subdifferential decomposes coordinatewise and $g$ has independent standard normal coordinates, the squared distance decomposes as a sum of independent contributions from coordinates in $T$ and $T^{c}$.\n\n- For $i \\in T$, the $i$th coordinate of any $v \\in \\tau \\partial \\|\\cdot\\|_{1}(x^\\star)$ is fixed as $v_{i} = \\tau\\,\\operatorname{sgn}(x^\\star_{i})$. Therefore, the squared distance contribution at coordinate $i$ is $(g_{i} - \\tau\\,\\operatorname{sgn}(x^\\star_{i}))^{2}$. Taking expectation over $g_{i} \\sim \\mathcal{N}(0,1)$ and using $\\mathbb{E}[g_{i}] = 0$ and $\\mathbb{E}[g_{i}^{2}] = 1$ yields\n$$\n\\mathbb{E}\\big[(g_{i} - \\tau\\,\\operatorname{sgn}(x^\\star_{i}))^{2}\\big] = \\mathbb{E}[g_{i}^{2}] + \\tau^{2} = 1 + \\tau^{2}.\n$$\nSumming over $k$ coordinates in $T$ gives the contribution $k(1+\\tau^{2})$.\n\n- For $j \\in T^{c}$, the feasible interval is $v_{j} \\in [-\\tau, \\tau]$. The closest point in $[-\\tau, \\tau]$ to a given $g_{j}$ is the projection $v_{j} = \\operatorname{clip}(g_{j}; -\\tau, \\tau)$, so the squared distance contribution is $(|g_{j}| - \\tau)_{+}^{2}$, where $(a)_{+} := \\max\\{a,0\\}$. Because $g_{j} \\sim \\mathcal{N}(0,1)$ and $|g_{j}|$ has a folded normal distribution, we can evaluate\n$$\n\\mathbb{E}\\big[(|g| - \\tau)_{+}^{2}\\big] = 2 \\int_{\\tau}^{\\infty} (u-\\tau)^{2}\\, \\phi(u)\\, \\mathrm{d}u,\n$$\nwhere $\\phi(u) := \\frac{1}{\\sqrt{2\\pi}} \\exp\\big(-\\frac{u^{2}}{2}\\big)$ is the standard normal density. Expanding and using well-known Gaussian tail identities\n$$\n\\int_{\\tau}^{\\infty} \\phi(u)\\, \\mathrm{d}u = Q(\\tau), \\quad \\int_{\\tau}^{\\infty} u\\, \\phi(u)\\, \\mathrm{d}u = \\phi(\\tau), \\quad \\int_{\\tau}^{\\infty} u^{2}\\, \\phi(u)\\, \\mathrm{d}u = \\tau\\, \\phi(\\tau) + Q(\\tau),\n$$\nwith $Q(\\tau) := 1 - \\Phi(\\tau)$ the complementary cumulative distribution function of the standard normal, we obtain\n$$\n\\mathbb{E}\\big[(|g| - \\tau)_{+}^{2}\\big] = 2\\Big( (1+\\tau^{2})\\, Q(\\tau) - \\tau\\, \\phi(\\tau) \\Big).\n$$\nSumming this contribution over the $n-k$ coordinates in $T^{c}$ yields $(n-k)$ times the above expectation.\n\nCombining, the objective function for the statistical dimension is\n$$\nF(\\tau) := \\mathbb{E}\\Big[ \\operatorname{dist}^{2}\\big(g, \\tau \\,\\partial \\| \\cdot \\|_{1}(x^\\star)\\big) \\Big] = k(1+\\tau^{2}) + (n-k)\\, 2\\Big( (1+\\tau^{2})\\, Q(\\tau) - \\tau\\, \\phi(\\tau) \\Big),\n$$\nand\n$$\n\\delta\\big(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star)\\big) = \\inf_{\\tau \\ge 0} F(\\tau).\n$$\n\nTo identify the minimizing $\\tau^\\star \\ge 0$, we differentiate $F(\\tau)$ and set the derivative to zero. Compute\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\tau}\\left[2\\Big( (1+\\tau^{2})\\, Q(\\tau) - \\tau\\, \\phi(\\tau) \\Big)\\right] = 4\\big( \\tau\\, Q(\\tau) - \\phi(\\tau) \\big),\n$$\nusing $\\frac{\\mathrm{d}}{\\mathrm{d}\\tau} Q(\\tau) = -\\phi(\\tau)$ and $\\frac{\\mathrm{d}}{\\mathrm{d}\\tau} \\phi(\\tau) = -\\tau\\, \\phi(\\tau)$. Therefore,\n$$\nF'(\\tau) = 2k\\, \\tau + (n-k)\\, 4\\big( \\tau\\, Q(\\tau) - \\phi(\\tau) \\big).\n$$\nThe first-order optimality condition $F'(\\tau^\\star)=0$ becomes\n$$\nk\\, \\tau^\\star + 2(n-k)\\big( \\tau^\\star\\, Q(\\tau^\\star) - \\phi(\\tau^\\star) \\big) = 0,\n$$\nequivalently,\n$$\nk\\, \\tau^\\star = 2(n-k)\\big( \\phi(\\tau^\\star) - \\tau^\\star\\, Q(\\tau^\\star) \\big).\n$$\nThis equation has a unique solution $\\tau^\\star \\ge 0$ because the left-hand side is strictly increasing in $\\tau$ and the right-hand side is strictly decreasing in $\\tau$.\n\nThus, the statistical dimension is\n$$\n\\delta\\big(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star)\\big) = F(\\tau^\\star) = k\\big(1+(\\tau^\\star)^{2}\\big) + 2(n-k)\\Big( \\big(1+(\\tau^\\star)^{2}\\big)\\, Q(\\tau^\\star) - \\tau^\\star\\, \\phi(\\tau^\\star) \\Big).\n$$\n\nWe now relate the Gaussian width and the statistical dimension. For a closed convex cone $C$, it is known that\n$$\nw\\big(C \\cap S^{n-1}\\big)^{2} \\le \\delta(C) \\le w\\big(C \\cap S^{n-1}\\big)^{2} + 1,\n$$\nso $\\delta(C)$ is an accurate proxy for $w(C \\cap S^{n-1})^{2}$, especially in high dimensions. In the conic BP setting, the phase transition for exact recovery with a Gaussian sensing matrix occurs when the number of measurements $m$ is approximately equal to $\\delta\\big(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star)\\big)$, matching the heuristic $m \\approx w(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star))^{2}$.\n\nTherefore, the squared Gaussian width and the predicted measurement threshold can be stated in the unified closed form\n$$\nw\\big(\\mathcal{D}(\\|\\cdot\\|_{1}, x^\\star)\\big)^{2} \\approx k\\big(1+(\\tau^\\star)^{2}\\big) + 2(n-k)\\Big( \\big(1+(\\tau^\\star)^{2}\\big)\\, Q(\\tau^\\star) - \\tau^\\star\\, \\phi(\\tau^\\star) \\Big),\n$$\nwhere $\\tau^\\star \\ge 0$ is the unique solution of $k \\tau = 2(n-k)\\big( \\phi(\\tau) - \\tau Q(\\tau) \\big)$, and $\\phi$ and $Q$ are the standard normal density and complementary cumulative distribution functions, respectively. Consequently, the BP phase transition is predicted by\n$$\nm_{\\mathrm{PT}}(n,k) \\approx k\\big(1+(\\tau^\\star)^{2}\\big) + 2(n-k)\\Big( \\big(1+(\\tau^\\star)^{2}\\big)\\, Q(\\tau^\\star) - \\tau^\\star\\, \\phi(\\tau^\\star) \\Big).\n$$\nNo numerical approximation is necessary; this is an analytic expression in terms of $n$, $k$, $\\phi$, $Q$, and $\\tau^\\star$.",
            "answer": "$$\\boxed{k\\big(1+(\\tau^\\star)^{2}\\big) + 2(n-k)\\Big( \\big(1+(\\tau^\\star)^{2}\\big)\\, Q(\\tau^\\star) - \\tau^\\star\\, \\phi(\\tau^\\star) \\Big)}$$"
        },
        {
            "introduction": "The geometric framework is especially powerful for comparing the efficiency of different recovery models, allowing us to precisely quantify the benefits of incorporating prior structural knowledge. This final practice extends the Gaussian width analysis to the group Lasso setting, where sparsity occurs at the level of predetermined groups of variables. By comparing the resulting measurement complexity to that of the standard $\\ell_1$ model, you will uncover the concrete gain in statistical efficiency achieved by exploiting this group structure .",
            "id": "3468742",
            "problem": "Consider the following high-dimensional recovery setting. Let $n = G g$ with $G \\in \\mathbb{N}$ disjoint groups $G_{1},\\dots,G_{G}$ each of size $g \\in \\mathbb{N}$. Let $x_{0} \\in \\mathbb{R}^{n}$ have exactly $s$ active groups, that is, the index set $S \\subset \\{1,\\dots,G\\}$ with $|S| = s$ satisfies $x_{0,G_{i}} \\neq 0$ if and only if $i \\in S$. Assume $x_{0,G_{i}}$ has generic nonzero entries for each $i \\in S$ and $x_{0,G_{i}} = 0$ for $i \\notin S$. Define the block norm $f_{\\mathrm{grp}}(x) = \\sum_{i=1}^{G} \\|x_{G_{i}}\\|_{2}$ (the group-sparsity norm with group size $g$) and the standard $\\ell_{1}$ norm $f_{1}(x) = \\|x\\|_{1}$. For a proper convex function $f$, define the descent cone at $x_{0}$ by $D(f,x_{0}) = \\{h \\in \\mathbb{R}^{n} : \\exists\\, t > 0 \\text{ with } f(x_{0} + t h) \\le f(x_{0})\\}$, and define the Gaussian mean width by $w(T) = \\mathbb{E}[\\sup_{u \\in T} \\langle g, u \\rangle]$ for $g \\sim \\mathcal{N}(0,I_{n})$.\n\nIt is a well-tested fact in conic integral geometry that the statistical dimension $\\delta(C)$ of a closed convex cone $C$ approximates the squared Gaussian width $w(C \\cap S^{n-1})^{2}$ to within an absolute constant error, and that, for descent cones of norms, $\\delta(D(f,x_{0}))$ admits the characterization\n$$\n\\delta(D(f,x_{0})) \\;=\\; \\inf_{\\tau \\ge 0} \\,\\mathbb{E}\\big[ \\mathrm{dist}^{2}\\big(g,\\, \\tau\\, \\partial f(x_{0})\\big) \\big],\n$$\nwhere $\\partial f(x_{0})$ is the subdifferential of $f$ at $x_{0}$ and $\\mathrm{dist}(y,S) = \\inf_{s \\in S}\\|y - s\\|_{2}$.\n\nUsing only the definitions above and standard properties of multivariate normal distributions and chi distributions (no other specialized results), do the following:\n\n1. For $f_{\\mathrm{grp}}$, derive from first principles the block-separable variational expression for $\\delta\\big(D(f_{\\mathrm{grp}},x_{0})\\big)$ as an explicit function of $G$, $s$, and $g$, expressed as an infimum over a nonnegative scalar parameter. Your derivation must start from the definitions of the subdifferential of $f_{\\mathrm{grp}}$ at $x_{0}$, the distance to a product set, and the distribution of the Euclidean norm of a Gaussian vector.\n\n2. For $f_{1}$ at a $k$-sparse $x_{0}$ with support size $k = s g$, derive the corresponding scalar variational expression for $\\delta\\big(D(f_{1},x_{0})\\big)$ as a function of $n = G g$ and $k$, again as an infimum over a nonnegative scalar parameter.\n\n3. In the asymptotic regime $G \\to \\infty$ with $g$ fixed and $s = s(G)$ satisfying $s = o(G)$, analyze the leading-order behavior of both variational expressions. You may use standard tail asymptotics for the absolute value of a standard normal random variable and for the chi distribution with $g$ degrees of freedom, but you must clearly show how they are applied. From your analysis, extract the leading-order terms of $\\delta\\big(D(f_{\\mathrm{grp}},x_{0})\\big)$ and $\\delta\\big(D(f_{1},x_{0})\\big)$, sufficient to determine their asymptotic ratio.\n\nReport, as your final answer, the leading-order asymptotic ratio of the measurement complexities (equivalently, the squared Gaussian widths of the tangent cones) for group Lasso versus standard $\\ell_{1}$ when $k = s g$ and $n = G g$, expressed purely as a closed-form analytic function of $g$. No numerical evaluation is required. The final answer must be a single symbolic expression.",
            "solution": "The problem asks for the derivation and asymptotic analysis of the statistical dimension of the descent cones for the group Lasso norm ($f_{\\mathrm{grp}}$) and the standard $\\ell_1$ norm ($f_1$). The final goal is to find the asymptotic ratio of these two quantities.\n\nThe statistical dimension of the descent cone $D(f, x_0)$ is given by the formula:\n$$\n\\delta(D(f,x_{0})) \\;=\\; \\inf_{\\tau \\ge 0} \\,\\mathbb{E}\\big[ \\mathrm{dist}^{2}\\big(\\mathbf{g},\\, \\tau\\, \\partial f(x_{0})\\big) \\big]\n$$\nwhere $\\mathbf{g} \\sim \\mathcal{N}(0,I_{n})$ is a standard multivariate normal random vector.\n\n### Part 1: Statistical Dimension for the Group-Sparsity Norm ($f_{\\mathrm{grp}}$)\n\nFirst, we derive the variational expression for $\\delta(D(f_{\\mathrm{grp}}, x_0))$.\nThe group-sparsity norm is $f_{\\mathrm{grp}}(x) = \\sum_{i=1}^{G} \\|x_{G_i}\\|_2$. Let $S$ be the set of indices of active groups, with $|S|=s$. For $i \\in S$, $x_{0,G_i} \\neq 0$, and for $i \\notin S$, $x_{0,G_i} = 0$.\n\nThe subdifferential of $f_{\\mathrm{grp}}$ at $x_0$ is the Cartesian product of the subdifferentials for each block:\n$\\partial f_{\\mathrm{grp}}(x_0) = \\prod_{i=1}^{G} \\partial (\\|\\cdot\\|_2)(x_{0,G_i})$.\nFor an active block ($i \\in S$), $x_{0,G_i} \\neq 0$. The subdifferential of the Euclidean norm $\\|\\cdot\\|_2$ at a non-zero vector $u$ is the singleton $\\{\\frac{u}{\\|u\\|_2}\\}$. Thus, $\\partial (\\|\\cdot\\|_2)(x_{0,G_i}) = \\{v_i\\}$, where $v_i = \\frac{x_{0,G_i}}{\\|x_{0,G_i}\\|_2}$ is a unit vector in $\\mathbb{R}^g$.\nFor an inactive block ($i \\notin S$), $x_{0,G_i} = 0$. The subdifferential of the Euclidean norm at the origin is the closed unit ball $\\mathbb{B}_2^g = \\{u \\in \\mathbb{R}^g : \\|u\\|_2 \\le 1\\}$.\n\nSo, the subdifferential is the product set $\\partial f_{\\mathrm{grp}}(x_0) = \\prod_{i=1}^G K_i$, where $K_i = \\{v_i\\}$ for $i \\in S$ and $K_i = \\mathbb{B}_2^g$ for $i \\notin S$. The set $\\tau \\partial f_{\\mathrm{grp}}(x_0)$ is the product $\\prod_{i=1}^G \\tau K_i$.\n\nLet $\\mathbf{g} \\sim \\mathcal{N}(0, I_n)$ be partitioned into blocks $\\mathbf{g}_i = \\mathbf{g}_{G_i} \\in \\mathbb{R}^g$ for $i=1, \\dots, G$. Each $\\mathbf{g}_i$ is an independent standard normal vector in $\\mathbb{R}^g$. The squared distance from $\\mathbf{g}$ to the product set $\\tau \\partial f_{\\mathrm{grp}}(x_0)$ is the sum of the squared distances from each block $\\mathbf{g}_i$ to the corresponding set $\\tau K_i$:\n$$\n\\mathrm{dist}^2(\\mathbf{g}, \\tau \\partial f_{\\mathrm{grp}}(x_0)) = \\sum_{i=1}^G \\mathrm{dist}^2(\\mathbf{g}_i, \\tau K_i) = \\sum_{i \\in S} \\mathrm{dist}^2(\\mathbf{g}_i, \\tau v_i) + \\sum_{i \\notin S} \\mathrm{dist}^2(\\mathbf{g}_i, \\tau \\mathbb{B}_2^g)\n$$\nFor $i \\in S$, the distance is to a point $\\tau v_i$:\n$\\mathrm{dist}^2(\\mathbf{g}_i, \\tau v_i) = \\|\\mathbf{g}_i - \\tau v_i\\|_2^2 = \\|\\mathbf{g}_i\\|_2^2 - 2\\tau\\langle \\mathbf{g}_i, v_i \\rangle + \\tau^2\\|v_i\\|_2^2 = \\|\\mathbf{g}_i\\|_2^2 - 2\\tau\\langle \\mathbf{g}_i, v_i \\rangle + \\tau^2$.\nFor $i \\notin S$, the distance is to a ball of radius $\\tau$. This distance is $(\\|\\mathbf{g}_i\\|_2 - \\tau)_+ = \\max(0, \\|\\mathbf{g}_i\\|_2 - \\tau)$. The squared distance is $\\left( (\\|\\mathbf{g}_i\\|_2 - \\tau)_+ \\right)^2$.\n\nNow we compute the expectation. By linearity of expectation:\n$\\mathbb{E}[\\mathrm{dist}^2(\\mathbf{g}, \\tau \\partial f_{\\mathrm{grp}}(x_0))] = \\sum_{i \\in S} \\mathbb{E}[\\|\\mathbf{g}_i\\|_2^2 - 2\\tau\\langle \\mathbf{g}_i, v_i \\rangle + \\tau^2] + \\sum_{i \\notin S} \\mathbb{E}[\\left( (\\|\\mathbf{g}_i\\|_2 - \\tau)_+ \\right)^2]$.\nFor $i \\in S$: $\\mathbf{g}_i \\sim \\mathcal{N}(0, I_g)$. $\\mathbb{E}[\\|\\mathbf{g}_i\\|_2^2] = g$. Since $v_i$ is a fixed unit vector, $\\langle \\mathbf{g}_i, v_i \\rangle \\sim \\mathcal{N}(0,1)$, so its expectation is $0$. Thus, $\\mathbb{E}[\\mathrm{dist}^2(\\mathbf{g}_i, \\tau v_i)] = g + \\tau^2$.\nFor $i \\notin S$: Let $\\chi_g = \\|\\mathbf{g}_i\\|_2$ denote a random variable following the chi-distribution with $g$ degrees of freedom. The expectation is $\\mathbb{E}[((\\chi_g - \\tau)_+)^2]$.\n\nThere are $s$ active groups and $G-s$ inactive groups. The total expectation is:\n$$\n\\mathbb{E}[\\mathrm{dist}^2(\\mathbf{g}, \\tau \\partial f_{\\mathrm{grp}}(x_0))] = s(g+\\tau^2) + (G-s) \\mathbb{E}[((\\chi_g - \\tau)_+)^2]\n$$\nTaking the infimum over $\\tau \\ge 0$, we get the final expression for the first part:\n$$\n\\delta(D(f_{\\mathrm{grp}}, x_0)) = \\inf_{\\tau \\ge 0} \\left\\{ s(g+\\tau^2) + (G-s) \\mathbb{E}[((\\chi_g - \\tau)_+)^2] \\right\\}\n$$\n\n### Part 2: Statistical Dimension for the $\\ell_1$ Norm ($f_1$)\n\nNext, we derive the expression for $\\delta(D(f_1, x_0))$ for a $k$-sparse vector $x_0$ with $k=sg$ and total dimension $n=Gg$. Let $T$ be the support of $x_0$, so $|T|=k=sg$.\nThe $\\ell_1$ norm is $f_1(x) = \\|x\\|_1 = \\sum_{j=1}^n |x_j|$. For $x_0$, we assume $x_{0,j} \\neq 0$ for $j \\in T$.\nThe subdifferential of $f_1$ at $x_0$ is $\\partial f_1(x_0) = \\{u \\in \\mathbb{R}^n : u_j = \\mathrm{sgn}(x_{0,j}) \\text{ for } j \\in T \\text{ and } u_j \\in [-1,1] \\text{ for } j \\notin T\\}$.\nLet $v=\\mathrm{sgn}(x_0)$. The set $\\tau\\partial f_1(x_0)$ consists of vectors $u$ such that $u_T = \\tau v_T$ and $u_{T^c}$ lies in the hypercube $[-\\tau, \\tau]^{n-k}$.\n\nThe squared distance from $\\mathbf{g} \\sim \\mathcal{N}(0,I_n)$ to this set is:\n$$\n\\mathrm{dist}^2(\\mathbf{g}, \\tau \\partial f_1(x_0)) = \\|\\mathbf{g}_T - \\tau v_T\\|_2^2 + \\mathrm{dist}^2(\\mathbf{g}_{T^c}, [-\\tau, \\tau]^{n-k})\n$$\nThe first term is $\\sum_{j \\in T} (\\mathbf{g}_j - \\tau v_j)^2$. The second term is $\\sum_{j \\notin T} (\\mathrm{dist}(\\mathbf{g}_j, [-\\tau, \\tau]))^2 = \\sum_{j \\notin T} ((|\\mathbf{g}_j|-\\tau)_+)^2$.\n\nWe take the expectation:\n$\\mathbb{E}[\\sum_{j \\in T} (\\mathbf{g}_j - \\tau v_j)^2] = \\sum_{j \\in T} \\mathbb{E}[\\mathbf{g}_j^2 - 2\\tau v_j \\mathbf{g}_j + \\tau^2 v_j^2]$. Since $\\mathbf{g}_j \\sim \\mathcal{N}(0,1)$ and $v_j^2=1$, this expectation is $1-0+\\tau^2 = 1+\\tau^2$ for each $j \\in T$. Summing over the $k$ elements of $T$ gives $k(1+\\tau^2)$.\nFor the second term, we have $n-k$ i.i.d. standard normal variables $\\mathbf{g}_j$. Let $Z \\sim \\mathcal{N}(0,1)$. The expectation is $(n-k) \\mathbb{E}[((|Z|-\\tau)_+)^2]$.\n\nThe total expectation is $k(1+\\tau^2) + (n-k) \\mathbb{E}[((|Z|-\\tau)_+)^2]$.\nSubstituting $k=sg$ and $n=Gg$, we have $n-k = Gg-sg = g(G-s)$.\n$$\n\\mathbb{E}[\\dots] = sg(1+\\tau^2) + g(G-s) \\mathbb{E}[((|Z|-\\tau)_+)^2]\n$$\nThe variable $|Z|$ is distributed as $\\chi_1$. The expression is:\n$$\n\\delta(D(f_1, x_0)) = \\inf_{\\tau \\ge 0} \\left\\{ sg(1+\\tau^2) + g(G-s) \\mathbb{E}[((|Z|-\\tau)_+)^2] \\right\\}\n$$\n\n### Part 3: Asymptotic Analysis\n\nWe analyze the two variational problems in the regime $G \\to \\infty$ with $g$ fixed and $s=o(G)$. This implies $s/G \\to 0$.\nThe objective functions are of the form $\\phi(\\tau) = A(\\tau) + B(\\tau)$, where $A(\\tau)$ is a simple quadratic term and $B(\\tau)$ involves the expectation of a truncated random variable. The optimal $\\tau^*$ minimizes $\\phi(\\tau)$. The first-order condition is $\\phi'(\\tau^*) = 0$.\n\nFor $\\delta_{\\mathrm{grp}}$, let $\\phi_{\\mathrm{grp}}(\\tau) = s(g+\\tau^2) + (G-s) \\mathbb{E}[((\\chi_g - \\tau)_+)^2]$.\nThe derivative is $\\frac{d}{d\\tau} \\mathbb{E}[((\\chi_g - \\tau)_+)^2] = \\mathbb{E}[2((\\chi_g - \\tau)_+) \\cdot (-1) \\cdot I(\\chi_g > \\tau)] = -2\\mathbb{E}[(\\chi_g - \\tau)_+]$.\nThe FOC is $2s\\tau - 2(G-s)\\mathbb{E}[(\\chi_g - \\tau)_+] = 0$, which simplifies to $s\\tau = (G-s)\\mathbb{E}[(\\chi_g - \\tau)_+]$.\n\nFor $\\delta_1$, let $\\phi_1(\\tau) = sg(1+\\tau^2) + g(G-s) \\mathbb{E}[((|Z|-\\tau)_+)^2]$.\nThe FOC is $2sg\\tau - 2g(G-s)\\mathbb{E}[(|Z|-\\tau)_+] = 0$, which simplifies to $s\\tau = (G-s)\\mathbb{E}[(|Z|-\\tau)_+]$. This is the same FOC as for the group Lasso case, with $g=1$ (since $|Z| \\sim \\chi_1$).\n\nLet $F_g(\\tau) = \\mathbb{E}[(\\chi_g - \\tau)_+]$. The FOC is $\\frac{s}{G-s} = \\frac{F_g(\\tau)}{\\tau}$.\nFor large $\\tau$, the tail probability $P(\\chi_g > x)$ has the asymptotic behavior $P(\\chi_g > x) \\sim p_g(x)/x$, where $p_g(x)$ is the pdf of $\\chi_g$. Furthermore, $F_g(\\tau) = \\int_\\tau^\\infty P(x)dx \\approx p_g(\\tau)/\\tau^2$.\nSo, the FOC becomes $\\frac{s}{G-s} \\approx \\frac{p_g(\\tau)}{\\tau^3}$. Since $p_g(\\tau) \\propto \\tau^{g-1}e^{-\\tau^2/2}$, we have $\\frac{s}{G-s} \\propto \\tau^{g-4}e^{-\\tau^2/2}$.\nTaking logarithms, $\\ln(\\frac{s}{G-s}) \\approx (g-4)\\ln\\tau - \\frac{\\tau^2}{2}$. As $s/G \\to 0$, we have $\\tau \\to \\infty$. The dominant balance is $\\frac{\\tau^2}{2} \\approx \\ln(\\frac{G-s}{s})$, so $\\tau^2 \\sim 2\\ln(G/s)$.\nThis holds for both optimal $\\tau$ values, $\\tau_{\\mathrm{grp}}^*$ and $\\tau_1^*$:\n$$\n(\\tau_{\\mathrm{grp}}^*)^2 \\sim 2\\ln(G/s) \\quad \\text{and} \\quad (\\tau_1^*)^2 \\sim 2\\ln(G/s)\n$$\nThe leading-order terms are identical, though subleading terms differ based on $g$.\n\nNow we find the leading-order term for the statistical dimensions.\nFor the group Lasso case, the value is $\\delta_{\\mathrm{grp}}^* = s(g+(\\tau_{\\mathrm{grp}}^*)^2) + (G-s)\\mathbb{E}[((\\chi_g - \\tau_{\\mathrm{grp}}^*)_+)^2]$.\nThe expectation term can be analyzed asymptotically: $\\mathbb{E}[((\\chi_g - \\tau)_+)^2] \\sim 2 p_g(\\tau)/\\tau^3$.\nUsing the FOC, $(G-s) \\mathbb{E}[((\\chi_g - \\tau_{\\mathrm{grp}}^*)_+)^2] \\approx (G-s) \\frac{2}{\\tau_{\\mathrm{grp}}^*} \\mathbb{E}[(\\chi_g - \\tau_{\\mathrm{grp}}^*)_+] = \\frac{2}{\\tau_{\\mathrm{grp}}^*}(s\\tau_{\\mathrm{grp}}^*) = 2s$. This term is asymptotically smaller than $s(\\tau_{\\mathrm{grp}}^*)^2$.\nThe dominant term in $\\delta_{\\mathrm{grp}}^*$ is $s(\\tau_{\\mathrm{grp}}^*)^2$.\n$$\n\\delta(D(f_{\\mathrm{grp}}, x_0)) \\sim s (\\tau_{\\mathrm{grp}}^*)^2 \\sim 2s\\ln(G/s).\n$$\nFor the $\\ell_1$ case, the value is $\\delta_1^* = sg(1+(\\tau_1^*)^2) + g(G-s)\\mathbb{E}[((|Z|-\\tau_1^*)_+)^2]$.\nSimilarly, the second term is asymptotically subdominant to the first. The dominant term is $sg(\\tau_1^*)^2$.\n$$\n\\delta(D(f_1, x_0)) \\sim sg (\\tau_1^*)^2 \\sim 2sg\\ln(G/s).\n$$\nThe ratio of the leading-order asymptotic terms is:\n$$\n\\frac{\\delta(D(f_{\\mathrm{grp}}, x_0))}{\\delta(D(f_1, x_0))} \\sim \\frac{2s\\ln(G/s)}{2sg\\ln(G/s)} = \\frac{1}{g}\n$$\nThis ratio represents the gain in measurement efficiency from exploiting group structure.",
            "answer": "$$\\boxed{\\frac{1}{g}}$$"
        }
    ]
}