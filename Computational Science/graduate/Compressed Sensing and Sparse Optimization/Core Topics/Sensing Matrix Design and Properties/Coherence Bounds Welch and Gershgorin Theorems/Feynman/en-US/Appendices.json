{
    "hands_on_practices": [
        {
            "introduction": "This first practice establishes a baseline for analyzing sensing matrices by connecting theoretical bounds to concrete metrics. You will generate a random matrix, compute its mutual coherence ($\\mu$), and compare it to the fundamental limit set by the Welch bound. Building on this, you will then apply the Gershgorin circle theorem to derive a practical upper bound on the restricted isometry constant ($\\delta_s$) and see how these properties can be improved through a greedy column pruning algorithm.",
            "id": "3434906",
            "problem": "You are given the task of quantitatively assessing how removing a fixed number of columns from a near-Equiangular Tight Frame (near-ETF) type sensing matrix affects key coherence-based recovery guarantees in compressed sensing and sparse optimization. Work strictly in a purely mathematical setting with real matrices and no physical units. All angles are irrelevant to this problem.\n\nStart from the following fundamental base:\n- The definitions of mutual coherence, Gram matrix, restricted isometry constant, and support-restricted submatrices.\n- The Welch bound on coherence for unit-norm frames.\n- The Gershgorin circle theorem.\n- The classical mutual coherence-based sufficient condition for exact recovery by Orthogonal Matching Pursuit (OMP), a greedy algorithm for sparse recovery.\n\nYour program must implement the following procedure for each test case:\n1. Construct a real sensing matrix with dimensions $m \\times n$ as follows. Draw a matrix with independent standard normal entries, and normalize each column to unit $\\ell_2$-norm. This constructs a random near-ETF in the sense that its mutual coherence is close to minimal for the given $(m,n)$ but not necessarily equal to the Welch bound.\n2. Compute the mutual coherence $\\mu$ as the maximum absolute off-diagonal entry of the Gram matrix of the column-normalized sensing matrix.\n3. Using only fundamental results, derive and compute:\n   - A lower bound on $\\mu$ predicted by the Welch theorem for the given $(m,n)$.\n   - An upper bound on the restricted isometry constant $\\delta_s$ based on Gershgorin discs applied to $s \\times s$ principal submatrices of the Gram matrix.\n   - The largest sparsity level $s_{\\text{OMP}}$ for which a mutual coherence-based OMP success guarantee holds, stated purely in terms of $\\mu$.\n4. Perform column pruning by eliminating $k$ columns that are most deleterious to coherence (and thus worst for the Gershgorin-based bound) using the following greedy rule: at each pruning step, compute for each column $j$ the sum over $i \\neq j$ of the absolute inner products of column $j$ with all other columns, and remove the column with the largest such sum. After removing one column, recompute these sums on the reduced matrix and repeat until $k$ columns are removed. Denote by $n' = n - k$ the post-pruning number of columns.\n5. Recompute all quantities from step $2$ and step $3$ for the pruned matrix.\n6. For a prescribed fixed sparsity level $s$, decide whether the OMP success guarantee is met before and after pruning by comparing $s$ against the pre- and post-pruning $s_{\\text{OMP}}$.\n\nRestrictions and conventions:\n- All columns are normalized to have unit $\\ell_2$-norm.\n- The Gram matrix must be computed exactly from the constructed sensing matrix.\n- If $n \\le m$, the Welch lower bound for $\\mu$ should be taken to be $0$.\n- The sparsity level $s$ is guaranteed in all test cases to satisfy $s \\le n'$ after pruning.\n- All computations are deterministic given the seeded random generator for each test case.\n- For all floating-point outputs, round to exactly six digits after the decimal point using standard rounding.\n\nTest suite:\nFor each of the following parameter tuples $(m,n,k,s,\\text{seed})$, execute the above procedure:\n- Test case A: $(m,n,k,s,\\text{seed}) = (30,60,10,4,1)$.\n- Test case B: $(m,n,k,s,\\text{seed}) = (40,80,0,6,2)$.\n- Test case C: $(m,n,k,s,\\text{seed}) = (20,100,40,8,3)$.\n- Test case D: $(m,n,k,s,\\text{seed}) = (50,55,4,3,4)$.\n- Test case E: $(m,n,k,s,\\text{seed}) = (30,90,20,10,5)$.\n\nFor each test case, your program must output a list in the following order:\n- The pre-pruning mutual coherence $\\mu$ as a float rounded to six decimals.\n- The post-pruning mutual coherence $\\mu'$ as a float rounded to six decimals.\n- The pre-pruning Gershgorin-based upper bound on $\\delta_s$ as a float rounded to six decimals.\n- The post-pruning Gershgorin-based upper bound on $\\delta_s$ as a float rounded to six decimals.\n- The pre-pruning largest OMP-guaranteed sparsity $s_{\\text{OMP}}$ as an integer.\n- The post-pruning largest OMP-guaranteed sparsity $s_{\\text{OMP}}'$ as an integer.\n- The boolean indicating whether the OMP guarantee holds for the fixed $s$ before pruning.\n- The boolean indicating whether the OMP guarantee holds for the fixed $s$ after pruning.\n- The Welch lower bound for $\\mu$ before pruning as a float rounded to six decimals.\n- The Welch lower bound for $\\mu$ after pruning as a float rounded to six decimals.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of per-test-case lists, enclosed in square brackets. For example, a valid output has the form\n\"[[case1_values...],[case2_values...],...]\" with no spaces. Each inner list must follow exactly the order specified above. No other text must be printed.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of compressed sensing and sparse optimization, well-posed with a clear algorithmic procedure, and objective in its formulation. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities.\n\nThe task is to analyze the effects of a specific greedy column-pruning strategy on the coherence-based recovery guarantees of a sensing matrix. For each test case, specified by parameters $(m, n, k, s, \\text{seed})$, we will perform a sequence of calculations before and after pruning the matrix.\n\n**1. Sensing Matrix Construction**\n\nFirst, a sensing matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is constructed. Its entries are drawn independently from a standard normal distribution, $\\mathcal{N}(0,1)$. Subsequently, each of its $n$ columns, denoted $\\mathbf{a}_i$ for $i \\in \\{1, \\dots, n\\}$, is normalized to have a unit Euclidean norm ($\\ell_2$-norm):\n$$\n\\|\\mathbf{a}_i\\|_2 = 1 \\quad \\forall i \\in \\{1, \\dots, n\\}\n$$\nThis process yields a random matrix whose columns form a unit-norm frame. The Gram matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times n}$ is then computed as $\\mathbf{G} = \\mathbf{A}^T\\mathbf{A}$. The entries of the Gram matrix are the inner products of the columns of $\\mathbf{A}$: $G_{ij} = \\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle$. Due to column normalization, the diagonal entries of $\\mathbf{G}$ are all unity, $G_{ii} = \\|\\mathbf{a}_i\\|_2^2 = 1$. The off-diagonal entries quantify the correlation between different columns.\n\n**2. Pre-Pruning Analysis of Coherence-Based Guarantees**\n\nBefore any columns are removed, we compute several key metrics based on the initial matrix $\\mathbf{A}$.\n\n*   **Mutual Coherence ($\\mu$):** The mutual coherence of $\\mathbf{A}$ is the maximum absolute value of the off-diagonal entries of its Gram matrix. It measures the worst-case correlation between any two distinct columns.\n    $$\n    \\mu = \\mu(\\mathbf{A}) = \\max_{i \\neq j} |\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle| = \\max_{i \\neq j} |G_{ij}|\n    $$\n*   **Welch Bound ($\\mu_W$):** This theorem provides a fundamental lower bound on the mutual coherence for any frame with $n$ unit-norm vectors in $\\mathbb{R}^m$. For $n  m$, the bound is given by:\n    $$\n    \\mu_W = \\sqrt{\\frac{n-m}{m(n-1)}}\n    $$\n    For cases where $n \\le m$, the problem specifies using $\\mu_W = 0$. Matrices that achieve this bound are called Equiangular Tight Frames (ETFs). The constructed random matrix is expected to be a near-ETF, meaning its coherence $\\mu$ will be close to $\\mu_W$.\n*   **Gershgorin-Based Bound on the Restricted Isometry Constant ($\\delta_s$):** The Restricted Isometry Property (RIP) is a central concept for recovery guarantees. A matrix $\\mathbf{A}$ has the RIP of order $s$ with constant $\\delta_s$ if for any $s$-sparse vector $\\mathbf{x}$, $(1-\\delta_s)\\|\\mathbf{x}\\|_2^2 \\le \\|\\mathbf{A}\\mathbf{x}\\|_2^2 \\le (1+\\delta_s)\\|\\mathbf{x}\\|_2^2$. This is equivalent to requiring that all eigenvalues of any $s \\times s$ principal submatrix $\\mathbf{G}_S$ of the Gram matrix $\\mathbf{G}$ lie in the interval $[1-\\delta_s, 1+\\delta_s]$. The Gershgorin circle theorem states that every eigenvalue of $\\mathbf{G}_S$ lies within the union of discs centered at its diagonal entries. Since $(\\mathbf{G}_S)_{ii}=1$, the eigenvalues $\\lambda$ must satisfy $|\\lambda - 1| \\le \\max_{i \\in S} \\sum_{j \\in S, j \\neq i} |G_{ij}|$. The sum is bounded by $(s-1)\\mu$. This gives a simple upper bound on $\\delta_s$:\n    $$\n    \\delta_s \\le (s-1)\\mu\n    $$\n    We compute this value, $\\delta_{s, \\text{Gersh}} = (s-1)\\mu$, for the prescribed sparsity level $s$.\n*   **OMP Recovery Guarantee ($s_{\\text{OMP}}$):** Orthogonal Matching Pursuit (OMP) is a greedy algorithm guaranteed to perfectly recover any $s$-sparse signal $\\mathbf{x}$ from measurements $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$, provided the sparsity $s$ satisfies the condition:\n    $$\n    s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)\n    $$\n    We calculate the largest integer sparsity level, $s_{\\text{OMP}}$, for which this guarantee holds. This is the largest integer strictly smaller than the bound:\n    $$\n    s_{\\text{OMP}} = \\left\\lceil \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right) \\right\\rceil - 1\n    $$\n\n**3. Greedy Column Pruning**\n\nThe procedure involves removing $k$ columns from $\\mathbf{A}$ using a greedy strategy. The goal is to remove columns that are \"most deleterious to coherence\". This is interpreted as removing the column that has the highest total absolute correlation with all other columns. At each of the $k$ pruning steps, we perform the following:\n1.  For the current matrix with columns $\\{\\mathbf{a}_j\\}$, compute the \"coherence contribution\" for each column $j$: $c_j = \\sum_{i \\neq j} |\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle|$.\n2.  Identify the column index $j^*$ that maximizes this sum: $j^* = \\arg\\max_j c_j$.\n3.  Remove column $\\mathbf{a}_{j^*}$ from the matrix.\n\nThis process is repeated $k$ times, reducing the number of columns from $n$ to $n' = n-k$. Let the resulting pruned matrix be denoted $\\mathbf{A}'$.\n\n**4. Post-Pruning Analysis**\n\nAfter pruning, we obtain the matrix $\\mathbf{A}' \\in \\mathbb{R}^{m \\times n'}$. All the quantities computed in Step 2 are re-evaluated for this new matrix:\n*   Post-pruning mutual coherence, $\\mu' = \\mu(\\mathbf{A}')$.\n*   Post-pruning Welch bound, $\\mu'_W$, calculated with dimension $n'$.\n*   Post-pruning Gershgorin-based bound on $\\delta_s$, which is $\\delta'_{s, \\text{Gersh}} = (s-1)\\mu'$.\n*   Post-pruning largest OMP-guaranteed sparsity, $s'_{\\text{OMP}}$, calculated using $\\mu'$.\n\n**5. OMP Guarantee Decision**\n\nFinally, for the prescribed fixed sparsity level $s$, we determine whether the OMP success guarantee is met both before and after pruning. This involves two boolean checks:\n1.  **Before pruning:** Is $s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$?\n2.  **After pruning:** Is $s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu'}\\right)$?\n\nFor each test case, these steps generate the required ten output values: $\\mu$, $\\mu'$, $\\delta_{s, \\text{Gersh}}$, $\\delta'_{s, \\text{Gersh}}$, $s_{\\text{OMP}}$, $s'_{\\text{OMP}}$, the two booleans, $\\mu_W$, and $\\mu'_W$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (m, n, k, s, seed)\n        (30, 60, 10, 4, 1),\n        (40, 80, 0, 6, 2),\n        (20, 100, 40, 8, 3),\n        (50, 55, 4, 3, 4),\n        (30, 90, 20, 10, 5),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        m, n, k, s, seed = case\n        result = process_case(m, n, k, s, seed)\n        all_results.append(result)\n\n    # Format the final output string as a list of lists, with no spaces.\n    results_as_strings = [str(r).replace(\" \", \"\") for r in all_results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\ndef process_case(m, n, k, s, seed):\n    \"\"\"\n    Processes a single test case according to the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct the sensing matrix\n    A = rng.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n\n    # 2. Pre-pruning analysis\n    mu_pre, delta_s_bound_pre, s_omp_pre, omp_guarantee_pre, welch_pre = analyze_matrix(A, s)\n\n    # 4. Perform column pruning\n    A_pruned = A.copy()\n    for _ in range(k):\n        # Ensure there are columns to prune\n        if A_pruned.shape[1] = 1:\n            break\n        \n        # Compute Gram matrix for the current matrix\n        G_prune = A_pruned.T @ A_pruned\n        np.fill_diagonal(G_prune, 0)\n        \n        # Find the column that maximizes the sum of absolute inner products\n        # with other columns.\n        coherence_sums = np.sum(np.abs(G_prune), axis=1)\n        idx_to_remove = np.argmax(coherence_sums)\n        \n        # Remove the identified column\n        A_pruned = np.delete(A_pruned, idx_to_remove, axis=1)\n    \n    # 5. Post-pruning analysis\n    if k == 0:\n        # If no pruning was done, post-pruning metrics are identical to pre-pruning.\n        mu_post = mu_pre\n        delta_s_bound_post = delta_s_bound_pre\n        s_omp_post = s_omp_pre\n        omp_guarantee_post = omp_guarantee_pre\n        welch_post = welch_pre\n    else:\n        mu_post, delta_s_bound_post, s_omp_post, omp_guarantee_post, welch_post = analyze_matrix(A_pruned, s)\n\n    # 6. Format and return results\n    return [\n        round(mu_pre, 6),\n        round(mu_post, 6),\n        round(delta_s_bound_pre, 6),\n        round(delta_s_bound_post, 6),\n        int(s_omp_pre),\n        int(s_omp_post),\n        omp_guarantee_pre,\n        omp_guarantee_post,\n        round(welch_pre, 6),\n        round(welch_post, 6)\n    ]\n\ndef analyze_matrix(A, s):\n    \"\"\"\n    Computes all required metrics for a given sensing matrix A and sparsity s.\n    \"\"\"\n    m, n = A.shape\n\n    if n = 1:\n        # Trivial case with 0 or 1 columns: coherence is 0.\n        mu = 0.0\n        delta_s_bound = 0.0\n        # OMP bound is infinite, so s_omp is effectively infinite. Use a large integer.\n        s_omp = 2**31 - 1\n        omp_guarantee = True\n        welch = 0.0\n        return mu, delta_s_bound, s_omp, omp_guarantee, welch\n\n    # Compute Gram matrix and mutual coherence\n    G = A.T @ A\n    np.fill_diagonal(G, 0)\n    mu = np.max(np.abs(G))\n\n    # Welch bound\n    if n  m:\n        welch = np.sqrt((n - m) / (m * (n - 1)))\n    else:\n        welch = 0.0\n\n    # Gershgorin-based upper bound on RIC\n    delta_s_bound = (s - 1) * mu\n\n    # OMP success guarantee\n    if mu == 0:\n        s_omp = 2**31 - 1\n        omp_guarantee = True\n    else:\n        omp_bound = 0.5 * (1 + 1 / mu)\n        # s_OMP is the largest integer s' such that s'  omp_bound.\n        s_omp = np.ceil(omp_bound) - 1\n        # Check if the specific sparsity s meets the guarantee.\n        omp_guarantee = (s  omp_bound)\n    \n    return mu, delta_s_bound, s_omp, omp_guarantee, welch\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Theoretical bounds are invaluable, but understanding their relationship to exact quantities is key to appreciating their utility and limitations. This exercise challenges you to compute the exact restricted isometry constant ($\\delta_s$) for small matrices, providing a rare opportunity to directly compare it with the more tractable Gershgorin-based estimate. Furthermore, you will explore how practical imperfections, modeled as column normalization errors, influence both the coherence and the accuracy of these RIP bounds .",
            "id": "3434936",
            "problem": "You are given the task of programmatically studying the interplay between the mutual coherence, Welch’s lower bound, and Gershgorin-based bounds on restricted isometry constants for random partial orthonormal matrices whose columns are perturbed by multiplicative normalization errors.\n\nStarting from the following base concepts only:\n- Definition of mutual coherence: for a matrix with columns, mutual coherence is the maximum absolute normalized inner product among pairs of distinct columns.\n- The Welch bound: for any set of a fixed number of unit vectors in a fixed dimension, the mutual coherence obeys a universal lower bound determined solely by the dimension and the number of vectors.\n- The Gershgorin circle theorem: every eigenvalue of a real symmetric matrix lies within at least one Gershgorin disc determined by the diagonal entry and the sum of absolute values of off-diagonal entries in the corresponding row.\n- The restricted isometry constant of order $s$ for a matrix: the smallest nonnegative number such that the squared norm of the product with any $s$-sparse vector is sandwiched between linear perturbations of the vector’s squared norm.\n\nYour program must implement the following pipeline for each test case:\n1. Construct a random orthonormal basis in $\\mathbb{R}^{n \\times n}$, select $m$ distinct rows to form a random $m \\times n$ partial orthonormal matrix, and then apply independent multiplicative column scaling of the form $1 + \\varepsilon \\cdot \\theta_j$ where each $\\theta_j$ is in $\\left[-1,1\\right]$, to introduce column normalization errors. All randomness must be controlled by a provided integer seed for reproducibility.\n2. Compute the empirical mutual coherence $\\mu$ by normalizing each pairwise inner product by the product of the corresponding column norms and taking the maximum absolute value over all distinct column pairs.\n3. Compute the Welch lower bound as a function of $m$ and $n$ and report it alongside $\\mu$. Also report whether $\\mu$ is greater than or equal to the Welch bound, up to standard numerical tolerance.\n4. Compute the exact restricted isometry constant $\\delta_s$ by exhaustively enumerating all column subsets of size $s$, forming their Gram matrices, and taking the maximum absolute deviation of eigenvalues from $1$ across all subsets. This is equivalent to optimizing the quadratic form over all unit vectors supported on a subset of size $s$ but must be carried out via subset enumeration and eigenvalue computation to ensure exactness.\n5. Compute two Gershgorin-based upper bounds for $\\delta_s$ using only directly computable matrix quantities:\n   - A coherence-based bound derived from Gershgorin applied to the Gram matrix of unit-normalized columns, which yields an $s$-dependent bound in terms of the mutual coherence.\n   - A bound derived from Gershgorin applied to the unnormalized Gram matrix, which incorporates both diagonal deviations from unity induced by column normalization errors and a uniform bound on off-diagonal magnitudes.\n6. Compare the exact $\\delta_s$ to each bound and report whether each bound upper-bounds the exact value within numerical tolerance.\n\nYour program must use the following test suite. For each tuple, the entries are $(\\text{seed}, n, m, s, \\varepsilon)$:\n- $(123, 16, 8, 3, 0.05)$: a balanced moderate-size case to test typical behavior.\n- $(456, 18, 15, 4, 0.10)$: a case with $m$ close to $n$ to test a small Welch bound and the effect of modest normalization errors.\n- $(789, 14, 7, 1, 0.20)$: a boundary case with $s=1$ to isolate diagonal deviations from unity.\n\nFinal output requirements:\n- For each test case, output a list with eight entries in the following order:\n  1. The empirical mutual coherence $\\mu$ as a float.\n  2. The Welch lower bound as a float.\n  3. A boolean indicating whether $\\mu$ is greater than or equal to the Welch bound within a small tolerance.\n  4. The exact $\\delta_s$ as a float.\n  5. The coherence-based Gershgorin upper bound for $\\delta_s$ as a float.\n  6. The unnormalized Gershgorin upper bound for $\\delta_s$ as a float.\n  7. A boolean indicating whether the exact $\\delta_s$ is less than or equal to the coherence-based bound within a small tolerance.\n  8. A boolean indicating whether the exact $\\delta_s$ is less than or equal to the unnormalized Gershgorin bound within a small tolerance.\n- Your program should produce a single line of output containing the results for all test cases as a list of lists, in the exact format:\n  [[case1_entry1,case1_entry2,...,case1_entry8],[case2_entry1,...,case2_entry8],[case3_entry1,...,case3_entry8]]\n\nNo physical units or angle units are involved. All computations must be real-valued. Ensure scientific realism by using exact subset enumeration when computing the exact restricted isometry constant and by deriving all reported bounds from the base concepts listed above without shortcuts or black-box formulas.",
            "solution": "We begin from foundational definitions and classical theorems and derive the quantities to be computed as well as the algorithmic steps.\n\nLet $A \\in \\mathbb{R}^{m \\times n}$ denote the sensing matrix. Let $a_j \\in \\mathbb{R}^m$ denote the $j$-th column of $A$. The mutual coherence is defined as\n$$\n\\mu \\triangleq \\max_{i \\neq j} \\frac{\\left| \\langle a_i, a_j \\rangle \\right|}{\\|a_i\\|_2 \\, \\|a_j\\|_2},\n$$\nthat is, the maximum absolute normalized inner product among distinct columns. This quantity is invariant to positive rescaling of individual columns.\n\nThe Welch bound arises by considering $n$ unit vectors in $\\mathbb{R}^m$, their Gram matrix $G \\in \\mathbb{R}^{n \\times n}$ with ones on the diagonal, and the Frobenius norm constraint tied to the dimension $m$. Specifically, let $G = X^\\top X$ for $X \\in \\mathbb{R}^{m \\times n}$ with columns $x_j$ of unit norm. The eigenvalues of $G$ are nonnegative and sum to $n$, with at most $m$ nonzero eigenvalues because $\\operatorname{rank}(G) \\le m$. Among all such $G$, the sum of squares of off-diagonal entries,\n$$\n\\sum_{i \\ne j} G_{ij}^2 = \\|G\\|_F^2 - \\operatorname{trace}(G^2) \\quad \\text{(with diagonal subtracted)},\n$$\nis minimized when the nonzero eigenvalues are equal, by convexity and symmetry. This yields the classical Welch lower bound on the maximum off-diagonal magnitude. Specializing to the setting at hand, for $n$ unit vectors in $\\mathbb{R}^m$, the mutual coherence obeys\n$$\n\\mu \\ge \\sqrt{\\frac{n - m}{m (n - 1)}}.\n$$\nIn our computations the columns of $A$ are generally not unit norm after row selection and scaling, but the mutual coherence as defined above properly normalizes the inner products, so we compare the empirical $\\mu$ to this universal lower bound.\n\nThe restricted isometry constant (RIC) of order $s$, denoted $\\delta_s$, is defined via the inequality\n$$\n(1 - \\delta_s) \\, \\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_s) \\, \\|x\\|_2^2\n\\quad \\text{for all $s$-sparse vectors $x \\in \\mathbb{R}^n$}.\n$$\nEquivalently, for every subset $S \\subset \\{1,\\dots,n\\}$ with $|S| = s$, letting $A_S$ denote the $m \\times s$ submatrix with columns indexed by $S$ and $G_S = A_S^\\top A_S$ its Gram matrix, we have\n$$\n\\lambda_{\\min}(G_S) \\ge 1 - \\delta_s, \\quad \\lambda_{\\max}(G_S) \\le 1 + \\delta_s,\n$$\nand hence\n$$\n\\delta_s = \\max_{|S|=s} \\max \\left\\{ 1 - \\lambda_{\\min}(G_S), \\, \\lambda_{\\max}(G_S) - 1 \\right\\}.\n$$\nTherefore, the exact $\\delta_s$ can be computed by enumerating all subsets $S$ of size $s$, computing the extreme eigenvalues of $G_S$, and aggregating their worst-case deviations from $1$.\n\nTo obtain tractable upper bounds for $\\delta_s$ from first principles, we use the Gershgorin circle theorem. For any real symmetric matrix $M$, every eigenvalue lies within at least one interval of the form $[M_{ii} - R_i, M_{ii} + R_i]$ where $R_i = \\sum_{j \\ne i} |M_{ij}|$ is the absolute row sum radius. Applying this to $G_S$ yields, for each $i \\in S$,\n$$\n\\lambda \\in \\left[ (G_S)_{ii} - \\sum_{j \\in S, j \\ne i} |(G_S)_{ij}|, \\, (G_S)_{ii} + \\sum_{j \\in S, j \\ne i} |(G_S)_{ij}| \\right].\n$$\nLet $d_j = \\|a_j\\|_2^2 = G_{jj}$ and let $\\nu = \\max_{i \\ne j} |G_{ij}|$. Then, uniformly over all $S$ with $|S| = s$ and all $i \\in S$, we get\n$$\n\\lambda \\in \\left[ d_i - (s - 1) \\nu, \\, d_i + (s - 1) \\nu \\right].\n$$\nIt follows that the deviation of any such eigenvalue from $1$ is bounded by\n$$\n| \\lambda - 1 | \\le | d_i - 1 | + (s - 1) \\nu.\n$$\nMaximizing over $i$ and $S$ yields the unnormalized Gershgorin bound\n$$\n\\delta_s \\le \\alpha + (s - 1) \\nu, \\quad \\text{where } \\alpha \\triangleq \\max_j | d_j - 1 |, \\;\\; \\nu \\triangleq \\max_{i \\ne j} | G_{ij} |.\n$$\n\nIf, in addition, all columns are normalized to unit norm so that $d_j = 1$ for all $j$, then $G_{ij}$ equals the normalized inner product. In that case, by bounding row sums using the mutual coherence,\n$$\n\\sum_{j \\in S, j \\ne i} | G_{ij} | \\le (s - 1) \\mu,\n$$\nand therefore the eigenvalues of $G_S$ lie in $[1 - (s - 1) \\mu, \\, 1 + (s - 1) \\mu]$, which implies the coherence-based Gershgorin bound\n$$\n\\delta_s \\le (s - 1) \\mu.\n$$\nIn our setting, the actual matrix $A$ has column normalization errors; thus the unnormalized Gershgorin bound applies directly to $A$, while the coherence-based bound corresponds to the matrix whose columns are individually normalized to unit norm. Consequently, the coherence-based bound may under-estimate the true RIC of the unnormalized $A$, whereas the unnormalized Gershgorin bound is guaranteed to upper-bound the true RIC.\n\nAlgorithmic design:\n1. Random partial orthonormal construction: generate a Haar-distributed orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ by performing a QR decomposition of a Gaussian random matrix and correcting column signs to make the distribution uniform. Select $m$ distinct rows to form $A_0 \\in \\mathbb{R}^{m \\times n}$.\n2. Column normalization errors: draw independent scales $s_j \\in [1 - \\varepsilon, 1 + \\varepsilon]$ and set $A = A_0 \\, \\mathrm{diag}(s)$.\n3. Mutual coherence: compute the Gram matrix $G = A^\\top A$ and column norms via $d_j = G_{jj}$; then compute normalized pairwise correlations $G_{ij}/\\sqrt{d_i d_j}$ and take the maximum absolute off-diagonal value.\n4. Welch bound: compute the lower bound in terms of $m$ and $n$.\n5. Exact RIC: enumerate all $\\binom{n}{s}$ supports $S$ of size $s$, compute $G_S$ and its extreme eigenvalues, and accumulate the maximum deviation from $1$.\n6. Gershgorin bounds: compute $\\alpha = \\max_j |d_j - 1|$, $\\nu = \\max_{i \\ne j} |G_{ij}|$, and form $\\alpha + (s - 1)\\nu$; also compute $(s - 1)\\mu$.\n7. Tolerance checks: test $\\mu$ against the Welch bound and $\\delta_s$ against each Gershgorin bound with a small nonnegative tolerance to account for floating-point arithmetic.\n\nThe test suite covers:\n- A typical regime with moderate dimensions and small scaling errors.\n- A case with $m$ close to $n$ where the Welch lower bound is small, and scaling errors are modest.\n- The boundary case $s = 1$, where the RIC reduces to maximum diagonal deviation and hence isolates the effect of column normalization errors.\n\nThe program prints a single line: a list containing, for each test case, a list with eight entries in the specified order. All reported quantities are computed directly from the definitions and the Gershgorin theorem as derived above, and the exact RIC is obtained by exhaustive subset enumeration with eigenvalue calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef haar_orthogonal(n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generate a Haar-distributed orthogonal matrix Q in R^{n x n}\n    using QR decomposition of a Gaussian random matrix with sign correction.\n    \"\"\"\n    X = rng.standard_normal((n, n))\n    Q, R = np.linalg.qr(X)\n    # Make Q Haar by ensuring R has positive diagonal\n    d = np.sign(np.diag(R))\n    d[d == 0] = 1.0\n    Q = Q @ np.diag(d)\n    return Q\n\ndef random_partial_with_scaling(seed: int, n: int, m: int, eps: float) - np.ndarray:\n    \"\"\"\n    Construct an m x n random partial orthonormal matrix with column scaling.\n    - Generate Haar orthogonal Q (n x n)\n    - Choose m random rows\n    - Scale columns by factors in [1 - eps, 1 + eps]\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    Q = haar_orthogonal(n, rng)\n    rows = rng.choice(n, size=m, replace=False)\n    A0 = Q[rows, :]  # m x n\n    scales = 1.0 + eps * rng.uniform(-1.0, 1.0, size=n)\n    A = A0 * scales  # broadcast column-wise scaling\n    return A\n\ndef mutual_coherence(A: np.ndarray) - float:\n    \"\"\"\n    Compute empirical mutual coherence:\n    max_{i != j} |a_i, a_j| / (||a_i|| ||a_j||)\n    \"\"\"\n    G = A.T @ A\n    norms = np.sqrt(np.clip(np.diag(G), 1e-18, None))\n    denom = norms[:, None] * norms[None, :]\n    C = G / denom\n    C = np.abs(C)\n    np.fill_diagonal(C, 0.0)\n    return float(np.max(C))\n\ndef welch_bound(m: int, n: int) - float:\n    \"\"\"\n    Welch lower bound for mutual coherence of n unit vectors in R^m.\n    \"\"\"\n    if n = 1 or m = 0:\n        return 0.0\n    # Guard against numerical issues when n == m\n    num = max(n - m, 0.0)\n    den = m * (n - 1.0)\n    if den = 0:\n        return 0.0\n    return float(np.sqrt(num / den))\n\ndef exact_ric_via_subsets(A: np.ndarray, s: int) - float:\n    \"\"\"\n    Compute the exact restricted isometry constant delta_s by exhaustive enumeration:\n    delta_s = max_{|S|=s} max(1 - lambda_min(G_S), lambda_max(G_S) - 1),\n    where G_S = A_S^T A_S.\n    \"\"\"\n    n = A.shape[1]\n    if s = 0 or s  n:\n        return 0.0\n    G = A.T @ A\n    worst = 0.0\n    for S in combinations(range(n), s):\n        idx = np.array(S, dtype=int)\n        Gs = G[np.ix_(idx, idx)]\n        # Eigenvalues of symmetric matrix\n        vals = np.linalg.eigvalsh(Gs)\n        dev = max(abs(float(vals[0]) - 1.0), abs(float(vals[-1]) - 1.0))\n        if dev  worst:\n            worst = dev\n    return worst\n\ndef gersh_bounds(A: np.ndarray, s: int) - tuple[float, float]:\n    \"\"\"\n    Compute two Gershgorin-based upper bounds for delta_s:\n    - coherence-based: (s - 1) * mu, where mu uses normalized columns\n    - unnormalized: alpha + (s - 1) * nu, where alpha = max |d_j - 1| (diagonal deviation),\n      nu = max_{i != j} |G_ij| for the unnormalized Gram matrix G = A^T A\n    \"\"\"\n    G = A.T @ A\n    # Unnormalized quantities\n    d = np.diag(G)\n    alpha = float(np.max(np.abs(d - 1.0))) if d.size  0 else 0.0\n    G_abs = np.abs(G.copy())\n    np.fill_diagonal(G_abs, 0.0)\n    nu = float(np.max(G_abs)) if G_abs.size  0 else 0.0\n    delta_gersh_unnorm = alpha + max(s - 1, 0) * nu\n    # Coherence-based\n    mu = mutual_coherence(A)\n    delta_gersh_coh = max(s - 1, 0) * mu\n    return delta_gersh_coh, delta_gersh_unnorm\n\ndef run_case(seed: int, n: int, m: int, s: int, eps: float) - list:\n    \"\"\"\n    Execute one test case and return the required list of results:\n    [mu, welch, mu_ge_welch, delta_true, delta_coh, delta_gersh, true_le_coh, true_le_gersh]\n    \"\"\"\n    A = random_partial_with_scaling(seed, n, m, eps)\n    mu = mutual_coherence(A)\n    wb = welch_bound(m, n)\n    # Exact RIC\n    delta_true = exact_ric_via_subsets(A, s)\n    # Gershgorin bounds\n    delta_coh, delta_gersh = gersh_bounds(A, s)\n    # Tolerance for comparisons\n    tol = 1e-10\n    mu_ge_wb = (mu + tol) = wb\n    ric_le_coh = (delta_true = delta_coh + tol)\n    ric_le_gersh = (delta_true = delta_gersh + tol)\n    # Return values; floats as native Python floats, booleans as bools\n    return [\n        float(mu),\n        float(wb),\n        bool(mu_ge_wb),\n        float(delta_true),\n        float(delta_coh),\n        float(delta_gersh),\n        bool(ric_le_coh),\n        bool(ric_le_gersh),\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (seed, n, m, s, eps)\n    test_cases = [\n        (123, 16, 8, 3, 0.05),\n        (456, 18, 15, 4, 0.10),\n        (789, 14, 7, 1, 0.20),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, m, s, eps = case\n        result = run_case(seed, n, m, s, eps)\n        results.append(result)\n\n    # Final print statement in the exact required format: single line list of lists\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "This final practice moves from analysis to proactive design, showcasing how theoretical principles can guide the creation of better sensing matrices. You will formulate and solve a convex optimization problem to pre-scale the columns of a dictionary, with the specific goal of minimizing its maximum Gershgorin radius. This exercise provides hands-on experience with an advanced technique that directly leverages Gershgorin's theorem to improve a matrix's RIP guarantees .",
            "id": "3434940",
            "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ representing a sensing dictionary in compressed sensing. Let $A = [a_1,\\dots,a_n]$ denote its columns, and define the Gram matrix $G = A^\\top A \\in \\mathbb{R}^{n \\times n}$. For a diagonal scaling $D = \\mathrm{diag}(d_1,\\dots,d_n)$ with strictly positive entries, the rescaled matrix is $A D$ and its Gram matrix is $G(D) = (A D)^\\top (A D) = D A^\\top A D = D G D$, whose entries are $g_{ij}(D) = d_i d_j \\langle a_i, a_j \\rangle$.\n\nThe Gershgorin Circle Theorem states that every eigenvalue of a real symmetric matrix lies within at least one disc centered at a diagonal entry $g_{ii}(D)$ with radius $r_i(D) = \\sum_{j \\neq i} |g_{ij}(D)|$. In the context of the Restricted Isometry Property (RIP), the goal is to keep eigenvalues of principal submatrices of $G(D)$ close to $1$, which can be conservatively promoted by reducing the maximum Gershgorin radius and keeping the diagonal entries near $1$. The mutual coherence is defined for unit-norm columns as $\\mu = \\max_{i \\neq j} \\left|\\frac{\\langle a_i, a_j \\rangle}{\\|a_i\\|_2 \\|a_j\\|_2}\\right|$, and the Welch bound gives a lower bound on $\\mu$ as $\\mu \\ge \\sqrt{\\frac{n - m}{m (n - 1)}}$ for unit-norm columns when $n  m$.\n\nYour tasks are:\n- Formulate and implement an optimization that chooses positive scaling factors $d_i$ to reduce the maximum Gershgorin radius $\\max_i r_i(D)$ of $G(D)$, while enforcing stability of the rescaled column norms $\\|d_i a_i\\|_2$ through box constraints $\\alpha_{\\min} \\le \\|d_i a_i\\|_2 \\le \\alpha_{\\max}$ for fixed scalars $\\alpha_{\\min}, \\alpha_{\\max}$. The optimization must be grounded in a fundamental base: use the Gershgorin Circle Theorem and convex analysis. You must work in variables $x_i = \\log d_i$ and design a convex objective based on a smooth approximation of $\\max_i r_i(D)$ using a log-sum-exp construction. Do not assume or require unit-norm columns in $A$.\n- Quantify the trade-off between improved RIP bounds and stability of column norms under noise in $A$. Use the Gershgorin-based upper bound on the $k$-restricted isometry constant $\\delta_k$ defined by\n$$\n\\delta_k \\le \\max_{i \\in \\{1,\\dots,n\\}} \\left( \\left| g_{ii}(D) - 1 \\right| + \\sum_{\\text{top }(k-1)\\text{ off-diagonals in row } i} \\left| g_{ij}(D) \\right| \\right),\n$$\nwhich is obtained by applying Gershgorin to all $k \\times k$ principal submatrices and upper bounding each disc radius by the sum of the $(k-1)$ largest off-diagonal magnitudes in the corresponding row. Measure stability by the maximum deviation of rescaled column norms from $1$, that is, $\\max_i \\left| \\|d_i a_i\\|_2 - 1 \\right|$.\n- Compute and report the mutual coherence of the normalized columns $\\tilde{a}_i = a_i / \\|a_i\\|_2$, namely $\\mu = \\max_{i \\neq j} |\\langle \\tilde{a}_i, \\tilde{a}_j \\rangle|$, and the Welch bound $\\sqrt{\\max\\left\\{0, \\frac{n - m}{m (n - 1)}\\right\\}}$.\n\nImplementation details:\n- Construct the convex objective by defining $x_i = \\log d_i$, use the weights $w_{ij} = |\\langle a_i, a_j \\rangle|$ for $i \\neq j$ and $w_{ii} = 0$, and minimize a smooth approximation of $\\max_i \\sum_{j \\neq i} w_{ij} \\exp(x_i + x_j)$ using a log-sum-exp surrogate. Enforce the constraints $\\log(\\alpha_{\\min}/\\|a_i\\|_2) \\le x_i \\le \\log(\\alpha_{\\max}/\\|a_i\\|_2)$.\n- Solve the optimization with a principled projected gradient method and backtracking line search on the smooth convex objective. Derive and implement the gradient analytically and project onto the box constraints.\n- After optimization, form $D = \\mathrm{diag}(e^{x_1},\\dots,e^{x_n})$, compute $G(D)$, the maximum Gershgorin radius $\\max_i r_i(D)$, and the bound on $\\delta_k$ given above using the sum of the $(k-1)$ largest off-diagonals per row, as well as the maximum norm deviation.\n\nTest suite:\n- Use four test cases with parameters $(m,n,k,\\text{seed},\\text{noise},\\alpha_{\\min},\\alpha_{\\max})$:\n    1. $(20,30,4,1,0.05,0.90,1.10)$\n    2. $(20,30,4,2,0.05,0.99,1.01)$\n    3. $(40,60,6,3,0.15,0.70,1.30)$\n    4. $(8,12,3,4,0.00,0.85,1.15)$\n- For each test case, generate $A$ with independent and identically distributed Gaussian entries of zero mean and unit variance using the given seed, and add a noise matrix scaled so that its Frobenius norm equals the specified fraction of the Frobenius norm of the base $A$. Explicitly, if $Z$ has independent and identically distributed $\\mathcal{N}(0,1)$ entries, set the noise as $\\eta \\, Z$ where $\\eta = \\text{noise} \\cdot \\|A\\|_F / \\|Z\\|_F$ and use $A \\leftarrow A + \\eta Z$.\n\nYour program must, for each test case, compute the following seven quantities:\n- The maximum Gershgorin radius of $G$ before scaling, $\\max_i \\sum_{j \\neq i} |g_{ij}|$.\n- The maximum Gershgorin radius of $G(D)$ after optimization, $\\max_i \\sum_{j \\neq i} |g_{ij}(D)|$.\n- The Gershgorin-based upper bound on $\\delta_k$ before scaling.\n- The Gershgorin-based upper bound on $\\delta_k$ after scaling.\n- The maximum deviation of rescaled column norms from $1$, $\\max_i \\left| \\|d_i a_i\\|_2 - 1 \\right|$.\n- The mutual coherence $\\mu$ of the normalized columns of $A$.\n- The Welch lower bound $\\sqrt{\\max\\left\\{0, \\frac{n - m}{m (n - 1)}\\right\\}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s metrics are expressed as a list of floats rounded to six decimal places. For example, the output should look like:\n$[ [x_{1,1}, x_{1,2}, \\dots, x_{1,7}], [x_{2,1}, \\dots, x_{2,7}], [x_{3,1}, \\dots, x_{3,7}], [x_{4,1}, \\dots, x_{4,7}] ]$.",
            "solution": "The user has provided a problem in the domain of compressed sensing and sparse optimization concerning the pre-scaling of a sensing matrix $A$ to improve its properties, specifically those related to the Restricted Isometry Property (RIP). The problem is to be approached using the Gershgorin Circle Theorem and convex optimization.\n\n### Step 1: Problem Validation\n\n**1.1. Givens Extraction**\n\n-   **Sensing Dictionary**: A real matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns $a_1, \\dots, a_n$.\n-   **Gram Matrix**: $G = A^\\top A \\in \\mathbb{R}^{n \\times n}$.\n-   **Scaling**: A diagonal matrix $D = \\mathrm{diag}(d_1, \\dots, d_n)$ with $d_i  0$.\n-   **Scaled Gram Matrix**: $G(D) = (AD)^\\top(AD) = D G D$, with entries $g_{ij}(D) = d_i d_j \\langle a_i, a_j \\rangle$.\n-   **Gershgorin Radii**: For $G(D)$, the radius for row $i$ is $r_i(D) = \\sum_{j \\neq i} |g_{ij}(D)|$.\n-   **Optimization Goal**: Minimize $\\max_i r_i(D)$.\n-   **Constraints**: Box constraints on scaled column norms: $\\alpha_{\\min} \\le \\|d_i a_i\\|_2 \\le \\alpha_{\\max}$.\n-   **Optimization Variables**: $x_i = \\log d_i$.\n-   **Objective Function**: A convex objective based on a log-sum-exp smooth approximation of $\\max_i r_i(D)$. The definition is specified as minimizing a surrogate for $\\max_i \\sum_{j \\neq i} w_{ij} \\exp(x_i + x_j)$, where $w_{ij} = |\\langle a_i, a_j \\rangle|$.\n-   **Optimization Algorithm**: Projected gradient method with backtracking line search.\n-   **RIP Bound ($\\delta_k$)**: The Gershgorin-based upper bound is given by $\\delta_k \\le \\max_{i} \\left( \\left| g_{ii}(D) - 1 \\right| + \\sum_{\\text{top }(k-1)} \\left| g_{ij}(D) \\right| \\right)$.\n-   **Stability Metric**: Maximum deviation of rescaled column norms from 1: $\\max_i | \\|d_i a_i\\|_2 - 1 |$.\n-   **Mutual Coherence**: $\\mu = \\max_{i \\neq j} |\\langle a_i/\\|a_i\\|_2, a_j/\\|a_j\\|_2 \\rangle|$.\n-   **Welch Bound**: $\\sqrt{\\max\\left\\{0, \\frac{n - m}{m (n - 1)}\\right\\}}$.\n-   **Test Suite**: Four specific sets of parameters $(m, n, k, \\text{seed}, \\text{noise}, \\alpha_{\\min}, \\alpha_{\\max})$.\n-   **Matrix Generation**: $A$ is generated from $\\mathcal{N}(0,1)$ entries with added scaled noise.\n-   **Required Outputs**: Seven specific numerical quantities for each test case.\n\n**1.2. Validation against Criteria**\nThe problem is evaluated against the validation criteria:\n-   **Scientifically Grounded**: The problem is firmly rooted in established concepts from linear algebra, compressed sensing, and convex optimization. Concepts like the Gram matrix, Gershgorin Circle Theorem, Restricted Isometry Property (RIP), mutual coherence, Welch bound, log-sum-exp smoothing, and projected gradient descent are all standard and mathematically rigorous.\n-   **Well-Posed**: The problem is well-posed. It specifies the input, the optimization problem (a convex minimization over a convex set, which has a unique solution), the algorithm to be used, and the exact outputs required. The setup is self-contained and sufficient to produce a unique result.\n-   **Objective**: The language is precise, mathematical, and free of any subjectivity or ambiguity.\n\nThe problem does not exhibit any of the invalidity flaws. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, trivial, or unverifiable.\n\n**1.3. Verdict**\nThe problem is **valid**.\n\n### Step 2: Solution Derivation and Design\n\nThe core of the problem is to solve a constrained optimization problem. We will first formalize the objective function and its gradient, then detail the projected gradient algorithm, and finally specify the computation of the required metrics.\n\n**2.1. Convex Optimization Formulation**\nLet $A \\in \\mathbb{R}^{m \\times n}$ be the sensing matrix. The goal is to find scaling factors $d_i  0$ to minimize the maximum Gershgorin radius of the scaled Gram matrix $G(D) = D A^\\top A D$.\nThe radius of the $i$-th Gershgorin disc is $r_i(D) = \\sum_{j \\neq i} |g_{ij}(D)| = \\sum_{j \\neq i} d_i d_j |\\langle a_i, a_j \\rangle|$.\nUsing the change of variables $x_i = \\log d_i$, so that $d_i = e^{x_i}$, the radius becomes a function of $\\mathbf{x} = [x_1, \\dots, x_n]^\\top$:\n$$ r_i(\\mathbf{x}) = \\sum_{j \\neq i} |\\langle a_i, a_j \\rangle| e^{x_i + x_j} $$\nWe aim to minimize $\\max_{i} r_i(\\mathbf{x})$. As specified, we use the log-sum-exp function as a smooth surrogate for the max function. The objective function to minimize is:\n$$ f(\\mathbf{x}) = \\log\\left(\\sum_{i=1}^n \\exp(r_i(\\mathbf{x}))\\right) $$\nThis function is convex, as each $r_i(\\mathbf{x})$ is a sum of exponentials (a convex function), and the log-sum-exp of a vector of convex functions is also convex.\n\n**2.2. Gradient Derivation**\nTo implement a gradient-based method, we need the gradient $\\nabla f(\\mathbf{x})$. The $k$-th component of the gradient is given by the chain rule:\n$$ \\frac{\\partial f}{\\partial x_k} = \\frac{1}{\\sum_{l=1}^n e^{r_l(\\mathbf{x})}} \\sum_{i=1}^n e^{r_i(\\mathbf{x})} \\frac{\\partial r_i(\\mathbf{x})}{\\partial x_k} $$\nLet $p_i(\\mathbf{x}) = \\frac{e^{r_i(\\mathbf{x})}}{\\sum_l e^{r_l(\\mathbf{x})}}$. Then $\\frac{\\partial f}{\\partial x_k} = \\sum_{i=1}^n p_i(\\mathbf{x}) \\frac{\\partial r_i(\\mathbf{x})}{\\partial x_k}$.\nThe partial derivatives of $r_i(\\mathbf{x})$ are:\n-   If $i=k$: $\\frac{\\partial r_k}{\\partial x_k} = \\sum_{j \\neq k} |\\langle a_k, a_j \\rangle| e^{x_k+x_j} = r_k(\\mathbf{x})$.\n-   If $i \\neq k$: $\\frac{\\partial r_i}{\\partial x_k} = |\\langle a_i, a_k \\rangle| e^{x_i+x_k}$.\n\nCombining these, the $k$-th component of the gradient is:\n$$ (\\nabla f)_k = p_k(\\mathbf{x}) r_k(\\mathbf{x}) + \\sum_{i \\neq k} p_i(\\mathbf{x}) |\\langle a_i, a_k \\rangle| e^{x_i+x_k} $$\nFor efficient computation, this can be vectorized. Let $W$ be the matrix with entries $W_{ij} = |\\langle a_i, a_j \\rangle|$ for $i \\neq j$ and $W_{ii} = 0$. Let $\\mathbf{e^x} = [e^{x_1}, \\dots, e^{x_n}]^\\top$.\n-   The radii vector is $\\mathbf{r}(\\mathbf{x}) = \\text{diag}(\\mathbf{e^x}) (W \\mathbf{e^x})$. More simply, $r_i(\\mathbf{x}) = \\sum_j W_{ij} e^{x_i}e^{x_j}$, which can be computed from `W * outer(exp(x), exp(x))`.\n-   The gradient can be expressed as:\n    $$ \\nabla f = \\mathbf{p} \\odot \\mathbf{r} + \\mathbf{e^x} \\odot (W (\\mathbf{p} \\odot \\mathbf{e^x})) $$\n    where $\\odot$ denotes element-wise multiplication, and $\\mathbf{p}$ is the vector of probabilities $p_i(\\mathbf{x})$.\n\n**2.3. Constraints and Projection**\nThe constraints are $\\alpha_{\\min} \\le \\|d_i a_i\\|_2 \\le \\alpha_{\\max}$, which translates to $\\alpha_{\\min} \\le e^{x_i} \\|a_i\\|_2 \\le \\alpha_{\\max}$. Taking the logarithm gives box constraints on $\\mathbf{x}$:\n$$ \\log\\left(\\frac{\\alpha_{\\min}}{\\|a_i\\|_2}\\right) \\le x_i \\le \\log\\left(\\frac{\\alpha_{\\max}}{\\|a_i\\|_2}\\right) $$\nLet these bounds be $L_i$ and $U_i$. The feasible set $C$ is a hyperrectangle. The projection operator $P_C(\\mathbf{y})$ onto this set is simply element-wise clipping: $P_C(\\mathbf{y})_i = \\max(L_i, \\min(U_i, y_i))$.\n\n**2.4. Projected Gradient Method with Backtracking Line Search**\nThe optimization proceeds as follows:\n1.  Initialize $\\mathbf{x}^{(0)}$ (e.g., at the center of the feasible box).\n2.  For $t = 0, 1, 2, \\dots$ until convergence:\n    a.  Compute the gradient $\\mathbf{g}^{(t)} = \\nabla f(\\mathbf{x}^{(t)})$.\n    b.  Find a step size $\\eta_t  0$ using backtracking line search. Start with $\\eta = 1$. While the Armijo condition $f(\\mathbf{x}_{proj})  f(\\mathbf{x}^{(t)}) + c \\cdot (\\mathbf{g}^{(t)})^\\top (\\mathbf{x}_{proj} - \\mathbf{x}^{(t)})$ is not met, reduce $\\eta \\leftarrow \\beta \\eta$, where $\\mathbf{x}_{proj} = P_C(\\mathbf{x}^{(t)} - \\eta \\mathbf{g}^{(t)})$. Typical values are $c=10^{-4}$ and $\\beta=0.5$.\n    c.  Update: $\\mathbf{x}^{(t+1)} = P_C(\\mathbf{x}^{(t)} - \\eta_t \\mathbf{g}^{(t)})$.\n3.  The final $\\mathbf{x}^*$ gives the optimal scaling factors $d_i^* = e^{x_i^*}$.\n\n**2.5. Computation of Metrics**\nAfter finding the optimal scaling $D = \\mathrm{diag}(e^{\\mathbf{x}^*})$, we compute the seven required metrics for both the original matrix $A$ and the scaled matrix $AD$.\n-   **Gershgorin Radii**: Computed from $G=A^\\top A$ and $G(D)=DGD$.\n-   **$\\delta_k$ Bound**: For each row $i$ of the respective Gram matrix ($G$ or $G(D)$), we calculate $|g_{ii} - 1| + S_i$, where $S_i$ is the sum of the $k-1$ largest off-diagonal absolute values in that row. The final bound is the maximum over all rows.\n-   **Norm Deviation**: Calculated as $\\max_i |d_i^* \\|a_i\\|_2 - 1|$.\n-   **Mutual Coherence  Welch Bound**: Calculated from the original matrix $A$ by first normalizing its columns.\n\nThe implementation will follow this principled design. For numerical stability, the log-sum-exp calculation will use the standard subtraction-of-maximum trick to prevent overflow.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates matrix generation, optimization, and metric calculation.\n    \"\"\"\n\n    def solve_single_case(m, n, k, seed, noise_frac, alpha_min, alpha_max):\n        \"\"\"\n        Solves the problem for a single set of parameters.\n        \"\"\"\n        \n        # 1. Generate matrix A with specified noise level\n        rng = np.random.default_rng(seed)\n        A_base = rng.standard_normal((m, n))\n        if noise_frac  0:\n            Z = rng.standard_normal((m, n))\n            norm_A_base = np.linalg.norm(A_base, 'fro')\n            norm_Z = np.linalg.norm(Z, 'fro')\n            if norm_Z  1e-12:\n                eta = noise_frac * norm_A_base / norm_Z\n                A = A_base + eta * Z\n            else:\n                A = A_base\n        else:\n            A = A_base\n\n        G = A.T @ A\n        col_norms = np.linalg.norm(A, axis=0)\n        # Prevent division by zero for potential zero-norm columns\n        col_norms[col_norms  1e-12] = 1e-12\n\n        # 2. Calculate metrics that are independent of scaling\n        # Metric 6: Mutual Coherence\n        A_normalized = A / col_norms\n        G_normalized = A_normalized.T @ A_normalized\n        np.fill_diagonal(G_normalized, 0)\n        mu = np.max(np.abs(G_normalized))\n        \n        # Metric 7: Welch Bound\n        if n  m:\n            welch_bound = np.sqrt((n - m) / (m * (n - 1)))\n        else:\n            welch_bound = 0.0\n\n        # 3. Calculate metrics for the original matrix A (before scaling)\n        def get_delta_k_bound(gram_matrix, k_val):\n            abs_G = np.abs(gram_matrix)\n            g_ii = np.diag(gram_matrix)\n            np.fill_diagonal(abs_G, 0)\n            \n            delta_k_terms = np.zeros(gram_matrix.shape[0])\n            for i in range(gram_matrix.shape[0]):\n                row = abs_G[i, :]\n                if k_val = 1:\n                    sum_k1_largest = 0.0\n                else:\n                    # Use partition for O(n) complexity vs O(n log n) for sort\n                    partition_idx = len(row) - (k_val - 1)\n                    sum_k1_largest = np.sum(np.partition(row, partition_idx)[partition_idx:])\n                delta_k_terms[i] = np.abs(g_ii[i] - 1) + sum_k1_largest\n            return np.max(delta_k_terms)\n\n        abs_G_unscaled = np.abs(G)\n        np.fill_diagonal(abs_G_unscaled, 0)\n        # Metric 1: Max Gershgorin radius before scaling\n        max_gersh_radius_before = np.max(np.sum(abs_G_unscaled, axis=1))\n        # Metric 3: Gershgorin-based delta_k bound before scaling\n        delta_k_before = get_delta_k_bound(G, k)\n        \n        # 4. Run the optimization\n        W = np.abs(G)\n        np.fill_diagonal(W, 0)\n\n        lower_bounds = np.log(alpha_min / col_norms)\n        upper_bounds = np.log(alpha_max / col_norms)\n        \n        # Initialize x at the center of the feasible box\n        x = (lower_bounds + upper_bounds) / 2.0\n        \n        # Hyperparameters for projected gradient descent\n        max_iter = 500\n        tolerance = 1e-7\n        beta = 0.5  # for backtracking line search\n        c_armijo = 1e-4\n\n        for _ in range(max_iter):\n            x_old = x.copy()\n            \n            # Compute objective value and gradient\n            exp_x = np.exp(x)\n            exp_x_outer = np.outer(exp_x, exp_x)\n            radii_vec = np.sum(W * exp_x_outer, axis=1)\n            \n            # Objective (stable log-sum-exp)\n            max_r = np.max(radii_vec)\n            exp_radii_shifted = np.exp(radii_vec - max_r)\n            sum_exp_radii = np.sum(exp_radii_shifted)\n            obj_val = max_r + np.log(sum_exp_radii)\n            \n            # Gradient (stable and vectorized)\n            p = exp_radii_shifted / sum_exp_radii\n            term1 = p * radii_vec\n            u = p * exp_x\n            v = W @ u\n            term2 = exp_x * v\n            grad = term1 + term2\n            \n            # Backtracking line search with Armijo condition\n            eta = 1.0\n            while True:\n                x_proj = np.clip(x - eta * grad, lower_bounds, upper_bounds)\n                \n                exp_x_proj = np.exp(x_proj)\n                exp_x_proj_outer = np.outer(exp_x_proj, exp_x_proj)\n                radii_vec_proj = np.sum(W * exp_x_proj_outer, axis=1)\n                max_r_proj = np.max(radii_vec_proj)\n                obj_val_proj = max_r_proj + np.log(np.sum(np.exp(radii_vec_proj - max_r_proj)))\n                \n                armijo_rhs = obj_val + c_armijo * np.dot(grad, x_proj - x)\n                \n                if obj_val_proj = armijo_rhs:\n                    break\n                \n                eta *= beta\n                if eta  1e-12:\n                    x_proj = np.clip(x - eta * grad, lower_bounds, upper_bounds)\n                    break\n            \n            x = x_proj\n            \n            if np.linalg.norm(x - x_old)  tolerance:\n                break\n                \n        # 5. Calculate metrics for the scaled matrix A*D (after scaling)\n        d_opt = np.exp(x)\n        D_opt = np.diag(d_opt)\n        G_scaled = D_opt @ G @ D_opt\n        \n        abs_G_scaled = np.abs(G_scaled)\n        np.fill_diagonal(abs_G_scaled, 0)\n        # Metric 2: Max Gershgorin radius after scaling\n        max_gersh_radius_after = np.max(np.sum(abs_G_scaled, axis=1))\n        # Metric 4: Gershgorin-based delta_k bound after scaling\n        delta_k_after = get_delta_k_bound(G_scaled, k)\n        \n        # Metric 5: Max deviation of rescaled column norms from 1\n        scaled_norms = d_opt * col_norms\n        max_norm_dev = np.max(np.abs(scaled_norms - 1))\n        \n        return [round(val, 6) for val in [\n            max_gersh_radius_before, max_gersh_radius_after,\n            delta_k_before, delta_k_after,\n            max_norm_dev, mu, welch_bound\n        ]]\n\n    # Define test cases from the problem statement\n    test_cases = [\n        (20, 30, 4, 1, 0.05, 0.90, 1.10),\n        (20, 30, 4, 2, 0.05, 0.99, 1.01),\n        (40, 60, 6, 3, 0.15, 0.70, 1.30),\n        (8, 12, 3, 4, 0.00, 0.85, 1.15),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = solve_single_case(*params)\n        all_results.append(result)\n\n    # Format the final output string to match the required format\n    inner_strs = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output_str = f\"[{','.join(inner_strs)}]\"\n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}