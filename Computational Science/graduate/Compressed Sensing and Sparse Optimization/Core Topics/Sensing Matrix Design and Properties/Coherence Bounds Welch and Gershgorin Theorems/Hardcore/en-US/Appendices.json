{
    "hands_on_practices": [
        {
            "introduction": "This first practice serves as a foundational exercise to bridge theory and computation. By implementing the core definitions from first principles, you will calculate the empirical mutual coherence $\\mu$ of a randomly generated sensing matrix and compare it against the theoretical floor set by the Welch bound. Furthermore, this exercise illuminates the practical utility of the Gershgorin circle theorem by tasking you with computing both the exact (but computationally expensive) Restricted Isometry Constant $\\delta_s$ and its more tractable Gershgorin-based upper bounds, revealing how these bounds perform in the presence of realistic column normalization errors .",
            "id": "3434936",
            "problem": "You are given the task of programmatically studying the interplay between the mutual coherence, Welch’s lower bound, and Gershgorin-based bounds on restricted isometry constants for random partial orthonormal matrices whose columns are perturbed by multiplicative normalization errors.\n\nStarting from the following base concepts only:\n- Definition of mutual coherence: for a matrix with columns, mutual coherence is the maximum absolute normalized inner product among pairs of distinct columns.\n- The Welch bound: for any set of a fixed number of unit vectors in a fixed dimension, the mutual coherence obeys a universal lower bound determined solely by the dimension and the number of vectors.\n- The Gershgorin circle theorem: every eigenvalue of a real symmetric matrix lies within at least one Gershgorin disc determined by the diagonal entry and the sum of absolute values of off-diagonal entries in the corresponding row.\n- The restricted isometry constant of order $s$ for a matrix: the smallest nonnegative number such that the squared norm of the product with any $s$-sparse vector is sandwiched between linear perturbations of the vector’s squared norm.\n\nYour program must implement the following pipeline for each test case:\n1. Construct a random orthonormal basis in $\\mathbb{R}^{n \\times n}$, select $m$ distinct rows to form a random $m \\times n$ partial orthonormal matrix, and then apply independent multiplicative column scaling of the form $1 + \\varepsilon \\cdot \\theta_j$ where each $\\theta_j$ is in $\\left[-1,1\\right]$, to introduce column normalization errors. All randomness must be controlled by a provided integer seed for reproducibility.\n2. Compute the empirical mutual coherence $\\mu$ by normalizing each pairwise inner product by the product of the corresponding column norms and taking the maximum absolute value over all distinct column pairs.\n3. Compute the Welch lower bound as a function of $m$ and $n$ and report it alongside $\\mu$. Also report whether $\\mu$ is greater than or equal to the Welch bound, up to standard numerical tolerance.\n4. Compute the exact restricted isometry constant $\\delta_s$ by exhaustively enumerating all column subsets of size $s$, forming their Gram matrices, and taking the maximum absolute deviation of eigenvalues from $1$ across all subsets. This is equivalent to optimizing the quadratic form over all unit vectors supported on a subset of size $s$ but must be carried out via subset enumeration and eigenvalue computation to ensure exactness.\n5. Compute two Gershgorin-based upper bounds for $\\delta_s$ using only directly computable matrix quantities:\n   - A coherence-based bound derived from Gershgorin applied to the Gram matrix of unit-normalized columns, which yields an $s$-dependent bound in terms of the mutual coherence.\n   - A bound derived from Gershgorin applied to the unnormalized Gram matrix, which incorporates both diagonal deviations from unity induced by column normalization errors and a uniform bound on off-diagonal magnitudes.\n6. Compare the exact $\\delta_s$ to each bound and report whether each bound upper-bounds the exact value within numerical tolerance.\n\nYour program must use the following test suite. For each tuple, the entries are $(\\text{seed}, n, m, s, \\varepsilon)$:\n- $(123, 16, 8, 3, 0.05)$: a balanced moderate-size case to test typical behavior.\n- $(456, 18, 15, 4, 0.10)$: a case with $m$ close to $n$ to test a small Welch bound and the effect of modest normalization errors.\n- $(789, 14, 7, 1, 0.20)$: a boundary case with $s=1$ to isolate diagonal deviations from unity.\n\nFinal output requirements:\n- For each test case, output a list with eight entries in the following order:\n  1. The empirical mutual coherence $\\mu$ as a float.\n  2. The Welch lower bound as a float.\n  3. A boolean indicating whether $\\mu$ is greater than or equal to the Welch bound within a small tolerance.\n  4. The exact $\\delta_s$ as a float.\n  5. The coherence-based Gershgorin upper bound for $\\delta_s$ as a float.\n  6. The unnormalized Gershgorin upper bound for $\\delta_s$ as a float.\n  7. A boolean indicating whether the exact $\\delta_s$ is less than or equal to the coherence-based bound within a small tolerance.\n  8. A boolean indicating whether the exact $\\delta_s$ is less than or equal to the unnormalized Gershgorin bound within a small tolerance.\n- Your program should produce a single line of output containing the results for all test cases as a list of lists, in the exact format:\n  [[case1_entry1,case1_entry2,...,case1_entry8],[case2_entry1,...,case2_entry8],[case3_entry1,...,case3_entry8]]\n\nNo physical units or angle units are involved. All computations must be real-valued. Ensure scientific realism by using exact subset enumeration when computing the exact restricted isometry constant and by deriving all reported bounds from the base concepts listed above without shortcuts or black-box formulas.",
            "solution": "We begin from foundational definitions and classical theorems and derive the quantities to be computed as well as the algorithmic steps.\n\nLet $A \\in \\mathbb{R}^{m \\times n}$ denote the sensing matrix. Let $a_j \\in \\mathbb{R}^m$ denote the $j$-th column of $A$. The mutual coherence is defined as\n$$\n\\mu \\triangleq \\max_{i \\neq j} \\frac{\\left| \\langle a_i, a_j \\rangle \\right|}{\\|a_i\\|_2 \\, \\|a_j\\|_2},\n$$\nthat is, the maximum absolute normalized inner product among distinct columns. This quantity is invariant to positive rescaling of individual columns.\n\nThe Welch bound arises by considering $n$ unit vectors in $\\mathbb{R}^m$, their Gram matrix $G \\in \\mathbb{R}^{n \\times n}$ with ones on the diagonal, and the Frobenius norm constraint tied to the dimension $m$. Specifically, let $G = X^\\top X$ for $X \\in \\mathbb{R}^{m \\times n}$ with columns $x_j$ of unit norm. The eigenvalues of $G$ are nonnegative and sum to $n$, with at most $m$ nonzero eigenvalues because $\\operatorname{rank}(G) \\le m$. Among all such $G$, the sum of squares of off-diagonal entries,\n$$\n\\sum_{i \\ne j} G_{ij}^2 = \\|G\\|_F^2 - \\operatorname{trace}(G^2) \\quad \\text{(with diagonal subtracted)},\n$$\nis minimized when the nonzero eigenvalues are equal, by convexity and symmetry. This yields the classical Welch lower bound on the maximum off-diagonal magnitude. Specializing to the setting at hand, for $n$ unit vectors in $\\mathbb{R}^m$, the mutual coherence obeys\n$$\n\\mu \\ge \\sqrt{\\frac{n - m}{m (n - 1)}}.\n$$\nIn our computations the columns of $A$ are generally not unit norm after row selection and scaling, but the mutual coherence as defined above properly normalizes the inner products, so we compare the empirical $\\mu$ to this universal lower bound.\n\nThe restricted isometry constant (RIC) of order $s$, denoted $\\delta_s$, is defined via the inequality\n$$\n(1 - \\delta_s) \\, \\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1 + \\delta_s) \\, \\|x\\|_2^2\n\\quad \\text{for all $s$-sparse vectors $x \\in \\mathbb{R}^n$}.\n$$\nEquivalently, for every subset $S \\subset \\{1,\\dots,n\\}$ with $|S| = s$, letting $A_S$ denote the $m \\times s$ submatrix with columns indexed by $S$ and $G_S = A_S^\\top A_S$ its Gram matrix, we have\n$$\n\\lambda_{\\min}(G_S) \\ge 1 - \\delta_s, \\quad \\lambda_{\\max}(G_S) \\le 1 + \\delta_s,\n$$\nand hence\n$$\n\\delta_s = \\max_{|S|=s} \\max \\left\\{ 1 - \\lambda_{\\min}(G_S), \\, \\lambda_{\\max}(G_S) - 1 \\right\\}.\n$$\nTherefore, the exact $\\delta_s$ can be computed by enumerating all subsets $S$ of size $s$, computing the extreme eigenvalues of $G_S$, and aggregating their worst-case deviations from $1$.\n\nTo obtain tractable upper bounds for $\\delta_s$ from first principles, we use the Gershgorin circle theorem. For any real symmetric matrix $M$, every eigenvalue lies within at least one interval of the form $[M_{ii} - R_i, M_{ii} + R_i]$ where $R_i = \\sum_{j \\ne i} |M_{ij}|$ is the absolute row sum radius. Applying this to $G_S$ yields, for each $i \\in S$,\n$$\n\\lambda \\in \\left[ (G_S)_{ii} - \\sum_{j \\in S, j \\ne i} |(G_S)_{ij}|, \\, (G_S)_{ii} + \\sum_{j \\in S, j \\ne i} |(G_S)_{ij}| \\right].\n$$\nLet $d_j = \\|a_j\\|_2^2 = G_{jj}$ and let $\\nu = \\max_{i \\ne j} |G_{ij}|$. Then, uniformly over all $S$ with $|S| = s$ and all $i \\in S$, we get\n$$\n\\lambda \\in \\left[ d_i - (s - 1) \\nu, \\, d_i + (s - 1) \\nu \\right].\n$$\nIt follows that the deviation of any such eigenvalue from $1$ is bounded by\n$$\n| \\lambda - 1 | \\le | d_i - 1 | + (s - 1) \\nu.\n$$\nMaximizing over $i$ and $S$ yields the unnormalized Gershgorin bound\n$$\n\\delta_s \\le \\alpha + (s - 1) \\nu, \\quad \\text{where } \\alpha \\triangleq \\max_j | d_j - 1 |, \\;\\; \\nu \\triangleq \\max_{i \\ne j} | G_{ij} |.\n$$\n\nIf, in addition, all columns are normalized to unit norm so that $d_j = 1$ for all $j$, then $G_{ij}$ equals the normalized inner product. In that case, by bounding row sums using the mutual coherence,\n$$\n\\sum_{j \\in S, j \\ne i} | G_{ij} | \\le (s - 1) \\mu,\n$$\nand therefore the eigenvalues of $G_S$ lie in $[1 - (s - 1) \\mu, \\, 1 + (s - 1) \\mu]$, which implies the coherence-based Gershgorin bound\n$$\n\\delta_s \\le (s - 1) \\mu.\n$$\nIn our setting, the actual matrix $A$ has column normalization errors; thus the unnormalized Gershgorin bound applies directly to $A$, while the coherence-based bound corresponds to the matrix whose columns are individually normalized to unit norm. Consequently, the coherence-based bound may under-estimate the true RIC of the unnormalized $A$, whereas the unnormalized Gershgorin bound is guaranteed to upper-bound the true RIC.\n\nAlgorithmic design:\n1. Random partial orthonormal construction: generate a Haar-distributed orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ by performing a QR decomposition of a Gaussian random matrix and correcting column signs to make the distribution uniform. Select $m$ distinct rows to form $A_0 \\in \\mathbb{R}^{m \\times n}$.\n2. Column normalization errors: draw independent scales $s_j \\in [1 - \\varepsilon, 1 + \\varepsilon]$ and set $A = A_0 \\, \\mathrm{diag}(s)$.\n3. Mutual coherence: compute the Gram matrix $G = A^\\top A$ and column norms via $d_j = G_{jj}$; then compute normalized pairwise correlations $G_{ij}/\\sqrt{d_i d_j}$ and take the maximum absolute off-diagonal value.\n4. Welch bound: compute the lower bound in terms of $m$ and $n$.\n5. Exact RIC: enumerate all $\\binom{n}{s}$ supports $S$ of size $s$, compute $G_S$ and its extreme eigenvalues, and accumulate the maximum deviation from $1$.\n6. Gershgorin bounds: compute $\\alpha = \\max_j |d_j - 1|$, $\\nu = \\max_{i \\ne j} |G_{ij}|$, and form $\\alpha + (s - 1)\\nu$; also compute $(s - 1)\\mu$.\n7. Tolerance checks: test $\\mu$ against the Welch bound and $\\delta_s$ against each Gershgorin bound with a small nonnegative tolerance to account for floating-point arithmetic.\n\nThe test suite covers:\n- A typical regime with moderate dimensions and small scaling errors.\n- A case with $m$ close to $n$ where the Welch lower bound is small, and scaling errors are modest.\n- The boundary case $s = 1$, where the RIC reduces to maximum diagonal deviation and hence isolates the effect of column normalization errors.\n\nThe program prints a single line: a list containing, for each test case, a list with eight entries in the specified order. All reported quantities are computed directly from the definitions and the Gershgorin theorem as derived above, and the exact RIC is obtained by exhaustive subset enumeration with eigenvalue calculations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef haar_orthogonal(n: int, rng: np.random.Generator) - np.ndarray:\n    \"\"\"\n    Generate a Haar-distributed orthogonal matrix Q in R^{n x n}\n    using QR decomposition of a Gaussian random matrix with sign correction.\n    \"\"\"\n    X = rng.standard_normal((n, n))\n    Q, R = np.linalg.qr(X)\n    # Make Q Haar by ensuring R has positive diagonal\n    d = np.sign(np.diag(R))\n    d[d == 0] = 1.0\n    Q = Q @ np.diag(d)\n    return Q\n\ndef random_partial_with_scaling(seed: int, n: int, m: int, eps: float) - np.ndarray:\n    \"\"\"\n    Construct an m x n random partial orthonormal matrix with column scaling.\n    - Generate Haar orthogonal Q (n x n)\n    - Choose m random rows\n    - Scale columns by factors in [1 - eps, 1 + eps]\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    Q = haar_orthogonal(n, rng)\n    rows = rng.choice(n, size=m, replace=False)\n    A0 = Q[rows, :]  # m x n\n    scales = 1.0 + eps * rng.uniform(-1.0, 1.0, size=n)\n    A = A0 * scales  # broadcast column-wise scaling\n    return A\n\ndef mutual_coherence(A: np.ndarray) - float:\n    \"\"\"\n    Compute empirical mutual coherence:\n    max_{i != j} |a_i, a_j| / (||a_i|| ||a_j||)\n    \"\"\"\n    G = A.T @ A\n    norms = np.sqrt(np.clip(np.diag(G), 1e-18, None))\n    denom = norms[:, None] * norms[None, :]\n    C = G / denom\n    C = np.abs(C)\n    np.fill_diagonal(C, 0.0)\n    return float(np.max(C))\n\ndef welch_bound(m: int, n: int) - float:\n    \"\"\"\n    Welch lower bound for mutual coherence of n unit vectors in R^m.\n    \"\"\"\n    if n = 1 or m = 0:\n        return 0.0\n    # Guard against numerical issues when n == m\n    num = max(n - m, 0.0)\n    den = m * (n - 1.0)\n    if den = 0:\n        return 0.0\n    return float(np.sqrt(num / den))\n\ndef exact_ric_via_subsets(A: np.ndarray, s: int) - float:\n    \"\"\"\n    Compute the exact restricted isometry constant delta_s by exhaustive enumeration:\n    delta_s = max_{|S|=s} max(1 - lambda_min(G_S), lambda_max(G_S) - 1),\n    where G_S = A_S^T A_S.\n    \"\"\"\n    n = A.shape[1]\n    if s = 0 or s  n:\n        return 0.0\n    G = A.T @ A\n    worst = 0.0\n    for S in combinations(range(n), s):\n        idx = np.array(S, dtype=int)\n        Gs = G[np.ix_(idx, idx)]\n        # Eigenvalues of symmetric matrix\n        vals = np.linalg.eigvalsh(Gs)\n        dev = max(abs(float(vals[0]) - 1.0), abs(float(vals[-1]) - 1.0))\n        if dev  worst:\n            worst = dev\n    return worst\n\ndef gersh_bounds(A: np.ndarray, s: int) - tuple[float, float]:\n    \"\"\"\n    Compute two Gershgorin-based upper bounds for delta_s:\n    - coherence-based: (s - 1) * mu, where mu uses normalized columns\n    - unnormalized: alpha + (s - 1) * nu, where alpha = max |d_j - 1| (diagonal deviation),\n      nu = max_{i != j} |G_ij| for the unnormalized Gram matrix G = A^T A\n    \"\"\"\n    G = A.T @ A\n    # Unnormalized quantities\n    d = np.diag(G)\n    alpha = float(np.max(np.abs(d - 1.0))) if d.size  0 else 0.0\n    G_abs = np.abs(G.copy())\n    np.fill_diagonal(G_abs, 0.0)\n    nu = float(np.max(G_abs)) if G_abs.size  0 else 0.0\n    delta_gersh_unnorm = alpha + max(s - 1, 0) * nu\n    # Coherence-based\n    mu = mutual_coherence(A)\n    delta_gersh_coh = max(s - 1, 0) * mu\n    return delta_gersh_coh, delta_gersh_unnorm\n\ndef run_case(seed: int, n: int, m: int, s: int, eps: float) - list:\n    \"\"\"\n    Execute one test case and return the required list of results:\n    [mu, welch, mu_ge_welch, delta_true, delta_coh, delta_gersh, true_le_coh, true_le_gersh]\n    \"\"\"\n    A = random_partial_with_scaling(seed, n, m, eps)\n    mu = mutual_coherence(A)\n    wb = welch_bound(m, n)\n    # Exact RIC\n    delta_true = exact_ric_via_subsets(A, s)\n    # Gershgorin bounds\n    delta_coh, delta_gersh = gersh_bounds(A, s)\n    # Tolerance for comparisons\n    tol = 1e-10\n    mu_ge_wb = (mu + tol) = wb\n    ric_le_coh = (delta_true = delta_coh + tol)\n    ric_le_gersh = (delta_true = delta_gersh + tol)\n    # Return values; floats as native Python floats, booleans as bools\n    return [\n        float(mu),\n        float(wb),\n        bool(mu_ge_wb),\n        float(delta_true),\n        float(delta_coh),\n        float(delta_gersh),\n        bool(ric_le_coh),\n        bool(ric_le_gersh),\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (seed, n, m, s, eps)\n    test_cases = [\n        (123, 16, 8, 3, 0.05),\n        (456, 18, 15, 4, 0.10),\n        (789, 14, 7, 1, 0.20),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, m, s, eps = case\n        result = run_case(seed, n, m, s, eps)\n        results.append(result)\n\n    # Final print statement in the exact required format: single line list of lists\n    print(str(results))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Having established how to compute and verify fundamental matrix properties, we now explore their practical implications. This exercise demonstrates why coherence is a critical consideration in dictionary design by linking it directly to the performance of the Orthogonal Matching Pursuit (OMP) algorithm. You will construct a matrix with a specific clustered structure, allowing you to experimentally investigate the distinct roles of intra-cluster and inter-cluster coherence in causing sparse recovery failures . This provides tangible insight into how matrix structure influences algorithmic behavior.",
            "id": "3434921",
            "problem": "You are given a matrix design and a selection procedure motivated by Orthogonal Matching Pursuit (OMP) under a clustered column structure. The goal is to model the Gram matrix with block structure, analyze selection behavior using Gershgorin circle theorem bounds, and empirically test the impact of reducing intra-cluster coherence versus reducing inter-cluster coherence on the risk of false selections.\n\nAssume a dictionary matrix $A \\in \\mathbb{R}^{M \\times N}$ with unit-norm columns organized into $C$ clusters, each of size $m$, so $N = C m$. Let the Gram matrix be $G = A^\\top A \\in \\mathbb{R}^{N \\times N}$. The dictionary is constructed as follows:\n- Choose $C$ orthonormal \"cluster center\" vectors $u_1, u_2, \\dots, u_C \\in \\mathbb{R}^M$.\n- Choose a global vector $g \\in \\mathbb{R}^M$ orthonormal to the cluster center subspace.\n- Choose $N$ orthonormal noise directions $w_1, w_2, \\dots, w_N \\in \\mathbb{R}^M$, each orthonormal to $\\{u_c\\}_{c=1}^C$ and $g$.\n- For any column index $j \\in \\{1,\\dots,N\\}$, let $c(j) \\in \\{1,\\dots,C\\}$ denote its cluster index. The $j$-th column is\n$$\na_j \\;=\\; \\alpha\\, u_{c(j)} \\;+\\; \\gamma\\, g \\;+\\; \\sqrt{1 - \\alpha^2 - \\gamma^2}\\, w_j,\n$$\nwith $0 \\le \\alpha^2 + \\gamma^2 \\le 1$.\n\nThis construction yields a Gram matrix $G$ with exact block-constant off-diagonals:\n- For $i \\neq j$ in the same cluster, $G_{ij} = \\alpha^2 + \\gamma^2$ (intra-cluster coherence).\n- For $i$ and $j$ in different clusters, $G_{ij} = \\gamma^2$ (inter-cluster coherence).\n- For all $j$, $G_{jj} = 1$.\n\nLet the measurement be $y = A x + e$, where $x \\in \\mathbb{R}^N$ is a sparse coefficient vector and $e \\in \\mathbb{R}^M$ is noise. Consider the first step of Orthogonal Matching Pursuit, which selects the index $j^\\star$ maximizing the absolute correlation\n$$\nc_j \\;=\\; a_j^\\top y \\;=\\; (A^\\top A x)_j \\;+\\; a_j^\\top e \\;=\\; (Gx)_j \\;+\\; a_j^\\top e.\n$$\n\nFundamental definitions:\n- The mutual coherence of $A$ is $\\mu(A) = \\max_{i \\neq j} |G_{ij}|$.\n- The Gershgorin circle theorem states that every eigenvalue of $G$ lies within at least one disk centered at $G_{jj}$ with radius $R_j = \\sum_{i \\neq j} |G_{ji}|$.\n- The Welch bound gives a lower bound on mutual coherence for unit-norm columns: for $A \\in \\mathbb{R}^{M \\times N}$,\n$$\n\\mu(A) \\;\\ge\\; \\sqrt{\\frac{N - M}{M(N - 1)}}.\n$$\n\nUsing $c_j = (Gx)_j + a_j^\\top e$ and the triangle inequality, one obtains a row-wise interference bound for any non-support index $j \\notin \\mathrm{supp}(x)$:\n$$\n|c_j| \\;\\le\\; \\sum_{i \\in \\mathrm{supp}(x)} |G_{ji}|\\,|x_i| \\;+\\; |a_j^\\top e|.\n$$\nWithin the block-constant model, for $j$ in the same cluster as the support, the first term amounts to a multiple of intra-cluster coherence $\\alpha^2 + \\gamma^2$, and for $j$ in other clusters it amounts to a multiple of inter-cluster coherence $\\gamma^2$. Since Gershgorin radii are $R_j = (m - 1)(\\alpha^2 + \\gamma^2) + (N - m)\\gamma^2$, large intra-cluster coherence increases $R_j$ and hence the worst-case interference magnitude that can lead to $|c_j|$ being large enough to cause false selections ($j^\\star \\notin \\mathrm{supp}(x)$).\n\nYour program must:\n1. Construct $A$ exactly as per the above model for specified $(C, m, \\alpha, \\gamma)$ using an orthonormal basis in $\\mathbb{R}^M$ with $M = C + 1 + N$ to ensure enough orthonormal noise vectors.\n2. For each test case, define a sparse $x$ supported on specified indices and with specified coefficients and signs, and define noise $e = \\sigma\\, g$ (with $g$ as above and a specified noise magnitude $\\sigma$), then compute the correlation vector $c = A^\\top(Ax + e)$.\n3. Determine the first-step OMP selection index $j^\\star = \\arg\\max_j |c_j|$ and whether $j^\\star \\in \\mathrm{supp}(x)$ (true or false).\n4. Compute the actual risk ratio\n$$\nr_{\\mathrm{actual}} \\;=\\; \\frac{\\max_{j \\notin \\mathrm{supp}(x)} |c_j|}{\\max_{i \\in \\mathrm{supp}(x)} |c_i|}.\n$$\n5. For the same basis $(u_c), g, (w_j)$ in each test case, perform two hypothetical design changes:\n   - Intra-cluster coherence reduction: replace $\\alpha$ by $\\alpha' = \\sqrt{t_{\\mathrm{intra}}}\\,\\alpha$ with the same $\\gamma$, where $t_{\\mathrm{intra}} \\in (0,1)$ is a fixed factor, and recompute $r_{\\mathrm{actual}}$.\n   - Inter-cluster coherence reduction: replace $\\gamma$ by $\\gamma' = \\sqrt{t_{\\mathrm{inter}}}\\,\\gamma$ with the same $\\alpha$, where $t_{\\mathrm{inter}} \\in (0,1)$ is a fixed factor, and recompute $r_{\\mathrm{actual}}$.\n   Then record the boolean result of whether the intra-cluster reduction produced a larger decrease in $r_{\\mathrm{actual}}$ compared to the inter-cluster reduction.\n\nTest Suite:\n- All angles are implicit inner products and thus unitless; no physical units are involved.\n- Use the following three test cases (with all numbers specified as scalars):\n  1. Case A (single cluster support, high intra-cluster coherence):\n     - $C = 3$, $m = 6$, so $N = 18$, $M = C + 1 + N = 22$.\n     - $\\alpha = \\sqrt{0.65}$, $\\gamma = \\sqrt{0.20}$ so intra-cluster coherence $\\alpha^2 + \\gamma^2 = 0.85$, inter-cluster coherence $\\gamma^2 = 0.20$.\n     - Support cluster: $c = 1$ (zero-based in code), support indices: first $5$ indices of cluster $1$, coefficients $x = [-0.08,-0.08,-0.08,0.06,0.06]$ in that cluster, all other entries zero.\n     - Noise: $e = \\sigma g$ with $\\sigma = 0.2$.\n     - Reduction factors: $t_{\\mathrm{intra}} = 0.5$, $t_{\\mathrm{inter}} = 0.5$.\n  2. Case B (multi-cluster support, moderate coherence, small noise):\n     - $C = 3$, $m = 6$, $N = 18$, $M = 22$.\n     - $\\alpha = \\sqrt{0.36}$, $\\gamma = \\sqrt{0.16}$ so intra-cluster coherence $= 0.52$, inter-cluster coherence $= 0.16$.\n     - Support clusters: $c = 0$ and $c = 1$, support indices: first $3$ indices of cluster $0$ and first $3$ indices of cluster $1$, coefficients $x = [0.08,0.08,0.08,0.08,0.08,0.08]$ on those supports, all other entries zero.\n     - Noise: $e = \\sigma g$ with $\\sigma = 0.05$.\n     - Reduction factors: $t_{\\mathrm{intra}} = 0.5$, $t_{\\mathrm{inter}} = 0.5$.\n  3. Case C (boundary case, low coherence, minimal noise):\n     - $C = 3$, $m = 6$, $N = 18$, $M = 22$.\n     - $\\alpha = \\sqrt{0.15}$, $\\gamma = \\sqrt{0.05}$ so intra-cluster coherence $= 0.20$, inter-cluster coherence $= 0.05$.\n     - Support cluster: $c = 2$, support indices: first $4$ indices of cluster $2$, coefficients $x = [0.05,0.05,0.05,0.05]$ on those supports, all other entries zero.\n     - Noise: $e = \\sigma g$ with $\\sigma = 0.02$.\n     - Reduction factors: $t_{\\mathrm{intra}} = 0.5$, $t_{\\mathrm{inter}} = 0.5$.\n\nFor each case, also compute the Gershgorin radius\n$$\nR \\;=\\; (m - 1)\\,(\\alpha^2 + \\gamma^2) \\;+\\; (N - m)\\,\\gamma^2,\n$$\nfor any row (they are identical under this construction), and the global mutual coherence $\\mu(A) = \\max\\{\\alpha^2 + \\gamma^2,\\gamma^2\\}$, as internal checks.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes an inner list with three entries:\n- A boolean indicating whether the base-case OMP selection chose a true support index.\n- A float giving $r_{\\mathrm{actual}}$ in the base case, rounded to six decimal places.\n- A boolean indicating whether reducing intra-cluster coherence produced a larger decrease in $r_{\\mathrm{actual}}$ than reducing inter-cluster coherence.\n\nFor example, the output must be of the form\n$$\n[ [\\text{bool}, \\text{float}, \\text{bool}], [\\text{bool}, \\text{float}, \\text{bool}], [\\text{bool}, \\text{float}, \\text{bool}] ].\n$$",
            "solution": "The user-provided problem has been validated and is determined to be sound, well-posed, and objective. It formalizes a conceptual question within the domain of sparse signal recovery into a concrete numerical experiment. This solution proceeds with the development of the required program, grounded in the principles of linear algebra and compressed sensing.\n\n### Principle-Based Design\n\nThe problem investigates the performance of the first step of Orthogonal Matching Pursuit (OMP) in the presence of a dictionary matrix $A$ with a specific clustered structure. The goal is to understand how different types of column coherence—intra-cluster versus inter-cluster—affect OMP's ability to correctly identify the support of a sparse signal $x$.\n\n**1. Matrix and Signal Model**\n\nThe dictionary matrix $A \\in \\mathbb{R}^{M \\times N}$ is constructed such that its Gram matrix $G = A^\\top A$ exhibits a block-constant structure. This is achieved by defining each column $a_j$ as a linear combination of three types of orthonormal vectors:\n- A cluster center vector $u_{c(j)}$ unique to the column's cluster, $c(j)$.\n- A global vector $g$ shared by all columns.\n- A noise direction vector $w_j$ unique to the column.\n\nThe formula for the $j$-th column is:\n$$\na_j = \\alpha u_{c(j)} + \\gamma g + \\sqrt{1 - \\alpha^2 - \\gamma^2} w_j\n$$\nwhere $\\alpha, \\gamma \\ge 0$ and $\\alpha^2 + \\gamma^2 \\le 1$ to ensure unit L2-norm, i.e., $a_j^\\top a_j = 1$. The orthonormality of the basis vectors $\\{u_c\\}, g, \\{w_j\\}$ is critical.\n\nThe inner product between two distinct columns $a_i$ and $a_j$ forms the off-diagonal entries of the Gram matrix $G$:\n- If $i$ and $j$ are in the same cluster ($c(i) = c(j)$, $i \\neq j$):\n  $G_{ij} = a_i^\\top a_j = (\\alpha u_{c(i)} + \\gamma g + \\dots)^\\top(\\alpha u_{c(j)} + \\gamma g + \\dots) = \\alpha^2 (u_{c(i)}^\\top u_{c(j)}) + \\gamma^2 (g^\\top g) = \\alpha^2 + \\gamma^2$.\n  This is the **intra-cluster coherence**.\n- If $i$ and $j$ are in different clusters ($c(i) \\neq c(j)$):\n  $G_{ij} = a_i^\\top a_j = \\alpha^2 (u_{c(i)}^\\top u_{c(j)}) + \\gamma^2 (g^\\top g) = 0 + \\gamma^2 = \\gamma^2$.\n  This is the **inter-cluster coherence**.\n\nThe parameters $\\alpha$ and $\\gamma$ thus directly and independently control these two types of coherence.\n\n**2. OMP Selection and Interference**\n\nOMP operates on a measurement vector $y = Ax + e$, where $x$ is a sparse signal and $e$ is measurement noise. In its first step, OMP identifies the column of $A$ most correlated with the measurement by selecting the index $j^\\star$ that maximizes the absolute correlation $|c_j|$:\n$$\nj^\\star = \\arg\\max_{j \\in \\{1,\\dots,N\\}} |c_j|\n$$\nwhere the correlation vector $c$ is given by $c = A^\\top y$. Substituting the measurement model, we get:\n$$\nc = A^\\top (Ax + e) = (A^\\top A) x + A^\\top e = Gx + A^\\top e\n$$\nFor an index $j \\notin \\mathrm{supp}(x)$, the correlation is purely due to interference from the true signal components and noise:\n$$\nc_j = \\sum_{i \\in \\mathrm{supp}(x)} G_{ji} x_i + a_j^\\top e\n$$\nA \"false selection\" occurs if $|c_j|$ for some $j \\notin \\mathrm{supp}(x)$ is larger than all $|c_i|$ for $i \\in \\mathrm{supp}(x)$.\n\nThe problem proposes a specific noise model $e = \\sigma g$. The noise term in the correlation becomes:\n$$\na_j^\\top e = a_j^\\top (\\sigma g) = \\sigma (\\alpha u_{c(j)} + \\gamma g + \\dots)^\\top g = \\sigma \\gamma (g^\\top g) = \\sigma \\gamma\n$$\nThis simplifies the analysis, as the noise contribution is constant for all columns.\n\n**3. Gershgorin Circles and Risk Analysis**\n\nThe Gershgorin circle theorem connects the diagonal entries and row sums of a matrix to the locations of its eigenvalues. While not directly used in the OMP selection rule, the row sums of the Gram matrix, which define the Gershgorin radii $R_j = \\sum_{i \\neq j} |G_{ji}|$, serve as a proxy for the total interference a column can experience. In our model, for any column $j$:\n$$\nR_j = (m-1)(\\alpha^2 + \\gamma^2) + (N-m)\\gamma^2\n$$\nA larger intra-cluster coherence $\\alpha^2 + \\gamma^2$ leads to a larger radius $R_j$, indicating a greater potential for interference from other columns within the same cluster. The problem seeks to test the hypothesis that reducing intra-cluster coherence is a more effective strategy for mitigating false selections than an equivalent reduction in inter-cluster coherence.\n\nThe risk of a false selection is quantified by the ratio:\n$$\nr_{\\mathrm{actual}} = \\frac{\\max_{j \\notin \\mathrm{supp}(x)} |c_j|}{\\max_{i \\in \\mathrm{supp}(x)} |c_i|}\n$$\nA value $r_{\\mathrm{actual}}  1$ guarantees a false selection. The experiment compares the decrease in this ratio under two scenarios: reducing $\\alpha$ versus reducing $\\gamma$.\n\n**4. Algorithmic Implementation**\n\nThe solution will be implemented by following these steps for each test case:\n1.  **Orthonormal Basis Construction**: An orthonormal basis in $\\mathbb{R}^M$ is required, with $M = C+1+N$ to ensure sufficient dimensions. The columns of the $M \\times M$ identity matrix provide a straightforward choice: $\\{u_c\\}$ will be the first $C$ columns, $g$ the $(C+1)$-th column, and $\\{w_j\\}$ the subsequent $N$ columns.\n2.  **Core Calculation Function**: A single function will be designed to perform the main computation. It takes parameters $(\\alpha, \\gamma)$ and the fixed basis, signal $x$, and noise structure $e$, then constructs the matrix $A$ and computes the correlation vector $c$ and the risk ratio $r_{\\mathrm{actual}}$.\n3.  **Simulation and Comparison**: For each test case, this core function is called three times with the same basis, $x$, and $\\sigma$:\n    a. With the base parameters $(\\alpha, \\gamma)$ to get the baseline selection result and $r_{\\mathrm{actual}}$.\n    b. With modified parameters $(\\alpha', \\gamma) = (\\sqrt{t_{\\mathrm{intra}}}\\alpha, \\gamma)$ to measure the effect of intra-cluster coherence reduction.\n    c. With modified parameters $(\\alpha, \\gamma') = (\\alpha, \\sqrt{t_{\\mathrm{inter}}}\\gamma)$ to measure the effect of inter-cluster coherence reduction.\n4.  **Result Aggregation**: The decrease in the risk ratio ($r_{\\mathrm{actual}}$) is compared between scenarios (b) and (c) to determine which strategy was more effective. The final results are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by constructing clustered dictionary matrices, simulating\n    the first step of OMP, and analyzing the impact of coherence reduction strategies.\n    \"\"\"\n\n    test_cases = [\n        # Case A: C, m, alpha^2, gamma^2, support_spec, x_coeffs, sigma, t_intra, t_inter\n        (3, 6, 0.65, 0.20, [(1, 5)], [-0.08, -0.08, -0.08, 0.06, 0.06], 0.2, 0.5, 0.5),\n        # Case B\n        (3, 6, 0.36, 0.16, [(0, 3), (1, 3)], [0.08] * 6, 0.05, 0.5, 0.5),\n        # Case C\n        (3, 6, 0.15, 0.05, [(2, 4)], [0.05] * 4, 0.02, 0.5, 0.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        C, m, alpha_sq, gamma_sq, support_spec, x_coeffs, sigma, t_intra, t_inter = case\n        \n        N = C * m\n        M = C + 1 + N\n\n        # 1. Construct the orthonormal basis shared across sub-calculations\n        basis_vectors = np.identity(M)\n        U = basis_vectors[:, :C]\n        g = basis_vectors[:, C]\n        W = basis_vectors[:, (C + 1):]\n\n        # 2. Define sparse signal x and noise e\n        support_indices = []\n        current_coeff_idx = 0\n        for cluster_idx, num_indices in support_spec:\n            start_index = cluster_idx * m\n            for i in range(num_indices):\n                support_indices.append(start_index + i)\n        \n        support_indices = np.array(support_indices)\n        \n        x = np.zeros(N)\n        if len(support_indices)  0:\n            x[support_indices] = x_coeffs\n            \n        e = sigma * g\n\n        def compute_metrics(alpha_val, gamma_val, U_basis, g_vec, W_basis, x_vec, e_vec):\n            \"\"\"\n            Core function to construct A and compute OMP metrics.\n            \"\"\"\n            alpha, gamma = alpha_val, gamma_val\n            beta = np.sqrt(max(0, 1 - alpha**2 - gamma**2))\n\n            A = np.zeros((M, N))\n            for j in range(N):\n                cluster_idx = j // m\n                u_c = U_basis[:, cluster_idx]\n                w_j = W_basis[:, j]\n                A[:, j] = alpha * u_c + gamma * g_vec + beta * w_j\n\n            # Compute correlation vector c = A^T * (A*x + e)\n            c = A.T @ (A @ x_vec + e_vec)\n\n            # OMP first step selection\n            j_star = np.argmax(np.abs(c))\n            is_in_support = j_star in support_indices\n\n            # Compute risk ratio r_actual\n            non_support_indices = np.setdiff1d(np.arange(N), support_indices)\n            c_abs = np.abs(c)\n            \n            # Use np.max on empty array with initial val to handle empty support/non-support\n            max_corr_off_support = np.max(c_abs[non_support_indices], initial=0.0)\n            max_corr_on_support = np.max(c_abs[support_indices], initial=0.0)\n\n            if max_corr_on_support == 0.0:\n                r_actual = np.inf if max_corr_off_support  0 else 0.0\n            else:\n                r_actual = max_corr_off_support / max_corr_on_support\n\n            return is_in_support, r_actual\n\n        # 3. Perform calculations for base case and hypothetical reductions\n        \n        # Base case\n        base_is_in_support, base_r = compute_metrics(\n            np.sqrt(alpha_sq), np.sqrt(gamma_sq), U, g, W, x, e)\n        \n        # Intra-cluster coherence reduction\n        alpha_prime = np.sqrt(t_intra) * np.sqrt(alpha_sq)\n        _, r_intra = compute_metrics(alpha_prime, np.sqrt(gamma_sq), U, g, W, x, e)\n\n        # Inter-cluster coherence reduction\n        gamma_prime = np.sqrt(t_inter) * np.sqrt(gamma_sq)\n        _, r_inter = compute_metrics(np.sqrt(alpha_sq), gamma_prime, U, g, W, x, e)\n        \n        # 4. Compare reductions and store results\n        delta_r_intra = base_r - r_intra\n        delta_r_inter = base_r - r_inter\n        intra_reduces_more = delta_r_intra  delta_r_inter\n\n        results.append([base_is_in_support, round(base_r, 6), intra_reduces_more])\n\n    # Final print statement in the exact required format\n    # Using str() on the list of lists produces the required format including spaces.\n    print(str(results).replace(\"'\", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Moving from analysis to design, this final practice challenges you to actively improve a sensing matrix's properties. You will begin with a randomly generated matrix and then implement a greedy column pruning algorithm to systematically reduce its mutual coherence. By comparing the coherence-based recovery guarantees for Orthogonal Matching Pursuit (OMP) before and after pruning, you will gain a practical understanding of the trade-offs in dictionary design and see firsthand how a simple modification can strengthen theoretical performance bounds .",
            "id": "3434906",
            "problem": "You are given the task of quantitatively assessing how removing a fixed number of columns from a near-Equiangular Tight Frame (near-ETF) type sensing matrix affects key coherence-based recovery guarantees in compressed sensing and sparse optimization. Work strictly in a purely mathematical setting with real matrices and no physical units. All angles are irrelevant to this problem.\n\nStart from the following fundamental base:\n- The definitions of mutual coherence, Gram matrix, restricted isometry constant, and support-restricted submatrices.\n- The Welch bound on coherence for unit-norm frames.\n- The Gershgorin circle theorem.\n- The classical mutual coherence-based sufficient condition for exact recovery by Orthogonal Matching Pursuit (OMP), where Orthogonal Matching Pursuit (OMP) is a greedy algorithm for sparse recovery.\n\nYour program must implement the following procedure for each test case:\n1. Construct a real sensing matrix with dimensions $m \\times n$ as follows. Draw a matrix with independent standard normal entries, and normalize each column to unit $\\ell_2$-norm. This constructs a random near-ETF in the sense that its mutual coherence is close to minimal for the given $(m,n)$ but not necessarily equal to the Welch bound.\n2. Compute the mutual coherence $\\mu$ as the maximum absolute off-diagonal entry of the Gram matrix of the column-normalized sensing matrix.\n3. Using only fundamental results, derive and compute:\n   - A lower bound on $\\mu$ predicted by the Welch theorem for the given $(m,n)$.\n   - An upper bound on the restricted isometry constant $\\delta_s$ based on Gershgorin discs applied to $s \\times s$ principal submatrices of the Gram matrix.\n   - The largest sparsity level $s_{\\text{OMP}}$ for which a mutual coherence-based OMP success guarantee holds, stated purely in terms of $\\mu$.\n4. Perform column pruning by eliminating $k$ columns that are most deleterious to coherence (and thus worst for the Gershgorin-based bound) using the following greedy rule: at each pruning step, compute for each column $j$ the sum over $i \\neq j$ of the absolute inner products of column $j$ with all other columns, and remove the column with the largest such sum. After removing one column, recompute these sums on the reduced matrix and repeat until $k$ columns are removed. Denote by $n' = n - k$ the post-pruning number of columns.\n5. Recompute all quantities from step $2$ and step $3$ for the pruned matrix.\n6. For a prescribed fixed sparsity level $s$, decide whether the OMP success guarantee is met before and after pruning by comparing $s$ against the pre- and post-pruning $s_{\\text{OMP}}$.\n\nRestrictions and conventions:\n- All columns are normalized to have unit $\\ell_2$-norm.\n- The Gram matrix must be computed exactly from the constructed sensing matrix.\n- If $n \\le m$, the Welch lower bound for $\\mu$ should be taken to be $0$.\n- The sparsity level $s$ is guaranteed in all test cases to satisfy $s \\le n'$ after pruning.\n- All computations are deterministic given the seeded random generator for each test case.\n- For all floating-point outputs, round to exactly six digits after the decimal point using standard rounding.\n\nTest suite:\nFor each of the following parameter tuples $(m,n,k,s,\\text{seed})$, execute the above procedure:\n- Test case A: $(m,n,k,s,\\text{seed}) = (30,60,10,4,1)$.\n- Test case B: $(m,n,k,s,\\text{seed}) = (40,80,0,6,2)$.\n- Test case C: $(m,n,k,s,\\text{seed}) = (20,100,40,8,3)$.\n- Test case D: $(m,n,k,s,\\text{seed}) = (50,55,4,3,4)$.\n- Test case E: $(m,n,k,s,\\text{seed}) = (30,90,20,10,5)$.\n\nFor each test case, your program must output a list in the following order:\n- The pre-pruning mutual coherence $\\mu$ as a float rounded to six decimals.\n- The post-pruning mutual coherence $\\mu'$ as a float rounded to six decimals.\n- The pre-pruning Gershgorin-based upper bound on $\\delta_s$ as a float rounded to six decimals.\n- The post-pruning Gershgorin-based upper bound on $\\delta_s$ as a float rounded to six decimals.\n- The pre-pruning largest OMP-guaranteed sparsity $s_{\\text{OMP}}$ as an integer.\n- The post-pruning largest OMP-guaranteed sparsity $s_{\\text{OMP}}'$ as an integer.\n- The boolean indicating whether the OMP guarantee holds for the fixed $s$ before pruning.\n- The boolean indicating whether the OMP guarantee holds for the fixed $s$ after pruning.\n- The Welch lower bound for $\\mu$ before pruning as a float rounded to six decimals.\n- The Welch lower bound for $\\mu$ after pruning as a float rounded to six decimals.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of per-test-case lists, enclosed in square brackets. For example, a valid output has the form\n\"[[case1_values...],[case2_values...],...]\" with no spaces. Each inner list must follow exactly the order specified above. No other text must be printed.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of compressed sensing and sparse optimization, well-posed with a clear algorithmic procedure, and objective in its formulation. All necessary data and definitions are provided, and there are no internal contradictions or ambiguities.\n\nThe task is to analyze the effects of a specific greedy column-pruning strategy on the coherence-based recovery guarantees of a sensing matrix. For each test case, specified by parameters $(m, n, k, s, \\text{seed})$, we will perform a sequence of calculations before and after pruning the matrix.\n\n**1. Sensing Matrix Construction**\n\nFirst, a sensing matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is constructed. Its entries are drawn independently from a standard normal distribution, $\\mathcal{N}(0,1)$. Subsequently, each of its $n$ columns, denoted $\\mathbf{a}_i$ for $i \\in \\{1, \\dots, n\\}$, is normalized to have a unit Euclidean norm ($\\ell_2$-norm):\n$$\n\\|\\mathbf{a}_i\\|_2 = 1 \\quad \\forall i \\in \\{1, \\dots, n\\}\n$$\nThis process yields a random matrix whose columns form a unit-norm frame. The Gram matrix $\\mathbf{G} \\in \\mathbb{R}^{n \\times n}$ is then computed as $\\mathbf{G} = \\mathbf{A}^T\\mathbf{A}$. The entries of the Gram matrix are the inner products of the columns of $\\mathbf{A}$: $G_{ij} = \\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle$. Due to column normalization, the diagonal entries of $\\mathbf{G}$ are all unity, $G_{ii} = \\|\\mathbf{a}_i\\|_2^2 = 1$. The off-diagonal entries quantify the correlation between different columns.\n\n**2. Pre-Pruning Analysis of Coherence-Based Guarantees**\n\nBefore any columns are removed, we compute several key metrics based on the initial matrix $\\mathbf{A}$.\n\n*   **Mutual Coherence ($\\mu$):** The mutual coherence of $\\mathbf{A}$ is the maximum absolute value of the off-diagonal entries of its Gram matrix. It measures the worst-case correlation between any two distinct columns.\n    $$\n    \\mu = \\mu(\\mathbf{A}) = \\max_{i \\neq j} |\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle| = \\max_{i \\neq j} |G_{ij}|\n    $$\n*   **Welch Bound ($\\mu_W$):** This theorem provides a fundamental lower bound on the mutual coherence for any frame with $n$ unit-norm vectors in $\\mathbb{R}^m$. For $n  m$, the bound is given by:\n    $$\n    \\mu_W = \\sqrt{\\frac{n-m}{m(n-1)}}\n    $$\n    For cases where $n \\le m$, the problem specifies using $\\mu_W = 0$. Matrices that achieve this bound are called Equiangular Tight Frames (ETFs). The constructed random matrix is expected to be a near-ETF, meaning its coherence $\\mu$ will be close to $\\mu_W$.\n*   **Gershgorin-Based Bound on the Restricted Isometry Constant ($\\delta_s$):** The Restricted Isometry Property (RIP) is a central concept for recovery guarantees. A matrix $\\mathbf{A}$ has the RIP of order $s$ with constant $\\delta_s$ if for any $s$-sparse vector $\\mathbf{x}$, $(1-\\delta_s)\\|\\mathbf{x}\\|_2^2 \\le \\|\\mathbf{A}\\mathbf{x}\\|_2^2 \\le (1+\\delta_s)\\|\\mathbf{x}\\|_2^2$. This is equivalent to requiring that all eigenvalues of any $s \\times s$ principal submatrix $\\mathbf{G}_S$ of the Gram matrix $\\mathbf{G}$ lie in the interval $[1-\\delta_s, 1+\\delta_s]$. The Gershgorin circle theorem states that every eigenvalue of $\\mathbf{G}_S$ lies within the union of discs centered at its diagonal entries. Since $(\\mathbf{G}_S)_{ii}=1$, the eigenvalues $\\lambda$ must satisfy $|\\lambda - 1| \\le \\max_{i \\in S} \\sum_{j \\in S, j \\neq i} |G_{ij}|$. The sum is bounded by $(s-1)\\mu$. This gives a simple upper bound on $\\delta_s$:\n    $$\n    \\delta_s \\le (s-1)\\mu\n    $$\n    We compute this value, $\\delta_{s, \\text{Gersh}} = (s-1)\\mu$, for the prescribed sparsity level $s$.\n*   **OMP Recovery Guarantee ($s_{\\text{OMP}}$):** Orthogonal Matching Pursuit (OMP) is a greedy algorithm guaranteed to perfectly recover any $s$-sparse signal $\\mathbf{x}$ from measurements $\\mathbf{y} = \\mathbf{A}\\mathbf{x}$, provided the sparsity $s$ satisfies the condition:\n    $$\n    s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)\n    $$\n    We calculate the largest integer sparsity level, $s_{\\text{OMP}}$, for which this guarantee holds. This is the largest integer strictly smaller than the bound:\n    $$\n    s_{\\text{OMP}} = \\left\\lceil \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right) \\right\\rceil - 1\n    $$\n\n**3. Greedy Column Pruning**\n\nThe procedure involves removing $k$ columns from $\\mathbf{A}$ using a greedy strategy. The goal is to remove columns that are \"most deleterious to coherence\". This is interpreted as removing the column that has the highest total absolute correlation with all other columns. At each of the $k$ pruning steps, we perform the following:\n1.  For the current matrix with columns $\\{\\mathbf{a}_j\\}$, compute the \"coherence contribution\" for each column $j$: $c_j = \\sum_{i \\neq j} |\\langle \\mathbf{a}_i, \\mathbf{a}_j \\rangle|$.\n2.  Identify the column index $j^*$ that maximizes this sum: $j^* = \\arg\\max_j c_j$.\n3.  Remove column $\\mathbf{a}_{j^*}$ from the matrix.\n\nThis process is repeated $k$ times, reducing the number of columns from $n$ to $n' = n-k$. Let the resulting pruned matrix be denoted $\\mathbf{A}'$.\n\n**4. Post-Pruning Analysis**\n\nAfter pruning, we obtain the matrix $\\mathbf{A}' \\in \\mathbb{R}^{m \\times n'}$. All the quantities computed in Step 2 are re-evaluated for this new matrix:\n*   Post-pruning mutual coherence, $\\mu' = \\mu(\\mathbf{A}')$.\n*   Post-pruning Welch bound, $\\mu'_W$, calculated with dimension $n'$.\n*   Post-pruning Gershgorin-based bound on $\\delta_s$, which is $\\delta'_{s, \\text{Gersh}} = (s-1)\\mu'$.\n*   Post-pruning largest OMP-guaranteed sparsity, $s'_{\\text{OMP}}$, calculated using $\\mu'$.\n\n**5. OMP Guarantee Decision**\n\nFinally, for the prescribed fixed sparsity level $s$, we determine whether the OMP success guarantee is met both before and after pruning. This involves two boolean checks:\n1.  **Before pruning:** Is $s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu}\\right)$?\n2.  **After pruning:** Is $s  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu'}\\right)$?\n\nFor each test case, these steps generate the required ten output values: $\\mu$, $\\mu'$, $\\delta_{s, \\text{Gersh}}$, $\\delta'_{s, \\text{Gersh}}$, $s_{\\text{OMP}}$, $s'_{\\text{OMP}}$, the two booleans, $\\mu_W$, and $\\mu'_W$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute the analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (m, n, k, s, seed)\n        (30, 60, 10, 4, 1),\n        (40, 80, 0, 6, 2),\n        (20, 100, 40, 8, 3),\n        (50, 55, 4, 3, 4),\n        (30, 90, 20, 10, 5),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        m, n, k, s, seed = case\n        result = process_case(m, n, k, s, seed)\n        all_results.append(result)\n\n    # Format the final output string as a list of lists, with no spaces.\n    results_as_strings = [str(r).replace(\" \", \"\") for r in all_results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\ndef process_case(m, n, k, s, seed):\n    \"\"\"\n    Processes a single test case according to the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct the sensing matrix\n    A = rng.standard_normal((m, n))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n\n    # 2. Pre-pruning analysis\n    mu_pre, delta_s_bound_pre, s_omp_pre, omp_guarantee_pre, welch_pre = analyze_matrix(A, s)\n\n    # 4. Perform column pruning\n    A_pruned = A.copy()\n    for _ in range(k):\n        # Ensure there are columns to prune\n        if A_pruned.shape[1] = 1:\n            break\n        \n        # Compute Gram matrix for the current matrix\n        G_prune = A_pruned.T @ A_pruned\n        np.fill_diagonal(G_prune, 0)\n        \n        # Find the column that maximizes the sum of absolute inner products\n        # with other columns.\n        coherence_sums = np.sum(np.abs(G_prune), axis=1)\n        idx_to_remove = np.argmax(coherence_sums)\n        \n        # Remove the identified column\n        A_pruned = np.delete(A_pruned, idx_to_remove, axis=1)\n    \n    # 5. Post-pruning analysis\n    if k == 0:\n        # If no pruning was done, post-pruning metrics are identical to pre-pruning.\n        mu_post = mu_pre\n        delta_s_bound_post = delta_s_bound_pre\n        s_omp_post = s_omp_pre\n        omp_guarantee_post = omp_guarantee_pre\n        welch_post = welch_pre\n    else:\n        mu_post, delta_s_bound_post, s_omp_post, omp_guarantee_post, welch_post = analyze_matrix(A_pruned, s)\n\n    # 6. Format and return results\n    return [\n        round(mu_pre, 6),\n        round(mu_post, 6),\n        round(delta_s_bound_pre, 6),\n        round(delta_s_bound_post, 6),\n        int(s_omp_pre),\n        int(s_omp_post),\n        omp_guarantee_pre,\n        omp_guarantee_post,\n        round(welch_pre, 6),\n        round(welch_post, 6)\n    ]\n\ndef analyze_matrix(A, s):\n    \"\"\"\n    Computes all required metrics for a given sensing matrix A and sparsity s.\n    \"\"\"\n    m, n = A.shape\n\n    if n = 1:\n        # Trivial case with 0 or 1 columns: coherence is 0.\n        mu = 0.0\n        delta_s_bound = 0.0\n        # OMP bound is infinite, so s_omp is effectively infinite. Use a large integer.\n        s_omp = 2**31 - 1\n        omp_guarantee = True\n        welch = 0.0\n        return mu, delta_s_bound, s_omp, omp_guarantee, welch\n\n    # Compute Gram matrix and mutual coherence\n    G = A.T @ A\n    np.fill_diagonal(G, 0)\n    mu = np.max(np.abs(G))\n\n    # Welch bound\n    if n  m:\n        welch = np.sqrt((n - m) / (m * (n - 1)))\n    else:\n        welch = 0.0\n\n    # Gershgorin-based upper bound on RIC\n    delta_s_bound = (s - 1) * mu\n\n    # OMP success guarantee\n    if mu == 0:\n        s_omp = 2**31 - 1\n        omp_guarantee = True\n    else:\n        omp_bound = 0.5 * (1 + 1 / mu)\n        # s_OMP is the largest integer s' such that s'  omp_bound.\n        s_omp = np.ceil(omp_bound) - 1\n        # Check if the specific sparsity s meets the guarantee.\n        omp_guarantee = (s  omp_bound)\n    \n    return mu, delta_s_bound, s_omp, omp_guarantee, welch\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}