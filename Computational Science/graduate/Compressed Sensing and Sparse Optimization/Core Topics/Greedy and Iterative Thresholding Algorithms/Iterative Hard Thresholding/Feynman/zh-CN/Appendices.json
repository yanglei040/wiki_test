{
    "hands_on_practices": [
        {
            "introduction": "理解任何迭代算法的第一步都是掌握其核心的单步更新机制。本练习将迭代硬阈值（IHT）算法分解为几个可操作的计算步骤：计算梯度、根据Lipschitz常数确定步长、执行梯度下降，以及最后应用硬阈值算子。通过手动完成一个具体的数值示例（），您将能够巩固对IHT每个组成部分的理解，为后续更复杂的分析和实现打下坚实的基础。",
            "id": "3454156",
            "problem": "考虑由 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 给出的带有最小二乘数据保真项的稀疏恢复目标，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $y \\in \\mathbb{R}^{m}$。从复合函数的梯度和二次目标函数梯度的范数诱导 Lipschitz 连续性的核心定义出发，您将对一个具体实例执行一步迭代硬阈值（Iterative Hard Thresholding, IHT）算法。\n\n设 $A \\in \\mathbb{R}^{2 \\times 3}$ 和 $y \\in \\mathbb{R}^{2}$ 定义如下：\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}.\n$$\n设初始迭代点为 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{3}$，稀疏度为 $k = 1$。IHT 步骤包含一个梯度下降步骤，其步长 $ \\mu $ 使用 $f(x)$ 梯度的 Lipschitz 常数来选择，然后应用 $k$-硬阈值算子 $H_{k}$，该算子保留幅值最大的 $k$ 个分量，并将其余分量置零。\n\n任务：\n- 从 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 出发，使用基本原理推导梯度 $\\nabla f(x)$ 并计算 $\\nabla f(x^{(0)})$。\n- 根据 $A$ 的结构确定 $\\nabla f$ 的 Lipschitz 常数 $L$，并选择步长 $\\mu = \\frac{1}{L}$。\n- 执行一次 IHT 迭代 $x^{(1)} = H_{k}\\!\\left(x^{(0)} - \\mu \\nabla f(x^{(0)})\\right)$。\n- 计算标量目标函数值 $f\\!\\left(x^{(1)}\\right)$。\n\n请以一个精确的实数形式提供最终答案。无需四舍五入。",
            "solution": "该问题是有效的，因为它在稀疏优化领域有科学依据，所有必要信息均已提供故而是适定的，并且陈述客观。我们开始求解。\n\n目标函数由 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 给出，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$y \\in \\mathbb{R}^{m}$。使用欧几里得范数的定义，这可以重写为 $f(x) = \\frac{1}{2}(Ax-y)^{T}(Ax-y)$。\n\n首先，我们推导梯度 $\\nabla f(x)$。展开 $f(x)$ 的表达式：\n$$f(x) = \\frac{1}{2}(x^{T}A^{T} - y^{T})(Ax - y) = \\frac{1}{2}(x^{T}A^{T}Ax - x^{T}A^{T}y - y^{T}Ax + y^{T}y)$$\n标量项 $y^{T}Ax$ 等于其转置 $(y^{T}Ax)^{T} = x^{T}A^{T}y$。因此，表达式简化为：\n$$f(x) = \\frac{1}{2}(x^{T}A^{T}Ax - 2y^{T}Ax + y^{T}y)$$\n为了求梯度 $\\nabla f(x)$，我们对向量 $x$ 求导。使用标准的矩阵微积分恒等式 $\\nabla_{x}(x^{T}Bx) = (B+B^{T})x$（对于方阵 $B$）和 $\\nabla_{x}(c^{T}x) = c$（对于向量 $c$），我们继续。矩阵 $A^{T}A$ 是对称的，所以 $\\nabla_{x}(x^{T}A^{T}Ax) = 2A^{T}Ax$。项 $y^{T}Ax$ 可以重写为 $(A^{T}y)^{T}x$，其梯度为 $A^{T}y$。项 $y^{T}y$ 相对于 $x$ 是常数。\n综合这些结果，梯度为：\n$$\\nabla f(x) = \\frac{1}{2}(2A^{T}Ax - 2A^{T}y) = A^{T}(Ax - y)$$\n问题给出了 $A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}$，$y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及初始迭代点 $x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。我们计算在 $x^{(0)}$ 处的梯度：\n$$\\nabla f(x^{(0)}) = A^{T}(Ax^{(0)} - y) = A^{T}(A \\cdot 0 - y) = -A^{T}y$$\n$A$ 的转置是 $A^{T} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix}$。\n$$\\nabla f(x^{(0)}) = -\\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = -\\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 2 \\\\ 0 \\cdot 1 + 1 \\cdot 2 \\\\ 1 \\cdot 1 + 1 \\cdot 2 \\end{pmatrix} = -\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -2 \\\\ -3 \\end{pmatrix}$$\n\n其次，我们确定 $\\nabla f(x)$ 的 Lipschitz 常数 $L$。梯度为 $\\nabla f(x) = A^{T}Ax - A^{T}y$。$f(x)$ 的 Hessian 矩阵是 $\\nabla^{2}f(x) = A^{T}A$。$\\nabla f(x)$ 的 Lipschitz 常数 $L$ 是其 Hessian 矩阵的谱范数，$L = \\|A^{T}A\\|_{2}$。对于像 $A^{T}A$ 这样的半正定矩阵，谱范数是其最大特征值，$L = \\lambda_{\\max}(A^{T}A)$。我们计算 $A^{T}A$：\n$$A^{T}A = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{pmatrix}$$\n为了求特征值，我们求解特征方程 $\\det(A^{T}A - \\lambda I) = 0$：\n$$\\det\\begin{pmatrix} 1-\\lambda  0  1 \\\\ 0  1-\\lambda  1 \\\\ 1  1  2-\\lambda \\end{pmatrix} = 0$$\n沿第一行进行代数余子式展开：\n$$(1-\\lambda) \\det\\begin{pmatrix} 1-\\lambda  1 \\\\ 1  2-\\lambda \\end{pmatrix} + 1 \\cdot \\det\\begin{pmatrix} 0  1-\\lambda \\\\ 1  1 \\end{pmatrix} = 0$$\n$$(1-\\lambda)((1-\\lambda)(2-\\lambda) - 1) + (-(1-\\lambda)) = 0$$\n$$(1-\\lambda)[(1-\\lambda)(2-\\lambda) - 1 - 1] = 0$$\n$$(1-\\lambda)(\\lambda^{2} - 3\\lambda + 2 - 2) = 0$$\n$$(1-\\lambda)(\\lambda^{2} - 3\\lambda) = 0$$\n$$\\lambda(1-\\lambda)(\\lambda - 3) = 0$$\n特征值为 $\\lambda \\in \\{0, 1, 3\\}$。最大特征值为 $\\lambda_{\\max} = 3$。因此，Lipschitz 常数为 $L = 3$。IHT 算法的步长选择为 $\\mu = \\frac{1}{L} = \\frac{1}{3}$。\n\n第三，我们执行一次 IHT 迭代。更新规则为 $x^{(1)} = H_{k}(x^{(0)} - \\mu \\nabla f(x^{(0)}))$，稀疏度为 $k=1$。首先，我们计算阈值算子内部的向量：\n$$v = x^{(0)} - \\mu \\nabla f(x^{(0)}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} -1 \\\\ -2 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 1 \\end{pmatrix}$$\n当 $k=1$ 时，硬阈值算子 $H_{k}(v)$ 保留 $v$ 中绝对值最大的单个分量，并将其余所有分量置零。$v$ 的各分量的绝对值为 $|1/3| = 1/3$，$|2/3| = 2/3$，和 $|1|=1$。最大绝对值为 $1$，对应于第三个分量。\n$$x^{(1)} = H_{1}(v) = H_{1}\\left(\\begin{pmatrix} 1/3 \\\\ 2/3 \\\\ 1 \\end{pmatrix}\\right) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n\n最后，我们计算这个新迭代点 $x^{(1)}$ 的目标函数值 $f(x^{(1)})$。\n$$f(x^{(1)}) = \\frac{1}{2}\\|Ax^{(1)} - y\\|_{2}^{2}$$\n我们计算 $Ax^{(1)}$ 项：\n$$Ax^{(1)} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\n接下来，我们计算残差向量 $Ax^{(1)} - y$：\n$$Ax^{(1)} - y = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$$\n该残差的 $L_2$ 范数平方为：\n$$\\|Ax^{(1)} - y\\|_{2}^{2} = 0^{2} + (-1)^{2} = 1$$\n将此代回 $f(x^{(1)})$ 的表达式中：\n$$f(x^{(1)}) = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}$$",
            "answer": "$$\\boxed{\\frac{1}{2}}$$"
        },
        {
            "introduction": "尽管迭代硬阈值算法在概念上很直观，但其收敛性并非对任意参数选择都成立。本练习（）聚焦于最关键的参数之一：步长 $\\mu$。您将通过代码构建一个具体的反例，亲眼见证一个过大的步长如何违反与测量矩阵谱特性相关的稳定性条件，并最终导致算法即使在已知确切稀疏度的情况下也无法成功恢复信号，从而深刻理解步长选择在算法收敛中的决定性作用。",
            "id": "3454165",
            "problem": "考虑压缩感知中的典范线性测量模型，其中测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个 $k$-稀疏信号 $x^\\star \\in \\mathbb{R}^n$ 生成测量值 $y = A x^\\star$。迭代硬阈值（IHT）在此定义为最小二乘数据保真度的梯度下降与硬阈值算子的组合，通过以下递推寻求 $x^\\star$ 的一个 $k$-稀疏估计\n$$\nx^{t+1} = H_k\\!\\left(x^t + \\mu A^\\top (y - A x^t)\\right),\n$$\n其中 $H_k(\\cdot)$ 表示硬阈值算子，它保留模最大的 $k$ 个分量并将余下分量置零，而 $\\mu > 0$ 是一个步长参数。在本问题中，你将构建并分析一个反例，其中尽管已知正确的 $k$，但由于步长 $\\mu$ 过于激进，IHT 未能恢复真实的 $k$-稀疏解。你将通过分析 $A^\\top A$ 限制在活动支撑集上的谱来刻画收敛与发散之间的边界。\n\n从基础出发：最小二乘数据保真度 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$，其梯度 $\\nabla f(x) = A^\\top (A x - y)$，硬阈值算子 $H_k$，向量的支撑集 $\\mathrm{supp}(x)$ 的定义，矩阵的谱半径，以及对称半正定矩阵的特征值。活动支撑集指的是在某次迭代中被 $H_k$ 选中的索引集。分析必须是纯数学的。\n\n构建以下具体实例：\n- 维度：$m = 4$，$n = 5$，$k = 2$。\n- 定义标准正交基向量 $e_1, e_2, e_3, e_4 \\in \\mathbb{R}^4$。\n- 定义 $A$ 的列如下：\n  - $a_1 = e_1$，\n  - $a_2 = c \\, e_1 + \\sqrt{1 - c^2}\\, e_2$，其中 $c = \\frac{9}{10}$，\n  - $a_3 = \\beta \\, e_1 + \\gamma \\, e_2 + \\sqrt{1 - \\beta^2 - \\gamma^2}\\, e_3$，其中 $\\beta = \\frac{3}{5}$ 且 $\\gamma = \\frac{1}{5}$，\n  - $a_4 = \\alpha_1 \\, e_1 - \\alpha_2 \\, e_2 + \\alpha_3 \\, e_3 + \\sqrt{1 - \\alpha_1^2 - \\alpha_2^2 - \\alpha_3^2}\\, e_4$，其中 $\\alpha_1 = \\frac{1}{10}$，$\\alpha_2 = \\frac{3}{10}$，$\\alpha_3 = \\frac{1}{5}$，\n  - $a_5 = \\delta_1 \\, e_1 + \\delta_2 \\, e_2 - \\delta_3 \\, e_3 + \\sqrt{1 - \\delta_1^2 - \\delta_2^2 - \\delta_3^2}\\, e_4$，其中 $\\delta_1 = \\frac{1}{5}$，$\\delta_2 = \\frac{1}{2}$，$\\delta_3 = \\frac{1}{5}$。\n- 令 $A = [a_1, a_2, a_3, a_4, a_5] \\in \\mathbb{R}^{4 \\times 5}$ 且 $x^\\star = [1, 1, 0, 0, 0]^\\top \\in \\mathbb{R}^5$，因此真实支撑集为 $S^\\star = \\{1, 2\\}$（为清晰描述，使用基于1的索引；实现中可使用基于0的索引）。\n- 令 $y = A x^\\star$。\n\n定义 IHT 算法，初始化为 $x^0 = 0$ 且 $k = 2$。用于收敛性分析的关键限制矩阵是 $A_{S^\\star}^\\top A_{S^\\star} \\in \\mathbb{R}^{2 \\times 2}$，其中 $A_{S^\\star}$ 表示由 $A$ 中 $S^\\star$ 索引的列构成的子矩阵。令 $\\lambda_{\\max}$ 为 $A_{S^\\star}^\\top A_{S^\\star}$ 的最大特征值。对于限制在 $S^\\star$ 上的梯度下降动态（即，假设活动支撑集保持为 $S^\\star$），线性化误差更新由矩阵 $I - \\mu A_{S^\\star}^\\top A_{S^\\star}$ 控制，而欧几里得范数下收敛的精确稳定性边界由关于谱半径的条件 $\\rho(I - \\mu A_{S^\\star}^\\top A_{S^\\star})  1$ 给出，这等价于 $\\mu \\in (0, 2/\\lambda_{\\max})$。\n\n你必须通过实验证明一个反例：使用正确的 $k=2$ 以及构建的 $A$ 和 $x^\\star$，选择一个超出稳定性边界的足够激进的步长 $\\mu$，运行 IHT 固定次数的迭代，并展示其未能恢复 $x^\\star$（不正确的支撑集和/或较大的估计误差）。并将其与严格低于边界和接近边界的步长进行比较。\n\n测试套件：\n- 从 $A_{S^\\star}^\\top A_{S^\\star}$ 计算 $\\lambda_{\\max}$ 并定义 $\\mu_\\mathrm{crit} = \\frac{2}{\\lambda_{\\max}}$。\n- 使用以下四个步长：\n  1. $\\mu_1 = \\frac{0.5}{\\lambda_{\\max}}$（远低于保守界限），\n  2. $\\mu_2 = 0.99 \\, \\mu_\\mathrm{crit}$（刚好在精确线性稳定性边界之下），\n  3. $\\mu_3 = 1.20 \\, \\mu_\\mathrm{crit}$（超出边界，预期会发散），\n  4. $\\mu_4 = 10$（远超边界，预期会明确失败）。\n- 使用固定的迭代上限 $T = 200$ 和容差 $\\varepsilon = 10^{-6}$ 来判断是否成功恢复，其中成功定义为 $\\mathrm{supp}(x^{T}) = S^\\star$ 且 $\\|x^{T} - x^\\star\\|_2 \\le \\varepsilon$。\n\n对于每个步长 $\\mu_i$，计算并记录：\n- 一个布尔值，指示 IHT 是否在 $T$ 次迭代内，在上述成功标准下成功恢复了 $x^\\star$。\n- 谱半径 $\\rho\\!\\left(I - \\mu_i A_{S^\\star}^\\top A_{S^\\star}\\right)$。\n\n你的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表。该列表必须按如下顺序排列：\n$$\n[\\mu_\\mathrm{crit},\\, \\text{success}(\\mu_1),\\, \\rho(I - \\mu_1 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_2),\\, \\rho(I - \\mu_2 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_3),\\, \\rho(I - \\mu_3 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_4),\\, \\rho(I - \\mu_4 A_{S^\\star}^\\top A_{S^\\star}) ]。\n$$\n不涉及物理单位。不涉及角度。不得使用百分比；所有标量必须是小数或整数。\n\n从第一性原理实现算法逻辑，仅使用提供的矩阵 $A$、向量 $x^\\star$ 和上述指定的步长。通过验证列归一化以及通过标准线性代数运算计算特征值和谱半径来确保科学真实性。最终输出必须严格遵循上述格式。",
            "solution": "我们从最小二乘公式 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_2^2$ 开始，其梯度为 $\\nabla f(x) = A^\\top (A x - y)$。迭代硬阈值（IHT）结合了梯度下降步骤和对迭代点的硬阈值操作以强制实现 $k$-稀疏性：\n$$\nx^{t+1} = H_k\\!\\left(x^t - \\mu \\nabla f(x^t)\\right) \n= H_k\\!\\left(x^t + \\mu A^\\top (y - A x^t)\\right),\n$$\n其中 $H_k(\\cdot)$ 选出模最大的 $k$ 个分量并将余下分量置零。我们使用的核心定义和性质如下：\n- 向量 $x$ 的支撑集，记作 $\\mathrm{supp}(x)$，是其非零分量的索引集合。\n- 硬阈值算子 $H_k$ 精确保留 $k$ 个绝对值最大的分量。\n- 对于一个对称半正定矩阵 $M$，其特征值为实数且非负，其谱半径 $\\rho(M)$ 等于其模最大的特征值。\n- 活动支撑集是在某次迭代中被 $H_k$ 选中的索引集合。\n\n由于非线性算子 $H_k$ 的存在，IHT 的分析变得很微妙。一种标准的分析方法是在活动支撑集被正确维持这一简化假设下，分离出算法在真实支撑集 $S^\\star$ 上的行为。在这个固定的支撑集上，最小二乘梯度动态退化为经典的线性迭代。令 $A_{S^\\star}$ 表示由 $A$ 中 $S^\\star$ 索引的列构成的子矩阵。如果 $x^t$ 是 $k$-稀疏的且 $\\mathrm{supp}(x^t) = S^\\star$，并且阈值操作维持了这个支撑集，那么在 $S^\\star$ 上的限制动态遵循\n$$\nx_{S^\\star}^{t+1} = x_{S^\\star}^{t} + \\mu A_{S^\\star}^\\top \\left(y - A_{S^\\star} x_{S^\\star}^{t}\\right).\n$$\n设误差为 $e^{t} = x_{S^\\star}^{t} - x^\\star_{S^\\star}$。利用 $y = A_{S^\\star} x^\\star_{S^\\star}$，我们有\n$$\ne^{t+1} = x_{S^\\star}^{t+1} - x^\\star_{S^\\star}\n= x_{S^\\star}^{t} + \\mu A_{S^\\star}^\\top \\left(A_{S^\\star} x^\\star_{S^\\star} - A_{S^\\star} x_{S^\\star}^{t}\\right) - x^\\star_{S^\\star}\n= \\left(I - \\mu A_{S^\\star}^\\top A_{S^\\star}\\right) e^{t}.\n$$\n因此，在正确的支撑集上，误差更新是线性的，迭代矩阵为 $I - \\mu A_{S^\\star}^\\top A_{S^\\star}$。当且仅当谱半径满足以下条件时，在欧几里得范数下收敛：\n$$\n\\rho\\!\\left(I - \\mu A_{S^\\star}^\\top A_{S^\\star}\\right)  1.\n$$\n令 $A_{S^\\star}^\\top A_{S^\\star}$ 的特征值为 $\\{\\lambda_i\\}_{i=1}^k$，其中 $\\lambda_{\\max} = \\max_i \\lambda_i$。那么 $I - \\mu A_{S^\\star}^\\top A_{S^\\star}$ 的特征值为 $\\{1 - \\mu \\lambda_i\\}_{i=1}^k$，其谱半径为\n$$\n\\rho\\!\\left(I - \\mu A_{S^\\star}^\\top A_{S^\\star}\\right) = \\max_i |1 - \\mu \\lambda_i|.\n$$\n因此，非扩张行为的精确线性稳定性边界为\n$$\n\\mu \\in (0, 2 / \\lambda_{\\max}), \\quad \\text{其中} \\quad \\mu_\\mathrm{crit} = 2/\\lambda_{\\max}.\n$$\n对于 $\\mu \\in (0, 2/\\lambda_{\\max})$，受限的线性误差会收缩（如果 $1 - \\mu \\lambda_i  0$ 可能伴随振荡），而对于 $\\mu > 2/\\lambda_{\\max}$，受限的误差会发散。在固定支撑集动态下，此条件是确保在真实支撑集上收缩的充分必要条件。\n\n然而，IHT 在每次迭代中都会应用 $H_k$。阈值操作可能会因 $A^\\top$ 作用于残差 $y - A x^t$ 所引起的过冲和串扰而改变支撑集。即使第一次阈值操作选择了正确的支撑集（这取决于 $A^\\top y$ 在不同索引上的相对大小），一个过于激进的 $\\mu$ 也可能导致在 $S^\\star$ 上产生大的振荡和误差放大。随着误差增长，残差 $r^t = y - A x^t$ 也会增长，而 $A^\\top r^t$ 可能会产生大的支撑集外分量。最终，硬阈值操作可能会将活动支撑集切换出 $S^\\star$，导致无法恢复 $x^\\star$。\n\n我们现在构建具体的反例。令 $m = 4$，$n = 5$，$k = 2$。如下定义 $A$ 在 $\\mathbb{R}^4$ 中的列。令 $e_1, e_2, e_3, e_4$ 为标准正交基。选择 $c = \\frac{9}{10}$，$\\beta = \\frac{3}{5}$，$\\gamma = \\frac{1}{5}$，$\\alpha_1 = \\frac{1}{10}$，$\\alpha_2 = \\frac{3}{10}$，$\\alpha_3 = \\frac{1}{5}$，$\\delta_1 = \\frac{1}{5}$，$\\delta_2 = \\frac{1}{2}$，$\\delta_3 = \\frac{1}{5}$。设\n$$\na_1 = e_1,\\quad \na_2 = c e_1 + \\sqrt{1 - c^2} \\, e_2,\\quad\na_3 = \\beta e_1 + \\gamma e_2 + \\sqrt{1 - \\beta^2 - \\gamma^2} \\, e_3,\n$$\n$$\na_4 = \\alpha_1 e_1 - \\alpha_2 e_2 + \\alpha_3 e_3 + \\sqrt{1 - \\alpha_1^2 - \\alpha_2^2 - \\alpha_3^2} \\, e_4,\\quad\na_5 = \\delta_1 e_1 + \\delta_2 e_2 - \\delta_3 e_3 + \\sqrt{1 - \\delta_1^2 - \\delta_2^2 - \\delta_3^2} \\, e_4.\n$$\n根据构造，这些列是单位范数的。令 $A = [a_1, a_2, a_3, a_4, a_5]$。选择真实的稀疏信号\n$$\nx^\\star = [1, 1, 0, 0, 0]^\\top,\n$$\n其真实支撑集为 $S^\\star = \\{1, 2\\}$，并令\n$$\ny = A x^\\star = a_1 + a_2.\n$$\n注意 $A_{S^\\star}^\\top A_{S^\\star} = \\begin{bmatrix} 1  c \\\\ c  1 \\end{bmatrix}$。其特征值为 $\\lambda_1 = 1 + c$ 和 $\\lambda_2 = 1 - c$，因此 $\\lambda_{\\max} = 1 + c = \\frac{19}{10}$。从而，受限动态的精确线性稳定性边界为\n$$\n\\mu_\\mathrm{crit} = \\frac{2}{\\lambda_{\\max}} = \\frac{2}{1 + c} = \\frac{20}{19}.\n$$\n我们考虑四个步长：\n$$\n\\mu_1 = \\frac{0.5}{\\lambda_{\\max}},\\quad \\mu_2 = 0.99 \\, \\mu_\\mathrm{crit},\\quad \\mu_3 = 1.20 \\, \\mu_\\mathrm{crit},\\quad \\mu_4 = 10.\n$$\n我们初始化 $x^0 = 0$ 并应用 IHT 迭代 $T = 200$ 次，每次迭代使用更新 $x^{t+1} = H_k(x^t + \\mu_i A^\\top (y - A x^t))$。如果 $\\mathrm{supp}(x^{T}) = S^\\star$ 且 $\\|x^{T} - x^\\star\\|_2 \\le \\varepsilon$（其中 $\\varepsilon = 10^{-6}$），则宣告成功。\n\n从第一性原理，我们可以预测：\n- 对于 $\\mu_1$ 和 $\\mu_2$，受限动态满足 $\\rho(I - \\mu A_{S^\\star}^\\top A_{S^\\star})  1$，确保在 $S^\\star$ 上的收缩。只要阈值操作能保持正确的支撑集（在这里初始时是这样的，因为 $A^\\top y$ 的两个最大模分量在 $S^\\star$ 上），我们预期算法会收敛。\n- 对于 $\\mu_3$ 和 $\\mu_4$，我们有 $\\rho(I - \\mu A_{S^\\star}^\\top A_{S^\\star}) > 1$，导致在 $S^\\star$ 上发散。误差会增长，并通过 $A^\\top (y - A x^t)$ 引入大的支撑集外分量，导致阈值操作最终切换支撑集，从而无法恢复 $x^\\star$。\n\n程序将计算 $\\lambda_{\\max}$、$\\mu_\\mathrm{crit}$，对每个 $\\mu_i$ 运行 IHT，计算谱半径 $\\rho(I - \\mu_i A_{S^\\star}^\\top A_{S^\\star}) = \\max\\{|1 - \\mu_i \\lambda_1|,\\, |1 - \\mu_i \\lambda_2|\\}$，并生成最终的单行聚合输出：\n$$\n[\\mu_\\mathrm{crit},\\, \\text{success}(\\mu_1),\\, \\rho(I - \\mu_1 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_2),\\, \\rho(I - \\mu_2 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_3),\\, \\rho(I - \\mu_3 A_{S^\\star}^\\top A_{S^\\star}),\\, \\text{success}(\\mu_4),\\, \\rho(I - \\mu_4 A_{S^\\star}^\\top A_{S^\\star}) ]。\n$$\n这展示了该反例，并通过限制在活动支撑集上的 $A^\\top A$ 的谱，刻画了收敛与发散之间的边界。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hard_threshold(x, k):\n    \"\"\"Keep k largest-magnitude entries of x, zero out the rest.\"\"\"\n    if k >= x.size:\n        return x.copy()\n    idx = np.argsort(np.abs(x))[::-1]\n    keep = idx[:k]\n    x_ht = np.zeros_like(x)\n    x_ht[keep] = x[keep]\n    return x_ht\n\ndef iht(A, y, k, mu, max_iters=200, tol=1e-6):\n    \"\"\"Iterative Hard Thresholding with fixed step size mu.\"\"\"\n    x = np.zeros(A.shape[1])\n    for _ in range(max_iters):\n        grad = A.T @ (y - A @ x)\n        z = x + mu * grad\n        x_new = hard_threshold(z, k)\n        # Early stop if converged\n        if np.linalg.norm(x_new - x) = 1e-12:\n            x = x_new\n            break\n        x = x_new\n    return x\n\ndef spectral_radius_restricted(A_S, mu):\n    \"\"\"Compute spectral radius of I - mu * (A_S^T A_S).\"\"\"\n    M = A_S.T @ A_S\n    evals = np.linalg.eigvalsh(M)\n    # eigenvalues of I - mu M are 1 - mu*lambda_i\n    vals = 1.0 - mu * evals\n    return float(np.max(np.abs(vals)))\n\ndef construct_matrix():\n    \"\"\"Construct the specific A as per problem statement.\"\"\"\n    # Basis vectors e1..e4\n    e1 = np.array([1.0, 0.0, 0.0, 0.0])\n    e2 = np.array([0.0, 1.0, 0.0, 0.0])\n    e3 = np.array([0.0, 0.0, 1.0, 0.0])\n    e4 = np.array([0.0, 0.0, 0.0, 1.0])\n\n    c = 0.9\n    beta = 0.6\n    gamma = 0.2\n    alpha1, alpha2, alpha3 = 0.1, 0.3, 0.2\n    delta1, delta2, delta3 = 0.2, 0.5, 0.2\n\n    a1 = e1\n    a2 = c * e1 + np.sqrt(1 - c**2) * e2\n    a3 = beta * e1 + gamma * e2 + np.sqrt(1 - beta**2 - gamma**2) * e3\n    a4 = alpha1 * e1 - alpha2 * e2 + alpha3 * e3 + np.sqrt(1 - alpha1**2 - alpha2**2 - alpha3**2) * e4\n    a5 = delta1 * e1 + delta2 * e2 - delta3 * e3 + np.sqrt(1 - delta1**2 - delta2**2 - delta3**2) * e4\n\n    A = np.column_stack([a1, a2, a3, a4, a5])\n    # Normalize columns (should already be unit-norm, but enforce numerically)\n    A = A / np.linalg.norm(A, axis=0, keepdims=True)\n    return A\n\ndef solve():\n    # Construct A and true signal\n    A = construct_matrix()\n    n = A.shape[1]\n    k = 2\n    # True support S* = {0, 1} in 0-based indexing\n    S_star = np.array([0, 1], dtype=int)\n    x_star = np.zeros(n)\n    x_star[S_star] = 1.0\n    y = A @ x_star\n\n    # Restricted matrix and stability boundary\n    A_S = A[:, S_star]\n    M = A_S.T @ A_S\n    eigs = np.linalg.eigvalsh(M)\n    lambda_max = float(np.max(eigs))\n    mu_crit = 2.0 / lambda_max\n\n    # Define test cases: step sizes\n    mu1 = 0.5 / lambda_max\n    mu2 = 0.99 * mu_crit\n    mu3 = 1.20 * mu_crit\n    mu4 = 10.0\n\n    test_cases = [mu1, mu2, mu3, mu4]\n\n    results = []\n    # First append mu_crit\n    results.append(mu_crit)\n\n    # Run IHT for each case\n    max_iters = 200\n    tol = 1e-6\n    for mu in test_cases:\n        x_est = iht(A, y, k, mu, max_iters=max_iters, tol=tol)\n        support_est = np.flatnonzero(np.abs(x_est) > 0)\n        # Check success: exact support and small error\n        success = (set(support_est.tolist()) == set(S_star.tolist())) and (np.linalg.norm(x_est - x_star) = tol)\n        rho = spectral_radius_restricted(A_S, mu)\n        results.append(success)\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为了将IHT算法置于稀疏恢复算法的更广阔背景下，本练习（）要求您并排实现IHT及其基于凸松弛的“近亲”——迭代软阈值算法（ISTA）。IHT直接处理非凸的 $\\ell_{0}$ 范数约束，而ISTA则求解其凸替代的 $\\ell_{1}$ 范数正则化问题。通过这个编码实践，您将直接比较两种算法在恢复信号时的动态轨迹和最终结果，揭示硬阈值的“贪婪”选择与软阈值的“收缩”效应之间的本质差异和实践权衡。",
            "id": "3454136",
            "problem": "构建一个完整的、可运行的程序，为线性模型 $y = A x_{\\star} + e$ 实现并比较两种迭代稀疏恢复算法。其中，$A \\in \\mathbb{R}^{m \\times n}$，$x_{\\star} \\in \\mathbb{R}^{n}$ 是一个 k-稀疏向量，$e \\in \\mathbb{R}^{m}$ 是加性噪声。您的程序必须实现迭代硬阈值算法 (Iterative Hard Thresholding, IHT) 以近似求解约束最小二乘问题 $\\min_{x} \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2}$ subject to $\\lVert x \\rVert_{0} \\le k$，以及迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA) 以近似求解其凸松弛问题 $\\min_{x} \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}$。请从以下基本原理出发，推导您所实现的迭代更新：平滑数据保真项的梯度为 $\\nabla f(x) = A^{\\top}(A x - y)$，k-稀疏向量的集合是所有 k-维坐标子空间的并集，$\\ell_{1}$-范数的邻近算子是逐坐标的软阈值操作。使用恒定步长 $\\mu$，其选择需满足标准的邻近梯度稳定性条件 $\\mu \\le 1 / L$，其中 $L$ 是数据保真项梯度的 Lipschitz 常数，且 $L = \\lVert A \\rVert_{2}^{2}$。\n\n您的程序必须：\n- 每个测试使用固定的随机数生成器种子，生成测量矩阵 $A$ 和真实信号 $x_{\\star}$。$A$ 的元素 $A_{ij}$ 独立同分布于 $\\mathcal{N}(0, 1/m)$，$x_{\\star}$ 的 $k$ 个非零元素独立抽样于 $\\mathcal{N}(0,1)$。\n- 生成测量值 $y = A x_{\\star} + e$，其中噪声项 $e_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$。\n- 将两种算法的初始值均设为 $x^{(0)} = 0$。\n- 对于 IHT，在每次迭代中，执行一次梯度步长，然后通过保留 $k$ 个最大幅值的坐标（如果需要，按索引顺序确定性地处理幅值相等的情况）投影到 k-稀疏向量集合上。\n- 对于 ISTA，在每次迭代中，执行一次梯度步长，然后进行逐坐标的软阈值操作，阈值与 $\\lambda \\mu$ 成正比；使用固定的正则化参数 $\\lambda = \\alpha \\lVert A^{\\top} y \\rVert_{\\infty}$，其中 $\\alpha \\in (0,1)$ 是一个给定的标量。\n- 对每个测试，将两种算法都运行 $T$ 次迭代。\n- 记录支撑集轨迹，即每次迭代时非零坐标的索引集序列。\n- 对每个测试报告：IHT 和 ISTA 的支撑集轨迹在任何一次迭代中是否不同，IHT 和 ISTA 最终与真实信号的 $\\ell_{2}$-误差，以及 IHT 和 ISTA 最终生成的支撑集大小。\n\n测试套件。您的程序必须运行以下四个测试，每个测试由 $(\\text{seed}, m, n, k, \\sigma, \\text{step\\_scale}, \\alpha, T)$ 完全指定，其中每次迭代的步长为 $\\mu = \\text{step\\_scale} / L$（$L = \\lVert A \\rVert_{2}^{2}$），正则化权重为 $\\lambda = \\alpha \\lVert A^{\\top} y \\rVert_{\\infty}$：\n- 测试 1：$(\\,$seed $= 42,\\, m = 32,\\, n = 64,\\, k = 6,\\, \\sigma = 0.01,\\, \\text{step\\_scale} = 0.9,\\, \\alpha = 0.15,\\, T = 40\\,)$。\n- 测试 2：$(\\,$seed $= 123,\\, m = 30,\\, n = 60,\\, k = 5,\\, \\sigma = 0,\\, \\text{step\\_scale} = 1,\\, \\alpha = 0.10,\\, T = 60\\,)$。\n- 测试 3：$(\\,$seed $= 7,\\, m = 20,\\, n = 40,\\, k = 1,\\, \\sigma = 0.02,\\, \\text{step\\_scale} = 0.95,\\, \\alpha = 0.20,\\, T = 40\\,)$。\n- 测试 4：$(\\,$seed $= 2025,\\, m = 24,\\, n = 48,\\, k = 10,\\, \\sigma = 0.03,\\, \\text{step\\_scale} = 0.9,\\, \\alpha = 0.12,\\, T = 50\\,)$。\n\n对于每个测试，程序必须按顺序输出一个包含以下条目的列表：\n- 一个布尔值，指示支撑集在任何迭代中是否不同（即，是否存在迭代 $t \\in \\{1,\\dots,T\\}$，使得 IHT 的支撑集不等于 ISTA 的支撑集）。\n- 最终的 IHT 误差 $\\lVert x_{\\text{IHT}}^{(T)} - x_{\\star} \\rVert_{2}$，表示为一个四舍五入到 $6$ 位小数的实数。\n- 最终的 ISTA 误差 $\\lVert x_{\\text{ISTA}}^{(T)} - x_{\\star} \\rVert_{2}$，表示为一个四舍五入到 $6$ 位小数的实数。\n- 最终的 IHT 支撑集大小（一个整数，根据构造应等于 $k$）。\n- 最终的 ISTA 支撑集大小（一个整数，软阈值处理后非零项的数量）。\n\n最终输出格式。您的程序应生成单行输出，其中包含 4 个测试的结果，形式为一个包含 4 个列表的逗号分隔列表，并用方括号括起来，不含空格，布尔值呈现为 $True$ 或 $False$，浮点数精确到小数点后 $6$ 位。例如，输出必须具有以下形式：$[[b_{1},e^{\\text{IHT}}_{1},e^{\\text{ISTA}}_{1},s^{\\text{IHT}}_{1},s^{\\text{ISTA}}_{1}],[b_{2},e^{\\text{IHT}}_{2},e^{\\text{ISTA}}_{2},s^{\\text{IHT}}_{2},s^{\\text{ISTA}}_{2}],[b_{3},e^{\\text{IHT}}_{3},e^{\\text{ISTA}}_{3},s^{\\text{IHT}}_{3},s^{\\text{ISTA}}_{3}],[b_{4},e^{\\text{IHT}}_{4},e^{\\text{ISTA}}_{4},s^{\\text{IHT}}_{4},s^{\\text{ISTA}}_{4}]]$.",
            "solution": "该问题要求实现并比较两种用于稀疏信号恢复的基本迭代算法：迭代硬阈值算法 (Iterative Hard Thresholding, IHT) 和迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA)。这两种算法都是被称为邻近梯度法 (proximal gradient methods) 的通用框架的实例，该框架旨在解决形如 $\\min_{x} f(x) + g(x)$ 的优化问题，其中 $f(x)$ 是一个平滑可微函数，$g(x)$ 是一个可能非平滑但“简单”的正则化项。\n\n线性模型由 $y = A x_{\\star} + e$ 给出，其中 $y \\in \\mathbb{R}^{m}$ 是测量值，$A \\in \\mathbb{R}^{m \\times n}$ 是测量矩阵，$x_{\\star} \\in \\mathbb{R}^{n}$ 是一个 k-稀疏的真实信号，$e \\in \\mathbb{R}^{m}$ 是加性噪声。\n\n目标是从 $y$ 和 $A$ 中恢复 $x_{\\star}$ 的一个近似值。这被构建为一个优化问题，即我们寻找一个稀疏向量 $x$ 来最小化数据保真项 $f(x) = \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2}$。该项的梯度为 $\\nabla f(x) = A^{\\top}(A x - y)$。迭代算法的稳定性取决于步长 $\\mu$，必须选择 $\\mu$ 以满足 $\\mu \\le 1/L$，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。对于这个特定的 $f(x)$，Lipschitz 常数为 $L = \\lVert A^{\\top}A \\rVert_{2} = \\lVert A \\rVert_{2}^{2}$，其中 $\\lVert A \\rVert_2$ 是 $A$ 的谱范数，即其最大奇异值。\n\n邻近梯度法的一般迭代更新公式为：\n$$\nx^{(t+1)} = \\text{prox}_{\\mu g}\\left(x^{(t)} - \\mu \\nabla f(x^{(t)})\\right),\n$$\n其中 $\\text{prox}_{\\mu g}(v) = \\arg\\min_{z} \\left( g(z) + \\tfrac{1}{2\\mu}\\lVert z - v \\rVert_{2}^{2} \\right)$ 是函数 $\\mu g$ 的邻近算子。\n\n**迭代硬阈值算法 (Iterative Hard Thresholding, IHT)**\n\nIHT 旨在解决 $\\ell_0$ 约束的最小二乘问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2} \\quad \\text{subject to} \\quad \\lVert x \\rVert_{0} \\le k.\n$$\n这是一个非凸且 NP-难的问题。IHT 是一种提供近似解的投影梯度下降算法。这可以被看作是一种邻近梯度法，其中 $g(x)$ 是约束集 $C_k = \\{x \\in \\mathbb{R}^n \\mid \\lVert x \\rVert_{0} \\le k\\}$ 的指示函数。指示函数定义为：如果 $x \\in C_k$，则 $I_{C_k}(x) = 0$；否则 $I_{C_k}(x) = \\infty$。\n\n$I_{C_k}(x)$ 的邻近算子是到集合 $C_k$ 上的欧几里得投影，记作 $\\Pi_{C_k}(\\cdot)$。将一个向量 $v$ 投影到 k-稀疏向量集合上的操作，是通过保留 $v$ 中 $k$ 个最大幅值的元素并将其余元素置零来实现的。此操作称为硬阈值，我们记作 $H_k(\\cdot)$。\n\nIHT 算法流程如下：\n1. 初始化 $x^{(0)} = 0$。\n2. 对于 $t = 0, 1, 2, \\dots, T-1$：\n    a. 计算梯度： $g^{(t)} = A^{\\top}(A x^{(t)} - y)$。\n    b. 执行梯度下降步： $v^{(t)} = x^{(t)} - \\mu g^{(t)}$。\n    c. 投影到 k-稀疏向量集： $x^{(t+1)} = H_k(v^{(t)})$。\n\n硬阈值算子 $H_k(v)$ 的实现方式是：找到 $v$ 中 $k$ 个最大幅值项的索引，并创建一个新向量，该向量在这些索引处保留 $v$ 的值，在其他位置均为零。如果出现幅值相等的情况，则通过优先选择较小索引的项来确定性地打破僵局。\n\n**迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA)**\n\nISTA 解决的是 $\\ell_0$ 问题的凸松弛，即著名的 LASSO (最小绝对收缩和选择算子) 问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}.\n$$\n这是 $\\min_x f(x) + g(x)$ 一般形式的一个实例，其中 $f(x) = \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2}$ 且 $g(x) = \\lambda \\lVert x \\rVert_{1}$。$\\ell_1$-范数是凸的，使得整个问题成为凸问题。\n\n$\\mu g(x) = \\mu \\lambda \\lVert x \\rVert_1$ 的邻近算子是逐坐标的软阈值算子 $S_{\\mu\\lambda}(\\cdot)$。对于标量 $v_i$，该操作定义为：\n$$\nS_{\\tau}(v_i) = \\text{sign}(v_i) \\max(|v_i| - \\tau, 0),\n$$\n其中阈值为 $\\tau = \\mu\\lambda$。\n\nISTA 算法流程如下：\n1. 初始化 $x^{(0)} = 0$。\n2. 对于 $t = 0, 1, 2, \\dots, T-1$：\n    a. 计算梯度： $g^{(t)} = A^{\\top}(A x^{(t)} - y)$。\n    b. 执行梯度下降步： $v^{(t)} = x^{(t)} - \\mu g^{(t)}$。\n    c. 应用软阈值算子： $x^{(t+1)} = S_{\\mu\\lambda}(v^{(t)})$。\n\n正则化参数 $\\lambda$ 控制着数据保真度和稀疏性之间的权衡。这里使用的一个常见启发式方法是，对于某个 $\\alpha \\in (0,1)$，设置 $\\lambda = \\alpha \\lVert A^{\\top}y \\rVert_{\\infty}$。这一选择可以确保，对于 $x^{(0)}=0$，只要 $\\alpha$ 不是太大，第一步就不会得到全零向量。\n\n程序将实现这两种算法，根据指定的分布生成数据，运行迭代，在每次迭代中跟踪支撑集（非零项的索引），并报告所需的比较指标和最终误差。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_recovery_test(seed, m, n, k, sigma, step_scale, alpha, T):\n    \"\"\"\n    Implements and compares IHT and ISTA for a single test case.\n    \n    Args:\n        seed (int): Random seed.\n        m (int): Number of measurements.\n        n (int): Signal dimension.\n        k (int): Sparsity level.\n        sigma (float): Noise standard deviation.\n        step_scale (float): Scaling factor for the step size.\n        alpha (float): Scaling factor for the LASSO regularization parameter.\n        T (int): Number of iterations.\n        \n    Returns:\n        list: A list containing [supports_differ, err_iht, err_ista, size_iht, size_ista].\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    A = rng.normal(0, 1/np.sqrt(m), size=(m, n))\n    \n    x_star = np.zeros(n)\n    support = rng.choice(n, k, replace=False)\n    x_star[support] = rng.normal(0, 1, size=k)\n    \n    noise = rng.normal(0, sigma, size=m)\n    y = A @ x_star + noise\n\n    # 2. Algorithm Parameters\n    L = np.linalg.svd(A, compute_uv=False)[0]**2\n    mu = step_scale / L\n    lambda_reg = alpha * np.linalg.norm(A.T @ y, ord=np.inf)\n    \n    # 3. Initialization\n    x_iht = np.zeros(n)\n    x_ista = np.zeros(n)\n    \n    supports_iht_traj = []\n    supports_ista_traj = []\n\n    # 4. Iterations\n    for _ in range(T):\n        # IHT step\n        grad_iht = A.T @ (A @ x_iht - y)\n        v_iht = x_iht - mu * grad_iht\n        \n        # Hard thresholding with deterministic tie-breaking.\n        # np.lexsort sorts by the last key first. We want to sort by magnitude\n        # descending (-abs) and then by index ascending (arange).\n        # lexsort sorts in ascending order, so we take the first k.\n        top_k_indices = np.lexsort((np.arange(n), -np.abs(v_iht)))[:k]\n        \n        x_iht_new = np.zeros(n)\n        x_iht_new[top_k_indices] = v_iht[top_k_indices]\n        x_iht = x_iht_new\n        \n        # ISTA step\n        grad_ista = A.T @ (A @ x_ista - y)\n        v_ista = x_ista - mu * grad_ista\n        \n        # Soft thresholding\n        threshold = mu * lambda_reg\n        x_ista = np.sign(v_ista) * np.maximum(np.abs(v_ista) - threshold, 0)\n        \n        # Record supports\n        supports_iht_traj.append(set(np.where(x_iht != 0)[0]))\n        supports_ista_traj.append(set(np.where(x_ista != 0)[0]))\n\n    # 5. Compute Metrics\n    supports_differ = any(s1 != s2 for s1, s2 in zip(supports_iht_traj, supports_ista_traj))\n\n    err_iht = np.linalg.norm(x_iht - x_star)\n    err_ista = np.linalg.norm(x_ista - x_star)\n    \n    size_iht = np.count_nonzero(x_iht)\n    size_ista = np.count_nonzero(x_ista)\n    \n    return [supports_differ, err_iht, err_ista, size_iht, size_ista]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        (42, 32, 64, 6, 0.01, 0.9, 0.15, 40),\n        (123, 30, 60, 5, 0, 1.0, 0.10, 60),\n        (7, 20, 40, 1, 0.02, 0.95, 0.20, 40),\n        (2025, 24, 48, 10, 0.03, 0.9, 0.12, 50),\n    ]\n\n    all_results_formatted = []\n    for case in test_cases:\n        seed, m, n, k, sigma, step_scale, alpha, T = case\n        results = run_recovery_test(seed, m, n, k, sigma, step_scale, alpha, T)\n        \n        supports_differ, err_iht, err_ista, size_iht, size_ista = results\n        \n        formatted_case = (\n            f\"[{str(supports_differ)},{err_iht:.6f},{err_ista:.6f},\"\n            f\"{size_iht},{size_ista}]\"\n        )\n        all_results_formatted.append(formatted_case)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```"
        }
    ]
}