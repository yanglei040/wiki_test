{
    "hands_on_practices": [
        {
            "introduction": "Our exploration of greedy algorithms begins with the most fundamental building block: finding the best possible approximation using just a single atom from the dictionary. This exercise  requires you to determine the optimal one-term approximation for a given signal by applying the principle of orthogonal projection. Mastering this foundational calculation provides direct insight into the selection criterion used at each step of more complex algorithms like Matching Pursuit.",
            "id": "3449256",
            "problem": "Let $A=[a_1,a_2,a_3]\\in\\mathbb{R}^{3\\times 3}$ be a dictionary with orthonormal columns, where\n$$\na_1=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\\\ 0\\end{pmatrix},\\quad\na_2=\\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}}\\end{pmatrix},\\quad\na_3=\\begin{pmatrix}\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\end{pmatrix}.\n$$\nConsider the synthesis model in which a signal $y\\in\\mathbb{R}^{3}$ is approximated by a linear combination $Ax$ with a coefficient vector $x\\in\\mathbb{R}^{3}$ that is $1$-sparse (i.e., has at most $1$ nonzero entry). Let\n$$\ny=\\begin{pmatrix}3\\\\ -1\\\\ 4\\end{pmatrix}.\n$$\nStarting from the definitions of a $1$-sparse synthesis approximation, orthonormality, and the Euclidean norm, determine the $1$-term approximation $\\hat{y}$ that minimizes the residual norm $\\|y-\\hat{y}\\|_2$ over all choices of a single column of $A$ and a scalar coefficient, and then compute the associated residual norm $\\|y-\\hat{y}\\|_2$. Provide a clear derivation that justifies the selection of the atom and coefficient based only on these definitions. \n\nExpress your final answer as the $4$-tuple $(\\hat{y}_1,\\hat{y}_2,\\hat{y}_3,\\|y-\\hat{y}\\|_2)$ with exact values. Do not round.",
            "solution": "The problem requires finding the optimal $1$-term synthesis approximation $\\hat{y}$ for a given signal $y$ from a dictionary $A$. The dictionary $A = [a_1, a_2, a_3]$ consists of three orthonormal columns in $\\mathbb{R}^3$, and the signal is $y \\in \\mathbb{R}^3$.\n\nA $1$-term approximation is a vector of the form $\\hat{y} = x_j a_j$ for some choice of atom $a_j$ from the dictionary (where $j \\in \\{1, 2, 3\\}$) and a scalar coefficient $x_j \\in \\mathbb{R}$. The goal is to find the specific atom $a_j$ and coefficient $x_j$ that minimize the Euclidean norm of the residual, $\\|y - \\hat{y}\\|_2$.\n\nThe optimization problem can be stated as:\n$$\n\\min_{j \\in \\{1, 2, 3\\}, x_j \\in \\mathbb{R}} \\|y - x_j a_j\\|_2\n$$\nThis problem can be solved in two stages. First, for each fixed atom $a_j$, we find the optimal coefficient $x_j$. Second, we compare the resulting minimal residuals for each choice of $j$ to find the overall best approximation.\n\nLet's fix an index $j \\in \\{1, 2, 3\\}$. We seek to find the scalar $x_j$ that minimizes the function $f(x_j) = \\|y - x_j a_j\\|_2$. Minimizing the norm is equivalent to minimizing its square, $f(x_j)^2 = \\|y - x_j a_j\\|_2^2$. Using the definition of the Euclidean norm as the dot product of a vector with itself, we have:\n$$\n\\|y - x_j a_j\\|_2^2 = (y - x_j a_j)^T (y - x_j a_j)\n$$\nExpanding the product gives:\n$$\n\\|y - x_j a_j\\|_2^2 = y^T y - x_j y^T a_j - x_j a_j^T y + x_j^2 a_j^T a_j\n$$\nSince $y^T a_j$ is a scalar, its transpose $a_j^T y$ is equal to it. Also, the problem states that the columns of $A$ are orthonormal, which implies they are unit vectors, so $a_j^T a_j = \\|a_j\\|_2^2 = 1$. The expression simplifies to a quadratic function of $x_j$:\n$$\n\\|y - x_j a_j\\|_2^2 = \\|y\\|_2^2 - 2x_j (y^T a_j) + x_j^2\n$$\nTo find the value of $x_j$ that minimizes this quadratic, we can take the derivative with respect to $x_j$ and set it to zero:\n$$\n\\frac{d}{dx_j} \\left( \\|y\\|_2^2 - 2x_j (y^T a_j) + x_j^2 \\right) = -2(y^T a_j) + 2x_j = 0\n$$\nSolving for $x_j$ yields the optimal coefficient for a given atom $a_j$:\n$$\nx_j = y^T a_j\n$$\nThis result shows that the best scalar multiple of $a_j$ to approximate $y$ is found by projecting $y$ orthogonally onto the line spanned by $a_j$.\n\nNow we must determine which atom $a_j$ provides the best approximation. We substitute the optimal coefficient $x_j = y^T a_j$ back into the expression for the squared residual norm:\n$$\n\\min_{x_j} \\|y - x_j a_j\\|_2^2 = \\|y\\|_2^2 - 2(y^T a_j)(y^T a_j) + (y^T a_j)^2 = \\|y\\|_2^2 - (y^T a_j)^2\n$$\nTo find the overall minimum residual over all possible choices of $j$, we must choose the index $j$ that minimizes $\\|y\\|_2^2 - (y^T a_j)^2$. Since $\\|y\\|_2^2$ is a constant term, this is equivalent to maximizing the term $(y^T a_j)^2$, or equivalently, maximizing its absolute value, $|y^T a_j|$.\n\nWe are given the signal $y = \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix}$ and the atoms:\n$$\na_1=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\\\ 0\\end{pmatrix},\\quad\na_2=\\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}}\\end{pmatrix},\\quad\na_3=\\begin{pmatrix}\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\end{pmatrix}\n$$\nWe now compute the inner products $y^T a_j$ for $j=1, 2, 3$:\n$$\ny^T a_1 = (3)\\left(\\frac{1}{\\sqrt{2}}\\right) + (-1)\\left(\\frac{1}{\\sqrt{2}}\\right) + (4)(0) = \\frac{3}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n$$\n$$\ny^T a_2 = (3)\\left(\\frac{1}{\\sqrt{6}}\\right) + (-1)\\left(-\\frac{1}{\\sqrt{6}}\\right) + (4)\\left(\\frac{2}{\\sqrt{6}}\\right) = \\frac{3}{\\sqrt{6}} + \\frac{1}{\\sqrt{6}} + \\frac{8}{\\sqrt{6}} = \\frac{12}{\\sqrt{6}} = \\frac{12\\sqrt{6}}{6} = 2\\sqrt{6}\n$$\n$$\ny^T a_3 = (3)\\left(\\frac{1}{\\sqrt{3}}\\right) + (-1)\\left(-\\frac{1}{\\sqrt{3}}\\right) + (4)\\left(-\\frac{1}{\\sqrt{3}}\\right) = \\frac{3}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} - \\frac{4}{\\sqrt{3}} = 0\n$$\nNow we compare the absolute values of these inner products:\n$$\n|y^T a_1| = \\sqrt{2} \\approx 1.414\n$$\n$$\n|y^T a_2| = 2\\sqrt{6} = \\sqrt{24} \\approx 4.899\n$$\n$$\n|y^T a_3| = 0\n$$\nThe largest absolute value is $|y^T a_2| = 2\\sqrt{6}$. Therefore, the best atom to approximate $y$ is $a_2$. The optimal coefficient is $x_2 = y^T a_2 = 2\\sqrt{6}$.\n\nThe best $1$-term approximation $\\hat{y}$ is thus:\n$$\n\\hat{y} = x_2 a_2 = (2\\sqrt{6}) \\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}}\\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\ 4 \\end{pmatrix}\n$$\nThe components of the approximation are $\\hat{y}_1=2$, $\\hat{y}_2=-2$, and $\\hat{y}_3=4$.\n\nFinally, we compute the norm of the residual, $\\|y - \\hat{y}\\|_2$. The residual vector is:\n$$\nr = y - \\hat{y} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -2 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe norm of the residual is:\n$$\n\\|y - \\hat{y}\\|_2 = \\|r\\|_2 = \\sqrt{1^2 + 1^2 + 0^2} = \\sqrt{2}\n$$\nAlternatively, using the formula derived earlier:\n$$\n\\|y - \\hat{y}\\|_2 = \\sqrt{\\|y\\|_2^2 - (y^T a_2)^2}\n$$\nFirst, $\\|y\\|_2^2 = 3^2 + (-1)^2 + 4^2 = 9 + 1 + 16 = 26$.\nThen, $(y^T a_2)^2 = (2\\sqrt{6})^2 = 4 \\times 6 = 24$.\n$$\n\\|y - \\hat{y}\\|_2 = \\sqrt{26 - 24} = \\sqrt{2}\n$$\nBoth methods yield the same result.\n\nThe final answer is the $4$-tuple $(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, \\|y-\\hat{y}\\|_2)$, which is $(2, -2, 4, \\sqrt{2})$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2  -2  4  \\sqrt{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the single-atom selection, we now extend this concept to iterative procedures that build up a multi-term approximation. This practice  walks you through the first two iterations of two cornerstone algorithms, Matching Pursuit (MP) and Orthogonal Matching Pursuit (OMP). By directly comparing their mechanics and resulting residuals, you will gain a concrete understanding of why OMP's orthogonalization step provides a more accurate approximation at each stage.",
            "id": "3449235",
            "problem": "Consider the real-valued linear model with data vector $y \\in \\mathbb{R}^{2}$ and dictionary matrix $A \\in \\mathbb{R}^{2 \\times 3}$ whose columns (atoms) are unit-norm and given by\n$$\nA = \\begin{pmatrix}\n1  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  1\n\\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\nLet the atoms be denoted by $a_{1}, a_{2}, a_{3} \\in \\mathbb{R}^{2}$, i.e., $A = [a_{1} \\ a_{2} \\ a_{3}]$. The greedy methods to be applied are Matching Pursuit (MP) and Orthogonal Matching Pursuit (OMP), defined as follows.\n\nThe Matching Pursuit (MP) algorithm initializes the residual $r^{(0)} = y$ and, at iteration $t \\geq 1$, selects the index $j_{t} \\in \\{1,2,3\\}$ that maximizes the absolute inner product $|\\langle r^{(t-1)}, a_{j} \\rangle|$ over $j \\in \\{1,2,3\\}$. It then updates the residual by subtracting the scalar projection of $r^{(t-1)}$ onto the selected atom, $r^{(t)} = r^{(t-1)} - \\langle r^{(t-1)}, a_{j_{t}} \\rangle a_{j_{t}}$, without modifying previously chosen coefficients.\n\nThe Orthogonal Matching Pursuit (OMP) algorithm also initializes $r^{(0)} = y$ and, at iteration $t \\geq 1$, selects the index $j_{t}$ that maximizes $|\\langle r^{(t-1)}, a_{j} \\rangle|$, but then recomputes the coefficient vector supported on the entire set of selected indices $S^{(t)} = \\{j_{1}, \\dots, j_{t}\\}$ by orthogonally projecting $y$ onto the subspace $\\operatorname{span}\\{a_{j}: j \\in S^{(t)}\\}$, i.e., finds $x^{(t)}$ minimizing $\\|y - A_{S^{(t)}} x\\|_{2}$ where $A_{S^{(t)}}$ is the submatrix of $A$ with columns indexed by $S^{(t)}$. The residual is then $r^{(t)} = y - A_{S^{(t)}} x^{(t)}$.\n\nStarting from $r^{(0)} = y$, run exactly two iterations ($t=1$ and $t=2$) of both Matching Pursuit (MP) and Orthogonal Matching Pursuit (OMP) on the given $y$ and $A$. At each iteration, identify the selected atom index, compute the updated residual, and compute its Euclidean norm. Use the fundamental definitions of inner products, scalar projections, and orthogonal projections in finite-dimensional Euclidean spaces. After completing the two iterations for both methods, compare the ordered pairs of selected atom indices and the residual norms. Finally, compute the difference of the residual norms after two iterations,\n$$\n\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} - \\|r^{(2)}_{\\mathrm{OMP}}\\|_{2},\n$$\nand provide this value as a single exact real number. No rounding is required.",
            "solution": "The problem requires running two iterations of the Matching Pursuit (MP) and Orthogonal Matching Pursuit (OMP) algorithms to find a sparse approximation of a vector $y \\in \\mathbb{R}^{2}$ using a dictionary $A \\in \\mathbb{R}^{2 \\times 3}$. We must then compute the difference between the final residual norms of the two methods.\n\nThe given data are:\nThe data vector $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe dictionary matrix $A = \\begin{pmatrix} 1  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  1 \\end{pmatrix}$.\nThe columns of $A$, called atoms, are $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $a_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$, and $a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. All atoms are unit-norm, i.e., $\\|a_j\\|_2 = 1$ for $j \\in \\{1,2,3\\}$.\nThe initial residual for both algorithms is $r^{(0)} = y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\n**Matching Pursuit (MP)**\n\n**Iteration 1 (t=1):**\nThe first step is to select the atom $a_j$ that best correlates with the current residual, $r^{(0)}$. We compute the absolute inner products $|\\langle r^{(0)}, a_j \\rangle|$ for $j \\in \\{1,2,3\\}$:\n$|\\langle r^{(0)}, a_1 \\rangle| = \\left| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right| = |1 \\cdot 1 + 2 \\cdot 0| = |1| = 1$.\n$|\\langle r^{(0)}, a_2 \\rangle| = \\left| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\right| = \\left| 1 \\cdot \\frac{1}{\\sqrt{2}} + 2 \\cdot \\frac{1}{\\sqrt{2}} \\right| = \\left| \\frac{3}{\\sqrt{2}} \\right| = \\frac{3}{\\sqrt{2}}$.\n$|\\langle r^{(0)}, a_3 \\rangle| = \\left| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right| = |1 \\cdot 0 + 2 \\cdot 1| = |2| = 2$.\n\nComparing the values, we have $1$, $\\frac{3}{\\sqrt{2}} \\approx 2.121$, and $2$. The maximum is $\\frac{3}{\\sqrt{2}}$.\nThus, the selected index is $j_1=2$.\n\nThe MP algorithm updates the residual by subtracting the projection of $r^{(0)}$ onto $a_2$:\n$r^{(1)}_{\\mathrm{MP}} = r^{(0)} - \\langle r^{(0)}, a_{j_1} \\rangle a_{j_1} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{3}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$.\nThe Euclidean norm of this residual is:\n$\\|r^{(1)}_{\\mathrm{MP}}\\|_{2} = \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}$.\n\n**Iteration 2 (t=2):**\nNow we select the atom that best correlates with the new residual $r^{(1)}_{\\mathrm{MP}}$:\n$|\\langle r^{(1)}_{\\mathrm{MP}}, a_1 \\rangle| = \\left| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right| = |-\\frac{1}{2}| = \\frac{1}{2}$.\n$|\\langle r^{(1)}_{\\mathrm{MP}}, a_2 \\rangle| = \\left| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\right| = \\left|-\\frac{1}{2\\sqrt{2}} + \\frac{1}{2\\sqrt{2}}\\right| = |0| = 0$.\n$|\\langle r^{(1)}_{\\mathrm{MP}}, a_3 \\rangle| = \\left| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right| = |\\frac{1}{2}| = \\frac{1}{2}$.\n\nA tie occurs between indices $j=1$ and $j=3$. A common tie-breaking rule is to select the atom with the smallest index. We choose $j_2=1$.\nThe residual is updated:\n$r^{(2)}_{\\mathrm{MP}} = r^{(1)}_{\\mathrm{MP}} - \\langle r^{(1)}_{\\mathrm{MP}}, a_{j_2} \\rangle a_{j_2} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} - \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$.\nThe Euclidean norm of the final MP residual is:\n$\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} = \\sqrt{0^2 + \\left(\\frac{1}{2}\\right)^2} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$.\nThe ordered pair of selected indices for MP is $(2, 1)$.\n\n**Orthogonal Matching Pursuit (OMP)**\n\n**Iteration 1 (t=1):**\nThe selection step is identical to MP. The same inner products are computed, and the same atom is selected.\nSelected index $j_1=2$. The set of selected indices is $S^{(1)} = \\{2\\}$.\nOMP computes the coefficients by orthogonally projecting $y$ onto the span of the selected atoms. Here, we project $y$ onto $\\operatorname{span}\\{a_2\\}$. The coefficient $x^{(1)}$ is found by solving the least-squares problem $\\min_{x} \\|y - a_2 x\\|_2$. Since $a_2$ is unit-norm, the solution is $x^{(1)} = \\langle y, a_2 \\rangle = \\frac{3}{\\sqrt{2}}$.\nThe approximation is $y_{\\text{approx}}^{(1)} = x^{(1)} a_2 = \\frac{3}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix}$.\nThe residual is $r^{(1)}_{\\mathrm{OMP}} = y - y_{\\text{approx}}^{(1)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$.\nThis is identical to the first residual in MP, as expected for the first iteration.\n$\\|r^{(1)}_{\\mathrm{OMP}}\\|_{2} = \\frac{1}{\\sqrt{2}}$.\n\n**Iteration 2 (t=2):**\nThe selection is based on the inner products with the current residual $r^{(1)}_{\\mathrm{OMP}}$, which is the same as $r^{(1)}_{\\mathrm{MP}}$. The selection process is identical to MP's second iteration.\nSelected index $j_2=1$. The set of selected indices is $S^{(2)} = \\{2, 1\\}$.\nOMP re-computes coefficients by projecting $y$ onto the subspace $\\operatorname{span}\\{a_1, a_2\\}$. We find the coefficient vector $x^{(2)}$ that minimizes $\\|y - A_{S^{(2)}} x\\|_2$, where $A_{S^{(2)}} = [a_1 \\ a_2] = \\begin{pmatrix} 1  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\nThe columns $a_1$ and $a_2$ are linearly independent and thus form a basis for $\\mathbb{R}^2$. Therefore, $\\operatorname{span}\\{a_1, a_2\\} = \\mathbb{R}^2$.\nThe orthogonal projection of any vector $y \\in \\mathbb{R}^2$ onto the entire space $\\mathbb{R}^2$ is the vector $y$ itself.\nSo, the OMP approximation is $y_{\\text{approx}}^{(2)} = y$.\nThe residual is therefore $r^{(2)}_{\\mathrm{OMP}} = y - y_{\\text{approx}}^{(2)} = y - y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe Euclidean norm of the final OMP residual is:\n$\\|r^{(2)}_{\\mathrm{OMP}}\\|_{2} = 0$.\nThe ordered pair of selected indices for OMP is $(2, 1)$.\n\n**Comparison and Final Calculation**\n- The ordered pair of selected indices is the same for both methods: $(j_1, j_2) = (2, 1)$.\n- The final residual norm for MP is $\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} = \\frac{1}{2}$.\n- The final residual norm for OMP is $\\|r^{(2)}_{\\mathrm{OMP}}\\|_{2} = 0$.\n\nThe problem asks for the difference of the residual norms after two iterations:\n$\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} - \\|r^{(2)}_{\\mathrm{OMP}}\\|_{2} = \\frac{1}{2} - 0 = \\frac{1}{2}$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "Beyond the 'pursuit' family of algorithms, another powerful class of greedy methods operates through iterative projection. This exercise  introduces you to Iterative Hard Thresholding (IHT), where each step involves moving in the direction of steepest descent on the least-squares cost function, followed by a 'hard thresholding' operation to enforce sparsity. Working through these steps demonstrates a distinct yet equally fundamental approach to solving sparse approximation problems, highlighting the interplay between optimization and sparsity constraints.",
            "id": "3449248",
            "problem": "Consider the least-squares objective $f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$ with a measurement matrix\n$$\nA = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix},\n$$\na measurement vector\n$$\ny = \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix},\n$$\nand a target sparsity level $k = 2$. Iterative Hard Thresholding (IHT) is the greedy algorithm that, at each iteration, performs a gradient step on $f(x)$ and then projects onto the set of $k$-sparse vectors via hard thresholding (keeping the $k$ largest-magnitude coordinates and setting the rest to zero). Starting from the zero vector $x^{(0)} = \\mathbf{0}$, perform $2$ iterations of IHT with step size $\\eta = 1$ using the given $A$, $y$, and $k$. In your work, explicitly identify the thresholded support at each iteration and the resulting iterate $x^{(t)}$.\n\nFinally, report the squared Euclidean norm of the residual after the second iteration, that is, the value of $\\|y - A x^{(2)}\\|_{2}^{2}$. Provide the exact value; no rounding is required.",
            "solution": "The Iterative Hard Thresholding (IHT) algorithm aims to find a $k$-sparse solution $x$ to the problem of minimizing $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$. The algorithm proceeds via the iterative update rule:\n$$\nx^{(t)} = H_k(x^{(t-1)} - \\eta \\nabla f(x^{(t-1)}))\n$$\nwhere $H_k(v)$ is the hard thresholding operator that sets all but the $k$ largest-magnitude components of the vector $v$ to zero. The gradient of the objective function is given by:\n$$\n\\nabla f(x) = A^T (A x - y)\n$$\nWe are given the initial iterate $x^{(0)} = \\mathbf{0}$, the step size $\\eta = 1$, and the target sparsity $k=2$. The problem matrices and vectors are:\n$$\nA = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n\\quad , \\quad\ny = \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix}\n$$\nThe transpose of $A$ is:\n$$\nA^T = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  -1  0 \\\\\n2  1  1\n\\end{bmatrix}\n$$\n\n**Iteration 1 ($t=1$):**\nWe start with $x^{(0)} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^T$.\nFirst, we compute the gradient at $x^{(0)}$:\n$$\n\\nabla f(x^{(0)}) = A^T (A x^{(0)} - y) = A^T(-\\boldsymbol{y}) = -A^T y\n$$\n$$\n\\nabla f(x^{(0)}) = -\\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  -1  0 \\\\\n2  1  1\n\\end{bmatrix}\n\\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix}\n= -\\begin{bmatrix}\n1(3) + 0(0) + 1(2) \\\\\n0(3) + 1(0) + 1(2) \\\\\n1(3) - 1(0) + 0(2) \\\\\n2(3) + 1(0) + 1(2)\n\\end{bmatrix}\n= -\\begin{bmatrix}\n5 \\\\\n2 \\\\\n3 \\\\\n8\n\\end{bmatrix}\n= \\begin{bmatrix}\n-5 \\\\\n-2 \\\\\n-3 \\\\\n-8\n\\end{bmatrix}\n$$\nNext, we perform the gradient update step to find the candidate vector $b^{(1)}$:\n$$\nb^{(1)} = x^{(0)} - \\eta \\nabla f(x^{(0)}) = \\mathbf{0} - (1) \\begin{bmatrix} -5 \\\\ -2 \\\\ -3 \\\\ -8 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 2 \\\\ 3 \\\\ 8 \\end{bmatrix}\n$$\nNow, we apply the hard thresholding operator $H_2(\\cdot)$ to $b^{(1)}$. We must identify the $k=2$ components with the largest magnitudes. The magnitudes are $|5|=5$, $|2|=2$, $|3|=3$, and $|8|=8$. The two largest are $8$ (at index $4$) and $5$ (at index $1$). Therefore, the support set for the first iteration is $S_1 = \\{1, 4\\}$.\nThe first iterate $x^{(1)}$ is obtained by keeping these components and setting the others to zero:\n$$\nx^{(1)} = H_2(b^{(1)}) = \\begin{bmatrix} 5 \\\\ 0 \\\\ 0 \\\\ 8 \\end{bmatrix}\n$$\n\n**Iteration 2 ($t=2$):**\nWe start with $x^{(1)} = \\begin{bmatrix} 5  0  0  8 \\end{bmatrix}^T$.\nFirst, we compute the gradient at $x^{(1)}$. We begin by calculating the residual precursor $Ax^{(1)} - y$:\n$$\nAx^{(1)} = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n\\begin{bmatrix}\n5 \\\\\n0 \\\\\n0 \\\\\n8\n\\end{bmatrix}\n= \\begin{bmatrix}\n1(5) + 2(8) \\\\\n1(8) \\\\\n1(5) + 1(8)\n\\end{bmatrix}\n= \\begin{bmatrix}\n21 \\\\\n8 \\\\\n13\n\\end{bmatrix}\n$$\n$$\nAx^{(1)} - y = \\begin{bmatrix}\n21 \\\\\n8 \\\\\n13\n\\end{bmatrix} - \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix} = \\begin{bmatrix}\n18 \\\\\n8 \\\\\n11\n\\end{bmatrix}\n$$\nNow we can compute the gradient:\n$$\n\\nabla f(x^{(1)}) = A^T(Ax^{(1)} - y) = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  -1  0 \\\\\n2  1  1\n\\end{bmatrix}\n\\begin{bmatrix}\n18 \\\\\n8 \\\\\n11\n\\end{bmatrix}\n= \\begin{bmatrix}\n1(18) + 1(11) \\\\\n1(8) + 1(11) \\\\\n1(18) - 1(8) \\\\\n2(18) + 1(8) + 1(11)\n\\end{bmatrix}\n= \\begin{bmatrix}\n29 \\\\\n19 \\\\\n10 \\\\\n55\n\\end{bmatrix}\n$$\nNext, we perform the gradient update step to find the candidate vector $b^{(2)}$:\n$$\nb^{(2)} = x^{(1)} - \\eta \\nabla f(x^{(1)}) = \\begin{bmatrix} 5 \\\\ 0 \\\\ 0 \\\\ 8 \\end{bmatrix} - (1) \\begin{bmatrix} 29 \\\\ 19 \\\\ 10 \\\\ 55 \\end{bmatrix} = \\begin{bmatrix} -24 \\\\ -19 \\\\ -10 \\\\ -47 \\end{bmatrix}\n$$\nApplying the hard thresholding operator $H_2(\\cdot)$ to $b^{(2)}$, we examine the magnitudes: $|-24|=24$, $|-19|=19$, $|-10|=10$, and $|-47|=47$. The two largest are $47$ (at index $4$) and $24$ (at index $1$). Thus, the support set for the second iteration is $S_2 = \\{1, 4\\}$.\nThe second iterate $x^{(2)}$ is:\n$$\nx^{(2)} = H_2(b^{(2)}) = \\begin{bmatrix} -24 \\\\ 0 \\\\ 0 \\\\ -47 \\end{bmatrix}\n$$\n\n**Final Calculation:**\nWe need to compute the squared Euclidean norm of the residual after the second iteration, $\\|y - A x^{(2)}\\|_{2}^{2}$.\nFirst, calculate the product $A x^{(2)}$:\n$$\nA x^{(2)} = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n\\begin{bmatrix}\n-24 \\\\\n0 \\\\\n0 \\\\\n-47\n\\end{bmatrix}\n= \\begin{bmatrix}\n1(-24) + 2(-47) \\\\\n1(-47) \\\\\n1(-24) + 1(-47)\n\\end{bmatrix}\n= \\begin{bmatrix}\n-24 - 94 \\\\\n-47 \\\\\n-24 - 47\n\\end{bmatrix}\n= \\begin{bmatrix}\n-118 \\\\\n-47 \\\\\n-71\n\\end{bmatrix}\n$$\nNext, compute the residual vector $r^{(2)} = y - A x^{(2)}$:\n$$\nr^{(2)} = \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix} - \\begin{bmatrix}\n-118 \\\\\n-47 \\\\\n-71\n\\end{bmatrix} = \\begin{bmatrix}\n3 - (-118) \\\\\n0 - (-47) \\\\\n2 - (-71)\n\\end{bmatrix} = \\begin{bmatrix}\n121 \\\\\n47 \\\\\n73\n\\end{bmatrix}\n$$\nFinally, we compute the squared Euclidean norm of the residual:\n$$\n\\|y - A x^{(2)}\\|_{2}^{2} = \\|r^{(2)}\\|_{2}^{2} = (121)^2 + (47)^2 + (73)^2\n$$\n$$\n\\|y - A x^{(2)}\\|_{2}^{2} = 14641 + 2209 + 5329 = 22179\n$$",
            "answer": "$$\n\\boxed{22179}\n$$"
        }
    ]
}