{
    "hands_on_practices": [
        {
            "introduction": "许多贪婪算法的核心在于从字典中识别出能够最佳逼近一个信号的单个原子。本练习将这一基本概念置于一个具体的计算情境中，它构成了匹配追踪等算法第一步的基础。通过使用一个正交字典，您将看到寻找最优1-稀疏近似如何简化为寻找与信号相关性最强的原子。",
            "id": "3449256",
            "problem": "设 $A=[a_1,a_2,a_3]\\in\\mathbb{R}^{3\\times 3}$ 是一个具有标准正交列的字典，其中\n$$\na_1=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\\\ 0\\end{pmatrix},\\quad\na_2=\\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}}\\end{pmatrix},\\quad\na_3=\\begin{pmatrix}\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\end{pmatrix}.\n$$\n考虑一个合成模型，其中信号 $y\\in\\mathbb{R}^{3}$ 由线性组合 $Ax$ 近似，系数向量 $x\\in\\mathbb{R}^{3}$ 是 1-稀疏的（即，最多有 1 个非零项）。设\n$$\ny=\\begin{pmatrix}3\\\\ -1\\\\ 4\\end{pmatrix}.\n$$\n从 1-稀疏合成近似、标准正交性和欧几里得范数的定义出发，在 $A$ 的所有单列和标量系数的可能选择中，确定使残差范数 $\\|y-\\hat{y}\\|_2$ 最小的 1-项近似 $\\hat{y}$，然后计算相关的残差范数 $\\|y-\\hat{y}\\|_2$。请给出一个清晰的推导过程，仅基于这些定义来证明所选原子和系数的合理性。\n\n将最终答案表示为 4-元组 $(\\hat{y}_1,\\hat{y}_2,\\hat{y}_3,\\|y-\\hat{y}\\|_2)$，使用精确值，不要四舍五入。",
            "solution": "该问题要求对于给定的信号 $y$ 和字典 $A$，找到最优的 1-项合成近似 $\\hat{y}$。字典 $A = [a_1, a_2, a_3]$ 由 $\\mathbb{R}^3$ 中的三个标准正交列组成，信号为 $y \\in \\mathbb{R}^3$。\n\n1-项近似是形如 $\\hat{y} = x_j a_j$ 的向量，其中 $a_j$ 是从字典中选择的某个原子（$j \\in \\{1, 2, 3\\}$），$x_j \\in \\mathbb{R}$ 是一个标量系数。目标是找到特定的原子 $a_j$ 和系数 $x_j$，使得残差的欧几里得范数 $\\|y - \\hat{y}\\|_2$ 最小。\n\n该优化问题可以表述为：\n$$\n\\min_{j \\in \\{1, 2, 3\\}, x_j \\in \\mathbb{R}} \\|y - x_j a_j\\|_2\n$$\n这个问题可以分两步解决。首先，对于每个固定的原子 $a_j$，我们找到最优系数 $x_j$。其次，我们比较每种 $j$ 的选择所产生的最小残差，以找到全局最优近似。\n\n我们固定一个索引 $j \\in \\{1, 2, 3\\}$。我们寻求找到使函数 $f(x_j) = \\|y - x_j a_j\\|_2$ 最小化的标量 $x_j$。最小化范数等价于最小化其平方 $f(x_j)^2 = \\|y - x_j a_j\\|_2^2$。根据欧几里得范数是向量与自身点积的定义，我们有：\n$$\n\\|y - x_j a_j\\|_2^2 = (y - x_j a_j)^T (y - x_j a_j)\n$$\n展开乘积可得：\n$$\n\\|y - x_j a_j\\|_2^2 = y^T y - x_j y^T a_j - x_j a_j^T y + x_j^2 a_j^T a_j\n$$\n由于 $y^T a_j$ 是一个标量，其转置 $a_j^T y$ 与其相等。此外，题目说明 $A$ 的列是标准正交的，这意味着它们是单位向量，因此 $a_j^T a_j = \\|a_j\\|_2^2 = 1$。该表达式简化为关于 $x_j$ 的二次函数：\n$$\n\\|y - x_j a_j\\|_2^2 = \\|y\\|_2^2 - 2x_j (y^T a_j) + x_j^2\n$$\n为了找到使这个二次函数最小化的 $x_j$ 的值，我们可以对 $x_j$ 求导并令其为零：\n$$\n\\frac{d}{dx_j} \\left( \\|y\\|_2^2 - 2x_j (y^T a_j) + x_j^2 \\right) = -2(y^T a_j) + 2x_j = 0\n$$\n解出 $x_j$ 可得给定原子 $a_j$ 的最优系数：\n$$\nx_j = y^T a_j\n$$\n这个结果表明，用于近似 $y$ 的 $a_j$ 的最佳标量倍数，是通过将 $y$ 正交投影到由 $a_j$ 张成的直线上得到的。\n\n现在我们必须确定哪个原子 $a_j$ 提供最佳近似。我们将最优系数 $x_j = y^T a_j$ 代回到残差范数的平方表达式中：\n$$\n\\min_{x_j} \\|y - x_j a_j\\|_2^2 = \\|y\\|_2^2 - 2(y^T a_j)(y^T a_j) + (y^T a_j)^2 = \\|y\\|_2^2 - (y^T a_j)^2\n$$\n为了在所有可能的 $j$ 中找到全局最小残差，我们必须选择使 $\\|y\\|_2^2 - (y^T a_j)^2$ 最小化的索引 $j$。由于 $\\|y\\|_2^2$ 是一个常数项，这等价于最大化 $(y^T a_j)^2$ 项，或者等价地，最大化其绝对值 $|y^T a_j|$。\n\n我们已知信号 $y = \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix}$ 和原子：\n$$\na_1=\\begin{pmatrix}\\frac{1}{\\sqrt{2}}\\\\ \\frac{1}{\\sqrt{2}}\\\\ 0\\end{pmatrix},\\quad\na_2=\\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}}\\end{pmatrix},\\quad\na_3=\\begin{pmatrix}\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\\\ -\\frac{1}{\\sqrt{3}}\\end{pmatrix}\n$$\n现在我们计算 $j=1, 2, 3$ 时的内积 $y^T a_j$：\n$$\ny^T a_1 = (3)\\left(\\frac{1}{\\sqrt{2}}\\right) + (-1)\\left(\\frac{1}{\\sqrt{2}}\\right) + (4)(0) = \\frac{3}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n$$\n$$\ny^T a_2 = (3)\\left(\\frac{1}{\\sqrt{6}}\\right) + (-1)\\left(-\\frac{1}{\\sqrt{6}}\\right) + (4)\\left(\\frac{2}{\\sqrt{6}}\\right) = \\frac{3}{\\sqrt{6}} + \\frac{1}{\\sqrt{6}} + \\frac{8}{\\sqrt{6}} = \\frac{12}{\\sqrt{6}} = \\frac{12\\sqrt{6}}{6} = 2\\sqrt{6}\n$$\n$$\ny^T a_3 = (3)\\left(\\frac{1}{\\sqrt{3}}\\right) + (-1)\\left(-\\frac{1}{\\sqrt{3}}\\right) + (4)\\left(-\\frac{1}{\\sqrt{3}}\\right) = \\frac{3}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} - \\frac{4}{\\sqrt{3}} = 0\n$$\n现在我们比较这些内积的绝对值：\n$$\n|y^T a_1| = \\sqrt{2} \\approx 1.414\n$$\n$$\n|y^T a_2| = 2\\sqrt{6} = \\sqrt{24} \\approx 4.899\n$$\n$$\n|y^T a_3| = 0\n$$\n最大的绝对值是 $|y^T a_2| = 2\\sqrt{6}$。因此，近似 $y$ 的最佳原子是 $a_2$。最优系数是 $x_2 = y^T a_2 = 2\\sqrt{6}$。\n\n因此，最佳的 1-项近似 $\\hat{y}$ 是：\n$$\n\\hat{y} = x_2 a_2 = (2\\sqrt{6}) \\begin{pmatrix}\\frac{1}{\\sqrt{6}}\\\\ -\\frac{1}{\\sqrt{6}}\\\\ \\frac{2}{\\sqrt{6}}\\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\ 4 \\end{pmatrix}\n$$\n该近似的分量是 $\\hat{y}_1=2$，$\\hat{y}_2=-2$ 和 $\\hat{y}_3=4$。\n\n最后，我们计算残差的范数 $\\|y - \\hat{y}\\|_2$。残差向量为：\n$$\nr = y - \\hat{y} = \\begin{pmatrix} 3 \\\\ -1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -2 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n残差的范数为：\n$$\n\\|y - \\hat{y}\\|_2 = \\|r\\|_2 = \\sqrt{1^2 + 1^2 + 0^2} = \\sqrt{2}\n$$\n或者，使用我们前面推导的公式：\n$$\n\\|y - \\hat{y}\\|_2 = \\sqrt{\\|y\\|_2^2 - (y^T a_2)^2}\n$$\n首先，$\\|y\\|_2^2 = 3^2 + (-1)^2 + 4^2 = 9 + 1 + 16 = 26$。\n然后，$(y^T a_2)^2 = (2\\sqrt{6})^2 = 4 \\times 6 = 24$。\n$$\n\\|y - \\hat{y}\\|_2 = \\sqrt{26 - 24} = \\sqrt{2}\n$$\n两种方法得到相同的结果。\n\n最终答案是 4-元组 $(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, \\|y-\\hat{y}\\|_2)$，即 $(2, -2, 4, \\sqrt{2})$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2  -2  4  \\sqrt{2} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在单原子选择的基础上，本练习比较了两种最基础的贪婪算法：匹配追踪（MP）和正交匹配追踪（OMP）。通过手动执行这两种算法的两次迭代，您将直接观察到它们在更新残差和构建近似过程中的关键区别。这个比较阐明了为何OMP的正交化步骤能更有效地减小误差，并防止算法重复选择相似的方向。",
            "id": "3449235",
            "problem": "考虑一个实值线性模型，其数据向量为 $y \\in \\mathbb{R}^{2}$，字典矩阵为 $A \\in \\mathbb{R}^{2 \\times 3}$。该矩阵的列（原子）是单位范数的，并由下式给出\n$$\nA = \\begin{pmatrix}\n1  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  1\n\\end{pmatrix}, \\quad y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\n设原子表示为 $a_{1}, a_{2}, a_{3} \\in \\mathbb{R}^{2}$，即 $A = [a_{1} \\ a_{2} \\ a_{3}]$。将要应用的贪婪方法是匹配追踪（MP）和正交匹配追踪（OMP），定义如下。\n\n匹配追踪（MP）算法将残差初始化为 $r^{(0)} = y$，并在每次迭代 $t \\geq 1$ 时，选择索引 $j_{t} \\in \\{1,2,3\\}$，使得内积的绝对值 $|\\langle r^{(t-1)}, a_{j} \\rangle|$ 在所有 $j \\in \\{1,2,3\\}$ 中最大。然后，通过从残差中减去 $r^{(t-1)}$ 在所选原子上的标量投影来更新残差，即 $r^{(t)} = r^{(t-1)} - \\langle r^{(t-1)}, a_{j_{t}} \\rangle a_{j_{t}}$，而不修改先前选择的系数。\n\n正交匹配追踪（OMP）算法也将残差初始化为 $r^{(0)} = y$，并在每次迭代 $t \\geq 1$ 时，选择使 $|\\langle r^{(t-1)}, a_{j} \\rangle|$ 最大化的索引 $j_{t}$。但随后，它通过将 $y$ 正交投影到由所有已选索引 $S^{(t)} = \\{j_{1}, \\dots, j_{t}\\}$ 对应的原子所张成的子空间 $\\operatorname{span}\\{a_{j}: j \\in S^{(t)}\\}$ 上，重新计算支撑在整个已选索引集上的系数向量。即，找到 $x^{(t)}$ 以最小化 $\\|y - A_{S^{(t)}} x\\|_{2}$，其中 $A_{S^{(t)}}$ 是由 $S^{(t)}$ 中索引的列组成的 $A$ 的子矩阵。然后残差为 $r^{(t)} = y - A_{S^{(t)}} x^{(t)}$。\n\n从 $r^{(0)} = y$ 开始，对给定的 $y$ 和 $A$ 分别运行匹配追踪（MP）和正交匹配追踪（OMP）各两次迭代（$t=1$ 和 $t=2$）。在每次迭代中，确定所选原子的索引，计算更新后的残差，并计算其欧几里得范数。请使用有限维欧几里得空间中内积、标量投影和正交投影的基本定义。在完成两种方法的两次迭代后，比较所选原子索引的有序对和残差范数。最后，计算两次迭代后残差范数的差值，\n$$\n\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} - \\|r^{(2)}_{\\mathrm{OMP}}\\|_{2},\n$$\n并将此值作为一个精确的实数给出。不需要四舍五入。",
            "solution": "题目要求运行匹配追踪（MP）和正交匹配追踪（OMP）算法两次迭代，以找到向量 $y \\in \\mathbb{R}^{2}$ 在字典 $A \\in \\mathbb{R}^{2 \\times 3}$ 下的稀疏逼近。然后我们必须计算两种方法最终残差范数之间的差值。\n\n给定的数据如下：\n数据向量 $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n字典矩阵 $A = \\begin{pmatrix} 1  \\frac{1}{\\sqrt{2}}  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  1 \\end{pmatrix}$。\n$A$ 的列，称为原子，分别为 $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$a_{2} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$，和 $a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。所有原子都是单位范数，即对于 $j \\in \\{1,2,3\\}$，$\\|a_j\\|_2 = 1$。\n两种算法的初始残差均为 $r^{(0)} = y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。\n\n**匹配追踪（MP）**\n\n**迭代 1 (t=1):**\n第一步是选择与当前残差 $r^{(0)}$ 最相关的原子 $a_j$。我们计算 $j \\in \\{1,2,3\\}$ 的内积绝对值 $|\\langle r^{(0)}, a_j \\rangle|$：\n$|\\langle r^{(0)}, a_1 \\rangle| = \\left| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right| = |1 \\cdot 1 + 2 \\cdot 0| = |1| = 1$。\n$|\\langle r^{(0)}, a_2 \\rangle| = \\left| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\right| = \\left| 1 \\cdot \\frac{1}{\\sqrt{2}} + 2 \\cdot \\frac{1}{\\sqrt{2}} \\right| = \\left| \\frac{3}{\\sqrt{2}} \\right| = \\frac{3}{\\sqrt{2}}$。\n$|\\langle r^{(0)}, a_3 \\rangle| = \\left| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right| = |1 \\cdot 0 + 2 \\cdot 1| = |2| = 2$。\n\n比较这些值，我们得到 $1$、$\\frac{3}{\\sqrt{2}} \\approx 2.121$ 和 $2$。最大值为 $\\frac{3}{\\sqrt{2}}$。\n因此，选择的索引是 $j_1=2$。\n\nMP 算法通过减去 $r^{(0)}$ 在 $a_2$ 上的投影来更新残差：\n$r^{(1)}_{\\mathrm{MP}} = r^{(0)} - \\langle r^{(0)}, a_{j_1} \\rangle a_{j_1} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{3}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$。\n该残差的欧几里得范数为：\n$\\|r^{(1)}_{\\mathrm{MP}}\\|_{2} = \\sqrt{\\left(-\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}$。\n\n**迭代 2 (t=2):**\n现在我们选择与新残差 $r^{(1)}_{\\mathrm{MP}}$ 最相关的原子：\n$|\\langle r^{(1)}_{\\mathrm{MP}}, a_1 \\rangle| = \\left| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right| = |-\\frac{1}{2}| = \\frac{1}{2}$。\n$|\\langle r^{(1)}_{\\mathrm{MP}}, a_2 \\rangle| = \\left| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\right| = \\left|-\\frac{1}{2\\sqrt{2}} + \\frac{1}{2\\sqrt{2}}\\right| = |0| = 0$。\n$|\\langle r^{(1)}_{\\mathrm{MP}}, a_3 \\rangle| = \\left| \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\right| = |\\frac{1}{2}| = \\frac{1}{2}$。\n\n在索引 $j=1$ 和 $j=3$ 之间出现了平局。一个常见的打破平局的规则是选择索引最小的原子。我们选择 $j_2=1$。\n残差更新如下：\n$r^{(2)}_{\\mathrm{MP}} = r^{(1)}_{\\mathrm{MP}} - \\langle r^{(1)}_{\\mathrm{MP}}, a_{j_2} \\rangle a_{j_2} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} - \\left(-\\frac{1}{2}\\right) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$。\n最终 MP 残差的欧几里得范数为：\n$\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} = \\sqrt{0^2 + \\left(\\frac{1}{2}\\right)^2} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$。\nMP 选定的索引有序对是 $(2, 1)$。\n\n**正交匹配追踪（OMP）**\n\n**迭代 1 (t=1):**\n选择步骤与 MP 相同。计算相同的内积，选择相同的原子。\n选择的索引为 $j_1=2$。所选索引集为 $S^{(1)} = \\{2\\}$。\nOMP 通过将 $y$ 正交投影到所选原子张成的空间上来计算系数。在这里，我们将 $y$ 投影到 $\\operatorname{span}\\{a_2\\}$ 上。系数 $x^{(1)}$ 是通过求解最小二乘问题 $\\min_{x} \\|y - a_2 x\\|_2$ 得到的。由于 $a_2$ 是单位范数，解为 $x^{(1)} = \\langle y, a_2 \\rangle = \\frac{3}{\\sqrt{2}}$。\n逼近值为 $y_{\\text{approx}}^{(1)} = x^{(1)} a_2 = \\frac{3}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix}$。\n残差为 $r^{(1)}_{\\mathrm{OMP}} = y - y_{\\text{approx}}^{(1)} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix}$。\n这与 MP 中的第一个残差相同，正如第一次迭代所预期的那样。\n$\\|r^{(1)}_{\\mathrm{OMP}}\\|_{2} = \\frac{1}{\\sqrt{2}}$。\n\n**迭代 2 (t=2):**\n选择基于与当前残差 $r^{(1)}_{\\mathrm{OMP}}$ 的内积，该残差与 $r^{(1)}_{\\mathrm{MP}}$ 相同。选择过程与 MP 的第二次迭代完全相同。\n选择的索引为 $j_2=1$。所选索引集为 $S^{(2)} = \\{2, 1\\}$。\nOMP 通过将 $y$ 投影到子空间 $\\operatorname{span}\\{a_1, a_2\\}$ 上来重新计算系数。我们找到最小化 $\\|y - A_{S^{(2)}} x\\|_2$ 的系数向量 $x^{(2)}$，其中 $A_{S^{(2)}} = [a_1 \\ a_2] = \\begin{pmatrix} 1  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$。\n列向量 $a_1$ 和 $a_2$ 线性无关，因此构成 $\\mathbb{R}^2$ 的一组基。所以，$\\operatorname{span}\\{a_1, a_2\\} = \\mathbb{R}^2$。\n任何向量 $y \\in \\mathbb{R}^2$ 在整个空间 $\\mathbb{R}^2$ 上的正交投影就是向量 $y$ 本身。\n所以，OMP 的逼近值为 $y_{\\text{approx}}^{(2)} = y$。\n因此残差为 $r^{(2)}_{\\mathrm{OMP}} = y - y_{\\text{approx}}^{(2)} = y - y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n最终 OMP 残差的欧几里得范数为：\n$\\|r^{(2)}_{\\mathrm{OMP}}\\|_{2} = 0$。\nOMP 选定的索引有序对是 $(2, 1)$。\n\n**比较与最终计算**\n- 两种方法选择的索引有序对相同：$(j_1, j_2) = (2, 1)$。\n- MP 的最终残差范数为 $\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} = \\frac{1}{2}$。\n- OMP 的最终残差范数为 $\\|r^{(2)}_{\\mathrm{OMP}}\\|_{2} = 0$。\n\n题目要求计算两次迭代后残差范数的差值：\n$\\|r^{(2)}_{\\mathrm{MP}}\\|_{2} - \\|r^{(2)}_{\\mathrm{OMP}}\\|_{2} = \\frac{1}{2} - 0 = \\frac{1}{2}$。",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "除了基于相关性的选择方法，另一类强大的贪婪算法利用了最小二乘目标函数的梯度信息。本练习介绍了迭代硬阈值（IHT）算法，它巧妙地将梯度下降步骤与强制稀疏性的硬阈值投影相结合。完成本练习将为您提供关于这一重要算法的实践经验，揭示了连续优化概念如何被应用于稀疏恢复问题。",
            "id": "3449248",
            "problem": "考虑最小二乘目标函数 $f(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}$，其中测量矩阵为\n$$\nA = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix},\n$$\n测量向量为\n$$\ny = \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix},\n$$\n目标稀疏度为 $k = 2$。迭代硬阈值（Iterative Hard Thresholding, IHT）是一种贪心算法，在每次迭代中，对 $f(x)$ 执行一个梯度步，然后通过硬阈值（保留 $k$ 个最大幅值的坐标，其余置零）投影到 $k$-稀疏向量的集合上。从零向量 $x^{(0)} = \\mathbf{0}$ 开始，使用给定的 $A$、$y$ 和 $k$，以步长 $\\eta = 1$ 执行 2 次 IHT 迭代。在你的计算中，明确指出每次迭代中阈值化的支撑集以及得到的迭代结果 $x^{(t)}$。\n\n最后，报告第二次迭代后残差的平方欧几里得范数，即 $\\|y - A x^{(2)}\\|_{2}^{2}$ 的值。请提供精确值，无需四舍五入。",
            "solution": "该问题是有效的，因为它是一个适定的标准优化算法应用，所有必要的数据都已提供且没有不一致之处。\n\n迭代硬阈值（IHT）算法旨在找到一个 $k$-稀疏解 $x$，以最小化 $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$。该算法通过以下迭代更新规则进行：\n$$\nx^{(t)} = H_k(x^{(t-1)} - \\eta \\nabla f(x^{(t-1)}))\n$$\n其中 $H_k(v)$ 是硬阈值算子，它将向量 $v$ 中除了 $k$ 个最大幅值分量外的所有分量都设置为零。目标函数的梯度由下式给出：\n$$\n\\nabla f(x) = A^T (A x - y)\n$$\n我们给定的初始迭代值为 $x^{(0)} = \\mathbf{0}$，步长为 $\\eta = 1$，目标稀疏度为 $k=2$。问题中的矩阵和向量如下：\n$$\nA = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n\\quad , \\quad\ny = \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix}\n$$\n$A$ 的转置是：\n$$\nA^T = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  -1  0 \\\\\n2  1  1\n\\end{bmatrix}\n$$\n\n**迭代 1 ($t=1$):**\n我们从 $x^{(0)} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^T$ 开始。\n首先，我们计算在 $x^{(0)}$ 处的梯度：\n$$\n\\nabla f(x^{(0)}) = A^T (A x^{(0)} - y) = A^T(-\\boldsymbol{y}) = -A^T y\n$$\n$$\n\\nabla f(x^{(0)}) = -\\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  -1  0 \\\\\n2  1  1\n\\end{bmatrix}\n\\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix}\n= -\\begin{bmatrix}\n1(3) + 0(0) + 1(2) \\\\\n0(3) + 1(0) + 1(2) \\\\\n1(3) - 1(0) + 0(2) \\\\\n2(3) + 1(0) + 1(2)\n\\end{bmatrix}\n= -\\begin{bmatrix}\n5 \\\\\n2 \\\\\n3 \\\\\n8\n\\end{bmatrix}\n= \\begin{bmatrix}\n-5 \\\\\n-2 \\\\\n-3 \\\\\n-8\n\\end{bmatrix}\n$$\n接下来，我们执行梯度更新步骤以找到候选向量 $b^{(1)}$：\n$$\nb^{(1)} = x^{(0)} - \\eta \\nabla f(x^{(0)}) = \\mathbf{0} - (1) \\begin{bmatrix} -5 \\\\ -2 \\\\ -3 \\\\ -8 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 2 \\\\ 3 \\\\ 8 \\end{bmatrix}\n$$\n现在，我们将硬阈值算子 $H_2(\\cdot)$ 应用于 $b^{(1)}$。我们必须找出 $k=2$ 个具有最大幅值的分量。这些分量的幅值分别为 |5|=5、|2|=2、|3|=3 和 |8|=8。两个最大的幅值是 $8$（在索引 4 处）和 $5$（在索引 1 处）。因此，第一次迭代的支撑集是 $S_1 = \\{1, 4\\}$。\n通过保留这些分量并将其他分量设置为零，得到第一次迭代的结果 $x^{(1)}$：\n$$\nx^{(1)} = H_2(b^{(1)}) = \\begin{bmatrix} 5 \\\\ 0 \\\\ 0 \\\\ 8 \\end{bmatrix}\n$$\n\n**迭代 2 ($t=2$):**\n我们从 $x^{(1)} = \\begin{bmatrix} 5  0  0  8 \\end{bmatrix}^T$ 开始。\n首先，我们计算在 $x^{(1)}$ 处的梯度。我们先计算残差前驱项 $Ax^{(1)} - y$：\n$$\nAx^{(1)} = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n\\begin{bmatrix}\n5 \\\\\n0 \\\\\n0 \\\\\n8\n\\end{bmatrix}\n= \\begin{bmatrix}\n1(5) + 2(8) \\\\\n1(8) \\\\\n1(5) + 1(8)\n\\end{bmatrix}\n= \\begin{bmatrix}\n21 \\\\\n8 \\\\\n13\n\\end{bmatrix}\n$$\n$$\nAx^{(1)} - y = \\begin{bmatrix}\n21 \\\\\n8 \\\\\n13\n\\end{bmatrix} - \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix} = \\begin{bmatrix}\n18 \\\\\n8 \\\\\n11\n\\end{bmatrix}\n$$\n现在我们可以计算梯度：\n$$\n\\nabla f(x^{(1)}) = A^T(Ax^{(1)} - y) = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  -1  0 \\\\\n2  1  1\n\\end{bmatrix}\n\\begin{bmatrix}\n18 \\\\\n8 \\\\\n11\n\\end{bmatrix}\n= \\begin{bmatrix}\n1(18) + 1(11) \\\\\n1(8) + 1(11) \\\\\n1(18) - 1(8) \\\\\n2(18) + 1(8) + 1(11)\n\\end{bmatrix}\n= \\begin{bmatrix}\n29 \\\\\n19 \\\\\n10 \\\\\n55\n\\end{bmatrix}\n$$\n接下来，我们执行梯度更新步骤以找到候选向量 $b^{(2)}$：\n$$\nb^{(2)} = x^{(1)} - \\eta \\nabla f(x^{(1)}) = \\begin{bmatrix} 5 \\\\ 0 \\\\ 0 \\\\ 8 \\end{bmatrix} - (1) \\begin{bmatrix} 29 \\\\ 19 \\\\ 10 \\\\ 55 \\end{bmatrix} = \\begin{bmatrix} -24 \\\\ -19 \\\\ -10 \\\\ -47 \\end{bmatrix}\n$$\n将硬阈值算子 $H_2(\\cdot)$ 应用于 $b^{(2)}$，我们检查各分量的幅值：|-24|=24、|-19|=19、|-10|=10 和 |-47|=47。两个最大的幅值是 $47$（在索引 4 处）和 $24$（在索引 1 处）。因此，第二次迭代的支撑集是 $S_2 = \\{1, 4\\}$。\n第二次迭代的结果 $x^{(2)}$ 是：\n$$\nx^{(2)} = H_2(b^{(2)}) = \\begin{bmatrix} -24 \\\\ 0 \\\\ 0 \\\\ -47 \\end{bmatrix}\n$$\n\n**最终计算：**\n我们需要计算第二次迭代后残差的平方欧几里得范数，即 $\\|y - A x^{(2)}\\|_{2}^{2}$。\n首先，计算乘积 $A x^{(2)}$：\n$$\nA x^{(2)} = \\begin{bmatrix}\n1  0  1  2 \\\\\n0  1  -1  1 \\\\\n1  1  0  1\n\\end{bmatrix}\n\\begin{bmatrix}\n-24 \\\\\n0 \\\\\n0 \\\\\n-47\n\\end{bmatrix}\n= \\begin{bmatrix}\n1(-24) + 2(-47) \\\\\n1(-47) \\\\\n1(-24) + 1(-47)\n\\end{bmatrix}\n= \\begin{bmatrix}\n-24 - 94 \\\\\n-47 \\\\\n-24 - 47\n\\end{bmatrix}\n= \\begin{bmatrix}\n-118 \\\\\n-47 \\\\\n-71\n\\end{bmatrix}\n$$\n接下来，计算残差向量 $r^{(2)} = y - A x^{(2)}$：\n$$\nr^{(2)} = \\begin{bmatrix}\n3 \\\\\n0 \\\\\n2\n\\end{bmatrix} - \\begin{bmatrix}\n-118 \\\\\n-47 \\\\\n-71\n\\end{bmatrix} = \\begin{bmatrix}\n3 - (-118) \\\\\n0 - (-47) \\\\\n2 - (-71)\n\\end{bmatrix} = \\begin{bmatrix}\n121 \\\\\n47 \\\\\n73\n\\end{bmatrix}\n$$\n最后，我们计算残差的平方欧几里得范数：\n$$\n\\|y - A x^{(2)}\\|_{2}^{2} = \\|r^{(2)}\\|_{2}^{2} = (121)^2 + (47)^2 + (73)^2\n$$\n$$\n\\|y - A x^{(2)}\\|_{2}^{2} = 14641 + 2209 + 5329 = 22179\n$$",
            "answer": "$$\n\\boxed{22179}\n$$"
        }
    ]
}