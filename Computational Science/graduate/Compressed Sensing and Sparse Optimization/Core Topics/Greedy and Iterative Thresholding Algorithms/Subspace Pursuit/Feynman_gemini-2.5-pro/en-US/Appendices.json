{
    "hands_on_practices": [
        {
            "introduction": "Subspace Pursuit, like many iterative recovery algorithms, relies on a core computational step: finding the best signal approximation within a given subspace. This is achieved by solving a linear least-squares problem, which geometrically corresponds to orthogonally projecting the measurement vector onto the column space of the selected submatrix. This practice  provides a concrete exercise in calculating this least-squares solution and the resulting residual, reinforcing the fundamental linear algebra that powers each iteration of the algorithm.",
            "id": "3484157",
            "problem": "Consider a single iteration of Subspace Pursuit (SP), where the currently selected support has size $2$. The associated submatrix $A_{T} \\in \\mathbb{R}^{4 \\times 2}$ and the measurement vector $y \\in \\mathbb{R}^{4}$ are given by\n$$\nA_{T} \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix},\n\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 0\n\\end{pmatrix}.\n$$\nLet $x_{T} \\in \\mathbb{R}^{2}$ denote the least-squares coefficients that minimize the Euclidean norm of the residual, namely\n$$\nx_{T} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\|y - A_{T} x\\|_{2}.\n$$\nDefine the residual $r := y - A_{T} x_{T}$ and its Euclidean norm $\\|r\\|_{2}$. Using only first principles for linear least squares and basic linear algebra, compute $x_{T}$ and $\\|r\\|_{2}$ exactly (no numerical approximations). Provide your final answer as the row vector $(x_{T,1},\\, x_{T,2},\\, \\|r\\|_{2})$ with exact simplified radicals where applicable.",
            "solution": "The problem is to find the solution to a linear least-squares problem, which is a fundamental task in linear algebra and optimization. The vector $x_{T} \\in \\mathbb{R}^{2}$ that minimizes the Euclidean norm of the residual, $\\|y - A_{T} x\\|_{2}$, is the orthogonal projection of the vector $y$ onto the column space of the matrix $A_{T}$. This solution is uniquely determined by the normal equations, provided that the columns of $A_T$ are linearly independent.\n\nThe normal equations are given by:\n$$\nA_{T}^T A_{T} x_{T} = A_{T}^T y\n$$\nThe matrix $A_{T}$ is given as\n$$\nA_{T} \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n$$\nIts columns, $\\begin{pmatrix} 1 & 0 & 1 & 0 \\end{pmatrix}^T$ and $\\begin{pmatrix} 0 & 1 & 1 & 1 \\end{pmatrix}^T$, are linearly independent. Therefore, the Gram matrix $A_{T}^T A_{T}$ is invertible, and a unique solution $x_{T}$ exists.\n\nFirst, we compute the transpose of $A_{T}$:\n$$\nA_{T}^T \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{pmatrix}\n$$\nNext, we compute the Gram matrix $A_{T}^T A_{T}$:\n$$\nA_{T}^T A_{T} = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 & 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 1 + 0 \\cdot 1 \\\\\n0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 0 & 0 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}\n$$\nNow, we compute the vector $A_{T}^T y$, where $y$ is given as $\\begin{pmatrix} 1 & 2 & 3 & 0 \\end{pmatrix}^T$:\n$$\nA_{T}^T y = \\begin{pmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 1 + 0 \\cdot 2 + 1 \\cdot 3 + 0 \\cdot 0 \\\\\n0 \\cdot 1 + 1 \\cdot 2 + 1 \\cdot 3 + 1 \\cdot 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n4 \\\\ 5\n\\end{pmatrix}\n$$\nThe normal equations become the following $2 \\times 2$ system of linear equations for $x_{T} = \\begin{pmatrix} x_{T,1} \\\\ x_{T,2} \\end{pmatrix}$:\n$$\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{T,1} \\\\\nx_{T,2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\ 5\n\\end{pmatrix}\n$$\nTo solve for $x_{T}$, we find the inverse of $A_{T}^T A_{T}$. The determinant is $\\det(A_{T}^T A_{T}) = (2)(3) - (1)(1) = 6 - 1 = 5$.\nThe inverse is:\n$$\n(A_{T}^T A_{T})^{-1} = \\frac{1}{5}\n\\begin{pmatrix}\n3 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n$$\nNow we can compute $x_{T}$:\n$$\nx_{T} = (A_{T}^T A_{T})^{-1} (A_{T}^T y) = \\frac{1}{5}\n\\begin{pmatrix}\n3 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\n4 \\\\ 5\n\\end{pmatrix}\n= \\frac{1}{5}\n\\begin{pmatrix}\n3 \\cdot 4 + (-1) \\cdot 5 \\\\\n-1 \\cdot 4 + 2 \\cdot 5\n\\end{pmatrix}\n= \\frac{1}{5}\n\\begin{pmatrix}\n12 - 5 \\\\\n-4 + 10\n\\end{pmatrix}\n= \\frac{1}{5}\n\\begin{pmatrix}\n7 \\\\\n6\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{7}{5} \\\\\n\\frac{6}{5}\n\\end{pmatrix}\n$$\nSo, the least-squares coefficients are $x_{T,1} = \\frac{7}{5}$ and $x_{T,2} = \\frac{6}{5}$.\n\nNext, we calculate the residual vector $r = y - A_{T} x_{T}$. First, we compute the projection $A_{T} x_{T}$:\n$$\nA_{T} x_{T} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{7}{5} \\\\\n\\frac{6}{5}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{7}{5} \\\\\n\\frac{6}{5} \\\\\n\\frac{7}{5} + \\frac{6}{5} \\\\\n\\frac{6}{5}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{7}{5} \\\\\n\\frac{6}{5} \\\\\n\\frac{13}{5} \\\\\n\\frac{6}{5}\n\\end{pmatrix}\n$$\nNow, we find the residual vector $r$:\n$$\nr = y - A_{T} x_{T} =\n\\begin{pmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 0\n\\end{pmatrix}\n-\n\\begin{pmatrix}\n\\frac{7}{5} \\\\\n\\frac{6}{5} \\\\\n\\frac{13}{5} \\\\\n\\frac{6}{5}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{5}{5} - \\frac{7}{5} \\\\\n\\frac{10}{5} - \\frac{6}{5} \\\\\n\\frac{15}{5} - \\frac{13}{5} \\\\\n\\frac{0}{5} - \\frac{6}{5}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-\\frac{2}{5} \\\\\n\\frac{4}{5} \\\\\n\\frac{2}{5} \\\\\n-\\frac{6}{5}\n\\end{pmatrix}\n$$\nFinally, we compute the Euclidean norm of the residual, $\\|r\\|_{2}$.\n$$\n\\|r\\|_{2}^2 = \\left(-\\frac{2}{5}\\right)^2 + \\left(\\frac{4}{5}\\right)^2 + \\left(\\frac{2}{5}\\right)^2 + \\left(-\\frac{6}{5}\\right)^2\n= \\frac{4}{25} + \\frac{16}{25} + \\frac{4}{25} + \\frac{36}{25}\n= \\frac{4 + 16 + 4 + 36}{25} = \\frac{60}{25} = \\frac{12}{5}\n$$\nTaking the square root gives the norm:\n$$\n\\|r\\|_{2} = \\sqrt{\\frac{12}{5}} = \\frac{\\sqrt{12}}{\\sqrt{5}} = \\frac{2\\sqrt{3}}{\\sqrt{5}}\n$$\nTo rationalize the denominator, we multiply the numerator and denominator by $\\sqrt{5}$:\n$$\n\\|r\\|_{2} = \\frac{2\\sqrt{3}\\sqrt{5}}{\\sqrt{5}\\sqrt{5}} = \\frac{2\\sqrt{15}}{5}\n$$\nThe required components for the final answer are $x_{T,1} = \\frac{7}{5}$, $x_{T,2} = \\frac{6}{5}$, and $\\|r\\|_2 = \\frac{2\\sqrt{15}}{5}$. These form the row vector $\\left(\\frac{7}{5}, \\frac{6}{5}, \\frac{2\\sqrt{15}}{5}\\right)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{5} & \\frac{6}{5} & \\frac{2\\sqrt{15}}{5}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "After identifying candidate indices and performing a least-squares fit, Subspace Pursuit must refine its guess to maintain the target sparsity $k$. This is accomplished through a critical pruning step, which embodies the 'greedy' nature of the algorithm by retaining only the most significant signal components. In this exercise , you will apply this pruning principle by selecting the indices corresponding to the largest-magnitude coefficients, a core mechanism for converging to the true sparse support.",
            "id": "3484154",
            "problem": "Consider the linear measurement model $y = A x + w$ with $y \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $w \\in \\mathbb{R}^{m}$. Assume $x$ is $k$-sparse with $k = 3$, and the columns of $A$ are $\\ell_{2}$-normalized. At an iteration of the Subspace Pursuit (SP) algorithm, the current support estimate is $T = \\{2,7,10\\}$, and the proxy step produces an augmentation set $D = \\{5,9,12\\}$ based on correlations with the residual. The candidate set for least-squares refinement is $U = T \\cup D = \\{2,5,7,9,10,12\\}$.\n\nA restricted least-squares fit over $U$ yields coefficients $\\hat{x}_{U}$ aligned with the ordered indices of $U$ as follows:\n$$\nU = (2,\\,5,\\,7,\\,9,\\,10,\\,12), \\quad \\hat{x}_{U} = (0.14,\\,-1.32,\\,0.87,\\,-0.09,\\,2.05,\\,0.34).\n$$\nIn the pruning step of SP, the support is updated by selecting the $k$ indices in $U$ corresponding to the largest values of $|\\hat{x}_{i}|$ and discarding the rest, producing the refined support $T^{\\text{new}}$.\n\nUsing only the Subspace Pursuit principle that the pruning step retains the $k$ indices with the largest coefficient magnitudes from the least-squares fit on the candidate set, compute the updated support $T^{\\text{new}}$. Express your final answer as a row matrix listing the indices in increasing order.",
            "solution": "The problem requires the computation of the updated support set, denoted as $T^{\\text{new}}$, in one iteration of the Subspace Pursuit (SP) algorithm. This will be accomplished by performing the pruning step, which follows a least-squares estimation on a candidate support set.\n\nFirst, a validation of the problem statement is necessary.\n\nGivens:\n1.  The measurement model is $y = A x + w$, with $y \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $w \\in \\mathbb{R}^{m}$.\n2.  The signal $x$ is assumed to be $k$-sparse, with the sparsity level $k = 3$.\n3.  The columns of the matrix $A$ are $\\ell_{2}$-normalized.\n4.  The current support estimate at this iteration is $T = \\{2,7,10\\}$.\n5.  The augmentation set from the proxy step is $D = \\{5,9,12\\}$.\n6.  The candidate set for least-squares refinement is $U = T \\cup D = \\{2,5,7,9,10,12\\}$.\n7.  A restricted least-squares fit on the columns of $A$ indexed by $U$ yields a coefficient vector $\\hat{x}_{U}$. The ordering of indices in $U$ is given as $U = (2,\\,5,\\,7,\\,9,\\,10,\\,12)$, and the corresponding coefficients are $\\hat{x}_{U} = (0.14,\\,-1.32,\\,0.87,\\,-0.09,\\,2.05,\\,0.34)$.\n8.  The pruning step updates the support by selecting the $k$ indices from $U$ that correspond to the coefficients in $\\hat{x}_{U}$ with the largest absolute values.\n\nValidation:\nThe problem is scientifically grounded, as it describes a standard step within the Subspace Pursuit algorithm, a well-established method in the field of compressed sensing. The problem is well-posed and self-contained; it provides all necessary information to execute the specified task, namely the candidate set $U$, the corresponding coefficient vector $\\hat{x}_{U}$, and the sparsity level $k$. The terminology is precise and objective. There are no contradictions or ambiguities. Therefore, the problem is deemed valid.\n\nSolution:\nThe Subspace Pursuit algorithm is an iterative greedy method for sparse signal recovery. Each iteration involves identifying a candidate support set, estimating the signal coefficients on this set, and then pruning the set to maintain the desired sparsity level. The pruning step is the final part of an iteration, where the support set is refined for the next iteration.\n\nThe principle of the pruning step is to select the most significant components from the candidate set $U$. The significance of each component, indexed by $i \\in U$, is determined by the magnitude of its corresponding coefficient, $|\\hat{x}_{i}|$, from the least-squares solution over $U$.\n\nThe candidate set of indices is given as an ordered tuple $U = (2,\\,5,\\,7,\\,9,\\,10,\\,12)$. The corresponding vector of least-squares coefficients is $\\hat{x}_{U} = (0.14,\\,-1.32,\\,0.87,\\,-0.09,\\,2.05,\\,0.34)$. The sparsity level is $k=3$.\n\nThe task is to identify the $k=3$ indices in $U$ that correspond to the $3$ largest-magnitude coefficients in $\\hat{x}_{U}$. To do this, we first compute the absolute value of each coefficient in $\\hat{x}_{U}$:\n$|\\hat{x}_{U}| = (|0.14|,\\,|-1.32|,\\,|0.87|,\\,|-0.09|,\\,|2.05|,\\,|0.34|) = (0.14,\\,1.32,\\,0.87,\\,0.09,\\,2.05,\\,0.34)$.\n\nLet's associate each magnitude with its corresponding index from the ordered set $U$:\n\\begin{itemize}\n    \\item Index $2$ corresponds to magnitude $0.14$.\n    \\item Index $5$ corresponds to magnitude $1.32$.\n    \\item Index $7$ corresponds to magnitude $0.87$.\n    \\item Index $9$ corresponds to magnitude $0.09$.\n    \\item Index $10$ corresponds to magnitude $2.05$.\n    \\item Index $12$ corresponds to magnitude $0.34$.\n\\end{itemize}\n\nNext, we sort these magnitudes in descending order to identify the top $k=3$:\n1.  Magnitude $2.05$, corresponding to index $10$.\n2.  Magnitude $1.32$, corresponding to index $5$.\n3.  Magnitude $0.87$, corresponding to index $7$.\n4.  Magnitude $0.34$, corresponding to index $12$.\n5.  Magnitude $0.14$, corresponding to index $2$.\n6.  Magnitude $0.09$, corresponding to index $9$.\n\nThe pruning step retains the indices corresponding to the three largest magnitudes. These indices are $10$, $5$, and $7$. Therefore, the updated support set is $T^{\\text{new}} = \\{5, 7, 10\\}$.\n\nThe problem requests the final answer as a row matrix with the indices listed in increasing order. Sorting the elements of $T^{\\text{new}}$ gives the ordered set $(5, 7, 10)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 7 & 10 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having practiced the individual components of Subspace Pursuit, we now integrate them to observe the algorithm's full iterative process. This comprehensive exercise  guides you through a complete recovery problem, from initialization to convergence, allowing you to witness how the support estimate is refined over successive iterations. By tracking the algorithm's state and applying various standard stopping criteria, you will gain insight into the practical dynamics and termination logic of SP.",
            "id": "3484171",
            "problem": "Consider a noiseless compressed sensing instance with measurement matrix $\\mathbf{A} \\in \\mathbb{R}^{4 \\times 5}$ whose columns $\\{\\mathbf{a}_j\\}_{j=1}^{5}$ are unit-norm and given by\n$$\n\\mathbf{a}_{1}=\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{2}=\\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{3}=\\frac{1}{\\sqrt{13}}\\begin{pmatrix}3\\\\2\\\\0\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{4}=\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{5}=\\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}.\n$$\nLet the true $k$-sparse signal be $\\mathbf{x}^{\\star}\\in\\mathbb{R}^{5}$ with sparsity level $k=2$ and support $\\operatorname{supp}(\\mathbf{x}^{\\star})=\\{1,4\\}$, with nonzero entries $x_{1}^{\\star}=2$ and $x_{4}^{\\star}=1$. The measurement is $\\mathbf{y}=\\mathbf{A}\\mathbf{x}^{\\star}$.\n\nApply the Subspace Pursuit (SP) algorithm (an iterative greedy method for sparse recovery in Compressed Sensing (CS)) with sparsity parameter $k=2$ under the following canonical update at iteration $t\\geq 1$: given the residual $\\mathbf{r}^{(t-1)}=\\mathbf{y}-\\mathbf{A}\\mathbf{x}^{(t-1)}$, form the proxy $\\mathbf{A}^{\\top}\\mathbf{r}^{(t-1)}$, select the index set $\\mathcal{J}^{(t)}$ of the $k$ largest entries in magnitude, form the union $\\mathcal{U}^{(t)}=\\mathcal{T}^{(t-1)}\\cup\\mathcal{J}^{(t)}$, compute the least-squares estimate on $\\mathcal{U}^{(t)}$, and then prune to the new support $\\mathcal{T}^{(t)}$ by keeping the $k$ largest coefficients in magnitude. The initialization at $t=0$ is the usual SP initialization: compute $\\mathcal{T}^{(0)}$ as the $k$ indices with largest entries of $\\mathbf{A}^{\\top}\\mathbf{y}$ in magnitude, then compute $\\mathbf{x}^{(0)}$ by least squares on $\\mathcal{T}^{(0)}$ and set $\\mathbf{r}^{(0)}=\\mathbf{y}-\\mathbf{A}\\mathbf{x}^{(0)}$. Ties in selection can be broken arbitrarily; assume any such tie-breaks do not alter least-squares uniqueness when the residual is nonzero.\n\nAssume that for this instance the support stabilizes after two iterations in the sense that $\\mathcal{T}^{(2)}=\\mathcal{T}^{(1)}\\neq \\mathcal{T}^{(0)}$. Consider the following four stopping rules, each producing a termination iteration $\\hat{t}$ and an associated reconstruction $\\mathbf{x}^{(\\hat{t})}$:\n- $\\mathrm{S1}$ (support stabilization): stop at the smallest $t\\geq 1$ with $\\mathcal{T}^{(t)}=\\mathcal{T}^{(t-1)}$.\n- $\\mathrm{S2}$ (nondecreasing residual norm): stop at the smallest $t\\geq 1$ with $\\|\\mathbf{r}^{(t)}\\|_{2}\\geq \\|\\mathbf{r}^{(t-1)}\\|_{2}$.\n- $\\mathrm{S3}$ (residual norm threshold): stop at the smallest $t\\geq 0$ with $\\|\\mathbf{r}^{(t)}\\|_{2}\\leq \\tau$, with threshold $\\tau=1$.\n- $\\mathrm{S4}$ (fixed iteration budget): stop after exactly one SP iteration beyond initialization, i.e., at $t=1$.\n\nDefine the coefficient-domain reconstruction error at termination as $E=\\|\\mathbf{x}^{(\\hat{t})}-\\mathbf{x}^{\\star}\\|_{2}$. Compute, from first principles and without invoking any unproven shortcut formulas, for each rule $\\mathrm{S1}$â€“$\\mathrm{S4}$: the termination iteration $\\hat{t}$ and the corresponding error $E$. Report your final answer as a single row vector in the order $(\\hat{t}_{\\mathrm{S1}},E_{\\mathrm{S1}},\\hat{t}_{\\mathrm{S2}},E_{\\mathrm{S2}},\\hat{t}_{\\mathrm{S3}},E_{\\mathrm{S3}},\\hat{t}_{\\mathrm{S4}},E_{\\mathrm{S4}})$.\n\nNo rounding is required. The final answer must be given as exact values with no units.",
            "solution": "The problem is first validated against the established criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Measurement matrix $\\mathbf{A} \\in \\mathbb{R}^{4 \\times 5}$ with unit-norm columns:\n$$\n\\mathbf{a}_{1}=\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{2}=\\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{3}=\\frac{1}{\\sqrt{13}}\\begin{pmatrix}3\\\\2\\\\0\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{4}=\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix},\\quad\n\\mathbf{a}_{5}=\\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}.\n$$\n- True signal $\\mathbf{x}^{\\star}\\in\\mathbb{R}^{5}$ with sparsity $k=2$.\n- Support of true signal: $\\operatorname{supp}(\\mathbf{x}^{\\star})=\\{1,4\\}$.\n- Nonzero entries of true signal: $x_{1}^{\\star}=2$, $x_{4}^{\\star}=1$.\n- Measurement model: $\\mathbf{y}=\\mathbf{A}\\mathbf{x}^{\\star}$.\n- Subspace Pursuit (SP) algorithm with sparsity parameter $k=2$.\n- SP algorithm steps for initialization ($t=0$) and iteration ($t\\geq 1$) are specified.\n- Assumption on tie-breaking: \"Ties in selection can be broken arbitrarily; assume any such tie-breaks do not alter least-squares uniqueness when the residual is nonzero.\"\n- Assumption on algorithm behavior for this instance: \"the support stabilizes after two iterations in the sense that $\\mathcal{T}^{(2)}=\\mathcal{T}^{(1)}\\neq \\mathcal{T}^{(0)}$.\"\n- Four stopping rules are defined: S1 (support stabilization), S2 (nondecreasing residual norm), S3 (residual norm threshold with $\\tau=1$), S4 (fixed iteration budget at $t=1$).\n- Definition of reconstruction error: $E=\\|\\mathbf{x}^{(\\hat{t})}-\\mathbf{x}^{\\star}\\|_{2}$.\n- Required output: A single row vector $(\\hat{t}_{\\mathrm{S1}},E_{\\mathrm{S1}},\\hat{t}_{\\mathrm{S2}},E_{\\mathrm{S2}},\\hat{t}_{\\mathrm{S3}},E_{\\mathrm{S3}},\\hat{t}_{\\mathrm{S4}},E_{\\mathrm{S4}})$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is situated within the well-established field of compressed sensing. The Subspace Pursuit algorithm is a standard greedy recovery method. The setup is conventional for this domain.\n- **Well-Posed**: The problem provides all necessary data and definitions to perform the required calculations. The procedure is deterministic, although it involves a choice in tie-breaking. The problem provides a constraint on the algorithm's trajectory (\"$\\mathcal{T}^{(2)}=\\mathcal{T}^{(1)}\\neq \\mathcal{T}^{(0)}$\") that serves to resolve any ambiguity arising from ties.\n- **Objective**: The problem is stated in precise mathematical language, free of subjective elements.\n- **Consistency**: The dimensions of the matrix $\\mathbf{A}$ ($4 \\times 5$) and vector $\\mathbf{x}^{\\star}$ ($5 \\times 1$) are compatible. The columns of $\\mathbf{A}$ are indeed unit-norm as stated. The problem is internally consistent.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A full solution is warranted.\n\n### Solution Derivation\n\nFirst, we compute the measurement vector $\\mathbf{y}$. The true signal is $\\mathbf{x}^{\\star} = (2, 0, 0, 1, 0)^{\\top}$.\n$$\n\\mathbf{y} = \\mathbf{A}\\mathbf{x}^{\\star} = x_{1}^{\\star}\\mathbf{a}_{1} + x_{4}^{\\star}\\mathbf{a}_{4} = 2\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix} = \\begin{pmatrix}2\\\\0\\\\1\\\\0\\end{pmatrix}.\n$$\nNow, we trace the Subspace Pursuit algorithm. The sparsity parameter is $k=2$.\n\n**Initialization: Iteration $t=0$**\n1.  Compute the proxy $\\mathbf{p}^{(0)} = \\mathbf{A}^{\\top}\\mathbf{y}$:\n    $$\n    \\mathbf{p}^{(0)} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 3/\\sqrt{13} & 2/\\sqrt{13} & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix}2\\\\0\\\\1\\\\0\\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 6/\\sqrt{13} \\\\ 1 \\\\ 0 \\end{pmatrix}.\n    $$\n2.  Identify the initial support $\\mathcal{T}^{(0)}$ as the set of $k=2$ indices corresponding to the largest absolute values in $\\mathbf{p}^{(0)}$. The magnitudes are $(2, 0, 6/\\sqrt{13}, 1, 0)$. Since $2 = \\sqrt{4} > 6/\\sqrt{13} \\approx 1.66 > 1$, the two largest are at indices $1$ and $3$. Thus, $\\mathcal{T}^{(0)} = \\{1, 3\\}$.\n3.  Compute the initial estimate $\\mathbf{x}^{(0)}$ via least squares on $\\mathcal{T}^{(0)}$. We solve $\\min_{\\mathbf{z}}\\|\\mathbf{y} - \\mathbf{A}_{\\mathcal{T}^{(0)}}\\mathbf{z}\\|_{2}$, where $\\mathbf{A}_{\\mathcal{T}^{(0)}} = [\\mathbf{a}_1, \\mathbf{a}_3]$. The normal equations are $\\mathbf{A}_{\\mathcal{T}^{(0)}}^{\\top}\\mathbf{A}_{\\mathcal{T}^{(0)}}\\mathbf{z} = \\mathbf{A}_{\\mathcal{T}^{(0)}}^{\\top}\\mathbf{y}$.\n    The Gram matrix is $\\mathbf{G}_{\\mathcal{T}^{(0)}} = \\begin{pmatrix} \\mathbf{a}_1^{\\top}\\mathbf{a}_1 & \\mathbf{a}_1^{\\top}\\mathbf{a}_3 \\\\ \\mathbf{a}_3^{\\top}\\mathbf{a}_1 & \\mathbf{a}_3^{\\top}\\mathbf{a}_3 \\end{pmatrix} = \\begin{pmatrix} 1 & 3/\\sqrt{13} \\\\ 3/\\sqrt{13} & 1 \\end{pmatrix}$.\n    The right-hand side is $\\begin{pmatrix} \\mathbf{a}_1^{\\top}\\mathbf{y} \\\\ \\mathbf{a}_3^{\\top}\\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 6/\\sqrt{13} \\end{pmatrix}$.\n    Solving $\\begin{pmatrix} 1 & 3/\\sqrt{13} \\\\ 3/\\sqrt{13} & 1 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 6/\\sqrt{13} \\end{pmatrix}$ yields $z_1 = 2$ and $z_3=0$.\n    So, $\\mathbf{x}^{(0)} = (2, 0, 0, 0, 0)^{\\top}$.\n4.  Compute the initial residual $\\mathbf{r}^{(0)} = \\mathbf{y} - \\mathbf{A}\\mathbf{x}^{(0)}$.\n    $$\n    \\mathbf{r}^{(0)} = \\begin{pmatrix}2\\\\0\\\\1\\\\0\\end{pmatrix} - 2\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix} = \\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}.\n    $$\n    The initial residual norm is $\\|\\mathbf{r}^{(0)}\\|_2 = 1$.\n\n**Iteration $t=1$**\n1.  Compute the proxy $\\mathbf{p}^{(1)} = \\mathbf{A}^{\\top}\\mathbf{r}^{(0)}$. Since $\\mathbf{r}^{(0)} = \\mathbf{a}_4$, the entries of $\\mathbf{p}^{(1)}$ are $\\mathbf{a}_j^{\\top}\\mathbf{a}_4$.\n    $$\n    \\mathbf{p}^{(1)} = (\\mathbf{a}_1^{\\top}\\mathbf{a}_4, \\mathbf{a}_2^{\\top}\\mathbf{a}_4, \\mathbf{a}_3^{\\top}\\mathbf{a}_4, \\mathbf{a}_4^{\\top}\\mathbf{a}_4, \\mathbf{a}_5^{\\top}\\mathbf{a}_4)^{\\top} = (0, 0, 0, 1, 0)^{\\top}.\n    $$\n2.  Identify candidate indices $\\mathcal{J}^{(1)}$ of the $k=2$ largest magnitudes in $\\mathbf{p}^{(1)}$. The largest magnitude is $1$ at index $4$. The second largest is $0$, which is tied among indices $\\{1,2,3,5\\}$. We must choose one. The problem assumption that $\\mathcal{T}^{(2)}=\\mathcal{T}^{(1)}\\neq \\mathcal{T}^{(0)}$ guides our choice. Let's provisionally choose index $1$, so $\\mathcal{J}^{(1)} = \\{1, 4\\}$, and verify this leads to the specified behavior.\n3.  Form the union support $\\mathcal{U}^{(1)} = \\mathcal{T}^{(0)} \\cup \\mathcal{J}^{(1)} = \\{1, 3\\} \\cup \\{1, 4\\} = \\{1, 3, 4\\}$.\n4.  Compute an intermediate estimate $\\mathbf{b}$ by least squares on $\\mathcal{U}^{(1)}$. We solve $\\min_{\\mathbf{b}}\\|\\mathbf{y} - \\mathbf{A}_{\\mathcal{U}^{(1)}}\\mathbf{b}_{\\mathcal{U}^{(1)}}\\|_2$. The normal equations are $\\mathbf{A}_{\\mathcal{U}^{(1)}}^{\\top}\\mathbf{A}_{\\mathcal{U}^{(1)}}\\mathbf{b}_{\\mathcal{U}^{(1)}} = \\mathbf{A}_{\\mathcal{U}^{(1)}}^{\\top}\\mathbf{y}$.\n    $\\mathbf{A}_{\\mathcal{U}^{(1)}} = [\\mathbf{a}_1, \\mathbf{a}_3, \\mathbf{a}_4]$. The Gram matrix is $\\mathbf{G}_{\\mathcal{U}^{(1)}} = \\begin{pmatrix} 1 & 3/\\sqrt{13} & 0 \\\\ 3/\\sqrt{13} & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$. The right-hand side is $(\\mathbf{a}_1^{\\top}\\mathbf{y}, \\mathbf{a}_3^{\\top}\\mathbf{y}, \\mathbf{a}_4^{\\top}\\mathbf{y})^{\\top} = (2, 6/\\sqrt{13}, 1)^{\\top}$.\n    Solving $\\mathbf{G}_{\\mathcal{U}^{(1)}}\\mathbf{b}_{\\{1,3,4\\}} = (2, 6/\\sqrt{13}, 1)^{\\top}$ yields $b_1=2$, $b_3=0$, $b_4=1$.\n5.  Prune to obtain the new support $\\mathcal{T}^{(1)}$. We select the $k=2$ indices from $\\mathcal{U}^{(1)}$ corresponding to the largest magnitudes in $\\mathbf{b}$. The coefficients are $(b_1, b_3, b_4) = (2,0,1)$. The two largest are $b_1=2$ and $b_4=1$. Thus, $\\mathcal{T}^{(1)} = \\{1, 4\\}$. This is the true support. Note that $\\mathcal{T}^{(1)} \\neq \\mathcal{T}^{(0)}$, consistent with the problem statement.\n6.  Compute the estimate $\\mathbf{x}^{(1)}$ via least squares on $\\mathcal{T}^{(1)}$. The columns $\\mathbf{a}_1, \\mathbf{a}_4$ are orthogonal, so $\\mathbf{A}_{\\mathcal{T}^{(1)}}^{\\top}\\mathbf{A}_{\\mathcal{T}^{(1)}} = \\mathbf{I}$.\n    $$\n    \\mathbf{x}^{(1)}_{\\mathcal{T}^{(1)}} = \\mathbf{A}_{\\mathcal{T}^{(1)}}^{\\top}\\mathbf{y} = \\begin{pmatrix} \\mathbf{a}_1^{\\top}\\mathbf{y} \\\\ \\mathbf{a}_4^{\\top}\\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.\n    $$\n    Thus, $\\mathbf{x}^{(1)} = (2, 0, 0, 1, 0)^{\\top}$, which is identical to the true signal $\\mathbf{x}^{\\star}$.\n7.  Compute the residual $\\mathbf{r}^{(1)} = \\mathbf{y} - \\mathbf{A}\\mathbf{x}^{(1)} = \\mathbf{y} - \\mathbf{A}\\mathbf{x}^{\\star} = \\mathbf{0}$. The residual norm is $\\|\\mathbf{r}^{(1)}\\|_2 = 0$.\n\n**Iteration $t=2$**\n1.  Compute the proxy $\\mathbf{p}^{(2)} = \\mathbf{A}^{\\top}\\mathbf{r}^{(1)} = \\mathbf{A}^{\\top}\\mathbf{0} = \\mathbf{0}$.\n2.  Identify $\\mathcal{J}^{(2)}$ from the $k=2$ largest magnitudes in $\\mathbf{p}^{(2)}$. All entries are $0$, so we can pick any two indices, e.g., $\\mathcal{J}^{(2)} = \\{1, 2\\}$.\n3.  Form the union support $\\mathcal{U}^{(2)} = \\mathcal{T}^{(1)} \\cup \\mathcal{J}^{(2)} = \\{1, 4\\} \\cup \\{1, 2\\} = \\{1, 2, 4\\}$.\n4.  Compute intermediate estimate $\\mathbf{b}$ on $\\mathcal{U}^{(2)}$. Columns $\\mathbf{a}_1, \\mathbf{a}_2, \\mathbf{a}_4$ are orthonormal.\n    $\\mathbf{b}_{\\mathcal{U}^{(2)}} = \\mathbf{A}_{\\mathcal{U}^{(2)}}^{\\top}\\mathbf{y} = (\\mathbf{a}_1^{\\top}\\mathbf{y}, \\mathbf{a}_2^{\\top}\\mathbf{y}, \\mathbf{a}_4^{\\top}\\mathbf{y})^{\\top} = (2, 0, 1)^{\\top}$.\n5.  Prune to obtain $\\mathcal{T}^{(2)}$. The largest coefficients in $\\mathbf{b}$ are $b_1=2$ and $b_4=1$. So $\\mathcal{T}^{(2)} = \\{1, 4\\}$.\n6.  Check algorithm behavior: $\\mathcal{T}^{(2)} = \\{1, 4\\} = \\mathcal{T}^{(1)}$. This confirms the given condition that the support stabilizes at $t=2$. Our tie-breaking choice was correct.\n7.  Compute $\\mathbf{x}^{(2)}$. Since $\\mathcal{T}^{(2)} = \\mathcal{T}^{(1)}$, the LS solution is the same: $\\mathbf{x}^{(2)} = \\mathbf{x}^{(1)} = \\mathbf{x}^{\\star}$.\n8.  Compute the residual $\\mathbf{r}^{(2)} = \\mathbf{y} - \\mathbf{A}\\mathbf{x}^{(2)} = \\mathbf{0}$, so $\\|\\mathbf{r}^{(2)}\\|_2 = 0$.\n\n**Summary of Iterations**\n- $t=0$: $\\mathcal{T}^{(0)}=\\{1,3\\}$, $\\mathbf{x}^{(0)}=(2,0,0,0,0)^{\\top}$, $\\|\\mathbf{r}^{(0)}\\|_2=1$.\n- $t=1$: $\\mathcal{T}^{(1)}=\\{1,4\\}$, $\\mathbf{x}^{(1)}=(2,0,0,1,0)^{\\top}$, $\\|\\mathbf{r}^{(1)}\\|_2=0$.\n- $t=2$: $\\mathcal{T}^{(2)}=\\{1,4\\}$, $\\mathbf{x}^{(2)}=(2,0,0,1,0)^{\\top}$, $\\|\\mathbf{r}^{(2)}\\|_2=0$.\n\n**Applying the Stopping Rules**\n\n- **S1 (support stabilization):** Stop at smallest $t\\geq 1$ with $\\mathcal{T}^{(t)}=\\mathcal{T}^{(t-1)}$.\n  - For $t=1$: $\\mathcal{T}^{(1)}=\\{1,4\\} \\neq \\{1,3\\}=\\mathcal{T}^{(0)}$. Do not stop.\n  - For $t=2$: $\\mathcal{T}^{(2)}=\\{1,4\\} = \\mathcal{T}^{(1)}$. Stop.\n  - $\\hat{t}_{\\mathrm{S1}} = 2$. The reconstruction is $\\mathbf{x}^{(2)}=\\mathbf{x}^{\\star}$.\n  - $E_{\\mathrm{S1}} = \\|\\mathbf{x}^{(2)} - \\mathbf{x}^{\\star}\\|_2 = 0$.\n\n- **S2 (nondecreasing residual norm):** Stop at smallest $t\\geq 1$ with $\\|\\mathbf{r}^{(t)}\\|_{2}\\geq \\|\\mathbf{r}^{(t-1)}\\|_{2}$.\n  - For $t=1$: $\\|\\mathbf{r}^{(1)}\\|_2 = 0 < 1 = \\|\\mathbf{r}^{(0)}\\|_2$. Do not stop.\n  - For $t=2$: $\\|\\mathbf{r}^{(2)}\\|_2 = 0 \\geq 0 = \\|\\mathbf{r}^{(1)}\\|_2$. Stop.\n  - $\\hat{t}_{\\mathrm{S2}} = 2$. The reconstruction is $\\mathbf{x}^{(2)}=\\mathbf{x}^{\\star}$.\n  - $E_{\\mathrm{S2}} = \\|\\mathbf{x}^{(2)} - \\mathbf{x}^{\\star}\\|_2 = 0$.\n\n- **S3 (residual norm threshold):** Stop at smallest $t\\geq 0$ with $\\|\\mathbf{r}^{(t)}\\|_{2}\\leq \\tau=1$.\n  - For $t=0$: $\\|\\mathbf{r}^{(0)}\\|_2 = 1 \\leq 1$. Stop.\n  - $\\hat{t}_{\\mathrm{S3}} = 0$. The reconstruction is $\\mathbf{x}^{(0)}=(2,0,0,0,0)^{\\top}$.\n  - $E_{\\mathrm{S3}} = \\|\\mathbf{x}^{(0)} - \\mathbf{x}^{\\star}\\|_2 = \\|(2,0,0,0,0)^{\\top} - (2,0,0,1,0)^{\\top}\\|_2 = \\|(0,0,0,-1,0)^{\\top}\\|_2 = 1$.\n\n- **S4 (fixed iteration budget):** Stop at $t=1$.\n  - $\\hat{t}_{\\mathrm{S4}} = 1$. The reconstruction is $\\mathbf{x}^{(1)}=\\mathbf{x}^{\\star}$.\n  - $E_{\\mathrm{S4}} = \\|\\mathbf{x}^{(1)} - \\mathbf{x}^{\\star}\\|_2 = 0$.\n\n**Final Result Vector**\nCombining the results, the vector $(\\hat{t}_{\\mathrm{S1}},E_{\\mathrm{S1}},\\hat{t}_{\\mathrm{S2}},E_{\\mathrm{S2}},\\hat{t}_{\\mathrm{S3}},E_{\\mathrm{S3}},\\hat{t}_{\\mathrm{S4}},E_{\\mathrm{S4}})$ is $(2, 0, 2, 0, 0, 1, 1, 0)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & 0 & 2 & 0 & 0 & 1 & 1 & 0\n\\end{pmatrix}\n}\n$$"
        }
    ]
}