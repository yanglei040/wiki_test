{
    "hands_on_practices": [
        {
            "introduction": "Orthogonal Matching Pursuit begins with a simple, powerful idea: iteratively select the dictionary atom most correlated with the remaining signal, or residual. This first exercise provides a concrete calculation of this foundational step. By working with the well-structured Walsh-Hadamard orthonormal basis, you will compute the outcome of the first iteration and directly see how the signal energy is partitioned between the selected component and the new residual .",
            "id": "1108855",
            "problem": "Consider a signal vector $x \\in \\mathbb{R}^N$ where $N=2^n$ for some integer $n \\geq 1$. The Walsh-Hadamard basis for $\\mathbb{R}^N$ is a set of $N$ orthogonal vectors, which are the columns of the normalized Hadamard matrix $\\Psi = \\frac{1}{\\sqrt{N}} H_n$. The Hadamard matrix $H_n$ is a $2^n \\times 2^n$ matrix defined recursively by:\n$$\nH_0 = (1), \\quad H_k = \\begin{pmatrix} H_{k-1}  H_{k-1} \\\\ H_{k-1}  -H_{k-1} \\end{pmatrix} \\text{ for } k \\ge 1.\n$$\nThe columns of $\\Psi$, denoted by $\\{\\psi_0, \\psi_1, \\dots, \\psi_{N-1}\\}$, form an orthonormal basis for $\\mathbb{R}^N$.\n\nOrthogonal Matching Pursuit (OMP) is a greedy algorithm used to find a sparse approximation of a signal $x$ using a dictionary of atoms, which in this case is the Walsh-Hadamard basis. The algorithm proceeds iteratively.\n\n**OMP Algorithm (First Iteration):**\n1.  **Initialization:** The initial residual is the signal itself, $r_0 = x$.\n2.  **Matching:** Find the basis vector $\\psi_{j_1}$ that is most correlated with the current residual. The index $j_1$ is chosen as:\n    $$\n    j_1 = \\arg\\max_{j \\in \\{0, \\dots, N-1\\}} |\\langle r_0, \\psi_j \\rangle|\n    $$\n    where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product. If there is a tie for the maximum, any of the maximizing indices can be chosen.\n3.  **Projection  Residual Update:** The signal $x$ is projected onto the span of the selected basis vector $\\psi_{j_1}$ to form the first approximation $\\hat{x}_1 = \\text{proj}_{\\text{span}(\\psi_{j_1})} x$. The new residual is then calculated as $r_1 = x - \\hat{x}_1$.\n\n**Problem:**\nFor the signal vector $x = (1, 1, 1, 1, 0, 0, 0, 0)^T \\in \\mathbb{R}^8$, find the exact value of the Euclidean norm ($L_2$ norm) of the residual vector $r_1$ after a single iteration of the Orthogonal Matching Pursuit (OMP) algorithm using the Walsh-Hadamard basis for $N=8$.",
            "solution": "1. Relevant equations  \n   The OMP first‐iteration residual $r_1$ satisfies\n   $$\n   r_1 \\;=\\; x - \\proj_{\\psi_{j_1}}x,\n   $$\n   and since $\\psi_{j_1}$ is a unit vector,\n   $$\n   \\|r_1\\|_2^2 \\;=\\;\\|x\\|_2^2 - |\\langle x,\\psi_{j_1}\\rangle|^2.\n   $$\n\n2. Compute $\\|x\\|_2^2$  \n   $$\n   x = (1,1,1,1,0,0,0,0)^T \n   \\quad\\Longrightarrow\\quad\n   \\|x\\|_2^2 = 1^2+1^2+1^2+1^2 = 4.\n   $$\n\n3. Compute the maximal correlation  \n   The Walsh–Hadamard basis vectors are $\\psi_j=\\tfrac1{\\sqrt8}h_j$ where $h_j\\in\\{\\pm1\\}^8$ is column $j$ of $H_3$.  Since $x$ has nonzero entries only in the first four coordinates, for each $j$  \n   $$\n   \\langle x,\\psi_j\\rangle\n   = \\frac1{\\sqrt8}\\sum_{i=0}^3 h_{i,j}.\n   $$\n   One checks that the sum of the first four entries of $h_j$ is maximal in absolute value when $j\\equiv0\\pmod4$, giving\n   $$\n   \\max_j|\\langle x,\\psi_j\\rangle|\n   = \\frac1{\\sqrt8}(4) = \\frac{4}{2\\sqrt2} = \\sqrt2.\n   $$\n\n4. Residual norm  \n   $$\n   \\|r_1\\|_2\n   = \\sqrt{\\|x\\|_2^2 - \\bigl(\\max_j|\\langle x,\\psi_j\\rangle|\\bigr)^2}\n   = \\sqrt{4 - (\\sqrt2)^2}\n   = \\sqrt{2}.\n   $$",
            "answer": "$$\\boxed{\\sqrt{2}}$$"
        },
        {
            "introduction": "While OMP is effective in many scenarios, its greedy nature is not infallible, and the geometry of the dictionary plays a critical role in its success. High coherence between atoms is a primary cause of failure, and this exercise presents a carefully constructed thought experiment where OMP is deliberately led astray. By calculating the correlation values in this specific setup, you will gain a firsthand understanding of this classic failure mode and appreciate why recovery guarantees for OMP are deeply tied to matrix properties like mutual coherence .",
            "id": "3387250",
            "problem": "Consider the linear inverse model $y = A x$ used in sparse data assimilation, where $A \\in \\mathbb{R}^{2 \\times 3}$ is a dictionary with unit-norm columns and $x \\in \\mathbb{R}^{3}$ is a sparse coefficient vector. The greedy recovery algorithm Orthogonal Matching Pursuit (OMP), defined as selecting at its first iteration the column (atom) of $A$ with the largest absolute correlation with the residual (initialized as $r^{(0)} = y$), can fail when the dictionary contains highly coherent atoms. \n\nConstruct the following dictionary with unit-norm atoms:\n- $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- $a_{2} = \\begin{pmatrix} \\cos\\theta \\\\ \\sin\\theta \\end{pmatrix}$ with $\\theta = \\arccos(0.99)$,\n- $a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nLet the true sparse signal be supported on atoms $a_{2}$ and $a_{3}$ with coefficients\n- $x_{2}^{\\star} = 1$,\n- $x_{3}^{\\star} = -\\sin\\theta$,\nand $x_{1}^{\\star} = 0$. The observed data is thus $y = A x^{\\star} = a_{2} x_{2}^{\\star} + a_{3} x_{3}^{\\star}$.\n\nCompute the three absolute correlations \n$$c_{i} = \\left| \\langle a_{i}, y \\rangle \\right|, \\quad i \\in \\{1,2,3\\},$$\nthat determine OMP’s first selection and thereby demonstrate the misleading effect of the high coherence between $a_{1}$ and $a_{2}$. Provide the numerical values of the three correlations. No rounding is required; use exact values implied by $\\cos\\theta = 0.99$ and $\\sin\\theta = \\sqrt{1 - 0.99^{2}}$.",
            "solution": "The problem requires the computation of the absolute correlations $c_{i} = \\left| \\langle a_{i}, y \\rangle \\right|$ for $i \\in \\{1,2,3\\}$ for a specific sparse recovery setup. The Orthogonal Matching Pursuit (OMP) algorithm, at its first step, selects the atom (column of the dictionary $A$) that has the largest absolute correlation with the measurement vector $y$. The true sparse signal $x^{\\star}$ is supported on atoms $a_{2}$ and $a_{3}$, meaning these are the \"correct\" atoms.\n\nThe first step is to compute the measurement vector $y$. The model is given by $y = A x^{\\star}$. Since the true signal is sparse with non-zero coefficients $x_2^{\\star}$ and $x_3^{\\star}$, the measurement vector $y$ is a linear combination of the corresponding atoms $a_{2}$ and $a_{3}$.\n\nThe given atoms are:\n$$a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a_{2} = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{pmatrix}, \\quad a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nwhere we denote $\\theta_0 = \\arccos(0.99)$ to distinguish it from a generic variable $\\theta$. We are given $\\cos\\theta_0 = 0.99$. As $\\theta_0$ is the principal value of the arccosine function, $\\theta_0 \\in [0, \\pi]$, which ensures $\\sin\\theta_0 = \\sqrt{1 - \\cos^2\\theta_0} \\ge 0$.\n\nThe true coefficients are $x_{1}^{\\star} = 0$, $x_{2}^{\\star} = 1$, and $x_{3}^{\\star} = -\\sin\\theta_0$.\nThe measurement vector $y$ is thus:\n$$y = a_{2} x_{2}^{\\star} + a_{3} x_{3}^{\\star} = a_{2}(1) + a_{3}(-\\sin\\theta_0)$$\nSubstituting the vector definitions:\n$$y = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{pmatrix} - \\sin\\theta_0 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 - \\sin\\theta_0 \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix}$$\n\nNow, we can compute the three absolute correlations $c_{1}$, $c_{2}$, and $c_{3}$. The inner product $\\langle u, v \\rangle$ is defined as the standard dot product $u^T v$.\n\n1.  Compute $c_{1} = |\\langle a_{1}, y \\rangle|$:\n    $$\\langle a_{1}, y \\rangle = a_{1}^T y = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix} = (1)(\\cos\\theta_0) + (0)(0) = \\cos\\theta_0$$\n    Given $\\cos\\theta_0 = 0.99$, which is positive, the absolute value is:\n    $$c_{1} = |\\cos\\theta_0| = 0.99$$\n\n2.  Compute $c_{2} = |\\langle a_{2}, y \\rangle|$:\n    $$\\langle a_{2}, y \\rangle = a_{2}^T y = \\begin{pmatrix} \\cos\\theta_0  \\sin\\theta_0 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix} = (\\cos\\theta_0)(\\cos\\theta_0) + (\\sin\\theta_0)(0) = \\cos^2\\theta_0$$\n    Since $\\cos^2\\theta_0$ is non-negative, the absolute value is:\n    $$c_{2} = |\\cos^2\\theta_0| = \\cos^2\\theta_0 = (0.99)^2 = 0.9801$$\n\n3.  Compute $c_{3} = |\\langle a_{3}, y \\rangle|$:\n    $$\\langle a_{3}, y \\rangle = a_{3}^T y = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix} = (0)(\\cos\\theta_0) + (1)(0) = 0$$\n    The absolute value is:\n    $$c_{3} = |0| = 0$$\n\nThe three correlations are $c_1 = 0.99$, $c_2 = 0.9801$, and $c_3 = 0$.\nThe OMP algorithm selects the atom $a_k$ corresponding to the largest correlation, $k = \\arg\\max_i c_i$. In this case, since $0.99 > 0.9801 > 0$, we have $c_1 > c_2 > c_3$. Therefore, OMP selects atom $a_1$ in its first iteration.\nThis demonstrates the failure of OMP for this specific case. The true signal is composed of atoms $a_2$ and $a_3$, but the algorithm incorrectly selects atom $a_1$. This is a direct consequence of the high coherence between atoms $a_1$ and $a_2$, where $\\langle a_1, a_2 \\rangle = \\cos\\theta_0 = 0.99$, and the specific construction of the signal coefficients.\n\nThe required numerical values of the three correlations are:\n$$c_{1} = 0.99$$\n$$c_{2} = 0.9801$$\n$$c_{3} = 0$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0.99  0.9801  0 \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A naive implementation of OMP, which re-solves a full least-squares problem at each step, is computationally inefficient. This advanced problem guides you through the logic of an efficient OMP implementation using rank-one updates to the Cholesky factorization of the Gram matrix $A_{S}^{\\top}A_{S}$ . Deriving this update reveals the deep connection between OMP's iterative process, the principles of numerical linear algebra, and the geometric interpretation of the algorithm's stability, providing insight into how sparse recovery algorithms are made practical.",
            "id": "3464863",
            "problem": "Consider Orthogonal Matching Pursuit (OMP) in a real-valued setting with a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ whose columns are denoted by $\\{a_{1},\\dots,a_{n}\\}$. At iteration $t$, OMP has selected a support set $S^{t}$ of size two, and solves the least-squares subproblem using the submatrix $A_{S^{t}} \\in \\mathbb{R}^{m \\times 2}$, with the $2 \\times 2$ Gram matrix $G_{t} = A_{S^{t}}^{\\top}A_{S^{t}}$. Let the symmetric positive definite matrix $G_{t}$ be written elementwise as\n$$\nG_{t}=\\begin{pmatrix}\ng_{11}  g_{12} \\\\\ng_{12}  g_{22}\n\\end{pmatrix}.\n$$\nSuppose OMP considers adding a new atom $a_{j}$, with the vector of cross-correlations $c = A_{S^{t}}^{\\top} a_{j} \\in \\mathbb{R}^{2}$ given by $c = \\begin{pmatrix} c_{1} \\\\ c_{2} \\end{pmatrix}$, and the self-inner-product $n = a_{j}^{\\top} a_{j} \\in \\mathbb{R}$. The augmented Gram matrix is then\n$$\nG_{t+1} = \\begin{pmatrix}\nG_{t}  c \\\\\nc^{\\top}  n\n\\end{pmatrix}.\n$$\n\nYou are asked to show how to maintain and update $(A_{S^{t}}^{\\top}A_{S^{t}})^{-1}$ when moving from $S^{t}$ to $S^{t+1}=S^{t}\\cup\\{j\\}$ using Cholesky rank-one updates, starting from the fundamental definitions of the least-squares normal equations and the Cholesky factorization. In particular, argue from first principles why the update can be carried out without explicitly forming matrix inverses, and identify the scalar that governs the update step. Then, discuss the numerical stability implications of this approach relative to direct inverse updates, focusing on sources of round-off amplification and on remedies that can be implemented in practice.\n\nFinally, derive a closed-form analytic expression for the scalar equal to the square of the new diagonal entry of the updated Cholesky factor associated with $G_{t+1}$, expressed solely in terms of $g_{11}$, $g_{12}$, $g_{22}$, $c_{1}$, $c_{2}$, and $n$. Provide your final answer as a single simplified expression. No numerical evaluation is required.",
            "solution": "The posed problem concerns the efficient implementation of Orthogonal Matching Pursuit (OMP), a greedy algorithm for sparse approximation. At each iteration, OMP augments the support set $S$ with a new atom and solves a least-squares problem. A numerically robust and efficient method to solve this sequence of evolving least-squares problems is to update the Cholesky factorization of the Gram matrix $A_S^\\top A_S$, rather than repeatedly computing or updating its inverse. The problem asks to elucidate this process.\n\nLet the support set at iteration $t$ be $S^t$, and the measurement vector be $y \\in \\mathbb{R}^m$. The OMP subproblem is to find the coefficient vector $x_t$ that minimizes the squared residual norm $\\|y - A_{S^t} x_t\\|_2^2$. The solution is given by the normal equations:\n$$ (A_{S^t}^\\top A_{S^t}) x_t = A_{S^t}^\\top y $$\nLet $G_t = A_{S^t}^\\top A_{S^t}$ be the Gram matrix. The system is $G_t x_t = A_{S^t}^\\top y$. A direct solution $x_t = G_t^{-1} (A_{S^t}^\\top y)$ is inadvisable due to the numerical instability associated with forming the matrix inverse, especially since its condition number $\\kappa(G_t) = \\kappa(A_{S^t}^\\top A_{S^t}) = (\\kappa(A_{S^t}))^2$ can be large.\n\nA superior approach is to use the Cholesky factorization of the symmetric positive definite matrix $G_t = L_t L_t^\\top$, where $L_t$ is a lower triangular matrix. The normal equations become $L_t L_t^\\top x_t = A_{S^t}^\\top y$. This system is solved stably and efficiently in two steps:\n1. Solve $L_t z = A_{S^t}^\\top y$ for $z$ via forward substitution.\n2. Solve $L_t^\\top x_t = z$ for $x_t$ via backward substitution.\nThis procedure works with $L_t$, whose condition number is $\\kappa(L_t) = \\sqrt{\\kappa(G_t)} = \\kappa(A_{S^t})$, thus avoiding the numerical degradation caused by squaring the condition number.\n\nThe core of the problem is to update this factorization when the support set is augmented to $S^{t+1} = S^t \\cup \\{j\\}$. The new submatrix of the dictionary is $A_{S^{t+1}} = \\begin{pmatrix} A_{S^t}  a_j \\end{pmatrix}$. The new Gram matrix $G_{t+1}$ has a block structure:\n$$ G_{t+1} = A_{S^{t+1}}^\\top A_{S^{t+1}} = \\begin{pmatrix} A_{S^t}^\\top A_{S^t}  A_{S^t}^\\top a_j \\\\ a_j^\\top A_{S^t}  a_j^\\top a_j \\end{pmatrix} = \\begin{pmatrix} G_t  c \\\\ c^\\top  n \\end{pmatrix} $$\nWe seek the new Cholesky factor $L_{t+1}$ such that $G_{t+1} = L_{t+1} L_{t+1}^\\top_$. We can posit a compatible block structure for $L_{t+1}$:\n$$ L_{t+1} = \\begin{pmatrix} L_t  0 \\\\ v^\\top  \\ell \\end{pmatrix} $$\nwhere $v$ is a column vector and $\\ell$ is a scalar. Equating $L_{t+1} L_{t+1}^\\top$ with $G_{t+1}$:\n$$ L_{t+1} L_{t+1}^\\top = \\begin{pmatrix} L_t  0 \\\\ v^\\top  \\ell \\end{pmatrix} \\begin{pmatrix} L_t^\\top  v \\\\ 0  \\ell \\end{pmatrix} = \\begin{pmatrix} L_t L_t^\\top  L_t v \\\\ v^\\top L_t^\\top  v^\\top v + \\ell^2 \\end{pmatrix} = \\begin{pmatrix} G_t  c \\\\ c^\\top  n \\end{pmatrix} $$\nThis yields two equations for the unknown entities $v$ and $\\ell$:\n1. $L_t v = c$\n2. $v^\\top v + \\ell^2 = n$\n\nThis is the principle of the Cholesky update. To find the new factor $L_{t+1}$, one does not need to form any matrix inverses. First, one solves the lower triangular system $L_t v = c$ for $v$ using forward substitution. Second, one computes $\\ell = \\sqrt{n - v^\\top v}$. This process directly updates the Cholesky factor, which is then used to solve the next least-squares problem. The question's premise of \"updating the inverse... using Cholesky\" is thus addressed by demonstrating a Cholesky-based method that entirely bypasses a direct update or use of the inverse.\n\nThe scalar that governs this update is $\\ell^2 = n - v^\\top v$. Since $v = L_t^{-1} c$, we have $\\ell^2 = n - (L_t^{-1} c)^\\top(L_t^{-1} c) = n - c^\\top (L_t L_t^\\top)^{-1} c = n - c^\\top G_t^{-1} c$. This term is the Schur complement of $G_t$ in $G_{t+1}$. For $G_{t+1}$ to be positive definite, we must have $\\ell^2  0$. This means $n  c^\\top G_t^{-1} c$. This condition has a profound geometric interpretation: let $P_{S^t} = A_{S^t}(A_{S^t}^\\top A_{S^t})^{-1}A_{S^t}^\\top$ be the orthogonal projector onto the subspace spanned by the columns of $A_{S^t}$. Then $c^\\top G_t^{-1} c = a_j^\\top A_{S^t} (A_{S^t}^\\top A_{S^t})^{-1} A_{S^t}^\\top a_j = a_j^\\top P_{S^t} a_j = \\|P_{S^t} a_j\\|_2^2$. Therefore, $\\ell^2 = \\|a_j\\|_2^2 - \\|P_{S^t} a_j\\|_2^2 = \\|a_j - P_{S^t} a_j\\|_2^2 = \\|(I - P_{S^t}) a_j\\|_2^2$. This is the squared norm of the component of the new atom $a_j$ that is orthogonal to the already selected subspace. If $\\ell^2=0$, $a_j$ is linearly dependent on the columns of $A_{S^t}$, and the new Gram matrix $G_{t+1}$ would be singular.\n\nRegarding numerical stability, the Cholesky update method is substantially more stable than a direct update of the inverse, e.g., via the block matrix inversion formula (a form of the Sherman-Morrison-Woodbury formula). Directly updating $G_t^{-1}$ to $G_{t+1}^{-1}$ requires computing the term $S^{-1} = (n - c^\\top G_t^{-1} c)^{-1} = 1/\\ell^2$. If the new atom $a_j$ is nearly linearly dependent on the existing atoms in $A_{S^t}$, then $\\ell^2$ will be very small, and its inversion will amplify round-off errors. Furthermore, the computation of $\\ell^2$ itself is prone to catastrophic cancellation if $n$ is close to $c^\\top G_t^{-1} c$. The Cholesky factor update method, by solving $L_t v = c$, is less susceptible to these issues. However, errors can still accumulate over many iterations. Practical remedies include periodic re-orthogonalization of the basis or restarting the Cholesky factorization from a freshly computed Gram matrix. An even more stable alternative is to update a QR factorization of $A_{S^t}$, which avoids forming the Gram matrix altogether.\n\nFinally, we derive a closed-form expression for $\\ell^2$, the square of the new diagonal entry of the updated Cholesky factor, in terms of the given quantities. The Gram matrix at iteration $t$ is $G_t = \\begin{pmatrix} g_{11}  g_{12} \\\\ g_{12}  g_{22} \\end{pmatrix}$. Its inverse is:\n$$ G_t^{-1} = \\frac{1}{g_{11} g_{22} - g_{12}^2} \\begin{pmatrix} g_{22}  -g_{12} \\\\ -g_{12}  g_{11} \\end{pmatrix} $$\nThe vector of cross-correlations is $c = \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix}$. We compute the quadratic form $c^\\top G_t^{-1} c$:\n$$ c^\\top G_t^{-1} c = \\begin{pmatrix} c_1  c_2 \\end{pmatrix} \\left( \\frac{1}{g_{11} g_{22} - g_{12}^2} \\begin{pmatrix} g_{22}  -g_{12} \\\\ -g_{12}  g_{11} \\end{pmatrix} \\right) \\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} $$\n$$ = \\frac{1}{g_{11} g_{22} - g_{12}^2} \\begin{pmatrix} c_1  c_2 \\end{pmatrix} \\begin{pmatrix} g_{22}c_1 - g_{12}c_2 \\\\ -g_{12}c_1 + g_{11}c_2 \\end{pmatrix} $$\n$$ = \\frac{c_1(g_{22}c_1 - g_{12}c_2) + c_2(-g_{12}c_1 + g_{11}c_2)}{g_{11} g_{22} - g_{12}^2} $$\n$$ = \\frac{g_{22}c_1^2 - 2g_{12}c_1c_2 + g_{11}c_2^2}{g_{11} g_{22} - g_{12}^2} $$\nThe desired scalar is $\\ell^2 = n - c^\\top G_t^{-1} c$. Substituting the expression above:\n$$ \\ell^2 = n - \\frac{g_{22}c_1^2 - 2g_{12}c_1c_2 + g_{11}c_2^2}{g_{11} g_{22} - g_{12}^2} $$\nCombining into a single fraction provides the final expression:\n$$ \\ell^2 = \\frac{n(g_{11}g_{22} - g_{12}^2) - (g_{22}c_1^2 - 2g_{12}c_1c_2 + g_{11}c_2^2)}{g_{11}g_{22} - g_{12}^2} $$\nThis expression represents the square of the new diagonal element in $L_{t+1}$, which is fundamental to both the update step and the stability of the OMP algorithm.",
            "answer": "$$ \\boxed{n - \\frac{g_{22}c_{1}^{2} - 2g_{12}c_{1}c_{2} + g_{11}c_{2}^{2}}{g_{11}g_{22} - g_{12}^{2}}} $$"
        }
    ]
}