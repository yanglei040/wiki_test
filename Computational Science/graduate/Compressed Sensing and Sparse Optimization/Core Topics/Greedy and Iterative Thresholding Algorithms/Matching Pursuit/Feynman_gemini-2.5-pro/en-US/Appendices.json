{
    "hands_on_practices": [
        {
            "introduction": "The Matching Pursuit (MP) algorithm's greedy, one-atom-at-a-time selection process is intuitively simple, but its behavior can be surprisingly complex and sometimes pathological. This first exercise uncovers a fundamental vulnerability of the basic MP algorithm when it operates on a dictionary whose atoms are not normalized to have equal energy. By constructing a simple two-dimensional example, you will demonstrate from first principles how a single, large-norm atom can perpetually trap the algorithm, preventing it from ever selecting other, potentially more useful, atoms and leading to extremely slow or non-existent convergence . This practice provides a crucial lesson on the importance of dictionary normalization as a prerequisite for robust performance.",
            "id": "3458955",
            "problem": "Consider a real Hilbert space $\\mathcal{H} = \\mathbb{R}^{2}$ with the standard Euclidean inner product $\\langle x, y \\rangle = x^{\\top} y$ and induced norm $\\|x\\|_{2} = \\sqrt{\\langle x, x \\rangle}$. A dictionary is a collection of atoms (vectors) $\\mathcal{D} = \\{ g_{j} \\}_{j}$ in $\\mathcal{H}$. In the Matching Pursuit algorithm, given a residual $r^{(t)}$ at iteration $t$, one selects an index $j_{t}$ that maximizes the magnitude of the inner product, $j_{t} \\in \\arg\\max_{j} |\\langle r^{(t)}, g_{j} \\rangle|$, and updates the residual by $r^{(t+1)} = r^{(t)} - \\langle r^{(t)}, g_{j_{t}} \\rangle g_{j_{t}}$.\n\nConstruct a dictionary and signal as follows. Let $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Fix a scalar $\\alpha > 1$ and define the dictionary $\\mathcal{D} = \\{ g_{1}, g_{2} \\}$ with $g_{1} = \\alpha u$ and $g_{2} = v$. Let the initial signal be $x = u$, so the initial residual is $r^{(0)} = x$.\n\nStarting from the fundamental definitions above, analyze the Matching Pursuit iterations for this dictionary and signal. Show from first principles that the selected index is always $j_{t} = 1$ for all $t \\geq 0$, so that the support never grows beyond the first element. Demonstrate the exact residual behavior over iterations by deriving a closed-form expression for $r^{(k)}$ and for its Euclidean norm $\\| r^{(k)} \\|_{2}$ after $k$ iterations, in terms of $\\alpha$ and $k$. Your final reported quantity must be the closed-form expression for $\\| r^{(k)} \\|_{2}$ as a function of $\\alpha$ and $k$. No rounding is required.",
            "solution": "We begin with the specified Hilbert space $\\mathcal{H} = \\mathbb{R}^{2}$ under the standard inner product $\\langle x, y \\rangle = x^{\\top} y$ and norm $\\|x\\|_{2} = \\sqrt{\\langle x, x \\rangle}$. The dictionary is $\\mathcal{D} = \\{ g_{1}, g_{2} \\}$ with $g_{1} = \\alpha u$ and $g_{2} = v$, where $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The initial residual is $r^{(0)} = x = u$.\n\nThe Matching Pursuit selection rule at iteration $t$ is\n$$\nj_{t} \\in \\arg\\max_{j \\in \\{1,2\\}} \\left| \\langle r^{(t)}, g_{j} \\rangle \\right|,\n$$\nand the residual update is\n$$\nr^{(t+1)} = r^{(t)} - \\langle r^{(t)}, g_{j_{t}} \\rangle g_{j_{t}}.\n$$\n\nWe will show inductively that for all $t \\geq 0$ the residual remains collinear with $u$, i.e., there exists a scalar $\\beta_{t}$ such that $r^{(t)} = \\beta_{t} u$, and that the selected index is $j_{t} = 1$ at every iteration. This will imply that the support never grows beyond the first atom $g_{1}$.\n\nBase case ($t = 0$): We have $r^{(0)} = u = 1 \\cdot u$, so the form $r^{(0)} = \\beta_{0} u$ holds with $\\beta_{0} = 1$.\n\nSelection at $t = 0$:\n- The correlation with $g_{1}$ is\n$$\n\\left| \\langle r^{(0)}, g_{1} \\rangle \\right| = \\left| \\langle u, \\alpha u \\rangle \\right| = \\left| \\alpha \\langle u, u \\rangle \\right| = \\alpha,\n$$\nsince $\\|u\\|_{2} = 1$.\n- The correlation with $g_{2}$ is\n$$\n\\left| \\langle r^{(0)}, g_{2} \\rangle \\right| = \\left| \\langle u, v \\rangle \\right| = \\left| 0 \\right| = 0,\n$$\nbecause $u$ and $v$ are orthogonal.\n\nHence $j_{0} = 1$ is the unique maximizer, as $\\alpha > 1$ implies $\\alpha > 0$.\n\nResidual update at $t = 0$:\n$$\nr^{(1)} = r^{(0)} - \\langle r^{(0)}, g_{1} \\rangle g_{1} = u - \\langle u, \\alpha u \\rangle \\alpha u = u - (\\alpha)(\\alpha u) = u - \\alpha^{2} u = (1 - \\alpha^{2}) u.\n$$\nTherefore $r^{(1)}$ is collinear with $u$, i.e., $r^{(1)} = \\beta_{1} u$ with $\\beta_{1} = 1 - \\alpha^{2}$.\n\nInductive step: Suppose for some $t \\geq 1$ we have $r^{(t)} = \\beta_{t} u$ for a scalar $\\beta_{t}$. Then the correlations at iteration $t$ are\n$$\n\\left| \\langle r^{(t)}, g_{1} \\rangle \\right| = \\left| \\langle \\beta_{t} u, \\alpha u \\rangle \\right| = \\left| \\beta_{t} \\alpha \\langle u, u \\rangle \\right| = \\left| \\beta_{t} \\alpha \\right|,\n$$\nand\n$$\n\\left| \\langle r^{(t)}, g_{2} \\rangle \\right| = \\left| \\langle \\beta_{t} u, v \\rangle \\right| = \\left| \\beta_{t} \\langle u, v \\rangle \\right| = 0.\n$$\nThus $j_{t} = 1$ remains the maximizer. The residual update gives\n$$\nr^{(t+1)} = r^{(t)} - \\langle r^{(t)}, g_{1} \\rangle g_{1} = \\beta_{t} u - \\left( \\beta_{t} \\alpha \\right) \\alpha u = \\beta_{t} u - \\beta_{t} \\alpha^{2} u = \\beta_{t} (1 - \\alpha^{2}) u.\n$$\nHence $r^{(t+1)} = \\beta_{t+1} u$ with $\\beta_{t+1} = \\beta_{t} (1 - \\alpha^{2})$.\n\nBy induction, for all $k \\geq 0$,\n$$\nr^{(k)} = \\beta_{k} u \\quad \\text{with} \\quad \\beta_{k} = (1 - \\alpha^{2})^{k}.\n$$\nTherefore, the support never grows beyond the first element, because $j_{t} = 1$ for all $t$.\n\nFinally, the Euclidean norm of the residual after $k$ iterations is\n$$\n\\| r^{(k)} \\|_{2} = \\| \\beta_{k} u \\|_{2} = | \\beta_{k} | \\| u \\|_{2} = | 1 - \\alpha^{2} |^{k} \\cdot 1 = | 1 - \\alpha^{2} |^{k}.\n$$\nSince $\\alpha > 1$, we have $|1 - \\alpha^{2}| = \\alpha^{2} - 1$, and thus\n$$\n\\| r^{(k)} \\|_{2} = (\\alpha^{2} - 1)^{k}.\n$$\nThis explicitly demonstrates the residual behavior over iterations and shows that the Matching Pursuit selection never adds a new support element beyond the first for this pathological, non-normalized dictionary.",
            "answer": "$$\\boxed{(\\alpha^{2}-1)^{k}}$$"
        },
        {
            "introduction": "While atom normalization is a necessary first step, it is not sufficient to guarantee that Matching Pursuit will succeed. The performance of greedy algorithms is deeply tied to the geometric structure of the dictionary, often characterized by metrics like Mutual Coherence and the Restricted Isometry Property (RIP). This practice challenges you to build a well-known \"worst-case\" scenario where MP fails on its very first step, incorrectly selecting an atom outside the true support of the signal . By analyzing this construction, you will directly compute key theoretical constants and see firsthand why coherence-based conditions, which govern pairwise atom relationships, provide a sharper and more direct explanation for the failure of a single greedy step than broader RIP-based conditions.",
            "id": "3458968",
            "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with unit-norm columns and a $k$-sparse signal $x^{\\star} \\in \\mathbb{R}^{n}$ with support $S \\subset \\{1,2,\\dots,n\\}$ satisfying $|S|=k$. Let Matching Pursuit (MP) denote the greedy procedure that, at the first iteration, selects the column of $A$ that maximizes the absolute inner product with the residual $r^{(0)}=y$, where $y = A x^{\\star}$. Define the Mutual Coherence (MC) $\\mu(A)$ as $\\mu(A)=\\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$, the Restricted Isometry Property (RIP) constant $\\delta_{s}$ of order $s$ as the smallest nonnegative number such that for all $s$-sparse vectors $z$, $(1-\\delta_{s})\\|z\\|_{2}^{2} \\leq \\|A z\\|_{2}^{2} \\leq (1+\\delta_{s})\\|z\\|_{2}^{2}$, and the Restricted Orthogonality constant $\\theta_{s,t}$ as the smallest nonnegative number such that for all vectors $u,v$ with disjoint supports and $| \\operatorname{supp}(u) | \\leq s$, $| \\operatorname{supp}(v) | \\leq t$, one has $|\\langle A u, A v \\rangle| \\leq \\theta_{s,t} \\|u\\|_{2} \\|v\\|_{2}$. It is known that $\\theta_{s,t} \\leq \\delta_{s+t}$.\n\nConstruct $A$ and $x^{\\star}$ as follows:\n- Let $S$ be a set of $k$ indices and let $\\{a_{i}\\}_{i \\in S}$ be orthonormal columns of $A$, i.e., $\\langle a_{i}, a_{j} \\rangle = 0$ for $i \\neq j$ and $\\|a_{i}\\|_{2}=1$ for all $i \\in S$.\n- Let there exist an index $\\ell \\notin S$ such that $a_{\\ell} = \\frac{1}{\\sqrt{k}} \\sum_{i \\in S} a_{i}$.\n- Let $x^{\\star}_{i} = \\alpha$ for all $i \\in S$, with $\\alpha > 0$, and $x^{\\star}_{j} = 0$ for all $j \\notin S$.\n\nTasks:\n1. Using the definitions above, derive the first-iteration selection criterion of MP in terms of inner products with $y$ and show, for your construction, that MP selects $a_{\\ell}$ rather than any $a_{i}$ with $i \\in S$.\n2. Compute the Restricted Isometry Property constant $\\delta_{k}$ and the Restricted Isometry Property constant $\\delta_{k+1}$ for your constructed $A$. Establish their values.\n3. Compute the Mutual Coherence $\\mu(A)$ for your constructed $A$.\n4. A standard coherence-based sufficient condition for MP to select a correct atom on the first iteration (in the equal-amplitude case) can be written in terms of $\\mu(A)$ and $k$ as $\\mu(A) < \\frac{1}{2k-1}$. Define the factor\n$$\nG(k) \\triangleq \\frac{\\mu(A)}{\\frac{1}{2k-1}},\n$$\nwhich quantifies how much larger the actual mutual coherence of your construction is compared to the coherence-based success threshold. Provide a closed-form expression for $G(k)$ in terms of $k$.\n\nYour final answer must be the analytic expression for $G(k)$, written in exact form. No rounding is required, and no units are involved.",
            "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in the theory of compressed sensing, well-posed with a clear objective, and internally consistent. The provided definitions and constructed entities are standard in the analysis of greedy algorithms like Matching Pursuit. The construction is a well-known counterexample illustrating the limitations of such algorithms. We will proceed with the four tasks as outlined.\n\nThe provided construction implicitly requires the sparsity level $k$ to be greater than $1$. If $k=1$, let $S=\\{j\\}$. The construction dictates $a_\\ell = \\frac{1}{\\sqrt{1}} \\sum_{i \\in \\{j\\}} a_i = a_j$. However, the problem specifies that $\\ell \\notin S$, leading to the contradiction $\\ell \\neq j$ and $\\ell=j$. Therefore, we must have $k > 1$.\n\n**Task 1: First-Iteration Selection of Matching Pursuit**\n\nThe Matching Pursuit (MP) algorithm, at its first iteration, selects an atom (a column of $A$) that is most correlated with the measurement vector $y$. The selection criterion is to find the index $j$ that maximizes the absolute value of the inner product $|\\langle a_j, y \\rangle|$. The initial residual is $r^{(0)} = y$.\n\nThe measurement vector $y$ is given by $y = A x^{\\star}$. Given the structure of $x^{\\star}$, which has non-zero entries only on the support set $S$, we can write:\n$$\ny = \\sum_{j=1}^{n} x^{\\star}_j a_j = \\sum_{i \\in S} x^{\\star}_i a_i\n$$\nSince $x^{\\star}_i = \\alpha$ for all $i \\in S$, this simplifies to:\n$$\ny = \\sum_{i \\in S} \\alpha a_i = \\alpha \\sum_{i \\in S} a_i\n$$\nNow, we evaluate the inner product for two cases: when the atom is in the true support $S$, and when the atom is the constructed incorrect atom $a_{\\ell}$.\n\nCase 1: Atom $a_j$ is in the support, i.e., $j \\in S$.\nThe inner product is:\n$$\n\\langle a_j, y \\rangle = \\left\\langle a_j, \\alpha \\sum_{i \\in S} a_i \\right\\rangle = \\alpha \\sum_{i \\in S} \\langle a_j, a_i \\rangle\n$$\nBy construction, the set of columns $\\{a_i\\}_{i \\in S}$ is orthonormal. This means $\\langle a_j, a_i \\rangle = \\delta_{ji}$, where $\\delta_{ji}$ is the Kronecker delta. The sum therefore collapses to the single term where $i=j$:\n$$\n\\langle a_j, y \\rangle = \\alpha \\langle a_j, a_j \\rangle = \\alpha \\|a_j\\|_2^2\n$$\nSince all columns of $A$ are unit-norm, $\\|a_j\\|_2 = 1$. Thus, for any $j \\in S$:\n$$\n|\\langle a_j, y \\rangle| = \\alpha\n$$\n\nCase 2: The atom is $a_{\\ell}$, where $\\ell \\notin S$.\nThe inner product is:\n$$\n\\langle a_{\\ell}, y \\rangle = \\left\\langle a_{\\ell}, \\alpha \\sum_{i \\in S} a_i \\right\\rangle\n$$\nSubstituting the definition of $a_{\\ell} = \\frac{1}{\\sqrt{k}} \\sum_{j \\in S} a_j$:\n$$\n\\langle a_{\\ell}, y \\rangle = \\left\\langle \\frac{1}{\\sqrt{k}} \\sum_{j \\in S} a_j, \\alpha \\sum_{i \\in S} a_i \\right\\rangle = \\frac{\\alpha}{\\sqrt{k}} \\left\\langle \\sum_{j \\in S} a_j, \\sum_{i \\in S} a_i \\right\\rangle\n$$\nThe inner product term is $\\left\\langle \\sum_{j \\in S} a_j, \\sum_{i \\in S} a_i \\right\\rangle = \\sum_{j \\in S} \\sum_{i \\in S} \\langle a_j, a_i \\rangle = \\sum_{j \\in S} \\sum_{i \\in S} \\delta_{ji} = \\sum_{j \\in S} 1 = k$.\nSo, the inner product becomes:\n$$\n\\langle a_{\\ell}, y \\rangle = \\frac{\\alpha}{\\sqrt{k}} \\cdot k = \\alpha \\sqrt{k}\n$$\nThe magnitude is:\n$$\n|\\langle a_{\\ell}, y \\rangle| = \\alpha \\sqrt{k}\n$$\nTo show that MP selects $a_{\\ell}$, we compare the magnitudes. We must show $|\\langle a_{\\ell}, y \\rangle| > |\\langle a_j, y \\rangle|$ for $j \\in S$.\n$$\n\\alpha \\sqrt{k} > \\alpha\n$$\nSince $\\alpha > 0$, we can divide by $\\alpha$ to get $\\sqrt{k} > 1$, which is true for all $k>1$. As established, the problem setup requires $k>1$. Thus, MP selects the incorrect atom $a_{\\ell}$ over any correct atom $a_j$ from the true support $S$.\n\n**Task 2: Computation of RIP Constants**\n\nThe properties of the matrix $A$ are determined by the constructed columns. We analyze the submatrix $A'$ comprising the $k+1$ columns $\\{a_i\\}_{i \\in S} \\cup \\{a_\\ell\\}$. The RIP constants are determined by the eigenvalues of Gram matrices of submatrices of $A'$.\n\nComputation of $\\delta_k$:\nThe constant $\\delta_k$ is defined by the extremal eigenvalues of all $k \\times k$ Gram submatrices. We consider two types of submatrices of size $k$:\n1. The submatrix $A_S$ with columns $\\{a_i\\}_{i \\in S}$. Its Gram matrix is $A_S^T A_S = I_k$, as the columns are orthonormal. All eigenvalues are $1$.\n2. Any submatrix $A_{T_j}$ formed by replacing one column $a_j$ from $A_S$ with $a_\\ell$. Let $T_j = (S \\setminus \\{j\\}) \\cup \\{\\ell\\}$ for any $j \\in S$. The Gram matrix $G_{T_j} = A_{T_j}^T A_{T_j}$ has entries $\\langle a_p, a_q \\rangle$ for $p, q \\in T_j$. The inner products are $\\langle a_p, a_q \\rangle = \\delta_{pq}$ for $p,q \\in S \\setminus \\{j\\}$, $\\|a_\\ell\\|_2^2=1$, and for $p \\in S \\setminus \\{j\\}$, $\\langle a_p, a_\\ell \\rangle = \\langle a_p, \\frac{1}{\\sqrt{k}} \\sum_{i \\in S} a_i \\rangle = \\frac{1}{\\sqrt{k}}$.\nThe Gram matrix has the form $G = \\begin{pmatrix} I_{k-1} & \\mathbf{v} \\\\ \\mathbf{v}^T & 1 \\end{pmatrix}$, where $\\mathbf{v}$ is a vector of size $k-1$ with all entries equal to $1/\\sqrt{k}$. The eigenvalues of this matrix are $1$ (with multiplicity $k-2$) and $1 \\pm \\|\\mathbf{v}\\|_2$. Here, $\\|\\mathbf{v}\\|_2^2 = (k-1) (1/\\sqrt{k})^2 = \\frac{k-1}{k}$. So the other two eigenvalues are $1 \\pm \\sqrt{\\frac{k-1}{k}}$.\nTo find $\\delta_k$, we take the most extreme eigenvalues over all $k \\times k$ submatrices.\n$1+\\delta_k = \\max(1, 1+\\sqrt{\\frac{k-1}{k}}) = 1+\\sqrt{\\frac{k-1}{k}}$.\n$1-\\delta_k = \\min(1, 1-\\sqrt{\\frac{k-1}{k}}) = 1-\\sqrt{\\frac{k-1}{k}}$.\nBoth equations yield $\\delta_k = \\sqrt{\\frac{k-1}{k}}$.\n\nComputation of $\\delta_{k+1}$:\nThere is only one submatrix of size $k+1$, which is $A'$ itself. We need the eigenvalues of its Gram matrix $G' = (A')^T A'$. Let the columns be ordered as $\\{a_i\\}_{i \\in S}$ followed by $a_\\ell$. The Gram matrix is $G' = \\begin{pmatrix} I_k & \\mathbf{c} \\\\ \\mathbf{c}^T & 1 \\end{pmatrix}$, where $\\mathbf{c}$ is a vector of size $k$ with entries $\\langle a_i, a_\\ell \\rangle = 1/\\sqrt{k}$ for $i \\in S$.\nThe eigenvalues are $1$ (with multiplicity $k-1$) and $1 \\pm \\|\\mathbf{c}\\|_2$. Here, $\\|\\mathbf{c}\\|_2^2 = k (1/\\sqrt{k})^2 = 1$. The other two eigenvalues are $1 \\pm 1$, which are $2$ and $0$.\nThe eigenvalues of $G'$ are $2$ (multiplicity $1$), $0$ (multiplicity $1$), and $1$ (multiplicity $k-1$).\nThen $1+\\delta_{k+1} = \\lambda_{\\max}(G') = 2$, which gives $\\delta_{k+1}=1$.\nAnd $1-\\delta_{k+1} = \\lambda_{\\min}(G') = 0$, which also gives $\\delta_{k+1}=1$.\nThus, $\\delta_{k+1}=1$. This signifies that there exists a non-zero $(k+1)$-sparse vector in the null space of $A$.\n\n**Task 3: Computation of Mutual Coherence**\n\nThe Mutual Coherence $\\mu(A)$ is the maximum absolute inner product between distinct columns of $A$. We evaluate this for the constructed columns.\nWe check the inner products between pairs of columns from the set $\\{a_i\\}_{i \\in S} \\cup \\{a_\\ell\\}$.\n1. For distinct $i,j \\in S$: $|\\langle a_i, a_j \\rangle| = |0| = 0$ due to orthonormality.\n2. For any $i \\in S$:\n$$\n|\\langle a_i, a_\\ell \\rangle| = \\left| \\left\\langle a_i, \\frac{1}{\\sqrt{k}} \\sum_{j \\in S} a_j \\right\\rangle \\right| = \\left| \\frac{1}{\\sqrt{k}} \\sum_{j \\in S} \\langle a_i, a_j \\rangle \\right| = \\left| \\frac{1}{\\sqrt{k}} \\langle a_i, a_i \\rangle \\right| = \\frac{1}{\\sqrt{k}}\n$$\nThe maximum of these values determines the mutual coherence of the constructed submatrix.\n$$\n\\mu(A) = \\max(\\{0, \\frac{1}{\\sqrt{k}}\\}) = \\frac{1}{\\sqrt{k}}\n$$\n\n**Task 4: Calculation of the Factor $G(k)$**\n\nThe problem defines the factor $G(k)$ as the ratio of the actual mutual coherence of our construction to the threshold from a standard sufficient condition for MP success:\n$$\nG(k) \\triangleq \\frac{\\mu(A)}{\\frac{1}{2k-1}}\n$$\nUsing the value of $\\mu(A)$ we found in Task 3:\n$$\nG(k) = \\frac{\\frac{1}{\\sqrt{k}}}{\\frac{1}{2k-1}} = \\frac{2k-1}{\\sqrt{k}}\n$$\nThis is the closed-form expression for $G(k)$.",
            "answer": "$$\n\\boxed{\\frac{2k-1}{\\sqrt{k}}}\n$$"
        },
        {
            "introduction": "Moving from idealized theory to real-world application requires confronting the challenge of measurement noise. When our signal is corrupted, $y = D x^{\\star} + w$, the core task of Matching Pursuit—selecting the atom most correlated with the signal—becomes ambiguous. Is a large correlation due to a true signal component, or is it merely an artifact of random noise? This final practice addresses this critical question by guiding you through a statistical analysis of the noise-only correlations . Using fundamental principles of Gaussian random variables, you will derive a high-probability \"universal threshold,\" a vital tool that provides a principled basis for deciding when a correlation is significant enough to be considered signal, thereby preventing the algorithm from chasing noise.",
            "id": "3458934",
            "problem": "Consider a noisy linear model in compressed sensing and sparse optimization given by $y = D x^\\star + w$, where $D \\in \\mathbb{R}^{m \\times p}$ has column atoms $\\{d_j\\}_{j=1}^p$ satisfying $\\|d_j\\|_2 = 1$ for all $j \\in \\{1,\\dots,p\\}$, and $w \\sim \\mathcal{N}(0,\\sigma^2 I_m)$ is Gaussian noise with variance parameter $\\sigma^2 > 0$. In the first step of a greedy selection rule such as Matching Pursuit (MP), one considers the statistic $M := \\max_{1 \\le j \\le p} |\\langle w, d_j \\rangle|$ to assess the size of noise-only correlations. Your goal is to characterize the law of $M$ and to derive a high-probability stopping threshold that controls false selections.\n\nStarting only from fundamental facts about Gaussian vectors and their linear functionals, and without assuming any special structure beyond the stated normalization, do the following:\n\n1. Argue that for each $j$, the scalar $\\langle w, d_j \\rangle$ is a mean-zero Gaussian with variance $\\sigma^2$, and identify the dependence structure across indices $j$ via the covariance $\\mathrm{Cov}(\\langle w, d_i \\rangle, \\langle w, d_j \\rangle)$.\n\n2. In the special case that the atoms are orthonormal, i.e., $D^\\top D = I_p$ and hence the scalars are independent, derive the exact cumulative distribution function of $M$.\n\n3. For a general normalized dictionary $D$ (not necessarily orthonormal), use Gaussian maxima principles together with elementary probability inequalities to produce a universal bound of the form $\\mathbb{P}(M \\ge t) \\le \\varepsilon(t)$ that does not depend on the specific correlations of $D$, and solve for the smallest $t$ in closed form such that $\\mathbb{P}(M \\ge t) \\le \\delta$, where $\\delta \\in (0,1)$ is a user-specified tolerance. Interpret this $t$ as a noise-only MP stopping threshold.\n\nProvide the final answer as the explicit symbolic expression for the universal threshold $t$ in terms of $\\sigma$, $p$, and $\\delta$. No numerical evaluation is required. Do not include any inequalities in your final answer; report only the closed-form $t(\\sigma,p,\\delta)$.",
            "solution": "The problem statement provides a valid, well-posed, and scientifically grounded question in the field of sparse optimization and compressed sensing. We may proceed with a full solution.\n\nThe problem asks for a three-part analysis of the statistic $M := \\max_{1 \\le j \\le p} |\\langle w, d_j \\rangle|$, where $w$ is a Gaussian noise vector. Let us define the random variables $Z_j := \\langle w, d_j \\rangle = d_j^\\top w$ for $j \\in \\{1,\\dots,p\\}$. The statistic is then $M = \\max_{1 \\le j \\le p} |Z_j|$.\n\n**Part 1: Distributional Properties of $Z_j$**\n\nWe are given that the noise vector $w$ follows a multivariate normal distribution, $w \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, where $I_m$ is the $m \\times m$ identity matrix. The quantities $Z_j = d_j^\\top w$ are linear functionals of the Gaussian vector $w$. A fundamental property of multivariate Gaussian distributions is that any linear transformation of a Gaussian vector results in a vector that is also Gaussian. Therefore, the vector $(Z_1, \\dots, Z_p)^\\top = D^\\top w$ is a multivariate Gaussian.\n\nFirst, we determine the mean of each $Z_j$:\n$$\n\\mathbb{E}[Z_j] = \\mathbb{E}[d_j^\\top w] = d_j^\\top \\mathbb{E}[w]\n$$\nSince $\\mathbb{E}[w] = 0$, we have:\n$$\n\\mathbb{E}[Z_j] = d_j^\\top 0 = 0\n$$\nThus, each $Z_j$ is a mean-zero random variable.\n\nNext, we find the variance of $Z_j$. The covariance matrix of $w$ is $\\mathrm{Cov}(w) = \\mathbb{E}[ww^\\top] - \\mathbb{E}[w]\\mathbb{E}[w]^\\top = \\sigma^2 I_m$.\n$$\n\\mathrm{Var}(Z_j) = \\mathbb{E}[(Z_j - \\mathbb{E}[Z_j])^2] = \\mathbb{E}[Z_j^2] = \\mathbb{E}[(d_j^\\top w)(w^\\top d_j)] = d_j^\\top \\mathbb{E}[ww^\\top] d_j\n$$\nSubstituting $\\mathbb{E}[ww^\\top] = \\sigma^2 I_m$:\n$$\n\\mathrm{Var}(Z_j) = d_j^\\top (\\sigma^2 I_m) d_j = \\sigma^2 (d_j^\\top d_j) = \\sigma^2 \\|d_j\\|_2^2\n$$\nThe problem states that all atoms are normalized, i.e., $\\|d_j\\|_2 = 1$ for all $j$. Therefore,\n$$\n\\mathrm{Var}(Z_j) = \\sigma^2\n$$\nSo, for each $j$, $Z_j$ is a mean-zero Gaussian with variance $\\sigma^2$, i.e., $Z_j \\sim \\mathcal{N}(0, \\sigma^2)$.\n\nFinally, we identify the dependence structure by calculating the covariance between $Z_i$ and $Z_j$ for any $i,j \\in \\{1,\\dots,p\\}$:\n$$\n\\mathrm{Cov}(Z_i, Z_j) = \\mathbb{E}[(Z_i - \\mathbb{E}[Z_i])(Z_j - \\mathbb{E}[Z_j])] = \\mathbb{E}[Z_i Z_j] = \\mathbb{E}[(d_i^\\top w)(w^\\top d_j)]\n$$\n$$\n\\mathrm{Cov}(Z_i, Z_j) = d_i^\\top \\mathbb{E}[ww^\\top] d_j = d_i^\\top (\\sigma^2 I_m) d_j = \\sigma^2 (d_i^\\top d_j)\n$$\nThe covariance is $\\sigma^2 \\langle d_i, d_j \\rangle$. This shows that the correlation between the random variables $Z_i$ and $Z_j$ is directly proportional to the inner product (i.e., cosine similarity) of the corresponding dictionary atoms $d_i$ and $d_j$.\n\n**Part 2: Exact CDF of $M$ for Orthonormal Atoms**\n\nIn the special case that the atoms are orthonormal, the dictionary matrix $D$ satisfies $D^\\top D = I_p$. This implies that the inner product $\\langle d_i, d_j \\rangle$ is equal to $1$ if $i=j$ and $0$ if $i \\ne j$. Let $\\delta_{ij}$ be the Kronecker delta.\nFrom the covariance result in Part 1:\n$$\n\\mathrm{Cov}(Z_i, Z_j) = \\sigma^2 \\langle d_i, d_j \\rangle = \\sigma^2 \\delta_{ij}\n$$\nThis means the random variables $\\{Z_j\\}_{j=1}^p$ are uncorrelated. Since they are jointly Gaussian, this implies they are also statistically independent. Furthermore, from Part 1, they are identically distributed as $\\mathcal{N}(0, \\sigma^2)$.\n\nWe wish to find the cumulative distribution function (CDF) of $M = \\max_{1 \\le j \\le p} |Z_j|$, which is $F_M(t) = \\mathbb{P}(M \\le t)$.\n$$\nF_M(t) = \\mathbb{P}(\\max_{1 \\le j \\le p} |Z_j| \\le t) = \\mathbb{P}(|Z_1| \\le t, |Z_2| \\le t, \\dots, |Z_p| \\le t)\n$$\nDue to the independence of the $Z_j$ variables, the joint probability is the product of the marginal probabilities:\n$$\nF_M(t) = \\prod_{j=1}^p \\mathbb{P}(|Z_j| \\le t)\n$$\nSince all $Z_j$ are identically distributed, this simplifies to:\n$$\nF_M(t) = \\left( \\mathbb{P}(|Z_1| \\le t) \\right)^p\n$$\nLet $\\Phi(\\cdot)$ denote the CDF of a standard normal variable $\\mathcal{N}(0,1)$. A variable $Z_1 \\sim \\mathcal{N}(0, \\sigma^2)$ can be written as $Z_1 = \\sigma X$ where $X \\sim \\mathcal{N}(0,1)$. The probability $\\mathbb{P}(|Z_1| \\le t)$ for $t \\ge 0$ is:\n$$\n\\mathbb{P}(|Z_1| \\le t) = \\mathbb{P}(-t \\le Z_1 \\le t) = \\mathbb{P}\\left(-\\frac{t}{\\sigma} \\le X \\le \\frac{t}{\\sigma}\\right)\n$$\n$$\n= \\Phi\\left(\\frac{t}{\\sigma}\\right) - \\Phi\\left(-\\frac{t}{\\sigma}\\right)\n$$\nUsing the symmetry property $\\Phi(-x) = 1 - \\Phi(x)$, this becomes:\n$$\n\\mathbb{P}(|Z_1| \\le t) = \\Phi\\left(\\frac{t}{\\sigma}\\right) - \\left(1 - \\Phi\\left(\\frac{t}{\\sigma}\\right)\\right) = 2\\Phi\\left(\\frac{t}{\\sigma}\\right) - 1\n$$\nSubstituting this back, the exact CDF of $M$ for $t \\ge 0$ in the orthonormal case is:\n$$\nF_M(t) = \\left( 2\\Phi\\left(\\frac{t}{\\sigma}\\right) - 1 \\right)^p\n$$\n\n**Part 3: Universal Threshold for a General Dictionary**\n\nFor a general dictionary $D$ where the atoms are not necessarily orthogonal, the variables $\\{Z_j\\}$ are correlated, and the exact CDF of their maximum is generally intractable. We seek a universal high-probability bound on $M$, meaning a bound that holds irrespective of the specific correlation structure of $D$.\n\nOur goal is to find the smallest threshold $t$ such that $\\mathbb{P}(M \\ge t) \\le \\delta$ for a given tolerance $\\delta \\in (0,1)$. We begin by applying the union bound (Boole's inequality), which is an elementary probability inequality:\n$$\n\\mathbb{P}(M \\ge t) = \\mathbb{P}(\\max_{1 \\le j \\le p} |Z_j| \\ge t) = \\mathbb{P}\\left(\\bigcup_{j=1}^p \\{|Z_j| \\ge t\\}\\right) \\le \\sum_{j=1}^p \\mathbb{P}(|Z_j| \\ge t)\n$$\nAs established in Part 1, all $Z_j \\sim \\mathcal{N}(0, \\sigma^2)$ have the same distribution. Thus, the bound simplifies to:\n$$\n\\mathbb{P}(M \\ge t) \\le p \\cdot \\mathbb{P}(|Z_1| \\ge t)\n$$\nNext, we bound the tail probability $\\mathbb{P}(|Z_1| \\ge t)$. By symmetry of the Gaussian distribution, for $t>0$:\n$$\n\\mathbb{P}(|Z_1| \\ge t) = \\mathbb{P}(Z_1 \\ge t) + \\mathbb{P}(Z_1 \\le -t) = 2\\mathbb{P}(Z_1 \\ge t)\n$$\nLet $X = Z_1/\\sigma \\sim \\mathcal{N}(0,1)$. Then $\\mathbb{P}(Z_1 \\ge t) = \\mathbb{P}(X \\ge t/\\sigma)$. We use a standard Chernoff bound for the tail of a standard Gaussian. For any $x \\ge 0$, and any $\\lambda > 0$, Markov's inequality states that $\\mathbb{P}(X \\ge x) = \\mathbb{P}(e^{\\lambda X} \\ge e^{\\lambda x}) \\le e^{-\\lambda x}\\mathbb{E}[e^{\\lambda X}]$. The moment-generating function of a standard normal is $\\mathbb{E}[e^{\\lambda X}] = e^{\\lambda^2/2}$. Optimizing the bound $e^{\\lambda^2/2 - \\lambda x}$ over $\\lambda > 0$ yields the tightest exponent at $\\lambda = x$, giving:\n$$\n\\mathbb{P}(X \\ge x) \\le e^{-x^2/2}\n$$\nApplying this bound with $x = t/\\sigma$:\n$$\n\\mathbb{P}(Z_1 \\ge t) \\le \\exp\\left(-\\frac{t^2}{2\\sigma^2}\\right)\n$$\nTherefore, the two-sided tail probability is bounded by:\n$$\n\\mathbb{P}(|Z_1| \\ge t) \\le 2\\exp\\left(-\\frac{t^2}{2\\sigma^2}\\right)\n$$\nSubstituting this into our union bound expression gives the universal bound $\\varepsilon(t)$:\n$$\n\\mathbb{P}(M \\ge t) \\le 2p \\exp\\left(-\\frac{t^2}{2\\sigma^2}\\right)\n$$\nTo find the required threshold $t$, we enforce that this upper bound is less than or equal to the desired tolerance $\\delta$. The smallest such $t$ will satisfy the equality:\n$$\n2p \\exp\\left(-\\frac{t^2}{2\\sigma^2}\\right) = \\delta\n$$\nWe now solve for $t$:\n$$\n\\exp\\left(-\\frac{t^2}{2\\sigma^2}\\right) = \\frac{\\delta}{2p}\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\frac{t^2}{2\\sigma^2} = \\ln\\left(\\frac{\\delta}{2p}\\right) = -\\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\n$$\nt^2 = 2\\sigma^2 \\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\nAssuming $2p/\\delta \\ge 1$, which is reasonable as typically $p \\gg 1$ and $\\delta \\ll 1$, we can take the square root of both sides to get the final expression for the threshold:\n$$\nt = \\sigma \\sqrt{2 \\ln\\left(\\frac{2p}{\\delta}\\right)}\n$$\nThis threshold, often called the universal threshold, ensures that the probability of the maximum noise correlation exceeding $t$ is no greater than $\\delta$. It correctly scales with the noise level $\\sigma$, increases with the number of atoms $p$ (to account for multiple comparisons), and increases as the tolerance $\\delta$ becomes stricter (smaller).",
            "answer": "$$\\boxed{\\sigma \\sqrt{2 \\ln\\left(\\frac{2p}{\\delta}\\right)}}$$"
        }
    ]
}