{
    "hands_on_practices": [
        {
            "introduction": "Analyzing an algorithm's performance is not just about calculating its speed but also understanding its limitations. This first exercise examines a classic greedy heuristic for the 0/1 Knapsack problem . By determining its time complexity and providing a counterexample to its optimality, you will practice distinguishing between an algorithm's efficiency and its correctness—a critical skill in algorithm design.",
            "id": "3279100",
            "problem": "Consider the $0/1$ knapsack problem with $n$ items, where item $i$ has positive integer weight $w_i$ and positive integer value $v_i$, and a knapsack of capacity $W$. A commonly proposed greedy heuristic is: compute the value-to-weight ratio $r_i = v_i / w_i$ for each item, sort the items in nonincreasing order of $r_i$, and then scan this order once, adding an item if and only if it fits without exceeding the remaining capacity, skipping otherwise. Using only fundamental definitions of asymptotic time complexity, the comparison-based sorting model, and the precise description of this greedy procedure, determine which statement best characterizes both the asymptotic time complexity of this greedy heuristic and a concrete reason it does not solve the $0/1$ knapsack problem optimally in general.\n\nSelect the single best option.\n\nA. The greedy heuristic requires sorting by $r_i$, which takes $O(n \\log n)$ time under comparison-based sorting, followed by a single linear pass of $O(n)$ time, for a total of $O(n \\log n)$. It is not optimal for the $0/1$ case; for example, with $W = 50$ and items $(w,v) \\in \\{(10,60),(20,100),(30,120)\\}$, the greedy-by-ratio order is $(10,60)$, then $(20,100)$, then $(30,120)$, yielding value $60+100=160$, whereas the optimal solution selects $(20,100)$ and $(30,120)$ for value $220$.\n\nB. The greedy heuristic runs in $O(n)$ time because it makes a single pass, and it is optimal whenever all ratios $r_i$ are distinct since the order is then uniquely determined.\n\nC. The greedy heuristic runs in $O(n \\log n)$ time, and it fails to be optimal only when there are ties in the ratios $r_i$; if all $r_i$ are distinct, then it is optimal.\n\nD. The greedy heuristic runs in $O(n^2)$ time because it must consider many combinations, and it fails even on small inputs; for instance, with $W = 50$ and items $(w,v) \\in \\{(10,60),(20,100),(30,120)\\}$, the heuristic first takes $(30,120)$ by its highest ratio and then cannot take any other, giving value $120$, but the optimal value is $220$.",
            "solution": "We begin from first principles. The $0/1$ knapsack problem provides a set of $n$ items with weights $w_i$ and values $v_i$, and a capacity $W$. The greedy heuristic described computes $r_i = v_i / w_i$, sorts items by nonincreasing $r_i$, and then makes a single pass, taking each item if it fits in the remaining capacity.\n\nTime complexity analysis:\n- Computing $r_i$ for all $i \\in \\{1,\\dots,n\\}$ is $O(n)$ arithmetic operations.\n- Sorting $n$ items by their real-valued keys $r_i$ in the comparison-based model requires, in the worst case, $\\Omega(n \\log n)$ comparisons by the classical lower bound for comparison-based sorting, and standard algorithms (such as mergesort or heapsort) achieve $O(n \\log n)$. Therefore, the sorting step is $\\Theta(n \\log n)$.\n- The subsequent single scan that attempts to add each item is $O(n)$.\nCombining these, the overall time complexity is dominated by the sorting step, which is $O(n \\log n)$.\n\nWhy the heuristic is not optimal for the $0/1$ case:\nThe greedy-by-ratio choice is provably optimal for the fractional knapsack problem, where one may take any fraction of an item, because an exchange argument shows that any deviation from the nonincreasing ratio solution cannot improve total value. However, in the $0/1$ variant, items are indivisible, so the exchange argument breaks down. A standard counterexample demonstrates failure even when ratios are distinct.\n\nConsider $W = 50$ and three items:\n- Item $1$: $(w_1,v_1) = (10,60)$ with $r_1 = 60/10 = 6$.\n- Item $2$: $(w_2,v_2) = (20,100)$ with $r_2 = 100/20 = 5$.\n- Item $3$: $(w_3,v_3) = (30,120)$ with $r_3 = 120/30 = 4$.\n\nThe greedy-by-ratio order is item $1$, then item $2$, then item $3$. The heuristic takes item $1$ (remaining capacity $40$), takes item $2$ (remaining capacity $20$), and cannot take item $3$ (would exceed $W$). The total value is $60 + 100 = 160$. The optimal $0/1$ solution takes items $2$ and $3$ with total weight $20 + 30 = 50$ and value $100 + 120 = 220$, which is strictly larger than $160$. Therefore, the greedy heuristic fails to produce an optimal solution.\n\nOption-by-option analysis:\n- Option A: It states the sorting step and single pass, leading to total time $O(n \\log n)$. This matches the derivation above. It provides the explicit counterexample with $W = 50$ and items $(10,60)$, $(20,100)$, $(30,120)$ and correctly explains the greedy choice and the superior optimal solution. Verdict — Correct.\n\n- Option B: It claims $O(n)$ time. That ignores the necessity of ordering by $r_i$, which in the comparison-based model requires $\\Omega(n \\log n)$. Furthermore, the claim of optimality when ratios are distinct is false: the counterexample above has distinct ratios $6$, $5$, and $4$, yet the heuristic is suboptimal. Verdict — Incorrect, both for time complexity and optimality.\n\n- Option C: It gives the correct $O(n \\log n)$ time but asserts failure only in the presence of ties in $r_i$. As shown, failure occurs even when all $r_i$ are distinct. Verdict — Incorrect.\n\n- Option D: It claims $O(n^2)$ time, which contradicts the $O(n \\log n)$ analysis. Its purported counterexample also misidentifies the greedy choice: with ratios $6$, $5$, and $4$, the heuristic would not first take $(30,120)$; it would first take $(10,60)$. Thus both the time complexity and the described behavior are incorrect. Verdict — Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "While tools like the Master Theorem provide a quick way to solve many divide-and-conquer recurrences, they don't cover all cases. This practice challenges you to analyze a recurrence whose non-recursive work term, $f(n) = n / \\ln n$, falls into a well-known gap in the theorem . By unrolling the recurrence using a recursion tree, you will gain a more fundamental understanding of how to derive an algorithm's complexity from first principles.",
            "id": "3279114",
            "problem": "Consider a divide-and-conquer (DC) procedure whose running time is captured by the recurrence\n$$T(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+\\frac{n}{\\ln n},$$\nfor all integer inputs $n \\geq 2$ that are powers of $2$, with the base case $T(1)=\\Theta(1)$. Here $\\ln$ denotes the natural logarithm. In standard analyses, the Master Theorem (MT) does not directly handle the case where the non-recursive work term is $f(n)=n/\\ln n$.\n\nStarting from the core definitions of asymptotic growth and the structure of D recurrences, and using only well-tested analysis tools (such as rigorous recursion-tree summation and integral approximation arguments), derive the tight asymptotic order of growth of $T(n)$ as $n \\to \\infty$. Express your final answer as a single standard complexity-class expression in terms of $n$ and elementary functions. No numerical rounding is required.",
            "solution": "We are tasked with finding the tight asymptotic order of growth for the recurrence relation\n$$T(n) = 2 T\\left(\\frac{n}{2}\\right) + \\frac{n}{\\ln n}$$\nwith the base case $T(1) = \\Theta(1)$, for values of $n$ that are integer powers of $2$, i.e., $n=2^k$ for some integer $k \\ge 0$.\n\nWe will use the recursion-tree method to solve this recurrence. Let $n=2^k$, which implies $k = \\log_2(n)$. The depth of the recursion tree is $k$.\n\nAt each level $i$ of the recursion tree, where the root is at level $i=0$, there are $2^i$ subproblems. The size of each subproblem at level $i$ is $n_i = n/2^i$. The work done at each node at level $i$ (excluding the recursive calls) is $f(n_i) = \\frac{n_i}{\\ln n_i}$.\n\nThe total work done at level $i$ is the number of nodes times the work per node:\n$$\\text{Work at level } i = 2^i \\times f\\left(\\frac{n}{2^i}\\right) = 2^i \\times \\frac{n/2^i}{\\ln(n/2^i)} = \\frac{n}{\\ln(n) - i \\ln(2)}$$\nThe total running time $T(n)$ is the sum of the work done at all levels of the tree, from the root (level $0$) to the level just before the leaves (level $k-1$), plus the work done at the leaves (level $k$).\n\nThe number of leaves is $2^k = n$. The size of the problem at each leaf is $n/2^k = 1$. The cost at each leaf is $T(1) = \\Theta(1)$. Thus, the total cost at the leaves is $n \\times T(1) = \\Theta(n)$.\n\nThe total non-recursive work is the sum of the work at levels $i=0$ to $k-1$:\n$$\\sum_{i=0}^{k-1} \\frac{n}{\\ln(n) - i \\ln(2)}$$\nTo simplify this sum, we substitute $\\ln(n) = \\ln(2^k) = k \\ln(2)$:\n$$\\text{Sum} = \\sum_{i=0}^{k-1} \\frac{n}{k \\ln(2) - i \\ln(2)} = \\frac{n}{\\ln(2)} \\sum_{i=0}^{k-1} \\frac{1}{k-i}$$\nLet us perform a change of index in the summation. Let $j = k-i$. As $i$ goes from $0$ to $k-1$, $j$ goes from $k$ down to $1$. The sum becomes:\n$$\\text{Sum} = \\frac{n}{\\ln(2)} \\sum_{j=1}^{k} \\frac{1}{j}$$\nThe summation $\\sum_{j=1}^{k} \\frac{1}{j}$ is the $k$-th harmonic number, denoted by $H_k$. It is a well-established result that for large $k$, the harmonic series has the following asymptotic behavior:\n$$H_k = \\ln(k) + \\gamma + O\\left(\\frac{1}{k}\\right)$$\nwhere $\\gamma$ is the Euler-Mascheroni constant. Therefore, $H_k = \\Theta(\\ln k)$.\n\nSubstituting this back into the expression for the sum of non-recursive work:\n$$\\text{Sum} = \\frac{n}{\\ln(2)} H_k = \\frac{n}{\\ln(2)} \\Theta(\\ln k) = \\Theta(n \\ln k)$$\nThe total running time $T(n)$ is the sum of the leaf costs and the non-recursive work:\n$$T(n) = \\Theta(n) + \\Theta(n \\ln k)$$\nAs $n \\to \\infty$, $k = \\log_2(n) \\to \\infty$. Since $\\ln(k)$ is a growing function of $k$, the term $\\Theta(n \\ln k)$ dominates the term $\\Theta(n)$.\nThus, the asymptotic behavior of $T(n)$ is given by:\n$$T(n) = \\Theta(n \\ln k)$$\nFinally, we must express this result in terms of $n$. We substitute $k = \\log_2(n)$:\n$$T(n) = \\Theta(n \\ln(\\log_2 n))$$\nWe can simplify the logarithmic term. Using the change of base formula for logarithms, $\\log_2(n) = \\frac{\\ln(n)}{\\ln(2)}$:\n$$\\ln(\\log_2 n) = \\ln\\left(\\frac{\\ln n}{\\ln 2}\\right) = \\ln(\\ln n) - \\ln(\\ln 2)$$\nAs $n \\to \\infty$, $\\ln(\\ln n) \\to \\infty$, while $\\ln(\\ln 2)$ is a negative constant. Therefore, the term $\\ln(\\ln n)$ dominates. This means that $\\ln(\\log_2 n) = \\Theta(\\ln(\\ln n))$.\n\nSubstituting this final simplification into our expression for $T(n)$:\n$$T(n) = \\Theta(n \\cdot \\Theta(\\ln(\\ln n))) = \\Theta(n \\ln(\\ln n))$$\nThis is the tight asymptotic order of growth for the given recurrence.",
            "answer": "$$\\boxed{\\Theta(n \\ln(\\ln n))}$$"
        },
        {
            "introduction": "So far, we have focused on finding the time complexity of a given algorithm—an upper bound on a problem's difficulty. But how do we know if a better algorithm is possible? This exercise shifts our perspective to lower bounds: the inherent complexity of a problem itself . By determining the absolute minimum number of comparisons any algorithm must make to find the second-smallest element, you will explore the powerful concept of information-theoretic limits and what it means for an algorithm to be truly optimal.",
            "id": "3279201",
            "problem": "Consider an algorithm that is allowed to access an array of $n$ elements only through pairwise comparisons, each query asking whether one element is less than another. Assume the elements are all distinct and come from a strict total order, and the algorithm is adaptive: it may choose which pair to compare next based on the outcomes of prior comparisons. In this comparison-only setting, we measure query complexity as the worst-case number of pairwise comparisons needed to guarantee correctness on all inputs.\n\nStarting from the fundamental basis of the comparison decision tree (CDT), where each comparison yields one bit of information and an algorithm corresponds to a binary decision tree whose leaves represent outcomes consistent with correct identification, and the well-tested facts that a binary tree of height $h$ has at most $2^{h}$ leaves and that selecting the minimum among $k$ elements by comparisons alone requires at least $k-1$ comparisons, derive the tight worst-case minimum number of pairwise comparisons required to identify the second-smallest element of the array. Your answer must be a single closed-form analytic expression in terms of $n$. No rounding is required.",
            "solution": "The derivation proceeds in two parts: first, establishing an upper bound by constructing an algorithm, and second, proving a matching lower bound that any comparison-based algorithm must respect.\n\n#### Upper Bound\n\nWe can find the second-smallest element, which we denote as $S_2$, by first identifying the smallest element, $S_1$.\n\n1.  **Finding the Minimum ($S_1$)**: We can model the process of finding the minimum element as a single-elimination tournament. The $n$ elements are the participants. We pair them up and compare them. The winners advance to the next round. This continues until a single overall winner remains. This winner is the minimum element, $S_1$. In such a tournament, every element except the winner must lose exactly one match. Since each comparison produces one loser, exactly $n-1$ comparisons are performed to determine $S_1$.\n\n2.  **Identifying Candidates for the Second-Minimum ($S_2$)**: The second-smallest element, $S_2$, must have lost a comparison to some other element. If $S_2$ lost a comparison to an element $x$, it must be that $x = S_1$. If $x$ were any other element, we would have the relation $S_1  x  S_2$, which would mean $S_2$ is at least the third-smallest element, a contradiction. Therefore, $S_2$ must be one of the elements that was directly compared with $S_1$ and lost.\n\n3.  **Finding the Second-Minimum ($S_2$)**: Let $K$ be the set of elements that lost directly to $S_1$ during the tournament. The second-smallest element $S_2$ is the minimum element within the set $K$.\n    The number of elements in $K$, which we denote as $|K|$, is the number of opponents $S_1$ faced and defeated on its path to becoming the winner of the tournament. In a balanced tournament structure, the winner participates in $\\lceil \\log_2(n) \\rceil$ comparisons. Thus, in the worst case (for the size of $K$), $|K| = \\lceil \\log_2(n) \\rceil$.\n    To find the minimum element of the set $K$, we require $|K|-1$ additional comparisons.\n\n4.  **Total Comparisons (Upper Bound)**: The total number of comparisons for this algorithm is the sum of comparisons from step 1 and step 3:\n    $$ C_{\\text{upper}} = (n-1) + (|K|-1) $$\n    In the worst case for this algorithm, $|K| = \\lceil \\log_2(n) \\rceil$.\n    $$ C_{\\text{upper}} = (n-1) + (\\lceil \\log_2(n) \\rceil - 1) = n + \\lceil \\log_2(n) \\rceil - 2 $$\n    This establishes an upper bound on the number of comparisons required.\n\n#### Lower Bound\n\nWe now derive a lower bound on the worst-case complexity for *any* comparison-based algorithm that finds the second-smallest element.\n\n1.  **Information Required**: Any correct algorithm must acquire sufficient information to certify its output. To certify that an element $y$ is $S_2$, the algorithm must have established that:\n    a. Exactly one element, $S_1$, is smaller than $y$.\n    b. All other $n-2$ elements are larger than $y$.\n\n2.  **Counting Necessary Comparison Outcomes**:\n    - **Finding $S_1$**: To establish that an element $S_1$ is the minimum, every other element $x$ must be shown to be larger than at least one other element (i.e., must lose at least one comparison). Each comparison has only one loser, so to ensure $n-1$ distinct elements have lost, at least $n-1$ comparisons are necessary.\n    - **Finding $S_2$**: As established previously, $S_2$ must be in the set $K$ of elements that lost directly to $S_1$. Let $|K| = k$. To identify $S_2$ as the minimum of the set $K$, the algorithm must establish that $S_2$ is smaller than the other $k-1$ elements in $K$. Proving this requires at least $k-1$ comparisons among the elements of $K$.\n\n3.  **Combining the Requirements**: Let's account for the total number of comparisons, $C$.\n    - A comparison $(x,y)$ where neither $x$ nor $y$ is $S_1$ establishes a loss for one element. To ensure the $n-1-k$ elements not in $\\{S_1\\} \\cup K$ are identified as non-minimal, at least $n-1-k$ such comparisons must occur where they lose.\n    - The $k$ elements in $K$ must lose to $S_1$. This requires exactly $k$ comparisons of the form $(S_1, y)$ where $y \\in K$.\n    - The $k-1$ elements in $K \\setminus \\{S_2\\}$ must be shown to be larger than $S_2$. This requires at least $k-1$ comparisons among the elements of $K$.\n\n    The information gained from these three types of necessary events is distinct. A comparison of type $(S_1, y)$ confirms $y \\in K$. A comparison of type $(y_i, y_j)$ where $y_i, y_j \\in K$ establishes ordering within $K$, which is necessary to find its minimum. This ordering information is not provided by the comparisons involving $S_1$. Therefore, we can sum the minimum number of comparisons required for these distinct informational needs to get a lower bound on the total number of comparisons.\n    $$ C_{\\text{lower}} \\ge (n-1-k) + k + (k-1) = n + k - 2 $$\n    This lower bound depends on $k$, the number of elements that are direct losers to the eventual minimum $S_1$.\n\n4.  **Lower Bounding $k$**: The expression $n+k-2$ provides a lower bound for a given $k$. To obtain a universal lower bound on complexity, we must determine the smallest possible value of $k$ that an adversary can force, regardless of the algorithm's strategy.\n    The number of elements $k$ corresponds to the number of comparisons won by $S_1$. After $k$ wins, an element can be certified as being smaller than at most $2^k-1$ other elements (this occurs if the element wins a perfectly balanced tournament of size $2^k$). To be certified as the minimum of $n$ elements, an element must be shown to be smaller than all $n-1$ other elements. Abstractly, the set of $n-1$ losers must be partitioned among the $k$ direct losers to $S_1$. Each of these $k$ direct losers is the \"winner\" of a subtree of defeated elements. A single element can be the winner of a group of at most $2^0=1$ element (itself) without any wins. With one win, it can be the winner of $2^1=2$ elements. With $c$ wins, it can be the winner of at most $2^c$ elements. The winner $S_1$ must have \"conquered\" a hierarchy containing all $n$ elements. The number of direct opponents, $k$, is the number of sub-tournaments whose winners were defeated by $S_1$. To cover $n$ elements, we must have $2^k \\ge n$.\n    This implies $k \\ge \\log_2(n)$. Since $k$ must be an integer, we have:\n    $$ k \\ge \\lceil \\log_2(n) \\rceil $$\n    Any algorithm can be forced by an adversary to make its eventual winner play at least $\\lceil \\log_2(n) \\rceil$ matches.\n\n5.  **Final Lower Bound**: Substituting the minimum possible value of $k$ into our lower bound for $C$:\n    $$ C_{\\text{lower}} \\ge n + \\lceil \\log_2(n) \\rceil - 2 $$\n\n#### Conclusion\n\nThe upper bound on the number of comparisons is $n + \\lceil \\log_2(n) \\rceil - 2$, and the worst-case lower bound for any algorithm is also $n + \\lceil \\log_2(n) \\rceil - 2$. Since the upper and lower bounds match, this is the tight worst-case complexity.",
            "answer": "$$\n\\boxed{n + \\lceil \\log_2(n) \\rceil - 2}\n$$"
        }
    ]
}