{
    "hands_on_practices": [
        {
            "introduction": "The efficiency of a recursive algorithm is naturally captured by a recurrence relation. While tools like the Master Theorem are invaluable, they do not apply to all recurrences. This exercise challenges you to analyze an algorithm with a non-standard recurrence , developing your skill in using techniques like change of variables to determine its true asymptotic complexity from first principles.",
            "id": "3227010",
            "problem": "Consider a deterministic algorithm whose running time on inputs of size $N$ is governed by the recurrence\n$$\nT(N) = \\sqrt{N}\\, T\\!\\left(\\sqrt{N}\\right) + N,\n$$\nfor all $N$ of the form $N = 2^{2^{k}}$ with $k \\in \\mathbb{N}$, and with base condition $T(2) = c_{0}$ for some fixed constant $c_{0} \\ge 0$. Work within the standard word-Random Access Machine (word-RAM) cost model where each arithmetic or comparison operation on constant-word-size operands has unit cost, and interpret $T(N)$ as the worst-case number of primitive operations as a function of input size $N$.\n\nUsing only foundational definitions of asymptotic growth and recurrences arising from recursive algorithms, derive from first principles a tight asymptotic rate $f(N)$ such that $T(N) \\in \\Theta(f(N))$. Report only the dominant-order function $f(N)$ (that is, suppress multiplicative constant factors), and use the natural logarithm $\\ln$ in your final expression. Your final answer must be a single closed-form analytic expression $f(N)$.",
            "solution": "The problem requires finding a tight asymptotic bound $f(N)$ for a function $T(N)$ defined by the recurrence relation\n$$\nT(N) = \\sqrt{N}\\, T\\!\\left(\\sqrt{N}\\right) + N\n$$\nThe recurrence holds for input sizes $N$ of the form $N = 2^{2^{k}}$ where $k \\in \\mathbb{N}$. The base case is given as $T(2) = c_{0}$ for some constant $c_{0} \\ge 0$. The base case $N=2$ corresponds to $2 = 2^{2^k}$, which implies $1 = 2^k$, so $k=0$. This establishes that the set of natural numbers is taken to be $\\mathbb{N} = \\{0, 1, 2, \\ldots\\}$.\n\nThe given recurrence relation is not in a standard form amenable to the Master Theorem due to the coefficient $\\sqrt{N}$ of the recursive term. We will solve this recurrence from first principles using a change of variables.\n\nFirst, let's normalize the recurrence by dividing both sides by $N$:\n$$\n\\frac{T(N)}{N} = \\frac{\\sqrt{N}\\, T\\!\\left(\\sqrt{N}\\right)}{N} + \\frac{N}{N}\n$$\n$$\n\\frac{T(N)}{N} = \\frac{T\\!\\left(\\sqrt{N}\\right)}{\\sqrt{N}} + 1\n$$\nThis transformation reveals a simpler structure. Let us define a new function $G(N)$ as:\n$$\nG(N) = \\frac{T(N)}{N}\n$$\nSubstituting $G(N)$ into the transformed recurrence, we get:\n$$\nG(N) = G\\!\\left(\\sqrt{N}\\right) + 1\n$$\nThis is a much simpler recurrence relation for $G(N)$. We can solve this by unrolling it, taking advantage of the specified structure of $N$. The problem states that $N$ is of the form $N=2^{2^k}$.\nLet's apply the recurrence for $G(N)$ repeatedly:\n$$\n\\begin{aligned}\nG(N) = G\\!\\left(N^{1/2}\\right) + 1 \\\\\n= \\left(G\\!\\left(\\left(N^{1/2}\\right)^{1/2}\\right) + 1\\right) + 1 = G\\!\\left(N^{1/4}\\right) + 2 \\\\\n= \\left(G\\!\\left(\\left(N^{1/4}\\right)^{1/2}\\right) + 1\\right) + 2 = G\\!\\left(N^{1/8}\\right) + 3\n\\end{aligned}\n$$\nBy continuing this process $m$ times, we can see a pattern:\n$$\nG(N) = G\\!\\left(N^{1/2^m}\\right) + m\n$$\nThe unrolling stops when the argument of $G$ reaches the base case, which is $N=2$. So we set $N^{1/2^m} = 2$.\nTo solve for $m$, we take the logarithm base $2$ of both sides:\n$$\n\\log_{2}\\left(N^{1/2^m}\\right) = \\log_{2}(2)\n$$\n$$\n\\frac{1}{2^m} \\log_{2}(N) = 1\n$$\n$$\n2^m = \\log_{2}(N)\n$$\nTaking the logarithm base $2$ again:\n$$\n\\log_{2}\\left(2^m\\right) = \\log_{2}(\\log_{2}(N))\n$$\n$$\nm = \\log_{2}(\\log_{2}(N))\n$$\nNow, we substitute this expression for $m$ back into the equation for $G(N)$:\n$$\nG(N) = G(2) + \\log_{2}(\\log_{2}(N))\n$$\nThe value of $G(2)$ can be found from the base case for $T(N)$:\n$$\nG(2) = \\frac{T(2)}{2} = \\frac{c_{0}}{2}\n$$\nSo, the exact expression for $G(N)$ is:\n$$\nG(N) = \\frac{c_{0}}{2} + \\log_{2}(\\log_{2}(N))\n$$\nTo find $T(N)$, we reverse the initial substitution $G(N) = T(N)/N$:\n$$\nT(N) = N \\cdot G(N) = N \\left(\\frac{c_{0}}{2} + \\log_{2}(\\log_{2}(N))\\right)\n$$\n$$\nT(N) = \\frac{c_{0}}{2}N + N\\log_{2}(\\log_{2}(N))\n$$\nWe are asked for a tight asymptotic rate $f(N)$ such that $T(N) \\in \\Theta(f(N))$. We must identify the dominant term in the expression for $T(N)$ as $N \\to \\infty$. The two terms are $\\frac{c_{0}}{2}N$ and $N\\log_{2}(\\log_{2}(N))$. As $N \\to \\infty$, the function $\\log_{2}(\\log_{2}(N))$ grows without bound. Therefore, the term $N\\log_{2}(\\log_{2}(N))$ grows asymptotically faster than the linear term $N$.\nThus, the asymptotic behavior of $T(N)$ is governed by the second term:\n$$\nT(N) \\in \\Theta\\left(N\\log_{2}(\\log_{2}(N))\\right)\n$$\nThe problem requires the final expression to use the natural logarithm, $\\ln$. We can convert the base of the logarithm using the change of base formula $\\log_{b}(x) = \\frac{\\ln(x)}{\\ln(b)}$.\n$$\n\\log_{2}(\\log_{2}(N)) = \\frac{\\ln(\\log_{2}(N))}{\\ln(2)} = \\frac{\\ln\\left(\\frac{\\ln(N)}{\\ln(2)}\\right)}{\\ln(2)} = \\frac{\\ln(\\ln(N)) - \\ln(\\ln(2))}{\\ln(2)}\n$$\nFor large $N$, $\\ln(\\ln(N))$ is the dominant part of this expression. The term $\\ln(\\ln(2))$ is a constant. Therefore:\n$$\n\\log_{2}(\\log_{2}(N)) = \\frac{1}{\\ln(2)}\\ln(\\ln(N)) - \\frac{\\ln(\\ln(2))}{\\ln(2)}\n$$\nAsymptotically, $\\log_{2}(\\log_{2}(N)) \\in \\Theta(\\ln(\\ln(N)))$, since constant factors and lower-order additive constants are ignored in $\\Theta$-notation.\nSubstituting this into the asymptotic bound for $T(N)$:\n$$\nT(N) \\in \\Theta(N \\cdot \\ln(\\ln(N)))\n$$\nThe problem asks for the dominant-order function $f(N)$, suppressing multiplicative constant factors. This function is:\n$$\nf(N) = N \\ln(\\ln(N))\n$$",
            "answer": "$$\n\\boxed{N \\ln(\\ln(N))}\n$$"
        },
        {
            "introduction": "Asymptotic notation gives us a powerful lens for comparing algorithms, but it describes behavior for infinitely large inputs. In practice, we work with finite data, where constant factors and lower-order terms matter. This problem  presents a thought experiment comparing two algorithms, forcing you to calculate the \"crossover point\" where the asymptotically slower algorithm is actually faster, a crucial concept for making real-world implementation choices.",
            "id": "3226973",
            "problem": "In the Random Access Machine (RAM) model, an algorithm is a finite, well-defined procedure that maps inputs to outputs and executes a sequence of primitive operations whose count can be expressed as a function of the input size. For input size $N \\in \\mathbb{N}$ with $N \\geq 1$, consider two algorithms $A$ and $B$ whose exact time complexities (primitive operation counts) are given by $C_{A}(N)$ and $C_{B}(N)$, respectively. Assume every primitive operation has unit cost.\n\nAlgorithm $A$ has exact time complexity $C_{A}(N) = 1000 N^{2}$ and algorithm $B$ has exact time complexity $C_{B}(N) = \\frac{2^{N}}{1000}$. Although algorithm $B$ is asymptotically worse than algorithm $A$ in the sense of growth rates, it may be faster for some finite $N$ due to smaller leading constants and the detailed structure of the exact cost functions.\n\nUsing the foundational definition that, for a fixed input size $N$, the algorithm with the smaller exact primitive operation count is faster, determine the largest integer $N$ for which algorithm $B$ is faster than algorithm $A$; that is, find the maximum $N \\in \\mathbb{N}$ such that $C_{B}(N)  C_{A}(N)$. Express your final answer as a single integer.",
            "solution": "The problem requires finding the largest integer $N \\in \\mathbb{N}$, with $N \\geq 1$, for which algorithm $B$ is faster than algorithm $A$. According to the problem's definition, this means we must find the largest integer $N$ that satisfies the inequality $C_{B}(N)  C_{A}(N)$.\n\nFirst, we state the givens for the exact time complexities of algorithms $A$ and $B$:\n$$ C_{A}(N) = 1000 N^{2} $$\n$$ C_{B}(N) = \\frac{2^{N}}{1000} $$\n\nThe condition that algorithm $B$ is faster than algorithm $A$ translates to the following inequality:\n$$ \\frac{2^{N}}{1000}  1000 N^{2} $$\n\nTo analyze this inequality, we can rearrange the terms. Multiplying both sides by $1000$ yields:\n$$ 2^{N}  1000^{2} N^{2} $$\n$$ 2^{N}  (10^3)^{2} N^{2} $$\n$$ 2^{N}  10^6 N^{2} $$\n\nThis is a transcendental inequality, which cannot be solved for $N$ using elementary algebraic operations. We must analyze the behavior of the functions on both sides of the inequality. Let $g(N) = 2^{N}$ and $h(N) = 10^6 N^{2}$. It is a fundamental principle of algorithm complexity that exponential functions like $g(N)$ grow significantly faster than polynomial functions like $h(N)$ for sufficiently large $N$. This implies that the inequality $g(N)  h(N)$ will be true for a certain range of initial integer values of $N$, but will eventually become false and remain false as $N$ increases beyond a \"crossover\" point. The problem asks for the largest integer $N$ for which the inequality still holds.\n\nTo locate this crossover point, we can test integer values of $N$. The exponential function $2^N$ grows very rapidly, so the crossover point will occur at a relatively small value of $N$. We can test values computationally to find where the inequality flips from true to false.\n\nCase 1: Test $N=29$\nWe need to determine if $2^{29}  10^6 (29)^{2}$.\nFirst, calculate the left-hand side (LHS):\n$$ \\text{LHS} = 2^{29} = 536,870,912 $$\nNext, calculate the right-hand side (RHS):\n$$ \\text{RHS} = 10^6 \\times (29)^{2} = 1,000,000 \\times 841 = 841,000,000 $$\nComparing the two values, we find:\n$$ 536,870,912  841,000,000 $$\nThe inequality holds true for $N=29$. Thus, for an input size of $N=29$, algorithm $B$ is faster than algorithm $A$.\n\nCase 2: Test $N=30$\nWe need to determine if $2^{30}  10^6 (30)^{2}$.\nFirst, calculate the LHS:\n$$ \\text{LHS} = 2^{30} = 2 \\times 2^{29} = 2 \\times 536,870,912 = 1,073,741,824 $$\nNext, calculate the RHS:\n$$ \\text{RHS} = 10^6 \\times (30)^{2} = 1,000,000 \\times 900 = 900,000,000 $$\nComparing the two values, we find:\n$$ 1,073,741,824 \\not 900,000,000 $$\nThe inequality does not hold for $N=30$. For an input size of $N=30$, algorithm $A$ is faster than algorithm $B$.\n\nSince the exponential function $2^N$ continues to grow faster than the polynomial function $10^6 N^2$ for all $N$ greater than their crossover point (which we have determined is between $29$ and $30$), the inequality $C_B(N)  C_A(N)$ will not hold for any integer $N \\ge 30$.\n\nTherefore, the largest integer $N$ for which algorithm $B$ is faster than algorithm $A$ is $29$.",
            "answer": "$$\\boxed{29}$$"
        },
        {
            "introduction": "An efficient algorithm is useless if it produces the wrong result; correctness is the most fundamental property. Proving correctness requires more than just testing; it demands formal reasoning, for which loop invariants are a cornerstone. This practice  demonstrates that simply finding an invariant is not enough, by asking you to diagnose a flaw in a correctness proof where the initialization and maintenance steps hold, but the algorithm still fails due to an incorrect termination condition.",
            "id": "3226962",
            "problem": "You are given an array $A[0 \\dots n-1]$ with $n \\ge 1$, and the following algorithm is claimed to compute $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.\n\nAlgorithm description (using ordinary sequencing and conditional control, with integer indices and comparisons):\n- Precondition: $n \\ge 1$.\n- Initialize $m \\leftarrow A[0]$, $i \\leftarrow 1$.\n- Define the loop invariant $I(i,m)$ to be: $m = \\min\\{A[j] \\mid 0 \\le j  i\\}$ and $1 \\le i \\le n-1$.\n- While $i  n-1$ do:\n  - If $A[i]  m$ then set $m \\leftarrow A[i]$.\n  - Set $i \\leftarrow i + 1$.\n- Return $m$.\n\nTake as fundamental base the standard definitions from the method of proving correctness of iterative algorithms by loop invariants: a loop invariant $I$ must be established by initialization, preserved by maintenance, and combined with a well-defined termination state to imply the postcondition; partial correctness means that, under the precondition, if the algorithm terminates then the postcondition holds; total correctness additionally requires termination.\n\nAssume you have already formally proved both the initialization and the maintenance of $I(i,m)$ for the above algorithm from these base definitions, so that at the start of each loop test $I(i,m)$ holds. However, it is observed that the algorithm can return an incorrect result when $A[n-1]$ is strictly less than all $A[0], A[1], \\dots, A[n-2]$.\n\nWhich additional property, beyond the fact that $I(i,m)$ is initialized and maintained, must be verified to complete a correct proof that the algorithm returns $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$ under the precondition $n \\ge 1$?\n\nA. The invariant $I(i,m)$ alone implies the postcondition at every iteration without reference to how or when the loop terminates.\n\nB. The initialization must set $i \\leftarrow 0$ instead of $i \\leftarrow 1$.\n\nC. The termination and boundary condition must ensure that the loop exits precisely when $i = n$, so that at termination $I(i,m)$ implies $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$.\n\nD. The algorithm must be made non-deterministic so that it can explore all possible index orders, thereby covering boundary cases implicitly.\n\nE. It must be shown that $m$ is monotonically non-increasing as $i$ advances, ensuring progress toward the postcondition regardless of the termination state.",
            "solution": "To prove the correctness of an iterative algorithm using a loop invariant, three properties must be satisfied:\n\n1.  **Initialization:** The invariant holds before the first iteration of the loop.\n2.  **Maintenance:** If the invariant holds before an iteration, it holds after that iteration.\n3.  **Termination:** When the loop terminates, the invariant, combined with the loop's termination condition, must imply the algorithm's desired postcondition.\n\nThe problem states that the Initialization and Maintenance properties are already proven. The failure must therefore lie in the Termination property.\n\nLet's analyze the algorithm at the moment of termination.\n- The loop condition is `while i  n-1`.\n- The loop terminates when the condition `i  n-1` is false, which means `i >= n-1`. Since `i` starts at 1 and increments by 1, it will first fail the condition when `i` equals `n-1`.\n- At this point of termination, the loop invariant $I(i,m)$, which is $m = \\min\\{A[j] \\mid 0 \\le j  i\\}$, is true for $i=n-1$.\n- Therefore, when the algorithm returns `m`, we only have the guarantee that $m = \\min\\{A[j] \\mid 0 \\le j  n-1\\}$, which is $m = \\min\\{A[0], A[1], \\dots, A[n-2]\\}$.\n\nThe desired postcondition is that $m$ is the minimum of the *entire* array, i.e., $m = \\min\\{A[j] \\mid 0 \\le j \\le n-1\\}$. The algorithm fails because it never examines the final element, $A[n-1]$. This is why the algorithm fails when $A[n-1]$ is the minimum value.\n\nNow, let's evaluate the options:\n\n**A:** This is incorrect. A loop invariant establishes a property that holds for a partial execution. It is only in combination with the termination condition that it can prove the final result.\n\n**B:** This is incorrect. Changing the initialization to `i - 0` would not fix the core problem, as the loop condition `i  n-1` would still cause the loop to terminate before processing the last element.\n\n**C:** This is correct. It precisely identifies the flaw. For the termination property to hold, the loop must continue until all elements have been processed. This means the loop should terminate when $i=n$. At that point, the invariant $m = \\min\\{A[j] \\mid 0 \\le j  i\\}$ would imply $m = \\min\\{A[j] \\mid 0 \\le j  n\\}$, which is the correct postcondition. The current algorithm's boundary condition (`i  n-1`) is wrong, and this option correctly states what is needed for a valid proof.\n\n**D:** This is incorrect. Introducing non-determinism is irrelevant to fixing this simple off-by-one error in a deterministic algorithm.\n\n**E:** This is incorrect. While it's true that $m$ is non-increasing, this property alone is insufficient to prove correctness. The algorithm can make \"progress\" towards a value that is a local minimum but not the global minimum of the entire array if it doesn't examine all elements. The termination state is critical.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}