{
    "hands_on_practices": [
        {
            "introduction": "Amortized analysis often begins with the aggregate method, which provides an intuitive, big-picture view of performance over a sequence of operations. This first practice applies the aggregate method to the familiar stack data structure. By analyzing the total cost of a long sequence of `PUSH` and `MULTI-POP` operations, we can understand the fundamental principle that each element can only be popped once for every time it is pushed, allowing us to find a tight bound on the overall workload. ",
            "id": "3204566",
            "problem": "Consider a stack Abstract Data Type (ADT) that supports the following operations under the Last-In-First-Out (LIFO) discipline: a $PUSH(x)$ operation, which places an element $x$ onto the top of the stack, and a $MULTI\\text{-}POP(k)$ operation, which repeatedly applies the primitive $POP$ up to $k$ times or until the stack becomes empty, whichever occurs first. Assume the stack is initially empty. Adopt the following unit-cost model: each $PUSH$ has actual cost $1$, and each successful primitive $POP$ has actual cost $1$; the actual cost of $MULTI\\text{-}POP(k)$ is the number of successful primitive $POP$ operations it performs. You are given a sequence of operations consisting of exactly $n^{2}$ occurrences of $PUSH$ and exactly $n$ occurrences of $MULTI\\text{-}POP(n)$, in an arbitrary interleaving.\n\nUsing the aggregate method of amortized analysis, start from the core definitions of stacks and the stated cost model to derive the exact worst-case total actual cost of the entire sequence as a function of $n$. Your answer must be a single closed-form expression in terms of $n$. No rounding is required, and no units should be included in the final expression.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of computer science, specifically the analysis of algorithms and data structures. It is well-posed, with all necessary conditions, definitions, and constraints clearly specified. The language is objective and formal.\n\nThe task is to find the worst-case total actual cost for a sequence of $n^2 + n$ operations, consisting of $n^2$ `PUSH` operations and $n$ `MULTI-POP(n)` operations, using the aggregate method.\n\nLet $C_{total}$ be the total actual cost of an entire sequence of operations. The total cost is the sum of the actual costs of all individual operations in the sequence.\n$$\nC_{total} = \\sum_{i=1}^{n^2+n} c_i\n$$\nwhere $c_i$ is the actual cost of the $i$-th operation in the sequence.\n\nThe sequence comprises two types of operations: `PUSH` and `MULTI-POP`. We can decompose the total cost into the sum of costs from all `PUSH` operations, let's call this $C_{PUSH}$, and the sum of costs from all `MULTI-POP` operations, $C_{MULTI\\text{-}POP}$.\n$$\nC_{total} = C_{PUSH} + C_{MULTI\\text{-}POP}\n$$\n\nFirst, let's analyze the cost contributed by the `PUSH` operations.\nThe problem states there are exactly $n^2$ occurrences of the `PUSH` operation.\nThe actual cost of each `PUSH` operation is defined as $1$.\nTherefore, the total cost from all `PUSH` operations is fixed, regardless of their position in the sequence:\n$$\nC_{PUSH} = n^2 \\times 1 = n^2\n$$\n\nNext, we analyze the cost contributed by the `MULTI-POP` operations.\nThere are $n$ occurrences of the `MULTI-POP(n)` operation.\nThe actual cost of a `MULTI-POP(k)` operation is defined as the number of successful primitive `POP` operations it performs. A primitive `POP` is successful if it removes an element from a non-empty stack.\nThe core principle of a stack is that an element can be popped only if it has been previously pushed. Since the stack is initially empty, every element that is popped must correspond to a unique `PUSH` operation that occurred earlier in the sequence. Each pushed element can be popped at most once.\n\nThis fundamental relationship provides a crucial bound for the aggregate analysis. The total number of successful primitive `POP`s across the entire sequence of operations cannot exceed the total number of `PUSH` operations performed.\nLet $N_{pop}$ be the total number of successful primitive `POP`s in the sequence, and $N_{push}$ be the total number of `PUSH` operations.\nWe have:\n$$\nN_{pop} \\le N_{push}\n$$\nIn this problem, $N_{push} = n^2$.\nThe total cost of all `MULTI-POP` operations, $C_{MULTI\\text{-}POP}$, is equal to the total number of successful primitive `POP`s, $N_{pop}$.\nThus, we have an upper bound on this component of the cost:\n$$\nC_{MULTI\\text{-}POP} = N_{pop} \\le n^2\n$$\n\nNow, we can establish an upper bound for the total actual cost, $C_{total}$, for any possible interleaving of the given operations.\n$$\nC_{total} = C_{PUSH} + C_{MULTI\\text{-}POP} \\le n^2 + n^2 = 2n^2\n$$\n\nTo determine if this upper bound represents the exact worst-case total actual cost, we must show that there exists at least one sequence of operations for which this cost is achieved. If we can construct such a sequence, then $2n^2$ is indeed the worst-case cost.\n\nConsider the following sequence, which is one possible arbitrary interleaving:\n1.  First, perform all $n^2$ `PUSH` operations consecutively.\n2.  Then, perform all $n$ `MULTI-POP(n)` operations consecutively.\n\nLet's calculate the total cost for this specific sequence, assuming $n \\ge 1$. The case $n=0$ is trivial, resulting in $0$ operations and $0$ cost, which matches $2(0)^2=0$.\n-   **Cost of the `PUSH` phase:** There are $n^2$ `PUSH` operations, each with a cost of $1$. The total cost for this phase is $n^2 \\times 1 = n^2$. After this phase, the stack contains $n^2$ elements.\n\n-   **Cost of the `MULTI-POP` phase:** This phase consists of $n$ calls to `MULTI-POP(n)`.\n    -   At the time of the 1st `MULTI-POP(n)` call, the stack contains $n^2$ elements. Since $n \\le n^2$ (for $n \\ge 1$), the operation successfully pops $n$ elements. Its cost is $n$. The stack now contains $n^2 - n$ elements.\n    -   At the time of the 2nd `MULTI-POP(n)` call, the stack contains $n^2 - n$ elements. Since $n \\le n^2 - n$ (for $n \\ge 2$), the operation successfully pops $n$ elements. Its cost is $n$. The stack now contains $n^2 - 2n$ elements.\n    -   In general, at the time of the $i$-th `MULTI-POP(n)` call (where $1 \\le i \\le n$), the stack contains $n^2 - (i-1)n$ elements. For the operation to pop $n$ elements, the stack size must be at least $n$. We check this condition:\n        $$\n        n^2 - (i-1)n = n(n - (i-1)) = n(n - i + 1)\n        $$\n        Since $i \\le n$, we have $n-i \\ge 0$, which means $n-i+1 \\ge 1$. Therefore, the stack size $n(n-i+1)$ is always greater than or equal to $n$.\n    -   This proves that each of the $n$ calls to `MULTI-POP(n)` will be fully successful, popping exactly $n$ elements.\n    -   The cost of each of the $n$ `MULTI-POP(n)` operations is $n$.\n    -   The total cost for this phase is $n \\times (\\text{cost per call}) = n \\times n = n^2$.\n\nThe total actual cost for this specific worst-case sequence is the sum of the costs from both phases:\n$$\nC_{total, worst} = (\\text{Cost of PUSHes}) + (\\text{Cost of MULTI-POPs}) = n^2 + n^2 = 2n^2\n$$\n\nSince we have established an upper bound of $2n^2$ for the total cost of any sequence and have constructed a specific sequence that achieves this cost, we conclude that the exact worst-case total actual cost of the entire sequence is $2n^2$. This completes the derivation using the aggregate method.",
            "answer": "$$\n\\boxed{2n^{2}}\n$$"
        },
        {
            "introduction": "While the aggregate method is powerful, the potential method offers a more structured approach by assigning a \"potential energy\" to the data structure's state. This exercise introduces the classic example for the potential method: the binary counter. We will define a potential function based on the number of set bits and use it to show that the amortized cost of an increment is constant, ultimately allowing us to derive an exact closed-form expression for the total cost of a sequence of increments. ",
            "id": "3204641",
            "problem": "A binary counter stores nonnegative integers in base $2$, with the least significant bit at position $0$. For each increment from state $i$ to state $i+1$, define the actual cost of the operation to be the number of bit positions that change value between the binary representations of $i$ and $i+1$. This cost is the Hamming distance between $i$ and $i+1$, where the Hamming distance is the number of coordinates at which two equal-length strings differ.\n\nStarting from the counter state $0$, perform exactly $m$ consecutive increments, for a fixed positive integer $m$. Let $\\mathcal{C}(m)$ be the total actual cost accumulated over these $m$ increments. Derive, from first principles and using any of the aggregate, accounting, or potential methods, an exact closed-form expression for $\\mathcal{C}(m)$ in terms of $m$ only, with no summations. You may use a function symbol $s_{2}(m)$ to denote the number of ones in the base-$2$ representation of $m$.\n\nYour final answer must be a single closed-form analytical expression for $\\mathcal{C}(m)$, with no inequalities or equations included in the answer. No rounding is required.",
            "solution": "The problem is valid as it is a standard, well-posed problem in the analysis of algorithms. It is scientifically grounded in mathematics and computer science, formally stated, and objective. We will proceed to derive the closed-form expression for the total cost $\\mathcal{C}(m)$ using the potential method of amortized analysis.\n\nLet the state of the binary counter after $i$ increments be represented by the integer $i$. The initial state is $0$. We perform a sequence of $m$ increments, leading the counter through states $0, 1, 2, \\dots, m$.\n\nLet $c_k$ be the actual cost of the $k$-th increment operation, which transitions the counter from state $k-1$ to state $k$. The problem defines this cost as the number of bits that flip during this transition. The total actual cost over $m$ increments is $\\mathcal{C}(m) = \\sum_{k=1}^{m} c_k$.\n\nTo apply the potential method, we define a potential function $\\Phi$ that maps the state of the data structure to a real number. A suitable choice for the potential of the counter in state $i$, let's call it $\\Phi(i)$, is the number of ones in the binary representation of $i$. We use the notation from the problem, $s_2(i)$, for this quantity.\n$$\n\\Phi(i) = s_2(i)\n$$\nFor the potential method to be valid, we must have $\\Phi(i) \\ge \\Phi(0)$ for all states $i$. The initial state is $0$, and $\\Phi(0) = s_2(0) = 0$. Since the number of ones in any non-negative integer's binary representation is non-negative, $s_2(i) \\ge 0$ for all $i \\ge 0$. Thus, the condition $\\Phi(i) \\ge \\Phi(0)$ is satisfied.\n\nThe amortized cost, $a_k$, of the $k$-th operation is defined as the actual cost plus the change in potential:\n$$\na_k = c_k + \\Phi(k) - \\Phi(k-1)\n$$\nLet's analyze the change in the counter for the $k$-th increment (from state $k-1$ to $k$). Suppose the binary representation of $k-1$ has $j$ trailing ones. That is, its binary form is $\\dots p_j 0 1 \\dots 1$, where there is a contiguous block of $j$ ones at the end, preceded by a zero.\nThe increment operation flips these $j$ ones to zeros and flips the rightmost zero (at position $j$) to a one. All bits to the left of position $j$ remain unchanged.\nThe number of bit flips in this operation is $j$ (for the $1 \\to 0$ flips) plus $1$ (for the $0 \\to 1$ flip). Therefore, the actual cost is:\n$$\nc_k = j+1\n$$\nThis also covers the case where $k-1$ is even, meaning its binary representation ends in $0$. In this case, $j=0$, and only the last bit flips from $0$ to $1$, so $c_k=1$, which matches the formula.\n\nNow, we evaluate the change in potential, $\\Phi(k) - \\Phi(k-1) = s_2(k) - s_2(k-1)$.\nThe number of ones in state $k-1$ can be expressed in terms of the number of ones in the bits higher than position $j$, let's call this $N$, and the $j$ trailing ones. So, $s_2(k-1) = N+j$.\nIn state $k$, the $j$ trailing ones have become zeros, and the zero at position $j$ has become a one. The higher-order bits are unchanged. Thus, the number of ones in state $k$ is $s_2(k) = N+1$.\nThe change in the number of ones is:\n$$\ns_2(k) - s_2(k-1) = (N+1) - (N+j) = 1-j\n$$\nThis gives us the change in potential: $\\Delta\\Phi_k = \\Phi(k) - \\Phi(k-1) = 1-j$.\n\nNow we can calculate the amortized cost $a_k$:\n$$\na_k = c_k + \\Delta\\Phi_k = (j+1) + (1-j) = 2\n$$\nThe amortized cost for each increment operation is a constant, $2$.\n\nThe total actual cost $\\mathcal{C}(m)$ is related to the total amortized cost by the fundamental theorem of amortized analysis:\n$$\n\\sum_{k=1}^{m} c_k = \\sum_{k=1}^{m} a_k - (\\Phi(m) - \\Phi(0))\n$$\nSubstituting the known quantities:\n$$\n\\mathcal{C}(m) = \\sum_{k=1}^{m} 2 - (s_2(m) - s_2(0))\n$$\nThe sum of the constant amortized costs is $\\sum_{k=1}^{m} 2 = 2m$. The potential of the initial state is $\\Phi(0) = s_2(0) = 0$. The potential of the final state is $\\Phi(m) = s_2(m)$.\nSubstituting these values, we obtain the expression for the total actual cost:\n$$\n\\mathcal{C}(m) = 2m - (s_2(m) - 0)\n$$\n$$\n\\mathcal{C}(m) = 2m - s_2(m)\n$$\nThis is the exact, closed-form expression for the total number of bit flips that occur during $m$ consecutive increments of a binary counter starting from $0$. The expression consists of the term $2m$ and a correction term which is the number of ones in the final state $m$.",
            "answer": "$$\\boxed{2m - s_{2}(m)}$$"
        },
        {
            "introduction": "True mastery of a concept comes from applying it to unfamiliar situations. This final practice challenges you to adapt your understanding of amortized analysis to a binary counter with a non-standard cost model, where flipping higher-order bits is more expensive. This variation prevents reliance on memorized formulas and forces a return to first principles, requiring you to construct a new potential function that accurately reflects the weighted costs and demonstrating the flexibility of both the potential and accounting methods. ",
            "id": "3204622",
            "problem": "You are given an unbounded binary counter whose bits are indexed by nonnegative integers, with bit $i$ having weight $2^{i}$ and bit $0$ being the Least Significant Bit (LSB). The counter starts at the all-zero state. An increment operation adds $1$ to the counter in binary, flipping the trailing block of consecutive $1$-bits (if any) to $0$ and then flipping the next higher $0$-bit to $1$. The actual cost model is as follows: flipping bit $i$ incurs cost $i+1$. Thus, the actual cost of an increment is the sum of $i+1$ over all bits $i$ flipped by that increment. Consider an arbitrary sequence of $m$ increments from the all-zero state, assuming no overflow (the counter has sufficiently many bits so that all $m$ increments are feasible).\n\nStarting only from the formal semantics of binary increment and the above cost model, do the following.\n- Using the aggregate method, derive a closed-form upper bound on the total cost $C(m)$ of $m$ increments by counting, for each bit $i$, how many times it flips in the first $m$ increments, and summing these contributions. Your derivation must begin from first principles and may use only basic series facts.\n- Using the accounting method, propose a fixed per-increment amortized charge $c$ and a credit invariant that assigns a nonnegative number of stored credits to each bit $i$ as a function of $i$ and whether the bit is currently $1$ or $0$, such that for every increment the credits released from bits that flip to $0$ together with the charged $c$ cover the actual cost of that increment and reestablish the invariant. Prove that this scheme works for all $m$ starting from the zero state, and determine the smallest constant $c$ that makes this possible.\n- Using the potential method, define a potential function $\\Phi$ depending only on the current bit pattern, and prove that the amortized cost of each increment, defined as the actual cost plus the change in potential, is exactly the same constant $c$ that you found via accounting, for every increment regardless of $m$.\n- Finally, use your aggregate analysis to evaluate $\\lim_{m \\to \\infty} \\frac{C(m)}{m}$ and argue that your constant $c$ is tight.\n\nReport the minimal constant $c$ as your final answer. No rounding is required, and no units are involved. The final answer must be a single real number.",
            "solution": "The problem requires an amortized analysis of a binary counter increment operation under a non-standard cost model. We are asked to use the aggregate, accounting, and potential methods to find and justify the tightest constant amortized cost for a sequence of $m$ increments.\n\nFirst, let's establish the cost model and operation dynamics. A binary counter starts at $0$. An increment operation adds $1$. This causes a block of trailing $1$s to flip to $0$s and the next $0$ to flip to a $1$. The cost of flipping bit $i$ is defined as $i+1$. The actual cost of an increment is the sum of costs of all bits flipped.\n\n**1. Aggregate Method**\n\nThe aggregate method calculates the total actual cost, $C(m)$, of a sequence of $m$ operations and then determines the average cost per operation, which is $C(m)/m$.\n\nTo find $C(m)$, we sum the costs of all bit flips over the course of $m$ increments. Instead of summing costs per increment, we can sum costs per bit. The total cost is $C(m) = \\sum_{i=0}^{\\infty} (\\text{cost to flip bit } i) \\times (\\text{number of times bit } i \\text{ flips in } m \\text{ increments})$.\n\nThe cost to flip bit $i$ is given as $i+1$.\nBit $i$ flips when the counter transitions from a value $v$ to $v+1$ where the binary representations of $v$ and $v+1$ differ at bit $i$. This occurs precisely when adding $1$ causes a carry that propagates up to bit $i$. Bit $i$ flips from $0$ to $1$ or $1$ to $0$ if and only if bits $0, 1, \\dots, i-1$ are all $1$s before the increment. This happens at counter values of the form $k \\cdot 2^i + (2^i - 1)$ for any integer $k \\ge 0$. The increment moves the value to $(k+1) \\cdot 2^i$. Therefore, bit $i$ flips on the $j$-th increment if $j$ is a multiple of $2^i$.\n\nIn a sequence of $m$ increments (from value $0$ to $m-1$, resulting in counter value $m$), bit $i$ flips on increments $1 \\cdot 2^i, 2 \\cdot 2^i, 3 \\cdot 2^i, \\dots, \\lfloor m/2^i \\rfloor \\cdot 2^i$. The number of times bit $i$ flips is exactly $\\lfloor m/2^i \\rfloor$.\n\nThe total cost $C(m)$ is the sum over all bits $i$:\n$$C(m) = \\sum_{i=0}^{\\infty} (i+1) \\left\\lfloor \\frac{m}{2^i} \\right\\rfloor$$\nThis sum is finite because for $2^i > m$, the term $\\lfloor m/2^i \\rfloor$ becomes $0$.\n\nTo find a closed-form upper bound, we use the inequality $\\lfloor x \\rfloor \\le x$:\n$$C(m) \\le \\sum_{i=0}^{\\infty} (i+1) \\frac{m}{2^i} = m \\sum_{i=0}^{\\infty} \\frac{i+1}{2^i}$$\nThe sum is an arithmetic-geometric series. Let $S = \\sum_{k=0}^{\\infty} (k+1)x^k$. This series is the derivative of a related geometric series. We know that for $|x|<1$, $\\sum_{k=0}^{\\infty} x^k = \\frac{1}{1-x}$. Differentiating with respect to $x$ gives $\\sum_{k=1}^{\\infty} kx^{k-1} = \\frac{1}{(1-x)^2}$. Letting $i=k-1$, this is $\\sum_{i=0}^{\\infty} (i+1)x^i = \\frac{1}{(1-x)^2}$.\nFor our sum, $x=1/2$, so:\n$$S = \\sum_{i=0}^{\\infty} \\frac{i+1}{2^i} = \\frac{1}{(1-1/2)^2} = \\frac{1}{(1/4)} = 4$$\nThus, the total cost is bounded above:\n$$C(m) \\le 4m$$\nThis suggests an amortized cost of $4$ per operation.\n\n**2. Accounting Method**\n\nIn the accounting method, we charge a fixed amortized cost $c$ for each operation. Part of this charge pays for the immediate actual cost, and the rest is stored as \"credit\" on the data structure. This credit can be used later to pay for expensive operations. The total credit must never be negative.\n\nLet's propose an amortized charge of $c=4$, based on our aggregate analysis. We need to define a credit invariant. A common strategy is to store credit on bits that are set to $1$. Let's define the credit invariant:\n*If bit $i$ is $1$, it stores $\\phi_i$ credits.*\n*If bit $i$ is $0$, it stores $0$ credits.*\n\nWe need to determine the function $\\phi_i$. The initial state is all zeros, so the total credit is $0$.\nConsider an increment operation that flips bits $0, 1, \\dots, k-1$ from $1$ to $0$, and bit $k$ from $0$ to $1$.\nThe actual cost of this operation is $A_k = (\\sum_{i=0}^{k-1} (i+1)) + (k+1) = \\frac{k(k+1)}{2} + (k+1) = \\frac{(k+1)(k+2)}{2}$.\nTo pay for this operation, we use the charged amount $c$ and any credits released by bits flipping from $1$ to $0$.\nCredits released: $\\sum_{i=0}^{k-1} \\phi_i$.\nAfter the operation, we must re-establish the invariant by storing credit on bit $k$, which is now $1$.\nCredits to be stored: $\\phi_k$.\n\nThe fundamental inequality of the accounting method is:\nAmortized Charge + Credits Released $\\ge$ Actual Cost + Credits Stored\n$$c + \\sum_{i=0}^{k-1} \\phi_i \\ge \\frac{(k+1)(k+2)}{2} + \\phi_k$$\nWe need to find a non-negative function $\\phi_i$ that satisfies this for $c=4$ and for all $k \\ge 0$. Let's try to find $\\phi_i$ by setting this to an equality:\n$$4 = \\frac{(k+1)(k+2)}{2} + \\phi_k - \\sum_{i=0}^{k-1} \\phi_i$$\nLet's test a simple linear function, like $\\phi_i = i+B$.\n$$4 = \\frac{k^2+3k+2}{2} + (k+B) - \\sum_{i=0}^{k-1} (i+B)$$\n$$4 = \\frac{k^2+3k+2}{2} + k+B - \\left(\\frac{(k-1)k}{2} + Bk\\right)$$\n$$4 = \\frac{k^2+3k+2}{2} + k+B - \\frac{k^2-k}{2} - Bk$$\n$$8 = k^2+3k+2 + 2k+2B - k^2+k - 2Bk$$\n$$8 = 6k+2+2B(1-k)$$\nThis equation depends on $k$, so $\\phi_i=i+B$ is incorrect.\n\nLet's derive $\\phi_i$ from first principles as developed during the thought process. The total cost after $m$ increments is $C(m) = \\sum_{j=1}^m (\\text{amortized cost}_j - \\Delta \\Phi_j) = mc - (\\Phi_m - \\Phi_0)$. With $\\Phi_0=0$, $C(m)=mc - \\Phi_m$. From the aggregate method $C(m) = \\sum_{i=0}^\\infty (i+1)\\lfloor m/2^i \\rfloor$. We set $mc - \\Phi_m \\ge C(m)_{exact}$. We need $\\Phi_m\\le mc-C(m) = m \\sum \\frac{i+1}{2^i} - \\sum(i+1)\\lfloor \\frac{m}{2^i} \\rfloor = \\sum (i+1)(m/2^i - \\lfloor m/2^i \\rfloor) = \\sum(i+1)\\{m/2^i\\}$.\nWe know $\\{m/2^i\\} = \\sum_{j=0}^{i-1} b_j 2^{j-i}$, where $b_j$ is the $j$-th bit of $m$.\n$\\Phi_m = \\sum_j b_j \\phi_j \\le \\sum_i (i+1) \\sum_{j=0}^{i-1} b_j 2^{j-i} = \\sum_j b_j (\\sum_{i=j+1}^\\infty (i+1)2^{j-i})$.\nThis is satisfied if we set $\\phi_j = \\sum_{i=j+1}^{\\infty} (i+1)2^{j-i} = 2^j \\sum_{i=j+1}^{\\infty} \\frac{i+1}{2^i}$.\nThe sum is $S - S_j = 4 - (4 - \\frac{j+3}{2^j}) = \\frac{j+3}{2^j}$.\nSo, $\\phi_j = 2^j (\\frac{j+3}{2^j}) = j+3$.\nOur credit invariant is: a bit $i$ set to $1$ stores $\\phi_i = i+3$ credits.\nLet's verify this for $c=4$.\n$$4 + \\sum_{i=0}^{k-1} (i+3) \\ge \\frac{(k+1)(k+2)}{2} + (k+3)$$\n$$4 + \\frac{(k-1)k}{2} + 3k \\ge \\frac{k^2+3k+2}{2} + k+3$$\n$$4 + \\frac{k^2-k+6k}{2} \\ge \\frac{k^2+3k+2+2k+6}{2}$$\n$$4 + \\frac{k^2+5k}{2} \\ge \\frac{k^2+5k+8}{2}$$\n$$\\frac{8+k^2+5k}{2} \\ge \\frac{k^2+5k+8}{2}$$\nThis is an equality, so it holds. For any increment, a charge of $c=4$ is exactly sufficient to pay the actual cost and maintain the credit invariant. The minimal constant charge is therefore $c=4$.\n\n**3. Potential Method**\n\nThe potential method defines a potential function $\\Phi$ that maps the state of the data structure to a non-negative real number, with $\\Phi=0$ for the initial state. The amortized cost $c_a$ of an operation is its actual cost $c_{act}$ plus the change in potential, $c_a = c_{act} + \\Delta\\Phi$.\n\nWe use the credit function from the accounting method to define our potential function. Let the state of the counter be defined by the set $S$ of indices of bits that are $1$.\n$$\\Phi(S) = \\sum_{i \\in S} (i+3)$$\nThe initial state is all zeros, so $S=\\emptyset$ and $\\Phi(\\emptyset)=0$. Since $i \\ge 0$, $\\Phi(S) \\ge 0$ for all states.\n\nConsider an increment that flips bits $0, 1, \\dots, k-1$ from $1$ to $0$ and bit $k$ from $0$ to $1$.\nThe state before is $S_{old} = \\{0, 1, \\dots, k-1\\} \\cup S'$, where $S'$ represents the bits greater than $k$ that are $1$.\nThe state after is $S_{new} = \\{k\\} \\cup S'$.\nThe actual cost is $c_{act} = \\frac{(k+1)(k+2)}{2}$.\nThe change in potential is $\\Delta\\Phi = \\Phi(S_{new}) - \\Phi(S_{old})$.\n$\\Phi(S_{new}) = (k+3) + \\sum_{i \\in S'} (i+3)$.\n$\\Phi(S_{old}) = \\sum_{i=0}^{k-1} (i+3) + \\sum_{i \\in S'} (i+3)$.\n$\\Delta\\Phi = (k+3) - \\sum_{i=0}^{k-1} (i+3) = (k+3) - \\left(\\frac{(k-1)k}{2} + 3k\\right) = k+3 - \\frac{k^2-k+6k}{2} = \\frac{2k+6 - k^2-5k}{2} = \\frac{-k^2-3k+6}{2}$.\nThe amortized cost is:\n$$c_a = c_{act} + \\Delta\\Phi = \\frac{k^2+3k+2}{2} + \\frac{-k^2-3k+6}{2} = \\frac{8}{2} = 4$$\nThe amortized cost of every increment operation is exactly $4$, regardless of the state of the counter. This confirms that $c=4$ is a valid constant amortized cost.\n\n**4. Asymptotic Analysis and Tightness**\n\nThe constant $c=4$ is an upper bound on the average cost. To show it is tight, we must show that the average cost per operation can be arbitrarily close to $4$. We can do this by analyzing the average cost as the number of operations $m$ goes to infinity.\n\nThe average cost after $m$ increments is $\\frac{C(m)}{m}$. Using the formula from the aggregate analysis:\n$$\\frac{C(m)}{m} = \\frac{1}{m} \\sum_{i=0}^{\\infty} (i+1) \\left\\lfloor \\frac{m}{2^i} \\right\\rfloor = \\sum_{i=0}^{\\infty} (i+1) \\frac{\\lfloor m/2^i \\rfloor}{m}$$\nAs $m \\to \\infty$, the term $\\frac{\\lfloor m/2^i \\rfloor}{m}$ approaches $\\frac{m/2^i}{m} = \\frac{1}{2^i}$. So we can find the limit:\n$$\\lim_{m\\to\\infty} \\frac{C(m)}{m} = \\lim_{m\\to\\infty} \\sum_{i=0}^{\\infty} (i+1) \\frac{\\lfloor m/2^i \\rfloor}{m} = \\sum_{i=0}^{\\infty} (i+1) \\lim_{m\\to\\infty} \\frac{\\lfloor m/2^i \\rfloor}{m}$$\n(Interchanging the limit and sum is justified by the Dominated Convergence Theorem, as the terms are bounded by an absolutely convergent series $(i+1)/2^i$.)\n$$\\lim_{m\\to\\infty} \\frac{C(m)}{m} = \\sum_{i=0}^{\\infty} \\frac{i+1}{2^i} = 4$$\nSince the average cost per operation approaches $4$, no constant amortized cost less than $4$ could possibly be valid for all $m$. If we chose $c' < 4$, for a sufficiently large $m$, the total payments $m c'$ would be less than the total actual cost $C(m) \\approx 4m$, violating the definition of amortized cost.\nTherefore, the constant $c=4$ is the smallest possible, i.e., it is tight.\n\nIn summary, all three methods of amortized analysis consistently point to a minimal constant amortized cost of $4$.",
            "answer": "$$\\boxed{4}$$"
        }
    ]
}