{
    "hands_on_practices": [
        {
            "introduction": "The primary benefit of a Huffman code is its prefix-free property, which ensures that an encoded bitstream can be decoded into a unique sequence of symbols without ambiguity. This first exercise provides direct practice in this decoding process. By working through a bitstream with a given codebook, you will simulate exactly how a receiving system reconstructs the original data, solidifying your understanding of why this property is so crucial. ",
            "id": "1630289",
            "problem": "A deep-space probe sends observational data back to Earth using a stream of four distinct markers, which we will denote as Alpha ($\\alpha$), Beta ($\\beta$), Gamma ($\\gamma$), and Delta ($\\delta$). To conserve transmission bandwidth, the data stream is compressed using a Huffman code. The codebook mapping each marker to its binary representation is as follows:\n*   $\\alpha$: `0`\n*   $\\beta$: `10`\n*   $\\gamma$: `110`\n*   $\\delta$: `111`\n\nA segment of a received transmission contains the binary sequence `11010011100`. Your task is to decode this sequence. Which of the following options represents the original sequence of markers?\n\nA. $\\gamma\\beta\\delta\\alpha\\alpha$\n\nB. $\\gamma\\beta\\alpha\\gamma\\alpha\\alpha$\n\nC. $\\delta\\alpha\\beta\\beta\\alpha\\gamma$\n\nD. $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$\n\nE. $\\gamma\\alpha\\beta\\delta\\alpha\\alpha$",
            "solution": "The problem requires us to decode a binary sequence using a given Huffman codebook. The fundamental property of a Huffman code is that it is a prefix code. This means no codeword is a prefix of any other codeword, which guarantees that any encoded sequence can be decoded unambiguously into a unique sequence of symbols.\n\nThe decoding algorithm proceeds by reading the binary stream from left to right. We accumulate bits one by one until the current string of bits matches a codeword in the dictionary. Once a match is found, we record the corresponding symbol and then continue the process from the very next bit in the stream.\n\nThe provided codebook is:\n*   $\\alpha$: `0`\n*   $\\beta$: `10`\n*   $\\gamma$: `110`\n*   $\\delta$: `111`\n\nThe binary sequence to be decoded is `11010011100`.\n\nLet's apply the decoding procedure step-by-step:\n\n1.  We start at the beginning of the sequence: `11010011100`.\n    -   We read the first bit, `1`. This is not a complete codeword in our codebook.\n    -   We read the second bit, forming the string `11`. This is also not in the codebook.\n    -   We read the third bit, forming `110`. This string matches the code for the symbol $\\gamma$. We record $\\gamma$ as the first symbol of our decoded sequence.\n    -   The remaining bit sequence is `10011100`.\n\n2.  We continue decoding from the start of the remaining sequence: `10011100`.\n    -   We read the first bit, `1`. This is not a codeword.\n    -   We read the second bit, forming `10`. This string matches the code for $\\beta$. We record $\\beta$ as the second symbol.\n    -   The remaining bit sequence is `011100`.\n\n3.  We proceed with the new sequence: `011100`.\n    -   We read the first bit, `0`. This string matches the code for $\\alpha$. We record $\\alpha$ as the third symbol.\n    -   The remaining bit sequence is `11100`.\n\n4.  We continue with the sequence `11100`.\n    -   We read the first bit, `1`. Not a codeword.\n    -   We read the second bit, forming `11`. Not a codeword.\n    -   We read the third bit, forming `111`. This string matches the code for $\\delta$. We record $\\delta$ as the fourth symbol.\n    -   The remaining bit sequence is `00`.\n\n5.  We proceed with the sequence `00`.\n    -   We read the first bit, `0`. This matches the code for $\\alpha$. We record $\\alpha$ as the fifth symbol.\n    -   The remaining bit sequence is `0`.\n\n6.  Finally, we decode the last part of the sequence: `0`.\n    -   We read the bit, `0`. This matches the code for $\\alpha$. We record $\\alpha$ as the sixth and final symbol.\n    -   The remaining sequence is now empty, so the decoding is complete.\n\nBy concatenating the symbols we identified in order, we find the original sequence to be $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$.\n\nWe now compare this result with the given multiple-choice options:\nA. $\\gamma\\beta\\delta\\alpha\\alpha$\nB. $\\gamma\\beta\\alpha\\gamma\\alpha\\alpha$\nC. $\\delta\\alpha\\beta\\beta\\alpha\\gamma$\nD. $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$\nE. $\\gamma\\alpha\\beta\\delta\\alpha\\alpha$\n\nOur decoded sequence, $\\gamma\\beta\\alpha\\delta\\alpha\\alpha$, matches option D.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Moving from using a code to creating one, this next practice challenges you to apply the Huffman algorithm from the ground up. Given a set of symbols and their corresponding probabilities, you will construct an optimal binary prefix code by building a Huffman tree. This process not only reinforces the mechanics of the algorithm but also connects it to the fundamental goal of data compression by having you calculate the average codeword length, a key measure of the code's efficiency. ",
            "id": "1630316",
            "problem": "A deep-space probe is monitoring the atmosphere of a distant exoplanet. It is equipped to detect six specific types of molecules, which it labels with the symbols $\\{S_1, S_2, S_3, S_4, S_5, S_6\\}$. Due to the planet's atmospheric chemistry, these molecules are detected with different frequencies. Over a long observation period, the probe has established the following stable probabilities of detecting each molecule type in any given measurement:\n$P(S_1) = 0.1$\n$P(S_2) = 0.1$\n$P(S_3) = 0.1$\n$P(S_4) = 0.1$\n$P(S_5) = 0.3$\n$P(S_6) = 0.3$\n\nTo conserve bandwidth for transmissions back to Earth, the data stream of detected symbols is to be encoded using an optimal binary prefix code. You are tasked with analyzing the efficiency of this encoding scheme.\n\nCalculate two fundamental quantities for this source:\n1. The average codeword length, $L$, of a Huffman code constructed for these symbols.\n2. The entropy, $H$, of the source.\n\nWhen constructing the Huffman code, if a tie in probabilities occurs when selecting nodes to combine, the choice between the tied nodes may be made arbitrarily.\n\nExpress your answers for $L$ and $H$, in that order, in units of bits per symbol. Round your final numerical answers to four significant figures.",
            "solution": "We are given six symbols with probabilities $p_{1}=0.1$, $p_{2}=0.1$, $p_{3}=0.1$, $p_{4}=0.1$, $p_{5}=0.3$, and $p_{6}=0.3$. To find the average codeword length $L$ of a Huffman code, we construct the code by iteratively combining the two least probable nodes:\n- Combine $0.1$ and $0.1$ to form $0.2$ (twice), giving multiset $\\{0.2,0.2,0.3,0.3\\}$.\n- Combine $0.2$ and $0.2$ to form $0.4$, giving $\\{0.3,0.3,0.4\\}$.\n- Combine $0.3$ and $0.3$ to form $0.6$, giving $\\{0.4,0.6\\}$.\n- Combine $0.4$ and $0.6$ to form $1.0$.\nTracing back, each symbol of probability $0.1$ attains codeword length $3$, and each symbol of probability $0.3$ attains codeword length $2$. Therefore, the average length is\n$$\nL=\\sum_{i=1}^{6}p_{i}l_{i}=4\\cdot 0.1\\cdot 3+2\\cdot 0.3\\cdot 2=1.2+1.2=2.4\\ \\text{bits per symbol}.\n$$\n\nThe entropy in bits per symbol is\n$$\nH=-\\sum_{i=1}^{6}p_{i}\\log_{2}p_{i}=-4\\cdot 0.1\\log_{2}(0.1)-2\\cdot 0.3\\log_{2}(0.3).\n$$\nUsing $\\log_{2}(0.1)\\approx -3.3219280949$ and $\\log_{2}(0.3)\\approx -1.7369655942$, we obtain\n$$\nH\\approx -0.4(-3.3219280949)-0.6(-1.7369655942)=1.3287712379+1.0421793565\\approx 2.3709505945.\n$$\nRounding to four significant figures gives $L=2.400$ and $H=2.371$, both in bits per symbol.",
            "answer": "$$\\boxed{\\begin{pmatrix}2.400  2.371\\end{pmatrix}}$$"
        },
        {
            "introduction": "While Huffman coding generates an optimal prefix code, \"optimal\" does not always imply a significant compression gain compared to simpler methods. This final exercise is a thought experiment that explores the limits of Huffman coding's effectiveness. You are asked to determine the precise conditions—in this case, a specific ratio of symbol probabilities—under which a Huffman code performs no better than a basic fixed-length code, providing deeper insight into the relationship between probability distributions and the potential for compression. ",
            "id": "1630282",
            "problem": "In the field of information theory, Huffman coding is a renowned algorithm for lossless data compression. It constructs a variable-length prefix code based on the estimated probabilities of occurrence for each symbol a source can produce. An alternative, simpler approach is to use a fixed-length code.\n\nConsider a source that generates symbols from an alphabet $\\mathcal{A} = \\{S_1, S_2, S_3, S_4\\}$. A fixed-length binary code for this four-symbol alphabet would assign a unique 2-bit codeword to each symbol, resulting in an average code length of exactly 2 bits per symbol.\n\nSuppose the probability distribution for the symbols is structured such that two symbols have a high probability $p_h$ and the other two symbols have a low probability $p_l$, with $p_h  p_l  0$. The probabilities must, of course, sum to unity.\n\nDetermine the specific numerical value of the ratio $R = p_h / p_l$ for which the average codeword length of a Huffman code is guaranteed to be exactly 2 bits per symbol, irrespective of the choices made to break ties during the algorithm's construction.",
            "solution": "Let the four symbol probabilities be $\\{p_{h},p_{h},p_{l},p_{l}\\}$ with $p_{h}p_{l}0$ and the normalization constraint\n$$\n2p_{h}+2p_{l}=1.\n$$\nHuffman coding repeatedly merges the two least probable nodes. The first merge combines the two $p_{l}$ leaves into a node of weight\n$$\nw=2p_{l}.\n$$\nAfter this merge, the multiset of node weights is $\\{p_{h},p_{h},2p_{l}\\}$. The next merge depends on comparing $2p_{l}$ and $p_{h}$:\n\n1) If $2p_{l}p_{h}$ (equivalently $R=\\frac{p_{h}}{p_{l}}2$), then the two least weights are $p_{h}$ and $p_{h}$. Merging these produces two subtrees each with two leaves, so all four codewords have length $2$, and the average length is exactly $2$.\n\n2) If $2p_{l}p_{h}$ (equivalently $R2$), then the two least weights are $2p_{l}$ and $p_{h}$. This yields a code with lengths $\\{1,2,3,3\\}$, where the length-$1$ codeword goes to one $p_{h}$, the length-$2$ codeword to the other $p_{h}$, and the two $p_{l}$ receive length $3$. The average length is\n$$\nL=1\\cdot p_{h}+2\\cdot p_{h}+3\\cdot p_{l}+3\\cdot p_{l}=3p_{h}+6p_{l}.\n$$\nUsing $2p_{h}+2p_{l}=1$ gives $p_{h}=\\frac{1}{2}-p_{l}$, hence\n$$\nL=3\\left(\\frac{1}{2}-p_{l}\\right)+6p_{l}=\\frac{3}{2}+3p_{l}.\n$$\nThe condition $2p_{l}p_{h}$ is equivalent (via $p_{h}=\\frac{1}{2}-p_{l}$) to $3p_{l}\\frac{1}{2}$, i.e., $p_{l}\\frac{1}{6}$, which implies $L=\\frac{3}{2}+3p_{l}2$. Thus, for $R2$, the Huffman average is strictly less than $2$.\n\n3) If $2p_{l}=p_{h}$ (equivalently $R=2$), then the three node weights $\\{p_{h},p_{h},2p_{l}\\}$ are equal. Any tie-breaking is possible:\n- If the two $p_{h}$ are merged, the resulting code is balanced with all lengths $2$, average $2$.\n- If $p_{h}$ is merged with $2p_{l}$, the lengths are $\\{1,2,3,3\\}$ and the average is\n$$\nL=\\frac{3}{2}+3p_{l}=\\frac{3}{2}+3\\cdot\\frac{1}{6}=2,\n$$\nsince $2p_{l}=p_{h}$ together with $2p_{h}+2p_{l}=1$ gives $p_{l}=\\frac{1}{6}$ and $p_{h}=\\frac{1}{3}$.\n\nTherefore, the Huffman average codeword length is guaranteed to be exactly $2$ bits per symbol for all tie-breakings if and only if $2p_{l}\\ge p_{h}$, i.e., $\\frac{p_{h}}{p_{l}}\\le 2$. The threshold ratio that characterizes this guarantee is\n$$\nR=2.\n$$",
            "answer": "$$\\boxed{2}$$"
        }
    ]
}