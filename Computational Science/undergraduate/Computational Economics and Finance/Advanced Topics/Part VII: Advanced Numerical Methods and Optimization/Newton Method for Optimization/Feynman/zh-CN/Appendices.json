{
    "hands_on_practices": [
        {
            "introduction": "牛顿法以其二次收敛速度而著称，但这并不意味着它总能成功找到最优解。在某些情况下，该方法可能无法收敛，甚至会陷入一个循环，在几个点之间来回振荡。这个练习旨在通过一个思想实验，让你亲手构造一个导致牛顿法进入严格2-循环的效用函数。通过分析这个看似反常的案例，你将对牛顿法的迭代动力学及其对函数曲率的敏感性有更深刻的理解。",
            "id": "2414715",
            "problem": "在计算经济学和金融学中，常使用迭代二阶方法来最大化单变量效用指数。设 $u:\\mathbb{R}\\to\\mathbb{R}$ 是二阶连续可微的。从当前迭代点 $x_k$ 开始以最大化 $u$ 的牛顿更新定义为\n$$\nx_{k+1}\\;=\\;x_k\\;-\\;\\frac{u'(x_k)}{u''(x_k)}\\,,\n$$\n当 $u''(x_k)\\neq 0$ 时。两点 $x=a$ 和 $x=b$ 之间的一个严格2-周期意味着，当 $x_k=a$ 时 $x_{k+1}=b$，且当 $x_k=b$ 时 $x_{k+1}=a$，同时满足 $u'(a)\\neq 0$，$u'(b)\\neq 0$，$u''(a)\\neq 0$ 和 $u''(b)\\neq 0$。\n\n以下哪个效用函数在用于最大化的牛顿更新下，在 $x=0$ 和 $x=1$ 之间生成一个严格2-周期（即当 $x_k=0$ 时 $x_{k+1}=1$，且当 $x_k=1$ 时 $x_{k+1}=0$），并且在 $x=0$ 和 $x=1$ 处的更新是良定义的，且一阶和二阶导数均非零？\n\nA. $u(x)=\\dfrac{x^3}{3}-\\dfrac{x^2}{2}+x$\n\nB. $u(x)=-\\dfrac{1}{2}\\left(x-\\dfrac{1}{2}\\right)^2$\n\nC. $u(x)=\\dfrac{x^3}{3}+\\dfrac{x^2}{2}+x$\n\nD. $u(x)=\\dfrac{x^3}{3}-x$",
            "solution": "首先将对问题陈述进行严格的验证过程。\n\n### 步骤1：提取已知条件\n问题陈述中明确给出的信息如下：\n- 效用指数是一个二阶连续可微的函数 $u:\\mathbb{R}\\to\\mathbb{R}$。\n- 从一个迭代点 $x_k$ 开始最大化 $u(x)$ 的牛顿更新由以下公式给出：\n$$\nx_{k+1} = x_k - \\frac{u'(x_k)}{u''(x_k)}\n$$\n此更新仅在 $u''(x_k) \\neq 0$ 时有定义。\n- 两个不同点 $x=a$ 和 $x=b$ 之间的一个严格2-周期由以下条件定义：\n    1.  当 $x_k=a$ 时，则 $x_{k+1}=b$。\n    2.  当 $x_k=b$ 时，则 $x_{k+1}=a$。\n    3.  $u'(a) \\neq 0$ 且 $u'(b) \\neq 0$。\n    4.  $u''(a) \\neq 0$ 且 $u''(b) \\neq 0$。\n- 具体问题要求确定一个在 $a=0$ 和 $b=1$ 之间生成严格2-周期的效用函数。\n\n### 步骤2：使用提取的已知条件进行验证\n根据既定标准对问题进行审查。\n- **科学依据**：该问题基于牛顿法，这是数值分析中应用于最优化的一个基本算法。不动点和周期的概念是由此类迭代方法生成的动力系统研究中的一个标准课题。这是一个完全有效的科学和数学背景。\n- **适定性**：问题对严格2-周期和具体的迭代映射给出了清晰无歧义的定义。它要求从给定函数中找出满足这些明确陈述的数学条件的函数。这个任务是良定义的，并且可以在给定选项中做出唯一的确定。\n- **客观性**：问题使用形式化的数学语言和定义进行陈述。没有主观性、歧义或基于观点的内容。\n\n### 步骤3：结论与行动\n问题陈述是有效的。它具有科学依据、是适定的且客观的。没有矛盾、信息缺失或其他逻辑缺陷。因此，我们可以着手推导解决方案。\n\n### 解题推导\n点 $a=0$ 和 $b=1$ 之间严格2-周期的条件必须转化为涉及效用函数 $u(x)$ 及其导数的方程。\n\n第一个条件是，从 $x_k=0$ 开始的迭代会得到 $x_{k+1}=1$。应用牛顿更新公式：\n$$\n1 = 0 - \\frac{u'(0)}{u''(0)}\n$$\n这可以简化为：\n$$\n1 = -\\frac{u'(0)}{u''(0)} \\implies u''(0) = -u'(0)\n$$\n\n第二个条件是，从 $x_k=1$ 开始的迭代会得到 $x_{k+1}=0$。再次应用牛顿更新公式：\n$$\n0 = 1 - \\frac{u'(1)}{u''(1)}\n$$\n这可以简化为：\n$$\n1 = \\frac{u'(1)}{u''(1)} \\implies u''(1) = u'(1)\n$$\n\n此外，具有*良定义*更新的*严格*2-周期的定义施加了以下非零约束：\n1.  $u'(0) \\neq 0$\n2.  $u'(1) \\neq 0$\n3.  $u''(0) \\neq 0$\n4.  $u''(1) \\neq 0$\n\n注意到，如果 $u'(0) \\neq 0$，则条件 $u''(0) = -u'(0)$ 自动蕴含 $u''(0) \\neq 0$。类似地，如果 $u'(1) \\neq 0$，则条件 $u''(1) = u'(1)$ 蕴含 $u''(1) \\neq 0$。因此，我们只需要对每个候选函数检查四个主要条件：\n1.  $u''(0) = -u'(0)$\n2.  $u''(1) = u'(1)$\n3.  $u'(0) \\neq 0$\n4.  $u'(1) \\neq 0$\n\n我们现在将根据这四个条件来评估每个选项。\n\n### 逐项分析\n\n**A. $u(x)=\\dfrac{x^3}{3}-\\dfrac{x^2}{2}+x$**\n首先，我们计算一阶和二阶导数：\n$u'(x) = \\frac{d}{dx}\\left(\\frac{x^3}{3}-\\frac{x^2}{2}+x\\right) = x^2 - x + 1$\n$u''(x) = \\frac{d}{dx}(x^2 - x + 1) = 2x - 1$\n\n现在，我们计算这些导数在 $x=0$ 和 $x=1$ 处的值。\n在 $x=0$ 处：\n$u'(0) = 0^2 - 0 + 1 = 1$\n$u''(0) = 2(0) - 1 = -1$\n检查在 $x=0$ 处的条件：\n- $u''(0) = -u'(0)$ 是否成立？是的，$-1 = -1$。\n- $u'(0) \\neq 0$ 是否成立？是的，$1 \\neq 0$。\n\n在 $x=1$ 处：\n$u'(1) = 1^2 - 1 + 1 = 1$\n$u''(1) = 2(1) - 1 = 1$\n检查在 $x=1$ 处的条件：\n- $u''(1) = u'(1)$ 是否成立？是的，$1 = 1$。\n- $u'(1) \\neq 0$ 是否成立？是的，$1 \\neq 0$。\n所有四个条件都满足。\n结论：**正确**\n\n**B. $u(x)=-\\dfrac{1}{2}\\left(x-\\dfrac{1}{2}\\right)^2$**\n首先，我们计算导数：\n$u'(x) = \\frac{d}{dx}\\left(-\\frac{1}{2}\\left(x-\\frac{1}{2}\\right)^2\\right) = -\\left(x-\\frac{1}{2}\\right) = -x + \\frac{1}{2}$\n$u''(x) = \\frac{d}{dx}\\left(-x + \\frac{1}{2}\\right) = -1$\n\n现在，我们计算在 $x=0$ 处的值：\n$u'(0) = -0 + \\frac{1}{2} = \\frac{1}{2}$\n$u''(0) = -1$\n检查条件1：$u''(0) = -u'(0)$ 是否成立？不成立，$-1 \\neq -\\frac{1}{2}$。\n第一个条件不满足。无需继续进行。\n结论：**不正确**\n\n**C. $u(x)=\\dfrac{x^3}{3}+\\dfrac{x^2}{2}+x$**\n首先，我们计算导数：\n$u'(x) = \\frac{d}{dx}\\left(\\frac{x^3}{3}+\\frac{x^2}{2}+x\\right) = x^2 + x + 1$\n$u''(x) = \\frac{d}{dx}(x^2 + x + 1) = 2x + 1$\n\n现在，我们计算在 $x=0$ 处的值：\n$u'(0) = 0^2 + 0 + 1 = 1$\n$u''(0) = 2(0) + 1 = 1$\n检查条件1：$u''(0) = -u'(0)$ 是否成立？不成立，$1 \\neq -1$。\n第一个条件不满足。\n结论：**不正确**\n\n**D. $u(x)=\\dfrac{x^3}{3}-x$**\n首先，我们计算导数：\n$u'(x) = \\frac{d}{dx}\\left(\\frac{x^3}{3}-x\\right) = x^2 - 1$\n$u''(x) = \\frac{d}{dx}(x^2 - 1) = 2x$\n\n现在，我们计算在 $x=0$ 处的值：\n$u''(0) = 2(0) = 0$\n严格2-周期的定义条件之一是 $u''(a)\\neq 0$ 和 $u''(b)\\neq 0$。此处，$u''(0)=0$，这意味着牛顿更新在 $x=0$ 处没有定义。因此，该函数不能生成包含点 $x=0$ 的周期。\n结论：**不正确**\n\n根据详尽的分析，只有选项A满足在 $x=0$ 和 $x=1$ 之间形成严格2-周期的所有必要条件。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "知道了纯粹的牛顿法可能失败，我们自然会为其添加“安全带”，例如回溯线搜索，以确保每一步都能使目标函数充分下降。但如果安全带本身就有问题呢？这个问题模拟了一个真实的调试场景：一个本应收敛的算法，其目标函数值却在不断上升。你的任务是扮演算法侦探，通过分析诊断日志，找出导致阿米霍(Armijo)条件失效的根本原因，从而掌握调试优化算法的关键技能。",
            "id": "2414722",
            "problem": "一家金融机构通过最小化逻辑斯蒂回归模型的负对数似然来估计一个二元违约概率模型，该方法使用牛顿法（Newton's method）和旨在满足 Armijo 充分下降条件的回溯线搜索。参数向量记为 $\\beta \\in \\mathbb{R}^p$，目标函数记为 $f(\\beta)$，梯度记为 $\\nabla f(\\beta)$，海森矩阵（Hessian）记为 $\\nabla^2 f(\\beta)$。该实现打印出以下诊断信息。\n\n在迭代 $k=0$ 时：\n- $f(\\beta_0) = 120.35$。\n- $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$。\n- $\\nabla^2 f(\\beta_0)$ 的最小和最大特征值分别为 $\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\max} = 214.2$。\n- 求解牛顿方向的线性系统，报告的内积为 $\\nabla f(\\beta_0)^\\top s_0 = -28.9$。\n- 参数为 $c_1 = 10^{-4}$ 的回溯线搜索接受步长 $t_0 = 1$，并显示信息“Armijo satisfied”，代码报告 $f(\\beta_0 + t_0 s_0) = 134.7$。\n\n在迭代 $k=1$ 时：\n- $f(\\beta_1) = 134.7$。\n- $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$。\n- 报告的 $\\nabla^2 f(\\beta_1)$ 的最小特征值为 $\\lambda_{\\min} = 2.7$。\n- 计算出的方向满足 $\\nabla f(\\beta_1)^\\top s_1 = -31.4$。\n- 线搜索再次接受步长 $t_1 = 1$，并显示信息“Armijo satisfied”，且 $f(\\beta_1 + t_1 s_1) = 150.2$。\n\n假设逻辑斯蒂模型的负对数似然被正确设定且二阶连续可微，并且数据集规模和条件适中，因此当海森矩阵为正定时，数值线性代数是稳定的。\n\n基于这些诊断信息以及带 Armijo 回溯的无约束最小化牛顿法的标准属性，哪种实现问题与观察到的不收敛现象最一致？\n\nA. 海森矩阵在组装时缺少了交叉偏导数项，导致其不定，并产生了非下降方向。\n\nB. 求解牛顿系统时符号错误，实际上使用了 $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$，这是一个上升方向。\n\nC. 线搜索中的 Armijo 充分下降测试在实现时不等号方向反了，因此错误地接受了使 $f$ 增加的步长。\n\nD. 算法在 $\\lVert \\nabla f(\\beta_k) \\rVert_2$ 上的停止准则过于严格，导致它在接近最小值点时继续迭代而不是终止。",
            "solution": "用户提供了一个问题陈述，描述了一个用于最小化负对数似然函数的牛顿法错误实现的输出。我必须验证该陈述，然后根据所提供的数据诊断出最可能的实现错误。\n\n**问题验证**\n\n首先，我将提取给定的信息并验证问题陈述。\n\n**已知条件：**\n-   **算法**：用于最小化目标函数 $f(\\beta)$ 的牛顿法。\n-   **更新规则**：$\\beta_{k+1} = \\beta_k + t_k s_k$，其中 $s_k$ 是牛顿方向。\n-   **牛顿方向**：$s_k$ 是 $\\nabla^2 f(\\beta_k) s_k = - \\nabla f(\\beta_k)$ 的解。\n-   **线搜索**：为满足 Armijo 充分下降条件而进行的回溯，参数为 $c_1 = 10^{-4}$。\n-   **Armijo 条件**：$f(\\beta_k + t_k s_k) \\le f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$。\n-   **假设**：函数 $f$ 是逻辑斯蒂模型的二阶连续可微的负对数似然。当海森矩阵为正定时，假设数值计算是稳定的。\n\n**迭代 $k=0$ 时的数据：**\n-   $f(\\beta_0) = 120.35$\n-   $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$\n-   $\\nabla^2 f(\\beta_0)$ 的特征值：$\\lambda_{\\min} = 3.1$，$\\lambda_{\\max} = 214.2$。\n-   $\\nabla f(\\beta_0)^\\top s_0 = -28.9$\n-   接受的步长：$t_0 = 1$。\n-   结果函数值：$f(\\beta_1) = f(\\beta_0 + t_0 s_0) = 134.7$。\n-   算法信息：“Armijo satisfied”。\n\n**迭代 $k=1$ 时的数据：**\n-   $f(\\beta_1) = 134.7$\n-   $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$\n-   $\\nabla^2 f(\\beta_1)$ 的最小特征值：$\\lambda_{\\min} = 2.7$。\n-   $\\nabla f(\\beta_1)^\\top s_1 = -31.4$\n-   接受的步长：$t_1 = 1$。\n-   结果函数值：$f(\\beta_2) = f(\\beta_1 + t_1 s_1) = 150.2$。\n-   算法信息：“Armijo satisfied”。\n\n**验证分析：**\n问题陈述在数值优化和统计学方面具有科学依据。所提供的数据似乎与算法的既定目标（通过 Armijo 规则进行最小化）相矛盾，但这是诊断问题的基础，而非问题陈述本身的缺陷。该问题是适定的、客观的，并包含足够的信息以进行逻辑诊断。对于逻辑斯蒂回归的负对数似然，其海森矩阵保证是半正定的，而对于非退化数据集，则是正定的。给定的正特征值（$\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\min} = 2.7$）与此性质一致。因此，该问题是有效的。\n\n**解题推导**\n\n算法的目标是最小化 $f(\\beta)$。这要求函数值在每一步都减小，即 $f(\\beta_{k+1}) < f(\\beta_k)$。Armijo 条件旨在保证这一点，甚至确保“充分”的下降。\n\n让我们分析迭代 $k=0$ 时的 Armijo 条件：\n条件是 $f(\\beta_0 + t_0 s_0) \\le f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0$。\n代入给定值：\n-   左侧 (LHS)：$f(\\beta_0 + s_0) = 134.7$。\n-   右侧 (RHS)：$f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0 = 120.35 + (10^{-4})(1)(-28.9) = 120.35 - 0.00289 = 120.34711$。\n\n正确的 Armijo 条件要求 $134.7 \\le 120.34711$，这是不成立的。尽管如此，算法仍然报告“Armijo satisfied”。此外，函数值增加了：$f(\\beta_1) = 134.7 > f(\\beta_0) = 120.35$。对于一个最小化算法来说，这是一个失败。\n\n让我们对迭代 $k=1$ 进行同样的分析：\n条件是 $f(\\beta_1 + t_1 s_1) \\le f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1$。\n代入给定值：\n-   左侧 (LHS)：$f(\\beta_1 + s_1) = 150.2$。\n-   右侧 (RHS)：$f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1 = 134.7 + (10^{-4})(1)(-31.4) = 134.7 - 0.00314 = 134.69686$。\n\n正确的 Armijo 条件要求 $150.2 \\le 134.69686$，这同样是不成立的。算法再次错误地报告“Armijo satisfied”，并且函数值增加：$f(\\beta_2) = 150.2 > f(\\beta_1) = 134.7$。\n\n该算法正在系统性地失败。它采取的步骤增加了目标函数值，并远离最小值点，梯度范数的增加也表明了这一点（$\\lVert \\nabla f(\\beta_1) \\rVert_2 > \\lVert \\nabla f(\\beta_0) \\rVert_2$）。在函数值增加的情况下，唯一可能打印出“Armijo satisfied”的原因是该条件的逻辑测试实现不正确。\n\n让我们考虑实现中不等号反向的可能性：\n$f(\\beta_k + t_k s_k) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$。\n\n-   在 $k=0$ 时，这将测试 $134.7 \\ge 120.34711$ 是否成立。这是成立的。\n-   在 $k=1$ 时，这将测试 $150.2 \\ge 134.69686$ 是否成立。这也是成立的。\n\n这个反向的不等式完美地解释了为什么步长 $t=1$ 在两次迭代中都被接受，尽管它导致了目标函数值的显著增加。\n\n**逐项分析**\n\nA. 海森矩阵在组装时缺少了交叉偏导数项，导致其不定，并产生了非下降方向。\n**分析**：这一说法与数据相矛盾。两次迭代中海森矩阵的最小特征值分别为 $\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\min} = 2.7$。由于最小特征值为正，海森矩阵是正定的，而不是不定的。此外，计算出的方向是下降方向，因为它们与梯度的内积 $\\nabla f(\\beta_0)^\\top s_0 = -28.9$ 和 $\\nabla f(\\beta_1)^\\top s_1 = -31.4$ 均为负数。\n**结论**：不正确。\n\nB. 求解牛顿系统时符号错误，实际上使用了 $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$，这是一个上升方向。\n**分析**：如果是这种情况，搜索方向 $s_k$ 将是一个上升方向。对于正定的海森矩阵 $\\nabla^2 f(\\beta_k)$，方向导数 $\\nabla f(\\beta_k)^\\top s_k$ 将会是 $\\nabla f(\\beta_k)^\\top [\\nabla^2 f(\\beta_k)]^{-1} \\nabla f(\\beta_k) > 0$。然而，提供的数据显示两次迭代中 $\\nabla f(\\beta_k)^\\top s_k < 0$。因此，计算出的方向是下降方向，而不是上升方向。\n**结论**：不正确。\n\nC. 线搜索中的 Armijo 充分下降测试在实现时不等号方向反了，因此错误地接受了使 $f$ 增加的步长。\n**分析**：如上面的推导所示，如果 Armijo 条件检查被实现为 $f(\\beta_{k+1}) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$，算法将接受步长 $t_0=1$ 和 $t_1=1$，并在每次迭代时报告“Armijo satisfied”。这一假设与观察到的函数值增加（$120.35 \\to 134.7 \\to 150.2$）完全一致，并与所有提供的数值数据相匹配。\n**结论**：正确。\n\nD. 算法在 $\\lVert \\nabla f(\\beta_k) \\rVert_2$ 上的停止准则过于严格，导致它在接近最小值点时继续迭代而不是终止。\n**分析**：停止准则的性质与每次迭代*内部*观察到的错误行为无关。核心问题是算法正在发散（函数值和梯度范数都在增加），而不是收敛缓慢且未能停止。一个严格的停止准则不会导致发散；它仅仅意味着如果算法正在收敛，它会运行更长时间。观察到的行为是优化步骤本身的根本性失败。\n**结论**：不正确。",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "在诊断了牛顿法的潜在陷阱和实现错误之后，是时候构建一个真正稳健的求解器了。在实践中，最有效的优化算法通常是混合的，它们结合了不同方法的优点。这个综合性的编程练习将指导你实现一个“智能”的牛顿法，它在情况有利时利用牛顿方向进行快速收敛，但在遇到非正定Hessian矩阵或不满足下降条件时，能自动切换到更可靠的梯度下降方向。完成这个练习后，你将拥有一个强大的、能解决广泛经济和金融问题的优化工具。",
            "id": "2414720",
            "problem": "考虑光滑实值函数的无约束最小化问题，其更新规则在每次迭代中都强制执行一个充分下降条件。令 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是二次连续可微函数。在迭代点 $x_k \\in \\mathbb{R}^n$ 处，定义梯度 $\\nabla f(x_k)$ 和 Hessian 矩阵 $\\nabla^2 f(x_k)$。候选更新 $x_{k+1} = x_k + \\alpha_k p_k$ 必须满足 Armijo 充分下降条件，其常数为 $c_1 \\in (0,1)$ 和 $\\rho \\in (0,1)$，即\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n其中 $\\alpha_k$ 通过回溯法选取，形式为 $\\alpha_k \\in \\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$，从 $\\alpha_k = 1$ 开始。对于一个给定的 $x_k$，首先尝试选择 $p_k$ 作为以下线性系统的解\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k).\n$$\n如果该候选方向未能在回溯序列中产生满足充分下降条件的步长 $\\alpha_k$，则拒绝该方向，转而选择 $p_k = -\\nabla f(x_k)$，并通过相同的回溯充分下降条件确定 $\\alpha_k$。当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或已执行最大迭代次数 $K$ 时，迭代终止。\n\n使用以下固定常数实现此迭代方案：$c_1 = 10^{-4}$，$\\rho = 0.5$，容差 $\\varepsilon = 10^{-8}$，每次迭代的回溯限制为 $M = 50$ 次缩减，以及最大迭代次数 $K = 50$。如果在某个 $x_k$ 处线性系统是奇异的或病态的，则将由 $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ 定义的方向视为失败，并如上所述采用 $p_k = -\\nabla f(x_k)$。所有计算都应在实数算术中执行，无需任何外部数据输入。\n\n将此方案应用于以下测试套件。在所有情况下，使用欧几里得范数 $\\|\\cdot\\|_2$ 作为终止准则，并在每次迭代中从步长 $\\alpha_k = 1$ 开始回溯。报告每种情况下由终止条件返回的极小点估计值。将报告的每个数字表示为四舍五入到小数点后恰好六位的小数。\n\n测试用例 A（双参数的二元选择负对数似然）。令参数为 $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$。令数据为 $\\{(x_i,y_i)\\}_{i=1}^5$，具体如下\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\n定义负对数似然\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\n使用初始点 $x_0 = (0,0)^\\top$。\n\n测试用例 B（病态二次函数）。令 $x \\in \\mathbb{R}^2$。定义\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6} & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\n使用初始点 $x_0 = (10,-10)^\\top$。\n\n测试用例 C（非凸一维四次函数）。令 $x \\in \\mathbb{R}$。定义\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\n使用初始点 $x_0 = 0.2$。\n\n输出规范。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果按顺序显示如下：\n- 对于测试用例 A：一个包含两个数字的列表 $[b_0^\\star,b_1^\\star]$,\n- 对于测试用例 B：一个包含两个数字的列表 $[x_1^\\star,x_2^\\star]$,\n- 对于测试用例 C：一个数字 $x^\\star$,\n其中每个数字都四舍五入到小数点后恰好六位。例如，格式必须为\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\n打印行中不允许有空格。所有数字都应表示为普通小数。",
            "solution": "所提出的问题是数值优化中一个定义明确的任务。它要求实现一个混合迭代算法，该算法结合了牛顿法和最速下降法，并辅以回溯线搜索以确保每一步都有充分的下降。该问题在科学上是合理的，基于非线性优化的既定原则，并为三个不同的测试用例提供了所有必要的参数、初始条件和目标函数。因此，该问题被认为是有效的。\n\n待实现的算法是针对二次连续可微函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化过程。给定一个迭代点 $x_k$，下一个迭代点 $x_{k+1}$ 通过 $x_{k+1} = x_k + \\alpha_k p_k$ 求得，其中 $p_k$是搜索方向，$\\alpha_k$是步长。\n\n该算法的核心是搜索方向 $p_k$ 的选择。主要选择是牛顿方向，通过求解以下线性系统获得：\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\n其中 $\\nabla f(x_k)$ 是 $f$ 在 $x_k$ 处的梯度，$\\nabla^2 f(x_k)$ 是 Hessian 矩阵。对于二次函数，该方向是最优的，并且在 Hessian 矩阵为正定的极小点附近提供快速的局部收敛（二次收敛）。然而，如果 Hessian 矩阵是奇异的，牛顿方向可能未定义；或者如果 Hessian 矩阵非正定，它可能不是一个下降方向。如果 $\\nabla f(x_k)^\\top p_k < 0$，则方向 $p_k$ 是一个下降方向。对于牛顿方向，$\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$，该值仅在 $\\nabla^2 f(x_k)$ 是正定时为负。如果牛顿方向不是下降方向或未能产生合适的步长，算法必须切换到一个稳健的替代方案。\n\n备用方向是最速下降方向，$p_k = -\\nabla f(x_k)$。只要梯度非零，该方向就保证是一个下降方向，因为 $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2 < 0$。虽然其收敛速度通常较慢（线性收敛），但它能确保在任何 $\\nabla f(x_k) \\neq 0$ 的点都能取得进展。\n\n步长 $\\alpha_k$ 由回溯线搜索确定。从试探步长 $\\alpha = 1$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$ 进行缩减，直到满足 Armijo 充分下降条件：\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\n对于常数 $c_1 \\in (0,1)$。问题设定 $c_1 = 10^{-4}$ 和 $\\rho = 0.5$。施加了 $M=50$ 步的回溯限制。\n\n整个迭代过程如下：\n对于 $k = 0, 1, 2, \\ldots, K-1$：\n1. 计算梯度 $g_k = \\nabla f(x_k)$。如果 $\\|g_k\\|_2 \\le \\varepsilon$，则终止。\n2. 尝试计算牛顿方向 $p_{\\text{Newton}}$。如果 $\\nabla^2f(x_k)$ 是奇异的，或如果产生的方向不是下降方向（$\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$），则此尝试失败。\n3. 如果牛顿方向有效，则执行回溯线搜索。如果在 $M$ 次缩减内找到了步长 $\\alpha_k$，则使用 $p_k = p_{\\text{Newton}}$ 和 $\\alpha_k$ 执行更新。\n4. 如果牛顿方向步骤因任何原因（奇异性、非下降方向或回溯失败）而失败，算法将恢复到最速下降方向 $p_k = -\\nabla f(x_k)$，并执行另一次回溯线搜索来寻找 $\\alpha_k$。\n5. 更新迭代点：$x_{k+1} = x_k + \\alpha_k p_k$。\n如果梯度范数低于容差 $\\varepsilon=10^{-8}$ 或达到最大迭代次数 $K=50$，则过程终止。\n\n该实现将把此逻辑应用于三个测试用例。对于每个用例，我们必须提供目标函数 $f(x)$、其梯度 $\\nabla f(x)$ 和其 Hessian 矩阵 $\\nabla^2 f(x)$ 的解析形式。\n\n**测试用例 A：二元选择负对数似然**\n目标函数是逻辑回归模型的负对数似然。\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$。\n令 $u_i(b) = b_0 + b_1 x_i$。sigmoid 函数是 $\\sigma(u) = 1/(1+e^{-u})$。\n梯度分量是：\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\nHessian 分量是（使用 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$）：\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n该 Hessian 矩阵总是半正定的，确保牛顿方向（如果已定义）是一个下降方向。\n\n**测试用例 B：病态二次函数**\n目标函数是 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$。\n梯度是 $\\nabla f(x) = Qx - b$。\nHessian 矩阵是常数：$\\nabla^2 f(x) = Q$。\n由于 Q 是一个常数正定矩阵，使用完整步长（$\\alpha=1$）的牛顿法将在单次迭代中找到精确的极小点 $x^*=Q^{-1}b$。\n\n**测试用例 C：非凸一维四次函数**\n目标函数是 $f(x) = x^4 - 3x^2 + x$。\n梯度（导数）是 $f'(x) = 4x^3 - 6x + 1$。\nHessian 矩阵（二阶导数）是 $f''(x) = 12x^2 - 6$。\n在初始点 $x_0 = 0.2$ 处，Hessian 矩阵为 $f''(0.2) = 12(0.04) - 6 = -5.52$，是负值。因此，在 $x_0$ 处的牛顿方向将是一个上升方向，算法在第一次迭代时必须回退到最速下降法。\n\n下面的 Python 代码实现了所述算法，并将其应用于三个测试用例，按规定格式化输出。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) <= EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton) < 0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) <= fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) <= fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        with np.errstate(over='ignore'):\n          # Sigmoid function: 1 / (1 + exp(-u))\n          sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        with np.errstate(over='ignore'):\n            sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[{','.join(formatted_parts)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}