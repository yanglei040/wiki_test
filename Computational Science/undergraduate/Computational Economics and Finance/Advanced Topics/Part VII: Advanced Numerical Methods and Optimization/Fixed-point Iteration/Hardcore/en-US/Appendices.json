{
    "hands_on_practices": [
        {
            "introduction": "The foundation of using fixed-point iteration effectively lies in choosing the right iteration function, $g(x)$. For any given equation $f(x)=0$, there are often multiple ways to rearrange it into the form $x=g(x)$. This exercise demonstrates that the choice of $g(x)$ is not arbitrary; it critically determines the speed of convergence. By analytically comparing the convergence rates of two different schemes for the same problem, you will gain a deeper, quantitative understanding of why one method can be vastly superior to another .",
            "id": "2214069",
            "problem": "The equation $f(x) = e^x - 2x - 1 = 0$ has a trivial root at $x=0$ and a unique positive root, which we denote by $\\alpha$. To numerically approximate this positive root $\\alpha$, two different fixed-point iteration schemes are proposed.\n\nScheme A is defined by the iteration function $g_A(x) = \\frac{e^x - 1}{2}$.\nScheme B is defined by the iteration function $g_B(x) = \\ln(2x+1)$.\n\nThe local behavior of a fixed-point iteration method, whether it converges to or diverges from a root, is characterized by its asymptotic rate constant. For an iteration function $g(x)$ and a root $\\alpha$, this constant is given by $C = |g'(\\alpha)|$. A value of $C  1$ indicates convergence, while $C  1$ indicates divergence.\n\nLet the rate constants for Scheme A and Scheme B at the root $\\alpha$ be $C_A = |g'_A(\\alpha)|$ and $C_B = |g'_B(\\alpha)|$, respectively.\n\nGiven that the positive root is $\\alpha \\approx 1.256431$, calculate the numerical value of the ratio $R = \\frac{C_A}{C_B}$.\n\nRound your final answer to four significant figures.",
            "solution": "We are given $f(x)=\\exp(x)-2x-1$ with a positive root $\\alpha$. Two fixed-point iterations are defined by $g_{A}(x)=\\frac{\\exp(x)-1}{2}$ and $g_{B}(x)=\\ln(2x+1)$. For a fixed-point iteration $x_{n+1}=g(x_{n})$ converging to $\\alpha$, the asymptotic rate constant is $C=|g'(\\alpha)|$.\n\nCompute derivatives:\n$$\ng_{A}'(x)=\\frac{1}{2}\\exp(x), \\quad g_{B}'(x)=\\frac{2}{2x+1}.\n$$\nTherefore,\n$$\nC_{A}=|g_{A}'(\\alpha)|=\\frac{1}{2}\\exp(\\alpha), \\quad C_{B}=|g_{B}'(\\alpha)|=\\frac{2}{2\\alpha+1}.\n$$\nThe ratio is\n$$\nR=\\frac{C_{A}}{C_{B}}=\\frac{\\frac{1}{2}\\exp(\\alpha)}{\\frac{2}{2\\alpha+1}}=\\frac{\\exp(\\alpha)\\,(2\\alpha+1)}{4}.\n$$\nSince $\\alpha$ satisfies $f(\\alpha)=0$, we have $\\exp(\\alpha)-2\\alpha-1=0$, hence $\\exp(\\alpha)=2\\alpha+1$. Substituting,\n$$\nR=\\frac{(2\\alpha+1)^{2}}{4}.\n$$\nWith $\\alpha\\approx 1.256431$,\n$$\n2\\alpha+1=2(1.256431)+1=3.512862,\n$$\n$$\n(2\\alpha+1)^{2}=(3.512862)^{2}=12.340199431044,\n$$\n$$\nR\\approx \\frac{12.340199431044}{4}=3.085049857761.\n$$\nRounding to four significant figures gives $R\\approx 3.085$.",
            "answer": "$$\\boxed{3.085}$$"
        },
        {
            "introduction": "Moving from theory to implementation, this practice challenges you to compare one of the most famous root-finding algorithms, Newton's method, with another fixed-point scheme. You will see firsthand how Newton's method, which can be viewed as a particularly powerful type of fixed-point iteration, often achieves its solution in remarkably few steps due to its quadratic convergence. This coding exercise provides a tangible feel for different orders of convergence and highlights the practical importance of an algorithm's robustness and its dependence on the initial guess .",
            "id": "2393795",
            "problem": "Consider the nonlinear equation $f(x;A)=x^2-A=0$ for a given parameter $A>0$. For each parameter pair $(A,x_0)$, define two iterative sequences $\\{x_k^{(F)}\\}_{k\\geq 0}$ and $\\{x_k^{(N)}\\}_{k\\geq 0}$ initialized at $x_0$ as follows:\n1. Sequence $F$: for $k\\geq 0$,\n$$\nx_{k+1}^{(F)} \\equiv x_k^{(F)} - \\lambda(A)\\,\\big((x_k^{(F)})^2 - A\\big),\n$$\nwhere $\\lambda(A)\\equiv \\dfrac{1}{2\\sqrt{A}}$.\n2. Sequence $N$: for $k\\geq 0$ with $x_k^{(N)}\\neq 0$,\n$$\nx_{k+1}^{(N)} \\equiv x_k^{(N)} - \\dfrac{(x_k^{(N)})^2 - A}{2\\,x_k^{(N)}}.\n$$\n\nFor each sequence and each test case, iterate until the residual satisfies\n$$\n\\left| (x_k)^2 - A \\right| \\leq \\varepsilon,\n$$\nwith tolerance $\\varepsilon = 10^{-12}$, or until the number of iterations exceeds $N_{\\max}=10^5$. Report, for each sequence and test case, the number of iterations taken to meet the tolerance. If the tolerance is not met within $N_{\\max}$ iterations, report the integer $N_{\\max}+1$ for that sequence and test case.\n\nTest suite:\n- Case $1$: $A=2$, $x_0=1$.\n- Case $2$: $A=10^{-8}$, $x_0=1$.\n- Case $3$: $A=10^{6}$, $x_0=1$.\n- Case $4$: $A=\\dfrac{1}{4}$, $x_0=10^{-2}$.\n- Case $5$: $A=10$, $x_0=20$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n\\big[n_F^{(1)},n_N^{(1)},n_F^{(2)},n_N^{(2)},n_F^{(3)},n_N^{(3)},n_F^{(4)},n_N^{(4)},n_F^{(5)},n_N^{(5)}\\big],\n$$\nwhere $n_F^{(i)}$ and $n_N^{(i)}$ are the iteration counts for sequence $F$ and sequence $N$ respectively on test case $i\\in\\{1,2,3,4,5\\}$. All reported values must be integers. No physical units are involved. Angles are not involved. Do not use percentage signs; there are no percentages required.",
            "solution": "The problem requires implementing and comparing two iterative schemes to find the positive root of $x^2 - A = 0$, which is $x^* = \\sqrt{A}$. Both schemes are instances of fixed-point iteration, $x_{k+1} = g(x_k)$.\n\n**Sequence N** is the classic Newton-Raphson method for $f(x) = x^2 - A$, with iteration function $g_N(x) = \\frac{1}{2}(x + \\frac{A}{x})$. At the fixed point $x^* = \\sqrt{A}$, the derivative is $g_N'(\\sqrt{A}) = 0$, indicating quadratic convergence.\n\n**Sequence F** is a specialized fixed-point iteration with function $g_F(x) = x - \\frac{1}{2\\sqrt{A}}(x^2 - A)$. Its derivative is $g_F'(x) = 1 - \\frac{x}{\\sqrt{A}}$. At the fixed point, $g_F'(\\sqrt{A}) = 0$, also implying quadratic convergence. However, convergence is only guaranteed locally, for iterates within the interval $(0, 2\\sqrt{A})$.\n\nThe solution code implements both algorithms, iterating until the residual $|(x_k)^2 - A|$ is within the tolerance $\\varepsilon=10^{-12}$ or the maximum number of iterations is reached.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given problem by implementing and comparing two iterative schemes\n    for finding the square root of a number.\n    \"\"\"\n    # Define constants from the problem statement.\n    EPSILON = 1e-12\n    N_MAX = 100000\n\n    def solve_f(A, x0):\n        \"\"\"\n        Calculates sqrt(A) using Sequence F (specialized fixed-point iteration).\n        \n        Args:\n            A (float): The parameter A  0.\n            x0 (float): The initial guess.\n\n        Returns:\n            int: The number of iterations required for convergence, or N_MAX + 1.\n        \"\"\"\n        x = float(x0)\n        A_f = float(A)\n        \n        # Pre-calculate lambda(A). This requires the true sqrt(A).\n        lambda_A = 1.0 / (2.0 * np.sqrt(A_f))\n\n        # Check initial guess (k=0 iterations).\n        if abs(x**2 - A_f) = EPSILON:\n            return 0\n\n        for k in range(1, N_MAX + 1):\n            x = x - lambda_A * (x**2 - A_f)\n            if abs(x**2 - A_f) = EPSILON:\n                return k\n        \n        return N_MAX + 1\n\n    def solve_n(A, x0):\n        \"\"\"\n        Calculates sqrt(A) using Sequence N (Newton's method).\n        \n        Args:\n            A (float): The parameter A  0.\n            x0 (float): The initial guess.\n\n        Returns:\n            int: The number of iterations required for convergence, or N_MAX + 1.\n        \"\"\"\n        x = float(x0)\n        A_f = float(A)\n\n        # Check initial guess (k=0 iterations).\n        if abs(x**2 - A_f) = EPSILON:\n            return 0\n            \n        for k in range(1, N_MAX + 1):\n            # Division by zero is a potential issue for Newton's method in general,\n            # but will not occur for this problem's test cases (x0  0, A  0).\n            if x == 0.0:\n                return N_MAX + 1\n            x = x - (x**2 - A_f) / (2.0 * x)\n            if abs(x**2 - A_f) = EPSILON:\n                return k\n        \n        return N_MAX + 1\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (A, x0)\n        (2, 1),\n        (1e-8, 1),\n        (1e6, 1),\n        (1/4, 1e-2),\n        (10, 20)\n    ]\n\n    results = []\n    for A_val, x0_val in test_cases:\n        n_f = solve_f(A_val, x0_val)\n        n_n = solve_n(A_val, x0_val)\n        results.extend([n_f, n_n])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In many practical applications in economics and finance, a model naturally leads to a fixed-point problem that converges, but does so with slow, linear convergence. When the convergence factor is close to $1$, obtaining a high-precision solution can require an impractical number of iterations. This exercise introduces a powerful acceleration technique, Aitken's $\\Delta^2$ method, which uses the sequence of iterates to extrapolate a much better estimate of the limit. By applying this method to a classic asset-pricing model, you will learn how to significantly speed up computations, a vital skill for more complex modeling tasks .",
            "id": "2393814",
            "problem": "Consider the linear asset-pricing problem for a perpetuity with constant dividend. In a risk-neutral, one-period model, the no-arbitrage pricing restriction for the time-$0$ price $p$ of a consol with constant dividend $d$ and discount factor $\\beta$ implies that the price satisfies the fixed-point relation $p = g(p)$, where $g(p)$ is a self-map on $\\mathbb{R}$. Specifically, under constant discount factor $\\beta$ with $|\\beta|  1$, and constant dividend $d$, the mapping is $g(p) = d + \\beta p$. The mapping $g$ is a contraction on $\\mathbb{R}$ with modulus $|\\beta|$, and hence the fixed-point iteration $p_{k+1} = g(p_k)$ converges linearly to the unique fixed point for any initial guess $p_0 \\in \\mathbb{R}$. The purpose of this exercise is to implement and compare the basic fixed-point iteration with an acceleration scheme based on Aitken’s $\\Delta^2$ process, which combines three consecutive iterates to accelerate a linearly converging sequence.\n\nYour tasks are:\n- From first principles, implement the basic fixed-point iteration for the mapping $g(p) = d + \\beta p$.\n- Derive and implement an Aitken $\\Delta^2$ acceleration step that uses three consecutive iterates from the basic fixed-point iteration to construct an accelerated iterate. Your implementation must be numerically robust: if the denominator required by the acceleration step is zero or numerically too small in magnitude (leading to potential division instability), then skip the acceleration in that cycle by proceeding with the unaccelerated iterate produced by the basic fixed-point iteration.\n- Use the absolute fixed-point residual $|g(x) - x|$ as the stopping criterion. Stop when $|g(x) - x| \\le \\text{tol}$, where $\\text{tol} > 0$ is a given tolerance.\n\nImplementation details to enforce:\n- For the basic fixed-point iteration, each application of $g$ counts as one function evaluation. Report the number of iterations (which equals the number of function evaluations in this case).\n- For the Aitken-accelerated procedure, organize the computation in cycles. In each cycle, produce two consecutive iterates from the basic map $g$ starting from the current $x_0$ to obtain $x_1$ and $x_2$, then compute one accelerated iterate from $(x_0, x_1, x_2)$. Counting of function evaluations must include the two evaluations to produce $x_1$ and $x_2$, plus one more evaluation to compute the residual at the accelerated iterate (namely, evaluate $g(x_{\\text{acc}})$). Thus, a successful acceleration cycle typically uses three evaluations of $g$. If acceleration is skipped due to numerical safety, still evaluate the residual at the chosen iterate and count this evaluation. Continue cycling until the residual tolerance is met or a maximum number of cycles is reached.\n- Use the absolute fixed-point residual $|g(x) - x|$ with a given tolerance $\\text{tol}$ as the termination condition for both methods. Use the same maximum-iteration cap $N_{\\max}$ for both the number of basic iterations and the number of Aitken cycles, respectively.\n\nTest suite:\nFor each parameter tuple $(\\beta, d, p_0, \\text{tol}, N_{\\max})$ below, run both methods and collect results.\n\n1. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (0.9, 1.0, 0.0, 10^{-12}, 10000)$\n2. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (0.99, 1.0, 0.0, 10^{-12}, 10000)$\n3. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (-0.8, 1.0, 0.0, 10^{-12}, 10000)$\n4. $(\\beta, d, p_0, \\text{tol}, N_{\\max}) = (0.0, 2.0, 5.0, 10^{-12}, 10000)$\n\nFor each test case, produce a list with the following six entries:\n- The approximate fixed point returned by the basic iteration (a float).\n- The total number of basic iterations performed (an integer).\n- The total number of function evaluations in the basic method (an integer).\n- The approximate fixed point returned by the Aitken-accelerated procedure (a float).\n- The total number of Aitken cycles performed (an integer).\n- The total number of function evaluations in the Aitken method (an integer).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of four inner lists (one for each test case) enclosed in square brackets. Each inner list must have the six entries in the order specified above. All floats must be rounded to $12$ decimals. For example, the output should look like $[[x_{11},x_{12},\\dots],[x_{21},x_{22},\\dots],\\dots]$ with the exact values for this problem’s test cases.\n- No additional text should be printed.",
            "solution": "The problem requires finding the fixed point of the linear mapping $g(p) = d + \\beta p$. The analytical solution is $p^* = d/(1-\\beta)$. We compare two numerical methods.\n\n**Method 1 (Basic Iteration)** is the direct application of the recurrence $p_{k+1} = g(p_k)$, which converges linearly. The iteration stops when $|p_{k+1} - p_k| \\le \\text{tol}$.\n\n**Method 2 (Aitken's $\\Delta^2$ Acceleration)** aims to speed up this linear convergence. Within each cycle, it generates two standard iterates, $x_1=g(x_0)$ and $x_2=g(x_1)$, from a starting point $x_0$. It then uses these three points $(x_0, x_1, x_2)$ to compute an accelerated estimate of the limit:\n$$ x_{\\text{acc}} = x_0 - \\frac{(x_1 - x_0)^2}{x_2 - 2x_1 + x_0} $$\nA check is performed to ensure the denominator is not close to zero to avoid numerical instability. The stopping criterion is checked for the new iterate $x_{\\text{acc}}$. The solution code implements both methods, tracks the number of function evaluations for each, and applies them to the test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares basic fixed-point iteration and Aitken-accelerated\n    iteration for an asset-pricing problem.\n    \"\"\"\n\n    test_cases = [\n        (0.9, 1.0, 0.0, 1e-12, 10000),\n        (0.99, 1.0, 0.0, 1e-12, 10000),\n        (-0.8, 1.0, 0.0, 1e-12, 10000),\n        (0.0, 2.0, 5.0, 1e-12, 10000),\n    ]\n\n    all_results = []\n\n    for beta, d, p0, tol, n_max in test_cases:\n        \n        # Define the mapping function g(p)\n        g = lambda p: d + beta * p\n\n        # --- Method 1: Basic Fixed-Point Iteration ---\n        p_basic = p0\n        evals_basic = 0\n        iters_basic = 0\n        for i in range(1, n_max + 1):\n            p_next = g(p_basic)\n            evals_basic += 1\n            iters_basic += 1\n            \n            residual = abs(p_next - p_basic)\n            p_basic = p_next\n            \n            if residual = tol:\n                break\n        \n        # --- Method 2: Aitken's Delta^2 Acceleration ---\n        p_aitken = p0\n        evals_aitken = 0\n        cycles_aitken = 0\n        denominator_threshold = 1e-16 # For numerical stability\n\n        for k in range(1, n_max + 1):\n            cycles_aitken += 1\n            \n            x0 = p_aitken\n            \n            # Step 1: Generate two standard iterates\n            x1 = g(x0)\n            x2 = g(x1)\n            evals_aitken += 2\n            \n            # Step 2  3: Compute denominator and check for stability\n            denominator = x2 - 2 * x1 + x0\n            \n            p_next = 0.0\n            if abs(denominator)  denominator_threshold:\n                # Skip acceleration, proceed with standard iterate\n                p_next = x2\n            else:\n                # Step 4: Compute accelerated iterate\n                numerator = (x1 - x0)**2\n                p_next = x0 - numerator / denominator\n            \n            # Step 5: Check stopping criterion\n            g_p_next = g(p_next)\n            evals_aitken += 1\n            residual = abs(g_p_next - p_next)\n            \n            p_aitken = p_next\n            \n            if residual = tol:\n                break\n        \n        case_results = [\n            p_basic, \n            iters_basic, \n            evals_basic,\n            p_aitken, \n            cycles_aitken, \n            evals_aitken\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res in all_results:\n        p_b, i_b, e_b, p_a, c_a, e_a = res\n        s = f\"[{p_b:.12f},{i_b},{e_b},{p_a:.12f},{c_a},{e_a}]\"\n        formatted_results.append(s)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}