## Introduction
Fixed-point iteration is a cornerstone numerical method used across science and engineering, offering a simple yet powerful approach to solving equations that often defy direct analytical solutions. Its significance lies in its ability to model and compute equilibrium states, which are often defined by conditions of self-consistencyâ€”where a system's state variable is a function of itself. The core problem this method addresses is how to find these equilibrium points reliably and understand the dynamics of convergence towards them. This article provides a comprehensive guide to this essential technique. In the following chapters, you will learn the theoretical foundations that govern the method's success, explore its vast applications in identifying equilibria across various disciplines, and prepare for practical implementation through guided exercises.

The journey begins in **"Principles and Mechanisms,"** where we will dissect the core idea of a fixed point and establish the crucial conditions for convergence, most notably the Contraction Mapping Theorem. Next, **"Applications and Interdisciplinary Connections"** will showcase the versatility of fixed-point thinking, demonstrating how it underpins the computation of equilibria in macroeconomic models, [game theory](@entry_id:140730), financial [derivative pricing](@entry_id:144008), and even Google's PageRank algorithm. Finally, **"Hands-On Practices"** will bridge theory and practice, offering coding challenges that solidify your understanding of convergence rates and acceleration techniques.

## Principles and Mechanisms

Fixed-point iteration is a powerful and versatile numerical method that lies at the heart of many algorithms in [computational economics](@entry_id:140923) and finance. It provides a straightforward framework for solving a wide array of equations, from finding market equilibria to analyzing the stability of dynamic systems. The core principle is to transform an equation into a form where a variable is expressed as a function of itself, and then to iteratively apply this function until the input and output values converge. This chapter delves into the fundamental principles that govern when and how this iterative process succeeds, and the mechanisms that determine its behavior.

### The Core Idea: Seeking a Fixed Point

A **fixed point** of a function $g(x)$ is a value $x^*$ such that $g(x^*) = x^*$. Geometrically, this is where the graph of the function $y=g(x)$ intersects the line $y=x$. The [fixed-point iteration method](@entry_id:168837) attempts to find such a point by generating a sequence of values according to the recurrence relation:

$$x_{n+1} = g(x_n)$$

starting from an initial guess $x_0$. If this sequence converges, i.e., $\lim_{n \to \infty} x_n = x^*$, then by continuity of $g$, the limit $x^*$ must be a fixed point:

$$x^* = \lim_{n \to \infty} x_{n+1} = \lim_{n \to \infty} g(x_n) = g(\lim_{n \to \infty} x_n) = g(x^*)$$

The first step in applying this method is often to rearrange the problem of interest into a [fixed-point equation](@entry_id:203270) $x = g(x)$. However, this rearrangement is not always unique, and the choice of $g(x)$ is critical for the success of the method.

For instance, consider the problem of finding a root for the equation $f(x) = x^3 - x - 1 = 0$, which is known to have a unique root in the interval $[1, 2]$. One could rearrange this equation in at least two ways :

(i) $x = x^3 - 1$, which suggests an iteration function $g_1(x) = x^3 - 1$.

(ii) $x = (x + 1)^{1/3}$, suggesting an iteration function $g_2(x) = (x + 1)^{1/3}$.

As we will see, while both functions share the same fixed point, the iterative scheme based on $g_1(x)$ will diverge, whereas the one based on $g_2(x)$ will converge. This observation motivates the central question of fixed-point theory: under what conditions is the sequence generated by $x_{n+1} = g(x_n)$ guaranteed to converge?

### The Condition for Convergence

The answer to this question is provided by the **Banach Fixed-Point Theorem**, also known as the **Contraction Mapping Theorem**. This theorem provides a set of [sufficient conditions](@entry_id:269617) for the [existence and uniqueness](@entry_id:263101) of a fixed point and the convergence of the iteration. For a function of a single real variable, the theorem states:

Let $I$ be a closed interval of the real line, and let $g(x)$ be a function such that:
1.  **Self-mapping:** $g$ maps the interval $I$ into itself; that is, for every $x \in I$, the value $g(x)$ is also in $I$. We write this as $g(I) \subseteq I$.
2.  **Contraction:** $g$ is a **contraction mapping** on $I$. This means there exists a constant $k$ with $0 \le k \lt 1$ such that for any two points $x, y \in I$, the inequality $|g(x) - g(y)| \le k |x - y|$ holds. The constant $k$ is called the contraction factor.

If both conditions are met, then $g(x)$ has a unique fixed point $x^*$ in the interval $I$, and the iteration $x_{n+1} = g(x_n)$ will converge to $x^*$ for any initial guess $x_0 \in I$.

For differentiable functions, the contraction condition can be verified more easily. The Mean Value Theorem states that for any $x, y \in I$, there exists a point $c$ between them such that $g(x) - g(y) = g'(c)(x-y)$. This implies $|g(x) - g(y)| = |g'(c)||x-y|$. Therefore, if we can find a constant $k  1$ such that $|g'(x)| \le k$ for all $x \in I$, then $g(x)$ is a contraction on $I$. This provides a practical criterion:

**A [sufficient condition](@entry_id:276242) for convergence on an interval $I$ is that for some constant $k  1$, $|g'(x)| \le k$ for all $x \in I$, and $g(I) \subseteq I$.**

Let's revisit the two schemes from problem  on the interval $I=[1, 2]$:
- For $g_1(x) = x^3 - 1$, the derivative is $g_1'(x) = 3x^2$. On the interval $[1, 2]$, we have $|g_1'(x)| \ge 3(1)^2 = 3$. Since this value is much greater than 1, the iteration is not a contraction and is not guaranteed to converge; in fact, it will diverge.
- For $g_2(x) = (x + 1)^{1/3}$, the derivative is $g_2'(x) = \frac{1}{3(x+1)^{2/3}}$. On $[1, 2]$, the derivative is positive and its maximum value occurs at $x=1$, which is $|g_2'(1)| = \frac{1}{3\sqrt[3]{4}} \approx \frac{1}{3 \times 1.587} \approx 0.21 \lt 1$. Furthermore, $g_2(x)$ is an increasing function, and since $g_2(1) = \sqrt[3]{2} \approx 1.26$ and $g_2(2) = \sqrt[3]{3} \approx 1.44$, both of which are in $[1, 2]$, the self-mapping condition $g_2(I) \subseteq I$ is also satisfied. Thus, the iteration based on $g_2(x)$ is guaranteed to converge.

A classic example that demonstrates these principles is the solution of the [transcendental equation](@entry_id:276279) $x = \cos(x)$ . Here, the iteration function is naturally $g(x) = \cos(x)$. Let's analyze this on the interval $I = [-1, 1]$.
1.  **Self-mapping:** For any real number $x$, the value of $\cos(x)$ is always in $[-1, 1]$. Thus, $g(I) \subseteq I$ is satisfied.
2.  **Contraction:** The derivative is $g'(x) = -\sin(x)$. On the interval $I = [-1, 1]$, the maximum value of the absolute derivative is $|g'(\pm 1)| = \sin(1) \approx 0.841$. Since $k = \sin(1) \lt 1$, the function is a contraction on $I$.

The conditions of the theorem are met, guaranteeing a unique solution in $[-1, 1]$ and convergence for any starting point $x_0 \in [-1, 1]$. In this particular case, we can even make a stronger statement: for any initial guess $x_0 \in \mathbb{R}$, the first iterate $x_1 = \cos(x_0)$ will always lie in the interval $[-1, 1]$. From that point onwards, all subsequent iterates remain in this interval, and the sequence is guaranteed to converge to the unique fixed point.

### The Geometry and Behavior of Convergence

The iterative process $x_{n+1} = g(x_n)$ has a compelling geometric interpretation . Imagine plotting the function $y = g(x)$ and the line $y = x$ on the same axes. The fixed point $x^*$ is their intersection. The iteration can be visualized as a journey:
1.  Start at $(x_n, x_n)$ on the line $y=x$.
2.  Move vertically to the curve $y = g(x)$, arriving at the point $(x_n, g(x_n))$.
3.  Since $x_{n+1} = g(x_n)$, this point is also $(x_n, x_{n+1})$.
4.  Move horizontally from this point to the line $y=x$, arriving at $(x_{n+1}, x_{n+1})$.
5.  Repeat the process.

The condition $|g'(x^*)| \lt 1$ means that near the fixed point, the slope of the curve $y=g(x)$ is less steep than the slope of the line $y=x$ (which is 1). This causes the path to spiral or staircase inwards towards the intersection point. Conversely, if $|g'(x^*)| \gt 1$, the curve is steeper than the line, and the path spirals or staircases outwards, away from the fixed point.

The behavior of the convergence is further refined by the sign of the derivative at the fixed point:
- If **$0 \lt g'(x^*) \lt 1$**, the errors $e_n = x_n - x^*$ maintain the same sign from one iteration to the next. This leads to **monotonic convergence**, where the iterates approach $x^*$ from one side, creating a "staircase" pattern in the geometric visualization.
- If **$-1 \lt g'(x^*) \lt 0$**, the error term $e_{n+1} \approx g'(x^*) e_n$ flips its sign in each iteration. This results in **oscillatory convergence**, where the iterates alternate between being above and below the fixed point, creating a "cobweb" or "spiral" pattern. Certain applications, such as analyzing [nonlinear feedback](@entry_id:180335) systems, may specifically seek iteration schemes that produce this type of behavior .

The value $|g'(x^*)|$ is known as the **asymptotic convergence factor**. For an iteration converging to $x^*$, the error $e_n = x_n - x^*$ approximately satisfies the relation $e_{n+1} \approx g'(x^*) e_n$. This shows that the convergence is **linear**, meaning the error is reduced by a roughly constant factor at each step. For example, in a simplified population model described by $P_{n+1} = \sqrt{2P_n + 1}$, the non-zero equilibrium is the fixed point $P^* = 1 + \sqrt{2}$. The iteration function is $g(P) = \sqrt{2P+1}$, with derivative $g'(P) = (2P+1)^{-1/2}$. The asymptotic convergence factor is $|g'(P^*)| = (2(1+\sqrt{2})+1)^{-1/2} = (3+2\sqrt{2})^{-1/2} = \sqrt{2}-1 \approx 0.414$. Since this value is positive and less than 1, the iteration will converge monotonically to the equilibrium population density .

### Advanced Topics and Edge Cases

While the condition $|g'(x^*)| \lt 1$ is a powerful tool, it is important to understand its limitations and what can happen in more complex scenarios.

#### The Boundary Case: $|g'(x^*)| = 1$

When $|g'(x^*)| = 1$, the linear analysis is inconclusive. Convergence may or may not occur, and if it does, it is typically very slow. A more detailed analysis of the error is required. Consider the iteration $x_{n+1} = g(x_n)$ with $g(x) = x^2 - x + 1$. The unique fixed point is $x^*=1$, and at this point, $g'(x) = 2x-1$, so $g'(1)=1$. To analyze the behavior, we examine the error $e_n = x_n - 1$:
$$x_{n+1} = (e_n+1)^2 - (e_n+1) + 1 = (e_n^2 + 2e_n + 1) - e_n = e_n^2 + e_n + 1$$
$$e_{n+1} = x_{n+1} - 1 = e_n^2 + e_n = e_n(1+e_n)$$
From this relation, one can show that if the initial guess $x_0$ is in the interval $[0, 1]$, the sequence will converge to 1. However, for any $x_0  1$, the error grows with each step, and the sequence diverges. This demonstrates that convergence can be highly sensitive and one-sided in such boundary cases .

#### Periodic Orbits

An iterative sequence does not have to either converge to a fixed point or diverge to infinity. It can also settle into a **[periodic orbit](@entry_id:273755)** or **cycle**. A prominent example in [economic modeling](@entry_id:144051) is a **2-cycle**, which can arise in cobweb models of price adjustment. A 2-cycle consists of two points, $x^*$ and $y^*$, such that $g(x^*) = y^*$ and $g(y^*) = x^*$, with $x^* \neq y^*$.

If one starts an iteration $x_{n+1} = g(x_n)$ at $x_0 = x^*$, the sequence will be $\{x^*, y^*, x^*, y^*, \dots\}$, which clearly does not converge to a single value . The stability of this cycle is determined by analyzing the fixed points of the second-iterate map, $h(x) = g(g(x))$. Note that both $x^*$ and $y^*$ are fixed points for $h$. By the [chain rule](@entry_id:147422), the derivative of $h$ at $x^*$ is $h'(x^*) = g'(g(x^*))g'(x^*) = g'(y^*)g'(x^*)$. The 2-cycle is stable (attracting) if $|h'(x^*)| = |g'(x^*)g'(y^*)| \lt 1$. If this condition holds, an iteration starting near the cycle will not converge to a single point but will have its even and odd terms converge to the two different points of the cycle.

### Applications in Multi-Equilibrium and Multi-Dimensional Systems

The principles of fixed-point iteration extend naturally to more complex systems common in economics and finance.

#### Basins of Attraction

Many economic models feature multiple equilibria. For example, the equilibrium prices in an asset market might be the roots of an [excess demand](@entry_id:136831) function $z(p)=0$. A fixed-point iteration can be formulated to find these roots, for instance, via a price adjustment rule like $p_{t+1} = g(p_t) = p_t - \alpha z(p_t)$. For such a system, each [equilibrium point](@entry_id:272705) $p^*$ is a fixed point. Whether it is stable (attracting) or unstable (repelling) depends on $|g'(p^*)| = |1 - \alpha z'(p^*)|$.

A different choice of the parameter $\alpha$ or a different adjustment rule can change which equilibria are stable. The set of all initial prices $p_0$ for which the iteration converges to a specific equilibrium $p^*$ is called its **[basin of attraction](@entry_id:142980)**. A stable fixed point, by definition, has a non-empty [basin of attraction](@entry_id:142980) containing an [open interval](@entry_id:144029) around it . A particularly effective choice is Newton's method, which corresponds to the iteration $g_N(p) = p - z(p)/z'(p)$. For this method, it can be shown that $g_N'(p^*) = 0$ for any [simple root](@entry_id:635422) $p^*$. This implies not only that all [simple roots](@entry_id:197415) are attracting fixed points but also that the convergence is **quadratic** (the error $e_{n+1}$ is proportional to $e_n^2$), which is significantly faster than the [linear convergence](@entry_id:163614) seen previously.

#### Multi-dimensional Systems

In most economic models, the state is described by a vector of variables, such as prices and quantities across multiple markets. The fixed-point iteration takes the vector form $z_{k+1} = T(z_k)$, where $z$ is a vector in $\mathbb{R}^m$ and $T$ is a vector-valued function. For a linear or affine iteration, $T(z) = A z + \alpha$, where $A$ is an $m \times m$ matrix and $\alpha$ is a vector.

The contraction condition generalizes: the iteration is guaranteed to converge if $T$ is a contraction in some [vector norm](@entry_id:143228). This is equivalent to requiring that the **[induced matrix norm](@entry_id:145756)** $\|A\|$ is less than 1. The value of an [induced matrix norm](@entry_id:145756) can depend on the underlying [vector norm](@entry_id:143228) used (e.g., $\| \cdot \|_1$, $\| \cdot \|_2$, or $\| \cdot \|_\infty$). For example, the $\| \cdot \|_1$ norm of a matrix is its maximum absolute column sum, while the $\| \cdot \|_\infty$ norm is its maximum absolute row sum. A matrix might be a contraction in one norm but not another .

A more fundamental condition for the convergence of a linear iteration $z_{k+1} = A z_k + \alpha$ exists. The iteration converges for any starting vector $z_0$ if and only if the **spectral radius** of the matrix $A$, denoted $\rho(A)$, is less than 1. The spectral radius is the largest absolute value of the eigenvalues of $A$. While it is always true that $\rho(A) \le \|A\|$ for any [induced matrix norm](@entry_id:145756), it is possible to have $\rho(A)  1$ even when $\|A\| \ge 1$ for some common norms. In such cases, the iteration still converges, even though the contraction mapping theorem cannot be directly applied with that specific norm. The [spectral radius](@entry_id:138984) provides the definitive criterion for the stability of linear dynamic systems.