{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an algorithm, there is no substitute for performing the calculations by hand. This first exercise guides you through two full iterations of the steepest descent method on a classic quadratic function. By calculating the gradients and performing an exact line search to find the optimal step size, $\\alpha_k$, you will gain a concrete intuition for the iterative process and observe firsthand how the algorithm navigates the contours of the objective function.",
            "id": "2221576",
            "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x, y) = 10x^2 + y^2$. The optimization process is initiated at the point $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$.\n\nThe steepest descent algorithm is to be used. For each iteration $k$, the step size, denoted by $\\alpha_k > 0$, is determined via an exact line search. This means that for a given point $\\mathbf{x}_k$ and descent direction $\\mathbf{p}_k$, the step size $\\alpha_k$ is chosen to find the global minimum of the single-variable function $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$.\n\nCalculate the coordinates of the point $\\mathbf{x}_2 = (x_2, y_2)$ after two complete iterations of this method. The coordinates in your final answer should be expressed as exact fractions in their simplest form.",
            "solution": "We minimize $f(x,y)=10x^{2}+y^{2}$ starting at $\\mathbf{x}_{0}=(1,1)$ using steepest descent with exact line search. The gradient and Hessian are\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}.\n$$\nAt iteration $k$, with $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$ and direction $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$, the exact line search minimizes $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$. Since $f$ is a quadratic function, the derivative of $g(\\alpha)$ is\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\nSetting $g'(\\alpha)=0$ yields the exact step size\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\nIteration $0$: $\\mathbf{x}_{0}=(1,1)$ gives\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0}=404,\\quad H\\mathbf{g}_{0}=\\begin{pmatrix}400\\\\4\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0}=8008,\n$$\nso\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\nThen\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1-20\\alpha_{0}\\\\1-2\\alpha_{0}\\end{pmatrix}=\\begin{pmatrix}1-\\frac{2020}{2002}\\\\1-\\frac{202}{2002}\\end{pmatrix}=\\begin{pmatrix}-\\frac{18}{2002}\\\\\\frac{1800}{2002}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\nIteration $1$: $\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$ gives\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\nCompute\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}=\\frac{(-180)^{2}+1800^{2}}{1001^{2}}=\\frac{3,272,400}{1001^{2}},\\qquad\nH\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{3600}{1001}\\\\\\frac{3600}{1001}\\end{pmatrix},\n$$\nand\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}=\\frac{(-180)(-3600) + (1800)(3600)}{1001^2} = \\frac{648,000 + 6,480,000}{1001^2} = \\frac{7,128,000}{1001^2}.\n$$\nThus\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}}=\\frac{3,272,400}{7,128,000}=\\frac{101}{220}.\n$$\nUpdate\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}-\\frac{101}{220}\\left(-\\frac{180}{1001}\\right)\\\\[4pt]\\frac{900}{1001}-\\frac{101}{220}\\left(\\frac{1800}{1001}\\right)\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}+\\frac{909}{11011}\\\\[4pt]\\frac{900}{1001}-\\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\\[4pt]\\frac{810}{11011}\\end{pmatrix}.\n$$\nThe fractions are already in simplest terms because $\\gcd(810,11011)=1$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} \\\\ \\frac{810}{11011} \\end{pmatrix}}$$"
        },
        {
            "introduction": "The zigzagging pattern that emerges from the calculations in the previous exercise () is a crucial phenomenon to understand. This practice shifts our focus from mechanical calculation to geometric insight, exploring why steepest descent can be slow. By analyzing the problem in the context of an Ordinary Least Squares (OLS) regression with nearly collinear variables—a common issue in econometrics—you will connect the algorithm's performance to the shape of the objective function's level sets, which are governed by the eigenvalues of the Hessian matrix.",
            "id": "2434016",
            "problem": "Consider the ordinary least squares (OLS) objective for a linear regression with $2$ regressors,\n$$\nf(\\beta) \\;=\\; \\frac{1}{2n}\\,\\lVert y - X\\beta\\rVert_2^2,\n$$\nwhere $X\\in\\mathbb{R}^{n\\times 2}$ has columns $x_1$ and $x_2$, and $y\\in\\mathbb{R}^n$. Suppose the regressors are standardized so that $x_1^\\top x_1 = x_2^\\top x_2 = n$ and $x_1^\\top x_2 = \\rho n$ with $\\rho\\in(0,1)$ close to $1$, i.e., $x_1$ and $x_2$ are nearly collinear. Let $\\beta^\\star$ denote the unique minimizer of $f(\\beta)$ and consider steepest descent (gradient descent) with exact line search applied to $f(\\beta)$, starting from some $\\beta^{(0)}\\neq \\beta^\\star$.\n\nWhich of the following statements about the geometry of the level sets of $f(\\beta)$ and the resulting path of steepest descent is/are correct in this nearly collinear setting?\n\nA. The level sets of $f(\\beta)$ are elongated ellipses whose long axis is aligned with the eigenvector associated with the smaller eigenvalue of $(1/n)X^\\top X$; steepest descent alternates directions across the valley, making slow progress along this long axis.\n\nB. For this strictly convex quadratic objective with exact line search, successive gradients are orthogonal; when $\\rho$ is close to $1$, this orthogonality produces a pronounced zig-zag across a narrow valley.\n\nC. Because the objective is nearly flat along the collinearity direction, exact line search chooses an unbounded step size in that direction, so steepest descent overshoots and diverges.\n\nD. The negative gradient direction at any iterate equals $-(\\beta-\\beta^\\star)$, so the path is a straight line pointing directly at the minimizer.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- Objective function: $f(\\beta) = \\frac{1}{2n}\\,\\lVert y - X\\beta\\rVert_2^2$.\n- Data matrix: $X \\in \\mathbb{R}^{n\\times 2}$ with columns $x_1$ and $x_2$.\n- Response vector: $y \\in \\mathbb{R}^n$.\n- Parameter vector: $\\beta \\in \\mathbb{R}^2$.\n- Regressor properties: $x_1^\\top x_1 = n$, $x_2^\\top x_2 = n$, and $x_1^\\top x_2 = \\rho n$.\n- Collinearity parameter: $\\rho \\in (0,1)$ and $\\rho$ is close to $1$.\n- Minimizer: $\\beta^\\star$ is the unique minimizer of $f(\\beta)$.\n- Algorithm: Steepest descent (gradient descent) with exact line search.\n- Initial point: $\\beta^{(0)} \\neq \\beta^\\star$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in numerical optimization, specifically the application of the steepest descent method to the ordinary least squares (OLS) problem. All components are well-defined and standard in the fields of statistics and optimization.\n\n1.  **Scientific Grounding**: The problem is grounded in the established theories of linear algebra, multivariate calculus, and numerical optimization. The OLS objective and gradient descent are fundamental concepts.\n2.  **Well-Posedness**: The objective function can be analyzed to determine its properties. The gradient of $f(\\beta)$ is $\\nabla f(\\beta) = \\frac{1}{n}(X^\\top X \\beta - X^\\top y)$. The Hessian matrix is $\\nabla^2 f(\\beta) = H = \\frac{1}{n}X^\\top X$. Using the given properties, we compute $X^\\top X$:\n    $$\n    X^\\top X = \\begin{pmatrix} x_1^\\top x_1 & x_1^\\top x_2 \\\\ x_2^\\top x_1 & x_2^\\top x_2 \\end{pmatrix} = \\begin{pmatrix} n & \\rho n \\\\ \\rho n & n \\end{pmatrix} = n \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n    $$\n    Thus, the Hessian is a constant matrix:\n    $$\n    H = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}\n    $$\n    The eigenvalues $\\lambda$ of $H$ are given by the characteristic equation $\\det(H - \\lambda I) = (1-\\lambda)^2 - \\rho^2 = 0$, which yields $\\lambda = 1 \\pm \\rho$. Since $\\rho \\in (0,1)$, the eigenvalues are $\\lambda_1 = 1-\\rho > 0$ and $\\lambda_2 = 1+\\rho > 0$. As both eigenvalues are positive, the Hessian is positive definite. This confirms that $f(\\beta)$ is a strictly convex quadratic function, which guarantees the existence of a unique minimizer $\\beta^\\star$, as stated in the problem. The problem is well-posed.\n3.  **Objectivity**: The problem is stated in precise mathematical language, free from ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a well-posed, scientifically sound problem. A full solution will be derived.\n\n### Derivation\nThe objective function can be written in terms of its minimizer $\\beta^\\star$ and Hessian $H$:\n$$\nf(\\beta) = f(\\beta^\\star) + \\frac{1}{2}(\\beta - \\beta^\\star)^\\top H (\\beta - \\beta^\\star)\n$$\nA level set of $f$ is a set of points $\\{\\beta \\in \\mathbb{R}^2 \\mid f(\\beta) = c\\}$ for some constant $c > f(\\beta^\\star)$. This corresponds to the equation of an ellipse centered at $\\beta^\\star$:\n$$\n(\\beta - \\beta^\\star)^\\top H (\\beta - \\beta^\\star) = 2(c - f(\\beta^\\star))\n$$\nThe geometry of these elliptical level sets is determined by the eigensystem of the Hessian $H = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$.\nThe eigenvalues are $\\lambda_1 = 1-\\rho$ and $\\lambda_2 = 1+\\rho$.\nThe condition number of the Hessian is $\\kappa(H) = \\frac{\\lambda_{\\text{max}}}{\\lambda_{\\text{min}}} = \\frac{\\lambda_2}{\\lambda_1} = \\frac{1+\\rho}{1-\\rho}$. Since $\\rho$ is close to $1$, let $\\rho = 1-\\epsilon$ for a small $\\epsilon>0$. Then $\\kappa(H) = \\frac{2-\\epsilon}{\\epsilon} \\gg 1$. A large condition number signifies that the level sets are highly elongated ellipses.\n\nThe axes of the ellipses are determined by the eigenvectors of $H$.\n- For the smaller eigenvalue $\\lambda_1 = 1-\\rho$, the eigenvector $v_1$ satisfies $(H - \\lambda_1 I)v_1 = 0$:\n  $$\n  \\begin{pmatrix} \\rho & \\rho \\\\ \\rho & \\rho \\end{pmatrix} v_1 = 0 \\implies v_1 \\propto \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n  $$\n- For the larger eigenvalue $\\lambda_2 = 1+\\rho$, the eigenvector $v_2$ satisfies $(H - \\lambda_2 I)v_2 = 0$:\n  $$\n  \\begin{pmatrix} -\\rho & \\rho \\\\ \\rho & -\\rho \\end{pmatrix} v_2 = 0 \\implies v_2 \\propto \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n  $$\nThe length of a semi-axis of the ellipse is inversely proportional to the square root of the corresponding eigenvalue. The long axis corresponds to the smaller eigenvalue $\\lambda_1$ and is aligned with the eigenvector $v_1$. The short axis corresponds to the larger eigenvalue $\\lambda_2$ and is aligned with $v_2$. The highly elongated ellipses form a long, narrow \"valley\" or \"canyon\" oriented in the direction $v_1 = [1, -1]^\\top$.\n\nThe steepest descent algorithm performs the update $\\beta^{(k+1)} = \\beta^{(k)} - \\alpha_k \\nabla f(\\beta^{(k)})$. The direction is the negative gradient, $d_k = -\\nabla f(\\beta^{(k)})$, which is orthogonal to the level set at $\\beta^{(k)}$. With exact line search, the step size $\\alpha_k$ is chosen to minimize $f(\\beta^{(k)} + \\alpha d_k)$. This implies that the gradient at the new point, $\\nabla f(\\beta^{(k+1)})$, is orthogonal to the search direction $d_k$.\n$$\n\\nabla f(\\beta^{(k+1)})^\\top d_k = 0 \\implies \\nabla f(\\beta^{(k+1)})^\\top (-\\nabla f(\\beta^{(k)})) = 0\n$$\nThis demonstrates that successive gradients, $\\nabla f(\\beta^{(k)})$ and $\\nabla f(\\beta^{(k+1)})$, are orthogonal.\n\nWhen this algorithm is applied to a function with elongated level sets, the gradient vector at a point $\\beta^{(k)}$ on the side of the \"valley\" points nearly perpendicularly to the valley floor (the long axis). The exact line search moves the iterate to the other side of the valley. The new gradient is orthogonal to the previous one and again points steeply across the valley. This creates a characteristic zig-zagging trajectory, where the algorithm makes slow progress along the bottom of the valley towards the minimum $\\beta^\\star$.\n\n### Option-by-Option Analysis\n\n**A. The level sets of $f(\\beta)$ are elongated ellipses whose long axis is aligned with the eigenvector associated with the smaller eigenvalue of $(1/n)X^\\top X$; steepest descent alternates directions across the valley, making slow progress along this long axis.**\nThis statement is entirely consistent with our derivation.\n- The level sets are indeed ellipses, and since the condition number $\\kappa(H) = \\frac{1+\\rho}{1-\\rho}$ is large, they are elongated.\n- The Hessian is $H = (1/n)X^\\top X$. The long axis of an elliptical level set corresponds to the direction of minimum curvature, which is the eigenvector associated with the smallest eigenvalue. This is correctly stated.\n- The description of the steepest descent path as \"alternating directions across the valley\" (zig-zagging) and making \"slow progress along this long axis\" is the classical behavior of the algorithm on ill-conditioned problems.\nVerdict: **Correct**.\n\n**B. For this strictly convex quadratic objective with exact line search, successive gradients are orthogonal; when $\\rho$ is close to $1$, this orthogonality produces a pronounced zig-zag across a narrow valley.**\nThis statement is also correct.\n- As shown, the objective is a strictly convex quadratic.\n- The property that successive gradients are orthogonal for steepest descent with exact line search is a fundamental result.\n- The condition $\\rho$ close to $1$ implies an ill-conditioned Hessian, which corresponds to a \"narrow valley\" geometry. The orthogonality of gradients is precisely the mechanism that forces the \"zig-zag\" path in such a geometry.\nVerdict: **Correct**.\n\n**C. Because the objective is nearly flat along the collinearity direction, exact line search chooses an unbounded step size in that direction, so steepest descent overshoots and diverges.**\nThis statement is incorrect. While the objective is \"nearly flat\" (has small curvature) along the $v_1 = [1, -1]^\\top$ direction, it is still a strictly convex quadratic. Along any line, the function is a parabola that opens upwards. The minimizer along this line is unique and finite. Therefore, the exact line search will always choose a finite step size $\\alpha_k > 0$. The algorithm is guaranteed to converge to the unique minimizer $\\beta^\\star$ and will not diverge.\nVerdict: **Incorrect**.\n\n**D. The negative gradient direction at any iterate equals $-(\\beta-\\beta^\\star)$, so the path is a straight line pointing directly at the minimizer.**\nThis statement is incorrect. The gradient is $\\nabla f(\\beta) = H(\\beta - \\beta^\\star)$. The negative gradient direction is $-H(\\beta - \\beta^\\star)$. This direction is equal to $-(\\beta-\\beta^\\star)$ (or proportional to $\\beta^\\star - \\beta$) only if $H$ is a scalar multiple of the identity matrix. In our case, $H = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}$ with $\\rho \\neq 0$, so $H$ is not a multiple of the identity matrix. The gradient does not, in general, point towards the minimizer. Consequently, the path is not a straight line. (A straight-line path would be characteristic of Newton's method on a quadratic, not steepest descent).\nVerdict: **Incorrect**.\n\nBoth statements A and B are correct descriptions of the system. Statement A focuses on the geometry and the resulting path, while Statement B explains the algorithmic property causing the path's shape. The question asks for all correct statements.",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Moving from manual computation to practical implementation is the final step in mastering the steepest descent method. This exercise challenges you to write a program that solves a utility maximization problem using the algorithm. You will implement a backtracking line search, a more practical approach than the idealized exact line search, and incorporate robust termination criteria. This practice bridges the gap between theory and application, equipping you to use steepest descent as a computational tool for economic and financial modeling.",
            "id": "2434090",
            "problem": "Consider the unconstrained utility maximization problem in two variables given by the quadratic utility function\n$$u(x) = b^{\\top} x - \\tfrac{1}{2} x^{\\top} Q x,$$\nwhere $x \\in \\mathbb{R}^{2}$, $b \\in \\mathbb{R}^{2}$, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric and positive definite. This problem can be equivalently expressed as the unconstrained minimization problem of the objective\n$$f(x) = -u(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x.$$\nLet the iteration be defined by $x^{(k+1)} = x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)$, where $\\nabla f(x) = Qx - b$. At each iteration, the step size $\\alpha_{k}$ must satisfy the sufficient decrease condition\n$$f\\left(x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) + c \\, \\alpha_{k} \\, \\nabla f\\left(x^{(k)}\\right)^{\\top}\\left(-\\nabla f\\left(x^{(k)}\\right)\\right),$$\nwith $\\alpha_{k}$ chosen from the geometric sequence $\\{\\alpha_{0} \\rho^{m} : m \\in \\mathbb{N} \\cup \\{0\\}\\}$ for fixed constants $\\alpha_{0} \\in \\mathbb{R}_{++}$, $\\rho \\in (0,1)$, and $c \\in (0,1)$. Use the Euclidean norm $\\|\\cdot\\|_{2}$ for any norm computation. The algorithm must terminate when $\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$ or when the iteration count reaches a maximum $N_{\\max}$.\n\nFor each test case, compute the terminal iterate $x^{(T)}$ produced by the described procedure, and also compute the unique maximizer $x^{\\star}$ of $u(x)$, which is equivalently the unique minimizer of $f(x)$ and satisfies $Q x^{\\star} = b$. For each test case, report the scalar Euclidean error $\\|x^{(T)} - x^{\\star}\\|_{2}$.\n\nUse the following fixed parameter values for all test cases: initial step parameter $\\alpha_{0} = 1$, backtracking shrinkage factor $\\rho = 0.5$, sufficient decrease constant $c = 10^{-4}$, tolerance $\\varepsilon = 10^{-8}$, and maximum iterations $N_{\\max} = 10000$. All matrices and vectors are specified exactly below.\n\nTest suite:\n- Test case $1$: $Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Test case $2$: $Q = \\begin{bmatrix} 100 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 100 \\\\ 1 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$.\n- Test case $3$: $Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} \\tfrac{2}{5} \\\\ -\\tfrac{1}{5} \\end{bmatrix}$.\n- Test case $4$: $Q = \\begin{bmatrix} 4 & 1.5 \\\\ 1.5 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, namely $[\\|x^{(T)}_{1} - x^{\\star}_{1}\\|_{2}, \\|x^{(T)}_{2} - x^{\\star}_{2}\\|_{2}, \\|x^{(T)}_{3} - x^{\\star}_{3}\\|_{2}, \\|x^{(T)}_{4} - x^{\\star}_{4}\\|_{2}]$. The results must be real numbers (floats). No physical units are involved in this problem.",
            "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization, specifically focusing on the steepest descent method applied to a quadratic objective function. All necessary parameters and data are provided, the terminology is precise, and there are no internal contradictions or logical flaws. The problem involves minimizing a strictly convex quadratic function, for which the steepest descent method with a backtracking line search is a standard and convergent algorithm. We will now proceed with the solution.\n\nThe problem is to minimize the quadratic objective function $f(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x$, where $x \\in \\mathbb{R}^{2}$, $b \\in \\mathbb{R}^{2}$, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric and positive definite matrix.\n\nFirst, we establish the existence and uniqueness of the solution. The Hessian of the objective function is $\\nabla^2 f(x) = Q$. Since $Q$ is given as positive definite for all test cases, the function $f(x)$ is strictly convex. A strictly convex function on $\\mathbb{R}^{n}$ has at most one minimizer. Since $f(x)$ is coercive (i.e., $f(x) \\to \\infty$ as $\\|x\\|_2 \\to \\infty$), a unique global minimizer, which we denote by $x^{\\star}$, is guaranteed to exist.\n\nThe first-order necessary condition for optimality states that the gradient of the objective function must be zero at the minimizer. The gradient of $f(x)$ is $\\nabla f(x) = Qx - b$. Setting the gradient to zero gives the optimality condition:\n$$\n\\nabla f(x^{\\star}) = Qx^{\\star} - b = 0\n$$\nThis is a system of linear equations $Qx^{\\star} = b$. Since $Q$ is positive definite, it is invertible. Therefore, the unique analytical solution is given by:\n$$\nx^{\\star} = Q^{-1} b\n$$\n\nThe numerical procedure to find the solution is the method of steepest descent. This is an iterative algorithm that generates a sequence of points $\\{x^{(k)}\\}_{k=0}^{\\infty}$ that converges to $x^{\\star}$. The iteration is defined as:\n$$\nx^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}\n$$\nwhere $p^{(k)}$ is the search direction and $\\alpha_k > 0$ is the step size. For the method of steepest descent, the search direction is chosen to be the negative of the gradient at the current iterate, as this is the direction of the most rapid decrease of the function locally.\n$$\np^{(k)} = -\\nabla f(x^{(k)}) = -(Qx^{(k)} - b) = b - Qx^{(k)}\n$$\nThe update rule is thus:\n$$\nx^{(k+1)} = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\n$$\n\nThe step size $\\alpha_k$ is determined using a backtracking line search procedure to satisfy the sufficient decrease condition, also known as the Armijo condition. This ensures that each step makes meaningful progress towards the minimum. The condition is:\n$$\nf(x^{(k+1)}) \\le f(x^{(k)}) + c \\, \\alpha_k \\, \\nabla f(x^{(k)})^{\\top} p^{(k)}\n$$\nSubstituting $p^{(k)} = -\\nabla f(x^{(k)})$, the condition becomes:\n$$\nf\\left(x^{(k)} - \\alpha_k \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) - c \\, \\alpha_k \\, \\|\\nabla f\\left(x^{(k)}\\right)\\|^{2}_{2}\n$$\nThe procedure for selecting $\\alpha_k$ at each iteration $k$ is as follows:\n$1$. Start with an initial step size $\\alpha = \\alpha_0 = 1$.\n$2$. While the Armijo condition is not satisfied, shrink the step size: $\\alpha \\leftarrow \\rho \\alpha$. Here, $\\rho = 0.5$.\n$3$. Once the condition is met, set $\\alpha_k = \\alpha$.\nThe parameters are given as $c = 10^{-4}$, $\\alpha_0 = 1$, and $\\rho = 0.5$.\n\nThe algorithm terminates when one of two conditions is met:\n$1$. The norm of the gradient is smaller than a specified tolerance $\\varepsilon = 10^{-8}$: $\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$. This indicates that the iterate is very close to the optimal solution $x^{\\star}$ where the gradient is zero.\n$2$. The number of iterations $k$ reaches the maximum allowed number, $N_{\\max} = 10000$.\n\nLet $x^{(T)}$ be the terminal iterate produced by the algorithm. The final required output for each test case is the Euclidean error $\\|x^{(T)} - x^{\\star}\\|_{2}$. For each given test case ($Q$, $b$, $x^{(0)}$), we will first compute the analytical solution $x^{\\star} = Q^{-1}b$ and then execute the described iterative algorithm to find $x^{(T)}$.\n\nFor test case $3$, we are given $Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, and $x^{(0)} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$. The analytical solution is $x^{\\star} = Q^{-1}b = \\frac{1}{5}\\begin{bmatrix} 2 & -1 \\\\ -1 & 3 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$. Thus, the initial point $x^{(0)}$ is the exact solution $x^{\\star}$. In this case, the gradient at the start is $\\nabla f(x^{(0)}) = Qx^{(0)} - b = 0$. The termination condition $\\|\\nabla f(x^{(0)})\\|_{2} = 0 \\le \\varepsilon$ is satisfied immediately at iteration $k=0$. The algorithm terminates, yielding $x^{(T)} = x^{(0)}$, and the error $\\|x^{(T)} - x^{\\star}\\|_{2}$ is $0$.\n\nThe implementation will now follow this logic for all test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained utility maximization problem for four test cases\n    using the steepest descent method with backtracking line search.\n    \"\"\"\n    # Fixed parameters for the algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    N_max = 10000\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"Q\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([2.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        {\n            \"Q\": np.array([[100.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([100.0, 1.0]),\n            \"x0\": np.array([10.0, -10.0]),\n        },\n        {\n            \"Q\": np.array([[3.0, 1.0], [1.0, 2.0]]),\n            \"b\": np.array([1.0, 0.0]),\n            \"x0\": np.array([2.0 / 5.0, -1.0 / 5.0]),\n        },\n        {\n            \"Q\": np.array([[4.0, 1.5], [1.5, 1.0]]),\n            \"b\": np.array([1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    def objective_function(x, Q, b):\n        \"\"\"Computes the value of the objective function f(x).\"\"\"\n        return 0.5 * x.T @ Q @ x - b.T @ x\n\n    for case in test_cases:\n        Q = case[\"Q\"]\n        b = case[\"b\"]\n        x_k = case[\"x0\"].copy()\n\n        # Compute the analytical solution x_star\n        x_star = np.linalg.solve(Q, b)\n\n        # Main loop for the steepest descent algorithm\n        for _ in range(N_max):\n            # Compute the gradient at the current iterate x_k\n            grad_f = Q @ x_k - b\n            grad_norm = np.linalg.norm(grad_f)\n\n            # Check for termination based on gradient norm\n            if grad_norm = epsilon:\n                break\n            \n            # Backtracking line search to find the step size alpha_k\n            alpha = alpha0\n            f_k = objective_function(x_k, Q, b)\n            grad_norm_sq = grad_norm**2 # More efficient than dot product\n\n            while True:\n                # Armijo condition check\n                f_new = objective_function(x_k - alpha * grad_f, Q, b)\n                if f_new = f_k - c * alpha * grad_norm_sq:\n                    break\n                \n                # Shrink alpha if condition is not met\n                alpha *= rho\n\n            # Update the iterate\n            x_k = x_k - alpha * grad_f\n        \n        # The loop terminates, x_k is the terminal iterate x_T\n        x_terminal = x_k\n\n        # Compute the final Euclidean error\n        error = np.linalg.norm(x_terminal - x_star)\n        results.append(error)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}