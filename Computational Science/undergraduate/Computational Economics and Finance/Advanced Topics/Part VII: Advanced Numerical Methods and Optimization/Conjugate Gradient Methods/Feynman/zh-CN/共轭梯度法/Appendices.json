{
    "hands_on_practices": [
        {
            "introduction": "要真正理解共轭梯度（CG）方法的威力，最好的方式莫过于将其与更基本的方法进行直接比较。第一个练习就是一个手算实践，旨在揭示最速下降法与共轭梯度法在性能上的根本差异。通过为一个简单的二维投资组合优化问题逐步计算，你将亲眼见证，共轭梯度法凭借其具备“记忆”的搜索方向，如何实现更高效的收敛。这个基础计算是建立对CG方法效率直观理解的关键一步。",
            "id": "2382887",
            "problem": "考虑一个包含两种风险资产的无约束均值-方差投资组合选择问题。目标是最小化二次函数\n$$f(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\Sigma \\, w \\;-\\; \\mu^{\\top} w,$$\n其中协方差矩阵为\n$$\\Sigma \\;=\\; \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix},$$\n期望收益向量为\n$$\\mu \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},$$\n初始投资组合为\n$$w_{0} \\;=\\; \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.$$\n\n在步骤 $k \\in \\{1,2\\}$，从相同的当前迭代点出发，比较两个搜索方向：最速下降方向 $d_{k}^{\\mathrm{SD}} \\,=\\, -\\nabla f(w_{k-1})$ 和共轭梯度 (CG) 方向 $d_{k}^{\\mathrm{CG}}$，该方向被构造为与前一个 CG 方向 $\\Sigma$-共轭（其中 $d_{1}^{\\mathrm{CG}} \\,=\\, -\\nabla f(w_{0})$ 并且 $d_{2}^{\\mathrm{CG}}$ 满足 $\\big(d_{2}^{\\mathrm{CG}}\\big)^{\\top} \\Sigma \\, d_{1}^{\\mathrm{CG}} \\,=\\, 0$）。对于每一步的每个方向，执行精确线搜索，该搜索使 $f$ 沿对应线最小化，并将步进后的结果点表示为 $w_{k}^{\\mathrm{SD}}$ 和 $w_{k}^{\\mathrm{CG}}$。将步骤 $k$ 的次优性定义为\n$$s_{k} \\;=\\; f\\!\\big(w_{k}^{\\mathrm{SD}}\\big) \\;-\\; f\\!\\big(w_{k}^{\\mathrm{CG}}\\big).$$\n\n精确计算 $s_{1}$ 和 $s_{2}$。以单个行向量 $\\big(s_{1} \\;\\; s_{2}\\big)$ 的形式报告您的最终答案，使用精确分数。不要包含单位，也不要四舍五入。",
            "solution": "问题陈述具有科学依据、良态且客观。它描述了一个标准的二次优化任务，为此指定的数值算法，即最速下降法 (SD) 和共轭梯度法 (CG)，都是良定义的。我们进行形式化的推导。\n\n要最小化的目标函数由下式给出\n$$f(w) = \\frac{1}{2} w^{\\top} \\Sigma w - \\mu^{\\top} w$$\n该函数的梯度是\n$$\\nabla f(w) = \\Sigma w - \\mu$$\n初始点为 $w_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。初始梯度为 $\\nabla f(w_0) = \\Sigma w_0 - \\mu = -\\mu = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n残差向量定义为 $r = -\\nabla f(w)$。初始残差为 $r_0 = -\\nabla f(w_0) = \\mu = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n从迭代点 $w_k$ 沿方向 $d_k$ 进行精确线搜索的最优步长 $\\alpha$ 由最小化 $f(w_k + \\alpha d_k)$ 的公式给出：\n$$\\alpha_k = -\\frac{\\nabla f(w_k)^{\\top} d_k}{d_k^{\\top} \\Sigma d_k}$$\n\n步骤 $k=1$：\n\n对于 SD 和 CG 两种方法，初始搜索方向都是负梯度：\n$$d_1^{\\mathrm{SD}} = d_1^{\\mathrm{CG}} = -\\nabla f(w_0) = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n两种方法的步长 $\\alpha_1$ 相同：\n$$\\alpha_1 = -\\frac{\\nabla f(w_0)^{\\top} d_1}{d_1^{\\top} \\Sigma d_1} = \\frac{r_0^{\\top} r_0}{r_0^{\\top} \\Sigma r_0}$$\n我们计算各项：\n$$r_0^{\\top} r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n$$r_0^{\\top} \\Sigma r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\n步长为 $\\alpha_1 = \\frac{1}{2}$。\n新的迭代点是相同的：\n$$w_1^{\\mathrm{SD}} = w_1^{\\mathrm{CG}} = w_0 + \\alpha_1 d_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n我们将这个共同的迭代点表示为 $w_1$。此时目标函数的值为：\n$$f(w_1) = \\frac{1}{2} w_1^{\\top} \\Sigma w_1 - \\mu^{\\top} w_1 = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n$$f(w_1) = \\frac{1}{2} \\begin{pmatrix} 1 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) - \\frac{1}{2} = \\frac{1}{4} - \\frac{1}{2} = -\\frac{1}{4}$$\n由于 $f(w_1^{\\mathrm{SD}}) = f(w_1^{\\mathrm{CG}}) = -\\frac{1}{4}$，步骤 1 的次优性为：\n$$s_1 = f(w_1^{\\mathrm{SD}}) - f(w_1^{\\mathrm{CG}}) = 0$$\n\n步骤 $k=2$：\n\n我们从共同的迭代点 $w_1 = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$ 开始。该点的梯度为：\n$$\\nabla f(w_1) = \\Sigma w_1 - \\mu = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\n新的残差为 $r_1 = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n\n对于 SD 方法，搜索方向为 $d_2^{\\mathrm{SD}} = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n步长为 $\\alpha_2^{\\mathrm{SD}} = \\frac{r_1^{\\top} r_1}{r_1^{\\top} \\Sigma r_1}$。\n分子：$r_1^{\\top} r_1 = (0)^2 + (-\\frac{1}{2})^2 = \\frac{1}{4}$。\n分母：$r_1^{\\top} \\Sigma r_1 = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} = \\frac{3}{4}$。\n$\\alpha_2^{\\mathrm{SD}} = \\frac{1/4}{3/4} = \\frac{1}{3}$。\nSD 迭代点为 $w_2^{\\mathrm{SD}} = w_1 + \\alpha_2^{\\mathrm{SD}} d_2^{\\mathrm{SD}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n函数值为 $f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n$f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1-\\frac{1}{6} \\\\ \\frac{1}{2}-\\frac{3}{6} \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2}\\left(\\frac{5}{12}\\right) - \\frac{1}{2} = -\\frac{7}{24}$。\n\n对于 CG 方法，搜索方向为 $d_2^{\\mathrm{CG}} = r_1 + \\beta_1 d_1^{\\mathrm{CG}}$，其中 $\\beta_1 = \\frac{r_1^{\\top}r_1}{r_0^{\\top}r_0}$。\n由于 $r_1^{\\top} r_1 = \\frac{1}{4}$ 且 $r_0^{\\top} r_0 = 1$，我们得到 $\\beta_1 = \\frac{1}{4}$。\n$$d_2^{\\mathrm{CG}} = r_1 + \\frac{1}{4}d_1^{\\mathrm{CG}} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix}$$\n步长为 $\\alpha_2^{\\mathrm{CG}} = -\\frac{\\nabla f(w_1)^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}} = \\frac{r_1^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}}$。\n分子：$r_1^{\\top} d_2^{\\mathrm{CG}} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\frac{1}{4}$。\n分母：$(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{5}{4} \\end{pmatrix} = \\frac{5}{8}$。\n$\\alpha_2^{\\mathrm{CG}} = \\frac{1/4}{5/8} = \\frac{2}{5}$。\nCG 迭代点为 $w_2^{\\mathrm{CG}} = w_1 + \\alpha_2^{\\mathrm{CG}} d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{2}{5} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}+\\frac{1}{10} \\\\ -\\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{1}{5} \\end{pmatrix}$。\n这正是精确的最小值点 $w^* = \\Sigma^{-1}\\mu$，正如在 $N=2$ 维空间中对 CG 方法所预期的那样。\n最小函数值为 $f(w_2^{\\mathrm{CG}}) = f(w^*) = -\\frac{1}{2}\\mu^\\top w^* = -\\frac{1}{2}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{pmatrix} 3/5 \\\\ -1/5 \\end{pmatrix} = -\\frac{3}{10}$。\n\n步骤 2 的次优性为：\n$$s_2 = f(w_2^{\\mathrm{SD}}) - f(w_2^{\\mathrm{CG}}) = -\\frac{7}{24} - \\left(-\\frac{3}{10}\\right) = -\\frac{7}{24} + \\frac{3}{10} = \\frac{-35 + 36}{120} = \\frac{1}{120}$$\n\n最终值为 $s_1 = 0$ 和 $s_2 = \\frac{1}{120}$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0 & \\frac{1}{120} \\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "这个练习将带你探索一个深刻的联系：问题的数学结构如何决定CG算法的性能。你将分析一个源于单因子资产模型的协方差矩阵，并证明其特殊的代数性质——即只有两个不同的特征值——确保了CG方法能在两步之内精确收敛 。这个实践将抽象的线性代数概念与具体的经济学解释联系起来，突显了CG方法在处理结构化问题时的强大能力。",
            "id": "2382876",
            "problem": "考虑一个四资产均值-方差框架，其中收益向量的协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{4 \\times 4}$，由下式给出\n$$\n\\Sigma \\;=\\; I_{4} \\;+\\; v v^{\\top}, \\quad v \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n设超额收益向量为 $b \\in \\mathbb{R}^{4}$，由下式给出\n$$\nb \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n你需要求解线性方程组\n$$\n\\Sigma x \\;=\\; b\n$$\n，需使用共轭梯度（CG）法，从 $x_{0} = 0_{4}$ 开始，并假设全程使用精确算术。\n\n任务：\n- 证明 $\\Sigma$ 恰好有两个不同的特征值，并且是对称正定矩阵。\n- 证明在该系统和精确算术下，共轭梯度法在两次迭代后精确终止，并计算满足 $\\Sigma x = b$ 的收敛解向量 $x$。\n- 简要解释 $\\Sigma$ 的特定形式对收益的联合分布所隐含的经济结构。\n\n仅报告收敛的解向量 $x$ 作为你的最终数值答案，写成单行矩阵的形式。无需四舍五入。",
            "solution": "首先将验证问题陈述的科学合理性和一致性。\n\n步骤 1：提取已知条件。\n- 协方差矩阵为 $\\Sigma = I_{4} + v v^{\\top}$，其中 $I_4$ 是 $4 \\times 4$ 的单位矩阵。\n- 向量 $v$ 由 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 给出。\n- 超额收益向量为 $b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}$。\n- 待求解的线性方程组为 $\\Sigma x = b$。\n- 求解方法为共轭梯度（CG）法。\n- 初始猜测为零向量，$x_{0} = 0_{4}$。\n- 计算假设使用精确算术。\n\n任务如下：\n1. 证明 $\\Sigma$ 恰好有两个不同的特征值，并且是对称正定（SPD）的。\n2. 证明 CG 方法在两次迭代后终止，并计算解向量 $x$。\n3. 解释 $\\Sigma$ 的形式所隐含的经济结构。\n\n步骤 2：使用提取的已知条件进行验证。\n该问题具有科学依据，是计算金融和数值线性代数中的一个标准问题。矩阵 $\\Sigma$ 是对单位矩阵的秩一更新，这是一种已得到充分理解的结构。共轭梯度法适用于矩阵为对称正定（SPD）的系统。为证实这一点，我们检查 $\\Sigma$ 的性质。\n\n对称性：$\\Sigma^{\\top} = (I_{4} + vv^{\\top})^{\\top} = I_{4}^{\\top} + (vv^{\\top})^{\\top} = I_{4} + (v^{\\top})^{\\top}v^{\\top} = I_{4} + vv^{\\top} = \\Sigma$。该矩阵是对称的。\n\n特征值：设 $u$ 是 $\\Sigma$ 的一个特征向量。特征值方程为 $(I_{4} + vv^{\\top})u = \\lambda u$，可重排为 $(vv^{\\top})u = (\\lambda - 1)u$。这表明 $u$ 也是秩一矩阵 $vv^{\\top}$ 的一个特征向量，其特征值为 $\\lambda - 1$。\n矩阵 $vv^{\\top}$ 最多有一个非零特征值。\n- 如果 $u$ 是 $v$ 的倍数，比如 $u=cv$（其中 $c$ 为某个非零标量），那么 $vv^{\\top}(cv) = v(v^{\\top}cv) = c(v^{\\top}v)v$。$vv^{\\top}$ 对应的特征值为 $\\lambda_{vv^{\\top}} = v^{\\top}v = 1^2 + 1^2 + 0^2 + 0^2 = 2$。\n- 如果 $u$ 与 $v$ 正交（$v^{\\top}u = 0$），那么 $vv^{\\top}u = v(v^{\\top}u) = v(0) = 0$。特征值为 $0$。在 $\\mathbb{R}^{4}$ 中与 $v$ 正交的向量空间维度为 $4-1=3$。\n因此，$vv^{\\top}$ 的特征值为 $2$（重数为 $1$）和 $0$（重数为 $3$）。\n$\\Sigma = I_4 + vv^{\\top}$ 的特征值由 $\\lambda_{\\Sigma} = 1 + \\lambda_{vv^{\\top}}$ 给出。因此，$\\Sigma$ 的特征值为 $1+2=3$（重数为 $1$）和 $1+0=1$（重数为 $3$）。\n由于 $\\Sigma$ 恰好有两个不同的特征值（$1$ 和 $3$），且两者均为正，因此对称矩阵 $\\Sigma$ 是正定的。\n\n该问题是适定的、完整的、一致的。这是一个有效的问题。\n\n步骤 3：结论与行动。\n问题有效。我现在将提供解答。\n\n按照要求，问题分三部分解答。\n\n第 1 部分：$\\Sigma$ 的特征值和对称正定（SPD）性质。\n如验证步骤所证，矩阵 $\\Sigma = I_4 + vv^{\\top}$ 是对称的。其特征值为 $3$ 和 $1$。由于所有特征值均为正，$\\Sigma$ 是对称正定的。这证实了共轭梯度法是适用的。\n\n第 2 部分：共轭梯度终止和求解。\n共轭梯度法保证在最多 $k$ 次迭代内找到 $\\Sigma x = b$ 的精确解（假设精确算术），其中 $k$ 是 $\\Sigma$ 的不同特征值的数量。由于 $\\Sigma$ 恰好有两个不同的特征值，该方法将在两次迭代后精确终止。\n\n要计算解 $x$，可以执行 CG 算法的两个步骤。然而，一个更有洞察力的方法是利用我们刚刚推导出的 $\\Sigma$ 的谱性质。这种方法更稳健，且不易出现算术错误。因为该方法在两步内收敛且 $x_0=0$，解 $x$ 必然位于 Krylov 子空间 $\\mathcal{K}_{2}(\\Sigma, r_0)$ 中。该子空间由 $\\{r_0, \\Sigma r_0\\}$ 张成，其中 $r_0=b-\\Sigma x_0=b$。\n\n精确解由 $x = \\Sigma^{-1}b$ 给出。我们可以通过将 $b$ 分解到 $\\Sigma$ 的特征空间中来计算。\n对应于 $\\lambda=3$ 的特征空间由向量 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 张成。\n对应于 $\\lambda=1$ 的特征空间是 $v$ 所张成空间的正交补，即满足 $v^{\\top}u=0$ 的向量 $u$ 的集合。\n\n我们将 $b$ 分解为一个平行于 $v$ 的分量 $b_{\\parallel}$ 和一个正交于 $v$ 的分量 $b_{\\perp}$。\n$b = b_{\\parallel} + b_{\\perp}$。\n平行分量是 $b$ 在 $v$ 上的投影：\n$$\nb_{\\parallel} = \\frac{b^{\\top}v}{v^{\\top}v} v = \\frac{\\begin{pmatrix} 2 & 1 & 5 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}}{\\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}} v = \\frac{2(1)+1(1)}{1^2+1^2} v = \\frac{3}{2} v = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n正交分量是 $b_{\\perp} = b - b_{\\parallel}$：\n$$\nb_{\\perp} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n我们可以验证 $v^{\\top}b_{\\perp} = 1(1/2) + 1(-1/2) + 0 + 0 = 0$，所以 $b_{\\perp}$ 确实在对应 $\\lambda=1$ 的特征空间中。\n\n现在我们将 $\\Sigma^{-1}$ 应用于 $b = b_{\\parallel} + b_{\\perp}$。由于 $b_{\\parallel}$ 是对应 $\\lambda=3$ 的特征向量，而 $b_{\\perp}$ 是对应 $\\lambda=1$ 的特征向量：\n$$\nx = \\Sigma^{-1}b = \\Sigma^{-1}(b_{\\parallel} + b_{\\perp}) = \\Sigma^{-1}b_{\\parallel} + \\Sigma^{-1}b_{\\perp}\n$$\n$$\nx = \\frac{1}{3} b_{\\parallel} + \\frac{1}{1} b_{\\perp}\n$$\n代入向量：\n$$\nx = \\frac{1}{3} \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2+1/2 \\\\ 1/2-1/2 \\\\ 0+5 \\\\ 0-4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n这就是精确解，CG 算法在两步内找到此解。\n\n我们来验证这个解：\n$$\n\\Sigma x = (I_4 + vv^{\\top})x = x + v(v^{\\top}x)\n$$\n$$\nv^{\\top}x = \\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} = 1(1) + 1(0) = 1.\n$$\n$$\n\\Sigma x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} 1+1 \\\\ 0+1 \\\\ 5+0 \\\\ -4+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} = b.\n$$\n解是正确的。\n\n第 3 部分：经济结构。\n资产收益的协方差矩阵 $\\Sigma = I_4 + vv^{\\top}$ 是单因子资产定价模型的典型特征。在此类模型中，资产 $i$ 的收益 $R_i$ 由其对共同市场因子 $F$ 的敞口和资产特定的异质性冲击 $\\epsilon_i$ 来描述：$R_i = \\beta_i F + \\epsilon_i$。收益向量 $R$ 的协方差矩阵是 $\\text{Cov}(R) = (\\beta\\beta^{\\top})\\sigma_F^2 + D$，其中 $\\beta$ 是因子载荷向量，$\\sigma_F^2$ 是因子的方差，而 $D$ 是异质性方差的对角矩阵。\n\n在本问题中，结构 $\\Sigma = vv^{\\top} + I_4$ 意味着：\n1.  存在一个方差为单位值（$\\sigma_F^2=1$）的单一系统性风险因子。\n2.  因子敞口（载荷）向量为 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$。这意味着只有前两个资产暴露于此共同风险因子，且载荷均为 $1$。资产 3 和 4 不暴露于此因子。\n3.  异质性风险矩阵是 $D = I_4$。这意味着每个资产都有一个独立的、特定的风险成分，其方差为 $1$。\n\n因此，资产 1 和 2 之间的非零协方差 $(\\Sigma)_{12} = 1$ 完全由它们对共同因子的共同敞口来解释。所有其他资产对都是不相关的，因为除非 $\\{i,j\\}=\\{1,2\\}$，否则对于 $i \\neq j$ 有 $(\\Sigma)_{ij} = 0$。资产 1 和 2 的总方差为 $(\\Sigma)_{11} = (\\Sigma)_{22} = 2$，由因子方差（$1^2 \\cdot 1 = 1$）和异质性方差（$1$）组成。资产 3 和 4 的总方差为 $(\\Sigma)_{33} = (\\Sigma)_{44} = 1$，由于它们的因子载荷为零，这纯粹是异质性方差。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 & 5 & -4 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理论告诉我们，共轭梯度法的效率核心在于其搜索方向序列满足 $A$-正交性。这个编程练习将让你通过实践来验证这一理论 。你需要实现CG算法，并在计算过程中故意扰动其中一个搜索方向。通过观察这个微小的改动如何破坏算法在有限步内收敛的保证，你将对CG方法背后精妙的数学原理获得更加具体和深刻的认识。",
            "id": "2382914",
            "problem": "给定一个源自惩罚均值-方差 Markowitz 投资组合模型的线性系统。考虑 $n$ 种资产，其收益向量为 $\\mu \\in \\mathbb{R}^n$，协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{n \\times n}$，该矩阵是对称正定 (SPD) 的。引入权重为 $\\eta > 0$ 的软预算惩罚，得到以下无约束二次目标函数\n$$\nf(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2,\n$$\n其中 $\\mathbf{1} \\in \\mathbb{R}^n$ 是全为 1 的向量。一阶条件 $\\nabla f(x) = 0$ 可简化为如下 SPD 线性系统\n$$\nA x = b,\\quad\\text{with}\\quad A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top},\\quad b \\equiv \\mu + \\eta\\, \\mathbf{1}.\n$$\n您的任务是实现共轭梯度 (CG) 法求解 $A x = b$，并展示扰动单个搜索方向如何破坏搜索方向的 $A$-正交性，并阻止其在至多 $n$ 步内精确收敛。\n\n使用以下为计算金融学中具有代表性的明确指定的实例：\n- 资产数量：$n = 6$。\n- 常数相关矩阵 $R \\in \\mathbb{R}^{6 \\times 6}$，相关系数为 $\\rho = 0.2$，定义为 $R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$。\n- 资产标准差 $s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]$；定义 $\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$。\n- 期望收益 $\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$。\n- 惩罚权重 $\\eta = 10^{-2}$。\n\n算法要求：\n- 实现从 $x_0 = 0$ 开始的共轭梯度 (CG) 法。您必须精确计算 $n$ 次迭代，不得提前终止，并使用在精确算术下能为 SPD 系统保持 $A$-共轭性的数学上合理的更新方式。\n- 实现一种机制来精确扰动一个搜索方向。具体来说，如果扰动迭代索引为 $k_{\\mathrm{perturb}} \\in \\{0,1,\\dots,n-1\\}$ 且标量扰动幅度为 $\\varepsilon \\geq 0$，则在迭代 $j = k_{\\mathrm{perturb}}$ 时，将当前搜索方向 $p_j$ 替换为\n$$\n\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1,\n$$\n其中 $e_1 = [1,0,\\dots,0]^{\\top} \\in \\mathbb{R}^n$ 是第一个标准基向量。如果 $k_{\\mathrm{perturb}} < 0$ 或 $k_{\\mathrm{perturb}} \\ge n$ 或 $\\varepsilon = 0$，则不应施加扰动。\n- 将所有 $n$ 个搜索方向按其在 CG 中使用的顺序列为矩阵 $P \\in \\mathbb{R}^{n \\times n}$ 的列。\n\n对于每次运行，在精确 $n$ 次迭代后，计算以下两项：\n1. $n$ 次迭代后的残差范数，定义为 $\\lVert b - A x_n \\rVert_2$。\n2. 搜索方向的 $A$-正交性缺陷，定义为格拉姆矩阵 $G \\equiv P^{\\top} A P$ 的非对角元素的最大绝对值，即\n$$\n\\max_{i \\ne j} \\left| G_{ij} \\right|.\n$$\n\n测试套件：\n对上述固定的 $(A,b)$，使用以下五组参数对 $(k_{\\mathrm{perturb}}, \\varepsilon)$ 运行您的实现：\n- 情况 1：$(-1, 0)$。\n- 情况 2：$(2, 1)$。\n- 情况 3：$(0, 1)$。\n- 情况 4：$(10, 1)$。\n- 情况 5：$(3, 10^{-3})$。\n\n最终输出格式：\n- 您的程序必须打印单行输出，其中包含一个扁平列表，每个测试用例包含 2 个数字，顺序如下：对于每个用例，首先是 $n$ 步后的残差范数，然后是 $A$-正交性缺陷。所有数字必须使用标准四舍五入精确到小数点后 $8$ 位。\n- 具体而言，输出必须是以下形式的单行：\n$$\n[\\text{res}_1,\\text{def}_1,\\text{res}_2,\\text{def}_2,\\dots,\\text{res}_5,\\text{def}_5],\n$$\n其中每个 $\\text{res}_i$ 和 $\\text{def}_i$ 都是小数点后恰好有 8 位数字的十进制数。",
            "solution": "该问题要求实现共轭梯度 (CG) 法，以求解一个源自均值-方差投资组合优化问题的线性系统 $Ax=b$。核心任务是通过分析定向扰动的影响，来证明在搜索方向之间保持 $A$-正交性的重要性。\n\n首先，我们对问题陈述进行形式化验证。\n\n给定条件如下：\n- 目标函数：$f(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2$。\n- 线性系统：$A x = b$。\n- 系统矩阵：$A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top}$。\n- 右端向量：$b \\equiv \\mu + \\eta\\, \\mathbf{1}$。\n- 系统维度：$n = 6$。\n- 相关系数：$\\rho = 0.2$。\n- 相关矩阵：$R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$。\n- 资产标准差：$s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]^{\\top}$。\n- 协方差矩阵：$\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$。\n- 期望收益向量：$\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$。\n- 惩罚权重：$\\eta = 10^{-2}$。\n- CG 起始向量：$x_0 = \\mathbf{0}$。\n- 迭代次数：精确为 $n=6$。\n- 扰动规则：对于 $j = k_{\\text{perturb}}$，用 $\\tilde{p}_j = p_j + \\varepsilon \\lVert p_j \\rVert_2 e_1$ 替换搜索方向 $p_j$。此规则在 $k_{\\text{perturb}} \\in \\{0, 1, \\dots, n-1\\}$ 且 $\\varepsilon > 0$ 时生效。\n- 度量指标：$\\lVert b - A x_n \\rVert_2$ 和 $\\max_{i \\ne j} \\left| (P^{\\top} A P)_{ij} \\right|$。\n\n该问题是有效的。它在科学上基于计算金融学和数值线性代数的既定原理。协方差矩阵 $\\Sigma$ 的构造使其成为对称正定 (SPD) 矩阵。系统矩阵 $A = \\Sigma + \\eta \\mathbf{1}\\mathbf{1}^{\\top}$ 是一个 SPD 矩阵 $\\Sigma$ 与一个对称半正定矩阵 $\\eta \\mathbf{1}\\mathbf{1}^{\\top}$ (因为 $\\eta = 10^{-2} > 0$) 之和，这确保了 $A$ 也是 SPD 的。因此，线性系统 $Ax=b$ 是良态的 (well-posed)，并且 CG 法是一种合适且理论上可靠的求解方法。所有参数和算法要求都具有足够的精度并且内部一致。\n\n我们首先构建系统各组件。维度为 $n=6$。向量 $\\mu \\in \\mathbb{R}^6$ 和 $s \\in \\mathbb{R}^6$ 已给出。向量 $\\mathbf{1}$ 是 $\\mathbb{R}^6$ 中所有元素为1的向量，$I$ 是 $6 \\times 6$ 的单位矩阵。\n相关矩阵 $R$ 为：\n$$ R = (1-0.2)I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} = 0.8 I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n协方差矩阵 $\\Sigma$ 的构造如下：\n$$ \\Sigma = \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) \\cdot R \\cdot \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) $$\n当 $\\eta = 0.01$ 时，系统矩阵 $A$ 和向量 $b$ 为：\n$$ A = \\Sigma + 0.01 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n$$ b = \\mu + 0.01 \\cdot \\mathbf{1} $$\n\n共轭梯度算法是一种迭代方法，通过生成一系列相互 $A$-正交（或共轭）的搜索方向 $\\{p_j\\}$（即当 $i \\ne j$ 时 $p_i^{\\top} A p_j = 0$）来求解 $Ax=b$。标准算法从 $x_0 = \\mathbf{0}$，$r_0 = b$ 和 $p_0 = r_0$ 开始，按以下步骤进行：\n对于 $j = 0, 1, \\dots, n-1$：\n1. 计算步长：$\\alpha_j = \\frac{r_j^{\\top} r_j}{p_j^{\\top} A p_j}$\n2. 更新解：$x_{j+1} = x_j + \\alpha_j p_j$\n3. 更新残差：$r_{j+1} = r_j - \\alpha_j A p_j$\n4. 计算改进因子：$\\beta_j = \\frac{r_{j+1}^{\\top} r_{j+1}}{r_j^{\\top} r_j}$\n5. 更新搜索方向：$p_{j+1} = r_{j+1} + \\beta_j p_j$\n\n在精确算术中，此过程会生成一组 $A$-正交的搜索方向基，并在至多 $n$ 次迭代内找到精确解。这个问题的核心是破坏这一性质。在指定的迭代次数 $j=k_{\\text{perturb}}$ 时，搜索方向 $p_j$ 被扰动：\n$$ \\tilde{p}_j = p_j + \\varepsilon \\lVert p_j \\rVert_2 e_1 $$\n其中 $e_1 = [1, 0, \\dots, 0]^{\\top}$。这个被扰动的方向 $\\tilde{p}_j$ 随后被用来替代 $p_j$ 以更新 $x_{j+1}$ 和 $r_{j+1}$。关键的是，后续的搜索方向 $p_{j+1}$ 是基于这个被扰动的方向构建的：\n$$ p_{j+1} = r_{j+1} + \\beta_j \\tilde{p}_j $$\n这种扰动打破了共轭链。新的方向 $\\tilde{p}_j$ 通常与之前的方向 $p_0, \\dots, p_{j-1}$ 不再 $A$-正交。这个误差会传播下去，因为所有后续方向都是在这个受污染的步骤之上构建的。结果，这组包含 $n$ 个执行过的搜索方向 $\\{p_0, \\dots, \\tilde{p}_j, \\dots, p_{n-1}\\}$ 不再是 $A$-正交基，因此在 $n$ 步内收敛的理论保证也就不复存在了。\n\n我们预期会观察到：\n1. 对于未受扰动的运行（情况1和4），最终残差范数 $\\lVert b - A x_n \\rVert_2$ 和 $A$-正交性缺陷 $\\max_{i \\ne j} | (P^{\\top} A P)_{ij} |$ 将接近机器精度（接近于 $0$）。矩阵 $P^{\\top} A P$ 将几乎是完美的对角矩阵。\n2. 对于受扰动的运行（情况2、3和5），两个度量值都将显著大于 $0$。这表明算法未能在 $n$ 步内收敛到精确解，并且搜索方向的底层共轭性质已被破坏。误差的大小将取决于扰动强度 $\\varepsilon$ 和迭代索引 $k_{\\text{perturb}}$。\n\n解决方案的实现首先根据问题数据定义系统矩阵。然后创建一个函数来执行 CG 算法。该函数包含了扰动指定搜索方向并存储所有使用过的方向的逻辑。在精确执行 $n=6$ 次迭代后，它从搜索方向矩阵中计算最终的残差范数和 $A$-正交性缺陷。对五个测试用例中的每一个都重复此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Sets up the portfolio optimization problem, runs the Conjugate Gradient (CG)\n    method for several test cases with and without perturbation, and\n    calculates the specified performance metrics.\n    \"\"\"\n    # Problem Constants and Data\n    n = 6\n    rho = 0.2\n    s = np.array([0.15, 0.20, 0.25, 0.30, 0.22, 0.18])\n    mu = np.array([0.08, 0.10, 0.12, 0.15, 0.11, 0.09])\n    eta = 1e-2\n\n    # Construct the linear system Ax = b\n    ones = np.ones((n, 1))\n    R = rho * (ones @ ones.T) + (1 - rho) * np.identity(n)\n    D = np.diag(s)\n    Sigma = D @ R @ D\n    A = Sigma + eta * (ones @ ones.T)\n    b = mu + eta * ones.flatten()\n\n    def run_cg_perturbed(A_mat, b_vec, k_perturb, epsilon, n_iter):\n        \"\"\"\n        Runs the Conjugate Gradient algorithm for n_iter steps with an\n        optional perturbation on a specified search direction.\n        \n        Args:\n            A_mat (np.ndarray): The system matrix (n x n).\n            b_vec (np.ndarray): The right-hand side vector (n,).\n            k_perturb (int): The index of the iteration to perturb.\n            epsilon (float): The magnitude of the perturbation.\n            n_iter (int): The number of iterations to run.\n\n        Returns:\n            tuple: A tuple containing:\n                - residual_norm (float): The L2 norm of the final residual.\n                - defect (float): The A-orthogonality defect.\n        \"\"\"\n        dim = A_mat.shape[0]\n        x = np.zeros(dim)\n        r = b_vec - A_mat @ x\n        p = r.copy()\n        rs_old = r.T @ r\n        \n        p_storage = []\n        e1 = np.zeros(dim)\n        e1[0] = 1.0\n\n        for j in range(n_iter):\n            p_effective = p.copy()\n            \n            # Apply perturbation if conditions are met\n            if j == k_perturb and epsilon > 0:\n                p_norm = np.linalg.norm(p)\n                perturbation = epsilon * p_norm * e1\n                p_effective = p + perturbation\n\n            p_storage.append(p_effective)\n            \n            Ap = A_mat @ p_effective\n            alpha = rs_old / (p_effective.T @ Ap)\n            \n            x = x + alpha * p_effective\n            r = r - alpha * Ap\n            \n            rs_new = r.T @ r\n            \n            beta = rs_new / rs_old\n            p = r + beta * p_effective\n            rs_old = rs_new\n\n        # 1. Calculate final residual norm\n        final_residual_norm = np.linalg.norm(b_vec - A_mat @ x)\n        \n        # 2. Calculate A-orthogonality defect\n        P = np.array(p_storage).T\n        G = P.T @ A_mat @ P\n        np.fill_diagonal(G, 0.0) # We only care about off-diagonal elements\n        orthogonality_defect = np.max(np.abs(G))\n        \n        return final_residual_norm, orthogonality_defect\n\n    test_cases = [\n        (-1, 0.0),    # Case 1: No perturbation (invalid index)\n        (2, 1.0),     # Case 2: Perturb p_2 with eps=1\n        (0, 1.0),     # Case 3: Perturb p_0 with eps=1\n        (10, 1.0),    # Case 4: No perturbation (index out of bounds)\n        (3, 1e-3),    # Case 5: Perturb p_3 with small eps\n    ]\n\n    results = []\n    for k_perturb, epsilon in test_cases:\n        # Per problem spec, apply perturbation only if k is valid and eps > 0\n        effective_k = k_perturb\n        effective_eps = epsilon\n        if not (0 = k_perturb  n and epsilon > 0):\n            effective_k = -1 # to disable perturbation in the function\n            effective_eps = 0.0\n\n        res_norm, defect = run_cg_perturbed(A, b, effective_k, effective_eps, n)\n        results.append(f\"{res_norm:.8f}\")\n        results.append(f\"{defect:.8f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}