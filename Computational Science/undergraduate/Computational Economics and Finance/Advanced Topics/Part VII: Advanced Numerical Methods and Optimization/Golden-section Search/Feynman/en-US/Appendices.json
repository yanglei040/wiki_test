{
    "hands_on_practices": [
        {
            "introduction": "Before implementing any algorithm, it's crucial to understand its underlying assumptions and potential pitfalls. The Golden-Section Search is powerful, but its guarantee of finding a minimum rests on the critical assumption that the function is unimodal within the search interval. This first practice problem challenges you to think critically about how reformulating a problem—in this case, turning a root-finding problem $g(x)=c$ into a minimization problem by minimizing $f(x) = (g(x)-c)^2$—can inadvertently create multiple local minima, leading GSS astray. Mastering these concepts  is fundamental to correctly applying optimization techniques in practice.",
            "id": "2421149",
            "problem": "An engineer in computational engineering needs to solve the scalar equation $g(x)=c$ over a closed interval $[a,b]$. To do so, the engineer proposes to minimize the squared residual $f(x)=(g(x)-c)^2$ on $[a,b]$ using Golden-Section Search (GSS). Assume $g$ is continuous on $[a,b]$ and at least once differentiable on $(a,b)$. Select all statements that are correct about the validity of this approach and its pitfalls related to local minima.\n\nA. If $g$ is continuous and strictly monotone on $[a,b]$ and $c\\in g([a,b])$, then $f(x)$ is unimodal on $[a,b]$ with a unique global minimizer at the unique solution of $g(x)=c$, so applying GSS to $f$ on any bracket $[a,b]$ that contains that solution will return a root.\n\nB. The only stationary points of $f(x)$ are the solutions of $g(x)=c$.\n\nC. If $g$ is non-monotone on $[a,b]$, then $f(x)$ can have local minima at points where $g'(x)=0$ and $g(x)\\neq c$, so GSS run on $f$ over such a bracket can converge to a non-root local minimizer depending on the initial bracket.\n\nD. If $c\\notin g([a,b])$, then minimizing $f$ with GSS on $[a,b]$ will not produce a root of $g(x)=c$; instead, it will converge to a point in $[a,b]$ (possibly an endpoint) with nonzero residual.\n\nE. Squaring the residual makes $f(x)$ convex on $[a,b]$ for any differentiable $g$, so no spurious local minima exist and GSS will always find a root if one exists anywhere on the real line.",
            "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n- Equation to solve: $g(x) = c$\n- Domain: Closed interval $[a, b]$\n- Proposed method: Minimize the function $f(x) = (g(x) - c)^2$\n- Optimization algorithm: Golden-Section Search (GSS)\n- Properties of $g(x)$:\n    - Continuous on $[a, b]$\n    - At least once differentiable on $(a, b)$\n- Task: Evaluate statements about the validity and pitfalls of this approach.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard technique in numerical analysis: reformulating a root-finding problem as a minimization problem. The functions, concepts (continuity, differentiability, monotonicity, unimodality), and algorithm (GSS) are well-defined within mathematics and computational engineering. The approach is scientifically valid.\n- **Well-Posed:** The problem provides sufficient information to analyze the mathematical properties of the function $f(x)$ based on the given properties of $g(x)$. The questions posed in the options are precise and can be answered with mathematical rigor.\n- **Objective:** The problem and the statements to be evaluated are objective and devoid of subjective claims. Their correctness can be determined through mathematical proof or counterexample.\n\nThe problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, objective, and formalizable.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n**Derivation of Core Principles**\n\nThe problem proposes to find a root of $g(x) = c$ by minimizing the squared residual function $f(x) = (g(x) - c)^2$. The global minima of $f(x)$ occur where $f(x) = 0$, which is true if and only if $g(x) = c$. Therefore, the global minimizers of $f(x)$ are precisely the roots of $g(x) = c$.\n\nGolden-Section Search (GSS) is an algorithm guaranteed to find the minimum of a function over an interval only if the function is **unimodal** on that interval. A function is unimodal on an interval if it has exactly one local minimum in that interval. If a function has multiple local minima (i.e., is not unimodal), GSS may converge to a local minimum that is not the global minimum, depending on the initial search bracket.\n\nTo understand the behavior of GSS on $f(x)$, we must analyze the stationary points of $f(x)$, which determine its local minima and maxima. Using the chain rule, the first derivative of $f(x)$ is:\n$$ f'(x) = 2(g(x) - c) \\cdot g'(x) $$\nThe stationary points of $f(x)$ are the values of $x$ for which $f'(x) = 0$. This occurs if and only if:\n$$ g(x) - c = 0 \\quad \\text{or} \\quad g'(x) = 0 $$\nThis shows that the stationary points of $f(x)$ consist of two sets:\n$1$. The roots of $g(x) = c$.\n$2$. The stationary points of $g(x)$ itself.\n\nThese latter points, where $g'(x)=0$ but $g(x) \\neq c$, can introduce \"spurious\" local minima in $f(x)$, which are not roots of the original equation.\n\nTo determine if a stationary point $x_0$ is a local minimum, we can examine the second derivative, $f''(x_0)$:\n$$ f''(x) = 2(g'(x))^2 + 2(g(x) - c)g''(x) $$\nAt a stationary point $x_0$ where $g'(x_0) = 0$ and $g(x_0) \\neq c$, the second derivative simplifies to:\n$$ f''(x_0) = 2(g(x_0) - c)g''(x_0) $$\nFor $x_0$ to be a local minimum of $f$, we require $f''(x_0) > 0$. This condition can be met, as shown in the analysis of option C.\n\n**Option-by-Option Analysis**\n\n**A. If $g$ is continuous and strictly monotone on $[a,b]$ and $c\\in g([a,b])$, then $f(x)$ is unimodal on $[a,b]$ with a unique global minimizer at the unique solution of $g(x)=c$, so applying GSS to $f$ on any bracket $[a,b]$ that contains that solution will return a root.**\n\nIf $g(x)$ is continuous and strictly monotone on $[a,b]$, and $c$ is in the range of $g$ on this interval, the Intermediate Value Theorem guarantees there is a unique solution $x^*$ in $[a,b]$ such that $g(x^*) = c$. At this point, $f(x^*) = (g(x^*) - c)^2 = 0$. Since $f(x) \\ge 0$ for all $x$, $x^*$ is a global minimizer.\n\nBecause $g(x)$ is strictly monotone, $g'(x)$ does not change sign on $(a, b)$ and is non-zero (except possibly at isolated points that do not constitute local extrema of $g$). The only way for $f'(x) = 2(g(x) - c)g'(x)$ to be zero is if $g(x) - c = 0$, which happens only at $x = x^*$. Thus, $f(x)$ has only one stationary point in the interval.\nLet's assume $g(x)$ is strictly increasing, so $g'(x) > 0$.\n- For $x < x^*$, $g(x) < g(x^*) = c$, so $g(x) - c < 0$. Thus, $f'(x) = 2(\\text{negative})(\\text{positive}) < 0$.\n- For $x > x^*$, $g(x) > g(x^*) = c$, so $g(x) - c > 0$. Thus, $f'(x) = 2(\\text{positive})(\\text{positive}) > 0$.\nThis shows $f(x)$ is decreasing for $x < x^*$ and increasing for $x > x^*$. Therefore, $f(x)$ is unimodal on $[a,b]$ with its unique minimum at $x^*$. GSS is designed for unimodal functions and will correctly converge to this unique minimizer.\nVerdict: **Correct**.\n\n**B. The only stationary points of $f(x)$ are the solutions of $g(x)=c$.**\n\nAs derived above, the stationary points of $f(x)$ occur when $f'(x) = 2(g(x) - c) g'(x) = 0$. This equation is satisfied when $g(x) = c$ OR when $g'(x) = 0$. If there exists a point $x_0$ where $g(x)$ has a local extremum (so $g'(x_0) = 0$) and $g(x_0) \\neq c$, then $x_0$ is a stationary point of $f(x)$ but not a solution to $g(x) = c$. For example, let $g(x) = x^2$ and $c=4$. The solutions are $x=\\pm 2$. The function to minimize is $f(x) = (x^2 - 4)^2$. The derivative is $f'(x) = 2(x^2 - 4)(2x) = 4x(x-2)(x+2)$. The stationary points are $x=0$, $x=2$, and $x=-2$. The point $x=0$ is a stationary point of $f(x)$ because $g'(0)=0$, but $g(0) = 0 \\neq 4$, so it is not a solution.\nVerdict: **Incorrect**.\n\n**C. If $g$ is non-monotone on $[a,b]$, then $f(x)$ can have local minima at points where $g'(x)=0$ and $g(x)\\neq c$, so GSS run on $f$ over such a bracket can converge to a non-root local minimizer depending on the initial bracket.**\n\nIf $g(x)$ is non-monotone, there must be at least one point $x_0 \\in (a,b)$ where $g'(x_0)=0$. This point is a stationary point of $f(x)$. We must check if it can be a local minimum. As shown in the general derivation, $f''(x_0) = 2(g(x_0) - c)g''(x_0)$. We can make this positive.\nConsider $g(x) = x^3 - 4x$. Then $g'(x) = 3x^2 - 4$, which is zero at $x_0 = \\pm 2/\\sqrt{3}$. Let's pick $x_0 = 2/\\sqrt{3}$. At this point, $g(x)$ a local minimum: $g(2/\\sqrt{3}) = (8/3\\sqrt{3}) - (8/\\sqrt{3}) = -16/(3\\sqrt{3})$ and $g''(x) = 6x$, so $g''(2/\\sqrt{3}) = 12/\\sqrt{3} > 0$.\nLet's find a root for $c < g(x_0)$. For example, let $c = -6$. The equation is $x^3-4x = -6$. The function to minimize is $f(x)=(x^3-4x+6)^2$. A real root $x^*$ exists (e.g., $g(-3)=-15$, $g(-2)=0$, so root is between $-3$ and $-2$).\nAt the stationary point $x_0=2/\\sqrt{3}$ of $g(x)$, we have $g(x_0) = -16/(3\\sqrt{3}) \\approx -3.078$, which is not equal to $c=-6$. The second derivative of $f(x)$ at $x_0$ is $f''(x_0) = 2(g(x_0)-c)g''(x_0) = 2(-16/(3\\sqrt{3}) - (-6)) (12/\\sqrt{3}) = 2(-3.078+6)(6.928) > 0$.\nSo, $x_0 = 2/\\sqrt{3}$ is a local minimum of $f(x)$. Since $f(x_0) = (g(x_0)-c)^2 > 0$ while the global minimum is $f(x^*)=0$, $f(x)$ is not unimodal. GSS started on an interval containing $x_0$ but not $x^*$ (e.g., $[0,2]$) could converge to the local minimizer $x_0$, which is not a root of $g(x)=c$.\nVerdict: **Correct**.\n\n**D. If $c\\notin g([a,b])$, then minimizing $f$ with GSS on $[a,b]$ will not produce a root of $g(x)=c$; instead, it will converge to a point in $[a,b]$ (possibly an endpoint) with nonzero residual.**\n\nThe premise is that $c$ is not in the range of $g(x)$ on the interval $[a,b]$. This means there is no $x \\in [a,b]$ for which $g(x) = c$. Thus, no root exists in the interval. The function $f(x) = (g(x) - c)^2$ is therefore strictly positive for all $x \\in [a,b]$. As a continuous function on a compact set $[a,b]$, $f(x)$ must attain a global minimum value on this interval, and this minimum value will be greater than zero. The point $x_{min}$ where this minimum occurs represents the best \"least-squares\" approximation to a solution in $[a,b]$.\nSince no root exists, GSS cannot produce a root. What it will do is search for a minimum of $f(x)$. GSS iteratively narrows the search interval. Assuming it converges (which it will, as the interval shrinks), it will converge to a point corresponding to a minimum of $f(x)$ within the initial bracket. Since the minimum value of $f(x)$ is strictly positive, the point found will have a non-zero residual, i.e., $f(x_{min}) > 0$. Such a minimum can occur at an endpoint if, for example, $f(x)$ is monotonic on $[a,b]$. The statement accurately describes that the outcome is a least-squares solution, not a root.\nVerdict: **Correct**.\n\n**E. Squaring the residual makes $f(x)$ convex on $[a,b]$ for any differentiable $g$, so no spurious local minima exist and GSS will always find a root if one exists anywhere on the real line.**\n\nThis statement makes several incorrect claims.\n$1$. **Convexity**: $f(x)$ is not generally convex. As shown above, $f''(x) = 2(g'(x))^2 + 2(g(x) - c)g''(x)$. The second term, $2(g(x) - c)g''(x)$, can be negative and large enough to make $f''(x) < 0$. For a counterexample, let $g(x) = \\sin(x)$ and $c=0$. Then $f(x) = \\sin^2(x)$. $f''(x) = 2\\cos(2x)$, which is negative for $x \\in (\\pi/4, 3\\pi/4)$, so $f(x)$ is not convex on this interval.\n$2$. **No spurious local minima**: As demonstrated for option C, spurious (non-root) local minima can and do exist when $g(x)$ is non-monotone. Convexity is a sufficient (but not necessary) condition for unimodality. Since $f(x)$ is not always convex, the claim that no spurious minima exist is false.\n$3$. **Always find a root anywhere**: GSS is a local search method restricted to its initial search bracket $[a, b]$. It has no mechanism to find roots that lie outside this bracket.\nEach part of this statement is false.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ACD}$$"
        },
        {
            "introduction": "With a solid theoretical foundation, it is time to implement the Golden-Section Search algorithm from first principles. This exercise  guides you through building a GSS solver to find the mode, or the most likely value, of a unimodal statistical distribution. You will work with the log-density of the Beta distribution, a common technique in econometrics and statistics to improve numerical stability, providing a robust and practical context for honing your implementation skills.",
            "id": "2398550",
            "problem": "You are tasked with implementing a Golden-Section Search (GSS) procedure to numerically compute the mode of a unimodal probability density function arising in a one-dimensional econometric setting. In many computational economics and finance problems, one needs to locate the most likely value of a parameter or latent variable under a parametric model by maximizing a one-dimensional likelihood or posterior density. For a Beta distribution with parameters $ \\alpha $ and $ \\beta $ satisfying $ \\alpha > 1 $ and $ \\beta > 1 $, the probability density function is supported on the interval $ [0,1] $ and is unimodal. The mode is the point $ x^\\star \\in [0,1] $ at which the density attains its maximum.\n\nFundamental base to use:\n- Definition of a mode: For a density $ f(x) $ on a closed interval $ [a,b] $, a mode is an $ x^\\star \\in [a,b] $ such that $ f(x^\\star) \\ge f(x) $ for all $ x \\in [a,b] $.\n- Unimodality: A function $ f $ is unimodal on $ [a,b] $ if there exists a unique $ x^\\star \\in (a,b) $ such that $ f $ is strictly increasing on $ [a,x^\\star] $ and strictly decreasing on $ [x^\\star,b] $.\n- Monotone transformations preserve argmax: If $ g $ is strictly increasing and $ \\ell(x) = g(f(x)) $, then $ \\arg\\max_{x \\in [a,b]} f(x) = \\arg\\max_{x \\in [a,b]} \\ell(x) $.\n\nTask:\n1. Implement a robust Golden-Section Search (GSS) algorithm from first principles to maximize a continuous unimodal function on a closed interval $ [a,b] $ without using any derivative information. Your design must ensure numerical stability and must avoid evaluating the function outside the domain.\n2. Apply your GSS implementation to the Beta distribution with parameters $ \\alpha > 1 $, $ \\beta > 1 $. To avoid numerical underflow and to remove inessential constants, maximize the log-density up to an additive constant:\n   - For $ x \\in (0,1) $, define $ \\ell(x) = (\\alpha - 1)\\log(x) + (\\beta - 1)\\log(1 - x) $. Note that the subtraction of $ \\log B(\\alpha,\\beta) $ is unnecessary since it does not affect the maximizer.\n   - Because $ \\log(0) $ is undefined, restrict the search to the closed interval $ [\\varepsilon, 1 - \\varepsilon] $ with $ \\varepsilon = 10^{-12} $.\n3. Use a termination rule based on the absolute interval width: stop when $ b - a \\le \\tau $, where $ \\tau $ is a given tolerance. Return the midpoint $ (a + b)/2 $ as the numerical maximizer.\n\nAngle units are not involved. There are no physical units. All numerical outputs must be floats.\n\nTest suite:\nYou must evaluate your implementation on the following five parameter sets, each of which defines a unimodal Beta distribution:\n- Case $ 1 $: $ (\\alpha,\\beta) = (5,2) $.\n- Case $ 2 $: $ (\\alpha,\\beta) = (2.5,3.5) $.\n- Case $ 3 $: $ (\\alpha,\\beta) = (1.1,1.1) $.\n- Case $ 4 $: $ (\\alpha,\\beta) = (20,20) $.\n- Case $ 5 $: $ (\\alpha,\\beta) = (1000.5,1.1) $.\n\nDesign for coverage:\n- Case $ 1 $ is a right-skewed distribution with interior mode well away from the boundaries.\n- Case $ 2 $ is moderately skewed, testing general performance.\n- Case $ 3 $ is nearly flat but still unimodal, testing numerical stability near $ \\alpha,\\beta \\downarrow 1 $.\n- Case $ 4 $ is sharply peaked and symmetric, testing convergence when curvature is high near the maximizer.\n- Case $ 5 $ has a mode very close to the right boundary, testing robustness near the edges of the support.\n\nImplementation requirements:\n- Use $ \\varepsilon = 10^{-12} $ for the search interval ends and a tolerance $ \\tau = 10^{-12} $.\n- All computations should be performed in floating point.\n- The final answers for each case should be the estimated modes, each rounded to exactly six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if there are three results $ r_1 $, $ r_2 $, $ r_3 $, the output must have the form $ [r_1,r_2,r_3] $. For this problem with five test cases, print $ [m_1,m_2,m_3,m_4,m_5] $, where $ m_i $ is the estimated mode for case $ i $, each rounded to six decimal places.",
            "solution": "The problem as stated is subjected to validation against criteria of scientific soundness, formal specifiability, and objectivity.\n\n**Step 1: Extracted Givens**\n- **Objective**: Implement a Golden-Section Search (GSS) algorithm to numerically compute the mode of a unimodal probability density function.\n- **Target Function**: Maximize the log-density of a Beta distribution, $\\ell(x) = (\\alpha - 1)\\log(x) + (\\beta - 1)\\log(1 - x)$, for parameters $\\alpha > 1$ and $\\beta > 1$.\n- **Search Domain**: The closed interval $[\\varepsilon, 1 - \\varepsilon]$, where $\\varepsilon = 10^{-12}$.\n- **Algorithm**: Golden-Section Search, implemented from first principles without derivative information.\n- **Termination Criterion**: The search terminates when the interval width, $b - a$, becomes less than or equal to a specified tolerance $\\tau = 10^{-12}$.\n- **Output of Algorithm**: The midpoint of the final interval, $(a + b) / 2$.\n- **Test Cases (Parameter sets for $(\\alpha, \\beta)$)**:\n    1.  $(5, 2)$\n    2.  $(2.5, 3.5)$\n    3.  $(1.1, 1.1)$\n    4.  $(20, 20)$\n    5.  $(1000.5, 1.1)$\n- **Output Formatting**: A single line containing a comma-separated list of the five estimated modes, enclosed in square brackets, with each mode rounded to six decimal places.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientific and Factual Soundness**: The problem is scientifically sound. The Beta distribution is a standard probability distribution. For $\\alpha > 1$ and $\\beta > 1$, its density is indeed unimodal on the interval $(0, 1)$. The mode is given analytically by $x^\\star = (\\alpha - 1) / (\\alpha + \\beta - 2)$, which confirms the existence and uniqueness of an interior mode. The use of the log-density $\\ell(x)$ for maximization is a standard and valid numerical technique, as the logarithm is a strictly increasing monotonic transformation that preserves the location of the maximizer (the argmax). The Golden-Section Search is the correct and standard derivative-free algorithm for finding the extremum of a unimodal function.\n- **Well-Posedness**: The problem is well-posed. It specifies a unimodal objective function, a closed search interval, a clear termination condition, and all necessary parameters. These conditions guarantee the existence of a unique solution that the proposed algorithm can converge to.\n- **Completeness and Consistency**: The problem statement is complete and self-contained. It provides all data required for implementation and testing. There are no contradictions in the provided information. The conditions $\\alpha > 1, \\beta > 1$ are met by all test cases. The search interval $[\\varepsilon, 1-\\varepsilon]$ correctly avoids the singularities of the logarithm at the boundaries $0$ and $1$.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a standard and well-defined problem in computational statistics and econometrics. A rigorous solution can be constructed.\n\n**Solution Derivation**\n\nThe task is to find the value $x^\\star$ that maximizes the probability density function (PDF) of a Beta distribution, $f(x; \\alpha, \\beta)$, for $x \\in [0, 1]$.\n$$\nf(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\n$$\nwhere $B(\\alpha, \\beta)$ is the Beta function, which serves as a normalization constant. The mode $x^\\star$ is defined as $x^\\star = \\arg\\max_{x \\in [0,1]} f(x; \\alpha, \\beta)$.\n\nSince the logarithm is a strictly increasing function, maximizing $f(x)$ is equivalent to maximizing its logarithm, $\\log(f(x))$. This transformation is numerically advantageous as it converts products into sums and mitigates potential underflow issues with very small density values.\n$$\n\\log(f(x; \\alpha, \\beta)) = (\\alpha - 1)\\log(x) + (\\beta - 1)\\log(1-x) - \\log(B(\\alpha, \\beta))\n$$\nThe term $\\log(B(\\alpha, \\beta))$ is a constant with respect to $x$ and can be discarded without affecting the location of the maximizer. Therefore, the problem correctly reduces to maximizing the function $\\ell(x)$:\n$$\n\\ell(x) = (\\alpha - 1)\\log(x) + (\\beta - 1)\\log(1 - x)\n$$\nThe conditions $\\alpha > 1$ and $\\beta > 1$ ensure that $\\ell(x)$ is strictly concave, and thus unimodal, on the interval $(0, 1)$. The function is undefined at $x=0$ and $x=1$, so the search is correctly restricted to a slightly smaller closed interval $[\\varepsilon, 1 - \\varepsilon]$, for a small positive constant $\\varepsilon$.\n\nThe Golden-Section Search (GSS) is an iterative algorithm designed to find the extremum of a unimodal function over a given interval. Its main advantage is its derivative-free nature and guaranteed linear convergence rate. The algorithm works by maintaining an interval $[a, b]$ where the maximizer is known to lie.\n\nThe core of the algorithm relies on the golden ratio, $\\phi = \\frac{1 + \\sqrt{5}}{2} \\approx 1.618034$. Its reciprocal is $\\psi = 1/\\phi = \\frac{\\sqrt{5} - 1}{2} \\approx 0.618034$. At each step, two interior points, $c$ and $d$, are chosen within the current interval $[a, b]$ such that $a < c < d < b$. These points are positioned symmetrically to maintain the golden ratio proportion:\n$$\nc = a + (1-\\psi)(b-a) = a + \\psi^2(b-a) \\approx a + 0.382(b-a)\n$$\n$$\nd = a + \\psi(b-a) \\approx a + 0.618(b-a)\n$$\nThe function $\\ell(x)$ is evaluated at these points, $\\ell(c)$ and $\\ell(d)$. Based on the comparison, the search interval is reduced:\n1.  If $\\ell(c) > \\ell(d)$, the maximizer must lie in the interval $[a, d]$ because of the unimodality property. The interval is thus updated: $[a', b'] = [a, d]$.\n2.  If $\\ell(d) \\ge \\ell(c)$, the maximizer must lie in the interval $[c, b]$. The interval is updated: $[a', b'] = [c, b]$.\n\nA key feature of GSS is its efficiency. When the interval is reduced, one of the old interior points becomes an interior point for the new interval, saving one function evaluation per iteration.\n- Case 1: If the new interval is $[a, d]$, the new interior points would be $c' = a + \\psi^2(d-a)$ and $d' = a + \\psi(d-a)$. It can be shown that the old point $c$ becomes the new point $d'$, i.e., $d' = a + \\psi(d-a) = a + \\psi \\cdot \\psi(b-a) = a + \\psi^2(b-a) = c$. So, we set $b \\leftarrow d$, $d \\leftarrow c$, and only need to compute the new $c$.\n- Case 2: If the new interval is $[c, b]$, the old point $d$ becomes the new point $c'$. So, we set $a \\leftarrow c$, $c \\leftarrow d$, and only need to compute the new $d$.\n\nThis iterative process continues until the width of the interval, $b - a$, is smaller than the specified tolerance $\\tau$. The final estimate for the maximizer is the midpoint of the final interval, $(a + b)/2$.\n\nThe algorithm is implemented as follows:\n1.  Initialize the search interval $[a, b] = [\\varepsilon, 1 - \\varepsilon]$ with $\\varepsilon = 10^{-12}$.\n2.  Define the constants $\\psi = (\\sqrt{5} - 1)/2$ and $\\psi^2$.\n3.  Calculate the initial interior points $c = a + \\psi^2(b-a)$ and $d = a + \\psi(b-a)$.\n4.  Evaluate the function $\\ell(c)$ and $\\ell(d)$.\n5.  Enter a loop that continues as long as $b - a > \\tau$.\n6.  Inside the loop, compare $\\ell(c)$ and $\\ell(d)$ and reduce the interval $[a,b]$ as described above, reusing one point and its function value and computing only one new point and its function value.\n7.  Once the loop terminates, return $(a+b)/2$ as the result.\nThis procedure will be applied to each of the five test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the mode of Beta distributions using Golden-Section Search.\n    \n    This function implements the entire process as specified:\n    1. Defines the test cases for Beta distribution parameters (alpha, beta).\n    2. Uses a robust Golden-Section Search (GSS) implementation to find the mode.\n    3. The GSS maximizes the log-density function to ensure numerical stability.\n    4. Collects the results, formats them as required, and prints them.\n    \"\"\"\n\n    def golden_section_search(f, a, b, tol=1e-12):\n        \"\"\"\n        Finds the maximum of a unimodal function f on a closed interval [a, b]\n        using the Golden-Section Search algorithm.\n\n        Args:\n            f: The unimodal function to maximize.\n            a: The lower bound of the interval.\n            b: The upper bound of the interval.\n            tol: The tolerance for the interval width to terminate the search.\n\n        Returns:\n            The estimated x-value of the maximum.\n        \"\"\"\n        # Golden ratio constants\n        inv_phi = (np.sqrt(5) - 1) / 2  # 1/phi, approx 0.618\n        inv_phi_sq = inv_phi**2         # 1/phi^2, approx 0.382\n\n        # Initialize the interior points\n        h = b - a\n        c = a + inv_phi_sq * h\n        d = a + inv_phi * h\n        \n        # Evaluate function at interior points\n        f_c = f(c)\n        f_d = f(d)\n\n        while (b - a) > tol:\n            if f_c > f_d:\n                # The maximum is in the interval [a, d]\n                b = d\n                d = c\n                f_d = f_c\n                h = b - a\n                c = a + inv_phi_sq * h\n                f_c = f(c)\n            else:\n                # The maximum is in the interval [c, b]\n                a = c\n                c = d\n                f_c = f_d\n                h = b - a\n                d = a + inv_phi * h\n                f_d = f(d)\n        \n        # Return the midpoint of the final interval\n        return (a + b) / 2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (5, 2),        # Case 1\n        (2.5, 3.5),    # Case 2\n        (1.1, 1.1),    # Case 3\n        (20, 20),      # Case 4\n        (1000.5, 1.1)  # Case 5\n    ]\n\n    # Constants for the search\n    epsilon = 1e-12\n    tolerance = 1e-12\n    \n    search_interval_a = epsilon\n    search_interval_b = 1 - epsilon\n    \n    results = []\n    \n    for alpha, beta in test_cases:\n        # Define the log-density function (up to an additive constant)\n        # for the Beta distribution with parameters alpha and beta.\n        # This is the function to be maximized.\n        log_density = lambda x: (alpha - 1) * np.log(x) + (beta - 1) * np.log(1 - x)\n        \n        # Find the mode using Golden-Section Search\n        mode_estimate = golden_section_search(\n            log_density, \n            search_interval_a, \n            search_interval_b, \n            tolerance\n        )\n        \n        results.append(mode_estimate)\n\n    # Final print statement in the exact required format.\n    # Each result is rounded to six decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final exercise demonstrates the true power of numerical optimization by applying your GSS skills to a sophisticated problem in quantitative finance. You will calibrate the jump intensity parameter, $\\lambda$, in the Merton jump-diffusion model, a cornerstone of option pricing that accounts for sudden market shocks. Because the model's pricing formula is complex, finding the parameter that best fits market data analytically is often intractable, making numerical methods like GSS an indispensable tool for practitioners .",
            "id": "2398577",
            "problem": "Write a complete, runnable program that uses Golden-Section Search (GSS) to calibrate the jump intensity parameter $\\lambda$ in a Merton jump-diffusion model by minimizing the sum of squared pricing errors between model and synthetic market prices of out-of-the-money European call options. Your program must implement, from first principles, the pricing model and the univariate optimizer, and must not rely on any external optimizer.\n\nFundamental base:\n- Under the risk-neutral measure, the price of a European call option with strike $K$ and maturity $T$ is given by the risk-neutral expectation $C = e^{-r T} \\mathbb{E}\\left[(S_T - K)^{+}\\right]$.\n- In the Merton jump-diffusion model, the asset dynamics under the risk-neutral measure are\n$$\n\\frac{dS_t}{S_{t^-}} = \\left(r - q - \\lambda k\\right)\\,dt + \\sigma\\, dW_t + (J - 1)\\, dN_t,\n$$\nwhere $r$ is the continuously compounded risk-free rate, $q$ is the continuous dividend yield, $\\sigma$ is the diffusion volatility, $N_t$ is a Poisson process with intensity $\\lambda$, and $J$ is the jump size with $\\ln J \\sim \\mathcal{N}(\\mu_J,\\sigma_J^2)$. The jump compensator is $k = \\mathbb{E}[J - 1] = e^{\\mu_J + \\tfrac{1}{2}\\sigma_J^2} - 1$.\n- Conditioning on $N_T = n$ leads to a Poisson mixture of log-normal distributions, yielding a closed-form mixture representation for the call price as a weighted sum of Black–Scholes terms.\n\nPricing model to implement:\n- Define $k = e^{\\mu_J + \\tfrac{1}{2}\\sigma_J^2} - 1$, the effective net drift $b(\\lambda) = r - q - \\lambda k$, and the per-jump scaling factor $s_J = 1 + k = e^{\\mu_J + \\tfrac{1}{2}\\sigma_J^2}$.\n- For each nonnegative integer $n$, define the conditional variance parameter\n$$\n\\sigma_n = \\sqrt{\\sigma^2 + \\frac{n\\,\\sigma_J^2}{T}},\n$$\nand the adjusted spot\n$$\nS_n = S_0\\, s_J^{\\,n}.\n$$\n- The Poisson weight is\n$$\n\\pi_n(\\lambda) = e^{-\\lambda T}\\frac{(\\lambda T)^n}{n!}.\n$$\n- The Black–Scholes (BS) call price with effective net drift $b(\\lambda)$ is\n$$\nC_{\\text{BS}}(S,K,r,q,\\sigma_n,T;b) = S\\,e^{-qT}\\,\\Phi(d_1) - K\\,e^{-rT}\\,\\Phi(d_2),\n$$\nwith\n$$\nd_1 = \\frac{\\ln\\left(\\frac{S}{K}\\right) + \\left(b + \\tfrac{1}{2}\\sigma_n^2\\right)T}{\\sigma_n \\sqrt{T}}, \\quad d_2 = d_1 - \\sigma_n \\sqrt{T},\n$$\nand $\\Phi(\\cdot)$ the standard normal cumulative distribution function.\n- The Merton call price is\n$$\nC_{\\text{Merton}}(\\lambda; S_0, K, r, q, \\sigma, \\mu_J, \\sigma_J, T) = \\sum_{n=0}^{\\infty} \\pi_n(\\lambda)\\, C_{\\text{BS}}(S_n, K, r, q, \\sigma_n, T; b(\\lambda)).\n$$\n- For numerical implementation, truncate the infinite sum by:\n  - summing over $n = 0,1,2,\\dots$ until the incremental Poisson weight $\\pi_n(\\lambda)$ falls below $10^{-12}$, or\n  - reaching a hard cap $n_{\\max} = 50$,\n  whichever occurs first.\n\nCalibration objective:\n- Given a set of out-of-the-money strikes $\\{K_i\\}_{i=1}^m$ and corresponding market call prices $\\{C^{\\text{mkt}}_i\\}_{i=1}^m$, define the objective\n$$\nJ(\\lambda) = \\sum_{i=1}^m \\left( C_{\\text{Merton}}(\\lambda; S_0, K_i, r, q, \\sigma, \\mu_J, \\sigma_J, T) - C^{\\text{mkt}}_i \\right)^2.\n$$\n- Calibrate $\\lambda$ by minimizing $J(\\lambda)$ over the closed interval $\\lambda \\in [0, 3]$ using Golden-Section Search. Terminate when the bracket length is less than $10^{-6}$ or after $200$ iterations, whichever is earlier.\n\nNumerical details:\n- Use $\\Phi(x) = \\tfrac{1}{2}\\left(1 + \\operatorname{erf}\\left(\\tfrac{x}{\\sqrt{2}}\\right)\\right)$.\n- All options are European calls, and all strikes in the test suite are strictly out-of-the-money: $K > S_0$.\n- No randomness is to be used; all numbers must be computed deterministically.\n\nTest suite:\nFor each case below, first generate synthetic market prices by evaluating the Merton price with the stated “true” $\\lambda$ and the specified numerical settings. Then, discard the true value and apply your calibration routine to recover $\\widehat{\\lambda}$ from the synthetic market prices.\n\n- Case A (happy path):\n  - $S_0 = 100.0$, $r = 0.02$, $q = 0.0$, $T = 0.75$,\n  - $\\sigma = 0.20$, $\\mu_J = -0.10$, $\\sigma_J = 0.25$,\n  - strikes $K \\in \\{105.0, 110.0, 120.0\\}$,\n  - true $\\lambda_{\\text{true}} = 0.60$.\n\n- Case B (boundary, no jumps):\n  - $S_0 = 100.0$, $r = 0.01$, $q = 0.0$, $T = 1.00$,\n  - $\\sigma = 0.15$, $\\mu_J = -0.20$, $\\sigma_J = 0.30$,\n  - strikes $K \\in \\{102.0, 110.0, 125.0\\}$,\n  - true $\\lambda_{\\text{true}} = 0.00$.\n\n- Case C (higher jump activity):\n  - $S_0 = 100.0$, $r = 0.03$, $q = 0.0$, $T = 2.00$,\n  - $\\sigma = 0.18$, $\\mu_J = 0.05$, $\\sigma_J = 0.20$,\n  - strikes $K \\in \\{105.0, 115.0, 130.0, 150.0\\}$,\n  - true $\\lambda_{\\text{true}} = 1.20$.\n\nFinal output format:\n- Your program should produce a single line of output containing the calibrated intensities for the three cases as a comma-separated list enclosed in square brackets, each value rounded to six decimal places, in the order A, B, C. For example, the output must look like\n$$\n[\\widehat{\\lambda}_A,\\widehat{\\lambda}_B,\\widehat{\\lambda}_C],\n$$\nwith each $\\widehat{\\lambda}$ rounded to $6$ decimal places (no units).",
            "solution": "The problem has been subjected to rigorous validation and is deemed valid. The formulation is scientifically grounded in the Merton jump-diffusion model, a standard in financial engineering. It is well-posed, providing a complete and internally consistent set of definitions, parameters, and numerical procedures for a deterministic optimization task. The objective is stated with mathematical precision, free from ambiguity or subjective content. All necessary components for a unique solution are provided.\n\nThe task is to calibrate the jump intensity parameter, $\\lambda$, of a Merton jump-diffusion model. This is accomplished by minimizing an objective function, $J(\\lambda)$, representing the sum of squared differences between option prices generated by the model and a set of synthetic market prices. The optimization is constrained to the interval $\\lambda \\in [0, 3]$ and must be performed using the Golden-Section Search (GSS) algorithm, implemented from first principles.\n\nThe core of the problem involves the implementation of two main components: the Merton pricing model and the GSS optimizer.\n\nFirst, we implement the pricing model for a European call option as specified. The Merton price, $C_{\\text{Merton}}$, is a weighted sum over prices of Black-Scholes-type options, conditioned on the number of jumps, $n$, occurring over the option's life, $T$. The price is given by:\n$$\nC_{\\text{Merton}}(\\lambda) = \\sum_{n=0}^{\\infty} \\pi_n(\\lambda)\\, C_{\\text{BS}}(S_n, K, r, q, \\sigma_n, T; b(\\lambda))\n$$\nThe Poisson weight $\\pi_n(\\lambda)$, representing the probability of $n$ jumps, is:\n$$\n\\pi_n(\\lambda) = e^{-\\lambda T}\\frac{(\\lambda T)^n}{n!}\n$$\nThe conditional Black-Scholes component, $C_{\\text{BS}}$, is defined for an adjusted spot price $S_n = S_0\\, s_J^{\\,n}$ and a conditional volatility $\\sigma_n = \\sqrt{\\sigma^2 + n\\,\\sigma_J^2/T}$. The per-jump scaling factor is $s_J = e^{\\mu_J + \\frac{1}{2}\\sigma_J^2}$. The component price is given by:\n$$\nC_{\\text{BS}} = S_n\\,e^{-qT}\\,\\Phi(d_1) - K\\,e^{-rT}\\,\\Phi(d_2)\n$$\nwith its parameters $d_1$ and $d_2$ dependent on an effective net drift $b(\\lambda) = r - q - \\lambda k$, where $k = s_J - 1$. The specific formula for $d_1$ is:\n$$\nd_1 = \\frac{\\ln\\left(\\frac{S_n}{K}\\right) + \\left(b(\\lambda) + \\tfrac{1}{2}\\sigma_n^2\\right)T}{\\sigma_n \\sqrt{T}}\n$$\nand $d_2 = d_1 - \\sigma_n \\sqrt{T}$. The standard normal cumulative distribution function, $\\Phi(\\cdot)$, is computed using the error function, $\\operatorname{erf}(\\cdot)$, as $\\Phi(x) = \\frac{1}{2}\\left(1 + \\operatorname{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$. For numerical implementation, the infinite sum is truncated. The summation proceeds over $n=0, 1, 2, \\dots$ and terminates when the term's Poisson weight $\\pi_n(\\lambda)$ falls below a tolerance of $10^{-12}$, or when a maximum of $n_{\\max}=50$ terms have been included, whichever occurs first. A robust iterative method is used to compute the Poisson weights, $\\pi_n(\\lambda) = \\pi_{n-1}(\\lambda) \\cdot \\frac{\\lambda T}{n}$, to avoid numerical overflow with large factorials.\n\nSecond, we define the calibration objective function, $J(\\lambda)$, as the sum of squared errors (SSE) over a set of $m$ options with different strikes $\\{K_i\\}_{i=1}^m$:\n$$\nJ(\\lambda) = \\sum_{i=1}^m \\left( C_{\\text{Merton}}(\\lambda; K_i) - C^{\\text{mkt}}_i \\right)^2\n$$\nwhere $C^{\\text{mkt}}_i$ are the given synthetic market prices.\n\nThird, we implement the Golden-Section Search (GSS) algorithm to minimize $J(\\lambda)$. GSS is a derivative-free optimization method for finding the extremum of a unimodal function by iteratively narrowing the search interval. The search begins on the interval $[a, b] = [0, 3]$. Two interior points, $c$ and $d$, are chosen to divide the interval according to the golden ratio, $\\phi = (1+\\sqrt{5})/2 \\approx 1.618$.\n$$\nc = b - \\frac{b-a}{\\phi} \\quad \\text{and} \\quad d = a + \\frac{b-a}{\\phi}\n$$\nThe objective function is evaluated at these points. If $J(c) < J(d)$, the minimum must lie in the interval $[a, d]$, so the new search interval becomes $[a, d]$. Otherwise, if $J(c) \\ge J(d)$, the new interval becomes $[c, b]$. This process reduces the interval length by a factor of $1/\\phi$ at each step, guaranteeing convergence. The algorithm terminates when the interval length $|b-a|$ is less than a tolerance of $10^{-6}$ or after a maximum of $200$ iterations. The midpoint of the final interval, $(a+b)/2$, is returned as the calibrated estimate $\\widehat{\\lambda}$.\n\nThe overall procedure involves first generating the synthetic market prices $C^{\\text{mkt}}_i$ for each test case by evaluating the specified $C_{\\text{Merton}}$ function at the given \"true\" intensity, $\\lambda_{\\text{true}}$. Then, this true value is conceptually discarded, and the GSS algorithm is employed to find the $\\widehat{\\lambda}$ that best reproduces these synthetic prices by minimizing $J(\\lambda)$. This process is repeated for all specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\n# Define a global constant for the Golden Ratio.\nGR = (1 + np.sqrt(5)) / 2\n\ndef norm_cdf(x):\n    \"\"\"\n    Computes the standard normal cumulative distribution function using the error function.\n    All mathematical entities must be in LaTeX: Phi(x) = 1/2 * (1 + erf(x/sqrt(2))).\n    \"\"\"\n    return 0.5 * (1 + erf(x / np.sqrt(2)))\n\ndef merton_bs_component(S, K, r, q, T, sigma_n, b):\n    \"\"\"\n    Computes a single Black-Scholes-like component of the Merton price series,\n    using the specific functional form provided in the problem statement.\n    \"\"\"\n    if sigma_n <= 0 or T <= 0:\n        return np.maximum(0, S - K) if S > K else 0.0\n\n    d1 = (np.log(S / K) + (b + 0.5 * sigma_n**2) * T) / (sigma_n * np.sqrt(T))\n    d2 = d1 - sigma_n * np.sqrt(T)\n    price = S * np.exp(-q * T) * norm_cdf(d1) - K * np.exp(-r * T) * norm_cdf(d2)\n    return price\n\ndef merton_price(lambda_val, S0, K, r, q, T, sigma, mu_J, sigma_J):\n    \"\"\"\n    Calculates the Merton jump-diffusion model call option price. The infinite sum\n    is truncated based on the Poisson weight or a maximum number of terms.\n    \"\"\"\n    # Jump-related parameters, constant with respect to lambda\n    k = np.exp(mu_J + 0.5 * sigma_J**2) - 1\n    s_J = 1 + k\n    \n    # Lambda-dependent effective drift\n    b = r - q - lambda_val * k\n    \n    total_price = 0.0\n    lambda_T = lambda_val * T\n    \n    n_max = 50\n    weight_tol = 1e-12\n\n    # Term for n=0 jumps (pure diffusion component)\n    poisson_weight = np.exp(-lambda_T)\n    sigma_0 = sigma\n    component_price_0 = merton_bs_component(S0, K, r, q, T, sigma_0, b)\n    total_price += poisson_weight * component_price_0\n    \n    # Terms for n=1 to n_max jumps\n    for n in range(1, n_max + 1):\n        poisson_weight *= lambda_T / n\n        if poisson_weight < weight_tol:\n            break\n        \n        sigma_n = np.sqrt(sigma**2 + n * sigma_J**2 / T)\n        S_n = S0 * (s_J**n)\n        \n        component_price = merton_bs_component(S_n, K, r, q, T, sigma_n, b)\n        total_price += poisson_weight * component_price\n        \n    return total_price\n\ndef objective_function(lambda_val, S0, r, q, T, sigma, mu_J, sigma_J, strikes, market_prices):\n    \"\"\"\n    Calculates the sum of squared errors (SSE) between model prices and market prices.\n    This is the function to be minimized.\n    \"\"\"\n    sse = 0.0\n    for i in range(len(strikes)):\n        model_price = merton_price(lambda_val, S0, strikes[i], r, q, T, sigma, mu_J, sigma_J)\n        sse += (model_price - market_prices[i])**2\n    return sse\n\ndef golden_section_search(f, a, b, tol=1e-6, max_iter=200):\n    \"\"\"\n    Performs Golden-Section Search to find the minimum of a univariate function 'f'\n    on the interval [a, b].\n    \"\"\"\n    inv_phi = 1 / GR\n    \n    # Initialize interior points\n    c = b - inv_phi * (b - a)\n    d = a + inv_phi * (b - a)\n    \n    fc = f(c)\n    fd = f(d)\n    \n    for _ in range(max_iter):\n        if abs(b - a) < tol:\n            break\n            \n        if fc < fd:\n            b = d\n            d = c\n            fd = fc\n            c = b - inv_phi * (b - a)\n            fc = f(c)\n        else:\n            a = c\n            c = d\n            fc = fd\n            d = a + inv_phi * (b - a)\n            fd = f(d)\n            \n    return (a + b) / 2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite, calibrate lambda for each case, and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Happy path\n        {'S0': 100.0, 'r': 0.02, 'q': 0.0, 'T': 0.75, 'sigma': 0.20,\n         'mu_J': -0.10, 'sigma_J': 0.25, 'strikes': [105.0, 110.0, 120.0],\n         'lambda_true': 0.60},\n        # Case B: Boundary case, no jumps\n        {'S0': 100.0, 'r': 0.01, 'q': 0.0, 'T': 1.00, 'sigma': 0.15,\n         'mu_J': -0.20, 'sigma_J': 0.30, 'strikes': [102.0, 110.0, 125.0],\n         'lambda_true': 0.00},\n        # Case C: Higher jump activity\n        {'S0': 100.0, 'r': 0.03, 'q': 0.0, 'T': 2.00, 'sigma': 0.18,\n         'mu_J': 0.05, 'sigma_J': 0.20, 'strikes': [105.0, 115.0, 130.0, 150.0],\n         'lambda_true': 1.20}\n    ]\n    \n    calibrated_lambdas = []\n    \n    for case in test_cases:\n        # 1. Generate synthetic market prices using the true lambda\n        market_prices = [\n            merton_price(case['lambda_true'], case['S0'], K, case['r'], case['q'], case['T'],\n                         case['sigma'], case['mu_J'], case['sigma_J'])\n            for K in case['strikes']\n        ]\n\n        # 2. Define objective function for this case, capturing all parameters except lambda\n        obj_func = lambda l: objective_function(\n            l, case['S0'], case['r'], case['q'], case['T'], case['sigma'],\n            case['mu_J'], case['sigma_J'], case['strikes'], market_prices\n        )\n\n        # 3. Run Golden-Section Search to find the calibrated lambda\n        lambda_hat = golden_section_search(obj_func, a=0.0, b=3.0, tol=1e-6, max_iter=200)\n        calibrated_lambdas.append(lambda_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{l:.6f}' for l in calibrated_lambdas])}]\")\n\nsolve()\n```"
        }
    ]
}