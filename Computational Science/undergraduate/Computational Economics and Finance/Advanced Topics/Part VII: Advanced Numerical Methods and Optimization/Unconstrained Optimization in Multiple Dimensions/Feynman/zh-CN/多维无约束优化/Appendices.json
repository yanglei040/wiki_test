{
    "hands_on_practices": [
        {
            "introduction": "我们将从一个经典的商业决策问题开始。这个练习将多变量微积分应用于实际的利润最大化场景：如何最优地分配广告预算。通过这个练习，你将学习如何将一个商业问题转化为一个数学优化模型，并利用一阶条件求解最优解 。",
            "id": "2445369",
            "problem": "一家公司将其广告支出分配到三个数字平台：谷歌（Google）、脸书（Facebook）和推特（Twitter）。设支出分别为 $x_G$、$x_F$ 和 $x_T$，单位均为美元。该公司的销售量函数由下式给出\n$$\nQ(x_G, x_F, x_T) = d + \\beta_G \\ln\\!\\big(1 + \\alpha_G x_G\\big) + \\beta_F \\ln\\!\\big(1 + \\alpha_F x_F\\big) + \\beta_T \\ln\\!\\big(1 + \\alpha_T x_T\\big),\n$$\n定义在域 $x_G > -1/\\alpha_G$、$x_F > -1/\\alpha_F$ 和 $x_T > -1/\\alpha_T$ 上，以确保对数函数有定义。公司每售出一单位产品，可获得固定的单位边际利润 $m$ 美元，并将广告支出作为成本。该公司的利润函数为\n$$\n\\pi(x_G, x_F, x_T) = m \\, Q(x_G, x_F, x_T) - \\big(x_G + x_F + x_T\\big).\n$$\n假设参数为 $d = 2000$，$m = 50$，$\\beta_G = 100$，$\\beta_F = 120$，$\\beta_T = 80$，$\\alpha_G = 0.002$，$\\alpha_F = 0.0015$ 和 $\\alpha_T = 0.001$。求 $\\pi(x_G, x_F, x_T)$ 在其定义域上的唯一无约束全局最大化点 $\\big(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast}\\big)$。请用精确值表示您的答案。最终答案中不要包含单位。",
            "solution": "对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n- 广告支出变量：$x_G$, $x_F$, $x_T$。\n- 销售量函数：$Q(x_G, x_F, x_T) = d + \\beta_G \\ln(1 + \\alpha_G x_G) + \\beta_F \\ln(1 + \\alpha_F x_F) + \\beta_T \\ln(1 + \\alpha_T x_T)$。\n- 定义域：$x_G > -1/\\alpha_G$, $x_F > -1/\\alpha_F$, $x_T > -1/\\alpha_T$。\n- 利润函数：$\\pi(x_G, x_F, x_T) = m \\, Q(x_G, x_F, x_T) - (x_G + x_F + x_T)$。\n- 参数：$d = 2000$, $m = 50$, $\\beta_G = 100$, $\\beta_F = 120$, $\\beta_T = 80$, $\\alpha_G = 0.002$, $\\alpha_F = 0.0015$, $\\alpha_T = 0.001$。\n- 目标：求利润函数的唯一无约束全局最大化点 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast})$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据**：该问题采用了对数响应模型来描述广告效应，这是经济学和市场营销科学中一种标准的、有经验支持的公式。利润函数被正确地定义为总收入（边际利润乘以数量）减去总成本。该问题具有科学依据。\n- **适定性**：该问题要求解一个函数的全局最大化点。必须验证解的存在性和唯一性。利润函数在其定义域上是二阶可微的。对其海森矩阵（Hessian matrix）的分析将确定其凹性。由于广告效应是可分离的，$\\pi$ 的海森矩阵是一个对角矩阵。其对角元素为 $\\frac{\\partial^2 \\pi}{\\partial x_i^2} = -m \\frac{\\beta_i \\alpha_i^2}{(1+\\alpha_i x_i)^2}$，其中 $i \\in \\{G, F, T\\}$。由于所有参数 $m, \\beta_i, \\alpha_i$ 均为正数，且定义域确保 $1+\\alpha_i x_i > 0$，因此海森矩阵的所有对角元素都严格为负。对角元素严格为负的对角矩阵是负定的。在一个凸定义域上，海森矩阵负定的函数是严格凹函数。严格凹函数的临界点是唯一的全局最大值点。因此，该问题是适定的。\n- **客观性和独立性**：问题陈述语言清晰、无歧义。所有必需的参数和函数形式都已提供。该问题是独立且客观的。\n\n**步骤 3：结论与行动**\n问题有效。将推导求解。\n\n目标是找到使利润函数 $\\pi(x_G, x_F, x_T)$ 最大化的 $x_G$、$x_F$ 和 $x_T$ 的值。利润函数由下式给出：\n$$\n\\pi(x_G, x_F, x_T) = m \\left( d + \\beta_G \\ln(1 + \\alpha_G x_G) + \\beta_F \\ln(1 + \\alpha_F x_F) + \\beta_T \\ln(1 + \\alpha_T x_T) \\right) - (x_G + x_F + x_T)\n$$\n为了找到该函数的无约束极值，我们必须应用一阶必要条件，即函数在临界点 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast})$ 的梯度必须为零向量。也就是说，$\\nabla \\pi(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast}) = \\mathbf{0}$。我们计算 $\\pi$ 对每个变量的偏导数：\n$$\n\\frac{\\partial \\pi}{\\partial x_G} = m \\frac{\\beta_G \\alpha_G}{1 + \\alpha_G x_G} - 1\n$$\n$$\n\\frac{\\partial \\pi}{\\partial x_F} = m \\frac{\\beta_F \\alpha_F}{1 + \\alpha_F x_F} - 1\n$$\n$$\n\\frac{\\partial \\pi}{\\partial x_T} = m \\frac{\\beta_T \\alpha_T}{1 + \\alpha_T x_T} - 1\n$$\n将这些偏导数设为零，我们得到一个由三个独立方程组成的方程组：\n$$\nm \\frac{\\beta_G \\alpha_G}{1 + \\alpha_G x_G^{\\ast}} - 1 = 0 \\implies 1 + \\alpha_G x_G^{\\ast} = m \\beta_G \\alpha_G \\implies x_G^{\\ast} = \\frac{m \\beta_G \\alpha_G - 1}{\\alpha_G}\n$$\n$$\nm \\frac{\\beta_F \\alpha_F}{1 + \\alpha_F x_F^{\\ast}} - 1 = 0 \\implies 1 + \\alpha_F x_F^{\\ast} = m \\beta_F \\alpha_F \\implies x_F^{\\ast} = \\frac{m \\beta_F \\alpha_F - 1}{\\alpha_F}\n$$\n$$\nm \\frac{\\beta_T \\alpha_T}{1 + \\alpha_T x_T^{\\ast}} - 1 = 0 \\implies 1 + \\alpha_T x_T^{\\ast} = m \\beta_T \\alpha_T \\implies x_T^{\\ast} = \\frac{m \\beta_T \\alpha_T - 1}{\\alpha_T}\n$$\n这就得出了唯一的临界点。为了确认这是一个全局最大值点，我们检验二阶充分条件。我们计算函数 $\\pi$ 的海森矩阵 $H$。二阶偏导数如下：\n$$\n\\frac{\\partial^2 \\pi}{\\partial x_G^2} = -m \\frac{\\beta_G \\alpha_G^2}{(1 + \\alpha_G x_G)^2}\n$$\n$$\n\\frac{\\partial^2 \\pi}{\\partial x_F^2} = -m \\frac{\\beta_F \\alpha_F^2}{(1 + \\alpha_F x_F)^2}\n$$\n$$\n\\frac{\\partial^2 \\pi}{\\partial x_T^2} = -m \\frac{\\beta_T \\alpha_T^2}{(1 + \\alpha_T x_T)^2}\n$$\n所有的混合偏导数都为零，例如 $\\frac{\\partial^2 \\pi}{\\partial x_G \\partial x_F} = 0$。因此，海森矩阵是一个对角矩阵：\n$$\nH = \\begin{pmatrix}\n-m \\frac{\\beta_G \\alpha_G^2}{(1 + \\alpha_G x_G)^2} & 0 & 0 \\\\\n0 & -m \\frac{\\beta_F \\alpha_F^2}{(1 + \\alpha_F x_F)^2} & 0 \\\\\n0 & 0 & -m \\frac{\\beta_T \\alpha_T^2}{(1 + \\alpha_T x_T)^2}\n\\end{pmatrix}\n$$\n鉴于所有参数（$m, \\beta_i, \\alpha_i$）均为正数，并且函数的定义域保证了 $1 + \\alpha_i x_i > 0$，所以平方项 $(1 + \\alpha_i x_i)^2$ 恒为正。因此，对于定义域中的所有点 $(x_G, x_F, x_T)$，海森矩阵的所有三个对角元素都严格为负。具有此性质的矩阵是负定的。\n在其整个凸定义域上海森矩阵为负定的函数是严格凹函数。严格凹函数的任何临界点都是其唯一的全局最大值点。因此，我们找到的临界点 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast})$ 是利润函数 $\\pi$ 的唯一全局最大化点。\n\n现在我们将给定的数值代入 $x_G^{\\ast}$、$x_F^{\\ast}$ 和 $x_T^{\\ast}$ 的表达式中。\n参数为：$m = 50$, $\\beta_G = 100$, $\\alpha_G = 0.002$, $\\beta_F = 120$, $\\alpha_F = 0.0015$, $\\beta_T = 80$, $\\alpha_T = 0.001$。\n\n对于 $x_G^{\\ast}$:\n$$\nx_G^{\\ast} = \\frac{50 \\times 100 \\times 0.002 - 1}{0.002} = \\frac{10 - 1}{0.002} = \\frac{9}{0.002} = 4500\n$$\n\n对于 $x_F^{\\ast}$:\n$$\nx_F^{\\ast} = \\frac{50 \\times 120 \\times 0.0015 - 1}{0.0015} = \\frac{9 - 1}{0.0015} = \\frac{8}{0.0015} = \\frac{8}{15/10000} = \\frac{80000}{15} = \\frac{16000}{3}\n$$\n\n对于 $x_T^{\\ast}$:\n$$\nx_T^{\\ast} = \\frac{50 \\times 80 \\times 0.001 - 1}{0.001} = \\frac{4 - 1}{0.001} = \\frac{3}{0.001} = 3000\n$$\n\n最优广告支出为 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast}) = \\left(4500, \\frac{16000}{3}, 3000\\right)$。这些值均为正数，且位于函数定义域内，证实这是一个有效的经济学解。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 4500 & \\frac{16000}{3} & 3000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "接下来，我们将提升难度，探讨优化在策略互动中的应用。我们不再是优化一个单一的目标函数，而是使用优化工具来寻找竞争市场中的平衡状态，即均衡点。这个关于伯特兰 (Bertrand) 竞争的练习展示了每个公司的最优决策如何依赖于竞争对手的决策，从而引出了经济学中的核心概念——纳什均衡 (Nash equilibrium)。",
            "id": "2445317",
            "problem": "两家由 $i \\in \\{1,2\\}$ 索引的厂商生产差异化产品，并同时选择价格 $p_1$ 和 $p_2$，其中 $p_1, p_2 \\in \\mathbb{R}$。市场需求系统是线性的，由以下公式给出：\n$$\nq_1 = 100 - 3 p_1 + p_2, \\quad q_2 = 100 - 3 p_2 + p_1,\n$$\n其中，当收取的价格向量为 $(p_1, p_2)$ 时，$q_i$ 是对厂商 $i$ 产品的需求量。厂商1的固定边际成本为 $c_1 = 10$，厂商2的固定边际成本为 $c_2 = 20$。每个厂商的利润为：\n$$\n\\pi_1(p_1,p_2) = (p_1 - c_1) q_1, \\quad \\pi_2(p_1,p_2) = (p_2 - c_2) q_2.\n$$\n一个纯策略纳什均衡是一个价格向量 $(p_1^\\star, p_2^\\star)$，使得 $p_1^\\star$ 在 $p_1 \\in \\mathbb{R}$ 上最大化 $\\pi_1(p_1, p_2^\\star)$，并且 $p_2^\\star$ 在 $p_2 \\in \\mathbb{R}$ 上最大化 $\\pi_2(p_1^\\star, p_2)$。\n\n找出唯一的纯策略纳什均衡价格向量 $(p_1^\\star, p_2^\\star)$。请提供精确值（不要四舍五入）。最终答案必须是价格的有序对。",
            "solution": "该问题要求在一个具有差异化产品和非对称成本的伯特兰竞争模型中，找到唯一的纯策略纳什均衡。问题陈述需要进行验证。\n\n**第1步：提取已知条件**\n提供了以下信息：\n-   厂商：$i \\in \\{1,2\\}$\n-   价格：$p_1, p_2 \\in \\mathbb{R}$\n-   需求函数：$q_1 = 100 - 3 p_1 + p_2$ 和 $q_2 = 100 - 3 p_2 + p_1$。\n-   边际成本：$c_1 = 10$ 和 $c_2 = 20$。\n-   利润函数：$\\pi_1(p_1,p_2) = (p_1 - c_1) q_1$ 和 $\\pi_2(p_1,p_2) = (p_2 - c_2) q_2$。\n-   纳什均衡的定义：一个价格向量 $(p_1^\\star, p_2^\\star)$，其中 $p_1^\\star$ 最大化 $\\pi_1(p_1, p_2^\\star)$ 且 $p_2^\\star$ 最大化 $\\pi_2(p_1^\\star, p_2)$。\n\n**第2步：使用提取的已知条件进行验证**\n该问题是微观经济博弈论中的一个标准练习。\n-   **科学依据**：该模型是经典的具有差异化产品的伯特兰双寡头模型，是产业组织理论中的一个基本概念。它在科学上和数学上都是合理的。\n-   **适定性**：该问题是适定的。目标陈述清晰，并且提供了所有必要的函数和参数以找到唯一解，利润函数的严格凹性也表明了这一点。\n-   **客观性**：该问题使用精确的数学语言陈述，没有主观或模糊的术语。\n-   **完整性**：该问题是自洽的，提供了所有必要的信息，没有矛盾之处。\n\n**第3步：结论与行动**\n该问题有效。将构建解答。\n\n每个厂商 $i$ 的利润函数由 $\\pi_i = (p_i - c_i) q_i$ 给出。将给定的需求函数和成本参数代入，得到厂商1和厂商2的利润函数。\n\n对于厂商1：\n$$\n\\pi_1(p_1, p_2) = (p_1 - 10)(100 - 3p_1 + p_2)\n$$\n对于厂商2：\n$$\n\\pi_2(p_1, p_2) = (p_2 - 20)(100 - 3p_2 + p_1)\n$$\n在纳什均衡中，每个厂商选择其价格以最大化自身利润，并将另一家厂商的价格视为给定。这个无约束优化问题通过为每个厂商找到一阶条件来解决。\n\n首先，我们求厂商1的最优反应函数。我们将 $\\pi_1$ 对 $p_1$ 求导，并令其导数为零。\n$$\n\\frac{\\partial \\pi_1}{\\partial p_1} = (1)(100 - 3p_1 + p_2) + (p_1 - 10)(-3) = 0\n$$\n$$\n100 - 3p_1 + p_2 - 3p_1 + 30 = 0\n$$\n$$\n130 - 6p_1 + p_2 = 0\n$$\n解出 $p_1$ 得到厂商1的最优反应函数 $p_1(p_2)$：\n$$\np_1 = \\frac{130 + p_2}{6}\n$$\n为确认这是一个最大值，我们检查二阶条件：\n$$\n\\frac{\\partial^2 \\pi_1}{\\partial p_1^2} = -6 < 0\n$$\n利润函数关于 $p_1$ 是严格凹的，因此一阶条件对于任何给定的 $p_2$ 确定了唯一的利润最大化价格。\n\n接下来，我们求厂商2的最优反应函数。我们将 $\\pi_2$ 对 $p_2$ 求导，并令其导数为零。\n$$\n\\frac{\\partial \\pi_2}{\\partial p_2} = (1)(100 - 3p_2 + p_1) + (p_2 - 20)(-3) = 0\n$$\n$$\n100 - 3p_2 + p_1 - 3p_2 + 60 = 0\n$$\n$$\n160 - 6p_2 + p_1 = 0\n$$\n解出 $p_2$ 得到厂商2的最优反应函数 $p_2(p_1)$：\n$$\np_2 = \\frac{160 + p_1}{6}\n$$\n二阶条件确认了这是一个最大值：\n$$\n\\frac{\\partial^2 \\pi_2}{\\partial p_2^2} = -6 < 0\n$$\n纳什均衡是同时满足两个最优反应函数的价格向量 $(p_1^\\star, p_2^\\star)$。我们必须解以下线性方程组：\n$$\n\\begin{cases}\np_1^\\star = \\frac{130 + p_2^\\star}{6} \\\\\np_2^\\star = \\frac{160 + p_1^\\star}{6}\n\\end{cases}\n$$\n将第二个方程代入第一个方程：\n$$\np_1^\\star = \\frac{1}{6} \\left( 130 + \\frac{160 + p_1^\\star}{6} \\right)\n$$\n两边乘以 $6$：\n$$\n6p_1^\\star = 130 + \\frac{160 + p_1^\\star}{6}\n$$\n再次乘以 $6$ 以消去分数：\n$$\n36p_1^\\star = 6(130) + 160 + p_1^\\star\n$$\n$$\n36p_1^\\star = 780 + 160 + p_1^\\star\n$$\n$$\n35p_1^\\star = 940\n$$\n$$\np_1^\\star = \\frac{940}{35} = \\frac{188 \\times 5}{7 \\times 5} = \\frac{188}{7}\n$$\n现在，将 $p_1^\\star$ 的值代入厂商2的最优反应函数中求 $p_2^\\star$：\n$$\np_2^\\star = \\frac{160 + p_1^\\star}{6} = \\frac{1}{6} \\left( 160 + \\frac{188}{7} \\right)\n$$\n$$\np_2^\\star = \\frac{1}{6} \\left( \\frac{160 \\times 7}{7} + \\frac{188}{7} \\right) = \\frac{1}{6} \\left( \\frac{1120 + 188}{7} \\right)\n$$\n$$\np_2^\\star = \\frac{1}{6} \\left( \\frac{1308}{7} \\right) = \\frac{1308}{42}\n$$\n分子和分母同除以 $6$：\n$$\np_2^\\star = \\frac{1308 \\div 6}{42 \\div 6} = \\frac{218}{7}\n$$\n因此，唯一的纯策略纳什均衡价格向量是 $(p_1^\\star, p_2^\\star) = (\\frac{188}{7}, \\frac{218}{7})$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{188}{7} & \\frac{218}{7} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "最后，我们将从解析方法（“纸笔”方法）过渡到驱动现代经济学和金融学的计算方法。现实世界中的大多数优化问题都过于复杂，无法直接求得解析解。这个练习将让你亲手实现梯度下降算法——一种通过迭代数值计算寻找最优解的基本方法，这是解决复杂问题的强大工具。",
            "id": "2445371",
            "problem": "给定多维光滑目标函数，这些函数源于计算经济学和金融学。对于每个指定的测试用例，构建一个迭代无约束最小化方法，该方法根据 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$ 生成序列 $\\{\\mathbf{x}_k\\}_{k \\ge 0}$，其中下降方向为 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$。在每次迭代 $k$ 中，步长 $\\alpha_k$ 必须从几何序列 $\\{\\alpha_0 \\rho^m: m \\in \\{0,1,2,\\dots\\}\\}$ 中选择，以满足 Armijo 充分下降条件。其中 $\\alpha_0 \\in (0,\\infty)$ 和 $\\rho \\in (0,1)$ 是固定且对所有测试用例通用的参数：\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k,\n$$\n其中 $c \\in (0,1)$ 是一个对所有测试用例通用的固定常数。从给定的起始点开始初始化，当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或当 $k$ 达到指定的最大迭代次数时终止。所有计算都在实数域上进行。不使用角度。不涉及物理单位。\n\n对所有测试用例使用以下固定参数：$\\alpha_0 = 1$，$\\rho = \\tfrac{1}{2}$，$c = 10^{-4}$，$\\varepsilon = 10^{-8}$，以及 $\\text{max\\_iter} = 10000$。\n\n测试套件（所有矩阵和向量均明确写出）：\n\n- 测试用例 1（金融学中的均值-方差二次型）：\n  - 决策变量 $\\mathbf{w} \\in \\mathbb{R}^2$。\n  - 目标函数\n    $$\n    f_1(\\mathbf{w}) = \\tfrac{1}{2}\\,\\mathbf{w}^\\top \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}^\\top \\mathbf{w},\n    \\quad\n    \\boldsymbol{\\Sigma} =\n    \\begin{bmatrix}\n    2 & 0.8 \\\\\n    0.8 & 1.5\n    \\end{bmatrix},\n    \\quad\n    \\boldsymbol{\\mu} =\n    \\begin{bmatrix}\n    0.5 \\\\\n    0.3\n    \\end{bmatrix}.\n    $$\n  - 初始条件 $\\mathbf{w}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n- 测试用例 2（病态二次型）：\n  - 决策变量 $\\mathbf{x} \\in \\mathbb{R}^2$。\n  - 目标函数\n    $\n    f_2(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x},\n    \\quad\n    \\mathbf{H} =\n    \\begin{bmatrix}\n    1000 & 0 \\\\\n    0 & 1\n    \\end{bmatrix},\n    \\quad\n    \\mathbf{b} =\n    \\begin{bmatrix}\n    1 \\\\\n    1\n    \\end{bmatrix}.\n    $\n  - 初始条件 $\\mathbf{x}_0 = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$。\n\n- 测试用例 3（逻辑斯蒂链接的二元选择负对数似然，非线性可分）：\n  - 参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$。\n  - 数据矩阵\n    $\n    \\mathbf{X} =\n    \\begin{bmatrix}\n    1 & -2 \\\\\n    1 & -1 \\\\\n    1 & 1 \\\\\n    1 & 2\n    \\end{bmatrix},\n    $\n    标签向量\n    $\n    \\mathbf{y} =\n    \\begin{bmatrix}\n    -1 \\\\\n    1 \\\\\n    -1 \\\\\n    1\n    \\end{bmatrix}.\n    $\n  - 目标函数\n    $$\n    f_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} \\log\\!\\big(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big),\n    $$\n    其中 $\\mathbf{x}_i^\\top$ 是 $\\mathbf{X}$ 的第 $i$ 行，$y_i$ 是 $\\mathbf{y}$ 的第 $i$ 个元素。\n  - 初始条件 $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n对于每个测试用例，使用上述通用参数运行迭代最小化，并报告终止时的最小化目标值 $f(\\mathbf{x}^\\star)$。你的程序不得读取任何输入。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内容为按测试用例顺序排列的结果，每个条目四舍五入到六位小数，例如 $[0.123456,1.234567,2.345678]$。",
            "solution": "问题陈述已经过严格评估，并被确定为有效。它在无约束优化领域提出了一组清晰、数学上合理且适定的任务。目标函数是计算经济学和金融学中的标准示例，所有必需的参数和初始条件都已明确提供，没有歧义或矛盾。所指定的算法，即基于 Armijo 条件并采用回溯线搜索的梯度下降法，是解决此类问题的基础且合适的方法。我们现在将进行形式化的推导和求解。\n\n问题的核心是实现最速下降法来最小化一个光滑函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$。这是一个迭代算法，它使用以下更新规则生成一个点序列 $\\{\\mathbf{x}_k\\}_{k \\ge 0}$：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\n方向 $\\mathbf{d}_k$ 被选为目标函数在当前迭代点 $\\mathbf{x}_k$ 处的负梯度，即最速下降方向：\n$$\n\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\n$$\n步长 $\\alpha_k > 0$ 通过回溯线搜索过程确定，以确保目标函数值有充分的下降。从初始猜测 $\\alpha = \\alpha_0$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$ 进行缩减，直到满足 Armijo 条件：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n$$\n其中 $c \\in (0,1)$ 是一个小常数。对于所有测试用例，给定参数固定为 $\\alpha_0 = 1$, $\\rho = \\frac{1}{2}$ 和 $c = 10^{-4}$。\n\n当梯度的欧几里得范数低于指定的容差 $\\varepsilon = 10^{-8}$ 时，或者当迭代次数 $k$ 达到最大限制 $\\text{max\\_iter} = 10000$ 时，算法终止，并将当前点 $\\mathbf{x}_k$ 报告为近似最小化点 $\\mathbf{x}^\\star$。\n\n我们现在将此通用过程应用于三个指定的测试用例。\n\n**测试用例 1：均值-方差二次型**\n目标函数是投资组合优化中的一个标准二次型：\n$$\nf_1(\\mathbf{w}) = \\tfrac{1}{2}\\,\\mathbf{w}^\\top \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}^\\top \\mathbf{w}\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^2$, $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 2 & 0.8 \\\\ 0.8 & 1.5 \\end{bmatrix}$，以及 $\\boldsymbol{\\mu} = \\begin{bmatrix} 0.5 \\\\ 0.3 \\end{bmatrix}$。对于对称矩阵 $\\mathbf{A}$，一般二次函数 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$ 的梯度是 $\\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} - \\mathbf{b}$。因此，$f_1$ 的梯度为：\n$$\n\\nabla f_1(\\mathbf{w}) = \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}\n$$\n$f_1$ 的海森矩阵是 $\\nabla^2 f_1(\\mathbf{w}) = \\boldsymbol{\\Sigma}$。矩阵 $\\boldsymbol{\\Sigma}$ 是对称的，其特征值约为 $2.55$ 和 $0.95$，两者均为正数。因此，$\\boldsymbol{\\Sigma}$ 是正定的，这意味着 $f_1$ 是严格凸的，并拥有唯一的全局最小值。最速下降算法保证收敛到该最小值。算法从 $\\mathbf{w}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ 开始初始化。\n\n**测试用例 2：病态二次型**\n目标函数是另一个二次型：\n$$\nf_2(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$, $\\mathbf{H} = \\begin{bmatrix} 1000 & 0 \\\\ 0 & 1 \\end{bmatrix}$，以及 $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。梯度为：\n$$\n\\nabla f_2(\\mathbf{x}) = \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}\n$$\n海森矩阵 $\\mathbf{H}$ 是正定的，因为其特征值为 $1000$ 和 $1$。因此，$f_2$ 是严格凸的，并有唯一的最小值。$\\mathbf{H}$ 的条件数是其最大特征值与最小特征值之比，即 $1000/1 = 1000$。这个高条件数意味着 $f_2$ 的水平集是高度拉长的椭圆，这通常会减慢最速下降法的收敛速度。算法从 $\\mathbf{x}_0 = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$ 开始。\n\n**测试用例 3：二元选择的负对数似然**\n目标函数是逻辑斯蒂回归模型的负对数似然：\n$$\nf_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} \\log\\!\\big(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big)\n$$\n其中 $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$。为求梯度，我们对 $\\boldsymbol{\\theta}$ 的一个分量 $\\theta_j$ 求导：\n$$\n\\frac{\\partial f_3}{\\partial \\theta_j} = \\sum_{i=1}^{4} \\frac{1}{1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})} \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\right)\n$$\n$$\n= \\sum_{i=1}^{4} \\frac{\\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})}{1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})} \\cdot (-y_i x_{ij})\n$$\n令 $\\sigma(z) = 1/(1+e^{-z})$ 为逻辑斯蒂 sigmoid 函数。表达式 $\\frac{\\exp(-z)}{1+\\exp(-z)}$ 可重写为 $\\frac{1}{e^z+1} = \\sigma(-z)$。\n因此，梯度向量 $\\nabla f_3(\\boldsymbol{\\theta})$ 的分量为：\n$$\n[\\nabla f_3(\\boldsymbol{\\theta})]_j = \\sum_{i=1}^{4} \\sigma(-y_i \\mathbf{x}_i^\\top \\boldsymbol{\\theta}) (-y_i x_{ij})\n$$\n这可以紧凑地写成向量形式。令 $\\mathbf{p}$ 是一个向量，其元素为 $p_i = \\sigma(y_i \\mathbf{x}_i^\\top \\boldsymbol{\\theta})$。使用恒等式 $\\sigma(-z) = 1 - \\sigma(z)$，梯度为：\n$$\n\\nabla f_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} (1-p_i) (-y_i \\mathbf{x}_i) = \\sum_{i=1}^{4} y_i(p_i-1) \\mathbf{x}_i = \\mathbf{X}^\\top (\\mathbf{y} \\odot (\\mathbf{p} - \\mathbf{1}))\n$$\n其中 $\\odot$ 表示逐元素乘法，$\\mathbf{1}$ 是一个全为一的向量。该函数的海森矩阵可以被证明是半正定的。由于数据矩阵 $\\mathbf{X}$ 具有线性无关的列（满列秩），海森矩阵实际上是正定的，从而确保 $f_3$ 是严格凸的，并有唯一的最小化点。算法从原点 $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 开始初始化。为了数值稳定性，$\\log(1+e^z)$ 的计算使用了 log-sum-exp 模式。\n\n该算法根据这些推导在软件中实现，以计算每种情况下的最终目标值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit, logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained optimization problems defined in the test suite\n    using the gradient descent method with backtracking line search.\n    \"\"\"\n    \n    # Define common parameters for the optimization algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    max_iter = 10000\n\n    def gradient_descent(f, grad_f, x0):\n        \"\"\"\n        Generic implementation of gradient descent with backtracking line search.\n\n        Args:\n            f (callable): The objective function.\n            grad_f (callable): The gradient of the objective function.\n            x0 (np.ndarray): The initial starting point.\n\n        Returns:\n            float: The minimized objective function value at termination.\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        \n        for k in range(max_iter):\n            grad = grad_f(x)\n            grad_norm = np.linalg.norm(grad)\n\n            # Termination condition: norm of the gradient is small enough\n            if grad_norm <= epsilon:\n                break\n            \n            d = -grad  # Steepest descent direction\n            \n            # Backtracking line search to find step length alpha\n            alpha = alpha0\n            fx = f(x)\n            grad_dot_d = np.dot(grad, d)\n            \n            # Armijo condition check\n            while f(x + alpha * d) > fx + c * alpha * grad_dot_d:\n                alpha = rho * alpha\n            \n            # Update the iterate\n            x = x + alpha * d\n            \n        return f(x)\n\n    results = []\n\n    # Test Case 1: Mean-variance quadratic\n    Sigma = np.array([[2.0, 0.8], [0.8, 1.5]])\n    mu = np.array([0.5, 0.3])\n    w0 = np.array([1.0, 1.0])\n    \n    def f1(w):\n        return 0.5 * w.T @ Sigma @ w - mu.T @ w\n    \n    def grad_f1(w):\n        return Sigma @ w - mu\n        \n    result1 = gradient_descent(f1, grad_f1, w0)\n    results.append(result1)\n\n    # Test Case 2: Ill-conditioned quadratic\n    H = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    b = np.array([1.0, 1.0])\n    x0_2 = np.array([10.0, 10.0])\n\n    def f2(x):\n        return 0.5 * x.T @ H @ x - b.T @ x\n    \n    def grad_f2(x):\n        return H @ x - b\n        \n    result2 = gradient_descent(f2, grad_f2, x0_2)\n    results.append(result2)\n\n    # Test Case 3: Negative log-likelihood for binary choice\n    X = np.array([[1.0, -2.0], [1.0, -1.0], [1.0, 1.0], [1.0, 2.0]])\n    y = np.array([-1.0, 1.0, -1.0, 1.0])\n    theta0 = np.array([0.0, 0.0])\n\n    def f3(theta):\n        # log(1+exp(z)) is computed robustly as log(exp(0)+exp(z))\n        z = -y * (X @ theta)\n        return np.sum(logsumexp(np.vstack((np.zeros_like(z), z)), axis=0))\n\n    def grad_f3(theta):\n        # Gradient of sum_i log(1+exp(-y_i*x_i'theta)) is sum_i y_i(p_i-1)x_i\n        # where p_i = sigmoid(y_i*x_i'theta)\n        h = y * (X @ theta)\n        p = expit(h)  # Numerically stable sigmoid function\n        # Vectorized gradient calculation\n        return X.T @ (y * (p - 1.0))\n        \n    result3 = gradient_descent(f3, grad_f3, theta0)\n    results.append(result3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}