{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的巩固方式就是付诸实践。本节的第一个练习将带你解决一个经典的商业决策问题：如何分配广告预算以实现利润最大化。这个问题  的巧妙之处在于，其利润函数中的销售额由各个渠道的对数函数相加构成，这意味着不同广告渠道的效果是相互独立的。这种函数的可分离性（separability）让我们可以独立地对每个变量求导，从而简化了优化过程，是应用多维优化一阶条件的绝佳入门练习。",
            "id": "2445369",
            "problem": "一家公司将广告支出分配到三个数字平台：Google、Facebook和Twitter。设支出分别为 $x_G$、$x_F$ 和 $x_T$，单位均为美元。该公司的销量函数由下式给出：\n$$\nQ(x_G, x_F, x_T) = d + \\beta_G \\ln\\!\\big(1 + \\alpha_G x_G\\big) + \\beta_F \\ln\\!\\big(1 + \\alpha_F x_F\\big) + \\beta_T \\ln\\!\\big(1 + \\alpha_T x_T\\big),\n$$\n其定义域为 $x_G  -1/\\alpha_G$、$x_F  -1/\\alpha_F$ 和 $x_T  -1/\\alpha_T$，以确保对数函数有明确定义。公司每售出一件产品可获得固定的单位边际利润 $m$ 美元，并将广告支出作为成本。公司的利润函数为\n$$\n\\pi(x_G, x_F, x_T) = m \\, Q(x_G, x_F, x_T) - \\big(x_G + x_F + x_T\\big).\n$$\n假设参数为 $d = 2000$、$m = 50$、$\\beta_G = 100$、$\\beta_F = 120$、$\\beta_T = 80$、$\\alpha_G = 0.002$、$\\alpha_F = 0.0015$ 和 $\\alpha_T = 0.001$。求 $\\pi(x_G, x_F, x_T)$ 在其定义域上的唯一无约束全局最大值点 $\\big(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast}\\big)$。请用精确值表示答案。最终答案中不要包含单位。",
            "solution": "对问题陈述进行验证。\n\n**第一步：提取已知条件**\n- 广告支出变量：$x_G$、$x_F$、$x_T$。\n- 销量函数：$Q(x_G, x_F, x_T) = d + \\beta_G \\ln(1 + \\alpha_G x_G) + \\beta_F \\ln(1 + \\alpha_F x_F) + \\beta_T \\ln(1 + \\alpha_T x_T)$。\n- 定义域：$x_G  -1/\\alpha_G$、$x_F  -1/\\alpha_F$、$x_T  -1/\\alpha_T$。\n- 利润函数：$\\pi(x_G, x_F, x_T) = m \\, Q(x_G, x_F, x_T) - (x_G + x_F + x_T)$。\n- 参数：$d = 2000$、$m = 50$、$\\beta_G = 100$、$\\beta_F = 120$、$\\beta_T = 80$、$\\alpha_G = 0.002$、$\\alpha_F = 0.0015$、$\\alpha_T = 0.001$。\n- 目标：求利润函数的唯一无约束全局最大值点 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast})$。\n\n**第二步：使用提取的已知条件进行验证**\n- **科学依据**：该问题采用对数响应模型来描述广告效果，这是经济学和营销科学中一种标准的、有实证支持的公式。利润函数被正确地设定为总收入（边际利润乘以数量）减去总成本。该问题具有科学依据。\n- **适定性**：问题要求解一个函数的全局最大值点。必须验证其存在性和唯一性。利润函数在其定义域上是二次可微的。对其Hessian矩阵的分析将确定其凹性。$\\pi$ 的Hessian矩阵是一个对角矩阵，因为广告效果是可分的。对角线元素为 $\\frac{\\partial^2 \\pi}{\\partial x_i^2} = -m \\frac{\\beta_i \\alpha_i^2}{(1+\\alpha_i x_i)^2}$，其中 $i \\in \\{G, F, T\\}$。由于所有参数 $m, \\beta_i, \\alpha_i$ 均为正，且定义域确保 $1+\\alpha_i x_i  0$，因此Hessian矩阵的所有对角线元素均为严格负数。对角线元素均为严格负数的对角矩阵是负定的。在一个凸域上，Hessian矩阵为负定的函数是严格凹函数。严格凹函数的临界点是其唯一的全局最大值点。因此，该问题是适定的。\n- **客观性与独立性**：问题陈述语言清晰明确。所有必需的参数和函数形式都已提供。问题是独立且客观的。\n\n**第三步：结论与行动**\n问题有效。下面将推导解答。\n\n目标是找到使利润函数 $\\pi(x_G, x_F, x_T)$ 最大化的 $x_G$、$x_F$ 和 $x_T$ 的值。利润函数由下式给出：\n$$\n\\pi(x_G, x_F, x_T) = m \\left( d + \\beta_G \\ln(1 + \\alpha_G x_G) + \\beta_F \\ln(1 + \\alpha_F x_F) + \\beta_T \\ln(1 + \\alpha_T x_T) \\right) - (x_G + x_F + x_T)\n$$\n为求此函数的无约束极值，我们必须应用一阶必要条件，即函数在临界点 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast})$ 的梯度必须为零向量。也就是说，$\\nabla \\pi(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast}) = \\mathbf{0}$。我们计算 $\\pi$ 对每个变量的偏导数：\n$$\n\\frac{\\partial \\pi}{\\partial x_G} = m \\frac{\\beta_G \\alpha_G}{1 + \\alpha_G x_G} - 1\n$$\n$$\n\\frac{\\partial \\pi}{\\partial x_F} = m \\frac{\\beta_F \\alpha_F}{1 + \\alpha_F x_F} - 1\n$$\n$$\n\\frac{\\partial \\pi}{\\partial x_T} = m \\frac{\\beta_T \\alpha_T}{1 + \\alpha_T x_T} - 1\n$$\n将这些偏导数设为零，我们得到一个由三个独立方程组成的方程组：\n$$\nm \\frac{\\beta_G \\alpha_G}{1 + \\alpha_G x_G^{\\ast}} - 1 = 0 \\implies 1 + \\alpha_G x_G^{\\ast} = m \\beta_G \\alpha_G \\implies x_G^{\\ast} = \\frac{m \\beta_G \\alpha_G - 1}{\\alpha_G}\n$$\n$$\nm \\frac{\\beta_F \\alpha_F}{1 + \\alpha_F x_F^{\\ast}} - 1 = 0 \\implies 1 + \\alpha_F x_F^{\\ast} = m \\beta_F \\alpha_F \\implies x_F^{\\ast} = \\frac{m \\beta_F \\alpha_F - 1}{\\alpha_F}\n$$\n$$\nm \\frac{\\beta_T \\alpha_T}{1 + \\alpha_T x_T^{\\ast}} - 1 = 0 \\implies 1 + \\alpha_T x_T^{\\ast} = m \\beta_T \\alpha_T \\implies x_T^{\\ast} = \\frac{m \\beta_T \\alpha_T - 1}{\\alpha_T}\n$$\n这就得出了唯一的临界点。为确认这一点是全局最大值点，我们考察二阶充分条件。我们计算函数 $\\pi$ 的Hessian矩阵 $H$。二阶偏导数如下：\n$$\n\\frac{\\partial^2 \\pi}{\\partial x_G^2} = -m \\frac{\\beta_G \\alpha_G^2}{(1 + \\alpha_G x_G)^2}\n$$\n$$\n\\frac{\\partial^2 \\pi}{\\partial x_F^2} = -m \\frac{\\beta_F \\alpha_F^2}{(1 + \\alpha_F x_F)^2}\n$$\n$$\n\\frac{\\partial^2 \\pi}{\\partial x_T^2} = -m \\frac{\\beta_T \\alpha_T^2}{(1 + \\alpha_T x_T)^2}\n$$\n所有的混合偏导数均为零，例如 $\\frac{\\partial^2 \\pi}{\\partial x_G \\partial x_F} = 0$。因此，Hessian矩阵是一个对角矩阵：\n$$\nH = \\begin{pmatrix}\n-m \\frac{\\beta_G \\alpha_G^2}{(1 + \\alpha_G x_G)^2}  0  0 \\\\\n0  -m \\frac{\\beta_F \\alpha_F^2}{(1 + \\alpha_F x_F)^2}  0 \\\\\n0  0  -m \\frac{\\beta_T \\alpha_T^2}{(1 + \\alpha_T x_T)^2}\n\\end{pmatrix}\n$$\n鉴于所有参数（$m, \\beta_i, \\alpha_i$）均为正数，且函数的定义域确保 $1 + \\alpha_i x_i  0$，平方项 $(1 + \\alpha_i x_i)^2$ 总是正的。因此，Hessian矩阵的所有三个对角线元素在定义域内的所有点 $(x_G, x_F, x_T)$ 处均为严格负数。具有此类性质的矩阵是负定的。\n一个在其整个凸定义域上Hessian矩阵为负定的函数是严格凹函数。严格凹函数的任何临界点都是其唯一的全局最大值点。因此，我们找到的临界点 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast})$ 是利润函数 $\\pi$ 的唯一全局最大值点。\n\n现在我们将给定的数值代入 $x_G^{\\ast}$、$x_F^{\\ast}$ 和 $x_T^{\\ast}$ 的表达式中。\n参数为：$m = 50$、$\\beta_G = 100$、$\\alpha_G = 0.002$、$\\beta_F = 120$、$\\alpha_F = 0.0015$、$\\beta_T = 80$、$\\alpha_T = 0.001$。\n\n对于 $x_G^{\\ast}$：\n$$\nx_G^{\\ast} = \\frac{50 \\times 100 \\times 0.002 - 1}{0.002} = \\frac{10 - 1}{0.002} = \\frac{9}{0.002} = 4500\n$$\n\n对于 $x_F^{\\ast}$：\n$$\nx_F^{\\ast} = \\frac{50 \\times 120 \\times 0.0015 - 1}{0.0015} = \\frac{9 - 1}{0.0015} = \\frac{8}{0.0015} = \\frac{8}{15/10000} = \\frac{80000}{15} = \\frac{16000}{3}\n$$\n\n对于 $x_T^{\\ast}$：\n$$\nx_T^{\\ast} = \\frac{50 \\times 80 \\times 0.001 - 1}{0.001} = \\frac{4 - 1}{0.001} = \\frac{3}{0.001} = 3000\n$$\n\n最优广告支出为 $(x_G^{\\ast}, x_F^{\\ast}, x_T^{\\ast}) = \\left(4500, \\frac{16000}{3}, 3000\\right)$。这些值均为正数，且在函数的定义域内，证实这是一个有效的经济学解。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 4500  \\frac{16000}{3}  3000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在掌握了变量可分离问题的求解后，我们将进入一个更贴近现实复杂性的场景。这个练习  模拟了求职者如何分配精力在不同求职渠道上，以最大化期望薪资。与上一个练习不同，该模型的目标函数包含交叉项 $n_F n_I$，这代表了不同求职渠道间的相互影响（协同或替代效应）。这意味着我们不能再孤立地看待每个决策，而必须通过求解一个线性方程组来找到联合最优点，从而深刻理解完整梯度和Hessian矩阵在分析变量相互作用时的重要性。",
            "id": "2445351",
            "problem": "一位求职者在一周内考虑两种申请渠道：正式渠道和非正式渠道。令 $n_F$ 表示通过正式渠道发送的申请强度，$n_I$ 表示通过非正式渠道发送的申请强度。将强度解释为短期内申请数量的连续代理变量，在独立和小到达概率的条件下，所获工作的周期望薪资可以通过在 $(n_F,n_I)$ 处的二阶展开来近似：\n$$\n\\mathbb{E}[S(n_F,n_I)] \\;=\\; \\beta_0 \\;+\\; \\beta_F \\, n_F \\;+\\; \\beta_I \\, n_I \\;-\\; \\frac{1}{2}\\Big( a_F \\, n_F^{2} \\;+\\; 2 a_{FI} \\, n_F n_I \\;+\\; a_I \\, n_I^{2} \\Big),\n$$\n其中 $\\beta_0$、$\\beta_F$、$\\beta_I$、$a_F$、$a_{FI}$ 和 $a_I$ 是参数。假设 $a_F \\gt 0$、$a_I \\gt 0$ 且 $a_F a_I - a_{FI}^{2} \\gt 0$，并且 $\\beta_F \\gt 0$ 和 $\\beta_I \\gt 0$。这些条件确保该近似在 $(n_F,n_I)$ 上是严格凹的，并产生一个唯一的内部最大化解。\n\n确定无约束优化解 $(n_F^{\\star}, n_I^{\\star})$，该解使 $\\mathbb{E}[S(n_F,n_I)]$ 相对于 $n_F$ 和 $n_I$ 最大化。将最终答案表示为单个闭式解析表达式。无需四舍五入。",
            "solution": "该问题要求找到周期望薪资函数的无约束优化解，该函数是关于两个变量的二次函数：通过正式渠道的申请强度 $n_F$ 和通过非正式渠道的申请强度 $n_I$。\n\n首先，需要对问题陈述进行验证。\n\n步骤1：提取已知条件。\n需要最大化的目标函数是：\n$$\n\\mathbb{E}[S(n_F,n_I)] = \\beta_0 + \\beta_F n_F + \\beta_I n_I - \\frac{1}{2}\\Big( a_F n_F^{2} + 2 a_{FI} n_F n_I + a_I n_I^{2} \\Big)\n$$\n优化变量是 $n_F$ 和 $n_I$。\n参数是 $\\beta_0$、$\\beta_F$、$\\beta_I$、$a_F$、$a_{FI}$ 和 $a_I$。\n给定以下条件：\n$a_F  0$\n$a_I  0$\n$a_F a_I - a_{FI}^{2}  0$\n$\\beta_F  0$\n$\\beta_I  0$\n\n步骤2：使用提取的已知条件进行验证。\n该问题在科学上和数学上都是合理的。它提出了一个针对二次函数的标准无约束优化问题。该函数表示一个二阶泰勒近似，这是包括计算经济学在内的许多科学领域中一种常用且有效的技术。这是一个适定的问题；给定的关于参数 $a_F$、$a_I$ 和 $a_{FI}$ 的条件，正是使二次部分的海森矩阵为正定所需的条件，这又使得目标函数是严格凹的，从而保证了唯一的全局最大值。该问题以精确定义的数学术语和约束进行了客观陈述。它是自洽的，并且不违反任何基本原则。\n\n步骤3：结论与行动。\n问题是有效的。将推导求解。\n\n令目标函数表示为 $J(n_F, n_I)$。为找到无约束最大值，我们必须应用一阶必要条件，即函数在最优点 $(n_F^{\\star}, n_I^{\\star})$ 的梯度必须为零向量。\n$J(n_F, n_I)$ 的梯度是其偏导数的向量：$\\nabla J = \\begin{pmatrix} \\frac{\\partial J}{\\partial n_F}  \\frac{\\partial J}{\\partial n_I} \\end{pmatrix}^T$。\n\n我们计算偏导数：\n$$\n\\frac{\\partial J}{\\partial n_F} = \\frac{\\partial}{\\partial n_F} \\left( \\beta_0 + \\beta_F n_F + \\beta_I n_I - \\frac{1}{2} a_F n_F^{2} - a_{FI} n_F n_I - \\frac{1}{2} a_I n_I^{2} \\right) = \\beta_F - a_F n_F - a_{FI} n_I\n$$\n$$\n\\frac{\\partial J}{\\partial n_I} = \\frac{\\partial}{\\partial n_I} \\left( \\beta_0 + \\beta_F n_F + \\beta_I n_I - \\frac{1}{2} a_F n_F^{2} - a_{FI} n_F n_I - \\frac{1}{2} a_I n_I^{2} \\right) = \\beta_I - a_{FI} n_F - a_I n_I\n$$\n\n将这些偏导数设为零，得到一个关于两个未知数 $n_F$ 和 $n_I$ 的二元线性方程组：\n$$\na_F n_F + a_{FI} n_I = \\beta_F\n$$\n$$\na_{FI} n_F + a_I n_I = \\beta_I\n$$\n\n该方程组可以用矩阵形式表示为：\n$$\n\\begin{pmatrix} a_F  a_{FI} \\\\ a_{FI}  a_I \\end{pmatrix} \\begin{pmatrix} n_F \\\\ n_I \\end{pmatrix} = \\begin{pmatrix} \\beta_F \\\\ \\beta_I \\end{pmatrix}\n$$\n\n最大值的二阶充分条件要求二阶偏导数的海森矩阵 $H$ 在临界点是负定的。我们来计算海森矩阵：\n$$\nH = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial n_F^2}  \\frac{\\partial^2 J}{\\partial n_F \\partial n_I} \\\\ \\frac{\\partial^2 J}{\\partial n_I \\partial n_F}  \\frac{\\partial^2 J}{\\partial n_I^2} \\end{pmatrix} = \\begin{pmatrix} -a_F  -a_{FI} \\\\ -a_{FI}  -a_I \\end{pmatrix}\n$$\n如果一个矩阵的顺序主子式从负号开始符号交替，那么该矩阵是负定的。\n一阶主子式为 $H_1 = -a_F$。由于问题陈述 $a_F  0$，我们有 $H_1  0$。\n二阶主子式是 $H$ 的行列式：\n$$\n\\det(H) = (-a_F)(-a_I) - (-a_{FI})^2 = a_F a_I - a_{FI}^2\n$$\n问题陈述 $a_F a_I - a_{FI}^2  0$。\n由于顺序主子式按要求符号交替（$-$，$+$），海森矩阵是负定的。这证实了一阶条件的解对应一个严格局部最大值。由于该函数是全局凹的，因此这是唯一的全局最大值。\n\n为了解出关于 $(n_F^{\\star}, n_I^{\\star})$ 的线性方程组，我们可以使用矩阵求逆法。令系数矩阵为 $A = \\begin{pmatrix} a_F  a_{FI} \\\\ a_{FI}  a_I \\end{pmatrix}$。其行列式为 $\\det(A) = a_F a_I - a_{FI}^2$，根据题意该值为正。$A$ 的逆矩阵是：\n$$\nA^{-1} = \\frac{1}{a_F a_I - a_{FI}^2} \\begin{pmatrix} a_I  -a_{FI} \\\\ -a_{FI}  a_F \\end{pmatrix}\n$$\n然后，通过用 $A^{-1}$ 左乘常数向量来找到解向量：\n$$\n\\begin{pmatrix} n_F^{\\star} \\\\ n_I^{\\star} \\end{pmatrix} = A^{-1} \\begin{pmatrix} \\beta_F \\\\ \\beta_I \\end{pmatrix} = \\frac{1}{a_F a_I - a_{FI}^2} \\begin{pmatrix} a_I  -a_{FI} \\\\ -a_{FI}  a_F \\end{pmatrix} \\begin{pmatrix} \\beta_F \\\\ \\beta_I \\end{pmatrix}\n$$\n执行矩阵-向量乘法，得到最优强度的表达式：\n$$\n\\begin{pmatrix} n_F^{\\star} \\\\ n_I^{\\star} \\end{pmatrix} = \\frac{1}{a_F a_I - a_{FI}^2} \\begin{pmatrix} a_I \\beta_F - a_{FI} \\beta_I \\\\ a_F \\beta_I - a_{FI} \\beta_F \\end{pmatrix}\n$$\n因此，优化解的分量是：\n$$\nn_F^{\\star} = \\frac{a_I \\beta_F - a_{FI} \\beta_I}{a_F a_I - a_{FI}^2}\n$$\n$$\nn_I^{\\star} = \\frac{a_F \\beta_I - a_{FI} \\beta_F}{a_F a_I - a_{FI}^2}\n$$\n这些表达式表示了使期望薪资最大化的唯一最优申请强度。",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{a_I \\beta_F - a_{FI} \\beta_I}{a_F a_I - a_{FI}^{2}}  \\frac{a_F \\beta_I - a_{FI} \\beta_F}{a_F a_I - a_{FI}^{2}} \\end{pmatrix}}$$"
        },
        {
            "introduction": "尽管解析解能为我们提供精确的答案，但在计算经济学和金融学的许多前沿问题中，目标函数往往异常复杂，无法通过手动推导求得最优解。此时，我们就需要借助计算机的强大算力，使用数值方法进行迭代求解。这个编码练习  将指导你实现最重要、最基础的优化算法之一——梯度下降法，并结合回溯线搜索（backtracking line search）来保证算法的有效收敛。通过解决从投资组合优化到逻辑回归等一系列问题，你将掌握将优化理论转化为实用代码的核心技能。",
            "id": "2445371",
            "problem": "给定来自计算经济学和金融学领域的多个多维光滑目标函数。对每个指定的测试用例，构建一个迭代无约束最小化方法，该方法根据 $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k$ 生成序列 $\\{\\mathbf{x}_k\\}_{k \\ge 0}$，其中下降方向为 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$。在每次迭代 $k$ 中，步长 $\\alpha_k$ 必须从几何序列 $\\{\\alpha_0 \\rho^m: m \\in \\{0,1,2,\\dots\\}\\}$ 中选取，以满足 Armijo 充分下降条件。其中 $\\alpha_0 \\in (0,\\infty)$ 和 $\\rho \\in (0,1)$ 是固定值，且对所有测试用例通用。\n$$\nf(\\mathbf{x}_k + \\alpha_k \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha_k \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k,\n$$\n其中 $c \\in (0,1)$ 是一个对所有测试用例固定的常数。从给定的起始点开始初始化，并在 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或当 $k$ 达到指定的最大迭代次数时终止。所有计算都在实数范围内进行。不使用角度。不涉及物理单位。\n\n对所有测试用例使用以下固定参数：$\\alpha_0 = 1$、$\\rho = \\tfrac{1}{2}$、 $c = 10^{-4}$、$\\varepsilon = 10^{-8}$ 以及 $\\text{max\\_iter} = 10000$。\n\n测试套件（所有矩阵和向量均已明确写出）：\n\n- 测试用例 1（金融学中的均值-方差二次型）：\n  - 决策变量 $\\mathbf{w} \\in \\mathbb{R}^2$。\n  - 目标\n    $$\n    f_1(\\mathbf{w}) = \\tfrac{1}{2}\\,\\mathbf{w}^\\top \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}^\\top \\mathbf{w},\n    \\quad\n    \\boldsymbol{\\Sigma} =\n    \\begin{bmatrix}\n    2  0.8 \\\\\n    0.8  1.5\n    \\end{bmatrix},\n    \\quad\n    \\boldsymbol{\\mu} =\n    \\begin{bmatrix}\n    0.5 \\\\\n    0.3\n    \\end{bmatrix}.\n    $$\n  - 初始条件 $\\mathbf{w}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n- 测试用例 2（病态二次型）：\n  - 决策变量 $\\mathbf{x} \\in \\mathbb{R}^2$。\n  - 目标\n    $\n    f_2(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x},\n    \\quad\n    \\mathbf{H} =\n    \\begin{bmatrix}\n    1000  0 \\\\\n    0  1\n    \\end{bmatrix},\n    \\quad\n    \\mathbf{b} =\n    \\begin{bmatrix}\n    1 \\\\\n    1\n    \\end{bmatrix}.\n    $\n  - 初始条件 $\\mathbf{x}_0 = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$。\n\n- 测试用例 3（使用 logistic 连接的二元选择负对数似然，非线性可分）：\n  - 参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$。\n  - 数据矩阵\n    $\n    \\mathbf{X} =\n    \\begin{bmatrix}\n    1  -2 \\\\\n    1  -1 \\\\\n    1  1 \\\\\n    1  2\n    \\end{bmatrix},\n    $\n    标签向量\n    $\n    \\mathbf{y} =\n    \\begin{bmatrix}\n    -1 \\\\\n    1 \\\\\n    -1 \\\\\n    1\n    \\end{bmatrix}.\n    $\n  - 目标\n    $$\n    f_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} \\log\\!\\big(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big),\n    $$\n    其中 $\\mathbf{x}_i^\\top$ 是 $\\mathbf{X}$ 的第 $i$ 行，而 $y_i$ 是 $\\mathbf{y}$ 的第 $i$ 个元素。\n  - 初始条件 $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n对每个测试用例，使用上述通用参数运行迭代最小化，并报告终止时的最小化目标值 $f(\\mathbf{x}^\\star)$。您的程序不得读取任何输入。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序排列结果，每个条目四舍五入到六位小数，例如 $[0.123456,1.234567,2.345678]$。",
            "solution": "问题陈述已经过严格评估，并被确定为有效。它在无约束优化领域提出了一组清晰、数学上合理且适定的任务。目标函数是计算经济学和金融学中的标准示例，所有必需的参数和初始条件都已明确提供，没有歧义或矛盾。所指定的算法，即基于 Armijo 条件的带回溯线搜索的梯度下降法，是解决此类问题的基本且合适的方法。我们现在将进行形式化的推导和求解。\n\n问题的核心是实现用于最小化光滑函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的最速下降法。这是一个迭代算法，它使用以下更新规则生成点序列 $\\{\\mathbf{x}_k\\}_{k \\ge 0}$：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\n方向 $\\mathbf{d}_k$ 被选为目标函数在当前迭代点 $\\mathbf{x}_k$ 处的负梯度，即最速下降方向：\n$$\n\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)\n$$\n步长 $\\alpha_k  0$ 通过回溯线搜索过程确定，以确保目标函数值有充分的下降。从初始猜测 $\\alpha = \\alpha_0$ 开始，步长连续乘以因子 $\\rho \\in (0,1)$ 直到满足 Armijo 条件：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c\\,\\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n$$\n其中 $c \\in (0,1)$ 是一个小常数。对于所有测试用例，给定参数固定为 $\\alpha_0 = 1$、$\\rho = \\frac{1}{2}$ 和 $c = 10^{-4}$。\n\n当梯度的欧几里得范数低于指定的容差 $\\varepsilon = 10^{-8}$，或者当迭代次数 $k$ 达到最大限制 $\\text{max\\_iter} = 10000$ 时，算法终止，并报告当前点 $\\mathbf{x}_k$ 作为近似最小值点 $\\mathbf{x}^\\star$。\n\n我们现在将此通用过程应用于三个指定的测试用例。\n\n**测试用例 1：均值-方差二次型**\n目标函数是来自投资组合优化的标准二次型：\n$$\nf_1(\\mathbf{w}) = \\tfrac{1}{2}\\,\\mathbf{w}^\\top \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}^\\top \\mathbf{w}\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^2$, $\\boldsymbol{\\Sigma} = \\begin{bmatrix} 2  0.8 \\\\ 0.8  1.5 \\end{bmatrix}$，以及 $\\boldsymbol{\\mu} = \\begin{bmatrix} 0.5 \\\\ 0.3 \\end{bmatrix}$。对于对称矩阵 $\\mathbf{A}$，一般二次函数 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$ 的梯度是 $\\nabla f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} - \\mathbf{b}$。因此，$f_1$ 的梯度为：\n$$\n\\nabla f_1(\\mathbf{w}) = \\boldsymbol{\\Sigma}\\,\\mathbf{w} - \\boldsymbol{\\mu}\n$$\n$f_1$ 的海森矩阵是 $\\nabla^2 f_1(\\mathbf{w}) = \\boldsymbol{\\Sigma}$。矩阵 $\\boldsymbol{\\Sigma}$ 是对称的，其特征值约为 $2.55$ 和 $0.95$，两者均为正。因此，$\\boldsymbol{\\Sigma}$ 是正定的，这意味着 $f_1$ 是严格凸的，并拥有唯一的全局最小值。最速下降法保证收敛到该最小值。算法从 $\\mathbf{w}_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$ 初始化。\n\n**测试用例 2：病态二次型**\n目标函数是另一个二次型：\n$$\nf_2(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^2$, $\\mathbf{H} = \\begin{bmatrix} 1000  0 \\\\ 0  1 \\end{bmatrix}$，以及 $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。梯度是：\n$$\n\\nabla f_2(\\mathbf{x}) = \\mathbf{H}\\,\\mathbf{x} - \\mathbf{b}\n$$\n海森矩阵 $\\mathbf{H}$ 是正定的，因为其特征值为 $1000$ 和 $1$。因此，$f_2$ 是严格凸的，并具有唯一的最小值。$\\mathbf{H}$ 的条件数是其最大特征值与最小特征值之比，即 $1000/1 = 1000$。这个高条件数意味着 $f_2$ 的水平集是高度拉长的椭圆，这通常会减慢最速下降法的收敛速度。算法从 $\\mathbf{x}_0 = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$ 开始。\n\n**测试用例 3：二元选择的负对数似然**\n目标函数是逻辑回归模型的负对数似然：\n$$\nf_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} \\log\\!\\big(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\big)\n$$\n其中 $\\boldsymbol{\\theta} \\in \\mathbb{R}^2$。为求梯度，我们对 $\\boldsymbol{\\theta}$ 的一个分量 $\\theta_j$ 求导：\n$$\n\\frac{\\partial f_3}{\\partial \\theta_j} = \\sum_{i=1}^{4} \\frac{1}{1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})} \\cdot \\frac{\\partial}{\\partial \\theta_j} \\left(1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})\\right)\n$$\n$$\n= \\sum_{i=1}^{4} \\frac{\\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})}{1 + \\exp(-y_i\\,\\mathbf{x}_i^\\top \\boldsymbol{\\theta})} \\cdot (-y_i x_{ij})\n$$\n令 $\\sigma(z) = 1/(1+e^{-z})$ 为 logistic sigmoid 函数。表达式 $\\frac{\\exp(-z)}{1+\\exp(-z)}$ 可以重写为 $\\frac{1}{e^z+1} = \\sigma(-z)$。\n因此，梯度向量 $\\nabla f_3(\\boldsymbol{\\theta})$ 的分量为：\n$$\n[\\nabla f_3(\\boldsymbol{\\theta})]_j = \\sum_{i=1}^{4} \\sigma(-y_i \\mathbf{x}_i^\\top \\boldsymbol{\\theta}) (-y_i x_{ij})\n$$\n这可以用向量形式紧凑地写出。令 $\\mathbf{p}$ 是一个向量，其元素为 $p_i = \\sigma(y_i \\mathbf{x}_i^\\top \\boldsymbol{\\theta})$。使用恒等式 $\\sigma(-z) = 1 - \\sigma(z)$，梯度为：\n$$\n\\nabla f_3(\\boldsymbol{\\theta}) = \\sum_{i=1}^{4} (1-p_i) (-y_i \\mathbf{x}_i) = \\sum_{i=1}^{4} y_i(p_i-1) \\mathbf{x}_i = \\mathbf{X}^\\top (\\mathbf{y} \\odot (\\mathbf{p} - \\mathbf{1}))\n$$\n其中 $\\odot$ 表示逐元素乘法，$\\mathbf{1}$ 是一个全为 1 的向量。可以证明该函数的海森矩阵是半正定的。由于数据矩阵 $\\mathbf{X}$ 具有线性无关的列（满列秩），海森矩阵实际上是正定的，从而确保 $f_3$ 是严格凸的，并有唯一的最小值点。算法从原点 $\\boldsymbol{\\theta}_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 初始化。为保证数值稳定性，$\\log(1+e^z)$ 的计算使用 log-sum-exp 模式进行。\n\n根据这些推导，在软件中实现该算法，以计算每种情况下的最终目标值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit, logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained optimization problems defined in the test suite\n    using the gradient descent method with backtracking line search.\n    \"\"\"\n    \n    # Define common parameters for the optimization algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    max_iter = 10000\n\n    def gradient_descent(f, grad_f, x0):\n        \"\"\"\n        Generic implementation of gradient descent with backtracking line search.\n\n        Args:\n            f (callable): The objective function.\n            grad_f (callable): The gradient of the objective function.\n            x0 (np.ndarray): The initial starting point.\n\n        Returns:\n            float: The minimized objective function value at termination.\n        \"\"\"\n        x = np.copy(x0).astype(np.float64)\n        \n        for k in range(max_iter):\n            grad = grad_f(x)\n            grad_norm = np.linalg.norm(grad)\n\n            # Termination condition: norm of the gradient is small enough\n            if grad_norm = epsilon:\n                break\n            \n            d = -grad  # Steepest descent direction\n            \n            # Backtracking line search to find step length alpha\n            alpha = alpha0\n            fx = f(x)\n            grad_dot_d = np.dot(grad, d)\n            \n            # Armijo condition check\n            while f(x + alpha * d)  fx + c * alpha * grad_dot_d:\n                alpha = rho * alpha\n            \n            # Update the iterate\n            x = x + alpha * d\n            \n        return f(x)\n\n    results = []\n\n    # Test Case 1: Mean-variance quadratic\n    Sigma = np.array([[2.0, 0.8], [0.8, 1.5]])\n    mu = np.array([0.5, 0.3])\n    w0 = np.array([1.0, 1.0])\n    \n    def f1(w):\n        return 0.5 * w.T @ Sigma @ w - mu.T @ w\n    \n    def grad_f1(w):\n        return Sigma @ w - mu\n        \n    result1 = gradient_descent(f1, grad_f1, w0)\n    results.append(result1)\n\n    # Test Case 2: Ill-conditioned quadratic\n    H = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    b = np.array([1.0, 1.0])\n    x0_2 = np.array([10.0, 10.0])\n\n    def f2(x):\n        return 0.5 * x.T @ H @ x - b.T @ x\n    \n    def grad_f2(x):\n        return H @ x - b\n        \n    result2 = gradient_descent(f2, grad_f2, x0_2)\n    results.append(result2)\n\n    # Test Case 3: Negative log-likelihood for binary choice\n    X = np.array([[1.0, -2.0], [1.0, -1.0], [1.0, 1.0], [1.0, 2.0]])\n    y = np.array([-1.0, 1.0, -1.0, 1.0])\n    theta0 = np.array([0.0, 0.0])\n\n    def f3(theta):\n        # log(1+exp(z)) is computed robustly as log(exp(0)+exp(z))\n        z = -y * (X @ theta)\n        return np.sum(logsumexp(np.vstack((np.zeros_like(z), z)), axis=0))\n\n    def grad_f3(theta):\n        # Gradient of sum_i log(1+exp(-y_i*x_i'theta)) is sum_i y_i(p_i-1)x_i\n        # where p_i = sigmoid(y_i*x_i'theta)\n        h = y * (X @ theta)\n        p = expit(h)  # Numerically stable sigmoid function\n        # Vectorized gradient calculation\n        return X.T @ (y * (p - 1.0))\n        \n    result3 = gradient_descent(f3, grad_f3, theta0)\n    results.append(result3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}