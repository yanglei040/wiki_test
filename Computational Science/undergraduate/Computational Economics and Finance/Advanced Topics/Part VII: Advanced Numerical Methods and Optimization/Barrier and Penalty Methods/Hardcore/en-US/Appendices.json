{
    "hands_on_practices": [
        {
            "introduction": "The power of penalty methods lies in transforming a difficult constrained problem into a sequence of more manageable unconstrained ones. This first exercise guides you through implementing the sequential quadratic penalty method, where you solve a series of subproblems for an increasing penalty sequence $\\rho_1 \\lt \\rho_2 \\lt \\dots \\lt \\rho_K$. By doing so, you will learn to progressively enforce constraints and discover the significant computational savings from \"warm-starting\" each subproblem with the solution from the previous one .",
            "id": "2423453",
            "problem": "You are asked to implement a sequential quadratic penalty method for constrained optimization and to experimentally quantify the benefit of warm-starting by reusing the previous subproblem’s solution as the initial guess for the next penalty parameter. Implement a gradient-based solver with a backtracking Armijo line search to minimize a sequence of unconstrained penalized subproblems. For a fixed sequence of penalty parameters $\\{\\rho_k\\}_{k=1}^K$ with $\\rho_1 < \\rho_2 < \\dots < \\rho_K$, compare two strategies for each test case: (i) cold-starting each subproblem from the same initial point, and (ii) warm-starting subproblem $k+1$ from the computed minimizer of subproblem $k$. Report the speedup factor defined as the ratio of the total number of gradient descent iterations taken by cold-starting to that taken by warm-starting across the full penalty sequence.\n\nFundamental base and definitions to use:\n- A constrained minimization problem has objective $f:\\mathbb{R}^n\\to\\mathbb{R}$, inequality constraints $g_i(x)\\le 0$ for $i\\in\\{1,\\dots,m\\}$, and equality constraints $h_j(x)=0$ for $j\\in\\{1,\\dots,p\\}$.\n- The classical quadratic penalty for inequalities is applied to the violation as $\\max\\{0, g_i(x)\\}^2$ and for equalities as $h_j(x)^2$.\n- The penalized subproblem for a given $\\rho>0$ is to minimize \n$$\n\\Phi_\\rho(x)=f(x)+\\rho\\left(\\sum_{i=1}^m \\max\\{0,g_i(x)\\}^2+\\sum_{j=1}^p h_j(x)^2\\right).\n$$\n- Use gradient descent with a backtracking Armijo rule: given current $x$, gradient $\\nabla\\Phi_\\rho(x)$, initial step size $t_0$, shrinkage factor $\\beta\\in(0,1)$, and Armijo parameter $c\\in(0,1)$, choose the largest $t$ from the sequence $\\{t_0, \\beta t_0, \\beta^2 t_0,\\dots\\}$ satisfying\n$$\n\\Phi_\\rho(x - t \\nabla \\Phi_\\rho(x)) \\le \\Phi_\\rho(x) - c\\,t\\,\\|\\nabla \\Phi_\\rho(x)\\|_2^2.\n$$\n- Stop the inner solver when $\\|\\nabla \\Phi_\\rho(x)\\|_2\\le \\varepsilon$.\n\nImplementation requirements:\n- Implement the quadratic penalty method and gradient descent with backtracking Armijo line search exactly as defined above.\n- For inequality constraints, treat only the positive violation by using the $\\max\\{0,\\cdot\\}$ structure in both the penalty value and its gradient. For equality constraints, penalize the squared residual.\n- Use the penalty sequence $\\rho\\in\\{10,10^2,10^3\\}$, i.e., $\\rho \\in \\{10,100,1000\\}$.\n- Use gradient tolerance $\\varepsilon=10^{-6}$, Armijo parameter $c=10^{-4}$, shrinkage factor $\\beta=\\tfrac{1}{2}$, and initial step size $t_0=1$ for all subproblems. Cap the maximum number of gradient iterations per subproblem at $N_{\\max}=10^4$.\n- Count the number of outer gradient descent iterations (each accepted step after line search) taken to converge a subproblem; do not count line search backtracking steps separately.\n\nTest suite:\nImplement and solve the following three two-dimensional test cases. In each case, return the speedup factor\n$$\nS=\\frac{N_{\\mathrm{cold}}}{N_{\\mathrm{warm}}},\n$$\nwhere $N_{\\mathrm{cold}}$ is the total number of gradient descent iterations summed over all penalty parameters when cold-starting each subproblem from the specified initial point, and $N_{\\mathrm{warm}}$ is the total when warm-starting each subproblem from the previous subproblem’s solution.\n\n- Case $\\mathbf{A}$ (convex quadratic with a binding linear inequality):\n  - Objective: $f(x,y)=(x-1)^2+2\\,(y+2)^2$.\n  - Inequality: $g_1(x,y)=1-x-y\\le 0$.\n  - No equalities.\n  - Initial point: $x_0=(0,0)$.\n\n- Case $\\mathbf{B}$ (convex quadratic with an equality):\n  - Objective: $f(x,y)=(x-3)^2+(y-1)^2$.\n  - Equality: $h_1(x,y)=x-y=0$.\n  - No inequalities.\n  - Initial point: $x_0=(0,0)$.\n\n- Case $\\mathbf{C}$ (convex quadratic with a curved inequality):\n  - Objective: $f(x,y)=(x+2)^2+y^2$.\n  - Inequality: $g_1(x,y)=x^2+y^2-1\\le 0$.\n  - No equalities.\n  - Initial point: $x_0=(0,0)$.\n\nOutput specification:\n- For each case, compute the speedup factor $S$ as defined above.\n- Your program should produce a single line of output containing the three speedup factors as a comma-separated list enclosed in square brackets, in the order $\\left[S_A,S_B,S_C\\right]$, where $S_A$ corresponds to Case $\\mathbf{A}$, $S_B$ to Case $\\mathbf{B}$, and $S_C$ to Case $\\mathbf{C}$. For example, output of the form $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$ with numeric values.\n- Express each speedup factor as a floating-point number. You may round internally, but the printed values must be standard decimal floats.\n\nNo physical units are involved. Angles are not used. Percentages are not used.\n\nThe final program must be self-contained, require no input, and adhere to the specified runtime environment. The correctness will be assessed by verifying that the implementation follows the definitions and that warm-starting yields strictly fewer iterations or at least not more, producing meaningful speedup factors for the specified cases. The output must be exactly one line in the specified format.",
            "solution": "The problem requires the implementation of a sequential quadratic penalty method to solve constrained optimization problems. The core of the task is to compare the computational efficiency of two initialization strategies for the sequence of unconstrained subproblems: a cold-start strategy versus a warm-start strategy. The efficiency is to be quantified by a speedup factor, defined as the ratio of total gradient descent iterations.\n\nThe general form of the constrained optimization problem is to minimize an objective function $f(x)$ subject to a set of inequality constraints $g_i(x) \\le 0$ for $i \\in \\{1, \\dots, m\\}$ and equality constraints $h_j(x) = 0$ for $j \\in \\{1, \\dots, p\\}$, where $x \\in \\mathbb{R}^n$.\n\nThe quadratic penalty method approximates the solution to this problem by solving a sequence of unconstrained minimization problems. For a given penalty parameter $\\rho > 0$, the penalized objective function $\\Phi_\\rho(x)$ is constructed by adding terms to the original objective function that penalize violations of the constraints. The specific form of the penalized function is:\n$$\n\\Phi_\\rho(x) = f(x) + \\rho \\left( \\sum_{i=1}^m \\left(\\max\\{0, g_i(x)\\}\\right)^2 + \\sum_{j=1}^p \\left(h_j(x)\\right)^2 \\right)\n$$\nThis function, $\\Phi_\\rho(x)$, is then minimized with respect to $x$. By solving this unconstrained problem for a sequence of increasing penalty parameters, $\\rho_1 < \\rho_2 < \\dots < \\rho_K$, the sequence of minimizers $x^*(\\rho_k)$ converges to the solution of the original constrained problem.\n\nTo minimize each unconstrained subproblem $\\min_x \\Phi_\\rho(x)$, a gradient-based method is required. The gradient of the penalized objective function, $\\nabla \\Phi_\\rho(x)$, is derived using the chain rule. For an inequality constraint term $P_i(x) = \\rho (\\max\\{0, g_i(x)\\})^2$, the gradient is $\\nabla P_i(x) = 2 \\rho \\max\\{0, g_i(x)\\} \\nabla g_i(x)$. For an equality constraint term $Q_j(x) = \\rho (h_j(x))^2$, the gradient is $\\nabla Q_j(x) = 2 \\rho h_j(x) \\nabla h_j(x)$. Combining these with the gradient of the objective function, the full gradient is:\n$$\n\\nabla \\Phi_\\rho(x) = \\nabla f(x) + 2\\rho \\left( \\sum_{i=1}^m \\max\\{0, g_i(x)\\} \\nabla g_i(x) + \\sum_{j=1}^p h_j(x) \\nabla h_j(x) \\right)\n$$\nThe unconstrained minimization is performed using gradient descent. Starting from a point $x_k$, the next point $x_{k+1}$ is found by moving in the direction of the negative gradient:\n$$\nx_{k+1} = x_k - t \\nabla \\Phi_\\rho(x_k)\n$$\nThe step size $t > 0$ is determined by a backtracking line search employing the Armijo condition. For a given descent direction $d_k = -\\nabla \\Phi_\\rho(x_k)$, we seek the largest $t$ from the sequence $\\{t_0, \\beta t_0, \\beta^2 t_0, \\dots\\}$ that satisfies:\n$$\n\\Phi_\\rho(x_k + t d_k) \\le \\Phi_\\rho(x_k) + c \\, t \\, \\nabla \\Phi_\\rho(x_k)^T d_k\n$$\nwhich, using $d_k = -\\nabla \\Phi_\\rho(x_k)$, simplifies to the form given in the problem statement:\n$$\n\\Phi_\\rho(x_k - t \\nabla \\Phi_\\rho(x_k)) \\le \\Phi_\\rho(x_k) - c \\, t \\, \\|\\nabla \\Phi_\\rho(x_k)\\|_2^2\n$$\nThe algorithm iterates until the norm of the gradient is below a specified tolerance $\\varepsilon$, i.e., $\\|\\nabla \\Phi_\\rho(x)\\|_2 \\le \\varepsilon$. The parameters for this solver are fixed: initial step size $t_0=1$, Armijo parameter $c=10^{-4}$, shrinkage factor $\\beta=0.5$, and gradient norm tolerance $\\varepsilon=10^{-6}$. The maximum number of iterations per subproblem is capped at $N_{\\max}=10^4$.\n\nThe experiment compares two strategies over the penalty parameter sequence $\\rho \\in \\{10, 100, 1000\\}$:\n1.  **Cold-Start:** Each subproblem for $\\rho_k$ is initialized from the same starting point $x_0$. The total number of iterations, $N_{\\mathrm{cold}}$, is the sum of iterations required to solve each subproblem independently.\n2.  **Warm-Start:** The first subproblem (for $\\rho_1=10$) is initialized from $x_0$. The subproblem for each subsequent $\\rho_{k+1}$ is initialized using the solution obtained from the previous subproblem for $\\rho_k$. The total number of iterations, $N_{\\mathrm{warm}}$, is the sum of iterations across this sequence.\n\nThe rationale for warm-starting is that the solution $x^*(\\rho_k)$ is expected to be a good initial guess for the minimizer of $\\Phi_{\\rho_{k+1}}(x)$, especially when $\\rho_{k+1}$ is not drastically larger than $\\rho_k$. This should lead to faster convergence. The performance gain is measured by the speedup factor $S = N_{\\mathrm{cold}} / N_{\\mathrm{warm}}$.\n\nThe implementation will proceed by defining Python functions for each test case's objective and constraint functions and their respective gradients. A generalized solver function will execute the gradient descent with Armijo line search. A top-level function will manage the sequence of penalty parameters, apply both cold-start and warm-start strategies, count the total iterations for each, and compute the speedup. This process will be repeated for all three test cases provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    # --- Solver Parameters ---\n    SOLVER_PARAMS = {\n        'epsilon': 1e-6,\n        'c_armijo': 1e-4,\n        'beta': 0.5,\n        't0': 1.0,\n        'n_max': 10000\n    }\n    PENALTY_PARAMS = [10.0, 100.0, 1000.0]\n\n    # --- Test Case Definitions ---\n    \n    # Case A: (x-1)^2 + 2(y+2)^2, s.t. 1-x-y <= 0\n    case_A = {\n        'f': lambda x: (x[0] - 1.0)**2 + 2.0 * (x[1] + 2.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 1.0), 4.0 * (x[1] + 2.0)]),\n        'g': [lambda x: 1.0 - x[0] - x[1]],\n        'grad_g': [lambda x: np.array([-1.0, -1.0])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    # Case B: (x-3)^2 + (y-1)^2, s.t. x-y = 0\n    case_B = {\n        'f': lambda x: (x[0] - 3.0)**2 + (x[1] - 1.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 3.0), 2.0 * (x[1] - 1.0)]),\n        'g': [],\n        'grad_g': [],\n        'h': [lambda x: x[0] - x[1]],\n        'grad_h': [lambda x: np.array([1.0, -1.0])],\n        'x0': np.array([0.0, 0.0])\n    }\n    \n    # Case C: (x+2)^2 + y^2, s.t. x^2+y^2-1 <= 0\n    case_C = {\n        'f': lambda x: (x[0] + 2.0)**2 + x[1]**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] + 2.0), 2.0 * x[1]]),\n        'g': [lambda x: x[0]**2 + x[1]**2 - 1.0],\n        'grad_g': [lambda x: np.array([2.0 * x[0], 2.0 * x[1]])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    \n    def get_penalized_funcs(case, rho):\n        \"\"\"Creates the penalized function and its gradient for a given case and rho.\"\"\"\n        \n        def phi(x):\n            f_val = case['f'](x)\n            g_sum = sum(max(0, g_func(x))**2 for g_func in case['g'])\n            h_sum = sum(h_func(x)**2 for h_func in case['h'])\n            return f_val + rho * (g_sum + h_sum)\n\n        def grad_phi(x):\n            grad_f_val = case['grad_f'](x)\n            \n            grad_g_sum = np.zeros_like(x)\n            for g_func, grad_g_func in zip(case['g'], case['grad_g']):\n                g_val = g_func(x)\n                if g_val > 0:\n                    grad_g_sum += 2.0 * g_val * grad_g_func(x)\n\n            grad_h_sum = np.zeros_like(x)\n            for h_func, grad_h_func in zip(case['h'], case['grad_h']):\n                h_val = h_func(x)\n                grad_h_sum += 2.0 * h_val * grad_h_func(x)\n                \n            return grad_f_val + rho * (grad_g_sum + grad_h_sum)\n        \n        return phi, grad_phi\n\n    def gradient_descent(phi, grad_phi, x_init, params):\n        \"\"\"\n        Performs gradient descent with backtracking Armijo line search.\n        \"\"\"\n        x = np.copy(x_init)\n        n_iters = 0\n        \n        for k in range(params['n_max']):\n            grad = grad_phi(x)\n            grad_norm_sq = np.dot(grad, grad)\n\n            if np.sqrt(grad_norm_sq) <= params['epsilon']:\n                break\n            \n            # Backtracking line search\n            t = params['t0']\n            phi_x = phi(x)\n            \n            while True:\n                x_new = x - t * grad\n                phi_new = phi(x_new)\n                armijo_check = phi_x - params['c_armijo'] * t * grad_norm_sq\n                \n                if phi_new <= armijo_check:\n                    break\n                t *= params['beta']\n            \n            x = x_new\n            n_iters += 1\n        \n        return x, n_iters\n\n    def run_penalty_method(case, solver_params, penalty_params):\n        \"\"\"\n        Runs the full sequential penalty method for a case,\n        calculating iterations for both cold and warm starts.\n        \"\"\"\n        # Cold start\n        total_iters_cold = 0\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            _, n_iters = gradient_descent(phi, grad_phi, case['x0'], solver_params)\n            total_iters_cold += n_iters\n            \n        # Warm start\n        total_iters_warm = 0\n        x_warm = np.copy(case['x0'])\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            x_sol, n_iters = gradient_descent(phi, grad_phi, x_warm, solver_params)\n            total_iters_warm += n_iters\n            x_warm = x_sol\n            \n        if total_iters_warm == 0:\n             # This case should not happen in this problem, but is a safeguard.\n             # If cold is also 0, speedup is 1. If cold > 0, speedup is \"infinite\".\n            return 1.0 if total_iters_cold == 0 else float('inf')\n            \n        return float(total_iters_cold) / float(total_iters_warm)\n\n    results = []\n    for case in test_cases:\n        speedup = run_penalty_method(case, SOLVER_PARAMS, PENALTY_PARAMS)\n        results.append(speedup)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond finding optimal solutions, penalty methods are powerful tools for solving feasibility problems. In many financial applications, the first challenge is simply to find a portfolio that satisfies a complex web of constraints, such as budget, risk limits, and expected returns. This hands-on problem asks you to apply the quadratic penalty method to exactly this task, turning the search for a feasible portfolio into a well-defined unconstrained optimization problem .",
            "id": "2374527",
            "problem": "Consider the task of constructing an initial portfolio vector that satisfies a set of economic and financial constraints. Let $n \\in \\mathbb{N}$ denote the number of assets, let $\\mu \\in \\mathbb{R}^n$ be the vector of expected returns, and let $\\Sigma \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite covariance matrix of returns. Let $u \\in \\mathbb{R}^n$ be a vector of componentwise upper bounds on weights. The portfolio weights are $w \\in \\mathbb{R}^n$. The constraints to be satisfied are:\n- Budget equality: $\\sum_{i=1}^n w_i = 1$.\n- Lower bounds (no short-selling): $w_i \\ge 0$ for all $i \\in \\{1,\\dots,n\\}$.\n- Upper bounds: $w_i \\le u_i$ for all $i \\in \\{1,\\dots,n\\}$.\n- Required expected return: $\\mu^\\top w \\ge R_{\\text{target}}$.\n- Risk cap: $w^\\top \\Sigma w \\le V_{\\max}$.\n\nDefine the inequality functions in the canonical form $g(w) \\le 0$ and the equality function $h(w) = 0$ as\n- $g_{\\text{ret}}(w) = R_{\\text{target}} - \\mu^\\top w$,\n- $g_{\\text{var}}(w) = w^\\top \\Sigma w - V_{\\max}$,\n- $g_{\\text{lo},i}(w) = -w_i$ for each $i \\in \\{1,\\dots,n\\}$,\n- $g_{\\text{up},i}(w) = w_i - u_i$ for each $i \\in \\{1,\\dots,n\\}$,\n- $h_{\\text{bud}}(w) = \\mathbf{1}^\\top w - 1$, where $\\mathbf{1}$ is the vector of ones in $\\mathbb{R}^n$.\n\nFor any penalty parameter $\\rho > 0$, define the merit function\n$$\nM_\\rho(w) \\;=\\; \\rho \\left( \\sum_{i=1}^n \\bigl(\\max\\{0, g_{\\text{lo},i}(w)\\}\\bigr)^2 \\;+\\; \\sum_{i=1}^n \\bigl(\\max\\{0, g_{\\text{up},i}(w)\\}\\bigr)^2 \\;+\\; \\bigl(\\max\\{0, g_{\\text{ret}}(w)\\}\\bigr)^2 \\;+\\; \\bigl(\\max\\{0, g_{\\text{var}}(w)\\}\\bigr)^2 \\;+\\; \\bigl(h_{\\text{bud}}(w)\\bigr)^2 \\right) \\;+\\; \\lambda \\,\\|w\\|_2^2,\n$$\nwhere $\\lambda > 0$ is a fixed regularization parameter and $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nFor a given tolerance $\\tau > 0$, define the maximum constraint violation at any $w$ as\n$$\n\\mathrm{vio}(w) \\;=\\; \\max\\!\\left( \\left| h_{\\text{bud}}(w) \\right|, \\;\\max\\{0, g_{\\text{ret}}(w)\\}, \\;\\max\\{0, g_{\\text{var}}(w)\\}, \\;\\max_{i=1,\\dots,n}\\max\\{0, g_{\\text{lo},i}(w)\\}, \\;\\max_{i=1,\\dots,n}\\max\\{0, g_{\\text{up},i}(w)\\} \\right).\n$$\n\nYour program must, for each test instance below, produce a vector $w$ that approximately minimizes $M_\\rho(w)$ for some $\\rho$ chosen from the sequence $\\{10^1, 10^2, 10^3, 10^4, 10^5, 10^6\\}$ and then report the value of $\\mathrm{vio}(w)$ at the obtained $w$. For each test instance, select the smallest $\\rho$ in the sequence that yields $\\mathrm{vio}(w) \\le \\tau$, if any; if no such $\\rho$ yields $\\mathrm{vio}(w) \\le \\tau$, report the $\\mathrm{vio}(w)$ achieved at the largest $\\rho = 10^6$. Use the fixed tolerance $\\tau = 10^{-6}$ and regularization $\\lambda = 10^{-8}$.\n\nTest suite:\n- Test case A:\n  - $n = 3$,\n  - $\\mu = [\\,0.06,\\; 0.10,\\; 0.14\\,]$,\n  - $\\Sigma = \\begin{bmatrix} 0.010 & 0.002 & 0.001 \\\\ 0.002 & 0.020 & 0.003 \\\\ 0.001 & 0.003 & 0.030 \\end{bmatrix}$,\n  - $u = [\\,0.8,\\; 0.8,\\; 0.8\\,]$,\n  - $R_{\\text{target}} = 0.09$,\n  - $V_{\\max} = 0.025$.\n- Test case B:\n  - $n = 3$,\n  - $\\mu = [\\,0.03,\\; 0.05,\\; 0.07\\,]$,\n  - $\\Sigma = \\begin{bmatrix} 0.008 & 0.001 & 0.0005 \\\\ 0.001 & 0.012 & 0.001 \\\\ 0.0005 & 0.001 & 0.015 \\end{bmatrix}$,\n  - $u = [\\,0.8,\\; 0.8,\\; 0.8\\,]$,\n  - $R_{\\text{target}} = 0.07$,\n  - $V_{\\max} = 0.05$.\n- Test case C:\n  - $n = 4$,\n  - $\\mu = [\\,0.05,\\; 0.08,\\; 0.12,\\; 0.04\\,]$,\n  - $\\Sigma = \\begin{bmatrix}\n  0.005 & 0.001 & 0.001 & 0.0005 \\\\\n  0.001 & 0.010 & 0.002 & 0.001 \\\\\n  0.001 & 0.002 & 0.020 & 0.0015 \\\\\n  0.0005 & 0.001 & 0.0015 & 0.004\n  \\end{bmatrix}$,\n  - $u = [\\,0.6,\\; 0.6,\\; 0.5,\\; 1.0\\,]$,\n  - $R_{\\text{target}} = 0.08$,\n  - $V_{\\max} = 0.012$.\n\nInitial conditions for all test instances: use any deterministic $w^{(0)} \\in \\mathbb{R}^n$; for example, the equal-weight vector $w^{(0)}$ with components $w^{(0)}_i = 1/n$.\n\nYour program must output, in a single line, the maximum constraint violations for the three test cases in the following exact format: a single list with three floating-point numbers rounded to exactly six digits after the decimal point and separated by commas, enclosed in square brackets, in the order of test cases A, B, C. For example, an output line must look like $[v_A,v_B,v_C]$, where each of $v_A$, $v_B$, $v_C$ is a float rounded to six decimal places. No additional text must be printed.",
            "solution": "The problem requires finding a portfolio weights vector $w \\in \\mathbb{R}^n$ that satisfies a set of linear and quadratic equality and inequality constraints. This is a feasibility problem in computational finance. The proposed method to find such a vector is the penalty method, which transforms the constrained problem into a sequence of unconstrained optimization problems.\n\nThe core of the method is the construction of a merit function, $M_\\rho(w)$, which must be minimized. This function, for a given penalty parameter $\\rho > 0$, is defined as:\n$$\nM_\\rho(w) \\;=\\; \\rho \\cdot P(w) \\;+\\; \\lambda \\,\\|w\\|_2^2\n$$\nwhere $P(w)$ is the penalty term and $\\lambda \\|w\\|_2^2$ is a regularization term. The penalty term aggregates the violations of all constraints:\n$$\nP(w) \\;=\\; \\sum_{j} \\bigl(\\max\\{0, g_j(w)\\}\\bigr)^2 \\;+\\; \\sum_{k} \\bigl(h_k(w)\\bigr)^2\n$$\nHere, $g_j(w) \\le 0$ are the inequality constraints and $h_k(w) = 0$ are the equality constraints. The problem statement explicitly defines these for the portfolio problem:\n- Inequalities: $g_{\\text{ret}}(w) = R_{\\text{target}} - \\mu^\\top w$, $g_{\\text{var}}(w) = w^\\top \\Sigma w - V_{\\max}$, $g_{\\text{lo},i}(w) = -w_i$, and $g_{\\text{up},i}(w) = w_i - u_i$.\n- Equality: $h_{\\text{bud}}(w) = \\mathbf{1}^\\top w - 1$.\n\nThe function $M_\\rho(w)$ is an unconstrained, continuously differentiable ($C^1$) function. Its properties are critical. The functions defining the constraints, $g_j(w)$ and $h_k(w)$, are either affine or, in the case of $g_{\\text{var}}(w)$, convex, because the covariance matrix $\\Sigma$ is positive definite. The function $\\max\\{0, \\cdot\\}$ is convex and non-decreasing. The composition of a convex function with a non-negative, non-decreasing, convex function (like $x \\mapsto x^2$ for $x \\ge 0$) preserves convexity. Therefore, each term $\\bigl(\\max\\{0, g_j(w)\\}\\bigr)^2$ is convex. Similarly, the square of an affine function, $\\bigl(h_k(w)\\bigr)^2$, is convex. As the regularization term $\\lambda \\|w\\|_2^2$ is strongly convex for $\\lambda > 0$, the merit function $M_\\rho(w)$, being a non-negative sum of convex functions including one strongly convex term, is itself strongly convex. This is a crucial property, as it guarantees that for any given $\\rho > 0$, $M_\\rho(w)$ has a unique global minimizer.\n\nTo find this minimizer numerically, we can employ a gradient-based optimization algorithm. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a suitable choice for this unconstrained $C^1$ minimization problem. For the algorithm to be efficient and accurate, we must provide the analytical gradient of the merit function, $\\nabla M_\\rho(w)$. Using the chain rule, the gradient is:\n$$\n\\nabla M_\\rho(w) = 2\\rho \\left( \\sum_{j} \\max\\{0, g_j(w)\\} \\nabla g_j(w) \\;+\\; \\sum_{k} h_k(w) \\nabla h_k(w) \\right) \\;+\\; 2\\lambda w\n$$\nThe gradients of the individual constraint functions are straightforward to compute:\n- $\\nabla g_{\\text{lo},i}(w) = -e_i$ (where $e_i$ is the $i$-th standard basis vector)\n- $\\nabla g_{\\text{up},i}(w) = e_i$\n- $\\nabla g_{\\text{ret}}(w) = -\\mu$\n- $\\nabla g_{\\text{var}}(w) = 2 \\Sigma w$ (since $\\Sigma$ is symmetric)\n- $\\nabla h_{\\text{bud}}(w) = \\mathbf{1}$ (a vector of ones)\n\nSubstituting these into the expression for $\\nabla M_\\rho(w)$ yields a complete formula for the gradient vector that can be implemented numerically.\n\nThe overall procedure, as specified by the problem, is as follows:\n1. Initialize the portfolio with an equal-weight vector, $w^{(0)}_i = 1/n$.\n2. Iterate through the prescribed sequence of penalty parameters, $\\rho \\in \\{10^1, 10^2, \\dots, 10^6\\}$.\n3. In each iteration, numerically solve the unconstrained minimization problem $w^* = \\arg\\min_w M_\\rho(w)$ using the BFGS algorithm, starting from the solution of the previous iteration (a warm-start strategy).\n4. After finding the optimal $w^*$ for the current $\\rho$, calculate the maximum constraint violation, $\\mathrm{vio}(w^*)$, as defined in the problem statement.\n5. If $\\mathrm{vio}(w^*) \\le \\tau = 10^{-6}$, the process terminates for the current test case, and this violation value is the result.\n6. If the tolerance is not met, continue to the next, larger value of $\\rho$. If the loop completes without meeting the tolerance, the violation from the final step (with $\\rho = 10^6$) is reported.\n\nThis systematic approach ensures that we find a feasible solution with the required precision if one is found within the given sequence of $\\rho$, or we report the \"best-effort\" solution corresponding to the highest penalty. Since the feasible regions for all test cases are non-empty, we expect the algorithm to find a solution satisfying the tolerance $\\tau$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final results.\n    \"\"\"\n    \n    # Global parameters as specified in the problem\n    LAMBDA = 1e-8\n    TAU = 1e-6\n    RHO_SEQUENCE = np.array([1e1, 1e2, 1e3, 1e4, 1e5, 1e6])\n\n    # Test cases data\n    test_cases = [\n        # Case A\n        {\n            \"n\": 3,\n            \"mu\": np.array([0.06, 0.10, 0.14]),\n            \"Sigma\": np.array([[0.010, 0.002, 0.001],\n                               [0.002, 0.020, 0.003],\n                               [0.001, 0.003, 0.030]]),\n            \"u\": np.array([0.8, 0.8, 0.8]),\n            \"R_target\": 0.09,\n            \"V_max\": 0.025,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        },\n        # Case B\n        {\n            \"n\": 3,\n            \"mu\": np.array([0.03, 0.05, 0.07]),\n            \"Sigma\": np.array([[0.008, 0.001, 0.0005],\n                               [0.001, 0.012, 0.001],\n                               [0.0005, 0.001, 0.015]]),\n            \"u\": np.array([0.8, 0.8, 0.8]),\n            \"R_target\": 0.07,\n            \"V_max\": 0.05,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        },\n        # Case C\n        {\n            \"n\": 4,\n            \"mu\": np.array([0.05, 0.08, 0.12, 0.04]),\n            \"Sigma\": np.array([[0.005, 0.001, 0.001, 0.0005],\n                               [0.001, 0.010, 0.002, 0.001],\n                               [0.001, 0.002, 0.020, 0.0015],\n                               [0.0005, 0.001, 0.0015, 0.004]]),\n            \"u\": np.array([0.6, 0.6, 0.5, 1.0]),\n            \"R_target\": 0.08,\n            \"V_max\": 0.012,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        violation = solve_one_case(case_params, RHO_SEQUENCE)\n        # Format to exactly 6 digits after the decimal point\n        results.append(f\"{violation:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef get_constraint_values(w, params):\n    \"\"\"Calculates the values of all constraint functions.\"\"\"\n    mu, Sigma, u, R_target, V_max = params['mu'], params['Sigma'], params['u'], params['R_target'], params['V_max']\n    \n    g_lo = -w\n    g_up = w - u\n    g_ret = R_target - mu.dot(w)\n    g_var = w.dot(Sigma.dot(w)) - V_max\n    h_bud = np.sum(w) - 1.0\n    \n    return g_lo, g_up, g_ret, g_var, h_bud\n\ndef merit_function(w, rho, params):\n    \"\"\"Calculates the value of the merit function M_rho(w).\"\"\"\n    lam = params['lambda']\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    p_lo = np.maximum(0, g_lo)\n    p_up = np.maximum(0, g_up)\n    p_ret = np.maximum(0, g_ret)\n    p_var = np.maximum(0, g_var)\n    \n    penalty_term = np.sum(p_lo**2) + np.sum(p_up**2) + p_ret**2 + p_var**2 + h_bud**2\n    regularization_term = lam * np.sum(w**2)\n    \n    return rho * penalty_term + regularization_term\n\ndef merit_gradient(w, rho, params):\n    \"\"\"Calculates the gradient of the merit function M_rho(w).\"\"\"\n    n, mu, Sigma, lam = params['n'], params['mu'], params['Sigma'], params['lambda']\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    p_lo = np.maximum(0, g_lo)\n    p_up = np.maximum(0, g_up)\n    p_ret = np.maximum(0, g_ret)\n    p_var = np.maximum(0, g_var)\n    \n    # Gradient of penalty term for lo/up bounds\n    grad_bounds = p_up - p_lo # component-wise\n    \n    # Gradient of penalty term for return\n    grad_ret = p_ret * (-mu)\n    \n    # Gradient of penalty term for variance\n    grad_var = p_var * (2 * Sigma.dot(w))\n    \n    # Gradient of penalty term for budget\n    grad_bud = h_bud * np.ones(n)\n    \n    # Combine all gradient components\n    grad = 2 * rho * (grad_bounds + grad_ret + grad_var + grad_bud)\n    \n    # Add gradient of regularization term\n    grad += 2 * lam * w\n    \n    return grad\n\ndef calculate_violation(w, params):\n    \"\"\"Calculates the maximum constraint violation vio(w).\"\"\"\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    vio_lo = np.max(np.maximum(0, g_lo))\n    vio_up = np.max(np.maximum(0, g_up))\n    vio_ret = np.maximum(0, g_ret)\n    vio_var = np.maximum(0, g_var)\n    vio_bud = np.abs(h_bud)\n    \n    return np.max([vio_lo, vio_up, vio_ret, vio_var, vio_bud])\n\ndef solve_one_case(params, rho_sequence):\n    \"\"\"Solves a single test case using the penalty method.\"\"\"\n    n = params['n']\n    tau = params['tau']\n    w0 = np.ones(n) / n\n    final_violation = -1.0\n    \n    for rho in rho_sequence:\n        res = minimize(\n            fun=merit_function,\n            x0=w0,\n            args=(rho, params),\n            method='BFGS',\n            jac=merit_gradient,\n            options={'gtol': 1e-9} \n        )\n        \n        w_opt = res.x\n        final_violation = calculate_violation(w_opt, params)\n        \n        if final_violation <= tau:\n            break\n        \n        w0 = w_opt # Warm start for the next iteration\n        \n    return final_violation\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Not all penalty functions are created equal. The quadratic penalty, while smooth and easy to optimize, requires the penalty parameter $\\rho$ to approach infinity to enforce constraints exactly. This final practice moves from implementation to a conceptual deep dive, comparing the quadratic penalty with the $L_1$ penalty, a classic example of an *exact* penalty function. By analyzing their properties, you will learn about the fundamental trade-off between a function's smoothness and its ability to find a true constrained solution with a finite penalty parameter .",
            "id": "2423474",
            "problem": "In a computational engineering design task, consider the equality-constrained optimization problem to minimize $f(x)$ subject to $h(x)=0$, where $x \\in \\mathbb{R}^n$, $f:\\mathbb{R}^n \\to \\mathbb{R}$, and $h:\\mathbb{R}^n \\to \\mathbb{R}$. Assume $f$ and $h$ are continuously differentiable and that at a constrained local minimizer $x^\\star$ the linear independence constraint qualification holds and second-order sufficient conditions are satisfied. Two unconstrained penalty formulations are used:\n( i ) the quadratic penalty $F_{\\mathrm{QP}}(x;\\rho) = f(x) + \\rho\\,h(x)^2$ with $\\rho > 0$,\n( ii ) the $L_1$ penalty $F_{L_1}(x;\\rho) = f(x) + \\rho\\,|h(x)|$ with $\\rho > 0$.\nCompare these penalty approaches in terms of smoothness, conditioning, and their ability to recover a constrained solution at finite versus infinite $\\rho$. Select all statements that are correct.\n\nA. If $f$ and $h$ are smooth, then $F_{\\mathrm{QP}}(x;\\rho)$ is smooth for all $x$ and any $\\rho>0$, but enforcing feasibility typically requires $\\rho \\to \\infty$, which can cause ill-conditioning of the Hessian near feasible points.\n\nB. The $L_1$ penalty can be exact for equality constraints: under standard regularity, there exists a finite $\\bar{\\rho} \\ge |\\lambda^\\star|$ such that the constrained local minimizer $x^\\star$ with Karush–Kuhn–Tucker (KKT) multiplier $\\lambda^\\star$ is also a local minimizer of $F_{L_1}(x;\\rho)$ for all $\\rho \\ge \\bar{\\rho}$; however, $F_{L_1}(x;\\rho)$ is nonsmooth at points where $h(x)=0$.\n\nC. Because $F_{L_1}(x;\\rho)$ lacks differentiability only away from feasibility, Newton’s method applied directly to $F_{L_1}(x;\\rho)$ enjoys quadratic convergence near feasible points.\n\nD. Quadratic penalty methods avoid ill-conditioning as $\\rho$ increases because the Hessian of $F_{\\mathrm{QP}}(x;\\rho)$ remains uniformly bounded in $\\rho$.\n\nE. When $h(x)$ is linear, the quadratic and $L_1$ penalties are equivalent in the sense that for any fixed $\\rho>0$ they produce identical minimizers for all smooth $f$.",
            "solution": "The problem statement will first be validated for its scientific and logical integrity.\n\n### Step 1: Extract Givens\n-   **Optimization Problem**: Minimize $f(x)$ subject to $h(x)=0$.\n-   **Domain and Functions**: $x \\in \\mathbb{R}^n$, $f:\\mathbb{R}^n \\to \\mathbb{R}$, $h:\\mathbb{R}^n \\to \\mathbb{R}$.\n-   **Assumptions**:\n    1.  $f$ and $h$ are continuously differentiable.\n    2.  $x^\\star$ is a constrained local minimizer.\n    3.  The Linear Independence Constraint Qualification (LICQ) holds at $x^\\star$.\n    4.  Second-order sufficient conditions (SOSC) are satisfied at $x^\\star$.\n-   **Penalty Formulations**:\n    1.  Quadratic penalty function: $F_{\\mathrm{QP}}(x;\\rho) = f(x) + \\rho\\,h(x)^2$ with $\\rho > 0$.\n    2.  $L_1$ penalty function: $F_{L_1}(x;\\rho) = f(x) + \\rho\\,|h(x)|$ with $\\rho > 0$.\n-   **Task**: Compare the two penalty formulations and evaluate the correctness of the given statements.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is set in the standard framework of nonlinear constrained optimization. The functions, assumptions, and methods described are canonical subjects in the study of computational optimization and engineering design.\n\n-   **Scientifically Grounded**: The problem deals with two fundamental penalty function methods: the quadratic penalty and the non-smooth $L_1$ exact penalty. All concepts, such as KKT conditions, LICQ, and SOSC, are cornerstone principles of optimization theory. The problem is scientifically rigorous.\n-   **Well-Posed**: The problem is well-defined. It asks for a qualitative and theoretical comparison of standard methods under standard assumptions. The conditions provided (smoothness, LICQ, SOSC) are precisely what is needed to guarantee the existence of a unique Lagrange multiplier $\\lambda^\\star$ and local stability of the solution $x^\\star$, which are essential for the analysis of both penalty methods. A unique and meaningful analysis is possible.\n-   **Objective**: The language is technical and unambiguous.\n\nThe problem statement has no scientific or logical flaws. It is a valid theoretical question in the field of numerical optimization.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full analysis will be performed.\n\n### Solution Derivation\n\nThe problem requires an analysis of two distinct penalty methods for solving the equality-constrained problem:\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{subject to} \\quad h(x) = 0\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions at a local minimizer $x^\\star$ state that there exists a Lagrange multiplier $\\lambda^\\star \\in \\mathbb{R}$ such that:\n$$\n\\nabla f(x^\\star) + \\lambda^\\star \\nabla h(x^\\star) = 0\n$$\n$$\nh(x^\\star) = 0\n$$\nThe provided assumptions (LICQ and SOSC) ensure that $x^\\star$ is a regular point and a strict local minimizer.\n\nLet us analyze each penalty formulation.\n\n**1. Quadratic Penalty Function: $F_{\\mathrm{QP}}(x;\\rho) = f(x) + \\rho\\,h(x)^2$**\n\n-   **Smoothness**: The problem states that $f$ and $h$ are continuously differentiable ($C^1$). The function $z \\mapsto z^2$ is infinitely differentiable ($C^\\infty$). The composition and sum of $C^1$ functions are $C^1$. Thus, $F_{\\mathrm{QP}}(x;\\rho)$ is continuously differentiable. If $f$ and $h$ are assumed to be twice continuously differentiable ($C^2$), then $F_{\\mathrm{QP}}(x;\\rho)$ is also $C^2$. Therefore, it is a smooth function, and standard unconstrained optimization algorithms (like Newton's method) can be applied.\n\n-   **Convergence and Exactness**: Let $x(\\rho)$ be a minimizer of $F_{\\mathrm{QP}}(x;\\rho)$. The first-order necessary condition is $\\nabla F_{\\mathrm{QP}}(x(\\rho);\\rho) = 0$, which gives:\n    $$\n    \\nabla f(x(\\rho)) + 2\\rho h(x(\\rho)) \\nabla h(x(\\rho)) = 0\n    $$\n    It is a standard result that as $\\rho \\to \\infty$, the sequence of minimizers $\\{x(\\rho)\\}$ converges to the constrained minimizer $x^\\star$. Comparing the optimality condition for $F_{\\mathrm{QP}}$ with the KKT condition, we can identify $2\\rho h(x(\\rho))$ as an approximation for the Lagrange multiplier $-\\lambda^\\star$. Thus, $h(x(\\rho)) \\approx -\\lambda^\\star / (2\\rho)$. For any finite $\\rho > 0$, we have $h(x(\\rho)) \\neq 0$ (unless $\\lambda^\\star=0$, a trivial case). This means the method is not *exact*; feasibility is achieved only in the limit $\\rho \\to \\infty$.\n\n-   **Conditioning**: The Hessian of $F_{\\mathrm{QP}}(x;\\rho)$ is:\n    $$\n    \\nabla^2 F_{\\mathrm{QP}}(x;\\rho) = \\nabla^2 f(x) + 2\\rho \\nabla h(x) \\nabla h(x)^T + 2\\rho h(x) \\nabla^2 h(x)\n    $$\n    As $\\rho \\to \\infty$, we have $x(\\rho) \\to x^\\star$ and $2\\rho h(x(\\rho)) \\to -\\lambda^\\star$. The Hessian at $x(\\rho)$ becomes:\n    $$\n    \\nabla^2 F_{\\mathrm{QP}}(x(\\rho);\\rho) \\approx \\left( \\nabla^2 f(x^\\star) + \\lambda^\\star \\nabla^2 h(x^\\star) \\right) + 2\\rho \\nabla h(x^\\star) \\nabla h(x^\\star)^T\n    $$\n    The term in the parentheses is the Hessian of the Lagrangian $\\nabla_{xx}^2 \\mathcal{L}(x^\\star, \\lambda^\\star)$. The second term, $2\\rho \\nabla h(x^\\star) \\nabla h(x^\\star)^T$, is a rank-one matrix whose norm grows linearly with $\\rho$. This term causes one eigenvalue of the Hessian matrix to approach infinity in the direction of $\\nabla h(x^\\star)$, while eigenvalues in directions orthogonal to $\\nabla h(x^\\star)$ remain bounded. Consequently, the condition number of $\\nabla^2 F_{\\mathrm{QP}}$ diverges as $\\rho \\to \\infty$, leading to severe numerical ill-conditioning.\n\n**2. $L_1$ Penalty Function: $F_{L_1}(x;\\rho) = f(x) + \\rho\\,|h(x)|$**\n\n-   **Smoothness**: The absolute value function $|z|$ is not differentiable at $z=0$. Therefore, $F_{L_1}(x;\\rho)$ is non-differentiable at all points $x$ where $h(x)=0$, which is precisely the feasible set of the original problem. This non-smoothness prevents the direct application of classical gradient-based methods like Newton's method that require differentiability.\n\n-   **Exactness**: This function is known as an *exact* penalty function. A key result in optimization theory states that if $x^\\star$ is a strict local minimizer satisfying the SOSC, there exists a threshold value $\\bar{\\rho}$ such that for all $\\rho > \\bar{\\rho}$, $x^\\star$ is also a strict local minimizer of $F_{L_1}(x;\\rho)$. The threshold is related to the magnitude of the Lagrange multiplier: one must choose $\\rho > |\\lambda^\\star|$. Thus, one can find the exact constrained solution by solving a single unconstrained (but non-smooth) minimization problem for a sufficiently large, but finite, $\\rho$.\n\n### Option-by-Option Analysis\n\n**A. If $f$ and $h$ are smooth, then $F_{\\mathrm{QP}}(x;\\rho)$ is smooth for all $x$ and any $\\rho>0$, but enforcing feasibility typically requires $\\rho \\to \\infty$, which can cause ill-conditioning of the Hessian near feasible points.**\n-   As established in our analysis, $F_{\\mathrm{QP}}(x;\\rho)$ is smooth.\n-   As established, feasibility is only obtained in the limit $\\rho \\to \\infty$.\n-   As established, the Hessian becomes ill-conditioned as $\\rho \\to \\infty$.\n-   **Verdict: Correct.**\n\n**B. The $L_1$ penalty can be exact for equality constraints: under standard regularity, there exists a finite $\\bar{\\rho} \\ge |\\lambda^\\star|$ such that the constrained local minimizer $x^\\star$ with Karush–Kuhn–Tucker (KKT) multiplier $\\lambda^\\star$ is also a local minimizer of $F_{L_1}(x;\\rho)$ for all $\\rho \\ge \\bar{\\rho}$; however, $F_{L_1}(x;\\rho)$ is nonsmooth at points where $h(x)=0$.**\n-   The first part of the statement correctly defines the property of an exact penalty function, with the threshold condition on $\\rho$ correctly related to the Lagrange multiplier $\\lambda^\\star$. The problem assumes \"standard regularity\" (LICQ, SOSC), which is what is required for this theorem to hold. Note that the condition is often stated as strict inequality, $\\rho > |\\lambda^\\star|$, for $x^\\star$ to be a *strict* local minimizer, but the statement with $\\rho \\ge |\\lambda^\\star|$ for a local minimizer is valid.\n-   The second part correctly identifies that the function is nonsmooth on the feasible set, where $h(x)=0$.\n-   **Verdict: Correct.**\n\n**C. Because $F_{L_1}(x;\\rho)$ lacks differentiability only away from feasibility, Newton’s method applied directly to $F_{L_1}(x;\\rho)$ enjoys quadratic convergence near feasible points.**\n-   The premise \"$F_{L_1}(x;\\rho)$ lacks differentiability only away from feasibility\" is factually incorrect. The non-differentiability occurs precisely *on* the feasible set (where $h(x)=0$), not away from it. Away from feasibility, where $h(x) \\neq 0$, the function is locally smooth.\n-   Because the solution $x^\\star$ lies on the non-differentiable manifold, standard Newton's method, which requires the Hessian, is not applicable. Nonsmooth optimization techniques are required. The conclusion about quadratic convergence is therefore baseless.\n-   **Verdict: Incorrect.**\n\n**D. Quadratic penalty methods avoid ill-conditioning as $\\rho$ increases because the Hessian of $F_{\\mathrm{QP}}(x;\\rho)$ remains uniformly bounded in $\\rho$.**\n-   This statement is the direct opposite of the truth. As demonstrated in the analysis of $F_{\\mathrm{QP}}$, the Hessian $\\nabla^2 F_{\\mathrm{QP}}$ contains the term $2\\rho \\nabla h(x) \\nabla h(x)^T$, which grows unboundedly with $\\rho$. This causes severe ill-conditioning. The Hessian does not remain uniformly bounded.\n-   **Verdict: Incorrect.**\n\n**E. When $h(x)$ is linear, the quadratic and $L_1$ penalties are equivalent in the sense that for any fixed $\\rho>0$ they produce identical minimizers for all smooth $f$.**\n-   This claim is false. The fundamental difference between a quadratic ($h^2$) and an absolute value ($|h|$) penalty term does not vanish just because $h(x)$ is linear. Let us consider the simple problem: minimize $f(x)=x^2$ subject to $h(x)=x-1=0$. The solution is $x^\\star=1$.\n-   The quadratic penalty minimizer is $x(\\rho) = (x^2 + \\rho(x-1)^2)' = 2x+2\\rho(x-1)=0 \\implies x=\\frac{\\rho}{1+\\rho}$.\n-   The $L_1$ penalty minimizer for a sufficiently large $\\rho$ (specifically $\\rho \\ge |\\lambda^\\star|=2$) is the exact solution $x=1$.\n-   For any $\\rho > 0$, $\\frac{\\rho}{1+\\rho} \\neq 1$. For example, if $\\rho=3$, the $F_{\\mathrm{QP}}$ minimizer is $x=3/4$, while the $F_{L_1}$ minimizer is $x=1$. The minimizers are not identical.\n-   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}