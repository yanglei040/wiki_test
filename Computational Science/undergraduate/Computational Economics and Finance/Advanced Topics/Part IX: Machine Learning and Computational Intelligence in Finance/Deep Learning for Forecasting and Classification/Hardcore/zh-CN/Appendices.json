{
    "hands_on_practices": [
        {
            "introduction": "理解神经网络的第一步是追踪信息从输入到输出的流动过程。本练习将通过一个具体的分类任务，让你亲手完成一次完整的“前向传播”计算，直观地感受一个预训练模型是如何做出决策的。",
            "id": "2387288",
            "problem": "您将处理一个二元分类任务，该任务的背景源于计算经济学和金融领域的公司治理分析。每个样本代表公司行为准则的一个部分，该部分被编码为一个实值特征向量，该向量概括了每个部分中概念类别的归一化频率。目标是分类判断一个部分是否表明内部控制存在潜在弱点。\n\n模型定义：\n- 设输入为一个向量 $x \\in \\mathbb{R}^{8}$，其分量是以下八个概念类别的归一化频率（无单位，在 $[0,1]$ 范围内），顺序如下：举报人报告清晰度、礼品政策严格性、利益冲突控制、审计委员会独立性、职责分离、关联方交易监督、豁免或例外频率、模糊语言频率。\n- 考虑一个带有一个隐藏层的前馈模型，该模型使用修正线性单元（ReLU）作为激活函数，并具有逻辑输出。定义\n$$\nh(x) = \\phi\\!\\left(W_1 x + b_1\\right) \\in \\mathbb{R}^{4}, \\quad \\phi(z)_i = \\max\\{0, z_i\\},\n$$\n$$\nz_2(x) = W_2^\\top h(x) + b_2 \\in \\mathbb{R}, \\quad p(x) = \\sigma\\!\\left(z_2(x)\\right) = \\frac{1}{1 + e^{-z_2(x)}} \\in (0,1).\n$$\n类别预测为 $\\hat{y}(x) = 1$ 如果 $p(x) \\geq 0.5$，否则为 $\\hat{y}(x) = 0$。\n- 参数是固定的，由以下给出\n$$\nW_1 =\n\\begin{bmatrix}\n-0.1  -0.1  -0.1  -0.1  -0.1  -0.1  0.6  0.6 \\\\\n0.4  0.4  0.3  0.3  0.3  0.3  -0.2  -0.2 \\\\\n0  0  0  0  0  0  0  0.8 \\\\\n0  0  0  0  0  0  0.8  0\n\\end{bmatrix},\n\\quad\nb_1 =\n\\begin{bmatrix}\n-0.1 \\\\ -0.05 \\\\ -0.2 \\\\ -0.2\n\\end{bmatrix},\n$$\n$$\nW_2 =\n\\begin{bmatrix}\n0.8 \\\\ -0.6 \\\\ 0.5 \\\\ 0.5\n\\end{bmatrix},\n\\quad\nb_2 = 0.\n$$\n\n任务：\n- 对于下面测试集中的每个测试输入 $x$，使用上述模型计算 $p(x)$ 和类别预测 $\\hat{y}(x)$。您必须只输出整数形式的类别预测。\n\n测试集（每个 $x$ 是 $\\mathbb{R}^{8}$ 中的一个元素，其条目在 $[0,1]$ 范围内）：\n- $x^{(1)} = [\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.0 \\,]$\n- $x^{(2)} = [\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.9,\\, 0.9 \\,]$\n- $x^{(3)} = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]$\n- $x^{(4)} = [\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5 \\,]$\n- $x^{(5)} = [\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 1.0,\\, 0.0 \\,]$\n\n覆盖设计：\n- 该测试集包括一个具有强控制信号且无风险信号的案例，一个具有弱控制和强风险信号的案例，一个输入为零向量的边界案例，一个混合的中等案例，以及一个具有低控制且单个风险因素被激活的案例。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，且无空格。例如，如果五个预测是 $\\hat{y}^{(1)},\\ldots,\\hat{y}^{(5)} \\in \\{0,1\\}$，请精确打印“[y1,y2,y3,y4,y5]”。",
            "solution": "问题陈述经过验证。\n\n**步骤 1：提取给定条件**\n\n- **输入**：一个特征向量 $x \\in \\mathbb{R}^{8}$，其分量在 $[0,1]$ 范围内。\n- **模型架构**：一个单隐藏层前馈网络。\n- **隐藏层**：\n  $$h(x) = \\phi\\!\\left(W_1 x + b_1\\right) \\in \\mathbb{R}^{4}$$\n  其中 $\\phi(z)_i = \\max\\{0, z_i\\}$ 是修正线性单元（ReLU）激活函数。\n- **输出层**：\n  $$z_2(x) = W_2^\\top h(x) + b_2 \\in \\mathbb{R}$$\n  $$p(x) = \\sigma\\!\\left(z_2(x)\\right) = \\frac{1}{1 + e^{-z_2(x)}} \\in (0,1)$$\n- **分类规则**：\n  $$\\hat{y}(x) = 1 \\text{ if } p(x) \\geq 0.5$$\n  $$\\hat{y}(x) = 0 \\text{ if } p(x)  0.5$$\n- **参数**：\n  $$\n  W_1 =\n  \\begin{bmatrix}\n  -0.1  -0.1  -0.1  -0.1  -0.1  -0.1  0.6  0.6 \\\\\n  0.4  0.4  0.3  0.3  0.3  0.3  -0.2  -0.2 \\\\\n  0  0  0  0  0  0  0  0.8 \\\\\n  0  0  0  0  0  0  0.8  0\n  \\end{bmatrix},\n  \\quad\n  b_1 =\n  \\begin{bmatrix}\n  -0.1 \\\\ -0.05 \\\\ -0.2 \\\\ -0.2\n  \\end{bmatrix}\n  $$\n  $$\n  W_2 =\n  \\begin{bmatrix}\n  0.8 \\\\ -0.6 \\\\ 0.5 \\\\ 0.5\n  \\end{bmatrix},\n  \\quad\n  b_2 = 0\n  $$\n- **测试集**：\n  - $x^{(1)} = [\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.0 \\,]^\\top$\n  - $x^{(2)} = [\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.9,\\, 0.9 \\,]^\\top$\n  - $x^{(3)} = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]^\\top$\n  - $x^{(4)} = [\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5 \\,]^\\top$\n  - $x^{(5)} = [\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 1.0,\\, 0.0 \\,]^\\top$\n- **任务**：为测试集中的每个输入向量计算 $\\hat{y}(x)$。\n\n**步骤 2：使用提取的给定条件进行验证**\n\n根据既定标准对问题进行评估。\n- **科学依据**：该问题描述了一个标准的前馈神经网络，这是机器学习和深度学习中的一个基本模型。数学运算（矩阵乘法、ReLU激活、逻辑S形函数）是标准的且正确的。应用背景是合理的。\n- **适定性**：问题是完全指定的。所有参数（$W_1, b_1, W_2, b_2$）、输入向量（$x^{(i)}$）和函数（$\\phi, \\sigma$）都已明确定义。所有矩阵和向量的维度都是一致的。例如，$W_1$ 是 $4 \\times 8$，$x$ 是 $8 \\times 1$，产生一个 $4 \\times 1$ 的结果，这与 $b_1$ 的维度相匹配。对于每个测试用例，都存在一个唯一的、稳定的解。\n- **客观性**：问题以精确、定量的术语陈述，没有歧义或主观声明。\n\n**步骤 3：结论与行动**\n\n此问题是**有效的**。它在科学上是合理的，是适定的、客观的，并且不包含矛盾或缺失的信息。可以推导出一个严谨的解。\n\n**求解推导**\n\n分类规则 $\\hat{y}(x) = 1$ 若 $p(x) \\geq 0.5$，等价于基于logit $z_2(x)$ 的符号进行分类。因为逻辑函数 $\\sigma(z)$ 是单调递增的，并且 $\\sigma(0) = 0.5$，所以条件 $p(x) \\geq 0.5$ 等价于 $z_2(x) \\geq 0$。我们将为每个测试用例计算 $z_2(x)$。\n\n计算步骤如下：\n1. 计算隐藏层的预激活向量：$z_1 = W_1 x + b_1$。\n2. 逐分量应用ReLU激活函数：$h = \\phi(z_1) = \\max\\{0, z_1\\}$。\n3. 计算logit（输出的预激活）：$z_2 = W_2^\\top h + b_2$。\n4. 确定类别：如果 $z_2 \\geq 0$，则 $\\hat{y} = 1$；如果 $z_2  0$，则 $\\hat{y} = 0$。\n\n**情况 1： $x^{(1)} = [\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.9,\\, 0.0,\\, 0.0 \\,]^\\top$**\n$z_1^{(1)} = W_1 x^{(1)} + b_1 = \\begin{bmatrix} -0.1(0.9)\\cdot 6 - 0.1 \\\\ (0.4\\cdot 2 + 0.3\\cdot 4)0.9 - 0.05 \\\\ 0 - 0.2 \\\\ 0 - 0.2 \\end{bmatrix} = \\begin{bmatrix} -0.54 - 0.1 \\\\ 1.8 - 0.05 \\\\ -0.2 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} -0.64 \\\\ 1.75 \\\\ -0.2 \\\\ -0.2 \\end{bmatrix}$\n$h^{(1)} = \\phi(z_1^{(1)}) = [\\, 0, 1.75, 0, 0 \\,]^\\top$\n$z_2^{(1)} = W_2^\\top h^{(1)} + b_2 = 0.8(0) - 0.6(1.75) + 0.5(0) + 0.5(0) + 0 = -1.05$\n由于 $z_2^{(1)} = -1.05  0$，预测为 $\\hat{y}^{(1)} = 0$。\n\n**情况 2： $x^{(2)} = [\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.1,\\, 0.9,\\, 0.9 \\,]^\\top$**\n$z_1^{(2)} = W_1 x^{(2)} + b_1 = \\begin{bmatrix} -0.1(0.1)\\cdot 6 + 0.6(0.9) + 0.6(0.9) - 0.1 \\\\ (0.4\\cdot 2 + 0.3\\cdot 4)0.1 + (-0.2)(0.9) + (-0.2)(0.9) - 0.05 \\\\ 0.8(0.9) - 0.2 \\\\ 0.8(0.9) - 0.2 \\end{bmatrix} = \\begin{bmatrix} -0.06 + 1.08 - 0.1 \\\\ 0.2 - 0.36 - 0.05 \\\\ 0.72 - 0.2 \\\\ 0.72 - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.92 \\\\ -0.21 \\\\ 0.52 \\\\ 0.52 \\end{bmatrix}$\n$h^{(2)} = \\phi(z_1^{(2)}) = [\\, 0.92, 0, 0.52, 0.52 \\,]^\\top$\n$z_2^{(2)} = W_2^\\top h^{(2)} + b_2 = 0.8(0.92) - 0.6(0) + 0.5(0.52) + 0.5(0.52) + 0 = 0.736 + 0.26 + 0.26 = 1.256$\n由于 $z_2^{(2)} = 1.256 > 0$，预测为 $\\hat{y}^{(2)} = 1$。\n\n**情况 3： $x^{(3)} = [\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0 \\,]^\\top$**\n$z_1^{(3)} = W_1 x^{(3)} + b_1 = W_1 \\cdot 0 + b_1 = b_1 = [\\, -0.1, -0.05, -0.2, -0.2 \\,]^\\top$\n$h^{(3)} = \\phi(z_1^{(3)}) = [\\, 0, 0, 0, 0 \\,]^\\top$\n$z_2^{(3)} = W_2^\\top h^{(3)} + b_2 = W_2^\\top \\cdot 0 + 0 = 0$\n由于 $z_2^{(3)} = 0$，条件 $z_2 \\geq 0$ 满足。预测为 $\\hat{y}^{(3)} = 1$。\n\n**情况 4： $x^{(4)} = [\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5,\\, 0.5 \\,]^\\top$**\n$z_1^{(4)} = W_1 x^{(4)} + b_1 = \\begin{bmatrix} (-0.1 \\cdot 6 + 0.6 \\cdot 2) \\cdot 0.5 - 0.1 \\\\ (0.4 \\cdot 2 + 0.3 \\cdot 4 - 0.2 \\cdot 2) \\cdot 0.5 - 0.05 \\\\ (0.8) \\cdot 0.5 - 0.2 \\\\ (0.8) \\cdot 0.5 - 0.2 \\end{bmatrix} = \\begin{bmatrix} (0.6) \\cdot 0.5 - 0.1 \\\\ (1.6) \\cdot 0.5 - 0.05 \\\\ 0.4 - 0.2 \\\\ 0.4 - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.2 \\\\ 0.75 \\\\ 0.2 \\\\ 0.2 \\end{bmatrix}$\n$h^{(4)} = \\phi(z_1^{(4)}) = [\\, 0.2, 0.75, 0.2, 0.2 \\,]^\\top$\n$z_2^{(4)} = W_2^\\top h^{(4)} + b_2 = 0.8(0.2) - 0.6(0.75) + 0.5(0.2) + 0.5(0.2) + 0 = 0.16 - 0.45 + 0.1 + 0.1 = -0.09$\n由于 $z_2^{(4)} = -0.09  0$，预测为 $\\hat{y}^{(4)} = 0$。\n\n**情况 5： $x^{(5)} = [\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 0.2,\\, 1.0,\\, 0.0 \\,]^\\top$**\n$z_1^{(5)} = W_1 x^{(5)} + b_1 = \\begin{bmatrix} (-0.1 \\cdot 6) \\cdot 0.2 + 0.6(1.0) - 0.1 \\\\ (0.4 \\cdot 2 + 0.3 \\cdot 4) \\cdot 0.2 - 0.2(1.0) - 0.05 \\\\ 0 - 0.2 \\\\ 0.8(1.0) - 0.2 \\end{bmatrix} = \\begin{bmatrix} -0.12 + 0.6 - 0.1 \\\\ 0.4 - 0.2 - 0.05 \\\\ -0.2 \\\\ 0.8 - 0.2 \\end{bmatrix} = \\begin{bmatrix} 0.38 \\\\ 0.15 \\\\ -0.2 \\\\ 0.6 \\end{bmatrix}$\n$h^{(5)} = \\phi(z_1^{(5)}) = [\\, 0.38, 0.15, 0, 0.6 \\,]^\\top$\n$z_2^{(5)} = W_2^\\top h^{(5)} + b_2 = 0.8(0.38) - 0.6(0.15) + 0.5(0) + 0.5(0.6) + 0 = 0.304 - 0.09 + 0.3 = 0.514$\n由于 $z_2^{(5)} = 0.514 > 0$，预测为 $\\hat{y}^{(5)} = 1$。\n\n最终的预测列表是 $[0, 1, 1, 0, 1]$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the class predictions for a given set of input vectors\n    using a predefined one-hidden-layer neural network.\n    \"\"\"\n    \n    # Define the model parameters as numpy arrays.\n    W1 = np.array([\n        [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.6, 0.6],\n        [0.4, 0.4, 0.3, 0.3, 0.3, 0.3, -0.2, -0.2],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8],\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0]\n    ])\n\n    b1 = np.array([-0.1, -0.05, -0.2, -0.2])\n\n    W2 = np.array([0.8, -0.6, 0.5, 0.5])\n    \n    # b2 is given as 0.0\n    b2 = 0.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.0, 0.0]),\n        np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.9, 0.9]),\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n        np.array([0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 1.0, 0.0]),\n    ]\n\n    results = []\n    for x in test_cases:\n        # Step 1: Calculate the pre-activation of the hidden layer.\n        # W1 is (4, 8), x is (8,). The result z1 is (4,).\n        z1 = W1 @ x + b1\n\n        # Step 2: Apply the ReLU activation function.\n        # np.maximum performs an element-wise maximum.\n        h = np.maximum(0, z1)\n\n        # Step 3: Calculate the pre-activation of the output layer (logit).\n        # W2.T is (4,), h is (4,). The result z2 is a scalar.\n        z2 = W2.T @ h + b2\n\n        # Step 4: Determine the class prediction.\n        # The classification rule is y_hat = 1 if p(x) = 0.5, which is equivalent\n        # to z2(x) = 0.\n        y_hat = 1 if z2 = 0 else 0\n        \n        results.append(y_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个模型是如何“学习”出最优参数的？这项实践将引导你从第一性原理出发，推导并实现基于梯度的优化算法，为一个多类别分类问题从零开始训练一个线性模型。",
            "id": "2387257",
            "problem": "给定一个简化的、完全指定的、列表排序分类问题，该问题旨在建模以下问题：在给定时间，基于一系列经济和政治整合的数值指标，从一组有限的候选国家中，预测哪一个最有可能成为下一个加入特定贸易集团的国家。每个候选国家由一个包含标准化整合度量的定长特征向量表示。您将把此任务作为一个概率分类任务来处理，使用一个单层模型。该模型为每个候选者分配一个标量分数，将分数转换为候选集合上的概率分布，并通过最大化观测到的历史结果的联合似然来进行训练。\n\n从以下基本原理出发：给定一组针对 $n$ 个互斥结果的分数 $\\{s_k\\}_{k=1}^{n}$，分类概率分布由归一化指数映射（softmax）定义，参数通过最大化似然（等价于最小化负对数似然）来估计。目标函数应包含对参数的标准 $\\ell_2$ 惩罚项。训练算法应使用基于第一性原理推导的梯度优化方法。\n\n数学设定：\n\n- 对于每个具有 $n_t$ 个候选国家的训练事件 $t$，有一个特征矩阵 $X_t \\in \\mathbb{R}^{n_t \\times d}$，其中每一行 $x_{t,k} \\in \\mathbb{R}^{d}$ 代表候选者 $k$ 的 $d$ 维特征向量，以及一个观测索引 $y_t \\in \\{0,1,\\dots,n_t-1\\}$，表示在该事件中实际下一个加入的国家。\n- 使用一个单层线性评分模型 $s_{t,k} = w^\\top x_{t,k} + b$，其参数为 $w \\in \\mathbb{R}^{d}$ 和 $b \\in \\mathbb{R}$，并通过对 $n_t$ 个候选者的 softmax 映射来定义事件 $t$ 的模型隐含概率。\n- 所有训练事件的总目标函数是负对数似然之和加上对 $w$ 的 $\\ell_2$ 惩罚项（系数为 $\\lambda  0$），不对 $b$ 进行正则化。\n\n您的任务：\n\n1) 从上述基本原理出发，明确地从似然定义和链式法则出发，推导出实现参数 $w$ 和 $b$ 的基于梯度的学习规则所需的表达式，不走捷径。\n\n2) 使用具有固定步长的批量梯度下降法实现一个训练程序。对 softmax 使用数值稳定的计算方法。训练过程必须是确定性的，不得依赖任何随机性。\n\n3) 在下面指定的训练集上进行训练后，通过计算每个测试案例中预测概率最高的候选者的索引（从零开始），在提供的测试集（训练中未使用的候选集）上评估训练好的模型。\n\n数据规格：\n\n- 每个特征向量的维度：$d = 5$。\n- 特征语义和范围（仅供解释；程序会将其视为纯数字）：\n  - $x_1$：与集团的标准化贸易份额，范围在 $[0,1]$，\n  - $x_2$：标准化关税政策整合度，范围在 $[0,1]$，\n  - $x_3$：标准化人均国内生产总值整合度，范围在 $[0,1]$，\n  - $x_4$：标准化民主与治理相似度，范围在 $[0,1]$，\n  - $x_5$：标准化外交政策投票相似度，范围在 $[0,1]$。\n\n训练事件（每个事件是一个候选者矩阵，后跟观测到的加入索引）：\n- 事件 $1$ ($n_1 = 3$)：\n  - 候选者：\n    - $[0.60, 0.70, 0.50, 0.60, 0.65]$\n    - $[0.40, 0.50, 0.60, 0.50, 0.45]$\n    - $[0.80, 0.80, 0.70, 0.70, 0.80]$\n  - 观测索引 $y_1 = 2$。\n- 事件 $2$ ($n_2 = 2$)：\n  - 候选者：\n    - $[0.30, 0.40, 0.50, 0.40, 0.40]$\n    - $[0.70, 0.60, 0.60, 0.80, 0.70]$\n  - 观测索引 $y_2 = 1$。\n- 事件 $3$ ($n_3 = 4$)：\n  - 候选者：\n    - $[0.20, 0.30, 0.40, 0.40, 0.30]$\n    - $[0.50, 0.60, 0.50, 0.50, 0.60]$\n    - $[0.90, 0.90, 0.90, 0.80, 0.85]$\n    - $[0.70, 0.50, 0.70, 0.60, 0.65]$\n  - 观测索引 $y_3 = 2$。\n\n超参数：\n- 学习率 $\\eta = 0.10$，\n- 正则化系数 $\\lambda = 10^{-3}$，\n- 全批量迭代次数（轮次）$T = 2000$。\n\n测试集（用于评估的候选集；每个测试案例输出一个整数）：\n- 测试案例 $1$ ($n = 3$)：\n  - $[0.65, 0.70, 0.55, 0.60, 0.60]$\n  - $[0.50, 0.50, 0.55, 0.45, 0.50]$\n  - $[0.75, 0.80, 0.75, 0.75, 0.82]$\n- 测试案例 $2$ ($n = 2$)：\n  - $[0.35, 0.45, 0.55, 0.50, 0.48]$\n  - $[0.60, 0.55, 0.50, 0.70, 0.60]$\n- 测试案例 $3$ ($n = 4$)：\n  - $[0.55, 0.60, 0.60, 0.60, 0.60]$\n  - $[0.60, 0.55, 0.55, 0.55, 0.55]$\n  - $[0.65, 0.65, 0.65, 0.65, 0.65]$\n  - $[0.50, 0.50, 0.50, 0.50, 0.50]$\n- 测试案例 $4$ ($n = 3$)：\n  - $[0.00, 0.00, 0.00, 0.00, 0.00]$\n  - $[0.20, 0.20, 0.20, 0.20, 0.20]$\n  - $[0.10, 0.10, 0.10, 0.10, 0.10]$\n\n最终输出规格：\n- 您的程序应使用指定的超参数在提供的训练事件上进行训练，然后生成一行输出，其中包含四个测试案例的预测索引（从零开始），格式为一个逗号分隔的列表，并用方括号括起来，例如 $[a,b,c,d]$，其中 $a、b、c、d$ 是整数，取值范围为 $\\{0,1,2,3\\}$，具体取决于测试案例的大小。",
            "solution": "所呈现的问题是一个定义明确、具有科学依据的分类任务，可以使用标准的多项逻辑回归模型来解决。其有效性已得到确认，并从第一性原理推导出解决方案。\n\n核心任务是训练一个带有参数 $w \\in \\mathbb{R}^{d}$ 和 $b \\in \\mathbb{R}$ 的线性模型，以预测一组候选国家中哪一个最有可能加入贸易集团。训练通过使用批量梯度下降法最小化一个正则化的负对数似然目标函数来执行。\n\n设有 $N$ 个训练事件，以 $t \\in \\{1, 2, \\dots, N\\}$ 索引。对于每个事件 $t$，我们有一组 $n_t$ 个候选国家，其特征由一个矩阵 $X_t \\in \\mathbb{R}^{n_t \\times d}$ 表示。每一行 $x_{t,k} \\in \\mathbb{R}^{d}$ 是候选者 $k \\in \\{0, 1, \\dots, n_t-1\\}$ 的特征向量。观测结果是加入国家的索引 $y_t$。\n\n事件 $t$ 中候选者 $k$ 的分数由一个线性函数给出：\n$$s_{t,k} = w^\\top x_{t,k} + b$$\n\n候选者 $k$ 被选中的概率由 softmax 函数建模，该函数将分数归一化为 $n_t$ 个候选者上的概率分布：\n$$P_{t,k} \\equiv P(y=k | X_t, w, b) = \\frac{\\exp(s_{t,k})}{\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})}$$\n\n目标是通过最大化观测到训练数据 $\\{y_t\\}_{t=1}^N$ 的联合似然来找到参数 $w$ 和 $b$。这等价于最小化总负对数似然 $L_{NLL}$。我们为权重向量 $w$ 添加一个 $\\ell_2$ 正则化项以防止过拟合。总目标函数 $L(w,b)$ 为：\n$$L(w, b) = L_{NLL} + L_{REG} = \\left( \\sum_{t=1}^{N} L_t(w, b) \\right) + \\frac{\\lambda}{2} \\|w\\|^2_2$$\n其中 $\\lambda  0$ 是正则化系数，$L_t$ 是单个事件 $t$ 的负对数似然：\n$$L_t(w,b) = -\\log(P_{t, y_t})$$\n代入分数和概率的表达式，我们可以将 $L_t$ 写为：\n$$L_t = -\\log\\left(\\frac{\\exp(s_{t,y_t})}{\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})}\\right) = -s_{t,y_t} + \\log\\left(\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})\\right)$$\n\n为了使用梯度下降法训练模型，我们必须计算总目标函数 $L$ 关于参数 $w$ 和 $b$ 的偏导数。参数 $\\theta$ 的梯度更新规则是 $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} L$，其中 $\\eta$ 是学习率。\n\n首先，我们推导关于权重向量 $w$ 的梯度。正则化项的梯度是直截了当的：\n$$\\frac{\\partial}{\\partial w} \\left(\\frac{\\lambda}{2} w^\\top w\\right) = \\lambda w$$\n负对数似然的梯度是每个事件梯度的总和：$\\frac{\\partial L_{NLL}}{\\partial w} = \\sum_{t=1}^N \\frac{\\partial L_t}{\\partial w}$。让我们使用链式法则来求单个事件 $t$ 的梯度。对于 $w$ 的第 $i$ 个分量，记作 $w_i$：\n$$\\frac{\\partial L_t}{\\partial w_i} = \\sum_{k=0}^{n_t-1} \\frac{\\partial L_t}{\\partial s_{t,k}} \\frac{\\partial s_{t,k}}{\\partial w_i}$$\n分数 $s_{t,k}$ 关于 $w_i$ 的导数是：\n$$\\frac{\\partial s_{t,k}}{\\partial w_i} = \\frac{\\partial}{\\partial w_i} \\left(\\sum_{l=1}^{d} w_l (x_{t,k})_l + b\\right) = (x_{t,k})_i$$\n损失 $L_t$ 关于分数 $s_{t,k}$ 的导数是：\n$$\\frac{\\partial L_t}{\\partial s_{t,k}} = \\frac{\\partial}{\\partial s_{t,k}} \\left(-s_{t,y_t} + \\log\\left(\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})\\right)\\right) = -\\delta_{k, y_t} + \\frac{\\exp(s_{t,k})}{\\sum_{j=0}^{n_t-1} \\exp(s_{t,j})} = P_{t,k} - \\delta_{k, y_t}$$\n其中 $\\delta_{k, y_t}$ 是克罗内克δ函数（Kronecker delta），如果 $k=y_t$ 则为 $1$，否则为 $0$。\n\n综合这些部分，$L_t$ 关于 $w_i$ 的梯度是：\n$$\\frac{\\partial L_t}{\\partial w_i} = \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) (x_{t,k})_i$$\n以向量形式表示，事件 $t$ 的梯度是：\n$$\\frac{\\partial L_t}{\\partial w} = \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) x_{t,k} = X_t^\\top (p_t - e_{y_t})$$\n其中 $p_t$ 是概率向量 $\\{P_{t,k}\\}_{k=0}^{n_t-1}$，$e_{y_t}$ 是真实标签 $y_t$ 的独热编码向量（one-hot encoded vector）。\n\n$w$ 的总梯度是所有训练事件的梯度之和加上正则化梯度：\n$$\\nabla_w L(w,b) = \\left( \\sum_{t=1}^{N} \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) x_{t,k} \\right) + \\lambda w$$\n\n接下来，我们推导关于偏置项 $b$ 的梯度。正则化项不依赖于 $b$。我们再次使用链式法则：\n$$\\frac{\\partial L_t}{\\partial b} = \\sum_{k=0}^{n_t-1} \\frac{\\partial L_t}{\\partial s_{t,k}} \\frac{\\partial s_{t,k}}{\\partial b}$$\n分数关于 $b$ 的导数很简单，因为偏置项对每个候选者的分数都有相同的贡献：\n$$\\frac{\\partial s_{t,k}}{\\partial b} = \\frac{\\partial}{\\partial b} (w^\\top x_{t,k} + b) = 1$$\n因此，$L_t$ 关于 $b$ 的梯度是：\n$$\\frac{\\partial L_t}{\\partial b} = \\sum_{k=0}^{n_t-1} (P_{t,k} - \\delta_{k, y_t}) \\cdot 1 = \\left(\\sum_{k=0}^{n_t-1} P_{t,k}\\right) - \\left(\\sum_{k=0}^{n_t-1} \\delta_{k, y_t}\\right) = 1 - 1 = 0$$\n这个结果表明，对于任何单个训练事件，损失函数关于共享偏置项 $b$ 的梯度都恰好为零。因此，$b$ 在所有事件上的总梯度也为零：\n$$\\nabla_b L(w,b) = \\sum_{t=1}^{N} \\frac{\\partial L_t}{\\partial b} = \\sum_{t=1}^{N} 0 = 0$$\n这意味着，在此模型参数化下，偏置项 $b$ 不会通过梯度下降进行更新，并将保持其初始值。优化过程将仅调整权重向量 $w$。\n\n批量梯度下降算法初始化参数 $w$ 和 $b$（例如，初始化为零），并迭代更新它们 $T$ 轮：\n$$w^{(i+1)} = w^{(i)} - \\eta \\nabla_w L(w^{(i)}, b^{(i)})$$\n$$b^{(i+1)} = b^{(i)} - \\eta \\nabla_b L(w^{(i)}, b^{(i)})$$\n为了数值稳定性，softmax 函数通过平移分数来计算：$s_k \\to s_k - \\max_j(s_j)$。这可以防止在对大分数进行指数运算时发生溢出，同时保持最终的概率不变。\n\n在使用学习率 $\\eta=0.10$ 和正则化系数 $\\lambda=10^{-3}$ 进行 $T=2000$ 次迭代后，最终的参数 $(w,b)$ 用于对测试案例进行预测。对于每个测试案例，我们计算所有候选者的分数，并选择分数最高的候选者的索引。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the listwise classification problem using batch gradient descent.\n    \"\"\"\n    # 1. Define problem data and hyperparameters\n    d = 5\n    eta = 0.10\n    lambda_reg = 1e-3\n    T = 2000\n\n    train_events = [\n        (np.array([\n            [0.60, 0.70, 0.50, 0.60, 0.65],\n            [0.40, 0.50, 0.60, 0.50, 0.45],\n            [0.80, 0.80, 0.70, 0.70, 0.80]\n        ]), 2),\n        (np.array([\n            [0.30, 0.40, 0.50, 0.40, 0.40],\n            [0.70, 0.60, 0.60, 0.80, 0.70]\n        ]), 1),\n        (np.array([\n            [0.20, 0.30, 0.40, 0.40, 0.30],\n            [0.50, 0.60, 0.50, 0.50, 0.60],\n            [0.90, 0.90, 0.90, 0.80, 0.85],\n            [0.70, 0.50, 0.70, 0.60, 0.65]\n        ]), 2)\n    ]\n\n    test_cases = [\n        np.array([\n            [0.65, 0.70, 0.55, 0.60, 0.60],\n            [0.50, 0.50, 0.55, 0.45, 0.50],\n            [0.75, 0.80, 0.75, 0.75, 0.82]\n        ]),\n        np.array([\n            [0.35, 0.45, 0.55, 0.50, 0.48],\n            [0.60, 0.55, 0.50, 0.70, 0.60]\n        ]),\n        np.array([\n            [0.55, 0.60, 0.60, 0.60, 0.60],\n            [0.60, 0.55, 0.55, 0.55, 0.55],\n            [0.65, 0.65, 0.65, 0.65, 0.65],\n            [0.50, 0.50, 0.50, 0.50, 0.50]\n        ]),\n        np.array([\n            [0.00, 0.00, 0.00, 0.00, 0.00],\n            [0.20, 0.20, 0.20, 0.20, 0.20],\n            [0.10, 0.10, 0.10, 0.10, 0.10]\n        ])\n    ]\n\n    # 2. Initialize model parameters\n    w = np.zeros(d)\n    b = 0.0\n\n    # 3. Training with Batch Gradient Descent\n    for _ in range(T):\n        grad_w_total = np.zeros(d)\n        grad_b_total = 0.0\n\n        # Loop over all training events to compute the full batch gradient\n        for X_t, y_t in train_events:\n            n_t = X_t.shape[0]\n\n            # Calculate scores: s = Xw + b\n            scores = X_t @ w + b\n            \n            # Numerically stable softmax to compute probabilities\n            scores_stable = scores - np.max(scores)\n            exp_scores = np.exp(scores_stable)\n            probs = exp_scores / np.sum(exp_scores)\n\n            # Create one-hot encoded target vector\n            target = np.zeros(n_t)\n            target[y_t] = 1.0\n            \n            # Calculate the error term (p - y_onehot)\n            error = probs - target\n            \n            # Calculate gradient contribution from this event\n            grad_w_t = X_t.T @ error\n            grad_b_t = np.sum(error)\n            \n            # Accumulate gradients for the batch\n            grad_w_total += grad_w_t\n            grad_b_total += grad_b_t\n            \n        # Add regularization gradient for w (L2 penalty)\n        grad_w_total += lambda_reg * w\n        \n        # Update parameters using the batch gradient\n        w -= eta * grad_w_total\n        b -= eta * grad_b_total\n\n    # 4. Evaluation on test suite\n    results = []\n    for X_test in test_cases:\n        # Calculate scores for the test case\n        scores = X_test @ w + b\n        # Prediction is the index of the highest score\n        predicted_index = np.argmax(scores)\n        results.append(predicted_index)\n\n    # 5. Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在金融等高风险领域，模型的鲁棒性与准确性同样重要。本练习将介绍对抗性攻击的概念，并指导你利用模型梯度寻找能“欺骗”模型的最小输入扰动，以此来量化和评估模型的稳定性。",
            "id": "2387277",
            "problem": "一个自动化的信用评分系统使用前馈神经网络将标准化的申请人特征向量映射到违约概率。您将通过计算每个申请人在沿着损失函数的输入梯度符号方向扰动时，导致模型预测类别发生变化的最小扰动幅度，来评估模型对限制在容许特征域内的小型对抗性扰动的鲁棒性。所有特征都经过预先归一化，位于闭合超立方体域 $[0,1]^d$ 内。\n\n模型定义。设输入维度为 $d=5$。定义一个具有双曲正切激活函数和一个逻辑输出的单隐藏层神经网络：\n- 隐藏层预激活：$z_1 = W_1 x + b_1 \\in \\mathbb{R}^h$，隐藏层大小 $h=4$。\n- 隐藏层激活：$a_1 = \\tanh(z_1)$，逐元素应用。\n- Logit（对数几率）：$z_2 = w_2^\\top a_1 + b_2 \\in \\mathbb{R}$。\n- 违约概率：$p_\\theta(x) = \\sigma(z_2) = \\dfrac{1}{1 + e^{-z_2}} \\in (0,1)$。\n- 预测类别：$\\hat{y}(x) = \\mathbb{1}\\{p_\\theta(x) \\ge 0.5\\}$。\n\n参数是固定且已知的：\n$$\nW_1 =\n\\begin{bmatrix}\n0.6  -0.4  0.2  0.1  -0.3\\\\\n-0.2  0.5  -0.1  0.4  0.2\\\\\n0.3  0.1  0.4  -0.5  0.2\\\\\n-0.1  0.3  0.2  0.2  -0.4\n\\end{bmatrix},\\quad\nb_1 =\n\\begin{bmatrix}\n0.0\\\\\n0.1\\\\\n-0.05\\\\\n0.05\n\\end{bmatrix},\\quad\nw_2 =\n\\begin{bmatrix}\n0.7\\\\\n-0.6\\\\\n0.5\\\\\n-0.4\n\\end{bmatrix},\\quad\nb_2 = 0.1.\n$$\n\n损失和对抗方向。对于给定的真实标签 $y \\in \\{0,1\\}$，使用二元交叉熵损失\n$$\n\\mathcal{L}(x,y) = -\\left[ y \\log p_\\theta(x) + (1-y)\\log\\left(1 - p_\\theta(x)\\right)\\right].\n$$\n令 $g(x,y) = \\nabla_x \\mathcal{L}(x,y)$ 为损失相对于输入的梯度。在 $x$ 处的快速梯度符号法 (Fast Gradient Sign Method, FGSM) 对抗方向是逐元素的符号向量 $\\mathrm{sign}\\!\\left(g(x,y)\\right) \\in \\{-1,0,1\\}^d$。对于一个扰动预算 $\\varepsilon \\ge 0$，定义扰动后的输入\n$$\nx_{\\mathrm{adv}}(\\varepsilon) = \\Pi_{[0,1]^d}\\!\\left(x + \\varepsilon \\cdot \\mathrm{sign}\\!\\left(g(x,y)\\right)\\right),\n$$\n其中 $\\Pi_{[0,1]^d}(\\cdot)$ 表示逐元素应用到盒子 $[0,1]^d$ 上的投影（裁剪）。\n\n沿 FGSM 方向的最小对抗预算。对于下方的每个测试用例 $(x,y)$，定义初始预测类别 $\\hat{y}_0 = \\hat{y}(x)$。在固定边界 $\\varepsilon_{\\max}$ 内，沿 FGSM 方向的最小对抗预算为\n$$\n\\varepsilon^\\star(x,y) = \n\\begin{cases}\n0.0, \\text{若 } \\hat{y}_0 \\ne y,\\\\\n\\inf\\left\\{\\varepsilon \\in [0,\\varepsilon_{\\max}] : \\hat{y}\\!\\left(x_{\\mathrm{adv}}(\\varepsilon)\\right) \\ne \\hat{y}_0 \\right\\},  \\text{其他情况},\n\\end{cases}\n$$\n约定如果不存在这样的 $\\varepsilon \\in [0,\\varepsilon_{\\max}]$，则 $\\varepsilon^\\star(x,y) = \\varepsilon_{\\max}$。\n\n您的任务：\n- 仅从链式法则和以上定义出发，推导出一个关于 $W_1$、$b_1$、$w_2$、$b_2$、$x$ 和 $y$ 的 $\\nabla_x \\mathcal{L}(x,y)$ 的显式且可实现的表达式。\n- 实现一个数值程序，为每个测试用例找到 $\\varepsilon^\\star(x,y)$。该程序使用从粗到精的搜索方法：首先通过在 $[0,\\varepsilon_{\\max}]$ 上进行均匀网格搜索来定位一个包围区间，然后通过二分法进行细化，直到区间宽度低于指定公差。假设映射 $\\varepsilon \\mapsto \\hat{y}(x_{\\mathrm{adv}}(\\varepsilon))$ 是具有有限个跳跃点的分段常数函数。对于每个扰动候选，通过投影精确地强制执行盒子约束 $[0,1]^d$。\n- 使用 $\\varepsilon_{\\max} = 0.5$（无量纲）和搜索公差 $10^{-6}$（无量纲）。粗网格必须包含 $[0,\\varepsilon_{\\max}]$ 中的 $N=512$ 个均匀间隔的点。\n\n测试套件。使用以下 6 个申请人特征向量和标签，其中每个 $x \\in [0,1]^5$ 按 $(\\text{收入}, \\text{负债率}, \\text{年龄}, \\text{信用历史长度}, \\text{拖欠次数})$ 的顺序作为归一化特征给出，而 $y \\in \\{0,1\\}$ 是给定的真实违约指示符：\n$$\nx^{(1)} = \\begin{bmatrix}0.7\\\\ 0.2\\\\ 0.4\\\\ 0.6\\\\ 0.3\\end{bmatrix},\\ y^{(1)} = 1;\\quad\nx^{(2)} = \\begin{bmatrix}0.1\\\\ 0.9\\\\ 0.2\\\\ 0.1\\\\ 0.8\\end{bmatrix},\\ y^{(2)} = 0;\\\\\nx^{(3)} = \\begin{bmatrix}0.5\\\\ 0.5\\\\ 0.5\\\\ 0.5\\\\ 0.5\\end{bmatrix},\\ y^{(3)} = 1;\\quad\nx^{(4)} = \\begin{bmatrix}0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0\\\\ 0.0\\end{bmatrix},\\ y^{(4)} = 0;\\\\\nx^{(5)} = \\begin{bmatrix}1.0\\\\ 1.0\\\\ 1.0\\\\ 1.0\\\\ 1.0\\end{bmatrix},\\ y^{(5)} = 0;\\quad\nx^{(6)} = \\begin{bmatrix}0.9\\\\ 0.1\\\\ 0.8\\\\ 0.2\\\\ 0.1\\end{bmatrix},\\ y^{(6)} = 1.\n$$\n\n输出规范。您的程序应生成单行输出，其中包含六个值 $\\left[\\varepsilon^\\star\\!\\left(x^{(i)},y^{(i)}\\right)\\right]_{i=1}^6$，以逗号分隔列表的形式用方括号括起来，每个值四舍五入到 4 位小数（使用十进制表示法，而非百分比）。例如，一个有效的输出格式为 $[0.0312,0.0000,0.5000,0.0125,0.2210,0.0984]$（这些仅为示例；您的程序必须为给定的模型和测试套件计算实际值）。",
            "solution": "所提出的问题经验证是自包含的、有科学依据且适定的。它构成了神经网络对抗性鲁棒性研究中的一个标准任务。我们现在开始求解，这需要两个部分：首先，损失梯度的解析推导；其次，指定数值搜索程序的实现。\n\n**1. 损失的输入梯度推导**\n\n目标是计算二元交叉熵损失 $\\mathcal{L}(x,y)$ 相对于输入特征向量 $x \\in \\mathbb{R}^d$ 的梯度 $g(x,y) = \\nabla_x \\mathcal{L}(x,y)$。这通过系统地应用链式法则，将导数从标量损失反向传播到输入向量来实现。\n\n模型的前向传播定义了以下函数依赖关系：\n$1.$ $z_1(x) = W_1 x + b_1$\n$2.$ $a_1(z_1) = \\tanh(z_1)$\n$3.$ $z_2(a_1) = w_2^\\top a_1 + b_2$\n$4.$ $p_\\theta(x) = \\sigma(z_2) = (1 + e^{-z_2})^{-1}$\n$5.$ $\\mathcal{L}(x,y) = -\\left[ y \\log p_\\theta(x) + (1-y)\\log\\left(1 - p_\\theta(x)\\right)\\right]$\n\n我们按相反顺序计算所需的偏导数。\n\n首先，损失 $\\mathcal{L}$ 相对于模型输出概率 $p_\\theta(x)$ 的导数：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p_\\theta(x)} = -\\frac{y}{p_\\theta(x)} + \\frac{1-y}{1-p_\\theta(x)} = \\frac{p_\\theta(x) - y}{p_\\theta(x)(1-p_\\theta(x))}\n$$\n\n其次，sigmoid 激活函数 $\\sigma(\\cdot)$ 相对于其输入，即 logit $z_2$ 的导数：\n$$\n\\frac{\\partial p_\\theta(x)}{\\partial z_2} = \\frac{d\\sigma(z_2)}{dz_2} = \\sigma(z_2)(1-\\sigma(z_2)) = p_\\theta(x)(1-p_\\theta(x))\n$$\n结合这两个结果，得到了一个众所周知的、至关重要的简化，即交叉熵损失相对于 logits 的梯度：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_2} = \\frac{\\partial \\mathcal{L}}{\\partial p_\\theta(x)} \\frac{\\partial p_\\theta(x)}{\\partial z_2} = \\left(\\frac{p_\\theta(x) - y}{p_\\theta(x)(1-p_\\theta(x))}\\right) \\left(p_\\theta(x)(1-p_\\theta(x))\\right) = p_\\theta(x) - y\n$$\n\n第三，logit $z_2$ 相对于隐藏层激活向量 $a_1$ 的梯度：\n$$\nz_2 = w_2^\\top a_1 + b_2 = \\sum_{j=1}^{h} (w_2)_j (a_1)_j + b_2 \\implies \\nabla_{a_1} z_2 = w_2\n$$\n\n第四，隐藏层激活向量 $a_1$ 相对于隐藏层预激活向量 $z_1$ 的梯度。由于 $a_1 = \\tanh(z_1)$ 是一个逐元素操作，其雅可比矩阵是对角矩阵：\n$$\n\\frac{\\partial a_1}{\\partial z_1} = \\mathrm{diag}\\left(\\frac{d}{dz_{1,j}} \\tanh(z_{1,j})\\right)_{j=1}^h = \\mathrm{diag}\\left(1 - \\tanh^2(z_{1,j})\\right)_{j=1}^h\n$$\n\n第五，隐藏层预激活向量 $z_1$ 相对于输入向量 $x$ 的梯度：\n$$\nz_1 = W_1 x + b_1 \\implies \\frac{\\partial z_1}{\\partial x} = W_1\n$$\n\n最后，我们使用链式法则组合出完整的梯度 $\\nabla_x \\mathcal{L}(x,y)$：\n$$\n\\nabla_x \\mathcal{L}(x,y)^\\top = \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial z_2} \\frac{\\partial z_2}{\\partial a_1} \\frac{\\partial a_1}{\\partial z_1} \\frac{\\partial z_1}{\\partial x}\n$$\n在向量-矩阵表示法中，对于一个列梯度向量：\n$$\n\\nabla_x \\mathcal{L}(x,y) = \\left(\\frac{\\partial z_1}{\\partial x}\\right)^\\top \\left(\\frac{\\partial a_1}{\\partial z_1}\\right)^\\top \\left(\\frac{\\partial z_2}{\\partial a_1}\\right)^\\top \\frac{\\partial \\mathcal{L}}{\\partial z_2} = W_1^\\top \\mathrm{diag}\\left(1 - \\tanh^2(z_1)\\right) w_2 \\left(p_\\theta(x) - y\\right)\n$$\n为了计算实现，使用逐元素（哈达玛）积 $\\odot$ 可以更高效地表达：\n$$\ng(x,y) = \\nabla_x \\mathcal{L}(x,y) = \\left(p_\\theta(x) - y\\right) W_1^\\top \\left( w_2 \\odot \\left(1 - a_1 \\odot a_1\\right) \\right)\n$$\n其中 $a_1 = \\tanh(W_1 x + b_1)$。这就是梯度的显式且可实现的表达式。\n\n**2. 最小对抗预算的数值程序**\n\n任务是为每个给定的对 $(x,y)$ 找到 $\\varepsilon^\\star(x,y)$。程序如下：\n\n1.  **初始预测和检查**：对于给定的测试用例 $(x,y)$，我们首先计算模型的初始预测 $\\hat{y}_0 = \\hat{y}(x) = \\mathbb{1}\\{p_\\theta(x) \\ge 0.5\\}$。问题定义，如果模型初始预测不正确，即 $\\hat{y}_0 \\ne y$，则所需的最小扰动预算为 $\\varepsilon^\\star(x,y) = 0.0$。\n\n2.  **对抗方向**：如果模型预测正确 ($\\hat{y}_0 = y$)，我们基于快速梯度符号法 (FGSM) 计算对抗方向。这包括使用上面推导的公式计算梯度 $g(x,y) = \\nabla_x \\mathcal{L}(x,y)$，并取其逐元素的符号：$v = \\mathrm{sign}(g(x,y)) \\in \\{-1, 0, 1\\}^d$。\n\n3.  **搜索最小扰动**：目标是找到导致分类翻转的最小 $\\varepsilon \\in [0, \\varepsilon_{\\max}]$。扰动后的输入为 $x_{\\mathrm{adv}}(\\varepsilon) = \\Pi_{[0,1]^d}(x + \\varepsilon v)$，其中 $\\Pi_{[0,1]^d}$ 是为将特征值保持在 $[0,1]$ 超立方体内而进行的逐元素裁剪操作。我们寻求 $\\varepsilon^\\star = \\inf\\{\\varepsilon \\in [0, \\varepsilon_{\\max}] : \\hat{y}(x_{\\mathrm{adv}}(\\varepsilon)) \\ne \\hat{y}_0 \\}$。采用两阶段数值搜索。\n\n    a.  **粗略网格搜索**：我们首先搜索一个包围区间。在 $[0, \\varepsilon_{\\max}]$ 上定义一个包含 $N=512$ 个点的均匀网格。设这些点为 $\\{\\varepsilon_i\\}_{i=0}^{N-1}$。我们从 $i=1$ 开始，为每个 $\\varepsilon_i$ 评估模型的预测 $\\hat{y}(x_{\\mathrm{adv}}(\\varepsilon_i))$。第一个出现 $\\hat{y}(x_{\\mathrm{adv}}(\\varepsilon_i)) \\ne \\hat{y}_0$ 的实例确定了一个包含阈值的包围区间 $[a, b] = [\\varepsilon_{i-1}, \\varepsilon_i]$。如果在 $[0, \\varepsilon_{\\max}]$ 内的任何 $\\varepsilon_i$ 都没有观察到这种翻转，则根据问题定义，最小预算取为 $\\varepsilon^\\star(x,y) = \\varepsilon_{\\max}$。\n\n    b.  **二分法细化**：一旦找到一个区间 $[a, b]$，使得 $\\hat{y}(x_{\\mathrm{adv}}(a)) = \\hat{y}_0$ 且 $\\hat{y}(x_{\\mathrm{adv}}(b)) \\ne \\hat{y}_0$，我们使用二分法来细化阈值的位置。该算法重复地将区间二等分，选择保留其端点预测变化的新子区间。该过程持续进行，直到区间宽度 $(b-a)$ 小于指定的公差 $10^{-6}$。最终的 $b$ 值作为 $\\varepsilon^\\star(x,y)$ 的数值近似。这个值是真实下确界的上界，并随着公差的减小而收敛于它。\n\n这个结构化程序保证了在给定的数值精度下，能够找到沿固定 FGSM 方向的最小扰动预算。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the minimal FGSM adversarial budget for a specified set of test cases.\n    \"\"\"\n    # Model Parameters\n    W1 = np.array([\n        [0.6, -0.4, 0.2, 0.1, -0.3],\n        [-0.2, 0.5, -0.1, 0.4, 0.2],\n        [0.3, 0.1, 0.4, -0.5, 0.2],\n        [-0.1, 0.3, 0.2, 0.2, -0.4]\n    ])\n    b1 = np.array([[0.0], [0.1], [-0.05], [0.05]])\n    w2 = np.array([[0.7], [-0.6], [0.5], [-0.4]])\n    b2 = 0.1\n\n    # Search Hyperparameters\n    eps_max = 0.5\n    N = 512\n    tol = 1e-6\n\n    # Test Cases\n    test_cases = [\n        (np.array([[0.7], [0.2], [0.4], [0.6], [0.3]]), 1),\n        (np.array([[0.1], [0.9], [0.2], [0.1], [0.8]]), 0),\n        (np.array([[0.5], [0.5], [0.5], [0.5], [0.5]]), 1),\n        (np.array([[0.0], [0.0], [0.0], [0.0], [0.0]]), 0),\n        (np.array([[1.0], [1.0], [1.0], [1.0], [1.0]]), 0),\n        (np.array([[0.9], [0.1], [0.8], [0.2], [0.1]]), 1),\n    ]\n\n    def forward(x):\n        \"\"\"Computes the forward pass of the neural network.\"\"\"\n        z1 = W1 @ x + b1\n        a1 = np.tanh(z1)\n        z2 = w2.T @ a1 + b2\n        p = 1.0 / (1.0 + np.exp(-z2))\n        return p.item(), z1, a1, z2.item()\n\n    def predict_from_logit(z2):\n        \"\"\"Predicts class from logit.\"\"\"\n        return 1 if z2 = 0 else 0\n\n    def gradient(y, p_theta, a1):\n        \"\"\"Computes the gradient of the loss with respect to the input.\"\"\"\n        scalar_term = p_theta - y\n        # Elementwise multiplication for grad_z1\n        grad_z1 = scalar_term * w2 * (1.0 - a1**2)\n        # Matrix multiplication for backpropagation to input\n        grad_x = W1.T @ grad_z1\n        return grad_x\n\n    def find_epsilon_star(x, y):\n        \"\"\"\n        Finds the minimal adversarial budget for a single (x, y) pair.\n        \"\"\"\n        p0, _, a1_0, z2_0 = forward(x)\n        y_hat0 = predict_from_logit(z2_0)\n\n        # Per problem definition, if model is already incorrect, eps_star is 0.\n        if y_hat0 != y:\n            return 0.0\n\n        grad_vec = gradient(y, p0, a1_0)\n        v = np.sign(grad_vec)\n\n        # If gradient is zero, no perturbation is applied, so no flip can occur.\n        if np.all(v == 0):\n            return eps_max\n\n        def get_prediction_at_eps(eps):\n            \"\"\"Helper function to get prediction for a perturbed input.\"\"\"\n            x_adv = x + eps * v\n            x_adv_clipped = np.clip(x_adv, 0.0, 1.0)\n            _, _, _, z2_adv = forward(x_adv_clipped)\n            return predict_from_logit(z2_adv)\n\n        # Coarse grid search to find a bracketing interval [a, b]\n        eps_grid = np.linspace(0.0, eps_max, N)\n        a_bracket, b_bracket = -1.0, -1.0\n        \n        for i in range(1, N):\n            if get_prediction_at_eps(eps_grid[i]) != y_hat0:\n                a_bracket, b_bracket = eps_grid[i-1], eps_grid[i]\n                break\n        \n        # If no flip is found across the entire grid, return eps_max\n        if b_bracket == -1.0:\n            return eps_max\n\n        # Bisection search to refine the interval\n        while (b_bracket - a_bracket)  tol:\n            mid = (a_bracket + b_bracket) / 2.0\n            # Handle precision limits where mid is identical to a bound\n            if mid == a_bracket or mid == b_bracket:\n                break\n            if get_prediction_at_eps(mid) != y_hat0:\n                b_bracket = mid\n            else:\n                a_bracket = mid\n        \n        return b_bracket\n\n    results = []\n    for x_vec, y_val in test_cases:\n        eps_star = find_epsilon_star(x_vec, y_val)\n        results.append(eps_star)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}