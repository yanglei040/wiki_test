{
    "hands_on_practices": [
        {
            "introduction": "本练习将带你进入信用风险建模的核心，这是一个在计算金融中至关重要的分类任务。你将使用支持向量机（SVM）来预测抵押贷款违约，并直接比较线性核与高斯径向基函数（RBF）核的性能。这个实践不仅能让你掌握使用交叉验证来评估和选择模型的关键技能，还能帮助你理解核函数的选择如何揭示信贷风险背后数据关系的线性或非线性本质。",
            "id": "2435431",
            "problem": "您面临一个计算经济学和金融学领域的二元分类任务：使用支持向量机 (SVM) 预测抵押贷款违约。标签为 $y \\in \\{-1, +1\\}$，其中 $+1$ 表示违约，$-1$ 表示不违约。每个观测值有 $3$ 个特征：贷款价值比 $(x_1)$（小数）、债务收入比 $(x_2)$（小数）和 FICO 信用分 $(x_3)$（整数）。数据集包含 $20$ 个观测值，索引为 $i \\in \\{1,2,\\ldots,20\\}$，表示为 $(x_{i1}, x_{i2}, x_{i3}; y_i)$：\n- $1$: $(0.65, 0.18, 780; -1)$\n- $2$: $(0.70, 0.20, 760; -1)$\n- $3$: $(0.75, 0.25, 740; -1)$\n- $4$: $(0.80, 0.22, 770; -1)$\n- $5$: $(0.68, 0.30, 720; -1)$\n- $6$: $(0.72, 0.28, 730; -1)$\n- $7$: $(0.85, 0.20, 750; -1)$\n- $8$: $(0.90, 0.18, 760; -1)$\n- $9$: $(0.78, 0.26, 740; -1)$\n- $10$: $(0.82, 0.27, 735; -1)$\n- $11$: $(0.95, 0.45, 660; +1)$\n- $12$: $(1.02, 0.40, 680; +1)$\n- $13$: $(0.88, 0.55, 620; +1)$\n- $14$: $(0.92, 0.50, 600; +1)$\n- $15$: $(1.05, 0.35, 650; +1)$\n- $16$: $(0.90, 0.60, 590; +1)$\n- $17$: $(0.98, 0.48, 630; +1)$\n- $18$: $(1.10, 0.30, 610; +1)$\n- $19$: $(0.84, 0.58, 605; +1)$\n- $20$: $(0.70, 0.40, 580; +1)$\n\n构建一个软间隔二元支持向量机 (SVM) 分类器，并使用 $k=5$ 的 $k$ 折交叉验证来评估其样本外分类准确率。使用以下三组参数集作为测试套件：\n- 测试 $A$：线性核，软间隔参数 $C = 10$。\n- 测试 $B$：高斯径向基函数 (RBF) 核，参数 $C = 10$ 且 $\\gamma = 0.5$。\n- 测试 $C$：高斯径向基函数 (RBF) 核，参数 $C = 10$ 且 $\\gamma = 2.0$。\n\n对于测试 $A$、$B$ 和 $C$ 中的每一个，计算其 5 折交叉验证的平均样本外分类准确率，结果为 $[0,1]$ 内的一个实数。将每个准确率四舍五入到三位小数。\n\n最后，根据核函数的选择推断从特征到违约的信用风险映射的性质。按如下方式定义并输出一个整数指示符：如果 RBF 准确率中的最优值（来自测试 $B$ 或 $C$）比较线性核准确率（来自测试 $A$）高出至少 $0.03$，则输出 $1$；否则输出 $0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，格式和顺序为 $[\\text{acc}_A,\\text{acc}_B,\\text{acc}_C,\\text{indicator}]$，其中 $\\text{acc}_A$ 是测试 $A$ 的平均准确率，$\\text{acc}_B$ 是测试 $B$ 的平均准确率，$\\text{acc}_C$ 是测试 $C$ 的平均准确率，$\\text{indicator}$ 是上述整数指示符。例如，格式应为 $[0.842,0.902,0.881,1]$。",
            "solution": "所给问题是机器学习中一个标准的二元分类任务，应用于计算金融领域。其目标是为预测抵押贷款违约构建并评估一个软间隔支持向量机 (SVM) 分类器。该问题具有科学依据，是一个适定问题，并且所有必需的数据和参数均已提供。因此，该问题被认为是有效的，并将为其构建一个正式的解法。\n\n该解法的核心在于求解软间隔 SVM 的对偶优化问题。对于一个包含 $N$ 个数据点 $(\\mathbf{x}_i, y_i)$ 的训练集，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\{-1, +1\\}$ 是标签，其对偶问题是一个二次规划 (QP) 问题，表述如下：\n$$\n\\max_{\\boldsymbol{\\alpha}} \\mathcal{L}_D(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n$$\n满足以下约束条件：\n$$\n\\sum_{i=1}^{N} \\alpha_i y_i = 0\n$$\n$$\n0 \\le \\alpha_i \\le C, \\quad \\text{for } i=1, \\ldots, N\n$$\n此处，$\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_N)$ 是拉格朗日乘子向量，$C$ 是正则化参数，用于控制对误分类的惩罚，$K(\\mathbf{x}_i, \\mathbf{x}_j)$ 是核函数。该问题通过最小化对偶拉格朗日函数 $-\\mathcal{L}_D(\\boldsymbol{\\alpha})$ 的负值来求解，这等价于最小化 $L(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{\\alpha}^T P \\boldsymbol{\\alpha} - \\mathbf{1}^T \\boldsymbol{\\alpha}$，其中矩阵 $P$ 的元素为 $P_{ij} = y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$。\n\n问题指定了两种类型的核函数：\n$1$. **线性核**：该核函数对应于原始特征空间中的线性决策边界。其定义如下：\n$$\nK(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n$$\n$2$. **高斯径向基函数 (RBF) 核**：该核函数通过将数据映射到无限维特征空间来实非线性决策边界。其定义如下：\n$$\nK(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)\n$$\n其中 $\\gamma > 0$ 是一个控制高斯函数宽度的参数。\n\n该解法通过使用 `scipy.optimize.minimize` 中提供的序列最小二乘规划 (SLSQP) 算法来数值求解此 QP 问题。在找到最优向量 $\\boldsymbol{\\alpha}^*$ 后，计算偏置项 $b$。一种稳健的方法是对所有满足 $0 < \\alpha_i^* < C$ 的支持向量进行平均，因为对于这些点，Karush-Kuhn-Tucker (KKT) 条件意味着间隔被精确满足：\n$$\nb = y_s - \\sum_{i=1}^{N} \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}_s)\n$$\n如果不存在这样的非边界支持向量，$b$ 则通过取由边界支持向量（$\\alpha_i^* = C$）的间隔约束所定义的可行区间的中点来确定。\n\n那么，对于一个新的数据点 $\\mathbf{x}$，其决策函数由下式给出：\n$$\nf(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right)\n$$\n\n该方法中的一个关键步骤是**特征缩放**。输入特征（$x_1, x_2, x_3$）的尺度差异巨大。若不进行处理，FICO 信用分 $x_3$ 将在任何基于距离的计算中（例如 RBF 核）占据主导地位。为解决此问题，我们对特征应用标准化（Z-score 归一化）。在交叉验证的每一折中，根据训练数据计算均值和标准差，然后用它们来缩放该折的训练数据和测试数据。这可以防止测试集的数据泄露到训练过程中。\n\n模型性能使用 $k=5$ 的 $k$ 折交叉验证进行评估。包含 $20$ 个观测值的数据集被划分为 $5$ 个不相交的折，每折包含 $4$ 个观测值。在 $5$ 次迭代的每一次中，使用一折作为测试集，其余 $4$ 折用于训练。对每一折计算其分类准确率，该准确率定义为测试集上被正确预测的标签所占的比例。对于给定的参数集，最终的样本外准确率是这 $5$ 个准确率的平均值。\n\n对所指定的三个测试用例（A, B, C）中的每一个都执行此过程。最后，计算一个指示变量。如果表现最佳的 RBF 核的准确率比较线性核的准确率高出至少 $0.03$，则该变量为 $1$，否则为 $0$。这种比较有助于推断类别分离的几何性质；RBF 性能的显著提升意味着非线性决策边界更适合此信用风险分类问题。最终输出将计算出的准确率和指示符组合成所要求的列表格式。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the SVM cross-validation problem.\n    \"\"\"\n    # Dataset definition\n    data = [\n        (0.65, 0.18, 780, -1), (0.70, 0.20, 760, -1), (0.75, 0.25, 740, -1),\n        (0.80, 0.22, 770, -1), (0.68, 0.30, 720, -1), (0.72, 0.28, 730, -1),\n        (0.85, 0.20, 750, -1), (0.90, 0.18, 760, -1), (0.78, 0.26, 740, -1),\n        (0.82, 0.27, 735, -1), (0.95, 0.45, 660, 1), (1.02, 0.40, 680, 1),\n        (0.88, 0.55, 620, 1), (0.92, 0.50, 600, 1), (1.05, 0.35, 650, 1),\n        (0.90, 0.60, 590, 1), (0.98, 0.48, 630, 1), (1.10, 0.30, 610, 1),\n        (0.84, 0.58, 605, 1), (0.70, 0.40, 580, 1)\n    ]\n    X = np.array([d[:3] for d in data])\n    y = np.array([d[3] for d in data])\n\n    # Test cases\n    test_cases = [\n        {'C': 10.0, 'kernel_type': 'linear', 'gamma': None},\n        {'C': 10.0, 'kernel_type': 'rbf', 'gamma': 0.5},\n        {'C': 10.0, 'kernel_type': 'rbf', 'gamma': 2.0},\n    ]\n\n    # --- Kernel Functions ---\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def make_rbf_kernel(gamma):\n        def rbf_kernel(x1, x2):\n            return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n        return rbf_kernel\n\n    # --- SVM Solver ---\n    def train_svm(X_train, y_train, C, kernel_func):\n        n_samples = X_train.shape[0]\n        \n        # Build Kernel/Gram matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = kernel_func(X_train[i], X_train[j])\n        \n        # QP problem formulation for scipy.optimize.minimize\n        # Minimize: 0.5 * alpha.T * P * alpha - 1.T * alpha\n        # P_ij = y_i * y_j * K_ij\n        P = np.outer(y_train, y_train) * K\n        # Add a small regularization for numerical stability\n        P += 1e-8 * np.eye(n_samples)\n\n\n        def objective(alpha):\n            return 0.5 * alpha.T @ P @ alpha - np.sum(alpha)\n\n        # Constraints: sum(alpha_i * y_i) = 0\n        eq_cons = {'type': 'eq',\n                   'fun': lambda alpha: y_train.T @ alpha,\n                   'jac': lambda alpha: y_train}\n\n        # Bounds: 0 = alpha_i = C\n        bounds = [(0, C) for _ in range(n_samples)]\n\n        # Initial guess\n        alpha0 = np.zeros(n_samples)\n\n        # Solve QP problem\n        res = minimize(objective, alpha0, method='SLSQP', bounds=bounds, constraints=[eq_cons])\n        alpha = res.x\n\n        # Find support vectors\n        sv_mask = alpha  1e-6\n        \n        # Compute bias term 'b'\n        non_bound_sv_mask = (alpha  1e-6)  (alpha  C - 1e-6)\n        if np.any(non_bound_sv_mask):\n            non_bound_sv_indices = np.where(non_bound_sv_mask)[0]\n            b_values = [y_train[s] - np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, s]) for s in non_bound_sv_indices]\n            b = np.mean(b_values)\n        else: # Fallback if no non-bound SVs are found\n            sv_indices = np.where(sv_mask)[0]\n            f_vals_pos = [np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, i]) for i in sv_indices if y_train[i] == 1]\n            f_vals_neg = [np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, i]) for i in sv_indices if y_train[i] == -1]\n            \n            if not f_vals_pos or not f_vals_neg:\n                # Can happen if all SVs are from one class. Use a simple mean.\n                b_vals = [y_train[i] - np.sum(alpha * y_train * K[:, i]) for i in sv_indices]\n                b = np.mean(b_vals) if b_vals else 0\n            else:\n                b = -0.5 * (np.min(f_vals_pos) + np.max(f_vals_neg))\n\n        return alpha, b\n\n    # --- Prediction Function ---\n    def predict(X_test, X_train, y_train, alpha, b, kernel_func):\n        sv_mask = alpha  1e-6\n        alpha_sv = alpha[sv_mask]\n        y_train_sv = y_train[sv_mask]\n        X_train_sv = X_train[sv_mask]\n        \n        y_pred = np.zeros(X_test.shape[0])\n        for i in range(X_test.shape[0]):\n            s = 0\n            for j in range(alpha_sv.shape[0]):\n                s += alpha_sv[j] * y_train_sv[j] * kernel_func(X_train_sv[j], X_test[i])\n            y_pred[i] = s + b\n\n        pred_labels = np.sign(y_pred)\n        pred_labels[pred_labels == 0] = 1 # Assign a class if decision value is exactly 0\n        return pred_labels\n\n    # --- Cross-Validation ---\n    def run_cross_validation(X, y, C, kernel_func, k=5):\n        n_samples = X.shape[0]\n        fold_size = n_samples // k\n        indices = np.arange(n_samples)\n        \n        accuracies = []\n        for i in range(k):\n            # Split data into train and test sets for the current fold\n            start, end = i * fold_size, (i + 1) * fold_size\n            test_indices = indices[start:end]\n            train_indices = np.delete(indices, test_indices)\n\n            X_train, y_train = X[train_indices], y[train_indices]\n            X_test, y_test = X[test_indices], y[test_indices]\n            \n            # --- Feature Scaling ---\n            mean = np.mean(X_train, axis=0)\n            std = np.std(X_train, axis=0)\n            std[std == 0] = 1.0 # Avoid division by zero\n            \n            X_train_scaled = (X_train - mean) / std\n            X_test_scaled = (X_test - mean) / std\n            \n            # Train SVM\n            alpha, b = train_svm(X_train_scaled, y_train, C, kernel_func)\n            \n            # Predict on test set\n            y_pred = predict(X_test_scaled, X_train_scaled, y_train, alpha, b, kernel_func)\n            \n            # Calculate accuracy\n            accuracy = np.mean(y_pred == y_test)\n            accuracies.append(accuracy)\n            \n        return np.mean(accuracies)\n\n    # --- Main Execution Logic ---\n    results = []\n    for case in test_cases:\n        C = case['C']\n        if case['kernel_type'] == 'linear':\n            kernel = linear_kernel\n        else: # rbf\n            kernel = make_rbf_kernel(case['gamma'])\n        \n        mean_accuracy = run_cross_validation(X, y, C, kernel)\n        results.append(round(mean_accuracy, 3))\n    \n    acc_A, acc_B, acc_C = results\n    \n    # Compute the final indicator\n    indicator = 1 if max(acc_B, acc_C) = acc_A + 0.03 else 0\n    results.append(indicator)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在金融时间序列预测中，一个普遍的认知是近期数据比远期数据更具预测价值。本练习将引导你超越标准核函数，通过设计一个定制的“时间衰减”核函数来将这一领域知识直接融入SVM模型中。通过这个动手实践，你将深刻体会到核技巧的强大灵活性，并学习如何构建能捕捉金融数据时效性的高级模型。",
            "id": "2435408",
            "problem": "您将处理一个受金融时间序列预测启发的二元分类任务。每个样本包含一个特征向量（表示近期回报率和波动率的摘要统计），一个时间戳（表示其在时间序列中的位置），以及一个标签（指示次日回报率是上涨还是下跌）。考虑以下包含四个样本的训练数据，其中每个特征向量位于 $\\mathbb{R}^2$ 空间，每个时间戳是一个整数，每个标签属于 $\\{-1,+1\\}$：\n- 样本 1：$x_1 = (0.00, 0.00)$，$t_1 = 1$，$y_1 = -1$。\n- 样本 2：$x_2 = (1.00, 0.20)$，$t_2 = 2$，$y_2 = +1$。\n- 样本 3：$x_3 = (0.90, 1.00)$，$t_3 = 3$，$y_3 = +1$。\n- 样本 4：$x_4 = (-0.80, -0.50)$，$t_4 = 4$，$y_4 = -1$。\n\n您必须使用支持向量机 (SVM)，并配备一个能够增强近期观测数据影响力的核函数。对于任意两个样本 $(x_i,t_i)$ 和 $(x_j,t_j)$，该核函数定义如下\n$$\nK_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big) \\;=\\; \\exp\\!\\big(-\\gamma \\,\\|x_i - x_j\\|_2^2\\big)\\;\\cdot\\;\\exp\\!\\big(\\alpha\\,(t_i + t_j)\\big),\n$$\n其中 $\\alpha \\ge 0$ 控制时间衰减强度，$\\gamma  0$ 控制径向基尺度。\n\n请使用此核函数在对偶形式下训练一个软间隔SVM。令 $n$ 表示训练样本的数量。其对偶优化问题为：\n$$\n\\max_{\\alpha_1,\\dots,\\alpha_n}\\;\\; \\sum_{i=1}^n \\alpha_i \\;-\\;\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j\\,y_i y_j\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big),\n$$\n约束条件为\n$$\n\\sum_{i=1}^n y_i \\alpha_i \\;=\\; 0,\\quad 0 \\le \\alpha_i \\le C \\;\\text{ for all } i.\n$$\n在获得最优解 $\\{\\alpha_i^\\star\\}_{i=1}^n$ 后，对于任意输入 $(z,t_z)$，决策函数定义如下\n$$\nf(z,t_z) \\;=\\; \\sum_{i=1}^n \\alpha_i^\\star y_i\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(z,t_z)\\big) \\;+\\; b,\n$$\n其中偏置项 $b$ 的选择需满足 Karush-Kuhn-Tucker 条件。具体而言，如果存在至少一个索引 $i$ 使得 $0  \\alpha_i^\\star  C$ 成立，则强制 $y_i\\,f(x_i,t_i)=1$，并将 $b$ 取为所有此类 $i$ 对应的 $y_i - \\sum_{j=1}^n \\alpha_j^\\star y_j K_{\\alpha,\\gamma}((x_j,t_j),(x_i,t_i))$ 的平均值。如果不存在这样的索引，则在与互补松弛不等式一致的数值区间内选择 $b$，\n$$\n\\begin{aligned}\n\\text{对于 } \\alpha_i^\\star = 0:\\;\\; y_i\\,f(x_i,t_i) \\ge 1,\\\\\n\\text{对于 } \\alpha_i^\\star = C:\\;\\; y_i\\,f(x_i,t_i) \\le 1,\n\\end{aligned}\n$$\n并且为确保输出唯一，将 $b$ 选为这些不等式所隐含的最紧可行区间的中点。\n\n请对以下三个测试输入进行分类，每个输入都在时间戳 $t_z = 5$ 处进行评估：\n- $z_1 = (0.95, 0.70)$,\n- $z_2 = (-0.70, -0.40)$,\n- $z_3 = (0.10, 0.05)$.\n使用 $f(z,t_z)$ 的符号来预测标签（属于 $\\{-1,+1\\}$），并约定非负值映射为 $+1$。\n\n您的程序必须针对下述每种参数配置，精确求解所述的SVM对偶问题，并为每种配置按指定顺序输出三个测试输入的预测标签。参数配置的测试集如下：\n- 情况 1：$(\\alpha,\\gamma,C) = (0.0, 1.0, 10.0)$。\n- 情况 2：$(\\alpha,\\gamma,C) = (0.5, 1.0, 10.0)$。\n- 情况 3：$(\\alpha,\\gamma,C) = (1.0, 0.5, 10.0)$。\n- 情况 4：$(\\alpha,\\gamma,C) = (0.0, 5.0, 1.0)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素本身是一个包含三个整数的列表，分别对应于每种情况下按顺序对 $(z_1,5)$、$(z_2,5)$ 和 $(z_3,5)$ 的预测标签。例如，输出格式必须为\n$$\n\\big[\\,[\\ell_{1,1},\\ell_{1,2},\\ell_{1,3}],\\;[\\ell_{2,1},\\ell_{2,2},\\ell_{2,3}],\\;[\\ell_{3,1},\\ell_{3,2},\\ell_{3,3}],\\;[\\ell_{4,1},\\ell_{4,2},\\ell_{4,3}]\\,\\big],\n$$\n其中每个 $\\ell_{k,j} \\in \\{-1,+1\\}$ 是一个整数。不应打印任何其他文本。",
            "solution": "该问题被认定为有效。它提出了一个标准的软间隔支持向量机 (SVM) 分类任务，尽管使用了自定义的核函数。该问题在科学上基于成熟的机器学习理论，被明确地表述为一个凸优化问题，客观陈述，并为获得唯一解提供了所有必要信息。\n\n对于每个给定的参数配置 $(\\alpha, \\gamma, C)$，求解过程包括几个不同的步骤。\n\n首先，我们将SVM对偶问题形式化为一个标准的二次规划 (QP) 问题，以便使用数值求解器求解。目标是找到拉格朗日乘子 $\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_n]^T$，以最大化对偶目标函数。这等价于最小化以下二次型：\n$$\n\\mathcal{L}(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{\\alpha}^T \\mathbf{H} \\boldsymbol{\\alpha} - \\mathbf{1}^T \\boldsymbol{\\alpha}\n$$\n约束条件为：\n$$\n\\mathbf{y}^T \\boldsymbol{\\alpha} = 0 \\quad \\text{and} \\quad \\mathbf{0} \\le \\boldsymbol{\\alpha} \\le C \\cdot \\mathbf{1}\n$$\n此处，$n=4$ 是训练样本的数量，$\\mathbf{y} = [-1, 1, 1, -1]^T$ 是标签向量，$\\mathbf{1}$ 是全一向量，$C$ 是正则化参数。矩阵 $\\mathbf{H}$ 是一个 $n \\times n$ 矩阵，其元素为 $H_{ij} = y_i y_j K_{ij}$，其中 $K_{ij}$ 是第 $i$ 个和第 $j$ 个训练样本的核函数值。\n\n核函数由以下公式给出：\n$$\nK_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big) \\;=\\; \\exp\\!\\big(-\\gamma \\,d_{ij}^2\\big)\\;\\cdot\\;\\exp\\!\\big(\\alpha\\,(t_i + t_j)\\big)\n$$\n其中 $d_{ij}^2 = \\|x_i - x_j\\|_2^2$ 是特征向量 $x_i$ 和 $x_j$ 之间的欧几里得距离的平方。对于给定的训练数据：\n$x_1 = (0.00, 0.00)$, $t_1 = 1$, $y_1 = -1$\n$x_2 = (1.00, 0.20)$, $t_2 = 2$, $y_2 = +1$\n$x_3 = (0.90, 1.00)$, $t_3 = 3$, $y_3 = +1$\n$x_4 = (-0.80, -0.50)$, $t_4 = 4$, $y_4 = -1$\n我们首先预先计算 $n \\times n$ 的 Gram 矩阵，记为 $\\mathbf{K}$，其元素为 $K_{ij} = K_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big)$。然后，我们使用公式 $H_{ij} = y_i y_j K_{ij}$ 构造矩阵 $\\mathbf{H}$。\n\n这个约束优化问题使用数值QP求解器来解决。我们使用 `scipy.optimize.minimize` 库函数中的序列最小二乘规划 (SLSQP) 算法。该函数接受目标函数 $\\mathcal{L}(\\boldsymbol{\\alpha})$、一个初始猜测值（例如 $\\boldsymbol{\\alpha}_0 = \\mathbf{0}$）、每个 $i$ 的边界条件 $0 \\le \\alpha_i \\le C$，以及线性等式约束 $\\sum_i y_i \\alpha_i = 0$。求解器返回最优向量 $\\boldsymbol{\\alpha}^\\star = [\\alpha_1^\\star, \\dots, \\alpha_n^\\star]^T$。由于数值精度的原因，将非常接近 $0$ 或 $C$ 的 $\\alpha_i^\\star$ 值调整为这些精确值。\n\n获得 $\\boldsymbol{\\alpha}^\\star$ 后，我们计算偏置项 $b$。计算方法取决于所得的 $\\alpha_i^\\star$ 值，并基于 Karush-Kuhn-Tucker (KKT) 条件。\n令 $S$ 为间隔边界上的支持向量的索引集合，其中 $0  \\alpha_i^\\star  C$。\n如果 $S$ 非空，则计算偏置项 $b$ 以确保对于任意 $i \\in S$，决策函数满足 $y_i f(x_i, t_i) = 1$。这引出以下表达式：\n$$\nb_i = y_i - \\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij}\n$$\n为了稳健性，我们对所有 $i \\in S$ 的这些值取平均：$b = \\frac{1}{|S|} \\sum_{i \\in S} b_i$。\n\n如果 $S$ 为空，则所有 $\\alpha_i^\\star$ 都在边界上（$0$ 或 $C$）。在这种情况下，$b$ 不是由单个方程唯一确定的，而是必须位于由KKT互补松弛条件定义的区间内：\n- 对于 $\\alpha_i^\\star = 0$：$y_i \\big(\\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij} + b\\big) \\ge 1$。\n- 对于 $\\alpha_i^\\star = C$：$y_i \\big(\\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij} + b\\big) \\le 1$。\n这些不等式为 $b$ 定义了一个可行区间。设 $b_{lower}$ 为从这些条件派生的下界的最大值，而 $b_{upper}$ 为上界的最小值。问题要求选择 $b$ 作为此最紧可行区间的中点：$b = (b_{lower} + b_{upper}) / 2$。\n\n最后，在确定 $\\boldsymbol{\\alpha}^\\star$ 和 $b$ 之后，我们对时间戳 $t_z = 5$ 处的三个新测试输入 $z_1, z_2, z_3$ 进行分类。对于每个测试点 $(z, t_z)$，评估决策函数：\n$$\nf(z,t_z) \\;=\\; \\sum_{i=1}^n \\alpha_i^\\star y_i\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(z,t_z)\\big) \\;+\\; b\n$$\n预测标签 $\\ell \\in \\{-1, +1\\}$ 由 $f(z,t_z)$ 的符号给出，并约定非负值映射为 $+1$：\n$$\n\\ell = \\begin{cases} +1  \\text{if } f(z,t_z) \\ge 0 \\\\ -1  \\text{if } f(z,t_z)  0 \\end{cases}\n$$\n对四个指定的参数配置中的每一个都执行这整个过程，并将所得的预测结果汇总成所要求的输出格式。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    # Define the training data from the problem statement.\n    X_train = np.array([\n        [0.00, 0.00],\n        [1.00, 0.20],\n        [0.90, 1.00],\n        [-0.80, -0.50]\n    ])\n    T_train = np.array([1, 2, 3, 4])\n    Y_train = np.array([-1, 1, 1, -1])\n    n_samples = len(X_train)\n\n    # Define the test inputs.\n    X_test = np.array([\n        [0.95, 0.70],\n        [-0.70, -0.40],\n        [0.10, 0.05]\n    ])\n    T_test = 5\n\n    # Define the parameter configurations from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 10.0),\n        (0.5, 1.0, 10.0),\n        (1.0, 0.5, 10.0),\n        (0.0, 5.0, 1.0),\n    ]\n\n    all_results = []\n    # Numerical tolerance for floating point comparisons\n    tol = 1e-9\n\n    for case in test_cases:\n        alpha_param, gamma, C = case\n        \n        # 1. Construct the Kernel Matrix K.\n        # Efficiently compute squared Euclidean distances between all pairs of training points.\n        sq_dists = np.sum(X_train**2, axis=1, keepdims=True) + np.sum(X_train**2, axis=1) - 2 * np.dot(X_train, X_train.T)\n        rbf_kernel = np.exp(-gamma * sq_dists)\n        time_component = np.exp(alpha_param * (T_train.reshape(-1, 1) + T_train))\n        K = rbf_kernel * time_component\n\n        # 2. Construct the matrix H for the QP problem.\n        H = np.outer(Y_train, Y_train) * K\n        \n        # 3. Define the QP objective function and constraints.\n        def objective_func(alphas):\n            return 0.5 * alphas.T @ H @ alphas - np.sum(alphas)\n        \n        constraints = ({'type': 'eq', 'fun': lambda alphas: np.dot(Y_train, alphas)})\n        bounds = Bounds([0] * n_samples, [C] * n_samples)\n        \n        # 4. Solve the QP problem for the optimal alphas.\n        result = minimize(objective_func, np.zeros(n_samples), method='SLSQP', bounds=bounds, constraints=constraints)\n        alpha_star = result.x\n        \n        # Clamp near-zero and near-C values for stable bias calculation.\n        alpha_star[alpha_star  tol] = 0\n        alpha_star[alpha_star  C - tol] = C\n        \n        # 5. Calculate the bias term b.\n        # Find support vectors on the margin (0  alpha_i  C).\n        sv_margin_indices = np.where((alpha_star  tol)  (alpha_star  C - tol))[0]\n\n        if len(sv_margin_indices)  0:\n            # Case 1: bias calculation from margin support vectors.\n            b_values = [Y_train[i] - np.sum(alpha_star * Y_train * K[:, i]) for i in sv_margin_indices]\n            b = np.mean(b_values)\n        else:\n            # Case 2: bias calculation from KKT conditions on non-margin SVs.\n            b_lowers, b_uppers = [], []\n            f_preds_no_b = K.T @ (alpha_star * Y_train)\n\n            for i in range(n_samples):\n                if not ((alpha_star[i]  tol) and (alpha_star[i]  C - tol)):\n                    if np.isclose(alpha_star[i], 0): # alpha_i = 0 implies y_i(f_i+b) >= 1\n                        if Y_train[i] == 1: b_lowers.append(1 - f_preds_no_b[i])\n                        else: b_uppers.append(-1 - f_preds_no_b[i])\n                    elif np.isclose(alpha_star[i], C): # alpha_i = C implies y_i(f_i+b) = 1\n                        if Y_train[i] == 1: b_uppers.append(1 - f_preds_no_b[i])\n                        else: b_lowers.append(-1 - f_preds_no_b[i])\n            \n            max_lower = max(b_lowers) if b_lowers else -np.inf\n            min_upper = min(b_uppers) if b_uppers else np.inf\n            b = (max_lower + min_upper) / 2.0\n\n        # 6. Classify test inputs using the calculated model.\n        case_predictions = []\n        for z in X_test:\n            # Calculate kernel values between training points and the test point.\n            sq_dists_test = np.sum(X_train**2, axis=1) + np.sum(z**2) - 2 * np.dot(X_train, z)\n            rbf_kernel_test = np.exp(-gamma * sq_dists_test)\n            time_component_test = np.exp(alpha_param * (T_train + T_test))\n            K_test = rbf_kernel_test * time_component_test\n            \n            # Evaluate the decision function.\n            f_z = np.sum(alpha_star * Y_train * K_test) + b\n            \n            # Predict the label.\n            prediction = 1 if f_z = 0 else -1\n            case_predictions.append(prediction)\n        \n        all_results.append(case_predictions)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "金融市场是动态变化的，一个在过去表现良好的模型可能无法适应未来的市场状况。本练习模拟了一个在滚动时间窗口上进行股票分类的真实场景，旨在量化模型“关键输入”的稳定性。你将通过识别每个窗口期的支持向量——那些决定决策边界的“最重要”的股票——并衡量这个集合随时间变化的程度，来深入理解模型在动态环境下的行为，这是评估任何量化策略稳健性的关键一步。",
            "id": "2435480",
            "problem": "给定一个横截面股票分类任务，其动机是计算经济学和金融学中的一个滚动窗口研究。其目标是，当在滚动窗口上重新估计预测任务时，形式化并量化由软间隔支持向量机 (SVM) 中的支持向量定义的“最重要”股票集合是否随时间变化。\n\n从以下基本原理开始：\n- 软间隔支持向量机 (SVM) 解决以下凸优化问题\n  $$\n  \\min_{\\mathbf{w}, b, \\boldsymbol{\\xi}} \\;\\; \\frac{1}{2}\\lVert \\mathbf{w} \\rVert_2^2 + C \\sum_{i=1}^{m} \\xi_i\n  \\quad \\text{subject to} \\quad\n  y_i \\big(\\mathbf{w}^\\top \\mathbf{x}_i + b\\big) \\ge 1 - \\xi_i, \\;\\; \\xi_i \\ge 0,\n  $$\n  其中 $C \\ge 0$ 是正则化常数，$\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征，$y_i \\in \\{-1, +1\\}$ 是标签，$\\xi_i$ 是松弛变量。\n- 其线性核的拉格朗日对偶问题可以写成如下二次规划\n  $$\n  \\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^m} \\;\\; \\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha}\n  \\quad \\text{subject to} \\quad\n  \\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0, \\;\\; 0 \\le \\alpha_i \\le C,\n  $$\n  其中 $\\mathbf{Q}$ 是一个矩阵，其元素为 $Q_{ij} = y_i y_j \\mathbf{x}_i^\\top \\mathbf{x}_j$，$\\mathbf{1}$ 是全一向量，$\\mathbf{y} = (y_1,\\dots,y_m)^\\top$。\n- 根据 Karush–Kuhn–Tucker (KKT) 条件，对偶变量 $\\alpha_i  0$ 的数据点是支持向量。满足 $0  \\alpha_i  C$ 的点位于间隔边界上；满足 $\\alpha_i = C$ 的点是违反间隔边界的点或被错误分类的点。原始解满足 $\\mathbf{w} = \\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i$。\n\n数据集构建与滚动窗口：\n- 有 $N = 8$ 只股票，索引为 $i \\in \\{0,1,\\dots,7\\}$，每只股票都有一个固定的二维特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^2$，代表简化的横截面特征（例如，动量和规模），由下式给出\n  $$\n  \\mathbf{x}_0 = (0.9, 0.2), \\;\\;\n  \\mathbf{x}_1 = (0.7, 0.4), \\;\\;\n  \\mathbf{x}_2 = (0.5, 0.1), \\;\\;\n  \\mathbf{x}_3 = (0.3, 0.3),\n  $$\n  $$\n  \\mathbf{x}_4 = (-0.2, -0.8), \\;\\;\n  \\mathbf{x}_5 = (-0.4, -0.5), \\;\\;\n  \\mathbf{x}_6 = (-0.6, -0.2), \\;\\;\n  \\mathbf{x}_7 = (-0.8, -0.4).\n  $$\n- 有 $T = 6$ 个时间点，索引为 $t \\in \\{0,1,2,3,4,5\\}$。在每个时间点 $t$，一个真实但未知的线性评分规则平滑地变化：\n  $$\n  \\mathbf{w}_t = (\\cos\\theta_t, \\sin\\theta_t), \\quad \\text{with} \\quad \\theta_t \\in \\{0^\\circ, 22.5^\\circ, 45^\\circ, 67.5^\\circ, 90^\\circ, 112.5^\\circ\\}.\n  $$\n  截距为 $b_t = 0$。标签通过 $y_{i,t} = \\mathrm{sign}(\\mathbf{w}_t^\\top \\mathbf{x}_i)$ 确定性地生成，因此对于所有的 $i,t$，都有 $y_{i,t} \\in \\{-1, +1\\}$。\n- 对于宽度为 $W$ 的滚动窗口，窗口 $k$（从时间 $t=k$ 开始）中的训练样本聚合了所有满足 $i \\in \\{0,\\dots,7\\}$ 和 $t \\in \\{k, k+1, \\dots, k+W-1\\}$ 的数据对 $(\\mathbf{x}_i, y_{i,t})$，从而产生 $m = N \\cdot W$ 个样本。令 $\\mathcal{S}_k$ 表示在窗口 $k$ 中作为支持向量出现的股票索引集合，即 $\\mathcal{S}_k = \\{i \\in \\{0,\\dots,7\\} \\mid \\exists$ 窗口 $k$ 中股票 $i$ 的样本满足 $\\alpha  0\\}$。\n\n稳定性度量：\n- 对于连续的窗口 $k$ 和 $k+1$，将集合相似性稳定性定义为 Jaccard 指数\n  $$\n  J(\\mathcal{S}_k, \\mathcal{S}_{k+1}) = \\frac{|\\mathcal{S}_k \\cap \\mathcal{S}_{k+1}|}{|\\mathcal{S}_k \\cup \\mathcal{S}_{k+1}|}.\n  $$\n- 对于给定的正则化常数 $C$ 和窗口宽度 $W$，以及 $K = T - W + 1$ 个窗口，计算平均连续窗口稳定性\n  $$\n  \\bar{J} = \\frac{1}{K-1}\\sum_{k=0}^{K-2} J(\\mathcal{S}_k, \\mathcal{S}_{k+1}).\n  $$\n\n任务：\n- 使用上述带有线性核的二次规划来实现 SVM 对偶优化，以获得对偶变量 $\\boldsymbol{\\alpha}$，识别每个窗口的支持向量，并计算稳定性度量 $\\bar{J}$。\n- 您的实现必须是一个完整的、可运行的程序，无需任何用户输入即可执行整个流程。\n\n测试套件和要求输出：\n- 使用以下 $(C, W)$ 参数对测试套件：\n  1. $(C, W) = (1.0, 3)$\n  2. $(C, W) = (10.0, 3)$\n  3. $(C, W) = (0.3, 2)$\n- 对于每个参数对，计算平均连续窗口稳定性 $\\bar{J}$，结果为浮点数并四舍五入到四位小数。\n- 最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[a,b,c]$，其中 $a$、$b$ 和 $c$ 是按给定顺序为三个测试用例计算出的 $\\bar{J}$ 值。\n- 角度以度为单位给出；没有需要报告的物理单位；所有输出都是无单位的浮点数。最终打印的值必须四舍五入到四位小数。",
            "solution": "问题陈述经评估有效。它在科学上基于支持向量机和凸优化的理论，其在计算金融中的应用是一个合理的研究领域。该问题定义明确，提供了所有必要的数据、定义和一个清晰、客观的计算目标。它不包含任何逻辑矛盾、模糊之处或事实不正确的前提。因此，我们将着手提供一个完整的解决方案。\n\n问题的核心是量化股票“重要性”随时间推移的稳定性。重要性是通过在软间隔支持向量机 (SVM) 分类任务中是否为支持向量来定义的。该分析在滚动窗口的基础上进行。解决方案通过遵循从第一性原理推导的一系列步骤来执行。\n\n首先，我们建立合成数据集。有 $N=8$ 只股票，每只股票都有一个固定的二维特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^2$。在 $T=6$ 个时间周期内，底层的分类规则平滑变化。这通过一个旋转的权重向量 $\\mathbf{w}_t = (\\cos\\theta_t, \\sin\\theta_t)^\\top$ 来建模，其中 $\\theta_t$ 是一系列角度。股票 $i$ 在时间 $t$ 的类别标签 $y_{i,t} \\in \\{-1, +1\\}$ 被确定性地确定为 $y_{i,t} = \\mathrm{sign}(\\mathbf{w}_t^\\top \\mathbf{x}_i)$。\n\n其次，我们实施滚动窗口分析。对于给定的窗口宽度 $W$ 和正则化参数 $C$，我们构建一系列训练数据集。第 $k$ 个窗口（对于 $k \\in \\{0, 1, \\dots, T-W\\}$）包含所有满足 $i \\in \\{0, \\dots, N-1\\}$ 和 $t \\in \\{k, \\dots, k+W-1\\}$ 的数据点 $(\\mathbf{x}_i, y_{i,t})$。这为每个窗口生成一个大小为 $m = N \\cdot W$ 的训练集。\n\n第三，对于每个窗口的训练数据，我们求解软间隔 SVM 的对偶问题。该问题是一个二次规划 (QP) 问题，定义如下：\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^m} \\;\\; \\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\mathbf{Q} \\boldsymbol{\\alpha} - \\mathbf{1}^\\top \\boldsymbol{\\alpha}\n$$\n受以下约束：\n$$\n\\mathbf{y}^\\top \\boldsymbol{\\alpha} = 0\n$$\n$$\n0 \\le \\alpha_j \\le C \\quad \\text{for } j=1, \\dots, m\n$$\n在此，$\\boldsymbol{\\alpha}$ 是拉格朗日乘子向量，$\\mathbf{y}$ 是窗口训练集中的标签向量，$\\mathbf{Q}$ 是一个 $m \\times m$ 矩阵，其元素为 $Q_{jl} = y_j y_l (\\mathbf{x}_j^\\top \\mathbf{x}_l)$。这个约束优化问题是凸的，可以使用数值方法可靠地求解。我们采用序列最小二乘规划 (SLSQP) 算法，该算法在 `scipy.optimize.minimize` 函数中实现。我们提供了目标函数、其梯度（雅可比矩阵）$\\nabla_{\\boldsymbol{\\alpha}}f(\\boldsymbol{\\alpha}) = \\mathbf{Q}\\boldsymbol{\\alpha} - \\mathbf{1}$、线性等式约束以及对 $\\boldsymbol{\\alpha}$ 的箱式约束。\n\n第四，在为窗口 $k$ 求解出最优对偶变量 $\\boldsymbol{\\alpha}^{(k)}$ 后，我们识别出重要股票的集合。根据 SVM 的 Karush–Kuhn–Tucker (KKT) 条件，如果数据点 $j$ 对应的拉格朗日乘子 $\\alpha_j$ 严格为正，则该点为支持向量。由于浮点运算，我们使用一个小的容差，将满足 $\\alpha_j  \\epsilon$（对于某个小的 $\\epsilon  0$）的点识别为支持向量。窗口 $k$ 的重要股票集合，表示为 $\\mathcal{S}_k$，是所有唯一股票索引 $i$ 的集合，其中至少有一个在窗口 $k$ 中对应的数据点是支持向量。\n\n第五，我们量化这些集合随时间推移的稳定性。两个连续窗口的支持向量集 $\\mathcal{S}_k$ 和 $\\mathcal{S}_{k+1}$ 之间的相似性通过 Jaccard 指数来衡量：\n$$\nJ(\\mathcal{S}_k, \\mathcal{S}_{k+1}) = \\frac{|\\mathcal{S}_k \\cap \\mathcal{S}_{k+1}|}{|\\mathcal{S}_k \\cup \\mathcal{S}_{k+1}|}\n$$\n对于给定的 $(C, W)$ 对，总体稳定性 $\\bar{J}$ 是从 $k=0$ 到 $k=K-2$ 的所有连续窗口对的 Jaccard 指数的算术平均值，其中 $K = T - W + 1$ 是窗口总数。\n\n最后，对测试套件中指定的每个 $(C, W)$ 对执行整个流程。为每个案例计算的平均稳定性 $\\bar{J}$ 四舍五入到四位小数，并作为最终结果报告。该实现包含在一个独立的 Python 脚本中，利用 `numpy` 进行高效的数值计算，并使用 `scipy` 完成核心优化任务，严格遵守问题要求。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the SVM stability analysis problem for the given test cases.\n    \"\"\"\n    \n    # Define fixed problem parameters and data\n    N = 8\n    T = 6\n    x_stocks = np.array([\n        [0.9, 0.2], [0.7, 0.4], [0.5, 0.1], [0.3, 0.3],\n        [-0.2, -0.8], [-0.4, -0.5], [-0.6, -0.2], [-0.8, -0.4]\n    ])\n    thetas_deg = np.array([0.0, 22.5, 45.0, 67.5, 90.0, 112.5])\n    \n    # Pre-calculate true weights and labels for all time points\n    thetas_rad = np.deg2rad(thetas_deg)\n    w_t_vectors = np.array([np.cos(thetas_rad), np.sin(thetas_rad)]).T\n    # y_labels[i, t] is the label for stock i at time t. Shape (8, 6)\n    y_labels = np.sign(x_stocks @ w_t_vectors.T)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 3),    # (C, W) pair 1\n        (10.0, 3),   # (C, W) pair 2\n        (0.3, 2),    # (C, W) pair 3\n    ]\n\n    results = []\n    \n    for C, W in test_cases:\n        K = T - W + 1  # Total number of rolling windows\n        all_S_sets = []\n\n        # Iterate over each rolling window\n        for k in range(K):\n            # 1. Construct the training dataset for the current window k\n            m = N * W\n            train_X = np.zeros((m, 2))\n            train_y = np.zeros(m)\n            train_stock_indices = np.zeros(m, dtype=int)\n            \n            idx = 0\n            for i in range(N):\n                for t_offset in range(W):\n                    t = k + t_offset\n                    train_X[idx] = x_stocks[i]\n                    train_y[idx] = y_labels[i, t]\n                    train_stock_indices[idx] = i\n                    idx += 1\n\n            # 2. Set up the dual SVM Quadratic Program\n            gram_matrix = train_X @ train_X.T\n            y_outer = np.outer(train_y, train_y)\n            Q = gram_matrix * y_outer\n\n            def objective_func(alpha):\n                return 0.5 * (alpha.T @ Q @ alpha) - np.sum(alpha)\n\n            def objective_jac(alpha):\n                return (alpha.T @ Q) - 1.0\n\n            # Equality constraint: y^T * alpha = 0\n            eq_cons = {\n                'type': 'eq',\n                'fun': lambda alpha: alpha @ train_y,\n                'jac': lambda alpha: train_y\n            }\n            \n            # Box constraints: 0 = alpha_i = C\n            bounds = [(0, C) for _ in range(m)]\n\n            # Initial guess for alpha\n            alpha_0 = np.zeros(m)\n            \n            # 3. Solve the QP using SLSQP\n            res = minimize(\n                fun=objective_func,\n                x0=alpha_0,\n                jac=objective_jac,\n                bounds=bounds,\n                constraints=[eq_cons],\n                method='SLSQP',\n                tol=1e-9\n            )\n            \n            alphas = res.x\n            \n            # 4. Identify support vector stock indices\n            # A stock is a support vector if any of its observations have alpha > 0\n            sv_mask = alphas  1e-7  # Tolerance for floating point precision\n            sv_stock_indices = train_stock_indices[sv_mask]\n            S_k = set(sv_stock_indices)\n            all_S_sets.append(S_k)\n            \n        # 5. Compute the average consecutive-window stability (Jaccard index)\n        jaccard_scores = []\n        if K  1:\n            for k_pair in range(K - 1):\n                S_k = all_S_sets[k_pair]\n                S_k_plus_1 = all_S_sets[k_pair + 1]\n                \n                intersection_len = len(S_k.intersection(S_k_plus_1))\n                union_len = len(S_k.union(S_k_plus_1))\n                \n                # J(A,B) = 1 if A and B are empty. Otherwise, |A intersect B| / |A union B|\n                if union_len == 0:\n                    jaccard_index = 1.0\n                else:\n                    jaccard_index = intersection_len / union_len\n                \n                jaccard_scores.append(jaccard_index)\n            \n            avg_jaccard = np.mean(jaccard_scores)\n        else: # Case with only one window, stability is not defined.\n            avg_jaccard = 0.0\n\n        results.append(round(avg_jaccard, 4))\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}