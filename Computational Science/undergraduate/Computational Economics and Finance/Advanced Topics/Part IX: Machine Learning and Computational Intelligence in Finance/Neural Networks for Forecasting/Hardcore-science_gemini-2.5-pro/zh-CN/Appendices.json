{
    "hands_on_practices": [
        {
            "introduction": "要想真正理解循环神经网络（RNN）如何处理序列数据，没有什么比亲自动手计算更能说明问题了。本练习将引导您完成一个简单RNN的前向传播过程，让您清楚地看到输入特征和隐藏状态如何随时间相互作用，从而产生最终的预测。通过完成这个例子 ，您将揭开循环模型核心机制的神秘面纱。",
            "id": "2387285",
            "problem": "考虑一个金融背景下的二元预测任务。对于每只股票，您将在一个固定的时间范围内观察到一个离散时间的参与度特征序列。在每个时间步 $t \\in \\{1,\\dots,T\\}$，特征向量为 $x_t \\in \\mathbb{R}^2$，其分量为 $x_t = (v_t, s_t)$，其中 $v_t$ 是一个标准化的评论速度度量，$s_t$ 是一个标准化的情绪得分。目标是为每个序列生成一个在 $(0,1)$ 区间内的单一概率，该概率表示该股票在下一个时间步将转变为高关注度状态，这里将其解释为潜在的“网红股”事件。\n\n给定一个预测规则，定义如下。设隐藏状态维度为 $d = 3$，参数如下\n$$\nW_{xh} = \\begin{bmatrix}\n0.8  -0.4 \\\\\n0.3  0.5 \\\\\n-0.2  0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1  0.0  -0.3 \\\\\n0.2  0.4  0.0 \\\\\n-0.5  0.1  0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix},\n$$\n$$\nW_{hy} = \\begin{bmatrix}\n0.6  -0.2  0.4\n\\end{bmatrix},\\quad\nb_y = -0.05.\n$$\n定义隐藏状态递归，初始条件为 $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$，且对于 $t=1,\\dots,T$：\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right),\n$$\n其中 $\\tanh(\\cdot)$ 逐元素作用。处理完整个序列后，构建对数几率\n$$\nz = W_{hy}\\,h_T + b_y,\n$$\n以及预测概率\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\n\n测试集。对于以下每个序列，您必须根据上述规则计算出相应的概率 $p$。\n\n- 情况 A（典型的参与度上升和积极情绪；$T=4$）：\n  $$\n  \\left[(0.1, 0.4),\\ (0.2, 0.5),\\ (0.3, 0.6),\\ (0.4, 0.7)\\right].\n  $$\n\n- 情况 B（零附近的近中性动态；$T=4$）：\n  $$\n  \\left[(0.0, 0.0),\\ (0.0, 0.1),\\ (0.0, -0.1),\\ (0.0, 0.0)\\right].\n  $$\n\n- 情况 C（高速度伴随负面情绪；$T=4$）：\n  $$\n  \\left[(0.5, -0.6),\\ (0.6, -0.5),\\ (0.7, -0.4),\\ (0.8, -0.3)\\right].\n  $$\n\n- 情况 D（振荡特征，包括负速度；$T=4$）：\n  $$\n  \\left[(-0.2, 0.9),\\ (0.2, -0.9),\\ (-0.1, 0.8),\\ (0.1, -0.8)\\right].\n  $$\n\n您的程序必须输出一行，其中包含与情况 A、B、C 和 D 按此顺序对应的四个概率列表。每个概率必须四舍五入到恰好 $6$ 位小数。输出格式必须是方括号内以逗号分隔的列表，例如：\n$$\n[\\text{p\\_A},\\text{p\\_B},\\text{p\\_C},\\text{p\\_D}]\n$$\n其中 $\\text{p\\_A}$、$\\text{p\\_B}$、$\\text{p\\_C}$ 和 $\\text{p\\_D}$ 分别是情况 A 到 D 的概率 $p$ 的四舍五入后的小数表示。",
            "solution": "问题陈述已经过验证，并被认定为有效。它具有科学依据，问题定义明确，客观且自洽。它描述了一个标准的离散时间循环神经网络（RNN），这是计算科学中的一个基本模型，并为确定性计算提供了所有必要的参数、初始条件和输入数据。陈述中没有矛盾、歧义或违反科学原则之处。因此，我们可以着手求解。\n\n该问题要求基于一个特征向量序列来计算预测概率 $p$。该模型是一个隐藏状态维度为 $d=3$ 的简单 RNN。对于从 $1$ 到固定范围 $T$ 的每个离散时间步 $t$，模型处理一个输入特征向量 $x_t \\in \\mathbb{R}^2$。该特征向量由两个分量组成，$x_t = (v_t, s_t)$，分别代表评论速度和情绪得分。\n\n模型的核心是隐藏状态递归。隐藏状态向量 $h_t \\in \\mathbb{R}^3$ 根据以下方程随时间演化：\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right)\n$$\n此计算针对 $t = 1, \\dots, T$ 执行。初始隐藏状态给定为零向量 $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$。函数 $\\tanh(\\cdot)$ 是双曲正切函数，逐元素地应用于其向量参数。参数包括权重矩阵 $W_{xh} \\in \\mathbb{R}^{3 \\times 2}$ 和 $W_{hh} \\in \\mathbb{R}^{3 \\times 3}$，以及一个偏置向量 $b_h \\in \\mathbb{R}^{3}$。其值提供如下：\n$$\nW_{xh} = \\begin{bmatrix}\n0.8  -0.4 \\\\\n0.3  0.5 \\\\\n-0.2  0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1  0.0  -0.3 \\\\\n0.2  0.4  0.0 \\\\\n-0.5  0.1  0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix}\n$$\n\n在处理完长度为 $T$ 的整个输入序列后，使用最终的隐藏状态 $h_T$ 来计算一个标量对数几率值 $z$。这代表了最终激活函数的输入。对数几率的计算公式为：\n$$\nz = W_{hy}\\,h_T + b_y\n$$\n其中 $W_{hy} \\in \\mathbb{R}^{1 \\times 3}$ 是一个权重矩阵（或向量），$b_y \\in \\mathbb{R}$ 是一个标量偏置。提供的值为：\n$$\nW_{hy} = \\begin{bmatrix}\n0.6  -0.2  0.4\n\\end{bmatrix},\\quad\nb_y = -0.05\n$$\n\n最后，通过将 sigmoid 函数 $\\sigma(\\cdot)$ 应用于对数几率 $z$，得到预测概率 $p$。sigmoid 函数将对数几率压缩到区间 $(0, 1)$ 内，这是概率值所要求的。\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\n每个测试用例的计算过程如下：\n$1$. 初始化隐藏状态为零向量：$h \\leftarrow \\begin{bmatrix} 0  0  0 \\end{bmatrix}^T$。\n$2$. 对于从 $1$ 到 $T=4$ 的每个时间步 $t$：\n    a. 从给定序列中取出输入向量 $x_t$。\n    b. 计算激活函数的参数：$u_t = W_{xh}\\,x_t + W_{hh}\\,h + b_h$。\n    c. 更新隐藏状态：$h \\leftarrow \\tanh(u_t)$。\n$3$. 循环结束后，$h$ 现在持有 $h_T$ 的值。\n$4$. 计算对数几率：$z = W_{hy}\\,h + b_y$。\n$5$. 计算最终概率：$p = 1 / (1 + \\exp(-z))$。\n\n将此确定性过程应用于四个指定的输入序列（情况 A、B、C 和 D），每个序列的长度均为 $T=4$。然后收集并按要求格式化所得的四个概率值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes forecast probabilities for four equity engagement sequences using a\n    specified Recurrent Neural Network (RNN) model.\n    \"\"\"\n    \n    # Define the parameters of the RNN model.\n    # All vectors are defined as column vectors for correct matrix algebra.\n    W_xh = np.array([\n        [0.8, -0.4],\n        [0.3, 0.5],\n        [-0.2, 0.7]\n    ])\n    W_hh = np.array([\n        [0.1, 0.0, -0.3],\n        [0.2, 0.4, 0.0],\n        [-0.5, 0.1, 0.2]\n    ])\n    b_h = np.array([[0.05], [-0.1], [0.0]])\n    W_hy = np.array([[0.6, -0.2, 0.4]])\n    b_y = -0.05\n\n    # Define the test cases from the problem statement.\n    # The order of cases is fixed to A, B, C, D.\n    test_cases = [\n        # Case A: Typical rising engagement and positive sentiment\n        [(0.1, 0.4), (0.2, 0.5), (0.3, 0.6), (0.4, 0.7)],\n        # Case B: Near-neutral dynamics around zero\n        [(0.0, 0.0), (0.0, 0.1), (0.0, -0.1), (0.0, 0.0)],\n        # Case C: High velocity with negative sentiment\n        [(0.5, -0.6), (0.6, -0.5), (0.7, -0.4), (0.8, -0.3)],\n        # Case D: Oscillatory features\n        [(-0.2, 0.9), (0.2, -0.9), (-0.1, 0.8), (0.1, -0.8)]\n    ]\n\n    results = []\n    \n    # Sigmoid function for final probability calculation.\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    for sequence in test_cases:\n        # Initialize hidden state h_0 = [0, 0, 0]^T.\n        # Dimension is (3, 1) to represent a column vector.\n        h = np.zeros((3, 1))\n\n        # Iterate through the time steps of the sequence.\n        for x_tuple in sequence:\n            # Reshape input tuple into a (2, 1) column vector.\n            x_t = np.array(x_tuple).reshape(2, 1)\n            \n            # Apply the RNN recurrence relation.\n            # h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n            h = np.tanh(W_xh @ x_t + W_hh @ h + b_h)\n        \n        # After the loop, h is the final hidden state h_T.\n        # Compute the logit z = W_hy * h_T + b_y.\n        # The result of matrix multiplication is a 1x1 array; .item() extracts the scalar.\n        z = (W_hy @ h + b_y).item()\n        \n        # Compute the final probability p = sigma(z).\n        p = sigmoid(z)\n        \n        results.append(p)\n\n    # Format the results to exactly 6 decimal places and create the output string.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一个强大的模型不仅在于其架构，还在于其训练方式。正则化是防止过拟合和提高模型泛化能力的一项基本技术。本练习  深入探讨了两种最常见的正则化类型，$L_1$ (Lasso) 和 $L_2$ (Ridge)，在一个简化的线性网络中展示它们如何影响模型权重，以及$L_1$正则化如何实现自动特征选择。",
            "id": "2414325",
            "problem": "你需要形式化并实现一个对照比较，研究在一个受神经网络启发的用于公司收益的线性预测模型中，$L_1$ 正则化与 $L_2$ 正则化对特征稀疏性的影响。该模型是一个单层线性网络（一个线性神经元），它是神经网络的一个特例，在此用于预测代表标准化公司收益的标准化目标。该设计的基础是在凸正则化下使用均方误差进行经验风险最小化。请从以下基础开始。\n\n1. 基于均方误差的经验风险最小化：给定输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和目标向量 $y \\in \\mathbb{R}^n$，选择权重 $w \\in \\mathbb{R}^d$ 以最小化\n$$\n\\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\, \\mathcal{R}(w),\n$$\n其中 $n$ 是样本数量，$d$ 是特征数量，$\\lambda \\ge 0$ 是正则化强度，$\\mathcal{R}$ 是一个惩罚泛函。\n\n2. $L_1$ 和 $L_2$ 正则化的定义：对于 $L_1$，定义 $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d \\lvert w_j \\rvert$；对于 $L_2$，定义 $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2 = \\frac{1}{2}\\sum_{j=1}^d w_j^2$。\n\n3. 特征稀疏性由支撑集大小量化，定义为满足 $\\lvert w_j \\rvert$ 超过一个小的数值阈值 $\\tau > 0$ 的索引 $j \\in \\{1,\\dots,d\\}$ 的数量。\n\n构建一个确定性设计，通过使用平滑变化且相关的基函数来模拟常用的金融预测因子进行收益预测。\n\nA. 数据构建（确定性，无随机性）：\n- 设 $n = 64$ 且 $t = 0,1,2,\\dots,n-1$。\n- 使用三角函数定义四个基础预测因子：\n  - $b_1(t) = \\cos\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$，\n  - $b_2(t) = \\sin\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$，\n  - $b_3(t) = \\cos\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$，\n  - $b_4(t) = \\sin\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$。\n- 将每个 $b_j$ 标准化为零均值和单位方差，得到 $s_j$，$j \\in \\{1,2,3,4\\}$。\n- 将目标（标准化公司收益）定义为线性组合：\n  $$\n  y = 0.8\\, s_1 + 1.2\\, s_3 - 1.5\\, s_4.\n  $$\n- 为了引入实际的多重共线性，定义六个额外的预测因子，作为标准化基的混合，并加入一个小的正交扰动。设 $q(t) = \\cos\\!\\left(2\\pi \\cdot 5 \\cdot t / n\\right)$ 并将其标准化为 $s_q$。然后定义：\n  - $f_1 = s_1$，\n  - $f_2 = s_2$，\n  - $f_3 = s_3$，\n  - $f_4 = s_4$，\n  - $f_5 = 0.8\\, s_1 + 0.2\\, s_2 + 0.01\\, s_q$，\n  - $f_6 = 0.5\\, s_2 - 0.3\\, s_3 + 0.01\\, s_q$，\n  - $f_7 = 0.1\\, s_1 + 0.9\\, s_4 + 0.01\\, s_q$，\n  - $f_8 = 0.3\\, s_3 + 0.4\\, s_4 + 0.3\\, s_2 + 0.01\\, s_q$，\n  - $f_9 = 0.6\\, s_1 - 0.1\\, s_3 + 0.01\\, s_q$，\n  - $f_{10} = 0.2\\, s_2 + 0.2\\, s_3 + 0.6\\, s_4 + 0.01\\, s_q$。\n- 将每个 $f_j$ 标准化为零均值和单位方差，以形成最终的设计矩阵列 $X_{\\cdot j}$，$j \\in \\{1,\\dots,10\\}$。将 $y$ 标准化为零均值（它已经是了，但在数值上强制执行）。这样得到 $X \\in \\mathbb{R}^{64 \\times 10}$ 和 $y \\in \\mathbb{R}^{64}$。\n\nB. 需要实现的求解器：\n- 对于 $L_2$ 正则化（岭回归），通过求解线性系统以闭式解的形式求解一阶最优性条件\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y,\n$$\n其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n- 对于 $L_1$ 正则化（lasso），实现带软阈值的循环坐标下降。对每个坐标 $j \\in \\{1,\\dots,d\\}$，定义\n$$\nH_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 \\quad \\text{和} \\quad \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right),\n$$\n并更新\n$$\nw_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j,\\lambda)}{H_j}, \\quad \\text{其中} \\quad \\operatorname{soft}(a,\\lambda) = \\operatorname{sign}(a)\\,\\max\\{ \\lvert a \\rvert - \\lambda, \\, 0 \\}.\n$$\n循环迭代坐标，直到 $w$ 在一次完整遍历中的 $\\ell_\\infty$ 范数变化小于一个容差 $\\varepsilon > 0$。\n\nC. 特征稀疏性度量：\n- 定义支撑集阈值为 $\\tau = 10^{-6}$。\n- 支撑集大小 $s(w)$ 是满足 $\\lvert w_j \\rvert > \\tau$ 的索引 $j$ 的数量。\n\n你必须：\n1. 按照规定构建 $X$ 和 $y$。\n2. 实现上述的 $L_2$（岭回归）求解器和 $L_1$（lasso）求解器。\n3. 对于下面的每个测试用例，计算学习到的 $w$ 并报告 $s(w)$。\n\n测试套件（五个用例）：\n- 用例 1：$L_2$ 正则化，$\\lambda = 0.0$。\n- 用例 2：$L_2$ 正则化，$\\lambda = 10.0$。\n- 用例 3：$L_1$ 正则化，$\\lambda = 0.0$。\n- 用例 4：$L_1$ 正则化，$\\lambda = 0.5$。\n- 用例 5：$L_1$ 正则化，$\\lambda = 2.2$。\n\n最终输出格式要求：\n- 你的程序应生成单行输出，包含一个由逗号分隔并用方括号括起来的列表，结果顺序与测试用例相同。具体来说，输出\n$$\n[ s(w^{(1)}), \\, s(w^{(2)}), \\, s(w^{(3)}), \\, s(w^{(4)}), \\, s(w^{(5)}) ].\n$$\n只打印此列表，不含任何额外文本。角度以弧度为单位；无物理单位适用；所有数值答案必须是整数。程序必须是完全确定性的，并且不需要用户输入。",
            "solution": "所给的问题陈述是一个在计算统计学及其在金融建模中应用的适定且科学合理的练习。它是自洽的，定义了所有必要的参数和步骤，并且没有矛盾或歧义。因此，该问题被认为是**有效的**，我将继续提供完整的解决方案。\n\n任务是比较 $L_1$ 正则化与 $L_2$ 正则化在线性预测模型中诱导稀疏性的效果。这将通过构建一个具有受控多重共线性的合成数据集，求解正则化回归问题，并量化所得权重向量的稀疏性来完成。\n\n**部分 A：数据构建**\n\n第一步是确定性地生成设计矩阵 $X \\in \\mathbb{R}^{64 \\times 10}$ 和目标向量 $y \\in \\mathbb{R}^{64}$。样本数为 $n=64$，特征数为 $d=10$。\n\n1. 定义时间索引向量为 $t = [0, 1, \\dots, n-1]$。\n2. 使用三角函数在离散时间索引 $t$ 上生成四个正交基础预测因子：\n    $$ b_1(t) = \\cos(2\\pi t / n), \\quad b_2(t) = \\sin(2\\pi t / n), \\quad b_3(t) = \\cos(4\\pi t / n), \\quad b_4(t) = \\sin(4\\pi t / n) $$\n3. 每个基础预测因子向量 $b_j$ 都被标准化为零均值和单位方差。向量 $v$ 的标准化由 $(v - \\mu_v) / \\sigma_v$ 给出，其中 $\\mu_v$ 是均值，$\\sigma_v$ 是总体标准差。设标准化后的向量表示为 $s_1, s_2, s_3, s_4$。\n4. 目标向量 $y$ 代表标准化的公司收益，被构建为这四个标准化基中的三个的已知线性组合：\n    $$ y = 0.8 s_1 + 1.2 s_3 - 1.5 s_4 $$\n    根据构造，$y$ 的均值为零，因此不需要进一步的均值中心化。\n5. 为了模拟经济数据中常见的多重共线性，构造了十个特征 $f_1, \\dots, f_{10}$。前四个是基础预测因子本身。随后的六个是这些基的线性组合，并加入了一个小的正交扰动以防止完全线性相关。该扰动源于 $q(t) = \\cos(10\\pi t / n)$，并将其标准化为向量 $s_q$。这些特征定义如下：\n    \\begin{align*}\n    f_1 = s_1 \\\\\n    f_2 = s_2 \\\\\n    f_3 = s_3 \\\\\n    f_4 = s_4 \\\\\n    f_5 = 0.8 s_1 + 0.2 s_2 + 0.01 s_q \\\\\n    f_6 = 0.5 s_2 - 0.3 s_3 + 0.01 s_q \\\\\n    f_7 = 0.1 s_1 + 0.9 s_4 + 0.01 s_q \\\\\n    f_8 = 0.3 s_3 + 0.4 s_4 + 0.3 s_2 + 0.01 s_q \\\\\n    f_9 = 0.6 s_1 - 0.1 s_3 + 0.01 s_q \\\\\n    f_{10} = 0.2 s_2 + 0.2 s_3 + 0.6 s_4 + 0.01 s_q\n    \\end{align*}\n6. 最后，将每个特征向量 $f_j$ 标准化为零均值和单位方差。这十个标准化向量构成了最终设计矩阵 $X$ 的列。\n\n**部分 B：求解器**\n\n问题要求为一般经验风险最小化问题实现两个求解器：\n$$ \\min_{w \\in \\mathbb{R}^d} \\left\\{ \\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\mathcal{R}(w) \\right\\} $$\n\n**$L_2$ 正则化（岭回归）**\n\n对于 $L_2$ 正则化，惩罚项为 $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2$。对于任何 $\\lambda > 0$，目标函数都是严格凸的，其梯度为：\n$$ \\nabla_w J(w) = \\frac{1}{n} X^\\top(Xw - y) + \\lambda w $$\n将梯度设为零，$\\nabla_w J(w) = 0$，得到一阶最优性条件：\n$$ \\frac{1}{n} X^\\top X w - \\frac{1}{n} X^\\top y + \\lambda w = 0 $$\n$$ \\left( \\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y $$\n这是问题陈述中指定的线性系统，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。该系统可以使用标准的线性代数程序直接求解。对于 $\\lambda = 0$ 的情况，这简化为普通最小二乘法（OLS）的正规方程。\n\n**$L_1$ 正则化（Lasso）**\n\n对于 $L_1$ 正则化，惩罚项为 $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d |w_j|$。目标函数是凸的，但在任何 $w_j = 0$ 的点上不可微。使用迭代方法，特别是循环坐标下降法来找到解。\n\n该算法一次更新一个权重坐标 $w_j$，同时保持所有其他权重 $w_k$（$k \\neq j$）固定。坐标 $j$ 的次梯度最优性条件导出更新规则：\n$$ w_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j, \\lambda)}{H_j} $$\n其中软阈值算子为 $\\operatorname{soft}(a, \\lambda) = \\operatorname{sign}(a) \\max(|a| - \\lambda, 0)$，且\n$$ \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right), \\quad H_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 $$\n由于设计矩阵 $X$ 的每一列都标准化为单位方差，因此对于所有 $j \\in \\{1, \\dots, d\\}$，$H_j = 1$。更新规则简化为：\n$$ w_j \\leftarrow \\operatorname{soft}(\\rho_j, \\lambda) $$\n算法重复地循环遍历所有坐标 $j=1, \\dots, d$，直到连续完整遍历之间权重向量 $w$ 的变化低于一个小的容差 $\\varepsilon$。对于此实现，对 $w$ 变化的 $\\ell_\\infty$-范数使用 $10^{-9}$ 的容差。对于 $\\lambda = 0$ 的情况，软阈值算子变为恒等函数，$\\operatorname{soft}(\\rho_j, 0) = \\rho_j$，算法变为求解 OLS 正规方程的高斯-赛德尔方法。\n\n**部分 C：特征稀疏性**\n\n稀疏性通过权重向量 $w$ 的支撑集大小来衡量。支撑集大小 $s(w)$ 是其绝对值大于数值阈值 $\\tau = 10^{-6}$ 的权重数量：\n$$ s(w) = |\\{j \\in \\{1, \\dots, d\\} : |w_j| > \\tau \\}| $$\n这个度量将为从五个指定测试用例中获得的每个解向量 $w$ 计算。\n\n实现将首先生成数据，然后对每个测试用例应用指定的求解器，最后为每个得到的权重向量计算支撑集大小。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a controlled comparison of L1 and L2 regularization\n    on feature sparsity in a linear forecasting model.\n    \"\"\"\n\n    # ------------------\n    # --- Parameters ---\n    # ------------------\n    N_SAMPLES = 64\n    N_FEATURES = 10\n    SPARSITY_THRESHOLD = 1e-6\n    LASSO_TOLERANCE = 1e-9\n    LASSO_MAX_ITER = 5000\n\n    # ----------------------------\n    # --- Part A: Data Construction ---\n    # ----------------------------\n\n    def standardize(v):\n        \"\"\"Standardizes a vector to have zero mean and unit variance.\"\"\"\n        # Using ddof=0 for population standard deviation, as is standard in ML.\n        return (v - v.mean()) / v.std(ddof=0)\n\n    t = np.arange(N_SAMPLES)\n\n    # Base predictors\n    b1 = np.cos(2 * np.pi * 1 * t / N_SAMPLES)\n    b2 = np.sin(2 * np.pi * 1 * t / N_SAMPLES)\n    b3 = np.cos(2 * np.pi * 2 * t / N_SAMPLES)\n    b4 = np.sin(2 * np.pi * 2 * t / N_SAMPLES)\n\n    # Standardized bases\n    s1 = standardize(b1)\n    s2 = standardize(b2)\n    s3 = standardize(b3)\n    s4 = standardize(b4)\n\n    # Target vector\n    y = 0.8 * s1 + 1.2 * s3 - 1.5 * s4\n    # Enforce zero mean numerically, as specified.\n    y = y - y.mean()\n\n    # Perturbation vector\n    q = np.cos(2 * np.pi * 5 * t / N_SAMPLES)\n    s_q = standardize(q)\n\n    # Feature construction\n    f_vectors = [\n        s1,\n        s2,\n        s3,\n        s4,\n        0.8 * s1 + 0.2 * s2 + 0.01 * s_q,\n        0.5 * s2 - 0.3 * s3 + 0.01 * s_q,\n        0.1 * s1 + 0.9 * s4 + 0.01 * s_q,\n        0.3 * s3 + 0.4 * s4 + 0.3 * s2 + 0.01 * s_q,\n        0.6 * s1 - 0.1 * s3 + 0.01 * s_q,\n        0.2 * s2 + 0.2 * s3 + 0.6 * s4 + 0.01 * s_q,\n    ]\n\n    # Final design matrix X\n    X = np.zeros((N_SAMPLES, N_FEATURES))\n    for j, f_vec in enumerate(f_vectors):\n        X[:, j] = standardize(f_vec)\n\n    # -----------------------\n    # --- Part B: Solvers ---\n    # -----------------------\n\n    def ridge_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Ridge regression using the closed-form solution.\"\"\"\n        n, d = X_mat.shape\n        A = (1 / n) * (X_mat.T @ X_mat) + lam * np.identity(d)\n        b = (1 / n) * (X_mat.T @ y_vec)\n        w = np.linalg.solve(A, b)\n        return w\n\n    def soft_threshold(a, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - lam, 0)\n    \n    def lasso_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Lasso regression using cyclic coordinate descent.\"\"\"\n        n, d = X_mat.shape\n        w = np.zeros(d)\n        \n        # H_j = (1/n) * sum(X_ij^2). Since columns are standardized to unit variance, H_j = 1.\n        # We can verify this for safety.\n        H_j = np.sum(X_mat**2, axis=0) / n\n\n        for _ in range(LASSO_MAX_ITER):\n            w_old = np.copy(w)\n            for j in range(d):\n                # Calculate rho_j according to the definition\n                # rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k w_k)\n                y_pred = X_mat @ w\n                residual_without_j = y_vec - y_pred + X_mat[:, j] * w[j]\n                rho_j = (1 / n) * (X_mat[:, j].T @ residual_without_j)\n                \n                # Update w_j\n                w[j] = soft_threshold(rho_j, lam) / H_j[j]\n\n            if np.linalg.norm(w - w_old, ord=np.inf)  LASSO_TOLERANCE:\n                break\n        return w\n\n    # --------------------------------\n    # --- Part C: Sparsity Metric  Execution ---\n    # --------------------------------\n\n    def calculate_support_size(w_vec):\n        \"\"\"Calculates the support size of a weight vector.\"\"\"\n        return np.sum(np.abs(w_vec) > SPARSITY_THRESHOLD)\n\n    test_cases = [\n        {'type': 'l2', 'lambda': 0.0},\n        {'type': 'l2', 'lambda': 10.0},\n        {'type': 'l1', 'lambda': 0.0},\n        {'type': 'l1', 'lambda': 0.5},\n        {'type': 'l1', 'lambda': 2.2},\n    ]\n\n    results = []\n    for case in test_cases:\n        lam = case['lambda']\n        if case['type'] == 'l2':\n            w_solution = ridge_solver(X, y, lam)\n        else: # type == 'l1'\n            w_solution = lasso_solver(X, y, lam)\n        \n        sparsity = calculate_support_size(w_solution)\n        results.append(sparsity)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在金融预测中，正确预测资产回报的*方向*有时比精确预测其*幅度*更为关键。标准的损失函数，如均方误差，并不能体现这一优先需求。本练习  让你扮演模型设计者的角色，构建一个自定义的损失函数，该函数会严厉惩罚方向性错误，从而使模型的训练目标与现实世界的金融目标更加一致。",
            "id": "2414391",
            "problem": "您正在为训练一个用于计算经济学和金融领域资产回报预测的神经网络设计一个损失函数。目标是严厉惩罚那些回报方向预测错误的预测，而不仅仅是误差的大小。工作在由 $t \\in \\{1,\\dots,T\\}$ 索引的离散时间中进行。设实现的简单回报为 $r_t \\in \\mathbb{R}$，模型的预测为 $\\hat{r}_t \\in \\mathbb{R}$。标准的基准是均方误差，即经验风险 $(1/T)\\sum_{t=1}^T (r_t - \\hat{r}_t)^2$。\n\n从统计学习中使用的基本原则出发：\n- 样本上的经验风险最小化是基于对每个样本损失的平均。\n- 平方损失鼓励精确的量值匹配。\n- 为了编码方向准确性，基于间隔的惩罚使用乘积 $r_t \\hat{r}_t$ 作为带符号的一致性度量，其中 $r_t \\hat{r}_t  0$ 表示符号一致，而 $r_t \\hat{r}_t  0$ 表示不一致。\n- 当满足间隔阈值时为 $0$，而在违反间隔时线性增长的最小凸非负惩罚是仿射函数的铰链形式。\n\n您的任务：\n1. 根据上述原则，推导出一个凸的单样本损失 $\\ell_t$。该损失在平方误差 $(r_t - \\hat{r}_t)^2$ 的基础上增加了一个方向违规惩罚项。该惩罚项仅依赖于间隔 $r_t \\hat{r}_t$ 和两个非负超参数：一个惩罚权重 $\\lambda \\ge 0$ 和一个非负的间隔阈值 $m \\ge 0$。该惩罚必须满足：\n   - 每当满足间隔要求时（即 $r_t \\hat{r}_t \\ge m$），该惩罚项恒为零。\n   - 每当 $r_t \\hat{r}_t  m$ 时，该惩罚项是相对于阈值的差额的一个线性函数，斜率为 $\\lambda$。\n   - 对于所有的 $(r_t,\\hat{r}_t)$，该惩罚项都是非负的。\n   - 使得总的单样本损失 $\\ell_t$ 对于 $\\hat{r}_t$ 是凸的。\n2. 将总体损失定义为样本平均值 $L = \\frac{1}{T}\\sum_{t=1}^T \\ell_t$。\n3. 实现一个程序，为下面这组 $(r,\\hat{r},\\lambda,m)$ 元组的测试套件计算 $L$。在每个案例中，$r$ 和 $\\hat{r}$ 是长度为 $T$ 的数组，而 $\\lambda$ 和 $m$ 是标量：\n   - 案例 A（理想情况，方向和大小均完美）：$r = [0.01,-0.02]$, $\\hat{r} = [0.01,-0.02]$, $\\lambda = 100$, $m = 0$。\n   - 案例 B（方向总是错误，零间隔）：$r = [0.01,-0.02]$, $\\hat{r} = [-0.01,0.02]$, $\\lambda = 100$, $m = 0$。\n   - 案例 C（边界/间隔测试，小回报和正间隔）：$r = [0.005,-0.003,0.004]$, $\\hat{r} = [0.0,-0.001,0.002]$, $\\lambda = 50$, $m = 0.001$。\n   - 案例 D（正确性混合，零间隔）：$r = [0.02,-0.01,-0.015]$, $\\hat{r} = [0.03,0.005,0.010]$, $\\lambda = 200$, $m = 0$。\n   - 案例 E（回退到纯平方误差）：$r = [0.01,-0.02,0.03]$, $\\hat{r} = [0.009,-0.018,0.029]$, $\\lambda = 0$, $m = 0.005$。\n4. 程序必须为每个案例计算标量损失 $L$，并生成单行输出，其中包含五个结果，形式为用方括号括起来的逗号分隔列表，每个数字四舍五入到 $6$ 位小数，例如 $[x_1,x_2,x_3,x_4,x_5]$。\n\n您的程序必须是一个完整、可运行的实现，无需用户输入即可执行这些计算。不涉及任何物理单位。不使用角度。任何地方都不得使用百分比；当出现可能被非正式地描述为百分比的量时，必须用小数表示。通过将回报视为小的实数，认识到方向准确性是通过 $r_t \\hat{r}_t$ 的符号来编码的，并且附加的惩罚项会强烈阻止符号错误，从而确保科学真实性。",
            "solution": "所提出的问题是在金融资产回报预测背景下，为训练神经网络设计一个专门的损失函数任务。该问题具有科学依据，定义明确，其所有组成部分都清晰界定。这是一个有效的数学工程问题。我将继续进行推导和求解。\n\n目标是推导单个时间步 $t$ 的单样本损失函数，记为 $\\ell_t$。该损失函数必须在标准平方误差的基础上增加一个对方向不准确性的惩罚项。大小为 $T$ 的样本的总损失是经验风险，即这些单样本损失的样本平均值。\n\n设 $r_t$ 为实现的回报，$\\hat{r}_t$ 为在时间 $t$ 的预测回报。单样本损失 $\\ell_t$ 由两部分组成：一个用于量值准确性的平方误差项和一个用于方向准确性的惩罚项，我们将其记为 $P_t$。\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + P_t(\\hat{r}_t)\n$$\n\n惩罚项 $P_t$ 必须遵守问题陈述中概述的几个原则。\n1.  它是间隔（由乘积 $r_t \\hat{r}_t$ 定义）、惩罚权重 $\\lambda \\ge 0$ 和间隔阈值 $m \\ge 0$ 的函数。\n2.  如果满足间隔条件，即 $r_t \\hat{r}_t \\ge m$，则惩罚必须为零。\n3.  如果违反间隔条件，即 $r_t \\hat{r}_t  m$，则惩罚必须是差额 $m - r_t \\hat{r}_t$ 的线性函数，斜率为 $\\lambda$。\n4.  惩罚必须为非负，即 $P_t \\ge 0$。\n\n这些条件唯一地确定了 $P_t$ 的形式。只有当 $m - r_t \\hat{r}_t  0$ 时，惩罚项才非零，此时它等于 $\\lambda (m - r_t \\hat{r}_t)$。在所有其他情况下，它为零。这正是铰链损失（hinge loss）的定义，可以使用正部函数 $\\max(0, x)$ 紧凑地写出。\n因此，惩罚项为：\n$$\nP_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\n这种形式满足所有要求。由于 $\\lambda \\ge 0$ 且 $\\max$ 函数总是非负的，所以 $P_t \\ge 0$。其他条件通过构造得以满足。\n\n因此，完整的单样本损失函数为：\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\n一个关键要求是 $\\ell_t$ 必须是关于预测 $\\hat{r}_t$ 的凸函数。由于凸函数的和也是凸函数，我们可以分别分析每一项。\n\n第一项 $L_{SE}(\\hat{r}_t) = (r_t - \\hat{r}_t)^2$ 是 $\\hat{r}_t$ 的二次函数。其关于 $\\hat{r}_t$ 的二阶导数为 $\\frac{\\partial^2}{\\partial \\hat{r}_t^2} (r_t - \\hat{r}_t)^2 = 2$，这是严格为正的。因此，平方误差项是严格凸的。\n\n第二项 $P_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$ 也是凸的。这可以通过认识到它是一个凸函数 $f(x) = \\max(0,x)$ 与一个关于 $\\hat{r}_t$ 的仿射函数 $g(\\hat{r}_t) = m - r_t \\hat{r}_t$ 的复合来确定。凸函数与仿射映射的复合是凸的。由于非负标量 $\\lambda$ 保持凸性，所以 $P_t(\\hat{r}_t)$ 关于 $\\hat{r}_t$ 是凸的。\n\n由于 $\\ell_t$ 的两项都关于 $\\hat{r}_t$ 是凸的，它们的和 $\\ell_t(\\hat{r}_t)$ 也是一个凸函数。这一性质对于确保最小化损失函数的优化问题是良态的（well-behaved），以及基于梯度的方法将收敛到全局最小值至关重要。\n\n整个样本的总体损失函数 $L$ 是经验风险，定义为 $T$ 个观测值的单样本损失的平均值：\n$$\nL = \\frac{1}{T} \\sum_{t=1}^T \\ell_t = \\frac{1}{T} \\sum_{t=1}^T \\left[ (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t) \\right]\n$$\n这就是要实现的损失函数的最终形式。\n\n对于问题的计算部分，我们将此公式应用于所提供的五个测试案例。对于每个元组 $(r, \\hat{r}, \\lambda, m)$，其中 $r$ 和 $\\hat{r}$ 是长度为 $T$ 的向量，计算过程如下：\n1.  为每个 $t \\in \\{1,\\dots,T\\}$ 计算逐元素的平方误差 $(r_t - \\hat{r}_t)^2$。\n2.  计算逐元素的间隔乘积 $r_t \\hat{r}_t$。\n3.  计算逐元素的惩罚项 $\\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$。\n4.  将每个样本点的平方误差和惩罚项相加，得到 $\\ell_t$。\n5.  计算这些单样本损失的平均值以求得 $L$。\n\n五个案例的计算值如下：\n- 案例 A: $L_A = 0.000000$\n- 案例 B: $L_B = 0.026000$\n- 案例 C: $L_C = 0.049828$\n- 案例 D: $L_D = 0.013650$\n- 案例 E: $L_E = 0.000002$\n\n实现将遵循此逻辑以生成所需的输出。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the custom loss function for several test cases in financial forecasting.\n\n    The loss function L is defined as the average of per-sample losses l_t:\n    L = (1/T) * sum_{t=1 to T} l_t\n    where l_t = (r_t - r_hat_t)^2 + lambda * max(0, m - r_t * r_hat_t)\n    - r_t: realized return\n    - r_hat_t: predicted return\n    - lambda: penalty weight\n    - m: margin threshold\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path, perfect direction and magnitude\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([0.01, -0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case B: direction always wrong, zero margin\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([-0.01, 0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case C: boundary/margin test with small returns and positive margin\n        {'r': np.array([0.005, -0.003, 0.004]), 'r_hat': np.array([0.0, -0.001, 0.002]), 'lam': 50.0, 'm': 0.001},\n        # Case D: mixed correctness, zero margin\n        {'r': np.array([0.02, -0.01, -0.015]), 'r_hat': np.array([0.03, 0.005, 0.010]), 'lam': 200.0, 'm': 0.0},\n        # Case E: fallback to pure squared error\n        {'r': np.array([0.01, -0.02, 0.03]), 'r_hat': np.array([0.009, -0.018, 0.029]), 'lam': 0.0, 'm': 0.005}\n    ]\n\n    results = []\n    for case in test_cases:\n        r = case['r']\n        r_hat = case['r_hat']\n        lam = case['lam']\n        m = case['m']\n\n        # Ensure inputs are NumPy arrays for vectorized operations\n        if not isinstance(r, np.ndarray):\n            r = np.array(r)\n        if not isinstance(r_hat, np.ndarray):\n            r_hat = np.array(r_hat)\n\n        # 1. Squared error term\n        squared_error = (r - r_hat)**2\n\n        # 2. Directional penalty term\n        # The margin is the product of the realized and predicted returns\n        margin = r * r_hat\n        # The penalty is applied when the margin is below the threshold m\n        penalty = lam * np.maximum(0, m - margin)\n\n        # 3. Per-sample loss is the sum of the two terms\n        per_sample_loss = squared_error + penalty\n\n        # 4. The overall loss is the mean of the per-sample losses\n        total_loss = np.mean(per_sample_loss)\n\n        # Append the formatted result\n        results.append(f\"{total_loss:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}