{
    "hands_on_practices": [
        {
            "introduction": "在深入研究逻辑回归的复杂实现之前，建立一个坚实的直觉至关重要：我们为什么需要它？这个练习旨在通过一个具体的计算例子，揭示使用普通线性回归进行分类任务的内在缺陷。通过解决这个问题，你将亲眼看到线性模型如何产生超出有效概率范围（即 $[0, 1]$ 区间之外）的荒谬预测，从而深刻理解为何逻辑回归等模型对于处理分类问题是必不可少的 。",
            "id": "2407549",
            "problem": "在一个计算金融中的信用风险分类任务中，考虑一个二元响应变量 $y \\in \\{0,1\\}$，它表示一家公司是否在一年内违约，以及一个单一预测变量 $x$，它是一个标准化的杠杆指数（无单位）。您观察到三家公司的样本，其 $(x,y)$ 对为：$(0,0)$、$(1,0)$ 和 $(2,1)$。\n\n使用通过普通最小二乘法 (OLS) 估计的线性概率模型，将模型 $y = \\beta_{0} + \\beta_{1} x$ 拟合到此样本，并计算在 $x = 3$ 处的 OLS 预测值。另外，回想一下，逻辑回归模型将预测变量值为 $x$ 时的违约概率指定为 $p(x) = \\frac{1}{1 + \\exp\\!\\big(-(\\gamma_{0} + \\gamma_{1} x)\\big)}$，对于任何实数参数 $\\gamma_{0}$ 和 $\\gamma_{1}$，其取值严格在开区间 $(0,1)$ 内。对于给定的参数 $\\gamma_{0} = -\\frac{1}{6}$ 和 $\\gamma_{1} = \\frac{1}{2}$，计算在 $x = 3$ 处的逻辑概率，并用它来对比这两种建模方法。\n\n作为您的最终答案，仅提供在 $x = 3$ 处的 OLS 预测值，四舍五入到四位有效数字。",
            "solution": "问题陈述具有科学依据，提法得当且客观。它展示了用于计量经济学和金融学中二元分类的两种标准统计模型之间的比较：线性概率模型 (LPM) 和逻辑回归模型。所提供的数据和参数是完整和一致的，从而可以得出一个唯一且有意义的解。因此，该问题是有效的。\n\n我们首先处理使用普通最小二乘法 (OLS) 对所提供的三个观测值样本 $(x_{1}, y_{1}) = (0, 0)$、$(x_{2}, y_{2}) = (1, 0)$ 和 $(x_{3}, y_{3}) = (2, 1)$ 拟合线性概率模型 $y = \\beta_{0} + \\beta_{1} x$ 的任务。\n\n简单线性回归系数的 OLS 估计量 $\\hat{\\beta}_{0}$ 和 $\\hat{\\beta}_{1}$ 是通过最小化残差平方和来计算的。这些估计量的公式是：\n$$\n\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})^2}\n$$\n$$\n\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1} \\bar{x}\n$$\n其中 $n$ 是样本量，$\\bar{x}$ 是预测变量的样本均值，$\\bar{y}$ 是响应变量的样本均值。\n\n对于给定的数据，其中 $n=3$：\n$x$ 的样本均值是：\n$$\n\\bar{x} = \\frac{0 + 1 + 2}{3} = \\frac{3}{3} = 1\n$$\n$y$ 的样本均值是：\n$$\n\\bar{y} = \\frac{0 + 0 + 1}{3} = \\frac{1}{3}\n$$\n\n接下来，我们计算必要的和。\n$x$ 的离差平方和是：\n$$\n\\sum_{i=1}^{3} (x_{i} - \\bar{x})^2 = (0 - 1)^2 + (1 - 1)^2 + (2 - 1)^2 = (-1)^2 + 0^2 + 1^2 = 1 + 0 + 1 = 2\n$$\n离差乘积之和是：\n$$\n\\sum_{i=1}^{3} (x_{i} - \\bar{x})(y_{i} - \\bar{y}) = (0 - 1)(0 - \\frac{1}{3}) + (1 - 1)(0 - \\frac{1}{3}) + (2 - 1)(1 - \\frac{1}{3})\n$$\n$$\n= (-1)(-\\frac{1}{3}) + (0)(-\\frac{1}{3}) + (1)(\\frac{2}{3}) = \\frac{1}{3} + 0 + \\frac{2}{3} = 1\n$$\n\n现在我们可以计算 OLS 估计量：\n$$\n\\hat{\\beta}_{1} = \\frac{1}{2}\n$$\n$$\n\\hat{\\beta}_{0} = \\frac{1}{3} - \\hat{\\beta}_{1} \\bar{x} = \\frac{1}{3} - (\\frac{1}{2})(1) = \\frac{2 - 3}{6} = -\\frac{1}{6}\n$$\n\n因此，拟合的线性概率模型是：\n$$\n\\hat{y} = -\\frac{1}{6} + \\frac{1}{2} x\n$$\n我们需要计算在 $x = 3$ 处的预测值。将 $x=3$ 代入拟合模型得到：\n$$\n\\hat{y}|_{x=3} = -\\frac{1}{6} + \\frac{1}{2}(3) = -\\frac{1}{6} + \\frac{3}{2} = -\\frac{1}{6} + \\frac{9}{6} = \\frac{8}{6} = \\frac{4}{3}\n$$\n\n为了按要求进行对比，我们现在计算 $x=3$ 时的逻辑概率。逻辑模型给出的概率 $p(x)$ 为：\n$$\np(x) = \\frac{1}{1 + \\exp(-(\\gamma_{0} + \\gamma_{1} x))}\n$$\n使用给定的参数 $\\gamma_{0} = -\\frac{1}{6}$ 和 $\\gamma_{1} = \\frac{1}{2}$，在 $x=3$ 处的概率是：\n$$\np(3) = \\frac{1}{1 + \\exp(-(-\\frac{1}{6} + \\frac{1}{2}(3)))} = \\frac{1}{1 + \\exp(-(-\\frac{1}{6} + \\frac{3}{2}))} = \\frac{1}{1 + \\exp(-(\\frac{8}{6}))} = \\frac{1}{1 + \\exp(-\\frac{4}{3})}\n$$\n数值上，$p(3) \\approx 0.7914$。\n\n两种模型之间的对比是鲜明的。来自 LPM 的 OLS 预测值为 $\\hat{y}|_{x=3} = \\frac{4}{3} \\approx 1.3333$，该值大于 $1$。将这个值解释为违约概率是毫无意义的，因为概率必须位于区间 $[0, 1]$ 内。这是线性概率模型一个众所周知的关键缺陷。相比之下，逻辑回归模型根据其数学定义，产生的概率估计值严格限制在 $0$ 和 $1$ 之间。值 $p(3) \\approx 0.7914$ 是一个有效的概率。这说明了逻辑回归在建模二元结果方面的理论优越性。\n\n问题要求给出在 $x=3$ 处的 OLS 预测值，并四舍五入到四位有效数字。\n$$\n\\frac{4}{3} = 1.3333...\n$$\n四舍五入到四位有效数字得到 $1.333$。",
            "answer": "$$\n\\boxed{1.333}\n$$"
        },
        {
            "introduction": "理解了逻辑回归的必要性之后，下一步便是将其应用于实际场景。这个练习将你置于计算金融学家的角色，任务是预测企业并购是否能为股东创造价值 。你将使用一个包含真实并购特征的数据集来训练一个带 $L_2$ 正则化的逻辑回归模型，并探索不同强度的正则化参数 $\\lambda$ 如何影响模型的预测结果，从而培养在现实世界约束下应用和调整模型的能力。",
            "id": "2407571",
            "problem": "考虑一个二元分类问题，其中每个观测值代表一次已完成的公司收购。对于观测值 $i \\in \\{1,\\dots,n\\}$，令 $\\mathbf{z}_i \\in \\mathbb{R}^d$ 表示在公告时测量的 $d$ 个可观测特征组成的向量，令 $y_i \\in \\{0,1\\}$ 为收购方在完成后一年内获得正异常回报的指示符。数据中的所有分数均已表示为 $[0,1]$ 区间内的小数。给定 $n = 14$ 个观测值，每个观测值有 $d = 4$ 个特征，这些数据被收集到矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 和标签向量 $\\mathbf{y} \\in \\{0,1\\}^n$ 中。这些特征按列分别为：相对于目标公司公告前价格的交易溢价、相对规模（目标公司市值除以收购方市值）、收购方净杠杆率以及行业重叠指数。训练数据如下\n$$\nZ=\\begin{bmatrix}\n0.20  0.30  0.25  0.80\\\\\n0.35  0.60  0.50  0.20\\\\\n0.10  0.20  0.15  0.90\\\\\n0.45  0.70  0.60  0.10\\\\\n0.25  0.40  0.30  0.70\\\\\n0.30  0.50  0.55  0.30\\\\\n0.15  0.25  0.20  0.85\\\\\n0.50  0.80  0.65  0.05\\\\\n0.18  0.35  0.22  0.75\\\\\n0.40  0.55  0.45  0.40\\\\\n0.22  0.32  0.28  0.60\\\\\n0.28  0.45  0.35  0.50\\\\\n0.38  0.65  0.58  0.25\\\\\n0.12  0.18  0.12  0.95\n\\end{bmatrix},\\quad\n\\mathbf{y}=\\begin{bmatrix}\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n0\\\\\n1\\\\\n1\\\\\n0\\\\\n1\n\\end{bmatrix}.\n$$\n假设采用以下概率模型。对于每个观测值 $i$，定义增广特征向量 $\\mathbf{x}_i = \\begin{bmatrix}1  \\mathbf{z}_i^\\top\\end{bmatrix}^\\top \\in \\mathbb{R}^{d+1}$，并令 $\\boldsymbol{\\theta}=\\begin{bmatrix}\\beta_0  \\boldsymbol{\\beta}^\\top\\end{bmatrix}^\\top \\in \\mathbb{R}^{d+1}$。在给定 $\\mathbf{z}_i$ 的条件下，$y_i=1$ 的条件概率为\n$$\n\\mathbb{P}\\!\\left(y_i=1 \\mid \\mathbf{z}_i\\right)=\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right),\\quad \\sigma(t)=\\frac{1}{1+e^{-t}}.\n$$\n参数 $\\boldsymbol{\\theta}$ 由惩罚对数似然函数的唯一最大化器（如果存在）确定\n$$\n\\ell_\\lambda(\\boldsymbol{\\theta})=\\sum_{i=1}^{n}\\left[y_i\\log\\!\\left(\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\right)+(1-y_i)\\log\\!\\left(1-\\sigma\\!\\left(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}\\right)\\right)\\right]-\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是给定的惩罚参数，截距 $\\beta_0$ 不受惩罚。\n\n您的任务是，对于下面的每个测试用例，计算具有特征 $\\mathbf{z}^\\star \\in \\mathbb{R}^d$ 的新收购创造价值的预测概率，定义为\n$$\np^\\star=\\sigma\\!\\left(\\begin{bmatrix}1  (\\mathbf{z}^\\star)^\\top\\end{bmatrix}\\boldsymbol{\\theta}_\\lambda^\\star\\right),\n$$\n其中 $\\boldsymbol{\\theta}_\\lambda^\\star$ 是对于指定的 $\\lambda$ 值，使用上述训练数据 $(Z,\\mathbf{y})$ 的 $\\ell_\\lambda(\\boldsymbol{\\theta})$ 的任意最大化器。\n\n测试套件（每个用例指定 $(\\lambda,\\mathbf{z}^\\star)$）：\n- 用例 1：$\\lambda=0.1$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.27\\\\0.40\\\\0.33\\\\0.55\\end{bmatrix}$。\n- 用例 2：$\\lambda=10.0$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.27\\\\0.40\\\\0.33\\\\0.55\\end{bmatrix}$。\n- 用例 3：$\\lambda=0.0$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.55\\\\0.90\\\\0.70\\\\0.05\\end{bmatrix}$。\n- 用例 4：$\\lambda=0.01$，$\\mathbf{z}^\\star=\\begin{bmatrix}0.08\\\\0.15\\\\0.10\\\\0.98\\end{bmatrix}$。\n\n要求：\n- 使用上述指定的模型和训练数据，为每个用例计算 $p^\\star$。\n- 将所有输出表示为在 $[0,1]$ 区间内的实数，并精确到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含按上述用例顺序排列的结果，格式为方括号内以逗号分隔的列表，例如 $\\texttt{[0.123456,0.234567,0.345678,0.456789]}$。",
            "solution": "所提出的问题是 L2 正则化逻辑回归在二元分类中的一个明确应用，这是计算金融学和计量经济学中的一种标准技术。任务是在给定一组训练数据和特定正则化参数的情况下，为几个测试用例计算出现正向结果的预测概率。该问题具有科学依据，数学上一致，并包含确定 $\\lambda > 0$ 情况下唯一解以及 $\\lambda=0$ 情况下数值可计算解所需的所有信息。\n\n该模型由具有特征 $\\mathbf{z}_i \\in \\mathbb{R}^d$ 的观测值出现正向结果 ($y_i=1$) 的条件概率定义：\n$$ \\mathbb{P}(y_i=1 \\mid \\mathbf{z}_i) = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\theta}) $$\n其中 $\\mathbf{x}_i = \\begin{bmatrix}1  \\mathbf{z}_i^\\top\\end{bmatrix}^\\top$ 是维度为 $d+1$ 的增广特征向量，$\\boldsymbol{\\theta} \\in \\mathbb{R}^{d+1}$ 是参数向量，$\\sigma(t) = (1+e^{-t})^{-1}$ 是逻辑 sigmoid 函数。\n\n参数向量 $\\boldsymbol{\\theta}_\\lambda^\\star = \\begin{bmatrix}\\beta_{0,\\lambda}^\\star  (\\boldsymbol{\\beta}_\\lambda^\\star)^\\top\\end{bmatrix}^\\top$ 通过最大化惩罚对数似然函数找到：\n$$ \\ell_\\lambda(\\boldsymbol{\\theta})=\\sum_{i=1}^{n}\\left[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)\\right]-\\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2 $$\n其中 $p_i = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\theta})$，$n=14$ 是观测值的数量，$\\boldsymbol{\\beta} = \\begin{bmatrix}\\beta_1  \\dots  \\beta_d\\end{bmatrix}^\\top$ 是 $d=4$ 个特征的系数，不包括截距 $\\beta_0$。\n\n为了解决这个最大化问题，我们等价地最小化惩罚对数似然函数的负数。设该目标函数为 $f(\\boldsymbol{\\theta}) = -\\ell_\\lambda(\\boldsymbol{\\theta})$。这是一个凸优化问题，其解可以使用数值方法找到。目标函数为：\n$$ f(\\boldsymbol{\\theta}) = -\\left( \\sum_{i=1}^{n}\\left[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)\\right] \\right) + \\frac{\\lambda}{2}\\left\\|\\boldsymbol{\\beta}\\right\\|_2^2 $$\n为了进行稳健的数值计算，交叉熵损失项可以表示为更稳定的形式。要最小化的总损失的正确表达式是：\n$$ f(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\left[ (1-y_i)\\log(1+e^{\\mathbf{x}_i^\\top\\boldsymbol{\\theta}}) + y_i\\log(1+e^{-\\mathbf{x}_i^\\top\\boldsymbol{\\theta}}) \\right] + \\frac{\\lambda}{2} \\sum_{j=1}^d \\beta_j^2 $$\n我们将使用一个拟牛顿优化算法，特别是 L-BFGS-B，它需要目标函数的梯度。梯度 $\\nabla f(\\boldsymbol{\\theta})$ 推导如下：\n$$ \\nabla f(\\boldsymbol{\\theta}) = X^\\top(\\mathbf{p} - \\mathbf{y}) + \\lambda \\boldsymbol{\\theta}_{\\text{pen}} $$\n这里，$X \\in \\mathbb{R}^{n \\times (d+1)}$ 是设计矩阵，通过在特征矩阵 $Z$ 前面加上一列全为 1 的列构成。向量 $\\mathbf{y} \\in \\{0,1\\}^n$ 包含标签，$\\mathbf{p} = \\sigma(X\\boldsymbol{\\theta})$ 是预测概率向量，而 $\\boldsymbol{\\theta}_{\\text{pen}} = \\begin{bmatrix}0  \\beta_1  \\dots  \\beta_d\\end{bmatrix}^\\top$ 是参数向量，其截距分量被置为零，以反映 $\\beta_0$ 不受惩罚的事实。\n\n对于由一对 $(\\lambda, \\mathbf{z}^\\star)$ 定义的每个测试用例，执行以下步骤：\n$1$. 为给定的 $\\lambda$ 定义目标函数 $f(\\boldsymbol{\\theta})$ 及其梯度 $\\nabla f(\\boldsymbol{\\theta})$。\n$2$. 一个数值优化器从初始猜测（例如，$\\boldsymbol{\\theta}^{(0)} = \\mathbf{0}$）开始最小化 $f(\\boldsymbol{\\theta})$，以找到最优参数向量 $\\boldsymbol{\\theta}_\\lambda^\\star$。\n$3$. 新的特征向量 $\\mathbf{z}^\\star$ 被增广为 $\\mathbf{x}^\\star = \\begin{bmatrix}1  (\\mathbf{z}^\\star)^\\top\\end{bmatrix}^\\top$。\n$4$. 预测概率 $p^\\star$ 计算为 $p^\\star = \\sigma((\\mathbf{x}^\\star)^\\top \\boldsymbol{\\theta}_\\lambda^\\star)$。\n\n对所有四个测试用例重复此过程，并将所得概率四舍五入到指定的精度。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the L2-regularized logistic regression problem for the given test cases.\n    \"\"\"\n    # Training data provided in the problem statement.\n    Z = np.array([\n        [0.20, 0.30, 0.25, 0.80],\n        [0.35, 0.60, 0.50, 0.20],\n        [0.10, 0.20, 0.15, 0.90],\n        [0.45, 0.70, 0.60, 0.10],\n        [0.25, 0.40, 0.30, 0.70],\n        [0.30, 0.50, 0.55, 0.30],\n        [0.15, 0.25, 0.20, 0.85],\n        [0.50, 0.80, 0.65, 0.05],\n        [0.18, 0.35, 0.22, 0.75],\n        [0.40, 0.55, 0.45, 0.40],\n        [0.22, 0.32, 0.28, 0.60],\n        [0.28, 0.45, 0.35, 0.50],\n        [0.38, 0.65, 0.58, 0.25],\n        [0.12, 0.18, 0.12, 0.95]\n    ])\n    \n    y = np.array([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1])\n\n    # Test suite from the problem statement.\n    test_cases = [\n        (0.1, np.array([0.27, 0.40, 0.33, 0.55])),\n        (10.0, np.array([0.27, 0.40, 0.33, 0.55])),\n        (0.0, np.array([0.55, 0.90, 0.70, 0.05])),\n        (0.01, np.array([0.08, 0.15, 0.10, 0.98]))\n    ]\n    \n    # Augment the feature matrix Z with an intercept column.\n    n, d = Z.shape\n    X = np.hstack([np.ones((n, 1)), Z])\n\n    def objective_function(theta, X_mat, y_vec, lambda_val):\n        \"\"\"\n        Computes the negative penalized log-likelihood (objective to minimize).\n        A small epsilon is used for numerical stability of the logarithm,\n        although L-BFGS-B with a good gradient is generally robust.\n        \"\"\"\n        m = X_mat.shape[0]\n        u = X_mat @ theta\n        p = expit(u)\n        \n        # To prevent log(0), clip probabilities to be within a safe range.\n        eps = 1e-15\n        p = np.clip(p, eps, 1 - eps)\n        \n        log_likelihood = np.sum(y_vec * np.log(p) + (1 - y_vec) * np.log(1 - p))\n        \n        # L2 penalty term (Ridge), excluding the intercept.\n        beta = theta[1:]\n        penalty = (lambda_val / 2) * np.sum(beta**2)\n        \n        return -(log_likelihood - penalty)\n\n    def gradient_function(theta, X_mat, y_vec, lambda_val):\n        \"\"\"\n        Computes the gradient of the negative penalized log-likelihood.\n        \"\"\"\n        m = X_mat.shape[0]\n        u = X_mat @ theta\n        p = expit(u)\n        \n        error = p - y_vec\n        grad_log_likelihood = X_mat.T @ error\n        \n        # Gradient of the L2 penalty term.\n        grad_penalty = lambda_val * theta\n        grad_penalty[0] = 0  # No penalty on the intercept.\n        \n        return grad_log_likelihood + grad_penalty\n\n    results = []\n    for lambda_val, z_star in test_cases:\n        # Initial guess for the parameters.\n        initial_theta = np.zeros(d + 1)\n\n        # Perform the optimization to find the best parameters.\n        opt_result = minimize(\n            fun=objective_function,\n            x0=initial_theta,\n            args=(X, y, lambda_val),\n            method='L-BFGS-B',\n            jac=gradient_function\n        )\n        theta_star = opt_result.x\n\n        # Augment the test vector and compute the prediction.\n        x_star = np.hstack([1, z_star])\n        p_star = expit(x_star @ theta_star)\n        \n        results.append(p_star)\n\n    # Format output according to the problem requirements.\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "为了达到对一个模型的真正精通，我们必须深入其内部工作原理。这个高级练习将引导你从零开始实现一个逻辑回归分类器，包括其核心的 Newton-Raphson 优化算法 。该问题设定在一个充满挑战性的“罕见事件”预测场景中——预测体育队伍是否能赢得冠军，并借此引入了处理不平衡数据的关键技术，如类别加权和一系列超越简单准确率的评估指标（如 F1 分数、Matthews 相关系数和 AUC-PR），这些都是在真实世界项目中取得成功的必备技能。",
            "id": "2407556",
            "problem": "要求您编写一个完整、可运行的程序，用于训练和评估一个逻辑回归分类器，以解决一个稀有事件预测问题，具体框架如下。一个球队赛季由一个标准化的赛季统计数据向量表示：胜率、每场平均分差、薪资与联盟中位数的比率以及季后赛经验指数。目标是该球队在那个赛季是否赢得总冠军。赢得总冠军是一个稀有事件。模型是带有截距项的逻辑回归，并为正类别提供一个可选的类别权重，以解决类别不平衡问题。评估重点在于不平衡类别的指标和阈值效应。\n\n从以下基本原理开始：\n- 每个观测的结果是一个伯努利随机变量，其成功概率由一个逻辑链接函数参数化。设 $y_i \\in \\{0,1\\}$ 为二元标签，$x_i \\in \\mathbb{R}^d$ 为特征向量。\n- 逻辑链接函数设定 $p_i = \\Pr(y_i = 1 \\mid x_i) = \\sigma(z_i)$，其中 $z_i = \\beta_0 + x_i^\\top \\beta$，且 $\\sigma(u) = \\dfrac{1}{1 + e^{-u}}$。\n- 需要最小化的是（可选）加权的负对数似然函数，并带有 $\\ell_2$ 正则化（岭回归）。如果 $w_i > 0$ 是观测权重，$\\lambda \\ge 0$ 是正则化强度，那么目标函数是\n$$\n\\mathcal{L}(\\beta_0, \\beta) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\lVert \\beta \\rVert_2^2,\n$$\n其中截距项 $\\beta_0$ 不受惩罚。\n\n您的程序必须：\n- 通过在完整数据集上（无小批量）使用 Newton–Raphson 方法最小化上述目标函数来实现逻辑回归训练。使用精确的梯度和 Hessian 矩阵，采用回溯线搜索以保证稳定性，并且不对截距项进行惩罚。通过为 $y_i=1$ 的样本设置 $w_i = w_+$，为 $y_i=0$ 的样本设置 $w_i = 1$，允许使用一个标量正类别权重 $w_+ > 1$；非加权训练则使用 $w_+ = 1$。\n- 训练后，在测试集上计算预测概率 $\\hat{p}_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$，并使用一个固定的阈值 $\\tau \\in (0,1)$ 将其转换为类别预测：如果 $\\hat{p}_i \\ge \\tau$，则预测 $\\hat{y}_i = 1$，否则预测 $\\hat{y}_i = 0$。\n- 在测试集上计算以下评估指标，其中 $\\mathrm{TP}$、$\\mathrm{FP}$、$\\mathrm{TN}$、$\\mathrm{FN}$ 分别是真正例、假正例、真负例和假负例：\n    - 精确率（Precision）：$\\mathrm{Prec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}}$，如果分母非零，否则定义为 $0$。\n    - 召回率（Recall，即真正例率）：$\\mathrm{Rec} = \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}}$，如果分母非零，否则定义为 $0$。\n    - $F_1$ 分数：$F_1 = \\dfrac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$，如果分母非零，否则定义为 $0$。\n    - 平衡准确率（Balanced accuracy）：$\\mathrm{BAcc} = \\dfrac{1}{2}\\left( \\dfrac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} + \\dfrac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} \\right)$，如果其中任何一个分数的分母为零，则该分数定义为 $0$。\n    - 马修斯相关系数（Matthews Correlation Coefficient, MCC）：$\\mathrm{MCC} = \\dfrac{\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}}{\\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}}$，如果分母为零，则定义为 $0$。\n    - 精确率-召回率曲线下面积（Area under the Precision–Recall curve, AUC-PR）：通过按 $\\hat{p}_i$ 降序对测试实例进行排序，并在累积阈值处评估精确率和召回率来构建精确率-召回率曲线；使用梯形法则在召回率上计算面积。\n\n训练和测试数据在下方以显式数值数组的形式提供。每个样本是一个包含四个特征的向量 $\\left[x_1, x_2, x_3, x_4\\right]$，分别对应胜率、每场平均分差、薪资与联盟中位数的比率以及季后赛经验指数。标签 $y$ 表示是否赢得总冠军。\n\n请完全按照给定的特征和标签数组使用。\n\n- 训练集 $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$，$n = 16$：\n    - $[0.55, -1.0, 0.90, 0.20] \\rightarrow 0$\n    - $[0.60, 1.50, 1.10, 0.30] \\rightarrow 0$\n    - $[0.48, -3.50, 0.85, 0.10] \\rightarrow 0$\n    - $[0.70, 5.00, 1.20, 0.50] \\rightarrow 0$\n    - $[0.75, 6.00, 1.40, 0.60] \\rightarrow 1$\n    - $[0.65, 3.00, 1.00, 0.40] \\rightarrow 0$\n    - $[0.80, 8.50, 1.50, 0.70] \\rightarrow 1$\n    - $[0.42, -6.00, 0.75, 0.10] \\rightarrow 0$\n    - $[0.58, 0.50, 0.95, 0.20] \\rightarrow 0$\n    - $[0.62, 2.00, 1.05, 0.35] \\rightarrow 0$\n    - $[0.68, 4.00, 1.10, 0.45] \\rightarrow 0$\n    - $[0.85, 10.00, 1.60, 0.80] \\rightarrow 1$\n    - $[0.50, -2.00, 0.90, 0.15] \\rightarrow 0$\n    - $[0.73, 5.50, 1.30, 0.55] \\rightarrow 0$\n    - $[0.66, 3.50, 1.15, 0.45] \\rightarrow 0$\n    - $[0.77, 7.00, 1.35, 0.60] \\rightarrow 0$\n\n- 测试集 A $\\left(X_{\\text{A}}, y_{\\text{A}}\\right)$，$m = 6$：\n    - $[0.74, 6.00, 1.25, 0.50] \\rightarrow 1$\n    - $[0.57, 0.00, 1.00, 0.20] \\rightarrow 0$\n    - $[0.82, 9.00, 1.55, 0.75] \\rightarrow 1$\n    - $[0.45, -4.50, 0.80, 0.05] \\rightarrow 0$\n    - $[0.63, 2.50, 1.10, 0.40] \\rightarrow 0$\n    - $[0.79, 7.50, 1.45, 0.65] \\rightarrow 1$\n\n- 测试集 B $\\left(X_{\\text{B}}, y_{\\text{B}}\\right)$，$m = 8$：\n    - $[0.61, 1.50, 1.05, 0.30] \\rightarrow 0$\n    - $[0.52, -1.50, 0.90, 0.12] \\rightarrow 0$\n    - $[0.76, 6.50, 1.40, 0.60] \\rightarrow 1$\n    - $[0.59, 0.50, 0.95, 0.25] \\rightarrow 0$\n    - $[0.47, -4.00, 0.82, 0.08] \\rightarrow 0$\n    - $[0.66, 3.00, 1.12, 0.40] \\rightarrow 0$\n    - $[0.64, 2.00, 1.08, 0.38] \\rightarrow 0$\n    - $[0.54, -0.50, 0.97, 0.20] \\rightarrow 0$\n\n测试套件。在所有情况下都在 $\\left(X_{\\text{train}}, y_{\\text{train}}\\right)$ 上进行训练，但改变阈值和正类别权重以测试不同方面：\n- 情况 1（理想路径）：非加权训练，$\\lambda = 1.0$，$\\tau = 0.5$，最大迭代次数 $= 100$，容忍度 $= 10^{-9}$；在测试集 A 上评估。\n- 情况 2（阈值边界）：非加权训练，$\\lambda = 1.0$，$\\tau = 0.9$，最大迭代次数 $= 100$，容忍度 $= 10^{-9}$；在测试集 A 上评估。\n- 情况 3（类别不平衡缓解）：加权训练，正类别权重 $w_+ = 3.0$，$\\lambda = 1.0$，$\\tau = 0.5$，最大迭代次数 $= 100$，容忍度 $= 10^{-9}$；在测试集 B 上评估。\n\n最终输出要求：\n- 对于每种情况，按此确切顺序将指标报告为列表：$[\\mathrm{Prec}, \\mathrm{Rec}, F_1, \\mathrm{BAcc}, \\mathrm{MCC}, \\mathrm{AUC\\mbox{-}PR}]$，每个值四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含结果，形式为一个由三个情况级别列表组成的逗号分隔列表，并用方括号括起来，例如：$[[a_1,\\dots,a_6],[b_1,\\dots,b_6],[c_1,\\dots,c_6]]$，其中每个 $a_j$、$b_j$、$c_j$ 都是一个四舍五入到 $6$ 位小数的浮点数。\n\n不允许用户输入或使用外部文件；程序必须是完全自包含和确定性的。本问题不使用角度。不涉及物理单位。所有答案都是浮点数，必须按规定进行四舍五入。",
            "solution": "所呈现的问题是计算统计学中一个明确定义的任务：为一个二元分类问题，特别是针对稀有事件场景，实现、训练和评估一个逻辑回归分类器。该问题具有科学依据，在数学上是完备的，并在算法上进行了规定。因此，它是有效的，我们将从第一性原理出发，构建一个严谨的解决方案。\n\n问题的核心是找到一个逻辑回归模型的参数，以最小化一个指定的目标函数。该模型通过逻辑函数 $p_i = \\sigma(\\beta_0 + x_i^\\top \\beta)$ 来预测具有特征向量 $x_i \\in \\mathbb{R}^d$ 的观测值为正例（$y_i=1$）的概率，其中 $\\sigma(u) = (1 + e^{-u})^{-1}$。为方便表示，我们将特征向量增广为 $\\tilde{x}_i = [1, x_i^\\top]^\\top$，将参数向量增广为 $\\tilde{\\beta} = [\\beta_0, \\beta^\\top]^\\top$，因此线性部分为 $z_i = \\tilde{x}_i^\\top \\tilde{\\beta}$。\n\n需要最小化的目标函数是加权的负对数似然函数，并对特征系数 $\\beta$（但不包括截距项 $\\beta_0$）施加 $\\ell_2$ 惩罚：\n$$\n\\mathcal{L}(\\tilde{\\beta}) = -\\sum_{i=1}^{n} w_i \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] + \\frac{\\lambda}{2} \\beta^\\top \\beta\n$$\n在这里，$n$ 是训练样本的数量，$w_i$ 是样本权重，$\\lambda$ 是正则化强度。如果 $y_i=1$，权重 $w_i$ 被设置为指定的正类别权重 $w_+$；如果 $y_i=0$，则权重为 $1$。\n\n优化过程使用 Newton-Raphson 方法，这是一种二阶迭代算法。每次迭代根据以下规则更新参数向量 $\\tilde{\\beta}$：\n$$\n\\tilde{\\beta}^{(k+1)} = \\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}\n$$\n其中 $\\alpha$ 是步长，$\\Delta \\tilde{\\beta}^{(k)}$ 是牛顿步长，通过求解以下线性系统得到：\n$$\n\\mathbf{H}(\\tilde{\\beta}^{(k)}) \\Delta \\tilde{\\beta}^{(k)} = -\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)})\n$$\n在此方程中，$\\nabla \\mathcal{L}$ 是目标函数 $\\mathcal{L}$ 的梯度（一阶偏导数向量），$\\mathbf{H}$ 是其 Hessian 矩阵（二阶偏导数矩阵）。\n\n梯度向量 $\\nabla \\mathcal{L}(\\tilde{\\beta})$ 推导如下：\n$$\n\\nabla \\mathcal{L}(\\tilde{\\beta}) = \\tilde{X}^\\top W (p - y) + \\lambda \\Lambda \\tilde{\\beta}\n$$\n这里，$\\tilde{X}$ 是 $n \\times (d+1)$ 的增广设计矩阵，$W$ 是一个由样本权重 $w_i$ 构成的 $n \\times n$ 对角矩阵，$p$ 是预测概率向量，$y$ 是真实标签向量，$\\Lambda$ 是一个 $(d+1) \\times (d+1)$ 的对角矩阵，其中 $\\Lambda_{00}=0$ 且对于 $j>0$ 有 $\\Lambda_{jj}=1$，这确保了截距项不被惩罚。\n\nHessian 矩阵 $\\mathbf{H}(\\tilde{\\beta})$ 推导如下：\n$$\n\\mathbf{H}(\\tilde{\\beta}) = \\tilde{X}^\\top S \\tilde{X} + \\lambda \\Lambda\n$$\n其中 $S$ 是一个 $n \\times n$ 对角矩阵，其元素为 $S_{ii} = w_i p_i (1 - p_i)$。由于此 Hessian 矩阵是对称的，并且对于 $\\lambda > 0$ 是正定的，因此求解牛顿步长的线性系统有唯一解。\n\n为确保稳健收敛，步长 $\\alpha$ 通过回溯线搜索确定。从 $\\alpha=1$ 开始，步长以因子 $\\rho$（例如，$0.5$）迭代减小，直到满足 Armijo-Goldstein 条件：\n$$\n\\mathcal{L}(\\tilde{\\beta}^{(k)} + \\alpha \\Delta \\tilde{\\beta}^{(k)}) \\le \\mathcal{L}(\\tilde{\\beta}^{(k)}) + c_1 \\alpha (\\nabla \\mathcal{L}(\\tilde{\\beta}^{(k)}))^\\top \\Delta \\tilde{\\beta}^{(k)}\n$$\n其中 $c_1$ 是一个小常数（例如，$10^{-4}$）。\n\n训练后，模型性能在测试集上进行评估。计算预测概率 $\\hat{p}_i$ 并使用指定的阈值 $\\tau$ 将其转换为类别标签 $\\hat{y}_i \\in \\{0,1\\}$。评估依赖于多个指标，这些指标根据真正例（$\\mathrm{TP}$）、假正例（$\\mathrm{FP}$）、真负例（$\\mathrm{TN}$）和假负例（$\\mathrm{FN}$）定义：\n- **精确率**：$\\mathrm{Prec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FP})$\n- **召回率**：$\\mathrm{Rec} = \\mathrm{TP} / (\\mathrm{TP} + \\mathrm{FN})$\n- **$F_1$ 分数**：$F_1 = 2 \\cdot (\\mathrm{Prec} \\cdot \\mathrm{Rec}) / (\\mathrm{Prec} + \\mathrm{Rec})$\n- **平衡准确率**：$\\mathrm{BAcc} = 0.5 \\cdot (\\mathrm{Rec} + \\mathrm{TN} / (\\mathrm{TN} + \\mathrm{FP}))$\n- **马修斯相关系数 (MCC)**：$\\mathrm{MCC} = (\\mathrm{TP}\\cdot \\mathrm{TN} - \\mathrm{FP}\\cdot \\mathrm{FN}) / \\sqrt{(\\mathrm{TP} + \\mathrm{FP})(\\mathrm{TP} + \\mathrm{FN})(\\mathrm{TN} + \\mathrm{FP})(\\mathrm{TN} + \\mathrm{FN})}$\n如果分母为零，每个指标都定义为 $0$。\n\n**精确率-召回率曲线下面积（AUC-PR）** 通过数值积分计算。测试实例按预测概率降序排序。通过依次考虑已排序实例的更大子集，生成一系列精确率和召回率值。然后使用梯形法则计算所得曲线下的面积。召回率值（非递减）作为积分点。曲线锚定在起始点 $(\\text{recall}=0, \\text{precision}=1)$，以正确计算召回率范围开始部分的面积。\n\n所提供的实现封装了整个过程。它定义了必要的数据结构，实现了带有回溯线搜索的 Newton-Raphson 训练器，并为问题陈述中指定的三个测试用例计算了全套评估指标。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve as solve_linear_system\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression experiments as specified.\n    \"\"\"\n    \n    # --- Data Definition ---\n    X_train = np.array([\n        [0.55, -1.0, 0.90, 0.20], [0.60, 1.50, 1.10, 0.30], [0.48, -3.50, 0.85, 0.10],\n        [0.70, 5.00, 1.20, 0.50], [0.75, 6.00, 1.40, 0.60], [0.65, 3.00, 1.00, 0.40],\n        [0.80, 8.50, 1.50, 0.70], [0.42, -6.00, 0.75, 0.10], [0.58, 0.50, 0.95, 0.20],\n        [0.62, 2.00, 1.05, 0.35], [0.68, 4.00, 1.10, 0.45], [0.85, 10.00, 1.60, 0.80],\n        [0.50, -2.00, 0.90, 0.15], [0.73, 5.50, 1.30, 0.55], [0.66, 3.50, 1.15, 0.45],\n        [0.77, 7.00, 1.35, 0.60]\n    ])\n    y_train = np.array([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n\n    X_A = np.array([\n        [0.74, 6.00, 1.25, 0.50], [0.57, 0.00, 1.00, 0.20], [0.82, 9.00, 1.55, 0.75],\n        [0.45, -4.50, 0.80, 0.05], [0.63, 2.50, 1.10, 0.40], [0.79, 7.50, 1.45, 0.65]\n    ])\n    y_A = np.array([1, 0, 1, 0, 0, 1])\n\n    X_B = np.array([\n        [0.61, 1.50, 1.05, 0.30], [0.52, -1.50, 0.90, 0.12], [0.76, 6.50, 1.40, 0.60],\n        [0.59, 0.50, 0.95, 0.25], [0.47, -4.00, 0.82, 0.08], [0.66, 3.00, 1.12, 0.40],\n        [0.64, 2.00, 1.08, 0.38], [0.54, -0.50, 0.97, 0.20]\n    ])\n    y_B = np.array([0, 0, 1, 0, 0, 0, 0, 0])\n    \n    test_cases = [\n        # (lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol)\n        (1.0, 1.0, 0.5, X_A, y_A, 100, 1e-9),\n        (1.0, 1.0, 0.9, X_A, y_A, 100, 1e-9),\n        (1.0, 3.0, 0.5, X_B, y_B, 100, 1e-9)\n    ]\n    \n    results = []\n    \n    for case in test_cases:\n        lambda_reg, w_plus, tau, X_test, y_test, max_iter, tol = case\n        \n        # Train model\n        beta = _logistic_regression_train(X_train, y_train, lambda_reg, w_plus, max_iter, tol)\n        \n        # Get predictions\n        y_proba = _predict_proba(X_test, beta)\n        y_pred = (y_proba >= tau).astype(int)\n        \n        # Evaluate metrics\n        metrics = _evaluate_metrics(y_test, y_pred, y_proba)\n        results.append(metrics)\n        \n    # Format and print the final output\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef _sigmoid(z):\n    # Clip to avoid overflow/underflow in exp\n    z_clipped = np.clip(z, -500, 500)\n    p = 1 / (1 + np.exp(-z_clipped))\n    # Clip probabilities to avoid log(0)\n    return np.clip(p, 1e-15, 1 - 1e-15)\n\ndef _calculate_cost(X_aug, y, beta, weights, lambda_reg):\n    p = _sigmoid(X_aug @ beta)\n    log_likelihood = -np.sum(weights * (y * np.log(p) + (1 - y) * np.log(1 - p)))\n    reg_term = (lambda_reg / 2) * np.dot(beta[1:], beta[1:])\n    return log_likelihood + reg_term\n\ndef _logistic_regression_train(X, y, lambda_reg, w_plus, max_iter, tol):\n    n_samples, n_features = X.shape\n    X_aug = np.c_[np.ones(n_samples), X]\n    beta = np.zeros(n_features + 1)\n    \n    weights = np.ones(n_samples)\n    weights[y == 1] = w_plus\n\n    lambda_vec = np.full(n_features + 1, lambda_reg)\n    lambda_vec[0] = 0.0\n\n    # Newton-Raphson iterations\n    for _ in range(max_iter):\n        z = X_aug @ beta\n        p = _sigmoid(z)\n        \n        error = p - y\n        grad = X_aug.T @ (weights * error) + lambda_vec * beta\n        \n        S_diag = weights * p * (1 - p)\n        H = (X_aug.T * S_diag) @ X_aug + np.diag(lambda_vec)\n        \n        # Solve H * delta_beta = -grad\n        # Use scipy.linalg.solve for stability and performance\n        search_dir = solve_linear_system(H, -grad)\n        \n        # Backtracking line search\n        alpha = 1.0\n        c1 = 1e-4\n        rho = 0.5\n        cost_current = _calculate_cost(X_aug, y, beta, weights, lambda_reg)\n        grad_dot_dir = np.dot(grad, search_dir)\n        \n        while True:\n            beta_new = beta + alpha * search_dir\n            cost_new = _calculate_cost(X_aug, y, beta_new, weights, lambda_reg)\n            if cost_new = cost_current + c1 * alpha * grad_dot_dir or alpha  1e-9:\n                break\n            alpha *= rho\n        \n        step = alpha * search_dir\n        beta += step\n        \n        if np.linalg.norm(step)  tol:\n            break\n            \n    return beta\n\ndef _predict_proba(X, beta):\n    X_aug = np.c_[np.ones(X.shape[0]), X]\n    return _sigmoid(X_aug @ beta)\n\ndef _evaluate_metrics(y_true, y_pred, y_proba):\n    tp = np.sum((y_true == 1)  (y_pred == 1))\n    tn = np.sum((y_true == 0)  (y_pred == 0))\n    fp = np.sum((y_true == 0)  (y_pred == 1))\n    fn = np.sum((y_true == 1)  (y_pred == 0))\n    \n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n    \n    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n    \n    tpr = recall\n    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n    balanced_accuracy = 0.5 * (tpr + tnr)\n\n    mcc_denom_sq = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    mcc = (tp * tn - fp * fn) / np.sqrt(mcc_denom_sq) if mcc_denom_sq > 0 else 0.0\n        \n    # AUC-PR calculation\n    sort_indices = np.argsort(y_proba)[::-1]\n    y_true_sorted = y_true[sort_indices]\n    \n    tps = np.cumsum(y_true_sorted)\n    fps = np.cumsum(1 - y_true_sorted)\n    \n    total_positives = np.sum(y_true)\n    if total_positives == 0:\n        auc_pr = 0.0\n    else:\n        recalls = tps / total_positives\n        precisions = tps / (tps + fps)\n        \n        # Keep only points where recall changes (at positive instances)\n        is_positive = (y_true_sorted == 1)\n        recall_points = recalls[is_positive]\n        precision_points = precisions[is_positive]\n\n        # Prepend starting point for trapezoidal rule\n        recall_aug = np.concatenate([[0.], recall_points])\n        precision_aug = np.concatenate([[1.], precision_points])\n        \n        auc_pr = np.trapz(precision_aug, recall_aug)\n\n    metrics = [precision, recall, f1_score, balanced_accuracy, mcc, auc_pr]\n    return [round(m, 6) for m in metrics]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}