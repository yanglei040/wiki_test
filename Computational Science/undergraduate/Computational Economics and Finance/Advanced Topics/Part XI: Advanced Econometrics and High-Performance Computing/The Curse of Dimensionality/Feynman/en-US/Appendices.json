{
    "hands_on_practices": [
        {
            "introduction": "The first step to understanding the curse of dimensionality is to appreciate the sheer scale it introduces. Many optimization problems, such as those in supply chain management, rely on defining the system's \"state.\" This practice uses basic combinatorics to calculate the size of the state space for a realistic, multi-echelon inventory system . By working through this calculation, you will see firsthand how adding even a few components or choices can cause the number of possible states to explode, making methods that require exploring the entire state space computationally intractable.",
            "id": "2439673",
            "problem": "Consider a discrete-time, periodic-review model of a global, multi-stage inventory system for two substitutable products. The network has three echelons: a single manufacturing plant, three regional distribution centers, and five retail stores per distribution center. Thus there are $1 + 3 + 15$ physical locations. Time is divided into equal periods, and all quantities below are measured in standardized container-loads.\n\nDefine the system state at the beginning of a period as the complete specification of:\n- For each location and each product, the on-hand inventory (nonnegative integer).\n- For each directed shipping link and each product, the vector of in-transit shipments by remaining lead time (nonnegative integer in each lead-time slot).\n- A single global demand-regime indicator that captures macro demand conditions.\n\nAssume the following capacity and discretization structure:\n- On-hand inventory per product at the plant is any integer in $\\{0,1,2,\\dots,40\\}$.\n- On-hand inventory per product at each distribution center is any integer in $\\{0,1,2,\\dots,30\\}$.\n- On-hand inventory per product at each retail store is any integer in $\\{0,1,2,\\dots,20\\}$.\n- Lead time from the plant to a distribution center is $2$ periods; for each plant-to-distribution-center link and each product, the pipeline is represented by a length-$2$ vector of nonnegative integers, each slot allowed to take any value in $\\{0,1,2,\\dots,10\\}$.\n- Lead time from a distribution center to a retail store is $1$ period; for each distribution-center-to-retail-store link and each product, the pipeline is represented by a length-$1$ vector (a single slot) allowed to take any value in $\\{0,1,2,\\dots,10\\}$.\n- The single global demand-regime indicator takes one of $3$ possible values.\n\nUnder this exact state definition, what is the total number of distinct system states implied by the model? Express your final answer as a single closed-form analytic expression (a product of integer powers). No approximation is required, and no units are to be reported.",
            "solution": "The total number of distinct system states, denoted by $N_{total}$, is found by applying the fundamental principle of counting (the rule of product). The total state space is the Cartesian product of the state spaces of its independent components. These components are the on-hand inventories at all locations, the in-transit inventories on all shipping links, and the global demand indicator.\n\nWe define the total number of states as:\n$$N_{total} = N_{inv} \\times N_{pipe} \\times N_{demand}$$\nwhere $N_{inv}$ is the number of states for on-hand inventory, $N_{pipe}$ is the number of states for in-transit inventory, and $N_{demand}$ is the number of states for the demand indicator. For a discrete variable that can take integer values in the set $\\{0, 1, \\dots, k\\}$, there are $k+1$ possible values.\n\n1.  **Calculation of On-Hand Inventory States ($N_{inv}$)**\n    The inventory state is specified for each of the $2$ products at every physical location.\n    -   **Plant:** There is $1$ plant. The inventory level for each of the $2$ products can take $40+1 = 41$ distinct values. The number of inventory states at the plant is $(41)^{2}$.\n    -   **Distribution Centers (DCs):** There are $3$ DCs. At each DC, the inventory for each of the $2$ products can take $30+1 = 31$ values. For a single DC, this gives $(31)^{2}$ states. Across all $3$ independent DCs, the number of states is $((31)^{2})^{3} = 31^{6}$.\n    -   **Retail Stores:** There are $3 \\times 5 = 15$ retail stores. At each store, the inventory for each of the $2$ products can take $20+1 = 21$ values. For a single store, this gives $(21)^{2}$ states. Across all $15$ independent stores, the number of states is $((21)^{2})^{15} = 21^{30}$.\n\n    The total number of states for on-hand inventory is the product of these values:\n    $$N_{inv} = 41^{2} \\times 31^{6} \\times 21^{30}$$\n\n2.  **Calculation of In-Transit Inventory States ($N_{pipe}$)**\n    The in-transit inventory, or pipeline, is specified for each shipping link and each product.\n    -   **Plant-to-DC Pipeline:** There are $3$ links from the plant to the DCs. For each of these links and for each of the $2$ products, the pipeline is a vector of length $2$ (corresponding to a lead time of $2$ periods). Each element of this vector can take $10+1 = 11$ values. Thus, for a single link-product pair, there are $11^{2}$ states. As there are $3 \\times 2 = 6$ such independent link-product pairs, the total number of states for this pipeline echelon is $(11^{2})^{6} = 11^{12}$.\n    -   **DC-to-Retail Pipeline:** There are $3 \\times 5 = 15$ links from DCs to retail stores. For each of these links and for each of the $2$ products, the pipeline consists of a single value (lead time of $1$ period), which can take $10+1 = 11$ values. For a single link-product pair, there are $11^{1} = 11$ states. Across all $15 \\times 2 = 30$ independent link-product pairs, the total number of states is $(11^{1})^{30} = 11^{30}$.\n\n    The total number of states for in-transit inventory is the product:\n    $$N_{pipe} = 11^{12} \\times 11^{30} = 11^{12+30} = 11^{42}$$\n\n3.  **Calculation of Demand-Regime Indicator States ($N_{demand}$)**\n    The problem specifies a single global indicator with $3$ possible values.\n    $$N_{demand} = 3$$\n\n4.  **Calculation of Total System States ($N_{total}$)**\n    We now combine the results for each major component to find the total size of the state space.\n    $$N_{total} = N_{inv} \\times N_{pipe} \\times N_{demand}$$\n    $$N_{total} = (41^{2} \\times 31^{6} \\times 21^{30}) \\times (11^{42}) \\times 3$$\n    To express this as a product of powers of prime numbers, we first rearrange the terms and then decompose the composite base $21$.\n    $$N_{total} = 3 \\times 11^{42} \\times 21^{30} \\times 31^{6} \\times 41^{2}$$\n    The base $21$ has prime factors $3$ and $7$, so $21^{30} = (3 \\times 7)^{30} = 3^{30} \\times 7^{30}$. Substituting this:\n    $$N_{total} = 3^{1} \\times 11^{42} \\times (3^{30} \\times 7^{30}) \\times 31^{6} \\times 41^{2}$$\n    Finally, we combine the powers of the base $3$ and order the factors by their prime bases:\n    $$N_{total} = 3^{1+30} \\times 7^{30} \\times 11^{42} \\times 31^{6} \\times 41^{2}$$\n    $$N_{total} = 3^{31} \\times 7^{30} \\times 11^{42} \\times 31^{6} \\times 41^{2}$$\nThis staggering number demonstrates the \"curse of dimensionality,\" a fundamental challenge in solving such large-scale dynamic optimization problems.",
            "answer": "$$ \\boxed{3^{31} \\times 7^{30} \\times 11^{42} \\times 31^{6} \\times 41^{2}} $$"
        },
        {
            "introduction": "A vast state space has a critical consequence: it becomes \"sparse,\" meaning a fixed number of data points become increasingly isolated from one another. This makes it difficult to learn about the system from data. This exercise  moves from counting states to quantifying this data sparsity. Using a standard result from non-parametric statistics, you will derive an analytical expression for how the required sample size $n$ must grow with dimension $d$ to maintain a constant level of estimation accuracy, revealing the exponential relationship between dimension and data requirements.",
            "id": "2439710",
            "problem": "In a sequential decision problem in computational economics and finance, suppose one uses a non-parametric regression estimator to approximate a conditional expectation with a $d$-dimensional state vector. Assume the unknown regression function belongs to a H\\\"older class with smoothness parameter $p \\in (0,\\infty)$, and let the performance criterion be the Mean Squared Error (MSE), denoted by $R_{d}(n)$ for sample size $n$. It is a well-tested fact from non-parametric estimation theory that there is a constant $C>0$, independent of $n$ and uniform over the range of dimensions of interest, such that for sufficiently large $n$,\n$$\nR_{d}(n) \\leq C \\, n^{-\\frac{p}{p+d}}.\n$$\nFix a target error level $\\varepsilon \\in (0,\\infty)$ that you wish to maintain as $d$ varies. Under the bound above, derive the minimal sample size $n(d)$, as a closed-form function of $p$, $d$, $C$, and $\\varepsilon$, that guarantees $R_{d}(n(d)) \\leq \\varepsilon$. Express your final answer as a single analytical expression. Do not provide an inequality. Do not include units. Do not round.",
            "solution": "The problem states that for a non-parametric regression estimator, the Mean Squared Error (MSE), denoted by $R_d(n)$, is bounded from above. The state vector has dimension $d$, the sample size is $n$, and the unknown regression function has H\\\"older smoothness $p$. The inequality is given as:\n$$\nR_{d}(n) \\leq C \\, n^{-\\frac{p}{p+d}}\n$$\nwhere $C > 0$ is a constant independent of $n$ and $d$.\n\nThe objective is to find the minimal sample size, which we denote $n(d)$, that guarantees the MSE does not exceed a prespecified target error level $\\varepsilon > 0$. That is, we require $R_{d}(n(d)) \\leq \\varepsilon$.\n\nTo guarantee this condition is met, we must ensure that the upper bound on $R_{d}(n)$ is itself less than or equal to $\\varepsilon$. This leads to the inequality:\n$$\nC \\, n^{-\\frac{p}{p+d}} \\leq \\varepsilon\n$$\n\nWe must solve this inequality for $n$. The parameters $p$ and $d$ are positive, so the exponent $-\\frac{p}{p+d}$ is negative. Consequently, the expression $C \\, n^{-\\frac{p}{p+d}}$ is a monotonically decreasing function of $n$. Therefore, to find the minimum sample size $n$ that satisfies the inequality, we should solve the corresponding equality:\n$$\nC \\, n^{-\\frac{p}{p+d}} = \\varepsilon\n$$\nAny $n$ larger than the solution to this equation will result in a smaller value for the bound, thus also satisfying the inequality. The solution to this equation gives the precise minimum $n$ that guarantees the bound is met.\n\nWe proceed to solve for $n$. First, isolate the term containing $n$:\n$$\nn^{-\\frac{p}{p+d}} = \\frac{\\varepsilon}{C}\n$$\nNext, to solve for $n$, we raise both sides of the equation to the power of the reciprocal of the exponent of $n$. The exponent is $-\\frac{p}{p+d}$, and its reciprocal is $-\\frac{p+d}{p}$.\n$$\n\\left( n^{-\\frac{p}{p+d}} \\right)^{-\\frac{p+d}{p}} = \\left( \\frac{\\varepsilon}{C} \\right)^{-\\frac{p+d}{p}}\n$$\nThis simplifies the left-hand side to $n^1 = n$:\n$$\nn = \\left( \\frac{\\varepsilon}{C} \\right)^{-\\frac{p+d}{p}}\n$$\nUsing the property of exponents $(a/b)^{-x} = (b/a)^x$, we can write the expression in a more direct form:\n$$\nn = \\left( \\frac{C}{\\varepsilon} \\right)^{\\frac{p+d}{p}}\n$$\nThis function, $n(d) = \\left( \\frac{C}{\\varepsilon} \\right)^{\\frac{p+d}{p}}$, represents the minimal sample size required to ensure the MSE is at most $\\varepsilon$, according to the provided bound. This expression demonstrates the \"curse of dimensionality\": for a fixed smoothness $p$ and target error $\\varepsilon$, the required sample size $n(d)$ grows exponentially with the dimension $d$. Specifically, we can write $n(d)$ as:\n$$\nn(d) = \\left( \\frac{C}{\\varepsilon} \\right)^{1 + \\frac{d}{p}} = \\left( \\frac{C}{\\varepsilon} \\right) \\cdot \\left[ \\left( \\frac{C}{\\varepsilon} \\right)^{\\frac{1}{p}} \\right]^d\n$$\nThis confirms the exponential dependence on $d$. The problem asked only for the closed-form function of $p$, $d$, $C$, and $\\varepsilon$, which has been derived.",
            "answer": "$$\n\\boxed{\\left(\\frac{C}{\\varepsilon}\\right)^{\\frac{p+d}{p}}}\n$$"
        },
        {
            "introduction": "Theoretical results, like the one derived in the previous exercise, are powerful but can feel abstract. A hands-on simulation can make the implications concrete and intuitive. In this practice , you will implement a multivariate Kernel Density Estimator (KDE), a widely used non-parametric tool, and numerically measure its performance. By observing that the estimator's error decreases much more slowly in higher dimensions, you will gain a practical, empirical feel for the curse of dimensionality that goes beyond formulas.",
            "id": "2439662",
            "problem": "You are asked to quantify how the convergence of a multivariate Kernel Density Estimator (KDE) slows as the dimension increases, illustrating the curse of dimensionality in computational economics and finance. Consider independent and identically distributed samples $X_1,\\dots,X_n \\in \\mathbb{R}^d$ drawn from the $d$-variate standard normal distribution with density\n$$\nf_d(x) = (2\\pi)^{-d/2}\\exp\\!\\left(-\\tfrac{1}{2}\\lVert x\\rVert_2^2\\right),\n$$\nwhere $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. Define the Gaussian product kernel\n$$\nK_d(u) = (2\\pi)^{-d/2}\\exp\\!\\left(-\\tfrac{1}{2}\\lVert u\\rVert_2^2\\right),\n$$\nand the Kernel Density Estimator (KDE) with bandwidth $h>0$ as\n$$\n\\widehat{f}_{n,d,h}(x) = \\frac{1}{n h^d}\\sum_{i=1}^n K_d\\!\\left(\\frac{x - X_i}{h}\\right).\n$$\nFor each combination of sample size $n$ and dimension $d$, set the bandwidth to\n$$\nh(n,d) = n^{-1/(d+4)}.\n$$\nDefine the Monte Carlo proxy for the mean integrated squared error with respect to the true distribution (that is, the mean squared error under $X\\sim f_d$) as\n$$\n\\operatorname{MSE}_{\\text{MC}}(n,d) = \\frac{1}{Q}\\sum_{j=1}^{Q}\\left(\\widehat{f}_{n,d,h(n,d)}(Z_j) - f_d(Z_j)\\right)^2,\n$$\nwhere $Z_1,\\dots,Z_Q$ are independent draws from the $d$-variate standard normal distribution.\n\nImplement a complete, runnable program that, for the following test suite, computes $\\operatorname{MSE}_{\\text{MC}}(n,d)$ and the empirical convergence slope in log-log scale:\n\n- Test suite parameters:\n  - Dimensions $d \\in \\{\\,1,\\,3,\\,6\\,\\}$.\n  - Sample sizes $n \\in \\{\\,200,\\,800,\\,3200\\,\\}$.\n  - Number of Monte Carlo evaluation points $Q = 1024$.\n- Randomness and reproducibility:\n  - For the sample $X_1,\\dots,X_n$ in a given $(n,d)$ case, use a pseudorandom number generator seeded with the integer\n    $$\n    s_{\\text{data}}(n,d) = 10^6 + 10^4 d + n.\n    $$\n  - For the evaluation points $Z_1,\\dots,Z_Q$ in a given $d$, use a pseudorandom number generator seeded with the integer\n    $$\n    s_{\\text{eval}}(d) = 2\\cdot 10^6 + 10^4 d.\n    $$\n  - All normal random variables must be standard normal with mean $0$ and variance $1$ in each coordinate, mutually independent.\n- For each fixed $d$, compute the least-squares slope $b_d$ of the regression of $\\log \\operatorname{MSE}_{\\text{MC}}(n,d)$ on $\\log n$ over the three $n$ values in the test suite. That is, for $n\\in\\{200,800,3200\\}$, fit\n  $$\n  \\log \\operatorname{MSE}_{\\text{MC}}(n,d) \\approx a_d + b_d \\log n\n  $$\n  in the ordinary least squares sense and return the estimated slope $b_d$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in this order:\n- The nine values $\\operatorname{MSE}_{\\text{MC}}(n,d)$ for $d=1,3,6$ (in ascending order) and, within each $d$, $n=200,800,3200$ (in ascending order).\n- Followed by the three slopes $b_d$ for $d=1,3,6$ (in ascending order).\n\nThus the output must have a total of twelve floating-point numbers in the order\n$$\n\\bigl[\\operatorname{MSE}_{\\text{MC}}(200,1),\\,\\operatorname{MSE}_{\\text{MC}}(800,1),\\,\\operatorname{MSE}_{\\text{MC}}(3200,1),\\,\\operatorname{MSE}_{\\text{MC}}(200,3),\\,\\operatorname{MSE}_{\\text{MC}}(800,3),\\,\\operatorname{MSE}_{\\text{MC}}(3200,3),\\,\\operatorname{MSE}_{\\text{MC}}(200,6),\\,\\operatorname{MSE}_{\\text{MC}}(800,6),\\,\\operatorname{MSE}_{\\text{MC}}(3200,6),\\,b_1,\\,b_3,\\,b_6\\bigr].\n$$\nNo other text should be printed. Angles and physical units are not involved; all outputs must be real numbers.",
            "solution": "**Methodology**\n\nThe task is to compute the Monte Carlo Mean Squared Error, $\\operatorname{MSE}_{\\text{MC}}(n,d)$, for several combinations of sample size $n$ and dimension $d$, and then to determine the empirical convergence rate. The procedure is as follows.\n\nFor each dimension $d \\in \\{1, 3, 6\\}$:\n1.  **Generate Evaluation Points:** We first generate $Q=1024$ evaluation points $Z_1, \\dots, Z_Q$ from the $d$-variate standard normal distribution, $f_d$. The pseudorandom number generator is seeded with $s_{\\text{eval}}(d) = 2 \\cdot 10^6 + 10^4 d$ to ensure reproducibility. These points are stored in a $Q \\times d$ matrix $Z$.\n\n2.  **Compute True Density:** The true density values $f_d(Z_j)$ for $j=1, \\dots, Q$ are computed using the formula $f_d(x) = (2\\pi)^{-d/2}\\exp(-\\frac{1}{2}\\lVert x\\rVert_2^2)$. This involves calculating the squared Euclidean norm $\\lVert Z_j\\rVert_2^2$ for each point.\n\n3.  **Iterate over Sample Sizes:** For each sample size $n \\in \\{200, 800, 3200\\}$:\n    a.  **Generate Data Samples:** $n$ data points $X_1, \\dots, X_n$ are drawn from $f_d$. The generator is seeded with $s_{\\text{data}}(n,d) = 10^6 + 10^4 d + n$. These points form an $n \\times d$ matrix $X$.\n    b.  **Determine Bandwidth:** The bandwidth $h$ is calculated according to the rule $h(n,d) = n^{-1/(d+4)}$.\n    c.  **Compute KDE:** The KDE, $\\widehat{f}_{n,d,h}(x)$, must be evaluated at each point $Z_j$. The definition is:\n    $$\n    \\widehat{f}_{n,d,h}(Z_j) = \\frac{1}{n h^d}\\sum_{i=1}^n K_d\\left(\\frac{Z_j - X_i}{h}\\right)\n    $$\n    Substituting the Gaussian kernel $K_d(u) = (2\\pi)^{-d/2}\\exp(-\\frac{1}{2}\\lVert u\\rVert_2^2)$ yields:\n    $$\n    \\widehat{f}_{n,d,h}(Z_j) = \\frac{(2\\pi)^{-d/2}}{n h^d}\\sum_{i=1}^n \\exp\\left(-\\frac{1}{2h^2}\\lVert Z_j - X_i\\rVert_2^2\\right)\n    $$\n    To compute this efficiently, we first form a $Q \\times n$ matrix of squared Euclidean distances, where the entry $(j,i)$ is $\\lVert Z_j - X_i\\rVert_2^2$. This is done using the `scipy.spatial.distance.cdist` function. The exponential term is then applied element-wise, the results are summed over the index $i$ for each $j$, and finally multiplied by the constant pre-factor $\\frac{(2\\pi)^{-d/2}}{n h^d}$.\n    d.  **Compute MSE:** The $\\operatorname{MSE}_{\\text{MC}}(n,d)$ is calculated by taking the mean of the squared differences between the estimated and true densities at the evaluation points:\n    $$\n    \\operatorname{MSE}_{\\text{MC}}(n,d) = \\frac{1}{Q}\\sum_{j=1}^{Q}\\left(\\widehat{f}_{n,d,h}(Z_j) - f_d(Z_j)\\right)^2\n    $$\n4.  **Estimate Convergence Slope:** After computing the three $\\operatorname{MSE}_{\\text{MC}}$ values for a fixed $d$, we estimate the slope $b_d$ of the relationship $\\log \\operatorname{MSE}_{\\text{MC}}(n,d) \\approx a_d + b_d \\log n$. This is a standard simple linear regression problem. We define the dependent variable as $y_k = \\log \\operatorname{MSE}_{\\text{MC}}(n_k, d)$ and the independent variable as $x_k = \\log n_k$ for $n_k \\in \\{200, 800, 3200\\}$. The slope $b_d$ is found by solving the least-squares problem, for which we use the `numpy.linalg.lstsq` function.\n\nThe theoretical asymptotic mean integrated squared error (MISE) for this setup converges at a rate of $\\mathcal{O}(n^{-4/(d+4)})$. Therefore, the logarithm of the MISE is a linear function of $\\log n$ with a slope of $-\\frac{4}{d+4}$. The computed empirical slope $b_d$ is expected to approximate this theoretical value. For $d=1, 3, 6$, the theoretical slopes are $-0.8$, $-4/7 \\approx -0.571$, and $-0.4$, respectively. The decrease in the magnitude of the slope as $d$ increases is a quantitative manifestation of the curse of dimensionality: the estimator's convergence rate slows down in higher dimensions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Computes the Mean Squared Error of a Kernel Density Estimator for various\n    dimensions and sample sizes, and estimates the convergence slope to illustrate\n    the curse of dimensionality.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    dimensions = [1, 3, 6]\n    sample_sizes = [200, 800, 3200]\n    Q = 1024\n\n    all_mse_values = []\n    all_slopes = []\n\n    # Helper function for multivariate normal PDF\n    def true_density_f_d(x, d):\n        if x.ndim == 1:\n            x = x.reshape(1, -1)\n        norm_sq = np.sum(x**2, axis=1)\n        return (2 * np.pi)**(-d / 2) * np.exp(-0.5 * norm_sq)\n\n    for d in dimensions:\n        # Generate evaluation points Z for the current dimension d.\n        # This is done once per dimension.\n        s_eval = 2 * 10**6 + 10**4 * d\n        rng_eval = np.random.default_rng(s_eval)\n        Z = rng_eval.normal(loc=0, scale=1, size=(Q, d))\n\n        # Compute the true density values f_d(Z_j) at the evaluation points.\n        f_true_vals = true_density_f_d(Z, d)\n\n        mse_for_current_d = []\n        log_n_values = np.log(sample_sizes)\n\n        for n in sample_sizes:\n            # Main logic to calculate the result for one case (n, d)\n            \n            # 1. Generate data samples X\n            s_data = 10**6 + 10**4 * d + n\n            rng_data = np.random.default_rng(s_data)\n            X = rng_data.normal(loc=0, scale=1, size=(n, d))\n            \n            # 2. Calculate bandwidth h\n            h = n**(-1 / (d + 4))\n\n            # 3. Calculate KDE estimates f_hat(Z_j)\n            # Use scipy.spatial.distance.cdist for efficient computation of squared\n            # Euclidean distances between each Z_j and X_i.\n            sq_dists = cdist(Z, X, 'sqeuclidean')  # Shape (Q, n)\n            \n            # The argument to the exponential function in the kernel sum\n            kernel_exp_arg = -0.5 / (h**2) * sq_dists\n            \n            # Sum of kernel values over index i\n            sum_of_exponentials = np.sum(np.exp(kernel_exp_arg), axis=1)\n            \n            # Prefactor for the KDE formula\n            prefactor = (2 * np.pi)**(-d / 2) / (n * h**d)\n            \n            # KDE estimates at points Z_j\n            f_hat_vals = prefactor * sum_of_exponentials\n            \n            # 4. Compute the Monte Carlo proxy for MSE\n            mse = np.mean((f_hat_vals - f_true_vals)**2)\n            mse_for_current_d.append(mse)\n\n        # Append the 3 MSE values for the current dimension to the main list\n        all_mse_values.extend(mse_for_current_d)\n        \n        # 5. Compute the least-squares slope for the current dimension d\n        log_mse_values = np.log(mse_for_current_d)\n        \n        # Set up the linear system A*beta = y for regression\n        # y = log_mse_values\n        # beta = [a_d, b_d] (intercept, slope)\n        # A = [[1, log_n_1], [1, log_n_2], [1, log_n_3]]\n        A = np.vstack([np.ones_like(log_n_values), log_n_values]).T\n        \n        # Solve for the coefficients using least squares\n        coeffs = np.linalg.lstsq(A, log_mse_values, rcond=None)[0]\n        slope_b_d = coeffs[1]\n        all_slopes.append(slope_b_d)\n\n    # Combine all results into a single list for printing\n    results = all_mse_values + all_slopes\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}