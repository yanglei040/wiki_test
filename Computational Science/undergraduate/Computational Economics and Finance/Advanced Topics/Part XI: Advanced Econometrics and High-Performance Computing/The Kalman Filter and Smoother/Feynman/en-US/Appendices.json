{
    "hands_on_practices": [
        {
            "introduction": "Financial data is often messy, and a common problem is dealing with missing observations, especially for illiquid assets like corporate bonds that don't trade every day. The Kalman filter provides a principled and powerful framework for imputing, or filling in, these missing values. This exercise  demonstrates how the filter's recursive prediction-update structure naturally handles this challenge: when data is absent, the filter simply relies on its prediction, elegantly carrying the state estimate forward until the next piece of information arrives.",
            "id": "2441481",
            "problem": "An illiquid corporate bond is traded infrequently, resulting in missing daily transaction prices. Consider the following linear Gaussian state-space model for the latent fundamental daily price level. For time index $t \\in \\{1,2,\\dots,T\\}$, the latent state $x_t$ (the fundamental price) evolves according to\n$$\nx_t = \\phi x_{t-1} + w_t,\n$$\nwhere $w_t \\sim \\mathcal{N}(0,q)$, and the observed transaction price $y_t$ satisfies\n$$\ny_t = x_t + v_t,\n$$\nwhere $v_t \\sim \\mathcal{N}(0,r)$, with $\\{w_t\\}$ and $\\{v_t\\}$ mutually independent over time and independent of the initial state. The initial state has prior\n$$\nx_0 \\sim \\mathcal{N}(m_0, P_0).\n$$\nWhen $y_t$ is unavailable on a given date, it is marked as the special symbol $\\text{NaN}$. For any date with a missing observation, define the real-time imputed price as the conditional expectation\n$$\n\\hat{y}_t := \\mathbb{E}[y_t \\mid \\{y_s : 1 \\le s \\le t \\text{ and } y_s \\text{ is observed}\\}],\n$$\nwhich uses information available up to and including time $t$ only. All imputed prices must be expressed in the same currency units as the input prices and reported rounded to $4$ decimal places.\n\nYou are given the following test suite. Each test case provides the parameters $(\\phi,q,r,m_0,P_0)$ and an observed price sequence $(y_1,\\dots,y_T)$ with missing entries indicated by $\\text{NaN}$. For each test case, output the list of imputed values $\\hat{y}_t$ at those times $t$ where $y_t$ is missing, in time order.\n\nTest Case $1$:\n- Parameters: $\\phi = 0.999$, $q = 0.04$, $r = 0.09$, $m_0 = 100.0$, $P_0 = 1.0$.\n- Observations: $[100.2,\\,100.5,\\,\\text{NaN},\\,100.7,\\,100.6,\\,\\text{NaN},\\,100.8]$.\n\nTest Case $2$:\n- Parameters: $\\phi = 1.0$, $q = 0.01$, $r = 0.04$, $m_0 = 50.0$, $P_0 = 4.0$.\n- Observations: $[49.8,\\,50.3,\\,50.0,\\,49.9,\\,50.1]$.\n\nTest Case $3$:\n- Parameters: $\\phi = 0.995$, $q = 0.10$, $r = 0.16$, $m_0 = 200.0$, $P_0 = 9.0$.\n- Observations: $[\\text{NaN},\\,\\text{NaN},\\,199.0,\\,199.5,\\,\\text{NaN},\\,200.2]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The $i$-th entry of this list must itself be a list containing the imputed values for the $i$-th test case, in time order, each rounded to $4$ decimal places. For a test case with no missing observations, output an empty list $[]$ for that case. For example, an output might look like\n$$\n[[\\hat{y}_{t_1},\\hat{y}_{t_2}],[],[\\hat{y}_{s_1},\\hat{y}_{s_2},\\hat{y}_{s_3}]],\n$$\nand must be printed with no spaces, with each number rounded to $4$ decimals, in the exact format\n\"[list\\_case1,list\\_case2,list\\_case3]\".",
            "solution": "The problem requires the computation of real-time imputed prices for a series of infrequently traded corporate bonds. The dynamics of the bond's fundamental price are described by a linear Gaussian state-space model, a standard framework for such problems in econometrics and finance. The task is to find the conditional expectation of the price at times when it is not observed, using only information available up to that time. This is a direct application of the Kalman filter.\n\nThe state-space model is defined as follows:\nState equation: $x_t = \\phi x_{t-1} + w_t$, with $w_t \\sim \\mathcal{N}(0, q)$\nObservation equation: $y_t = x_t + v_t$, with $v_t \\sim \\mathcal{N}(0, r)$\n\nHere, $x_t$ is the latent fundamental price at time $t$, and $y_t$ is the observed transaction price. The terms $w_t$ and $v_t$ are independent, zero-mean Gaussian noise processes. The initial state $x_0$ is drawn from a Gaussian distribution, $x_0 \\sim \\mathcal{N}(m_0, P_0)$.\n\nThe quantity to be computed is the real-time imputed price, $\\hat{y}_t$, for any time $t$ where the observation $y_t$ is missing. This is defined as:\n$$\n\\hat{y}_t := \\mathbb{E}[y_t \\mid \\{y_s : 1 \\le s \\le t \\text{ and } y_s \\text{ is observed}\\}]\n$$\nLet $\\mathcal{I}_t^{\\text{obs}}$ denote the set of observed prices up to and including time $t$. If $y_t$ is missing, this set is equivalent to the set of observed prices up to time $t-1$, which we can denote as $\\mathcal{I}_{t-1}^{\\text{obs}}$.\nUsing the observation equation, we can write:\n$$\n\\hat{y}_t = \\mathbb{E}[x_t + v_t \\mid \\mathcal{I}_{t-1}^{\\text{obs}}] = \\mathbb{E}[x_t \\mid \\mathcal{I}_{t-1}^{\\text{obs}}] + \\mathbb{E}[v_t \\mid \\mathcal{I}_{t-1}^{\\text{obs}}]\n$$\nThe observation noise $v_t$ is independent of all states and observations in the past. Therefore, $\\mathbb{E}[v_t \\mid \\mathcal{I}_{t-1}^{\\text{obs}}] = \\mathbb{E}[v_t] = 0$. This simplifies the imputed price to:\n$$\n\\hat{y}_t = \\mathbb{E}[x_t \\mid \\mathcal{I}_{t-1}^{\\text{obs}}]\n$$\nThis quantity is the one-step-ahead prediction of the state $x_t$, given the history of observations up to time $t-1$. The Kalman filter provides a recursive algorithm to compute this value.\n\nLet $m_{t|t-1}$ and $P_{t|t-1}$ be the mean and variance of the predicted state distribution $p(x_t | y_1, \\dots, y_{t-1}^{\\text{obs}})$, and let $m_{t|t}$ and $P_{t|t}$ be the mean and variance of the filtered state distribution $p(x_t | y_1, \\dots, y_t^{\\text{obs}})$. The algorithm proceeds as follows:\n\n**Initialization:**\nSet the initial filtered estimate at time $t=0$ to the prior distribution:\n$m_{0|0} = m_0$\n$P_{0|0} = P_0$\n\n**Iteration:**\nFor each time step $t=1, 2, \\dots, T$:\n\n**1. Prediction Step:**\nThe predicted mean and variance for the state at time $t$ are derived from the state equation and the filtered estimates at time $t-1$:\n$$\nm_{t|t-1} = \\phi \\cdot m_{t-1|t-1}\n$$\n$$\nP_{t|t-1} = \\phi^2 \\cdot P_{t-1|t-1} + q\n$$\n\n**2. Update Step:**\nThis step depends on whether the observation $y_t$ is available.\n\n**Case A: $y_t$ is observed.**\nWe update our estimate of the state by incorporating the new information from $y_t$.\nThe innovation, or prediction error, is $\\nu_t = y_t - m_{t|t-1}$.\nThe variance of the innovation is $S_t = P_{t|t-1} + r$.\nThe Kalman gain, which determines the weight given to the innovation, is $K_t = P_{t|t-1} / S_t$.\nThe updated (filtered) state mean and variance are:\n$$\nm_{t|t} = m_{t|t-1} + K_t \\cdot \\nu_t\n$$\n$$\nP_{t|t} = (1 - K_t) \\cdot P_{t|t-1}\n$$\n\n**Case B: $y_t$ is missing (NaN).**\nThe required imputed price is the predicted state mean:\n$$\n\\hat{y}_t = m_{t|t-1}\n$$\nThis value is recorded. Since no new information is available at time $t$, the filtered distribution is simply the predicted distribution. The update step becomes a trivial identity transformation:\n$$\nm_{t|t} = m_{t|t-1}\n$$\n$$\nP_{t|t} = P_{t|t-1}\n$$\nThis ensures that the uncertainty (variance) does not decrease and correctly propagates forward until the next available observation.\n\nBy iterating these steps from $t=1$ to $T$, we can process the entire observation sequence and collect the imputed values $\\hat{y}_t$ for all time steps where $y_t$ is missing.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for real-time imputed prices in a linear Gaussian state-space model\n    using the Kalman filter for several test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"params\": {\"phi\": 0.999, \"q\": 0.04, \"r\": 0.09, \"m0\": 100.0, \"P0\": 1.0},\n            \"obs\": [100.2, 100.5, np.nan, 100.7, 100.6, np.nan, 100.8],\n        },\n        {\n            \"params\": {\"phi\": 1.0, \"q\": 0.01, \"r\": 0.04, \"m0\": 50.0, \"P0\": 4.0},\n            \"obs\": [49.8, 50.3, 50.0, 49.9, 50.1],\n        },\n        {\n            \"params\": {\"phi\": 0.995, \"q\": 0.10, \"r\": 0.16, \"m0\": 200.0, \"P0\": 9.0},\n            \"obs\": [np.nan, np.nan, 199.0, 199.5, np.nan, 200.2],\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # Extract parameters and observations for the current case\n        phi = case[\"params\"][\"phi\"]\n        q = case[\"params\"][\"q\"]\n        r = case[\"params\"][\"r\"]\n        obs = case[\"obs\"]\n        \n        # Initialize filtered state mean and variance from the prior\n        m_filt = case[\"params\"][\"m0\"]\n        P_filt = case[\"params\"][\"P0\"]\n        \n        imputed_values = []\n        \n        # Iterate through time\n        for y_t in obs:\n            # Prediction step\n            m_pred = phi * m_filt\n            P_pred = phi**2 * P_filt + q\n            \n            # Update step\n            if not np.isnan(y_t):\n                # Observation is available\n                nu_t = y_t - m_pred       # Innovation\n                S_t = P_pred + r          # Innovation variance\n                K_t = P_pred / S_t        # Kalman gain\n                m_filt = m_pred + K_t * nu_t\n                P_filt = (1.0 - K_t) * P_pred\n            else:\n                # Observation is missing (NaN)\n                # The imputed price is the one-step-ahead prediction of the state.\n                imputed_values.append(m_pred)\n                # The filtered estimate is just the prediction\n                m_filt = m_pred\n                P_filt = P_pred\n        \n        all_results.append(imputed_values)\n        \n    # Format the output string precisely as specified\n    # The output format is a list of lists, with no spaces, and numbers rounded to 4 decimals.\n    case_results_str = []\n    for res in all_results:\n        # Format numbers to 4 decimal places\n        num_strs = [f\"{v:.4f}\" for v in res]\n        # Join numbers with commas and enclose in brackets\n        case_results_str.append(f\"[{','.join(num_strs)}]\")\n            \n    # Join the string representations of each case's results and enclose in brackets\n    final_output = f\"[{','.join(case_results_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "A central task in economics and finance is to distinguish between permanent trends and temporary fluctuations within a time series. For instance, is a recent drop in a stock's price a sign of a fundamental revaluation or just short-term market noise? This exercise  guides you through building a state-space model to decompose a series into a permanent (random walk) component and a transitory (autocorrelated) component. By implementing the Kalman filter and Rauch-Tung-Striebel (RTS) smoother, you will learn to estimate these unobservable latent variables and gain deeper insight into the underlying structure of the data.",
            "id": "2441543",
            "problem": "You are given a linear-Gaussian state-space decomposition of a stock’s return into a permanent and a transitory component. Let the observed return be $y_t$, the permanent component be $\\alpha_t$, and the transitory component be $v_t$. Assume\n- $y_t = \\alpha_t + v_t$,\n- $\\alpha_t = \\alpha_{t-1} + \\eta_t$ with $\\eta_t \\sim \\mathcal{N}(0,\\sigma_{\\eta}^2)$,\n- $v_t = \\phi\\, v_{t-1} + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^2)$,\n- $\\eta_t$ and $\\varepsilon_t$ are mutually independent for all $t$ and independent of initial states.\n\nThis induces a state-space model with state $x_t = \\begin{bmatrix}\\alpha_t \\\\ v_t\\end{bmatrix}$, state transition matrix $F = \\begin{bmatrix}1 & 0 \\\\ 0 & \\phi\\end{bmatrix}$, measurement matrix $H = \\begin{bmatrix}1 & 1\\end{bmatrix}$, process noise covariance $Q = \\mathrm{diag}(\\sigma_{\\eta}^2,\\sigma_{\\varepsilon}^2)$, and measurement noise variance $R = 0$. Let the initial state be $x_0=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$ and the initial covariance be $P_0=\\mathrm{diag}(100,100)$.\n\nYour tasks are:\n- From first principles of the linear-Gaussian state-space model, implement the recursive estimator that yields the minimum mean squared error state estimate given observations up to time $t$ for each $t$, and then implement the fixed-interval backward pass that yields the minimum mean squared error smoothed state estimates given all observations.\n- Use the model to generate synthetic data by simulating $\\alpha_t$, $v_t$, and $y_t$ for each test case below, using independent Gaussian innovations and the specified random seed to ensure reproducibility.\n- For each test case, compute the root mean squared error (RMSE) between the smoothed estimates $\\widehat{\\alpha}_t$ and the true $\\alpha_t$ over the entire sample, and likewise the RMSE between the smoothed estimates $\\widehat{v}_t$ and the true $v_t$.\n\nSimulation details:\n- For each test case, set $x_0=\\begin{bmatrix}0\\\\0\\end{bmatrix}$ deterministically and draw $\\eta_t \\sim \\mathcal{N}(0,\\sigma_{\\eta}^2)$ and $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^2)$ independently across $t$.\n- Generate the series $\\{\\alpha_t\\}_{t=1}^T$ and $\\{v_t\\}_{t=1}^T$ recursively from their laws of motion, then set $y_t=\\alpha_t+v_t$ for $t=1,\\dots,T$.\n- Use the same $F$, $H$, $Q$, $R$, $x_0$, and $P_0$ as stated to run your estimator forward in time and then smooth backward over the full sample.\n\nTest suite:\n- Case $1$: $T=120$, $\\phi=0.5$, $\\sigma_{\\eta}=0.05$, $\\sigma_{\\varepsilon}=0.2$, seed $=7$.\n- Case $2$: $T=300$, $\\phi=0.95$, $\\sigma_{\\eta}=0.15$, $\\sigma_{\\varepsilon}=0.1$, seed $=11$.\n- Case $3$: $T=80$, $\\phi=-0.6$, $\\sigma_{\\eta}=0.1$, $\\sigma_{\\varepsilon}=0.25$, seed $=42$.\n\nOutput specification:\n- For each case, report two floating-point numbers: $\\mathrm{RMSE}_{\\alpha}=\\sqrt{\\frac{1}{T}\\sum_{t=1}^T(\\widehat{\\alpha}_t-\\alpha_t)^2}$ and $\\mathrm{RMSE}_{v}=\\sqrt{\\frac{1}{T}\\sum_{t=1}^T(\\widehat{v}_t-v_t)^2}$.\n- Round each reported value to exactly $6$ decimal places.\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets, in the order $[\\mathrm{RMSE}_{\\alpha}^{(1)},\\mathrm{RMSE}_{v}^{(1)},\\mathrm{RMSE}_{\\alpha}^{(2)},\\mathrm{RMSE}_{v}^{(2)},\\mathrm{RMSE}_{\\alpha}^{(3)},\\mathrm{RMSE}_{v}^{(3)}]$, where the superscript indicates the case index.\n- No physical units are involved.\n\nCoverage rationale for the test suite:\n- Case $1$ is a well-conditioned baseline (moderate persistence and higher transitory noise).\n- Case $2$ stresses near-nonstationary transitory dynamics with a highly persistent $\\phi$ close to $1$.\n- Case $3$ tests a negatively autocorrelated transitory component with larger transitory noise.\n\nYour implementation must be a complete, runnable program that carries out the simulation, estimation, and evaluation for the specified cases, and prints the results in exactly the required format.",
            "solution": "We begin from the linear-Gaussian state-space model, which states that if the latent state evolves linearly with Gaussian process noise and is observed linearly with Gaussian measurement noise, then the conditional distribution of the state given observations up to time $t$ is Gaussian. The minimum mean squared error estimator is the conditional expectation of the state given the data. Under linearity and Gaussianity, this estimator is computed recursively by the Kalman filter. The fixed-interval minimum mean squared error estimator conditional on all observations is obtained by a backward recursion known as the Rauch–Tung–Striebel (RTS) smoother.\n\nModel specification. The state is $x_t=\\begin{bmatrix}\\alpha_t\\\\v_t\\end{bmatrix}$ with state transition\n$$\nx_t = F x_{t-1} + w_t,\\quad F=\\begin{bmatrix}1 & 0\\\\ 0 & \\phi\\end{bmatrix},\\quad w_t\\sim \\mathcal{N}(0,Q),\\quad Q=\\mathrm{diag}(\\sigma_{\\eta}^2,\\sigma_{\\varepsilon}^2).\n$$\nThe measurement equation is\n$$\ny_t = H x_t + u_t,\\quad H=\\begin{bmatrix}1 & 1\\end{bmatrix},\\quad u_t\\sim \\mathcal{N}(0,R),\\quad R=0.\n$$\nThe initial condition is $x_0\\sim \\mathcal{N}(m_0,P_0)$ with $m_0=\\begin{bmatrix}0\\\\0\\end{bmatrix}$ and $P_0=\\mathrm{diag}(100,100)$, independent of $\\{w_t,u_t\\}$.\n\nForward recursion (Kalman filter). Given the filtered estimate and covariance at time $t-1$, denoted $m_{t-1|t-1}$ and $P_{t-1|t-1}$, the one-step-ahead prediction is\n$$\nm_{t|t-1} = F m_{t-1|t-1},\\quad P_{t|t-1} = F P_{t-1|t-1} F^\\top + Q.\n$$\nGiven observation $y_t$, compute the innovation\n$$\n\\tilde{y}_t = y_t - H m_{t|t-1}, \\quad S_t = H P_{t|t-1} H^\\top + R,\n$$\nand the Kalman gain\n$$\nK_t = P_{t|t-1} H^\\top S_t^{-1}.\n$$\nThen the filtered estimate and covariance are updated as\n$$\nm_{t|t} = m_{t|t-1} + K_t \\tilde{y}_t,\\quad P_{t|t} = (I - K_t H) P_{t|t-1}.\n$$\nIn our model, $R=0$ and $S_t$ is a scalar because $H$ is $1\\times 2$, so $S_t^{-1}$ is simply the reciprocal of the variance $S_t$.\n\nBackward recursion (Rauch–Tung–Striebel smoother). The smoothed estimates $m_{t|T}=\\mathbb{E}[x_t\\mid y_{1:T}]$ are obtained starting from $m_{T|T}$ and $P_{T|T}$ and moving backward for $t=T-1,\\dots,1$ using\n$$\nJ_t = P_{t|t} F^\\top (P_{t+1|t})^{-1},\\quad m_{t|T} = m_{t|t} + J_t (m_{t+1|T} - m_{t+1|t}),\n$$\n$$\nP_{t|T} = P_{t|t} + J_t (P_{t+1|T} - P_{t+1|t}) J_t^\\top,\n$$\nwhere $m_{t+1|t}=F m_{t|t}$ and $P_{t+1|t}=F P_{t|t} F^\\top + Q$ by the state dynamics.\n\nSimulation. For each test case, with given $T$, $\\phi$, $\\sigma_{\\eta}$, $\\sigma_{\\varepsilon}$, and seed, we simulate the latent processes and observations:\n- Initialize $\\alpha_0=0$ and $v_0=0$.\n- For $t=1,\\dots,T$, draw $\\eta_t\\sim \\mathcal{N}(0,\\sigma_{\\eta}^2)$ and $\\varepsilon_t\\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^2)$ independently, set $\\alpha_t=\\alpha_{t-1}+\\eta_t$ and $v_t=\\phi v_{t-1}+\\varepsilon_t$, and define $y_t=\\alpha_t+v_t$.\n\nEstimation and evaluation. With the simulated $\\{y_t\\}$ we run the Kalman filter using the specified $F$, $H$, $Q$, $R$, $m_0$, and $P_0$, store the filtered means and covariances, then run the RTS smoother to obtain $m_{t|T}$. The components of $m_{t|T}$ are the smoothed estimates $\\widehat{\\alpha}_t$ and $\\widehat{v}_t$. We then compute\n$$\n\\mathrm{RMSE}_{\\alpha} = \\sqrt{\\frac{1}{T}\\sum_{t=1}^T (\\widehat{\\alpha}_t - \\alpha_t)^2},\\quad \\mathrm{RMSE}_{v} = \\sqrt{\\frac{1}{T}\\sum_{t=1}^T (\\widehat{v}_t - v_t)^2}.\n$$\n\nAlgorithmic considerations.\n- The innovation variance $S_t$ is positive because $P_{t|t-1}$ is positive semidefinite and $H$ has full row rank; thus $S_t^{-1}$ exists even with $R=0$.\n- The matrix inverses needed for $S_t^{-1}$ and $(P_{t+1|t})^{-1}$ can be computed via linear solves for numerical stability.\n- The initial covariance $P_0$ is set large to reflect diffuse prior uncertainty about $x_0$.\n\nTest suite coverage justification.\n- Case $1$ (moderate $\\phi$ and larger $\\sigma_{\\varepsilon}$ relative to $\\sigma_{\\eta}$) assesses typical conditions.\n- Case $2$ ($\\phi$ close to $1$) yields highly persistent transitory dynamics, challenging the smoother to disentangle components.\n- Case $3$ negative $\\phi$ stresses the algorithm with sign-flipping serial correlation and a higher transitory noise.\n\nThe program implements the simulation, filtering, and smoothing for all cases, computes the requested RMSEs, rounds each to $6$ decimal places, and prints a single list in the specified order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_returns(T, phi, sigma_eta, sigma_eps, seed):\n    rng = np.random.default_rng(seed)\n    eta = rng.normal(0.0, sigma_eta, size=T)\n    eps = rng.normal(0.0, sigma_eps, size=T)\n    alpha = np.zeros(T)\n    v = np.zeros(T)\n    # Initial states are zero deterministically\n    a_prev = 0.0\n    v_prev = 0.0\n    for t in range(T):\n        a_prev = a_prev + eta[t]\n        v_prev = phi * v_prev + eps[t]\n        alpha[t] = a_prev\n        v[t] = v_prev\n    y = alpha + v\n    return y, alpha, v\n\ndef kalman_filter_rts(y, F, H, Q, R, x0, P0):\n    \"\"\"\n    y: shape (T,)\n    F: shape (n,n)\n    H: shape (1,n)\n    Q: shape (n,n)\n    R: scalar (float)\n    x0: shape (n,)\n    P0: shape (n,n)\n    Returns smoothed means of shape (T,n)\n    \"\"\"\n    T = y.shape[0]\n    n = x0.shape[0]\n    # Preallocate arrays\n    m_pred = np.zeros((T, n))\n    P_pred = np.zeros((T, n, n))\n    m_filt = np.zeros((T, n))\n    P_filt = np.zeros((T, n, n))\n    I = np.eye(n)\n\n    m_prev = x0.copy()\n    P_prev = P0.copy()\n    H_mat = H.reshape(1, n)  # ensure shape\n    HT = H_mat.T\n\n    for t in range(T):\n        # Predict\n        m_prior = F @ m_prev\n        P_prior = F @ P_prev @ F.T + Q\n\n        # Innovation\n        yhat = (H_mat @ m_prior.reshape(n, 1)).item()\n        innov = y[t] - yhat\n        S = (H_mat @ P_prior @ HT).item() + R  # scalar\n\n        # Kalman gain\n        K = (P_prior @ HT) / S  # shape (n,1)\n\n        # Update\n        m_post = m_prior + (K.flatten() * innov)\n        P_post = (I - K @ H_mat) @ P_prior\n\n        # Store\n        m_pred[t] = m_prior\n        P_pred[t] = P_prior\n        m_filt[t] = m_post\n        P_filt[t] = P_post\n\n        # Prepare for next\n        m_prev = m_post\n        P_prev = P_post\n\n    # RTS smoother\n    m_smooth = np.zeros_like(m_filt)\n    P_smooth = np.zeros_like(P_filt)\n    m_smooth[-1] = m_filt[-1]\n    P_smooth[-1] = P_filt[-1]\n\n    for t in range(T - 2, -1, -1):\n        # Compute predicted next from filtered t\n        P_pred_next = F @ P_filt[t] @ F.T + Q\n        # Smoother gain\n        # Solve P_pred_next * X = F @ P_filt[t]  => X^T = P_filt[t] @ F.T @ inv(P_pred_next)\n        # We compute J = P_filt[t] @ F.T @ inv(P_pred_next)\n        # Use solve for inv(P_pred_next)\n        J = P_filt[t] @ F.T @ np.linalg.inv(P_pred_next)\n        m_pred_next = F @ m_filt[t]\n        m_smooth[t] = m_filt[t] + J @ (m_smooth[t + 1] - m_pred_next)\n        P_smooth[t] = P_filt[t] + J @ (P_smooth[t + 1] - P_pred_next) @ J.T\n\n    return m_smooth, P_smooth\n\ndef rmse(a, b):\n    return float(np.sqrt(np.mean((np.asarray(a) - np.asarray(b)) ** 2)))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (T, phi, sigma_eta, sigma_eps, seed)\n    test_cases = [\n        (120, 0.5, 0.05, 0.2, 7),\n        (300, 0.95, 0.15, 0.1, 11),\n        (80, -0.6, 0.1, 0.25, 42),\n    ]\n\n    results = []\n    for T, phi, sigma_eta, sigma_eps, seed in test_cases:\n        # Simulate data\n        y, alpha_true, v_true = simulate_returns(T, phi, sigma_eta, sigma_eps, seed)\n        # Build model matrices\n        F = np.array([[1.0, 0.0],\n                      [0.0, float(phi)]])\n        H = np.array([1.0, 1.0])  # shape (2,)\n        Q = np.diag([sigma_eta ** 2, sigma_eps ** 2])\n        R = 0.0\n        x0 = np.array([0.0, 0.0])\n        P0 = np.diag([100.0, 100.0])\n\n        # Kalman filter and RTS smoother\n        m_smooth, _ = kalman_filter_rts(y, F, H, Q, R, x0, P0)\n\n        alpha_hat = m_smooth[:, 0]\n        v_hat = m_smooth[:, 1]\n\n        rmse_alpha = rmse(alpha_hat, alpha_true)\n        rmse_v = rmse(v_hat, v_true)\n\n        # Round to 6 decimals as strings\n        results.append(f\"{rmse_alpha:.6f}\")\n        results.append(f\"{rmse_v:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Economic and financial systems are rarely static; their underlying dynamics can shift due to policy changes, technological innovations, or financial crises. This exercise  tackles the challenge of a 'structural break,' where a key parameter of our model changes at a specific point in time. You will learn to adapt the Kalman filter to a time-varying system and explore how knowledge of the break impacts both real-time predictions and retrospective analysis using the smoother. This practice is crucial for modeling non-stationary environments and understanding how the filter processes information about changes in the data-generating process.",
            "id": "2441535",
            "problem": "Consider a linear Gaussian state-space model used to describe a latent market factor with a possible structural break in its persistence. The latent state is one-dimensional and evolves according to a time-varying first-order autoregression. Let $t \\in \\{0,1,\\dots,T-1\\}$ index observation times. The model is\n$$\nx_{t+1} = F_t\\, x_t + w_t,\\quad w_t \\sim \\mathcal{N}(0,q),\n$$\n$$\ny_t = H\\, x_t + v_t,\\quad v_t \\sim \\mathcal{N}(0,r),\n$$\nwith $H=1$. The initial state is distributed as $x_0 \\sim \\mathcal{N}(m_0,P_0)$. A structural break occurs at index $t_k$ by changing the state transition matrix $F_t$ from a pre-break value $a_1$ to a post-break value $a_2$:\n$$\nF_t = \\begin{cases}\na_1,& \\text{for } t < t_k,\\\\\na_2,& \\text{for } t \\ge t_k.\n\\end{cases}\n$$\nAll parameters $(T,a_1,a_2,q,r,m_0,P_0,t_k)$ are given and satisfy standard regularity (finite, real, with $q>0$, $r>0$, $P_0>0$). Angles are not involved. There are no physical units.\n\nYour tasks are:\n- Using the above definitions, simulate a latent path $\\{x_t\\}_{t=0}^{T}$ and observations $\\{y_t\\}_{t=0}^{T-1}$ with the specified $(F_t)$ schedule. Use independent Gaussian draws for $w_t$ and $v_t$, and use the same simulated data to evaluate all algorithms in each test case. For reproducibility across cases, use a deterministic seed $12345+i$ for case index $i$ starting at $0$.\n- Implement the optimal linear recursive estimator (Kalman filter) for the time-varying model with known $(F_t)$, and the Rauch–Tung–Striebel smoother for fixed-interval smoothing.\n- Define two quantitative response metrics at the structural break:\n  1. The immediate structural response $D$ at the break is the magnitude of the change in the one-step-ahead predicted mean due solely to switching from $a_1$ to $a_2$ at transition index $t_k \\to t_k+1$. If $m_{t_k|t_k}$ denotes the filtered mean at time $t_k$ under the break-aware model, then\n  $$\n  D \\equiv \\left| \\, a_2\\, m_{t_k|t_k} - a_1\\, m_{t_k|t_k} \\, \\right| = \\left| (a_2 - a_1)\\, m_{t_k|t_k} \\right|.\n  $$\n  2. The smoothed break adjustment $A$ is the difference in the smoothed state mean at the break time $t_k$ between the break-aware model and a counterfactual model that ignores the break (i.e., sets $F_t \\equiv a_1$ for all $t$) while keeping all other elements of the data-generating process fixed:\n  $$\n  A \\equiv m^{\\text{break}}_{t_k|T-1} - m^{\\text{no-break}}_{t_k|T-1},\n  $$\n  where $m^{\\text{break}}_{t_k|T-1}$ is the smoothed mean at time $t_k$ using the known structural break $(F_t)$, and $m^{\\text{no-break}}_{t_k|T-1}$ is the smoothed mean at time $t_k$ under the no-break assumption $(F_t \\equiv a_1)$, both computed on the same simulated observations $\\{y_t\\}_{t=0}^{T-1}$.\n- Edge case handling: If $t_k = T-1$, still compute $D$ using $m_{t_k|t_k}$ as above. The smoothed break adjustment $A$ remains well-defined at time $t_k$.\n\nImplement a program that performs the above for each parameter set in the following test suite:\n- Case $1$: $T=60$, $a_1=0.9$, $a_2=0.5$, $q=0.04$, $r=0.09$, $m_0=0.0$, $P_0=1.0$, $t_k=30$.\n- Case $2$: $T=50$, $a_1=0.95$, $a_2=0.2$, $q=0.01$, $r=0.04$, $m_0=0.0$, $P_0=1.0$, $t_k=1$.\n- Case $3$: $T=60$, $a_1=0.9$, $a_2=0.9$, $q=0.04$, $r=0.04$, $m_0=0.0$, $P_0=1.0$, $t_k=30$.\n- Case $4$: $T=60$, $a_1=0.7$, $a_2=-0.7$, $q=0.04$, $r=0.01$, $m_0=0.0$, $P_0=1.0$, $t_k=59$.\n\nFinal output format:\n- For each case, compute the pair $(D,A)$ as defined. Your program should produce a single line of output containing all results as a comma-separated list of floats rounded to six decimal places, ordered as $[D_1,A_1,D_2,A_2,D_3,A_3,D_4,A_4]$ and enclosed in square brackets (e.g., $[0.123456,-0.000321,\\dots]$).",
            "solution": "The problem requires the analysis of a one-dimensional linear Gaussian state-space model subject to a structural break in its autoregressive parameter. This is a standard problem in econometrics and time series analysis. The solution involves simulation of the process, estimation via the Kalman filter, and retrospective analysis using the Rauch-Tung-Striebel (RTS) smoother. I will first present the theoretical foundations and then describe the computational procedure for deriving the required metrics.\n\nThe model is defined by a state equation and an observation equation for time $t \\in \\{0, 1, \\dots, T-1\\}$:\n$$\nx_{t+1} = F_t x_t + w_t, \\quad w_t \\sim \\mathcal{N}(0, q)\n$$\n$$\ny_t = H x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0, r)\n$$\nThe state $x_t$ is a latent scalar variable, and $y_t$ is its scalar observation. The initial state is $x_0 \\sim \\mathcal{N}(m_0, P_0)$. The observation matrix is given as $H=1$. The key feature of this model is the time-varying state transition parameter, $F_t$, which exhibits a structural break at a known time $t_k$:\n$$\nF_t = \\begin{cases}\na_1, & \\text{for } t < t_k \\\\\na_2, & \\text{for } t \\ge t_k\n\\end{cases}\n$$\nThis implies that the persistence of the latent factor changes at time $t_k$. The task is to quantify the impact of this break.\n\nFirst, a single realization of the state path $\\{x_t\\}_{t=0}^{T}$ and observation path $\\{y_t\\}_{t=0}^{T-1}$ must be simulated for each set of parameters. This is achieved by initializing a pseudo-random number generator with a deterministic seed for reproducibility. The initial state $x_0$ is drawn from $\\mathcal{N}(m_0, P_0)$. Subsequently, for each time step $t$ from $0$ to $T-1$, the observation $y_t$ is generated from $x_t$, and the next state $x_{t+1}$ is generated from $x_t$ using the appropriate transition parameter $F_t$.\n\nSecond, we must estimate the latent state path using the generated observations $\\{y_t\\}$. The optimal linear recursive estimator for this task is the Kalman filter, which produces estimates of the state mean and variance given information up to the current time, denoted $m_{t|t} = \\mathbb{E}[x_t | y_0, \\dots, y_t]$ and $P_{t|t} = \\text{Var}(x_t | y_0, \\dots, y_t)$, respectively. The filter proceeds in two steps for each time point $t$:\n\n1.  **Prediction:** The state at time $t$ is predicted based on information up to $t-1$.\n    $$\n    m_{t|t-1} = F_{t-1} m_{t-1|t-1}\n    $$\n    $$\n    P_{t|t-1} = F_{t-1} P_{t-1|t-1} F_{t-1}^T + q\n    $$\n    The process is initialized with $m_{0|-1} = m_0$ and $P_{0|-1} = P_0$.\n\n2.  **Update:** The prediction is updated using the new observation $y_t$.\n    $$\n    \\tilde{y}_t = y_t - H m_{t|t-1} \\quad (\\text{Innovation})\n    $$\n    $$\n    S_t = H P_{t|t-1} H^T + r \\quad (\\text{Innovation Covariance})\n    $$\n    $$\n    K_t = P_{t|t-1} H^T S_t^{-1} \\quad (\\text{Kalman Gain})\n    $$\n    $$\n    m_{t|t} = m_{t|t-1} + K_t \\tilde{y}_t \\quad (\\text{Filtered Mean})\n    $$\n    $$\n    P_{t|t} = (I - K_t H) P_{t|t-1} \\quad (\\text{Filtered Covariance})\n    $$\n\nThird, to obtain the best possible estimate of the state at each time $t$ using the entire observation sequence, we employ the Rauch-Tung-Striebel (RTS) smoother. The RTS smoother is a backward pass that starts from the final filtered estimate $m_{T-1|T-1}$ and $P_{T-1|T-1}$ and recursively computes the smoothed estimates $m_{t|T-1} = \\mathbb{E}[x_t | y_0, \\dots, y_{T-1}]$ and $P_{t|T-1} = \\text{Var}(x_t | y_0, \\dots, y_{T-1})$. The smoother equations are:\n$$\nJ_t = P_{t|t} F_t^T P_{t+1|t}^{-1} \\quad (\\text{Smoother Gain})\n$$\n$$\nm_{t|T-1} = m_{t|t} + J_t (m_{t+1|T-1} - m_{t+1|t})\n$$\n$$\nP_{t|T-1} = P_{t|t} + J_t (P_{t+1|T-1} - P_{t+1|t}) J_t^T\n$$\nThe recursion runs for $t = T-2, T-3, \\dots, 0$.\n\nTo analyze the structural break, two models are estimated on the same simulated data $\\{y_t\\}$:\n1.  **Break-Aware Model:** Uses the true, time-varying transition matrix $F_t$ as defined by the problem.\n2.  **No-Break Model:** A misspecified model that incorrectly assumes $F_t = a_1$ for all time steps $t$.\n\nFinally, the two specified metrics are calculated:\n1.  The **immediate structural response**, $D$, is defined as $D \\equiv |(a_2 - a_1) m_{t_k|t_k}|$. This metric quantifies the magnitude of the instantaneous change in the one-step-ahead prediction $m_{t_k+1|t_k}$ caused by the switch from $a_1$ to $a_2$. The prediction would have been $a_1 m_{t_k|t_k}$ under the old regime and becomes $a_2 m_{t_k|t_k}$ under the new regime. The value $m_{t_k|t_k}$ is the filtered mean from the break-aware model.\n\n2.  The **smoothed break adjustment**, $A$, is defined as $A \\equiv m^{\\text{break}}_{t_k|T-1} - m^{\\text{no-break}}_{t_k|T-1}$. This metric compares the full-information (smoothed) estimate of the state at the break time $t_k$ from the correctly specified break-aware model with that from the misspecified no-break model. It measures how knowledge of the break and subsequent data alters the retrospective assessment of the state at the moment the break occurred.\n\nThe computational procedure involves implementing these steps for each test case, carefully managing the model parameters and time indices as specified.",
            "answer": "```python\nimport numpy as np\n\ndef simulate_data(T, a1, a2, q, r, m0, P0, tk, seed):\n    \"\"\"\n    Simulates a latent path and observations for the state-space model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    \n    # Simulate initial state\n    x[0] = rng.normal(m0, np.sqrt(P0))\n    \n    # State-space model parameters\n    H = 1.0 # scalar\n    \n    # Simulation loop\n    for t in range(T):\n        # Generate observation\n        y[t] = H * x[t] + rng.normal(0, np.sqrt(r))\n        \n        # Determine transition matrix\n        Ft = a1 if t < tk else a2\n        \n        # Generate next state\n        x[t+1] = Ft * x[t] + rng.normal(0, np.sqrt(q))\n        \n    return x, y\n\ndef kalman_filter(y, F, q, H, r, m0, P0):\n    \"\"\"\n    Implements the Kalman filter for a 1D linear Gaussian state-space model.\n    F can be a time-varying array.\n    \"\"\"\n    T = len(y)\n    \n    # Arrays to store filtered and predicted estimates\n    m_filt = np.zeros(T)\n    P_filt = np.zeros(T)\n    m_pred = np.zeros(T + 1)\n    P_pred = np.zeros(T + 1)\n    \n    # Initialization\n    m_pred[0] = m0\n    P_pred[0] = P0\n    \n    for t in range(T):\n        # --- Update step using y[t] ---\n        # Use prediction from previous step (or initialization)\n        m_t_pred = m_pred[t]\n        P_t_pred = P_pred[t]\n        \n        # Innovation\n        y_tilde = y[t] - H * m_t_pred\n        S = H * P_t_pred * H + r\n        \n        # Kalman Gain\n        K = P_t_pred * H / S\n        \n        # Filtered estimates\n        m_filt[t] = m_t_pred + K * y_tilde\n        P_filt[t] = (1 - K * H) * P_t_pred\n        \n        # --- Prediction step for t+1 ---\n        # This occurs T times, from t=0 to t=T-1\n        Ft = F[t]\n        m_pred[t+1] = Ft * m_filt[t]\n        P_pred[t+1] = Ft * P_filt[t] * Ft + q\n        \n    return m_filt, P_filt, m_pred, P_pred\n\ndef rts_smoother(m_filt, P_filt, m_pred, P_pred, F):\n    \"\"\"\n    Implements the Rauch-Tung-Striebel smoother.\n    \"\"\"\n    T = len(m_filt)\n    \n    # Arrays to store smoothed estimates\n    m_smooth = np.zeros(T)\n    P_smooth = np.zeros(T)\n    \n    # Initialization at the last time step\n    m_smooth[T-1] = m_filt[T-1]\n    P_smooth[T-1] = P_filt[T-1]\n    \n    # Backward pass\n    for t in range(T - 2, -1, -1):\n        Ft = F[t]\n        # Smoother gain\n        J = P_filt[t] * Ft / P_pred[t+1]\n        \n        # Smoothed estimates\n        m_smooth[t] = m_filt[t] + J * (m_smooth[t+1] - m_pred[t+1])\n        P_smooth[t] = P_filt[t] + J * (P_smooth[t+1] - P_pred[t+1]) * J\n        \n    return m_smooth, P_smooth\n\ndef solve():\n    \"\"\"\n    Main solver function to run test cases and compute metrics.\n    \"\"\"\n    test_cases = [\n        # (T, a1, a2, q, r, m0, P0, tk)\n        (60, 0.9, 0.5, 0.04, 0.09, 0.0, 1.0, 30),\n        (50, 0.95, 0.2, 0.01, 0.04, 0.0, 1.0, 1),\n        (60, 0.9, 0.9, 0.04, 0.04, 0.0, 1.0, 30),\n        (60, 0.7, -0.7, 0.04, 0.01, 0.0, 1.0, 59),\n    ]\n\n    results = []\n    \n    for i, case in enumerate(test_cases):\n        T, a1, a2, q, r, m0, P0, tk = case\n        \n        # 1. Simulate data using a deterministic seed for the case\n        seed = 12345 + i\n        _, y_sim = simulate_data(T, a1, a2, q, r, m0, P0, tk, seed)\n        \n        H = 1.0\n\n        # --- Break-Aware Model ---\n        F_break = np.array([a1 if t < tk else a2 for t in range(T)])\n        \n        m_filt_break, P_filt_break, m_pred_break, P_pred_break = kalman_filter(y_sim, F_break, q, H, r, m0, P0)\n        \n        m_smooth_break, _ = rts_smoother(m_filt_break, P_filt_break, m_pred_break, P_pred_break, F_break)\n        \n        # --- No-Break Model ---\n        F_nobreak = np.full(T, a1)\n        \n        m_filt_nobreak, P_filt_nobreak, m_pred_nobreak, P_pred_nobreak = kalman_filter(y_sim, F_nobreak, q, H, r, m0, P0)\n\n        m_smooth_nobreak, _ = rts_smoother(m_filt_nobreak, P_filt_nobreak, m_pred_nobreak, P_pred_nobreak, F_nobreak)\n\n        # 2. Compute metrics\n        # Metric D: Immediate structural response\n        m_tk_filt_break = m_filt_break[tk]\n        D = np.abs((a2 - a1) * m_tk_filt_break)\n        \n        # Metric A: Smoothed break adjustment\n        m_tk_smooth_break = m_smooth_break[tk]\n        m_tk_smooth_nobreak = m_smooth_nobreak[tk]\n        A = m_tk_smooth_break - m_tk_smooth_nobreak\n        \n        results.extend([D, A])\n\n    # Final print statement in the exact required format\n    formatted_results = [f'{v:.6f}' for v in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}