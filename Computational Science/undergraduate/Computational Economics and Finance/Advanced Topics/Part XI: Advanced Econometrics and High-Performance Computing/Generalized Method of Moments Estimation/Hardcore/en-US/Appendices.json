{
    "hands_on_practices": [
        {
            "introduction": "To build a strong foundation, we begin with a problem that strips GMM down to its essential logic. Instead of a typical linear model, this exercise  introduces a moment condition based on a trigonometric function, often used for modeling cyclical or circular data. By deriving the GMM estimator analytically, you will directly engage with the core principle of minimizing a quadratic form of sample moments, providing a clear and tangible understanding of how the GMM objective function works without the complexity of matrix algebra or numerical optimization.",
            "id": "2397110",
            "problem": "Consider a setting in computational finance where cyclical behavior is modeled on the unit circle. You observe $T$ independent and identically distributed angles $X_1, X_2, \\ldots, X_T \\in (-\\pi, \\pi]$ that represent the phase of a latent cyclical factor in intraday market activity. The true phase parameter is $\\theta \\in (-\\pi, \\pi]$. The structural moment condition implied by the model is\n$$\n\\mathbb{E}\\!\\left[\\sin(X_t - \\theta)\\right] = 0.\n$$\nUsing the Generalized Method of Moments (GMM) with the identity weighting matrix (a scalar equal to $1$), derive the estimator $\\hat{\\theta}_T$ as a closed-form analytic expression in terms of the sample $\\{X_t\\}_{t=1}^T$. Assume that $\\sum_{t=1}^{T} \\cos(X_t) > 0$ so that the estimator is uniquely pinned down in the principal value range $(-\\pi, \\pi]$. Provide your final answer as a single analytic expression. No rounding is required, and no units are applicable.",
            "solution": "The problem requires the derivation of a Generalized Method of Moments (GMM) estimator. A rigorous validation of the problem statement is the mandatory first step.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Data**: $T$ independent and identically distributed (i.i.d.) observations of angles, $\\{X_t\\}_{t=1}^T$, where each $X_t \\in (-\\pi, \\pi]$.\n*   **Parameter**: The true parameter is $\\theta \\in (-\\pi, \\pi]$.\n*   **Moment Condition**: The population moment condition is $\\mathbb{E}\\!\\left[\\sin(X_t - \\theta)\\right] = 0$.\n*   **GMM Setup**: The weighting matrix is the identity, which in this scalar case is $W=1$.\n*   **Assumption**: It is given that $\\sum_{t=1}^{T} \\cos(X_t) > 0$.\n*   **Objective**: Derive the estimator $\\hat{\\theta}_T$ as a closed-form analytic expression.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientific Groundedness**: The problem is formulated within the standard framework of econometric estimation (GMM). The moment condition $\\mathbb{E}[\\sin(X_t - \\theta)] = 0$ is a common and valid way to define a central parameter for circular data, such as in the von Mises distribution, where $\\theta$ represents the mean direction. The problem is scientifically sound.\n*   **Well-Posedness**: The problem seeks a unique estimator. The provided assumption, $\\sum_{t=1}^{T} \\cos(X_t) > 0$, is explicitly included to resolve the ambiguity inherent in inverting the tangent function, thereby ensuring a unique solution exists in the specified principal value range. The problem is well-posed.\n*   **Objectivity**: The problem is stated using precise mathematical language and definitions. It is entirely free of subjective or ambiguous terminology.\n*   **Completeness and Consistency**: All information required to derive the estimator is provided: the moment condition, the sample data structure, the estimation framework (GMM with a specified weight matrix), and a critical assumption for uniqueness. The setup is self-contained and free of contradictions.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. It is a well-defined statistical estimation task. I will proceed with the derivation.\n\n### Derivation of the Estimator\n\nThe Generalized Method of Moments (GMM) estimator, $\\hat{\\theta}_T$, is the value of the parameter $\\theta$ that minimizes the GMM objective function, $J(\\theta)$. This function is a quadratic form of the sample moment conditions.\n\nThe population moment condition is given by $g(\\theta) = \\mathbb{E}[\\sin(X_t - \\theta)] = 0$.\nThe corresponding sample moment condition is its empirical analogue:\n$$\ng_T(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} \\sin(X_t - \\theta)\n$$\nThe GMM objective function is $J(\\theta) = T \\cdot g_T(\\theta)' W g_T(\\theta)$. In this problem, we have a single moment condition, so $g_T(\\theta)$ is a scalar. The weighting matrix is specified as the identity, so $W=1$. The objective function simplifies to:\n$$\nJ(\\theta) = T \\left( \\frac{1}{T} \\sum_{t=1}^{T} \\sin(X_t - \\theta) \\right)^2 = \\frac{1}{T} \\left( \\sum_{t=1}^{T} \\sin(X_t - \\theta) \\right)^2\n$$\nThe GMM estimator $\\hat{\\theta}_T$ is found by minimizing $J(\\theta)$ with respect to $\\theta$:\n$$\n\\hat{\\theta}_T = \\arg\\min_{\\theta \\in (-\\pi, \\pi]} J(\\theta)\n$$\nSince $J(\\theta)$ is a squared quantity scaled by a positive constant $1/T$, its minimum value is $0$. This minimum is achieved if and only if the term inside the square is zero. Therefore, the minimization problem is equivalent to solving the following equation for $\\theta$:\n$$\n\\sum_{t=1}^{T} \\sin(X_t - \\hat{\\theta}_T) = 0\n$$\nTo solve for $\\hat{\\theta}_T$, we apply the trigonometric identity for the sine of a difference, $\\sin(A - B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$:\n$$\n\\sum_{t=1}^{T} \\left[ \\sin(X_t)\\cos(\\hat{\\theta}_T) - \\cos(X_t)\\sin(\\hat{\\theta}_T) \\right] = 0\n$$\nSince $\\cos(\\hat{\\theta}_T)$ and $\\sin(\\hat{\\theta}_T)$ are constant with respect to the summation index $t$, we can factor them out:\n$$\n\\cos(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\sin(X_t) - \\sin(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\cos(X_t) = 0\n$$\nRearranging the terms, we get:\n$$\n\\sin(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\cos(X_t) = \\cos(\\hat{\\theta}_T) \\sum_{t=1}^{T} \\sin(X_t)\n$$\nLet us define $S_T = \\sum_{t=1}^{T} \\sin(X_t)$ and $C_T = \\sum_{t=1}^{T} \\cos(X_t)$. The equation becomes $\\sin(\\hat{\\theta}_T) C_T = \\cos(\\hat{\\theta}_T) S_T$.\n\nWe must ensure we can divide to isolate $\\hat{\\theta}_T$. The problem states the assumption $C_T = \\sum_{t=1}^{T} \\cos(X_t) > 0$. This implies $C_T \\neq 0$. Furthermore, if $\\cos(\\hat{\\theta}_T) = 0$, the equation would become $\\sin(\\hat{\\theta}_T) C_T = 0$. Since $C_T \\neq 0$, this would require $\\sin(\\hat{\\theta}_T) = 0$, which is impossible as $\\sin(\\theta)$ and $\\cos(\\theta)$ are never simultaneously zero. Thus, $\\cos(\\hat{\\theta}_T) \\neq 0$.\n\nWe can safely divide both sides by $\\cos(\\hat{\\theta}_T)$ and by $C_T$:\n$$\n\\frac{\\sin(\\hat{\\theta}_T)}{\\cos(\\hat{\\theta}_T)} = \\tan(\\hat{\\theta}_T) = \\frac{\\sum_{t=1}^{T} \\sin(X_t)}{\\sum_{t=1}^{T} \\cos(X_t)}\n$$\nThis equation defines the estimator $\\hat{\\theta}_T$. To obtain a unique solution, we must correctly invert the tangent function. The equation $\\sin(\\hat{\\theta}_T) C_T = \\cos(\\hat{\\theta}_T) S_T$ implies that the vector $(\\cos(\\hat{\\theta}_T), \\sin(\\hat{\\theta}_T))$ is parallel to the vector $(C_T, S_T)$. As the magnitude of $(\\cos(\\theta), \\sin(\\theta))$ is $1$, this means $\\cos(\\hat{\\theta}_T)$ must have the same sign as $C_T$.\n\nGiven the explicit assumption $C_T = \\sum_{t=1}^{T} \\cos(X_t) > 0$, we must have $\\cos(\\hat{\\theta}_T) > 0$. This condition restricts the solution for $\\hat{\\theta}_T$ to the interval $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$. The standard arctangent function, $\\arctan(\\cdot)$, has a principal value range of precisely $(-\\frac{\\pi}{2}, \\frac{\\pi}{2})$. Therefore, the ambiguity is resolved, and the unique GMM estimator is given by:\n$$\n\\hat{\\theta}_T = \\arctan\\left(\\frac{\\sum_{t=1}^{T} \\sin(X_t)}{\\sum_{t=1}^{T} \\cos(X_t)}\\right)\n$$\nThis expression provides the closed-form analytic solution for the estimator $\\hat{\\theta}_T$ in terms of the sample data, as required.",
            "answer": "$$\n\\boxed{\\arctan\\left(\\frac{\\sum_{t=1}^{T} \\sin(X_t)}{\\sum_{t=1}^{T} \\cos(X_t)}\\right)}\n$$"
        },
        {
            "introduction": "Moving from analytical derivation to computational practice, this exercise  challenges you to apply GMM to a powerful and widely used technique: quantile regression. This problem demonstrates the flexibility of GMM in handling models derived from non-differentiable objective functions, a common scenario in modern econometrics. You will translate the theoretical first-order condition of quantile regression into a valid moment condition for GMM and implement a complete two-step estimation procedure, bridging the gap between abstract theory and practical, code-based problem-solving.",
            "id": "2397079",
            "problem": "You are given the task of formulating and computing a Generalized Method of Moments (GMM) estimator for a linear quantile regression model using a moment condition derived from the check loss. Consider observations $\\{(y_i, x_i, z_i)\\}_{i=1}^n$, where $y_i \\in \\mathbb{R}$ is the dependent variable, $x_i \\in \\mathbb{R}^p$ is a column vector of regressors, and $z_i \\in \\mathbb{R}^{p_z}$ is a column vector of instruments. The linear quantile regression model specifies the $\\tau$-th conditional quantile of $y_i$ given $x_i$ as $Q_{\\tau}(y_i \\mid x_i) = x_i^{\\prime}\\beta$, for a parameter vector $\\beta \\in \\mathbb{R}^p$ and a quantile index $\\tau \\in (0,1)$. The check function is defined by $\\rho_{\\tau}(u) = u(\\tau - I(u<0))$, where $I(\\cdot)$ is the indicator function.\n\nStarting from the definition of the check function and subgradient optimality for quantile loss minimization, derive a valid unconditional moment condition suitable for GMM based on instruments, and then define the GMM sample objective to estimate $\\beta$. From first principles, base your derivation on the definitions of the check function, subgradients of convex functions, and the method of moments principle that the population moment equals zero at the true parameter. Do not assume or use any pre-packaged shortcut formulae for the target moment condition.\n\nYour program must implement a two-step GMM estimator using the following design:\n\n- Use the sample moment function $g_n(\\beta) = \\frac{1}{n}\\sum_{i=1}^n z_i \\cdot \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta)$, where $\\psi_{\\tau}(u)$ is a subgradient of $\\rho_{\\tau}(u)$, and the GMM objective $J_n(\\beta; W) = g_n(\\beta)^{\\prime} W g_n(\\beta)$ for a positive definite weighting matrix $W \\in \\mathbb{R}^{p_z \\times p_z}$.\n- Step 1: Use the identity weighting matrix $W = I_{p_z}$ to obtain a preliminary estimate $\\hat{\\beta}_1$ by minimizing $J_n(\\beta; I_{p_z})$ over $\\beta \\in \\mathbb{R}^p$.\n- Step 2: Compute a feasible weighting matrix using the i.i.d. approximation of the moment covariance, $S \\approx \\tau(1-\\tau)\\,\\mathbb{E}[z_i z_i^{\\prime}]$, and estimate it by the sample analog $\\hat{S} = \\tau(1-\\tau)\\,\\frac{1}{n}\\sum_{i=1}^n z_i z_i^{\\prime}$. Use $W = \\hat{S}^{-1}$ (or the Moore–Penrose pseudoinverse if necessary) and minimize $J_n(\\beta; \\hat{S}^{-1})$ to obtain the two-step GMM estimate $\\hat{\\beta}_{\\text{GMM}}$.\n\nYour program must solve the minimizations using a deterministic, derivative-free global-local search procedure:\n\n- Construct a hyper-rectangle of plausible parameter values as follows. If the first column of $X$ is an intercept (all ones), define the intercept bound using robust location and scale of $y$: let $q_{0.1}$ and $q_{0.9}$ be the $0.1$ and $0.9$ sample quantiles of $y$, and let $\\text{IQR} = q_{0.75} - q_{0.25}$. The intercept is restricted to $[q_{0.1} - 2\\,\\text{IQR},\\; q_{0.9} + 2\\,\\text{IQR}]$. For each non-intercept regressor $x_{\\cdot j}$, define $R_j = 10 \\cdot \\frac{\\text{sd}(y)}{\\max(\\text{sd}(x_{\\cdot j}), 10^{-8})}$, center the bound at the ordinary least squares coefficient for that regressor (computed when feasible), and restrict the coefficient to $[\\hat{\\beta}^{\\text{OLS}}_j - R_j,\\; \\hat{\\beta}^{\\text{OLS}}_j + R_j]$. If no intercept is present, apply this rule to all coefficients using the ordinary least squares center when feasible; otherwise center at zero.\n- Perform a coarse grid search with exactly $81$ equally spaced points per dimension over this hyper-rectangle to minimize $J_n(\\beta; W)$ and select the best grid point, breaking ties by choosing the candidate with the smallest Euclidean norm of $\\beta$.\n- Build a refined hyper-rectangle centered at the best coarse solution with half-width equal to $\\max\\{0.25$ times the original half-width, $10^{-3}\\}$ in each dimension. Perform a second grid search with exactly $81$ points per dimension over this refined region, again with the same tie-breaking rule.\n- Use the refined grid solution as $\\hat{\\beta}_1$ in Step 1. Then compute $\\hat{S}$ and $W = \\hat{S}^{-1}$ and repeat the two-stage grid search (coarse then refined) to produce the final two-step GMM estimate $\\hat{\\beta}_{\\text{GMM}}$.\n\nFor all test cases below, use instruments equal to regressors, i.e., $z_i = x_i$ and $Z = X$.\n\nImplement your program to solve the following three test cases. Each test case provides $(y, X, \\tau)$, with $Z=X$.\n\n- Test 1 (happy path, median regression with intercept and slope):\n  - $n=12$, $x = (-2.0, -1.5, -1.0, -0.3, 0.0, 0.2, 0.5, 1.0, 1.2, 1.8, 2.2, 2.5)$ and $X = [\\mathbf{1}, x]$.\n  - $y$ is generated deterministically by $y_i = 1.0 + 0.8\\,x_i + \\varepsilon_i$ with $\\varepsilon = (0.1, -0.2, 0.05, 0.1, -0.05, 0.0, 0.2, -0.1, 0.0, 0.15, -0.15, 0.05)$.\n  - $\\tau = 0.5$.\n- Test 2 (boundary structure, intercept-only model targeting a lower quantile):\n  - $n=11$, $X = \\mathbf{1}$, $y = (2.2, 1.0, 1.5, 0.8, 3.2, 2.0, 1.8, 2.5, 2.3, 0.9, 1.7)$.\n  - $\\tau = 0.25$.\n- Test 3 (robustness to an outlier at a high quantile):\n  - $n=15$, $x = (-1.0, -0.5, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4)$ and $X = [\\mathbf{1}, x]$.\n  - $y_i = 0.5 + 0.6\\,x_i + \\eta_i$ with $\\eta = (0.0, 0.1, -0.05, 0.0, 0.02, -0.03, 0.04, -0.02, 0.01, 0.0, -0.05, 0.03, 0.02, 8.0, -0.04)$.\n  - $\\tau = 0.9$.\n\nYour program should output the two-step GMM estimates $\\hat{\\beta}_{\\text{GMM}}$ for each test case as a list of lists, where each inner list is the parameter vector corresponding to the test case, with all entries rounded to six decimal places. The final output must be a single line containing these three lists as a comma-separated sequence enclosed in square brackets, for example, $[\\,[\\cdot,\\cdot],\\,[\\cdot],\\,[\\cdot,\\cdot]\\,]$.\n\nAll quantities are dimensionless; no physical units are involved. Angles are not used. The outputs must be lists of floating-point numbers only. The program must not read any input and must not access any external files or networks. It must use a deterministic procedure as described above so that the same inputs yield the same outputs. The numerical libraries allowed are the Python standard library, NumPy, and SciPy.",
            "solution": "The problem requires the formulation and implementation of a two-step Generalized Method of Moments (GMM) estimator for a linear quantile regression model. The process begins with a formal derivation of the underlying moment condition, followed by an explanation of the specified numerical estimation algorithm.\n\n**1. Derivation of the GMM Moment Condition**\n\nThe linear quantile regression model posits that the $\\tau$-th conditional quantile of a dependent variable $y_i$ given a column vector of regressors $x_i \\in \\mathbb{R}^p$ is a linear function of $x_i$:\n$$\nQ_{\\tau}(y_i \\mid x_i) = x_i^{\\prime}\\beta_0\n$$\nwhere $\\beta_0 \\in \\mathbb{R}^p$ is the true parameter vector. The standard estimator for $\\beta_0$ is found by minimizing the sum of check losses:\n$$\n\\hat{\\beta} = \\arg\\min_{\\beta \\in \\mathbb{R}^p} \\sum_{i=1}^n \\rho_{\\tau}(y_i - x_i^{\\prime}\\beta)\n$$\nThe check function, $\\rho_{\\tau}(u): \\mathbb{R} \\to \\mathbb{R}$, is defined as:\n$$\n\\rho_{\\tau}(u) = u(\\tau - I(u<0))\n$$\nwhere $I(\\cdot)$ is the indicator function, which equals $1$ if its argument is true and $0$ otherwise. This function is convex, and therefore, a necessary and sufficient condition for a minimum is that the zero vector must be an element of the subgradient of the objective function at the solution $\\hat{\\beta}$.\n\nThe subgradient of the objective function with respect to $\\beta$ is obtained using the chain rule for subgradients. Let $u_i(\\beta) = y_i - x_i^{\\prime}\\beta$. The subgradient with respect to $\\beta$ is:\n$$\n\\partial_{\\beta} \\left( \\sum_{i=1}^n \\rho_{\\tau}(u_i(\\beta)) \\right) = \\sum_{i=1}^n \\partial_{\\beta} \\rho_{\\tau}(u_i(\\beta)) = \\sum_{i=1}^n (-x_i) \\cdot \\partial_u \\rho_{\\tau}(u_i(\\beta))\n$$\nThe subgradient of the check function $\\rho_{\\tau}(u)$ with respect to its argument $u$ is given by:\n$$\n\\partial_u \\rho_{\\tau}(u) =\n\\begin{cases}\n    \\{\\tau - 1\\} & \\text{if } u < 0 \\\\\n    [\\tau - 1, \\tau] & \\text{if } u = 0 \\\\\n    \\{\\tau\\} & \\text{if } u > 0\n\\end{cases}\n$$\nThe problem specifies using the function $\\psi_{\\tau}(u) = \\tau - I(u<0)$ as a selection from this subgradient set. Note that for $u=0$, $\\psi_{\\tau}(0) = \\tau$, which is a valid selection from the interval $[\\tau - 1, \\tau]$.\n\nThe first-order condition for minimization is that zero must be in the subgradient set, which can be stated as:\n$$\n\\frac{1}{n} \\sum_{i=1}^n x_i \\psi_{\\tau}(y_i - x_i^{\\prime}\\hat{\\beta}) \\approx \\mathbf{0}\n$$\nBy analogy, the population moment condition, which holds exactly at the true parameter $\\beta_0$, is $\\mathbb{E}[x_i \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta_0)] = \\mathbf{0}$. For GMM estimation, we generalize this condition by using a vector of instruments $z_i \\in \\mathbb{R}^{p_z}$, which are assumed to be uncorrelated with the quantile error term. The population moment condition becomes:\n$$\n\\mathbb{E}[z_i \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta_0)] = \\mathbf{0}\n$$\nThe corresponding sample moment vector, which is a function of a candidate parameter vector $\\beta$, is its sample analog:\n$$\ng_n(\\beta) = \\frac{1}{n} \\sum_{i=1}^n z_i \\psi_{\\tau}(y_i - x_i^{\\prime}\\beta)\n$$\n\n**2. Two-Step GMM Estimation**\n\nThe GMM estimator $\\hat{\\beta}_{\\text{GMM}}$ minimizes a quadratic form of the sample moment vector:\n$$\nJ_n(\\beta; W) = g_n(\\beta)^{\\prime} W g_n(\\beta)\n$$\nwhere $W$ is a $p_z \\times p_z$ positive definite weighting matrix. The estimation proceeds in two steps.\n\n**Step 1:** A preliminary, consistent estimate of $\\beta$ is obtained using a sub-optimal but simple weighting matrix. As specified, the identity matrix $W_1 = I_{p_z}$ is used. The first-step estimator is:\n$$\n\\hat{\\beta}_1 = \\arg\\min_{\\beta} J_n(\\beta; I_{p_z}) = \\arg\\min_{\\beta} g_n(\\beta)^{\\prime} g_n(\\beta)\n$$\n\n**Step 2:** An asymptotically efficient estimator is obtained by using a weighting matrix that is a consistent estimate of the inverse of the asymptotic covariance matrix of the sample moments. The problem specifies a particular approximation for this covariance matrix, $\\hat{S}$, given by:\n$$\n\\hat{S} = \\tau(1-\\tau) \\left( \\frac{1}{n} \\sum_{i=1}^n z_i z_i^{\\prime} \\right) = \\frac{\\tau(1-\\tau)}{n} Z^{\\prime}Z\n$$\nwhere $Z$ is the $n \\times p_z$ matrix of instruments. The second-step weighting matrix is $W_2 = \\hat{S}^{-1}$. For numerical stability, the Moore-Penrose pseudoinverse is used. The two-step GMM estimator is then:\n$$\n\\hat{\\beta}_{\\text{GMM}} = \\arg\\min_{\\beta} J_n(\\beta; \\hat{S}^{-1})\n$$\n\n**3. Numerical Minimization via Grid Search**\n\nBoth minimization problems are solved using a deterministic two-stage grid search procedure.\n\n**Search Space Definition:** A hyper-rectangular search space for $\\beta$ is constructed based on sample data.\n- For an intercept parameter, the search interval is $[q_{0.1} - 2\\,\\text{IQR}, q_{0.9} + 2\\,\\text{IQR}]$, where $q$ denotes sample quantiles of $y$ and $\\text{IQR} = q_{0.75} - q_{0.25}$.\n- For a non-intercept coefficient $\\beta_j$, the interval is centered at the Ordinary Least Squares (OLS) estimate $\\hat{\\beta}^{\\text{OLS}}_j$ with a range determined by data variability: $[\\hat{\\beta}^{\\text{OLS}}_j - R_j, \\hat{\\beta}^{\\text{OLS}}_j + R_j]$, where $R_j = 10 \\cdot \\frac{\\text{sd}(y)}{\\max(\\text{sd}(x_{\\cdot j}), 10^{-8})}$. If OLS is not feasible or no intercept is present, the interval is centered at $0$.\n\n**Grid Search Procedure:**\n1.  **Coarse Grid Search:** The objective function $J_n(\\beta; W)$ is evaluated at $81$ equally spaced points along each dimension of the initial hyper-rectangle. The parameter vector $\\beta$ yielding the minimum objective value is selected. Ties are broken by choosing the candidate vector with the smallest Euclidean norm.\n2.  **Refined Grid Search:** A new, smaller hyper-rectangle is defined. It is centered at the solution from the coarse search. Its half-width in each dimension is $\\max\\{0.25 \\times \\text{original half-width}, 10^{-3}\\}$. A second grid search with $81$ points per dimension is performed over this refined region. The result is the solution for that GMM step.\n\nThis entire two-stage search procedure is executed for Step 1 to find $\\hat{\\beta}_1$, and then repeated for Step 2 (including re-defining the initial search space) to find the final estimator $\\hat{\\beta}_{\\text{GMM}}$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GMM estimation for all specified test cases.\n    \"\"\"\n\n    def gmm_objective(beta, Y, X, Z, tau, W):\n        \"\"\"\n        Computes the GMM objective function J(beta; W) = g_n(beta)' * W * g_n(beta).\n        \"\"\"\n        n_obs, p_dim = X.shape\n        beta = np.asarray(beta).reshape(p_dim, 1)\n\n        residuals = Y.reshape(n_obs, 1) - X @ beta\n        psi_vals = tau - (residuals < 0).astype(float)\n        \n        # moment_vals has shape (n_obs, p_z)\n        moment_vals = Z * psi_vals\n        \n        # g_n is the mean of moment_vals, shape (p_z,)\n        g_n = np.mean(moment_vals, axis=0)\n        \n        # J = g_n' W g_n\n        objective_value = g_n.T @ W @ g_n\n        return objective_value\n\n    def build_search_bounds(Y, X):\n        \"\"\"\n        Constructs the hyper-rectangle for the grid search based on problem specification.\n        \"\"\"\n        n_obs, p_dim = X.shape\n        bounds = np.zeros((p_dim, 2))\n        \n        has_intercept = np.all(X[:, 0] == 1)\n        \n        try:\n            # Use lstsq for robust OLS estimation\n            beta_ols, _, _, _ = np.linalg.lstsq(X, Y, rcond=None)\n            ols_feasible = True\n        except np.linalg.LinAlgError:\n            beta_ols = np.zeros(p_dim)\n            ols_feasible = False\n            \n        sd_y = np.std(Y)\n\n        for j in range(p_dim):\n            if j == 0 and has_intercept:\n                q = np.quantile(Y, [0.1, 0.9, 0.25, 0.75])\n                q01, q09, q025, q075 = q[0], q[1], q[2], q[3]\n                iqr = q075 - q025\n                bounds[j, :] = [q01 - 2 * iqr, q09 + 2 * iqr]\n            else:\n                center = beta_ols[j] if ols_feasible else 0.0\n                sd_xj = np.std(X[:, j])\n                R_j = 10.0 * sd_y / max(sd_xj, 1e-8)\n                bounds[j, :] = [center - R_j, center + R_j]\n                \n        return bounds\n\n    def grid_search_minimizer(obj_func, bounds):\n        \"\"\"\n        Performs the two-stage (coarse + refined) grid search.\n        \"\"\"\n        p_dim = bounds.shape[0]\n        n_points = 81\n\n        # --- Coarse Grid Search ---\n        grid_axes = [np.linspace(b[0], b[1], n_points) for b in bounds]\n        grid_mesh = np.meshgrid(*grid_axes)\n        grid_points = np.vstack([m.ravel() for m in grid_mesh]).T\n\n        min_obj_val = np.inf\n        best_beta_coarse = None\n\n        for beta_candidate in grid_points:\n            val = obj_func(beta_candidate)\n            if val < min_obj_val:\n                min_obj_val = val\n                best_beta_coarse = beta_candidate\n            elif np.isclose(val, min_obj_val):\n                if np.linalg.norm(beta_candidate) < np.linalg.norm(best_beta_coarse):\n                    best_beta_coarse = beta_candidate\n        \n        # --- Refined Grid Search ---\n        refined_bounds = np.zeros_like(bounds)\n        for j in range(p_dim):\n            original_half_width = (bounds[j, 1] - bounds[j, 0]) / 2.0\n            new_half_width = max(0.25 * original_half_width, 1e-3)\n            refined_bounds[j, :] = [best_beta_coarse[j] - new_half_width, best_beta_coarse[j] + new_half_width]\n\n        refined_grid_axes = [np.linspace(b[0], b[1], n_points) for b in refined_bounds]\n        refined_grid_mesh = np.meshgrid(*refined_grid_axes)\n        refined_grid_points = np.vstack([m.ravel() for m in refined_grid_mesh]).T\n\n        min_obj_val_refined = np.inf\n        best_beta_refined = None\n\n        for beta_candidate in refined_grid_points:\n            val = obj_func(beta_candidate)\n            if val < min_obj_val_refined:\n                min_obj_val_refined = val\n                best_beta_refined = beta_candidate\n            elif np.isclose(val, min_obj_val_refined):\n                if np.linalg.norm(beta_candidate) < np.linalg.norm(best_beta_refined):\n                    best_beta_refined = beta_candidate\n        \n        return best_beta_refined\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        {\n            \"y\": np.array([1.0 + 0.8 * x_i + eps_i for x_i, eps_i in zip(\n                [-2.0, -1.5, -1.0, -0.3, 0.0, 0.2, 0.5, 1.0, 1.2, 1.8, 2.2, 2.5],\n                [0.1, -0.2, 0.05, 0.1, -0.05, 0.0, 0.2, -0.1, 0.0, 0.15, -0.15, 0.05]\n            )]),\n            \"X\": np.array([\n                [1.0, -2.0], [1.0, -1.5], [1.0, -1.0], [1.0, -0.3], [1.0, 0.0], [1.0, 0.2],\n                [1.0, 0.5], [1.0, 1.0], [1.0, 1.2], [1.0, 1.8], [1.0, 2.2], [1.0, 2.5]\n            ]),\n            \"tau\": 0.5\n        },\n        {\n            \"y\": np.array([2.2, 1.0, 1.5, 0.8, 3.2, 2.0, 1.8, 2.5, 2.3, 0.9, 1.7]),\n            \"X\": np.ones((11, 1)),\n            \"tau\": 0.25\n        },\n        {\n            \"y\": np.array([0.5 + 0.6 * x_i + eta_i for x_i, eta_i in zip(\n                [-1.0, -0.5, 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4],\n                [0.0, 0.1, -0.05, 0.0, 0.02, -0.03, 0.04, -0.02, 0.01, 0.0, -0.05, 0.03, 0.02, 8.0, -0.04]\n            )]),\n            \"X\": np.array([\n                [1.0, -1.0], [1.0, -0.5], [1.0, 0.0], [1.0, 0.2], [1.0, 0.4], [1.0, 0.6],\n                [1.0, 0.8], [1.0, 1.0], [1.0, 1.2], [1.0, 1.4], [1.0, 1.6], [1.0, 1.8],\n                [1.0, 2.0], [1.0, 2.2], [1.0, 2.4]\n            ]),\n            \"tau\": 0.9\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        Y, X, tau = case[\"y\"], case[\"X\"], case[\"tau\"]\n        Z = X  # Instruments are regressors\n        n, pz = Z.shape\n        \n        # Define search space once for both steps\n        bounds = build_search_bounds(Y, X)\n\n        # --- Step 1 GMM ---\n        W1 = np.identity(pz)\n        obj_func_1 = lambda beta: gmm_objective(beta, Y, X, Z, tau, W1)\n        # We don't need beta_1, but we run the procedure as specified.\n        # The problem statement says to repeat the two-stage grid search for Step 2.\n        # This implies running the full procedure, though beta_1 is not used further.\n        _ = grid_search_minimizer(obj_func_1, bounds)\n        \n        # --- Step 2 GMM ---\n        S_hat = tau * (1.0 - tau) * (Z.T @ Z) / n\n        W2 = np.linalg.pinv(S_hat)\n        obj_func_2 = lambda beta: gmm_objective(beta, Y, X, Z, tau, W2)\n        beta_gmm_2_step = grid_search_minimizer(obj_func_2, bounds)\n        \n        # Round and format results\n        result = [round(b, 6) for b in beta_gmm_2_step]\n        final_results.append(result)\n\n    # Format final output string\n    output_str = \"[\"\n    for i, res in enumerate(final_results):\n        res_str = f\"[{','.join(map(str, res))}]\"\n        output_str += res_str\n        if i < len(final_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Print in the exact required format\n    # The f-string representation of lists might have spaces, so we build it manually\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Estimation is only half the battle; a crucial part of econometric modeling is testing the validity of your assumptions. This final practice  focuses on GMM-based hypothesis testing, a vital skill for model evaluation and diagnostics. Through a Monte Carlo simulation, you will investigate and compare the statistical power of the omnibus J-test for overidentifying restrictions against a more targeted difference test (D-test). This exercise provides invaluable intuition on how to detect misspecified models and understand the trade-offs between general and specific hypothesis tests in an instrumental variables context.",
            "id": "2397116",
            "problem": "You are given a linear instrumental variables environment in which a scalar structural parameter $ \\beta \\in \\mathbb{R} $ is identified by a set of unconditional moments. For each observation $ t \\in \\{1,\\dots,n\\} $, let $ z_{1t}, z_{2t}, z_{3t} \\in \\mathbb{R} $ be instruments, $ x_t \\in \\mathbb{R} $ be a regressor, and $ y_t \\in \\mathbb{R} $ be an outcome. Consider the following data generating process, with all random variables mutually independent unless explicitly linked, and all expectations taken with respect to the joint distribution of the draws:\n- Instruments: $ z_{1t} \\sim \\mathcal{N}(0,1) $, $ z_{2t} \\sim \\mathcal{N}(0,1) $, $ z_{3t} \\sim \\mathcal{N}(0,1) $.\n- First-stage disturbance: $ v_t \\sim \\mathcal{N}(0,1) $.\n- Structural disturbance: $ e_t \\sim \\mathcal{N}(0,1) $.\n- Regressor: $ x_t = z_{1t} + 0.5 z_{3t} + v_t $.\n- Structural error with potential invalidity of $ z_{2t} $: $ u_t = \\delta \\, z_{2t} + e_t $ where $ \\delta \\in \\mathbb{R} $.\n- Outcome: $ y_t = \\beta_0 \\, x_t + u_t $ with true parameter $ \\beta_0 = 1 $.\n\nDefine the $ 3 \\times 1 $ vector of instruments $ Z_t = \\big(z_{1t}, z_{2t}, z_{3t}\\big)^{\\prime} $. For any candidate $ \\beta \\in \\mathbb{R} $, define the $ 3 \\times 1 $ moment function $ g_t(\\beta) = Z_t \\, \\big(y_t - \\beta x_t\\big) $. Let $ \\mathbb{E}[g_t(\\beta)] = 0 $ denote the target moment conditions. When $ \\delta = 0 $, all three instruments are valid for the linear model and the moments are correctly specified. When $ \\delta \\neq 0 $, instrument $ z_{2t} $ is invalid while $ z_{1t} $ and $ z_{3t} $ remain valid.\n\nUsing the Generalized Method of Moments (GMM), define the sample average moment $ \\bar{g}_n(\\beta) = \\frac{1}{n} \\sum_{t=1}^n g_t(\\beta) $ and the GMM criterion $ J_n(\\beta; W) = n \\, \\bar{g}_n(\\beta)^{\\prime} W \\, \\bar{g}_n(\\beta) $ for any symmetric positive definite weighting matrix $ W \\in \\mathbb{R}^{3 \\times 3} $. The efficient GMM weighting matrix is the inverse of the long-run covariance of the moments. Let $ \\widehat{\\beta}_{\\text{full}} $ be the efficient GMM estimator using the full $ 3 \\times 1 $ moment vector $ g_t(\\beta) $ and let $ J_{\\text{full}} = J_n\\big(\\widehat{\\beta}_{\\text{full}}; \\widehat{W}_{\\text{full}}\\big) $ be the minimized criterion value, where $ \\widehat{W}_{\\text{full}} $ is the efficient estimate of the optimal weighting matrix for the full set of moments. Similarly, let $ \\widehat{\\beta}_{\\text{rest}} $ be the efficient GMM estimator using only the restricted $ 2 \\times 1 $ subset of moments corresponding to $ (z_{1t}, z_{3t}) $ and let $ J_{\\text{rest}} = J_n\\big(\\widehat{\\beta}_{\\text{rest}}; \\widehat{W}_{\\text{rest}}\\big) $ be its minimized criterion value, with $ \\widehat{W}_{\\text{rest}} $ the corresponding efficient weight. Define:\n- The overidentifying restrictions test statistic (J-test): $ J_{\\text{full}} $, which under the null has an asymptotic $ \\chi^2 $ distribution with $ 3 - 1 = 2 $ degrees of freedom.\n- The targeted difference test (D-test, also called C-test) for the subset of moments associated with $ z_{2t} $: $ D = J_{\\text{full}} - J_{\\text{rest}} $, which under the null has an asymptotic $ \\chi^2 $ distribution with $ 1 $ degree of freedom.\n\nFor a significance level $ \\alpha = 0.05 $, you must compute, by simulation, the empirical rejection probability (interpreted as empirical power when $ \\delta \\neq 0 $ and as empirical size when $ \\delta = 0 $) of:\n- The J-test using the full set of three moments.\n- The D-test targeting the subset consisting of the moment(s) that involve $ z_{2t} $ by comparing the full set $ \\{z_{1t}, z_{2t}, z_{3t}\\} $ to the restricted set $ \\{z_{1t}, z_{3t}\\} $.\n\nFor each test, a rejection occurs when the corresponding statistic exceeds the $ (1-\\alpha) $ quantile of its reference $ \\chi^2 $ distribution with the specified degrees of freedom.\n\nImplement a simulation using independent and identically distributed draws according to the data generating process above. For reproducibility, use the fixed random seed $ 123456 $. For each parameter tuple $ (n,\\delta,R) $ in the test suite below, generate $ R $ independent samples of size $ n $, compute $ J_{\\text{full}} $ and $ D $ for each replication, record whether each test rejects at level $ \\alpha $, and report the empirical rejection probabilities as the average of the binary rejection indicators across the $ R $ replications.\n\nTest suite:\n- Case $ 1 $: $ (n,\\delta,R) = (400, 0.0, 500) $.\n- Case $ 2 $: $ (n,\\delta,R) = (400, 0.3, 500) $.\n- Case $ 3 $: $ (n,\\delta,R) = (400, 0.6, 500) $.\n- Case $ 4 $: $ (n,\\delta,R) = (120, 0.6, 500) $.\n\nYour program must output a single line that is a comma-separated list enclosed in square brackets, containing, in order, the empirical rejection probabilities for the J-test and the D-test for each case, rounded to three decimal places. The required output format is:\n$ \\big[ p_{J,1}, p_{D,1}, p_{J,2}, p_{D,2}, p_{J,3}, p_{D,3}, p_{J,4}, p_{D,4} \\big] $,\nwhere $ p_{J,i} $ is the empirical rejection probability of the J-test for case $ i $ and $ p_{D,i} $ is the empirical rejection probability of the D-test for case $ i $.",
            "solution": "The problem as stated constitutes a valid, well-posed exercise in computational econometrics. It is scientifically grounded in the established theory of the Generalized Method of Moments (GMM), providing a complete and consistent specification for a Monte Carlo simulation to evaluate the properties of standard hypothesis tests. I will now proceed with the solution.\n\nThe objective is to compute, by simulation, the empirical size and power of two hypothesis tests—the J-test for overidentifying restrictions and the D-test for a subset of moments—in a linear instrumental variables (IV) setting. We are given a data generating process (DGP) for $R$ replications of samples of size $n$. For each replication, we must estimate the model parameters using GMM and compute the specified test statistics.\n\nThe structural model is given by $y_t = \\beta_0 x_t + u_t$, where the true parameter is $\\beta_0 = 1$. The regressor $x_t$ is endogenous, meaning $\\mathbb{E}[x_t u_t] \\neq 0$, because both $x_t$ and $u_t$ depend on other random variables in a potentially correlated manner. Specifically, $x_t = z_{1t} + 0.5 z_{3t} + v_t$ and $u_t = \\delta z_{2t} + e_t$. The endogeneity arises from the correlation between $x_t$ and $u_t$ when $\\delta \\neq 0$ through the omitted variable $z_{2t}$, or if $v_t$ and $e_t$ were correlated (though here they are specified as independent). The problem specifies three instruments in the vector $Z_t = (z_{1t}, z_{2t}, z_{3t})'$, which are used to form moment conditions. The validity of these instruments depends on the parameter $\\delta$.\n\nThe GMM framework is based on the population moment conditions $\\mathbb{E}[g_t(\\beta_0)] = 0$. For this problem, the $3 \\times 1$ vector of moment functions is $g_t(\\beta) = Z_t (y_t - \\beta x_t)$. When all instruments are valid (i.e., $\\mathbb{E}[Z_t u_t]=0$, which occurs when $\\delta=0$), all three moment conditions hold at the true parameter $\\beta_0=1$. When $\\delta \\neq 0$, the second instrument $z_{2t}$ becomes invalid because $\\mathbb{E}[z_{2t}u_t] = \\mathbb{E}[z_{2t}(\\delta z_{2t} + e_t)] = \\delta \\mathbb{E}[z_{2t}^2] = \\delta \\neq 0$. The moment condition $\\mathbb{E}[z_{2t}(y_t - \\beta_0 x_t)] = 0$ is violated.\n\nThe GMM estimator $\\widehat{\\beta}$ for a given symmetric positive definite weighting matrix $W$ is found by minimizing the quadratic form $J_n(\\beta; W) = n \\, \\bar{g}_n(\\beta)' W \\bar{g}_n(\\beta)$, where $\\bar{g}_n(\\beta) = \\frac{1}{n} \\sum_{t=1}^n g_t(\\beta)$ is the sample analogue of the population moments. For this linear model, the sample moments are $\\bar{g}_n(\\beta) = \\frac{1}{n}(Z'y - Z'x\\beta)$. The GMM estimator has the analytical solution $\\widehat{\\beta}(W) = (x'Z W Z'x)^{-1} (x'Z W Z'y)$.\n\nThe problem requires the use of the efficient GMM estimator, which employs the optimal weighting matrix $W^* = S^{-1}$, where $S = \\mathbb{E}[g_t(\\beta_0) g_t(\\beta_0)']$ is the long-run covariance matrix of the moment functions. Since $S$ is unknown, a two-step procedure is implemented:\n1.  **First Step**: Obtain a consistent estimate of $\\beta$, denoted $\\widehat{\\beta}^{(1)}$, using a suboptimal but valid weighting matrix, such as the identity matrix $W^{(1)}=I$ or the matrix corresponding to the two-stage least squares (2SLS) estimator, $W^{(1)}=(Z'Z/n)^{-1}$. The 2SLS estimator is a conventional choice for this step.\n2.  **Second Step**: Use $\\widehat{\\beta}^{(1)}$ to form residuals $\\widehat{u}_t = y_t - \\widehat{\\beta}^{(1)} x_t$ and construct a consistent estimator of $S$, given by $\\widehat{S} = \\frac{1}{n} \\sum_{t=1}^n Z_t Z_t' \\widehat{u}_t^2$. The estimated optimal weighting matrix is then $\\widehat{W} = \\widehat{S}^{-1}$. The efficient GMM estimator is then computed as $\\widehat{\\beta}_{\\text{GMM}} = (x'Z \\widehat{W} Z'x)^{-1} (x'Z \\widehat{W} Z'y)$.\n\nThe simulation requires calculating two test statistics:\n\n1.  **The J-test statistic**: $J_{\\text{full}} = n \\cdot \\bar{g}_n(\\widehat{\\beta}_{\\text{full}})' \\widehat{W}_{\\text{full}} \\bar{g}_n(\\widehat{\\beta}_{\\text{full}})$. This statistic tests the null hypothesis that all moment conditions are correctly specified. Here, $\\widehat{\\beta}_{\\text{full}}$ and $\\widehat{W}_{\\text{full}}$ are the estimator and optimal weight matrix derived from the full set of $k=3$ instruments. Under the null hypothesis, $J_{\\text{full}}$ follows an asymptotic $\\chi^2$ distribution with $k - m = 3 - 1 = 2$ degrees of freedom, where $m=1$ is the number of estimated parameters. A rejection suggests model misspecification.\n\n2.  **The D-test (or C-test) statistic**: $D = J_{\\text{full}} - J_{\\text{rest}}$. This statistic tests the validity of a specific subset of moment conditions—in this case, the one associated with the instrument $z_{2t}$. It compares the minimized GMM criterion from the full model, $J_{\\text{full}}$, with the minimized criterion from a restricted model, $J_{\\text{rest}}$. The restricted model uses only the subset of instruments assumed to be valid under the alternative, $\\{z_{1t}, z_{3t}\\}$. Accordingly, $J_{\\text{rest}}$ is computed using the two-step efficient GMM procedure with this smaller set of $k_{rest}=2$ instruments. Under the null hypothesis that the additional moment condition (from $z_{2t}$) is valid, the $D$ statistic has an asymptotic $\\chi^2$ distribution with $k - k_{rest} = 3 - 2 = 1$ degree of freedom. This test is often more powerful than the omnibus J-test for detecting failure of the specific moment condition being tested.\n\nThe simulation algorithm proceeds as follows for each parameter tuple $(n, \\delta, R)$:\n1.  Set the random number generator seed to $123456$ for reproducibility.\n2.  Determine the critical values for the tests from the $\\chi^2(2)$ and $\\chi^2(1)$ distributions at the $\\alpha = 0.05$ significance level.\n3.  Initialize rejection counters for both tests to zero.\n4.  Execute a loop for $R=500$ replications. In each replication:\n    a.  Generate a sample of size $n$ from the DGP using the specified value of $\\delta$ and $\\beta_0=1$.\n    b.  Using the full set of instruments $Z=(z_1, z_2, z_3)$, compute the two-step efficient GMM estimator $\\widehat{\\beta}_{\\text{full}}$ and the corresponding J-statistic, $J_{\\text{full}}$.\n    c.  Using the restricted set of instruments $Z_{\\text{rest}}=(z_1, z_3)$, compute the two-step efficient GMM estimator $\\widehat{\\beta}_{\\text{rest}}$ and the corresponding J-statistic, $J_{\\text{rest}}$.\n    d.  Calculate the D-statistic as $D = J_{\\text{full}} - J_{\\text{rest}}$.\n    e.  If $J_{\\text{full}}$ exceeds its critical value, increment the J-test rejection counter.\n    f.  If $D$ exceeds its critical value, increment the D-test rejection counter.\n5.  After the loop, compute the empirical rejection probability for each test by dividing its rejection count by the total number of replications, $R$.\n6.  The results from all four test cases are collected and reported in the specified format. In cases where $\\delta=0$, the rejection probability represents the empirical size of the test. When $\\delta \\neq 0$, it represents the empirical power.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef gmm_solver(y, x, Z):\n    \"\"\"\n    Computes the two-step efficient GMM estimator, J-statistic, and optimal weight matrix.\n\n    Args:\n        y (np.ndarray): Dependent variable, shape (n, 1).\n        x (np.ndarray): Endogenous regressor, shape (n, 1).\n        Z (np.ndarray): Matrix of instruments, shape (n, k).\n\n    Returns:\n        tuple: (J-statistic, GMM beta estimate). Returns (np.nan, np.nan) on failure.\n    \"\"\"\n    n, k = Z.shape\n    if n < k:\n        return np.nan, np.nan\n\n    # Step 1: First-step consistent estimation using 2SLS (W = (Z'Z)^-1)\n    try:\n        ZTZ = Z.T @ Z\n        if np.linalg.matrix_rank(ZTZ) < k:\n            return np.nan, np.nan\n        inv_ZTZ = np.linalg.inv(ZTZ)\n    except np.linalg.LinAlgError:\n        return np.nan, np.nan\n\n    xTZ = x.T @ Z\n    ZTx = Z.T @ x\n    ZTy = Z.T @ y\n\n    den_b1 = (xTZ @ inv_ZTZ @ ZTx)[0, 0]\n    if np.isclose(den_b1, 0):\n        return np.nan, np.nan\n    num_b1 = (xTZ @ inv_ZTZ @ ZTy)[0, 0]\n    beta1 = num_b1 / den_b1\n\n    # Step 2: Form optimal weighting matrix\n    u_hat = y - x * beta1\n    # S_hat = (1/n) * Sum(u_hat_t^2 * Z_t @ Z_t.T)\n    # This is equivalent to (1/n) * Z_with_residuals.T @ Z_with_residuals\n    S_hat = (Z * u_hat).T @ (Z * u_hat) / n\n\n    try:\n        if np.linalg.matrix_rank(S_hat) < k:\n            return np.nan, np.nan\n        W_hat = np.linalg.inv(S_hat)\n    except np.linalg.LinAlgError:\n        return np.nan, np.nan\n\n    # Step 3: Efficient GMM estimator\n    den_b2 = (xTZ @ W_hat @ ZTx)[0, 0]\n    if np.isclose(den_b2, 0):\n        return np.nan, np.nan\n    num_b2 = (xTZ @ W_hat @ ZTy)[0, 0]\n    beta_gmm = num_b2 / den_b2\n\n    # J-statistic calculation\n    g_bar = (ZTy - ZTx * beta_gmm) / n\n    J_stat = n * g_bar.T @ W_hat @ g_bar\n\n    return J_stat[0, 0], beta_gmm\n\ndef solve():\n    \"\"\"\n    Main function to run the Monte Carlo simulation for GMM tests.\n    \"\"\"\n    seed = 123456\n    rng = np.random.default_rng(seed)\n    alpha = 0.05\n\n    # Test cases: (n, delta, R)\n    test_cases = [\n        (400, 0.0, 500),\n        (400, 0.3, 500),\n        (400, 0.6, 500),\n        (120, 0.6, 500),\n    ]\n\n    # Chi-squared critical values\n    crit_val_j = chi2.ppf(1 - alpha, df=2)  # Full model: 3 instruments, 1 param -> df=2\n    crit_val_d = chi2.ppf(1 - alpha, df=1)  # D-test: 3-2=1 df\n\n    # True parameter value\n    beta_0 = 1.0\n    \n    final_results = []\n\n    for n, delta, R in test_cases:\n        j_reject_count = 0\n        d_reject_count = 0\n\n        for _ in range(R):\n            # 1. Generate data\n            z1 = rng.normal(size=(n, 1))\n            z2 = rng.normal(size=(n, 1))\n            z3 = rng.normal(size=(n, 1))\n            v = rng.normal(size=(n, 1))\n            e = rng.normal(size=(n, 1))\n            \n            Z_full = np.hstack([z1, z2, z3])\n            \n            x = z1 + 0.5 * z3 + v\n            u = delta * z2 + e\n            y = beta_0 * x + u\n\n            # 2. GMM Estimation for the full model\n            J_full, _ = gmm_solver(y, x, Z_full)\n            \n            # 3. GMM Estimation for the restricted model\n            Z_rest = Z_full[:, [0, 2]]\n            J_rest, _ = gmm_solver(y, x, Z_rest)\n\n            # Skip replication if GMM fails (e.g., singular matrix)\n            if np.isnan(J_full) or np.isnan(J_rest):\n                continue\n\n            # 4. Compute D-statistic\n            D = J_full - J_rest\n\n            # 5. Perform tests\n            if J_full > crit_val_j:\n                j_reject_count += 1\n            # Note: D can be negative in finite samples. The test is D > crit,\n            # so a negative value correctly results in non-rejection.\n            if D > crit_val_d:\n                d_reject_count += 1\n\n        # 6. Calculate empirical rejection probabilities\n        p_j = j_reject_count / R\n        p_d = d_reject_count / R\n        \n        final_results.extend([p_j, p_d])\n    \n    # Format and print the final output\n    formatted_results = [f\"{res:.3f}\" for res in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}