{
    "hands_on_practices": [
        {
            "introduction": "At the heart of the Metropolis-Hastings algorithm lies the acceptance probability, a clever mechanism that decides whether the simulation explores a new state or remains at its current position. This calculation is the engine that drives the sampler towards regions of high probability. This first exercise provides a direct, hands-on opportunity to compute this crucial value for a discrete target distribution, solidifying your understanding of the core mechanic of the algorithm .",
            "id": "1962612",
            "problem": "A statistician is implementing a Markov Chain Monte Carlo (MCMC) simulation to generate samples from a target probability distribution. The chosen target distribution is a Poisson distribution, which models the number of events, $k$, occurring in a fixed interval of time or space. The probability mass function for a Poisson distribution is given by:\n$$ P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\nwhere $\\lambda$ is the average rate of events. For this specific simulation, the parameter is set to $\\lambda = 5$.\n\nThe simulation uses the Metropolis-Hastings algorithm with a symmetric proposal distribution, meaning the probability of proposing a move from state $k_1$ to state $k_2$ is the same as proposing a move from $k_2$ to $k_1$.\n\nSuppose the current state of the chain is $k=5$. The algorithm then proposes a move to a new state, $k'=6$.\n\nCalculate the acceptance probability for this proposed move from $k=5$ to $k'=6$. Express your answer as a decimal rounded to three significant figures.",
            "solution": "In the Metropolis-Hastings algorithm with a symmetric proposal distribution, the acceptance probability for a proposed move from state $k$ to $k'$ is\n$$\n\\alpha = \\min\\left(1, \\frac{\\pi(k')}{\\pi(k)}\\right),\n$$\nwhere $\\pi(\\cdot)$ is the target probability mass function. For a Poisson distribution with parameter $\\lambda$, the PMF is\n$$\n\\pi(k) = \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!}.\n$$\nTherefore, the ratio simplifies as\n$$\n\\frac{\\pi(k')}{\\pi(k)} = \\frac{\\lambda^{k'} \\exp(-\\lambda) / k'!}{\\lambda^{k} \\exp(-\\lambda) / k!} = \\lambda^{k'-k} \\frac{k!}{k'!}.\n$$\nWith $k=5$, $k'=6$, and $\\lambda=5$, we have $k'-k=1$ and $6! = 6 \\cdot 5!$, so\n$$\n\\frac{\\pi(6)}{\\pi(5)} = \\lambda \\cdot \\frac{5!}{6!} = \\lambda \\cdot \\frac{1}{6} = \\frac{5}{6}.\n$$\nHence, the acceptance probability is\n$$\n\\alpha = \\min\\left(1, \\frac{5}{6}\\right) = \\frac{5}{6}.\n$$\nExpressed as a decimal rounded to three significant figures, this is $0.833$.",
            "answer": "$$\\boxed{0.833}$$"
        },
        {
            "introduction": "While a high acceptance rate in an MCMC simulation might seem desirable, it can sometimes be a red flag. It may indicate that the sampler is only taking very small, cautious steps, preventing it from exploring the full landscape of the target distribution, a property known as good \"mixing.\" This is especially problematic for multimodal distributions, where the sampler can become trapped in a local mode. This problem explores this common practical pitfall, forcing you to think critically about what constitutes a \"good\" MCMC chain beyond simple acceptance rates .",
            "id": "1962668",
            "problem": "A data scientist is analyzing the posterior probability distribution for a parameter $\\theta$ of a complex climate model. The analysis reveals that the posterior distribution, denoted as $p(\\theta)$, is bimodal, with two distinct peaks of high probability located at $\\theta_A$ and $\\theta_B$, separated by a wide region of very low probability. To explore this distribution and estimate properties like the posterior mean, the scientist employs the Metropolis-Hastings (M-H) algorithm.\n\nThe M-H sampler is initialized with a starting value $\\theta_0$ located within the high-probability region around the first peak, $\\theta_A$. A symmetric proposal distribution $q(\\theta' | \\theta) = \\mathcal{N}(\\theta' | \\theta, \\sigma^2)$ is used, where $\\mathcal{N}$ is a normal distribution centered at the current state $\\theta$ with a standard deviation $\\sigma$, which represents the proposal step size. The scientist, aiming for a high acceptance rate, chooses a very small value for $\\sigma$ relative to the distance between the two modes, $|\\theta_A - \\theta_B|$.\n\nAfter running the M-H sampler for a very large number of iterations, which of the following descriptions most accurately characterizes the expected outcome of this simulation?\n\nA. The sampler will efficiently find the global maximum of the posterior distribution $p(\\theta)$ and remain there, thus providing an excellent point estimate for the parameter.\n\nB. The acceptance rate for proposed states will be very low, causing the chain to remain near the initial state $\\theta_0$ and explore very little of the parameter space.\n\nC. The generated chain of samples will be highly autocorrelated, and its histogram will largely represent the shape of the mode around $\\theta_A$, while failing to discover the mode around $\\theta_B$.\n\nD. The samples will alternate between the two modes in a systematic fashion, jumping from the region of $\\theta_A$ to the region of $\\theta_B$ and back again with regular frequency.\n\nE. The states of the chain will be nearly independent of one another, indicating that the sampler has successfully converged to the true bimodal posterior distribution.",
            "solution": "We analyze the Metropolis-Hastings (M-H) sampler with a symmetric proposal and very small proposal scale relative to the distance between two separated modes of the posterior $p(\\theta)$ at $\\theta_{A}$ and $\\theta_{B}$.\n\nFor a symmetric proposal $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta' \\mid \\theta, \\sigma^{2})$, the Metropolis-Hastings acceptance probability is\n$$\na(\\theta \\rightarrow \\theta') = \\min\\left\\{1, \\frac{p(\\theta')}{p(\\theta)}\\right\\}.\n$$\nThe chain is initialized at $\\theta_{0}$ in the high-probability region around $\\theta_{A}$. Because $\\sigma$ is chosen to be very small compared to the separation $|\\theta_{A} - \\theta_{B}|$, proposed moves satisfy $|\\theta' - \\theta| = O(\\sigma)$ and thus remain very close to the current state. In a high-probability region near a mode, $p(\\theta')$ is close to $p(\\theta)$ for small steps, so\n$$\n\\frac{p(\\theta')}{p(\\theta)} \\approx 1,\n$$\nimplying that $a(\\theta \\rightarrow \\theta') \\approx 1$ and the local acceptance rate is high. Hence, option B (very low acceptance rate) is contradicted by the small-step, within-mode behavior.\n\nNext, consider transitions between the modes. The probability to directly propose a state near $\\theta_{B}$ from a current state near $\\theta_{A}$ under the Gaussian proposal is\n$$\nq(\\theta_{B} \\mid \\theta) \\propto \\exp\\left(-\\frac{|\\theta_{B} - \\theta|^{2}}{2 \\sigma^{2}}\\right).\n$$\nSince $\\sigma \\ll |\\theta_{A} - \\theta_{B}|$, this proposal probability is exponentially small in $|\\theta_{A} - \\theta_{B}|^{2} / \\sigma^{2}$. Therefore, direct jumps across the low-probability valley are essentially never proposed on practical time scales. Alternatively, crossing the valley via many small steps requires repeatedly proposing moves into regions where $p(\\theta') \\ll p(\\theta)$, for which\n$$\na(\\theta \\rightarrow \\theta') = \\min\\left\\{1, \\frac{p(\\theta')}{p(\\theta)}\\right\\} \\ll 1,\n$$\nso such steps are overwhelmingly rejected. Consequently, the chain becomes effectively trapped near $\\theta_{A}$ for a very long time, failing to discover $\\theta_{B}$ in practice.\n\nBecause moves are very local and the chain stays within the same mode, successive samples are highly autocorrelated. The empirical histogram thus reflects the local shape around $\\theta_{A}$ but does not capture the separated mode near $\\theta_{B}$. This rules out option E (independence and successful convergence) and option D (regular alternation between modes). Option A is incorrect because M-H is a sampler targeting $p(\\theta)$, not an optimizer; moreover, with small steps and bimodality, it neither efficiently finds nor remains at a global maximum.\n\nTherefore, the most accurate description is that the chain exhibits high autocorrelation, predominantly samples the neighborhood of $\\theta_{A}$, and fails to discover the second mode $\\theta_{B}$.\n\nThe correct choice is C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "The true power and artistry of applying the Metropolis-Hastings algorithm come to light when dealing with complex, multidimensional distributions. The efficiency of the sampler hinges on a well-designed proposal distribution that respects the \"geometry\" of the target. This final practice demonstrates this principle vividly by examining a bivariate distribution with highly correlated parameters. By comparing two different proposal strategies , you will see how a \"smarter\" proposal that moves along the high-probability ridge dramatically outperforms a naive one, a key insight for building effective samplers in real-world economic and financial models.",
            "id": "1962663",
            "problem": "A statistician is using the Metropolis-Hastings algorithm to sample from a bivariate target distribution on $\\mathbb{R}^2$ with a probability density function (PDF) $\\pi(x, y)$ that is proportional to\n$$\n\\pi(x, y) \\propto \\exp\\left(-\\frac{x^2 - 2\\rho xy + y^2}{2(1-\\rho^2)}\\right)\n$$\nwhere $\\rho$ is a constant correlation parameter such that $0  \\rho  1$.\n\nThe current state of the Markov chain is $(X_t, Y_t) = (c, c)$, where $c$ is a positive constant. Two different symmetric random-walk proposal schemes are considered to generate a candidate state $(X', Y')$.\n\n**Scheme 1:** A proposed state $(X'_1, Y'_1)$ is generated such that the step taken is $(X'_1 - X_t, Y'_1 - Y_t) = (\\epsilon, -\\epsilon)$ for some small positive constant $\\epsilon$. Thus, the proposed state is $(X'_1, Y'_1) = (c+\\epsilon, c-\\epsilon)$.\n\n**Scheme 2:** A proposed state $(X'_2, Y'_2)$ is generated such that the step taken is $(X'_2 - X_t, Y'_2 - Y_t) = (\\epsilon, \\epsilon)$, using the same constant $\\epsilon$. Thus, the proposed state is $(X'_2, Y'_2) = (c+\\epsilon, c+\\epsilon)$.\n\nLet $\\alpha_1$ be the Metropolis-Hastings acceptance probability for the proposal under Scheme 1, and let $\\alpha_2$ be the acceptance probability for the proposal under Scheme 2. Determine the ratio $R = \\frac{\\alpha_2}{\\alpha_1}$. Express your answer as a single closed-form analytic expression in terms of $c$, $\\epsilon$, and $\\rho$.",
            "solution": "For a symmetric random-walk Metropolis-Hastings proposal, the acceptance probability is $\\alpha=\\min\\left\\{1, \\frac{\\pi(x',y')}{\\pi(x,y)}\\right\\}$. Since the proposal is symmetric in both schemes, the Hastings ratio cancels and\n$$\n\\frac{\\pi(x',y')}{\\pi(c,c)}=\\exp\\left(E(x',y')-E(c,c)\\right),\n$$\nwhere the log-density is\n$$\nE(x,y)=-\\frac{x^{2}-2\\rho xy+y^{2}}{2(1-\\rho^{2})},\n$$\nand we define\n$$\nQ(x,y)=x^{2}-2\\rho xy+y^{2},\\quad E(x,y)=-\\frac{Q(x,y)}{2(1-\\rho^{2})}.\n$$\nThus\n$$\n\\ln\\left(\\frac{\\pi(x',y')}{\\pi(c,c)}\\right)=-\\frac{Q(x',y')-Q(c,c)}{2(1-\\rho^{2})}.\n$$\n\nAt the current state $(c,c)$,\n$Q(c,c)=c^{2}-2\\rho c^{2}+c^{2}=2c^{2}(1-\\rho)$.\n\nScheme 1: $(x',y')=(c+\\epsilon,c-\\epsilon)$. Compute\n$x'^{2}=c^{2}+2c\\epsilon+\\epsilon^{2}$, $y'^{2}=c^{2}-2c\\epsilon+\\epsilon^{2}$, $x'y'=c^{2}-\\epsilon^{2}$.\nHence\n$Q_{1}=x'^{2}-2\\rho x'y'+y'^{2}=(2c^{2}+2\\epsilon^{2})-2\\rho(c^{2}-\\epsilon^{2})=2c^{2}(1-\\rho)+2\\epsilon^{2}(1+\\rho)$.\nTherefore\n$Q_{1}-Q(c,c)=2\\epsilon^{2}(1+\\rho)$,\nand\n$$\n\\ln\\left(\\frac{\\pi(x'_{1},y'_{1})}{\\pi(c,c)}\\right)=-\\frac{2\\epsilon^{2}(1+\\rho)}{2(1-\\rho^{2})}=-\\frac{\\epsilon^{2}}{1-\\rho}.\n$$\nSince this is negative for $0\\rho1$, the acceptance probability is\n$\\alpha_{1}=\\exp\\left(-\\frac{\\epsilon^{2}}{1-\\rho}\\right)$.\n\nScheme 2: $(x',y')=(c+\\epsilon,c+\\epsilon)$. Compute\n$x'^{2}=c^{2}+2c\\epsilon+\\epsilon^{2}$, $y'^{2}=c^{2}+2c\\epsilon+\\epsilon^{2}$, $x'y'=c^{2}+2c\\epsilon+\\epsilon^{2}$.\nHence\n$Q_{2}=x'^{2}-2\\rho x'y'+y'^{2}=2(1-\\rho)(c^{2}+2c\\epsilon+\\epsilon^{2})$,\nso\n$Q_{2}-Q(c,c)=2(1-\\rho)(2c\\epsilon+\\epsilon^{2})$,\nand\n$$\n\\ln\\left(\\frac{\\pi(x'_{2},y'_{2})}{\\pi(c,c)}\\right)=-\\frac{2(1-\\rho)(2c\\epsilon+\\epsilon^{2})}{2(1-\\rho^{2})}=-\\frac{2c\\epsilon+\\epsilon^{2}}{1+\\rho}.\n$$\nThis is also negative for $c0$, $\\epsilon0$, $0\\rho1$, so\n$\\alpha_{2}=\\exp\\left(-\\frac{2c\\epsilon+\\epsilon^{2}}{1+\\rho}\\right)$.\n\nTherefore, the ratio $R=\\alpha_{2}/\\alpha_{1}$ is\n$$R=\\exp\\left(-\\frac{2c\\epsilon+\\epsilon^{2}}{1+\\rho}+\\frac{\\epsilon^{2}}{1-\\rho}\\right)\n=\\exp\\left(-\\frac{2c\\epsilon}{1+\\rho}+\\epsilon^{2}\\left[\\frac{1}{1-\\rho}-\\frac{1}{1+\\rho}\\right]\\right)\n=\\exp\\left(-\\frac{2c\\epsilon}{1+\\rho}+\\frac{2\\rho\\,\\epsilon^{2}}{1-\\rho^{2}}\\right).$$",
            "answer": "$$\\boxed{\\exp\\left(\\frac{2\\rho\\,\\epsilon^{2}}{1-\\rho^{2}}-\\frac{2c\\,\\epsilon}{1+\\rho}\\right)}$$"
        }
    ]
}