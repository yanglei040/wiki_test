{
    "hands_on_practices": [
        {
            "introduction": "“El Farol Bar”问题是基于主体的计算经济学中的一个经典模型，用于研究个体在资源有限环境下的协调困境。本练习通过引入结盟和通信机制，对原始模型进行了扩展，要求你模拟主体如何根据历史回报调整其决策倾向。通过这个实践，你将深入理解有界理性的主体如何通过简单的学习规则，自下而上地涌现出复杂的宏观动态，并探索协作对系统效率的影响。",
            "id": "2370498",
            "problem": "您需要实现并分析一个艾尔法罗酒吧问题（El Farol Bar problem）的基于代理人的模型，其中代理人可以被组织成联盟，这些联盟能以给定的成功内部沟通概率来协调其行动。该模型必须在固定的离散时期内进行模拟，并具有明确定义的代理人行为、联盟激活和基于收益的学习机制。所有随机性必须通过对伪随机数生成器进行显式播种来确保可复现。\n\n模型定义：\n- 共有 $N$ 个代理人，索引为 $i \\in \\{0,1,\\dots,N-1\\}$，以及一个容量为 $C \\in \\mathbb{N}$ 的酒吧。时间以离散时期 $t=0,1,\\dots,T-1$ 进行。\n- 每个代理人 $i$ 在每个时期选择一个行动 $a_i(t) \\in \\{0,1\\}$，其中 $a_i(t)=1$ 表示去酒吧，$a_i(t)=0$ 表示待在家里。设总参与人数为 $A(t)=\\sum_{i=0}^{N-1} a_i(t)$。\n- 代理人被外生地划分为不相交的联盟 $g \\in \\mathcal{G}$，这些联盟构成了代理人集合的完全划分。一个联盟 $g$ 的大小为 $|g| \\ge 1$。大小为 $|g|=1$ 的联盟是单体联盟。对于每个大小为 $|g| \\ge 2$ 的联盟 $g$，定义其配额 $q_g = \\left\\lfloor C \\cdot \\frac{|g|}{N} \\right\\rfloor$。对于大小为 $|g|=1$ 的单体联盟，从不执行协调，也不应用配额。\n- 每个代理人 $i$ 维护两个吸引力（倾向性）：去酒吧的吸引力 $A_i^{\\text{att}}(t)$ 和待在家的吸引力 $A_i^{\\text{stay}}(t)$，初始值均为 $A_i^{\\text{att}}(0)=0$ 和 $A_i^{\\text{stay}}(0)=0$。定义参数 $\\rho \\in (0,1)$（近期性）和 $\\lambda  0$（学习步长）。\n- 私人意图阶段：在每个时期 $t$ 开始时，每个代理人根据吸引力形成一个私人意图 $b_i(t) \\in \\{0,1\\}$：如果 $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$，则 $b_i(t)=1$；如果 $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$，则 $b_i(t)=0$；如果 $A_i^{\\text{att}}(t)=A_i^{\\text{stay}}(t)$，则 $b_i(t)$ 通过一次公平的伯努利抛硬币决定。为保证可复现性，平局情况和所有抛硬币都必须使用指定的伪随机数生成器和种子。\n- 联盟激活与协调：对于每个大小为 $|g|\\ge 2$ 的联盟 $g$，通过一次成功概率为 $p_{\\text{comm}} \\in [0,1]$ 的独立伯努利试验来确定 $g$ 在时期 $t$ 是否被激活。如果未激活，所有成员将私人意图作为最终行动。如果在时期 $t$ 被激活，联盟将观察其成员的私人意图向量 $\\{b_i(t): i \\in g\\}$，并应用以下向下调整规则来产生最终行动 $\\{a_i(t): i \\in g\\}$：\n  1. 令 $S_g(t)=\\sum_{i \\in g} b_i(t)$ 为意图去酒吧的成员数量。\n  2. 如果 $S_g(t) \\le q_g$，则对所有 $i \\in g$，$a_i(t)=b_i(t)$。\n  3. 如果 $S_g(t)  q_g$，则从子集 $\\{i \\in g: b_i(t)=1\\}$ 中，精确选择 $q_g$ 名成员去酒吧，选择的标准是优先选择 $\\Delta_i(t)=A_i^{\\text{att}}(t)-A_i^{\\text{stay}}(t)$ 值最大的成员；$\\Delta_i(t)$ 的平局情况通过优先选择较小的代理人索引来打破。为选中的 $q_g$ 名成员设置 $a_i(t)=1$，并为该子集中的其余成员设置 $a_i(t)=0$。对于 $b_i(t)=0$ 的代理人，设置 $a_i(t)=0$。\n- 对于任何大小为 $|g|=1$ 的单体联盟 $g$，其唯一成员 $i$ 的最终行动是 $a_i(t)=b_i(t)$。\n- 收益与学习：在所有联盟确定行动后，总参与人数 $A(t)=\\sum_i a_i(t)$ 得以实现。每个代理人的时期收益 $u_i(t)$ 为\n  $$u_i(t)=\\begin{cases}\n  1  \\text{若 } a_i(t)=1 \\text{ 且 } A(t) \\le C,\\\\\n  1  \\text{若 } a_i(t)=0 \\text{ 且 } A(t)  C,\\\\\n  0  \\text{其他情况。}\n  \\end{cases}$$\n  吸引力按如下方式更新：\n  $$A_i^{\\text{att}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{att}}(t) + \\lambda\\,u_i(t)  \\text{若 } a_i(t)=1,\\\\\n  (1-\\rho)\\,A_i^{\\text{att}}(t)  \\text{若 } a_i(t)=0,\n  \\end{cases}$$\n  $$A_i^{\\text{stay}}(t+1)=\\begin{cases}\n  (1-\\rho)\\,A_i^{\\text{stay}}(t) + \\lambda\\,u_i(t)  \\text{若 } a_i(t)=0,\\\\\n  (1-\\rho)\\,A_i^{\\text{stay}}(t)  \\text{若 } a_i(t)=1.\n  \\end{cases}$$\n\n衡量指标定义：\n- 对于给定的视界 $T$ 和尾部窗口长度 $H$（其中 $1 \\le H \\le T$），定义在时期 $t \\in \\{T-H, T-H+1, \\dots, T-1\\}$ 上的尾部平均值：\n  1. 平均参与人数差距 $\\bar{G} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( A(t) - C \\right)$。\n  2. 过度拥挤频率 $\\bar{O} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\mathbf{1}\\{A(t)  C\\}$，以 $[0,1]$ 范围内的小数表示。\n  3. 每时期平均协调反转比例 $\\bar{F} = \\frac{1}{H}\\sum_{t=T-H}^{T-1} \\left( \\frac{1}{N} \\sum_{i=0}^{N-1} \\mathbf{1}\\{a_i(t) \\ne b_i(t)\\} \\right)$，即因联盟协调导致最终行动与其私人意图不同的代理人的平均比例。\n\n随机性与可复现性：\n- 所有随机性，包括意图决策中的平局打破和联盟激活，都必须由一个伪随机数生成器产生，并为每个测试用例按指定方式播种。请将给定的种子精确地用作该测试用例中随机数生成器的初始化种子。\n\n测试套件：\n实现该模拟，并为以下四个参数集中的每一个计算 $(\\bar{G},\\bar{O},\\bar{F})$。在每种情况下，按指定顺序报告在最后 $H$ 个时期计算出的结果。\n\n- 情况1：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由包含51个全部为1的条目的列表给出（每个代理人都是一个单体），$p_{\\text{comm}}=0.5$，种子 $=42$。\n- 情况2：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由列表 $[3,3,3,3,3,3,3,30]$ 给出（按代理人索引递增的顺序），$p_{\\text{comm}}=1.0$，种子 $=43$。\n- 情况3：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由列表 $[3,3,3,3,3,3,3,30]$ 给出，$p_{\\text{comm}}=0.0$，种子 $=44$。\n- 情况4：$N=51$，$C=25$，$T=2000$，$H=500$，$\\rho=0.01$，$\\lambda=0.2$，联盟大小由一个包含十个等于5的条目后跟一个等于1的条目的列表给出（即 $[5,5,5,5,5,5,5,5,5,5,1]$），$p_{\\text{comm}}=0.8$，种子 $=45$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。每个测试用例的结果本身必须是一个包含三个浮点数 $[\\bar{G},\\bar{O},\\bar{F}]$ 的列表，顺序与上述测试用例相同。例如，整体输出格式必须是\n$[[g_1,o_1,f_1],[g_2,o_2,f_2],[g_3,o_3,f_3],[g_4,o_4,f_4]]$，\n其中每个 $g_k$、$o_k$ 和 $f_k$ 都以十进制数报告。",
            "solution": "该问题陈述已经过严格验证，并被确定为在基于代理人的计算经济学领域内是良构的、具有科学依据且内部一致的。所有参数、行为规则以及一个客观评估标准都得到了明确无误的规定。因此，我将继续提供一个完整的解决方案。\n\n该问题要求实现一个离散时间的基于代理人的仿真模型。系统状态由 $N$ 个代理人的吸引力定义，这些吸引力根据他们的行动和由此产生的收益在 $T$ 个时期内演化。该解决方案的核心是一个精确执行模型动态的计算算法。设计分为三个主要阶段：初始化、主仿真循环和最终测量。\n\n首先，一个初始化阶段建立仿真环境。这包括设置 $N$ 个代理人，并根据指定的联盟大小列表将他们划分为不相交的联盟 $\\mathcal{G}$。对于每个大小为 $|g| \\ge 2$ 的联盟 $g \\in \\mathcal{G}$，预先计算一个协调配额 $q_g = \\lfloor C \\cdot \\frac{|g|}{N} \\rfloor$，其中 $C$ 是酒吧的容量。每个代理人 $i$ 的去酒吧和待在家的吸引力都初始化为零，即 $A_i^{\\text{att}}(0)=0$ 和 $A_i^{\\text{stay}}(0)=0$。为每个测试用例指定一个伪随机数生成器的种子，以确保所有随机事件的可复现性。\n\n其次，主仿真循环从时间 $t=0$ 迭代到 $t=T-1$。每个时期 $t$ 包含一系列直接源于问题定义的步骤：\n\n1.  **私人意图形成**：每个代理人 $i$ 形成一个私人意图 $b_i(t) \\in \\{0,1\\}$，决定是去酒吧（1）还是待在家里（0）。这个决策基于对其当前吸引力的比较。如果 $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$，意图为 $b_i(t)=1$。如果 $A_i^{\\text{att}}(t)  A_i^{\\text{stay}}(t)$，意图为 $b_i(t)=0$。在平局的情况下，即 $A_i^{\\text{att}}(t) = A_i^{\\text{stay}}(t)$，意图 $b_i(t)$ 由使用已播种的随机数生成器进行的公平抛硬币决定。\n\n2.  **联盟协调**：确定最终行动 $a_i(t)$。对于单体联盟（$|g|=1$）中的代理人，其最终行动总是他们的私人意图，即 $a_i(t) = b_i(t)$。对于每个非单体联盟 $g$（$|g| \\ge 2$），通过一次成功概率为 $p_{\\text{comm}}$ 的伯努利试验来决定该联盟是否激活以进行协调。如果未激活，所有成员 $i \\in g$ 设置 $a_i(t) = b_i(t)$。如果激活，则将意图总和 $S_g(t) = \\sum_{i \\in g} b_i(t)$ 与联盟的配额 $q_g$ 进行比较。如果 $S_g(t) \\le q_g$，则无需协调，对所有 $i \\in g$ 都有 $a_i(t)=b_i(t)$。然而，如果 $S_g(t)  q_g$，则应用向下调整规则。在 $S_g(t)$ 名意图去酒吧的成员中，精确选择 $q_g$ 名去酒吧。选择标准是首先根据他们的吸引力差异 $\\Delta_i(t) = A_i^{\\text{att}}(t) - A_i^{\\text{stay}}(t)$ 按降序排序，然后按代理人索引 $i$ 的升序来打破平局。这 $q_g$ 名代理人的最终行动被设为 $a_i(t)=1$。其余 $S_g(t) - q_g$ 名意图去酒吧的成员的行动被“反转”为最终行动 $a_i(t)=0$。最初意图待在家里（$b_i(t)=0$）的代理人保持其行动 $a_i(t)=0$。\n\n3.  **收益与学习**：一旦所有最终行动确定，计算总参与人数 $A(t) = \\sum_{i=0}^{N-1} a_i(t)$。每个代理人 $i$ 因做出“正确”决策（去一个未过度拥挤的酒吧，或因酒吧过度拥挤而待在家里）而获得收益 $u_i(t)=1$，否则收益为 $u_i(t)=0$。随后，使用指定的 Roth-Erev 学习规则更新吸引力。对于选择行动 $a_i(t) \\in \\{0, 1\\}$ 的代理人 $i$，所选行动的吸引力通过 $A_i^{\\text{chosen}}(t+1) = (1-\\rho)A_i^{\\text{chosen}}(t) + \\lambda u_i(t)$ 更新，而未选行动的吸引力则通过 $A_i^{\\text{unchosen}}(t+1) = (1-\\rho)A_i^{\\text{unchosen}}(t)$ 更新。这里，$\\rho$ 是近期性参数，$\\lambda$ 是学习步长。\n\n第三，在模拟完成后，进行最终分析。所需的指标——平均参与人数差距 $\\bar{G}$、过度拥挤频率 $\\bar{O}$ 和平均协调反转比例 $\\bar{F}$——通过对它们各自的时间序列在模拟的最后 $H$ 个时期（从 $t=T-H$ 到 $t=T-1$）上取平均值来计算。该实现利用 NumPy 库提供的矢量化操作来提高涉及代理人状态数组的计算效率。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(N, C, T, H, rho, lambda_param, p_comm, coalition_sizes, seed):\n    \"\"\"\n    Implements and simulates the described agent-based El Farol Bar model with coalitions.\n    \"\"\"\n    # Initialize the pseudo-random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    # 1. Initialization\n    # Construct coalition structure: a list of lists of agent indices.\n    coalitions = []\n    # Store quotas for non-singleton coalitions, indexed by coalition index.\n    quotas = {}\n    agent_idx_counter = 0\n    for g_idx, size in enumerate(coalition_sizes):\n        members = list(range(agent_idx_counter, agent_idx_counter + size))\n        coalitions.append(members)\n        if size >= 2:\n            # Per the problem, quota is floor(C * |g| / N).\n            quotas[g_idx] = int(C * size / N)\n        agent_idx_counter += size\n\n    # Agent state variables initialized to 0.\n    attractions = np.zeros(N, dtype=float)\n    stays = np.zeros(N, dtype=float)\n\n    # Data structures for storing tail-end history for final metrics.\n    attendance_history = []\n    flips_history = []\n\n    # 2. Main Simulation Loop\n    for t in range(T):\n        # 2a. Private Intention Phase\n        deltas = attractions - stays\n        intentions = np.zeros(N, dtype=int)\n        intentions[deltas > 0] = 1\n        \n        # Resolve ties with a fair coin flip.\n        tie_indices = np.where(deltas == 0)[0]\n        if len(tie_indices) > 0:\n            intentions[tie_indices] = rng.integers(0, 2, size=len(tie_indices))\n        \n        # Final actions start as intentions; may be modified by coordination.\n        actions = intentions.copy()\n\n        # 2b. Coalition Coordination Phase\n        for g_idx, g_members in enumerate(coalitions):\n            # Singleton coalitions (|g|=1) do not coordinate.\n            if len(g_members)  2:\n                continue\n\n            # Bernoulli trial for coalition activation.\n            is_active = rng.random()  p_comm\n            if not is_active:\n                continue\n\n            # Active coalition coordination logic.\n            member_intentions = intentions[g_members]\n            S_g = np.sum(member_intentions)\n            q_g = quotas[g_idx]\n\n            if S_g > q_g:\n                # Identify members who intended to attend.\n                intending_member_local_indices = np.where(member_intentions == 1)[0]\n                global_intending_indices = [g_members[i] for i in intending_member_local_indices]\n                \n                # Create a list of candidates to be sorted.\n                candidates = []\n                for agent_idx in global_intending_indices:\n                    candidates.append((deltas[agent_idx], agent_idx))\n                \n                # Sort: 1st key descending delta, 2nd key ascending agent index.\n                candidates.sort(key=lambda x: (-x[0], x[1]))\n                \n                # The agents beyond the quota are 'flipped' to stay home.\n                for _, agent_idx in candidates[q_g:]:\n                    actions[agent_idx] = 0\n\n        # 2c. Payoff and Learning\n        A_t = np.sum(actions)\n        \n        payoffs = np.zeros(N, dtype=float)\n        actions_mask = (actions == 1) # Boolean mask for agents who attended.\n        \n        if A_t = C:  # Bar not crowded: attenders get payoff 1.\n            payoffs[actions_mask] = 1.0\n        else:  # Bar crowded: stayers get payoff 1.\n            payoffs[~actions_mask] = 1.0\n\n        # Update attractions for t+1 using vectorized operations.\n        # First, apply the recency discount to all.\n        attractions = (1 - rho) * attractions\n        stays = (1 - rho) * stays\n\n        # Then, add the reinforcement term for the chosen action.\n        attractions[actions_mask] += lambda_param * payoffs[actions_mask]\n        stays[~actions_mask] += lambda_param * payoffs[~actions_mask]\n\n        # 2d. Data Recording for Tail Averages\n        if t >= T - H:\n            attendance_history.append(A_t)\n            flips_history.append(np.sum(actions != intentions))\n\n    # 3. Final Metrics Calculation\n    attendance_history = np.array(attendance_history, dtype=float)\n    flips_history = np.array(flips_history, dtype=float)\n\n    mean_gap = np.mean(attendance_history - C)\n    overcrowding_freq = np.mean(attendance_history > C)\n    mean_flip_share = np.mean(flips_history / N)\n\n    return [mean_gap, overcrowding_freq, mean_flip_share]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: All agents are singletons.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2, \n         'coalition_sizes': [1] * 51, 'p_comm': 0.5, 'seed': 42},\n        # Case 2: Coalitions, full communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 1.0, 'seed': 43},\n        # Case 3: Coalitions, no communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [3,3,3,3,3,3,3,30], 'p_comm': 0.0, 'seed': 44},\n        # Case 4: Another coalition structure, partial communication.\n        {'N': 51, 'C': 25, 'T': 2000, 'H': 500, 'rho': 0.01, 'lambda_param': 0.2,\n         'coalition_sizes': [5]*10 + [1], 'p_comm': 0.8, 'seed': 45},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "生产性活动与“寻租”活动之间的权衡是经济学的核心议题之一。这个练习旨在模拟一个经济体，其中每个主体都需要决定如何分配其精力于创造价值的生产或争奪存量财富的寻租。主体们采用一种短视的收益梯度学习规则来动态调整策略，这让你能够亲手实现一个基于边际收益的适应性行为模型。通过探索不同参数（如再分配比例 $\\tau$）下的系统演化，你可以直观地看到制度设计如何影响宏观经济的生产效率和均衡状态。",
            "id": "2370544",
            "problem": "要求您设计并实现一个基于主体的计算经济学（ACE）模拟，该模拟涉及一个主体群体，这些主体重复选择如何在一个生产性活动和一个寻租活动之间分配单位努力。该模型必须由以下基本组成部分和规则构建。\n\n基本基础和核心定义：\n- 有 $N$ 个主体，由 $i \\in \\{1,\\dots,N\\}$ 索引。时间是离散的，由 $t \\in \\{0,1,2,\\dots,T-1\\}$ 索引。\n- 主体 $i$ 选择其单位努力的一部分比例 $x_{i,t} \\in [0,1]$ 分配给生产性活动（标记为 $K$），并将其余比例 $r_{i,t} = 1 - x_{i,t}$ 分配给寻租。\n- 每个主体在时期 $t$ 的生产性产出为 $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$，其中 $A  0$ 且 $\\alpha \\in (0,1)$；这是一个标准的凹性生产函数。总生产性产出为 $Y_t = \\sum_{j=1}^{N} y_{j,t}$。\n- 总产量的一部分比例 $\\tau \\in [0,1]$ 被征税并重新分配给寻租者。时期 $t$ 的寻租池为 $\\tau \\cdot Y_t$。令 $R_t = \\sum_{j=1}^{N} r_{j,t}$ 表示总寻租努力。如果 $R_t  0$，每个主体 $i$ 将获得等于 $\\tau \\cdot Y_t \\cdot \\frac{r_{i,t}}{R_t}$ 的寻租转移。如果 $R_t = 0$，所有主体的寻租转移均为零。\n- 主体 $i$ 的时期总收益为\n$$\n\\pi_{i,t} \\;=\\; (1 - \\tau)\\, y_{i,t} \\;+\\; \\tau \\, Y_t \\,\\frac{r_{i,t}}{\\max(R_t, \\varepsilon)},\n$$\n其中 $\\varepsilon  0$ 是一个数值正则化常数，仅用于在计算中避免除以零（见下文）。\n\n行为调整规则（短视收益梯度学习）：\n- 主体通过一个遵守边界的收益单调调整来更新其生产性分配：\n$$\nx_{i,t+1} \\;=\\; \\mathrm{proj}_{[0,1]}\\!\\Big(x_{i,t} \\;+\\; \\eta \\cdot \\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} \\cdot x_{i,t}\\,\\big(1-x_{i,t}\\big)\\Big),\n$$\n其中 $\\eta  0$ 是一个学习率参数，$\\mathrm{proj}_{[0,1]}(\\cdot)$ 表示到 $[0,1]$ 上的投影，而 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ 是 $x_{i,t}$ 对 $\\pi_{i,t}$ 的时期内边际效应。乘法因子 $x_{i,t}(1-x_{i,t})$ 确保更新量在边界处消失。第二个正则化常数 $\\delta_x  0$ 必须仅在导数计算中使用，以避免在 $x_{i,t} = 0$ 处的奇点，方法是在任何 $x_{i,t}$ 出现在指数小于 $1$ 的幂次内部时，将其替换为 $\\max(x_{i,t}, \\delta_x)$。\n\n推导目标和实现细节：\n- 从上述定义出发，通过将 $x_{i,t}$ 视为在时间 $t$ 主体 $i$ 的唯一选择变量（保持其他主体的选择固定），计算导数 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$。使用恒等式 $y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$，$Y_t = \\sum_{j=1}^{N} y_{j,t}$，$r_{i,t} = 1 - x_{i,t}$ 和 $R_t = \\sum_{j=1}^{N} r_{j,t}$。您的实现必须在所有除以 $R_t$ 的运算中使用正则化分母 $\\max(R_t,\\varepsilon)$，并且必须在涉及 $x_{i,t}^{\\alpha - 1}$ 的项内部将 $x_{i,t}$ 替换为 $\\max(x_{i,t},\\delta_x)$，以避免出现未定义的值。\n- 对所有主体 $i$，使用相同的初始分配 $x_{i,0} = x_0 \\in [0,1]$ 进行初始化。\n\n数值单位和报告：\n- 所有作为比例的参数（如 $\\tau$、$x_{i,t}$ 和 $x_0$）必须表示为 $[0,1]$ 范围内的小数，而不是百分比。\n- 本问题不涉及角度。\n- 本问题不涉及物理单位。\n\n任务：\n- 实现一个完整的、可运行的程序，为下面的测试套件中的每个参数集模拟系统 $T$ 个时期。对于每个参数集，返回一个浮点数，该数等于经过 $T$ 次更新后的群体平均值 $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$（即最终时期的平均生产性分配）。每个报告的数字必须四舍五入到小数点后恰好 $6$ 位。\n\n测试套件（五个参数集）：\n- 测试 $1$ (正常路径): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.3, 0.05, 1000, 0.5, 10^{-8}, 10^{-8})$。\n- 测试 $2$ (无再分配边界): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.0, 0.05, 500, 0.2, 10^{-8}, 10^{-8})$。\n- 测试 $3$ (高再分配边缘): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.5, 0.9, 0.02, 1000, 0.8, 10^{-8}, 10^{-8})$。\n- 测试 $4$ (低生产率情况): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 0.2, 0.5, 0.5, 0.05, 1000, 0.5, 10^{-8}, 10^{-8})$。\n- 测试 $5$ (近线性生产): $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x) = (200, 1.0, 0.95, 0.4, 0.02, 1500, 0.5, 10^{-8}, 10^{-8})$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个数字格式化为小数点后恰好 $6$ 位，并按上述测试的顺序排列。例如：$[0.123456,0.654321,0.500000,0.750000,0.333333]$。",
            "solution": "用户提供了一个问题陈述，描述了一个关于生产和寻租的基于主体的计算经济学（ACE）模型。任务是验证此模型，如果有效，则推导模拟所需的方程，实现模拟，并报告给定测试用例集的结果。\n\n### 第1步：提取已知条件\n\n-   **主体与时间**：$N$ 个主体，由 $i \\in \\{1,\\dots,N\\}$ 索引。离散时间 $t \\in \\{0,1,2,\\dots,T-1\\}$。\n-   **选择变量**：主体 $i$ 选择 $x_{i,t} \\in [0,1]$（用于生产的努力）。\n-   **寻租努力**：$r_{i,t} = 1 - x_{i,t}$。\n-   **生产函数**：$y_{i,t} = A \\cdot x_{i,t}^{\\alpha}$，参数 $A  0$ 且 $\\alpha \\in (0,1)$。\n-   **总产量**：$Y_t = \\sum_{j=1}^{N} y_{j,t}$。\n-   **寻租池**：$\\tau \\cdot Y_t$，其中 $\\tau \\in [0,1]$ 是税收/再分配率。\n-   **总寻租努力**：$R_t = \\sum_{j=1}^{N} r_{j,t}$。\n-   **寻租收益**：如果 $R_t  0$，则为 $\\tau \\cdot Y_t \\cdot \\frac{r_{i,t}}{R_t}$；如果 $R_t = 0$，则为 $0$。\n-   **总收益函数**：$\\pi_{i,t} = (1 - \\tau)\\, y_{i,t} \\;+\\; \\tau \\, Y_t \\,\\frac{r_{i,t}}{\\max(R_t, \\varepsilon)}$，其中正则化常数 $\\varepsilon  0$。\n-   **行为更新规则**：$x_{i,t+1} = \\mathrm{proj}_{[0,1]}\\!\\Big(x_{i,t} \\;+\\; \\eta \\cdot \\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} \\cdot x_{i,t}\\,\\big(1-x_{i,t}\\big)\\Big)$，其中 $\\eta  0$ 是学习率。\n-   **导数正则化**：在求导计算中，当 $x_{i,t}^{\\alpha-1}$ 出现时，用 $\\max(x_{i,t}, \\delta_x)$ 替换 $x_{i,t}$，其中 $\\delta_x  0$。\n-   **初始条件**：对所有 $i \\in \\{1, \\dots, N\\}$，$x_{i,0} = x_0$。\n-   **任务**：对每个参数集，模拟 $T$ 个时期并报告最终的平均生产性分配 $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$，四舍五入到小数点后6位。\n-   **测试套件**：提供了五个参数集 $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x)$。\n\n### 第2步：使用提取的已知条件进行验证\n\n根据验证标准对问题陈述进行严格评估。\n\n-   **科学依据**：该模型是竞争或寻租博弈的标准表示，在公共选择理论和计算经济学中被广泛使用。生产函数是标准的柯布-道格拉斯形式。学习规则是收益单调动态（类似复制动态）的一种常见形式。作为一个经济权衡的程式化表示，该模型在科学上是合理的。\n-   **适定性**：该问题对于模拟是适定的。所有参数、函数和初始条件都已明确定义。更新规则是确定性的。使用投影 $\\mathrm{proj}_{[0,1]}(\\cdot)$ 和正则化常数 $\\varepsilon$、$\\delta_x$ 确保了模拟保持在有效的状态空间内，并避免了如除以零或非实数等数值奇点。\n-   **客观性**：语言是形式化、精确的，并且没有主观或模糊的术语。\n\n该问题没有表现出任何诸如科学不合理、不可形式化、不完整性、矛盾或不可行性等缺陷。它是在指定领域内一个定义明确的计算任务。\n\n### 第3步：结论和行动\n\n问题是**有效的**。将开发一个解决方案。\n\n### 收益梯度的推导\n\n为了实现行为更新规则，我们必须首先计算主体 $i$ 的收益 $\\pi_{i,t}$ 关于其自身选择 $x_{i,t}$ 的偏导数，同时保持所有其他主体（$j \\neq i$ 的 $x_{j,t}$）的选择不变。\n\n主体 $i$ 在时间 $t$ 的收益函数为：\n$$\n\\pi_{i,t} = (1 - \\tau)\\, y_{i,t} + \\tau \\, Y_t \\,\\frac{r_{i,t}}{R_t}\n$$\n我们将逐项对 $x_{i,t}$ 求导。我们有以下关系及其关于 $x_{i,t}$ 的导数：\n-   $y_{i,t} = A x_{i,t}^{\\alpha} \\implies \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = A \\alpha x_{i,t}^{\\alpha-1}$\n-   $r_{i,t} = 1 - x_{i,t} \\implies \\frac{\\partial r_{i,t}}{\\partial x_{i,t}} = -1$\n-   $Y_t = y_{i,t} + \\sum_{j \\neq i} y_{j,t} \\implies \\frac{\\partial Y_t}{\\partial x_{i,t}} = \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = A \\alpha x_{i,t}^{\\alpha-1}$\n-   $R_t = r_{i,t} + \\sum_{j \\neq i} r_{j,t} \\implies \\frac{\\partial R_t}{\\partial x_{i,t}} = \\frac{\\partial r_{i,t}}{\\partial x_{i,t}} = -1$\n\n收益函数第一项的导数很简单：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left [ (1 - \\tau) y_{i,t} \\right ] = (1 - \\tau) \\frac{\\partial y_{i,t}}{\\partial x_{i,t}} = (1 - \\tau) A \\alpha x_{i,t}^{\\alpha-1}\n$$\n\n对于第二项，我们对 $\\tau \\left( Y_t \\cdot \\frac{r_{i,t}}{R_t} \\right)$ 使用乘法法则求导：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( Y_t \\frac{r_{i,t}}{R_t} \\right) = \\left( \\frac{\\partial Y_t}{\\partial x_{i,t}} \\right) \\left( \\frac{r_{i,t}}{R_t} \\right) + Y_t \\left( \\frac{\\partial}{\\partial x_{i,t}} \\frac{r_{i,t}}{R_t} \\right)\n$$\n我们需要份额项 $\\frac{r_{i,t}}{R_t}$ 的导数，为此我们使用商法则：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( \\frac{r_{i,t}}{R_t} \\right) = \\frac{(\\frac{\\partial r_{i,t}}{\\partial x_{i,t}})R_t - r_{i,t}(\\frac{\\partial R_t}{\\partial x_{i,t}})}{R_t^2} = \\frac{(-1)R_t - r_{i,t}(-1)}{R_t^2} = \\frac{-R_t + r_{i,t}}{R_t^2} = -\\frac{R_t - r_{i,t}}{R_t^2}\n$$\n令 $R_{t,-i} = R_t - r_{i,t} = \\sum_{j \\neq i} r_{j,t}$ 为所有其他主体的总寻租努力。则份额项的导数为 $-\\frac{R_{t,-i}}{R_t^2}$。\n\n将此代回乘法法则表达式中：\n$$\n\\frac{\\partial}{\\partial x_{i,t}} \\left( Y_t \\frac{r_{i,t}}{R_t} \\right) = \\left( A \\alpha x_{i,t}^{\\alpha-1} \\right) \\left( \\frac{r_{i,t}}{R_t} \\right) + Y_t \\left( -\\frac{R_{t,-i}}{R_t^2} \\right)\n$$\n\n合并所有部分，收益函数的完整导数为：\n$$\n\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}} = (1 - \\tau) A \\alpha x_{i,t}^{\\alpha-1} + \\tau \\left[ A \\alpha x_{i,t}^{\\alpha-1} \\frac{r_{i,t}}{R_t} - \\frac{Y_t R_{t,-i}}{R_t^2} \\right]\n$$\n这个表达式给出了主体 $i$ 将单位努力从寻租转向生产的边际回报。\n\n### 实现计划\n\n模拟使用 Python 和 `numpy` 库实现，以进行高效的向量计算，这对于在多个时间步上处理 $N=200$ 个主体的群体至关重要。\n\n1.  **初始化**：对于每个测试用例，设置参数 $(N, A, \\alpha, \\tau, \\eta, T, x_0, \\varepsilon, \\delta_x)$。初始化一个大小为 $N$ 的 `numpy` 数组 `x`，所有元素设置为 $x_0$。\n\n2.  **模拟循环**：模拟进行 $T$ 个时间步。在每个步骤 $t$ 中：\n    a.  计算当前时期的值：寻租努力向量 `r` 计算为 $1-x$。通过对相应的主体级向量求和，计算总寻租努力 `R_t` 和总产量 `Y_t`。\n    b.  应用正则化：根据问题陈述，计算正则化分母 $R_{t,\\text{reg}} = \\max(R_t, \\varepsilon)$。为了进行导数计算，将选择变量正则化为 $x_{\\text{reg}} = \\max(x, \\delta_x)$，以防止当 $x=0$ 时 $x^{\\alpha-1}$ 中出现未定义值。\n    c.  计算梯度：使用向量化操作实现 $\\frac{\\partial \\pi_{i,t}}{\\partial x_{i,t}}$ 的推导公式。所有项，包括边际生产率 $A \\alpha x_{\\text{reg}}^{\\alpha-1}$ 和外部性项 $R_{t,-i} = R_t - r_i$，都作为 `numpy` 数组计算，从而得到一个梯度向量 `grad_pi`。\n    d.  更新状态：根据更新规则计算生产性分配的变化量 $\\Delta x_t$：$\\Delta x_t = \\eta \\cdot \\text{grad\\_pi} \\cdot x \\cdot (1-x)$。状态向量更新为：$x_{t+1} = x_t + \\Delta x_t$。\n    e.  投影：使用 `numpy.clip(x, 0.0, 1.0)` 将新的状态向量 $x_{t+1}$ 投影到有效区间 $[0,1]$ 上，这确保了对所有主体都有 $x_{i,t+1} \\in [0,1]$。\n\n3.  **结果提取**：经过 $T$ 次迭代后，计算最终的群体平均生产努力 $\\frac{1}{N}\\sum_{i=1}^{N} x_{i,T}$。\n\n4.  **格式化和输出**：每个测试用例的结果被格式化为小数点后 $6$ 位。所有结果被收集并以指定格式打印：`[result1,result2,...]`。\n\n此设计直接实现了理论模型，并遵守所有指定的计算和格式化要求。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ACE simulation for all test cases and print results.\n    \"\"\"\n\n    # Test suite (N, A, alpha, tau, eta, T, x0, epsilon, delta_x)\n    test_cases = [\n        (200, 1.0, 0.5, 0.3, 0.05, 1000, 0.5, 1e-8, 1e-8),\n        (200, 1.0, 0.5, 0.0, 0.05, 500, 0.2, 1e-8, 1e-8),\n        (200, 1.0, 0.5, 0.9, 0.02, 1000, 0.8, 1e-8, 1e-8),\n        (200, 0.2, 0.5, 0.5, 0.05, 1000, 0.5, 1e-8, 1e-8),\n        (200, 1.0, 0.95, 0.4, 0.02, 1500, 0.5, 1e-8, 1e-8),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, A, alpha, tau, eta, T, x0, epsilon, delta_x = case\n\n        # Initialize the state vector for productive allocation\n        x = np.full(N, x0, dtype=np.float64)\n\n        # Main simulation loop for T periods\n        for _ in range(T):\n            # 1. Calculate current period aggregate variables\n            r = 1.0 - x\n            R_t = np.sum(r)\n            \n            # Using x directly for production as per y = A*x^alpha\n            y = A * np.power(x, alpha)\n            Y_t = np.sum(y)\n\n            # 2. Implement regularization for derivative calculation\n            R_t_reg = np.maximum(R_t, epsilon)\n            x_reg = np.maximum(x, delta_x)\n\n            # 3. Calculate the payoff gradient for each agent (vectorized)\n            # Marginal productivity: d(y_i)/d(x_i)\n            marginal_y = A * alpha * np.power(x_reg, alpha - 1)\n            \n            # Rent-seeking effort of all other agents: R_t - r_i\n            R_t_minus_i = R_t - r\n\n            # Term 1: Marginal gain from private production\n            grad_term1 = (1.0 - tau) * marginal_y\n\n            # Term 2: Marginal effect through rent-seeking payoff\n            # Sub-term 2a: Effect of increased total production on the rent pool\n            grad_term2a = tau * marginal_y * (r / R_t_reg)\n            # Sub-term 2b: Effect of changing own rent-seeking share\n            grad_term2b = -tau * Y_t * R_t_minus_i / (R_t_reg**2)\n\n            grad_pi = grad_term1 + grad_term2a + grad_term2b\n\n            # 4. Update the state vector based on the learning rule\n            update_term = eta * grad_pi * x * (1.0 - x)\n            x = x + update_term\n\n            # 5. Project the state vector back onto [0, 1]\n            x = np.clip(x, 0.0, 1.0)\n        \n        # After T periods, calculate the average productive allocation\n        final_avg_x = np.mean(x)\n        results.append(f\"{final_avg_x:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "预测市场是汇集分散信息以预测未来事件的强大工具，但其有效性面临着错误信息和恶意操纵的挑战。本练习将带你构建一个基于对数市场计分规则（LMSR）的预测市场模型，其中的主体是根据私有信号更新信念的贝叶斯决策者。此模拟的核心挑战是引入了“虚假信息传播者”这一角色，他们会故意交易错误的信息。通过这个实践，你将探索信息市场的运作机制、信念更新过程，并评估不同程度的虚假信息对市场预测准确性的影响。",
            "id": "2370584",
            "problem": "您的任务是设计并实现一个基于主体的计算经济学（Agent-Based Computational Economics, ACE）模拟，该模拟针对一个采用对数市场计分规则（Logarithmic Market Scoring Rule, LMSR）做市商的二元结果预测市场。一系列短视的（myopic）主体相继到达。每个主体观察到一个关于事件的私人二元信号，并在单次交易预算的约束下与做市商进行交易。每个主体有固定的概率故意反转其私人信号再进行交易，以此传播错误信息。\n\n基本原理：\n- 基于主体的计算经济学（ACE）中的主体与信号：事件结果是一个二元随机变量 $Y \\in \\{0,1\\}$，其真实概率为 $\\theta = \\mathbb{P}(Y=1)$，此概率对主体是未知的。在给定 $Y$ 的条件下，每个主体观察到一个独立的私人信号 $s \\in \\{0,1\\}$，使得 $\\mathbb{P}(s=1 \\mid Y=1) = \\alpha$ 且 $\\mathbb{P}(s=1 \\mid Y=0) = 1-\\alpha$，其中 $\\alpha \\in (0,1)$ 是一个已知的信号准确度参数。在给定 $Y$ 的情况下，各个主体的信号是条件独立的。\n- 贝叶斯更新：如果先验信念为 $p = \\mathbb{P}(Y=1)$，那么在观察到 $s \\in \\{0,1\\}$ 后，根据贝叶斯法则，后验信念 $b$ 为：\n  - 如果 $s=1$：$b = \\dfrac{\\alpha p}{\\alpha p + (1-\\alpha)(1-p)}$。\n  - 如果 $s=0$：$b = \\dfrac{(1-\\alpha) p}{(1-\\alpha) p + \\alpha (1-p)}$。\n- 对数市场计分规则（LMSR）：做市商维护一个份额向量 $\\mathbf{q} = (q_0,q_1)$ 和一个流动性参数 $b_L  0$。成本函数为\n  $$C(\\mathbf{q}) = b_L \\log\\left(\\exp\\left(\\frac{q_0}{b_L}\\right) + \\exp\\left(\\frac{q_1}{b_L}\\right)\\right)。$$\n  结果1的瞬时价格是梯度分量，\n  $$p = \\frac{\\partial C}{\\partial q_1} = \\frac{\\exp\\left(\\frac{q_1}{b_L}\\right)}{\\exp\\left(\\frac{q_0}{b_L}\\right) + \\exp\\left(\\frac{q_1}{b_L}\\right)}。$$\n  在二元情况下，对数几率满足\n  $$\\log\\left(\\frac{p}{1-p}\\right) = \\frac{q_1 - q_0}{b_L}。$$\n  仅通过购买“是”（结果1）的份额来将价格从 $p_{\\text{old}}$ 移动到 $p_{\\text{new}}$，对应于将 $q_1$ 增加 $\\Delta = b_L\\left(\\log\\left(\\frac{p_{\\text{new}}}{1-p_{\\text{new}}}\\right) - \\log\\left(\\frac{p_{\\text{old}}}{1-p_{\\text{old}}}\\right)\\right)$。当 $p_{\\text{new}}  p_{\\text{old}}$ 时，仅购买“否”（结果0）的份额对应于将 $q_0$ 增加一个类似的正常数。任何此类交易的货币成本是成本函数的差值，$\\Delta C = C(\\mathbf{q}_{\\text{new}}) - C(\\mathbf{q}_{\\text{old}})$。\n\n模拟环境：\n- 初始化：LMSR 以价格 $p_0 = \\pi_0 \\in (0,1)$ 开始。这可以通过设置初始份额向量为任何满足 $(q_{1,0}-q_{0,0})/b_L = \\log\\left(\\frac{\\pi_0}{1-\\pi_0}\\right)$ 的 $\\mathbf{q}_0$ 来实现。为具体起见，您可以取 $q_{0,0} = 0$ 和 $q_{1,0} = b_L \\log\\left(\\frac{\\pi_0}{1-\\pi_0}\\right)$。\n- 到达与行为：一个包含 $T$ 个主体的序列到达。在交易前，每个主体观察当前市场价格 $p$，并将其视为先验概率 $\\mathbb{P}(Y=1)$。然后，该主体从上述信号过程中抽取一个私人信号 $s \\in \\{0,1\\}$。主体有 $p_{\\text{fake}} \\in [0,1]$ 的概率故意传播错误信息，即使用 $s' = 1-s$ 进行贝叶斯更新。否则，他们使用真实的信号 $s' = s$。主体的后验信念 $b$ 是通过贝叶斯法则，从 $p$ 和 $s'$ 计算得出的。\n- 有预算的短视交易：每个主体都是风险中性且短视的。他们试图通过与 LMSR 做市商交易，将市场价格 $p$ 推向其信念 $b$，但他们面临严格的单次交易预算 $w  0$。主体选择能将价格向 $b$ 移动的最大可行幅度，条件是所支付的 LMSR 成本不超过 $w$。如果将价格完全移动到 $b$ 的成本小于或等于 $w$，则主体将价格精确移动到 $b$。否则，主体部分地向 $b$ 移动，选择位于 $p$ 和 $b$ 之间连线上的唯一价格，使得 LMSR 成本差等于 $w$。您必须通过沿一维路径（当 $b \\ge p$ 时仅购买“是”，或当 $b  p$ 时仅购买“否”）精确计算此部分移动，并在必要时对成本差使用数值求根方法。\n- 实现与评估：真实结果 $Y$ 在开始时从 $\\text{Bernoulli}(\\theta)$ 分布中抽取一次，并仅在最后揭晓。评估指标是最终市场概率的布莱尔分数（Brier score），定义为 $(p_T - Y)^2$，以十进制数表示。\n\n随机性与可复现性：\n- 所有随机性必须使用伪随机数生成器生成，并为每个测试用例设置固定的种子，以确保可复现性。结果 $Y$ 和所有主体的信号都应使用该种子生成器抽取。没有其他随机源。\n\n数值细节与约束：\n- 对 LMSR 成本和价格使用稳定的数值计算（例如，使用稳定的 log-sum-exp 计算）。将任何概率参数限制在 $(\\varepsilon,1-\\varepsilon)$ 范围内，其中 $\\varepsilon = 10^{-12}$，以避免除以零或对零取对数。角度单位不适用。此问题中没有物理单位。所有概率和准确度参数必须以 $[0,1]$ 区间内的小数形式给出。\n- 输出格式：对于每个测试用例，在最终输出行上产生两个浮点数：最终价格 $p_T$ 和布莱尔分数 $(p_T - Y)^2$，每个都四舍五入到 $6$ 位小数。将所有测试用例的结果汇总到一个单行的扁平化 Python 风格列表中，例如 $[p_1,\\text{Brier}_1,p_2,\\text{Brier}_2,\\dots]$。\n\n测试套件：\n实现您的程序，以完全按此顺序运行以下四个测试用例。对于每个用例，使用提供的种子初始化伪随机数生成器，并按规定格式打印结果。\n\n- 案例A（理想情况，无错误信息）：\n  - 种子：$123$\n  - 主体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个主体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.7$\n  - 错误信息概率：$p_{\\text{fake}} = 0.0$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n- 案例B（混合行为）：\n  - 种子：$456$\n  - 主体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个主体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.7$\n  - 错误信息概率：$p_{\\text{fake}} = 0.5$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n- 案例C（所有主体都提供错误信息）：\n  - 种子：$789$\n  - 主体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个主体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.7$\n  - 错误信息概率：$p_{\\text{fake}} = 1.0$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n- 案例D（无信息量信号边界情况）：\n  - 种子：$321$\n  - 主体数量：$T = 200$\n  - 流动性：$b_L = 10.0$\n  - 单个主体预算：$w = 1.0$\n  - 信号准确度：$\\alpha = 0.5$\n  - 错误信息概率：$p_{\\text{fake}} = 0.7$\n  - 真实事件概率：$\\theta = 0.6$\n  - 初始先验价格：$\\pi_0 = 0.5$\n\n您的程序应生成单行输出，其中包含一个以逗号分隔并用方括号括起来的结果列表（例如，$[0.512345,0.237890,0.498765,0.251234,0.476543,0.226789,0.500000,0.250000]$）。将每个浮点数四舍五入到 $6$ 位小数。不应打印任何其他文本。",
            "solution": "该问题要求实现一个二元预测市场的基于主体的计算经济学（ACE）模拟。该市场由一个对数市场计分规则（LMSR）自动做市商运营。一系列主体与该市场进行交互。该问题在科学上是良定的，所有参数、主体行为和市场机制都已明确定义。它构成了计算金融和计算经济学中的一个标准模拟任务。我将首先详细阐述数学模型，然后概述模拟算法，从而构建解决方案。\n\nLMSR 市场的状态由两个互斥结果 $Y=0$ 和 $Y=1$ 的已发行份额数量 $\\mathbf{q} = (q_0, q_1)$ 定义。结果1的市场价格，记为 $p$，由 LMSR 成本函数 $C(\\mathbf{q})$ 的梯度导出。二元结果 LMSR 的一个关键特性是，价格仅是份额数量之差 $q_1 - q_0$ 和流动性参数 $b_L$ 的函数：\n$$ \\log\\left(\\frac{p}{1-p}\\right) = \\frac{q_1 - q_0}{b_L} $$\n这意味着就价格而言，整个市场状态可以由单个变量，即价格 $p$ 本身来捕捉。我们无需单独追踪 $q_0$ 和 $q_1$。模拟从初始价格 $p_0 = \\pi_0$ 开始。\n\n主体与市场的交互是一个两步过程：信念形成和交易。\n首先，一个主体在市场价格为 $p_k$ 时到达，将此价格视为其先验概率，即 $\\mathbb{P}(Y=1) = p_k$。他观察到一个私人信号 $s \\in \\{0,1\\}$，该信号与真实（但未知）的结果 $Y$ 相关。信号的准确度由 $\\alpha = \\mathbb{P}(s=1 \\mid Y=1) = \\mathbb{P}(s=0 \\mid Y=0)$ 给出。主体有指定的概率 $p_{\\text{fake}}$ 充当信息误导者，并使用反转的信号 $s' = 1-s$ 进行信念更新。否则，使用真实信号 $s' = s$。主体使用贝叶斯法则，通过 $s'$ 计算其后验信念 $b$：\n- 如果 $s'=1$：$b = \\dfrac{\\alpha p_k}{\\alpha p_k + (1-\\alpha)(1-p_k)}$\n- 如果 $s'=0$：$b = \\dfrac{(1-\\alpha) p_k}{(1-\\alpha) p_k + \\alpha(1-p_k)}$\n\n其次，短视的、风险中性的主体进行交易，以将市场价格从 $p_k$ 移动到其后验信念 $b$。该交易受预算约束 $w$ 的限制。将价格从 $p_{\\text{old}}$ 变动到 $p_{\\text{new}}$ 的成本可以从 LMSR 成本函数 $C(\\mathbf{q}) = b_L \\log\\left(\\exp(q_0/b_L) + \\exp(q_1/b_L)\\right)$ 导出。\n一个增加价格的交易（即 $b  p_k$）涉及购买结果1的份额，这只改变 $q_1$。将价格从 $p_k$ 移动到 $p_{\\text{new}}$ 的成本为 $\\Delta C = b_L (\\log(1-p_k) - \\log(1-p_{\\text{new}}))$。\n一个降低价格的交易（即 $b  p_k$）涉及购买结果0的份额，这只改变 $q_0$。成本为 $\\Delta C = b_L (\\log(p_k) - \\log(p_{\\text{new}}))$。\n这些以价格表示的简化成本公式，避免了追踪 $q_0$ 和 $q_1$ 的需要，并且在价格远离 $0$ 和 $1$ 的情况下是数值稳定的。\n\n主体希望将价格移动到 $b$。\n如果 $b  p_k$，完全移动到 $b$ 的成本为 $\\Delta C_{\\text{full}} = b_L (\\log(1-p_k) - \\log(1-b))$。\n- 如果 $\\Delta C_{\\text{full}} \\le w$，主体进行完整交易，新价格为 $p_{k+1} = b$。\n- 如果 $\\Delta C_{\\text{full}}  w$，主体将花费其全部预算 $w$。我们必须找到新的价格 $p_{k+1}$，使得 $w = b_L (\\log(1-p_k) - \\log(1-p_{k+1}))$。这个方程可以为 $p_{k+1}$ 解析求解：\n  $$ p_{k+1} = 1 - (1-p_k) \\exp\\left(-\\frac{w}{b_L}\\right) $$\n如果 $b  p_k$，完全移动到 $b$ 的成本为 $\\Delta C_{\\text{full}} = b_L (\\log(p_k) - \\log(b))$。\n- 如果 $\\Delta C_{\\text{full}} \\le w$，新价格为 $p_{k+1} = b$。\n- 如果 $\\Delta C_{\\text{full}}  w$，我们求解 $w = b_L (\\log(p_k) - \\log(p_{k+1}))$。解析解为：\n  $$ p_{k+1} = p_k \\exp\\left(-\\frac{w}{b_L}\\right) $$\n与问题描述的建议相反，由于存在这些精确的解析解，数值求根并非必要。如果 $b=p_k$，则不发生交易，且 $p_{k+1} = p_k$。\n\n模拟按以下步骤进行：\n1. 用给定的种子初始化伪随机数生成器（RNG）。\n2. 通过从参数为 $\\theta$ 的伯努利分布中抽样，确定真实结果 $Y \\in \\{0, 1\\}$。\n3. 初始化市场价格 $p = \\pi_0$。\n4. 对 $T$ 个主体中的每一个：\n    a. 设当前价格为 $p_k = p$。为确保数值稳定性，将 $p_k$ 限制在 $[\\varepsilon, 1-\\varepsilon]$ 区间内，其中 $\\varepsilon=10^{-12}$。\n    b. 抽取一个随机数来决定主体是否提供错误信息（基于 $p_{\\text{fake}}$）。\n    c. 根据真实结果 $Y$ 和 $\\alpha$，从条件分布中抽取主体的私人信号 $s \\in \\{0,1\\}$。\n    d. 根据是否提供错误信息的决定，确定要使用的信号 $s'$。\n    e. 使用先验概率 $p_k$ 和信号 $s'$，通过贝叶斯法则计算主体的后验信念 $b$。将 $b$ 限制在 $[\\varepsilon, 1-\\varepsilon]$ 区间内。\n    f. 使用上述预算约束下的交易逻辑计算新的市场价格 $p_{k+1}$。\n    g. 更新市场价格：$p = p_{k+1}$。\n5. 在所有 $T$ 个主体交易完毕后，最终价格为 $p_T = p$。模拟的性能通过布莱尔分数（Brier score）$(p_T - Y)^2$ 来衡量。\n\n对于给定的种子和参数集，此过程是确定性的，从而可以获得可复现的结果。实现将对每个指定的测试用例遵循此逻辑。一个值得注意的特例是当 $\\alpha=0.5$ 时；信号不具信息量，因此主体的后验信念 $b$ 将始终等于其先验信念 $p_k$，导致没有交易，市场价格保持不变。",
            "answer": "```python\nimport numpy as np\nfrom math import log, exp\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A (happy path, no misinformation)\n        {\"seed\": 123, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 0.0, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case B (mixed behavior)\n        {\"seed\": 456, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 0.5, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case C (all agents misinform)\n        {\"seed\": 789, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.7, \"p_fake\": 1.0, \"theta\": 0.6, \"pi_0\": 0.5},\n        # Case D (uninformative signals boundary)\n        {\"seed\": 321, \"T\": 200, \"bL\": 10.0, \"w\": 1.0, \"alpha\": 0.5, \"p_fake\": 0.7, \"theta\": 0.6, \"pi_0\": 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        p_final, brier_score = run_simulation(**params)\n        results.append(f\"{p_final:.6f}\")\n        results.append(f\"{brier_score:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_simulation(seed, T, bL, w, alpha, p_fake, theta, pi_0):\n    \"\"\"\n    Runs a single ACE simulation for a given set of parameters.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    eps = 1e-12\n\n    # Step 1: Draw the true outcome Y for the event\n    Y = 1 if rng.random()  theta else 0\n\n    # Step 2: Initialize market price\n    p = pi_0\n\n    # Step 3: Sequentially process T agents\n    for _ in range(T):\n        p_current = np.clip(p, eps, 1 - eps)\n\n        # Agent behavior:\n        # 1. Observe signal s based on true outcome Y\n        if Y == 1:\n            signal_s = 1 if rng.random()  alpha else 0\n        else: # Y == 0\n            signal_s = 1 if rng.random()  (1 - alpha) else 0\n\n        # 2. Decide whether to misinform\n        is_misinforming = rng.random()  p_fake\n        signal_s_prime = 1 - signal_s if is_misinforming else signal_s\n\n        # 3. Form posterior belief 'b' using Bayes' rule\n        # The agent uses the current market price p_current as their prior\n        if signal_s_prime == 1:\n            numerator = alpha * p_current\n            denominator = alpha * p_current + (1 - alpha) * (1 - p_current)\n        else: # signal_s_prime == 0\n            numerator = (1 - alpha) * p_current\n            denominator = (1 - alpha) * p_current + alpha * (1 - p_current)\n        \n        # If denominator is zero (due to extreme alpha/p), belief doesn't change\n        b = numerator / denominator if denominator > 0 else p_current\n        b = np.clip(b, eps, 1 - eps)\n\n        # Myopic trade logic with budget constraint w:\n        p_next = p_current \n        if b > p_current:\n            # Agent buys \"YES\" shares, wants to move price up to b\n            cost_full_move = bL * (log(1 - p_current) - log(1 - b))\n            if cost_full_move = w:\n                p_next = b\n            else:\n                # Spend full budget w\n                p_next = 1 - (1 - p_current) * exp(-w / bL)\n        elif b  p_current:\n            # Agent buys \"NO\" shares, wants to move price down to b\n            cost_full_move = bL * (log(p_current) - log(b))\n            if cost_full_move = w:\n                p_next = b\n            else:\n                # Spend full budget w\n                p_next = p_current * exp(-w / bL)\n        \n        # Update market price\n        p = p_next\n\n    # Step 4: Calculate final price and Brier score\n    p_final = p\n    brier_score = (p_final - Y)**2\n    \n    return p_final, brier_score\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}