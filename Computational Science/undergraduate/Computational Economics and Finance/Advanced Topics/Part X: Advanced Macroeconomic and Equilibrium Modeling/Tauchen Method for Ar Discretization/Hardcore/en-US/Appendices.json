{
    "hands_on_practices": [
        {
            "introduction": "This first practice serves as your entry point into the mechanics of the Tauchen method. You will implement the complete procedure for a standard first-order autoregressive (AR(1)) process, from building the state grid and transition matrix to calculating the resulting stationary distribution. By assessing the accuracy of your approximation against the known theoretical moments of the process , you will develop a fundamental understanding of how the method works and how to verify its performance.",
            "id": "2436547",
            "problem": "Consider the first-order Autoregressive (AR) process defined by the stochastic difference equation $z_{t+1} = \\rho z_t + \\varepsilon_{t+1}$, where $\\varepsilon_{t}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\varepsilon}^2$. For $|\\rho|  1$, the process is covariance-stationary with unconditional variance $\\sigma_z^2 = \\sigma_{\\varepsilon}^2 / (1 - \\rho^2)$ and unconditional mean $0$. The goal is to approximate this continuous-state process by a finite-state Markov chain using the Tauchen method and to investigate its performance when the persistence parameter is negative, with a particular emphasis on $\\rho = -0.5$.\n\nImplement a program that, for given parameters $(\\rho, \\sigma_{\\varepsilon}, N, m)$:\n- Constructs an evenly spaced grid $\\{z_1, \\ldots, z_N\\}$ over $[-m \\sigma_z, m \\sigma_z]$, where $\\sigma_z = \\sigma_{\\varepsilon}/\\sqrt{1-\\rho^2}$.\n- Builds the transition probability matrix $P \\in \\mathbb{R}^{N \\times N}$ using the Tauchen method: interpret each grid point as the center of a bin, use midpoints to define bin boundaries, and compute transition probabilities by integrating the Gaussian conditional distribution of $z_{t+1}$ given $z_t$ over those bins. Use the Standard Normal cumulative distribution function (CDF).\n- Computes the stationary distribution $\\pi$ of the Markov chain as the invariant distribution satisfying $\\pi = \\pi P$ with $\\sum_{i=1}^N \\pi_i = 1$.\n- Using $\\pi$ and $P$, computes:\n  1. The approximate unconditional mean $\\mu_{\\text{disc}} = \\sum_{i=1}^N \\pi_i z_i$.\n  2. The approximate unconditional variance $\\sigma_{\\text{disc}}^2 = \\sum_{i=1}^N \\pi_i (z_i - \\mu_{\\text{disc}})^2$.\n  3. The implied first-order autocorrelation $\\rho_{\\text{disc}}$, defined as the correlation between $z_t$ and $z_{t+1}$ when $z_t$ is drawn from the stationary distribution of the Markov chain. Compute it via the covariance $\\operatorname{Cov}(z_t, z_{t+1}) = \\sum_{i=1}^N \\pi_i z_i \\left(\\sum_{j=1}^N P_{ij} z_j\\right) - \\mu_{\\text{disc}}^2$ divided by $\\sigma_{\\text{disc}}^2$.\n\nFor each test case, report the following three quantitative diagnostics:\n- The absolute mean error $|\\mu_{\\text{disc}} - 0|$.\n- The relative variance error $\\left|\\sigma_{\\text{disc}}^2 - \\sigma_z^2\\right| / \\sigma_z^2$.\n- The absolute autocorrelation error $|\\rho_{\\text{disc}} - \\rho|$.\n\nFundamental base you may assume without proof:\n- Properties of the Gaussian distribution, including the Standard Normal cumulative distribution function (CDF).\n- The formula for the stationary variance $\\sigma_z^2 = \\sigma_{\\varepsilon}^2/(1-\\rho^2)$ of the AR(1) process when $|\\rho|  1$.\n- Existence and uniqueness of the invariant distribution for an ergodic finite-state Markov chain.\n\nTest Suite:\nEvaluate your implementation on the following parameter sets $(\\rho, \\sigma_{\\varepsilon}, N, m)$:\n- Case $1$ (happy path): $(\\rho, \\sigma_{\\varepsilon}, N, m) = (-0.5, 0.2, 9, 3)$.\n- Case $2$ (high-magnitude negative persistence): $(\\rho, \\sigma_{\\varepsilon}, N, m) = (-0.95, 0.2, 21, 4)$.\n- Case $3$ (small innovation variance): $(\\rho, \\sigma_{\\varepsilon}, N, m) = (-0.5, 0.001, 7, 3)$.\n- Case $4$ (coarse grid): $(\\rho, \\sigma_{\\varepsilon}, N, m) = (-0.5, 0.2, 3, 3)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case should yield a list of three floats $[e_{\\mu}, e_{\\sigma^2}, e_{\\rho}]$, where $e_{\\mu}$ is the absolute mean error, $e_{\\sigma^2}$ is the relative variance error, and $e_{\\rho}$ is the absolute autocorrelation error. Aggregate them into a list in the same order as the test suite. Print all floats rounded to six decimal places. For example, the output should look like:\n[[e_mu_1,e_var_1,e_rho_1],[e_mu_2,e_var_2,e_rho_2],[e_mu_3,e_var_3,e_rho_3],[e_mu_4,e_var_4,e_rho_4]]",
            "solution": "The problem statement has been subjected to rigorous validation. All specifications are found to be scientifically grounded, mathematically consistent, and well-posed. The problem is therefore deemed **valid**. We proceed with a complete solution.\n\nThe objective is to discretize a continuous-state, first-order autoregressive process, AR($1$), using the method proposed by Tauchen. The process is defined by the stochastic difference equation:\n$$ z_{t+1} = \\rho z_t + \\varepsilon_{t+1} $$\nwhere $\\varepsilon_t$ are independent and identically distributed random variables from a normal distribution with mean $0$ and variance $\\sigma_{\\varepsilon}^2$, denoted as $\\varepsilon_{t} \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$. For the process to be covariance-stationary, the persistence parameter must satisfy $|\\rho|  1$. Under this condition, the process has an unconditional mean of $E[z_t] = 0$ and an unconditional variance of $\\sigma_z^2 = \\sigma_{\\varepsilon}^2 / (1 - \\rho^2)$.\n\nThe solution is constructed in five steps: grid generation, transition matrix construction, determination of the stationary distribution, calculation of the discrete-process statistics, and finally, computation of error metrics.\n\n**Step 1: Grid Construction**\nThe continuous state space of the variable $z$ is approximated by a finite, discrete grid. Given the number of grid points $N$ and a scaling factor $m$, we construct an evenly spaced grid $Z = \\{z_1, z_2, \\ldots, z_N\\}$.\nFirst, the unconditional standard deviation of the continuous process is calculated:\n$$ \\sigma_z = \\sqrt{\\frac{\\sigma_{\\varepsilon}^2}{1 - \\rho^2}} $$\nThe grid spans the interval $[-m \\sigma_z, m \\sigma_z]$. The endpoints of the grid are $z_1 = -m \\sigma_z$ and $z_N = m \\sigma_z$. For $N > 1$, the constant step size between adjacent grid points is:\n$$ \\Delta z = \\frac{z_N - z_1}{N-1} = \\frac{2 m \\sigma_z}{N-1} $$\nIf $N=1$, the grid consists of a single point, $z_1=0$.\n\n**Step 2: Transition Matrix Construction**\nThe dynamics of the discretized process are described by an $N \\times N$ Markov transition matrix $P$. The element $P_{ij}$ represents the probability of transitioning from state $z_i$ at time $t$ to state $z_j$ at time $t+1$.\nGiven $z_t = z_i$, the conditional distribution of $z_{t+1}$ is normal with mean $\\rho z_i$ and variance $\\sigma_{\\varepsilon}^2$, i.e., $z_{t+1} | (z_t = z_i) \\sim \\mathcal{N}(\\rho z_i, \\sigma_{\\varepsilon}^2)$.\nThe Tauchen method defines a set of $N$ bins, where each grid point $z_j$ is the center of a bin. The boundaries of these bins are the midpoints between adjacent grid points. The probability $P_{ij}$ is the integral of the conditional normal density over the interval corresponding to bin $j$.\nThe bins are defined as follows:\n- Bin $1$: $(-\\infty, z_1 + \\Delta z/2]$\n- Bin $j$ for $j \\in \\{2, \\ldots, N-1\\}$: $(z_j - \\Delta z/2, z_j + \\Delta z/2]$\n- Bin $N$: $(z_N - \\Delta z/2, \\infty)$\n\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution, $\\mathcal{N}(0,1)$. The transition probabilities are calculated by standardizing the bin boundaries and using the CDF:\n$$\nP_{ij} = \\mathbb{P}(z_{t+1} \\in \\text{bin}_j | z_t = z_i) =\n\\begin{cases}\n\\Phi\\left(\\frac{(z_1 + \\Delta z/2) - \\rho z_i}{\\sigma_{\\varepsilon}}\\right)  \\text{for } j=1 \\\\\n\\Phi\\left(\\frac{(z_j + \\Delta z/2) - \\rho z_i}{\\sigma_{\\varepsilon}}\\right) - \\Phi\\left(\\frac{(z_j - \\Delta z/2) - \\rho z_i}{\\sigma_{\\varepsilon}}\\right)  \\text{for } 1  j  N \\\\\n1 - \\Phi\\left(\\frac{(z_N - \\Delta z/2) - \\rho z_i}{\\sigma_{\\varepsilon}}\\right)  \\text{for } j=N\n\\end{cases}\n$$\nThis construction ensures that for any initial state $z_i$, the probabilities of transitioning to all possible next states sum to unity, i.e., $\\sum_{j=1}^N P_{ij} = 1$.\n\n**Step 3: Stationary Distribution**\nThe stationary distribution of the Markov chain is the probability vector $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_N)$ that remains invariant under the application of the transition matrix. It must satisfy two conditions:\n$$ \\pi P = \\pi \\quad \\text{and} \\quad \\sum_{i=1}^N \\pi_i = 1 $$\nThis means $\\pi$ is the left eigenvector of $P$ corresponding to the eigenvalue $\\lambda = 1$. Equivalently, its transpose $\\pi^T$ is the right eigenvector of the transposed matrix $P^T$ for the eigenvalue $\\lambda=1$:\n$$ P^T \\pi^T = \\pi^T $$\nFor a well-behaved (ergodic) Markov chain, as produced by this method, such a unique stationary distribution is guaranteed to exist. It can be found numerically by computing the eigensystem of $P^T$, identifying the eigenvector associated with the eigenvalue closest to $1$, and normalizing it to sum to $1$.\n\n**Step 4: Approximate Moments of the Discretized Process**\nUsing the stationary distribution $\\pi$ and the grid $Z$, we compute the statistical moments of the approximate process.\n- **Unconditional Mean**: The mean of the process in its stationary state is the expected value over the grid, weighted by the stationary probabilities.\n  $$ \\mu_{\\text{disc}} = E[z] = \\sum_{i=1}^N \\pi_i z_i = \\pi \\cdot Z $$\n- **Unconditional Variance**:\n  $$ \\sigma_{\\text{disc}}^2 = E[(z-\\mu_{\\text{disc}})^2] = \\sum_{i=1}^N \\pi_i (z_i - \\mu_{\\text{disc}})^2 = \\left(\\sum_{i=1}^N \\pi_i z_i^2\\right) - \\mu_{\\text{disc}}^2 $$\n- **First-Order Autocorrelation**: The autocorrelation $\\rho_{\\text{disc}}$ is the correlation between $z_t$ and $z_{t+1}$ when the process is in its stationary state. It is defined as $\\rho_{\\text{disc}} = \\operatorname{Cov}(z_t, z_{t+1}) / \\sigma_{\\text{disc}}^2$. The covariance is given by:\n  $$ \\operatorname{Cov}(z_t, z_{t+1}) = E[z_t z_{t+1}] - E[z_t]E[z_{t+1}] $$\n  By stationarity, $E[z_t] = E[z_{t+1}] = \\mu_{\\text{disc}}$. The term $E[z_t z_{t+1}]$ is computed using the law of total expectation:\n  $$ E[z_t z_{t+1}] = E_{z_t} [z_t E[z_{t+1}|z_t]] = \\sum_{i=1}^N \\pi_i z_i \\left( \\sum_{j=1}^N P_{ij} z_j \\right) $$\n  Thus, the covariance is:\n  $$ \\operatorname{Cov}(z_t, z_{t+1}) = \\left(\\sum_{i=1}^N \\pi_i z_i \\sum_{j=1}^N P_{ij} z_j\\right) - \\mu_{\\text{disc}}^2 $$\n  And the autocorrelation is:\n  $$ \\rho_{\\text{disc}} = \\frac{\\operatorname{Cov}(z_t, z_{t+1})}{\\sigma_{\\text{disc}}^2} $$\n\n**Step 5: Error Metrics**\nThe accuracy of the discretization is assessed by comparing the moments of the Markov chain approximation to their theoretical counterparts from the continuous AR($1$) process.\n- **Absolute Mean Error**: $e_{\\mu} = |\\mu_{\\text{disc}} - 0| = |\\mu_{\\text{disc}}|$.\n- **Relative Variance Error**: $e_{\\sigma^2} = \\frac{|\\sigma_{\\text{disc}}^2 - \\sigma_z^2|}{\\sigma_z^2}$.\n- **Absolute Autocorrelation Error**: $e_{\\rho} = |\\rho_{\\text{disc}} - \\rho|$.\nThese three metrics quantify the performance of the Tauchen method for the given parameterization.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path): (rho, sigma_e, N, m)\n        (-0.5, 0.2, 9, 3),\n        # Case 2 (high-magnitude negative persistence): (rho, sigma_e, N, m)\n        (-0.95, 0.2, 21, 4),\n        # Case 3 (small innovation variance): (rho, sigma_e, N, m)\n        (-0.5, 0.001, 7, 3),\n        # Case 4 (coarse grid): (rho, sigma_e, N, m)\n        (-0.5, 0.2, 3, 3),\n    ]\n\n    results = []\n    for params in test_cases:\n        rho, sigma_e, N, m = params\n        \n        # --- Step 1: Grid Construction ---\n        # Theoretical unconditional variance and standard deviation\n        sigma_z_sq = sigma_e**2 / (1 - rho**2)\n        sigma_z = np.sqrt(sigma_z_sq)\n        \n        # Create an evenly spaced grid\n        if N > 1:\n            z_grid = np.linspace(-m * sigma_z, m * sigma_z, N)\n            delta_z = z_grid[1] - z_grid[0]\n        else: # N=1 is a special case\n            z_grid = np.array([0.0])\n            delta_z = 0.0\n\n        # --- Step 2: Transition Matrix Construction ---\n        P = np.zeros((N, N))\n        for i in range(N):\n            mu_cond = rho * z_grid[i]\n            \n            # Handle bin probabilities\n            if N > 1:\n                # Bin 1: (-inf, z_1 + delta_z/2]\n                upper_bound_1 = z_grid[0] + delta_z / 2\n                P[i, 0] = norm.cdf((upper_bound_1 - mu_cond) / sigma_e)\n                \n                # Bin N: (z_{N-1} + delta_z/2, inf)\n                lower_bound_N = z_grid[N-1] - delta_z / 2\n                P[i, N-1] = 1 - norm.cdf((lower_bound_N - mu_cond) / sigma_e)\n                \n                # Bins 2 to N-1\n                for j in range(1, N - 1):\n                    lower_bound_j = z_grid[j] - delta_z / 2\n                    upper_bound_j = z_grid[j] + delta_z / 2\n                    P[i, j] = norm.cdf((upper_bound_j - mu_cond) / sigma_e) - \\\n                              norm.cdf((lower_bound_j - mu_cond) / sigma_e)\n            else: # N=1 case, P is just [[1.0]]\n                 P[i, 0] = 1.0\n\n        # --- Step 3: Stationary Distribution ---\n        # Find the eigenvector of P.T with eigenvalue 1\n        eigvals, eigvecs = np.linalg.eig(P.T)\n        idx = np.argmin(np.abs(eigvals - 1.0))\n        pi_vec = eigvecs[:, idx].real\n        # Normalize to get a probability distribution\n        pi = pi_vec / pi_vec.sum()\n\n        # --- Step 4: Calculation of Diagnostic Statistics ---\n        # Approximate unconditional mean\n        mu_disc = np.dot(pi, z_grid)\n        \n        # Approximate unconditional variance\n        var_disc = np.dot(pi, (z_grid - mu_disc)**2)\n\n        # Approximate first-order autocorrelation\n        # E[z' | z] = sum_j(P_ij * z_j) for each i\n        expected_z_next = P @ z_grid\n        # E[z * z'] = sum_i(pi_i * z_i * E[z' | z=z_i])\n        E_z_z_next = np.dot(pi * z_grid, expected_z_next)\n        \n        cov_disc = E_z_z_next - mu_disc**2\n        \n        if var_disc == 0:\n            rho_disc = 0.0 # Avoid division by zero if variance is zero\n        else:\n            rho_disc = cov_disc / var_disc\n\n        # --- Step 5: Error Metrics ---\n        # Absolute mean error\n        e_mu = np.abs(mu_disc)\n        \n        # Relative variance error\n        e_var = np.abs(var_disc - sigma_z_sq) / sigma_z_sq\n        \n        # Absolute autocorrelation error\n        e_rho = np.abs(rho_disc - rho)\n        \n        results.append([e_mu, e_var, e_rho])\n\n    # Final print statement in the exact required format.\n    result_strings = [f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the univariate case, this exercise extends the Tauchen method to a second-order autoregressive (AR(2)) process, a common structure in economic modeling. You will learn to handle higher-order dynamics by recasting the process into a vector autoregressive (VAR) form with a two-dimensional state vector. This practice will challenge you to discretize a multi-dimensional state space and compute the corresponding transition kernel, a crucial skill for applying the method to realistic dynamic models .",
            "id": "2436526",
            "problem": "You are given a mean-zero Gaussian Autoregressive (AR) process of order two, defined by the recursion\n$$\ny_t = \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + \\epsilon_t,\n$$\nwhere $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2)$ independently and identically across $t$, and the parameters $(\\rho_1,\\rho_2)$ imply a stationary distribution. Define the $2$-dimensional state vector $x_t = \\begin{bmatrix} y_t \\\\ y_{t-1} \\end{bmatrix}$ and consider the first-order Markov representation\n$$\nx_{t+1} = A x_t + B \\epsilon_{t+1}, \\quad A = \\begin{bmatrix} \\rho_1  \\rho_2 \\\\ 1  0 \\end{bmatrix}, \\quad B = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\nConstruct a finite-state time-homogeneous Markov chain that approximates the dynamics of $x_t$ by using a uniform grid of $N$ nodes for the $y$-dimension, symmetrically centered at $0$ and spanning $\\pm m$ times the stationary standard deviation of $y_t$. The grid on the $y$-dimension is denoted by the vector $Y = (y_1,\\dots,y_N)$ with $y_1  \\cdots  y_N$, where $y_1=-m\\,\\sigma_y$ and $y_N=+m\\,\\sigma_y$, $\\sigma_y$ is the stationary standard deviation of $y_t$, and the interior nodes are equally spaced. The full state space is the Cartesian product $S = Y \\times Y$, with states represented as ordered pairs $(y_i,y_j)$.\n\nFor any current state $(y_i,y_j)$, the conditional distribution of $y_{t+1}$ is Gaussian with conditional mean $\\mu_{i,j}=\\rho_1 y_i + \\rho_2 y_j$ and conditional standard deviation $\\sigma_\\epsilon$. Define bin boundaries for $Y$ by midpoints:\n$$\nb_k = \\frac{y_k+y_{k+1}}{2} \\quad \\text{for } k=1,\\dots,N-1,\\quad b_0=-\\infty,\\quad b_N=+\\infty.\n$$\nDefine the one-step transition probability from $(y_i,y_j)$ to $(y_k,y_i)$ as the probability that $y_{t+1}$ falls in the interval $(b_{k-1},b_k]$ under the Gaussian law with mean $\\mu_{i,j}$ and standard deviation $\\sigma_\\epsilon$. All other transitions from $(y_i,y_j)$ to $(y_k,y_\\ell)$ with $\\ell \\ne i$ have probability zero. This defines a time-homogeneous Markov transition kernel on $S$ fully determined by $(\\rho_1,\\rho_2,\\sigma_\\epsilon,N,m)$.\n\nCompute, for each parameter set in the test suite below, the following two scalar diagnostics of the constructed Markov chain:\n- The maximum absolute deviation of any row sum from $1$, defined as\n$$\n\\Delta_{\\text{row}} = \\max_{(i,j)} \\left| \\sum_{k=1}^N p_{(i,j)\\to (y_k,y_i)} - 1 \\right|.\n$$\n- The maximum absolute conditional expectation approximation error over all states, defined as\n$$\n\\Delta_{\\mathbb{E}} = \\max_{(i,j)} \\left| \\sum_{k=1}^N y_k \\, p_{(i,j)\\to (y_k,y_i)} - \\mu_{i,j} \\right|.\n$$\n\nThe stationary covariance matrix of $x_t$ is the unique symmetric positive definite solution $\\Sigma$ to the discrete-time Lyapunov equation\n$$\n\\Sigma = A \\Sigma A^\\top + \\sigma_\\epsilon^2\\, B B^\\top,\n$$\nand the stationary variance of $y_t$ equals the $(1,1)$ entry of $\\Sigma$, so $\\sigma_y=\\sqrt{\\Sigma_{11}}$. Use this $\\sigma_y$ to define the grid $Y$.\n\nTest suite (each case provides $(\\rho_1,\\rho_2,\\sigma_\\epsilon,N,m)$):\n- Case $1$: $(0.5,\\,0.2,\\,0.1,\\,7,\\,3)$.\n- Case $2$: $(1.8,\\,-0.81,\\,0.05,\\,9,\\,4)$.\n- Case $3$: $(0.0,\\,0.0,\\,1.0,\\,5,\\,3)$.\n- Case $4$: $(0.0,\\,0.5,\\,0.2,\\,3,\\,3)$.\n\nYour program must compute, for each test case, the pair $[\\Delta_{\\text{row}},\\Delta_{\\mathbb{E}}]$ as a list of two real numbers. The final output must be a single line containing a list of these pairs in the order of the test cases, with no spaces, for example\n$$\n[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]].\n$$\nThere are no physical units involved. All angles, if any appear, are not applicable here. All outputs must be numerical real values in standard decimal notation. The program must not read any input and must print exactly one line in the specified format.",
            "solution": "The problem requires the construction and analysis of a finite-state Markov chain to approximate a continuous-state second-order autoregressive process, AR(2). This is a standard problem in computational economics and applied macroeconomics, generalizing the method proposed by Tauchen (1986) for AR(1) processes to a higher-order case by using a vector autoregressive (VAR) representation. The objective is to compute two specific diagnostics that measure the quality of this approximation for several parameterizations. The problem is well-posed, scientifically sound, and computationally tractable.\n\nThe AR(2) process is given by\n$$y_t = \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + \\epsilon_t, \\quad \\epsilon_t \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$$\nIts state-space representation is $x_{t+1} = A x_t + B \\epsilon_{t+1}$, where $x_t = \\begin{bmatrix} y_t \\\\ y_{t-1} \\end{bmatrix}$, $A = \\begin{bmatrix} \\rho_1  \\rho_2 \\\\ 1  0 \\end{bmatrix}$, and $B = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\nThe solution proceeds in four logical steps for each given set of parameters $(\\rho_1, \\rho_2, \\sigma_\\epsilon, N, m)$.\n\nStep 1: Computation of the Stationary Variance\nThe first step is to determine the stationary standard deviation of the process $y_t$, denoted $\\sigma_y$. This is required to define the grid for the state space. The stationary unconditional covariance matrix of the state vector $x_t$, denoted $\\Sigma = \\text{Cov}(x_t, x_t^\\top)$, is the symmetric positive definite solution to the discrete-time Lyapunov equation:\n$$\\Sigma = A \\Sigma A^\\top + Q$$\nwhere $Q = \\sigma_\\epsilon^2 B B^\\top = \\begin{bmatrix} \\sigma_\\epsilon^2  0 \\\\ 0  0 \\end{bmatrix}$. The stationary variance of $y_t$ is the $(1,1)$ element of $\\Sigma$, that is, $\\sigma_y^2 = \\Sigma_{11}$. We solve this matrix equation numerically for $\\Sigma$ and then compute $\\sigma_y = \\sqrt{\\Sigma_{11}}$. For the given parameter sets, the stationarity conditions on $(\\rho_1, \\rho_2)$ are satisfied, ensuring a unique solution for $\\Sigma$ exists.\n\nStep 2: Construction of the State Space Grid\nThe continuous state space of the process is approximated by a discrete grid. The grid for the $y$-dimension, denoted $Y$, consists of $N$ points, symmetric around $0$, and spanning the interval $[-m\\sigma_y, m\\sigma_y]$. The grid points are equally spaced. The vector $Y = (y_1, \\dots, y_N)$ is constructed such that $y_1 = -m\\sigma_y$ and $y_N = m\\sigma_y$. The full state space of the Markov chain, $S$, is the Cartesian product $S = Y \\times Y$, consisting of $N^2$ states of the form $(y_i, y_j)$, where $y_i, y_j \\in Y$.\n\nStep 3: Computation of Transition Probabilities\nThe transition probabilities of the discrete Markov chain must approximate the transition dynamics of the original AR(2) process. The problem specifies a particular discretization scheme. Given the current state is $(y_t, y_{t-1}) = (y_i, y_j)$, the next period's value $y_{t+1}$ is normally distributed with conditional mean $\\mu_{i,j} = \\rho_1 y_i + \\rho_2 y_j$ and standard deviation $\\sigma_\\epsilon$.\nThe state vector evolves from $x_t = (y_i, y_j)$ to $x_{t+1} = (y_{t+1}, y_i)$. The new state must be of the form $(y_k, y_i)$ for some $y_k \\in Y$. The probability of this transition, $p_{(y_i,y_j) \\to (y_k,y_i)}$, is the probability that $y_{t+1}$ falls into an interval associated with the grid point $y_k$. These intervals (or bins) are defined by the midpoints between adjacent grid points:\n$$b_k = \\frac{y_k + y_{k+1}}{2} \\quad \\text{for } k=1, \\dots, N-1$$\nwith the outer boundaries set as $b_0 = -\\infty$ and $b_N = +\\infty$. The $k$-th interval is $(b_{k-1}, b_k]$.\nThe transition probability is thus given by the cumulative distribution function (CDF) of the conditional normal distribution:\n$$p_{(y_i,y_j) \\to (y_k,y_i)} = P(b_{k-1}  y_{t+1} \\le b_k \\mid y_t=y_i, y_{t-1}=y_j)$$\n$$= \\Phi\\left(\\frac{b_k - \\mu_{i,j}}{\\sigma_\\epsilon}\\right) - \\Phi\\left(\\frac{b_{k-1} - \\mu_{i,j}}{\\sigma_\\epsilon}\\right)$$\nwhere $\\Phi(\\cdot)$ is the CDF of the standard normal distribution $\\mathcal{N}(0,1)$. For a given source state $(y_i, y_j)$, we compute these $N$ probabilities for $k=1, \\dots, N$. Transitions to any state not of the form $(y_k, y_i)$ have probability $0$.\n\nStep 4: Calculation of Diagnostic Metrics\nFinally, we compute the two specified diagnostics, which measure the accuracy of the discretization. We iterate over all $N^2$ initial states $(y_i, y_j) \\in S$.\nThe first diagnostic, $\\Delta_{\\text{row}}$, measures the maximum deviation of the sum of transition probabilities from a single state from unity.\n$$\\Delta_{\\text{row}} = \\max_{(i,j)} \\left| \\left( \\sum_{k=1}^N p_{(y_i,y_j)\\to (y_k,y_i)} \\right) - 1 \\right|$$\nTheoretically, this sum is a telescoping series that equals $\\Phi(\\infty) - \\Phi(-\\infty) = 1$. Thus, $\\Delta_{\\text{row}}$ will be non-zero only due to floating-point precision limitations.\n\nThe second diagnostic, $\\Delta_{\\mathbb{E}}$, measures the maximum absolute error in the approximated conditional expectation. The approximated conditional expectation from state $(y_i, y_j)$ is $\\sum_{k=1}^N y_k \\, p_{(y_i,y_j)\\to(y_k,y_i)}$. The true conditional expectation is $\\mu_{i,j} = \\rho_1 y_i + \\rho_2 y_j$. The error is then:\n$$\\Delta_{\\mathbb{E}} = \\max_{(i,j)} \\left| \\sum_{k=1}^N y_k \\, p_{(y_i,y_j)\\to (y_k,y_i)} - \\mu_{i,j} \\right|$$\nThis metric quantifies how well the discretized process preserves the first moment of the original process's dynamics. A smaller $\\Delta_{\\mathbb{E}}$ indicates a better approximation.\n\nThe following program implements this procedure for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_discrete_lyapunov\nfrom scipy.stats import norm\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (rho1, rho2, sigma_eps, N, m)\n        (0.5, 0.2, 0.1, 7, 3),\n        (1.8, -0.81, 0.05, 9, 4),\n        (0.0, 0.0, 1.0, 5, 3),\n        (0.0, 0.5, 0.2, 3, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho1, rho2, sigma_eps, N, m = case\n\n        # Step 1: Compute stationary variance\n        A = np.array([[rho1, rho2], [1.0, 0.0]])\n        B = np.array([[1.0], [0.0]])\n        Q = (sigma_eps**2) * (B @ B.T)\n        \n        Sigma = solve_discrete_lyapunov(A, Q)\n        sigma_y = np.sqrt(Sigma[0, 0])\n\n        # Step 2: Construct the state space grid\n        y_grid = np.linspace(-m * sigma_y, m * sigma_y, N)\n\n        # Step 3: Define bin boundaries\n        step = y_grid[1] - y_grid[0]\n        bin_boundaries_mid = y_grid[:-1] + step / 2.0\n        # Full boundaries for CDF calculation\n        b = np.concatenate(([-np.inf], bin_boundaries_mid, [np.inf]))\n\n        # Step 4: Compute diagnostics\n        max_row_dev = 0.0\n        max_exp_err = 0.0\n\n        for i in range(N):\n            for j in range(N):\n                y_i = y_grid[i]\n                y_j = y_grid[j]\n                \n                # Conditional mean\n                mu_ij = rho1 * y_i + rho2 * y_j\n                \n                # Compute transition probabilities\n                cdf_at_bins = norm.cdf((b - mu_ij) / sigma_eps)\n                probs = np.diff(cdf_at_bins)\n\n                # Diagnostic 1: Row sum deviation\n                row_sum = np.sum(probs)\n                row_dev = abs(row_sum - 1.0)\n                if row_dev > max_row_dev:\n                    max_row_dev = row_dev\n                \n                # Diagnostic 2: Conditional expectation error\n                approx_exp = np.sum(y_grid * probs)\n                exp_err = abs(approx_exp - mu_ij)\n                if exp_err > max_exp_err:\n                    max_exp_err = exp_err\n                    \n        results.append([max_row_dev, max_exp_err])\n\n    # Final print statement in the exact required format.\n    # Format each pair as [val1,val2] and join them with commas.\n    pair_strs = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(pair_strs)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "This final practice deepens your understanding by reversing the problem. Instead of generating a Markov chain from known parameters, you will estimate the parameters of an underlying AR(1) process given its discretized grid and transition matrix. This exercise requires you to derive estimators from first principles by exploiting the conditional moment properties that the Tauchen method preserves . Successfully completing this \"reverse Tauchen\" task demonstrates a mastery of the core theoretical linkage between the continuous process and its discrete approximation.",
            "id": "2436597",
            "problem": "Implement a complete, runnable program that performs a \"reverse\" Tauchen method for a zero-mean Gaussian Autoregressive process of order one (AR(1)). The data-generating process is assumed to be an Autoregressive (AR) model of order one with Gaussian innovations, defined as $z_{t+1} = \\rho z_t + \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^2)$ and $|\\rho|1$. A Tauchen discretization constructs a finite-state Markov chain approximation on a grid, under the assumptions of Gaussian innovations and linear conditional expectation. In the forward Tauchen method, a symmetric grid of $N$ nodes is defined by equally spaced points on the interval $[-m \\cdot \\sigma_z, m \\cdot \\sigma_z]$, where $\\sigma_z = \\sigma_{\\epsilon}/\\sqrt{1-\\rho^2}$ is the stationary standard deviation of $z_t$, and transition probabilities are computed using the standard normal cumulative distribution function evaluated at bin boundaries between grid midpoints.\n\nYour task is to implement the reverse problem: given a state grid and a transition matrix that have been generated by a Tauchen-like scheme for some unknown $\\rho$ and $\\sigma_{\\epsilon}$, estimate $\\rho$ and $\\sigma_{\\epsilon}$. You must base your estimator only on the defining properties of the AR(1) with Gaussian innovations, in particular that its conditional expectation is linear in the current state and that its conditional variance is constant across states, and on how a Markov chain transition matrix encodes conditional distributions over the next grid state given the current one. Do not assume access to any formulas for the estimator; instead, derive a consistent estimator from first principles by relating conditional moments implied by the Markov chain to those of the continuous AR(1) process.\n\nTo make the task concrete and testable, your program must implement both:\n- A forward Tauchen generator, which for given parameters $(\\rho,\\sigma_{\\epsilon},N,m)$ constructs a symmetric, equally spaced grid $\\{z_j\\}_{j=1}^N$ on $[-m \\cdot \\sigma_z, m \\cdot \\sigma_z]$ and a transition matrix $P \\in \\mathbb{R}^{N \\times N}$ where the entry $P_{ij}$ equals the probability that the next state falls into the $j$-th grid cell when the current state is the $i$-th grid point, computed from a Gaussian distribution with mean $\\rho z_i$ and standard deviation $\\sigma_{\\epsilon}$ integrated over bin intervals defined by midpoints of adjacent grid points, with the lower-most and upper-most bins extending to $-\\infty$ and $+\\infty$, respectively. Use the standard normal cumulative distribution function for these integrals.\n- A reverse Tauchen estimator, which takes $(\\{z_j\\}_{j=1}^N, P)$ as input and outputs estimates $(\\widehat{\\rho},\\widehat{\\sigma}_{\\epsilon})$ by exploiting the relationship between the Markov chainâ€™s conditional moments and those of the continuous AR(1) process.\n\nYour program must execute the following test suite. For each parameter set, first generate $(\\{z_j\\},P)$ using your forward Tauchen implementation, then apply your reverse estimator to recover $(\\widehat{\\rho},\\widehat{\\sigma}_{\\epsilon})$. Use the following test cases:\n- Test case $1$: $(\\rho,\\sigma_{\\epsilon},N,m) = (0.9, 0.1, 7, 3.0)$.\n- Test case $2$: $(\\rho,\\sigma_{\\epsilon},N,m) = (0.0, 0.2, 5, 3.0)$.\n- Test case $3$: $(\\rho,\\sigma_{\\epsilon},N,m) = (0.99, 0.05, 11, 3.5)$.\n- Test case $4$: $(\\rho,\\sigma_{\\epsilon},N,m) = (-0.5, 0.15, 9, 3.0)$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing a flat list of floating-point numbers with the estimates for each test case in order, formatted as $[\\widehat{\\rho}_1,\\widehat{\\sigma}_{\\epsilon,1},\\widehat{\\rho}_2,\\widehat{\\sigma}_{\\epsilon,2},\\widehat{\\rho}_3,\\widehat{\\sigma}_{\\epsilon,3},\\widehat{\\rho}_4,\\widehat{\\sigma}_{\\epsilon,4}]$.\n- Each floating-point number must be rounded to exactly $6$ decimal places.\n- No additional text should be printed.\n\nAll angles, if any, are irrelevant here. No physical units are involved. The only required numerical specification is that all printed floating-point numbers must be rounded to $6$ decimal places. The output must be a single line: a comma-separated list enclosed in square brackets as described above.",
            "solution": "The problem statement submitted for consideration is valid. It is scientifically grounded, well-posed, objective, and internally consistent. It describes a concrete computational task in the domain of quantitative economics: the discretization of a continuous-time stochastic process and the subsequent estimation of the underlying process parameters from the discretized approximation. The task is to first implement the standard Tauchen method for a Gaussian AR(1) process and then to derive and implement a \"reverse\" procedure to estimate the process parameters $(\\rho, \\sigma_{\\epsilon})$ from the resulting discrete state grid and transition matrix. This is a standard problem in numerical methods and econometrics.\n\nI will now provide the complete derivation and solution. The solution is structured in two parts: first, the formal construction of the forward Tauchen discretization, and second, the derivation of the estimators for the reverse problem from first principles.\n\nLet the data-generating process be a zero-mean Autoregressive process of order one (AR(1)):\n$$z_{t+1} = \\rho z_t + \\epsilon_t$$\nwhere the innovation term $\\epsilon_t$ is an independent and identically distributed random variable following a normal distribution with mean zero and variance $\\sigma_{\\epsilon}^2$, denoted $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma_{\\epsilon}^2)$. The persistence parameter $\\rho$ satisfies $|\\rho|  1$ for stationarity. The unconditional variance of the process $z_t$ is $\\sigma_z^2 = \\text{Var}(z_t) = \\sigma_{\\epsilon}^2 / (1-\\rho^2)$.\n\n**Part 1: The Forward Tauchen Method**\n\nThe forward Tauchen method constructs a finite-state Markov chain that approximates the continuous AR(1) process. This involves defining a discrete state space (a grid) and a transition probability matrix.\n\n1.  **State Grid Construction:**\n    The state space is a symmetric grid of $N$ points, denoted $\\{z_j\\}_{j=1}^N$. The grid is constructed to span a certain number of unconditional standard deviations of the process.\n    -   The stationary standard deviation is $\\sigma_z = \\sigma_{\\epsilon} / \\sqrt{1-\\rho^2}$.\n    -   The grid covers the interval $[-m\\sigma_z, m\\sigma_z]$.\n    -   The $N$ grid points are equally spaced. The maximum and minimum values are $z_N = m\\sigma_z$ and $z_1 = -m\\sigma_z$.\n    -   The grid points can be generated using a linear space function:\n        $$z_j \\quad \\text{for } j=1, \\dots, N \\quad \\text{are given by } \\text{linspace}(-m\\sigma_z, m\\sigma_z, N)$$\n\n2.  **Transition Matrix Construction:**\n    The transition matrix $P \\in \\mathbb{R}^{N \\times N}$ provides the probabilities $P_{ij}$ of transitioning from state $z_i$ to state $z_j$. These probabilities are calculated by integrating the conditional density of $z_{t+1}$ over intervals (bins) associated with each grid point $z_j$.\n    -   Given $z_t = z_i$, the next state $z_{t+1}$ follows a normal distribution:\n        $$z_{t+1} | z_t=z_i \\sim \\mathcal{N}(\\rho z_i, \\sigma_{\\epsilon}^2)$$\n    -   The bin boundaries are defined as the midpoints between adjacent grid points. Let $\\Delta z = z_{j+1} - z_j$ be the constant step size. The midpoint between $z_j$ and $z_{j+1}$ is $b_j = z_j + \\Delta z / 2$ for $j=1, \\dots, N-1$.\n    -   The bins are defined as:\n        -   Bin $1$: $(-\\infty, b_1]$\n        -   Bin $j$: $(b_{j-1}, b_j]$ for $j=2, \\dots, N-1$\n        -   Bin $N$: $(b_{N-1}, +\\infty)$\n    -   The transition probability $P_{ij}$ is the probability that $z_{t+1}$ falls into Bin $j$, given $z_t = z_i$. Let $\\Phi(\\cdot)$ be the Cumulative Distribution Function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\n        $$P(z_{t+1} \\le x | z_t=z_i) = P(\\rho z_i + \\epsilon_t \\le x) = P\\left(\\frac{\\epsilon_t}{\\sigma_\\epsilon} \\le \\frac{x-\\rho z_i}{\\sigma_\\epsilon}\\right) = \\Phi\\left(\\frac{x-\\rho z_i}{\\sigma_\\epsilon}\\right)$$\n    -   The elements of the transition matrix are thus:\n        -   For $j=1$:\n            $$P_{i1} = P(z_{t+1} \\in \\text{Bin } 1 | z_t=z_i) = \\Phi\\left(\\frac{b_1 - \\rho z_i}{\\sigma_\\epsilon}\\right)$$\n        -   For $j=2, \\dots, N-1$:\n            $$P_{ij} = P(z_{t+1} \\in \\text{Bin } j | z_t=z_i) = \\Phi\\left(\\frac{b_j - \\rho z_i}{\\sigma_\\epsilon}\\right) - \\Phi\\left(\\frac{b_{j-1} - \\rho z_i}{\\sigma_\\epsilon}\\right)$$\n        -   For $j=N$:\n            $$P_{iN} = P(z_{t+1} \\in \\text{Bin } N | z_t=z_i) = 1 - \\Phi\\left(\\frac{b_{N-1} - \\rho z_i}{\\sigma_\\epsilon}\\right)$$\n\n**Part 2: The Reverse Tauchen Method (Estimator Derivation)**\n\nThe reverse problem is to estimate the parameters $(\\rho, \\sigma_{\\epsilon})$ given the grid $\\mathbf{z} = [z_1, \\dots, z_N]^T$ and the transition matrix $P$. The derivation relies on equating the theoretical conditional moments of the AR(1) process with their empirical counterparts from the Markov chain approximation.\n\n1.  **Fundamental Moment Conditions of the AR(1) Process:**\n    -   **Conditional Expectation:** The expectation of the next state, conditional on the current state $z_t$, is linear:\n        $$E[z_{t+1} | z_t] = E[\\rho z_t + \\epsilon_t | z_t] = \\rho z_t + E[\\epsilon_t] = \\rho z_t$$\n    -   **Conditional Variance:** The variance of the next state, conditional on the current state $z_t$, is constant:\n        $$\\text{Var}(z_{t+1} | z_t) = \\text{Var}(\\rho z_t + \\epsilon_t | z_t) = \\text{Var}(\\epsilon_t) = \\sigma_{\\epsilon}^2$$\n\n2.  **Estimating $\\rho$:**\n    The Markov chain approximation provides the conditional probability distribution for the next grid state. For each current state $z_i$, we can compute the expected value of the next state, $E[z'|z=z_i]$:\n    $$E[z'|z=z_i] = \\sum_{j=1}^N z_j P_{ij}$$\n    We treat this as an approximation of the true conditional expectation: $E[z'|z=z_i] \\approx E[z_{t+1}|z_t=z_i] = \\rho z_i$. This gives us a system of $N$ equations:\n    $$\\sum_{j=1}^N z_j P_{ij} \\approx \\rho z_i \\quad \\text{for } i=1, \\dots, N$$\n    This is an overdetermined linear system for the unknown $\\rho$. A natural method to solve this is Ordinary Least Squares (OLS). Let $Y_i = \\sum_{j=1}^N z_j P_{ij}$ and $X_i = z_i$. We fit the model $Y_i = \\rho X_i + \\text{error}$. The OLS estimator for regression through the origin is:\n    $$\\widehat{\\rho} = \\frac{\\sum_{i=1}^N X_i Y_i}{\\sum_{i=1}^N X_i^2} = \\frac{\\sum_{i=1}^N z_i \\left(\\sum_{j=1}^N z_j P_{ij}\\right)}{\\sum_{i=1}^N z_i^2}$$\n    In vector notation, let the vector of conditional expectations be $\\mathbf{E_z} = P\\mathbf{z}$. Then the estimator is:\n    $$\\widehat{\\rho} = \\frac{\\mathbf{z}^T \\mathbf{E_z}}{\\mathbf{z}^T \\mathbf{z}} = \\frac{\\mathbf{z}^T P \\mathbf{z}}{\\mathbf{z}^T \\mathbf{z}}$$\n\n3.  **Estimating $\\sigma_{\\epsilon}$:**\n    The conditional variance of the process is constant, $\\sigma_{\\epsilon}^2$. For each state $z_i$ in the Markov chain, we can compute the conditional variance of the next state, $\\text{Var}(z'|z=z_i)$:\n    $$\\text{Var}(z'|z=z_i) = E[(z')^2|z=z_i] - (E[z'|z=z_i])^2$$\n    where the conditional second moment is $E[(z')^2|z=z_i] = \\sum_{j=1}^N z_j^2 P_{ij}$.\n    Each of these $N$ calculated conditional variances, $V_i = \\text{Var}(z'|z=z_i)$, is an estimate of the true constant variance $\\sigma_{\\epsilon}^2$. A consistent estimator for $\\sigma_{\\epsilon}^2$ can be formed by taking the average of these individual variance estimates:\n    $$\\widehat{\\sigma}_{\\epsilon}^2 = \\frac{1}{N} \\sum_{i=1}^N \\text{Var}(z'|z=z_i) = \\frac{1}{N} \\sum_{i=1}^N \\left[ \\left(\\sum_{j=1}^N z_j^2 P_{ij}\\right) - \\left(\\sum_{j=1}^N z_j P_{ij}\\right)^2 \\right]$$\n    The estimator for the standard deviation of the innovation is then simply the square root:\n    $$\\widehat{\\sigma}_{\\epsilon} = \\sqrt{\\widehat{\\sigma}_{\\epsilon}^2}$$\n    This completes the derivation of the estimators for the reverse Tauchen problem. The implementation will follow these derived formulas.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite for the reverse Tauchen method.\n    It first generates a discrete Markov approximation (grid and transition matrix)\n    for a given AR(1) process using the forward Tauchen method. Then, it uses\n    the reverse Tauchen method to estimate the original AR(1) parameters.\n    \"\"\"\n\n    def forward_tauchen(rho, sigma_eps, N, m):\n        \"\"\"\n        Generates a state grid and transition matrix for an AR(1) process\n        using Tauchen's method.\n\n        Args:\n            rho (float): The persistence parameter of the AR(1) process.\n            sigma_eps (float): The standard deviation of the innovation term.\n            N (int): The number of grid points.\n            m (float): The number of standard deviations the grid should span.\n\n        Returns:\n            tuple: A tuple containing:\n                - z_grid (np.ndarray): The state grid.\n                - P (np.ndarray): The transition matrix.\n        \"\"\"\n        # Step 1: Create the state grid\n        # Unconditional standard deviation of the process z\n        if abs(rho) >= 1.0:\n            raise ValueError(\"rho must be less than 1 for stationarity.\")\n            \n        sigma_z = sigma_eps / np.sqrt(1 - rho**2)\n        z_max = m * sigma_z\n        z_grid = np.linspace(-z_max, z_max, N)\n\n        # Step 2: Create the transition matrix\n        P = np.zeros((N, N))\n        step = z_grid[1] - z_grid[0]\n        \n        # Define bin boundaries as midpoints between grid points\n        midpoints = z_grid[:-1] + step / 2\n\n        for i in range(N):\n            # Conditional mean of z_{t+1} given z_t = z_grid[i]\n            mu_z_prime = rho * z_grid[i]\n            \n            # Probability for the first bin (-inf, midpoint_1]\n            z_val = (midpoints[0] - mu_z_prime) / sigma_eps\n            P[i, 0] = norm.cdf(z_val)\n            \n            # Probabilities for the middle bins (midpoint_{j-1}, midpoint_j]\n            for j in range(1, N - 1):\n                z_val_upper = (midpoints[j] - mu_z_prime) / sigma_eps\n                z_val_lower = (midpoints[j-1] - mu_z_prime) / sigma_eps\n                P[i, j] = norm.cdf(z_val_upper) - norm.cdf(z_val_lower)\n            \n            # Probability for the last bin (midpoint_{N-1}, +inf)\n            z_val = (midpoints[-1] - mu_z_prime) / sigma_eps\n            P[i, N - 1] = 1 - norm.cdf(z_val)\n            \n        return z_grid, P\n\n    def reverse_tauchen(z_grid, P):\n        \"\"\"\n        Estimates the parameters of an AR(1) process from its Tauchen\n        discretization.\n\n        Args:\n            z_grid (np.ndarray): The state grid.\n            P (np.ndarray): The transition matrix.\n\n        Returns:\n            tuple: A tuple containing the estimated parameters:\n                - rho_hat (float): Estimated persistence parameter.\n                - sigma_eps_hat (float): Estimated standard deviation of innovation.\n        \"\"\"\n        # Step 1: Estimate rho using OLS on E[z'|z] = rho*z\n        # Conditional expectation of the next state: E[z'|z] = P @ z\n        cond_exp = P @ z_grid\n        \n        # OLS estimator for regression through the origin: (z' * E[z'|z]) / (z' * z)\n        rho_hat = (z_grid @ cond_exp) / (z_grid @ z_grid)\n        \n        # Step 2: Estimate sigma_epsilon\n        # Conditional second moment: E[(z')^2|z] = P @ (z^2)\n        z_grid_sq = z_grid**2\n        cond_sec_moment = P @ z_grid_sq\n        \n        # Conditional variance: Var(z'|z) = E[(z')^2|z] - (E[z'|z])^2\n        cond_var = cond_sec_moment - cond_exp**2\n        \n        # sigma_eps^2 is the constant conditional variance, so we average\n        # the estimates from each state.\n        sigma_eps_sq_hat = np.mean(cond_var)\n        sigma_eps_hat = np.sqrt(sigma_eps_sq_hat)\n        \n        return rho_hat, sigma_eps_hat\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.9, 0.1, 7, 3.0),\n        (0.0, 0.2, 5, 3.0),\n        (0.99, 0.05, 11, 3.5),\n        (-0.5, 0.15, 9, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, sigma_eps, N, m = case\n        \n        # Generate the discrete approximation\n        z, P = forward_tauchen(rho, sigma_eps, N, m)\n        \n        # Estimate the parameters from the approximation\n        rho_hat, sigma_eps_hat = reverse_tauchen(z, P)\n        \n        results.append(rho_hat)\n        results.append(sigma_eps_hat)\n\n    # Format the results for the final print statement\n    formatted_results = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}