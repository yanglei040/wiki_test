{
    "hands_on_practices": [
        {
            "introduction": "线性二次（LQ）模型是平均场博弈理论的基石。这类模型因其特殊的数学结构而备受关注，它允许我们使用最优控制理论中的工具（如Riccati方程）来获得解析或半解析解。通过这个关于城市增长的练习 ，你将亲手实践如何通过求解一个倒向的Riccati方程和正向的方差演化来刻画一个平均场均衡，从而掌握解决LQ-MFG问题的核心思路。",
            "id": "2409391",
            "problem": "考虑一个由$i$索引的城市连续体，时间是离散的$t \\in \\{0,1,\\dots,T\\}$。每个城市$i$都有一个对数规模$x_t^i \\in \\mathbb{R}$。令$m_t \\in \\mathbb{R}$表示在时间$t$时$\\{x_t^i\\}_{i}$的横截面均值。城市$i$通过选择一个控制量$u_t^i \\in \\mathbb{R}$来控制其增长，该控制量通过以下确定性运动定律影响其下一周期的对数规模：\n$$\nx_{t+1}^i = (1-\\beta)x_t^i + \\beta m_t + u_t^i\n$$\n其中$\\beta \\in [0,1]$是一个给定参数。对数规模的初始横截面分布使得$x_0^i$的均值为$m_0 \\in \\mathbb{R}$，方差为$s_0^2 \\ge 0$。对于给定的确定性平均场路径$\\{m_t\\}_{t=0}^T$，每个城市$i$最小化以下二次目标函数：\n$$\nJ^i = \\sum_{t=0}^{T-1} \\left(\\frac{1}{2}q(x_t^i - m_t)^2 + \\frac{1}{2}r(u_t^i)^2\\right) + \\frac{1}{2}q_T(x_T^i - m_T)^2\n$$\n其中$q \\ge 0$，$r > 0$和$q_T \\ge 0$是给定的常数。一个平均场均衡是一个序列$\\{m_t\\}_{t=0}^T$和一个可测的反馈定律$u_t^i = \\pi_t(x_t^i,m_t)$，使得对于每个城市$i$，在给定$\\{m_t\\}_{t=0}^T$的情况下，控制量$u_t^i$最小化$J^i$，并且当所有城市都使用最优策略且$x_0^i$的均值为$m_0$、方差为$s_0^2$时，所引出的横截面均值满足不动点条件$m_t = \\mathbb{E}[x_t^i]$，对所有$t \\in \\{0,1,\\dots,T\\}$成立。\n\n您的任务是，对于下面测试套件中的每组参数，计算表征上述模型下唯一平均场均衡的以下两个量：\n- 在均衡平均场路径下，由时间$t=0$时的线性关系$u_0^i = -K_0(x_0^i - m_0)$定义的最优初始反馈系数$K_0 \\in \\mathbb{R}$。\n- 终端时刻对数规模的横截面方差，$\\mathrm{Var}(x_T^i)$。\n\n假设除了上述给定的确定性动态之外，没有外生冲击。将报告的每个数字表示为四舍五入到六位小数的实数。所有角度都是无量纲的。不涉及物理单位。\n\n待评估的参数值测试套件：\n- 情况 1：$T = 10$，$\\beta = 0.3$， $q = 1.0$， $r = 0.5$， $q_T = 1.0$， $m_0 = 2.0$， $s_0^2 = 1.0$。\n- 情况 2：$T = 5$， $\\beta = 0.0$， $q = 1.0$， $r = 1.0$， $q_T = 1.0$， $m_0 = 0.0$， $s_0^2 = 0.25$。\n- 情况 3：$T = 20$， $\\beta = 0.9$， $q = 0.5$， $r = 1.0$， $q_T = 0.5$， $m_0 = -1.0$，$s_0^2 = 2.0$。\n- 情况 4：$T = 8$， $\\beta = 0.2$， $q = 1.0$， $r = 0.01$，$q_T = 1.0$， $m_0 = 1.0$， $s_0^2 = 3.0$。\n- 情况 5：$T = 12$， $\\beta = 0.4$， $q = 0.01$，$r = 1.0$， $q_T = 0.01$，$m_0 = 0.5$， $s_0^2 = 1.5$。\n- 情况 6：$T = 7$， $\\beta = 0.5$， $q = 1.0$， $r = 1.0$， $q_T = 1.0$， $m_0 = 3.0$， $s_0^2 = 0.0$。\n\n最终输出格式：\n- 对于每种情况，输出一个双元素列表$[K_0, \\mathrm{Var}(x_T^i)]$，其中两个条目都四舍五入到六位小数。\n- 将所有情况的结果汇总到一行中，该行是一个用方括号括起来的逗号分隔列表，例如$[[a_1,b_1],[a_2,b_2],\\dots]$，不含任何额外文本。",
            "solution": "对所提供的问题进行验证。\n\n逐字提取给定条件：\n- 由$i$索引的城市连续体。\n- 离散时间$t \\in \\{0, 1, \\dots, T\\}$。\n- 城市$i$的对数规模：$x_t^i \\in \\mathbb{R}$。\n- 横截面均值对数规模：$m_t = \\mathbb{E}[x_t^i]$。\n- 城市$i$的控制量：$u_t^i \\in \\mathbb{R}$。\n- 运动定律：$x_{t+1}^i = (1-\\beta)x_t^i + \\beta m_t + u_t^i$，其中$\\beta \\in [0,1]$。\n- $x_0^i$的初始分布：均值$m_0 \\in \\mathbb{R}$，方差$s_0^2 \\ge 0$。\n- 城市$i$的目标函数：$J^i = \\sum_{t=0}^{T-1} (\\frac{1}{2}q(x_t^i - m_t)^2 + \\frac{1}{2}r(u_t^i)^2) + \\frac{1}{2}q_T(x_T^i - m_T)^2$，其中$q \\ge 0$, $r > 0$, $q_T \\ge 0$。\n- 平均场均衡定义：一个路径$\\{m_t\\}_{t=0}^T$和一个反馈定律$u_t^i = \\pi_t(x_t^i, m_t)$，使得在给定$\\{m_t\\}$的情况下，$u_t^i$对每个城市$i$都是最优的，并且一致性条件$m_t = \\mathbb{E}[x_t^i]$对所有$t$都成立。\n- 任务：从$u_0^i = -K_0(x_0^i - m_0)$计算初始反馈系数$K_0$和终端方差$\\mathrm{Var}(x_T^i)$。\n\n该问题是平均场博弈论中一个定义明确的标准问题，具体来说是一个离散时间上的线性二次模型。它具有科学依据，是适定的，并以客观的数学语言表达。求解所需的所有参数均已提供。因此，该问题被认为是有效的。\n\n求解过程如下。首先，我们刻画平均场均衡。单个城市$i$求解一个线性二次最优控制问题，并将平均场路径$\\{m_t\\}_{t=0}^T$视为给定。均衡要求遵循其最优策略的城市的聚合行为能够再现给定的平均场路径。\n\n令$\\tilde{x}_t^i = x_t^i - m_t$为城市$i$的对数规模与均值的偏差。该偏差的运动定律为：\n$$\n\\tilde{x}_{t+1}^i = x_{t+1}^i - m_{t+1} = \\big((1-\\beta)x_t^i + \\beta m_t + u_t^i\\big) - m_{t+1}\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)(\\tilde{x}_t^i + m_t) + \\beta m_t + u_t^i - m_{t+1} = (1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}\n$$\n成本函数变为：\n$$\nJ^i = \\sum_{t=0}^{T-1} \\left(\\frac{1}{2}q(\\tilde{x}_t^i)^2 + \\frac{1}{2}r(u_t^i)^2\\right) + \\frac{1}{2}q_T(\\tilde{x}_T^i)^2\n$$\n这是一个标准的线性二次调节器问题，其中$(m_t - m_{t+1})$充当外生扰动。价值函数是二次的，$V_t(\\tilde{x}_t^i) = \\frac{1}{2}P_t(\\tilde{x}_t^i)^2 + \\dots$，其中与$\\tilde{x}_t^i$无关的项被省略。时间$t$的Hamilton-Jacobi-Bellman方程为：\n$$\nV_t(\\tilde{x}_t^i) = \\min_{u_t^i} \\left\\{ \\frac{1}{2}q(\\tilde{x}_t^i)^2 + \\frac{1}{2}r(u_t^i)^2 + V_{t+1}(\\tilde{x}_{t+1}^i) \\right\\}\n$$\n对括号内的项关于$u_t^i$进行最小化，得到一阶条件：\n$$\nr u_t^i + \\frac{\\partial V_{t+1}}{\\partial \\tilde{x}_{t+1}^i} \\frac{\\partial \\tilde{x}_{t+1}^i}{\\partial u_t^i} = r u_t^i + P_{t+1}\\tilde{x}_{t+1}^i(1) = r u_t^i + P_{t+1}((1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}) = 0\n$$\n求解最优控制$u_t^i$：\n$$\n(r+P_{t+1})u_t^i = -P_{t+1}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n$$\nu_t^i = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n平均场一致性条件是$\\mathbb{E}[x_t^i] = m_t$，这意味着对所有$t$都有$\\mathbb{E}[\\tilde{x}_t^i] = 0$。对原始状态动态取期望，得$m_{t+1} = (1-\\beta)m_t + \\beta m_t + \\mathbb{E}[u_t^i] = m_t + \\mathbb{E}[u_t^i]$，这意味着$\\mathbb{E}[u_t^i] = m_{t+1} - m_t$。\n对最优控制律取期望：\n$$\n\\mathbb{E}[u_t^i] = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\mathbb{E}[\\tilde{x}_t^i] + m_t - m_{t+1}) = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1})\n$$\n令$\\mathbb{E}[u_t^i]$的两个表达式相等：\n$$\nm_{t+1} - m_t = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1}) \\implies \\left(1 + \\frac{P_{t+1}}{r+P_{t+1}}\\right)(m_{t+1} - m_t) = 0\n$$\n由于$r > 0$且$P_{t+1} \\ge 0$ (因为它代表成本)，括号中的项是严格为正的。因此，必须有$m_{t+1} - m_t = 0$对所有$t$成立。这证明了均衡平均场路径必须是常数：$m_t = m_0$对所有$t = 0, \\dots, T$成立。\n\n当$m_t$为常数时，问题显著简化。偏差的动态变为$\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i + u_t^i$。现在问题变成了关于偏差状态$\\tilde{x}_t^i$的标准确定性LQR问题。最优控制简化为线性反馈定律：\n$$\nu_t^i = -\\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i = -K_t \\tilde{x}_t^i\n$$\n其中$K_t = \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}$是反馈增益。系数$P_t$由离散时间Riccati方程确定，从$t=T-1$向后求解至$t=0$：\n$$\nP_t = q + (1-\\beta)^2 P_{t+1} - \\frac{((1-\\beta)P_{t+1})^2}{r+P_{t+1}} = q + \\frac{r P_{t+1}(1-\\beta)^2}{r+P_{t+1}}\n$$\n递归从终端成本推导出的终端条件$P_T = q_T$开始。\n然后，我们关注的量$K_0$计算为$K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$，这需要计算序列$\\{P_t\\}_{t=1}^T$。对于$T=0$，这个没有定义，但所有测试用例都有$T \\ge 1$。\n\n接下来，我们推导横截面方差的演化，$s_t^2 = \\mathrm{Var}(x_t^i) = \\mathrm{Var}(\\tilde{x}_t^i)$。将最优控制代入偏差的动态方程：\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i - K_t \\tilde{x}_t^i = \\left((1-\\beta) - \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\left(1 - \\frac{P_{t+1}}{r+P_{t+1}}\\right)\\tilde{x}_t^i = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n方差根据前向递归演化。令$v_t = s_t^2$：\n$$\nv_{t+1} = \\mathrm{Var}(\\tilde{x}_{t+1}^i) = \\mathbb{E}\\left[\\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i\\right)^2\\right] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 \\mathbb{E}[(\\tilde{x}_t^i)^2] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 v_t\n$$\n从给定的初始方差$v_0 = s_0^2$开始，我们可以通过一次前向传递，使用先前计算出的序列$\\{P_t\\}$来计算序列$\\{v_t\\}_{t=1}^T$。终端方差为$\\mathrm{Var}(x_T^i) = v_T$。\n\n算法如下：\n1. 初始化一个大小为$T+1$的数组$P$，并令$P_T = q_T$。\n2. 从$t = T-1$向下迭代到$t=0$，通过求解Riccati方程来填充数组$P$。\n3. 计算初始反馈增益$K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$。对于$T=0$，此值未定义，但所有测试用例均满足$T \\ge 1$。\n4. 初始化方差$v_0 = s_0^2$。\n5. 从$t=0$到$t=T-1$进行前向迭代，使用存储的$P_{t+1}$值计算方差的演化。\n6. 每种情况的最终结果是$[K_0, v_T]$。\n\n下面为所提供的测试用例实现了此过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a discrete-time linear-quadratic mean-field game for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, beta, q, r, q_T, m_0, s0_sq)\n        (10, 0.3, 1.0, 0.5, 1.0, 2.0, 1.0),\n        (5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25),\n        (20, 0.9, 0.5, 1.0, 0.5, -1.0, 2.0),\n        (8, 0.2, 1.0, 0.01, 1.0, 1.0, 3.0),\n        (12, 0.4, 0.01, 1.0, 0.01, 0.5, 1.5),\n        (7, 0.5, 1.0, 1.0, 1.0, 3.0, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        T, beta, q, r, q_T, m_0, s0_sq = case\n        \n        # 1. Backward pass: Solve the Riccati equation for P_t\n        # P is an array of size T+1 for P_0, ..., P_T\n        P = np.zeros(T + 1)\n        P[T] = q_T\n        \n        # The equation for P_t depends on P_{t+1}. We iterate t from T-1 down to 0.\n        for t in range(T - 1, -1, -1):\n            P_next = P[t + 1]\n            term = (r * P_next * (1 - beta)**2) / (r + P_next)\n            P[t] = q + term\n\n        # 2. Calculate the initial feedback coefficient K_0\n        # K_0 depends on P_1. If T=0, there is no P_1 and no control u_0.\n        # Problem statement implies T >= 1 for control to be meaningful.\n        if T > 0:\n            K0 = (P[1] * (1 - beta)) / (r + P[1])\n        else:\n            # If T=0, there is no control phase, so K_0 is not applicable.\n            # Based on the problem structure, K_0=0 is a reasonable assignment.\n            K0 = 0.0\n\n        # 3. Forward pass: Evolve the cross-sectional variance\n        var = s0_sq\n        \n        # The evolution of var_t to var_{t+1} depends on P_{t+1}\n        for t in range(T):\n            P_next = P[t + 1]\n            factor = (r * (1 - beta)) / (r + P_next)\n            var = (factor**2) * var\n            \n        var_T = var\n\n        # Append rounded results for the current case\n        results.append([round(K0, 6), round(var_T, 6)])\n\n    # Format the final output string as specified.\n    # e.g., [[val1, val2], [val3, val4], ...]\n    final_output = \"[\" + \",\".join([f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "然而，大多数平均场博弈问题无法解析求解，必须依赖数值方法。不动点迭代（或称“虚拟博弈”）是解决这类问题的一种通用且强大的计算范式。这个练习  将引导你为一个离散的交易模型实现这一核心算法，你将通过交替执行“向后”求解个体最优策略和“向前”模拟群体动态这两个步骤，直到系统收敛到均衡状态。",
            "id": "2409414",
            "problem": "考虑一个具有有限期$T \\in \\mathbb{N}$的离散时间交易平均场博弈 (MFG)。时间由$t \\in \\{0,1,\\dots,T-1\\}$索引。在时间$t$，每个代理人有一个库存状态$i_t \\in \\{-1,0,1\\}$。在每个时间$t$，代理人选择一个控制（行动）$a_t \\in \\{-1,0,1\\}$，分别解释为“卖出”（$-1$）、“持有”（$0$）或“买入”（$+1$），并受制于库存约束，即下一个库存为：\n$$\ni_{t+1} = \\min\\{1,\\max\\{-1, i_t + a_t\\}\\}\n$$\n设$p_t(i)$表示在时间$t$库存状态$i \\in \\{-1,0,1\\}$上的群体分布，满足$p_t(-1)+p_t(0)+p_t(1)=1$。在时间$t$的群体平均行动是：\n$$\nm_t \\equiv \\sum_{i \\in \\{-1,0,1\\}} p_t(i) a_t(i)\n$$\n其中$a_t(i)$是由最优反馈控制在时间$t$为状态$i$指定的行动。市场漂移信号由下式给出：\n$$\n\\mu_t = \\beta m_t + \\xi_t\n$$\n其中$\\beta \\in \\mathbb{R}$是一个耦合参数，$\\xi_t \\in \\mathbb{R}$是一个外生漂移输入。在时间$t$，处于状态$i_t$的代理人采取行动$a_t$的单步收益是：\n$$\nr_t(i_t,a_t;\\mu_t) = \\mu_t i_{t+1} - \\lambda i_{t+1}^2 - c |a_t|\n$$\n其中库存惩罚为$\\lambda \\ge 0$，交易成本为$c \\ge 0$，而$i_{t+1}$是由$(i_t,a_t)$产生的交易后库存。代理人的目标是最大化整个时间期内单步收益的折扣总和，折扣因子为$\\gamma \\in (0,1]$，在时间$T$的终端价值为$0$。也就是说，给定一个序列$(\\mu_t)_{t=0}^{T-1}$，最优控制是任何最大化下式的反馈行动序列$a_t(i) \\in \\{-1,0,1\\}$：\n$$\n\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\gamma^t r_t(i_t,a_t;\\mu_t)\\right]\n$$\n受制于库存动态和约束集。如果在给定的状态-时间对$(i,t)$上有多个行动达到相同的最大值，则通过选择$a_t(i)=0$来处理等值情况；如果$a_t(i)=0$不在最大化者之中，则选择$a_t(i)=1$；否则选择$a_t(i)=-1$。\n\nMFG均衡是一个序列$(m_t)_{t=0}^{T-1}$，使得当每个代理人在给定$\\mu_t=\\beta m_t + \\xi_t$的序列$(\\mu_t)_{t=0}^{T-1}$下进行最优控制时，每个时间点导致的平均行动等于$m_t$。给定在$\\{-1,0,1\\}$上的初始群体分布$p_0$，群体运动定律由最优反馈控制通过确定性库存转移导出。\n\n基于此模型，编写一个完整的程序，对下面测试套件中的每个参数集，计算满足上述不动点定义的均衡平均行动序列$(m_0,\\dots,m_{T-1})$。对每个测试用例，输出序列$(m_0,\\dots,m_{T-1})$，并四舍五入到6位小数。\n\n使用以下参数集测试套件，每个由$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0)$指定，其中$\\xi=(\\xi_0,\\xi_1,\\xi_2)$且$p_0=(p_0(-1),p_0(0),p_0(1))$：\n- 测试 $1$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (3, 0.2, 0.95, 0.02, 0.01, (0.0,0.0,0.0), (0.3,0.4,0.3))$。\n- 测试 $2$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (3, 0.0, 0.95, 0.02, 0.01, (0.0,0.0,0.0), (0.3,0.4,0.3))$。\n- 测试 $3$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (3, 0.3, 0.95, 1.0, 0.01, (0.0,0.0,0.0), (0.3,0.4,0.3))$。\n- 测试 $4$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (3, 0.2, 0.95, 0.02, 0.01, (-0.08,-0.02,0.0), (0.2,0.5,0.3))$。\n- 测试 $5$：$(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (3, 0.02, 0.95, 0.02, 0.01, (0.0,0.0,0.0), (0.3,0.4,0.3))$。\n\n您的程序应生成单行输出，包含一个 Python 风格的列表的列表，其中每个内部列表对于相应的测试用例等于$[m_0,m_1,m_2]$，并且所有数字都四舍五入到6位小数。格式必须严格为\n$$\n[[m_0^{(1)},m_1^{(1)},m_2^{(1)}],[m_0^{(2)},m_1^{(2)},m_2^{(2)}],\\dots,[m_0^{(5)},m_1^{(5)},m_2^{(5)}]]\n$$\n逗号后无空格。",
            "solution": "所提出的问题是一个定义明确的、离散时间的、有限状态的最优交易平均场博弈 (MFG)。它在科学上基于已建立的 MFG 理论，在数学上是一致的，并为计算唯一解提供了所有必要的参数和条件。因此，该问题被认为是有效的。任务是找到 MFG 均衡，该均衡被定义为在平均行动轨迹空间上的一个映射的不动点。我们将构建一个基于不动点迭代方法的数值解，该方法在此背景下也被称为虚拟博弈 (fictitious play)。\n\n均衡是平均行动序列$(m_t)_{t=0}^{T-1}$，如果所有代理人都预期此序列并相应地优化其个人交易策略，则他们行动的聚合将再现完全相同的序列。让我们定义一个映射$\\mathcal{F}$，它以一个推测的平均行动序列$m = (m_t)_{t=0}^{T-1}$作为输入，并返回由代理人优化和群体动态产生的平均行动序列$\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$。均衡是该映射的一个不动点，即$m = \\mathcal{F}(m)$。\n\n该算法通过以下步骤进行，并迭代直至收敛：\n\n1.  **解决代表性代理人的最优控制问题**：对于给定的平均行动序列$(m_t)_{t=0}^{T-1}$，代表性代理人的市场漂移信号确定为$\\mu_t = \\beta m_t + \\xi_t$，对于每个时间$t \\in \\{0, \\dots, T-1\\}$。代理人的问题是选择一个行动序列$(a_t)_{t=0}^{T-1}$来最大化总折扣收益。这是一个标准的有限期动态规划问题。设$V_t(i)$为价值函数，表示从时间$t$的状态$i \\in \\{-1, 0, 1\\}$开始可实现的最大收益。该问题通过使用贝尔曼方程的反向归纳法求解。终端价值指定为零，因此对所有$i$都有$V_T(i) = 0$。对于$t = T-1, \\dots, 0$，价值函数计算如下：\n    $$ V_t(i) = \\max_{a \\in \\{-1, 0, 1\\}} \\left\\{ r_t(i, a; \\mu_t) + \\gamma V_{t+1}(i') \\right\\} $$\n    其中$i' = \\min\\{1, \\max\\{-1, i+a\\}\\}$是后续的库存状态。单步收益由$r_t(i, a; \\mu_t) = \\mu_t i' - \\lambda (i')^2 - c|a|$给出。\n    对于每个状态-时间对$(i, t)$，我们首先为每个可能的行动$a \\in \\{-1, 0, 1\\}$计算“Q值”：\n    $$ Q_t(i, a) = \\mu_t i' - \\lambda (i')^2 - c|a| + \\gamma V_{t+1}(i') $$\n    然后从$Q_t(i, a)$的最大化者集合中选择最优行动$a_t(i)$。问题指定了一个严格的平局决胜规则以确保唯一的最优行动：首先，如果$a=0$是一个最大化者，则选择它；如果不是，如果$a=1$是一个最大化者，则选择它；否则，选择$a=-1$。这个过程为所有的$i$和$t$定义了一个唯一的最优策略（反馈控制）$a_t(i)$。\n\n2.  **模拟群体动态**：在确定了最优策略$a_t(i)$后，我们模拟库存状态上群体分布的演化。从$t=0$的给定初始分布$p_0$开始，我们将系统随时间向前演化。在每个步骤$t$，通过对当前群体分布$p_t$上的最优行动进行平均来计算新的平均行动$\\hat{m}_t$：\n    $$ \\hat{m}_t = \\sum_{i \\in \\{-1, 0, 1\\}} p_t(i) a_t(i) $$\n    然后计算下一个时间步的群体分布$p_{t+1}$。由于状态转移是确定性的，状态$i$的每个子群体$p_t(i)$完全转移到下一个状态$i' = \\min\\{1, \\max\\{-1, i+a_t(i)\\}\\}$。因此，通过对所有转移到每个状态的群体的概率求和来找到新的分布：\n    $$ p_{t+1}(j) = \\sum_{i \\in \\{-1,0,1\\} \\text{ s.t. } \\min\\{1,\\max\\{-1,i+a_t(i)\\}\\}=j} p_t(i) $$\n    这个前向传播过程产生一个新的平均行动序列$\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$。\n\n3.  **迭代直至收敛**：计算出的序列$\\hat{m}$是映射$\\mathcal{F}$的输出，因此我们设置$m^{(k+1)} = \\hat{m}$，其中$k$是迭代索引。然后我们重复此过程，将$m^{(k+1)}$反馈回步骤1。迭代从一个初始猜测开始，例如$m^{(0)} = (0, \\dots, 0)$。当平均行动序列收敛时，即当连续迭代值之间的最大绝对差小于一个小的容差$\\epsilon$时，过程终止：$\\|m^{(k+1)} - m^{(k)}\\|_{\\infty}  \\epsilon$。最终得到的序列就是所求的 MFG 均衡。\n\n为了实现，库存状态$\\{-1, 0, 1\\}$被方便地映射到数组索引$\\{0, 1, 2\\}$。然后将这整个过程应用于测试套件中提供的每个参数集。",
            "answer": "```python\nimport numpy as np\n\ndef solve_mfg_equilibrium(T, beta, gamma, c, lambda_val, xi, p0):\n    \"\"\"\n    Computes the Mean Field Game equilibrium for a discrete-time trading model.\n\n    Args:\n        T (int): Time horizon.\n        beta (float): Coupling parameter for market drift.\n        gamma (float): Discount factor.\n        c (float): Transaction cost.\n        lambda_val (float): Inventory holding penalty.\n        xi (tuple): Exogenous drift sequence.\n        p0 (tuple): Initial population distribution over states {-1, 0, 1}.\n\n    Returns:\n        list: The equilibrium mean-action sequence [m_0, m_1, ..., m_{T-1}].\n    \"\"\"\n    states = np.array([-1, 0, 1])\n    actions = np.array([-1, 0, 1])\n\n    # Fixed-point iteration\n    m = np.zeros(T)\n    max_iter = 1000\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        m_old = m.copy()\n\n        # 1. Calculate market drift given the mean-field m\n        mu = beta * m + np.array(xi)\n\n        # 2. Solve agent's optimal control problem (backward induction)\n        V = np.zeros(3)  # V_T(i) = 0 for states {-1, 0, 1}\n        policy = np.zeros((T, 3), dtype=int)  # policy[t, s] where s is state index\n\n        for t in range(T - 1, -1, -1):\n            V_next = V.copy()\n            V_current = np.zeros(3)\n            for s, i in enumerate(states):  # s is state index {0,1,2}, i is state value {-1,0,1}\n                q_values = np.zeros(3)\n                for a_idx, a in enumerate(actions):\n                    i_next = min(1, max(-1, i + a))\n                    s_next = i_next + 1\n                    \n                    reward = mu[t] * i_next - lambda_val * (i_next**2) - c * abs(a)\n                    q = reward + gamma * V_next[s_next]\n                    q_values[a_idx] = q\n\n                # Find optimal action with the specified tie-breaking rule\n                max_q = np.max(q_values)\n                optimal_actions_mask = np.isclose(q_values, max_q, atol=1e-12)\n                \n                if optimal_actions_mask[1]:  # Action a=0 is optimal\n                    best_action = 0\n                elif optimal_actions_mask[2]:  # Action a=1 is optimal\n                    best_action = 1\n                else:  # Action a=-1 is optimal\n                    best_action = -1\n                \n                policy[t, s] = best_action\n                # Index in q_values for action `a` is `a+1`\n                V_current[s] = q_values[best_action + 1]\n            V = V_current\n\n        # 3. Simulate population dynamics (forward pass)\n        p = np.array(p0)\n        m_new = np.zeros(T)\n        for t in range(T):\n            # Calculate mean action at time t\n            # policy[t, s] gives action for state_index s\n            actions_t = policy[t, :]\n            m_new[t] = np.dot(p, actions_t)\n\n            # Evolve population distribution\n            p_next = np.zeros(3)\n            for s, i in enumerate(states): # s is current state index\n                action = policy[t, s]\n                i_next = min(1, max(-1, i + action))\n                s_next = i_next + 1 # next state index\n                p_next[s_next] += p[s]\n            p = p_next\n\n        m = m_new\n\n        # 4. Check for convergence\n        if np.max(np.abs(m - m_old))  tolerance:\n            break\n            \n    return m.tolist()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        # (T, beta, gamma, c, lambda_val, xi, p0)\n        (3, 0.2, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.0, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.3, 0.95, 1.0, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.2, 0.95, 0.02, 0.01, (-0.08, -0.02, 0.0), (0.2, 0.5, 0.3)),\n        (3, 0.02, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n    ]\n    \n    results_str_list = []\n    for params in test_cases:\n        m_equilibrium = solve_mfg_equilibrium(*params)\n        m_rounded = [round(val, 6) for val in m_equilibrium]\n        \n        # Format list to string '[v1,v2,...]' without spaces\n        case_result_str = \"[\" + \",\".join(map(str, m_rounded)) + \"]\"\n        results_str_list.append(case_result_str)\n\n    # Format final output string '[[...],[...],...]'\n    final_output = \"[\" + \",\".join(results_str_list) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "找到一个均衡只是分析的第一步，更关键的问题是：这个均衡是稳定的吗？在现实世界的经济系统中，信息传递往往存在延迟，这可能会对系统的稳定性产生深远影响。这个高级练习  让你有机会探索这一前沿课题，你将学习如何将一个带有延迟的平均场系统转化为一个增广状态空间模型，并利用其系统矩阵的谱半径来判断均衡的稳定性。",
            "id": "2409420",
            "problem": "考虑一个由大量相同个体组成的群体，它们通过平均场博弈（Mean Field Games, MFG）框架进行交互，其中每个代表性个体选择一个控制来影响其状态。状态动态是线性的和确定性的，运行成本惩罚控制努力以及与带有时滞观测的群体均值的偏差。具体来说，每个个体的状态为标量$x(t)$，控制为$u(t)$，群体均值为$m(t) \\equiv \\mathbb{E}[x(t)]$。带有时滞的观测值为$m(t - \\delta)$，其中时滞$\\delta \\ge 0$是已知的。假设一个线性状态动态模型，其具有稳定的线性反馈解和一个二次运行成本函数，该函数会引起状态的局部阻尼以及与带有时滞的均值的耦合。根据线性二次问题中平均场博弈（MFG）的基本推理，系统在均衡状态下简化为一个关于群体均值的闭环线性时滞常微分方程（ODE），其形式为：\n$$\n\\dot{m}(t) = - a m(t) - \\kappa m(t - \\delta)\n$$\n其中$a > 0$表示局部阻尼，$\\kappa \\ge 0$表示与平均场的时滞耦合强度。$a$和$\\kappa$都是有效参数，由对二次成本的线性二次调节器（Linear Quadratic Regulator）类型的最优响应以及均值的定点一致性条件内生决定，但为了计算目的，您可以将$a$和$\\kappa$视为给定值。\n\n您的任务是研究在信息延迟下的平均场均衡的稳定性。在数值上测试稳定性的一种标准方法是使用步长为$h > 0$的前向欧拉方法对时滞常微分方程进行离散化，并将得到的时滞差分方程表示为一个线性增广系统。使用前向欧拉方法，定义离散时间均值序列$\\{ m_t \\}_{t \\in \\mathbb{N}}$，其中$m_t \\approx m(t h)$，将时滞近似为整数步数$d \\equiv \\lceil \\delta / h \\rceil$，并得到以下形式的线性递推关系：\n$$\nm_{t+1} = (1 - a h) m_t - \\kappa h m_{t-d}\n$$\n将此递推关系表示为增广状态$z_t \\in \\mathbb{R}^{d+1}$上的一阶线性系统，其中$z_t$叠加了$m_t, m_{t-1}, \\ldots, m_{t-d}$，并写出相应的伴随矩阵$A(a,\\kappa,\\delta,h) \\in \\mathbb{R}^{(d+1)\\times(d+1)}$。当且仅当矩阵$A(a,\\kappa,\\delta,h)$的谱半径严格小于$1$时，离散化均衡被称为数值稳定（对于给定的离散化方法）。在$d = 0$（即$\\delta = 0$）的特殊情况下，更新规则简化为$m_{t+1} = (1 - (a + \\kappa) h) m_t$，稳定性等价于$| 1 - (a + \\kappa) h |  1$。\n\n从以下基本原理出发：\n- 平均场博弈（MFG）均衡的定义，即个体最优响应与群体均值之间的不动点。\n- 由Hamilton-Jacobi-Bellman (HJB) 方程编码的最优性原理以及线性二次设置中一阶条件的线性性。\n- 用于离散化常微分方程（ODE）的前向欧拉方法。\n- 线性时不变离散时间系统的谱半径稳定性判据。\n\n推导增广系统，并为欧拉离散化时滞系统的谱半径稳定性判据提供理由。然后实现一个程序，该程序：\n- 使用规则$d = \\lceil \\delta / h \\rceil$构建矩阵$A(a,\\kappa,\\delta,h)$。\n- 计算其谱半径。\n- 返回一个布尔值，说明离散化系统是否稳定（定义为谱半径严格小于$1$）。\n\n使用以下参数集$(a,\\kappa,\\delta,h)$的测试套件：\n- 测试 1：$(a,\\kappa,\\delta,h) = (1.0, 0.3, 0.5, 0.01)$。\n- 测试 2（无时滞的边界情况）：$(a,\\kappa,\\delta,h) = (5.0, 5.0, 0.0, 0.2)$。\n- 测试 3（无耦合，时滞无关）：$(a,\\kappa,\\delta,h) = (0.1, 0.0, 2.0, 0.5)$。\n- 测试 4（大时滞耦合）：$(a,\\kappa,\\delta,h) = (0.5, 15.0, 1.0, 0.2)$。\n\n您的程序必须按所列顺序计算每个测试的稳定性布尔值，并生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如“[True,False,True,False]”。不应打印任何额外文本。此问题中的所有量都是无量纲的，因此不需要物理单位。最终答案必须是代码。",
            "solution": "所提出的问题是一个关于离散化线性时滞系统稳定性分析的适定性练习，这是一类线性二次平均场博弈的标准简化形式。该问题具有科学依据，内部一致，并包含了解决该问题所需的所有必要信息。我将开始进行解答。\n\n群体均值$m(t)$的连续时间动态由以下线性时滞常微分方程给出：\n$$\n\\dot{m}(t) = - a m(t) - \\kappa m(t - \\delta)\n$$\n其中$a > 0$是局部阻尼系数，$\\kappa \\ge 0$是与时滞均值的耦合强度，$\\delta \\ge 0$是时间延迟。\n\n为了对该系统进行数值分析，我们使用步长为$h > 0$的前向欧拉方法对其进行离散化。导数$\\dot{m}(t)$被近似为$\\frac{m(t+h) - m(t)}{h}$。令$m_t$表示$m(th)$的离散时间近似。该常微分方程变为：\n$$\n\\frac{m_{t+1} - m_t}{h} \\approx - a m_t - \\kappa m_{t-d}\n$$\n在这里，连续时滞$\\delta$被近似为一个整数步数$d$，定义为$d = \\lceil \\delta / h \\rceil$。这意味着$m(t - \\delta) \\approx m(th - dh) = m_{t-d}$。重新整理该方程，得到线性递推关系：\n$$\nm_{t+1} = m_t - a h m_t - \\kappa h m_{t-d} = (1 - a h) m_t - \\kappa h m_{t-d}\n$$\n这是一个$(d+1)$阶线性齐次差分方程。为了分析其稳定性，我们将其转换为一个关于增广状态向量的一阶线性系统。令状态向量$z_t \\in \\mathbb{R}^{d+1}$定义为当前均值和过去$d$个均值的堆叠：\n$$\nz_t = \\begin{pmatrix} m_t \\\\ m_{t-1} \\\\ m_{t-2} \\\\ \\vdots \\\\ m_{t-d} \\end{pmatrix}\n$$\n我们寻求一个矩阵$A \\in \\mathbb{R}^{(d+1)\\times(d+1)}$，使得$z_{t+1} = A z_t$。在时间$t+1$的状态是：\n$$\nz_{t+1} = \\begin{pmatrix} m_{t+1} \\\\ m_t \\\\ m_{t-1} \\\\ \\vdots \\\\ m_{t-d+1} \\end{pmatrix}\n$$\n我们现在将$z_{t+1}$的每个分量表示为$z_t$分量的线性组合。\n第一个分量$m_{t+1}$由递推关系给出：\n$$\nm_{t+1} = (1 - a h) m_t - \\kappa h m_{t-d}\n$$\n$z_{t+1}$的第二个分量是$m_t$，它就是$z_t$的第一个分量。\n$z_{t+1}$的第三个分量是$m_{t-1}$，它是$z_t$的第二个分量。\n这个模式继续下去，对于$i \\in \\{1, \\ldots, d\\}$，$z_{t+1}$的第$(i+1)$个分量是$z_t$的第$i$个分量。\n\n这定义了线性变换$z_{t+1} = A z_t$，其中$A$是伴随矩阵：\n$$\nA = \\begin{pmatrix}\n1-ah  0  \\cdots  0  -\\kappa h \\\\\n1  0  \\cdots  0  0 \\\\\n0  1  \\cdots  0  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n0  0  \\cdots  1  0\n\\end{pmatrix}\n$$\n矩阵$A$的第一行编码了$m_{t+1}$的递推关系。随后的行编码了过去状态的移位：在步骤$t+1$时的$m_{t-i}$就是在步骤$t$时的$m_{t-i-1}$。\n\n稳定性判据的理由基于离散时间线性时不变系统的性质。系统从初始状态$z_0$的演化由$z_t = A^t z_0$给出。渐近稳定性要求对于任何初始条件$z_0$，当$t \\to \\infty$时，$z_t \\to 0$。这等价于矩阵的幂$A^t$在$t \\to \\infty$时收敛于零矩阵的条件。线性代数和动力系统的一个基本定理指出，$\\lim_{t\\to\\infty} A^t = 0$当且仅当$A$的谱半径（记作$\\rho(A)$）严格小于$1$。谱半径定义为矩阵$A$的特征值的绝对值（对于复数则是模）的最大值：\n$$\n\\rho(A) = \\max_{i} |\\lambda_i| \\quad \\text{其中 } \\lambda_i \\text{ 是 } A \\text{ 的特征值}\n$$\n因此，离散化平均场均衡是稳定的，当且仅当$\\rho(A(a,\\kappa,\\delta,h))  1$。\n\n对于时滞$\\delta = 0$的特殊情况，整数时滞$d = \\lceil 0/h \\rceil = 0$。状态向量变成一个标量$z_t = (m_t) \\in \\mathbb{R}^1$。递推关系简化为：\n$$\nm_{t+1} = (1 - a h) m_t - \\kappa h m_{t-0} = (1 - (a + \\kappa) h) m_t\n$$\n在这种情况下，矩阵$A$是一个$1 \\times 1$的矩阵（一个标量）$A = [1 - (a+\\kappa)h]$。它的唯一特征值是$1 - (a+\\kappa)h$，其谱半径为$\\rho(A) = |1 - (a+\\kappa)h|$。稳定性条件$\\rho(A)  1$正确地简化为$|1 - (a+\\kappa)h|  1$，这验证了问题的假设。\n\n实现部分将根据参数$(a, \\kappa, \\delta, h)$构建矩阵$A$，使用数值库计算其特征值，找出其绝对值的最大值以确定谱半径，并测试该半径是否严格小于$1$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MFG stability problem for a given set of test cases.\n    Constructs the companion matrix for the discretized delay-difference equation\n    and checks if its spectral radius is strictly less than 1.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, kappa, delta, h)\n        (1.0, 0.3, 0.5, 0.01),  # Test 1\n        (5.0, 5.0, 0.0, 0.2),   # Test 2 (boundary case with no delay)\n        (0.1, 0.0, 2.0, 0.5),   # Test 3 (no coupling, delay irrelevant)\n        (0.5, 15.0, 1.0, 0.2),  # Test 4 (large delayed coupling)\n    ]\n\n    results = []\n    for a, kappa, delta, h in test_cases:\n        # Calculate the integer delay d from the continuous delay delta and step size h.\n        # d must be an integer, using ceiling function as specified.\n        # For delta = 0, np.ceil(0) correctly returns 0.\n        if h = 0:\n            # Step size must be positive, invalid case according to problem statement.\n            # Handle defensively although test data is valid.\n            raise ValueError(\"Step size h must be positive.\")\n            \n        d = int(np.ceil(delta / h))\n\n        # The dimension of the augmented system is (d+1).\n        dim = d + 1\n        \n        # Construct the companion matrix A.\n        if d == 0:\n            # Special case for no delay (d=0). The system is 1D.\n            # The recurrence is m_{t+1} = (1 - (a+kappa)*h) * m_t.\n            # The matrix A is a 1x1 scalar.\n            A = np.array([[1.0 - (a + kappa) * h]])\n        else:\n            # General case for d > 0.\n            A = np.zeros((dim, dim))\n            \n            # First row encodes the recurrence relation:\n            # m_{t+1} = (1 - a*h)*m_t - kappa*h*m_{t-d}\n            A[0, 0] = 1.0 - a * h\n            A[0, d] = -kappa * h\n            \n            # The other rows create the shift property: m_{t-i} becomes m_{t-i+1}.\n            # This is achieved by placing a 1 on the sub-diagonal.\n            # A[i, i-1] = 1 for i = 1, ..., d.\n            # A numpy-idiomatic way to set the sub-diagonal.\n            for i in range(1, dim):\n                A[i, i - 1] = 1.0\n        \n        # Calculate the eigenvalues of the matrix A.\n        eigenvalues = np.linalg.eigvals(A)\n        \n        # The spectral radius is the maximum of the absolute values of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        # The discretized system is stable if the spectral radius is strictly less than 1.\n        is_stable = spectral_radius  1.0\n        \n        results.append(is_stable)\n\n    # Final print statement in the exact required format.\n    # Convert boolean list to string representation as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver.\nsolve()\n```"
        }
    ]
}