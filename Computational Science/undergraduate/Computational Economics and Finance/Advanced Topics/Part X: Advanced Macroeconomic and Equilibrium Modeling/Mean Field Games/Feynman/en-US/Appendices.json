{
    "hands_on_practices": [
        {
            "introduction": "We begin with a foundational model in Mean Field Games: the linear-quadratic (LQ) framework. This exercise models the growth of cities as agents whose decisions are based on their size relative to the average, a classic economic concept. By working through this problem , you will see how the LQ structure allows for an elegant solution via the Riccati equation, providing a clear window into optimal feedback strategies and the evolution of the population's variance.",
            "id": "2409391",
            "problem": "Consider a continuum of cities indexed by $i$ on discrete times $t \\in \\{0,1,\\dots,T\\}$. Each city $i$ has a log-size $x_t^i \\in \\mathbb{R}$. Let $m_t \\in \\mathbb{R}$ denote the cross-sectional mean of $\\{x_t^i\\}_{i}$ at time $t$. City $i$ controls its growth by choosing a control $u_t^i \\in \\mathbb{R}$ that affects its next-period log-size through the deterministic law of motion\n$$\nx_{t+1}^i \\;=\\; (1-\\beta)\\,x_t^i \\;+\\; \\beta\\,m_t \\;+\\; u_t^i,\n$$\nwhere $\\beta \\in [0,1]$ is a given parameter. The initial cross-sectional distribution of log-sizes is such that $x_0^i$ has mean $m_0 \\in \\mathbb{R}$ and variance $s_0^2 \\ge 0$. For a given deterministic mean-field path $\\{m_t\\}_{t=0}^T$, each city $i$ minimizes the quadratic objective\n$$\nJ^i \\;=\\; \\sum_{t=0}^{T-1} \\left(\\tfrac{1}{2}\\,q\\,\\big(x_t^i - m_t\\big)^2 \\;+\\; \\tfrac{1}{2}\\,r\\,\\big(u_t^i\\big)^2\\right) \\;+\\; \\tfrac{1}{2}\\,q_T\\,\\big(x_T^i - m_T\\big)^2,\n$$\nwhere $q \\ge 0$, $r > 0$, and $q_T \\ge 0$ are given constants. A mean-field equilibrium is a sequence $\\{m_t\\}_{t=0}^T$ and a measurable feedback law $u_t^i = \\pi_t(x_t^i,m_t)$ such that, for every city $i$, the control $u_t^i$ minimizes $J^i$ given $\\{m_t\\}_{t=0}^T$, and the induced cross-sectional mean satisfies the fixed-point condition $m_t = \\mathbb{E}[x_t^i]$ for all $t \\in \\{0,1,\\dots,T\\}$ when all cities use the optimal policy and $x_0^i$ has mean $m_0$ and variance $s_0^2$.\n\nYour task is to compute, for each parameter set in the test suite below, the following two quantities that characterize the unique mean-field equilibrium under the model above:\n- The optimal initial feedback coefficient $K_0 \\in \\mathbb{R}$ defined by the linear relation $u_0^i = -K_0\\,(x_0^i - m_0)$ at time $t=0$ under the equilibrium mean-field path.\n- The cross-sectional variance of log-sizes at the terminal time, $\\mathrm{Var}(x_T^i)$.\n\nAssume there are no exogenous shocks beyond the deterministic dynamics given above. Express every reported number as a real number rounded to six decimal places. All angles are dimensionless. No physical units are involved.\n\nTest suite of parameter values to evaluate:\n- Case $1$: $T = 10$, $\\beta = 0.3$, $q = 1.0$, $r = 0.5$, $q_T = 1.0$, $m_0 = 2.0$, $s_0^2 = 1.0$.\n- Case $2$: $T = 5$, $\\beta = 0.0$, $q = 1.0$, $r = 1.0$, $q_T = 1.0$, $m_0 = 0.0$, $s_0^2 = 0.25$.\n- Case $3$: $T = 20$, $\\beta = 0.9$, $q = 0.5$, $r = 1.0$, $q_T = 0.5$, $m_0 = -1.0$, $s_0^2 = 2.0$.\n- Case $4$: $T = 8$, $\\beta = 0.2$, $q = 1.0$, $r = 0.01$, $q_T = 1.0$, $m_0 = 1.0$, $s_0^2 = 3.0$.\n- Case $5$: $T = 12$, $\\beta = 0.4$, $q = 0.01$, $r = 1.0$, $q_T = 0.01$, $m_0 = 0.5$, $s_0^2 = 1.5$.\n- Case $6$: $T = 7$, $\\beta = 0.5$, $q = 1.0$, $r = 1.0$, $q_T = 1.0$, $m_0 = 3.0$, $s_0^2 = 0.0$.\n\nFinal output format:\n- For each case, output a two-element list $[K_0,\\mathrm{Var}(x_T^i)]$ with both entries rounded to six decimal places.\n- Aggregate the results for all cases into a single line that is a comma-separated list enclosed in square brackets, for example, $[[a_1,b_1],[a_2,b_2],\\dots]$ without any additional text.",
            "solution": "The supplied problem is subjected to validation.\n\nGivens are extracted verbatim:\n- A continuum of cities indexed by $i$.\n- Discrete time $t \\in \\{0, 1, \\dots, T\\}$.\n- City $i$ log-size: $x_t^i \\in \\mathbb{R}$.\n- Cross-sectional mean log-size: $m_t = \\mathbb{E}[x_t^i]$.\n- City $i$ control: $u_t^i \\in \\mathbb{R}$.\n- Law of motion: $x_{t+1}^i = (1-\\beta)x_t^i + \\beta m_t + u_t^i$, with $\\beta \\in [0,1]$.\n- Initial distribution of $x_0^i$: mean $m_0 \\in \\mathbb{R}$, variance $s_0^2 \\ge 0$.\n- Objective function for city $i$: $J^i = \\sum_{t=0}^{T-1} (\\frac{1}{2}q(x_t^i - m_t)^2 + \\frac{1}{2}r(u_t^i)^2) + \\frac{1}{2}q_T(x_T^i - m_T)^2$, with $q \\ge 0$, $r > 0$, $q_T \\ge 0$.\n- Mean-field equilibrium definition: A path $\\{m_t\\}_{t=0}^T$ and a feedback law $u_t^i = \\pi_t(x_t^i, m_t)$ such that $u_t^i$ is optimal for each city $i$ given $\\{m_t\\}$, and the consistency condition $m_t = \\mathbb{E}[x_t^i]$ holds for all $t$.\n- Task: Compute the initial feedback coefficient $K_0$ from $u_0^i = -K_0(x_0^i - m_0)$ and the terminal variance $\\mathrm{Var}(x_T^i)$.\n\nThe problem is a well-defined standard problem in the theory of mean-field games, specifically a linear-quadratic model in discrete time. It is scientifically grounded, well-posed, and expressed in objective mathematical language. All parameters required for a solution are provided. The problem is thus deemed valid.\n\nThe solution proceeds as follows. First, we characterize the mean-field equilibrium. An individual city $i$ solves a linear-quadratic optimal control problem, taking the mean-field path $\\{m_t\\}_{t=0}^T$ as given. The equilibrium requires that the aggregate behavior of cities, following their optimal strategies, reproduces the given mean-field path.\n\nLet $\\tilde{x}_t^i = x_t^i - m_t$ be the deviation of city $i$'s log-size from the mean. The law of motion for this deviation is:\n$$\n\\tilde{x}_{t+1}^i = x_{t+1}^i - m_{t+1} = \\big((1-\\beta)x_t^i + \\beta m_t + u_t^i\\big) - m_{t+1}\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)(\\tilde{x}_t^i + m_t) + \\beta m_t + u_t^i - m_{t+1} = (1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}\n$$\nThe cost function becomes:\n$$\nJ^i = \\sum_{t=0}^{T-1} \\left(\\tfrac{1}{2}q(\\tilde{x}_t^i)^2 + \\tfrac{1}{2}r(u_t^i)^2\\right) + \\tfrac{1}{2}q_T(\\tilde{x}_T^i)^2\n$$\nThis is a standard linear-quadratic regulator problem, where $(m_t - m_{t+1})$ acts as an exogenous disturbance. The value function is quadratic, $V_t(\\tilde{x}_t^i) = \\frac{1}{2}P_t(\\tilde{x}_t^i)^2 + \\dots$, where terms independent of $\\tilde{x}_t^i$ are omitted. The Hamilton-Jacobi-Bellman equation for time $t$ is:\n$$\nV_t(\\tilde{x}_t^i) = \\min_{u_t^i} \\left\\{ \\tfrac{1}{2}q(\\tilde{x}_t^i)^2 + \\tfrac{1}{2}r(u_t^i)^2 + V_{t+1}(\\tilde{x}_{t+1}^i) \\right\\}\n$$\nMinimizing the bracketed term with respect to $u_t^i$ yields the first-order condition:\n$$\nr u_t^i + \\frac{\\partial V_{t+1}}{\\partial \\tilde{x}_{t+1}^i} \\frac{\\partial \\tilde{x}_{t+1}^i}{\\partial u_t^i} = r u_t^i + P_{t+1}\\tilde{x}_{t+1}^i(1) = r u_t^i + P_{t+1}((1-\\beta)\\tilde{x}_t^i + u_t^i + m_t - m_{t+1}) = 0\n$$\nSolving for the optimal control $u_t^i$:\n$$\n(r+P_{t+1})u_t^i = -P_{t+1}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\n$$\nu_t^i = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\tilde{x}_t^i + m_t - m_{t+1})\n$$\nThe mean-field consistency condition is $\\mathbb{E}[x_t^i] = m_t$, which implies $\\mathbb{E}[\\tilde{x}_t^i] = 0$ for all $t$. Taking the expectation of the original state dynamics, $m_{t+1} = (1-\\beta)m_t + \\beta m_t + \\mathbb{E}[u_t^i] = m_t + \\mathbb{E}[u_t^i]$, which implies $\\mathbb{E}[u_t^i] = m_{t+1} - m_t$.\nTaking the expectation of the optimal control law:\n$$\n\\mathbb{E}[u_t^i] = -\\frac{P_{t+1}}{r+P_{t+1}}((1-\\beta)\\mathbb{E}[\\tilde{x}_t^i] + m_t - m_{t+1}) = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1})\n$$\nEquating the two expressions for $\\mathbb{E}[u_t^i]$:\n$$\nm_{t+1} - m_t = -\\frac{P_{t+1}}{r+P_{t+1}}(m_t - m_{t+1}) \\implies \\left(1 + \\frac{P_{t+1}}{r+P_{t+1}}\\right)(m_{t+1} - m_t) = 0\n$$\nSince $r > 0$ and $P_{t+1} \\ge 0$ (as it represents cost), the term in the parenthesis is strictly positive. Thus, it must be that $m_{t+1} - m_t = 0$ for all $t$. This proves that the equilibrium mean-field path must be constant: $m_t = m_0$ for all $t = 0, \\dots, T$.\n\nWith $m_t$ constant, the problem simplifies significantly. The dynamics of the deviation become $\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i + u_t^i$. The problem is now a standard deterministic LQR problem for the deviation state $\\tilde{x}_t^i$. The optimal control simplifies to a linear feedback law:\n$$\nu_t^i = -\\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i = -K_t \\tilde{x}_t^i\n$$\nwhere $K_t = \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}$ is the feedback gain. The coefficients $P_t$ are determined by the discrete-time Riccati equation, solved backwards from $t=T-1$ to $t=0$:\n$$\nP_t = q + (1-\\beta)^2 P_{t+1} - \\frac{((1-\\beta)P_{t+1})^2}{r+P_{t+1}} = q + \\frac{r P_{t+1}(1-\\beta)^2}{r+P_{t+1}}\n$$\nThe recursion starts with the terminal condition $P_T = q_T$, derived from the terminal cost.\nThe quantity of interest $K_0$ is then computed as $K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$, requiring the calculation of the sequence $\\{P_t\\}_{t=1}^T$.\n\nNext, we derive the evolution of the cross-sectional variance, $s_t^2 = \\mathrm{Var}(x_t^i) = \\mathrm{Var}(\\tilde{x}_t^i)$. Substituting the optimal control into the dynamics for the deviation:\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\tilde{x}_t^i - K_t \\tilde{x}_t^i = \\left((1-\\beta) - \\frac{P_{t+1}(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\n$$\n\\tilde{x}_{t+1}^i = (1-\\beta)\\left(1 - \\frac{P_{t+1}}{r+P_{t+1}}\\right)\\tilde{x}_t^i = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)\\tilde{x}_t^i\n$$\nThe variance evolves according to a forward recursion. Let $v_t = s_t^2$:\n$$\nv_{t+1} = \\mathrm{Var}(\\tilde{x}_{t+1}^i) = \\mathbb{E}\\left[\\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\tilde{x}_t^i\\right)^2\\right] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 \\mathbb{E}[(\\tilde{x}_t^i)^2] = \\left(\\frac{r(1-\\beta)}{r+P_{t+1}}\\right)^2 v_t\n$$\nStarting with the given initial variance $v_0 = s_0^2$, we can compute the sequence $\\{v_t\\}_{t=1}^T$ in a forward pass, using the previously computed sequence $\\{P_t\\}$. The terminal variance is $\\mathrm{Var}(x_T^i) = v_T$.\n\nThe algorithm is as follows:\n1. Initialize an array $P$ of size $T+1$ with $P_T = q_T$.\n2. Solve the Riccati equation by backward iteration from $t = T-1$ down to $t=0$ to populate the array $P$.\n3. Compute the initial feedback gain $K_0 = \\frac{P_1(1-\\beta)}{r+P_1}$. For $T=0$, this is not defined but all test cases have $T \\ge 1$.\n4. Initialize the variance $v_0 = s_0^2$.\n5. Compute the variance evolution by forward iteration from $t=0$ to $t=T-1$ using the stored values of $P_{t+1}$.\n6. The final result for each case is $[K_0, v_T]$.\n\nThis procedure is implemented below for the provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a discrete-time linear-quadratic mean-field game for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, beta, q, r, q_T, m_0, s0_sq)\n        (10, 0.3, 1.0, 0.5, 1.0, 2.0, 1.0),\n        (5, 0.0, 1.0, 1.0, 1.0, 0.0, 0.25),\n        (20, 0.9, 0.5, 1.0, 0.5, -1.0, 2.0),\n        (8, 0.2, 1.0, 0.01, 1.0, 1.0, 3.0),\n        (12, 0.4, 0.01, 1.0, 0.01, 0.5, 1.5),\n        (7, 0.5, 1.0, 1.0, 1.0, 3.0, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        T, beta, q, r, q_T, m_0, s0_sq = case\n        \n        # 1. Backward pass: Solve the Riccati equation for P_t\n        # P is an array of size T+1 for P_0, ..., P_T\n        P = np.zeros(T + 1)\n        P[T] = q_T\n        \n        # The equation for P_t depends on P_{t+1}. We iterate t from T-1 down to 0.\n        for t in range(T - 1, -1, -1):\n            P_next = P[t + 1]\n            term = (r * P_next * (1 - beta)**2) / (r + P_next)\n            P[t] = q + term\n\n        # 2. Calculate the initial feedback coefficient K_0\n        # K_0 depends on P_1. If T=0, there is no P_1 and no control u_0.\n        # Problem statement implies T >= 1 for control to be meaningful.\n        if T > 0:\n            K0 = (P[1] * (1 - beta)) / (r + P[1])\n        else:\n            # If T=0, there is no control phase, so K_0 is not applicable.\n            # Based on the problem structure, K_0=0 is a reasonable assignment.\n            K0 = 0.0\n\n        # 3. Forward pass: Evolve the cross-sectional variance\n        var = s0_sq\n        \n        # The evolution of var_t to var_{t+1} depends on P_{t+1}\n        for t in range(T):\n            P_next = P[t + 1]\n            factor = (r * (1 - beta)) / (r + P_next)\n            var = (factor**2) * var\n            \n        var_T = var\n\n        # Append rounded results for the current case\n        results.append([round(K0, 6), round(var_T, 6)])\n\n    # Format the final output string as specified.\n    # e.g., [[val1, val2], [val3, val4], ...]\n    final_output = \"[\" + \",\".join([f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "While linear-quadratic models are insightful, most real-world problems are not so cleanly structured. This next practice  plunges you into a more general, discrete-time trading scenario where agents have a finite set of actions. You will implement the cornerstone numerical method for solving MFGs, known as fictitious play, by iterating between solving an agent's control problem and simulating the population's evolution to find a self-consistent equilibrium.",
            "id": "2409414",
            "problem": "Consider a discrete-time Mean Field Game (MFG) of trading with a finite horizon $T \\in \\mathbb{N}$. Time is indexed by $t \\in \\{0,1,\\dots,T-1\\}$. Each agent has an inventory state $i_t \\in \\{-1,0,1\\}$ at time $t$. At each time $t$, the agent chooses a control (action) $a_t \\in \\{-1,0,1\\}$ interpreted as \"sell\" ($-1$), \"hold\" ($0$), or \"buy\" ($+1$), subject to the inventory constraint that the next inventory is\n$$\ni_{t+1} = \\min\\{1,\\max\\{-1,\\, i_t + a_t\\}\\}.\n$$\nLet $p_t(i)$ denote the population distribution over inventory states $i \\in \\{-1,0,1\\}$ at time $t$, with $p_t(-1)+p_t(0)+p_t(1)=1$. The population-mean action at time $t$ is\n$$\nm_t \\equiv \\sum_{i \\in \\{-1,0,1\\}} p_t(i)\\, a_t(i),\n$$\nwhere $a_t(i)$ is the action prescribed for state $i$ at time $t$ by an optimal feedback control. The market drift signal is given by\n$$\n\\mu_t = \\beta\\, m_t + \\xi_t,\n$$\nwhere $\\beta \\in \\mathbb{R}$ is a coupling parameter and $\\xi_t \\in \\mathbb{R}$ is an exogenous drift input. The one-step payoff for an agent taking action $a_t$ in state $i_t$ at time $t$ is\n$$\nr_t(i_t,a_t;\\mu_t) = \\mu_t\\, i_{t+1} - \\lambda\\, i_{t+1}^2 - c\\, |a_t|,\n$$\nwith inventory penalty $\\lambda \\ge 0$ and transaction cost $c \\ge 0$, and where $i_{t+1}$ is the post-trade inventory resulting from $(i_t,a_t)$. Agents maximize the discounted sum of one-step payoffs over the horizon with discount factor $\\gamma \\in (0,1]$ and terminal value $0$ at time $T$. That is, given a sequence $(\\mu_t)_{t=0}^{T-1}$, an optimal control is any sequence of feedback actions $a_t(i) \\in \\{-1,0,1\\}$ that maximizes\n$$\n\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\gamma^t\\, r_t(i_t,a_t;\\mu_t)\\right],\n$$\nsubject to the inventory dynamics and the constraint set. If multiple actions attain the same maximal value at a given state-time pair $(i,t)$, resolve the tie by choosing $a_t(i)=0$; if $a_t(i)=0$ is not among the maximizers, choose $a_t(i)=1$; otherwise choose $a_t(i)=-1$.\n\nAn MFG equilibrium is a sequence $(m_t)_{t=0}^{T-1}$ such that, when each agent optimally controls given $(\\mu_t)_{t=0}^{T-1}$ with $\\mu_t=\\beta m_t + \\xi_t$, the induced mean action at each time equals $m_t$. The initial population distribution $p_0$ over $\\{-1,0,1\\}$ is given and the population law of motion is induced by the optimal feedback control via the deterministic inventory transitions.\n\nGiven this model, write a complete program that, for each parameter set in the test suite below, computes an equilibrium mean-action sequence $(m_t)_{t=0}^{T-1}$ that satisfies the fixed-point definition above. For each test case, output the sequence $(m_0,\\dots,m_{T-1})$ rounded to $6$ decimal places.\n\nUse the following test suite of parameter sets, each specified by $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0)$, where $\\xi=(\\xi_0,\\xi_1,\\xi_2)$ and $p_0=(p_0(-1),p_0(0),p_0(1))$:\n- Test $1$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.2,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n- Test $2$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.0,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n- Test $3$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.3,\\,0.95,\\,1.0,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n- Test $4$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.2,\\,0.95,\\,0.02,\\,0.01,\\;(-0.08,-0.02,0.0),\\,(0.2,0.5,0.3)\\,)$.\n- Test $5$: $(T,\\beta,\\gamma,c,\\lambda,\\xi,p_0) = (\\,3,\\,0.02,\\,0.95,\\,0.02,\\,0.01,\\,(0.0,0.0,0.0),\\,(0.3,0.4,0.3)\\,)$.\n\nYour program should produce a single line of output containing the results as a Python-style list of lists, with each inner list equal to $[m_0,m_1,m_2]$ for the corresponding test case, and with all numbers rounded to $6$ decimal places. The format must be exactly\n$$\n[[m_0^{(1)},m_1^{(1)},m_2^{(1)}],[m_0^{(2)},m_1^{(2)},m_2^{(2)}],\\dots,[m_0^{(5)},m_1^{(5)},m_2^{(5)}]]\n$$\nwith no spaces after commas.",
            "solution": "The problem presented is a well-defined, discrete-time, finite-state Mean Field Game (MFG) of optimal trading. It is scientifically grounded in the established theory of MFGs, is mathematically consistent, and provides all necessary parameters and conditions for a unique solution to be computed. The problem is therefore deemed valid. The task is to find the MFG equilibrium, which is defined as a fixed point of a mapping on the space of mean-action trajectories. We shall construct a numerical solution based on the method of fixed-point iteration, also known as fictitious play in this context.\n\nThe equilibrium is a sequence of mean actions $(m_t)_{t=0}^{T-1}$ such that if all agents expect this sequence and optimize their individual trading strategies accordingly, the aggregation of their actions reproduces the very same sequence. Let us define a mapping $\\mathcal{F}$ that takes a conjectured mean-action sequence $m = (m_t)_{t=0}^{T-1}$ as input and returns the resulting mean-action sequence $\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$ that arises from agent optimization and population dynamics. The equilibrium is a fixed point of this map, i.e., $m = \\mathcal{F}(m)$.\n\nThe algorithm proceeds via the following steps, iterated until convergence:\n\n1.  **Solve the Representative Agent's Optimal Control Problem**: For a given mean-action sequence $(m_t)_{t=0}^{T-1}$, the market drift signal for the representative agent is determined as $\\mu_t = \\beta m_t + \\xi_t$ for each time $t \\in \\{0, \\dots, T-1\\}$. The agent's problem is to choose a sequence of actions $(a_t)_{t=0}^{T-1}$ to maximize the total discounted payoff. This is a standard finite-horizon dynamic programming problem. Let $V_t(i)$ be the value function, representing the maximum achievable payoff starting from state $i \\in \\{-1, 0, 1\\}$ at time $t$. The problem is solved via backward induction using the Bellman equation. The terminal value is specified as zero, so $V_T(i) = 0$ for all $i$. For $t = T-1, \\dots, 0$, the value function is computed as:\n    $$ V_t(i) = \\max_{a \\in \\{-1, 0, 1\\}} \\left\\{ r_t(i, a; \\mu_t) + \\gamma V_{t+1}(i') \\right\\} $$\n    where $i' = \\min\\{1, \\max\\{-1, i+a\\}\\}$ is the subsequent inventory state. The single-step payoff is given by $r_t(i, a; \\mu_t) = \\mu_t i' - \\lambda (i')^2 - c|a|$.\n    For each state-time pair $(i, t)$, we first compute the \"Q-value\" for each possible action $a \\in \\{-1, 0, 1\\}$:\n    $$ Q_t(i, a) = \\mu_t i' - \\lambda (i')^2 - c|a| + \\gamma V_{t+1}(i') $$\n    The optimal action $a_t(i)$ is then chosen from the set of maximizers of $Q_t(i, a)$. The problem specifies a strict tie-breaking rule to ensure a unique optimal action: first, select $a=0$ if it is a maximizer; if not, select $a=1$ if it is a maximizer; otherwise, select $a=-1$. This procedure defines a unique optimal policy (feedback control) $a_t(i)$ for all $i$ and $t$.\n\n2.  **Simulate the Population Dynamics**: With the optimal policy $a_t(i)$ determined, we simulate the evolution of the population distribution over the inventory states. Starting with the given initial distribution $p_0$ at $t=0$, we evolve the system forward in time. At each step $t$, the new mean action $\\hat{m}_t$ is computed by averaging the optimal actions over the current population distribution $p_t$:\n    $$ \\hat{m}_t = \\sum_{i \\in \\{-1, 0, 1\\}} p_t(i) a_t(i) $$\n    The population distribution for the next time step, $p_{t+1}$, is then calculated. Since the state transitions are deterministic, each sub-population $p_t(i)$ at state $i$ moves entirely to the next state $i' = \\min\\{1, \\max\\{-1, i+a_t(i)\\}\\}$. Thus, the new distribution is found by summing the probabilities of all populations that transition into each state:\n    $$ p_{t+1}(j) = \\sum_{i \\in \\{-1,0,1\\} \\text{ s.t. } \\min\\{1,\\max\\{-1,i+a_t(i)\\}\\}=j} p_t(i) $$\n    This forward pass yields a new mean-action sequence $\\hat{m} = (\\hat{m}_t)_{t=0}^{T-1}$.\n\n3.  **Iterate to Convergence**: The computed sequence $\\hat{m}$ is the output of the map $\\mathcal{F}$, so we set $m^{(k+1)} = \\hat{m}$, where $k$ is the iteration index. We then repeat the process, feeding $m^{(k+1)}$ back into Step 1. The iteration starts with an initial guess, such as $m^{(0)} = (0, \\dots, 0)$. The process is terminated when the mean-action sequence converges, i.e., when the maximum absolute difference between consecutive iterates falls below a small tolerance $\\epsilon$: $\\|m^{(k+1)} - m^{(k)}\\|_{\\infty} < \\epsilon$. The resulting sequence is the desired MFG equilibrium.\n\nFor implementation, the inventory states $\\{-1, 0, 1\\}$ are conveniently mapped to array indices $\\{0, 1, 2\\}$. This entire procedure is then applied to each parameter set provided in the test suite.",
            "answer": "```python\nimport numpy as np\n\ndef solve_mfg_equilibrium(T, beta, gamma, c, lambda_val, xi, p0):\n    \"\"\"\n    Computes the Mean Field Game equilibrium for a discrete-time trading model.\n\n    Args:\n        T (int): Time horizon.\n        beta (float): Coupling parameter for market drift.\n        gamma (float): Discount factor.\n        c (float): Transaction cost.\n        lambda_val (float): Inventory holding penalty.\n        xi (tuple): Exogenous drift sequence.\n        p0 (tuple): Initial population distribution over states {-1, 0, 1}.\n\n    Returns:\n        list: The equilibrium mean-action sequence [m_0, m_1, ..., m_{T-1}].\n    \"\"\"\n    states = np.array([-1, 0, 1])\n    actions = np.array([-1, 0, 1])\n\n    # Fixed-point iteration\n    m = np.zeros(T)\n    max_iter = 1000\n    tolerance = 1e-12\n\n    for _ in range(max_iter):\n        m_old = m.copy()\n\n        # 1. Calculate market drift given the mean-field m\n        mu = beta * m + np.array(xi)\n\n        # 2. Solve agent's optimal control problem (backward induction)\n        V = np.zeros(3)  # V_T(i) = 0 for states {-1, 0, 1}\n        policy = np.zeros((T, 3), dtype=int)  # policy[t, s] where s is state index\n\n        for t in range(T - 1, -1, -1):\n            V_next = V.copy()\n            V_current = np.zeros(3)\n            for s, i in enumerate(states):  # s is state index {0,1,2}, i is state value {-1,0,1}\n                q_values = np.zeros(3)\n                for a_idx, a in enumerate(actions):\n                    i_next = min(1, max(-1, i + a))\n                    s_next = i_next + 1\n                    \n                    reward = mu[t] * i_next - lambda_val * (i_next**2) - c * abs(a)\n                    q = reward + gamma * V_next[s_next]\n                    q_values[a_idx] = q\n\n                # Find optimal action with the specified tie-breaking rule\n                max_q = np.max(q_values)\n                optimal_actions_mask = np.isclose(q_values, max_q, atol=1e-12)\n                \n                if optimal_actions_mask[1]:  # Action a=0 is optimal\n                    best_action = 0\n                elif optimal_actions_mask[2]:  # Action a=1 is optimal\n                    best_action = 1\n                else:  # Action a=-1 is optimal\n                    best_action = -1\n                \n                policy[t, s] = best_action\n                # Index in q_values for action `a` is `a+1`\n                V_current[s] = q_values[best_action + 1]\n            V = V_current\n\n        # 3. Simulate population dynamics (forward pass)\n        p = np.array(p0)\n        m_new = np.zeros(T)\n        for t in range(T):\n            # Calculate mean action at time t\n            # policy[t, s] gives action for state_index s\n            actions_t = policy[t, :]\n            m_new[t] = np.dot(p, actions_t)\n\n            # Evolve population distribution\n            p_next = np.zeros(3)\n            for s, i in enumerate(states): # s is current state index\n                action = policy[t, s]\n                i_next = min(1, max(-1, i + action))\n                s_next = i_next + 1 # next state index\n                p_next[s_next] += p[s]\n            p = p_next\n\n        m = m_new\n\n        # 4. Check for convergence\n        if np.max(np.abs(m - m_old)) < tolerance:\n            break\n            \n    return m.tolist()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results in the required format.\n    \"\"\"\n    test_cases = [\n        # (T, beta, gamma, c, lambda_val, xi, p0)\n        (3, 0.2, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.0, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.3, 0.95, 1.0, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n        (3, 0.2, 0.95, 0.02, 0.01, (-0.08, -0.02, 0.0), (0.2, 0.5, 0.3)),\n        (3, 0.02, 0.95, 0.02, 0.01, (0.0, 0.0, 0.0), (0.3, 0.4, 0.3)),\n    ]\n    \n    results_str_list = []\n    for params in test_cases:\n        m_equilibrium = solve_mfg_equilibrium(*params)\n        m_rounded = [round(val, 6) for val in m_equilibrium]\n        \n        # Format list to string '[v1,v2,...]' without spaces\n        case_result_str = \"[\" + \",\".join(map(str, m_rounded)) + \"]\"\n        results_str_list.append(case_result_str)\n\n    # Format final output string '[[...],[...],...]'\n    final_output = \"[\" + \",\".join(results_str_list) + \"]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Finding an equilibrium is only half the story; we must also ask if it is stable. This final exercise  explores this crucial question by introducing information delays, a pervasive feature of real markets. You will learn to analyze the stability of a mean-field equilibrium by discretizing its dynamics, constructing the corresponding system matrix, and applying the spectral radius criterion to determine if small perturbations will die out or grow.",
            "id": "2409420",
            "problem": "Consider a large population of identical agents interacting through a Mean Field Games (MFG) framework, where each representative agent chooses a control to influence her state. The state dynamics are linear and deterministic, and the running cost penalizes both the control effort and deviations from the population mean that is observed with a delay. Specifically, each agent has scalar state $x(t)$ and control $u(t)$, and the population mean is $m(t) \\equiv \\mathbb{E}[x(t)]$. The delayed observation is $m(t - \\delta)$, with known delay $\\delta \\ge 0$. Assume a linear state dynamics model with a stabilizing linear feedback solution and a quadratic running cost function that induces local damping of the state and coupling to the delayed mean. From fundamental Mean Field Games (MFG) reasoning for linear-quadratic problems, the system at equilibrium reduces to a closed-loop linear time-delay Ordinary Differential Equation (ODE) for the population mean of the form\n$$\n\\dot{m}(t) = - a \\, m(t) - \\kappa \\, m(t - \\delta),\n$$\nwith $a > 0$ denoting local damping and $\\kappa \\ge 0$ denoting the strength of delayed coupling to the mean field. Both $a$ and $\\kappa$ are effective parameters determined endogenously by Linear Quadratic Regulator-type best responses to the quadratic cost and by the fixed-point consistency condition for the mean, but for computational purposes you may treat $a$ and $\\kappa$ as given.\n\nYour task is to investigate stability of the mean-field equilibrium under delayed information. A standard way to test stability numerically is to discretize the time-delay ODE using the forward Euler method with step size $h > 0$, and to represent the resulting delay-difference equation as a linear augmented system. Using the forward Euler method, define the discrete-time mean sequence $\\{ m_t \\}_{t \\in \\mathbb{N}}$ with $m_t \\approx m(t h)$, approximate the delay by an integer number of steps $d \\equiv \\lceil \\delta / h \\rceil$, and obtain a linear recurrence of the form\n$$\nm_{t+1} = (1 - a h) \\, m_t - \\kappa h \\, m_{t-d}.\n$$\nRepresent this recurrence as a first-order linear system on the augmented state $z_t \\in \\mathbb{R}^{d+1}$ that stacks $m_t, m_{t-1}, \\ldots, m_{t-d}$, and write the corresponding companion matrix $A(a,\\kappa,\\delta,h) \\in \\mathbb{R}^{(d+1)\\times(d+1)}$. The discretized equilibrium is called numerically stable (for the given discretization) if and only if the spectral radius of $A(a,\\kappa,\\delta,h)$ is strictly less than $1$. In the special case $d = 0$ (that is, $\\delta = 0$), the update reduces to $m_{t+1} = \\big(1 - (a + \\kappa) h\\big) m_t$, and stability is equivalent to $\\lvert 1 - (a + \\kappa) h \\rvert < 1$.\n\nStarting from the following fundamental bases:\n- The definition of a Mean Field Games (MFG) equilibrium as a fixed point between optimal individual responses and the population mean.\n- The optimality principle encoded by the Hamilton-Jacobi-Bellman (HJB) equation and the linearity of first-order conditions in linear-quadratic settings.\n- The forward Euler method for discretizing an Ordinary Differential Equation (ODE).\n- The spectral radius criterion for stability of a linear time-invariant discrete-time system.\n\nDerive the augmented system and justify the spectral radius stability criterion for the Euler-discretized delay system. Then implement a program that:\n- Constructs the matrix $A(a,\\kappa,\\delta,h)$ using the rule $d = \\lceil \\delta / h \\rceil$.\n- Computes its spectral radius.\n- Returns a boolean stating whether the discretized system is stable, defined as spectral radius strictly less than $1$.\n\nUse the following test suite of parameter sets $(a,\\kappa,\\delta,h)$:\n- Test $1$: $(a,\\kappa,\\delta,h) = (1.0, 0.3, 0.5, 0.01)$.\n- Test $2$ (boundary case with no delay): $(a,\\kappa,\\delta,h) = (5.0, 5.0, 0.0, 0.2)$.\n- Test $3$ (no coupling, delay irrelevant): $(a,\\kappa,\\delta,h) = (0.1, 0.0, 2.0, 0.5)$.\n- Test $4$ (large delayed coupling): $(a,\\kappa,\\delta,h) = (0.5, 15.0, 1.0, 0.2)$.\n\nYour program must compute the stability boolean for each test in the listed order and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example \"[True,False,True,False]\". No additional text should be printed. All quantities are dimensionless in this problem, so no physical units are required. The final answer must be code.",
            "solution": "The problem presented is a well-posed exercise in the stability analysis of a discretized linear time-delay system, which is a standard reduction of a class of linear-quadratic Mean Field Games. The problem is scientifically grounded, internally consistent, and contains all necessary information for its resolution. I will proceed with the solution.\n\nThe problem requires the derivation of an augmented state-space representation for a discretized time-delay ordinary differential equation (ODE) and a justification for the stability criterion based on the spectral radius of the system matrix.\n\nThe continuous-time dynamics for the population mean, $m(t)$, are given by the linear time-delay ODE:\n$$\n\\dot{m}(t) = - a \\, m(t) - \\kappa \\, m(t - \\delta)\n$$\nwhere $a > 0$ is a local damping coefficient, $\\kappa \\ge 0$ is the coupling strength to the delayed mean, and $\\delta \\ge 0$ is the time delay.\n\nTo analyze this system numerically, we discretize it using the forward Euler method with a time step $h > 0$. The derivative $\\dot{m}(t)$ is approximated as $\\frac{m(t+h) - m(t)}{h}$. Let $m_t$ denote the discrete-time approximation of $m(th)$. The ODE becomes:\n$$\n\\frac{m_{t+1} - m_t}{h} \\approx - a \\, m_t - \\kappa \\, m_{t-d}\n$$\nHere, the continuous delay $\\delta$ is approximated by an integer number of steps, $d$, defined as $d = \\lceil \\delta / h \\rceil$. This implies $m(t - \\delta) \\approx m(th - dh) = m_{t-d}$. Rearranging the equation gives the linear recurrence relation:\n$$\nm_{t+1} = m_t - a h \\, m_t - \\kappa h \\, m_{t-d} = (1 - a h) \\, m_t - \\kappa h \\, m_{t-d}\n$$\nThis is a $(d+1)$-th order linear homogeneous difference equation. To analyze its stability, we convert it into a first-order linear system on an augmented state vector. Let the state vector $z_t \\in \\mathbb{R}^{d+1}$ be defined as the stack of the current and past $d$ values of the mean:\n$$\nz_t = \\begin{pmatrix} m_t \\\\ m_{t-1} \\\\ m_{t-2} \\\\ \\vdots \\\\ m_{t-d} \\end{pmatrix}\n$$\nWe seek a matrix $A \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ such that $z_{t+1} = A z_t$. The state at time $t+1$ is:\n$$\nz_{t+1} = \\begin{pmatrix} m_{t+1} \\\\ m_t \\\\ m_{t-1} \\\\ \\vdots \\\\ m_{t-d+1} \\end{pmatrix}\n$$\nWe now express each component of $z_{t+1}$ as a linear combination of the components of $z_t$.\nThe first component, $m_{t+1}$, is given by the recurrence relation:\n$$\nm_{t+1} = (1 - a h) m_t - \\kappa h m_{t-d}\n$$\nThe second component of $z_{t+1}$ is $m_t$, which is simply the first component of $z_t$.\nThe third component of $z_{t+1}$ is $m_{t-1}$, which is the second component of $z_t$.\nThis pattern continues, where the $(i+1)$-th component of $z_{t+1}$ is the $i$-th component of $z_t$ for $i \\in \\{1, \\ldots, d\\}$.\n\nThis defines the linear transformation $z_{t+1} = A z_t$, where $A$ is the companion matrix:\n$$\nA = \\begin{pmatrix}\n1-ah & 0 & \\cdots & 0 & -\\kappa h \\\\\n1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & 1 & 0\n\\end{pmatrix}\n$$\nThe first row of matrix $A$ encodes the recurrence relation for $m_{t+1}$. The subsequent rows encode the shifting of past states: $m_{t-i}$ at step $t+1$ is what $m_{t-i-1}$ was at step $t$.\n\nThe justification for the stability criterion rests on the properties of discrete-time linear time-invariant systems. The evolution of the system from an initial state $z_0$ is given by $z_t = A^t z_0$. Asymptotic stability requires that $z_t \\to 0$ as $t \\to \\infty$ for any initial condition $z_0$. This is equivalent to the condition that the matrix power $A^t$ converges to the zero matrix as $t \\to \\infty$. A fundamental theorem of linear algebra and dynamical systems states that $\\lim_{t\\to\\infty} A^t = 0$ if and only if the spectral radius of $A$, denoted $\\rho(A)$, is strictly less than $1$. The spectral radius is defined as the maximum of the absolute values (or magnitudes for complex numbers) of the eigenvalues of $A$:\n$$\n\\rho(A) = \\max_{i} |\\lambda_i| \\quad \\text{where } \\lambda_i \\text{ are the eigenvalues of } A\n$$\nTherefore, the discretized mean-field equilibrium is stable if and only if $\\rho(A(a,\\kappa,\\delta,h)) < 1$.\n\nFor the special case where the delay $\\delta = 0$, the integer delay $d = \\lceil 0/h \\rceil = 0$. The state vector becomes a scalar $z_t = (m_t) \\in \\mathbb{R}^1$. The recurrence simplifies to:\n$$\nm_{t+1} = (1 - a h) m_t - \\kappa h m_{t-0} = (1 - (a + \\kappa) h) m_t\n$$\nIn this case, the matrix $A$ is the $1 \\times 1$ matrix (a scalar) $A = [1 - (a+\\kappa)h]$. Its single eigenvalue is $1 - (a+\\kappa)h$, and its spectral radius is $\\rho(A) = |1 - (a+\\kappa)h|$. The stability condition $\\rho(A) < 1$ correctly reduces to $|1 - (a+\\kappa)h| < 1$, which validates the problem's premise.\n\nThe implementation will construct the matrix $A$ based on the parameters $(a, \\kappa, \\delta, h)$, compute its eigenvalues using a numerical library, find the maximum of their absolute values to determine the spectral radius, and test if this radius is strictly less than $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MFG stability problem for a given set of test cases.\n    Constructs the companion matrix for the discretized delay-difference equation\n    and checks if its spectral radius is strictly less than 1.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (a, kappa, delta, h)\n        (1.0, 0.3, 0.5, 0.01),  # Test 1\n        (5.0, 5.0, 0.0, 0.2),   # Test 2 (boundary case with no delay)\n        (0.1, 0.0, 2.0, 0.5),   # Test 3 (no coupling, delay irrelevant)\n        (0.5, 15.0, 1.0, 0.2),  # Test 4 (large delayed coupling)\n    ]\n\n    results = []\n    for a, kappa, delta, h in test_cases:\n        # Calculate the integer delay d from the continuous delay delta and step size h.\n        # d must be an integer, using ceiling function as specified.\n        # For delta = 0, np.ceil(0) correctly returns 0.\n        d = int(np.ceil(delta / h))\n\n        if h <= 0:\n            # Step size must be positive, invalid case according to problem statement.\n            # Handle defensively although test data is valid.\n            raise ValueError(\"Step size h must be positive.\")\n\n        # The dimension of the augmented system is (d+1).\n        dim = d + 1\n        \n        # Construct the companion matrix A.\n        if d == 0:\n            # Special case for no delay (d=0). The system is 1D.\n            # The recurrence is m_{t+1} = (1 - (a+kappa)*h) * m_t.\n            # The matrix A is a 1x1 scalar.\n            A = np.array([[1.0 - (a + kappa) * h]])\n        else:\n            # General case for d > 0.\n            A = np.zeros((dim, dim))\n            \n            # First row encodes the recurrence relation:\n            # m_{t+1} = (1 - a*h)*m_t - kappa*h*m_{t-d}\n            A[0, 0] = 1.0 - a * h\n            A[0, d] = -kappa * h\n            \n            # The other rows create the shift property: m_{t-i} becomes m_{t-i+1}.\n            # This is achieved by placing a 1 on the sub-diagonal.\n            # A[i, i-1] = 1 for i = 1, ..., d.\n            # A numpy-idiomatic way to set the sub-diagonal.\n            for i in range(1, dim):\n                A[i, i - 1] = 1.0\n        \n        # Calculate the eigenvalues of the matrix A.\n        eigenvalues = np.linalg.eigvals(A)\n        \n        # The spectral radius is the maximum of the absolute values of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        # The discretized system is stable if the spectral radius is strictly less than 1.\n        is_stable = spectral_radius < 1.0\n        \n        results.append(is_stable)\n\n    # Final print statement in the exact required format.\n    # Convert boolean list to string representation as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver.\nsolve()\n```"
        }
    ]
}