{
    "hands_on_practices": [
        {
            "introduction": "掌握Bewley-Huggett-Aiyagari模型的第一步是理解其核心输出——稳态分布。这个练习将家庭的最优决策规则视为已知，让你能够专注于模型动态的核心机制：个体行为和外生冲击如何共同作用，最终形成一个稳定的宏观经济体横截面分布。通过构建描述状态转移的马尔可夫矩阵并求解其不变分布，你将为后续更复杂的计算分析奠定坚实的基础。",
            "id": "2437609",
            "problem": "考虑一个 Bewley-Huggett-Aiyagari 模型，其中有单位质量的事前同质的代理人，他们面临异质性劳动收入风险，并通过在单一无风险资产中储蓄来进行自我保险。设异质性收入过程为一个两状态马尔可夫链 $z \\in \\{z_{L}, z_{H}\\}$，其转移概率由下式给出\n$$\n\\begin{pmatrix}\n\\mathbb{P}(z' = z_{L} \\mid z = z_{L})  \\mathbb{P}(z' = z_{H} \\mid z = z_{L}) \\\\\n\\mathbb{P}(z' = z_{L} \\mid z = z_{H})  \\mathbb{P}(z' = z_{H} \\mid z = z_{H})\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{1}{5} \\\\\n\\frac{1}{5}  \\frac{4}{5}\n\\end{pmatrix}.\n$$\n资产网格是离散的，有两个点 $a \\in \\{a_{0}, a_{1}\\}$，其中 $a_{0}  a_{1}$。假设最优的下一期资产决策（通过给定价格求解家庭问题得到）是一个确定性函数 $a' = g(a,z)$，它在网格上取值，并由以下给出\n- $g(a_{0}, z_{L}) = a_{0}$，\n- $g(a_{0}, z_{H}) = a_{1}$，\n- $g(a_{1}, z_{L}) = a_{0}$，\n- $g(a_{1}, z_{H}) = a_{1}$。\n将联合状态定义为 $s = (a,z)$，并按以下顺序枚举4个状态\n$$\ns_{1} = (a_{0}, z_{L}), \\quad s_{2} = (a_{0}, z_{H}), \\quad s_{3} = (a_{1}, z_{L}), \\quad s_{4} = (a_{1}, z_{H}).\n$$\n直方图法（也称为离散化方法）将平稳分布计算为此有限状态空间上截面测度的线性运动定律的不动点。长面板模拟方法模拟 $N$ 个代理人在 $T$ 个时期内的行为，舍弃初始的预烧期，然后形成经验频率。\n\n下列哪个陈述是正确的？\n\nA. 由直方图法计算出的关于 $(s_{1}, s_{2}, s_{3}, s_{4})$ 的平稳分布是\n$$\n\\pi = \\left(\\frac{2}{5}, \\frac{1}{10}, \\frac{1}{10}, \\frac{2}{5}\\right).\n$$\n\nB. 对于一个 $N$ 和 $T$ 都很大且舍弃了预烧期的长面板模拟，经验截面状态频率几乎必然收敛到与选项A中相同的平稳分布。\n\nC. 在 $a = a_{1}$ 的状态上的平稳质量等于 $\\frac{1}{2}$。\n\nD. 如果面板被精确地初始化于平稳分布，且决策 $g$ 如上所述是确定性的，那么再经过一期后，对于任何有限的 $N$，模拟的截面分布将保持完全相同。\n\nE. 直方图法引入了抽样误差，该误差仅在 $N \\to \\infty$ 时消失；相比之下，如果从平稳状态初始化，长面板模拟没有抽样误差。",
            "solution": "问题陈述需进行验证。\n\n**步骤 1：提取给定信息**\n- **模型**: Bewley-Huggett-Aiyagari。\n- **代理人**: 单位质量，事前同质。\n- **收入过程**: $z \\in \\{z_{L}, z_{H}\\}$ 的两状态马尔可夫链，转移矩阵为\n$$\n\\mathbf{\\Pi}_{z} =\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{1}{5} \\\\\n\\frac{1}{5}  \\frac{4}{5}\n\\end{pmatrix}.\n$$\n- **资产网格**: 离散的，$a \\in \\{a_{0}, a_{1}\\}$ 其中 $a_{0}  a_{1}$。\n- **决策函数**: $a' = g(a,z)$ 由以下给出：\n  - $g(a_{0}, z_{L}) = a_{0}$\n  - $g(a_{0}, z_{H}) = a_{1}$\n  - $g(a_{1}, z_{L}) = a_{0}$\n  - $g(a_{1}, z_{H}) = a_{1}$\n- **状态空间**: 联合状态为 $s = (a,z)$，枚举如下：\n  $s_{1} = (a_{0}, z_{L})$, $s_{2} = (a_{0}, z_{H})$, $s_{3} = (a_{1}, z_{L})$, $s_{4} = (a_{1}, z_{H})$。\n\n**步骤 2：使用提取的给定信息进行验证**\n问题是定义明确且有科学依据的。它提出了一个标准的、简化的异质性代理人模型，用于计算经济学。分析截面分布动态所需的所有组成部分——状态空间、收入过程和决策函数——都已明确提供且相互一致。确定性决策规则 $g(a,z)$ 和随机收入过程的结合，在联合状态空间 $S = \\{s_{1}, s_{2}, s_{3}, s_{4}\\}$ 上产生了一个定义明确的有限状态马尔可夫链。此链是不可约的，保证了唯一平稳分布的存在性。因此，该问题是有效的。\n\n**步骤 3：结论与行动**\n问题是有效的。将推导解答。\n\n**推导**\n\n问题的核心是找到4个状态 $s = (a,z)$ 上的马尔可夫链的平稳分布 $\\pi$。这需要构建一个 $4 \\times 4$ 的转移矩阵 $\\mathbf{P}$，其中 $P_{ij} = \\mathbb{P}(s' = s_{j} \\mid s = s_{i})$。处于状态 $s = (a,z)$ 的代理人转移到一个新的资产状态 $a' = g(a,z)$ 和一个新的收入状态 $z'$，后者根据转移概率 $\\mathbf{\\Pi}_{z}$ 抽取。\n\n让我们来构建 $\\mathbf{P}$ 的各行：\n1.  从 $s_{1}=(a_{0}, z_{L})$: 下一资产状态为 $a' = g(a_{0}, z_{L}) = a_{0}$。收入状态 $z'$ 以 $\\frac{4}{5}$ 的概率变为 $z_{L}$，或以 $\\frac{1}{5}$ 的概率变为 $z_{H}$。因此，下一状态以 $\\frac{4}{5}$ 的概率为 $s_{1}=(a_{0}, z_{L})$，或以 $\\frac{1}{5}$ 的概率为 $s_{2}=(a_{0}, z_{H})$。$\\mathbf{P}$ 的第一行为 $(\\frac{4}{5}, \\frac{1}{5}, 0, 0)$。\n2.  从 $s_{2}=(a_{0}, z_{H})$: 下一资产状态为 $a' = g(a_{0}, z_{H}) = a_{1}$。收入状态 $z'$ 以 $\\frac{1}{5}$ 的概率变为 $z_{L}$，或以 $\\frac{4}{5}$ 的概率变为 $z_{H}$。因此，下一状态以 $\\frac{1}{5}$ 的概率为 $s_{3}=(a_{1}, z_{L})$，或以 $\\frac{4}{5}$ 的概率为 $s_{4}=(a_{1}, z_{H})$。$\\mathbf{P}$ 的第二行为 $(0, 0, \\frac{1}{5}, \\frac{4}{5})$。\n3.  从 $s_{3}=(a_{1}, z_{L})$: 下一资产状态为 $a' = g(a_{1}, z_{L}) = a_{0}$。收入状态 $z'$ 以 $\\frac{4}{5}$ 的概率变为 $z_{L}$，或以 $\\frac{1}{5}$ 的概率变为 $z_{H}$。因此，下一状态以 $\\frac{4}{5}$ 的概率为 $s_{1}=(a_{0}, z_{L})$，或以 $\\frac{1}{5}$ 的概率为 $s_{2}=(a_{0}, z_{H})$。$\\mathbf{P}$ 的第三行为 $(\\frac{4}{5}, \\frac{1}{5}, 0, 0)$。\n4.  从 $s_{4}=(a_{1}, z_{H})$: 下一资产状态为 $a' = g(a_{1}, z_{H}) = a_{1}$。收入状态 $z'$ 以 $\\frac{1}{5}$ 的概率变为 $z_{L}$，或以 $\\frac{4}{5}$ 的概率变为 $z_{H}$。因此，下一状态以 $\\frac{1}{5}$ 的概率为 $s_{3}=(a_{1}, z_{L})$，或以 $\\frac{4}{5}$ 的概率为 $s_{4}=(a_{1}, z_{H})$。$\\mathbf{P}$ 的第四行为 $(0, 0, \\frac{1}{5}, \\frac{4}{5})$。\n\n转移矩阵为：\n$$\n\\mathbf{P} =\n\\begin{pmatrix}\n\\frac{4}{5}  \\frac{1}{5}  0  0 \\\\\n0  0  \\frac{1}{5}  \\frac{4}{5} \\\\\n\\frac{4}{5}  \\frac{1}{5}  0  0 \\\\\n0  0  \\frac{1}{5}  \\frac{4}{5}\n\\end{pmatrix}\n$$\n平稳分布 $\\pi = (\\pi_{1}, \\pi_{2}, \\pi_{3}, \\pi_{4})$ 是 $\\mathbf{P}$ 对应于特征值 $1$ 的左特征向量，满足 $\\pi \\mathbf{P} = \\pi$ 和 $\\sum_{i=1}^{4} \\pi_{i} = 1$。这得到以下线性方程组：\n$$\n\\pi_{1} = \\frac{4}{5}\\pi_{1} + \\frac{4}{5}\\pi_{3} \\\\\n\\pi_{2} = \\frac{1}{5}\\pi_{1} + \\frac{1}{5}\\pi_{3} \\\\\n\\pi_{3} = \\frac{1}{5}\\pi_{2} + \\frac{1}{5}\\pi_{4} \\\\\n\\pi_{4} = \\frac{4}{5}\\pi_{2} + \\frac{4}{5}\\pi_{4}\n$$\n由第一个方程，$\\frac{1}{5}\\pi_{1} = \\frac{4}{5}\\pi_{3}$，这意味着 $\\pi_{1} = 4\\pi_{3}$。\n由第四个方程，$\\frac{1}{5}\\pi_{4} = \\frac{4}{5}\\pi_{2}$，这意味着 $\\pi_{4} = 4\\pi_{2}$。\n将 $\\pi_{1} = 4\\pi_{3}$ 代入第二个方程得到 $\\pi_{2} = \\frac{1}{5}(4\\pi_{3}) + \\frac{1}{5}\\pi_{3} = \\pi_{3}$。\n所以我们有关系：$\\pi_{2} = \\pi_{3}$，$\\pi_{1} = 4\\pi_{3} = 4\\pi_{2}$，以及 $\\pi_{4} = 4\\pi_{2}$。\n使用归一化条件 $\\pi_{1} + \\pi_{2} + \\pi_{3} + \\pi_{4} = 1$：\n$$\n4\\pi_{2} + \\pi_{2} + \\pi_{2} + 4\\pi_{2} = 1 \\\\\n10\\pi_{2} = 1 \\implies \\pi_{2} = \\frac{1}{10}\n$$\n由此，我们找到其他分量：\n$\\pi_{1} = 4 \\cdot \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5}$\n$\\pi_{3} = \\frac{1}{10}$\n$\\pi_{4} = 4 \\cdot \\frac{1}{10} = \\frac{4}{10} = \\frac{2}{5}$\n平稳分布是 $\\pi = (\\frac{2}{5}, \\frac{1}{10}, \\frac{1}{10}, \\frac{2}{5})$。\n\n**逐项分析**\n\nA. 由直方图法计算出的关于 $(s_{1}, s_{2}, s_{3}, s_{4})$ 的平稳分布是 $\\pi = \\left(\\frac{2}{5}, \\frac{1}{10}, \\frac{1}{10}, \\frac{2}{5}\\right)$。\n这个陈述是**正确的**。上面的计算，对应于直方图法，恰好得到了这个分布。\n\nB. 对于一个 $N$ 和 $T$ 都很大且舍弃了预烧期的长面板模拟，经验截面状态频率几乎必然收敛到与选项A中相同的平稳分布。\n这个陈述是**正确的**。构建的马尔可夫链是有限且不可约的，这意味着它是遍历的。马尔可夫链的遍历定理保证了对于大量的代理人 $N$ 和长的时间序列 $T$，模拟中状态的经验分布将几乎必然收敛到唯一的平稳分布 $\\pi$。\n\nC. 在 $a = a_{1}$ 的状态上的平稳质量等于 $\\frac{1}{2}$。\n这个陈述是**正确的**。对应于资产水平 $a_{1}$ 的状态是 $s_{3} = (a_{1}, z_{L})$ 和 $s_{4} = (a_{1}, z_{H})$。这些状态上的总平稳概率质量为 $\\pi_{3} + \\pi_{4} = \\frac{1}{10} + \\frac{2}{5} = \\frac{1}{10} + \\frac{4}{10} = \\frac{5}{10} = \\frac{1}{2}$。\n\nD. 如果面板被精确地初始化于平稳分布，且决策 $g$ 如上所述是确定性的，那么再经过一期后，对于任何有限的 $N$，模拟的截面分布将保持完全相同。\n这个陈述是**不正确的**。设代理人数量为 $N$。如果系统初始化时每个状态 $i$ 有 $N_{i} = N\\pi_{i}$ 个代理人，那么转移到状态 $j$ 的代理人数量是伯努利随机变量的总和。对于任何有限的 $N$，由于抽样变异，这个实现的数量几乎必然会与其期望值不同。大数定律意味着实现的分布仅在 $N \\to \\infty$ 时才收敛到平稳分布。对于有限的 $N$，分布不会*完全*保持相同。\n\nE. 直方图法引入了抽样误差，该误差仅在 $N \\to \\infty$ 时消失；相比之下，如果从平稳状态初始化，长面板模拟没有抽样误差。\n这个陈述是**不正确的**。它从根本上错误地描述了误差的来源。直方图法对于给定的网格是一个确定性的数值程序；其误差是*离散化误差*（来自对连续状态空间的近似）和数值求解器误差，而不是与代理人数量 $N$ 相关的抽样误差。一个有限数量代理人 $N$ 的模拟在每个时期都内在地包含*抽样误差*，无论如何初始化，因为截面的演化是一个随机过程。",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "上一个练习假设了决策规则是已知的，但这些规则从何而来？本练习将深入探讨用于求解家庭最优决策的强大算法——策略函数迭代（Policy Function Iteration, PFI）。这个概念性问题旨在阐明当模型同时包含连续状态（如资产）和离散状态（如就业状况）时，如何正确地构建和执行该算法。理解这一点对于将理论模型转化为可执行的计算机代码至关重要。",
            "id": "2419722",
            "problem": "考虑一个无限期界消费-储蓄问题，其状态变量包括连续的资本变量 $k \\in \\mathcal{K} \\subset \\mathbb{R}_{+}$ 和离散的就业状态 $s \\in \\{U,E\\}$。就业状态服从一个有限状态马尔可夫链，其转移矩阵为 $\\Pi = \\left[\\pi_{s,s'}\\right]_{s,s' \\in \\{U,E\\}}$，其中 $\\pi_{s,s'} = \\mathbb{P}(s_{t+1}=s' \\mid s_t=s)$ 且 $\\sum_{s'} \\pi_{s,s'} = 1$。偏好由 $\\sum_{t=0}^{\\infty} \\beta^{t} u(c_t)$ 给出，其中折扣因子为 $\\beta \\in (0,1)$，时期效用函数 $u(\\cdot)$ 是严格递增且严格凹的。收入为 $y(s)$，预算约束为\n$$\nc + k' = y(s) + R k, \\quad k' \\ge \\underline{k},\n$$\n其中总回报率为 $R \\ge 1$，借贷限制为 $\\underline{k} \\ge 0$。贝尔曼方程为\n$$\nV(k,s) \\;=\\; \\max_{k' \\ge \\underline{k}} \\left\\{ u\\!\\left( y(s) + R k - k' \\right) + \\beta \\sum_{s' \\in \\{U,E\\}} \\pi_{s,s'} \\, V(k', s') \\right\\}.\n$$\n假设您在一个有限网格 $\\{k_i\\}_{i=1}^{N}$ 上对连续状态 $k$ 执行策略函数迭代 (Policy Function Iteration, PFI)，允许 $k'$ 在网格之外取值，并根据需要使用插值法来评估 $k'$ 处的未来价值。以下哪个陈述最准确地描述了 PFI 如何正确处理连续状态 $k$ 和离散状态 $s$ 的共存问题？\n\nA. 将状态扩展为 $(k,s)$，并对每个 $s$ 在 $k$ 网格上维护一个独立的策略函数 $g(\\cdot,s)$。在策略评估步骤中，求解线性系统 $(I - \\beta \\tilde{Q}) \\mathbf{V} = \\mathbf{r}$，其中 $\\mathbf{r}$ 集合了 $u(y(s)+Rk_i - g(k_i,s))$，而 $\\tilde{Q}$ 则嵌入了离散转移概率 $\\pi_{s,s'}$ 以及从 $k' = g(k_i,s)$ 到所有 $s'$ 的下一期 $k$ 网格的插值。在策略改进步骤中，对每个 $(k_i,s)$ 选择 $k'$ 以最大化 $u(y(s)+Rk_i - k') + \\beta \\sum_{s'} \\pi_{s,s'} V(k',s')$，并在 $k'$ 上使用插值。\n\nB. 将 $s$ 视为控制变量而非状态变量：在每个 $(k_i,s)$，同时选择 $k'$ 和 $s'$ 以最大化当前效用加上未来价值，然后因为 $s'$ 是最优选择的，所以在没有期望的情况下对贝尔曼算子进行迭代来评估策略。\n\nC. 离散化 $k$ 并将 $k'$ 限制在同一网格上，从而无需插值；然后 PFI 在一个纯离散状态空间上简化为价值函数迭代 (Value Function Iteration, VFI)，使得策略评估步骤变得不必要。\n\nD. 对每个固定的 $s$，通过强制执行关于 $k$ 的欧拉方程来求解连续部分，并对 $s$ 单独通过策略迭代来求解离散部分；在两者之间交替进行直到收敛，省略对 $s'$ 的期望，因为它将被交替过程所捕获。\n\nE. 用两个独立的贝尔曼方程（一个用于 $s=U$，一个用于 $s=E$）替换联合贝尔曼方程，在策略评估期间忽略 $U$ 和 $E$ 之间的转移，从而可以像 $s$ 是吸收态一样分别求解；最后再合并解。",
            "solution": "问题陈述经过验证。\n\n### 步骤1：提取已知条件\n-   **模型类型**: 无限期界消费-储蓄问题。\n-   **状态变量**: 一个连续状态变量，资本 $k \\in \\mathcal{K} \\subset \\mathbb{R}_{+}$，以及一个离散状态变量，就业状态 $s \\in \\{U,E\\}$。\n-   **随机过程**: 离散状态 $s$ 服从一个有限状态马尔可夫链，其转移矩阵为 $\\Pi = \\left[\\pi_{s,s'}\\right]_{s,s' \\in \\{U,E\\}}$，其中 $\\pi_{s,s'} = \\mathbb{P}(s_{t+1}=s' \\mid s_t=s)$ 且 $\\sum_{s'} \\pi_{s,s'} = 1$。\n-   **偏好**: 代理人最大化期望折扣效用 $\\sum_{t=0}^{\\infty} \\beta^{t} u(c_t)$。折扣因子为 $\\beta \\in (0,1)$。时期效用函数 $u(\\cdot)$ 是严格递增和严格凹的。\n-   **资源与约束**: 收入由函数 $y(s)$ 给出。预算约束为 $c + k' = y(s) + R k$，其中 $k'$ 是下一期的资本。借贷限制为 $k' \\ge \\underline{k}$，且 $\\underline{k} \\ge 0$。资本的总回报率为 $R \\ge 1$。\n-   **贝尔曼方程**: 价值函数 $V(k,s)$ 满足：\n    $$\n    V(k,s) \\;=\\; \\max_{k' \\ge \\underline{k}} \\left\\{ u\\!\\left( y(s) + R k - k' \\right) + \\beta \\sum_{s' \\in \\{U,E\\}} \\pi_{s,s'} \\, V(k', s') \\right\\}.\n    $$\n-   **数值实现**: 要使用的算法是策略函数迭代 (PFI)。连续状态 $k$ 在一个有限网格 $\\{k_i\\}_{i=1}^{N}$ 上被离散化。下一期资本 $k'$ 的选择被允许在网格之外。使用插值法来评估网格之外点的价值函数。\n\n### 步骤2：使用提取的已知条件进行验证\n-   **科学依据**: 该问题描述了一个经典的 Bewley-Huggett-Aiyagari 模型，这是现代宏观经济学中研究特质性收入风险下的消费、储蓄和财富不平等的基本框架。所有组成部分——偏好、约束和随机过程——都是标准的且科学合理的。\n-   **适定性**: 对效用函数的假设（$u$ 是严格递增和严格凹的）和折扣因子的假设（$\\beta \\in (0,1)$），结合预算约束和马尔可夫过程的结构，确保贝尔曼算子是一个压缩映射。这保证了存在一个唯一的、连续且有界的价值函数 $V(k,s)$ 以及一个相关的最优策略函数 $g(k,s) = k'$。从数理经济学的角度来看，这个问题是适定的。\n-   **客观性**: 问题以精确的数学术语陈述，没有歧义或主观性语言。\n-   **完整性与一致性**: 问题提供了理解经济环境和计算任务所需的所有必要信息。问题是关于针对这个定义明确的模型正确实施特定算法 (PFI) 的直接询问。不存在内部矛盾。\n\n### 步骤3：结论与行动\n问题陈述是有效的。这是一个计算经济学中的标准、适定的问题。可以继续求解过程。\n\n### 求解推导\n该问题要求针对一个具有连续-离散混合状态 $(k,s)$ 的模型，正确实现策略函数迭代 (PFI)。数值解的状态空间是 $\\{k_i\\}_{i=1}^N \\times \\{U,E\\}$。PFI 的关键特征是其两步迭代过程：策略评估和策略改进。\n\n设 $g_j(k,s)$ 为第 $j$ 次迭代时的策略函数（即选择 $k'$ 的规则）。\n\n**1. 策略评估步骤：**\n给定策略函数 $g_j(k,s)$，相应的价值函数（我们称之为 $V_j(k,s)$）满足以下函数方程：\n$$\nV_j(k,s) = u(y(s) + Rk - g_j(k,s)) + \\beta \\sum_{s' \\in \\{U,E\\}} \\pi_{s,s'} V_j(g_j(k,s), s')\n$$\n这个方程必须对我们网格上的每个状态 $(k_i, s)$ 都成立。总共有 $2N$ 个这样的状态。$V_j(g_j(k_i,s), s')$ 这一项是有问题的，因为 $k' = g_j(k_i,s)$ 可能不是网格点。如问题所述，我们使用插值法。这意味着 $V_j(k',s')$ 可以表示为函数 $V_j(\\cdot, s')$ 在相邻网格点上值的线性组合。例如，对于线性插值，如果 $k_l \\le k' \\le k_{l+1}$，我们有 $V_j(k',s') = w V_j(k_l, s') + (1-w) V_j(k_{l+1}, s')$，其中 $w = (k_{l+1}-k')/(k_{l+1}-k_l)$。\n\n设 $\\mathbf{V}$ 是一个大小为 $2N \\times 1$ 的列向量，包含未知值 $\\{V_j(k_i, s)\\}_{i=1,..,N; s \\in \\{U,E\\}}$。设 $\\mathbf{r}$ 是一个同样大小的列向量，包含当期效用 $\\{u(y(s) + Rk_i - g_j(k_i,s))\\}_{i=1,..,N; s \\in \\{U,E\\}}$。持续价值项 $\\beta \\sum_{s'} \\pi_{s,s'} V_j(g_j(k_i,s), s')$ 是 $\\mathbf{V}$ 中元素的线性函数。我们可以用一个 $2N \\times 2N$ 的矩阵（我们称之为 $\\beta \\tilde{Q}$）来表示这些线性关系。因此，矩阵 $\\tilde{Q}$ 包含了离散状态之间的转移概率 $\\pi_{s,s'}$ 和连续状态的插值权重。关于 $\\mathbf{V}$ 的方程组是：\n$$\n\\mathbf{V} = \\mathbf{r} + \\beta \\tilde{Q} \\mathbf{V}\n$$\n这可以重写为一个标准的线性系统来求解 $\\mathbf{V}$：\n$$\n(I - \\beta \\tilde{Q}) \\mathbf{V} = \\mathbf{r}\n$$\n其中 $I$ 是单位矩阵。求解这个系统可以得到与策略 $g_j$ 对应的价值函数 $V_j$。\n\n**2. 策略改进步骤：**\n给定从评估步骤得到的价值函数 $V_j(k,s)$，我们通过求解原始贝尔曼方程右侧的最大化问题，为每个状态 $(k_i,s)$ 找到一个新的、期望更优的策略函数 $g_{j+1}(k,s)$：\n$$\ng_{j+1}(k_i,s) = \\arg\\max_{k' \\ge \\underline{k}} \\left\\{ u(y(s) + Rk_i - k') + \\beta \\sum_{s' \\in \\{U,E\\}} \\pi_{s,s'} V_j(k',s') \\right\\}\n$$\n在这个最大化过程中，对于每个候选的 $k'$，我们需要评估 $V_j(k',s')$，这同样需要插值，因为 $V_j$ 只在网格点 $\\{k_i\\}$ 上是已知的。由于目标函数是凹的，这个最大化通常使用数值优化方法（例如，黄金分割搜索、Brent方法）来执行。\n\n重复这两个步骤，直到策略函数收敛，即 $g_{j+1} \\approx g_j$。\n\n### 逐项分析\n\n**A. 将状态扩展为 $(k,s)$，并对每个 $s$ 在 $k$ 网格上维护一个独立的策略函数 $g(\\cdot,s)$。在策略评估步骤中，求解线性系统 $(I - \\beta \\tilde{Q}) \\mathbf{V} = \\mathbf{r}$，其中 $\\mathbf{r}$ 集合了 $u(y(s)+Rk_i - g(k_i,s))$，而 $\\tilde{Q}$ 则嵌入了离散转移概率 $\\pi_{s,s'}$ 以及从 $k' = g(k_i,s)$ 到所有 $s'$ 的下一期 $k$ 网格的插值。在策略改进步骤中，对每个 $(k_i,s)$ 选择 $k'$ 以最大化 $u(y(s)+Rk_i - k') + \\beta \\sum_{s'} \\pi_{s,s'} V(k',s')$，并在 $k'$ 上使用插值。**\n\n该选项精确且完整地描述了应用于此问题的 PFI 算法。\n-   状态被正确地识别为对 $(k,s)$。策略函数 $g(k,s)$ 必须依赖于两者，因此为每个 $s \\in \\{U,E\\}$ 维护一个独立的函数 $g(\\cdot, s)$ 是正确的。\n-   将策略评估步骤描述为求解线性系统 $(I - \\beta \\tilde{Q}) \\mathbf{V} = \\mathbf{r}$ 是准确的。对向量 $\\mathbf{r}$ 和矩阵 $\\tilde{Q}$ 的描述与上面的推导完全匹配。$\\tilde{Q}$ 正确地结合了离散转移概率 $\\pi_{s,s'}$ 和由策略 $g(k_i,s)$ 及必要的插值所隐含的连续转移。\n-   对策略改进步骤的描述也完全准确。它涉及最大化贝尔曼方程的右侧，使用刚刚计算出的价值函数 $V$ 并对 $k'$ 的网格之外值采用插值。\n\n结论：**正确**。\n\n**B. 将 $s$ 视为控制变量而非状态变量：在每个 $(k_i,s)$，同时选择 $k'$ 和 $s'$ 以最大化当前效用加上未来价值，然后因为 $s'$ 是最优选择的，所以在没有期望的情况下对贝尔曼算子进行迭代来评估策略。**\n\n就业状态 $s$ 是一个由马尔可夫过程控制的外生随机变量；它是状态的基本组成部分，而不是代理人的选择（控制）变量。代理人观察 $s$ 但不选择它。因此，对未来状态 $s'$ 的期望算子是必不可少的。该选项从根本上误解了具有外生冲击的动态规划问题的结构。\n\n结论：**不正确**。\n\n**C. 离散化 $k$ 并将 $k'$ 限制在同一网格上，从而无需插值；然后 PFI 在一个纯离散状态空间上简化为价值函数迭代 (VFI) (Value Function Iteration)，使得策略评估步骤变得不必要。**\n\n该选项包含多个错误。\n-   它与问题陈述相矛盾，问题明确允许 $k'$ 在网格之外选择，需要插值。\n-   它错误地声称 PFI 简化为 VFI。PFI 和 VFI 是不同的算法。VFI 将评估和改进步骤合并为贝尔曼算子的单次迭代。PFI 将它们保留为两个独立的步骤。即使 $k'$ 的选择集是离散的，PFI 仍然是 PFI。\n-   它错误地声称策略评估步骤变得不必要。策略评估步骤是 PFI 的定义性特征，将其与 VFI 区分开来。\n\n结论：**不正确**。\n\n**D. 对每个固定的 $s$，通过强制执行关于 $k$ 的欧拉方程来求解连续部分，并对 $s$ 单独通过策略迭代来求解离散部分；在两者之间交替进行直到收敛，省略对 $s'$ 的期望，因为它将被交替过程所捕获。**\n\n这描述了一个毫无意义的算法。\n-   使用欧拉方程是投影方法或时间迭代的特征，而不是 PFI 的特征，PFI 是基于贝尔曼方程工作的。\n-   状态 $s$ 不受“策略”的影响；它的演变是外生的。谈论“对 s 进行策略迭代”是毫无意义的。\n-   省略对 $s'$ 的期望严重违反了问题的动态结构。代理人必须考虑未来就业状态的不确定性。\n\n结论：**不正确**。\n\n**E. 用两个独立的贝尔曼方程（一个用于 $s=U$，一个用于 $s=E$）替换联合贝尔曼方程，在策略评估期间忽略 $U$ 和 $E$ 之间的转移，从而可以像 $s$ 是吸收态一样分别求解；最后再合并解。**\n\n这种方法存在根本性缺陷。受雇和失业状态的价值函数 $V(k,E)$ 和 $V(k,U)$ 通过转移概率 $\\pi_{E,U}$ 和 $\\pi_{U,E}$ 耦合在一起。今天处于状态 $s$ 的价值取决于明天可能处于所有状态 $s'$ 的价值。只有当转移矩阵 $\\Pi$ 是单位矩阵（即 $\\pi_{E,U} = \\pi_{U,E} = 0$），意味着状态是吸收态时，解耦方程才有效。这在问题中没有被假设，并且在这类模型中通常不是这种情况。忽略转移会导致错误的解。\n\n结论：**不正确**。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "现在，我们将综合运用前面学到的所有概念，完成一个完整的计算经济学实验。这个综合性编码练习要求你从头开始，通过求解家庭的贝尔曼方程来计算最优策略，然后确定经济在政策变化前后的稳态分布。最终，你将模拟从一个稳态到另一个稳态的完整过渡路径，从而亲身体验如何利用这些微观基础模型来分析宏观政策（如失业救济金变化）的动态影响。",
            "id": "2437637",
            "problem": "考虑一个标准的异质代理人不完全市场经济，其遵循 Bewley-Huggett-Aiyagari 模型的精神，其中单位质量的无限期存活家庭面临特异性就业风险且无宏观冲击。时间是离散的。每个家庭以恒定相对风险厌恶偏好最大化其预期终身效用，该效用由 $E_{0}\\left[\\sum_{t=0}^{\\infty} \\beta^{t} u(c_{t})\\right]$ 给出，其中 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ (当 $\\sigma \\neq 1$)，折现因子 $\\beta \\in (0,1)$，消费 $c \\ge 0$。家庭拥有资产 $a \\in \\mathcal{A} = [a_{\\min}, a_{\\max}]$，并受到借贷约束 $a^{\\prime} \\ge a_{\\min}$ 和 $a_{\\min} \\ge 0$。每期预算约束为 $c + a^{\\prime} = (1+r) a + y(z)$，其中 $r$ 是外生实际利率，就业状态 $z \\in \\{0,1\\}$ 服从一个双状态马尔可夫链，其转移矩阵为\n$$\nP = \\begin{bmatrix} \\Pr(z^{\\prime}=0 \\mid z=0)  \\Pr(z^{\\prime}=1 \\mid z=0) \\\\ \\Pr(z^{\\prime}=0 \\mid z=1)  \\Pr(z^{\\prime}=1 \\mid z=1) \\end{bmatrix}.\n$$\n劳动收入在就业时为 $y(1) = w$，在失业时为 $y(0) = b$，其中 $w$ 是外生工资，$b$ 是由政策决定的失业救济金。价格 $(r,w)$ 是固定的，不存在用于决定价格的总体生产或资本市场出清；重点纯粹在于局部均衡的分布动态。\n\n令单个家庭的动态规划为贝尔曼方程\n$$\nV(a,z) = \\max_{a^{\\prime} \\in \\mathcal{A},\\, a^{\\prime} \\ge a_{\\min}} \\left\\{ u\\left((1+r)a + y(z) - a^{\\prime}\\right) + \\beta \\sum_{z^{\\prime} \\in \\{0,1\\}} P_{z z^{\\prime}} V(a^{\\prime}, z^{\\prime}) \\right\\}.\n$$\n令 $\\pi(a,z)$ 表示一个从 $(a,z)$ 到下一期资产 $a^{\\prime}$ 的最优平稳策略映射。给定离散化的资产网格 $\\{a_{i}\\}_{i=1}^{N_{a}}$ 和就业状态 $\\{0,1\\}$，最优策略 $\\pi$ 在大小为 $2 N_{a}$ 的有限状态空间 $\\{(a_{i},z)\\}$ 上引出一个马尔可夫转移核 $Q$，其中 $Q$ 的每一行包含从当前状态 $(a_{i},z)$ 到下一状态 $(a_{j},z^{\\prime})$ 的转移概率，这由最优的下一期资产选择 $a_{j} = \\pi(a_{i},z)$ 和就业转移概率 $P_{z z^{\\prime}}$ 给出。平稳分布 $\\mu^{\\star}$ 是满足 $\\mu^{\\star} = \\mu^{\\star} Q$ 的不变测度。\n\n假设经济最初处于失业救济金水平为 $b_{0}$ 下的平稳状态，其相关的最优平稳策略为 $\\pi_{0}$，转移核为 $Q_{0}$，以及关于 $(a,z)$ 的横截面平稳分布为 $\\mu_{0}$。在时间 $t=0$ 时，失业救济金发生了一次未预料到的永久性增加，从 $b_{0}$ 升至 $b_{1} > 0$。在 $b_{1}$ 下，令新的最优策略为 $\\pi_{1}$，新的转移核为 $Q_{1}$，新的平稳分布为 $\\mu_{1}^{\\star}$。政策变更后，对于 $t \\ge 0$，横截面分布遵循确定性运动定律 $\\mu_{t+1} = \\mu_{t} Q_{1}$，从 $t=0$ 时的 $\\mu_{0}$ 开始。定义时间 $t$ 的总资产为 $A_{t} = \\sum_{i,z} \\mu_{t}(a_{i},z) \\cdot a_{i}$，新的平稳总资产为 $A_{\\infty} = \\sum_{i,z} \\mu_{1}^{\\star}(a_{i},z) \\cdot a_{i}$。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 通过在离散化网格上求解贝尔曼方程，为给定的救济金 $b$ 解决家庭问题，以获得最优策略 $\\pi$ 和引出的转移核 $Q$。\n- 通过在有限状态空间上对分布进行前向迭代，计算给定转移核 $Q$ 下的平稳分布 $\\mu^{\\star}$。\n- 计算在 $Q_{1}$ 下从 $\\mu_{0}$ 开始的分布的转移路径，并追踪 $A_{t}$。\n- 对于下述每个测试用例，返回三个量：满足 $\\frac{|A_{\\tau} - A_{\\infty}|}{\\max\\{1, |A_{\\infty}|\\}} \\le \\varepsilon$ （给定容差 $\\varepsilon$）的最小非负整数时间 $\\tau$，初始总资产 $A_{0}$，以及新的平稳总资产 $A_{\\infty}$。\n\n在所有测试用例中，除非另有说明，否则使用以下固定参数：$\\beta = 0.96$，$\\sigma = 2.0$，$r = 0.01$，$w = 1.0$，\n$$\nP = \\begin{bmatrix} 0.70  0.30 \\\\ 0.05  0.95 \\end{bmatrix},\n$$\n其中行对应于按 $(0,1)$ 顺序排列的 $z \\in \\{0,1\\}$，资产网格 $\\{a_{i}\\}_{i=1}^{N_{a}}$ 在 $[a_{\\min}, a_{\\max}]$ 上线性间隔，其中 $a_{\\min} = 0.0$ 且 $a_{\\max} = 50.0$，就业收入映射为 $y(1) = w$，$y(0) = b$。每个测试用例使用指定的 $N_{a}$。在计算效用时，施加可行性约束 $c = (1+r)a + y(z) - a^{\\prime} \\ge 0$，并将 $c \\le 0$ 视为不可行，其效用为 $-\\infty$。在资产网格上使用价值函数迭代（VFI）或任何数值上正确的基于压缩映射的方法；插值不是必需的，但如果您选择使用也可以。对于转移核 $Q$ 下的平稳分布，向前迭代一个分布，直到上确界范数下的收敛低于一个小的容差。\n\n将转移时间的收敛容差定义为在相对度量 $\\frac{|A_{t} - A_{\\infty}|}{\\max\\{1, |A_{\\infty}|\\}}$ 下 $\\varepsilon = 10^{-6}$。如果 $b_{1} = b_{0}$，根据构造，转移时间应为 $\\tau = 0$。在搜索 $\\tau$ 时，使用 $T_{\\max} = 2000$ 期的最大期限；如果到 $T_{\\max}$ 仍未满足条件，则返回 $\\tau = T_{\\max}$。\n\n测试套件。您的程序必须为以下三个参数集中的每一个计算三元组 $(\\tau, A_{0}, A_{\\infty})$：\n- 测试用例 1（基准增加）：$b_{0} = 0.10$，$b_{1} = 0.20$，$N_{a} = 80$。\n- 测试用例 2（无变化边界）：$b_{0} = 0.15$，$b_{1} = 0.15$，$N_{a} = 80$。\n- 测试用例 3（大幅增加）：$b_{0} = 0.05$，$b_{1} = 0.40$，$N_{a} = 80$。\n\n答案规范和最终输出格式：\n- 对于每个测试用例，输出整数 $\\tau$ 以及浮点数 $A_{0}$ 和 $A_{\\infty}$，四舍五入到六位小数。\n- 将所有测试用例的结果聚合到程序打印的单行中，格式为方括号内以逗号分隔的列表 $[\\tau_{1}, A_{0,1}, A_{\\infty,1}, \\tau_{2}, A_{0,2}, A_{\\infty,2}, \\tau_{3}, A_{0,3}, A_{\\infty,3}]$，无空格。\n- 此问题中没有物理单位。不涉及角度。不要显示百分比；如果出现任何比率，必须表示为小数，并遵守对 $A_{0}$ 和 $A_{\\infty}$ 的舍入指令。\n\n科学真实性、基本基础和推导期望：您的解决方案应建立在动态规划、由最优平稳策略引出的离散状态空间上的马尔可夫链以及有限马尔可夫链的不变测度的基本定义之上。请勿使用任何无法从这些基础上推导出的快捷公式。确保所有数值程序（网格离散化、价值函数的基于压缩映射的不动点迭代、分布的前向迭代）都以科学上可靠且自洽的方式实现。",
            "solution": "该问题被评估为有效。它构成了一个适定且标准的计算经济学练习，基于已建立的 Bewley-Huggett-Aiyagari 框架。任务是分析失业救济金永久性变化所带来的分布性和总体性后果。解决方案需要解决家庭的动态规划问题，确定财富和就业的引致马尔可夫过程，并模拟经济向新稳态的过渡。该方法将按如下方式进行数值实现。\n\n首先，我们定义离散化的状态空间。家庭的连续状态是其资产持有量 $a$ 及其就业状况 $z$。资产空间 $\\mathcal{A} = [a_{\\min}, a_{\\max}]$ 被离散化为 $N_a$ 个点的网格 $\\{a_i\\}_{i=1}^{N_a}$，这些点是线性间隔的。就业状态 $z$ 是离散的，$z \\in \\{0, 1\\}$。因此，我们数值问题的完整状态空间是有限集 $S = \\{a_i\\}_{i=1}^{N_a} \\times \\{0, 1\\}$，包含 $2 N_a$ 个不同的状态。我们可以用一个有序对 $(a_i, z)$ 来表示一个状态。\n\n模型的核心是家庭的优化问题，我们使用价值函数迭代（VFI）来解决。对于处于状态 $(a,z)$ 的家庭，其贝尔曼方程为：\n$$ V(a,z) = \\max_{a' \\ge a_{\\min}} \\left\\{ u(c) + \\beta \\sum_{z' \\in \\{0,1\\}} P_{zz'} V(a', z') \\right\\} $$\n受制于预算约束 $c + a' = (1+r)a + y(z)$ 和消费的非负约束 $c \\ge 0$。效用函数为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$。\n为了在我们的离散网格上解决这个问题，我们对价值函数进行迭代，该函数表示为一个大小为 $2 \\times N_a$ 的矩阵 $V$。从一个初始猜测 $V_0$ 开始，我们迭代 $V_{k+1} = T(V_k)$，其中 $T$ 是贝尔曼算子。对于每个状态 $(a_i, z)$，算子为：\n$$ (T V)(a_i, z) = \\max_{j \\in \\{1,...,N_a\\}} \\left\\{ u\\left((1+r)a_i + y(z) - a_j\\right) + \\beta \\sum_{z' \\in \\{0,1\\}} P_{zz'} V_k(a_j, z') \\right\\} $$\n我们必须强制执行消费 $c = (1+r)a_i + y(z) - a_j$ 必须为正的约束；否则，效用被视为 $-\\infty$，使得该 $a_j$ 的选择不可行。迭代持续进行，直到价值函数收敛，即连续迭代值之差的上确界范数 $\\|V_{k+1} - V_k\\|_{\\infty}$ 小于一个小的容差。这个过程产生收敛的价值函数 $V^{\\star}$ 和一个最优策略函数 $\\pi(a_i, z) = a_j$，该函数将每个状态 $(a_i, z)$ 映射到一个最优的下一期资产选择 $a_j$。这通过向量化得以高效实现。\n\n最优策略 $\\pi$ 和外生就业转移矩阵 $P$ 共同在离散状态空间 $S$ 上定义了一个时间齐次的马尔可夫链。我们构建一个 $2N_a \\times 2N_a$ 的转移矩阵 $Q$。让状态被排序，使得前 $N_a$ 个索引对应于状态 $(a_i, 0)$（$i=1,\\dots,N_a$），后 $N_a$ 个索引对应于状态 $(a_i, 1)$。元素 $Q_{(i,z), (k,z')}$ 给出了从状态 $(a_i, z)$ 转移到 $(a_k, z')$ 的概率。只有当状态 $(a_i, z)$ 的策略指定选择 $a_k$ 作为下一期资产时，该概率才非零。形式上：\n$$ Q_{(a_i,z), (a_k,z')} = \\begin{cases} P_{zz'}  \\text{if } \\pi(a_i,z) = a_k \\\\ 0  \\text{otherwise} \\end{cases} $$\n这个矩阵 $Q$ 描述了单个家庭在状态空间上的运动定律。\n\n给定转移矩阵 $Q$，家庭的横截面分布 $\\mu$ 根据 $\\mu_{t+1} = \\mu_t Q$ 演化，其中 $\\mu_t$ 是一个大小为 $2N_a$ 的行向量，表示在时间 $t$ 处于每个状态的家庭质量。平稳分布 $\\mu^{\\star}$ 是此映射的一个不动点，满足 $\\mu^{\\star} = \\mu^{\\star} Q$。由于潜在的冲击过程是遍历的，并且家庭进行预防性储蓄，因此这个马尔可夫链也是遍历的，这保证了唯一的平稳分布的存在。我们通过前向迭代计算 $\\mu^{\\star}$：从一个任意的初始分布（例如，均匀分布）开始，我们重复应用更新 $\\mu_{k+1} = \\mu_k Q$，直到 $\\|\\mu_{k+1} - \\mu_k\\|_{\\infty}$ 小于一个收敛容差。\n\n该问题要求我们分析一个政策转移。我们首先计算与失业救济金水平 $b_0$ 相对应的初始平稳状态。这包括为 $b_0$ 找到最优策略 $\\pi_0$、转移核 $Q_0$ 和平稳分布 $\\mu_0$。从 $\\mu_0$ 中，我们计算初始总资产 $A_0 = \\sum_{i=1}^{N_a} \\sum_{z \\in \\{0,1\\}} \\mu_0(a_i, z) \\cdot a_i$。\n接下来，我们为新的救济金水平 $b_1$ 计算新的平稳状态。这将产生一个新的转移核 $Q_1$ 和一个新的平稳分布 $\\mu_1^{\\star}$，从中我们计算新的长期总资产 $A_{\\infty} = \\sum_{i=1}^{N_a} \\sum_{z \\in \\{0,1\\}} \\mu_1^{\\star}(a_i, z) \\cdot a_i$。\n\n最后，我们模拟转移路径。经济在时间 $t=0$ 时以分布 $\\mu_0$ 开始。政策变为 $b_1$ 意味着动态现在由 $Q_1$ 控制。分布按 $\\mu_{t+1} = \\mu_t Q_1$ 演化。我们逐期模拟此路径，在每一步计算总资产 $A_t = \\sum_{i,z} \\mu_t(a_i, z) \\cdot a_i$。我们追踪时间 $t$ 并在满足收敛准则时停止，确定最小的非负整数 $\\tau$ 使得 $\\frac{|A_{\\tau} - A_{\\infty}|}{\\max\\{1, |A_{\\infty}|\\}} \\le \\varepsilon$。如果在最大期限 $T_{max}$ 内未满足此条件，我们设置 $\\tau=T_{\\max}$。每个测试用例的最终输出是三元组 $(\\tau, A_0, A_{\\infty})$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def solve_economy(b, Na, params):\n        \"\"\"\n        Solves the household problem and computes the transition kernel for a given benefit level 'b'.\n        \"\"\"\n        beta, sigma, r, w, P, a_min, a_max = params\n        a_grid = np.linspace(a_min, a_max, Na)\n        y = np.array([b, w])\n\n        # Value Function Iteration\n        v = np.zeros((2, Na))\n        policy_idx = np.zeros((2, Na), dtype=int)\n        vfi_tol = 1e-8\n        max_iter_vfi = 2000\n        \n        for k in range(max_iter_vfi):\n            v_old = v.copy()\n            expected_v = P @ v_old # E[V(a', z') | z] = P_z . V(a',:)\n\n            for z in range(2): # 0: unemployed, 1: employed\n                # Vectorized calculation over current assets a_i\n                cash_on_hand = (1 + r) * a_grid + y[z]\n                \n                # consumption_matrix[i, j] = consumption if current asset is a_i and next asset is a_j\n                consumption_matrix = cash_on_hand.reshape(-1, 1) - a_grid.reshape(1, -1)\n                \n                utility_matrix = np.full((Na, Na), -np.inf)\n                feasible = consumption_matrix > 0\n                utility_matrix[feasible] = (consumption_matrix[feasible]**(1 - sigma)) / (1 - sigma)\n\n                # Bellman RHS for each (i, j) pair\n                # continuation_value[j] = beta * E[V(a_j, z') | z]\n                continuation_value = beta * expected_v[z, :]\n                value_matrix = utility_matrix + continuation_value.reshape(1, -1)\n                \n                v[z, :] = np.max(value_matrix, axis=1)\n                policy_idx[z, :] = np.argmax(value_matrix, axis=1)\n\n            if np.max(np.abs(v - v_old))  vfi_tol:\n                break\n        \n        # Construct transition matrix Q\n        num_states = 2 * Na\n        Q = np.zeros((num_states, num_states))\n        for z in range(2):\n            for i in range(Na):\n                current_state_idx = z * Na + i\n                next_asset_idx = policy_idx[z, i]\n                \n                # Transition to unemployed state with next asset\n                next_state_unemp_idx = 0 * Na + next_asset_idx\n                Q[current_state_idx, next_state_unemp_idx] = P[z, 0]\n                \n                # Transition to employed state with next asset\n                next_state_emp_idx = 1 * Na + next_asset_idx\n                Q[current_state_idx, next_state_emp_idx] = P[z, 1]\n                \n        return Q, a_grid\n\n    def compute_stationary_distribution(Q, Na):\n        \"\"\"\n        Computes the stationary distribution for a given transition matrix Q.\n        \"\"\"\n        num_states = 2 * Na\n        mu = np.ones(num_states) / num_states\n        dist_tol = 1e-12\n        max_iter_dist = 5000\n\n        for _ in range(max_iter_dist):\n            mu_new = mu @ Q\n            if np.max(np.abs(mu_new - mu))  dist_tol:\n                return mu_new\n            mu = mu_new\n        return mu\n\n    def run_case(b0, b1, Na):\n        \"\"\"\n        Runs a single test case scenario.\n        \"\"\"\n        # Fixed parameters\n        beta = 0.96\n        sigma = 2.0\n        r = 0.01\n        w = 1.0\n        P = np.array([[0.70, 0.30], [0.05, 0.95]])\n        a_min = 0.0\n        a_max = 50.0\n        params = (beta, sigma, r, w, P, a_min, a_max)\n        \n        # Transition path parameters\n        eps_tau = 1e-6\n        T_max = 2000\n\n        # --- Initial Steady State (b0) ---\n        Q0, a_grid = solve_economy(b0, Na, params)\n        mu0 = compute_stationary_distribution(Q0, Na)\n        full_asset_vector = np.tile(a_grid, 2)\n        A0 = np.dot(mu0, full_asset_vector)\n\n        # --- Handle no-change case ---\n        if b0 == b1:\n            A_inf = A0\n            tau = 0\n            return tau, A0, A_inf\n            \n        # --- New Steady State (b1) ---\n        Q1, _ = solve_economy(b1, Na, params)\n        mu1_star = compute_stationary_distribution(Q1, Na)\n        A_inf = np.dot(mu1_star, full_asset_vector)\n        \n        # --- Transition Path ---\n        mu_t = mu0\n        tau = T_max\n        denominator = max(1.0, abs(A_inf))\n\n        for t in range(T_max + 1):\n            A_t = np.dot(mu_t, full_asset_vector)\n            if abs(A_t - A_inf) / denominator = eps_tau:\n                tau = t\n                break\n            mu_t = mu_t @ Q1\n\n        return tau, A0, A_inf\n\n    # Define test cases\n    test_cases = [\n        {'b0': 0.10, 'b1': 0.20, 'Na': 80},  # Test case 1\n        {'b0': 0.15, 'b1': 0.15, 'Na': 80},  # Test case 2\n        {'b0': 0.05, 'b1': 0.40, 'Na': 80},  # Test case 3\n    ]\n\n    all_results = []\n    for case in test_cases:\n        tau, A0, A_inf = run_case(case['b0'], case['b1'], case['Na'])\n        all_results.append(tau)\n        all_results.append(f\"{A0:.6f}\")\n        all_results.append(f\"{A_inf:.6f}\")\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}