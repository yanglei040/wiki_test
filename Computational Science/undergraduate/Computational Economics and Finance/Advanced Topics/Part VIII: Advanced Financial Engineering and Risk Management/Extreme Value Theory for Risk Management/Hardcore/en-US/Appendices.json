{
    "hands_on_practices": [
        {
            "introduction": "Our journey into the practical application of Extreme Value Theory begins with the block maxima method, a foundational and intuitive approach for modeling the largest events. By partitioning a time series into non-overlapping periods, such as months or years, and extracting the maximum value from each, we can focus directly on the behavior of extreme outcomes. This practice guides you through a complete risk analysis workflow, from simulating seasonal electricity demand to fitting a Generalized Extreme Value (GEV) distribution and calculating crucial risk metrics like Value-at-Risk (VaR) and Expected Shortfall (ES) . Mastering this process builds a solid base for understanding how EVT is used to manage risks tied to rare but high-impact events.",
            "id": "2391840",
            "problem": "Write a complete program that applies Extreme Value Theory (EVT) to model the maximum daily electricity demand in Texas to support pricing peak-load electricity derivatives and assessing grid stability risk. Your implementation must start from the following fundamental base: the Fisher–Tippett–Gnedenko theorem, the definition of block maxima and the Generalized Extreme Value distribution, the definitions of quantile and Expected Shortfall, and standard principles of risk-neutral valuation. You must not use any shortcut formulas; compute quantities from first principles and, when needed, by numerical integration. Angles used inside trigonometric functions must be in radians. All electricity quantities must be treated in gigawatts (GW). Monetary scaling is normalized to $1$ per gigawatt so that derivative prices are expressed in the same numerical units as demand. All probabilities must be expressed as decimals.\n\nThe data-generating mechanism for synthetic daily electricity demand is as follows. Let daily demand $D_{t}$ for day index $t \\in \\{0,1,\\dots,N-1\\}$ be\n$$\nD_{t} = \\max\\left\\{0, \\ \\mu + A \\sin\\left(\\frac{2\\pi \\, (t \\bmod 365)}{365}\\right) + \\sigma \\, \\varepsilon_{t}\\right\\},\n$$\nwhere $\\varepsilon_{t}$ are independent and identically distributed draws from a Student $t$ distribution with $\\nu$ degrees of freedom. The symbol $\\max\\{\\cdot,\\cdot\\}$ enforces nonnegativity. The parameter $N$ equals $365 \\times Y$ for a sample spanning $Y$ years.\n\nYour program must perform the following steps for each test case:\n- Step $1$: Simulate the daily demand series $\\{D_{t}\\}_{t=0}^{N-1}$ using the specified random seed and parameters $(Y,\\ \\mu,\\ A,\\ \\sigma,\\ \\nu)$.\n- Step $2$: Partition the days into non-overlapping blocks of $B$ days (discard any leftover days) to form block maxima $\\{M_{i}\\}_{i=1}^{m}$ where $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$, and $M_{i} = \\max\\{D_{t} : t \\in \\text{block } i\\}$.\n- Step $3$: Fit a Generalized Extreme Value (GEV) distribution to the block maxima by maximum likelihood, obtaining parameters $(\\xi, \\mu_{\\text{gev}}, \\sigma_{\\text{gev}})$ where $\\xi$ is the shape parameter, $\\mu_{\\text{gev}}$ is the location, and $\\sigma_{\\text{gev}}$ is the scale.\n- Step $4$: Compute the one-step-ahead block-maximum Value-at-Risk at level $\\alpha$, defined as the $\\alpha$-quantile $q_{\\alpha}$ of the fitted GEV distribution for the maximum in the next block of length $B$ days.\n- Step $5$: Compute the one-step-ahead block-maximum Expected Shortfall at level $\\alpha$, defined as\n$$\n\\text{ES}_{\\alpha} \\equiv \\mathbb{E}\\!\\left[M \\mid M  q_{\\alpha}\\right] = \\frac{1}{1-\\alpha}\\int_{\\alpha}^{1} Q(u)\\,du,\n$$\nwhere $Q(u)$ is the quantile function of the fitted GEV distribution for $M$. Evaluate the integral numerically to sufficient precision.\n- Step $6$: Consider a European call option on the upcoming block maximum with maturity equal to the block length, i.e., $\\tau = \\frac{B}{365}$ years. The option payoff is $(M - K)^{+}$, where $K$ is the strike and $(x)^{+} \\equiv \\max\\{x,0\\}$. Under a risk-neutral approximation that identifies the fitted GEV with the risk-neutral distribution, compute the undiscounted expected payoff\n$$\n\\mathbb{E}\\!\\left[(M - K)^{+}\\right] = \\int_{0}^{1} \\max\\{Q(u) - K, 0\\}\\,du,\n$$\nand discount it at the continuously compounded risk-free rate $r$ to obtain the price $P = e^{-r \\tau} \\, \\mathbb{E}\\!\\left[(M - K)^{+}\\right]$. Evaluate any required integral numerically.\n- Step $7$: Given a capacity level $L$, compute the grid failure probability $p_{\\text{fail}} = \\mathbb{P}[M  L]$ for the next block maximum under the fitted GEV distribution.\n\nYour program must return, for each test case, a list with the following elements in order:\n$[q_{\\alpha},\\ \\text{ES}_{\\alpha},\\ P,\\ \\xi,\\ p_{\\text{fail}}]$,\nrounded as follows: $q_{\\alpha}$ to $3$ decimals, $\\text{ES}_{\\alpha}$ to $3$ decimals, $P$ to $3$ decimals, $\\xi$ to $4$ decimals, and $p_{\\text{fail}}$ to $6$ decimals. All quantities are floats without unit symbols in the output.\n\nTest suite. Use the following four test cases, each specified as an ordered tuple\n$(\\text{seed},\\ Y,\\ B,\\ \\mu,\\ A,\\ \\sigma,\\ \\nu,\\ \\alpha,\\ K,\\ L,\\ r)$:\n\n- Case $1$: $(12345,\\ 12,\\ 30,\\ 55.0,\\ 12.0,\\ 5.0,\\ 3.5,\\ 0.99,\\ 80.0,\\ 95.0,\\ 0.02)$.\n- Case $2$: $(2024,\\ 5,\\ 7,\\ 45.0,\\ 8.0,\\ 4.0,\\ 5.0,\\ 0.975,\\ 65.0,\\ 85.0,\\ 0.01)$.\n- Case $3$: $(7,\\ 8,\\ 14,\\ 50.0,\\ 6.0,\\ 3.0,\\ 30.0,\\ 0.995,\\ 75.0,\\ 90.0,\\ 0.0)$.\n- Case $4$: $(999,\\ 3,\\ 30,\\ 60.0,\\ 15.0,\\ 7.0,\\ 2.8,\\ 0.98,\\ 85.0,\\ 100.0,\\ 0.03)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of the per-case result lists, enclosed in square brackets. For example, a valid shape would be\n$[[x_{1},y_{1},z_{1},u_{1},v_{1}],[x_{2},y_{2},z_{2},u_{2},v_{2}],\\dots]$,\nwith no additional text before or after.",
            "solution": "We model extremes of electricity demand using block maxima and the Generalized Extreme Value (GEV) distribution. The Fisher–Tippett–Gnedenko theorem states that, under broad conditions, the properly normalized maximum of independent and identically distributed observations converges in distribution to a member of the GEV family. We therefore take block maxima of daily demands and fit a GEV model by maximum likelihood.\n\nData generation. We simulate daily demand $D_{t}$ as\n$$\nD_{t} = \\max\\{0,\\ \\mu + A \\sin(2\\pi \\, t/365) + \\sigma \\varepsilon_{t}\\},\n$$\nwith $\\varepsilon_{t}$ independent and identically distributed draws from a Student $t$ distribution with $\\nu$ degrees of freedom. The angle is in radians. The sine term captures seasonal variation, and the heavy-tailed noise captures rare spikes. Nonnegativity is enforced by the $\\max$ operator.\n\nBlock maxima. We partition the series into blocks of $B$ days and take maxima $M_{i}$ for each block. If we have $N = 365 Y$ total days, the number of full blocks is $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ and any leftover days are discarded to maintain equal block sizes. We collect $\\{M_{i}\\}_{i=1}^{m}$.\n\nGEV fitting. We fit a GEV distribution by maximum likelihood to $\\{M_{i}\\}$. The GEV has cumulative distribution function\n$$\nF(x) = \\exp\\left\\{-\\left[1 + \\xi \\left(\\frac{x - \\mu_{\\text{gev}}}{\\sigma_{\\text{gev}}}\\right)\\right]^{-1/\\xi}\\right\\}\n$$\non the support where $1 + \\xi \\left(\\frac{x - \\mu_{\\text{gev}}}{\\sigma_{\\text{gev}}}\\right)  0$, with the Gumbel case $\\xi = 0$ interpreted by continuity. In the implementation, we use a standard scientific library whose parameterization employs a shape $c$ with $c = -\\xi$. After fitting, we report $\\xi = -c$.\n\nRisk measures. The one-step-ahead block-maximum Value-at-Risk at level $\\alpha$ is the quantile\n$$\nq_{\\alpha} = Q(\\alpha),\n$$\nwhere $Q$ is the quantile function (inverse cumulative distribution function) of the fitted GEV distribution. The Expected Shortfall at level $\\alpha$ is defined by\n$$\n\\text{ES}_{\\alpha} = \\mathbb{E}[M \\mid M  q_{\\alpha}],\n$$\nand can be written via the quantile function as\n$$\n\\text{ES}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} Q(u)\\,du.\n$$\nWe evaluate the integral numerically using adaptive quadrature. For heavy-tailed cases with $\\xi \\ge 1$, this integral would diverge; in our synthetic cases, the fitted $\\xi$ remains below $1$ and the integral is finite. To avoid numerical issues at $u = 1$, we integrate up to $u = 1 - \\varepsilon$ with a very small $\\varepsilon$.\n\nDerivative pricing. Consider a European call on the next block maximum $M$ with strike $K$ and maturity $\\tau = B/365$ years. Under a risk-neutral approximation where the fitted GEV is treated as the risk-neutral distribution, the call price is\n$$\nP = e^{-r \\tau} \\, \\mathbb{E}[(M - K)^{+}].\n$$\nUsing the quantile function, we express the undiscounted expectation as\n$$\n\\mathbb{E}[(M - K)^{+}] = \\int_{0}^{1} \\max\\{Q(u) - K, 0\\}\\,du = \\int_{u_{0}}^{1} Q(u)\\,du - (1 - u_{0}) K,\n$$\nwhere $u_{0} = F(K)$ is the cumulative distribution function at $K$. This identity follows because the area under the tail of the quantile function above $K$ equals the expected exceedance above $K$. We evaluate the integral numerically over $[u_{0}, 1 - \\varepsilon]$ and apply the discount factor $e^{-r \\tau}$.\n\nGrid failure probability. Given a capacity $L$, the failure probability for the next block is\n$$\np_{\\text{fail}} = \\mathbb{P}[M  L] = 1 - F(L),\n$$\ncomputed directly from the fitted GEV cumulative distribution function.\n\nAlgorithmic steps per test case:\n- Fix the seed and generate $N = 365 Y$ days of noise $\\varepsilon_{t}$ from the Student $t$ distribution with $\\nu$ degrees of freedom. Compute seasonal means $\\mu + A \\sin(2\\pi (t \\bmod 365)/365)$ and add scaled noise $\\sigma \\varepsilon_{t}$. Truncate at $0$ to get $D_{t}$.\n- Partition into $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ blocks of length $B$, compute $M_{i} = \\max$ in each block.\n- Fit the GEV by maximum likelihood. Extract shape $c$, location $\\mu_{\\text{gev}}$, and scale $\\sigma_{\\text{gev}}$. Set $\\xi = -c$.\n- Compute $q_{\\alpha} = Q(\\alpha)$ from the fitted model.\n- Compute $\\text{ES}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} Q(u)\\,du$ by numerical quadrature with a small upper cutoff $\\varepsilon$ to avoid singularities.\n- Compute $u_{0} = F(K)$. If $u_{0} \\ge 1$, set the option value to $0$. Otherwise compute the integral $\\int_{u_{0}}^{1} Q(u)\\,du$ numerically and set $P = e^{-r \\tau}\\left(\\int_{u_{0}}^{1} Q(u)\\,du - (1 - u_{0}) K\\right)$ clipped at zero for numerical safety.\n- Compute $p_{\\text{fail}} = 1 - F(L)$ and bound it to $[0,1]$ for numerical stability.\n- Round $q_{\\alpha}$ and $\\text{ES}_{\\alpha}$ to $3$ decimals, $P$ to $3$ decimals, $\\xi$ to $4$ decimals, and $p_{\\text{fail}}$ to $6$ decimals.\n\nThe final output is a single line containing the list of per-case results $[[q_{\\alpha},\\ \\text{ES}_{\\alpha},\\ P,\\ \\xi,\\ p_{\\text{fail}}], \\dots]$ for the four test cases, with no additional text.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import genextreme\nfrom scipy.integrate import quad\n\ndef simulate_daily_demand(seed, years, mu, amplitude, sigma, df):\n    \"\"\"\n    Simulate daily electricity demand (in GW) over 'years' years,\n    with seasonality and heavy-tailed Student-t noise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    days = 365 * years\n    t = np.arange(days)\n    # Angle in radians; seasonality repeats every 365 days\n    seasonal = amplitude * np.sin(2.0 * np.pi * (t % 365) / 365.0)\n    noise = sigma * rng.standard_t(df, size=days)\n    demand = mu + seasonal + noise\n    # Enforce non-negativity\n    demand = np.maximum(0.0, demand)\n    return demand\n\ndef block_maxima(series, block_days):\n    \"\"\"\n    Partition 'series' into non-overlapping blocks of length 'block_days'\n    and return the list of maxima in each block. Discard remainder.\n    \"\"\"\n    n = len(series)\n    m = n // block_days\n    if m = 0:\n        return np.array([])\n    trimmed = series[:m * block_days]\n    reshaped = trimmed.reshape(m, block_days)\n    maxima = reshaped.max(axis=1)\n    return maxima\n\ndef fit_gev_mle(maxima):\n    \"\"\"\n    Fit GEV distribution to block maxima using MLE via scipy.stats.genextreme.\n    SciPy's genextreme parameterization uses shape c = -xi.\n    Returns (xi, loc, scale).\n    \"\"\"\n    # Ensure we have variability\n    if len(maxima)  3 or np.allclose(np.std(maxima), 0.0):\n        # Fallback: trivial fit with tiny scale to avoid errors\n        loc = float(np.mean(maxima)) if len(maxima)  0 else 0.0\n        scale = float(np.std(maxima)) if len(maxima)  0 else 1.0\n        c = 0.0\n    else:\n        c, loc, scale = genextreme.fit(maxima)\n        # Sometimes fit may return non-positive scale; guard it\n        if scale = 0:\n            # Adjust scale to a small positive value\n            scale = max(1e-6, float(np.std(maxima)))\n    xi = -c\n    return xi, loc, scale, c\n\ndef gev_quantile(u, c, loc, scale):\n    \"\"\"Quantile function Q(u) for the fitted GEV in SciPy's parameterization.\"\"\"\n    return genextreme.ppf(u, c, loc=loc, scale=scale)\n\ndef gev_cdf(x, c, loc, scale):\n    \"\"\"CDF F(x) for the fitted GEV in SciPy's parameterization.\"\"\"\n    return genextreme.cdf(x, c, loc=loc, scale=scale)\n\ndef expected_shortfall_alpha(alpha, c, loc, scale):\n    \"\"\"\n    Compute ES_alpha = (1/(1-alpha)) * integral_{alpha}^{1} Q(u) du\n    via numerical quadrature. Integrate up to 1 - eps to avoid endpoint issues.\n    \"\"\"\n    eps = 1e-12\n    upper = 1.0 - eps\n    if alpha = 1.0:\n        return float('inf')\n    # Define integrand with safety checks\n    def integrand(u):\n        q = gev_quantile(u, c, loc, scale)\n        return q\n    try:\n        val, _ = quad(integrand, alpha, upper, epsabs=1e-6, epsrel=1e-6, limit=200)\n        es = val / (1.0 - alpha)\n        return es\n    except Exception:\n        return float('inf')\n\ndef call_price_on_block_max(K, r, block_days, c, loc, scale):\n    \"\"\"\n    Price of a European call option on the next block maximum M with strike K,\n    maturity tau = block_days/365 years, under risk-neutral approximation\n    using the fitted GEV distribution. Uses quantile integral identity:\n    E[(M-K)+] = \\int_{u0}^{1} Q(u) du - (1-u0)K, where u0 = F(K).\n    \"\"\"\n    tau = block_days / 365.0\n    u0 = gev_cdf(K, c, loc, scale)\n    if not np.isfinite(u0):\n        return 0.0\n    if u0 = 1.0:\n        undiscounted = 0.0\n    else:\n        eps = 1e-12\n        upper = 1.0 - eps\n        def integrand(u):\n            return gev_quantile(u, c, loc, scale)\n        try:\n            integral_val, _ = quad(integrand, u0, upper, epsabs=1e-6, epsrel=1e-6, limit=200)\n            undiscounted = integral_val - (1.0 - u0) * K\n            if not np.isfinite(undiscounted):\n                undiscounted = 0.0\n        except Exception:\n            undiscounted = 0.0\n    price = np.exp(-r * tau) * max(0.0, undiscounted)\n    return price\n\ndef risk_metrics_for_case(case):\n    \"\"\"\n    Compute [VaR_alpha, ES_alpha, call_price, xi, p_fail] for one test case.\n    Rounding:\n      VaR, ES, price - 3 decimals\n      xi - 4 decimals\n      p_fail - 6 decimals\n    \"\"\"\n    (seed, Y, B, mu, A, sigma, nu, alpha, K, L, r) = case\n    # Simulate daily demand\n    demand = simulate_daily_demand(seed, Y, mu, A, sigma, nu)\n    # Block maxima\n    maxima = block_maxima(demand, B)\n    if len(maxima)  3:\n        # Ensure there are enough maxima; otherwise pad with mean\n        if len(maxima) == 0:\n            maxima = np.array([mu])\n        elif len(maxima) == 1:\n            maxima = np.array([maxima[0], maxima[0] * 0.99 + 0.01, maxima[0] * 1.01 - 0.01])\n        else:\n            maxima = np.concatenate([maxima, [np.mean(maxima)]*(3 - len(maxima))])\n    # Fit GEV\n    xi, loc, scale, c = fit_gev_mle(maxima)\n    # VaR\n    try:\n        q_alpha = float(gev_quantile(alpha, c, loc, scale))\n        if not np.isfinite(q_alpha):\n            # Fallback: use high empirical quantile\n            q_alpha = float(np.quantile(maxima, alpha))\n    except Exception:\n        q_alpha = float(np.quantile(maxima, alpha))\n    # ES\n    es_alpha = expected_shortfall_alpha(alpha, c, loc, scale)\n    # Call price\n    price = call_price_on_block_max(K, r, B, c, loc, scale)\n    # Failure probability\n    try:\n        p_fail = 1.0 - float(gev_cdf(L, c, loc, scale))\n        if not np.isfinite(p_fail):\n            # Fallback approximation\n            p_fail = max(0.0, min(1.0, 1.0 - float(np.mean(maxima = L))))\n    except Exception:\n        p_fail = max(0.0, min(1.0, 1.0 - float(np.mean(maxima = L))))\n    p_fail = max(0.0, min(1.0, p_fail))\n    # Rounding\n    result = [\n        round(q_alpha, 3),\n        round(es_alpha, 3),\n        round(price, 3),\n        round(xi, 4),\n        round(p_fail, 6),\n    ]\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (seed, Y, B, mu, A, sigma, nu, alpha, K, L, r)\n    test_cases = [\n        (12345, 12, 30, 55.0, 12.0, 5.0, 3.5, 0.99, 80.0, 95.0, 0.02),\n        (2024, 5, 7, 45.0, 8.0, 4.0, 5.0, 0.975, 65.0, 85.0, 0.01),\n        (7, 8, 14, 50.0, 6.0, 3.0, 30.0, 0.995, 75.0, 90.0, 0.0),\n        (999, 3, 30, 60.0, 15.0, 7.0, 2.8, 0.98, 85.0, 100.0, 0.03),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = risk_metrics_for_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line with the nested list representation.\n    print(f\"{results}\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While the block maxima method is powerful, it can be inefficient as it discards all but one data point from each block. The Peak-Over-Threshold (POT) approach offers a more data-rich alternative by modeling all observations that exceed a sufficiently high threshold. In this exercise, you will apply the POT method to a modern operational risk problem: modeling extreme website latency to predict the likelihood of a catastrophic failure . This hands-on task will teach you how to select an appropriate threshold, fit the corresponding Generalized Pareto Distribution (GPD), and use the model to quantify the probability of extreme operational disruptions.",
            "id": "2391805",
            "problem": "You are given a scenario from operational risk management for a major online retailer. During a peak sales event, website latency spikes can trigger cascading failures that constitute catastrophic outages. Extreme Value Theory (EVT) posits that exceedances of a sufficiently high threshold are well modeled by the Generalized Pareto Distribution (GPD) in the limit. Your task is to write a complete program that uses this principle to estimate the tail index (the GPD shape parameter) from synthetic latency data and to compute the probability that at least one catastrophic failure occurs during a finite event, assuming independent requests.\n\nFundamental base:\n- In the peak-over-threshold framework, for a sufficiently high threshold $u$, the distribution of exceedances above $u$ converges to a Generalized Pareto Distribution (GPD) as a limit law.\n- The tail index is the GPD shape parameter, often denoted by $\\xi$.\n- The law of rare events under independence implies that if the per-request catastrophic failure probability is $p$, then the probability of at least one catastrophic failure across $N$ independent requests is $1-(1-p)^N$.\n\nYour program must, for each test case:\n1. Generate a synthetic dataset of website latency observations (in milliseconds) of size $n$ using a two-component mixture:\n   - With probability $p_{\\text{tail}}$, draw from a Pareto tail with minimum $x_m$ and shape $\\alpha$ using the transformation $X_{\\text{tail}} = x_m \\cdot (1 + P)$, where $P$ follows a standard Pareto distribution with shape $\\alpha$ and support on $(0,\\infty)$.\n   - With probability $1-p_{\\text{tail}}$, draw from a Lognormal distribution with log-location parameter $\\mu$ and log-scale parameter $\\sigma$, yielding $X_{\\text{base}} \\sim \\text{Lognormal}(\\mu,\\sigma)$ on $(0,\\infty)$.\n   - Combine draws as $X = X_{\\text{tail}}$ with probability $p_{\\text{tail}}$ and $X = X_{\\text{base}}$ with probability $1-p_{\\text{tail}}$.\n2. Choose a high threshold $u$ as the empirical quantile at level $q_u$ of the sample.\n3. Form exceedances $Y = X - u$ for all observations where $X  u$, and estimate the GPD parameters (shape $\\xi$ and scale $\\beta$) for the exceedances using maximum likelihood, enforcing the GPD location parameter to be $0$.\n4. Estimate the tail probability beyond $u$ as $p_u = k/n$, where $k$ is the number of exceedances.\n5. For a catastrophic latency threshold $z$ with $z  u$, approximate the probability of latency exceeding $z$ via the GPD tail approximation combined with the empirical exceedance rate. Use $p_z \\approx p_u$ multiplied by the conditional tail probability above $u$, and handle the special case $\\xi = 0$ by its continuous limit.\n6. Compute the probability of at least one catastrophic failure during an event comprising $N$ independent requests using the independence assumption.\n7. Report this final probability as a float rounded to six decimal places.\n\nAngle units are not involved. Latency input data are in milliseconds, but the requested outputs are probabilities and therefore unitless. You must express the final probability results as decimals.\n\nTest suite:\nProvide results for the following three parameter sets. For each case, apply the exact values given.\n\n- Case A:\n  - Seed $= 202311$\n  - $n = 50000$\n  - $p_{\\text{tail}} = 0.15$\n  - Pareto shape $\\alpha = 2.0$\n  - Pareto minimum $x_m = 150$\n  - Lognormal $\\mu = 3.7$\n  - Lognormal $\\sigma = 0.35$\n  - Threshold quantile $q_u = 0.95$\n  - Catastrophic threshold $z = 1000$\n  - Event size $N = 200000$\n- Case B:\n  - Seed $= 7$\n  - $n = 80000$\n  - $p_{\\text{tail}} = 0.05$\n  - Pareto shape $\\alpha = 1.4$\n  - Pareto minimum $x_m = 200$\n  - Lognormal $\\mu = 3.5$\n  - Lognormal $\\sigma = 0.5$\n  - Threshold quantile $q_u = 0.97$\n  - Catastrophic threshold $z = 1500$\n  - Event size $N = 1000000$\n- Case C:\n  - Seed $= 4242$\n  - $n = 60000$\n  - $p_{\\text{tail}} = 0.10$\n  - Pareto shape $\\alpha = 3.5$\n  - Pareto minimum $x_m = 120$\n  - Lognormal $\\mu = 3.6$\n  - Lognormal $\\sigma = 0.4$\n  - Threshold quantile $q_u = 0.90$\n  - Catastrophic threshold $z = 800$\n  - Event size $N = 100000$\n\nFinal output format:\n- Your program should produce a single line of output containing the probabilities for the three cases as a comma-separated list enclosed in square brackets, with each probability rounded to six decimal places. For example, the format must be exactly like $[r_1,r_2,r_3]$ where each $r_i$ is a decimal rendered with six digits after the decimal point.",
            "solution": "The problem statement is deemed valid. It presents a well-posed, scientifically grounded problem in quantitative risk management based on Extreme Value Theory. All necessary components for a unique, verifiable solution are provided. We proceed with the derivation and implementation of the solution.\n\nThe core task is to estimate the probability of a catastrophic website latency event by modeling the tail of the latency distribution. The methodology follows the principles of the Peak-Over-Threshold (POT) framework.\n\n1.  **Synthetic Data Generation**\n    The latency data $X$ is synthesized from a two-component mixture model of size $n$. This model captures both the typical behavior and the extreme events.\n    -   A 'base' component, drawn from a Lognormal distribution $X_{\\text{base}} \\sim \\text{Lognormal}(\\mu, \\sigma)$, represents the majority of latency observations. The Lognormal distribution is frequently used to model non-negative quantities with positive skew.\n    -   A 'tail' component, representing extreme latencies, is drawn from a Pareto distribution. The problem specifies the generation as $X_{\\text{tail}} = x_m \\cdot (1 + P)$, where $P$ is a standard Pareto variate on $(0, \\infty)$ with shape $\\alpha$. A standard Pareto distribution on $(0, \\infty)$ is interpreted as a Lomax distribution with scale parameter $\\lambda=1$. A variate $P$ from this distribution can be generated via inverse transform sampling as $P = U^{-1/\\alpha} - 1$, where $U \\sim \\text{Uniform}(0,1)$. Substituting this into the expression for $X_{\\text{tail}}$ gives $X_{\\text{tail}} = x_m (1 + (U^{-1/\\alpha} - 1)) = x_m U^{-1/\\alpha}$. This is the formula for generating a variate from a Pareto Type I distribution with minimum value $x_m$ and shape parameter $\\alpha$. This interpretation is both standard and self-consistent.\n    Each sample is drawn from the tail component with probability $p_{\\text{tail}}$ and from the base component with probability $1 - p_{\\text{tail}}$.\n\n2.  **Threshold Selection and Exceedances**\n    The POT method requires defining a high threshold $u$ to separate extreme events from the bulk of the data. We select $u$ as the empirical quantile of the synthetic dataset at a high probability level $q_u$. Observations $X_i$ that exceed this threshold give rise to exceedances, defined as $Y_i = X_i - u$.\n\n3.  **Generalized Pareto Distribution (GPD) Fitting**\n    According to the Pickands–Balkema–de Haan theorem, for a sufficiently high threshold $u$, the distribution of exceedances $Y = X - u$ converges to a Generalized Pareto Distribution (GPD). The GPD cumulative distribution function (CDF) is given by:\n    $$ G_{\\xi, \\beta}(y) = \\begin{cases} 1 - \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-1/\\xi}  \\text{if } \\xi \\neq 0 \\\\ 1 - \\exp(-y/\\beta)  \\text{if } \\xi = 0 \\end{cases} $$\n    for $y  0$. The parameters are the shape $\\xi$ (the tail index) and the scale $\\beta  0$. The location parameter is fixed at $0$ as per the definition of exceedances.\n    The parameters $(\\xi, \\beta)$ are estimated by maximizing the log-likelihood function of the GPD for the observed exceedances. This is a numerical optimization problem which is reliably solved using established library functions, such as `scipy.stats.genpareto.fit`, which implements Maximum Likelihood Estimation (MLE).\n\n4.  **Extreme Event Probability Estimation**\n    The probability of a single latency $X$ exceeding the catastrophic threshold $z$ (where $z  u$) is denoted $p_z = P(X  z)$. Using the law of total probability, we decompose this as:\n    $$ p_z = P(X  z | X  u) \\cdot P(X  u) $$\n    -   The term $P(X  u)$ is estimated empirically from the data. Let $k$ be the number of observations exceeding $u$ out of a total of $n$ observations. Then, $P(X  u) \\approx p_u = k/n$.\n    -   The conditional probability $P(X  z | X  u)$ is the probability that an exceedance $Y = X-u$ is greater than $z-u$. This probability is given by the survival function of the fitted GPD, $S_{\\text{GPD}}(y) = 1 - G_{\\text{GPD}}(y)$, evaluated at $y = z-u$.\n    $$ P(X  z | X  u) = S_{\\text{GPD}}(z-u) = \\left(1 + \\frac{\\hat{\\xi} (z-u)}{\\hat{\\beta}}\\right)^{-1/\\hat{\\xi}} $$\n    where $\\hat{\\xi}$ and $\\hat{\\beta}$ are the MLE estimates of the GPD parameters. The case $\\xi=0$ is handled by its continuous limit, resulting in an exponential survival function.\n    Combining these gives the final estimate for $p_z$:\n    $$ p_z \\approx \\frac{k}{n} \\left(1 + \\frac{\\hat{\\xi} (z-u)}{\\hat{\\beta}}\\right)^{-1/\\hat{\\xi}} $$\n\n5.  **Aggregate Risk Probability**\n    The final quantity of interest is the probability of at least one catastrophic failure during an event comprising $N$ independent requests. If the probability of a single failure is $p_z$, the probability of no failures in $N$ trials is $(1 - p_z)^N$. Therefore, the probability of at least one failure is:\n    $$ P(\\text{at least one}) = 1 - (1 - p_z)^N $$\n    For small $p_z$, direct computation of this expression can lead to loss of numerical precision. A more stable calculation is achieved by using logarithmic and exponential-minus-one functions: $P(\\text{at least one}) = -\\text{expm1}(N \\cdot \\text{log1p}(-p_z))$. This avoids catastrophic cancellation and maintains precision. The final result is rounded to six decimal places as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import genpareto\n\ndef run_case(seed, n, p_tail, alpha, xm, mu, sigma, q_u, z, N):\n    \"\"\"\n    Solves a single test case for catastrophic failure probability estimation.\n    \"\"\"\n    # Step 1: Generate synthetic dataset from a two-component mixture model.\n    rng = np.random.default_rng(seed)\n    \n    # Determine which samples come from the tail vs. the base distribution.\n    is_tail = rng.random(size=n)  p_tail\n    n_tail = np.sum(is_tail)\n    n_base = n - n_tail\n\n    # Generate Pareto tail data using inverse transform sampling.\n    # The generation rule X_tail = xm * (1 + P) where P is standard Pareto on (0,inf)\n    # simplifies to sampling from a Pareto Type I distribution with minimum xm.\n    uniform_samples = rng.random(size=n_tail)\n    data_tail = xm / (uniform_samples**(1/alpha))\n\n    # Generate Lognormal base data.\n    data_base = rng.lognormal(mean=mu, sigma=sigma, size=n_base)\n\n    # Combine into a single dataset.\n    data = np.empty(n, dtype=float)\n    data[is_tail] = data_tail\n    data[~is_tail] = data_base\n\n    # Step 2: Choose a high threshold 'u' as an empirical quantile.\n    u = np.quantile(data, q_u)\n\n    # Step 3: Form exceedances and fit the Generalized Pareto Distribution (GPD).\n    exceedances = data[data  u] - u\n    \n    if len(exceedances) == 0:\n        # If there are no exceedances, the probability of an even more extreme event is zero.\n        return 0.0\n\n    # Use scipy's robust Maximum Likelihood Estimation for GPD parameters (xi, beta).\n    # 'floc=0' enforces the location parameter to be 0, as per the definition of exceedances.\n    # The fit returns (shape, location, scale) which correspond to (xi, 0, beta).\n    xi, _, beta = genpareto.fit(exceedances, floc=0)\n\n    # Step 4: Estimate the empirical probability of exceeding the threshold u.\n    k = len(exceedances)\n    p_u = k / n\n\n    # Step 5: Approximate the probability of a single latency exceeding z.\n    # p_z = P(X  z) = P(X  u) * P(X  z | X  u)\n    # The conditional probability is calculated using the GPD survival function.\n    # The problem statement ensures z  u, so (z - u)  0.\n    y_z = z - u\n    prob_cond_exceed_z = genpareto.sf(y_z, c=xi, scale=beta, loc=0)\n    p_z = p_u * prob_cond_exceed_z\n\n    # Step 6: Compute the probability of at least one catastrophic failure in N requests.\n    # The calculation uses numerically stable functions to avoid precision loss.\n    # P(at least one) = 1 - (1 - p_z)^N = -expm1(N * log1p(-p_z))\n    if p_z = 1.0:\n        # If a single event is guaranteed to be catastrophic, so is any sequence of events.\n        prob_final = 1.0\n    elif p_z = 0.0:\n        prob_final = 0.0\n    else:\n        prob_final = -np.expm1(N * np.log1p(-p_z))\n\n    return prob_final\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        (202311, 50000, 0.15, 2.0, 150, 3.7, 0.35, 0.95, 1000, 200000),\n        # Case B\n        (7, 80000, 0.05, 1.4, 200, 3.5, 0.5, 0.97, 1500, 1000000),\n        # Case C\n        (4242, 60000, 0.10, 3.5, 120, 3.6, 0.4, 0.90, 800, 100000),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Execute the main logic for one case.\n        final_probability = run_case(*case)\n        # Format the result to six decimal places.\n        results.append(f\"{final_probability:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Financial crises are rarely caused by a single isolated event; they often arise from the simultaneous occurrence of extreme shocks across multiple markets. To analyze this systemic risk, we must move beyond univariate models to the domain of multivariate Extreme Value Theory. This practice introduces the use of copulas to model the dependence structure between extreme events, such as a concurrent spike in oil prices and a major stock market decline . By combining marginal GPD models for each risk factor with a Gumbel-Hougaard copula, you will learn how to compute the joint probability of a compound crisis, a vital skill for modern financial risk management.",
            "id": "2391764",
            "problem": "You are tasked with implementing a program that computes the joint tail probability of two financial risk events using multivariate extreme value theory. Let $X$ denote the one-day oil spot price jump in United States Dollar (USD) per barrel (right tail, large positive moves), and let $Y$ denote the one-day equity index loss magnitude in percentage points (right tail, large losses expressed as absolute percent points, not with a percentage sign). The objective is to compute, for specified thresholds $x^\\star$ (in USD) and $y^\\star$ (in percentage points), the probability $\\mathbb{P}(X  x^\\star, Y  y^\\star)$ under the following model assumptions.\n\nModel specification:\n- Marginal tails: For $X$ and $Y$, use threshold exceedance models with the Generalized Pareto Distribution (GPD) beyond fixed thresholds $u_X$ and $u_Y$ respectively. For $X$, define the excess $Z_X = X - u_X$ given $X  u_X$. For $Y$, define the excess $Z_Y = Y - u_Y$ given $Y  u_Y$. The conditional survival function for a GPD excess is\n  $$ \\mathbb{P}(Z  z \\mid Z \\ge 0) = \\begin{cases}\n  \\left(1 + \\dfrac{\\xi z}{\\beta}\\right)^{-1/\\xi},  \\xi \\ne 0, \\, 1 + \\dfrac{\\xi z}{\\beta}  0, \\\\\n  \\exp\\!\\left(-\\dfrac{z}{\\beta}\\right),  \\xi = 0,\n  \\end{cases} $$\n  where $\\beta  0$ is the scale parameter and $\\xi$ is the shape parameter. For $\\xi  0$, the support is bounded above by $z  -\\beta/\\xi$; beyond this bound, the tail probability is zero. The unconditional tail probability at any $t \\ge u$ is\n  $$ \\mathbb{P}(X  t) = \\mathbb{P}(X  u_X) \\cdot \\mathbb{P}(Z_X  t - u_X \\mid X  u_X), \\quad \\mathbb{P}(Y  t) = \\mathbb{P}(Y  u_Y) \\cdot \\mathbb{P}(Z_Y  t - u_Y \\mid Y  u_Y). $$\n  The quantities $\\mathbb{P}(X  u_X)$ and $\\mathbb{P}(Y  u_Y)$ are given as inputs.\n- Dependence structure: The joint distribution of $(X,Y)$ is specified by an extreme value copula of Gumbel–Hougaard form with parameter $\\theta \\ge 1$:\n  $$ C(u,v) = \\exp\\!\\left(-\\left((-\\ln u)^{\\theta} + (-\\ln v)^{\\theta}\\right)^{1/\\theta}\\right), \\quad u,v \\in (0,1]. $$\n- Joint tail probability: For any thresholds $x^\\star \\ge u_X$ and $y^\\star \\ge u_Y$, with marginal distribution functions $F_X$ and $F_Y$, the joint tail probability satisfies\n  $$ \\mathbb{P}(X  x^\\star, Y  y^\\star) = 1 - F_X(x^\\star) - F_Y(y^\\star) + C(F_X(x^\\star), F_Y(y^\\star)). $$\n\nGlobal thresholds:\n- Oil price jump threshold $u_X = 10$ USD.\n- Equity loss threshold $u_Y = 2$ percentage points.\n\nYour task:\n- For each test case below, compute the value of $\\mathbb{P}(X  x^\\star, Y  y^\\star)$ as a decimal number.\n- If a threshold $t$ exceeds the finite upper endpoint implied by a negative shape parameter $\\xi  0$ (i.e., $t - u \\ge -\\beta/\\xi$), the corresponding marginal tail probability is zero.\n- All results must be expressed as decimals (no percentage sign) and rounded to six decimal places.\n\nTest suite (each line specifies $(\\beta_X, \\xi_X, p_{X0}, \\beta_Y, \\xi_Y, p_{Y0}, \\theta, x^\\star, y^\\star)$, where $p_{X0} = \\mathbb{P}(X  u_X)$ and $p_{Y0} = \\mathbb{P}(Y  u_Y)$):\n1. $(\\beta_X = 8, \\, \\xi_X = 0.2, \\, p_{X0} = 0.05, \\, \\beta_Y = 1.5, \\, \\xi_Y = 0.1, \\, p_{Y0} = 0.10, \\, \\theta = 2, \\, x^\\star = 20, \\, y^\\star = 5)$\n2. $(\\beta_X = 5, \\, \\xi_X = 0, \\, p_{X0} = 0.02, \\, \\beta_Y = 1.2, \\, \\xi_Y = 0.2, \\, p_{Y0} = 0.04, \\, \\theta = 1, \\, x^\\star = 25, \\, y^\\star = 3)$\n3. $(\\beta_X = 10, \\, \\xi_X = 0.3, \\, p_{X0} = 0.03, \\, \\beta_Y = 2.0, \\, \\xi_Y = 0.3, \\, p_{Y0} = 0.08, \\, \\theta = 10, \\, x^\\star = 10, \\, y^\\star = 2)$\n4. $(\\beta_X = 6, \\, \\xi_X = 0.2, \\, p_{X0} = 0.04, \\, \\beta_Y = 1.5, \\, \\xi_Y = -0.2, \\, p_{Y0} = 0.03, \\, \\theta = 1.5, \\, x^\\star = 20, \\, y^\\star = 10)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3,r4]\"), in the same order as the test cases. Each $r_i$ must be a float rounded to six decimal places.",
            "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded in the established principles of multivariate extreme value theory, a standard methodology in computational finance for risk management. The problem is well-posed, providing all necessary definitions, parameters, and a clear, objective computational goal. It is self-contained and free of contradictions or ambiguities. Therefore, a solution will be provided.\n\nThe objective is to compute the joint tail probability $\\mathbb{P}(X  x^\\star, Y  y^\\star)$ for two random variables, $X$ and $Y$, representing financial risk events. The problem specifies a model for the marginal tails and their dependence structure. The solution is derived by systematically applying the provided formulas.\n\nThe fundamental formula for the joint tail probability, based on the inclusion-exclusion principle and the definition of a copula, is given as:\n$$ \\mathbb{P}(X  x^\\star, Y  y^\\star) = 1 - F_X(x^\\star) - F_Y(y^\\star) + C(F_X(x^\\star), F_Y(y^\\star)) $$\nwhere $F_X$ and $F_Y$ are the marginal cumulative distribution functions (CDFs) of $X$ and $Y$, respectively, and $C(u,v)$ is their copula.\n\nThe process to compute this probability consists of three main steps:\n1.  Compute the marginal CDF value for $X$, $F_X(x^\\star)$.\n2.  Compute the marginal CDF value for $Y$, $F_Y(y^\\star)$.\n3.  Substitute these values into the copula function and then into the main joint probability formula.\n\n**Step 1  2: Calculation of Marginal CDFs**\n\nThe CDF is related to the survival function (or tail probability) by $F(t) = 1 - \\mathbb{P}(X  t)$. The problem provides a model for the unconditional tail probability for any threshold $t$ greater than or equal to a high threshold $u$. For variable $X$, this is:\n$$ \\mathbb{P}(X  t) = \\mathbb{P}(X  u_X) \\cdot \\mathbb{P}(Z_X  t - u_X \\mid X  u_X) $$\nwhere $u_X = 10$ is the fixed threshold for $X$. Let $p_{X0} = \\mathbb{P}(X  u_X)$ be the given probability of exceeding this threshold. The term $\\mathbb{P}(Z_X  t - u_X \\mid X  u_X)$ is the survival function of the excess $Z_X = X - u_X$ over the threshold $u_X$. This excess is modeled by a Generalized Pareto Distribution (GPD).\n\nThe survival function for a GPD-distributed random variable $Z$ with scale parameter $\\beta  0$ and shape parameter $\\xi$ is given for an excess $z  0$ as:\n$$ S_{GPD}(z; \\beta, \\xi) = \\mathbb{P}(Z  z \\mid Z \\ge 0) = \\begin{cases}\n\\left(1 + \\dfrac{\\xi z}{\\beta}\\right)^{-1/\\xi},  \\text{if } \\xi \\ne 0 \\\\\n\\exp\\left(-\\dfrac{z}{\\beta}\\right),  \\text{if } \\xi = 0\n\\end{cases} $$\nThis formula is valid for $z$ such that $1 + \\xi z / \\beta  0$. If $\\xi  0$, the distribution has a finite upper endpoint at $z_{max} = -\\beta/\\xi$. For any excess $z \\ge z_{max}$, the survival probability is $0$.\n\nThus, for a given threshold $x^\\star \\ge u_X$, we first calculate the excess $z_X = x^\\star - u_X$. Then we find the conditional survival probability $S_{GPD}(z_X; \\beta_X, \\xi_X)$. The unconditional tail probability for $X$ is:\n$$ \\mathbb{P}(X  x^\\star) = p_{X0} \\cdot S_{GPD}(z_X; \\beta_X, \\xi_X) $$\nFinally, the marginal CDF value is $F_X(x^\\star) = 1 - \\mathbb{P}(X  x^\\star)$.\n\nAn identical procedure is followed for variable $Y$, using its specific parameters $(\\beta_Y, \\xi_Y, p_{Y0})$ and threshold $y^\\star$ relative to $u_Y = 2$.\n\n**Step 3: Calculation of Joint Probability**\n\nThe dependence between the marginal distributions is modeled by the Gumbel-Hougaard copula with parameter $\\theta \\ge 1$:\n$$ C(u,v) = \\exp\\left(-\\left((-\\ln u)^{\\theta} + (-\\ln v)^{\\theta}\\right)^{1/\\theta}\\right) $$\nwhere $u = F_X(x^\\star)$ and $v = F_Y(y^\\star)$ are the marginal CDF values computed in the previous steps.\n\nThe algorithm to solve for each test case is as follows:\n1.  For a given set of parameters $(\\beta_X, \\xi_X, p_{X0}, \\beta_Y, \\xi_Y, p_{Y0}, \\theta, x^\\star, y^\\star)$ and global thresholds $u_X=10, u_Y=2$:\n2.  Calculate the excess for $X$: $z_X = x^\\star - u_X$.\n3.  Calculate the GPD survival probability for $z_X$, $S_{GPD,X}$, accounting for the cases $\\xi_X = 0$ and $\\xi_X  0$.\n4.  Calculate the unconditional tail probability for $X$: $S_X = p_{X0} \\cdot S_{GPD,X}$.\n5.  Calculate the marginal CDF for $X$: $F_X = 1 - S_X$.\n6.  Repeat steps 2-5 for variable $Y$ to find $F_Y = 1 - p_{Y0} \\cdot S_{GPD}(y^\\star - u_Y; \\beta_Y, \\xi_Y)$.\n7.  If either $F_X=1$ or $F_Y=1$, the joint tail probability $\\mathbb{P}(X  x^\\star, Y  y^\\star)$ is $0$, as the event $(X  x^\\star, Y  y^\\star)$ is a subset of an event with probability zero.\n8.  Otherwise, calculate the copula value $C(F_X, F_Y)$ using the Gumbel-Hougaard formula.\n9.  Compute the final joint tail probability: $P_{joint} = 1 - F_X - F_Y + C(F_X, F_Y)$.\n10. Round the result to six decimal places as required.\n\nThis procedure rigorously integrates the specified models for marginal tails and dependence to arrive at the desired risk measure.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the joint tail probability of two financial risk events\n    using multivariate extreme value theory.\n    \"\"\"\n\n    # Global thresholds for the models.\n    u_X = 10.0  # USD\n    u_Y = 2.0   # percentage points\n\n    # Test cases: (beta_X, xi_X, p_X0, beta_Y, xi_Y, p_Y0, theta, x_star, y_star)\n    test_cases = [\n        (8.0, 0.2, 0.05, 1.5, 0.1, 0.10, 2.0, 20.0, 5.0),\n        (5.0, 0.0, 0.02, 1.2, 0.2, 0.04, 1.0, 25.0, 3.0),\n        (10.0, 0.3, 0.03, 2.0, 0.3, 0.08, 10.0, 10.0, 2.0),\n        (6.0, 0.2, 0.04, 1.5, -0.2, 0.03, 1.5, 20.0, 10.0),\n    ]\n\n    def gpd_survival(z, beta, xi):\n        \"\"\"\n        Calculates the survival function P(Z  z) for a GPD.\n        \"\"\"\n        # Excess z must be non-negative as z = t - u, and problem states t = u.\n        if z  0:\n            return 1.0\n\n        # Case 1: xi  0 (distribution has a finite upper endpoint)\n        if xi  0:\n            upper_endpoint = -beta / xi\n            if z = upper_endpoint:\n                return 0.0\n        \n        # Case 2: xi = 0 (Exponential distribution)\n        if xi == 0.0:\n            return np.exp(-z / beta)\n\n        # Case 3: xi != 0 (General GPD case)\n        # The check for xi  0 above ensures base  0 for valid z.\n        base = 1.0 + xi * z / beta\n        return base**(-1.0 / xi)\n\n    results = []\n    for case in test_cases:\n        beta_X, xi_X, p_X0, beta_Y, xi_Y, p_Y0, theta, x_star, y_star = case\n\n        # --- Calculate marginal probability for X ---\n        if x_star  u_X:\n            # Per problem, x_star = u_X, but handle for robustness\n            S_X = 1.0\n        else:\n            z_X = x_star - u_X\n            cond_surv_X = gpd_survival(z_X, beta_X, xi_X)\n            S_X = p_X0 * cond_surv_X\n        F_X = 1.0 - S_X\n\n        # --- Calculate marginal probability for Y ---\n        if y_star  u_Y:\n            # Per problem, y_star = u_Y, but handle for robustness\n            S_Y = 1.0\n        else:\n            z_Y = y_star - u_Y\n            cond_surv_Y = gpd_survival(z_Y, beta_Y, xi_Y)\n            S_Y = p_Y0 * cond_surv_Y\n        F_Y = 1.0 - S_Y\n\n        # If one of the marginal tail probabilities is zero, the joint probability is zero.\n        if S_X == 0.0 or S_Y == 0.0:\n            joint_tail_prob = 0.0\n        else:\n            # --- Calculate Gumbel-Hougaard copula value ---\n            # Handles special cases of independence (theta=1) and comonotonicity (theta-inf)\n            term_u = (-np.log(F_X))**theta\n            term_v = (-np.log(F_Y))**theta\n            copula_val = np.exp(-((term_u + term_v)**(1.0 / theta)))\n\n            # --- Calculate joint tail probability ---\n            # P(X  x, Y  y) = 1 - F_X(x) - F_Y(y) + C(F_X(x), F_Y(y))\n            joint_tail_prob = 1.0 - F_X - F_Y + copula_val\n        \n        results.append(round(joint_tail_prob, 6))\n\n    # Final print statement in the exact required format.\n    # The output format requires string representations of floats, rounded.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}