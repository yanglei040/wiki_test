{
    "hands_on_practices": [
        {
            "introduction": "矩阵范数，特别是 2-范数，是衡量矩阵“大小”或“放大能力”的核心工具。但它的计算并非总是直截了当，因为它涉及到寻找一个能被矩阵最大程度拉伸的向量。本练习将指导你通过幂迭代法这一经典的数值算法，从第一性原理出发来估计矩阵的 2-范数，而无需构建复杂的矩阵乘积 $A^\\top A$。通过这个实践，你将深刻理解矩阵 2-范数、最大奇异值以及 $A^\\top A$ 的最大特征值之间的内在联系，并掌握一个在实际中广泛应用的计算方法 。",
            "id": "2449590",
            "problem": "要求您设计并实现一个确定性程序，该程序使用基于幂迭代的算法来估计实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 $2$-范数 $\\lVert A \\rVert_2$。推导过程仅能从向量和矩阵范数的基本定义以及对称矩阵特征值的基本性质出发。您的目标是推导、论证并编写一个算法，该算法不依赖于显式构造除标准矩阵-向量乘法之外的任何矩阵乘积，并且对方法矩阵和长方矩阵都适用。\n\n需要完成的任务：\n1. 从诱导矩阵 $2$-范数的核心定义 $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$ 以及对于任意实矩阵 $A$，矩阵 $A^\\top A$ 是对称半正定的这一事实出发，推导出一个迭代方案，通过重复应用形如 $A \\mathbf{x}$ 和 $A^\\top \\mathbf{y}$ 的矩阵-向量乘法来估计 $\\lVert A \\rVert_2$，而无需显式构造 $A^\\top A$。您的推导必须基于这些定义和性质，并应包含一个基于估计值变化的明确停止准则。\n2. 将推导出的算法实现为一个完整的、可运行的程序。该算法必须：\n   - 用 $\\mathbb{R}^n$ 中的一个确定性非零向量进行初始化，在 $\\ell_2$ 意义下对其进行归一化，并在每次迭代中仅使用 $A \\mathbf{x}$ 和 $A^\\top \\mathbf{y}$ 运算。\n   - 当连续迭代中估计范数的相对变化小于容差 $\\varepsilon = 10^{-10}$ 时，或当达到最大迭代次数 $10^4$ 次时终止，以先到者为准。\n   - 稳健地处理 $A = 0$ 的边界情况，得出估计值 $\\lVert A \\rVert_2 = 0$。\n   - 返回 $\\lVert A \\rVert_2$ 的一个非负估计值。\n3. 您的程序必须评估以下矩阵测试套件，并按指定格式报告估计的范数：\n   - 情况 1 (方阵，对称正定):\n     $$A_1 = \\begin{bmatrix} 3  1 \\\\ 1  3 \\end{bmatrix}.$$\n   - 情况 2 (方阵，高度非正规):\n     $$A_2 = \\begin{bmatrix} 1  10  0 \\\\ 0  1  0 \\\\ 0  0  0.1 \\end{bmatrix}.$$\n   - 情况 3 (高矩形矩阵):\n     $$A_3 = \\begin{bmatrix} 1  2 \\\\ 0  1 \\\\ 2  0 \\\\ 0  0 \\end{bmatrix}.$$\n   - 情况 4 (宽矩形矩阵):\n     $$A_4 = \\begin{bmatrix} 1  0  2  0 \\\\ 0  1  0  1 \\end{bmatrix}.$$\n   - 情况 5 (零矩阵):\n     $$A_5 = \\begin{bmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{bmatrix}.$$\n   - 情况 6 (方阵，具有近似相等的多个主奇异值):\n     $$A_6 = \\begin{bmatrix} 1  0 \\\\ 0  0.999 \\end{bmatrix}.$$\n4. 最终输出格式：您的程序应生成单行输出，其中包含六个估计范数的结果，按 $A_1$ 到 $A_6$ 的顺序排列，以逗号分隔，并用方括号括起来。每个值必须四舍五入到 $8$ 位小数。例如，有效的输出格式为\n   $$[\\text{v}_1,\\text{v}_2,\\text{v}_3,\\text{v}_4,\\text{v}_5,\\text{v}_6],$$\n   其中每个 $\\text{v}_i$ 是一个四舍五入到 $8$ 位的小数。不应打印任何额外的文本或行。\n\n实现约束：\n- 程序必须是完全自包含的，不需要用户输入，并且只使用 Python 标准库和允许的库。\n- 本问题不涉及角度。\n- 不涉及物理单位。",
            "solution": "该问题要求推导并实现一个迭代算法，以估计实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 $2$-范数 $\\lVert A \\rVert_2$。推导过程必须基于第一性原理，并避免显式构造如 $A^\\top A$ 这样的矩阵乘积。\n\n首先对问题陈述进行验证。\n\n**步骤 1：提取已知条件**\n- **定义**：诱导矩阵 $2$-范数定义为 $\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}$。\n- **性质**：对于任意实矩阵 $A$，矩阵 $A^\\top A$ 是对称半正定的。\n- **算法目标**：推导一个仅使用形如 $A \\mathbf{x}$ 和 $A^\\top \\mathbf{y}$ 的矩阵-向量乘法来估计 $\\lVert A \\rVert_2$ 的迭代方案。\n- **实现约束**：\n  - **初始化**：使用 $\\mathbb{R}^n$ 中的一个确定性、归一化、非零的向量。\n  - **停止准则**：当范数估计值的相对变化小于容差 $\\varepsilon = 10^{-10}$ 时，或在达到最大迭代次数 $10^4$ 次后终止。\n  - **边界情况**：正确处理零矩阵 $A = 0$，得出估计值 $0$。\n  - **返回值**：函数必须返回 $\\lVert A \\rVert_2$ 的一个非负估计值。\n- **测试套件**：提供六个矩阵（$A_1$ 到 $A_6$）用于评估。\n- **输出格式**：单行输出，包含一个由六个估计范数组成的逗号分隔列表，四舍五入到 $8$ 位小数，并用方括号括起来。\n\n**步骤 2：使用提取的已知条件进行验证**\n1.  **科学依据**：该问题基于诱导 $2$-范数的标准定义及其与 $A^\\top A$ 最大特征值的基本关系。所提出的方法，即幂迭代法，是数值线性代数中用于寻找主特征值的经典且科学上合理的算法。该问题牢固地建立在已有的数学原理之上。\n2.  **适定性**：该问题是适定的。它要求估计一个唯一定义的数学量（$\\lVert A \\rVert_2$）。算法的终止由最大迭代次数限制和收敛准则保证。\n3.  **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n4.  **缺陷分析**：\n    - **科学或事实不健全**：无。前提是正确的。\n    - **无法形式化或不相关**：无。该问题是计算工程和数值分析中的一个标准任务，直接涉及向量和矩阵范数。\n    - **不完整或矛盾的设置**：无。所有必要的组成部分都已指定：目标、方法约束、终止准则和测试用例。\n    - **不切实际或不可行**：无。该算法是实用的，测试矩阵是标准示例。\n    - **不适定或结构不良**：无。结构清晰，从理论推导到实现都有指导。\n    - **无法进行科学验证**：无。算法的正确性及其结果的准确性可以通过与已知的解析解或标准库函数（例如，奇异值分解）进行比较来验证。\n\n**步骤 3：结论与行动**\n该问题被认为是**有效的**。将提供一个完整的、有理有据的解决方案。\n\n**推导与算法设计**\n\n起点是矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 $2$-范数的定义：\n$$\n\\lVert A \\rVert_2 = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n, \\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2}\n$$\n由于范数总是非负的，我们可以考虑其平方：\n$$\n\\lVert A \\rVert_2^2 = \\left( \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2}{\\lVert \\mathbf{x} \\rVert_2} \\right)^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\lVert A \\mathbf{x} \\rVert_2^2}{\\lVert \\mathbf{x} \\rVert_2^2}\n$$\n使用欧几里得范数的定义 $\\lVert \\mathbf{v} \\rVert_2^2 = \\mathbf{v}^\\top \\mathbf{v}$，我们可以将表达式重写为：\n$$\n\\lVert A \\rVert_2^2 = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{(A \\mathbf{x})^\\top (A \\mathbf{x})}{\\mathbf{x}^\\top \\mathbf{x}} = \\sup_{\\mathbf{x} \\neq \\mathbf{0}} \\frac{\\mathbf{x}^\\top A^\\top A \\mathbf{x}}{\\mathbf{x}^\\top \\mathbf{x}}\n$$\n这个表达式是矩阵 $B = A^\\top A$ 的瑞利商。线性代数中的一个基本定理指出，对称矩阵的瑞利商的上确界是其最大特征值 $\\lambda_{\\text{max}}$。矩阵 $B = A^\\top A$ 确实是对称的（因为 $(A^\\top A)^\\top = A^\\top (A^\\top)^\\top = A^\\top A$）并且是半正定的。因此，我们得到关键关系：\n$$\n\\lVert A \\rVert_2^2 = \\lambda_{\\text{max}}(A^\\top A)\n$$\n这意味着诱导矩阵 $2$-范数是 $A^\\top A$ 最大特征值的平方根：\n$$\n\\lVert A \\rVert_2 = \\sqrt{\\lambda_{\\text{max}}(A^\\top A)}\n$$\n根据定义，值 $\\sqrt{\\lambda_{\\text{max}}(A^\\top A)}$ 也是 $A$ 的最大奇异值，记作 $\\sigma_1(A)$。\n\n问题现在简化为在不显式计算矩阵 $A^\\top A$ 的情况下找到 $\\lambda_{\\text{max}}(A^\\top A)$。这可以使用**幂迭代**法来实现。幂迭代法是一种迭代算法，用于找到绝对值最大的特征值（主特征值）及其对应的特征向量。对于像 $A^\\top A$ 这样的对称半正定矩阵，所有特征值都是实数且非负，因此绝对值最大的特征值就是 $\\lambda_{\\text{max}}$。\n\n矩阵 $B$ 的标准幂迭代法如下：\n1.  从一个非零向量 $\\mathbf{v}_0$ 开始。\n2.  对 $k = 1, 2, \\dots$ 进行迭代：$\\mathbf{v}_k = \\frac{B \\mathbf{v}_{k-1}}{\\lVert B \\mathbf{v}_{k-1} \\rVert_2}$。\n只要初始向量 $\\mathbf{v}_0$ 在对应 $\\lambda_{\\text{max}}(B)$ 的特征向量方向上有非零分量，向量序列 $\\{\\mathbf{v}_k\\}$ 就会收敛到该特征向量。\n\n在我们的情况下，$B = A^\\top A$。迭代步骤为 $\\mathbf{v}_k \\propto (A^\\top A) \\mathbf{v}_{k-1}$。按照要求，我们通过执行两次连续的矩阵-向量乘积来避免构造 $A^\\top A$：\n1.  首先，计算 $\\mathbf{y}_{k-1} = A \\mathbf{v}_{k-1}$。\n2.  然后，计算 $\\mathbf{x}_k = A^\\top \\mathbf{y}_{k-1}$。\n因此，核心更新是 $\\mathbf{x}_k = A^\\top (A \\mathbf{v}_{k-1})$。下一个归一化向量是 $\\mathbf{v}_k = \\mathbf{x}_k / \\lVert \\mathbf{x}_k \\rVert_2$。\n\n我们还需要在每次迭代中估计 $\\lambda_{\\text{max}}(A^\\top A)$。这可以通过使用当前的特征向量估计值 $\\mathbf{v}_{k-1}$ 从瑞利商获得：\n$$\n\\lambda_k \\approx \\frac{\\mathbf{v}_{k-1}^\\top (A^\\top A) \\mathbf{v}_{k-1}}{\\mathbf{v}_{k-1}^\\top \\mathbf{v}_{k-1}}\n$$\n由于 $\\mathbf{v}_{k-1}$ 是一个单位向量（$\\lVert \\mathbf{v}_{k-1} \\rVert_2 = 1$），其分母为 $1$。分子变为：\n$$\n\\mathbf{v}_{k-1}^\\top A^\\top A \\mathbf{v}_{k-1} = (A \\mathbf{v}_{k-1})^\\top (A \\mathbf{v}_{k-1}) = \\mathbf{y}_{k-1}^\\top \\mathbf{y}_{k-1} = \\lVert \\mathbf{y}_{k-1} \\rVert_2^2\n$$\n因此，在第 $k$ 次迭代时，最大特征值的估计值为 $\\lambda_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2^2$。\n因此，矩阵 $2$-范数的估计值 $\\sigma_k = \\sqrt{\\lambda_k}$ 就简化为：\n$$\n\\sigma_k = \\lVert A \\mathbf{v}_{k-1} \\rVert_2 = \\lVert \\mathbf{y}_{k-1} \\rVert_2\n$$\n这提供了一种在每次迭代中简单高效地更新范数估计值的方法。\n\n**最终算法：**\n设 $A$ 是一个 $m \\times n$ 矩阵。设 $\\varepsilon = 10^{-10}$ 为容差，$K_{\\text{max}} = 10^4$ 为最大迭代次数。\n\n1.  **处理平凡情况**：如果 $n=0$，则定义域为空，所以 $\\lVert A \\rVert_2 = 0$。\n2.  **初始化**：\n    - 选择一个确定性的非零起始向量 $\\mathbf{v}_0 \\in \\mathbb{R}^n$。一个标准的选择是全为 1 的向量。\n    - 将其归一化：$\\mathbf{v} \\leftarrow \\frac{\\mathbf{v}_0}{\\lVert \\mathbf{v}_0 \\rVert_2}$。\n    - 初始化范数估计值，例如，$\\sigma_{\\text{new}} \\leftarrow 0$。\n3.  **迭代**：对于 $k=1, \\dots, K_{\\text{max}}$：\n    a.  存储上一次的估计值：$\\sigma_{\\text{old}} \\leftarrow \\sigma_{\\text{new}}$。\n    b.  应用第一个矩阵-向量乘积：$\\mathbf{y} \\leftarrow A \\mathbf{v}$。\n    c.  更新范数估计值：$\\sigma_{\\text{new}} \\leftarrow \\lVert \\mathbf{y} \\rVert_2$。\n    d.  **检查收敛**：如果 $k>1$ 且 $|\\sigma_{\\text{new}} - \\sigma_{\\text{old}}|  \\varepsilon \\cdot \\sigma_{\\text{new}}$，则跳出循环。\n    e.  **处理零矩阵情况**：如果 $\\sigma_{\\text{new}} = 0$，则意味着 $\\mathbf{y}=\\mathbf{0}$。这表示 $A\\mathbf{v}=\\mathbf{0}$。矩阵 $A$ 是奇异的，或者可能是零矩阵。算法正确地得出 $\\sigma_{\\text{new}} = 0$ 并应终止。我们可以跳出循环。\n    f.  应用第二个矩阵-向量乘积：$\\mathbf{x} \\leftarrow A^\\top \\mathbf{y}$。\n    g.  将结果向量归一化用于下一次迭代：$\\mathbf{v} \\leftarrow \\frac{\\mathbf{x}}{\\lVert \\mathbf{x} \\rVert_2}$。如果 $\\lVert \\mathbf{x} \\rVert_2 = 0$，则跳出循环。这种情况发生在 $\\mathbf{y}$ 位于 $A^\\top$ 的零空间中，对于 $\\mathbf{y}=A\\mathbf{v}$ 而言，这意味着 $A\\mathbf{v}=\\mathbf{0}$，从而正确地得到范数为 $0$。\n4.  **返回**：最终估计值 $\\sigma_{\\text{new}}$。\n\n该算法仅使用矩阵-向量乘积，遵守所有约束，并能正确估计 $\\lVert A \\rVert_2$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef estimate_induced_2_norm(A, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Estimates the induced matrix 2-norm (largest singular value) of a real matrix A\n    using the power iteration method.\n\n    The algorithm iteratively computes v_k = A^T * A * v_{k-1} without explicitly\n    forming the matrix A^T*A. The norm is estimated as ||A*v_k||_2.\n\n    Args:\n        A (np.ndarray): The input matrix, m x n.\n        tol (float): The relative tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        float: The estimated induced 2-norm of A.\n    \"\"\"\n    # Get matrix dimensions\n    m, n = A.shape\n\n    # Handle the edge case of a matrix with zero columns.\n    if n == 0:\n        return 0.0\n\n    # Initialize with a deterministic non-zero vector in R^n.\n    # A vector of ones is a standard deterministic choice.\n    # The power method might fail if this initial vector is orthogonal\n    # to the dominant eigenvector of A^T*A. In practice, for general\n    # matrices and with finite-precision arithmetic, this is rare.\n    v = np.ones(n)\n    v /= np.linalg.norm(v)\n\n    norm_est = 0.0\n\n    for _ in range(max_iter):\n        norm_est_prev = norm_est\n\n        # First matrix-vector product: y = A*v\n        y = A @ v\n\n        # Update the norm estimate: ||A||_2 approx ||y||_2\n        norm_est = np.linalg.norm(y)\n\n        # Check for convergence using relative change.\n        # This check is safe because for a non-zero matrix, norm_est converges\n        # to a positive value.\n        if norm_est > 0 and abs(norm_est - norm_est_prev)  tol * norm_est:\n            break\n        \n        # Handle the case where A is the zero matrix or v is in the null space of A.\n        if norm_est == 0:\n            return 0.0\n\n        # Second matrix-vector product: x = A^T*y\n        x = A.T @ y\n        \n        # Normalize the vector for the next iteration.\n        norm_x = np.linalg.norm(x)\n\n        # If norm_x is zero, the iteration has converged to the null space.\n        if norm_x == 0:\n            break\n            \n        v = x / norm_x\n\n    return norm_est\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the norm estimation, and prints the results.\n    \"\"\"\n    \n    A1 = np.array([[3.0, 1.0], \n                   [1.0, 3.0]])\n\n    A2 = np.array([[1.0, 10.0, 0.0],\n                   [0.0, 1.0, 0.0],\n                   [0.0, 0.0, 0.1]])\n\n    A3 = np.array([[1.0, 2.0],\n                   [0.0, 1.0],\n                   [2.0, 0.0],\n                   [0.0, 0.0]])\n\n    A4 = np.array([[1.0, 0.0, 2.0, 0.0],\n                   [0.0, 1.0, 0.0, 1.0]])\n\n    A5 = np.array([[0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0],\n                   [0.0, 0.0, 0.0]])\n\n    A6 = np.array([[1.0, 0.0],\n                   [0.0, 0.999]])\n                   \n    test_cases = [A1, A2, A3, A4, A5, A6]\n\n    results = []\n    for A in test_cases:\n        norm_estimate = estimate_induced_2_norm(A, tol=1e-10, max_iter=10000)\n        results.append(f\"{norm_estimate:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在经济学和数据科学中，我们经常需要根据一系列特征对公司或数据点进行分组，而“距离”是衡量相似性的关键。不同的向量范数（如 $L_1$、$L_2$ 和 $L_\\infty$ 范数）定义了不同的距离度量方式，从而影响聚类结果。本练习将让你亲手实现一个 K-均值聚类算法，并观察在不同范数下，对同一组代表公司财务比率的向量进行聚类会产生怎样的差异。这有助于你直观地理解不同范数所对应的几何直觉，以及它们在解决实际分类问题中的重要性 。",
            "id": "2447279",
            "problem": "一组公司由向量表示，每个向量代表一家公司的三个财务比率，且位于 $\\mathbb{R}^3$ 空间中。考虑 $\\mathbb{R}^3$ 上的三种向量范数：$L_1$ 范数定义为 $\\lVert x \\rVert_1 = \\sum_{j=1}^3 \\lvert x_j \\rvert$，$L_2$ 范数定义为 $\\lVert x \\rVert_2 = \\left(\\sum_{j=1}^3 x_j^2\\right)^{1/2}$，以及 $L_\\infty$ 范数定义为 $\\lVert x \\rVert_\\infty = \\max\\{\\lvert x_1 \\rvert, \\lvert x_2 \\rvert, \\lvert x_3 \\rvert\\}$。对于任意两个向量 $x,y \\in \\mathbb{R}^3$，在范数 $p \\in \\{1,2,\\infty\\}$ 下的距离定义为 $d_p(x,y) = \\lVert x-y \\rVert_p$。\n\n您必须实现以下聚类过程，该过程由范数 $p \\in \\{1,2,\\infty\\}$ 的选择进行参数化：\n- 聚类数量为 $K=2$。\n- 按照下面每个数据集的指定，初始化聚类质心 $\\mu_1^{(0)}, \\mu_2^{(0)} \\in \\mathbb{R}^3$。\n- 在迭代 $t \\in \\{0,1,2,\\dots\\}$ 时，执行分配步骤：对于每个数据向量 $x_i$，将其分配给使 $d_p(x_i,\\mu_{k}^{(t)})$ 最小化的聚类索引 $k^\\star \\in \\{1,2\\}$。如果距离相等，则选择达到最小值的最小聚类索引 $k^\\star$。\n- 执行更新步骤：对于每个聚类 $k \\in \\{1,2\\}$，如果在迭代 $t$ 中分配给聚类 $k$ 的向量集合非空，则将 $\\mu_k^{(t+1)}$ 设置为这些分配向量的算术平均值；如果为空，则设置 $\\mu_k^{(t+1)} = \\mu_k^{(t)}$。\n- 当一次迭代与前一次迭代相比在任何分配上都没有产生变化时，或在 $T=10$ 次迭代之后停止，以先发生者为准。\n\n所有量均为无单位的比率；不适用任何物理单位。不涉及角度。不出现百分比。\n\n您的任务是针对下面测试套件中列出的数据集和范数运行上述过程，并为每个测试用例报告最终的聚类分配向量。该向量应为一个由 $\\{1,2\\}$ 中的整数组成的列表，其顺序与公司列出的顺序相同。聚类标签必须按规定从 1 开始索引。算术平均值是 $\\mathbb{R}^3$ 中的分量均值。\n\n测试套件：\n- 数据集 A（理想路径；分离良好的聚类）：\n  - 公司（按顺序）：$x_1=(0.9,1.1,1.0)$, $x_2=(1.2,0.9,1.1)$, $x_3=(1.0,1.2,0.8)$, $x_4=(8.1,7.9,8.0)$, $x_5=(7.8,8.2,8.1)$, $x_6=(8.0,8.1,7.9)$。\n  - 初始质心：$\\mu_1^{(0)}=(1.0,1.0,1.0)$, $\\mu_2^{(0)}=(8.0,8.0,8.0)$。\n  - 待测试的范数：$p \\in \\{2,1,\\infty\\}$，产生三个不同的测试用例。\n- 数据集 B（边界条件；需要打破平局的等距点）：\n  - 公司（按顺序）：$x_1=(0.0,0.0,0.0)$, $x_2=(2.0,0.0,0.0)$, $x_3=(1.0,0.0,0.0)$。\n  - 初始质心：$\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(2.0,0.0,0.0)$。\n  - 待测试的范数：$p \\in \\{2\\}$，产生一个测试用例。\n- 数据集 C（边缘案例；各向异性和一个对不同范数产生不同影响的离群值）：\n  - 公司（按顺序）：$x_1=(0.1,0.1,0.1)$, $x_2=(0.0,0.0,0.0)$, $x_3=(0.2,0.1,0.0)$, $x_4=(4.2,0.1,0.0)$, $x_5=(3.8,-0.1,0.0)$, $x_6=(3.0,0.0,5.0)$。\n  - 初始质心：$\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(4.0,0.0,0.0)$。\n  - 待测试的范数：$p \\in \\{2,\\infty,1\\}$，产生三个不同的测试用例。\n\n因此，总共有 $7$ 个测试用例，顺序如下：\n$($A, $p=2)$, $($A, $p=1)$, $($A, $p=\\infty)$, $($B, $p=2)$, $($C, $p=2)$, $($C, $p=\\infty)$, $($C, $p=1)$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个元素本身也是一个整数列表，按上述顺序指示相应测试用例的最终聚类标签。例如，包含两个测试用例的输出可能看起来像 $[[1,1,2],[2,1]]$。",
            "solution": "所提出的问题是一个良构且计算上可行的任务。它要求实现 K-means 聚类算法的一个变体，这是无监督机器学习和数据分析中的一个基本过程。该问题具有科学依据，依赖于向量范数（$L_1$、$L_2$ 和 $L_\\infty$）和算术平均值的标准定义。所有参数、初始条件、数据和过程规则——包括终止条件和打破平局规则——都以足够的精度被指定，以保证一个唯一的、确定性的结果。将这些方法应用于财务比率向量的背景是计算经济学和金融学中的一个标准应用，其中不同的范数可以代表经济实体之间“距离”或“相异性”的不同概念。例如，$L_2$ 范数对应于我们熟悉的欧几里得距离，$L_1$ 范数（或“曼哈顿距离”）计算各个比率绝对差的总和，而 $L_\\infty$ 范数（或“切比雪夫距离”）则由任何比率中的单一最大差异决定，这可能与最坏情况分析相关。范数的选择从根本上改变了空间的几何形状，从而影响了聚类边界的形状，并因此影响了最终的分配结果。因此，该问题是在理解理论范数的实际应用方面一个有效且有启发性的练习。\n\n要实现的算法是一个迭代过程，定义如下。给定 $\\mathbb{R}^3$ 中的一组数据向量 $\\{x_i\\}_{i=1}^N$、一组 $K=2$ 的初始质心 $\\{\\mu_1^{(0)}, \\mu_2^{(0)}\\}$ 和一个选定的范数 $p \\in \\{1, 2, \\infty\\}$，我们按 $t=0, 1, 2, \\dots$ 索引的迭代进行。每次迭代包括两个步骤。\n\n首先是**分配步骤**：每个数据向量 $x_i$ 被分配到一个聚类 $k^\\star \\in \\{1, 2\\}$。分配是通过找到与 $x_i$ 最接近的聚类质心 $\\mu_k^{(t)}$ 来完成的，距离由 $d_p(x_i, \\mu_k^{(t)}) = \\lVert x_i - \\mu_k^{(t)} \\rVert_p$ 衡量。因此，聚类索引 $k^\\star$ 由下式给出：\n$$\nk^\\star = \\underset{k \\in \\{1,2\\}}{\\arg\\min} \\lVert x_i - \\mu_k^{(t)} \\rVert_p\n$$\n问题指定了一个打破平局的规则：如果到两个质心的距离相等，则该点被分配给聚类 1。设 $C_k^{(t)}$ 为在迭代 $t$ 中分配给聚类 $k$ 的数据向量的索引集合。\n\n其次是**更新步骤**：下一次迭代的质心 $\\mu_k^{(t+1)}$ 将被重新计算。对于每个聚类 $k$，如果分配的向量集合非空（即 $C_k^{(t)} \\neq \\emptyset$），则新的质心是分配给该聚类的所有向量的分量算术平均值：\n$$\n\\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{i \\in C_k^{(t)}} x_i\n$$\n如果一个聚类变为空（$C_k^{(t)} = \\emptyset$），其质心将不被更新：$\\mu_k^{(t+1)} = \\mu_k^{(t)}$。\n\n该过程在以下两个条件之一满足时**终止**：要么在迭代 $t$ 时的聚类分配与迭代 $t-1$ 时的分配完全相同，表明已收敛；要么已完成最多 $T=10$ 次迭代。\n\n解决方案将由一个程序产生，该程序会为七个指定的测试用例中的每一个系统地执行此算法，这些测试用例结合了三个数据集（A、B、C）和不同的范数（$p=2, 1, \\infty$）。该程序将为每个案例计算最终的聚类分配，并根据指定的要求格式化输出。使用 NumPy 进行高效和准确的向量和矩阵运算是合适的，包括计算范数和均值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the clustering problem for all test cases as specified.\n    \"\"\"\n\n    def get_norm_func(p):\n        \"\"\"Returns a function to compute the distance based on the specified norm.\"\"\"\n        if p == 1:\n            return lambda a, b: np.linalg.norm(a - b, ord=1)\n        elif p == 2:\n            return lambda a, b: np.linalg.norm(a - b, ord=2)\n        elif p == 'inf':\n            return lambda a, b: np.linalg.norm(a - b, ord=np.inf)\n        else:\n            raise ValueError(\"Unsupported norm type.\")\n\n    def run_clustering(data, initial_centroids, p, max_iter=10):\n        \"\"\"\n        Runs the specified k-means variant clustering algorithm.\n\n        Args:\n            data (np.ndarray): The dataset of points (N, D).\n            initial_centroids (np.ndarray): The initial cluster centroids (K, D).\n            p (int or 'inf'): The norm to use for distance calculation (1, 2, or 'inf').\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            list: A list of final cluster assignments (1-indexed).\n        \"\"\"\n        num_points = data.shape[0]\n        num_clusters = initial_centroids.shape[0]\n        \n        centroids = np.copy(initial_centroids)\n        assignments = np.zeros(num_points, dtype=int)\n        distance_func = get_norm_func(p)\n\n        for _ in range(max_iter):\n            old_assignments = np.copy(assignments)\n\n            # Assignment step\n            for i in range(num_points):\n                point = data[i]\n                \n                dist_to_c1 = distance_func(point, centroids[0])\n                dist_to_c2 = distance_func(point, centroids[1])\n                \n                if dist_to_c1 = dist_to_c2:\n                    assignments[i] = 1\n                else:\n                    assignments[i] = 2\n\n            # Termination check\n            if np.array_equal(assignments, old_assignments):\n                break\n\n            # Update step\n            new_centroids = np.copy(centroids)\n            for k in range(1, num_clusters + 1):\n                # Get all points assigned to cluster k\n                cluster_points = data[assignments == k]\n                \n                if cluster_points.shape[0] > 0:\n                    # Update centroid to the mean of the assigned points\n                    new_centroids[k-1] = np.mean(cluster_points, axis=0)\n                # Else: centroid remains unchanged, as handled by copying `centroids` beforehand.\n            \n            centroids = new_centroids\n\n        return assignments.tolist()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 1,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [1.0, 0.0, 0.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 1,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_clustering(case[\"data\"], case[\"centroids\"], case[\"norm\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    results_str_list = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_str = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "处理高维数据时，一个核心挑战是如何在保留关键信息的同时简化数据，这正是矩阵近似的用武之地。本练习将使用弗罗贝尼乌斯范数 (Frobenius norm) 作为度量标准，来寻找一个资产收益矩阵的最佳秩-1 近似。这个过程不仅让你动手实践奇异值分解 (SVD) 这一强大的工具，还将其与金融单因子模型的构建直接联系起来，揭示了矩阵范数在模型简化和数据降维中的巨大价值 。",
            "id": "2447261",
            "problem": "考虑一个资产收益矩阵 $R \\in \\mathbb{R}^{n \\times T}$，其中行对应 $n$ 个资产，列对应 $T$ 个时间周期。所有收益都以小数形式表示（例如，条目 $0.01$ 代表小数形式的收益率 $0.01$）。矩阵 $A$ 的 Frobenius 范数定义为 $\\lVert A \\rVert_{F} = \\sqrt{\\sum_{i,j} A_{ij}^{2}}$。在 Frobenius 范数下，与 $R$ 最接近的秩为 $1$ 的矩阵是求解以下问题的任意矩阵 $X$\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times T}} \\ \\lVert R - X \\rVert_{F} \\quad \\text{subject to} \\quad \\operatorname{rank}(X) \\le 1.\n$$\n任何秩为 $1$ 的矩阵都可以写成 $X = b f^{\\top}$ 的形式，其中 $b \\in \\mathbb{R}^{n}$ 且 $f \\in \\mathbb{R}^{T}$。在单因子金融模型的背景下，$b$ 可以解释为资产载荷向量，$f$ 可以解释为因子收益时间序列。为使 $(b,f)$ 唯一，我们施加归一化条件 $\\lVert f \\rVert_{2} = 1$ 以及符号约定，即 $f$ 的第一个非零元素为非负数。如果 $R$ 是零矩阵（所有元素均为 $0$），则定义 $b$ 为 $\\mathbb{R}^{n}$ 中的零向量，$f$ 为 $\\mathbb{R}^{T}$ 中的第一个标准基向量。\n\n对于下方的每个测试用例，执行以下操作：\n- 在上述归一化和符号约定下，计算 Frobenius 范数下与 $R$ 最接近的秩为 $1$ 的矩阵 $R_{1}$，表示为 $R_{1} = b f^{\\top}$。\n- 计算残差 Frobenius 范数 $\\lVert R - R_{1} \\rVert_{F}$。\n- 计算由 $R_{1}$ 解释的变异份额 $s$\n$$\ns = \\begin{cases}\n\\frac{\\lVert R_{1} \\rVert_{F}^{2}}{\\lVert R \\rVert_{F}^{2}},  \\text{if } \\lVert R \\rVert_{F} \\ne 0, \\\\\n0,  \\text{if } \\lVert R \\rVert_{F} = 0.\n\\end{cases}\n$$\n\n将每个数值输出四舍五入到六位小数。对于每个测试用例，输出一个单一列表\n$$\n[\\ \\lVert R - R_{1} \\rVert_{F},\\ s,\\ b_{1},\\ldots,b_{n},\\ f_{1},\\ldots,f_{T}\\ ],\n$$\n其中 $b_{i}$ 是 $b$ 的元素，$f_{j}$ 是 $f$ 的元素。您的程序应生成单行输出，其中包含所有测试用例的结果，形式为由逗号分隔的各测试用例列表，并包含在一对总的方括号内。\n\n测试套件：\n- 测试用例 1（一般情况，带有小噪声的秩为 1 结构）。设 $R \\in \\mathbb{R}^{3 \\times 4}$ 为\n$$\nR = \\begin{bmatrix}\n0.006  -0.0105  0.0078  -0.0002 \\\\\n-0.0023  0.0044  -0.0031  0.0002 \\\\\n0.0012  -0.0021  0.002  -0.0003\n\\end{bmatrix}.\n$$\n- 测试用例 2（边界情况，零矩阵）。设 $R \\in \\mathbb{R}^{2 \\times 3}$ 为\n$$\nR = \\begin{bmatrix}\n0  0  0 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- 测试用例 3（精确秩为 1 的情况）。设 $R \\in \\mathbb{R}^{4 \\times 3}$ 为\n$$\nR = \\begin{bmatrix}\n0.0006  -0.0003  0.00015 \\\\\n0.0002  -0.0001  0.00005 \\\\\n-0.0004  0.0002  -0.0001 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n\n最终输出格式：\n- 单行输出，包含一个含三个项目的列表（每个测试用例一个项目），其中每个项目是上述描述的列表。例如，该行应如下所示\n$$\n[ [\\ \\cdots\\ ],[\\ \\cdots\\ ],[\\ \\cdots\\ ] ].\n$$\n所有数字必须四舍五入到六位小数，并且收益率必须作为小数（而不是百分比）处理。",
            "solution": "该问题要求找到给定资产收益矩阵 $R \\in \\mathbb{R}^{n \\times T}$ 的最接近的秩为 $1$ 的近似，并计算相关的金融指标。这是一个线性代数中的经典问题，其解由 Eckart-Young-Mirsky 定理给出。该定理指出，矩阵 $R$ 在 Frobenius 范数下的最佳秩 $k$ 近似由截断的奇异值分解（SVD）给出。\n\n首先，我们将矩阵 $R$ 的 SVD 定义为：\n$$\nR = U \\Sigma V^{\\top}\n$$\n其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{T \\times T}$ 是正交矩阵（$U^{\\top}U = I_n$，$V^{\\top}V = I_T$），而 $\\Sigma \\in \\mathbb{R}^{n \\times T}$ 是一个包含按非递增顺序排列的奇异值 $\\sigma_i$ 的矩形对角矩阵：$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(n,T)} \\ge 0$。$U$ 的列（表示为 $u_i$）是左奇异向量，$V$ 的列（表示为 $v_i$）是右奇异向量。\n\nEckart-Young-Mirsky 定理指明，最小化问题\n$$\n\\min_{X} \\lVert R - X \\rVert_{F} \\quad \\text{subject to} \\quad \\operatorname{rank}(X) \\le 1\n$$\n的解是由最大奇异值 $\\sigma_1$ 及其对应的奇异向量 $u_1$ 和 $v_1$ 构建的矩阵 $R_1$：\n$$\nR_1 = \\sigma_1 u_1 v_1^{\\top}\n$$\n这个矩阵 $R_1$ 是 $R$ 的最接近的秩为 $1$ 的近似。\n\n问题要求我们将 $R_1$ 表示为单因子模型的形式，$R_1 = b f^{\\top}$，其中 $b \\in \\mathbb{R}^{n}$ 是资产载荷向量，$f \\in \\mathbb{R}^{T}$ 是因子收益时间序列。为确保表示的唯一性，施加了两个条件：\n1. 归一化：因子时间序列的欧几里得范数必须为单位范数，即 $\\lVert f \\rVert_2 = 1$。\n2. 符号约定：$f$ 的第一个非零元素必须为非负数。\n\n我们可以将 SVD 的分量与因子 $b$ 和 $f$ 联系起来。根据 $R_1 = \\sigma_1 u_1 v_1^{\\top}$ 和 $R_1 = b f^{\\top}$，并且知道来自 SVD 的奇异向量已经是归一化的（$\\lVert u_1 \\rVert_2 = 1$，$\\lVert v_1 \\rVert_2 = 1$），我们可以做出如下识别：\n$$\nf \\propto v_1 \\quad \\text{and} \\quad b \\propto u_1\n$$\n为了满足归一化条件 $\\lVert f \\rVert_2 = 1$，我们可以设置 $f = v_1$ 和 $b = \\sigma_1 u_1$。然而，SVD 分量 $(u_1, v_1)$ 仅在同时改变符号的情况下是唯一的，即 $(u_1, v_1)$ 和 $(-u_1, -v_1)$ 都会产生相同的矩阵 $R_1$。我们必须使用符号约定来解决这种模糊性。\n\n让我们定义一个临时因子向量 $f_{\\text{temp}} = v_1$。我们找到 $f_{\\text{temp}}$ 的第一个非零元素。如果该元素为负，我们必须翻转两个向量的符号。我们定义一个符号乘数 $\\alpha \\in \\{-1, 1\\}$。\n$$\n\\alpha = \\begin{cases}\n-1  \\text{if the first non-zero element of } v_1 \\text{ is negative} \\\\\n+1  \\text{otherwise}\n\\end{cases}\n$$\n那么唯一的因子由以下公式给出：\n$$\nf = \\alpha v_1\n$$\n$$\nb = \\alpha \\sigma_1 u_1\n$$\n这种构造满足 $R_1 = b f^{\\top} = (\\alpha \\sigma_1 u_1)(\\alpha v_1)^{\\top} = \\alpha^2 \\sigma_1 u_1 v_1^{\\top} = \\sigma_1 u_1 v_1^{\\top}$，以及 $\\lVert f \\rVert_2 = \\lVert \\alpha v_1 \\rVert_2 = |\\alpha|\\lVert v_1 \\rVert_2=1$，并符合对 $f$ 的符号约定。\n\n问题为零矩阵 $R=0_{n \\times T}$ 指定了一个特殊情况。在这种情况下，所有奇异值都为零。问题明确定义 $b$ 为 $\\mathbb{R}^n$ 中的零向量，$f$ 为 $\\mathbb{R}^T$ 中的第一个标准基向量，即 $f = e_1 = [1, 0, \\dots, 0]^{\\top}$。这得出 $R_1=0 \\cdot e_1^\\top = 0$。\n\n接下来，我们计算所需的指标：\n1. 残差 Frobenius 范数，$\\lVert R - R_{1} \\rVert_{F}$。SVD 的性质表明，矩阵的 Frobenius 范数的平方是其奇异值平方的总和：$\\lVert R \\rVert_{F}^{2} = \\sum_i \\sigma_i^2$。秩为 1 的近似误差是 SVD 和的剩余部分的 Frobenius 范数。\n$$\n\\lVert R - R_{1} \\rVert_{F} = \\left\\lVert \\left(\\sum_{i=1}^{\\min(n,T)} \\sigma_i u_i v_i^{\\top}\\right) - \\sigma_1 u_1 v_1^{\\top} \\right\\rVert_{F} = \\left\\lVert \\sum_{i=2}^{\\min(n,T)} \\sigma_i u_i v_i^{\\top} \\right\\rVert_{F}\n$$\n由于奇异向量的正交性，这可以简化为：\n$$\n\\lVert R - R_{1} \\rVert_{F} = \\sqrt{\\sum_{i=2}^{\\min(n,T)} \\sigma_i^2} = \\sqrt{\\lVert R \\rVert_F^2 - \\sigma_1^2}\n$$\n对于一个精确秩为 $1$ 的矩阵 $R$，除 $\\sigma_1$ 外所有奇异值均为零，因此残差范数为 $0$。\n\n2. 解释的变异份额，$s$。这是近似矩阵的方差与原始数据方差的比率。\n$$\ns = \\frac{\\lVert R_{1} \\rVert_{F}^{2}}{\\lVert R \\rVert_{F}^{2}}\n$$\n我们知道 $\\lVert R \\rVert_F^2 = \\sum_i \\sigma_i^2$。近似矩阵的范数是 $\\lVert R_1 \\rVert_F = \\lVert \\sigma_1 u_1 v_1^{\\top} \\rVert_F = \\sigma_1$。因此，$\\lVert R_1 \\rVert_F^2 = \\sigma_1^2$。解释的份额是：\n$$\ns = \\frac{\\sigma_1^2}{\\sum_i \\sigma_i^2}\n$$\n如果 $R$ 是零矩阵，则 $\\lVert R \\rVert_F = 0$，问题定义 $s=0$。\n\n总体算法如下：\n1. 对于给定的矩阵 $R$，首先检查它是否为零矩阵。如果是，则使用为 $b$ 和 $f$ 提供的定义，并计算 $\\lVert R - R_1 \\rVert_F = 0$ 和 $s=0$。\n2. 如果 $R$ 不是零矩阵，计算其 SVD 以找到 $\\sigma_i$、$u_1$ 和 $v_1$。\n3. 对 $v_1$ 应用符号约定以确定符号乘数 $\\alpha$。\n4. 计算 $b = \\alpha \\sigma_1 u_1$ 和 $f = \\alpha v_1$。\n5. 计算残差范数 $\\lVert R - R_{1} \\rVert_{F} = \\sqrt{\\sum_{i \\ge 2} \\sigma_i^2}$。\n6. 计算解释的份额 $s = \\sigma_1^2 / (\\sum_i \\sigma_i^2)$。\n7. 将所有数值结果整理到一个列表中，每个数字四舍五入到六位小数。\n\n此过程将应用于每个测试用例。",
            "answer": "```python\nimport numpy as np\n\ndef solve_case(R):\n    \"\"\"\n    Computes the rank-1 approximation and related metrics for a given matrix R.\n\n    Args:\n        R (np.ndarray): The input matrix.\n\n    Returns:\n        list: A list containing the residual norm, explained share, \n              factor loadings (b), and factor returns (f).\n    \"\"\"\n    n, T = R.shape\n    \n    # Handle the boundary case of a zero matrix\n    if np.allclose(R, 0):\n        residual_norm = 0.0\n        explained_share = 0.0\n        b = np.zeros(n)\n        f = np.zeros(T)\n        f[0] = 1.0\n        \n        result_list = [residual_norm, explained_share] + b.tolist() + f.tolist()\n        return [np.round(x, 6) for x in result_list]\n\n    # Compute the full SVD\n    # Use full_matrices=False for efficiency\n    U, s_vals, Vh = np.linalg.svd(R, full_matrices=False)\n    \n    # Extract the components for the rank-1 approximation\n    sigma1 = s_vals[0]\n    u1 = U[:, 0]\n    v1 = Vh[0, :]\n    \n    # Apply sign convention: first non-zero entry of f must be non-negative\n    # Find the index of the first non-zero element with a small tolerance\n    try:\n        first_nonzero_idx = np.nonzero(np.abs(v1) > 1e-12)[0][0]\n        sign_multiplier = np.sign(v1[first_nonzero_idx])\n        # A sign of 0 can occur if the value is extremely small. Default to 1.\n        if sign_multiplier == 0:\n            sign_multiplier = 1.0\n    except IndexError:\n        # This case (v1 is all zeros) should not happen if R is not a zero matrix.\n        sign_multiplier = 1.0\n        \n    f = v1 * sign_multiplier\n    b = u1 * sigma1 * sign_multiplier\n    \n    # Compute the residual Frobenius norm\n    R_norm_fro_sq = np.sum(s_vals**2)\n    residual_norm_sq = R_norm_fro_sq - sigma1**2\n    # Clamp to zero to avoid negative due to floating point inaccuracies\n    residual_norm = np.sqrt(max(0, residual_norm_sq))\n    \n    # Compute the explained share of variation\n    if R_norm_fro_sq == 0:\n        explained_share = 0.0\n    else:\n        explained_share = sigma1**2 / R_norm_fro_sq\n    \n    # Assemble the final list of results\n    result_list = [residual_norm, explained_share] + b.tolist() + f.tolist()\n    \n    # Round all numerical outputs to six decimal places\n    rounded_results = [np.round(x, 6) for x in result_list]\n    \n    return rounded_results\n\ndef solve():\n    \"\"\"\n    Defines test cases and solves them, printing the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: general case\n        np.array([\n            [0.006, -0.0105, 0.0078, -0.0002],\n            [-0.0023, 0.0044, -0.0031, 0.0002],\n            [0.0012, -0.0021, 0.002, -0.0003]\n        ]),\n        # Test Case 2: boundary case (zero matrix)\n        np.array([\n            [0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0]\n        ]),\n        # Test Case 3: exact rank-1 case\n        np.array([\n            [0.0006, -0.0003, 0.00015],\n            [0.0002, -0.0001, 0.00005],\n            [-0.0004, 0.0002, -0.0001],\n            [0.0, 0.0, 0.0]\n        ])\n    ]\n\n    all_results = []\n    for R in test_cases:\n        all_results.append(solve_case(R))\n    \n    # Format the final output string as a list of lists.\n    # Convert rounded numbers to strings to ensure proper formatting e.g., '0.0'\n    outer_list = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places, handling -0.0 cases.\n        formatted_list = [f\"{x:.6f}\" if x == 0.0 else str(x) for x in res_list]\n        inner_list_str = '[' + ','.join(formatted_list) + ']'\n        outer_list.append(inner_list_str)\n    \n    final_output_str = '[' + ','.join(outer_list) + ']'\n    \n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}