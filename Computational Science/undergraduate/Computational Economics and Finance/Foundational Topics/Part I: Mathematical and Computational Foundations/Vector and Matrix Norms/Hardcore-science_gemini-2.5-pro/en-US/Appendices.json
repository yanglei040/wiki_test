{
    "hands_on_practices": [
        {
            "introduction": "Vector norms provide a fundamental way to measure the 'size' of a vector, but their true power in data analysis comes from using them to define the distance between two points. This exercise explores how the choice of distance metric—derived from the $L_1$, $L_2$, or $L_\\infty$ norms—can lead to different outcomes in a k-means clustering algorithm. By grouping companies based on their financial ratios, you will gain a hands-on understanding of how the underlying geometry of these norms influences data interpretation .",
            "id": "2447279",
            "problem": "A set of companies is represented by vectors of three financial ratios per company, with each vector in $\\mathbb{R}^3$. Consider three vector norms on $\\mathbb{R}^3$: the $L_1$ norm defined by $\\lVert x \\rVert_1 = \\sum_{j=1}^3 \\lvert x_j \\rvert$, the $L_2$ norm defined by $\\lVert x \\rVert_2 = \\left(\\sum_{j=1}^3 x_j^2\\right)^{1/2}$, and the $L_\\infty$ norm defined by $\\lVert x \\rVert_\\infty = \\max\\{\\lvert x_1 \\rvert, \\lvert x_2 \\rvert, \\lvert x_3 \\rvert\\}$. For any two vectors $x,y \\in \\mathbb{R}^3$, define the distance under norm $p \\in \\{1,2,\\infty\\}$ by $d_p(x,y) = \\lVert x-y \\rVert_p$.\n\nYou must implement the following clustering procedure, parameterized by the choice of norm $p \\in \\{1,2,\\infty\\}$:\n- The number of clusters is $K=2$.\n- Initialize cluster centroids $\\mu_1^{(0)}, \\mu_2^{(0)} \\in \\mathbb{R}^3$ as specified for each dataset below.\n- At iteration $t \\in \\{0,1,2,\\dots\\}$, perform the assignment step: for each data vector $x_i$, assign it to the cluster index $k^\\star \\in \\{1,2\\}$ minimizing $d_p(x_i,\\mu_{k}^{(t)})$. If there is a tie, select the smallest cluster index $k^\\star$ achieving the minimum.\n- Perform the update step: for each cluster $k \\in \\{1,2\\}$, if the set of assigned vectors to cluster $k$ at iteration $t$ is nonempty, set $\\mu_k^{(t+1)}$ to the arithmetic mean of those assigned vectors; if it is empty, set $\\mu_k^{(t+1)} = \\mu_k^{(t)}$.\n- Stop when an iteration produces no change in any assignment compared to the previous iteration, or after $T=10$ iterations, whichever occurs first.\n\nAll quantities are unitless ratios; no physical units apply. Angles are not involved. Percentages do not appear.\n\nYour task is to run the above procedure on the datasets and norms listed in the test suite below and report, for each test case, the final cluster assignment vector as a list of integers in $\\{1,2\\}$, in the same order as the companies are listed. Cluster labels must be $1$-indexed as specified. The arithmetic mean is the componentwise mean in $\\mathbb{R}^3$.\n\nTest suite:\n- Dataset A (happy path; well-separated clusters):\n  - Companies (in order): $x_1=(0.9,1.1,1.0)$, $x_2=(1.2,0.9,1.1)$, $x_3=(1.0,1.2,0.8)$, $x_4=(8.1,7.9,8.0)$, $x_5=(7.8,8.2,8.1)$, $x_6=(8.0,8.1,7.9)$.\n  - Initial centroids: $\\mu_1^{(0)}=(1.0,1.0,1.0)$, $\\mu_2^{(0)}=(8.0,8.0,8.0)$.\n  - Norms to test: $p \\in \\{2,1,\\infty\\}$, producing three distinct test cases.\n- Dataset B (boundary condition; equidistant point requiring tie-breaking):\n  - Companies (in order): $x_1=(0.0,0.0,0.0)$, $x_2=(2.0,0.0,0.0)$, $x_3=(1.0,0.0,0.0)$.\n  - Initial centroids: $\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(2.0,0.0,0.0)$.\n  - Norms to test: $p \\in \\{2\\}$, producing one test case.\n- Dataset C (edge case; anisotropy and an outlier affecting norms differently):\n  - Companies (in order): $x_1=(0.1,0.1,0.1)$, $x_2=(0.0,0.0,0.0)$, $x_3=(0.2,0.1,0.0)$, $x_4=(4.2,0.1,0.0)$, $x_5=(3.8,-0.1,0.0)$, $x_6=(3.0,0.0,5.0)$.\n  - Initial centroids: $\\mu_1^{(0)}=(0.0,0.0,0.0)$, $\\mu_2^{(0)}=(4.0,0.0,0.0)$.\n  - Norms to test: $p \\in \\{2,\\infty,1\\}$, producing three distinct test cases.\n\nTherefore, there are $7$ test cases total, in this order:\n$($A with $p=2)$, $($A with $p=1)$, $($A with $p=\\infty)$, $($B with $p=2)$, $($C with $p=2)$, $($C with $p=\\infty)$, $($C with $p=1)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of integers indicating the final cluster labels for the corresponding test case, in the order specified above. For example, an output with two test cases might look like $[[1,1,2],[2,1]]$.",
            "solution": "The problem presented is a well-posed and computationally tractable task. It requires the implementation of a variant of the K-means clustering algorithm, a fundamental procedure in unsupervised machine learning and data analysis. The problem is scientifically grounded, relying on standard definitions of vector norms ($L_1$, $L_2$, and $L_\\infty$) and the arithmetic mean. All parameters, initial conditions, data, and procedural rules—including termination and tie-breaking—are specified with sufficient precision to guarantee a unique, deterministic outcome. The context of applying these methods to vectors of financial ratios is a standard application within computational economics and finance, where different norms can represent distinct concepts of 'distance' or 'dissimilarity' between economic entities. For example, the $L_2$ norm corresponds to the familiar Euclidean distance, the $L_1$ norm (or 'Manhattan distance') sums the absolute differences across ratios, and the $L_\\infty$ norm (or 'Chebyshev distance') is determined by the single largest difference in any ratio, which can be relevant for worst-case analysis. The choice of norm fundamentally alters the geometry of the space, thereby influencing the shape of cluster boundaries and, consequently, the final assignments. This problem is therefore a valid and instructive exercise in understanding the practical implications of theoretical norms.\n\nThe algorithm to be implemented is an iterative procedure defined as follows. Given a set of data vectors $\\{x_i\\}_{i=1}^N$ in $\\mathbb{R}^3$, an initial set of $K=2$ centroids $\\{\\mu_1^{(0)}, \\mu_2^{(0)}\\}$, and a chosen norm $p \\in \\{1, 2, \\infty\\}$, we proceed through iterations indexed by $t=0, 1, 2, \\dots$. Each iteration consists of two steps.\n\nFirst, the **Assignment Step**: Each data vector $x_i$ is assigned to a cluster $k^\\star \\in \\{1, 2\\}$. The assignment is made by finding the cluster centroid $\\mu_k^{(t)}$ that is closest to $x_i$ as measured by the distance $d_p(x_i, \\mu_k^{(t)}) = \\lVert x_i - \\mu_k^{(t)} \\rVert_p$. The cluster index $k^\\star$ is thus given by:\n$$\nk^\\star = \\underset{k \\in \\{1,2\\}}{\\arg\\min} \\lVert x_i - \\mu_k^{(t)} \\rVert_p\n$$\nThe problem specifies a tie-breaking rule: if distances to both centroids are equal, the point is assigned to cluster $1$. Let $C_k^{(t)}$ be the set of indices of data vectors assigned to cluster $k$ at iteration $t$.\n\nSecond, the **Update Step**: The centroids for the next iteration, $\\mu_k^{(t+1)}$, are recalculated. For each cluster $k$, if the set of assigned vectors is non-empty (i.e., $C_k^{(t)} \\neq \\emptyset$), the new centroid is the component-wise arithmetic mean of all vectors assigned to that cluster:\n$$\n\\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{i \\in C_k^{(t)}} x_i\n$$\nIf a cluster becomes empty ($C_k^{(t)} = \\emptyset$), its centroid is not updated: $\\mu_k^{(t+1)} = \\mu_k^{(t)}$.\n\nThe process **Terminates** under one of two conditions: either the cluster assignments at iteration $t$ are identical to the assignments at iteration $t-1$, indicating convergence, or a maximum of $T=10$ iterations have been completed.\n\nThe solution will be produced by a program that systematically executes this algorithm for each of the seven specified test cases, which combine three datasets (A, B, C) with different norms ($p=2, 1, \\infty$). The program will compute the final cluster assignments for each case and format the output according to the specified requirements. The use of NumPy is appropriate for efficient and accurate vector and matrix operations, including the calculation of norms and means.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the clustering problem for all test cases as specified.\n    \"\"\"\n\n    def get_norm_func(p):\n        \"\"\"Returns a function to compute the distance based on the specified norm.\"\"\"\n        if p == 1:\n            return lambda a, b: np.linalg.norm(a - b, ord=1)\n        elif p == 2:\n            return lambda a, b: np.linalg.norm(a - b, ord=2)\n        elif p == 'inf':\n            return lambda a, b: np.linalg.norm(a - b, ord=np.inf)\n        else:\n            raise ValueError(\"Unsupported norm type.\")\n\n    def run_clustering(data, initial_centroids, p, max_iter=10):\n        \"\"\"\n        Runs the specified k-means variant clustering algorithm.\n\n        Args:\n            data (np.ndarray): The dataset of points (N, D).\n            initial_centroids (np.ndarray): The initial cluster centroids (K, D).\n            p (int or 'inf'): The norm to use for distance calculation (1, 2, or 'inf').\n            max_iter (int): Maximum number of iterations.\n\n        Returns:\n            list: A list of final cluster assignments (1-indexed).\n        \"\"\"\n        num_points = data.shape[0]\n        num_clusters = initial_centroids.shape[0]\n        \n        centroids = np.copy(initial_centroids)\n        assignments = np.zeros(num_points, dtype=int)\n        distance_func = get_norm_func(p)\n\n        for _ in range(max_iter):\n            old_assignments = np.copy(assignments)\n\n            # Assignment step\n            for i in range(num_points):\n                point = data[i]\n                \n                # Use a small epsilon for floating point comparisons to be robust,\n                # but the problem rule is a direct tie-break.\n                # d1 = d2 is used to handle ties, assigning to cluster 1.\n                dist_to_c1 = distance_func(point, centroids[0])\n                dist_to_c2 = distance_func(point, centroids[1])\n                \n                if dist_to_c1 = dist_to_c2:\n                    assignments[i] = 1\n                else:\n                    assignments[i] = 2\n\n            # Termination check\n            if np.array_equal(assignments, old_assignments):\n                break\n\n            # Update step\n            new_centroids = np.copy(centroids)\n            for k in range(1, num_clusters + 1):\n                # Get all points assigned to cluster k\n                cluster_points = data[assignments == k]\n                \n                if cluster_points.shape[0] > 0:\n                    # Update centroid to the mean of the assigned points\n                    new_centroids[k-1] = np.mean(cluster_points, axis=0)\n                # Else: centroid remains unchanged, as handled by copying `centroids` beforehand.\n            \n            centroids = new_centroids\n\n        return assignments.tolist()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 1,\n        },\n        {\n            \"data\": np.array([\n                [0.9, 1.1, 1.0], [1.2, 0.9, 1.1], [1.0, 1.2, 0.8],\n                [8.1, 7.9, 8.0], [7.8, 8.2, 8.1], [8.0, 8.1, 7.9]\n            ]),\n            \"centroids\": np.array([[1.0, 1.0, 1.0], [8.0, 8.0, 8.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [1.0, 0.0, 0.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 2,\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 'inf',\n        },\n        {\n            \"data\": np.array([\n                [0.1, 0.1, 0.1], [0.0, 0.0, 0.0], [0.2, 0.1, 0.0],\n                [4.2, 0.1, 0.0], [3.8, -0.1, 0.0], [3.0, 0.0, 5.0]\n            ]),\n            \"centroids\": np.array([[0.0, 0.0, 0.0], [4.0, 0.0, 0.0]]),\n            \"norm\": 1,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_clustering(case[\"data\"], case[\"centroids\"], case[\"norm\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    results_str_list = [f\"[{','.join(map(str, r))}]\" for r in results]\n    final_output_str = f\"[{','.join(results_str_list)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond individual data points, we often work with entire datasets structured as matrices, such as a time series of returns for multiple assets. This practice introduces the Frobenius norm as a tool for matrix approximation, a key technique in simplifying complex financial data. You will find the best rank-1 approximation to an asset return matrix, a procedure that is mathematically equivalent to building a single-factor model, and in doing so, see how abstract linear algebra provides the foundation for powerful models in finance .",
            "id": "2447261",
            "problem": "Consider an asset return matrix $R \\in \\mathbb{R}^{n \\times T}$, where rows correspond to $n$ assets and columns correspond to $T$ time periods. All returns are expressed as decimals (for example, an entry $0.01$ represents a return of $0.01$ in decimal form). The Frobenius norm of a matrix $A$ is defined as $\\lVert A \\rVert_{F} = \\sqrt{\\sum_{i,j} A_{ij}^{2}}$. The nearest rank-$1$ matrix to $R$ in the Frobenius norm is any matrix $X$ that solves\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times T}} \\ \\lVert R - X \\rVert_{F} \\quad \\text{subject to} \\quad \\operatorname{rank}(X) \\le 1.\n$$\nAny rank-$1$ matrix can be written as $X = b f^{\\top}$ with $b \\in \\mathbb{R}^{n}$ and $f \\in \\mathbb{R}^{T}$. In the context of a single-factor financial model, $b$ can be interpreted as the vector of asset loadings and $f$ as the factor return time series. To make $(b,f)$ unique, impose the normalization $\\lVert f \\rVert_{2} = 1$ and the sign convention that the first nonzero entry of $f$ is nonnegative. If $R$ is the zero matrix (all entries equal to $0$), define $b$ to be the zero vector in $\\mathbb{R}^{n}$ and $f$ to be the first standard basis vector in $\\mathbb{R}^{T}$.\n\nFor each test case below, do the following:\n- Compute the nearest rank-$1$ matrix $R_{1}$ to $R$ in the Frobenius norm, represented as $R_{1} = b f^{\\top}$ under the normalization and sign convention above.\n- Compute the residual Frobenius norm $\\lVert R - R_{1} \\rVert_{F}$.\n- Compute the explained share of variation by $R_{1}$ as\n$$\ns = \\begin{cases}\n\\frac{\\lVert R_{1} \\rVert_{F}^{2}}{\\lVert R \\rVert_{F}^{2}},  \\text{if } \\lVert R \\rVert_{F} \\ne 0, \\\\\n0,  \\text{if } \\lVert R \\rVert_{F} = 0.\n\\end{cases}\n$$\n\nRound every numerical output to six decimal places. For each test case, output a single list\n$$\n[\\ \\lVert R - R_{1} \\rVert_{F},\\ s,\\ b_{1},\\ldots,b_{n},\\ f_{1},\\ldots,f_{T}\\ ],\n$$\nwhere $b_{i}$ are the entries of $b$ and $f_{j}$ are the entries of $f$. Your program should produce a single line of output containing the results for all test cases as a comma-separated list of these per-test-case lists, enclosed in a single pair of square brackets.\n\nTest Suite:\n- Test Case $1$ (general case, rank-$1$ structure with small noise). Let $R \\in \\mathbb{R}^{3 \\times 4}$ be\n$$\nR = \\begin{bmatrix}\n0.006  -0.0105  0.0078  -0.0002 \\\\\n-0.0023  0.0044  -0.0031  0.0002 \\\\\n0.0012  -0.0021  0.002  -0.0003\n\\end{bmatrix}.\n$$\n- Test Case $2$ (boundary case, zero matrix). Let $R \\in \\mathbb{R}^{2 \\times 3}$ be\n$$\nR = \\begin{bmatrix}\n0  0  0 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- Test Case $3$ (exact rank-$1$ case). Let $R \\in \\mathbb{R}^{4 \\times 3}$ be\n$$\nR = \\begin{bmatrix}\n0.0006  -0.0003  0.00015 \\\\\n0.0002  -0.0001  0.00005 \\\\\n-0.0004  0.0002  -0.0001 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n\nFinal Output Format:\n- A single line containing a list of three items (one per test case), where each item is the list described above. For example, the line should look like\n$$\n[ [\\ \\cdots\\ ],[\\ \\cdots\\ ],[\\ \\cdots\\ ] ].\n$$\nAll numbers must be rounded to six decimal places, and returns must be treated as decimals (not percentages).",
            "solution": "The problem requires finding the nearest rank-$1$ approximation to a given asset return matrix $R \\in \\mathbb{R}^{n \\times T}$ and calculating associated financial metrics. This is a classical problem in linear algebra, the solution to which is provided by the Eckart-Young-Mirsky theorem. The theorem states that the best rank-$k$ approximation of a matrix $R$ in the Frobenius norm is given by the truncated Singular Value Decomposition (SVD).\n\nFirst, we define the SVD of the matrix $R$ as:\n$$\nR = U \\Sigma V^{\\top}\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{T \\times T}$ are orthogonal matrices ($U^{\\top}U = I_n$, $V^{\\top}V = I_T$), and $\\Sigma \\in \\mathbb{R}^{n \\times T}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i$ in non-increasing order: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(n,T)} \\ge 0$. The columns of $U$, denoted $u_i$, are the left singular vectors, and the columns of $V$, denoted $v_i$, are the right singular vectors.\n\nThe Eckart-Young-Mirsky theorem specifies that the solution to the minimization problem\n$$\n\\min_{X} \\lVert R - X \\rVert_{F} \\quad \\text{subject to} \\quad \\operatorname{rank}(X) \\le 1\n$$\nis the matrix $R_1$ constructed from the largest singular value $\\sigma_1$ and its corresponding singular vectors $u_1$ and $v_1$:\n$$\nR_1 = \\sigma_1 u_1 v_1^{\\top}\n$$\nThis matrix $R_1$ is the nearest rank-$1$ approximation to $R$.\n\nThe problem requires us to represent $R_1$ in the form of a single-factor model, $R_1 = b f^{\\top}$, where $b \\in \\mathbb{R}^{n}$ is the vector of asset loadings and $f \\in \\mathbb{R}^{T}$ is the factor return time series. To ensure a unique representation, two conditions are imposed:\n$1.$ Normalization: The factor time series must have a unit Euclidean norm, $\\lVert f \\rVert_2 = 1$.\n$2.$ Sign Convention: The first non-zero entry of $f$ must be non-negative.\n\nWe can relate the SVD components to the factors $b$ and $f$. From $R_1 = \\sigma_1 u_1 v_1^{\\top}$ and $R_1 = b f^{\\top}$, and knowing that singular vectors from SVD are already normalized ($\\lVert u_1 \\rVert_2 = 1$, $\\lVert v_1 \\rVert_2 = 1$), we can make the identification:\n$$\nf \\propto v_1 \\quad \\text{and} \\quad b \\propto u_1\n$$\nTo satisfy the normalization condition $\\lVert f \\rVert_2 = 1$, we can set $f = v_1$ and $b = \\sigma_1 u_1$. However, the SVD components $(u_1, v_1)$ are only unique up to a simultaneous sign change, i.e., $(u_1, v_1)$ and $(-u_1, -v_1)$ both produce the same matrix $R_1$. We must use the sign convention to resolve this ambiguity.\n\nLet's define a temporary factor vector $f_{\\text{temp}} = v_1$. We find the first non-zero element of $f_{\\text{temp}}$. If this element is negative, we must flip the signs of both vectors. We define a sign multiplier $\\alpha \\in \\{-1, 1\\}$.\n$$\n\\alpha = \\begin{cases}\n-1  \\text{if the first non-zero element of } v_1 \\text{ is negative} \\\\\n+1  \\text{otherwise}\n\\end{cases}\n$$\nThe unique factors are then given by:\n$$\nf = \\alpha v_1\n$$\n$$\nb = \\alpha \\sigma_1 u_1\n$$\nThis construction satisfies $R_1 = b f^{\\top} = (\\alpha \\sigma_1 u_1)(\\alpha v_1)^{\\top} = \\alpha^2 \\sigma_1 u_1 v_1^{\\top} = \\sigma_1 u_1 v_1^{\\top}$, as well as $\\lVert f \\rVert_2 = \\lVert \\alpha v_1 \\rVert_2 = |\\alpha|\\lVert v_1 \\rVert_2=1$, and the sign convention on $f$.\n\nThe problem specifies a special case for a zero matrix $R=0_{n \\times T}$. In this case, all singular values are zero. The problem explicitly defines $b$ as a zero vector in $\\mathbb{R}^n$ and $f$ as the first standard basis vector in $\\mathbb{R}^T$, i.e., $f = e_1 = [1, 0, \\dots, 0]^{\\top}$. This gives $R_1=0 \\cdot e_1^\\top = 0$.\n\nNext, we compute the required metrics:\n$1.$ The residual Frobenius norm, $\\lVert R - R_{1} \\rVert_{F}$. The properties of SVD provide that the squared Frobenius norm of a matrix is the sum of its squared singular values: $\\lVert R \\rVert_{F}^{2} = \\sum_i \\sigma_i^2$. The error of the rank-$1$ approximation is the Frobenius norm of the remaining part of the SVD sum.\n$$\n\\lVert R - R_{1} \\rVert_{F} = \\left\\lVert \\left(\\sum_{i=1}^{\\min(n,T)} \\sigma_i u_i v_i^{\\top}\\right) - \\sigma_1 u_1 v_1^{\\top} \\right\\rVert_{F} = \\left\\lVert \\sum_{i=2}^{\\min(n,T)} \\sigma_i u_i v_i^{\\top} \\right\\rVert_{F}\n$$\nDue to the orthogonality of singular vectors, this simplifies to:\n$$\n\\lVert R - R_{1} \\rVert_{F} = \\sqrt{\\sum_{i=2}^{\\min(n,T)} \\sigma_i^2} = \\sqrt{\\lVert R \\rVert_F^2 - \\sigma_1^2}\n$$\nFor a matrix $R$ that is exactly rank-$1$, all singular values except $\\sigma_1$ are zero, thus the residual norm is $0$.\n\n$2.$ The explained share of variation, $s$. This is the ratio of the variance of the approximation to the variance of the original data.\n$$\ns = \\frac{\\lVert R_{1} \\rVert_{F}^{2}}{\\lVert R \\rVert_{F}^{2}}\n$$\nWe know $\\lVert R \\rVert_F^2 = \\sum_i \\sigma_i^2$. The norm of the approximation is $\\lVert R_1 \\rVert_F = \\lVert \\sigma_1 u_1 v_1^{\\top} \\rVert_F = \\sigma_1$. Thus, $\\lVert R_1 \\rVert_F^2 = \\sigma_1^2$. The explained share is:\n$$\ns = \\frac{\\sigma_1^2}{\\sum_i \\sigma_i^2}\n$$\nIf $R$ is the zero matrix, $\\lVert R \\rVert_F = 0$, and the problem defines $s=0$.\n\nThe overall algorithm is as follows:\n$1.$ For a given matrix $R$, first check if it is a zero matrix. If so, use the provided definitions for $b$ and $f$, and compute $\\lVert R - R_1 \\rVert_F = 0$ and $s=0$.\n$2.$ If $R$ is not a zero matrix, compute its SVD to find $\\sigma_i$, $u_1$, and $v_1$.\n$3.$ Apply the sign convention to $v_1$ to determine the sign multiplier $\\alpha$.\n$4.$ Calculate $b = \\alpha \\sigma_1 u_1$ and $f = \\alpha v_1$.\n$5.$ Calculate the residual norm $\\lVert R - R_{1} \\rVert_{F} = \\sqrt{\\sum_{i \\ge 2} \\sigma_i^2}$.\n$6.$ Calculate the explained share $s = \\sigma_1^2 / (\\sum_i \\sigma_i^2)$.\n$7.$ Collate all numerical results into a list, with each number rounded to six decimal places.\n\nThis procedure will be applied to each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve_case(R):\n    \"\"\"\n    Computes the rank-1 approximation and related metrics for a given matrix R.\n\n    Args:\n        R (np.ndarray): The input matrix.\n\n    Returns:\n        list: A list containing the residual norm, explained share, \n              factor loadings (b), and factor returns (f).\n    \"\"\"\n    n, T = R.shape\n    \n    # Handle the boundary case of a zero matrix\n    if np.allclose(R, 0):\n        residual_norm = 0.0\n        explained_share = 0.0\n        b = np.zeros(n)\n        f = np.zeros(T)\n        f[0] = 1.0\n        \n        result_list = [residual_norm, explained_share] + b.tolist() + f.tolist()\n        return [np.round(x, 6) for x in result_list]\n\n    # Compute the full SVD\n    # Use full_matrices=False for efficiency\n    U, s_vals, Vh = np.linalg.svd(R, full_matrices=False)\n    \n    # Extract the components for the rank-1 approximation\n    sigma1 = s_vals[0]\n    u1 = U[:, 0]\n    v1 = Vh[0, :]\n    \n    # Apply sign convention: first non-zero entry of f must be non-negative\n    # Find the index of the first non-zero element with a small tolerance\n    try:\n        first_nonzero_idx = np.nonzero(np.abs(v1) > 1e-12)[0][0]\n        sign_multiplier = np.sign(v1[first_nonzero_idx])\n        # A sign of 0 can occur if the value is extremely small. Default to 1.\n        if sign_multiplier == 0:\n            sign_multiplier = 1.0\n    except IndexError:\n        # This case (v1 is all zeros) should not happen if R is not a zero matrix.\n        sign_multiplier = 1.0\n        \n    f = v1 * sign_multiplier\n    b = u1 * sigma1 * sign_multiplier\n    \n    # Compute the residual Frobenius norm\n    R_norm_fro_sq = np.sum(s_vals**2)\n    residual_norm_sq = R_norm_fro_sq - sigma1**2\n    # Clamp to zero to avoid negative due to floating point inaccuracies\n    residual_norm = np.sqrt(max(0, residual_norm_sq))\n    \n    # Compute the explained share of variation\n    if R_norm_fro_sq == 0:\n        explained_share = 0.0\n    else:\n        explained_share = sigma1**2 / R_norm_fro_sq\n    \n    # Assemble the final list of results\n    result_list = [residual_norm, explained_share] + b.tolist() + f.tolist()\n    \n    # Round all numerical outputs to six decimal places\n    rounded_results = [np.round(x, 6) for x in result_list]\n    \n    return rounded_results\n\ndef solve():\n    \"\"\"\n    Defines test cases and solves them, printing the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: general case\n        np.array([\n            [0.006, -0.0105, 0.0078, -0.0002],\n            [-0.0023, 0.0044, -0.0031, 0.0002],\n            [0.0012, -0.0021, 0.002, -0.0003]\n        ]),\n        # Test Case 2: boundary case (zero matrix)\n        np.array([\n            [0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0]\n        ]),\n        # Test Case 3: exact rank-1 case\n        np.array([\n            [0.0006, -0.0003, 0.00015],\n            [0.0002, -0.0001, 0.00005],\n            [-0.0004, 0.0002, -0.0001],\n            [0.0, 0.0, 0.0]\n        ])\n    ]\n\n    all_results = []\n    for R in test_cases:\n        all_results.append(solve_case(R))\n    \n    # Format the final output string as a list of lists.\n    # Convert rounded numbers to strings to ensure proper formatting e.g., '0.0'\n    outer_list = []\n    for res_list in all_results:\n        # Format each number to 6 decimal places, handling -0.0 cases.\n        formatted_list = [f\"{x:.6f}\" if x == 0.0 else str(x) for x in res_list]\n        inner_list_str = '[' + ','.join(formatted_list) + ']'\n        outer_list.append(inner_list_str)\n    \n    final_output_str = '[' + ','.join(outer_list) + ']'\n    \n    print(final_output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "Many computational problems in economics and finance, from estimating econometric models to pricing derivatives, rely on solving linear systems of the form $Ax=b$. However, not all systems are created equal; some are exquisitely sensitive to small errors or noise in the input data $b$. This numerical experiment will guide you in demonstrating this phenomenon, known as ill-conditioning, and show how matrix norms are central to defining the 'condition number,' a key diagnostic for the reliability of your computational results .",
            "id": "2449583",
            "problem": "Design and implement a complete, runnable program that performs a numerical experiment demonstrating that for an ill-conditioned matrix $A$, a small relative error in $b$ can lead to a large relative error in the solution $x$ of the linear system $A x = b$. The experiment must be based strictly on first principles: norms, relative errors, and the definition of the linear solve. All quantities must use the vector and matrix $2$-norm. The program must compute, for each specified test case, the amplification factor defined as\n$$\nr \\;=\\; \\frac{\\|x_{\\epsilon} - x^\\star\\|_{2} / \\|x^\\star\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}},\n$$\nwhere $x^\\star$ is the exact solution corresponding to the unperturbed right-hand side $b$, $\\delta b$ is a perturbation of $b$, and $x_{\\epsilon}$ is the solution of $A x = b + \\delta b$. The program must use the following experiment setup for every test case:\n- Let $x^\\star$ be the vector in $\\mathbb{R}^n$ with all components equal to $1$.\n- Let $b = A x^\\star$.\n- Let the perturbation direction $v \\in \\mathbb{R}^n$ be defined componentwise by $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$ for $i = 1, \\dots, n$ so that $\\|v\\|_2 = 1$.\n- Let $\\delta b = \\epsilon \\, \\|b\\|_2 \\, v$, where $\\epsilon$ is the prescribed relative perturbation magnitude for the test.\n- Let $x_{\\epsilon}$ be the solution of $A x = b + \\delta b$.\n\nYour program must compute, for each test case, the amplification factor $r$ as a floating-point number. The set of test cases is as follows, each identified by its case number and executed in the listed order:\n- Case $1$: $A$ is the Hilbert matrix $H \\in \\mathbb{R}^{5 \\times 5}$ with entries $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1,\\dots,5\\}$, and $\\epsilon = 10^{-8}$.\n- Case $2$: $A$ is the Hilbert matrix $H \\in \\mathbb{R}^{10 \\times 10}$ with entries $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1,\\dots,10\\}$, and $\\epsilon = 10^{-8}$.\n- Case $3$: $A$ is the identity matrix $I \\in \\mathbb{R}^{8 \\times 8}$, and $\\epsilon = 10^{-8}$.\n- Case $4$: $A \\in \\mathbb{R}^{2 \\times 2}$ is given by\n$$\nA = \\begin{bmatrix}\n1  1 \\\\\n1  1 + 10^{-10}\n\\end{bmatrix},\n$$\nand $\\epsilon = 10^{-12}$.\n\nAll norms must be the $2$-norm. There are no physical units involved. Angles are not used. Percentages must not be used; all ratios and magnitudes must be expressed as decimal floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[r1,r2,r3,r4]\"). Each entry must be the amplification factor $r$ for the corresponding case, rounded to $6$ significant digits, in the order of cases $1$ through $4$.",
            "solution": "The experiment is designed from first principles of linear algebra and norm-based error analysis. We consider a linear system $A x = b$ with exact data and its perturbed counterpart $A x = b + \\delta b$. For a given matrix $A \\in \\mathbb{R}^{n \\times n}$ and an exact solution vector $x^\\star \\in \\mathbb{R}^n$, we define $b = A x^\\star$. We then construct a perturbation $\\delta b$ with prescribed relative magnitude $\\epsilon$ as follows. Let $v \\in \\mathbb{R}^n$ with entries $v_i = \\frac{(-1)^{i-1}}{\\sqrt{n}}$, for $i = 1, \\dots, n$. This $v$ satisfies $\\|v\\|_2 = 1$ by construction:\n$$\n\\|v\\|_2^2 = \\sum_{i=1}^n \\left(\\frac{1}{\\sqrt{n}}\\right)^2 = \\frac{n}{n} = 1.\n$$\nWe set $\\delta b = \\epsilon \\, \\|b\\|_2 \\, v$. Then the relative perturbation in $b$ is exactly $\\frac{\\|\\delta b\\|_2}{\\|b\\|_2} = \\epsilon$ because\n$$\n\\|\\delta b\\|_2 = \\epsilon \\, \\|b\\|_2 \\, \\|v\\|_2 = \\epsilon \\, \\|b\\|_2.\n$$\nLet $x_\\epsilon$ denote the solution to the perturbed system $A x = b + \\delta b$. The error in the solution equals\n$$\nx_\\epsilon - x^\\star = A^{-1}\\,(b+\\delta b) - A^{-1} b = A^{-1}\\,\\delta b.\n$$\nTherefore the relative error in $x$ is\n$$\n\\frac{\\|x_\\epsilon - x^\\star\\|_2}{\\|x^\\star\\|_2} = \\frac{\\|A^{-1}\\,\\delta b\\|_2}{\\|x^\\star\\|_2}.\n$$\nThe amplification factor $r$ that the program reports for each case is\n$$\nr = \\frac{\\|x_{\\epsilon} - x^\\star\\|_{2} / \\|x^\\star\\|_{2}}{\\|\\delta b\\|_{2} / \\|b\\|_{2}}.\n$$\nFrom norm properties and the definition of the matrix $2$-norm, we can relate this amplification to the condition number in the matrix $2$-norm, $\\kappa_2(A) = \\|A\\|_2 \\, \\|A^{-1}\\|_2$. Specifically, using $\\|A^{-1} \\delta b\\|_2 \\le \\|A^{-1}\\|_2 \\, \\|\\delta b\\|_2$ and $\\|b\\|_2 = \\|A x^\\star\\|_2 \\le \\|A\\|_2 \\, \\|x^\\star\\|_2$, we obtain\n$$\n\\frac{\\|x_\\epsilon - x^\\star\\|_2}{\\|x^\\star\\|_2} \\le \\|A^{-1}\\|_2 \\, \\|\\delta b\\|_2 \\,\\frac{1}{\\|x^\\star\\|_2}\n\\le \\|A^{-1}\\|_2 \\, \\|A\\|_2 \\, \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} = \\kappa_2(A) \\, \\frac{\\|\\delta b\\|_2}{\\|b\\|_2}.\n$$\nHence\n$$\nr \\le \\kappa_2(A).\n$$\nThis inequality shows that the relative error in the solution can be amplified by up to approximately the condition number. For well-conditioned matrices such as the identity matrix $I$, we have $\\kappa_2(I) = 1$, and thus $r$ should be close to $1$. For ill-conditioned matrices such as Hilbert matrices, $\\kappa_2(A)$ is very large, and even a very small $\\epsilon$ can result in a large relative error in $x$, producing a large $r$.\n\nThe test suite covers several regimes:\n- Case $1$ uses a Hilbert matrix of size $5$, which is ill-conditioned but moderate in size.\n- Case $2$ uses a Hilbert matrix of size $10$, which is more ill-conditioned, typically yielding a much larger amplification $r$.\n- Case $3$ uses the identity matrix of size $8$, which is perfectly conditioned, so $r$ should be approximately $1$.\n- Case $4$ uses a nearly singular $2 \\times 2$ matrix with entries differing by $10^{-10}$ on one element, producing a very large amplification.\n\nFor each case, the program constructs $x^\\star$, $b$, the unit-norm perturbation direction $v$, the perturbation $\\delta b$ with the specified $\\epsilon$, solves for $x_\\epsilon$, computes the relative errors, and reports $r$ rounded to $6$ significant digits. The final output is a single line containing the list $[r_1, r_2, r_3, r_4]$ in that order. This procedure directly and transparently exhibits how ill-conditioning in $A$ magnifies small relative perturbations in $b$ into large relative errors in the solution $x$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hilbert(n: int) -> np.ndarray:\n    # H[i,j] = 1 / (i + j + 1) with zero-based i,j; but use one-based formula directly\n    i = np.arange(1, n + 1).reshape(-1, 1)\n    j = np.arange(1, n + 1).reshape(1, -1)\n    return 1.0 / (i + j - 1.0)\n\ndef alternating_unit_vector(n: int) -> np.ndarray:\n    # v_i = (-1)^(i-1) / sqrt(n), i = 1..n\n    signs = (-1.0) ** np.arange(n)\n    v = signs / np.sqrt(n)\n    # Ensure unit norm numerically\n    return v / np.linalg.norm(v, 2)\n\ndef amplification_factor(A: np.ndarray, eps: float) -> float:\n    n = A.shape[0]\n    x_star = np.ones(n, dtype=float)\n    b = A @ x_star\n    nb = np.linalg.norm(b, 2)\n    if nb == 0.0:\n        # Degenerate, but not expected with provided tests; return NaN-like large value\n        return float('nan')\n    v = alternating_unit_vector(n)\n    delta_b = eps * nb * v\n    b_tilde = b + delta_b\n    # Solve for perturbed solution\n    x_tilde = np.linalg.solve(A, b_tilde)\n    # Relative errors\n    rel_b = np.linalg.norm(delta_b, 2) / nb\n    rel_x = np.linalg.norm(x_tilde - x_star, 2) / np.linalg.norm(x_star, 2)\n    # Amplification factor\n    return rel_x / rel_b\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case is a tuple: (A_matrix, epsilon)\n    A1 = hilbert(5)\n    eps1 = 1e-8\n\n    A2 = hilbert(10)\n    eps2 = 1e-8\n\n    A3 = np.eye(8, dtype=float)\n    eps3 = 1e-8\n\n    A4 = np.array([[1.0, 1.0],\n                   [1.0, 1.0 + 1e-10]], dtype=float)\n    eps4 = 1e-12\n\n    test_cases = [\n        (A1, eps1),\n        (A2, eps2),\n        (A3, eps3),\n        (A4, eps4),\n    ]\n\n    results = []\n    for A, eps in test_cases:\n        r = amplification_factor(A, eps)\n        # Round to 6 significant digits\n        if np.isnan(r) or np.isinf(r):\n            results.append(\"nan\")\n        else:\n            results.append(f\"{r:.6g}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}