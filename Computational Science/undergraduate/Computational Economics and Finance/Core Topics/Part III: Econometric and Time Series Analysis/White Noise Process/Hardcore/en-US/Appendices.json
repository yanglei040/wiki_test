{
    "hands_on_practices": [
        {
            "introduction": "To model unpredictable shocks in financial markets or communication systems, we first need a way to generate them. This exercise provides a foundational hands-on skill: creating a realization of a Gaussian white noise process using a standard random number generator. By applying the Box-Muller transform, you will see how to convert uniformly distributed numbers into the normally distributed, uncorrelated values that define white noise, making an abstract concept concrete .",
            "id": "1350034",
            "problem": "An engineer is developing a simulation for a digital communication system and needs to model channel noise. The noise is modeled as a discrete-time Gaussian white noise process, $\\{W[n]\\}_{n=1,2,...}$, which is a sequence of independent and identically distributed random variables, each following a normal distribution with a mean of zero and a specific variance.\n\nThe engineer uses a standard pseudo-random number generator that produces a sequence of numbers $\\{U_n\\}$ drawn from a uniform distribution on the interval $[0, 1)$. To convert these into standard normal random variables $\\{Z_n\\}$ (with mean $\\mu=0$ and variance $\\sigma^2=1$), the engineer employs the Box-Muller transform. This method takes two independent uniform random variables, $U_1$ and $U_2$, and produces two independent standard normal random variables, $Z_1$ and $Z_2$, using the following equations:\n$$Z_1 = \\sqrt{-2 \\ln(U_1)} \\cos(2\\pi U_2)$$\n$$Z_2 = \\sqrt{-2 \\ln(U_1)} \\sin(2\\pi U_2)$$\nThe Gaussian white noise process $\\{W[n]\\}$ is then generated by appropriately scaling the sequence of standard normal variables $\\{Z[n]\\}$ to achieve a desired variance of $\\sigma_W^2 = 7.5$.\n\nSuppose the first two numbers produced by the uniform random number generator are $U_1 = \\exp(-4.5)$ and $U_2 = 1/3$. Using the first standard normal variable $Z_1$ generated by these two uniform numbers, calculate the corresponding first sample of the white noise process, $W[1]$.\n\nExpress your answer as a single real number rounded to three significant figures.",
            "solution": "We use the Boxâ€“Muller transform. Given $U_{1}=\\exp(-4.5)$ and $U_{2}=\\frac{1}{3}$, the first standard normal variate is\n$$\nZ_{1}=\\sqrt{-2\\ln(U_{1})}\\,\\cos(2\\pi U_{2}).\n$$\nSince $\\ln(U_{1})=\\ln(\\exp(-4.5))=-4.5$, we have\n$$\n-2\\ln(U_{1})=-2(-4.5)=9,\\quad \\sqrt{-2\\ln(U_{1})}=\\sqrt{9}=3.\n$$\nAlso,\n$$\n\\cos(2\\pi U_{2})=\\cos\\!\\left(2\\pi\\cdot \\frac{1}{3}\\right)=\\cos\\!\\left(\\frac{2\\pi}{3}\\right)=-\\frac{1}{2}.\n$$\nTherefore,\n$$\nZ_{1}=3\\left(-\\frac{1}{2}\\right)=-\\frac{3}{2}.\n$$\nTo obtain the white noise sample with variance $\\sigma_{W}^{2}=7.5$, scale the standard normal by $\\sigma_{W}=\\sqrt{7.5}$:\n$$\nW[1]=\\sqrt{\\sigma_{W}^{2}}\\,Z_{1}=\\sqrt{7.5}\\left(-\\frac{3}{2}\\right)=-\\frac{3}{2}\\sqrt{7.5}.\n$$\nNumerically, $\\sqrt{7.5}\\approx 2.738612787$, hence\n$$\nW[1]\\approx -1.5\\times 2.738612787\\approx -4.107919181,\n$$\nwhich rounded to three significant figures is $-4.11$.",
            "answer": "$$\\boxed{-4.11}$$"
        },
        {
            "introduction": "White noise is the fundamental building block for many important time series models used in economics and finance. This practice moves from generating noise to using it by analyzing a simple yet powerful Moving Average (MA) process built directly from white noise shocks. Calculating the autocovariance will reveal how the structure of the model creates short-term memory, a key feature of many economic and financial data series .",
            "id": "1350040",
            "problem": "A digital signal processing engineer is analyzing the output of a newly designed filter. The output signal at discrete time points $t$, denoted as $X_t$, is modeled by a stochastic process. This model describes how the output is influenced by a stream of unpredictable, random electronic fluctuations. The relationship is given by the equation:\n\n$$X_t = W_t + \\theta W_{t-1}$$\n\nHere, the sequence $\\{W_t\\}$ for all integer times $t$ represents a discrete-time white noise process. This means that each $W_t$ is a random variable with the following properties:\n1.  The mean (expected value) is zero for all $t$: $E[W_t] = 0$.\n2.  The variance is a constant value $\\sigma_W^2$ for all $t$: $\\text{Var}(W_t) = \\sigma_W^2$.\n3.  The values at different time points are uncorrelated: $\\text{Cov}(W_t, W_s) = 0$ for any $t \\neq s$.\n\nThe parameter $\\theta$ is a fixed, real-valued constant that characterizes the filter's design. To understand how a random fluctuation at one moment affects the signal at the next, the engineer needs to compute the covariance between the signal's value at time $t$ and its value at the immediately succeeding time step, $t+1$.\n\nDetermine the covariance $\\text{Cov}(X_t, X_{t+1})$. Your final answer should be a symbolic expression in terms of $\\theta$ and $\\sigma_W^2$.",
            "solution": "We are given the moving-average process of order one:\n$$X_{t} = W_{t} + \\theta W_{t-1}, \\quad X_{t+1} = W_{t+1} + \\theta W_{t}.$$\nThe covariance at lag one is defined by\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\mathbb{E}[X_{t}X_{t+1}] - \\mathbb{E}[X_{t}]\\,\\mathbb{E}[X_{t+1}].$$\nSince $\\mathbb{E}[W_{t}] = 0$ for all $t$, we have\n$$\\mathbb{E}[X_{t}] = \\mathbb{E}[W_{t} + \\theta W_{t-1}] = 0 + \\theta \\cdot 0 = 0,$$\nand similarly $\\mathbb{E}[X_{t+1}] = 0$. Therefore,\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\mathbb{E}[X_{t}X_{t+1}].$$\nCompute the product:\n$$X_{t}X_{t+1} = (W_{t} + \\theta W_{t-1})(W_{t+1} + \\theta W_{t})\n= W_{t}W_{t+1} + \\theta W_{t}^{2} + \\theta W_{t-1}W_{t+1} + \\theta^{2} W_{t-1}W_{t}.$$\nTaking expectations and using the white-noise properties $\\mathbb{E}[W_{s}W_{t}] = 0$ for $s \\neq t$ and $\\mathbb{E}[W_{t}^{2}] = \\text{Var}(W_{t}) = \\sigma_{W}^{2}$, we obtain\n$$\\mathbb{E}[X_{t}X_{t+1}] = 0 + \\theta\\,\\mathbb{E}[W_{t}^{2}] + 0 + 0 = \\theta \\sigma_{W}^{2}.$$\nThus,\n$$\\text{Cov}(X_{t}, X_{t+1}) = \\theta \\sigma_{W}^{2}.$$",
            "answer": "$$\\boxed{\\theta \\sigma_{W}^{2}}$$"
        },
        {
            "introduction": "After building an economic model, how do we know if it's a good one? A crucial diagnostic step is to examine the model's errors, or residuals, to see if any predictable patterns remain. This capstone practice guides you through implementing the formal statistical tests used to determine if a residual series is indistinguishable from weak white noise, a sign that your model has successfully captured the underlying structure in the data .",
            "id": "2448045",
            "problem": "You are analyzing whether seasonality-adjusted daily sales residuals from a firm behave like weak white noise in the sense of zero mean and no linear autocorrelation. Use the following fundamental base: by definition, a weak white noise process $\\{e_t\\}$ satisfies $\\mathbb{E}[e_t] = 0$ and $\\operatorname{Cov}(e_t, e_{t-k}) = 0$ for all integers $k \\neq 0$. To decide if a finite observed sequence is consistent with weak white noise at a given significance level, proceed from first principles as follows: test the mean using a two-sided Student's $t$-test for $\\mathbb{E}[e_t]=0$, and test the joint null of zero autocorrelation up to lag $h$ using the Ljung-Box portmanteau statistic.\n\nImplementation details to be followed by your program:\n- Mean-zero test. Let the sample size be $n$, the sample mean be $\\bar{e} = \\frac{1}{n}\\sum_{t=1}^{n} e_t$, and the unbiased sample standard deviation be $s = \\sqrt{\\frac{1}{n-1}\\sum_{t=1}^{n}(e_t - \\bar{e})^2}$. Under the null hypothesis $\\mathbb{E}[e_t] = 0$, the statistic\n$$\nT = \\frac{\\bar{e}}{s/\\sqrt{n}}\n$$\nhas a Student's $t$ distribution with $n-1$ degrees of freedom for independent, identically distributed data with finite second moment. Compute the two-sided $p$-value and compare to the given significance level $\\alpha$.\n- Autocorrelation test. For each lag $k \\in \\{1,\\dots,h^\\ast\\}$ where $h^\\ast = \\min(h, n-1)$, compute the sample autocorrelation\n$$\n\\hat{r}_k = \\frac{\\sum_{t=k+1}^{n}(e_t - \\bar{e})(e_{t-k} - \\bar{e})}{\\sum_{t=1}^{n}(e_t - \\bar{e})^2}.\n$$\nThen compute the Ljung-Box statistic\n$$\nQ = n(n+2)\\sum_{k=1}^{h^\\ast} \\frac{\\hat{r}_k^2}{n-k}.\n$$\nUnder the joint null hypothesis that $\\hat{r}_k = 0$ for all $k=1,\\dots,h^\\ast$, $Q$ is approximately distributed as chi-square with $h^\\ast$ degrees of freedom for large $n$. Compute the corresponding $p$-value and compare to $\\alpha$.\n- Decision rule. Declare a residual series \"weak white noise\" if and only if both tests fail to reject at level $\\alpha$, that is, both $p_{\\text{mean}} \\ge \\alpha$ and $p_{\\text{LB}} \\ge \\alpha$ hold.\n- Degenerate variance handling. If the unbiased sample variance is exactly zero, then all $e_t$ are equal. In that case, declare the series \"not white noise\" unless all $e_t$ are exactly zero, in which case treat it as white noise by convention for this exercise. This rule prevents undefined divisions in the test statistics.\n\nAngle unit requirement: any angles used below are in radians.\n\nTest suite. Apply the above decision rule to the following four seasonality-adjusted residual sequences, each with its specified significance level $\\alpha$ and maximum autocorrelation lag $h$:\n\n- Case 1 (happy path: sparse uncorrelated residuals): $n=60$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = 0$ for all $t$ except $e_{6} = 1.0$ and $e_{51} = -1.0$.\n- Case 2 (autocorrelated leftovers from weekly pattern): $n=56$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = \\sin\\!\\left(\\frac{2\\pi t}{7}\\right)$ for $t=0,1,\\dots,55$, where $\\pi$ is the mathematical constant and the angle is in radians.\n- Case 3 (misadjusted constant bias; boundary case with zero variance): $n=30$, $h=5$, $\\alpha=0.05$. The residuals are constant $e_t = 0.3$ for all $t=1,\\dots,30$.\n- Case 4 (small sample, large $h$ close to $n$; boundary on $h$): $n=12$, $h=10$, $\\alpha=0.05$. The residuals are $e_t = 0$ for all $t$ except $e_{1} = 1.0$ and $e_{12} = -1.0$.\n\nYour program must compute, for each case, a boolean indicating whether the residuals are weak white noise at level $\\alpha$ according to the above rule. The final output format must be exactly one line containing the four boolean results in order as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False]\". No additional text should be printed.",
            "solution": "The problem requires implementing a two-part statistical test for weak white noise and applying it to four specific time series. A series is deemed weak white noise if it passes both a mean-zero test (Student's t-test) and a no-autocorrelation test (Ljung-Box test) at a given significance level $\\alpha$.\n\nHere is a logical breakdown of the expected results for each case:\n\n- **Case 1**: The series has only two non-zero values, $e_6=1.0$ and $e_{51}=-1.0$, in a sample of $n=60$.\n  - **Mean Test**: The sample mean $\\bar{e} = (1.0 - 1.0)/60 = 0$. The $t$-statistic is 0, leading to a $p$-value of 1.0. Since $1.0 \\ge 0.05$, this test passes.\n  - **Autocorrelation Test**: The non-zero values are 45 lags apart. The Ljung-Box test considers lags up to $h=10$. For any lag $k \\in [1, 10]$, the product $(e_t - \\bar{e})(e_{t-k} - \\bar{e})$ is always zero, so all sample autocorrelations $\\hat{r}_k$ are zero. This results in a $Q$-statistic of 0 and a $p$-value of 1.0. Since $1.0 \\ge 0.05$, this test passes.\n  - **Conclusion**: Since both tests pass, the series is classified as weak white noise. **Result: True**.\n\n- **Case 2**: The series is a perfect sinusoid, $e_t = \\sin(2\\pi t/7)$, over $n=56$ points (8 full periods).\n  - **Mean Test**: The sample mean over an integer number of full periods is 0. The $t$-test will pass with a $p$-value of 1.0.\n  - **Autocorrelation Test**: The series is perfectly periodic with a period of 7. The sample autocorrelation at lag 7, $\\hat{r}_7$, will be very close to 1. This large autocorrelation will produce a very large $Q$-statistic, leading to a $p$-value that is nearly 0. Since $p_{LB}  0.05$, the test for no autocorrelation fails decisively.\n  - **Conclusion**: Since the autocorrelation test fails, the series is not weak white noise. **Result: False**.\n\n- **Case 3**: The series is a constant $e_t=0.3$ for all $t$.\n  - **Degenerate Case**: The sample variance is exactly zero. The problem specifies a special rule for this case: the series is not considered white noise unless all its values are exactly zero. Since the values are 0.3, this rule applies.\n  - **Conclusion**: The series is classified as not weak white noise. **Result: False**.\n\n- **Case 4**: A sparse series with $n=12$, $e_1=1.0$, and $e_{12}=-1.0$. Lags up to $h=10$ are tested.\n  - **Mean Test**: The sample mean is $\\bar{e} = (1.0 - 1.0)/12 = 0$. The $t$-test passes with a $p$-value of 1.0.\n  - **Autocorrelation Test**: The non-zero values are 11 lags apart. Since the maximum lag tested is $h=10$, no sample autocorrelation calculation will involve both non-zero points. All $\\hat{r}_k$ for $k \\in [1, 10]$ will be zero. The $Q$-statistic is 0, and the $p$-value is 1.0. The test passes.\n  - **Conclusion**: Both tests pass, so the series is classified as weak white noise. **Result: True**.\n\nThe final output is an ordered list of these boolean results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t, chi2\n\ndef is_weak_white_noise(e: np.ndarray, h: int, alpha: float) - bool:\n    \"\"\"\n    Tests if a time series behaves like weak white noise.\n\n    Args:\n        e: A numpy array representing the time series of residuals.\n        h: The maximum lag for the Ljung-Box test.\n        alpha: The significance level for the tests.\n\n    Returns:\n        True if the series is classified as weak white noise, False otherwise.\n    \"\"\"\n    n = len(e)\n    if n == 0:\n        return True # Convention for empty series\n\n    e_bar = np.mean(e)\n    sum_sq_dev = np.sum((e - e_bar)**2)\n\n    # Degenerate variance handling rule\n    if np.isclose(sum_sq_dev, 0):\n        # Variance is zero, so all elements are equal to the mean.\n        # It's white noise by convention only if the mean is also zero.\n        return np.isclose(e_bar, 0)\n\n    # 1. Mean-zero test (Student's t-test)\n    # Unbiased sample standard deviation\n    s = np.sqrt(sum_sq_dev / (n - 1))\n    \n    # Check for s=0 shouldn't be needed due to sum_sq_dev check, but for robustness:\n    if np.isclose(s, 0):\n        # This case is already covered by the sum_sq_dev check, but as a safeguard.\n        return np.isclose(e_bar, 0)\n        \n    t_statistic = e_bar / (s / np.sqrt(n))\n    df_t = n - 1\n    p_mean = t.sf(np.abs(t_statistic), df=df_t) * 2\n\n    # 2. Autocorrelation test (Ljung-Box)\n    h_star = min(h, n - 1)\n    \n    lb_sum = 0.0\n    for k in range(1, h_star + 1):\n        # Numerator: sum_{i=k to n-1} (e[i] - e_bar) * (e[i-k] - e_bar)\n        # using 0-indexed array 'e'\n        numerator = np.dot(e[k:] - e_bar, e[:-k] - e_bar)\n        r_k = numerator / sum_sq_dev\n        lb_sum += (r_k**2) / (n - k)\n\n    q_statistic = n * (n + 2) * lb_sum\n    df_q = h_star\n    \n    # Handle df_q=0 case (e.g., n=1, so h_star=0)\n    if df_q == 0:\n        p_lb = 1.0 # No autocorrelations to test\n    else:\n        p_lb = chi2.sf(q_statistic, df=df_q)\n\n    # 3. Decision rule\n    return p_mean = alpha and p_lb = alpha\n\ndef solve():\n    \"\"\"\n    Solves the problem by applying the white noise test to four specified cases.\n    \"\"\"\n    \n    # Case 1 (happy path: sparse uncorrelated residuals)\n    n1, h1, alpha1 = 60, 10, 0.05\n    e1 = np.zeros(n1)\n    e1[5] = 1.0   # e_6 in 1-based indexing\n    e1[50] = -1.0 # e_51 in 1-based indexing\n    \n    # Case 2 (autocorrelated leftovers from weekly pattern)\n    n2, h2, alpha2 = 56, 10, 0.05\n    t2 = np.arange(n2)\n    e2 = np.sin(2 * np.pi * t2 / 7)\n    \n    # Case 3 (misadjusted constant bias; boundary case with zero variance)\n    n3, h3, alpha3 = 30, 5, 0.05\n    e3 = np.full(n3, 0.3)\n    \n    # Case 4 (small sample, large h close to n; boundary on h)\n    n4, h4, alpha4 = 12, 10, 0.05\n    e4 = np.zeros(n4)\n    e4[0] = 1.0   # e_1 in 1-based indexing\n    e4[11] = -1.0 # e_12 in 1-based indexing\n\n    test_cases = [\n        (e1, h1, alpha1),\n        (e2, h2, alpha2),\n        (e3, h3, alpha3),\n        (e4, h4, alpha4),\n    ]\n\n    results = []\n    for e, h, alpha in test_cases:\n        result = is_weak_white_noise(e, h, alpha)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}