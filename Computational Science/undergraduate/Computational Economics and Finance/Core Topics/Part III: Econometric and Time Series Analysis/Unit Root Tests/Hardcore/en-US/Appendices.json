{
    "hands_on_practices": [
        {
            "introduction": "A core reason economists and financial analysts use unit root tests is to answer a practical question: are the effects of a shock temporary or permanent? This exercise puts this question into a concrete context by analyzing synthetic data on a game's player count following a content update. By applying the Augmented Dickey-Fuller (ADF) test, you will practice distinguishing between a stationary process, where a shock like an update causes a temporary spike before the series returns to its mean, and a non-stationary (unit root) process, where the shock leads to a permanent shift in the player base . This practice is fundamental to understanding how economic systems and financial assets absorb new information.",
            "id": "2445622",
            "problem": "Consider three synthetic time series representing daily concurrent player counts for a single online game. Each series includes a single content update at a specified day. For each series, determine whether the content update produces a permanent level change or a temporary spike by conducting a unit root test based on the Augmented Dickey–Fuller (ADF) framework.\n\nDefinitions and testing rule:\n- Let $\\{y_t\\}_{t=0}^{T}$ be a univariate time series. Define $\\Delta y_t \\equiv y_t - y_{t-1}$.\n- The ADF regression with an intercept (no deterministic trend) and exactly $p=2$ augmenting lags is\n$$\n\\Delta y_t \\;=\\; \\alpha \\;+\\; \\gamma \\, y_{t-1} \\;+\\; \\beta_1 \\, \\Delta y_{t-1} \\;+\\; \\beta_2 \\, \\Delta y_{t-2} \\;+\\; \\varepsilon_t,\\quad t=3,4,\\dots,T,\n$$\nwhere $\\varepsilon_t$ are regression disturbances.\n- The null hypothesis is the presence of a unit root: $H_0: \\gamma = 0$. The alternative is stationarity around a constant: $H_1: \\gamma < 0$.\n- Compute the ordinary least squares $t$-statistic for $\\gamma$ in the above regression and compare it to the Dickey–Fuller $5\\%$ critical value for the specification with intercept (no trend), which is $-2.86$. Reject $H_0$ if and only if the $t$-statistic is less than or equal to $-2.86$.\n- Classification rule for the content update: if $H_0$ is rejected, classify the update’s effect as temporary (the process is stationary and shocks decay); if $H_0$ is not rejected, classify the effect as permanent (the process has a unit root and shocks persist).\n\nData-generating processes (the test suite):\nAll innovations are independent and identically distributed Gaussian random variables. Use a fixed pseudo-random number generator seed equal to $12345$ for reproducibility. For each case below, take $y_0$ as specified and generate $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independently for each $t$.\n\n- Case A (random walk with drift and a permanent update jump):\n  - Parameters: $T = 500$, $y_0 = 1000$, $\\mu = 0.2$, $\\sigma = 5$, $\\Delta = 50$, update day $\\tau = 300$.\n  - Recursion for $t=1,2,\\dots,T$:\n    $$\n    y_t \\;=\\; y_{t-1} \\;+\\; \\mu \\;+\\; \\varepsilon_t \\;+\\; \\Delta \\cdot \\mathbf{1}\\{t=\\tau\\}.\n    $$\n- Case B (stationary autoregression with an impulse update shock):\n  - Parameters: $T = 500$, $y_0 = 1000$, $\\phi = 0.6$, target mean $\\bar{y}=1000$, hence intercept $c = \\bar{y}\\,(1-\\phi) = 400$, $\\sigma = 10$, impulse size $A=120$, update day $\\tau = 300$.\n  - Recursion for $t=1,2,\\dots,T$:\n    $$\n    y_t \\;=\\; c \\;+\\; \\phi\\, y_{t-1} \\;+\\; \\varepsilon_t \\;+\\; A \\cdot \\mathbf{1}\\{t=\\tau\\}.\n    $$\n- Case C (highly persistent but stationary autoregression with an impulse update shock):\n  - Parameters: $T = 1200$, $y_0 = 1000$, $\\phi = 0.9$, target mean $\\bar{y}=1000$, hence intercept $c = \\bar{y}\\,(1-\\phi) = 100$, $\\sigma = 5$, impulse size $A=100$, update day $\\tau = 800$.\n  - Recursion for $t=1,2,\\dots,T$:\n    $$\n    y_t \\;=\\; c \\;+\\; \\phi\\, y_{t-1} \\;+\\; \\varepsilon_t \\;+\\; A \\cdot \\mathbf{1}\\{t=\\tau\\}.\n    $$\n\nTasks:\n- For each of the three cases, generate the series using the specified parameters and seed, compute the ADF $t$-statistic as defined above with exactly $p=2$ augmenting lags and an intercept, perform the hypothesis test at significance level $\\alpha = 0.05$ using the critical value $-2.86$, and classify the update’s effect as permanent or temporary according to the rule stated above.\n\nRequired final output format:\n- Your program should produce a single line of output containing the three classification results in order $\\text{[Case A, Case B, Case C]}$, formatted as a Python-style list of booleans with no spaces, for example $\\text{[True,False,True]}$, where $\\text{True}$ denotes a permanent effect and $\\text{False}$ denotes a temporary effect.",
            "solution": "The posed problem is subjected to validation.\n\n### Step 1: Extract Givens\nThe problem provides the following data, definitions, and rules:\n- A univariate time series $\\{y_t\\}_{t=0}^{T}$.\n- The first difference operator: $\\Delta y_t \\equiv y_t - y_{t-1}$.\n- The Augmented Dickey–Fuller (ADF) regression model with an intercept and $p=2$ lags:\n$$\n\\Delta y_t \\;=\\; \\alpha \\;+\\; \\gamma \\, y_{t-1} \\;+\\; \\beta_1 \\, \\Delta y_{t-1} \\;+\\; \\beta_2 \\, \\Delta y_{t-2} \\;+\\; \\varepsilon_t,\\quad t=3,4,\\dots,T.\n$$\n- The null hypothesis of a unit root is $H_0: \\gamma = 0$.\n- The alternative hypothesis of stationarity is $H_1: \\gamma < 0$.\n- The decision rule is to reject $H_0$ at the $5\\%$ significance level if the ordinary least squares (OLS) $t$-statistic for $\\gamma$ is less than or equal to the critical value of $-2.86$.\n- A classification rule is defined: if $H_0$ is not rejected, the effect is 'permanent'; if $H_0$ is rejected, the effect is 'temporary'.\n- A fixed pseudo-random number generator seed of $12345$ must be used.\n- Three cases with specific data-generating processes (DGPs) are specified:\n    - Case A: $y_t = y_{t-1} + \\mu + \\varepsilon_t + \\Delta \\cdot \\mathbf{1}\\{t=\\tau\\}$, with $T = 500$, $y_0 = 1000$, $\\mu = 0.2$, $\\sigma = 5$, $\\Delta = 50$, $\\tau = 300$.\n    - Case B: $y_t = c + \\phi y_{t-1} + \\varepsilon_t + A \\cdot \\mathbf{1}\\{t=\\tau\\}$, with $T = 500$, $y_0 = 1000$, $\\phi = 0.6$, $c = 400$, $\\sigma = 10$, $A = 120$, $\\tau = 300$.\n    - Case C: $y_t = c + \\phi y_{t-1} + \\varepsilon_t + A \\cdot \\mathbf{1}\\{t=\\tau\\}$, with $T = 1200$, $y_0 = 1000$, $\\phi = 0.9$, $c = 100$, $\\sigma = 5$, $A = 100$, $\\tau = 800$.\n- The required output is a boolean list `[Case A result, Case B result, Case C result]`, where `True` signifies a 'permanent' effect and `False` signifies a 'temporary' effect.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the specified validation criteria.\n- **Scientifically Grounded**: The problem is based on standard, well-established principles of time series econometrics, specifically the theory of unit root processes and the application of the Augmented Dickey-Fuller test. The DGPs represent canonical models: a random walk with drift (non-stationary) and autoregressive processes (stationary).\n- **Well-Posed**: The problem is fully specified. All necessary parameters for data generation ($T, y_0, \\sigma$, etc.) and for the ADF test ($p=2$, intercept-only model, critical value) are provided. The task is a direct computational procedure with a clear, unique outcome for a given random seed.\n- **Objective**: The problem is stated in precise, quantitative terms, free from any subjective or ambiguous language.\n- **Other Flaws**: The problem setup is internally consistent. For example, in Case B, the intercept $c$ is consistent with the target mean $\\bar{y}$ and autoregressive parameter $\\phi$: $c = \\bar{y}(1-\\phi) = 1000(1-0.6) = 400$. Similarly, for Case C, $c = 1000(1-0.9) = 100$. There are no contradictions, missing information, or scientifically unsound premises.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined computational task in econometrics. A solution will be provided.\n\n### Solution Derivation\nThe task is to classify the effect of a content update as 'permanent' or 'temporary' for three synthetic time series. The classification depends on the outcome of an Augmented Dickey–Fuller (ADF) unit root test. The procedure for each case is as follows.\n\n1.  **Time Series Generation**: For each case, a time series $\\{y_t\\}_{t=0}^T$ of length $T+1$ is generated according to its specified data-generating process (DGP). A distinct set of pseudo-random Gaussian innovations, $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, is generated for each series, starting from a reset seed of $12345$.\n\n2.  **ADF Regression Setup**: The ADF test regression is specified as:\n    $$\n    \\Delta y_t = \\alpha + \\gamma y_{t-1} + \\beta_1 \\Delta y_{t-1} + \\beta_2 \\Delta y_{t-2} + \\varepsilon_t\n    $$\n    The regression is performed for $t = 3, 4, \\dots, T$. This yields $N = T-2$ observations for the regression. Let us define the vector of observations for the dependent variable as $Y = [\\Delta y_3, \\Delta y_4, \\dots, \\Delta y_T]^T$. The design matrix $X$ of independent variables is constructed with $N$ rows and $k=4$ columns:\n    $$\n    X = \\begin{bmatrix}\n    1 & y_2 & \\Delta y_2 & \\Delta y_1 \\\\\n    1 & y_3 & \\Delta y_3 & \\Delta y_2 \\\\\n    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n    1 & y_{T-1} & \\Delta y_{T-1} & \\Delta y_{T-2}\n    \\end{bmatrix}\n    $$\n    The model can be written in matrix form as $Y = X \\mathbf{\\theta} + \\mathbf{\\varepsilon}$, where $\\mathbf{\\theta} = [\\alpha, \\gamma, \\beta_1, \\beta_2]^T$.\n\n3.  **OLS Estimation and t-Statistic Calculation**: The vector of regression coefficients $\\hat{\\mathbf{\\theta}}$ is estimated using Ordinary Least Squares (OLS):\n    $$\n    \\hat{\\mathbf{\\theta}} = (X^T X)^{-1} X^T Y\n    $$\n    The coefficient of interest is $\\hat{\\gamma}$, which is the second element of $\\hat{\\mathbf{\\theta}}$. To compute its $t$-statistic, we first need its standard error, $\\text{SE}(\\hat{\\gamma})$.\n    The variance of the regression error, $\\sigma^2_\\varepsilon$, is estimated from the sum of squared residuals (SSR):\n    $$\n    \\hat{\\sigma}^2_\\varepsilon = \\frac{\\text{SSR}}{N-k} = \\frac{(Y-X\\hat{\\mathbf{\\theta}})^T(Y-X\\hat{\\mathbf{\\theta}})}{N-k}\n    $$\n    The estimated covariance matrix of the coefficient vector is:\n    $$\n    \\widehat{\\text{Cov}}(\\hat{\\mathbf{\\theta}}) = \\hat{\\sigma}^2_\\varepsilon (X^T X)^{-1}\n    $$\n    The variance of $\\hat{\\gamma}$ is the second diagonal element (index $1,1$) of this matrix, let us call it $\\widehat{\\text{Var}}(\\hat{\\gamma})$. The standard error is its square root:\n    $$\n    \\text{SE}(\\hat{\\gamma}) = \\sqrt{\\widehat{\\text{Var}}(\\hat{\\gamma})}\n    $$\n    The $t$-statistic for $\\gamma$ is then computed as:\n    $$\n    t_{\\gamma} = \\frac{\\hat{\\gamma}}{\\text{SE}(\\hat{\\gamma})}\n    $$\n\n4.  **Hypothesis Test and Classification**: The computed $t$-statistic $t_{\\gamma}$ is compared against the provided $5\\%$ critical value, $c_{0.05} = -2.86$.\n    - If $t_{\\gamma} \\le -2.86$, the null hypothesis $H_0: \\gamma=0$ is rejected. The test suggests the series is stationary. According to the problem's rule, the update effect is classified as 'temporary', which corresponds to a boolean value of `False`.\n    - If $t_{\\gamma} > -2.86$, we fail to reject the null hypothesis. The test does not provide sufficient evidence against the presence of a unit root. The effect is classified as 'permanent', corresponding to a boolean value of `True`.\n\nThis entire procedure is implemented computationally and applied to each of the three defined cases.\n\n- **Case A** represents a random walk, which is a process with a unit root by definition ($\\gamma=0$). We expect the test to fail to reject the null, resulting in a 'permanent' classification (`True`).\n- **Case B** is a stationary AR(1) process with $\\phi=0.6$, far from the unit root boundary. The true $\\gamma = \\phi - 1 = -0.4$. The test should have high power to reject the null, leading to a 'temporary' classification (`False`).\n- **Case C** is a stationary AR(1) process with $\\phi=0.9$, which is highly persistent and close to a unit root. The true $\\gamma = \\phi - 1 = -0.1$. ADF tests are known to have low power in such cases, meaning they often fail to reject the null hypothesis even when it is false. It is plausible that the test will classify this process as having a unit root, leading to a 'permanent' classification (`True`).\n\nThe final output will be a list of these three boolean outcomes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_adf_test(params):\n    \"\"\"\n    Generates a time series based on a given DGP and performs an ADF test.\n\n    Args:\n        params (dict): A dictionary containing parameters for the DGP and the test.\n                       Keys: 'T', 'y0', 'sigma', 'tau', 'type', and other DGP-specific params.\n\n    Returns:\n        bool: True if the effect is classified as 'permanent', False otherwise.\n    \"\"\"\n    # Unpack parameters\n    T = params['T']\n    y0 = params['y0']\n    sigma = params['sigma']\n    tau = params['tau']\n    dgp_type = params['type']\n\n    # Set seed for reproducibility for this specific case\n    rng = np.random.default_rng(12345)\n    \n    # Generate random innovations\n    eps = rng.normal(0, sigma, size=T)\n\n    # Generate the time series y from t=0 to T\n    y = np.zeros(T + 1)\n    y[0] = y0\n\n    if dgp_type == 'A':\n        mu = params['mu']\n        delta = params['delta']\n        for t in range(1, T + 1):\n            jump = delta if t == tau else 0\n            y[t] = y[t-1] + mu + eps[t-1] + jump\n    elif dgp_type in ['B', 'C']:\n        c = params['c']\n        phi = params['phi']\n        A = params['A']\n        for t in range(1, T + 1):\n            impulse = A if t == tau else 0\n            y[t] = c + phi * y[t-1] + eps[t-1] + impulse\n            \n    # Prepare data for ADF regression\n    # The regression uses observations from t=3 to T\n    # Total observations in regression: N = T - 2\n    \n    # First differences: delta_y_t = y_t - y_{t-1} for t=1,...,T\n    delta_y = y[1:] - y[:-1]\n\n    # Dependent variable: Y = [delta_y_3, ..., delta_y_T]\n    # Corresponds to delta_y[2:]\n    Y = delta_y[2:]\n    \n    # Number of observations and parameters in the regression\n    N = T - 2\n    k = 4 # alpha, gamma, beta1, beta2\n\n    # Design matrix X\n    # Columns: intercept, y_{t-1}, delta_y_{t-1}, delta_y_{t-2}\n    # For t=3..T, this corresponds to:\n    # y[2:-1] for y_{t-1}\n    # delta_y[1:-1] for delta_y_{t-1}\n    # delta_y[0:-2] for delta_y_{t-2}\n    X = np.zeros((N, k))\n    X[:, 0] = 1.0  # Intercept alpha\n    X[:, 1] = y[2:-1]  # Lagged level y_{t-1}\n    X[:, 2] = delta_y[1:-1]  # Lagged difference delta_y_{t-1}\n    X[:, 3] = delta_y[0:-2]  # Second lagged difference delta_y_{t-2}\n\n    # Perform OLS regression to find coefficients theta_hat\n    # theta_hat = (X.T @ X)^-1 @ X.T @ Y\n    # Using np.linalg.lstsq is more numerically stable\n    coeffs, ssr_array, _, _ = np.linalg.lstsq(X, Y, rcond=None)\n    \n    gamma_hat = coeffs[1]\n    \n    # Calculate t-statistic for gamma\n    if ssr_array.size == 0:\n        # This case happens if the model is a perfect fit, which is highly unlikely\n        # with random data. Return a default to avoid crashing.\n        return True\n\n    ssr = ssr_array[0]\n    dof = N - k\n    sigma_eps_sq_hat = ssr / dof\n\n    # Covariance matrix of coefficients\n    try:\n        X_T_X_inv = np.linalg.inv(X.T @ X)\n    except np.linalg.LinAlgError:\n        # In case of perfect multicollinearity\n        return True # Cannot reject H0 if test cannot be computed\n\n    var_gamma_hat = sigma_eps_sq_hat * X_T_X_inv[1, 1]\n    se_gamma_hat = np.sqrt(var_gamma_hat)\n\n    if se_gamma_hat == 0:\n        return True # Avoid division by zero, non-rejection is a safe default\n\n    t_statistic = gamma_hat / se_gamma_hat\n    \n    # Critical value from the problem\n    critical_value = -2.86\n    \n    # Decision rule\n    # H0 not rejected -> permanent -> True\n    # H0 rejected -> temporary -> False\n    return t_statistic > critical_value\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all three cases and print the result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'type': 'A', 'T': 500, 'y0': 1000, 'mu': 0.2, 'sigma': 5,\n            'delta': 50, 'tau': 300\n        },\n        {\n            'type': 'B', 'T': 500, 'y0': 1000, 'phi': 0.6, 'c': 400,\n            'sigma': 10, 'A': 120, 'tau': 300\n        },\n        {\n            'type': 'C', 'T': 1200, 'y0': 1000, 'phi': 0.9, 'c': 100,\n            'sigma': 5, 'A': 100, 'tau': 800\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        is_permanent = run_adf_test(case)\n        results.append(is_permanent)\n\n    # Format output as specified: [True,False,True]\n    result_str = f\"[{','.join(map(str, results))}]\"\n    print(result_str)\n\nsolve()\n```"
        },
        {
            "introduction": "After learning to apply a statistical tool, it is crucial to understand its limitations. Unit root tests are designed under the assumption that the data comes from a linear stochastic process. This practice explores a fascinating and important pitfall: what happens when we apply this test to a series that is deterministic but highly complex and chaotic? Using the famous logistic map to generate data, you will investigate how a standard Dickey-Fuller test can misclassify a stationary, deterministic process as a non-stationary random walk . This exercise provides a powerful lesson in critically evaluating whether your tools are appropriate for your data.",
            "id": "2433700",
            "problem": "A deterministic chaotic sequence can be produced by the logistic map. Let the logistic map be defined by $x_{t+1} = r\\,x_t\\,(1-x_t)$ for $t \\ge 0$ with parameter $r \\in (0,4]$ and initial condition $x_0 \\in (0,1)$. Consider the question of stationarity versus non-stationarity for series derived from this map when evaluated using a unit root test.\n\nDefine the following formal hypothesis test for a given univariate time series $\\{y_t\\}_{t=1}^T$ with sample size $T \\ge 3$. Construct the first difference $\\Delta y_t = y_t - y_{t-1}$ for $t=2,\\dots,T$, and consider the linear model\n$$\n\\Delta y_t = \\alpha + \\gamma\\,y_{t-1} + \\varepsilon_t \\quad \\text{for } t=2,\\dots,T,\n$$\nwhere $\\alpha$ and $\\gamma$ are unknown constants and $\\varepsilon_t$ is an unobserved disturbance. Let $\\widehat{\\gamma}$ be the ordinary least squares estimator of $\\gamma$ and let $\\operatorname{se}(\\widehat{\\gamma})$ be its ordinary least squares standard error computed from the same regression. Define the Dickey–Fuller statistic\n$$\n\\tau = \\frac{\\widehat{\\gamma}}{\\operatorname{se}(\\widehat{\\gamma})}.\n$$\nAt significance level $\\alpha_{\\text{test}} = 0.05$, use the intercept-only Dickey–Fuller $5\\%$ critical value $c_{0.05} = -2.86$. The decision rule is: reject the unit root null hypothesis if and only if $\\tau < c_{0.05}$; otherwise, do not reject. In this problem, report the boolean classification “unit root” for each case as follows: return $\\text{True}$ if the null is not rejected (interpreted as “classified as unit root”) and $\\text{False}$ if the null is rejected (interpreted as “classified as stationary in levels”).\n\nConstruct series and apply the test for the following test suite. In all cases, use exactly the specified parameter values and sample sizes, and do not apply any pre-filtering beyond what is explicitly defined.\n\n- Case A (deterministic chaotic, levels): logistic map with $r=4$, $x_0=0.123456789$, produce $\\{x_t\\}_{t=1}^T$ with $T=2000$, and set $y_t = x_t$.\n\n- Case B (deterministic chaotic, integrated of centered levels): logistic map with $r=4$, $x_0=0.123456789$, produce $\\{x_t\\}_{t=1}^T$ with $T=2000$, define $z_t = x_t - \\tfrac{1}{2}$ for $t=1,\\dots,T$, and set $y_t = \\sum_{i=1}^t z_i$ for $t=1,\\dots,T$.\n\n- Case C (stochastic unit root, random walk): define $y_1 = 0$ and $y_t = y_{t-1} + \\varepsilon_t$ for $t=2,\\dots,T$ with $T=2000$, where $\\{\\varepsilon_t\\}$ are independent and identically distributed normal random variables with mean $0$ and variance $1$, generated using a pseudo-random number generator seeded at $s = 314159$.\n\n- Case D (boundary condition, short chaotic sample): logistic map with $r=4$, $x_0=0.123456789$, produce $\\{x_t\\}_{t=1}^T$ with $T=64$, and set $y_t = x_t$.\n\nYour program must implement the test statistic and the decision rule exactly as stated and compute the resulting boolean for each of the four cases in the order A, B, C, D. The required final output format is a single line containing a comma-separated list of the four booleans enclosed in square brackets, for example, $[\\text{False},\\text{True},\\text{True},\\text{False}]$. No additional text should be printed.",
            "solution": "The problem statement has been rigorously evaluated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It presents a standard exercise in computational econometrics, investigating the behavior of a unit root test on both stochastic and deterministic time series. We will now proceed with the formal solution.\n\nThe problem requires the implementation of a simplified Dickey-Fuller test to classify four different time series as either possessing a unit root ('non-stationary', `True`) or not ('stationary', `False`). The decision is based on a fixed critical value.\n\nThe core of the test is the Ordinary Least Squares (OLS) estimation of the model:\n$$\n\\Delta y_t = \\alpha + \\gamma\\,y_{t-1} + \\varepsilon_t\n$$\nfor the time indices $t=2, \\dots, T$. Here, $\\{y_t\\}_{t=1}^T$ is the time series of interest with sample size $T$, and $\\Delta y_t = y_t - y_{t-1}$ is its first difference. Let $N_{reg} = T-1$ be the number of observations available for the regression.\n\nWe can express this regression in matrix form as $\\mathbf{Y}_{\\text{reg}} = \\mathbf{X}_{\\text{reg}}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where:\n- $\\mathbf{Y}_{\\text{reg}} = (\\Delta y_2, \\Delta y_3, \\dots, \\Delta y_T)^T$ is the $(N_{reg} \\times 1)$ vector of the dependent variable.\n- $\\mathbf{X}_{\\text{reg}}$ is the $(N_{reg} \\times 2)$ design matrix, whose first column is a vector of ones (for the intercept $\\alpha$) and whose second column is the vector of lagged levels $(y_1, y_2, \\dots, y_{T-1})^T$.\n- $\\boldsymbol{\\beta} = (\\alpha, \\gamma)^T$ is the $(2 \\times 1)$ vector of parameters to be estimated.\n- $\\boldsymbol{\\varepsilon} = (\\varepsilon_2, \\varepsilon_3, \\dots, \\varepsilon_T)^T$ is the vector of unobserved disturbances.\n\nThe OLS estimator for $\\boldsymbol{\\beta}$ is given by the standard formula:\n$$\n\\widehat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\widehat{\\alpha} \\\\ \\widehat{\\gamma} \\end{pmatrix} = (\\mathbf{X}_{\\text{reg}}^T \\mathbf{X}_{\\text{reg}})^{-1} \\mathbf{X}_{\\text{reg}}^T \\mathbf{Y}_{\\text{reg}}\n$$\nWe are interested in the estimate $\\widehat{\\gamma}$. The next step is to compute its standard error, $\\operatorname{se}(\\widehat{\\gamma})$. First, we calculate the regression residuals $\\widehat{\\varepsilon}_t = \\Delta y_t - (\\widehat{\\alpha} + \\widehat{\\gamma}y_{t-1})$ and estimate the variance of the disturbance term:\n$$\n\\widehat{\\sigma}^2 = \\frac{1}{N_{reg} - p} \\sum_{t=2}^T \\widehat{\\varepsilon}_t^2 = \\frac{1}{T-3} \\sum_{t=2}^T \\widehat{\\varepsilon}_t^2\n$$\nwhere $p=2$ is the number of estimated parameters ($\\alpha$ and $\\gamma$). The variance-covariance matrix of the estimated coefficients is:\n$$\n\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}}) = \\widehat{\\sigma}^2 (\\mathbf{X}_{\\text{reg}}^T \\mathbf{X}_{\\text{reg}})^{-1}\n$$\nThe variance of $\\widehat{\\gamma}$ is the second diagonal element of this matrix. The standard error is its square root:\n$$\n\\operatorname{se}(\\widehat{\\gamma}) = \\sqrt{\\left[\\widehat{\\operatorname{Var}}(\\widehat{\\boldsymbol{\\beta}})\\right]_{2,2}}\n$$\nFinally, the Dickey-Fuller statistic is the t-ratio for $\\widehat{\\gamma}$:\n$$\n\\tau = \\frac{\\widehat{\\gamma}}{\\operatorname{se}(\\widehat{\\gamma})}\n$$\nThe null hypothesis is that the series has a unit root ($\\gamma=0$ in the underlying data generating process $y_t = y_{t-1} + \\dots$). The decision rule is to reject this null hypothesis if $\\tau < c_{0.05} = -2.86$. A rejection implies the series is stationary, yielding a result of `False`. Failure to reject implies the series is classified as having a unit root, yielding `True`.\n\nWe now apply this procedure to each of the four specified cases.\n\n**Case A: Deterministic chaotic, levels**\nThe series is $\\{y_t\\}_{t=1}^{2000}$ generated by the logistic map $x_{t+1} = 4x_t(1-x_t)$ with $x_0=0.123456789$. For $r=4$, the logistic map is ergodic and produces a chaotic but stationary sequence bounded within $(0,1)$. A correctly specified test with sufficient data should identify the series as stationary. With $T=2000$, the sample size is large, and we expect the test to have high power. Therefore, we anticipate $\\tau < -2.86$, leading to a rejection of the unit root null hypothesis. The classification should be `False`.\n\n**Case B: Deterministic chaotic, integrated of centered levels**\nThis series is constructed by taking the cumulative sum of a centered chaotic sequence, $y_t = \\sum_{i=1}^t z_i$, where $z_t = x_t - 0.5$. The series $\\{z_t\\}$ is stationary with a mean of zero. The process of cumulation, or integration, induces non-stationarity. This construction is a deterministic analogue of a stochastic random walk. The Dickey-Fuller test is designed to detect such integrated processes. We expect the test to fail to reject the null hypothesis, correctly identifying the non-stationary nature of the series. The classification should be `True`.\n\n**Case C: Stochastic unit root, random walk**\nThe series is a canonical random walk: $y_t = y_{t-1} + \\varepsilon_t$ with $y_1=0$ and $\\varepsilon_t \\sim \\text{i.i.d. } N(0,1)$. This is the archetypal process for which the Dickey-Fuller test was designed. By construction, it contains a unit root. The test should, with high probability, fail to reject the null hypothesis. The classification should be `True`.\n\n**Case D: Boundary condition, short chaotic sample**\nThis series is generated by the same stationary chaotic process as in Case A, but the sample size is severely limited to $T=64$. It is a well-known property of the Dickey-Fuller test that its statistical power is low in small samples. This means the test often fails to distinguish a stationary process with high persistence from a true unit root process when data is scarce. Chaotic series can exhibit strong short-term correlations that mimic the behavior of a random walk. Consequently, it is plausible, and indeed expected, that the test will fail to reject the null hypothesis, thus misclassifying the stationary chaotic series as a non-stationary unit root process. The classification should be `True`.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Dickey-Fuller test statistic and classification for four specified time series.\n    \"\"\"\n\n    def compute_dickey_fuller_tau(y_series: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Dickey-Fuller tau statistic for a given time series.\n        The regression model is Delta(y_t) = alpha + gamma * y_{t-1} + eps_t.\n\n        Args:\n            y_series: A 1D numpy array representing the time series {y_t}.\n\n        Returns:\n            The calculated tau statistic.\n        \"\"\"\n        T = len(y_series)\n        if T < 3:\n            raise ValueError(\"Series must have at least 3 observations for the test.\")\n\n        # Construct variables for regression\n        # Y_reg = Delta(y_t) for t=2,...,T\n        # X_reg = [1, y_{t-1}] for t=2,...,T\n        y_reg = y_series[1:] - y_series[:-1]  # Delta(y_t)\n        x_lagged = y_series[:-1]  # y_{t-1}\n\n        N_reg = T - 1  # Number of observations in the regression\n        \n        # Design matrix includes a constant and the lagged variable\n        X_reg = np.vstack((np.ones(N_reg), x_lagged)).T\n\n        # OLS estimation: beta_hat = (X'X)^-1 * X'Y\n        try:\n            XTX_inv = np.linalg.inv(X_reg.T @ X_reg)\n        except np.linalg.LinAlgError:\n            # This would happen if X'X is singular, e.g., if y_lagged is constant.\n            # For the given problems, this is highly unlikely.\n            return np.nan\n\n        beta_hat = XTX_inv @ X_reg.T @ y_reg\n        gamma_hat = beta_hat[1]\n\n        # Calculate standard error of gamma_hat\n        residuals = y_reg - X_reg @ beta_hat\n        p = 2  # Number of parameters (alpha, gamma)\n        df = N_reg - p\n        if df <= 0:\n            return np.nan\n\n        sigma2_hat = np.sum(residuals**2) / df\n        \n        # Variance of gamma_hat is sigma2_hat * (2,2)-element of (X'X)^-1\n        var_gamma_hat = sigma2_hat * XTX_inv[1, 1]\n        se_gamma_hat = np.sqrt(var_gamma_hat)\n        \n        if se_gamma_hat == 0:\n            # Avoid division by zero\n            return np.inf if gamma_hat != 0 else np.nan\n\n        tau_statistic = gamma_hat / se_gamma_hat\n        return tau_statistic\n\n    def generate_logistic_map_series(r, x0, T):\n        \"\"\"Generates a series from the logistic map.\"\"\"\n        series = np.zeros(T)\n        x_current = x0\n        for i in range(T):\n            x_current = r * x_current * (1 - x_current)\n            series[i] = x_current\n        return series\n\n    # Test parameters\n    r_val = 4.0\n    x0_val = 0.123456789\n    c_critical = -2.86\n    prng_seed = 314159\n\n    results = []\n\n    # Case A\n    T_A = 2000\n    y_A = generate_logistic_map_series(r_val, x0_val, T_A)\n    tau_A = compute_dickey_fuller_tau(y_A)\n    results.append(tau_A >= c_critical)\n    \n    # Case B\n    T_B = 2000\n    x_B = generate_logistic_map_series(r_val, x0_val, T_B)\n    z_B = x_B - 0.5\n    y_B = np.cumsum(z_B)\n    tau_B = compute_dickey_fuller_tau(y_B)\n    results.append(tau_B >= c_critical)\n    \n    # Case C\n    T_C = 2000\n    rng = np.random.default_rng(prng_seed)\n    # Generate T-1 shocks for t=2...T\n    epsilon_shocks = rng.normal(loc=0.0, scale=1.0, size=T_C - 1)\n    # y_1 = 0, y_t = y_{t-1} + eps_t for t>=2\n    # y_t = sum_{i=2 to t} eps_i\n    y_C = np.concatenate(([0.0], np.cumsum(epsilon_shocks)))\n    tau_C = compute_dickey_fuller_tau(y_C)\n    results.append(tau_C >= c_critical)\n    \n    # Case D\n    T_D = 64\n    y_D = generate_logistic_map_series(r_val, x0_val, T_D)\n    tau_D = compute_dickey_fuller_tau(y_D)\n    results.append(tau_D >= c_critical)\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having applied the ADF test and explored its conceptual boundaries, the final step in mastering this tool is to build it from first principles. This advanced practice guides you through the complete, from-scratch implementation of a robust ADF testing procedure. You will move beyond using fixed parameters by learning to automatically select the optimal number of lags using the Akaike Information Criterion (AIC) and to approximate the test's critical values using a parametric bootstrap . Completing this exercise will solidify a deep, procedural understanding of how unit root tests are constructed and function in a real-world computational environment.",
            "id": "2445580",
            "problem": "Write a complete, runnable program that implements a unit root test using the Augmented Dickey-Fuller (ADF) methodology to determine whether a univariate time series is stable (integrated of order zero, $I(0)$) or unstable (integrated of order one, $I(1)$). The task must be solved from first principles using foundational time-series definitions and standard computational steps, without relying on prepackaged unit root test functions. Your program must decide, for each of several provided synthetic time series, whether to reject the unit root null hypothesis at significance level $\\alpha = 0.05$ and then report whether the series is $I(0)$ (reject) or $I(1)$ (do not reject).\n\nFundamental base and definitions to be used:\n- A univariate process $\\{y_t\\}$ is $I(0)$ if it is weakly stationary with finite, time-invariant mean and autocovariances. A process is $I(1)$ if $\\Delta y_t = y_t - y_{t-1}$ is $I(0)$ but $y_t$ itself is not $I(0)$.\n- The Augmented Dickey-Fuller (ADF) test is based on ordinary least squares regression of the first difference $\\Delta y_t$ on a lagged level $y_{t-1}$, a finite number of lagged differences $\\Delta y_{t-i}$ for $i \\in \\{1,\\dots,p\\}$ to absorb short-run serial correlation, and allowed deterministic regressors (an intercept and optionally a linear time trend). The test statistic is the usual ratio of the estimated coefficient on $y_{t-1}$ to its estimated standard error. Because the null is a unit root, the statistic has a nonstandard distribution whose critical values depend on which deterministic regressors are included.\n- To preserve scientific realism without invoking pre-tabulated critical values, approximate the null distribution of the ADF statistic via a parametric bootstrap under the unit-root null with the same sample size and the same set of deterministic regressors.\n\nRequired algorithmic choices and constraints:\n- Always include an intercept in the ADF regression. Include a linear trend regressor only when specified by the test case.\n- Select the lag order $p$ by the Akaike Information Criterion (AIC) over the set $\\{0,1,\\dots,p_{\\max}\\}$, where $p_{\\max} = \\left\\lfloor 12\\,(T/100)^{1/4}\\right\\rfloor$ and $T$ is the sample size. Break ties by choosing the smallest $p$.\n- Estimate each regression by ordinary least squares and compute the test statistic for the coefficient on $y_{t-1}$ as the estimated coefficient divided by its estimated standard error.\n- Approximate the left-tail critical value at level $\\alpha = 0.05$ using a bootstrap with $B = 1200$ replications under the unit-root null. Under the null, simulate $y_t$ as a random walk initialized at $0$ with independent and identically distributed Gaussian innovations with zero mean and unit variance, and include the same deterministic regressors (intercept, and trend if required by the case) in the ADF regression. Use the $p$ selected on the original series for all bootstrap replications in that case.\n- Reject the null (classify as $I(0)$) if the observed ADF test statistic is less than or equal to the bootstrapped $\\alpha$-quantile; otherwise, do not reject (classify as $I(1)$).\n\nTest suite to implement and evaluate:\nGenerate the following four synthetic time series, each of length $T = 400$, using independent and identically distributed Gaussian innovations with variance $1$ and the seeds indicated for reproducibility. For each series, specify whether to include a linear trend regressor in the ADF regression.\n\n- Case $1$ (stable autoregressive): $y_t = \\phi y_{t-1} + \\varepsilon_t$ with $\\phi = 0.6$, $y_0 = 0$, no deterministic trend in the data-generating process, intercept included in the ADF regression, no trend in the ADF regression. Use seed $s_1 = 202405$.\n- Case $2$ (random walk with drift): $y_t = y_{t-1} + \\delta + \\varepsilon_t$ with $\\delta = 0.1$, $y_0 = 0$, intercept included in the ADF regression, no trend in the ADF regression. Use seed $s_2 = 202406$.\n- Case $3$ (trend-stationary autoregressive): $y_t = \\beta t + u_t$ where $u_t = \\phi u_{t-1} + \\varepsilon_t$ with $\\phi = 0.7$, $\\beta = 0.05$, $u_0 = 0$, intercept and linear trend included in the ADF regression. Use seed $s_3 = 202407$.\n- Case $4$ (unit root with moving-average increment): $y_t = y_{t-1} + v_t$ with $v_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}$ and $\\theta = 0.5$, $y_0=0$, $\\varepsilon_{-1}=0$, intercept included in the ADF regression, no trend in the ADF regression. Use seed $s_4 = 202408$.\n\nImplementation details to enforce:\n- In all cases, the innovation sequence $\\{\\varepsilon_t\\}$ is Gaussian with mean $0$ and variance $1$.\n- The bootstrap must use a fixed seed so that results are reproducible.\n- The output per case is a Boolean: output $\\text{True}$ if the series is classified as $I(0)$ (reject the unit root null at $\\alpha = 0.05$), and $\\text{False}$ if classified as $I(1)$ (do not reject).\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of Booleans in the order of Cases $1,2,3,4$, with elements comma-separated and enclosed in square brackets, for example, a line of the form \"[True,False,True,False]\".",
            "solution": "The problem requires the implementation of the Augmented Dickey-Fuller (ADF) test from first principles to classify several synthetic time series as either integrated of order zero, $I(0)$, or integrated of order one, $I(1)$. The process involves regression estimation, model selection, and hypothesis testing using a parametric bootstrap for critical value estimation.\n\nThe problem statement is evaluated and found to be valid. It is scientifically grounded in established time-series econometrics, well-posed with a complete and consistent set of specifications, and expressed in objective, formal language. All necessary parameters, data-generating processes, and algorithmic choices are provided, enabling the construction of a unique and verifiable computational solution. We may therefore proceed with the solution.\n\nThe solution methodology comprises the following steps:\n\n1.  **Formulation of the Augmented Dickey-Fuller Regression**\n    The ADF test investigates the null hypothesis of a unit root in a time series $\\{y_t\\}$. This is done by estimating a regression model. For a chosen lag order $p$, the model is:\n    $$ \\Delta y_t = \\gamma y_{t-1} + \\sum_{i=1}^p \\beta_i \\Delta y_{t-i} + \\mathbf{d}_t^T \\boldsymbol{\\delta} + u_t $$\n    where $\\Delta y_t = y_t - y_{t-1}$ is the first difference of the series, $y_{t-1}$ is the lagged level, $\\Delta y_{t-i}$ are lagged differences to account for serial correlation in the error term $u_t$, and $\\mathbf{d}_t$ is a vector of deterministic terms (e.g., a constant and/or a linear time trend). The key parameter is $\\gamma$. The null hypothesis of a unit root corresponds to $H_0: \\gamma = 0$, against the one-sided alternative hypothesis of stationarity, $H_a: \\gamma < 0$.\n\n2.  **Ordinary Least Squares (OLS) Estimation and Test Statistic**\n    The regression model can be expressed in matrix form as $\\mathbf{z} = \\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{u}$, where $\\mathbf{z}$ is the vector of observations on the dependent variable $\\Delta y_t$, $\\mathbf{X}$ is the matrix of observations on all regressors ($y_{t-1}$, lagged differences, and deterministic terms), and $\\boldsymbol{\\theta}$ is the vector of parameters to be estimated ($\\gamma$, $\\beta_i$'s, and $\\boldsymbol{\\delta}$).\n\n    The OLS estimator for the parameter vector is given by:\n    $$ \\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{z} $$\n    The residuals are calculated as $\\mathbf{e} = \\mathbf{z} - \\mathbf{X}\\hat{\\boldsymbol{\\theta}}$. The estimated variance of the regression error is $s^2 = \\frac{\\mathbf{e}^T \\mathbf{e}}{N - k}$, where $N$ is the number of observations used in the regression and $k$ is the number of estimated parameters (the number of columns in $\\mathbf{X}$).\n\n    The ADF test statistic is the standard $t$-statistic for the coefficient $\\gamma$, which is the first element of $\\boldsymbol{\\theta}$. It is calculated as:\n    $$ t_\\gamma = \\frac{\\hat{\\gamma}}{\\widehat{\\text{SE}}(\\hat{\\gamma})} $$\n    where $\\hat{\\gamma}$ is the OLS estimate of $\\gamma$ and its standard error is $\\widehat{\\text{SE}}(\\hat{\\gamma}) = \\sqrt{s^2 (\\mathbf{X}^T \\mathbf{X})^{-1}_{11}}$, with $(\\mathbf{X}^T \\mathbf{X})^{-1}_{11}$ being the first diagonal element of the inverse of the cross-product matrix of regressors.\n\n3.  **Lag Order Selection via Akaike Information Criterion (AIC)**\n    The lag order $p$ must be chosen to ensure the residuals $u_t$ are approximately white noise. We select $p$ from the range $\\{0, 1, \\dots, p_{\\max}\\}$, where $p_{\\max} = \\lfloor 12 (T/100)^{1/4} \\rfloor$. For a sample size of $T=400$, this yields $p_{\\max} = \\lfloor 12 (400/100)^{1/4} \\rfloor = \\lfloor 12 \\cdot 4^{1/4} \\rfloor = \\lfloor 12 \\sqrt{2} \\rfloor = 16$.\n\n    For each candidate lag order $p$, we estimate the ADF regression and compute the AIC, defined as:\n    $$ \\text{AIC}(p) = N_p \\ln\\left(\\frac{\\text{RSS}_p}{N_p}\\right) + 2k_p $$\n    where $N_p = T-1-p$ is the number of effective observations for a model with $p$ lags, $\\text{RSS}_p$ is the residual sum of squares $(\\mathbf{e}^T \\mathbf{e})$, and $k_p$ is the total number of regressors (including deterministic terms). The optimal lag order, $p^*$, is the one that minimizes the AIC. In case of a tie, the smallest $p$ is chosen.\n\n4.  **Critical Value Approximation via Parametric Bootstrap**\n    Under the null hypothesis $H_0: \\gamma = 0$, the $t$-statistic $t_\\gamma$ does not follow a standard Student's $t$-distribution. Its distribution, often called the Dickey-Fuller distribution, depends on the sample size and the deterministic terms included in the regression. We approximate this distribution using a parametric bootstrap.\n\n    The procedure is as follows:\n    a. Generate $B=1200$ bootstrap samples $\\{y_t^*\\}_{t=1}^T$, each from the null data-generating process, which is a simple random walk: $y_t^* = y_{t-1}^* + \\eta_t$, with $y_0^*=0$ and innovations $\\eta_t \\sim \\mathcal{N}(0,1)$ being independent and identically distributed standard Gaussian variates.\n    b. For each bootstrap sample, perform the ADF test using the same specification as for the original data: the same deterministic regressors and the same optimal lag order $p^*$ selected in the previous step.\n    c. Calculate and store the resulting $t$-statistic, $t_\\gamma^*$, for each of the $B$ replications.\n    d. The collection of these $B$ statistics forms an empirical distribution that approximates the true null distribution of the ADF statistic. The left-tail critical value for a significance level $\\alpha=0.05$, denoted $c_{0.05}$, is the $5$-th percentile of this empirical distribution.\n\n5.  **Hypothesis Test and Classification**\n    The decision rule is to compare the ADF statistic $t_\\gamma$ computed from the original data with the bootstrapped critical value $c_{0.05}$.\n    - If $t_\\gamma \\le c_{0.05}$, we reject the null hypothesis $H_0$. This provides evidence against the presence of a unit root, and the series is classified as stationary, or $I(0)$. The result is $\\text{True}$.\n    - If $t_\\gamma > c_{0.05}$, we fail to reject the null hypothesis $H_0$. There is insufficient evidence to rule out a unit root, and the series is classified as non-stationary, or $I(1)$. The result is $\\text{False}$.\n\nThis completes the logical design of the procedure, which will now be implemented for each of the specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef _generate_series(case_params: dict) -> np.ndarray:\n    \"\"\"Generates a time series based on the specified case parameters.\"\"\"\n    T = case_params[\"T\"]\n    seed = case_params[\"seed\"]\n    dgp_type = case_params[\"dgp_type\"]\n    \n    rng = np.random.default_rng(seed)\n    eps = rng.standard_normal(T)\n    \n    y = np.zeros(T)\n\n    if dgp_type == 1:  # Stable AR(1)\n        phi = 0.6\n        y[0] = 0\n        for t in range(1, T):\n            y[t] = phi * y[t-1] + eps[t]\n    elif dgp_type == 2:  # Random walk with drift\n        delta = 0.1\n        y[0] = 0\n        for t in range(1, T):\n            y[t] = y[t-1] + delta + eps[t]\n    elif dgp_type == 3:  # Trend-stationary AR\n        phi = 0.7\n        beta = 0.05\n        u = np.zeros(T)\n        u[0] = 0\n        for t in range(1, T):\n            u[t] = phi * u[t-1] + eps[t]\n        time_trend = np.arange(1, T + 1)\n        y = beta * time_trend + u\n    elif dgp_type == 4:  # Unit root with MA(1) noise\n        theta = 0.5\n        v = np.zeros(T)\n        v[0] = eps[0]  # eps_{-1} = 0\n        for t in range(1, T):\n            v[t] = eps[t] + theta * eps[t-1]\n        y = np.cumsum(v)\n\n    return y\n\ndef _run_adf_regression(y: np.ndarray, p: int, include_trend: bool) -> tuple:\n    \"\"\"\n    Constructs matrices and performs OLS for the ADF regression.\n    \n    Returns:\n        t_stat (float): The t-statistic for the lagged level coefficient.\n        aic (float): The Akaike Information Criterion value.\n    \"\"\"\n    T = len(y)\n    delta_y = np.diff(y)\n    \n    # Effective number of observations\n    n_eff = T - 1 - p\n    \n    # Dependent variable: delta_y[t] for t = p, ..., T-2\n    # This corresponds to Delta y_{p+1} ... Delta y_{T-1}\n    z = delta_y[p:]\n    \n    # Regressors\n    # 1. Lagged level: y[t-1] for t=p+1,...,T-1. Corresponds to y[p:T-1]\n    regressors = [y[p:-1]]\n    \n    # 2. Lagged differences: Delta y_{t-i} for i=1..p\n    for i in range(1, p + 1):\n        # For a given t, Delta y_{t-i} is delta_y[t-1-i]\n        # For t running from p+1 to T-1, this is a slice\n        # from (p+1)-1-i to (T-1)-1-i, i.e., p-i to T-2-i\n        regressors.append(delta_y[p - i : n_eff + p - i])\n\n    # 3. Deterministic terms\n    regressors.append(np.ones(n_eff))\n    if include_trend:\n        # Time index t runs from p+1 to T-1\n        regressors.append(np.arange(p + 1, T))\n\n    X = np.stack(regressors, axis=1)\n    k = X.shape[1]\n    \n    try:\n        # OLS estimation: beta_hat = (X'X)^{-1} X'z\n        beta_hat = np.linalg.solve(X.T @ X, X.T @ z)\n        \n        residuals = z - X @ beta_hat\n        rss = residuals @ residuals\n        \n        # AIC\n        if n_eff > 0:\n            aic = n_eff * np.log(rss / n_eff) + 2 * k\n        else:\n            aic = np.inf\n\n        # T-statistic for gamma (coefficient on y_{t-1})\n        sigma2 = rss / (n_eff - k)\n        var_cov = sigma2 * np.linalg.inv(X.T @ X)\n        se_gamma = np.sqrt(var_cov[0, 0])\n        t_stat = beta_hat[0] / se_gamma\n\n    except np.linalg.LinAlgError:\n        # Handle cases of perfect multicollinearity (unlikely but possible)\n        t_stat, aic = np.nan, np.inf\n\n    return t_stat, aic\n\ndef solve():\n    \"\"\"\n    Main function to run the ADF tests for all specified cases.\n    \"\"\"\n    test_cases = [\n        {\"dgp_type\": 1, \"T\": 400, \"seed\": 202405, \"include_trend\": False, \"B\": 1200, \"alpha\": 0.05},\n        {\"dgp_type\": 2, \"T\": 400, \"seed\": 202406, \"include_trend\": False, \"B\": 1200, \"alpha\": 0.05},\n        {\"dgp_type\": 3, \"T\": 400, \"seed\": 202407, \"include_trend\": True, \"B\": 1200, \"alpha\": 0.05},\n        {\"dgp_type\": 4, \"T\": 400, \"seed\": 202408, \"include_trend\": False, \"B\": 1200, \"alpha\": 0.05},\n    ]\n\n    results = []\n    \n    # Fixed seed for bootstrap to ensure reproducibility\n    BOOTSTRAP_SEED = 12345\n    boot_rng = np.random.default_rng(BOOTSTRAP_SEED)\n\n    for case in test_cases:\n        # 1. Generate the time series\n        y = _generate_series(case)\n        T = case[\"T\"]\n        \n        # 2. Select optimal lag order p using AIC\n        p_max = int(12 * (T / 100)**(1/4))\n        aics = []\n        for p in range(p_max + 1):\n            _, aic = _run_adf_regression(y, p, case[\"include_trend\"])\n            aics.append((aic, p))\n        \n        # Smallest p wins ties due to stable sort nature or explicit handling.\n        # min() on tuples (aic, p) naturally prefers smaller p for same aic.\n        best_p = min(aics)[1]\n        \n        # 3. Calculate ADF statistic for the original series\n        observed_t_stat, _ = _run_adf_regression(y, best_p, case[\"include_trend\"])\n        \n        # 4. Run bootstrap to find the critical value\n        bootstrap_t_stats = []\n        for _ in range(case[\"B\"]):\n            # Generate a series under the null (random walk)\n            innovations = boot_rng.standard_normal(T)\n            y_star = np.cumsum(innovations)\n            \n            # Calculate t-stat on this bootstrapped series\n            t_stat_star, _ = _run_adf_regression(y_star, best_p, case[\"include_trend\"])\n            if not np.isnan(t_stat_star):\n                 bootstrap_t_stats.append(t_stat_star)\n        \n        # 5. Determine the critical value and make a decision\n        critical_value = np.quantile(bootstrap_t_stats, case[\"alpha\"])\n        \n        is_I0 = observed_t_stat <= critical_value\n        results.append(is_I0)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}