{
    "hands_on_practices": [
        {
            "introduction": "A core feature of the GARCH model is its ability to capture volatility persistence, which is the tendency for periods of high or low volatility to cluster together. The GARCH(1,1) parameters, $\\alpha_1$ and $\\beta_1$, directly control the rate at which the impact of a volatility shock fades over time. This exercise provides a tangible way to interpret these parameters by calculating the \"half-life\" of a shock—the time it takes for a shock's influence on future volatility to decay by half—allowing for a direct comparison of volatility dynamics across different financial assets .",
            "id": "2395686",
            "problem": "Consider a univariate Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model of order $\\left(1,1\\right)$ for a zero-mean return process $\\left\\{r_t\\right\\}$:\n$$r_t=\\epsilon_t,\\quad \\epsilon_t=\\sqrt{h_t}\\,z_t,$$\nwhere $\\left\\{z_t\\right\\}$ is independent and identically distributed with $\\mathbb{E}\\left[z_t\\right]=0$ and $\\mathrm{Var}\\left[z_t\\right]=1$, and the conditional variance $\\left\\{h_t\\right\\}$ follows\n$$h_t=\\omega+\\alpha_1\\,\\epsilon_{t-1}^2+\\beta_1\\,h_{t-1},$$\nwith parameters $\\omega>0$, $\\alpha_1\\ge 0$, $\\beta_1\\ge 0$. Under the covariance stationarity condition $\\alpha_1+\\beta_1<1$, the impact of a unit volatility shock decays geometrically at the rate $\\alpha_1+\\beta_1$. Define the half-life of a volatility shock as the number of periods $k$ required for the expected impact to decline to one half of its initial magnitude, given by\n$$k=\\frac{\\ln\\left(0.5\\right)}{\\ln\\left(\\alpha_1+\\beta_1\\right)},$$\nwhere $\\ln\\left(\\cdot\\right)$ denotes the natural logarithm. For the boundary case $\\alpha_1+\\beta_1=1$, take the half-life to be $+\\infty$.\n\nYour task is to compute the half-life $k$ for each of the following parameter sets, each representing a different financial asset. Treat each pair $\\left(\\alpha_1,\\beta_1\\right)$ as a distinct test case. Assume that $\\omega>0$ is such that $h_t$ is well-defined, and that $\\alpha_1,\\beta_1$ satisfy the stated nonnegativity constraints. Use the convention that if $\\alpha_1+\\beta_1=1$, the output for that test case must be $+\\infty$.\n\nTest suite (assets and parameters):\n- Asset A: $\\alpha_1=0.05$, $\\beta_1=0.90$.\n- Asset B: $\\alpha_1=0.05$, $\\beta_1=0.949$.\n- Asset C: $\\alpha_1=0.10$, $\\beta_1=0.40$.\n- Asset D: $\\alpha_1=0.10$, $\\beta_1=0.90$.\n- Asset E: $\\alpha_1=0.02$, $\\beta_1=0.92$.\n\nRequirements for the program output:\n- For each asset, compute the half-life $k$ using the formula above.\n- For the boundary case $\\alpha_1+\\beta_1=1$, output $+\\infty$.\n- For all finite $k$, round to exactly $6$ decimal places.\n- The program should produce a single line of output containing all results in the order A, B, C, D, E, formatted as a comma-separated list enclosed in square brackets with no spaces, for example, $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C,\\text{result}_D,\\text{result}_E\\right]$.\n- Represent $+\\infty$ literally as $\\text{inf}$ in the output line.\n\nNo physical units apply. Angles are not involved. The final answers for each test case must be floats, with the boundary case represented by $\\text{inf}$ as specified. The solution should work for the exact parameter values provided above and respect the ordering of the test suite.",
            "solution": "The problem presented is scientifically well-posed and provides sufficient information for a unique solution. The task is to compute the half-life of a volatility shock for a Generalized Autoregressive Conditional Heteroskedasticity, or GARCH($1,1$), process under several parameterizations.\n\nThe GARCH($1,1$) model for a zero-mean return series $r_t$ is defined by the equations:\n$$r_t = \\epsilon_t$$\n$$\\epsilon_t = \\sqrt{h_t} z_t$$\nwhere $\\{z_t\\}$ is an i.i.d. process with zero mean and unit variance. The conditional variance, $h_t$, evolves according to:\n$$h_t = \\omega + \\alpha_1 \\epsilon_{t-1}^2 + \\beta_1 h_{t-1}$$\nThe persistence of a volatility shock is determined by the sum of the parameters $\\alpha_1$ and $\\beta_1$. This sum, which we denote as $S = \\alpha_1 + \\beta_1$, dictates the rate at which the impact of a past shock on future conditional variance decays. For a covariance-stationary process, it is required that $S < 1$.\n\nThe half-life, $k$, is defined as the number of time periods required for the expected impact of a shock to decay to $50\\%$ of its initial value. This is governed by the relation $(\\alpha_1 + \\beta_1)^k = 0.5$. Solving for $k$ yields the formula:\n$$k = \\frac{\\ln(0.5)}{\\ln(\\alpha_1 + \\beta_1)}$$\nThis formula is valid for $\\alpha_1 + \\beta_1 < 1$. The logarithm of a number between $0$ and $1$ is negative, so both the numerator and denominator are negative, yielding a positive half-life $k$.\n\nIn the special case where $\\alpha_1 + \\beta_1 = 1$, the process is known as an Integrated GARCH (IGARCH) model. In this regime, shocks have an infinitely persistent effect on the conditional variance, and thus the half-life $k$ is taken to be $+\\infty$.\n\nWe will now compute the half-life for each specified set of parameters.\n\n1.  **Asset A**: The parameters are $\\alpha_1 = 0.05$ and $\\beta_1 = 0.90$.\n    The persistence is $S = 0.05 + 0.90 = 0.95$.\n    The half-life is calculated as:\n    $$k_A = \\frac{\\ln(0.5)}{\\ln(0.95)} \\approx \\frac{-0.693147}{-0.051293} \\approx 13.513427$$\n    The result, rounded to $6$ decimal places, is $13.513427$.\n\n2.  **Asset B**: The parameters are $\\alpha_1 = 0.05$ and $\\beta_1 = 0.949$.\n    The persistence is $S = 0.05 + 0.949 = 0.999$. This indicates very high persistence.\n    The half-life is calculated as:\n    $$k_B = \\frac{\\ln(0.5)}{\\ln(0.999)} \\approx \\frac{-0.693147}{-0.0010005} \\approx 692.800063$$\n    The result, rounded to $6$ decimal places, is $692.800063$.\n\n3.  **Asset C**: The parameters are $\\alpha_1 = 0.10$ and $\\beta_1 = 0.40$.\n    The persistence is $S = 0.10 + 0.40 = 0.50$.\n    The half-life is calculated as:\n    $$k_C = \\frac{\\ln(0.5)}{\\ln(0.5)} = 1$$\n    The result, expressed with $6$ decimal places, is $1.000000$.\n\n4.  **Asset D**: The parameters are $\\alpha_1 = 0.10$ and $\\beta_1 = 0.90$.\n    The persistence is $S = 0.10 + 0.90 = 1.0$.\n    This corresponds to the IGARCH boundary case. The half-life is infinite.\n    $$k_D = +\\infty$$\n\n5.  **Asset E**: The parameters are $\\alpha_1 = 0.02$ and $\\beta_1 = 0.92$.\n    The persistence is $S = 0.02 + 0.92 = 0.94$.\n    The half-life is calculated as:\n    $$k_E = \\frac{\\ln(0.5)}{\\ln(0.94)} \\approx \\frac{-0.693147}{-0.061875} \\approx 11.202302$$\n    The result, rounded to $6$ decimal places, is $11.202302$.\n\nThe final results are compiled into a single list as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the volatility shock half-life for a GARCH(1,1) model\n    for a given test suite of parameters.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains (alpha_1, beta_1) for an asset.\n    test_cases = [\n        (0.05, 0.90),   # Asset A\n        (0.05, 0.949),  # Asset B\n        (0.10, 0.40),   # Asset C\n        (0.10, 0.90),   # Asset D\n        (0.02, 0.92)    # Asset E\n    ]\n\n    results = []\n    for alpha_1, beta_1 in test_cases:\n        # Calculate the shock persistence parameter\n        persistence = alpha_1 + beta_1\n\n        # Check for the boundary case (IGARCH) where persistence is 1.\n        # np.isclose is used for safe floating-point comparison.\n        if np.isclose(persistence, 1.0):\n            result_str = 'inf'\n        else:\n            # Calculate the half-life k using the provided formula.\n            # k = ln(0.5) / ln(alpha_1 + beta_1)\n            half_life = np.log(0.5) / np.log(persistence)\n            # Format the result to exactly 6 decimal places.\n            result_str = f\"{half_life:.6f}\"\n        \n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    # The format is a comma-separated list of results enclosed in square brackets.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Understanding a model's properties is the first step, but fitting it to data requires a robust estimation procedure, typically Maximum Likelihood Estimation (MLE). To efficiently find the parameters that maximize the log-likelihood function, numerical optimizers rely on the function's gradient, which indicates the direction of steepest ascent. This practice takes you \"under the hood\" of GARCH estimation by requiring the derivation and implementation of the analytical gradient, providing a deep, practical understanding of the computational mechanics that link the model's parameters to the data .",
            "id": "2395726",
            "problem": "Consider a univariate return series $\\{r_t\\}_{t=1}^T$ modeled by a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model of order $\\left(1,1\\right)$ with zero conditional mean and normally distributed errors. The model is given by\n$$\nr_t = \\varepsilon_t, \\quad \\varepsilon_t \\mid \\mathcal{F}_{t-1} \\sim \\mathcal{N}\\!\\left(0, h_t\\right),\n$$\nwith conditional variance dynamics\n$$\nh_t = \\omega + \\alpha \\,\\varepsilon_{t-1}^2 + \\beta \\,h_{t-1},\n$$\nfor all $t \\in \\{1,\\dots,T\\}$, where the parameter vector is $\\theta = \\left(\\omega,\\alpha,\\beta\\right)$, with $\\omega > 0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta < 1$. The log-likelihood of the sample under the normal distribution is\n$$\n\\ell\\!\\left(\\theta\\right) = -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\log\\!\\left(2\\pi\\right) + \\log\\!\\left(h_t\\right) + \\frac{\\varepsilon_t^2}{h_t}\\right].\n$$\nAssume the initialization\n$$\nh_0 = \\frac{\\omega}{1-\\alpha-\\beta}, \\qquad \\varepsilon_0^2 = h_0.\n$$\n\nTask:\n1. Derive, from first principles, the analytical gradient vector of the total log-likelihood $\\ell\\!\\left(\\theta\\right)$ with respect to $\\theta = \\left(\\omega,\\alpha,\\beta\\right)$ under the initialization stated above.\n2. Implement a program that, given the data and a parameter vector $\\theta$, computes the analytical gradient (not a numerical approximation) at $\\theta$.\n\nTest suite:\n- Use the return sequence of length $T = 12$ given by\n$$\n\\{r_t\\}_{t=1}^{12} = \\{\\,0.004,\\,-0.002,\\,0.006,\\,0.000,\\,-0.007,\\,0.005,\\,-0.0035,\\,0.001,\\,0.000,\\,0.0045,\\,-0.0025,\\,0.003\\,\\}.\n$$\n- Evaluate the gradient at the following four parameter vectors, each satisfying $\\omega > 0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta < 1$:\n  1. $\\theta_1 = \\left(0.00001,\\,0.05,\\,0.90\\right)$,\n  2. $\\theta_2 = \\left(0.00001,\\,0.09,\\,0.90\\right)$,\n  3. $\\theta_3 = \\left(0.00002,\\,0.00,\\,0.80\\right)$,\n  4. $\\theta_4 = \\left(0.00002,\\,0.10,\\,0.00\\right)$.\n\nRequired final output format:\n- Your program must produce a single line of output containing the list of gradient vectors for the four test cases, in the same order as listed above, as a comma-separated list enclosed in square brackets. Represent each gradient vector as a list of three real numbers in the order $\\left[\\frac{\\partial \\ell}{\\partial \\omega},\\,\\frac{\\partial \\ell}{\\partial \\alpha},\\,\\frac{\\partial \\ell}{\\partial \\beta}\\right]$. For example, the output should have the form\n$$\n\\big[\\,[g_{1,\\omega}, g_{1,\\alpha}, g_{1,\\beta}],\\,[g_{2,\\omega}, g_{2,\\alpha}, g_{2,\\beta}],\\,[g_{3,\\omega}, g_{3,\\alpha}, g_{3,\\beta}],\\,[g_{4,\\omega}, g_{4,\\alpha}, g_{4,\\beta}]\\,\\big].\n$$\n\nAll answers are real numbers; no physical units apply. Angles are not involved. Percentages, if any appear during your work, must be expressed as decimals in the final output. The output must be exactly one line in the format specified above.",
            "solution": "The problem requires the derivation and implementation of the analytical gradient for the log-likelihood of a GARCH($1$,$1$) model. The problem statement is scientifically sound, mathematically well-posed, and complete. We proceed with the solution.\n\nThe specified GARCH($1$,$1$) model for a return series $\\{r_t\\}_{t=1}^T$ with zero conditional mean is:\n$$\nr_t = \\varepsilon_t, \\quad \\varepsilon_t \\mid \\mathcal{F}_{t-1} \\sim \\mathcal{N}(0, h_t)\n$$\n$$\nh_t = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta h_{t-1}\n$$\nThe parameter vector is $\\theta = (\\omega, \\alpha, \\beta)^T$. The log-likelihood function for a sample of size $T$ is given by:\n$$\n\\ell(\\theta) = -\\frac{1}{2}\\sum_{t=1}^{T}\\left[\\log(2\\pi) + \\log(h_t) + \\frac{r_t^2}{h_t}\\right]\n$$\nwhere we have substituted $\\varepsilon_t = r_t$ as defined by the model.\n\nThe gradient vector $\\nabla_\\theta \\ell(\\theta)$ has components $\\frac{\\partial \\ell}{\\partial \\omega}$, $\\frac{\\partial \\ell}{\\partial \\alpha}$, and $\\frac{\\partial \\ell}{\\partial \\beta}$. Let $\\theta_j$ be a generic component of $\\theta$. Applying the chain rule to the summation yields the partial derivative of the total log-likelihood with respect to $\\theta_j$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{t=1}^{T} \\frac{\\partial}{\\partial \\theta_j} \\left(-\\frac{1}{2} \\left[ \\log(h_t) + \\frac{r_t^2}{h_t} \\right]\\right) = \\sum_{t=1}^{T} \\left( -\\frac{1}{2} \\left[ \\frac{1}{h_t} - \\frac{r_t^2}{h_t^2} \\right] \\right) \\frac{\\partial h_t}{\\partial \\theta_j}\n$$\nThis expression simplifies to:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{t=1}^{T} \\frac{1}{2h_t^2} \\left( r_t^2 - h_t \\right) \\frac{\\partial h_t}{\\partial \\theta_j}\n$$\nTo evaluate this gradient, we must compute the time series of conditional variances $\\{h_t\\}_{t=1}^T$ and their partial derivatives with respect to each parameter, $\\{\\frac{\\partial h_t}{\\partial \\theta_j}\\}_{t=1}^T$. These are obtained via recurrence relations derived from the GARCH($1$,$1$) variance equation.\n\nFirst, we establish the recursion for $\\{h_t\\}$. The problem specifies the initialization $h_0 = \\frac{\\omega}{1-\\alpha-\\beta}$ and $\\varepsilon_0^2 = h_0$. For $t>1$, $\\varepsilon_{t-1}^2 = r_{t-1}^2$ is given by the data. The conditional variance at $t=1$ is:\n$$\nh_1 = \\omega + \\alpha \\varepsilon_0^2 + \\beta h_0 = \\omega + (\\alpha + \\beta) h_0 = \\omega + (\\alpha + \\beta) \\frac{\\omega}{1-\\alpha-\\beta} = \\frac{\\omega(1-\\alpha-\\beta) + \\omega(\\alpha+\\beta)}{1-\\alpha-\\beta} = \\frac{\\omega}{1-\\alpha-\\beta}\n$$\nThis is the unconditional variance of the process. For $t=2, \\dots, T$, the variance is computed recursively:\n$$\nh_t = \\omega + \\alpha r_{t-1}^2 + \\beta h_{t-1}\n$$\n\nNext, we derive the recursive formulas for the partial derivatives of $h_t$.\n\n**1. Derivative with respect to $\\omega$**\nWe differentiate the expressions for $h_t$ with respect to $\\omega$.\nFor $t=1$:\n$$ \\frac{\\partial h_1}{\\partial \\omega} = \\frac{\\partial}{\\partial \\omega} \\left(\\frac{\\omega}{1-\\alpha-\\beta}\\right) = \\frac{1}{1-\\alpha-\\beta} $$\nFor $t > 1$:\n$$ \\frac{\\partial h_t}{\\partial \\omega} = \\frac{\\partial}{\\partial \\omega} (\\omega + \\alpha r_{t-1}^2 + \\beta h_{t-1}) = 1 + \\beta \\frac{\\partial h_{t-1}}{\\partial \\omega} $$\nThis defines a simple linear recurrence for $\\{\\frac{\\partial h_t}{\\partial \\omega}\\}_{t=1}^T$.\n\n**2. Derivative with respect to $\\alpha$**\nWe differentiate with respect to $\\alpha$. For $t=1$:\n$$ \\frac{\\partial h_1}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} \\left(\\frac{\\omega}{1-\\alpha-\\beta}\\right) = \\omega(-1)(1-\\alpha-\\beta)^{-2}(-1) = \\frac{\\omega}{(1-\\alpha-\\beta)^2} $$\nFor $t > 1$, $r_{t-1}^2$ is a fixed data point and has zero derivative with respect to $\\alpha$:\n$$ \\frac{\\partial h_t}{\\partial \\alpha} = \\frac{\\partial}{\\partial \\alpha} (\\omega + \\alpha r_{t-1}^2 + \\beta h_{t-1}) = r_{t-1}^2 + \\beta \\frac{\\partial h_{t-1}}{\\partial \\alpha} $$\n\n**3. Derivative with respect to $\\beta$**\nWe differentiate with respect to $\\beta$. For $t=1$:\n$$ \\frac{\\partial h_1}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left(\\frac{\\omega}{1-\\alpha-\\beta}\\right) = \\omega(-1)(1-\\alpha-\\beta)^{-2}(-1) = \\frac{\\omega}{(1-\\alpha-\\beta)^2} $$\nFor $t > 1$:\n$$ \\frac{\\partial h_t}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} (\\omega + \\alpha r_{t-1}^2 + \\beta h_{t-1}) = h_{t-1} + \\beta \\frac{\\partial h_{t-1}}{\\partial \\beta} $$\n\n**Algorithmic Summary**\n\nTo compute the gradient vector $\\nabla_\\theta \\ell(\\theta)$ for a given parameter set $\\theta = (\\omega, \\alpha, \\beta)$ and return series $\\{r_t\\}_{t=1}^T$, we perform the following steps:\n\n1.  Initialize arrays of length $T$ to store the sequences $h_t$, $\\frac{\\partial h_t}{\\partial \\omega}$, $\\frac{\\partial h_t}{\\partial \\alpha}$, and $\\frac{\\partial h_t}{\\partial \\beta}$ for $t=1, \\dots, T$.\n2.  Compute the initial values at $t=1$:\n    -   $h_1 = \\frac{\\omega}{1-\\alpha-\\beta}$\n    -   $\\frac{\\partial h_1}{\\partial \\omega} = \\frac{1}{1-\\alpha-\\beta}$\n    -   $\\frac{\\partial h_1}{\\partial \\alpha} = \\frac{\\omega}{(1-\\alpha-\\beta)^2}$\n    -   $\\frac{\\partial h_1}{\\partial \\beta} = \\frac{\\omega}{(1-\\alpha-\\beta)^2}$\n3.  Iterate from $t=2$ to $T$, computing the values for each sequence using the previously computed values at $t-1$:\n    -   $h_t = \\omega + \\alpha r_{t-1}^2 + \\beta h_{t-1}$\n    -   $\\frac{\\partial h_t}{\\partial \\omega} = 1 + \\beta \\frac{\\partial h_{t-1}}{\\partial \\omega}$\n    -   $\\frac{\\partial h_t}{\\partial \\alpha} = r_{t-1}^2 + \\beta \\frac{\\partial h_{t-1}}{\\partial \\alpha}$\n    -   $\\frac{\\partial h_t}{\\partial \\beta} = h_{t-1} + \\beta \\frac{\\partial h_{t-1}}{\\partial \\beta}$\n4.  Initialize the gradient components $(\\frac{\\partial \\ell}{\\partial \\omega}, \\frac{\\partial \\ell}{\\partial \\alpha}, \\frac{\\partial \\ell}{\\partial \\beta})$ to zero.\n5.  Iterate from $t=1$ to $T$, calculating the contribution of each time step to the total gradient and accumulating the sum:\n    -   Common factor: $c_t = \\frac{1}{2h_t^2} ( r_t^2 - h_t )$\n    -   $\\frac{\\partial \\ell}{\\partial \\omega} \\mathrel{+}= c_t \\cdot \\frac{\\partial h_t}{\\partial \\omega}$\n    -   $\\frac{\\partial \\ell}{\\partial \\alpha} \\mathrel{+}= c_t \\cdot \\frac{\\partial h_t}{\\partial \\alpha}$\n    -   $\\frac{\\partial \\ell}{\\partial \\beta} \\mathrel{+}= c_t \\cdot \\frac{\\partial h_t}{\\partial \\beta}$\n6.  The resulting accumulated sums form the gradient vector. This procedure is implemented in the provided program.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_garch_gradient(r, omega, alpha, beta):\n    \"\"\"\n    Computes the analytical gradient of the GARCH(1,1) log-likelihood.\n\n    Args:\n        r (np.ndarray): The series of returns.\n        omega (float): GARCH parameter omega.\n        alpha (float): GARCH parameter alpha.\n        beta (float): GARCH parameter beta.\n\n    Returns:\n        list[float]: The gradient vector [d_ell/d_omega, d_ell/d_alpha, d_ell/d_beta].\n    \"\"\"\n    T = len(r)\n    r_sq = r**2\n\n    # Arrays to store h_t and its derivatives\n    h = np.zeros(T)\n    dh_domega = np.zeros(T)\n    dh_dalpha = np.zeros(T)\n    dh_dbeta = np.zeros(T)\n    \n    # Check for stationarity condition to avoid division by zero\n    stat_denom = 1.0 - alpha - beta\n    if stat_denom = 0:\n        # Parameters violate the stationarity constraint, which would make h0 infinite or negative.\n        # This case is not expected for the given valid test cases but is good practice.\n        return [np.nan, np.nan, np.nan]\n\n    # Initialization for t=1 (index 0)\n    # h_1 = omega / (1 - alpha - beta)\n    h[0] = omega / stat_denom\n    \n    # Derivatives of h_1\n    # dh1/d_omega = 1 / (1 - alpha - beta)\n    dh_domega[0] = 1.0 / stat_denom\n    # dh1/d_alpha = omega / (1 - alpha - beta)^2\n    dh_dalpha[0] = omega / (stat_denom**2)\n    # dh1/d_beta = omega / (1 - alpha - beta)^2\n    dh_dbeta[0] = omega / (stat_denom**2)\n    \n    # Recursive calculation for t = 2 to T (indices 1 to T-1)\n    for t in range(1, T):\n        # h_t = omega + alpha * r_{t-1}^2 + beta * h_{t-1}\n        h[t] = omega + alpha * r_sq[t-1] + beta * h[t-1]\n        \n        # dh_t/d_omega = 1 + beta * dh_{t-1}/d_omega\n        dh_domega[t] = 1.0 + beta * dh_domega[t-1]\n        \n        # dh_t/d_alpha = r_{t-1}^2 + beta * dh_{t-1}/d_alpha\n        dh_dalpha[t] = r_sq[t-1] + beta * dh_dalpha[t-1]\n        \n        # dh_t/d_beta = h_{t-1} + beta * dh_{t-1}/d_beta\n        dh_dbeta[t] = h[t-1] + beta * dh_dbeta[t-1]\n        \n    # Compute the gradient of the log-likelihood\n    grad_omega = 0.0\n    grad_alpha = 0.0\n    grad_beta = 0.0\n    \n    for t in range(T):\n        common_factor = 0.5 * (r_sq[t] / h[t] - 1.0) / h[t]\n        grad_omega += common_factor * dh_domega[t]\n        grad_alpha += common_factor * dh_dalpha[t]\n        grad_beta += common_factor * dh_dbeta[t]\n\n    return [grad_omega, grad_alpha, grad_beta]\n\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    # Define the data and test cases from the problem statement.\n    returns_data = np.array([\n        0.004, -0.002, 0.006, 0.000, -0.007, 0.005, \n        -0.0035, 0.001, 0.000, 0.0045, -0.0025, 0.003\n    ])\n    \n    test_cases = [\n        (0.00001, 0.05, 0.90),  # theta_1\n        (0.00001, 0.09, 0.90),  # theta_2\n        (0.00002, 0.00, 0.80),  # theta_3\n        (0.00002, 0.10, 0.00)   # theta_4\n    ]\n\n    results = []\n    for params in test_cases:\n        omega, alpha, beta = params\n        gradient = compute_garch_gradient(returns_data, omega, alpha, beta)\n        results.append(gradient)\n\n    # Format the results into the required string format.\n    # The string representation of a list in Python already includes brackets and spaces.\n    # Joining the string representations of each gradient list with a comma\n    # and enclosing in outer brackets produces the desired format.\n    # e.g., \"[[g1_w, g1_a, g1_b],[g2_w, g2_a, g2_b]]\"\n    result_str = f\"[{','.join(map(str, results))}]\"\n    \n    # To remove spaces for a more compact representation, though not strictly required by example\n    result_str_no_space = result_str.replace(\" \", \"\")\n\n    print(result_str_no_space)\n\nsolve()\n```"
        },
        {
            "introduction": "A powerful model is only as reliable as its underlying assumptions, and a crucial skill for any practitioner is the ability to recognize model misspecification. An unmodeled structural break in a time series' mean, for example, can generate patterns in the data that standard diagnostic tools, like the Engle LM test, may incorrectly identify as GARCH effects. This simulation exercise teaches a vital lesson in critical thinking and diagnostics, demonstrating how a phenomenon entirely unrelated to conditional volatility can lead to spurious evidence of GARCH, highlighting the importance of carefully examining your data before fitting complex models .",
            "id": "2399496",
            "problem": "You are asked to design and implement a simulation and testing program to study how a structural break in the mean of a time series can be misidentified as a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) of order one-one, denoted GARCH(1,1), by standard tests for autoregressive conditional heteroskedasticity. Your program must be fully self-contained and produce results without any user interaction.\n\nFundamental base to be used includes the following well-tested definitions and facts.\n\n- A time series $\\{y_t\\}_{t=1}^T$ exhibits a structural break in the mean at a known time $T_b$ if its deterministic mean changes from $\\mu_1$ to $\\mu_2$ at $t = T_b + 1$, while the innovation variance remains constant. Specifically, for $t \\le T_b$, $y_t = \\mu_1 + \\varepsilon_t$ and for $t  T_b$, $y_t = \\mu_2 + \\varepsilon_t$, with $\\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2)$.\n- An Ordinary Least Squares (OLS) mean model is $y_t = x_t^{\\prime}\\beta + \\varepsilon_t$, where $x_t$ includes an intercept and, when appropriate, additional regressors such as a break dummy $d_t$ where $d_t = 0$ for $t \\le T_b$ and $d_t = 1$ for $t  T_b$. The OLS residual is $e_t = y_t - x_t^{\\prime}\\hat{\\beta}$.\n- The GARCH(1,1) model for zero-mean innovations $\\{u_t\\}$ is $u_t = \\sqrt{h_t} z_t$ with $z_t \\overset{iid}{\\sim} \\mathcal{N}(0,1)$ and conditional variance $h_t = \\omega + \\alpha u_{t-1}^2 + \\beta h_{t-1}$, with parameters satisfying $\\omega  0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta  1$ to ensure a finite unconditional variance. The observed series is $y_t = \\mu + u_t$.\n- The Engle Lagrange Multiplier (LM) test for autoregressive conditional heteroskedasticity of order $q$ on a sequence of OLS residuals $\\{e_t\\}$ proceeds by regressing $e_t^2$ on a constant and $q$ lags of $e_t^2$, computing the coefficient of determination $R^2$, and forming the statistic $LM = n R^2$, where $n$ is the number of usable observations in that regression. Under the null of no autoregressive conditional heteroskedasticity up to lag $q$, $LM$ is asymptotically $\\chi^2_q$ distributed. The $p$-value is $1 - F_{\\chi^2_q}(LM)$, where $F_{\\chi^2_q}$ is the cumulative distribution function of the chi-square distribution with $q$ degrees of freedom.\n\nYour task is to implement the following steps precisely.\n\n1. Simulation of data generating processes (DGPs):\n   - Structural break in the mean with constant variance: For given parameters $T$, $\\mu_1$, $\\mu_2$, $\\sigma$, and break fraction $b \\in (0,1)$, simulate $\\{y_t\\}$ with $T_b = \\lfloor b T \\rfloor$ and $\\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2)$ as described above.\n   - Independent and identically distributed (iid) homoskedastic series without break: $y_t = \\mu + \\varepsilon_t$, $\\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0,\\sigma^2)$.\n   - GARCH(1,1) series without mean break: $y_t = \\mu + u_t$ with $u_t$ following GARCH(1,1) as defined above. Initialize $h_0$ at the unconditional variance $h_0 = \\omega/(1-\\alpha-\\beta)$ and use an initial burn-in of $500$ steps to mitigate initialization effects.\n\n2. Modeling and residual extraction:\n   - For each simulated series, fit two OLS mean models to obtain residuals $\\{e_t\\}$:\n     - Misspecified mean: Intercept-only model $x_t = [1]$ for all $t$.\n     - Well-specified mean: Intercept plus break dummy when the DGP has a structural break, i.e., $x_t = [1, d_t]$, and intercept-only otherwise.\n\n3. Autoregressive conditional heteroskedasticity testing:\n   - For each set of residuals and a specified lag order $q$, run the Engle LM test as stated above and compute the $p$-value based on the $\\chi^2_q$ distribution.\n\n4. Interpretation principle (for your reasoning, not part of the output): A low $p$-value indicates rejection of the null of no autoregressive conditional heteroskedasticity. The central phenomenon to observe is that a structural break in the mean can induce serial dependence in $e_t^2$ when the mean is misspecified, leading to a spurious detection of GARCH-type behavior.\n\nUse the following test suite of parameter sets. All random draws must be generated with the same fixed seed $12345$ so that results are reproducible.\n\n- Test case $1$ (happy path, strong mid-sample break):\n  - DGP: Structural mean break with constant variance.\n  - Parameters: $T = 4000$, $\\mu_1 = 0.0$, $\\mu_2 = 2.0$, $\\sigma = 1.0$, $b = 0.5$, $q = 5$.\n\n- Test case $2$ (boundary condition, late break, minimal lag in test):\n  - DGP: Structural mean break with constant variance.\n  - Parameters: $T = 4000$, $\\mu_1 = 0.0$, $\\mu_2 = 3.0$, $\\sigma = 1.0$, $b = 0.9$, $q = 1$.\n\n- Test case $3$ (edge case, genuine GARCH(1,1)):\n  - DGP: GARCH(1,1) without mean break.\n  - Parameters: $T = 5000$, $\\mu = 0.0$, $\\omega = 0.1$, $\\alpha = 0.05$, $\\beta = 0.9$, $q = 5$.\n\n- Test case $4$ (control, iid homoskedastic):\n  - DGP: Homoskedastic iid without mean break.\n  - Parameters: $T = 4000$, $\\mu = 0.0$, $\\sigma = 1.0$, $q = 5$.\n\nRequired outputs per test case:\n\n- For each test case $i \\in \\{1,2,3,4\\}$, compute two $p$-values:\n  - $p^{(i)}_{\\text{misspec}}$: The LM test $p$-value using the intercept-only mean model.\n  - $p^{(i)}_{\\text{well}}$: The LM test $p$-value using the well-specified mean model (with break dummy only when applicable).\n\nFinal output format:\n\n- Your program should produce a single line of output containing the $8$ results as a comma-separated list enclosed in square brackets, in the exact order\n  $[p^{(1)}_{\\text{misspec}}, p^{(1)}_{\\text{well}}, p^{(2)}_{\\text{misspec}}, p^{(2)}_{\\text{well}}, p^{(3)}_{\\text{misspec}}, p^{(3)}_{\\text{well}}, p^{(4)}_{\\text{misspec}}, p^{(4)}_{\\text{well}}]$,\n  with each value rounded to six decimal places. For example, an output might look like $[0.000001,0.845210,0.000004,0.612345,0.000000,0.000000,0.523410,0.523410]$.\n\nAll quantities in this problem are unitless real numbers, and no physical units are involved. Angles are not used. Percentages, when conceptually referenced, must be treated as decimal fractions; however, you must output only the specified $p$-values as described above.",
            "solution": "The problem statement has been subjected to rigorous validation and is found to be valid. It is scientifically grounded in established econometric theory, specifically the study of time series misspecification. The problem is well-posed, providing a complete and consistent set of definitions, parameters, and procedures required for a unique, verifiable solution. There are no logical contradictions, ambiguities, or factual inaccuracies. The task is a standard computational exercise in econometrics, not a request for subjective opinion or speculative reasoning. We now proceed with the systematic solution.\n\nThe objective is to demonstrate computationally how a structural break in the mean of a time series, if not correctly modeled, can produce spurious evidence of Autoregressive Conditional Heteroskedasticity (ARCH), a phenomenon that can lead to incorrect model selection in practice. The solution is structured into three principal stages: data simulation, model estimation with residual extraction, and hypothesis testing.\n\nFirst, we address the simulation of the three distinct\nData Generating Processes (DGPs) as specified. A fixed random seed, $12345$, is used for all stochastic operations to ensure reproducibility.\n\n1.  **Structural Break series**: A time series $\\{y_t\\}_{t=1}^T$ of length $T$ with a single break in its mean is generated according to the model:\n    $$\n    y_t =\n    \\begin{cases}\n    \\mu_1 + \\varepsilon_t  \\text{for } t \\le T_b \\\\\n    \\mu_2 + \\varepsilon_t  \\text{for } t  T_b\n    \\end{cases}\n    $$\n    where the break point is $T_b = \\lfloor bT \\rfloor$ for a given break fraction $b \\in (0,1)$, and the innovations $\\varepsilon_t$ are independent and identically distributed (iid) draws from a normal distribution, $\\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)$.\n\n2.  **GARCH(1,1) series**: A time series $\\{y_t\\}_{t=1}^T$ governed by a Generalized Autoregressive Conditional Heteroskedasticity process of order $(1,1)$ is generated as $y_t = \\mu + u_t$. The innovation term $u_t$ is defined by $u_t = \\sqrt{h_t} z_t$, where $z_t \\overset{iid}{\\sim} \\mathcal{N}(0,1)$. The conditional variance $h_t$ evolves according to:\n    $$\n    h_t = \\omega + \\alpha u_{t-1}^2 + \\beta h_{t-1}\n    $$\n    The parameters must satisfy the stationarity conditions $\\omega  0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta  1$. The simulation is initialized with $h_0$ set to the unconditional variance, $h_{uncond} = \\frac{\\omega}{1-\\alpha-\\beta}$, and an initial sample of $500$ points is generated and subsequently discarded to mitigate initialization bias.\n\n3.  **IID Homoskedastic series**: This is a control series serving as a baseline, generated as $y_t = \\mu + \\varepsilon_t$, with $\\varepsilon_t \\overset{iid}{\\sim} \\mathcal{N}(0, \\sigma^2)$.\n\nSecond, for each simulated series $\\{y_t\\}$, residuals are obtained from two distinct Ordinary Least Squares (OLS) mean model specifications. The general OLS model is $y_t = x_t^{\\prime}\\beta + e_t$, where $x_t$ is a vector of regressors, $\\beta$ is the coefficient vector, and $e_t$ is the residual. The OLS estimator for $\\beta$ is $\\hat{\\beta} = (X'X)^{-1}X'y$, where $X$ and $y$ are the matrix and vector forms of the regressors and the dependent variable, respectively. The residuals are then computed as $e = y - X\\hat{\\beta}$.\n\n1.  **Misspecified Model**: This model consistently assumes a simple constant mean, implemented with a single regressor: an intercept. So, $x_t = [1]$ for all $t$. This model is misspecified for the DGP with a structural break.\n\n2.  **Well-Specified Model**: This model correctly reflects the underlying mean structure of the DGP. For the structural break DGP, the regressors are an intercept and a break dummy variable $d_t$, where $d_t=0$ for $t \\le T_b$ and $d_t=1$ for $t  T_b$. Thus, $x_t = [1, d_t]$. For the GARCH and IID DGPs, which have a constant mean, the well-specified model is identical to the misspecified model, consisting only of an intercept.\n\nThird, the extracted residual series $\\{e_t\\}$ from each model are subjected to the Engle Lagrange Multiplier (LM) test for ARCH effects of order $q$. The null hypothesis of this test is that there is no ARCH up to order $q$, i.e., $H_0: \\gamma_1 = \\gamma_2 = \\dots = \\gamma_q = 0$ in the auxiliary regression:\n$$\ne_t^2 = \\gamma_0 + \\gamma_1 e_{t-1}^2 + \\dots + \\gamma_q e_{t-q}^2 + \\nu_t\n$$\nThe LM test statistic is calculated as $LM = nR^2$, where $n$ is the number of observations in the auxiliary regression ($n = T-q$), and $R^2$ is the coefficient of determination from this regression. Under the null hypothesis, the $LM$ statistic is asymptotically distributed as a chi-square distribution with $q$ degrees of freedom, $LM \\sim \\chi^2_q$. The $p$-value, which is the probability of observing a test statistic at least as extreme as the one computed, is given by $1 - F_{\\chi^2_q}(LM)$, where $F_{\\chi^2_q}$ is the cumulative distribution function for the $\\chi^2_q$ distribution.\n\nA low $p$-value (e.g., $ 0.05$) leads to rejection of the null hypothesis, suggesting the presence of ARCH effects. The core of the analysis lies in comparing the $p$-values from the misspecified and well-specified models for the structural break DGP. Spurious detection of ARCH is confirmed if the misspecified model yields a low $p$-value while the well-specified model yields a high one. For the GARCH DGP, both models are expected to yield low $p$-values. For the IID DGP, both are expected to yield high $p$-values. The entire procedure is implemented for each of the four specified test cases to produce the required output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    # Establish a reproducible random number generator\n    rng = np.random.default_rng(12345)\n\n    def generate_structural_break(T, mu1, mu2, sigma, b, rng_gen):\n        \"\"\"Generates a time series with a structural break in the mean.\"\"\"\n        Tb = int(np.floor(b * T))\n        innovations = rng_gen.normal(loc=0.0, scale=sigma, size=T)\n        y = np.zeros(T)\n        y[:Tb] = mu1 + innovations[:Tb]\n        y[Tb:] = mu2 + innovations[Tb:]\n        return y, Tb\n\n    def generate_garch(T, mu, omega, alpha, beta, rng_gen):\n        \"\"\"Generates a GARCH(1,1) time series.\"\"\"\n        burn_in = 500\n        total_len = T + burn_in\n        \n        z = rng_gen.normal(loc=0.0, scale=1.0, size=total_len)\n        u = np.zeros(total_len)\n        h = np.zeros(total_len)\n        \n        h[0] = omega / (1 - alpha - beta)\n        u[0] = np.sqrt(h[0]) * z[0]\n        \n        for t in range(1, total_len):\n            h[t] = omega + alpha * u[t-1]**2 + beta * h[t-1]\n            u[t] = np.sqrt(h[t]) * z[t]\n        \n        y = mu + u[burn_in:]\n        return y\n\n    def generate_iid(T, mu, sigma, rng_gen):\n        \"\"\"Generates an IID homoskedastic time series.\"\"\"\n        innovations = rng_gen.normal(loc=0.0, scale=sigma, size=T)\n        y = mu + innovations\n        return y\n\n    def get_residuals(y, with_break_dummy, Tb=None):\n        \"\"\"\n        Fits an OLS model to the data and returns the residuals.\n        \"\"\"\n        T = len(y)\n        if with_break_dummy:\n            if Tb is None:\n                raise ValueError(\"Tb must be provided for break dummy model.\")\n            X = np.ones((T, 2))\n            dummy = np.zeros(T)\n            dummy[Tb:] = 1.0\n            X[:, 1] = dummy\n        else:\n            X = np.ones((T, 1))\n\n        beta_hat = np.linalg.lstsq(X, y, rcond=None)[0]\n        y_hat = X @ beta_hat\n        residuals = y - y_hat\n        return residuals\n\n    def engle_lm_test(residuals, q):\n        \"\"\"\n        Performs the Engle LM test for ARCH effects.\n        \"\"\"\n        T = len(residuals)\n        e_sq = residuals**2\n        \n        # Dependent variable for the auxiliary regression\n        Y_aux = e_sq[q:]\n        n = len(Y_aux) # n = T - q\n        \n        # Independent variables (constant + q lags of e_sq)\n        X_aux = np.ones((n, q + 1))\n        for i in range(q):\n            # lag i+1\n            X_aux[:, i + 1] = e_sq[q - 1 - i : T - 1 - i]\n            \n        # OLS on the auxiliary regression\n        try:\n            # lstsq returns sum of squared residuals in the second element\n            rss_val = np.linalg.lstsq(X_aux, Y_aux, rcond=None)[1][0]\n        except IndexError:\n            # This can happen if the problem is perfectly determined, rss is empty.\n            rss_val = 0.0\n\n        # Total sum of squares of the dependent variable\n        tss = np.sum((Y_aux - np.mean(Y_aux))**2)\n        \n        if tss  1e-12: # Handle cases with zero variance\n             R2 = 0.0\n        else:\n            R2 = 1 - rss_val / tss\n\n        lm_stat = n * R2\n        p_value = 1 - chi2.cdf(lm_stat, q)\n        \n        return p_value\n\n    test_cases = [\n        {'type': 'break', 'params': {'T': 4000, 'mu1': 0.0, 'mu2': 2.0, 'sigma': 1.0, 'b': 0.5, 'q': 5}},\n        {'type': 'break', 'params': {'T': 4000, 'mu1': 0.0, 'mu2': 3.0, 'sigma': 1.0, 'b': 0.9, 'q': 1}},\n        {'type': 'garch', 'params': {'T': 5000, 'mu': 0.0, 'omega': 0.1, 'alpha': 0.05, 'beta': 0.9, 'q': 5}},\n        {'type': 'iid', 'params': {'T': 4000, 'mu': 0.0, 'sigma': 1.0, 'q': 5}}\n    ]\n    \n    results = []\n    \n    # Test case 1\n    case = test_cases[0]\n    params = case['params']\n    y, Tb = generate_structural_break(params['T'], params['mu1'], params['mu2'], params['sigma'], params['b'], rng)\n    res_misspec = get_residuals(y, with_break_dummy=False)\n    p_misspec = engle_lm_test(res_misspec, params['q'])\n    res_well = get_residuals(y, with_break_dummy=True, Tb=Tb)\n    p_well = engle_lm_test(res_well, params['q'])\n    results.extend([p_misspec, p_well])\n    \n    # Test case 2\n    case = test_cases[1]\n    params = case['params']\n    y, Tb = generate_structural_break(params['T'], params['mu1'], params['mu2'], params['sigma'], params['b'], rng)\n    res_misspec = get_residuals(y, with_break_dummy=False)\n    p_misspec = engle_lm_test(res_misspec, params['q'])\n    res_well = get_residuals(y, with_break_dummy=True, Tb=Tb)\n    p_well = engle_lm_test(res_well, params['q'])\n    results.extend([p_misspec, p_well])\n\n    # Test case 3\n    case = test_cases[2]\n    params = case['params']\n    y = generate_garch(params['T'], params['mu'], params['omega'], params['alpha'], params['beta'], rng)\n    # For GARCH DGP, both misspecified and well-specified models are intercept-only\n    res = get_residuals(y, with_break_dummy=False)\n    p_val = engle_lm_test(res, params['q'])\n    results.extend([p_val, p_val])\n\n    # Test case 4\n    case = test_cases[3]\n    params = case['params']\n    y = generate_iid(params['T'], params['mu'], params['sigma'], rng)\n    # For IID DGP, both misspecified and well-specified models are intercept-only\n    res = get_residuals(y, with_break_dummy=False)\n    p_val = engle_lm_test(res, params['q'])\n    results.extend([p_val, p_val])\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{v:.6f}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}