{
    "hands_on_practices": [
        {
            "introduction": "This first practice serves as your entry point into the world of cointegration testing. You will implement the foundational two-step Engle-Granger procedure, which involves first estimating a potential long-run equilibrium via regression and then testing the resulting error term for stationarity. This initial hands-on exercise  is crucial for building an intuition that distinguishes a meaningful long-run relationship from a purely spurious correlation between non-stationary series.",
            "id": "2380057",
            "problem": "You are given a decision problem formulated in purely mathematical terms about long-run equilibrium relations between two stochastic processes representing a firm's quarterly research and development expenditure and its subsequent quarterly revenues with a one-quarter lag. For each parameter set in the provided test suite, construct two sequences $\\{x_t\\}_{t=0}^{T}$ and $\\{y_t\\}_{t=1}^{T}$ according to the following definitions.\n\n1. Data-generating components and notation.\n   - Let $T \\in \\mathbb{N}$ denote the sample size in quarters.\n   - Let $x_0 = 0$. For $t \\in \\{1,2,\\dots,T\\}$, let\n     $$x_t = x_{t-1} + \\varepsilon_t,$$\n     where $\\{\\varepsilon_t\\}_{t=1}^{T}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\varepsilon}^2$.\n   - Define the lagged input used in the revenue relation as $\\{x_{t-1}\\}_{t=1}^{T}$.\n   - Two alternative specifications for the disturbance in the revenue relation are allowed, parameterized by a type indicator $\\text{residual\\_type} \\in \\{\\text{stationary}, \\text{random\\_walk}\\}$:\n     - If $\\text{residual\\_type} = \\text{stationary}$, define $v_0 = 0$ and for $t \\in \\{1,2,\\dots,T\\}$,\n       $$v_t = \\phi\\, v_{t-1} + \\eta_t,$$\n       where $|\\phi| < 1$ and $\\{\\eta_t\\}_{t=1}^{T}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\eta}^2$. In this case, set\n       $$y_t = c + \\beta\\, x_{t-1} + v_t.$$\n     - If $\\text{residual\\_type} = \\text{random\\_walk}$, define $s_0 = 0$ and for $t \\in \\{1,2,\\dots,T\\}$,\n       $$s_t = s_{t-1} + \\eta_t,$$\n       where $\\{\\eta_t\\}_{t=1}^{T}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\eta}^2$. In this case, set\n       $$y_t = c + \\beta\\, x_{t-1} + s_t.$$\n\n2. Decision rule to be applied for each parameter set.\n   - Consider the static long-run relation\n     $$y_t = \\alpha + \\beta^{\\ast} x_{t-1} + u_t,$$\n     and define $\\{\\hat{u}_t\\}_{t=1}^{T}$ as the residuals from the Ordinary Least Squares (OLS) regression of $\\{y_t\\}_{t=1}^{T}$ on a constant and $\\{x_{t-1}\\}_{t=1}^{T}$.\n   - Compute the Augmented Dickey-Fuller (ADF) regression without deterministic terms and with one lag of the first difference:\n     $$\\Delta \\hat{u}_t = \\varphi\\, \\hat{u}_{t-1} + \\gamma\\, \\Delta \\hat{u}_{t-1} + \\varepsilon_t^{\\ast}, \\quad t = 3,4,\\dots,T.$$\n     Let $\\tau$ denote the $t$-statistic for the null hypothesis $H_0: \\varphi = 0$ against the one-sided alternative $H_1: \\varphi < 0$, computed from this regression.\n   - Use the significance level $\\alpha = 0.05$ with the Engle-Granger residual-based critical value $c_{0.05} = -3.34$ for the model with an intercept in the cointegrating regression and no deterministic terms in the ADF regression. Declare that $\\{y_t\\}$ and $\\{x_{t-1}\\}$ are cointegrated if and only if $\\tau \\le c_{0.05}$.\n\n3. Test suite. For each parameter set $\\theta$, generate the sequences using the above definitions with the given parameters and an independent pseudo-random number generator initialized at the specified seed. The parameters are:\n   - Case A: $\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 101, 0.5, 1.3, 1.0, 0.3, 0.4, \\text{stationary}\\right)$.\n   - Case B: $\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 202, 0.5, 1.3, 1.0, 0.8, 0.0, \\text{random\\_walk}\\right)$.\n   - Case C: $\\left(T, \\text{seed}, c, \\beta, \\sigma_{\\varepsilon}, \\sigma_{\\eta}, \\phi, \\text{residual\\_type}\\right) = \\left(400, 303, -0.2, -0.8, 0.8, 0.5, 0.7, \\text{stationary}\\right)$.\n\n4. Required final output format. Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, in the order of the test cases above, where each boolean is $\\text{True}$ if cointegration is declared under the rule in item $2$ and $\\text{False}$ otherwise. For example, an output with three cases should look like\n`[True,False,True]`.",
            "solution": "The problem statement presents a valid and well-posed computational exercise in time series econometrics. It requires the implementation of the Engle-Granger two-step procedure for testing cointegration between two simulated time series. The problem is scientifically grounded, formally specified, and all parameters and procedures required for a unique solution are provided for each test case. We shall proceed with the solution.\n\nThe core of the problem is to determine whether a long-run equilibrium relationship, known as cointegration, exists between two stochastic processes, $\\{x_t\\}$ and $\\{y_t\\}$. This is resolved by generating the series according to specified data-generating processes (DGPs) and then applying a statistical hypothesis test. The overall procedure is implemented for each parameter set provided in the test suite.\n\n**Step 1: Data Generation**\n\nFor each test case, we first simulate the time series data. A specific seed is used to initialize a pseudo-random number generator for reproducibility.\nLet $T$ be the sample size. We generate two sequences of independent and identically distributed (i.i.d.) Gaussian random shocks: $\\{\\varepsilon_t\\}_{t=1}^{T} \\sim N(0, \\sigma_{\\varepsilon}^2)$ and $\\{\\eta_t\\}_{t=1}^{T} \\sim N(0, \\sigma_{\\eta}^2)$.\n\nThe first series, $\\{x_t\\}_{t=0}^{T}$, representing R&D expenditure, is constructed as a pure random walk:\n$$x_t = x_{t-1} + \\varepsilon_t, \\quad \\text{for } t=1, \\dots, T$$\nwith the initial condition $x_0 = 0$. A process like $\\{x_t\\}$ is non-stationary and is said to be integrated of order one, denoted $I(1)$, because its first difference, $\\Delta x_t = \\varepsilon_t$, is stationary.\n\nThe second series, $\\{y_t\\}_{t=1}^{T}$, representing revenues, is constructed based on a relationship with the lagged expenditure, $\\{x_{t-1}\\}_{t=1}^{T}$, and an error term whose nature depends on the `residual_type` parameter.\n\n- **Case 1: `residual_type` = `stationary`**. The error term $\\{v_t\\}_{t=1}^{T}$ follows a stationary first-order autoregressive (AR(1)) process:\n$$v_t = \\phi v_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, T$$\nwith $v_0 = 0$ and autoregressive parameter $|\\phi|  1$. The revenue series is then:\n$$y_t = c + \\beta x_{t-1} + v_t$$\nIn this case, the linear combination $y_t - \\beta x_{t-1} - c = v_t$ is stationary. By definition, if a linear combination of two $I(1)$ variables is stationary, the variables are cointegrated. Thus, we expect the test to detect cointegration.\n\n- **Case 2: `residual_type` = `random_walk`**. The error term $\\{s_t\\}_{t=1}^{T}$ is itself a random walk:\n$$s_t = s_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, T$$\nwith $s_0 = 0$. The revenue series is:\n$$y_t = c + \\beta x_{t-1} + s_t$$\nHere, the linear combination $y_t - \\beta x_{t-1} - c = s_t$ is a random walk and therefore non-stationary ($I(1)$). The variables are not cointegrated. This is a classic case of a spurious regression, where a high correlation may be observed despite no meaningful long-run relationship.\n\n**Step 2: Engle-Granger Cointegration Test**\n\nThe test proceeds in two stages.\n\n**Stage 1: Estimate the Long-Run Relationship**\nWe estimate the parameters of the hypothesized long-run cointegrating relationship using Ordinary Least Squares (OLS):\n$$y_t = \\alpha + \\beta^{\\ast} x_{t-1} + u_t, \\quad \\text{for } t=1, \\dots, T$$\nWe regress the vector $Y = (y_1, \\dots, y_T)^T$ on a design matrix $X_{ols}$ of dimension $T \\times 2$, where the first column is a vector of ones and the second column is the vector $(x_0, \\dots, x_{T-1})^T$. The OLS coefficient estimates are given by $\\hat{\\theta} = (\\hat{\\alpha}, \\hat{\\beta}^{\\ast})^T = (X_{ols}^T X_{ols})^{-1} X_{ols}^T Y$.\n\nThe residuals from this regression are then computed:\n$$\\hat{u}_t = y_t - (\\hat{\\alpha} + \\hat{\\beta}^{\\ast} x_{t-1}), \\quad \\text{for } t=1, \\dots, T$$\nIf $y_t$ and $x_{t-1}$ are cointegrated, these residuals should be stationary, $I(0)$. If they are not cointegrated, the residuals will be non-stationary, $I(1)$.\n\n**Stage 2: Test Residuals for a Unit Root**\nTo test the stationarity of the residuals $\\{\\hat{u}_t\\}$, we perform an Augmented Dickey-Fuller (ADF) test. The null hypothesis is that the residuals have a unit root (are non-stationary), which implies no cointegration. The alternative is stationarity, which implies cointegration.\n\nThe specific ADF regression model to be estimated is:\n$$\\Delta \\hat{u}_t = \\varphi \\hat{u}_{t-1} + \\gamma \\Delta \\hat{u}_{t-1} + \\varepsilon_t^{\\ast}, \\quad \\text{for } t=3, \\dots, T$$\nThis is an OLS regression of the change in residuals, $\\Delta \\hat{u}_t$, on the lagged level of the residuals, $\\hat{u}_{t-1}$, and the lagged change, $\\Delta \\hat{u}_{t-1}$. The regression is run over $T-2$ observations.\n\nLet $Z$ be the $(T-2) \\times 1$ vector of the dependent variable, $(\\Delta \\hat{u}_3, \\dots, \\Delta \\hat{u}_T)^T$. Let $W$ be the $(T-2) \\times 2$ matrix of independent variables, whose rows are $(\\hat{u}_{t-1}, \\Delta \\hat{u}_{t-1})$ for $t=3, \\dots, T$. The OLS estimates for $(\\varphi, \\gamma)$ are $\\hat{\\psi} = (\\hat{\\varphi}, \\hat{\\gamma})^T = (W^T W)^{-1} W^T Z$.\n\nThe test statistic is the t-statistic for the coefficient $\\varphi$:\n$$\\tau = \\frac{\\hat{\\varphi}}{\\text{SE}(\\hat{\\varphi})}$$\nwhere $SE(\\hat{\\varphi})$ is the standard error of the estimate $\\hat{\\varphi}$. This is calculated as the square root of the first diagonal element of the coefficient covariance matrix, $\\hat{\\sigma}_{\\varepsilon^{\\ast}}^2 (W^T W)^{-1}$. The variance of the regression-error term $\\varepsilon_t^{\\ast}$ is estimated as:\n$$\\hat{\\sigma}_{\\varepsilon^{\\ast}}^2 = \\frac{1}{T-2-2} \\sum_{t=3}^T (\\varepsilon_t^{\\ast})^2 = \\frac{\\text{SSR}}{T-4}$$\nwhere $SSR$ is the sum of squared residuals from the ADF regression.\n\n**Step 3: Decision Rule**\n\nThe null hypothesis $H_0: \\varphi = 0$ (unit root in residuals, no cointegration) is tested against the one-sided alternative $H_1: \\varphi  0$ (stationarity, cointegration).\n\nThe calculated t-statistic, $\\tau$, is compared against a specific critical value. Because the test is performed on residuals from a regression of $I(1)$ variables, the distribution of $\\tau$ is non-standard and is known as the Engle-Granger distribution. The problem provides the appropriate critical value for a significance level of $\\alpha = 0.05$, which is $c_{0.05} = -3.34$.\n\nThe decision rule is:\n- If $\\tau \\le -3.34$, reject $H_0$. We conclude that the series are cointegrated. The result for the test case is `True`.\n- If $\\tau > -3.34$, do not reject $H_0$. We conclude there is no evidence of cointegration. The result for the test case is `False`.\n\nThis complete procedure is applied to each of the three test cases to determine the final boolean output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Engle-Granger two-step cointegration test for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (T, seed, c, beta, sigma_eps, sigma_eta, phi, residual_type)\n        (400, 101, 0.5, 1.3, 1.0, 0.3, 0.4, \"stationary\"),\n        (400, 202, 0.5, 1.3, 1.0, 0.8, 0.0, \"random_walk\"),\n        (400, 303, -0.2, -0.8, 0.8, 0.5, 0.7, \"stationary\"),\n    ]\n\n    results = []\n    critical_value = -3.34\n\n    for case in test_cases:\n        T, seed, c, beta, sigma_eps, sigma_eta, phi, residual_type = case\n\n        # Step 1: Data Generation\n        rng = np.random.default_rng(seed)\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=T)\n        eta = rng.normal(loc=0.0, scale=sigma_eta, size=T)\n\n        # Generate x_t as a random walk\n        x = np.zeros(T + 1)\n        for t in range(1, T + 1):\n            x[t] = x[t - 1] + eps[t - 1]\n\n        # Generate y_t based on the residual type\n        # In the main vector y, y[t-1] corresponds to the mathematical y_t\n        if residual_type == \"stationary\":\n            v = np.zeros(T + 1)\n            for t in range(1, T + 1):\n                v[t] = phi * v[t-1] + eta[t-1]\n            error_term = v[1:T+1]\n        elif residual_type == \"random_walk\":\n            s = np.zeros(T + 1)\n            for t in range(1, T + 1):\n                s[t] = s[t-1] + eta[t-1]\n            error_term = s[1:T+1]\n        \n        y = c + beta * x[0:T] + error_term\n\n        # Step 2: Engle-Granger Stage 1 - OLS regression to get residuals\n        # Regress y_t on a constant and x_{t-1}\n        X_ols1 = np.vstack((np.ones(T), x[0:T])).T\n        \n        # Using lstsq is numerically more stable than direct matrix inversion\n        coeffs_ols1, _, _, _ = np.linalg.lstsq(X_ols1, y, rcond=None)\n        u_hat = y - X_ols1 @ coeffs_ols1\n\n        # Step 3: Engle-Granger Stage 2 - ADF test on residuals\n        # ADF model: Delta(u_hat)_t = phi*u_hat_{t-1} + gamma*Delta(u_hat)_{t-1} + e_t\n        # for t = 3, ..., T. This gives T-2 observations.\n        delta_u_hat = u_hat[1:] - u_hat[:-1]  # Corresponds to Delta(u_hat)_2, ..., Delta(u_hat)_T\n\n        # Dependent variable: Delta(u_hat)_t for t=3,...,T\n        Y_adf = delta_u_hat[1:]  # Corresponds to Delta(u_hat)_3, ..., Delta(u_hat)_T\n\n        # Independent variables\n        # u_hat_{t-1} for t=3,...,T\n        X_adf_col1 = u_hat[1:-1]\n        # Delta(u_hat)_{t-1} for t=3,...,T\n        X_adf_col2 = delta_u_hat[:-1]\n        \n        X_adf = np.vstack((X_adf_col1, X_adf_col2)).T\n\n        # Perform OLS for the ADF regression\n        coeffs_adf, ssr_adf, _, _ = np.linalg.lstsq(X_adf, Y_adf, rcond=None)\n        phi_hat = coeffs_adf[0]\n\n        # Calculate the t-statistic for phi_hat\n        n_obs_adf = T - 2\n        k_params_adf = 2\n        df = n_obs_adf - k_params_adf\n\n        # Estimate of the regression error variance\n        sigma2_hat_adf = ssr_adf[0] / df\n\n        # Variance-covariance matrix of the ADF coefficients\n        try:\n            cov_matrix = sigma2_hat_adf * np.linalg.inv(X_adf.T @ X_adf)\n            se_phi_hat = np.sqrt(cov_matrix[0, 0])\n            tau_statistic = phi_hat / se_phi_hat\n        except np.linalg.LinAlgError:\n            # Handle cases of singular matrix, which is unlikely but possible\n            # A very large t-stat would not reject the null, which is a safe default\n            tau_statistic = 0.0\n\n        # Step 4: Decision\n        is_cointegrated = tau_statistic = critical_value\n        results.append(is_cointegrated)\n        \n    # Format and print the final output\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While foundational, the classic Engle-Granger test often uses tabulated critical values that may not be suitable for every dataset, especially in smaller samples. This exercise  elevates your toolkit by introducing a more robust method: generating custom critical values via a parametric bootstrap procedure. By simulating the null hypothesis of no cointegration, you will learn to derive statistical thresholds that are tailored to the specific characteristics of your data, a hallmark of modern econometric practice.",
            "id": "2380094",
            "problem": "You are given a stylized task motivated by pricing in Dynamic Random Access Memory (DRAM) supply chains and equity markets. Let $x_t$ denote a synthetic series representing the logarithm of DRAM spot prices and let $y_t$ denote a synthetic series representing the logarithm of a DRAM manufacturer’s stock price. Both $x_t$ and $y_t$ will be generated as integrated of order one series, denoted $I(1)$, under different data-generating processes. Your task is to implement, from first principles, a residual-based Engle–Granger cointegration test using the Augmented Dickey–Fuller (ADF) test on the estimated cointegration residuals, with the decision threshold obtained via a parametric bootstrap under the null of no cointegration.\n\nStart from the following fundamental definitions and facts:\n- A time series $\\{z_t\\}$ is integrated of order one, $I(1)$, if $\\Delta z_t = z_t - z_{t-1}$ is covariance stationary and $z_t$ itself is not stationary.\n- Two $I(1)$ series $\\{x_t\\}$ and $\\{y_t\\}$ are cointegrated if there exists a vector $(1,-\\beta)$ and a constant $\\alpha$ such that the residual $u_t = y_t - \\alpha - \\beta x_t$ is covariance stationary. By the Granger Representation Theorem, if $\\{x_t,y_t\\}$ are cointegrated, then their short-run dynamics admit an Error Correction Model (ECM) in which $\\Delta y_t$ includes the lagged residual $u_{t-1}$ as an error-correction term with a nonzero coefficient.\n- The Engle–Granger procedure estimates $\\alpha$ and $\\beta$ by ordinary least squares and then applies the Augmented Dickey–Fuller (ADF) test to the residuals $u_t$ to test the null hypothesis of a unit root in $u_t$ against the alternative of stationarity. The ADF regression for a given lag order $k \\in \\{0,1,2,\\dots\\}$ is:\n$$\n\\Delta u_t = \\phi\\, u_{t-1} + \\sum_{j=1}^{k} \\gamma_j\\, \\Delta u_{t-j} + \\varepsilon_t,\n$$\nwith no intercept or trend, where $\\Delta u_t = u_t - u_{t-1}$ and the test statistic (the $\\tau$-statistic) is the $t$-statistic for $\\phi$.\n\nYou must implement the following components without using any specialized time-series library:\n1. Simulate the series $\\{x_t\\}$ and $\\{y_t\\}$ according to the specified data-generating processes in the test suite below.\n2. Estimate the cointegration regression $y_t = \\alpha + \\beta x_t + u_t$ by ordinary least squares (OLS) and construct the residuals $\\{\\hat{u}_t\\}$.\n3. Select the ADF lag order $k$ by minimizing the Bayesian Information Criterion (BIC) over a candidate set $k \\in \\{0,1,\\dots,k_{\\max}\\}$, where\n$$\nk_{\\max} = \\left\\lfloor 12 \\cdot \\left(\\frac{T}{100}\\right)^{1/4} \\right\\rfloor,\n$$\n$T$ is the sample size, the ADF regression includes $u_{t-1}$ and $\\Delta u_{t-j}$ for $j=1,\\dots,k$ but no constant, and the BIC for an OLS regression with residual sum of squares $\\mathrm{RSS}$, sample size $n$, and $m$ regressors is\n$$\n\\mathrm{BIC} = \\ln\\!\\left(\\frac{\\mathrm{RSS}}{n}\\right) + \\frac{m \\ln(n)}{n}.\n$$\n4. Compute the ADF $\\tau$-statistic for $\\phi$ in the selected model.\n5. Use a parametric bootstrap under the null of no cointegration to obtain the empirical critical value. Under the null, generate bootstrap series $\\{y_t^{\\ast}\\}$ as an $I(1)$ random walk independent of the observed $\\{x_t\\}$:\n$$\ny_0^{\\ast} = y_0,\\quad y_t^{\\ast} = y_{t-1}^{\\ast} + \\eta_t^{\\ast},\\quad \\eta_t^{\\ast} \\sim \\mathcal{N}(0,\\hat{\\sigma}_{\\Delta y}^2),\n$$\nwhere $\\hat{\\sigma}_{\\Delta y}^2$ is the sample variance of $\\Delta y_t$. For each bootstrap replication, run the Engle–Granger procedure on $(x_t, y_t^{\\ast})$ exactly as in steps $2$–$4$, collect the bootstrap $\\tau$-statistics, and take the left-tail empirical $\\alpha$-quantile as the critical value. Use $\\alpha = 0.05$.\n6. Decision rule: reject the null of no cointegration if the observed $\\tau$-statistic is less than or equal to the bootstrap critical value. Output a boolean for each test case indicating whether cointegration is detected at level $\\alpha = 0.05$.\n\nData-generating processes for the test suite:\n- Case $\\mathrm{A}$ (cointegrated, stable error correction): Let $T=240$, $\\beta=1.5$, $\\rho=0.6$, $\\sigma_e=1.0$, $\\sigma_v=0.5$. Generate innovations $e_t \\sim \\mathcal{N}(0,\\sigma_e^2)$ and $v_t \\sim \\mathcal{N}(0,\\sigma_v^2)$ independently. Construct $x_0=0$, $z_0=0$, and for $t=1,\\dots,T$:\n$$\nx_t = x_{t-1} + e_t,\\quad z_t = \\rho z_{t-1} + v_t,\\quad y_t = \\alpha + \\beta x_t + z_t,\\quad \\alpha = 0.\n$$\n- Case $\\mathrm{B}$ (not cointegrated, independent stochastic trends): Let $T=240$, $\\sigma_e=1.0$, $\\sigma_w=1.0$. Generate $e_t \\sim \\mathcal{N}(0,\\sigma_e^2)$ and $w_t \\sim \\mathcal{N}(0,\\sigma_w^2)$ independently. Construct $x_0=0$, $y_0=0$, and for $t=1,\\dots,T$:\n$$\nx_t = x_{t-1} + e_t,\\quad y_t = y_{t-1} + w_t.\n$$\n- Case $\\mathrm{C}$ (cointegrated, near-boundary error-correction): Let $T=120$, $\\beta=1.2$, $\\rho=0.95$, $\\sigma_e=1.0$, $\\sigma_v=0.5$. Generate $e_t \\sim \\mathcal{N}(0,\\sigma_e^2)$ and $v_t \\sim \\mathcal{N}(0,\\sigma_v^2)$ independently. Construct $x_0=0$, $z_0=0$, and for $t=1,\\dots,T$:\n$$\nx_t = x_{t-1} + e_t,\\quad z_t = \\rho z_{t-1} + v_t,\\quad y_t = \\alpha + \\beta x_t + z_t,\\quad \\alpha = 0.\n$$\n\nImplementation details and fixed settings:\n- Use the same numerical seed for each case as specified here for reproducibility: Case $\\mathrm{A}$ seed $=12345$, Case $\\mathrm{B}$ seed $=23456$, Case $\\mathrm{C}$ seed $=34567$.\n- For the bootstrap, use $R=400$ replications per case and the same seed as the case’s seed to initialize the bootstrap random number generator. Use a Gaussian generator with variance calibrated by the sample variance of $\\Delta y_t$ in that case.\n- Use $k_{\\max} = \\left\\lfloor 12 \\cdot \\left(\\frac{T}{100}\\right)^{1/4} \\right\\rfloor$ for each case individually, and select $k$ by BIC as above.\n- All regressions must be computed by ordinary least squares using linear algebra; do not call any prepackaged unit-root or cointegration routines.\n\nFinal output specification:\n- Your program must compute, for each of the three cases $(\\mathrm{A},\\mathrm{B},\\mathrm{C})$, a boolean indicating whether cointegration is detected at level $\\alpha=0.05$ using the bootstrap critical value computed as described.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\mathrm{A},\\mathrm{B},\\mathrm{C}]$. For example, a valid output format is $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$. No additional text should be printed.",
            "solution": "The problem as stated is valid. It is scientifically grounded in established econometric theory, specifically the Engle–Granger two-step procedure for cointegration testing. It is well-posed, providing all necessary parameters, data-generating processes, and algorithmic specifications to arrive at a unique, verifiable solution. The terminology is precise and the objectives are objective and formalizable. I will now proceed with the solution.\n\nThe task is to implement a residual-based Engle–Granger cointegration test, where the critical value for the test statistic is determined via a parametric bootstrap. This procedure must be applied to three distinct data-generating processes (DGPs). The entire implementation must be from first principles, utilizing only basic linear algebra operations.\n\nThe overall procedure for each test case is as follows:\n1.  Generate the time series $\\{x_t\\}_{t=0}^T$ and $\\{y_t\\}_{t=0}^T$ according to the specified DGP.\n2.  Compute the test statistic $\\tau_{obs}$ from the observed data $(x_t, y_t)$ using the Engle–Granger method.\n3.  Generate an empirical distribution of test statistics under the null hypothesis of no cointegration using a parametric bootstrap.\n4.  Determine the critical value $c_\\alpha$ from this empirical distribution at the significance level $\\alpha=0.05$.\n5.  Compare $\\tau_{obs}$ to $c_\\alpha$ to decide whether to reject the null hypothesis.\n\nLet us detail each of these steps.\n\n**Step 1: Data-Generating Processes**\n\nWe must generate sample data for three cases. All series are of length $T+1$.\n\n- **Case A (Cointegrated):** With $T=240$, $\\beta=1.5$, $\\rho=0.6$, $\\sigma_e=1.0$, $\\sigma_v=0.5$, and $\\alpha=0$. The innovations $e_t \\sim \\mathcal{N}(0, \\sigma_e^2)$ and $v_t \\sim \\mathcal{N}(0, \\sigma_v^2)$ are independent. The series are constructed as:\n$$\nx_t = x_{t-1} + e_t, \\quad x_0=0\n$$\n$$\nz_t = \\rho z_{t-1} + v_t, \\quad z_0=0\n$$\n$$\ny_t = \\beta x_t + z_t\n$$\nHere, $x_t$ is an $I(1)$ process (a random walk). The term $z_t$ is a stationary AR(1) process since $|\\rho|1$. The linear combination $y_t - \\beta x_t = z_t$ is stationary, so by definition, $x_t$ and $y_t$ are cointegrated.\n\n- **Case B (Not Cointegrated):** With $T=240$, $\\sigma_e=1.0$, and $\\sigma_w=1.0$. The innovations $e_t \\sim \\mathcal{N}(0, \\sigma_e^2)$ and $w_t \\sim \\mathcal{N}(0, \\sigma_w^2)$ are independent. The series are constructed as two independent random walks:\n$$\nx_t = x_{t-1} + e_t, \\quad x_0=0\n$$\n$$\ny_t = y_{t-1} + w_t, \\quad y_0=0\n$$\nAs $x_t$ and $y_t$ are driven by independent stochastic trends, no linear combination of them can be stationary. They are not cointegrated.\n\n- **Case C (Cointegrated, Near Unit Root):** This case is structurally identical to Case A but with parameters $T=120$, $\\beta=1.2$, and $\\rho=0.95$. The high value of $\\rho$ makes the stationary component $z_t$ highly persistent, which presents a greater challenge for the unit root test to distinguish from a non-stationary $I(1)$ process.\n\n**Step 2: The Engle–Granger Test Procedure**\n\nThis procedure is applied to a pair of series $(x_t, y_t)$ to yield a single test statistic $\\tau$.\n\n**2.1. Cointegrating Regression**\nFirst, we estimate the long-run relationship $y_t = \\alpha + \\beta x_t + u_t$ using Ordinary Least Squares (OLS) over the sample $t=0, \\dots, T$. We define the vector of observations $\\mathbf{y} = [y_0, \\dots, y_T]^T$ and the design matrix $\\mathbf{X}$ with a column of ones and a column of the $x_t$ values.\n$$\n\\mathbf{X} = \\begin{bmatrix} 1  x_0 \\\\ 1  x_1 \\\\ \\vdots  \\vdots \\\\ 1  x_T \\end{bmatrix}\n$$\nThe OLS estimator for the coefficient vector $\\mathbf{b} = [\\alpha, \\beta]^T$ is given by:\n$$\n\\hat{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\nThe residuals are then computed as $\\hat{u}_t = y_t - \\hat{\\alpha} - \\hat{\\beta} x_t$ for $t=0, \\dots, T$.\n\n**2.2. Augmented Dickey–Fuller (ADF) Test on Residuals**\nThe core of the test is to check if the residuals $\\{\\hat{u}_t\\}$ are stationary. We test the null hypothesis $H_0: \\phi=0$ against the alternative $H_A: \\phi0$ in the ADF regression:\n$$\n\\Delta \\hat{u}_t = \\phi \\hat{u}_{t-1} + \\sum_{j=1}^{k} \\gamma_j \\Delta \\hat{u}_{t-j} + \\varepsilon_t\n$$\nThe inclusion of lagged difference terms $\\Delta \\hat{u}_{t-j}$ is to account for potential serial correlation in the error term $\\varepsilon_t$. The test statistic is the $\\tau$-statistic (or $t$-statistic) associated with the coefficient $\\hat{\\phi}$.\n\n**2.3. Lag Order Selection**\nThe number of lags, $k$, is a crucial parameter. It is selected by minimizing the Bayesian Information Criterion (BIC) over a set of candidate values $k \\in \\{0, 1, \\dots, k_{\\max}\\}$. The maximum lag order is given by:\n$$\nk_{\\max} = \\left\\lfloor 12 \\cdot \\left(\\frac{T}{100}\\right)^{1/4} \\right\\rfloor\n$$\nFor each candidate lag $k$, an OLS regression is performed. The regression for a given $k$ runs over $t=k+1, \\dots, T$, resulting in a sample of size $n = T-k$. The number of estimated parameters is $m = k+1$ (one for $\\phi$ and $k$ for the $\\gamma_j$ coefficients). The BIC is then calculated as:\n$$\n\\text{BIC}(k) = \\ln\\left(\\frac{\\text{RSS}_k}{n}\\right) + \\frac{m \\ln(n)}{n}\n$$\nwhere $\\text{RSS}_k$ is the residual sum of squares from the ADF regression with $k$ lags. The optimal lag $k^*$ is the one that yields the minimum BIC.\n\n**2.4. ADF Test Statistic Calculation**\nUsing the optimal lag order $k^*$, we perform the ADF regression. Let $\\mathbf{Y}_{\\text{ADF}}$ be the vector of $\\Delta \\hat{u}_t$ and $\\mathbf{X}_{\\text{ADF}}$ be the matrix of regressors $(\\hat{u}_{t-1}, \\Delta \\hat{u}_{t-1}, \\dots, \\Delta \\hat{u}_{t-k^*})$. The OLS coefficient vector is $\\hat{\\boldsymbol{\\theta}} = (\\mathbf{X}_{\\text{ADF}}^T \\mathbf{X}_{\\text{ADF}})^{-1} \\mathbf{X}_{\\text{ADF}}^T \\mathbf{Y}_{\\text{ADF}}$, where the first element is $\\hat{\\phi}$. The test statistic is the $t$-ratio for $\\hat{\\phi}$:\n$$\n\\tau = \\frac{\\hat{\\phi}}{\\text{SE}(\\hat{\\phi})}\n$$\nThe standard error $\\text{SE}(\\hat{\\phi})$ is the square root of the first diagonal element of the coefficient covariance matrix, $\\hat{\\Sigma}_{\\hat{\\boldsymbol{\\theta}}}$. This matrix is estimated as:\n$$\n\\hat{\\Sigma}_{\\hat{\\boldsymbol{\\theta}}} = \\hat{\\sigma}_\\varepsilon^2 (\\mathbf{X}_{\\text{ADF}}^T \\mathbf{X}_{\\text{ADF}})^{-1}, \\quad \\text{where} \\quad \\hat{\\sigma}_\\varepsilon^2 = \\frac{\\text{RSS}_{k^*}}{n - m}\n$$\nThe resulting $\\tau$ is the observed test statistic, $\\tau_{obs}$.\n\n**Step 3: Parametric Bootstrap for Critical Values**\n\nStandard Dickey-Fuller critical values are not applicable here because the ADF test is performed on *estimated* residuals, not on an observed series. The distribution of the $\\tau$-statistic depends on the properties of $x_t$ and the sample size. We therefore generate the critical values using a parametric bootstrap under the null hypothesis of no cointegration.\n\nUnder $H_0$, $x_t$ and $y_t$ are independent $I(1)$ processes. The procedure is:\n1.  For the observed data, calculate the first differences of the $y_t$ series, $\\Delta y_t = y_t - y_{t-1}$, and compute their sample variance, $\\hat{\\sigma}_{\\Delta y}^2$.\n2.  Generate $R=400$ bootstrap samples. For each replication $r=1, \\dots, R$:\n    a. Keep the original $x_t$ series fixed.\n    b. Generate a new series $y_t^*$ as a random walk, independent of $x_t$. The innovations $\\eta_t^*$ are drawn from a normal distribution calibrated with the observed data: $\\eta_t^* \\sim \\mathcal{N}(0, \\hat{\\sigma}_{\\Delta y}^2)$.\n    $$\n    y_0^* = y_0, \\quad y_t^* = y_{t-1}^* + \\eta_t^* \\quad \\text{for } t=1, \\dots, T\n    $$\n    c. Apply the complete Engle–Granger test procedure (Steps 2.1 to 2.4) to the pair $(x_t, y_t^*)$ to obtain one bootstrap test statistic, $\\tau^*_r$.\n3.  Collect all $R$ bootstrap statistics $\\{\\tau^*_1, \\dots, \\tau^*_R\\}$ to form an empirical distribution of the test statistic under the null.\n4.  The critical value $c_\\alpha$ for a test of size $\\alpha=0.05$ is the $5$-th percentile of this sorted empirical distribution. For $R=400$, this corresponds to the 20th value in the sorted list of $\\tau^*$ statistics.\n\n**Step 4: Decision Rule**\n\nThe null hypothesis of no cointegration (which corresponds to a unit root in the residuals, $H_0: \\phi=0$) is tested against the one-sided alternative of stationarity ($H_A: \\phi0$). We therefore use a left-tailed test.\n- If $\\tau_{obs} \\leq c_\\alpha$, we reject the null hypothesis and conclude that the series are cointegrated.\n- If $\\tau_{obs} > c_\\alpha$, we fail to reject the null and conclude there is no evidence of cointegration.\n\nThis complete methodology is to be implemented and applied to each of the three cases to determine the final boolean outcome.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the cointegration test for all cases.\n    \"\"\"\n\n    def perform_ols(y, X):\n        \"\"\"\n        Performs Ordinary Least Squares (OLS) regression.\n        \n        Args:\n            y (np.ndarray): Dependent variable vector.\n            X (np.ndarray): Matrix of independent variables (regressors).\n\n        Returns:\n            tuple: A tuple containing:\n                - beta_hat (np.ndarray): Estimated coefficients.\n                - residuals (np.ndarray): Regression residuals.\n                - rss (float): Residual sum of squares.\n                - cov_beta (np.ndarray): Covariance matrix of the coefficients.\n        \"\"\"\n        try:\n            # Using np.linalg.solve is more stable than inv()\n            XtX = X.T @ X\n            XtY = X.T @ y\n            beta_hat = np.linalg.solve(XtX, XtY)\n            residuals = y - X @ beta_hat\n            rss = residuals.T @ residuals\n            n, m = X.shape\n            # Degrees of freedom for variance estimation should be n - m\n            # If n = m, regression is not identified, return NaN to signal failure\n            if n = m:\n                return beta_hat, residuals, rss, np.full((m, m), np.nan)\n            \n            # Unbiased estimator for error variance\n            sigma2_hat = rss / (n - m)\n            # Covariance matrix of estimators\n            cov_beta = sigma2_hat * np.linalg.inv(XtX)\n            return beta_hat, residuals, rss, cov_beta\n        except np.linalg.LinAlgError:\n            # In case of singular matrix, return NaNs\n            m = X.shape[1]\n            return np.full(m, np.nan), np.full_like(y, np.nan), np.nan, np.full((m, m), np.nan)\n\n    def engle_granger_test_stat(x_series, y_series):\n        \"\"\"\n        Computes the Engle-Granger ADF test statistic for a given pair of series.\n        \n        Args:\n            x_series (np.ndarray): The x time series.\n            y_series (np.ndarray): The y time series.\n\n        Returns:\n            float: The calculated ADF tau-statistic.\n        \"\"\"\n        T_full = len(x_series) - 1\n        \n        # Step 2: OLS Cointegration Regression\n        X_coint = np.c_[np.ones(T_full + 1), x_series]\n        b_hat, u_hat, _, _ = perform_ols(y_series, X_coint)\n\n        if np.isnan(b_hat).any():\n             return np.nan\n\n        # Step 3: ADF Lag Selection\n        k_max = int(12 * ((T_full / 100) ** 0.25))\n        delta_u = np.diff(u_hat)\n        T_diff = len(delta_u) # This is T_full\n        \n        best_k = -1\n        min_bic = np.inf\n        \n        # Store results of best k regression to avoid re-computation\n        best_k_results = {}\n\n        for k in range(k_max + 1):\n            # Construct ADF regression matrices\n            # The regression runs from t=k+1 to T, so there are T-k observations.\n            # Python slicing makes this easier to manage.\n            y_adf = delta_u[k:]\n            \n            # Regressors\n            u_lag1 = u_hat[k:-1]\n            X_adf_cols = [u_lag1]\n            for j in range(1, k + 1):\n                lagged_delta_u = delta_u[k-j:-j]\n                X_adf_cols.append(lagged_delta_u)\n            \n            X_adf = np.stack(X_adf_cols, axis=1)\n\n            n, m = X_adf.shape # n = T-k, m = k+1\n            if n = m:\n                continue\n\n            # OLS for ADF regression\n            beta_adf, _, rss_adf, cov_beta_adf = perform_ols(y_adf, X_adf)\n            \n            if np.isnan(rss_adf):\n                continue\n\n            bic = np.log(rss_adf / n) + (m * np.log(n)) / n\n            \n            if bic  min_bic:\n                min_bic = bic\n                best_k = k\n                best_k_results['beta'] = beta_adf\n                best_k_results['cov'] = cov_beta_adf\n        \n        if best_k == -1 or 'beta' not in best_k_results:\n            return np.nan\n\n        # Step 4: Compute ADF tau-statistic from the best model\n        phi_hat = best_k_results['beta'][0]\n        se_phi = np.sqrt(best_k_results['cov'][0, 0])\n        \n        if se_phi == 0 or np.isnan(se_phi):\n            return np.nan\n            \n        tau_statistic = phi_hat / se_phi\n        return tau_statistic\n\n    test_cases = [\n        {'name': 'A', 'T': 240, 'beta': 1.5, 'rho': 0.6, 'sigma_e': 1.0, 'sigma_v': 0.5, 'seed': 12345},\n        {'name': 'B', 'T': 240, 'sigma_e': 1.0, 'sigma_w': 1.0, 'seed': 23456},\n        {'name': 'C', 'T': 120, 'beta': 1.2, 'rho': 0.95, 'sigma_e': 1.0, 'sigma_v': 0.5, 'seed': 34567},\n    ]\n\n    R = 400\n    alpha = 0.05\n    results = []\n\n    for case in test_cases:\n        T = case['T']\n        seed = case['seed']\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Generate Data\n        x = np.zeros(T + 1)\n        y = np.zeros(T + 1)\n\n        if case['name'] in ['A', 'C']:\n            e = rng.normal(0, case['sigma_e'], size=T)\n            v = rng.normal(0, case['sigma_v'], size=T)\n            z = np.zeros(T + 1)\n            x[1:] = np.cumsum(e)\n            for t in range(1, T + 1):\n                z[t] = case['rho'] * z[t-1] + v[t-1]\n            y = case['beta'] * x + z\n        else: # Case B\n            e = rng.normal(0, case['sigma_e'], size=T)\n            w = rng.normal(0, case['sigma_w'], size=T)\n            x[1:] = np.cumsum(e)\n            y[1:] = np.cumsum(w)\n        \n        # Compute observed test statistic\n        tau_observed = engle_granger_test_stat(x, y)\n        \n        # Step 5: Parametric Bootstrap\n        boot_rng = np.random.default_rng(seed)\n        delta_y = np.diff(y)\n        sigma_delta_y_sq = np.var(delta_y, ddof=1)\n        \n        tau_bootstrap = []\n        for _ in range(R):\n            eta_star = boot_rng.normal(0, np.sqrt(sigma_delta_y_sq), size=T)\n            y_star = np.zeros(T + 1)\n            # Initial y_0 is 0 for all cases\n            y_star[0] = 0.0 \n            y_star[1:] = np.cumsum(eta_star)\n            \n            tau_boot_stat = engle_granger_test_stat(x, y_star)\n            if not np.isnan(tau_boot_stat):\n                tau_bootstrap.append(tau_boot_stat)\n\n        tau_bootstrap.sort()\n        \n        # Get critical value from bootstrap distribution\n        num_valid_boots = len(tau_bootstrap)\n        if num_valid_boots == 0:\n            # If all bootstrap runs failed, cannot make a decision\n            # The problem setup should be robust enough to avoid this\n            results.append(False) \n            continue\n\n        cv_index = int(num_valid_boots * alpha)\n        # Ensure index is within bounds\n        cv_index = min(cv_index, num_valid_boots - 1)\n        critical_value = tau_bootstrap[cv_index]\n        \n        # Step 6: Decision Rule\n        is_cointegrated = tau_observed = critical_value\n        results.append(is_cointegrated)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Economic and financial relationships rarely remain stable indefinitely. This final practice moves beyond static testing to the dynamic problem of monitoring a cointegrating relationship for structural breaks . You will implement a recursive estimation algorithm that continuously updates the long-run parameters and uses forecast errors to detect potential regime changes. This skill is vital for risk management and adaptive forecasting in evolving market environments.",
            "id": "2380077",
            "problem": "You are given two synthetic time series constructed to follow a long-run relation that may experience a structural change at an unknown time. Let $\\{x_t\\}_{t=0}^{T-1}$ and $\\{y_t\\}_{t=0}^{T-1}$ be defined as follows. For all $t \\in \\{0,1,\\dots,T-1\\}$, let $x_0 = 0$ and for $t \\ge 1$,\n$$\nx_t = x_{t-1} + \\sigma_e \\,\\varepsilon_t,\n$$\nand\n$$\ny_t = \\alpha + \\beta(t)\\, x_t + \\sigma_u \\,\\eta_t,\n$$\nwhere $\\{\\varepsilon_t\\}$ and $\\{\\eta_t\\}$ are independent sequences of independent and identically distributed standard normal random variables, and $\\beta(t)$ is a piecewise constant function\n$$\n\\beta(t) = \\begin{cases}\n\\beta_1,  t \\le T_b,\\\\\n\\beta_2,  t > T_b.\n\\end{cases}\n$$\nThe constant $T_b$ denotes the (potential) break index at which the long-run cointegrating slope changes from $\\beta_1$ to $\\beta_2$. If no break is intended, take $T_b \\ge T$ so that $\\beta(t) = \\beta_1$ for all $t$. The intercept $\\alpha$ is constant.\n\nThe shocks $\\{\\varepsilon_t\\}$ and $\\{\\eta_t\\}$ must be generated deterministically from a specified uniform pseudo-random sequence using a linear congruential generator to ensure reproducibility. Define the multiplicative linear congruential sequence $\\{U_k\\}$ on integers by\n$$\nU_{k} = (a \\cdot U_{k-1}) \\bmod m,\\quad k \\ge 1,\\quad U_0 = s,\n$$\nwith modulus $m = 2^{31}-1$, multiplier $a = 16807$, and integer seed $s \\in \\{1,2,\\dots,m-1\\}$. Map to uniforms $R_k \\in (0,1)$ via $R_k = U_k/m$. Convert to independent standard normal innovations using the Box–Muller transform: for each pair $(R_{2j-1}, R_{2j})$,\n$$\nZ_{2j-1} = \\sqrt{-2\\ln R_{2j-1}}\\,\\cos(2\\pi R_{2j}),\\quad\nZ_{2j} = \\sqrt{-2\\ln R_{2j-1}}\\,\\sin(2\\pi R_{2j}),\n$$\nwhere all angles in the trigonometric functions are in radians. Use the first $T$ normal draws for $\\{\\varepsilon_t\\}$ and the next $T$ normal draws for $\\{\\eta_t\\}$.\n\nFor detection, define a recursive sequence of Ordinary Least Squares (OLS) estimators for the cointegrating relation $y_s = \\alpha + \\beta x_s + \\text{error}$, computed on expanding samples. For each $t \\in \\{T_{\\min}, T_{\\min}+1, \\dots, T-2\\}$, estimate $(\\widehat{\\alpha}_t,\\widehat{\\beta}_t)$ by minimizing $\\sum_{s=0}^{t} (y_s - \\alpha - \\beta x_s)^2$. Equivalently, with $n_t = t+1$, sample means $\\overline{x}_t = \\frac{1}{n_t}\\sum_{s=0}^{t} x_s$ and $\\overline{y}_t = \\frac{1}{n_t}\\sum_{s=0}^{t} y_s$, and centered sums $S_{xx,t} = \\sum_{s=0}^{t} (x_s-\\overline{x}_t)^2$, $S_{xy,t} = \\sum_{s=0}^{t} (x_s-\\overline{x}_t)(y_s-\\overline{y}_t)$,\n$$\n\\widehat{\\beta}_t = \\frac{S_{xy,t}}{S_{xx,t}},\\quad \\widehat{\\alpha}_t = \\overline{y}_t - \\widehat{\\beta}_t\\,\\overline{x}_t.\n$$\nLet the residual standard deviation be\n$$\n\\widehat{\\sigma}_t = \\sqrt{\\frac{1}{n_t - 2}\\sum_{s=0}^{t} \\left(y_s - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_s\\right)^2},\n$$\ndefined for $n_t \\ge 3$. Form the one-step-ahead standardized forecast error at time $t+1$,\n$$\nz_{t+1} = \\frac{y_{t+1} - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_{t+1}}{\\widehat{\\sigma}_t}.\n$$\nGiven a threshold $c > 0$ and a run length $L \\in \\mathbb{N}$, define the indicator $I_s = \\mathbf{1}\\{|z_s| \\ge c\\}$ for $s \\in \\{T_{\\min}+1, \\dots, T-1\\}$. The detected break index is the smallest index $s^\\star$ such that $I_{s^\\star} = I_{s^\\star+1} = \\dots = I_{s^\\star+L-1} = 1$. If no such index exists, return $-1$. Report the detected index $s^\\star$ in zero-based time indexing.\n\nImplement the above data construction and detection as a complete program for the following test suite of parameter sets, each specified as a tuple\n$$\n(T, T_b, \\alpha, \\beta_1, \\beta_2, \\sigma_e, \\sigma_u, T_{\\min}, c, L, s).\n$$\n- Test case A: $(240, 150, 0.5, 1.0, 1.6, 1.0, 0.05, 40, 3.0, 2, 12345)$.\n- Test case B: $(240, 10^9, 0.5, 1.0, 1.0, 1.0, 0.05, 40, 3.0, 2, 54321)$.\n- Test case C: $(220, 210, 0.3, 0.8, 1.9, 0.9, 0.05, 30, 2.5, 2, 20231107)$.\n- Test case D: $(180, 60, 0.2, 1.2, 1.4, 0.8, 0.1, 25, 2.0, 3, 424242)$.\n\nYour program should compute and return, for each test case, a single integer: either the detected break index $s^\\star$ (in zero-based indexing) or $-1$ if no break is detected under the rule. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[rA,rB,rC,rD]\"), where $rA$, $rB$, $rC$, and $rD$ are the integers corresponding to test cases A, B, C, and D, respectively.",
            "solution": "The problem presents a well-defined task in computational econometrics: the detection of a structural break in a linear cointegrating relationship between two non-stationary time series. The entire procedure, from data generation to break detection, is specified algorithmically, which permits a unique and verifiable solution. A rigorous analysis proceeds in two stages: first, the deterministic generation of the synthetic data, and second, the application of a recursive monitoring algorithm.\n\nThe foundation of this exercise is a reproducible data generation process. To ensure this, we do not use system-level sources of randomness but instead construct the required random sequences from a specified seed, $s$, using a multiplicative linear congruential generator (LCG). The sequence of integers $\\{U_k\\}$ is given by the recurrence $U_{k} = (a \\cdot U_{k-1}) \\bmod m$ for $k \\ge 1$, with $U_0 = s$. The problem specifies the industry-standard parameters $m = 2^{31}-1$ and $a = 16807$. These integers are mapped to a sequence of uniform pseudo-random numbers $\\{R_k\\}$ in the interval $(0, 1)$ via the transformation $R_k = U_k/m$.\n\nFrom this uniform sequence, we must generate independent standard normal random variates. The Box-Muller transform is specified for this purpose. For each pair of uniform variates $(R_{2j-1}, R_{2j})$, we produce a pair of independent standard normal variates $(Z_{2j-1}, Z_{2j})$:\n$$\nZ_{2j-1} = \\sqrt{-2\\ln R_{2j-1}}\\,\\cos(2\\pi R_{2j})\n$$\n$$\nZ_{2j} = \\sqrt{-2\\ln R_{2j-1}}\\,\\sin(2\\pi R_{2j})\n$$\nThe problem requires a total of $2T$ normal draws to construct the series. The first $T$ draws form the sequence $\\{\\varepsilon_t\\}_{t=0}^{T-1}$, and the subsequent $T$ draws form $\\{\\eta_t\\}_{t=0}^{T-1}$.\n\nWith these shock sequences, the time series are constructed. The series $\\{x_t\\}$ is an I($1$) process, specifically a random walk without drift, starting from $x_0 = 0$:\n$$\nx_t = x_{t-1} + \\sigma_e \\,\\varepsilon_t, \\quad \\text{for } t \\in \\{1, 2, \\dots, T-1\\}.\n$$\nThe series $\\{y_t\\}$ is constructed to be cointegrated with $\\{x_t\\}$, subject to a potential structural break. Its definition is:\n$$\ny_t = \\alpha + \\beta(t)\\, x_t + \\sigma_u \\,\\eta_t, \\quad \\text{for } t \\in \\{0, 1, \\dots, T-1\\}.\n$$\nThe parameter $\\beta(t)$ is a step function that changes its value at the break index $T_b$:\n$$\n\\beta(t) = \\beta_1 \\text{ if } t \\le T_b, \\quad \\text{and} \\quad \\beta(t) = \\beta_2 \\text{ if } t > T_b.\n$$\nA value of $T_b \\ge T$ indicates that no break occurs within the sample period, so $\\beta(t) = \\beta_1$ for all $t$.\n\nThe second stage is the detection of the break point. This is accomplished using a recursive monitoring scheme based on Ordinary Least Squares (OLS) estimation over an expanding data window. For each time index $t$ from a starting point $T_{\\min}$ up to $T-2$, we estimate the coefficients of the static cointegrating regression $y_s = \\alpha + \\beta x_s + \\text{error}$ using all available data from $s=0$ to $s=t$.\n\nTo perform this estimation efficiently, we do not recompute the required sums for each regression. Instead, we maintain running sums of the relevant quantities: $\\sum x_s$, $\\sum y_s$, $\\sum x_s^2$, $\\sum y_s^2$, and $\\sum x_s y_s$. At each step $t$, we update these sums with the new data point $(x_t, y_t)$. Let $n_t = t+1$ be the sample size. The OLS estimates $(\\widehat{\\alpha}_t, \\widehat{\\beta}_t)$ are then calculated using the standard formulae derived from these sums:\n$$\n\\widehat{\\beta}_t = \\frac{\\sum_{s=0}^{t} x_s y_s - n_t \\overline{x}_t \\overline{y}_t}{\\sum_{s=0}^{t} x_s^2 - n_t \\overline{x}_t^2}\n\\quad \\text{and} \\quad\n\\widehat{\\alpha}_t = \\overline{y}_t - \\widehat{\\beta}_t \\overline{x}_t,\n$$\nwhere $\\overline{x}_t$ and $\\overline{y}_t$ are the sample means over $\\{0, \\dots, t\\}$.\n\nThe core of the detection mechanism is the one-step-ahead standardized forecast error, $z_{t+1}$. This statistic measures how surprising the next observation, $(x_{t+1}, y_{t+1})$, is, given the model estimated up to time $t$. It is defined as:\n$$\nz_{t+1} = \\frac{y_{t+1} - (\\widehat{\\alpha}_t + \\widehat{\\beta}_t x_{t+1})}{\\widehat{\\sigma}_t}\n$$\nThe denominator, $\\widehat{\\sigma}_t$, is the estimated standard deviation of the regression residuals, which serves to standardize the forecast error. It is computed as:\n$$\n\\widehat{\\sigma}_t = \\sqrt{\\frac{1}{n_t - 2}\\sum_{s=0}^{t} (y_s - \\widehat{\\alpha}_t - \\widehat{\\beta}_t x_s)^2}\n$$\nThe sum of squared residuals in the numerator can be calculated efficiently as $S_{yy,t} - \\widehat{\\beta}_t S_{xy,t}$, where $S_{yy,t}$ and $S_{xy,t}$ are the centered sum of squares and cross-products.\n\nWhen a structural break occurs, the OLS estimates, which are based on data that may span both pre-break and post-break regimes, become biased. This bias produces systematically poor forecasts for subsequent periods, leading to a sequence of large forecast errors. The detection rule formalizes this intuition. We define an indicator $I_s = 1$ if $|z_s| \\ge c$ and $I_s = 0$ otherwise, for a given threshold $c$. A break is signaled at the first time index $s^\\star$ that initiates a sequence of at least $L$ consecutive large standardized forecast errors, i.e., $I_{s^\\star} = I_{s^\\star+1} = \\dots = I_{s^\\star+L-1} = 1$. The search for $s^\\star$ begins at index $T_{\\min}+1$. If no such sequence is found within the observation window, we conclude no break was detected and report $-1$. The entire procedure is implemented deterministically for each provided parameter set.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # (T, T_b, alpha, beta_1, beta_2, sigma_e, sigma_u, T_min, c, L, s)\n        (240, 150, 0.5, 1.0, 1.6, 1.0, 0.05, 40, 3.0, 2, 12345),\n        (240, 10**9, 0.5, 1.0, 1.0, 1.0, 0.05, 40, 3.0, 2, 54321),\n        (220, 210, 0.3, 0.8, 1.9, 0.9, 0.05, 30, 2.5, 2, 20231107),\n        (180, 60, 0.2, 1.2, 1.4, 0.8, 0.1, 25, 2.0, 3, 424242),\n    ]\n\n    results = []\n    for params in test_cases:\n        result = _run_case(params)\n        results.append(result)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _run_case(params):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    T, T_b, alpha, beta_1, beta_2, sigma_e, sigma_u, T_min, c, L, s = params\n\n    # --- 1. Data Generation ---\n\n    # LCG parameters\n    m = 2**31 - 1\n    a = 16807\n    \n    # Generate 2*T uniform random numbers\n    uniforms = np.zeros(2 * T)\n    U_k = s\n    for k in range(2 * T):\n        U_k = (a * U_k) % m\n        uniforms[k] = U_k / m\n\n    # Generate 2*T standard normal random numbers using Box-Muller transform\n    normals = np.zeros(2 * T)\n    for j in range(T):\n        R1 = uniforms[2*j]\n        R2 = uniforms[2*j+1]\n        \n        log_R1 = np.log(R1)\n        term1 = np.sqrt(-2.0 * log_R1)\n        term2 = 2.0 * np.pi * R2\n\n        normals[2*j] = term1 * np.cos(term2)\n        normals[2*j+1] = term1 * np.sin(term2)\n    \n    epsilons = normals[:T]\n    etas = normals[T:]\n\n    # Generate time series x_t and y_t\n    x = np.zeros(T, dtype=np.float64)\n    y = np.zeros(T, dtype=np.float64)\n    \n    # x_0 = 0 is default\n    for t in range(1, T):\n        x[t] = x[t-1] + sigma_e * epsilons[t]\n\n    for t in range(T):\n        beta_t = beta_1 if t = T_b else beta_2\n        y[t] = alpha + beta_t * x[t] + sigma_u * etas[t]\n\n    # --- 2. Break Detection ---\n    \n    # Array to store indicators of threshold exceedance\n    z_indicators = np.zeros(T, dtype=int)\n    \n    # Initialize running sums for recursive OLS\n    s_x, s_y, s_x2, s_y2, s_xy = 0.0, 0.0, 0.0, 0.0, 0.0\n    \n    # Pre-computation for the initial window up to T_min\n    for t in range(T_min):\n        s_x += x[t]\n        s_y += y[t]\n        s_x2 += x[t]**2\n        s_y2 += y[t]**2\n        s_xy += x[t] * y[t]\n\n    # Recursive estimation and forecast error calculation\n    for t in range(T_min, T - 1):\n        # Update sums with the value at time t\n        s_x += x[t]\n        s_y += y[t]\n        s_x2 += x[t]**2\n        s_y2 += y[t]**2\n        s_xy += x[t] * y[t]\n        \n        n_t = t + 1\n        \n        # Calculate OLS estimates\n        x_bar = s_x / n_t\n        y_bar = s_y / n_t\n        \n        s_xx = s_x2 - n_t * x_bar**2\n        s_yy = s_y2 - n_t * y_bar**2\n        s_xy_t = s_xy - n_t * x_bar * y_bar\n        \n        # Avoid division by zero, though unlikely with a random walk\n        if s_xx == 0:\n            continue\n            \n        beta_hat = s_xy_t / s_xx\n        alpha_hat = y_bar - beta_hat * x_bar\n        \n        # Calculate residual standard deviation\n        ssr = s_yy - beta_hat * s_xy_t\n        # Ensure non-negativity due to potential floating point inaccuracies\n        ssr = max(0, ssr)\n        \n        df = n_t - 2\n        if df = 0:\n            continue\n            \n        sigma_hat_sq = ssr / df\n        \n        if sigma_hat_sq = 0:\n            continue \n        \n        sigma_hat = np.sqrt(sigma_hat_sq)\n        \n        # Calculate standardized one-step-ahead forecast error for time t+1\n        forecast_error = y[t+1] - (alpha_hat + beta_hat * x[t+1])\n        z_t_plus_1 = forecast_error / sigma_hat if sigma_hat > 0 else np.inf\n        \n        if np.abs(z_t_plus_1) >= c:\n            z_indicators[t + 1] = 1\n\n    # --- 3. Search for Break Index ---\n    \n    # Search for the first run of L consecutive indicators\n    # The search range for the start of the run (s_star)\n    # is from T_min+1 to T-L (inclusive).\n    for s_star in range(T_min + 1, T - L + 1):\n        # Check for a run of length L\n        if np.all(z_indicators[s_star : s_star + L] == 1):\n            return s_star\n\n    return -1\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}