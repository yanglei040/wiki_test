{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of the bootstrap method is its ability to approximate the sampling distribution of complex statistics for which analytical formulas are unavailable or unwieldy. The sample median is a prime example, especially for skewed data like financial returns where it serves as a more robust measure of central tendency than the mean. This first exercise  walks you through the fundamental mechanics of constructing a non-parametric bootstrap confidence interval for the median, providing a solid foundation for all subsequent applications.",
            "id": "2377547",
            "problem": "Consider a dataset of venture capital deal-level simple returns defined for each deal $i$ as $r_i = \\dfrac{\\text{cash\\_out}_i - \\text{cash\\_in}_i}{\\text{cash\\_in}_i}$, so that $r_i \\in [-1, +\\infty)$. Assume the observed returns are realizations from an unknown distribution, and that the data are independent and identically distributed (IID). Let the sample median of a multiset $x_1, x_2, \\ldots, x_n$ be defined as the middle order statistic when $n$ is odd, and as the average of the two middle order statistics when $n$ is even.\n\nYour task is to compute, for each test case below, a two-sided confidence interval for the population median based on the following definition of the bootstrap percentile interval. Let $T$ denote the sample median functional. For a given dataset $D = [x_1, x_2, \\ldots, x_n]$, define the empirical distribution $\\hat{F}_n$ that places mass $1/n$ on each observed value (counting multiplicities). For a given integer $B \\ge 1$, draw $B$ IID bootstrap resamples, each of size $n$, from $\\hat{F}_n$ (sampling with replacement from the multiset $D$). For each resample $b \\in \\{1, 2, \\ldots, B\\}$, compute $T_b = T(D^{\\ast}_b)$, the sample median of the $b$-th resample. Let $T_{(1)} \\le T_{(2)} \\le \\cdots \\le T_{(B)}$ be the sorted values. For a nominal two-sided confidence level $1 - \\alpha \\in (0,1)$, define the lower endpoint as the empirical $p$-quantile with $p = \\alpha/2$, and the upper endpoint as the empirical $q$-quantile with $q = 1 - \\alpha/2$, where for any probability level $u \\in [0,1]$ the empirical quantile $Q(u)$ is defined by linear interpolation on sorted indices:\n- Let $k = 1 + (B - 1)u$.\n- If $k \\le 1$, set $Q(u) = T_{(1)}$; if $k \\ge B$, set $Q(u) = T_{(B)}$.\n- Otherwise, write $k = m + \\delta$ with $m = \\lfloor k \\rfloor$ and $\\delta \\in (0,1)$, and set $Q(u) = (1 - \\delta) T_{(m)} + \\delta T_{(m+1)}$.\n\nFor reproducibility, for each test case use a pseudo-random number generator initialized with the specified integer seed before drawing the $B$ bootstrap resamples.\n\nTest suite:\n- Case $1$: $D_1 = [-1.0, -0.8, -0.6, -0.5, -0.3, -0.1, 0.0, 0.2, 0.25, 0.35, 0.4, 0.5, 0.6, 0.9, 1.2, 1.5, 2.0, 3.0, 5.0]$, $B_1 = 20000$, $\\alpha_1 = 0.10$, seed $S_1 = 202311$.\n- Case $2$: $D_2 = [-1.0, -0.9, -0.9, -0.5, -0.2, -0.1, 0.0, 0.05]$, $B_2 = 15000$, $\\alpha_2 = 0.20$, seed $S_2 = 7$.\n- Case $3$: $D_3 = [-1.0, -1.0, -0.5, -0.5, 0.0, 0.0, 0.4, 0.4, 0.4, 1.0, 1.0, 2.5]$, $B_3 = 30000$, $\\alpha_3 = 0.05$, seed $S_3 = 12345$.\n- Case $4$: $D_4 = [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]$, $B_4 = 10000$, $\\alpha_4 = 0.10$, seed $S_4 = 99$.\n\nYour program must compute for each case the pair $[\\ell, u] = [Q(\\alpha/2), Q(1 - \\alpha/2)]$ as defined above. Each bound must be reported as a decimal number rounded to six digits after the decimal point.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, where each case is a two-element list $[\\ell, u]$. For example: $[[\\ell_1,u_1],[\\ell_2,u_2],[\\ell_3,u_3],[\\ell_4,u_4]]$ with each numeric value rounded to six digits after the decimal point.",
            "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and provides a complete and consistent set of givens for a standard computational statistics problem. The task is to compute bootstrap percentile confidence intervals for the population median, given several datasets and parameters.\n\nThe method to be implemented is the bootstrap percentile interval. This is a non-parametric statistical technique used to estimate the sampling distribution of a statistic, and thereby construct confidence intervals, without making strong assumptions about the underlying population distribution. The fundamental principle of the bootstrap is to use the empirical distribution function, $\\hat{F}_n$, of the observed sample $D = \\{x_1, x_2, \\ldots, x_n\\}$ as an approximation to the true, unknown population distribution $F$. In $\\hat{F}_n$, each observed data point $x_i$ is assigned a probability mass of $1/n$.\n\nThe algorithm proceeds as follows:\n$1$.\nFor a given dataset $D$ of size $n$, we draw a large number, $B$, of bootstrap resamples. Each resample, denoted $D^{\\ast}_b$ for $b \\in \\{1, 2, \\ldots, B\\}$, is of size $n$ and is obtained by sampling with replacement from the original dataset $D$. This process is a computational realization of drawing IID samples from the empirical distribution $\\hat{F}_n$.\n\n$2$.\nFor each bootstrap resample $D^{\\ast}_b$, we compute the statistic of interest. In this problem, the statistic is the sample median, denoted as $T$. Let $T_b = T(D^{\\ast}_b)$ be the sample median of the $b$-th resample. The sample median is defined as the middle order statistic for an odd-sized sample and the arithmetic mean of the two middle order statistics for an even-sized sample.\n\n$3$.\nThe collection of $B$ bootstrap statistics, $\\{T_1, T_2, \\ldots, T_B\\}$, serves as an empirical approximation to the sampling distribution of the sample median, $T$.\n\n$4$.\nTo construct a two-sided confidence interval with a nominal confidence level of $1-\\alpha$, we use the percentile method. This involves finding the appropriate quantiles of the sorted bootstrap statistics, $T_{(1)} \\le T_{(2)} \\le \\cdots \\le T_{(B)}$. The lower bound of the confidence interval, $\\ell$, is the empirical $(\\alpha/2)$-quantile of this distribution, and the upper bound, $u$, is the empirical $(1 - \\alpha/2)$-quantile.\n\n$5$.\nThe problem provides a precise definition for the calculation of the empirical quantile, $Q(u)$, for any probability level $u \\in [0,1]$. It uses linear interpolation between order statistics. The procedure involves calculating an index $k = 1 + (B - 1)u$ and, if $1  k  B$, interpolating between the values $T_{(\\lfloor k \\rfloor)}$ and $T_{(\\lfloor k \\rfloor + 1)}$. This specific rule corresponds to the standard linear interpolation method for quantiles, implemented for instance in the `numpy.quantile` function with the `interpolation='linear'` option. Using this specific function ensures adherence to the problem definition.\n\n$6$.\nFor scientific reproducibility, the pseudo-random number generator used for resampling must be initialized with a specific seed for each test case. This ensures that the sequence of bootstrap resamples is deterministic and the result can be independently verified.\n\nThe following implementation will execute this algorithm for each of the four test cases provided, computing the specified confidence interval bounds and rounding them to six decimal places as required. For the special case where all data points in the original sample are identical, such as in Case $4$, any bootstrap resample will also consist of identical values. Consequently, the median of every bootstrap resample will be this same value, and the resulting confidence interval will collapse to a single point.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are required or permitted.\n\ndef solve():\n    \"\"\"\n    Computes bootstrap percentile confidence intervals for the median for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            'D': [-1.0, -0.8, -0.6, -0.5, -0.3, -0.1, 0.0, 0.2, 0.25, 0.35, 0.4, 0.5, 0.6, 0.9, 1.2, 1.5, 2.0, 3.0, 5.0],\n            'B': 20000,\n            'alpha': 0.10,\n            'seed': 202311\n        },\n        # Case 2\n        {\n            'D': [-1.0, -0.9, -0.9, -0.5, -0.2, -0.1, 0.0, 0.05],\n            'B': 15000,\n            'alpha': 0.20,\n            'seed': 7\n        },\n        # Case 3\n        {\n            'D': [-1.0, -1.0, -0.5, -0.5, 0.0, 0.0, 0.4, 0.4, 0.4, 1.0, 1.0, 2.5],\n            'B': 30000,\n            'alpha': 0.05,\n            'seed': 12345\n        },\n        # Case 4\n        {\n            'D': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n            'B': 10000,\n            'alpha': 0.10,\n            'seed': 99\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract parameters for the current case.\n        D = np.array(case['D'])\n        B = case['B']\n        alpha = case['alpha']\n        seed = case['seed']\n        n = len(D)\n\n        # 1. Initialize the pseudo-random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 2. Generate B bootstrap resamples and compute the median for each.\n        # Pre-allocate array for efficiency.\n        bootstrap_medians = np.zeros(B)\n        for i in range(B):\n            # Draw a bootstrap resample of size n with replacement.\n            resample = rng.choice(D, size=n, replace=True)\n            # Compute the median of the resample.\n            bootstrap_medians[i] = np.median(resample)\n\n        # 3. Sort the bootstrap medians to prepare for quantile calculation.\n        bootstrap_medians.sort()\n\n        # 4. Compute the lower and upper quantiles for the confidence interval.\n        # The problem's quantile definition matches numpy's 'linear' interpolation.\n        p_lower = alpha / 2.0\n        p_upper = 1.0 - alpha / 2.0\n\n        lower_bound = np.quantile(bootstrap_medians, p_lower, interpolation='linear')\n        upper_bound = np.quantile(bootstrap_medians, p_upper, interpolation='linear')\n\n        # 5. Round the results to six decimal places as required by the problem.\n        l_rounded = round(lower_bound, 6)\n        u_rounded = round(upper_bound, 6)\n        \n        results.append([l_rounded, u_rounded])\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, with numeric values rounded to 6 decimal places.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function.\nsolve()\n```"
        },
        {
            "introduction": "The standard bootstrap is non-parametric, meaning it makes no assumptions about the underlying data distribution beyond what is contained in the sample itself. However, what if we have good reason to believe our data follows a specific parametric form, such as a Gamma distribution for non-negative quantities like insurance losses or transaction durations? This exercise  guides you through a comparison of the standard \"empirical\" bootstrap and a \"model-based\" parametric bootstrap. By implementing both, you will gain firsthand insight into the critical trade-off between the efficiency gained from correct model assumptions and the robustness of the non-parametric approach.",
            "id": "2377478",
            "problem": "You are given independent and identically distributed observations that represent nonnegative economic quantities such as transaction durations or loss severities. Assume that the true data-generating process for each test case is a Gamma family with unknown parameters. Your task is to construct two-sided confidence intervals for the population mean using two different resampling paradigms and to evaluate their behavior against known ground truth.\n\nLet the sample be denoted by $\\{x_1,\\dots,x_n\\}$ with $n \\in \\mathbb{N}$. Let the Gamma family be parameterized by a shape parameter $k \\in (0,\\infty)$ and a scale parameter $\\theta \\in (0,\\infty)$ so that the population mean is $\\mu = k \\theta$. For each test case below, the data must be generated by drawing a sample of size $n$ from the stated Gamma distribution with the stated data-generation seed, and then two two-sided confidence intervals for $\\mu$ at coverage level $1-\\alpha$ must be constructed as follows:\n\n- Model-based resampling: Impose the Gamma family on the unknown distribution of the data and estimate $(k,\\theta)$ from the realized sample by maximum likelihood under the restriction that the location parameter is zero. Use the fitted model to approximate the sampling distribution of the sample mean by resampling.\n- Empirical resampling: Treat the empirical distribution that places mass $1/n$ on each observed $x_i$ as the data-generating distribution and approximate the sampling distribution of the sample mean by resampling.\n\nIn both paradigms, approximate the distribution of the sample mean by generating $B$ independent resamples of size $n$ from the respective distribution, computing the sample mean for each resample, and then taking the lower and upper endpoints to be the empirical $\\alpha/2$ and $1-\\alpha/2$ quantiles of those resampled means.\n\nFor reproducibility, the following requirements apply:\n\n- For each test case, the dataset must be generated using a separate pseudorandom stream initialized with the stated data-generation seed for that case.\n- For all resampling operations across all test cases, use a single pseudorandom stream initialized once with the seed $2025$.\n- Use the same $B$ and $\\alpha$ for all test cases.\n\nTest suite:\n\n- Case $1$: shape $k=2.5$, scale $\\theta=1.2$, sample size $n=80$, data-generation seed $1729$.\n- Case $2$: shape $k=1.1$, scale $\\theta=2.0$, sample size $n=20$, data-generation seed $2027$.\n- Case $3$: shape $k=5.0$, scale $\\theta=0.5$, sample size $n=50$, data-generation seed $12345$.\n- Case $4$: shape $k=0.7$, scale $\\theta=1.5$, sample size $n=12$, data-generation seed $314159$.\n\nCommon settings: nominal coverage $1-\\alpha=0.95$ (so $\\alpha=0.05$), number of resamples $B=10000$.\n\nFor each case, in the order $1$ through $4$, compute and record the following $7$ quantities in order:\n\n- $L_{\\mathrm{M}}$, the lower endpoint of the model-based interval.\n- $U_{\\mathrm{M}}$, the upper endpoint of the model-based interval.\n- $L_{\\mathrm{E}}$, the lower endpoint of the empirical-resampling interval.\n- $U_{\\mathrm{E}}$, the upper endpoint of the empirical-resampling interval.\n- An indicator $I$ that equals $\\mathrm{True}$ if the model-based interval length is strictly smaller than the empirical-resampling interval length, and $\\mathrm{False}$ otherwise.\n- A coverage indicator $C_{\\mathrm{M}}$ that equals $\\mathrm{True}$ if the true mean $\\mu = k \\theta$ lies within the model-based interval $[L_{\\mathrm{M}},U_{\\mathrm{M}}]$, and $\\mathrm{False}$ otherwise.\n- A coverage indicator $C_{\\mathrm{E}}$ that equals $\\mathrm{True}$ if the true mean $\\mu = k \\theta$ lies within the empirical-resampling interval $[L_{\\mathrm{E}},U_{\\mathrm{E}}]$, and $\\mathrm{False}$ otherwise.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must concatenate the $7$ outputs from case $1$, then the $7$ outputs from case $2$, then case $3$, then case $4$, in that order. For example, the output must have the form $[L_{\\mathrm{M}}^{(1)},U_{\\mathrm{M}}^{(1)},L_{\\mathrm{E}}^{(1)},U_{\\mathrm{E}}^{(1)},I^{(1)},C_{\\mathrm{M}}^{(1)},C_{\\mathrm{E}}^{(1)},\\dots,L_{\\mathrm{M}}^{(4)},U_{\\mathrm{M}}^{(4)},L_{\\mathrm{E}}^{(4)},U_{\\mathrm{E}}^{(4)},I^{(4)},C_{\\mathrm{M}}^{(4)},C_{\\mathrm{E}}^{(4)}]$, where superscripts denote the case index. All numbers are real numbers, and indicators are boolean values. There are no physical units required.",
            "solution": "The problem concerns constructing and comparing two approximate confidence intervals for the population mean of a positive-valued economic variable when the data are assumed independent and identically distributed. The central quantity of interest is the population mean $\\mu = \\mathbb{E}[X]$, where $X$ denotes a single observation.\n\nFoundational setup:\n\n- Let $X_1,\\dots,X_n$ be independent and identically distributed with common distribution supported on $[0,\\infty)$.\n- The sample mean is $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n- We seek a two-sided interval $[L,U]$ of nominal coverage $1-\\alpha$, such that $\\mathbb{P}(\\mu \\in [L,U]) \\approx 1-\\alpha$.\n\nResampling logic from first principles:\n\n- The fundamental bootstrap idea is to approximate the sampling distribution of a statistic by repeated sampling under a distribution that is either a fitted parametric model or the empirical distribution. For a statistic $T = t(X_1,\\dots,X_n)$, the sampling distribution of $T$ under the true model is generally unknown. However, one may approximate it by resampling from a proxy distribution and recomputing $T$ repeatedly to build an empirical distribution for $T$.\n\nTwo resampling paradigms required here:\n\n- Model-based resampling under the Gamma family:\n\n  - Postulate a parametric family $\\{ \\mathrm{Gamma}(k,\\theta) : k0,\\ \\theta0\\}$ for the distribution of $X_i$. The probability density function is\n    $$ f(x;k,\\theta) = \\frac{1}{\\Gamma(k)\\,\\theta^k} x^{k-1} e^{-x/\\theta}, \\quad x\\ge 0, $$\n    where $\\Gamma(\\cdot)$ is the Gamma function, $k$ is a shape parameter, and $\\theta$ is a scale parameter, implying a mean $\\mu = k\\theta$.\n  - Estimate $(k,\\theta)$ by maximum likelihood from the observed sample under the constraint that the location is zero. The maximum likelihood estimator $(\\hat{k},\\hat{\\theta})$ is defined by\n    $$ (\\hat{k},\\hat{\\theta}) \\in \\arg\\max_{k0,\\ \\theta0} \\sum_{i=1}^n \\log f(x_i;k,\\theta). $$\n    This estimation uses only the observed sample and the Gamma family.\n  - To approximate the sampling distribution of $\\bar{X}_n$, generate $B$ independent resamples, each consisting of $n$ independent draws from $\\mathrm{Gamma}(\\hat{k},\\hat{\\theta})$. For each resample $b\\in\\{1,\\dots,B\\}$, compute the resample mean $\\bar{X}_n^{(b)}$. The empirical distribution of $\\{\\bar{X}_n^{(1)},\\dots,\\bar{X}_n^{(B)}\\}$ approximates the sampling distribution of $\\bar{X}_n$ under the parametric model.\n\n- Empirical resampling:\n\n  - Consider the empirical distribution $\\hat{F}_n$ that assigns mass $1/n$ to each observed $x_i$.\n  - Generate $B$ independent resamples, each formed by sampling $n$ observations independently from $\\hat{F}_n$ (sampling with replacement from the observed sample). For each resample $b\\in\\{1,\\dots,B\\}$, compute the resample mean $\\bar{X}_n^{*(b)}$. The empirical distribution of $\\{\\bar{X}_n^{*(1)},\\dots,\\bar{X}_n^{*(B)}\\}$ approximates the sampling distribution of $\\bar{X}_n$ under the empirical distribution.\n\nInterval construction principle:\n\n- For either set of resample means $\\{M_b\\}_{b=1}^B$, define the two-sided $(1-\\alpha)$ interval by the empirical quantiles,\n  $$ L = \\mathrm{Quantile}_{\\alpha/2}\\left(\\{M_b\\}_{b=1}^B\\right), \\quad U = \\mathrm{Quantile}_{1-\\alpha/2}\\left(\\{M_b\\}_{b=1}^B\\right). $$\n  This is the percentile method, which directly uses the approximate sampling distribution of the statistic to set endpoints.\n\nDeterminism and reproducibility:\n\n- Determinism is ensured by fixing the pseudorandom number generator seeds. For each test case, the data are generated from a Gamma distribution with known shape $k$ and scale $\\theta$ using the specified data-generation seed, yielding a concrete realized sample. For resampling, a single pseudorandom stream with a fixed seed is used for all $B$ resamples across all cases.\n\nVerification and comparison:\n\n- For each case, the true mean is $\\mu_{\\mathrm{true}} = k\\theta$. After obtaining the model-based interval $[L_{\\mathrm{M}},U_{\\mathrm{M}}]$ and the empirical-resampling interval $[L_{\\mathrm{E}},U_{\\mathrm{E}}]$, compute:\n  - The length comparison indicator $I = \\mathbf{1}\\{(U_{\\mathrm{M}}-L_{\\mathrm{M}})  (U_{\\mathrm{E}}-L_{\\mathrm{E}})\\}$.\n  - Coverage indicators $C_{\\mathrm{M}} = \\mathbf{1}\\{\\mu_{\\mathrm{true}} \\in [L_{\\mathrm{M}},U_{\\mathrm{M}}]\\}$ and $C_{\\mathrm{E}} = \\mathbf{1}\\{\\mu_{\\mathrm{true}} \\in [L_{\\mathrm{E}},U_{\\mathrm{E}}]\\}$.\n\nAlgorithmic mapping:\n\n- For each test case $(k,\\theta,n,\\text{seed})$:\n  - Generate the dataset $x_1,\\dots,x_n$ by sampling $n$ independent draws from $\\mathrm{Gamma}(k,\\theta)$ using the stated data-generation seed.\n  - Compute the maximum likelihood estimates $(\\hat{k},\\hat{\\theta})$ for the Gamma family with zero location.\n  - With a fixed resampling seed, generate $B$ model-based resamples of size $n$ from $\\mathrm{Gamma}(\\hat{k},\\hat{\\theta})$ and compute the vector of resample means; compute $L_{\\mathrm{M}}$ and $U_{\\mathrm{M}}$ as the $\\alpha/2$ and $1-\\alpha/2$ empirical quantiles of those means.\n  - With the same fixed resampling seed stream (continuing the same pseudorandom generator), generate $B$ empirical resamples by sampling with replacement from $\\{x_i\\}_{i=1}^n$ and compute the vector of resample means; compute $L_{\\mathrm{E}}$ and $U_{\\mathrm{E}}$ as the $\\alpha/2$ and $1-\\alpha/2$ empirical quantiles.\n  - Compute $I$, $C_{\\mathrm{M}}$, and $C_{\\mathrm{E}}$ as defined above.\n\nComplexity and numerical considerations:\n\n- Each resampling step is vectorized: generating a matrix of size $B \\times n$ and averaging along rows gives the $B$ means efficiently. Quantiles are then computed over $B$ values. With $B=10000$ and moderate $n$, the total computation remains tractable.\n- Fixing the resampling seed ensures reproducible results, which is necessary for a deterministic single-line output.\n\nThe requested output is the concatenation, in order, of $[L_{\\mathrm{M}},U_{\\mathrm{M}},L_{\\mathrm{E}},U_{\\mathrm{E}},I,C_{\\mathrm{M}},C_{\\mathrm{E}}]$ for cases $1$ through $4$, printed as a single list on one line.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import gamma as sp_gamma\n\ndef percentile_interval(samples, alpha):\n    # Compute the (alpha/2, 1 - alpha/2) empirical quantiles\n    q_low = np.quantile(samples, alpha / 2.0, method=\"linear\")\n    q_high = np.quantile(samples, 1.0 - alpha / 2.0, method=\"linear\")\n    return float(q_low), float(q_high)\n\ndef generate_data(shape_k, scale_theta, n, seed):\n    rng = np.random.default_rng(seed)\n    data = rng.gamma(shape_k, scale_theta, size=n)\n    return data\n\ndef fit_gamma_mle_zero_loc(data):\n    # Fit Gamma distribution with zero location via maximum likelihood\n    # scipy.stats.gamma.fit returns (shape, loc, scale)\n    # We fix location to 0 using floc=0\n    k_hat, loc_hat, theta_hat = sp_gamma.fit(data, floc=0.0)\n    # loc_hat should be 0.0 by construction\n    return float(k_hat), float(theta_hat)\n\ndef bootstrap_intervals_for_mean(data, B, alpha, rng_boot):\n    n = data.shape[0]\n    # Parametric (model-based) bootstrap using fitted Gamma\n    k_hat, theta_hat = fit_gamma_mle_zero_loc(data)\n    # Generate B x n Gamma draws and compute means\n    param_draws = rng_boot.gamma(shape=k_hat, scale=theta_hat, size=(B, n))\n    param_means = param_draws.mean(axis=1)\n    L_M, U_M = percentile_interval(param_means, alpha)\n\n    # Empirical (nonparametric) bootstrap: resample with replacement from data\n    idx = rng_boot.integers(0, n, size=(B, n))\n    emp_resamples = data[idx]\n    emp_means = emp_resamples.mean(axis=1)\n    L_E, U_E = percentile_interval(emp_means, alpha)\n\n    return L_M, U_M, L_E, U_E\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (shape k, scale theta, n, data-generation seed)\n    test_cases = [\n        (2.5, 1.2, 80, 1729),     # Case 1\n        (1.1, 2.0, 20, 2027),     # Case 2\n        (5.0, 0.5, 50, 12345),    # Case 3\n        (0.7, 1.5, 12, 314159),   # Case 4\n    ]\n\n    alpha = 0.05\n    B = 10000\n\n    # Single pseudorandom stream for all resampling operations\n    rng_boot = np.random.default_rng(2025)\n\n    results = []\n    for (k_true, theta_true, n, data_seed) in test_cases:\n        # Generate data for this case\n        data = generate_data(k_true, theta_true, n, data_seed)\n\n        # Compute intervals\n        L_M, U_M, L_E, U_E = bootstrap_intervals_for_mean(data, B, alpha, rng_boot)\n\n        # Interval length comparison indicator\n        len_M = U_M - L_M\n        len_E = U_E - L_E\n        shorter_parametric = len_M  len_E\n\n        # Coverage indicators wrt true mean\n        mu_true = k_true * theta_true\n        cover_M = (L_M = mu_true) and (mu_true = U_M)\n        cover_E = (L_E = mu_true) and (mu_true = U_E)\n\n        # Append in the specified order\n        results.extend([L_M, U_M, L_E, U_E, shorter_parametric, cover_M, cover_E])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Since the bootstrap is a computer simulation, its results are subject to random Monte Carlo error. A natural and critical question then arises: how many bootstrap replications, denoted by $B$, are sufficient for a reliable result? This final practice  addresses this question by having you investigate the stability of bootstrap confidence interval endpoints. By quantifying how the variability of the interval bounds changes with $B$, you will develop a practical understanding of how to choose this crucial parameter to ensure your statistical inferences are both stable and reproducible.",
            "id": "2377512",
            "problem": "Consider a single asset with independent and identically distributed (i.i.d.) daily log-returns. Let a realized sample be denoted by $\\{X_i\\}_{i=1}^n$. Define the sample mean $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_i$. For a given number of bootstrap replications $B$, define the bootstrap sampling distribution of $\\hat{\\mu}$ by repeatedly resampling, with replacement, $n$ observations from the realized sample and computing the mean for each resample. The two-sided confidence interval at confidence level $0.95$ is defined by the empirical quantiles at levels $0.025$ and $0.975$ of the bootstrap distribution of $\\hat{\\mu}$, using linear interpolation between order statistics. Denote these endpoints by $L(B)$ (lower) and $U(B)$ (upper).\n\nTo quantify the stability of the endpoints with respect to $B$, consider $\\mathcal{R}$ independent repetitions of the entire bootstrap procedure using the same realized sample but different random seeds; label the endpoints in repetition $j \\in \\{1,\\dots,\\mathcal{R}\\}$ by $\\ell_j(B)$ and $u_j(B)$. Define the stability measures as population standard deviations\n$$\ns_\\ell(B) = \\left(\\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}}\\left(\\ell_j(B) - \\bar{\\ell}(B)\\right)^2\\right)^{1/2},\\quad\n\\bar{\\ell}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}} \\ell_j(B),\n$$\nand\n$$\ns_u(B) = \\left(\\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}}\\left(u_j(B) - \\bar{u}(B)\\right)^2\\right)^{1/2},\\quad\n\\bar{u}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=1}^{\\mathcal{R}} u_j(B).\n$$\n\nFor reproducibility, the realized sample for each test case must be generated synthetically as specified below, and it must be reused unchanged across all $\\mathcal{R}$ repetitions for that test case. In each repetition $j \\in \\{0,1,\\dots,\\mathcal{R}-1\\}$, the bootstrap procedure must use a pseudorandom seed equal to the given base seed plus $j$. For the generation of the realized sample, use the provided sample seed exactly. All pseudorandom number generation must be based on a modern pseudorandom generator with the given integer seeds.\n\nReturn models:\n- Gaussian returns: $X_i \\sim \\mathcal{N}(\\mu,\\sigma^2)$.\n- Heavy-tailed returns: $X_i = \\mu + \\sigma \\sqrt{\\frac{\\nu - 2}{\\nu}}\\, T_{\\nu}$, where $T_{\\nu}$ has a Studentâ€™s $t$ distribution with $\\nu  2$ degrees of freedom and unit scale, so that $\\mathrm{Var}(X_i)$ equals $\\sigma^2$.\n\nStatistic of interest:\n- The parameter is the mean return $\\mu$ and the statistic is the sample mean $\\hat{\\mu}$.\n\nConfidence interval:\n- Endpoints are the empirical quantiles at levels $0.025$ and $0.975$ of the bootstrap distribution of $\\hat{\\mu}$, with linear interpolation between order statistics.\n\nStability metric:\n- Report $s_\\ell(B)$ and $s_u(B)$ as defined above for each test case, rounded to $6$ decimals.\n\nTest suite:\nFor each test case, parameters are listed as $(\\text{dist}, n, \\mu, \\sigma, \\nu, B, \\mathcal{R}, \\text{sample\\_seed}, \\text{bootstrap\\_base\\_seed})$; when $\\text{dist}=\\text{N}$, the entry $\\nu$ is not used.\n1. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 25,\\, 32,\\, 20231101,\\, 770000)$.\n2. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 200,\\, 32,\\, 20231101,\\, 770100)$.\n3. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 1000,\\, 32,\\, 20231101,\\, 770200)$.\n4. $(\\text{N},\\, 252,\\, 0.0005,\\, 0.01,\\, -,\\, 2000,\\, 32,\\, 20231101,\\, 770300)$.\n5. $(\\text{N},\\, 30,\\, 0.0005,\\, 0.01,\\, -,\\, 200,\\, 32,\\, 20231111,\\, 880100)$.\n6. $(\\text{T},\\, 252,\\, 0.0005,\\, 0.02,\\, 3,\\, 1000,\\, 32,\\, 20231221,\\, 990100)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[s_\\ell(B), s_u(B)]$ in the same order as the test cases, with each value rounded to $6$ decimals. For example, a valid output for three cases would look like $[[0.001234,0.001987],[0.000456,0.000765],[0.000123,0.000321]]$.\n\nNo physical units are involved. All numeric results must be floats.",
            "solution": "The problem presents a valid and well-posed task in computational statistics, specifically concerning the stability analysis of bootstrap confidence intervals. I will provide a systematic solution.\n\nThe core of the problem is to quantify the variability of bootstrap confidence interval endpoints. This variability arises because the bootstrap procedure itself is a random process, dependent on a random seed. By repeating the entire bootstrap procedure multiple times with different seeds, we can observe the distribution of the interval endpoints and measure their stability, for which the standard deviation is a natural metric.\n\nThe overall procedure for each test case is as follows:\n1.  Synthetically generate a single realized sample of asset log-returns, $\\{X_i\\}_{i=1}^n$, of size $n$. This sample is generated once using a specified `sample_seed` and remains fixed throughout the subsequent steps for a given test case.\n2.  Perform $\\mathcal{R}$ independent repetitions of the bootstrap confidence interval estimation. For each repetition $j \\in \\{0, 1, \\dots, \\mathcal{R}-1\\}$:\n    a.  Set the seed for the pseudorandom number generator (PRNG) to `bootstrap_base_seed` $+ j$. This ensures that each of the $\\mathcal{R}$ repetitions is statistically independent yet computationally reproducible.\n    b.  Generate $B$ bootstrap samples. Each bootstrap sample is created by drawing $n$ observations from the original realized sample $\\{X_i\\}_{i=1}^n$ with replacement.\n    c.  For each of the $B$ bootstrap samples, compute the sample mean. This collection of $B$ means forms the empirical bootstrap distribution of the statistic $\\hat{\\mu}$.\n    d.  From this empirical distribution, determine the two-sided $95\\%$ confidence interval. The problem specifies that the lower and upper endpoints, denoted $\\ell_j(B)$ and $u_j(B)$, are the empirical quantiles at levels $q_{low} = 0.025$ and $q_{high} = 0.975$, respectively. Linear interpolation is used to estimate the quantiles if they fall between ordered data points.\n3.  After completing all $\\mathcal{R}$ repetitions, we will have two sets of data: $\\{\\ell_j(B)\\}_{j=0}^{\\mathcal{R}-1}$ and $\\{u_j(B)\\}_{j=0}^{\\mathcal{R}-1}$.\n4.  Finally, calculate the stability of these endpoints. The problem defines the stability measures as the population standard deviations of these two sets of endpoints. Let the collection of lower endpoints be $\\boldsymbol{\\ell} = (\\ell_0(B), \\dots, \\ell_{\\mathcal{R}-1}(B))$ and upper endpoints be $\\boldsymbol{u} = (u_0(B), \\dots, u_{\\mathcal{R}-1}(B))$. The means are $\\bar{\\ell}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1} \\ell_j(B)$ and $\\bar{u}(B) = \\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1} u_j(B)$. The stability metrics are then:\n$$\ns_\\ell(B) = \\sqrt{\\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1}\\left(\\ell_j(B) - \\bar{\\ell}(B)\\right)^2}\n$$\n$$\ns_u(B) = \\sqrt{\\frac{1}{\\mathcal{R}}\\sum_{j=0}^{\\mathcal{R}-1}\\left(u_j(B) - \\bar{u}(B)\\right)^2}\n$$\nThis corresponds to a standard deviation calculation with a divisor of $\\mathcal{R}$, not $\\mathcal{R}-1$.\n\nThe generation of the realized sample depends on the specified distribution.\n- For Gaussian returns, $X_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we draw from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$.\n- For heavy-tailed returns, $X_i = \\mu + \\sigma \\sqrt{\\frac{\\nu - 2}{\\nu}}\\, T_{\\nu}$, where $T_{\\nu}$ follows a standard Student's t-distribution with $\\nu$ degrees of freedom. This formulation is chosen so that $\\mathrm{E}[X_i] = \\mu$ and $\\mathrm{Var}(X_i) = \\sigma^2$, provided $\\nu  2$.\n\nComputationally, the implementation will adhere to these steps. A modern pseudorandom number generator, specifically `numpy.random.PCG64`, will be used for all random number generation to ensure reproducibility as specified. The calculation of quantiles with linear interpolation is handled by `numpy.quantile` with its default `method='linear'`. The population standard deviation is computed using `numpy.std` with `ddof=0`. The entire process is encapsulated in a function that iterates through the provided test suite, calculates the pair $(s_\\ell(B), s_u(B))$ for each case, and formats the output according to the strict requirements.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef compute_stability_for_case(dist, n, mu, sigma, nu, B, R, sample_seed, bootstrap_base_seed):\n    \"\"\"\n    Computes the stability of bootstrap confidence interval endpoints for a single test case.\n\n    Args:\n        dist (str): Distribution type ('N' for Gaussian, 'T' for Student's-t).\n        n (int): Size of the realized sample.\n        mu (float): Mean of the return distribution.\n        sigma (float): Standard deviation of the return distribution.\n        nu (int): Degrees of freedom for the Student's-t distribution.\n        B (int): Number of bootstrap replications.\n        R (int): Number of independent repetitions of the bootstrap procedure.\n        sample_seed (int): Seed for generating the realized sample.\n        bootstrap_base_seed (int): Base seed for the bootstrap procedure repetitions.\n\n    Returns:\n        list: A list containing [s_l, s_u], the stability measures for the lower and upper\n              confidence interval endpoints, rounded to 6 decimals.\n    \"\"\"\n    # Step 1: Generate the realized sample\n    rng_sample = np.random.Generator(np.random.PCG64(sample_seed))\n    \n    if dist == 'N':\n        realized_sample = rng_sample.normal(loc=mu, scale=sigma, size=n)\n    elif dist == 'T':\n        # Generate standard Student's t-distributed random variables\n        t_samples = t.rvs(df=nu, size=n, random_state=rng_sample)\n        # Scale to match mean mu and variance sigma^2\n        scaling_factor = sigma * np.sqrt((nu - 2) / nu)\n        realized_sample = mu + scaling_factor * t_samples\n    else:\n        raise ValueError(\"Invalid distribution type specified.\")\n\n    lower_endpoints = []\n    upper_endpoints = []\n\n    # Step 2: Outer loop for R independent repetitions\n    for j in range(R):\n        current_seed = bootstrap_base_seed + j\n        rng_bootstrap = np.random.Generator(np.random.PCG64(current_seed))\n\n        # Perform B bootstrap resamples efficiently\n        # Generate all indices for B resamples at once\n        bootstrap_indices = rng_bootstrap.choice(n, size=(B, n), replace=True)\n        \n        # Create bootstrap samples and compute their means\n        bootstrap_samples = realized_sample[bootstrap_indices]\n        bootstrap_means = bootstrap_samples.mean(axis=1)\n\n        # Step 2d: Calculate confidence interval endpoints\n        q_low, q_high = np.quantile(bootstrap_means, [0.025, 0.975])\n        \n        lower_endpoints.append(q_low)\n        upper_endpoints.append(q_high)\n\n    # Step 4: Calculate stability measures (population standard deviation)\n    s_l = np.std(lower_endpoints, ddof=0)\n    s_u = np.std(upper_endpoints, ddof=0)\n\n    return [round(s_l, 6), round(s_u, 6)]\n\ndef solve():\n    \"\"\"\n    Main solver function that runs all test cases and prints the final result.\n    \"\"\"\n    test_cases = [\n        # (dist, n, mu, sigma, nu, B, R, sample_seed, bootstrap_base_seed)\n        ('N', 252, 0.0005, 0.01, None, 25, 32, 20231101, 770000),\n        ('N', 252, 0.0005, 0.01, None, 200, 32, 20231101, 770100),\n        ('N', 252, 0.0005, 0.01, None, 1000, 32, 20231101, 770200),\n        ('N', 252, 0.0005, 0.01, None, 2000, 32, 20231101, 770300),\n        ('N', 30, 0.0005, 0.01, None, 200, 32, 20231111, 880100),\n        ('T', 252, 0.0005, 0.02, 3, 1000, 32, 20231221, 990100),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_stability_for_case(*case)\n        results.append(result)\n\n    # Format the final output string exactly as required, without spaces after commas.\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}