{
    "hands_on_practices": [
        {
            "introduction": "Before solving any dynamic programming problem, one must first master the art of its formulation. This exercise  serves as a fundamental check on your ability to translate a classic economic narrative—the life-cycle consumption and savings decision—into the precise mathematical language of the Bellman equation. By correctly identifying the terminal value and recursive structure, you build the essential blueprint for tackling a wide range of sequential optimization problems.",
            "id": "2437277",
            "problem": "Consider a finite-horizon, deterministic life-cycle consumption-savings problem with periods indexed by $t \\in \\{0,1,\\dots,T-1\\}$. At the beginning of each period $t$, the agent holds wealth $W_t \\ge 0$, chooses consumption $c_t \\ge 0$, and carries wealth into the next period according to the law of motion\n$$\nW_{t+1} \\;=\\; (1+r)\\,(W_t - c_t) \\;+\\; y_{t+1},\n$$\nwhere $r  -1$ is a constant net interest rate and $\\{y_t\\}_{t=1}^T$ is an exogenous sequence of nonnegative incomes. Preferences are given by\n$$\n\\sum_{t=0}^{T-1} \\beta^t\\,u(c_t) \\;+\\; \\beta^T\\,u_B(W_T),\n$$\nwith discount factor $\\beta \\in (0,1)$, period utility $u:\\mathbb{R}_+ \\to \\mathbb{R}$, and bequest utility $u_B:\\mathbb{R}_+ \\to \\mathbb{R}$. Assume $u$ and $u_B$ are strictly increasing, strictly concave, and twice continuously differentiable, and assume that at the optimum all relevant constraints are slack so that interior first-order conditions apply.\n\nWhich option correctly specifies the Bellman recursion (including the terminal condition) and the resulting first-order condition in period $T-1$?\n\nA. \n- Terminal condition: $V_T(W_T) \\;=\\; u_B(W_T)$.\n- Recursion for $t \\in \\{0,1,\\dots,T-1\\}$:\n$$\nV_t(W_t) \\;=\\; \\max_{c_t \\ge 0} \\left\\{ u(c_t) \\;+\\; \\beta\\,V_{t+1}\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\right\\}.\n$$\n- First-order condition in $t=T-1$ (interior): $u'(c_{T-1}) \\;=\\; \\beta\\,(1+r)\\,u_B'(W_T)$.\n\nB.\n- Terminal condition: $V_T(W_T) \\;=\\; \\beta\\,u_B(W_T)$.\n- Recursion for $t \\in \\{0,1,\\dots,T-1\\}$:\n$$\nV_t(W_t) \\;=\\; \\max_{c_t \\ge 0} \\left\\{ u(c_t) \\;+\\; \\beta\\,V_{t+1}\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\right\\}.\n$$\n- First-order condition in $t=T-1$ (interior): $u'(c_{T-1}) \\;=\\; \\beta\\,u_B'(W_T)$.\n\nC.\n- Terminal condition: $V_T(W_T) \\;=\\; u(W_T)$.\n- Recursion for $t \\in \\{0,1,\\dots,T-1\\}$:\n$$\nV_t(W_t) \\;=\\; \\max_{c_t \\ge 0} \\left\\{ u(c_t) \\;+\\; \\beta\\,V_{t+1}\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\right\\}.\n$$\n- First-order condition in $t=T-1$ (interior): $u'(c_{T-1}) \\;=\\; \\beta\\,(1+r)\\,u'(c_T)$.\n\nD.\n- Terminal condition: $V_T(W_T) \\;=\\; u_B(W_T)$.\n- Recursion for $t \\in \\{0,1,\\dots,T-1\\}$:\n$$\nV_t(W_t) \\;=\\; \\max_{c_t \\ge 0} \\left\\{ u(c_t) \\;+\\; \\beta\\,\\Big( V_{t+1}\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\;+\\; u_B\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\Big) \\right\\}.\n$$\n- First-order condition in $t=T-1$ (interior): $u'(c_{T-1}) \\;=\\; \\beta\\,(1+r)\\,\\Big(u'(c_T) \\;+\\; u_B'(W_T)\\Big)$.",
            "solution": "The problem statement will first be subjected to rigorous validation.\n\n### Step 1: Extract Givens\nThe problem describes a finite-horizon, deterministic life-cycle consumption-savings model.\n-   **Time horizon**: Discrete time periods $t \\in \\{0, 1, \\dots, T-1\\}$.\n-   **State variable**: Wealth at the beginning of period $t$, $W_t \\ge 0$.\n-   **Control variable**: Consumption in period $t$, $c_t \\ge 0$.\n-   **Law of motion for wealth**: $W_{t+1} = (1+r)(W_t - c_t) + y_{t+1}$.\n-   **Net interest rate**: $r  -1$, constant.\n-   **Exogenous income stream**: $\\{y_t\\}_{t=1}^T$, where $y_t \\ge 0$.\n-   **Objective function (preferences)**: $\\sum_{t=0}^{T-1} \\beta^t\\,u(c_t) + \\beta^T\\,u_B(W_T)$.\n-   **Discount factor**: $\\beta \\in (0,1)$.\n-   **Period utility function**: $u:\\mathbb{R}_+ \\to \\mathbb{R}$.\n-   **Bequest utility function**: $u_B:\\mathbb{R}_+ \\to \\mathbb{R}$.\n-   **Assumptions on utility functions**: $u$ and $u_B$ are strictly increasing, strictly concave, and twice continuously differentiable ($C^2$).\n-   **Solution characteristic**: The optimum is interior, so first-order conditions apply.\n\n### Step 2: Validate Using Extracted Givens\nThe problem presents a canonical model in dynamic macroeconomics and computational economics.\n\n-   **Scientific Grounding**: The model is a standard representation of intertemporal choice under certainty. The formulation, including the objective function, law of motion, and assumptions on utility, is a cornerstone of economic theory. It is entirely scientifically sound.\n-   **Well-Posedness**: The problem is well-posed. The assumptions of strict concavity on the utility functions, along with the structure of the problem, ensure that a unique, well-behaved optimal consumption plan exists for a given initial wealth $W_0$.\n-   **Objectivity**: The problem is stated using precise mathematical language, free from any subjective or ambiguous terminology.\n-   **Check for Flaws**:\n    1.  **Scientific/Factual Unsoundness**: None. The model is a standard theoretical construct.\n    2.  **Non-Formalizable/Irrelevant**: The problem is a formal mathematical exercise directly related to the Bellman equation, as required.\n    3.  **Incomplete/Contradictory Setup**: The problem is complete and self-consistent. All necessary components for setting up the Bellman equation are provided.\n    4.  **Unrealistic/Infeasible**: The model is a simplification of reality, but it is not scientifically implausible or physically impossible. The condition $r  -1$ is standard and ensures the budget constraint is not pathological.\n    5.  **Ill-Posed/Poorly Structured**: None. The problem structure is clear and leads to a unique solution.\n    6.  **Outside Scientific Verifiability**: The problem is a mathematical one and is entirely verifiable through standard methods of dynamic programming.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. I will now proceed with the derivation and analysis.\n\n---\n\nThe problem is solved using the principle of dynamic programming, working backward from the final period $T$. Let $V_t(W_t)$ be the value function, representing the maximum attainable utility from period $t$ onward, given wealth $W_t$. The standard definition of the value function, which normalizes the discount factor at each stage, is:\n$$\nV_t(W_t) \\;=\\; \\max_{\\{c_s\\}_{s=t}^{T-1}} \\left\\{ \\sum_{s=t}^{T-1} \\beta^{s-t} u(c_s) \\;+\\; \\beta^{T-t} u_B(W_T) \\right\\}\n$$\n\n**1. Derivation of the Terminal Condition**\n\nAt the final date $t=T$, no more consumption decisions are made. The agent is left with wealth $W_T$. The remaining utility is derived solely from the bequest. According to the definition of the value function, for $t=T$, we have:\n$$\nV_T(W_T) \\;=\\; \\beta^{T-T} u_B(W_T) \\;=\\; u_B(W_T)\n$$\nThis is the terminal condition for the Bellman recursion. It states that the value of being at the end of life with wealth $W_T$ is simply the utility derived from bequeathing that amount.\n\n**2. Derivation of the Bellman Recursion**\n\nFor any period $t  T$, the value function can be expressed recursively by breaking the decision into the current choice ($c_t$) and the consequences for the future.\n$$\nV_t(W_t) \\;=\\; \\max_{c_t} \\left\\{ u(c_t) \\;+\\; \\beta \\left( \\max_{\\{c_s\\}_{s=t+1}^{T-1}} \\left\\{ \\sum_{s=t+1}^{T-1} \\beta^{s-(t+1)} u(c_s) \\;+\\; \\beta^{T-(t+1)} u_B(W_T) \\right\\} \\right) \\right\\}\n$$\nThe expression inside the outer parentheses is, by definition, the value function for the next period, $V_{t+1}(W_{t+1})$. Therefore, we obtain the Bellman equation:\n$$\nV_t(W_t) \\;=\\; \\max_{c_t \\ge 0} \\left\\{ u(c_t) \\;+\\; \\beta\\,V_{t+1}(W_{t+1}) \\right\\}\n$$\nSubstituting the law of motion for wealth, $W_{t+1} = (1+r)(W_t - c_t) + y_{t+1}$, we get the full recursion for $t \\in \\{0, 1, \\dots, T-1\\}$:\n$$\nV_t(W_t) \\;=\\; \\max_{c_t \\ge 0} \\left\\{ u(c_t) \\;+\\; \\beta\\,V_{t+1}\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\right\\}\n$$\n\n**3. Derivation of the First-Order Condition in Period $t=T-1$**\n\nWe now specialize the Bellman equation to the last period of decision-making, $t=T-1$.\n$$\nV_{T-1}(W_{T-1}) \\;=\\; \\max_{c_{T-1} \\ge 0} \\left\\{ u(c_{T-1}) \\;+\\; \\beta\\,V_T(W_T) \\right\\}\n$$\nUsing the terminal condition $V_T(W_T) = u_B(W_T)$, this becomes:\n$$\nV_{T-1}(W_{T-1}) \\;=\\; \\max_{c_{T-1} \\ge 0} \\left\\{ u(c_{T-1}) \\;+\\; \\beta\\,u_B(W_T) \\right\\}\n$$\nNext, we substitute the law of motion connecting $W_T$ to the choice $c_{T-1}$: $W_T = (1+r)(W_{T-1} - c_{T-1}) + y_T$.\n$$\nV_{T-1}(W_{T-1}) \\;=\\; \\max_{c_{T-1} \\ge 0} \\left\\{ u(c_{T-1}) \\;+\\; \\beta\\,u_B\\!\\left((1+r)\\,(W_{T-1} - c_{T-1}) + y_T\\right) \\right\\}\n$$\nTo find the optimal consumption $c_{T-1}$, we differentiate the expression inside the maximization operator with respect to $c_{T-1}$ and set it to zero, as an interior solution is assumed.\n$$\n\\frac{\\partial}{\\partial c_{T-1}} \\left[ u(c_{T-1}) \\;+\\; \\beta\\,u_B\\!\\left((1+r)\\,(W_{T-1} - c_{T-1}) + y_T\\right) \\right] \\;=\\; 0\n$$\nApplying the chain rule:\n$$\nu'(c_{T-1}) \\;+\\; \\beta\\,u_B'(W_T) \\cdot \\frac{\\partial W_T}{\\partial c_{T-1}} \\;=\\; 0\n$$\nThe derivative of $W_T$ with respect to $c_{T-1}$ is:\n$$\n\\frac{\\partial W_T}{\\partial c_{T-1}} \\;=\\; \\frac{\\partial}{\\partial c_{T-1}} \\left[ (1+r)(W_{T-1} - c_{T-1}) + y_T \\right] \\;=\\; -(1+r)\n$$\nSubstituting this into the first-order condition:\n$$\nu'(c_{T-1}) \\;+\\; \\beta\\,u_B'(W_T) \\cdot (-(1+r)) \\;=\\; 0\n$$\nRearranging the terms yields the final first-order condition for period $T-1$:\n$$\nu'(c_{T-1}) \\;=\\; \\beta\\,(1+r)\\,u_B'(W_T)\n$$\nThis is the Euler equation for the final period. It equates the marginal utility of consuming one unit of wealth today with the discounted marginal utility of saving that unit, letting it grow at rate $r$, and bequeathing it at time $T$.\n\n**Evaluation of Options**\n\n-   **Option A**:\n    -   Terminal condition: $V_T(W_T) = u_B(W_T)$. This is correct.\n    -   Recursion: $V_t(W_t) = \\max_{c_t \\ge 0} \\left\\{ u(c_t) + \\beta\\,V_{t+1}\\!\\left((1+r)\\,(W_t - c_t) + y_{t+1}\\right) \\right\\}$. This is correct.\n    -   First-order condition in $t=T-1$: $u'(c_{T-1}) = \\beta\\,(1+r)\\,u_B'(W_T)$. This is correct.\n    -   Verdict: **Correct**. All three components are derived correctly.\n\n-   **Option B**:\n    -   Terminal condition: $V_T(W_T) = \\beta\\,u_B(W_T)$. This is incorrect. The value function at time $t$ is conventionally defined as the present value of utility from time $t$ onward, discounted to time $t$. The $\\beta$ factor is not part of the terminal value.\n    -   Recursion: The recursion formula is stated correctly.\n    -   First-order condition: $u'(c_{T-1}) = \\beta\\,u_B'(W_T)$. This is incorrect. It is missing the factor $(1+r)$, which represents the gross return on savings. This condition would only hold if $r=0$.\n    -   Verdict: **Incorrect**.\n\n-   **Option C**:\n    -   Terminal condition: $V_T(W_T) = u(W_T)$. This is incorrect. The utility at the terminal date is given by the bequest utility function $u_B$, not the period consumption utility function $u$.\n    -   Recursion: The recursion formula is stated correctly.\n    -   First-order condition: $u'(c_{T-1}) = \\beta\\,(1+r)\\,u'(c_T)$. This is fundamentally incorrect. There is no consumption choice $c_T$ in period $T$. Utility at time $T$ depends on the stock of wealth $W_T$, not a flow of consumption. This equation seems to confuse the finite-horizon model with an infinite-horizon Euler equation.\n    -   Verdict: **Incorrect**.\n\n-   **Option D**:\n    -   Terminal condition: $V_T(W_T) = u_B(W_T)$. This is correct.\n    -   Recursion: The recursion is stated as $V_t(W_t) = \\max_{c_t \\ge 0} \\{ u(c_t) + \\beta\\,\\Big( V_{t+1}(\\dots) + u_B(\\dots) \\Big) \\}$. This is incorrect. It incorrectly adds the bequest utility in every period's recursion. The bequest motive applies only once, to the terminal wealth $W_T$. This formulation is a serious misstatement of the dynamic programming principle.\n    -   First-order condition: $u'(c_{T-1}) = \\beta\\,(1+r)\\,\\Big(u'(c_T) + u_B'(W_T)\\Big)$. This is incorrect, suffering from both the error of including a non-existent $c_T$ and being derived from an invalid recursive structure.\n    -   Verdict: **Incorrect**.\n\nBased on a rigorous derivation from first principles, Option A is the only one that correctly states the terminal condition, the Bellman recursion, and the period $T-1$ first-order condition.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Once the Bellman equation is formulated, the next step is to solve for the optimal policy. This problem  demonstrates how to find an exact analytical solution in a setting with a discrete state space, using a repeated game as a creative and insightful example. You will apply the principle of optimality to a system of value function equations, a powerful technique that allows you to determine the precise conditions under which a particular strategy, such as cooperation, becomes optimal.",
            "id": "2437325",
            "problem": "Consider an infinite-horizon, discrete-time repeated Prisoner’s Dilemma between two players with common knowledge of the stage-game payoffs. In each period $t \\in \\{1,2,\\dots\\}$, each player chooses an action $a_t \\in \\{C,D\\}$. The stage payoff to the decision-maker (Player $1$) is given by the standard Prisoner’s Dilemma ordering with the following numerical values: if both cooperate $(C,C)$ the payoff is $R=3$, if Player $1$ defects while the opponent cooperates $(D,C)$ the payoff is $T=5$, if both defect $(D,D)$ the payoff is $P=1$, and if Player $1$ cooperates while the opponent defects $(C,D)$ the payoff is $S=0$, which satisfies $TRPS$ and $2RT+S$. The opponent (Player $2$) follows the fixed Tit-for-Tat (TFT) strategy: the opponent cooperates in period $t=1$ and, for all $t \\geq 2$, plays the action that Player $1$ played in period $t-1$. Player $1$ discounts future payoffs with discount factor $\\delta \\in (0,1)$ and chooses actions to maximize the infinite-horizon discounted sum $\\sum_{t=1}^{\\infty} \\delta^{t-1} u_t$, where $u_t$ is the stage payoff in period $t$.\n\nBy viewing the environment faced by Player $1$ as a two-state controlled Markov process induced by the opponent’s TFT strategy, derive from first principles (the definition of the infinite-horizon discounted value and the principle of optimality) the Bellman optimality conditions for Player $1$’s value functions in each state. Using these conditions, determine the minimal discount factor $\\delta^{\\ast} \\in (0,1)$ such that the Bellman-optimal best response by Player $1$ cooperates in the initial period $t=1$. Provide your answer in exact form (no rounding).",
            "solution": "The problem is evaluated to be scientifically grounded, well-posed, objective, and internally consistent. It is a standard problem in dynamic programming applied to game theory. All necessary data are provided. We may proceed with the solution.\n\nThe problem asks for two things: first, to derive the Bellman optimality conditions for Player $1$'s decision problem, and second, to calculate the minimum discount factor $\\delta^{\\ast}$ for which it is optimal for Player $1$ to cooperate in the first period.\n\nFirst, we formalize the problem as a controlled Markov process from Player $1$'s perspective. The opponent, Player $2$, follows a fixed Tit-for-Tat (TFT) strategy. This means Player $2$'s action in period $t \\geq 2$ is determined by Player $1$'s action in period $t-1$. Player $2$'s action in period $t=1$ is to cooperate ($C$). Therefore, the state of the system at the beginning of any period $t$ can be fully characterized by Player $2$'s impending action. We define two states:\n\\begin{itemize}\n    \\item State $S_C$: Player $2$ is set to Cooperate in the current period.\n    \\item State $S_D$: Player $2$ is set to Defect in the current period.\n\\end{itemize}\nAt the beginning of period $t=1$, the system is in state $S_C$.\n\nPlayer $1$'s available actions in any state are $\\{C, D\\}$. The state transitions are deterministic based on Player $1$'s action, due to Player $2$'s TFT strategy.\n\\begin{itemize}\n    \\item If in state $S_C$, Player $1$ plays $C$: The outcome is $(C,C)$, Player $1$'s payoff is $R=3$. Player $2$ will cooperate in the next period. The system transitions to state $S_C$.\n    \\item If in state $S_C$, Player $1$ plays $D$: The outcome is $(D,C)$, Player $1$'s payoff is $T=5$. Player $2$ will defect in the next period. The system transitions to state $S_D$.\n    \\item If in state $S_D$, Player $1$ plays $C$: The outcome is $(C,D)$, Player $1$'s payoff is $S=0$. Player $2$ will cooperate in the next period. The system transitions to state $S_C$.\n    \\item If in state $S_D$, Player $1$ plays $D$: The outcome is $(D,D)$, Player $1$'s payoff is $P=1$. Player $2$ will defect in the next period. The system transitions to state $S_D$.\n\\end{itemize}\n\nLet $V(S_C)$ and $V(S_D)$ be the value functions for Player $1$ representing the maximum discounted sum of future payoffs starting from states $S_C$ and $S_D$, respectively. The Bellman principle of optimality states that the optimal value function must satisfy a recursive relationship. The value in a given state is the maximum achievable value over all possible actions, where the value of an action is the immediate payoff plus the discounted value of the subsequent state.\n\nFor state $S_C$, Player $1$ chooses between cooperating and defecting:\n\\begin{itemize}\n    \\item Choose $C$: Payoff is $R$. Next state is $S_C$. Value is $R + \\delta V(S_C)$.\n    \\item Choose $D$: Payoff is $T$. Next state is $S_D$. Value is $T + \\delta V(S_D)$.\n\\end{itemize}\nThe Bellman optimality condition for state $S_C$ is:\n$$V(S_C) = \\max\\{R + \\delta V(S_C), \\; T + \\delta V(S_D)\\}$$\n\nFor state $S_D$, Player $1$ chooses between cooperating and defecting:\n\\begin{itemize}\n    \\item Choose $C$: Payoff is $S$. Next state is $S_C$. Value is $S + \\delta V(S_C)$.\n    \\item Choose $D$: Payoff is $P$. Next state is $S_D$. Value is $P + \\delta V(S_D)$.\n\\end{itemize}\nThe Bellman optimality condition for state $S_D$ is:\n$$V(S_D) = \\max\\{S + \\delta V(S_C), \\; P + \\delta V(S_D)\\}$$\nThis system of two equations constitutes the Bellman optimality conditions for Player $1$'s value functions.\n\nNext, we must find the minimal discount factor $\\delta^{\\ast}$ such that Player $1$ cooperates in the initial period. The initial state is $S_C$. Player $1$ chooses to cooperate if the value of cooperating is at least as large as the value of defecting:\n$$R + \\delta V(S_C) \\geq T + \\delta V(S_D)$$\nTo evaluate this inequality, we need to find the optimal value functions $V(S_C)$ and $V(S_D)$, which depend on the optimal policy. The optimal policy itself depends on $\\delta$. We must find the range of $\\delta$ for which cooperation is the optimal action in $S_C$.\n\nLet's assume the optimal policy is to cooperate in state $S_C$. This implies that $V(S_C) = R + \\delta V(S_C)$, which gives $V(S_C)(1-\\delta) = R$. This step is incorrect, as it presumes that $R + \\delta V(S_C)$ is the maximum. The correct approach is to test candidate stationary policies. A key candidate policy is \"Always Cooperate\" (AC): cooperate in both states $S_C$ and $S_D$. Let us determine the range of $\\delta$ for which AC is optimal.\n\nUnder the AC policy, Player $1$ always plays $C$.\nIf starting in $S_C$, Player $1$ plays $C$, Player $2$ plays $C$, the state remains $S_C$. The payoff stream is $(R, R, R, \\dots)$. The value is $V^{AC}(S_C) = \\sum_{t=0}^{\\infty} \\delta^t R = \\frac{R}{1-\\delta}$.\nIf starting in $S_D$, Player $1$ plays $C$, Player $2$ plays $D$. Payoff is $S$. The next state is $S_C$. The subsequent value is $\\delta V^{AC}(S_C)$. So, $V^{AC}(S_D) = S + \\delta V^{AC}(S_C) = S + \\frac{\\delta R}{1-\\delta}$.\n\nFor AC to be the Bellman-optimal policy, it must be incentive-compatible in each state.\n1.  Check optimality in $S_C$: Must prefer $C$ over $D$.\n    $$R + \\delta V^{AC}(S_C) \\geq T + \\delta V^{AC}(S_D)$$\n    Substituting the values:\n    $$\\frac{R}{1-\\delta} \\geq T + \\delta \\left(S + \\frac{\\delta R}{1-\\delta}\\right)$$\n    $$R \\geq T(1-\\delta) + \\delta S(1-\\delta) + \\delta^2 R$$\n    $$R \\geq T - T\\delta + S\\delta - S\\delta^2 + R\\delta^2$$\n    $$0 \\geq (T-R) - (T-S)\\delta + (R-S)\\delta^2$$\n    Substituting the numerical values $R=3, T=5, P=1, S=0$:\n    $$0 \\geq (5-3) - (5-0)\\delta + (3-0)\\delta^2$$\n    $$3\\delta^2 - 5\\delta + 2 \\leq 0$$\n    The roots of the quadratic $3x^2 - 5x + 2 = 0$ are $x = \\frac{5 \\pm \\sqrt{25-24}}{6} = \\frac{5 \\pm 1}{6}$, which gives $x_1 = 2/3$ and $x_2=1$. Since the parabola opens upward, the inequality holds for $\\delta \\in [2/3, 1]$.\n\n2.  Check optimality in $S_D$: Must prefer $C$ over $D$.\n    $$S + \\delta V^{AC}(S_C) \\geq P + \\delta V^{AC}(S_D)$$\n    Substituting the values:\n    $$S + \\frac{\\delta R}{1-\\delta} \\geq P + \\delta \\left(S + \\frac{\\delta R}{1-\\delta}\\right)$$\n    $$V^{AC}(S_D) \\geq P + \\delta V^{AC}(S_D)$$\n    $$V^{AC}(S_D)(1-\\delta) \\geq P$$\n    $$\\left(S + \\frac{\\delta R}{1-\\delta}\\right)(1-\\delta) \\geq P$$\n    $$S(1-\\delta) + \\delta R \\geq P$$\n    Substituting numerical values:\n    $$0(1-\\delta) + 3\\delta \\geq 1$$\n    $$3\\delta \\geq 1 \\implies \\delta \\geq \\frac{1}{3}$$\n\nFor the AC policy to be optimal, both conditions must hold. The intersection of $\\delta \\in [2/3, 1]$ and $\\delta \\geq 1/3$ is $\\delta \\in [2/3, 1)$. Thus, for any discount factor $\\delta \\geq 2/3$, the \"Always Cooperate\" policy is an optimal strategy for Player $1$. This policy involves cooperating in the initial state $S_C$.\n\nFor values of $\\delta  2/3$, it can be shown that the optimal policy involves defecting in state $S_C$. For example, for $\\delta \\in [1/4, 2/3)$, the optimal policy is to defect in $S_C$ and cooperate in $S_D$ (an \"Anti-TFT\" strategy). For $\\delta \\leq 1/4$, the optimal policy is to always defect. In all cases where $\\delta  2/3$, the optimal action for Player $1$ in the initial period is to defect.\n\nThe question asks for the minimal discount factor $\\delta^{\\ast}$ for which cooperation in the initial period is an optimal action. This corresponds to the lower boundary of the region where the optimal policy involves cooperating in state $S_C$. As we have found, this occurs for $\\delta \\geq 2/3$. At the boundary $\\delta=2/3$, Player $1$ is indifferent between a policy starting with cooperation (AC) and one starting with defection (Anti-TFT), making cooperation one of the optimal actions.\n\nTherefore, the minimal discount factor is $\\delta^{\\ast} = 2/3$.",
            "answer": "$$\\boxed{\\frac{2}{3}}$$"
        },
        {
            "introduction": "While analytical solutions are elegant, most real-world models are too complex and must be solved numerically. This practice  transitions you from theory to computation by asking you to implement Value Function Iteration (VFI), a cornerstone algorithm for solving Bellman equations. You will not only compute the solution to a standard consumption-savings model but also empirically verify a key theoretical result: the Bellman operator's nature as a contraction mapping, where the discount factor $\\beta$ governs the speed of convergence.",
            "id": "2437296",
            "problem": "Consider a deterministic consumption-savings problem with a single asset state. Preferences are given by a Constant Relative Risk Aversion (CRRA) utility function with risk aversion parameter $\\sigma > 0$ and per-period utility $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ for $\\sigma \\neq 1$. The agent receives an exogenous income $y$ each period, has asset holdings $a$ that evolve deterministically, faces a gross return $1+r$ on assets, and cannot borrow. The state variable is assets $a$ and the control is next-period assets $a'$. The one-period budget constraint is $c = (1+r)a + y - a'$, with non-negativity constraint $c \\ge 0$ and borrowing constraint $a' \\ge 0$.\n\nDefine a finite grid $\\mathcal{A}_N = \\{a_0, a_1, \\dots, a_{N-1}\\}$ of $N$ evenly spaced asset points on $[0, A_{\\max}]$. The dynamic programming problem is to compute the value function $V(a)$ that solves the Bellman equation\n$$\nV(a) = \\max_{a' \\in \\mathcal{A}_N} \\left\\{ u\\big((1+r)a + y - a'\\big) + \\beta V(a') \\right\\},\n$$\nsubject to $c = (1+r)a + y - a' \\ge 0$ and $a' \\ge 0$, where $\\beta \\in (0,1)$ is the discount factor. Let $\\mathcal{T}$ denote the Bellman operator defined by the right-hand side mapping from a function $V$ to a new function $\\mathcal{T}V$ on the grid.\n\nTasks:\n- Starting from the fundamental definition of the Bellman operator $\\mathcal{T}$ and the principles of dynamic programming under bounded and continuous utility and compact state-action sets, implement value function iteration by repeatedly applying $\\mathcal{T}$ to an initial guess $V_0(a) = 0$ on $\\mathcal{A}_N$ until convergence in the supremum norm. For each iteration $k$, compute the error $\\epsilon_k = \\|V_{k+1} - V_k\\|_\\infty = \\max_i |V_{k+1}(a_i) - V_k(a_i)|$.\n- Use a convergence tolerance of $\\text{tol} = 10^{-4}$ in the supremum norm and a maximum number of iterations of $1000$. Record the total number of iterations required for convergence for each grid size.\n- Compute an empirical convergence rate estimate by the ratio of successive errors $\\rho_k = \\epsilon_{k+1} / \\epsilon_k$. Report the observed rate as the median of the last $M$ available ratios prior to termination, where $M = 10$ or fewer if fewer ratios are available. Round this observed rate to three decimal places.\n- Use parameter values $\\beta = 0.90$, $r = 0.03$, $y = 1.0$, $\\sigma = 2.0$, and $A_{\\max} = 5.0$. For infeasible consumption $c \\le 0$, treat $u(c)$ as a very large negative value to enforce feasibility in maximization.\n\nTest suite:\n- Evaluate the program for the following asset grid sizes $N \\in \\{5, 20, 80, 160\\}$.\n\nRequired final output:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the list corresponds to one grid size $N$ and must itself be a list of the form $[N, \\text{iterations}, \\text{observed_rate}]$, where `iterations` is the integer number of iterations to convergence and `observed_rate` is the float rounded to three decimals. For example, an output with two test cases would look like $[[N_1, I_1, R_1],[N_2, I_2, R_2]]$.\n\nScientific realism and derivation basis:\n- Begin from core definitions: the Bellman operator $\\mathcal{T}$, the supremum norm $\\|\\cdot\\|_\\infty$, and the contraction mapping principle under discounting. Do not assume any closed-form solution for $V(a)$ and do not use any acceleration methods; strictly use value function iteration with full grid search over $a' \\in \\mathcal{A}_N$.",
            "solution": "The problem requires the numerical solution of a deterministic consumption-savings model using value function iteration. The solution must be derived from fundamental principles of dynamic programming.\n\n**1. Theoretical Foundation: The Bellman Equation and Contraction Mappings**\n\nThe agent's problem can be formulated recursively. The value function, $V(a)$, represents the maximum lifetime utility an agent can achieve starting with asset level $a$. It must satisfy the Bellman equation, which defines a functional fixed-point problem:\n$$\nV(a) = \\max_{a' \\in [0, (1+r)a+y]} \\left\\{ u\\big((1+r)a + y - a'\\big) + \\beta V(a') \\right\\}\n$$\nHere, $u(c)$ is the per-period utility function, $c$ is consumption, $a$ is the current asset level (state), $a'$ is the next-period asset level (control), $r$ is the interest rate, $y$ is income, and $\\beta \\in (0,1)$ is the discount factor. The maximization is subject to the budget constraint $c = (1+r)a + y - a'$ and non-negativity constraints $c \\ge 0$ and $a' \\ge 0$.\n\nThis equation can be expressed using the Bellman operator, $\\mathcal{T}$, which maps a candidate value function $W$ to a new function $\\mathcal{T}W$:\n$$\n(\\mathcal{T}W)(a) = \\max_{a'} \\left\\{ u(c) + \\beta W(a') \\right\\}\n$$\nThe true value function $V$ is the unique fixed point of this operator, i.e., $V = \\mathcal{T}V$. The existence and uniqueness of this fixed point, and the convergence of iterative methods to it, are guaranteed by the Contraction Mapping Theorem. For a complete metric space of bounded functions equipped with the supremum norm $\\lVert f \\rVert_\\infty = \\sup_x |f(x)|$, an operator $\\mathcal{T}$ is a contraction if there exists a constant $\\gamma \\in [0,1)$ such that for any two functions $f_1, f_2$ in the space, $\\lVert \\mathcal{T}f_1 - \\mathcal{T}f_2 \\rVert_\\infty \\le \\gamma \\lVert f_1 - f_2 \\rVert_\\infty$. The Bellman operator $\\mathcal{T}$ is a contraction with modulus $\\beta$ because of discounting.\n\nThis property guarantees that for any initial bounded guess $V_0$, the sequence generated by value function iteration, $V_{k+1} = \\mathcal{T}V_k$, converges to the unique solution $V$. The rate of convergence is geometric, with the distance to the fixed point decreasing by at least a factor of $\\beta$ at each step. This implies that the distance between successive iterates also shrinks: $\\lVert V_{k+1} - V_k \\rVert_\\infty \\le \\beta \\lVert V_k - V_{k-1} \\rVert_\\infty$.\n\n**2. Discretization and Algorithmic Implementation**\n\nTo solve the problem computationally, we must discretize the continuous state space. The asset holdings $a$ are restricted to a finite grid of $N$ points, $\\mathcal{A}_N = \\{a_0, a_1, \\ldots, a_{N-1}\\}$, evenly spaced on $[0, A_{\\max}]$. The value function is thus represented by a vector $\\mathbf{V}$ of length $N$, where $\\mathbf{V}[i] = V(a_i)$.\n\nThe value function iteration algorithm is as follows:\n- **Step 1: Initialization.** Start with an initial guess for the value function vector, typically $\\mathbf{V}_0 = \\mathbf{0}$. Set an iteration counter $k=0$ and a convergence tolerance $\\text{tol}$. The specified parameters are $\\beta=0.90$, $r=0.03$, $y=1.0$, $\\sigma=2.0$, $A_{\\max}=5.0$, and $\\text{tol}=10^{-4}$. The utility function is $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma} = -c^{-1}$ for $\\sigma=2.0$.\n- **Step 2: Iteration.** Apply the Bellman operator to update the value function. For each grid point $a_i \\in \\mathcal{A}_N$, solve the maximization problem:\n$$\n\\mathbf{V}_{k+1}[i] = \\max_{j \\in \\{0, \\ldots, N-1\\}} \\left\\{ u\\big((1+r)a_i + y - a_j\\big) + \\beta \\mathbf{V}_k[j] \\right\\}\n$$\nThe choice set for $a'$ is also restricted to the grid $\\mathcal{A}_N$. Choices $a_j$ that lead to non-positive consumption ($c_{ij} = (1+r)a_i+y-a_j \\le 0$) are infeasible. As specified, this is handled by assigning a very large negative utility to such choices, effectively excluding them from the maximization.\n- **Step 3: Convergence Check.** After each full update of the value function vector, compute the sup-norm error:\n$$\n\\epsilon_k = \\lVert \\mathbf{V}_{k+1} - \\mathbf{V}_k \\rVert_\\infty = \\max_{i \\in \\{0, \\ldots, N-1\\}} |\\mathbf{V}_{k+1}[i] - \\mathbf{V}_k[i]|\n$$\nThe loop continues until $\\epsilon_k  \\text{tol}$ or a maximum number of iterations is reached.\n\n**3. Efficient Vectorized Computation**\n\nA naive implementation with nested loops is computationally expensive, scaling as $\\mathcal{O}(N^2)$ per iteration. A superior approach uses vectorization. Let $\\mathbf{a}$ be the $N \\times 1$ column vector of asset grid points. The cash-on-hand for each state is the vector $\\mathbf{z} = (1+r)\\mathbf{a} + y$. We can construct an $N \\times N$ matrix of potential consumption values, $\\mathbf{C}$, where $\\mathbf{C}_{ij} = z_i - a_j$. This is computed via broadcasting as $\\mathbf{C} = \\mathbf{z} - \\mathbf{a}^T$.\n\nA corresponding $N \\times N$ utility matrix $\\mathbf{U}$ is then formed:\n$$\n\\mathbf{U}_{ij} =\n\\begin{cases}\n- (\\mathbf{C}_{ij})^{-1}  \\text{if } \\mathbf{C}_{ij}  0 \\\\\n-\\infty  \\text{if } \\mathbf{C}_{ij} \\le 0\n\\end{cases}\n$$\nThe value function update for all states can then be performed in a single, efficient operation:\n$$\n\\mathbf{V}_{k+1} = \\max_{\\text{axis}=1}(\\mathbf{U} + \\beta \\mathbf{V}_k^T)\n$$\nwhere `axis=1` indicates maximization over the columns (the choice variable $a'$ index $j$) and $\\mathbf{V}_k^T$ is broadcast across the rows of $\\mathbf{U}$.\n\n**4. Empirical Convergence Rate**\n\nThe theoretical convergence rate of the iteration is $\\beta$. We can estimate this empirically from the sequence of errors $\\{\\epsilon_k\\}$. The ratio of successive errors $\\rho_k = \\epsilon_{k+1} / \\epsilon_k$ should converge to $\\beta$. To obtain a stable estimate, we compute the median of the last $M=10$ such ratios available before the algorithm terminates. This provides the `observed_rate`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef vfi_solver(N, beta, sigma, y, r, A_max, tol, max_iter, M):\n    \"\"\"\n    Solves the deterministic consumption-savings problem using Value Function Iteration.\n\n    Args:\n        N (int): Number of points in the asset grid.\n        beta (float): Discount factor.\n        sigma (float): Coefficient of relative risk aversion.\n        y (float): Exogenous income.\n        r (float): Net interest rate.\n        A_max (float): Maximum asset level.\n        tol (float): Convergence tolerance for the supremum norm.\n        max_iter (int): Maximum number of iterations.\n        M (int): Number of recent error ratios to use for rate calculation.\n\n    Returns:\n        list: A list containing [N, number_of_iterations, observed_rate].\n    \"\"\"\n    # 1. Discretize the state space\n    a_grid = np.linspace(0, A_max, N)\n\n    # 2. Initialize value function and error list\n    V = np.zeros(N)\n    errors = []\n\n    # 3. Vectorized setup for efficient computation\n    # Reshape grids for broadcasting\n    a_grid_col = a_grid.reshape(N, 1)  # Current assets 'a'\n    a_grid_row = a_grid.reshape(1, N)  # Next period assets 'a''\n\n    # Calculate cash-on-hand for each current asset level\n    cash_on_hand = (1 + r) * a_grid_col + y\n\n    # Calculate consumption for all (a, a') pairs\n    consumption = cash_on_hand - a_grid_row\n    \n    # Define the utility function\n    # Note: As per problem, sigma is not 1.\n    def u(c):\n        return (c**(1 - sigma)) / (1 - sigma)\n\n    # Pre-calculate utility matrix.\n    # Set utility of non-positive consumption to a very large negative number\n    # to enforce feasibility during maximization.\n    utility = np.full((N, N), -np.inf)\n    positive_c_mask = consumption  0\n    utility[positive_c_mask] = u(consumption[positive_c_mask])\n    \n    # 4. Value Function Iteration loop\n    num_iterations = 0\n    for k in range(max_iter):\n        # Apply the Bellman operator in a vectorized fashion\n        # V is (N,), broadcasting makes it equivalent to V.reshape(1, N)\n        # So (utility + beta * V) is an (N, N) matrix.\n        next_V = np.max(utility + beta * V, axis=1)\n        \n        # Calculate supremum norm error\n        error = np.max(np.abs(next_V - V))\n        errors.append(error)\n        \n        # Update value function\n        V = next_V\n        num_iterations = k + 1\n        \n        # Check for convergence\n        if error  tol:\n            break\n\n    # 5. Calculate the empirical convergence rate\n    observed_rate = np.nan\n    if len(errors)  1:\n        # Ratios are epsilon_{k+1} / epsilon_k\n        ratios = np.array(errors[1:]) / np.array(errors[:-1])\n        \n        # Take the median of the last M ratios (or fewer if not enough exist)\n        num_ratios_to_consider = min(M, len(ratios))\n        if num_ratios_to_consider  0:\n            last_ratios = ratios[-num_ratios_to_consider:]\n            observed_rate = float(np.median(last_ratios))\n\n    # Round the rate to three decimal places\n    if not np.isnan(observed_rate):\n        observed_rate = round(observed_rate, 3)\n\n    return [N, num_iterations, observed_rate]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    params = {\n        'beta': 0.90,\n        'sigma': 2.0,\n        'y': 1.0,\n        'r': 0.03,\n        'A_max': 5.0,\n        'tol': 1e-4,\n        'max_iter': 1000,\n        'M': 10\n    }\n    \n    # Define the test cases from the problem statement.\n    test_cases = [5, 20, 80, 160]\n\n    results = []\n    for N in test_cases:\n        # Run the solver for one grid size\n        result = vfi_solver(N=N, **params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [[N1,I1,R1],[N2,I2,R2]]\n    # Using str().replace() is a reliable way to get this format without spaces.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}