{
    "hands_on_practices": [
        {
            "introduction": "This first practice confronts a core concept: \"pseudorandom\" does not mean \"unpredictable.\" We will act as cryptanalysts to \"crack\" a Linear Congruential Generator (LCG) by observing its output. By solving a simple system of linear congruences, you will see how the deterministic nature of an LCG makes its future outputs perfectly predictable, a stark contrast to cryptographically secure generators . This exercise provides a crucial lesson on the limitations of simple PRNGs and the importance of choosing the right tool for the job.",
            "id": "2423284",
            "problem": "You are given two classes of generators that are used to produce discrete uniform variates in computational economics and finance: a linear congruential generator and a cryptographically secure generator constructed from a cryptographic hash in counter mode. Your task is to implement a predictor that, after passively observing three consecutive outputs of a generator, attempts to predict the next output. The predictor should succeed for the linear congruential generator and fail for the cryptographically secure generator.\n\nFundamental base and definitions:\n- A linear congruential generator (LCG) is defined by the recurrence\n$$X_{t+1} \\equiv a X_t + c \\pmod{m},$$\nwith state $X_t \\in \\{0,1,\\dots,m-1\\}$, modulus $m \\in \\mathbb{N}$, multiplier $a \\in \\{0,1,\\dots,m-1\\}$, increment $c \\in \\{0,1,\\dots,m-1\\}$, and an initial seed $X_0$. It produces discrete uniform variates by mapping $U_t = X_t/m \\in [0,1)$.\n- The cryptographically secure generator used here is specified as follows: let $H$ be the Secure Hash Algorithm 256-bit hash function (SHA-256). For a given byte-string seed $s$ and a prime modulus $m$, define\n$$X_t \\equiv \\mathrm{int}\\big(H(s \\,\\|\\, \\mathrm{encode}_{8}(t))\\big) \\bmod m,$$\nwhere $\\mathrm{encode}_{8}(t)$ is the $8$-byte big-endian encoding of the nonnegative integer $t$, $\\|$ denotes concatenation, and $\\mathrm{int}(\\cdot)$ interprets the $32$-byte hash output as a nonnegative integer in base $256$. This construction is a standard counter-based pseudorandom number generator using a cryptographic primitive, and its outputs are intended to be computationally unpredictable.\n\nYour program must implement the following:\n1. A function that, given a modulus $m$ and three consecutive observed states $(x_0,x_1,x_2)$ from a single generator, attempts to infer parameters $(a,c)$ consistent with the LCG definition and predict the next state $\\hat{x}_3 \\equiv a x_2 + c \\pmod{m}$. If multiple $(a,c)$ satisfy the congruences with the given observations or if modular inversion is impossible in your method, you may choose any consistent rule, but you must return a single integer prediction $\\hat{x}_3 \\in \\{0,1,\\dots,m-1\\}$.\n2. For each test case, compute the true next state $x_3$ using the specified ground-truth generator. Report whether your predictor’s $\\hat{x}_3$ equals the true $x_3$ as a boolean.\n\nTest suite:\n- Case A (LCG, happy path): $m = 2147483647$, $a = 48271$, $c = 0$, $X_0 = 12345$.\n- Case B (LCG, nonzero increment): $m = 1000003$, $a = 35011$, $c = 7919$, $X_0 = 424242$.\n- Case C (LCG, boundary where $a = 1$): $m = 101$, $a = 1$, $c = 1$, $X_0 = 17$.\n- Case D (cryptographically secure generator): prime modulus $m = 4294967291$, seed $s$ equal to the ASCII byte-string of the text “econ-secure”. The generator is $X_t \\equiv \\mathrm{int}\\big(H(s \\,\\|\\, \\mathrm{encode}_{8}(t))\\big) \\bmod m$ with $H$ being SHA-256.\n\nFor each case, proceed as follows:\n- Generate the first four consecutive states $X_0, X_1, X_2, X_3$ using the specified ground-truth generator and parameters.\n- Give the predictor only $(m, X_0, X_1, X_2)$ and let it compute $\\hat{X}_3$ by fitting an LCG as per item $1$ above.\n- Compute a boolean $b$ that is $\\mathrm{True}$ if $\\hat{X}_3 = X_3$ and $\\mathrm{False}$ otherwise.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets (e.g., “[True,False,True,False]”), in the order Case A, Case B, Case C, Case D.\n\nNotes and requirements:\n- All quantities are dimensionless counts; no physical units are involved.\n- Angles are not involved.\n- Do not use percentage signs; no percentages are required.\n- Design your predictor strictly from the definitions provided. Do not hard-code the answers.\n- Ensure your program is self-contained and does not read any input.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the fields of number theory and cryptography, is well-posed with a clear and achievable objective, and is free of ambiguity or contradiction. It correctly frames a fundamental dichotomy between statistically random sequences, which may be predictable, and cryptographically secure sequences, which are designed to be unpredictable. We shall therefore proceed with a full solution.\n\nThe task is to differentiate between a linear congruential generator (LCG) and a cryptographically secure pseudorandom number generator (CSPRNG) by attempting to predict the next term in a sequence after observing three consecutive terms. The success or failure of this prediction serves to classify the generator.\n\nThe LCG is defined by the linear recurrence relation:\n$$X_{t+1} \\equiv a X_t + c \\pmod{m}$$\nwhere $a$ is the multiplier, $c$ is the increment, and $m$ is the modulus. This linear structure is its critical vulnerability. Given three observed consecutive states, $x_0$, $x_1$, and $x_2$, we can construct a system of two linear congruences with two unknowns, a and c:\n$$\n\\begin{cases}\nx_1 \\equiv a x_0 + c \\pmod{m} \\\\\nx_2 \\equiv a x_1 + c \\pmod{m}\n\\end{cases}\n$$\nTo solve for the unknown parameters, we first eliminate $c$ by subtracting the first congruence from the second:\n$$x_2 - x_1 \\equiv (a x_1 + c) - (a x_0 + c) \\pmod{m}$$\n$$x_2 - x_1 \\equiv a (x_1 - x_0) \\pmod{m}$$\nLet $\\Delta_0 = x_1 - x_0$ and $\\Delta_1 = x_2 - x_1$. The congruence simplifies to $\\Delta_1 \\equiv a \\Delta_0 \\pmod{m}$. We can solve for $a$ by multiplying by the modular multiplicative inverse of $\\Delta_0$ modulo $m$. This inverse, $\\Delta_0^{-1}$, exists if and only if $\\text{gcd}(\\Delta_0, m) = 1$. The test cases utilize prime moduli $m$, for which this condition holds unless $\\Delta_0$ is a multiple of $m$, i.e., $x_1 \\equiv x_0 \\pmod{m}$. Assuming $x_1 \\not\\equiv x_0 \\pmod{m}$, we find a unique solution for the multiplier $\\hat{a}$:\n$$\\hat{a} \\equiv \\Delta_1 \\cdot (\\Delta_0 \\pmod m)^{-1} \\pmod{m}$$\nOnce $\\hat{a}$ is determined, we substitute it back into the first congruence to solve for the increment $\\hat{c}$:\n$$\\hat{c} \\equiv x_1 - \\hat{a} x_0 \\pmod{m}$$\nHaving recovered the parameters $(\\hat{a}, \\hat{c})$, we can predict the next state, $\\hat{x}_3$, using the LCG recurrence:\n$$\\hat{x}_3 \\equiv \\hat{a} x_2 + \\hat{c} \\pmod{m}$$\nIf the observed sequence was indeed produced by an LCG, our recovered parameters $(\\hat{a}, \\hat{c})$ will be identical to the generator's true parameters, and our prediction $\\hat{x}_3$ will equal the true state $X_3$.\n\nThe cryptographically secure generator, in contrast, is designed to make such prediction computationally infeasible. Its definition is:\n$$X_t \\equiv \\mathrm{int}\\big(H(s \\,\\|\\, \\mathrm{encode}_{8}(t))\\big) \\bmod m$$\nHere, $H$ is the SHA-$256$ cryptographic hash function. The output $X_t$ is a complex, non-linear transformation of a simple counter $t$. There is no simple algebraic relationship between consecutive terms $X_t$ and $X_{t+1}$. While our predictor can still be applied to a sequence from this generator—it will always find some LCG parameters $(\\hat{a}, \\hat{c})$ that fit any three points $(x_0, x_1, x_2)$ (provided $x_1 \\not\\equiv x_0 \\pmod m$)—these parameters are mere artifacts. They describe a line passing through three effectively random points and have no bearing on the underlying generation process. The true next state, $X_3$, is determined by an independent evaluation of the hash function at $t=3$. The probability that our prediction $\\hat{x}_3$ matches $X_3$ is vanishingly small, approximately $1/m$.\n\nThus, our algorithm will generate the true sequence $(X_0, X_1, X_2, X_3)$ for each test case, apply the LCG predictor to $(X_0, X_1, X_2)$ to compute $\\hat{X}_3$, and report whether $\\hat{X}_3 = X_3$. We expect a positive result for all LCG cases and a negative result for the CSPRNG case, thereby demonstrating the principle of unpredictability.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport hashlib\n\ndef solve():\n    \"\"\"\n    Main function to run the prediction tests and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A (LCG, happy path)\",\n            \"type\": \"lcg\",\n            \"m\": 2147483647,\n            \"a\": 48271,\n            \"c\": 0,\n            \"x0\": 12345,\n            \"seed\": None,\n        },\n        {\n            \"name\": \"Case B (LCG, nonzero increment)\",\n            \"type\": \"lcg\",\n            \"m\": 1000003,\n            \"a\": 35011,\n            \"c\": 7919,\n            \"x0\": 424242,\n            \"seed\": None,\n        },\n        {\n            \"name\": \"Case C (LCG, boundary where a = 1)\",\n            \"type\": \"lcg\",\n            \"m\": 101,\n            \"a\": 1,\n            \"c\": 1,\n            \"x0\": 17,\n            \"seed\": None,\n        },\n        {\n            \"name\": \"Case D (cryptographically secure generator)\",\n            \"type\": \"csg\",\n            \"m\": 4294967291,\n            \"a\": None,\n            \"c\": None,\n            \"x0\": None,\n            \"seed\": b\"econ-secure\",\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Generate the ground-truth sequence of 4 states (X0, X1, X2, X3)\n        if case[\"type\"] == \"lcg\":\n            true_sequence = generate_lcg_sequence(\n                case[\"m\"], case[\"a\"], case[\"c\"], case[\"x0\"], 4\n            )\n        elif case[\"type\"] == \"csg\":\n            true_sequence = generate_csg_sequence(case[\"m\"], case[\"seed\"], 4)\n        else:\n            raise ValueError(f\"Unknown generator type: {case['type']}\")\n\n        x0, x1, x2, x3_true = true_sequence\n\n        # Predict the next state assuming an LCG model\n        x3_pred = predict_next_lcg(case[\"m\"], x0, x1, x2)\n\n        # Compare the prediction with the true state\n        results.append(x3_pred == x3_true)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_lcg_sequence(m, a, c, x0, count):\n    \"\"\"\n    Generates a sequence of numbers from a Linear Congruential Generator.\n    X_{t+1} = (a * X_t + c) % m\n    \"\"\"\n    sequence = []\n    x_current = x0\n    for _ in range(count):\n        sequence.append(x_current)\n        x_current = (a * x_current + c) % m\n    return sequence\n\ndef generate_csg_sequence(m, seed, count):\n    \"\"\"\n    Generates a sequence from a SHA-256 counter-based generator.\n    X_t = int(SHA256(seed || encode_8(t))) % m\n    \"\"\"\n    sequence = []\n    for t in range(count):\n        t_bytes = t.to_bytes(8, 'big')\n        data_to_hash = seed + t_bytes\n        h = hashlib.sha256(data_to_hash).digest()\n        int_hash = int.from_bytes(h, 'big')\n        x_t = int_hash % m\n        sequence.append(x_t)\n    return sequence\n\ndef predict_next_lcg(m, x0, x1, x2):\n    \"\"\"\n    Predicts the next state (x3) from three previous states (x0, x1, x2)\n    by inferring the parameters of an LCG.\n    \"\"\"\n    # System to solve for a_hat, c_hat:\n    # x1 = a*x0 + c (mod m)\n    # x2 = a*x1 + c (mod m)\n    # Subtracting gives: x2 - x1 = a * (x1 - x0) (mod m)\n    \n    delta0 = (x1 - x0) % m\n    delta1 = (x2 - x1) % m\n\n    try:\n        # To solve a_hat * delta0 = delta1 (mod m), we need the modular inverse of delta0.\n        # pow(base, -1, mod) computes this efficiently.\n        inv_delta0 = pow(delta0, -1, m)\n        a_hat = (delta1 * inv_delta0) % m\n    except ValueError:\n        # This occurs if gcd(delta0, m) != 1. Since all test case moduli are prime,\n        # this only happens if delta0 = 0 (mod m), i.e., x1 == x0.\n        # If x1 = x0, the congruence is 0 = delta1 (mod m), so we must have x2 = x1.\n        if delta1 == 0:  # Case x0 = x1 = x2, a constant sequence\n            # The recurrence is x0 = a*x0 + c. Prediction for x3 is a*x2 + c = a*x0 + c = x0.\n            # So, the next term is also x0 (or x2, which is the same).\n            return x2\n        else: # Case x0 = x1 but x2 != x1.\n            # This sequence cannot be from an LCG. As per problem, we must return a prediction.\n            # We choose a default of 0.\n            return 0\n\n    # Solve for c_hat using c_hat = x1 - a_hat * x0 (mod m)\n    c_hat = (x1 - a_hat * x0) % m\n\n    # Predict x3_hat = a_hat * x2 + c_hat (mod m)\n    x3_hat = (a_hat * x2 + c_hat) % m\n\n    return x3_hat\n\nsolve()\n```"
        },
        {
            "introduction": "Having seen that simple generators can be predictable, we now turn to a different kind of pitfall: introducing bias through incorrect usage. A common task is to generate a random integer within a specific range, and a tempting shortcut is to use the modulo operator. This exercise challenges you to analytically prove and quantify the bias of this naive method using the total variation distance, and to understand why the rejection sampling method guarantees a truly uniform outcome . Mastering this distinction is fundamental to ensuring the statistical integrity of any simulation.",
            "id": "2423278",
            "problem": "You are given access to a base Pseudorandom Number Generator (PRNG) that produces independent draws uniformly over the integer set $\\{0,1,\\dots,m-1\\}$, where $m = 2^{32}$. Your task is to analyze how to generate a random integer in a target range $[a,b]$ using this source and to quantify the bias introduced by a naive implementation. This problem is motivated by Monte Carlo algorithms widely used in computational economics and finance, where biased discrete uniform sampling can distort estimates (for example, state transitions, resampling steps in Sequential Monte Carlo, or discrete choice simulation). You must derive the relevant expressions from first principles and implement a program that computes exact bias metrics and resource usage without simulation.\n\nFundamental definitions and assumptions:\n- The base PRNG output $X$ is exactly uniform over $\\{0,1,\\dots,m-1\\}$ with $m=2^{32}$.\n- Let $n = b-a+1$ be the size of the target integer range $[a,b]$.\n- The naive “modulo” mapping produces $Y_{\\mathrm{mod}} = a + (X \\bmod n)$.\n- The rejection sampling mapping defines $q = \\lfloor m/n \\rfloor$, accepts a draw if $X  qn$, and returns $Y_{\\mathrm{rej}} = a + (X \\bmod n)$ on acceptance; otherwise it redraws from the base PRNG until acceptance.\n\nYour tasks:\n1) Derive from first principles the exact probability mass function of $Y_{\\mathrm{mod}}$ in terms of $m$, $n$, $q = \\lfloor m/n \\rfloor$, and $r = m - qn$, and then derive a closed-form expression for the total variation distance between the distribution of $Y_{\\mathrm{mod}}$ and the ideal discrete uniform distribution on $[a,b]$. The total variation distance between two distributions with probabilities $\\{p_i\\}$ and $\\{u_i\\}$ over $n$ outcomes is defined as\n$$\n\\mathrm{TVD} = \\frac{1}{2}\\sum_{i=1}^{n} \\left| p_i - u_i \\right|.\n$$\n\n2) Prove that the rejection sampling mapping yields an exactly uniform distribution on $[a,b]$ (that is, the total variation distance is $0$), and derive its acceptance probability $\\alpha$ and the expected number of base PRNG draws per accepted output. Express your answers in terms of $m$, $n$, $q$, and $r$.\n\n3) Implement a program that, for each test case below, computes three quantities using your derived formulas (not by simulation):\n- The total variation distance for the modulo method, $\\mathrm{TVD}_{\\mathrm{mod}}$.\n- The total variation distance for the rejection method, $\\mathrm{TVD}_{\\mathrm{rej}}$.\n- The expected number of base PRNG draws per accepted output under rejection sampling, $\\mathbb{E}[D]$.\n\nTest suite (use the following $(a,b)$ pairs; these are designed to include a common case, boundary cases, and edge cases):\n- Case 1 (general case): $(a,b) = (0, 9)$ so $n=10$.\n- Case 2 (degenerate range): $(a,b) = (5, 5)$ so $n=1$.\n- Case 3 (includes negative): $(a,b) = (-2, 2)$ so $n=5$.\n- Case 4 (divides $m$ exactly): $(a,b) = (0, 65535)$ so $n=65536$.\n- Case 5 (large nondividing): $(a,b) = (-10, 10^{6})$ so $n = 1{,}000{,}011$.\n\nPrecise output requirements:\n- For each test case, output a list $[\\mathrm{TVD}_{\\mathrm{mod}}, \\mathrm{TVD}_{\\mathrm{rej}}, \\mathbb{E}[D]]$ in this order.\n- Your program should produce a single line of output containing the results for all test cases, as a comma-separated list of these per-case lists, enclosed in square brackets. For example: \"[[x1,y1,z1],[x2,y2,z2],...]\" but with no spaces. Your program must output exactly one such line.\n- All results must be numeric (floating-point is acceptable). No random sampling is permitted in your solution; compute values using closed-form expressions only.",
            "solution": "We begin from first principles. The base Pseudorandom Number Generator (PRNG) draws $X$ uniformly from the finite set $\\{0,1,\\dots,m-1\\}$, with $m = 2^{32}$. For a target integer interval $[a,b]$, define $n = b-a+1 \\in \\mathbb{N}$.\n\nNaive modulo mapping analysis:\nDefine the naive mapping $Y_{\\mathrm{mod}} = a + (X \\bmod n)$. Since $X$ is uniform on $\\{0,1,\\dots,m-1\\}$ and $(X \\bmod n)$ takes values in $\\{0,1,\\dots,n-1\\}$, the probability that $(X \\bmod n) = i$ is proportional to how many integers in $\\{0,1,\\dots,m-1\\}$ have remainder $i$ modulo $n$.\n\nLet $q = \\left\\lfloor \\frac{m}{n} \\right\\rfloor$ and $r = m - qn = m \\bmod n$, where $r \\in \\{0,1,\\dots,n-1\\}$. There are exactly $r$ remainders that occur $q+1$ times and $n-r$ remainders that occur $q$ times when we partition $\\{0,1,\\dots,m-1\\}$ into congruence classes modulo $n$. Therefore, for remainder $i$,\n- For $r$ many values of $i$, $\\mathbb{P}[(X \\bmod n)=i] = \\frac{q+1}{m}$.\n- For $n-r$ many values of $i$, $\\mathbb{P}[(X \\bmod n)=i] = \\frac{q}{m}$.\n\nThe ideal discrete uniform distribution on $\\{0,1,\\dots,n-1\\}$ assigns probability $\\frac{1}{n}$ to each remainder. The absolute deviations are\n$$\n\\left|\\frac{q+1}{m} - \\frac{1}{n}\\right| = \\frac{n-r}{mn}, \\quad\n\\left|\\frac{q}{m} - \\frac{1}{n}\\right| = \\frac{r}{mn}.\n$$\nThe total variation distance is thus\n$$\n\\mathrm{TVD}_{\\mathrm{mod}} = \\frac{1}{2}\\left( r \\cdot \\frac{n-r}{mn} + (n-r) \\cdot \\frac{r}{mn} \\right) = \\frac{r(n-r)}{mn}.\n$$\nThis closed-form expression quantifies the bias introduced by the modulo method whenever $r \\neq 0$ (that is, whenever $n$ does not divide $m$). If $r=0$, then $\\mathrm{TVD}_{\\mathrm{mod}} = 0$ and the mapping is exactly uniform.\n\nRejection sampling analysis:\nDefine $q = \\left\\lfloor \\frac{m}{n} \\right\\rfloor$ as before and accept a draw if $X  qn$, returning $Y_{\\mathrm{rej}} = a + (X \\bmod n)$ on acceptance; otherwise, redraw from the base PRNG. Because the accepted set has size $qn$ and is partitioned into $n$ congruence classes each of size exactly $q$, conditional on acceptance the remainder $(X \\bmod n)$ is exactly uniform over $\\{0,1,\\dots,n-1\\}$. Therefore,\n$$\n\\mathrm{TVD}_{\\mathrm{rej}} = 0.\n$$\nThe acceptance probability is $\\alpha = \\frac{qn}{m} = 1 - \\frac{r}{m}$, and the number of base draws $D$ required is a geometric random variable with success probability $\\alpha$, so the expected number of base draws per accepted output is\n$$\n\\mathbb{E}[D] = \\frac{1}{\\alpha} = \\frac{1}{1 - \\frac{r}{m}}.\n$$\nIn the special case $r=0$ (that is, when $n$ divides $m$), we have $\\alpha = 1$ and $\\mathbb{E}[D] = 1$.\n\nAlgorithmic design:\nGiven $(a,b)$, compute $n = b-a+1$, then compute $q = \\left\\lfloor \\frac{m}{n} \\right\\rfloor$ and $r = m - qn$. Using these, evaluate:\n- $\\mathrm{TVD}_{\\mathrm{mod}} = \\frac{r(n-r)}{mn}$.\n- $\\mathrm{TVD}_{\\mathrm{rej}} = 0$.\n- $\\mathbb{E}[D] = \\frac{1}{1 - \\frac{r}{m}}$ (with the convention $\\mathbb{E}[D]=1$ when $r=0$).\nNo random sampling is required; these are closed-form computations.\n\nTest suite coverage:\n- Case $n=10$ tests a general nondividing case ($r \\neq 0$).\n- Case $n=1$ tests a degenerate interval (always uniform, zero bias).\n- Case $n=5$ tests inclusion of negative bounds and a small nondividing case.\n- Case $n=65536$ tests the exact-dividing case ($n$ divides $m$).\n- Case $n=1{,}000{,}011$ tests a large nondividing case.\n\nOutput format:\nFor each case, output $[\\mathrm{TVD}_{\\mathrm{mod}}, \\mathrm{TVD}_{\\mathrm{rej}}, \\mathbb{E}[D]]$. Aggregate all five case results as a single comma-separated list enclosed in square brackets, with no spaces, e.g., \"[[x1,y1,z1],[x2,y2,z2],...]\" printed on a single line.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_metrics(a: int, b: int, m: int) - tuple[float, float, float]:\n    \"\"\"\n    Compute (TVD_mod, TVD_rej, E[D]) for target range [a,b],\n    given base PRNG modulus m (here m=2^32).\n    \"\"\"\n    n = b - a + 1\n    if n = 0:\n        raise ValueError(\"Invalid range: b must be = a.\")\n    # Compute q and r\n    q = m // n\n    r = m - q * n  # same as m % n, but avoids negatives\n    # Total variation distance for modulo method: r*(n-r)/(m*n)\n    tvd_mod = (r * (n - r)) / (m * n)\n    # Rejection sampling is exactly uniform\n    tvd_rej = 0.0\n    # Expected number of base draws per accepted output: 1 / (1 - r/m)\n    if r == 0:\n        expected_draws = 1.0\n    else:\n        expected_draws = 1.0 / (1.0 - (r / m))\n    return float(tvd_mod), float(tvd_rej), float(expected_draws)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 9),          # n=10\n        (5, 5),          # n=1\n        (-2, 2),         # n=5\n        (0, 65535),      # n=65536 (divides 2^32)\n        (-10, 10**6),    # n=1_000_011\n    ]\n    # Base PRNG modulus m = 2^32\n    m = 2 ** 32\n\n    results_str = []\n    for a, b in test_cases:\n        tvd_mod, tvd_rej, expected_draws = compute_metrics(a, b, m)\n        # Format numbers using a consistent, compact representation\n        # Use .17g for good precision without excessive digits\n        sub = \"[\" + \",\".join([\n            format(tvd_mod, \".17g\"),\n            format(tvd_rej, \".17g\"),\n            format(expected_draws, \".17g\"),\n        ]) + \"]\"\n        results_str.append(sub)\n\n    # Final print statement in the exact required format (no spaces).\n    print(f\"[{','.join(results_str)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "We now transition from analyzing PRNG flaws to leveraging their properties for powerful applications. This final practice brings theory to life by using uniform variates to construct an approximation of the standard Normal distribution, a cornerstone of financial modeling. You will implement a simulation based on the Central Limit Theorem, which posits that the sum of independent random variables tends toward a normal distribution, and then use rigorous statistical tests to verify the quality of your approximation . This exercise demonstrates a classic and elegant method for generating non-uniform variates and provides a concrete feel for one of the most important theorems in statistics.",
            "id": "2423303",
            "problem": "Design a complete, runnable program that demonstrates, via simulation, that the sum of $12$ independent and identically distributed Uniform random variables on $[0,1]$ is a good approximation to a Standard Normal random variable. Your program must build from first principles used in computational economics and finance: start with the definition of a Uniform distribution, the linearity of expectation, variance additivity under independence, and the Central Limit Theorem (CLT). Do not rely on any built-in Normal random number generator to create the approximation; instead, simulate Uniform random numbers using a pseudorandom number generator, then form the sum.\n\nYou must use a Linear Congruential Generator (LCG) to produce Uniform variates. A Linear Congruential Generator is defined by the recurrence\n$$\nX_{k+1} = (a X_k + c) \\bmod m,\n$$\nwith integers $a$, $c$, and modulus $m$. The pseudorandom variates $U_k$ are obtained by scaling $X_k$ to $[0,1)$ via $U_k = X_k / m$. Use the well-known choice $m = 2^{32}$, $a = 1664525$, and $c = 1013904223$. To obtain $N$ independent rows of $12$ Uniform draws, initialize $N$ parallel streams with states $X_0^{(i)} = (\\text{seed} + i) \\bmod m$ for $i \\in \\{0,1,\\dots,N-1\\}$, and advance each stream $12$ steps to generate $12$ Uniform draws per row.\n\nFor each row $i$, compute\n$$Z_i = \\sum_{j=1}^{12} U_{i,j} - 6.$$\nUse the following facts as the fundamental base: for $U \\sim \\text{Uniform}(0,1)$, $E[U] = 1/2$ and $\\operatorname{Var}(U) = 1/12$, sums of independent variables have mean equal to the sum of means and variance equal to the sum of variances, and the Central Limit Theorem asserts that standardized sums of independent and identically distributed variables converge in distribution to the Standard Normal as the number of summands grows. Note that the above centering by $6$ standardizes the sum to have mean $0$ and variance $1$ because the variance of the sum of $12$ independent Uniform random variables on $[0,1]$ is $12 \\cdot (1/12) = 1$.\n\nYour program must quantify how close the simulated $\\{Z_i\\}_{i=1}^N$ are to a Standard Normal distribution $N(0,1)$ using three diagnostics:\n- the sample mean $\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N Z_i$,\n- the sample variance $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N (Z_i - \\hat{\\mu})^2$ (use denominator $N$),\n- the Kolmogorov–Smirnov distance $D_N = \\sup_{z \\in \\mathbb{R}} \\left| \\hat{F}_N(z) - \\Phi(z) \\right|$, where $\\hat{F}_N$ is the empirical cumulative distribution function of $\\{Z_i\\}$ and $\\Phi$ is the cumulative distribution function of $N(0,1)$.\n\nConstruct a test suite of three cases to cover a general case, a low-sample boundary case, and a large-seed edge case. For each case, the parameters are $(N, \\text{seed}, \\tau_\\mu, \\tau_{\\sigma^2}, \\tau_{KS})$, where $\\tau_\\mu$ is the tolerance for $|\\hat{\\mu} - 0|$, $\\tau_{\\sigma^2}$ is the tolerance for $|\\hat{\\sigma}^2 - 1|$, and $\\tau_{KS}$ is the tolerance for $D_N$. The three test cases to implement are:\n- Case A (general, large sample): $(N, \\text{seed}, \\tau_\\mu, \\tau_{\\sigma^2}, \\tau_{KS}) = (200000, 987654321, 0.004, 0.004, 0.020)$.\n- Case B (boundary, low sample): $(N, \\text{seed}, \\tau_\\mu, \\tau_{\\sigma^2}, \\tau_{KS}) = (4000, 2024, 0.050, 0.060, 0.080)$.\n- Case C (edge, extreme seed): $(N, \\text{seed}, \\tau_\\mu, \\tau_{\\sigma^2}, \\tau_{KS}) = (150000, 4294967295, 0.004, 0.005, 0.020)$.\n\nFor each case, produce a boolean result defined as\n$$\n\\text{pass} = \\left( |\\hat{\\mu}| \\le \\tau_\\mu \\right) \\land \\left( |\\hat{\\sigma}^2 - 1| \\le \\tau_{\\sigma^2} \\right) \\land \\left( D_N \\le \\tau_{KS} \\right).\n$$\n\nYour program should produce a single line of output containing the results for Cases A, B, and C in order, as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\"). There are no physical units or angles to report, and no percentages are required; all numerical comparisons are absolute tolerances as specified above.",
            "solution": "The problem statement has been analyzed and is deemed valid. It presents a well-posed, scientifically grounded, and objective computational task rooted in fundamental principles of probability theory and numerical methods. It is a standard exercise in computational statistics, relevant to the field of computational economics and finance. There are no contradictions, ambiguities, or factual inaccuracies. We proceed with a complete solution.\n\nThe objective is to demonstrate, via simulation, the Central Limit Theorem (CLT) by approximating a Standard Normal distribution. This is achieved by summing a fixed number of independent and identically distributed (i.i.d.) uniform random variables. The theoretical basis for this approximation is firm.\n\nFirst, consider a random variable $U$ uniformly distributed on the interval $[0,1]$. Its probability density function is $f_U(u) = 1$ for $u \\in [0,1]$ and $0$ otherwise. The key properties of this distribution are its expectation (mean) and variance:\n$$\nE[U] = \\int_0^1 u \\, du = \\frac{1}{2}\n$$\n$$\n\\operatorname{Var}(U) = E[U^2] - (E[U])^2 = \\int_0^1 u^2 \\, du - \\left(\\frac{1}{2}\\right)^2 = \\frac{1}{3} - \\frac{1}{4} = \\frac{1}{12}\n$$\nThe problem proposes constructing a new random variable, $S_{12}$, as the sum of $n=12$ such i.i.d. uniform variables, $\\{U_j\\}_{j=1}^{12}$:\n$$\nS_{12} = \\sum_{j=1}^{12} U_j\n$$\nBy the linearity of expectation, the mean of this sum is the sum of the means:\n$$\nE[S_{12}] = E\\left[\\sum_{j=1}^{12} U_j\\right] = \\sum_{j=1}^{12} E[U_j] = 12 \\times \\frac{1}{2} = 6\n$$\nBecause the variables $U_j$ are independent, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(S_{12}) = \\operatorname{Var}\\left(\\sum_{j=1}^{12} U_j\\right) = \\sum_{j=1}^{12} \\operatorname{Var}(U_j) = 12 \\times \\frac{1}{12} = 1\n$$\nThe Central Limit Theorem states that, for a sufficiently large number of summands $n$, the distribution of the standardized sum of i.i.d. random variables approaches the Standard Normal distribution, $\\mathcal{N}(0,1)$. The standardized variable, denoted $Z$, is obtained by subtracting the mean and dividing by the standard deviation. In our case, with $n=12$, the standard deviation is $\\sqrt{\\operatorname{Var}(S_{12})} = \\sqrt{1} = 1$. Thus, the standardized variable is:\n$$\nZ = \\frac{S_{12} - E[S_{12}]}{\\sqrt{\\operatorname{Var}(S_{12})}} = \\sum_{j=1}^{12} U_j - 6\n$$\nThe problem posits that $n=12$ is sufficiently large for this approximation to be reasonably accurate. Our task is to verify this claim computationally.\n\nThe simulation algorithm proceeds as follows. We must first generate the uniform variates $U_j$. The problem specifies a Linear Congruential Generator (LCG), a common class of pseudorandom number generators defined by the recurrence relation:\n$$\nX_{k+1} = (a X_k + c) \\bmod m\n$$\nThe integer sequence $\\{X_k\\}$ is then mapped to the interval $[0,1)$ to produce pseudorandom uniform variates $U_k$:\n$$\nU_k = \\frac{X_k}{m}\n$$\nThe specified parameters are the modulus $m = 2^{32}$, the multiplier $a = 1664525$, and the increment $c = 1013904223$.\n\nTo generate $N$ independent samples of $Z$, we must ensure that the sequences of $12$ uniform variates used for each sample are themselves independent. This is achieved by running $N$ parallel LCG streams, each starting from a different initial state (seed). The seed for the $i$-th stream (for $i \\in \\{0, 1, \\dots, N-1\\}$) is defined as $X_0^{(i)} = (\\text{seed} + i) \\bmod m$. For each stream $i$, we generate $12$ variates $\\{U_{i,j}\\}_{j=1}^{12}$ by iterating its LCG $12$ times. From these, we compute one sample of our target variable:\n$$\nZ_i = \\left(\\sum_{j=1}^{12} U_{i,j}\\right) - 6\n$$\nThis process is repeated for all $N$ streams to obtain a sample $\\{Z_i\\}_{i=1}^N$.\n\nTo quantify the quality of the approximation, we compute three diagnostic statistics on the generated sample $\\{Z_i\\}$:\n1.  The sample mean, $\\hat{\\mu} = \\frac{1}{N}\\sum_{i=1}^N Z_i$. This should be close to the theoretical mean, $E[Z] = 0$.\n2.  The sample variance, $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_{i=1}^N (Z_i - \\hat{\\mu})^2$. This should be close to the theoretical variance, $\\operatorname{Var}(Z) = 1$. The use of denominator $N$ is specified.\n3.  The Kolmogorov-Smirnov (KS) distance, $D_N = \\sup_{z \\in \\mathbb{R}} \\left| \\hat{F}_N(z) - \\Phi(z) \\right|$. Here, $\\hat{F}_N(z)$ is the empirical cumulative distribution function (ECDF) of the sample, and $\\Phi(z)$ is the CDF of the Standard Normal distribution. The KS distance measures the maximum absolute difference between the observed and theoretical CDFs, providing a robust measure of distributional fit.\n\nFor each of the three test cases, a boolean result is determined by comparing the absolute deviation of the sample mean from $0$, the absolute deviation of the sample variance from $1$, and the KS distance against their respective tolerances $(\\tau_\\mu, \\tau_{\\sigma^2}, \\tau_{KS})$. The case passes if and only if all three diagnostics fall within their tolerance bounds. The implementation will perform these calculations efficiently using vectorized operations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest, norm\n\ndef solve():\n    \"\"\"\n    Runs a simulation to validate that the sum of 12 uniform random variables\n    approximates a standard normal distribution, based on the Central Limit Theorem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (general, large sample)\n        (200000, 987654321, 0.004, 0.004, 0.020),\n        # Case B (boundary, low sample)\n        (4000, 2024, 0.050, 0.060, 0.080),\n        # Case C (edge, extreme seed)\n        (150000, 4294967295, 0.004, 0.005, 0.020),\n    ]\n\n    results = []\n    \n    # LCG parameters\n    m_val = 2**32\n    a_val = 1664525\n    c_val = 1013904223\n    \n    # Use 64-bit unsigned integers for LCG calculations to handle intermediate products safely.\n    m = np.uint64(m_val)\n    a = np.uint64(a_val)\n    c = np.uint64(c_val)\n\n    for case in test_cases:\n        N, seed, tau_mu, tau_sigma2, tau_ks = case\n        \n        # 1. Initialize N parallel LCG streams.\n        # We use Python's arbitrary-precision integers for the initial seed calculation\n        # to correctly handle the `(seed + i) % m` operation, especially for large seeds.\n        # The result is then cast to uint64 for vectorized numpy operations.\n        initial_states = np.array([(seed + i) % m_val for i in range(N)], dtype=np.uint64)\n        \n        # 2. Generate 12 uniform variates for each stream and sum them up.\n        # Vectorized operations are used for efficiency.\n        current_states = initial_states\n        sum_of_uniforms = np.zeros(N, dtype=np.float64)\n        \n        for _ in range(12):\n            # Advance the state of all N streams\n            current_states = (a * current_states + c) % m\n            # Convert integer state to uniform variate in [0, 1) and add to sum\n            sum_of_uniforms += current_states.astype(np.float64) / m_val\n            \n        # 3. Form the approximately standard normal variates Z\n        # Z_i = (sum of 12 uniforms) - 6\n        z_samples = sum_of_uniforms - 6.0\n        \n        # 4. Compute diagnostics\n        # Sample mean\n        mu_hat = np.mean(z_samples)\n        \n        # Sample variance (with denominator N, which is default for np.var)\n        sigma2_hat = np.var(z_samples)\n        \n        # Kolmogorov-Smirnov distance against the standard normal distribution N(0,1)\n        # kstest against 'norm' tests for N(0,1) by default.\n        ks_result = kstest(z_samples, 'norm')\n        ks_dist = ks_result.statistic\n        \n        # 5. Check if the results are within the given tolerances.\n        pass_mu = abs(mu_hat) = tau_mu\n        pass_sigma2 = abs(sigma2_hat - 1.0) = tau_sigma2\n        pass_ks = ks_dist = tau_ks\n        \n        passed = pass_mu and pass_sigma2 and pass_ks\n        results.append(passed)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}