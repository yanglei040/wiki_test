{
    "hands_on_practices": [
        {
            "introduction": "This first practice exercise grounds the Policy Function Iteration (PFI) algorithm in the foundational neoclassical growth model. You will compute the optimal savings policy for an agent in a deterministic world, but with a crucial real-world feature: an occasionally binding constraint on investment. By implementing PFI to solve this problem , you will gain core skills in handling constrained optimization within dynamic models and verifying your solution's accuracy using the Euler equation residuals.",
            "id": "2419733",
            "problem": "Consider the infinite-horizon, deterministic dynamic programming problem of a representative agent who chooses next-period capital to maximize lifetime utility. The agent has utility over consumption given by the Constant Relative Risk Aversion (CRRA) utility function $u(c)$, defined as $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ when $\\sigma \\neq 1$ and $u(c) = \\log(c)$ when $\\sigma = 1$. The resource constraint is $c + k' = A k^{\\alpha} + (1-\\delta) k$, where $k$ is current capital, $k'$ is next-period capital, $A$ is total factor productivity, $\\alpha$ is the capital share, and $\\delta$ is the depreciation rate. The agent discounts the future at factor $\\beta \\in (0,1)$. The next-period capital is subject to an occasionally binding inequality constraint $k' \\ge \\bar{k}$. Feasibility also requires nonnegative consumption, which implies $k' \\le A k^{\\alpha} + (1-\\delta) k$. The state space for capital is restricted to a grid $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$ with $k_1 = \\bar{k}$ and $k_N = k_{\\max}$, where the grid is evenly spaced with $N$ points.\n\nFor each parameter set below, compute the stationary optimal policy function $g: \\mathcal{K} \\to \\mathcal{K}$ that maps each grid point $k \\in \\mathcal{K}$ to an optimal choice $k' = g(k)$ on the same grid, subject to $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$, and define the following diagnostics:\n\n- Diagnostic $1$: the fraction of grid points at which the lower bound is binding, computed as the fraction of $k \\in \\mathcal{K}$ for which $g(k) \\le \\bar{k} + \\tau$, where the binding tolerance is $\\tau = 10^{-10}$.\n\n- Diagnostic $2$: the policy value at the median grid index, i.e., $g(k_m)$ where $m = \\lfloor (N-1)/2 \\rfloor + 1$ is the median index in $\\{1,2,\\dots,N\\}$.\n\n- Diagnostic $3$: the maximum absolute Euler equation residual across the grid under the computed policy, defined for each $k \\in \\mathcal{K}$ as\n$$\nr(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|,\n$$\nwhere $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$ and $u'(c)$ is the marginal utility of consumption, given by $u'(c) = c^{-\\sigma}$. The diagnostic $3$ to report is $\\max_{k \\in \\mathcal{K}} r(k)$.\n\nThe capital grid is defined as the evenly spaced set $\\mathcal{K}$ with $N$ points between $\\bar{k}$ and $k_{\\max}$ inclusive. For all computations, use the feasibility condition $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$ at each $k \\in \\mathcal{K}$, and only evaluate $u(c)$ when $c > 0$.\n\nTest Suite:\n- Test $1$: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$.\n- Test $2$: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$.\n- Test $3$: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$.\n\nYour program must compute the optimal stationary policy function $g$ on the specified grid for each test and return, for each test in order, the three diagnostics described above, as real numbers. The final output format must aggregate all tests into a single line containing a comma-separated list enclosed in square brackets, consisting of the $3$ diagnostics for Test $1$, followed by the $3$ diagnostics for Test $2$, followed by the $3$ diagnostics for Test $3$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\").",
            "solution": "The user has provided a well-defined problem from computational macroeconomics. I will first confirm its validity and then present a complete solution.\n\n### Step 1: Extract Givens\n\n- **Utility Function**: $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ for $\\sigma \\neq 1$, and $u(c) = \\log(c)$ for $\\sigma = 1$.\n- **Resource Constraint**: $c + k' = A k^{\\alpha} + (1-\\delta) k$.\n- **Parameters**: Total factor productivity $A$, capital share $\\alpha$, depreciation rate $\\delta$, CRRA coefficient $\\sigma$, and discount factor $\\beta \\in (0,1)$.\n- **State and Choice Variables**: Current capital $k$ (state), next-period capital $k'$ (choice).\n- **Constraints**: $k' \\ge \\bar{k}$ (lower bound) and $k' \\le A k^{\\alpha} + (1-\\delta) k$ (feasibility). $c>0$ is required.\n- **State Space**: A discrete grid $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$, which is an evenly spaced set of $N$ points from $k_1 = \\bar{k}$ to $k_N = k_{\\max}$.\n- **Policy Function**: A stationary mapping $g: \\mathcal{K} \\to \\mathcal{K}$ such that $k' = g(k)$.\n- **Diagnostic 1**: Fraction of grid points $k \\in \\mathcal{K}$ where $g(k) \\le \\bar{k} + \\tau$ with tolerance $\\tau = 10^{-10}$.\n- **Diagnostic 2**: Policy value $g(k_m)$ at the median grid index $m = \\lfloor (N-1)/2 \\rfloor + 1$.\n- **Diagnostic 3**: Maximum absolute Euler equation residual, $\\max_{k \\in \\mathcal{K}} r(k)$, where\n  $r(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|$,\n  with $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$ and $u'(c)$ being the marginal utility.\n- **Test Cases**:\n  - Test 1: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$\n  - Test 2: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$\n  - Test 3: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem describes the standard neoclassical growth model (also called the Ramsey-Cass-Koopmans model) in a discrete-time, deterministic setting. This is a foundational model in macroeconomics. All components—CRRA utility, Cobb-Douglas production technology, capital accumulation dynamics—are standard and rigorously established. The problem is scientifically sound.\n- **Well-Posed**: The problem is to find a stationary optimal policy function for a discounted dynamic programming problem. The state space $\\mathcal{K}$ is a compact set, and the utility function is bounded for any feasible policy. By the Contraction Mapping Theorem, a unique stationary optimal policy function exists. The problem is well-posed.\n- **Objective**: The problem is stated using precise mathematical definitions and objective language. There are no subjective or ambiguous terms.\n- **Conclusion**: The problem does not violate any of the specified invalidity criteria. It is a complete, consistent, and solvable scientific problem.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A solution will be provided.\n\n### Principle-Based Solution Design\n\nThe problem requires finding the stationary optimal policy function $g(k)$ for an infinite-horizon dynamic programming problem. The Bellman equation for this problem is:\n$$\nV(k) = \\max_{k' \\in \\mathcal{K} \\text{ and feasible}} \\left\\{ u(A k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k') \\right\\}\n$$\nwhere $V(k)$ is the value function. The optimal policy function $g(k)$ is the function that gives the $k'$ that achieves the maximum for each $k$.\n\nWe will solve for the policy function $g(k)$ using the **Policy Function Iteration (PFI)** algorithm. This iterative method finds the fixed point of the Bellman operator, which corresponds to the stationary optimal policy.\n\nThe PFI algorithm proceeds in the following steps:\n1.  **Initialization**:\n    -   Define the capital grid $\\mathcal{K}$ as an array of $N$ points from $\\bar{k}$ to $k_{\\max}$.\n    -   Start with an initial guess for the policy function, $g_0(k)$. A simple and feasible initial policy is to choose the minimum possible capital for the next period, i.e., $g_0(k) = \\bar{k}$ for all $k \\in \\mathcal{K}$.\n\n2.  **Iteration**: Repeat the following two steps until the policy function converges. Let the current iteration be $j$.\n    a. **Policy Evaluation**: Given the current policy $g_j(k)$, compute the corresponding value function $V_j(k)$. This value function is the solution to the linear functional equation:\n       $$\n       V_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k)) + \\beta V_j(g_j(k))\n       $$\n       This represents a system of $N$ linear equations for the $N$ values $\\{V_j(k_i)\\}_{i=1}^N$. Instead of solving this system directly via matrix inversion, which is computationally intensive ($O(N^3)$), we can find $V_j$ by iterating on the value function for a fixed policy:\n       $$\n       V_{j, s+1}(k) = R_j(k) + \\beta V_{j,s}(g_j(k))\n       $$\n       where $R_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k))$ is the reward under policy $g_j$. This inner loop is iterated until $V_{j,s}$ converges to $V_j$.\n\n    b. **Policy Improvement**: With the value function $V_j$ from the evaluation step, find an improved policy $g_{j+1}(k)$ by solving the maximization problem for each state $k \\in \\mathcal{K}$:\n       $$\n       g_{j+1}(k) = \\underset{k' \\in \\mathcal{K} \\text{ s.t. } \\bar{k} \\le k' < Y(k)}{\\arg\\max} \\left\\{ u(Y(k) - k') + \\beta V_j(k') \\right\\}\n       $$\n       where $Y(k) = A k^{\\alpha} + (1-\\delta)k$ is the total available resource. The search for the optimal $k'$ is performed over all grid points that satisfy the feasibility constraints. The strict inequality $k' < Y(k)$ ensures that consumption is always positive, as required for the logarithmic and CRRA utility functions.\n\n3.  **Convergence**: The algorithm terminates when the policy function no longer changes between iterations, i.e., $g_{j+1}(k) = g_j(k)$ for all $k \\in \\mathcal{K}$. The resulting policy is the stationary optimal policy, $g(k)$.\n\nOnce the optimal policy function $g(k)$ is computed, the three diagnostics are calculated as follows:\n- **Diagnostic 1**: The fraction of grid points where the lower bound on capital is binding. Since the policy choices $g(k)$ must lie on the grid $\\mathcal{K}$ and the first grid point is $\\bar{k}$, the condition $g(k) \\le \\bar{k} + \\tau$ for a small $\\tau$ is equivalent to $g(k) = \\bar{k}$. We compute the fraction of states $k \\in \\mathcal{K}$ for which $g(k) = \\bar{k}$.\n- **Diagnostic 2**: The value of the policy function at the median capital grid point, $k_m$. The index $m$ is $1$-based in the problem, so for a $0$-indexed array of size $N$, this corresponds to index $\\lfloor(N-1)/2\\rfloor$. We report $g(k_{\\lfloor(N-1)/2\\rfloor})$.\n- **Diagnostic 3**: The maximum absolute Euler equation error across the grid. For each state $k$, we compute the consumption $c(k) = Y(k) - g(k)$ and the consumption in the next period, $c(g(k)) = Y(g(k)) - g(g(k))$. Then, we calculate the residual $r(k)$ using the provided formula and find the maximum value over all $k \\in \\mathcal{K}$. This serves as a measure of the accuracy of our computed policy.\n\nThis principled approach guarantees finding the discrete optimal policy and allows for a straightforward computation of the required diagnostics.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the dynamic programming problem for all test cases\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0),\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0),\n        (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        alpha, beta, delta, A, sigma, k_bar, N, k_max = params\n        diagnostics = compute_policy_and_diagnostics(\n            alpha, beta, delta, A, sigma, k_bar, int(N), k_max\n        )\n        results.extend(diagnostics)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_policy_and_diagnostics(alpha, beta, delta, A, sigma, k_bar, N, k_max):\n    \"\"\"\n    Computes the stationary optimal policy function using Policy Function Iteration\n    and calculates the required diagnostics.\n    \"\"\"\n    # 1. Setup the economic environment\n    k_grid = np.linspace(k_bar, k_max, N)\n\n    # Define utility and marginal utility functions based on sigma\n    if sigma == 1.0:\n        u = lambda c: np.log(c)\n        u_prime = lambda c: 1.0 / c\n    else:\n        u = lambda c: (c**(1.0 - sigma)) / (1.0 - sigma)\n        u_prime = lambda c: c**(-sigma)\n\n    # 2. Policy Function Iteration (PFI)\n    policy_idx = np.zeros(N, dtype=int)  # Initial policy: g(k) = k_bar for all k\n    \n    max_pfi_iter = 200\n    pfi_tol = 1e-9\n\n    V = np.zeros(N) # Initialize value function\n\n    for pfi_iter in range(max_pfi_iter):\n        # --- Policy Evaluation ---\n        # Iteratively find the value function for the current policy\n        max_v_iter = 5000\n        v_tol = 1e-9\n        \n        k_prime_eval = k_grid[policy_idx]\n        c_eval = A * k_grid**alpha + (1.0 - delta) * k_grid - k_prime_eval\n        \n        # Consumption must be positive\n        R = np.full(N, -np.inf)\n        positive_c_mask = c_eval > 0\n        R[positive_c_mask] = u(c_eval[positive_c_mask])\n        \n        for v_iter in range(max_v_iter):\n            V_next = R + beta * V[policy_idx]\n            if np.max(np.abs(V_next - V)) < v_tol:\n                break\n            V = V_next\n        else: # This 'else' belongs to the for-loop, runs if no 'break'\n            pass # Convergence not reached, but proceed with current V\n\n        # --- Policy Improvement ---\n        new_policy_idx = np.zeros(N, dtype=int)\n        y = A * k_grid**alpha + (1.0 - delta) * k_grid\n\n        for i in range(N):\n            # Find feasible choices for k' (must allow for positive consumption)\n            # k_prime < y[i]\n            valid_k_prime_end_idx = np.searchsorted(k_grid, y[i], side='left')\n\n            if valid_k_prime_end_idx == 0:\n                # No feasible choice leads to positive consumption\n                new_policy_idx[i] = 0 # Default to k_bar\n                continue\n\n            choice_indices = np.arange(valid_k_prime_end_idx)\n            k_prime_choices = k_grid[choice_indices]\n            c_choices = y[i] - k_prime_choices\n            \n            value_choices = u(c_choices) + beta * V[choice_indices]\n            \n            best_choice_local_idx = np.argmax(value_choices)\n            new_policy_idx[i] = choice_indices[best_choice_local_idx]\n\n        # --- Convergence Check ---\n        if np.max(np.abs(new_policy_idx - policy_idx)) < 1:\n            policy_idx = new_policy_idx\n            break\n        \n        policy_idx = new_policy_idx\n\n    # 3. Calculate Diagnostics\n    # Final policy g(k) = k'\n    g_k = k_grid[policy_idx]\n    \n    # Diagnostic 1: Fraction of grid points where the lower bound is binding\n    # Since g(k) is on the grid and k_bar is the first grid point,\n    # g(k) <= k_bar + tau is equivalent to g(k) == k_bar.\n    diag1 = np.mean(policy_idx == 0)\n    \n    # Diagnostic 2: Policy value at the median grid index\n    # Problem: m = floor((N-1)/2) + 1 (1-based), Python: (N-1)//2 (0-based)\n    median_idx = (N - 1) // 2\n    diag2 = g_k[median_idx]\n    \n    # Diagnostic 3: Maximum absolute Euler equation residual\n    c = A * k_grid**alpha + (1.0 - delta) * k_grid - g_k\n    \n    # Find g(g(k))\n    g_of_g_k_policy_idx = policy_idx[policy_idx]\n    g_of_g_k = k_grid[g_of_g_k_policy_idx]\n    \n    # Consumption at the next state, c' = c(g(k))\n    c_prime = A * g_k**alpha + (1.0 - delta) * g_k - g_of_g_k\n    \n    # Ensure all consumptions are positive before taking logs or negative powers\n    if np.any(c <= 0) or np.any(c_prime <= 0):\n        # This indicates a problem, as feasible policies should yield c > 0.\n        # However, to prevent crashes, we'll signal an error with a large residual.\n        # In a well-behaved model, this branch is not taken.\n        return diag1, diag2, np.inf\n\n    u_prime_c = u_prime(c)\n    u_prime_c_prime = u_prime(c_prime)\n    \n    # Marginal return on capital (k')\n    return_on_k_prime = alpha * A * g_k**(alpha - 1.0) + (1.0 - delta)\n    \n    residuals = np.abs(u_prime_c - beta * u_prime_c_prime * return_on_k_prime)\n    diag3 = np.max(residuals)\n    \n    return diag1, diag2, diag3\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "Building on the deterministic case, this exercise introduces stochasticity, a key feature of modern macroeconomic models. Here, you will apply Policy Function Iteration to a stochastic optimal growth model where productivity follows a Markov chain . This practice will teach you how to incorporate expectations into the Bellman equation and, by computing the economy's stationary distribution, demonstrates how to use the optimal policy to analyze long-run aggregate behavior.",
            "id": "2419647",
            "problem": "You are given a stationary infinite-horizon dynamic programming problem of optimal capital accumulation with discrete state and action spaces. The state is a pair $(k,z)$ where $k$ is current capital and $z$ is current total factor productivity. The choice is next-period capital $k'$. The per-period utility is $u(c)$ with constant relative risk aversion (CRRA), and the resource constraint is $c = z k^{\\alpha} + (1-\\delta)k - k'$. The intertemporal objective is the unique fixed point of the Bellman equation\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\},\n$$\nwhere $\\mathcal{K}$ is a finite grid of capital choices and $z$ follows a finite-state time-homogeneous Markov chain with transition matrix $P$. The utility function is $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$ for $c > 0$ and $u(c) = -10^{12}$ for $c \\le 0$ (a large negative number to encode infeasibility). The expectation $\\mathbb{E}[V(k',z') \\mid z]$ is taken with respect to the next-period productivity $z'$ conditional on current $z$, using the given Markov transition matrix $P$. The action space is restricted to the grid $\\mathcal{K}$.\n\nThe state space is discrete: $z$ takes values in a finite set $\\mathcal{Z} = \\{z_1,\\dots,z_S\\}$ and $k$ takes values in the grid $\\mathcal{K} = \\{k_1,\\dots,k_N\\}$, where $k_1 = \\underline{k}$, $k_N = \\overline{k}$, and the grid is evenly spaced between these bounds. The Markov transition matrix $P$ is an $S \\times S$ row-stochastic matrix with entries $P_{ij} = \\Pr(z' = z_j \\mid z = z_i)$.\n\nFor each parameter set below, compute:\n1. The optimal stationary policy $\\pi(k,z) \\in \\mathcal{K}$ that solves the Bellman equation on the given grids, with the tie-breaking rule that if multiple $k'$ attain the maximum, select the smallest such $k'$ in $\\mathcal{K}$.\n2. The corresponding value function $V(k,z)$ associated with $\\pi$.\n3. The Markov transition matrix $Q$ on the joint state space $\\mathcal{K} \\times \\mathcal{Z}$ induced by $(\\pi,P)$, which has entries $Q\\left((k,z),(k',z')\\right) = \\mathbf{1}\\{k' = \\pi(k,z)\\} \\cdot P(z,z')$.\n4. The unique stationary distribution $\\mu$ on $\\mathcal{K} \\times \\mathcal{Z}$ satisfying $\\mu = \\mu Q$, obtained as the limit of repeated application of $Q$ to an initial uniform distribution on $\\mathcal{K} \\times \\mathcal{Z}$.\n5. The expected capital $\\mathbb{E}_{\\mu}[k]$ and expected consumption $\\mathbb{E}_{\\mu}[c]$ under the stationary distribution, where $c(k,z) = z k^{\\alpha} + (1-\\delta)k - \\pi(k,z)$.\n6. The Bellman residual at the computed solution, defined as\n$$\nR = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} P(z,z') V(k',z') \\right\\} \\right) - V(k,z) \\right|.\n$$\n\nYour program must compute the above for each parameter set in the test suite. For each parameter set, produce a list of three real numbers: $[\\mathbb{E}_{\\mu}[k], \\mathbb{E}_{\\mu}[c], R]$, with each entry rounded to $6$ decimals. The final program output must be a single line containing the concatenation of these lists for all test cases, as a single comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4,x_5,x_6,\\dots]$.\n\nUse the following test suite of parameter sets:\n- Test case $1$ (baseline):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.08$, $\\sigma = 2.0$.\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$ with transition matrix $P = \\begin{bmatrix}0.9 & 0.1 \\\\ 0.1 & 0.9\\end{bmatrix}$.\n  - Capital grid $\\mathcal{K}$: $N = 200$ evenly spaced points from $\\underline{k} = 0.001$ to $\\overline{k} = 5.0$.\n- Test case $2$ (high patience):\n  - $\\alpha = 0.35$, $\\beta = 0.995$, $\\delta = 0.08$, $\\sigma = 2.0$.\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$ with transition matrix $P = \\begin{bmatrix}0.9 & 0.1 \\\\ 0.1 & 0.9\\end{bmatrix}$.\n  - Capital grid $\\mathcal{K}$: $N = 200$ evenly spaced points from $\\underline{k} = 0.001$ to $\\overline{k} = 5.0$.\n- Test case $3$ (low volatility, different curvature and depreciation):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.10$, $\\sigma = 3.0$.\n  - $\\mathcal{Z} = \\{1.0, 1.0\\}$ with transition matrix $P = \\begin{bmatrix}0.5 & 0.5 \\\\ 0.5 & 0.5\\end{bmatrix}$.\n  - Capital grid $\\mathcal{K}$: $N = 180$ evenly spaced points from $\\underline{k} = 0.001$ to $\\overline{k} = 4.0$.\n\nAngle units are not applicable. No physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered as $[\\mathbb{E}_{\\mu}[k]^{(1)},\\mathbb{E}_{\\mu}[c]^{(1)},R^{(1)},\\mathbb{E}_{\\mu}[k]^{(2)},\\mathbb{E}_{\\mu}[c]^{(2)},R^{(2)},\\mathbb{E}_{\\mu}[k]^{(3)},\\mathbb{E}_{\\mu}[c]^{(3)},R^{(3)}]$, where the superscripts indicate the test case index. Each numeric entry must be rounded to $6$ decimals.",
            "solution": "The problem presented is a canonical stationary infinite-horizon dynamic programming problem, specifically a discretized version of the stochastic optimal growth model. The task is to find the optimal savings policy and characterize the long-run behavior of the system. The problem is well-posed; the Bellman operator is a contraction mapping under the given parameters, guaranteeing the existence and uniqueness of the value function and an associated optimal stationary policy. We will solve this problem using the Policy Function Iteration (PFI) algorithm.\n\nThe state of the system is a pair $(k,z)$, where $k \\in \\mathcal{K}$ is the capital stock and $z \\in \\mathcal{Z}$ is the productivity level. The agent chooses the next-period capital stock $k' \\in \\mathcal{K}$ to maximize the discounted sum of future utilities. The objective is to find the value function $V(k,z)$ that is the unique fixed point of the Bellman equation:\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\}\n$$\nThe utility function is of the Constant Relative Risk Aversion (CRRA) form, $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$, with a large penalty for non-positive consumption. The expectation is taken with respect to the Markov process for productivity $z$, $\\mathbb{E}[V(k',z') \\mid z_i] = \\sum_{j=1}^S P_{ij} V(k', z_j)$.\n\nThe Policy Function Iteration (PFI) algorithm is an iterative procedure that finds the optimal policy $\\pi(k,z)$ by alternating between two steps:\n\n1.  **Policy Evaluation**: For a given policy function $\\pi_j$, we compute the value $V^{\\pi_j}$ associated with following this policy indefinitely. This value function is the solution to the linear system of equations defined by the Bellman operator for a fixed policy:\n    $$\n    V^{\\pi_j}(k,z) = u\\left(z k^{\\alpha} + (1-\\delta)k - \\pi_j(k,z)\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(\\pi_j(k,z), z_{s'})\n    $$\n    This system can be solved either by matrix inversion, $(I - \\beta M_{\\pi_j})^{-1} u_{\\pi_j}$, where $M_{\\pi_j}$ is the transition matrix induced by the policy, or, more practically for large state spaces, by iterating the operator above until the value function converges. We shall employ the latter iterative method within each evaluation step.\n\n2.  **Policy Improvement**: Given the value function $V^{\\pi_j}$ from the evaluation step, we find a new, improved policy $\\pi_{j+1}$ by solving the one-step maximization problem for each state $(k,z)$:\n    $$\n    \\pi_{j+1}(k,z) = \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(k', z_{s'}) \\right\\}\n    $$\n    The specified tie-breaking rule—selecting the smallest $k'$ among optimizers—is applied.\n\nThe algorithm starts with an initial guess for the policy, $\\pi_0$, and repeats these two steps until the policy function no longer changes, i.e., $\\pi_{j+1} = \\pi_j$. At this point, the optimal policy $\\pi^*$ and its corresponding value function $V^*$ have been found.\n\nAfter obtaining the converged policy $\\pi^*$ and value function $V^*$, we compute the required statistics:\n\n-   **State Transition Matrix $Q$**: The Markov transition matrix on the joint discrete state space $\\mathcal{K} \\times \\mathcal{Z}$ is constructed. Its elements are $Q((k,z),(k',z')) = \\mathbf{1}\\{k' = \\pi^*(k,z)\\} \\cdot P(z,z')$. This is an $(NS \\times NS)$ matrix, where $N=|\\mathcal{K}|$ and $S=|\\mathcal{Z}|$.\n\n-   **Stationary Distribution $\\mu$**: The unique stationary distribution $\\mu$ is a row vector of length $NS$ satisfying the property $\\mu = \\mu Q$. It is computed using the power method: starting with an initial uniform distribution $\\mu_0$, we iterate $\\mu_{m+1} = \\mu_m Q$ until convergence.\n\n-   **Expected Values**: With the stationary distribution $\\mu$ represented as a matrix of probabilities $\\mu_{is}$ for each state $(k_i, z_s)$, we compute the expected capital and consumption:\n    $$\n    \\mathbb{E}_{\\mu}[k] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} k_i\n    $$\n    $$\n    \\mathbb{E}_{\\mu}[c] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} \\left( z_s k_i^{\\alpha} + (1-\\delta)k_i - k_{\\pi^*(i,s)} \\right)\n    $$\n    where $k_{\\pi^*(i,s)}$ is the capital level corresponding to the index returned by the policy function.\n\n-   **Bellman Residual $R$**: This quantity measures the precision of the numerical solution. It is defined as the maximum absolute difference between the computed value function $V^*$ and the result of applying the full Bellman operator one more time:\n    $$\n    R = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(c(k,z,k')\\right) + \\beta \\sum_{z'} P(z,z') V^*(k',z') \\right\\} \\right) - V^*(k,z) \\right|\n    $$\n\nThe implementation will use vectorized `NumPy` operations to ensure computational efficiency. The tie-breaking rule is naturally handled by `numpy.argmax`, which returns the index of the first maximum.",
            "answer": "```python\nimport numpy as np\n\ndef solve_model(alpha, beta, delta, sigma, z_grid, P, k_grid):\n    \"\"\"\n    Solves the optimal growth model using Policy Function Iteration.\n    \"\"\"\n    N = len(k_grid)\n    S = len(z_grid)\n    \n    # Utility function\n    # Note: The case sigma = 1 is not in test cases, so we ignore it.\n    def u(c):\n        utility = np.full(c.shape, -1e12)\n        positive_c = c > 0\n        utility[positive_c] = (c[positive_c]**(1 - sigma)) / (1 - sigma)\n        return utility\n\n    # --- Policy Function Iteration Setup ---\n    # V: value function (N, S)\n    # pi: policy function, stores index of k' (N, S)\n    V = np.zeros((N, S))\n    pi = np.zeros((N, S), dtype=int) \n\n    # PFI convergence parameters\n    max_pfi_iter = 200\n    pfi_iter = 0\n    policy_converged = False\n\n    # Policy Evaluation convergence parameters\n    max_eval_iter = 2000\n    eval_tol = 1e-9\n\n    # Pre-calculate consumption for all state-action pairs\n    # Shape: (N_k, N_z, N_k') -> (i, s, j)\n    k_matrix_i = k_grid[:, np.newaxis, np.newaxis]\n    z_matrix_s = z_grid[np.newaxis, :, np.newaxis]\n    k_matrix_j = k_grid[np.newaxis, np.newaxis, :]\n    \n    consumption = z_matrix_s * (k_matrix_i**alpha) + (1 - delta) * k_matrix_i - k_matrix_j\n    utility_all_choices = u(consumption)\n\n    # --- PFI Loop ---\n    while not policy_converged and pfi_iter < max_pfi_iter:\n        pfi_iter += 1\n        pi_old = pi.copy()\n\n        # 1. Policy Evaluation Step\n        V_eval = V.copy() # Use V from previous PFI step as starting point\n        for _ in range(max_eval_iter):\n            k_prime_indices = pi # Shape (N, S)\n            \n            # Expected value term: E[V(k', z')|z]\n            # V @ P.T gives E_z'[V(k, z')] for each k, z\n            EV = V_eval @ P.T # Shape (N, S)\n            \n            # Select the expected value for the policy-dictated k'\n            # EV[pi[i, s], s] for each (i,s)\n            expected_val_at_pi = EV[pi, np.arange(S)] # Shape (N, S)\n\n            # Consumption under current policy\n            k_prime_values = k_grid[k_prime_indices] # Shape (N, S)\n            c_at_pi = z_grid[None,:] * (k_grid[:,None]**alpha) + (1-delta)*k_grid[:,None] - k_prime_values\n            \n            # Value function update\n            V_new = u(c_at_pi) + beta * expected_val_at_pi\n            \n            if np.max(np.abs(V_new - V_eval)) < eval_tol:\n                break\n            V_eval = V_new\n        V = V_eval\n\n        # 2. Policy Improvement Step\n        EV = V @ P.T # Expected value, shape (N,S)\n        \n        # RHS of Bellman equation for all choices\n        # utility_all_choices (N, S, N)\n        # EV.T (S, N) -> broadcast to (N, S, N)\n        rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n\n        # Find new policy and new value function\n        # argmax provides the tie-breaking rule (smallest index/k')\n        pi = np.argmax(rhs_bellman, axis=2)\n        \n        # Check for convergence\n        if np.array_equal(pi, pi_old):\n            policy_converged = True\n\n    if not policy_converged:\n        print(\"Warning: Policy function iteration did not converge.\")\n\n    # --- Post-convergence calculations ---\n    # Final value function from one VFI step on optimal policy\n    EV = V @ P.T\n    rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n    V_final = np.max(rhs_bellman, axis=2)\n\n    # Bellman Residual\n    residual = np.max(np.abs(V_final - V))\n\n    # Construct the transition matrix Q on the joint state space\n    NS = N * S\n    Q = np.zeros((NS, NS))\n    for i in range(N):\n        for s in range(S):\n            old_idx = s * N + i\n            policy_k_idx = pi[i, s]\n            for t in range(S):\n                new_idx = t * N + policy_k_idx\n                Q[old_idx, new_idx] = P[s, t]\n\n    # Stationary Distribution mu\n    mu = np.ones(NS) / NS\n    max_mu_iter = 5000\n    mu_tol = 1e-10\n    for _ in range(max_mu_iter):\n        mu_new = mu @ Q\n        if np.max(np.abs(mu_new - mu)) < mu_tol:\n            mu = mu_new\n            break\n        mu = mu_new\n    \n    mu_matrix = mu.reshape((S, N)).T # Shape (N, S)\n\n    # Expected Capital\n    expected_k = np.sum(mu_matrix * k_grid[:, np.newaxis])\n\n    # Expected Consumption\n    k_prime_indices_opt = pi\n    k_prime_values_opt = k_grid[k_prime_indices_opt]\n    consumption_opt = z_grid[None,:] * (k_grid[:,None]**alpha) + \\\n                      (1 - delta) * k_grid[:,None] - k_prime_values_opt\n    expected_c = np.sum(mu_matrix * consumption_opt)\n    \n    return [round(expected_k, 6), round(expected_c, 6), round(residual, 6)]\n\ndef solve():\n    test_cases = [\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.995, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.10, \"sigma\": 3.0,\n            \"z_grid\": np.array([1.0, 1.0]),\n            \"P\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"N\": 180, \"k_min\": 0.001, \"k_max\": 4.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_grid = np.linspace(case[\"k_min\"], case[\"k_max\"], case[\"N\"])\n        \n        result_triplet = solve_model(\n            case[\"alpha\"], case[\"beta\"], case[\"delta\"], case[\"sigma\"],\n            case[\"z_grid\"], case[\"P\"], k_grid\n        )\n        results.extend(result_triplet)\n    \n    # Format the output as per problem specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice challenges you to apply the Policy Function Iteration framework to a cutting-edge scenario where the economic environment's dynamics are governed by a neural network. You will solve a deterministic growth model where the productivity transition is no longer a simple analytical function but a known, non-linear mapping . This exercise not only demonstrates the remarkable flexibility of PFI but also introduces an advanced, exact policy evaluation method based on graph theory, highlighting how computational techniques can be tailored to the problem's structure.",
            "id": "2419687",
            "problem": "Consider a deterministic infinite-horizon dynamic optimization problem solved by Policy Function Iteration (PFI) in a discrete-state, discrete-action setting. The agent chooses next-period capital from a fixed grid to maximize expected discounted utility, where the exogenous productivity state evolves according to a known Artificial Neural Network (ANN). The setting is purely mathematical and logical and can be implemented in any modern programming language.\n\nFundamental base and model definitions:\n- The agent maximizes the objective $\\sum_{t=0}^{\\infty} \\beta^{t} u(c_{t})$ with discount factor $\\beta \\in (0,1)$ and instantaneous utility $u(c) = \\log(c)$ for $c > 0$.\n- The production function is $f(k) = k^{\\alpha}$ with $\\alpha \\in (0,1)$.\n- The law of motion for capital follows the resource constraint $c + k' = z \\cdot f(k) + (1-\\delta) k$, where $\\delta \\in (0,1)$ is the depreciation rate, $k$ is current capital, $k'$ is next-period capital, and $z$ is the productivity state.\n- The state is the pair $(k, z)$, where $k$ lies on a fixed capital grid $\\mathcal{K}$ and $z$ lies on a fixed productivity grid $\\mathcal{Z}$.\n- The productivity transition is deterministic and governed by a known one-hidden-layer ANN mapping $z$ to $z'$. Let the hidden layer be $h(z) = \\tanh(W z + b)$ with $W \\in \\mathbb{R}^{2 \\times 1}$, $b \\in \\mathbb{R}^{2}$, and output $z' = 1 + c^{\\top} h(z)$ with $c \\in \\mathbb{R}^{2}$. The constants are:\n  - $W = \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix}$, $c = \\begin{bmatrix} 0.06 \\\\ 0.03 \\end{bmatrix}$.\n  - Thus, $z' = 1 + \\begin{bmatrix} 0.06 & 0.03 \\end{bmatrix} \\tanh\\!\\left( \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix} z + \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix} \\right)$, where $\\tanh(\\cdot)$ is applied elementwise.\n- The productivity grid is $\\mathcal{Z} = \\{ z_{j} \\}_{j=1}^{N_{z}}$ with $N_{z} = 21$ equally spaced points from $0.9$ to $1.1$. The capital grid is $\\mathcal{K} = \\{ k_{i} \\}_{i=1}^{N_{k}}$ with $N_{k} = 61$ equally spaced points from $0.5$ to $2.0$.\n- The deterministic transition from current $(k,z)$ to next state uses two steps:\n  1. Given $z$, compute $z'$ from the ANN and then map to the nearest grid point in $\\mathcal{Z}$.\n  2. Given the chosen $k' \\in \\mathcal{K}$, map to that grid point index directly.\n\nBellman equation and PFI framework:\n- Let $V(k,z)$ be the value function and $g(k,z) \\in \\mathcal{K}$ be a stationary policy mapping current state to next-period capital. For any fixed policy $g$, define the one-step return $r(k,z; g) = u\\big(z k^{\\alpha} + (1-\\delta)k - g(k,z)\\big)$ when the argument of $u(\\cdot)$ is strictly positive and define $r(k,z; g)$ to be a very negative number otherwise (to enforce feasibility).\n- The Bellman equation under a fixed policy is $V(k,z) = r(k,z; g) + \\beta V(k', z')$, where $k' = g(k,z)$ and $z'$ is the ANN-mapped next productivity rounded to the nearest grid point. In the discrete grid, this yields a system of linear equations in $V$.\n- Policy Function Iteration alternates between:\n  1. Policy evaluation: solve the linear system induced by the fixed policy $g$ to obtain $V$.\n  2. Policy improvement: for each state, choose $k' \\in \\mathcal{K}$ that maximizes $u\\big(z k^{\\alpha} + (1-\\delta)k - k'\\big) + \\beta V(k', z')$ with $z'$ given by the ANN and nearest-grid mapping.\n\nImplementation details and requirements:\n- Use $\\alpha = 0.33$. All numerical entities such as $0.33$ must be treated as exact numeric constants.\n- For infeasible consumption (i.e., $z k^{\\alpha} + (1-\\delta)k - k' \\le 0$), assign a large negative payoff to enforce the constraint.\n- Implement exact policy evaluation by exploiting the deterministic transition structure of the fixed policy. The directed graph induced by the policy maps each state to a unique next state. The exact value function can be computed by identifying cycles and trees that feed into those cycles and using finite geometric series to solve values on cycles, followed by backward propagation along the trees.\n- Initialize the policy by myopic maximization of the one-step return (set the continuation value to zero in the first improvement) or any other feasible policy. Iterate until the policy stabilizes (no change across all states) or a maximum number of iterations is reached.\n\nTest suite and answers to compute:\n- For each parameter case, run PFI to convergence and return the optimal next-period capital choices $g^{\\star}(k,z)$ for specific query states, reported as elements of $\\mathcal{K}$.\n- Fixed parameters across all cases: $\\alpha = 0.33$, $\\delta = 0.10$, the ANN weights $(W,b,c)$ as above, and the grids $\\mathcal{K}$ from $0.5$ to $2.0$ with $61$ points and $\\mathcal{Z}$ from $0.9$ to $1.1$ with $21$ points.\n- Cases differ by the discount factor $\\beta$:\n  - Case A (happy path): $\\beta = 0.95$. Query states: $(k,z) = (1.0, 1.0)$, $(0.5, 0.9)$, $(2.0, 1.1)$.\n  - Case B (more patient): $\\beta = 0.98$. Query states: $(k,z) = (1.0, 1.0)$, $(1.5, 1.1)$, $(0.6, 0.95)$.\n  - Case C (less patient): $\\beta = 0.90$. Query states: $(k,z) = (1.0, 1.0)$, $(1.8, 0.9)$, $(0.7, 1.05)$.\n- For each query, use the nearest grid point in $\\mathcal{K}$ and $\\mathcal{Z}$ to represent the state.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order: Case A three results, followed by Case B three results, followed by Case C three results. Each result is the optimal $k'$ selected by the converged policy for the corresponding query state, expressed as a floating-point number (no units). For example, the output format must be exactly like: \"[x1,x2,x3,x4,x5,x6,x7,x8,x9]\".",
            "solution": "The problem presented is a deterministic infinite-horizon dynamic optimization problem, a standard formulation in computational macroeconomics. The task is to solve for the optimal policy function using the Policy Function Iteration (PFI) algorithm. A critical analysis of the problem statement is the mandatory first step before any solution is attempted.\n\n### Step 1: Extract Givens\n\n- **Objective Function**: Maximize $\\sum_{t=0}^{\\infty} \\beta^{t} u(c_{t})$\n- **Discount Factor**: $\\beta \\in (0,1)$\n- **Utility Function**: $u(c) = \\log(c)$ for $c > 0$\n- **Production Function**: $f(k) = k^{\\alpha}$ with $\\alpha = 0.33$\n- **Depreciation Rate**: $\\delta = 0.10$\n- **Resource Constraint**: $c + k' = z \\cdot f(k) + (1-\\delta) k$\n- **State Space**: $(k, z)$, where $k \\in \\mathcal{K}$ and $z \\in \\mathcal{Z}$\n- **Capital Grid ($\\mathcal{K}$)**: $N_{k} = 61$ equally spaced points from $0.5$ to $2.0$\n- **Productivity Grid ($\\mathcal{Z}$)**: $N_{z} = 21$ equally spaced points from $0.9$ to $1.1$\n- **Productivity Transition**: Deterministic, via an Artificial Neural Network (ANN).\n  - Hidden layer: $h(z) = \\tanh(W z + b)$\n  - Output: $z' = 1 + c^{\\top} h(z)$\n  - ANN Parameters: $W = \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix}$, $c = \\begin{bmatrix} 0.06 \\\\ 0.03 \\end{bmatrix}$\n  - Grid Mapping: The computed $z'$ is mapped to the nearest point in $\\mathcal{Z}$.\n- **Algorithm**: Policy Function Iteration (PFI).\n  - **Policy Evaluation**: Exact solution of the linear system $V(s) = r(s, g(s)) + \\beta V(s')$ by identifying cycles and trees in the state transition graph induced by the policy $g$.\n  - **Policy Improvement**: For each state, update the policy by maximizing the right-hand side of the Bellman equation: $\\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u(z k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k', z') \\right\\}$.\n- **Constraint Handling**: A very large negative payoff is assigned for non-positive consumption levels.\n- **Initialization**: Any feasible policy, such as a myopic one, is permitted.\n- **Test Cases**:\n  - Case A: $\\beta = 0.95$. Query states $(k,z)$: $(1.0, 1.0)$, $(0.5, 0.9)$, $(2.0, 1.1)$.\n  - Case B: $\\beta = 0.98$. Query states $(k,z)$: $(1.0, 1.0)$, $(1.5, 1.1)$, $(0.6, 0.95)$.\n  - Case C: $\\beta = 0.90$. Query states $(k,z)$: $(1.0, 1.0)$, $(1.8, 0.9)$, $(0.7, 1.05)$.\n- **Query State Handling**: Query states are mapped to the nearest point on their respective grids.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientific Groundedness**: The problem is a neoclassical growth model, which is a fundamental and well-established framework in economics. The use of an ANN to model the exogenous shock process is a modern, but mathematically rigorous, extension. The problem is firmly grounded in the principles of dynamic optimization and computational mathematics. It is valid.\n- **Well-Posedness**: The problem is well-posed. The objective function is concave, the state space is compact, and the discount factor $\\beta$ is less than one. These conditions, via Blackwell's sufficiency conditions, guarantee the existence of a unique fixed point for the Bellman operator, which corresponds to the unique optimal value function and an associated stationary optimal policy. The PFI algorithm is guaranteed to converge to this solution. It is valid.\n- **Objectivity**: The problem is described using precise mathematical and algorithmic language. All parameters are specified numerically. There are no subjective or ambiguous statements. It is valid.\n- **Completeness and Consistency**: The problem provides all necessary information to be solved: model parameters, grid specifications, transition dynamics (including the ANN architecture and weights), and the algorithm to be used. There are no contradictions. It is valid.\n- **Realism and Feasibility**: The parameter values are standard in the relevant literature. The computational task, involving a state space of $61 \\times 21 = 1281$ states, is entirely feasible with modern computing resources. The requirement for an exact policy evaluation via graph traversal is specific but computationally sound for a deterministic transition function. It is valid.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. It is a well-posed, scientifically sound, and complete problem in computational economics. We may proceed to construct the solution.\n\n### Algorithmic Solution\n\nThe solution will be implemented by following the Policy Function Iteration (PFI) procedure. The state of the system is given by the pair $(k, z)$, which we will represent by integer indices $(i, j)$ corresponding to their positions in the capital grid $\\mathcal{K}$ and productivity grid $\\mathcal{Z}$, respectively. The total number of states is $N = N_k \\times N_z$.\n\n**Step 1: Initialization**\nFirst, we establish the discrete state space and the state transition mechanics.\n- The capital grid $\\mathcal{K}$ and productivity grid $\\mathcal{Z}$ are constructed as specified.\n- The productivity transition map is pre-computed. For each productivity grid point $z_j \\in \\mathcal{Z}$, we compute its successor $z'$ using the given ANN formula:\n  $$ z'_{\\text{raw}} = 1 + \\begin{bmatrix} 0.06 & 0.03 \\end{bmatrix} \\tanh\\!\\left( \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix} z_j + \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix} \\right) $$\n  We then find the index $j'$ of the grid point in $\\mathcal{Z}$ that is closest to $z'_{\\text{raw}}$. This gives us a deterministic mapping $j \\to j'$, which we store in an array `z_next_idx`.\n- The initial policy function, $g_0(k_i, z_j)$, is set to the lowest capital level, $k_0 \\in \\mathcal{K}$, for all states $(i, j)$. This constitutes a simple and feasible starting policy.\n\n**Step 2: Policy Function Iteration Loop**\nThe algorithm iterates until the policy function converges. Let $g_t$ be the policy at iteration $t$.\n\n**Step 2a: Policy Evaluation**\nFor a given policy $g_t$, we must find the corresponding value function $V_{g_t}$ (denoted as $V$ for simplicity) that solves the system of linear equations:\n$$ V(k_i, z_j) = r(k_i, z_j; g_t) + \\beta V(k_{i'}, z_{j'}) $$\nwhere $k_{i'} = g_t(k_i, z_j)$ and $z_{j'}$ is determined by the pre-computed productivity transition from $z_j$. $r(\\cdot)$ is the one-step return, $\\log(c)$.\n\nSince the transitions under a fixed policy are deterministic, each state $(i, j)$ maps to a unique successor state $(i', j')$. This structure defines a directed graph where each node has an out-degree of exactly one. Such a graph consists of several disjoint components, each being a set of trees that lead into a single cycle.\n\nThe solution for $V$ is found by the specified graph-based method:\n1.  **Graph Construction**: For each state index $s$, determine its unique successor state index $s_{\\text{next}}$ and its one-step reward $R_s = r(s, g_t(s))$.\n2.  **Cycle Detection**: Using a Depth First Search (DFS), we traverse the graph to identify all cycles.\n3.  **Cycle Value Calculation**: For each discovered cycle of length $L$ with nodes $\\{s_1, \\dots, s_L\\}$ where $s_L$ transitions to $s_1$:\n    The value of the first node in the cycle, $s_1$, is given by the sum of a discounted geometric series:\n    $$ V(s_1) = \\frac{1}{1 - \\beta^L} \\sum_{m=0}^{L-1} \\beta^m R(s_{1+m})$$\n    where the rewards $R$ correspond to the transitions within the cycle. Once $V(s_1)$ is known, the values for all other nodes in the cycle, $V(s_2), \\dots, V(s_L)$, are found by simple back-substitution using the Bellman equation, e.g., $V(s_L) = R(s_L) + \\beta V(s_1)$.\n4.  **Backward Propagation for Trees**: The values for all nodes not in cycles (i.e., nodes on trees) are computed by working backward from the nodes whose values are already known (the cycle nodes). We iterate through all states with unknown values. If a state $s$'s successor $s_{\\text{next}}$ has a known value $V(s_{\\text{next}})$, we can compute $V(s) = R(s) + \\beta V(s_{\\text{next}})$. This process is repeated until the values for all states have been computed.\n\n**Step 2b: Policy Improvement**\nWith the value function $V_{g_t}$ computed, we find a new, improved policy $g_{t+1}$ by maximizing the right-hand side of the Bellman equation for each state $(k_i, z_j)$:\n$$ g_{t+1}(k_i, z_j) = \\underset{k_m \\in \\mathcal{K}}{\\operatorname{argmax}} \\left\\{ \\log\\left(z_j k_i^{\\alpha} + (1-\\delta)k_i - k_m\\right) + \\beta V_{g_t}(k_m, z_{j'}) \\right\\} $$\nwhere $z_{j'}$ is the successor to $z_j$. The maximization is performed over all possible next-period capital choices $k_m$ from the grid $\\mathcal{K}$. If consumption $c = z_j k_i^{\\alpha} + (1-\\delta)k_i - k_m$ is not strictly positive, the utility is taken to be $-\\infty$, effectively enforcing the non-negativity constraint on consumption.\n\n**Step 3: Convergence**\nThe new policy $g_{t+1}$ is compared to the old policy $g_t$. If $g_{t+1}(s) = g_t(s)$ for all states $s$, the policy has converged, and the algorithm terminates. The resulting policy $g_{t+1}$ is the optimal stationary policy $g^*$. Otherwise, we set $g_t \\leftarrow g_{t+1}$ and return to Step 2a.\n\n**Step 4: Answer Reporting**\nOnce the optimal policy $g^*$ is obtained, we answer the queries for each test case. For each query state $(k_q, z_q)$, we first find the nearest grid points $k_i \\in \\mathcal{K}$ and $z_j \\in \\mathcal{Z}$. The reported answer is the optimal next-period capital choice $g^*(k_i, z_j)$, which is itself an element of $\\mathcal{K}$. The results are collected and formatted as specified.\n\nThis procedure provides a complete and rigorous method for solving the given problem, adhering to all specified constraints and methodological requirements.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the deterministic dynamic optimization problem using Policy Function Iteration.\n    \"\"\"\n\n    def setup_grids_and_transitions():\n        \"\"\"Initializes grids and pre-computes productivity transitions.\"\"\"\n        # Grid parameters\n        nk, k_min, k_max = 61, 0.5, 2.0\n        nz, z_min, z_max = 21, 0.9, 1.1\n\n        k_grid = np.linspace(k_min, k_max, nk)\n        z_grid = np.linspace(z_min, z_max, nz)\n\n        # ANN parameters\n        W = np.array([[1.3], [-0.9]])\n        b = np.array([[0.15], [-0.25]])\n        c = np.array([[0.06], [0.03]])\n\n        # Pre-compute productivity transitions\n        z_next_idx = np.zeros(nz, dtype=int)\n        for j, z in enumerate(z_grid):\n            h = np.tanh(W * z + b)\n            z_prime_raw = 1.0 + c.T @ h\n            z_next_idx[j] = np.argmin(np.abs(z_grid - z_prime_raw.item()))\n        \n        return k_grid, z_grid, z_next_idx\n\n    def find_nearest_idx(grid, value):\n        \"\"\"Finds the index of the nearest grid point to a given value.\"\"\"\n        return np.argmin(np.abs(grid - value))\n\n    def evaluate_policy_exact(policy, rewards, z_next_idx, beta, nk, nz):\n        \"\"\"\n        Computes the value function for a given policy using the exact graph-based method.\n        \"\"\"\n        n_states = nk * nz\n        values = np.full(n_states, np.nan)\n        next_state = np.zeros(n_states, dtype=int)\n        \n        # 1. Build graph structure (next_state map)\n        for i in range(nk):\n            for j in range(nz):\n                s_idx = i * nz + j\n                k_prime_idx = policy[i, j]\n                z_prime_idx = z_next_idx[j]\n                next_state[s_idx] = k_prime_idx * nz + z_prime_idx\n\n        # 2. Find cycles and compute their values\n        visited = np.zeros(n_states, dtype=bool)\n        for s_idx in range(n_states):\n            if not visited[s_idx]:\n                path = []\n                path_set = set()\n                curr = s_idx\n                # Trace path until a visited node or a cycle is found\n                while curr not in path_set and not visited[curr]:\n                    path.append(curr)\n                    path_set.add(curr)\n                    curr = next_state[curr]\n\n                # If a cycle is found in the current traversal\n                if curr in path_set:\n                    cycle_start_idx = path.index(curr)\n                    cycle = path[cycle_start_idx:]\n                    cycle_len = len(cycle)\n                    \n                    # Calculate value of the first node in the cycle using geometric series\n                    discounted_rewards_sum = 0.0\n                    for m in range(cycle_len):\n                        node_in_cycle = cycle[m]\n                        next_node_in_cycle = cycle[(m + 1) % cycle_len]\n                        \n                        reward_val = rewards.flat[node_in_cycle]\n                        discounted_rewards_sum += (beta**m) * reward_val\n                    \n                    v_cycle_start = discounted_rewards_sum / (1.0 - beta**cycle_len)\n                    values[cycle[0]] = v_cycle_start\n\n                    # Propagate values around the cycle\n                    for m in range(cycle_len - 1):\n                        node = cycle[m]\n                        next_node = cycle[m + 1]\n                        values[next_node] = (values[node] - rewards.flat[node]) / beta\n                \n                # Mark entire path as visited\n                for node in path:\n                    visited[node] = True\n        \n        # 3. Backward propagation for tree nodes\n        while np.any(np.isnan(values)):\n            for s_idx in range(n_states):\n                if np.isnan(values[s_idx]):\n                    s_next_idx = next_state[s_idx]\n                    if not np.isnan(values[s_next_idx]):\n                        values[s_idx] = rewards.flat[s_idx] + beta * values[s_next_idx]\n                        \n        return values.reshape(nk, nz)\n\n    def run_pfi(beta, alpha, delta, k_grid, z_grid, z_next_idx, max_iter=100):\n        \"\"\"Runs the Policy Function Iteration algorithm.\"\"\"\n        nk, nz = len(k_grid), len(z_grid)\n        \n        # Initialize policy: choose lowest capital level\n        policy = np.zeros((nk, nz), dtype=int)\n        \n        # Pre-compute cash-on-hand for all states\n        k_mesh, z_mesh = np.meshgrid(k_grid, z_grid, indexing='ij')\n        cash_on_hand = z_mesh * (k_mesh**alpha) + (1.0 - delta) * k_mesh\n\n        for iteration in range(max_iter):\n            # Policy Evaluation Step\n            # Compute one-step rewards for the current policy\n            k_prime_vals = k_grid[policy]\n            consumption = cash_on_hand - k_prime_vals\n            \n            rewards = np.full((nk, nz), -np.inf)\n            feasible = consumption > 0\n            rewards[feasible] = np.log(consumption[feasible])\n\n            # Exact policy evaluation\n            value_function = evaluate_policy_exact(policy, rewards, z_next_idx, beta, nk, nz)\n\n            # Policy Improvement Step\n            # Vectorized calculation of Bellman equation RHS for all possible choices\n            consumption_all_choices = cash_on_hand[:, :, np.newaxis] - k_grid[np.newaxis, np.newaxis, :]\n            \n            utility_all_choices = np.full((nk, nz, nk), -np.inf)\n            feasible_choices = consumption_all_choices > 0\n            utility_all_choices[feasible_choices] = np.log(consumption_all_choices[feasible_choices])\n            \n            continuation_values = beta * value_function[:, z_next_idx] # Shape (nk, nz)\n            rhs_bellman = utility_all_choices + continuation_values[np.newaxis, :, :].transpose(2,1,0).squeeze(axis=2)\n            \n            new_policy = np.argmax(rhs_bellman, axis=2)\n\n            # Check for convergence\n            if np.array_equal(policy, new_policy):\n                break\n            \n            policy = new_policy\n        \n        return policy\n\n    # Main execution\n    alpha = 0.33\n    delta = 0.10\n    k_grid, z_grid, z_next_idx = setup_grids_and_transitions()\n\n    test_cases = [\n        {'beta': 0.95, 'queries': [(1.0, 1.0), (0.5, 0.9), (2.0, 1.1)]},\n        {'beta': 0.98, 'queries': [(1.0, 1.0), (1.5, 1.1), (0.6, 0.95)]},\n        {'beta': 0.90, 'queries': [(1.0, 1.0), (1.8, 0.9), (0.7, 1.05)]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        beta = case['beta']\n        queries = case['queries']\n        \n        optimal_policy = run_pfi(beta, alpha, delta, k_grid, z_grid, z_next_idx)\n        \n        for k_q, z_q in queries:\n            k_idx = find_nearest_idx(k_grid, k_q)\n            z_idx = find_nearest_idx(z_grid, z_q)\n            \n            optimal_k_prime_idx = optimal_policy[k_idx, z_idx]\n            result = k_grid[optimal_k_prime_idx]\n            all_results.append(result)\n\n    # Format output as specified\n    formatted_results = [f\"{x:.10f}\".rstrip('0').rstrip('.') for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}