{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex coding, it's crucial to understand the computational scale of the problems we aim to solve. This exercise provides a stark illustration of the \"curse of dimensionality\" . By performing a simple calculation, you will see how the size of the discretized state space grows exponentially with the number of state variables, revealing the primary challenge that limits the direct application of grid-based methods like Value Function Iteration.",
            "id": "2439741",
            "problem": "In a computational economics application, consider solving a finite-state Markov Decision Process (MDP) for a dynamic problem by value function iteration over a discretized continuous state vector. The state vector is $d$-dimensional, and each component of the state vector is discretized into $10$ equally spaced bins. Assume a regular Cartesian grid is formed over the state space.\n\nCompute the total number of distinct grid cells for $d=2$ and for $d=10$. Provide your answer as an ordered pair corresponding to $\\left(d=2,\\ d=10\\right)$. Report exact values; no rounding is required. No units are needed.",
            "solution": "The problem statement will first be subjected to a rigorous validation process.\n\nStep 1: Extraction of Givens.\nThe explicit data provided in the problem statement are as follows:\n- The context is a finite-state Markov Decision Process (MDP) in computational economics.\n- The solution method is value function iteration over a discretized continuous state vector.\n- The state vector is $d$-dimensional.\n- Each component of the state vector is discretized into $10$ equally spaced bins.\n- The grid structure is a regular Cartesian grid over the state space.\n- The task is to compute the total number of distinct grid cells for two cases: dimension $d=2$ and dimension $d=10$.\n- The answer must be provided as an ordered pair corresponding to $(d=2, d=10)$.\n- The values must be exact.\n\nStep 2: Validation Using Extracted Givens.\nThe problem is assessed against the required criteria.\n- **Scientific Grounding**: The problem is grounded in the well-established field of dynamic programming and computational economics. The discretization of a continuous state space into a Cartesian grid is a standard technique for approximating solutions to continuous-state MDPs. The \"curse of dimensionality,\" which this problem illustrates, is a fundamental concept in this domain. Thus, the problem is scientifically sound.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary information to compute a unique, deterministic answer. The number of bins per dimension and the dimensions themselves are specified, and the structure of the grid is clearly defined as Cartesian.\n- **Objectivity**: The problem is stated in precise, technical, and objective language, free of any subjective or ambiguous terms.\n\nBased on this analysis, the problem is deemed valid and a solution can be derived.\n\nThe problem requires the calculation of the total number of states in a discretized state space. A state in this context is a single grid cell in a multi-dimensional grid.\n\nLet the dimensionality of the state vector be denoted by $d$.\nLet the number of discretization intervals, or bins, for each dimension be denoted by $N$.\nFrom the problem statement, we are given $N = 10$.\n\nThe state space is formed by a regular Cartesian grid. This means that for each of the $d$ dimensions, we select one of the $N$ available bins. A specific state is defined by a unique combination of bin selections across all dimensions.\n\nFor a general dimension $d$, the total number of distinct grid cells, which we will denote by $S_d$, is the total number of possible combinations. Since there are $N$ choices for the first dimension, $N$ choices for the second dimension, and so on, up to the $d$-th dimension, the total number of combinations is the product of the number of choices for each dimension.\n\n$$S_d = \\underbrace{N \\times N \\times \\dots \\times N}_{d \\text{ times}}$$\n\nThis can be expressed in exponential form as:\n\n$$S_d = N^{d}$$\n\nThis exponential relationship between the number of states and the dimensionality is the mathematical foundation of the \"curse of dimensionality.\" As $d$ increases, the size of the state space $S_d$ grows exponentially, rendering exhaustive computational methods like value function iteration infeasible.\n\nThe problem requires us to compute $S_d$ for two specific values of $d$: $d=2$ and $d=10$.\n\nCase 1: $d=2$\nFor a $2$-dimensional state space, we substitute $d=2$ and $N=10$ into the formula:\n$$S_2 = N^{2} = 10^{2} = 100$$\nThus, for a $2$-dimensional problem, the state space is discretized into $100$ cells.\n\nCase 2: $d=10$\nFor a $10$-dimensional state space, we substitute $d=10$ and $N=10$ into the formula:\n$$S_{10} = N^{10} = 10^{10} = 10,000,000,000$$\nFor a $10$-dimensional problem, the state space is discretized into ten billion cells. This dramatic increase starkly illustrates the computational burden imposed by higher dimensions.\n\nThe final answer should be presented as an ordered pair $(S_2, S_{10})$.\nThe ordered pair is therefore $(100, 10^{10})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n100 & 10^{10}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "This practice moves from conceptual challenges to hands-on implementation by tackling a canonical problem in modern macroeconomics. You will implement the Value Function Iteration algorithm to solve a stochastic consumption-savings model, a workhorse for studying economic dynamics . Mastering this exercise demonstrates a full grasp of the VFI workflow, from discretizing the state space and iterating on the Bellman equation to extracting the optimal policy function and verifying the solution's quality.",
            "id": "2446471",
            "problem": "Consider an infinite-horizon consumption–savings problem with a single representative agent who faces a two-dimensional state comprised of physical capital and an exogenous productivity level. Time is discrete and indexed by $t \\in \\{0,1,2,\\dots\\}$. The period utility is Constant Relative Risk Aversion (CRRA), defined for consumption $c_t$ by\n$$\nu(c_t) = \n\\begin{cases}\n\\dfrac{c_t^{1-\\sigma}}{1-\\sigma}, & \\text{if } \\sigma \\neq 1, \\\\\n\\log(c_t), & \\text{if } \\sigma = 1,\n\\end{cases}\n$$\nwhere $\\sigma > 0$ is the coefficient of relative risk aversion. The production technology is Cobb–Douglas with capital share parameter $\\alpha \\in (0,1)$ and depreciation rate $\\delta \\in (0,1)$, so that output is $y_t = z_t k_t^{\\alpha}$ and the law of motion for capital satisfies\n$$\nc_t + k_{t+1} = z_t k_t^{\\alpha} + (1 - \\delta) k_t,\n$$\nwith the nonnegativity constraint $c_t \\ge 0$ and a borrowing constraint $k_{t+1} \\ge 0$. The discount factor is $\\beta \\in (0,1)$.\n\nThe exogenous productivity state $z_t$ takes values on a finite grid $\\{z_1,\\dots,z_{N_z}\\}$ and follows a time-homogeneous Markov chain with transition matrix $P \\in \\mathbb{R}^{N_z \\times N_z}$, where $P_{ij} = \\mathbb{P}(z_{t+1}=z_j \\mid z_t=z_i)$ and each row of $P$ sums to $1$.\n\nDefine the value function $V(k,z)$ as the supremum over feasible plans of the expected discounted sum of utilities starting from state $(k,z)$. The value function satisfies the Bellman equation\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\!\\left(z k^{\\alpha} + (1-\\delta)k - k' \\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} \\mathbb{P}(z' \\mid z) V(k', z') \\right\\},\n$$\nwhere $\\mathcal{Z} = \\{z_1,\\dots,z_{N_z}\\}$ and $\\mathcal{K}$ is a discrete grid of capital choices. The feasible set is characterized by $k' \\in \\mathcal{K}$ such that $z k^{\\alpha} + (1-\\delta)k - k' \\ge 0$.\n\nYour program must:\n- Discretize the capital space on an equally spaced grid $\\mathcal{K} = \\{k_{\\min}, k_{\\min} + \\Delta k, \\dots, k_{\\max}\\}$ with $N_k$ points (including endpoints).\n- Compute the unique fixed point of the Bellman equation on the Cartesian product $\\mathcal{K} \\times \\mathcal{Z}$, and the associated policy function for next-period capital $k'(k,z)$ restricted to $\\mathcal{K}$.\n- For each test case below, report the following five quantities:\n  1. The number of iterations taken to satisfy the stopping criterion.\n  2. The value function evaluated at the median capital grid index and the median shock index, i.e., $V\\!\\left(k_{\\lfloor (N_k-1)/2 \\rfloor+1}, z_{\\lfloor (N_z-1)/2 \\rfloor+1}\\right)$.\n  3. The optimal next-period capital at the lowest capital and lowest shock, i.e., $k'(k_{\\min}, z_{\\min})$.\n  4. A boolean indicating whether, for each fixed shock $z$, the policy $k'(k,z)$ is weakly increasing in $k$ across the entire capital grid.\n  5. The maximum absolute Euler equation residual over interior states (excluding states where the policy at $(k,z)$ selects the lowest or highest capital grid point, or where any implied consumption is nonpositive), where the residual at a state $(k,z)$ is defined as\n     $$\n     \\left| 1 - \\dfrac{\\beta \\, \\mathbb{E}\\!\\left[ u'(c') \\left( \\alpha z' k'^{\\alpha - 1} + 1 - \\delta \\right) \\,\\big|\\, z \\right]}{u'(c)} \\right|,\n     $$\n     with $c = z k^{\\alpha} + (1-\\delta)k - k'$, $k' = k'(k,z)$, $c' = z' k'^{\\alpha} + (1-\\delta)k' - k''$, and $k'' = k'(k', z')$. Here $u'(c)$ denotes the marginal utility of consumption.\n\nUse the following test suite. For each test case, use the specified parameters, shock grid, transition matrix, and capital grid:\n\n- Test Case A (baseline):\n  - $\\beta = 0.96$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.08$.\n  - Shock grid: $N_z = 3$ with $z = [0.9, 1.0, 1.1]$.\n  - Transition matrix\n    $$\n    P = \\begin{bmatrix}\n    0.90 & 0.09 & 0.01 \\\\\n    0.09 & 0.82 & 0.09 \\\\\n    0.01 & 0.09 & 0.90\n    \\end{bmatrix}.\n    $$\n  - Capital grid: $N_k = 80$, $k_{\\min} = 0.01$, $k_{\\max} = 3.0$.\n\n- Test Case B (lower patience and more volatile shocks):\n  - $\\beta = 0.90$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.08$.\n  - Shock grid: $N_z = 3$ with $z = [0.8, 1.0, 1.2]$.\n  - Transition matrix\n    $$\n    P = \\begin{bmatrix}\n    0.85 & 0.10 & 0.05 \\\\\n    0.10 & 0.80 & 0.10 \\\\\n    0.05 & 0.10 & 0.85\n    \\end{bmatrix}.\n    $$\n  - Capital grid: $N_k = 60$, $k_{\\min} = 0.01$, $k_{\\max} = 3.0$.\n\n- Test Case C (higher patience and lower depreciation):\n  - $\\beta = 0.985$, $\\sigma = 2.0$, $\\alpha = 0.36$, $\\delta = 0.02$.\n  - Shock grid: $N_z = 3$ with $z = [0.95, 1.0, 1.05]$.\n  - Transition matrix\n    $$\n    P = \\begin{bmatrix}\n    0.92 & 0.07 & 0.01 \\\\\n    0.07 & 0.86 & 0.07 \\\\\n    0.01 & 0.07 & 0.92\n    \\end{bmatrix}.\n    $$\n  - Capital grid: $N_k = 100$, $k_{\\min} = 0.01$, $k_{\\max} = 4.0$.\n\nStopping criterion and numerical conventions:\n- Initialize the value function arbitrarily on $\\mathcal{K} \\times \\mathcal{Z}$.\n- Iterate on the Bellman equation until the sup-norm distance between successive value function iterates is less than $\\varepsilon = 10^{-6}$, or until a maximum of $N_{\\text{iter},\\max} = 1000$ iterations is reached.\n- When computing utilities, treat any consumption $c \\le 0$ as infeasible. You must enforce $c \\ge 0$; any infeasible choice must not be selected in the maximization.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with three sublists, one per test case in the order A, B, C. Each sublist must contain the five quantities in the order specified above: $[\\text{iterations}, V_{\\text{mid}}, k'_{\\text{min,low}~z}, \\text{is\\_monotone}, \\text{max\\_Euler\\_residual}]$.\n- All floating-point numbers must be rounded to six decimal places; the boolean must be either True or False.\n- Concretely, the output must look like\n  $$\n  \\big[ [n_A, v_A, k_A, b_A, e_A], [n_B, v_B, k_B, b_B, e_B], [n_C, v_C, k_C, b_C, e_C] \\big],\n  $$\n  printed as a single line with brackets and commas exactly as shown, where $n_\\cdot$ are integers, $v_\\cdot, k_\\cdot, e_\\cdot$ are floats rounded to six decimals, and $b_\\cdot$ are booleans.",
            "solution": "The problem is a stationary, infinite-horizon dynamic program with a compact, discretized state space and a bounded reward function on the feasible set. The Bellman operator $T$ mapping bounded functions into bounded functions is defined pointwise for $(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}$ by\n$$\n(TV)(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\!\\left(z k^{\\alpha} + (1 - \\delta)k - k' \\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} \\mathbb{P}(z' \\mid z) V(k', z') \\right\\}.\n$$\nBy Blackwell's sufficient conditions, $T$ is a contraction mapping on the space of bounded functions under the sup norm: it is monotone because if $V \\le W$ then $TV \\le TW$, and it satisfies discounting since $(TV + a)(k,z) \\le (TV)(k,z) + \\beta a$ for any constant $a$, with modulus $\\beta \\in (0,1)$. Therefore, $T$ has a unique fixed point $V^\\star$, and for any initial $V_0$, the sequence $V_{n+1} = TV_n$ converges to $V^\\star$ in the sup norm.\n\nTo compute the fixed point numerically, we discretize both the capital state and the decision $k'$ on the same finite grid $\\mathcal{K} = \\{k_1,\\dots,k_{N_k}\\}$, and the exogenous shock space on $\\mathcal{Z} = \\{z_1,\\dots,z_{N_z}\\}$. For each shock $z_j$ and each current capital $k_i$, we enumerate all candidate next-period capitals $k' \\in \\mathcal{K}$, compute the implied consumption $c = z_j k_i^{\\alpha} + (1 - \\delta) k_i - k'$, discard infeasible candidates with $c \\le 0$, evaluate period utility $u(c)$, and add the discounted expected continuation value $\\beta \\sum_{m=1}^{N_z} P_{j m} V(k', z_m)$. The maximizer over $k'$ yields the updated value and the corresponding policy index. This construction delivers the Bellman operator applied to a value function iterate in a vectorized manner. Repeating this update until the sup-norm distance between successive iterates falls below $\\varepsilon = 10^{-6}$ (or until the iteration cap is reached) yields an $\\varepsilon$-accurate approximation to $V^\\star$ and its associated policy function $k'(k,z)$.\n\nOnce the policy is obtained, we evaluate additional diagnostics:\n\n- The value at the median indices: Let $i^\\star = \\left\\lfloor \\dfrac{N_k - 1}{2} \\right\\rfloor + 1$ and $j^\\star = \\left\\lfloor \\dfrac{N_z - 1}{2} \\right\\rfloor + 1$. Report $V(k_{i^\\star}, z_{j^\\star})$.\n\n- The optimal next-period capital at the boundary state $(k_{\\min}, z_{\\min})$: Report $k'(k_1, z_1)$.\n\n- Monotonicity of the policy in capital: For each fixed shock $z_j$, verify whether the mapping $i \\mapsto k'(k_i, z_j)$ is weakly increasing for $i = 1,\\dots,N_k$. This property is theoretically supported by the monotonicity and concavity of the Bellman operator in standard concave growth models, although numerical discretization can introduce minor violations. We compute the boolean by checking all adjacent pairs.\n\n- The Euler equation residual: The first-order condition for interior solutions asserts\n$$\nu'(c_t) = \\beta \\, \\mathbb{E} \\left[ u'(c_{t+1}) \\left( \\alpha z_{t+1} k_{t+1}^{\\alpha - 1} + 1 - \\delta \\right) \\bigg| z_t \\right].\n$$\nUsing the discrete policy $k' = k'(k,z)$ and the Markov chain over $z$, we approximate the residual at each interior state $(k_i, z_j)$ as\n$$\nR(k_i,z_j) = \\left| 1 - \\dfrac{\\beta \\sum_{m=1}^{N_z} P_{j m} \\, u'(c'(k_i,z_j; z_m)) \\, \\left( \\alpha z_m \\, k'(k_i,z_j)^{\\alpha - 1} + 1 - \\delta \\right)}{u'(c(k_i,z_j))} \\right|,\n$$\nwhere $c(k_i,z_j) = z_j k_i^{\\alpha} + (1 - \\delta) k_i - k'(k_i,z_j)$ and $c'(k_i,z_j; z_m) = z_m k'(k_i,z_j)^{\\alpha} + (1 - \\delta) k'(k_i,z_j) - k''$, with $k'' = k'(k'(k_i,z_j), z_m)$. We exclude states where the policy chooses the lowest or highest $k'$ (to avoid corner solutions) and any states where $c$ or $c'$ is nonpositive. The maximum residual across the remaining states measures the global quality of the discrete policy relative to the Euler condition.\n\nNumerical details:\n- For CRRA with $\\sigma \\neq 1$, $u'(c) = c^{-\\sigma}$. For $\\sigma = 1$, one can use $u(c) = \\log(c)$ and $u'(c) = 1/c$; the implementation accommodates both cases.\n- Feasibility $c \\ge 0$ is enforced by assigning a value of negative infinity to infeasible choices in the maximization step, ensuring they are never selected.\n- The expected continuation value for a fixed current shock $z_j$ and candidate $k'$ is computed as a row-wise product of $P$ with the stacked value function over $z'$ evaluated at $k'$.\n\nApplying this procedure to each of the three specified test cases with their given parameters, shock grids, transition matrices, and capital grids yields, for each case, the five requested outputs: the iteration count, the value at median indices, the policy at $(k_{\\min}, z_{\\min})$, the monotonicity boolean, and the maximum Euler residual. All floating-point outputs are rounded to six decimal places, and the final print is a single-line list structured as requested.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef crra_utility(c, sigma):\n    \"\"\"CRRA utility u(c); returns -inf for nonpositive c to enforce feasibility.\"\"\"\n    c = np.asarray(c)\n    u = np.full_like(c, -np.inf, dtype=np.float64)\n    positive = c > 0\n    if sigma == 1.0:\n        u[positive] = np.log(c[positive])\n    else:\n        u[positive] = (np.power(c[positive], 1.0 - sigma)) / (1.0 - sigma)\n    return u\n\ndef crra_marginal_utility(c, sigma):\n    \"\"\"Marginal utility u'(c) for CRRA; returns nan for nonpositive c.\"\"\"\n    c = np.asarray(c)\n    mu = np.full_like(c, np.nan, dtype=np.float64)\n    positive = c > 0\n    if sigma == 1.0:\n        mu[positive] = 1.0 / c[positive]\n    else:\n        mu[positive] = np.power(c[positive], -sigma)\n    return mu\n\ndef vfi(k_grid, z_grid, P, beta, sigma, alpha, delta, tol=1e-6, max_iter=1000):\n    \"\"\"\n    Perform value function iteration on discretized grids.\n    Returns:\n        V: value function array shape (Nz, Nk)\n        policy_idx: optimal k' indices shape (Nz, Nk)\n        iters: iterations performed\n    \"\"\"\n    Nz = len(z_grid)\n    Nk = len(k_grid)\n    V = np.zeros((Nz, Nk), dtype=np.float64)\n    V_new = np.empty_like(V)\n    policy_idx = np.zeros((Nz, Nk), dtype=np.int64)\n\n    # Precompute resources y[j, i] = z_j * k_i^alpha + (1 - delta) * k_i\n    k_alpha = np.power(k_grid, alpha)\n    y = (z_grid[:, None] * k_alpha[None, :]) + (1.0 - delta) * k_grid[None, :]\n\n    # For each shock z_j, continuation value over k' is beta * P[j] @ V_old\n    iters = 0\n    diff = np.inf\n    neg_inf = -1.0e12  # numerical -inf surrogate for infeasible choices\n    while iters < max_iter and diff > tol:\n        # Expected continuation values for choosing each k' at current shock j\n        # EV has shape (Nz, Nk): for each j, EV[j, k_idx] = beta * sum_m P[j, m] * V[m, k_idx]\n        EV = beta * (P @ V)\n        # Loop over shocks to form return matrices and maximize w.r.t k'\n        for j in range(Nz):\n            # consumption matrix for current shock j: C[i, kprime] = y[j,i] - k_grid[kprime]\n            C = y[j, :, None] - k_grid[None, :]\n            U = crra_utility(C, sigma)\n            # mask infeasible as neg_inf to avoid NaNs in max\n            U[~np.isfinite(U)] = neg_inf\n            # add continuation value for each k' (broadcast across current k_i)\n            # total return W[i, kprime]\n            W = U + EV[j][None, :]\n            # maximize over k' axis=1\n            pol_idx = np.argmax(W, axis=1)\n            V_new[j, :] = W[np.arange(Nk), pol_idx]\n            policy_idx[j, :] = pol_idx\n\n        diff = np.max(np.abs(V_new - V))\n        V[:, :] = V_new\n        iters += 1\n\n    return V, policy_idx, iters\n\ndef policy_monotone(policy_idx):\n    \"\"\"\n    Check monotonicity of policy in k for each z: nondecreasing indices across k.\n    Returns True if monotone for all z, else False.\n    \"\"\"\n    Nz, Nk = policy_idx.shape\n    for j in range(Nz):\n        if np.any(np.diff(policy_idx[j, :]) < 0):\n            return False\n    return True\n\ndef euler_residual_max(k_grid, z_grid, P, beta, sigma, alpha, delta, policy_idx):\n    \"\"\"\n    Compute the maximum absolute Euler residual over interior states.\n    Excludes states where policy picks boundary k' or any implied c or c' is nonpositive.\n    Residual at (k_i, z_j): |1 - RHS / LHS| with LHS = u'(c), RHS = beta * E[u'(c') * (alpha z' k'^{alpha-1} + 1 - delta)].\n    \"\"\"\n    Nz = len(z_grid)\n    Nk = len(k_grid)\n    # Precompute\n    k_alpha = np.power(k_grid, alpha)\n    y = (z_grid[:, None] * k_alpha[None, :]) + (1.0 - delta) * k_grid[None, :]\n\n    max_resid = 0.0\n    # Iterate over states\n    for j in range(Nz):\n        for i in range(Nk):\n            kp_idx = policy_idx[j, i]\n            # exclude boundary policies\n            if kp_idx == 0 or kp_idx == Nk - 1:\n                continue\n            k = k_grid[i]\n            z = z_grid[j]\n            kp = k_grid[kp_idx]\n            # Current consumption\n            c = z * (k ** alpha) + (1.0 - delta) * k - kp\n            if c <= 0:\n                continue\n            mu_c = (1.0 / c) if sigma == 1.0 else c ** (-sigma)\n            # Next period marginal utility and returns term\n            # For each z' compute c' and k'' = policy(k', z')\n            kp_alpha = kp ** alpha\n            # Precompute gross return at next period given z'\n            R_terms = alpha * z_grid * (kp ** (alpha - 1.0)) + (1.0 - delta)\n            # Compute c' for each z'\n            y_next = z_grid * kp_alpha + (1.0 - delta) * kp  # shape (Nz,)\n            # For each z' state m, need k'' index policy_idx[m, kp_idx]\n            kpp_idx_vec = policy_idx[:, kp_idx]  # shape (Nz,)\n            kpp_vec = k_grid[kpp_idx_vec]        # shape (Nz,)\n            c_next = y_next - kpp_vec            # shape (Nz,)\n            # Exclude if any c' is nonpositive by setting their marginal utility to nan and ignoring via weights\n            mu_c_next = crra_marginal_utility(c_next, sigma)  # nan where nonpositive\n            # Expected RHS conditional on current z_j using P[j, :]\n            valid = np.isfinite(mu_c_next)\n            if not np.any(valid):\n                continue\n            RHS = beta * np.sum(P[j, valid] * mu_c_next[valid] * R_terms[valid])\n            resid = abs(1.0 - RHS / mu_c)\n            if np.isfinite(resid):\n                if resid > max_resid:\n                    max_resid = resid\n    return float(max_resid)\n\ndef run_test_case(params):\n    beta = params[\"beta\"]\n    sigma = params[\"sigma\"]\n    alpha = params[\"alpha\"]\n    delta = params[\"delta\"]\n    z = np.array(params[\"z\"], dtype=np.float64)\n    P = np.array(params[\"P\"], dtype=np.float64)\n    Nk = params[\"Nk\"]\n    k_min = params[\"k_min\"]\n    k_max = params[\"k_max\"]\n    k_grid = np.linspace(k_min, k_max, Nk, dtype=np.float64)\n\n    V, pol_idx, iters = vfi(k_grid, z, P, beta, sigma, alpha, delta, tol=1e-6, max_iter=1000)\n\n    # Median indices\n    mid_k_idx = (Nk - 1) // 2\n    mid_z_idx = (len(z) - 1) // 2\n    V_mid = V[mid_z_idx, mid_k_idx]\n    # Policy at (k_min, z_min)\n    kp_min_lowz = k_grid[pol_idx[0, 0]]\n    # Monotonicity\n    is_mono = policy_monotone(pol_idx)\n    # Euler residual\n    max_euler_resid = euler_residual_max(k_grid, z, P, beta, sigma, alpha, delta, pol_idx)\n\n    # Round floats to six decimals\n    result = [\n        int(iters),\n        round(float(V_mid), 6),\n        round(float(kp_min_lowz), 6),\n        bool(is_mono),\n        round(float(max_euler_resid), 6),\n    ]\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"beta\": 0.96,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.08,\n            \"z\": [0.9, 1.0, 1.1],\n            \"P\": [\n                [0.90, 0.09, 0.01],\n                [0.09, 0.82, 0.09],\n                [0.01, 0.09, 0.90],\n            ],\n            \"Nk\": 80,\n            \"k_min\": 0.01,\n            \"k_max\": 3.0,\n        },\n        {\n            \"name\": \"B\",\n            \"beta\": 0.90,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.08,\n            \"z\": [0.8, 1.0, 1.2],\n            \"P\": [\n                [0.85, 0.10, 0.05],\n                [0.10, 0.80, 0.10],\n                [0.05, 0.10, 0.85],\n            ],\n            \"Nk\": 60,\n            \"k_min\": 0.01,\n            \"k_max\": 3.0,\n        },\n        {\n            \"name\": \"C\",\n            \"beta\": 0.985,\n            \"sigma\": 2.0,\n            \"alpha\": 0.36,\n            \"delta\": 0.02,\n            \"z\": [0.95, 1.0, 1.05],\n            \"P\": [\n                [0.92, 0.07, 0.01],\n                [0.07, 0.86, 0.07],\n                [0.01, 0.07, 0.92],\n            ],\n            \"Nk\": 100,\n            \"k_min\": 0.01,\n            \"k_max\": 4.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure booleans and numbers are printed without extra spaces.\n    def format_item(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, (int, np.integer)):\n            return str(int(x))\n        elif isinstance(x, float) or isinstance(x, np.floating):\n            # Ensure fixed rounding to 6 decimals; preserve trailing zeros\n            return f\"{x:.6f}\"\n        else:\n            return str(x)\n\n    formatted = []\n    for res in results:\n        formatted.append(\"[\" + \",\".join(format_item(x) for x in res) + \"]\")\n    print(\"[\" + \",\".join(formatted) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While robust, standard Value Function Iteration can be computationally slow. This advanced exercise introduces a powerful and often more efficient alternative: Howard's Policy Improvement Algorithm . By separating the solution process into distinct 'improvement' and 'evaluation' steps, this method can significantly reduce the number of costly maximization operations. Implementing and comparing both algorithms will provide deep insight into the computational trade-offs involved in solving dynamic programming problems.",
            "id": "2446390",
            "problem": "Consider the infinite-horizon, deterministic, discrete-state, discrete-choice capital accumulation problem defined as follows. The state is capital $k \\in \\mathcal{K}$, where $\\mathcal{K}$ is a finite grid on an interval $[k_{\\min}, k_{\\max}]$. The per-period utility is $u(c) = \\log(c)$ for $c > 0$ and $u(c) = -\\infty$ otherwise. Production is $f(k) = z k^{\\alpha}$ with $z = 1$, capital depreciates at rate $\\delta \\in (0,1)$, and the discount factor is $\\beta \\in (0,1)$. The law of motion is\n$$\nk' \\in \\mathcal{K}, \\quad c = f(k) + (1-\\delta)k - k'.\n$$\nLet the Bellman equation be\n$$\nV(k) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V(k') \\right\\}.\n$$\nUse the following discrete approximation protocol:\n- The grid $\\mathcal{K}$ is a set of $N_k$ evenly spaced points on $[k_{\\min}, k_{\\max}]$.\n- Define the deterministic steady state $k^{\\star}$ by the first-order condition $f'(k^{\\star}) = \\frac{1}{\\beta} - 1 + \\delta$, i.e.,\n$$\n\\alpha (k^{\\star})^{\\alpha-1} = \\frac{1}{\\beta} - 1 + \\delta, \\quad \\text{so} \\quad k^{\\star} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}.\n$$\n- For each test case, the interval endpoints are $k_{\\min} = a \\cdot k^{\\star}$ and $k_{\\max} = b \\cdot k^{\\star}$ with given $a$ and $b$, and $\\mathcal{K}$ is the uniform grid on this interval with $N_k$ points.\n- The initial value function is $V_0(k) = 0$ for all $k \\in \\mathcal{K}$.\n- Infeasible choices with $c \\le 0$ are evaluated as $u(c) = -\\infty$ and are therefore never optimal.\n\nDefine two iteration methods to compute an approximate fixed point of the Bellman equation, both using the same tolerance $\\varepsilon$ and the same uniform grid:\n- Method A (full maximization at every iteration): Construct the sequence $\\{V_n\\}_{n \\ge 0}$ by $V_{n+1}(k) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V_n(k') \\right\\}$ for all $k \\in \\mathcal{K}$. Stop at the smallest integer $n \\ge 0$ such that $\\|V_{n+1} - V_n\\|_{\\infty} < \\varepsilon$. Let $M_A$ be the number of iterations performed, which equals the number of full state-wise maximization sweeps applied.\n- Method B (alternating improvement and evaluation): Starting from $V_0$, repeat the following until convergence. First, for each $k \\in \\mathcal{K}$, compute a decision rule $\\pi(k) \\in \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\big(f(k) + (1-\\delta)k - k'\\big) + \\beta V(k') \\right\\}$. Then, holding $\\pi$ fixed, perform exactly $m$ policy evaluation sweeps, each sweep updating $V$ synchronously by $V_{\\text{new}}(k) = u\\big(f(k) + (1-\\delta)k - \\pi(k)\\big) + \\beta V\\big(\\pi(k)\\big)$ for all $k \\in \\mathcal{K}$. After every block of $m$ evaluation sweeps, test convergence by comparing the current value function to the one at the start of the block, and stop when the $\\ell_{\\infty}$ distance is less than $\\varepsilon$. Let $M_B$ be the total number of improvement steps performed, which equals the number of times the decision rule $\\pi$ is recomputed (and therefore the number of full state-wise maximization sweeps).\n\nFor each test case below, compute and report the tuple containing:\n- $M_A$,\n- $M_B$,\n- a boolean indicating whether $M_B < M_A$,\n- the $\\ell_{\\infty}$ distance between the final value functions obtained by Method A and Method B.\n\nAll logarithms are natural. The convergence tolerance is $\\varepsilon = 10^{-5}$. If an algorithm does not converge within $I_{\\max}$ outer iterations, terminate it and return the number of iterations performed up to termination. Use $I_{\\max} = 10{,}000$ for Method A, and $I_{\\max} = 10{,}000$ improvement steps for Method B.\n\nTest Suite:\n- Case $1$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.33, 0.96, 0.08, 200, 0.5, 1.5, 20)$.\n- Case $2$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.36, 0.99, 0.025, 300, 0.25, 1.75, 25)$.\n- Case $3$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.30, 0.95, 0.10, 80, 0.30, 1.70, 1)$.\n- Case $4$: $(\\alpha, \\beta, \\delta, N_k, a, b, m) = (0.40, 0.97, 0.05, 250, 0.40, 1.60, 50)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list in the order $[M_A, M_B, \\text{boolean}, \\text{distance}]$. For example, a syntactically valid output looks like\n\"[ [12,3,True,0.00001], [ ... ], [ ... ], [ ... ] ]\" (the specific numbers must be those computed by your program).",
            "solution": "The problem is a deterministic, infinite-horizon dynamic programming model typically referred to as a discrete-state, discrete-choice version of the neoclassical growth model. The objective is to compute an approximate fixed point of the Bellman equation on a finite grid using two distinct iteration schemes and to compare their performance using a well-defined metric.\n\nFundamental setup:\n- The state space is a finite grid $\\mathcal{K} = \\{k_1, k_2, \\ldots, k_{N_k}\\}$ over $[k_{\\min}, k_{\\max}]$, built from $k_{\\min} = a \\cdot k^{\\star}$ and $k_{\\max} = b \\cdot k^{\\star}$. The steady state $k^{\\star}$ is defined by the stationarity requirement implied by the Euler equation in this deterministic setting: $f'(k^{\\star}) = \\frac{1}{\\beta} - 1 + \\delta$, leading to \n$$\nk^{\\star} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}.\n$$\n- The Bellman operator $\\mathcal{T}$ on the space of bounded functions $V:\\mathcal{K} \\to \\mathbb{R}$ is\n$$\n(\\mathcal{T}V)(k) = \\max_{k' \\in \\mathcal{K}} \\Big\\{ u\\big( f(k) + (1-\\delta)k - k' \\big) + \\beta V(k') \\Big\\}.\n$$\n- With $u(c) = \\log(c)$ for $c>0$ and $u(c) = -\\infty$ for $c \\le 0$, infeasible choices are automatically eliminated by the maximization. On a finite grid, this operator is a contraction mapping under the sup norm with modulus $\\beta \\in (0,1)$ when rewards are bounded above; thus, iterating the operator from any bounded initial guess converges to the unique fixed point.\n\nDiscrete approximation and precomputation:\n- Since $\\mathcal{K}$ is finite and fixed, one can precompute the matrix of instantaneous utilities $U \\in \\mathbb{R}^{N_k \\times N_k}$ with entries\n$$\nU_{ij} = \\begin{cases}\n\\log\\!\\Big( f(k_i) + (1-\\delta)k_i - k_j \\Big), & \\text{if } f(k_i) + (1-\\delta)k_i - k_j > 0,\\\\\n-\\infty, & \\text{otherwise.}\n\\end{cases}\n$$\n- The Bellman update can then be written in vectorized form as \n$$\nV_{\\text{new}} = \\max_{j \\in \\{1,\\ldots,N_k\\}} \\left\\{ U_{\\cdot j} + \\beta V_j \\right\\},\n$$\nwhere $U_{\\cdot j}$ is the $j$th column of $U$ and $V_j$ is the scalar $V(k_j)$ broadcast across states.\n\nMethod A (full maximization at every iteration):\n- Initialize $V_0(k) = 0$ for all $k \\in \\mathcal{K}$. For $n = 0,1,2,\\ldots$, compute $V_{n+1} = \\mathcal{T} V_n$ using the discrete maximization over all $k' \\in \\mathcal{K}$ at each state. Stop at the smallest $n$ for which $\\|V_{n+1} - V_n\\|_{\\infty} < \\varepsilon$, and set $M_A = n+1$, the count of full maximization sweeps needed.\n\nMethod B (alternating improvement and evaluation):\n- Given a current value $V$, compute an improved decision rule $\\pi:\\mathcal{K}\\to\\mathcal{K}$ by\n$$\n\\pi(k_i) \\in \\arg\\max_{k_j \\in \\mathcal{K}} \\left\\{ U_{ij} + \\beta V(k_j) \\right\\}.\n$$\n- Define the policy evaluation operator associated with $\\pi$ as\n$$\n(\\mathcal{T}_{\\pi}V)(k_i) = U_{i,\\pi(i)} + \\beta V\\big(\\pi(k_i)\\big),\n$$\nwhere $U_{i,\\pi(i)} = u\\big(f(k_i) + (1-\\delta)k_i - \\pi(k_i)\\big)$. Holding $\\pi$ fixed, apply $\\mathcal{T}_{\\pi}$ exactly $m$ times, synchronously updating all states each sweep. This is a contraction with modulus $\\beta$ since it is affine in $V$ with slope $\\beta$ along the mapping induced by $\\pi$.\n- After $m$ sweeps, compare the new value function to the value at the start of this block using the sup norm. If the distance is below $\\varepsilon$, stop. Otherwise, recompute $\\pi$ using the current value, increment the improvement counter, and repeat. The number $M_B$ is the total number of improvement steps, equal to the total number of full maximization sweeps. Since each improvement requires only one maximization across choices, and policy evaluation contains no maximization, this count isolates precisely the expensive maximization operations.\n\nPerformance metric and comparison:\n- The metric $M_A$ reflects the number of full maximization sweeps required by the full maximization iteration to reach the tolerance. The metric $M_B$ reflects the number of full maximization sweeps when interleaving policy evaluation sweeps between improvements. Theory and practice indicate that holding a policy fixed for $m \\ge 1$ sweeps accelerates convergence of the value function between improvements, often reducing the count $M_B$ relative to $M_A$ because improvements are needed less frequently.\n- To verify that both methods are computing the same fixed point, compute the $\\ell_{\\infty}$ distance between their final value functions. With identical discretizations, discount factor $\\beta \\in (0,1)$, and a bounded reward structure on a finite grid, both methods converge to the same fixed point within tolerance, so the distance should be small, typically on the order of $\\varepsilon$.\n\nImplementation details consistent with the statement:\n- Construct $\\mathcal{K}$ using the provided $(\\alpha,\\beta,\\delta,N_k,a,b)$ via $k^{\\star}$ and uniform spacing on $[a k^{\\star}, b k^{\\star}]$.\n- Precompute $U$ safely, assigning a very large negative value in place of $-\\infty$ for numerical maximization while guaranteeing such entries are never selected when feasible entries exist.\n- Use tolerance $\\varepsilon = 10^{-5}$ and iteration caps as specified. For Method B, test convergence after each block of $m$ policy evaluation sweeps.\n- For each test case, record $M_A$, $M_B$, the boolean $(M_B < M_A)$, and the sup norm distance between the final value functions from the two methods, and format the results as a single-line list of lists.\n\nThe expected qualitative outcome across the provided test cases is:\n- For $m > 1$, $M_B$ is typically strictly less than $M_A$, especially when $\\beta$ is close to $1$ and $N_k$ is larger, due to stronger benefits from policy evaluation sweeps.\n- When $m = 1$, the methods become closely related, and $M_B$ is expected to be similar to $M_A$.\n- The sup norm distance between the final value functions is expected to be very small, typically on the order of $\\varepsilon$, confirming that both methods compute the same fixed point within the prescribed tolerance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef steady_state_k(alpha, beta, delta):\n    # k* = (alpha / (1/beta - 1 + delta))^(1/(1-alpha))\n    num = alpha\n    den = (1.0 / beta) - 1.0 + delta\n    kstar = (num / den) ** (1.0 / (1.0 - alpha))\n    return kstar\n\ndef build_grid(kstar, a, b, Nk):\n    kmin = a * kstar\n    kmax = b * kstar\n    return np.linspace(kmin, kmax, Nk)\n\ndef precompute_utility_matrix(kgrid, alpha, delta):\n    # U[i, j] = log(c) if c > 0 else a large negative sentinel\n    # c = k_i^alpha + (1 - delta) * k_i - k'_j\n    k = kgrid\n    kp = kgrid\n    K = k[:, None]            # shape (N, 1)\n    KP = kp[None, :]          # shape (1, N)\n    output = (K ** alpha)\n    C = output + (1.0 - delta) * K - KP\n    # Use a large negative number as -inf sentinel\n    neg_inf = -1.0e12\n    with np.errstate(divide='ignore', invalid='ignore'):\n        U = np.where(C > 0.0, np.log(C), neg_inf)\n    return U  # shape (N, N)\n\ndef method_A_vfi(U, beta, tol, max_iter):\n    \"\"\"\n    Full maximization at every iteration:\n    V_{n+1}(i) = max_j { U[i,j] + beta * V_n[j] }\n    Returns: V, iterations (number of full maximization sweeps)\n    \"\"\"\n    N = U.shape[0]\n    V = np.zeros(N, dtype=float)\n    count = 0\n    for it in range(max_iter):\n        # Compute RHS matrix: U + beta * V_j (broadcast along rows)\n        RHS = U + beta * V[None, :]\n        V_new = RHS.max(axis=1)\n        count += 1\n        diff = np.max(np.abs(V_new - V))\n        V = V_new\n        if diff < tol:\n            break\n    return V, count\n\ndef method_B_policy_improvement(U, beta, tol, m_eval, max_improve):\n    \"\"\"\n    Alternating improvement and evaluation:\n    - Improvement: pi(i) = argmax_j { U[i,j] + beta * V[j] }\n    - Evaluation: V_{t+1}(i) = U[i, pi(i)] + beta * V_t[pi(i)]  (apply m_eval times)\n    Convergence test after each block of m_eval evaluation sweeps:\n    ||V_after - V_before||_inf < tol\n    Returns: V, improvements (number of improvement steps = full maximization sweeps)\n    \"\"\"\n    N = U.shape[0]\n    V = np.zeros(N, dtype=float)\n    improvements = 0\n    for imp in range(max_improve):\n        V_before = V.copy()\n        # Improvement step: one full maximization sweep\n        RHS = U + beta * V[None, :]\n        pi = np.argmax(RHS, axis=1)  # tie-break: first maximizer (np.argmax behavior)\n        # m_eval policy evaluation sweeps with fixed pi\n        idx = np.arange(N)\n        U_pi = U[idx, pi]  # immediate utility under policy\n        for _ in range(m_eval):\n            V = U_pi + beta * V[pi]\n        improvements += 1\n        # Convergence check after the block of evaluations\n        diff = np.max(np.abs(V - V_before))\n        if diff < tol:\n            break\n    return V, improvements\n\ndef solve():\n    # Tolerance and iteration caps\n    tol = 1e-5\n    max_iter_A = 10000\n    max_improve_B = 10000\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (alpha, beta, delta, N_k, a, b, m_eval)\n    test_cases = [\n        (0.33, 0.96, 0.08, 200, 0.5, 1.5, 20),\n        (0.36, 0.99, 0.025, 300, 0.25, 1.75, 25),\n        (0.30, 0.95, 0.10, 80, 0.30, 1.70, 1),\n        (0.40, 0.97, 0.05, 250, 0.40, 1.60, 50),\n    ]\n\n    results = []\n    for (alpha, beta, delta, Nk, a, b, m_eval) in test_cases:\n        # Build grid and utilities\n        kstar = steady_state_k(alpha, beta, delta)\n        kgrid = build_grid(kstar, a, b, Nk)\n        U = precompute_utility_matrix(kgrid, alpha, delta)\n\n        # Method A\n        V_A, it_A = method_A_vfi(U, beta, tol, max_iter_A)\n\n        # Method B\n        V_B, imp_B = method_B_policy_improvement(U, beta, tol, m_eval, max_improve_B)\n\n        # Compare value functions\n        dist = float(np.max(np.abs(V_A - V_B)))\n\n        results.append([it_A, imp_B, imp_B < it_A, dist])\n\n    # Final print statement in the exact required format.\n    # Ensure booleans and floats/ints are properly represented.\n    # Convert nested lists to string representation as required.\n    def format_element(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, float):\n            # Use repr to avoid excessive rounding; ensure it's a valid Python float literal\n            return repr(x)\n        else:\n            return str(x)\n\n    formatted_cases = []\n    for case in results:\n        formatted_cases.append(\"[\" + \",\".join(format_element(x) for x in case) + \"]\")\n    print(\"[\" + \",\".join(formatted_cases) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}