{
    "hands_on_practices": [
        {
            "introduction": "The first step in building any neural network potential is to convert the raw Cartesian coordinates of atoms into a representation that a neural network can process. This representation, known as a descriptor or symmetry function, must inherently respect the fundamental physical symmetries of the system, such as invariance to translation, rotation, and the permutation of identical atoms. This exercise guides you through the implementation of a basic two-body radial symmetry function, a foundational building block for constructing robust and accurate high-dimensional neural network potentials. ",
            "id": "2457438",
            "problem": "Implement a program that derives and computes a basic two-body Behler–Parrinello-style symmetry function for Argon (Ar) atoms and evaluates it on a small test suite of geometries. The purpose is to connect invariance requirements of machine-learned interatomic potentials to a concrete descriptor and to demonstrate numerical behavior under different parameter choices. The overall context is that the total potential energy surface of a system can be approximated as a sum of atomic contributions, each depending on a localized, symmetry-invariant representation of its neighborhood, as in High-Dimensional Neural Network Potentials (HDNNP). Your task is to derive, from invariance principles, a two-body radial symmetry function and implement it.\n\nStart from the following fundamental base:\n- Translational and rotational invariance of a scalar potential energy imply that a local descriptor for an atom must be constructed from internal coordinates such as interatomic distances.\n- For finite-range interactions and locality in the learned mapping, impose a smooth cutoff at a finite radius so that distant atoms beyond a cutoff do not contribute and forces remain well-behaved.\n- To resolve the radial distribution around an atom, use a radial basis with tunable width and center parameters so that the representation can distinguish environments at different length scales.\n\nFrom these principles, derive and then implement a two-body radial symmetry function $G^2$ for a chosen central atom $i$ of the form\n- a sum over neighbor atoms $j \\neq i$,\n- a smooth, finite-range cutoff function that is $C^1$-continuous and equals zero at the cutoff radius,\n- and a localized radial weight that can shift and sharpen around a chosen distance.\n\nConcretely specify and use the following forms in your derivation and implementation:\n- Use the cosine cutoff\n$$\nf_c(r; R_c) = \n\\begin{cases}\n\\dfrac{1}{2}\\left[\\cos\\!\\left(\\dfrac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n0, & r > R_c,\n\\end{cases}\n$$\nwith the cosine argument in radians.\n- Use a Gaussian-like radial basis\n$$\n\\exp\\!\\left[-\\eta\\,(r - R_s)^2\\right],\n$$\nwith width parameter $\\eta$ and shift $R_s$.\n- Combine them in the two-body symmetry function\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c),\n$$\nwhere $r_{ij}$ is the Euclidean distance between atoms $i$ and $j$.\n\nAll atoms are Argon (Ar), treated as a single chemical species, so no species-dependent weighting is required. Distances $r_{ij}$, cutoff $R_c$, and shift $R_s$ must be expressed in Ångström, and $\\eta$ in $\\text{Å}^{-2}$. The cosine function must take its argument in radians.\n\nProgram requirements:\n- Implement a function that, given a set of Cartesian coordinates (in Ångström), an index $i$ for the central atom, and parameters $(\\eta, R_s, R_c)$, computes $G_i^{2}(\\eta, R_s, R_c)$ using the formulas above.\n- Use standard three-dimensional Euclidean distance. Do not apply periodic boundary conditions.\n- Numerical stability: Exclude self-interaction ($j = i$). Distances $r_{ij}$ are strictly nonnegative; do not special-case $r_{ij} = 0$ beyond excluding self-interaction.\n\nTest suite:\nEvaluate $G_i^{2}$ for each of the following five cases. Each case specifies $(\\text{positions}, i, R_c, \\eta, R_s)$, with all distances in Ångström and $\\eta$ in $\\text{Å}^{-2}$:\n- Case A:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n- Case B:\n  - positions: $\\big[(0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 3.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case C:\n  - positions: $\\big[(0,0,0),(5.0,0,0),(-5.0,0,0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 1.0$\n  - $R_s = 0.0$\n- Case D:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 0$\n  - $R_c = 5.0$\n  - $\\eta = 2.0$\n  - $R_s = 2.5$\n- Case E:\n  - positions: $\\big[(0,0,0),(2.0,0,0),(0,3.0,0),(0,0,4.0)\\big]$\n  - $i = 1$\n  - $R_c = 5.0$\n  - $\\eta = 0.5$\n  - $R_s = 0.0$\n\nOutput specification:\n- For each case, compute a single floating-point value $G_i^{2}$.\n- Round each result to exactly $6$ decimal places using standard rounding.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order A, B, C, D, E. For example, an output with generic placeholders should look like \"[0.123456,0.000000,0.000000,1.234567,0.654321]\".",
            "solution": "The problem presented is valid, scientifically sound, and well-posed. It requires the derivation and implementation of a two-body radial symmetry function, a fundamental component of modern machine-learned interatomic potentials such as High-Dimensional Neural Network Potentials (HDNNPs). We shall first deduce the form of this function from first principles, and then detail the algorithm for its computation.\n\nThe potential energy $E$ of a system of atoms is a scalar quantity. For it to be physically meaningful, it must be invariant under translation and rotation of the entire system, as well as under the permutation of identical atoms. In the HDNNP scheme, the total energy is decomposed into atomic contributions $E_i$, where $E = \\sum_i E_i$. Each atomic energy $E_i$ is a function of the local environment of atom $i$, characterized by a set of descriptors or \"symmetry functions,\" $\\{G_i\\}$. Therefore, these symmetry functions must themselves be invariant to the aforementioned transformations.\n\n1.  **Translational and Rotational Invariance**: These symmetries dictate that the descriptors for atom $i$ must depend only on the internal coordinates of its local environment, not on the absolute Cartesian coordinates of the atoms in a global frame. The simplest set of internal coordinates consists of the scalar distances $r_{ij}$ between the central atom $i$ and its neighbors $j$. Any function of these distances, $G_i = F(\\{r_{ij}\\}_{j \\neq i})$, is automatically invariant to rigid translation and rotation of the atomic assembly.\n\n2.  **Permutational Invariance**: The energy contribution of atom $i$ must not depend on the arbitrary labeling of its identical neighbors. If atoms $j$ and $k$ are of the same species, swapping them must not change the value of the descriptor. The simplest mathematical construct that satisfies this is a sum over all neighbors. Thus, we propose a descriptor of the form $G_i = \\sum_{j \\neq i} g(r_{ij})$, where $g$ is some function of the interatomic distance. This form is the basis of the two-body symmetry function.\n\n3.  **Locality and Smoothness**: Physical interactions are local in nature; the influence of very distant atoms is negligible. To model this, we introduce a smooth cutoff function, $f_c(r_{ij}; R_c)$, which multiplies the contribution of each neighbor. This function must be equal to $1$ for small distances, and smoothly go to $0$ as the distance $r_{ij}$ approaches a cutoff radius $R_c$. For distances $r_{ij} > R_c$, the contribution is exactly zero. The requirement for smoothness, specifically $C^1$-continuity (continuous first derivative), is critical. The forces on atoms are calculated as the negative gradient of the potential energy, $\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$. Discontinuities in the first derivative of the energy would lead to unphysical, discontinuous forces. The provided cosine cutoff function is:\n    $$\n    f_c(r; R_c) = \n    \\begin{cases}\n    \\frac{1}{2}\\left[\\cos\\left(\\frac{\\pi r}{R_c}\\right) + 1\\right], & r \\le R_c,\\\\\n    0, & r > R_c.\n    \\end{cases}\n    $$\n    At the cutoff radius $r = R_c$, the function value is $f_c(R_c; R_c) = \\frac{1}{2}[\\cos(\\pi) + 1] = \\frac{1}{2}[-1 + 1] = 0$, ensuring continuity. Its derivative is $f'_c(r; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\frac{\\pi r}{R_c})$. At $r = R_c$, the derivative is $f'_c(R_c; R_c) = -\\frac{\\pi}{2R_c}\\sin(\\pi) = 0$, which matches the derivative of the zero function for $r > R_c$. Thus, the function is $C^1$-continuous as required.\n\n4.  **Radial Resolution**: A simple sum of cutoff functions would only provide a weighted count of neighbors within the cutoff sphere. To create a descriptor that can distinguish different radial structures, we introduce a radial basis function. The specified Gaussian form, $\\exp[-\\eta(r_{ij} - R_s)^2]$, serves this purpose. This function is centered at a distance $R_s$ and has a characteristic width controlled by the parameter $\\eta$. A larger $\\eta$ corresponds to a narrower, more sharply peaked Gaussian. By using a set of these functions with different parameters $(\\eta, R_s)$, one can resolve the radial distribution of neighbors around the central atom $i$.\n\nCombining these four principles—invariance from using distances, permutation symmetry from summation, locality from a smooth cutoff, and resolution from a radial basis—we arrive at the specified two-body radial symmetry function, designated as $G_i^2$:\n$$\nG_i^{2}(\\eta, R_s, R_c) = \\sum_{j \\ne i} \\exp\\!\\left[-\\eta\\,(r_{ij} - R_s)^2\\right]\\, f_c(r_{ij}; R_c)\n$$\nThe sum is over all atoms $j$ in the system, excluding the central atom $i$. For each neighbor $j$, we calculate its contribution only if its distance $r_{ij}$ from atom $i$ is less than or equal to the cutoff radius $R_c$.\n\nThe computational procedure is as follows:\nGiven a set of Cartesian coordinates for $N$ atoms, $\\{\\mathbf{r}_k\\}_{k=0,..,N-1}$, a central atom index $i$, and parameters $\\eta$, $R_s$, and $R_c$:\n1.  Initialize the symmetry function value, $G_i^2$, to $0$.\n2.  Identify the coordinate vector of the central atom, $\\mathbf{r}_i$.\n3.  Iterate through all other atoms $j$ where $j \\in \\{0, 1, ..., N-1\\}$ and $j \\neq i$.\n4.  For each neighbor $j$, compute the Euclidean distance $r_{ij} = ||\\mathbf{r}_j - \\mathbf{r}_i|| = \\sqrt{(x_j-x_i)^2 + (y_j-y_i)^2 + (z_j-z_i)^2}$.\n5.  Check if $r_{ij} \\le R_c$. If not, the contribution from atom $j$ is $0$, and we proceed to the next neighbor.\n6.  If $r_{ij} \\le R_c$, calculate the two components of the term:\n    -   The radial basis term: $T_{\\text{rad}} = \\exp[-\\eta(r_{ij} - R_s)^2]$.\n    -   The cutoff function term: $T_{\\text{cut}} = \\frac{1}{2}[\\cos(\\frac{\\pi r_{ij}}{R_c}) + 1]$.\n7.  Add the product of these terms, $T_{\\text{rad}} \\times T_{\\text{cut}}$, to the running sum for $G_i^2$.\n8.  After iterating through all neighbors $j$, the final sum is the value of the symmetry function for atom $i$.\n\nThis procedure will now be implemented and applied to the five specified test cases. All units must be consistent; distances ($r_{ij}$, $R_s$, $R_c$) are in Ångström ($\\text{Å}$), and the parameter $\\eta$ is in $\\text{Å}^{-2}$, ensuring the argument of the exponential is dimensionless.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing the Behler-Parrinello G2 symmetry function\n    for a series of test cases.\n    \"\"\"\n\n    def compute_g2(positions, i, R_c, eta, R_s):\n        \"\"\"\n        Computes the G2 symmetry function for a central atom i.\n        \n        Args:\n            positions (np.ndarray): Array of shape (N, 3) with Cartesian coordinates.\n            i (int): Index of the central atom.\n            R_c (float): Cutoff radius in Angstrom.\n            eta (float): Width parameter in Angstrom^-2.\n            R_s (float): Shift parameter in Angstrom.\n        \n        Returns:\n            float: The computed value of the G2 symmetry function.\n        \"\"\"\n        if positions.shape[0] <= 1:\n            return 0.0\n\n        central_atom_pos = positions[i]\n        g2_value = 0.0\n\n        for j in range(positions.shape[0]):\n            if i == j:\n                continue\n\n            neighbor_pos = positions[j]\n            # Calculate Euclidean distance\n            r_ij = np.linalg.norm(central_atom_pos - neighbor_pos)\n\n            # Apply the cutoff condition\n            if r_ij <= R_c:\n                # Cosine cutoff function\n                fc = 0.5 * (np.cos(np.pi * r_ij / R_c) + 1.0)\n                \n                # Gaussian-like radial basis function\n                radial_term = np.exp(-eta * (r_ij - R_s)**2)\n                \n                # Add contribution to the sum\n                g2_value += radial_term * fc\n        \n        return g2_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n        # Case B\n        {'positions': np.array([[0.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 3.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case C\n        {'positions': np.array([[0.0, 0.0, 0.0], [5.0, 0.0, 0.0], [-5.0, 0.0, 0.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 1.0, 'R_s': 0.0},\n        # Case D\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 0, 'R_c': 5.0, 'eta': 2.0, 'R_s': 2.5},\n        # Case E\n        {'positions': np.array([[0.0, 0.0, 0.0], [2.0, 0.0, 0.0], [0.0, 3.0, 0.0], [0.0, 0.0, 4.0]]),\n         'i': 1, 'R_c': 5.0, 'eta': 0.5, 'R_s': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_g2(\n            positions=case['positions'],\n            i=case['i'],\n            R_c=case['R_c'],\n            eta=case['eta'],\n            R_s=case['R_s']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format string \"{:.6f}\" handles rounding to 6 decimal places.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After constructing descriptors that respect physical symmetries, a natural question arises: what happens if these symmetries are not perfectly enforced? This practice explores that crucial question by simulating a system with a potential that contains small, controlled \"leaks\" that explicitly break translational and rotational invariance. By running a molecular dynamics simulation and monitoring fundamental conserved quantities like linear and angular momentum, you will gain a profound, hands-on understanding of why strict adherence to symmetry principles is not just an elegant theoretical detail but a practical necessity for stable and physically meaningful simulations. ",
            "id": "2456269",
            "problem": "You are given a reduced-unit model of Molecular Dynamics (MD) in which the potential energy is constructed in the style of a high-dimensional neural network potential (NNP), with a small controlled violation of perfect translation and rotation invariance to emulate a non-perfectly-invariant learned model. All quantities are to be treated in reduced units without physical dimensions. Consider $N$ particles with positions $\\mathbf{r}_i \\in \\mathbb{R}^3$, velocities $\\mathbf{v}_i \\in \\mathbb{R}^3$, and masses $m_i = 1$ for all $i \\in \\{1,\\dots,N\\}$. The total potential energy is\n$$\nE(\\{\\mathbf{r}_i\\}_{i=1}^N) = \\sum_{i=1}^N \\left[g\\!\\left(S_i\\right) + \\varepsilon_t\\,\\boldsymbol{c}\\cdot \\mathbf{r}_i + \\varepsilon_r\\,\\mathbf{r}_i^\\mathsf{T}\\mathbf{Q}\\,\\mathbf{r}_i\\right],\n$$\nwhere\n$$\nS_i = \\sum_{\\substack{j=1 \\\\ j\\neq i}}^N \\exp\\!\\left(-\\alpha\\left(\\|\\mathbf{r}_i - \\mathbf{r}_j\\| - r_0\\right)\\right), \\quad g(x) = a\\,x + b\\,\\tanh(x),\n$$\nand the constants are\n$$\n\\alpha = 1.2,\\quad r_0 = 1.0,\\quad a = 0.5,\\quad b = 0.2,\n$$\n$$\n\\boldsymbol{c} = \\begin{bmatrix}0.3\\\\-0.2\\\\0.1\\end{bmatrix},\\quad\n\\mathbf{Q} = \\begin{bmatrix}0.3 & 0.1 & 0.0\\\\ 0.1 & -0.2 & 0.05\\\\ 0.0 & 0.05 & -0.1\\end{bmatrix}.\n$$\nLet $\\mathbf{F}_i = -\\nabla_{\\mathbf{r}_i} E$ be the force on particle $i$. The equations of motion are the Newtonian equations\n$$\n\\frac{d\\mathbf{r}_i}{dt} = \\mathbf{v}_i,\\qquad \\frac{d\\mathbf{v}_i}{dt} = \\mathbf{F}_i.\n$$\nYou must evolve the system in discrete time using the following time-discretized update for a time step $\\Delta t$ and $K$ steps:\n1. Set the acceleration $\\mathbf{a}_i^{(n)} = \\mathbf{F}_i(\\{\\mathbf{r}_k^{(n)}\\}_{k=1}^N)$.\n2. Update positions\n$$\n\\mathbf{r}_i^{(n+1)} = \\mathbf{r}_i^{(n)} + \\Delta t\\,\\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t^2\\,\\mathbf{a}_i^{(n)}.\n$$\n3. Evaluate the new accelerations $\\mathbf{a}_i^{(n+1)} = \\mathbf{F}_i(\\{\\mathbf{r}_k^{(n+1)}\\}_{k=1}^N)$.\n4. Update velocities\n$$\n\\mathbf{v}_i^{(n+1)} = \\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t\\,\\left(\\mathbf{a}_i^{(n)} + \\mathbf{a}_i^{(n+1)}\\right).\n$$\nDefine the total linear momentum and total angular momentum about the origin at step $n$ by\n$$\n\\mathbf{P}^{(n)} = \\sum_{i=1}^N \\mathbf{v}_i^{(n)},\\qquad\n\\mathbf{L}^{(n)} = \\sum_{i=1}^N \\mathbf{r}_i^{(n)} \\times \\mathbf{v}_i^{(n)}.\n$$\nFor each test case below, starting from the specified initial conditions, evolve the system for $K$ steps with time step $\\Delta t$ and compute the drift magnitudes\n$$\np_{\\mathrm{drift}} = \\left\\|\\mathbf{P}^{(K)} - \\mathbf{P}^{(0)}\\right\\|_2,\\qquad\n\\ell_{\\mathrm{drift}} = \\left\\|\\mathbf{L}^{(K)} - \\mathbf{L}^{(0)}\\right\\|_2.\n$$\nAll outputs must be reported in reduced units and rounded to $6$ decimal places.\n\nTest suite (each line specifies a case $[N,\\{\\mathbf{r}_i(0)\\}_{i=1}^N,\\{\\mathbf{v}_i(0)\\}_{i=1}^N,\\varepsilon_t,\\varepsilon_r,\\Delta t,K]$):\n- Case A (nominal invariance, multi-particle):\n$[\\,3,\\ \\{(-0.8,0.0,0.0),\\ (0.4,0.692820323,0.0),\\ (0.4,-0.692820323,0.0)\\},\\ \\{(0.0,0.0,0.0),\\ (0.0,0.0,0.0),\\ (0.0,0.0,0.0)\\},\\ 0.0,\\ 0.0,\\ 0.002,\\ 5000\\,]$.\n- Case B (small invariance leak, multi-particle):\n$[\\,3,\\ \\{(-0.8,0.0,0.0),\\ (0.4,0.692820323,0.0),\\ (0.4,-0.692820323,0.0)\\},\\ \\{(0.0,0.0,0.0),\\ (0.0,0.0,0.0),\\ (0.0,0.0,0.0)\\},\\ 0.001,\\ 0.0005,\\ 0.002,\\ 5000\\,]$.\n- Case C (single-particle edge case with leak):\n$[\\,1,\\ \\{(0.3,0.4,0.1)\\},\\ \\{(0.0,0.0,0.0)\\},\\ 0.002,\\ 0.001,\\ 0.002,\\ 4000\\,]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the form\n$[[p_A,\\ell_A],[p_B,\\ell_B],[p_C,\\ell_C]]$, where $p_X$ and $\\ell_X$ are the drift magnitudes for the corresponding case $X\\in\\{\\mathrm{A},\\mathrm{B},\\mathrm{C}\\}$, each rounded to $6$ decimal places in reduced units. No other output is permitted.",
            "solution": "The problem requires the simulation of a system of $N$ particles in reduced units governed by a custom potential energy function using the Velocity Verlet integration algorithm. The task is to calculate the drift in total linear and angular momentum over a fixed number of time steps for three distinct test cases. The problem is scientifically grounded, well-posed, and all necessary parameters and initial conditions are provided. It represents a valid computational physics problem.\n\nThe solution proceeds as follows:\n1.  Derive the analytical expression for the force $\\mathbf{F}_i$ on each particle $i$ by taking the negative gradient of the potential energy $E$ with respect to the particle's position $\\mathbf{r}_i$.\n2.  Implement a numerical function to compute these forces for any given configuration of particle positions $\\{\\mathbf{r}_i\\}$.\n3.  Implement the Velocity Verlet algorithm to evolve the system's positions $\\{\\mathbf{r}_i\\}$ and velocities $\\{\\mathbf{v}_i\\}$ over time.\n4.  Execute the simulation for each of the three specified test cases.\n5.  Compute the drift magnitudes for total linear momentum, $p_{\\mathrm{drift}}$, and total angular momentum, $\\ell_{\\mathrm{drift}}$, as defined in the problem statement.\n\nFirst, we must derive the force $\\mathbf{F}_k = -\\nabla_{\\mathbf{r}_k} E$ for an arbitrary particle $k$. The potential energy $E$ is given by:\n$$\nE(\\{\\mathbf{r}_i\\}) = \\sum_{i=1}^N \\left[g\\!\\left(S_i\\right) + \\varepsilon_t\\,\\boldsymbol{c}\\cdot \\mathbf{r}_i + \\varepsilon_r\\,\\mathbf{r}_i^\\mathsf{T}\\mathbf{Q}\\,\\mathbf{r}_i\\right]\n$$\nThe gradient $\\nabla_{\\mathbf{r}_k}$ acts on each term. The terms with $\\varepsilon_t$ and $\\varepsilon_r$ only contribute when $i=k$, as they are independent of $\\mathbf{r}_i$ for $i \\neq k$. The gradients of these \"leak\" terms are straightforward:\n$$\n\\nabla_{\\mathbf{r}_k} (\\varepsilon_t\\,\\boldsymbol{c}\\cdot \\mathbf{r}_k) = \\varepsilon_t \\boldsymbol{c}\n$$\n$$\n\\nabla_{\\mathbf{r}_k} (\\varepsilon_r\\,\\mathbf{r}_k^\\mathsf{T}\\mathbf{Q}\\,\\mathbf{r}_k) = \\varepsilon_r (\\mathbf{Q} + \\mathbf{Q}^\\mathsf{T})\\mathbf{r}_k = 2\\varepsilon_r \\mathbf{Q}\\,\\mathbf{r}_k\n$$\nwhere we have used the fact that the given matrix $\\mathbf{Q}$ is symmetric.\n\nThe gradient of the many-body term $\\sum_{i=1}^N g(S_i)$ is more complex. Using the chain rule:\n$$\n\\nabla_{\\mathbf{r}_k} \\sum_{i=1}^N g(S_i) = \\sum_{i=1}^N \\frac{dg}{dS_i} \\nabla_{\\mathbf{r}_k} S_i = \\sum_{i=1}^N g'(S_i) \\nabla_{\\mathbf{r}_k} S_i\n$$\nThe gradient $\\nabla_{\\mathbf{r}_k} S_i$ is non-zero only when $S_i$ depends on $\\mathbf{r}_k$. $S_i = \\sum_{j\\neq i} \\exp(-\\alpha(\\|\\mathbf{r}_i - \\mathbf{r}_j\\| - r_0)) = \\sum_{j\\neq i} f_{ij}$. This dependency occurs when $i=k$ or when one of the summation indices $j$ is equal to $k$.\n\nFor $i=k$:\n$$\n\\nabla_{\\mathbf{r}_k} S_k = \\nabla_{\\mathbf{r}_k} \\sum_{j\\neq k} f_{kj} = \\sum_{j\\neq k} f_{kj} (-\\alpha) \\nabla_{\\mathbf{r}_k} \\|\\mathbf{r}_k - \\mathbf{r}_j\\| = \\sum_{j\\neq k} \\left(-\\alpha f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{\\|\\mathbf{r}_k - \\mathbf{r}_j\\|}\\right)\n$$\nFor $i \\neq k$: The only term in the sum for $S_i$ that depends on $\\mathbf{r}_k$ is $f_{ik}$.\n$$\n\\nabla_{\\mathbf{r}_k} S_i = \\nabla_{\\mathbf{r}_k} f_{ik} = f_{ik} (-\\alpha) \\nabla_{\\mathbf{r}_k} \\|\\mathbf{r}_i - \\mathbf{r}_k\\| = f_{ik} (-\\alpha) \\frac{\\mathbf{r}_k - \\mathbf{r}_i}{\\|\\mathbf{r}_i - \\mathbf{r}_k\\|} = \\alpha f_{ik} \\frac{\\mathbf{r}_i - \\mathbf{r}_k}{\\|\\mathbf{r}_i - \\mathbf{r}_k\\|}\n$$\nCombining these results, the gradient of the symmetric part of the energy is:\n$$\n\\nabla_{\\mathbf{r}_k} \\left(\\sum_i g(S_i)\\right) = g'(S_k) \\nabla_{\\mathbf{r}_k} S_k + \\sum_{i\\neq k} g'(S_i) \\nabla_{\\mathbf{r}_k} S_i\n$$\nSubstituting the expressions for the gradients and relabeling the summation index $i$ to $j$ in the second term gives:\n$$\n= g'(S_k) \\sum_{j\\neq k} \\left(-\\alpha f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{r_{kj}}\\right) + \\sum_{j\\neq k} g'(S_j) \\left(\\alpha f_{jk} \\frac{\\mathbf{r}_j - \\mathbf{r}_k}{r_{jk}}\\right)\n$$\nwhere $r_{kj} = \\|\\mathbf{r}_k-\\mathbf{r}_j\\|$. Since $f_{kj}=f_{jk}$, $r_{kj}=r_{jk}$, and $\\mathbf{r}_j-\\mathbf{r}_k = -(\\mathbf{r}_k-\\mathbf{r}_j)$, this simplifies to:\n$$\n= -\\alpha \\sum_{j\\neq k} (g'(S_k) + g'(S_j)) f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{r_{kj}}\n$$\nThe derivative of $g(x) = a\\,x + b\\,\\tanh(x)$ is $g'(x) = a + b\\,\\mathrm{sech}^2(x) = a+b(1-\\tanh^2(x))$.\n\nThe total force on particle $k$ is the negative of the total gradient:\n$$\n\\mathbf{F}_k = \\alpha \\sum_{j\\neq k} \\left(g'(S_k) + g'(S_j)\\right) f_{kj} \\frac{\\mathbf{r}_k - \\mathbf{r}_j}{r_{kj}} - \\varepsilon_t\\boldsymbol{c} - 2\\varepsilon_r\\mathbf{Q}\\mathbf{r}_k\n$$\nFor the single-particle case ($N=1$), the summation over $j\\neq k$ is empty, so $S_1=0$. The force simplifies to $\\mathbf{F}_1 = - \\varepsilon_t \\boldsymbol{c} - 2\\varepsilon_r \\mathbf{Q} \\mathbf{r}_1$.\n\nThe dynamics are evolved using the Velocity Verlet algorithm for $K$ steps with a time step $\\Delta t$. With masses $m_i=1$, the accelerations are $\\mathbf{a}_i = \\mathbf{F}_i$.\n1.  Initialize $\\mathbf{r}^{(0)}, \\mathbf{v}^{(0)}$. Calculate $\\mathbf{a}^{(0)} = \\mathbf{F}(\\{\\mathbf{r}_k^{(0)}\\})$.\n2.  For $n=0, \\dots, K-1$:\n    a.  $\\mathbf{r}_i^{(n+1)} = \\mathbf{r}_i^{(n)} + \\Delta t\\,\\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t^2\\,\\mathbf{a}_i^{(n)}$\n    b.  $\\mathbf{a}_i^{(n+1)} = \\mathbf{F}_i(\\{\\mathbf{r}_k^{(n+1)}\\})$\n    c.  $\\mathbf{v}_i^{(n+1)} = \\mathbf{v}_i^{(n)} + \\tfrac{1}{2}\\,\\Delta t\\,\\left(\\mathbf{a}_i^{(n)} + \\mathbf{a}_i^{(n+1)}\\right)$\nThe initial total linear momentum $\\mathbf{P}^{(0)} = \\sum_i \\mathbf{v}_i^{(0)}$ and angular momentum $\\mathbf{L}^{(0)} = \\sum_i \\mathbf{r}_i^{(0)} \\times \\mathbf{v}_i^{(0)}$ are calculated. After $K$ steps, the final values $\\mathbf{P}^{(K)}$ and $\\mathbf{L}^{(K)}$ are computed, and the Euclidean norms of the differences, $p_{\\mathrm{drift}} = \\|\\mathbf{P}^{(K)} - \\mathbf{P}^{(0)}\\|$ and $\\ell_{\\mathrm{drift}} = \\|\\mathbf{L}^{(K)} - \\mathbf{L}^{(0)}\\|$, are determined.\nThe implementation uses `numpy` for efficient vectorized calculations of forces and state updates.\n\n-   Case A ($\\varepsilon_t = 0, \\varepsilon_r = 0$): The potential is perfectly invariant to translation and rotation. Conservation of linear and angular momentum is expected, so drifts should be nearly zero, limited only by numerical precision.\n-   Case B ($\\varepsilon_t > 0, \\varepsilon_r > 0$): The invariance is explicitly broken. The net force $\\sum_i \\mathbf{F}_i$ and net torque $\\sum_i \\mathbf{r}_i \\times \\mathbf{F}_i$ are non-zero, leading to a secular drift in $\\mathbf{P}$ and $\\mathbf{L}$. Non-zero drift magnitudes are expected.\n-   Case C ($N=1$): The particle is subject to an external field. As it starts from rest, it will accelerate, leading to non-zero final momentum and angular momentum, hence significant drifts.\n\nThe provided Python code implements this logic to solve for the three cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n\n    def calculate_forces(r, N, alpha, r0, a, b, eps_t, eps_r, c_vec, Q_mat):\n        \"\"\"\n        Calculates the forces on all particles.\n        \"\"\"\n        if N == 1:\n            force = -eps_t * c_vec - 2 * eps_r * (Q_mat @ r[0])\n            return force.reshape(1, 3)\n\n        # Vectorized calculation for N > 1\n        # Pairwise differences and distances\n        diffs = r[:, np.newaxis, :] - r[np.newaxis, :, :]  # Shape (N, N, 3)\n        dists = np.linalg.norm(diffs, axis=2)  # Shape (N, N)\n        \n        # Avoid division by zero for diagonal elements, which are not used anyway\n        dists_safe = np.copy(dists)\n        np.fill_diagonal(dists_safe, 1.0)\n        unit_vectors = diffs / dists_safe[:, :, np.newaxis]\n\n        # Calculate exponential term f_ij\n        f_exp = np.exp(-alpha * (dists - r0))\n        np.fill_diagonal(f_exp, 0)  # Sum is over j != i\n\n        # Calculate S_i and g'(S_i)\n        S = np.sum(f_exp, axis=1)  # Shape (N,)\n        tanh_S = np.tanh(S)\n        g_prime_S = a + b * (1 - tanh_S**2)  # Shape (N,)\n\n        # Calculate symmetric part of the force\n        g_prime_sum = g_prime_S[:, np.newaxis] + g_prime_S[np.newaxis, :] # Shape (N, N)\n        term_scalar = alpha * g_prime_sum * f_exp  # Shape (N, N)\n        \n        force_sym_contrib = term_scalar[:, :, np.newaxis] * unit_vectors\n        force_sym = np.sum(force_sym_contrib, axis=1)\n\n        # Calculate leak part of the force\n        force_leak = -eps_t * c_vec - 2 * eps_r * (r @ Q_mat)\n\n        forces = force_sym + force_leak\n        return forces\n\n    def run_simulation(N, r_init, v_init, eps_t, eps_r, dt, K):\n        \"\"\"\n        Runs a single MD simulation for a given set of parameters.\n        \"\"\"\n        # Constants\n        alpha = 1.2\n        r0_const = 1.0\n        a = 0.5\n        b = 0.2\n        c_vec = np.array([0.3, -0.2, 0.1])\n        Q_mat = np.array([[0.3, 0.1, 0.0], \n                          [0.1, -0.2, 0.05], \n                          [0.0, 0.05, -0.1]])\n\n        # Initial state\n        r = np.array(r_init, dtype=float)\n        v = np.array(v_init, dtype=float)\n        \n        # Initial momentum and angular momentum (masses m_i = 1)\n        P0 = np.sum(v, axis=0) if N > 0 else np.zeros(3)\n        L0 = np.sum(np.cross(r, v), axis=0) if N > 0 else np.zeros(3)\n\n        # Initial acceleration\n        acc = calculate_forces(r, N, alpha, r0_const, a, b, eps_t, eps_r, c_vec, Q_mat)\n\n        # Main loop (Velocity Verlet)\n        for _ in range(K):\n            # Update positions\n            r += dt * v + 0.5 * dt**2 * acc\n            # Store old acceleration\n            acc_old = acc\n            # Calculate new acceleration\n            acc = calculate_forces(r, N, alpha, r0_const, a, b, eps_t, eps_r, c_vec, Q_mat)\n            # Update velocities\n            v += 0.5 * dt * (acc_old + acc)\n\n        # Final momentum and angular momentum\n        PK = np.sum(v, axis=0) if N > 0 else np.zeros(3)\n        LK = np.sum(np.cross(r, v), axis=0) if N > 0 else np.zeros(3)\n\n        # Calculate drifts\n        p_drift = np.linalg.norm(PK - P0)\n        l_drift = np.linalg.norm(LK - L0)\n\n        return p_drift, l_drift\n\n    # Test cases from the problem statement\n    test_cases = [\n        (3, [(-0.8, 0.0, 0.0), (0.4, 0.692820323, 0.0), (0.4, -0.692820323, 0.0)], \n            [(0.0, 0.0, 0.0), (0.0, 0.0, 0.0), (0.0, 0.0, 0.0)], \n            0.0, 0.0, 0.002, 5000), # Case A\n        (3, [(-0.8, 0.0, 0.0), (0.4, 0.692820323, 0.0), (0.4, -0.692820323, 0.0)], \n            [(0.0, 0.0, 0.0), (0.0, 0.0, 0.0), (0.0, 0.0, 0.0)], \n            0.001, 0.0005, 0.002, 5000), # Case B\n        (1, [(0.3, 0.4, 0.1)], \n            [(0.0, 0.0, 0.0)], \n            0.002, 0.001, 0.002, 4000) # Case C\n    ]\n    \n    results = []\n    for case in test_cases:\n        p_drift, l_drift = run_simulation(*case)\n        results.append((p_drift, l_drift))\n\n    formatted_results = [f\"[{p:.6f},{l:.6f}]\" for p, l in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A robust NNP model requires both a well-designed, symmetric architecture and high-quality training data. In this exercise, we shift our focus from the model's structure to the data itself, investigating the principle of \"garbage in, garbage out\" in a practical context. You will deliberately \"poison\" a clean training dataset with a few highly erroneous energy labels—a situation that can occur in real research—and analyze the disproportionate and non-local impact on the resulting potential energy surface, affecting not just energies but also predicted geometries and forces. ",
            "id": "2456326",
            "problem": "You are given a one-dimensional benchmark of a diatomic molecule whose ground-truth potential energy surface (PES) is defined by the Morse potential. Let the true energy as a function of bond length be given by\n$$\nE_{\\text{true}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2 - D_e,\n$$\nwith parameters $D_e = 4.744$ electronvolts (eV), $r_e = 0.7414$ angstroms ($\\text{\\AA}$), and $a = 1.942$ $\\text{\\AA}^{-1}$. All distances must be expressed in $\\text{\\AA}$ and all energies in eV.\n\nConsider a high-dimensional neural network potential (NNP) model that maps a vector of symmetry functions to a predicted energy. Define the atom-centered radial descriptor vector $\\mathbf{G}(r) \\in \\mathbb{R}^M$ of length $M = 8$ by\n$$\nG_i(r) = f_c(r)\\,\\exp\\!\\left(-\\eta \\left(r - R_{s,i}\\right)^2\\right),\n$$\nwhere the smooth cutoff function $f_c(r)$ is\n$$\nf_c(r) =\n\\begin{cases}\n\\frac{1}{2}\\left[\\cos\\!\\left(\\pi \\frac{r}{R_c}\\right) + 1\\right], & r \\le R_c, \\\\\n0, & r > R_c,\n\\end{cases}\n$$\nwith $R_c = 3.0$ $\\text{\\AA}$, $\\eta = 4.0$ $\\text{\\AA}^{-2}$, and centers $R_{s,i}$ given by the list $R_{s} = \\{0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7\\}$ in $\\text{\\AA}$ (that is, $R_{s,1} = 0.6$, $R_{s,2} = 0.9$, $R_{s,3} = 1.2$, $R_{s,4} = 1.5$, $R_{s,5} = 1.8$, $R_{s,6} = 2.1$, $R_{s,7} = 2.4$, $R_{s,8} = 2.7$).\n\nDefine the neural network potential as\n$$\nE_{\\boldsymbol{\\theta}}(r) = \\mathbf{w}_2^\\top \\,\\phi\\!\\left(\\mathbf{W}_1\\,\\mathbf{G}(r) + \\mathbf{b}_1\\right) + b_2,\n$$\nwhere $\\phi(x) = \\tanh(x)$ is applied elementwise, $\\mathbf{W}_1 \\in \\mathbb{R}^{H \\times M}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{H}$, $\\mathbf{w}_2 \\in \\mathbb{R}^{H}$, $b_2 \\in \\mathbb{R}$, and the hidden layer size is $H = 10$. The model parameters $\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ are to be obtained by minimizing the mean squared error objective\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{n=1}^{N}\\left(E_{\\boldsymbol{\\theta}}(r_n) - E_{\\text{true}}(r_n)\\right)^2,\n$$\ngiven training pairs $\\{(r_n, E_{\\text{true}}(r_n))\\}_{n=1}^{N}$. Use $N_{\\text{train}} = 64$ training distances $r_n$ uniformly spaced over the closed interval $[0.5, 2.8]$ $\\text{\\AA}$.\n\nParameter estimation must be performed by batch gradient descent with the following exact specifications: learning rate $\\alpha = 10^{-2}$, number of iterations $T = 3000$, weight initialization using a fixed pseudo-random seed equal to $42$, with independent normal draws of zero mean and standard deviations $\\sigma_{W_1} = 1/\\sqrt{M}$ and $\\sigma_{w_2} = 1/\\sqrt{H}$ for the entries of $\\mathbf{W}_1$ and $\\mathbf{w}_2$, respectively, and zero initialization for $\\mathbf{b}_1$ and $b_2$. No other stochasticity is permitted.\n\nConstruct a baseline model by training on the clean dataset $\\{(r_n, E_{\\text{true}}(r_n))\\}_{n=1}^{N}$. Then construct four poisoned datasets by replacing the energy values at specific training distances with erroneous values intended to mimic highly erroneous Density Functional Theory (DFT) labels. For a given target distance $r^\\star$ and offset $\\Delta$, identify the training index $k$ that minimizes $|r_k - r^\\star|$ and set the corresponding energy label to $E_{\\text{true}}(r_k) + \\Delta$; if multiple targets map to the same $k$, accumulate the offsets by addition. The four poisoning scenarios are:\n- Scenario $1$ (boundary case): no poisoning (empty set of targets).\n- Scenario $2$: a single poisoned point at $r^\\star = 0.7414$ $\\text{\\AA}$ with offset $\\Delta = +8.0$ eV.\n- Scenario $3$: two poisoned points at $r^\\star = 2.2$ $\\text{\\AA}$ and $r^\\star = 2.4$ $\\text{\\AA}$, each with offset $\\Delta = -4.0$ eV.\n- Scenario $4$ (edge case): one poisoned point at $r^\\star = 0.6$ $\\text{\\AA}$ with offset $\\Delta = -10.0$ eV.\n\nFor each poisoned scenario, re-train a new model from scratch under the same optimization settings and initialization seed as the baseline. For evaluation, define a uniform grid of $N_{\\text{eval}} = 201$ distances over the closed interval $[0.5, 3.0]$ $\\text{\\AA}$, and compute the following three quantitative measures that capture the difference between the poisoned model’s predicted PES and the baseline model’s predicted PES:\n1. The root-mean-square energy deviation\n$$\nm_1 = \\sqrt{\\frac{1}{N_{\\text{eval}}}\\sum_{j=1}^{N_{\\text{eval}}}\\left(E_{\\boldsymbol{\\theta}^{\\text{poison}}}(r_j) - E_{\\boldsymbol{\\theta}^{\\text{base}}}(r_j)\\right)^2}\\ \\text{in eV}.\n$$\n2. The absolute shift in the equilibrium bond length (minimum-energy position) predicted on the evaluation grid,\n$$\nm_2 = \\left|\\operatorname*{argmin}_{j}\\,E_{\\boldsymbol{\\theta}^{\\text{poison}}}(r_j) - \\operatorname*{argmin}_{j}\\,E_{\\boldsymbol{\\theta}^{\\text{base}}}(r_j)\\right|\\ \\text{in }\\text{\\AA},\n$$\nwhere $\\operatorname*{argmin}_{j}$ returns the grid value $r_j$ at which the discrete minimum occurs.\n3. The maximum absolute deviation in predicted forces over the grid,\n$$\nm_3 = \\max_{j}\\,\\left|F_{\\boldsymbol{\\theta}^{\\text{poison}}}(r_j) - F_{\\boldsymbol{\\theta}^{\\text{base}}}(r_j)\\right|\\ \\text{in eV}/\\text{\\AA},\n$$\nwhere the force is defined as $F_{\\boldsymbol{\\theta}}(r) = -\\frac{d}{dr}E_{\\boldsymbol{\\theta}}(r)$ and the derivative must be approximated on the evaluation grid by finite differences using the grid spacing implied by the uniform grid.\n\nAngles do not appear in this one-dimensional problem. All outputs must be reported in the units specified above. Your program must compute $(m_1, m_2, m_3)$ for each of the four scenarios in the order listed. Round each scalar to exactly six decimal places.\n\nYour program should produce a single line of output containing the results as a list of lists in the form\n$$\n\\big[\\,[m_{1,1}, m_{2,1}, m_{3,1}],\\ [m_{1,2}, m_{2,2}, m_{3,2}],\\ [m_{1,3}, m_{2,3}, m_{3,3}],\\ [m_{1,4}, m_{2,4}, m_{3,4}]\\,\\big],\n$$\nwith each numeric entry rounded to six decimal places, for the four scenarios $1$ through $4$.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extracted Givens**\n\n- **Ground-Truth Potential Energy Surface (PES):** Morse potential, $E_{\\text{true}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2 - D_e$.\n  - Parameters: $D_e = 4.744$ eV, $r_e = 0.7414$ Å, $a = 1.942$ Å⁻¹.\n- **Symmetry Function Descriptors:** For a diatomic system with bond length $r$, the descriptor vector is $\\mathbf{G}(r) \\in \\mathbb{R}^M$ where $M=8$.\n  - Components: $G_i(r) = f_c(r)\\,\\exp\\!\\left(-\\eta \\left(r - R_{s,i}\\right)^2\\right)$.\n  - Cutoff function: $f_c(r) = \\frac{1}{2}\\left[\\cos\\!\\left(\\pi \\frac{r}{R_c}\\right) + 1\\right]$ for $r \\le R_c$, and $0$ otherwise.\n  - Parameters: $R_c = 3.0$ Å, $\\eta = 4.0$ Å⁻², and centers $R_{s} = \\{0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7\\}$ Å.\n- **Neural Network Potential (NNP) Model:** A single-hidden-layer perceptron.\n  - Architecture: $E_{\\boldsymbol{\\theta}}(r) = \\mathbf{w}_2^\\top \\,\\phi\\!\\left(\\mathbf{W}_1\\,\\mathbf{G}(r) + \\mathbf{b}_1\\right) + b_2$.\n  - Hidden layer size: $H = 10$.\n  - Activation: $\\phi(x) = \\tanh(x)$.\n  - Parameters: $\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$.\n- **Training Procedure:**\n  - Objective: Minimize Mean Squared Error, $\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N}\\sum_{n=1}^{N}\\left(E_{\\boldsymbol{\\theta}}(r_n) - E_{\\text{true}}(r_n)\\right)^2$.\n  - Dataset: $N_{\\text{train}} = 64$ points, with distances $r_n$ uniformly spaced in $[0.5, 2.8]$ Å.\n  - Optimizer: Batch Gradient Descent.\n  - Hyperparameters: Learning rate $\\alpha = 10^{-2}$, iterations $T = 3000$.\n  - Initialization: Fixed pseudo-random seed of $42$. Weights $\\mathbf{W}_1$ and $\\mathbf{w}_2$ from normal distributions with zero mean and standard deviations $\\sigma_{W_1} = 1/\\sqrt{M}$ and $\\sigma_{w_2} = 1/\\sqrt{H}$, respectively. Biases $\\mathbf{b}_1, b_2$ initialized to zero.\n- **Data Poisoning Scenarios:** Modify training energy labels $E_{\\text{true}}(r_k)$ to $E_{\\text{true}}(r_k) + \\Delta$ at index $k$ that minimizes $|r_k - r^\\star|$.\n  - Scenario $1$: No poisoning.\n  - Scenario $2$: $r^\\star = 0.7414$ Å, $\\Delta = +8.0$ eV.\n  - Scenario $3$: $r^\\star = 2.2$ Å, $\\Delta = -4.0$ eV and $r^\\star = 2.4$ Å, $\\Delta = -4.0$ eV.\n  - Scenario $4$: $r^\\star = 0.6$ Å, $\\Delta = -10.0$ eV.\n- **Evaluation Metrics:** Compare each poisoned model to a baseline model (trained on un-poisoned data) on a grid of $N_{\\text{eval}} = 201$ distances in $[0.5, 3.0]$ Å.\n  1. $m_1$: Root-mean-square energy deviation (eV).\n  2. $m_2$: Absolute shift in equilibrium bond length (Å).\n  3. $m_3$: Maximum absolute deviation in predicted forces (eV/Å), where force $F = -dE/dr$ is approximated by finite differences.\n\n**Step 2: Validation of Givens**\n\nThe problem is examined for validity.\n- **Scientifically Grounded:** It is. The problem describes a simplified but standard benchmark in computational chemistry for testing neural network potentials. The Morse potential is a canonical model for diatomic interactions, and the Behler-Parrinello-type NNP architecture represents a foundational method in the field. All parameters and units are physically coherent.\n- **Well-Posed:** It is. The task is a well-defined supervised regression problem with a deterministic training procedure (batch gradient descent from a fixed initialization). This guarantees the existence of a unique, computable solution.\n- **Objective:** It is. The problem is stated using precise mathematical definitions and quantitative measures, free of any subjective or ambiguous language.\n\nThe problem does not violate any criteria for invalidity. It is a complete, consistent, and scientifically meaningful computational task.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n**Principle-Based Solution**\n\nThe objective is to quantify the impact of erroneous training data points on a neural network potential (NNP) by training several models and comparing their predictions against a baseline. The core of the solution involves implementing a complete machine learning workflow: data generation, featurization, model training, and evaluation.\n\nFirst, we define the ground-truth potential energy surface (PES) using the Morse potential:\n$$\nE_{\\text{true}}(r) = D_e \\left(1 - e^{-a (r - r_e)}\\right)^2 - D_e\n$$\nThis function provides the target energy labels for our training dataset, which consists of $N_{\\text{train}} = 64$ distances $r_n$ uniformly sampled from the interval $[0.5, 2.8]$ Å.\n\nNext, we define the transformation from a bond distance $r$ to a feature vector $\\mathbf{G}(r) \\in \\mathbb{R}^M$, which serves as the input to the NNP. This is accomplished using a set of $M=8$ atom-centered radial symmetry functions:\n$$\nG_i(r) = f_c(r)\\,\\exp\\!\\left(-\\eta \\left(r - R_{s,i}\\right)^2\\right)\n$$\nwhere a cosine-based cutoff function $f_c(r)$ ensures that the descriptor smoothly goes to zero at the cutoff radius $R_c = 3.0$ Å. This featurization converts the raw geometric information into a format that is suitable for a machine learning model and invariant to permutations of identical atoms (a trivial property in a diatomic case, but a crucial design principle for general NNPs).\n\nThe NNP itself is a standard feed-forward neural network with one hidden layer of size $H=10$ and a hyperbolic tangent activation function, $\\phi(x) = \\tanh(x)$. The predicted energy for an input descriptor vector $\\mathbf{G}(r)$ is given by:\n$$\nE_{\\boldsymbol{\\theta}}(r) = \\mathbf{w}_2^\\top \\,\\phi\\!\\left(\\mathbf{W}_1\\,\\mathbf{G}(r) + \\mathbf{b}_1\\right) + b_2\n$$\nThe parameters $\\boldsymbol{\\theta} = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ are optimized by minimizing the mean squared error (MSE) loss function between the predicted energies and the training labels over the entire dataset:\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N_{\\text{train}}}\\sum_{n=1}^{N_{\\text{train}}}\\left(E_{\\boldsymbol{\\theta}}(r_n) - E_{\\text{label}}(r_n)\\right)^2\n$$\nOptimization is performed using batch gradient descent for $T=3000$ iterations with a learning rate of $\\alpha=10^{-2}$. The gradients of the loss with respect to the parameters are derived using the backpropagation algorithm. For a parameter $P \\in \\boldsymbol{\\theta}$, the update rule is:\n$$\nP \\leftarrow P - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial P}\n$$\nThe gradients, such as $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_1}$, are computed by applying the chain rule backward from the output layer to the input layer. For example, the gradient for the first-layer weights $\\mathbf{W}_1$ is a function of the input descriptors $\\mathbf{G}$, the error at the output, and the derivatives of the activation function. A fixed random seed ensures that the initial parameter values are identical for every training run, isolating the effect of the data poisoning.\n\nWe first establish a baseline by training a model, $\\boldsymbol{\\theta}^{\\text{base}}$, on the clean dataset where $E_{\\text{label}}(r_n) = E_{\\text{true}}(r_n)$. Then, for each of the three poisoning scenarios, we create a new training dataset by altering the energy labels at specific points. For each of these \"poisoned\" datasets, we train a new model, $\\boldsymbol{\\theta}^{\\text{poison}}$, from the same initial state.\n\nFinally, we evaluate the impact of poisoning. We generate a fine evaluation grid of $N_{\\text{eval}} = 201$ distances in $[0.5, 3.0]$ Å. For each poisoned model, we compute three metrics relative to the baseline model's predictions on this grid:\n1.  The root-mean-square deviation of energies, $m_1$, measures the average global change in the predicted PES.\n2.  The shift in the predicted equilibrium bond length, $m_2$, probes the effect on a critical physical property of the molecule. This is found by identifying the grid point $r_j$ that minimizes the predicted energy for each model.\n3.  The maximum absolute deviation of forces, $m_3$, evaluates the impact on energy derivatives, which are essential for applications like molecular dynamics. The force $F(r) = -dE/dr$ is approximated numerically using central finite differences on the evaluation grid.\n\nThe first scenario, with no poisoning, serves as a control. The \"poisoned\" model is identical to the baseline model, so all three deviation metrics $(m_1, m_2, m_3)$ are trivially zero. For the other scenarios, these metrics provide quantitative insight into the NNP's sensitivity to localized errors in the training data.",
            "answer": "```python\nimport numpy as np\n\n# Global constants defined in the problem statement\nDE = 4.744\nRE = 0.7414\nA = 1.942\nRC = 3.0\nETA = 4.0\nRS = np.array([0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7])\nM = 8\nH = 10\nN_TRAIN = 64\nN_EVAL = 201\nR_TRAIN_MIN, R_TRAIN_MAX = 0.5, 2.8\nR_EVAL_MIN, R_EVAL_MAX = 0.5, 3.0\nLR = 1e-2\nT = 3000\nSEED = 42\n\ndef morse_potential(r):\n    \"\"\"Calculates the Morse potential energy.\"\"\"\n    return DE * (1 - np.exp(-A * (r - RE)))**2 - DE\n\ndef cutoff_function(r):\n    \"\"\"Calculates the smooth cutoff function value.\"\"\"\n    return np.where(r <= RC, 0.5 * (np.cos(np.pi * r / RC) + 1), 0)\n\ndef compute_G_matrix(r_vec):\n    \"\"\"Computes the matrix of symmetry function vectors for a vector of distances.\"\"\"\n    r_vec = r_vec.reshape(-1, 1)\n    fc_r = cutoff_function(r_vec)\n    G = fc_r * np.exp(-ETA * (r_vec - RS)**2)\n    return G\n\ndef initialize_parameters(rng):\n    \"\"\"Initializes NNP parameters according to the specification.\"\"\"\n    # Using HxM for W1 to match problem's math notation\n    W1 = rng.normal(0, 1 / np.sqrt(M), size=(H, M))\n    b1 = np.zeros((H, 1))\n    w2 = rng.normal(0, 1 / np.sqrt(H), size=(H, 1))\n    b2 = 0.0\n    return {\"W1\": W1, \"b1\": b1, \"w2\": w2, \"b2\": b2}\n\ndef forward_pass(G, params):\n    \"\"\"Performs a forward pass through the NNP.\"\"\"\n    # G is (N, M), W1 is (H, M) -> W1 @ G.T is (H, N)\n    Z1 = params[\"W1\"] @ G.T + params[\"b1\"]  # Shape (H, N)\n    A1 = np.tanh(Z1)  # Shape (H, N)\n    # A1.T is (N, H), w2 is (H, 1) -> A1.T @ w2 is (N, 1)\n    E_pred = A1.T @ params[\"w2\"] + params[\"b2\"]  # Shape (N, 1)\n    \n    # an_z1 contains intermediate values needed for backpropagation\n    cache = {\"Z1\": Z1, \"A1\": A1}\n    return E_pred, cache\n\ndef train_model(r_train, E_train):\n    \"\"\"Trains an NNP model using batch gradient descent.\"\"\"\n    rng = np.random.default_rng(SEED)\n    params = initialize_parameters(rng)\n    \n    N = len(r_train)\n    G_train = compute_G_matrix(r_train)\n    E_train_col = E_train.reshape(-1, 1)\n\n    for _ in range(T):\n        # Forward pass\n        E_pred, cache = forward_pass(G_train, params)\n        \n        # Backward pass (Gradient Calculation)\n        d_E_pred = (2 / N) * (E_pred - E_train_col)  # Shape (N, 1)\n        \n        # Gradients for output layer\n        d_b2 = np.sum(d_E_pred)\n        d_w2 = cache[\"A1\"] @ d_E_pred  # A1 is (H,N), d_E_pred is (N,1) -> (H,1)\n        \n        # Backpropagate to hidden layer\n        d_A1_T = d_E_pred @ params[\"w2\"].T # (N,1) @ (1,H) -> (N,H)\n        d_Z1_T = d_A1_T * (1 - cache[\"A1\"].T**2) # (N,H) * (N,H) -> (N,H)\n        \n        # Gradients for input layer\n        d_b1 = np.sum(d_Z1_T, axis=0, keepdims=True).T # (H,1)\n        d_W1 = d_Z1_T.T @ G_train # (H,N) @ (N,M) -> (H,M)\n        \n        # Update parameters\n        params[\"W1\"] -= LR * d_W1\n        params[\"b1\"] -= LR * d_b1\n        params[\"w2\"] -= LR * d_w2\n        params[\"b2\"] -= LR * d_b2\n        \n    return params\n\ndef poison_dataset(E_base, r_train, targets):\n    \"\"\"Modifies the energy labels based on poisoning targets.\"\"\"\n    E_poisoned = np.copy(E_base)\n    # Accumulate offsets for targets that map to the same point\n    modifications = {}\n    for r_star, delta in targets:\n        k = np.argmin(np.abs(r_train - r_star))\n        modifications[k] = modifications.get(k, 0) + delta\n    \n    for k, total_delta in modifications.items():\n        E_poisoned[k] += total_delta\n        \n    return E_poisoned\n    \ndef solve():\n    # 1. Setup grids and true data\n    r_train = np.linspace(R_TRAIN_MIN, R_TRAIN_MAX, N_TRAIN)\n    r_eval = np.linspace(R_EVAL_MIN, R_EVAL_MAX, N_EVAL)\n    dr_eval = (R_EVAL_MAX - R_EVAL_MIN) / (N_EVAL - 1)\n    E_true_train = morse_potential(r_train)\n    \n    # 2. Define poisoning scenarios\n    scenarios = [\n        {\"targets\": []},\n        {\"targets\": [(0.7414, 8.0)]},\n        {\"targets\": [(2.2, -4.0), (2.4, -4.0)]},\n        {\"targets\": [(0.6, -10.0)]},\n    ]\n\n    # 3. Train baseline model\n    params_base = train_model(r_train, E_true_train)\n    G_eval = compute_G_matrix(r_eval)\n    E_pred_base, _ = forward_pass(G_eval, params_base)\n    \n    # Pre-calculate baseline properties for evaluation\n    idx_min_base = np.argmin(E_pred_base)\n    r_min_base = r_eval[idx_min_base]\n    F_base = -np.gradient(E_pred_base.flatten(), dr_eval)\n\n    all_results = []\n\n    # 4. Handle all scenarios\n    for i, scen in enumerate(scenarios):\n        if i == 0:\n            # Scenario 1 is the baseline vs itself, so deviations are zero.\n            results = [0.0, 0.0, 0.0]\n        else:\n            # Create poisoned dataset\n            E_train_poisoned = poison_dataset(E_true_train, r_train, scen[\"targets\"])\n            \n            # Train model from scratch on poisoned data\n            params_poison = train_model(r_train, E_train_poisoned)\n            \n            # Evaluate on the fine grid\n            E_pred_poison, _ = forward_pass(G_eval, params_poison)\n            \n            # Compute metrics\n            m1 = np.sqrt(np.mean((E_pred_poison - E_pred_base)**2))\n            \n            idx_min_poison = np.argmin(E_pred_poison)\n            r_min_poison = r_eval[idx_min_poison]\n            m2 = np.abs(r_min_poison - r_min_base)\n\n            F_poison = -np.gradient(E_pred_poison.flatten(), dr_eval)\n            m3 = np.max(np.abs(F_poison - F_base))\n            \n            results = [m1, m2, m3]\n\n        all_results.append([round(x, 6) for x in results])\n\n    # 5. Format and print final output\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}