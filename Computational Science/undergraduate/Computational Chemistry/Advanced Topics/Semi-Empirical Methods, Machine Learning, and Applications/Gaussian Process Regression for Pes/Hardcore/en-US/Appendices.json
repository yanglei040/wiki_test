{
    "hands_on_practices": [
        {
            "introduction": "To truly understand Gaussian Process Regression, we must move from theory to practice. This first exercise provides a foundational experience by having you model a simple two-dimensional potential energy surface. By strategically placing training data, you will explore a crucial concept: the stark difference between interpolation (predicting within the bounds of your data) and extrapolation (predicting outside them). This practice  highlights why the placement of training points is paramount for accurately capturing important features like reaction barriers, demonstrating the data-driven nature of GPR models.",
            "id": "2455992",
            "problem": "You are given a two-dimensional potential energy surface (PES) defined in dimensionless units by the analytic function\n$$\nV(x,y) = (x^2 - 1)^2 + a\\, y^2,\n$$\nwith parameter $a = 0.5$. This surface has two minima near $(x,y) = (-1,0)$ and $(x,y) = (1,0)$ and a first-order saddle point at $(x,y) = (0,0)$ with energy $V(0,0) = 1$. Consider a zero-mean Gaussian Process (GP) prior for the PES values with a squared-exponential covariance kernel\n$$\nk(\\mathbf{r}, \\mathbf{r}') = \\sigma_f^2 \\exp\\!\\left(-\\tfrac{1}{2}\\sum_{i=1}^{2} \\frac{(r_i - r'_i)^2}{\\ell^2}\\right),\n$$\nwhere $\\mathbf{r} = (x,y)$, $\\ell$ is an isotropic length scale, and $\\sigma_f$ is the signal amplitude. Assume independent and identically distributed Gaussian observation noise with variance $\\sigma_n^2$ added to the energy values. The posterior predictive mean of the GP at a query point $\\mathbf{r}_\\star$ is defined by the standard Gaussian conditioning formula in terms of the kernel matrix on the training inputs, the cross-kernel vector between training inputs and $\\mathbf{r}_\\star$, and the observed energies.\n\nYour task is to implement a program that computes the GP posterior predictive mean at the saddle point $\\mathbf{r}_\\star = (0,0)$, using the exact PES above to generate noiseless training energies, under the following four test cases. All quantities are dimensionless. Use the fixed hyperparameters $\\sigma_f = 1$, and $\\sigma_n^2 = 10^{-12}$ unless otherwise specified. The length scale $\\ell$ is specified per test case below.\n\nDefine the PES parameter $a = 0.5$ exactly as above in all cases.\n\nTraining inputs must be used exactly as listed. For each training input $(x_j,y_j)$, compute the training target $V(x_j,y_j)$ using the given analytic expression.\n\nTest Case 1 (without near-saddle sampling):\n- Length scale: $\\ell = 0.4$.\n- Training inputs (list of $(x,y)$):\n  - $(-1, 0)$, $(1, 0)$,\n  - $(-1, 0.5)$, $(1, 0.5)$, $(-1, -0.5)$, $(1, -0.5)$,\n  - $(0.9, 0)$, $(-0.9, 0)$,\n  - $(-0.75, 0.8)$, $(0.75, -0.8)$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nTest Case 2 (with near-saddle sampling):\n- Length scale: $\\ell = 0.4$.\n- Training inputs: all points from Test Case $1$ plus the additional points $(0.1, 0)$, $(-0.1, 0)$, $(0, 0.1)$, $(0, -0.1)$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nTest Case 3 (boundary case: extremely short correlation without near-saddle sampling):\n- Length scale: $\\ell = 0.1$.\n- Training inputs: identical to Test Case $1$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nTest Case 4 (boundary case: exact observation at the saddle):\n- Length scale: $\\ell = 0.4$.\n- Training inputs: all points from Test Case $1$ plus the exact saddle point $(0,0)$.\n- Task: Predict the GP posterior mean at $(0,0)$.\n\nProgram input: There is no external input; all data are specified here. Program output: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Test Cases $1,2,3,4$. Each number must be the posterior predictive mean at $(0,0)$ for the respective test case, rounded to $6$ decimal places. For example: \"[m1,m2,m3,m4]\".\n\nYour program must compute all quantities from first principles using the definitions above. All energies must be computed in the specified dimensionless units, and all angles, if any appear, are not relevant here. The final outputs are floats. The four-test-case suite covers the general prediction without local data, the improved prediction with nearby data, the effect of very short correlation length without nearby data, and the limiting case when the query point is itself a training input.",
            "solution": "The problem statement has been subjected to rigorous validation and is found to be scientifically sound, well-posed, and free of contradictions. It presents a standard, well-defined exercise in the application of Gaussian Process regression for modeling a potential energy surface, a common task in computational chemistry. We shall therefore proceed with a complete, principled solution.\n\nThe objective is to compute the posterior predictive mean of a Gaussian Process (GP) model at a specific query point, the saddle point $\\mathbf{r}_\\star = (0,0)$ of a given two-dimensional potential energy surface (PES). The solution is derived from the fundamental principles of Bayesian inference applied to function spaces.\n\nThe PES is defined by the analytical function:\n$$\nV(\\mathbf{r}) = (x^2 - 1)^2 + a y^2\n$$\nwhere $\\mathbf{r} = (x,y)$ and the parameter $a$ is fixed at $a=0.5$. The true energy at the saddle point $\\mathbf{r}_\\star = (0,0)$ is $V(0,0) = (0^2-1)^2 + 0.5(0^2) = 1$.\n\nWe model the PES using a zero-mean Gaussian Process, $f(\\mathbf{r}) \\sim \\mathcal{GP}(0, k(\\mathbf{r}, \\mathbf{r}'))$. The covariance between the function values at any two points $\\mathbf{r}$ and $\\mathbf{r}'$ is given by the squared-exponential kernel:\n$$\nk(\\mathbf{r}, \\mathbf{r}') = \\sigma_f^2 \\exp\\!\\left(-\\frac{\\|\\mathbf{r} - \\mathbf{r}'\\|^2}{2\\ell^2}\\right)\n$$\nHere, $\\|\\mathbf{r} - \\mathbf{r}'\\|^2$ is the squared Euclidean distance, $\\sigma_f$ is the signal amplitude, and $\\ell$ is the characteristic length scale. The problem specifies $\\sigma_f = 1$.\n\nWe are given a set of $N$ training inputs $\\mathbf{X} = \\{\\mathbf{r}_1, \\ldots, \\mathbf{r}_N\\}$ and corresponding noiseless training targets $\\mathbf{y} = \\{y_1, \\ldots, y_N\\}$, where $y_j = V(\\mathbf{r}_j)$. Although the training targets are noiseless, the model includes an observation noise term with variance $\\sigma_n^2$, which is a standard technique to ensure numerical stability. The posterior distribution of the function value $f_\\star = f(\\mathbf{r}_\\star)$ at a test point $\\mathbf{r}_\\star$, conditioned on the training data $(\\mathbf{X}, \\mathbf{y})$, is also a Gaussian distribution. Its mean, which is our desired quantity, is given by the equation for GP regression:\n$$\n\\bar{f}_\\star = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}\n$$\nwhere:\n- $K$ is the $N \\times N$ Gram matrix of the training inputs, with elements $K_{ij} = k(\\mathbf{r}_i, \\mathbf{r}_j)$.\n- $\\mathbf{k}_\\star$ is the $N \\times 1$ vector of covariances between the training inputs and the test point, with elements $(\\mathbf{k}_\\star)_i = k(\\mathbf{r}_i, \\mathbf{r}_\\star)$.\n- $I$ is the $N \\times N$ identity matrix.\n- $\\sigma_n^2$ is the noise variance, specified as $10^{-12}$. This small value, often called a \"nugget,\" regularizes the matrix $K$, guaranteeing its invertibility.\n\nThe computational procedure is as follows:\n1.  For each test case, assemble the specified training inputs $\\mathbf{X}$ and the fixed hyperparameters $\\ell$, $\\sigma_f$, and $\\sigma_n^2$.\n2.  Compute the training targets $\\mathbf{y}$ by evaluating the function $V(\\mathbf{r})$ at each training input $\\mathbf{r}_j \\in \\mathbf{X}$.\n3.  Construct the kernel matrix $K$ by computing $k(\\mathbf{r}_i, \\mathbf{r}_j)$ for all pairs of training inputs.\n4.  Construct the cross-covariance vector $\\mathbf{k}_\\star$ by computing $k(\\mathbf{r}_i, \\mathbf{r}_\\star)$ for all training inputs $\\mathbf{r}_i$ with respect to the query point $\\mathbf{r}_\\star=(0,0)$.\n5.  Instead of explicit matrix inversion, which is numerically unstable, we solve the more stable linear system $(K + \\sigma_n^2 I) \\boldsymbol{\\alpha} = \\mathbf{y}$ for the weight vector $\\boldsymbol{\\alpha}$.\n6.  The posterior mean is then calculated as the inner product $\\bar{f}_\\star = \\mathbf{k}_\\star^T \\boldsymbol{\\alpha}$.\n\nThis procedure will be applied to each of the four specified test cases, which are designed to probe different aspects of GP regression:\n- **Test Case 1** establishes a baseline prediction where training data are relatively distant from the query point. The prediction will be a weighted average of the training targets, with weights decaying with distance, pulled towards the prior mean of $0$.\n- **Test Case 2** demonstrates the effect of adding local data near the query point. This new information is expected to dominate the prediction, driving the posterior mean much closer to the true value of $V(0,0) = 1$.\n- **Test Case 3** investigates the impact of a very short correlation length $\\ell$. This makes the model assume the function varies rapidly, so the influence of distant training points diminishes sharply. The prediction at $(0,0)$ will revert more strongly towards the prior mean of $0$.\n- **Test Case 4** confirms the interpolation behavior of the GP. By including the query point $(0,0)$ in the training set, the model is constrained to pass very close to the observed value $V(0,0)=1$, with any deviation being a consequence of the small regularization term $\\sigma_n^2$. The result should be extremely close to $1$.\n\nThe implementation will strictly follow this validated methodology.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian Process Regression problem for four test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    # PES analytic function V(x,y)\n    a_param = 0.5\n    def v_pes(r):\n        x, y = r\n        return (x**2 - 1)**2 + a_param * y**2\n\n    # Squared-exponential kernel function k(r, r')\n    def kernel(r1, r2, sigma_f, ell):\n        dist_sq = np.sum((np.array(r1) - np.array(r2))**2)\n        return sigma_f**2 * np.exp(-dist_sq / (2 * ell**2))\n\n    # --- Fixed Parameters ---\n    sigma_f_val = 1.0\n    sigma_n_sq_val = 1e-12\n    r_star = np.array([0.0, 0.0])\n\n    # --- Test Case Definitions ---\n    train_inputs_case1 = [\n        (-1.0, 0.0), (1.0, 0.0),\n        (-1.0, 0.5), (1.0, 0.5), (-1.0, -0.5), (1.0, -0.5),\n        (0.9, 0.0), (-0.9, 0.0),\n        (-0.75, 0.8), (0.75, -0.8)\n    ]\n\n    train_inputs_case2 = train_inputs_case1 + [\n        (0.1, 0.0), (-0.1, 0.0), (0.0, 0.1), (0.0, -0.1)\n    ]\n\n    train_inputs_case4 = train_inputs_case1 + [(0.0, 0.0)]\n\n    test_cases = [\n        {\"ell\": 0.4, \"train_inputs\": train_inputs_case1},\n        {\"ell\": 0.4, \"train_inputs\": train_inputs_case2},\n        {\"ell\": 0.1, \"train_inputs\": train_inputs_case1},\n        {\"ell\": 0.4, \"train_inputs\": train_inputs_case4},\n    ]\n\n    # --- Main Calculation Loop ---\n    results = []\n    for case in test_cases:\n        ell_val = case[\"ell\"]\n        train_inputs = np.array(case[\"train_inputs\"])\n        n_train = len(train_inputs)\n\n        # 1. Generate training targets y\n        y_train = np.array([v_pes(r) for r in train_inputs])\n\n        # 2. Construct kernel matrix K\n        K = np.zeros((n_train, n_train))\n        for i in range(n_train):\n            for j in range(n_train):\n                K[i, j] = kernel(train_inputs[i], train_inputs[j], sigma_f_val, ell_val)\n        \n        # 3. Add regularization term\n        K_reg = K + sigma_n_sq_val * np.identity(n_train)\n        \n        # 4. Construct cross-kernel vector k_star\n        k_star = np.array([kernel(r, r_star, sigma_f_val, ell_val) for r in train_inputs])\n        \n        # 5. Solve for weights alpha\n        alpha = np.linalg.solve(K_reg, y_train)\n        \n        # 6. Compute posterior mean\n        mean_prediction = np.dot(k_star, alpha)\n        \n        results.append(mean_prediction)\n\n    # Round results to 6 decimal places for the final output\n    rounded_results = [round(res, 6) for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, rounded_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the fundamentals, we now advance to a more chemically realistic scenario. A molecule's potential energy surface is shaped by various types of internal motions—such as bond stretches and dihedral rotations—each with distinct physical characteristics. In this exercise , you will construct a 'composite kernel,' a powerful GPR technique that combines specialized kernel functions to honor the different physics of these motions, allowing for a more accurate and flexible PES model.",
            "id": "2456000",
            "problem": "You are modeling a reduced potential energy surface for a molecular fragment described by two bond stretches and one dihedral angle. Let each configuration be represented by the descriptor vector $\\mathbf{x} = (r_1, r_2, \\phi)$, where $r_1$ and $r_2$ are bond lengths in $\\text{\\AA}$, and $\\phi$ is a dihedral angle in radians. You will assume a Gaussian process prior with zero mean and a composite kernel that is the sum of a squared-exponential term over the bond stretches and a periodic term over the dihedral angle:\n$$\nk(\\mathbf{x},\\mathbf{x}') \\equiv k_{\\mathrm{SE}}(\\mathbf{x},\\mathbf{x}') + k_{\\mathrm{Per}}(\\mathbf{x},\\mathbf{x}'),\n$$\nwhere\n$$\nk_{\\mathrm{SE}}(\\mathbf{x},\\mathbf{x}') = \\sigma_s^2 \\exp\\left(-\\tfrac{1}{2}\\sum_{i=1}^{2}\\left(\\frac{r_i - r_i'}{\\ell_s}\\right)^2\\right),\n$$\nand\n$$\nk_{\\mathrm{Per}}(\\mathbf{x},\\mathbf{x}') = \\sigma_p^2 \\exp\\left(-\\frac{2\\sin^2\\left(\\tfrac{\\phi - \\phi'}{2}\\right)}{\\ell_p^2}\\right).\n$$\nUse the hyperparameters $\\sigma_s = 2.5$ $\\text{eV}$, $\\ell_s = 0.2$ $\\text{\\AA}$, $\\sigma_p = 0.4$ $\\text{eV}$, and $\\ell_p = 0.5$ (dimensionless in radians). Assume independent and identically distributed observation noise with variance $\\sigma_n^2$, where $\\sigma_n = 0.02$ $\\text{eV}$.\n\nThe observed training energies are generated from a physically motivated reference energy function composed of two identical Morse bond terms and a torsional term:\n$$\nE_{\\mathrm{ref}}(r_1,r_2,\\phi) = \\sum_{i=1}^{2}\\left[D\\left(1 - e^{-a(r_i - r_0)}\\right)^2 - D\\right] + V_0\\left(1 - \\cos(n\\phi)\\right),\n$$\nwith parameters $D = 4.5$ $\\text{eV}$, $a = 1.7$ $\\text{\\AA}^{-1}$, $r_0 = 1.09$ $\\text{\\AA}$, $V_0 = 0.18$ $\\text{eV}$, and $n = 3$. The training inputs $\\{\\mathbf{x}_j\\}_{j=1}^{N}$ and corresponding outputs $\\{y_j\\}_{j=1}^{N}$ are defined by $y_j = E_{\\mathrm{ref}}(\\mathbf{x}_j)$ at the following $N = 5$ configurations (units: $r$ in $\\text{\\AA}$, $\\phi$ in radians):\n- $\\mathbf{x}_1 = (r_0, r_0, 0.0)$,\n- $\\mathbf{x}_2 = (r_0 + 0.1, r_0, 0.5)$,\n- $\\mathbf{x}_3 = (r_0, r_0 + 0.15, 1.0)$,\n- $\\mathbf{x}_4 = (r_0 + 0.2, r_0 + 0.2, 2.0)$,\n- $\\mathbf{x}_5 = (r_0 - 0.1, r_0, 3.0)$.\n\nAssume a Gaussian process prior with zero mean and covariance function $k(\\cdot,\\cdot)$ as above, and a likelihood with additive Gaussian noise of variance $\\sigma_n^2$. Let the test inputs be the following $M = 4$ configurations (units: $r$ in $\\text{\\AA}$, $\\phi$ in radians):\n- $\\mathbf{x}_1^\\star = (r_0, r_0, 0.0)$,\n- $\\mathbf{x}_2^\\star = (r_0, r_0, \\pi)$,\n- $\\mathbf{x}_3^\\star = (r_0 + 0.3, r_0, -\\pi)$,\n- $\\mathbf{x}_4^\\star = (r_0 + 0.05, r_0 + 0.05, 1.0)$.\n\nCompute the posterior predictive mean of the Gaussian process at each test input, defined as the conditional expectation of the predicted energies given the training data under the joint multivariate normal model with the specified kernel and noise variance. Express all predicted energies in $\\text{eV}$. No angles in degrees are allowed; all angles must be in radians.\n\nYour program must implement the above specification exactly, using the stated hyperparameters and training data generated from $E_{\\mathrm{ref}}$. The test suite consists of the $M = 4$ test configurations listed above and covers the following cases: a point coinciding with a training configuration, a maximally staggered dihedral at $\\phi = \\pi$, a periodic wrap at $\\phi = -\\pi$, and a small symmetric bond stretch with a moderate dihedral. The required outputs are the $M = 4$ posterior mean predictions as real numbers in $\\text{eV}$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ decimal places, for example, $[x_1,x_2,x_3,x_4]$ where each $x_i$ is a decimal numeral representing an energy in $\\text{eV}$.",
            "solution": "The problem as stated is a well-defined and standard application of Gaussian process regression (GPR) to model a molecular potential energy surface. It is scientifically sound, mathematically consistent, and contains all necessary information for a unique solution. We shall proceed with the derivation.\n\nThe fundamental task is to compute the posterior predictive mean of a Gaussian process (GP) at a set of test points, given a set of training data. A GP defines a distribution over functions, and in this context, we model the potential energy $E$ as a function of the molecular configuration $\\mathbf{x}$, such that $E(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))$.\n\nThe problem specifies a zero mean function, $m(\\mathbf{x}) = 0$, and a composite covariance function (kernel) $k(\\mathbf{x}, \\mathbf{x}')$. The observations, denoted by $y$, are assumed to be noisy measurements of the true energy function $E(\\mathbf{x})$, such that $y = E(\\mathbf{x}) + \\epsilon$, where the noise $\\epsilon$ is drawn from an independent, identically distributed Gaussian distribution with zero mean and variance $\\sigma_n^2$, i.e., $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nLet the training data be a set of $N$ configurations $X = \\{\\mathbf{x}_j\\}_{j=1}^{N}$ and their corresponding observed energies $\\mathbf{y} = \\{y_j\\}_{j=1}^{N}$. Let the test data be a set of $M$ configurations $X_* = \\{\\mathbf{x}_i^\\star\\}_{i=1}^{M}$ for which we wish to predict the energies $\\mathbf{E}_* = \\{E(\\mathbf{x}_i^\\star)\\}_{i=1}^M$.\n\nUnder the GP model, the joint distribution of the observed training outputs $\\mathbf{y}$ and the latent function values at the test points $\\mathbf{E}_*$ is a multivariate Gaussian:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{E}_* \\end{pmatrix} \\sim \\mathcal{N}\\left(\n\\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix},\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I & K(X, X_*) \\\\\nK(X_*, X) & K(X_*, X_*)\n\\end{pmatrix}\n\\right)\n$$\nHere, $K(X, X)$ is the $N \\times N$ matrix of covariances evaluated at all pairs of training points, with entries $(K(X, X))_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$. Similarly, $K(X_*, X)$ is the $M \\times N$ matrix with entries $(K(X_*, X))_{ij} = k(\\mathbf{x}_i^\\star, \\mathbf{x}_j)$, and $K(X_*, X_*) $ is the $M \\times M$ matrix of covariances at test points. $I$ is the $N \\times N$ identity matrix.\n\nThe posterior predictive distribution $p(\\mathbf{E}_* | X, \\mathbf{y}, X_*)$ is also Gaussian. Its mean, which is the desired quantity, is given by the standard formula for conditional Gaussian distributions:\n$$\n\\bar{\\mathbf{E}}_* = \\mathbb{E}[\\mathbf{E}_* | X, \\mathbf{y}, X_*] = K(X_*, X) \\left[K(X, X) + \\sigma_n^2 I\\right]^{-1} \\mathbf{y}\n$$\nThe solution involves three main steps:\n\n1.  **Generate Training Data:** The training outputs $\\mathbf{y}$ are not given directly but must be computed using the reference energy function $E_{\\mathrm{ref}}(r_1, r_2, \\phi)$ for each training input $\\mathbf{x}_j$.\n    The function is:\n    $$\n    E_{\\mathrm{ref}}(r_1,r_2,\\phi) = \\sum_{i=1}^{2}\\left[D\\left(1 - e^{-a(r_i - r_0)}\\right)^2 - D\\right] + V_0\\left(1 - \\cos(n\\phi)\\right)\n    $$\n    with parameters $D = 4.5\\ \\text{eV}$, $a = 1.7\\ \\text{\\AA}^{-1}$, $r_0 = 1.09\\ \\text{\\AA}$, $V_0 = 0.18\\ \\text{eV}$, and $n=3$. We compute $y_j = E_{\\mathrm{ref}}(\\mathbf{x}_j)$ for the $N=5$ given training configurations.\n\n2.  **Construct Kernel Matrices:** We must construct the covariance matrices $K(X, X)$ and $K(X_*, X)$. The kernel is a sum of a squared-exponential term for the two bond lengths $(r_1, r_2)$ and a periodic term for the dihedral angle $\\phi$:\n    $$\n    k(\\mathbf{x}, \\mathbf{x}') = \\sigma_s^2 \\exp\\left(-\\frac{1}{2\\ell_s^2}\\sum_{i=1}^{2}(r_i - r_i')^2\\right) + \\sigma_p^2 \\exp\\left(-\\frac{2\\sin^2\\left(\\tfrac{\\phi - \\phi'}{2}\\right)}{\\ell_p^2}\\right)\n    $$\n    The hyperparameters are given as $\\sigma_s = 2.5\\ \\text{eV}$, $\\ell_s = 0.2\\ \\text{\\AA}$, $\\sigma_p = 0.4\\ \\text{eV}$, and $\\ell_p = 0.5$. The noise level is $\\sigma_n = 0.02\\ \\text{eV}$, so the noise variance is $\\sigma_n^2 = 0.0004\\ \\text{eV}^2$. We compute the $5 \\times 5$ matrix $K(X, X)$ and the $4 \\times 5$ matrix $K(X_*, X)$.\n\n3.  **Compute the Posterior Mean:** The final calculation for the predictive mean $\\bar{\\mathbf{E}}_*$ is performed. For numerical stability, we do not explicitly invert the matrix $\\left[K(X, X) + \\sigma_n^2 I\\right]$. Instead, we first solve the linear system of equations:\n    $$\n    \\left[K(X, X) + \\sigma_n^2 I\\right] \\boldsymbol{\\alpha} = \\mathbf{y}\n    $$\n    for the weight vector $\\boldsymbol{\\alpha}$. The posterior mean is then obtained by a matrix-vector product:\n    $$\n    \\bar{\\mathbf{E}}_* = K(X_*, X) \\boldsymbol{\\alpha}\n    $$\nThis procedure is implemented exactly as described to find the GPR-predicted energies for the $M=4$ test configurations. All calculations are performed using floating-point arithmetic with angles in radians.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Gaussian process regression problem for a potential energy surface.\n    \"\"\"\n\n    # --- Define Constants and Hyperparameters ---\n    \n    # Reference energy function parameters\n    D = 4.5  # eV\n    a = 1.7  # 1/Angstrom\n    r0 = 1.09 # Angstrom\n    V0 = 0.18 # eV\n    n = 3    # dimensionless\n\n    # GPR kernel hyperparameters\n    sigma_s = 2.5  # eV\n    ell_s = 0.2    # Angstrom\n    sigma_p = 0.4  # eV\n    ell_p = 0.5    # dimensionless (for radians)\n    \n    # Observation noise\n    sigma_n = 0.02 # eV\n\n    # --- Define Training and Test Data ---\n\n    # Training configurations (N=5)\n    train_x = np.array([\n        [r0, r0, 0.0],\n        [r0 + 0.1, r0, 0.5],\n        [r0, r0 + 0.15, 1.0],\n        [r0 + 0.2, r0 + 0.2, 2.0],\n        [r0 - 0.1, r0, 3.0]\n    ])\n\n    # Test configurations (M=4)\n    test_x = np.array([\n        [r0, r0, 0.0],\n        [r0, r0, np.pi],\n        [r0 + 0.3, r0, -np.pi],\n        [r0 + 0.05, r0 + 0.05, 1.0]\n    ])\n\n    # --- Step 1: Generate Training Outputs (y) ---\n\n    def E_ref(r1, r2, phi):\n        \"\"\"Computes the reference energy.\"\"\"\n        morse_term = D * (1 - np.exp(-a * (r1 - r0)))**2 - D + \\\n                     D * (1 - np.exp(-a * (r2 - r0)))**2 - D\n        torsional_term = V0 * (1 - np.cos(n * phi))\n        return morse_term + torsional_term\n\n    train_y = np.array([E_ref(x[0], x[1], x[2]) for x in train_x])\n\n    # --- Step 2: Construct Kernel Matrices ---\n\n    def kernel(x1, x2):\n        \"\"\"Computes the composite kernel value k(x1, x2).\"\"\"\n        r1_1, r2_1, phi_1 = x1\n        r1_2, r2_2, phi_2 = x2\n\n        # Squared-exponential kernel for bond stretches\n        sq_dist_r = (r1_1 - r1_2)**2 + (r2_1 - r2_2)**2\n        k_se = (sigma_s**2) * np.exp(-0.5 * sq_dist_r / (ell_s**2))\n\n        # Periodic kernel for dihedral angle\n        d_phi = phi_1 - phi_2\n        k_per = (sigma_p**2) * np.exp(-2.0 * (np.sin(d_phi / 2.0)**2) / (ell_p**2))\n        \n        return k_se + k_per\n    \n    N = len(train_x)\n    M = len(test_x)\n\n    # K(X, X): training data covariance matrix (N x N)\n    K_XX = np.zeros((N, N))\n    for i in range(N):\n        for j in range(N):\n            K_XX[i, j] = kernel(train_x[i], train_x[j])\n\n    # K(X_*, X): test-training data covariance matrix (M x N)\n    K_starX = np.zeros((M, N))\n    for i in range(M):\n        for j in range(N):\n            K_starX[i, j] = kernel(test_x[i], train_x[j])\n\n    # --- Step 3: Compute the Posterior Mean ---\n    \n    # Add noise variance to the diagonal of K(X, X)\n    K_y = K_XX + (sigma_n**2) * np.eye(N)\n    \n    # Solve the linear system for alpha: K_y * alpha = y\n    try:\n        alpha = np.linalg.solve(K_y, train_y)\n    except np.linalg.LinAlgError:\n        # Fallback to pseudoinverse if solve fails, though not expected here\n        alpha = np.linalg.pinv(K_y) @ train_y\n\n    # Compute posterior predictive mean: E_star = K(X_*, X) * alpha\n    E_star_mean = K_starX @ alpha\n    \n    # --- Final Output Formatting ---\n    \n    results = [f\"{val:.6f}\" for val in E_star_mean]\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After learning to build GPR models, a practitioner must also learn to critique them and understand their limitations. This final practice is a thought experiment that challenges your intuition about a model's 'domain of applicability.' You will reason about the expected performance of a GPR model trained on one chemical isomer when asked to predict the energy of a different one . This exercise solidifies the key lesson that GPR models are powerful interpolators whose predictive power is fundamentally tied to the coverage of the training data in feature space.",
            "id": "2455968",
            "problem": "A Gaussian process regression (GPR) model is trained to approximate the potential energy surface (PES) mapping nuclear coordinates $\\mathbf{x}$ to total electronic energy $E(\\mathbf{x})$. The model uses a rotation-, translation-, and permutation-invariant descriptor $\\phi(\\mathbf{x})$ and a stationary, smooth kernel $k\\!\\left(\\phi(\\mathbf{x}),\\phi(\\mathbf{x}')\\right)$. The prior mean is $m(\\mathbf{x}) = 0$. The training set consists exclusively of geometries sampled around low-energy conformations of the linear isomer (butane) of $\\mathrm{C}_4\\mathrm{H}_{10}$. No geometries of the branched isomer (isobutane) are included in training. How would this GPR model be expected to perform when predicting $E(\\mathbf{x}_*)$ for geometries $\\mathbf{x}_*$ belonging to isobutane?\n\nA. It will generally be unreliable: because isobutane geometries lie far in descriptor space from the butane training set, the kernel similarities are small, the predictive mean tends toward the prior mean with large predictive variance, and errors can be large.\n\nB. It will be accurate: because both molecules have the same molecular formula $\\mathrm{C}_4\\mathrm{H}_{10}$, the PES is essentially the same, so the model will interpolate well.\n\nC. It will be accurate up to an additive constant: a stationary kernel guarantees that predictions on isobutane differ from the true energies only by a constant energy shift.\n\nD. It will be accurate provided a linear kernel is used: linear models extrapolate better than nonlinear kernels, so the lack of isobutane data is not a problem.",
            "solution": "The user wants me to act as a strict Russian professor and solve a problem. Here is my thinking process. First, I will validate the problem statement. The problem describes a standard application of Gaussian process regression (GPR) in computational chemistry: fitting a potential energy surface (PES).\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Model: A Gaussian process regression (GPR) model.\n- Task: To approximate the potential energy surface (PES), which is a mapping from nuclear coordinates $\\mathbf{x}$ to total electronic energy $E(\\mathbf{x})$.\n- Input Feature: A rotation-, translation-, and permutation-invariant descriptor $\\phi(\\mathbf{x})$.\n- Kernel Function: A stationary, smooth kernel $k(\\phi(\\mathbf{x}), \\phi(\\mathbf{x}'))$.\n- Prior Mean Function: $m(\\mathbf{x}) = 0$.\n- Training Data: A set of geometries sampled exclusively around low-energy conformations of the linear isomer of $\\mathrm{C}_4\\mathrm{H}_{10}$ (butane).\n- Excluded Data: No geometries of the branched isomer of $\\mathrm{C}_4\\mathrm{H}_{10}$ (isobutane) are included in the training set.\n- Question: What is the expected performance of this GPR model when predicting the energy $E(\\mathbf{x}_*)$ for a geometry $\\mathbf{x}_*$ belonging to isobutane?\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem setup is entirely grounded in established principles of computational chemistry and machine learning. Using GPR to model PESs is a common and valid technique. The distinction between isomers (butane and isobutane) and the use of invariant descriptors are standard concepts. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-posed. It asks for the expected behavior of a GPR model in a specific extrapolation scenario. The principles of GPR provide a clear framework for determining this behavior. A unique, meaningful conclusion can be derived.\n- **Objective:** The problem statement is objective and uses precise, unambiguous technical language. There are no subjective or opinion-based claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, describing a realistic scenario in the application of machine learning to chemistry. I will proceed with the solution.\n\n**Derivation of Solution**\n\nA Gaussian process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP is fully specified by a mean function $m(\\mathbf{x})$ and a covariance or kernel function $k(\\mathbf{x}, \\mathbf{x}')$. In this problem, the inputs to the GP are the descriptors $\\phi(\\mathbf{x})$.\n\nThe prior distribution over the function $f(\\phi)$ is given by:\n$$f(\\phi) \\sim \\mathcal{GP}(m(\\phi), k(\\phi, \\phi'))$$\nGiven the training data $\\{(\\phi_i, y_i)\\}_{i=1}^N$, where $\\phi_i = \\phi(\\mathbf{x}_i)$ are the descriptors of the training geometries and $y_i = E(\\mathbf{x}_i)$ are their corresponding energies, the GPR model computes the posterior distribution for a new test point $\\phi_* = \\phi(\\mathbf{x}_*)$.\n\nThe predictive mean $\\mu_*$ and predictive variance $\\sigma_*^2$ at the test point $\\phi_*$ are given by the standard GPR equations. Assuming the prior mean is $m(\\phi)=0$ as stated, and allowing for some observation noise with variance $\\sigma_n^2$:\n$$ \\mu_* = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y} $$\n$$ \\sigma_*^2 = k(\\phi_*, \\phi_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_* $$\nHere, $\\mathbf{K}$ is the $N \\times N$ kernel matrix of the training points, with entries $K_{ij} = k(\\phi_i, \\phi_j)$. The vector $\\mathbf{k}_*$ contains the kernel evaluations between the test point and each training point, with entries $k_{*i} = k(\\phi_*, \\phi_i)$. The vector $\\mathbf{y}$ contains the training target energies.\n\nThe central issue is the relationship between the test point descriptor $\\phi_*$ for isobutane and the training set descriptors $\\{\\phi_i\\}$ for butane. Butane and isobutane are structural isomers. They have the same chemical formula, $\\mathrm{C}_4\\mathrm{H}_{10}$, but different atomic connectivity and three-dimensional structures. A descriptor $\\phi(\\mathbf{x})$ that is invariant to rotation, translation, and permutation of identical atoms must still be able to distinguish between different isomers. Therefore, the region of descriptor space corresponding to isobutane conformations will be distinct and distant from the region corresponding to butane conformations. The model is being asked to perform extrapolation far from its training data.\n\nThe problem states the kernel $k$ is stationary and smooth. A typical example is the squared exponential (or RBF) kernel:\n$$ k(\\phi_i, \\phi_j) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2} ||\\phi_i - \\phi_j||^2\\right) $$\nwhere $\\sigma_f^2$ is the signal variance and $l$ is the length scale. For stationary kernels like this one, the kernel value decreases as the distance between inputs increases. When the test point $\\phi_*$ is far from all training points $\\{\\phi_i\\}$, the distance $||\\phi_* - \\phi_i||$ is large for all $i = 1, \\dots, N$. Consequently, the kernel values $k(\\phi_*, \\phi_i)$ will be very close to zero. The vector $\\mathbf{k}_*$ will approach the zero vector: $\\mathbf{k}_* \\to \\mathbf{0}$.\n\nLet us analyze the predictive equations in this limit:\n1.  **Predictive Mean:** As $\\mathbf{k}_* \\to \\mathbf{0}$, the term $\\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}$ also goes to zero. Thus, the predictive mean reverts to the prior mean:\n    $$ \\mu_* \\to m(\\phi_*) = 0 $$\n2.  **Predictive Variance:** As $\\mathbf{k}_* \\to \\mathbf{0}$, the second term in the variance equation, $\\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*$, goes to zero. The predictive variance reverts to the prior variance:\n    $$ \\sigma_*^2 \\to k(\\phi_*, \\phi_*) $$\n    For a stationary kernel, $k(\\phi_*, \\phi_*) = k(\\mathbf{0})$, which is the prior variance of the function, $\\sigma_f^2$. This is the maximum possible predictive variance.\n\nIn summary, when predicting for isobutane, the GPR model, trained only on butane, will predict an energy close to the prior mean of $0$ and report a very high uncertainty (large predictive variance). Since the true electronic energy of isobutane is a large negative number (relative to separated atoms), the prediction will be highly inaccurate. The large variance is the model's way of indicating its lack of knowledge in this uncharted region of the descriptor space.\n\n**Option-by-Option Analysis**\n\n**A. It will generally be unreliable: because isobutane geometries lie far in descriptor space from the butane training set, the kernel similarities are small, the predictive mean tends toward the prior mean with large predictive variance, and errors can be large.**\nThis statement correctly identifies that isobutane is distinct from butane in descriptor space. It correctly deduces that this leads to small kernel similarities. It correctly states the consequences for GPR: the mean prediction reverts to the prior mean (here, $0$) and the predictive variance becomes large (reverting to the prior variance). This combination leads to unreliable predictions and large errors. This analysis is fully consistent with the principles of GPR.\n**Verdict: Correct**\n\n**B. It will be accurate: because both molecules have the same molecular formula $\\mathrm{C}_4\\mathrm{H}_{10}$, the PES is essentially the same, so the model will interpolate well.**\nThis statement contains a fundamental scientific error. Structural isomers like butane and isobutane have profoundly different potential energy surfaces. They are distinct chemical species with different connectivities, bond lengths, angles, and energies. A model trained on one isomer's local PES cannot simply \"interpolate\" to the other. The task is one of extrapolation, not interpolation.\n**Verdict: Incorrect**\n\n**C. It will be accurate up to an additive constant: a stationary kernel guarantees that predictions on isobutane differ from the true energies only by a constant energy shift.**\nThis is a misinterpretation of the property of a stationary kernel. A stationary kernel, $k(\\phi, \\phi') = k(\\phi - \\phi')$, means the GP's statistical properties are invariant to translation in the input (descriptor) space. It does not imply anything about the predictive accuracy in a region far from training data. As shown in the derivation, the prediction reverts to the prior, not to the true energy shifted by a constant.\n**Verdict: Incorrect**\n\n**D. It will be accurate provided a linear kernel is used: linear models extrapolate better than nonlinear kernels, so the lack of isobutane data is not a problem.**\nThis statement is flawed for multiple reasons. First, a linear kernel $k(\\phi, \\phi') = \\phi^T \\phi'$ is not stationary, which contradicts the problem statement. Second, the claim that linear models extrapolate \"better\" is a dangerous oversimplification. A linear model will extrapolate linearly, which is only appropriate if the underlying function is truly linear. Since the global PES of $\\mathrm{C}_4\\mathrm{H}_{10}$ is highly complex and non-linear, a linear extrapolation from the butane region is almost certain to be catastrophically wrong for the isobutane region. A non-linear GPR with a decaying kernel correctly reports its high uncertainty during extrapolation, which is a more \"honest\" and desirable behavior than making a confident but wrong linear prediction.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}