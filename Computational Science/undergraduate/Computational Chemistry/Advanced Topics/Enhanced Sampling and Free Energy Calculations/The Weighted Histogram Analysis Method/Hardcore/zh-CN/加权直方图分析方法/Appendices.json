{
    "hands_on_practices": [
        {
            "introduction": "本练习旨在解决任何大规模计算研究中至关重要的第一步：资源预估。在启动复杂的二维WHAM计算之前，必须估算其内存需求，以确保任务能够在可用硬件上运行。本练习将指导您一步步计算所需的随机存取存储器（RAM），帮助您深入理解采样窗口数量和网格分辨率等模拟参数如何影响计算成本。",
            "id": "2465741",
            "problem": "在二维加权直方图分析方法 (WHAM) 中，研究者结合来自多个伞形采样窗口的有偏直方图，以在反应坐标的二维网格上估计自由能面。考虑一个科学上真实的二维WHAM实现，在每次自洽迭代期间，它会同时在随机存取存储器 (RAM) 中存储以下数组：\n\n- 对于每个索引为 $i \\in \\{1,\\dots,K\\}$ 的窗口，在 $M_x \\times M_y$ 个箱 $((j,k) \\in \\{1,\\dots,M_x\\}\\times\\{1,\\dots,M_y\\})$ 的笛卡尔网格上有一个分箱计数直方图 $n_i(x_j,y_k)$。每个计数存储为 $8$ 字节整数。\n- 对于每个窗口 $i$，一个在每个网格箱上评估的预计算折合偏置势数组 $b_i(x_j,y_k)$，存储为 $8$ 字节浮点数。\n- 网格上折合概率密度 $P(x_j,y_k)$ 的当前估计值，存储为 $8$ 字节浮点数。\n- 一个工作数组 $D(x_j,y_k)$，用于在网格上累积多直方图分母，存储为 $8$ 字节浮点数。\n- 网格上密度估计值的一个临时副本 $P^{\\text{new}}(x_j,y_k)$，存储为 $8$ 字节浮点数。\n- 每个窗口的自由能偏移量 $f_i$，存储为 $8$ 字节浮点数。\n- 每个窗口的总样本数 $N_i$，存储为 $8$ 字节整数（每个窗口一个）。\n- 沿每个轴的箱中心的网格坐标数组，$x_j$（对于 $j=1,\\dots,M_x$）和 $y_k$（对于 $k=1,\\dots,M_y$），每个都存储为 $8$ 字节浮点数。\n- 对于简谐伞形偏置，保留每个窗口的参数 $(k_{x,i},k_{y,i},x_{0,i},y_{0,i})$；假设 $k_{x,i}$ 和 $k_{y,i}$ 合并为单个有效标量 $k_i$，因此每个窗口恰好存储三个 $8$ 字节浮点值 $(k_i,x_{0,i},y_{0,i})$。\n\n假设有 $K=100$ 个窗口和一个 $M_x \\times M_y = 500 \\times 500$ 的网格。令 $M = M_x M_y$。使用以下基本事实：一个数组所需的内存等于其存储的元素数量乘以每个元素的字节数，并且独立数组的内存需求以字节为单位线性相加。定义 $1\\,\\text{GB}$ 为 $1\\times 10^{9}$ 字节。\n\n从这些基础和定义出发，推导并计算同时容纳上述所有数组所需的总RAM。以吉字节 (GB) 为单位表示您的最终答案，并将结果四舍五入到三位有效数字。",
            "solution": "该问题要求计算一个二维加权直方图分析方法 (2D-WHAM) 实现中，同时存储所有指定数据数组所需的总随机存取存储器 (RAM) 容量。计算必须基于所提供的参数和定义。该问题陈述经证实具有科学依据、是良定的且客观的。它包含了所有必要信息，没有矛盾之处。\n\n首先，我们定义符号及其给定值：\n伞形采样窗口的数量为 $K=100$。\n沿两个反应坐标的网格箱数量分别为 $M_x=500$ 和 $M_y=500$。\n二维网格上的总箱数为 $M = M_x \\times M_y$。\n每个存储的数据元素（整数或浮点数）的大小为 $B=8$ 字节。\n吉字节的转换因子为 $1\\,\\text{GB} = 1 \\times 10^9$ 字节。\n\n我们接着计算问题陈述中列出的每个数据结构所需的内存。总内存 $S_{\\text{total}}$ 是每个组件所需内存的总和。\n\n1.  计数直方图，$n_i(x_j,y_k)$：有 $K$ 个窗口，每个窗口有一个跨越 $M$ 个网格箱的直方图。每个计数为 $B$ 字节。所需内存为 $S_1 = K \\times M \\times B$。\n\n2.  偏置势数组，$b_i(x_j,y_k)$：与直方图类似，有 $K$ 个数组，每个数组大小为 $M$，元素大小为 $B$。所需内存为 $S_2 = K \\times M \\times B$。\n\n3.  概率密度，$P(x_j,y_k)$：这是一个覆盖 $M$ 个网格箱的单一数组。所需内存为 $S_3 = M \\times B$。\n\n4.  分母数组，$D(x_j,y_k)$：这也是一个大小为 $M$ 的单一数组。所需内存为 $S_4 = M \\times B$。\n\n5.  新的密度估计，$P^{\\text{new}}(x_j,y_k)$：这是第三个大小为 $M$ 的单一数组。所需内存为 $S_5 = M \\times B$。\n\n6.  自由能偏移量，$f_i$：$K$ 个窗口中的每一个都有一个偏移量。所需内存为 $S_6 = K \\times B$。\n\n7.  总样本数，$N_i$：$K$ 个窗口中的每一个都有一个计数值。所需内存为 $S_7 = K \\times B$。\n\n8.  网格坐标数组，$x_j$：此数组存储沿x轴的 $M_x$ 个箱的坐标。所需内存为 $S_8 = M_x \\times B$。\n\n9.  网格坐标数组，$y_k$：此数组存储沿y轴的 $M_y$ 个箱的坐标。所需内存为 $S_9 = M_y \\times B$。\n\n10. 简谐偏置参数，$(k_i,x_{0,i},y_{0,i})$：对于 $K$ 个窗口中的每一个，存储 $3$ 个参数。所需内存为 $S_{10} = 3 \\times K \\times B$。\n\n以字节为单位的总内存 $S_{\\text{total}}$ 是这些单独存储需求的总和：\n$$S_{\\text{total}} = S_1 + S_2 + S_3 + S_4 + S_5 + S_6 + S_7 + S_8 + S_9 + S_{10}$$\n代入符号表达式：\n$$S_{\\text{total}} = (K \\times M \\times B) + (K \\times M \\times B) + (M \\times B) + (M \\times B) + (M \\times B) + (K \\times B) + (K \\times B) + (M_x \\times B) + (M_y \\times B) + (3 \\times K \\times B)$$\n我们可以提出公因式 $B$ 并合并相似项：\n$$S_{\\text{total}} = B \\times [ (2 \\times K \\times M) + (3 \\times M) + (5 \\times K) + M_x + M_y ]$$\n现在，我们代入给定的数值：$K=100$，$M_x=500$，$M_y=500$ 和 $B=8$。\n首先，我们计算网格箱的总数：\n$$M = M_x \\times M_y = 500 \\times 500 = 250,000$$\n接下来，我们将这些值代入 $S_{\\text{total}}$ 的表达式中：\n$$S_{\\text{total}} = 8 \\times [ (2 \\times 100 \\times 250,000) + (3 \\times 250,000) + (5 \\times 100) + 500 + 500 ]$$\n我们计算括号内的各项：\n$$2 \\times 100 \\times 250,000 = 50,000,000$$\n$$3 \\times 250,000 = 750,000$$\n$$5 \\times 100 = 500$$\n$$500 + 500 = 1,000$$\n将这些项相加：\n$$50,000,000 + 750,000 + 500 + 1,000 = 50,751,500$$\n现在，乘以 $B=8$：\n$$S_{\\text{total}} = 8 \\times 50,751,500 = 406,012,000 \\text{ 字节}$$\n问题要求以吉字节 (GB) 为单位给出答案，使用定义 $1\\,\\text{GB} = 10^9$ 字节。\n$$S_{\\text{GB}} = \\frac{S_{\\text{total}}}{10^9} = \\frac{406,012,000}{1,000,000,000} = 0.406012\\,\\text{GB}$$\n最后，我们将结果四舍五入到三位有效数字。前三位有效数字是 $4$、$0$ 和 $6$。随后的数字是 $0$，所以我们向下取整。\n$$S_{\\text{GB}} \\approx 0.406$$\n因此，所需的总 RAM 为 $0.406$ GB。",
            "answer": "$$\n\\boxed{0.406}\n$$"
        },
        {
            "introduction": "本练习将带您深入WHAM方法的核心，您将亲手实现该算法，并用它来研究统计估计中的一个基本概念：偏差-方差权衡。您将为一个模型体系生成模拟数据，然后应用您编写的WHAM代码来重建平均力势（PMF），并分析直方图的“箱子”宽度（bin width） $\\Delta x$ 的选择如何影响结果的准确性。通过亲手实践，您将巩固对WHAM方程的理解，并揭示系统误差（偏差）与统计噪声（方差）之间的微妙平衡。",
            "id": "2465743",
            "problem": "您将实现一项关于加权直方图分析方法 (WHAM) 的数值研究，以从第一性原理出发，量化重构的平均力势 (PMF) 中系统偏差与统计方差之间，随直方图箱宽 $\\,\\Delta x\\,$ 变化的权衡关系。您的实现必须遵循以下数学模型和任务。\n\n背景与核心定义：\n- 在一维情况下，沿反应坐标 $\\,x\\,$ 的无偏倚平衡概率密度为 $\\,p(x) \\propto \\exp\\!\\left(-\\beta U(x)\\right)\\,$，其中 $\\,U(x)\\,$ 是势能，$\\,\\beta = 1/(k_\\mathrm{B} T)\\,$。在本问题中，使用约化单位，设定 $\\,\\beta = 1\\,$，因此能量单位为 $\\,k_\\mathrm{B}T\\,$。\n- 平均力势 (PMF) 定义为 $\\,F(x) = -\\ln p(x)\\,$（相差一个加性常数）。当 $\\,\\beta = 1\\,$ 且 $\\,p(x) \\propto \\exp(-U(x))\\,$ 时，真实的 PMF 等于 $\\,U(x)\\,$ 加上一个常数。\n- 伞形采样在 $\\,K\\,$ 个偏置势 $\\,U_k^\\text{tot}(x) = U(x) + w_k(x)\\,$ 下测量数据，其中 $\\,w_k(x)\\,$ 是一个已知的偏置势，此处为简谐势：$\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x - x_k)^2\\,$，$\\,\\kappa > 0\\,$ 为弹性常数，$\\,x_k\\,$ 为窗口中心。\n- 直方图估计器将定义域划分为宽度为 $\\,\\Delta x\\,$ 的箱，并使用箱内计数。加权直方图分析方法 (WHAM) 结合来自多个偏置窗口的直方图，通过求解一个关于密度和窗口自由能偏移量的自洽系统，来估计无偏倚的密度，并由此得到 PMF。\n\n您的任务：\n1) 从第一性原理生成合成数据。设真实势为对称双势阱\n$$\nU(x) \\;=\\; \\alpha \\,\\bigl(x^2 - c^2\\bigr)^2,\n$$\n其中 $\\,\\alpha > 0\\,$ 且 $\\,c > 0\\,$，并考虑定义域 $\\,x \\in [-L, L]\\,$。对于每个伞形窗口 $\\,k \\in \\{1,\\dots,K\\}\\,$，其简谐偏置为 $\\,w_k(x) = \\tfrac{1}{2}\\kappa\\,(x-x_k)^2\\,$，总样本量为 $\\,N_k\\,$，通过以下步骤生成合成的直方图计数：\n- 通过对 $\\,[-L,L]\\,$ 上的一个足够精细的网格进行数值积分，从连续密度 $\\,p_k(x) \\propto \\exp\\!\\bigl(-U(x) - w_k(x)\\bigr)\\,$ 计算出精确的偏置箱概率。\n- 从总数为 $\\,N_k\\,$ 和已计算出的箱概率的多项分布中，为窗口 $\\,k\\,$ 的每个箱抽取计数。这直接反映了偏置下的玻尔兹曼分布，不依赖于捷径或封闭形式的采样，从而确保了科学真实性。\n\n2) WHAM 重构。对于给定的 $\\,\\Delta x\\,$，实现 WHAM 以估计箱中心的无偏倚密度。您的实现必须使用不动点迭代，该迭代强制执行密度的正确归一化，并确定每个窗口的自由能偏移量。PMF 的估计值为 $\\,\\widehat{F}(x_b) = -\\ln \\widehat{p}(x_b)\\,$（相差一个加性常数）；为了进行数值比较，选择常数使得 $\\,\\min_b \\widehat{F}(x_b) = 0\\,$，同样地，将真实 PMF $\\,F_\\text{true}(x)=U(x)\\,$ 在相同的箱中心上求值时，也设定为 $\\,\\min_b F_\\text{true}(x_b) = 0\\,$。\n\n3) 作为 $\\,\\Delta x\\,$ 函数的偏差-方差分析。对于每个候选的 $\\,\\Delta x\\,$，重复 $\\,R\\,$ 次独立的合成数据集生成和 WHAM 重构，以经验性地估计：\n- 逐点平均 PMF $\\,\\overline{F}_{\\Delta x}(x_b)\\,$ 和方差 $\\,\\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr]\\,$。\n- 积分平方偏差\n$$\nB^2(\\Delta x) \\;=\\; \\sum_b \\bigl(\\,\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\,\\bigr)^2 \\,\\Delta x,\n$$\n和积分方差\n$$\nV(\\Delta x) \\;=\\; \\sum_b \\operatorname{Var}_{\\Delta x}\\!\\bigl[F(x_b)\\bigr] \\,\\Delta x.\n$$\n- 平均积分平方误差 (MISE) $\\,\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)\\,$，它反映了基本的权衡关系：增加 $\\,\\Delta x\\,$ 会增加分箱平均偏差，但会减少统计方差；减小 $\\,\\Delta x\\,$ 会减少偏差，但由于每个箱的计数减少，会增加方差。\n\n测试套件：\n在约化单位中采用以下固定的物理和数值参数：\n- 双势阱：$\\,\\alpha = 2\\,$，$\\,c = 1\\,$，定义域 $\\,[-L,L] = [-2,2]\\,$。\n- 伞形窗口数量：$\\,K = 7\\,$，中心位于 $\\,x_k \\in \\{-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5\\}\\,$，弹性常数 $\\,\\kappa = 20\\,$。\n- 用于偏差-方差估计的独立重复实验次数：$\\,R = 40\\,$。\n- 在所有情况下，假设每个窗口的计数相等 $\\,N_k = N\\,$。\n\n通过改变 $\\,N\\,$ 和候选的箱宽，定义三个测试用例，分别探索方差主导、平衡和偏差主导的情况：\n- 案例 $\\,1\\,$ (中等采样量): $\\,N = 2000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 案例 $\\,2\\,$ (低采样量): $\\,N = 500\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n- 案例 $\\,3\\,$ (高采样量): $\\,N = 10000\\,$，候选 $\\,\\Delta x \\in \\{0.02,\\,0.05,\\,0.10,\\,0.20\\}\\,$。\n\n您的程序必须做到：\n- 对于每个测试用例和每个候选的 $\\,\\Delta x\\,$，执行上述基于重复实验的偏差-方差分析，并计算 $\\,\\mathrm{MISE}(\\Delta x)\\,$。\n- 对于每个测试用例，选择使 $\\,\\mathrm{MISE}(\\Delta x)\\,$ 最小的 $\\,\\Delta x\\,$。如果出现平局，则在最小化者中选择最小的 $\\,\\Delta x\\,$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例所选的最优箱宽，格式为逗号分隔的列表并用方括号括起，例如 $\\,\\texttt{[0.05,0.10,0.05]}\\,$。不应打印任何额外文本。\n\n重要说明：\n- 所有能量单位均为 $\\,k_\\mathrm{B}T\\,$，$\\,x\\,$ 是无量纲的。\n- 不涉及角度。\n- 随机性必须在内部处理且可复现；请固定一个随机种子。\n- 实现必须是自包含的，不得读取或写入任何文件，也不需要用户输入。",
            "solution": "该问题要求对加权直方图分析方法 (WHAM) 的偏差-方差权衡进行数值研究，该权衡是直方图箱宽 $\\Delta x$ 的函数。该问题在科学上是有效的、适定的，并提供了所有必要的参数。它代表了计算化学和统计力学中的一个标准任务，基于已建立的原理。我将着手提供一个完整的解决方案。\n\n解决方案分三个主要阶段实施：合成数据生成、使用 WHAM 进行 PMF 重构，以及在多个重复实验中进行偏差-方差分析。\n\n### 1. 合成数据生成\n\n为确保科学上真实的分析，合成数据必须直接从底层的玻尔兹曼分布生成。真实的、无偏倚的势是一个对称双势阱势，由 $U(x) = \\alpha(x^2 - c^2)^2$ 给出，其中 $\\alpha=2$ 且 $c=1$。定义域为 $x \\in [-2, 2]$。所有计算均在 $\\beta = (k_B T)^{-1} = 1$ 的约化单位下进行。\n\n在伞形采样中，系统在 $K$ 个不同的偏置势 $w_k(x)$ 下进行模拟，以增强对高能区域的采样。此处使用简谐偏置 $w_k(x) = \\frac{1}{2}\\kappa(x - x_k)^2$，其中 $\\kappa=20$，$K=7$ 个窗口的中心位于 $x_k \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5\\}$。\n\n对于每个窗口 $k$，总势为 $U_k^\\text{tot}(x) = U(x) + w_k(x)$，对应的平衡概率密度为 $p_k(x) \\propto \\exp(-U_k^\\text{tot}(x))$。为了生成给定箱宽 $\\Delta x$ 的直方图计数，我们首先将定义域 $[-L, L]$ 划分为离散的箱。在窗口 $k$ 中观察到样本落在箱 $b$（从 $x_b^{\\text{start}}$ 到 $x_b^{\\text{end}}$）的精确概率由归一化密度的积分给出：\n$$\nP_{k,b} = \\frac{\\int_{x_b^{\\text{start}}}^{x_b^{\\text{end}}} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}{\\int_{-L}^{L} \\exp\\left(-U_k^\\text{tot}(x)\\right) dx}\n$$\n这些积分通过在一个精细网格上进行数值积分来计算。使用高分辨率网格（其间距远小于任何 $\\Delta x$）通过离散求和（梯形法则）来近似积分。\n\n对于每个窗口 $k$，利用箱概率向量 $\\{P_{k,b}\\}$，从具有 $N_k$ 次总试验（样本）的多项分布中抽取合成的直方图计数 $\\{N_{k,b}\\}$。此过程精确地模拟了分子模拟中采样的统计特性。\n\n### 2. WHAM 重构\n\nWHAM 提供了一种方法，可以结合来自多个偏置模拟的数据，以计算无偏倚概率分布 $p(x)$ 的最优估计，从而得到平均力势 (PMF) $F(x) = -\\ln p(x)$。该方法通过求解一组关于每个箱中心 $x_b$ 处的无偏倚概率 $p_b$ 和每个模拟窗口的无量纲自由能 $f_k$ 的自洽方程。当 $\\beta=1$ 时，WHAM 方程为：\n$$\np_b = \\frac{\\sum_{k=1}^K N_{k,b}}{\\sum_{k=1}^K N_k \\exp(f_k - w_k(x_b))}\n$$\n$$\n\\exp(-f_k) = \\sum_b p_b \\exp(-w_k(x_b))\n$$\n此处，$p_b$ 是与真实密度成正比的未归一化概率。这些方程通过不动点迭代求解：\n1. 初始化所有自由能 $f_k = 0$。\n2. 重复使用当前的 $f_k$ 计算概率 $p_b$。\n3. 使用新的 $p_b$ 计算更新后的自由能 $f_k^{\\text{new}}$。\n4. 为防止漂移，施加一个约束，例如固定一个自由能（如 $f_1=0$）。\n5. 迭代继续，直到自由能收敛到指定公差。\n\n收敛后，最终的概率 $p_b$ 用于计算给定数据集的 PMF 估计值：$\\widehat{F}(x_b) = -\\ln p_b$。为了进行比较，估计的 PMF 通过一个加性常数进行平移，使其最小值为零。真实的 PMF，$F_{\\text{true}}(x_b) = U(x_b)$，也在同一组箱中心上进行归一化，使其最小值为零。\n\n当一个箱 $b$ 在所有窗口中的计数均为零时，即 $\\sum_k N_{k,b} = 0$，会出现一个关键问题。在这种情况下，$p_b=0$ 且估计的 PMF $\\widehat{F}(x_b)$ 为无穷大。这表示该箱的估计器发生了灾难性失效。\n\n### 3. 偏差-方差分析\n\n问题的核心是量化系统误差（偏差）和统计误差（方差）之间随箱宽 $\\Delta x$ 变化的权衡关系。对于每个候选的 $\\Delta x$，我们执行 $R=40$ 次独立的数据生成和 WHAM 重构过程的重复实验。\n\n在这 $R$ 次重复实验中，我们计算：\n- 平均估计 PMF：$\\overline{F}_{\\Delta x}(x_b) = \\frac{1}{R} \\sum_{r=1}^R \\widehat{F}_r(x_b)$。\n- PMF 的样本方差：$\\operatorname{Var}_{\\Delta x}[F(x_b)] = \\frac{1}{R-1} \\sum_{r=1}^R (\\widehat{F}_r(x_b) - \\overline{F}_{\\Delta x}(x_b))^2$。\n\n然后将这些值在定义域上积分，得到积分平方偏差 $B^2(\\Delta x)$ 和积分方差 $V(\\Delta x)$：\n$$\nB^2(\\Delta x) = \\sum_b \\left(\\overline{F}_{\\Delta x}(x_b) - F_\\text{true}(x_b)\\right)^2 \\Delta x\n$$\n$$\nV(\\Delta x) = \\sum_b \\operatorname{Var}_{\\Delta x}[F(x_b)] \\Delta x\n$$\n平均积分平方误差 (MISE) 是其总和：$\\mathrm{MISE}(\\Delta x) = B^2(\\Delta x) + V(\\Delta x)$。\n\n$\\Delta x$ 的选择决定了这种权衡：\n- **小 $\\Delta x$**：偏差低，因为分箱平均误差最小。然而，由于每个箱的样本较少，统计方差会很高。这增加了出现空箱的概率，从而导致 PMF 估计值为无穷大。如果任何一次重复实验中任何一个箱的 PMF 为无穷大，那么该箱的平均 PMF 也将为无穷大，导致 MISE 为无穷大。\n- **大 $\\Delta x$**：方差低，因为更多的样本被汇集到每个箱中，减少了空箱的几率。然而，由于在更宽的区域上对势进行平均，偏差会增加。\n\n我们的实现通过将无穷大的 MISE 分配给任何在 $R$ 次重复实验中导致一个或多个空箱的 $\\Delta x$ 来处理无穷大的 PMF 值。然后，最优的 $\\Delta x$ 是在产生有限 MISE 的选项中使 MISE 最小的那个。这正确地惩罚了对于给定样本量而言过小的箱宽。最终的算法会遍历每个测试用例的候选箱宽，计算 MISE，并根据指定标准选择最优的 $\\Delta x$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the WHAM bias-variance analysis for all test cases.\n    \"\"\"\n    \n    # --- Fixed Physical and Numerical Parameters ---\n    ALPHA = 2.0\n    C = 1.0\n    L = 2.0\n    K_UMBRELLA = 7\n    X_CENTERS = np.array([-1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5])\n    KAPPA = 20.0\n    R_REPLICATES = 40\n    \n    # --- WHAM Solver Parameters ---\n    WHAM_MAX_ITER = 10000\n    WHAM_TOL = 1e-8\n    \n    # --- Data Generation Parameters ---\n    FINE_GRID_FACTOR = 100\n    RANDOM_SEED = 1234\n    \n    RNG = np.random.default_rng(RANDOM_SEED)\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case 1 (moderate sampling)\n        (2000, [0.02, 0.05, 0.10, 0.20]),\n        # Case 2 (low sampling)\n        (500, [0.02, 0.05, 0.10, 0.20]),\n        # Case 3 (high sampling)\n        (10000, [0.02, 0.05, 0.10, 0.20]),\n    ]\n    \n    # --- Helper Functions ---\n\n    def true_potential(x, alpha, c):\n        return alpha * (x**2 - c**2)**2\n\n    def bias_potential(x, x_k, kappa):\n        return 0.5 * kappa * (x - x_k)**2\n\n    def generate_synthetic_data(n_samples, delta_x):\n        \"\"\"\n        Generates one set of synthetic histogram data for all K windows.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        n_bins = len(bins) - 1\n        \n        fine_x = np.linspace(-L, L, n_bins * FINE_GRID_FACTOR)\n        fine_dx = fine_x[1] - fine_x[0]\n\n        U_fine = true_potential(fine_x, ALPHA, C)\n        \n        hist_counts = np.zeros((K_UMBRELLA, n_bins), dtype=np.int32)\n        \n        for k in range(K_UMBRELLA):\n            w_k_fine = bias_potential(fine_x, X_CENTERS[k], KAPPA)\n            U_tot_k_fine = U_fine + w_k_fine\n            \n            # Numerically integrate to get bin probabilities\n            unnorm_p_density = np.exp(-U_tot_k_fine)\n            Z_k = np.sum(unnorm_p_density) * fine_dx\n            \n            p_k_bins = np.zeros(n_bins)\n            for b in range(n_bins):\n                bin_mask = (fine_x >= bins[b]) & (fine_x < bins[b+1])\n                p_k_bins[b] = np.sum(unnorm_p_density[bin_mask]) * fine_dx / Z_k\n            \n            # Ensure probabilities sum to 1 due to potential floating point errors\n            p_k_bins /= np.sum(p_k_bins)\n            \n            # Generate counts from multinomial distribution\n            hist_counts[k, :] = RNG.multinomial(n_samples, p_k_bins)\n            \n        return hist_counts\n\n    def run_wham(hist_counts, delta_x, n_samples_vec):\n        \"\"\"\n        Implements the WHAM fixed-point iteration to find the PMF.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n\n        # Pre-compute biases at bin centers\n        w_kb = np.zeros((K_UMBRELLA, n_bins))\n        for k in range(K_UMBRELLA):\n            w_kb[k, :] = bias_potential(bin_centers, X_CENTERS[k], KAPPA)\n\n        f_k = np.zeros(K_UMBRELLA)\n        \n        for _ in range(WHAM_MAX_ITER):\n            f_k_old = f_k.copy()\n            \n            # Equation for probabilities p_b\n            numer = np.sum(hist_counts, axis=0) # N_b\n            log_denom_terms = f_k[:, np.newaxis] - w_kb\n            max_log = np.max(log_denom_terms, axis=0)\n            denom_arg = n_samples_vec[:, np.newaxis] * np.exp(log_denom_terms - max_log)\n            denom = np.exp(max_log) * np.sum(denom_arg, axis=0)\n\n            # p_b are unnormalized probabilities\n            p_b = np.zeros_like(numer, dtype=float)\n            non_zero_denom = denom > 1e-15 # Avoid division by zero\n            p_b[non_zero_denom] = numer[non_zero_denom] / denom[non_zero_denom]\n\n            # Equation for free energies f_k\n            exp_neg_w_kb = np.exp(-w_kb)\n            f_k_new_arg = np.sum(p_b[np.newaxis, :] * exp_neg_w_kb, axis=1)\n\n            # Avoid log(0) for windows with no overlap with data\n            f_k = -np.log(f_k_new_arg, where=f_k_new_arg > 0, out=np.full_like(f_k_new_arg, np.inf))\n            \n            # Shift to set f_1 = 0\n            f_k -= f_k[0]\n            \n            if np.linalg.norm(f_k - f_k_old)  WHAM_TOL:\n                break\n        \n        # Final PMF calculation\n        pmf = -np.log(p_b, where=p_b > 0, out=np.full_like(p_b, np.inf))\n        \n        if not np.all(np.isfinite(pmf)):\n             return pmf # Return with inf values\n        \n        pmf -= np.min(pmf)\n        return pmf\n\n    def analyze_bias_variance(n_samples, delta_x):\n        \"\"\"\n        Performs the full bias-variance analysis for a given N and delta_x.\n        \"\"\"\n        bins = np.arange(-L, L + delta_x, delta_x)\n        bin_centers = (bins[:-1] + bins[1:]) / 2.0\n        n_bins = len(bin_centers)\n        \n        # True PMF on bin centers, normalized\n        F_true = true_potential(bin_centers, ALPHA, C)\n        F_true -= np.min(F_true)\n        \n        all_pmfs = np.zeros((R_REPLICATES, n_bins))\n        n_samples_vec = np.full(K_UMBRELLA, n_samples)\n        \n        for r in range(R_REPLICATES):\n            hist_counts = generate_synthetic_data(n_samples, delta_x)\n            pmf_estimate = run_wham(hist_counts, delta_x, n_samples_vec)\n            all_pmfs[r, :] = pmf_estimate\n\n        # If any replicate resulted in an empty bin, MISE is infinite\n        if np.isinf(all_pmfs).any():\n            return np.inf\n            \n        # Calculate mean and variance of PMF estimates\n        mean_pmf = np.mean(all_pmfs, axis=0)\n        var_pmf = np.var(all_pmfs, axis=0, ddof=1) # Sample variance\n        \n        # Integrated squared bias\n        bias_sq = np.sum((mean_pmf - F_true)**2) * delta_x\n        \n        # Integrated variance\n        variance = np.sum(var_pmf) * delta_x\n        \n        return bias_sq + variance\n\n    # --- Main Loop ---\n    \n    optimal_dx_results = []\n    \n    for n_samples, dx_candidates in test_cases:\n        mises = []\n        for dx in dx_candidates:\n            mise = analyze_bias_variance(n_samples, dx)\n            mises.append(mise)\n        \n        mises_array = np.array(mises)\n        \n        # Find the minimum finite MISE\n        min_mise = np.min(mises_array[np.isfinite(mises_array)]) if np.any(np.isfinite(mises_array)) else np.inf\n\n        if not np.isfinite(min_mise):\n             # This case should not happen with the given parameters\n             # If all MISE are inf, pick the largest dx as it's most robust\n            optimal_dx = dx_candidates[-1]\n        else:\n            # Find all indices matching the minimum MISE\n            best_indices = np.where(mises_array == min_mise)[0]\n            # Tie-breaking rule: choose smallest dx\n            optimal_dx = dx_candidates[best_indices[0]]\n\n        optimal_dx_results.append(optimal_dx)\n\n    # Format the final output\n    print(f\"[{','.join(f'{x:.2f}' for x in optimal_dx_results)}]\")\n\n\nsolve()\n```"
        },
        {
            "introduction": "在基础分析之上，这个高级练习将挑战一个更具实践意义的问题：如何高效地设计计算实验。通常，初始模拟只能提供一个粗糙的PMF估计，其在某些区域的不确定性很高。本练习要求您开发一个基于第一性原理的贪心算法，利用这个初始的PMF估计来智能地决定在何处放置新的伞形采样窗口，从而最有效地降低整体的不确定性。通过从重要性采样的基本原理出发构建策略，您将学会如何自动化并优化数据收集过程，这是高级计算研究中的一项关键技能。",
            "id": "2465751",
            "problem": "给定一个一维反应坐标 $x \\in [x_{\\min}, x_{\\max}]$ (单位为纳米) 和一个平均力势 (PMF) 的粗略初始估计，记作 $\\widehat{F}(x)$ (单位为千焦/摩尔)。该系统先前已使用带谐波偏置势的伞形采样方法进行了采样，您的任务是设计并实现一个有原则的算法，以确定在何处放置新的伞形窗口，从而在使用加权直方图分析方法 (WHAM) 进行分析时，能最快地减少 PMF 的不确定性。\n\n您的算法必须从第一性原理出发，基于玻尔兹曼分布以及重要性采样及其有效样本量的定义进行推导。您不能假定任何 WHAM 的专用公式；相反，您必须仅使用以下基本要素来构建算法：\n- 玻尔兹曼分布：沿 $x$ 的无偏概率密度满足 $p(x) \\propto \\exp(-\\beta F(x))$，其中 $\\beta = 1/(k_{\\mathrm{B}} T)$。这里 $k_{\\mathrm{B}}$ 是玻尔兹曼常数 (每摩尔)，$T$ 是绝对温度。\n- 在一个中心位于 $c$、弹簧常数为 $k$ (能量单位/平方纳米) 的谐波伞形势下，偏置势为 $U(x) = \\tfrac{1}{2} k (x - c)^2$，有偏采样密度与 $p(x)\\exp(-\\beta U(x))$ 成正比。\n- 对于权重为 $w$ 的重要性采样，加权平均的有效样本量可以使用标准的重要性采样恒等式 $N_{\\mathrm{eff}} = \\dfrac{(\\sum w)^2}{\\sum w^2}$ 来估计，这是一个经过充分检验的统计结果。\n\n问题目标与约束：\n- PMF 的不确定性在无偏密度 $p(x)$ 最难确定的地方最大。请仅使用每个离散区间内的有效样本量，构建一个沿坐标轴的不确定性代理指标，并定义一个标量目标函数来聚合沿坐标轴的这种不确定性。您的目标函数必须是关于有效样本量的单调递减函数在所有区间上的总和 (例如，与 $1/\\sqrt{N_{\\mathrm{eff}}}$ 成正比的表达式是可以接受的)，并且必须包含一个小的正则化项 $\\varepsilon$ 以避免除以零。\n- 给定当前的伞形窗口集合及其样本数量，使用粗略的 PMF $\\widehat{F}(x)$ 来预测每个窗口的样本在各个区间上的分布。然后，估算所有当前窗口合并贡献的每个区间的有效样本量。您可以通过在区间中心点进行求值来近似区间平均量。\n- 为选择新的伞形窗口，请使用贪心策略：一次放置一个新窗口，方法是评估一组离散的候选中心，并选择当其预期贡献被加入时，能最大程度减少聚合不确定性目标的那个中心。重复此过程，直到放置了所需数量的新窗口。在任何一对窗口中心 (无论是已有的还是新选择的) 之间，强制执行最小间距约束 $\\Delta_{\\min}$。\n- 所有积分必须通过在均匀网格上的黎曼和来近似。所有概率必须进行适当的归一化。\n\n数值与物理设置：\n- 单位：\n  - $x$ 的单位为纳米。\n  - $F$ 和 $U$ 的单位为千焦/摩尔。\n  - $k_{\\mathrm{B}} = 8.31446261815324 \\times 10^{-3}$ 千焦/摩尔/开尔文。\n  - $T$ 的单位为开尔文。\n- 本问题不使用角度。\n- 除上述选择所隐含的单位换算外，没有其他单位换算。\n\n离散化与候选点：\n- 将 $[x_{\\min}, x_{\\max}]$ 离散化为 $M$ 个等间距的点 $x_j$，间距为常数 $\\Delta x$；使用区间中心作为新窗口中心的候选点。\n- 对于每个候选中心 $z$，通过建模一个具有指定弹簧常数和样本数的新谐波伞形势，预测其对有效样本量的影响，并将其贡献加到当前窗口的现有贡献中。\n\n您的程序必须实现上述算法，并将其应用于以下测试套件。在每个案例中，根据提供的公式明确构建 $\\widehat{F}(x)$，使用指定的现有窗口，并在满足最小间距约束的条件下选择所需数量的新窗口。\n\n测试套件：\n- 所有案例的通用设置：\n  - $x_{\\min} = 0$ 纳米, $x_{\\max} = 1$ 纳米。\n  - 网格点数：$M = 201$。\n  - 温度：$T = 300$ 开尔文。\n  - 有效样本量的正则化项：$\\varepsilon = 10^{-12}$ (无量纲，仅为防止除零而加在任何平方根或分母内部)。\n  - 最小中心间距：$\\Delta_{\\min} = 0.05$ 纳米。\n  - 在区间中心点评估所有与区间相关的量。\n\n- 案例1 (单势阱 PMF，典型的内部不确定性)：\n  - PMF：$\\widehat{F}(x) = \\tfrac{1}{2} a (x - 0.5)^2$，其中 $a = 100$ 千焦/摩尔/平方纳米。\n  - 现有窗口：两个窗口，中心 $c = [0.2, 0.8]$ 纳米，弹簧常数 $k = [1000, 1000]$ 千焦/摩尔/平方纳米，样本数 $N = [1000, 1000]$。\n  - 待放置的新窗口：一次一个，总共选择 $m_{\\text{new}} = 2$ 个窗口，每个窗口的 $k_{\\text{new}} = 1000$ 千焦/摩尔/平方纳米，样本数 $N_{\\text{new}} = 1000$。\n\n- 案例2 (带中心势垒的双势阱 PMF，势垒附近重叠差)：\n  - PMF：$\\widehat{F}(x) = \\min\\{50(x-0.3)^2,\\,50(x-0.7)^2\\} + 10 \\exp\\!\\big(-\\tfrac{1}{2}\\big(\\tfrac{x-0.5}{0.05}\\big)^2\\big)$，所有能量单位为千焦/摩尔，$x$ 单位为纳米。\n  - 现有窗口：两个窗口，中心 $c = [0.25, 0.35]$ 纳米，弹簧常数 $k = [1500, 1500]$ 千焦/摩尔/平方纳米，样本数 $N = [1500, 1500]$。\n  - 待放置的新窗口：$m_{\\text{new}} = 1$ 个窗口，其 $k_{\\text{new}} = 1500$ 千焦/摩尔/平方纳米，样本数 $N_{\\text{new}} = 1500$。\n\n- 案例3 (无现有数据，均匀 PMF，初始设计的边缘情况)：\n  - PMF：对于所有 $x$，$\\widehat{F}(x) = 0$。\n  - 现有窗口：无。\n  - 待放置的新窗口：$m_{\\text{new}} = 1$ 个窗口，其 $k_{\\text{new}} = 500$ 千焦/摩尔/平方纳米，样本数 $N_{\\text{new}} = 2000$。\n\n输出要求：\n- 对于每个案例，输出所选新窗口中心的列表 (单位为纳米)，四舍五入到三位小数。\n- 最终程序输出必须是一行，包含一个由逗号分隔的三个案例结果的列表，并用一对单独的方括号括起来。每个案例结果本身必须是一个浮点数列表。例如：[[c1_1,c1_2],[c2_1],[c3_1]]，无空格。",
            "solution": "该问题要求阐述并实现一个贪心算法，以确定新伞形采样窗口的最佳放置位置。其目标是最小化平均力势 (PMF) 的全局不确定性，该不确定性是使用加权直方图分析方法 (WHAM) 估算的。推导过程必须基于统计力学和重要性采样的第一性原理。\n\n首先，我们定义系统和概率。一维反应坐标 $x$ 被离散化为 $M$ 个区间，由其中心点 $x_j$ (其中 $j=1, \\dots, M$) 表示。系统处于区间 $j$ 的无偏概率由玻尔兹曼分布给出，$p(x_j) \\propto \\exp(-\\beta F(x_j))$，其中 $\\beta = 1/(k_{\\mathrm{B}}T)$ 是逆温度，$F(x)$ 是 PMF。对于我们的预测算法，我们使用提供的估计值 $\\widehat{F}(x)$。因此，区间 $j$ 中的未归一化概率取为 $p^{\\text{un}}_j = \\exp(-\\beta \\widehat{F}(x_j))$。\n\n一次伞形采样模拟 $i$ 由中心 $c_i$、谐波弹簧常数 $k_i$ 和收集的样本数 $N_i$ 来表征。这引入了一个偏置势 $U_i(x) = \\frac{1}{2} k_i (x - c_i)^2$。在模拟 $i$ 期间，在区间 $j$ 中观测到系统的概率被此偏置所改变：\n$$\nq_{ij} \\propto p(x_j) \\exp(-\\beta U_i(x_j))\n$$\n在所有区间上进行归一化后，概率变为：\n$$\nq_{ij} = \\frac{p^{\\text{un}}_j \\exp(-\\beta U_{ij})}{\\sum_{k=1}^M p^{\\text{un}}_k \\exp(-\\beta U_{ik})} = \\frac{p^{\\text{un}}_j \\exp(-\\beta U_{ij})}{Z_i}\n$$\n其中 $Z_i$ 是有偏模拟 $i$ 的配分函数。来自模拟 $i$ 且落入区间 $j$ 的预期样本数为 $n_{ij} = N_i q_{ij}$。\n\n为了合并所有模拟的数据，我们使用重要性采样。来自模拟 $i$ 位于位置 $x_j$ 的一个样本必须被重新加权，以对无偏系综做出贡献。相应的重要性权重 $w_{ij}$ 是目标概率与采样概率之比：\n$$\nw_{ij} = \\frac{p(x_j)}{q_{ij}(x_j)} \\propto \\frac{\\exp(-\\beta \\widehat{F}(x_j))}{\\exp(-\\beta (\\widehat{F}(x_j) + U_{ij}))} = \\exp(\\beta U_{ij})\n$$\n比例常数与有效样本量的计算无关，因为它会被消掉。\n\n问题指出，PMF 的不确定性在样本统计最差的地方最高。我们用每个区间 $j$ 的有效样本量 $N_{\\mathrm{eff}}$ 来代理这一不确定性。给定来自所有 $L$ 次模拟、落在区间 $j$ 的样本集合，其中每次模拟 $i$ 有 $n_{ij}$ 个权重为 $w_{ij}$ 的样本，则区间 $j$ 的总有效样本量由标准公式给出：\n$$\nN_{\\mathrm{eff}, j} = \\frac{\\left( \\sum_{i=1}^L n_{ij} w_{ij} \\right)^2}{\\sum_{i=1}^L n_{ij} w_{ij}^2}\n$$\n区间 $j$ 的不确定性被认为与 $\\sqrt{N_{\\mathrm{eff}, j}}$ 成反比。我们的全局目标函数 $J$ (我们旨在最小化它) 是这些不确定性在所有区间上的总和，并带有一个小的正则化项 $\\varepsilon$ 以确保数值稳定性：\n$$\nJ = \\sum_{j=1}^M \\frac{1}{\\sqrt{N_{\\mathrm{eff}, j} + \\varepsilon}}\n$$\n为了在计算 $N_{\\mathrm{eff}, j}$ 时确保数值稳定性，我们重构了求和。分子求和中的项是 $n_{ij} w_{ij} = (N_i p^{\\text{un}}_j/Z_i)$。分母求和中的项是 $n_{ij} w_{ij}^2 = (N_i p^{\\text{un}}_j/Z_i) \\exp(\\beta U_{ij})$。这导致：\n$$\nN_{\\mathrm{eff}, j} = p^{\\text{un}}_j \\frac{\\left(\\sum_i N_i/Z_i\\right)^2}{\\sum_i (N_i/Z_i) \\exp(\\beta U_{ij})}\n$$\n项 $\\exp(\\beta U_{ij})$ 可能会非常大，导致溢出。为了稳健地计算分母中的和，我们使用 log-sum-exp 方法。对于每个区间 $j$，我们计算和中每一项的对数，即 $\\log(N_i) - \\log(Z_i) + \\beta U_{ij}$，然后使用数值稳定的 `logaddexp` 函数来找到和的对数，再通过取指数来恢复和本身。\n\n选择 $m_{\\text{new}}$ 个新窗口中心的算法是贪心算法。我们从现有窗口及其参数集开始。\n1. 用任何现有窗口的中心初始化当前窗口中心集。\n2. 对于要放置的 $m_{\\text{new}}$ 个窗口中的每一个：\n    a. 生成所有有效候选中心的列表。如果一个候选点 (网格点 $x_j$) 与当前集合中每个中心的距离都至少为 $\\Delta_{\\min}$，则该候选点有效。\n    b. 对于每个有效的候选中心 $z$，假设性地将一个中心为 $z$ 且具有指定参数 ($k_{\\text{new}}, N_{\\text{new}}$) 的新窗口添加到当前集合中。\n    c. 为这个假设的完整窗口集计算目标函数 $J$。\n    d. 选择导致 $J$ 值最小的候选中心 $z^*$。\n    e. 将这个新窗口 (以 $z^*$ 为中心) 永久添加到当前窗口集合中，以供下一次迭代使用。\n3. 最终选定的中心列表 $\\{z^*_1, \\dots, z^*_{m_{\\text{new}}}\\}$ 就是结果。\n\n此过程确保每个新窗口都被放置在预计能为 PMF 整体不确定性提供最大边际减少的位置。",
            "answer": "```python\nimport numpy as np\n\n# Physical constants and settings\nKB_J_MOL_K = 8.31446261815324\nKB = KB_J_MOL_K / 1000.0  # In kJ/mol/K\n\ndef calculate_objective(centers, ks, Ns, F_hat, x_grid, beta, epsilon):\n    \"\"\"\n    Calculates the aggregate uncertainty objective J based on a stable N_eff formulation.\n    \"\"\"\n    num_bins = len(x_grid)\n    num_windows = len(centers)\n\n    if num_windows == 0:\n        # If there are no windows, N_eff is 0 for all bins.\n        return num_bins / np.sqrt(epsilon)\n\n    # Calculate unnormalized unbiased probabilities stably\n    log_p_un = -beta * F_hat\n\n    # For each window i, calculate U_i(j) and its partition function Z_i\n    U_matrix = np.zeros((num_windows, num_bins), dtype=np.float64)\n    log_Z_values = np.zeros(num_windows, dtype=np.float64)\n    for i in range(num_windows):\n        U_matrix[i, :] = 0.5 * ks[i] * (x_grid - centers[i])**2\n        log_integrand_i = log_p_un - beta * U_matrix[i, :]\n        log_Z_values[i] = np.logaddexp.reduce(log_integrand_i)\n        \n    log_N_values = np.log(np.array(Ns, dtype=np.float64))\n    log_Ni_minus_log_Zi = log_N_values - log_Z_values\n\n    # Calculate S1_j = p_un_j * sum_i(N_i/Z_i)\n    # The sum part is constant over j.\n    C = np.sum(np.exp(log_Ni_minus_log_Zi))\n    S1_j = np.exp(log_p_un) * C\n\n    # Calculate S2_j = p_un_j * sum_i( (N_i/Z_i) * exp(beta * U_ij) )\n    # The term in the sum is exp(log(N_i) - log(Z_i) + beta * U_ij)\n    log_terms_for_S2 = log_Ni_minus_log_Zi[:, np.newaxis] + beta * U_matrix\n    \n    # logsumexp over windows i for each bin j\n    log_sum_S2_terms = np.logaddexp.reduce(log_terms_for_S2, axis=0)\n    S2_j = np.exp(log_p_un + log_sum_S2_terms)\n\n    # Calculate N_eff_j = S1_j^2 / S2_j\n    n_eff = np.zeros(num_bins, dtype=np.float64)\n    non_zero_S2 = S2_j > 1e-300 # Avoid division by zero\n    n_eff[non_zero_S2] = (S1_j[non_zero_S2]**2) / S2_j[non_zero_S2]\n    \n    # Calculate final objective J\n    objective = np.sum(1.0 / np.sqrt(n_eff + epsilon))\n    return objective\n\ndef solve_case(params):\n    \"\"\"\n    Solves a single test case using the greedy window placement algorithm.\n    \"\"\"\n    x_grid = np.linspace(params['x_min'], params['x_max'], params['M'])\n    beta = 1.0 / (KB * params['T'])\n\n    # Define and calculate the PMF estimate on the grid\n    if params['case_id'] == 1:\n        a = 100\n        F_hat = 0.5 * a * (x_grid - 0.5)**2\n    elif params['case_id'] == 2:\n        well1 = 50 * (x_grid - 0.3)**2\n        well2 = 50 * (x_grid - 0.7)**2\n        barrier = 10 * np.exp(-0.5 * ((x_grid - 0.5) / 0.05)**2)\n        F_hat = np.minimum(well1, well2) + barrier\n    else: # Case 3\n        F_hat = np.zeros_like(x_grid)\n\n    current_centers = list(params['existing_c'])\n    current_ks = list(params['existing_k'])\n    current_Ns = list(params['existing_N'])\n    \n    newly_selected_centers = []\n    \n    for _ in range(params['m_new']):\n        best_objective = np.inf\n        best_center = None\n        \n        # Filter candidate centers based on minimum separation\n        valid_candidates = []\n        for cand_c in x_grid:\n            if all(np.abs(cand_c - c) >= params['delta_min'] for c in current_centers):\n                valid_candidates.append(cand_c)\n\n        if not valid_candidates:\n            break\n\n        for cand_center in valid_candidates:\n            temp_centers = current_centers + [cand_center]\n            temp_ks = current_ks + [params['k_new']]\n            temp_Ns = current_Ns + [params['N_new']]\n            \n            objective = calculate_objective(\n                temp_centers, temp_ks, temp_Ns,\n                F_hat, x_grid, beta, params['epsilon']\n            )\n            \n            if objective  best_objective:\n                best_objective = objective\n                best_center = cand_center\n        \n        if best_center is not None:\n            current_centers.append(best_center)\n            current_ks.append(params['k_new'])\n            current_Ns.append(params['N_new'])\n            newly_selected_centers.append(best_center)\n            \n    return [round(c, 3) for c in newly_selected_centers]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and orchestrates their solution.\n    \"\"\"\n    common_settings = {\n        'x_min': 0.0,\n        'x_max': 1.0,\n        'M': 201,\n        'T': 300.0,\n        'epsilon': 1e-12,\n        'delta_min': 0.05,\n    }\n\n    test_cases = [\n        {\n            'case_id': 1,\n            **common_settings,\n            'existing_c': [0.2, 0.8],\n            'existing_k': [1000.0, 1000.0],\n            'existing_N': [1000, 1000],\n            'm_new': 2,\n            'k_new': 1000.0,\n            'N_new': 1000,\n        },\n        {\n            'case_id': 2,\n            **common_settings,\n            'existing_c': [0.25, 0.35],\n            'existing_k': [1500.0, 1500.0],\n            'existing_N': [1500, 1500],\n            'm_new': 1,\n            'k_new': 1500.0,\n            'N_new': 1500,\n        },\n        {\n            'case_id': 3,\n            **common_settings,\n            'existing_c': [],\n            'existing_k': [],\n            'existing_N': [],\n            'm_new': 1,\n            'k_new': 500.0,\n            'N_new': 2000,\n        },\n    ]\n\n    results = [solve_case(case) for case in test_cases]\n    \n    # Format the output string exactly as specified.\n    # repr() adds spaces, so we remove them.\n    formatted_results = [repr(res).replace(' ', '') for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}