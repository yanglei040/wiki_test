{
    "hands_on_practices": [
        {
            "introduction": "In any molecular dynamics simulation, ensuring the conservation of fundamental quantities is the first check of validity. In the microcanonical (NVE) ensemble, this means the total energy must remain constant. This exercise  challenges you to act as a simulation detective, diagnosing a common and subtle cause of energy non-conservation. It highlights the crucial interplay between the numerical integrator and the mathematical properties of the potential energy function, specifically the difference between a continuous potential and a continuous force.",
            "id": "2453046",
            "problem": "A system of $N$ identical particles is simulated with classical Molecular Dynamics (MD) in the microcanonical (NVE) ensemble using the velocity-Verlet integrator with a fixed time step $\\Delta t$. The interaction is pairwise and given by a truncated and potential-shifted Lennard–Jones potential. Specifically, the base potential is\n$$\nV_{\\mathrm{LJ}}(r) = 4\\varepsilon\\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right],\n$$\nand the implemented interaction is\n$$\nV_{\\mathrm{ps}}(r) =\n\\begin{cases}\nV_{\\mathrm{LJ}}(r) - V_{\\mathrm{LJ}}(r_c), & r \\le r_c,\\\\\n0, & r > r_c,\n\\end{cases}\n$$\nwith forces computed as $-\\mathrm{d}V_{\\mathrm{ps}}/\\mathrm{d}r$ inside the cutoff and set to $0$ outside. The simulation uses periodic boundary conditions and no thermostat. Despite choosing $\\Delta t$ small enough to resolve the fastest vibrations, the total energy $E(t)$ exhibits a systematic drift over long times instead of remaining constant up to bounded fluctuations.\n\nWhich diagnosis best explains, in terms of the implemented potential $V(r)$, the most likely cause of the observed energy drift?\n\nA. The truncation leaves a jump discontinuity in the force $-\\mathrm{d}V/\\mathrm{d}r$ at $r = r_c$, so pairs crossing $r_c$ incur integration errors from the unresolved force jump that accumulate into a drift.\n\nB. The potential was not shifted to satisfy $V(r_c) = 0$, so $V(r)$ has a finite jump at $r = r_c$ and energy changes discontinuously when pairs cross $r_c$.\n\nC. The short-range repulsive wall of $V_{\\mathrm{LJ}}(r)$ is too stiff, and even with a smooth cutoff any symplectic Verlet-type integrator produces a secular drift in $E(t)$ for stiff potentials.\n\nD. Neglecting analytical long-range tail corrections to the energy for $r > r_c$ in the microcanonical simulation causes a systematic drift in $E(t)$ over time.",
            "solution": "The problem statement must first be validated for scientific soundness and consistency.\n\nStep 1: Extracted Givens\n- System: A system of $N$ identical particles.\n- Simulation method: Classical Molecular Dynamics (MD) in the microcanonical ($NVE$) ensemble.\n- Integrator: Velocity-Verlet integrator with a fixed time step $\\Delta t$.\n- Condition on time step: $\\Delta t$ is small enough to resolve the fastest vibrations.\n- Base potential: $V_{\\mathrm{LJ}}(r) = 4\\varepsilon\\left[\\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^6\\right]$.\n- Implemented potential: $V_{\\mathrm{ps}}(r) = \\begin{cases} V_{\\mathrm{LJ}}(r) - V_{\\mathrm{LJ}}(r_c), & r \\le r_c,\\\\ 0, & r > r_c, \\end{cases}$.\n- Force calculation: The force is calculated as $-\\mathrm{d}V_{\\mathrm{ps}}/\\mathrm{d}r$ for $r \\le r_c$ and is set to $0$ for $r > r_c$.\n- Boundary conditions: Periodic boundary conditions are used.\n- Thermostat: No thermostat is used.\n- Observation: The total energy $E(t)$ exhibits a systematic drift over long times.\n- Question: What is the most likely cause of the observed energy drift, related to the potential $V(r)$?\n\nStep 2: Validation\nThe problem describes a standard scenario in molecular simulation. The components—microcanonical ensemble, velocity-Verlet integrator, Lennard-Jones potential, and potential truncation schemes—are all fundamental concepts in computational physics and chemistry. The observation of energy drift is a well-known numerical artifact under certain conditions. The problem is scientifically grounded, well-posed, and objective. It contains sufficient information to determine the most likely cause from the given options. No scientific principles are violated, the setup is realistic, and the question is precisely stated.\n\nStep 3: Verdict\nThe problem statement is valid. I will proceed with the derivation of the solution.\n\nDerivation of the Correct Answer\nIn a classical MD simulation within the $NVE$ ensemble, the total energy of the system, $E(t) = K(t) + U(t)$ (the sum of kinetic and potential energies), must be a conserved quantity. Numerical integration schemes, such as the velocity-Verlet algorithm, approximate the true dynamics. For a time-reversible and symplectic integrator like velocity-Verlet, when applied to a system with a time-independent, conservative, and sufficiently smooth Hamiltonian, the total energy should not exhibit a systematic drift. Instead, it should show bounded fluctuations around a constant mean value. The observed systematic drift in $E(t)$ indicates a violation of one of these conditions.\n\nThe source of the problem must be traced to the properties of the implemented potential, $V_{\\mathrm{ps}}(r)$, and the corresponding force, $F(r) = -\\mathrm{d}V_{\\mathrm{ps}}(r)/\\mathrm{d}r$.\n\nLet us analyze the potential and force at the cutoff radius, $r_c$.\n\n1.  **Continuity of the Potential $V_{\\mathrm{ps}}(r)$**:\n    The potential for $r \\le r_c$ is defined as $V_{\\mathrm{ps}}(r) = V_{\\mathrm{LJ}}(r) - V_{\\mathrm{LJ}}(r_c)$. At the cutoff, for $r=r_c$, this expression yields $V_{\\mathrm{ps}}(r_c) = V_{\\mathrm{LJ}}(r_c) - V_{\\mathrm{LJ}}(r_c) = 0$.\n    For $r > r_c$, the potential is defined as $V_{\\mathrm{ps}}(r) = 0$.\n    Since the value of the potential approaches $0$ from both sides of $r_c$, the potential function $V_{\\mathrm{ps}}(r)$ is continuous everywhere. This is the purpose of the \"potential-shift\" term, $-V_{\\mathrm{LJ}}(r_c)$.\n\n2.  **Continuity of the Force $F(r)$**:\n    The force is the negative gradient of the potential.\n    For $r \\le r_c$, the force is $F(r) = -\\frac{\\mathrm{d}}{\\mathrm{d}r} \\left( V_{\\mathrm{LJ}}(r) - V_{\\mathrm{LJ}}(r_c) \\right) = -\\frac{\\mathrm{d}V_{\\mathrm{LJ}}(r)}{\\mathrm{d}r}$, since $V_{\\mathrm{LJ}}(r_c)$ is a constant.\n    The derivative of the Lennard-Jones potential is:\n    $$ \\frac{\\mathrm{d}V_{\\mathrm{LJ}}(r)}{\\mathrm{d}r} = 4\\varepsilon \\left( -12 \\frac{\\sigma^{12}}{r^{13}} + 6 \\frac{\\sigma^6}{r^7} \\right) = \\frac{24\\varepsilon}{r} \\left( -\\frac{2\\sigma^{12}}{r^{12}} + \\frac{\\sigma^6}{r^6} \\right) $$\n    For $r > r_c$, the force is explicitly set to $F(r) = 0$.\n\n    Let us evaluate the force at the boundary $r=r_c$.\n    The limit of the force as $r$ approaches $r_c$ from below is:\n    $$ F(r \\to r_c^-) = -\\frac{\\mathrm{d}V_{\\mathrm{LJ}}(r)}{\\mathrm{d}r}\\bigg|_{r=r_c} = -\\frac{24\\varepsilon}{r_c} \\left( -\\frac{2\\sigma^{12}}{r_c^{12}} + \\frac{\\sigma^6}{r_c^6} \\right) $$\n    The limit of the force as $r$ approaches $r_c$ from above is:\n    $$ F(r \\to r_c^+) = 0 $$\n    In general, $\\frac{\\mathrm{d}V_{\\mathrm{LJ}}}{\\mathrm{d}r}$ is non-zero at $r=r_c$. It is only zero at the potential minimum, $r_m = 2^{1/6}\\sigma$, which is not a sensible choice for a cutoff radius. Therefore, $F(r \\to r_c^-) \\neq F(r \\to r_c^+)$. The force field has a jump discontinuity at $r=r_c$.\n\nThe derivation of the velocity-Verlet algorithm, and the proof of its symplecticity, relies on the assumption that the potential function is smooth (at least twice differentiable, so the force is continuously differentiable). The discontinuity in the force $F(r)$ violates this condition. When a pair of particles crosses the cutoff distance $r_c$, the integrator must handle an instantaneous change in force, which it is not designed to do. This results in a small error in the integration of the equations of motion for that step. Because particle pairs cross the cutoff boundary systematically in a simulation, these small, directed errors accumulate over time, leading to a systematic drift in the total energy rather than bounded oscillations.\n\nEvaluation of Options\nA. The truncation leaves a jump discontinuity in the force $-\\mathrm{d}V/\\mathrm{d}r$ at $r = r_c$, so pairs crossing $r_c$ incur integration errors from the unresolved force jump that accumulate into a drift.\nThis statement is perfectly aligned with our derivation. The implemented potential, while continuous itself, generates a discontinuous force at the cutoff. This discontinuity breaks the symplectic nature of the Verlet integration scheme, causing an accumulation of errors that manifests as a systematic energy drift. This is the canonical explanation for energy drift with simple truncated potentials.\n**Verdict: Correct.**\n\nB. The potential was not shifted to satisfy $V(r_c) = 0$, so $V(r)$ has a finite jump at $r = r_c$ and energy changes discontinuously when pairs cross $r_c$.\nThis statement is factually incorrect. The problem explicitly describes a potential-shifted form: $V_{\\mathrm{ps}}(r) = V_{\\mathrm{LJ}}(r) - V_{\\mathrm{LJ}}(r_c)$ for $r \\le r_c$. As demonstrated above, this specific construction ensures that the potential $V_{\\mathrm{ps}}(r)$ is continuous at $r = r_c$, with $V_{\\mathrm{ps}}(r_c) = 0$. A discontinuous potential would cause infinite forces and catastrophic energy non-conservation, which is a more severe problem than a mere drift.\n**Verdict: Incorrect.**\n\nC. The short-range repulsive wall of $V_{\\mathrm{LJ}}(r)$ is too stiff, and even with a smooth cutoff any symplectic Verlet-type integrator produces a secular drift in $E(t)$ for stiff potentials.\nThis statement is misleading. While stiff potentials (i.e., those with large second derivatives) require a very small time step $\\Delta t$ for stable and accurate integration, the problem states that $\\Delta t$ was chosen appropriately (\"small enough to resolve the fastest vibrations\"). For any smooth potential, stiff or not, a symplectic integrator with a sufficiently small time step will conserve energy up to bounded fluctuations, without a secular (long-term, systematic) drift. The stiffness affects the amplitude of these fluctuations, but does not cause a drift. The cause of drift is the loss of symplecticity, which happens here due to the non-smoothness of the potential (discontinuous force), not its stiffness.\n**Verdict: Incorrect.**\n\nD. Neglecting analytical long-range tail corrections to the energy for $r > r_c$ in the microcanonical simulation causes a systematic drift in $E(t)$ over time.\nLong-range tail corrections are constant adjustments made to observables like energy and pressure to account for interactions beyond the cutoff. For energy, this correction is a constant value, typically calculated assuming a uniform radial distribution function for $r > r_c$. This correction affects the absolute value of the total energy reported, but it does not influence the dynamics of the simulation, as the forces are derived from the truncated potential $V_{\\mathrm{ps}}(r)$. The quantity that is supposed to be conserved by the integrator is the Hamiltonian of the simulated system, $E(t) = K(t) + U_{\\mathrm{ps}}(t)$. Adding a constant does not change its time derivative; thus, neglecting a constant correction cannot introduce a drift over time.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A primary goal of simulation is to connect microscopic interactions to macroscopic, measurable properties. The radial distribution function, $g(r)$, provides a direct link between the simulated particle positions and experimental scattering data. This problem  explores a common pitfall in calculating $g(r)$ from simulation data, demonstrating how an incorrect handling of periodic boundary conditions in the analysis code can create significant, unphysical artifacts. Mastering this concept is essential for producing reliable structural data.",
            "id": "2453040",
            "problem": "In a Molecular Dynamics (MD) simulation of a homogeneous, isotropic liquid of $N$ particles at number density $\\rho$ in a cubic box of side length $L$, one intends to use periodic boundary conditions (PBC) to mimic an infinite system. The radial distribution function $g(r)$ is defined so that the mean number of particles found in a spherical shell of radius $r$ and thickness $dr$ around an arbitrary particle is $\\rho\\,g(r)\\,4\\pi r^{2}dr$. In a well-equilibrated homogeneous liquid, $g(r)\\to 1$ as $r\\to \\infty$.\n\nSuppose the MD integrator correctly evolved the dynamics with PBC, but the analysis code that computes $g(r)$ contained an implementation error: when forming pair distances, it ignored periodicity (no minimum-image wrapping) and counted only neighbors whose unwrapped positions lie within the primary box, i.e., it did not include pairs that cross a box boundary. The analysis still normalizes pair counts by the full shell volume $4\\pi r^{2}dr$ and by $\\rho$ in the standard way.\n\nFrom first principles and the above definitions, which of the following artifacts would most directly appear in the computed $g(r)$?\n\nA. An artificial depletion of $g(r)$ below $1$ for larger $r$ approaching $L/2$, so that $g(r)$ fails to approach $1$ and instead drops due to missing cross-boundary neighbors.\n\nB. A uniform rescaling that leaves the large-$r$ limit at exactly $1$ while multiplying all peak heights by the same constant factor independent of $r$.\n\nC. A systematic shift of all peak positions of $g(r)$ to smaller $r$ by a constant factor, reflecting an apparent compression of the structure.\n\nD. The emergence of sharp, $\\delta$-function-like peaks in $g(r)$ at $r=L$, $r=\\sqrt{2}\\,L$, and $r=\\sqrt{3}\\,L$, caused by counting each particle and its own periodic images as distinct neighbors within the usual cutoff $r\\le L/2$.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- System: A Molecular Dynamics (MD) simulation of a homogeneous, isotropic liquid of $N$ particles.\n- Number density: $\\rho$.\n- Simulation box: A cubic box of side length $L$.\n- Boundary conditions: Periodic boundary conditions (PBC) were used in the MD integrator.\n- Definition of $g(r)$: The mean number of particles in a spherical shell of radius $r$ and thickness $dr$ around an arbitrary particle is given by $\\rho\\,g(r)\\,4\\pi r^{2}dr$.\n- Asymptotic behavior: For a well-equilibrated homogeneous liquid, $g(r)\\to 1$ as $r\\to \\infty$.\n- Implementation error in analysis code:\n    1. Periodicity is ignored (no minimum-image wrapping).\n    2. Only pairs of particles whose unwrapped positions are both within the primary box are counted.\n    3. The normalization is performed correctly using the full shell volume $4\\pi r^{2}dr$ and the density $\\rho$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in the principles of statistical mechanics and computational chemistry. The concepts of MD simulations, PBC, and the radial distribution function $g(r)$ are standard. The described error is a plausible and common implementation mistake.\n- **Well-Posedness**: The problem is well-posed. It describes a correct simulation followed by a specific, flawed analysis procedure and asks for the resulting artifact. The premises are sufficient to deduce a unique consequence.\n- **Objectivity**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, self-contained, and well-posed. A solution will be derived.\n\n**Derivation**\n\nThe radial distribution function, $g(r)$, is formally defined such that for a system with number density $\\rho$, the average number of particles, $d\\langle n(r) \\rangle$, in a spherical shell of radius $r$ and thickness $dr$ around a central particle is:\n$$ d\\langle n(r) \\rangle = \\rho \\, g(r) \\, 4\\pi r^2 dr $$\nFrom this, the theoretical $g(r)$ is:\n$$ g(r) = \\frac{1}{\\rho \\, 4\\pi r^2} \\frac{d\\langle n(r) \\rangle}{dr} $$\nIn a computer simulation, one computes $g(r)$ by building a histogram of pair distances. The number of pairs found in a finite shell between $r$ and $r+\\Delta r$ is counted and averaged over time and over all particles as centers, giving $\\langle N_{pairs}(r, \\Delta r) \\rangle$. The computed $g_{comp}(r)$ for a bin centered at $r$ is then calculated as:\n$$ g_{comp}(r) = \\frac{\\langle N_{pairs}(r, \\Delta r) \\rangle}{N_{ideal}(r, \\Delta r)} $$\nwhere $N_{ideal}(r, \\Delta r) = \\frac{1}{2} N \\rho (4\\pi r^2 \\Delta r)$ is the number of pairs expected in an ideal gas in the same volume shell (the factor of $\\frac{1}{2}$ and using $N$ instead of $N-1$ are conventions that do not affect the reasoning here).\n\nThe correct simulation uses Periodic Boundary Conditions (PBC). This means that to calculate the distance between particles $i$ and $j$, the minimum image convention is applied. The system effectively mimics an infinite, periodic lattice of simulation boxes. When calculating $g(r)$, this ensures that any spherical shell around a central particle is fully populated with neighbors according to the bulk density, regardless of the central particle's position. For a homogeneous liquid, this correctly yields the limiting behavior $g(r) \\to 1$ for large $r$ (practically, for $r$ up to $L/2$).\n\nThe problem states that the analysis code contains an error: it ignores periodicity. This means that for any two particles with coordinate vectors $\\vec{r}_i$ and $\\vec{r}_j$ inside the primary box (i.e., coordinates in $[0, L)^3$), the code computes the distance as simply $|\\vec{r}_i - \\vec{r}_j|$. It does not check for shorter distances to periodic images of particle $j$.\n\nLet us analyze the consequence. Consider a particle $i$ at position $\\vec{r}_i$ as the center. The analysis code attempts to find neighbors in a spherical shell of radius $r$. However, it only considers other particles $j$ that are also within the primary box $[0, L)^3$.\n- If the spherical shell around $\\vec{r}_i$ is entirely contained within the box, the code correctly samples the local environment. This is only possible if the particle is far from all boundaries and $r$ is small.\n- If the particle is near a boundary, or if $r$ is large, a part of the spherical shell lies outside the primary box. The analysis code, ignoring PBC, finds no particles in this external region. A correct code would have found the periodic images of particles from the other side of the box in this region.\n\nThis error leads to a systematic undercounting of pairs. The effective volume from which pairs are sampled is not the full shell volume $4\\pi r^2 dr$, but the volume of the intersection of this shell with the cubic box. Let us denote the volume of this intersection, for a central particle at $\\vec{r}_i$, as $V_{intersect}(r, \\vec{r}_i)$. The average effective volume, averaged over all particle positions in the homogeneous fluid, is $\\langle V_{intersect}(r) \\rangle$. This average volume will be a function of $r$.\n- For $r \\to 0$, nearly all shells are fully inside the box, so $\\langle V_{intersect}(r) \\rangle \\approx 4\\pi r^2 dr$.\n- As $r$ increases, the probability of a shell being \"clipped\" by the box boundaries increases. Therefore, the ratio $f(r) = \\frac{\\langle V_{intersect}(r) \\rangle}{4\\pi r^2 dr}$ becomes progressively smaller than $1$.\n- For $r \\geq L/2$, it is geometrically impossible for the sphere of radius $r$ to be fully contained in the box, regardless of the central particle's position. Thus, $f(r)$ drops significantly for $r$ approaching and exceeding $L/2$.\n\nThe computed $g_{comp}(r)$ is based on the number of pairs found in this truncated volume, but it is normalized by the full ideal gas count in the full shell volume. Let $g_{true}(r)$ be the true radial distribution function. Then:\n$$ g_{comp}(r) \\approx \\frac{\\rho \\, g_{true}(r) \\, \\langle V_{intersect}(r) \\rangle}{\\rho \\, (4\\pi r^2 dr)} = g_{true}(r) \\cdot f(r) $$\nSince $g_{true}(r) \\to 1$ for large $r$ in a homogeneous liquid, the computed function will behave as $g_{comp}(r) \\to f(r)$. As we have established that $f(r)$ is a function that decreases from $1$ as $r$ increases, the computed $g(r)$ will show an artificial depletion at large $r$ and will fail to approach the expected limit of $1$.\n\n**Option-by-Option Analysis**\n\nA. **An artificial depletion of $g(r)$ below $1$ for larger $r$ approaching $L/2$, so that $g(r)$ fails to approach $1$ and instead drops due to missing cross-boundary neighbors.**\nThis statement accurately describes the derived effect. The geometric truncation of the sampling volume results in a multiplicative factor $f(r) < 1$ that becomes more severe as $r$ increases. This is a depletion caused by the failure to count neighbors that are close via PBC but are on opposite sides of the primary box.\n**Verdict: Correct.**\n\nB. **A uniform rescaling that leaves the large-$r$ limit at exactly $1$ while multiplying all peak heights by the same constant factor independent of $r$.**\nThe reduction factor $f(r)$ is strongly dependent on $r$, it is not a constant. Therefore, the rescaling is non-uniform. Furthermore, the large-$r$ limit will be $f(r)$, which is less than $1$, not exactly $1$.\n**Verdict: Incorrect.**\n\nC. **A systematic shift of all peak positions of $g(r)$ to smaller $r$ by a constant factor, reflecting an apparent compression of the structure.**\nThe error is in the number of pairs counted at a given distance $r$, not in the calculation of the distance $r$ itself for the pairs that are found. The effect is on the magnitude (height) of $g(r)$, not the position of its features along the $r$-axis.\n**Verdict: Incorrect.**\n\nD. **The emergence of sharp, $\\delta$-function-like peaks in $g(r)$ at $r=L$, $r=\\sqrt{2}\\,L$, and $r=\\sqrt{3}\\,L$, caused by counting each particle and its own periodic images as distinct neighbors within the usual cutoff $r\\le L/2$.**\nThis describes an artifact caused by an incorrect *application* of PBC (e.g., counting a particle and its own images), not by *ignoring* PBC as stated in the problem. The error described in the problem is one of omission, not one of adding spurious pairs at distances corresponding to the box dimensions. Also, these distances are typically beyond the standard cutoff of $L/2$ for calculating $g(r)$.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Perhaps the most critical question in any simulation is: \"Have I sampled enough?\" This exercise  delves into the subtle and crucial concept of ergodicity. It presents a common scenario where a key diagnostic, the kinetic temperature, appears perfectly stable and correct, yet the simulation is failing to explore the relevant conformational space. Understanding this deceptive nature of the kinetic temperature is a hallmark of an expert simulator, as it forces a deeper consideration of what constitutes proper sampling.",
            "id": "2453013",
            "problem": "A classical Molecular Dynamics (MD) simulation of a cluster with $N=50$ particles is run under a thermostat set to a target temperature $T=300\\,\\mathrm{K}$. The Hamiltonian is separable into kinetic and potential terms, $H(\\mathbf{p},\\mathbf{q})=K(\\mathbf{p})+U(\\mathbf{q})$, and the equations of motion are integrated with a time step small enough to ensure numerical stability. Over a trajectory of length $10\\,\\mathrm{ns}$, the system remains confined to a single metastable basin separated by large free-energy barriers from other basins. The running time-average of the kinetic-temperature observable, defined via the instantaneous kinetic energy and the number of active degrees of freedom, matches the target temperature $T$ to within $1\\%$.\n\nStarting from first principles of the canonical ensemble, the definition of ergodicity as the equality of time and ensemble averages in the long-time limit, and the separability of kinetic and potential energy in classical mechanics, reason about how the time-averaged kinetic temperature can match the target even when the simulation is non-ergodic (i.e., trapped in a basin), and why agreement of the kinetic temperature with $T$ is a misleading diagnostic of proper sampling.\n\nWhich statements correctly explain both how this is possible and why relying on kinetic temperature alone can misdiagnose sampling quality? Select all that apply.\n\nA. In the canonical ensemble with separable Hamiltonian $H(\\mathbf{p},\\mathbf{q})=K(\\mathbf{p})+U(\\mathbf{q})$, the joint distribution factorizes, so a thermostat can enforce the correct Maxwell–Boltzmann momentum marginal and thereby $\\langle K\\rangle$ consistent with $T$, even if the coordinate distribution is restricted to one basin; thus the time-averaged kinetic temperature can match $T$ while configurational sampling is biased.\n\nB. Matching the target temperature guarantees that all ensemble averages are correct, because time averages equal ensemble averages regardless of ergodicity.\n\nC. The kinetic temperature depends primarily on fast velocity fluctuations and is largely insensitive to rare barrier-crossing events in slow collective coordinates; therefore it can appear correct while slow modes remain unsampled, making it an unreliable indicator of configurational sampling.\n\nD. Non-ergodicity necessarily causes the kinetic temperature to drift away from the setpoint over long times, so observing a match between the time-averaged kinetic temperature and $T$ implies ergodicity.\n\nE. Accounting for constraints reduces the number of degrees of freedom, and once this count is correct the temperature will always match $T$, which also implies that the coordinate distribution is correct.",
            "solution": "The problem statement will first be validated for scientific soundness and completeness.\n\n### Step 1: Extract Givens\n\n-   Simulation type: Classical Molecular Dynamics (MD)\n-   Number of particles: $N=50$\n-   Thermostat target temperature: $T=300\\,\\mathrm{K}$\n-   Hamiltonian: Separable, $H(\\mathbf{p},\\mathbf{q})=K(\\mathbf{p})+U(\\mathbf{q})$\n-   Integration time step: Sufficiently small for numerical stability.\n-   Trajectory length: $10\\,\\mathrm{ns}$\n-   System dynamics: The system is confined to a single metastable basin, separated by large free-energy barriers from other basins.\n-   Observation: The running time-average of the kinetic-temperature observable matches the target temperature $T$ to within $1\\%$.\n-   Definition of kinetic temperature: Derived from the instantaneous kinetic energy and the number of active degrees of freedom.\n-   Task: Explain how the time-averaged kinetic temperature can match the target $T$ despite the simulation being non-ergodic, and why this agreement is a misleading diagnostic of proper sampling.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientific Groundedness**: The problem describes a canonical scenario in computational statistical mechanics. A separable Hamiltonian is the standard form in classical mechanics. The concept of a system being trapped in a metastable basin is a central challenge in MD, known as the sampling problem. Thermostats, the canonical ensemble, and the distinction between kinetic and configurational sampling are all fundamental and well-established principles. The problem is scientifically sound.\n-   **Well-Posedness**: The question is unambiguous. It asks for a mechanistic explanation for a common observation in MD simulations and requires evaluation of provided statements against this explanation. The setup is self-contained and allows for a definitive answer based on the principles of statistical mechanics.\n-   **Objectivity**: The problem is stated in precise, objective, and technical language. There are no subjective or opinion-based elements.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective. A rigorous solution can be derived from first principles.\n\n### Derivation from First Principles\n\nIn the canonical (NVT) ensemble, the equilibrium probability distribution of states in phase space $(\\mathbf{q}, \\mathbf{p})$ is given by the Boltzmann distribution:\n$$\n\\rho(\\mathbf{q}, \\mathbf{p}) = \\frac{1}{Z} e^{-\\beta H(\\mathbf{q}, \\mathbf{p})}\n$$\nwhere $\\beta = (k_B T)^{-1}$, $k_B$ is the Boltzmann constant, $T$ is the thermodynamic temperature, and $Z$ is the canonical partition function.\n\nThe problem states that the Hamiltonian is separable: $H(\\mathbf{p},\\mathbf{q}) = K(\\mathbf{p}) + U(\\mathbf{q})$. This has a profound consequence. The probability distribution factorizes into independent distributions for momenta $\\mathbf{p}$ and coordinates $\\mathbf{q}$:\n$$\n\\rho(\\mathbf{q}, \\mathbf{p}) = \\frac{1}{Z_p Z_q} e^{-\\beta K(\\mathbf{p})} e^{-\\beta U(\\mathbf{q})} = \\rho_p(\\mathbf{p}) \\rho_q(\\mathbf{q})\n$$\nwhere $\\rho_p(\\mathbf{p}) = Z_p^{-1} \\exp(-\\beta K(\\mathbf{p}))$ is the marginal probability distribution for momenta, and $\\rho_q(\\mathbf{q}) = Z_q^{-1} \\exp(-\\beta U(\\mathbf{q}))$ is the marginal probability distribution for coordinates. The key insight is that momenta and coordinates are statistically independent in the canonical ensemble for a separable Hamiltonian.\n\nA thermostat coupled to an MD simulation primarily acts to modify the equations of motion for the momenta $\\mathbf{p}$ to ensure that their distribution samples $\\rho_p(\\mathbf{p})$ correctly. This is the Maxwell-Boltzmann distribution. The ensemble average of the kinetic energy $K(\\mathbf{p})$ is then determined solely by this momentum distribution:\n$$\n\\langle K \\rangle_{ens} = \\int K(\\mathbf{p}) \\rho_p(\\mathbf{p}) d\\mathbf{p}\n$$\nAccording to the equipartition theorem, which applies here, each quadratic term in the kinetic energy contributes $\\frac{1}{2} k_B T$ to this average. If there are $N_{dof}$ kinetic degrees of freedom, then:\n$$\n\\langle K \\rangle_{ens} = \\frac{N_{dof}}{2} k_B T\n$$\nThe instantaneous kinetic temperature is defined as $T_{kin}(t) = \\frac{2 K(\\mathbf{p}(t))}{N_{dof} k_B}$. A working thermostat ensures that the time-average of this quantity, $\\langle T_{kin} \\rangle_{time}$, converges to the target temperature $T$. This equilibration happens on a very fast timescale, characteristic of molecular vibrations and collisions (femtoseconds to picoseconds).\n\nNow consider the configurational part. The ergodic hypothesis states that for a sufficiently long trajectory, the time average of an observable is equal to its ensemble average. In this problem, the simulation is explicitly stated to be non-ergodic over the $10\\,\\mathrm{ns}$ timescale because it is trapped in a single metastable basin. This means the trajectory only explores a subset of the configuration space $\\mathbf{q}$. Consequently, a time-average of any configurational observable $A(\\mathbf{q})$ will only reflect an average over that single basin, not the full Boltzmann-weighted average over all accessible configurations.\n$$\n\\langle A(\\mathbf{q}) \\rangle_{time} \\approx \\langle A(\\mathbf{q}) \\rangle_{basin} \\neq \\langle A(\\mathbf{q}) \\rangle_{ens}\n$$\nBecause the dynamics of $\\mathbf{p}$ and $\\mathbf{q}$ are statistically decoupled in the ensemble, and because thermostats act efficiently on the fast momentum degrees of freedom, it is entirely possible for $\\langle K \\rangle_{time} \\approx \\langle K \\rangle_{ens}$ (and thus $\\langle T_{kin} \\rangle_{time} \\approx T$) while simultaneously $\\langle U \\rangle_{time} \\neq \\langle U \\rangle_{ens}$ and sampling of configuration space is severely biased.\n\nTherefore, agreement of the kinetic temperature with the target value $T$ is a check on the performance of the thermostat and the equilibration of kinetic degrees of freedom, but it provides no information about the sampling of the configurational landscape. Relying on it as a sole indicator of overall simulation quality or \"equilibration\" is a common but serious error, as the slow degrees of freedom responsible for conformational changes and chemical processes may be completely unsampled.\n\n### Option-by-Option Analysis\n\n**A. In the canonical ensemble with separable Hamiltonian $H(\\mathbf{p},\\mathbf{q})=K(\\mathbf{p})+U(\\mathbf{q})$, the joint distribution factorizes, so a thermostat can enforce the correct Maxwell–Boltzmann momentum marginal and thereby $\\langle K\\rangle$ consistent with $T$, even if the coordinate distribution is restricted to one basin; thus the time-averaged kinetic temperature can match $T$ while configurational sampling is biased.**\nThis statement is a precise summary of the statistical mechanics principles derived above. It correctly identifies the separability of the Hamiltonian and the resulting factorization of the probability distribution as the fundamental reason. It correctly explains that the thermostat acts on the momentum distribution independently of the (potentially poor) configurational sampling. This explains both *how* the phenomenon is possible and *why* kinetic temperature is a misleading diagnostic.\n**Verdict: Correct**\n\n**B. Matching the target temperature guarantees that all ensemble averages are correct, because time averages equal ensemble averages regardless of ergodicity.**\nThis statement is fundamentally false. The equivalence of time and ensemble averages is the definition of ergodicity. For a non-ergodic system, this equality does not hold. The problem explicitly describes a non-ergodic sampling situation. Therefore, this statement is in direct contradiction with the principles of statistical mechanics.\n**Verdict: Incorrect**\n\n**C. The kinetic temperature depends primarily on fast velocity fluctuations and is largely insensitive to rare barrier-crossing events in slow collective coordinates; therefore it can appear correct while slow modes remain unsampled, making it an unreliable indicator of configurational sampling.**\nThis statement provides a kinetic and timescale-based explanation which is complementary to the statistical mechanical argument in A. Momenta $\\mathbf{p}$ (and thus velocities) fluctuate on a very fast timescale ($\\sim 10^{-15} - 10^{-12}\\,\\mathrm{s}$). A thermostat can equilibrate these fluctuations quickly. Barrier-crossing events involve slow, collective motions of coordinates $\\mathbf{q}$ that occur on much longer timescales ($\\sim 10^{-9}\\,\\mathrm{s}$ or longer). A $10\\,\\mathrm{ns}$ simulation is long enough to average over the fast kinetic fluctuations, yielding a correct $\\langle T_{kin} \\rangle_{time}$, but may be far too short to observe the rare configurational events. This correctly identifies the timescale separation as the reason for the misleading diagnostic.\n**Verdict: Correct**\n\n**D. Non-ergodicity necessarily causes the kinetic temperature to drift away from the setpoint over long times, so observing a match between the time-averaged kinetic temperature and $T$ implies ergodicity.**\nThis statement is incorrect. As explained, non-ergodicity in configuration space does not prevent a thermostat from maintaining the kinetic temperature at the target value. The proper function of a thermostat is precisely to prevent such drifts. Therefore, observing a match between kinetic temperature and the target $T$ absolutely does not imply ergodicity in the full phase space. This statement posits the exact opposite of the correct physical picture.\n**Verdict: Incorrect**\n\n**E. Accounting for constraints reduces the number of degrees of freedom, and once this count is correct the temperature will always match $T$, which also implies that the coordinate distribution is correct.**\nThis statement is flawed. While it is true that one must use the correct number of degrees of freedom, $N_{dof}$, to calculate $T_{kin}$ from $K$, this is merely a matter of correct calculation. It does not guarantee that the temperature will match $T$; the thermostat does that. The final conclusion, that this implies the coordinate distribution is correct, is a non-sequitur and is fundamentally wrong for the reasons detailed in the analysis of options A and C.\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{AC}$$"
        }
    ]
}