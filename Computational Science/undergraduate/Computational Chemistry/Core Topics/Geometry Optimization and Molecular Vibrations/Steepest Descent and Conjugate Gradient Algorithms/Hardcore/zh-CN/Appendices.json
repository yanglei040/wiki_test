{
    "hands_on_practices": [
        {
            "introduction": "最速下降法在概念上很简单，但其实际效果在很大程度上取决于步长的选择。本练习提供了一个动手编程的机会，你将探索一个固定的步长如何影响算法在简单一维势能面上的收敛性 。通过观察不同情况下的结果，你将具体理解为什么步长是如此关键的参数，以及它如何导致算法收敛、发散或振荡。",
            "id": "2463036",
            "problem": "给定一个一维简谐振子势，定义为 $V(x) = \\tfrac{1}{2} k x^2$，其中 $k  0$ 是一个给定的力常数，$x$ 是标量坐标。考虑由下式给出的固定步长最速下降（SD）迭代：\n$$\nx_{n+1} = x_n - \\alpha \\,\\frac{dV}{dx}(x_n),\n$$\n其中步长 $\\alpha \\in \\mathbb{R}$ 是一个常数，初始值为 $x_0 \\in \\mathbb{R}$。对于该势，精确的极小值点位于 $x^\\star = 0$。对于给定的最大迭代次数 $N_{\\max}$ 和容差 $\\varepsilon  0$，将收敛到极小值点定义为：存在一个索引 $n \\le N_{\\max}$ 使得 $|x_n| \\le \\varepsilon$。如果在 $N_{\\max}$ 次迭代内不存在这样的 $n$，则判断为不收敛。\n\n编写一个完整的、可运行的程序，针对以下每个测试用例，应用该迭代来确定其收敛或不收敛，并输出一个布尔值结果：\n- 测试用例 1：$k = 3.0$，$\\alpha = 0.3$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n- 测试用例 2：$k = 3.0$，$\\alpha = 0.7$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n- 测试用例 3：$k = 2.0$，$\\alpha = 1.0$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n- 测试用例 4：$k = 1.5$，$\\alpha = 0.0$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n- 测试用例 5：$k = 1.5$，$\\alpha = -0.2$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n- 测试用例 6：$k = 2.0$，$\\alpha = 0.99$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n\n在本次计算练习中，所有量均视为无量纲，最终答案为不带物理单位的布尔值。您的程序必须将六个测试用例的结果汇总到单行输出中，该行包含一个由方括号括起来的、以逗号分隔的布尔值列表，不含任何其他文本。例如，输出必须看起来像单行上的一个 Python 风格的列表。",
            "solution": "已审阅问题陈述，并确定其有效。该问题具有科学依据、提法明确、客观，并包含获得唯一解所需的所有信息。这项任务是应用于一个基本物理模型的数值优化标准练习。我们着手进行分析和求解。\n\n问题要求确定固定步长最速下降（SD）算法在最小化由下式给出的一维简谐振子势时的收敛性：\n$$\nV(x) = \\frac{1}{2} k x^2\n$$\n其中 $k  0$ 是力常数。该势的极小值点位于 $x^\\star = 0$。\n\nSD 迭代定义为：\n$$\nx_{n+1} = x_n - \\alpha g_n\n$$\n其中 $g_n$ 是势在 $x_n$ 处的梯度，$\\alpha$ 是一个常数步长。对于这个一维问题，梯度就是对 $x$ 的导数：\n$$\ng_n = \\frac{dV}{dx}(x_n) = k x_n\n$$\n将该导数代入迭代公式中，得到一个线性递推关系：\n$$\nx_{n+1} = x_n - \\alpha (k x_n) = (1 - \\alpha k) x_n\n$$\n这是一个公比为 $r = 1 - \\alpha k$ 的等比数列。该递推关系的解由下式给出：\n$$\nx_n = (1 - \\alpha k)^n x_0\n$$\n为使序列 $\\{x_n\\}$ 在 $n \\to \\infty$ 时收敛到极小值点 $x^\\star = 0$，公比的绝对值必须小于 1：\n$$\n|r| = |1 - \\alpha k|  1\n$$\n这个不等式等价于以下两个条件：\n$$\n-1  1 - \\alpha k  1\n$$\n让我们分别分析这两个不等式。\n1. 右侧不等式：\n$$\n1 - \\alpha k  1 \\implies -\\alpha k  0\n$$\n由于给定的力常数 $k$ 为正 ($k  0$)，这可简化为 $\\alpha  0$。\n2. 左侧不等式：\n$$\n-1  1 - \\alpha k \\implies \\alpha k  2\n$$\n同样，由于 $k  0$，这可简化为 $\\alpha  2/k$。\n\n综合这两个结果，最速下降算法对此势收敛到最小值的充要条件是：\n$$\n0  \\alpha  \\frac{2}{k}\n$$\n如果满足这个条件，序列 $\\{x_n\\}$ 将收敛到 0。我们现在必须根据这个解析条件来评估每个测试用例。在计算上，收敛被定义为在某个迭代 $n \\le N_{\\max}$ 时达到 $|x_n| \\le \\varepsilon$ 的状态。\n\n用例 1：$k = 3.0$，$\\alpha = 0.3$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0  \\alpha  2/3.0$，约等于 $0  \\alpha  0.667$。给定的步长 $\\alpha = 0.3$ 在此范围内。公比为 $r = 1 - (0.3)(3.0) = 0.1$。由于 $|r|  1$，迭代将会收敛。结果为 **True**。\n\n用例 2：$k = 3.0$，$\\alpha = 0.7$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0  \\alpha  0.667$。步长 $\\alpha = 0.7$ 在此范围之外。公比为 $r = 1 - (0.7)(3.0) = 1 - 2.1 = -1.1$。由于 $|r|  1$，序列将会发散，后续的点会交替变号且数值增大。结果为 **False**。\n\n用例 3：$k = 2.0$，$\\alpha = 1.0$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0  \\alpha  2/2.0$，即 $0  \\alpha  1.0$。步长 $\\alpha = 1.0$ 是一个边界情况，此时 $\\alpha = 2/k$。公比为 $r = 1 - (1.0)(2.0) = -1.0$。序列变为 $x_{n+1} = -x_n$。从 $x_0 = 1.0$ 开始，值将是 $1.0, -1.0, 1.0, -1.0, \\dots$。数值 $|x_n|$ 保持在 $1.0$ 不变，永远不会小于或等于 $\\varepsilon = 10^{-12}$。它不收敛。结果为 **False**。\n\n用例 4：$k = 1.5$，$\\alpha = 0.0$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n步长为 $\\alpha = 0.0$。这是收敛条件 $0  \\alpha  2/k$ 的一个边界情况。迭代变为 $x_{n+1} = x_n - 0 \\cdot (kx_n) = x_n$。位置永远不会偏离其初始值 $x_0 = 1.0$。由于 $|1.0|  \\varepsilon = 10^{-12}$，收敛条件永远不会满足。算法停滞。结果为 **False**。\n\n用例 5：$k = 1.5$，$\\alpha = -0.2$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n步长 $\\alpha = -0.2$ 是负数，违反了收敛条件 $\\alpha  0$。公比为 $r = 1 - (-0.2)(1.5) = 1 + 0.3 = 1.3$。由于 $|r|  1$，序列将单调发散。结果为 **False**。\n\n用例 6：$k = 2.0$，$\\alpha = 0.99$，$x_0 = 1.0$，$N_{\\max} = 5000$，$\\varepsilon = 10^{-12}$。\n$\\alpha$ 的收敛范围是 $0  \\alpha  2/2.0$，即 $0  \\alpha  1.0$。步长 $\\alpha = 0.99$ 在此范围内。公比为 $r = 1 - (0.99)(2.0) = 1 - 1.98 = -0.98$。由于 $|r| = |-0.98|  1$，迭代将会收敛，尽管由于公比接近-1会导致振荡收敛，速度会较慢。结果为 **True**。\n\n现在将构建一个计算程序，以实现每个用例的迭代并验证这些解析结论。程序将循环最多 $N_{\\max}$ 次迭代，在每一步（$n=0, 1, \\dots, N_{\\max}$）检查收敛条件 $|x_n| \\le \\varepsilon$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the steepest descent convergence problem for six test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (k, alpha, x0, N_max, epsilon)\n    test_cases = [\n        (3.0, 0.3, 1.0, 5000, 1e-12),\n        (3.0, 0.7, 1.0, 5000, 1e-12),\n        (2.0, 1.0, 1.0, 5000, 1e-12),\n        (1.5, 0.0, 1.0, 5000, 1e-12),\n        (1.5, -0.2, 1.0, 5000, 1e-12),\n        (2.0, 0.99, 1.0, 5000, 1e-12),\n    ]\n\n    results = []\n    for case in test_cases:\n        k, alpha, x0, N_max, epsilon = case\n        \n        # Initialize position x and convergence flag\n        x = x0\n        converged = False\n\n        # The problem asks if there is an index n = N_max such that |x_n| = epsilon.\n        # This means we check x_0, x_1, ..., x_{N_max}.\n        # This requires N_max+1 checks and N_max updates.\n        for n in range(N_max + 1):\n            \n            # Check for convergence at the current position x_n\n            if abs(x) = epsilon:\n                converged = True\n                break\n\n            # To avoid an unnecessary update after the last check, we add this condition.\n            if n  N_max:\n                # Calculate the gradient for the simple harmonic oscillator potential V(x) = 1/2*k*x^2.\n                # The gradient (derivative) is dV/dx = k*x.\n                gradient = k * x\n                \n                # Apply the steepest descent update rule.\n                x = x - alpha * gradient\n        \n        results.append(converged)\n\n    # Final print statement in the exact required format.\n    # The output should be a string that looks like a Python list of booleans.\n    # str(True) is 'True', str(False) is 'False', so this works as intended.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在了解了步长选择带来的挑战之后，让我们来看一下最速下降法的理想工作情景。本练习要求你分析算法在一个完全对称的碗状势能面上的优化路径 。通过证明下降路径是一条直达最小值的直线，你将理解最速下降法在何种几何条件下效率最高，并领悟为何在更复杂的各向异性势能面上，它常常会走出“之”字形的曲折路径。",
            "id": "2463076",
            "problem": "在计算化学中，势能面的局部最小化通常通过最速下降（SD）法进行。考虑一个二维各向同性谐波势能面，其能量函数在无量纲单位下由 $E(x,y)=x^{2}+y^{2}$ 给出。最速下降（SD）法通过以下方式更新当前点 $\\mathbf{r}_{k}=(x_{k},y_{k})$：\n$$\n\\mathbf{r}_{k+1}=\\mathbf{r}_{k}-\\alpha_{k}\\,\\nabla E(\\mathbf{r}_{k}),\n$$\n其中 $\\alpha_{k}\\geq 0$ 是一个步长。在第 $k$ 步的精确线搜索选择 $\\alpha_{k}$ 以在 $\\alpha\\geq 0$ 的范围内最小化 $E(\\mathbf{r}_{k}-\\alpha\\,\\nabla E(\\mathbf{r}_{k}))$。与最速下降法相关的连续时间梯度流定义为\n$$\n\\frac{d\\mathbf{r}(t)}{dt}=-\\nabla E(\\mathbf{r}(t)).\n$$\n设 $\\mathbf{r}_{0}=(x_{0},y_{0})\\neq(0,0)$ 为一个任意初始点。\n\n任务：\n- 仅使用梯度的定义和上述更新规则，证明连续时间梯度流轨迹 $\\mathbf{r}(t)$ 和离散SD迭代点 $\\{\\mathbf{r}_{k}\\}$ 都保持在通过 $\\mathbf{r}_{0}$ 和原点的直线上，并因此沿着该直线趋近于原点。\n- 在从 $\\mathbf{r}_{0}$ 开始的第一个SD步骤中进行精确线搜索，确定最优步长 $\\alpha_{\\ast}$，使得 $E(\\mathbf{r}_{0}-\\alpha\\,\\nabla E(\\mathbf{r}_{0}))$ 相对于 $\\alpha\\geq 0$ 最小化。\n\n答案规格：仅提供 $\\alpha_{\\ast}$ 的值作为最终答案。无需四舍五入。不要包含单位。",
            "solution": "该问题要求对二维各向同性谐波势下的最速下降（SD）法及其连续时间模拟——梯度流进行分析。首先，确认问题陈述的有效性。该问题具有科学依据，是适定的、客观的，并且包含了获得唯一解所需的所有必要信息。这是数值优化中的一个标准练习。\n\n势能面由函数 $E(x, y) = x^2 + y^2$ 给出。用向量表示法，令 $\\mathbf{r} = (x, y)$，则 $E(\\mathbf{r}) = \\mathbf{r} \\cdot \\mathbf{r} = \\|\\mathbf{r}\\|^2$。\n\n第一步是计算能量函数 $E(\\mathbf{r})$ 的梯度 $\\nabla E(\\mathbf{r})$。\n梯度的分量是关于 $x$ 和 $y$ 的偏导数：\n$$\n\\frac{\\partial E}{\\partial x} = \\frac{\\partial}{\\partial x}(x^2 + y^2) = 2x\n$$\n$$\n\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y}(x^2 + y^2) = 2y\n$$\n因此，梯度向量为 $\\nabla E(\\mathbf{r}) = (2x, 2y) = 2\\mathbf{r}$。\n\n首先，我们分析连续时间梯度流的轨迹 $\\mathbf{r}(t)$。其控制微分方程为：\n$$\n\\frac{d\\mathbf{r}(t)}{dt} = -\\nabla E(\\mathbf{r}(t))\n$$\n代入梯度的表达式，我们得到一个一阶线性常微分方程：\n$$\n\\frac{d\\mathbf{r}(t)}{dt} = -2\\mathbf{r}(t)\n$$\n该方程在初始条件 $\\mathbf{r}(0) = \\mathbf{r}_0 = (x_0, y_0)$ 下求解。解为：\n$$\n\\mathbf{r}(t) = \\mathbf{r}_0 \\exp(-2t)\n$$\n这个结果表明，对于任何时间 $t \\geq 0$，位置向量 $\\mathbf{r}(t)$ 是初始位置向量 $\\mathbf{r}_0$ 的一个标量倍。该标量是 $\\exp(-2t)$。因此，整个轨迹 $\\mathbf{r}(t)$ 位于穿过原点 $(0,0)$ 和初始点 $\\mathbf{r}_0$ 的直线上。当 $t \\to \\infty$ 时，$\\exp(-2t) \\to 0$，因此 $\\mathbf{r}(t) \\to (0,0)$，沿着这条线趋近于原点的最小值。\n\n接下来，我们分析离散SD迭代点 $\\{\\mathbf{r}_k\\}$。更新规则由下式给出：\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\nabla E(\\mathbf{r}_k)\n$$\n其中 $\\alpha_k \\ge 0$ 是步长。代入梯度 $\\nabla E(\\mathbf{r}_k) = 2\\mathbf{r}_k$：\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k (2\\mathbf{r}_k) = (1 - 2\\alpha_k)\\mathbf{r}_k\n$$\n这是一个递归关系。我们从初始点 $\\mathbf{r}_0$ 开始。\n第一个迭代点是 $\\mathbf{r}_1 = (1 - 2\\alpha_0)\\mathbf{r}_0$。这表明 $\\mathbf{r}_1$ 是 $\\mathbf{r}_0$ 的一个标量倍，因此位于穿过原点的同一条直线上。\n第二个迭代点是 $\\mathbf{r}_2 = (1 - 2\\alpha_1)\\mathbf{r}_1 = (1 - 2\\alpha_1)(1 - 2\\alpha_0)\\mathbf{r}_0$。同样，$\\mathbf{r}_2$ 是 $\\mathbf{r}_0$ 的一个标量倍。\n通过数学归纳法，如果我们假设 $\\mathbf{r}_k$ 是 $\\mathbf{r}_0$ 的一个标量倍，那么表达式 $\\mathbf{r}_{k+1} = (1 - 2\\alpha_k)\\mathbf{r}_k$ 表明 $\\mathbf{r}_{k+1}$ 也是 $\\mathbf{r}_0$ 的一个标量倍。由于当 $k=0$ 时基本情况成立（因为 $\\mathbf{r}_0=1\\cdot\\mathbf{r}_0$），所有迭代点 $\\{\\mathbf{r}_k\\}$ 都保持在穿过原点和 $\\mathbf{r}_0$ 的直线上。\n\n最后，我们确定从 $\\mathbf{r}_0 = (x_0, y_0)$ 开始的第一个SD步骤的最优步长 $\\alpha_*$。精确线搜索要求我们找到使函数 $g(\\alpha) = E(\\mathbf{r}_0 - \\alpha\\nabla E(\\mathbf{r}_0))$ 最小化的 $\\alpha \\ge 0$ 的值。\n设步长为 $\\alpha$ 之后的点为 $\\mathbf{r}(\\alpha) = \\mathbf{r}_0 - \\alpha \\nabla E(\\mathbf{r}_0)$。\n代入 $\\nabla E(\\mathbf{r}_0) = 2\\mathbf{r}_0$：\n$$\n\\mathbf{r}(\\alpha) = \\mathbf{r}_0 - \\alpha (2\\mathbf{r}_0) = (1 - 2\\alpha)\\mathbf{r}_0\n$$\n现在我们计算这个新点的能量 $E$：\n$$\ng(\\alpha) = E(\\mathbf{r}(\\alpha)) = E((1 - 2\\alpha)\\mathbf{r}_0) = \\|(1 - 2\\alpha)\\mathbf{r}_0\\|^2\n$$\n使用范数的性质 $\\|c\\mathbf{v}\\| = |c|\\|\\mathbf{v}\\|$：\n$$\ng(\\alpha) = |1 - 2\\alpha|^2 \\|\\mathbf{r}_0\\|^2 = (1 - 2\\alpha)^2 \\|\\mathbf{r}_0\\|^2\n$$\n由于 $\\|\\mathbf{r}_0\\|^2 = x_0^2 + y_0^2 = E(\\mathbf{r}_0)$，我们有：\n$$\ng(\\alpha) = (1 - 2\\alpha)^2 E(\\mathbf{r}_0)\n$$\n为了找到 $g(\\alpha)$ 相对于 $\\alpha$ 的最小值，我们计算其导数并令其为零。\n$$\n\\frac{dg}{d\\alpha} = \\frac{d}{d\\alpha} \\left[ (1 - 2\\alpha)^2 E(\\mathbf{r}_0) \\right] = 2(1 - 2\\alpha)(-2) E(\\mathbf{r}_0) = -4(1 - 2\\alpha) E(\\mathbf{r}_0)\n$$\n将导数设为零：\n$$\n-4(1 - 2\\alpha) E(\\mathbf{r}_0) = 0\n$$\n问题陈述 $\\mathbf{r}_0 \\neq (0,0)$，这意味着 $E(\\mathbf{r}_0) = x_0^2 + y_0^2  0$。因此，因子 $E(\\mathbf{r}_0)$ 非零，我们必须有：\n$$\n1 - 2\\alpha = 0\n$$\n求解 $\\alpha$ 得到最优值 $\\alpha_*$：\n$$\n2\\alpha = 1 \\implies \\alpha_* = \\frac{1}{2}\n$$\n该值满足条件 $\\alpha \\ge 0$。为确认这是一个最小值，我们检查二阶导数：\n$$\n\\frac{d^2g}{d\\alpha^2} = \\frac{d}{d\\alpha} \\left[ (-4 + 8\\alpha) E(\\mathbf{r}_0) \\right] = 8 E(\\mathbf{r}_0)\n$$\n由于 $E(\\mathbf{r}_0)  0$，二阶导数为正，这证实了 $\\alpha_* = \\frac{1}{2}$ 对应一个局部最小值。由于 $g(\\alpha)$ 是关于 $\\alpha$ 的抛物线，这也是一个全局最小值。\n第一个SD步骤的最优步长是 $\\frac{1}{2}$。",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "虽然最速下降法很直观，但它在狭长势能“山谷”中收敛缓慢是一个主要缺点，而共轭梯度法正是为解决此问题而生。本次计算练习将让这两种算法在一系列难度递增（通过矩阵条件数控制）的二次型问题上展开直接较量 。通过比较收敛所需的迭代次数，你将定量地见证共轭梯度法在处理计算化学中常见的病态问题时所展现出的巨大性能优势。",
            "id": "2463021",
            "problem": "给定二次能量函数 $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$，其中 $\\mathbf{A}$ 是一个大小为 $n \\times n$ 的实对称正定矩阵。考虑两种用于寻找 $E(\\mathbf{x})$ 最小化子的迭代方法：最速下降法 (Steepest Descent, SD) 和共轭梯度法 (Conjugate Gradient, CG)。对于每种方法，给定 $\\mathbf{A}$ 和初始点 $\\mathbf{x}_0$，迭代次数定义为满足以下基于残差的停止准则的最小非负整数 $m$：\n$$\n\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon\n$$\n其中 $\\varepsilon$ 是一个预设的容差，$\\lVert \\cdot \\rVert_2$ 是欧几里得范数。如果在预设的迭代上限 $m_{\\max}$ 内仍未满足该准则，则将 $m_{\\max}$ 报告为迭代次数。\n\n通过设置 $n = 60$，选择一个所有分量均为 $1$ 的初始向量 $\\mathbf{x}_0 \\in \\mathbb{R}^n$，并对于给定的条件数 $\\kappa \\ge 1$，将 $\\mathbf{A}_\\kappa$ 的对角元素 $\\lambda_i$ 定义为一个从 $1$ 到 $\\kappa$ 的几何级数，来构建一系列具有指定条件数的对角测试矩阵 $\\mathbf{A}_\\kappa$：\n$$\n\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}, \\quad i = 1,2,\\dots,n,\n$$\n因此 $\\mathbf{A}_\\kappa = \\mathrm{diag}(\\lambda_1,\\lambda_2,\\dots,\\lambda_n)$ 的条件数为 $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\kappa$。两种方法均使用基于残差的停止容差 $\\varepsilon = 10^{-8}$ 和迭代上限 $m_{\\max} = 100000$。\n\n测试套件：\n- 矩阵维度：$n = 60$。\n- 初始向量：$\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{60}$。\n- 容差：$\\varepsilon = 10^{-8}$。\n- 迭代上限：$m_{\\max} = 100000$。\n- 待测试的条件数：$\\kappa \\in \\{1,5,20,100,500\\}$。\n\n对于测试套件中的每个 $\\kappa$，计算：\n- 最速下降法满足停止准则所需的迭代次数 $m_{\\mathrm{SD}}(\\kappa)$（如果在上限内未满足，则为 $m_{\\max}$）。\n- 共轭梯度法满足停止准则所需的迭代次数 $m_{\\mathrm{CG}}(\\kappa)$（如果在上限内未满足，则为 $m_{\\max}$）。\n\n最终输出格式：\n您的程序应生成单行输出，该输出是一个用方括号括起来的逗号分隔列表，每个测试用例对应一对内部方括号。每个内部对方括号内必须是两个整数 $[m_{\\mathrm{SD}}(\\kappa),m_{\\mathrm{CG}}(\\kappa)]$，其顺序与列出的测试条件数顺序相同，并且行内任何地方都不能有空格。具体来说，对于指定的五个测试用例，要求的输出格式是形如下面的单行：\n$$\n\\bigl[\\,[m_{\\mathrm{SD}}(1),m_{\\mathrm{CG}}(1)],\\,[m_{\\mathrm{SD}}(5),m_{\\mathrm{CG}}(5)],\\,[m_{\\mathrm{SD}}(20),m_{\\mathrm{CG}}(20)],\\,[m_{\\mathrm{SD}}(100),m_{\\mathrm{CG}}(100)],\\,[m_{\\mathrm{SD}}(500),m_{\\mathrm{CG}}(500)]\\,\\bigr].\n$$\n条目必须是整数，并且必须严格按照所示格式显示，只包含方括号和逗号，不含任何空格。",
            "solution": "对问题陈述进行严格分析后，认定其有效。\n\n**1. 问题验证**\n\n**步骤 1：提取已知条件**\n- 能量函数：$E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$。\n- 矩阵 $\\mathbf{A}$：一个大小为 $n \\times n$ 的实对称正定矩阵。\n- 迭代方法：最速下降法 (SD) 和共轭梯度法 (CG)。\n- 迭代次数 $m$：满足停止准则的最小非负整数。\n- 停止准则：$\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon$。\n- 容差：$\\varepsilon = 10^{-8}$。\n- 迭代上限：$m_{\\max} = 100000$。\n- 测试矩阵构造：对于一个条件数 $\\kappa$，$\\mathbf{A}_\\kappa = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$，其中 $\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}$，$i=1, \\dots, n$。\n- 矩阵维度：$n = 60$。\n- 初始向量：$\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}}$。\n- 测试套件：$\\kappa \\in \\{1, 5, 20, 100, 500\\}$。\n- 任务：计算每个 $\\kappa$ 对应的迭代次数 $m_{\\mathrm{SD}}(\\kappa)$ 和 $m_{\\mathrm{CG}}(\\kappa)$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是数值线性代数中的一个标准练习，旨在比较两种基本优化算法在定义明确的二次型上的收敛速度。最小化 $E(\\mathbf{x})$ 等价于求解线性系统 $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$。由于 $\\mathbf{A}$ 是正定的，该系统的唯一解为 $\\mathbf{x} = \\mathbf{0}$。基于残差的停止准则是迭代方法中的标准做法。\n- **适定性：** 该问题是适定的。测试矩阵的构造是明确的。对角元素 $\\lambda_i$ 构成一个从 $\\lambda_1 = \\kappa^{\\frac{0}{n-1}} = 1$ 到 $\\lambda_n = \\kappa^{\\frac{n-1}{n-1}} = \\kappa$ 的几何级数。对于对称正定矩阵，其 2-范数条件数是最大特征值与最小特征值之比，即 $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\kappa}{1} = \\kappa$。因此，该构造是正确的。所有参数（$n, \\mathbf{x}_0, \\varepsilon, m_{\\max}, \\kappa$ 的值）都已明确给出。解的存在性和唯一性得到保证。\n- **客观性：** 该问题以精确、客观的数学语言表述，没有歧义或主观看法。\n\n**步骤 3：结论与行动**\n该问题科学合理、适定且客观，因此是有效的。将提供一个解决方案。\n\n**2. 求解推导**\n\n该问题要求最小化二次函数 $E(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\mathsf{T}}\\mathbf{A}\\mathbf{x}$。该函数的最小值出现在其关于 $\\mathbf{x}$ 的梯度为零的点。梯度由下式给出：\n$$\n\\nabla E(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\n$$\n将梯度设为零，我们得到线性系统 $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$。由于矩阵 $\\mathbf{A}$ 被指定为正定矩阵，因此它是可逆的，且 $E(\\mathbf{x})$ 的唯一最小化子是平凡解 $\\mathbf{x}^* = \\mathbf{0}$。\n\n问题在于，从 $\\mathbf{x}_0 = (1,1,\\dots,1)^{\\mathsf{T}}$ 出发，求最速下降法 (SD) 和共轭梯度法 (CG) 收敛到此解所需的迭代次数。收敛性由停止准则 $\\frac{\\lVert \\mathbf{A}\\mathbf{x}_m \\rVert_2}{\\lVert \\mathbf{A}\\mathbf{x}_0 \\rVert_2} \\le \\varepsilon$ 来衡量。向量 $\\mathbf{r}_m = \\mathbf{0} - \\mathbf{A}\\mathbf{x}_m = -\\mathbf{A}\\mathbf{x}_m$ 是系统 $\\mathbf{A}\\mathbf{x} = \\mathbf{0}$ 的残差。因此，该准则等价于一个相对残差停止准则：$\\frac{\\lVert \\mathbf{r}_m \\rVert_2}{\\lVert \\mathbf{r}_0 \\rVert_2} \\le \\varepsilon$。\n\n测试矩阵 $\\mathbf{A}_\\kappa$ 是对角矩阵，其元素为 $\\lambda_i = \\kappa^{\\frac{i-1}{n-1}}$，$i=1, \\dots, n=60$。这种构造确保了特征值在 $\\lambda_{\\min} = 1$ 和 $\\lambda_{\\max} = \\kappa$ 之间呈对数间隔分布，从而得到一个具有指定条件数 $\\mathrm{cond}_2(\\mathbf{A}_\\kappa) = \\kappa$ 的矩阵。\n\n**最速下降法 (SD) 算法**\n最速下降法是一种迭代优化算法，每一步都沿着负梯度方向移动。对于函数 $E(\\mathbf{x})$，在迭代点 $\\mathbf{x}_k$ 处的搜索方向是 $\\mathbf{p}_k = -\\nabla E(\\mathbf{x}_k) = -\\mathbf{A}\\mathbf{x}_k$。下一个迭代点通过线搜索找到：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。使 $E(\\mathbf{x}_{k+1})$ 最小化的最优步长 $\\alpha_k$ 由下式给出：\n$$\n\\alpha_k = \\frac{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{p}_k}{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{p}_k} = \\frac{(-\\mathbf{A}\\mathbf{x}_k)^{\\mathsf{T}}(-\\mathbf{A}\\mathbf{x}_k)}{(-\\mathbf{A}\\mathbf{x}_k)^{\\mathsf{T}}\\mathbf{A}(-\\mathbf{A}\\mathbf{x}_k)}\n$$\n如果我们定义残差 $\\mathbf{r}_k = -\\mathbf{A}\\mathbf{x}_k$，则算法简化为：\n1. 初始化 $\\mathbf{x}_0$, $\\mathbf{r}_0 = -\\mathbf{A}\\mathbf{x}_0$。\n2. 对于 $k=0, 1, 2, \\dots$：\n   a. 计算步长：$\\alpha_k = \\frac{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{r}_k}$。\n   b. 更新解：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{r}_k$。\n   c. 更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A}\\mathbf{r}_k$。\n   d. 检查停止准则。\n\n**共轭梯度法 (CG) 算法**\n共轭梯度法通过选择 $\\mathbf{A}$-正交（或“共轭”）的搜索方向，对最速下降法进行了改进。这可以防止破坏先前方向上取得的最小化进展，并保证在精确算术中最多 $n$ 步收敛。\n1. 初始化 $\\mathbf{x}_0$，$\\mathbf{r}_0 = -\\mathbf{A}\\mathbf{x}_0$，$\\mathbf{p}_0 = \\mathbf{r}_0$。\n2. 对于 $k=0, 1, 2, \\dots$：\n   a. 计算步长：$\\alpha_k = \\frac{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}{\\mathbf{p}_k^{\\mathsf{T}}\\mathbf{A}\\mathbf{p}_k}$。\n   b. 更新解：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n   c. 更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k \\mathbf{A}\\mathbf{p}_k$。\n   d. 检查停止准则。\n   e. 计算改进因子：$\\beta_k = \\frac{\\mathbf{r}_{k+1}^{\\mathsf{T}}\\mathbf{r}_{k+1}}{\\mathbf{r}_k^{\\mathsf{T}}\\mathbf{r}_k}$。\n   f. 更新搜索方向：$\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$。\n\n实现将遵循这些算法。它将从 $m=1$ 开始，对每个迭代点 $\\mathbf{x}_m$ 检查停止准则，并报告第一个满足该准则的 $m$ 值。如果循环完成，则报告 $m_{\\max}$。由于矩阵 $\\mathbf{A}_\\kappa$ 是对角矩阵，矩阵-向量积 $\\mathbf{A}\\mathbf{v}$ 可以通过对角线元素（特征值）与向量 $\\mathbf{v}$ 进行逐元素相乘来高效计算。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_sd(lambdas, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=0 using Steepest Descent for a diagonal matrix A.\n\n    Args:\n        lambdas (np.ndarray): Diagonal entries of matrix A.\n        x0 (np.ndarray): Initial vector.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required for convergence.\n    \"\"\"\n    x = x0.copy()\n    \n    # The term in the stopping criterion is v_m = A @ x_m.\n    # We use the fact that r_m = -v_m for the system Ax=0.\n    v0 = lambdas * x0\n    norm_v0 = np.linalg.norm(v0)\n    \n    # If x0 is the solution, iterations = 0.\n    if norm_v0 == 0:\n        return 0\n\n    # For SD, the search direction is the residual r.\n    # For Ax=0, r = 0 - Ax = -Ax.\n    r = -v0\n    \n    for m in range(1, max_iter + 1):\n        # Efficiently compute A @ r since A is diagonal\n        Ar = lambdas * r\n        \n        # Calculate optimal step size alpha\n        # np.dot is used for vector dot products.\n        r_sq_norm = np.dot(r, r)\n        alpha = r_sq_norm / np.dot(r, Ar)\n        \n        # Update solution and residual\n        x += alpha * r\n        r -= alpha * Ar\n        \n        # Check stopping criterion: ||A*x_m|| / ||A*x_0|| = tol\n        # Since r_m = -A*x_m, we can use norm(r) which is ||r_m||.\n        norm_v_m = np.linalg.norm(r)\n        \n        if norm_v_m / norm_v0 = tol:\n            return m\n            \n    return max_iter\n\ndef solve_cg(lambdas, x0, tol, max_iter):\n    \"\"\"\n    Solves Ax=0 using Conjugate Gradient for a diagonal matrix A.\n\n    Args:\n        lambdas (np.ndarray): Diagonal entries of matrix A.\n        x0 (np.ndarray): Initial vector.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required for convergence.\n    \"\"\"\n    x = x0.copy()\n    \n    v0 = lambdas * x0\n    norm_v0 = np.linalg.norm(v0)\n    \n    if norm_v0 == 0:\n        return 0\n    \n    r = -v0\n    p = r.copy() # Initial search direction\n    rs_old = np.dot(r, r)\n    \n    for m in range(1, max_iter + 1):\n        # Efficiently compute A @ p\n        Ap = lambdas * p\n        \n        alpha = rs_old / np.dot(p, Ap)\n        \n        # Update solution and residual\n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        \n        # Check stopping criterion using the norm of the new residual\n        norm_v_m = np.sqrt(rs_new)\n        if norm_v_m / norm_v0 = tol:\n            return m\n            \n        # Update search direction\n        beta = rs_new / rs_old\n        p = r + beta * p\n        \n        rs_old = rs_new\n            \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [1.0, 5.0, 20.0, 100.0, 500.0]\n\n    # Parameters from problem description\n    n = 60\n    x0 = np.ones(n)\n    tol = 1e-8\n    max_iter = 100000\n    \n    results = []\n    for kappa in test_cases:\n        # Construct the diagonal entries (eigenvalues) of matrix A\n        if kappa == 1.0:\n            lambdas = np.ones(n)\n        else:\n            i = np.arange(1, n + 1)\n            power = (i - 1) / (n - 1)\n            lambdas = np.power(kappa, power)\n        \n        # Run Steepest Descent and Conjugate Gradient methods\n        m_sd = solve_sd(lambdas, x0, tol, max_iter)\n        m_cg = solve_cg(lambdas, x0, tol, max_iter)\n        \n        result_str = f\"[{m_sd},{m_cg}]\"\n        results.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}