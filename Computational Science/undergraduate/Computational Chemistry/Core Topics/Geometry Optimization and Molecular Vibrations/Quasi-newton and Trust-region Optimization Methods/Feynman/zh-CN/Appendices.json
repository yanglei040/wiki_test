{
    "hands_on_practices": [
        {
            "introduction": "拟牛顿法的心脏是其海森矩阵的近似更新公式。本练习将引导你亲手实现并比较经典的 DFP (Davidon–Fletcher–Powell) 更新和现代计算化学软件中作为标准的 BFGS (Broyden–Fletcher–Goldfarb–Shanno) 更新。通过这个实践，你将直接体验到为何 BFGS 方法在几何优化中表现得更为稳健和广泛应用 。",
            "id": "2461204",
            "problem": "您必须编写一个完整且可运行的程序，在一个旨在以约化单位模拟耦合键伸缩和角弯曲的二维光滑势能面上，比较两种拟牛顿优化方法。设决策变量为$u$（一个约化的键长位移）和$v$（一个以弧度为单位的约化角位移）。定义势能函数\n$$\nV(u,v) \\;=\\; \\tfrac{1}{2}\\,k_r\\,(u - r_0)^2 \\;+\\; \\tfrac{1}{2}\\,k_{\\theta}\\,(v - \\theta_0)^2 \\;+\\; k_c\\,(u - r_0)^2\\,(v - \\theta_0) \\;+\\; \\gamma\\,\\big(u - r_0 - \\alpha\\,(v - \\theta_0)^2\\big)^2,\n$$\n其中$k_r$、$k_{\\theta}$、$k_c$、$\\gamma$、$\\alpha$、$r_0$和$\\theta_0$是实数参数。所有量都被视为无量纲的约化单位，其中角位移$v$以弧度为单位。\n\n您的程序必须在每次迭代$k$中使用两种不同的逆海森矩阵更新公式，从给定的起始点在$\\mathbb{R}^2$中最小化$V(u,v)$：\n- 用于逆海森矩阵近似$H_k \\in \\mathbb{R}^{2 \\times 2}$的 Davidon–Fletcher–Powell (DFP) 更新，\n$$\nH_{k+1} \\;=\\; H_k \\;+\\; \\frac{s_k s_k^{\\mathsf{T}}}{s_k^{\\mathsf{T}} y_k} \\;-\\; \\frac{H_k y_k y_k^{\\mathsf{T}} H_k}{y_k^{\\mathsf{T}} H_k y_k},\n$$\n- 用于逆海森矩阵近似$H_k$的 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 更新，\n$$\nH_{k+1} \\;=\\; \\big(I - \\rho_k s_k y_k^{\\mathsf{T}}\\big)\\,H_k\\,\\big(I - \\rho_k y_k s_k^{\\mathsf{T}}\\big) \\;+\\; \\rho_k\\, s_k s_k^{\\mathsf{T}}, \\quad \\rho_k \\;=\\; \\frac{1}{y_k^{\\mathsf{T}} s_k},\n$$\n其中$s_k = x_{k+1} - x_k$且$y_k = \\nabla V(x_{k+1}) - \\nabla V(x_k)$，而$x_k = \\begin{bmatrix}u_k \\\\ v_k\\end{bmatrix}$，$I$是$2 \\times 2$的单位矩阵。在每次迭代中，搜索方向必须是$p_k = - H_k \\nabla V(x_k)$，步进为$x_{k+1} = x_k + \\alpha_k p_k$，其中$\\alpha_k > 0$是某个标量步长。如果对于指定的容差$\\varepsilon$，满足$\\lVert \\nabla V(x_k)\\rVert_2 \\leq \\varepsilon$，则算法必须成功终止；否则，如果超过指定的最大迭代次数，则视为未能收敛。\n\n您必须将这两种更新公式应用于以下三个测试案例中的每一个。在所有案例中，使用相同的终止容差$\\varepsilon = 10^{-5}$和欧几里得范数。必须使用单位矩阵作为初始逆海森矩阵近似$H_0$。\n\n测试套件：\n1. 案例 A (良态谷，一般顺利路径)：参数$k_r = 1$、$k_{\\theta} = 1$、$k_c = 0.1$、$\\gamma = 100$、$\\alpha = 1$、$r_0 = 0$、$\\theta_0 = 0$；起始点$x_0 = \\begin{bmatrix}-1.5 \\\\ 1.0\\end{bmatrix}$；最大迭代次数$N_{\\max} = 200$。\n2. 案例 B (旨在对更新施加压力的高度弯曲的各向异性谷)：参数$k_r = 1$、$k_{\\theta} = 1$、$k_c = 0.2$、$\\gamma = 10^6$、$\\alpha = 10$、$r_0 = 0$、$\\theta_0 = 0$；起始点$x_0 = \\begin{bmatrix}3.0 \\\\ -2.0\\end{bmatrix}$；最大迭代次数$N_{\\max} = 60$。\n3. 案例 C (最小值点处的边界条件)：参数$k_r = 1$、$k_{\\theta} = 1$、$k_c = 0$、$\\gamma = 50$、$\\alpha = 0.5$、$r_0 = 0$、$\\theta_0 = 0$；起始点$x_0 = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$；最大迭代次数$N_{\\max} = 1$。\n\n对于每个案例，您的程序必须返回两个布尔值，分别指示基于 DFP 的优化是否收敛和基于 BFGS 的优化是否收敛，这取决于是否在迭代次数上限内满足停止准则$\\lVert \\nabla V(x_k)\\rVert_2 \\leq \\varepsilon$。将案例 A、B 和 C 的六个布尔结果按顺序聚合到单行输出中，格式为 Python 风格的列表字面量，元素之间用逗号分隔，无空格，即\n- 输出格式：单行包含 [bA_DFP,bA_BFGS,bB_DFP,bB_BFGS,bC_DFP,bC_BFGS]，\n其中每个 b... 是 True 或 False。\n\n角度必须以弧度为单位进行解释。输出中不需要其他物理单位。您必须确保程序的输出在单个打印语句中与指定格式完全匹配。",
            "solution": "所述问题构成了数值优化领域中一个有效且定义明确的任务，特别是在计算化学中用于势能面最小化的背景下。在进行求解之前，有必要进行一项关键的验证。\n\n给定条件是：\n- 势能函数$V(u,v) = \\tfrac{1}{2}\\,k_r\\,(u - r_0)^2 + \\tfrac{1}{2}\\,k_{\\theta}\\,(v - \\theta_0)^2 + k_c\\,(u - r_0)^2\\,(v - \\theta_0) + \\gamma\\,\\big(u - r_0 - \\alpha\\,(v - \\theta_0)^2\\big)^2$。\n- 两种用于逆海森矩阵近似$H_k$的拟牛顿更新公式：Davidon–Fletcher–Powell (DFP) 和 Broyden–Fletcher–Goldfarb–Shanno (BFGS)。\n- 一个迭代方案：$x_{k+1} = x_k + \\alpha_k p_k$，搜索方向为$p_k = - H_k \\nabla V(x_k)$。\n- 初始逆海森矩阵：$H_0 = I$。\n- 终止准则：$\\lVert \\nabla V(x_k)\\rVert_2 \\leq \\varepsilon$，其中$\\varepsilon = 10^{-5}$。\n- 三个测试案例（A、B、C），具有指定的参数（$k_r, k_{\\theta}, k_c, \\gamma, \\alpha, r_0, \\theta_0$）、起始点$x_0$和最大迭代次数$N_{\\max}$。\n\n该问题具有科学依据，采用了计算科学中的标准模型和算法。然而，陈述“步进$x_{k+1} = x_k + \\alpha_k p_k$，其中$\\alpha_k > 0$是某个标量步长”是不完整的。确定$\\alpha_k$的方法未被指定。为了使拟牛顿方法具有鲁棒性，并使 DFP 和 BFGS 之间的比较有意义，必须通过线搜索过程系统地选择$\\alpha_k$。一个朴素的选择（例如，固定的$\\alpha_k=1$）将导致性能不佳或发散，从而使比较无效。因此，假定必须使用标准的线搜索算法来找到一个满足强 Wolfe 条件的步长$\\alpha_k$。这既能确保势能的充分下降，又能保证曲率条件$y_k^{\\mathsf{T}} s_k > 0$成立，这对于 BFGS 更新的稳定性和正定性至关重要。这一假设使得问题是适定且可解的。所提供的测试案例是客观且可验证的。案例 C 的起始点即为最小值点，它正确地测试了在迭代$k=0$时的终止逻辑。因此，在这一必要且标准的假设下，该问题被验证为可解。\n\n解决方案需要实现一个拟牛顿优化框架。该算法的核心是迭代地构建海森矩阵的逆矩阵的近似$H_k \\approx (\\nabla^2 V(x_k))^{-1}$，以指导搜索势能函数$V(x)$的最小值，其中$x = [u, v]^{\\mathsf{T}}$。\n\n首先，我们必须推导势能函数$V(u,v)$的解析梯度。设$\\Delta u = u - r_0$和$\\Delta v = v - \\theta_0$。函数为：\n$$V(u,v) = \\tfrac{1}{2} k_r (\\Delta u)^2 + \\tfrac{1}{2} k_\\theta (\\Delta v)^2 + k_c (\\Delta u)^2 (\\Delta v) + \\gamma (\\Delta u - \\alpha (\\Delta v)^2)^2$$\n梯度$\\nabla V(u,v) = \\begin{bmatrix} \\partial V / \\partial u \\\\ \\partial V / \\partial v \\end{bmatrix}$为：\n$$ \\frac{\\partial V}{\\partial u} = k_r (\\Delta u) + 2 k_c (\\Delta u)(\\Delta v) + 2\\gamma\\big(\\Delta u - \\alpha (\\Delta v)^2\\big) $$\n$$ \\frac{\\partial V}{\\partial v} = k_\\theta (\\Delta v) + k_c (\\Delta u)^2 - 4\\alpha\\gamma (\\Delta v)\\big(\\Delta u - \\alpha (\\Delta v)^2\\big) $$\n\n每种方法（DFP 和 BFGS）的优化算法如下：\n1.  初始化迭代计数器$k = 0$、位置向量$x_0$和逆海森矩阵近似$H_0 = I$，其中$I$是$2 \\times 2$的单位矩阵。\n2.  计算初始梯度$\\nabla V(x_0)$。如果其欧几里得范数$\\lVert \\nabla V(x_0) \\rVert_2 \\leq \\varepsilon = 10^{-5}$，则过程成功终止。测试案例 C 即属此种情况。\n3.  对于$k = 0, 1, \\dots, N_{\\max}-1$：\n    a. 计算搜索方向：$p_k = -H_k \\nabla V(x_k)$。\n    b. 执行线搜索以找到一个合适的步长$\\alpha_k > 0$。搜索必须找到一个满足强 Wolfe 条件的$\\alpha_k$，这保证了$V$的充分下降和曲率条件的满足。如果线搜索未能找到这样的步长，则优化失败。\n    c. 更新位置：$x_{k+1} = x_k + \\alpha_k p_k$。\n    d. 定义位置变化$s_k = x_{k+1} - x_k$和梯度变化$y_k = \\nabla V(x_{k+1}) - \\nabla V(x_k)$。\n    e. 曲率条件$y_k^{\\mathsf{T}} s_k > 0$由成功的线搜索所保证。该量用于两种更新公式的分母中。\n    f. 使用 DFP 或 BFGS 公式将逆海森矩阵近似$H_k$更新为$H_{k+1}$。\n        -   **DFP 更新：**\n            $$ H_{k+1} = H_k + \\frac{s_k s_k^{\\mathsf{T}}}{s_k^{\\mathsf{T}} y_k} - \\frac{H_k y_k y_k^{\\mathsf{T}} H_k}{y_k^{\\mathsf{T}} H_k y_k} $$\n            DFP 更新在数值上很敏感，并且可能会失去正定性。实现了一个保险措施：如果分母$y_k^{\\mathsf{T}} H_k y_k$接近于零或为负，则跳过更新，并设置$H_{k+1} = H_k$。\n        -   **BFGS 更新：**\n            $$ H_{k+1} = \\big(I - \\rho_k s_k y_k^{\\mathsf{T}}\\big)\\,H_k\\,\\big(I - \\rho_k y_k s_k^{\\mathsf{T}}\\big) + \\rho_k\\, s_k s_k^{\\mathsf{T}}, \\quad \\text{其中 } \\rho_k = \\frac{1}{y_k^{\\mathsf{T}} s_k} $$\n            已知 BFGS 更新更为鲁棒，并且如果$H_0$是正定的且曲率条件成立，它能保持$H_k$的正定性。\n    g. 检查终止条件：如果$\\lVert \\nabla V(x_{k+1}) \\rVert_2 \\leq \\varepsilon$，则过程成功终止。请注意，我们在开始下一次迭代之前检查*新*点的梯度。我的实现在循环开始时检查点$x_k$的梯度，这与前述做法是等效的。\n4.  如果循环完成而未满足收敛标准，则表示优化未能在$N_{\\max}$次迭代内收敛。\n\n程序将使用 DFP 和 BFGS 更新对三个测试案例中的每一个执行此算法，并为六次运行中的每一次报告一个布尔值，以指示成功（True）或失败（False）。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef get_potential_and_gradient(params):\n    \"\"\"\n    Creates the potential energy function V and its gradient grad_V for a given set of parameters.\n    \"\"\"\n    kr, ktheta, kc, gamma, alpha, r0, theta0 = params\n\n    def V(x):\n        u, v = x\n        du = u - r0\n        dv = v - theta0\n        term1 = 0.5 * kr * du**2\n        term2 = 0.5 * ktheta * dv**2\n        term3 = kc * du**2 * dv\n        term4 = gamma * (du - alpha * dv**2)**2\n        return term1 + term2 + term3 + term4\n\n    def grad_V(x):\n        u, v = x\n        du = u - r0\n        dv = v - theta0\n        \n        common_term = du - alpha * dv**2\n        \n        # dV/du\n        grad_u = kr * du + 2.0 * kc * du * dv + 2.0 * gamma * common_term\n        \n        # dV/dv\n        grad_v = ktheta * dv + kc * du**2 - 4.0 * alpha * gamma * dv * common_term\n        \n        return np.array([grad_u, grad_v], dtype=float)\n        \n    return V, grad_V\n\ndef quasi_newton_optimizer(potential_params, x0, n_max, epsilon, update_formula):\n    \"\"\"\n    Performs minimization of a potential energy surface using a quasi-Newton method.\n\n    Args:\n        potential_params: Tuple of parameters for the potential function.\n        x0: Initial guess for the coordinates [u, v].\n        n_max: Maximum number of iterations.\n        epsilon: Convergence tolerance for the gradient norm.\n        update_formula: String specifying the update formula ('DFP' or 'BFGS').\n\n    Returns:\n        bool: True if converged, False otherwise.\n    \"\"\"\n    V, grad_V = get_potential_and_gradient(potential_params)\n    \n    x_k = np.array(x0, dtype=float)\n    H_k = np.identity(2, dtype=float)\n    \n    # Check for convergence at the starting point\n    grad_k = grad_V(x_k)\n    if np.linalg.norm(grad_k) <= epsilon:\n        return True\n\n    for k in range(n_max):\n        # Compute search direction\n        p_k = -np.dot(H_k, grad_k)\n        \n        # Perform line search to find step length alpha_k\n        # scipy.optimize.line_search finds a step satisfying the Strong Wolfe conditions\n        alpha_k, _, _, _, _, _ = line_search(V, grad_V, x_k, p_k, gfk=grad_k, maxiter=100)\n        \n        # If line search fails, optimization has failed\n        if alpha_k is None:\n            return False\n            \n        # Update position\n        s_k = alpha_k * p_k\n        x_k_plus_1 = x_k + s_k\n        \n        # Calculate new gradient and check for convergence\n        grad_k_plus_1 = grad_V(x_k_plus_1)\n        if np.linalg.norm(grad_k_plus_1) <= epsilon:\n            return True\n            \n        # Define y_k for the Hessian update\n        y_k = grad_k_plus_1 - grad_k\n        \n        # The curvature condition y_k.T @ s_k > 0 is guaranteed by the Wolfe conditions\n        # from a successful line search. The denominator will be positive.\n        denom_sy = np.dot(y_k.T, s_k)\n\n        # Avoid updating if curvature condition is not sufficiently positive\n        if denom_sy <= 1e-9:\n             x_k = x_k_plus_1\n             grad_k = grad_k_plus_1\n             continue\n\n        # Update inverse Hessian approximation H_k\n        if update_formula == 'DFP':\n            term1_numerator = np.outer(s_k, s_k)\n            term1 = term1_numerator / denom_sy\n            \n            Hy = np.dot(H_k, y_k)\n            yHy = np.dot(y_k.T, Hy)\n            \n            # Safeguard for DFP: skip update if H_k is not positive definite\n            # along the y_k direction.\n            if yHy <= 1e-9:\n                H_k_plus_1 = H_k\n            else:\n                term2_numerator = np.outer(Hy, Hy)\n                term2 = term2_numerator / yHy\n                H_k_plus_1 = H_k + term1 - term2\n        \n        elif update_formula == 'BFGS':\n            rho_k = 1.0 / denom_sy\n            I = np.identity(2, dtype=float)\n            \n            # Use the more stable implementation for BFGS update\n            term_mat = I - rho_k * np.outer(s_k, y_k)\n            H_k_plus_1 = np.dot(term_mat, np.dot(H_k, term_mat.T)) + rho_k * np.outer(s_k, s_k)\n        else:\n            raise ValueError(\"Invalid update formula specified.\")\n            \n        # Prepare for next iteration\n        x_k = x_k_plus_1\n        grad_k = grad_k_plus_1\n        H_k = H_k_plus_1\n\n    # If loop finishes, convergence was not reached within n_max iterations\n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'params': (1.0, 1.0, 0.1, 100.0, 1.0, 0.0, 0.0), 'x0': [-1.5, 1.0], 'N_max': 200},\n        # Case B\n        {'params': (1.0, 1.0, 0.2, 1e6, 10.0, 0.0, 0.0), 'x0': [3.0, -2.0], 'N_max': 60},\n        # Case C\n        {'params': (1.0, 1.0, 0.0, 50.0, 0.5, 0.0, 0.0), 'x0': [0.0, 0.0], 'N_max': 1}\n    ]\n    \n    epsilon = 1e-5\n    results = []\n\n    for case in test_cases:\n        # Run DFP optimizer\n        converged_dfp = quasi_newton_optimizer(case['params'], case['x0'], case['N_max'], epsilon, 'DFP')\n        results.append(converged_dfp)\n        \n        # Run BFGS optimizer\n        converged_bfgs = quasi_newton_optimizer(case['params'], case['x0'], case['N_max'], epsilon, 'BFGS')\n        results.append(converged_bfgs)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在构建了基于线搜索的优化器后，理解其局限性至关重要。这个思想实验探讨了一个经典的失败场景：当优化从一个完美的“山顶”（即极大值点）或鞍点开始时会发生什么。通过分析梯度为零（$\\nabla f = 0$）但该点并非能量最低点的情况，你将能更深刻地体会到收敛判据的重要性，以及在复杂势能面上进行导航时所固有的挑战 。",
            "id": "2461264",
            "problem": "在计算化学的分子几何优化中，一个共同的目标是找到势能面的一个局部极小值。考虑由完美对称山丘$f(x,y) = -x^2 - y^2$给出的光滑势能面，并假设您试图从其精确顶点$x = 0, \\; y = 0$开始对其进行最小化。您使用一种为无约束最小化设计的标准线搜索拟牛頓法（例如，Broyden–Fletcher–Goldfarb–Shanno方法），其初始逆度量是正定的，并采用典型的Wolfe型线搜索。假设使用精确算术，没有扰动或正则化。\n\n根据基本原理，回想一下，驻点满足$\\nabla f = 0$，对于一个在某点的梯度为$g$的可微函数，其下降方向$p$必须满足$g^\\top p < 0$，并且线搜索方法通过选择一个方向然后选择一个步长来减小目标函数值，从而生成试探步。\n\n哪种结果最能描述发生的情况及其原因？\n\nA. 该方法在起始点立即终止，因为计算出的搜索方向是零向量，线搜索无法减小目标函数值；它报告收敛到一个驻点，即使该点是一个极大值点。\n\nB. 该方法沿任意方向迈出任意非零长度的一步，因为负定曲率迫使线搜索移动到信赖域的边界，从而逃离顶点。\n\nC. 该方法计算出一个牛顿步，该步最初指向上坡方向并被Wolfe条件拒绝；这会触发一次更新，使度量变为负定，之后该方法迅速找到一个附近的极小值点。\n\nD. 由于曲面的对称性，该方法进入一种循环行为，在正交方向之间以小步长交替进行，直到数值噪声打破这个循环。",
            "solution": "必须首先验证问题陈述的科学性和逻辑完整性。\n\n步骤1：提取已知条件\n- 目标函数：一个由$f(x,y) = -x^2 - y^2$给出的光滑势能面。\n- 起始点：$(x_0, y_0) = (0, 0)$。\n- 优化算法：一种标准的线搜索拟牛顿法（例如，BFGS）。\n- 初始逆度量（逆Hessian近似）$H_0$：正定。\n- 线搜索条件：Wolfe型。\n- 假設：精确算術，无扰动或正则化。\n- 供回顾的定义：\n    - 驻点条件：$\\nabla f = 0$。\n    - 梯度为$g$时的下降方向$p$：$g^\\top p < 0$。\n    - 线搜索方法操作：选择一个方向，然后选择一个步长。\n\n步骤2：使用提取的已知条件进行验证\n该问题描述了无约束优化算法的一个经典测试用例。函数$f(x,y)$是一个简单的、无限可微的二次函数。起始点是其唯一的驻点。指定的算法是数值优化和计算化学中的一个标准主力工具。初始条件（正定的$H_0$）和假设（精确算术）是对此类算法进行理论分析时的标准配置。\n\n- **科学依据**：该问题在数值优化理论及其在计算化学中的应用方面有坚实的理论基础。所使用的概念是基础性的且陈述正确。\n- **适定性**：该问题是适定的。函数、起始点和算法类型都得到了明確定义，这会导向一个可确定的结果。\n- **客观性**：该问题以精确、客观的数学语言陈述。\n\n该问题不违反任何无效性标准。它是一个数值分析领域中的有效、可形式化的问题。\n\n步骤3：结论与行动\n问题有效。我现在开始推導解答。\n\n任务是确定从点$(0,0)$开始，应用线搜索拟牛顿法来最小化函数$f(x,y) = -x^2 - y^2$的结果。\n\n令变量向量为$x_{\\text{vec}} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$。目标函数是$f(x_{\\text{vec}}) = -x_{\\text{vec}}^\\top x_{\\text{vec}}$。起始点是$x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n一次线搜索拟牛顿迭代按以下步骤进行：\n1.  在第$k$步，计算梯度$g_k = \\nabla f(x_k)$。\n2.  检查收敛性。一个标准的终止条件是梯度的范数足够小，即$||g_k|| < \\epsilon$，其中$\\epsilon$是一个很小的正容差。\n3.  使用当前的逆Hessian矩阵近似$H_k$计算搜索方向$p_k$。该方向由$p_k = -H_k g_k$给出。\n4.  执行线搜索以找到一个合适的步长$\\alpha_k > 0$，使得新点$x_{k+1} = x_k + \\alpha_k p_k$满足Wolfe条件。\n5.  将位置更新为$x_{k+1}$，并将逆Hessian近似更新为$H_{k+1}$。\n\n让我们分析第一次迭代（$k=0$）。\n起始点是$x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n首先，我们计算$f(x,y) = -x^2 - y^2$的梯度。\n梯度为$$g(x,y) = \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} -2x \\\\ -2y \\end{pmatrix}$$。\n\n在起始点$x_0$，梯度为：\n$$g_0 = g(0,0) = \\begin{pmatrix} -2(0) \\\\ -2(0) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = 0$$。\n\n问题陈述我们使用的是精确算术。在起始点的梯度恰好是零向量。\n算法的第一步是检查收敛性。由于$||g_0|| = 0$，对于任何$\\epsilon > 0$，收敛准则$||g_k|| < \\epsilon$都被满足。因此，算法必须在迭代$k=0$时立即终止。\n\n如果我们暂时忽略终止检查，继续计算搜索方向，我们会得到：\n$p_0 = -H_0 g_0$\n由于$g_0 = 0$，搜索方向为$p_0 = -H_0 \\cdot 0 = 0$。无论初始逆Hessian近似$H_0$是什么，搜索方向都是零向量。\n\n当搜索方向为零时，任何后续步骤的形式都是$x_1 = x_0 + \\alpha_0 p_0 = x_0 + \\alpha_0 \\cdot 0 = x_0$。不可能从起始点移动，并且目标函数值无法改变，更不用说减小了。对于$\\alpha_0 > 0$的线搜索将無法找到任何能够使$f$严格减小的步长。\n\n驻点的性质由二阶导数的Hessian矩阵确定：\n$$H_f(x,y) = \\nabla^2 f(x,y) = \\begin{pmatrix} -2 & 0 \\\\ 0 & -2 \\end{pmatrix}$$\n该矩阵的特征值均为$-2$。由于特征值全部为负，Hessian矩阵是负定的，因此点$(0,0)$处的驻点是一个严格局部极大值。该算法旨在进行最小化，但它找到了一个驻点。对该点进行分类不是算法的主要职责；它的设计目的是寻找梯度为零的点。\n\n基于此分析，该方法将立即停止并报告已收敛到一个驻点。\n\n现在，我们评估给定的选项。\n\nA. 该方法在起始点立即终止，因为计算出的搜索方向是零向量，线搜索无法减小目标函数值；它报告收敛到一个驻点，即使该点是一个极大值点。\n- 这个陈述准确地描述了事件的顺序。梯度为零，因此算法基于标准的$||g||=0$准则终止。如果它继续进行，搜索方向$p_0$将是零向量。无法做出任何移动来减小目标函数值。算法报告收敛是因为它找到了一个梯度为零的点。这个点确实是一个极大值点。\n- 结论：**正确**。\n\nB. 该方法沿任意方向迈出任意非零长度的一步，因为负定曲率迫使线搜索移动到信赖域的边界，从而逃离顶点。\n- 这个选项错误地描述了算法。问题指定的是线搜索方法，而不是信赖域方法。信赖域方法有處理非正定Hessian矩阵的机制，但它们的操作方式不同。在线搜索方法中，首先计算搜索方向（本例中$p_0 = 0$），然后寻找步长。方向不是任意的；它被确定性地确定为零。信赖域边界的概念不适用。\n- 结论：**不正确**。\n\nC. 该方法计算出一个牛顿步，该步最初指向上坡方向并被Wolfe条件拒绝；这会触发一次更新，使度量变为负定，之后该方法迅速找到一个附近的极小值点。\n- 该方法计算的是一个拟牛顿步，而不是牛顿步。步进方向是$p_0 = -H_0 g_0 = 0$，即零向量。一个方向$p$仅在$g^\\top p > 0$时才是\"上坡\"的。这里，$g_0^\\top p_0 = 0^\\top 0 = 0$。该方向不是上坡方向。由于没有迈出任何步（$s_0 = x_1 - x_0 = 0$），Hessian近似不会更新。算法终止，因此不会继续寻找任何其他点。\n- 结论：**不正确**。\n\nD. 由于曲面的对称性，该方法进入一种循环行为，在正交方向之间以小步长交替进行，直到数值噪声打破这个循环。\n- 算法在第一步就终止了，因为梯度为零。它没有迈出任何步，无论是小步还是其他步。因此，不可能出现循环行为。问题还明确假设“精确算术”，这排除了数值噪声的影响。\n- 结论：**不正确**。\n\n唯一正确描述结果的是选项A。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "我们如何设计出比简单线搜索方法更稳健的优化器，尤其是在面对负曲率区域时？本练习将向你介绍一种强大且可靠的替代方案：信赖域 (trust-region) 框架。通过编写一个简单的一维信赖域优化器，你将见证它如何通过动态调整其信任半径$\\Delta_k$并评估模型与真实函数的一致性比率$\\rho_k$，从而智能地避免“步子迈得太大”并成功地找到最小值，即使是从一个具有挑战性的起始点出发 。",
            "id": "2461247",
            "problem": "考虑一个以无量纲单位表示的单变量目标函数 $f(x)=x^4-x^2$，其一阶导数为 $f'(x)=4x^3-2x$，二阶导数为 $f''(x)=12x^2-2$。驻点为位于 $x=0$ 处的局部极大值点和位于 $x_{\\pm}=\\pm 1/\\sqrt{2}$ 处的局部极小值点。对于一个迭代点 $x_k\\in\\mathbb{R}$，定义二次模型 $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ 和一个信赖域半径 $\\Delta_k>0$。信赖域子问题是找到一个步长 $s_k\\in\\mathbb{R}$，该步长在约束 $\\lvert s\\rvert\\le \\Delta_k$ 下使 $m_k(s)$ 最小化。令预测下降量为 $\\operatorname{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$，实际下降量为 $\\operatorname{ared}_k=f(x_k)-f(x_k+s_k)$。当 $\\operatorname{pred}_k>0$ 时，定义接受率 $\\rho_k=\\operatorname{ared}_k/\\operatorname{pred}_k$；如果 $\\operatorname{pred}_k\\le 0$，则设置 $\\rho_k=-\\infty$。如果 $\\rho_k\\ge \\eta_1$，则步长被接受，否则被拒绝。信赖域半径根据以下规则更新，其中参数 $\\eta_1\\in(0,1)$, $\\eta_2\\in(\\eta_1,1)$, $\\gamma_1\\in(0,1)$ 和 $\\gamma_2>1$ 为固定值：\n- 如果 $\\rho_k<\\eta_1$，设置 $\\Delta_{k+1}=\\gamma_1\\Delta_k$。\n- 如果 $\\rho_k\\ge \\eta_2$ 且 $\\lvert s_k\\rvert\\ge 0.8\\,\\Delta_k$，设置 $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$。\n- 否则，设置 $\\Delta_{k+1}=\\Delta_k$。\n如果一个步长被拒绝，则保持 $x_{k+1}=x_k$；如果被接受，则设置 $x_{k+1}=x_k+s_k$。当 $\\lvert f'(x_k)\\rvert\\le \\varepsilon_g$、对于一个被接受的步长 $\\lvert s_k\\rvert\\le \\varepsilon_s$、$\\Delta_k\\le \\varepsilon_s$ 或达到固定的迭代次数上限时，迭代终止。\n\n在一维空间中，二次模型在约束 $\\lvert s\\rvert\\le \\Delta$ 下的唯一全局极小值点的特征如下。对于给定的 $g\\in\\mathbb{R}$ 和 $h\\in\\mathbb{R}$，其中 $g=f'(x)$ 和 $h=f''(x)$：\n- 如果 $h>0$ 且无约束极小值点 $s_N=-g/h$ 满足 $\\lvert s_N\\rvert\\le \\Delta$，那么 $s^\\star=s_N$；否则 $s^\\star=-\\operatorname{sign}(g)\\,\\Delta$。\n- 如果 $h\\le 0$，那么当 $g<0$ 时 $s^\\star=\\Delta$，当 $g>0$ 时 $s^\\star=-\\Delta$，当 $g=0$ 时 $s^\\star=\\Delta$。\n\n固定数值参数 $\\eta_1=0.1$, $\\eta_2=0.9$, $\\gamma_1=0.25$, $\\gamma_2=2$, $\\Delta_{\\max}=10$，梯度容差 $\\varepsilon_g=10^{-8}$，步长容差 $\\varepsilon_s=10^{-10}$，以及最大迭代次数为 $100$ 次。对于下面的每个测试用例，从给定的初始点 $x_0$ 和初始半径 $\\Delta_0$ 开始，并在每一步应用上述迭代，使用一维模型极小化器 $s_k$。\n\n为每个测试用例定义三个标量输出：\n- $b_1$：逻辑值，当且仅当所有被接受的步长都未增加目标函数值时为真，即对于所有被接受的步长 $k$，有 $f(x_{k+1})\\le f(x_k)$。\n- $b_2$：逻辑值，当且仅当第一次迭代时的完整无约束牛顿步 $s_N=-f'(x_0)/f''(x_0)$（当 $f''(x_0)\\ne 0$ 时；如果 $f''(x_0)=0$，则定义 $b_2$ 为假）产生了一个更高的目标函数值时为真，即 $f(x_0+s_N)>f(x_0)$。\n- $d$：最终接受的迭代点 $x_{\\mathrm{final}}$ 与最近的局部极小值点之间的绝对距离，即 $d=\\min\\{\\lvert x_{\\mathrm{final}}-1/\\sqrt{2}\\rvert,\\lvert x_{\\mathrm{final}}+1/\\sqrt{2}\\rvert\\}$。\n\n待评估的测试套件参数：\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，并用方括号括起来。对于每个测试用例，输出一个列表 $[b_1,b_2,d]$，其中 $b_1$ 和 $b_2$ 是小写字符串“true”或“false”，$d$ 是小数点后恰好有六位的小数四舍五入浮点数。因此，最终输出必须是形如\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$\n的单行，该行中任何地方都没有空格。",
            "solution": "该问题是有效的。它提出了一个在数值优化领域中适定的、自洽的且科学上合理的练习，具体而言，是将信赖域算法应用于一维势能函数。所有必要的参数、算法规则、函数和终止准则都以数学精度给出。我们将给出一个完整的解决方案。\n\n该问题的核心是实现并分析一个信赖域优化算法。这类方法是计算科学的基础，尤其在计算化学中用于定位稳定的分子几何构型，这些构型对应于势能面上的极小值点。目标函数 $f(x) = x^4 - x^2$ 是一个典型的一维双阱势，代表一个在 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 处具有两个稳定态（极小值）和在 $x=0$ 处具有一个不稳定过渡态（极大值）的系统。\n\n信赖域算法通过构造目标函数的简化模型来迭代地寻找极小值，该模型仅在当前迭代点 $x_k$ 的一个邻域内是可信的。这个邻域是一个半径为 $\\Delta_k$ 的“信赖域”。该模型是一个二次函数 $m_k(s)$，由 $f(x)$ 在 $x_k$ 附近作二阶泰勒展开得到：\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\n其中 $s$ 是从 $x_k$ 出发的步长。该模型在约束 $\\lvert s \\rvert \\le \\Delta_k$ 下关于 $s$ 进行最小化。这个约束最小化问题被称为信赖域子问题。\n\n如题目所给，一维子问题的解取决于模型的曲率，该曲率由二阶导数 $h = f''(x_k)$ 给出。\n1.  如果 $h > 0$，模型是凸的（一条开口向上的抛物线）。无约束极小值点是牛顿步 $s_N = -g/h$，其中 $g = f'(x_k)$。如果这一步长在信赖域内，即 $\\lvert s_N \\rvert \\le \\Delta_k$，它就是最优步长 $s_k$。否则，模型在信赖域的边界处取得最小值，$s_k = -\\operatorname{sign}(g)\\Delta_k$，即在最速下降方向上尽可能地移动。\n2.  如果 $h \\le 0$，模型是局部凹的或线性的。它在区间 $[-\\Delta_k, \\Delta_k]$ 上的最小值必然位于边界之一。在 $s = \\Delta_k$ 和 $s = -\\Delta_k$ 之间的选择由梯度 $g$ 的符号决定，梯度符号指示了下降方向。\n\n一旦计算出试探步长 $s_k$，就需要通过比较目标函数的*实际下降量* $\\operatorname{ared}_k = f(x_k) - f(x_k + s_k)$ 与模型的*预测下降量* $\\operatorname{pred}_k = m_k(0) - m_k(s_k)$ 来评估其质量。它们的比率 $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$ 用于衡量模型的保真度。\n\n-   如果 $\\rho_k$ 接近 1，说明模型是一个极好的预测器。步长被接受，并且如果当前步长已经接近信赖域边界，我们可以扩大信赖域（$\\Delta_{k+1} = \\gamma_2 \\Delta_k$），以允许更激进的步长。\n-   如果 $\\rho_k$ 为正但不大，说明模型是足够的。步长被接受，但信赖域大小保持不变（$\\Delta_{k+1} = \\Delta_k$）。\n-   如果 $\\rho_k$ 很小或为负，说明模型很差。步长被拒绝（$x_{k+1} = x_k$），并且信赖域被收缩（$\\Delta_{k+1} = \\gamma_1 \\Delta_k$），以在后续迭代中提高模型精度。\n\n步长接受规则是 $\\rho_k \\ge \\eta_1$。由于 $\\eta_1 = 0.1 > 0$ 且子问题的解保证了 $\\operatorname{pred}_k \\ge 0$，任何被接受的步长都必须满足 $\\operatorname{ared}_k \\ge \\eta_1 \\operatorname{pred}_k \\ge 0$。如果 $\\operatorname{pred}_k > 0$，那么 $\\operatorname{ared}_k > 0$，这保证了 $f(x_{k+1}) < f(x_k)$。只有当模型预测函数值会下降（$\\operatorname{pred}_k > 0$）时，步长才可能被接受。因此，$b_1$ 的条件——即没有被接受的步长会增加目标函数值——由该算法本身的构造所保证，必然为真。任何偏差都将表明实现有误。\n\n输出 $b_2$ 探究了纯牛顿-拉夫逊方法的局限性。无约束牛顿步 $s_N = -f'(x_0)/f''(x_0)$ 找到了二次模型的极值点。在极大值点附近，例如在 $x_0=0.0$ 或 $x_0=0.1$ 处，其Hessian $f''(x_0)$ 为负。因此，牛顿步寻求的是局部二次模型的*极大值*，这对于最小化全局函数 $f(x)$ 来说是一个糟糕的策略，并且很可能导致一个上升步，即 $f(x_0+s_N) > f(x_0)$。信赖域框架通过约束步长大小来纠正这个缺陷。\n\n最终输出 $d$ 衡量了算法收敛到其中一个真实极小值点 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 的精度。鉴于所有初始点都是非负的，该算法预期将收敛到正的极小值点 $x_+ = 1/\\sqrt{2}$。\n\n实现将首先定义目标函数及其导数。然后，为每个测试用例计算 $b_2$ 的值。接着执行主迭代循环，在每一步 $k$ 中包括：检查终止条件，求解信赖域子问题以获得 $s_k$，通过 $\\rho_k$ 评估步长质量，并根据指定规则更新状态变量 $x_k$ 和 $\\Delta_k$。在整个迭代过程中跟踪 $b_1$ 的值。终止时，计算最终距离 $d$。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region optimization problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x):\n        return x**4 - x**2\n\n    def f_prime(x):\n        return 4 * x**3 - 2 * x\n\n    def f_double_prime(x):\n        return 12 * x**2 - 2\n\n    # --- Algorithm Parameters ---\n    eta1 = 0.1\n    eta2 = 0.9\n    gamma1 = 0.25\n    gamma2 = 2.0\n    delta_max = 10.0\n    eps_g = 1e-8\n    eps_s = 1e-10\n    max_iter = 100\n    \n    minimizers = [-1/np.sqrt(2), 1/np.sqrt(2)]\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.1, 0.05),\n        (0.1, 2.0),\n        (0.0, 0.5),\n        (0.1, 0.001)\n    ]\n\n    results = []\n\n    for x0, delta0 in test_cases:\n        # --- Output variable initialization ---\n        b1_flag = True\n        \n        # --- Calculate b2 before starting iterations ---\n        g0 = f_prime(x0)\n        h0 = f_double_prime(x0)\n        \n        if h0 == 0:\n            b2 = False\n        else:\n            s_N = -g0 / h0\n            b2 = f(x0 + s_N) > f(x0)\n            \n        # --- Main Trust-Region Loop ---\n        x_k = x0\n        delta_k = delta0\n        final_x = x0 # Will hold the last accepted iterate value\n        \n        for _ in range(max_iter):\n            g_k = f_prime(x_k)\n            \n            # --- Termination check 1: Gradient ---\n            if abs(g_k) <= eps_g:\n                final_x = x_k\n                break\n\n            # --- Solve the trust-region subproblem ---\n            h_k = f_double_prime(x_k)\n            s_k = 0.0\n            \n            if h_k > 0:\n                s_N = -g_k / h_k\n                if abs(s_N) <= delta_k:\n                    s_k = s_N\n                else:\n                    s_k = -np.sign(g_k) * delta_k\n            else: # h_k <= 0\n                if g_k < 0:\n                    s_k = delta_k\n                elif g_k > 0:\n                    s_k = -delta_k\n                else: # g_k == 0\n                    s_k = delta_k\n\n            # --- Evaluate step quality ---\n            pred_k = -(g_k * s_k + 0.5 * h_k * s_k**2)\n            ared_k = f(x_k) - f(x_k + s_k)\n            \n            rho_k = 0.0\n            # Use small tolerance for pred_k to avoid division by zero instability\n            if pred_k > 1e-12: # Check for meaningful predicted reduction\n                rho_k = ared_k / pred_k\n            else:\n                rho_k = -np.inf # Model predicts no improvement or a trivial step\n\n            delta_kp1 = delta_k\n            \n            # --- Step acceptance/rejection and state update ---\n            if rho_k >= eta1: # Accept step\n                if f(x_k + s_k) > f(x_k):\n                    b1_flag = False\n                \n                x_k += s_k\n                final_x = x_k\n                \n                # --- Termination check 2: Step size for an accepted step ---\n                if abs(s_k) <= eps_s:\n                    break\n                    \n                # --- Update trust radius (for accepted step) ---\n                if rho_k >= eta2 and abs(s_k) >= 0.8 * delta_k:\n                    delta_kp1 = min(gamma2 * delta_k, delta_max)\n                # else: delta_kp1 remains delta_k\n                \n            else: # Reject step\n                # x_k remains the same\n                delta_kp1 = gamma1 * delta_k\n            \n            delta_k = delta_kp1\n            \n            # --- Termination check 3: Trust radius size ---\n            if delta_k <= eps_s:\n                break\n        \n        # --- Calculate final distance d ---\n        d = min(abs(final_x - m) for m in minimizers)\n        \n        # Format results for the current test case\n        b1_str = \"true\" if b1_flag else \"false\"\n        b2_str = \"true\" if b2 else \"false\"\n        d_str = \"{:.6f}\".format(d)\n        results.append(f'[{b1_str},{b2_str},{d_str}]')\n        \n    # --- Final Print ---\n    # The final output is a single line, formatted as a list of lists.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}