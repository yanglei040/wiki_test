{
    "hands_on_practices": [
        {
            "introduction": "To build effective randomness extractors, we first need a way to measure the 'quality' of our imperfect random sources. This exercise introduces the concept of min-entropy, $H_{\\infty}$, a worst-case measure of randomness, by applying it to a common theoretical model known as a 'bit-fixing source' . By completing this practice, you will gain a concrete understanding of how min-entropy quantifies the number of unpredictable bits in a source.",
            "id": "1441918",
            "problem": "In the study of computational complexity and cryptography, we often analyze imperfect sources of randomness. One such model is a \"bit-fixing source.\"\n\nConsider a source that generates $n$-bit binary strings. This source is known to be partially compromised: a specific set of $t$ bit positions are always fixed to the value 0, while the remaining $n-t$ bit positions are chosen independently and uniformly at random (i.e., each of these $n-t$ bits is 0 with probability $1/2$ and 1 with probability $1/2$).\n\nThe quality of a random source can be measured by its min-entropy. For a random variable $X$ that takes values from a set of possible outcomes $\\Omega$, its min-entropy, denoted $H_{\\infty}(X)$, is defined as:\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\max_{x \\in \\Omega} P(X=x)\\right)$$\nwhere $P(X=x)$ is the probability of a specific outcome $x$.\n\nSuppose a specific bit-fixing source generates $n=128$ bit strings, with $t=20$ of its bit positions fixed. Calculate the min-entropy of this source.",
            "solution": "Let $X$ be the random variable representing the $n$-bit string generated by the source. The set of all possible outcomes is denoted by $\\Omega$.\n\nThe source generates $n$-bit strings where $t$ bits are fixed and $n-t$ bits are random. The random bits are chosen uniformly and independently. This means that for each of the $n-t$ non-fixed positions, there are 2 equally likely possibilities (0 or 1).\n\nThe total number of distinct strings that the source can produce is determined by the number of combinations for the random part. This is $2^{n-t}$. So, the size of the sample space is $|\\Omega| = 2^{n-t}$.\n\nSince the $n-t$ bits are chosen uniformly at random, every possible string that the source can generate is equally likely. The probability of any specific outcome $x \\in \\Omega$ is given by:\n$$P(X=x) = \\frac{1}{\\text{Total number of possible outcomes}} = \\frac{1}{|\\Omega|} = \\frac{1}{2^{n-t}}$$\n\nThe definition of min-entropy requires us to find the maximum probability among all possible outcomes. Since all outcomes are equally likely, the maximum probability is the same as the probability of any single outcome:\n$$\\max_{x \\in \\Omega} P(X=x) = \\frac{1}{2^{n-t}}$$\n\nNow we substitute this maximum probability into the formula for min-entropy:\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\max_{x \\in \\Omega} P(X=x)\\right)$$\n$$H_{\\infty}(X) = -\\log_{2}\\left(\\frac{1}{2^{n-t}}\\right)$$\n\nUsing the logarithm property $\\log(1/a) = -\\log(a)$, we get:\n$$H_{\\infty}(X) = - \\left(-\\log_{2}\\left(2^{n-t}\\right)\\right)$$\n$$H_{\\infty}(X) = \\log_{2}\\left(2^{n-t}\\right)$$\n\nUsing the property $\\log_b(b^y) = y$, we can simplify this to:\n$$H_{\\infty}(X) = n-t$$\n\nThe problem provides the numerical values $n=128$ and $t=20$. We can substitute these into our derived expression:\n$$H_{\\infty}(X) = 128 - 20 = 108$$\n\nThus, the min-entropy of the source is 108.",
            "answer": "$$\\boxed{108}$$"
        },
        {
            "introduction": "While min-entropy measures the randomness of an input source, the goal of an extractor is to produce an output that is nearly indistinguishable from a truly uniform distribution. This is quantified by the statistical distance, a fundamental metric in cryptography and complexity theory . This practice provides a hands-on calculation to help you build intuition for what statistical distance measures and how it captures the difference between two probability distributions.",
            "id": "1441905",
            "problem": "In the field of cryptography and computational complexity, the quality of a random source is crucial. A common way to quantify the difference between a real-world, imperfect random source and a truly uniform random source is by calculating their statistical distance.\n\nConsider a simple random number generator designed to produce 2-bit integers. An ideal version of this generator would produce each of the four possible outcomes in the set $\\{0, 1, 2, 3\\}$ with equal likelihood. Let this ideal probability distribution be denoted by $U$.\n\nA particular physical implementation of this generator is found to be defective. An analysis reveals that the outcome $0$ occurs in exactly half of all trials. The other three outcomes, $\\{1, 2, 3\\}$, are observed to occur with equal frequency among the remaining trials. Let this flawed probability distribution be denoted by $X$.\n\nThe statistical distance, $\\Delta(P, Q)$, between two discrete probability distributions $P$ and $Q$ over the same finite sample space $\\Omega$ is defined as:\n$$ \\Delta(P, Q) = \\frac{1}{2} \\sum_{\\omega \\in \\Omega} |P(\\omega) - Q(\\omega)| $$\n\nCalculate the statistical distance $\\Delta(U, X)$ between the ideal distribution $U$ and the flawed distribution $X$. Express your answer as an exact fraction.",
            "solution": "We consider the finite sample space $\\Omega=\\{0,1,2,3\\}$. The ideal distribution $U$ assigns $U(\\omega)=\\frac{1}{4}$ for each $\\omega\\in\\Omega$. The flawed distribution $X$ has $X(0)=\\frac{1}{2}$ and, since the remaining probability mass $\\frac{1}{2}$ is split equally among $\\{1,2,3\\}$, we have $X(1)=X(2)=X(3)=\\frac{1}{6}$.\n\nBy definition, the statistical distance is\n$$\n\\Delta(U,X)=\\frac{1}{2}\\sum_{\\omega\\in\\Omega}\\left|U(\\omega)-X(\\omega)\\right|.\n$$\nWe compute the absolute differences term by term:\n$$\n|U(0)-X(0)|=\\left|\\frac{1}{4}-\\frac{1}{2}\\right|=\\frac{1}{4},\n$$\nand for each $i\\in\\{1,2,3\\}$,\n$$\n|U(i)-X(i)|=\\left|\\frac{1}{4}-\\frac{1}{6}\\right|=\\left|\\frac{3-2}{12}\\right|=\\frac{1}{12}.\n$$\nSumming these four terms gives\n$$\n\\sum_{\\omega\\in\\Omega}\\left|U(\\omega)-X(\\omega)\\right|=\\frac{1}{4}+3\\cdot\\frac{1}{12}=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\nTherefore,\n$$\n\\Delta(U,X)=\\frac{1}{2}\\cdot\\frac{1}{2}=\\frac{1}{4}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{4}}$$"
        },
        {
            "introduction": "The security guarantee of a randomness extractor is formally captured by its error parameter, $\\epsilon$, which bounds the statistical distance between its output and a perfect uniform distribution. But what does this parameter mean in practice? This problem challenges you to think critically about the implications of a large error value, demonstrating why an extractor with $\\epsilon = 1/2$ is considered cryptographically broken . Understanding this limit is crucial for appreciating the stringent requirements of secure cryptographic systems.",
            "id": "1441851",
            "problem": "In the field of cryptography, a one-time pad requires a key that is truly random and as long as the message itself. In practice, perfect randomness is scarce, so cryptographers rely on randomness extractors to distill high-quality random bits from weaker, biased sources.\n\nA randomness extractor is a function, `Ext`, that takes a weakly random source `X` and a short, truly random seed `S` to produce an output that is statistically close to uniform. Formally, a function $Ext: \\{0,1\\}^n \\times \\{0,1\\}^d \\to \\{0,1\\}^m$ is a **strong $(k, \\epsilon)$-min-entropy extractor** if for every random variable $X$ on $\\{0,1\\}^n$ with min-entropy $H_\\infty(X) \\ge k$, the joint distribution $(S, Ext(X,S))$ is $\\epsilon$-close in statistical distance to the uniform distribution $(U_d, U_m)$, where $S$ is a uniform seed from $U_d$, and $U_m$ is the uniform distribution on $m$-bit strings. The min-entropy $H_\\infty(X) = - \\log_2(\\max_x \\Pr[X=x])$ measures the unpredictability of the source $X$. The statistical distance between two distributions $P_1$ and $P_2$ over a set $\\mathcal{Z}$ is defined as $\\Delta(P_1, P_2) = \\frac{1}{2}\\sum_{z \\in \\mathcal{Z}} |P_1(z) - P_2(z)|$.\n\nA startup claims to have developed a novel hardware device that acts as a strong $(k, \\epsilon)$-min-entropy extractor for some large min-entropy $k$. However, in their technical specification, they state the statistical distance error of their extractor is $\\epsilon = 1/2$. A security analyst immediately dismisses the device as cryptographically useless for applications like generating one-time pads.\n\nWhich of the following statements provides the most accurate and fundamental reason why an extractor with an error of $\\epsilon = 1/2$ is considered completely useless for cryptographic purposes?\n\nA. An error of $\\epsilon = 1/2$ means the extractor fails to produce a valid output string exactly half of the time, making it unreliable.\nB. It permits the output distribution to be structured in a way that is easily distinguishable from uniform, such as having at least one bit fixed to a constant value, allowing an adversary to gain significant information.\nC. Any extractor with a non-zero error, $\\epsilon > 0$, is theoretically insecure; only a perfect extractor with $\\epsilon=0$ is acceptable for cryptographic use.\nD. An error of $\\epsilon = 1/2$ implies that the min-entropy $k$ of the source must be at least twice the input length $n$, which is a physical impossibility.\nE. A trivial function that simply outputs the first $m$ bits of the random seed $S$ can achieve an error of $\\epsilon=1/2$, meaning the extractor provides no actual \"extraction\" benefit.",
            "solution": "We recall the definition of statistical distance between distributions $P$ and $Q$ over the same finite space $\\mathcal{Z}$:\n$$\n\\Delta(P,Q)=\\frac{1}{2}\\sum_{z\\in\\mathcal{Z}}|P(z)-Q(z)|.\n$$\nA strong $(k,\\epsilon)$-extractor requires that for every source $X$ with $H_{\\infty}(X)\\ge k$, the joint distribution $(S,\\mathrm{Ext}(X,S))$ is within statistical distance at most $\\epsilon$ of $(U_{d},U_{m})$.\n\nTo see why $\\epsilon=\\frac{1}{2}$ is cryptographically useless, consider a distribution on $m$-bit outputs with one bit fixed, e.g., the first output bit is always $0$ while the remaining $m-1$ bits are uniform and independent of the seed $S$. Let $P$ denote the joint distribution of $(S,Y)$ where $S\\sim U_{d}$ and $Y$ is such that $Y_{1}=0$ and $(Y_{2},\\dots,Y_{m})\\sim U_{m-1}$ independently of $S$. Let $Q$ denote $(U_{d},U_{m})$, i.e., $S\\sim U_{d}$ and $Y\\sim U_{m}$ independent.\n\nCompute $\\Delta(P,Q)$ directly. The sample space is pairs $(s,y)$ with $s\\in\\{0,1\\}^{d}$ and $y\\in\\{0,1\\}^{m}$. Partition by whether $y_{1}=0$ or $y_{1}=1$.\n- If $y_{1}=0$, then\n$$\nP(s,y)=2^{-d}2^{-(m-1)}=2^{-d-m+1},\\quad Q(s,y)=2^{-(d+m)}.\n$$\nThe absolute difference for each such pair is\n$$\n|P(s,y)-Q(s,y)|=2^{-d-m+1}-2^{-d-m}=2^{-d-m}.\n$$\nThere are $2^{d}\\cdot 2^{m-1}$ such pairs.\n- If $y_{1}=1$, then $P(s,y)=0$ while $Q(s,y)=2^{-(d+m)}$, so\n$$\n|P(s,y)-Q(s,y)|=2^{-(d+m)}.\n$$\nThere are again $2^{d}\\cdot 2^{m-1}$ such pairs.\n\nSumming absolute differences and halving, we get\n$$\n\\Delta(P,Q)=\\frac{1}{2}\\left(2^{d}2^{m-1}\\cdot 2^{-(d+m)}+2^{d}2^{m-1}\\cdot 2^{-(d+m)}\\right)\n=\\frac{1}{2}\\left(2^{-1}+2^{-1}\\right)=\\frac{1}{2}.\n$$\nThus a joint distribution in which the output has at least one fixed bit is exactly at statistical distance $\\frac{1}{2}$ from the ideal $(U_{d},U_{m})$. Therefore, an error tolerance of $\\epsilon=\\frac{1}{2}$ does not rule out extractors whose outputs leak one full bit deterministically and are trivially distinguishable from uniform with advantage $\\frac{1}{2}$. Such leakage renders the output useless for cryptographic tasks like one-time pads, where even learning one bit of the key with certainty is unacceptable.\n\nNow evaluate the options:\n- A is incorrect because $\\epsilon$ is a statistical distance bound, not a failure probability.\n- B is correct: $\\epsilon=\\frac{1}{2}$ allows the output distribution to have gross structure such as a fixed bit, which is highly distinguishable from uniform and leaks significant information, as shown above.\n- C is incorrect: cryptography tolerates small nonzero $\\epsilon$, and security degrades by at most $\\epsilon$ in information-theoretic settings.\n- D is incorrect: $\\epsilon$ does not impose any constraint like $k\\ge 2n$.\n- E is not the fundamental reason and is generally false for $m>1$; the function outputting the first $m$ bits of $S$ has statistical distance at least $1-2^{-m}$ from independence, which exceeds $\\frac{1}{2}$ for $m\\ge 2$.\n\nTherefore, the most accurate and fundamental reason is that $\\epsilon=\\frac{1}{2}$ permits outputs with easily detectable, deterministic structure (e.g., a fixed bit), making the extractor cryptographically useless.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}