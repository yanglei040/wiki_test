{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of proving approximation hardness via the PCP theorem is the reduction of a verifier's logic into a large-scale optimization problem. This transformation typically yields a Constraint Satisfaction Problem (CSP), where each distinct check the verifier might perform corresponds to a single constraint. This practice  provides a hands-on look at this construction, asking you to quantify the size of the resulting CSP based on the verifier's core parameters.",
            "id": "1418606",
            "problem": "A theoretical model for a Probabilistically Checkable Proof (PCP) verifier is constructed to analyze the structure of the constraint systems they generate. This verifier is designed for decision problems on inputs of size $n$. Its operation is defined as follows:\n\n1.  The verifier uses a random string $R$ of total length $r(n)$. This string is a concatenation of two independent parts, $R_1$ and $R_2$, with lengths $r_1(n)$ and $r_2(n)$ respectively, so that $r(n) = r_1(n) + r_2(n)$.\n2.  The first part, $R_1$, is used to deterministically select a set of $q(n)$ distinct locations to query within a given proof string $\\pi$. We denote the set of query locations generated from $R_1$ as $Q(R_1)$.\n3.  The second part, $R_2$, is used to deterministically select an acceptance predicate, denoted $V_{R_2}$. An acceptance predicate is a boolean function that takes $q(n)$ bits as input and returns a single bit indicating acceptance (1) or rejection (0).\n4.  For a given full random string $R = R_1R_2$, the verifier accepts the proof $\\pi$ if and only if the predicate $V_{R_2}$ evaluates to 1 on the specific bits of the proof read from the locations in $Q(R_1)$.\n\nIn the standard method to show hardness of approximation, this PCP system is converted into an instance of a Constraint Satisfaction Problem (CSP). A distinct constraint is generated for each unique pair $(Q, V)$, consisting of a query location set $Q$ and an acceptance predicate $V$, that the verifier can possibly produce from its random strings.\n\nAssuming that the mappings from random strings to outcomes are maximally diverse (i.e., different random strings produce different outcomes whenever possible), what is the maximum possible number of constraints that can be generated for the resulting CSP instance? Express your answer as a single closed-form analytic expression in terms of $r_1(n)$, $r_2(n)$, and $q(n)$. For simplicity in the final expression, you may write these functions as $r_1, r_2$, and $q$.",
            "solution": "There are $2^{r_{1}}$ possible strings for $R_{1}$ and $2^{r_{2}}$ possible strings for $R_{2}$. Under the maximal diversity assumption, the mapping $R_{1} \\mapsto Q(R_{1})$ can be injective across all $2^{r_{1}}$ possibilities, yielding at most $2^{r_{1}}$ distinct query sets. For the acceptance predicate, the total number of distinct boolean predicates on $q$ bits is $2^{2^{q}}$, so the mapping $R_{2} \\mapsto V_{R_{2}}$ can produce at most $\\min\\!\\left(2^{r_{2}},\\,2^{2^{q}}\\right)$ distinct predicates.\n\nBecause $R_{1}$ and $R_{2}$ are independent and concatenated to form $R = R_{1}R_{2}$, every pair of outcomes combines, so the set of distinct verifier outcomes is the Cartesian product of the sets of distinct $Q$ and $V$. Hence, the maximum number of distinct pairs $(Q,V)$—and thus the maximum number of constraints generated—is\n$$\n2^{r_{1}} \\cdot \\min\\!\\left(2^{r_{2}},\\,2^{2^{q}}\\right).\n$$\nThis quantity is always at most $2^{r_{1}+r_{2}}$, the total number of random strings, as required.",
            "answer": "$$\\boxed{2^{r_{1}}\\min\\!\\left(2^{r_{2}},\\,2^{2^{q}}\\right)}$$"
        },
        {
            "introduction": "The magic of the PCP theorem lies in the \"gap\" it creates between the satisfaction level of YES instances and NO instances. This problem  explores the profound consequences of this gap through a compelling thought experiment. By considering the hypothetical existence of an efficient approximation algorithm (a PTAS), you will uncover why the PCP gap is precisely the barrier that makes certain optimization problems computationally hard to approximate.",
            "id": "1418576",
            "problem": "In computational complexity theory, the Probabilistically Checkable Proofs (PCP) theorem establishes a profound connection between the hardness of approximation for optimization problems and proof verification. This problem explores a hypothetical consequence of discovering an efficient approximation algorithm for a particular problem.\n\nConsider the Maximum Exactly-3-Linear-Equations-mod-2 (MAX-E3-LIN-2) problem. An instance of MAX-E3-LIN-2 consists of a set of variables $x_1, \\dots, x_m$ over the field $\\mathbb{F}_2 = \\{0, 1\\}$ and a collection of constraints, where each constraint is a linear equation involving exactly three distinct variables, i.e., of the form $x_i \\oplus x_j \\oplus x_k = b$, with $b \\in \\{0, 1\\}$ and $\\oplus$ denoting the XOR operation (addition modulo 2). The objective is to find an assignment of values to the variables that maximizes the total number of satisfied equations.\n\nNow, imagine a family of PCP verifiers, indexed by input size $n$, designed to decide a language $L$. For any input string of size $n$, the verifier $V_n$ operates on a proof string. The verification process involves a set of local checks, where each check can be perfectly represented as a constraint in a MAX-E3-LIN-2 instance. The verifier's performance is characterized by its completeness, $c(n)$, and its soundness, $s(n)$.\n- If an input is in $L$ (a YES instance), there exists a proof that causes the verifier to accept with probability $c(n)$.\n- If an input is not in $L$ (a NO instance), any proof causes the verifier to accept with probability at most $s(n)$.\n\nYou are given the following two premises:\n1. It has been discovered that MAX-E3-LIN-2 admits a Polynomial-Time Approximation Scheme (PTAS). A PTAS is an algorithm that, for any given instance $I$ and any error parameter $\\epsilon  0$, produces a solution whose value is within a multiplicative factor of $(1-\\epsilon)$ of the optimal value. For any fixed $\\epsilon$, the algorithm's running time is polynomial in the size of the instance $|I|$.\n2. The language $L$ is not in P, the class of decision problems solvable in deterministic polynomial time.\n\nGiven these premises, which of the following statements about the verifier's completeness $c(n)$ and soundness $s(n)$ must be true?\n\nA. $c(n) = 1$ and $s(n) \\le 1/2$ for all sufficiently large $n$.\n\nB. The gap, defined as $c(n) - s(n)$, must be a constant greater than zero for all sufficiently large $n$.\n\nC. The ratio $s(n)/c(n)$ must be a constant strictly less than 1 for all sufficiently large $n$.\n\nD. The ratio $s(n)/c(n)$ must approach 1 as $n \\to \\infty$.\n\nE. The gap $c(n) - s(n)$ must be $O(1/n^k)$ for some constant $k0$.",
            "solution": "We formalize the verifier-to-optimization connection and the effect of a PTAS.\n\nFor an input string $x$ of length $n$, let $V_{n}$ be the verifier. By hypothesis, each local check of $V_{n}$ is exactly one constraint in a MAX-E3-LIN-2 instance. Construct an instance $I(x)$ by aggregating the checks, with $M$ total constraints. Let $p^{\\ast}(x)$ denote the maximum acceptance probability over all proofs for $V_{n}$ on input $x$. Because every check corresponds to a constraint and acceptance is the event that the queried constraint is satisfied, the optimum fraction of satisfiable constraints equals $p^{\\ast}(x)$:\n$$\n\\frac{\\operatorname{OPT}(I(x))}{M}=p^{\\ast}(x).\n$$\nBy completeness and soundness,\n$$\nx \\in L \\implies p^{\\ast}(x) \\ge c(n), \\qquad x \\notin L \\implies p^{\\ast}(x) \\le s(n).\n$$\n\nA PTAS for MAX-E3-LIN-2 guarantees that for any $\\epsilon0$, on instance $I(x)$ it returns an assignment with value $\\operatorname{val}(I(x))$ satisfying\n$$\n\\frac{\\operatorname{val}(I(x))}{M} \\ge (1-\\epsilon)\\,\\frac{\\operatorname{OPT}(I(x))}{M}=(1-\\epsilon)\\,p^{\\ast}(x).\n$$\n\nSuppose there exists a constant $\\delta0$ and $n_{0}$ such that for all $n \\ge n_{0}$,\n$$\n\\frac{s(n)}{c(n)} \\le 1-\\delta.\n$$\nFix $\\epsilon$ with $0\\epsilon\\delta$. For any YES instance with $|x|=n \\ge n_{0}$,\n$$\n\\frac{\\operatorname{val}(I(x))}{M} \\ge (1-\\epsilon)\\,p^{\\ast}(x) \\ge (1-\\epsilon)\\,c(n)  (1-\\delta)\\,c(n) \\ge s(n).\n$$\nFor any NO instance, every assignment satisfies at most an $s(n)$ fraction of constraints, so\n$$\n\\frac{\\operatorname{val}(I(x))}{M} \\le s(n).\n$$\nThus, comparing the achieved fraction to any threshold in $(s(n),(1-\\epsilon)c(n))$ decides $L$ in deterministic polynomial time (the PTAS runs in time polynomial in $|I(x)|$ for fixed $\\epsilon$, and evaluating the fraction is polynomial), implying $L \\in \\mathrm{P}$. This contradicts the premise that $L \\notin \\mathrm{P}$. Therefore, it cannot be that $s(n)/c(n)$ is bounded away from $1$ for all sufficiently large $n$. Equivalently, we must have\n$$\n\\lim_{n \\to \\infty} \\frac{s(n)}{c(n)}=1.\n$$\n\nNow evaluate the options:\n- A would give $\\frac{s(n)}{c(n)} \\le \\frac{1}{2}$ for large $n$, contradicting the necessity just derived.\n- B asserts a constant additive gap $c(n)-s(n) \\ge \\gamma0$; combined with any lower bound $c(n) \\ge c_{0}0$ it would imply $\\frac{s(n)}{c(n)} \\le 1-\\gamma/c_{0}1$, again contradicting the necessity. Even without a uniform $c_{0}$, B is not required by the premises and in general would enable a polynomial-time decision via a fixed $\\epsilon$, so it cannot be true given $L \\notin \\mathrm{P}$.\n- C explicitly states a constant ratio strictly less than $1$, which we just ruled out.\n- D states exactly the required condition $\\frac{s(n)}{c(n)} \\to 1$.\n- E requires a specific polynomial decay rate for the additive gap. The premises only force the relative gap to vanish; the additive gap could, for example, be $\\Theta(1/\\ln n)$, which is not $O(1/n^{k})$ for any $k0$. Hence E is not implied.\n\nTherefore, the only statement that must be true is D.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "After exploring the mechanics of creating a gapped problem and the theory behind why this implies hardness, it's time to see the entire process in action. This exercise  provides a concrete, end-to-end example of a gap-preserving reduction, one of the most powerful techniques in complexity theory. By translating a gapped Constraint Satisfaction Problem into an instance of Set Cover, you will quantitatively determine the resulting hardness-of-approximation factor, solidifying your understanding of how PCPs lead to inapproximability results.",
            "id": "1418609",
            "problem": "The Probabilistically Checkable Proofs (PCP) theorem provides a powerful tool for proving the hardness of approximation for many optimization problems. This is achieved by creating reductions from NP-complete problems (like 3-SAT) to instances of optimization problems with a calculable \"gap\" in the objective function's value between YES instances (satisfiable 3-SAT formulas) and NO instances (unsatisfiable 3-SAT formulas).\n\nConsider a specific PCP-based reduction that transforms a 3-SAT formula into a special type of Constraint Satisfaction Problem (CSP). This CSP, let's call it $\\psi$, has the following properties, guaranteed by the PCP theorem:\n1.  The CSP $\\psi$ is defined on a set of $N$ variables, $\\{x_1, \\dots, x_N\\}$, and a set of $M$ constraints, $\\{C_1, \\dots, C_M\\}$.\n2.  The reduction guarantees a \"satisfaction gap\":\n    -   If the original 3-SAT formula is satisfiable (a YES case), then there exists an assignment for the $N$ variables that satisfies all $M$ constraints of $\\psi$.\n    -   If the original 3-SAT formula is not satisfiable (a NO case), then no assignment for the $N$ variables can satisfy more than a fraction $s$ of the $M$ constraints, where $s1$.\n3.  Furthermore, the resulting CSP is regular, meaning every variable $x_i$ appears in exactly $D$ constraints.\n\nNow, consider a standard reduction from this CSP $\\psi$ to an instance of the Set Cover problem. The Set Cover instance $(\\mathcal{U}, \\mathcal{S})$ is constructed as follows:\n-   The universe to be covered, $\\mathcal{U}$, is the set of all $M$ constraints of $\\psi$.\n-   The collection of available subsets, $\\mathcal{S}$, consists of one set $S_{i,v}$ for each variable $x_i$ and each possible binary value $v \\in \\{0, 1\\}$. The set $S_{i,v}$ is defined as the set of all constraints that are satisfied if the variable $x_i$ is assigned the value $v$.\n\nYou are given a CSP instance $\\psi$ generated by this process with the following parameters: $N=2000$ variables, $M=25000$ constraints, a satisfaction gap parameter of $s=0.6$, and a regularity parameter of $D=25$.\n\nYour task is to analyze the size of the optimal set cover for this instance.\n1.  In the YES case (when the CSP is fully satisfiable), what is the size of the minimum set cover, denoted $k_{YES}$?\n2.  Next, consider the NO case. Any collection of $N$ sets that corresponds to a valid assignment (i.e., one set for each variable) can cover at most $sM$ constraints. To cover the remaining constraints, additional sets must be added. Assuming the most optimistic scenario where each additional set covers the maximum possible number of *new, previously uncovered* constraints, what is the minimum number of *additional* sets required to cover all $M$ constraints?\n3.  Using your answers from the previous parts, what is the minimum size of a set cover in the NO case, denoted $k_{NO}$?\n4.  Finally, calculate the approximation hardness gap, $\\gamma = \\frac{k_{NO}}{k_{YES}}$, established by this reduction for the Set Cover problem.\n\nProvide the single numerical value for the approximation hardness gap $\\gamma$. Round your final answer to four significant figures.",
            "solution": "We are given a CSP instance with $N$ variables, $M$ constraints, regularity $D$, and a satisfaction gap $s1$. The Set Cover instance uses the universe $\\mathcal{U}$ of all $M$ constraints and the family of sets $\\{S_{i,v}\\}$, where $S_{i,v}$ contains all constraints satisfied when $x_{i}=v$. Regularity implies each variable appears in exactly $D$ constraints, so for any $i$ and $v$, the size of $S_{i,v}$ is at most $D$.\n\n1) YES case. If the CSP is satisfiable, there exists an assignment $a:[N]\\to\\{0,1\\}$ satisfying all $M$ constraints. Selecting the $N$ sets $\\{S_{i,a(i)}: i\\in[N]\\}$ covers all constraints, hence\n$$\nk_{YES}\\leq N.\n$$\nUnder the standard analysis for this reduction, we take\n$$\nk_{YES}=N.\n$$\nWith $N=2000$, this yields $k_{YES}=2000$.\n\n2) NO case additional sets. Any collection of $N$ sets corresponding to a single assignment covers at most $sM$ constraints. Thus the number of uncovered constraints after those $N$ sets is\n$$\nM - sM = (1-s)M.\n$$\nIn the most optimistic scenario, each additional set covers the maximum possible number of new, previously uncovered constraints, which is at most $D$ by regularity. Therefore the minimum number of additional sets required is\n$$\n\\left\\lceil \\frac{(1-s)M}{D} \\right\\rceil.\n$$\nSubstituting $s=0.6$, $M=25000$, and $D=25$,\n$$\n(1-s)M = 0.4 \\times 25000 = 10000,\\quad \\frac{10000}{25}=400,\n$$\nso the minimum number of additional sets is $400$.\n\n3) NO case cover size. Hence the minimum size of a set cover in the NO case is\n$$\nk_{NO} = N + \\left\\lceil \\frac{(1-s)M}{D} \\right\\rceil = 2000 + 400 = 2400.\n$$\n\n4) Approximation hardness gap. The gap is\n$$\n\\gamma = \\frac{k_{NO}}{k_{YES}} = \\frac{2400}{2000} = 1.2.\n$$\nRounded to four significant figures, this is $1.200$.",
            "answer": "$$\\boxed{1.200}$$"
        }
    ]
}