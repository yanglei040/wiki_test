{
    "hands_on_practices": [
        {
            "introduction": "The power of the complexity class BPP lies not in its initial error bound, but in our ability to shrink that error to almost zero through a process called amplification. This practice makes this abstract concept concrete by asking a practical question: how many times must we run a BPP algorithm to make it more reliable than the physical hardware it runs on? By working through this, you'll gain a quantitative feel for the exponential power of amplification .",
            "id": "1450962",
            "problem": "An algorithm in the complexity class BPP (Bounded-error Probabilistic Polynomial time) is designed to solve a decision problem. For any given input, it has a two-sided error probability, meaning it might return the wrong answer (either \"yes\" when it should be \"no\", or vice-versa) with a probability of at most $p=1/3$.\n\nTo improve the reliability of this algorithm, a standard amplification technique is employed: the algorithm is run $k$ times on the same input, and the results are aggregated. For the purpose of this problem, we define the overall procedure as having failed if the number of incorrect individual runs is at least half of the total number of runs, $k$.\n\nYour task is to determine how robust this amplification must be to outperform the physical limitations of the hardware it runs on. Calculate the minimum integer number of repetitions, $k$, required to ensure that the algorithm's overall failure probability is strictly less than the probability of a hardware error from a random high-energy particle. Assume the probability of such a hardware error during a single computation, for instance a bit-flip caused by a cosmic ray, is $P_{\\text{cosmic}} = 2.5 \\times 10^{-15}$.\n\nFor your calculation, you are provided with the following form of a Chernoff bound. Let $X_1, \\dots, X_k$ be independent indicator random variables where $P(X_i=1)=p$. Let $X = \\sum_{i=1}^k X_i$ be the sum of these variables, and let $\\mu = E[X] = kp$ be the expectation of the sum. For any $\\delta > 0$, the probability that $X$ deviates from its mean is bounded by:\n$$P(X \\ge (1+\\delta)\\mu) \\le \\exp\\left(-\\frac{\\delta^2 \\mu}{3}\\right)$$\n\nProvide your answer as the smallest integer $k$ that satisfies the condition. You may use the numerical values $\\ln(10) \\approx 2.3026$ and $\\ln(2.5) \\approx 0.9163$.",
            "solution": "We consider $k$ independent repetitions of the BPP algorithm. Let $X_{i}$ be the indicator of an incorrect outcome on run $i$, with $P(X_{i}=1)=p=\\frac{1}{3}$, and define $X=\\sum_{i=1}^{k}X_{i}$. Then $X \\sim \\text{Binomial}(k,p)$ and $\\mu=E[X]=kp=\\frac{k}{3}$. The overall procedure is defined to fail if at least half of the runs are incorrect, i.e., the event $\\{X \\ge \\frac{k}{2}\\}$.\n\nWe apply the given Chernoff bound. Choose $\\delta>0$ such that $(1+\\delta)\\mu=\\frac{k}{2}$. Substituting $\\mu=\\frac{k}{3}$ gives\n$$\n(1+\\delta)\\frac{k}{3}=\\frac{k}{2}\\quad\\Rightarrow\\quad 1+\\delta=\\frac{3}{2}\\quad\\Rightarrow\\quad \\delta=\\frac{1}{2}.\n$$\nThe Chernoff bound states\n$$\nP\\!\\left(X \\ge (1+\\delta)\\mu\\right) \\le \\exp\\!\\left(-\\frac{\\delta^{2}\\mu}{3}\\right).\n$$\nWith $\\delta=\\frac{1}{2}$ and $\\mu=\\frac{k}{3}$, we obtain\n$$\nP\\!\\left(X \\ge \\frac{k}{2}\\right) \\le \\exp\\!\\left(-\\frac{\\left(\\frac{1}{2}\\right)^{2}\\cdot \\frac{k}{3}}{3}\\right)=\\exp\\!\\left(-\\frac{k}{36}\\right).\n$$\n\nWe require the overall algorithmic failure probability to be strictly less than the hardware error probability $P_{\\text{cosmic}}=2.5 \\times 10^{-15}$:\n$$\n\\exp\\!\\left(-\\frac{k}{36}\\right)  2.5 \\times 10^{-15}.\n$$\nTaking natural logarithms and solving for $k$,\n$$\n-\\frac{k}{36}  \\ln(2.5) + \\ln\\!\\left(10^{-15}\\right)=\\ln(2.5) - 15\\ln(10),\n$$\n$$\n\\frac{k}{36}  15\\ln(10) - \\ln(2.5),\n$$\n$$\nk  36\\bigl(15\\ln(10) - \\ln(2.5)\\bigr).\n$$\nUsing the provided approximations $\\ln(10)\\approx 2.3026$ and $\\ln(2.5)\\approx 0.9163$,\n$$\n15\\ln(10) - \\ln(2.5) \\approx 15\\cdot 2.3026 - 0.9163 = 34.539 - 0.9163 = 33.6227,\n$$\n$$\n36 \\cdot 33.6227 \\approx 1210.4172.\n$$\nSince the inequality is strict, the smallest integer $k$ satisfying it is\n$$\nk=1211.\n$$",
            "answer": "$$\\boxed{1211}$$"
        },
        {
            "introduction": "Robust complexity classes, like P, are closed under basic logical operations such as union and intersection. This raises a natural question: is BPP also closed under these operations? This exercise challenges you to construct a new probabilistic machine that decides the intersection of two BPP languages and to analyze its error probability, providing insight into the structural properties of probabilistic computation .",
            "id": "1450946",
            "problem": "In computational complexity theory, the class BPP, which stands for Bounded-error Probabilistic Polynomial time, represents the set of decision problems solvable by a Probabilistic Turing Machine (PTM) in polynomial time with an error probability of at most 1/3.\n\nLet $L_1$ and $L_2$ be two languages, both of which are in BPP. This implies the existence of two PTMs, $M_1$ and $M_2$, that run in polynomial time and satisfy the following conditions for any given input string $x$:\n\n- For machine $M_1$ deciding language $L_1$:\n    - If $x \\in L_1$, the probability that $M_1$ accepts is $\\Pr[M_1(x) = 1] \\ge 2/3$.\n    - If $x \\notin L_1$, the probability that $M_1$ accepts is $\\Pr[M_1(x) = 1] \\le 1/3$.\n\n- For machine $M_2$ deciding language $L_2$:\n    - If $x \\in L_2$, the probability that $M_2$ accepts is $\\Pr[M_2(x) = 1] \\ge 2/3$.\n    - If $x \\notin L_2$, the probability that $M_2$ accepts is $\\Pr[M_2(x) = 1] \\le 1/3$.\n\nA new PTM, denoted $M_{int}$, is constructed to decide the intersection language $L_{int} = L_1 \\cap L_2$. The procedure for $M_{int}$ on an input $x$ is as follows:\n1. Run $M_1(x)$.\n2. If $M_1(x)$ outputs 1 (accepts), then subsequently run $M_2(x)$ and output its result.\n3. If $M_1(x)$ outputs 0 (rejects), then immediately output 0.\n\nAssume that the random choices made by $M_1$ and $M_2$ are statistically independent. What is the smallest value $\\epsilon$ such that the error probability of $M_{int}$ is guaranteed to be less than or equal to $\\epsilon$ for any input $x$?\n\nA. 1/3\n\nB. 4/9\n\nC. 5/9\n\nD. 2/3\n\nE. 8/9",
            "solution": "Let $p_{1}=\\Pr[M_{1}(x)=1]$ and $p_{2}=\\Pr[M_{2}(x)=1]$. By construction, $M_{\\text{int}}$ accepts $x$ if and only if $M_{1}$ accepts and then $M_{2}$ accepts. Under the stated independence of the random choices of $M_{1}$ and $M_{2}$, the acceptance probability of $M_{\\text{int}}$ is\n$$\n\\Pr[M_{\\text{int}}(x)=1]=\\Pr[M_{1}(x)=1]\\Pr[M_{2}(x)=1]=p_{1}p_{2}.\n$$\nHence the rejection probability is $1-p_{1}p_{2}$.\n\nConsider first the case $x\\in L_{1}\\cap L_{2}$. Then $p_{1}\\ge \\frac{2}{3}$ and $p_{2}\\ge \\frac{2}{3}$. The error of $M_{\\text{int}}$ on such $x$ is the probability it outputs $0$, which equals $1-p_{1}p_{2}$. Using the lower bounds,\n$$\n\\Pr[\\text{error}\\mid x\\in L_{1}\\cap L_{2}]=1-p_{1}p_{2}\\le 1-\\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right)=1-\\frac{4}{9}=\\frac{5}{9}.\n$$\n\nNow consider $x\\notin L_{1}\\cap L_{2}$. Then at least one of the following holds.\n\n- If $x\\notin L_{1}$, then $p_{1}\\le \\frac{1}{3}$, while $p_{2}$ is constrained by membership in $L_{2}$: if $x\\in L_{2}$ then $p_{2}\\ge \\frac{2}{3}$ (and can be as large as $1$), and if $x\\notin L_{2}$ then $p_{2}\\le \\frac{1}{3}$. In either subcase,\n$$\n\\Pr[\\text{error}]=\\Pr[M_{\\text{int}}(x)=1]=p_{1}p_{2}\\le \\max\\left\\{\\left(\\frac{1}{3}\\right)\\cdot 1,\\left(\\frac{1}{3}\\right)\\left(\\frac{1}{3}\\right)\\right\\}=\\frac{1}{3}.\n$$\n\n- If $x\\in L_{1}$ but $x\\notin L_{2}$, then $p_{1}\\ge \\frac{2}{3}$ (up to $1$) and $p_{2}\\le \\frac{1}{3}$, so\n$$\n\\Pr[\\text{error}]=p_{1}p_{2}\\le 1\\cdot \\frac{1}{3}=\\frac{1}{3}.\n$$\n\nThus, for all $x\\notin L_{1}\\cap L_{2}$, the error probability is at most $\\frac{1}{3}$.\n\nTaking the worst case over all inputs, the error probability of $M_{\\text{int}}$ is bounded by\n$$\n\\epsilon=\\max\\left\\{\\frac{5}{9},\\frac{1}{3}\\right\\}=\\frac{5}{9}.\n$$\nTherefore, the smallest $\\epsilon$ that is guaranteed for all inputs is $\\frac{5}{9}$, corresponding to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "The power of BPP seems to stem from its access to a source of randomness. But how much randomness is truly necessary for a problem to be considered probabilistic rather than deterministic? This thought experiment explores a fascinating boundary case where a probabilistic algorithm has access to only a logarithmic number of random bits. By analyzing this scenario, you will uncover a surprising and fundamental result that connects BPP directly back to the deterministic class P, highlighting how a limited amount of randomness can sometimes be fully derandomized .",
            "id": "1450965",
            "problem": "A technology startup has developed a novel probabilistic algorithm, called `PathCheck`, for a decision problem they term `DYNAMIC-PATH-VALIDITY`. The problem is to determine if a specified path in a network remains valid under a complex set of dynamically changing constraints. The precise nature of these constraints is proprietary, but the performance characteristics of the `PathCheck` algorithm are known.\n\nFor any given input instance (encoding the network and the path) of size $n$, the `PathCheck` algorithm has the following properties:\n1.  It always halts within a time bounded by the polynomial $p(n) = n^4 + 100n^2$.\n2.  It uses exactly $k(n) = 10 \\log_2(n)$ random bits to make its decision.\n3.  The algorithm has a two-sided error probability of at most $\\epsilon = 1/4$. That is, for any input, the probability that `PathCheck` returns the incorrect answer is at most $1/4$.\n\nRecall the definitions of the following complexity classes:\n- **P (Polynomial Time)**: The class of decision problems solvable by a deterministic algorithm in a number of steps that is a polynomial function of the input size $n$.\n- **BPP (Bounded-error Probabilistic Polynomial time)**: The class of decision problems solvable by a probabilistic algorithm in polynomial time, where for any input, the algorithm gives the correct answer with a probability of at least $2/3$. The constant $2/3$ can be replaced by any constant greater than $1/2$ without changing the class.\n\nBased *only* on the documented properties of the `PathCheck` algorithm, what is the strongest certain conclusion that can be drawn about the complexity class of the `DYNAMIC-PATH-VALIDITY` problem?\n\nA. `DYNAMIC-PATH-VALIDITY` is in P.\n\nB. `DYNAMIC-PATH-VALIDITY` is in BPP, but there is not enough information to conclude it is in P.\n\nC. `DYNAMIC-PATH-VALIDITY` is in NP, but there is not enough information to conclude it is in BPP.\n\nD. `DYNAMIC-PATH-VALIDITY` is not in BPP because its error probability of $1/4$ is not less than or equal to $1/3$.\n\nE. The properties of `PathCheck` are insufficient to place `DYNAMIC-PATH-VALIDITY` in any complexity class smaller than EXP (Exponential Time).",
            "solution": "We are given a probabilistic algorithm PathCheck for input size $n$ with the following properties:\n- Running time bounded by the polynomial $p(n) = n^{4} + 100 n^{2}$.\n- Uses exactly $k(n) = 10 \\log_{2}(n)$ random bits.\n- Two-sided error probability at most $\\epsilon = \\frac{1}{4}$ on any input.\n\nFirst, note that the total number of random bit strings (seeds) used by PathCheck on inputs of size $n$ is\n$$\nS(n) = 2^{k(n)} = 2^{10 \\log_{2}(n)} = \\left(2^{\\log_{2}(n)}\\right)^{10} = n^{10}.\n$$\nFor any fixed input $x$, the error probability bound implies that the number of seeds that cause an incorrect answer, denoted $B(x)$, satisfies\n$$\n|B(x)| \\le \\epsilon \\cdot S(n) = \\frac{1}{4} n^{10}.\n$$\nTherefore, the number of seeds that produce the correct answer, denoted $G(x)$, satisfies\n$$\n|G(x)| = S(n) - |B(x)| \\ge n^{10} - \\frac{1}{4} n^{10} = \\frac{3}{4} n^{10}  \\frac{1}{2} n^{10}.\n$$\nHence, for any input $x$, strictly more than half of all seeds yield the correct answer.\n\nConstruct a deterministic algorithm for the decision problem as follows: enumerate all $S(n) = n^{10}$ seeds $r \\in \\{0,1\\}^{k(n)}$, run PathCheck on input $x$ with randomness fixed to $r$, and take the majority output. This works because the majority of seeds yield the correct answer, so the majority vote equals the correct decision on $x$.\n\nThe running time of this deterministic simulation is at most\n$$\nT(n) \\le S(n) \\cdot p(n) = n^{10} \\left(n^{4} + 100 n^{2}\\right) = n^{14} + 100 n^{12},\n$$\nwhich is a polynomial in $n$. Therefore, there exists a deterministic polynomial-time algorithm for the problem, implying that the language is in P.\n\nAlthough the given properties also certify membership in BPP (since correctness probability $\\frac{3}{4} \\ge \\frac{2}{3}$ and runtime is polynomial), the strongest certain conclusion from the additional fact of using only $O(\\log n)$ random bits is that the problem is in P by exhaustive enumeration of all seeds within polynomial time. Thus, the correct choice is A.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}