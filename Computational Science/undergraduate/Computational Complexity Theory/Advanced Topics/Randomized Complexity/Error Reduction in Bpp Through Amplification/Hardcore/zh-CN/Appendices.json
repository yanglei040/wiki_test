{
    "hands_on_practices": [
        {
            "introduction": "要掌握放大技术，第一步是能够量化其效果。本练习提供了一个核心场景：对于一个典型的BPP算法，我们需要重复运行多少次才能将其错误率降低到几乎可以忽略不计的目标值？通过这个计算 ，你将直接体会到重复和多数投票策略在增强算法可靠性方面的强大威力。",
            "id": "1422498",
            "problem": "一个计算理论研究小组为一个判定问题设计了一种新的概率算法。该算法属于复杂度类 BPP（Bounded-error Probabilistic Polynomial-time，有界错误概率多项式时间），这意味着对于任何给定的输入，它产生正确“是”或“否”答案的概率至少为 $2/3$。因此，任何单次运行出错的概率 $\\epsilon_0$ 至多为 $1/3$。\n\n为了提高结果的可信度，研究人员采用了一种放大策略：他们在相同的输入上独立运行该算法 $k$ 次，并将多数票的结果作为最终答案。他们希望对一次特别重要的计算达到高度的确定性，要求最终多数票出错的概率小于 $2^{-100}$。\n\n对于这个问题，你可以使用以下来自计算理论的标准结果，它是 Chernoff 界的推论：经过 $k$ 次试验后，多数票程序的错误概率 $\\delta_k$ 受以下不等式约束：\n$$ \\delta_k \\le \\exp\\left(-2k\\left(\\frac{1}{2} - \\epsilon_0\\right)^2\\right) $$\n其中 $\\epsilon_0$ 是单次试验的错误概率，你应该取其最坏情况下的值。\n\n确定所需的最小整数试验次数 $k$，以确保最终错误概率小于 $2^{-100}$。在你的计算中，请使用近似值 $\\ln(2) \\approx 0.69315$。",
            "solution": "我们已知多数票在 $k$ 次独立运行后的错误率的 Chernoff 型界为：\n$$\n\\delta_{k} \\le \\exp\\left(-2k\\left(\\frac{1}{2}-\\epsilon_{0}\\right)^{2}\\right),\n$$\n且单次运行在最坏情况下的错误率为 $\\epsilon_{0}=\\frac{1}{3}$。代入 $\\epsilon_{0}=\\frac{1}{3}$ 可得\n$$\n\\left(\\frac{1}{2}-\\epsilon_{0}\\right)=\\frac{1}{2}-\\frac{1}{3}=\\frac{1}{6},\n$$\n所以\n$$\n\\delta_{k} \\le \\exp\\left(-2k\\left(\\frac{1}{6}\\right)^{2}\\right)=\\exp\\left(-\\frac{k}{18}\\right).\n$$\n为确保最终错误概率小于 $2^{-100}$，我们只需满足\n$$\n\\exp\\left(-\\frac{k}{18}\\right)  2^{-100}.\n$$\n对两边取自然对数，得到\n$$\n-\\frac{k}{18}  -100\\,\\ln(2),\n$$\n这等价于\n$$\n\\frac{k}{18} > 100\\,\\ln(2) \\quad \\Longrightarrow \\quad k > 1800\\,\\ln(2).\n$$\n由于 $k$ 必须是整数，所以最小允许值为\n$$\nk_{\\min}=\\lceil 1800\\,\\ln(2)\\rceil.\n$$\n使用给定的近似值 $\\ln(2)\\approx 0.69315$，\n$$\n1800\\,\\ln(2)\\approx 1800\\times 0.69315=1247.67\\ldots,\n$$\n因此\n$$\nk_{\\min}=1248.\n$$",
            "answer": "$$\\boxed{1248}$$"
        },
        {
            "introduction": "初始错误率的微小差异在放大后会产生多大的影响？这个问题通过直接比较两个具有不同初始可靠性的算法来探讨这一关键点 。你会发现，初始成功率的适度提升可以显著减少达到相同可靠性目标所需的重复次数，从而揭示了放大过程中初始优势的非线性放大效应。",
            "id": "1422496",
            "problem": "考虑两个概率算法 $\\mathcal{A}_1$ 和 $\\mathcal{A}_2$，它们被设计用来解决同一个判定问题，输入大小为 $n$。\n\n任一算法单次执行的运行时间由一个多项式函数 $T(n)$ 给出。\n\n对于任意给定的输入，算法 $\\mathcal{A}_1$ 返回正确答案的概率至少为 $3/4$。相比之下，算法 $\\mathcal{A}_2$ 的可靠性较低，其返回正确答案的概率至少为 $2/3$。两种算法的错误都是双边的，这意味着它们在问题的“是”实例和“否”实例上都可能出错。\n\n为了提高它们的可靠性，对这两种算法都采用了一种标准的放大方法。该方法包括将算法独立运行 $k$ 次，并将结果的多数票作为最终答案。对于每种算法，我们想要确定所需的最小重复次数（$\\mathcal{A}_1$ 为 $k_1$，$\\mathcal{A}_2$ 为 $k_2$），以确保放大后算法的最终错误概率最多为 $2^{-n}$。\n\n设 $Time_1(n) = k_1 T(n)$ 和 $Time_2(n) = k_2 T(n)$ 分别是 $\\mathcal{A}_1$ 和 $\\mathcal{A}_2$ 放大版本的总时间复杂度。假设 $n$ 很大，确定比率 $\\frac{Time_2(n)}{Time_1(n)}$ 的值。\n\n将你的答案表示为一个精确的分数。",
            "solution": "问题要求计算将两个概率算法放大到目标错误概率为 $2^{-n}$ 时，其总运行时间的渐近比率 $\\frac{Time_2(n)}{Time_1(n)}$。这个比率等同于每种算法所需的重复次数之比 $\\frac{k_2}{k_1}$，因为两种算法的单次运行时间 $T(n)$ 是相同的。\n\n首先，我们来确定每种算法单次运行的错误概率。\n对于算法 $\\mathcal{A}_1$，成功概率为 $p_1 = 3/4$，所以错误概率为 $\\epsilon_1 = 1 - p_1 = 1 - 3/4 = 1/4$。\n对于算法 $\\mathcal{A}_2$，成功概率为 $p_2 = 2/3$，所以错误概率为 $\\epsilon_2 = 1 - p_2 = 1 - 2/3 = 1/3$。\n两种错误概率都满足 $\\epsilon_i  1/2$，这是通过多数票决定的放大方法有效的必要条件。\n\n放大过程包括将一个错误率为 $\\epsilon$ 的算法运行 $k$ 次独立试验，并取多数票。一个常用的Chernoff界表明，多数结果出错的概率的上界为：\n$$ P(\\text{error}) \\le \\exp(-2k(1/2 - \\epsilon)^2) $$\n我们希望这个最终错误概率最多为 $2^{-n}$。因此，我们建立不等式：\n$$ \\exp(-2k(1/2 - \\epsilon)^2) \\le 2^{-n} $$\n对两边取自然对数：\n$$ -2k(1/2 - \\epsilon)^2 \\le n \\ln(2^{-1}) = -n \\ln(2) $$\n两边乘以 $-1$ 并反转不等号：\n$$ 2k(1/2 - \\epsilon)^2 \\ge n \\ln(2) $$\n解出 $k$，我们得到所需的最小重复次数：\n$$ k \\ge \\frac{n \\ln(2)}{2(1/2 - \\epsilon)^2} $$\n对于大的 $n$，我们可以用右侧的值来近似所需的最小重复次数 $k$，因为向上取整函数对比率的影响可以忽略不计。\n$$ k \\approx \\frac{n \\ln(2)}{2(1/2 - \\epsilon)^2} $$\n\n现在，我们计算每种算法所需的重复次数。\n\n对于算法 $\\mathcal{A}_1$，其 $\\epsilon_1 = 1/4$：\n$$ k_1 \\approx \\frac{n \\ln(2)}{2(1/2 - 1/4)^2} = \\frac{n \\ln(2)}{2(1/4)^2} = \\frac{n \\ln(2)}{2(1/16)} = 8n \\ln(2) $$\n\n对于算法 $\\mathcal{A}_2$，其 $\\epsilon_2 = 1/3$：\n$$ k_2 \\approx \\frac{n \\ln(2)}{2(1/2 - 1/3)^2} = \\frac{n \\ln(2)}{2(1/6)^2} = \\frac{n \\ln(2)}{2(1/36)} = 18n \\ln(2) $$\n\n最后，我们计算总运行时间的比率。\n$$ \\frac{Time_2(n)}{Time_1(n)} = \\frac{k_2 T(n)}{k_1 T(n)} = \\frac{k_2}{k_1} $$\n代入 $k_1$ 和 $k_2$ 的表达式：\n$$ \\frac{k_2}{k_1} \\approx \\frac{18n \\ln(2)}{8n \\ln(2)} = \\frac{18}{8} = \\frac{9}{4} $$\n该比率是常数，不依赖于 $n$ 或 $T(n)$。",
            "answer": "$$\\boxed{\\frac{9}{4}}$$"
        },
        {
            "introduction": "标准的放大策略是简单地运行算法固定的 $k$ 次，但我们能否设计出更智能的方案？本练习将挑战你分析一种自适应放大方案，该方案根据初始运行结果的明确程度来决定是否需要更多的计算 。这促使我们将理解从简单的公式应用提升到更复杂的算法设计和分析层面。",
            "id": "1422501",
            "problem": "考虑一个在复杂性类别 BPP（有界错误概率多项式时间）中的语言 $L$。这意味着存在一个概率图灵机 (PTM) $M$，它能在多项式时间内判定 $L$，且错误概率最多为 $1/3$。也就是说，对于任意输入字符串 $x$，$M(x)$ 输出正确答案（即，如果 $x \\in L$ 则输出‘是’，如果 $x \\notin L$ 则输出‘否’）的概率至少为 $2/3$。\n\n为了降低对于给定输入 $x$ 的错误概率，我们提出一种新颖的自适应放大方案。该方案工作如下：\n\n1.  对输入 $x$ 独立运行机器 $M$ 共 $k$ 次。设 $N_1$ 为输出‘是’的次数，$N_0$ 为输出‘否’的次数，其中 $N_1 + N_0 = k$。\n2.  计算票数差，定义为 $|N_1 - N_0|$。\n3.  如果票数差 $|N_1 - N_0|$ 大于 $\\sqrt{k}$，算法停止并输出这 $k$ 次试验的多数票结果。\n4.  如果票数差 $|N_1 - N_0|$ 小于或等于 $\\sqrt{k}$，算法将额外执行 $k$ 次独立试验。然后输出总共 $2k$ 次试验的多数票结果。\n\n假设对于足够多次独立的伯努利试验 $m$，每次试验的成功概率为 $p$，成功次数 $Z$ 集中在其均值 $mp$ 附近。您可以使用以下版本的 Chernoff 界来分析这种集中现象：对于任意 $\\epsilon > 0$，\n$$\n\\Pr\\left[\\left|\\frac{Z}{m} - p\\right| > \\epsilon\\right] \\le 2 \\exp(-2m\\epsilon^2)\n$$\n\n确定该自适应方案输出错误答案的总概率的上界的一个闭式解析表达式。您的表达式应为 $k$ 的函数。",
            "solution": "设 $p$ 为机器 $M$ 对给定输入 $x$ 给出正确答案的概率。根据 BPP 的定义，我们有 $p \\ge 2/3$。我们将在最坏情况下进行分析，即取 $p = 2/3$。\n\n令 $S_m$ 为在 $m$ 次独立运行中得到正确答案的次数。不失一般性，我们假设正确答案是‘是’。那么‘是’的票数 $N_1 = S_m$，‘否’的票数 $N_0 = m - S_m$。票数差为 $|N_1 - N_0| = |S_m - (m - S_m)| = |2S_m - m|$。\n\n算法在两种互斥的情况下会产生错误：\n1.  **事件A**：在第一阶段（$k$ 次运行后）停止并给出错误答案。这要求 $|2S_k - k| > \\sqrt{k}$ 且多数票是错误的（即 $S_k  k/2$）。\n2.  **事件B**：算法进入第二阶段（额外进行 $k$ 次运行）并给出错误答案。这要求 $|2S_k - k| \\le \\sqrt{k}$ 且在总共 $2k$ 次运行后多数票是错误的（即 $S_{2k}  k$）。\n\n总错误概率为 $P(\\text{错误}) = P(A) + P(B)$。\n\n**分析事件A：**\n条件 $|2S_k - k| > \\sqrt{k}$ 等价于 $|S_k - k/2| > \\sqrt{k}/2$。结合错误条件 $S_k  k/2$，我们得到 $S_k  k/2 - \\sqrt{k}/2$。\n因此，$P(A) = P(S_k  k/2 - \\sqrt{k}/2) = P(S_k/k  1/2 - 1/(2\\sqrt{k}))$。\n我们要使用问题中给出的Chernoff界来约束此概率。设 $Z=S_k$, $m=k$, $p=2/3$。\n事件 $S_k/k  1/2 - 1/(2\\sqrt{k})$ 意味着 $S_k/k - p  (1/2 - 1/(2\\sqrt{k})) - 2/3 = -1/6 - 1/(2\\sqrt{k})$。\n这意味着 $|S_k/k - p| > 1/6 + 1/(2\\sqrt{k})$。令 $\\epsilon_A = 1/6 + 1/(2\\sqrt{k})$。\n根据给定的Chernoff界：\n$P(A) \\le P(|S_k/k - p| > \\epsilon_A) \\le 2\\exp(-2k\\epsilon_A^2)$\n$P(A) \\le 2\\exp\\left(-2k\\left(\\frac{1}{6} + \\frac{1}{2\\sqrt{k}}\\right)^2\\right) = 2\\exp\\left(-2k\\left(\\frac{1}{36} + \\frac{1}{6\\sqrt{k}} + \\frac{1}{4k}\\right)\\right)$\n$P(A) \\le 2\\exp\\left(-\\frac{k}{18} - \\frac{\\sqrt{k}}{3} - 1\\right)$。\n注意：这里的计算中有一点小错误，$\\exp(-2k(1/(4k)))$ 应该是 $\\exp(-1/2)$，而不是 $\\exp(-1)$。修正如下：\n$P(A) \\le 2\\exp\\left(-\\frac{k}{18} - \\frac{\\sqrt{k}}{3} - \\frac{1}{2}\\right)$.\n\n**分析事件B：**\n$P(B) = P(|2S_k - k| \\le \\sqrt{k} \\text{ and } S_{2k}  k)$。我们可以通过忽略第一个条件来获得一个上界：\n$P(B) \\le P(S_{2k}  k) = P(S_{2k}/(2k)  1/2)$。\n我们再次使用给定的Chernoff界。设 $Z=S_{2k}$, $m=2k$, $p=2/3$。\n事件 $S_{2k}/(2k)  1/2$ 意味着 $S_{2k}/(2k) - p  1/2 - 2/3 = -1/6$。\n这意味着 $|S_{2k}/(2k) - p| > 1/6$。令 $\\epsilon_B = 1/6$。\n根据给定的Chernoff界：\n$P(B) \\le P(|S_{2k}/(2k) - p| > \\epsilon_B) \\le 2\\exp(-2m\\epsilon_B^2)$\n$P(B) \\le 2\\exp(-2(2k)(1/6)^2) = 2\\exp(-4k/36) = 2\\exp(-k/9)$。\n\n**总错误概率上界：**\n将两个事件的概率上界相加，我们得到总错误概率的上界：\n$P(\\text{错误}) \\le P(A) + P(B) \\le 2\\exp\\left(-\\frac{k}{18} - \\frac{\\sqrt{k}}{3} - \\frac{1}{2}\\right) + 2\\exp\\left(-\\frac{k}{9}\\right)$。",
            "answer": "$$\\boxed{2\\exp\\left(-\\frac{k}{18}-\\frac{\\sqrt{k}}{3}-\\frac{1}{2}\\right)+2\\exp\\left(-\\frac{k}{9}\\right)}$$"
        }
    ]
}