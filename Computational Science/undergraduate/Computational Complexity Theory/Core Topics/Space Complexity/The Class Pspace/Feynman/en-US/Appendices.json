{
    "hands_on_practices": [
        {
            "introduction": "Understanding a complexity class often begins with exploring its fundamental properties, such as which operations it is \"closed\" under. This first exercise  challenges you to think about how to combine two polynomial-space algorithms to solve a new problem, specifically for the concatenation of two languages. The solution hinges on the crucial concept of space reuse, a defining characteristic of PSPACE algorithms that separates them from time-bounded computations.",
            "id": "1454902",
            "problem": "In computational complexity theory, a complexity class is a set of problems that can be solved by a computational model under certain resource constraints. The class PSPACE (Polynomial Space) consists of all decision problems that can be solved by a deterministic Turing machine using a polynomial amount of space in relation to the size of the input.\n\nLet $L_1$ and $L_2$ be two languages decided by deterministic Turing machines $M_1$ and $M_2$ in space bounded by polynomials $p_1(n)$ and $p_2(n)$ respectively, where $n$ is the length of the input string. Thus, by definition, $L_1 \\in \\text{PSPACE}$ and $L_2 \\in \\text{PSPACE}$.\n\nThe concatenation of these two languages, denoted $L = L_1 \\circ L_2$, is defined as the set of all strings $w$ that can be formed by concatenating a string $x$ from $L_1$ with a string $y$ from $L_2$. That is, $L = \\{xy \\mid x \\in L_1 \\text{ and } y \\in L_2\\}$.\n\nWe wish to show that the class PSPACE is closed under the concatenation operation, meaning that if $L_1, L_2 \\in \\text{PSPACE}$, then $L_1 \\circ L_2 \\in \\text{PSPACE}$. This requires constructing a Turing machine $M$ that decides $L_1 \\circ L_2$ using only a polynomial amount of space.\n\nGiven an input string $w$ of length $n$, which of the following choices describes a correct high-level algorithm for a deterministic Turing machine $M$ to decide if $w \\in L$ while using only polynomial space?\n\nA. The machine $M$ first enumerates and stores on its tape all strings in $L_1$ of length up to $n$. It then enumerates and stores all strings in $L_2$ of length up to $n$. Finally, for every stored string $x$ from $L_1$ and every stored string $y$ from $L_2$, it checks if their concatenation $xy$ equals the input $w$. If a match is found, it accepts; otherwise, it rejects.\n\nB. The machine $M$ systematically tries every possible way to split the input string $w$ into two non-empty parts, $w = xy$. For each such split, $M$ first simulates the machine $M_1$ on the prefix $x$. If $M_1$ accepts, $M$ erases its work tape and then simulates machine $M_2$ on the suffix $y$. If $M_2$ also accepts, machine $M$ halts and accepts. If all possible splits are checked and none result in both simulations accepting, $M$ halts and rejects.\n\nC. The machine $M$ splits the input string $w$ exactly in half, into a prefix $x$ and a suffix $y$. It then uses a two-tape Turing machine to simulate $M_1$ on $x$ on the first tape and $M_2$ on $y$ on the second tape. If both simulations accept, $M$ accepts. Otherwise, $M$ rejects.\n\nD. Since the languages $L_1$ and $L_2$ could be infinite, it is impossible to check all possible pairs $(x, y)$ to see if their concatenation forms $w$. Therefore, the problem of deciding membership in $L_1 \\circ L_2$ is undecidable, and no such Turing machine $M$ can exist.",
            "solution": "We are given $L_{1},L_{2} \\in \\text{PSPACE}$ with deterministic deciders $M_{1}$ and $M_{2}$ using at most $p_{1}(n)$ and $p_{2}(n)$ space, respectively, on inputs of length $n$. For $w \\in \\Sigma^{*}$ of length $n$, $w \\in L_{1} \\circ L_{2}$ iff there exists an index $i$ with $0 \\leq i \\leq n$ such that the prefix $x = w[1..i] \\in L_{1}$ and the suffix $y = w[i+1..n] \\in L_{2}$. A deterministic machine $M$ can decide this by iterating over all splits $i$ and simulating $M_{1}$ on $x$ and $M_{2}$ on $y$.\n\nSpace analysis: define $p(n) = \\max\\{p_{1}(n),p_{2}(n)\\}$. For a fixed split $i$, $|x| = i \\leq n$ and $|y| = n - i \\leq n$, so the simulations use at most $p_{1}(i) \\leq p_{1}(n) \\leq p(n)$ space and $p_{2}(n-i) \\leq p_{2}(n) \\leq p(n)$ space. Since $M$ first simulates $M_{1}$ on $x$ and, only if it accepts, erases its work tape and simulates $M_{2}$ on $y$, the peak space used is at most $p(n)$ plus overhead to manage the split and the simulation. To feed $x$ and $y$ to the simulators, $M$ can either:\n- keep the input on a read-only tape and maintain a counter for the split index $i$, interpreting attempts by the simulated machine to read past the designated end as blanks; the counter uses $O(\\log n)$ space; or\n- copy the relevant substring onto a work tape before each simulation, which uses $O(n)$ space.\n\nIn both cases, the total space per split is bounded by\n$$\nq(n) = p(n) + O(n),\n$$\nwhich is polynomial in $n$. The loop over all $i \\in \\{0,1,\\dots,n\\}$ is sequential, so space does not accumulate across iterations; only time may blow up, which does not affect PSPACE membership. Therefore, this procedure decides $L_{1} \\circ L_{2}$ in polynomial space, establishing closure of PSPACE under concatenation.\n\nEvaluation of options:\n- A stores all strings up to length $n$ from $L_{1}$ and $L_{2}$, which requires exponential space in $n$, so it is not polynomial-space.\n- B describes the standard polynomial-space procedure: try each split, simulate $M_{1}$ on the prefix, and if it accepts, simulate $M_{2}$ on the suffix after erasing the work tape. This uses at most $q(n)$ space as argued. To cover all cases exactly matching the definition $L_{1} \\circ L_{2} = \\{xy : x \\in L_{1}, y \\in L_{2}\\}$, one should include the splits $i=0$ and $i=n$ as well; this is a trivial adjustment that does not change the space bound.\n- C only checks the split at $i = \\lfloor n/2 \\rfloor$, which is insufficient, since valid decompositions may occur at any $i$.\n- D is false: the membership problem for $L_{1} \\circ L_{2}$ is decidable in polynomial space by the algorithm above.\n\nHence the correct high-level algorithm is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "One of the cornerstone results in complexity theory is Savitch's Theorem, which shows that non-deterministic space can be simulated deterministically with only a polynomial-squared increase in space. The core of its proof is a clever recursive \"midpoint\" technique for solving reachability problems. This practice problem  puts you in the driver's seat, asking you to analyze the space efficiency of an algorithm based on this exact principle, providing an intuitive feel for how this powerful method works.",
            "id": "1454887",
            "problem": "Consider a computational system known as the \"$n$-bit Configuration Lock\". A \"configuration\" in this system is an $n$-bit binary string, which can be interpreted as an integer from $0$ to $2^n - 1$. The system evolves in discrete time steps. From any given configuration $C$, it can transition to a new configuration $C'$ in a single step through one of two possible non-deterministic operations. These operations are defined based on the integer value $v$ of the configuration string $C$:\n\n1.  Operation A: The new configuration's integer value is $v' = (3v + 1) \\pmod{2^n}$.\n2.  Operation B: The new configuration's integer value is $v' = (5v + 1) \\pmod{2^n}$.\n\nWe are interested in the reachability problem: can a starting configuration $C_{start}$ reach a target configuration $C_{target}$ in exactly $k$ steps?\n\nA recursive algorithm named `CanReach(C_1, C_2, t)` is proposed to solve this problem. It determines if configuration $C_2$ is reachable from $C_1$ in exactly $t$ steps. The logic of the algorithm is as follows:\n\n-   **Base Case:** If $t=1$, the function checks if $C_2$ can be reached from $C_1$ in a single step using either Operation A or Operation B. It returns `True` if a valid transition exists, and `False` otherwise.\n-   **Recursive Step:** If $t > 1$, the function iterates through all $2^n$ possible configurations for an intermediate state, `C_mid`. For each `C_mid`, it checks if `C_mid` is reachable from `C_1` in $\\lfloor t/2 \\rfloor$ steps AND `C_2` is reachable from `C_mid` in $\\lceil t/2 \\rceil$ steps by making two separate recursive calls. If both calls return `True` for any `C_mid`, the function immediately returns `True`. If the loop finishes without finding such a `C_mid`, the function returns `False`.\n\nYour task is to analyze the space complexity of this algorithm. An initial call is made to `CanReach(C_{start}, C_{target}, 2^n)`. Assume that storing one $n$-bit configuration requires space proportional to $n$, and storing the integer step count $t$ (which can be as large as $2^n$) also requires space proportional to $n$. The total space required by the recursion stack can be written in big-O notation as $O(f(n))$ for some function $f(n)$. Determine this function $f(n)$.",
            "solution": "We analyze the recursion stack space of the function call tree. Each activation record (stack frame) must store its parameters and loop state:\n- Each configuration parameter, $C_{1}$ and $C_{2}$, requires space proportional to $n$, so $c_{1} n$ and $c_{2} n$ for fixed positive constants $c_{1},c_{2}$.\n- The step count $t$ can be as large as $2^{n}$, which requires space proportional to $n$, say $c_{3} n$.\n- The loop iterates over all $2^{n}$ possible $C_{\\text{mid}}$, but at any time it only stores the current $C_{\\text{mid}}$, which requires space proportional to $n$, say $c_{4} n$.\n- Any additional control or constant-sized state is $O(1)$ and is subsumed in a constant $c_{5}$.\n\nThus the space per frame satisfies\n$$\ns(n) \\leq (c_{1}+c_{2}+c_{3}+c_{4})\\,n + c_{5} = a\\,n + c_{5}, \n$$\nfor some constant $a>0$, hence $s(n)=\\Theta(n)$.\n\nNext, we determine the maximum recursion depth. Let $D(t)$ denote the maximum depth for input step count $t$. The base case uses one frame, so $D(1)=1$. For $t>1$, the function makes two recursive calls with step counts $\\lfloor t/2 \\rfloor$ and $\\lceil t/2 \\rceil$ but evaluates them sequentially (the second only after the first returns). Therefore, the maximum stack depth at any time is\n$$\nD(t) = 1 + \\max\\!\\big(D(\\lfloor t/2 \\rfloor),\\, D(\\lceil t/2 \\rceil)\\big) \\leq 1 + D(\\lceil t/2 \\rceil).\n$$\nBy repeated halving, after $m$ such steps the argument size is at most $\\lceil t/2^{m} \\rceil$. The recursion stops when this becomes $1$, which occurs for the smallest $m$ with $t \\leq 2^{m}$, i.e., $m=\\lceil \\log_{2} t \\rceil$. Hence\n$$\nD(t) \\leq \\lceil \\log_{2} t \\rceil + 1.\n$$\n\nThe initial call has $t=2^{n}$, so\n$$\nD(2^{n}) \\leq \\lceil \\log_{2}(2^{n}) \\rceil + 1 = n + 1 = \\Theta(n).\n$$\n\nThe total recursion stack space is the product of the maximum depth and the per-frame space:\n$$\nS(n) \\leq D(2^{n}) \\cdot s(n) \\leq (n+1)\\,(a n + c_{5}) = a n^{2} + (a + c_{5}) n + c_{5}.\n$$\nTherefore $S(n) = O(n^{2})$. Since $S(n)$ is expressed as $O(f(n))$, we conclude\n$$\nf(n) = n^{2}.\n$$",
            "answer": "$$\\boxed{n^{2}}$$"
        },
        {
            "introduction": "After learning how to build and analyze PSPACE algorithms, the next step is to classify the hardest problems in the class. A problem is PSPACE-complete if it is both in PSPACE and is PSPACE-hard, meaning it captures the full difficulty of the entire class. This final exercise  is a conceptual checkpoint that solidifies this crucial two-part definition, explaining why establishing an upper bound (membership in PSPACE) is just as important as establishing a lower bound (hardness).",
            "id": "1454906",
            "problem": "In computational complexity theory, the definition of completeness for a complexity class is used to identify the \"hardest\" problems within that class. Let `C` be an arbitrary complexity class, and let `L` be a language, which represents a decision problem.\n\nA language `L` is defined as **`C-hard`** if, for every language `L'` in `C`, `L'` is reducible to `L` in polynomial time. This is denoted as `L' \\le_p L`.\n\nA language `L` is defined as **`C-complete`** if it satisfies two distinct conditions:\n1. `L` is a member of the class `C` (i.e., `L \\in C`).\n2. `L` is `C-hard`.\n\nNow, consider the complexity class PSPACE, which is the set of all decision problems solvable by a Turing machine using a polynomial amount of space. A computer science student is attempting to prove that a newly discovered problem, which we will call `GAME-SOLVABILITY`, is PSPACE-complete. The student has already successfully completed the difficult part of the proof: they have formally shown that `GAME-SOLVABILITY` is PSPACE-hard.\n\nAccording to the formal definitions provided, which of the following statements best explains why it is still an essential and non-negotiable step for the student to also prove that `GAME-SOLVABILITY` is a member of the class PSPACE?\n\nA. Proving `GAME-SOLVABILITY` is in PSPACE demonstrates that it is at least as hard as any other problem in PSPACE, which is the primary definition of PSPACE-hardness.\nB. Proving `GAME-SOLVABILITY` is in PSPACE establishes a formal upper bound on its complexity, ensuring it is not fundamentally harder than the problems that define the class PSPACE itself.\nC. The polynomial-time reduction used to prove PSPACE-hardness is only considered valid if the target problem of the reduction (in this case, `GAME-SOLVABILITY`) is itself a member of PSPACE.\nD. If `GAME-SOLVABILITY` is not shown to be in PSPACE, it might be an unsolvable problem, which would contradict the fact that it is PSPACE-hard.",
            "solution": "By definition, for a complexity class $C$ and a language $L$, $L$ is $C$-hard if for every $L' \\in C$, there is a polynomial-time reduction $L' \\le_{p} L$. A language $L$ is $C$-complete if and only if it satisfies both $L \\in C$ and $L$ is $C$-hard. Therefore, to conclude that a language is $C$-complete, one must prove both a lower bound (hardness) and an upper bound (membership in $C$).\n\nIn the present case, with $C = \\mathrm{PSPACE}$ and $L = \\text{GAME-SOLVABILITY}$, the student has already shown that $L$ is $\\mathrm{PSPACE}$-hard; that is, for every $L' \\in \\mathrm{PSPACE}$, $L' \\le_{p} L$. To prove $\\mathrm{PSPACE}$-completeness, it remains essential to establish $L \\in \\mathrm{PSPACE}$, which provides the matching upper bound and thus pins the complexity of $L$ within $\\mathrm{PSPACE}$.\n\nEvaluate the options:\n- Option A is incorrect because showing $L \\in \\mathrm{PSPACE}$ does not demonstrate hardness; hardness is already captured by the reductions $L' \\le_{p} L$. Membership does not imply hardness.\n- Option B is correct because $L \\in \\mathrm{PSPACE}$ is precisely the needed upper bound that ensures $L$ is not harder than $\\mathrm{PSPACE}$, and together with hardness it yields completeness.\n- Option C is incorrect because the definition and validity of a polynomial-time reduction $L' \\le_{p} L$ do not require the target language $L$ to lie in $\\mathrm{PSPACE}$; reductions are syntactic transformations computed in polynomial time.\n- Option D is incorrect because being $\\mathrm{PSPACE}$-hard does not imply decidability or solvability within $\\mathrm{PSPACE}$. In fact, there exist undecidable languages that are $\\mathrm{PSPACE}$-hard under polynomial-time many-one reductions, so failing to show $L \\in \\mathrm{PSPACE}$ does not contradict hardness; it only prevents concluding completeness.\n\nThus, the essential reason to prove $L \\in \\mathrm{PSPACE}$ is to establish the formal upper bound required by the definition of completeness.",
            "answer": "$$\\boxed{B}$$"
        }
    ]
}