{
    "hands_on_practices": [
        {
            "introduction": "Understanding how to analyze nested loops is a fundamental skill in complexity analysis. This first practice problem  presents a common real-world scenario: finding common items between two distinct lists. By analyzing the straightforward nested-loop approach, you will practice quantifying the total number of operations in the worst case and expressing it using Big-O notation, which is essential for comparing algorithmic efficiency.",
            "id": "1469548",
            "problem": "A programmer is tasked with developing a feature for an e-commerce platform. The feature needs to identify which items from a user's wishlist are currently on sale. The wishlist is represented as an array of unique integer product IDs, let's call it `wishlist`, of size $m$. The list of items on sale is represented by a second array of unique integer product IDs, `saleItems`, of size $n$.\n\nTo find the common products, the programmer implements a straightforward algorithm. The algorithm iterates through each product ID in the `wishlist` array. For each product ID from the wishlist, it then iterates through every product ID in the `saleItems` array to check for a match.\n\nWhat is the worst-case time complexity of this algorithm, expressed using Big-O notation, as a function of the array sizes $m$ and $n$?\n\nA. $O(m + n)$\n\nB. $O(m \\log n + n \\log m)$\n\nC. $O(mn)$\n\nD. $O(\\max(m, n))$\n\nE. $O(\\min(m, n))$",
            "solution": "Let the outer loop iterate over the $m$ elements of the array $wishlist$, and for each element, the inner loop scans all $n$ elements of the array $saleItems$ to check for a match.\n\nAssume each comparison and loop overhead takes constant time $c>0$. Then the total number of constant-time operations in the worst case (when no early termination occurs and each inner loop scans all $n$ items) is\n$$\nT(m,n) = \\sum_{i=1}^{m} \\sum_{j=1}^{n} c = c \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} 1 \\right) = c \\sum_{i=1}^{m} n = c m n.\n$$\nThus, asymptotically,\n$$\nT(m,n) \\in O(mn).\n$$\nNo preprocessing or auxiliary data structures change this bound in the described algorithm, and the uniqueness of IDs does not reduce the worst-case number of comparisons per outer iteration.\n\nTherefore, the worst-case time complexity is $O(mn)$, which corresponds to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Algorithms are often composed of multiple, distinct phases, and the worst-case performance is determined by the path that maximizes the total work. This exercise  introduces a 'Skip-Scan Search' algorithm, which combines a skipping phase with a linear scanning phase. Your task is to dissect this two-part process to determine the exact number of comparisons in the worst-case scenario, highlighting how performance can depend on algorithmic parameters and input structure.",
            "id": "1469569",
            "problem": "A programmer is designing a custom search algorithm, called \"Skip-Scan Search,\" to find a target value $x$ within a large, sorted array `A` that contains $n$ distinct elements. The array is 0-indexed, meaning its elements are `A[0], A[1], ..., A[n-1]`. The algorithm's behavior is determined by an integer step size $k$, where $1  k  n$.\n\nThe Skip-Scan Search algorithm operates in two sequential phases:\n\n1.  **Skip Phase:** The algorithm first inspects the element at index $k-1$. If this element is less than $x$, it \"skips\" forward to inspect the element at index $2k-1$. This process continues, with the algorithm inspecting elements at indices $j \\cdot k - 1$ for $j = 1, 2, 3, \\ldots$, as long as the index $j \\cdot k - 1$ is within the array bounds (i.e., less than $n$) and the element $A[j \\cdot k - 1]$ is less than the target $x$.\n\n2.  **Scan Phase:** The Skip Phase terminates when, at some step $j$, either the element $A[j \\cdot k - 1]$ is greater than or equal to $x$, or the index $j \\cdot k - 1$ is no longer less than $n$.\n    *   If the Skip Phase stopped because $A[j \\cdot k - 1] \\ge x$, the algorithm proceeds to linearly scan the block of elements starting from index $(j-1) \\cdot k$.\n    *   If the Skip Phase stopped because it checked all possible skip points up to index $m \\cdot k - 1$ (where $(m+1) \\cdot k - 1 \\ge n$) and found all of them to be less than $x$, it proceeds to linearly scan the final segment of the array, starting from index $m \\cdot k$.\n    The linear scan continues until the element $x$ is found or the block is fully scanned. For efficiency, any element that was already inspected during the Skip Phase is not inspected again during the Scan Phase.\n\nYour task is to determine the maximum possible number of comparisons the algorithm performs in the worst-case scenario. A single \"comparison\" consists of accessing one array element and comparing its value to $x$. Express your answer as a closed-form analytical expression in terms of $n$ and $k$.",
            "solution": "Let the array length be $n$ and the step size be $k$ with $1kn$. The skip indices inspected in the Skip Phase are\n$$\np_{j}=j k-1,\\quad j=1,2,\\ldots\n$$\nas long as $p_{j}n$. The largest $j$ with $p_{j}n$ satisfies $j k\\le n$, hence\n$$\nm=\\left\\lfloor \\frac{n}{k}\\right\\rfloor\n$$\nis the total number of skip points available, occurring at indices $p_{1},\\ldots,p_{m}$.\n\nCount comparisons in each phase:\n\n1) Skip Phase. If the Skip Phase stops at the first $j$ such that $A[p_{j}]\\ge x$, then the number of Skip Phase comparisons is $j$. If it never finds such a $j$ in bounds (i.e., all $m$ skip points satisfy $A[p_{j}]x$), then the number of Skip Phase comparisons is $m$.\n\n2) Scan Phase. There are two cases determined by how the Skip Phase stops.\n\na) Early stop at $A[p_{j}]\\ge x$ with some $1\\le j\\le m$. Then the Scan Phase linearly scans the block from index $(j-1)k$ up to $p_{j}-1=j k-2$, not rechecking $p_{j}$. The number of comparisons in this scan is the block length\n$$\n\\bigl(j k-2\\bigr)-\\bigl((j-1)k\\bigr)+1=(k-1).\n$$\nThus the total comparisons for a stop at $j$ are\n$$\nT_{1}(j)=j+(k-1).\n$$\nSince $T_{1}(j)$ increases with $j$, the worst case within this scenario is attained at $j=m$, yielding\n$$\nT_{1,\\max}=m+(k-1).\n$$\n\nb) Run-out (all skip points are less than $x$). Then the Scan Phase starts at index $m k$ and scans to $n-1$, for a segment of length\n$$\nL=n-m k.\n$$\nThe total comparisons are\n$$\nT_{2}=m+L=m+(n-m k)=n-m(k-1).\n$$\n\nTake the maximum over the two scenarios. Compare\n$$\nT_{1,\\max}-T_{2}=\\bigl(m+(k-1)\\bigr)-\\bigl(n-m(k-1)\\bigr)=k(m+1)-1-n.\n$$\nUsing $m=\\left\\lfloor \\frac{n}{k}\\right\\rfloor$, we have $m k\\le n\\le (m+1)k-1$, hence $k(m+1)-1-n\\ge 0$. Therefore\n$$\nT_{1,\\max}\\ge T_{2},\n$$\nwith equality only when $n=(m+1)k-1$.\n\nConsequently, the maximum possible number of comparisons in the worst case is\n$$\nm+(k-1)=\\left\\lfloor \\frac{n}{k}\\right\\rfloor + (k-1).\n$$",
            "answer": "$$\\boxed{\\left\\lfloor \\frac{n}{k}\\right\\rfloor + k - 1}$$"
        },
        {
            "introduction": "Recursive algorithms, which solve problems by breaking them into smaller instances of the same problem, are analyzed using recurrence relations. This final practice problem  explores an algorithm with a non-standard recursive structure, where the subproblems are of unequal sizes. Solving its recurrence relation will demonstrate how to determine the overall complexity when the work done at each step (the non-recursive part) dominates the work done in the recursive calls, a key concept in the analysis of divide-and-conquer strategies.",
            "id": "1469561",
            "problem": "An experimental algorithm, designed for a specialized form of hierarchical data analysis, operates on an input array of size $n$. For simplicity, assume $n$ is always a power of 4. The algorithm's procedure is defined recursively as follows:\n\n1.  If the input size $n$ is 1, the algorithm terminates, taking a constant amount of time.\n2.  If $n  1$, the algorithm first performs a global consolidation operation on the entire array. The time required for this operation is proportional to the square of the array's size, given by $c n^2$ for some positive constant $c$.\n3.  After the consolidation, the algorithm recursively calls itself on two distinct, non-overlapping subproblems:\n    a. The first recursive call processes a sub-array of size $n/2$.\n    b. The second recursive call processes a different sub-array of size $n/4$.\n\nThe total time complexity $T(n)$ is the sum of the time for the consolidation step and the time for the two recursive calls.\n\nWhich of the following represents the tightest asymptotic upper and lower bound (Big-Theta notation) for the worst-case time complexity $T(n)$ of this algorithm?\n\nA. $\\Theta(n \\log n)$\n\nB. $\\Theta(n^{\\log_{2}(\\phi)})$, where $\\phi = \\frac{1+\\sqrt{5}}{2}$ is the golden ratio.\n\nC. $\\Theta(n^2)$\n\nD. $\\Theta(n^2 \\log n)$\n\nE. $\\Theta(n^3)$",
            "solution": "We formalize the recurrence from the description. Let $T(1)=\\Theta(1)$, and for $n1$,\n$$\nT(n)=T\\!\\left(\\frac{n}{2}\\right)+T\\!\\left(\\frac{n}{4}\\right)+c n^{2},\n$$\nwhere $c0$ is a constant. We analyze this using the Akra–Bazzi theorem. Write the recurrence in the form $T(x)=\\sum_{i=1}^{k} a_{i} T(b_{i} x)+g(x)$ with $a_{1}=1$, $a_{2}=1$, $b_{1}=\\frac{1}{2}$, $b_{2}=\\frac{1}{4}$, and $g(x)=c x^{2}$. Solve for $p$ from\n$$\n\\sum_{i=1}^{2} a_{i} b_{i}^{p}=1 \\quad \\Longleftrightarrow \\quad \\left(\\frac{1}{2}\\right)^{p}+\\left(\\frac{1}{4}\\right)^{p}=1.\n$$\nLet $y=2^{-p}$. Then $y+y^{2}=1$, i.e.,\n$$\ny^{2}+y-1=0.\n$$\nThe positive root is\n$$\ny=\\frac{-1+\\sqrt{1+4}}{2}=\\frac{-1+\\sqrt{5}}{2}=\\frac{1}{\\phi},\n$$\nwhere $\\phi=\\frac{1+\\sqrt{5}}{2}$ is the golden ratio. Hence $2^{-p}=\\frac{1}{\\phi}$, which implies\n$$\np=\\log_{2}(\\phi).\n$$\nSince $1\\phi2$, it follows that $0p1$.\n\nBy the Akra–Bazzi theorem,\n$$\nT(x)=\\Theta\\!\\left(x^{p}\\left(1+\\int_{1}^{x} \\frac{g(u)}{u^{p+1}}\\,du\\right)\\right).\n$$\nWith $g(u)=c u^{2}$, the integrand is\n$$\n\\frac{g(u)}{u^{p+1}}=c\\,u^{2-(p+1)}=c\\,u^{1-p}.\n$$\nBecause $1-p0$, we compute\n$$\n\\int_{1}^{x} c\\,u^{1-p}\\,du=c\\,\\left[\\frac{u^{2-p}}{2-p}\\right]_{1}^{x}=\\frac{c}{2-p}\\left(x^{2-p}-1\\right).\n$$\nTherefore,\n$$\nT(x)=\\Theta\\!\\left(x^{p}\\left(1+\\frac{c}{2-p}\\left(x^{2-p}-1\\right)\\right)\\right)=\\Theta\\!\\left(x^{p}\\cdot x^{2-p}\\right)=\\Theta\\!\\left(x^{2}\\right).\n$$\nRewriting in terms of $n$, we obtain $T(n)=\\Theta(n^{2})$, which corresponds to option C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}