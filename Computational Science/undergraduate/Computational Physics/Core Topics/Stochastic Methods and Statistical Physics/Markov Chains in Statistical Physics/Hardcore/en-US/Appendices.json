{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on journey, we first build the essential mathematical toolkit for describing systems that evolve randomly in time. This exercise uses the intuitive example of daily weather patterns to construct a discrete-time Markov chain from scratch . By estimating a transition matrix from data and calculating future state probabilities, you will gain a practical understanding of key concepts like the stationary distribution and the principle of detailed balance, which are foundational to statistical physics.",
            "id": "2411650",
            "problem": "You will build a complete computational model of a discrete-time, finite-state, time-homogeneous Markov chain for a simplified weather system with three states: sunny, cloudy, and rainy. The model must be derived and implemented from first principles. The state space is $\\{\\text{sunny},\\text{cloudy},\\text{rainy}\\}$, and the states are ordered as $\\text{sunny} \\rightarrow 0$, $\\text{cloudy} \\rightarrow 1$, $\\text{rainy} \\rightarrow 2$. You will be given historical transition counts between these states, which you must convert into a stochastic transition matrix. Then, you will compute predictions of future weather distributions and assess reversibility via the detailed balance condition.\n\nFundamental base to use:\n- A discrete-time Markov chain on a finite state space is defined by the Markov property: for states $X_t \\in \\{0,1,2\\}$ and times $t \\in \\mathbb{Z}_{\\ge 0}$, one has $\\mathbb{P}(X_{t+1}=j \\mid X_t=i, X_{t-1}=i_{t-1},\\dots) = \\mathbb{P}(X_{t+1}=j \\mid X_t=i)$ for all states $i,j$.\n- A transition matrix $P$ has entries $P_{ij} = \\mathbb{P}(X_{t+1}=j \\mid X_t=i)$, with each row stochastic: $\\sum_{j} P_{ij} = 1$ and $P_{ij} \\ge 0$.\n- Given transition counts $N_{ij}$ from state $i$ to $j$, the maximum likelihood estimator subject to row-stochastic constraints uses normalized frequencies. To robustly handle zero counts in a principled way, you may also employ a symmetric Dirichlet prior with concentration parameter $\\alpha \\ge 0$ (Dirichlet-multinomial modeling) to regularize the estimate, interpreting $\\alpha$ as pseudocounts added to each possible transition in a row.\n- The $n$-step prediction from an initial distribution $\\boldsymbol{\\pi}_0$ is $\\boldsymbol{\\pi}_n = \\boldsymbol{\\pi}_0 P^n$.\n- A stationary distribution $\\boldsymbol{\\pi}^\\star$ satisfies $\\boldsymbol{\\pi}^\\star = \\boldsymbol{\\pi}^\\star P$ with $\\sum_i \\pi^\\star_i = 1$ and $\\pi^\\star_i \\ge 0$.\n- The detailed balance (reversibility) condition requires $\\pi^\\star_i P_{ij} = \\pi^\\star_j P_{ji}$ for all states $i,j$.\n\nYour program must:\n1. Estimate the transition probability matrix $\\widehat{P}$ from the given transition counts $N_{ij}$ and the specified symmetric Dirichlet prior concentration $\\alpha \\ge 0$. Use the principled construction that enforces that rows are stochastic and that increasing $\\alpha$ biases the estimate towards a uniform distribution over next states within each row.\n2. Given an initial distribution $\\boldsymbol{\\pi}_0$ and a positive integer horizon $n$, compute the $n$-step predicted distribution $\\boldsymbol{\\pi}_n = \\boldsymbol{\\pi}_0 \\widehat{P}^n$.\n3. Compute a stationary distribution $\\boldsymbol{\\pi}^\\star$ of $\\widehat{P}$ that satisfies $\\boldsymbol{\\pi}^\\star = \\boldsymbol{\\pi}^\\star \\widehat{P}$, $\\sum_i \\pi^\\star_i = 1$, and $\\pi^\\star_i \\ge 0$.\n4. Check the detailed balance equalities $\\pi^\\star_i \\widehat{P}_{ij} = \\pi^\\star_j \\widehat{P}_{ji}$ for all $i,j$ within a given absolute tolerance $\\varepsilon > 0$. Return a boolean that is true if and only if $\\max_{i,j} \\left|\\pi^\\star_i \\widehat{P}_{ij} - \\pi^\\star_j \\widehat{P}_{ji}\\right| \\le \\varepsilon$.\n\nFor each test case, your program must output a list containing:\n- The predicted probability of rainy weather after $n$ steps, i.e., $(\\boldsymbol{\\pi}_n)_2$, rounded to six decimal places.\n- The boolean result of the detailed balance check with the provided tolerance $\\varepsilon$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of the per-test-case results, with no spaces, enclosed in square brackets. For example: \"[[0.123456,False],[0.500000,True]]\".\n\nTest suite:\nUse the following four test cases. In each case, the weather states are ordered as $(\\text{sunny},\\text{cloudy},\\text{rainy})$.\n- Case $1$ (general mixed chain):\n  - Transition counts matrix $N^{(1)}$:\n    $$\n    N^{(1)}=\\begin{bmatrix}\n    30 & 15 & 5 \\\\\n    10 & 25 & 15 \\\\\n    5 & 15 & 20\n    \\end{bmatrix}\n    $$\n  - Prior concentration $\\alpha^{(1)} = 0$.\n  - Initial distribution $\\boldsymbol{\\pi}_0^{(1)} = [0.6,\\,0.3,\\,0.1]$.\n  - Horizon $n^{(1)}=2$.\n  - Tolerance $\\varepsilon^{(1)} = 10^{-12}$.\n- Case $2$ (deterministic $3$-cycle, periodic):\n  - Transition counts matrix $N^{(2)}$:\n    $$\n    N^{(2)}=\\begin{bmatrix}\n    0 & 10 & 0 \\\\\n    0 & 0 & 10 \\\\\n    10 & 0 & 0\n    \\end{bmatrix}\n    $$\n  - Prior concentration $\\alpha^{(2)} = 0$.\n  - Initial distribution $\\boldsymbol{\\pi}_0^{(2)} = [1.0,\\,0.0,\\,0.0]$.\n  - Horizon $n^{(2)}=2$.\n  - Tolerance $\\varepsilon^{(2)} = 10^{-12}$.\n- Case $3$ (absorbing rainy state):\n  - Transition counts matrix $N^{(3)}$:\n    $$\n    N^{(3)}=\\begin{bmatrix}\n    0 & 0 & 5 \\\\\n    0 & 0 & 7 \\\\\n    0 & 0 & 40\n    \\end{bmatrix}\n    $$\n  - Prior concentration $\\alpha^{(3)} = 0$.\n  - Initial distribution $\\boldsymbol{\\pi}_0^{(3)} = [0.0,\\,1.0,\\,0.0]$.\n  - Horizon $n^{(3)}=3$.\n  - Tolerance $\\varepsilon^{(3)} = 10^{-12}$.\n- Case $4$ (symmetric counts, reversible):\n  - Transition counts matrix $N^{(4)}$:\n    $$\n    N^{(4)}=\\begin{bmatrix}\n    10 & 2 & 3 \\\\\n    2 & 8 & 4 \\\\\n    3 & 4 & 12\n    \\end{bmatrix}\n    $$\n  - Prior concentration $\\alpha^{(4)} = 0$.\n  - Initial distribution $\\boldsymbol{\\pi}_0^{(4)} = [0.2,\\,0.5,\\,0.3]$.\n  - Horizon $n^{(4)}=5$.\n  - Tolerance $\\varepsilon^{(4)} = 10^{-12}$.\n\nFinal output format:\n- The program must print exactly one line: a single string encoding a list of four inner lists, one per test case, in the exact format \"[[x1,b1],[x2,b2],[x3,b3],[x4,b4]]\" where each $xk$ is the rainy probability rounded to six decimal places using fixed-point notation and each $bk$ is either True or False. There must be no spaces anywhere in this string.",
            "solution": "The problem as stated is a standard exercise in the theory and application of finite-state, discrete-time, time-homogeneous Markov chains. It is scientifically grounded, well-posed, and objective. We shall proceed with a formal derivation of the required computational procedures. The state space is given as $\\{0, 1, 2\\}$, corresponding to $\\{\\text{sunny}, \\text{cloudy}, \\text{rainy}\\}$.\n\nFirst, we address the estimation of the transition probability matrix $\\widehat{P}$ from empirical transition counts $N_{ij}$. The problem specifies the use of a symmetric Dirichlet prior with concentration parameter $\\alpha$ to regularize the maximum likelihood estimate. For a system with $K$ states, the posterior predictive probability of transitioning from state $i$ to state $j$ is given by the expectation of the posterior distribution, which is a Dirichlet distribution. This yields the formula:\n$$\n\\widehat{P}_{ij} = \\frac{N_{ij} + \\alpha}{\\sum_{k=0}^{K-1} (N_{ik} + \\alpha)} = \\frac{N_{ij} + \\alpha}{N_i + K\\alpha}\n$$\nwhere $N_i = \\sum_{k=0}^{K-1} N_{ik}$ is the total number of observed transitions originating from state $i$. In our case, the number of states is $K=3$. For the special case where $\\alpha=0$, this formula reduces to the standard maximum likelihood estimator, $\\widehat{P}_{ij} = N_{ij} / N_i$, provided $N_i > 0$. All test cases provided have $N_i > 0$ for all $i$.\n\nSecond, we compute the predicted state distribution $\\boldsymbol{\\pi}_n$ at a future time step $n$. Given an initial distribution vector $\\boldsymbol{\\pi}_0$ and the time-homogeneous transition matrix $\\widehat{P}$, the distribution at step $t+1$ is $\\boldsymbol{\\pi}_{t+1} = \\boldsymbol{\\pi}_t \\widehat{P}$. By recursion, the distribution after $n$ steps is obtained by matrix exponentiation:\n$$\n\\boldsymbol{\\pi}_n = \\boldsymbol{\\pi}_0 \\widehat{P}^n\n$$\nThe specific quantity requested is the probability of the rainy state, which is the third component of this vector, denoted as $(\\boldsymbol{\\pi}_n)_2$.\n\nThird, we must compute a stationary distribution $\\boldsymbol{\\pi}^\\star$ of the Markov chain. A stationary distribution is a probability vector that remains unchanged after application of the transition matrix. It is a left eigenvector of $\\widehat{P}$ corresponding to an eigenvalue of $\\lambda=1$:\n$$\n\\boldsymbol{\\pi}^\\star \\widehat{P} = \\boldsymbol{\\pi}^\\star\n$$\nThis is equivalent to solving the linear system $\\boldsymbol{\\pi}^\\star (\\widehat{P} - I) = \\mathbf{0}$, where $I$ is the identity matrix and $\\mathbf{0}$ is the zero vector, subject to the normalization constraint $\\sum_i \\pi^\\star_i = 1$ and non-negativity $\\pi^\\star_i \\ge 0$. A robust numerical method is to find the eigenvectors of the transpose of the transition matrix, $\\widehat{P}^T$. The stationary distribution corresponds to the right eigenvector of $\\widehat{P}^T$ associated with the eigenvalue $\\lambda=1$. For any stochastic matrix, such an eigenvalue is guaranteed to exist. We find the eigenvector $\\mathbf{v}$ corresponding to the eigenvalue closest to $1$, take its real part (as complex artifacts can arise from numerical computation), and normalize it to sum to unity: $\\boldsymbol{\\pi}^\\star = \\mathbf{v} / (\\sum_i v_i)$.\n\nFourth, we must assess whether the chain is reversible by checking the detailed balance condition with respect to the computed stationary distribution $\\boldsymbol{\\pi}^\\star$. A Markov chain is reversible if and only if there exists a distribution $\\boldsymbol{\\pi}$ such that for all states $i$ and $j$:\n$$\n\\pi_i \\widehat{P}_{ij} = \\pi_j \\widehat{P}_{ji}\n$$\nIf such a distribution exists, it is a stationary distribution. We check this condition using the computed $\\boldsymbol{\\pi}^\\star$. Numerically, we verify if the maximum absolute difference between the two sides of the equation is within a specified tolerance $\\varepsilon > 0$:\n$$\n\\max_{i,j \\in \\{0,1,2\\}} \\left| \\pi^\\star_i \\widehat{P}_{ij} - \\pi^\\star_j \\widehat{P}_{ji} \\right| \\le \\varepsilon\n$$\nThe result is a boolean value indicating whether this condition holds.\n\nThese four procedures form a complete algorithm to solve the problem for each test case. The implementation requires standard numerical linear algebra operations, which are provided below.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to execute all test cases for the Markov chain weather model.\n    \"\"\"\n\n    def solve_case(N_counts, alpha, pi_0, n, epsilon):\n        \"\"\"\n        Solves a single instance of the Markov chain problem.\n\n        Args:\n            N_counts (list-of-lists): The transition counts matrix N.\n            alpha (float): The concentration parameter for the Dirichlet prior.\n            pi_0 (np.ndarray): The initial state distribution.\n            n (int): The number of steps for prediction.\n            epsilon (float): The tolerance for the detailed balance check.\n\n        Returns:\n            list: A list containing the predicted rainy probability (str) and\n                  the detailed balance check result (bool).\n        \"\"\"\n        N = np.array(N_counts, dtype=float)\n        num_states = N.shape[0]\n\n        # 1. Estimate the transition probability matrix P_hat\n        row_sums = N.sum(axis=1, keepdims=True)\n        denominator = row_sums + num_states * alpha\n        \n        # Handle cases where a row sum is zero and alpha is zero\n        P = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n\n        # 2. Compute the n-step predicted distribution pi_n\n        if n > 0:\n            P_n = np.linalg.matrix_power(P, n)\n            pi_n = pi_0 @ P_n\n        else: # n=0 case\n            pi_n = pi_0\n        \n        rainy_prob = pi_n[2]\n\n        # 3. Compute the stationary distribution pi_star\n        eigenvalues, eigenvectors = np.linalg.eig(P.T)\n        \n        # Find the index of the eigenvalue closest to 1.0\n        idx = np.argmin(np.abs(eigenvalues - 1.0))\n        \n        # The corresponding eigenvector is the unnormalized stationary distribution\n        pi_star_unnormalized = eigenvectors[:, idx]\n        \n        # Take the real part and normalize\n        pi_star = np.real(pi_star_unnormalized)\n        pi_star = pi_star / np.sum(pi_star)\n\n        # 4. Check the detailed balance condition for reversibility\n        db_matrix = np.diag(pi_star) @ P\n        max_abs_diff = np.max(np.abs(db_matrix - db_matrix.T))\n        is_reversible = max_abs_diff = epsilon\n        \n        return [f\"{rainy_prob:.6f}\", is_reversible]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General mixed chain\n        (\n            [[30, 15, 5], [10, 25, 15], [5, 15, 20]],\n            0.0,\n            np.array([0.6, 0.3, 0.1]),\n            2,\n            1e-12\n        ),\n        # Case 2: Deterministic 3-cycle, periodic\n        (\n            [[0, 10, 0], [0, 0, 10], [10, 0, 0]],\n            0.0,\n            np.array([1.0, 0.0, 0.0]),\n            2,\n            1e-12\n        ),\n        # Case 3: Absorbing rainy state\n        (\n            [[0, 0, 5], [0, 0, 7], [0, 0, 40]],\n            0.0,\n            np.array([0.0, 1.0, 0.0]),\n            3,\n            1e-12\n        ),\n        # Case 4: Symmetric counts, reversible\n        (\n            [[10, 2, 3], [2, 8, 4], [3, 4, 12]],\n            0.0,\n            np.array([0.2, 0.5, 0.3]),\n            5,\n            1e-12\n        ),\n    ]\n\n    # Process all test cases and collect results.\n    results = []\n    for case in test_cases:\n        N_counts, alpha, pi_0, n, epsilon = case\n        result = solve_case(N_counts, alpha, pi_0, n, epsilon)\n        results.append(f\"[{result[0]},{str(result[1]).lower()}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having mastered the abstract mechanics of Markov chains, we now apply them to a classic problem in physics: the random walk. This practice explores how the simple, memoryless steps of a random walker give rise to the macroscopic phenomenon of diffusion . You will derive the diffusion coefficient from first principles and use simulations to test a fascinating hypothesis: whether the diffusion rate in a simple system depends on the microscopic geometry of the lattice.",
            "id": "2445720",
            "problem": "You will study unbiased discrete-time nearest-neighbor random walks on two infinite two-dimensional lattices: the square lattice and the honeycomb lattice. Each walker starts at the origin at time step $0$, takes steps of fixed length $a$ (in meters) at fixed time interval $\\tau$ (in seconds), and at each step chooses uniformly among the nearest neighbors allowed by the lattice geometry. The honeycomb lattice is modeled with three bond directions at mutual angles of $120^\\circ$; the square lattice is modeled with four cardinal directions. Your objective is to quantify and compare diffusion on these two lattices from first principles and to confirm or refute any lattice-dependent difference in the long-time diffusion coefficient. All numerical results must be expressed in $\\mathrm{m}^2/\\mathrm{s}$.\n\nRequired derivation and implementation tasks:\n- Starting from the definition of the random walk displacement $\\mathbf{r}_n = \\sum_{k=1}^{n} \\mathbf{s}_k$ after $n$ steps and the defining relation for diffusion in two spatial dimensions, derive an estimator for the diffusion coefficient $D$ in terms of the mean squared displacement $\\langle r_n^2 \\rangle$ accumulated over $n$ steps during a total physical time $t = n \\tau$. Your derivation must rely only on the core properties that the increments are independent given the current state, have zero mean by lattice symmetry at each step, and have fixed length $a$. Do not assume any lattice-specific shortcut formulas.\n- Implement two independent simulators:\n  1. Square lattice: at each step choose uniformly among the $4$ unit vectors $(\\pm 1, 0)$, $(0, \\pm 1)$ scaled by $a$.\n  2. Honeycomb lattice: use the three bond vectors of length $a$, $\\mathbf{b}_1 = a (1, 0)$, $\\mathbf{b}_2 = a (-\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2})$, $\\mathbf{b}_3 = a (-\\tfrac{1}{2}, -\\tfrac{\\sqrt{3}}{2})$, such that from one sublattice the outgoing choices are $\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}$ and from the other sublattice they are $\\{-\\mathbf{b}_1, -\\mathbf{b}_2, -\\mathbf{b}_3\\}$. Start at the origin on one sublattice and alternate sublattices with each step.\n- For each test case below, simulate $M$ independent walkers for $n$ steps on each lattice, compute the mean squared displacement $\\langle r_n^2 \\rangle$ over the $M$ walkers for each lattice separately, and convert it into an estimate $\\hat{D}$ of the diffusion coefficient using your derived estimator. Use the same $M$, $n$, $a$, and $\\tau$ for both lattices in a given test case. If a random seed is provided, use it to produce reproducible results.\n\nPhysical units and output requirements:\n- Report every diffusion coefficient in $\\mathrm{m}^2/\\mathrm{s}$ as a floating-point number. Do not print unit strings; only print the numeric values.\n- Angles, where relevant in the honeycomb geometry, must be understood in radians internally; the algorithm as specified above already encodes the directions, so you do not need to handle angles explicitly in the program output.\n\nTest suite (each tuple is $(n, a, \\tau, M, \\text{seed})$ and all quantities are to be interpreted in the International System of Units):\n- Case A (happy path): $(20000, 1.0 \\times 10^{-9}, 1.0 \\times 10^{-12}, 200, 12345)$.\n- Case B (boundary condition, single step): $(1, 5.0 \\times 10^{-10}, 2.0 \\times 10^{-13}, 1000, 54321)$.\n- Case C (short walk, many trials): $(2000, 2.0 \\times 10^{-10}, 1.0 \\times 10^{-13}, 500, 202311)$.\n- Case D (different scales): $(5000, 3.0 \\times 10^{-10}, 5.0 \\times 10^{-13}, 300, 777)$.\n\nFinal output format:\n- Your program must produce a single line of output containing a list of results, one per test case, in the order A, B, C, D. For each test case, output a list of three floating-point numbers $[\\hat{D}_{\\text{square}}, \\hat{D}_{\\text{honeycomb}}, D_{\\text{theory}}]$, where $D_{\\text{theory}}$ is the diffusion coefficient predicted by your derivation under the stated assumptions. The full output must therefore be a list of lists, for example, $[[x_1, y_1, z_1], [x_2, y_2, z_2], [x_3, y_3, z_3], [x_4, y_4, z_4]]$, with all values in $\\mathrm{m}^2/\\mathrm{s}$. Only this single-line list must be printed.",
            "solution": "The problem requires a first-principles derivation of an estimator for the diffusion coefficient, $D$, for a two-dimensional discrete random walk, and its subsequent numerical verification on square and honeycomb lattices. The validation of the problem statement finds it to be scientifically sound, well-posed, and complete. We proceed with the solution.\n\nThe primary task is to establish a relationship between the microscopic parameters of the random walk and the macroscopic diffusion coefficient. The position of a walker after $n$ steps, $\\mathbf{r}_n$, is the sum of individual step vectors $\\mathbf{s}_k$:\n$$ \\mathbf{r}_n = \\sum_{k=1}^{n} \\mathbf{s}_k $$\nThe quantity of interest is the mean squared displacement (MSD), $\\langle r_n^2 \\rangle$, where the average $\\langle \\cdot \\rangle$ is taken over an ensemble of independent walkers. The squared displacement is the dot product of the position vector with itself:\n$$ r_n^2 = \\mathbf{r}_n \\cdot \\mathbf{r}_n = \\left( \\sum_{i=1}^{n} \\mathbf{s}_i \\right) \\cdot \\left( \\sum_{j=1}^{n} \\mathbf{s}_j \\right) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\mathbf{s}_i \\cdot \\mathbf{s}_j $$\nBy linearity of the expectation operator, the MSD is:\n$$ \\langle r_n^2 \\rangle = \\left\\langle \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\mathbf{s}_i \\cdot \\mathbf{s}_j \\right\\rangle = \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle $$\nWe decompose this double summation into terms where the indices are equal ($i=j$) and terms where they are not ($i \\neq j$):\n$$ \\langle r_n^2 \\rangle = \\sum_{i=1}^{n} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_i \\rangle + \\sum_{i \\neq j} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle $$\nThe problem states that the step increments are independent. Therefore, for $i \\neq j$, the expectation of the product is the product of the expectations: $\\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle = \\langle \\mathbf{s}_i \\rangle \\cdot \\langle \\mathbfs_j \\rangle$. The problem also specifies that the walk is unbiased, meaning the choice of step direction is from a symmetric set, which results in a zero mean for any step vector: $\\langle \\mathbf{s}_k \\rangle = \\mathbf{0}$ for all $k$. This holds true for both the square and honeycomb lattices as defined. Consequently, all cross-terms ($i \\neq j$) vanish:\n$$ \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_j \\rangle = \\mathbf{0} \\cdot \\mathbf{0} = 0 \\quad \\text{for } i \\neq j $$\nThe MSD expression simplifies to the sum of the diagonal terms:\n$$ \\langle r_n^2 \\rangle = \\sum_{i=1}^{n} \\langle \\mathbf{s}_i \\cdot \\mathbf{s}_i \\rangle = \\sum_{i=1}^{n} \\langle |\\mathbf{s}_i|^2 \\rangle $$\nThe step length is given as a fixed constant, $|\\mathbf{s}_k| = a$. Thus, its square is also constant, $|\\mathbf{s}_k|^2 = a^2$, and its expectation is simply $a^2$. The sum becomes:\n$$ \\langle r_n^2 \\rangle = n a^2 $$\nThis is the theoretical MSD for an unbiased random walk with fixed step length after $n$ steps.\nIn the continuous limit, diffusion in two dimensions is described by the Einstein relation, which connects the MSD to the diffusion coefficient $D$ and time $t$:\n$$ \\langle r^2(t) \\rangle = 4 D t $$\nWe relate our discrete model to this continuous description by setting the total time $t$ to be the number of steps $n$ multiplied by the time per step $\\tau$, i.e., $t = n \\tau$. Equating the two expressions for MSD gives:\n$$ n a^2 = 4 D (n \\tau) $$\nSolving for $D$ yields the theoretical diffusion coefficient, which we denote as $D_{\\text{theory}}$:\n$$ D_{\\text{theory}} = \\frac{a^2}{4 \\tau} $$\nThis theoretical result is general for any lattice that satisfies the initial assumptions and is, notably, independent of the lattice coordination number or geometry. This derivation refutes the idea of a lattice-dependent diffusion coefficient under these model conditions.\n\nFor the numerical part of the problem, we must estimate $D$ from simulation data. A simulation of $M$ walkers for $n$ steps produces a set of final squared displacements $\\{r_{n,1}^2, r_{n,2}^2, \\dots, r_{n,M}^2\\}$. The simulated MSD, $\\langle r_n^2 \\rangle_{\\text{sim}}$, is the sample mean of these values. We use this empirical result in the diffusion relation to find our estimator, $\\hat{D}$:\n$$ \\langle r_n^2 \\rangle_{\\text{sim}} = 4 \\hat{D} (n \\tau) \\implies \\hat{D} = \\frac{\\langle r_n^2 \\rangle_{\\text{sim}}}{4 n \\tau} $$\nThis is the estimator to be implemented.\n\nThe implementation will consist of two independent simulators. Both will track a population of $M$ walkers, represented by a NumPy array of shape $(M, 2)$, initialized to zeros. For each of the $n$ time steps, a random step vector is chosen for each walker and added to its current position. This process is vectorized for efficiency.\n\nFor the square lattice, the set of possible step vectors is $\\{\\,(a, 0), (-a, 0), (0, a), (0, -a)\\,\\}$. At each step, one of these $4$ vectors is chosen with uniform probability $p=1/4$.\n\nFor the honeycomb lattice, the structure is bipartite. The set of step vectors depends on the sublattice the walker currently occupies. Let the sublattices be $A$ and $B$. A walker starting on $A$ must step to $B$, and from $B$ must step to $A$. We define the basis bond vectors as $\\mathbf{b}_1 = a(1, 0)$, $\\mathbf{b}_2 = a(-\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2})$, and $\\mathbf{b}_3 = a(-\\tfrac{1}{2}, -\\tfrac{\\sqrt{3}}{2})$.\n- From sublattice $A$ (e.g., at even-numbered steps $0, 2, \\dots$), the allowed steps are $\\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}$, each with probability $p=1/3$.\n- From sublattice $B$ (e.g., at odd-numbered steps $1, 3, \\dots$), the allowed steps are $\\{-\\mathbf{b}_1, -\\mathbf{b}_2, -\\mathbf{b}_3\\}$, each with probability $p=1/3$.\n\nAfter $n$ steps, the final squared displacement for each walker is computed, the mean is taken over all $M$ walkers to get $\\langle r_n^2 \\rangle_{\\text{sim}}$, and this is used to calculate $\\hat{D}_{\\text{square}}$ and $\\hat{D}_{\\text{honeycomb}}$. These simulated values will be compared against the derived $D_{\\text{theory}} = a^2 / (4\\tau)$. Due to statistical fluctuations inherent in a finite simulation ($M  \\infty$), the estimated values $\\hat{D}$ will deviate slightly from $D_{\\text{theory}}$, but should converge to it as $M$ and $n$ become large.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_square(n, a, tau, M, seed):\n    \"\"\"\n    Simulates a random walk on a 2D square lattice.\n\n    Args:\n        n (int): Number of steps.\n        a (float): Step length in meters.\n        tau (float): Time interval per step in seconds.\n        M (int): Number of independent walkers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: Estimated diffusion coefficient in m^2/s.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Initialize all M walkers at the origin (0, 0).\n    positions = np.zeros((M, 2), dtype=np.float64)\n    # Define the four possible step vectors.\n    steps_set = a * np.array([[1.0, 0.0], [-1.0, 0.0], [0.0, 1.0], [0.0, -1.0]])\n\n    for _ in range(n):\n        # For each of the M walkers, choose one of the 4 steps randomly.\n        choices = rng.integers(0, 4, size=M)\n        # Get the corresponding displacement vectors for all walkers.\n        displacements = steps_set[choices]\n        # Update all positions simultaneously.\n        positions += displacements\n    \n    # For n=0, division by zero occurs. Problem cases have n>=1.\n    if n == 0:\n        return 0.0\n\n    # Calculate the squared displacement from the origin for each walker.\n    squared_displacements = np.sum(positions**2, axis=1)\n    # Compute the mean squared displacement over all walkers.\n    mean_squared_displacement = np.mean(squared_displacements)\n    \n    # Use the derived estimator for the diffusion coefficient D.\n    D_hat = mean_squared_displacement / (4.0 * n * tau)\n    return D_hat\n\ndef simulate_honeycomb(n, a, tau, M, seed):\n    \"\"\"\n    Simulates a random walk on a 2D honeycomb lattice.\n\n    Args:\n        n (int): Number of steps.\n        a (float): Step length in meters.\n        tau (float): Time interval per step in seconds.\n        M (int): Number of independent walkers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: Estimated diffusion coefficient in m^2/s.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Initialize all M walkers at the origin (0, 0) on sublattice A.\n    positions = np.zeros((M, 2), dtype=np.float64)\n    \n    sqrt3_div_2 = np.sqrt(3.0) / 2.0\n    # Define bond vectors for sublattice A -> B transitions.\n    steps_A = a * np.array([[1.0, 0.0], [-0.5, sqrt3_div_2], [-0.5, -sqrt3_div_2]])\n    # Bond vectors for sublattice B -> A are the negative of A -> B.\n    steps_B = -steps_A\n\n    for i in range(n):\n        # For each of the M walkers, choose one of the 3 steps randomly.\n        choices = rng.integers(0, 3, size=M)\n        # Select step set based on sublattice (even/odd step number).\n        if i % 2 == 0:  # Walker is on sublattice A\n            displacements = steps_A[choices]\n        else:  # Walker is on sublattice B\n            displacements = steps_B[choices]\n        # Update all positions simultaneously.\n        positions += displacements\n\n    if n == 0:\n        return 0.0\n\n    # Calculate and average the squared displacements.\n    squared_displacements = np.sum(positions**2, axis=1)\n    mean_squared_displacement = np.mean(squared_displacements)\n    \n    # Estimate D.\n    D_hat = mean_squared_displacement / (4.0 * n * tau)\n    return D_hat\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, a, tau, M, seed)\n        (20000, 1.0e-9, 1.0e-12, 200, 12345),  # Case A\n        (1, 5.0e-10, 2.0e-13, 1000, 54321),    # Case B\n        (2000, 2.0e-10, 1.0e-13, 500, 202311), # Case C\n        (5000, 3.0e-10, 5.0e-13, 300, 777),      # Case D\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n, a, tau, M, seed = case\n        \n        # Run simulation for the square lattice.\n        D_square = simulate_square(n, a, tau, M, seed)\n        \n        # Run simulation for the honeycomb lattice.\n        D_honeycomb = simulate_honeycomb(n, a, tau, M, seed)\n        \n        # Calculate the theoretical diffusion coefficient.\n        D_theory = (a**2) / (4.0 * tau)\n        \n        all_results.append([D_square, D_honeycomb, D_theory])\n\n    # Format the final output string to match the required format.\n    # The output is a string representation of a list of lists.\n    # e.g., [[val1,val2,val3],[val4,val5,val6]]\n    # Using str() on each sublist and then joining is a reliable way\n    # to produce the python literal representation.\n    result_strings = [str(res) for res in all_results]\n    \n    # The default str representation includes spaces. To match the example\n    # style [[x1,y1,z1],...] without spaces, we can build the string manually.\n    formatted_results = []\n    for res in all_results:\n        # Format each sublist as a string \"[v1,v2,v3]\"\n        formatted_results.append(f\"[{','.join(map(str, res))}]\")\n    \n    # Join all sublist strings and wrap in outer brackets.\n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "We now advance to one of the most important applications of Markov chains in statistical physics: simulating interacting many-body systems. This exercise challenges you to implement a Gibbs sampler, a powerful Markov Chain Monte Carlo algorithm, to explore the behavior of the 2D Ising model, a cornerstone for understanding phase transitions . By measuring observables like magnetization and the Binder cumulant, you will witness firsthand how simple microscopic interactions lead to complex, collective phenomena and learn a state-of-the-art technique for computational physics research.",
            "id": "2411722",
            "problem": "You will implement a Markov Chain Monte Carlo simulator using Gibbs sampling (also called the heat-bath algorithm) for a two-dimensional binary alloy modeled as an Ising-like lattice system. Consider a square lattice with periodic boundary conditions of linear size $L$, so the number of lattice sites is $N = L^2$. Each site $i$ carries a binary variable $s_i \\in \\{-1, +1\\}$, representing one of two atomic species. The energy of a configuration is given by the Ising Hamiltonian\n$$\n\\mathcal{H}(s) = - J \\sum_{\\langle i,j \\rangle} s_i s_j,\n$$\nwhere the sum is over nearest-neighbor pairs on the square lattice, and $J$ is a coupling constant. You will use Gibbs sampling to construct a Markov chain that is ergodic and satisfies detailed balance with respect to the Boltzmann distribution at inverse temperature $\\beta = 1/T$ with Boltzmann constant $k_B$ set to $1$ (that is, temperatures are expressed in units of $J/k_B$ with $k_B=1$). You must explicitly implement periodic boundary conditions and use a checkerboard (red-black) update scheme so that all sites of one sublattice are updated simultaneously using the current values of their neighbors.\n\nStarting from the fundamental definition that equilibrium probabilities are proportional to the Boltzmann weight $e^{-\\beta \\mathcal{H}(s)}$, derive the single-site conditional probability that underlies the Gibbs update used to resample a site $s_i$ given its neighbors at fixed temperature $T$ and coupling $J$. Use that conditional to implement a single-site Gibbs sampler that updates the entire lattice by alternating sublattice updates. Initialize spins randomly and allow a burn-in period before collecting measurements.\n\nFor each state of the Markov chain after burn-in, compute the following observables:\n- The instantaneous magnetization per site $m = \\frac{1}{N} \\sum_i s_i$ and its absolute value $|m|$.\n- The instantaneous energy per site $e = \\frac{1}{N} \\mathcal{H}(s)$, computed with each nearest-neighbor pair counted exactly once.\n- The Binder cumulant $U_4 = 1 - \\frac{\\langle m^4 \\rangle}{3 \\langle m^2 \\rangle^2}$, where angle brackets denote the time average over measurement samples of the Markov chain at fixed temperature and lattice size.\n\nDesign your simulator so that, for a fixed random number generator seed, it produces reproducible results. Your program must run the following test suite of parameter sets and report, for each test case, the time-averaged absolute magnetization $\\langle |m| \\rangle$, the time-averaged energy per site $\\langle e \\rangle$, and the Binder cumulant $U_4$. All temperatures are in units where $k_B=1$, so $T$ is expressed in units of $J$. Use periodic boundary conditions in both directions.\n\nTest suite (each tuple is $(L, J, T, n_{\\text{therm}}, n_{\\text{meas}})$):\n- Happy path, ordered phase: $(40, 1.0, 1.8, 800, 800)$.\n- Near the two-dimensional Ising critical point: $(40, 1.0, 2.269, 1500, 1500)$.\n- Disordered phase: $(40, 1.0, 3.5, 800, 800)$.\n- Boundary case (no interactions): $(40, 0.0, 1.0, 500, 1000)$.\n\nYour program must compute the three requested observables for each test case, average them over the $n_{\\text{meas}}$ measurement sweeps, and round each of the three outputs to $4$ decimal places for reporting. There are no physical units in the final output because all quantities are dimensionless in the chosen units with $k_B=1$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The result corresponding to each test case must itself be a bracketed, comma-separated list of the three rounded floats $[\\langle |m| \\rangle, \\langle e \\rangle, U_4]$, in the same order as the test suite above. For example, the output should look like\n$[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$\nwith each $x_i, y_i, z_i$ rounded to $4$ decimal places and no other text printed.",
            "solution": "The supplied problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **System Model**: A two-dimensional square lattice of linear size $L$, with $N = L^2$ sites. Each site $i$ has a spin variable $s_i \\in \\{-1, +1\\}$.\n- **Hamiltonian**: The energy of a configuration $s$ is given by $\\mathcal{H}(s) = -J \\sum_{\\langle i,j \\rangle} s_i s_j$, where the sum is over nearest-neighbor pairs.\n- **Thermodynamics**: The system is in equilibrium with a heat bath at inverse temperature $\\beta = 1/T$, with the Boltzmann constant $k_B$ set to $1$. Probabilities follow the Boltzmann distribution, $P(s) \\propto e^{-\\beta \\mathcal{H}(s)}$.\n- **Algorithm**: Markov Chain Monte Carlo simulation using Gibbs sampling (heat-bath algorithm).\n- **Update Scheme**: A checkerboard (red-black) update scheme is mandated, where all sites of one sublattice are updated simultaneously.\n- **Boundary Conditions**: Periodic boundary conditions in both lattice directions.\n- **Simulation Protocol**: Start from a random spin configuration, perform $n_{\\text{therm}}$ thermalization sweeps, followed by $n_{\\text{meas}}$ measurement sweeps.\n- **Observables to Compute**:\n    - Instantaneous magnetization per site: $m = \\frac{1}{N} \\sum_i s_i$.\n    - Instantaneous absolute magnetization per site: $|m|$.\n    - Instantaneous energy per site: $e = \\frac{1}{N} \\mathcal{H}(s)$.\n    - Binder cumulant: $U_4 = 1 - \\frac{\\langle m^4 \\rangle}{3 \\langle m^2 \\rangle^2}$, where $\\langle \\cdot \\rangle$ denotes the average over measurement samples.\n- **Reproducibility**: The simulation must be reproducible for a fixed random number generator seed.\n- **Test Suite**: A list of test cases, each defined by the tuple $(L, J, T, n_{\\text{therm}}, n_{\\text{meas}})$:\n    1. $(40, 1.0, 1.8, 800, 800)$\n    2. $(40, 1.0, 2.269, 1500, 1500)$\n    3. $(40, 1.0, 3.5, 800, 800)$\n    4. $(40, 0.0, 1.0, 500, 1000)$\n- **Output Format**: For each test case, report a list containing the time-averaged absolute magnetization $\\langle |m| \\rangle$, time-averaged energy per site $\\langle e \\rangle$, and the Binder cumulant $U_4$, with each value rounded to $4$ decimal places. The final output must be a single line: `[[...],[...],[...],[...]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is fundamentally sound. The Ising model, Gibbs sampling, and the specified observables are central topics in statistical and computational physics. The setup uses established principles and models.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary information: the physical model, the simulation algorithm, specific parameters for each run, and a clear definition of the required outputs. With a fixed random seed, the algorithm is deterministic and will produce a unique solution.\n- **Objectivity**: The problem statement is objective and uses precise, unambiguous scientific language.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard, well-defined problem in computational statistical mechanics.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution\nThe objective is to simulate a two-dimensional Ising model using a specific Markov Chain Monte Carlo method, namely Gibbs sampling, and to compute several key statistical observables.\n\n**1. Gibbs Sampling and Conditional Probability**\n\nThe Gibbs sampling algorithm, also known as the heat-bath algorithm, updates the state of a single component of the system, $s_i$, by drawing a new value from its conditional probability distribution, given the current state of all other components. For the Ising model, the state of a spin $s_i$ only depends on its immediate neighbors due to the nearest-neighbor nature of the Hamiltonian $\\mathcal{H}$.\n\nThe conditional probability of finding spin $i$ in state $s_i$, given the fixed configuration of its neighbors, is proportional to the Boltzmann factor associated with the energy contribution from that spin:\n$$ P(s_i | \\{s_j\\}_{j \\in \\text{nn}(i)}) \\propto e^{-\\beta E_i} $$\nwhere $E_i$ is the part of the total energy that depends on $s_i$. This energy is $E_i = -J s_i \\sum_{j \\in \\text{nn}(i)} s_j$. Let us define the local field $h_i = \\sum_{j \\in \\text{nn}(i)} s_j$, which is the sum of the four neighboring spins. Then, $E_i = -J s_i h_i$.\n\nThe conditional probability for $s_i$ to be in the state $+1$ is given by normalizing over the two possible states, $s_i = +1$ and $s_i = -1$:\n$$ P(s_i = +1 | h_i) = \\frac{e^{-\\beta (-J(+1)h_i)}}{e^{-\\beta (-J(+1)h_i)} + e^{-\\beta (-J(-1)h_i)}} = \\frac{e^{\\beta J h_i}}{e^{\\beta J h_i} + e^{-\\beta J h_i}} $$\nThis expression can be rewritten in a numerically stable form using the hyperbolic tangent or, more conveniently for implementation, as:\n$$ P(s_i = +1 | h_i) = \\frac{1}{1 + e^{-2\\beta J h_i}} $$\nThe probability for $s_i$ to be $-1$ is simply $P(s_i = -1 | h_i) = 1 - P(s_i = +1 | h_i)$.\nThe Gibbs update rule for a single spin $s_i$ is as follows:\n1. Calculate the sum of its neighbors, $h_i$.\n2. Calculate the probability $p_+ = P(s_i = +1 | h_i)$.\n3. Draw a uniform random number $r \\in [0, 1)$.\n4. If $r  p_+$, set $s_i = +1$; otherwise, set $s_i = -1$.\n\n**2. Checkerboard Update Scheme**\n\nTo parallelize the updates and ensure correctness, a checkerboard (or red-black) update scheme is employed. The lattice sites are partitioned into two sublattices, 'red' and 'black', like squares on a chessboard. A site $(i,j)$ is 'red' if $i+j$ is even and 'black' if $i+j$ is odd. The crucial property is that all neighbors of a red site are black, and vice-versa.\n\nA full sweep over the lattice consists of two steps:\n1.  **Update Red Sublattice**: Simultaneously update all spins on the red sublattice. For each red spin, its neighbors are all on the black sublattice. The update for each red spin depends only on the current state of the black spins, so these updates are independent of each other.\n2.  **Update Black Sublattice**: Subsequently, simultaneously update all spins on the black sublattice. The neighbors of these spins are on the red sublattice, so their update will use the newly computed values of the red spins from step $1$.\n\nThis scheme guarantees that the detailed balance condition is met for the sweep as a whole and allows for efficient, vectorized implementation.\n\n**3. Observables and Their Computation**\n\nAfter a thermalization period of $n_{\\text{therm}}$ sweeps to allow the system to reach equilibrium, measurements are taken over $n_{\\text{meas}}$ sweeps.\n\n- **Magnetization per site ($m$)**: For a given spin configuration, this is the average spin value: $m = \\frac{1}{N} \\sum_{i=1}^N s_i$. We will need its absolute value $|m|$, as well as $m^2$ and $m^4$ for the Binder cumulant.\n\n- **Energy per site ($e$)**: The total energy is $\\mathcal{H} = -J \\sum_{\\langle i,j \\rangle} s_i s_j$. To avoid double-counting pairs, we sum over each site's interaction with its neighbor to the \"right\" and \"down\" (with periodic boundaries).\n$$ \\mathcal{H} = -J \\sum_{i,j} s_{i,j} (s_{i,j+1} + s_{i+1,j}) $$\nwhere indices are taken modulo $L$. The energy per site is $e = \\mathcal{H} / N$.\n\n- **Binder Cumulant ($U_4$)**: This is a higher-order moment ratio used to locate phase transitions. It is defined as:\n$$ U_4 = 1 - \\frac{\\langle m^4 \\rangle}{3 \\langle m^2 \\rangle^2} $$\nwhere $\\langle m^2 \\rangle$ and $\\langle m^4 \\rangle$ are the means of the squared and fourth power of the instantaneous magnetization, averaged over the measurement sweeps. This quantity has the useful property of being approximately independent of system size at the critical temperature. It approaches $2/3$ in the ordered phase and $0$ in the disordered phase. For the non-interacting case ($J=0$), for a finite system, $U_4 = 2/(3N)$, which is close to $0$.\n\n**4. Algorithmic Implementation**\n\nThe simulation will proceed as follows for each test case $(L, J, T, n_{\\text{therm}}, n_{\\text{meas}})$:\n1.  Initialize an $L \\times L$ lattice with random spins ($+1$ or $-1$ with equal probability).\n2.  Define the red and black sublattice masks.\n3.  Set $\\beta = 1/T$ (if $T > 0$).\n4.  **Thermalization**: Perform $n_{\\text{therm}}$ full checkerboard sweeps without taking measurements.\n5.  **Measurement**: Perform $n_{\\text{meas}}$ sweeps. After each sweep:\n    a. Calculate instantaneous magnetization $m$ and energy per site $e$.\n    b. Store $|m|$, $e$, $m^2$, and $m^4$.\n6.  **Averaging**: After the measurement loop, compute the averages of the stored quantities: $\\langle |m| \\rangle$, $\\langle e \\rangle$, $\\langle m^2 \\rangle$, and $\\langle m^4 \\rangle$.\n7.  Calculate the Binder cumulant $U_4$ from the averaged moments.\n8.  Return the final three observables, rounded as specified.\n\nThe use of `numpy` allows for efficient vectorization of the neighbor sum calculation (using `numpy.roll`) and the probabilistic updates across an entire sublattice at once.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(L, J, T, n_therm, n_meas, rng):\n    \"\"\"\n    Runs a Markov Chain Monte Carlo simulation for the 2D Ising model.\n\n    Args:\n        L (int): Linear size of the square lattice.\n        J (float): Coupling constant.\n        T (float): Temperature.\n        n_therm (int): Number of thermalization sweeps.\n        n_meas (int): Number of measurement sweeps.\n        rng (np.random.Generator): Random number generator instance.\n\n    Returns:\n        tuple: A tuple containing the averaged absolute magnetization,\n               averaged energy per site, and the Binder cumulant.\n    \"\"\"\n    N = L * L\n    \n    # Initialize spins randomly\n    spins = rng.choice([-1, 1], size=(L, L))\n    \n    # Create checkerboard masks\n    x, y = np.meshgrid(range(L), range(L))\n    red_mask = (x + y) % 2 == 0\n    black_mask = ~red_mask\n    \n    beta = 1.0 / T if T > 0 else float('inf')\n\n    # Simulation sweeps\n    for _ in range(n_therm + n_meas):\n        # Gibbs sweep\n        for mask in [red_mask, black_mask]:\n            # Sum of neighbors using np.roll for periodic boundaries\n            neighbors_sum = (np.roll(spins, 1, axis=0) +\n                             np.roll(spins, -1, axis=0) +\n                             np.roll(spins, 1, axis=1) +\n                             np.roll(spins, -1, axis=1))\n            \n            # Argument for the exponential in the probability calculation\n            # For J=0, delta_E_arg is 0, prob is 0.5, which is correct.\n            delta_E_arg = 2.0 * J * neighbors_sum * beta\n            \n            # Probability to be in the +1 state, numerically stable form\n            prob_plus_one = 1.0 / (1.0 + np.exp(-delta_E_arg))\n            \n            # Generate random numbers and update spins\n            rand_flips = rng.random(size=(L,L))\n            new_spins = 2 * (rand_flips  prob_plus_one) - 1\n            \n            # Apply update only to the current sublattice\n            spins[mask] = new_spins[mask]\n\n        # Start measurements after thermalization\n        if _ >= n_therm:\n            if _ == n_therm:\n                # Initialize accumulator lists\n                m_vals = []\n                e_vals = []\n                m2_vals = []\n                m4_vals = []\n\n            # Calculate observables\n            m = np.mean(spins)\n            \n            # Energy calculation (each bond counted once)\n            energy_total = -J * np.sum(spins * (np.roll(spins, 1, axis=0) + np.roll(spins, 1, axis=1)))\n            e = energy_total / N\n            \n            # Store values for averaging\n            m_vals.append(np.abs(m))\n            e_vals.append(e)\n            m2_vals.append(m**2)\n            m4_vals.append(m**4)\n        \n    # Calculate final averages\n    avg_abs_m = np.mean(m_vals)\n    avg_e = np.mean(e_vals)\n    avg_m2 = np.mean(m2_vals)\n    avg_m4 = np.mean(m4_vals)\n    \n    # Calculate Binder cumulant, handle division by zero\n    if avg_m2 > 1e-12:\n        binder_cumulant = 1.0 - (avg_m4 / (3.0 * avg_m2**2))\n    else:\n        # Occurs if m is always zero (highly unlikely in finite simulation)\n        # or for J=0 case where avg_m2 is small. The theoretical value is 0.\n        binder_cumulant = 0.0\n\n    return avg_abs_m, avg_e, binder_cumulant\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, J, T, n_therm, n_meas)\n        (40, 1.0, 1.8, 800, 800),\n        (40, 1.0, 2.269, 1500, 1500),\n        (40, 1.0, 3.5, 800, 800),\n        (40, 0.0, 1.0, 500, 1000),\n    ]\n\n    # For reproducibility\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for case in test_cases:\n        L, J, T, n_therm, n_meas = case\n        avg_abs_m, avg_e, binder_cumulant = run_simulation(L, J, T, n_therm, n_meas, rng)\n        \n        # Round results to 4 decimal places\n        result_tuple = [\n            round(avg_abs_m, 4),\n            round(avg_e, 4),\n            round(binder_cumulant, 4)\n        ]\n        results.append(str(result_tuple).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}