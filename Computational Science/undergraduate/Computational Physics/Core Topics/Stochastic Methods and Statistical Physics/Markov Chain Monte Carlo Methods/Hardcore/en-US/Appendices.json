{
    "hands_on_practices": [
        {
            "introduction": "The Metropolis algorithm is a cornerstone of MCMC methods, and its power lies in the elegant logic of its acceptance probability. This simple rule determines whether the simulation accepts a proposed move, striking a crucial balance between exploiting high-probability regions and exploring the entire state space. This exercise provides a direct, hands-on calculation of this probability, offering a clear window into the engine that drives the sampler. ",
            "id": "1371728",
            "problem": "A data scientist is implementing a Markov Chain Monte Carlo (MCMC) simulation to draw samples from a posterior probability distribution for a parameter $x$. The target distribution, $\\pi(x)$, is proportional to the exponential of the negative absolute value of the parameter, such that $\\pi(x) \\propto \\exp(-|x|)$.\n\nThe scientist uses the Metropolis algorithm with a symmetric proposal distribution $q(x'|x)$, where the probability of proposing a new state $x'$ given the current state $x$ is equal to the probability of proposing $x$ given $x'$ (i.e., $q(x'|x) = q(x|x')$).\n\nSuppose that at a certain step in the simulation, the current state of the chain is $x = 1.5$. The algorithm then proposes a move to a new candidate state $x' = 2.0$.\n\nCalculate the acceptance probability for this specific move. Your answer should be a dimensionless real number. Round your final answer to four significant figures.",
            "solution": "The Metropolis acceptance probability for a move from $x$ to $x'$ with a symmetric proposal $q(x'|x)=q(x|x')$ is\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\frac{\\pi(x')q(x|x')}{\\pi(x)q(x'|x)}\\right)=\\min\\left(1,\\frac{\\pi(x')}{\\pi(x)}\\right).\n$$\nGiven the target distribution $\\pi(x)\\propto \\exp(-|x|)$, the ratio simplifies to\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\frac{\\exp(-|x'|)}{\\exp(-|x|)}=\\exp\\!\\left(-\\left(|x'|-|x|\\right)\\right).\n$$\nWith $x=1.5$ and $x'=2.0$, we have $|x|=1.5$ and $|x'|=2.0$, so\n$$\n\\frac{\\pi(x')}{\\pi(x)}=\\exp\\!\\left(-\\left(2.0-1.5\\right)\\right)=\\exp(-0.5).\n$$\nTherefore,\n$$\n\\alpha(x \\to x')=\\min\\left(1,\\exp(-0.5)\\right)=\\exp(-0.5).\n$$\nNumerically, $\\exp(-0.5)\\approx 0.6065$ when rounded to four significant figures.",
            "answer": "$$\\boxed{0.6065}$$"
        },
        {
            "introduction": "While the Metropolis algorithm proposes and then accepts or rejects moves, Gibbs sampling offers a powerful alternative, especially when conditional distributions are easy to sample from. This method updates one variable at a time by drawing its new value conditional on the current state of all other variables. This practice demonstrates the mechanics of a Gibbs sampler in a simple, intuitive system, allowing you to trace a single step of the process and understand how local conditional updates guide the chain's evolution. ",
            "id": "1316579",
            "problem": "Consider a simplified model of social influence between two individuals, Alice and Bob. Each individual can be in one of two states: having adopted a new technology (state 1) or not having adopted it (state 0). Let their states be represented by binary random variables $X_A$ and $X_B$, respectively, where $X_A, X_B \\in \\{0, 1\\}$.\n\nThe two individuals influence each other. The joint probability distribution for their combined state $(x_A, x_B)$ is found to follow a simple energy-based model, where the probability is proportional to an interaction term:\n$$\nP(x_A, x_B) \\propto \\exp(J x_A x_B)\n$$\nHere, $J$ is a positive real constant representing the strength of the mutual influence; a higher $J$ means they are more likely to be in the same state.\n\nTo simulate the evolution of their states, a Gibbs sampling procedure is employed. One full step of this procedure consists of updating each individual's state sequentially, using the most recent information available. The update order is fixed: first Alice, then Bob.\n\nSuppose the system is initially in the state $(X_A, X_B) = (0, 1)$, meaning Alice has not adopted the technology, but Bob has. What is the probability that after one full step of the Gibbs sampling procedure, the system is in the state $(1, 1)$, with both individuals having adopted the technology?\n\nExpress your answer as a closed-form analytic expression in terms of the interaction constant $J$.",
            "solution": "We are given a joint distribution over $(X_{A}, X_{B}) \\in \\{0,1\\}^{2}$ proportional to\n$$\nP(x_{A}, x_{B}) \\propto \\exp(J x_{A} x_{B}).\n$$\nGibbs sampling updates each variable from its conditional distribution given the other, using the most recent value. We start from $(X_{A}, X_{B})=(0,1)$ and update Alice first, then Bob.\n\nFirst derive the conditional distributions. For any fixed value $b \\in \\{0,1\\}$,\n$$\nP(X_{A}=x \\mid X_{B}=b)=\\frac{\\exp(J x b)}{\\sum_{x' \\in \\{0,1\\}} \\exp(J x' b)}=\\frac{\\exp(J x b)}{1+\\exp(J b)}.\n$$\nHence\n$$\nP(X_{A}=1 \\mid X_{B}=b)=\\frac{\\exp(J b)}{1+\\exp(J b)}.\n$$\nSimilarly, for any fixed $a \\in \\{0,1\\}$,\n$$\nP(X_{B}=1 \\mid X_{A}=a)=\\frac{\\exp(J a)}{1+\\exp(J a)}.\n$$\n\nNow apply the sequential updates from the initial state $(0,1)$.\n\nStep 1 (update Alice given $X_{B}=1$):\n$$\nP(\\text{Alice becomes }1 \\mid X_{B}=1)=\\frac{\\exp(J)}{1+\\exp(J)}.\n$$\n\nStep 2 (update Bob given the updated $X_{A}$):\nTo end in $(1,1)$, Alice must be $1$ after Step 1, and then Bob must be updated to $1$ given $X_{A}=1$. Thus\n$$\nP(\\text{Bob becomes }1 \\mid X_{A}=1)=\\frac{\\exp(J)}{1+\\exp(J)}.\n$$\n\nTherefore, the probability that after one full Gibbs step the state is $(1,1)$ is the product of these two sequential probabilities:\n$$\nP\\big((1,1)\\text{ after one full step} \\mid (0,1)\\text{ initially}\\big)=\\left(\\frac{\\exp(J)}{1+\\exp(J)}\\right)^{2}=\\frac{\\exp(2J)}{(1+\\exp(J))^{2}}.\n$$",
            "answer": "$$\\boxed{\\frac{\\exp(2J)}{(1+\\exp(J))^{2}}}$$"
        },
        {
            "introduction": "Beyond understanding the mechanics of an algorithm, a skilled practitioner must also recognize its potential failure modes. A critical challenge in MCMC arises when the target distribution has multiple, well-separated peaks (modes), as a poorly tuned sampler can become trapped in one region and fail to discover the others. This thought experiment explores this common pitfall, asking you to diagnose the behavior of a sampler and understand why a seemingly reasonable choice of parameters can lead to a profoundly misleading result. ",
            "id": "1932795",
            "problem": "A data scientist is employing a Markov chain Monte Carlo (MCMC) method to draw samples from a complex, one-dimensional target probability density function, $\\pi(x)$. The target distribution is known to be a symmetric bimodal distribution, specifically an equal-weight mixture of two Gaussian distributions. The density is proportional to the sum of two Gaussian probability density functions:\n$$ \\pi(x) \\propto \\exp\\left(-\\frac{(x - \\mu_A)^2}{2\\sigma_{mode}^2}\\right) + \\exp\\left(-\\frac{(x - \\mu_B)^2}{2\\sigma_{mode}^2}\\right) $$\nThe parameters are given as $\\mu_A = -10$, $\\mu_B = 10$, and $\\sigma_{mode} = 1$. This structure results in two narrow, well-separated probability modes centered at $x=-10$ and $x=10$, with a region of extremely low probability density between them.\n\nThe scientist uses a random-walk Metropolis algorithm. At each step, a new state $x'$ is proposed from a Gaussian distribution centered at the current state $x_t$, i.e., $x' \\sim N(x_t, \\sigma_{step}^2)$. Seeking to achieve a high acceptance rate, the scientist chooses a very small step size variance, setting $\\sigma_{step} = 0.1$. The MCMC chain is initialized at the peak of one of the modes, $x_0 = -10$, and is run for $N=10^6$ iterations.\n\nWhich of the following statements most accurately describes the behavior of the MCMC sampler and the statistical properties of the resulting sample set $\\{x_1, x_2, \\dots, x_N\\}$?\n\nA. The samples will be distributed around the true mean of the target distribution, which is $x=0$. The sample mean will be close to 0, but the sample variance will be large (greater than 100), accurately reflecting the significant separation between the two modes.\n\nB. The acceptance rate of proposed moves will be very low (close to 0) because the step size is not well-tuned to the overall scale of the target distribution. The chain will remain at or very near its initial position, $x_0 = -10$.\n\nC. The acceptance rate of proposed moves will be very high (close to 1). The generated samples will thoroughly explore the region corresponding to the mode at $x=-10$, but the chain will fail to transition to the other mode at $x=10$. The sample mean will be approximately $-10$.\n\nD. The sampler will efficiently explore the entire state space. The chain will frequently jump back and forth between the two modes, and the histogram of the samples will correctly form two distinct peaks centered at $x=-10$ and $x=10$.\n\nE. The sampler will behave like a simple random walk, causing the samples to diffuse away from the starting point. The final collection of samples will be approximately uniformly distributed over a wide interval centered at $x=-10$.",
            "solution": "We model the target as an equal-weight mixture of two Gaussian densities with common standard deviation $\\sigma_{mode}$ and means $\\mu_{A}$ and $\\mu_{B}$. Up to proportionality,\n$$\n\\pi(x) \\propto \\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right) + \\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nWith a random-walk Metropolis sampler using a symmetric Gaussian proposal $q(x' \\mid x)=\\mathcal{N}(x,\\sigma_{step}^{2})$, the Metropolisâ€“Hastings acceptance probability at state $x_{t}$ for a proposal $x'$ is\n$$\n\\alpha(x_{t},x')=\\min\\!\\left(1,\\frac{\\pi(x')}{\\pi(x_{t})}\\right).\n$$\n\nInitialize at $x_{0}=\\mu_{A}$. Because the modes are well separated, when $x$ is near $\\mu_{A}$ the contribution from the $\\mu_{B}$-component in $\\pi(x)$ is negligible relative to that from the $\\mu_{A}$-component. For a small proposal increment $\\epsilon:=x'-x$ with $|\\epsilon| \\ll \\sigma_{mode}$, the dominant-ratio approximation gives\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\frac{\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}{\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right)}=\\exp\\!\\left(-\\frac{(x'-\\mu_{A})^{2}-(x-\\mu_{A})^{2}}{2\\sigma_{mode}^{2}}\\right).\n$$\nAt $x\\approx\\mu_{A}$, this simplifies for small $\\epsilon$ to\n$$\n\\frac{\\pi(x')}{\\pi(x)} \\approx \\exp\\!\\left(-\\frac{\\epsilon^{2}}{2\\sigma_{mode}^{2}}\\right),\n$$\nso the acceptance probability is close to $1$ when $\\sigma_{step} \\ll \\sigma_{mode}$ because typical $|\\epsilon|$ is on the order of $\\sigma_{step}$. Hence the acceptance rate is very high while the chain explores the vicinity of the starting mode.\n\nA direct jump from the neighborhood of $\\mu_{A}$ to the neighborhood of $\\mu_{B}$ in one proposal requires a displacement of order $|\\mu_{B}-\\mu_{A}|$. Under a Gaussian proposal with variance $\\sigma_{step}^{2}$, the probability of such a jump is of order\n$$\n\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma_{step}^{2}}\\right),\n$$\nwhich is negligible when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{step}$.\n\nCrossing the low-density region via many small accepted steps is also overwhelmingly unlikely within a finite run because the stationary density in the valley is exponentially smaller than at the peak. At the midpoint $x^{\\star}=(\\mu_{A}+\\mu_{B})/2$, the target density is\n$$\n\\pi(x^{\\star}) \\propto 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right),\n$$\nwhile near $x=\\mu_{A}$ it is $\\pi(\\mu_{A}) \\propto 1$ (the other component there is negligible). Thus the ratio\n$$\n\\frac{\\pi(x^{\\star})}{\\pi(\\mu_{A})} \\approx 2\\exp\\!\\left(-\\frac{(\\mu_{B}-\\mu_{A})^{2}}{8\\sigma_{mode}^{2}}\\right)\n$$\nis exponentially small when $|\\mu_{B}-\\mu_{A}| \\gg \\sigma_{mode}$. This implies an exponentially large expected time to reach the valley or the other mode, on the order of the inverse of this ratio, which far exceeds the given $N$ when the separation is large and the proposal is very local.\n\nTherefore, with $\\sigma_{step}$ chosen very small relative to $\\sigma_{mode}$ and with well-separated modes, the chain has a very high acceptance rate, thoroughly explores the local basin around the starting mode at $x=\\mu_{A}$, essentially never transitions to the other mode within the run, and yields a sample mean approximately equal to $\\mu_{A}$. Among the options, this corresponds to statement C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}