{
    "hands_on_practices": [
        {
            "introduction": "One of the powerful features of importance sampling is its flexibility. Once you have invested computational effort to generate a set of samples from a proposal distribution, you can often reuse those samples to estimate integrals of *different* functions. This exercise challenges you to think critically about how this is done correctly and what the statistical consequences are, particularly for the variance of your new estimate. Understanding this principle is key to maximizing the value of your simulations .",
            "id": "2402977",
            "problem": "Consider a measurable domain $D \\subset \\mathbb{R}^d$ with Lebesgue measure $|D| \\in (0,+\\infty]$. Let $f:D \\to [0,+\\infty)$ be measurable, strictly positive almost everywhere on $D$, and integrable, with $K \\equiv \\int_D f(x)\\,dx  +\\infty$. You have generated $N$ independent and identically distributed samples $x_1,\\dots,x_N$ from a proposal probability density $p(x)$ that satisfies $p(x)  0$ wherever $f(x)  0$. For estimating the integral $I_f \\equiv \\int_D f(x)\\,dx$, you stored the usual importance sampling weights $w_i \\equiv f(x_i)/p(x_i)$ associated with these samples. You now wish to estimate $J \\equiv \\int_D \\sqrt{f(x)}\\,dx$.\n\nWhich option best describes whether you can reuse the same samples and weights to obtain a statistically sound estimate of $J$, and what variance you should expect, particularly in the common design choice $p(x) \\propto f(x)$?\n\nA. Reuse both the samples $x_i$ and the original weights $w_i = f(x_i)/p(x_i)$ without change. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i$ is unbiased for $J$ and has minimal variance when $p(x) \\propto f(x)$.\n\nB. You may reuse the same samples $x_i$, but you must recompute weights as $w_i' = \\sqrt{f(x_i)}/p(x_i)$. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i'$ is unbiased for $J$ with variance $\\frac{1}{N}\\left(\\int_D \\frac{f(x)}{p(x)}\\,dx - J^2\\right)$. In particular, if $p(x) = f(x)/K$ and $f(x)  0$ almost everywhere on $D$, then $\\mathrm{Var} = \\frac{1}{N}\\left(K\\,|D| - J^2\\right)$, which is infinite when $|D| = +\\infty$.\n\nC. You cannot reuse the samples $x_i$ at all. To avoid bias, you must draw a fresh set of samples from a proposal density proportional to $\\sqrt{f(x)}$, and any attempt to reuse the original samples leads to bias.\n\nD. Self-normalizing the original weights as $\\tilde{w}_i = w_i/\\sum_{j=1}^N w_j$ and reusing them with the same samples yields an unbiased estimator for $J$ that always has lower variance than the unnormalized importance sampling estimator, regardless of $p(x)$.",
            "solution": "The problem statement is validated as scientifically sound, well-posed, and objective. It poses a standard question in the theory of Monte Carlo integration. We may proceed with the solution.\n\nThe fundamental principle of importance sampling is to estimate an integral $I_g = \\int_D g(x) \\,dx$ by rewriting it as an expectation with respect to a proposal probability density $p(x)$. The proposal density must satisfy $p(x)  0$ for all $x \\in D$ where $g(x) \\neq 0$. The integral is expressed as:\n$$ I_g = \\int_D \\frac{g(x)}{p(x)} p(x) \\,dx = \\mathbb{E}_{X \\sim p} \\left[ \\frac{g(X)}{p(X)} \\right] $$\nGiven $N$ independent and identically distributed (i.i.d.) samples $x_1, \\dots, x_N$ drawn from $p(x)$, an unbiased estimator for $I_g$ is given by the sample mean:\n$$ \\hat{I}_g = \\frac{1}{N} \\sum_{i=1}^N \\frac{g(x_i)}{p(x_i)} $$\nIn this problem, we wish to estimate the integral $J \\equiv \\int_D \\sqrt{f(x)}\\,dx$. We identify the integrand as $g(x) = \\sqrt{f(x)}$. The problem states that samples $x_i$ have been drawn from a proposal density $p(x)$ that satisfies $p(x)  0$ wherever $f(x)  0$. Since $f(x)  0$ implies $\\sqrt{f(x)}  0$, the support condition for estimating $J$ is satisfied. Therefore, we can reuse the existing samples $x_i$.\n\nThe correct importance sampling estimator for $J$ is constructed by setting $g(x) = \\sqrt{f(x)}$:\n$$ \\hat{J} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\sqrt{f(x_i)}}{p(x_i)} $$\nThe terms in this sum, $w_i' \\equiv \\frac{\\sqrt{f(x_i)}}{p(x_i)}$, are the new importance weights required for estimating $J$. They are distinct from the original weights $w_i = f(x_i)/p(x_i)$ used for estimating $I_f = \\int_D f(x)\\,dx$.\n\nThe statistical properties of this estimator $\\hat{J}$ must be examined.\nFirst, its expectation (bias):\n$$ \\mathbb{E}[\\hat{J}] = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^N \\frac{\\sqrt{f(X_i)}}{p(X_i)}\\right] = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{E}\\left[\\frac{\\sqrt{f(X_i)}}{p(X_i)}\\right] $$\nSince the samples are i.i.d., the expectation of each term is identical:\n$$ \\mathbb{E}\\left[\\frac{\\sqrt{f(X)}}{p(X)}\\right] = \\int_D \\frac{\\sqrt{f(x)}}{p(x)} p(x) \\,dx = \\int_D \\sqrt{f(x)} \\,dx = J $$\nThus, $\\mathbb{E}[\\hat{J}] = \\frac{1}{N} (N \\cdot J) = J$. The estimator is unbiased.\n\nSecond, its variance:\nThe variance of the sample mean of $N$ i.i.d. random variables is $\\frac{1}{N}$ times the variance of a single variable.\n$$ \\mathrm{Var}[\\hat{J}] = \\frac{1}{N} \\mathrm{Var}\\left[\\frac{\\sqrt{f(X)}}{p(X)}\\right] $$\nUsing the formula $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$ with $Y = \\frac{\\sqrt{f(X)}}{p(X)}$, we have $\\mathbb{E}[Y] = J$. The second moment is:\n$$ \\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\frac{\\sqrt{f(X)}}{p(X)}\\right)^2\\right] = \\mathbb{E}\\left[\\frac{f(X)}{p(X)^2}\\right] = \\int_D \\frac{f(x)}{p(x)^2} p(x) \\,dx = \\int_D \\frac{f(x)}{p(x)} \\,dx $$\nTherefore, the variance of the estimator $\\hat{J}$ is:\n$$ \\mathrm{Var}[\\hat{J}] = \\frac{1}{N} \\left( \\int_D \\frac{f(x)}{p(x)} \\,dx - J^2 \\right) $$\nNow, let us consider the specific design choice $p(x) \\propto f(x)$. For $p(x)$ to be a valid probability density, it must integrate to $1$. Given $\\int_D f(x)\\,dx = K$, the correct normalization is $p(x) = f(x)/K$. Substituting this into the variance expression:\n$$ \\int_D \\frac{f(x)}{p(x)} \\,dx = \\int_D \\frac{f(x)}{f(x)/K} \\,dx = \\int_D K \\,dx = K \\int_D 1 \\,dx = K|D| $$\nwhere $|D|$ is the Lebesgue measure of the domain $D$. The variance becomes:\n$$ \\mathrm{Var}[\\hat{J}] = \\frac{1}{N} \\left( K|D| - J^2 \\right) $$\nThis result is valid only if the integral $\\int_D K \\,dx$ converges. If the domain $D$ has infinite measure, $|D| = +\\infty$, then this integral diverges, and the variance of the estimator $\\hat{J}$ is infinite.\n\nWith these derivations, we evaluate each option.\n\n**A. Reuse both the samples $x_i$ and the original weights $w_i = f(x_i)/p(x_i)$ without change. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i$ is unbiased for $J$ and has minimal variance when $p(x) \\propto f(x)$.**\nThe estimator proposed is $\\hat{I}_f = \\frac{1}{N}\\sum_{i=1}^N w_i$. Its expectation is $\\mathbb{E}[\\hat{I}_f] = \\int_D f(x)\\,dx = K$. This estimator is unbiased for $I_f$, not for $J = \\int_D \\sqrt{f(x)}\\,dx$. The statement that it is unbiased for $J$ is false. The claim about minimal variance for this choice of $p(x)$ pertains to estimating $I_f$, not $J$.\nVerdict: **Incorrect**.\n\n**B. You may reuse the same samples $x_i$, but you must recompute weights as $w_i' = \\sqrt{f(x_i)}/p(x_i)$. The estimator $\\frac{1}{N}\\sum_{i=1}^N w_i'$ is unbiased for $J$ with variance $\\frac{1}{N}\\left(\\int_D \\frac{f(x)}{p(x)}\\,dx - J^2\\right)$. In particular, if $p(x) = f(x)/K$ and $f(x)  0$ almost everywhere on $D$, then $\\mathrm{Var} = \\frac{1}{N}\\left(K\\,|D| - J^2\\right)$, which is infinite when $|D| = +\\infty$.**\nThis option correctly states that samples can be reused but require new weights $w_i' = \\sqrt{f(x_i)}/p(x_i)$. Our derivation confirms that the estimator $\\hat{J} = \\frac{1}{N}\\sum_{i=1}^N w_i'$ is indeed unbiased for $J$. The expression for the variance $\\frac{1}{N}\\left(\\int_D \\frac{f(x)}{p(x)}\\,dx - J^2\\right)$ is precisely what we derived. The subsequent analysis of the special case $p(x) = f(x)/K$, leading to a variance of $\\frac{1}{N}\\left(K|D| - J^2\\right)$ and its divergence for $|D|=\\infty$, is also correct. Every part of this statement is validated by our rigorous analysis.\nVerdict: **Correct**.\n\n**C. You cannot reuse the samples $x_i$ at all. To avoid bias, you must draw a fresh set of samples from a proposal density proportional to $\\sqrt{f(x)}$, and any attempt to reuse the original samples leads to bias.**\nThis statement is fundamentally flawed. A key strength of importance sampling is the ability to reuse a single set of samples to estimate multiple different integrals, provided the support condition holds for each. Our analysis for option B demonstrates that reusing the samples $x_i$ with recomputed weights yields a provably unbiased estimator for $J$. The claim that this procedure leads to bias is false. While sampling from $p(x) \\propto \\sqrt{f(x)}$ would be optimal for estimating $J$ (it would yield zero variance), it is not the only way to obtain an unbiased estimate.\nVerdict: **Incorrect**.\n\n**D. Self-normalizing the original weights as $\\tilde{w}_i = w_i/\\sum_{j=1}^N w_j$ and reusing them with the same samples yields an unbiased estimator for $J$ that always has lower variance than the unnormalized importance sampling estimator, regardless of $p(x)$.**\nThis option describes a self-normalized importance sampling scheme. First, any such estimator for a quantity not proportional to $I_f$ is generally biased for finite sample size $N$, although it is typically asymptotically unbiased. The claim of being unbiased is false. Second, the claim that it *always* has lower variance is also false; while self-normalization can be beneficial, especially if the normalization constant of the proposal is unknown, it does not guarantee variance reduction in all cases. Third, the description of how to form an estimator for $J$ using these normalized weights $\\tilde{w}_i$ is ambiguous and, under standard interpretations, does not lead to an estimator for $J$.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The efficiency of an importance sampling scheme hinges on how well the proposal distribution $p(x)$ matches the integrand $f(x)$. This practice explores a crucial aspect of this principle: choosing a sampling scheme that respects the symmetries of the problem. By analytically comparing the variance of two different samplers—one in Cartesian coordinates and another in polar coordinates—for a radially symmetric integral, you will gain a deep, quantitative understanding of how a smart choice of coordinates and distributions can dramatically reduce variance .",
            "id": "2402986",
            "problem": "You must write a complete, runnable program that compares the efficiency of two mathematically defined importance sampling strategies for estimating a radially symmetric two-dimensional integral. Consider the scalar integral\n$$\nI(\\alpha) \\equiv \\int_{\\mathbb{R}^2} \\exp\\!\\left(-\\alpha\\,(x^2+y^2)\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y,\n$$\nwhere $\\alphagt;0$ and $(x,y)\\in\\mathbb{R}^2$. Let $f(x,y;\\alpha) \\equiv \\exp\\!\\left(-\\alpha\\,(x^2+y^2)\\right)$ denote the integrand. Define two importance sampling strategies as follows, each of which induces a single-sample importance weight whose variance quantifies sampling efficiency.\n\n1. Cartesian scheme. Draw $(X,Y)$ from a two-dimensional normal density with independent components of variance $\\sigma^2$, that is\n$$\nq_{\\mathrm{C}}(x,y;\\sigma) \\equiv \\frac{1}{2\\pi\\,\\sigma^2}\\,\\exp\\!\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right),\n$$\nwith $\\sigmagt;0$. The corresponding single-sample importance weight is\n$$\nW_{\\mathrm{C}} \\equiv \\frac{f(X,Y;\\alpha)}{q_{\\mathrm{C}}(X,Y;\\sigma)}.\n$$\n\n2. Polar scheme. Work in polar coordinates $(r,\\theta)$ with the standard Jacobian factor $r$, where $\\theta$ is an angle in radians. Draw $\\Theta$ uniformly on $[0,2\\pi)$ and draw $R$ from the radial density\n$$\nq_{r}(r;\\beta) \\equiv 2\\beta\\,r\\,\\exp\\!\\left(-\\beta\\,r^2\\right),\\quad r\\ge 0,\n$$\nwith $\\betagt;0$, independently of $\\Theta$. The joint proposal is $q_{\\mathrm{P}}(r,\\theta;\\beta) \\equiv q_{r}(r;\\beta)\\,q_{\\theta}(\\theta)$ with $q_{\\theta}(\\theta)\\equiv \\frac{1}{2\\pi}$ on $[0,2\\pi)$. The corresponding single-sample importance weight for the polar integral\n$$\nI(\\alpha) \\equiv \\int_{0}^{2\\pi}\\!\\int_{0}^{\\infty} \\exp\\!\\left(-\\alpha\\,r^2\\right)\\,r\\,\\mathrm{d}r\\,\\mathrm{d}\\theta\n$$\nis\n$$\nW_{\\mathrm{P}} \\equiv \\frac{\\exp\\!\\left(-\\alpha\\,R^2\\right)\\,R}{q_{r}(R;\\beta)\\,q_{\\theta}(\\Theta)}.\n$$\n\nFor each parameter set $(\\alpha,\\sigma,\\beta)$ in the test suite below, compute the ratio\n$$\n\\rho(\\alpha,\\sigma,\\beta) \\equiv \\frac{\\operatorname{Var}(W_{\\mathrm{C}})}{\\operatorname{Var}(W_{\\mathrm{P}})},\n$$\nunder the conditions $\\alphagt;\\frac{1}{4\\sigma^2}$ and $0lt;\\betalt;2\\alpha$ which ensure both variances are finite. Larger values of $\\rho$ indicate greater efficiency of the polar scheme relative to the Cartesian scheme for the same number of samples, since the variance of the sample mean scales inversely with the same sample size for both schemes.\n\nTest suite: evaluate and report $\\rho(\\alpha,\\sigma,\\beta)$ for the following parameter sets, each written as $(\\alpha,\\sigma,\\beta)$:\n- $(\\alpha,\\sigma,\\beta) = (1.0, 1.0, 0.7)$,\n- $(\\alpha,\\sigma,\\beta) = (0.26, 1.0, 0.1)$,\n- $(\\alpha,\\sigma,\\beta) = (0.55, 0.7, 1.08)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[v1,v2,v3]\"), where each $v_i$ is the corresponding $\\rho$ rounded to exactly six decimal places. No physical units apply in this problem, and all angles are in radians.",
            "solution": "The problem requires the analytical computation of the ratio $\\rho(\\alpha,\\sigma,\\beta) \\equiv \\frac{\\operatorname{Var}(W_{\\mathrm{C}})}{\\operatorname{Var}(W_{\\mathrm{P}})}$, which compares the efficiency of a Cartesian and a Polar importance sampling scheme for a specific two-dimensional integral. A higher value of $\\rho$ indicates that the Polar scheme is more efficient. The problem is well-posed, scientifically sound, and all parameters are defined such that the variances are finite. We proceed with the analytical derivation.\n\nThe variance of an importance weight $W$ is given by $\\operatorname{Var}(W) = \\mathbb{E}[W^2] - (\\mathbb{E}[W])^2$. For any valid importance sampling scheme, the expectation of the weight, $\\mathbb{E}[W]$, is equal to the true value of the integral, $I(\\alpha)$.\nFirst, we compute the integral $I(\\alpha)$:\n$$\nI(\\alpha) \\equiv \\int_{\\mathbb{R}^2} \\exp\\!\\left(-\\alpha\\,(x^2+y^2)\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y\n$$\nBy converting to polar coordinates $(r, \\theta)$, where $x^2+y^2=r^2$ and $\\mathrm{d}x\\,\\mathrm{d}y = r\\,\\mathrm{d}r\\,\\mathrm{d}\\theta$, we obtain:\n$$\nI(\\alpha) = \\int_{0}^{2\\pi} \\mathrm{d}\\theta \\int_{0}^{\\infty} \\exp(-\\alpha r^2) r\\,\\mathrm{d}r = 2\\pi \\left[ -\\frac{1}{2\\alpha} \\exp(-\\alpha r^2) \\right]_{0}^{\\infty} = 2\\pi \\left( 0 - \\left(-\\frac{1}{2\\alpha}\\right) \\right) = \\frac{\\pi}{\\alpha}\n$$\nThus, for both schemes, $(\\mathbb{E}[W])^2 = (I(\\alpha))^2 = (\\pi/\\alpha)^2$. The core of the task is to compute $\\mathbb{E}[W_{\\mathrm{C}}^2]$ and $\\mathbb{E}[W_{\\mathrm{P}}^2]$. This is given by the general formula $\\mathbb{E}[W^2] = \\int \\frac{f(z)^2}{q(z)}\\,\\mathrm{d}z$, where $z$ represents the integration variables.\n\n**1. Variance of the Cartesian weight, $\\operatorname{Var}(W_{\\mathrm{C}})$**\n\nThe integrand is $f(x,y;\\alpha) = \\exp(-\\alpha(x^2+y^2))$, and the proposal distribution is $q_{\\mathrm{C}}(x,y;\\sigma) = \\frac{1}{2\\pi\\sigma^2}\\exp(-\\frac{x^2+y^2}{2\\sigma^2})$. The second moment of the weight is:\n$$\n\\mathbb{E}[W_{\\mathrm{C}}^2] = \\int_{\\mathbb{R}^2} \\frac{\\left[ \\exp(-\\alpha(x^2+y^2)) \\right]^2}{\\frac{1}{2\\pi\\sigma^2}\\exp(-\\frac{x^2+y^2}{2\\sigma^2})} \\,\\mathrm{d}x\\,\\mathrm{d}y = 2\\pi\\sigma^2 \\int_{\\mathbb{R}^2} \\exp\\left(-\\left(2\\alpha - \\frac{1}{2\\sigma^2}\\right)(x^2+y^2)\\right) \\,\\mathrm{d}x\\,\\mathrm{d}y\n$$\nThis integral is of the form $I(\\alpha')$ where $\\alpha' = 2\\alpha - \\frac{1}{2\\sigma^2}$. The problem's condition $\\alpha  \\frac{1}{4\\sigma^2}$ ensures $\\alpha'0$, so the integral converges.\n$$\n\\mathbb{E}[W_{\\mathrm{C}}^2] = 2\\pi\\sigma^2 \\cdot I(\\alpha') = 2\\pi\\sigma^2 \\cdot \\frac{\\pi}{\\alpha'} = 2\\pi\\sigma^2 \\frac{\\pi}{2\\alpha - \\frac{1}{2\\sigma^2}} = \\frac{4\\pi^2\\sigma^4}{4\\alpha\\sigma^2 - 1}\n$$\nThe variance is then:\n$$\n\\operatorname{Var}(W_{\\mathrm{C}}) = \\mathbb{E}[W_{\\mathrm{C}}^2] - \\left(\\frac{\\pi}{\\alpha}\\right)^2 = \\frac{4\\pi^2\\sigma^4}{4\\alpha\\sigma^2 - 1} - \\frac{\\pi^2}{\\alpha^2} = \\pi^2\\left(\\frac{4\\alpha^2\\sigma^4 - (4\\alpha\\sigma^2-1)}{\\alpha^2(4\\alpha\\sigma^2 - 1)}\\right) = \\pi^2 \\frac{(2\\alpha\\sigma^2 - 1)^2}{\\alpha^2(4\\alpha\\sigma^2 - 1)}\n$$\n\n**2. Variance of the Polar weight, $\\operatorname{Var}(W_{\\mathrm{P}})$**\n\nIn polar coordinates, the integrand including the Jacobian is $g(r,\\theta) = r\\exp(-\\alpha r^2)$. The proposal distribution is $q_{\\mathrm{P}}(r,\\theta;\\beta) = q_{r}(r;\\beta)q_{\\theta}(\\theta) = \\left(2\\beta r \\exp(-\\beta r^2)\\right) \\left(\\frac{1}{2\\pi}\\right)$. The second moment of the weight is:\n$$\n\\mathbb{E}[W_{\\mathrm{P}}^2] = \\int_{0}^{2\\pi}\\int_{0}^{\\infty} \\frac{\\left[r\\exp(-\\alpha r^2)\\right]^2}{\\frac{2\\beta r}{2\\pi}\\exp(-\\beta r^2)} \\,\\mathrm{d}r\\,\\mathrm{d}\\theta = \\frac{\\pi}{\\beta} \\int_{0}^{2\\pi}\\mathrm{d}\\theta \\int_{0}^{\\infty} r \\exp\\left(-(2\\alpha-\\beta)r^2\\right)\\,\\mathrm{d}r\n$$\nThe integral over $\\theta$ yields $2\\pi$. The integral over $r$ is of a familiar form. Let $\\alpha'' = 2\\alpha - \\beta$. The condition $0  \\beta  2\\alpha$ ensures $\\alpha''0$.\n$$\n\\mathbb{E}[W_{\\mathrm{P}}^2] = \\frac{\\pi}{\\beta} \\cdot 2\\pi \\cdot \\int_{0}^{\\infty} r \\exp(-\\alpha'' r^2) \\,\\mathrm{d}r = \\frac{2\\pi^2}{\\beta} \\left[ -\\frac{1}{2\\alpha''} \\exp(-\\alpha'' r^2) \\right]_0^\\infty = \\frac{2\\pi^2}{\\beta} \\frac{1}{2\\alpha''} = \\frac{\\pi^2}{\\beta(2\\alpha-\\beta)}\n$$\nThe variance is then:\n$$\n\\operatorname{Var}(W_{\\mathrm{P}}) = \\mathbb{E}[W_{\\mathrm{P}}^2] - \\left(\\frac{\\pi}{\\alpha}\\right)^2 = \\frac{\\pi^2}{\\beta(2\\alpha-\\beta)} - \\frac{\\pi^2}{\\alpha^2} = \\pi^2\\left(\\frac{\\alpha^2 - \\beta(2\\alpha-\\beta)}{\\alpha^2\\beta(2\\alpha-\\beta)}\\right) = \\pi^2 \\frac{(\\alpha-\\beta)^2}{\\alpha^2\\beta(2\\alpha-\\beta)}\n$$\n\n**3. The Ratio $\\rho(\\alpha,\\sigma,\\beta)$**\n\nFinally, we compute the ratio of the two variances. The common factors of $\\pi^2$ and $\\alpha^2$ cancel:\n$$\n\\rho(\\alpha,\\sigma,\\beta) = \\frac{\\operatorname{Var}(W_{\\mathrm{C}})}{\\operatorname{Var}(W_{\\mathrm{P}})} = \\frac{\\frac{(2\\alpha\\sigma^2 - 1)^2}{4\\alpha\\sigma^2 - 1}}{\\frac{(\\alpha-\\beta)^2}{\\beta(2\\alpha-\\beta)}} = \\frac{(2\\alpha\\sigma^2 - 1)^2 \\beta (2\\alpha-\\beta)}{(4\\alpha\\sigma^2 - 1) (\\alpha-\\beta)^2}\n$$\nThis is the final analytical formula. We now apply it to the test cases.\n\nFor $(\\alpha,\\sigma,\\beta) = (1.0, 1.0, 0.7)$:\n$$ \\rho = \\frac{(2(1.0)(1.0)^2 - 1)^2 (0.7) (2(1.0)-0.7)}{(4(1.0)(1.0)^2 - 1) (1.0-0.7)^2} = \\frac{(1)^2 (0.7)(1.3)}{(3)(0.3)^2} = \\frac{0.91}{0.27} \\approx 3.370370 $$\nFor $(\\alpha,\\sigma,\\beta) = (0.26, 1.0, 0.1)$:\n$$ \\rho = \\frac{(2(0.26)(1.0)^2 - 1)^2 (0.1) (2(0.26)-0.1)}{(4(0.26)(1.0)^2 - 1) (0.26-0.1)^2} = \\frac{(-0.48)^2 (0.1)(0.42)}{(0.04)(0.16)^2} = \\frac{0.0096768}{0.001024} = 9.450000 $$\nFor $(\\alpha,\\sigma,\\beta) = (0.55, 0.7, 1.08)$:\n$$ \\sigma^2 = 0.49 \\implies \\rho = \\frac{(2(0.55)(0.49) - 1)^2 (1.08) (2(0.55)-1.08)}{(4(0.55)(0.49) - 1) (0.55-1.08)^2} = \\frac{(-0.461)^2 (1.08)(0.02)}{(0.078)(-0.53)^2} \\approx 0.209512 $$\nThe implementation will programmatically evaluate these expressions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the ratio of variances for two importance sampling schemes\n    for a given set of test parameters.\n    \"\"\"\n    # Define the test cases from the problem statement as tuples (alpha, sigma, beta).\n    test_cases = [\n        (1.0, 1.0, 0.7),\n        (0.26, 1.0, 0.1),\n        (0.55, 0.7, 1.08),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, sigma, beta = case\n\n        # The analytical formula for the ratio of variances, rho, is:\n        # rho = [ (2*alpha*sigma^2 - 1)^2 * beta * (2*alpha - beta) ] /\n        #       [ (4*alpha*sigma^2 - 1) * (alpha - beta)^2 ]\n        # This formula was derived by analytically computing Var(W_C) and Var(W_P).\n\n        sigma_sq = sigma**2\n\n        # Numerator calculation\n        term_num_1 = (2 * alpha * sigma_sq - 1)**2\n        term_num_2 = beta * (2 * alpha - beta)\n        numerator = term_num_1 * term_num_2\n\n        # Denominator calculation\n        term_den_1 = (4 * alpha * sigma_sq - 1)\n        term_den_2 = (alpha - beta)**2\n        denominator = term_den_1 * term_den_2\n        \n        # In this problem, the conditions on alpha, sigma, and beta ensure\n        # that the denominator is not zero.\n        rho = numerator / denominator\n        results.append(rho)\n\n    # Format the results to six decimal places and create the output string.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "It's time to apply your knowledge to a complete, hands-on coding problem from the realm of random matrix theory. In this exercise, you will estimate the probability of a random matrix being nearly singular—a question that can be framed as a rare-event estimation problem, an ideal scenario for importance sampling. You will build the full pipeline: defining target and proposal distributions in a multidimensional space, generating samples, calculating importance weights, and finally assembling the estimate, reinforcing the practical skills needed for real-world computational physics problems .",
            "id": "2402958",
            "problem": "You are given a random matrix model and an event defined by the determinant magnitude. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be a random matrix with the matrix normal distribution with zero mean, row covariance $U \\in \\mathbb{R}^{2 \\times 2}$, and column covariance $V \\in \\mathbb{R}^{2 \\times 2}$. This is denoted by $A \\sim \\mathcal{MN}\\!\\left(0, U, V\\right)$ and implies that the vectorization $\\operatorname{vec}(A) \\in \\mathbb{R}^{4}$ is multivariate normal with zero mean and covariance $\\Sigma = V \\otimes U$, where $\\otimes$ denotes the Kronecker product. For a given threshold $\\tau \\ge 0$, define the event $E_{\\tau} = \\{A : |\\det(A)| \\le \\tau\\}$ and the target probability\n$$\nP(\\tau) = \\mathbb{P}\\left(|\\det(A)| \\le \\tau\\right) = \\int_{\\mathbb{R}^{4}} \\mathbf{1}\\left(|\\det(A)| \\le \\tau\\right) \\, p_{\\Sigma}\\!\\left(\\operatorname{vec}(A)\\right) \\, d\\operatorname{vec}(A),\n$$\nwhere $p_{\\Sigma}$ is the density of the multivariate normal $\\mathcal{N}\\!\\left(0, \\Sigma\\right)$.\n\nYou will compute $P(\\tau)$ via Monte Carlo importance sampling using a proposal matrix normal distribution $Q$ with zero mean and covariances $U_{q}$ and $V_{q}$, which induces a proposal covariance $\\Sigma_{q} = V_{q} \\otimes U_{q}$ for $\\operatorname{vec}(A)$. The corresponding target covariance is $\\Sigma_{p} = V \\otimes U$.\n\nAll matrices used below are symmetric and positive definite and are specified explicitly as\n$$\nU = \\begin{pmatrix} 1  0.8 \\\\ 0.8  1 \\end{pmatrix}, \\quad\nV = \\begin{pmatrix} 1  0.3 \\\\ 0.3  1 \\end{pmatrix},\n$$\n$$\nU_{q} = \\begin{pmatrix} 1  0.98 \\\\ 0.98  1 \\end{pmatrix}, \\quad\nV_{q} = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}.\n$$\n\nInput-free program requirement. Your program must, without reading any input, estimate $P(\\tau)$ for the following test suite of thresholds, sample sizes, and random seeds:\n- Case A (boundary event with measure essentially zero): $\\tau = 0$, $N = 100000$, seed $= 12345$.\n- Case B (typical intermediate scale): $\\tau = 0.5$, $N = 100000$, seed $= 24680$.\n- Case C (high threshold near certainty): $\\tau = 10$, $N = 100000$, seed $= 13579$.\n\nIn all cases, sampling must be performed from the proposal distribution with covariance $\\Sigma_{q}$ and weighted by the importance ratio of the target with covariance $\\Sigma_{p}$ relative to the proposal. The final answer in each case is the Monte Carlo estimate of $P(\\tau)$ as a real number.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order Case A, Case B, Case C, with each number rounded to exactly six digits after the decimal point (for example, $[0.000000,0.123456,0.999999]$). No additional text should be printed.",
            "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, and objective. It presents a clear computational task in the domain of statistical physics, specifically the estimation of a probability integral using the Monte Carlo importance sampling method. All parameters and distributions are rigorously defined.\n\nThe objective is to compute the probability $P(\\tau) = \\mathbb{P}(|\\det(A)| \\le \\tau)$ for a random matrix $A \\in \\mathbb{R}^{2 \\times 2}$ drawn from a matrix normal distribution $A \\sim \\mathcal{MN}(0, U, V)$. This probability can be expressed as an integral over the space of vectorized matrices $\\mathbb{R}^4$:\n$$\nP(\\tau) = \\int_{\\mathbb{R}^{4}} \\mathbf{1}(|\\det(A)| \\le \\tau) \\, p_{\\Sigma_p}(\\operatorname{vec}(A)) \\, d\\operatorname{vec}(A)\n$$\nwhere $p_{\\Sigma_p}$ is the probability density function (PDF) of the target multivariate normal distribution $\\mathcal{N}(0, \\Sigma_p)$, with covariance $\\Sigma_p = V \\otimes U$. The function $\\mathbf{1}(\\cdot)$ is the indicator function.\n\nDirect Monte Carlo estimation would involve sampling from $p_{\\Sigma_p}$ and calculating the fraction of samples satisfying the event condition. This can be inefficient, especially for small $\\tau$ where the event is rare. The problem specifies the use of importance sampling, which can improve efficiency by sampling from a different proposal distribution, $q$, that concentrates samples in the region of interest. Here, the proposal distribution is another matrix normal, $A_q \\sim \\mathcal{MN}(0, U_q, V_q)$, inducing the PDF $p_{\\Sigma_q}$ for $\\operatorname{vec}(A_q) \\sim \\mathcal{N}(0, \\Sigma_q)$, where $\\Sigma_q = V_q \\otimes U_q$.\n\nThe core principle of importance sampling relies on rewriting the integral as an expectation with respect to the proposal distribution:\n$$\nP(\\tau) = \\mathbb{E}_{p_{\\Sigma_p}}[\\mathbf{1}(|\\det(A)| \\le \\tau)] = \\int \\mathbf{1}(|\\det(A)| \\le \\tau) \\frac{p_{\\Sigma_p}(\\operatorname{vec}(A))}{p_{\\Sigma_q}(\\operatorname{vec}(A))} p_{\\Sigma_q}(\\operatorname{vec}(A)) \\, d\\operatorname{vec}(A)\n$$\nThis leads to the expression:\n$$\nP(\\tau) = \\mathbb{E}_{p_{\\Sigma_q}}\\left[\\mathbf{1}(|\\det(A)| \\le \\tau) \\, w(\\operatorname{vec}(A))\\right]\n$$\nwhere $w(x) = p_{\\Sigma_p}(x) / p_{\\Sigma_q}(x)$ is the importance weight for a state $x = \\operatorname{vec}(A)$.\n\nThe Monte Carlo estimator for $P(\\tau)$ is the sample mean obtained by drawing $N$ independent samples $A^{(i)}$ from the proposal distribution:\n$$\n\\hat{P}_N(\\tau) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}(|\\det(A^{(i)})| \\le \\tau) \\, w(\\operatorname{vec}(A^{(i)}))\n$$\nThe PDF for a $k$-dimensional zero-mean multivariate normal distribution $\\mathcal{N}(0, \\Sigma)$ is given by $p_{\\Sigma}(x) = ((2\\pi)^k \\det(\\Sigma))^{-1/2} \\exp(-\\frac{1}{2} x^T \\Sigma^{-1} x)$. For our $k=4$ case, the importance weight $w(x)$ is the ratio of the target PDF to the proposal PDF:\n$$\nw(x) = \\frac{p_{\\Sigma_p}(x)}{p_{\\Sigma_q}(x)} = \\sqrt{\\frac{\\det(\\Sigma_q)}{\\det(\\Sigma_p)}} \\exp\\left(-\\frac{1}{2} x^T (\\Sigma_p^{-1} - \\Sigma_q^{-1}) x\\right)\n$$\nThe required covariance matrices are constructed using the Kronecker product, $\\Sigma_p = V \\otimes U$ and $\\Sigma_q = V_q \\otimes U_q$. Their inverses are likewise given by $\\Sigma_p^{-1} = V^{-1} \\otimes U^{-1}$ and $\\Sigma_q^{-1} = V_q^{-1} \\otimes U_q^{-1}$.\n\nThe computational algorithm proceeds as follows:\n1.  Define the matrices $U$, $V$, $U_q$, and $V_q$.\n2.  Construct the target and proposal covariance matrices, $\\Sigma_p$ and $\\Sigma_q$, using the Kronecker product.\n3.  Pre-compute the constant part of the weight, $C = \\sqrt{\\det(\\Sigma_q)/\\det(\\Sigma_p)}$, and the matrix for the quadratic form in the exponent, $M = \\Sigma_p^{-1} - \\Sigma_q^{-1}$.\n4.  For each specified test case $(\\tau, N, \\text{seed})$:\n    a. Initialize a random number generator with the given seed for reproducibility.\n    b. Generate $N$ samples $x^{(i)}$ from the proposal distribution $\\mathcal{N}(0, \\Sigma_q)$. Each $x^{(i)}$ is a $4$-element vector.\n    c. For each sample $x^{(i)}$, reconstruct the corresponding matrix $A^{(i)}$ using the convention that $\\operatorname{vec}(A)$ stacks the columns of $A$. Specifically, if $x = [x_0, x_1, x_2, x_3]^T$, then $A = \\begin{pmatrix} x_0  x_2 \\\\ x_1  x_3 \\end{pmatrix}$.\n    d. Calculate the determinant $\\det(A^{(i)}) = x_0^{(i)}x_3^{(i)} - x_1^{(i)}x_2^{(i)}$.\n    e. If $|\\det(A^{(i)})| \\le \\tau$, compute the importance weight $w(x^{(i)}) = C \\exp(-\\frac{1}{2} (x^{(i)})^T M x^{(i)})$ and add it to a running total.\n    f. The estimate for the case is the total accumulated weight divided by $N$.\n\nFor Case A with $\\tau = 0$, the event is $|\\det(A)| = 0$. The set of singular $2 \\times 2$ matrices forms a lower-dimensional manifold in $\\mathbb{R}^4$ and thus has Lebesgue measure zero. The probability of a continuously distributed random variable falling on this set is $0$. Therefore, the theoretical value is $P(0) = 0$, and the Monte Carlo estimate will almost surely be $0$ as well.\n\nThe proposal distribution, with higher correlations in $U_q$ and $V_q$ compared to $U$ and $V$, is chosen to generate matrices that are more frequently \"nearly singular\". This concentrates the sampling effort in the region where $\\det(A)$ is small, which is the most relevant region for calculating the probability, thereby increasing the statistical efficiency of the estimator.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Estimates the probability P(tau) = P(|det(A)| = tau) for a random matrix A\n    using Monte Carlo importance sampling.\n    \"\"\"\n    # Define covariance matrices from the problem statement.\n    # Target distribution covariances\n    U = np.array([[1.0, 0.8], [0.8, 1.0]], dtype=np.float64)\n    V = np.array([[1.0, 0.3], [0.3, 1.0]], dtype=np.float64)\n    # Proposal distribution covariances\n    U_q = np.array([[1.0, 0.98], [0.98, 1.0]], dtype=np.float64)\n    V_q = np.array([[1.0, 0.9], [0.9, 1.0]], dtype=np.float64)\n\n    # Construct the full 4x4 covariance matrices using the Kronecker product.\n    # Sigma_p for the target distribution p(x)\n    Sigma_p = np.kron(V, U)\n    # Sigma_q for the proposal distribution q(x)\n    Sigma_q = np.kron(V_q, U_q)\n\n    # Pre-compute components of the importance weight w(x) = p(x)/q(x).\n    # The weight is w(x) = sqrt(det(Sigma_q)/det(Sigma_p)) * exp(-0.5 * x^T * (inv(Sigma_p) - inv(Sigma_q)) * x)\n    \n    # Determinants\n    det_p = np.linalg.det(Sigma_p)\n    det_q = np.linalg.det(Sigma_q)\n\n    # Inverses\n    inv_Sigma_p = np.linalg.inv(Sigma_p)\n    inv_Sigma_q = np.linalg.inv(Sigma_q)\n\n    # Constant part of the weight calculation\n    C = np.sqrt(det_q / det_p)\n    \n    # Matrix for the quadratic form in the exponent\n    M = inv_Sigma_p - inv_Sigma_q\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (tau, N, seed)\n        (0.0, 100000, 12345),  # Case A\n        (0.5, 100000, 24680),  # Case B\n        (10.0, 100000, 13579)  # Case C\n    ]\n\n    results = []\n    for tau, N, seed in test_cases:\n        # Set up a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Generate N samples from the proposal distribution N(0, Sigma_q).\n        mean = np.zeros(4, dtype=np.float64)\n        samples = rng.multivariate_normal(mean, Sigma_q, size=N, method='cholesky')\n        \n        total_weight_sum = 0.0\n        \n        for x in samples:\n            # The vector x is vec(A), where A = [[x[0], x[2]], [x[1], x[3]]].\n            # Calculate the determinant of the matrix A.\n            det_A = x[0] * x[3] - x[2] * x[1]\n            \n            # Check if the event |det(A)| = tau occurs.\n            if abs(det_A) = tau:\n                # If the event occurs, calculate the importance weight.\n                # The exponent is -0.5 * x^T * M * x\n                quad_form = x.T @ M @ x\n                weight = C * np.exp(-0.5 * quad_form)\n                total_weight_sum += weight\n        \n        # The estimate of the probability is the average of the weights.\n        estimate = total_weight_sum / N\n        results.append(estimate)\n\n    # Final print statement in the exact required format.\n    # Format each result to exactly six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}