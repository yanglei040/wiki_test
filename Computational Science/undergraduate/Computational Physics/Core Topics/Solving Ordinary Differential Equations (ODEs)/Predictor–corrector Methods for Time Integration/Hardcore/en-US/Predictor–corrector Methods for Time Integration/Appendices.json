{
    "hands_on_practices": [
        {
            "introduction": "Before deploying a numerical method, we must verify that it performs as theoretically expected. This practice introduces a fundamental technique for assessing a method's quality: measuring its order of convergence. By applying a simple predictor-corrector scheme to an ordinary differential equation (ODE) with a known solution, you will empirically determine the relationship between step size $h$ and global error $E$, confirming the method's accuracy and revealing how performance can degrade under challenging conditions .",
            "id": "2429754",
            "problem": "Consider the initial-value problem for the autonomous ordinary differential equation (ODE) $y' = y^2$ with initial condition $y(0) = 1$. The unique analytical solution is $y(t) = \\dfrac{1}{1 - t}$ for $t \\in [0,1)$, which exhibits a finite-time singularity at $t = 1$. Let $f(t,y) = y^2$. Consider the following two-stage predictor–corrector time-stepping scheme applied with a constant step size $h > 0$ on a uniform grid $t_n = t_0 + n h$ for $n = 0,1,\\dots,N$, where $N h = t_{\\text{end}} - t_0$ and $t_0 = 0$:\n1. Predictor (explicit Euler): $y^{\\text{pred}}_{n+1} = y_n + h\\, f(t_n, y_n)$.\n2. Corrector (trapezoidal update using the predictor): $y_{n+1} = y_n + \\dfrac{h}{2}\\,\\left[f(t_n, y_n) + f(t_{n+1}, y^{\\text{pred}}_{n+1})\\right]$.\nFor each test case below, you must:\n- Integrate from $t = 0$ to $t = t_{\\text{end}}$ using the above scheme and the specified list of step sizes $h$ (each chosen so that $t_{\\text{end}} / h$ is a positive integer).\n- For each $h$ in the list, compute the absolute global error at $t = t_{\\text{end}}$, defined as $E(h) = \\left|y_{\\text{num}}(t_{\\text{end}};h) - \\dfrac{1}{1 - t_{\\text{end}}}\\right|$, where $y_{\\text{num}}(t_{\\text{end}};h)$ is the numerical approximation produced by the scheme with step size $h$.\n- Using all $(h, E(h))$ pairs within a test case, estimate the observed convergence order $p$ as the slope of the least-squares linear fit to the points $\\left(\\log(h), \\log(E(h))\\right)$, that is, fit $\\log(E(h)) \\approx C + p \\log(h)$ and report $p$ for the test case.\n- Round each reported $p$ to three decimals and report dimensionless values.\n\nTest suite:\n- Case A: $t_{\\text{end}} = 0.1$ with $h \\in \\{0.02, 0.01, 0.005, 0.0025\\}$.\n- Case B: $t_{\\text{end}} = 0.5$ with $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n- Case C: $t_{\\text{end}} = 0.9$ with $h \\in \\{0.1, 0.05, 0.025, 0.0125\\}$.\n- Case D: $t_{\\text{end}} = 0.95$ with $h \\in \\{0.05, 0.025, 0.0125\\}$.\n\nYour program should produce a single line of output containing the four rounded estimates $[p_A, p_B, p_C, p_D]$ as a comma-separated list enclosed in square brackets, for example, $[2.000,2.000,1.950,1.700]$. No additional text should be printed. All angles, units, and quantities are dimensionless; report the final four values as decimals rounded to three digits after the decimal point.",
            "solution": "The supplied problem is a well-defined exercise in computational physics concerning the numerical solution of an ordinary differential equation (ODE). It is scientifically grounded, logically consistent, and contains all necessary information for a unique solution. Therefore, the problem is valid, and we proceed with its resolution.\n\nThe problem requires the analysis of a numerical method applied to the initial value problem (IVP) given by the autonomous ODE:\n$$\ny'(t) = y(t)^2, \\quad y(0) = 1\n$$\nThis equation is a specific case of a Riccati equation. The analytical solution can be found by separation of variables:\n$$\n\\frac{dy}{y^2} = dt \\implies \\int \\frac{dy}{y^2} = \\int dt \\implies -\\frac{1}{y} = t + C\n$$\nUsing the initial condition $y(0)=1$, we find the constant of integration $C$:\n$$\n-\\frac{1}{1} = 0 + C \\implies C = -1\n$$\nThus, the unique analytical solution is:\n$$\ny(t) = \\frac{1}{1 - t}\n$$\nThis solution exists for $t \\in [0, 1)$ and exhibits a finite-time singularity at $t=1$. The function to be integrated, as defined in the problem, is $f(t,y) = y^2$. Since the ODE is autonomous, the function $f$ does not explicitly depend on $t$.\n\nThe numerical scheme to be used is the following two-stage predictor-corrector method:\n1.  **Predictor (Explicit Euler):** A predicted value $y^{\\text{pred}}_{n+1}$ at time $t_{n+1} = t_n + h$ is computed using the forward Euler method:\n    $$\n    y^{\\text{pred}}_{n+1} = y_n + h\\, f(t_n, y_n) = y_n + h\\, y_n^2\n    $$\n2.  **Corrector (Trapezoidal Update):** The final value $y_{n+1}$ is computed by averaging the slope at the beginning of the interval, $f(t_n, y_n)$, and the slope at the end of the interval, where the function is evaluated at the predicted value, $f(t_{n+1}, y^{\\text{pred}}_{n+1})$:\n    $$\n    y_{n+1} = y_n + \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y^{\\text{pred}}_{n+1}) \\right] = y_n + \\frac{h}{2} \\left[ y_n^2 + (y^{\\text{pred}}_{n+1})^2 \\right]\n    $$\nThis scheme is widely known as Heun's method or the improved Euler method. It belongs to the family of second-order Runge-Kutta methods. For a sufficiently smooth solution, the local truncation error of this method is of order $\\mathcal{O}(h^3)$, which results in a global error $E(h)$ at a fixed time $t_{\\text{end}}$ that is of order $\\mathcal{O}(h^2)$.\n\nThe problem requires estimating the observed order of convergence, $p$. For a method with global error $E(h) \\approx K h^p$ for some constant $K$, we can take the natural logarithm of both sides:\n$$\n\\log(E(h)) \\approx \\log(K) + p \\log(h)\n$$\nThis equation has the form of a linear relationship $Y = C + pX$, where $Y = \\log(E(h))$, $X = \\log(h)$, and the intercept is $C = \\log(K)$. The order of convergence $p$ is the slope of this line. We compute $p$ by performing a linear least-squares regression on the set of data points $(\\log(h_i), \\log(E_i))$, where $E_i = E(h_i)$ is the absolute global error at $t=t_{\\text{end}}$ for each step size $h_i$ in a given test case.\n\nThe algorithm to obtain the result for each test case is as follows:\n1.  Define the integration endpoint $t_{\\text{end}}$ and the set of step sizes $\\{h_i\\}$.\n2.  Calculate the exact solution at the endpoint, $y_{\\text{exact}} = (1 - t_{\\text{end}})^{-1}$.\n3.  For each step size $h_i$ in the set:\n    a. Determine the number of steps, $N_i = \\text{integer}(t_{\\text{end}} / h_i)$.\n    b. Initialize the numerical solution at $t_0=0$ with $y_0=1$.\n    c. Iterate $N_i$ times using the predictor-corrector formulas to find the numerical solution $y_{\\text{num}}(t_{\\text{end}}; h_i) = y_{N_i}$.\n    d. Compute the absolute global error $E_i = |y_{\\text{num}}(t_{\\text{end}}; h_i) - y_{\\text{exact}}|$.\n    e. Store the pair $(\\log(h_i), \\log(E_i))$.\n4.  Using all stored pairs for the test case, compute the slope $p$ of the best-fit line through the points.\n5.  Report the value of $p$, rounded to three decimal places.\n\nThe theoretical order of convergence for Heun's method is $p=2$. This is expected to be observed in test cases where $t_{\\text{end}}$ is far from the singularity at $t=1$. As $t_{\\text{end}}$ approaches $1$, the higher-order derivatives of the solution $y(t)$ become very large, which increases the magnitude of the leading error terms. This can lead to a degradation of the method's performance, and the observed order of convergence $p$ may fall below the theoretical value of $2$ for the given range of step sizes.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given problem by calculating the observed convergence order\n    for a predictor-corrector method on a specific ODE for four test cases.\n    \"\"\"\n    \n    # Define the four test cases from the problem statement.\n    test_cases = [\n        {\"t_end\": 0.1, \"h_values\": [0.02, 0.01, 0.005, 0.0025]},\n        {\"t_end\": 0.5, \"h_values\": [0.1, 0.05, 0.025, 0.0125]},\n        {\"t_end\": 0.9, \"h_values\": [0.1, 0.05, 0.025, 0.0125]},\n        {\"t_end\": 0.95, \"h_values\": [0.05, 0.025, 0.0125]}\n    ]\n\n    # The ODE function y' = f(t, y) = y^2. It is autonomous.\n    def f_ode(y):\n        return y * y\n\n    def integrate_ode(t_end, h):\n        \"\"\"\n        Integrates the ODE y'=y^2 from t=0 to t_end using the specified\n        predictor-corrector scheme (Heun's method).\n        \n        Args:\n            t_end (float): The final integration time.\n            h (float): The constant step size.\n\n        Returns:\n            float: The numerical approximation of y(t_end).\n        \"\"\"\n        y = 1.0  # Initial condition y(0) = 1\n        num_steps = int(round(t_end / h))\n\n        for _ in range(num_steps):\n            # Predictor step (Explicit Euler)\n            f_yn = f_ode(y)\n            y_pred = y + h * f_yn\n            \n            # Corrector step (Trapezoidal update)\n            f_y_pred = f_ode(y_pred)\n            y = y + 0.5 * h * (f_yn + f_y_pred)\n            \n        return y\n\n    results = []\n    for case in test_cases:\n        t_end = case[\"t_end\"]\n        h_values = case[\"h_values\"]\n        \n        # Calculate the exact solution at t_end\n        y_exact = 1.0 / (1.0 - t_end)\n        \n        log_h_vals = []\n        log_E_vals = []\n        \n        for h in h_values:\n            # Compute numerical solution\n            y_num = integrate_ode(t_end, h)\n            \n            # Compute absolute global error\n            error = np.abs(y_num - y_exact)\n            \n            # Store log of step size and log of error\n            log_h_vals.append(np.log(h))\n            log_E_vals.append(np.log(error))\n            \n        # Perform a linear least-squares fit to find the convergence order p (the slope)\n        # log(E) = p * log(h) + C\n        p, _ = np.polyfit(log_h_vals, log_E_vals, 1)\n        \n        results.append(f\"{p:.3f}\")\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "One of the most powerful features of predictor-corrector methods is their built-in mechanism for estimating local error, which enables the creation of efficient, adaptive solvers. The discrepancy between the predicted state $\\vec{y}_p$ and the corrected state $\\vec{y}_c$ gives a direct hint about the accuracy of a step, allowing the algorithm to adjust its step size on the fly. In this exercise, you will implement an adaptive step-size controller to navigate the complex dynamics of the Lorenz system, a classic example of chaos where the required precision can vary dramatically .",
            "id": "2429776",
            "problem": "Write a complete, runnable program that advances the solution of the Lorenz initial value problem using a single-step predictor–corrector method with adaptive step-size control based on the difference between the predicted and corrected states. The Lorenz system is the three-dimensional autonomous system\n$$\n\\frac{d}{dt}\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix} = \\begin{bmatrix}\n\\sigma \\left(y - x\\right) \\\\\nx \\left(\\rho - z\\right) - y \\\\\nx y - \\beta z\n\\end{bmatrix},\n$$\nwhere $\\sigma$, $\\rho$, and $\\beta$ are dimensionless parameters, $t$ is dimensionless time, and the state vector is $\\vec{y}(t) = \\begin{bmatrix}x(t) \\\\ y(t) \\\\ z(t)\\end{bmatrix}$. All quantities in this problem are dimensionless.\n\nAt each step from $\\left(t_n,\\vec{y}_n\\right)$ with step size $h$, compute a predicted state $\\vec{y}_p$ and a corrected state $\\vec{y}_c$ using the following explicit predictor–corrector pair:\n$$\n\\vec{y}_p = \\vec{y}_n + h\\,\\vec{f}\\!\\left(\\vec{y}_n\\right), \\quad\n\\vec{y}_c = \\vec{y}_n + \\frac{h}{2}\\left(\\vec{f}\\!\\left(\\vec{y}_n\\right) + \\vec{f}\\!\\left(\\vec{y}_p\\right)\\right),\n$$\nwhere $\\vec{f}(\\vec{y})$ denotes the right-hand side of the Lorenz system. Define the local discrepancy\n$$\ne = \\left\\|\\vec{y}_c - \\vec{y}_p\\right\\|_2.\n$$\nUse the following adaptive step-size control:\n- Accept the step if $e \\le \\text{tol}$, in which case advance to $\\left(t_{n+1},\\vec{y}_{n+1}\\right) = \\left(t_n + h,\\vec{y}_c\\right)$.\n- If the step is rejected $(e > \\text{tol})$, do not advance time and recompute the step with a smaller $h$.\n- On each attempt (whether the previous attempt was accepted or rejected), update the trial step size according to\n$$\nh \\leftarrow h \\cdot \\operatorname{clip}\\!\\left(s \\left(\\frac{\\text{tol}}{\\max(e,\\epsilon)}\\right)^{1/2}, g_{\\min}, g_{\\max}\\right),\n$$\nwhere $s$ is a safety factor, $g_{\\min}$ and $g_{\\max}$ bound the multiplicative change per attempt, $\\epsilon$ is a positive number used to avoid division by zero, and $\\operatorname{clip}(u,a,b)$ denotes the value of $u$ limited to the interval $\\left[a,b\\right]$. On acceptance, enforce $h \\le h_{\\max}$. On all attempts, enforce $h \\ge h_{\\min}$. If $t + h > t_f$, then replace $h$ by $t_f - t$ for that attempt to end exactly at $t_f$.\n\nImplement the method above to advance from $t_0$ to $t_f$ for each test case below, starting with the given initial step size $h_0$, and count only the accepted steps. Use the Euclidean norm $\\left\\|\\cdot\\right\\|_2$ for $e$. Use $\\epsilon = 10^{-30}$ in the step-size update. All computations must be performed in double precision.\n\nTest Suite:\n- Case A (typical chaotic parameters, moderate tolerance):\n  - $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}$; $t_0 = 0$, $t_f = 2$; $\\text{tol} = 10^{-5}$; $h_0 = 10^{-2}$; $h_{\\min} = 10^{-6}$; $h_{\\max} = 5 \\times 10^{-2}$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n- Case B (looser tolerance, larger maximum step):\n  - $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}1 \\\\ 1 \\\\ 1\\end{bmatrix}$; $t_0 = 0$, $t_f = 2$; $\\text{tol} = 10^{-2}$; $h_0 = 10^{-2}$; $h_{\\min} = 10^{-6}$; $h_{\\max} = 1/2$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n- Case C (tighter tolerance, different initial state, shorter horizon):\n  - $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}0 \\\\ 1 \\\\ 1\\end{bmatrix}$; $t_0 = 0$, $t_f = 1/2$; $\\text{tol} = 10^{-7}$; $h_0 = 10^{-3}$; $h_{\\min} = 10^{-7}$; $h_{\\max} = 5 \\times 10^{-3}$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n- Case D (non-chaotic parameter, convergence toward the origin):\n  - $\\sigma = 10$, $\\rho = 0$, $\\beta = 8/3$; $\\vec{y}_0 = \\begin{bmatrix}5 \\\\ 5 \\\\ 5\\end{bmatrix}$; $t_0 = 0$, $t_f = 1$; $\\text{tol} = 10^{-6}$; $h_0 = 10^{-2}$; $h_{\\min} = 10^{-7}$; $h_{\\max} = 10^{-1}$; $s = 0.9$; $g_{\\min} = 0.2$; $g_{\\max} = 2.0$.\n\nRequired final output format:\n- For each case in the order A, B, C, D, produce the three components of the final state $\\vec{y}(t_f) = \\begin{bmatrix}x(t_f) \\\\ y(t_f) \\\\ z(t_f)\\end{bmatrix}$ rounded to six digits after the decimal point, followed by the integer number of accepted steps used to reach $t_f$.\n- Aggregate all results into a single line as a comma-separated list enclosed in square brackets, in the order\n$$\n\\left[x_A, y_A, z_A, N_A, x_B, y_B, z_B, N_B, x_C, y_C, z_C, N_C, x_D, y_D, z_D, N_D\\right],\n$$\nwhere $x_\\bullet$, $y_\\bullet$, $z_\\bullet$ are floats with six digits after the decimal point and $N_\\bullet$ are integers. Print exactly this single line as the only output of the program.",
            "solution": "The Lorenz system is a first-order autonomous system defined by $\\vec{f}(\\vec{y}) = \\begin{bmatrix}\\sigma(y-x) \\\\ x(\\rho - z) - y \\\\ xy - \\beta z\\end{bmatrix}$, with state $\\vec{y}(t) = \\begin{bmatrix}x(t) \\\\ y(t) \\\\ z(t)\\end{bmatrix}$. To perform time integration from $t_0$ to $t_f$ with adaptive step-size control using a predictor–corrector method, we use the explicit Euler step as a predictor paired with the explicit trapezoidal rule (also called the improved Euler or Heun method) as the corrector. At each step, given $\\vec{y}_n$ at time $t_n$ and a tentative step size $h$, the predictor and corrector are defined by\n$$\n\\vec{y}_p = \\vec{y}_n + h\\,\\vec{f}(\\vec{y}_n), \\qquad\n\\vec{y}_c = \\vec{y}_n + \\frac{h}{2}\\big(\\vec{f}(\\vec{y}_n) + \\vec{f}(\\vec{y}_p)\\big).\n$$\nA step-size controller requires a measure of local error. For one-step methods, a standard approach is to use an embedded estimate derived from the difference between two approximations of different orders applied to the same step. Here, the difference $\\vec{y}_c - \\vec{y}_p$ provides an estimate with magnitude proportional to the local truncation error. To see this, expand $\\vec{y}(t_n + h)$ in a Taylor series:\n$$\n\\vec{y}(t_n + h) = \\vec{y}_n + h \\vec{f}(\\vec{y}_n) + \\frac{h^2}{2} \\vec{J}(\\vec{y}_n)\\,\\vec{f}(\\vec{y}_n) + \\mathcal{O}(h^3),\n$$\nwhere $\\vec{J}(\\vec{y})$ is the Jacobian of $\\vec{f}$. The explicit Euler predictor $\\vec{y}_p$ matches terms up to $\\mathcal{O}(h)$, while the trapezoidal corrector $\\vec{y}_c$ matches terms up to $\\mathcal{O}(h^2)$, making the corrected value second-order accurate. The difference $\\vec{y}_c - \\vec{y}_p$ scales like $\\mathcal{O}(h^2)$, hence its Euclidean norm\n$$\ne = \\left\\|\\vec{y}_c - \\vec{y}_p\\right\\|_2\n$$\nserves as a proxy for the local error.\n\nTo keep the local error controlled relative to a user-defined tolerance $\\text{tol}$, we accept a step when $e \\le \\text{tol}$ and reject it otherwise. Classical step-size control uses the relation $e \\propto h^2$ to derive an update for the next trial step size $h_{\\text{new}}$:\n$$\nh_{\\text{new}} = h \\cdot \\left(\\frac{\\text{tol}}{e}\\right)^{1/2}.\n$$\nTo make this robust in practice, we add a safety factor $s \\in (0,1)$ and bound the multiplicative change by $g_{\\min} \\le \\frac{h_{\\text{new}}}{h} \\le g_{\\max}$, giving\n$$\nh \\leftarrow h \\cdot \\operatorname{clip}\\!\\left(s \\left(\\frac{\\text{tol}}{\\max(e,\\epsilon)}\\right)^{1/2}, g_{\\min}, g_{\\max}\\right),\n$$\nwith a very small $\\epsilon > 0$ to avoid division by zero if $e=0$. On each attempt, we enforce $h \\ge h_{\\min}$, and we do not exceed $h_{\\max}$ after acceptance. We also ensure that the final step lands exactly at $t_f$ by shortening the last attempt if $t + h > t_f$.\n\nAlgorithmically, for each case:\n1. Initialize $t \\leftarrow t_0$, $\\vec{y} \\leftarrow \\vec{y}_0$, $h \\leftarrow h_0$, and an accepted-step counter $N \\leftarrow 0$.\n2. While $t < t_f$:\n   - Set $h \\leftarrow \\min\\left(h, t_f - t\\right)$ to avoid overshooting the final time.\n   - Compute the predictor $\\vec{y}_p = \\vec{y} + h\\,\\vec{f}(\\vec{y})$.\n   - Compute the corrector $\\vec{y}_c = \\vec{y} + \\frac{h}{2}\\left(\\vec{f}(\\vec{y}) + \\vec{f}(\\vec{y}_p)\\right)$.\n   - Compute $e = \\left\\|\\vec{y}_c - \\vec{y}_p\\right\\|_2$.\n   - Compute a multiplicative factor $\\phi = s \\left(\\frac{\\text{tol}}{\\max(e,\\epsilon)}\\right)^{1/2}$, then bound it: $\\phi \\leftarrow \\operatorname{clip}(\\phi, g_{\\min}, g_{\\max})$.\n   - If $e \\le \\text{tol}$:\n     - Accept: set $\\vec{y} \\leftarrow \\vec{y}_c$, $t \\leftarrow t + h$, $N \\leftarrow N + 1$.\n     - Update $h \\leftarrow \\min\\left(h \\cdot \\phi, h_{\\max}\\right)$ and enforce $h \\ge h_{\\min}$.\n   - Else:\n     - Reject: update $h \\leftarrow \\max\\left(h \\cdot \\phi, h_{\\min}\\right)$ and retry without advancing $t$ or $\\vec{y}$.\n3. After termination, report $\\vec{y}(t_f)$ and $N$.\n\nThe Lorenz right-hand side is evaluated as\n$$\n\\vec{f}\\!\\left(\\begin{bmatrix}x \\\\ y \\\\ z\\end{bmatrix}\\right) = \\begin{bmatrix}\n\\sigma\\left(y - x\\right) \\\\\nx\\left(\\rho - z\\right) - y \\\\\nx y - \\beta z\n\\end{bmatrix}.\n$$\nAll cases in the test suite use this $\\vec{f}$ with specified parameters, initial conditions, tolerances, and step-size controller constants. The outputs required are the components of the final state at $t_f$ rounded to six digits after the decimal point, followed by the integer number of accepted steps, concatenated for the four cases in order. The program prints these values as a single comma-separated list enclosed in square brackets.\n\nThis approach is grounded in the local error behavior of the predictor–corrector pair and provides a systematic means to adapt the step size to meet the specified tolerance while respecting lower and upper bounds on the step and ensuring exact termination at $t_f$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lorenz_rhs(y, sigma, rho, beta):\n    x, yv, z = y\n    return np.array([\n        sigma * (yv - x),\n        x * (rho - z) - yv,\n        x * yv - beta * z\n    ], dtype=np.float64)\n\ndef integrate_lorenz_pc_adaptive(y0, t0, tf, sigma, rho, beta,\n                                 tol, h0, hmin, hmax,\n                                 safety=0.9, gmin=0.2, gmax=2.0,\n                                 eps=1e-30):\n    y = np.array(y0, dtype=np.float64)\n    t = float(t0)\n    h = float(h0)\n    accepted_steps = 0\n\n    # Ensure bounds are sensible\n    hmin = float(hmin)\n    hmax = float(hmax)\n    safety = float(safety)\n    gmin = float(gmin)\n    gmax = float(gmax)\n\n    # Main adaptive loop\n    while t < tf:\n        # Prevent overshooting the final time\n        if t + h > tf:\n            h = tf - t\n        # Enforce min/max bounds on h for this attempt\n        if h < hmin:\n            h = hmin\n        if h > hmax:\n            h = hmax\n\n        f_n = lorenz_rhs(y, sigma, rho, beta)\n        y_pred = y + h * f_n\n        f_pred = lorenz_rhs(y_pred, sigma, rho, beta)\n        y_corr = y + 0.5 * h * (f_n + f_pred)\n\n        # Error estimate: Euclidean norm of difference\n        err = np.linalg.norm(y_corr - y_pred, ord=2)\n\n        # Compute multiplicative factor for next h\n        # Avoid division by zero using eps\n        ratio = tol / max(err, eps)\n        phi = safety * (ratio ** 0.5)\n        # Clip by growth/shrinkage limits\n        if phi < gmin:\n            phi = gmin\n        elif phi > gmax:\n            phi = gmax\n\n        if err <= tol:\n            # Accept step\n            y = y_corr\n            t = t + h\n            accepted_steps += 1\n            # Update h for next attempt, enforce bounds\n            h = h * phi\n            if h > hmax:\n                h = hmax\n            if h < hmin:\n                h = hmin\n        else:\n            # Reject step: shrink h and retry\n            h = h * phi\n            if h < hmin:\n                h = hmin\n            # Do not advance t or y\n\n        # Safety net to avoid pathological infinite loops:\n        # if h becomes effectively zero and cannot advance time, break.\n        if h <= 0.0:\n            break\n\n    return y, accepted_steps\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (sigma, rho, beta, y0, t0, tf, tol, h0, hmin, hmax, safety, gmin, gmax)\n    test_cases = [\n        # Case A\n        (10.0, 28.0, 8.0/3.0, (1.0, 1.0, 1.0), 0.0, 2.0, 1e-5, 1e-2, 1e-6, 5e-2, 0.9, 0.2, 2.0),\n        # Case B\n        (10.0, 28.0, 8.0/3.0, (1.0, 1.0, 1.0), 0.0, 2.0, 1e-2, 1e-2, 1e-6, 5e-1, 0.9, 0.2, 2.0),\n        # Case C\n        (10.0, 28.0, 8.0/3.0, (0.0, 1.0, 1.0), 0.0, 0.5, 1e-7, 1e-3, 1e-7, 5e-3, 0.9, 0.2, 2.0),\n        # Case D\n        (10.0, 0.0, 8.0/3.0, (5.0, 5.0, 5.0), 0.0, 1.0, 1e-6, 1e-2, 1e-7, 1e-1, 0.9, 0.2, 2.0),\n    ]\n\n    results_str = []\n    for case in test_cases:\n        sigma, rho, beta, y0, t0, tf, tol, h0, hmin, hmax, safety, gmin, gmax = case\n        y_final, n_acc = integrate_lorenz_pc_adaptive(\n            y0=y0, t0=t0, tf=tf,\n            sigma=sigma, rho=rho, beta=beta,\n            tol=tol, h0=h0, hmin=hmin, hmax=hmax,\n            safety=safety, gmin=gmin, gmax=gmax, eps=1e-30\n        )\n        # Append formatted floats and integer as specified\n        results_str.append(f\"{y_final[0]:.6f}\")\n        results_str.append(f\"{y_final[1]:.6f}\")\n        results_str.append(f\"{y_final[2]:.6f}\")\n        results_str.append(str(int(n_acc)))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A numerical method creates a discrete dynamical system meant to approximate a continuous one, but this approximation can sometimes introduce behaviors that are not present in the original problem. This thought-provoking exercise exposes a critical pitfall: the creation of spurious, non-physical fixed points by a standard predictor-corrector scheme. By analyzing a simple linear ODE, you will derive the exact conditions under which the numerical simulation qualitatively fails, providing a crucial lesson on the importance of critically assessing computational results .",
            "id": "2429768",
            "problem": "Design a self-contained computational physics task that demonstrates how a standard predictor–corrector method can introduce spurious fixed points that are not present in the original continuous-time system. Use only the following fundamental bases: the definition of an ordinary differential equation, the definition of an equilibrium (fixed point) of a continuous dynamical system, and the explicit formula of a one-step predictor–corrector method constructed from the explicit Euler predictor and the trapezoidal-rule corrector.\n\nConsider the scalar ordinary differential equation in one dimension given by $\\,\\dfrac{dx}{dt} = f(x)\\,$, where $\\,x \\in \\mathbb{R}\\,$ and $\\,t \\in \\mathbb{R}\\,$ denotes time. An equilibrium (fixed point) of the continuous system is any $\\,x^\\ast\\,$ such that $\\,f(x^\\ast)=0\\,$. A one-step predictor–corrector method with explicit Euler prediction and trapezoidal-rule correction advances from $\\,x_n\\,$ at time $\\,t_n\\,$ to $\\,x_{n+1}\\,$ at time $\\,t_{n+1}=t_n+h\\,$ using the following two stages:\n- Predictor: $\\,x_{\\mathrm{p}} = x_n + h\\, f(x_n)\\,$.\n- Corrector: $\\,x_{n+1} = x_n + \\dfrac{h}{2}\\,\\big(f(x_n) + f(x_{\\mathrm{p}})\\big)\\,$.\n\nYour tasks are:\n1. Construct a concrete example by taking $\\,f(x)=\\lambda\\,x\\,$ with a constant parameter $\\,\\lambda \\in \\mathbb{R}\\,$. Interpret $\\,\\lambda\\,$ as having units of inverse time, that is $\\,\\lambda\\,$ is measured in $\\,\\mathrm{s}^{-1}\\,$, and use a uniform time step $\\,h>0\\,$ measured in $\\,\\mathrm{s}\\,$. Derive from first principles the discrete one-step map $\\,x_{n+1} = \\Phi_{h,\\lambda}(x_n)\\,$ produced by the above predictor–corrector method for this linear $\\,f(x)\\,$.\n2. Using only the definitions of a fixed point of a map and an equilibrium of a differential equation, determine the condition on the parameters $\\,(\\lambda,h)\\,$ under which the discrete map $\\,\\Phi_{h,\\lambda}\\,$ possesses nonzero fixed points $\\,x^\\ast \\neq 0\\,$ even though the continuous system has only the equilibrium $\\,x^\\ast=0\\,$ for $\\,\\lambda \\neq 0\\,$. Explain why such nonzero fixed points of the discrete map are spurious and non-physical relative to the original continuous system.\n3. Implement a program that, for each provided test case $\\,(\\lambda,h)\\,$, decides whether the discrete map $\\,\\Phi_{h,\\lambda}\\,$ has spurious nonzero fixed points. You must use a numerical tolerance of $\\,\\varepsilon=10^{-12}\\,$ when comparing real numbers to determine equality conditions. The decision should be reported as a Boolean: return $\\text{True}$ if there exist spurious nonzero fixed points and $\\text{False}$ otherwise, under the assumption $\\,\\lambda \\neq 0\\,$. If $\\,\\lambda=0\\,$, recall that the continuous system already has every $\\,x\\,$ as an equilibrium, so no spurious non-physical fixed points are introduced; in that case your decision must be $\\text{False}$.\n\nPhysical units:\n- Time step $\\,h\\,$ must be specified in $\\,\\mathrm{s}\\,$.\n- The parameter $\\,\\lambda\\,$ must be specified in $\\,\\mathrm{s}^{-1}\\,$.\n- There is no angle quantity in this problem.\n- There is no requirement to output units; the outputs are Booleans.\n\nTest suite:\nEvaluate your decision function on the following parameter pairs $\\,(\\lambda,h)\\,$:\n- Case $\\,1\\,$: $\\,(\\lambda,h)=(-1,\\;0.1)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,2\\,$: $\\,(\\lambda,h)=(-1,\\;2.0)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,3\\,$: $\\,(\\lambda,h)=(-4,\\;0.5)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,4\\,$: $\\,(\\lambda,h)=(3,\\;0.5)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n- Case $\\,5\\,$: $\\,(\\lambda,h)=(-2.5,\\;0.8)\\,$ with $\\,\\lambda\\,$ in $\\,\\mathrm{s}^{-1}\\,$ and $\\,h\\,$ in $\\,\\mathrm{s}\\,$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of Booleans enclosed in square brackets with no spaces, in the order of the test suite above. For example, a valid output looks like $\\,\\big[\\text{False},\\text{True},\\ldots\\big]\\,$. Your program must not read any input and must not print anything else.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical analysis and dynamical systems, is well-posed, and all its components are objectively and rigorously defined. We shall proceed with the solution.\n\nThe task is to analyze how a specific predictor-corrector numerical integration scheme can introduce spurious, non-physical fixed points that are absent in the original continuous-time dynamical system. We consider the scalar ordinary differential equation (ODE) given by $\\frac{dx}{dt} = f(x)$, with the specific linear form $f(x) = \\lambda x$, where $\\lambda \\in \\mathbb{R}$ is a constant parameter.\n\nFirst, we identify the equilibrium points of the continuous system. An equilibrium, or fixed point, $x^\\ast$ is a state where the system does not evolve, which mathematically corresponds to $\\frac{dx}{dt} = 0$. For our system, this condition is $f(x^\\ast) = \\lambda x^\\ast = 0$. For any nonzero parameter $\\lambda \\neq 0$, the one and only equilibrium of the continuous system is the trivial one, $x^\\ast = 0$. If $\\lambda=0$, the ODE becomes $\\frac{dx}{dt} = 0$, and every point $x \\in \\mathbb{R}$ is an equilibrium.\n\nNext, we derive the discrete one-step map, $x_{n+1} = \\Phi_{h,\\lambda}(x_n)$, that is generated by applying the specified predictor-corrector method to this ODE. The method uses a time step $h > 0$.\n\nThe predictor step is the explicit Euler method:\n$$x_{\\mathrm{p}} = x_n + h f(x_n)$$\nSubstituting $f(x_n) = \\lambda x_n$, we obtain the predicted value $x_{\\mathrm{p}}$:\n$$x_{\\mathrm{p}} = x_n + h (\\lambda x_n) = x_n(1 + h\\lambda)$$\n\nThe corrector step is based on the trapezoidal rule, which averages the slope at the beginning and the predicted end of the interval:\n$$x_{n+1} = x_n + \\frac{h}{2} \\big(f(x_n) + f(x_{\\mathrm{p}})\\big)$$\nSubstituting $f(x_n) = \\lambda x_n$ and $f(x_{\\mathrm{p}}) = \\lambda x_{\\mathrm{p}}$:\n$$x_{n+1} = x_n + \\frac{h\\lambda}{2} \\big(x_n + x_n(1 + h\\lambda)\\big)$$\nFactoring out $x_n$ from the bracketed term gives:\n$$x_{n+1} = x_n + \\frac{h\\lambda}{2} \\big(x_n(1 + (1 + h\\lambda))\\big) = x_n + \\frac{h\\lambda}{2} \\big(x_n(2 + h\\lambda)\\big)$$\nFinally, factoring out $x_n$ from the entire expression yields the discrete map:\n$$x_{n+1} = x_n \\left(1 + \\frac{h\\lambda}{2}(2 + h\\lambda)\\right) = x_n \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right)$$\nSo, the one-step map is $\\Phi_{h,\\lambda}(x_n) = x_n \\left(1 + h\\lambda + \\frac{1}{2}(h\\lambda)^2\\right)$. This is a linear map where the state at step $n+1$ is simply a multiple of the state at step $n$.\n\nWe now seek the fixed points of this discrete map. A fixed point $x^\\ast$ of the map must satisfy the condition $x^\\ast = \\Phi_{h,\\lambda}(x^\\ast)$:\n$$x^\\ast = x^\\ast \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right)$$\nTo find the values of $x^\\ast$ that solve this equation, we rearrange it:\n$$x^\\ast - x^\\ast \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right) = 0$$\n$$x^\\ast \\left(1 - \\left(1 + h\\lambda + \\frac{h^2\\lambda^2}{2}\\right)\\right) = 0$$\n$$x^\\ast \\left(-h\\lambda - \\frac{h^2\\lambda^2}{2}\\right) = 0$$\n$$-x^\\ast h\\lambda \\left(1 + \\frac{h\\lambda}{2}\\right) = 0$$\nThis equation has solutions if either of its factors is zero.\n$1$. The first possibility is $x^\\ast = 0$. This fixed point exists for all values of $h$ and $\\lambda$ and corresponds to the true equilibrium of the original ODE.\n$2$. The second possibility is that the other factor is zero: $-h\\lambda \\left(1 + \\frac{h\\lambda}{2}\\right) = 0$. Since we are given $h > 0$ and are searching for spurious points which only make sense when $\\lambda \\neq 0$, the term $-h\\lambda$ is non-zero. Thus, this condition simplifies to:\n$$1 + \\frac{h\\lambda}{2} = 0$$\n$$h\\lambda = -2$$\n\nIf the condition $h\\lambda = -2$ is met, the equation for the fixed points becomes $-x^\\ast \\cdot 0 = 0$, which is true for *any* value of $x^\\ast \\in \\mathbb{R}$. This means that under the condition $h\\lambda = -2$, every point on the real line is a fixed point of the discrete map. The map itself becomes the identity map: $x_{n+1} = x_n(1 - 2 + \\frac{(-2)^2}{2}) = x_n(1 - 2 + 2) = x_n$.\n\nThese fixed points are deemed spurious because for $\\lambda \\neq 0$, the continuous system possesses only one equilibrium, $x^\\ast = 0$. The numerical method, for the specific combination of parameters satisfying $h\\lambda = -2$, incorrectly implies that *any* initial state $x_0$ is a stationary solution, since $x_n = x_0$ for all $n \\ge 0$. This is a severe qualitative error. For example, if $\\lambda < 0$, the true solution $x(t) = x_0 e^{\\lambda t}$ decays exponentially to $0$. The numerical method with $h\\lambda=-2$ falsely predicts a static system. These infinitely many nonzero fixed points are therefore non-physical artifacts of the discretization.\n\nBased on this analysis, the criterion for the existence of spurious nonzero fixed points (for $\\lambda \\neq 0$) is precisely $h\\lambda = -2$. The case $\\lambda = 0$ is excluded, as the problem statement correctly notes that both the continuous system and the discrete map have all points as equilibria/fixed points, so no spurious fixed points are introduced.\n\nFor implementation, we must use a numerical tolerance $\\varepsilon = 10^{-12}$. The decision logic is as follows:\n- If $|\\lambda| < \\varepsilon$, we consider $\\lambda=0$. The result is $\\text{False}$.\n- Otherwise, we calculate the product $p = h\\lambda$.\n- Spurious nonzero fixed points exist if and only if $|p - (-2)| < \\varepsilon$, which is $|p + 2| < \\varepsilon$. If this condition holds, the result is $\\text{True}$; otherwise, it is $\\text{False}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef has_spurious_fixed_points(lambda_val: float, h: float, epsilon: float = 1e-12) -> bool:\n    \"\"\"\n    Determines if the discrete map has spurious nonzero fixed points.\n\n    Args:\n        lambda_val: The parameter lambda from the ODE dx/dt = lambda * x.\n        h: The time step for the numerical method.\n        epsilon: The numerical tolerance for equality checks.\n\n    Returns:\n        True if spurious nonzero fixed points exist, False otherwise.\n    \"\"\"\n    # According to the problem statement, if lambda is 0, the continuous system\n    # already has every point as an equilibrium. Thus, the fixed points of the\n    # discrete map are not considered spurious.\n    if abs(lambda_val) < epsilon:\n        return False\n\n    # The condition for the existence of spurious nonzero fixed points is derived\n    # to be h * lambda = -2. We check this condition using the provided tolerance.\n    product = h * lambda_val\n    if abs(product + 2.0) < epsilon:\n        return True\n    else:\n        return False\n\ndef solve():\n    \"\"\"\n    Runs the analysis for the test cases provided in the problem statement\n    and prints the results in the required format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (lambda, h).\n    test_cases = [\n        (-1.0, 0.1),    # Case 1\n        (-1.0, 2.0),    # Case 2\n        (-4.0, 0.5),    # Case 3\n        (3.0, 0.5),     # Case 4\n        (-2.5, 0.8)     # Case 5\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_val, h = case\n        result = has_spurious_fixed_points(lambda_val, h)\n        results.append(result)\n\n    # Final print statement in the exact required format: [Boolean,Boolean,...]\n    # The str() of a Python bool ('True', 'False') matches the implicitly desired format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver.\nsolve()\n```"
        }
    ]
}