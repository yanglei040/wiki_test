{
    "hands_on_practices": [
        {
            "introduction": "We begin by exploring the deep connection between the Lanczos algorithm and the theory of orthogonal polynomials. This practice reveals that the three-term recurrence central to the method is mathematically equivalent to the recurrence that defines families of orthogonal polynomials. By constructing a special matrix whose eigenvalues are known through its link to Chebyshev polynomials, you will gain fundamental insight into why the Lanczos process produces a tridiagonal matrix and how its output relates to polynomial approximation theory .",
            "id": "2406049",
            "problem": "You will implement and analyze the Lanczos method for real symmetric eigenproblems using a purposely constructed test matrix whose characteristic polynomial coincides with a classical orthogonal polynomial. The goal is to connect the three-term recurrence underlying orthogonal polynomials to the tridiagonal form produced by the Lanczos process, and to validate the construction numerically through a small test suite.\n\nStart from the following fundamental bases:\n- The eigenvalue problem for a real symmetric matrix: given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, its eigenvalues are real, and the Lanczos method constructs an orthonormal Krylov basis that yields a real symmetric tridiagonal projection $T_m \\in \\mathbb{R}^{m \\times m}$ whose eigenvalues (Ritz values) approximate those of $A$.\n- The Laplace expansion (cofactor expansion) of a determinant, and the fact that determinants of tridiagonal matrices obey a three-term recurrence.\n- The definition of classical orthogonal polynomials on the real line via a three-term recurrence. In particular, for the Chebyshev polynomials of the second kind $U_k(x)$, the recurrence is $U_0(x)=1$, $U_1(x)=2x$, and $U_k(x)=2x\\,U_{k-1}(x)-U_{k-2}(x)$ for $k \\ge 2$.\n\nConstruct the $n \\times n$ real symmetric tridiagonal matrix $A_n$ with $0$ on the main diagonal and $1$ on the first sub- and super-diagonals, that is,\n$$\nA_n = \\begin{bmatrix}\n0 & 1 & 0 & \\cdots & 0 \\\\\n1 & 0 & 1 & \\ddots & \\vdots \\\\\n0 & 1 & 0 & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & 1 \\\\\n0 & \\cdots & 0 & 1 & 0\n\\end{bmatrix}.\n$$\nFrom the determinant expansion for tridiagonal matrices and no other special-purpose formulas, derive that the characteristic polynomial of $A_n$ satisfies a three-term recurrence that matches that of the Chebyshev polynomials of the second kind evaluated at a scaled argument. Conclude the exact identity of the characteristic polynomial of $A_n$ with a suitable scaling of $U_n(x)$.\n\nImplement the Lanczos method for a real symmetric matrix $A$ and an initial vector $v_0 \\in \\mathbb{R}^n$ with $v_0 \\ne 0$. The algorithm must generate orthonormal vectors $v_1,\\dots,v_m$ spanning the Krylov subspace of order $m$, and the tridiagonal matrix $T_m$ with diagonal entries $\\alpha_j$ and sub/super-diagonal entries $\\beta_j$ produced by the three-term recurrence. Use only standard linear algebra operations and ensure numerical robustness for breakdown checks.\n\nYour program must do the following, for each specified test case:\n\n- Build $A_n$ as above.\n- Run $m$ iterations of Lanczos starting from the given $v_0$, producing $T_m$ with entries $\\{\\alpha_j\\}_{j=1}^m$ and $\\{\\beta_j\\}_{j=1}^{m-1}$.\n- For selected comparisons, compute the exact eigenvalues of $A_n$ using the identity between the characteristic polynomial of $A_n$ and the Chebyshev polynomials of the second kind. Use these exact eigenvalues to define quantitative error measures for the Ritz values (eigenvalues of $T_m$).\n- Where required, evaluate the characteristic polynomial of a tridiagonal matrix $T_m$ at real arguments $\\lambda$ using the determinant three-term recurrence $D_0(\\lambda)=1$, $D_1(\\lambda)=\\lambda-\\alpha_1$, and $D_k(\\lambda)=(\\lambda-\\alpha_k) D_{k-1}(\\lambda)-\\beta_{k-1}^2 D_{k-2}(\\lambda)$ for $k \\ge 2$. Compare this with the evaluation of $U_m(\\lambda/2)$ computed via its defining three-term recurrence.\n\nUse the following test suite; all vectors are in $\\mathbb{R}^n$:\n- Test Case $1$: $n=6$, $m=6$, $v_0 = e_1$ where $e_1$ is the first standard basis vector in $\\mathbb{R}^n$. Define the result as the maximum absolute difference between the Ritz values of $T_m$ and the exact eigenvalues of $A_n$.\n- Test Case $2$: $n=10$, $m=5$, $v_0 = e_1$. Define two quantities: (i) the maximum deviation of the computed $\\alpha_j$ from $0$ and of the computed $\\beta_j$ from $1$, aggregated as the maximum of these two maxima; and (ii) the maximum over a uniform grid of $17$ points in the interval $[-2.5,2.5]$ of the absolute difference between $\\det(\\lambda I - T_m)$ and $U_m(\\lambda/2)$, with both evaluated by three-term recurrences. The result for this test case is the maximum of these two quantities.\n- Test Case $3$: $n=20$, $m=12$, $v_0$ is a deterministic pseudo-random vector with entries drawn from the standard normal distribution with a fixed seed $314159$, then normalized to unit Euclidean norm. Define the result as the absolute difference between the largest Ritz value of $T_m$ and the largest exact eigenvalue of $A_n$.\n- Test Case $4$: $n=8$, $m=1$, $v_0=e_1$. Define the result as the maximum absolute difference, over the set of points $\\{-3,-1,0,1,3\\}$, between $\\det(\\lambda I - T_1)$ and $U_1(\\lambda/2)$.\n\nAll answers are pure numbers; no physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list of decimal numbers enclosed in square brackets, in the order of the four test cases, for example, \"[$r_1,r_2,r_3,r_4$]\". Each $r_j$ should be a floating-point number. You may round results to within an absolute tolerance of $10^{-12}$ for readability. No other output should be printed.",
            "solution": "The problem as stated is scientifically sound, well-posed, and objective. It is a standard exercise in numerical linear algebra, connecting the theory of orthogonal polynomials with the behavior of the Lanczos algorithm. All parameters and objectives are clearly defined. The problem is valid. We proceed with the solution.\n\nThe solution is partitioned into two components: first, the analytical derivation of the identity between the characteristic polynomial of the matrix $A_n$ and the Chebyshev polynomials of the second kind; second, the description of the numerical implementation of the Lanczos method and the evaluation of the specified test cases.\n\n**1. Derivation of the Characteristic Polynomial**\n\nLet $A_n$ be the $n \\times n$ real symmetric tridiagonal matrix defined by $(A_n)_{i,i} = 0$, $(A_n)_{i,i+1} = (A_n)_{i+1,i} = 1$, and all other entries being $0$. The characteristic polynomial of $A_n$ is $p_n(\\lambda) = \\det(\\lambda I_n - A_n)$. The matrix $\\lambda I_n - A_n$ is:\n$$\n\\lambda I_n - A_n = \\begin{bmatrix}\n\\lambda & -1 & 0 & \\cdots & 0 \\\\\n-1 & \\lambda & -1 & \\ddots & \\vdots \\\\\n0 & -1 & \\lambda & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & -1 \\\\\n0 & \\cdots & 0 & -1 & \\lambda\n\\end{bmatrix}\n$$\nLet $p_k(\\lambda)$ denote the determinant of the leading principal $k \\times k$ submatrix of $\\lambda I_n - A_n$, which is $\\lambda I_k - A_k$.\nFor $k \\ge 2$, we can compute $p_k(\\lambda)$ using the Laplace (cofactor) expansion along the last row. This yields:\n$$\np_k(\\lambda) = \\lambda \\cdot p_{k-1}(\\lambda) - (-1) \\cdot \\det(M_{k,k-1})\n$$\nwhere $M_{k,k-1}$ is the submatrix obtained by removing row $k$ and column $k-1$. The matrix $M_{k,k-1}$ is an upper triangular matrix with diagonal entries $-1, -1, ..., -1$ ($k-2$ times) and $p_{k-2}(\\lambda)$ in an off-diagonal block structure. More simply, by expanding along the last column of $\\lambda I_k - A_k$:\n$$\np_k(\\lambda) = \\lambda \\cdot p_{k-1}(\\lambda) - (-1) \\cdot \\det\\begin{pmatrix} \\lambda I_{k-2}-A_{k-2} & \\mathbf{0} \\\\ \\mathbf{c}^T & -1 \\end{pmatrix}\n$$\nThe determinant of this block lower triangular matrix is $-p_{k-2}(\\lambda)$. A more direct expansion on the last column gives the terms coming from entries $(k,k)$ and $(k-1,k)$.\nThe entry at $(k,k)$ is $\\lambda$, giving the term $\\lambda \\cdot p_{k-1}(\\lambda)$.\nThe entry at $(k-1,k)$ is $-1$. Its cofactor is $(-1)^{(k-1)+k}$ times the determinant of the matrix with row $k-1$ and column $k$ removed. This matrix is block triangular with a determinant of $-1 \\cdot p_{k-2}(\\lambda)$.\nThus, the contribution is $(-1) \\cdot (-1)^{2k-1} \\cdot (-p_{k-2}(\\lambda)) = -p_{k-2}(\\lambda)$.\nThe full recurrence relation is:\n$$\np_k(\\lambda) = \\lambda p_{k-1}(\\lambda) - p_{k-2}(\\lambda) \\quad \\text{for } k \\ge 2.\n$$\nWe establish the base cases:\n$p_1(\\lambda) = \\det([\\lambda]) = \\lambda$.\n$p_2(\\lambda) = \\det \\begin{pmatrix} \\lambda & -1 \\\\ -1 & \\lambda \\end{pmatrix} = \\lambda^2 - 1$.\nTo initiate the recurrence, we require a value for $p_0(\\lambda)$. Using the recurrence for $k=2$: $p_2(\\lambda) = \\lambda p_1(\\lambda) - p_0(\\lambda)$, which implies $\\lambda^2 - 1 = \\lambda(\\lambda) - p_0(\\lambda)$, so we must define $p_0(\\lambda) = 1$.\n\nNow, consider the Chebyshev polynomials of the second kind, $U_k(x)$, defined by the recurrence:\n$U_0(x)=1$, $U_1(x)=2x$, and $U_k(x) = 2x U_{k-1}(x) - U_{k-2}(x)$ for $k \\ge 2$.\nLet us make the substitution $\\lambda = 2x$, so $x = \\lambda/2$. The recurrence for $p_k(\\lambda)$ becomes $p_k(2x) = 2x p_{k-1}(2x) - p_{k-2}(2x)$.\nThis is identical in form to the recurrence for $U_k(x)$. We check if the initial conditions for the sequence $\\{p_k(2x)\\}$ match those for $\\{U_k(x)\\}$.\n- For $k=0$: $p_0(2x) = 1$, which matches $U_0(x)=1$.\n- For $k=1$: $p_1(2x) = 2x$, which matches $U_1(x)=2x$.\nSince the recurrence relations and the first two terms are identical, we conclude by induction that $p_k(2x) = U_k(x)$ for all $k \\ge 0$. Substituting $x=\\lambda/2$ back, we have established the identity for the characteristic polynomial of $A_k$:\n$$\np_k(\\lambda) = U_k(\\lambda/2).\n$$\nThe eigenvalues of $A_n$ are the roots of $p_n(\\lambda) = 0$, which are the roots of $U_n(\\lambda/2)=0$. The roots of $U_n(x)$ are $x_j = \\cos\\left(\\frac{j\\pi}{n+1}\\right)$ for $j=1, \\dots, n$. Therefore, the exact eigenvalues of $A_n$ are $\\lambda_j = 2x_j = 2\\cos\\left(\\frac{j\\pi}{n+1}\\right)$ for $j=1, \\dots, n$.\n\n**2. Numerical Implementation and Analysis**\n\nThe implementation consists of four main components:\n1.  A function to construct the matrix $A_n$ for a given dimension $n$.\n2.  An implementation of the Lanczos algorithm for a real symmetric matrix $A$, a starting vector $v_0$, and a number of iterations $m$. The algorithm generates the coefficients $\\{\\alpha_j\\}_{j=1}^m$ and $\\{\\beta_j\\}_{j=1}^{m-1}$ of the tridiagonal matrix $T_m$. The process starts by normalizing the initial vector $v_0$ to obtain the first Lanczos vector $q_1$.\n3.  A function to evaluate the characteristic polynomial $\\det(\\lambda I_k - T_k)$ of a given tridiagonal matrix $T_k$ at a point $\\lambda$, using the three-term recurrence $D_k(\\lambda)=(\\lambda-\\alpha_k) D_{k-1}(\\lambda)-\\beta_{k-1}^2 D_{k-2}(\\lambda)$, with initial conditions $D_0(\\lambda)=1$ and $D_1(\\lambda)=\\lambda-\\alpha_1$.\n4.  A function to evaluate the Chebyshev polynomial $U_k(x)$ at a point $x$ using its defining recurrence.\n\nThese components are used to solve the four test cases specified.\n\n- For Test Case $1$ ($n=6, m=6, v_0=e_1$), the Lanczos algorithm on $(A_n, e_1)$ for $n$ steps theoretically yields $T_n=A_n$. Thus, the Ritz values (eigenvalues of $T_6$) should match the exact eigenvalues of $A_6$ to within machine precision.\n- For Test Case $2$ ($n=10, m=5, v_0=e_1$), the algorithm generates the leading principal $5 \\times 5$ submatrix of $A_{10}$, which is $A_5$. Thus, the coefficients $\\alpha_j$ must be $0$ and $\\beta_j$ must be $1$. The characteristic polynomial of $T_5 = A_5$ is exactly $U_5(\\lambda/2)$. The specified error quantities should both be near zero.\n- For Test Case $3$ ($n=20, m=12, v_0$ is a random vector), the Lanczos algorithm provides an approximation. The largest Ritz value of $T_{12}$ is expected to be a very good approximation to the largest eigenvalue of $A_{20}$, as the method converges fastest for extremal eigenvalues.\n- For Test Case $4$ ($n=8, m=1, v_0=e_1$), a single step of the Lanczos method is performed. The resulting matrix is $T_1 = [\\alpha_1]$. The calculation gives $\\alpha_1 = v_1^T A v_1 = e_1^T A_8 e_1 = 0$. So $T_1=[0]$ and its characteristic polynomial is $\\det(\\lambda I_1 - T_1) = \\lambda$. This is identical to $U_1(\\lambda/2) = 2(\\lambda/2) = \\lambda$. The difference is analytically zero.\n\nThe final code implements these procedures and computes the required results for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the Lanczos method for a specific real symmetric matrix\n    whose characteristic polynomial corresponds to Chebyshev polynomials.\n    \"\"\"\n\n    def build_A(n):\n        \"\"\"Builds the n x n test matrix A_n.\"\"\"\n        A = np.zeros((n, n), dtype=float)\n        A += np.diag(np.ones(n - 1), k=1)\n        A += np.diag(np.ones(n - 1), k=-1)\n        return A\n\n    def lanczos_gvl(A, v_start, m):\n        \"\"\"\n        Performs m iterations of the Lanczos algorithm.\n        Based on algorithm 10.1.1 from Golub & Van Loan, \"Matrix Computations\" 4th ed.\n        \n        Args:\n            A (np.ndarray): The symmetric matrix.\n            v_start (np.ndarray): The starting vector.\n            m (int): The number of iterations.\n            \n        Returns:\n            tuple[np.ndarray, np.ndarray]: (alphas, betas) for the tridiagonal matrix T_m.\n        \"\"\"\n        n = A.shape[0]\n        q_prev = np.zeros(n, dtype=float)\n        r = v_start.copy()\n        beta = np.linalg.norm(r)\n\n        alphas = np.zeros(m, dtype=float)\n        betas = np.zeros(m - 1, dtype=float)\n\n        for j in range(m):\n            if beta < 1e-15:  # Breakdown check\n                # For this problem, breakdown is not expected.\n                # If it occurs, we would return truncated T matrix coeffs.\n                # Here we assume it runs for m steps successfully.\n                alphas = alphas[:j]\n                betas = betas[:j - 1] if j > 0 else np.array([])\n                break\n\n            q = r / beta\n            u = A @ q\n            alpha = np.dot(q, u)\n            alphas[j] = alpha\n\n            r = u - alpha * q - beta * q_prev\n\n            if j < m - 1:\n                beta_next = np.linalg.norm(r)\n                betas[j] = beta_next\n                beta = beta_next\n            \n            q_prev = q\n\n        return alphas, betas\n\n    def get_exact_eigenvalues(n):\n        \"\"\"Computes the exact eigenvalues of the matrix A_n.\"\"\"\n        j = np.arange(1, n + 1, dtype=float)\n        eigvals = 2 * np.cos(j * np.pi / (n + 1))\n        return np.sort(eigvals)\n\n    def eval_det_poly(alphas, betas, lam):\n        \"\"\"\n        Evaluates the characteristic polynomial det(lambda*I - T) of a tridiagonal matrix T\n        using the three-term recurrence for determinants.\n        \"\"\"\n        m = len(alphas)\n        if m == 0:\n            return 1.0\n        \n        d_prev = 1.0  # D_0(lambda)\n        d_curr = lam - alphas[0]  # D_1(lambda)\n\n        for k in range(1, m):\n            beta_sq = betas[k-1]**2\n            d_next = (lam - alphas[k]) * d_curr - beta_sq * d_prev\n            d_prev = d_curr\n            d_curr = d_next\n            \n        return d_curr\n\n    def eval_U(k, x):\n        \"\"\"\n        Evaluates the Chebyshev polynomial of the second kind U_k(x) via its recurrence relation.\n        \"\"\"\n        if k == 0:\n            return 1.0\n        if k == 1:\n            return 2.0 * x\n        \n        u_prev = 1.0  # U_0\n        u_curr = 2.0 * x  # U_1\n        \n        for _ in range(2, k + 1):\n            u_next = 2.0 * x * u_curr - u_prev\n            u_prev = u_curr\n            u_curr = u_next\n            \n        return u_curr\n\n    test_cases = [\n        {'n': 6, 'm': 6, 'v0_type': 'e1'},\n        {'n': 10, 'm': 5, 'v0_type': 'e1'},\n        {'n': 20, 'm': 12, 'v0_type': 'random', 'seed': 314159},\n        {'n': 8, 'm': 1, 'v0_type': 'e1'}\n    ]\n    results = []\n\n    # Test Case 1\n    params = test_cases[0]\n    n, m = params['n'], params['m']\n    A = build_A(n)\n    v0 = np.zeros(n)\n    v0[0] = 1.0\n    alphas, betas = lanczos_gvl(A, v0, m)\n    T_m = np.diag(alphas) + np.diag(betas, k=1) + np.diag(betas, k=-1)\n    ritz_values = np.linalg.eigvalsh(T_m)\n    exact_eigvals = get_exact_eigenvalues(n)\n    res1 = np.max(np.abs(ritz_values - exact_eigvals))\n    results.append(res1)\n\n    # Test Case 2\n    params = test_cases[1]\n    n, m = params['n'], params['m']\n    A = build_A(n)\n    v0 = np.zeros(n)\n    v0[0] = 1.0\n    alphas, betas = lanczos_gvl(A, v0, m)\n    \n    max_dev_alpha = np.max(np.abs(alphas)) if len(alphas) > 0 else 0.0\n    max_dev_beta = np.max(np.abs(betas - 1.0)) if len(betas) > 0 else 0.0\n    q1 = max(max_dev_alpha, max_dev_beta)\n    \n    grid = np.linspace(-2.5, 2.5, 17)\n    max_diff_poly = 0.0\n    for lam in grid:\n        det_val = eval_det_poly(alphas, betas, lam)\n        cheby_val = eval_U(m, lam / 2.0)\n        max_diff_poly = max(max_diff_poly, np.abs(det_val - cheby_val))\n    \n    res2 = max(q1, max_diff_poly)\n    results.append(res2)\n\n    # Test Case 3\n    params = test_cases[2]\n    n, m, seed = params['n'], params['m'], params['seed']\n    A = build_A(n)\n    rng = np.random.default_rng(seed)\n    v0_unnormalized = rng.standard_normal(n)\n    v0 = v0_unnormalized / np.linalg.norm(v0_unnormalized)\n    \n    alphas, betas = lanczos_gvl(A, v0, m)\n    T_m = np.diag(alphas) + np.diag(betas, k=1) + np.diag(betas, k=-1)\n    ritz_values = np.linalg.eigvalsh(T_m)\n    exact_eigvals = get_exact_eigenvalues(n)\n    \n    max_ritz = np.max(ritz_values)\n    max_exact = np.max(exact_eigvals)\n    res3 = np.abs(max_ritz - max_exact)\n    results.append(res3)\n\n    # Test Case 4\n    params = test_cases[3]\n    n, m = params['n'], params['m']\n    A = build_A(n)\n    v0 = np.zeros(n)\n    v0[0] = 1.0\n    alphas, betas = lanczos_gvl(A, v0, m)\n    \n    test_points = np.array([-3.0, -1.0, 0.0, 1.0, 3.0])\n    max_diff_poly = 0.0\n    for lam in test_points:\n        det_val = eval_det_poly(alphas, betas, lam)\n        cheby_val = eval_U(m, lam / 2.0)\n        max_diff_poly = max(max_diff_poly, np.abs(det_val - cheby_val))\n    \n    res4 = max_diff_poly\n    results.append(res4)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "Having explored its theoretical underpinnings, we now apply the Lanczos method to a crucial task in numerical analysis: estimating the condition number of a large, sparse matrix. This exercise demonstrates the power of the method to efficiently find the extremal eigenvalues, $\\lambda_{\\max}$ and $\\lambda_{\\min}$, which are essential for assessing matrix stability and the potential difficulty of solving linear systems. You will work with matrices that are staples in computational physics, providing a direct link between the algorithm and practical problem-solving .",
            "id": "2406053",
            "problem": "You are given a collection of real symmetric sparse matrices and are asked to estimate the condition number in the Euclidean norm for each matrix. For a symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$, the $2$-norm condition number is defined by\n$$\n\\kappa(A) \\equiv \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)},\n$$\nwhere $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ denote, respectively, the largest and smallest eigenvalues of $A$. For each matrix defined in the test suite below, compute an estimate of $\\kappa(A)$ by approximating $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$.\n\nAll matrices in this problem are real, symmetric, and positive definite, and are defined entirely by the parameters provided. There are no physical units required and no angles are involved.\n\nTest Suite (matrices to be constructed in this exact order):\n- Case $1$ (one-dimensional Dirichlet Laplacian): Let $n = 200$. Define $A \\in \\mathbb{R}^{n \\times n}$ by a tridiagonal structure with diagonal entries equal to $2$ and first sub- and super-diagonal entries equal to $-1$. Formally, $A = \\mathrm{tridiag}(-1, 2, -1)$ of size $n$.\n- Case $2$ (diagonal with logarithmically spaced spectrum): Let $n = 300$. Define $A = \\mathrm{diag}(d_1,\\dots,d_n)$ with diagonal entries $d_k = 10^{x_k}$, where $x_k$ are linearly spaced between $-2$ and $0$ for $k = 1,\\dots,n$.\n- Case $3$ (two-dimensional Dirichlet Laplacian via Kronecker sum): Let $N = 20$ and $n = N^2$. Define $L \\in \\mathbb{R}^{N \\times N}$ as in Case $1$ with diagonal entries equal to $2$ and first sub- and super-diagonals equal to $-1$. Let $I$ denote the $N \\times N$ identity. Define $A \\in \\mathbb{R}^{n \\times n}$ by the Kronecker sum\n$$\nA = I \\otimes L + L \\otimes I,\n$$\nwhere $\\otimes$ denotes the Kronecker product.\n- Case $4$ (scaled identity): Let $n = 300$ and $c = 3.5$. Define $A = c I_n$, where $I_n$ is the $n \\times n$ identity matrix.\n\nYour program must:\n- For each case, construct $A$ exactly as specified.\n- Compute estimates of $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ and then form the estimate of $\\kappa(A)$ as $\\lambda_{\\max}(A)/\\lambda_{\\min}(A)$.\n- For each case, produce a floating-point value equal to the estimated condition number rounded to exactly $6$ decimal places.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, with the case order matching the order above. For example, an output line should look like\n[`$\\kappa_1, \\kappa_2, \\kappa_3, \\kappa_4$`]\nwhere each $\\kappa_i$ is a decimal rounded to exactly $6$ places (e.g., $[123.456789,100.000000,177.882001,1.000000]$).",
            "solution": "The problem statement is subjected to validation and found to be valid. It is scientifically grounded in established principles of linear algebra, well-posed with all necessary parameters for matrix construction provided, and expressed objectively without ambiguity. The premise that all specified matrices are real, symmetric, and positive definite is correct. This ensures that the $2$-norm condition number $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$ is well-defined, since $\\lambda_{\\min}(A) > 0$. We proceed with the solution.\n\nThe objective is to compute the condition number $\\kappa(A)$ for four specified matrices. For a symmetric matrix $A$, this requires finding its largest and smallest eigenvalues, $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$. The matrices in question vary in structure; some are large and sparse, while others have trivial eigenspectra. The solution approach will be tailored to each case for correctness and efficiency. For large sparse matrices, computation of extremal eigenvalues is performed using an iterative method based on the Lanczos algorithm, which is the state-of-the-art for this task. The `scipy.sparse.linalg.eigsh` function provides such an implementation.\n\nCase $1$: One-dimensional Dirichlet Laplacian\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=200$ is defined as $A = \\mathrm{tridiag}(-1, 2, -1)$. This is a sparse, symmetric matrix. Due to its size, we avoid direct diagonalization. We compute the extremal eigenvalues using the Lanczos method.\n- $\\lambda_{\\max}(A)$ is found by seeking the eigenvalue with the largest algebraic value.\n- $\\lambda_{\\min}(A)$ is found by seeking the eigenvalue with the smallest algebraic value.\nThe matrix $A$ is constructed as a sparse matrix. Then, two calls to an iterative eigensolver are made, one for $\\lambda_{\\max}(A)$ and one for $\\lambda_{\\min}(A)$. The condition number is the ratio of these computed values.\n\nCase $2$: Diagonal with logarithmically spaced spectrum\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=300$ is diagonal, with entries $d_k = 10^{x_k}$ where $x_k$ are linearly spaced in the interval $[-2, 0]$. The eigenvalues of a diagonal matrix are its diagonal entries. Therefore, no numerical estimation is required.\n- The values $x_k$ range from $-2$ to $0$. Since $10^x$ is a monotonically increasing function, the extremal eigenvalues correspond to the extremal values of $x_k$.\n- $\\lambda_{\\min}(A) = \\min_k(d_k) = 10^{\\min(x_k)} = 10^{-2} = 0.01$.\n- $\\lambda_{\\max}(A) = \\max_k(d_k) = 10^{\\max(x_k)} = 10^0 = 1$.\n- The condition number is exactly $\\kappa(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\frac{1}{0.01} = 100$.\n\nCase $3$: Two-dimensional Dirichlet Laplacian\nThe matrix $A$ is of size $n \\times n$ where $n=N^2$ and $N=20$. It is defined by the Kronecker sum $A = I \\otimes L + L \\otimes I$, where $L$ is the $N \\times N$ 1D Laplacian from Case $1$ and $I$ is the $N \\times N$ identity matrix. The resulting matrix $A$ is of size $400 \\times 400$, and is sparse and symmetric. As in Case $1$, we compute the extremal eigenvalues $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ using the Lanczos algorithm. The matrix $A$ is constructed using sparse matrix routines for the identity, the 1D Laplacian, and the Kronecker product.\n\nCase $4$: Scaled identity\nThe matrix $A \\in \\mathbb{R}^{n \\times n}$ with $n=300$ is defined as $A = c I_n$, where $I_n$ is the identity matrix and $c = 3.5$. This matrix is diagonal. All of its eigenvalues are equal to the constant $c$.\n- $\\lambda_{\\min}(A) = 3.5$.\n- $\\lambda_{\\max}(A) = 3.5$.\n- The condition number is exactly $\\kappa(A) = \\frac{3.5}{3.5} = 1$.\nNo numerical estimation is needed.\n\nThe final results are computed based on these principles and rounded to $6$ decimal places as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, identity, kron\nfrom scipy.sparse.linalg import eigsh\n\ndef solve():\n    \"\"\"\n    Computes condition number estimates for a suite of test matrices.\n    \"\"\"\n    test_cases_params = [\n        {'type': 'laplacian_1d', 'n': 200},\n        {'type': 'diag_log', 'n': 300},\n        {'type': 'laplacian_2d', 'N': 20},\n        {'type': 'scaled_identity', 'n': 300, 'c': 3.5}\n    ]\n\n    results = []\n    for params in test_cases_params:\n        case_type = params['type']\n        \n        if case_type == 'laplacian_1d':\n            # Case 1: 1D Dirichlet Laplacian\n            n = params['n']\n            # A = tridiag(-1, 2, -1)\n            diagonals = [-1 * np.ones(n - 1), 2 * np.ones(n), -1 * np.ones(n - 1)]\n            A = diags(diagonals, [-1, 0, 1], shape=(n, n), format='csr')\n            \n            # Find largest eigenvalue\n            lambda_max = eigsh(A, k=1, which='LA', return_eigenvectors=False)[0]\n            # Find smallest eigenvalue\n            lambda_min = eigsh(A, k=1, which='SA', return_eigenvectors=False)[0]\n            \n            kappa = lambda_max / lambda_min\n            results.append(kappa)\n\n        elif case_type == 'diag_log':\n            # Case 2: Diagonal matrix with logarithmically spaced spectrum\n            # Eigenvalues are the diagonal entries.\n            # d_k = 10^x_k, with x_k in [-2, 0].\n            # Smallest eigenvalue is 10^-2, largest is 10^0.\n            lambda_min = 10**(-2.0)\n            lambda_max = 10**(0.0)\n            \n            kappa = lambda_max / lambda_min\n            results.append(kappa)\n\n        elif case_type == 'laplacian_2d':\n            # Case 3: 2D Dirichlet Laplacian via Kronecker sum\n            N = params['N']\n            n = N * N\n            \n            # 1D Laplacian L of size N\n            diagonals_L = [-1 * np.ones(N - 1), 2 * np.ones(N), -1 * np.ones(N - 1)]\n            L = diags(diagonals_L, [-1, 0, 1], shape=(N, N), format='csr')\n            \n            # Identity I of size N\n            I = identity(N, format='csr')\n            \n            # Kronecker sum A = I kron L + L kron I\n            A = kron(I, L) + kron(L, I)\n            \n            # Find largest eigenvalue\n            lambda_max = eigsh(A, k=1, which='LA', return_eigenvectors=False)[0]\n            # Find smallest eigenvalue. Use a small shift sigma=0 for stability with some solvers,\n            # though 'SA' is generally robust for positive definite matrices.\n            lambda_min = eigsh(A, k=1, which='SA', return_eigenvectors=False)[0]\n\n            kappa = lambda_max / lambda_min\n            results.append(kappa)\n            \n        elif case_type == 'scaled_identity':\n            # Case 4: Scaled identity matrix\n            # A = c * I, all eigenvalues are c.\n            c = params['c']\n            kappa = c / c\n            results.append(kappa)\n\n    # Format results to exactly 6 decimal places.\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "An iterative algorithm is only as good as its stopping rule, so our final practice addresses this key implementation detail. You will develop and test a robust termination criterion for the Lanczos iteration based on the residual norm of the computed Ritz pairs. This exercise  teaches you how to efficiently monitor convergence and decide when an approximate eigenvalue is \"good enough,\" a critical skill for creating reliable and efficient computational tools.",
            "id": "2406056",
            "problem": "Given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero starting vector $v_{1} \\in \\mathbb{R}^{n}$ with $\\|v_{1}\\|_{2} = 1$, consider the $k$-dimensional Krylov subspace $\\mathcal{K}_{k}(A, v_{1}) = \\mathrm{span}\\{v_{1}, A v_{1}, \\ldots, A^{k-1} v_{1}\\}$. Let $Q_{k} \\in \\mathbb{R}^{n \\times k}$ have orthonormal columns that form a basis of $\\mathcal{K}_{k}(A, v_{1})$, and let $T_{k} = Q_{k}^{\\top} A Q_{k} \\in \\mathbb{R}^{k \\times k}$ be the Rayleigh–Ritz projection of $A$ onto $\\mathcal{K}_{k}(A, v_{1})$. Denote by $(\\theta_{i}^{(k)}, u_{i}^{(k)})$ the eigenpairs of $T_{k}$ with $\\|u_{i}^{(k)}\\|_{2} = 1$, and define the corresponding Ritz vectors in the original space by $y_{i}^{(k)} = Q_{k} u_{i}^{(k)}$ with $\\|y_{i}^{(k)}\\|_{2} = 1$. For each Ritz pair $(\\theta_{i}^{(k)}, y_{i}^{(k)})$, define the residual $r_{i}^{(k)} = A y_{i}^{(k)} - \\theta_{i}^{(k)} y_{i}^{(k)}$ and its norm $\\|r_{i}^{(k)}\\|_{2}$.\n\nDevelop a robust termination criterion that accepts a Ritz pair $(\\theta_{i}^{(k)}, y_{i}^{(k)})$ when the inequality\n$$\n\\|r_{i}^{(k)}\\|_{2} \\leq \\max\\big(\\mathrm{atol}, \\ \\mathrm{rtol} \\cdot \\max(1, |\\theta_{i}^{(k)}|)\\big)\\, \\|A\\|_{2}\n$$\nholds, where $\\mathrm{rtol} > 0$ and $\\mathrm{atol} \\ge 0$ are user-specified tolerances and $\\|A\\|_{2}$ denotes the spectral norm (the largest singular value, which for symmetric $A$ equals the largest absolute eigenvalue). For a given target specification, let $\\mathcal{I}_{k}$ denote the index set of the $r$ Ritz values chosen according to the target rule:\n- If the target is “smallest,” then $\\mathcal{I}_{k}$ comprises the indices of the $r$ algebraically smallest Ritz values among $\\{\\theta_{i}^{(k)}\\}_{i=1}^{k}$.\n- If the target is “largest,” then $\\mathcal{I}_{k}$ comprises the indices of the $r$ algebraically largest Ritz values among $\\{\\theta_{i}^{(k)}\\}_{i=1}^{k}$.\n\nDefine the minimal iteration count $k_{\\mathrm{acc}}$ as the smallest $k \\in \\{1, 2, \\ldots, k_{\\max}\\}$ for which at least $r$ Ritz pairs in the target set $\\mathcal{I}_{k}$ satisfy the acceptance inequality above. If no such $k$ exists up to $k_{\\max}$, define $k_{\\mathrm{acc}} = -1$.\n\nImplement a program that, for each of the following test cases, constructs the specified symmetric matrix $A$, uses the starting vector $v_{1}$ with components $v_{1,j} = 1$ for $j \\in \\{1,\\ldots,n\\}$ normalized to unit $2$-norm, and returns the corresponding value of $k_{\\mathrm{acc}}$.\n\nTest suite:\n- Test case $1$: $A$ is the discrete one-dimensional Dirichlet Laplacian of size $n = 50$, that is, a tridiagonal matrix with diagonal entries $2$ and sub- and super-diagonal entries $-1$. Target: “smallest,” with $r = 2$. Tolerances: $\\mathrm{rtol} = 10^{-8}$, $\\mathrm{atol} = 10^{-12}$. Maximum iteration $k_{\\max} = 50$.\n- Test case $2$: $A$ is the symmetric Toeplitz matrix of size $n = 60$ defined by $A_{ij} = \\exp\\!\\big(-((i-j)/\\sigma)^{2}\\big)$ with $\\sigma = 5$. Target: “largest,” with $r = 1$. Tolerances: $\\mathrm{rtol} = 10^{-6}$, $\\mathrm{atol} = 10^{-10}$. Maximum iteration $k_{\\max} = 60$.\n- Test case $3$: $A = cI$ with $c = 7$ and size $n = 20$. Target: “smallest,” with $r = 1$. Tolerances: $\\mathrm{rtol} = 10^{-12}$, $\\mathrm{atol} = 0$. Maximum iteration $k_{\\max} = 5$.\n- Test case $4$: $A$ is the discrete one-dimensional Dirichlet Laplacian of size $n = 40$. Target: “smallest,” with $r = 1$. Tolerances: $\\mathrm{rtol} = 10^{-14}$, $\\mathrm{atol} = 0$. Maximum iteration $k_{\\max} = 5$.\n\nYour program should produce a single line of output containing the four values $[k_{\\mathrm{acc}}^{(1)}, k_{\\mathrm{acc}}^{(2)}, k_{\\mathrm{acc}}^{(3)}, k_{\\mathrm{acc}}^{(4)}]$ as a comma-separated list enclosed in square brackets, in that order. Each $k_{\\mathrm{acc}}^{(t)}$ must be an integer, with $-1$ indicating that no acceptance occurred within the prescribed $k_{\\max}$ for test case $t$.",
            "solution": "The problem as stated is a well-defined exercise in computational physics and numerical linear algebra. It concerns the implementation of the Lanczos algorithm for finding eigenvalues of real symmetric matrices, coupled with a practical termination criterion based on the norm of the residual. All parameters and matrices are specified unambiguously, and the problem is scientifically sound, resting on foundational principles of matrix computations. Therefore, the problem is valid and we shall proceed with a complete solution.\n\nThe core of the problem is the Lanczos algorithm. For a given real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_1$ with $\\|v_1\\|_2 = 1$, the algorithm iteratively constructs an orthonormal basis $\\{v_1, v_2, \\ldots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_k(A, v_1) = \\mathrm{span}\\{v_1, A v_1, \\ldots, A^{k-1} v_1\\}$. The matrix of these basis vectors is $Q_k = [v_1 | v_2 | \\ldots | v_k] \\in \\mathbb{R}^{n \\times k}$.\n\nThe procedure is governed by a three-term recurrence relation:\n$$\n\\beta_{j+1} v_{j+1} = A v_j - \\alpha_j v_j - \\beta_j v_{j-1}\n$$\nwith initial conditions $v_0 = 0$ and $\\beta_1 = 0$. The coefficients are determined as $\\alpha_j = v_j^\\top A v_j$ and $\\beta_{j+1} = \\|A v_j - \\alpha_j v_j - \\beta_j v_{j-1}\\|_2$. These coefficients form a symmetric tridiagonal matrix $T_k \\in \\mathbb{R}^{k \\times k}$:\n$$\nT_k = Q_k^\\top A Q_k = \\begin{pmatrix}\n\\alpha_1 & \\beta_2 & & \\\\\n\\beta_2 & \\alpha_2 & \\ddots & \\\\\n& \\ddots & \\ddots & \\beta_k \\\\\n& & \\beta_k & \\alpha_k\n\\end{pmatrix}\n$$\nThe eigenpairs $(\\theta_i^{(k)}, u_i^{(k)})$ of this small matrix $T_k$ are known as Ritz pairs. The Ritz values $\\theta_i^{(k)}$ are approximations to the eigenvalues of $A$, and the Ritz vectors $y_i^{(k)} = Q_k u_i^{(k)}$ are approximations to the corresponding eigenvectors of $A$.\n\nA critical component of an efficient implementation is the calculation of the residual norm $\\|r_i^{(k)}\\|_2 = \\|A y_i^{(k)} - \\theta_i^{(k)} y_i^{(k)}\\|_2$. A naive calculation would be computationally expensive. Instead, we exploit a fundamental property of the Lanczos factorization, $A Q_k = Q_k T_k + \\beta_{k+1} v_{k+1} e_k^\\top$, where $e_k$ is the $k$-th canonical basis vector in $\\mathbb{R}^k$. By substituting $y_i^{(k)} = Q_k u_i^{(k)}$ and using the eigen-relation $T_k u_i^{(k)} = \\theta_i^{(k)} u_i^{(k)}$, we derive:\n$$\nA y_i^{(k)} = A (Q_k u_i^{(k)}) = (Q_k T_k + \\beta_{k+1} v_{k+1} e_k^\\top) u_i^{(k)} = Q_k (T_k u_i^{(k)}) + \\beta_{k+1} v_{k+1} (e_k^\\top u_i^{(k)})\n$$\n$$\nA y_i^{(k)} = Q_k (\\theta_i^{(k)} u_i^{(k)}) + \\beta_{k+1} u_{i,k}^{(k)} v_{k+1} = \\theta_i^{(k)} (Q_k u_i^{(k)}) + \\beta_{k+1} u_{i,k}^{(k)} v_{k+1}\n$$\nwhere $u_{i,k}^{(k)}$ is the $k$-th (last) component of the eigenvector $u_i^{(k)}$. The residual vector is therefore $r_i^{(k)} = A y_i^{(k)} - \\theta_i^{(k)} y_i^{(k)} = \\beta_{k+1} u_{i,k}^{(k)} v_{k+1}$. Since the Lanczos vectors are orthonormal ($_2 = 1$), the residual norm simplifies elegantly to:\n$$\n\\|r_i^{(k)}\\|_2 = |\\beta_{k+1}| \\cdot |u_{i,k}^{(k)}|\n$$\nThis formula provides a computationally inexpensive way to check the termination criterion at each iteration $k$.\n\nThe overall algorithm to find the minimal iteration count $k_{\\mathrm{acc}}$ is as follows:\n$1$. For each test case, construct the matrix $A$ and the normalized starting vector $v_1$.\n$2$. Compute the spectral norm $\\|A\\|_2$. For a symmetric matrix, this is $\\max(|\\lambda_{\\min}|, |\\lambda_{\\max}|)$.\n$3$. Iterate $k$ from $1$ to $k_{\\max}$. In each iteration:\n    a. Perform one step of the Lanczos algorithm to compute $\\alpha_k$ and $\\beta_{k+1}$, extending the tridiagonal matrix $T_{k-1}$ to $T_k$.\n    b. Solve the eigenproblem for the $k \\times k$ matrix $T_k$ to obtain its full set of Ritz pairs $\\{(\\theta_i^{(k)}, u_i^{(k)})\\}_{i=1}^k$. For this, we use a specialized and efficient solver for symmetric tridiagonal matrices.\n    c. Identify the $r$ target Ritz pairs according to the 'smallest' or 'largest' criterion specified in the test case.\n    d. For each of these $r$ target pairs, calculate the residual norm using the formula $\\|r_i^{(k)}\\|_2 = |\\beta_{k+1}| |u_{i,k}^{(k)}|$.\n    e. Compare this norm to the threshold $\\tau_i^{(k)} = \\max(\\mathrm{atol}, \\mathrm{rtol} \\cdot \\max(1, |\\theta_i^{(k)}|)) \\cdot \\|A\\|_2$.\n    f. Count the number of target pairs that satisfy $\\|r_i^{(k)}\\|_2 \\leq \\tau_i^{(k)}$.\n$4$. If this count is greater than or equal to $r$, the current iteration number $k$ is the answer, $k_{\\mathrm{acc}}$. The search for this test case is complete.\n$5$. If the loop finishes without meeting the condition for any $k \\le k_{\\max}$, then $k_{\\mathrm{acc}} = -1$.\nThis procedure is repeated for all four test cases to obtain the final result vector.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef calculate_k_acc(A, v1_unnormalized, r, target, rtol, atol, k_max):\n    \"\"\"\n    Calculates the minimal Lanczos iteration count k_acc for convergence.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Normalize the starting vector to have a 2-norm of 1.\n    v1 = v1_unnormalized / np.linalg.norm(v1_unnormalized)\n\n    # Pre-compute the spectral norm of A for the tolerance calculation.\n    norm_A = np.linalg.norm(A, 2)\n    \n    # Lists to store Lanczos coefficients\n    alphas = []\n    betas_offdiag = []  # Stores beta_2, beta_3, ... for T_k's off-diagonal\n\n    # Lanczos algorithm initialization\n    v_prev = np.zeros(n)\n    v_curr = v1\n    beta_curr = 0.0  # beta_1 is defined as 0\n\n    for k in range(1, k_max + 1):\n        # Perform one step of the Lanczos iteration\n        w = A @ v_curr - beta_curr * v_prev\n        \n        alpha_curr = np.dot(v_curr, w)\n        alphas.append(alpha_curr)\n        \n        w -= alpha_curr * v_curr\n        \n        v_prev = v_curr\n        beta_next = np.linalg.norm(w)\n        \n        # At iteration k, we have T_k of size k x k.\n        # Its diagonal is 'alphas' (k elements).\n        # Its off-diagonal is 'betas_offdiag' (k-1 elements).\n        diag = np.array(alphas)\n        \n        if k == 1:\n            ritz_values = diag\n            # Eigenvector of a 1x1 matrix is [1].\n            ritz_vecs = np.array([[1.0]])\n        else:\n            offdiag = np.array(betas_offdiag)\n            # eigh_tridiagonal is efficient for this task and returns sorted eigenvalues.\n            ritz_values, ritz_vecs = eigh_tridiagonal(diag, offdiag, eigvals_only=False)\n        \n        # Only check for convergence if we have at least r Ritz values.\n        if k >= r:\n            # Identify the indices of the r target Ritz pairs.\n            if target == 'smallest':\n                target_indices = range(r)\n            else:  # 'largest'\n                target_indices = range(k - r, k)\n            \n            num_converged = 0\n            for i in target_indices:\n                theta_i = ritz_values[i]\n                # Last component of the i-th eigenvector of T_k.\n                u_ik = ritz_vecs[-1, i]\n                \n                # Calculate the residual norm efficiently.\n                res_norm = abs(beta_next * u_ik)\n                \n                # Calculate the dynamic tolerance threshold.\n                threshold = max(atol, rtol * max(1.0, abs(theta_i))) * norm_A\n                \n                if res_norm <= threshold:\n                    num_converged += 1\n            \n            if num_converged >= r:\n                return k\n\n        # If breakdown occurs (beta_next is zero), the Krylov subspace is exhausted.\n        # If convergence was not declared, it means the required pairs did not converge.\n        if np.isclose(beta_next, 0.0):\n            return -1\n        \n        # Prepare for the next iteration.\n        v_curr = w / beta_next\n        beta_curr = beta_next\n        betas_offdiag.append(beta_curr)\n\n    # If the loop completes, convergence was not achieved within k_max iterations.\n    return -1\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_suite = [\n        # Test case 1\n        {'n': 50, 'type': 'laplacian', 'r': 2, 'target': 'smallest', 'rtol': 1e-8, 'atol': 1e-12, 'k_max': 50},\n        # Test case 2\n        {'n': 60, 'type': 'toeplitz', 'sigma': 5.0, 'r': 1, 'target': 'largest', 'rtol': 1e-6, 'atol': 1e-10, 'k_max': 60},\n        # Test case 3\n        {'n': 20, 'type': 'identity', 'c': 7.0, 'r': 1, 'target': 'smallest', 'rtol': 1e-12, 'atol': 0.0, 'k_max': 5},\n        # Test case 4\n        {'n': 40, 'type': 'laplacian', 'r': 1, 'target': 'smallest', 'rtol': 1e-14, 'atol': 0.0, 'k_max': 5},\n    ]\n\n    results = []\n    for params in test_suite:\n        n = params['n']\n        \n        # Construct the matrix A based on the test case type.\n        if params['type'] == 'laplacian':\n            A = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n        elif params['type'] == 'toeplitz':\n            sigma = params['sigma']\n            indices = np.arange(n, dtype=float).reshape(-1, 1)\n            A = np.exp(-(((indices - indices.T) / sigma) ** 2))\n        elif params['type'] == 'identity':\n            c = params['c']\n            A = c * np.eye(n)\n        \n        # The unnormalized starting vector has all components equal to 1.\n        v1_unnormalized = np.ones(n)\n        \n        # Calculate k_acc for the current test case.\n        k_acc = calculate_k_acc(A, v1_unnormalized, params['r'], params['target'], params['rtol'], params['atol'], params['k_max'])\n        results.append(k_acc)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}