{
    "hands_on_practices": [
        {
            "introduction": "Understanding the condition number begins with a tangible feel for what \"sensitivity\" means. This first practice provides a direct, hands-on experience with an ill-conditioned system, where a tiny, one-percent change in the data vector is enough to flip the solution into an entirely different quadrant. By explicitly constructing this perturbation and then calculating the condition number from first principles, you will build a strong intuition connecting the dramatic behavior of the solution to the large value of the condition number $\\kappa(A)$ .",
            "id": "2428587",
            "problem": "In computational engineering, the conditioning of a linear system is assessed by how sensitively its solution responds to small perturbations in the data. Consider the linear system $A x = b$ in $\\mathbb{R}^2$, where\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}.\n$$\nUse the Euclidean vector norm and the induced matrix norm (matrix $2$-norm) throughout.\n\n1) Starting from the definitions of solution sensitivity and induced norms, verify by explicit construction that there exists a perturbation $\\Delta b$ with relative size $\\|\\Delta b\\|_{2}/\\|b\\|_{2} = 0.01$ such that the solution $\\tilde{x}$ to $A \\tilde{x} = b + \\Delta b$ lies in a different open quadrant of $\\mathbb{R}^2$ than the solution $x$ to $A x = b$. You must build $\\Delta b$ explicitly and justify every step from first principles.\n\n2) Using the definition of the condition number in the matrix $2$-norm, and without invoking any unproven shortcut formulas, compute the condition number $\\kappa_{2}(A)$ for the given matrix $A$.\n\nProvide as your final answer only the numerical value of $\\kappa_{2}(A)$, rounded to four significant figures. No units are required.",
            "solution": "The problem statement is subjected to validation.\n\nGivens extracted verbatim are:\n- Linear system: $A x = b$ in $\\mathbb{R}^2$.\n- Matrix: $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix}$.\n- Vector: $b = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}$.\n- Norms: Euclidean vector norm ($\\|\\cdot\\|_{2}$) and the induced matrix norm ($2$-norm).\n- Perturbed system: $A \\tilde{x} = b + \\Delta b$.\n- Perturbation constraint: $\\|\\Delta b\\|_{2}/\\|b\\|_{2} = 0.01$.\n- Task 1: Construct $\\Delta b$ such that $\\tilde{x}$ and $x$ are in different open quadrants.\n- Task 2: Compute the condition number $\\kappa_{2}(A)$ from its definition.\n\nValidation Verdict:\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard exercise in numerical linear algebra concerning the sensitivity of linear systems, a core topic in computational engineering. The matrix $A$ is invertible as its determinant is $\\det(A) = (1)(1.05) - (1)(1) = 0.05 \\neq 0$, ensuring a unique solution exists. All data and constraints are provided, and there are no contradictions or ambiguities. The problem is valid.\n\nThe solution proceeds.\n\nPart 1: Construction of a suitable perturbation $\\Delta b$.\n\nFirst, we solve the unperturbed system $A x = b$ to find the original solution $x$.\n$$\n\\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.015 \\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n$x_1 + x_2 = 1$\n$x_1 + 1.05 x_2 = 1.015$\n\nSubtracting the first equation from the second yields:\n$(x_1 + 1.05 x_2) - (x_1 + x_2) = 1.015 - 1$\n$0.05 x_2 = 0.015$\n$x_2 = \\frac{0.015}{0.05} = \\frac{15}{50} = \\frac{3}{10} = 0.3$.\n\nSubstituting $x_2 = 0.3$ into the first equation:\n$x_1 + 0.3 = 1 \\implies x_1 = 0.7$.\n\nThe solution is $x = \\begin{pmatrix} 0.7 \\\\ 0.3 \\end{pmatrix}$. Since $x_1 > 0$ and $x_2 > 0$, this vector lies in the first open quadrant of $\\mathbb{R}^2$.\n\nNext, we analyze the perturbed system $A \\tilde{x} = b + \\Delta b$. The solution is $\\tilde{x} = A^{-1}(b + \\Delta b) = A^{-1}b + A^{-1}\\Delta b = x + \\Delta x$, where $\\Delta x = A^{-1}\\Delta b$. We must construct a perturbation $\\Delta b$ such that $\\tilde{x}$ lies in a different open quadrant. For example, we can aim for the fourth quadrant, which requires $\\tilde{x}_1 > 0$ and $\\tilde{x}_2 < 0$.\n\nThe new solution's components are $\\tilde{x}_1 = x_1 + \\Delta x_1 = 0.7 + \\Delta x_1$ and $\\tilde{x}_2 = x_2 + \\Delta x_2 = 0.3 + \\Delta x_2$. To move to the fourth quadrant, we need $\\tilde{x}_2 < 0$, which implies $0.3 + \\Delta x_2 < 0$, or $\\Delta x_2 < -0.3$.\n\nTo find the relationship between $\\Delta x$ and $\\Delta b$, we must compute the inverse of $A$.\n$$\nA^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} 1.05 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\frac{1}{0.05} \\begin{pmatrix} 1.05 & -1 \\\\ -1 & 1 \\end{pmatrix} = 20 \\begin{pmatrix} 1.05 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 21 & -20 \\\\ -20 & 20 \\end{pmatrix}.\n$$\nThe change in the solution is $\\Delta x = \\begin{pmatrix} \\Delta x_1 \\\\ \\Delta x_2 \\end{pmatrix} = A^{-1} \\Delta b = \\begin{pmatrix} 21 & -20 \\\\ -20 & 20 \\end{pmatrix} \\begin{pmatrix} \\Delta b_1 \\\\ \\Delta b_2 \\end{pmatrix}$.\nThe second component is $\\Delta x_2 = -20 \\Delta b_1 + 20 \\Delta b_2 = 20(\\Delta b_2 - \\Delta b_1)$.\nOur condition $\\Delta x_2 < -0.3$ becomes $20(\\Delta b_2 - \\Delta b_1) < -0.3$, which simplifies to $\\Delta b_2 - \\Delta b_1 < -0.015$.\n\nThe perturbation $\\Delta b$ must also satisfy the relative size constraint $\\|\\Delta b\\|_{2} / \\|b\\|_{2} = 0.01$. First, we compute the norm of $b$.\n$$\n\\|b\\|_2^2 = 1^2 + 1.015^2 = 1 + 1.030225 = 2.030225.\n$$\nSo, $\\|b\\|_{2} = \\sqrt{2.030225}$.\nThe constraint on $\\Delta b$ is $\\|\\Delta b\\|_{2} = 0.01 \\|b\\|_{2} = 0.01 \\sqrt{2.030225}$.\nThis implies $\\|\\Delta b\\|_2^2 = (0.01)^{2} (2.030225) = 0.0002030225$.\n\nWe need to find a vector $\\Delta b = \\begin{pmatrix} \\Delta b_1 \\\\ \\Delta b_2 \\end{pmatrix}$ that satisfies both $\\Delta b_2 - \\Delta b_1 < -0.015$ and $\\Delta b_1^2 + \\Delta b_2^2 = 0.0002030225$.\nTo satisfy the first condition as strongly as possible for a fixed norm, we should choose $\\Delta b$ such that the quantity $\\Delta b_2 - \\Delta b_1$ is minimized. This occurs when $\\Delta b$ is proportional to the vector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ but with a negative projection on the y-axis and positive on the x-axis. Thus, we choose $\\Delta b$ to be in the direction of the vector $\\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$.\n\nLet $\\Delta b = R \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\end{pmatrix}$, where $R = \\|\\Delta b\\|_{2} = 0.01 \\sqrt{2.030225}$.\nSo, $\\Delta b_1 = \\frac{R}{\\sqrt{2}}$ and $\\Delta b_2 = -\\frac{R}{\\sqrt{2}}$.\nThe vector is explicitly:\n$$\n\\Delta b = \\frac{0.01 \\sqrt{2.030225}}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}.\n$$\nLet us verify this construction. The norm is $\\|\\Delta b\\|_{2} = \\sqrt{(\\frac{R}{\\sqrt{2}})^2 + (-\\frac{R}{\\sqrt{2}})^2} = \\sqrt{\\frac{R^2}{2} + \\frac{R^2}{2}} = R$, which is correct by construction.\nNow check the condition on the components:\n$\\Delta b_2 - \\Delta b_1 = -\\frac{R}{\\sqrt{2}} - \\frac{R}{\\sqrt{2}} = -\\frac{2R}{\\sqrt{2}} = -R\\sqrt{2}$.\nSubstituting $R = 0.01 \\sqrt{2.030225}$:\n$$\n\\Delta b_2 - \\Delta b_1 = -(0.01 \\sqrt{2.030225})\\sqrt{2} = -0.01 \\sqrt{4.06045}.\n$$\nSince $2^2=4$, we have $\\sqrt{4.06045} > 2$. Thus, $\\Delta b_2 - \\Delta b_1 < -0.01 \\times 2 = -0.02$.\nAs $-0.02 < -0.015$, our condition is satisfied.\nLet's compute the change in the solution vector $x$.\n$\\Delta x_2 = 20(\\Delta b_2 - \\Delta b_1) = 20(-0.01\\sqrt{4.06045}) = -0.2\\sqrt{4.06045}$.\nNumerically, $\\Delta x_2 \\approx -0.2 \\times 2.01505... \\approx -0.403$.\nThe new component $\\tilde{x}_2 = x_2 + \\Delta x_2 = 0.3 + \\Delta x_2 \\approx 0.3 - 0.403 = -0.103 < 0$.\n\nFor completeness, we check $\\tilde{x}_1$.\n$\\Delta x_1 = 21 \\Delta b_1 - 20 \\Delta b_2 = 21(\\frac{R}{\\sqrt{2}}) - 20(-\\frac{R}{\\sqrt{2}}) = \\frac{41R}{\\sqrt{2}} = \\frac{41(0.01\\sqrt{2.030225})}{\\sqrt{2}} > 0$.\nSo $\\tilde{x}_1 = x_1 + \\Delta x_1 > 0.7 > 0$.\nThe new solution $\\tilde{x}$ has components $\\tilde{x}_1 > 0$ and $\\tilde{x}_2 < 0$, placing it in the fourth quadrant. The construction is valid.\n\nPart 2: Computation of the condition number $\\kappa_{2}(A)$.\n\nThe condition number in the matrix $2$-norm is defined as $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$.\nThe problem requires this to be computed from first principles, not by invoking unproven shortcuts. The matrix $2$-norm (or spectral norm) of a matrix $M$ is defined as its largest singular value, $\\|M\\|_{2} = \\sigma_{\\max}(M) = \\sqrt{\\lambda_{\\max}(M^{\\top}M)}$, where $\\lambda_{\\max}(M^{\\top}M)$ is the largest eigenvalue of $M^{\\top}M$.\n\nThe given matrix $A = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1.05 \\end{pmatrix}$ is a symmetric matrix, i.e., $A = A^{\\top}$.\nFor a symmetric matrix $A$, the product $A^{\\top}A$ becomes $A^{2}$.\nThe eigenvalues of $A^{2}$ are the squares of the eigenvalues of $A$. If $\\lambda$ is an eigenvalue of $A$ with eigenvector $v$, then $A^{2}v = A(Av) = A(\\lambda v) = \\lambda(Av) = \\lambda^2 v$.\nThus, $\\lambda_{\\max}(A^{\\top}A) = \\lambda_{\\max}(A^{2}) = (\\max_i |\\lambda_i(A)|)^{2}$, where $\\lambda_i(A)$ are the eigenvalues of $A$.\nTherefore, for a symmetric matrix $A$, its $2$-norm is the maximum absolute eigenvalue:\n$$\n\\|A\\|_{2} = \\sqrt{(\\max_i |\\lambda_i(A)|)^{2}} = \\max_i |\\lambda_i(A)|.\n$$\nThe inverse $A^{-1}$ is also symmetric. Its eigenvalues are the reciprocals of the eigenvalues of $A$.\nSo, $\\|A^{-1}\\|_{2} = \\max_i |1/\\lambda_i(A)| = 1 / \\min_i |\\lambda_i(A)|$.\nThis holds provided that $A$ is invertible, which it is.\nCombining these results, the condition number for a symmetric matrix $A$ is the ratio of its largest to its smallest absolute eigenvalues:\n$$\n\\kappa_{2}(A) = \\frac{\\max_i |\\lambda_i(A)|}{\\min_i |\\lambda_i(A)|}.\n$$\nWe now find the eigenvalues of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 1-\\lambda & 1 \\\\ 1 & 1.05-\\lambda \\end{pmatrix} = (1-\\lambda)(1.05-\\lambda) - 1 = 0.\n$$\n$$\n\\lambda^{2} - 1.05\\lambda - \\lambda + 1.05 - 1 = 0\n$$\n$$\n\\lambda^{2} - 2.05\\lambda + 0.05 = 0.\n$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$\n\\lambda = \\frac{2.05 \\pm \\sqrt{(-2.05)^{2} - 4(1)(0.05)}}{2} = \\frac{2.05 \\pm \\sqrt{4.2025 - 0.2}}{2} = \\frac{2.05 \\pm \\sqrt{4.0025}}{2}.\n$$\nBoth the trace ($2.05 > 0$) and determinant ($0.05 > 0$) of $A$ are positive, so $A$ is positive definite, and its eigenvalues are positive.\nThe maximum and minimum eigenvalues are:\n$$\n\\lambda_{\\max} = \\frac{2.05 + \\sqrt{4.0025}}{2}\n$$\n$$\n\\lambda_{\\min} = \\frac{2.05 - \\sqrt{4.0025}}{2}\n$$\nThe condition number is their ratio:\n$$\n\\kappa_{2}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{(2.05 + \\sqrt{4.0025})/2}{(2.05 - \\sqrt{4.0025})/2} = \\frac{2.05 + \\sqrt{4.0025}}{2.05 - \\sqrt{4.0025}}.\n$$\nTo compute the numerical value:\n$\\sqrt{4.0025} \\approx 2.0006249377...$\n$$\n\\kappa_{2}(A) \\approx \\frac{2.05 + 2.0006249377}{2.05 - 2.0006249377} = \\frac{4.0506249377}{0.0493750623} \\approx 82.038160...\n$$\nRounding to four significant figures, we get $82.04$.",
            "answer": "$$\\boxed{82.04}$$"
        },
        {
            "introduction": "Ill-conditioning is not just a property of abstract matrices; it often arises from concrete choices made during model construction. This exercise demonstrates how something as seemingly innocuous as changing units—from meters to millimeters—can drastically alter the condition number of a system matrix derived from a simple physical model. By analyzing this effect, you will learn the critical importance of proper scaling in setting up numerically stable computational problems .",
            "id": "2381774",
            "problem": "A straight, slender beam of length $L$ is clamped at $x=0$ and experiences a small transverse deflection $y(x)$ under a gentle load. Over the short interval between two positions $x=0$ and $x=L$, an engineer models the deflection by the linear relation $y(x) = a\\,x + b$, where $a$ and $b$ are unknown constants. Two measurements of the deflection are taken at positions $x=0$ and $x=L$, producing a $2\\times 2$ linear system for the parameter vector $(a,b)^{\\top}$ with system matrix\n$$\nA = \\begin{pmatrix}\nx_1 & 1 \\\\\nx_2 & 1\n\\end{pmatrix},\n$$\nwhere $x_1 = 0$ and $x_2 = L$. Consider the case $L = 1$ meter. Now, suppose instead that the same physical positions are recorded in millimeters, so that the numerical values for the positions become $x_1' = 0$ and $x_2' = 1000$ when $L$ is expressed as $1000$ millimeters. This changes only the numerical entries of the system matrix (not the physics). Let $A_{\\mathrm{m}}$ denote the matrix constructed using positions in meters and $A_{\\mathrm{mm}}$ the matrix constructed using positions in millimeters.\n\nCompute the ratio of the condition numbers in the matrix $2$-norm, $\\kappa_{2}(A_{\\mathrm{mm}})/\\kappa_{2}(A_{\\mathrm{m}})$. Round your final answer to four significant figures. The final answer is a unitless real number.",
            "solution": "The problem statement must first be validated for scientific and logical integrity.\n\nStep 1: Extract Givens\n- A straight, slender beam of length $L$.\n- Clamped at $x=0$.\n- Small transverse deflection modeled by a linear relation: $y(x) = a\\,x + b$.\n- Parameters to be determined: $(a,b)^{\\top}$.\n- Two measurements at positions $x=0$ and $x=L$.\n- The linear system for the parameters has a system matrix $A = \\begin{pmatrix} x_1 & 1 \\\\ x_2 & 1 \\end{pmatrix}$, with $x_1=0$ and $x_2=L$.\n- Case 1 (meters): $L = 1$ meter, giving $x_1=0$, $x_2=1$. The matrix is denoted $A_{\\mathrm{m}}$.\n- Case 2 (millimeters): The same physical length $L$ is expressed as $1000$ millimeters. The numerical values for the positions are $x_1' = 0$, $x_2' = 1000$. The matrix is denoted $A_{\\mathrm{mm}}$.\n- The task is to compute the ratio of the condition numbers in the matrix $2$-norm: $\\kappa_{2}(A_{\\mathrm{mm}})/\\kappa_{2}(A_{\\mathrm{m}})$.\n- The final answer must be a unitless real number rounded to four significant figures.\n\nStep 2: Validate Using Extracted Givens\nThe physical premise of modeling a clamped beam's deflection as $y(x) = ax+b$ is a coarse simplification. A physically accurate model for a clamped beam would require $y(0)=0$ and slope $y'(0)=0$. The linear model can satisfy $y(0)=0$ if $b=0$, but it cannot satisfy $y'(0)=0$ unless $a=0$, which implies zero deflection. However, the problem is not one of solid mechanics but of numerical linear algebra. It poses a question about the numerical stability of a specific linear system that an \"engineer models,\" regardless of the model's fidelity to the underlying physics. The core mathematical question concerning the condition number's dependence on the scaling of coordinates is well-posed, objective, and scientifically grounded within the domain of computational science. The problem is self-contained, providing all necessary information, and its solution is unique and verifiable. Therefore, the problem is deemed valid.\n\nStep 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe problem requires the computation of the ratio of the $2$-norm condition numbers of two matrices. The condition number in the $2$-norm is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. For an invertible matrix $A$, this is equivalent to the ratio of its largest singular value to its smallest singular value, $\\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$. The singular values of $A$ are the square roots of the eigenvalues of the matrix $A^{\\top}A$.\n\nLet us consider a general matrix of the form given in the problem, with $x_1=0$ and $x_2=L$:\n$$\nA_L = \\begin{pmatrix} 0 & 1 \\\\ L & 1 \\end{pmatrix}\n$$\nWe first compute $A_L^{\\top}A_L$:\n$$\nA_L^{\\top}A_L = \\begin{pmatrix} 0 & L \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ L & 1 \\end{pmatrix} = \\begin{pmatrix} L^2 & L \\\\ L & 2 \\end{pmatrix}\n$$\nNext, we find the eigenvalues of this symmetric matrix by solving the characteristic equation $\\det(A_L^{\\top}A_L - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} L^2 - \\lambda & L \\\\ L & 2 - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(L^2 - \\lambda)(2 - \\lambda) - L^2 = 0\n$$\n$$\n2L^2 - L^2\\lambda - 2\\lambda + \\lambda^2 - L^2 = 0\n$$\n$$\n\\lambda^2 - (L^2 + 2)\\lambda + L^2 = 0\n$$\nThe eigenvalues of $A_L^{\\top}A_L$, denoted $\\lambda_{\\pm}$, are given by the quadratic formula:\n$$\n\\lambda_{\\pm} = \\frac{(L^2 + 2) \\pm \\sqrt{(L^2 + 2)^2 - 4L^2}}{2} = \\frac{(L^2 + 2) \\pm \\sqrt{L^4 + 4L^2 + 4 - 4L^2}}{2} = \\frac{L^2 + 2 \\pm \\sqrt{L^4 + 4}}{2}\n$$\nThe singular values of $A_L$ are $\\sigma_{\\pm} = \\sqrt{\\lambda_{\\pm}}$. The condition number $\\kappa_2(A_L)$ is the ratio of the largest to the smallest singular value:\n$$\n\\kappa_2(A_L) = \\frac{\\sigma_+}{\\sigma_-} = \\sqrt{\\frac{\\lambda_+}{\\lambda_-}} = \\sqrt{\\frac{L^2 + 2 + \\sqrt{L^4 + 4}}{L^2 + 2 - \\sqrt{L^4 + 4}}}\n$$\nTo simplify this expression, we multiply the numerator and denominator inside the square root by the conjugate of the denominator:\n$$\n\\kappa_2(A_L)^2 = \\frac{L^2 + 2 + \\sqrt{L^4 + 4}}{L^2 + 2 - \\sqrt{L^4 + 4}} \\cdot \\frac{L^2 + 2 + \\sqrt{L^4 + 4}}{L^2 + 2 + \\sqrt{L^4 + 4}} = \\frac{(L^2 + 2 + \\sqrt{L^4 + 4})^2}{(L^2 + 2)^2 - (L^4 + 4)}\n$$\n$$\n\\kappa_2(A_L)^2 = \\frac{(L^2 + 2 + \\sqrt{L^4 + 4})^2}{L^4 + 4L^2 + 4 - L^4 - 4} = \\frac{(L^2 + 2 + \\sqrt{L^4 + 4})^2}{4L^2}\n$$\nTaking the square root gives a simplified general formula for the condition number:\n$$\n\\kappa_2(A_L) = \\frac{L^2 + 2 + \\sqrt{L^4 + 4}}{2L}\n$$\nWe now apply this formula to the two cases.\n\nCase 1: Matrix $A_{\\mathrm{m}}$, constructed with positions in meters. Here, $L=1$:\n$$\n\\kappa_2(A_{\\mathrm{m}}) = \\kappa_2(A_{L=1}) = \\frac{1^2 + 2 + \\sqrt{1^4 + 4}}{2(1)} = \\frac{3 + \\sqrt{5}}{2}\n$$\n\nCase 2: Matrix $A_{\\mathrm{mm}}$, constructed with positions in millimeters. Here, $L=1000$:\n$$\n\\kappa_2(A_{\\mathrm{mm}}) = \\kappa_2(A_{L=1000}) = \\frac{1000^2 + 2 + \\sqrt{1000^4 + 4}}{2(1000)} = \\frac{1000002 + \\sqrt{10^{12} + 4}}{2000}\n$$\nThe desired ratio is $\\kappa_{2}(A_{\\mathrm{mm}})/\\kappa_{2}(A_{\\mathrm{m}})$:\n$$\n\\frac{\\kappa_{2}(A_{\\mathrm{mm}})}{\\kappa_{2}(A_{\\mathrm{m}})} = \\frac{\\left( \\frac{1000002 + \\sqrt{10^{12} + 4}}{2000} \\right)}{\\left( \\frac{3 + \\sqrt{5}}{2} \\right)} = \\frac{1000002 + \\sqrt{10^{12} + 4}}{1000(3 + \\sqrt{5})}\n$$\nWe now evaluate this expression numerically.\nFirst, the denominator:\n$3 + \\sqrt{5} \\approx 3 + 2.236067977 = 5.236067977$\n$1000(3 + \\sqrt{5}) \\approx 5236.067977$\nNext, the numerator:\n$\\sqrt{10^{12} + 4} = 10^6 \\sqrt{1 + 4 \\times 10^{-12}} \\approx 10^6 (1 + \\frac{1}{2} \\cdot 4 \\times 10^{-12}) = 10^6 + 2 \\times 10^{-6} = 1000000.000002$\n$1000002 + \\sqrt{10^{12} + 4} \\approx 1000002 + 1000000.000002 = 2000002.000002$\nThe ratio is approximately:\n$$\n\\frac{2000002.000002}{5236.067977} \\approx 381.96699933\n$$\nRounding this result to four significant figures gives $382.0$.",
            "answer": "$$\\boxed{382.0}$$"
        },
        {
            "introduction": "Having learned to diagnose ill-conditioning, the next crucial step is to learn how to manage it, especially when dealing with noisy experimental data. This practice introduces Truncated Singular Value Decomposition (TSVD), a powerful technique for regularizing ill-posed problems by filtering out the solution components most sensitive to noise. By implementing TSVD on a notoriously ill-conditioned Hilbert matrix, you will gain hands-on experience in using the singular value spectrum to separate signal from noise and compute a stable, meaningful solution .",
            "id": "2381778",
            "problem": "You are given a family of square linear systems that are known to be ill-conditioned, constructed from the Hilbert matrix. Consider, for a positive integer $n$, the matrix $A \\in \\mathbb{R}^{n \\times n}$ with entries $A_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1,2,\\ldots,n\\}$. Let the unknown vector be $x^\\star \\in \\mathbb{R}^n$ with components $x^\\star_j = \\dfrac{(-1)^{j+1}}{j}$ for $j \\in \\{1,2,\\ldots,n\\}$. The clean data vector is $b_{\\mathrm{clean}} = A x^\\star$. To model experimental noise in a deterministic and reproducible way, define $w \\in \\mathbb{R}^n$ by $w_i = \\cos(i)$ for $i \\in \\{1,2,\\ldots,n\\}$, and then define the noise vector $e = \\eta \\, \\lVert b_{\\mathrm{clean}} \\rVert_2 \\, \\dfrac{w}{\\lVert w \\rVert_2}$ for a given relative noise level $\\eta > 0$. The measured data are $b = b_{\\mathrm{clean}} + e$.\n\nLet the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^\\top$, where $\\Sigma = \\operatorname{diag}(\\sigma_1,\\sigma_2,\\ldots,\\sigma_n)$ with singular values ordered so that $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$, and $U,V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices. For a given relative threshold $\\tau \\in (0,1)$, define the index set of retained modes $K_\\tau = \\{ i \\in \\{1,2,\\ldots,n\\} \\mid \\sigma_i / \\sigma_1 \\ge \\tau \\}$. Define the truncated SVD estimate of $x^\\star$ by\n$$\n\\hat{x}_\\tau \\;=\\; \\sum_{i \\in K_\\tau} \\frac{u_i^\\top b}{\\sigma_i} \\, v_i,\n$$\nwhere $u_i$ and $v_i$ denote the $i$-th columns of $U$ and $V$, respectively. Define the effective numerical rank by $r_\\tau = |K_\\tau|$. Define the filtered condition number\n$$\n\\kappa_\\tau(A) \\;=\\; \\frac{\\max_{i \\in K_\\tau} \\sigma_i}{\\min_{i \\in K_\\tau} \\sigma_i},\n$$\nwith the convention that if $K_\\tau = \\varnothing$ then $\\kappa_\\tau(A) = +\\infty$. Finally, define the relative reconstruction error\n$$\nE_\\tau \\;=\\; \\frac{\\lVert \\hat{x}_\\tau - x^\\star \\rVert_2}{\\lVert x^\\star \\rVert_2}.\n$$\n\nYour task is to compute, for each test case below, the triplet consisting of $(E_\\tau, r_\\tau, \\kappa_\\tau(A))$. All vector and matrix norms are the Euclidean norm ($2$-norm). All computations are dimensionless.\n\nTest suite (each test case is a tuple $(n,\\eta,\\tau)$):\n- Test $1$: $(n,\\eta,\\tau) = (10, 10^{-6}, 10^{-12})$.\n- Test $2$: $(n,\\eta,\\tau) = (10, 10^{-4}, 10^{-8})$.\n- Test $3$: $(n,\\eta,\\tau) = (12, 10^{-4}, 10^{-4})$.\n- Test $4$: $(n,\\eta,\\tau) = (12, 10^{-2}, 10^{-4})$.\n- Test $5$: $(n,\\eta,\\tau) = (12, 10^{-2}, 10^{-2})$.\n\nFinal output format requirements:\n- For each test case, output a list $[E_\\tau, r_\\tau, \\kappa_\\tau(A)]$ where $E_\\tau$ and $\\kappa_\\tau(A)$ are floating-point numbers rounded to $8$ decimal places, and $r_\\tau$ is an integer.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each element being the list for a test case, in the same order as specified above. For example: $[[E_1,r_1,\\kappa_1],[E_2,r_2,\\kappa_2],\\ldots]$.",
            "solution": "The problem requires the analysis of a linear system $A x = b$ where the matrix $A$ is a Hilbert matrix, which is known to be severely ill-conditioned. The ill-conditioning implies that small perturbations in the data vector $b$ can lead to large variations in the solution vector $x$. To address this, a regularization technique known as Truncated Singular Value Decomposition (TSVD) is employed. We are tasked with computing several quantities that characterize the performance of this method for given system parameters. The analysis will be performed for several test cases, each defined by a triplet $(n, \\eta, \\tau)$, where $n$ is the dimension of the system, $\\eta$ is the relative noise level, and $\\tau$ is the regularization threshold.\n\nThe procedure for each test case $(n, \\eta, \\tau)$ is as follows:\n\nStep $1$: Construct the linear system components.\nThe Hilbert matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined by its entries\n$$A_{ij} = \\frac{1}{i + j - 1}$$\nfor $i, j \\in \\{1, 2, \\ldots, n\\}$. The true, unknown solution vector $x^\\star \\in \\mathbb{R}^n$ is given by\n$$x^\\star_j = \\frac{(-1)^{j+1}}{j}$$\nfor $j \\in \\{1, 2, \\ldots, n\\}$. The \"clean\" data vector $b_{\\mathrm{clean}} \\in \\mathbb{R}^n$ is the result of applying the matrix $A$ to the true solution $x^\\star$:\n$$b_{\\mathrm{clean}} = A x^\\star$$\nTo simulate measurement noise, a deterministic noise vector $e \\in \\mathbb{R}^n$ is constructed. First, an auxiliary vector $w \\in \\mathbb{R}^n$ is defined as $w_i = \\cos(i)$ for $i \\in \\{1, 2, \\ldots, n\\}$. The noise vector $e$ is then scaled relative to the norm of the clean data:\n$$e = \\eta \\, \\lVert b_{\\mathrm{clean}} \\rVert_2 \\, \\frac{w}{\\lVert w \\rVert_2}$$\nwhere $\\eta > 0$ is the relative noise level and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm (or $2$-norm). The measured, or noisy, data vector $b \\in \\mathbb{R}^n$ is the sum of the clean data and the noise:\n$$b = b_{\\mathrm{clean}} + e$$\n\nStep $2$: Perform the Singular Value Decomposition (SVD).\nThe SVD of the matrix $A$ is computed, yielding the factorization $A = U \\Sigma V^\\top$. Here, $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices whose columns are the left and right singular vectors, respectively. $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix containing the singular values $\\sigma_i$ of $A$, ordered non-increasingly: $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$. The positivity of all singular values is guaranteed as the Hilbert matrix is non-singular.\n\nStep $3$: Apply the TSVD regularization.\nThe core of TSVD is to filter out the singular modes that are most sensitive to noise. These correspond to the smallest singular values. A relative threshold $\\tau \\in (0,1)$ is used to define the set of retained modes:\n$$K_\\tau = \\{ i \\in \\{1, 2, \\ldots, n\\} \\mid \\sigma_i / \\sigma_1 \\ge \\tau \\}$$\nThis set includes indices of singular values that are not \"too small\" relative to the largest singular value $\\sigma_1$. The cardinality of this set, $r_\\tau = |K_\\tau|$, is the effective numerical rank of the matrix $A$ at the given threshold $\\tau$.\n\nStep $4$: Compute the TSVD solution and associated quantities.\nThe TSVD-regularized solution, $\\hat{x}_\\tau$, is constructed by summing only over the retained modes:\n$$\\hat{x}_\\tau = \\sum_{i \\in K_\\tau} \\frac{u_i^\\top b}{\\sigma_i} v_i$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$. This formula effectively inverts the well-behaved part of the system while discarding the ill-behaved part.\n\nWith the set $K_\\tau$ determined, the filtered condition number $\\kappa_\\tau(A)$ is calculated as the ratio of the largest retained singular value to the smallest retained singular value:\n$$\\kappa_\\tau(A) = \\frac{\\max_{i \\in K_\\tau} \\sigma_i}{\\min_{i \\in K_\\tau} \\sigma_i} = \\frac{\\sigma_1}{\\min_{i \\in K_\\tau} \\sigma_i}$$\nThe equality holds because $\\sigma_1$ is the maximum singular value and its index $i=1$ is always in $K_\\tau$ for $\\tau \\le 1$.\n\nFinally, the quality of the reconstruction is assessed by computing the relative reconstruction error $E_\\tau$:\n$$E_\\tau = \\frac{\\lVert \\hat{x}_\\tau - x^\\star \\rVert_2}{\\lVert x^\\star \\rVert_2}$$\nThis metric compares the distance between the estimated solution and the true solution, relative to the magnitude of the true solution.\n\nFor each test case, we will numerically implement these steps using standard double-precision floating-point arithmetic to obtain the triplet $(E_\\tau, r_\\tau, \\kappa_\\tau(A))$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a suite of test cases involving ill-conditioned\n    linear systems regularized with Truncated SVD.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (10, 1e-6, 1e-12),\n        (10, 1e-4, 1e-8),\n        (12, 1e-4, 1e-4),\n        (12, 1e-2, 1e-4),\n        (12, 1e-2, 1e-2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, eta, tau = case\n\n        # Step 1: Construct the linear system components\n        # Construct Hilbert matrix A\n        i = np.arange(1, n + 1).reshape(-1, 1)\n        j = np.arange(1, n + 1).reshape(1, -1)\n        A = 1.0 / (i + j - 1)\n\n        # Construct true solution vector x_star\n        j_vec = np.arange(1, n + 1)\n        x_star = ((-1)**(j_vec + 1)) / j_vec\n\n        # Calculate clean data vector b_clean\n        b_clean = A @ x_star\n        norm_b_clean = np.linalg.norm(b_clean)\n\n        # Construct noise vector e\n        i_vec = np.arange(1, n + 1)\n        w = np.cos(i_vec)\n        norm_w = np.linalg.norm(w)\n        e = eta * norm_b_clean * w / norm_w\n\n        # Calculate measured data vector b\n        b = b_clean + e\n\n        # Step 2: Perform Singular Value Decomposition (SVD)\n        U, s, Vt = np.linalg.svd(A)\n        # V from SVD is Vt.T\n        V = Vt.T\n\n        # Step 3: Apply TSVD regularization\n        # Find the set of retained modes K_tau\n        sigma_1 = s[0]\n        retained_indices = np.where(s / sigma_1 >= tau)[0]\n        \n        # Calculate effective numerical rank r_tau\n        r_tau = len(retained_indices)\n\n        # Step 4: Compute the TSVD solution and associated quantities\n        \n        # Calculate filtered condition number kappa_tau(A)\n        # Note: s is sorted descending, so the smallest retained singular\n        # value is at the last index of retained_indices.\n        min_retained_sigma = s[retained_indices[-1]]\n        kappa_tau = sigma_1 / min_retained_sigma\n        \n        # Calculate TSVD solution hat_x_tau\n        # The sum is over i in K_tau of (u_i^T * b / sigma_i) * v_i\n        # In matrix terms: V * Sigma_inv_trunc * U^T * b\n        hat_x_tau = np.zeros(n)\n        for idx in retained_indices:\n            u_i = U[:, idx]\n            v_i = V[:, idx]\n            sigma_i = s[idx]\n            hat_x_tau += (u_i.T @ b / sigma_i) * v_i\n            \n        # Calculate relative reconstruction error E_tau\n        norm_x_star = np.linalg.norm(x_star)\n        err_norm = np.linalg.norm(hat_x_tau - x_star)\n        E_tau = err_norm / norm_x_star\n\n        # Store the result triplet, rounded as specified\n        result_triplet = [\n            round(E_tau, 8),\n            int(r_tau),\n            round(kappa_tau, 8)\n        ]\n        results.append(result_triplet)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}