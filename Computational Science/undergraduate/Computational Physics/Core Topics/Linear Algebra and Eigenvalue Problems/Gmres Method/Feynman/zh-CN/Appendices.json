{
    "hands_on_practices": [
        {
            "introduction": "理论是基础，但真正的理解始于实践。本练习将引导你手动执行 GMRES 算法的第一步，将抽象的理论转化为具体的计算。通过为一个简单的 $2 \\times 2$ 系统找到第一个近似解，你将亲身体验 GMRES 如何在克雷洛夫子空间中通过最小化残差范数来逼近真实解，从而牢固掌握其核心迭代机制。",
            "id": "2214790",
            "problem": "广义最小残差（GMRES）方法是一种迭代算法，用于求解线性方程组 $Ax=b$ 的近似解。考虑由矩阵 $A$ 和向量 $b$ 定义的线性系统：\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\n从初始猜测 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，应用一步GMRES算法来找到第一个近似解 $x_1$。将您的答案表示为分量为有理数的列向量。",
            "solution": "GMRES在仿射空间 $x_{0}+\\mathcal{K}_{1}(A,r_{0})$ 上最小化残差的2-范数，其中 $r_{0}=b-Ax_{0}$ 且 $\\mathcal{K}_{1}(A,r_{0})=\\mathrm{span}\\{r_{0}\\}$。当 $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ 时，我们有\n$$\nr_{0}=b=\\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\n经过一步GMRES迭代，$x_{1}$ 的形式为 $x_{1}=x_{0}+\\alpha r_{0}$，其中选择 $\\alpha$ 以最小化\n$$\n\\|b-A(x_{0}+\\alpha r_{0})\\|_{2}=\\|r_{0}-\\alpha A r_{0}\\|_{2}.\n$$\n令 $w=A r_{0}$。则最小化问题为 $\\min_{\\alpha}\\|r_{0}-\\alpha w\\|_{2}^{2}$。使用欧几里得内积，定义\n$$\n\\phi(\\alpha)=(r_{0}-\\alpha w)^{T}(r_{0}-\\alpha w)=r_{0}^{T}r_{0}-2\\alpha w^{T}r_{0}+\\alpha^{2}w^{T}w.\n$$\n令 $\\frac{d\\phi}{d\\alpha}=0$ 可得\n$$\n-2\\,w^{T}r_{0}+2\\alpha\\,w^{T}w=0 \\quad\\Rightarrow\\quad \\alpha=\\frac{w^{T}r_{0}}{w^{T}w}.\n$$\n计算 $w$ 和所需的内积：\n$$\nw=A r_{0}=A b=\\begin{pmatrix}2 & 1\\\\ -1 & 3\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}4\\\\5\\end{pmatrix},\\quad\nw^{T}r_{0}=\\begin{pmatrix}4 & 5\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=14,\\quad\nw^{T}w=4^{2}+5^{2}=41.\n$$\n因此\n$$\n\\alpha=\\frac{14}{41},\\qquad x_{1}=x_{0}+\\alpha r_{0}=\\frac{14}{41}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}.\n$$\n这个 $x_{1}$ 是GMRES(1)的迭代结果，即在 $x_{0}+\\mathrm{span}\\{r_{0}\\}$ 上残差范数的最小化子。",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}}$$"
        },
        {
            "introduction": "GMRES 方法的一个关键但常常令人困惑的特性是，它保证了残差的单调不增，但并不保证解的误差也同样减小。这个练习通过一个精心设计的例子，揭示了这一重要现象。你将计算并发现，在某些情况下，一次迭代可以使残差变得更小，但得到的近似解却离真实解更远了，这有助于你深刻理解 GMRES 对非正规矩阵的收敛行为。",
            "id": "2398705",
            "problem": "考虑线性系统 $A x = b$，其中 $A$ 是一个 $2 \\times 2$ 矩阵\n$$\nA = \\begin{pmatrix}\n1 & 10 \\\\\n0 & 1\n\\end{pmatrix},\n$$\n右端项为 $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，初始猜测为 $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。令 $\\|\\cdot\\|$ 表示欧几里得范数。对于任意 $x$，其残差为 $r(x) = b - A x$。广义最小残差（GMRES）方法将第一次迭代的结果 $x_{1}$ 定义为仿射空间 $x_{0} + \\mathcal{K}_{1}(A, r_{0})$ 中的唯一向量，该向量能够最小化 $\\|r(x)\\|$，其中 $r_{0} = r(x_{0})$。\n\n计算比值\n$$\n\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|},\n$$\n其中 $x$ 是 $A x = b$ 的精确解，使用欧几里得范数。将答案四舍五入到四位有效数字。",
            "solution": "该问题陈述科学严谨、定义明确且内容完整。它描述了广义最小残差（GMRES）方法的一个标准应用。我们将着手解决此问题。\n\n目标是计算比值 $\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|}$，其中 $x$ 是线性系统 $A x = b$ 的精确解，$x_{0}$ 是给定的初始猜测，$x_{1}$ 是通过 GMRES 方法生成的第一次迭代结果。范数为欧几里得范数。\n\n首先，我们确定精确解 $x$。该线性系统为：\n$$\nA x = b \\implies \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_a \\\\ x_b \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n使用回代法，第二行给出 $0 \\cdot x_a + 1 \\cdot x_b = 1$，这意味着 $x_b = 1$。\n将 $x_b=1$ 代入第一行，得到 $1 \\cdot x_a + 10 \\cdot (1) = 1$，这得出 $x_a = 1 - 10 = -9$。\n因此，精确解为 $x = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}$。\n\n接下来，我们计算初始残差 $r_0$。初始猜测为 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n残差定义为 $r(x) = b - A x$。初始残差 $r_0$ 为：\n$$\nr_0 = r(x_0) = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nGMRES 第一次迭代的结果 $x_1$ 被定义为在仿射空间 $x_0 + \\mathcal{K}_1(A, r_0)$ 中使残差范数 $\\|r(x)\\|$ 最小化的向量。第一个克雷洛夫子空间 $\\mathcal{K}_1(A, r_0)$ 是由初始残差向量张成的空间，即 $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$。\n因此，$x_1$ 可以表示为：\n$$\nx_1 = x_0 + \\alpha r_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n对于某个标量 $\\alpha \\in \\mathbb{R}$。\n\n为了找到最优的 $\\alpha$，我们必须最小化相应残差的范数，即 $r(x_1) = b - A x_1$。\n代入 $x_1$ 的表达式并使用 $b=r_0$，我们得到：\n$$\nr(x_1) = b - A (\\alpha r_0) = r_0 - \\alpha (A r_0)\n$$\n我们需要找到使欧几里得范数的平方 $\\|r_0 - \\alpha A r_0\\|^2$ 最小化的 $\\alpha$。这是一个标准的线性最小二乘问题，其解是 $r_0$ 在 $A r_0$ 上的正交投影。系数 $\\alpha$ 由下式给出：\n$$\n\\alpha = \\frac{\\langle r_0, A r_0 \\rangle}{\\|A r_0\\|^2}\n$$\n我们计算向量 $A r_0$：\n$$\nA r_0 = \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 10 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 1 \\end{pmatrix}\n$$\n现在，我们计算内积和范数的平方：\n$$\n\\langle r_0, A r_0 \\rangle = (1)(11) + (1)(1) = 12\n$$\n$$\n\\|A r_0\\|^2 = 11^2 + 1^2 = 121 + 1 = 122\n$$\n代入这些值，我们求得 $\\alpha$：\n$$\n\\alpha = \\frac{12}{122} = \\frac{6}{61}\n$$\n有了这个 $\\alpha$ 值，GMRES 的第一次迭代结果是：\n$$\nx_1 = \\frac{6}{61} r_0 = \\frac{6}{61} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix}\n$$\n现在我们计算误差向量 $e_0 = x - x_0$ 和 $e_1 = x - x_1$ 及其范数。\n初始误差为：\n$$\ne_0 = x - x_0 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}\n$$\n初始误差的范数为：\n$$\n\\|e_0\\| = \\|x - x_0\\| = \\sqrt{(-9)^2 + 1^2} = \\sqrt{81 + 1} = \\sqrt{82}\n$$\n第一次迭代后的误差为：\n$$\ne_1 = x - x_1 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix} = \\begin{pmatrix} -9 - \\frac{6}{61} \\\\ 1 - \\frac{6}{61} \\end{pmatrix} = \\begin{pmatrix} -\\frac{549+6}{61} \\\\ \\frac{61-6}{61} \\end{pmatrix} = \\begin{pmatrix} -555/61 \\\\ 55/61 \\end{pmatrix}\n$$\n该误差向量的范数为：\n$$\n\\|e_1\\| = \\|x - x_1\\| = \\sqrt{\\left(-\\frac{555}{61}\\right)^2 + \\left(\\frac{55}{61}\\right)^2} = \\frac{1}{61}\\sqrt{(-555)^2 + 55^2} = \\frac{1}{61}\\sqrt{308025 + 3025} = \\frac{\\sqrt{311050}}{61}\n$$\n最后，我们计算所要求的比值：\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} = \\frac{\\sqrt{311050}/61}{\\sqrt{82}} = \\sqrt{\\frac{311050}{61^2 \\cdot 82}} = \\sqrt{\\frac{311050}{3721 \\cdot 82}} = \\sqrt{\\frac{311050}{305122}}\n$$\n我们现在对这个表达式进行数值计算：\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} \\approx \\sqrt{1.0194280} \\approx 1.00966727\n$$\n问题要求将答案四舍五入到四位有效数字。将 $1.00966727$ 四舍五入得到 $1.010$。当 GMRES 应用于高度非正规矩阵（如本题中给出的矩阵）时，误差范数增大的现象是已知的。虽然残差范数保证是单调不增的，但误差范数并非如此。",
            "answer": "$$\\boxed{1.010}$$"
        },
        {
            "introduction": "从理论计算到实际应用是成为一名计算科学家或工程师的必经之路。本练习要求你编写程序来实现带有预处理的 GMRES 方法，并直接比较左预处理和右预处理两种策略的效果。通过对不同物理问题的数值实验，你将不仅巩固对 GMRES 算法本身的理解，还将深入探究预处理如何影响收敛性，以及两种预处理方式在监控真实残差和最终解精度上的关键差异。",
            "id": "2398732",
            "problem": "给定方形、非奇异实数矩阵 $A \\in \\mathbb{R}^{n \\times n}$，右端向量 $b \\in \\mathbb{R}^{n}$，以及非奇异实数预条件子 $M \\in \\mathbb{R}^{n \\times n}$。考虑线性系统 $A x = b$。定义左预处理为求解 $M^{-1} A x = M^{-1} b$ 以得到 $x$，右预处理为求解 $A M^{-1} y = b$ 以得到 $y$，随后计算 $x = M^{-1} y$。对于每种情况，以零向量为初始猜测，生成一个近似解序列，该序列在每次迭代时，在相应的 Krylov 子空间上最小化残差的欧几里得范数。在每次迭代 $k$ 时，计算真实残差向量 $r_k = b - A x_k$ 及其欧几里得范数 $\\|r_k\\|_2$。\n\n你的任务是实现一个程序，对于下面指定的每个测试用例，该程序能生成：\n- 最终左预处理近似解与右预处理近似解之差的欧几里得范数，即 $\\|x^{(L)}_{\\text{final}} - x^{(R)}_{\\text{final}}\\|_2$，\n- 左预处理公式的真实残差范数完整历史记录 $\\{\\|r_k^{(L)}\\|_2\\}_{k=1}^{K_L}$，\n- 右预处理公式的真实残差范数完整历史记录 $\\{\\|r_k^{(R)}\\|_2\\}_{k=1}^{K_R}$，\n\n其中 $K_L$ 和 $K_R$ 是每种公式满足停止准则所需的实际迭代次数。使用停止准则 $\\|r_k\\|_2 \\le \\tau \\|b\\|_2$，相对容差为 $\\tau = 10^{-10}$；或者当迭代次数达到矩阵维度 $n$ 时终止。所有角度（如适用）必须以弧度为单位。\n\n测试套件（请严格使用以下规范）：\n\n- 测试用例 $1$（对称正定，Jacobi 预条件子，构造解）：\n  - 规模：$n = 20$。\n  - 矩阵 $A$：三对角矩阵，主对角线元素为 $2$，第一副对角线和第一超对角线元素为 $-1$（标准一维 Poisson 模板，带齐次 Dirichlet 边界）。\n  - 预条件子 $M$：$A$ 的对角部分。\n  - 精确解 $x^{\\star}$，其元素为 $x^{\\star}_i = \\sin\\left(\\frac{\\pi i}{n+1}\\right)$，其中 $i = 1, 2, \\dots, n$（角度以弧度为单位）。\n  - 右端项 $b = A x^{\\star}$。\n\n- 测试用例 $2$（非对称对流扩散，Jacobi 预条件子）：\n  - 规模：$n = 30$。\n  - 网格间距 $h = \\frac{1}{n+1}$。\n  - 参数：扩散系数 $\\varepsilon = 10^{-2}$，对流速度 $c = 1$。\n  - 矩阵 $A = \\frac{\\varepsilon}{h^2} T_{\\text{diff}} + \\frac{c}{h} T_{\\text{adv}}$，其中 $T_{\\text{diff}}$ 为三对角矩阵，主对角线为 $2$，副对角线和超对角线为 $-1$；$T_{\\text{adv}}$ 对应于后向差分，主对角线为 $-1$，第一副对角线为 $+1$（其他位置为零）。\n  - 预条件子 $M$：$A$ 的对角部分。\n  - 右端项 $b$ 是 $\\mathbb{R}^n$ 中的全一向量。\n\n- 测试用例 $3$（单位预条件子边界情况）：\n  - $A$ 和 $b$ 与测试用例 $1$ 相同。\n  - 预条件子 $M = I$（$n \\times n$ 单位矩阵）。\n\n数值规格：\n- 所有运行的初始猜测：$x_0 = 0$。\n- 使用上述定义的相对容差 $\\tau = 10^{-10}$。\n- 最大迭代次数 $k_{\\max} = n$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，输出一个形式为 $[\\|x^{(L)}_{\\text{final}} - x^{(R)}_{\\text{final}}\\|_2, \\text{residuals\\_left}, \\text{residuals\\_right}]$ 的列表，其中 $\\text{residuals\\_left}$ 和 $\\text{residuals\\_right}$ 是每次迭代的浮点数残差范数列表。\n- 具体而言，最终输出必须看起来像一个长度为 3 的 Python 风格列表，其元素分别对应于测试用例 1、2 和 3。例如，打印的结构必须具有以下形式：\n  $[[d_1,[r_{1,1},\\dots,r_{1,K_L}],[s_{1,1},\\dots,s_{1,K_R}]], [d_2,[\\dots],[\\dots]], [d_3,[\\dots],[\\dots]]]$\n  所有数字均以十进制或科学记数法显示（无百分号）。\n\n所有量都是无量纲的。不应读取任何外部输入；所有数据都必须在程序内部构建。所需的输出是如上所述的浮点数和浮点数列表，并且程序必须严格遵守本说明中描述的最终输出格式。",
            "solution": "该问题要求实现广义最小残差 (GMRES) 方法来求解形如 $A x = b$ 的线性系统，并考虑使用给定预条件子 $M$ 进行左预处理和右预处理。对于每个测试用例，我们必须计算通过左预处理和右预处理获得的最终解之间的差异，并追踪每次迭代 $k$ 时真实残差范数 $\\|r_k\\|_2 = \\|b - A x_k\\|_2$ 的历史记录。\n\n解决方案的基础在于自定义实现的 GMRES 算法，因为标准库函数可能无法提供对停止准则的必要控制，该准则根据真实残差而非预处理后的残差来指定。\n\nGMRES 方法生成一个近似解序列 $x_k$，使得所求解系统的残差的欧几里得范数在维度为 $k$ 的相应 Krylov 子空间上最小化。该残差的性质取决于预处理策略。\n\n**1. 预处理策略**\n\n设原始系统为 $A x = b$。预条件子 $M$ 是一个近似于 $A$ 的矩阵，但求解系统 $Mz=d$ 比求解原始系统要容易得多。\n\n- **左预处理**：系统被变换为 $M^{-1} A x = M^{-1} b$。令 $\\tilde{A} = M^{-1}A$ 且 $\\tilde{b} = M^{-1}b$。将 GMRES 应用于 $\\tilde{A}x = \\tilde{b}$。在迭代 $k$ 时，该方法找到一个迭代解 $x_k$，它在 Krylov 子空间 $\\mathcal{K}_k(\\tilde{A}, \\tilde{r}_0)$ 上最小化预处理后残差的范数，即 $\\|\\tilde{r}_k\\|_2 = \\|\\tilde{b} - \\tilde{A} x_k\\|_2 = \\|M^{-1}(b - A x_k)\\|_2 = \\|M^{-1}r_k\\|_2$。问题要求监控真实残差范数 $\\|r_k\\|_2$。这个范数无法从预处理系统的 GMRES 最小化过程中直接获得。因此，在每次迭代 $k$ 时，必须显式构造迭代解 $x_k$ 来计算 $\\|r_k\\|_2 = \\|b - A x_k\\|_2$。\n\n- **右预处理**：通过变量替换来变换系统。令 $x = M^{-1}y$。系统变为 $A M^{-1} y = b$。令 $\\hat{A} = AM^{-1}$。将 GMRES 应用于 $\\hat{A}y = b$。在迭代 $k$ 时，该方法找到一个迭代解 $y_k$，它在 Krylov 子空间 $\\mathcal{K}_k(\\hat{A}, \\hat{r}_0)$ 上最小化 $\\|\\hat{r}_k\\|_2 = \\|b - \\hat{A} y_k\\|_2$。原始系统的相应解为 $x_k = M^{-1}y_k$。真实残差为 $r_k = b - A x_k = b - A(M^{-1} y_k) = b - \\hat{A} y_k = \\hat{r}_k$。在这种情况下，GMRES 最小化的残差与真实残差相同。因此，在 GMRES 算法的每一步都可以经济地获得真实残差范数 $\\|r_k\\|_2$。最终解通过计算 $x_{\\text{final}} = M^{-1}y_{\\text{final}}$ 得到。\n\n**2. GMRES 算法实现**\n\n该方法的核心是 Arnoldi 迭代，它为 Krylov 子空间 $\\mathcal{K}_{k+1}(C, r_0)$ 构建一个标准正交基 $V_{k+1} = [v_1, \\dots, v_{k+1}]$，其中 $C$ 是所求解系统的算子（$M^{-1}A$ 或 $AM^{-1}$），$r_0$ 是初始残差。我们因其数值稳定性而使用修正的 Gram-Schmidt 过程。这个过程产生关系式 $C V_k = V_{k+1} \\bar{H}_k$，其中 $\\bar{H}_k$ 是一个 $(k+1) \\times k$ 的上 Hessenberg 矩阵。\n\nGMRES 的迭代解 $x_k$ 表示为 $x_k = x_0 + V_k y_k$（对于左预处理）或 $y_k = y_0 + V_k z_k$（对于右预处理），其中 $y_k$ 或 $z_k$ 是以下小型最小二乘问题的解：\n$$ \\min_{z \\in \\mathbb{R}^k} \\|\\beta e_1 - \\bar{H}_k z\\|_2 $$\n其中 $\\beta = \\|r_0\\|_2$ 且 $e_1 = [1, 0, \\dots, 0]^T \\in \\mathbb{R}^{k+1}$。通过应用一系列 Givens 旋转将 $\\bar{H}_k$ 变换为一个上三角矩阵 $R_k$ 并将右端向量 $\\beta e_1$ 更新为一个向量 $g_k$，可以高效地解决这个问题。迭代 $k$ 时的残差范数由变换后的右端向量的第 $(k+1)$ 个分量的大小给出。\n\n我的 GMRES 实现按以下步骤进行：\n1. 用 $x_0 = 0$ 初始化。根据预处理类型确定系统算子 $C$ 和初始残差 $r_0$。\n2. 归一化初始残差以获得第一个基向量 $v_1 = r_0 / \\|r_0\\|_2$。\n3. 对于每次迭代 $k = 1, 2, \\dots, n$：\n    a. 使用带修正 Gram-Schmidt 的 Arnoldi 过程生成下一个基向量 $v_{k+1}$ 和 Hessenberg 矩阵 $\\bar{H}_k$ 的第 $k$ 列。\n    b. 使用 Givens 旋转更新 $\\bar{H}_k$ 的 QR 分解。这将三角矩阵 $R_{k-1}$ 更新为 $R_k$，并将变换后的右端项 $g_{k-1}$ 更新为 $g_k$。\n    c. 对于右预处理（和无预处理），真实残差范数 $\\|r_k\\|_2$ 就是 $g_k$ 最后一个元素的大小，即 $|g_{k+1}|$。\n    d. 对于左预处理，必须显式计算真实残差范数。我们求解三角系统 $R_k y_k = g_k(1:k)$ 以得到 $y_k$，构造解的迭代 $x_k = x_0 + V_k y_k$，然后计算 $\\|r_k\\|_2 = \\|b - A x_k\\|_2$。\n    e. 存储计算出的真实残差范数 $\\|r_k\\|_2$。\n    f. 检查停止准则：$\\|r_k\\|_2 \\le \\tau \\|b\\|_2$。如果满足，或者 $k=n$，则迭代终止。\n4. 在循环于第 $K$ 次迭代终止后，通过求解最终的三角系统得到系数，并构成基向量的线性组合来计算最终解。对于右预处理情况，需要进行一次额外的反变换 $x_{\\text{final}} = M^{-1} y_{\\text{final}}$。\n\n**3. 测试用例构造**\n\n测试用例如下规定进行构造：\n- **用例 1**：来自一维 Poisson 问题的对称正定系统。$A$ 是一个三对角矩阵，元素为 $(-1, 2, -1)$。规模为 $n=20$。预条件子 $M$ 是 $A$ 的对角部分，即 $M = 2I$。右端项 $b$ 是通过已知解 $x^{\\star}_i = \\sin\\left(\\frac{\\pi i}{n+1}\\right)$ 构造的，使得 $b = A x^{\\star}$。\n- **用例 2**：来自一维对流扩散问题的非对称系统。矩阵为 $A = \\frac{\\varepsilon}{h^2} T_{\\text{diff}} + \\frac{c}{h} T_{\\text{adv}}$，其中 $n=30$，$\\varepsilon=10^{-2}$，$c=1$，且 $h=\\frac{1}{n+1}$。$T_{\\text{diff}}$ 在副对角线、主对角线和超对角线上分别为 $(-1, 2, -1)$，$T_{\\text{adv}}$ 分别为 $(1, -1, 0)$。$M$ 是所得矩阵 $A$ 的对角部分。向量 $b$ 由全一元素构成。\n- **用例 3**：与用例 1 相同，但使用单位预条件子 $M=I$。这等价于未经预处理的 GMRES。\n\n该代码实现了这些步骤，为每个测试用例计算所需的量，并将输出格式化为单行字符串。",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef gmres_custom(A, b, precon_type='none', M_inv=None, tol=1e-10, maxiter=None):\n    \"\"\"\n    Custom GMRES implementation to solve Ax=b.\n\n    This implementation provides fine-grained control over the stopping criterion,\n    which is based on the true residual norm ||b - A*x_k||_2, and records its history.\n    \"\"\"\n    n = A.shape[0]\n    if maxiter is None:\n        maxiter = n\n    \n    x0 = np.zeros(n)\n    res_hist = []\n\n    # System setup based on preconditioning type\n    if precon_type == 'left':\n        op = lambda v: M_inv(A @ v)\n        r_start = M_inv(b - A @ x0)\n    elif precon_type == 'right':\n        # We solve A M^-1 y = b for y, and then x = M^-1 y.\n        # Initial guess y0 = M x0. Since x0=0, y0=0.\n        op = lambda v: A @ M_inv(v)\n        r_start = b - op(np.zeros(n))\n    else: # 'none'\n        op = lambda v: A @ v\n        r_start = b - A @ x0\n    \n    b_norm = np.linalg.norm(b)\n    stop_thresh = tol * b_norm\n\n    r_norm = np.linalg.norm(r_start)\n    if r_norm == 0:\n        return x0, []\n\n    V = np.zeros((n, maxiter + 1))\n    H = np.zeros((maxiter + 1, maxiter))\n    \n    V[:, 0] = r_start / r_norm\n    \n    s = np.zeros(maxiter + 1)\n    s[0] = r_norm\n    \n    cs = np.zeros(maxiter)\n    sn = np.zeros(maxiter)\n\n    k = -1\n    for k in range(maxiter):\n        # Arnoldi process with Modified Gram-Schmidt\n        w = op(V[:, k])\n        for j in range(k + 1):\n            H[j, k] = np.dot(w, V[:, j])\n            w -= H[j, k] * V[:, j]\n        \n        H[k+1, k] = np.linalg.norm(w)\n\n        # Apply previous Givens rotations to new column of H\n        for j in range(k):\n            temp = cs[j] * H[j, k] + sn[j] * H[j+1, k]\n            H[j+1, k] = -sn[j] * H[j, k] + cs[j] * H[j+1, k]\n            H[j, k] = temp\n        \n        # Calculate new Givens rotation\n        denom = np.sqrt(H[k, k]**2 + H[k+1, k]**2)\n        if denom == 0:\n            cs[k] = 1.0\n            sn[k] = 0.0\n        else:\n            cs[k] = H[k, k] / denom\n            sn[k] = H[k+1, k] / denom\n        \n        # Apply new rotation\n        H[k, k] = cs[k] * H[k, k] + sn[k] * H[k+1, k]\n        H[k+1, k] = 0.0\n        \n        s[k+1] = -sn[k] * s[k]\n        s[k] = cs[k] * s[k]\n        \n        # Calculate true residual and check for convergence\n        true_res_norm = 0\n        if precon_type == 'left':\n            y = scipy.linalg.solve_triangular(H[:k+1, :k+1], s[:k+1], check_finite=False)\n            x_k = x0 + V[:, :k+1] @ y\n            true_res_norm = np.linalg.norm(b - A @ x_k)\n        else: # 'right' or 'none'\n            true_res_norm = abs(s[k+1])\n        \n        res_hist.append(true_res_norm)\n        \n        if true_res_norm  stop_thresh or H[k+1, k] == 0:\n            break\n    \n    # After loop, k is the last iteration index (0-based)\n    # Total iterations = k + 1\n    num_iters = k + 1\n\n    # Solve for the final solution\n    y = scipy.linalg.solve_triangular(H[:num_iters, :num_iters], s[:num_iters], check_finite=False)\n    \n    if precon_type == 'right':\n        y_final = V[:, :num_iters] @ y\n        x_final = M_inv(y_final)\n    else: # 'left' or 'none'\n        x_final = x0 + V[:, :num_iters] @ y\n        \n    return x_final, res_hist\n\ndef setup_case_1():\n    n = 20\n    diag = np.full(n, 2.0)\n    off_diag = np.full(n - 1, -1.0)\n    A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    \n    M_diag = np.diag(A)\n    M_inv = lambda v: v / M_diag\n    \n    i = np.arange(1, n + 1)\n    x_true = np.sin(np.pi * i / (n + 1))\n    b = A @ x_true\n    \n    return A, b, M_inv, n\n\ndef setup_case_2():\n    n = 30\n    h = 1.0 / (n + 1)\n    eps = 1e-2\n    c = 1.0\n\n    T_diff = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    T_adv = np.diag(np.full(n, -1.0)) + np.diag(np.full(n - 1, 1.0), k=-1)\n    \n    A = (eps / h**2) * T_diff + (c / h) * T_adv\n    \n    M_diag = np.diag(A)\n    M_inv = lambda v: v / M_diag\n    \n    b = np.ones(n)\n\n    return A, b, M_inv, n\n\ndef setup_case_3():\n    A, b, _, n = setup_case_1()\n    M_inv = lambda v: v # Identity preconditioner\n    return A, b, M_inv, n\n\ndef solve():\n    test_cases_setup = [setup_case_1, setup_case_2, setup_case_3]\n    results = []\n    \n    for setup_func in test_cases_setup:\n        A, b, M_inv, n = setup_func()\n        tau = 1e-10\n\n        # Left preconditioning\n        x_L, res_L = gmres_custom(A, b, precon_type='left', M_inv=M_inv, tol=tau, maxiter=n)\n        \n        # Right preconditioning\n        x_R, res_R = gmres_custom(A, b, precon_type='right', M_inv=M_inv, tol=tau, maxiter=n)\n\n        # Calculate norm of difference\n        diff_norm = np.linalg.norm(x_L - x_R)\n\n        results.append([diff_norm, res_L, res_R])\n\n    # Final print statement in the exact required format (list of lists, no spaces)\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}