{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Generalized Minimal Residual (GMRES) method lies in its strategy to iteratively minimize the norm of the residual vector. To begin our hands-on exploration, we will dissect the very first step of this powerful algorithm. This practice  provides a concrete calculation for a simple $2 \\times 2$ system, guiding you through finding the optimal approximation within the initial search direction defined by the first Krylov subspace, $\\mathcal{K}_1$. Mastering this fundamental calculation is crucial for understanding how the full algorithm builds its sequence of improved solutions by progressively expanding this subspace.",
            "id": "2214790",
            "problem": "The Generalized Minimum Residual (GMRES) method is an iterative algorithm used to find an approximate solution to a system of linear equations $Ax=b$. Consider the linear system defined by the matrix $A$ and the vector $b$:\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\nStarting with an initial guess of $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one step of the GMRES algorithm to find the first approximate solution, $x_1$. Express your answer as a column vector with rational components.",
            "solution": "GMRES minimizes the 2-norm of the residual over the affine space $x_{0}+\\mathcal{K}_{1}(A,r_{0})$, where $r_{0}=b-Ax_{0}$ and $\\mathcal{K}_{1}(A,r_{0})=\\mathrm{span}\\{r_{0}\\}$. With $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ we have\n$$\nr_{0}=b=\\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\nAfter one GMRES step, $x_{1}$ has the form $x_{1}=x_{0}+\\alpha r_{0}$, where $\\alpha$ is chosen to minimize\n$$\n\\|b-A(x_{0}+\\alpha r_{0})\\|_{2}=\\|r_{0}-\\alpha A r_{0}\\|_{2}.\n$$\nLet $w=A r_{0}$. Then the minimization problem is $\\min_{\\alpha}\\|r_{0}-\\alpha w\\|_{2}^{2}$. Using the Euclidean inner product, define\n$$\n\\phi(\\alpha)=(r_{0}-\\alpha w)^{T}(r_{0}-\\alpha w)=r_{0}^{T}r_{0}-2\\alpha w^{T}r_{0}+\\alpha^{2}w^{T}w.\n$$\nSetting $\\frac{d\\phi}{d\\alpha}=0$ gives\n$$\n-2\\,w^{T}r_{0}+2\\alpha\\,w^{T}w=0 \\quad\\Rightarrow\\quad \\alpha=\\frac{w^{T}r_{0}}{w^{T}w}.\n$$\nCompute $w$ and the needed inner products:\n$$\nw=A r_{0}=A b=\\begin{pmatrix}2 & 1\\\\ -1 & 3\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}4\\\\5\\end{pmatrix},\\quad\nw^{T}r_{0}=\\begin{pmatrix}4 & 5\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=14,\\quad\nw^{T}w=4^{2}+5^{2}=41.\n$$\nTherefore\n$$\n\\alpha=\\frac{14}{41},\\qquad x_{1}=x_{0}+\\alpha r_{0}=\\frac{14}{41}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}.\n$$\nThis $x_{1}$ is the GMRES(1) iterate, i.e., the minimizer over $x_{0}+\\mathrm{span}\\{r_{0}\\}$ of the residual norm.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}}$$"
        },
        {
            "introduction": "While GMRES guarantees that the norm of the residual is minimized at each step, what does this imply for the actual error in our solution, defined as the distance to the true solution? This next exercise  uses a carefully constructed scenario to explore this subtle but critical question. By working through this example, you will discover that minimizing the residual does not always mean minimizing the error, a key insight into the convergence behavior of GMRES, particularly when applied to systems with non-normal matrices.",
            "id": "2398705",
            "problem": "Consider the linear system $A x = b$ with the $2 \\times 2$ matrix\n$$\nA = \\begin{pmatrix}\n1 & 10 \\\\\n0 & 1\n\\end{pmatrix},\n$$\nthe right-hand side $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, and the initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Let $\\|\\cdot\\|$ denote the Euclidean norm. The residual for any $x$ is $r(x) = b - A x$. The Generalized Minimal Residual (GMRES) method defines the first iterate $x_{1}$ as the unique vector in the affine space $x_{0} + \\mathcal{K}_{1}(A, r_{0})$, where $r_{0} = r(x_{0})$, that minimizes $\\|r(x)\\|$.\n\nCompute the ratio\n$$\n\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|},\n$$\nwhere $x$ is the exact solution of $A x = b$, using the Euclidean norm. Round your answer to four significant figures.",
            "solution": "The objective is to compute the ratio $\\frac{\\|x - x_{1}\\|}{\\|x - x_{0}\\|}$, where $x$ is the exact solution of the linear system $A x = b$, $x_{0}$ is the given initial guess, and $x_{1}$ is the first iterate generated by the GMRES method. The norm is the Euclidean norm.\n\nFirst, we determine the exact solution $x$. The linear system is given by:\n$$\nA x = b \\implies \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_a \\\\ x_b \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nUsing back substitution, the second row gives $0 \\cdot x_a + 1 \\cdot x_b = 1$, which implies $x_b = 1$.\nSubstituting $x_b=1$ into the first row gives $1 \\cdot x_a + 10 \\cdot (1) = 1$, which yields $x_a = 1 - 10 = -9$.\nThus, the exact solution is $x = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}$.\n\nNext, we calculate the initial residual, $r_0$. The initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe residual is defined as $r(x) = b - A x$. The initial residual $r_0$ is:\n$$\nr_0 = r(x_0) = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nThe first GMRES iterate, $x_1$, is defined as the vector in the affine space $x_0 + \\mathcal{K}_1(A, r_0)$ that minimizes the norm of the residual $\\|r(x)\\|$. The first Krylov subspace, $\\mathcal{K}_1(A, r_0)$, is the space spanned by the initial residual vector, $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$.\nTherefore, $x_1$ can be expressed as:\n$$\nx_1 = x_0 + \\alpha r_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\alpha \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nfor some scalar $\\alpha \\in \\mathbb{R}$.\n\nTo find the optimal $\\alpha$, we must minimize the norm of the corresponding residual, $r(x_1) = b - A x_1$.\nSubstituting the expression for $x_1$ and using $b=r_0$, we get:\n$$\nr(x_1) = b - A (\\alpha r_0) = r_0 - \\alpha (A r_0)\n$$\nWe need to find $\\alpha$ that minimizes the squared Euclidean norm $\\|r_0 - \\alpha A r_0\\|^2$. This is a standard linear least-squares problem, whose solution is the orthogonal projection of $r_0$ onto $A r_0$. The coefficient $\\alpha$ is given by:\n$$\n\\alpha = \\frac{\\langle r_0, A r_0 \\rangle}{\\|A r_0\\|^2}\n$$\nWe compute the vector $A r_0$:\n$$\nA r_0 = \\begin{pmatrix} 1 & 10 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 10 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 1 \\end{pmatrix}\n$$\nNow, we compute the inner product and the squared norm:\n$$\n\\langle r_0, A r_0 \\rangle = (1)(11) + (1)(1) = 12\n$$\n$$\n\\|A r_0\\|^2 = 11^2 + 1^2 = 121 + 1 = 122\n$$\nSubstituting these values, we find $\\alpha$:\n$$\n\\alpha = \\frac{12}{122} = \\frac{6}{61}\n$$\nWith this value of $\\alpha$, the first GMRES iterate is:\n$$\nx_1 = \\frac{6}{61} r_0 = \\frac{6}{61} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix}\n$$\nNow we compute the error vectors $e_0 = x - x_0$ and $e_1 = x - x_1$, and their norms.\nThe initial error is:\n$$\ne_0 = x - x_0 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix}\n$$\nThe norm of the initial error is:\n$$\n\\|e_0\\| = \\|x - x_0\\| = \\sqrt{(-9)^2 + 1^2} = \\sqrt{81 + 1} = \\sqrt{82}\n$$\nThe error after the first iteration is:\n$$\ne_1 = x - x_1 = \\begin{pmatrix} -9 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 6/61 \\\\ 6/61 \\end{pmatrix} = \\begin{pmatrix} -9 - \\frac{6}{61} \\\\ 1 - \\frac{6}{61} \\end{pmatrix} = \\begin{pmatrix} -\\frac{549+6}{61} \\\\ \\frac{61-6}{61} \\end{pmatrix} = \\begin{pmatrix} -555/61 \\\\ 55/61 \\end{pmatrix}\n$$\nThe norm of this error vector is:\n$$\n\\|e_1\\| = \\|x - x_1\\| = \\sqrt{\\left(-\\frac{555}{61}\\right)^2 + \\left(\\frac{55}{61}\\right)^2} = \\frac{1}{61}\\sqrt{(-555)^2 + 55^2} = \\frac{1}{61}\\sqrt{308025 + 3025} = \\frac{\\sqrt{311050}}{61}\n$$\nFinally, we compute the required ratio:\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} = \\frac{\\sqrt{311050}/61}{\\sqrt{82}} = \\sqrt{\\frac{311050}{61^2 \\cdot 82}} = \\sqrt{\\frac{311050}{3721 \\cdot 82}} = \\sqrt{\\frac{311050}{305122}}\n$$\nWe now evaluate this expression numerically:\n$$\n\\frac{\\|x - x_1\\|}{\\|x - x_0\\|} \\approx \\sqrt{1.0194280} \\approx 1.00966727\n$$\nThe problem asks for the answer to be rounded to four significant figures. Rounding $1.00966727$ gives $1.010$. The fact that the error norm increases is a known phenomenon for GMRES when applied to highly non-normal matrices, such as the one given in this problem. While the residual norm is guaranteed to be monotonically non-increasing, the error norm is not.",
            "answer": "$$\\boxed{1.010}$$"
        },
        {
            "introduction": "Having explored the core mechanics on small systems, it's time to bridge the gap to real-world applications where performance is key. This final practice  is a comprehensive coding exercise where you will implement preconditioned GMRES, a vital technique for accelerating convergence. You will investigate the crucial differences between left and right preconditioning and their impact on performance by tracking the true residual norm, solidifying your understanding of how to effectively apply and analyze GMRES in practical computational settings.",
            "id": "2398732",
            "problem": "You are given square, non-singular real matrices $A \\in \\mathbb{R}^{n \\times n}$, right-hand side vectors $b \\in \\mathbb{R}^{n}$, and non-singular real preconditioners $M \\in \\mathbb{R}^{n \\times n}$. Consider the linear system $A x = b$. Define left preconditioning as solving $M^{-1} A x = M^{-1} b$ for $x$, and right preconditioning as solving $A M^{-1} y = b$ for $y$ followed by $x = M^{-1} y$. For each case, starting from the zero vector as initial guess, generate the sequence of approximations that minimize the Euclidean norm of the residual over the corresponding Krylov subspace at each iteration. At every iteration $k$, compute the true residual vector $r_k = b - A x_k$ and its Euclidean norm $\\|r_k\\|_2$. \n\nYour task is to implement a program that, for each specified test case below, produces:\n- The Euclidean norm of the difference between the final left-preconditioned and right-preconditioned approximate solutions, i.e., $\\|x^{(L)}_{\\text{final}} - x^{(R)}_{\\text{final}}\\|_2$,\n- The full history of true residual norms $\\{\\|r_k^{(L)}\\|_2\\}_{k=1}^{K_L}$ for the left-preconditioned formulation,\n- The full history of true residual norms $\\{\\|r_k^{(R)}\\|_2\\}_{k=1}^{K_R}$ for the right-preconditioned formulation,\n\nwhere $K_L$ and $K_R$ are the actual numbers of iterations needed to meet the stopping criterion for each formulation. Use the stopping criterion $\\|r_k\\|_2 \\le \\tau \\|b\\|_2$ with relative tolerance $\\tau = 10^{-10}$, or terminate if the iteration count reaches the matrix dimension $n$. All angles, where applicable, must be in radians.\n\nTest suite (use exactly these specifications):\n\n- Test case $1$ (symmetric positive definite, Jacobi preconditioner, manufactured solution):\n  - Size: $n = 20$.\n  - Matrix $A$: tri-diagonal with main diagonal entries $2$, and first sub- and super-diagonals entries $-1$ (standard $1$-D Poisson stencil with homogeneous Dirichlet boundaries).\n  - Preconditioner $M$: diagonal of $A$.\n  - True solution $x^{\\star}$ with entries $x^{\\star}_i = \\sin\\left(\\frac{\\pi i}{n+1}\\right)$ for $i = 1, 2, \\dots, n$ (angles in radians).\n  - Right-hand side $b = A x^{\\star}$.\n\n- Test case $2$ (nonsymmetric advection-diffusion, Jacobi preconditioner):\n  - Size: $n = 30$.\n  - Grid spacing $h = \\frac{1}{n+1}$.\n  - Parameters: diffusion $\\varepsilon = 10^{-2}$, advection speed $c = 1$.\n  - Matrix $A = \\frac{\\varepsilon}{h^2} T_{\\text{diff}} + \\frac{c}{h} T_{\\text{adv}}$, where $T_{\\text{diff}}$ is tri-diagonal with main diagonal $2$, sub- and super-diagonals $-1$, and $T_{\\text{adv}}$ corresponds to the backward difference with main diagonal $-1$ and first sub-diagonal $+1$ (zeros elsewhere).\n  - Preconditioner $M$: diagonal of $A$.\n  - Right-hand side $b$ is the vector of ones in $\\mathbb{R}^n$.\n\n- Test case $3$ (identity preconditioner boundary case):\n  - Same $A$ and $b$ as in Test case $1$.\n  - Preconditioner $M = I$ (the $n \\times n$ identity matrix).\n\nNumerical specifications:\n- Initial guess for all runs: $x_0 = 0$.\n- Use relative tolerance $\\tau = 10^{-10}$ as defined above.\n- Maximum number of iterations $k_{\\max} = n$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of the form $[\\|x^{(L)}_{\\text{final}} - x^{(R)}_{\\text{final}}\\|_2, \\text{residuals\\_left}, \\text{residuals\\_right}]$, where $\\text{residuals\\_left}$ and $\\text{residuals\\_right}$ are lists of floating-point residual norms at each iteration.\n- Concretely, the final output must look like a Python-style list of length $3$, whose elements correspond to Test cases $1$, $2$, and $3$ respectively. For example, the printed structure must have the form\n  $[[d_1,[r_{1,1},\\dots,r_{1,K_L}],[s_{1,1},\\dots,s_{1,K_R}]], [d_2,[\\dots],[\\dots]], [d_3,[\\dots],[\\dots]]]$\n  with all numbers shown in decimal or scientific notation (no percentage signs).\n\nAll quantities are dimensionless. No external input should be read; all data must be constructed within the program. The required outputs are floats and lists of floats as specified above, and the program must adhere exactly to the final output format described in this statement.",
            "solution": "The problem requires the implementation of the Generalized Minimal Residual (GMRES) method for solving linear systems of the form $A x = b$, considering both left and right preconditioning with a given preconditioner $M$. For each test case, we must compute the difference between the final solutions obtained from left and right preconditioning, and track the history of the true residual norm, $\\|r_k\\|_2 = \\|b - A x_k\\|_2$, at each iteration $k$.\n\nThe foundation of the solution lies in a custom implementation of the GMRES algorithm, as standard library functions may not provide the necessary control over the stopping criterion, which is specified in terms of the true residual, not the preconditioned residual.\n\nThe GMRES method generates a sequence of approximations $x_k$ such that the Euclidean norm of the residual of the system being solved is minimized over the corresponding Krylov subspace of dimension $k$. The nature of this residual depends on the preconditioning strategy.\n\n**1. Preconditioning Strategies**\n\nLet the original system be $A x = b$. A preconditioner $M$ is a matrix that approximates $A$, but for which the system $Mz=d$ is much easier to solve than the original system.\n\n- **Left Preconditioning**: The system is transformed to $M^{-1} A x = M^{-1} b$. Let $\\tilde{A} = M^{-1}A$ and $\\tilde{b} = M^{-1}b$. GMRES is applied to $\\tilde{A}x = \\tilde{b}$. At iteration $k$, the method finds an iterate $x_k$ that minimizes the norm of the preconditioned residual, $\\|\\tilde{r}_k\\|_2 = \\|\\tilde{b} - \\tilde{A} x_k\\|_2 = \\|M^{-1}(b - A x_k)\\|_2 = \\|M^{-1}r_k\\|_2$, over the Krylov subspace $\\mathcal{K}_k(\\tilde{A}, \\tilde{r}_0)$. The problem demands monitoring the true residual norm, $\\|r_k\\|_2$. This norm is not directly available from the GMRES minimization process for the preconditioned system. Therefore, at each iteration $k$, the iterate $x_k$ must be explicitly constructed to compute $\\|r_k\\|_2 = \\|b - A x_k\\|_2$.\n\n- **Right Preconditioning**: The system is transformed by a change of variables. Let $x = M^{-1}y$. The system becomes $A M^{-1} y = b$. Let $\\hat{A} = AM^{-1}$. GMRES is applied to $\\hat{A}y = b$. At iteration $k$, the method finds an iterate $y_k$ that minimizes $\\|\\hat{r}_k\\|_2 = \\|b - \\hat{A} y_k\\|_2$ over the Krylov subspace $\\mathcal{K}_k(\\hat{A}, \\hat{r}_0)$. The corresponding solution for the original system is $x_k = M^{-1}y_k$. The true residual is $r_k = b - A x_k = b - A(M^{-1} y_k) = b - \\hat{A} y_k = \\hat{r}_k$. In this case, the residual minimized by GMRES is identical to the true residual. Consequently, the true residual norm $\\|r_k\\|_2$ is obtained economically at each step of the GMRES algorithm. The final solution is found by computing $x_{\\text{final}} = M^{-1}y_{\\text{final}}$.\n\n**2. GMRES Algorithm Implementation**\n\nThe core of the method is the Arnoldi iteration, which builds an orthonormal basis $V_{k+1} = [v_1, \\dots, v_{k+1}]$ for the Krylov subspace $\\mathcal{K}_{k+1}(C, r_0)$, where $C$ is the operator of the system being solved ($M^{-1}A$ or $AM^{-1}$) and $r_0$ is the initial residual. We use the Modified Gram-Schmidt process for its numerical stability. This process yields the relation $C V_k = V_{k+1} \\bar{H}_k$, where $\\bar{H}_k$ is a $(k+1) \\times k$ upper-Hessenberg matrix.\n\nThe GMRES iterate $x_k$ is expressed as $x_k = x_0 + V_k y_k$ (for left preconditioning) or $y_k = y_0 + V_k z_k$ (for right preconditioning), where $y_k$ or $z_k$ is the solution to the small least-squares problem:\n$$ \\min_{z \\in \\mathbb{R}^k} \\|\\beta e_1 - \\bar{H}_k z\\|_2 $$\nwhere $\\beta = \\|r_0\\|_2$ and $e_1 = [1, 0, \\dots, 0]^T \\in \\mathbb{R}^{k+1}$. This problem is solved efficiently by applying a sequence of Givens rotations to transform $\\bar{H}_k$ into an upper-triangular matrix $R_k$ and updating the right-hand side vector $\\beta e_1$ to a vector $g_k$. The residual norm at iteration $k$ is given by the magnitude of the $(k+1)$-th component of the transformed right-hand side vector.\n\nMy implementation of GMRES proceeds as follows:\n1. Initialize with $x_0 = 0$. Determine the system operator $C$ and initial residual $r_0$ based on the preconditioning type.\n2. Normalize the initial residual to get the first basis vector $v_1 = r_0 / \\|r_0\\|_2$.\n3. For each iteration $k = 1, 2, \\dots, n$:\n    a. Generate the next basis vector $v_{k+1}$ and the $k$-th column of the Hessenberg matrix $\\bar{H}_k$ using the Arnoldi process with Modified Gram-Schmidt.\n    b. Update the QR factorization of $\\bar{H}_k$ using Givens rotations. This updates the triangular matrix $R_{k-1}$ to $R_k$ and the transformed right-hand side $g_{k-1}$ to $g_k$.\n    c. For right preconditioning (and no preconditioning), the true residual norm $\\|r_k\\|_2$ is simply the magnitude of the last element of $g_k$, $|g_{k+1}|$.\n    d. For left preconditioning, the true residual norm must be computed explicitly. We solve the triangular system $R_k y_k = g_k(1:k)$ for $y_k$, form the solution iterate $x_k = x_0 + V_k y_k$, and then compute $\\|r_k\\|_2 = \\|b - A x_k\\|_2$.\n    e. Store the computed true residual norm $\\|r_k\\|_2$.\n    f. Check the stopping criterion: $\\|r_k\\|_2 \\le \\tau \\|b\\|_2$. If satisfied, or if $k=n$, the iteration terminates.\n4. After the loop terminates at iteration $K$, the final solution is computed by solving the final triangular system for the coefficients and forming the linear combination of basis vectors. For the right-preconditioned case, an additional back-transformation $x_{\\text{final}} = M^{-1} y_{\\text{final}}$ is required.\n\n**3. Test Case Construction**\n\nThe test cases are constructed as specified:\n- **Case 1**: A symmetric positive definite system from a $1$-D Poisson problem. $A$ is a tridiagonal matrix with entries $(-1, 2, -1)$. The size is $n=20$. The preconditioner $M$ is the diagonal of $A$, i.e., $M = 2I$. The right-hand side $b$ is manufactured from a known solution $x^{\\star}_i = \\sin\\left(\\frac{\\pi i}{n+1}\\right)$ such that $b = A x^{\\star}$.\n- **Case 2**: A nonsymmetric system from a $1$-D advection-diffusion problem. The matrix is $A = \\frac{\\varepsilon}{h^2} T_{\\text{diff}} + \\frac{c}{h} T_{\\text{adv}}$ with $n=30$, $\\varepsilon=10^{-2}$, $c=1$, and $h=\\frac{1}{n+1}$. $T_{\\text{diff}}$ is $(-1, 2, -1)$ and $T_{\\text{adv}}$ is $(1, -1, 0)$ on the sub, main, and super diagonals, respectively. $M$ is the diagonal of the resulting $A$. The vector $b$ consists of all ones.\n- **Case 3**: Same as Case 1, but with the identity preconditioner $M=I$. This is equivalent to unpreconditioned GMRES.\n\nThe code implements these steps, calculates the required quantities for each test case, and formats the output as a single-line string.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef gmres_custom(A, b, precon_type='none', M_inv=None, tol=1e-10, maxiter=None):\n    \"\"\"\n    Custom GMRES implementation to solve Ax=b.\n\n    This implementation provides fine-grained control over the stopping criterion,\n    which is based on the true residual norm ||b - A*x_k||_2, and records its history.\n    \"\"\"\n    n = A.shape[0]\n    if maxiter is None:\n        maxiter = n\n    \n    x0 = np.zeros(n)\n    res_hist = []\n\n    # System setup based on preconditioning type\n    if precon_type == 'left':\n        op = lambda v: M_inv(A @ v)\n        r_start = M_inv(b - A @ x0)\n    elif precon_type == 'right':\n        # We solve A M^-1 y = b for y, and then x = M^-1 y.\n        # Initial guess y0 = M x0. Since x0=0, y0=0.\n        op = lambda v: A @ M_inv(v)\n        r_start = b - op(np.zeros(n))\n    else: # 'none'\n        op = lambda v: A @ v\n        r_start = b - A @ x0\n    \n    b_norm = np.linalg.norm(b)\n    stop_thresh = tol * b_norm\n\n    r_norm = np.linalg.norm(r_start)\n    if r_norm == 0:\n        return x0, []\n\n    V = np.zeros((n, maxiter + 1))\n    H = np.zeros((maxiter + 1, maxiter))\n    \n    V[:, 0] = r_start / r_norm\n    \n    s = np.zeros(maxiter + 1)\n    s[0] = r_norm\n    \n    cs = np.zeros(maxiter)\n    sn = np.zeros(maxiter)\n\n    k = -1\n    for k in range(maxiter):\n        # Arnoldi process with Modified Gram-Schmidt\n        w = op(V[:, k])\n        for j in range(k + 1):\n            H[j, k] = np.dot(w, V[:, j])\n            w -= H[j, k] * V[:, j]\n        \n        H[k+1, k] = np.linalg.norm(w)\n\n        # Apply previous Givens rotations to new column of H\n        for j in range(k):\n            temp = cs[j] * H[j, k] + sn[j] * H[j+1, k]\n            H[j+1, k] = -sn[j] * H[j, k] + cs[j] * H[j+1, k]\n            H[j, k] = temp\n        \n        # Calculate new Givens rotation\n        denom = np.sqrt(H[k, k]**2 + H[k+1, k]**2)\n        if denom == 0:\n            cs[k] = 1.0\n            sn[k] = 0.0\n        else:\n            cs[k] = H[k, k] / denom\n            sn[k] = H[k+1, k] / denom\n        \n        # Apply new rotation\n        H[k, k] = cs[k] * H[k, k] + sn[k] * H[k+1, k]\n        H[k+1, k] = 0.0\n        \n        s[k+1] = -sn[k] * s[k]\n        s[k] = cs[k] * s[k]\n        \n        # Calculate true residual and check for convergence\n        true_res_norm = 0\n        if precon_type == 'left':\n            y = scipy.linalg.solve_triangular(H[:k+1, :k+1], s[:k+1], check_finite=False)\n            x_k = x0 + V[:, :k+1] @ y\n            true_res_norm = np.linalg.norm(b - A @ x_k)\n        else: # 'right' or 'none'\n            true_res_norm = abs(s[k+1])\n        \n        res_hist.append(true_res_norm)\n        \n        if true_res_norm < stop_thresh or H[k+1, k] == 0:\n            break\n    \n    # After loop, k is the last iteration index (0-based)\n    # Total iterations = k + 1\n    num_iters = k + 1\n\n    # Solve for the final solution\n    y = scipy.linalg.solve_triangular(H[:num_iters, :num_iters], s[:num_iters], check_finite=False)\n    \n    if precon_type == 'right':\n        y_final = V[:, :num_iters] @ y\n        x_final = M_inv(y_final)\n    else: # 'left' or 'none'\n        x_final = x0 + V[:, :num_iters] @ y\n        \n    return x_final, res_hist\n\ndef setup_case_1():\n    n = 20\n    diag = np.full(n, 2.0)\n    off_diag = np.full(n - 1, -1.0)\n    A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n    \n    M_diag = np.diag(A)\n    M_inv = lambda v: v / M_diag\n    \n    i = np.arange(1, n + 1)\n    x_true = np.sin(np.pi * i / (n + 1))\n    b = A @ x_true\n    \n    return A, b, M_inv, n\n\ndef setup_case_2():\n    n = 30\n    h = 1.0 / (n + 1)\n    eps = 1e-2\n    c = 1.0\n\n    T_diff = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    T_adv = np.diag(np.full(n, -1.0)) + np.diag(np.full(n - 1, 1.0), k=-1)\n    \n    A = (eps / h**2) * T_diff + (c / h) * T_adv\n    \n    M_diag = np.diag(A)\n    M_inv = lambda v: v / M_diag\n    \n    b = np.ones(n)\n\n    return A, b, M_inv, n\n\ndef setup_case_3():\n    A, b, _, n = setup_case_1()\n    M_inv = lambda v: v # Identity preconditioner\n    return A, b, M_inv, n\n\ndef solve():\n    test_cases_setup = [setup_case_1, setup_case_2, setup_case_3]\n    results = []\n    \n    for setup_func in test_cases_setup:\n        A, b, M_inv, n = setup_func()\n        tau = 1e-10\n\n        # Left preconditioning\n        x_L, res_L = gmres_custom(A, b, precon_type='left', M_inv=M_inv, tol=tau, maxiter=n)\n        \n        # Right preconditioning\n        x_R, res_R = gmres_custom(A, b, precon_type='right', M_inv=M_inv, tol=tau, maxiter=n)\n\n        # Calculate norm of difference\n        diff_norm = np.linalg.norm(x_L - x_R)\n\n        results.append([diff_norm, res_L, res_R])\n\n    # Final print statement in the exact required format (list of lists, no spaces)\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}