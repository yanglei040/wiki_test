{
    "hands_on_practices": [
        {
            "introduction": "The best way to solidify your understanding of an algorithm is to execute its steps yourself. This first practice exercise guides you through a complete, single iteration of the Conjugate Gradient (CG) method. By starting with an initial guess of zero, you will compute the initial residual, determine the optimal step size, and find the first updated approximation to the solution, putting all the fundamental components of the algorithm into action .",
            "id": "1393666",
            "problem": "Consider the linear system of equations $Ax=b$, where the matrix $A$ and the vector $b$ are given by:\n$$\nA = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}\n$$\nThe matrix $A$ is symmetric and positive-definite.\n\nStarting with an initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one iteration of the Conjugate Gradient method to find the first updated solution, $x_1$.\n\nExpress your answer as a row matrix containing the components of $x_1$ as exact fractions.",
            "solution": "We apply the Conjugate Gradient method for a symmetric positive-definite matrix $A$ starting from $x_{0}$. The standard first-iteration formulas are:\n$$\nr_{0} = b - A x_{0}, \\quad p_{0} = r_{0}, \\quad \\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}}, \\quad x_{1} = x_{0} + \\alpha_{0} p_{0}.\n$$\nGiven $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}$, compute\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}, \\quad p_{0} = r_{0} = \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix}.\n$$\nNext, compute $A p_{0}$:\n$$\nA p_{0} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 4 + (-1)(-3) \\\\ (-1) \\cdot 4 + 3 \\cdot (-3) \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ -13 \\end{pmatrix}.\n$$\nCompute the scalar products:\n$$\nr_{0}^{T} r_{0} = 4^{2} + (-3)^{2} = 16 + 9 = 25, \\quad p_{0}^{T} A p_{0} = r_{0}^{T} (A p_{0}) = 4 \\cdot 11 + (-3) \\cdot (-13) = 44 + 39 = 83.\n$$\nThus,\n$$\n\\alpha_{0} = \\frac{r_{0}^{T} r_{0}}{p_{0}^{T} A p_{0}} = \\frac{25}{83}.\n$$\nUpdate the solution:\n$$\nx_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{25}{83} \\begin{pmatrix} 4 \\\\ -3 \\end{pmatrix} = \\begin{pmatrix} \\frac{100}{83} \\\\ -\\frac{75}{83} \\end{pmatrix}.\n$$\nExpressed as a row matrix, the components of $x_{1}$ are $\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{100}{83} & -\\frac{75}{83} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While the Conjugate Gradient method is guaranteed to find the exact solution for an $n \\times n$ system in at most $n$ steps, its convergence can be much faster under special conditions. This exercise explores a fascinating scenario where the method converges in a single step . Investigating this hypothetical case reveals a deep connection between the initial error and the eigenvectors of the system matrix $A$, providing profound insight into the geometry of the solution space and the mechanics of CG convergence.",
            "id": "1393668",
            "problem": "The conjugate gradient (CG) method is an iterative algorithm for solving systems of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a symmetric and positive-definite matrix. The performance of the method can depend on the choice of the initial guess, $\\mathbf{x}_0$.\n\nConsider the linear system defined by the matrix $A = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$ and the vector $\\mathbf{b} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}$. The CG method is initialized with a starting vector of the form $\\mathbf{x}_0 = \\begin{pmatrix} 1 \\\\ y_0 \\end{pmatrix}$, where $y_0$ is a real number.\n\nFind all possible values of $y_0$ for which the CG method converges to the exact solution in a single iteration. This means the first iterate, $\\mathbf{x}_1$, is the exact solution. You may assume that the initial guess $\\mathbf{x}_0$ is not the exact solution itself. Calculate the sum of all such valid values of $y_0$.",
            "solution": "The conjugate gradient method for symmetric positive-definite $A$ starts with $x_{0}$, residual $r_{0}=b-Ax_{0}$, search direction $p_{0}=r_{0}$, step size\n$$\n\\alpha_{0}=\\frac{r_{0}^{T}r_{0}}{p_{0}^{T}Ap_{0}},\n$$\nand update $x_{1}=x_{0}+\\alpha_{0}p_{0}$. The exact solution $x^{*}$ satisfies $Ax^{*}=b$. Define the initial error $e_{0}=x^{*}-x_{0}$. Then\n$$\nr_{0}=b-Ax_{0}=Ax^{*}-Ax_{0}=A(x^{*}-x_{0})=Ae_{0}.\n$$\nConvergence in one iteration means $x_{1}=x^{*}$, i.e.,\n$$\nx^{*}=x_{0}+\\alpha_{0}r_{0}\\quad\\Longleftrightarrow\\quad e_{0}=\\alpha_{0}r_{0}=\\alpha_{0}Ae_{0}.\n$$\nThus $Ae_{0}=(1/\\alpha_{0})e_{0}$, so $e_{0}$ must be an eigenvector of $A$. Conversely, if $Ae_{0}=\\lambda e_{0}$, then $r_{0}=\\lambda e_{0}$ and\n$$\n\\alpha_{0}=\\frac{r_{0}^{T}r_{0}}{r_{0}^{T}Ar_{0}}=\\frac{\\lambda^{2}e_{0}^{T}e_{0}}{\\lambda^{3}e_{0}^{T}e_{0}}=\\frac{1}{\\lambda},\n$$\nwhich gives $x_{1}=x_{0}+\\alpha_{0}r_{0}=x_{0}+(1/\\lambda)\\lambda e_{0}=x^{*}$. Therefore, the necessary and sufficient condition for one-step convergence is that $e_{0}$ be an eigenvector of $A$.\n\nCompute $x^{*}$ for $A=\\begin{pmatrix}2 & -1\\\\ -1 & 2\\end{pmatrix}$ and $b=\\begin{pmatrix}1\\\\ 3\\end{pmatrix}$. Solve\n$$\n\\begin{cases}\n2x_{1}-x_{2}=1,\\\\\n-x_{1}+2x_{2}=3,\n\\end{cases}\n$$\nwhich yields $x_{1}=5/3$, $x_{2}=7/3$, hence $x^{*}=\\begin{pmatrix}5/3\\\\ 7/3\\end{pmatrix}$.\n\nWith $x_{0}=\\begin{pmatrix}1\\\\ y_{0}\\end{pmatrix}$, the initial error is\n$$\ne_{0}=x^{*}-x_{0}=\\begin{pmatrix}5/3-1\\\\ 7/3-y_{0}\\end{pmatrix}=\\begin{pmatrix}2/3\\\\ 7/3-y_{0}\\end{pmatrix}.\n$$\nThe eigenvalues of $A$ are $\\lambda_{1}=1$ with eigenvector $v_{1}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$ and $\\lambda_{2}=3$ with eigenvector $v_{2}=\\begin{pmatrix}1\\\\ -1\\end{pmatrix}$. Thus $e_{0}$ must be parallel to $v_{1}$ or $v_{2}$.\n\nCase 1: $e_{0}=c\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$. Then $2/3=c$ and $7/3-y_{0}=c$, so $y_{0}=7/3-2/3=5/3$.\n\nCase 2: $e_{0}=c\\begin{pmatrix}1\\\\ -1\\end{pmatrix}$. Then $2/3=c$ and $7/3-y_{0}=-c=-2/3$, so $y_{0}=7/3+2/3=3$.\n\nBoth values yield $e_{0}\\neq 0$, so $x_{0}\\neq x^{*}$ as required. The sum of all such $y_{0}$ is\n$$\n\\frac{5}{3}+3=\\frac{5}{3}+\\frac{9}{3}=\\frac{14}{3}.\n$$",
            "answer": "$$\\boxed{\\frac{14}{3}}$$"
        },
        {
            "introduction": "A critical aspect of applying any numerical algorithm is understanding its limitations. The Conjugate Gradient method's robustness is guaranteed for symmetric positive-definite (SPD) matrices, but what happens when this crucial assumption is not met? This advanced computational practice challenges you to implement the CG algorithm and observe its behavior on matrices that are indefinite or singular, identifying the exact iteration where the method breaks down due to non-positive curvature . This is essential for diagnosing problems in real-world applications where the properties of the system matrix may not be known beforehand.",
            "id": "2382410",
            "problem": "Implement a program that, for each provided symmetric matrix and right-hand side vector, runs the Conjugate Gradient (CG) method starting from the zero vector and identifies the first iteration index at which the denominator used to compute the next step length is nonpositive. Specifically, at iteration index $k$ (with $k$ starting at $0$), define the search direction $p_k$ and evaluate the quadratic form $p_k^{\\mathsf{T}} A p_k$. The algorithm is said to break down at the smallest $k$ such that $p_k^{\\mathsf{T}} A p_k \\le 0$. If no such index occurs within a prescribed maximum number of iterations, return $-1$.\n\nBackground and required reasoning base: Start from the quadratic minimization problem with a symmetric matrix $A$ and vector $b$, namely the energy functional $\\phi(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The Conjugate Gradient (CG) method can be understood as producing iterates $x_k$ by line-search minimization of $\\phi(x_k + \\alpha p_k)$ along $A$-conjugate directions $p_k$, where the gradient is $\\nabla \\phi(x) = A x - b$ and the residual is defined as $r_k = b - A x_k$. In the strongly convex case (symmetric positive definite, SPD), every nonzero vector $v$ satisfies $v^{\\mathsf{T}} A v > 0$, which guarantees positive curvature and well-defined step lengths. In the indefinite or semidefinite case, directions can exist with $v^{\\mathsf{T}} A v \\le 0$, destroying the guarantee of a unique minimizer along $p_k$ and causing algorithmic breakdown. Your program must embody this principle by computing $p_k^{\\mathsf{T}} A p_k$ at each iteration and reporting the first $k$ where it is nonpositive.\n\nPrecise computational task: For each test case below, run the Conjugate Gradient iterations with the standard residual and direction recurrences, using the zero vector as the initial guess $x_0 = 0$. At each iteration, before computing the step length, evaluate $p_k^{\\mathsf{T}} A p_k$. If $p_k^{\\mathsf{T}} A p_k \\le 0$, report the current index $k$ for that test case and stop processing that case. If convergence is reached without encountering $p_k^{\\mathsf{T}} A p_k \\le 0$, or if no such event occurs within a maximum of $N_{\\max}$ iterations, report $-1$ for that case. Use a relative stopping test based on the residual norm $\\lVert r_k \\rVert_2 \\le \\varepsilon \\lVert b \\rVert_2$ with tolerance $\\varepsilon = 10^{-12}$. Use $N_{\\max} = 5n$, where $n$ is the dimension of $A$.\n\nTest suite to ensure coverage:\n- Case $\\mathbf{1}$ (SPD, no breakdown expected): \n  - $A_1 = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$,\n  - $b_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$.\n- Case $\\mathbf{2}$ (indefinite, delayed breakdown):\n  - $A_2 = \\begin{bmatrix} 2 & 1 \\\\ 1 & -2 \\end{bmatrix}$,\n  - $b_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- Case $\\mathbf{3}$ (singular positive semidefinite, zero curvature at start):\n  - $A_3 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$,\n  - $b_3 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n- Case $\\mathbf{4}$ (indefinite, immediate negative curvature):\n  - $A_4 = \\begin{bmatrix} 0 & 1 \\\\ 1 & 0 \\end{bmatrix}$,\n  - $b_4 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n\nAlgorithmic conventions:\n- Initialize $x_0 = \\mathbf{0}$, $r_0 = b - A x_0 = b$, and $p_0 = r_0$.\n- At iteration $k$, evaluate $p_k^{\\mathsf{T}} A p_k$. If $p_k^{\\mathsf{T}} A p_k \\le 0$, return $k$ for that test.\n- Otherwise, proceed with the Conjugate Gradient iteration and continue until convergence or $N_{\\max}$ iterations, whichever comes first.\n- If no breakdown occurs, return $-1$.\n\nFinal required output: Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of integers enclosed in square brackets, in the order of the cases above; that is, a single line of the form $[k_1,k_2,k_3,k_4]$, where $k_j$ is the first iteration index with $p_k^{\\mathsf{T}} A p_k \\le 0$ for case $j$, or $-1$ if no such index occurs within $N_{\\max}$ iterations for that case. No units are involved in this problem, and no angles or percentages are required.",
            "solution": "The problem requires the implementation of a modified Conjugate Gradient (CG) algorithm to detect breakdown when the system matrix $A$ is not symmetric positive definite (SPD). The breakdown is defined as the first iteration $k$ at which the quadratic form $p_k^{\\mathsf{T}} A p_k$ is non-positive. We will first review the theoretical foundation of the CG method and its failure modes, then detail the specific algorithm to be implemented, and finally analyze the provided test cases.\n\nThe Conjugate Gradient method is an iterative algorithm for solving systems of linear equations of the form $A x = b$, where the matrix $A$ is symmetric and positive definite. The method can be derived by considering the equivalent optimization problem of minimizing the quadratic energy functional:\n$$\n\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x\n$$\nThe gradient of this functional is $\\nabla \\phi(x) = A x - b$. The solution to $A x = b$ is the unique minimizer of $\\phi(x)$, where the gradient is zero. The negative of the gradient, $r = b - A x$, is defined as the residual.\n\nThe standard CG algorithm, starting from an initial guess $x_0$, generates a sequence of iterates $x_k$ that progressively minimize $\\phi(x)$. The update from $x_k$ to $x_{k+1}$ is performed by a line search along a carefully chosen search direction $p_k$. The search directions $\\{p_0, p_1, \\dots\\}$ are constructed to be $A$-conjugate (or $A$-orthogonal), meaning $p_i^{\\mathsf{T}} A p_j = 0$ for all $i \\neq j$.\n\nThe canonical algorithm for an $n \\times n$ SPD matrix $A$ is as follows:\nInitialize:\n$x_0 = \\mathbf{0}$\n$r_0 = b - A x_0 = b$\n$p_0 = r_0$\n\nFor $k = 0, 1, 2, \\dots$ until convergence:\n1.  Compute the step length $\\alpha_k$, which is the minimizer of $\\phi(x_k + \\alpha p_k)$:\n    $$\n    \\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}\n    $$\n2.  Update the solution estimate:\n    $$\n    x_{k+1} = x_k + \\alpha_k p_k\n    $$\n3.  Update the residual using a computationally efficient recurrence:\n    $$\n    r_{k+1} = r_k - \\alpha_k A p_k\n    $$\n4.  Compute the coefficient for the next search direction:\n    $$\n    \\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k}\n    $$\n5.  Update the search direction to be $A$-conjugate to previous directions:\n    $$\n    p_{k+1} = r_{k+1} + \\beta_k p_k\n    $$\n\nThe validity of this algorithm fundamentally relies on the matrix $A$ being SPD. The positive-definiteness ensures that for any non-zero vector $v$, the quadratic form $v^{\\mathsf{T}} A v > 0$. This guarantees that the denominator in the expression for $\\alpha_k$, namely $p_k^{\\mathsf{T}} A p_k$, is strictly positive for any non-zero search direction $p_k$. A positive denominator ensures that $\\alpha_k$ is well-defined and positive (since $r_k^{\\mathsf{T}} r_k > 0$ unless the solution is found), guaranteeing that each step is a descent step for the functional $\\phi(x)$.\n\nThe problem at hand concerns the behavior of the CG algorithm when $A$ is not SPD, i.e., when it is symmetric indefinite or positive semidefinite. In such cases, a search direction $p_k$ may be generated for which $p_k^{\\mathsf{T}} A p_k \\le 0$. This leads to an algorithmic breakdown.\n-   **Case 1: $p_k^{\\mathsf{T}} A p_k = 0$ (Zero Curvature).** This occurs if $A$ is positive semidefinite and $p_k$ is a direction of zero curvature (i.e., $A p_k$ is orthogonal to $p_k$). If $r_k \\neq 0$, the formula for the step length $\\alpha_k$ involves division by zero. The algorithm cannot proceed. This situation is encountered if $A$ is singular and a search direction is generated that has a component in the null space of $A$.\n-   **Case 2: $p_k^{\\mathsf{T}} A p_k < 0$ (Negative Curvature).** This occurs if $A$ is indefinite. A negative value for $p_k^{\\mathsf{T}} A p_k$ implies that the functional $\\phi(x)$ is unbounded below along the direction $p_k$. Minimization along this line is not possible, as $\\phi(x)$ decreases without bound as we move in the direction given by $\\text{sign}(\\alpha_k) p_k$. The CG algorithm, in its standard form, is not equipped to handle this and must terminate.\n\nThe task is to implement a modified CG procedure that explicitly checks for this breakdown condition. At each iteration $k$, starting with $k=0$, we must first compute $p_k^{\\mathsf{T}} A p_k$ and test its sign.\n\nThe specific algorithm is therefore:\nInitialize:\n$x_0 = \\mathbf{0}$, $r_0 = b$, $p_0 = r_0$. Let $n$ be the dimension of $A$. Set maximum iterations $N_{\\max} = 5n$ and tolerance $\\varepsilon = 10^{-12}$. Check initial convergence: if $\\lVert r_0 \\rVert_2 \\le \\varepsilon \\lVert b \\rVert_2$, terminate and report $-1$.\n\nFor $k = 0, 1, 2, \\dots, N_{\\max}-1$:\n1.  Compute the matrix-vector product $v_k = A p_k$.\n2.  Compute the quadratic form $d_k = p_k^{\\mathsf{T}} v_k = p_k^{\\mathsf{T}} A p_k$.\n3.  **Breakdown Check:** If $d_k \\le 0$, the algorithm has broken down. Report the current iteration index $k$ and terminate for this test case.\n4.  If no breakdown, proceed with the standard CG update:\n    $\\alpha_k = (r_k^{\\mathsf{T}} r_k) / d_k$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k v_k$\n5.  **Convergence Check:** If $\\lVert r_{k+1} \\rVert_2 \\le \\varepsilon \\lVert b \\rVert_2$, the algorithm has converged. Report $-1$ and terminate for this test case.\n6.  Compute $\\beta_k = (r_{k+1}^{\\mathsf{T}} r_{k+1}) / (r_k^{\\mathsf{T}} r_k)$.\n7.  Update the search direction: $p_{k+1} = r_{k+1} + \\beta_k p_k$.\n\nIf the loop completes without breakdown or convergence, report $-1$.\n\nWe now analyze the given test cases:\n-   **Case 1:** $A_1$ is an SPD matrix. Its eigenvalues are approximately $4.44$, $2.56$, and $2.0$. Since all are positive, $p_k^{\\mathsf{T}} A_1 p_k$ will always be positive for any non-zero $p_k$. No breakdown is expected. The algorithm should converge. The expected result is $-1$.\n-   **Case 2:** $A_2$ is an indefinite matrix with eigenvalues $\\pm\\sqrt{5}$. Breakdown is possible. For $k=0$, $p_0 = b_2 = [1, 1]^{\\mathsf{T}}$. Then $p_0^{\\mathsf{T}} A_2 p_0 = [1, 1] [2, 1; 1, -2] [1; 1] = 2 > 0$. The algorithm proceeds. In the next step, for $k=1$, a search direction $p_1$ is generated for which $p_1^{\\mathsf{T}} A_2 p_1 < 0$. Breakdown is expected at $k=1$.\n-   **Case 3:** $A_3$ is a singular positive semidefinite matrix with eigenvalues $1$ and $0$. Breakdown is possible. For $k=0$, $p_0 = b_3 = [0, 1]^{\\mathsf{T}}$. This vector is an eigenvector of $A_3$ corresponding to the eigenvalue $0$ (i.e., it is in the null space of $A_3$). Thus, $p_0^{\\mathsf{T}} A_3 p_0 = 0$. The breakdown condition $p_k^{\\mathsf{T}} A p_k \\le 0$ is met immediately. The expected result is $k=0$.\n-   **Case 4:** $A_4$ is an indefinite matrix with eigenvalues $\\pm 1$. Breakdown is possible. For $k=0$, $p_0 = b_4 = [1, -1]^{\\mathsf{T}}$. We compute $p_0^{\\mathsf{T}} A_4 p_0 = [1, -1] [0, 1; 1, 0] [1; -1] = -2 < 0$. The breakdown condition is met immediately. The expected result is $k=0$.\n\nThe implementation will follow this logic precisely for each case.\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the modified Conjugate Gradient\n    algorithm, and print the results in the specified format.\n    \"\"\"\n\n    def cg_breakdown_detector(A, b, epsilon, n_max):\n        \"\"\"\n        Runs the Conjugate Gradient method and detects breakdown.\n\n        Args:\n            A (np.ndarray): The symmetric matrix of the system.\n            b (np.ndarray): The right-hand side vector.\n            epsilon (float): The relative tolerance for convergence.\n            n_max (int): The maximum number of iterations.\n\n        Returns:\n            int: The iteration index k at which breakdown (p_k^T * A * p_k = 0)\n                 occurs. Returns -1 if the method converges or reaches n_max\n                 iterations without breakdown.\n        \"\"\"\n        # Initialize x_0 = 0, r_0 = b - A*x_0 = b, p_0 = r_0\n        x = np.zeros_like(b, dtype=float)\n        r = b.copy()\n        p = r.copy()\n\n        # Check for trivial case b = 0, where x = 0 is the exact solution.\n        b_norm = np.linalg.norm(b)\n        if b_norm == 0:\n            return -1  # Converged at k=0, no breakdown.\n\n        # Check for convergence at k=0\n        if np.linalg.norm(r) = epsilon * b_norm:\n            return -1 # Already converged, no breakdown.\n\n        # Precompute r_k^T * r_k for the first iteration\n        rs_old = np.dot(r, r)\n\n        for k in range(n_max):\n            # Compute A*p_k\n            Ap = A @ p\n\n            # The denominator of the step length alpha_k is p_k^T * A * p_k\n            pAp = np.dot(p, Ap)\n\n            # --- Breakdown Check ---\n            # This is the primary condition requested by the problem.\n            if pAp = 0:\n                return k  # Breakdown detected at iteration k.\n\n            # --- Standard CG Iteration ---\n            # Compute step length\n            alpha = rs_old / pAp\n\n            # Update solution and residual\n            x += alpha * p\n            r -= alpha * Ap\n\n            # --- Convergence Check ---\n            if np.linalg.norm(r) = epsilon * b_norm:\n                return -1  # Converged successfully, no breakdown.\n\n            # Update search direction\n            rs_new = np.dot(r, r)\n            beta = rs_new / rs_old\n            p = r + beta * p\n            \n            # Prepare for next iteration\n            rs_old = rs_new\n\n        # Reached n_max iterations without convergence or breakdown.\n        return -1\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[4, 1, 0], [1, 3, 0], [0, 0, 2]], dtype=float),\n            \"b\": np.array([1, 2, 3], dtype=float)\n        },\n        {\n            \"A\": np.array([[2, 1], [1, -2]], dtype=float),\n            \"b\": np.array([1, 1], dtype=float)\n        },\n        {\n            \"A\": np.array([[1, 0], [0, 0]], dtype=float),\n            \"b\": np.array([0, 1], dtype=float)\n        },\n        {\n            \"A\": np.array([[0, 1], [1, 0]], dtype=float),\n            \"b\": np.array([1, -1], dtype=float)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        n = A.shape[0]\n        \n        # Set algorithm parameters as specified\n        n_max = 5 * n\n        epsilon = 1e-12\n\n        result = cg_breakdown_detector(A, b, epsilon, n_max)\n        results.append(result)\n\n    # The problem asks for the output of the program.\n    # The final print statement in the original code is:\n    # print(f\"[{','.join(map(str, results))}]\")\n    # This function returns the calculated results to be placed in the answer tag.\n    return f\"[{','.join(map(str, results))}]\"\n\n# The call to solve() would produce the final answer string.\n# Based on the analysis, the results are [-1, 1, 0, 0].\n# The function would thus return \"[-1,1,0,0]\".\n# This is what goes into the answer tag.\n```\nThe python `solve()` function would print `[-1,1,0,0]`. The final step is to combine all this into the XML output.",
            "answer": "[-1,1,0,0]"
        }
    ]
}