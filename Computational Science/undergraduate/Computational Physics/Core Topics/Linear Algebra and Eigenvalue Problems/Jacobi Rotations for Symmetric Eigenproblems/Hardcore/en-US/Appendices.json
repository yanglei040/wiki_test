{
    "hands_on_practices": [
        {
            "introduction": "Before diving into implementation, it's crucial to sharpen our conceptual understanding of the Jacobi method. A common question is whether a symmetric matrix can be diagonalized in a fixed number of steps, one for each off-diagonal element. This exercise  challenges you to think critically about the effect of each rotation and understand why the Jacobi method is fundamentally an iterative, convergent process, not a direct one.",
            "id": "2405322",
            "problem": "In the Jacobi method for the symmetric eigenproblem, one applies successive plane rotations (Givens rotations) to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ as orthogonal similarity transformations $A \\leftarrow G^{\\mathsf{T}} A G$, where $G$ is an identity matrix modified in the $(p,q)$-plane by a $2 \\times 2$ rotation. Each such rotation can be chosen to annihilate a single off-diagonal entry $a_{pq}$ of the current matrix. Consider the specific case $n = 3$.\n\nStarting from the fundamental facts that any real symmetric matrix admits an orthonormal eigenbasis (hence an orthogonal diagonalization $A = Q D Q^{\\mathsf{T}}$ with $Q \\in \\mathbb{R}^{3 \\times 3}$ orthogonal and $D$ diagonal), and that a Jacobi rotation acting in the $(p,q)$-plane changes only the rows and columns $p$ and $q$ (thereby generally altering multiple off-diagonal entries), evaluate the following statements about the minimum number of Jacobi rotations needed to guarantee diagonalization of an arbitrary real symmetric $3 \\times 3$ matrix:\n\nChoose all statements that are true.\n\nA. Exactly $3$ Jacobi rotations always suffice, because there are exactly $3$ distinct off-diagonal entries in a $3 \\times 3$ symmetric matrix and each rotation can annihilate one of them.\n\nB. Three Jacobi rotations are not guaranteed to diagonalize an arbitrary $3 \\times 3$ symmetric matrix when each rotation is chosen to annihilate the current value of a single off-diagonal entry; subsequent rotations can reintroduce previously zeroed entries, so convergence typically requires iterative sweeps.\n\nC. There exists, for any real symmetric $3 \\times 3$ matrix, a choice of three plane rotations whose product $Q$ exactly diagonalizes $A$ by $Q^{\\mathsf{T}} A Q = D$, but the angles of these rotations are not, in general, the same as those obtained by the Jacobi rule of successively annihilating currently nonzero off-diagonal entries.\n\nD. The minimum number of Jacobi rotations that guarantees diagonalization of any real symmetric $3 \\times 3$ matrix is $5$.\n\nE. Without special structure in $A$, there is no fixed finite number $k$ such that performing $k$ Jacobi annihilations (according to the rule “choose an angle that zeros the current $a_{pq}$”) is guaranteed to produce an exactly diagonal matrix for all real symmetric $3 \\times 3$ inputs; rather, the method converges asymptotically over sweeps to a chosen tolerance.",
            "solution": "The problem statement is scientifically sound and well-posed. It concerns the standard Jacobi eigenvalue algorithm for real symmetric matrices, a fundamental topic in numerical linear algebra. The premises, including the existence of an orthogonal diagonalization and the effect of a plane rotation, are correct. The problem is valid for analysis.\n\nThe analysis hinges on understanding the effect of a single Jacobi rotation and the distinction between the iterative Jacobi method and direct diagonalization. A Jacobi rotation, represented by an orthogonal matrix $G$ (a Givens rotation), acts on a symmetric matrix $A$ via a similarity transformation, $A' = G^{\\mathsf{T}}AG$. For a rotation in the $(p,q)$-plane, the matrix $G$ is the identity matrix except for the entries $g_{pp} = g_{qq} = \\cos\\theta$ and $g_{pq} = -g_{qp} = \\sin\\theta$ for some angle $\\theta$. The angle $\\theta$ in the standard Jacobi method is chosen specifically to make the new off-diagonal element $a'_{pq}$ equal to zero. This is achieved when $\\cot(2\\theta) = \\frac{a_{pp} - a_{qq}}{2a_{pq}}$.\n\nThe key issue is that this transformation, while zeroing out $a_{pq}$, alters all other elements in rows $p$ and $q$ and columns $p$ and $q$. Specifically, for any $k \\neq p, q$, the new off-diagonal entries $a'_{pk}$ and $a'_{qk}$ are linear combinations of the old entries $a_{pk}$ and $a_{qk}$:\n$$ a'_{pk} = a_{pk} \\cos\\theta + a_{qk} \\sin\\theta $$\n$$ a'_{qk} = -a_{pk} \\sin\\theta + a_{qk} \\cos\\theta $$\nThis means that a subsequent Jacobi rotation, say to annihilate $a'_{pk}$, will generally destroy the zero that was just created at the $(p,q)$ position. This behavior is central to the analysis of the options.\n\nLet us consider the specific case of a $3 \\times 3$ real symmetric matrix $A$:\n$$ A = \\begin{pmatrix} a_{11}  a_{12}  a_{13} \\\\ a_{12}  a_{22}  a_{23} \\\\ a_{13}  a_{23}  a_{33} \\end{pmatrix} $$\nThe distinct off-diagonal entries are $a_{12}$, $a_{13}$, and $a_{23}$.\n\nA. Exactly $3$ Jacobi rotations always suffice, because there are exactly $3$ distinct off-diagonal entries in a $3 \\times 3$ symmetric matrix and each rotation can annihilate one of them.\n\nThis statement is **Incorrect**. The reasoning is naive. Suppose we perform three successive annihilations for $a_{12}$, $a_{13}$, and $a_{23}$.\n1. Annihilate $a_{12}$ using a rotation $G_{12}$. The new matrix $A' = G_{12}^{\\mathsf{T}} A G_{12}$ has $a'_{12} = 0$. However, the entries $a'_{13}$ and $a'_{23}$ are now different from the original $a_{13}$ and $a_{23}$.\n2. Annihilate $a'_{13}$ using a rotation $G_{13}$. The new matrix $A'' = G_{13}^{\\mathsf{T}} A' G_{13}$ has $a''_{13} = 0$. But this rotation acts on rows/columns $1$ and $3$. It will alter the $(1,2)$ entry, which was previously zeroed. The new entry $a''_{12}$ is given by $a''_{12} = a'_{12} \\cos\\theta_{13} + a'_{32} \\sin\\theta_{13} = 0 \\cdot \\cos\\theta_{13} + a'_{23} \\sin\\theta_{13}$. In general, $a'_{23} \\neq 0$ and $\\sin\\theta_{13} \\neq 0$, so $a''_{12}$ becomes non-zero.\nThe process of zeroing one off-diagonal element generally \"un-zeros\" others. Therefore, one cycle (or \"sweep\") of $3$ rotations is not sufficient for diagonalization.\n\nB. Three Jacobi rotations are not guaranteed to diagonalize an arbitrary $3 \\times 3$ symmetric matrix when each rotation is chosen to annihilate the current value of a single off-diagonal entry; subsequent rotations can reintroduce previously zeroed entries, so convergence typically requires iterative sweeps.\n\nThis statement is **Correct**. As demonstrated in the analysis of option A, the standard iterative Jacobi procedure, where each step annihilates a single off-diagonal element, suffers from the \"whack-a-mole\" effect. Zeroing an element in one step can (and generally does) create non-zero values in positions that were previously zeroed. For this reason, the Jacobi method is not a direct method that terminates in a fixed number of steps. Instead, it is an iterative algorithm that converges to a diagonal matrix over multiple \"sweeps,\" where a sweep consists of rotating all off-diagonal pairs.\n\nC. There exists, for any real symmetric $3 \\times 3$ matrix, a choice of three plane rotations whose product $Q$ exactly diagonalizes $A$ by $Q^{\\mathsf{T}} A Q = D$, but the angles of these rotations are not, in general, the same as those obtained by the Jacobi rule of successively annihilating currently nonzero off-diagonal entries.\n\nThis statement is **Correct**. It makes two distinct claims.\n1. Existence: The spectral theorem for real symmetric matrices guarantees the existence of an orthogonal matrix $Q$ such that $Q^{\\mathsf{T}}AQ = D$, where $D$ is diagonal. For $n=3$, any such orthogonal matrix $Q \\in O(3)$ can be decomposed into a product of at most three plane (Givens) rotations. This is a known result from linear algebra related to Euler angles and other similar rotation sequences (e.g., a Yaw-Pitch-Roll decomposition). Thus, a set of three rotations exists that can directly diagonalize $A$.\n2. Distinction from Jacobi method: The angles of these three rotations must be calculated simultaneously based on the *global* properties of the matrix $A$ (i.e., its eigenvectors). This is a difficult, non-linear problem. The standard Jacobi method, by contrast, uses a simple, *local* formula to compute each rotation angle based only on three elements ($a_{pp}, a_{qq}, a_{pq}$) of the *current* matrix. This greedy, local choice of angle is not the same as the globally determined angles required for direct diagonalization in three steps. Hence, the angles are different in general.\n\nD. The minimum number of Jacobi rotations that guarantees diagonalization of any real symmetric $3 \\times 3$ matrix is $5$.\n\nThis statement is **Incorrect**. There is no such fixed finite number for the standard iterative Jacobi method. The method is proven to converge, meaning the sum of the squares of the off-diagonal elements approaches zero as the number of rotations increases. However, for a general matrix, it will not reach exactly zero in a predetermined number of steps. The number of steps required to reach a certain tolerance depends on the matrix itself. The number $5$ is arbitrary and has no theoretical backing in this context.\n\nE. Without special structure in $A$, there is no fixed finite number $k$ such that performing $k$ Jacobi annihilations (according to the rule “choose an angle that zeros the current $a_{pq}$”) is guaranteed to produce an exactly diagonal matrix for all real symmetric $3 \\times 3$ inputs; rather, the method converges asymptotically over sweeps to a chosen tolerance.\n\nThis statement is **Correct**. It accurately describes the fundamental nature of the Jacobi method when applied to general matrices. It is an iterative, not a direct, method. While each step $A \\to G^{\\mathsf{T}}AG$ reduces the sum of squares of the off-diagonal elements, it does not reduce it by a fixed fraction, and certainly not to zero in a fixed number of steps. The process is one of asymptotic convergence. One performs rotations until the magnitude of all off-diagonal elements is smaller than a pre-defined tolerance $\\epsilon > 0$. Achieving an *exactly* diagonal matrix (tolerance $\\epsilon=0$) would, in general, require an infinite number of steps. Therefore, no finite $k$ can guarantee exact diagonalization for all possible input matrices.\n\nIn summary, statements B, C, and E are all true.\n- B correctly points out the flaw in assuming one sweep is enough.\n- E correctly generalizes this to any finite number of steps, describing the method's asymptotic nature.\n- C correctly introduces the subtle but important distinction between the *existence* of a 3-rotation direct solution and the *procedure* of the iterative Jacobi algorithm.",
            "answer": "$$\\boxed{BCE}$$"
        },
        {
            "introduction": "Having established the iterative nature of the Jacobi method, the next step is to bring the algorithm to life through code. In this practice , you will implement the Jacobi rotation algorithm from scratch to find the eigenvalues and eigenvectors of a symmetric matrix. This exercise will allow you to test the core predictions of the spectral theorem: the diagonality of the transformed matrix, the orthogonality of the eigenvector matrix, and the ability to reconstruct the original matrix from its eigen-decomposition.",
            "id": "2405358",
            "problem": "You are to write a complete program that verifies the spectral theorem for real symmetric matrices using Jacobi rotations, and quantitatively checks the orthogonality of the resulting eigenvectors. For each test case, construct a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ as follows: given an integer seed $s$ and real bounds $a_{\\min}$ and $a_{\\max}$, use a pseudorandom number generator initialized with seed $s$ to draw an $n \\times n$ matrix $M$ with entries independently sampled from the continuous uniform distribution on the interval $\\left[a_{\\min}, a_{\\max}\\right]$. Then define $A = \\dfrac{1}{2}\\left(M + M^{\\mathsf{T}}\\right)$ so that $A$ is real and symmetric. Your task is to compute an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ and a diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that $A \\approx Q \\Lambda Q^{\\mathsf{T}}$. Use a convergence tolerance $\\varepsilon = 10^{-10}$ with respect to the Frobenius norm of the off-diagonal part of $Q^{\\mathsf{T}} A Q$, and enforce a hard cap of $10^{6}$ similarity rotations. If any rotation angles are computed internally, they must be in radians. For each test case, report the following three real numbers:\n- $r_{\\mathrm{off}} = \\left\\| \\mathrm{offdiag}\\left(Q^{\\mathsf{T}} A Q\\right) \\right\\|_{F}$, where $\\mathrm{offdiag}(X)$ denotes the matrix $X$ with its diagonal entries set to zero and $\\|\\cdot\\|_{F}$ denotes the Frobenius norm,\n- $r_{\\mathrm{rec}} = \\left\\| A - Q \\Lambda Q^{\\mathsf{T}} \\right\\|_{F}$,\n- $r_{\\mathrm{orth}} = \\left\\| Q^{\\mathsf{T}} Q - I \\right\\|_{F}$, where $I$ is the identity matrix in $\\mathbb{R}^{n \\times n}$.\n\nAll three quantities must be reported as floating-point numbers formatted in scientific notation with exactly $10$ digits after the decimal point.\n\nThe test suite consists of the following parameter sets $(n, s, a_{\\min}, a_{\\max})$:\n- $(n, s, a_{\\min}, a_{\\max}) = (1, 1, -1, 1)$,\n- $(n, s, a_{\\min}, a_{\\max}) = (2, 2, -1, 1)$,\n- $(n, s, a_{\\min}, a_{\\max}) = (5, 7, -1, 1)$,\n- $(n, s, a_{\\min}, a_{\\max}) = (6, 11, -10, 10)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of per-test-case triplets, each triplet ordered as $\\left[r_{\\mathrm{off}}, r_{\\mathrm{rec}}, r_{\\mathrm{orth}}\\right]$ and the entire collection enclosed in square brackets. For example, an output with two cases would look like $[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}]]$, where each symbol $x_{i}, y_{i}, z_{i}$ denotes a floating-point number in the specified format. Ensure your exact output format is a single line with no additional text, using scientific notation for each number with exactly $10$ digits after the decimal point.",
            "solution": "The problem requires the numerical verification of the spectral theorem for real symmetric matrices using the Jacobi eigenvalue algorithm. The spectral theorem states that for any real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, there exists an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ and a real diagonal matrix $\\Lambda \\in \\mathbb{R}^{n \\times n}$ such that $A = Q \\Lambda Q^{\\mathsf{T}}$. The columns of $Q$ are the orthonormal eigenvectors of $A$, and the diagonal entries of $\\Lambda$ are the corresponding real eigenvalues.\n\nOur approach is to implement the Jacobi rotation method to compute the matrices $Q$ and $\\Lambda$ for a given symmetric matrix $A$. The matrix $A$ is constructed from a pseudorandom matrix $M$ with entries sampled from a uniform distribution $U(a_{\\min}, a_{\\max})$, ensuring its symmetry by defining $A = \\frac{1}{2}(M + M^{\\mathsf{T}})$.\n\nThe Jacobi method is an iterative algorithm that progressively reduces the magnitude of the off-diagonal elements of $A$ to zero through a sequence of similarity transformations. Each transformation is a Givens rotation, designed to annihilate a specific off-diagonal element $a_{pq}$.\nA single Jacobi rotation is a similarity transformation of the form $A' = J^{\\mathsf{T}} A J$, where $J \\equiv J(p, q, \\theta)$ is a rotation matrix in the $(p, q)$-plane. The matrix $J$ is an identity matrix except for four entries:\n$$\nJ_{pp} = \\cos\\theta, \\quad J_{qq} = \\cos\\theta \\\\\nJ_{pq} = \\sin\\theta, \\quad J_{qp} = -\\sin\\theta\n$$\nThe rotation angle $\\theta$ is chosen such that the element $a'_{pq}$ of the transformed matrix $A'$ becomes zero. This condition leads to the equation:\n$$\n\\cot(2\\theta) = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}\n$$\nFor numerical stability and efficiency, we avoid directly computing $\\theta$. Instead, we compute $t = \\tan\\theta$. Let $\\tau = \\frac{a_{qq} - a_{pp}}{2 a_{pq}}$. The equation for $t$ is $t^2 + 2\\tau t - 1 = 0$. We choose the root with the smaller magnitude to ensure $|\\theta| \\le \\pi/4$, which enhances stability. This root is given by:\n$$\nt = \\frac{\\operatorname{sgn}(\\tau)}{|\\tau| + \\sqrt{1 + \\tau^2}}\n$$\nfor $\\tau \\neq 0$. If $\\tau = 0$ (i.e., $a_{pp} = a_{qq}$), then $\\theta = \\pi/4$ and $t=1$. From $t$, we find $c = \\cos\\theta = 1/\\sqrt{1+t^2}$ and $s = \\sin\\theta = t \\cdot c$.\n\nThe algorithm proceeds as follows:\n1.  Initialize the matrix of eigenvectors $Q$ as the identity matrix $I \\in \\mathbb{R}^{n \\times n}$.\n2.  Iteratively apply rotations. We use a cyclic Jacobi approach, where in each \"sweep,\" we iterate through all unique off-diagonal pairs $(p, q)$ with $p  q$.\n3.  For each pair $(p, q)$, if $a_{pq}$ is not numerically zero, we compute $c$ and $s$ and update the matrices $A$ and $Q$:\n    -   $A \\leftarrow J^{\\mathsf{T}} A J$\n    -   $Q \\leftarrow Q J$\n    These updates only affect rows and columns $p$ and $q$ of the respective matrices. The eigenvector matrix $Q$ accumulates the product of all rotation matrices.\n4.  The process terminates when the Frobenius norm of the off-diagonal part of $A$, defined as $S = \\sqrt{\\sum_{i \\neq j} a_{ij}^2}$, falls below a specified tolerance $\\varepsilon = 10^{-10}$, or when a maximum number of rotations ($10^6$) is reached.\n\nUpon convergence, the matrix $A$ has been transformed into an approximately diagonal matrix $\\Lambda_{\\text{approx}}$, whose diagonal entries are the eigenvalues. The final matrix $Q$ contains the corresponding eigenvectors as its columns.\n\nTo verify the computed decomposition, we calculate three error metrics:\n1.  The off-diagonal error, $r_{\\mathrm{off}} = \\left\\| \\mathrm{offdiag}(Q^{\\mathsf{T}} A Q) \\right\\|_{F}$. This measures how close the transformed matrix is to being perfectly diagonal.\n2.  The reconstruction error, $r_{\\mathrm{rec}} = \\left\\| A - Q \\Lambda Q^{\\mathsf{T}} \\right\\|_{F}$, where $\\Lambda$ is the purely diagonal matrix formed from the diagonal of $Q^{\\mathsf{T}} A Q$. This quantifies how well the computed eigenvalues and eigenvectors reconstruct the original matrix $A$.\n3.  The orthogonality error, $r_{\\mathrm{orth}} = \\left\\| Q^{\\mathsf{T}} Q - I \\right\\|_{F}$. This measures how close the computed eigenvector matrix $Q$ is to being orthogonal.\n\nFor the trivial case $n=1$, the matrix $A = [a_{11}]$ is already diagonal. We have $Q=[1]$ and $\\Lambda=[a_{11}]$, and all three error metrics are exactly $0$. For all other cases, the Jacobi algorithm is applied.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the eigenvalues and eigenvectors of real symmetric matrices\n    using the Jacobi rotation method and verifies the spectral theorem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, s, a_min, a_max)\n        (1, 1, -1, 1),\n        (2, 2, -1, 1),\n        (5, 7, -1, 1),\n        (6, 11, -10, 10),\n    ]\n    \n    TOLERANCE = 1e-10\n    MAX_ROTATIONS = 1_000_000\n\n    results_all_cases = []\n    for n, s, a_min, a_max in test_cases:\n        \n        # Handle the trivial case n=1\n        if n == 1:\n            results_all_cases.append([0.0, 0.0, 0.0])\n            continue\n            \n        # 1. Construct the real symmetric matrix A\n        rng = np.random.default_rng(seed=s)\n        M = rng.uniform(low=a_min, high=a_max, size=(n, n))\n        A = 0.5 * (M + M.T)\n        A_orig = A.copy()\n        \n        # Initialize eigenvector matrix Q to identity\n        Q = np.identity(n, dtype=np.float64)\n\n        # 2. Jacobi Iteration\n        rotation_count = 0\n        max_sweeps = 100 # A generous limit for sweeps\n        \n        for sweep in range(max_sweeps):\n            # Calculate sum of squares of off-diagonal elements\n            off_diag_sq_sum = np.sum(A**2) - np.sum(np.diag(A)**2)\n            \n            # Check for convergence\n            if np.sqrt(off_diag_sq_sum)  TOLERANCE:\n                break\n            \n            # Perform a sweep through all off-diagonal elements\n            for p in range(n):\n                for q in range(p + 1, n):\n                    if rotation_count = MAX_ROTATIONS:\n                        break\n\n                    apq = A[p, q]\n                    # Skip rotation if element is already numerically zero\n                    if abs(apq)  1e-20:\n                        continue\n\n                    # Calculate rotation parameters c and s\n                    app = A[p, p]\n                    aqq = A[q, q]\n                    tau = (aqq - app) / (2.0 * apq)\n                    \n                    if tau == 0:\n                        t = 1.0\n                    else:\n                        t = np.sign(tau) / (abs(tau) + np.sqrt(1.0 + tau**2))\n                    \n                    c = 1.0 / np.sqrt(1.0 + t**2)\n                    s = c * t\n\n                    # Apply rotation to A (transform A - J^T * A * J)\n                    app_old = A[p, p]\n                    aqq_old = A[q, q]\n                    A[p, p] = c*c*app_old - 2*c*s*apq + s*s*aqq_old\n                    A[q, q] = s*s*app_old + 2*c*s*apq + c*c*aqq_old\n                    A[p, q] = 0.0\n                    A[q, p] = 0.0\n\n                    for i in range(n):\n                        if i != p and i != q:\n                            aip_old = A[i, p]\n                            aiq_old = A[i, q]\n                            A[i, p] = c * aip_old - s * aiq_old\n                            A[p, i] = A[i, p]\n                            A[i, q] = s * aip_old + c * aiq_old\n                            A[q, i] = A[i, q]\n\n                    # Apply rotation to Q (transform Q - Q * J)\n                    Q_col_p = Q[:, p].copy()\n                    Q_col_q = Q[:, q].copy()\n                    Q[:, p] = c * Q_col_p - s * Q_col_q\n                    Q[:, q] = s * Q_col_p + c * Q_col_q\n                    \n                    rotation_count += 1\n\n                if rotation_count = MAX_ROTATIONS:\n                    break\n            \n            if rotation_count = MAX_ROTATIONS:\n                break\n\n        # 3. Calculate final metrics\n        A_diag = A\n        \n        # r_off: Frobenius norm of the off-diagonal part of Q^T A Q\n        off_diag_part = A_diag - np.diag(np.diag(A_diag))\n        r_off = np.linalg.norm(off_diag_part, 'fro')\n\n        # r_rec: Frobenius norm of A - Q Lambda Q^T\n        Lambda = np.diag(np.diag(A_diag))\n        reconstruction = Q @ Lambda @ Q.T\n        r_rec = np.linalg.norm(A_orig - reconstruction, 'fro')\n        \n        # r_orth: Frobenius norm of Q^T Q - I\n        identity = np.identity(n, dtype=np.float64)\n        ortho_check = Q.T @ Q - identity\n        r_orth = np.linalg.norm(ortho_check, 'fro')\n        \n        results_all_cases.append([r_off, r_rec, r_orth])\n    \n    # Format and print the final results\n    output_str = \"[\"\n    for i, case_results in enumerate(results_all_cases):\n        res_str = f\"[{case_results[0]:.10e},{case_results[1]:.10e},{case_results[2]:.10e}]\"\n        output_str += res_str\n        if i  len(results_all_cases) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The power of numerical methods like the Jacobi algorithm lies in their ability to solve physics problems that lack simple analytical solutions. This exercise  demonstrates a prime application: finding the allowed energy levels of a quantum particle by solving the time-independent Schrödinger equation. By discretizing space, you will transform this differential equation into a matrix eigenvalue problem, making it solvable with the numerical techniques you've been studying.",
            "id": "2405321",
            "problem": "Given the one-dimensional time-independent Schrödinger eigenvalue problem in dimensionless form\n$$\n\\left(-\\frac{d^2}{dx^2} + V(x)\\right)\\psi(x) = E\\,\\psi(x),\n$$\non a closed interval $[x_{\\min},x_{\\max}]$ with Dirichlet boundary conditions $\\psi(x_{\\min})=0$ and $\\psi(x_{\\max})=0$, where the dimensionless choice of units enforces $\\hbar^2/(2m)=1$, compute approximations to the lowest eigenvalues $E$ for several specified potentials $V(x)$. The spatial domain and sampling resolution are prescribed by a uniform grid of $N$ points on $[x_{\\min},x_{\\max}]$. The sought outputs are the lowest $k$ eigenvalues for each case, expressed as dimensionless real numbers.\n\nFor each test case, the program must:\n- Interpret $[x_{\\min},x_{\\max}]$ as the spatial domain.\n- Use a uniform grid of $N$ points on $[x_{\\min},x_{\\max}]$.\n- Impose Dirichlet boundary conditions $\\psi(x_{\\min})=0$ and $\\psi(x_{\\max})=0$.\n- Treat $V(x)$ as given below for each case.\n- Return the lowest $k$ eigenvalues $E$ in ascending order for that case, each rounded to exactly $6$ decimal places.\n\nTest Suite:\n1. Case A (harmonic oscillator): $V(x) = \\tfrac{1}{2}\\,\\omega^2 x^2$, with $\\omega=1$. Domain: $[x_{\\min},x_{\\max}] = [-8,8]$. Grid size: $N=81$. Return the lowest $k=3$ eigenvalues.\n2. Case B (particle in a box): $V(x) = 0$. Domain: $[x_{\\min},x_{\\max}] = [0,1]$. Grid size: $N=81$. Return the lowest $k=3$ eigenvalues.\n3. Case C (symmetric double well): $V(x) = a\\,(x^2-b^2)^2$, with $a=25$ and $b=1$. Domain: $[x_{\\min},x_{\\max}] = [-2,2]$. Grid size: $N=81$. Return the lowest $k=2$ eigenvalues.\n4. Case D (harmonic oscillator, coarse grid): $V(x) = \\tfrac{1}{2}\\,\\omega^2 x^2$, with $\\omega=0.5$. Domain: $[x_{\\min},x_{\\max}] = [-6,6]$. Grid size: $N=41$. Return the lowest $k=1$ eigenvalue.\n\nFinal Output Format:\nYour program must produce a single line containing a JSON-like list of lists of floats with no spaces. Each inner list corresponds to one test case in the order A, B, C, D and contains the $k$ eigenvalues for that case, each rounded to exactly $6$ decimal places. For example, the format must be exactly like\n$[\\,[e_{A,1},e_{A,2},e_{A,3}],\\,[e_{B,1},e_{B,2},e_{B,3}],\\,[e_{C,1},e_{C,2}],\\,[e_{D,1}]\\,]$\nbut with numerical values and no spaces anywhere, e.g.,\n$[[0.123456,0.234567,0.345678],[...],[...],[...]]$.\n\nAll returned values are dimensionless; no physical units are to be reported. Angles are not involved, so no angle unit is required. The program must be self-contained and require no user input.",
            "solution": "The problem requires the computation of the lowest eigenvalues of the one-dimensional time-independent Schrödinger equation, given by\n$$\n\\hat{H}\\psi(x) = \\left(-\\frac{d^2}{dx^2} + V(x)\\right)\\psi(x) = E\\,\\psi(x)\n$$\nfor several potentials $V(x)$ on a finite domain $[x_{\\min}, x_{\\max}]$ with Dirichlet boundary conditions $\\psi(x_{\\min})=0$ and $\\psi(x_{\\max})=0$. The provided form of the equation implies a system of units where $\\hbar^2/(2m)=1$.\n\nTo solve this eigenvalue problem numerically, we employ the finite difference method. The continuous spatial domain $[x_{\\min}, x_{\\max}]$ is discretized into a uniform grid of $N$ points, denoted by $x_i = x_{\\min} + i \\cdot \\Delta x$ for $i = 0, 1, \\dots, N-1$. The grid spacing is $\\Delta x = (x_{\\max} - x_{\\min})/(N-1)$. The continuous wavefunction $\\psi(x)$ is approximated by its values at these grid points, $\\psi_i = \\psi(x_i)$.\n\nThe Dirichlet boundary conditions dictate that $\\psi_0 = \\psi(x_0) = 0$ and $\\psi_{N-1} = \\psi(x_{N-1}) = 0$. This leaves $M=N-2$ unknown values of the wavefunction, corresponding to the interior points $x_1, \\dots, x_{N-2}$.\n\nThe second derivative operator, $d^2/dx^2$, is approximated using a second-order central difference formula at each interior grid point $x_i$:\n$$\n\\frac{d^2\\psi}{dx^2}\\bigg|_{x=x_i} \\approx \\frac{\\psi(x_{i+1}) - 2\\psi(x_i) + \\psi(x_{i-1})}{(\\Delta x)^2} = \\frac{\\psi_{i+1} - 2\\psi_i + \\psi_{i-1}}{(\\Delta x)^2}\n$$\nSubstituting this approximation into the Schrödinger equation for an interior point $x_i$ gives a system of linear algebraic equations:\n$$\n-\\frac{\\psi_{i+1} - 2\\psi_i + \\psi_{i-1}}{(\\Delta x)^2} + V(x_i)\\psi_i = E \\psi_i\n$$\nThis equation can be rearranged for each $i \\in \\{1, 2, \\dots, M\\}$:\n$$\n-\\frac{1}{(\\Delta x)^2}\\psi_{i-1} + \\left(\\frac{2}{(\\Delta x)^2} + V(x_i)\\right)\\psi_i - \\frac{1}{(\\Delta x)^2}\\psi_{i+1} = E \\psi_i\n$$\nThis system of $M$ equations can be cast into a matrix eigenvalue problem, $\\mathbf{H}\\vec{\\psi} = E\\vec{\\psi}$, where $\\vec{\\psi} = (\\psi_1, \\psi_2, \\dots, \\psi_M)^T$ is the vector of the wavefunction values at the interior points, and $\\mathbf{H}$ is an $M \\times M$ matrix representing the discretized Hamiltonian operator.\n\nThe matrix $\\mathbf{H}$ is real, symmetric, and tridiagonal. Its elements are given by:\n$$\nH_{ij} = \\begin{cases} \\frac{2}{(\\Delta x)^2} + V(x_i)  \\text{if } i=j \\\\ -\\frac{1}{(\\Delta x)^2}  \\text{if } |i-j|=1 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nHere, the indices $i$ and $j$ run from $1$ to $M=N-2$. The potential $V(x_i)$ is evaluated at the interior grid points.\n\nThe problem of finding the energy eigenvalues $E$ is transformed into the problem of finding the eigenvalues of the matrix $\\mathbf{H}$. Since $\\mathbf{H}$ is a real symmetric matrix, its eigenvalues are all real, which is consistent with the physical requirement that energy eigenvalues are real. We use standard numerical linear algebra libraries, such as NumPy's `linalg.eigh` function, to efficiently compute these eigenvalues. This function is specifically designed for Hermitian (or real symmetric) matrices and returns the eigenvalues sorted in ascending order.\n\nFor each test case, we construct the corresponding Hamiltonian matrix $\\mathbf{H}$ based on the given potential $V(x)$, domain $[x_{\\min}, x_{\\max}]$, and grid size $N$. We then compute its eigenvalues and select the lowest $k$ values as required, rounding them to the specified precision. This procedure provides accurate numerical approximations to the low-lying energy levels of the quantum system.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 1D time-independent Schrödinger equation for a set of test cases\n    using the finite difference method.\n    \"\"\"\n\n    def solve_case(V_func, domain, N, k):\n        \"\"\"\n        Solves the 1D Schrödinger equation for a single given potential.\n        \n        Args:\n            V_func (callable): The potential function V(x).\n            domain (list or tuple): The spatial domain [xmin, xmax].\n            N (int): The number of grid points.\n            k (int): The number of lowest eigenvalues to return.\n            \n        Returns:\n            list: The lowest k eigenvalues, rounded to 6 decimal places.\n        \"\"\"\n        xmin, xmax = domain\n        \n        # Grid spacing\n        dx = (xmax - xmin) / (N - 1)\n        \n        # The number of interior points defines the matrix size\n        M = N - 2\n        \n        # Discretize the domain for interior points\n        x_interior = np.linspace(xmin + dx, xmax - dx, num=M, dtype=float)\n        \n        # Evaluate the potential at interior points\n        V_interior = V_func(x_interior)\n        \n        # Construct the Hamiltonian matrix H\n        # The kinetic energy part contributes to the main and off-diagonals.\n        # Main diagonal elements\n        diag = (2.0 / (dx**2)) + V_interior\n        \n        # Off-diagonal elements (constant for a uniform grid)\n        offdiag_val = -1.0 / (dx**2)\n        offdiag = np.full(M - 1, offdiag_val)\n        \n        # Create the tridiagonal matrix\n        H_matrix = np.diag(diag) + np.diag(offdiag, k=1) + np.diag(offdiag, k=-1)\n        \n        # Find eigenvalues. numpy.linalg.eigh is efficient for Hermitian (real symmetric)\n        # matrices and returns sorted eigenvalues and corresponding eigenvectors.\n        # We only need the eigenvalues, which are the first element of the returned tuple.\n        eigenvalues = np.linalg.eigh(H_matrix)[0]\n        \n        # Select the lowest k eigenvalues\n        lowest_k_eigenvalues = eigenvalues[:k]\n        \n        # Round the results to the specified precision\n        return [round(e, 6) for e in lowest_k_eigenvalues]\n\n    # Definition of the test suite as per the problem statement\n    test_cases = [\n        # Case A: Harmonic oscillator, V(x) = 0.5 * omega^2 * x^2, omega=1\n        {\n            \"V_func\": lambda x: 0.5 * (1.0**2) * x**2,\n            \"domain\": [-8.0, 8.0],\n            \"N\": 81,\n            \"k\": 3\n        },\n        # Case B: Particle in a box, V(x) = 0\n        {\n            \"V_func\": lambda x: np.zeros_like(x),\n            \"domain\": [0.0, 1.0],\n            \"N\": 81,\n            \"k\": 3\n        },\n        # Case C: Symmetric double well, V(x) = a * (x^2 - b^2)^2, a=25, b=1\n        {\n            \"V_func\": lambda x, a=25.0, b=1.0: a * (x**2 - b**2)**2,\n            \"domain\": [-2.0, 2.0],\n            \"N\": 81,\n            \"k\": 2\n        },\n        # Case D: Harmonic oscillator, coarse grid, V(x) = 0.5 * omega^2 * x^2, omega=0.5\n        {\n            \"V_func\": lambda x: 0.5 * (0.5**2) * x**2,\n            \"domain\": [-6.0, 6.0],\n            \"N\": 41,\n            \"k\": 1\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_case(case[\"V_func\"], case[\"domain\"], case[\"N\"], case[\"k\"])\n        all_results.append(result)\n\n    # Format the final output string into a JSON-like list of lists with no spaces\n    # Example: [[valA1,valA2],[valB1],[valC1,valC2,valC3]]\n    output_str = '[' + ','.join(['[' + ','.join(map(str, res)) + ']' for res in all_results]) + ']'\n    \n    print(output_str)\n\nsolve()\n```"
        }
    ]
}