{
    "hands_on_practices": [
        {
            "introduction": "To truly understand LU factorization, it is best to start with a concrete and familiar example. This first exercise challenges you to derive the analytical $LU$ decomposition of a two-dimensional rotation matrix, a transformation fundamental to many areas of physics. By working through the factorization algebraically, you will gain direct, hands-on experience with the steps of Gaussian elimination that underpin the decomposition and see how they translate into the structure of the $L$ and $U$ matrices .",
            "id": "2409827",
            "problem": "In computational physics, linear transformations in the plane are often represented by matrices whose factorization facilitates efficient numerical solvers. Consider the two-dimensional rotation by an angle $\\theta$ (in radians), represented by the $2 \\times 2$ matrix\n$$\nR(\\theta) = \\begin{pmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{pmatrix}.\n$$\nDetermine the lower-upper (LU) factorization of $R(\\theta)$ in the convention where the lower triangular factor $L$ has ones on its diagonal and no row pivoting is used. Assume $\\cos\\theta \\neq 0$ so that no row pivoting is required. Express $R(\\theta)$ as $R(\\theta) = L U$ with $L$ unit lower triangular and $U$ upper triangular, and give the matrices $L(\\theta)$ and $U(\\theta)$ in closed form as functions of $\\theta$.\n\nAnswer format requirement: Provide your final answer as a single analytic expression consisting of a row matrix whose two entries are the matrices $L(\\theta)$ and $U(\\theta)$, respectively. No rounding is required and no units are involved.",
            "solution": "The objective is to find a unit lower triangular matrix $L$ and an upper triangular matrix $U$ such that $R(\\theta) = L U$. Given that $R(\\theta)$ is a $2 \\times 2$ matrix, the matrices $L$ and $U$ will also be $2 \\times 2$. Their general forms are:\n$$\nL = \\begin{pmatrix} 1 & 0 \\\\ l_{21} & 1 \\end{pmatrix}\n\\quad \\text{and} \\quad\nU = \\begin{pmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{pmatrix}\n$$\nHere, $l_{21}$, $u_{11}$, $u_{12}$, and $u_{22}$ are the unknown elements that we must determine as functions of $\\theta$.\n\nThe factorization equation is:\n$$\nR(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix} = L U = \\begin{pmatrix} 1 & 0 \\\\ l_{21} & 1 \\end{pmatrix} \\begin{pmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{pmatrix}\n$$\nPerforming the matrix multiplication on the right-hand side yields:\n$$\n\\begin{pmatrix} 1 & 0 \\\\ l_{21} & 1 \\end{pmatrix} \\begin{pmatrix} u_{11} & u_{12} \\\\ 0 & u_{22} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot u_{11} + 0 \\cdot 0 & 1 \\cdot u_{12} + 0 \\cdot u_{22} \\\\ l_{21} \\cdot u_{11} + 1 \\cdot 0 & l_{21} \\cdot u_{12} + 1 \\cdot u_{22} \\end{pmatrix} = \\begin{pmatrix} u_{11} & u_{12} \\\\ l_{21} u_{11} & l_{21} u_{12} + u_{22} \\end{pmatrix}\n$$\nBy equating the corresponding elements of $R(\\theta)$ and the resulting $LU$ product, we obtain a system of equations:\n$$\n\\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{pmatrix} = \\begin{pmatrix} u_{11} & u_{12} \\\\ l_{21} u_{11} & l_{21} u_{12} + u_{22} \\end{pmatrix}\n$$\nWe can solve for the unknowns sequentially. From the first row:\n1.  $u_{11} = \\cos\\theta$\n2.  $u_{12} = -\\sin\\theta$\n\nFrom the second row:\n3.  $l_{21} u_{11} = \\sin\\theta$. Substituting the expression for $u_{11}$, we have $l_{21} \\cos\\theta = \\sin\\theta$. Since the problem states that $\\cos\\theta \\neq 0$, we can divide by $\\cos\\theta$ to find $l_{21}$:\n    $$\n    l_{21} = \\frac{\\sin\\theta}{\\cos\\theta} = \\tan\\theta\n    $$\n4.  $l_{21} u_{12} + u_{22} = \\cos\\theta$. We substitute the now known values of $l_{21}$ and $u_{12}$:\n    $$\n    (\\tan\\theta)(-\\sin\\theta) + u_{22} = \\cos\\theta\n    $$\n    Solving for $u_{22}$:\n    $$\n    u_{22} = \\cos\\theta + \\tan\\theta \\sin\\theta = \\cos\\theta + \\frac{\\sin\\theta}{\\cos\\theta} \\sin\\theta = \\frac{\\cos^2\\theta + \\sin^2\\theta}{\\cos\\theta}\n    $$\n    Using the fundamental trigonometric identity $\\cos^2\\theta + \\sin^2\\theta = 1$, this simplifies to:\n    $$\n    u_{22} = \\frac{1}{\\cos\\theta} = \\sec\\theta\n    $$\nHaving found all unknown elements, we can construct the matrices $L(\\theta)$ and $U(\\theta)$:\n$$\nL(\\theta) = \\begin{pmatrix} 1 & 0 \\\\ \\tan\\theta & 1 \\end{pmatrix}\n$$\n$$\nU(\\theta) = \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ 0 & \\sec\\theta \\end{pmatrix}\n$$\nThese are the unit lower triangular and upper triangular factors of the rotation matrix $R(\\theta)$, valid for all $\\theta$ where $\\cos\\theta \\neq 0$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\tan\\theta & 1 \\end{pmatrix} & \\begin{pmatrix} \\cos\\theta & -\\sin\\theta \\\\ 0 & \\sec\\theta \\end{pmatrix} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Once a matrix $A$ is factored into $L$ and $U$, we can efficiently solve the linear system $A\\mathbf{x} = \\mathbf{b}$. This practice reinforces the meaning of the factorization by reversing the typical workflow. Instead of finding an unknown solution $\\mathbf{x}$, you are given the factors $L$ and $U$ along with the solution $\\mathbf{x}$, and your task is to reconstruct the original right-hand side vector $\\mathbf{b}$. This process illuminates how the two-step solution method—solving $L\\mathbf{y} = \\mathbf{b}$ followed by $U\\mathbf{x} = \\mathbf{y}$—is equivalent to solving the original system .",
            "id": "2409873",
            "problem": "In computational modeling of electrostatics, consider the one-dimensional Poisson boundary-value problem on the domain $(0,1)$ with Dirichlet boundary conditions. Using a uniform grid with $N=3$ interior points and the standard second-order central-difference approximation, the nondimensionalized linear system for the interior values can be written in the form $A\\,\\mathbf{x}=\\mathbf{b}$, where the coefficient matrix $A$ is tridiagonal with entries $2$ on the diagonal and $-1$ on the first sub- and super-diagonals. Suppose a Lower-Upper (LU) factorization without pivoting of $A$ is given by\n$$\nL=\\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & 1 & 0 \\\\\n0 & -\\frac{2}{3} & 1\n\\end{pmatrix},\\quad\nU=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n0 & \\frac{3}{2} & -1 \\\\\n0 & 0 & \\frac{4}{3}\n\\end{pmatrix},\n$$\nso that $A=L\\,U$. Assume that the solution vector at the three interior grid points is\n$$\n\\mathbf{x}=\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}.\n$$\nDetermine the right-hand side vector $\\mathbf{b}$ such that $A\\,\\mathbf{x}=\\mathbf{b}$. Express your final answer as a row vector listing the three components in order corresponding to the interior grid points. No rounding is required.",
            "solution": "The linear system is $A\\,\\mathbf{x}=\\mathbf{b}$ with $A=L\\,U$. By the definition of matrix multiplication and factorization, this implies\n$$\n\\mathbf{b}=A\\,\\mathbf{x}=(L\\,U)\\,\\mathbf{x}=L\\,(U\\,\\mathbf{x}).\n$$\nWe first compute $U\\,\\mathbf{x}$. With\n$$\nU=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n0 & \\frac{3}{2} & -1 \\\\\n0 & 0 & \\frac{4}{3}\n\\end{pmatrix},\\quad\n\\mathbf{x}=\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix},\n$$\nwe have\n$$\nU\\,\\mathbf{x}=\\begin{pmatrix}\n2\\cdot 1 + (-1)\\cdot 1 + 0\\cdot 1 \\\\\n0\\cdot 1 + \\frac{3}{2}\\cdot 1 + (-1)\\cdot 1 \\\\\n0\\cdot 1 + 0\\cdot 1 + \\frac{4}{3}\\cdot 1\n\\end{pmatrix}\n=\\begin{pmatrix}\n1 \\\\ \\frac{1}{2} \\\\ \\frac{4}{3}\n\\end{pmatrix}.\n$$\nNext, compute $\\mathbf{b}=L\\,(U\\,\\mathbf{x})$. With\n$$\nL=\\begin{pmatrix}\n1 & 0 & 0 \\\\\n-\\frac{1}{2} & 1 & 0 \\\\\n0 & -\\frac{2}{3} & 1\n\\end{pmatrix},\\quad\nU\\,\\mathbf{x}=\\begin{pmatrix}1 \\\\ \\frac{1}{2} \\\\ \\frac{4}{3}\\end{pmatrix},\n$$\nwe obtain\n$$\n\\mathbf{b}=L\\,(U\\,\\mathbf{x})=\\begin{pmatrix}\n1\\cdot 1 + 0\\cdot \\frac{1}{2} + 0\\cdot \\frac{4}{3} \\\\\n-\\frac{1}{2}\\cdot 1 + 1\\cdot \\frac{1}{2} + 0\\cdot \\frac{4}{3} \\\\\n0\\cdot 1 + \\left(-\\frac{2}{3}\\right)\\cdot \\frac{1}{2} + 1\\cdot \\frac{4}{3}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1 \\\\ 0 \\\\ 1\n\\end{pmatrix}.\n$$\nTherefore, the right-hand side vector $\\mathbf{b}$ is $\\begin{pmatrix}1 \\\\ 0 \\\\ 1\\end{pmatrix}$, which in row form is $\\begin{pmatrix}1 & 0 & 1\\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix}1 & 0 & 1\\end{pmatrix}}$$"
        },
        {
            "introduction": "The theoretical elegance of an algorithm can face challenges when implemented on a computer with finite-precision arithmetic. This advanced practice delves into the critical topic of numerical stability, focusing on the back-substitution step of solving $U\\mathbf{x} = \\mathbf{y}$. By analyzing and simulating how a very small diagonal element in the matrix $U$ can dramatically amplify floating-point errors, you will gain a deeper appreciation for the practical limitations of numerical algorithms and understand the crucial role of pivoting strategies in robust scientific software .",
            "id": "2409891",
            "problem": "Consider solving the linear system $U x = y$ that arises after an $L U$ factorization (where $L$ denotes a unit lower triangular matrix and $U$ denotes an upper triangular matrix) using back substitution in floating-point arithmetic. Adopt the standard floating-point model: each elementary operation satisfies $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b) (1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff. Assume base $\\beta = 2$ and precision $p = 53$ (so $u = 2^{-53}$). Back substitution computes $x$ from $U x = y$ by iterating from $i = n$ down to $i = 1$ using the recurrence $x_i = \\left(y_i - \\sum_{j=i+1}^{n} U_{ij} x_j\\right) / U_{ii}$. You will analyze how floating-point perturbations in the last right-hand-side entry $y_n$ are amplified when $|U_{nn}|$ is very small.\n\nTasks:\n1) Starting only from the floating-point model and the definition of back substitution, derive a bound for the absolute error in the last component computed by back substitution, $\\widehat{x}_n$, when the true (exact arithmetic) value is $x_n^{\\star} = y_n / U_{nn}$ but the available right-hand-side is perturbed additively as $y_n + \\varepsilon$ with unknown $\\varepsilon$ satisfying $|\\varepsilon| \\le \\eta$, and the subtraction and division are carried out in floating-point arithmetic. Your bound must be expressed in terms of $|U_{nn}|$, $|y_n|$, $u$, and $\\eta$, and it must not assume any cancellation beyond what the model implies. Focus on the single back substitution step for $i = n$.\n2) Specialize your bound to the case $x^{\\star} = \\mathbf{1}$ (all ones), $U$ is upper triangular with the last row equal to zero except for $U_{nn} = \\alpha > 0$, and $y = U x^{\\star}$. Suppose an absolute perturbation of size exactly $u$ is added to $y_n$ immediately before computing $x_n$ (and all other operations are performed in floating-point arithmetic). Define the amplification factor $A(\\alpha) = \\dfrac{|\\widehat{x}_n - 1|}{u}$. Using your bound, predict $A(\\alpha)$ as a function of $\\alpha$ for small $u$.\n3) Implement a complete, runnable program that constructs, for each test $(n,\\alpha)$, an $n \\times n$ upper triangular matrix $U$ with $U_{ii} = 1$ for $i = 1, \\dots, n-1$, $U_{nn} = \\alpha$, and $U_{ij} = 1/2$ for $i < j \\le n-1$ (and $U_{nj} = 0$ for $j < n$). Let $x^{\\star} = \\mathbf{1}$ and $y = U x^{\\star}$. Perform a standard back substitution in double precision to compute $\\widehat{x}$, but just before dividing to compute the last component, inject an absolute perturbation of size exactly $u$ by replacing $y_n$ with $y_n + u$. For each test, report the quantity $R(\\alpha) = \\alpha \\cdot \\dfrac{|\\widehat{x}_n - 1|}{u}$. The analysis in item $2$ predicts $R(\\alpha) \\approx 1$ when $u$ is small.\n4) Use the following test suite of $(n,\\alpha)$ pairs to exercise typical, small, and extremely small last pivots, including a boundary-scale case:\n- $(n,\\alpha) = (6, 1)$\n- $(n,\\alpha) = (6, 2^{-10})$\n- $(n,\\alpha) = (6, 2^{-40})$\n- $(n,\\alpha) = (6, 2^{-53})$\n- $(n,\\alpha) = (6, 2^{-100})$\n- $(n,\\alpha) = (6, 2^{-300})$\nYour program must output a single line containing the values of $R(\\alpha)$ for the test suite as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,\\dots,r_6]$, where each $r_k$ is a floating-point number.\n\nNotes:\n- No physical units are involved.\n- Angles are not used.\n- The final output format must be exactly one line with the bracketed, comma-separated list described above.",
            "solution": "We analyze a single back substitution step for the last equation under the standard floating-point model. The exact last equation is $U_{nn} x_n^{\\star} = y_n$. In back substitution, the computed last component uses the formula\n$$\nx_n = \\frac{y_n - \\sum_{j=n+1}^{n} U_{nj} x_j}{U_{nn}} = \\frac{y_n}{U_{nn}},\n$$\nsince there are no terms with $j > n$. Suppose that the available right-hand-side is additively perturbed as $y_n + \\varepsilon$ with $|\\varepsilon| \\le \\eta$, and the floating-point subtraction and division are applied to compute $\\widehat{x}_n$.\n\nUnder the floating-point model $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b) (1 + \\delta)$ with $|\\delta| \\le u$, and restricting attention to the last step, we have the computed sequence for the last equation:\n- The subtraction reduces to a no-op because the sum is zero; however, to account for the model we still allow a rounding in forming the intermediate value. Let the intermediate value be\n$$\n\\widehat{y}_n = \\operatorname{fl}(y_n + \\varepsilon) = (y_n + \\varepsilon) (1 + \\delta_1), \\quad |\\delta_1| \\le u.\n$$\n- The division is performed in floating point:\n$$\n\\widehat{x}_n = \\operatorname{fl}\\left( \\frac{\\widehat{y}_n}{U_{nn}} \\right) = \\left( \\frac{\\widehat{y}_n}{U_{nn}} \\right) (1 + \\delta_2), \\quad |\\delta_2| \\le u.\n$$\nCombining these two steps gives\n$$\n\\widehat{x}_n = \\left( \\frac{y_n + \\varepsilon}{U_{nn}} \\right) (1 + \\delta_1)(1 + \\delta_2).\n$$\nLet $x_n^{\\star} = \\dfrac{y_n}{U_{nn}}$ denote the exact value. Then\n$$\n\\widehat{x}_n - x_n^{\\star} = \\frac{y_n + \\varepsilon}{U_{nn}} (1 + \\delta_1)(1 + \\delta_2) - \\frac{y_n}{U_{nn}}.\n$$\nRewriting and adding and subtracting $\\dfrac{y_n}{U_{nn}} (1 + \\delta_1)(1 + \\delta_2)$ yields\n$$\n\\widehat{x}_n - x_n^{\\star} = \\frac{\\varepsilon}{U_{nn}} (1 + \\delta_1)(1 + \\delta_2) + \\frac{y_n}{U_{nn}} \\left[ (1 + \\delta_1)(1 + \\delta_2) - 1 \\right].\n$$\nUsing $|(1 + \\delta_1)(1 + \\delta_2)| \\le (1+u)^2 \\le 1 + 3u$ for small $u$ and $|(1 + \\delta_1)(1 + \\delta_2) - 1| \\le |\\delta_1| + |\\delta_2| + |\\delta_1 \\delta_2| \\le 2u + u^2 \\le 3u$, we obtain the bound\n$$\n|\\widehat{x}_n - x_n^{\\star}| \\le \\frac{|\\varepsilon|}{|U_{nn}|} (1 + 3u) + \\frac{|y_n|}{|U_{nn}|} \\cdot 3u.\n$$\nNeglecting terms of order $u^2$ and higher (appropriate for advanced undergraduate analysis under $u \\ll 1$), a clean bound is\n$$\n|\\widehat{x}_n - x_n^{\\star}| \\le \\frac{|\\varepsilon|}{|U_{nn}|} + C \\, u \\, \\frac{|y_n|}{|U_{nn}|},\n$$\nwith a modest constant, for example $C = 3$. This establishes that an absolute perturbation $|\\varepsilon|$ in $y_n$ is amplified in absolute error by a factor approximately $1/|U_{nn}|$, with an additional small contribution proportional to $u \\, |y_n|/|U_{nn}|$ from the two floating-point operations.\n\nWe now specialize to the case $x^{\\star} = \\mathbf{1}$, $U$ upper triangular with the last row zero except for $U_{nn} = \\alpha > 0$, and $y = U x^{\\star}$. In exact arithmetic for the last row, $y_n = \\alpha$ and $x_n^{\\star} = 1$. If an absolute perturbation of size exactly $u$ is injected into $y_n$ immediately before computing $x_n$, we have $|\\varepsilon| = u$ and $|y_n| = \\alpha$. The error bound becomes\n$$\n|\\widehat{x}_n - 1| \\le \\frac{u}{\\alpha} + C \\, u \\, \\frac{\\alpha}{\\alpha} = \\frac{u}{\\alpha} + C u.\n$$\nFor small $\\alpha$ the dominant term is $u/\\alpha$, and the amplification factor\n$$\nA(\\alpha) = \\frac{|\\widehat{x}_n - 1|}{u}\n$$\nsatisfies\n$$\nA(\\alpha) \\approx \\frac{1}{\\alpha} \\quad \\text{for small } \\alpha,\n$$\nwith a lower-order additive term $C$ from the subtraction and division roundings. Therefore, the scaled quantity\n$$\nR(\\alpha) = \\alpha \\cdot \\frac{|\\widehat{x}_n - 1|}{u}\n$$\nis predicted to satisfy $R(\\alpha) \\approx 1$ for a wide range of $\\alpha$ values, provided $u$ is small.",
            "answer": "```python\nimport numpy as np\n\ndef build_upper_triangular(n: int, alpha: float) -> np.ndarray:\n    \"\"\"\n    Build an n x n upper triangular matrix U with:\n      - U[i,i] = 1 for i=0..n-2\n      - U[n-1,n-1] = alpha\n      - U[i,j] = 0.5 for i<j<=n-2\n      - U[n-1,j] = 0 for j<n-1\n      - U[i,n-1] = 0.5 for i < n-1 (so last column above diagonal is 0.5)\n    \"\"\"\n    U = np.zeros((n, n), dtype=np.float64)\n    # Set diagonal\n    for i in range(n - 1):\n        U[i, i] = 1.0\n    U[n - 1, n - 1] = alpha\n    # Set strictly upper entries in columns 0..n-2\n    for i in range(n - 1):\n        for j in range(i + 1, n - 1):\n            U[i, j] = 0.5\n    # Set last column above diagonal to 0.5\n    for i in range(n - 1):\n        U[i, n - 1] = 0.5\n    # Last row strictly upper remains zero by construction\n    return U\n\ndef back_substitution_with_injected_yn(U: np.ndarray, b: np.ndarray, inject_eps: float) -> np.ndarray:\n    \"\"\"\n    Solve U x = b by back substitution in double precision.\n    Before computing the last component x[n-1], add an absolute perturbation inject_eps to the effective y_n.\n    \"\"\"\n    n = U.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    for i in range(n - 1, -1, -1):\n        # Compute the sum of known terms\n        s = 0.0\n        # Use straightforward loop to keep operations explicit\n        for j in range(i + 1, n):\n            s += U[i, j] * x[j]\n        yi = b[i] - s\n        if i == n - 1:\n            yi = yi + inject_eps  # inject absolute perturbation into y_n\n        x[i] = yi / U[i, i]\n    return x\n\ndef solve():\n    # Unit roundoff for IEEE 754 double precision: u = 2^{-53} = eps/2\n    u = np.finfo(np.float64).eps / 2.0\n\n    # Define the test suite as specified: (n, alpha)\n    test_cases = [\n        (6, 1.0),\n        (6, 2.0**-10),\n        (6, 2.0**-40),\n        (6, 2.0**-53),\n        (6, 2.0**-100),\n        (6, 2.0**-300),\n    ]\n\n    results = []\n    for n, alpha in test_cases:\n        U = build_upper_triangular(n, alpha)\n        # Exact intended solution x* = ones\n        x_star = np.ones(n, dtype=np.float64)\n        # Form b = U x*\n        b = U @ x_star\n        # Perform back substitution with injected absolute perturbation u into y_n right before computing x_n\n        x_hat = back_substitution_with_injected_yn(U, b, inject_eps=u)\n        # Compute R(alpha) = alpha * |x_hat[n-1] - 1| / u\n        err_last = abs(x_hat[-1] - 1.0)\n        R = alpha * (err_last / u)\n        results.append(R)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}