{
    "hands_on_practices": [
        {
            "introduction": "Converting continuous physical signals into discrete digital data is the first step in any computational analysis of experimental results. This process, known as analog-to-digital conversion, is not perfect and inevitably introduces quantization noise, which places a fundamental limit on measurement precision. This practice allows you to simulate this crucial process, giving you a tangible understanding of how factors like bit depth affect data quality, a concept quantified by the Signal-to-Quantization-Noise Ratio (SQNR) .",
            "id": "2438146",
            "problem": "A continuous-time sinusoidal signal representing a single-tone test input is defined by the function $x(t) = A \\sin(2 \\pi f_{0} t + \\varphi)$ with amplitude $A$, frequency $f_{0}$, and phase $\\varphi = 0$ radians. The signal is sampled uniformly in time at sampling frequency $f_{s}$ for a duration $T$, producing $N = \\lfloor f_{s} T \\rfloor$ discrete-time samples $x[n] = x(t_{n})$ at times $t_{n} = n / f_{s}$ for $n = 0, 1, \\ldots, N-1$. The samples are then digitized by an ideal, uniform, mid-tread quantizer with $B$ bits and full-scale peak amplitude $L$ (in volts), as follows. Define $K = 2^{B-1} - 1$ and step size $\\Delta = L / K$. The reconstruction levels are $k \\Delta$ for integer $k$ satisfying $-K \\leq k \\leq K$. The quantized sample $y_{q}[n]$ is obtained by first mapping $x[n]$ to the nearest level $k \\Delta$ via rounding and then saturating to the extreme levels when necessary:\n$$\nk[n] = \\mathrm{clip}\\left(\\mathrm{round}\\left(\\frac{x[n]}{\\Delta}\\right), -K, K\\right), \\quad y_{q}[n] = k[n] \\Delta,\n$$\nwhere $\\mathrm{round}(\\cdot)$ denotes rounding to the nearest integer and $\\mathrm{clip}(u, a, b)$ limits $u$ to the interval $[a, b]$. The quantization error is $e[n] = y_{q}[n] - x[n]$. Using the sampled data, define the average signal power and average error power as\n$$\nP_{s} = \\frac{1}{N} \\sum_{n=0}^{N-1} x[n]^{2}, \\quad P_{e} = \\frac{1}{N} \\sum_{n=0}^{N-1} e[n]^{2},\n$$\nand the signal-to-quantization-noise ratio (SQNR) in decibels as\n$$\n\\mathrm{SQNR} = 10 \\log_{10}\\left(\\frac{P_{s}}{P_{e}}\\right).\n$$\nAll angles must be in radians. Voltages must be in volts.\n\nYour task is to compute $\\mathrm{SQNR}$ for each parameter set in the test suite below. For each test case, the amplitude is specified relative to the full-scale peak amplitude as a multiple of $L$; that is, use the absolute amplitude $A$ equal to the specified multiple times $L$. Express the final $\\mathrm{SQNR}$ values in decibels as floating-point numbers rounded to exactly six digits after the decimal point.\n\nTest suite (each case is a tuple $(B, f_{0}, f_{s}, T, L, \\text{amplitude multiple})$):\n- Case $1$: $(16, 1234, 48000, 1.0, 1.0, 0.9)$\n- Case $2$: $(12, 1000, 44100, 0.5, 2.0, 0.8)$\n- Case $3$: $(8, 5000, 96000, 0.25, 1.0, 0.5)$\n- Case $4$: $(3, 200, 8000, 1.0, 1.0, 0.95)$\n- Case $5$: $(16, 3000, 48000, 0.75, 1.0, 0.01)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, in the same order as the test suite (for example, \"[r1,r2,r3,r4,r5]\"), where each $r_i$ is the $\\mathrm{SQNR}$ of case $i$, rounded to exactly six digits after the decimal point.",
            "solution": "The problem statement is subjected to validation and is found to be valid. It is a well-posed problem in digital signal processing, providing a clear and complete set of definitions, parameters, and constraints. All parameters are physically and computationally reasonable, and the underlying principles are scientifically sound. The task is to compute the Signal-to-Quantization-Noise Ratio (SQNR) for a set of given scenarios. We proceed with the solution.\n\nThe core of the problem is the simulation of a signal acquisition chain, which consists of sampling and quantization. The solution is obtained by algorithmically executing the steps defined in the problem for each test case.\n\n1.  **Signal Generation**:\n    The continuous-time signal is a pure sinusoid defined as $x(t) = A \\sin(2 \\pi f_{0} t)$, since the phase $\\varphi$ is given as $0$. The amplitude $A$ is determined from the quantizer's full-scale peak amplitude $L$ and the specified amplitude multiple, such that $A = (\\text{amplitude multiple}) \\times L$.\n    This signal is sampled uniformly at a frequency $f_{s}$ for a duration $T$. The total number of discrete samples is $N = \\lfloor f_{s} T \\rfloor$. The samples are taken at time instances $t_{n} = n/f_{s}$ for $n = 0, 1, \\ldots, N-1$. The resulting discrete-time signal is:\n    $$x[n] = x(t_{n}) = A \\sin\\left(2 \\pi f_{0} \\frac{n}{f_{s}}\\right)$$\n\n2.  **Quantizer Definition**:\n    The system uses a uniform mid-tread quantizer with $B$ bits of resolution and a full-scale peak amplitude of $L$ volts. The number of positive (and negative) quantization levels is determined by $K = 2^{B-1} - 1$. The total number of reconstruction levels is $2K+1$. The quantization step size, $\\Delta$, which is the voltage difference between adjacent reconstruction levels, is given by:\n    $$\\Delta = \\frac{L}{K}$$\n    The reconstruction levels are then $\\{k\\Delta \\mid k \\in \\{-K, -K+1, \\ldots, K-1, K\\}\\}$.\n\n3.  **Quantization Process and Error Calculation**:\n    Each sample $x[n]$ is mapped to the nearest reconstruction level. This is a two-step process. First, the sample's voltage is normalized by the step size $\\Delta$, and the result is rounded to the nearest integer. This integer is then clipped to ensure it stays within the range of valid level indices $[-K, K]$. The resulting index is $k[n]$:\n    $$k[n] = \\mathrm{clip}\\left(\\mathrm{round}\\left(\\frac{x[n]}{\\Delta}\\right), -K, K\\right)$$\n    The quantized sample $y_{q}[n]$ is then reconstructed by scaling this index back by the step size:\n    $$y_{q}[n] = k[n] \\Delta$$\n    The quantization error for each sample is the difference between the quantized value and the original sample value:\n    $$e[n] = y_{q}[n] - x[n]$$\n\n4.  **Power and SQNR Calculation**:\n    The average power of the original discrete-time signal, $P_{s}$, and the average power of the quantization error signal, $P_{e}$, are computed by averaging the square of their respective sample values over all $N$ samples:\n    $$P_{s} = \\frac{1}{N} \\sum_{n=0}^{N-1} x[n]^{2}$$\n    $$P_{e} = \\frac{1}{N} \\sum_{n=0}^{N-1} e[n]^{2}$$\n    Finally, the Signal-to-Quantization-Noise Ratio (SQNR) is computed in decibels (dB) using the formula:\n    $$\\mathrm{SQNR} = 10 \\log_{10}\\left(\\frac{P_{s}}{P_{e}}\\right)$$\n    This entire procedure is implemented computationally for each parameter set provided in the test suite. The numerical results are rounded to six decimal places as required.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Signal-to-Quantization-Noise Ratio (SQNR) for a series\n    of test cases involving a sampled and quantized sinusoidal signal.\n    \"\"\"\n    # Test suite: each case is a tuple (B, f0, fs, T, L, amplitude_multiple)\n    test_cases = [\n        (16, 1234, 48000, 1.0, 1.0, 0.9),\n        (12, 1000, 44100, 0.5, 2.0, 0.8),\n        (8, 5000, 96000, 0.25, 1.0, 0.5),\n        (3, 200, 8000, 1.0, 1.0, 0.95),\n        (16, 3000, 48000, 0.75, 1.0, 0.01)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        B, f0, fs, T, L, amp_multiple = case\n\n        # Step 1: Signal Generation\n        # Amplitude of the sinusoidal signal\n        A = amp_multiple * L\n        # Number of samples, N = floor(fs * T)\n        N = int(fs * T)\n        # Time vector\n        t = np.arange(N) / fs\n        # Discrete-time signal x[n]\n        x_n = A * np.sin(2 * np.pi * f0 * t)\n\n        # Step 2: Quantizer Definition\n        # K determines the number of quantization levels on one side of zero\n        K = (2**(B - 1)) - 1\n        # Quantization step size\n        delta = L / K\n\n        # Step 3: Quantization Process\n        # Normalize signal and round to nearest integer to get level index\n        # np.round() implements rounding to the nearest integer, with .5 cases\n        # rounded to the nearest even integer.\n        k_n_unclipped = np.round(x_n / delta)\n        # Clip the indices to the valid range [-K, K]\n        k_n = np.clip(k_n_unclipped, -K, K)\n        # Reconstruct the quantized signal\n        y_q_n = k_n * delta\n\n        # Calculate the quantization error signal\n        e_n = y_q_n - x_n\n\n        # Step 4: Power and SQNR Calculation\n        # Average signal power\n        Ps = np.mean(x_n**2)\n        # Average error (noise) power\n        Pe = np.mean(e_n**2)\n        \n        # The problem setup ensures Pe > 0 for the given test cases.\n        # Otherwise, an if Pe == 0 check would be needed.\n        sqnr = 10 * np.log10(Ps / Pe)\n\n        results.append(sqnr)\n\n    # Format the final results as a comma-separated list of strings,\n    # with each number rounded to exactly six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A common and vital task when analyzing experimental data is calculating its rate of change, or derivative. However, measurement noise, even if seemingly small, can be dramatically amplified by standard numerical differentiation algorithms, often rendering the result useless. In this exercise, you will explore this challenge by comparing a naive differentiation approach with a more robust method involving pre-smoothing the data with a Gaussian filter, illustrating a classic trade-off between noise reduction and signal fidelity .",
            "id": "2438105",
            "problem": "You are provided with a synthetic time series model for a scalar experimental measurement with additive noise, discretely sampled in time. Let the sampling frequency be $f_s = 1000$ in $\\text{s}^{-1}$, the number of samples be $N = 4096$, and the sampling interval be $\\Delta t = 1/f_s$. Define the continuous-time underlying signal $x(t)$ and its exact time derivative $x'(t)$ by\n$$\nx(t) = A_1 \\sin(2\\pi f_1 t) + A_2 \\cos(2\\pi f_2 t) + A_3 t,\n$$\nwith $A_1 = 1$, $f_1 = 5$ in $\\text{s}^{-1}$, $A_2 = 0.3$, $f_2 = 40$ in $\\text{s}^{-1}$, and $A_3 = 0.2$ in $\\text{s}^{-1}$. Angles are in radians. The exact derivative is\n$$\nx'(t) = 2\\pi f_1 A_1 \\cos(2\\pi f_1 t) - 2\\pi f_2 A_2 \\sin(2\\pi f_2 t) + A_3.\n$$\nLet the discrete sampling times be $t_n = n \\Delta t$ for integers $n \\in \\{0,1,\\dots,N-1\\}$. The measured discrete-time signal $y[n]$ is defined as\n$$\ny[n] = x(t_n) + \\eta[n],\n$$\nwhere $\\eta[n]$ are independent and identically distributed zero-mean Gaussian random variables with standard deviation $\\sigma_{\\eta}$ in the same units as $x(t)$. Use a fixed pseudorandom number generator seed equal to $S = 1337$ for reproducibility of $\\eta[n]$ across all test cases.\n\nYour task is to estimate the time derivative of the noisy measurement by two methods and compare their accuracy against the exact derivative $x'(t_n)$:\n1. Direct central differencing on the noisy data:\n$$\n\\widehat{d}_{\\text{direct}}[n] = \\frac{y[n+1] - y[n-1]}{2\\Delta t},\n$$\ndefined for $n \\in \\{1,2,\\dots,N-2\\}$.\n2. Gaussian smoothing followed by the same central differencing:\n   - Define the discrete zero-phase Gaussian kernel with standard deviation $\\sigma_g$ in samples as\n   $$\n   g[k] = \\frac{\\exp\\!\\left(-\\frac{k^2}{2\\sigma_g^2}\\right)}{\\sum_{m=-K}^{K} \\exp\\!\\left(-\\frac{m^2}{2\\sigma_g^2}\\right)},\n   $$\n   where $K = \\lceil 3\\sigma_g \\rceil$ and $k \\in \\{-K,-K+1,\\dots,0,\\dots,K-1,K\\}$. If $\\sigma_g = 0$, define $g[0] = 1$ and $g[k] = 0$ for $k \\neq 0$.\n   - Define the smoothed signal as the discrete convolution\n   $$\n   z[n] = \\sum_{k=-K}^{K} g[k]\\, y[n-k],\n   $$\n   with the understanding that indices outside $\\{0,\\dots,N-1\\}$ are handled implicitly by considering only the valid samples where subsequent differencing is well-defined and comparing errors only for $n \\in \\{1,2,\\dots,N-2\\}$.\n   - Apply the same central difference to $z[n]$:\n   $$ \n   \\widehat{d}_{\\text{smooth}}[n] = \\frac{z[n+1] - z[n-1]}{2\\Delta t},\n   $$\n   for $n \\in \\{1,2,\\dots,N-2\\}$.\n\nFor each method, quantify the accuracy using the relative root-mean-square error (rRMSE):\n$$\n\\mathrm{rRMSE}_{\\text{method}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{n=n_{\\min}}^{n_{\\max}}\\left(\\widehat{d}_{\\text{method}}[n] - x'(t_n)\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{n=n_{\\min}}^{n_{\\max}} \\left(x'(t_n)\\right)^2}},\n$$\nwhere $n_{\\min} = 1$, $n_{\\max} = N-2$, and $M = n_{\\max}-n_{\\min}+1$. This quantity is dimensionless. Also compute the improvement factor\n$$\n\\rho = \\frac{\\mathrm{rRMSE}_{\\text{direct}}}{\\mathrm{rRMSE}_{\\text{smooth}}}.\n$$\n\nImplement a complete, runnable program that constructs the signal and noise as defined, applies the two estimation methods, and computes the metrics for each of the following test cases. Each test case is uniquely specified by $(\\sigma_{\\eta}, \\sigma_g)$ where $\\sigma_{\\eta}$ is the noise standard deviation and $\\sigma_g$ is the Gaussian kernel standard deviation in samples:\n- Case $1$: $(\\sigma_{\\eta}, \\sigma_g) = (0.5, 0.0)$.\n- Case $2$: $(\\sigma_{\\eta}, \\sigma_g) = (0.5, 2.0)$.\n- Case $3$: $(\\sigma_{\\eta}, \\sigma_g) = (2.0, 4.0)$.\n- Case $4$: $(\\sigma_{\\eta}, \\sigma_g) = (0.0, 4.0)$.\n\nAll final numerical outputs must be unitless and expressed as decimal floats. For each test case, return a list containing three values $[\\mathrm{rRMSE}_{\\text{direct}}, \\mathrm{rRMSE}_{\\text{smooth}}, \\rho]$, each rounded to exactly $6$ decimal places. Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a bracketed, comma-separated list. For example, the format must be\n\"[ [d1,s1,r1],[d2,s2,r2],[d3,s3,r3],[d4,s4,r4] ]\"\nwith each $d_i$, $s_i$, and $r_i$ replaced by the corresponding rounded decimal values.",
            "solution": "The problem presented is a well-defined exercise in computational physics, specifically in the domain of signal processing of experimental data. It requires the estimation of the time derivative of a noisy signal using two distinct methods and a quantitative comparison of their accuracy. The problem is scientifically grounded, objective, and provides all necessary information for a unique solution to be computed. It is therefore deemed valid.\n\nI will proceed with the solution by first constructing the signal and its derivative, then implementing the two estimation methods, and finally calculating the specified error metrics for each test case.\n\n**1. Preliminaries: Signal and System Definition**\n\nThe problem provides a set of fixed parameters for the simulation.\n- Sampling frequency: $f_s = 1000$ s$^{-1}$\n- Number of samples: $N = 4096$\n- Sampling interval: $\\Delta t = 1/f_s = 0.001$ s\n- Time vector: $t_n = n \\Delta t$ for $n \\in \\{0, 1, \\dots, N-1\\}$\n- Pseudorandom number generator seed: $S = 1337$\n\nThe continuous-time signal $x(t)$ is a sum of two sinusoids and a linear trend:\n$$\nx(t) = A_1 \\sin(2\\pi f_1 t) + A_2 \\cos(2\\pi f_2 t) + A_3 t\n$$\nwith parameters $A_1 = 1.0$, $f_1 = 5.0$ s$^{-1}$, $A_2 = 0.3$, $f_2 = 40.0$ s$^{-1}$, and $A_3 = 0.2$ s$^{-1}$.\n\nThe exact time derivative is given by:\n$$\nx'(t) = 2\\pi f_1 A_1 \\cos(2\\pi f_1 t) - 2\\pi f_2 A_2 \\sin(2\\pi f_2 t) + A_3\n$$\n\nThe measured signal $y[n]$ is the discrete-time clean signal $x(t_n)$ corrupted by additive white Gaussian noise $\\eta[n]$ with standard deviation $\\sigma_{\\eta}$:\n$$\ny[n] = x(t_n) + \\eta[n]\n$$\n\n**2. Derivative Estimation Methods**\n\nThe core of the task is to compare two methods for estimating the derivative $x'(t_n)$. Both methods are evaluated over the interval $n \\in \\{1, 2, \\dots, N-2\\}$.\n\n**Method 1: Direct Central Differencing**\n\nThis method approximates the derivative using a standard central difference formula applied directly to the noisy data $y[n]$:\n$$\n\\widehat{d}_{\\text{direct}}[n] = \\frac{y[n+1] - y[n-1]}{2\\Delta t}\n$$\nThis calculation is straightforward. However, applying a difference operator to a noisy signal is known to amplify the noise. The variance of the noise component in the derivative estimate is proportional to $\\sigma_{\\eta}^2 / (\\Delta t)^2$, which can be substantial for small $\\Delta t$.\n\n**Method 2: Smoothing followed by Central Differencing**\n\nThis method attempts to mitigate noise amplification by first applying a low-pass filter to the signal $y[n]$ and then differentiating the smoothed result.\n\n- **Smoothing**: The smoothing is performed via discrete convolution with a zero-phase Gaussian kernel $g[k]$.\n  - The kernel width is controlled by the parameter $\\sigma_g$ (in units of samples).\n  - The kernel is defined for $k \\in \\{-K, \\dots, K\\}$, where $K = \\lceil 3\\sigma_g \\rceil$.\n  - The formula is:\n    $$\n    g[k] = \\frac{\\exp\\!\\left(-\\frac{k^2}{2\\sigma_g^2}\\right)}{Z}, \\quad \\text{where } Z = \\sum_{m=-K}^{K} \\exp\\!\\left(-\\frac{m^2}{2\\sigma_g^2}\\right)\n    $$\n  - A special case is defined for $\\sigma_g = 0$, where the kernel becomes a discrete delta function, $g[0]=1$ and $g[k]=0$ for $k \\neq 0$. In this case, convolution with $g[k]$ does not change the signal.\n  - The smoothed signal is $z[n] = (y * g)[n] = \\sum_{k=-K}^{K} g[k] y[n-k]$. This is a standard convolution operation. To produce an output $z[n]$ of the same length as $y[n]$, we use a 'same' convolution mode, which implicitly handles boundary effects by padding the input signal (typically with zeros). These boundary effects will introduce errors near the start and end of the smoothed signal $z[n]$.\n\n- **Differentiation**: The same central difference formula is then applied to the smoothed signal $z[n]$:\n  $$\n  \\widehat{d}_{\\text{smooth}}[n] = \\frac{z[n+1] - z[n-1]}{2\\Delta t}\n  $$\n\nThis two-step process embodies a classic trade-off: the smoothing filter reduces noise (variance) but can also distort the underlying signal by attenuating its higher-frequency components, introducing a systematic error (bias). The optimal choice of $\\sigma_g$ depends on the signal's frequency content and the noise level.\n\n**3. Error Quantification**\n\nThe accuracy of each method is measured by the relative root-mean-square error (rRMSE). This metric compares the RMS of the estimation error to the RMS of the true derivative signal over the evaluation interval $n \\in \\{1, \\dots, N-2\\}$.\n\n$$\n\\mathrm{rRMSE}_{\\text{method}} = \\frac{\\sqrt{\\frac{1}{M}\\sum_{n=1}^{N-2}\\left(\\widehat{d}_{\\text{method}}[n] - x'(t_n)\\right)^2}}{\\sqrt{\\frac{1}{M}\\sum_{n=1}^{N-2} \\left(x'(t_n)\\right)^2}}\n$$\nwhere $M = N-2$.\n\nThe improvement factor $\\rho$ quantifies the performance gain of the smoothed method over the direct method:\n$$\n\\rho = \\frac{\\mathrm{rRMSE}_{\\text{direct}}}{\\mathrm{rRMSE}_{\\text{smooth}}}\n$$\nA value of $\\rho > 1$ indicates that smoothing improved the derivative estimate.\n\n**4. Implementation Strategy**\n\nThe solution will be implemented in Python using the `numpy` library for numerical operations and array handling, and `scipy.signal` for the convolution. A single function will execute the entire pipeline for each test case specified.\n\n- **Initialization**: Set up all constants and the pseudorandom number generator with the specified seed.\n- **Signal Generation**: Generate the discrete time vector `t`, the clean signal `x`, and the exact derivative `x_prime`.\n- **Main Loop**: Iterate through the provided test cases $(\\sigma_{\\eta}, \\sigma_g)$.\n- **Inside the Loop**:\n  1. Generate the noise vector $\\eta$ scaled by $\\sigma_{\\eta}$ and add it to `x` to create the noisy signal `y`.\n  2. **Method 1**: Compute `d_direct_est` by applying central differencing to `y`.\n  3. **Method 2**:\n     - If $\\sigma_g > 0$, construct the Gaussian kernel `g`, then compute the smoothed signal `z` using `scipy.signal.convolve(y, g, mode='same')`.\n     - If $\\sigma_g = 0$, set `z = y`.\n     - Compute `d_smooth_est` by applying central differencing to `z`.\n  4. **Error Calculation**:\n     - Define the target derivative `d_exact` by slicing `x_prime` to match the evaluation interval.\n     - Compute the RMS of the errors for both methods and the RMS of the exact derivative signal.\n     - Calculate $\\mathrm{rRMSE}_{\\text{direct}}$, $\\mathrm{rRMSE}_{\\text{smooth}}$, and $\\rho$.\n  5. Store the rounded results for the current test case.\n- **Output**: Format the collected results into the specified string format and print to standard output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Solves the signal processing problem as defined.\n    Constructs a synthetic signal, adds noise, and compares two methods\n    for estimating its time derivative.\n    \"\"\"\n    # Define problem constants\n    f_s = 1000.0  # Sampling frequency in s^-1\n    N = 4096      # Number of samples\n    dt = 1.0 / f_s  # Sampling interval in s\n\n    A1 = 1.0\n    f1 = 5.0      # s^-1\n    A2 = 0.3\n    f2 = 40.0     # s^-1\n    A3 = 0.2      # s^-1\n\n    S = 1337      # Pseudorandom number generator seed\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (sigma_eta, sigma_g)\n        (0.5, 0.0),\n        (0.5, 2.0),\n        (2.0, 4.0),\n        (0.0, 4.0),\n    ]\n\n    # Generate time vector, clean signal, and its exact derivative\n    t = np.arange(N) * dt\n    x = (A1 * np.sin(2 * np.pi * f1 * t) +\n         A2 * np.cos(2 * np.pi * f2 * t) +\n         A3 * t)\n    x_prime = (2 * np.pi * f1 * A1 * np.cos(2 * np.pi * f1 * t) -\n               2 * np.pi * f2 * A2 * np.sin(2 * np.pi * f2 * t) +\n               A3)\n\n    # Initialize the random number generator\n    rng = np.random.default_rng(seed=S)\n    \n    results = []\n    \n    # Process each test case\n    for sigma_eta, sigma_g in test_cases:\n        # Generate the noisy measurement\n        # The problem statement implies the stochastic process should be reproducible,\n        # which is achieved by using the same generator instance over the loop.\n        noise = sigma_eta * rng.standard_normal(N)\n        y = x + noise\n\n        # Method 1: Direct central differencing\n        # Valid for n in {1, ..., N-2}, resulting in N-2 points\n        d_direct_est = (y[2:] - y[:-2]) / (2 * dt)\n\n        # Method 2: Gaussian smoothing followed by central differencing\n        if sigma_g == 0.0:\n            z = y  # No smoothing\n        else:\n            # Define the discrete zero-phase Gaussian kernel\n            K = int(np.ceil(3 * sigma_g))\n            k = np.arange(-K, K + 1)\n            kernel_vals = np.exp(-k**2 / (2 * sigma_g**2))\n            g = kernel_vals / np.sum(kernel_vals)\n            \n            # Convolve the signal with the kernel\n            # 'same' mode ensures output length is N, with boundary effects\n            z = signal.convolve(y, g, mode='same')\n        \n        # Apply central difference to the smoothed signal\n        d_smooth_est = (z[2:] - z[:-2]) / (2 * dt)\n\n        # Quantify accuracy using relative RMS error\n        # The evaluation range is n in {1, ..., N-2}, so we slice the exact derivative\n        d_exact_eval = x_prime[1:-1]\n        \n        # Denominator for rRMSE: RMS of the exact derivative\n        rms_exact_deriv = np.sqrt(np.mean(d_exact_eval**2))\n\n        # Calculate rRMSE for the direct method\n        rms_err_direct = np.sqrt(np.mean((d_direct_est - d_exact_eval)**2))\n        rRMSE_direct = rms_err_direct / rms_exact_deriv\n\n        # Calculate rRMSE for the smoothed method\n        rms_err_smooth = np.sqrt(np.mean((d_smooth_est - d_exact_eval)**2))\n        rRMSE_smooth = rms_err_smooth / rms_exact_deriv\n        \n        # Handle potential division by zero if rRMSE_smooth is zero\n        if rRMSE_smooth == 0.0:\n            # This can happen in noise-free cases if the method is perfect.\n            # If direct is also zero, improvement is 1. Otherwise, infinite.\n            # Given the nature of the problem, this is unlikely.\n            rho = 1.0 if rRMSE_direct == 0.0 else np.inf\n        else:\n            rho = rRMSE_direct / rRMSE_smooth\n            \n        case_result = [\n            round(rRMSE_direct, 6),\n            round(rRMSE_smooth, 6),\n            round(rho, 6)\n        ]\n        results.append(case_result)\n\n    # Format the final output string as per requirements\n    inner_strings = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n\n```"
        },
        {
            "introduction": "Beyond basic processing, signal analysis techniques are powerful tools for scientific discovery, capable of uncovering faint signals buried in noise. This practice introduces a real-world statistical method from high-energy astrophysics, the H-test, which leverages Fourier analysis to search for periodic signals that are not purely sinusoidal. By implementing this test, you will learn how to frame a signal detection problem as a statistical hypothesis and apply it to synthetic data that mimics the search for pulsars from sparse photon-arrival records .",
            "id": "2438151",
            "problem": "You are given folded photon phases from sparse gamma-ray arrival data. For a set of $N$ photon phases $\\{\\phi_n\\}_{n=1}^N$ with each $\\phi_n \\in [0,2\\pi)$ in radians, define for each harmonic $k \\in \\{1,2,\\dots,m_{\\text{max}}\\}$ the trigonometric Fourier averages\n$$\nC_k = \\frac{1}{N}\\sum_{n=1}^N \\cos(k \\phi_n),\\quad\nS_k = \\frac{1}{N}\\sum_{n=1}^N \\sin(k \\phi_n).\n$$\nLet the cumulative Rayleigh power up to harmonic $m$ be\n$$\nZ_m^2 = 2N \\sum_{k=1}^m \\left(C_k^2 + S_k^2\\right).\n$$\nThe H statistic (commonly called the H-test in high-energy astrophysics) for non-sinusoidal periodicity is defined by\n$$\nH = \\max_{1 \\le m \\le m_{\\text{max}}}\\left[Z_m^2 - 4(m-1)\\right].\n$$\nAssume the null-hypothesis tail probability (p-value) for $H$ is approximated by\n$$\np = \\exp(-\\lambda H),\n$$\nwith $\\lambda = 0.4$. Adopt a significance level $\\alpha = 10^{-3}$ and declare a detection if and only if $p < \\alpha$.\n\nAngles must be treated in radians throughout. There are no other physical units involved in this computation. Your program must compute the H statistic and the p-value for each test case below and return a boolean detection decision for each case as specified.\n\nUse $m_{\\text{max}} = 20$. For each test case, you are given a deterministic construction of the phase list $\\{\\phi_n\\}$:\n- Test case A (uniform phases, null case): $N_A = 100$ and\n$$\n\\phi_n^{(A)} = \\frac{2\\pi n}{N_A},\\quad n = 0,1,2,\\dots,N_A-1.\n$$\n- Test case B (single narrow top-hat pulse): $N_B = 500$, window fractional width $w_B = 0.1$, and\n$$\n\\phi_n^{(B)} = 2\\pi\\, w_B\\,\\frac{n+\\tfrac{1}{2}}{N_B},\\quad n = 0,1,2,\\dots,N_B-1.\n$$\n- Test case C (two narrow pulses): $N_C = 400$, two equal peaks each of fractional width $w_C = 0.05$, centered near phases $0.10$ and $0.60$ cycles, respectively. Construct two length-$200$ phase blocks,\n$$\n\\phi_n^{(C,1)} = 2\\pi\\left(0.10 + w_C\\,\\frac{n+\\tfrac{1}{2}}{200}\\right),\\quad\n\\phi_n^{(C,2)} = 2\\pi\\left(0.60 + w_C\\,\\frac{n+\\tfrac{1}{2}}{200}\\right),\n$$\nfor $n = 0,1,2,\\dots,199$, and take $\\{\\phi_n^{(C)}\\}$ to be the concatenation of these two blocks.\n- Test case D (weakly pulsed with strong uniform background): $N_D = 25$ consisting of $N_U = 23$ uniform phases and $N_P = 2$ perfectly aligned pulsed phases. Take\n$$\n\\phi_n^{(U)} = \\frac{2\\pi n}{N_U},\\quad n = 0,1,2,\\dots,N_U-1,\n$$\nand\n$$\n\\phi_1^{(P)} = 0,\\quad \\phi_2^{(P)} = 0,\n$$\nand let $\\{\\phi_n^{(D)}\\}$ be the union of $\\{\\phi_n^{(U)}\\}$ and $\\{\\phi_j^{(P)}\\}_{j=1}^{N_P}$.\n\nYour task:\n- For each test case $X \\in \\{A,B,C,D\\}$, compute the H statistic $H_X$, the p-value $p_X = \\exp(-\\lambda H_X)$ with $\\lambda = 0.4$, and decide detection if $p_X < \\alpha$ with $\\alpha = 10^{-3}$.\n- Your program should produce a single line of output containing the four boolean detection results in the order $[A,B,C,D]$ as a comma-separated list enclosed in square brackets. For example, a valid output format is\n\"[False,True,True,False]\".\n\nTest suite summary to be implemented exactly as stated above:\n- $m_{\\text{max}} = 20$, $\\lambda = 0.4$, $\\alpha = 10^{-3}$.\n- Case A: $N_A = 100$, $\\phi_n^{(A)} = 2\\pi n/N_A$.\n- Case B: $N_B = 500$, $w_B = 0.1$, $\\phi_n^{(B)} = 2\\pi w_B (n+\\tfrac{1}{2})/N_B$.\n- Case C: $N_C = 400$, $w_C = 0.05$, two blocks at $0.10$ and $0.60$ cycles with $200$ phases each, as defined above.\n- Case D: $N_D = 25$, $N_U = 23$ uniform grid and $N_P = 2$ at $\\phi = 0$ as defined above.\n\nYour final output must be a single line of the form \"[b_A,b_B,b_C,b_D]\" with each $b_X$ equal to either True or False. No additional text should be printed.",
            "solution": "The problem statement presented is valid. It is scientifically grounded in the established statistical methods of high-energy astrophysics, specifically the H-test for periodicity in sparse time-series data. The problem is well-posed, providing a complete and unambiguous set of definitions, parameters, and test cases. There are no internal contradictions, missing information, or subjective elements. Therefore, a rigorous and unique solution can be constructed.\n\nThe task is to compute the H statistic for four different datasets of photon phases and determine, for each case, whether a periodic signal is detected. The H-test is designed to be sensitive to non-sinusoidal pulse shapes, which are common in astrophysical signals like those from pulsars. The statistic accomplishes this by summing the power from multiple harmonics of the fundamental frequency.\n\nThe core of the problem is the computation of the H statistic, $H$, defined as:\n$$\nH = \\max_{1 \\le m \\le m_{\\text{max}}}\\left[Z_m^2 - 4(m-1)\\right]\n$$\nHere, $m_{\\text{max}}$ is the maximum number of harmonics to consider, given as $m_{\\text{max}} = 20$. The term $Z_m^2$ represents the cumulative Rayleigh power up to the $m$-th harmonic:\n$$\nZ_m^2 = 2N \\sum_{k=1}^m \\left(C_k^2 + S_k^2\\right)\n$$\nwhere $N$ is the number of photons. The coefficients $C_k$ and $S_k$ are the sample means of the cosine and sine of the $k$-th harmonic of the phases $\\{\\phi_n\\}_{n=1}^N$:\n$$\nC_k = \\frac{1}{N}\\sum_{n=1}^N \\cos(k \\phi_n)\n$$\n$$\nS_k = \\frac{1}{N}\\sum_{n=1}^N \\sin(k \\phi_n)\n$$\nThe penalty term $-4(m-1)$ in the definition of $H$ corrects for the fact that adding more harmonics ($m > 1$) increases the power $Z_m^2$ even for random noise. The H-test finds the number of harmonics $m$ that maximizes the power after this penalty is applied.\n\nOnce the maximum value $H$ is found, the statistical significance is assessed by computing a p-value. The problem provides the approximation for the null-hypothesis tail probability:\n$$\np = \\exp(-\\lambda H)\n$$\nwith the constant $\\lambda = 0.4$. A detection is claimed if this p-value is below a specified significance level $\\alpha = 10^{-3}$. That is, a detection occurs if and only if $p < \\alpha$.\n\nWe will implement a procedure that follows these definitions. For each of the four test cases, we first construct the array of $N$ phases $\\{\\phi_n\\}$ according to the specified rules. Then, we execute the following algorithm:\n\n1.  Initialize a list to store the values of $H_m = Z_m^2 - 4(m-1)$ for each $m \\in \\{1, 2, \\dots, m_{\\text{max}}\\}$.\n2.  Initialize the cumulative Rayleigh power term, $Z^2_{\\text{cumulative}}$, to $0$.\n3.  Iterate with a harmonic index $k$ from $1$ to $m_{\\text{max}}$:\n    a. For the current harmonic $k$, compute the coefficients $C_k$ and $S_k$ by taking the average of $\\cos(k\\phi_n)$ and $\\sin(k\\phi_n)$ over all $N$ phases.\n    b. Update the cumulative power: $Z^2_{\\text{cumulative}} \\leftarrow Z^2_{\\text{cumulative}} + 2N(C_k^2 + S_k^2)$. At step $k$, this value corresponds to $Z_k^2$.\n    c. Compute the test statistic for $m=k$ harmonics: $H_k = Z^2_{\\text{cumulative}} - 4(k-1)$.\n    d. Store this value $H_k$.\n4.  After the loop, find the overall H statistic by taking the maximum value among all computed $H_k$: $H = \\max_{k} H_k$.\n5.  Calculate the p-value $p = \\exp(-\\lambda H)$.\n6.  Compare $p$ with $\\alpha$ to yield a boolean decision: `True` for detection ($p < \\alpha$) and `False` for non-detection ($p \\ge \\alpha$).\n\nThis procedure is applied to each of the four test cases.\n\n-   **Case A**: Phases are uniformly distributed. Theory predicts that for a perfectly uniform distribution over $[0, 2\\pi)$, the sums for $C_k$ and $S_k$ will be zero for all $k$, leading to $H=0$, $p=1$, and thus no detection.\n-   **Case B**: Phases are concentrated in a single narrow pulse. This non-uniformity should lead to large values for $C_k$ and $S_k$ for several harmonics, resulting in a large $H$ and a significant detection.\n-   **Case C**: Phases are concentrated in two narrow pulses. This represents a more complex, non-sinusoidal periodic signal. The H-test is specifically designed for such cases, and we expect a strong detection.\n-   **Case D**: A small number of pulsed events are added to a larger uniform background. This tests the sensitivity of the statistic. With only $N_P=2$ pulsed events out of $N_D=25$ total events, the signal is weak, and detection is not guaranteed. Our analytical check suggested non-detection.\n\nThe implementation will use the `numpy` library for efficient vectorized calculations of trigonometric functions and summations over the phase arrays. The final output will be a list of four boolean values, corresponding to the detection decision for each case, formatted as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_h_test_decision(phases: np.ndarray, m_max: int, lambda_val: float, alpha: float) -> bool:\n    \"\"\"\n    Computes the H-test statistic for a given set of phases and returns a detection decision.\n\n    Args:\n        phases: A NumPy array of photon phases in radians, in the range [0, 2*pi).\n        m_max: The maximum number of harmonics to consider.\n        lambda_val: The constant in the p-value approximation p = exp(-lambda * H).\n        alpha: The significance level for detection.\n\n    Returns:\n        A boolean value: True if a detection is made (p < alpha), False otherwise.\n    \"\"\"\n    N = len(phases)\n    if N == 0:\n        return False\n\n    h_values = []\n    z_squared_cumulative = 0.0\n\n    for m in range(1, m_max + 1):\n        # Calculate C_m and S_m for the current harmonic m\n        # In the problem, the sum is over k=1..m, so our loop variable should be k\n        # Here, m in the loop corresponds to the harmonic index k in the formula's sum\n        k = m\n        k_phi = k * phases\n        c_k = np.mean(np.cos(k_phi))\n        s_k = np.mean(np.sin(k_phi))\n\n        # Update the cumulative Rayleigh power Z_m^2\n        # Z_m^2 = 2N * sum_{k=1 to m} (C_k^2 + S_k^2)\n        # We compute this iteratively\n        z_squared_cumulative += 2 * N * (c_k**2 + s_k**2)\n        \n        # Calculate the statistic H_m for m harmonics\n        # H_m = Z_m^2 - 4(m-1)\n        h_m = z_squared_cumulative - 4 * (m - 1)\n        h_values.append(h_m)\n\n    # The H statistic is the maximum over all m\n    H = -np.inf if not h_values else np.max(h_values)\n\n    # Calculate p-value and make a decision\n    p_value = np.exp(-lambda_val * H)\n    \n    return p_value < alpha\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the H-test on four predefined test cases.\n    \"\"\"\n    # Define global parameters\n    M_MAX = 20\n    LAMBDA = 0.4\n    ALPHA = 1e-3\n\n    results = []\n\n    # Test Case A: Uniform phases\n    N_A = 100\n    phases_A = (2 * np.pi / N_A) * np.arange(N_A)\n    decision_A = compute_h_test_decision(phases_A, M_MAX, LAMBDA, ALPHA)\n    results.append(decision_A)\n\n    # Test Case B: Single narrow top-hat pulse\n    N_B = 500\n    w_B = 0.1\n    phases_B = 2 * np.pi * w_B * (np.arange(N_B) + 0.5) / N_B\n    decision_B = compute_h_test_decision(phases_B, M_MAX, LAMBDA, ALPHA)\n    results.append(decision_B)\n\n    # Test Case C: Two narrow pulses\n    N_C = 400\n    N_C_block = 200\n    w_C = 0.05\n    n_vals_C = np.arange(N_C_block)\n    phases_C1 = 2 * np.pi * (0.10 + w_C * (n_vals_C + 0.5) / N_C_block)\n    phases_C2 = 2 * np.pi * (0.60 + w_C * (n_vals_C + 0.5) / N_C_block)\n    phases_C = np.concatenate((phases_C1, phases_C2))\n    decision_C = compute_h_test_decision(phases_C, M_MAX, LAMBDA, ALPHA)\n    results.append(decision_C)\n\n    # Test Case D: Weakly pulsed with strong uniform background\n    N_D = 25\n    N_U = 23\n    N_P = 2\n    phases_U = (2 * np.pi / N_U) * np.arange(N_U)\n    phases_P = np.zeros(N_P)\n    phases_D = np.concatenate((phases_U, phases_P))\n    decision_D = compute_h_test_decision(phases_D, M_MAX, LAMBDA, ALPHA)\n    results.append(decision_D)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}