{
    "hands_on_practices": [
        {
            "introduction": "Many fundamental physical laws, from Maxwell's equations in electromagnetism to fluid dynamics, are expressed as systems of coupled first-order partial differential equations. A powerful technique is to combine these into a single higher-order equation for one field variable. This practice guides you through that process, starting with a simple first-order system and deriving a single second-order PDE, which you will then classify . This exercise is fundamental to revealing the intrinsic nature of a system, such as whether it supports wave-like phenomena.",
            "id": "2380226",
            "problem": "Consider smooth scalar fields $u(x,t)$ and $v(x,t)$ on $\\mathbb{R} \\times \\mathbb{R}$ that satisfy the first-order system\n$$\nu_{t} = v_{x}, \\qquad v_{t} = u_{x}.\n$$\nDerive a single second-order Partial Differential Equation (PDE) for $u(x,t)$ alone. Then express that PDE in the standard second-order form\n$$\nA\\,u_{xx} + 2 B\\,u_{xt} + C\\,u_{tt} + \\text{(lower-order terms)} = 0\n$$\nand report the value of the discriminant $B^{2} - A C$. Provide your final answer as this discriminant value. Do not include any units, and do not include the classification name in your final answer.",
            "solution": "The problem as stated is valid. It is a well-posed mathematical exercise in the field of partial differential equations, free of any scientific or logical inconsistencies. We shall proceed with the derivation.\n\nWe are given a system of two first-order partial differential equations for two smooth scalar fields, $u(x,t)$ and $v(x,t)$:\n$$\nu_{t} = v_{x} \\quad (1)\n$$\n$$\nv_{t} = u_{x} \\quad (2)\n$$\nOur objective is to derive a single second-order PDE for the field $u(x,t)$ and then determine the value of its discriminant. To eliminate the field $v(x,t)$ from the system, we differentiate the given equations.\n\nFirst, we differentiate equation $(1)$ with respect to the variable $t$:\n$$\n\\frac{\\partial}{\\partial t}(u_{t}) = \\frac{\\partial}{\\partial t}(v_{x})\n$$\nThis gives us the second partial derivative of $u$ with respect to $t$ on the left-hand side, and a mixed partial derivative of $v$ on the right-hand side:\n$$\nu_{tt} = v_{xt} \\quad (3)\n$$\nNext, we differentiate equation $(2)$ with respect to the variable $x$:\n$$\n\\frac{\\partial}{\\partial x}(v_{t}) = \\frac{\\partial}{\\partial x}(u_{x})\n$$\nThis yields a mixed partial derivative of $v$ on the left-hand side and the second partial derivative of $u$ with respect to $x$ on the right-hand side:\n$$\nv_{tx} = u_{xx} \\quad (4)\n$$\nThe problem statement specifies that $u(x,t)$ and $v(x,t)$ are \"smooth\" fields. This is a standard term implying that the functions are sufficiently differentiable, at least of class $C^{2}$. According to Clairaut's theorem on the equality of mixed partials, for a $C^{2}$ function $v(x,t)$, the order of differentiation does not matter. Therefore, we have:\n$$\nv_{xt} = v_{tx}\n$$\nUsing this equality, we can equate the left-hand side of equation $(3)$ with the right-hand side of equation $(4)$:\n$$\nu_{tt} = u_{xx}\n$$\nThis is the second-order partial differential equation for $u(x,t)$ alone. To classify this equation, we must write it in the standard form provided in the problem statement:\n$$\nA\\,u_{xx} + 2 B\\,u_{xt} + C\\,u_{tt} + \\text{(lower-order terms)} = 0\n$$\nRearranging our derived equation $u_{tt} = u_{xx}$ gives:\n$$\nu_{xx} - u_{tt} = 0\n$$\nOr, to be more explicit in matching the standard form:\n$$\n(1)u_{xx} + (0)u_{xt} + (-1)u_{tt} = 0\n$$\nBy comparing this to the standard form, we can identify the coefficients $A$, $B$, and $C$:\n- The coefficient of $u_{xx}$ is $A = 1$.\n- The coefficient of $u_{xt}$ is $2B = 0$, which implies $B = 0$.\n- The coefficient of $u_{tt}$ is $C = -1$.\n\nThe final step is to calculate the discriminant, which is defined as $B^{2} - A C$. Substituting the values we have found for $A$, $B$, and $C$:\n$$\nB^{2} - A C = (0)^{2} - (1)(-1)\n$$\n$$\nB^{2} - A C = 0 - (-1)\n$$\n$$\nB^{2} - A C = 1\n$$\nThe value of the discriminant is $1$. This confirms the equation is of hyperbolic type, although the problem explicitly asks not to state the classification.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "The physical behavior described by a PDE is not always uniform across the entire domain; it can change its character from one region to another. This exercise explores a PDE with variable coefficients, where its classification as elliptic, parabolic, or hyperbolic depends on the spatial coordinates $(x,y)$ . By calculating the discriminant as a function of position, you will learn to map out the distinct regions of behavior, a crucial skill for understanding complex systems like transonic flows and for designing adaptive numerical methods.",
            "id": "2380287",
            "problem": "Consider the second-order linear partial differential equation (PDE)\n$$(x^{2}-1)\\,u_{xx} + 2\\,x y\\,u_{xy} + (y^{2}-1)\\,u_{yy} = 0,$$\nposed on the entire $\\mathbb{R}^{2}$, where $u=u(x,y)$ is twice continuously differentiable. Classify this PDE as elliptic, parabolic, or hyperbolic in different regions of the $x y$-plane by analyzing its principal part from first principles. Identify the precise boundary curve that separates regions of different type and specify which side of this boundary corresponds to each type.\n\nYour final reported answer must be a single simplified analytic expression in the variables $x$ and $y$ whose zero level set is exactly the classification boundary you identify. No units are involved. Do not provide inequalities or piecewise descriptions in the final reported answer.",
            "solution": "The classification of a general second-order linear partial differential equation in two variables,\n$$A(x,y) u_{xx} + 2B(x,y) u_{xy} + C(x,y) u_{yy} + \\dots = 0,$$\ndepends on the sign of the discriminant of its principal part, which is defined as $\\Delta(x,y) = B(x,y)^{2} - A(x,y)C(x,y)$. The type of the equation at a point $(x,y)$ is determined as follows:\n-   If $\\Delta  0$, the equation is hyperbolic.\n-   If $\\Delta = 0$, the equation is parabolic.\n-   If $\\Delta  0$, the equation is elliptic.\n\nThe given equation is\n$$(x^{2}-1)\\,u_{xx} + 2\\,x y\\,u_{xy} + (y^{2}-1)\\,u_{yy} = 0.$$\nBy comparing this with the general form `A u_xx + 2B u_xy + C u_yy = 0`, we identify the coefficients of the principal part:\n-   $A(x,y) = x^{2}-1$\n-   $2B(x,y) = 2xy \\implies B(x,y) = xy$\n-   $C(x,y) = y^{2}-1$\n\nNow, we compute the discriminant $\\Delta(x,y) = B^2 - AC$:\n$$\n\\Delta = (xy)^{2} - (x^{2}-1)(y^{2}-1)\n$$\nExpanding the terms, we get:\n$$\n\\Delta = x^{2}y^{2} - (x^{2}y^{2} - x^{2} - y^{2} + 1)\n$$\n$$\n\\Delta = x^{2}y^{2} - x^{2}y^{2} + x^{2} + y^{2} - 1\n$$\n$$\n\\Delta = x^{2} + y^{2} - 1\n$$\nThe sign of $\\Delta$ is determined by the sign of the expression $x^{2} + y^{2} - 1$. We analyze the three cases.\n\nCase 1: Hyperbolic region ($\\Delta  0$)\nThe equation is hyperbolic where $x^{2} + y^{2} - 1  0$. This simplifies to:\n$$x^{2} + y^{2}  1$$\nThis inequality describes the set of all points $(x,y)$ in the plane that are outside the unit circle centered at the origin.\n\nCase 2: Parabolic boundary ($\\Delta = 0$)\nThe equation is parabolic where $x^{2} + y^{2} - 1 = 0$. This simplifies to:\n$$x^{2} + y^{2} = 1$$\nThis is the equation of the unit circle centered at the origin. This curve is the boundary that separates the regions of different types.\n\nCase 3: Elliptic region ($\\Delta  0$)\nThe equation is elliptic where $x^{2} + y^{2} - 1  0$. This simplifies to:\n$$x^{2} + y^{2}  1$$\nThis inequality describes the set of all points $(x,y)$ in the open disk of radius $1$ centered at the origin, which is the region inside the unit circle.\n\nIn summary:\n-   The PDE is **hyperbolic** for all points $(x,y)$ in the region exterior to the unit circle, where $x^{2} + y^{2}  1$.\n-   The PDE is **parabolic** on the unit circle itself, where $x^{2} + y^{2} = 1$.\n-   The PDE is **elliptic** for all points $(x,y)$ in the region interior to the unit circle, where $x^{2} + y^{2}  1$.\n\nThe problem asks for a single analytic expression whose zero level set is the classification boundary. The boundary is defined by the condition $\\Delta = 0$, which is equivalent to $x^{2} + y^{2} - 1 = 0$. The required expression is therefore $x^{2} + y^{2} - 1$.",
            "answer": "$$\n\\boxed{x^{2} + y^{2} - 1}\n$$"
        },
        {
            "introduction": "In computational and experimental science, we often face an \"inverse problem\": we have a dataset representing a physical field but may not know the precise governing equation. This practice puts you in the role of a data analyst tasked with reverse-engineering the nature of an unknown PDE from its solution sampled on a grid . By approximating derivatives with finite differences and analyzing their local correlations, you will computationally determine if the underlying physics is elliptic or hyperbolic, bridging the gap between abstract mathematical theory and practical data-driven discovery.",
            "id": "2380244",
            "problem": "You are given only values of a scalar field $u(x,y)$ sampled on a uniform Cartesian grid, with no direct access to the underlying partial differential equation (PDE). The goal is to use only local data correlations of the sampled data to computationally estimate whether the unknown linear, second-order PDE that $u$ plausibly satisfies in the region is elliptic or hyperbolic. The classification refers to the sign of the discriminant $D = B^2 - AC$ of the principal part $A u_{xx} + 2 B u_{xy} + C u_{yy}$: if $D  0$ the PDE is elliptic, and if $D  0$ the PDE is hyperbolic. All computations are dimensionless, and any trigonometric functions must use angles in radians.\n\nConstruct a program that, for each test case below, takes the provided $u(x,y)$ on a uniform grid and outputs an integer label: $+1$ if you estimate the underlying PDE to be elliptic, and $-1$ if you estimate it to be hyperbolic. Your method must rely solely on local data correlations of the sampled field values. No external input is allowed; all data are fully specified below.\n\nGrid specification common to all test cases:\n- Domain: $x \\in [-1,1]$, $y \\in [-1,1]$.\n- Grid size: $N \\times N$ with $N = 61$ uniformly spaced nodes in each direction.\n- Grid spacing: $h = \\dfrac{2}{N-1}$.\n- Coordinates: $(x_i,y_j)$ with $x_i = -1 + (i-1)h$, $y_j = -1 + (j-1)h$, for integer indices $i,j \\in \\{1,2,\\dots,N\\}$.\n\nTest suite of four scalar fields $u(x,y)$:\n- Case #1 (elliptic prototype): $u_1(x,y) = x^2 - y^2$.\n- Case #2 (parabolic prototype): $u_2(x,y) = \\cos\\!\\big(3 \\pi (x - y)\\big)$.\n- Case #3 (elliptic, higher-order harmonic): $u_3(x,y) = x^3 - 3 x y^2$.\n- Case #4 (parabolic with smooth hyperbolic contamination): $u_4(x,y) = \\cos\\!\\big(2 \\pi (x - y)\\big) + 0.05 \\cos(4 \\pi x)\\cos(4 \\pi y)$.\n\nRequired output:\n- Produce a single line of output containing the four integer labels in order of the test cases $(\\#1,\\#2,\\#3,\\#4)$, as a comma-separated list enclosed in square brackets, for example $[1,-1,1,-1]$.\n\nYour program must be self-contained and run without user input or external files. The final answers have no physical units, and angles must be in radians. The final output must be a single line exactly in the specified list format. The answer for each test case must be an integer from the set $\\{+1,-1\\}$.",
            "solution": "The problem as stated is valid. It is a well-posed problem in computational physics, specifically in the domain of system identification for partial differential equations (PDEs). The problem is scientifically grounded, objective, and provides all necessary information to construct a computational solution. There are no contradictions, and the specified test cases are standard examples for the targeted PDE classes.\n\nThe core of the problem is to estimate the type (elliptic or hyperbolic) of an unknown linear, second-order PDE of the form\n$$\nA u_{xx} + 2B u_{xy} + C u_{yy} + \\dots = 0\n$$\ngiven only a discrete sampling of its solution, $u(x,y)$, on a uniform Cartesian grid. The classification depends on the sign of the discriminant $D = B^2 - AC$. The method must be based on \"local data correlations.\"\n\nWe will formalize this requirement by approximating the partial derivatives using finite difference stencils and then identifying the linear algebraic relationship that holds among these approximations. This is a regression problem for the unknown constant coefficients $A$, $B$, and $C$.\n\nLet the grid data be $u_{i,j} = u(x_j, y_i)$, where $i,j \\in \\{0, 1, \\dots, N-1\\}$ are zero-based indices, $N=61$, and the grid spacing is $h = 2/(N-1)$. We approximate the second-order derivatives at interior grid points using second-order accurate central differences. The underlying PDE suggests that a linear combination of these derivatives equals zero (ignoring lower-order terms and source terms, which is a common assumption in discovering the principal part of an equation).\n\nThe discrete operators corresponding to the derivatives are given by applying stencils to the grid data. For an interior point $(i,j)$, where $i,j \\in \\{1, \\dots, N-2\\}$, we define:\n\\begin{align*}\ns_{xx}[u]_{i,j} = u_{i,j+1} - 2u_{i,j} + u_{i,j-1} \\\\\ns_{yy}[u]_{i,j} = u_{i+1,j} - 2u_{i,j} + u_{i,j-1} \\\\\ns_{xy}[u]_{i,j} = u_{i+1,j+1} - u_{i+1,j-1} - u_{i-1,j+1} + u_{i-1,j-1}\n\\end{align*}\nNote that these stencils are proportional to the finite difference approximations of the derivatives:\n$$\nu_{xx} \\approx \\frac{s_{xx}[u]}{h^2}, \\quad u_{yy} \\approx \\frac{s_{yy}[u]}{h^2}, \\quad u_{xy} \\approx \\frac{s_{xy}[u]}{4h^2}\n$$\nSubstituting these into the principal part of the PDE gives:\n$$\nA \\frac{s_{xx}[u]}{h^2} + 2B \\frac{s_{xy}[u]}{4h^2} + C \\frac{s_{yy}[u]}{h^2} \\approx 0\n$$\nMultiplying by $h^2$ yields a relationship between the stencil responses themselves:\n$$\nA s_{xx}[u] + \\frac{B}{2} s_{xy}[u] + C s_{yy}[u] \\approx 0\n$$\nWe seek coefficients $(k_1, k_2, k_3)$ such that $k_1 s_{xx}[u] + k_2 s_{xy}[u] + k_3 s_{yy}[u] \\approx 0$ across the grid. By comparing with the equation above, we can identify $A=k_1$, $B/2=k_2$, and $C=k_3$. This implies $B=2k_2$. The discriminant can then be estimated as:\n$$\nD_{est} = B^2 - AC = (2k_2)^2 - k_1 k_3 = 4k_2^2 - k_1 k_3\n$$\nTo find the coefficients $(k_1, k_2, k_3)$, we treat the stencil responses at all $(N-2)^2$ interior points as vectors. Let $\\mathbf{v}_{xx}$, $\\mathbf{v}_{xy}$, and $\\mathbf{v}_{yy}$ be the flattened vectors of the stencil grids $s_{xx}[u]$, $s_{xy}[u]$, and $s_{yy}[u]$, respectively. The problem is to find a coefficient vector $\\mathbf{k} = [k_1, k_2, k_3]^T$ that minimizes $\\|\\mathbf{v}_{xx} k_1 + \\mathbf{v}_{xy} k_2 + \\mathbf{v}_{yy} k_3 \\|_2^2$, subject to $\\|\\mathbf{k}\\|_2=1$ to avoid the trivial solution $\\mathbf{k}=0$.\n\nThis is a standard total least squares problem. If we form a data matrix $M = [\\mathbf{v}_{xx} | \\mathbf{v}_{xy} | \\mathbf{v}_{yy}]$, the problem is equivalent to minimizing $\\|M \\mathbf{k}\\|_2^2 = \\mathbf{k}^T (M^T M) \\mathbf{k}$. The solution $\\mathbf{k}$ is the eigenvector corresponding to the smallest eigenvalue of the $3 \\times 3$ symmetric matrix $M^T M$. The matrix $M^T M$ is precisely the covariance matrix of the derivative fields, thus this method directly implements the idea of finding \"local data correlations.\"\n\nA special case arises if one of the derivative fields is identically zero (or numerically insignificant), for instance, if $u_{xy}=0$ for the given function $u(x,y)$. In this situation, the corresponding column vector in $M$ would be null. Following the principle of parsimony (Occam's razor), we should seek the simplest PDE. This implies setting the coefficient for the null derivative to zero and solving the reduced regression problem. For example, if $\\mathbf{v}_{xy} \\approx \\mathbf{0}$, we set $k_2=0$ and find the best-fit $[k_1, k_3]$ for the remaining two vectors.\n\nThe complete algorithm is as follows:\n1. For each test case, generate the $N \\times N$ data grid $U$ by sampling the given function $u(x,y)$.\n2. Compute the $(N-2) \\times (N-2)$ stencil response grids $S_{xx}$, $S_{xy}$, and $S_{yy}$.\n3. Flatten these grids into vectors $\\mathbf{v}_{xx}$, $\\mathbf{v}_{xy}$, and $\\mathbf{v}_{yy}$.\n4. A-priori handle trivial derivatives. Calculate the norms of these vectors. If a norm is negligible compared to the maximum norm (e.g., smaller than $10^{-9}$ times the max), its corresponding coefficient is set to zero, and the vector is excluded from the regression analysis.\n5. Form the data matrix $M$ from the non-trivial vectors.\n6. Compute the covariance matrix $C_M = M^T M$.\n7. Find the eigenvalues and eigenvectors of $C_M$. The eigenvector corresponding to the smallest eigenvalue provides the coefficients for the non-trivial derivatives.\n8. Reconstruct the full coefficient vector $\\mathbf{k}=[k_1, k_2, k_3]^T$.\n9. Calculate the discriminant estimate $D_{est} = 4k_2^2 - k_1 k_3$.\n10. Classify the PDE: if $D_{est}  0$, the label is $+1$ (elliptic); otherwise, the label is $-1$ (hyperbolic).\n\nThis procedure provides a robust and objective method for estimating the PDE type from sampled data.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PDE classification problem for the given test cases.\n    \"\"\"\n    N = 61\n    h = 2.0 / (N - 1)\n    \n    # Create the grid coordinates\n    x_coords = np.linspace(-1, 1, N)\n    y_coords = np.linspace(-1, 1, N)\n    x, y = np.meshgrid(x_coords, y_coords, indexing='xy')\n\n    # Define the four test case functions\n    test_functions = [\n        lambda x, y: x**2 - y**2,\n        lambda x, y: np.cos(3 * np.pi * (x - y)),\n        lambda x, y: x**3 - 3 * x * y**2,\n        lambda x, y: np.cos(2 * np.pi * (x - y)) + 0.05 * np.cos(4 * np.pi * x) * np.cos(4 * np.pi * y),\n    ]\n\n    results = []\n    \n    for u_func in test_functions:\n        # 1. Sample the scalar field on the grid\n        U = u_func(x, y)\n\n        # 2. Compute stencil responses on the interior grid\n        # Stencil for u_xx, proportional to s_xx\n        s_xx_grid = U[1:-1, 2:] - 2 * U[1:-1, 1:-1] + U[1:-1, :-2]\n        # Stencil for u_yy, proportional to s_yy\n        s_yy_grid = U[2:, 1:-1] - 2 * U[1:-1, 1:-1] + U[:-2, 1:-1]\n        # Stencil for u_xy, proportional to s_xy\n        s_xy_grid = U[2:, 2:] - U[2:, :-2] - U[:-2, 2:] + U[:-2, :-2]\n\n        # 3. Flatten the stencil grids into vectors\n        v_xx = s_xx_grid.flatten()\n        v_yy = s_yy_grid.flatten()\n        v_xy = s_xy_grid.flatten()\n\n        # 4. Handle trivial derivatives (Principle of Parsimony)\n        norms = np.array([np.linalg.norm(v) for v in [v_xx, v_xy, v_yy]])\n        max_norm = np.max(norms)\n        \n        # Use a relative tolerance to detect null vectors\n        trivial_tol = 1e-9\n        is_trivial = norms  trivial_tol * max_norm if max_norm > 0 else np.ones(3, dtype=bool)\n\n        active_indices = np.where(~is_trivial)[0]\n        active_vectors = [v for i, v in enumerate([v_xx, v_xy, v_yy]) if i in active_indices]\n\n        # 5. Form the data matrix M\n        if not active_vectors:\n            # All derivatives are zero, cannot classify. This should not happen for the given cases.\n            k_full = np.zeros(3)\n        elif len(active_vectors) == 1:\n            # Equation is of the form k*u_deriv = 0, implies k=0. Again, non-physical.\n            k_full = np.zeros(3)\n        else:\n            M = np.stack(active_vectors, axis=1)\n\n            # 6. Compute the covariance-like matrix C_M = M^T * M\n            C_M = M.T @ M\n\n            # 7. Find the eigenvector corresponding to the smallest eigenvalue\n            eigenvalues, eigenvectors = np.linalg.eigh(C_M)\n            k_active = eigenvectors[:, 0]\n\n            # 8. Reconstruct the full coefficient vector\n            k_full = np.zeros(3)\n            k_full[active_indices] = k_active\n        \n        k1, k2, k3 = k_full\n        \n        # 9. Calculate the discriminant\n        # The regression finds k1, k2, k3 for k1*s_xx + k2*s_xy + k3*s_yy = 0.\n        # This corresponds to PDE coefficients A=k1, B/2=k2 (so B=2*k2), and C=k3.\n        # The discriminant is D = B^2 - A*C, so its sign is determined by (2*k2)^2 - k1*k3.\n        discriminant = 4 * k2**2 - k1 * k3\n        \n        # 10. Classify and store the result\n        if discriminant  0:\n            results.append(1)  # Elliptic\n        else:\n            results.append(-1) # Hyperbolic (includes Parabolic case D=0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}