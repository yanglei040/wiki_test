{
    "hands_on_practices": [
        {
            "introduction": "Before we can solve the Poisson equation, we must first accurately represent the continuous Laplacian operator, $\\nabla^2$, in a discrete form suitable for a computer. This exercise provides a direct way to test the quality of our approximation. By starting with a known analytical potential $\\phi(x,y)$ and using the discrete Laplacian to compute the corresponding source density $\\rho(x,y)$, we can investigate the accuracy and limitations of the fundamental five-point stencil .",
            "id": "2427919",
            "problem": "You are given a scalar potential map $\\phi(x,y)$ on a square domain $[0,1] \\times [0,1]$ sampled on a uniform Cartesian grid of size $n \\times n$ that includes the boundary points. Assume a dimensionless formulation in which the Poisson equation is\n$$\n\\nabla^{2}\\phi(x,y) = - \\rho(x,y).\n$$\nLet the grid spacing be $h = \\frac{1}{n-1}$ and the grid nodes be $x_i = i h$ and $y_j = j h$ for $i,j \\in \\{0,1,\\dots,n-1\\}$. Define the standard five-point discrete Laplacian operator on interior nodes $(i,j)$ with $i \\in \\{1,\\dots,n-2\\}$ and $j \\in \\{1,\\dots,n-2\\}$ by\n$$\n(\\Delta_h \\phi)_{i,j} = \\frac{\\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} - 4 \\phi_{i,j}}{h^{2}}.\n$$\nDefine the discrete source density on interior nodes by\n$$\n\\rho_{i,j}^{\\mathrm{comp}} = - (\\Delta_h \\phi)_{i,j}.\n$$\nFor each test case below, compute the maximum absolute error between the computed source density $\\rho_{i,j}^{\\mathrm{comp}}$ and the exact analytical source density $\\rho^{\\mathrm{exact}}(x_i,y_j)$ over all interior nodes,\n$$\nE = \\max_{1 \\le i \\le n-2,\\; 1 \\le j \\le n-2} \\left| \\rho_{i,j}^{\\mathrm{comp}} - \\rho^{\\mathrm{exact}}(x_i,y_j) \\right|.\n$$\nAll quantities are dimensionless. No physical units are required.\n\nTest suite:\n- Case $1$ (smooth trigonometric interior, nonzero source): $n = 33$, $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, $\\rho^{\\mathrm{exact}}(x,y) = 2\\pi^{2}\\sin(\\pi x)\\sin(\\pi y)$.\n- Case $2$ (quadratic polynomial, constant source): $n = 5$, $\\phi(x,y) = x^{2} + y^{2}$, $\\rho^{\\mathrm{exact}}(x,y) = -4$.\n- Case $3$ (bilinear polynomial, zero source): $n = 3$, $\\phi(x,y) = x y$, $\\rho^{\\mathrm{exact}}(x,y) = 0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the three error values $E$ for the cases above, in the order listed, as a comma-separated list enclosed in square brackets, with each value rounded to $10$ decimal places (for example, $[0.1234567890,0.0000000000,0.0000012346]$).",
            "solution": "The problem as stated is subjected to validation.\n\n**Step 1: Extracted Givens**\n- Domain: A square domain $[0,1] \\times [0,1]$.\n- Grid: A uniform Cartesian grid of size $n \\times n$ containing all boundary points.\n- Grid Spacing: $h = \\frac{1}{n-1}$.\n- Grid Nodes: $(x_i, y_j)$ where $x_i = i h$ and $y_j = j h$ for $i,j \\in \\{0,1,\\dots,n-1\\}$.\n- Governing Equation: The Poisson equation, $\\nabla^{2}\\phi(x,y) = - \\rho(x,y)$.\n- Discrete Laplacian Operator: For interior nodes $(i,j)$ where $i,j \\in \\{1,\\dots,n-2\\}$, the five-point stencil is given by $(\\Delta_h \\phi)_{i,j} = \\frac{\\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} - 4 \\phi_{i,j}}{h^{2}}$.\n- Computed Source Density: $\\rho_{i,j}^{\\mathrm{comp}} = - (\\Delta_h \\phi)_{i,j}$.\n- Error Metric: The maximum absolute error over all interior nodes, $E = \\max_{1 \\le i \\le n-2,\\; 1 \\le j \\le n-2} \\left| \\rho_{i,j}^{\\mathrm{comp}} - \\rho^{\\mathrm{exact}}(x_i,y_j) \\right|$.\n- Test Cases:\n    1. $n = 33$, $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, $\\rho^{\\mathrm{exact}}(x,y) = 2\\pi^{2}\\sin(\\pi x)\\sin(\\pi y)$.\n    2. $n = 5$, $\\phi(x,y) = x^{2} + y^{2}$, $\\rho^{\\mathrm{exact}}(x,y) = -4$.\n    3. $n = 3$, $\\phi(x,y) = x y$, $\\rho^{\\mathrm{exact}}(x,y) = 0$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is scientifically grounded. The Poisson equation is fundamental in physics, and the five-point stencil is a standard second-order accurate finite difference approximation of the Laplacian. The analytical relationship between the provided potential functions $\\phi(x,y)$ and exact source densities $\\rho^{\\mathrm{exact}}(x,y)$ is correct for all three test cases, as verified by direct computation of the continuous Laplacian:\n- For Case $1$: $\\nabla^2(\\sin(\\pi x)\\sin(\\pi y)) = -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - \\pi^2\\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2\\sin(\\pi x)\\sin(\\pi y)$, thus $\\rho^{\\mathrm{exact}} = - \\nabla^2\\phi = 2\\pi^2\\sin(\\pi x)\\sin(\\pi y)$.\n- For Case $2$: $\\nabla^2(x^2+y^2) = 2+2=4$, thus $\\rho^{\\mathrm{exact}} = - \\nabla^2\\phi = -4$.\n- For Case $3$: $\\nabla^2(xy) = 0+0=0$, thus $\\rho^{\\mathrm{exact}} = - \\nabla^2\\phi = 0$.\nThe problem is well-posed, objective, and contains all necessary information for a unique solution. It is a standard numerical methods exercise for verifying a discretization scheme.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A reasoned solution will be provided.\n\n**Principle-Based Solution**\n\nThe objective is to quantify the error of a finite difference approximation. The core of the problem lies in the difference between the continuous Laplacian operator, $\\nabla^2$, and its discrete approximation, $\\Delta_h$.\n\nThe discrete five-point Laplacian $(\\Delta_h \\phi)_{i,j}$ at a grid point $(x_i, y_j)$ is derived from Taylor series expansions. For a sufficiently smooth function $\\phi(x,y)$, the central difference approximation for the second partial derivative with respect to $x$ is:\n$$ \\frac{\\phi(x_i+h, y_j) - 2\\phi(x_i, y_j) + \\phi(x_i-h, y_j)}{h^2} = \\frac{\\partial^2\\phi}{\\partial x^2}\\bigg|_{(x_i,y_j)} + \\frac{h^2}{12}\\frac{\\partial^4\\phi}{\\partial x^4}\\bigg|_{(x_i,y_j)} + \\mathcal{O}(h^4) $$\nAn analogous expression holds for the partial derivative with respect to $y$. Summing these gives the approximation for the Laplacian:\n$$ (\\Delta_h \\phi)_{i,j} = \\nabla^2\\phi\\bigg|_{(x_i,y_j)} + \\frac{h^2}{12}\\left(\\frac{\\partial^4\\phi}{\\partial x^4} + \\frac{\\partial^4\\phi}{\\partial y^4}\\right)\\bigg|_{(x_i,y_j)} + \\mathcal{O}(h^4) $$\nThe leading term of the error in the approximation is called the truncation error, which is of order $h^2$.\n\nThe computed source density is $\\rho_{i,j}^{\\mathrm{comp}} = -(\\Delta_h \\phi)_{i,j}$, and the exact source density is $\\rho^{\\mathrm{exact}}(x_i,y_j) = -\\nabla^2\\phi|_{(x_i,y_j)}$. The point-wise error is therefore:\n$$ \\rho_{i,j}^{\\mathrm{comp}} - \\rho^{\\mathrm{exact}}(x_i,y_j) = -\\frac{h^2}{12}\\left(\\frac{\\partial^4\\phi}{\\partial x^4} + \\frac{\\partial^4\\phi}{\\partial y^4}\\right)\\bigg|_{(x_i,y_j)} - \\mathcal{O}(h^4) $$\nThe task is to compute the maximum of the absolute value of this error over the interior grid nodes.\n\n**Algorithmic Procedure:**\nFor each test case with given parameters $n$ and functions $\\phi(x,y)$ and $\\rho^{\\mathrm{exact}}(x,y)$:\n1.  Define the grid. The grid spacing is $h=1/(n-1)$, and the grid nodes are $x_i = i h$, $y_j = j h$ for $i,j \\in \\{0, \\dots, n-1\\}$.\n2.  Sample the potential. Construct an $n \\times n$ matrix $\\Phi$ where each element $\\Phi_{i,j} = \\phi(x_i, y_j)$.\n3.  Compute the source density. For each interior node $(i,j)$ where $i,j \\in \\{1, \\dots, n-2\\}$, calculate $\\rho_{i,j}^{\\mathrm{comp}}$ using the five-point stencil formula:\n    $$ \\rho_{i,j}^{\\mathrm{comp}} = -\\frac{\\Phi_{i+1,j} + \\Phi_{i-1,j} + \\Phi_{i,j+1} + \\Phi_{i,j-1} - 4\\Phi_{i,j}}{h^2} $$\n    This can be efficiently vectorized using array slicing.\n4.  Compute the error. Sample the exact source density $\\rho^{\\mathrm{exact}}(x_i, y_j)$ on the interior grid nodes. Calculate the absolute difference $|\\rho_{i,j}^{\\mathrm{comp}} - \\rho^{\\mathrm{exact}}(x_i, y_j)|$ for all interior nodes.\n5.  Find the maximum error $E$, which is the maximum value among all computed absolute differences.\n\n**Analysis of Test Cases:**\n- **Case 1**: $\\phi(x,y) = \\sin(\\pi x)\\sin(\\pi y)$. The fourth partial derivatives are non-zero: $\\frac{\\partial^4\\phi}{\\partial x^4} = \\pi^4\\phi(x,y)$ and $\\frac{\\partial^4\\phi}{\\partial y^4} = \\pi^4\\phi(x,y)$. The error is expected to be non-zero and proportional to $h^2$. With $n=33$, $h = 1/32$, the error should be small but non-negligible.\n- **Case 2**: $\\phi(x,y) = x^2 + y^2$. This is a quadratic polynomial. All partial derivatives of order three and higher are identically zero. The truncation error term $\\frac{h^2}{12}(\\dots)$ is zero. Therefore, the finite difference approximation is exact for this function. The error $E$ must be $0$, limited only by floating-point precision.\n- **Case 3**: $\\phi(x,y) = xy$. This is a bilinear polynomial. All partial derivatives of order two and higher are identically zero. Again, the truncation error is zero, and the finite difference formula is exact. The error $E$ must be $0$.\n\nThe implementation will follow this procedure for each case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_error(n, phi_func, rho_exact_func):\n    \"\"\"\n    Computes the maximum absolute error for a given test case.\n\n    Args:\n        n (int): The grid size (n x n).\n        phi_func (callable): A function for the scalar potential phi(x, y).\n        rho_exact_func (callable): A function for the exact source rho(x, y).\n\n    Returns:\n        float: The maximum absolute error E over the interior nodes.\n    \"\"\"\n    # 1. Define the grid.\n    if n < 3:\n        # No interior points for n < 3. Error is trivially 0.\n        return 0.0\n        \n    h = 1.0 / (n - 1)\n    # Create grid coordinates. 'ij' indexing ensures that the first index\n    # corresponds to x and the second to y, matching the problem's notation.\n    x = np.linspace(0.0, 1.0, n)\n    y = np.linspace(0.0, 1.0, n)\n    xx, yy = np.meshgrid(x, y, indexing='ij')\n\n    # 2. Sample the potential on the full grid.\n    phi_grid = phi_func(xx, yy)\n\n    # 3. Compute the source density on the interior grid.\n    # The five-point stencil is applied using vectorized numpy slicing.\n    # phi_grid[1:-1, 1:-1] corresponds to phi_ij for interior i,j\n    # phi_grid[2:, 1:-1]   corresponds to phi_{i+1,j}\n    # phi_grid[:-2, 1:-1]  corresponds to phi_{i-1,j}\n    # etc.\n    laplacian_phi_interior = (phi_grid[2:, 1:-1] + phi_grid[:-2, 1:-1] +\n                              phi_grid[1:-1, 2:] + phi_grid[1:-1, :-2] -\n                              4 * phi_grid[1:-1, 1:-1]) / (h**2)\n    \n    rho_comp_interior = -laplacian_phi_interior\n\n    # 4. Compute the error.\n    # Sample the exact source density on the interior grid.\n    rho_exact_interior = rho_exact_func(xx[1:-1, 1:-1], yy[1:-1, 1:-1])\n    \n    error_matrix = np.abs(rho_comp_interior - rho_exact_interior)\n\n    # 5. Find the maximum error.\n    max_error = np.max(error_matrix)\n    \n    return max_error\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Smooth trigonometric function\n        {\n            \"n\": 33,\n            \"phi_func\": lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"rho_exact_func\": lambda x, y: 2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y)\n        },\n        # Case 2: Quadratic polynomial\n        {\n            \"n\": 5,\n            \"phi_func\": lambda x, y: x**2 + y**2,\n            \"rho_exact_func\": lambda x, y: -4.0 + 0 * x  # 0*x ensures array output\n        },\n        # Case 3: Bilinear polynomial\n        {\n            \"n\": 3,\n            \"phi_func\": lambda x, y: x * y,\n            \"rho_exact_func\": lambda x, y: 0.0 + 0 * x\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        error = calculate_error(\n            n=case[\"n\"],\n            phi_func=case[\"phi_func\"],\n            rho_exact_func=case[\"rho_exact_func\"]\n        )\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.10f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Discretizing the Poisson equation transforms a single PDE into a large system of coupled linear equations, often too large for direct solution methods. This is where iterative solvers become essential, but basic methods like the Jacobi iteration can converge very slowly. This practice dives into the powerful technique of Chebyshev acceleration, demonstrating how a sophisticated choice of iteration parameters, based on the spectral properties of the system matrix, can dramatically speed up convergence .",
            "id": "2427931",
            "problem": "Consider the two-dimensional Poisson problem on the unit square with homogeneous Dirichlet boundary conditions, posed as the discrete linear system $A \\mathbf{u} = \\mathbf{b}$ arising from a standard five-point finite difference discretization of $- \\Delta u = f$ on a uniform grid. Let there be $n$ interior points per coordinate direction, so that the grid spacing is $h = \\frac{1}{n+1}$ and the interior grid points are $(x_i, y_j)$ with $x_i = i h$, $y_j = j h$ for $i,j \\in \\{1, \\dots, n\\}$. Define the discrete operator by\n$$\n(A u)_{i,j} = \\frac{4 u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2},\n$$\nwith the convention that $u_{i,j} = 0$ whenever $(i,j)$ lies outside $\\{1,\\dots,n\\} \\times \\{1,\\dots,n\\}$, corresponding to homogeneous Dirichlet boundary conditions.\n\nLet the exact analytic solution be $u(x,y) = \\sin(\\pi x) \\sin(\\pi y)$. Define the forcing as $f(x,y) = 2 \\pi^2 \\sin(\\pi x) \\sin(\\pi y)$ so that $- \\Delta u = f$ in the continuous sense. Construct $\\mathbf{b}$ by evaluating $f$ at the interior grid points: $b_{i,j} = f(x_i, y_j)$.\n\nLet $D$ denote the diagonal of $A$, which satisfies $D = \\frac{4}{h^2} I$. Consider two discrete-time iterative processes to approximate the solution $\\mathbf{u}$, both starting from the zero initial guess $\\mathbf{u}^{(0)} = \\mathbf{0}$ and using the residual $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{u}^{(k)}$ together with the preconditioned residual $M^{-1} \\mathbf{r}^{(k)} = D^{-1} \\mathbf{r}^{(k)} = \\frac{h^2}{4} \\mathbf{r}^{(k)}$:\n- Process J: $ \\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + D^{-1} \\mathbf{r}^{(k)}$.\n- Process C: $ \\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\alpha_k D^{-1} \\mathbf{r}^{(k)}$, where the step sizes $\\alpha_k$ are non-constant and depend on spectral bounds of the symmetric positive definite (SPD) matrix $D^{-1} A$. For the five-point Laplacian on the $n \\times n$ interior grid, the extremal eigenvalues of $D^{-1} A$ are\n$$\n\\lambda_{\\min} = 2 \\sin^2\\!\\left(\\frac{\\pi}{2 (n+1)}\\right), \\quad\n\\lambda_{\\max} = 2 \\cos^2\\!\\left(\\frac{\\pi}{2 (n+1)}\\right).\n$$\nDefine $c = \\frac{\\lambda_{\\max} + \\lambda_{\\min}}{2}$ and $\\delta = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{2}$. For a prescribed positive integer number of iterations $m$, choose, for $k \\in \\{1,\\dots,m\\}$,\n$$\n\\alpha_k = \\frac{1}{c - \\delta \\cos\\!\\left(\\frac{\\pi (2k - 1)}{2 m}\\right)},\n$$\nwith all angles understood to be in radians.\n\nFor each process, after exactly $m$ iterations, define the discrete Euclidean norm of the error as\n$$\n\\| e \\|_2 = \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left( u^{(m)}_{i,j} - \\sin(\\pi x_i)\\sin(\\pi y_j) \\right)^2 \\right)^{1/2}.\n$$\n\nYour task is to compute, for each test case, the ratio $r = \\frac{\\| e \\|_{2,\\mathrm{C}}}{\\| e \\|_{2,\\mathrm{J}}}$, where $\\| e \\|_{2,\\mathrm{C}}$ and $\\| e \\|_{2,\\mathrm{J}}$ are the error norms after $m$ iterations of Process C and Process J, respectively. All trigonometric arguments must be in radians.\n\nTest Suite:\n- Test $1$: $n = 32$, $m = 40$.\n- Test $2$: $n = 8$, $m = 10$.\n- Test $3$: $n = 64$, $m = 1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the three ratios for the test cases, in the order given above, each rounded to six decimal places, as a comma-separated list enclosed in square brackets (for example, $[0.123456,0.234567,0.345678]$). No other text should be printed.",
            "solution": "We discretize the two-dimensional Poisson equation $- \\Delta u = f$ on the unit square with homogeneous Dirichlet boundary conditions using a uniform grid with $n$ interior points in each spatial direction and mesh spacing $h = \\frac{1}{n+1}$. The standard five-point finite difference stencil yields the linear system $A \\mathbf{u} = \\mathbf{b}$ with\n$$\n(A u)_{i,j} = \\frac{4 u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1}}{h^2},\n$$\nwhere indices outside the domain correspond to boundary values and are set to zero, consistently enforcing homogeneous Dirichlet boundary conditions. The continuous exact solution is $u(x,y) = \\sin(\\pi x) \\sin(\\pi y)$ and the forcing is $f(x,y) = 2 \\pi^2 \\sin(\\pi x) \\sin(\\pi y)$. The right-hand side vector is constructed by $b_{i,j} = f(x_i, y_j)$ with $x_i = i h$ and $y_j = j h$.\n\nWe consider iterative approximations to the solution, starting from $\\mathbf{u}^{(0)} = \\mathbf{0}$. Let $D$ be the diagonal of $A$, which for this operator is $D = \\frac{4}{h^2} I$. Define the residual at iteration $k$ by $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{u}^{(k)}$ and the preconditioned residual by $D^{-1}\\mathbf{r}^{(k)} = \\frac{h^2}{4} \\mathbf{r}^{(k)}$.\n\nProcess J is the classical Jacobi method expressed in preconditioned Richardson form,\n$$\n\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + D^{-1} \\mathbf{r}^{(k)}.\n$$\nThis arises from splitting $A = D - (L + U)$ and writing\n$$\n\\mathbf{u}^{(k+1)} = D^{-1} \\left( (L + U) \\mathbf{u}^{(k)} + \\mathbf{b} \\right) = \\mathbf{u}^{(k)} + D^{-1} (\\mathbf{b} - A \\mathbf{u}^{(k)}).\n$$\n\nProcess C is a non-stationary preconditioned Richardson iteration with iteration-dependent step sizes $\\alpha_k$ chosen to minimize the maximum error amplification over the spectrum of $D^{-1}A$, based on the minimax property of Chebyshev polynomials of the first kind. For the five-point Laplacian with homogeneous Dirichlet boundary conditions and $n$ interior points in each direction, the eigenvalues of $D^{-1} A$ are\n$$\n\\lambda_{i,j} = \\sin^2\\!\\left( \\frac{\\pi i}{2 (n+1)} \\right) + \\sin^2\\!\\left( \\frac{\\pi j}{2 (n+1)} \\right), \\quad i,j \\in \\{1,\\dots,n\\}.\n$$\nThus, the smallest and largest eigenvalues are\n$$\n\\lambda_{\\min} = \\sin^2\\!\\left( \\frac{\\pi}{2 (n+1)} \\right) + \\sin^2\\!\\left( \\frac{\\pi}{2 (n+1)} \\right) = 2 \\sin^2\\!\\left( \\frac{\\pi}{2 (n+1)} \\right),\n$$\n$$\n\\lambda_{\\max} = \\sin^2\\!\\left( \\frac{\\pi n}{2 (n+1)} \\right) + \\sin^2\\!\\left( \\frac{\\pi n}{2 (n+1)} \\right) = 2 \\cos^2\\!\\left( \\frac{\\pi}{2 (n+1)} \\right),\n$$\nusing the identity $\\sin\\!\\left( \\frac{\\pi n}{2 (n+1)} \\right) = \\cos\\!\\left( \\frac{\\pi}{2 (n+1)} \\right)$. Let $c = \\frac{\\lambda_{\\max} + \\lambda_{\\min}}{2}$ and $\\delta = \\frac{\\lambda_{\\max} - \\lambda_{\\min}}{2}$. The Chebyshev semi-iteration of degree $m$ is realized by setting\n$$\n\\alpha_k = \\frac{1}{c - \\delta \\cos\\!\\left( \\frac{\\pi (2k - 1)}{2 m} \\right)}, \\quad k = 1, \\dots, m,\n$$\nand updating\n$$\n\\mathbf{u}^{(k+1)} = \\mathbf{u}^{(k)} + \\alpha_k D^{-1} \\mathbf{r}^{(k)}.\n$$\nThis choice of $\\alpha_k$ is derived by mapping the spectrum interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ to $[-1, 1]$, selecting nodes corresponding to the extrema of the Chebyshev polynomial $T_m$, and choosing the step sizes to minimize the maximal magnitude of the resulting degree-$m$ error polynomial on the spectral interval. Specifically, the error after $m$ steps can be expressed as\n$$\n\\mathbf{e}^{(m)} = p_m(D^{-1} A) \\mathbf{e}^{(0)},\n$$\nwith $p_m$ the degree-$m$ polynomial satisfying $p_m(0) = 1$ and minimizing $\\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |p_m(\\lambda)|$. The given $\\alpha_k$ produce this optimal $p_m$ via the product representation tied to the Chebyshev nodes.\n\nFor each process, after $m$ iterations, we compute the discrete Euclidean norm of the error\n$$\n\\| e \\|_2 = \\left( \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\left( u^{(m)}_{i,j} - \\sin(\\pi x_i)\\sin(\\pi y_j) \\right)^2 \\right)^{1/2},\n$$\nwhich directly compares the iterate to the analytic solution sampled at grid points. Although this norm mixes discretization error with iteration error, the comparison between Process C and Process J under identical discretization and iteration budget isolates the effect of the iteration process. The ratio\n$$\nr = \\frac{\\| e \\|_{2,\\mathrm{C}}}{\\| e \\|_{2,\\mathrm{J}}}\n$$\nquantifies the relative effectiveness of Process C versus Process J for the same $n$ and $m$.\n\nAlgorithmic realization proceeds by:\n- Constructing $u_{\\text{true}, i,j} = \\sin(\\pi x_i) \\sin(\\pi y_j)$ and $b_{i,j} = 2 \\pi^2 u_{\\text{true}, i,j}$ for interior points.\n- Implementing $A u$ via the five-point stencil with homogeneous Dirichlet boundary conditions, which can be handled by zero padding.\n- Iterating Process J: $u \\leftarrow u + \\frac{h^2}{4} (b - A u)$ for exactly $m$ steps.\n- Iterating Process C: with $\\lambda_{\\min}$ and $\\lambda_{\\max}$ as above, compute $c$ and $\\delta$, then for $k = 1, \\dots, m$, set $\\alpha_k = \\frac{1}{c - \\delta \\cos(\\frac{\\pi (2k - 1)}{2 m})}$ and update $u \\leftarrow u + \\alpha_k \\frac{h^2}{4} (b - A u)$.\n- Computing the error norms and their ratio.\n\nThe test suite covers a general case with moderate grid and iteration budget ($n = 32$, $m = 40$), a small grid case ($n = 8$, $m = 10$), and an edge case with a single iteration on a larger grid ($n = 64$, $m = 1$). The final program reports the three ratios rounded to six decimal places in the required format.",
            "answer": "```python\nimport numpy as np\n\ndef setup_problem(n):\n    # Grid spacing and coordinates\n    h = 1.0 / (n + 1)\n    i = np.arange(1, n + 1)\n    x = i * h\n    y = x.copy()\n    X, Y = np.meshgrid(x, y, indexing='ij')\n    # True solution and forcing\n    u_true = np.sin(np.pi * X) * np.sin(np.pi * Y)\n    f = 2.0 * (np.pi ** 2) * u_true\n    b = f  # Right-hand side at interior points\n    return h, u_true, b\n\ndef apply_A(u, h):\n    # Apply 5-point Laplacian stencil with homogeneous Dirichlet boundaries (zero outside)\n    up = np.pad(u, 1, mode='constant', constant_values=0.0)\n    Au = (4.0 * up[1:-1, 1:-1]\n          - up[0:-2, 1:-1] - up[2:, 1:-1]\n          - up[1:-1, 0:-2] - up[1:-1, 2:]) / (h * h)\n    return Au\n\ndef jacobi(u0, b, h, m):\n    # Jacobi expressed as preconditioned Richardson with D^{-1} = (h^2/4) I\n    u = u0.copy()\n    invD = (h * h) / 4.0\n    for _ in range(m):\n        r = b - apply_A(u, h)\n        u = u + invD * r\n    return u\n\ndef chebyshev(u0, b, h, n, m):\n    # Chebyshev semi-iteration with M=D (Jacobi preconditioning)\n    u = u0.copy()\n    invD = (h * h) / 4.0\n    theta = np.pi / (2.0 * (n + 1))\n    lam_min = 2.0 * (np.sin(theta) ** 2)\n    lam_max = 2.0 * (np.cos(theta) ** 2)\n    c = 0.5 * (lam_max + lam_min)\n    delta = 0.5 * (lam_max - lam_min)\n    # Perform exactly m steps with Chebyshev step sizes\n    for k in range(1, m + 1):\n        alpha = 1.0 / (c - delta * np.cos(np.pi * (2 * k - 1) / (2.0 * m)))\n        r = b - apply_A(u, h)\n        u = u + alpha * invD * r\n    return u\n\ndef error_ratio(n, m):\n    h, u_true, b = setup_problem(n)\n    u0 = np.zeros_like(u_true)\n    u_j = jacobi(u0, b, h, m)\n    u_c = chebyshev(u0, b, h, n, m)\n    err_j = np.linalg.norm(u_j - u_true)\n    err_c = np.linalg.norm(u_c - u_true)\n    # Avoid division by zero, though it should not occur for finite m > 0\n    ratio = err_c / err_j if err_j != 0.0 else float('inf')\n    return ratio\n\ndef solve():\n    # Define the test cases: (n, m)\n    test_cases = [\n        (32, 40),\n        (8, 10),\n        (64, 1),\n    ]\n    results = []\n    for n, m in test_cases:\n        r = error_ratio(n, m)\n        results.append(f\"{r:.6f}\")\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Real-world problems rarely fit into perfect squares; they often involve complex geometries with sharp corners and boundaries. This practice moves beyond simple domains to solve Laplace's equation (a special case of Poisson's equation with zero source) on an L-shaped domain. More than just finding a solution, this exercise guides you to use the numerical results as a tool for scientific investigation, allowing you to characterize the singular behavior of the electric field near a re-entrant corner and verify theoretical predictions .",
            "id": "2427929",
            "problem": "Consider the two-dimensional Laplace equation for an electrostatic potential $u(x,y)$ on a bounded planar domain $\\Omega$,\n$$\n\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0 \\quad \\text{in } \\Omega,\n$$\nwith Dirichlet boundary conditions specified on $\\partial \\Omega$ as\n$$\nu(0,y) = 1 \\quad \\text{for } y \\in [0,1] \\text{ that belong to } \\partial \\Omega, \\quad u = 0 \\quad \\text{on the rest of } \\partial \\Omega.\n$$\nTwo domains are to be considered:\n- The L-shaped domain $\\Omega_L = \\{(x,y) \\in [0,1]^2 \\,:\\, x \\le 0.5 \\text{ or } y \\le 0.5\\}$, which has a re-entrant corner at $c = (0.5,0.5)$ with interior angle $\\omega = 3\\pi/2$.\n- The square domain $\\Omega_S = [0,1]^2$, for which the point $c = (0.5,0.5)$ lies in the interior and there is no re-entrant corner.\n\nFor a given integer resolution parameter $N \\ge 2$, define the uniform grid with spacing $h = 1/N$ and grid nodes $(x_i,y_j) = (i h, j h)$ for integers $i,j \\in \\{0,1,\\dots,N\\}$. Assume $N$ is even so that the re-entrant corner $c$ aligns with grid lines. Let $u_{i,j}$ denote the numerical approximation to $u(x_i,y_j)$ obtained by exactly enforcing the Dirichlet boundary conditions and by solving the Laplace equation in the interior of the chosen domain.\n\nDefine the gradient magnitude at grid nodes where it is well-defined by centered differences as\n$$\n\\left|\\nabla u\\right|_{i,j} = \\sqrt{\\left(\\frac{u_{i+1,j} - u_{i-1,j}}{2h}\\right)^2 + \\left(\\frac{u_{i,j+1} - u_{i,j-1}}{2h}\\right)^2},\n$$\nwhenever all referenced values belong to the domain. For the fixed point $c = (0.5,0.5)$, define its discrete distance to a grid node $(x_i,y_j)$ by\n$$\nr_{i,j} = \\sqrt{(x_i - 0.5)^2 + (y_j - 0.5)^2}.\n$$\nFor each integer ring index $k \\in \\{2,3,\\dots,10\\}$, define the ring\n$$\n\\mathcal{A}_k = \\left\\{ (i,j) \\,:\\, \\text{the gradient magnitude is defined at } (i,j) \\text{ and } k h \\le r_{i,j} < (k+1) h \\right\\}.\n$$\nFor each nonempty ring $\\mathcal{A}_k$, compute the median gradient magnitude\n$$\nG_k = \\operatorname{median}\\left\\{ \\left|\\nabla u\\right|_{i,j} \\,:\\, (i,j) \\in \\mathcal{A}_k \\right\\},\n$$\nand associate it with the representative radius $R_k = k h$.\n\nDefine the estimated singularity exponent $\\hat{\\alpha}$ as the slope of the best-fit line, in the least-squares sense, of the data $(\\log R_k, \\log G_k)$ over all nonempty rings $k \\in \\{2,3,\\dots,10\\}$. That is,\n$$\n\\hat{\\alpha} = \\arg\\min_{a,b} \\sum_{k} \\left(\\log G_k - (a \\log R_k + b)\\right)^2, \\quad \\text{and we report } a \\text{ as } \\hat{\\alpha}.\n$$\n\nTest Suite:\n- Case $1$: $\\Omega = \\Omega_L$, $N = 96$.\n- Case $2$: $\\Omega = \\Omega_L$, $N = 144$.\n- Case $3$: $\\Omega = \\Omega_S$, $N = 96$.\n\nAnswer Specification:\n- For each case, compute the corresponding $\\hat{\\alpha}$ as a floating-point number.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, in the order of Cases $1$, $2$, $3$, with each value rounded to three decimal places (for example, [-0.333, -0.332, 0.000]).\n- No physical units are involved. All angles, when implicitly present through $\\omega$, are in radians by definition.",
            "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational physics, specifically concerning the numerical solution of the two-dimensional Laplace equation and the analysis of solution singularities. The problem statement is complete, unambiguous, and all provided data are consistent. Therefore, a rigorous solution can be constructed.\n\nThe core of the problem is to solve the Laplace equation, $\\Delta u = 0$, on two distinct domains: an L-shaped domain $\\Omega_L$ and a square domain $\\Omega_S$. The equation is discretized using a uniform grid with spacing $h = 1/N$. At each interior grid node $(x_i, y_j)$, the Laplacian operator $\\Delta$ is approximated by the standard five-point finite difference stencil. This leads to the discrete equation:\n$$\n\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0\n$$\nwhich simplifies to an algebraic relation for the potential $u_{i,j}$ at an interior node in terms of its four nearest neighbors:\n$$\n4u_{i,j} - u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} = 0\n$$\nThis equation is applied to every interior node of the domain. Nodes located on the boundary $\\partial \\Omega$ have their potential values prescribed by the Dirichlet conditions: $u(0,y) = 1$ for the applicable portion of the left boundary and $u=0$ elsewhere.\n\nApplying the finite difference stencil at all interior nodes results in a large, sparse system of linear algebraic equations of the form $A\\mathbf{x} = \\mathbf{b}$. Here, $\\mathbf{x}$ is the vector of unknown potential values $u_{i,j}$ at all interior nodes, the matrix $A$ encodes the connectivity defined by the five-point stencil, and the vector $\\mathbf{b}$ incorporates the values from the fixed boundary conditions. This linear system is guaranteed to have a unique solution, which can be found efficiently using a sparse linear algebra solver, such as `spsolve` from the `scipy.sparse.linalg` library.\n\nA key aspect of this problem is the behavior of the solution's gradient near the point $c = (0.5, 0.5)$.\nFor the L-shaped domain $\\Omega_L$, this point is a re-entrant corner with an interior angle $\\omega = 3\\pi/2$. It is a well-established result from the theory of elliptic partial differential equations that the solution exhibits a singularity at such a corner. The potential $u$ behaves as $u \\sim r^{\\alpha}$ near the corner, where $r$ is the radial distance from the corner and the exponent $\\alpha$ is given by $\\pi/\\omega$. For $\\omega = 3\\pi/2$, we have $\\alpha = \\pi / (3\\pi/2) = 2/3$. Consequently, the magnitude of the gradient is expected to behave as $|\\nabla u| \\sim r^{\\alpha-1} = r^{2/3 - 1} = r^{-1/3}$. Therefore, a log-log plot of $|\\nabla u|$ versus $r$ should yield a straight line with a slope of approximately $-1/3$. The problem asks for the estimation of this slope, denoted $\\hat{\\alpha}$.\n\nFor the square domain $\\Omega_S$, the point $c = (0.5, 0.5)$ lies in the interior of the domain, far from any boundary corners. The solution to the Laplace equation inside a convex domain with smooth boundary data is analytic (infinitely differentiable). Thus, the potential $u(x,y)$ and its gradient are smooth functions in the neighborhood of $c$. The gradient magnitude $|\\nabla u|$ will be approximately constant for small distances $r$ from $c$. A log-log plot of a nearly constant function versus $r$ will have a slope close to $0$. We therefore expect the estimated singularity exponent $\\hat{\\alpha}$ to be approximately $0$ for this case.\n\nThe solution proceeds as follows:\n1.  For each test case (domain type and resolution $N$), generate a grid and identify the interior and boundary nodes. The condition that $N$ is even ensures the point $c$ aligns with a grid node.\n2.  Construct the sparse matrix $A$ and the right-hand side vector $\\mathbf{b}$ corresponding to the system of finite difference equations. The boundary conditions are enforced by moving known boundary values to the vector $\\mathbf{b}$.\n3.  Solve the linear system $A\\mathbf{x} = \\mathbf{b}$ to find the potential $u_{i,j}$ at all interior nodes.\n4.  Compute the gradient magnitude $|\\nabla u|_{i,j}$ at each grid node $(i,j)$ where the centered difference stencil is valid (i.e., where all four neighbors used in the stencil, $(i \\pm 1, j)$ and $(i, j \\pm 1)$, lie within the domain or on its boundary).\n5.  For each integer ring index $k$ from $2$ to $10$, identify all valid grid nodes $(i,j)$ whose distance $r_{i,j}$ from $c$ satisfies $k h \\le r_{i,j} < (k+1) h$.\n6.  For each non-empty ring $\\mathcal{A}_k$, calculate the median gradient magnitude, $G_k$. This use of the median provides a robust measure against outliers.\n7.  Collect the data pairs $(\\log R_k, \\log G_k)$, where $R_k = k h$.\n8.  Perform a linear least-squares regression on these data points to find the slope of the best-fit line. This slope is the estimated singularity exponent $\\hat{\\alpha}$.\n\nThis systematic procedure will be implemented for the three specified test cases to obtain the required numerical values for $\\hat{\\alpha}$. The results for the L-shaped domain with increasing $N$ are expected to converge toward the theoretical value of $-1/3$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef get_domain_masks(N, domain_type):\n    \"\"\"\n    Generates boolean masks for the domain, identifying interior points.\n    N must be an even integer.\n    \"\"\"\n    is_in_domain = np.zeros((N + 1, N + 1), dtype=bool)\n    is_interior = np.zeros((N + 1, N + 1), dtype=bool)\n\n    if domain_type == 'L_shape':\n        ic = N // 2\n        for i in range(N + 1):\n            for j in range(N + 1):\n                if i <= ic or j <= ic:\n                    is_in_domain[i, j] = True\n    elif domain_type == 'square':\n        is_in_domain.fill(True)\n    else:\n        raise ValueError(\"Unknown domain type\")\n\n    # An interior point must be in the domain, and so must its 4 cardinal neighbors.\n    for i in range(1, N):\n        for j in range(1, N):\n            if (is_in_domain[i, j] and\n                    is_in_domain[i - 1, j] and is_in_domain[i + 1, j] and\n                    is_in_domain[i, j - 1] and is_in_domain[i, j + 1]):\n                is_interior[i, j] = True\n\n    return is_in_domain, is_interior\n\ndef solve_laplace(N, domain_type):\n    \"\"\"\n    Solves the 2D Laplace equation on a uniform grid for a given domain.\n    \"\"\"\n    h = 1.0 / N\n    is_in_domain, is_interior = get_domain_masks(N, domain_type)\n\n    u = np.zeros((N + 1, N + 1), dtype=float)\n\n    # Apply Dirichlet boundary conditions\n    # u(0,y) = 1\n    u[0, :] = 1.0\n    # Rest of the boundary is u=0, which is the default from np.zeros.\n\n    # Map interior grid points to a 1D index for the linear system\n    interior_points = np.argwhere(is_interior)\n    num_unknowns = len(interior_points)\n    point_to_idx = {tuple(p): i for i, p in enumerate(interior_points)}\n\n    # Assemble the sparse matrix A and vector b for Ax=b\n    A = lil_matrix((num_unknowns, num_unknowns), dtype=float)\n    b = np.zeros(num_unknowns, dtype=float)\n\n    for k, (i, j) in enumerate(interior_points):\n        A[k, k] = 4.0\n        # Check neighbors\n        for ni, nj in [(i - 1, j), (i + 1, j), (i, j - 1), (i, j + 1)]:\n            if is_interior[ni, nj]:\n                # Neighbor is another unknown\n                neighbor_idx = point_to_idx[(ni, nj)]\n                A[k, neighbor_idx] = -1.0\n            else:\n                # Neighbor is a boundary point with a known value\n                b[k] += u[ni, nj]\n\n    # Solve the sparse linear system\n    # Convert to CSC format for efficient solving\n    A_csc = A.tocsc()\n    x = spsolve(A_csc, b)\n\n    # Transfer solution vector back to the 2D grid\n    for k, (i, j) in enumerate(interior_points):\n        u[i, j] = x[k]\n\n    return u, h, is_in_domain\n\ndef estimate_singularity_exponent(u, h, N, is_in_domain):\n    \"\"\"\n    Estimates the singularity exponent alpha_hat from the numerical solution.\n    \"\"\"\n    # Identify points where the centered-difference gradient is well-defined\n    grad_definable = np.zeros((N + 1, N + 1), dtype=bool)\n    for i in range(1, N):\n        for j in range(1, N):\n            if (is_in_domain[i - 1, j] and is_in_domain[i + 1, j] and\n                    is_in_domain[i, j - 1] and is_in_domain[i, j + 1]):\n                grad_definable[i, j] = True\n\n    definable_points = np.argwhere(grad_definable)\n\n    log_R_k_list, log_G_k_list = [], []\n\n    # Analyze rings k = 2, 3, ..., 10\n    for k in range(2, 11):\n        R_k = k * h\n        r_min, r_max = k * h, (k + 1) * h\n        gradients_in_ring = []\n\n        for i, j in definable_points:\n            x, y = i * h, j * h\n            r_ij = np.sqrt((x - 0.5)**2 + (y - 0.5)**2)\n\n            if r_min <= r_ij < r_max:\n                du_dx = (u[i + 1, j] - u[i - 1, j]) / (2 * h)\n                du_dy = (u[i, j + 1] - u[i, j - 1]) / (2 * h)\n                grad_mag = np.sqrt(du_dx**2 + du_dy**2)\n                gradients_in_ring.append(grad_mag)\n\n        if gradients_in_ring:\n            G_k = np.median(gradients_in_ring)\n            # Avoid log(0) in case median is zero\n            if G_k > 1e-12:\n                log_R_k_list.append(np.log(R_k))\n                log_G_k_list.append(np.log(G_k))\n\n    # Cannot perform regression with fewer than 2 points\n    if len(log_R_k_list) < 2:\n        return 0.0\n\n    # Perform linear least-squares regression: log(G) = a*log(R) + b\n    # The slope 'a' is the desired exponent alpha_hat.\n    coeffs = np.polyfit(log_R_k_list, log_G_k_list, 1)\n    alpha_hat = coeffs[0]\n\n    return alpha_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        ('L_shape', 96),\n        ('L_shape', 144),\n        ('square', 96),\n    ]\n\n    results = []\n    for domain_type, N in test_cases:\n        u, h, is_in_domain = solve_laplace(N, domain_type)\n        alpha_hat = estimate_singularity_exponent(u, h, N, is_in_domain)\n        results.append(alpha_hat)\n\n    # Format output as specified: comma-separated list in brackets,\n    # with each value rounded to three decimal places.\n    formatted_results = [f'{r:.3f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}