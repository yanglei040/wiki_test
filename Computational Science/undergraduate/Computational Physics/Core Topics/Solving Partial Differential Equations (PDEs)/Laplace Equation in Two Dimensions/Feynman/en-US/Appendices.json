{
    "hands_on_practices": [
        {
            "introduction": "Before diving into complex numerical simulations, it is crucial to understand the fundamental behavior of solutions to the Laplace equation. This exercise explores the Maximum Principle, a cornerstone theorem stating that a non-constant harmonic function must attain its extreme values on the boundary of its domain. Applying this principle  allows you to predict and verify the behavior of physical fields like electrostatic potentials or steady-state temperature distributions without solving the full system.",
            "id": "2249485",
            "problem": "In physics, the real part of an analytic complex function often represents a scalar potential, such as electrostatic potential or temperature distribution. Such functions are harmonic, meaning they satisfy Laplace's equation in regions where the complex function is analytic.\n\nConsider a harmonic potential function $\\Phi(x, y)$ defined on the closed unit disk, $D = \\{z \\in \\mathbb{C} : |z| \\leq 1\\}$, where $z=x+iy$. The potential is given by $\\Phi(x, y) = \\text{Re}(F(z))$ for the complex function $F(z) = (z-a)^2$. The constant $a$ is a real number specified as $a=1/2$.\n\nYour task is to locate the points on the disk $D$ where the potential $\\Phi(x,y)$ reaches its absolute maximum and absolute minimum values. Which of the following options correctly identifies the locations of the maximum (denoted $z_{max}$) and minimum (denoted $z_{min}$)?\n\nA. $z_{max} = 1$; $z_{min} = -1$.\n\nB. $z_{max} = -1$; $z_{min}$ are the two points in the set $\\{ \\frac{1}{4} + i\\frac{\\sqrt{15}}{4}, \\frac{1}{4} - i\\frac{\\sqrt{15}}{4} \\}$.\n\nC. $z_{max}$ are the two points in the set $\\{1, -1\\}$; $z_{min}$ are the two points in the set $\\{i, -i\\}$.\n\nD. $z_{max} = 1/2$; $z_{min} = -1$.\n\nE. $z_{max} = 1$; $z_{min}$ are the two points in the set $\\{ \\frac{1}{2} + i\\frac{\\sqrt{3}}{2}, \\frac{1}{2} - i\\frac{\\sqrt{3}}{2} \\}$.",
            "solution": "We are given $F(z)=(z-a)^{2}$ with $a=\\frac{1}{2}$ real, and $\\Phi(x,y)=\\operatorname{Re}(F(z))$ on the closed unit disk $D=\\{z\\in\\mathbb{C}:|z|\\leq 1\\}$. Since $F$ is analytic on a neighborhood of $D$, $\\Phi$ is harmonic on $D$. By the maximum/minimum principle for harmonic functions, a non-constant harmonic function attains its absolute maximum and minimum on the boundary $|z|=1$.\n\nWrite $z=x+iy$ and set $u=x-\\frac{1}{2}$, $v=y$. Then\n$$\nF(z)=(u+iv)^{2}=u^{2}-v^{2}+2iuv,\n$$\nso\n$$\n\\Phi(x,y)=\\operatorname{Re}(F(z))=u^{2}-v^{2}=(x-\\tfrac{1}{2})^{2}-y^{2}.\n$$\nRestricting to the boundary $x^{2}+y^{2}=1$, parametrize by $x=\\cos\\theta$, $y=\\sin\\theta$. Then\n$$\n\\Phi(\\theta)=(\\cos\\theta-\\tfrac{1}{2})^{2}-\\sin^{2}\\theta=\\cos^{2}\\theta-\\cos\\theta+\\tfrac{1}{4}-(1-\\cos^{2}\\theta)=2\\cos^{2}\\theta-\\cos\\theta-\\tfrac{3}{4}.\n$$\nLet $u=\\cos\\theta\\in[-1,1]$. Then\n$$\n\\Phi(u)=2u^{2}-u-\\tfrac{3}{4}.\n$$\nThis is a quadratic in $u$ with positive leading coefficient, so its minimum over $[-1,1]$ occurs at the vertex $u=\\frac{1}{4}$, and its maximum over $[-1,1]$ occurs at an endpoint. Compute the values:\n$$\n\\Phi\\!\\left(\\tfrac{1}{4}\\right)=2\\cdot\\tfrac{1}{16}-\\tfrac{1}{4}-\\tfrac{3}{4}=\\tfrac{1}{8}-1=-\\tfrac{7}{8},\n$$\n$$\n\\Phi(-1)=2\\cdot 1-(-1)-\\tfrac{3}{4}=\\tfrac{9}{4},\\quad \\Phi(1)=2\\cdot 1-1-\\tfrac{3}{4}=\\tfrac{1}{4}.\n$$\nTherefore, the absolute maximum is at $u=-1$, i.e., $x=\\cos\\theta=-1$, $y=0$, so $z_{\\max}=-1$. The absolute minima occur where $u=\\cos\\theta=\\frac{1}{4}$, which gives $x=\\frac{1}{4}$ and $y=\\pm\\sqrt{1-\\frac{1}{16}}=\\pm\\frac{\\sqrt{15}}{4}$, i.e., $z_{\\min}\\in\\left\\{\\frac{1}{4}\\pm i\\frac{\\sqrt{15}}{4}\\right\\}$.\n\nThese locations match option B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The true power of computational physics lies in creating tools to solve problems that are intractable analytically, especially those with complex geometries or boundary conditions. This core practice guides you through building a versatile finite difference solver for the 2D Laplace equation from first principles . You will derive and implement the discrete approximations for both the interior domain and the more nuanced Neumann (insulating) boundary conditions, a fundamental skill for tackling a wide range of physics and engineering problems.",
            "id": "2406696",
            "problem": "Consider the two-dimensional Laplace equation, which is the prototype elliptic partial differential equation (PDE) for steady-state, source-free fields. The governing equation in a rectangular domain is\n$$\n\\nabla^2 u(x,y) \\equiv \\frac{\\partial^2 u}{\\partial x^2}(x,y) + \\frac{\\partial^2 u}{\\partial y^2}(x,y) = 0 \\quad \\text{for} \\quad (x,y)\\in \\Omega,\n$$\nwith $\\Omega = [0,1]\\times[0,1]$. Boundary conditions are prescribed on the boundary $\\partial\\Omega$, and may be of Dirichlet type $u = d(x,y)$ or Neumann type $\\frac{\\partial u}{\\partial n} = g(x,y)$, where $\\frac{\\partial}{\\partial n}$ denotes the outward normal derivative. The aim is to construct a computational solver that can handle a mix of Dirichlet and Neumann conditions on different, possibly piecewise, segments of the boundary.\n\nYour task is to write a complete, runnable program that:\n- Discretizes the domain $\\Omega$ with a uniform Cartesian grid of $N\\times N$ nodes, where $N$ is specified per test case.\n- Derives from first principles a second-order accurate finite difference method for the interior discretization of $\\nabla^2 u = 0$ based on Taylor expansions, and a second-order accurate treatment of Neumann boundary conditions along the inward normal direction (do not assume any formula without derivation; use the definitions of derivatives and Taylor series to construct second-order approximations).\n- Assembles and solves the resulting linear system for $u$ at all grid nodes.\n- Supports boundary conditions specified piecewise along each side of the square. For each side, segments are given in terms of a parameter $s\\in[0,1]$ that traverses the side: for the left and right sides $s=y$, and for the bottom and top sides $s=x$. At each grid boundary node, select the appropriate boundary condition type and value by locating $s$ within the prescribed segment intervals. At the four corner nodes, impose Dirichlet values taken from the exact solution specified for the test case to remove any ambiguity.\n- For Neumann data, the outward normal derivative must be used. The outward unit normal is $(-\\hat{\\boldsymbol{i}})$ on the left, $(+\\hat{\\boldsymbol{i}})$ on the right, $(-\\hat{\\boldsymbol{j}})$ on the bottom, and $(+\\hat{\\boldsymbol{j}})$ on the top sides.\n\nUse the following test suite. In each case, an exact harmonic function $u^\\star(x,y)$ is provided. You must construct boundary condition data consistent with $u^\\star(x,y)$ so that the exact solution satisfies both the PDE and the boundary conditions. For Neumann data, compute $g(x,y)=\\frac{\\partial u^\\star}{\\partial n}(x,y)$ using the outward normal definition above.\n\n- Test case A (general mixed, smooth polynomial):\n  - Domain: $\\Omega=[0,1]\\times[0,1]$.\n  - Grid: $N=51$ (so the grid spacing is $h=\\frac{1}{N-1}$).\n  - Exact solution: $u^\\star(x,y)=x^2 - y^2$.\n  - Boundary segmentation:\n    - Left side ($x=0$): Dirichlet on $s\\in[0,1]$ with $u = u^\\star(0,y)$.\n    - Right side ($x=1$): Dirichlet on $s\\in[0,1]$ with $u = u^\\star(1,y)$.\n    - Bottom side ($y=0$): Neumann on $s\\in[0,1]$ with $\\frac{\\partial u}{\\partial n} = -\\frac{\\partial u^\\star}{\\partial y}(x,0)$.\n    - Top side ($y=1$): Neumann on $s\\in[0,1]$ with $\\frac{\\partial u}{\\partial n} = +\\frac{\\partial u^\\star}{\\partial y}(x,1)$.\n    - Corners: Dirichlet set to $u^\\star$.\n- Test case B (piecewise mix on a side):\n  - Domain: $\\Omega=[0,1]\\times[0,1]$.\n  - Grid: $N=51$.\n  - Exact solution: $u^\\star(x,y)=x$.\n  - Boundary segmentation:\n    - Left side ($x=0$): Dirichlet on $s\\in[0,1]$ with $u = u^\\star(0,y)$.\n    - Right side ($x=1$): Neumann on $s\\in[0,1]$ with $\\frac{\\partial u}{\\partial n} = +\\frac{\\partial u^\\star}{\\partial x}(1,y)$.\n    - Bottom side ($y=0$): Dirichlet on $s\\in[0,1]$ with $u = u^\\star(x,0)$.\n    - Top side ($y=1$): Piecewise along $s=x$:\n      - Neumann on $s\\in[0,0.4]$ with $\\frac{\\partial u}{\\partial n} = +\\frac{\\partial u^\\star}{\\partial y}(x,1)$,\n      - Dirichlet on $s\\in(0.4,1]$ with $u = u^\\star(x,1)$.\n    - Corners: Dirichlet set to $u^\\star$.\n- Test case C (edge case with small grid and homogeneous Neumann on two sides):\n  - Domain: $\\Omega=[0,1]\\times[0,1]$.\n  - Grid: $N=9$.\n  - Exact solution: $u^\\star(x,y)=y$.\n  - Boundary segmentation:\n    - Left side ($x=0$): Neumann on $s\\in[0,1]$ with $\\frac{\\partial u}{\\partial n} = -\\frac{\\partial u^\\star}{\\partial x}(0,y)$.\n    - Right side ($x=1$): Neumann on $s\\in[0,1]$ with $\\frac{\\partial u}{\\partial n} = +\\frac{\\partial u^\\star}{\\partial x}(1,y)$.\n    - Bottom side ($y=0$): Dirichlet on $s\\in[0,1]$ with $u = u^\\star(x,0)$.\n    - Top side ($y=1$): Dirichlet on $s\\in[0,1]$ with $u = u^\\star(x,1)$.\n    - Corners: Dirichlet set to $u^\\star$.\n\nRequired numerical design:\n- Interior discretization: Derive a second-order accurate five-point finite difference approximation to $\\nabla^2 u = 0$ on interior nodes using Taylor expansions about a grid node and the definition of second derivatives as limits of symmetric difference quotients.\n- Neumann boundary discretization: Derive a second-order accurate one-sided finite difference approximation for the outward normal derivative at boundary nodes along the inward normal direction. Construct a linear relation between the boundary node and the first two interior nodes along the inward normal direction and the Neumann data $g$ that is consistent to second order in $h$.\n- Dirichlet boundary discretization: Enforce the prescribed value of $u$ at boundary nodes exactly.\n\nFor each test case, compute the maximum absolute error\n$$\nE_\\infty = \\max_{0\\le i,j \\le N-1} \\left| u_{i,j}^{\\text{num}} - u^\\star(x_i,y_j) \\right|\n$$\nover all grid nodes, where $x_i = i\\,h$, $y_j = j\\,h$, and $h=\\frac{1}{N-1}$.\n\nFinal output format:\n- Your program must produce a single line of output containing a list with the three error values for Test cases A, B, and C, in this order, rounded to eight decimal places. The format must be exactly\n- A single line: \"[eA,eB,eC]\" where each entry is a decimal numeral (e.g., \"[0.00000000,0.00000000,0.00000000]\").\n\nNo physical units are involved in this problem. No angles are used. No percentages are required. The program must be fully self-contained and must not read any input. It must run \"as is\" and produce the required line of output. Ensure scientific realism and internal consistency in all choices and derivations. Your algorithm must start from the fundamental definitions of derivatives and Taylor expansions; no shortcut formulas may be assumed without derivation in your implementation design. The solver must be general enough to handle the piecewise boundary specifications given above.",
            "solution": "The user requests a computational solution to the two-dimensional Laplace equation on a unit square, $\\nabla^2 u = 0$, utilizing a second-order finite difference method for both interior nodes and Neumann boundary conditions. The problem is well-defined, scientifically sound, and all necessary parameters for the test cases are provided. The problem validity check is passed. We shall proceed with the derivation and construction of the solver.\n\nThe governing partial differential equation (PDE) is the Laplace equation in a domain $\\Omega = [0,1]\\times[0,1]$:\n$$\n\\frac{\\partial^2 u}{\\partial x^2}(x,y) + \\frac{\\partial^2 u}{\\partial y^2}(x,y) = 0\n$$\n\nWe discretize the domain $\\Omega$ using a uniform Cartesian grid with $N \\times N$ nodes. The grid points are denoted by $(x_i, y_j) = (i h, j h)$ for $i, j \\in \\{0, 1, \\dots, N-1\\}$, where the grid spacing is $h = \\frac{1}{N-1}$. The value of the function $u$ at a grid point $(x_i, y_j)$ is denoted by $u_{i,j}$.\n\nDerivation of the Finite Difference Stencils\n\nThe core of the method is the approximation of partial derivatives using Taylor series expansions.\n\n1. Interior Node Discretization\n\nFor an interior node $(i,j)$, where $0 < i < N-1$ and $0 < j < N-1$, we approximate the second partial derivatives. The Taylor series expansions of $u$ around $(x_i, y_j)$ in the $x$-direction are:\n$$\nu(x_i+h, y_j) = u_{i,j} + h \\left(\\frac{\\partial u}{\\partial x}\\right)_{i,j} + \\frac{h^2}{2} \\left(\\frac{\\partial^2 u}{\\partial x^2}\\right)_{i,j} + \\frac{h^3}{6} \\left(\\frac{\\partial^3 u}{\\partial x^3}\\right)_{i,j} + O(h^4)\n$$\n$$\nu(x_i-h, y_j) = u_{i,j} - h \\left(\\frac{\\partial u}{\\partial x}\\right)_{i,j} + \\frac{h^2}{2} \\left(\\frac{\\partial^2 u}{\\partial x^2}\\right)_{i,j} - \\frac{h^3}{6} \\left(\\frac{\\partial^3 u}{\\partial x^3}\\right)_{i,j} + O(h^4)\n$$\nAdding these two expansions eliminates the odd-order derivative terms:\n$$\nu_{i+1,j} + u_{i-1,j} = 2u_{i,j} + h^2 \\left(\\frac{\\partial^2 u}{\\partial x^2}\\right)_{i,j} + O(h^4)\n$$\nSolving for the second derivative, we obtain the second-order accurate central difference approximation:\n$$\n\\left(\\frac{\\partial^2 u}{\\partial x^2}\\right)_{i,j} = \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + O(h^2)\n$$\nBy analogy, the approximation for the second derivative in the $y$-direction is:\n$$\n\\left(\\frac{\\partial^2 u}{\\partial y^2}\\right)_{i,j} = \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} + O(h^2)\n$$\nSubstituting these into the Laplace equation $\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$ gives the discrete equation for an interior node:\n$$\n\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0\n$$\nMultiplying by $h^2$, we arrive at the well-known five-point stencil for the discrete Laplacian:\n$$\nu_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0\n$$\n\n2. Neumann Boundary Condition Discretization\n\nThe problem requires a second-order accurate one-sided approximation for the normal derivative $\\frac{\\partial u}{\\partial n} = g(x,y)$. Let us derive this for the right boundary at $x=1$ (i.e., for nodes $(N-1, j)$), where the outward normal is in the $+x$ direction, so $\\frac{\\partial u}{\\partial n} = \\frac{\\partial u}{\\partial x}$. We must construct a linear relation involving the boundary node $u_{N-1,j}$ and the two adjacent interior nodes along the inward normal, $u_{N-2,j}$ and $u_{N-3,j}$.\n\nWe write the Taylor expansions for $u_{N-2,j}$ and $u_{N-3,j}$ around the boundary node $(x_{N-1}, y_j)$:\n$$\nu_{N-2,j} = u(x_{N-1}-h, y_j) = u_{N-1,j} - h \\left(\\frac{\\partial u}{\\partial x}\\right)_{N-1,j} + \\frac{h^2}{2!} \\left(\\frac{\\partial^2 u}{\\partial x^2}\\right)_{N-1,j} + O(h^3)\n$$\n$$\nu_{N-3,j} = u(x_{N-1}-2h, y_j) = u_{N-1,j} - 2h \\left(\\frac{\\partial u}{\\partial x}\\right)_{N-1,j} + \\frac{(2h)^2}{2!} \\left(\\frac{\\partial^2 u}{\\partial x^2}\\right)_{N-1,j} + O(h^3)\n$$\nWe form a linear combination $A u_{N-2,j} + B u_{N-3,j}$ to eliminate the second derivative term $\\frac{\\partial^2 u}{\\partial x^2}$. We need $A \\frac{h^2}{2} + B \\frac{4h^2}{2} = 0$, which implies $A + 4B = 0$. Choosing $A=4$ and $B=-1$ yields:\n$$\n4 u_{N-2,j} - u_{N-3,j} = (4-1) u_{N-1,j} + (-4h+2h) \\left(\\frac{\\partial u}{\\partial x}\\right)_{N-1,j} + O(h^3)\n$$\n$$\n4 u_{N-2,j} - u_{N-3,j} = 3 u_{N-1,j} - 2h \\left(\\frac{\\partial u}{\\partial x}\\right)_{N-1,j} + O(h^3)\n$$\nSolving for the first derivative gives the second-order accurate one-sided finite difference formula:\n$$\n\\left(\\frac{\\partial u}{\\partial x}\\right)_{N-1,j} = \\frac{3u_{N-1,j} - 4u_{N-2,j} + u_{N-3,j}}{2h} + O(h^2)\n$$\nGiven the Neumann condition $\\frac{\\partial u}{\\partial x}|_{N-1,j} = g_{N-1,j}$, the discrete equation for this boundary node becomes:\n$$\n3u_{N-1,j} - 4u_{N-2,j} + u_{N-3,j} = 2h g_{N-1,j}\n$$\nAnalogous formulas are derived for the other three boundaries, taking into account the direction of the outward normal:\n- Left boundary ($i=0, \\frac{\\partial u}{\\partial n} = -\\frac{\\partial u}{\\partial x} = g$): $3u_{0,j} - 4u_{1,j} + u_{2,j} = -2h g_{0,j}$\n- Bottom boundary ($j=0, \\frac{\\partial u}{\\partial n} = -\\frac{\\partial u}{\\partial y} = g$): $3u_{i,0} - 4u_{i,1} + u_{i,2} = -2h g_{i,0}$\n- Top boundary ($j=N-1, \\frac{\\partial u}{\\partial n} = \\frac{\\partial u}{\\partial y} = g$): $3u_{i,N-1} - 4u_{i,N-2} + u_{i,N-3} = 2h g_{i,N-1}$\n\nSystem Assembly and Solution\n\nThe discrete equations for all $N^2$ nodes form a large linear system $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector containing all unknown values $u_{i,j}$. We map the 2D grid index $(i,j)$ to a 1D vector index $k = j \\times N + i$. The matrix $A$ and vector $\\mathbf{b}$ are constructed row by row, with one row for each grid node $k$:\n\n1.  **Interior Node $(i,j)$**: The equation is $u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0$. The corresponding matrix row $k$ will have a coefficient of $-4$ on the main diagonal ($A_{k,k}$), and coefficients of $1$ at columns corresponding to its four neighbors. The right-hand side entry $b_k$ is $0$.\n\n2.  **Dirichlet Boundary Node $(i,j)$**: The condition is $u_{i,j} = d_{i,j}$. This is enforced directly. The matrix row $k$ becomes an identity row: $A_{k,k} = 1$, all other $A_{k,m}=0$, and $b_k = d_{i,j}$.\n\n3.  **Neumann Boundary Node $(i,j)$**: The equation is one of the one-sided stencils derived above. For a node on the right boundary, the equation $3u_{N-1,j} - 4u_{N-2,j} + u_{N-3,j} = 2h g_{N-1,j}$ is used. The matrix row $k$ corresponding to $(N-1, j)$ will have coefficients $3, -4, 1$ at columns mapping to $(N-1,j), (N-2,j), (N-3,j)$, respectively. The right-hand side is $b_k = 2h g_{N-1,j}$.\n\n4.  **Corner Nodes**: The problem specifies that corner nodes are always assigned Dirichlet values from the exact solution $u^\\star$. This is a special case of a Dirichlet boundary condition and is crucial for removing ambiguity, especially if two Neumann boundaries meet at a corner.\n\nThe resulting linear system is large and sparse. It is constructed using a sparse matrix format (e.g., `scipy.sparse.lil_matrix`) for efficiency and solved using a direct sparse solver (`scipy.sparse.linalg.spsolve`). After solving for the vector $\\mathbf{u}$, it is reshaped back into an $N \\times N$ grid representing the numerical solution $u_{i,j}^{\\text{num}}$. This solution is then compared to the exact solution $u^\\star(x_i, y_j)$ on the grid to compute the maximum absolute error $E_\\infty = \\max_{i,j} |u_{i,j}^{\\text{num}} - u^\\star(x_i, y_j)|$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve_laplace_2d(N, u_star, bc_config):\n    \"\"\"\n    Solves the 2D Laplace equation on a unit square using a second-order\n    finite difference method with mixed Dirichlet/Neumann boundary conditions.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    num_unknowns = N * N\n    \n    # Initialize sparse matrix A and vector b\n    A = lil_matrix((num_unknowns, num_unknowns))\n    b = np.zeros(num_unknowns)\n    \n    # Create coordinate grids\n    x_coords = np.linspace(0, 1, N)\n    y_coords = np.linspace(0, 1, N)\n    \n    for j in range(N):\n        for i in range(N):\n            k = j * N + i  # Row-major mapping from (i, j) to k\n            x, y = x_coords[i], y_coords[j]\n            \n            # 1. Corner nodes (always Dirichlet from exact solution)\n            is_corner = (i == 0 or i == N - 1) and (j == 0 or j == N - 1)\n            if is_corner:\n                A[k, k] = 1.0\n                b[k] = u_star(x, y)\n                continue\n\n            # 2. Boundary nodes (not corners)\n            side_name = None\n            s_coord = -1.0\n            if i == 0:\n                side_name, s_coord = 'left', y\n            elif i == N - 1:\n                side_name, s_coord = 'right', y\n            elif j == 0:\n                side_name, s_coord = 'bottom', x\n            elif j == N - 1:\n                side_name, s_coord = 'top', x\n\n            if side_name:\n                segments = bc_config[side_name]\n                bc_type, bc_value_func = None, None\n                \n                # Find the correct piecewise segment for this node\n                for seg_s_max, seg_type, seg_func in segments:\n                    # The condition `s_coord <= seg_s_max` correctly handles\n                    # intervals like [0, 0.4] and (0.4, 1.0]. A small\n                    # tolerance is added for floating point comparison.\n                    if s_coord <= seg_s_max + 1e-9:\n                        bc_type = seg_type\n                        bc_value_func = seg_func\n                        break\n                \n                if bc_type == 'D': # Dirichlet condition\n                    A[k, k] = 1.0\n                    b[k] = bc_value_func(x, y)\n                elif bc_type == 'N': # Neumann condition\n                    g_val = bc_value_func(x, y)\n                    if side_name == 'left': # d/dn = -d/dx = g -> 3u_0-4u_1+u_2 = -2hg\n                        A[k, j * N + 0] = 3.0\n                        A[k, j * N + 1] = -4.0\n                        A[k, j * N + 2] = 1.0\n                        b[k] = -2.0 * h * g_val\n                    elif side_name == 'right': # d/dn = d/dx = g -> 3u_{N-1}-4u_{N-2}+u_{N-3} = 2hg\n                        A[k, j * N + (N - 1)] = 3.0\n                        A[k, j * N + (N - 2)] = -4.0\n                        A[k, j * N + (N - 3)] = 1.0\n                        b[k] = 2.0 * h * g_val\n                    elif side_name == 'bottom': # d/dn = -d/dy = g -> 3u_0-4u_1+u_2 = -2hg\n                        A[k, (0 * N) + i] = 3.0\n                        A[k, (1 * N) + i] = -4.0\n                        A[k, (2 * N) + i] = 1.0\n                        b[k] = -2.0 * h * g_val\n                    elif side_name == 'top': # d/dn = d/dy = g -> 3u_{N-1}-4u_{N-2}+u_{N-3} = 2hg\n                        A[k, ((N - 1) * N) + i] = 3.0\n                        A[k, ((N - 2) * N) + i] = -4.0\n                        A[k, ((N - 3) * N) + i] = 1.0\n                        b[k] = 2.0 * h * g_val\n                continue\n\n            # 3. Interior nodes\n            A[k, k] = -4.0\n            A[k, k - 1] = 1.0  # u_{i-1, j}\n            A[k, k + 1] = 1.0  # u_{i+1, j}\n            A[k, k - N] = 1.0  # u_{i, j-1}\n            A[k, k + N] = 1.0  # u_{i, j+1}\n            b[k] = 0.0\n\n    # Solve the linear system\n    A_csc = A.tocsc()\n    u_vec = spsolve(A_csc, b)\n    u_numerical = u_vec.reshape((N, N))\n\n    # Calculate exact solution and error\n    u_exact_grid = np.zeros((N, N))\n    for j in range(N):\n        for i in range(N):\n            u_exact_grid[j, i] = u_star(x_coords[i], y_coords[j])\n            \n    max_error = np.max(np.abs(u_numerical - u_exact_grid))\n    return max_error\n\n\ndef solve():\n    # Test case A (general mixed, smooth polynomial)\n    N_A = 51\n    u_star_A = lambda x, y: x**2 - y**2\n    u_dy_A = lambda x, y: -2*y\n    bc_A = {\n        'left':   [(1.0, 'D', lambda x, y: u_star_A(0, y))],\n        'right':  [(1.0, 'D', lambda x, y: u_star_A(1, y))],\n        'bottom': [(1.0, 'N', lambda x, y: -u_dy_A(x, 0))], # d/dn = -d/dy\n        'top':    [(1.0, 'N', lambda x, y: u_dy_A(x, 1))]  # d/dn = +d/dy\n    }\n    case_A = (N_A, u_star_A, bc_A)\n\n    # Test case B (piecewise mix on a side)\n    N_B = 51\n    u_star_B = lambda x, y: x\n    u_dx_B = lambda x, y: 1.0\n    u_dy_B = lambda x, y: 0.0\n    bc_B = {\n        'left':   [(1.0, 'D', lambda x, y: u_star_B(0, y))],\n        'right':  [(1.0, 'N', lambda x, y: u_dx_B(1, y))], # d/dn = +d/dx\n        'bottom': [(1.0, 'D', lambda x, y: u_star_B(x, 0))],\n        'top':    [(0.4, 'N', lambda x, y: u_dy_B(x, 1)), # d/dn = +d/dy\n                   (1.0, 'D', lambda x, y: u_star_B(x, 1))]\n    }\n    case_B = (N_B, u_star_B, bc_B)\n\n    # Test case C (edge case with small grid and homogeneous Neumann)\n    N_C = 9\n    u_star_C = lambda x, y: y\n    u_dx_C = lambda x, y: 0.0\n    bc_C = {\n        'left':   [(1.0, 'N', lambda x, y: -u_dx_C(0, y))], # d/dn = -d/dx\n        'right':  [(1.0, 'N', lambda x, y: u_dx_C(1, y))], # d/dn = +d/dx\n        'bottom': [(1.0, 'D', lambda x, y: u_star_C(x, 0))],\n        'top':    [(1.0, 'D', lambda x, y: u_star_C(x, 1))]\n    }\n    case_C = (N_C, u_star_C, bc_C)\n    \n    test_cases = [case_A, case_B, case_C]\n    \n    results = []\n    for case in test_cases:\n        error = solve_laplace_2d(*case)\n        results.append(f\"{error:.8f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Discretizing the Laplace equation transforms a single partial differential equation into a large system of coupled linear algebraic equations. Solving this system efficiently is paramount, and this practice delves into this challenge by comparing the performance of three classic iterative methods . By implementing and measuring the convergence rates of the Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR) algorithms, you will gain practical insight into their relative strengths and the dramatic impact that algorithmic choice has on computational cost.",
            "id": "2406769",
            "problem": "Consider the two-dimensional Laplace equation $\\nabla^2 u = 0$ on the square domain $\\Omega = (0,1)\\times(0,1)$ with Dirichlet boundary conditions. Let the boundary data be $u(x,0) = \\sin(\\pi x)$, $u(x,1) = 0$, $u(0,y)=0$, and $u(1,y)=0$, where angles are in radians. Discretize the domain using a uniform grid with $N$ interior points in each spatial direction (so that the grid spacing is $h = \\frac{1}{N+1}$ and there are $(N+2)\\times(N+2)$ total grid points including the boundary). Use the standard second-order centered finite-difference approximation for the Laplacian to obtain a linear system for the interior unknowns. Starting from the discrete Laplace operator definition derived from the continuous equation and central differences, implement three stationary iterative methods to solve the discrete equations: the Jacobi method, the Gauss–Seidel method, and the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega$ satisfying $0 < \\omega < 2$. For each method, use the following fundamental base and definitions:\n\n- The discrete Laplace equation at an interior grid point $(i,j)$ is obtained from the central-difference approximation to $\\nabla^2 u = 0$:\n$$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0,$$\nwhich is algebraically equivalent to the stencil equation\n$$-u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} + 4 u_{i,j} = 0.$$\n- From this, define the discrete residual on the interior as\n$$r_{i,j} = 4u_{i,j} - \\left(u_{i+1,j}+u_{i-1,j}+u_{i,j+1}+u_{i,j-1}\\right),$$\nand its infinity norm as $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$.\n- Use the stopping rule $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol}\\cdot \\|r^{(0)}\\|_{\\infty}$, where $\\text{tol}$ is a given tolerance and $k$ is the iteration index, with the initial guess $u^{(0)}$ equal to zero on all interior points and fixed boundary values on the boundary points.\n\nYour program must:\n- Implement the Jacobi iteration that updates all interior values using only the previous iteration’s values.\n- Implement the Gauss–Seidel iteration in a way that uses the most recently available neighbor values (you may use a red–black ordering to achieve this).\n- Implement the SOR iteration using red–black ordering with relaxation parameter $\\omega$ via\n$$u^{(k+1)}_{i,j} = u^{(k)}_{i,j} + \\omega\\left(\\frac{1}{4}\\left(u^{(*)}_{i+1,j}+u^{(*)}_{i-1,j}+u^{(*)}_{i,j+1}+u^{(*)}_{i,j-1}\\right) - u^{(k)}_{i,j}\\right),$$\nwhere $u^{(*)}$ denotes the most up-to-date values consistent with Gauss–Seidel ordering on each color. Take $\\omega = 1$ to recover Gauss–Seidel.\n\nYour task is to compare the convergence rates of the three methods quantitatively by reporting, for each test case, the number of iterations required by each method to satisfy the stopping rule. Use the same discretization, boundary conditions, and stopping criterion for all methods, and report iteration counts as integers.\n\nTest suite. Run your program on the following parameter sets, where each test case is a triple $(N, \\text{tol}, \\omega)$:\n- Test $1$: $(20, 10^{-5}, 1.5)$.\n- Test $2$: $(20, 10^{-5}, 1.0)$.\n- Test $3$: $(10, 10^{-8}, 1.8)$.\n- Test $4$: $(40, 10^{-4}, 1.9)$.\n\nFor each test case, your program must produce a list $[n_J, n_{GS}, n_{SOR}]$ containing the iteration counts required by the Jacobi, Gauss–Seidel, and SOR methods, respectively, to meet the stopping rule. Aggregate the results of all tests into a single line as a comma-separated list of these lists, with no spaces, enclosed in square brackets. For example, your output must look exactly like\n$[[n_J^{(1)},n_{GS}^{(1)},n_{SOR}^{(1)}],[n_J^{(2)},n_{GS}^{(2)},n_{SOR}^{(2)}],[n_J^{(3)},n_{GS}^{(3)},n_{SOR}^{(3)}],[n_J^{(4)},n_{GS}^{(4)},n_{SOR}^{(4)}]]$,\nprinted as a single line. No physical units are involved. Angles in the sine function must be interpreted in radians.",
            "solution": "The problem as stated is a standard, well-posed exercise in the numerical solution of elliptic partial differential equations. It is scientifically sound, self-contained, and algorithmically specified. There are no contradictions, ambiguities, or factual errors. Therefore, we proceed directly to the solution.\n\nThe problem requires solving the two-dimensional Laplace equation $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$ on a unit square domain $\\Omega = (0,1)\\times(0,1)$. The potential $u(x,y)$ is subject to Dirichlet boundary conditions: $u(x,0) = \\sin(\\pi x)$, and $u=0$ on the other three boundaries.\n\nThe first step is to discretize the continuous problem. The domain is covered by a uniform grid with $(N+2) \\times (N+2)$ points, where $N$ is the number of interior points in each direction. The grid spacing is $h = \\frac{1}{N+1}$. A grid point is denoted by $(x_i, y_j) = (ih, jh)$ for indices $i,j \\in \\{0, 1, \\dots, N+1\\}$. The value of the potential at this point is $u_{i,j} \\approx u(x_i, y_j)$.\n\nThe Laplacian operator $\\nabla^2$ is approximated at each interior grid point $(i,j)$ using the second-order central difference formula:\n$$ \\nabla^2 u \\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} $$\nSetting this approximation to $0$ gives the discrete Laplace equation for the interior points ($1 \\le i,j \\le N$):\n$$ u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0 $$\nThis five-point stencil equation can be rearranged to express $u_{i,j}$ as the average of its four neighbors:\n$$ u_{i,j} = \\frac{1}{4} \\left( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\right) $$\nThis set of $N^2$ linear equations for the $N^2$ interior unknown values forms a large, sparse linear system of the form $A\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u}$ is a vector of the unknowns $u_{i,j}$ and the matrix $A$ represents the discrete Laplacian operator. The right-hand side vector $\\mathbf{b}$ incorporates the fixed boundary values. Such systems are well-suited for solution by iterative methods.\n\nWe are tasked to implement three classical stationary iterative methods: Jacobi, Gauss-Seidel, and Successive Over-Relaxation (SOR). These methods start with an initial guess $u^{(0)}$ and generate a sequence of approximations $u^{(k)}$ that converges to the true solution.\n\nThe Jacobi method is the simplest iterative scheme. For each point $(i,j)$, the new value $u_{i,j}^{(k+1)}$ is computed using only the values from the previous iteration, $u^{(k)}$. The update rule is a direct application of the averaging formula:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} \\right) $$\nThis update can be performed for all interior points simultaneously (or in any order), as the calculation for each point is independent of the others within the same iteration. In a vectorized implementation, a full copy of the grid from iteration $k$ is required to compute the grid for iteration $k+1$.\n\nThe Gauss-Seidel method improves upon Jacobi by using the most recently computed values within the current iteration. The update rule is:\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(*)} + u_{i-1,j}^{(*)} + u_{i,j+1}^{(*)} + u_{i,j-1}^{(*)} \\right) $$\nwhere $u^{(*)}$ denotes the most up-to-date value available. For example, in a lexicographical ordering (row by row, column by column), the calculation for $u_{i,j}^{(k+1)}$ would use $u_{i-1,j}^{(k+1)}$ and $u_{i,j-1}^{(k+1)}$ from the current iteration $k+1$, and $u_{i+1,j}^{(k)}$ and $u_{i,j+1}^{(k)}$ from the previous iteration $k$. This dependency on the update order makes parallelization complex. The red-black ordering scheme circumvents this. The grid points are colored like a checkerboard. All \"red\" points are updated first, using values from their \"black\" neighbors (from the previous iteration). Then, all \"black\" points are updated, using the newly computed values from their \"red\" neighbors. Each of the two stages (red update, black update) can be fully vectorized.\n\nThe Successive Over-Relaxation (SOR) method is an extrapolation of the Gauss-Seidel method, designed to accelerate convergence. It computes the Gauss-Seidel update, and then pushes the solution further in that direction, controlled by a relaxation parameter $\\omega$. The update formula is:\n$$ u_{i,j}^{(k+1)} = u_{i,j}^{(k)} + \\omega \\left( u_{i,j}^{\\text{GS}} - u_{i,j}^{(k)} \\right) = (1-\\omega)u_{i,j}^{(k)} + \\omega u_{i,j}^{\\text{GS}} $$\nwhere $u_{i,j}^{\\text{GS}}$ is the value that would be computed by the Gauss-Seidel step at that point. Like Gauss-Seidel, SOR is implemented using the red-black ordering to efficiently use the most recent values. When $\\omega=1$, the SOR method reduces exactly to the Gauss-Seidel method. For Laplacetype problems, choosing an optimal $\\omega$ in the range $1 < \\omega < 2$ (over-relaxation) typically leads to a significant speedup in convergence.\n\nThe stopping criterion is based on the infinity norm of the discrete residual, defined as $\\|r^{(k)}\\|_{\\infty} = \\max_{i,j} |r_{i,j}^{(k)}|$, where $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})$. The iteration stops when $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol} \\cdot \\|r^{(0)}\\|_{\\infty}$, where $\\text{tol}$ is a given tolerance and $\\|r^{(0)}\\|_{\\infty}$ is the residual norm of the initial guess ($u^{(0)}=0$ on the interior). This relative criterion ensures a fair comparison between different problem setups.\n\nThe implementation consists of three main functions. One function sets up the $(N+2) \\times (N+2)$ grid, initializing the interior to $0$ and setting the boundary conditions. A second function implements the Jacobi iteration. A third function implements the SOR iteration with red-black ordering, which is also used for the Gauss-Seidel method by setting $\\omega=1$. A helper function calculates the residual norm at each step. The main program iterates through the test cases, calls the appropriate solver functions, and records the number of iterations required for convergence.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_initial_state(N):\n    \"\"\"\n    Initializes the grid with boundary conditions and zero interior.\n\n    Args:\n        N (int): Number of interior points in each direction.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The (N+2)x(N+2) grid `u`.\n            - float: The grid spacing `h`.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    u = np.zeros((N + 2, N + 2))\n    \n    # Set boundary condition u(x,0) = sin(pi*x)\n    # The j=0 row corresponds to y=0.\n    x_coords = np.linspace(0, 1, N + 2)\n    u[0, :] = np.sin(np.pi * x_coords)\n    \n    # Other boundaries u(x,1)=0, u(0,y)=0, u(1,y)=0 are already zero.\n    return u, h\n\ndef calculate_residual_norm(u, N):\n    \"\"\"\n    Calculates the infinity norm of the residual on the interior grid.\n\n    Args:\n        u (np.ndarray): The full (N+2)x(N+2) grid.\n        N (int): Number of interior grid points.\n\n    Returns:\n        float: The infinity norm of the residual.\n    \"\"\"\n    interior = u[1:N + 1, 1:N + 1]\n    neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                     u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n    residual = 4 * interior - neighbors_sum\n    return np.max(np.abs(residual))\n\ndef solve_jacobi(N, tol):\n    \"\"\"\n    Solves the Laplace equation using the Jacobi method.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n    \n    threshold = tol * r0_norm\n    \n    k = 0\n    while True:\n        k += 1\n        \n        u_old = u.copy()\n        \n        neighbors_sum = (u_old[1:N + 1, 2:N + 2] + u_old[1:N + 1, 0:N] +\n                         u_old[2:N + 2, 1:N + 1] + u_old[0:N, 1:N + 1])\n        u[1:N + 1, 1:N + 1] = 0.25 * neighbors_sum\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve_sor(N, tol, omega):\n    \"\"\"\n    Solves the Laplace equation using SOR with red-black ordering.\n    Recovers Gauss-Seidel for omega=1.0.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n        omega (float): Relaxation parameter.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n        \n    threshold = tol * r0_norm\n    \n    # Create red-black masks for the interior (N x N) grid.\n    # (j, i) indices for the interior part start from 0.\n    # Grid point (j_grid, i_grid) where j_grid, i_grid in [1,N]\n    # corresponds to mask point (j_grid-1, i_grid-1).\n    # Color depends on (j_grid + i_grid). (j_grid-1) + (i_grid-1) has same parity.\n    I, J = np.meshgrid(np.arange(N), np.arange(N))\n    red_mask = (I + J) % 2 == 0\n    black_mask = ~red_mask\n    \n    k = 0\n    while True:\n        k += 1\n        \n        # Keep a copy of the interior from the start of the iteration\n        # for the (1-omega) term.\n        u_old_interior = u[1:N + 1, 1:N + 1].copy()\n\n        # Update red points. Neighbors are black, use values from start of iteration.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][red_mask] = (1 - omega) * u_old_interior[red_mask] + \\\n                                      omega * gs_update[red_mask]\n\n        # Update black points. Neighbors are red, use newly updated values.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][black_mask] = (1 - omega) * u_old_interior[black_mask] + \\\n                                        omega * gs_update[black_mask]\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (20, 1e-5, 1.5),\n        (20, 1e-5, 1.0),\n        (10, 1e-8, 1.8),\n        (40, 1e-4, 1.9),\n    ]\n\n    results = []\n    for N, tol, omega_sor in test_cases:\n        # Calculate iterations for Jacobi\n        n_J = solve_jacobi(N, tol)\n        \n        # Calculate iterations for Gauss-Seidel (SOR with omega=1.0)\n        n_GS = solve_sor(N, tol, 1.0)\n        \n        # Calculate iterations for SOR with the specified omega\n        n_SOR = solve_sor(N, tol, omega_sor)\n        \n        results.append([n_J, n_GS, n_SOR])\n\n    # Format the output string as specified: [[r1,r2,r3],[...],...]\n    formatted_results = [f'[{\",\".join(map(str, r))}]' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}