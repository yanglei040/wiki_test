{
    "hands_on_practices": [
        {
            "introduction": "To truly master relaxation methods, we begin by understanding the fundamental structure of the simplest algorithm: the Jacobi method. This first practice focuses on the method's synchronous, or \"out-of-place,\" update rule, which is the key to its massive parallelism on modern hardware like Graphics Processing Units (GPUs) . By emulating a parallel solver, you will gain a concrete understanding of how an algorithm's design dictates its performance and derive the theoretical convergence rate from first principles.",
            "id": "2433927",
            "problem": "You are to design and implement a synchronous relaxation solver for a model elliptic boundary-value problem that emulates the execution model of a Graphics Processing Unit (GPU). The Physical problem is the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the unit square. Your implementation must mimic the behavior of a GPU kernel in which each thread updates exactly one grid point in parallel and all updates occur synchronously. Because the execution environment for this task is a Central Processing Unit (CPU), you must emulate the GPU by applying a single, vectorized update over the entire interior grid without using pointwise Python loops over grid points. You must use the Jacobi method and explicitly avoid in-place updates of interior values within an iteration. Define the OpenGL Shading Language (GLSL) and the Compute Unified Device Architecture (CUDA) on first mention: OpenGL Shading Language (GLSL) and Compute Unified Device Architecture (CUDA).\n\nStart from fundamental principles: discretize the partial differential equation using a uniform Cartesian grid and the standard five-point stencil for the Laplacian, and from this discretization derive the synchronous point update rule of the Jacobi method. Let the domain be $\\Omega = [0,1]\\times[0,1]$, let the number of interior points per coordinate direction be $N$, and define the grid spacing as $h = \\frac{1}{N+1}$. Let $u(x,y)$ denote the exact solution and $f(x,y)$ the source term in\n$$\n\\nabla^2 u(x,y) = f(x,y)\\quad \\text{for}\\ (x,y)\\in \\Omega,\\qquad\nu(x,y)=0\\quad \\text{for}\\ (x,y)\\in \\partial\\Omega.\n$$\nLet $u_{i,j}$ denote the discrete unknown at grid point $(x_i,y_j)$ with $x_i = i h$ and $y_j = j h$ for $i,j\\in\\{0,1,\\dots,N+1\\}$, where the boundary values satisfy $u_{i,0}=u_{i,N+1}=u_{0,j}=u_{N+1,j}=0$. Derive, from first principles, the synchronous Jacobi update that uses only neighbor values from the previous iteration and the source term. Explain why synchronous updates are natural for Single Instruction, Multiple Threads (SIMT) execution on a GPU.\n\nImplement a program that:\n- Constructs the discrete right-hand side $f_{i,j}$ for a prescribed source $f(x,y)$.\n- Initializes the interior state with $u_{i,j}^{(0)}=0$ for all interior $(i,j)$.\n- Iterates the Jacobi method until the discrete $\\ell^2$-norm of the residual,\n$$\n\\|r^{(k)}\\|_{2,h} = \\left(h^2 \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\left(f_{i,j} - (A u^{(k)})_{i,j}\\right)^2\\right)^{1/2},\n$$\nfalls below a specified tolerance $\\varepsilon$ or until a maximum number of iterations $k_{\\max}$ is reached. Here $(A u)_{i,j}$ is the five-point discrete Laplacian applied to $u$ and all angles used in trigonometric functions must be in radians.\n- Estimates the asymptotic convergence factor $\\hat{q}$ from the last available ratios $\\|r^{(k)}\\|_{2,h} / \\|r^{(k-1)}\\|_{2,h}$, averaged over up to the last $10$ iterations that have both $\\|r^{(k)}\\|_{2,h}$ and $\\|r^{(k-1)}\\|_{2,h}$ defined. If fewer than $2$ iterations occur, define $\\hat{q}=0$.\n- Computes the theoretical asymptotic convergence factor $q$ of the Jacobi iteration on this problem by analyzing the eigenmodes of the iteration operator on a uniform grid. Do not assume any formula for $q$ without justification; the value of $q$ must be computed from first principles in your solution and implemented in your code using your derived expression.\n\nIn addition to the residual norm, when an exact solution $u(x,y)$ is known, compute the discrete $\\ell^2$-error\n$$\n\\|e^{(k)}\\|_{2,h} = \\left(h^2 \\sum_{i=1}^{N}\\sum_{j=1}^{N} \\left(u_{i,j}^{(k)} - u(x_i,y_j)\\right)^2\\right)^{1/2}.\n$$\n\nTest suite. Your program must run the following three test cases and aggregate their results:\n- Case A (happy path): $N=15$, $f(x,y)=-2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, exact solution $u(x,y)=\\sin(\\pi x)\\sin(\\pi y)$, tolerance $\\varepsilon=10^{-8}$, $k_{\\max}=10000$.\n- Case B (larger grid): $N=31$, same $f$ and exact solution as Case A, tolerance $\\varepsilon=10^{-6}$, $k_{\\max}=100000$.\n- Case C (boundary condition edge case): $N=15$, $f(x,y)=0$ and exact solution $u(x,y)=0$, tolerance $\\varepsilon=10^{-12}$, $k_{\\max}=1000$.\n\nFor each case, your program must output a list of five values in the order:\n$[k_{\\text{it}}, \\|r^{(k)}\\|_{2,h}, \\|e^{(k)}\\|_{2,h}, \\hat{q}, q]$, where $k_{\\text{it}}$ is the number of Jacobi iterations actually performed, $\\|r^{(k)}\\|_{2,h}$ is the final residual norm, $\\|e^{(k)}\\|_{2,h}$ is the final error norm (use $0$ when no analytical solution is available), $\\hat{q}$ is the empirical convergence factor estimate, and $q$ is the theoretical factor derived from your analysis.\n\nFinal output format. Your program should produce a single line of output containing the three case results as a comma-separated list of lists, with numeric entries rounded to six significant figures, and no extra characters or whitespace. For example, the final line must look like\n$[[k_1,r_1,e_1,\\hat{q}_1,q_1],[k_2,r_2,e_2,\\hat{q}_2,q_2],[k_3,r_3,e_3,\\hat{q}_3,q_3]]$\nwith each $k_i$ an integer and each $r_i$, $e_i$, $\\hat{q}_i$, $q_i$ a floating-point number. The angle unit used in all trigonometric evaluations must be radians.",
            "solution": "The problem presented is valid; it is a standard, well-posed exercise in computational physics concerning the numerical solution of elliptic partial differential equations. It is scientifically grounded, internally consistent, and contains all necessary information for a unique numerical solution. We shall proceed with the derivation and implementation.\n\nFirst, we must define the computational architectures mentioned, as requested. The OpenGL Shading Language (GLSL) is a high-level shading language with a syntax based on the C programming language. It is used to create programs, known as shaders, that execute on a Graphics Processing Unit (GPU) as part of the graphics pipeline. The Compute Unified Device Architecture (CUDA) is a proprietary parallel computing platform and application programming interface (API) model created by Nvidia. It allows software developers to use a CUDA-enabled GPU for general-purpose processing, an approach known as GPGPU (General-Purpose computing on Graphics Processing Units).\n\nThe core of the problem is to solve the two-dimensional Poisson equation on the unit square $\\Omega = [0,1]\\times[0,1]$ with homogeneous Dirichlet boundary conditions:\n$$\n\\nabla^2 u(x,y) = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x,y) \\quad \\text{for } (x,y) \\in \\Omega\n$$\n$$\nu(x,y) = 0 \\quad \\text{for } (x,y) \\in \\partial\\Omega\n$$\nWe discretize the domain $\\Omega$ using a uniform Cartesian grid with $N$ interior points in each direction. The grid spacing is $h = \\frac{1}{N+1}$. The grid points are $(x_i, y_j)$, where $x_i = ih$ and $y_j = jh$ for integers $i, j \\in \\{0, 1, \\dots, N+1\\}$. The discrete solution at these points is denoted by $u_{i,j}$. The boundary conditions imply $u_{i,0} = u_{i,N+1} = u_{0,j} = u_{N+1,j} = 0$.\n\nTo discretize the Laplacian operator $\\nabla^2$, we use second-order accurate central finite differences derived from Taylor series expansions:\n$$\n\\frac{\\partial^2 u}{\\partial x^2}\\bigg|_{(x_i,y_j)} = \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\mathcal{O}(h^2)\n$$\n$$\n\\frac{\\partial^2 u}{\\partial y^2}\\bigg|_{(x_i,y_j)} = \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} + \\mathcal{O}(h^2)\n$$\nSubstituting these into the Poisson equation yields the five-point stencil discrete equation for each interior grid point $(i,j)$ where $i,j \\in \\{1, \\dots, N\\}$:\n$$\n\\frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j}}{h^2} = f_{i,j}\n$$\nThis is a large, sparse system of $N^2$ linear equations for the $N^2$ unknown interior values $u_{i,j}$. To solve this system iteratively, we derive the Jacobi method by isolating the term $u_{i,j}$ and introducing an iteration index $k$:\n$$\n4u_{i,j} = u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - h^2 f_{i,j}\n$$\nThe Jacobi update rule computes the new value $u_{i,j}^{(k+1)}$ using only values from the previous iteration, $u^{(k)}$:\n$$\nu_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} - h^2 f_{i,j} \\right)\n$$\nThis update is performed for all interior points $i,j \\in \\{1, \\dots, N\\}$.\n\nThe structure of this update rule is highly amenable to parallel computation, particularly on SIMT (Single Instruction, Multiple Threads) architectures like GPUs. The key observation is that the calculation for each point $u_{i,j}^{(k+1)}$ is completely independent of all other new values $u_{l,m}^{(k+1)}$ for $(l,m) \\neq (i,j)$. All values on the right-hand side belong to the previous state, $u^{(k)}$. A GPU can assign a thread to each grid point $(i,j)$, and all threads can execute the same instruction—the Jacobi update formula—simultaneously on their assigned data. This requires two memory buffers: one to read the old state $u^{(k)}$ and another to write the new state $u^{(k+1)}$. This process, known as a synchronous update, avoids data races and perfectly matches the hardware paradigm. Emulating this on a CPU involves using two arrays and performing a vectorized update, as specified.\n\nNext, we derive the theoretical asymptotic convergence factor, $q$. The Jacobi iteration can be written in matrix form as $\\mathbf{u}^{(k+1)} = T_J \\mathbf{u}^{(k)} + \\mathbf{c}$, where $T_J$ is the Jacobi iteration matrix. The convergence rate is determined by the spectral radius of $T_J$, defined as $q = \\rho(T_J) = \\max_m |\\lambda_m|$, where $\\lambda_m$ are the eigenvalues of $T_J$. The eigenvectors of the discrete Laplacian operator on this domain are known to be the discrete sine functions:\n$$\nv_{i,j}^{(p,q)} = \\sin\\left(\\frac{p\\pi i}{N+1}\\right) \\sin\\left(\\frac{q\\pi j}{N+1}\\right) \\quad \\text{for } p,q \\in \\{1, \\dots, N\\}\n$$\nThese are also the eigenvectors of the Jacobi iteration matrix $T_J$. Applying the Jacobi operator to such an eigenvector gives:\n$$\n(T_J v^{(p,q)})_{i,j} = \\frac{1}{4} \\left( v_{i+1,j}^{(p,q)} + v_{i-1,j}^{(p,q)} + v_{i,j+1}^{(p,q)} + v_{i,j-1}^{(p,q)} \\right)\n$$\nUsing the trigonometric identity $\\sin(A+B) + \\sin(A-B) = 2\\sin(A)\\cos(B)$, we simplify the terms:\n$$\nv_{i+1,j}^{(p,q)} + v_{i-1,j}^{(p,q)} = 2\\cos\\left(\\frac{p\\pi}{N+1}\\right) v_{i,j}^{(p,q)}\n$$\n$$\nv_{i,j+1}^{(p,q)} + v_{i,j-1}^{(p,q)} = 2\\cos\\left(\\frac{q\\pi}{N+1}\\right) v_{i,j}^{(p,q)}\n$$\nSubstituting these back, we find the eigenvalue $\\lambda_{p,q}$ corresponding to the eigenvector $v^{(p,q)}$:\n$$\n(T_J v^{(p,q)})_{i,j} = \\frac{1}{2} \\left[ \\cos\\left(\\frac{p\\pi}{N+1}\\right) + \\cos\\left(\\frac{q\\pi}{N+1}\\right) \\right] v_{i,j}^{(p,q)}\n$$\nThe spectral radius is the maximum of the absolute value of these eigenvalues over all $p, q \\in \\{1, \\dots, N\\}$. The cosine function is positive and monotonically decreasing on the interval $[0, \\pi/2]$. Thus, the maximum eigenvalue occurs for the smallest arguments, i.e., $p=1$ and $q=1$.\n$$\nq = \\rho(T_J) = \\lambda_{1,1} = \\frac{1}{2} \\left[ \\cos\\left(\\frac{\\pi}{N+1}\\right) + \\cos\\left(\\frac{\\pi}{N+1}\\right) \\right] = \\cos\\left(\\frac{\\pi}{N+1}\\right)\n$$\nThis is the required theoretical asymptotic convergence factor. It approaches $1$ as $N$ increases, indicating a significant slowdown in convergence for finer grids.\n\nThe implementation will follow these derived principles. A main loop will iterate the Jacobi update, checking for convergence at the beginning of each step. The residual norm $\\|r^{(k)}\\|_{2,h}$ and error norm $\\|e^{(k)}\\|_{2,h}$ will be computed using vectorized `numpy` operations. The empirical convergence factor $\\hat{q}$ will be estimated from the ratios of successive residual norms, and all numerical outputs will be formatted as specified.",
            "answer": "```python\nimport numpy as np\nimport math\n\ndef format_num(n):\n    \"\"\"Formats a number to 6 significant figures, or as an integer.\"\"\"\n    if isinstance(n, int):\n        return str(n)\n    if n == 0.0:\n        return \"0.0\"\n    return f\"{n:.6g}\"\n\ndef run_case(N, f_func, u_exact_func, epsilon, k_max):\n    \"\"\"\n    Solves the 2D Poisson BVP for a single test case using the Jacobi method.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    \n    # Create grid coordinates (including boundaries)\n    i_coords = np.arange(0, N + 2)\n    x = i_coords * h\n    X, Y = np.meshgrid(x, x, indexing='ij')\n\n    # Initialize solution array u (with zero boundaries) and source term f\n    u = np.zeros((N + 2, N + 2))\n    f_grid = f_func(X, Y)\n    \n    u_exact_grid = None\n    if u_exact_func is not None:\n        u_exact_grid = u_exact_func(X, Y)\n\n    residual_history = []\n    k_it = 0\n\n    for k in range(k_max + 1):\n        # Calculate the residual of the current state u^{(k)}\n        # A*u = (1/h^2) * (u_{i+1,j} + ... - 4u_{i,j})\n        # r = f - A*u\n        laplacian_u_interior = (u[2:, 1:-1] + u[:-2, 1:-1] + u[1:-1, 2:] + u[1:-1, :-2] - 4 * u[1:-1, 1:-1]) / (h**2)\n        f_interior = f_grid[1:-1, 1:-1]\n        \n        residual_matrix = f_interior - laplacian_u_interior\n        residual_norm = np.sqrt(h**2 * np.sum(residual_matrix**2))\n        residual_history.append(residual_norm)\n        \n        k_it = k\n        # Check for convergence before performing the update\n        if residual_norm  epsilon:\n            break\n        \n        # Check if max iterations reached\n        if k == k_max:\n            break\n\n        # Perform one synchronous Jacobi iteration (emulating GPU)\n        # Use a new array for the updated values, reading only from the old 'u'\n        u_new = np.zeros_like(u)\n        u_new[1:-1, 1:-1] = 0.25 * (u[2:, 1:-1] + u[:-2, 1:-1] +u[1:-1, 2:] + u[1:-1, :-2] - h**2 * f_grid[1:-1, 1:-1])\n        u = u_new\n\n    # --- Post-processing after loop termination ---\n    final_residual_norm = residual_history[-1]\n    \n    # Calculate final discrete L2 error norm if exact solution is available\n    final_error_norm = 0.0\n    if u_exact_grid is not None:\n        error_matrix = u[1:-1, 1:-1] - u_exact_grid[1:-1, 1:-1]\n        final_error_norm = np.sqrt(h**2 * np.sum(error_matrix**2))\n\n    # Estimate empirical convergence factor q_hat\n    q_hat = 0.0\n    # Per problem statement: q_hat=0 if fewer than 2 iterations occur.\n    if k_it >= 2:\n        ratios = []\n        # residual_history has k_it+1 elements (for k=0, 1, ..., k_it)\n        for i in range(1, len(residual_history)):\n            # Avoid division by zero\n            if residual_history[i-1] > 1e-16:\n                ratios.append(residual_history[i] / residual_history[i-1])\n        \n        num_ratios_to_avg = min(len(ratios), 10)\n        if num_ratios_to_avg > 0:\n            q_hat = np.mean(ratios[-num_ratios_to_avg:])\n            \n    # Calculate theoretical convergence factor q\n    q_theory = np.cos(np.pi / (N + 1))\n    \n    return [\n        format_num(k_it),\n        format_num(final_residual_norm),\n        format_num(final_error_norm),\n        format_num(q_hat),\n        format_num(q_theory)\n    ]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the Jacobi solver.\n    \"\"\"\n    test_cases = [\n        # Case A: Happy path\n        {\n            \"N\": 15,\n            \"f_func\": lambda x, y: -2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"u_exact_func\": lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"epsilon\": 1e-8,\n            \"k_max\": 10000\n        },\n        # Case B: Larger grid\n        {\n            \"N\": 31,\n            \"f_func\": lambda x, y: -2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"u_exact_func\": lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y),\n            \"epsilon\": 1e-6,\n            \"k_max\": 100000\n        },\n        # Case C: Trivial solution, boundary edge case\n        {\n            \"N\": 15,\n            \"f_func\": lambda x, y: np.zeros_like(x),\n            \"u_exact_func\": lambda x, y: np.zeros_like(x),\n            \"epsilon\": 1e-12,\n            \"k_max\": 1000\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(\n            case[\"N\"],\n            case[\"f_func\"],\n            case[\"u_exact_func\"],\n            case[\"epsilon\"],\n            case[\"k_max\"]\n        )\n        # Format each list of results into the required string format\n        results.append(f\"[{','.join(result)}]\")\n\n    # Print the final aggregated output string\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While iterative methods are powerful, they are not universally guaranteed to succeed. This practice moves beyond simple implementation to an exploration of convergence itself, comparing the Jacobi method to the closely related Gauss-Seidel method. You will investigate a physical scenario involving anisotropic diffusion where the simple Jacobi method fails, but the sequential updates of Gauss-Seidel ensure a solution . This exercise provides critical insight into the numerical stability of relaxation schemes and connects the abstract mathematical condition on the spectral radius to tangible properties of the physical system.",
            "id": "2433986",
            "problem": "You are asked to construct and analyze a linear system arising from a physically meaningful elliptic boundary value problem, and to identify a configuration where the Jacobi iterative method diverges while the Gauss-Seidel method converges. The analysis and the algorithm must be grounded in first principles: the definition of ellipticity, the construction of a consistent finite difference discretization, and the standard linear iterative splittings.\n\nConsider the steady anisotropic diffusion equation, written in tensor form on a bounded planar domain,\n$$\n-\\nabla \\cdot \\big( \\boldsymbol{K} \\nabla u \\big) = 0 \\quad \\text{in } \\Omega,\n$$\nsubject to homogeneous Dirichlet boundary conditions\n$$\nu = 0 \\quad \\text{on } \\partial \\Omega.\n$$\nAssume an anisotropic, symmetric, positive-definite diffusion tensor\n$$\n\\boldsymbol{K} = \\boldsymbol{R}(\\theta)\\begin{bmatrix}\\kappa  0 \\\\ 0  1\\end{bmatrix}\\boldsymbol{R}(\\theta)^{\\top},\n$$\nwhere $ \\kappa \\ge 1 $ is the anisotropy ratio and $ \\boldsymbol{R}(\\theta) $ is the $ 2\\times 2 $ rotation matrix through an angle $ \\theta $ (in degrees, to be interpreted in radians when constructing $ \\boldsymbol{R}(\\theta) $). For a uniform grid with spacing $ h $ on the unit square $ \\Omega = (0,1)\\times(0,1) $, use the standard second-order central difference approximations for $ u_{xx} $, $ u_{yy} $, and $ u_{xy} $ to derive a $ 9 $-point stencil for the operator\n$$\n- \\left( a\\,u_{xx} + 2b\\,u_{xy} + c\\,u_{yy} \\right) = 0,\n$$\nwhere $ a = K_{xx} $, $ b = K_{xy} $, and $ c = K_{yy} $. The central differences are\n$$\nu_{xx}\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2},\\quad\nu_{yy}\\approx \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2},\\quad\nu_{xy}\\approx \\frac{u_{i+1,j+1}-u_{i+1,j-1}-u_{i-1,j+1}+u_{i-1,j-1}}{4h^2}.\n$$\nAssemble the corresponding linear system $ \\boldsymbol{A}\\boldsymbol{u}=\\boldsymbol{b} $ for the interior unknowns with $ \\boldsymbol{b}=\\boldsymbol{0} $ due to homogeneous Dirichlet boundary conditions, using only the fundamental definition above and moving all boundary contributions consistently to the right-hand side. The matrix $ \\boldsymbol{A} $ is real and, for physically admissible $ \\boldsymbol{K} $, symmetric.\n\nStarting from the definitions of the Jacobi and Gauss-Seidel schemes as linear stationary methods derived from matrix splittings,\n$$\n\\boldsymbol{A}=\\boldsymbol{D}+\\boldsymbol{L}+\\boldsymbol{U}, \\quad\n\\text{Jacobi: } \\boldsymbol{T}_{J}=\\boldsymbol{I}-\\boldsymbol{D}^{-1}\\boldsymbol{A}, \\quad\n\\text{Gauss\\text{-}Seidel: } \\boldsymbol{T}_{GS}=-(\\boldsymbol{D}+\\boldsymbol{L})^{-1}\\boldsymbol{U},\n$$\nrecall the well-tested fact that a necessary and sufficient condition for convergence of a linear stationary iteration is that the spectral radius $ \\rho(\\cdot) $ of the iteration matrix is less than $ 1 $:\n$$\n\\rho(\\boldsymbol{T})  1 \\Longleftrightarrow \\text{convergence}.\n$$\nUse this principle to classify convergence.\n\nTask requirements:\n- Construct $ \\boldsymbol{A} $ for the unit square with homogeneous Dirichlet boundary conditions and the $ 9 $-point stencil implied by the definitions above. Use a uniform mesh with $ n $ interior points per coordinate direction and grid spacing $ h = 1/(n+1) $.\n- For each test case below, compute the spectral radii $ \\rho(\\boldsymbol{T}_J) $ and $ \\rho(\\boldsymbol{T}_{GS}) $, and decide whether the Jacobi method diverges while the Gauss-Seidel method converges. Your program must return, for each case, the boolean value $ \\texttt{True} $ if and only if Jacobi diverges and Gauss-Seidel converges; otherwise return $ \\texttt{False} $.\n- To ensure physical and numerical plausibility, use $ \\kappa \\ge 1 $ and treat the angle $ \\theta $ in degrees. There are no physical units required for the output. Angles must be interpreted in radians internally, but all angle inputs are specified in degrees.\n\nTest suite (covering a nominal isotropic case, a moderately anisotropic aligned case, and a strongly rotated anisotropy expected to challenge Jacobi):\n- Case $ 1 $: $ n=8 $, $ \\kappa=1.0 $, $ \\theta=0^\\circ $.\n- Case $ 2 $: $ n=8 $, $ \\kappa=10.0 $, $ \\theta=30^\\circ $.\n- Case $ 3 $: $ n=8 $, $ \\kappa=1000.0 $, $ \\theta=45^\\circ $.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of Python boolean literals enclosed in square brackets (e.g., $ [\\texttt{False},\\texttt{False},\\texttt{True}] $), corresponding in order to the three test cases above.",
            "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Governing Equation**: Steady anisotropic diffusion equation, $-\\nabla \\cdot \\big( \\boldsymbol{K} \\nabla u \\big) = 0$, on the unit square domain $\\Omega = (0,1)\\times(0,1)$.\n- **Boundary Conditions**: Homogeneous Dirichlet boundary conditions, $u = 0$ on $\\partial \\Omega$.\n- **Diffusion Tensor**: $\\boldsymbol{K} = \\boldsymbol{R}(\\theta)\\begin{bmatrix}\\kappa  0 \\\\ 0  1\\end{bmatrix}\\boldsymbol{R}(\\theta)^{\\top}$, where $\\kappa \\ge 1$ is the anisotropy ratio and $\\boldsymbol{R}(\\theta)$ is the $2 \\times 2$ rotation matrix. The angle $\\theta$ is given in degrees.\n- **Operator Form**: The equation is equivalent to $- \\left( a\\,u_{xx} + 2b\\,u_{xy} + c\\,u_{yy} \\right) = 0$, where $a = K_{xx}$, $b = K_{xy}$, and $c = K_{yy}$.\n- **Discretization**: A uniform grid with $n$ interior points per direction and spacing $h = 1/(n+1)$. Standard second-order central differences are specified:\n  - $u_{xx}\\approx \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2}$\n  - $u_{yy}\\approx \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2}$\n  - $u_{xy}\\approx \\frac{u_{i+1,j+1}-u_{i+1,j-1}-u_{i-1,j+1}+u_{i-1,j-1}}{4h^2}$\n- **Linear System**: Assemble $\\boldsymbol{A}\\boldsymbol{u}=\\boldsymbol{b}$, where $\\boldsymbol{b}=\\boldsymbol{0}$.\n- **Iterative Methods**:\n  - Matrix Splitting: $\\boldsymbol{A}=\\boldsymbol{D}+\\boldsymbol{L}+\\boldsymbol{U}$.\n  - Jacobi Iteration Matrix: $\\boldsymbol{T}_{J}=\\boldsymbol{I}-\\boldsymbol{D}^{-1}\\boldsymbol{A}$.\n  - Gauss-Seidel Iteration Matrix: $\\boldsymbol{T}_{GS}=-(\\boldsymbol{D}+\\boldsymbol{L})^{-1}\\boldsymbol{U}$.\n- **Convergence Condition**: A method converges if and only if the spectral radius of its iteration matrix is less than $1$, i.e., $\\rho(\\boldsymbol{T})  1$.\n- **Task**: For each test case, determine if the Jacobi method diverges ($\\rho(\\boldsymbol{T}_J) \\ge 1$) while the Gauss-Seidel method converges ($\\rho(\\boldsymbol{T}_{GS})  1$).\n- **Test Cases**:\n  1. $n=8$, $\\kappa=1.0$, $\\theta=0^\\circ$.\n  2. $n=8$, $\\kappa=10.0$, $\\theta=30^\\circ$.\n  3. $n=8$, $\\kappa=1000.0$, $\\theta=45^\\circ$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It presents a standard problem in computational physics involving the numerical solution of an elliptic partial differential equation. The diffusion equation, finite difference method, and iterative solvers (Jacobi, Gauss-Seidel) are fundamental concepts. The convergence criterion based on the spectral radius is a cornerstone theorem of numerical linear algebra. The diffusion tensor $\\boldsymbol{K}$ is defined as symmetric and positive-definite, which guarantees the ellipticity of the PDE. The problem is self-contained and provides all necessary information to construct a solution. There are no scientific flaws, contradictions, or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Solution\nThe task is to determine, for specific physical parameters, whether the Jacobi iterative method diverges while the Gauss-Seidel method converges for the linear system arising from a finite difference discretization of the anisotropic diffusion equation.\n\n**1. Derivation of PDE Coefficients**\nFirst, we determine the components $a$, $b$, and $c$ of the diffusion operator. The diffusion tensor is $\\boldsymbol{K} = \\boldsymbol{R}(\\theta)\\boldsymbol{D}_\\kappa\\boldsymbol{R}(\\theta)^{\\top}$, where $\\boldsymbol{D}_\\kappa = \\begin{bmatrix}\\kappa  0 \\\\ 0  1\\end{bmatrix}$ and $\\boldsymbol{R}(\\theta) = \\begin{bmatrix}\\cos\\phi  -\\sin\\phi \\\\ \\sin\\phi  \\cos\\phi\\end{bmatrix}$ with $\\phi = \\theta \\cdot \\pi/180$.\nPerforming the matrix multiplication gives:\n$$\n\\boldsymbol{K} =\n\\begin{bmatrix}\n\\kappa \\cos^2\\phi + \\sin^2\\phi  (\\kappa-1)\\sin\\phi\\cos\\phi \\\\\n(\\kappa-1)\\sin\\phi\\cos\\phi  \\kappa \\sin^2\\phi + \\cos^2\\phi\n\\end{bmatrix}\n$$\nThus, the coefficients are:\n- $a = K_{xx} = \\kappa \\cos^2\\phi + \\sin^2\\phi$\n- $b = K_{xy} = (\\kappa-1)\\sin\\phi\\cos\\phi = \\frac{\\kappa-1}{2}\\sin(2\\phi)$\n- $c = K_{yy} = \\kappa \\sin^2\\phi + \\cos^2\\phi$\n\nThe PDE is elliptic because $\\det(\\boldsymbol{K}) = \\kappa \\ge 1  0$, ensuring $ac-b^2  0$.\n\n**2. Finite Difference Stencil and System Matrix $\\boldsymbol{A}$**\nWe substitute the given central difference approximations into the operator $- \\left( a\\,u_{xx} + 2b\\,u_{xy} + c\\,u_{yy} \\right) = 0$. At a grid point $(i,j)$, this yields:\n$$\n-a \\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2} - 2b \\frac{u_{i+1,j+1}-u_{i+1,j-1}-u_{i-1,j+1}+u_{i-1,j-1}}{4h^2} - c \\frac{u_{i,j+1}-2u_{i,j}+u_{i,j-1}}{h^2} = 0\n$$\nMultiplying by $h^2$ (a constant scaling that does not affect the iteration matrices' spectral radii) and collecting terms for each grid point gives the equation for row $k$ of the matrix $\\boldsymbol{A}$, corresponding to grid point $(i,j)$:\n$$\n(2a+2c)u_{i,j} - a(u_{i-1,j}+u_{i+1,j}) - c(u_{i,j-1}+u_{i,j+1}) - \\frac{b}{2}(u_{i+1,j+1}-u_{i+1,j-1}-u_{i-1,j+1}+u_{i-1,j-1}) = 0\n$$\nThis defines a $9$-point stencil. The unknowns $u_{i,j}$ for $i,j \\in \\{1,\\dots,n\\}$ are arranged into a vector $\\boldsymbol{u}$ of size $N=n^2$ using a row-major ordering, where the 0-based index $k$ for grid point $(i,j)$ is $k = jn+i$. The matrix $\\boldsymbol{A}$ is constructed based on this stencil. For homogeneous Dirichlet boundary conditions, any stencil point falling on the boundary corresponds to a value of $u=0$ and thus does not contribute to the matrix entries for the unknown vector. The resulting matrix $\\boldsymbol{A}$ is symmetric because the operator is self-adjoint and the discretization scheme is consistent.\n\n**3. Convergence Analysis**\nThe convergence of the Jacobi and Gauss-Seidel methods depends on the spectral radii of their respective iteration matrices, $\\boldsymbol{T}_J$ and $\\boldsymbol{T}_{GS}$.\nSince the PDE is elliptic and the discretization is standard, the resulting matrix $\\boldsymbol{A}$ is Symmetric Positive-Definite (SPD). A key theorem in numerical linear algebra states that for an SPD matrix $\\boldsymbol{A}$, the Gauss-Seidel method is guaranteed to converge. Therefore, $\\rho(\\boldsymbol{T}_{GS})  1$ for all valid test cases.\n\nThe problem thus reduces to determining when the Jacobi method diverges, i.e., when $\\rho(\\boldsymbol{T}_J) \\ge 1$. For an SPD matrix $\\boldsymbol{A}$, Jacobi converges if and only if the matrix $2\\boldsymbol{D}-\\boldsymbol{A}$ is also positive definite. The presence of a large mixed-derivative term ($b \\neq 0$) can lead to $2\\boldsymbol{D}-\\boldsymbol{A}$ not being positive definite, causing Jacobi to diverge. The coefficient $b$ is maximized for a given $\\kappa$ when $\\sin(2\\phi)$ is maximal, which occurs at $\\theta=45^\\circ$. Thus, strong anisotropy ($\\kappa \\gg 1$) combined with a rotation of $\\theta=45^\\circ$ is a classic scenario where Jacobi fails while Gauss-Seidel converges.\n\n**4. Algorithm**\nFor each test case ($n, \\kappa, \\theta$):\n1.  Calculate the coefficients $a,b,c$ using the value of $\\theta$ in radians.\n2.  Construct the $n^2 \\times n^2$ matrix $\\boldsymbol{A}$ based on the derived $9$-point stencil, applying homogeneous Dirichlet boundary conditions.\n3.  Decompose $\\boldsymbol{A}$ into its diagonal ($\\boldsymbol{D}$), strictly lower ($\\boldsymbol{L}$), and strictly upper ($\\boldsymbol{U}$) triangular parts.\n4.  Compute the Jacobi iteration matrix $\\boldsymbol{T}_J = -\\boldsymbol{D}^{-1}(\\boldsymbol{L}+\\boldsymbol{U})$ and the Gauss-Seidel iteration matrix $\\boldsymbol{T}_{GS} = -(\\boldsymbol{D}+\\boldsymbol{L})^{-1}\\boldsymbol{U}$.\n5.  Calculate the spectral radii $\\rho(\\boldsymbol{T}_J)$ and $\\rho(\\boldsymbol{T}_{GS})$ by finding the maximum absolute eigenvalue of each matrix.\n6.  Evaluate the boolean condition $\\rho(\\boldsymbol{T}_J) \\ge 1 \\text{ and } \\rho(\\boldsymbol{T}_{GS})  1$.\n\nThis procedure is implemented for the given test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef analyze_convergence(n: int, kappa: float, theta_deg: float) -> bool:\n    \"\"\"\n    Analyzes the convergence of Jacobi and Gauss-Seidel methods for the given parameters.\n\n    Args:\n        n: Number of interior grid points per dimension.\n        kappa: Anisotropy ratio.\n        theta_deg: Rotation angle in degrees.\n\n    Returns:\n        True if Jacobi diverges and Gauss-Seidel converges, False otherwise.\n    \"\"\"\n    # 1. Calculate PDE coefficients\n    theta_rad = np.deg2rad(theta_deg)\n    cos_t, sin_t = np.cos(theta_rad), np.sin(theta_rad)\n    \n    a = kappa * cos_t**2 + sin_t**2\n    b = (kappa - 1) * sin_t * cos_t\n    c_pde = kappa * sin_t**2 + cos_t**2\n\n    # 2. Build the system matrix A\n    N = n * n\n    A = np.zeros((N, N), dtype=float)\n\n    for j in range(n):  # 0-based row index of the grid point\n        for i in range(n):  # 0-based column index of the grid point\n            k = j * n + i  # 1D index using row-major ordering\n\n            # Stencil coefficients for the discrete operator.\n            # The common factor 1/h^2 is omitted as it scales the entire matrix\n            # and does not affect the spectral radii of the iteration matrices.\n            \n            # Diagonal term\n            A[k, k] = 2.0 * a + 2.0 * c_pde\n\n            # Neighboring terms from u_xx and u_yy\n            if i > 0:       A[k, k - 1] = -a      # (i-1, j)\n            if i  n - 1:   A[k, k + 1] = -a      # (i+1, j)\n            if j > 0:       A[k, k - n] = -c_pde  # (i, j-1)\n            if j  n - 1:   A[k, k + n] = -c_pde  # (i, j+1)\n\n            # Mixed derivative terms from u_xy\n            if i > 0 and j > 0:         A[k, k - n - 1] = b / 2.0  # (i-1, j-1) -> sign fixed\n            if i  n - 1 and j > 0:     A[k, k - n + 1] = -b / 2.0   # (i+1, j-1) -> sign fixed\n            if i > 0 and j  n - 1:     A[k, k + n - 1] = -b / 2.0   # (i-1, j+1) -> sign fixed\n            if i  n - 1 and j  n - 1: A[k, k + n + 1] = b / 2.0  # (i+1, j+1) -> sign fixed\n    \n    # Correction based on derivation: -2b * (u_i+1,j+1 - u_i+1,j-1 - u_i-1,j+1 + u_i-1,j-1) / 4h^2\n    # So terms are: -b/2 u_i+1,j+1, +b/2 u_i+1,j-1, +b/2 u_i-1,j+1, -b/2 u_i-1,j-1\n    # My initial code had the signs reversed. Let's fix this by re-building A.\n    A.fill(0)\n    for j in range(n):\n        for i in range(n):\n            k = j * n + i\n            A[k, k] = 2.0 * a + 2.0 * c_pde\n            if i > 0:       A[k, k - 1] = -a\n            if i  n - 1:   A[k, k + 1] = -a\n            if j > 0:       A[k, k - n] = -c_pde\n            if j  n - 1:   A[k, k + n] = -c_pde\n            \n            # Correct mixed derivative terms\n            if i > 0 and j > 0:         A[k, k - n - 1] += b / 2.0\n            if i  n - 1 and j > 0:     A[k, k - n + 1] -= b / 2.0\n            if i > 0 and j  n - 1:     A[k, k + n - 1] -= b / 2.0\n            if i  n - 1 and j  n - 1: A[k, k + n + 1] += b / 2.0\n\n    # 3. Decompose A into D (diagonal), L (lower), U (upper)\n    D = np.diag(np.diag(A))\n    L = np.tril(A, k=-1)\n    U = np.triu(A, k=1)\n\n    # 4. Compute iteration matrices\n    # Jacobi: T_J = -D^{-1}(L+U)\n    D_inv = np.linalg.inv(D)\n    T_J = -D_inv @ (L + U)\n\n    # Gauss-Seidel: T_GS = -(D+L)^{-1}U\n    DL_inv = np.linalg.inv(D + L)\n    T_GS = -DL_inv @ U\n\n    # 5. Compute spectral radii\n    rho_J = np.max(np.abs(np.linalg.eigvals(T_J)))\n    rho_GS = np.max(np.abs(np.linalg.eigvals(T_GS)))\n    \n    # 6. Check the condition for Jacobi divergence and Gauss-Seidel convergence\n    jacobi_diverges = rho_J >= 1.0\n    gauss_seidel_converges = rho_GS  1.0\n\n    return jacobi_diverges and gauss_seidel_converges\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, kappa, theta_deg)\n        (8, 1.0, 0.0),       # Case 1\n        (8, 10.0, 30.0),     # Case 2\n        (8, 1000.0, 45.0),   # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        n, kappa, theta_deg = case\n        result = analyze_convergence(n, kappa, theta_deg)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world physics problems are rarely confined to simple square boxes. This final practice challenges you to adapt your skills to a new geometry by solving the Laplace equation on an annular domain using polar coordinates. You will derive the finite-difference approximation for the Laplacian operator in this new coordinate system, a crucial skill for tackling problems with cylindrical or spherical symmetry . Implementing a solver with the more advanced Successive Over-Relaxation (SOR) method here will solidify your ability to apply relaxation techniques to a much broader and more realistic class of physical systems.",
            "id": "2433975",
            "problem": "Implement a complete, runnable program that computes the electrostatic potential governed by the Laplace equation inside an annular domain using a relaxation method on a polar grid. The governing partial differential equation is the Laplace equation in polar coordinates, which is a classical elliptic equation, expressed by the well-tested formula $\\,\\nabla^{2}\\phi = \\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r \\frac{\\partial \\phi}{\\partial r}\\right) + \\frac{1}{r^{2}}\\frac{\\partial^{2}\\phi}{\\partial \\theta^{2}}\\,$. The domain is the annulus $\\{(r,\\theta)\\,|\\, r \\in [r_{\\mathrm{in}}, r_{\\mathrm{out}}], \\theta \\in [0, 2\\pi)\\}$ with periodicity in the angular direction. You must use a relaxation method, specifically Successive Over-Relaxation (SOR), to solve for the discrete potential on a uniform polar grid. Angles must be in radians. There are no physical units to report in this problem.\n\nStarting from the divergence form of the Laplacian and the definition of central finite differences on a uniform grid, derive a consistent second-order finite-difference discretization of the operator $\\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r \\frac{\\partial \\phi}{\\partial r}\\right)$ and of $\\frac{1}{r^{2}}\\frac{\\partial^{2}\\phi}{\\partial \\theta^{2}}$ at an interior grid point. Then, using this discrete equation, construct a pointwise linear update for interior nodes that enforces the discrete Laplace equation. Embed this pointwise update in a Successive Over-Relaxation (SOR) sweep with relaxation factor $\\omega$ and iterate until the discrete residual is below a specified tolerance. The residual at an interior node must be defined by substituting the current iterate into your derived discrete Laplace operator. Use periodic boundary conditions in the angular direction and Dirichlet boundary conditions at $r=r_{\\mathrm{in}}$ and $r=r_{\\mathrm{out}}$.\n\nYour program must:\n- Build a uniform polar grid with $N_{r}$ nodes in the radial direction and $N_{\\theta}$ nodes in the angular direction. Use $r_{i} = r_{\\mathrm{in}} + i\\,\\Delta r$ for $i \\in \\{0,1,\\dots,N_{r}-1\\}$ with $\\Delta r = \\frac{r_{\\mathrm{out}}-r_{\\mathrm{in}}}{N_{r}-1}$ and $\\theta_{j} = j\\,\\Delta \\theta$ for $j \\in \\{0,1,\\dots,N_{\\theta}-1\\}$ with $\\Delta \\theta = \\frac{2\\pi}{N_{\\theta}}$. Angles must be in radians.\n- Impose Dirichlet boundary values at $r=r_{\\mathrm{in}}$ and $r=r_{\\mathrm{out}}$ as specified per test case below, and enforce periodicity in $\\theta$ by indexing with wrap-around for $\\theta_{j\\pm 1}$.\n- Use Successive Over-Relaxation (SOR) with a fixed relaxation factor $\\omega$ to iteratively update interior grid values until the $\\ell_{\\infty}$-norm of the discrete residual across all interior points is less than a tolerance $\\varepsilon$. Use a maximum iteration cap to avoid non-termination if convergence fails.\n- After convergence, compute the maximum absolute error on the entire grid against an analytic solution provided for each test case.\n\nDerivation constraints:\n- Begin from the divergence form identity for the Laplacian and central finite-difference definitions. Do not assume any pre-derived discrete update for polar coordinates; derive it explicitly from first principles.\n- The discretization must be second-order accurate in both $r$ and $\\theta$ on a uniform grid.\n- The SOR update must be constructed from the derived discrete equation by isolating the unknown at an interior node and applying the over-relaxation with factor $\\omega$.\n\nConvergence criterion and parameters:\n- Use the residual-based stopping criterion $\\max_{i,j} | \\mathcal{L}_{h}[\\phi]_{i,j} |  \\varepsilon$, where $\\mathcal{L}_{h}$ denotes your discrete Laplacian applied to the current iterate, and the maximum is taken over all interior grid points only.\n- Use relaxation factor $\\omega = 1.85$.\n- Use tolerance $\\varepsilon = 10^{-8}$.\n- Use a maximum of $N_{\\mathrm{it,max}} = 50000$ iterations.\n\nTest suite:\n- Case $1$ (radial Dirichlet data with analytic logarithmic solution):\n  - $r_{\\mathrm{in}} = 1$, $r_{\\mathrm{out}} = 2$, $N_{r} = 32$, $N_{\\theta} = 64$.\n  - Boundary values: $\\phi(r_{\\mathrm{in}},\\theta) = 0$ and $\\phi(r_{\\mathrm{out}},\\theta) = 1$ for all $\\theta$.\n  - Analytic solution: $\\phi_{\\mathrm{exact}}(r,\\theta) = \\dfrac{\\ln r - \\ln r_{\\mathrm{in}}}{\\ln r_{\\mathrm{out}} - \\ln r_{\\mathrm{in}}}$, independent of $\\theta$.\n- Case $2$ (single Fourier mode at the outer boundary):\n  - $r_{\\mathrm{in}} = 1$, $r_{\\mathrm{out}} = 2$, $N_{r} = 32$, $N_{\\theta} = 64$.\n  - Boundary values: $\\phi(r_{\\mathrm{in}},\\theta) = 0$ and $\\phi(r_{\\mathrm{out}},\\theta) = \\cos(n\\theta)$ with $n=3$ and $\\theta$ in radians.\n  - Analytic solution: $\\phi_{\\mathrm{exact}}(r,\\theta) = \\left(a\\,r^{n} + b\\,r^{-n}\\right)\\cos(n\\theta)$, where $a$ and $b$ satisfy $a\\,r_{\\mathrm{in}}^{n} + b\\,r_{\\mathrm{in}}^{-n} = 0$ and $a\\,r_{\\mathrm{out}}^{n} + b\\,r_{\\mathrm{out}}^{-n} = 1$. For $n=3$, $a = \\dfrac{1}{r_{\\mathrm{out}}^{3} - r_{\\mathrm{in}}^{6}\\,r_{\\mathrm{out}}^{-3}}$ and $b = -a\\,r_{\\mathrm{in}}^{6}$.\n- Case $3$ (thin annulus with radial Dirichlet data):\n  - $r_{\\mathrm{in}} = 0.5$, $r_{\\mathrm{out}} = 0.6$, $N_{r} = 16$, $N_{\\theta} = 64$.\n  - Boundary values: $\\phi(r_{\\mathrm{in}},\\theta) = 0$ and $\\phi(r_{\\mathrm{out}},\\theta) = 1$ for all $\\theta$.\n  - Analytic solution: $\\phi_{\\mathrm{exact}}(r,\\theta) = \\dfrac{\\ln r - \\ln r_{\\mathrm{in}}}{\\ln r_{\\mathrm{out}} - \\ln r_{\\mathrm{in}}}$, independent of $\\theta$.\n\nQuantifiable answer:\n- For each case, compute the maximum absolute error $E_{\\max} = \\max_{i,j}|\\phi_{i,j} - \\phi_{\\mathrm{exact}}(r_{i},\\theta_{j})|$ over all grid points, including boundaries. Each case yields a single floating-point number.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each floating-point value rounded and formatted in scientific notation with $6$ significant digits (for example, $\\texttt{[1.234567e-04,2.345678e-05,3.456789e-06]}$).",
            "solution": "The problem statement has been critically examined and is determined to be **valid**. It presents a well-posed, scientifically grounded problem in computational physics, providing all necessary definitions, constants, and boundary conditions for a unique solution to be computed and verified. The task is to solve the Laplace equation in an annular domain using a Successive Over-Relaxation (SOR) method, which is a standard and appropriate technique for this class of elliptic partial differential equations. The problem is free from ambiguity, logical contradictions, and factual errors. We may proceed with the solution.\n\n### 1. Finite-Difference Discretization of the Laplacian\nThe governing equation is the Laplace equation in polar coordinates $(r, \\theta)$:\n$$ \\nabla^2 \\phi = \\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r \\frac{\\partial \\phi}{\\partial r}\\right) + \\frac{1}{r^2}\\frac{\\partial^2\\phi}{\\partial\\theta^2} = 0 $$\nWe discretize this equation on a uniform polar grid defined by points $(r_i, \\theta_j)$, where $r_i = r_{\\mathrm{in}} + i\\Delta r$ for $i \\in \\{0, 1, ..., N_r-1\\}$ and $\\theta_j = j\\Delta\\theta$ for $j \\in \\{0, 1, ..., N_\\theta-1\\}$. The grid spacings are $\\Delta r = (r_{\\mathrm{out}}-r_{\\mathrm{in}})/(N_r-1)$ and $\\Delta\\theta = 2\\pi/N_\\theta$. The potential at a grid point is denoted by $\\phi_{i,j} \\equiv \\phi(r_i, \\theta_j)$.\n\nThe discretization must be second-order accurate. We will derive the discrete form for each term at an interior grid point $(r_i, \\theta_j)$.\n\n**Angular Term:** The term $\\frac{1}{r^2}\\frac{\\partial^2\\phi}{\\partial\\theta^2}$ is discretized at $r=r_i$ using a standard second-order central difference for the second derivative in $\\theta$:\n$$ \\left.\\frac{1}{r^2}\\frac{\\partial^2\\phi}{\\partial\\theta^2}\\right|_{(r_i, \\theta_j)} \\approx \\frac{1}{r_i^2} \\left( \\frac{\\phi_{i,j+1} - 2\\phi_{i,j} + \\phi_{i,j-1}}{(\\Delta\\theta)^2} \\right) $$\n\n**Radial Term:** The term $\\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r \\frac{\\partial \\phi}{\\partial r}\\right)$ is given in divergence form. To preserve second-order accuracy, we discretize it by evaluating the derivative at $r_i$ using centered differences on half-integer grid points $r_{i\\pm 1/2} = r_i \\pm \\Delta r/2$:\n$$ \\left. \\frac{1}{r}\\frac{\\partial}{\\partial r}\\left(r \\frac{\\partial \\phi}{\\partial r}\\right) \\right|_{(r_i, \\theta_j)} \\approx \\frac{1}{r_i} \\frac{ \\left. r\\frac{\\partial\\phi}{\\partial r} \\right|_{r_{i+1/2}} - \\left. r\\frac{\\partial\\phi}{\\partial r} \\right|_{r_{i-1/2}} }{\\Delta r} $$\nThe derivatives $\\frac{\\partial\\phi}{\\partial r}$ at the half-points are also approximated by central differences:\n$$ \\left. \\frac{\\partial\\phi}{\\partial r} \\right|_{r_{i+1/2}} \\approx \\frac{\\phi_{i+1,j} - \\phi_{i,j}}{\\Delta r} \\quad \\text{and} \\quad \\left. \\frac{\\partial\\phi}{\\partial r} \\right|_{r_{i-1/2}} \\approx \\frac{\\phi_{i,j} - \\phi_{i-1,j}}{\\Delta r} $$\nSubstituting these into the expression for the radial term yields:\n$$ \\frac{1}{r_i \\Delta r} \\left[ r_{i+1/2} \\left(\\frac{\\phi_{i+1,j} - \\phi_{i,j}}{\\Delta r}\\right) - r_{i-1/2} \\left(\\frac{\\phi_{i,j} - \\phi_{i-1,j}}{\\Delta r}\\right) \\right] $$\nUsing $r_{i\\pm 1/2} = r_i \\pm \\frac{\\Delta r}{2}$, the expression becomes:\n$$ \\frac{1}{r_i (\\Delta r)^2} \\left[ (r_i + \\frac{\\Delta r}{2})(\\phi_{i+1,j} - \\phi_{i,j}) - (r_i - \\frac{\\Delta r}{2})(\\phi_{i,j} - \\phi_{i-1,j}) \\right] $$\nRearranging terms with respect to the potential $\\phi$:\n$$ \\frac{1}{(\\Delta r)^2} \\left[ \\left(1 + \\frac{\\Delta r}{2r_i}\\right)\\phi_{i+1,j} - 2\\phi_{i,j} + \\left(1 - \\frac{\\Delta r}{2r_i}\\right)\\phi_{i-1,j} \\right] $$\nThis is the required second-order accurate finite-difference approximation for the radial part.\n\n**Complete Discrete Equation:**\nThe full discrete Laplace equation, $\\mathcal{L}_h[\\phi]_{i,j} = 0$, is the sum of the discrete radial and angular terms:\n$$ \\mathcal{L}_h[\\phi]_{i,j} = \\frac{1}{(\\Delta r)^2} \\left[ \\left(1 + \\frac{\\Delta r}{2r_i}\\right)\\phi_{i+1,j} + \\left(1 - \\frac{\\Delta r}{2r_i}\\right)\\phi_{i-1,j} - 2\\phi_{i,j} \\right] + \\frac{1}{r_i^2 (\\Delta\\theta)^2} [\\phi_{i,j+1} + \\phi_{i,j-1} - 2\\phi_{i,j}] = 0 $$\n\n### 2. Successive Over-Relaxation (SOR) Algorithm\nThe discrete equation defines a large system of linear equations for the unknown potential values $\\phi_{i,j}$ at interior grid points. We solve this system iteratively using SOR. To derive the SOR update rule, we first isolate $\\phi_{i,j}$ from the discrete equation:\n$$ \\left( \\frac{2}{(\\Delta r)^2} + \\frac{2}{r_i^2 (\\Delta\\theta)^2} \\right) \\phi_{i,j} = \\frac{\\left(1 + \\frac{\\Delta r}{2r_i}\\right)}{(\\Delta r)^2}\\phi_{i+1,j} + \\frac{\\left(1 - \\frac{\\Delta r}{2r_i}\\right)}{(\\Delta r)^2}\\phi_{i-1,j} + \\frac{1}{r_i^2 (\\Delta\\theta)^2}(\\phi_{i,j+1} + \\phi_{i,j-1}) $$\nA Gauss-Seidel iteration updates $\\phi_{i,j}$ to a new value, $\\phi_{i,j}^{\\text{GS}}$, that solves this equation using the most recently computed values for its neighbors. Iterating over points in lexicographical order (increasing $i$, then increasing $j$), the update uses $\\phi_{i-1,j}^{\\text{new}}$, $\\phi_{i,j-1}^{\\text{new}}$, $\\phi_{i+1,j}^{\\text{old}}$, and $\\phi_{i,j+1}^{\\text{old}}$. The value $\\phi_{i,j}^{\\text{GS}}$ is:\n$$ \\phi_{i,j}^{\\text{GS}} = \\frac{ \\frac{1}{(\\Delta r)^2} \\left[ \\left(1 + \\frac{\\Delta r}{2r_i}\\right)\\phi_{i+1,j} + \\left(1 - \\frac{\\Delta r}{2r_i}\\right)\\phi_{i-1,j} \\right] + \\frac{1}{r_i^2 (\\Delta\\theta)^2} (\\phi_{i,j+1} + \\phi_{i,j-1}) } { \\frac{2}{(\\Delta r)^2} + \\frac{2}{r_i^2 (\\Delta\\theta)^2} } $$\nThe SOR method modifies this by introducing a relaxation factor $\\omega$. The new value $\\phi_{i,j}^{(k+1)}$ at iteration $k+1$ is a weighted average of the old value $\\phi_{i,j}^{(k)}$ and the Gauss-Seidel update:\n$$ \\phi_{i,j}^{(k+1)} = (1-\\omega)\\phi_{i,j}^{(k)} + \\omega\\,\\phi_{i,j}^{\\text{GS}} $$\nFor this problem, $\\omega = 1.85$. The iteration proceeds by sweeping through all interior grid points $(i, j)$ for $i \\in \\{1,...,N_r-2\\}$ and $j \\in \\{0,...,N_\\theta-1\\}$, applying this update rule at each point.\n\n### 3. Implementation and Convergence\nThe algorithm is implemented as follows:\n1.  **Grid and Initialization**: An $N_r \\times N_\\theta$ array is created for $\\phi$. The boundary values are set according to the Dirichlet conditions at $r=r_{\\mathrm{in}}$ ($i=0$) and $r=r_{\\mathrm{out}}$ ($i=N_r-1$). Interior points are initialized to $0$.\n2.  **Iteration**: A loop runs for a maximum of $N_{\\mathrm{it,max}}=50000$ iterations.\n3.  **SOR Sweep**: Inside the loop, a full sweep over all interior points is performed, updating $\\phi_{i,j}$ using the SOR formula. Periodicity in $\\theta$ is handled by using modulo arithmetic for the $j$ index (e.g., $j-1$ becomes $N_\\theta-1$ if $j=0$).\n4.  **Convergence Check**: After each full SOR sweep, the discrete residual $\\mathcal{L}_h[\\phi]_{i,j}$ is calculated for all interior points using the newly updated field $\\phi$. The $\\ell_\\infty$-norm of the residual, $\\max_{i,j} |\\mathcal{L}_h[\\phi]_{i,j}|$, is computed. If this norm is less than the tolerance $\\varepsilon = 10^{-8}$, the iteration terminates.\n5.  **Error Calculation**: Upon convergence, the numerical solution $\\phi_{i,j}$ is compared against the provided analytic solution $\\phi_{\\mathrm{exact}}(r_i, \\theta_j)$. The maximum absolute error, $E_{\\max} = \\max_{i,j} |\\phi_{i,j} - \\phi_{\\mathrm{exact}}(r_i, \\theta_j)|$, is computed over the entire grid, including boundaries.\n\nThis procedure is applied to each test case specified in the problem statement. The resulting maximum errors are collected and formatted as the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(r_in, r_out, N_r, N_theta, bc_in_func, bc_out_func, analytic_func, omega, epsilon, n_it_max):\n    \"\"\"\n    Solves the Laplace equation on an annulus using the SOR method for a single test case.\n    \"\"\"\n    # 1. Grid and Parameter Setup\n    dr = (r_out - r_in) / (N_r - 1)\n    dth = 2 * np.pi / N_theta\n    \n    r = np.linspace(r_in, r_out, N_r)\n    theta = np.linspace(0, 2 * np.pi, N_theta, endpoint=False)\n    \n    # 2D potential grid\n    phi = np.zeros((N_r, N_theta))\n    \n    # 2. Boundary Conditions\n    phi[0, :] = bc_in_func(r_in, theta)\n    phi[-1, :] = bc_out_func(r_out, theta)\n    \n    # Pre-calculate some inverse values for efficiency in loops\n    dr2_inv = 1.0 / (dr**2)\n    dth2_inv = 1.0 / (dth**2)\n\n    # 3. SOR Iteration Loop\n    for k in range(n_it_max):\n        # --- SOR Sweep ---\n        for i in range(1, N_r - 1):\n            # Coefficients that depend on r_i\n            ri = r[i]\n            ri_inv = 1.0 / ri\n            \n            coeff_ip1 = dr2_inv * (1.0 + 0.5 * dr * ri_inv)\n            coeff_im1 = dr2_inv * (1.0 - 0.5 * dr * ri_inv)\n            coeff_j_term = dth2_inv * ri_inv**2\n            \n            # Denominator of the Gauss-Seidel update term\n            den = 2.0 * dr2_inv + 2.0 * coeff_j_term\n            den_inv = 1.0 / den\n\n            for j in range(N_theta):\n                # Periodic boundary conditions in theta\n                j_plus_1 = (j + 1) % N_theta\n                j_minus_1 = (j - 1 + N_theta) % N_theta\n                \n                # Neighbors' values (using most recent updates for i-1 and j-1)\n                term_neighbors = (coeff_ip1 * phi[i + 1, j] +\n                                  coeff_im1 * phi[i - 1, j] +\n                                  coeff_j_term * (phi[i, j_plus_1] + phi[i, j_minus_1]))\n                \n                # Gauss-Seidel update value\n                phi_gs = term_neighbors * den_inv\n                \n                # SOR update\n                phi[i, j] = (1.0 - omega) * phi[i, j] + omega * phi_gs\n\n        # --- Convergence Check using Residual ---\n        max_residual = 0.0\n        for i in range(1, N_r - 1):\n            ri = r[i]\n            # Vectorized calculation for all theta points at a given radius\n            phi_i = phi[i, :]\n            phi_ip1 = phi[i+1, :]\n            phi_im1 = phi[i-1, :]\n            phi_jp1 = np.roll(phi_i, -1)\n            phi_jm1 = np.roll(phi_i, 1)\n\n            radial_term = ((1.0 + 0.5 * dr / ri) * phi_ip1 + \n                           (1.0 - 0.5 * dr / ri) * phi_im1 - \n                           2.0 * phi_i) / (dr**2)\n            angular_term = (phi_jp1 + phi_jm1 - 2.0 * phi_i) / (ri**2 * dth**2)\n            \n            residual_at_i = radial_term + angular_term\n            max_residual = max(max_residual, np.max(np.abs(residual_at_i)))\n            \n        if max_residual  epsilon:\n            break\n\n    # 4. Error Calculation\n    R_grid, THETA_grid = np.meshgrid(r, theta, indexing='ij')\n    phi_exact = analytic_func(R_grid, THETA_grid)\n    \n    max_abs_error = np.max(np.abs(phi - phi_exact))\n    \n    return max_abs_error\n\ndef solve():\n    # Define common parameters for the SOR solver\n    omega = 1.85\n    epsilon = 1e-8\n    n_it_max = 50000\n\n    # Test Case 1\n    case1_params = {\n        \"r_in\": 1.0, \"r_out\": 2.0, \"N_r\": 32, \"N_theta\": 64,\n        \"bc_in_func\": lambda r, th: np.zeros_like(th),\n        \"bc_out_func\": lambda r, th: np.ones_like(th),\n        \"analytic_func\": lambda r, th: (np.log(r) - np.log(1.0)) / (np.log(2.0) - np.log(1.0)),\n        \"omega\": omega, \"epsilon\": epsilon, \"n_it_max\": n_it_max\n    }\n\n    # Test Case 2\n    r_in_c2, r_out_c2, n_c2 = 1.0, 2.0, 3\n    a_c2 = 1.0 / (r_out_c2**n_c2 - r_in_c2**(2*n_c2) * r_out_c2**(-n_c2))\n    b_c2 = -a_c2 * r_in_c2**(2*n_c2)\n    case2_params = {\n        \"r_in\": r_in_c2, \"r_out\": r_out_c2, \"N_r\": 32, \"N_theta\": 64,\n        \"bc_in_func\": lambda r, th: np.zeros_like(th),\n        \"bc_out_func\": lambda r, th: np.cos(n_c2 * th),\n        \"analytic_func\": lambda r, th: (a_c2 * r**n_c2 + b_c2 * r**(-n_c2)) * np.cos(n_c2 * th),\n        \"omega\": omega, \"epsilon\": epsilon, \"n_it_max\": n_it_max\n    }\n\n    # Test Case 3\n    r_in_c3, r_out_c3 = 0.5, 0.6\n    case3_params = {\n        \"r_in\": r_in_c3, \"r_out\": r_out_c3, \"N_r\": 16, \"N_theta\": 64,\n        \"bc_in_func\": lambda r, th: np.zeros_like(th),\n        \"bc_out_func\": lambda r, th: np.ones_like(th),\n        \"analytic_func\": lambda r, th: (np.log(r) - np.log(r_in_c3)) / (np.log(r_out_c3) - np.log(r_in_c3)),\n        \"omega\": omega, \"epsilon\": epsilon, \"n_it_max\": n_it_max\n    }\n    \n    test_cases = [case1_params, case2_params, case3_params]\n    results = []\n    \n    for params in test_cases:\n        error = solve_case(**params)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6e}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}