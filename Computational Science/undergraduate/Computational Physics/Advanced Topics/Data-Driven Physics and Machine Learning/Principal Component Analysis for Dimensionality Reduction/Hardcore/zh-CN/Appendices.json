{
    "hands_on_practices": [
        {
            "introduction": "理论是实践的基础。要真正掌握主成分分析（PCA），最好的方法是从基本原理出发，构建并验证其有效性。本练习将指导你生成具有已知协方差结构的合成数据集，并应用PCA来检验你的实现是否能准确地恢复底层的“真实”主成分和对应的方差。通过这个过程，你不仅能加深对PCA理论基础的理解，还能锻炼进行数值验证的核心能力 。",
            "id": "2430049",
            "problem": "您将编写一个完整的程序，该程序从具有已知协方差矩阵的多元高斯分布构造合成数据，执行主成分分析，并验证经验主成分与总体主成分在指定容差范围内匹配。请完全基于概率论和线性代数的基本原理进行工作。最终程序必须按如下规定生成单行输出。\n\n定义和设置：\n- 令 $d \\in \\mathbb{N}$ 表示维度，令 $N \\in \\mathbb{N}$ 表示独立样本的数量。\n- 令 $\\Sigma \\in \\mathbb{R}^{d \\times d}$ 为一个对称正定协方差矩阵，其特征分解为\n$$\n\\Sigma = Q \\,\\Lambda\\, Q^\\top,\n$$\n其中 $Q \\in \\mathbb{R}^{d \\times d}$ 是一个正交矩阵，其列是总体特征向量 $\\{q_i\\}_{i=1}^d$，且 $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d)$，其中总体特征值为 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\gt 0$。\n- 从均值为零、协方差为 $\\Sigma$ 的多元高斯分布（记作 $\\mathcal{N}(0,\\Sigma)$）中生成 $N$ 个独立样本 $x^{(1)},\\dots,x^{(N)} \\in \\mathbb{R}^d$。\n- 通过减去样本均值并使用无偏估计量，从样本中构建经验协方差矩阵 $S \\in \\mathbb{R}^{d \\times d}$：\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)}, \\quad\nS = \\frac{1}{N-1} \\sum_{n=1}^{N} \\big(x^{(n)} - \\bar{x}\\big)\\big(x^{(n)} - \\bar{x}\\big)^\\top.\n$$\n- 对 $S$ 执行主成分分析 (PCA)，通过计算其特征分解\n$$\nS = U \\, M \\, U^\\top,\n$$\n其中 $U \\in \\mathbb{R}^{d \\times d}$ 具有标准正交列 $\\{u_i\\}_{i=1}^d$（经验特征向量），且 $M = \\mathrm{diag}(\\mu_1,\\dots,\\mu_d)$，其中经验特征值按 $\\mu_1 \\ge \\mu_2 \\ge \\cdots \\ge \\mu_d \\ge 0$ 排序。\n\n验证标准：\n- 特征值：定义最大相对误差\n$$\n\\varepsilon_\\lambda = \\max_{1 \\le i \\le d} \\frac{|\\mu_i - \\lambda_i|}{\\lambda_i}.\n$$\n- 非简并特征值的特征向量：对于任何索引 $i$，如果其对应的总体特征值 $\\lambda_i$ 与所有其他总体特征值都不同，则通过以下方式定义其对齐程度\n$$\nc_i = |q_i^\\top u_i|, \\quad s_i = \\sqrt{1 - c_i^2}.\n$$\n- 简并特征值的特征子空间：对于任何大小为 $|C| \\ge 2$ 的索引集 $C \\subset \\{1,\\dots,d\\}$，其中所有的 $\\{\\lambda_i\\}_{i \\in C}$ 都相等，定义总体投影算子 $P_{\\mathrm{true}}(C) = Q_{[:,C]} Q_{[:,C]}^\\top$ 和经验投影算子 $P_{\\mathrm{est}}(C) = U_{[:,C]} U_{[:,C]}^\\top$，并度量\n$$\n\\Delta_P(C) = \\| P_{\\mathrm{est}}(C) - P_{\\mathrm{true}}(C) \\|_F,\n$$\n其中 $\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数 (Frobenius norm)。\n\n对于每个测试用例，当且仅当以下所有条件同时成立时，判定为成功：\n- $\\varepsilon_\\lambda \\le \\tau_\\lambda$，\n- 对于所有单元素索引集 $C = \\{i\\}$，$s_i \\le \\tau_v$，\n- 对于所有 $|C| \\ge 2$ 的简并索引集 $C$，$\\Delta_P(C) \\le \\tau_P$。\n\n对所有用例使用以下全局容差：$\\tau_\\lambda = 0.06$，$\\tau_v = 0.10$，$\\tau_P = 0.20$。\n\n为确保可复现性的构造细节：\n- 对于每个测试用例，通过对一个 $d \\times d$ 的矩阵进行 $\\mathrm{QR}$ 分解来构造正交矩阵 $Q$。该矩阵的元素是独立的标准正态分布，由一个使用指定整数种子初始化的伪随机数生成器生成。并调整列的符号，使得 $\\mathrm{QR}$ 分解得到的 $R$ 矩阵的对角线元素为非负。根据指定的总体特征值构造 $\\Lambda$，并设置 $\\Sigma = Q \\Lambda Q^\\top$。\n- 使用相同的随机数生成器（使用该用例指定的相同种子初始化）生成一个具有独立标准正态分布元素的矩阵 $Z \\in \\mathbb{R}^{N \\times d}$，从而生成 $N$ 个样本，并设置 $X = Z \\, B^\\top$，其中 $B = Q \\,\\Lambda^{1/2}$，且 $\\Lambda^{1/2} = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_d})$。\n\n测试套件：\n- 用例 1（非简并谱，3 维）：\n  - $d = 3$, $N = 80000$, 总体特征值 $[\\lambda_1,\\lambda_2,\\lambda_3] = [4.0, 1.0, 0.25]$, 种子 $= 11$, 简并索引集 $= \\{\\{1\\},\\{2\\},\\{3\\}\\}$。\n- 用例 2（简并主子空间，5 维）：\n  - $d = 5$, $N = 100000$, 总体特征值 $[\\lambda_1,\\dots,\\lambda_5] = [5.0, 5.0, 2.0, 1.0, 0.5]$, 种子 $= 22$, 简并索引集 $= \\{\\{1,2\\},\\{3\\},\\{4\\},\\{5\\}\\}$。\n- 用例 3（高各向异性，2 维）：\n  - $d = 2$, $N = 40000$, 总体特征值 $[\\lambda_1,\\lambda_2] = [9.0, 0.09]$, 种子 $= 33$, 简并索引集 $= \\{\\{1\\},\\{2\\}\\}$。\n- 用例 4（完全各向同性，4 维）：\n  - $d = 4$, $N = 20000$, 总体特征值 $[\\lambda_1,\\dots,\\lambda_4] = [1.0, 1.0, 1.0, 1.0]$, 种子 $= 44$, 简并索引集 $= \\{\\{1,2,3,4\\}\\}$。\n\n所有公式中的索引均按书写方式从 1 开始；在采用从 0 开始索引的语言实现时，请相应地调整索引。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，针对上述顺序的测试用例，每个条目都是一个布尔值，表示该用例是否成功（例如，\"[True,False,True,True]\"）。",
            "solution": "目标是验证对经验数据进行主成分分析能否在指定的容差范围内恢复多元高斯分布的总体协方差矩阵的特征结构。解决方案从多元高斯分布、协方差和特征分解的定义出发。\n\n1. 总体构造。对于每个用例，我们给定一个维度 $d$、一组按非递增顺序排列的总体特征值 $\\{\\lambda_i\\}_{i=1}^d$ 和一个种子。我们构造一个正交矩阵 $Q \\in \\mathbb{R}^{d \\times d}$ 来定义特征向量，并设置\n$$\n\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_d), \\quad \\Sigma = Q \\Lambda Q^\\top.\n$$\n根据构造，$\\Sigma$ 是对称正定的，因为对于所有 $i$，$\\lambda_i \\gt 0$ 且 $Q$ 是正交的。\n\n为获得 $Q$，我们采样一个具有独立标准正态分布元素的随机矩阵 $A \\in \\mathbb{R}^{d \\times d}$，并计算其 $\\mathrm{QR}$ 分解 $A = \\tilde{Q} R$，其中 $\\tilde{Q}$ 是正交的，$R$ 是上三角矩阵。为了消除列符号的模糊性并使映射具有确定性，我们调整符号，使得 $R$ 的对角线元素为非负：如果 $R_{ii} \\lt 0$，我们将 $\\tilde{Q}$ 的第 $i$ 列和 $R$ 的第 $i$ 行乘以 $-1$。得到的 $Q$ 是正交的，其列在施蒂费尔流形 (Stiefel manifold) 上均匀分布。\n\n2. 数据生成。对于一个多元高斯分布 $\\mathcal{N}(0,\\Sigma)$，如果 $Z \\in \\mathbb{R}^{N \\times d}$ 具有独立的标准正态分布元素，并且 $B \\in \\mathbb{R}^{d \\times d}$ 满足 $B B^\\top = \\Sigma$，那么 $X = Z B^\\top$ 就包含了来自 $\\mathcal{N}(0,\\Sigma)$ 的 $N$ 个独立样本。使用 $\\Sigma$ 的特征分解，我们设置\n$$\nB = Q \\Lambda^{1/2}, \\quad \\Lambda^{1/2} = \\mathrm{diag}(\\sqrt{\\lambda_1},\\dots,\\sqrt{\\lambda_d}),\n$$\n这满足 $B B^\\top = Q \\Lambda^{1/2} (Q \\Lambda^{1/2})^\\top = Q \\Lambda Q^\\top = \\Sigma$。我们使用一个为该用例指定的种子初始化的伪随机数生成器来生成 $Z$，以确保可复现性。因此，我们得到数据矩阵 $X \\in \\mathbb{R}^{N \\times d}$，其行是样本向量 $x^{(n)}$。\n\n3. 经验协方差。我们计算样本均值\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{n=1}^{N} x^{(n)},\n$$\n和无偏经验协方差矩阵\n$$\nS = \\frac{1}{N-1} \\sum_{n=1}^{N} \\big(x^{(n)} - \\bar{x}\\big)\\big(x^{(n)} - \\bar{x}\\big)^\\top.\n$$\n根据强大数定律，当 $N \\to \\infty$ 时，$S \\to \\Sigma$ 几乎必然成立。\n\n4. 主成分分析。我们计算 $S$ 的特征分解：\n$$\nS = U M U^\\top,\n$$\n其中 $U$ 具有标准正交列，$M$ 是对角矩阵且其元素非负。我们将经验特征值排序为 $\\mu_1 \\ge \\mu_2 \\ge \\cdots \\ge \\mu_d \\ge 0$，并一致地排列相应的特征向量。\n\n5. 比较指标。\n- 特征值：我们计算最大相对误差\n$$\n\\varepsilon_\\lambda = \\max_{1 \\le i \\le d} \\frac{|\\mu_i - \\lambda_i|}{\\lambda_i}.\n$$\n由于 $S \\to \\Sigma$，我们预期当 $N$ 足够大时，$\\mu_i \\to \\lambda_i$，因此 $\\varepsilon_\\lambda$ 会很小。\n\n- 非简并特征值的特征向量：当 $\\lambda_i$ 是单重的（即不同于所有其他总体特征值）时，经典的矩阵扰动理论结果表明，相关的经验特征向量 $u_i$ 会收敛到 $q_i$（相差一个任意符号）。我们通过计算以下式子来处理符号模糊性\n$$\nc_i = |q_i^\\top u_i|, \\quad s_i = \\sqrt{1 - c_i^2}.\n$$\n这里，$s_i$ 是由 $q_i$ 和 $u_i$ 张成的一维子空间之间主角的正弦值。我们要求 $s_i \\le \\tau_v$。\n\n- 简并特征值的特征子空间：当一组总体特征值相等时，相应的（不变）子空间是可识别的，但该子空间内的任何标准正交基都是可接受的。令 $C$ 为一个大小 $|C| \\ge 2$ 的索引集，使得对于所有 $i \\in C$，$\\lambda_i$ 都相等。我们比较投射到这些子空间上的经验投影算子和总体投影算子：\n$$\nP_{\\mathrm{true}}(C) = Q_{[:,C]} Q_{[:,C]}^\\top, \\quad P_{\\mathrm{est}}(C) = U_{[:,C]} U_{[:,C]}^\\top,\n$$\n并通过弗罗贝尼乌斯范数 (Frobenius norm) 来度量差异\n$$\n\\Delta_P(C) = \\| P_{\\mathrm{est}}(C) - P_{\\mathrm{true}}(C) \\|_F.\n$$\n该度量对于张成子空间的标准正交基的选择是不变的。随着 $N$ 的增长，$\\Delta_P(C) \\to 0$。\n\n6. 容差与通过标准。我们强制执行三个容差：特征值相对误差 $\\tau_\\lambda = 0.06$；由 $s_i$ 度量的一维子空间错位 $\\tau_v = 0.10$；以及简并子空间投影算子差异 $\\tau_P = 0.20$。如果对于相应的索引和子空间，所有三个条件同时满足，则测试用例通过。\n\n7. 测试套件。我们评估四个用例：\n- 用例 1：$d = 3$, $N = 80000$，非简并谱，其中 $[\\lambda_1,\\lambda_2,\\lambda_3] = [4.0, 1.0, 0.25]$，种子 $= 11$，简并索引集 $\\{\\{1\\},\\{2\\},\\{3\\}\\}$。\n- 用例 2：$d = 5$, $N = 100000$，具有简并的前导特征值对，其中 $[\\lambda_1,\\dots,\\lambda_5] = [5.0, 5.0, 2.0, 1.0, 0.5]$，种子 $= 22$，简并索引集 $\\{\\{1,2\\},\\{3\\},\\{4\\},\\{5\\}\\}$。\n- 用例 3：$d = 2$, $N = 40000$，高各向异性，其中 $[\\lambda_1,\\lambda_2] = [9.0, 0.09]$，种子 $= 33$，简并索引集 $\\{\\{1\\},\\{2\\}\\}$。\n- 用例 4：$d = 4$, $N = 20000$，各向同性，其中 $[\\lambda_1,\\dots,\\lambda_4] = [1.0, 1.0, 1.0, 1.0]$，种子 $= 44$，简并索引集 $\\{\\{1,2,3,4\\}\\}$。在这种情况下，对于全空间簇，$P_{\\mathrm{true}}(C) = I$ 且 $P_{\\mathrm{est}}(C) = I$，因此 $\\Delta_P(C) = 0$，这反映了特征向量不可识别，但子空间（整个空间）完全匹配的事实。\n\n8. 输出。对于每个用例，计算表示所有标准是否通过的布尔值，并按顺序打印这四个布尔值的列表，格式为单行“[b1,b2,b3,b4]”。\n\n该方法的合理性在于样本协方差收敛于总体协方差，以及在对称矩阵扰动下特征值和不变子空间的连续性。指定的容差考虑了量级为 $N^{-1/2}$ 的有限样本波动，并且投影算子度量为简并特征空间提供了一个与基无关的比较方法。",
            "answer": "```python\nimport numpy as np\n\ndef orthogonal_from_seed(d: int, seed: int) -> np.ndarray:\n    \"\"\"\n    Construct a deterministic orthogonal matrix Q in R^{d x d} from a given seed\n    by QR decomposition of a standard normal matrix with sign correction to\n    ensure R has nonnegative diagonal entries.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((d, d))\n    Q, R = np.linalg.qr(A)\n    # Ensure deterministic sign: make diagonal of R nonnegative\n    signs = np.sign(np.diag(R))\n    signs[signs == 0.0] = 1.0\n    Q = Q * signs  # broadcast column scaling\n    return Q\n\ndef generate_samples(N: int, lambdas: np.ndarray, Q: np.ndarray, seed: int) -> np.ndarray:\n    \"\"\"\n    Generate N samples from N(0, Sigma) where Sigma = Q diag(lambdas) Q^T.\n    Uses X = Z B^T with B = Q diag(sqrt(lambdas)), Z ~ N(0, I).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(lambdas)\n    Z = rng.standard_normal((N, d))\n    sqrt_lambdas = np.sqrt(lambdas)\n    B = Q * sqrt_lambdas  # each column i of Q scaled by sqrt(lambdas[i])\n    X = Z @ B.T\n    return X\n\ndef empirical_covariance(X: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute unbiased empirical covariance matrix S from data matrix X (N x d),\n    by centering and using factor 1/(N-1).\n    \"\"\"\n    Xc = X - X.mean(axis=0, keepdims=True)\n    N = X.shape[0]\n    S = (Xc.T @ Xc) / (N - 1)\n    return S\n\ndef pca_eigendecomposition(S: np.ndarray):\n    \"\"\"\n    Compute eigen-decomposition of symmetric matrix S, returning eigenvalues\n    and eigenvectors sorted in descending order of eigenvalues.\n    \"\"\"\n    w, V = np.linalg.eigh(S)\n    idx = np.argsort(w)[::-1]\n    w = w[idx]\n    V = V[:, idx]\n    return w, V\n\ndef max_relative_eigenvalue_error(mu: np.ndarray, lam: np.ndarray) -> float:\n    return float(np.max(np.abs(mu - lam) / lam))\n\ndef singleton_vector_misalignment(U: np.ndarray, Q: np.ndarray, singletons: list) -> float:\n    \"\"\"\n    Compute max s_i = sqrt(1 - |q_i^T u_i|^2) over singleton index sets {i}.\n    Indices are 0-based.\n    \"\"\"\n    if not singletons:\n        return 0.0\n    s_vals = []\n    for i in singletons:\n        ci = float(abs(Q[:, i].T @ U[:, i]))\n        ci = max(min(ci, 1.0), 0.0)\n        si = float(np.sqrt(max(0.0, 1.0 - ci * ci)))\n        s_vals.append(si)\n    return float(np.max(s_vals)) if s_vals else 0.0\n\ndef max_projector_discrepancy(U: np.ndarray, Q: np.ndarray, clusters: list) -> float:\n    \"\"\"\n    For each cluster C with |C| >= 2, compute Frobenius norm of projector difference.\n    Return the maximum over such clusters; return 0.0 if none.\n    \"\"\"\n    deltas = []\n    for C in clusters:\n        if len(C) >= 2:\n            Uc = U[:, C]\n            Qc = Q[:, C]\n            Pest = Uc @ Uc.T\n            Ptrue = Qc @ Qc.T\n            D = Pest - Ptrue\n            delta = float(np.linalg.norm(D, ord='fro'))\n            deltas.append(delta)\n    return float(np.max(deltas)) if deltas else 0.0\n\ndef run_case(d: int, N: int, lambdas_list: list, seed: int, clusters_1based: list,\n             tau_lambda: float, tau_v: float, tau_P: float) -> bool:\n    \"\"\"\n    Run a single test case with specified parameters and tolerances.\n    clusters_1based is a list of index lists using 1-based indices; convert to 0-based.\n    \"\"\"\n    lambdas = np.array(lambdas_list, dtype=float)\n    # Build Q and Sigma\n    Q = orthogonal_from_seed(d, seed)\n    # Generate samples\n    X = generate_samples(N, lambdas, Q, seed)\n    # Empirical covariance and PCA\n    S = empirical_covariance(X)\n    mu, U = pca_eigendecomposition(S)\n    # Ensure eigenvalues are in descending order corresponding to lambdas (which are given descending)\n    # Prepare index clusters\n    clusters0 = [[i - 1 for i in C] for C in clusters_1based]\n    # Split clusters into singletons and degenerate\n    singleton_indices = [C[0] for C in clusters0 if len(C) == 1]\n    # Metrics\n    eps_lambda = max_relative_eigenvalue_error(mu, lambdas)\n    s_max = singleton_vector_misalignment(U, Q, singleton_indices)\n    deltaP_max = max_projector_discrepancy(U, Q, clusters0)\n    # Pass criteria\n    return (eps_lambda <= tau_lambda) and (s_max <= tau_v) and (deltaP_max <= tau_P)\n\ndef solve():\n    # Global tolerances\n    tau_lambda = 0.06\n    tau_v = 0.10\n    tau_P = 0.20\n\n    # Define the test cases in the specified order\n    test_cases = [\n        # Case 1: d=3, N=80000, lambdas=[4.0,1.0,0.25], seed=11, clusters={{1},{2},{3}}\n        {\"d\": 3, \"N\": 80000, \"lambdas\": [4.0, 1.0, 0.25], \"seed\": 11, \"clusters\": [[1], [2], [3]]},\n        # Case 2: d=5, N=100000, lambdas=[5.0,5.0,2.0,1.0,0.5], seed=22, clusters={{1,2},{3},{4},{5}}\n        {\"d\": 5, \"N\": 100000, \"lambdas\": [5.0, 5.0, 2.0, 1.0, 0.5], \"seed\": 22, \"clusters\": [[1, 2], [3], [4], [5]]},\n        # Case 3: d=2, N=40000, lambdas=[9.0,0.09], seed=33, clusters={{1},{2}}\n        {\"d\": 2, \"N\": 40000, \"lambdas\": [9.0, 0.09], \"seed\": 33, \"clusters\": [[1], [2]]},\n        # Case 4: d=4, N=20000, lambdas=[1.0,1.0,1.0,1.0], seed=44, clusters={{1,2,3,4}}\n        {\"d\": 4, \"N\": 20000, \"lambdas\": [1.0, 1.0, 1.0, 1.0], \"seed\": 44, \"clusters\": [[1, 2, 3, 4]]},\n    ]\n\n    results = []\n    for case in test_cases:\n        ok = run_case(\n            d=case[\"d\"],\n            N=case[\"N\"],\n            lambdas_list=case[\"lambdas\"],\n            seed=case[\"seed\"],\n            clusters_1based=case[\"clusters\"],\n            tau_lambda=tau_lambda,\n            tau_v=tau_v,\n            tau_P=tau_P\n        )\n        results.append(ok)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在实际应用中，数据预处理是至关重要的一步。主成分分析（PCA）通过寻找数据方差最大的方向来工作，这意味着它对各个特征的尺度（scale）非常敏感。如果一个特征的数值范围远大于其他特征，它将在分析中占据主导地位，这可能会掩盖数据中更精细的结构。本练习将通过一个具体案例，让你亲身体会特征标准化对PCA结果的巨大影响，并理解为何在应用PCA前通常需要进行这一预处理步骤 。",
            "id": "2430028",
            "problem": "给定一个数据矩阵族，其列具有迥然不同的数值尺度。对于每种情况，考虑一个实值数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其中有 $d=3$ 个特征和 $n$ 个样本。对于下方的每个测试用例，通过显式公式确定性地定义 $X$。任务是比较在均值中心化数据和标准化数据上执行主成分分析（PCA）的结果，并量化由于标准化，主导主成分方向及其解释的方差比例如何变化。此处，主成分分析（PCA）定义为对转换后数据的样本协方差矩阵进行特征分解。特征标准化在此定义为对每列进行中心化（减去其样本均值）并除以其样本标准差；如果某列的标准差为零，则在中心化后将该标准化列保持为零。\n\n使用的定义：\n- 给定 $X \\in \\mathbb{R}^{n \\times d}$，令 $\\bar{\\mathbf{x}} \\in \\mathbb{R}^{d}$ 为列样本均值，并令 $X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$ 表示中心化数据，其中 $\\mathbf{1} \\in \\mathbb{R}^{n}$ 是全1向量。样本协方差矩阵为 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n- PCA特征值和特征向量是 $S$ 的特征对 $\\{(\\lambda_k,\\mathbf{v}_k)\\}_{k=1}^d$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_d \\ge 0$，且特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分的方差解释率为 $r_1 = \\frac{\\lambda_1}{\\sum_{j=1}^d \\lambda_j}$。\n- 对于标准化数据，计算 $X_c$ 的列样本标准差 $\\sigma_j$。对于所有 $\\sigma_j \\neq 0$ 的 $j$，通过 $Z_{:,j} = X_{c,:,j}/\\sigma_j$ 形成 $Z$；对于任何 $\\sigma_j = 0$ 的 $j$，则 $Z_{:,j} = \\mathbf{0}$。然后定义 $S^{(z)} = \\frac{1}{n-1} Z^\\top Z$ 及其特征对 $\\{(\\mu_k,\\mathbf{u}_k)\\}_{k=1}^d$，排序为 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3 \\ge 0$，其方差解释率为 $r_1^{(z)} = \\frac{\\mu_1}{\\sum_{j=1}^d \\mu_j}$。\n- 两个单位主方向 $\\mathbf{v}_1$ 和 $\\mathbf{u}_1$ 之间的对齐度通过 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1| \\in [0,1]$ 来衡量。特征向量的符号不确定性通过绝对值来处理。\n- 为了量化原始坐标轴对成分的主导性，定义 $i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$ 和 $i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$，特征索引使用从零开始的编号。\n\n对于每个测试用例，你必须计算以下有序的量值列表：\n- 从 $S$ 计算的 $r_1$，\n- 从 $S^{(z)}$ 计算的 $r_1^{(z)}$，\n- $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$，\n- $i_{\\mathrm{before}}$，\n- $i_{\\mathrm{after}}$。\n\n测试套件（每个用例定义 $n$，然后是 $i \\in \\{0,\\dots,n-1\\}$ 的 $t_i$，以及作为 $t_i$ 函数的三个特征）：\n- 用例 1：$n=200$。对于每个 $i \\in \\{0,\\dots,199\\}$，令 $t_i = \\frac{i}{199}$，并定义\n  - $x_{i1} = 1000 \\cos(2\\pi t_i)$，\n  - $x_{i2} = \\sin(2\\pi t_i) + 0.1 \\cos(4\\pi t_i)$，\n  - $x_{i3} = 0.001\\, t_i$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 用例 2：$n=100$。对于每个 $i \\in \\{0,\\dots,99\\}$，令 $t_i = \\frac{i}{99}$，并定义\n  - $x_{i1} = 10^6\\, t_i$，\n  - $x_{i2} = 0$，\n  - $x_{i3} = 10\\,(t_i - 0.5)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n- 用例 3：$n=150$。对于每个 $i \\in \\{0,\\dots,149\\}$，令 $t_i = \\frac{i}{149}$，并定义\n  - $x_{i1} = 1000\\,(2 t_i - 1)$，\n  - $x_{i2} = (2 t_i - 1) + 0.1 \\sin(3\\pi t_i)$，\n  - $x_{i3} = 0.01 \\cos(5\\pi t_i)$。\n  通过将这三个特征堆叠为列来组合成 $X$。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表。对于每个用例，输出有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。将三个用例聚合到一个包含三个列表的列表中，顺序为用例 1、2、3。例如，整体打印的结构必须是 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ 的形式。\n\n所有答案均为指定的无量纲实数或整数。不需要物理单位或角度单位，因为所有要求的量都是纯数。",
            "solution": "用户提供了一个有效且需要解决方案的问题。问题陈述在科学上基于线性代数和统计学领域，特别是主成分分析（PCA）。它是一个适定问题，为构建数据提供了确定性指令，定义了所有必要的数学对象和过程，并要求一组特定的、可计算的量。语言是客观的，没有歧义。因此，可以构建一个合理的、循序渐进的解决方案。\n\n该问题要求比较对三个不同案例的均值中心化数据与标准化数据执行PCA的结果。问题的核心在于观察缩放如何影响PCA的结果。PCA识别数据集中的最大方差方向。当特征（数据矩阵的列）具有迥然不同的尺度时，具有最大方差的特征将主导第一个主成分，而不管底层的数据结构如何。标准化通过将每个特征重新缩放至均值为0、标准差为1，将所有特征置于平等地位，从而防止这种情况发生。\n\n总体步骤如下：\n1. 对于每个测试用例，构建 $n \\times d$ 数据矩阵 $X$，其中 $d=3$。\n2. 对均值中心化数据 $X_c$ 执行PCA。\n    a. 计算列样本均值向量 $\\bar{\\mathbf{x}}$。\n    b. 对数据进行中心化：$X_c = X - \\mathbf{1}\\bar{\\mathbf{x}}^\\top$。\n    c. 计算样本协方差矩阵 $S = \\frac{1}{n-1} X_c^\\top X_c$。\n    d. 通过求解特征问题 $S\\mathbf{v}_k = \\lambda_k\\mathbf{v}_k$ 来找到 $S$ 的特征值 $\\lambda_k$ 和特征向量 $\\mathbf{v}_k$。特征值已排序，$\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3$，特征向量 $\\{\\mathbf{v}_k\\}$ 是标准正交的。第一主成分方向是 $\\mathbf{v}_1$。\n    e. 计算第一主成分的方差解释率：$r_1 = \\lambda_1 / (\\sum_{j=1}^d \\lambda_j)$。\n    f. 识别主导 $\\mathbf{v}_1$ 的原始特征：$i_{\\mathrm{before}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{v}_1)_j|$。\n\n3. 对标准化数据 $Z$ 执行PCA。\n    a. 使用除数 $n-1$ 计算 $X$ 的列样本标准差 $\\sigma_j$。\n    b. 构建标准化数据矩阵 $Z$。每列 $Z_{:,j}$ 是通过将相应的中心化列 $X_{c,:,j}$ 乘以 $1/\\sigma_j$ 得到的。如果 $\\sigma_j=0$，则该列 $Z_{:,j}$ 设置为零向量。\n    c. 计算 $Z$ 的样本协方差矩阵：$S^{(z)} = \\frac{1}{n-1} Z^\\top Z$。该矩阵等价于 $X$ 的样本相关矩阵。对于任何非恒定特征，其对角线元素为1。\n    d. 找到 $S^{(z)}$ 的特征值 $\\mu_k$ 和特征向量 $\\mathbf{u}_k$，排序使得 $\\mu_1 \\ge \\mu_2 \\ge \\mu_3$。标准化数据的第一主成分方向是 $\\mathbf{u}_1$。\n    e. 计算相应的方差解释率：$r_1^{(z)} = \\mu_1 / (\\sum_{j=1}^d \\mu_j)$。分母中的总和 $\\text{Tr}(S^{(z)})$ 等于非恒定特征的数量。\n    f. 识别主导 $\\mathbf{u}_1$ 的原始特征：$i_{\\mathrm{after}} = \\arg\\max_{j \\in \\{0,1,2\\}} |(\\mathbf{u}_1)_j|$。\n\n4. 通过计算对齐度量 $a = |\\mathbf{v}_1^\\top \\mathbf{u}_1|$ 来比较两次分析的结果，该度量测量两个主方向之间的夹角余弦。\n\n5. 对于每个案例，最终输出是有序列表 $[r_1, r_1^{(z)}, a, i_{\\mathrm{before}}, i_{\\mathrm{after}}]$。\n\n案例具体分析：\n- **案例 1**：数据由三个特征组成，尺度分别为 $O(10^3)$、$O(1)$ 和 $O(10^{-3})$。第一个特征 $x_1 = 1000 \\cos(2\\pi t_i)$ 的方差将远远大于其他特征。因此，未标准化数据的第一个主成分 $\\mathbf{v}_1$ 预计将几乎完全与第一个特征轴对齐。这将得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}} = 0$。标准化后，所有特征的方差都为单位1，其结构关系（$(x_1, x_2)$ 平面上的椭圆轨迹）将变得明显。方差将更均匀地分布，导致较小的 $r_1^{(z)}$，并且 $\\mathbf{u}_1$ 将是特征1和2的组合。\n- **案例 2**：第一个特征 $x_1 = 10^6 t_i$ 具有巨大的尺度。第二个特征 $x_2=0$ 是常数，方差为零。第三个特征 $x_3 = 10(t_i-0.5)$ 的尺度远小于第一个。对于未标准化的数据，PCA 将由特征1主导，得到 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}}=0$。标准化后，常数特征 $x_2$ 仍然是零向量。特征1和3都是 $t_i$ 的线性函数，在中心化和缩放后将变得完全相关。它们的标准化版本将是相同的，$Z_{:,1} = Z_{:,3}$。数据将在 $(Z_1, Z_3)$ 平面上坍缩到一个方向上。这将导致 $r_1^{(z)}=1$（因为非恒定特征中的有效秩为1）和一个形如 $[1/\\sqrt{2}, 0, 1/\\sqrt{2}]^\\top$ 的特征向量 $\\mathbf{u}_1$。\n- **案例 3**：第一个特征 $x_1 = 1000(2t_i-1)$ 具有大尺度。第二个特征 $x_2 = (2t_i-1) + 0.1 \\sin(3\\pi t_i)$ 与第一个特征高度相关，但尺度小得多。第三个特征的尺度可以忽略不计。与其他案例一样，未标准化的PCA将由第一个特征的尺度决定，所以 $r_1 \\approx 1$ 和 $i_{\\mathrm{before}}=0$。标准化后，特征1和2之间的强线性关系将是最突出的特征。第一个主成分 $\\mathbf{u}_1$ 将捕捉到这种共享方差，代表一个沿着相关云团主轴的方向，大致在特征1和2的标准化轴之间成45度角。\n\n实现将利用 `numpy` 进行所有数值计算，特别是使用 `numpy.linalg.eigh` 对对称协方差矩阵进行特征分解。将注意处理特征值的降序排序以及零标准差的情况。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PCA comparison problem for three given test cases.\n    \"\"\"\n\n    def generate_case_1_data():\n        n = 200\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * np.cos(2 * np.pi * t)\n        x2 = np.sin(2 * np.pi * t) + 0.1 * np.cos(4 * np.pi * t)\n        x3 = 0.001 * t\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_2_data():\n        n = 100\n        t = np.linspace(0, 1, n)\n        x1 = 1e6 * t\n        x2 = np.zeros(n)\n        x3 = 10 * (t - 0.5)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def generate_case_3_data():\n        n = 150\n        t = np.linspace(0, 1, n)\n        x1 = 1000 * (2 * t - 1)\n        x2 = (2 * t - 1) + 0.1 * np.sin(3 * np.pi * t)\n        x3 = 0.01 * np.cos(5 * np.pi * t)\n        return np.stack([x1, x2, x3], axis=1)\n\n    def perform_pca_analysis(X):\n        \"\"\"\n        Performs PCA on both mean-centered and standardized data, returning the required metrics.\n        \"\"\"\n        n, d = X.shape\n        \n        # --- PCA on Mean-Centered Data ---\n        X_c = X - X.mean(axis=0)\n        S = (X_c.T @ X_c) / (n - 1)\n        \n        # Eigendecomposition. eigh returns sorted eigenvalues (ascending).\n        # We reverse them to get principal components in descending order of variance.\n        evals, evecs = np.linalg.eigh(S)\n        evals = evals[::-1]\n        evecs = evecs[:, ::-1]\n        \n        lambda_1 = evals[0]\n        v1 = evecs[:, 0]\n        \n        # Explained variance ratio\n        total_variance = np.sum(evals)\n        r1 = lambda_1 / total_variance if total_variance > 0 else 0\n        \n        # Dominant feature index\n        i_before = np.argmax(np.abs(v1))\n\n        # --- PCA on Standardized Data ---\n        stds = np.std(X_c, axis=0, ddof=1)\n        \n        # Handle features with zero standard deviation\n        Z = np.zeros_like(X_c)\n        non_zero_std_mask = stds > 0\n        if np.any(non_zero_std_mask):\n            Z[:, non_zero_std_mask] = X_c[:, non_zero_std_mask] / stds[non_zero_std_mask]\n\n        S_z = (Z.T @ Z) / (n - 1)\n        \n        # Eigendecomposition of the correlation matrix\n        mu, u = np.linalg.eigh(S_z)\n        mu = mu[::-1]\n        u = u[:, ::-1]\n        \n        mu_1 = mu[0]\n        u1 = u[:, 0]\n        \n        # Explained variance ratio for standardized data\n        total_variance_z = np.sum(mu)\n        r1_z = mu_1 / total_variance_z if total_variance_z > 0 else 0\n        \n        # Dominant feature index after standardization\n        i_after = np.argmax(np.abs(u1))\n        \n        # Alignment between the first principal components\n        alignment = np.abs(np.dot(v1, u1))\n        \n        return [r1, r1_z, alignment, int(i_before), int(i_after)]\n\n    test_cases = [\n        generate_case_1_data,\n        generate_case_2_data,\n        generate_case_3_data\n    ]\n    \n    all_results = []\n    for case_generator in test_cases:\n        X = case_generator()\n        result = perform_pca_analysis(X)\n        all_results.append(result)\n\n    # Format the output string without spaces as [[...],[...],[...]]\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "与特征尺度问题类似，主成分分析（PCA）对异常值（outliers）也同样敏感。由于PCA旨在最大化方差，一个远离数据主体分布的异常点会极大地增加其所在方向的方差，从而对主成分的方向产生不成比例的巨大影响。本练习将让你通过在一个结构清晰的二维数据集上添加单个异常点，来量化分析第一主成分方向发生的变化。这个实践将帮助你直观地理解为何在应用PCA时，识别和处理异常值是一个不可忽视的环节 。",
            "id": "2430058",
            "problem": "您需要量化单个远距离离群点如何影响一个$2$维数据集中第一主成分的方向。考虑一个由$M$个点组成的确定性基础点云，这些点位于一个以原点为中心的椭圆上，由以下参数集定义：\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\tfrac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\tfrac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0,1,2,\\dots,M-1,\n$$\n其中 $r_x \\gt 0$ 且 $r_y \\gt 0$。通过添加一个额外的点（离群点）来构成一个增广数据集，该点的坐标为：\n$$\n\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}.\n$$\n设 $\\mathbf{S}_0$ 为基础点云的样本协方差矩阵，$\\mathbf{S}_1$ 为包含 $M$ 个椭圆点和该离群点的增广数据集的样本协方差矩阵。第一主成分定义为相应样本协方差矩阵的与其最大特征值相关联的单位特征向量。用 $\\mathbf{u}_0 \\in \\mathbb{R}^2$ 表示与 $\\mathbf{S}_0$ 的最大特征值对应的单位特征向量，用 $\\mathbf{u}_1 \\in \\mathbb{R}^2$ 表示 $\\mathbf{S}_1$ 的类似特征向量。由于特征向量的定义可以相差一个符号，我们将两个主方向之间的锐角差 $\\Delta$ 定义为：\n$$\n\\Delta = \\arccos\\!\\left(\\left|\\mathbf{u}_0^\\top \\mathbf{u}_1\\right|\\right).\n$$\n您必须以弧度为单位计算 $\\Delta$。所有最终数值答案必须以弧度表示，并四舍五入到 $6$ 位小数。\n\n测试套件。对于下面的每个参数元组 $(r_x, r_y, M, o_x, o_y)$，请按规定构建基础点云和增广数据集，计算 $\\Delta$ 并报告结果：\n- 情况 $1$（一般情况）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 10.0, 10.0)$。\n- 情况 $2$（边界情况，无影响）：$(r_x, r_y, M, o_x, o_y) = (3.0, 1.0, 60, 0.0, 0.0)$。\n- 情况 $3$（极端情况，离群点主导正交方向）：$(r_x, r_y, M, o_x, o_y) = (2.0, 1.0, 40, 0.0, 1000.0)$。\n- 情况 $4$（近各向同性基础，中等离群点）：$(r_x, r_y, M, o_x, o_y) = (1.05, 1.0, 50, 5.0, 0.2)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果与上述情况的顺序相同，每个值都四舍五入到 $6$ 位小数。例如，一个有效的输出格式是：\n\"[x_1,x_2,x_3,x_4]\"\n其中每个 $x_i$ 是一个以弧度为单位的浮点数，小数点后恰好有 $6$ 位数字。",
            "solution": "该问题已经过验证，被认为是有效的。这是一个计算物理学和线性代数中的适定问题，基于已确立的科学原理。所有术语的定义都足够严谨，所提供的数据也是一致和完整的。\n\n任务是计算当引入单个离群点时，二维数据集的第一主成分的角度偏差 $\\Delta$。基础数据集是在椭圆上均匀分布的 $M$ 个点的点云，而增广数据集则额外包含一个离群点。\n\n数据集的第一主成分是最大方差的方向，它对应于样本协方差矩阵最大特征值所关联的单位特征向量。设 $N$ 个数据点的集合为 $\\{\\mathbf{p}_i\\}_{i=1}^N$，其中每个 $\\mathbf{p}_i \\in \\mathbb{R}^2$。样本均值为 $\\bar{\\mathbf{p}} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{p}_i$。样本协方差矩阵 $\\mathbf{S}$ 由下式给出：\n$$\n\\mathbf{S} = \\frac{1}{N-1}\\sum_{i=1}^N (\\mathbf{p}_i - \\bar{\\mathbf{p}})(\\mathbf{p}_i - \\bar{\\mathbf{p}})^\\top\n$$\n请注意，分母选择 $N$ 还是 $N-1$ 并不重要，因为它只缩放协方差矩阵而不会改变其特征向量。数值实现将使用分母为 $N-1$ 的常规无偏估计量。\n\n首先，我们分析由以下公式定义的 $M$ 个点的基础点云 $\\{\\mathbf{x}_k\\}_{k=0}^{M-1}$：\n$$\n\\mathbf{x}_k = \\begin{bmatrix} r_x \\cos\\left(\\frac{2\\pi k}{M}\\right) \\\\ r_y \\sin\\left(\\frac{2\\pi k}{M}\\right) \\end{bmatrix}, \\quad k = 0, 1, \\dots, M-1\n$$\n由于余弦和正弦函数在整个周期内的对称性，当 $M > 1$ 时，基础点云的样本均值 $\\bar{\\mathbf{x}}_0$ 是零向量：\n$$\n\\bar{\\mathbf{x}}_0 = \\frac{1}{M}\\sum_{k=0}^{M-1} \\mathbf{x}_k = \\begin{bmatrix} \\frac{r_x}{M}\\sum_{k=0}^{M-1}\\cos(\\frac{2\\pi k}{M}) \\\\ \\frac{r_y}{M}\\sum_{k=0}^{M-1}\\sin(\\frac{2\\pi k}{M}) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\n基础点云的样本协方差矩阵 $\\mathbf{S}_0$ 则为：\n$$\n\\mathbf{S}_0 = \\frac{1}{M-1}\\sum_{k=0}^{M-1} \\mathbf{x}_k \\mathbf{x}_k^\\top = \\frac{1}{M-1} \\sum_{k=0}^{M-1} \\begin{bmatrix} r_x^2 \\cos^2(\\theta_k)  r_x r_y \\cos(\\theta_k)\\sin(\\theta_k) \\\\ r_x r_y \\cos(\\theta_k)\\sin(\\theta_k)  r_y^2 \\sin^2(\\theta_k) \\end{bmatrix}\n$$\n其中 $\\theta_k = \\frac{2\\pi k}{M}$。对于 $M > 2$，非对角线项的总和为零。对角线项的总和为 $\\sum \\cos^2(\\theta_k) = M/2$ 和 $\\sum \\sin^2(\\theta_k) = M/2$。因此，$\\mathbf{S}_0$ 是一个对角矩阵：\n$$\n\\mathbf{S}_0 = \\frac{M}{2(M-1)} \\begin{bmatrix} r_x^2  0 \\\\ 0  r_y^2 \\end{bmatrix}\n$$\n对角矩阵的特征向量是标准基向量。最大特征值对应于 $r_x^2$ 和 $r_y^2$ 中的较大者。如果 $r_x > r_y$，第一主成分是 $\\mathbf{u}_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。如果 $r_y > r_x$，则是 $\\mathbf{u}_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。这与椭圆的半长轴方向一致。\n\n接下来，我们分析增广数据集，它包含基础点云的 $M$ 个点以及离群点 $\\mathbf{z} = \\begin{bmatrix} o_x \\\\ o_y \\end{bmatrix}$。总点数为 $N = M+1$。增广数据集的均值 $\\bar{\\mathbf{x}}_1$ 为：\n$$\n\\bar{\\mathbf{x}}_1 = \\frac{1}{M+1}\\left(\\sum_{k=0}^{M-1}\\mathbf{x}_k + \\mathbf{z}\\right) = \\frac{1}{M+1}\\mathbf{z}\n$$\n协方差矩阵 $\\mathbf{S}_1$ 是基于这 $M+1$ 个点计算的。\n$$\n\\mathbf{S}_1 = \\frac{1}{M} \\left( \\sum_{k=0}^{M-1}(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)(\\mathbf{x}_k - \\bar{\\mathbf{x}}_1)^\\top + (\\mathbf{z} - \\bar{\\mathbf{x}}_1)(\\mathbf{z} - \\bar{\\mathbf{x}}_1)^\\top \\right)\n$$\n一般来说，$\\mathbf{S}_1$ 不是一个对角矩阵。离群点，特别是当其坐标 $(o_x, o_y)$ 很大时，将显著改变均值并为协方差和贡献一个大项，从而旋转数据分布的主轴。第一主成分 $\\mathbf{u}_1$ 是与 $\\mathbf{S}_1$ 的最大特征值对应的单位特征向量。这可以通过对数值计算出的 $\\mathbf{S}_1$ 进行特征分解来找到。\n\n角度差 $\\Delta$ 计算为两个主成分向量 $\\mathbf{u}_0$ 和 $\\mathbf{u}_1$ 之间的锐角：\n$$\n\\Delta = \\arccos(|\\mathbf{u}_0^\\top \\mathbf{u}_1|)\n$$\n点积的绝对值确保了角度是锐角，这是考虑到一个特征向量与其负向量是等价的。\n\n解决每个测试用例的算法步骤如下：\n$1$. 构建由 $r_x$ 和 $r_y$ 定义的椭圆上的 $M$ 个点的基础数据集。\n$2$. 计算基础数据集的样本协方差矩阵 $\\mathbf{S}_0$。\n$3$. 求出 $\\mathbf{S}_0$ 的特征向量和特征值。与最大特征值对应的特征向量 $\\mathbf{u}_0$ 是第一主成分。\n$4$. 通过将离群点 $\\mathbf{z}=(o_x, o_y)$ 添加到基础数据集中来构建增广数据集。\n$5$. 计算增广数据集的样本协方差矩阵 $\\mathbf{S}_1$。\n$6$. 求出 $\\mathbf{S}_1$ 的特征向量和特征值。与最大特征值对应的特征向量 $\\mathbf{u}_1$ 是新的第一主成分。\n$7$. 以弧度为单位计算角度差 $\\Delta = \\arccos(|\\mathbf{u}_0 \\cdot \\mathbf{u}_1|)$。点积的参数应被裁剪到 $[-1, 1]$ 范围内，以避免数值误差。\n$8$. 将结果四舍五入到 $6$ 位小数。\n对每组提供的参数都实施此过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the angular difference in the first principal component of a 2D dataset\n    due to the addition of an outlier.\n    \"\"\"\n    test_cases = [\n        # (r_x, r_y, M, o_x, o_y)\n        (3.0, 1.0, 60, 10.0, 10.0),\n        (3.0, 1.0, 60, 0.0, 0.0),\n        (2.0, 1.0, 40, 0.0, 1000.0),\n        (1.05, 1.0, 50, 5.0, 0.2),\n    ]\n\n    results = []\n    \n    for r_x, r_y, M, o_x, o_y in test_cases:\n        # Step 1: Construct the base dataset\n        theta = (2 * np.pi / M) * np.arange(M)\n        x_coords = r_x * np.cos(theta)\n        y_coords = r_y * np.sin(theta)\n        base_cloud = np.stack((x_coords, y_coords), axis=1)\n\n        # Step 2  3: PCA on the base cloud\n        # Covariance matrix for a centered ellipse is diagonal, so we can determine u0 analytically.\n        # This is more robust and faster than numerical computation for this specific geometry.\n        # Although numerical computation would yield the same result.\n        # Let's use numerical computation for generality and to match the full algorithm described.\n        # The choice of divisor (N or N-1) does not affect the eigenvectors.\n        # np.cov uses N-1 by default.\n        cov_0 = np.cov(base_cloud, rowvar=False)\n        eigvals_0, eigvecs_0 = np.linalg.eigh(cov_0)\n        # eigh sorts eigenvalues in ascending order, so the last eigenvector is the principal one.\n        u_0 = eigvecs_0[:, -1]\n\n        # Step 4: Construct the augmented dataset\n        outlier = np.array([[o_x, o_y]])\n        augmented_cloud = np.vstack((base_cloud, outlier))\n\n        # Step 5  6: PCA on the augmented dataset\n        cov_1 = np.cov(augmented_cloud, rowvar=False)\n        eigvals_1, eigvecs_1 = np.linalg.eigh(cov_1)\n        u_1 = eigvecs_1[:, -1]\n\n        # Step 7: Compute the angular difference\n        # Dot product between the two unit eigenvectors\n        dot_product = np.dot(u_0, u_1)\n        \n        # Take the absolute value to find the acute angle\n        abs_dot_product = np.abs(dot_product)\n        \n        # Clip to handle potential floating point inaccuracies > 1.0\n        clipped_dot = np.clip(abs_dot_product, -1.0, 1.0)\n        \n        delta = np.arccos(clipped_dot)\n\n        # Step 8: Append rounded result\n        results.append(round(delta, 6))\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}