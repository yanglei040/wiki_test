{
    "hands_on_practices": [
        {
            "introduction": "感知器可以作为一种通用的函数逼近器，但其有效性取决于所使用的优化算法。本练习将探讨如何在标准的梯度下降更新规则中加入一个源于物理学的“动量”项，以显著加速训练过程，这就像惯性帮助物体克服其路径上的小障碍一样。您将把这项技术应用于一个经典的物理问题：从谐振子的位置和速度中学习其能量。",
            "id": "2425757",
            "problem": "要求您实现并分析一个单神经元感知机，该感知机使用基于动量的优化器在确定性物理数据上进行训练。该感知机使用线性（恒等）激活函数。训练目标是通过在基于物理学原理的特征上拟合一个线性模型，从测量的位置和速度中恢复一维谐振子的能量。\n\n数据和模型规格：\n\n- 考虑一个质量为 $m$、弹簧常数为 $k$ 的一维谐振子，其能量由下式给出\n$$\nE(x,v) = \\tfrac{1}{2} m v^2 + \\tfrac{1}{2} k x^2 \\, .\n$$\n- 使用固定的物理参数 $m = 1\\,\\mathrm{kg}$ 和 $k = 4\\,\\mathrm{N/m}$。\n- 从笛卡尔网格构建一个确定性的训练集\n$$\n\\mathcal{X} = \\{-1.0\\,\\mathrm{m}, -0.5\\,\\mathrm{m}, 0.0\\,\\mathrm{m}, 0.5\\,\\mathrm{m}, 1.0\\,\\mathrm{m}\\},\\quad\n\\mathcal{V} = \\{-1.0\\,\\mathrm{m/s}, 0.0\\,\\mathrm{m/s}, 1.0\\,\\mathrm{m/s}\\} \\, ,\n$$\n构成所有的点对 $(x,v) \\in \\mathcal{X} \\times \\mathcal{V}$。对于每对点，计算其目标能量 $E(x,v)$，单位为焦耳。\n- 使用特征映射\n$$\n\\phi(x,v) = \\begin{bmatrix} x^2 \\\\ v^2 \\\\ 1 \\end{bmatrix},\n$$\n以及一个具有恒等激活函数的感知机，其预测为\n$$\n\\hat{E}(x,v; \\mathbf{w}) = \\mathbf{w}^\\top \\phi(x,v) \\, ,\n$$\n其中 $\\mathbf{w} \\in \\mathbb{R}^3$ 是可训练的权重向量。除了 $\\phi$ 中的常数特征外，该模型没有单独的偏置项。\n\n成本函数和优化：\n\n- 使用均方误差成本函数，并带有传统的二分之一因子：\n$$\nC(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\hat{E}_i - E_i\\right)^2 \\, ,\n$$\n其中 $N$ 是样本总数，$\\hat{E}_i = \\mathbf{w}^\\top \\phi(x_i,v_i)$，且 $E_i = E(x_i,v_i)$。\n- 通过动量（惯性）更新规则训练 $\\mathbf{w}$\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla C(\\mathbf{w}_{t}) + \\beta \\left(\\mathbf{w}_{t} - \\mathbf{w}_{t-1}\\right),\n$$\n其中学习率为 $\\eta > 0$，动量系数为 $\\beta \\in [0,1)$。\n- 初始化为 $\\mathbf{w}_0 = \\mathbf{0}$ 和 $\\mathbf{w}_{-1} = \\mathbf{w}_0$，以使初始惯性项为零。\n\n停止规则和数值参数：\n\n- 使用终止准则 $C(\\mathbf{w}_t) \\le \\varepsilon$，其中容差 $\\varepsilon = 10^{-6}\\,\\mathrm{J}^2$（焦耳的平方）。如果在最大迭代次数 $T_{\\max} = 20000$ 内未满足此条件，则终止并认为该次运行未收敛。\n\n测试套件：\n\n在以下参数集 $(\\eta,\\beta)$ 下，评估满足停止规则所需的迭代次数：\n\n- 情况 A: $(\\eta,\\beta) = (0.05, 0.00)$。\n- 情况 B: $(\\eta,\\beta) = (0.05, 0.50)$。\n- 情况 C: $(\\eta,\\beta) = (0.05, 0.90)$。\n- 情况 D: $(\\eta,\\beta) = (0.05, 0.99)$。\n\n答案规格：\n\n- 对于每种情况，输出一个整数：满足 $C(\\mathbf{w}_t) \\le \\varepsilon$ 的最小迭代索引 $t \\in \\{0,1,2,\\dots\\}$。如果算法在 $T_{\\max}$ 次迭代内未能满足容差，则对该情况输出整数 $-1$。\n- 您的程序应生成单行输出，其中包含按 A、B、C、D 顺序排列的结果，格式为方括号内的逗号分隔列表。\n\n物理和数值单位：\n\n- 能量目标 $E(x,v)$ 必须以焦耳为单位计算。成本 $C(\\mathbf{w})$ 必须解释为焦耳的平方。最终报告的迭代次数是无单位的整数。",
            "solution": "所述问题已经过验证，并被认定为有效。它具有科学依据，数学上是适定的，并且所有参数都得到了明确的规定。它描述了一个以神经网络和计算物理学语言表述的标准线性回归问题，该问题存在唯一的最佳解。因此，我们将进行完整的推导和实现。\n\n目标是训练一个单神经元感知机来学习一维谐振子的能量 $E$。能量由以下函数给出：\n$$\nE(x,v) = \\frac{1}{2} k x^2 + \\frac{1}{2} m v^2\n$$\n其中 $x$ 是位置，$v$ 是速度，$m$ 是质量，$k$ 是弹簧常数。给定的物理参数为 $m = 1\\,\\mathrm{kg}$ 和 $k = 4\\,\\mathrm{N/m}$。\n\n感知机模型使用线性激活函数，并将能量预测为特征的加权和：\n$$\n\\hat{E}(x,v; \\mathbf{w}) = \\mathbf{w}^\\top \\phi(x,v)\n$$\n特征向量 $\\phi(x,v)$ 定义为：\n$$\n\\phi(x,v) = \\begin{bmatrix} x^2 \\\\ v^2 \\\\ 1 \\end{bmatrix}\n$$\n通过选择这些特征，模型的预测为 $\\hat{E} = w_1 x^2 + w_2 v^2 + w_3$。该模型能够精确表达真实的能量函数。通过将真实能量 $E = (\\frac{k}{2}) x^2 + (\\frac{m}{2}) v^2 + 0$ 与模型进行比较，我们可以看到，当权重向量 $\\mathbf{w} = [w_1, w_2, w_3]^\\top$ 等于真实参数向量 $\\mathbf{w}^*$ 时，即可实现完美拟合：\n$$\n\\mathbf{w}^* = \\begin{bmatrix} k/2 \\\\ m/2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 4/2 \\\\ 1/2 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2.0 \\\\ 0.5 \\\\ 0.0 \\end{bmatrix}\n$$\n训练过程旨在找到这个最优权重向量 $\\mathbf{w}^*$。\n\n训练数据包含 $N=15$ 个样本，由位置集合 $\\mathcal{X} = \\{-1.0, -0.5, 0.0, 0.5, 1.0\\}\\,\\mathrm{m}$ 和速度集合 $\\mathcal{V} = \\{-1.0, 0.0, 1.0\\}\\,\\mathrm{m/s}$ 的笛卡尔积生成。我们可以构建一个大小为 $N \\times 3$ 的设计矩阵 $\\Phi$ 和一个大小为 $N \\times 1$ 的目标向量 $\\mathbf{E}$。$\\Phi$ 的第 $i$ 行是特征向量 $\\phi(x_i, v_i)^\\top$，$\\mathbf{E}$ 的第 $i$ 个元素是真实能量 $E(x_i, v_i)$。\n\n需要最小化的成本函数是均方误差（MSE）：\n$$\nC(\\mathbf{w}) = \\frac{1}{2N}\\sum_{i=1}^{N}\\left(\\mathbf{w}^\\top \\phi_i - E_i\\right)^2 = \\frac{1}{2N} (\\Phi \\mathbf{w} - \\mathbf{E})^\\top (\\Phi \\mathbf{w} - \\mathbf{E})\n$$\n这是关于 $\\mathbf{w}$ 的凸函数，因此基于梯度的优化方法保证能收敛到全局最小值，即对应于 $\\mathbf{w}^*$。优化更新规则需要成本函数关于权重 $\\mathbf{w}$ 的梯度。其计算如下：\n$$\n\\nabla_{\\mathbf{w}} C(\\mathbf{w}) = \\frac{1}{2N} \\nabla_{\\mathbf{w}} \\left( (\\Phi \\mathbf{w})^\\top (\\Phi \\mathbf{w}) - 2 (\\Phi \\mathbf{w})^\\top \\mathbf{E} + \\mathbf{E}^\\top \\mathbf{E} \\right)\n$$\n$$\n\\nabla_{\\mathbf{w}} C(\\mathbf{w}) = \\frac{1}{2N} \\left( 2 \\Phi^\\top \\Phi \\mathbf{w} - 2 \\Phi^\\top \\mathbf{E} \\right) = \\frac{1}{N} \\Phi^\\top (\\Phi \\mathbf{w} - \\mathbf{E})\n$$\n优化过程使用动量更新规则，这是梯度下降法的一种变体，旨在加速收敛，尤其是在成本函数曲面的“峡谷”中。在每次迭代 $t$ 时，权重根据以下规则进行更新：\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla C(\\mathbf{w}_{t}) + \\beta (\\mathbf{w}_{t} - \\mathbf{w}_{t-1})\n$$\n这里，$\\eta$ 是学习率，$\\beta$ 是动量系数。初始条件为 $\\mathbf{w}_0 = \\mathbf{0}$ 和 $\\mathbf{w}_{-1} = \\mathbf{w}_0 = \\mathbf{0}$，这确保了第一次迭代时动量项为零。\n\n对于每个给定的参数集 $(\\eta, \\beta)$，算法按以下步骤进行：\n1. 初始化权重 $\\mathbf{w}_0 = [0,0,0]^\\top$ 和 $\\mathbf{w}_{-1} = [0,0,0]^\\top$。\n2. 计算初始成本 $C(\\mathbf{w}_0)$。如果 $C(\\mathbf{w}_0) \\le \\varepsilon = 10^{-6}$，则过程以 $0$ 次迭代终止。\n3. 对于从 $1$ 到 $T_{\\max} = 20000$ 的每次迭代 $t$：\n    a. 使用当前权重 $\\mathbf{w}_{t-1}$ 计算梯度 $\\nabla C(\\mathbf{w}_{t-1})$。\n    b. 使用动量更新规则，通过 $\\mathbf{w}_{t-1}$ 和 $\\mathbf{w}_{t-2}$ 计算下一个权重向量 $\\mathbf{w}_{t}$。请注意实现的索引偏移：我们使用 $\\mathbf{w}_{\\text{current}}$ 和 $\\mathbf{w}_{\\text{previous}}$ 来计算 $\\mathbf{w}_{\\text{next}}$。\n    c. 使用新计算的权重计算成本 $C(\\mathbf{w}_t)$。\n    d. 如果 $C(\\mathbf{w}_t) \\le \\varepsilon$，则算法已收敛。迭代次数记录为 $t$，当前情况的处理终止。\n4. 如果循环完成而成本未能降至 $\\varepsilon$ 以下，则认为该次运行未收敛，结果记录为 $-1$。\n\n对四个指定的 $(\\eta, \\beta)$ 对中的每一个执行此过程，以确定收敛所需的迭代次数。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a single-neuron perceptron trained on deterministic physics data\n    using a momentum-based optimizer to recover the energy of a harmonic oscillator.\n    \"\"\"\n    #\n    # --- Problem Constants and Setup ---\n    #\n    \n    # Physical parameters\n    M_KG = 1.0  # Mass in kg\n    K_N_PER_M = 4.0  # Spring constant in N/m\n    \n    # Numerical parameters\n    TOLERANCE_EPSILON = 1e-6  # Convergence tolerance in J^2\n    MAX_ITERATIONS = 20000    # Maximum number of iterations\n\n    #\n    # --- Data Generation ---\n    #\n\n    # Define the Cartesian grid for position and velocity\n    x_vals = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    v_vals = np.array([-1.0, 0.0, 1.0])\n\n    # Create all pairs (x, v) from the grid\n    positions, velocities = np.meshgrid(x_vals, v_vals)\n    positions_flat = positions.flatten()\n    velocities_flat = velocities.flatten()\n\n    num_samples = len(positions_flat)\n\n    # Construct the feature matrix (design matrix) Phi\n    # Features are [x^2, v^2, 1]\n    phi_matrix = np.vstack([\n        positions_flat**2,\n        velocities_flat**2,\n        np.ones(num_samples)\n    ]).T\n\n    # Construct the target energy vector E\n    # E = 0.5 * k * x^2 + 0.5 * m * v^2\n    energy_vector = (0.5 * K_N_PER_M * positions_flat**2 + 0.5 * M_KG * velocities_flat**2).reshape(-1, 1)\n\n    #\n    # --- Optimization ---\n    #\n\n    # Test suite parameters (eta, beta)\n    test_cases = [\n        (0.05, 0.00),  # Case A\n        (0.05, 0.50),  # Case B\n        (0.05, 0.90),  # Case C\n        (0.05, 0.99),  # Case D\n    ]\n\n    results = []\n\n    def calculate_cost(w, phi, E, n):\n        \"\"\"Calculates the mean squared error cost.\"\"\"\n        error = phi @ w - E\n        return (0.5 / n) * np.sum(error**2)\n\n    def calculate_gradient(w, phi, E, n):\n        \"\"\"Calculates the gradient of the cost function.\"\"\"\n        error = phi @ w - E\n        return (1.0 / n) * phi.T @ error\n\n    # Iterate through each test case\n    for eta, beta in test_cases:\n        # Initialize weights for the current run\n        # w_current corresponds to w_t, w_previous corresponds to w_{t-1}\n        w_current = np.zeros((3, 1))\n        w_previous = np.zeros((3, 1))\n\n        # Check cost at initial state (t=0)\n        cost = calculate_cost(w_current, phi_matrix, energy_vector, num_samples)\n        if cost = TOLERANCE_EPSILON:\n            results.append(0)\n            continue\n            \n        converged = False\n        # Loop for iterations t = 1, 2, ..., T_max\n        for t in range(1, MAX_ITERATIONS + 1):\n            # Calculate gradient at the current weights (w_{t-1} in the problem notation)\n            grad = calculate_gradient(w_current, phi_matrix, energy_vector, num_samples)\n            \n            # Calculate the next weight vector (w_t in problem notation)\n            w_next = w_current - eta * grad + beta * (w_current - w_previous)\n            \n            # Update weights for the next iteration\n            w_previous = w_current\n            w_current = w_next\n            \n            # Check for convergence with the new weights\n            cost = calculate_cost(w_current, phi_matrix, energy_vector, num_samples)\n            if cost = TOLERANCE_EPSILON:\n                results.append(t)\n                converged = True\n                break\n        \n        # If the loop finished without converging\n        if not converged:\n            results.append(-1)\n            \n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "学习过程本身就是一个具有记忆的动力学系统。本练习通过展示感知器训练中的“磁滞”现象，揭示了机器学习与凝聚态物理之间深刻的类比。通过周期性地改变训练数据的标签，您将观察到感知器的权重所描绘的路径依赖于其历史，形成一个面积非零的回路，这与铁磁材料的磁化强度响应外部磁场的方式如出一辙。",
            "id": "2425812",
            "problem": "考虑一个带有硬阈值激活函数的感知器，该感知器在一个数据集族上进行在线训练。该数据集族随一个标量控制参数循环变化，类似于外部磁场。该感知器有一个权重向量 $\\mathbf{w}\\in \\mathbb{R}^2$ 和一个标量偏置 $b\\in \\mathbb{R}$。对于一个输入 $\\mathbf{x}\\in \\mathbb{R}^2$，感知器的输出为 $\\hat{y}=\\operatorname{sign}(\\mathbf{w}\\cdot \\mathbf{x}+b)$，其中如果 $z\\ge 0$，则 $\\operatorname{sign}(z)=+1$，否则 $\\operatorname{sign}(z)=-1$。学习规则是经典的 Rosenblatt 感知器更新：对于一个被错误标记的样本 $(\\mathbf{x},y)$（其中 $y\\in\\{-1,+1\\}$ 且满足 $y(\\mathbf{w}\\cdot \\mathbf{x}+b)\\le 0$），通过 $\\mathbf{w}\\leftarrow \\mathbf{w}+\\eta y \\mathbf{x}$ 和 $b\\leftarrow b+\\eta y$ 进行更新，其中 $\\eta0$ 是学习率。如果 $y(\\mathbf{w}\\cdot \\mathbf{x}+b)0$，则不执行更新。\n\n定义一个由 $\\mathbb{R}^2$ 中单位圆上的 $N$ 个点组成的固定、确定性的训练集：对于 $k\\in\\{0,1,\\dots,N-1\\}$，$\\mathbf{x}_k=\\big(\\cos\\theta_k,\\sin\\theta_k\\big)$，其中 $\\theta_k=\\frac{2\\pi k}{N}$。对于一个给定的标量偏移量 $a\\in\\mathbb{R}$，标签由 $y_k(a)=\\operatorname{sign}(x_{k,1}+a)$ 定义，其中 $x_{k,1}$ 是 $\\mathbf{x}_k$ 的第一个分量。参数 $a$ 将以均匀的步长从 $-A$ 循环变化到 $+A$，再回到 $-A$，其中 $A\\ge 0$。\n\n初始化 $\\mathbf{w}=\\mathbf{0}$ 和 $b=0$。对于从 $a_0=-A$ 到 $a_M=+A$ 的前向扫描中的每个 $a$ 值（步长均匀，为 $\\Delta a=\\frac{2A}{M}$，因此 $a_i=-A+i\\Delta a$ 对于 $i\\in\\{0,1,\\dots,M\\}$），使用当前标签 $y_k(a_i)$，按照固定的顺序 $k=0,1,\\dots,N-1$ 对 $N$ 个样本进行 $E$ 轮（epochs）在线训练。在 $a_i$ 处的这 $E$ 轮训练之后，记录下数据对 $(a_i,w_1)$，其中 $w_1$ 是 $\\mathbf{w}$ 的第一个分量。然后对于 $i=M-1,M-2,\\dots,0$ 使用相同的过程执行后向扫描，从当前的 $(\\mathbf{w},b)$ 继续训练，并在每一步的 $E$ 轮训练后再次记录 $(a_i,w_1)$。这会在 $(a,w_1)$ 平面中产生一个闭合回线。\n\n将回线面积 $\\mathcal{A}$ 定义为由记录的沿闭合路径的点序列 $(a,w_1)$ 所围成的多边形面积，通过鞋带公式计算。您的程序必须为以下每个测试用例计算 $\\mathcal{A}$，并将面积作为浮点数输出。\n\n测试套件：\n- 测试用例 1 (非平凡循环变化)：$N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.9$。\n- 测试用例 2 (无学习边界)：$N=32$, $\\eta=0.0$, $M=60$, $E=1$, $A=0.9$。\n- 测试用例 3 (无变化边界)：$N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.0$。\n\n要求：\n- 严格按照上述描述实现感知器和训练协议，并设置 $\\operatorname{sign}(0)=+1$。\n- 对于每一轮训练和每一个 $a$ 值，都使用固定的样本顺序 $k=0,1,\\dots,N-1$。\n- 通过对前向扫描和后向扫描（按此顺序）得到的有序点序列应用鞋带公式来计算回线面积 $\\mathcal{A}$，并通过返回到初始点来闭合多边形。\n- 答案是纯数字，不带物理单位，也不带角度。\n- 最终输出格式：您的程序应生成单行输出，其中包含按测试用例顺序排列的结果，格式为方括号内以逗号分隔的列表，例如 $[\\mathcal{A}_1,\\mathcal{A}_2,\\mathcal{A}_3]$。",
            "solution": "所呈现的问题陈述经过了严格的验证。\n\n### 第 1 步：提取已知信息\n\n- **感知器定义**：\n  - 权重向量：$\\mathbf{w} \\in \\mathbb{R}^2$\n  - 偏置：$b \\in \\mathbb{R}$\n  - 输入：$\\mathbf{x} \\in \\mathbb{R}^2$\n  - 输出函数：$\\hat{y} = \\operatorname{sign}(\\mathbf{w} \\cdot \\mathbf{x} + b)$\n  - 激活函数：如果 $z \\ge 0$，则 $\\operatorname{sign}(z) = +1$；如果 $z  0$，则 $\\operatorname{sign}(z) = -1$。\n\n- **学习规则**：\n  - 更新条件：对于样本 $(\\mathbf{x}, y)$，如果 $y(\\mathbf{w} \\cdot \\mathbf{x} + b) \\le 0$，则进行更新。\n  - 权重更新：$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y \\mathbf{x}$\n  - 偏置更新：$b \\leftarrow b + \\eta y$\n  - 学习率：$\\eta  0$\n\n- **数据集**：\n  - 样本数量：$N$\n  - 样本向量：对于 $k \\in \\{0, 1, \\dots, N-1\\}$，$\\mathbf{x}_k = (\\cos\\theta_k, \\sin\\theta_k)$。\n  - 样本角度：$\\theta_k = \\frac{2\\pi k}{N}$。\n  - 标签生成：$y_k(a) = \\operatorname{sign}(x_{k,1} + a)$，其中 $x_{k,1}$ 是 $\\mathbf{x}_k$ 的第一个分量。\n\n- **训练协议**：\n  - 初始化：$\\mathbf{w} = \\mathbf{0}$, $b = 0$。\n  - 控制参数：$a$ 从 $-A$ 到 $+A$ 再返回，循环变化。\n  - 前向扫描：对于 $i \\in \\{0, 1, \\dots, M\\}$，$a_i = -A + i \\Delta a$，其中 $\\Delta a = \\frac{2A}{M}$。\n  - 后向扫描：对于 $i \\in \\{M-1, M-2, \\dots, 0\\}$，$a_i = -A + i \\Delta a$。\n  - 每步的轮数：对于每个 $a_i$ 值，进行 $E$ 轮训练。\n  - 样本顺序：固定为 $k=0, 1, \\dots, N-1$。\n  - 测量：在每个 $a_i$ 处进行 $E$ 轮训练后，记录 $(a_i, w_1)$，其中 $w_1$ 是 $\\mathbf{w}$ 的第一个分量。\n\n- **分析**：\n  - 回线面积 $\\mathcal{A}$：使用鞋带公式对整个周期中记录的有序点序列 $(a, w_1)$ 进行计算。\n\n- **测试用例**：\n  - 用例 1：$N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.9$。\n  - 用例 2：$N=32$, $\\eta=0.0$, $M=60$, $E=1$, $A=0.9$。\n  - 用例 3：$N=32$, $\\eta=0.2$, $M=60$, $E=1$, $A=0.0$。\n\n### 第 2 步：使用提取的已知信息进行验证\n\n根据既定标准对问题进行分析。\n\n- **科学性**：该问题是计算物理和机器学习领域的一个练习。它使用标准的 Rosenblatt 感知器，这是神经网络中的一个基本模型。问题的框架——一个由外部循环变化的参数驱动的学习系统——是研究复杂系统中滞后和记忆效应的常见且有效的范式。它不含伪科学。\n- **良态问题 (Well-Posed)**：该问题具有数学上的精确性。所有参数、初始条件、更新规则以及数据生成和分析的程序都已明确定义。该算法是确定性的，确保每个测试用例都有唯一的解。\n- **客观性**：问题以客观、技术性的语言陈述，没有主观或模糊的术语。\n\n### 第 3 步：结论与行动\n\n该问题是**有效的**。它是一个定义明确的计算任务，基于机器学习的既定原则及其在物理现象建模中的应用。将提供一个解决方案。\n\n### 解决方案\n\n任务是模拟一个单层感知器，该感知器受一个循环变化的训练数据集的影响，并量化在驱动参数 ($a$) 和系统响应 ($w_1$) 的参数空间中产生的滞后回线。解决方案包括直接实现指定的模拟协议，然后进行几何面积计算。\n\n**1. 系统和数据准备**\n\n首先，我们定义系统的静态组件。训练数据输入 $\\{\\mathbf{x}_k\\}$ 是单位圆上的固定点。对于给定的 $N$，每个样本 $k$ 的向量是 $\\mathbf{x}_k = [\\cos(\\frac{2\\pi k}{N}), \\sin(\\frac{2\\pi k}{N})]^T$。这些应该预先计算好。\n\n标签 $\\{y_k\\}$ 取决于控制参数 $a$。对于周期中的每个 $a$ 值，根据 $y_k(a) = \\operatorname{sign}(x_{k,1} + a)$ 生成相应的标签。必须严格遵守规则：当 $z \\ge 0$ 时，$\\operatorname{sign}(z)=+1$。\n\n**2. 驱动系统的模拟**\n\n问题的核心是时间演化模拟。“时间”在此上下文中是指控制参数 $a$ 在其周期中的演进。系统状态由权重向量 $\\mathbf{w}$ 和偏置 $b$ 描述。\n\n模拟过程如下：\n- 初始化 $\\mathbf{w} = [0, 0]^T$ 和 $b=0$。\n- 为整个周期创建一个有序的 $a$ 值列表。前向扫描包括从 $a_0 = -A$ 到 $a_M = A$ 的 $M+1$ 个值。后向扫描包括从 $a_{M-1}$ 到 $a_0$ 的 $M$ 个值。$a$ 的总路径将有 $(M+1) + M = 2M+1$ 个步骤。\n- 一个列表 `path_points` 将用于存储记录的数据对 $(a, w_1)$。\n\n然后我们遍历有序的 $a$ 值列表。在每一步 $a$：\n- 首先，根据当前的 $a$ 值重新计算整个标签集 $\\{y_k(a)\\}$。\n- 然后，执行 $E$ 轮训练。在每一轮中，按照固定的顺序 $k=0, 1, \\dots, N-1$ 遍历所有数据样本 $(\\mathbf{x}_k, y_k(a))$。\n- 对于每个样本，检查感知器的分类。感知器的内部状态为 $z_k = \\mathbf{w} \\cdot \\mathbf{x}_k + b$。如果 $y_k(a) \\cdot z_k \\le 0$，则发生错分。\n- 如果检测到错分，则使用 Rosenblatt 规则更新权重和偏置：\n  $$ \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_k(a) \\mathbf{x}_k $$\n  $$ b \\leftarrow b + \\eta y_k(a) $$\n- 在完成当前 $a$ 值的所有 $E$ 轮训练后，我们通过将数据对 $(a, w_1)$ 附加到 `path_points` 列表中来记录状态。$w_1$ 是当前权重向量 $\\mathbf{w}$ 的第一个分量。\n\n对周期中的每个 $a$ 值重复此过程，从一个 $a$ 值到下一个 $a$ 值继续更新 $\\mathbf{w}$ 和 $b$。\n\n**3. 滞后回线面积计算**\n\n在模拟完成整个周期后，`path_points` 列表将包含 $(a, w_1)$ 平面中一个多边形的 $2M+1$ 个顶点。设这些顶点为 $(x_j, y_j)$，其中 $j=0, \\dots, 2M$，$x_j$ 是 $a$ 值，$y_j$ 是 $w_1$ 值。\n\n该多边形的面积 $\\mathcal{A}$ 使用鞋带公式计算：\n$$ \\mathcal{A} = \\frac{1}{2} \\left| \\sum_{j=0}^{2M} (x_j y_{j+1} - x_{j+1} y_j) \\right| $$\n其中索引是循环的，即 $(x_{2M+1}, y_{2M+1}) = (x_0, y_0)$。该公式可以使用向量运算高效实现。具体来说，如果 $\\mathbf{x} = [x_0, \\dots, x_{2M}]$ 且 $\\mathbf{y} = [y_0, \\dots, y_{2M}]$，则总和可以计算为 $\\mathbf{x}^T \\mathbf{y}' - \\mathbf{y}^T \\mathbf{x}'$，其中 $\\mathbf{x}'$ 和 $\\mathbf{y}'$ 是 $\\mathbf{x}$ 和 $\\mathbf{y}$ 的循环移位版本。\n\n**4. 测试用例分析**\n\n- **用例 1 ($A=0.9, \\eta=0.2$)：** 这代表了一般情况。随着 $a$ 的变化，目标分类边界 $x_1 = -a$ 扫过数据点，导致标签翻转。非零学习率 $\\eta$ 使感知器能够适应。由 $\\mathbf{w} \\cdot \\mathbf{x} + b=0$ 定义的感知器决策边界试图跟随移动的目标边界。由于学习率有限且一次只处理一个样本，这种适应存在延迟或滞后。这种滞后导致 $a$ 在前向扫描期间 $w_1$ 的路径与后向扫描期间的路径不同，从而形成一个面积非零的滞后回线 $\\mathcal{A}  0$。\n\n- **用例 2 ($A=0.9, \\eta=0.0$)：** 在此情况下，学习率为零。初始化为 0 的权重和偏置将永远不会被更新。因此，在整个模拟过程中，$w_1$ 始终为 0。记录的路径只是在 $a$ 轴上对区间 $[-0.9, 0.9]$ 的一次往返。这是一个退化的多边形（一条线段），其面积为 $\\mathcal{A} = 0$。\n\n- **用例 3 ($A=0.0, \\eta=0.2$)：** 在此情况下，驱动参数的振幅为零，意味着在整个周期内 $a=0$。标签 $y_k = \\operatorname{sign}(x_{k,1})$是固定的。感知器在这个相同的静态数据集上被重复训练了 $(2M+1) \\times E$ 轮。虽然权重 $w_1$ 会从其初始值 0 开始变化，但参数 $a$ 不会变。所有记录的点都将是 $(0, w_{1,j})$ 的形式。最终的路径是 $w_1$ 轴上的一条垂直线段。一条线段是一个退化的多边形，其面积为 $\\mathcal{A}=0$。\n\n这些对控制用例的分析性考虑为实现的正确性提供了可靠的检验。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the perceptron hysteresis problem for a given suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (nontrivial cyclic variation)\n        {'N': 32, 'eta': 0.2, 'M': 60, 'E': 1, 'A': 0.9},\n        # Case 2 (no learning boundary)\n        {'N': 32, 'eta': 0.0, 'M': 60, 'E': 1, 'A': 0.9},\n        # Case 3 (no variation boundary)\n        {'N': 32, 'eta': 0.2, 'M': 60, 'E': 1, 'A': 0.0},\n    ]\n\n    def run_simulation(N, eta, M, E, A):\n        \"\"\"\n        Runs the full simulation for one set of parameters.\n\n        Args:\n            N (int): Number of points in the dataset.\n            eta (float): Learning rate.\n            M (int): Number of steps in half a cycle.\n            E (int): Number of epochs per step.\n            A (float): Amplitude of the control parameter 'a'.\n\n        Returns:\n            list: A list of (a, w1) tuples representing the path in the parameter space.\n        \"\"\"\n        # Initialize weights and bias\n        w = np.zeros(2)\n        b = 0.0\n\n        # Generate the fixed training set inputs on the unit circle\n        theta_k = (2.0 * np.pi / N) * np.arange(N)\n        x_k = np.stack((np.cos(theta_k), np.sin(theta_k)), axis=1)\n\n        # Define the sign function as specified: sign(z) = +1 if z >= 0, -1 otherwise\n        def sign_func(z):\n            return np.where(z >= 0, 1.0, -1.0)\n\n        # Generate the sequence of 'a' values for the full cycle\n        if M == 0: # Handle edge case where A > 0 but M = 0\n             delta_a = 0.0\n             a_forward = np.array([-A]) if A > 0 else np.array([0.0])\n             a_backward = np.array([])\n        else:\n             delta_a = 2.0 * A / M\n             a_forward = -A + np.arange(M + 1) * delta_a\n             a_backward = -A + np.arange(M - 1, -1, -1) * delta_a\n        \n        a_cycle = np.concatenate((a_forward, a_backward))\n        \n        path_points = []\n        \n        # Main simulation loop\n        for a_val in a_cycle:\n            # Generate labels for the current 'a' value\n            y_k = sign_func(x_k[:, 0] + a_val)\n            \n            # Perform E epochs of training\n            for _ in range(E):\n                for i in range(N):\n                    # Calculate activation\n                    activation = np.dot(w, x_k[i]) + b\n                    \n                    # Check for misclassification and update\n                    if y_k[i] * activation = 0:\n                        w += eta * y_k[i] * x_k[i]\n                        b += eta * y_k[i]\n            \n            # Record the point (a, w1)\n            path_points.append((a_val, w[0]))\n            \n        return path_points\n\n    def shoelace_area(points):\n        \"\"\"\n        Calculates the area of a polygon using the shoelace formula.\n        \n        Args:\n            points (list): A list of (x, y) tuples for the polygon vertices.\n\n        Returns:\n            float: The area of the polygon.\n        \"\"\"\n        if len(points)  3:\n            return 0.0\n\n        # Convert list of tuples to a NumPy array for vectorized operations\n        poly = np.array(points)\n        x = poly[:, 0]\n        y = poly[:, 1]\n        \n        # Shoelace formula implementation using NumPy for efficiency\n        # Area = 0.5 * |(x0*y1 + x1*y2 + ...) - (y0*x1 + y1*x2 + ...)|\n        area = 0.5 * np.abs(np.dot(x, np.roll(y, -1)) - np.dot(y, np.roll(x, -1)))\n        \n        return area\n\n    results = []\n    for case in test_cases:\n        path = run_simulation(**case)\n        area = shoelace_area(path)\n        results.append(area)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "经典的感知器依赖于静态激活函数，其输出是输入的瞬时函数。然而，生物神经元是动态系统，它们随时间整合输入并通过“脉冲”进行交流。本练习将带您超越静态模型，实现一个“漏积分-发放”（Leaky Integrate-and-Fire, LIF）神经元，这是计算神经科学中的一个基本模型，它捕捉了神经激活的时间动态特性。",
            "id": "2425782",
            "problem": "实现一个单神经元感知器，其激活机制为受生物学启发的漏积分-发放 (Leaky Integrate-and-Fire, LIF) 模型。该神经元接收一个时间序列输入，并产生一个输出脉冲序列。时间是离散的，所有物理参数均以指定的单位给出。该模型的定义如下。\n\n一个 LIF 感知器具有膜电位 $V$，它在离散时间步 $t \\in \\{0,1,\\dots,T-1\\}$ 中以固定的步长 $\\Delta t$（单位为 $\\mathrm{ms}$）进行更新。在时间步 $t$ 的突触输入电流为\n$$\nI(t) = \\sum_{i=1}^{d} w_i\\,x_i(t) + b,\n$$\n其中 $d$ 是输入数量，$w_i$ 是权重（单位为 $\\mathrm{nA}$/单位输入），$x_i(t)$ 是输入振幅（无量纲），$b$ 是偏置电流（单位为 $\\mathrm{nA}$）。膜电位 $V$（单位为 $\\mathrm{mV}$）通过对 LIF 常微分方程进行前向欧拉积分来更新：\n$$\nV \\leftarrow V + \\frac{\\Delta t}{C}\\left(-g_L\\,(V - E_L) + I(t)\\right),\n$$\n其中 $C$ 是膜电容（单位为 $\\mathrm{nF}$），$g_L$ 是漏电导（单位为 $\\mu \\mathrm{S}$），$E_L$ 是漏泄反转电位（单位为 $\\mathrm{mV}$）。当且仅当神经元不处于不应期状态且更新后的膜电位满足以下条件时，会发放一个脉冲：\n$$\nV \\ge V_{\\mathrm{th}},\n$$\n其中 $V_{\\mathrm{th}}$ 是阈值（单位为 $\\mathrm{mV}$）。在时间步 $t$ 产生一个脉冲后，记录一个在时间 $t\\,\\Delta t$（单位为 $\\mathrm{ms}$）的脉冲，然后将 $V$ 重置为 $V_{\\mathrm{reset}}$（单位为 $\\mathrm{mV}$），并在接下来的 $\\tau_{\\mathrm{ref}}$ 个时间步内进入绝对不应期状态。在此期间，不进行积分，且 $V$ 被钳制在 $V_{\\mathrm{reset}}$。初始条件为 $V(0) = V_{\\mathrm{reset}}$，并且在 $t=0$ 时神经元不处于不应期。在以下所有测试用例中，取 $E_L = 0\\,\\mathrm{mV}$。\n\n您的任务是编写一个程序，针对以下每个测试用例，计算在指定时间范围内产生的脉冲时间列表（每个时间单位为 $\\mathrm{ms}$）。将每个脉冲时间表示为以毫秒为单位的整数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素是对应测试用例的脉冲时间列表（例如，$\\left[\\left[1,3\\right],\\left[\\right],\\left[2\\right]\\right]$）。\n\n测试套件：\n\n- 测试用例 $1$（在恒定驱动和不应期下的规律性脉冲发放）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 50$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=3$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,49\\}$，$x_1(t) = 0.4$。\n\n- 测试用例 $2$（阈下恒定驱动，无脉冲）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 50$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=3$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,49\\}$，$x_1(t) = 0.02$。\n\n- 测试用例 $3$（两次更新后精确达到阈值，无不应期）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 10$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=0$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,9\\}$，$x_1(t) = I^\\star$，其中 $I^\\star = \\dfrac{0.05}{1 - (1-0.05)^2} \\approx 0.5128205128205128$。\n\n- 测试用例 $4$（带有抑制性暂停的兴奋性驱动）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 25$, $d=2$, $w_1 = 1.0\\,\\mathrm{nA}$, $w_2 = -1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.05\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=1$ 步。\n  - 输入：\n    - 对于 $t \\in \\{0,1,\\dots,19\\}$，$x_1(t) = 1.0$，其他情况下 $x_1(t)=0.0$。\n    - 对于 $t \\in \\{10,11,12,13,14\\}$，$x_2(t) = 1.0$，其他情况下 $x_2(t)=0.0$。\n\n- 测试用例 $5$（高漏电抑制脉冲发放）：\n  - 参数：$\\Delta t = 1\\,\\mathrm{ms}$, $T = 30$, $d=1$, $w_1 = 1.0\\,\\mathrm{nA}$, $b = 0.0\\,\\mathrm{nA}$, $C=1.0\\,\\mathrm{nF}$, $g_L=0.5\\,\\mu\\mathrm{S}$, $E_L=0.0\\,\\mathrm{mV}$, $V_{\\mathrm{th}}=1.0\\,\\mathrm{mV}$, $V_{\\mathrm{reset}}=0.0\\,\\mathrm{mV}$, $\\tau_{\\mathrm{ref}}=2$ 步。\n  - 输入：对于所有 $t \\in \\{0,1,\\dots,29\\}$，$x_1(t) = 0.4$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个 Python 风格的列表的列表，第 $k$ 个内部列表包含测试用例 $k$ 的脉冲时间（单位为 $\\mathrm{ms}$，为整数）。例如，$\\left[\\left[2,8\\right],\\left[\\right],\\left[1,3,5\\right]\\right]$。",
            "solution": "问题陈述已经过严格验证，并被确定为有效。它具有科学依据，问题提法恰当，没有矛盾或含糊之处。所描述的模型是标准的漏积分-发放 (LIF) 神经元，这是计算神经科学中的一个基本构造。所有参数、单位、初始条件和动态规则都已足够精确地指定，从而可以得到唯一且可验证的解。因此，我们将进行形式化的推导和模拟。\n\n单神经元感知器的状态由其膜电位 $V$ 定义，该电位在大小为 $\\Delta t$ 的离散时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 上演化。该演化过程遵循电容器上的电荷平衡原理，并计入漏电流和外部突触输入电流 $I(t)$。\n\n在时间步 $t$ 的输入电流是 $d$ 个外部输入 $x_i(t)$ 的加权和，再加上一个偏置电流 $b$：\n$$\nI(t) = \\sum_{i=1}^{d} w_i\\,x_i(t) + b\n$$\n膜电位 $V$ 根据 LIF 微分方程的前向欧拉离散化进行更新：\n$$\n\\frac{dV}{dt} = \\frac{1}{C}\\left(-g_L\\,(V - E_L) + I(t)\\right)\n$$\n这得出了离散更新规则：\n$$\nV(t+\\Delta t) \\approx V(t) + \\frac{\\Delta t}{C}\\left(-g_L\\,(V(t) - E_L) + I(t)\\right)\n$$\n鉴于问题的表述中新值由箭头表示，我们维持使用 $V_t$ 表示在步骤 $t$ 开始时的电位，$V_{t+1}$ 表示在步骤 $t$ 内更新后的电位。\n$$\nV_{t+1} \\leftarrow V_{t} + \\frac{\\Delta t}{C}\\left(-g_L\\,(V_{t} - E_L) + I(t)\\right)\n$$\n所有参数（$C$, $g_L$, $E_L$, $\\Delta t$）和变量（$V$, $I$）都以一套一致的生物物理单位（$\\mathrm{nF}$, $\\mu\\mathrm{S}$, $\\mathrm{mV}$, $\\mathrm{ms}$, $\\mathrm{nA}$）给出，确保了量纲的正确性，无需进行单位转换，因为 $\\frac{\\mathrm{ms}}{\\mathrm{nF}} \\cdot \\mu\\mathrm{S}$ 是无量纲的，且 $\\frac{\\mathrm{ms}}{\\mathrm{nF}} \\cdot \\mathrm{nA} = \\mathrm{mV}$。使用指定值 $E_L=0\\,\\mathrm{mV}$，更新规则简化为：\n$$\nV_{t+1} \\leftarrow V_{t} \\left(1 - \\frac{\\Delta t \\cdot g_L}{C}\\right) + \\frac{\\Delta t}{C} I(t)\n$$\n\n神经元的激活机制由一个脉冲发放规则定义。如果在时间步 $t$ 更新后的电位 $V_{t+1}$ 达到或超过阈值 $V_{\\mathrm{th}}$，并且神经元不处于不应期状态，则会触发一个脉冲。\n形式上，如果 $V_{t+1} \\ge V_{\\mathrm{th}}$：\n1.  在时间 $t \\cdot \\Delta t$ 记录一个脉冲。\n2.  膜电位被重置为 $V_{\\mathrm{reset}}$。\n3.  神经元在随后的 $\\tau_{\\mathrm{ref}}$ 个时间步内进入绝对不应期。在此期间（从步骤 $t+1$到 $t+\\tau_{\\mathrm{ref}}$），电位被钳制在 $V_{\\mathrm{reset}}$，不发生积分或进一步的脉冲发放。\n\n每个测试用例的模拟算法如下：\n1.  初始化膜电位 $V_0 = V_{\\mathrm{reset}}$，一个不应期计数器 `ref_counter` 为 $0$，以及一个空的脉冲时间列表。\n2.  根据测试用例的规范构建输入信号矩阵 $x_i(t)$。\n3.  对从 $0$ 到 $T-1$ 的每个时间步 $t$ 进行迭代：\n    a. 检查不应期状态。如果 `ref_counter`  0，则将 $V_t$ 钳制在 $V_{\\mathrm{reset}}$，将 `ref_counter` 减一，并进入下一个时间步。\n    b. 如果不处于不应期（`ref_counter` = 0），计算输入电流 $I(t) = \\sum_{i} w_i x_i(t) + b$。\n    c. 使用前向欧拉规则，根据当前电位 $V_t$ 更新膜电位 $V_{t+1}$。\n    d. 检查是否有脉冲：如果 $V_{t+1} \\ge V_{\\mathrm{th}}$，记录脉冲时间 $t \\cdot \\Delta t$，将电位重置为 $V_{t+1} = V_{\\mathrm{reset}}$，并设置 `ref_counter` = $\\tau_{\\mathrm{ref}}$。\n    e. 如果没有脉冲发生，下一步的电位就是 $V_{t+1}$。\n4.  循环完成后，收集到的脉冲时间列表即为该测试用例的结果。\n\n这个过程是确定性的，并将为所提供的五个测试用例中的每一个进行实现。最终输出是所得脉冲时间列表的汇总。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Leaky Integrate-and-Fire neuron simulation problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1 (regular spiking)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 50, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 3},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.4)\n        },\n        # Test case 2 (subthreshold)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 50, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 3},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.02)\n        },\n        # Test case 3 (exact threshold crossing)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 10, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 0},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.05 / (1 - (1 - 0.05)**2))\n        },\n        # Test case 4 (inhibitory pause)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 25, \"d\": 2, \"w\": np.array([1.0, -1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.05, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 1},\n            \"input_gen\": lambda T, d: (\n                x := np.zeros((d, T)),\n                x.__setitem__((0, slice(0, 20)), 1.0),\n                x.__setitem__((1, slice(10, 15)), 1.0),\n                x\n            )[-1]\n        },\n        # Test case 5 (high leak)\n        {\n            \"params\": {\"dt\": 1.0, \"T\": 30, \"d\": 1, \"w\": np.array([1.0]), \"b\": 0.0,\n                       \"C\": 1.0, \"gL\": 0.5, \"EL\": 0.0, \"V_th\": 1.0, \"V_reset\": 0.0,\n                       \"tau_ref\": 2},\n            \"input_gen\": lambda T, d: np.full((d, T), 0.4)\n        }\n    ]\n\n    def simulate_lif(params, input_gen):\n        \"\"\"Simulates a single LIF neuron for one test case.\"\"\"\n        dt, T, d, w, b, C, gL, EL, V_th, V_reset, tau_ref = params.values()\n        \n        inputs = input_gen(T, d)\n        \n        v = V_reset\n        refractory_counter = 0\n        spike_times = []\n\n        for t in range(T):\n            if refractory_counter > 0:\n                v = V_reset\n                refractory_counter -= 1\n                continue\n\n            # Calculate input current I(t)\n            current_input = inputs[:, t]\n            i_t = np.dot(w, current_input) + b\n            \n            # Update membrane potential V using forward Euler\n            # Note: The problem statement uses V on the right side, so it means V at the start of the step\n            v_old = v\n            v = v_old + (dt / C) * (-gL * (v_old - EL) + i_t)\n\n            # Check for spike\n            if v >= V_th:\n                spike_times.append(int(t * dt))\n                v = V_reset\n                refractory_counter = tau_ref\n\n        return spike_times\n\n    results = []\n    for case in test_cases:\n        result = simulate_lif(case[\"params\"], case[\"input_gen\"])\n        results.append(result)\n    \n    # Format the output as a compact string representation of a list of lists.\n    string_lists = [f\"[{','.join(map(str, s))}]\" for s in results]\n    final_output = f\"[{','.join(string_lists)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}