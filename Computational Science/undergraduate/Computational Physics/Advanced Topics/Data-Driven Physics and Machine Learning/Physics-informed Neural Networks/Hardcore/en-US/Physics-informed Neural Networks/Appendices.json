{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds you in the core principle of Physics-Informed Neural Networks: translating a physical law and its constraints into a mathematical objective for optimization. We will use the classic Poisson's equation, which governs phenomena like electrostatic potentials, to construct a PINN's loss function. This exercise will guide you in formulating the two essential components of the lossâ€”the term that enforces the governing PDE within the domain, and the term that ensures the solution respects the specified boundary conditions .",
            "id": "2126324",
            "problem": "A researcher is building a Physics-Informed Neural Network (PINN) to find an approximate solution for the electrostatic potential, $V(x,y)$, within a two-dimensional square region. The physical behavior of the potential is described by the Poisson equation:\n$$\n\\nabla^2 V(x,y) = -f(x,y)\n$$\nwhere $f(x,y)$ represents a given charge distribution density and $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$ is the Laplace operator. The potential is defined over the domain $D = \\{(x,y) \\mid -L \\le x \\le L, -L \\le y \\le L\\}$. The boundary of this domain, $\\partial D$, is held at a zero potential (grounded), which imposes the boundary condition $V(x,y) = 0$ for all $(x,y) \\in \\partial D$.\n\nThe PINN model, denoted by $\\hat{V}(x,y; \\theta)$, learns to approximate $V(x,y)$ by minimizing a loss function $L(\\theta)$ that incorporates the physics of the problem. Here, $\\theta$ represents all the trainable parameters of the neural network. The loss function is calculated using two sets of discrete points:\n1.  A set of $N_{pde}$ collocation points, $S_{pde} = \\{(x_i, y_i) \\mid i=1, \\dots, N_{pde}\\}$, located in the interior of the domain $D$.\n2.  A set of $N_{bc}$ boundary points, $S_{bc} = \\{(x_j, y_j) \\mid j=1, \\dots, N_{bc}\\}$, located on the boundary $\\partial D$.\n\nThe total loss function, $L(\\theta)$, is the sum of two mean squared error terms: one for the governing partial differential equation ($L_{pde}$) and one for the boundary conditions ($L_{bc}$).\n\nConstruct the mathematical expression for the total loss function $L(\\theta) = L_{pde} + L_{bc}$. Your expression should be in terms of the network's output $\\hat{V}$, its second partial derivatives, the function $f$, the given point sets, and their respective sizes $N_{pde}$ and $N_{bc}$.",
            "solution": "We begin from the governing Poisson equation and boundary condition:\n$$\n\\nabla^{2}V(x,y)=-f(x,y), \\quad V(x,y)=0 \\text{ for } (x,y)\\in \\partial D.\n$$\nA Physics-Informed Neural Network approximates $V$ by $\\hat{V}(x,y;\\theta)$. The PDE residual at an interior collocation point $(x_{i},y_{i})\\in S_{pde}$ is defined by imposing the Poisson equation on $\\hat{V}$:\n$$\nr_{i}(\\theta)=\\nabla^{2}\\hat{V}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nUsing the definition of the Laplacian in two dimensions, this is equivalently\n$$\nr_{i}(\\theta)=\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i}).\n$$\nThe mean squared error enforcing the PDE over $S_{pde}$ is then\n$$\nL_{pde}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(r_{i}(\\theta)\\right)^{2}=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}.\n$$\nThe boundary condition $V=0$ on $\\partial D$ is enforced by penalizing the deviation of $\\hat{V}$ from zero at boundary points $(x_{j},y_{j})\\in S_{bc}$:\n$$\nL_{bc}(\\theta)=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)-0\\right)^{2}=\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$\nTherefore, the total loss is the sum of the two mean squared error terms:\n$$\nL(\\theta)=L_{pde}(\\theta)+L_{bc}(\\theta)=\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\frac{1}{N_{pde}}\\sum_{i=1}^{N_{pde}}\\left(\\frac{\\partial^{2}\\hat{V}}{\\partial x^{2}}(x_{i},y_{i};\\theta)+\\frac{\\partial^{2}\\hat{V}}{\\partial y^{2}}(x_{i},y_{i};\\theta)+f(x_{i},y_{i})\\right)^{2}+\\frac{1}{N_{bc}}\\sum_{j=1}^{N_{bc}}\\left(\\hat{V}(x_{j},y_{j};\\theta)\\right)^{2}}$$"
        },
        {
            "introduction": "While penalizing boundary errors in the loss function is a versatile approach, we can often achieve better training stability and accuracy by enforcing boundary conditions exactly. This exercise explores an elegant architectural method to \"hard-code\" Dirichlet boundary conditions directly into the network's output function. By designing a transformation , you will learn how to guarantee that the network's approximation satisfies the boundary conditions by construction, independent of its trainable parameter values.",
            "id": "2126300",
            "problem": "In the field of scientific computing, Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for solving differential equations. A key aspect of designing a PINN is ensuring that its output, which approximates the solution, respects the given boundary conditions. One robust method to achieve this is to structure the network's final output function so that it satisfies these conditions by construction.\n\nConsider a one-dimensional problem on the spatial domain $x \\in [0, L]$. A neural network provides a raw, unconstrained output function denoted by $\\hat{u}_{NN}(x)$. We wish to use this network to find an approximate solution, $u(x)$, to a differential equation that is subject to the following non-homogeneous Dirichlet boundary conditions:\n$$u(0) = A$$\n$$u(L) = B$$\nHere, $A$, $B$, and $L > 0$ are given real constants.\n\nYour task is to devise a transformation that takes the raw network output $\\hat{u}_{NN}(x)$ and produces a new function, $u_{NN}(x)$, that serves as the final approximation. This transformation must guarantee that $u_{NN}(x)$ strictly satisfies the specified boundary conditions, regardless of the function $\\hat{u}_{NN}(x)$ produced by the network.\n\nProvide an expression for $u_{NN}(x)$ in terms of the raw network output $\\hat{u}_{NN}(x)$ and the parameters $x$, $L$, $A$, and $B$.",
            "solution": "We seek a transformation that maps the raw network output $\\hat{u}_{NN}(x)$ to a function $u_{NN}(x)$ that enforces the Dirichlet boundary conditions $u_{NN}(0)=A$ and $u_{NN}(L)=B$ for any $\\hat{u}_{NN}(x)$. A standard construction is to decompose $u_{NN}(x)$ as\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\nwhere $g(x)$ is any fixed function that satisfies the boundary conditions and $s(x)$ is any function that vanishes at both boundaries. Specifically, we require\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\nA convenient choice is the linear interpolant for $g(x)$,\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\nand the simple vanishing factor\n$$\ns(x)=x(L-x),\n$$\nwhich satisfies $s(0)=0$ and $s(L)=0$. Therefore, define\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\nTo verify the boundary conditions, evaluate at $x=0$ and $x=L$:\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\nThus, for any $\\hat{u}_{NN}(x)$, the constructed $u_{NN}(x)$ strictly satisfies $u_{NN}(0)=A$ and $u_{NN}(L)=B$.",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        },
        {
            "introduction": "This final exercise transitions from theoretical formulation to a complete hands-on implementation, allowing you to investigate a subtle but critical aspect of neural network training known as \"spectral bias.\" You will build and train a PINN from the ground up to solve an ordinary differential equation whose solution contains both low and high-frequency components . By analyzing which component the network learns first, you will gain practical insight into the training dynamics of PINNs and a deeper understanding of why they often excel at learning smooth, low-frequency solutions.",
            "id": "2427229",
            "problem": "You will implement a complete, runnable program that demonstrates the spectral bias of a Physics-Informed Neural Network (PINN). The central idea is to train a PINN to solve a one-dimensional boundary value problem whose known solution is the superposition of a low-frequency and a high-frequency sine, namely $u(x) = \\sin(x) + \\sin(25x)$, and to quantitatively observe which frequency component is learned first during training. Angles must be in radians throughout.\n\nStart from the following physically consistent ordinary differential equation (ODE) with periodic boundary conditions:\nGiven the domain $x \\in [0, 2\\pi]$, consider\n$$\nu''(x) + u(x) = -624 \\sin(25x),\n$$\nwith periodic boundary conditions\n$$\nu(0) = u(2\\pi), \\quad u'(0) = u'(2\\pi).\n$$\nIt is a well-tested fact that if $u(x) = \\sin(x) + \\sin(25x)$ then $u''(x) + u(x) = -624 \\sin(25x)$ and the periodic boundary conditions hold. You must not use any labeled training data for $u(x)$ except the boundary conditions; instead, use the ODE residual and boundary residuals in the loss, as is standard for a Physics-Informed Neural Network (PINN).\n\nConstruct a single-hidden-layer neural network $u_{\\theta}(x)$ with $H$ hidden units and hyperbolic tangent activation as the trial solution. Define the hidden pre-activations as $z_i(x) = w_i x + b_i$ for $i \\in \\{1,\\dots,H\\}$, the hidden activations as $h_i(x) = \\tanh(z_i(x))$, and the output as\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i h_i(x) + c.\n$$\nUse the chain rule and the product rule to compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$ in closed form. Recall the standard identities for the hyperbolic tangent and its derivatives:\n$$\n\\tanh'(z) = \\operatorname{sech}^2(z), \\quad \\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z), \\quad \\operatorname{sech}^2(z) = 1 - \\tanh^2(z).\n$$\nDefine the pointwise physics residual for collocation points $\\{x_n\\}_{n=1}^{N}$ as\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n),\n$$\nand the periodic boundary residuals as\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi), \\quad r_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi).\n$$\nUse the mean-squared residual loss with a boundary weight $\\lambda_{\\text{bc}}$:\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right).\n$$\nTrain the parameters $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$ by gradient-based optimization from random initialization. To quantitatively assess spectral bias, at the end of a short training budget, project the learned function $u_{\\theta}(x)$ onto the two basis functions $\\sin(x)$ and $\\sin(25x)$ over a dense uniform grid on $[0, 2\\pi)$ by least squares. That is, find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ minimizing\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2,\n$$\nwith $x_m$ uniformly spaced in $[0, 2\\pi)$. Define the learned amplitudes as $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. Spectral bias is deemed present at early training if $A_1 > A_{25}$.\n\nImplement the program with a fully vectorized training loop and closed-form gradients with respect to all network parameters using only the ODE residual and boundary residuals. Do not use any external automatic differentiation library.\n\nTest Suite and Output Specification:\n- Use the following three test cases to exercise different regimes. Each case specifies $(H, N, K, \\eta)$ where $H$ is the number of hidden units, $N$ is the number of collocation points, $K$ is the number of gradient steps, and $\\eta$ is the learning rate. Use $\\lambda_{\\text{bc}} = 1$ in all cases. Angles are in radians.\n  1. Case $1$: $(H, N, K, \\eta) = (20, 128, 60, 0.01)$.\n  2. Case $2$: $(H, N, K, \\eta) = (10, 64, 80, 0.01)$.\n  3. Case $3$: $(H, N, K, \\eta) = (5, 128, 120, 0.01)$.\n- For each case, initialize parameters with a fixed seed so that results are deterministic. After training for $K$ steps, compute $A_1$ and $A_{25}$ by least squares projection over a dense grid of $M$ points with $M = 4096$. Record a boolean result for the case defined as\n$$\n\\text{result} = \\begin{cases}\n\\text{True}, & \\text{if } A_1 > A_{25},\\\\\n\\text{False}, & \\text{otherwise.}\n\\end{cases}\n$$\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,True,False]\").\n\nYour program must be self-contained, receive no input, and run as-is. Angles must be in radians. All numerical answers are dimensionless, and the final outputs are booleans. The training and projections must be implemented in pure linear algebra using the formulas above, without any external machine learning framework. The goal is to demonstrate, via these test cases, that the low-frequency component $\\sin(x)$ is learned earlier than the high-frequency component $\\sin(25x)$ by a Physics-Informed Neural Network (PINN), consistent with spectral bias.",
            "solution": "The problem presented is a valid and well-posed exercise in computational physics, specifically demonstrating the spectral bias phenomenon in Physics-Informed Neural Networks (PINNs). It is scientifically grounded, with a correctly stated differential equation and its analytical solution. All parameters and procedures are specified, permitting a unique and verifiable computational outcome. I will proceed with a solution.\n\nThe objective is to train a neural network $u_{\\theta}(x)$ to approximate the solution of the one-dimensional ordinary differential equation (ODE)\n$$\nu''(x) + u(x) = -624 \\sin(25x)\n$$\non the domain $x \\in [0, 2\\pi]$ with periodic boundary conditions $u(0) = u(2\\pi)$ and $u'(0) = u'(2\\pi)$. The analytical solution, $u(x) = \\sin(x) + \\sin(25x)$, is a superposition of a low-frequency component and a high-frequency component. We will demonstrate that gradient-based optimization of the PINN loss causes the network to learn the low-frequency component, $\\sin(x)$, faster than the high-frequency component, $\\sin(25x)$.\n\nFirst, we define the neural network ansatz, a single-hidden-layer perceptron with $H$ neurons and $\\tanh$ activation function:\n$$\nu_{\\theta}(x) = \\sum_{i=1}^{H} a_i \\tanh(w_i x + b_i) + c\n$$\nThe parameters of the network are $\\theta = \\{a_i, w_i, b_i, c\\}_{i=1}^{H}$. To enforce the ODE, we must compute the first and second derivatives of $u_{\\theta}(x)$ with respect to $x$. Using the chain rule and the identities $\\frac{d}{dz}\\tanh(z) = \\operatorname{sech}^2(z)$ and $\\frac{d}{dz}\\operatorname{sech}^2(z) = -2\\operatorname{sech}^2(z)\\tanh(z)$, we obtain:\n$$\nu'_{\\theta}(x) = \\frac{d u_{\\theta}}{dx} = \\sum_{i=1}^{H} a_i w_i \\operatorname{sech}^2(w_i x + b_i)\n$$\n$$\nu''_{\\theta}(x) = \\frac{d^2 u_{\\theta}}{dx^2} = -2 \\sum_{i=1}^{H} a_i w_i^2 \\operatorname{sech}^2(w_i x + b_i) \\tanh(w_i x + b_i)\n$$\n\nThe network is trained by minimizing a loss function composed of the mean squared error of the ODE residual and the boundary condition residuals. The physics residual at a set of $N$ collocation points $\\{x_n\\}$ is:\n$$\nr_{\\text{phys}}(x_n;\\theta) = u_{\\theta}''(x_n) + u_{\\theta}(x_n) + 624 \\sin(25 x_n)\n$$\nThe periodic boundary condition residuals are:\n$$\nr_{\\text{bc},1}(\\theta) = u_{\\theta}(0) - u_{\\theta}(2\\pi)\n$$\n$$\nr_{\\text{bc},2}(\\theta) = u_{\\theta}'(0) - u_{\\theta}'(2\\pi)\n$$\nThe total loss function is a weighted sum:\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{phys}} + \\lambda_{\\text{bc}} \\mathcal{L}_{\\text{bc}} = \\frac{1}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n;\\theta)^2 + \\lambda_{\\text{bc}}\\left(r_{\\text{bc},1}(\\theta)^2 + r_{\\text{bc},2}(\\theta)^2\\right)\n$$\nwhere $\\lambda_{\\text{bc}}$ is a hyperparameter to balance the terms, given as $\\lambda_{\\text{bc}} = 1$.\n\nTraining is performed using gradient descent. The parameters are updated according to $\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)$, where $\\eta$ is the learning rate. We must derive the analytical gradients $\\nabla_{\\theta} \\mathcal{L}(\\theta)$. The gradient of the loss with respect to any parameter $p \\in \\theta$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{2}{N}\\sum_{n=1}^{N} r_{\\text{phys}}(x_n) \\left(\\frac{\\partial u''_{\\theta}(x_n)}{\\partial p} + \\frac{\\partial u_{\\theta}(x_n)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},1}\\left(\\frac{\\partial u_{\\theta}(0)}{\\partial p} - \\frac{\\partial u_{\\theta}(2\\pi)}{\\partial p}\\right) + 2\\lambda_{\\text{bc}}r_{\\text{bc},2}\\left(\\frac{\\partial u'_{\\theta}(0)}{\\partial p} - \\frac{\\partial u'_{\\theta}(2\\pi)}{\\partial p}\\right)\n$$\nThe derivatives of the network output and its spatial derivatives with respect to the parameters $\\{a_k, w_k, b_k, c\\}$ are computed via the chain rule. These derivations are tedious but systematic and are implemented in vectorized form for computational efficiency. For example, the gradient with respect to an output weight $a_k$ involves terms like $\\frac{\\partial u_{\\theta}(x)}{\\partial a_k} = \\tanh(w_k x + b_k)$. Complete expressions for all gradients are implemented in the code.\n\nAfter training for a specified number of steps, we quantify the learned frequency components. We evaluate the trained network $u_{\\theta}(x)$ over a dense grid of $M$ points $\\{x_m\\}$ in $[0, 2\\pi)$. We then project this learned function onto the basis functions $\\sin(x)$ and $\\sin(25x)$ by solving a linear least-squares problem to find coefficients $(\\hat{\\alpha}_1, \\hat{\\alpha}_{25})$ that minimize:\n$$\n\\sum_{m=1}^{M}\\left(u_{\\theta}(x_m) - \\hat{\\alpha}_1 \\sin(x_m) - \\hat{\\alpha}_{25}\\sin(25 x_m)\\right)^2\n$$\nThe solution to this problem is given by $\\hat{\\boldsymbol{\\alpha}} = (\\mathbf{B}^T\\mathbf{B})^{-1}\\mathbf{B}^T\\mathbf{y}$, where $\\mathbf{y}$ is the vector of network predictions $u_{\\theta}(x_m)$ and $\\mathbf{B}$ is the design matrix with columns $\\sin(x_m)$ and $\\sin(25x_m)$. The learned amplitudes are $A_1 = |\\hat{\\alpha}_1|$ and $A_{25} = |\\hat{\\alpha}_{25}|$. We conclude that spectral bias is observed if $A_1 > A_{25}$.\n\nThe implementation will follow these principles, using `numpy` for vectorized numerical computation, including a fully analytical gradient calculation and a standard gradient descent loop. Parameter initialization will use a fixed random seed and Glorot/Xavier scaling for reproducibility and stable training.",
            "answer": "```python\nimport numpy as np\n\nclass PINN:\n    \"\"\"\n    A Physics-Informed Neural Network to demonstrate spectral bias.\n    The implementation is fully vectorized and uses analytical gradients.\n    \"\"\"\n    def __init__(self, H, N, seed):\n        \"\"\"\n        Initializes the PINN.\n        H: number of hidden units\n        N: number of collocation points\n        seed: random seed for parameter initialization\n        \"\"\"\n        self.H = H\n        self.N = N\n        self.lambda_bc = 1.0\n        self.rng = np.random.default_rng(seed)\n\n        # Xavier/Glorot initialization\n        # For weights w, n_in=1, n_out=1 (conceptual). limit = sqrt(6 / (1+1)) = sqrt(3)\n        limit_w = np.sqrt(3.0)\n        self.w = self.rng.uniform(-limit_w, limit_w, size=(1, self.H))\n        \n        # For weights a, n_in=H, n_out=1. limit = sqrt(6 / (H+1))\n        limit_a = np.sqrt(6.0 / (self.H + 1.0))\n        self.a = self.rng.uniform(-limit_a, limit_a, size=(self.H, 1))\n\n        self.b = np.zeros((1, self.H))\n        self.c = np.zeros((1, 1))\n\n        self.x_colloc = np.linspace(0, 2 * np.pi, self.N, endpoint=False).reshape(-1, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Computes the network output u and its derivatives u', u'' w.r.t. x.\n        x: input points, shape (num_points, 1)\n        \"\"\"\n        z = x @ self.w + self.b\n        h = np.tanh(z)\n        s = 1.0 - h**2  # sech^2(z)\n\n        u = h @ self.a + self.c\n        u_prime = (s * self.w) @ self.a\n        u_double_prime = (-2.0 * s * h * (self.w**2)) @ self.a\n\n        return u, u_prime, u_double_prime, h, s\n\n    def _compute_gradients(self):\n        \"\"\"\n        Computes the loss and the gradients of the loss w.r.t. all parameters.\n        All calculations are vectorized.\n        \"\"\"\n        # --- Physics Loss and Gradients ---\n        u, _, u_pp, H_c, S_c = self.forward(self.x_colloc)\n        \n        f_term = -624.0 * np.sin(25.0 * self.x_colloc)\n        r_phys = u_pp + u - f_term\n        loss_phys = np.mean(r_phys**2)\n\n        # Common factor for physics gradients\n        grad_common_phys = (2.0 / self.N) * r_phys\n        \n        # Gradient w.r.t. a\n        d_u_da = H_c\n        d_u_pp_da = -2.0 * S_c * H_c * self.w**2\n        grad_a_phys = (d_u_da + d_u_pp_da).T @ grad_common_phys\n\n        # Gradient w.r.t. b\n        d_u_db = self.a.T * S_c\n        d_u_pp_db = self.a.T * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        grad_b_phys = np.sum(grad_common_phys * (d_u_db + d_u_pp_db), axis=0)\n\n        # Gradient w.r.t. w\n        d_u_dw = self.a.T * self.x_colloc * S_c\n        d_u_pp_dw = self.a.T * (\n            -4.0 * self.w * S_c * H_c + \n            self.x_colloc * (4.0 * self.w**2 * S_c * H_c**2 - 2.0 * self.w**2 * S_c**2)\n        )\n        grad_w_phys = np.sum(grad_common_phys * (d_u_dw + d_u_pp_dw), axis=0)\n        \n        # Gradient w.r.t. c\n        grad_c_phys = np.sum(grad_common_phys)\n\n        # --- Boundary Loss and Gradients ---\n        x_bc = np.array([[0.0], [2 * np.pi]])\n        u_bc, u_p_bc, _, H_bc, S_bc = self.forward(x_bc)\n        \n        u0, u2pi = u_bc[0], u_bc[1]\n        u0_p, u2pi_p = u_p_bc[0], u_p_bc[1]\n\n        r_bc1 = u0 - u2pi\n        r_bc2 = u0_p - u2pi_p\n        loss_bc = r_bc1**2 + r_bc2**2\n        \n        H0, H2pi = H_bc[0:1, :], H_bc[1:2, :]\n        S0, S2pi = S_bc[0:1, :], S_bc[1:2, :]\n        \n        # Common factors for BC gradients\n        common1 = 2.0 * self.lambda_bc * r_bc1\n        common2 = 2.0 * self.lambda_bc * r_bc2\n\n        # Gradient w.r.t. a\n        delta_u_da = (H0 - H2pi).T\n        delta_u_p_da = (self.w * (S0 - S2pi)).T\n        grad_a_bc = common1 * delta_u_da + common2 * delta_u_p_da\n        \n        # Gradient w.r.t. b\n        delta_u_db = self.a.T * (S0 - S2pi)\n        delta_u_p_db = -2.0 * self.a.T * self.w * (S0 * H0 - S2pi * H2pi)\n        grad_b_bc = common1 * delta_u_db + common2 * delta_u_p_db\n\n        # Gradient w.r.t. w\n        delta_u_dw = -2.0 * np.pi * self.a.T * S2pi\n        delta_u_p_dw = self.a.T * (S0 - S2pi) + 4.0 * np.pi * self.a.T * self.w * S2pi * H2pi\n        grad_w_bc = common1 * delta_u_dw + common2 * delta_u_p_dw\n\n        # Gradient w.r.t. c (is zero)\n        grad_c_bc = 0.0\n\n        # --- Total Loss and Gradients ---\n        loss = loss_phys + self.lambda_bc * loss_bc\n        grad_a = grad_a_phys + grad_a_bc\n        grad_w = grad_w_phys.reshape(1, -1) + grad_w_bc\n        grad_b = grad_b_phys.reshape(1, -1) + grad_b_bc\n        grad_c = grad_c_phys + grad_c_bc\n\n        return loss, grad_a, grad_w, grad_b, grad_c\n\n    def train(self, K, eta):\n        \"\"\"\n        Trains the network using gradient descent.\n        K: number of training steps\n        eta: learning rate\n        \"\"\"\n        for _ in range(K):\n            loss, grad_a, grad_w, grad_b, grad_c = self._compute_gradients()\n            \n            self.a -= eta * grad_a\n            self.w -= eta * grad_w\n            self.b -= eta * grad_b\n            self.c -= eta * grad_c\n\n    def project_and_analyze(self):\n        \"\"\"\n        Projects the learned function onto sin(x) and sin(25x) and checks for spectral bias.\n        \"\"\"\n        M = 4096\n        x_dense = np.linspace(0, 2 * np.pi, M, endpoint=False).reshape(-1, 1)\n        \n        u_pred, _, _, _, _ = self.forward(x_dense)\n        u_pred = u_pred.flatten()\n        \n        # Create design matrix for least squares\n        B = np.zeros((M, 2))\n        B[:, 0] = np.sin(x_dense.flatten())\n        B[:, 1] = np.sin(25.0 * x_dense.flatten())\n        \n        # Solve least squares problem: B * alpha = u_pred\n        alpha, _, _, _ = np.linalg.lstsq(B, u_pred, rcond=None)\n        \n        A1 = np.abs(alpha[0])\n        A25 = np.abs(alpha[1])\n        \n        return A1 > A25\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        (20, 128, 60, 0.01),  # Case 1: (H, N, K, eta)\n        (10, 64, 80, 0.01),   # Case 2\n        (5, 128, 120, 0.01),  # Case 3\n    ]\n\n    results = []\n    base_seed = 42\n\n    for i, (H, N, K, eta) in enumerate(test_cases):\n        seed = base_seed + i\n        pinn = PINN(H=H, N=N, seed=seed)\n        pinn.train(K=K, eta=eta)\n        result = pinn.project_and_analyze()\n        results.append(result)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}