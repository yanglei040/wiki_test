{
    "hands_on_practices": [
        {
            "introduction": "物理信息神经网络（PINN）的基石在于其损失函数，它将一个物理问题——包括其控制方程、初始状态和边界限制——转化为神经网络可以理解和优化的数学“语言”。第一个练习将引导你为一个具有挑战性且重要的物理现象，即由非线性的无粘性伯格斯方程（inviscid Burgers' equation）描述的冲击波的形成，构建一个完整的损失函数。通过将来自控制偏微分方程（PDE）、初始条件和边界条件的残差整合在一起，你将学会如何构建一个驱动PINN发现物理可行解的核心目标函数 。",
            "id": "2126315",
            "problem": "物理信息神经网络 (PINN) 是一种神经网络，其训练目标是在遵守由一般非线性偏微分方程 (PDE) 描述的给定物理定律的同时，解决监督学习任务。我们的目标是为一个旨在模拟冲击波形成的 PINN 构建训练目标，即损失函数。\n\n该物理系统由一维、瞬态、无粘性伯格斯方程控制：\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0\n$$\n解 $u(t, x)$ 在由 $x \\in [-1, 1]$ 和 $t \\in [0, T]$（对于某个最终时间 $T > 0$）定义的时空域内求解。\n\n系统从一个光滑的初始条件演化而来，该条件由下式给出：\n$$\nu(0, x) = -\\sin(\\pi x) \\quad \\text{for } x \\in [-1, 1]\n$$\n该系统满足周期性边界条件：\n$$\nu(t, -1) = u(t, 1) \\quad \\text{for } t \\in [0, T]\n$$\n\n一个表示为 $\\hat{u}(t, x; \\theta)$ 的神经网络被用来近似真实解 $u(t, x)$。这里，$\\theta$ 代表网络中所有可训练的参数（权重和偏置）。训练过程包括最小化一个损失函数 $\\mathcal{L}(\\theta)$，该函数在一系列离散的采样点集上进行评估：\n1.  **初始条件点**：一个集合 $S_{IC} = \\{(0, x_i)\\}_{i=1}^{N_{IC}}$，包含沿初始时间切片的 $N_{IC}$ 个点。\n2.  **边界条件点**：一个集合 $S_{BC} = \\{(t_j, -1), (t_j, 1)\\}_{j=1}^{N_{BC}}$，包含在不同时间点上位于空间边界上的 $N_{BC}$ 对点。\n3.  **配置点**：一个集合 $S_{PDE} = \\{(t_k, x_k)\\}_{k=1}^{N_{PDE}}$，包含分布在域内部 $(t,x) \\in (0, T] \\times (-1, 1)$ 的 $N_{PDE}$ 个点。\n\n您的任务是构建总损失函数 $\\mathcal{L}(\\theta)$。该损失函数应被表述为对应于初始条件、边界条件和偏微分方程残差的均方误差之和。假设这三个损失分量的所有权重因子都等于1。请给出 $\\mathcal{L}(\\theta)$ 的最终表达式。",
            "solution": "我们使用神经网络 $\\hat{u}(t,x;\\theta)$ 来近似无粘性伯格斯方程 $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0$ 的解 $u(t,x)$。物理定律通过将 $\\hat{u}$ 代入偏微分方程得到的残差驱动为零来强制执行，而数据约束则通过匹配初始和边界条件来强制执行。\n\n在任意点 $(t,x)$ 的偏微分方程残差定义为将 $\\hat{u}$ 代入控制方程得到：\n$$\nr_{\\theta}(t,x) \\equiv \\frac{\\partial \\hat{u}}{\\partial t}(t,x;\\theta) + \\hat{u}(t,x;\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t,x;\\theta).\n$$\n在配置点集 $S_{PDE} = \\{(t_{k},x_{k})\\}_{k=1}^{N_{PDE}}$ 上，均方残差为\n$$\n\\mathcal{L}_{PDE}(\\theta) = \\frac{1}{N_{PDE}} \\sum_{k=1}^{N_{PDE}} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta) + \\hat{u}(t_{k},x_{k};\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta) \\right)^{2}.\n$$\n\n初始条件为 $u(0,x) = -\\sin(\\pi x)$。在初始条件点集 $S_{IC} = \\{(0,x_{i})\\}_{i=1}^{N_{IC}}$ 上，均方初始条件误差为\n$$\n\\mathcal{L}_{IC}(\\theta) = \\frac{1}{N_{IC}} \\sum_{i=1}^{N_{IC}} \\left( \\hat{u}(0,x_{i};\\theta) + \\sin(\\pi x_{i}) \\right)^{2}.\n$$\n\n周期性边界条件为 $u(t,-1) = u(t,1)$。在边界点集 $S_{BC} = \\{(t_{j},-1),(t_{j},1)\\}_{j=1}^{N_{BC}}$ 上，均方边界条件误差为\n$$\n\\mathcal{L}_{BC}(\\theta) = \\frac{1}{N_{BC}} \\sum_{j=1}^{N_{BC}} \\left( \\hat{u}(t_{j},-1;\\theta) - \\hat{u}(t_{j},1;\\theta) \\right)^{2}.\n$$\n\n所有分量的权重均为1时，总损失函数为各项之和\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_{IC}(\\theta) + \\mathcal{L}_{BC}(\\theta) + \\mathcal{L}_{PDE}(\\theta).\n$$\n代入各分量的定义，得到显式表达式\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N_{IC}} \\sum_{i=1}^{N_{IC}} \\left( \\hat{u}(0,x_{i};\\theta) + \\sin(\\pi x_{i}) \\right)^{2} + \\frac{1}{N_{BC}} \\sum_{j=1}^{N_{BC}} \\left( \\hat{u}(t_{j},-1;\\theta) - \\hat{u}(t_{j},1;\\theta) \\right)^{2} + \\frac{1}{N_{PDE}} \\sum_{k=1}^{N_{PDE}} \\left( \\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta) + \\hat{u}(t_{k},x_{k};\\theta)\\,\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta) \\right)^{2}.\n$$",
            "answer": "$$\\boxed{\\mathcal{L}(\\theta)=\\frac{1}{N_{IC}}\\sum_{i=1}^{N_{IC}}\\left(\\hat{u}(0,x_{i};\\theta)+\\sin(\\pi x_{i})\\right)^{2}+\\frac{1}{N_{BC}}\\sum_{j=1}^{N_{BC}}\\left(\\hat{u}(t_{j},-1;\\theta)-\\hat{u}(t_{j},1;\\theta)\\right)^{2}+\\frac{1}{N_{PDE}}\\sum_{k=1}^{N_{PDE}}\\left(\\frac{\\partial \\hat{u}}{\\partial t}(t_{k},x_{k};\\theta)+\\hat{u}(t_{k},x_{k};\\theta)\\frac{\\partial \\hat{u}}{\\partial x}(t_{k},x_{k};\\theta)\\right)^{2}}$$"
        },
        {
            "introduction": "虽然将边界误差作为惩罚项加入损失函数是一种通用策略（即“软约束”），但这并非唯一的方法。本练习将介绍一种更巧妙且通常更稳健的技术：通过特殊设计网络输出的数学形式，直接在网络架构中“硬性”施加边界条件。你将设计一个变换，它作用于神经网络的原始输出，确保最终解无论网络的可训练参数如何变化，都能精确满足给定的边界值。掌握这种“硬约束”方法可以为解决某些问题提供更高效、更精确的途径 。",
            "id": "2126300",
            "problem": "在科学计算领域，物理信息神经网络（PINNs）已成为求解微分方程的强大工具。设计PINN的一个关键方面是确保其输出（即解的近似）满足给定的边界条件。一种实现此目标的鲁棒方法是构造网络的最终输出函数，使其通过构造就能满足这些条件。\n\n考虑一个在一维空间域 $x \\in [0, L]$ 上的问题。一个神经网络提供了一个原始的、无约束的输出函数，记为 $\\hat{u}_{NN}(x)$。我们希望使用这个网络来为一个微分方程找到一个近似解 $u(x)$，该解满足以下非齐次狄利克雷边界条件：\n$$u(0) = A$$\n$$u(L) = B$$\n这里，$A$、$B$ 和 $L > 0$ 是给定的实常数。\n\n您的任务是设计一个变换，它接收原始网络输出 $\\hat{u}_{NN}(x)$ 并生成一个新函数 $u_{NN}(x)$，作为最终的近似解。此变换必须保证，无论网络生成的函数 $\\hat{u}_{NN}(x)$ 是什么， $u_{NN}(x)$ 都严格满足指定的边界条件。\n\n请提供 $u_{NN}(x)$ 的表达式，用原始网络输出 $\\hat{u}_{NN}(x)$ 和参数 $x$、$L$、$A$、$B$ 来表示。",
            "solution": "我们寻求一个变换，将原始网络输出 $\\hat{u}_{NN}(x)$ 映射到一个函数 $u_{NN}(x)$，该函数对任意的 $\\hat{u}_{NN}(x)$ 都强制满足狄利克雷边界条件 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。一个标准的构造方法是将 $u_{NN}(x)$ 分解为\n$$\nu_{NN}(x)=g(x)+s(x)\\,\\hat{u}_{NN}(x),\n$$\n其中 $g(x)$ 是任何满足边界条件的固定函数，而 $s(x)$ 是任何在两个边界上都为零的函数。具体来说，我们要求\n$$\ng(0)=A,\\quad g(L)=B,\\quad s(0)=0,\\quad s(L)=0.\n$$\n一个方便的选择是，对于 $g(x)$ 使用线性插值函数，\n$$\ng(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)=A+\\frac{B-A}{L}\\,x,\n$$\n对于 $s(x)$ 使用简单的零化因子\n$$\ns(x)=x(L-x),\n$$\n它满足 $s(0)=0$ 和 $s(L)=0$。因此，定义\n$$\nu_{NN}(x)=A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x(L-x)\\,\\hat{u}_{NN}(x).\n$$\n为验证边界条件，在 $x=0$ 和 $x=L$ 处求值：\n$$\nu_{NN}(0)=A\\left(1-0\\right)+B\\left(0\\right)+0\\cdot L\\,\\hat{u}_{NN}(0)=A,\n$$\n$$\nu_{NN}(L)=A\\left(1-1\\right)+B\\left(\\frac{L}{L}\\right)+L( L-L)\\,\\hat{u}_{NN}(L)=B.\n$$\n因此，对于任意的 $\\hat{u}_{NN}(x)$，构造出的 $u_{NN}(x)$ 都严格满足 $u_{NN}(0)=A$ 和 $u_{NN}(L)=B$。",
            "answer": "$$\\boxed{A\\left(1-\\frac{x}{L}\\right)+B\\left(\\frac{x}{L}\\right)+x\\left(L-x\\right)\\hat{u}_{NN}(x)}$$"
        }
    ]
}