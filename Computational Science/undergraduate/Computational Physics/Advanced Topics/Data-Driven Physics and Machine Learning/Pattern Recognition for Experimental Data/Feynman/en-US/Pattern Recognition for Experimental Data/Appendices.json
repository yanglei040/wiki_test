{
    "hands_on_practices": [
        {
            "introduction": "Pattern recognition in experimental data often begins with searching for a specific, well-defined signature. This first exercise provides a concrete example from particle physics, where you will search for a particular Feynman diagram topology within simulated collision events . By translating a formal graph-theoretic definition into a computational algorithm, you will practice the essential skill of implementing precise, multi-conditional checks to isolate signals of interest from a complex dataset.",
            "id": "2425431",
            "problem": "You are given a simplified, graph-theoretic formulation of recognizing a specific Feynman diagram topology within simulated scattering events. Each simulated event is represented as a finite, simple, undirected, node-labeled graph. The graph is specified by a symmetric adjacency matrix $\\mathbf{A} \\in \\{0,1\\}^{n \\times n}$ with zero diagonal and a label vector $\\mathbf{L}$ of length $n$, where $\\mathbf{L}[i]$ is the label of node $i$. Labels are drawn from the finite set $\\{\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-, \\gamma\\}$; in the program representation these are the ASCII strings `'Z'`, `'e+'`, `'e-'`, `'mu+'`, `'mu-'`, and `'gamma'`.\n\nThe target topology is a specific induced subgraph that abstracts an $\\mathrm{s}$-channel resonance: a star graph on $5$ vertices (isomorphic to $K_{1,4}$), with a central node labeled $\\mathrm{Z}$ connected to exactly four leaves, and the four leaves labeled exactly once each as $\\mathrm{e^+}$, $\\mathrm{e^-}$, $\\mu^+$, and $\\mu^-$. There are no edges among the leaves in this induced subgraph. Formally, an occurrence is an induced subgraph on a node set $\\{c, \\ell_1, \\ell_2, \\ell_3, \\ell_4\\}$ such that:\n- $\\mathbf{L}[c] = \\mathrm{Z}$,\n- $\\deg(c) = 4$ within the full graph, and its neighbor set is exactly $\\{\\ell_1, \\ell_2, \\ell_3, \\ell_4\\}$,\n- $\\{\\mathbf{L}[\\ell_1], \\mathbf{L}[\\ell_2], \\mathbf{L}[\\ell_3], \\mathbf{L}[\\ell_4]\\}$ equals $\\{\\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-\\}$ as a multiset,\n- for all distinct $i,j$ in $\\{1,2,3,4\\}$, $\\mathbf{A}[\\ell_i,\\ell_j] = 0$.\n\nYour task is to write a complete program that:\n- Counts, for each event, the number of distinct centers $c$ that realize such an induced subgraph. Count each qualifying center once, regardless of the ordering of its neighbors.\n\nFundamental base you may rely on:\n- A simple, undirected graph is represented by a symmetric $\\{0,1\\}$ adjacency matrix with zero diagonal.\n- The degree $\\deg(i)$ of node $i$ equals $\\sum_{j=1}^{n} \\mathbf{A}[i,j]$.\n- An induced subgraph on node set $S$ contains exactly the edges between nodes in $S$ that appear in the original graph.\n\nTest suite. Use the following $6$ events, each specified by its adjacency matrix $\\mathbf{A}^{(k)}$ and label vector $\\mathbf{L}^{(k)}$.\n\nEvent $1$ ($n=5$). A single valid $\\mathrm{Z}$-centered star.\n$$\n\\mathbf{A}^{(1)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(1)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\nEvent $2$ ($n=10$). Two disjoint valid stars.\n$$\n\\mathbf{A}^{(2)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(2)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-,\n\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\nEvent $3$ ($n=5$). A $\\mathrm{Z}$-centered star with an incorrect leaf multiset (duplicate $\\mathrm{e^+}$ and missing $\\mu^+$), which should not count.\n$$\n\\mathbf{A}^{(3)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(3)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mathrm{e^+}, \\mu^-).\n$$\n\nEvent $4$ ($n=6$). A $\\mathrm{Z}$ node of degree $5$ connected to four correct leaves and an extra $\\gamma$ neighbor; degree constraint violated, should not count.\n$$\n\\mathbf{A}^{(4)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(4)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-, \\gamma).\n$$\n\nEvent $5$ ($n=5$). A $\\mathrm{Z}$-centered star with an extra edge among leaves (breaking the induced-star condition), which should not count.\n$$\n\\mathbf{A}^{(5)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 1 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(5)}=(\\mathrm{Z}, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\nEvent $6$ ($n=5$). A $\\gamma$-centered star with the correct leaf multiset; center label constraint violated, should not count.\n$$\n\\mathbf{A}^{(6)}=\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\\\\\n1 & 0 & 0 & 0 & 0\n\\end{bmatrix},\\quad\n\\mathbf{L}^{(6)}=(\\gamma, \\mathrm{e^+}, \\mathrm{e^-}, \\mu^+, \\mu^-).\n$$\n\nYour program must implement the recognition rule implied by the induced-subgraph definition and compute, for each event $k \\in \\{1,2,3,4,5,6\\}$, the integer count $c^{(k)}$ of distinct centers realizing the target topology. The final output must be a single line containing all six counts as a comma-separated list enclosed in square brackets, for example, the format should be like `[result1,result2,result3,result4,result5,result6]`.\n\nNo physical units or angle units are required in this problem. All answers are integers.",
            "solution": "The problem is valid. It is a well-defined task in computational pattern recognition on graphs, grounded in a conventional abstraction used in experimental physics data analysis. The conditions are precise, complete, and logically consistent.\n\nThe task is to count, for each of $6$ given graph-based events, the number of nodes that can serve as the center of a specific induced subgraph topology. This topology models an s-channel resonance involving a `'Z'` boson decaying into four specific leptons. Let a given event be represented by a symmetric adjacency matrix $\\mathbf{A}$ and a node-label vector $\\mathbf{L}$ for a graph with $n$ vertices. The algorithm must identify and count the number of distinct nodes $c$ that satisfy a strict set of criteria.\n\nThe core of the solution is an algorithm that iterates through every node $i$ (where $i \\in \\{0, 1, \\dots, n-1\\}$) of a given graph, treating each as a potential center $c$. For each candidate node $i$, a sequence of validation checks is performed. If any check fails, the node is disqualified, and the algorithm proceeds to the next candidate. A counter is incremented only for nodes that pass all checks.\n\nThe validation steps for a candidate center node $i$ are as follows:\n\n$1$. **Center Label Verification**: The first condition concerns the identity of the central particle. The label of the candidate node, $\\mathbf{L}[i]$, must be `'Z'`. If $\\mathbf{L}[i] \\neq `'Z'$, node $i$ is not a valid center, and no further checks are needed for this node.\n\n$2$. **Degree Verification**: The problem specifies a star graph topology with exactly four leaves, which implies the central node must have a degree of exactly $4$. The degree of node $i$ is calculated as $\\deg(i) = \\sum_{j=0}^{n-1} \\mathbf{A}[i,j]$. If $\\deg(i) \\neq 4$, the node is disqualified.\n\n$3$. **Neighbor Identification**: If the degree of node $i$ is $4$, its four neighbors are identified. Let the set of neighbor indices be $N(i) = \\{\\ell_1, \\ell_2, \\ell_3, \\ell_4\\}$, which are the indices $j$ such that $\\mathbf{A}[i,j] = 1$.\n\n$4$. **Leaf Label Multiset Verification**: The four neighbors must correspond to the specific decay products. The multiset of labels of the neighbors, $\\{\\mathbf{L}[\\ell_1], \\mathbf{L}[\\ell_2], \\mathbf{L}[\\ell_3], \\mathbf{L}[\\ell_4]\\}$, must be equal to the target multiset `{'e+', 'e-', 'mu+', 'mu-'}`. An efficient way to check this is to sort the list of collected neighbor labels and compare it to a pre-sorted list of the target labels. If they are not identical, the condition is not met.\n\n$5$. **Induced Subgraph Verification**: The specified topology is an *induced* subgraph, meaning there must be no edges connecting the leaves to each other. This is a critical constraint. For all distinct pairs of neighbors $\\ell_j, \\ell_k \\in N(i)$, the entry in the adjacency matrix must be zero: $\\mathbf{A}[\\ell_j, \\ell_k] = 0$. This check is performed for all $\\binom{4}{2} = 6$ pairs of neighbors. If any edge is found (i.e., $\\mathbf{A}[\\ell_j, \\ell_k] = 1$ for any pair), node $i$ is disqualified.\n\nA node $i$ is counted as a valid center if and only if it successfully passes all five of these consecutive checks. The total count for an event is the sum of all such valid centers found in its corresponding graph. This entire process is then repeated for all $6$ test events specified in the problem statement.\n\nFor example, in Event $1$, node $0$ is evaluated. Its label is `'Z'` (pass), its degree is $4$ (pass), its neighbors are nodes $\\{1, 2, 3, 4\\}$, and their labels `{'e+', 'e-', 'mu+', 'mu-'}` form the correct multiset (pass). Finally, the submatrix of $\\mathbf{A}$ corresponding to these neighbors is a zero matrix, so no edges exist between them (pass). Thus, node $0$ is a valid center, and the count for Event $1$ is $1$. Conversely, Event $4$ fails the degree check, Event $3$ fails the leaf label check, and Event $5$ fails the induced subgraph check, resulting in counts of $0$ for each.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the graph pattern recognition problem by counting valid Z-boson\n    decay topologies in a series of simulated events.\n    \"\"\"\n    \n    # Define the 6 test cases from the problem statement.\n    test_cases = [\n        # Event 1: n=5, a single valid Z-centered star.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-']\n        ),\n        # Event 2: n=10, two disjoint valid stars.\n        (\n            np.array([\n                [0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-', 'Z', 'e+', 'e-', 'mu+', 'mu-']\n        ),\n        # Event 3: n=5, incorrect leaf multiset.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'e+', 'mu-']\n        ),\n        # Event 4: n=6, center degree is 5.\n        (\n            np.array([\n                [0, 1, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-', 'gamma']\n        ),\n        # Event 5: n=5, extra edge between leaves.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 1, 0],\n                [1, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['Z', 'e+', 'e-', 'mu+', 'mu-']\n        ),\n        # Event 6: n=5, incorrect center label.\n        (\n            np.array([\n                [0, 1, 1, 1, 1],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0]\n            ]),\n            ['gamma', 'e+', 'e-', 'mu+', 'mu-']\n        )\n    ]\n\n    results = []\n    target_leaf_labels = sorted(['e+', 'e-', 'mu+', 'mu-'])\n\n    for A, L in test_cases:\n        num_nodes = A.shape[0]\n        valid_centers_count = 0\n\n        for i in range(num_nodes):\n            # Let node i be the potential center c.\n            \n            # 1. Check center label: Must be 'Z'.\n            if L[i] != 'Z':\n                continue\n\n            # 2. Check center degree: Must be 4.\n            degree = np.sum(A[i, :])\n            if degree != 4:\n                continue\n\n            # 3. Identify neighbors and check their labels.\n            neighbors = np.where(A[i, :] == 1)[0]\n            \n            leaf_labels = sorted([L[j] for j in neighbors])\n            if leaf_labels != target_leaf_labels:\n                continue\n            \n            # 4. Check induced subgraph condition: No edges between leaves.\n            has_leaf_edge = False\n            for j1_idx in range(4):\n                for j2_idx in range(j1_idx + 1, 4):\n                    n1 = neighbors[j1_idx]\n                    n2 = neighbors[j2_idx]\n                    if A[n1, n2] == 1:\n                        has_leaf_edge = True\n                        break\n                if has_leaf_edge:\n                    break\n            \n            if has_leaf_edge:\n                continue\n\n            # If all checks pass, this is a valid center.\n            valid_centers_count += 1\n            \n        results.append(valid_centers_count)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond matching local templates, pattern recognition also involves identifying large-scale, emergent properties in physical systems. This practice uses the classic model of a forest fire to explore the concept of percolation, a fundamental phenomenon in statistical physics . You will combine Monte Carlo simulation with image analysis techniques to determine the critical threshold at which a spanning 'fire' becomes possible, providing a hands-on introduction to phase transitions and the computational methods used to study them.",
            "id": "2425393",
            "problem": "You will implement a reproducible Monte Carlo (MC) estimator for the critical vegetation density in a forest-fire percolation model by treating each simulated lattice as a binary image and using connected-component labeling to assess whether fire can span the system. The forest is an $N \\times N$ square lattice. Each lattice site is vegetated independently with probability $p$ and empty with probability $1-p$. Fire is assumed to spread deterministically through adjacent vegetated sites via nearest-neighbor contact (four-neighbor von Neumann connectivity). The question of whether a fire can traverse from one side of the forest to the other is equivalent to whether the binary image of vegetated sites contains a connected component that touches both the left and right image boundaries. The critical density $p_c$ is defined for a fixed lattice size $N$ as the value where the left-right spanning probability is approximately $1/2$.\n\nFundamental base:\n- The occupancy of each site is an independent Bernoulli random variable with success probability $p$.\n- A connected component in a binary image is a maximal set of sites where each is reachable from another via a path of nearest-neighbor (four-neighbor) adjacencies.\n- The spanning event is defined as the existence of at least one connected component that intersects both the leftmost and rightmost columns of the lattice.\n- The spanning probability for a finite $N$ at a given $p$ is the expectation of an indicator random variable that is $1$ if the sampled lattice spans and $0$ otherwise. The MC estimator is the sample mean of the indicator over independent trials, which converges to the true probability by the law of large numbers.\n\nTasks to implement:\n1. For a given $N$, $p$, and integer seed $s$, generate $T$ independent binary images by thresholding independent and identically distributed uniform random numbers on $[0,1)$, treating a value less than $p$ as a vegetated site ($1$) and otherwise empty ($0$). Each image is an $N \\times N$ array.\n2. For each image, use connected-component labeling under four-neighbor connectivity to label clusters of vegetated sites. Declare the image as spanning if and only if there exists a labeled component that touches both the left $(x=0)$ and right $(x=N-1)$ boundaries. Compute the MC estimate $\\hat{\\pi}_N(p)$ as the fraction of images that span.\n3. For a fixed $N$, estimate $p_c(N)$ using bisection on $p$ within a bracket $[p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$ such that $\\hat{\\pi}_N(p_{\\mathrm{lo}}) < 1/2$ and $\\hat{\\pi}_N(p_{\\mathrm{hi}}) \\ge 1/2$. At each bisection step evaluate $\\hat{\\pi}_N(p)$ using the same seed $s$ and number of trials $T$, and update the bracket depending on whether $\\hat{\\pi}_N(p) \\ge 1/2$. Terminate when the bracket width is less than a specified tolerance $\\varepsilon$, and report the midpoint as the estimate $p_c(N)$.\n4. To ensure reproducibility and monotonicity of the MC estimate in $p$ during bisection, use the same base seed $s$ for all calls to the spanning probability at different $p$ for a given $N$, so that each trial uses the same uniform random numbers and only the threshold changes.\n\nAngle units are not involved. No physical units are involved. All probabilities must be reported as decimals, not percentages.\n\nTest suite and required outputs:\n- Test case $1$: Estimate $p_c$ for $N=32$ using $T=64$, $p_{\\mathrm{lo}}=0.3$, $p_{\\mathrm{hi}}=0.8$, $\\varepsilon=10^{-3}$, and seed $s=12345$. Output a single float rounded to $4$ decimal places.\n- Test case $2$: Estimate $p_c$ for $N=64$ using $T=64$, $p_{\\mathrm{lo}}=0.3$, $p_{\\mathrm{hi}}=0.8$, $\\varepsilon=10^{-3}$, and seed $s=24680$. Output a single float rounded to $4$ decimal places.\n- Test case $3$: Compute the spanning probability $\\hat{\\pi}_N(p)$ at $p=0.2$ for $N=64$ with $T=200$ and seed $s=98765$. Output a single float rounded to $4$ decimal places.\n- Test case $4$: Compute the spanning probability $\\hat{\\pi}_N(p)$ at $p=0.8$ for $N=64$ with $T=200$ and seed $s=54321$. Output a single float rounded to $4$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $4$, each rounded to $4$ decimal places. For example, a valid output format is like `[0.5925,0.5929,0.0125,0.9975]`.",
            "solution": "The problem presented is a well-defined exercise in computational statistical physics, specifically focusing on the phenomenon of percolation. It is valid and possesses all necessary components for a rigorous solution. We are tasked with numerically estimating the critical vegetation density, $p_c$, for a finite $N \\times N$ discrete lattice, which is a foundational model for phenomena such as fluid flow in porous media, the spread of epidemics, and, as in this case, forest fires. The estimation will be performed using a Monte Carlo method, coupled with connected-component analysis and a bisection search algorithm.\n\nThe core of the problem lies in estimating the spanning probability, $\\pi_N(p)$, which is the probability that a randomly generated forest of size $N \\times N$ with vegetation density $p$ contains a continuous path of vegetated sites connecting the left boundary to the right boundary. The critical density $p_c(N)$ is defined as the density for which this probability is approximately $1/2$.\n\nLet us formalize the procedure.\n\n1.  **Lattice Generation and Monte Carlo Method**\n\nThe forest is represented as an $N \\times N$ grid $\\mathcal{L}$. Each site $(i, j)$ on this grid, where $i, j \\in \\{0, 1, \\dots, N-1\\}$, is either vegetated or empty. The state of a site is determined by a Bernoulli random variable $\\sigma_{i,j}$ with probability of success (vegetation) $p$. Thus, $P(\\sigma_{i,j}=1) = p$ and $P(\\sigma_{i,j}=0) = 1-p$. The states of all sites are independent.\n\nTo estimate the spanning probability $\\pi_N(p)$, we employ a Monte Carlo simulation. We generate $T$ independent realizations of the lattice, which we call trials. For each trial $k \\in \\{1, \\dots, T\\}$, we generate an $N \\times N$ matrix of occupation numbers. We define an indicator variable $I_k(p)$ for each trial:\n$$\nI_k(p) = \\begin{cases} 1 & \\text{if trial } k \\text{ results in a spanning configuration} \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nThe Monte Carlo estimator for the spanning probability, $\\hat{\\pi}_N(p)$, is the sample average of this indicator variable over the $T$ trials:\n$$\n\\hat{\\pi}_N(p) = \\frac{1}{T} \\sum_{k=1}^T I_k(p)\n$$\nBy the law of large numbers, this estimator converges to the true spanning probability $\\pi_N(p)$ as the number of trials $T \\to \\infty$.\n\n2.  **Spanning Cluster Detection**\n\nTo ascertain whether a given lattice configuration spans, we must identify connected clusters of vegetated sites. A cluster is a set of vegetated sites where any site in the cluster can be reached from any other site in the cluster by a path of adjacent vegetated sites. The adjacency is defined by the $4$-neighbor (von Neumann) rule: a site $(i, j)$ is adjacent to its neighbors at $(i\\pm1, j)$ and $(i, j\\pm1)$, provided they are within the lattice boundaries.\n\nThis task is algorithmically equivalent to connected-component labeling (CCL) in a binary image. We will use an established CCL algorithm, which assigns a unique integer label to each distinct cluster. The algorithm processes the binary lattice of vegetated sites (value $1$) and background (value $0$). After labeling, we have a new $N \\times N$ integer matrix where all sites belonging to the same cluster share the same positive integer label, and empty sites are labeled $0$.\n\nA spanning event occurs if there exists at least one cluster that has a presence in both the leftmost column ($j=0$) and the rightmost column ($j=N-1$). We can verify this by:\n-   Extracting the set of unique non-zero labels present in column $0$, let this be $\\mathcal{C}_{\\text{left}}$.\n-   Extracting the set of unique non-zero labels present in column $N-1$, let this be $\\mathcal{C}_{\\text{right}}$.\n-   The lattice spans if and only if the intersection of these two sets is non-empty: $\\mathcal{C}_{\\text{left}} \\cap \\mathcal{C}_{\\text{right}} \\neq \\emptyset$.\n\n3.  **Reproducibility and Monotonicity for Bisection Search**\n\nThe critical density $p_c(N)$ is defined by the condition $\\pi_N(p_c(N)) \\approx 1/2$. To find $p_c$, we must solve the equation $\\hat{\\pi}_N(p) - 1/2 = 0$ for $p$. Since $\\hat{\\pi}_N(p)$ is a step function (as it is a sum of indicators), a bisection search is an appropriate method, provided the function is monotonic.\n\nThe standard Monte Carlo estimator $\\hat{\\pi}_N(p)$ is not guaranteed to be monotonic for finite $T$ if independent random numbers are used for each evaluation at different $p$. To enforce monotonicity, we use a single set of underlying random numbers. A single large array of random numbers, $U_{k,i,j} \\sim \\mathcal{U}[0,1)$, of size $T \\times N \\times N$, is generated once using a fixed seed $s$. For any given probability threshold $p$, a lattice configuration is generated by setting $\\sigma_{k,i,j} = 1$ if $U_{k,i,j} < p$, and $\\sigma_{k,i,j} = 0$ otherwise.\n\nWith this construction, if we consider two probabilities $p_1$ and $p_2$ with $p_1 < p_2$, the set of vegetated sites corresponding to $p_1$ is a subset of the set of vegetated sites for $p_2$. Consequently, if a cluster spans for $p_1$, it must also span for $p_2$. This guarantees that our estimator $\\hat{\\pi}_N(p)$ is a non-decreasing function of $p$, which is a necessary condition for the bisection algorithm to work reliably.\n\n4.  **Bisection Algorithm for $p_c(N)$**\n\nThe bisection method proceeds as follows:\n-   An initial bracket $[p_{\\mathrm{lo}}, p_{\\mathrm{hi}}]$ is chosen such that $\\hat{\\pi}_N(p_{\\mathrm{lo}}) < 1/2$ and $\\hat{\\pi}_N(p_{\\mathrm{hi}}) \\ge 1/2$.\n-   The algorithm iteratively refines this bracket:\n    1.  Calculate the midpoint $p_{\\mathrm{mid}} = (p_{\\mathrm{lo}} + p_{\\mathrm{hi}})/2$.\n    2.  Evaluate the spanning probability $\\hat{\\pi}_N(p_{\\mathrm{mid}})$ using the fixed set of random numbers.\n    3.  If $\\hat{\\pi}_N(p_{\\mathrm{mid}}) < 1/2$, the critical point must lie in the upper half of the interval. We update the lower bound: $p_{\\mathrm{lo}} \\leftarrow p_{\\mathrm{mid}}$.\n    4.  Otherwise, if $\\hat{\\pi}_N(p_{\\mathrm{mid}}) \\ge 1/2$, the critical point is in the lower half. We update the upper bound: $p_{\\mathrm{hi}} \\leftarrow p_{\\mathrm{mid}}$.\n-   This process is repeated until the width of the bracket, $p_{\\mathrm{hi}} - p_{\\mathrm{lo}}$, is smaller than a specified tolerance $\\varepsilon$.\n-   The final estimate for $p_c(N)$ is the midpoint of the final bracket, $(p_{\\mathrm{lo}} + p_{\\mathrm{hi}})/2$.\n\nThis complete methodology allows for a reproducible and scientifically sound estimation of the critical percolation threshold for the specified finite systems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.ndimage import label\n\ndef _check_span(lattice: np.ndarray) -> bool:\n    \"\"\"\n    Checks for a left-right spanning cluster in a single binary lattice.\n    A spanning cluster is a connected component of 1s that touches both\n    the first and last columns of the lattice.\n    \"\"\"\n    if not np.any(lattice):\n        return False\n        \n    # Define 4-neighbor (von Neumann) connectivity\n    structure = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n    \n    # Perform connected-component labeling\n    labeled_lattice, num_labels = label(lattice, structure=structure)\n    \n    # If there are no vegetated clusters, it cannot span\n    if num_labels == 0:\n        return False\n\n    # Get unique labels on left and right boundaries\n    # The `_` is to handle the case where a boundary is all 0s\n    left_labels = np.unique(labeled_lattice[:, 0])\n    right_labels = np.unique(labeled_lattice[:, -1])\n\n    # Remove background label (0), which corresponds to empty sites\n    left_labels = left_labels[left_labels != 0]\n    right_labels = right_labels[right_labels != 0]\n    \n    # If either boundary has no clusters, no spanning is possible\n    if left_labels.size == 0 or right_labels.size == 0:\n        return False\n\n    # Check for any common labels between the left and right boundaries\n    return np.intersect1d(left_labels, right_labels, assume_unique=True).size > 0\n\ndef compute_spanning_prob(N: int, p: float, T: int, seed: int) -> float:\n    \"\"\"\n    Computes spanning probability via Monte Carlo simulation for given parameters.\n    Each trial uses an independent set of random numbers.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    spanning_count = 0\n    for _ in range(T):\n        # Generate one lattice configuration\n        lattice = rng.random((N, N)) < p\n        if _check_span(lattice):\n            spanning_count += 1\n    return spanning_count / T\n\ndef estimate_pc(N: int, T: int, p_lo: float, p_hi: float, epsilon: float, seed: int) -> float:\n    \"\"\"\n    Estimates the critical probability p_c using a bisection search.\n    Crucially, it uses the same set of base random numbers for all evaluations\n    of the spanning probability to ensure monotonicity of the estimator with p.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate one large block of random numbers to be used for all p values\n    base_random_numbers = rng.random((T, N, N))\n    \n    # Internal helper function that computes spanning probability for a given p\n    # using the pre-generated random numbers.\n    def _get_pi_monotonic(p_val: float) -> float:\n        # Create T lattices by thresholding the base random numbers\n        lattices = base_random_numbers < p_val\n        spanning_count = 0\n        for i in range(T):\n            if _check_span(lattices[i, :, :]):\n                spanning_count += 1\n        return spanning_count / T\n\n    # Bisection search loop\n    current_p_lo, current_p_hi = p_lo, p_hi\n    while (current_p_hi - current_p_lo) >= epsilon:\n        p_mid = (current_p_lo + current_p_hi) / 2.0\n        pi_mid = _get_pi_monotonic(p_mid)\n        \n        if pi_mid < 0.5:\n            current_p_lo = p_mid\n        else:\n            current_p_hi = p_mid\n            \n    return (current_p_lo + current_p_hi) / 2.0\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'type': 'pc', 'params': {'N': 32, 'T': 64, 'p_lo': 0.3, 'p_hi': 0.8, 'epsilon': 1e-3, 'seed': 12345}},\n        {'type': 'pc', 'params': {'N': 64, 'T': 64, 'p_lo': 0.3, 'p_hi': 0.8, 'epsilon': 1e-3, 'seed': 24680}},\n        {'type': 'pi', 'params': {'N': 64, 'p': 0.2, 'T': 200, 'seed': 98765}},\n        {'type': 'pi', 'params': {'N': 64, 'p': 0.8, 'T': 200, 'seed': 54321}},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'pc':\n            result = estimate_pc(**case['params'])\n        elif case['type'] == 'pi':\n            result = compute_spanning_prob(**case['params'])\n        results.append(result)\n\n    # Format results to 4 decimal places for the final output\n    formatted_results = [f\"{res:.4f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solution\nsolve()\n```"
        },
        {
            "introduction": "The most advanced forms of pattern recognition move from deterministic detection to probabilistic inference, allowing us to quantify our certainty about patterns in noisy data. This final exercise challenges you to implement a Bayesian change-point detection algorithm, a powerful technique for identifying when a system's underlying properties have shifted over time . By working from first principles to compute model evidences and posterior probabilities, you will gain experience with a sophisticated statistical framework that is central to modern data analysis across the sciences.",
            "id": "2425429",
            "problem": "You are given a pattern recognition task on synthetic experimental time-series data that models a physical sensor measuring a scalar quantity in steady operation before a degradation event. The data are generated from a piecewise-constant mean process corrupted by additive Gaussian noise, a common and well-tested model justified by the Central Limit Theorem. Your goal is to perform Bayesian change-point detection to infer whether a change-point exists and, if so, where it most likely occurs, from first principles.\n\nAssume the following generative model. For time indices $t = 1, 2, \\dots, N$, the observation $y_t$ is distributed as\n- $y_t \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ for $t \\le \\tau$,\n- $y_t \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$ for $t > \\tau$,\n\nwhere $\\sigma^2$ is known, $\\mu_1$ and $\\mu_2$ are unknown means, and $\\tau \\in \\{1, 2, \\dots, N-1\\}$ is an unknown change-point. The measurements are independent and identically distributed within each segment. Use the following Bayesian prior assumptions: $\\mu_1 \\sim \\mathcal{N}(m_0, s_0^2)$ and $\\mu_2 \\sim \\mathcal{N}(m_0, s_0^2)$ independently, and $\\tau$ is uniformly distributed on $\\{1, \\dots, N-1\\}$. To assess the presence of a change, consider two models: $M_1$ (one change-point with two means as above) and $M_0$ (no change-point, a single mean for all $t$), with prior probabilities $p(M_1) = p(M_0) = 1/2$. Under $M_0$, the model is $y_t \\sim \\mathcal{N}(\\mu, \\sigma^2)$ for all $t$, with $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$.\n\nStarting only from Bayes’ theorem, independence of observations given parameters, and properties of Gaussian distributions, derive the expressions needed to:\n- compute the marginal likelihood of a segment by analytically integrating out the unknown segment mean under the Gaussian prior with known variance $\\sigma^2$,\n- compute the marginal likelihood of the entire sequence under $M_0$ (single segment) and under $M_1$ for each candidate change-point $\\tau$,\n- compute the posterior distribution $p(\\tau \\mid \\mathbf{y}, M_1)$ up to a multiplicative constant, the maximum a posteriori estimate $\\hat{\\tau}$, and the posterior probability $p(M_1 \\mid \\mathbf{y})$ by combining model evidences with prior model probabilities.\n\nAlgorithmic constraints:\n- The computation must be numerically stable for moderate $N$ by working in the logarithmic domain when forming sums over candidate $\\tau$ values.\n- To ensure efficiency, your implementation should use cumulative sums to compute per-segment sufficient statistics such as the sample mean and the residual sum of squares in constant time for any candidate $\\tau$.\n- The result for each test case must be a two-element list containing the MAP estimate $\\hat{\\tau}$ and the posterior probability $p(M_1 \\mid \\mathbf{y})$, where $\\hat{\\tau}$ is an integer in the set $\\{1, \\dots, N-1\\}$ and $p(M_1 \\mid \\mathbf{y})$ is a float rounded to six decimal places.\n\nData simulation protocol:\n- For each test case, generate a sequence of length $N$ as follows. Draw $y_t \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ for $t \\le \\tau_{\\text{true}}$ and $y_t \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$ for $t > \\tau_{\\text{true}}$. If a test case specifies $\\mu_2 = \\mu_1$, the data follow the no-change model even though $\\tau_{\\text{true}}$ is provided; in that case, the correct inference should reflect low evidence for a change. Use a fixed random seed per test case as specified below to ensure reproducibility.\n- Use the same Gaussian prior for all test cases: $m_0 = 0$ and $s_0 = 1$.\n\nTest suite:\n- Case A (clear change, interior): $N = 200$, $\\tau_{\\text{true}} = 120$, $\\mu_1 = 0$, $\\mu_2 = 0.8$, $\\sigma = 0.3$, seed $= 2021$.\n- Case B (early change): $N = 200$, $\\tau_{\\text{true}} = 5$, $\\mu_1 = 0.1$, $\\mu_2 = 0.6$, $\\sigma = 0.4$, seed $= 2022$.\n- Case C (late change): $N = 200$, $\\tau_{\\text{true}} = 195$, $\\mu_1 = 0.2$, $\\mu_2 = -0.5$, $\\sigma = 0.35$, seed $= 2023$.\n- Case D (no change): $N = 200$, $\\tau_{\\text{true}} = 100$, $\\mu_1 = 0$, $\\mu_2 = 0$, $\\sigma = 0.5$, seed $= 2024$.\n\nFinal output specification:\n- For each test case, compute the maximum a posteriori estimate $\\hat{\\tau}$ under $M_1$ and the posterior probability $p(M_1 \\mid \\mathbf{y})$ obtained by combining the model evidences $p(\\mathbf{y} \\mid M_1)$ and $p(\\mathbf{y} \\mid M_0)$ with prior model probabilities $p(M_1) = p(M_0) = 1/2$.\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each element is itself a two-element list containing $\\hat{\\tau}$ and $p(M_1 \\mid \\mathbf{y})$, and the probability is rounded to six decimal places. For example, a syntactically valid output has the form `[[a,b],[c,d],[e,f],[g,h]]` with no spaces after commas.\n\nAngle units are not involved. No physical units are required for the output. All numerical answers must be provided as pure numbers without unit symbols.",
            "solution": "The problem presented is a standard exercise in Bayesian model selection and parameter estimation. It is scientifically grounded in the principles of probability theory and Bayesian inference, well-posed with all necessary information provided, and objective in its formulation. The problem is valid. We proceed with the derivation and solution.\n\nThe task is to infer the existence and location of a change-point in a time series. We are given two competing models: model $M_0$, which posits no change-point and a single mean for all data points, and model $M_1$, which posits a single change-point $\\tau$ separating two segments with distinct means. Our goal is to compute the maximum a posteriori (MAP) estimate of the change-point, $\\hat{\\tau}$, under model $M_1$, and to compute the posterior probability of model $M_1$, $p(M_1 \\mid \\mathbf{y})$, given the observed data $\\mathbf{y} = (y_1, \\dots, y_N)$.\n\nThe foundation of our analysis is Bayes' theorem. To compare models $M_0$ and $M_1$, we must compute their respective evidences, which are the marginal likelihoods of the data under each model, $p(\\mathbf{y} \\mid M_0)$ and $p(\\mathbf{y} \\mid M_1)$.\n\nFirst, we derive the marginal likelihood for a single segment of data $\\mathbf{x} = (x_1, \\dots, x_k)$ generated from a Gaussian distribution with a known variance and an unknown mean that has a Gaussian prior.\nLet the data be generated as $x_t \\sim \\mathcal{N}(\\mu, \\sigma^2)$ for $t=1, \\dots, k$. The variance $\\sigma^2$ is known. The prior on the mean is $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$.\nThe marginal likelihood, or evidence, for this segment is obtained by integrating out the unknown mean $\\mu$:\n$$p(\\mathbf{x} \\mid \\sigma^2, m_0, s_0^2) = \\int p(\\mathbf{x} \\mid \\mu, \\sigma^2) p(\\mu \\mid m_0, s_0^2) d\\mu$$\nThe likelihood of the data given $\\mu$ is:\n$$p(\\mathbf{x} \\mid \\mu, \\sigma^2) = \\prod_{t=1}^k \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_t - \\mu)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-k/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{t=1}^k (x_t - \\mu)^2\\right)$$\nThe prior on $\\mu$ is:\n$$p(\\mu \\mid m_0, s_0^2) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left(-\\frac{(\\mu - m_0)^2}{2s_0^2}\\right)$$\nThe product in the integrand is:\n$$p(\\mathbf{x} \\mid \\mu, \\sigma^2) p(\\mu) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{t=1}^k (x_t - \\mu)^2 - \\frac{1}{2s_0^2} (\\mu - m_0)^2\\right)$$\nThe exponent is a quadratic function of $\\mu$. By completing the square with respect to $\\mu$, we can identify the parameters of the posterior distribution $p(\\mu \\mid \\mathbf{x})$, which is also Gaussian, $\\mathcal{N}(m_k, s_k^2)$. The posterior precision is the sum of the prior precision and the data precision: $1/s_k^2 = 1/s_0^2 + k/\\sigma^2$. The posterior mean is a weighted average of the prior mean and the sample mean: $m_k = s_k^2 (m_0/s_0^2 + k\\bar{x}/\\sigma^2)$, where $\\bar{x} = \\frac{1}{k}\\sum_{t=1}^k x_t$.\nThe integral of an unnormalized Gaussian is its normalizing constant. The marginal likelihood can be found using the identity $p(\\mathbf{x}) = p(\\mathbf{x} \\mid \\mu)p(\\mu)/p(\\mu \\mid \\mathbf{x})$. A more direct calculation yields the following expression for the log-marginal likelihood:\n$$ \\log p(\\mathbf{x}) = -\\frac{k}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2}\\log(s_k^2) - \\frac{1}{2}\\log(s_0^2) - \\frac{1}{2\\sigma^2}\\sum_{t=1}^k x_t^2 - \\frac{m_0^2}{2s_0^2} + \\frac{m_k^2}{2s_k^2} $$\nwhere\n$$ s_k^2 = \\left(\\frac{1}{s_0^2} + \\frac{k}{\\sigma^2}\\right)^{-1} $$\n$$ m_k = s_k^2 \\left(\\frac{m_0}{s_0^2} + \\frac{k\\bar{x}}{\\sigma^2}\\right) $$\nThis expression allows for the numerically stable computation of the evidence for any data segment.\n\nNow, we apply this result to our two models.\n\nModel $M_0$ (no change): The entire data sequence $\\mathbf{y} = (y_1, \\dots, y_N)$ is considered a single segment. The evidence for $M_0$ is simply the marginal likelihood of the full sequence:\n$$ \\log p(\\mathbf{y} \\mid M_0) = \\log p(\\mathbf{y}_{1:N}) $$\nThis value is computed using the formula above with $k=N$.\n\nModel $M_1$ (one change-point): For a given change-point $\\tau \\in \\{1, \\dots, N-1\\}$, the data is split into two independent segments: $\\mathbf{y}_{1:\\tau}$ (of length $\\tau$) and $\\mathbf{y}_{\\tau+1:N}$ (of length $N-\\tau$). Since the priors for the means $\\mu_1$ and $\\mu_2$ are independent, the conditional evidence for a given $\\tau$ is the product of the evidences of the two segments:\n$$ p(\\mathbf{y} \\mid \\tau, M_1) = p(\\mathbf{y}_{1:\\tau}) p(\\mathbf{y}_{\\tau+1:N}) $$\nIn the logarithmic domain:\n$$ \\log p(\\mathbf{y} \\mid \\tau, M_1) = \\log p(\\mathbf{y}_{1:\\tau}) + \\log p(\\mathbf{y}_{\\tau+1:N}) $$\nThe full evidence for model $M_1$ is obtained by marginalizing over all possible change-points $\\tau$:\n$$ p(\\mathbf{y} \\mid M_1) = \\sum_{\\tau=1}^{N-1} p(\\mathbf{y} \\mid \\tau, M_1) p(\\tau \\mid M_1) $$\nGiven the uniform prior on the change-point, $p(\\tau \\mid M_1) = 1/(N-1)$, this becomes:\n$$ p(\\mathbf{y} \\mid M_1) = \\frac{1}{N-1} \\sum_{\\tau=1}^{N-1} p(\\mathbf{y} \\mid \\tau, M_1) $$\nTo avoid numerical underflow, this sum of exponentials is computed using the log-sum-exp trick. Let $L_\\tau = \\log p(\\mathbf{y} \\mid \\tau, M_1)$. Then\n$$ \\log p(\\mathbf{y} \\mid M_1) = -\\log(N-1) + \\text{logsumexp}_{\\tau} (L_\\tau) $$\nwhere $\\text{logsumexp}(L) = L_{\\max} + \\log(\\sum e^{L_\\tau - L_{\\max}})$.\n\nWith the model evidences computed, we can find the required posterior quantities.\n\nThe posterior distribution of the change-point under model $M_1$ is given by Bayes' theorem:\n$$ p(\\tau \\mid \\mathbf{y}, M_1) = \\frac{p(\\mathbf{y} \\mid \\tau, M_1)p(\\tau \\mid M_1)}{p(\\mathbf{y} \\mid M_1)} $$\nSince the prior $p(\\tau \\mid M_1)$ and the denominator $p(\\mathbf{y} \\mid M_1)$ are constant with respect to $\\tau$, the posterior is proportional to the likelihood:\n$$ p(\\tau \\mid \\mathbf{y}, M_1) \\propto p(\\mathbf{y} \\mid \\tau, M_1) $$\nThe MAP estimate $\\hat{\\tau}$ is the value of $\\tau$ that maximizes this posterior probability. This is equivalent to maximizing the log-likelihood $\\log p(\\mathbf{y} \\mid \\tau, M_1)$:\n$$ \\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, N-1\\}} \\log p(\\mathbf{y} \\mid \\tau, M_1) $$\n\nFinally, the posterior probability of model $M_1$ is calculated by comparing its evidence against the total evidence:\n$$ p(M_1 \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid M_1) p(M_1)}{p(\\mathbf{y} \\mid M_0) p(M_0) + p(\\mathbf{y} \\mid M_1) p(M_1)} $$\nWith equal prior probabilities $p(M_0) = p(M_1) = 1/2$, this simplifies to:\n$$ p(M_1 \\mid \\mathbf{y}) = \\frac{p(\\mathbf{y} \\mid M_1)}{p(\\mathbf{y} \\mid M_0) + p(\\mathbf{y} \\mid M_1)} = \\frac{1}{1 + \\frac{p(\\mathbf{y} \\mid M_0)}{p(\\mathbf{y} \\mid M_1)}} $$\nThis is computed using the previously derived log-evidences, $\\log p(\\mathbf{y} \\mid M_0)$ and $\\log p(\\mathbf{y} \\mid M_1)$, as:\n$$ p(M_1 \\mid \\mathbf{y}) = \\frac{1}{1 + \\exp(\\log p(\\mathbf{y} \\mid M_0) - \\log p(\\mathbf{y} \\mid M_1))} $$\n\nFor efficient implementation, the sufficient statistics for each segment—the sum $\\sum x_t$ and sum of squares $\\sum x_t^2$—are computed in $O(1)$ time using pre-calculated cumulative sums of the data sequence $\\mathbf{y}$. This reduces the total complexity of finding $\\hat{\\tau}$ and the model evidences to $O(N)$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef log_marginal_likelihood_segment(sum_y, sum_y2, k, m0, s0, sigma):\n    \"\"\"\n    Computes the log marginal likelihood for a single segment of data.\n    \"\"\"\n    if k == 0:\n        return 0.0\n\n    s02 = s0**2\n    sigma2 = sigma**2\n    mean_y = sum_y / k\n\n    # Posterior variance of the mean mu\n    sk2_inv = 1.0 / s02 + k / sigma2\n    sk2 = 1.0 / sk2_inv\n\n    # Posterior mean of mu\n    mk = sk2 * (m0 / s02 + k * mean_y / sigma2)\n\n    # Log marginal likelihood expression from derivation\n    log_p = (-k / 2.0 * np.log(2.0 * np.pi * sigma2) +\n             0.5 * np.log(sk2) -\n             0.5 * np.log(s02) -\n             sum_y2 / (2.0 * sigma2) -\n             m0**2 / (2.0 * s02) +\n             mk**2 / (2.0 * sk2))\n\n    return log_p\n\ndef analyze_sequence(y, m0, s0, sigma):\n    \"\"\"\n    Performs Bayesian change-point analysis on a time series.\n    \"\"\"\n    N = len(y)\n    \n    # Pre-compute cumulative sums for efficient segment statistic calculation\n    # cum_y[j] = sum(y_i) for i from 0 to j-1\n    cum_y = np.concatenate(([0.0], np.cumsum(y)))\n    cum_y2 = np.concatenate(([0.0], np.cumsum(y**2)))\n\n    # --- Evidence for M0 (no change-point) ---\n    sum_y_total = cum_y[N] - cum_y[0]\n    sum_y2_total = cum_y2[N] - cum_y2[0]\n    log_evidence_m0 = log_marginal_likelihood_segment(sum_y_total, sum_y2_total, N, m0, s0, sigma)\n\n    # --- Likelihoods for M1 (one change-point) for each tau ---\n    log_p_tau = np.zeros(N - 1)\n    \n    # tau is the change-point index from 1 to N-1\n    # a change at tau means y[:tau] is segment 1, y[tau:] is segment 2\n    for tau in range(1, N):\n        # Segment 1: y[0...tau-1] (length tau)\n        k1 = tau\n        sum_y1 = cum_y[tau] - cum_y[0]\n        sum_y2_1 = cum_y2[tau] - cum_y2[0]\n        log_p1 = log_marginal_likelihood_segment(sum_y1, sum_y2_1, k1, m0, s0, sigma)\n\n        # Segment 2: y[tau...N-1] (length N-tau)\n        k2 = N - tau\n        sum_y2 = cum_y[N] - cum_y[tau]\n        sum_y2_2 = cum_y2[N] - cum_y2[tau]\n        log_p2 = log_marginal_likelihood_segment(sum_y2, sum_y2_2, k2, m0, s0, sigma)\n\n        log_p_tau[tau - 1] = log_p1 + log_p2\n\n    # --- MAP estimate for tau under M1 ---\n    # np.argmax returns 0-based index. tau is 1-based.\n    hat_tau = np.argmax(log_p_tau) + 1\n\n    # --- Evidence for M1 ---\n    # Marginalize over tau using uniform prior p(tau)=1/(N-1)\n    # log p(y|M1) = log( sum_tau p(y|tau,M1) * p(tau|M1) )\n    #             = log( sum_tau exp(log_p_tau) * 1/(N-1) )\n    #             = logsumexp(log_p_tau) - np.log(N-1)\n    log_evidence_m1 = logsumexp(log_p_tau) - np.log(N - 1)\n\n    # --- Posterior probability of M1 ---\n    # p(M1|y) = 1 / (1 + p(y|M0)/p(y|M1))\n    # p(y|M0)/p(y|M1) = exp(log_evidence_m0 - log_evidence_m1)\n    log_bayes_factor_01 = log_evidence_m0 - log_evidence_m1\n    p_m1_posterior = 1.0 / (1.0 + np.exp(log_bayes_factor_01))\n\n    return [int(hat_tau), round(p_m1_posterior, 6)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'N': 200, 'tau_true': 120, 'mu1': 0.0, 'mu2': 0.8, 'sigma': 0.3, 'seed': 2021},\n        {'N': 200, 'tau_true': 5, 'mu1': 0.1, 'mu2': 0.6, 'sigma': 0.4, 'seed': 2022},\n        {'N': 200, 'tau_true': 195, 'mu1': 0.2, 'mu2': -0.5, 'sigma': 0.35, 'seed': 2023},\n        {'N': 200, 'tau_true': 100, 'mu1': 0.0, 'mu2': 0.0, 'sigma': 0.5, 'seed': 2024},\n    ]\n\n    # Shared prior parameters for all cases\n    m0 = 0.0\n    s0 = 1.0\n\n    results = []\n    for case in test_cases:\n        # Generate synthetic data\n        rng = np.random.default_rng(case['seed'])\n        y = np.zeros(case['N'])\n        seg1_len = case['tau_true']\n        seg2_len = case['N'] - case['tau_true']\n        \n        y[:seg1_len] = rng.normal(loc=case['mu1'], scale=case['sigma'], size=seg1_len)\n        if seg2_len > 0:\n            y[seg1_len:] = rng.normal(loc=case['mu2'], scale=case['sigma'], size=seg2_len)\n\n        # Perform analysis\n        result = analyze_sequence(y, m0, s0, case['sigma'])\n        results.append(result)\n\n    # Format output as specified\n    # e.g., [[120,0.999999],[5,0.987654],...]\n    output_str = '['\n    for i, res in enumerate(results):\n        output_str += f'[{res[0]},{res[1]}]'\n        if i < len(results) - 1:\n            output_str += ','\n    output_str += ']'\n    \n    print(output_str)\n\nsolve()\n\n```"
        }
    ]
}