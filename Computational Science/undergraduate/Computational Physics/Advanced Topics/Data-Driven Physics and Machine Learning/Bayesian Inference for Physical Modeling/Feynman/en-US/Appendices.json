{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a quintessential example of Bayesian parameter estimation. By analyzing simulated data from a Geiger counter, you will infer the decay rate $\\lambda$ of a radioactive source . This exercise demonstrates the elegant mechanics of conjugate priors, where the posterior distribution takes the same functional form as the prior, offering a clear and intuitive look at how belief is updated in light of new evidence.",
            "id": "2375997",
            "problem": "A Geiger counter records discrete detection counts from a stationary radioactive source. The detection process is modeled as a homogeneous Poisson process of constant decay rate $\\lambda$ in $\\mathrm{s^{-1}}$. Over non-overlapping time windows of lengths $\\{t_i\\}$ in $\\mathrm{s}$, the counts $\\{k_i\\}$ are assumed to be conditionally independent given $\\lambda$ with $k_i$ distributed as $\\mathrm{Poisson}(\\lambda t_i)$.\n\nAssume a prior on $\\lambda$ that follows the Gamma distribution with shape $\\alpha_0$ and rate $\\beta_0$, parameterized by the density\n$$\np(\\lambda \\mid \\alpha_0,\\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}, \\quad \\lambda > 0,\n$$\nwhere $\\alpha_0 > 0$ is dimensionless and $\\beta_0 > 0$ has units $\\mathrm{s}$.\n\nTask: For each test case below, compute the posterior mean $\\mathbb{E}[\\lambda \\mid \\{(t_i,k_i)\\}]$ in $\\mathrm{s^{-1}}$. Express each result in $\\mathrm{s^{-1}}$, rounded to six decimal places.\n\nTest suite:\n- Case 1 (general): counts $k = (1,0,2)$, durations $t = (1.0,2.0,3.0)\\,\\mathrm{s}$, prior $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n- Case 2 (zero counts): counts $k = (0,0)$, durations $t = (10.0,20.0)\\,\\mathrm{s}$, prior $\\alpha_0 = 2.0$, $\\beta_0 = 5.0\\,\\mathrm{s}$.\n- Case 3 (long exposure, high counts): counts $k = (130,250,410)$, durations $t = (100.0,200.0,300.0)\\,\\mathrm{s}$, prior $\\alpha_0 = 0.5$, $\\beta_0 = 0.1\\,\\mathrm{s}$.\n- Case 4 (very short windows): counts $k = (0,1)$, durations $t = (0.001,0.002)\\,\\mathrm{s}$, prior $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets, ordered as Case $1$, Case $2$, Case $3$, Case $4$. For example, a valid format is $[x_1,x_2,x_3,x_4]$, where each $x_j$ is the posterior mean in $\\mathrm{s^{-1}}$ rounded to six decimal places.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of nuclear physics and Bayesian statistics, well-posed, and contains all necessary information for a unique solution. We shall proceed with the derivation.\n\nThe objective is to find the posterior mean of the decay rate parameter $\\lambda$ of a Poisson process. The foundation of this task is Bayes' theorem, which states that the posterior probability is proportional to the product of the likelihood and the prior probability.\n$$\np(\\text{parameter} \\mid \\text{data}) \\propto p(\\text{data} \\mid \\text{parameter}) \\cdot p(\\text{parameter})\n$$\nIn this problem, the parameter is $\\lambda$ and the data, which we denote as $\\mathcal{D}$, consists of a set of observations $\\{ (t_i, k_i) \\}_{i=1}^N$, where $k_i$ is the number of detection counts in a time interval of length $t_i$.\n\nFirst, we define the likelihood function, $p(\\mathcal{D} \\mid \\lambda)$. The problem states that each count $k_i$ is drawn from a Poisson distribution with parameter $\\lambda t_i$. The probability mass function for a single observation is:\n$$\nP(k_i \\mid \\lambda, t_i) = \\frac{(\\lambda t_i)^{k_i} e^{-\\lambda t_i}}{k_i!}\n$$\nGiven that the observations are conditionally independent, the likelihood for the entire dataset $\\mathcal{D}$ is the product of the individual probabilities:\n$$\np(\\mathcal{D} \\mid \\lambda) = \\prod_{i=1}^{N} P(k_i \\mid \\lambda, t_i) = \\prod_{i=1}^{N} \\frac{(\\lambda t_i)^{k_i} e^{-\\lambda t_i}}{k_i!}\n$$\nIn Bayesian analysis, we are interested in the functional form of the likelihood with respect to the parameter $\\lambda$. We can therefore discard terms that do not depend on $\\lambda$, absorbing them into a normalization constant.\n$$\np(\\mathcal{D} \\mid \\lambda) \\propto \\prod_{i=1}^{N} (\\lambda t_i)^{k_i} e^{-\\lambda t_i} = \\left( \\prod_{i=1}^{N} t_i^{k_i} \\right) \\left( \\prod_{i=1}^{N} \\lambda^{k_i} \\right) \\left( \\prod_{i=1}^{N} e^{-\\lambda t_i} \\right)\n$$\n$$\np(\\mathcal{D} \\mid \\lambda) \\propto \\lambda^{\\sum_{i=1}^{N} k_i} e^{-\\lambda \\sum_{i=1}^{N} t_i}\n$$\nLet us define the total counts $K = \\sum_{i=1}^{N} k_i$ and the total observation time $T = \\sum_{i=1}^{N} t_i$. The likelihood simplifies to:\n$$\np(\\mathcal{D} \\mid \\lambda) \\propto \\lambda^K e^{-\\lambda T}\n$$\n\nNext, we consider the prior distribution for $\\lambda$, which is given as a Gamma distribution with shape parameter $\\alpha_0 > 0$ and rate parameter $\\beta_0 > 0$.\n$$\np(\\lambda \\mid \\alpha_0, \\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)} \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}\n$$\nThe kernel of the prior distribution, ignoring the normalization constant, is:\n$$\np(\\lambda \\mid \\alpha_0, \\beta_0) \\propto \\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}\n$$\n\nNow, we combine the likelihood and the prior to find the posterior distribution, $p(\\lambda \\mid \\mathcal{D}, \\alpha_0, \\beta_0)$:\n$$\np(\\lambda \\mid \\mathcal{D}, \\alpha_0, \\beta_0) \\propto p(\\mathcal{D} \\mid \\lambda) \\cdot p(\\lambda \\mid \\alpha_0, \\beta_0)\n$$\n$$\np(\\lambda \\mid \\mathcal{D}, \\alpha_0, \\beta_0) \\propto (\\lambda^K e^{-\\lambda T}) \\cdot (\\lambda^{\\alpha_0 - 1} e^{-\\beta_0 \\lambda}) = \\lambda^{K + \\alpha_0 - 1} e^{-(\\beta_0 + T)\\lambda}\n$$\nThis expression is the kernel of a Gamma distribution. This confirms that the Gamma distribution is a conjugate prior for the Poisson likelihood, meaning the posterior distribution is also a Gamma distribution. The parameters of this posterior Gamma distribution, which we denote $\\alpha_N$ and $\\beta_N$, are found by inspection:\n$$\n\\alpha_N = \\alpha_0 + K = \\alpha_0 + \\sum_{i=1}^{N} k_i\n$$\n$$\n\\beta_N = \\beta_0 + T = \\beta_0 + \\sum_{i=1}^{N} t_i\n$$\nThe task is to compute the posterior mean, $\\mathbb{E}[\\lambda \\mid \\mathcal{D}]$. The expected value of a random variable $X$ following a Gamma distribution with shape $\\alpha$ and rate $\\beta$ is $\\mathbb{E}[X] = \\alpha/\\beta$. Applying this to our posterior distribution, we obtain the posterior mean of $\\lambda$:\n$$\n\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{\\alpha_N}{\\beta_N} = \\frac{\\alpha_0 + \\sum k_i}{\\beta_0 + \\sum t_i}\n$$\nThis is the general formula for the solution. We now apply it to each specified test case.\n\nCase 1: $k = (1, 0, 2)$, $t = (1.0, 2.0, 3.0)\\,\\mathrm{s}$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n$\\sum k_i = 1+0+2=3$.\n$\\sum t_i = 1.0+2.0+3.0=6.0\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{1.0 + 3}{1.0 + 6.0} = \\frac{4.0}{7.0} \\approx 0.571429\\,\\mathrm{s^{-1}}$.\n\nCase 2: $k = (0, 0)$, $t = (10.0, 20.0)\\,\\mathrm{s}$, $\\alpha_0 = 2.0$, $\\beta_0 = 5.0\\,\\mathrm{s}$.\n$\\sum k_i = 0+0=0$.\n$\\sum t_i = 10.0+20.0=30.0\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{2.0 + 0}{5.0 + 30.0} = \\frac{2.0}{35.0} \\approx 0.057143\\,\\mathrm{s^{-1}}$.\n\nCase 3: $k = (130, 250, 410)$, $t = (100.0, 200.0, 300.0)\\,\\mathrm{s}$, $\\alpha_0 = 0.5$, $\\beta_0 = 0.1\\,\\mathrm{s}$.\n$\\sum k_i = 130+250+410=790$.\n$\\sum t_i = 100.0+200.0+300.0=600.0\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{0.5 + 790}{0.1 + 600.0} = \\frac{790.5}{600.1} \\approx 1.317280\\,\\mathrm{s^{-1}}$.\n\nCase 4: $k = (0, 1)$, $t = (0.001, 0.002)\\,\\mathrm{s}$, $\\alpha_0 = 1.0$, $\\beta_0 = 1.0\\,\\mathrm{s}$.\n$\\sum k_i = 0+1=1$.\n$\\sum t_i = 0.001+0.002=0.003\\,\\mathrm{s}$.\n$\\mathbb{E}[\\lambda \\mid \\mathcal{D}] = \\frac{1.0 + 1}{1.0 + 0.003} = \\frac{2.0}{1.003} \\approx 1.994018\\,\\mathrm{s^{-1}}$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior mean of a Poisson rate parameter for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: k=(1,0,2), t=(1.0,2.0,3.0), a0=1.0, b0=1.0\n        {'k': [1, 0, 2], 't': [1.0, 2.0, 3.0], 'a0': 1.0, 'b0': 1.0},\n        # Case 2: k=(0,0), t=(10.0,20.0), a0=2.0, b0=5.0\n        {'k': [0, 0], 't': [10.0, 20.0], 'a0': 2.0, 'b0': 5.0},\n        # Case 3: k=(130,250,410), t=(100.0,200.0,300.0), a0=0.5, b0=0.1\n        {'k': [130, 250, 410], 't': [100.0, 200.0, 300.0], 'a0': 0.5, 'b0': 0.1},\n        # Case 4: k=(0,1), t=(0.001,0.002), a0=1.0, b0=1.0\n        {'k': [0, 1], 't': [0.001, 0.002], 'a0': 1.0, 'b0': 1.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        # The posterior distribution for lambda is a Gamma distribution with\n        # updated parameters alpha_N and beta_N.\n        # alpha_N = alpha_0 + sum(k_i)\n        # beta_N = beta_0 + sum(t_i)\n        # The posterior mean E[lambda | data] is alpha_N / beta_N.\n\n        # Calculate total counts and total time\n        total_counts = np.sum(case['k'])\n        total_time = np.sum(case['t'])\n        \n        # Get prior parameters\n        alpha_0 = case['a0']\n        beta_0 = case['b0']\n        \n        # Calculate posterior parameters\n        alpha_n = alpha_0 + total_counts\n        beta_n = beta_0 + total_time\n        \n        # Calculate posterior mean\n        posterior_mean = alpha_n / beta_n\n        \n        # Format the result as a string rounded to six decimal places.\n        # The format specifier \"{:.6f}\" handles both rounding and formatting.\n        results.append(f\"{posterior_mean:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Moving beyond simple parameter estimation, this exercise delves into the domain of Bayesian model selection. Using noisy position measurements, your task is to determine the most probable quantum state $n$ for a particle in an infinite potential well . This practice introduces the crucial concept of marginalization over latent variables and illustrates how the Bayesian framework naturally allows for comparing the evidence for a discrete set of competing physical models.",
            "id": "2375965",
            "problem": "A nonrelativistic particle of mass $m$ confined in a one-dimensional infinite potential well of length $L$ has stationary states indexed by a positive integer quantum number $n$. The spatial wavefunction is normalized and the position probability density is given by the square modulus of the wavefunction. Consider an experimental procedure that measures the particle position with additive, independent, zero-mean Gaussian noise of known standard deviation $\\sigma$. The true position is supported only on the interval $[0, L]$, but the measured value can lie anywhere on the real line due to noise. You are asked to perform Bayesian inference on the integer quantum number $n$ from a finite set of such noisy position measurements.\n\nYour program must implement the following modeling assumptions and compute the requested outputs:\n\n- The infinite square well stationary-state wavefunction leads to a position probability density $p(x \\mid n)$ supported on $x \\in [0,L]$.\n- Each measurement $y$ is generated by first drawing $x$ from $p(x \\mid n)$ and then drawing $y$ from a normal distribution with mean $x$ and variance $\\sigma^2$, independently across measurements. That is, the conditional density $p(y \\mid x)$ is Gaussian with standard deviation $\\sigma$.\n- Measurements are conditionally independent given $n$.\n- A prior $p(n)$ over a finite candidate set of integers is provided for each test case.\n- The posterior $p(n \\mid y_1,\\dots,y_M)$ is proportional to the product of the prior and the likelihood across all measurements, with the likelihood for each measurement obtained by marginalizing over the latent true position $x$.\n- The maximum a posteriori estimate is the integer $n$ that maximizes the posterior. In case of ties within numerical precision, choose the smallest $n$.\n\nAll physical quantities must be treated in the International System of Units (SI). All positions, including $L$, $\\sigma$, and the measurements $y$, must be in meters. Angles, where they appear in trigonometric functions, are dimensionless arguments constructed from physical quantities so that the arguments are unitless. Your final answers are integers and thus unitless.\n\nYour program must solve the following test suite of four cases. For each case, compute the maximum a posteriori estimate of $n$ from the provided data and priors. Use only the provided data; do not generate any random numbers.\n\n- Case A (moderate noise, tri-modal structure): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.02\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with a uniform prior over these candidates, measurements $y$ given by the list $[0.151, 0.170, 0.497, 0.512, 0.835, 0.820, 0.505, 0.166, 0.842, 0.158]$ in meters.\n- Case B (near-noiseless, single-peaked structure): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.005\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with a uniform prior over these candidates, measurements $y$ given by the list $[0.480, 0.514, 0.492, 0.508, 0.501]$ in meters.\n- Case C (high noise, prior-dominated): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.15\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with prior $p(n) \\propto 1/n^2$ over these candidates, measurements $y$ given by the list $[0.10, 0.90, 0.25, 0.75]$ in meters.\n- Case D (measurements outside the well due to noise): $L = 1.0\\,\\mathrm{m}$, $\\sigma = 0.03\\,\\mathrm{m}$, candidate set $\\{1,2,3,4,5\\}$ with a uniform prior over these candidates, measurements $y$ given by the list $[-0.03, 1.04, 0.24, 0.76, 0.26, 0.50]$ in meters.\n\nRequirements for the computation:\n\n- Use the standard infinite square well model: the position probability density for state index $n$ is proportional to $\\sin^2(n \\pi x / L)$ on $[0, L]$ and zero outside, normalized to integrate to one over $[0, L]$.\n- Use a Gaussian model for measurement noise as specified above.\n- Assume conditional independence of measurements given $n$ to form the likelihood of the dataset as a product over individual measurements.\n- Marginalize the latent true position $x$ out of each per-measurement likelihood by integrating over the physical interval $[0, L]$.\n- Compute posterior probabilities up to a multiplicative constant and identify the maximizing $n$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the same order as the cases A, B, C, D. For example, an output might look like \"[3,1,1,2]\" where each entry is the maximum a posteriori $n$ for the corresponding case. The exact format, including brackets and commas without spaces, is mandatory.",
            "solution": "The problem presented requires the application of Bayesian inference to determine the most probable quantum state of a particle in a one-dimensional infinite potential well, given a set of noisy position measurements. The problem is scientifically sound, well-posed, and all necessary information for its solution is provided.\n\nThe core of the problem is to compute the posterior probability distribution for the quantum number $n$ given a set of measurements $D = \\{y_1, y_2, \\dots, y_M\\}$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(n | D) \\propto p(D | n) p(n)\n$$\nThe measurements are assumed to be conditionally independent given the quantum number $n$. Therefore, the total likelihood for the dataset $D$ is the product of the likelihoods for each individual measurement:\n$$\np(D | n) = \\prod_{i=1}^{M} p(y_i | n)\n$$\nFor numerical stability, it is preferable to work with log-probabilities. The log-posterior is given by:\n$$\n\\log p(n | D) = \\log p(n) + \\sum_{i=1}^{M} \\log p(y_i | n) + C\n$$\nwhere $C$ is a normalization constant that does not depend on $n$. Our objective is to find the maximum a posteriori (MAP) estimate of $n$, which is the value of $n$ from the candidate set that maximizes this log-posterior:\n$$\nn_{\\text{MAP}} = \\underset{n}{\\arg\\max} \\left( \\log p(n) + \\sum_{i=1}^{M} \\log p(y_i | n) \\right)\n$$\nThe central task is to compute the likelihood for a single measurement, $p(y_i | n)$. A measurement $y_i$ is a noisy version of some true particle position $x$. The true position $x$ is not observed and is therefore a latent variable. We must marginalize (integrate out) this latent variable to find the likelihood. The marginalization is an application of the law of total probability:\n$$\np(y_i | n) = \\int p(y_i, x | n) dx = \\int p(y_i | x, n) p(x | n) dx\n$$\nThe problem states that the measurement $y_i$ depends only on the true position $x$, so $p(y_i | x, n) = p(y_i | x)$. The integral becomes:\n$$\np(y_i | n) = \\int p(y_i | x) p(x | n) dx\n$$\nWe must now define the two probability density functions in the integrand.\n\n$1$. The position probability density $p(x | n)$ for a particle in stationary state $n$ of an infinite square well of length $L$ is given by the squared modulus of the normalized wavefunction. The wavefunction is $\\psi_n(x) = \\sqrt{\\frac{2}{L}} \\sin\\left(\\frac{n \\pi x}{L}\\right)$ for $x \\in [0, L]$ and zero otherwise. Thus, the probability density is:\n$$\np(x | n) = \\frac{2}{L} \\sin^2\\left(\\frac{n \\pi x}{L}\\right), \\quad x \\in [0, L]\n$$\n\n$2$. The measurement noise is modeled as a Gaussian (normal) distribution with mean $x$ (the true position) and standard deviation $\\sigma$. The conditional probability of observing $y_i$ given a true position $x$ is:\n$$\np(y_i | x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right)\n$$\nThe integration domain for the latent variable $x$ is the physical extent of the potential well, which is $[0, L]$. Combining these expressions, the per-measurement likelihood is:\n$$\np(y_i | n) = \\int_{0}^{L} \\left[ \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right) \\right] \\left[ \\frac{2}{L} \\sin^2\\left(\\frac{n \\pi x}{L}\\right) \\right] dx\n$$\nThis integral does not have a simple closed-form solution and must be evaluated numerically for each pair of $(y_i, n)$. We utilize numerical quadrature for this purpose. Since the factors $\\frac{2}{L\\sqrt{2\\pi}\\sigma}$ are constant with respect to $n$ for a given test case, they can be omitted when finding the argmax, simplifying the proportional log-posterior to:\n$$\nLP(n) \\propto \\log p_{\\text{unnorm}}(n) + \\sum_{i=1}^{M} \\log \\left( \\int_{0}^{L} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right) \\sin^2\\left(\\frac{n \\pi x}{L}\\right) dx \\right)\n$$\nwhere $p_{\\text{unnorm}}(n)$ is the unnormalized prior. For a uniform prior, $\\log p(n)$ is constant and can be ignored. For the prior $p(n) \\propto 1/n^2$, we use $\\log p_{\\text{unnorm}}(n) = -2 \\log n$.\n\nThe overall algorithm is as follows:\nFor each test case:\n$1$. For each candidate integer $n$ in the set $\\{1, 2, 3, 4, 5\\}$:\n    a. Calculate the log-prior term, $\\log p_{\\text{unnorm}}(n)$.\n    b. Initialize a total log-likelihood for this $n$ to $0$.\n    c. For each measurement $y_i$ in the provided list:\n        i. Numerically compute the integral $I(y_i, n) = \\int_{0}^{L} \\exp\\left(-\\frac{(y_i-x)^2}{2\\sigma^2}\\right) \\sin^2\\left(\\frac{n \\pi x}{L}\\right) dx$.\n        ii. Add $\\log(I(y_i, n))$ to the total log-likelihood.\n    d. The unnormalized log-posterior for $n$ is the sum of the log-prior term and the total log-likelihood.\n$2$. After computing the log-posterior for all candidate $n$'s, identify the value of $n$ that yields the maximum log-posterior.\n$3$. In case of ties, the smallest value of $n$ is chosen as specified.\n\nThis procedure is implemented for each of the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for the four specified test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Format: (L, sigma, n_candidates, prior_spec, measurements)\n    test_cases = [\n        # Case A\n        (1.0, 0.02, np.array([1, 2, 3, 4, 5]), ('uniform',), \n         np.array([0.151, 0.170, 0.497, 0.512, 0.835, 0.820, 0.505, 0.166, 0.842, 0.158])),\n        # Case B\n        (1.0, 0.005, np.array([1, 2, 3, 4, 5]), ('uniform',), \n         np.array([0.480, 0.514, 0.492, 0.508, 0.501])),\n        # Case C\n        (1.0, 0.15, np.array([1, 2, 3, 4, 5]), ('inv_sq',), \n         np.array([0.10, 0.90, 0.25, 0.75])),\n        # Case D\n        (1.0, 0.03, np.array([1, 2, 3, 4, 5]), ('uniform',), \n         np.array([-0.03, 1.04, 0.24, 0.76, 0.26, 0.50]))\n    ]\n\n    results = []\n\n    for case in test_cases:\n        L, sigma, n_candidates, prior_spec, measurements = case\n        log_posteriors = np.zeros(len(n_candidates))\n\n        for i, n in enumerate(n_candidates):\n            # Calculate log prior\n            log_prior = 0.0\n            if prior_spec[0] == 'inv_sq':\n                # p(n) is proportional to 1/n^2, so log p(n) is proportional to -2*log(n).\n                log_prior = -2.0 * np.log(n)\n            # For uniform prior, log_prior is constant and can be taken as 0 for argmax.\n\n            # Calculate total log likelihood\n            total_log_likelihood = 0.0\n            for y in measurements:\n                # Define the integrand for the marginal likelihood.\n                # Wwe can omit constant factors that do not depend on n.\n                # The integrand is p(y|x) * p(x|n) up to constants.\n                def integrand(x):\n                    # p(x|n) term, proportional to sin^2(n*pi*x/L)\n                    psi_sq_part = np.sin(n * np.pi * x / L)**2\n                    \n                    # p(y|x) term, proportional to exp(-(y-x)^2/(2*sigma^2))\n                    gaussian_part = np.exp(-0.5 * ((y - x) / sigma)**2)\n                    \n                    return gaussian_part * psi_sq_part\n                \n                # Numerically integrate over the well [0, L]\n                integral_val, _ = quad(integrand, 0, L)\n                \n                # Add the log of the integral to the total log likelihood.\n                # If integral_val is 0, its log is -inf, which is correct.\n                if integral_val > 0:\n                    total_log_likelihood += np.log(integral_val)\n                else:\n                    total_log_likelihood += -np.inf\n\n            # Unnormalized log posterior\n            log_posteriors[i] = log_prior + total_log_likelihood\n\n        # Find the n that maximizes the posterior.\n        # Use np.isclose for floating point comparison to handle ties.\n        max_log_post = np.max(log_posteriors)\n        \n        # Check for -inf case (likelihood was zero for all n)\n        if np.isneginf(max_log_post):\n            # This is an edge case, unlikely here. If it happens, any n is equally bad.\n            # The problem asks for the smallest n in case of a tie.\n            best_n = n_candidates[0]\n        else:\n            # Find all indices that are close to the maximum value\n            best_indices = np.where(np.isclose(log_posteriors, max_log_post))[0]\n            # Choose the one corresponding to the smallest n as per the tie-breaking rule.\n            # Since n_candidates is sorted, the first index corresponds to the smallest n.\n            best_n = n_candidates[best_indices[0]]\n            \n        results.append(best_n)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Our final practice tackles a classic challenge in computational physics: the ill-posed inverse problem. You will use sparse temperature readings taken at later times to infer the initial temperature profile $u(x,0)$ of a 1D rod, effectively running the heat equation backward in time . This exercise powerfully demonstrates how a prior distribution acts as a regularization term, enabling stable and physically plausible solutions to problems that are otherwise numerically unsolvable.",
            "id": "2376011",
            "problem": "Consider a one-dimensional rod of length $L$ with homogeneous Dirichlet boundary conditions and thermal diffusivity $\\kappa$. The temperature field $u(x,t)$ satisfies the heat equation (a linear Partial Differential Equation (PDE)) given by\n$$\n\\frac{\\partial u}{\\partial t}(x,t) \\;=\\; \\kappa \\,\\frac{\\partial^2 u}{\\partial x^2}(x,t), \\quad 0<x<L,\\; t>0,\n$$\nwith boundary conditions $u(0,t)=0$ and $u(L,t)=0$ for all $t\\ge 0$. The initial temperature profile $u(x,0)$ is unknown and is modeled by a truncated sine series with $M$ modes,\n$$\nu(x,0) \\;=\\; \\sum_{n=1}^{M} \\theta_n \\,\\sin\\!\\Big(\\frac{n\\pi x}{L}\\Big),\n$$\nwhere the coefficients $\\theta_n$ are in Kelvin. By separation of variables, the solution for $t>0$ is\n$$\nu(x,t) \\;=\\; \\sum_{n=1}^{M} \\theta_n \\,\\exp\\!\\Big(-\\kappa \\Big(\\frac{n\\pi}{L}\\Big)^2 t\\Big)\\,\\sin\\!\\Big(\\frac{n\\pi x}{L}\\Big).\n$$\nTwo pointwise sensor readings at strictly positive times are taken at locations $(x_1,t_1)$ and $(x_2,t_2)$, producing measurements $y_1$ and $y_2$ (in Kelvin). Assume an additive independent Gaussian noise model with known standard deviation $\\sigma$ (in Kelvin), so that\n$$\n\\begin{bmatrix} y_1 \\\\[4pt] y_2 \\end{bmatrix}\n\\;\\sim\\;\n\\mathcal{N}\\!\\Big(\\,\\Phi\\,\\theta,\\; \\sigma^2 I_2\\Big), \\quad\n\\Phi_{j,n} \\;=\\; \\exp\\!\\Big(-\\kappa \\Big(\\frac{n\\pi}{L}\\Big)^2 t_j\\Big)\\,\\sin\\!\\Big(\\frac{n\\pi x_j}{L}\\Big),\n$$\nwith $j\\in\\{1,2\\}$ and $n\\in\\{1,\\dots,M\\}$, and where $\\theta = [\\theta_1,\\dots,\\theta_M]^\\top$. Place a zero-mean Gaussian prior on the coefficients,\n$$\n\\theta \\;\\sim\\; \\mathcal{N}\\!\\big(0,\\;\\tau^2 I_M\\big),\n$$\nwith known prior standard deviation $\\tau$ (in Kelvin). Using Bayesian inference, one can obtain the posterior distribution of $\\theta$ conditioned on the two observations, and from it the posterior mean estimate of the initial profile $u(x,0)$.\n\nYour task is to write a complete program that, for each test case in the test suite below, computes the posterior mean estimate of the initial temperature $u(x,0)$ at three query positions $x_{\\mathrm{q}} \\in \\{x_{\\mathrm{q},1},x_{\\mathrm{q},2},x_{\\mathrm{q},3}\\}$ and returns these three values as floats. The forward model angles use radians. All lengths must be in meters, time in seconds, and temperature in Kelvin. The output values must be in Kelvin. The program must round each reported float to $6$ decimal places.\n\nFundamental base and modeling assumptions to use:\n- The heat equation as stated above with homogeneous Dirichlet boundary conditions and separation of variables into sine eigenfunctions.\n- The linear-Gaussian measurement model with independent and identically distributed Gaussian noise of variance $\\sigma^2$.\n- A zero-mean Gaussian prior on the coefficients with covariance $\\tau^2 I_M$.\n\nDerive from these bases the linear operator $\\Phi$ and the posterior distribution for $\\theta$ implied by Bayes' rule in a linear-Gaussian model. Use the posterior mean of $\\theta$ to form the posterior mean initial profile $\\widehat{u}(x,0) = \\sum_{n=1}^{M} \\widehat{\\theta}_n \\sin\\!\\big(\\frac{n\\pi x}{L}\\big)$, where $\\widehat{\\theta}$ is the posterior mean vector.\n\nAngle unit specification: all trigonometric evaluations use radians.\n\nFor reproducibility and to ensure scientific realism, in each test case below the observed data vector $y \\in \\mathbb{R}^2$ must be generated deterministically from a fixed reference coefficient vector $\\theta^{\\star}$ with zero additive noise, i.e.,\n$$\ny \\;=\\; \\Phi\\,\\theta^{\\star}.\n$$\nYour program must construct $y$ this way from the provided $\\theta^{\\star}$ and the stated parameters. The inference must still treat $\\sigma$ as specified in each test case.\n\nTest suite (three cases). For all cases, use $M=4$, $L=1.0$ (meters), and the same reference vector\n$$\n\\theta^{\\star} \\;=\\; [50.0,\\; -20.0,\\; 10.0,\\; 5.0]^\\top \\;\\text{(Kelvin)}.\n$$\nUse the three query positions\n$$\nx_{\\mathrm{q},1}=0.25,\\quad x_{\\mathrm{q},2}=0.50,\\quad x_{\\mathrm{q},3}=0.75 \\;\\text{(meters)}.\n$$\n- Case A (happy path, moderate times and noise): $\\kappa=1.1\\times 10^{-4}$ (square meters per second), $\\tau=50.0$ (Kelvin), $\\sigma=0.5$ (Kelvin), $(x_1,t_1)=(0.25,\\; 500.0)$, $(x_2,t_2)=(0.75,\\; 2000.0)$.\n- Case B (lower noise, different sensor locations and times): $\\kappa=1.1\\times 10^{-4}$, $\\tau=100.0$, $\\sigma=0.1$, $(x_1,t_1)=(0.30,\\; 200.0)$, $(x_2,t_2)=(0.60,\\; 800.0)$.\n- Case C (edge case: sensors near boundaries and late times): $\\kappa=1.1\\times 10^{-4}$, $\\tau=30.0$, $\\sigma=1.0$, $(x_1,t_1)=(0.05,\\; 1500.0)$, $(x_2,t_2)=(0.95,\\; 3000.0)$.\n\nRequired numerical outputs:\n- For each case, compute the posterior mean estimate of the initial temperature at the three query points, in Kelvin, as a list of three floats rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of these three-element lists, enclosed in square brackets, with no whitespace. For example, an output with generic placeholders would look like\n$[[a_1,a_2,a_3],[b_1,b_2,b_3],[c_1,c_2,c_3]]$,\nwhere each $a_i$, $b_i$, $c_i$ is a float number in Kelvin rounded to $6$ decimal places.\n\nYour program must not read any input. It must compute and print the required single-line output exactly as specified.",
            "solution": "The problem presented is a classical linear-Gaussian inverse problem, which seeks to infer the initial condition of a system governed by the one-dimensional heat equation from sparse, noisy measurements. The task is to compute the posterior mean estimate of the initial temperature profile, given the specified model structure. The derivation proceeds from the fundamental principles of Bayesian inference.\n\nThe statistical model is defined by two components: the likelihood and the prior.\n\nThe likelihood function quantifies the probability of observing the data vector $y = [y_1, y_2]^\\top$ given a specific set of model parameters $\\theta = [\\theta_1, \\dots, \\theta_M]^\\top$. The problem states a linear forward model with additive, independent, and identically distributed Gaussian noise:\n$$\ny = \\Phi\\theta + \\epsilon, \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_2)\n$$\nHere, $\\Phi \\in \\mathbb{R}^{2 \\times M}$ is the forward operator that maps the parameters $\\theta$ to the noise-free sensor outputs, and $\\epsilon$ is the measurement noise vector with known variance $\\sigma^2$. This implies the likelihood distribution is a multivariate Gaussian:\n$$\np(y|\\theta) = \\mathcal{N}(y | \\Phi\\theta, \\sigma^2 I_2) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\|y - \\Phi\\theta\\|^2_2 \\right)\n$$\n\nThe prior distribution encodes our belief about the parameters $\\theta$ before observing any data. It is given as a zero-mean Gaussian distribution with a spherical covariance structure, determined by the prior standard deviation $\\tau$:\n$$\np(\\theta) = \\mathcal{N}(\\theta | 0, \\tau^2 I_M) \\propto \\exp\\left( -\\frac{1}{2\\tau^2} \\|\\theta\\|^2_2 \\right)\n$$\nThis prior acts as a regularizer, penalizing solutions with large-magnitude coefficients and thus ensuring that the inverse problem is well-posed.\n\nAccording to Bayes' rule, the posterior distribution of the parameters given the data is proportional to the product of the likelihood and the prior:\n$$\np(\\theta|y) \\propto p(y|\\theta) p(\\theta)\n$$\nAs both the likelihood and the prior are Gaussian distributions, the resulting posterior distribution is also Gaussian. The posterior mean, which minimizes the expected squared error loss and coincides with the posterior mode for a symmetric posterior, is found by maximizing the posterior probability. This is equivalent to minimizing the negative of its logarithm, which forms the objective function $J(\\theta)$:\n$$\nJ(\\theta) = -\\log p(\\theta|y) = \\frac{1}{2\\sigma^2} (y - \\Phi\\theta)^\\top(y - \\Phi\\theta) + \\frac{1}{2\\tau^2} \\theta^\\top\\theta + \\text{constant}\n$$\nTo find the minimum of this quadratic function, we compute its gradient with respect to $\\theta$ and set it to zero:\n$$\n\\nabla_\\theta J(\\theta) = -\\frac{1}{\\sigma^2} \\Phi^\\top(y - \\Phi\\theta) + \\frac{1}{\\tau^2} \\theta = 0\n$$\nRearranging the terms to solve for $\\theta$ yields the following linear system:\n$$\n\\left( \\frac{1}{\\sigma^2} \\Phi^\\top\\Phi + \\frac{1}{\\tau^2} I_M \\right) \\theta = \\frac{1}{\\sigma^2} \\Phi^\\top y\n$$\nThe solution to this system is the posterior mean estimate, denoted $\\widehat{\\theta}$. By multiplying through by $\\sigma^2$, we obtain the standard formula used in ridge regression:\n$$\n\\widehat{\\theta} = \\left( \\Phi^\\top\\Phi + \\frac{\\sigma^2}{\\tau^2} I_M \\right)^{-1} \\Phi^\\top y\n$$\nThe term $\\alpha = (\\sigma/\\tau)^2$ is a Tikhonov regularization parameter, which balances the influence of the data fit (likelihood) against the simplicity of the solution (prior).\n\nOnce the posterior mean coefficient vector $\\widehat{\\theta}$ is computed, the posterior mean estimate of the initial temperature profile, $\\widehat{u}(x,0)$, is constructed using the given sine series expansion:\n$$\n\\widehat{u}(x,0) = \\sum_{n=1}^{M} \\widehat{\\theta}_n \\sin\\left(\\frac{n\\pi x}{L}\\right)\n$$\nThe computational procedure for each test case is therefore as follows:\n$1$. Using the given physical parameters ($L$, $\\kappa$), modeling parameters ($M$), and sensor locations and times ($(x_j, t_j)$), construct the $2 \\times M$ forward matrix $\\Phi$, where its elements are given by $\\Phi_{j,n} = \\exp(-\\kappa (n\\pi/L)^2 t_j)\\sin(n\\pi x_j/L)$.\n$2$. With the provided reference coefficient vector $\\theta^\\star$, generate the deterministic observation vector $y$ via the forward model: $y = \\Phi\\theta^\\star$.\n$3$. Compute the regularization parameter $\\alpha = (\\sigma/\\tau)^2$.\n$4$. Form the $M \\times M$ matrix $A = \\Phi^\\top\\Phi + \\alpha I_M$ and the $M \\times 1$ vector $b = \\Phi^\\top y$.\n$5$. Solve the linear system $A\\widehat{\\theta} = b$ for the posterior mean coefficient vector $\\widehat{\\theta}$. This is performed using a robust numerical linear solver.\n$6$. Construct a query matrix $S \\in \\mathbb{R}^{3 \\times M}$ where $S_{k,n} = \\sin(n\\pi x_{\\mathrm{q},k}/L)$ for the three specified query positions $x_{\\mathrm{q},k}$.\n$7$. Compute the final estimated temperatures at the query points as the matrix-vector product $u_{\\mathrm{q}} = S\\widehat{\\theta}$. This algorithm is implemented for each of the three test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior mean estimate of the initial temperature for a 1D heat\n    rod problem using Bayesian inference for three different test cases.\n    \"\"\"\n    \n    # Global parameters\n    M = 4\n    L = 1.0\n    theta_star = np.array([50.0, -20.0, 10.0, 5.0])\n    x_q = np.array([0.25, 0.50, 0.75])\n\n    # Test suite definition\n    test_cases = [\n        {\n            # Case A: happy path, moderate times and noise\n            \"kappa\": 1.1e-4, \"tau\": 50.0, \"sigma\": 0.5,\n            \"sensors\": [(0.25, 500.0), (0.75, 2000.0)]\n        },\n        {\n            # Case B: lower noise, different sensor locations and times\n            \"kappa\": 1.1e-4, \"tau\": 100.0, \"sigma\": 0.1,\n            \"sensors\": [(0.30, 200.0), (0.60, 800.0)]\n        },\n        {\n            # Case C: edge case: sensors near boundaries and late times\n            \"kappa\": 1.1e-4, \"tau\": 30.0, \"sigma\": 1.0,\n            \"sensors\": [(0.05, 1500.0), (0.95, 3000.0)]\n        }\n    ]\n\n    all_results = []\n    \n    # Mode indices n=1, ..., M\n    n_modes = np.arange(1, M + 1)\n\n    # Construct the query matrix S, which maps coefficients to temperatures at query points\n    # S has shape (3, M) where 3 is the number of query points\n    S = np.sin(n_modes * np.pi * x_q[:, np.newaxis] / L)\n\n    for case in test_cases:\n        kappa = case[\"kappa\"]\n        tau = case[\"tau\"]\n        sigma = case[\"sigma\"]\n        sensors = case[\"sensors\"]\n        \n        num_sensors = len(sensors)\n        \n        # Construct the forward operator matrix Phi (num_sensors x M)\n        Phi = np.zeros((num_sensors, M))\n        for j, (x_j, t_j) in enumerate(sensors):\n            decay_exponents = -kappa * (n_modes * np.pi / L)**2 * t_j\n            exp_terms = np.exp(decay_exponents)\n            sin_terms = np.sin(n_modes * np.pi * x_j / L)\n            Phi[j, :] = exp_terms * sin_terms\n            \n        # Generate the synthetic data vector y = Phi * theta_star\n        y = Phi @ theta_star\n        \n        # Calculate the posterior mean coefficient vector theta_hat from:\n        # (Phi^T * Phi + (sigma/tau)^2 * I) * theta_hat = Phi^T * y\n        regularization_param = (sigma / tau)**2\n        A = Phi.T @ Phi + regularization_param * np.eye(M)\n        b = Phi.T @ y\n        \n        # Solve the linear system for theta_hat\n        theta_hat = np.linalg.solve(A, b)\n        \n        # Compute the posterior mean initial profile at query points\n        u_hat_q = S @ theta_hat\n        \n        all_results.append(u_hat_q)\n\n    # Format the final output string exactly as specified, with no whitespace\n    # and 6 decimal places for each float number.\n    outer_list_str = []\n    for inner_list in all_results:\n        inner_list_str_parts = [f\"{x:.6f}\" for x in inner_list]\n        outer_list_str.append(f\"[{','.join(inner_list_str_parts)}]\")\n    final_output_str = f\"[{','.join(outer_list_str)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```"
        }
    ]
}