{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a tangible application of multidimensional root-finding to a problem in classical mechanics. Your task is to determine the static equilibrium configuration of a tensegrity structure, a system where rigid struts are held in tension by elastic cables. This practice is valuable because it demonstrates how to translate fundamental physical principles, such as force balance and geometric constraints, into a system of nonlinear equations ready for numerical solution. ",
            "id": "2415350",
            "problem": "You are given a two-dimensional model of a minimal tensegrity structure composed of two free nodes connected by one inextensible strut and several elastic cables attached to fixed anchors. The objective is to determine the equilibrium configuration, defined as the configuration in which the net force on each free node is zero and each strut length constraint is exactly satisfied. All distances must be expressed in meters.\n\nModeling assumptions:\n- Each cable is modeled as a linear spring obeying Hooke’s law. If a free node at position $\\mathbf{r}_i \\in \\mathbb{R}^2$ is connected by a cable to a fixed anchor at position $\\mathbf{a} \\in \\mathbb{R}^2$ with stiffness $k$ and rest length $L_0$, then the force exerted on the node by that cable is\n$$\n\\mathbf{F}_{\\text{cable}} = k \\,\\frac{\\|\\mathbf{a}-\\mathbf{r}_i\\| - L_0}{\\|\\mathbf{a}-\\mathbf{r}_i\\|} \\, (\\mathbf{a}-\\mathbf{r}_i),\n$$\nwith the convention that all forces are in newtons and all lengths are in meters.\n- Each strut is perfectly inextensible and massless, with a prescribed length $L_s$. For a strut connecting two free nodes at positions $\\mathbf{r}_i$ and $\\mathbf{r}_j$, introduce a scalar Lagrange multiplier $\\lambda$ to enforce the constraint\n$$\ng(\\mathbf{r}_i,\\mathbf{r}_j) = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|^2 - L_s^2 = 0.\n$$\nThis produces equal and opposite internal forces $\\mathbf{F}_{\\text{strut},i} = \\lambda \\, (\\mathbf{r}_i - \\mathbf{r}_j)$ on node $i$ and $\\mathbf{F}_{\\text{strut},j} = -\\lambda \\, (\\mathbf{r}_i - \\mathbf{r}_j)$ on node $j$.\n- At equilibrium, for each free node $i$, the vector sum of all incident cable forces and strut forces must be exactly $\\mathbf{0}$, and every strut constraint must satisfy $g=0$.\n\nUnknowns and equations:\n- Let the two free nodes be $C$ and $D$, with unknown coordinates $\\mathbf{r}_C = (x_C, y_C)$ and $\\mathbf{r}_D = (x_D, y_D)$.\n- There is one strut between $C$ and $D$ with unknown Lagrange multiplier $\\lambda$ and prescribed length $L_s$.\n- For each free node, write the two scalar equilibrium equations for the $x$- and $y$-components of the net force being zero. Append the scalar strut constraint equation. The total number of equations is $5$, matching the number of unknowns $x_C$, $y_C$, $x_D$, $y_D$, and $\\lambda$.\n\nTest suite:\nFor each test case, you are given the fixed anchor positions, the cable parameters, the strut length, and an initial guess for $(x_C, y_C, x_D, y_D)$. All coordinates and lengths are in meters, and all stiffnesses are in newtons per meter.\n\n- Test case $1$ (symmetric base, moderate pretension):\n  - Anchors: $A=(0.0, 0.0)$, $B=(1.0, 0.0)$.\n  - Cables: $C$–$A$, $C$–$B$, $D$–$A$, $D$–$B$, each with stiffness $k = 100.0$ and rest length $L_0 = 0.7$.\n  - Strut between $C$ and $D$ with prescribed length $L_s = 1.0$.\n  - Initial guess for free nodes: $(x_C, y_C, x_D, y_D)=(0.0, 0.5, 1.0, 0.5)$.\n\n- Test case $2$ (longer strut, softer cables):\n  - Anchors: $A=(0.0, 0.0)$, $B=(1.0, 0.0)$.\n  - Cables: $C$–$A$, $C$–$B$, $D$–$A$, $D$–$B$, each with stiffness $k = 80.0$ and rest length $L_0 = 0.8$.\n  - Strut between $C$ and $D$ with prescribed length $L_s = 1.2$.\n  - Initial guess for free nodes: $(x_C, y_C, x_D, y_D)=(0.0, 0.6, 1.2, 0.6)$.\n\n- Test case $3$ (asymmetric base, stronger cables):\n  - Anchors: $A=(0.0, 0.0)$, $B=(1.2, 0.0)$.\n  - Cables: $C$–$A$, $C$–$B$, $D$–$A$, $D$–$B$, each with stiffness $k = 120.0$ and rest length $L_0 = 0.75$.\n  - Strut between $C$ and $D$ with prescribed length $L_s = 1.1$.\n  - Initial guess for free nodes: $(x_C, y_C, x_D, y_D)=(0.0, 0.55, 1.1, 0.55)$.\n\nTask:\n- For each test case, solve for $(x_C, y_C, x_D, y_D)$ and $\\lambda$ such that the two-vector equilibrium at node $C$, the two-vector equilibrium at node $D$, and the scalar strut constraint are all satisfied exactly. Report only the coordinates $(x_C, y_C, x_D, y_D)$ for each test case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list containing the four coordinates for a test case in the order $[x_C, y_C, x_D, y_D]$.\n- Express every coordinate in meters, rounded to six decimal places.\n- The exact required format is:\n  - An opening square bracket, followed by three bracketed comma-separated lists of four decimal numbers each (rounded to six decimal places), separated by commas, and a closing square bracket, with no spaces.\n  - For example: $[[x_{C1},y_{C1},x_{D1},y_{D1}],[x_{C2},y_{C2},x_{D2},y_{D2}],[x_{C3},y_{C3},x_{D3},y_{D3}]]$ where all entries are decimal numbers in meters rounded to six decimal places.",
            "solution": "The problem requires finding the static equilibrium configuration of a two-dimensional tensegrity structure. This is a classic problem in computational mechanics, which reduces to finding the roots of a system of nonlinear algebraic equations. The system is defined by the principles of force balance and geometric constraints.\n\nThe unknowns of the system form a $5$-dimensional state vector $\\mathbf{x} = (x_C, y_C, x_D, y_D, \\lambda)^T$, where $\\mathbf{r}_C = (x_C, y_C)$ and $\\mathbf{r}_D = (x_D, y_D)$ are the coordinates of the two free nodes, and $\\lambda$ is the Lagrange multiplier associated with the strut length constraint. The problem is to solve the vector equation $\\mathbf{H}(\\mathbf{x}) = \\mathbf{0}$, where $\\mathbf{H}$ is a vector-valued function of $\\mathbf{x}$ representing the physical laws governing the system.\n\nThe function $\\mathbf{H}: \\mathbb{R}^5 \\to \\mathbb{R}^5$ is composed of five scalar equations. The first four equations enforce static equilibrium, stating that the net force on each free node is zero. The fifth equation is the geometric constraint that the strut maintains its prescribed length.\n\nLet the anchor positions be $\\mathbf{a}_A$ and $\\mathbf{a}_B$. The net force on node $C$, denoted $\\mathbf{F}_C$, is the vector sum of forces from the two cables connecting it to anchors $A$ and $B$, and the internal force from the strut connecting it to node $D$.\n$$\n\\mathbf{F}_C(\\mathbf{r}_C, \\mathbf{r}_D, \\lambda) = \\mathbf{F}_{CA}(\\mathbf{r}_C) + \\mathbf{F}_{CB}(\\mathbf{r}_C) + \\mathbf{F}_{\\text{strut},C}(\\mathbf{r}_C, \\mathbf{r}_D) = \\mathbf{0}\n$$\nAccording to the provided model, the cable force exerted on node $C$ by the cable connected to an anchor $\\mathbf{a}$ is:\n$$\n\\mathbf{F}_{\\text{cable}}(\\mathbf{r}_C, \\mathbf{a}) = k \\frac{\\|\\mathbf{a}-\\mathbf{r}_C\\| - L_0}{\\|\\mathbf{a}-\\mathbf{r}_C\\|} (\\mathbf{a}-\\mathbf{r}_C)\n$$\nThe force exerted on node $C$ by the strut connected to node $D$ is:\n$$\n\\mathbf{F}_{\\text{strut},C}(\\mathbf{r}_C, \\mathbf{r}_D) = \\lambda (\\mathbf{r}_C - \\mathbf{r}_D)\n$$\nSimilarly, the net force on node $D$, denoted $\\mathbf{F}_D$, is:\n$$\n\\mathbf{F}_D(\\mathbf{r}_C, \\mathbf{r}_D, \\lambda) = \\mathbf{F}_{DA}(\\mathbf{r}_D) + \\mathbf{F}_{DB}(\\mathbf{r}_D) + \\mathbf{F}_{\\text{strut},D}(\\mathbf{r}_C, \\mathbf{r}_D) = \\mathbf{0}\n$$\nThe cable forces $\\mathbf{F}_{DA}$ and $\\mathbf{F}_{DB}$ are defined analogously. The strut exerts an equal and opposite force on node $D$:\n$$\n\\mathbf{F}_{\\text{strut},D}(\\mathbf{r}_C, \\mathbf{r}_D) = -\\lambda (\\mathbf{r}_C - \\mathbf{r}_D) = \\lambda (\\mathbf{r}_D - \\mathbf{r}_C)\n$$\nThe fifth equation is the holonomic constraint for the strut of length $L_s$:\n$$\ng(\\mathbf{r}_C, \\mathbf{r}_D) = \\|\\mathbf{r}_C - \\mathbf{r}_D\\|^2 - L_s^2 = (x_C - x_D)^2 + (y_C - y_D)^2 - L_s^2 = 0\n$$\nThese equilibrium and constraint conditions form the system of five nonlinear scalar equations, $H_i(\\mathbf{x})=0$ for $i=1, \\dots, 5$:\n$H_1: k\\frac{\\|\\mathbf{a}_A-\\mathbf{r}_C\\| - L_0}{\\|\\mathbf{a}_A-\\mathbf{r}_C\\|}(a_{Ax}-x_C) + k\\frac{\\|\\mathbf{a}_B-\\mathbf{r}_C\\| - L_0}{\\|\\mathbf{a}_B-\\mathbf{r}_C\\|}(a_{Bx}-x_C) + \\lambda(x_C-x_D) = 0$\n$H_2: k\\frac{\\|\\mathbf{a}_A-\\mathbf{r}_C\\| - L_0}{\\|\\mathbf{a}_A-\\mathbf{r}_C\\|}(a_{Ay}-y_C) + k\\frac{\\|\\mathbf{a}_B-\\mathbf{r}_C\\| - L_0}{\\|\\mathbf{a}_B-\\mathbf{r}_C\\|}(a_{By}-y_C) + \\lambda(y_C-y_D) = 0$\n$H_3: k\\frac{\\|\\mathbf{a}_A-\\mathbf{r}_D\\| - L_0}{\\|\\mathbf{a}_A-\\mathbf{r}_D\\|}(a_{Ax}-x_D) + k\\frac{\\|\\mathbf{a}_B-\\mathbf{r}_D\\| - L_0}{\\|\\mathbf{a}_B-\\mathbf{r}_D\\|}(a_{Bx}-x_D) + \\lambda(x_D-x_C) = 0$\n$H_4: k\\frac{\\|\\mathbf{a}_A-\\mathbf{r}_D\\| - L_0}{\\|\\mathbf{a}_A-\\mathbf{r}_D\\|}(a_{Ay}-y_D) + k\\frac{\\|\\mathbf{a}_B-\\mathbf{r}_D\\| - L_0}{\\|\\mathbf{a}_B-\\mathbf{r}_D\\|}(a_{By}-y_D) + \\lambda(y_D-y_C) = 0$\n$H_5: (x_C - x_D)^2 + (y_C - y_D)^2 - L_s^2 = 0$\n\nThis system is not amenable to analytical solution and must be solved numerically. A standard technique for such problems is the Newton-Raphson method. This iterative method starts with an initial guess $\\mathbf{x}_0$ and refines it according to the update rule $\\mathbf{x}_{n+1} = \\mathbf{x}_n - J^{-1}(\\mathbf{x}_n) \\mathbf{H}(\\mathbf{x}_n)$, where $J$ is the Jacobian matrix of $\\mathbf{H}$. For practical implementation, robust numerical solvers such as those provided by the `scipy.optimize` library are employed.\n\nThe solution is implemented by first defining a Python function that computes the vector $\\mathbf{H}(\\mathbf{x})$ for a given state vector $\\mathbf{x}$ and a set of physical parameters (anchor locations, $k$, $L_0$, $L_s$). An initial guess for the state vector, $\\mathbf{x}_0$, is constructed from the coordinates provided in each test case, augmented with an initial guess for the Lagrange multiplier, which can be reasonably set to $\\lambda_0 = 0$. The `scipy.optimize.root` function is then invoked to find the solution $\\mathbf{x}^*$ that satisfies $\\mathbf{H}(\\mathbf{x}^*) \\approx \\mathbf{0}$. This process is repeated for each of the three test cases specified in the problem statement. The final reported result consists of the coordinate components $(x_C, y_C, x_D, y_D)$ of the solution vector $\\mathbf{x}^*$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import root\n\ndef solve():\n    \"\"\"\n    Solves for the equilibrium configuration of a tensegrity structure for given test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"anchors\": {\"A\": np.array([0.0, 0.0]), \"B\": np.array([1.0, 0.0])},\n            \"k\": 100.0,\n            \"L0\": 0.7,\n            \"Ls\": 1.0,\n            \"initial_guess\": np.array([0.0, 0.5, 1.0, 0.5])\n        },\n        # Test case 2\n        {\n            \"anchors\": {\"A\": np.array([0.0, 0.0]), \"B\": np.array([1.0, 0.0])},\n            \"k\": 80.0,\n            \"L0\": 0.8,\n            \"Ls\": 1.2,\n            \"initial_guess\": np.array([0.0, 0.6, 1.2, 0.6])\n        },\n        # Test case 3\n        {\n            \"anchors\": {\"A\": np.array([0.0, 0.0]), \"B\": np.array([1.2, 0.0])},\n            \"k\": 120.0,\n            \"L0\": 0.75,\n            \"Ls\": 1.1,\n            \"initial_guess\": np.array([0.0, 0.55, 1.1, 0.55])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        aA = case[\"anchors\"][\"A\"]\n        aB = case[\"anchors\"][\"B\"]\n        k = case[\"k\"]\n        L0 = case[\"L0\"]\n        Ls = case[\"Ls\"]\n        initial_coords = case[\"initial_guess\"]\n\n        def equations(x):\n            \"\"\"\n            Defines the system of 5 non-linear equations for equilibrium.\n            x: state vector [xc, yc, xd, yd, lambda]\n            \"\"\"\n            xc, yc, xd, yd, lam = x\n            rC = np.array([xc, yc])\n            rD = np.array([xd, yd])\n            \n            # Helper for cable forces to avoid code repetition.\n            # Handles the force model F = k * (1 - L0/d) * (a - r)\n            def cable_force(node_pos, anchor_pos):\n                vec = anchor_pos - node_pos\n                dist = np.linalg.norm(vec)\n                # Add a small epsilon to prevent division by zero if a node is at an anchor\n                if dist  1e-9:\n                    return np.array([0.0, 0.0])\n                force_magnitude_factor = k * (1.0 - L0 / dist)\n                return force_magnitude_factor * vec\n\n            # Forces on node C\n            F_CA = cable_force(rC, aA)\n            F_CB = cable_force(rC, aB)\n            F_strut_C = lam * (rC - rD)\n            F_net_C = F_CA + F_CB + F_strut_C\n\n            # Forces on node D\n            F_DA = cable_force(rD, aA)\n            F_DB = cable_force(rD, aB)\n            F_strut_D = lam * (rD - rC)\n            F_net_D = F_DA + F_DB + F_strut_D\n\n            # Strut length constraint\n            g = np.linalg.norm(rC - rD)**2 - Ls**2\n\n            return np.array([F_net_C[0], F_net_C[1], F_net_D[0], F_net_D[1], g])\n\n        # Initial guess for the full state vector (coords + lambda)\n        x0 = np.append(initial_coords, 0.0)\n\n        # Solve the system of equations\n        sol = root(equations, x0, method='hybr')\n        \n        if sol.success:\n            # Extract the coordinates from the solution vector\n            result_coords = sol.x[:4]\n            results.append(result_coords)\n        else:\n            # Append a failure indicator if solver does not converge.\n            # This should not happen for the given test cases.\n            results.append([np.nan, np.nan, np.nan, np.nan])\n    \n    # Format the output string exactly as required\n    formatted_lists = []\n    for r in results:\n        formatted_lists.append(f\"[{','.join([f'{val:.6f}' for val in r])}]\")\n    \n    final_output = f\"[{','.join(formatted_lists)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the setup of a physical system, this problem explores a classic challenge in computational geometry known as the Problem of Apollonius. You will be implementing a Newton-Raphson solver to find a circle that is tangent to three other given circles, a situation governed by a neat but nonlinear set of equations. This practice focuses on the core algorithm, requiring you to analytically derive the Jacobian matrix and confront a common challenge in nonlinear systems: the existence of multiple distinct solutions. ",
            "id": "2415340",
            "problem": "You are to compute the parameters of a circle in the plane that is simultaneously tangent to three given circles under the condition of external tangency. The desired circle has center $\\,(x,y)\\,$ and radius $\\,r \\ge 0\\,$ and must satisfy the following geometric constraint for each given circle with center $\\,(x_i,y_i)\\,$ and radius $\\,r_i \\ge 0\\,$: the Euclidean distance between centers equals the sum of radii. Formally, using the Euclidean norm and basic geometry, external tangency between two circles is defined by the condition that the distance between their centers equals the sum of their radii. This leads to the system of equations\n$$\n\\sqrt{(x-x_i)^2+(y-y_i)^2} - \\big(r + r_i\\big) = 0,\\quad i\\in\\{1,2,3\\}.\n$$\nThis is the classical problem of Apollonius specialized to all-external tangency. In general, there can be up to $\\,2\\,$ distinct solutions for the all-external case, or fewer in degenerate configurations. Your task is to find all solutions with $\\,r \\ge 0\\,$ that satisfy the system to within a tight numerical tolerance.\n\nDerive your method from the following fundamental base:\n- The Euclidean distance between two points $\\,(x,y)\\,$ and $\\,(x_i,y_i)\\,$ is $\\,\\sqrt{(x-x_i)^2+(y-y_i)^2}\\,$.\n- The definition of external tangency of circles states that the distance between centers equals the sum of the radii. \n- Multidimensional root-finding seeks $\\,\\mathbf{u}^\\star \\in \\mathbb{R}^n\\,$ such that $\\,\\mathbf{F}(\\mathbf{u}^\\star)=\\mathbf{0}\\,$, typically by iteratively linearizing with the Jacobian and updating using a Newton-type step.\n\nYour program must:\n- Formulate the system of three nonlinear equations in the unknowns $\\,x\\,$, $\\,y\\,$, and $\\,r\\,$ using the above geometric constraints.\n- Use a multidimensional root-finding method based on Newton’s method in three dimensions to solve for $\\,x\\,$, $\\,y\\,$, and $\\,r\\,$. You must derive an analytic Jacobian from first principles of differentiation of the Euclidean distance. \n- Find all distinct solutions with $\\,r \\ge 0\\,$ by using multiple initial guesses, and deduplicate solutions that are numerically identical within a small tolerance. \n- Enforce external tangency by constraining $\\,r \\ge 0\\,$ throughout the search.\n- Accept a solution if the maximum absolute residual across the three equations is at most $\\,10^{-8}\\,$.\n\nNumerical units: There are no physical units in this problem. Angles are not involved.\n\nTest suite:\n- Case $\\,1\\,$ (general configuration, two solutions expected): three circles with parameters $\\,(x_1,y_1,r_1)=(0.0,0.0,1.0)\\,$, $\\,(x_2,y_2,r_2)=(4.0,0.0,1.0)\\,$, and $\\,(x_3,y_3,r_3)=(0.0,3.0,0.5)\\,$.\n- Case $\\,2\\,$ (boundary configuration with degenerate radii, one solution expected): three circles with parameters $\\,(x_1,y_1,r_1)=(0.0,0.0,0.0)\\,$, $\\,(x_2,y_2,r_2)=(4.0,0.0,0.0)\\,$, and $\\,(x_3,y_3,r_3)=(0.0,3.0,0.0)\\,$.\n- Case $\\,3\\,$ (asymmetric configuration, two solutions expected): three circles with parameters $\\,(x_1,y_1,r_1)=(0.0,0.0,1.0)\\,$, $\\,(x_2,y_2,r_2)=(6.0,0.0,2.0)\\,$, and $\\,(x_3,y_3,r_3)=(2.0,5.0,0.5)\\,$.\n\nAnswer specification:\n- For each test case, find all distinct solutions $\\,(x,y,r)\\,$ with $\\,r \\ge 0\\,$ that satisfy the system to within tolerance. \n- Sort the solutions for each case in ascending order by $\\,r\\,$; if two solutions have radii equal within $\\,10^{-6}\\,$, break ties lexicographically by $\\,x\\,$ and then $\\,y\\,$.\n- Round each of $\\,x\\,$, $\\,y\\,$, and $\\,r\\,$ to $\\,6\\,$ decimal places in the final output.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of solution triples. For example:\n\"[ [ [x11,y11,r11], [x12,y12,r12] ], [ [x21,y21,r21] ], [ [x31,y31,r31], [x32,y32,r32] ] ]\"\nwith each numeric entry rounded as instructed.",
            "solution": "The user-provided problem is rigorously validated and found to be well-posed, scientifically grounded, and objective. It is a standard problem in computational geometry, a specialization of the Problem of Apollonius, which is solvable using numerical methods. All necessary data are provided, and the problem is free of contradictions or ambiguities.\n\nThe task is to find a circle with center $(x, y)$ and radius $r$ that is externally tangent to three given circles, each defined by its center $(x_i, y_i)$ and radius $r_i$ for $i \\in \\{1, 2, 3\\}$.\n\n**1. Mathematical Formulation**\n\nThe geometric condition for external tangency between the solution circle $(x, y, r)$ and a given circle $(x_i, y_i, r_i)$ is that the Euclidean distance between their centers is equal to the sum of their radii. This gives us a system of three nonlinear equations:\n$$\nd_i = r + r_i, \\quad i \\in \\{1, 2, 3\\}\n$$\nwhere $d_i = \\sqrt{(x-x_i)^2 + (y-y_i)^2}$ is the distance between the centers. To solve this using a root-finding algorithm, we formulate the system as $\\mathbf{F}(\\mathbf{u}) = \\mathbf{0}$, where $\\mathbf{u} = [x, y, r]^T$ is the vector of unknowns. The functions $f_i$ are defined as the residuals:\n$$\nf_i(x,y,r) = \\sqrt{(x-x_i)^2 + (y-y_i)^2} - r - r_i = 0\n$$\nThis gives the system:\n$$\n\\mathbf{F}(\\mathbf{u}) =\n\\begin{pmatrix}\nf_1(x,y,r) \\\\\nf_2(x,y,r) \\\\\nf_3(x,y,r)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\sqrt{(x-x_1)^2+(y-y_1)^2} - r - r_1 \\\\\n\\sqrt{(x-x_2)^2+(y-y_2)^2} - r - r_2 \\\\\n\\sqrt{(x-x_3)^2+(y-y_3)^2} - r - r_3\n\\end{pmatrix}\n=\n\\mathbf{0}\n$$\nWe seek solutions $\\mathbf{u}^\\star$ that satisfy this system, subject to the physical constraint $r \\ge 0$.\n\n**2. Methodology: Newton-Raphson Method**\n\nThis system of nonlinear equations is solved using the Newton-Raphson method. It is an iterative algorithm that starts with an initial guess $\\mathbf{u}_0$ and generates a sequence of approximations $\\mathbf{u}_k$ that converge to a root $\\mathbf{u}^\\star$. The core of the method is the linearization of the system at each step using the Jacobian matrix $J$. The update rule is given by:\n$$\n\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\Delta \\mathbf{u}_k\n$$\nwhere the update step $\\Delta \\mathbf{u}_k$ is the solution to the linear system:\n$$\nJ(\\mathbf{u}_k) \\Delta \\mathbf{u}_k = -\\mathbf{F}(\\mathbf{u}_k)\n$$\n\n**3. Jacobian Matrix Derivation**\n\nThe Jacobian matrix $J(\\mathbf{u})$ is a $3 \\times 3$ matrix whose elements are the partial derivatives of the functions $f_i$ with respect to the variables $x, y, r$.\n$$\nJ(\\mathbf{u}) = \\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x}  \\frac{\\partial f_1}{\\partial y}  \\frac{\\partial f_1}{\\partial r} \\\\\n\\frac{\\partial f_2}{\\partial x}  \\frac{\\partial f_2}{\\partial y}  \\frac{\\partial f_2}{\\partial r} \\\\\n\\frac{\\partial f_3}{\\partial x}  \\frac{\\partial f_3}{\\partial y}  \\frac{\\partial f_3}{\\partial r}\n\\end{pmatrix}\n$$\nWe compute the partial derivatives for a generic function $f_i$. Let $d_i = \\sqrt{(x-x_i)^2 + (y-y_i)^2}$.\nThe derivative with respect to $x$ is:\n$$\n\\frac{\\partial f_i}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( d_i - r - r_i \\right) = \\frac{\\partial d_i}{\\partial x} = \\frac{1}{2 d_i} \\cdot 2(x - x_i) = \\frac{x-x_i}{d_i}\n$$\nThe derivative with respect to $y$ is:\n$$\n\\frac{\\partial f_i}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( d_i - r - r_i \\right) = \\frac{\\partial d_i}{\\partial y} = \\frac{1}{2 d_i} \\cdot 2(y - y_i) = \\frac{y-y_i}{d_i}\n$$\nThe derivative with respect to $r$ is:\n$$\n\\frac{\\partial f_i}{\\partial r} = \\frac{\\partial}{\\partial r} \\left( d_i - r - r_i \\right) = -1\n$$\nThese derivatives are well-defined provided $d_i \\neq 0$, i.e., the center of the solution circle does not coincide with the center of any given circle. Assembling these gives the complete Jacobian matrix:\n$$\nJ(x, y, r) = \\begin{pmatrix}\n\\frac{x-x_1}{d_1}  \\frac{y-y_1}{d_1}  -1 \\\\\n\\frac{x-x_2}{d_2}  \\frac{y-y_2}{d_2}  -1 \\\\\n\\frac{x-x_3}{d_3}  \\frac{y-y_3}{d_3}  -1\n\\end{pmatrix}\n$$\n\n**4. Algorithmic Strategy**\n\nThe Newton-Raphson method is a local search algorithm, meaning it will converge to a single root that is \"close\" to the initial guess. As the problem may have multiple solutions (typically zero, one, or two for this configuration), a strategy of using multiple, diverse initial guesses is required to find all distinct solutions.\n\nOur strategy employs the following initial guesses for $(x, y, r)$:\n1.  **Centroid-based guess:** The center $(x, y)$ is initialized to the centroid of the centers of the three given circles. The radius $r$ is initialized to $0$. This guess is aimed at finding a small solution circle located in the gap between the three given circles.\n2.  **Circumcenter-based guess:** The center $(x, y)$ is initialized to the circumcenter of the triangle formed by the three given circle centers. The initial radius is estimated based on the distance from this circumcenter to one of the circle centers. This guess is designed to find a large circle that envelops the three given circles. This is only applicable if the centers are not collinear.\n3.  **Midpoint-based guesses:** The center $(x,y)$ is initialized to the midpoint of the centers of each pair of given circles, with an initial radius of $0$. These serve as additional starting points to improve the chances of finding all solutions.\n\nFor each initial guess, the Newton-Raphson iteration proceeds until either the norm of the update step $\\Delta\\mathbf{u}_k$ falls below a tolerance, or a maximum number of iterations is reached. A potential solution is accepted if it satisfies two criteria:\n1.  The maximum absolute value of the final residual vector, $\\|\\mathbf{F}(\\mathbf{u}^\\star)\\|_\\infty$, is less than or equal to $10^{-8}$.\n2.  The radius $r$ is non-negative, i.e., $r \\ge 0$.\n\nFinally, the collected set of valid solutions is post-processed to remove duplicates. Any two solutions that are closer than a tolerance of $10^{-6}$ in the infinity norm are considered identical. The unique solutions for each test case are then sorted in ascending order of radius $r$, with ties broken lexicographically by $x$ and then $y$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the parameters of circles externally tangent to three given circles.\n    \"\"\"\n    test_cases = [\n        # Case 1: (x1,y1,r1), (x2,y2,r2), (x3,y3,r3)\n        [(0.0, 0.0, 1.0), (4.0, 0.0, 1.0), (0.0, 3.0, 0.5)],\n        # Case 2\n        [(0.0, 0.0, 0.0), (4.0, 0.0, 0.0), (0.0, 3.0, 0.0)],\n        # Case 3\n        [(0.0, 0.0, 1.0), (6.0, 0.0, 2.0), (2.0, 5.0, 0.5)],\n    ]\n\n    all_results = []\n\n    for circles in test_cases:\n        c1, c2, c3 = [np.array(c) for c in circles]\n        \n        # --- Generate initial guesses ---\n        initial_guesses = []\n        \n        # 1. Centroid of centers, r=0\n        centroid = (c1[:2] + c2[:2] + c3[:2]) / 3.0\n        initial_guesses.append(np.array([centroid[0], centroid[1], 0.0]))\n        \n        # 2. Circumcenter of centers\n        x1, y1, r1 = c1\n        x2, y2, r2 = c2\n        x3, y3, r3 = c3\n        \n        D_mat = 2 * np.array([[x2 - x1, y2 - y1], [x3 - x2, y3 - y2]])\n        b_vec = np.array([x2**2 + y2**2 - x1**2 - y1**2, \n                          x3**2 + y3**2 - x2**2 - y2**2])\n        \n        if np.linalg.det(D_mat) > 1e-9:\n            try:\n                circ_center = np.linalg.solve(D_mat, b_vec)\n                r_est = np.linalg.norm(circ_center - c1[:2]) - r1\n                initial_guesses.append(np.array([circ_center[0], circ_center[1], max(0, r_est)]))\n            except np.linalg.LinAlgError:\n                pass\n\n        # 3. Midpoints of centers, r=0\n        mid12 = (c1[:2] + c2[:2]) / 2.0\n        initial_guesses.append(np.array([mid12[0], mid12[1], 0.0]))\n        mid13 = (c1[:2] + c3[:2]) / 2.0\n        initial_guesses.append(np.array([mid13[0], mid13[1], 0.0]))\n        mid23 = (c2[:2] + c3[:2]) / 2.0\n        initial_guesses.append(np.array([mid23[0], mid23[1], 0.0]))\n\n        solutions = []\n        for u0 in initial_guesses:\n            sol = newton_solver(u0, circles)\n            if sol is not None:\n                solutions.append(sol)\n        \n        # --- Deduplicate solutions ---\n        unique_solutions = []\n        if solutions:\n            # Sort to make deduplication deterministic\n            solutions.sort(key=lambda s: (s[2], s[0], s[1]))\n            unique_solutions.append(solutions[0])\n            for sol in solutions[1:]:\n                is_duplicate = False\n                for unique_sol in unique_solutions:\n                    if np.linalg.norm(sol - unique_sol, ord=np.inf)  1e-6:\n                        is_duplicate = True\n                        break\n                if not is_duplicate:\n                    unique_solutions.append(sol)\n        \n        # --- Sort solutions ---\n        # Tie-breaking with 1e-6 tolerance on radii as per problem statement\n        unique_solutions.sort(key=lambda s: (round(s[2] / 1e-6) * 1e-6, s[0], s[1]))\n        \n        # --- Format for output ---\n        formatted_solutions = [[round(val, 6) for val in sol] for sol in unique_solutions]\n        all_results.append(formatted_solutions)\n\n    print(str(all_results).replace(\"'\", '\"'))\n\ndef newton_solver(u0, circles, max_iter=100, step_tol=1e-9, residual_tol=1e-8):\n    \"\"\"\n    Solves the system F(u)=0 using Newton's method.\n    u = [x, y, r]\n    \"\"\"\n    u = np.copy(u0)\n    \n    for _ in range(max_iter):\n        x, y, r = u\n        \n        # Calculate F(u)\n        F = np.zeros(3)\n        distances = np.zeros(3)\n        for i, c in enumerate(circles):\n            xi, yi, ri = c\n            d = np.sqrt((x - xi)**2 + (y - yi)**2)\n            if d  1e-9: # Avoid division by zero\n                return None\n            distances[i] = d\n            F[i] = d - r - ri\n        \n        # Calculate Jacobian J(u)\n        J = np.zeros((3, 3))\n        for i, c in enumerate(circles):\n            xi, yi, _ = c\n            J[i, 0] = (x - xi) / distances[i]\n            J[i, 1] = (y - yi) / distances[i]\n            J[i, 2] = -1.0\n            \n        # Solve J * du = -F\n        try:\n            du = np.linalg.solve(J, -F)\n        except np.linalg.LinAlgError:\n            return None # Singular matrix\n        \n        # Update u\n        u += du\n        \n        # Check for step convergence\n        if np.linalg.norm(du, ord=np.inf)  step_tol:\n            break\n            \n    # Final validation\n    x, y, r = u\n    F_final = np.zeros(3)\n    for i, c in enumerate(circles):\n        xi, yi, ri = c\n        d = np.sqrt((x - xi)**2 + (y - yi)**2)\n        F_final[i] = d - r - ri\n        \n    if np.linalg.norm(F_final, ord=np.inf) = residual_tol and u[2] >= -1e-9: # Allow for small numerical error around zero radius\n        u[2] = max(0.0, u[2]) # ensure r is not negative\n        return u\n        \n    return None\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice introduces a more advanced and powerful application of root-finding: parameter continuation. Instead of finding a single solution for a fixed system, you will trace a path of solutions as a parameter $\\alpha$ is varied. This exercise is crucial for understanding how to explore the behavior of a system across a range of conditions, a technique fundamental to studying a system's stability and discovering critical points like bifurcations. ",
            "id": "2415400",
            "problem": "A parametric vector equation is given by the nonlinear map $F:\\mathbb{R}^2 \\times \\mathbb{R} \\to \\mathbb{R}^2$ defined as\n$$\nF(\\mathbf{x},\\alpha) = \\begin{bmatrix}\n\\sin(x_1) + x_2 - \\alpha \\\\\nx_1 + \\cos(x_2) - 1\n\\end{bmatrix},\n$$\nwhere $\\mathbf{x} = (x_1,x_2)^{\\top}$ and $\\alpha$ is a scalar parameter. All angles must be interpreted in radians. A root is any $\\mathbf{x}_{\\star}$ such that $F(\\mathbf{x}_{\\star},\\alpha)=\\mathbf{0}$.\n\nYour task is to compute a solution path $\\{\\mathbf{x}_k\\}_{k=0}^{N}$ that approximately satisfies $F(\\mathbf{x}_k,\\alpha_k)=\\mathbf{0}$ for a sequence of parameter values $\\{\\alpha_k\\}_{k=0}^{N}$, where each $\\alpha_k$ is generated by a uniform partition between a specified start value $\\alpha_0$ and end value $\\alpha_N$. Use the previously found solution $\\mathbf{x}_{k-1}$ as the initial guess to find $\\mathbf{x}_k$ at the next parameter value $\\alpha_k$.\n\nStarting from the core definition of a root of a vector function and the first-order Taylor expansion of $F$ with respect to $\\mathbf{x}$, design and implement a corrector step based on solving a linearized system at each iteration to reduce the residual $F(\\mathbf{x},\\alpha_k)$. Ensure global convergence by employing a backtracking strategy that reduces the step length whenever the nonlinear residual fails to sufficiently decrease. Avoid using any black-box root finders; instead, implement the iteration from first principles by constructing and solving the appropriate linearized system at each step. Use the Euclidean norm for residuals and steps. Terminate the corrector at parameter value $\\alpha_k$ when the residual norm is at most a tolerance $\\varepsilon_F$ and a maximum number of iterations is not exceeded. Use $\\varepsilon_F=10^{-10}$ and at most $50$ corrector iterations per parameter value. If the linear system at a corrector step is ill-conditioned or singular, compute a least-squares step.\n\nImplement this parameter continuation procedure for the following test suite. In all cases, use the initial guess $\\mathbf{x}_0=[0.0,0.0]^{\\top}$ at the starting parameter value.\n\n- Test case $1$ (happy path): $\\alpha$ from $\\alpha_0=0.0$ to $\\alpha_N=1.0$ with $N=20$ uniform steps.\n- Test case $2$ (coarse stepping boundary): $\\alpha$ from $\\alpha_0=0.0$ to $\\alpha_N=1.0$ with $N=2$ uniform steps.\n- Test case $3$ (reverse direction edge case): $\\alpha$ from $\\alpha_0=0.0$ to $\\alpha_N=-1.0$ with $N=10$ uniform steps.\n\nFor each test case, report only the final solution $\\mathbf{x}_N$ at the last parameter value $\\alpha_N$. The required final output format is a single line containing the results as a comma-separated list of lists with each floating-point entry rounded to six decimal places, for example\n$[\\,[x_{1,\\mathrm{T1}},x_{2,\\mathrm{T1}}],[x_{1,\\mathrm{T2}},x_{2,\\mathrm{T2}}],[x_{1,\\mathrm{T3}},x_{2,\\mathrm{T3}}]\\,]$,\nwhere $(x_{1,\\mathrm{Tj}},x_{2,\\mathrm{Tj}})$ is the final solution for test case $j$. No physical units are involved. Angles are in radians. The program must produce exactly this single-line output.",
            "solution": "The posed problem is a well-defined exercise in computational science, requiring the implementation of a parameter continuation method to trace the solution manifold of a nonlinear system of equations. The problem is scientifically grounded, mathematically consistent, and provides all necessary information for its resolution. It is therefore deemed valid.\n\nThe core of the problem is to find a sequence of solutions $\\{\\mathbf{x}_k\\}_{k=0}^{N}$ for the parametric vector equation $F(\\mathbf{x}, \\alpha) = \\mathbf{0}$, where the parameter $\\alpha$ is varied incrementally. The function $F: \\mathbb{R}^2 \\times \\mathbb{R} \\to \\mathbb{R}^2$ is given by:\n$$\nF(\\mathbf{x}, \\alpha) = \\begin{bmatrix}\nf_1(\\mathbf{x}, \\alpha) \\\\\nf_2(\\mathbf{x}, \\alpha)\n\\end{bmatrix} = \\begin{bmatrix}\n\\sin(x_1) + x_2 - \\alpha \\\\\nx_1 + \\cos(x_2) - 1\n\\end{bmatrix}\n$$\nwhere $\\mathbf{x} = (x_1, x_2)^\\top$. This task will be accomplished using a predictor-corrector scheme, specifically natural parameter continuation coupled with a Newton-Raphson corrector.\n\nFirst, the parameter range from a starting value $\\alpha_0$ to an ending value $\\alpha_N$ is discretized into $N$ uniform steps, defining a sequence of parameter values $\\{\\alpha_k\\}_{k=0}^{N}$ where $\\alpha_k = \\alpha_0 + k \\cdot (\\alpha_N - \\alpha_0)/N$.\n\nThe continuation procedure begins at $k=0$ with the parameter value $\\alpha_0$. The initial guess for the solution $\\mathbf{x}_0$ is provided as $[0.0, 0.0]^\\top$. For each subsequent parameter value $\\alpha_k$ (for $k  0$), the predictor step consists of using the previously computed solution $\\mathbf{x}_{k-1}$ as the initial guess for the new solution $\\mathbf{x}_k$. This is a zero-order or \"natural\" predictor.\n\nFor each fixed parameter value $\\alpha_k$, a corrector method is employed to iteratively refine the current guess, which we denote $\\mathbf{x}^{(j)}$, to find a root of $F(\\mathbf{x}, \\alpha_k) = \\mathbf{0}$. The specified corrector is based on the first-order Taylor expansion of $F$ around $\\mathbf{x}^{(j)}$:\n$$\nF(\\mathbf{x}^{(j)} + \\Delta\\mathbf{x}, \\alpha_k) \\approx F(\\mathbf{x}^{(j)}, \\alpha_k) + J_F(\\mathbf{x}^{(j)}) \\Delta\\mathbf{x}\n$$\nHere, $\\Delta\\mathbf{x}$ is the correction step and $J_F(\\mathbf{x})$ is the Jacobian matrix of $F$ with respect to $\\mathbf{x}$. We seek $\\Delta\\mathbf{x}$ such that the linear approximation becomes the zero vector, which leads to the Newton-Raphson linear system:\n$$\nJ_F(\\mathbf{x}^{(j)}) \\Delta\\mathbf{x} = -F(\\mathbf{x}^{(j)}, \\alpha_k)\n$$\nThe Jacobian matrix for the given function $F$ is:\n$$\nJ_F(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1}  \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1}  \\frac{\\partial f_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} \\cos(x_1)  1 \\\\ 1  -\\sin(x_2) \\end{bmatrix}\n$$\nThis matrix is independent of the parameter $\\alpha$. The correction step $\\Delta\\mathbf{x}$ is found by solving this $2 \\times 2$ linear system. The problem requires that if the Jacobian matrix $J_F$ is singular or ill-conditioned—which may occur at turning points in the solution path—the system must be solved in a least-squares sense. This provides a robust update direction even when a unique solution to the linear system does not exist.\n\nTo ensure convergence of the corrector iterations, particularly when the initial guess from the predictor step is not sufficiently close to the true root, a backtracking line search is implemented. The full Newton step $\\Delta\\mathbf{x}$ is scaled by a factor $\\lambda \\in (0, 1]$, and the next iterate is computed as:\n$$\n\\mathbf{x}^{(j+1)} = \\mathbf{x}^{(j)} + \\lambda \\Delta\\mathbf{x}\n$$\nThe step length $\\lambda$ is chosen to ensure a sufficient decrease in the Euclidean norm of the residual, $\\|F(\\mathbf{x}, \\alpha_k)\\|_2$. A simple and effective strategy is to start with $\\lambda=1$ and successively reduce it (e.g., by halving) until the condition $\\|F(\\mathbf{x}^{(j+1)}, \\alpha_k)\\|_2  \\|F(\\mathbf{x}^{(j)}, \\alpha_k)\\|_2$ is satisfied.\n\nThe corrector process for a given $\\alpha_k$ is terminated when the Euclidean norm of the residual falls below a specified tolerance, $\\|F(\\mathbf{x}^{(j)}, \\alpha_k)\\|_2 \\le \\varepsilon_F = 10^{-10}$, or after a maximum of $50$ iterations is reached.\n\nThe overall algorithm proceeds as follows:\n$1$. Initialize the solution with the provided guess for the initial parameter value: $\\mathbf{x}_{\\text{current}} = [0.0, 0.0]^\\top$.\n$2$. Generate the sequence of parameter values $\\alpha_0, \\alpha_1, \\ldots, \\alpha_N$.\n$3$. For each $\\alpha_k$ in the sequence:\n    a. Set the initial guess for the corrector: $\\mathbf{x}^{(0)} = \\mathbf{x}_{\\text{current}}$.\n    b. Start the corrector loop (for $j = 0, 1, \\ldots, 49$):\n        i. Compute the residual vector $_res = F(\\mathbf{x}^{(j)}, \\alpha_k)$.\n        ii. If $\\|\\text{res}\\|_2 \\le 10^{-10}$, the corrector has converged. Store $\\mathbf{x}_{\\text{current}} = \\mathbf{x}^{(j)}$ and break the inner loop to proceed to the next $\\alpha_{k+1}$.\n        iii. Compute the Jacobian matrix $J = J_F(\\mathbf{x}^{(j)})$.\n        iv. Solve the linear system $J \\Delta\\mathbf{x} = -\\text{res}$ for $\\Delta\\mathbf{x}$ using a least-squares method for robustness.\n        v. Perform a backtracking line search to find a suitable step length $\\lambda$, starting with $\\lambda=1$.\n        vi. Update the solution guess: $\\mathbf{x}^{(j+1)} = \\mathbf{x}^{(j)} + \\lambda \\Delta\\mathbf{x}$.\n    c. After the inner loop (either by convergence or reaching max iterations), the resulting $\\mathbf{x}^{(j)}$ becomes the new $\\mathbf{x}_{\\text{current}}$.\n$4$. After iterating through all parameter values up to $\\alpha_N$, the final $\\mathbf{x}_{\\text{current}}$ is the desired solution for the test case. This procedure is repeated for all specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a parameter continuation method to solve a nonlinear system F(x, alpha) = 0.\n    The method uses a natural parameter predictor and a Newton-Raphson corrector\n    with a backtracking line search and least-squares steps for robustness.\n    \"\"\"\n\n    # Define the nonlinear vector function F(x, alpha)\n    def F(x, alpha):\n        x1, x2 = x\n        f1 = np.sin(x1) + x2 - alpha\n        f2 = x1 + np.cos(x2) - 1.0\n        return np.array([f1, f2])\n\n    # Define the Jacobian matrix J(x)\n    def J(x):\n        x1, x2 = x\n        j11 = np.cos(x1)\n        j12 = 1.0\n        j21 = 1.0\n        j22 = -np.sin(x2)\n        return np.array([[j11, j12], [j21, j22]])\n\n    # Define parameters for the numerical method\n    F_TOLERANCE = 1e-10\n    MAX_CORRECTOR_ITER = 50\n    MIN_LAMBDA = 1e-8\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # (alpha_start, alpha_end, num_steps)\n        (0.0, 1.0, 20),  # Test case 1\n        (0.0, 1.0, 2),   # Test case 2\n        (0.0, -1.0, 10), # Test case 3\n    ]\n\n    final_results = []\n\n    for alpha_start, alpha_end, N in test_cases:\n        # Generate the sequence of parameter values\n        alphas = np.linspace(alpha_start, alpha_end, N + 1)\n        \n        # Initial guess for the solution at the starting parameter value\n        x_sol = np.array([0.0, 0.0])\n\n        # Loop over the parameter steps (continuation loop)\n        for alpha_k in alphas:\n            x_guess = np.copy(x_sol)\n            \n            # Corrector loop (Newton-Raphson with line search)\n            for _ in range(MAX_CORRECTOR_ITER):\n                residual = F(x_guess, alpha_k)\n                res_norm = np.linalg.norm(residual)\n\n                # Check for convergence\n                if res_norm = F_TOLERANCE:\n                    break\n                \n                # Compute Jacobian and solve the linear system for the Newton step\n                jacobian = J(x_guess)\n                # Use lstsq for robustness against singularity\n                delta_x, _, _, _ = np.linalg.lstsq(jacobian, -residual, rcond=None)\n\n                # Backtracking line search for globalization\n                lambda_val = 1.0\n                while lambda_val > MIN_LAMBDA:\n                    x_new = x_guess + lambda_val * delta_x\n                    new_res_norm = np.linalg.norm(F(x_new, alpha_k))\n                    \n                    if new_res_norm  res_norm:\n                        x_guess = x_new\n                        break\n                    \n                    lambda_val /= 2.0\n                else:\n                    # If lambda becomes too small, step might be non-productive.\n                    # Stop this corrector iteration and hope for the best.\n                    break\n            \n            # Update the solution for the current parameter value\n            x_sol = x_guess\n\n        # Append the final solution (at alpha_N) for the current test case\n        final_results.append(x_sol)\n\n    # Format and print the final output as specified\n    formatted_strings = [f'[{res[0]:.6f},{res[1]:.6f}]' for res in final_results]\n    print(f\"[{','.join(formatted_strings)}]\")\n\nsolve()\n```"
        }
    ]
}