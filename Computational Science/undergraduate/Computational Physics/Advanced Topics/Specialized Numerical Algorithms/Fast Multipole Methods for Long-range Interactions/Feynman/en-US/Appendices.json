{
    "hands_on_practices": [
        {
            "introduction": "The Fast Multipole Method's power comes from its ability to hierarchically group sources and represent their collective influence through multipole expansions. A key part of this process involves translating and rotating these expansions to different reference frames within the adaptive tree structure. This exercise  provides hands-on practice with these fundamental moment transformation operators in two dimensions, using the elegant formalism of complex numbers to verify the algebraic rules for translation and rotation.",
            "id": "2392057",
            "problem": "You are given a system of point sources in two spatial dimensions that generate a scalar potential governed by the Laplace equation. The fundamental solution in two dimensions is the logarithmic kernel, with potential generated by a set of sources located at positions $\\{(x_j,y_j)\\}_{j=1}^{N}$ with strengths $\\{q_j\\}_{j=1}^{N}$ defined by\n$$\n\\phi(\\mathbf{r}) \\;=\\; -\\frac{1}{2\\pi}\\sum_{j=1}^{N} q_j \\,\\log\\left(\\|\\mathbf{r}-\\mathbf{r}_j\\|\\right),\n$$\nwhere $\\mathbf{r}=(x,y)$ and $\\mathbf{r}_j=(x_j,y_j)$. Consider the complex representation $z=x+\\mathrm{i}y$ and the multipole moments of order $n$ about a center $c=c_x+\\mathrm{i}c_y$ defined by\n$$\nM_0 \\;=\\; \\sum_{j=1}^{N} q_j,\\qquad M_n \\;=\\; \\sum_{j=1}^{N} q_j\\,(z_j-c)^n\\quad\\text{for } n\\ge 1,\n$$\nwhere $z_j=x_j+\\mathrm{i}y_j$.\n\nA rigid rotation of all source positions about the center $c$ by an angle $\\theta$ (in radians) in the counterclockwise sense transforms the moments according to\n$$\nM_n^{(\\mathrm{rot})} \\;=\\; \\sum_{j=1}^{N} q_j\\,(z_j'-c)^n,\\quad \\text{where } z_j'-c \\;=\\; e^{\\mathrm{i}\\theta}\\,(z_j-c).\n$$\nA translation of the expansion center from $c$ to $c' = c + d$ with $d=d_x+\\mathrm{i}d_y$ relates the moments by the binomial identity\n$$\nM_n^{(\\mathrm{trans})}(c') \\;=\\; \\sum_{k=0}^{n} \\binom{n}{k}\\,(-d)^{\\,n-k}\\,M_k(c).\n$$\n\nNow consider representing planar vectors as pure quaternions for the purpose of rotation. A planar vector $\\mathbf{v}=(x,y)$ is embedded as the pure quaternion $v = 0 + x\\,\\mathbf{i} + y\\,\\mathbf{j} + 0\\,\\mathbf{k}$. A counterclockwise rotation by an angle $\\theta$ about the $\\mathbf{k}$-axis is given by the unit quaternion\n$$\nu(\\theta) \\;=\\; \\cos\\!\\left(\\frac{\\theta}{2}\\right) \\;+\\; \\sin\\!\\left(\\frac{\\theta}{2}\\right)\\,\\mathbf{k},\n$$\nacting on $v$ via conjugation $v' \\,=\\, u(\\theta)\\,v\\,u(\\theta)^{-1}$, which yields the rotated planar vector.\n\nTask. Implement a program that, for each test case below and for a specified nonnegative integer order $p$, computes the following two maximum absolute discrepancies:\n1. Rotation discrepancy: The maximum absolute difference over $n=0,1,\\dots,p$ between the moments recomputed from the sources rotated about $c$ by the quaternion conjugation formula and the moments predicted by the complex-phase rule $M_n \\mapsto e^{\\mathrm{i}n\\theta}M_n$.\n2. Translation discrepancy: The maximum absolute difference over $n=0,1,\\dots,p$ between the moments recomputed at the translated center $c' = c + d$ and the moments predicted by the binomial translation formula given above.\n\nAll angles must be in radians. The final outputs for each test case are real numbers (floating-point) representing these two maximum absolute discrepancies. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[E_{\\mathrm{rot}},E_{\\mathrm{trans}}]$ in that order. For example, the output format must be of the form\n$$\n\\big[\\,[E_{\\mathrm{rot}}^{(1)},E_{\\mathrm{trans}}^{(1)}],\\,[E_{\\mathrm{rot}}^{(2)},E_{\\mathrm{trans}}^{(2)}],\\,\\dots\\,\\big].\n$$\n\nTest suite. For each case below, use the specified sources $\\{(x_j,y_j),q_j\\}$, center $c=(c_x,c_y)$, order $p$, rotation angle $\\theta$, and translation increment $d=(d_x,d_y)$.\n\n- Case A (general configuration):\n  - $p=6$,\n  - $\\{(x_j,y_j)\\}_{j=1}^{4} = \\{(0.3,-0.1),\\,(1.1,0.7),\\,(-0.8,0.5),\\,(0.0,0.0)\\}$,\n  - $\\{q_j\\}_{j=1}^{4} = \\{1.0,\\,-2.0,\\,0.5,\\,-0.3\\}$,\n  - $c=(0.2,-0.1)$,\n  - $\\theta=\\pi/3$,\n  - $d=(0.1,-0.2)$.\n\n- Case B (zero-strength edge case):\n  - $p=5$,\n  - $\\{(x_j,y_j)\\}_{j=1}^{3} = \\{(-0.4,0.9),\\,(0.9,-0.2),\\,(0.3,-0.7)\\}$,\n  - $\\{q_j\\}_{j=1}^{3} = \\{0.0,\\,0.0,\\,0.0\\}$,\n  - $c=(0.0,0.0)$,\n  - $\\theta=\\pi/2$,\n  - $d=(0.5,0.5)$.\n\n- Case C (single-source, coincident center; zero rotation/translation):\n  - $p=7$,\n  - $\\{(x_j,y_j)\\}_{j=1}^{1} = \\{(0.7,-1.2)\\}$,\n  - $\\{q_j\\}_{j=1}^{1} = \\{2.5\\}$,\n  - $c=(0.7,-1.2)$,\n  - $\\theta=0.0$,\n  - $d=(0.0,0.0)$.\n\n- Case D (symmetric configuration, nontrivial translation and half-turn rotation):\n  - $p=8$,\n  - $\\{(x_j,y_j)\\}_{j=1}^{4} = \\{(0.6,0.0),\\,(-0.6,0.0),\\,(0.0,0.9),\\,(0.0,-0.9)\\}$,\n  - $\\{q_j\\}_{j=1}^{4} = \\{1.0,\\,1.0,\\,-1.0,\\,-1.0\\}$,\n  - $c=(0.0,0.0)$,\n  - $\\theta=\\pi$,\n  - $d=(-0.2,0.3)$.\n\nYour program should compute, for each case, the pair of floating-point numbers $[E_{\\mathrm{rot}},E_{\\mathrm{trans}}]$ and output them as a single line in the exact format described above. No external input is provided, and no physical units are involved beyond the dimensionless quantities specified. Angles are in radians. All numerical results must be given as raw decimal numbers in the program’s single-line output, without additional text.",
            "solution": "The problem is valid. It is a well-defined computational task based on established principles of potential theory and the Fast Multipole Method (FMM) in computational physics. The problem asks for the numerical verification of two fundamental transformation identities for multipole moments in a two-dimensional system governed by the Laplace equation.\n\nThe solution proceeds by implementing a direct computational process for each test case. The core of the task is to calculate two maximum absolute discrepancies, $E_{\\mathrm{rot}}$ and $E_{\\mathrm{trans}}$, up to a specified multipole order $p$. These discrepancies quantify the numerical difference between two equivalent methods of calculating transformed multipole moments. The mathematical equivalence implies that any non-zero result is attributable to a difference in floating-point error accumulation between the two computational pathways.\n\nWe begin by defining the multipole moments of a system of $N$ point sources with strengths $\\{q_j\\}_{j=1}^{N}$ at complex positions $\\{z_j\\}_{j=1}^{N}$ with respect to an expansion center $c$. The moment of order $n$ is given by\n$$\nM_n(c) = \\sum_{j=1}^{N} q_j (z_j - c)^n, \\quad n=0, 1, \\dots, p.\n$$\nThese moments are computed once and stored as an array of $p+1$ complex numbers, forming the basis for subsequent transformations.\n\n**1. Rotation Discrepancy ($E_{\\mathrm{rot}}$)**\n\nThe first task is to verify the consistency of the moment rotation formula. A counterclockwise rotation of all source positions about the center $c$ by an angle $\\theta$ transforms the relative position vectors $z_j - c$ to $(z_j - c)e^{\\mathrm{i}\\theta}$. The problem states that this rotation can be represented by quaternion conjugation, which for a planar vector is equivalent to multiplication by the complex phase factor $e^{\\mathrm{i}\\theta}$.\n\nWe compute the rotated moments in two ways:\n\na) **Re-computation from Rotated Sources**: The new source positions $z_j'$ after rotation about $c$ are given by $z_j' = c + (z_j - c)e^{\\mathrm{i}\\theta}$. We use these new positions to re-calculate the moments with respect to the original center $c$. Let us call these the recomputed moments, $M_n^{(\\mathrm{comp\\_rot})}(c)$.\n$$\nM_n^{(\\mathrm{comp\\_rot})}(c) = \\sum_{j=1}^{N} q_j (z_j' - c)^n = \\sum_{j=1}^{N} q_j \\left( (z_j - c)e^{\\mathrm{i}\\theta} \\right)^n.\n$$\n\nb) **Prediction via Transformation Rule**: The transformation rule for moments can be derived from the definition above:\n$$\nM_n^{(\\mathrm{comp\\_rot})}(c) = \\left(e^{\\mathrm{i}\\theta}\\right)^n \\sum_{j=1}^{N} q_j (z_j - c)^n = e^{\\mathrm{i}n\\theta} M_n(c).\n$$\nThis gives a direct formula for the predicted rotated moments, $M_n^{(\\mathrm{pred\\_rot})}(c)$, based on the original moments $M_n(c)$.\n$$\nM_n^{(\\mathrm{pred\\_rot})}(c) = e^{\\mathrm{i}n\\theta} M_n(c).\n$$\n\nThe rotation discrepancy, $E_{\\mathrm{rot}}$, is the maximum absolute difference between these two sets of moments over all orders $n=0, 1, \\dots, p$.\n$$\nE_{\\mathrm{rot}} = \\max_{0 \\le n \\le p} \\left| M_n^{(\\mathrm{comp\\_rot})}(c) - M_n^{(\\mathrm{pred\\_rot})}(c) \\right|.\n$$\n\n**2. Translation Discrepancy ($E_{\\mathrm{trans}}$)**\n\nThe second task is to verify the moment translation formula, also known as an M-to-M (multipole-to-multipole) translation. When the expansion center is shifted from $c$ to a new center $c' = c + d$, the moments change accordingly.\n\nWe compute the moments at the new center $c'$ in two ways:\n\na) **Re-computation at New Center**: The moments are calculated directly from the definition using the new center $c'$. Let these be the recomputed translated moments, $M_n^{(\\mathrm{comp\\_trans})}(c')$.\n$$\nM_n^{(\\mathrm{comp\\_trans})}(c') = \\sum_{j=1}^{N} q_j (z_j - c')^n = \\sum_{j=1}^{N} q_j (z_j - (c+d))^n.\n$$\n\nb) **Prediction via Transformation Rule**: The moments at the new center $c'$ can be expressed in terms of the moments at the old center $c$ using the binomial identity provided in the problem statement.\n$$\n(z_j - c')^n = ((z_j - c) - d)^n = \\sum_{k=0}^{n} \\binom{n}{k} (z_j-c)^k (-d)^{n-k}.\n$$\nSubstituting this into the moment definition and swapping summations yields the M-to-M translation formula, which we use for prediction:\n$$\nM_n^{(\\mathrm{pred\\_trans})}(c') = \\sum_{k=0}^{n} \\binom{n}{k} M_k(c) (-d)^{n-k}.\n$$\n\nThe translation discrepancy, $E_{\\mathrm{trans}}$, is the maximum absolute difference between these two sets of moments over all orders $n=0, 1, \\dots, p$.\n$$\nE_{\\mathrm{trans}} = \\max_{0 \\le n \\le p} \\left| M_n^{(\\mathrm{comp\\_trans})}(c') - M_n^{(\\mathrm{pred\\_trans})}(c') \\right|.\n$$\n\nFor each test case, the program implements these computations using complex arithmetic and calculates the two required discrepancy values, $[E_{\\mathrm{rot}}, E_{\\mathrm{trans}}]$. The final output aggregates these pairs into a single list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the formatted result.\n    \"\"\"\n\n    test_cases = [\n        {\n            'p': 6,\n            'sources': [(0.3, -0.1), (1.1, 0.7), (-0.8, 0.5), (0.0, 0.0)],\n            'strengths': [1.0, -2.0, 0.5, -0.3],\n            'center': (0.2, -0.1),\n            'theta': np.pi / 3,\n            'd_vec': (0.1, -0.2),\n        },\n        {\n            'p': 5,\n            'sources': [(-0.4, 0.9), (0.9, -0.2), (0.3, -0.7)],\n            'strengths': [0.0, 0.0, 0.0],\n            'center': (0.0, 0.0),\n            'theta': np.pi / 2,\n            'd_vec': (0.5, 0.5),\n        },\n        {\n            'p': 7,\n            'sources': [(0.7, -1.2)],\n            'strengths': [2.5],\n            'center': (0.7, -1.2),\n            'theta': 0.0,\n            'd_vec': (0.0, 0.0),\n        },\n        {\n            'p': 8,\n            'sources': [(0.6, 0.0), (-0.6, 0.0), (0.0, 0.9), (0.0, -0.9)],\n            'strengths': [1.0, 1.0, -1.0, -1.0],\n            'center': (0.0, 0.0),\n            'theta': np.pi,\n            'd_vec': (-0.2, 0.3),\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        solver = FMMDiscrepancyCalculator(\n            p=case['p'],\n            sources=case['sources'],\n            strengths=case['strengths'],\n            center=case['center'],\n            theta=case['theta'],\n            d_vec=case['d_vec']\n        )\n        result_pair = solver.calculate_all_discrepancies()\n        all_results.append(result_pair)\n\n    # Format the final output string as specified in the problem statement.\n    output_str = f\"[{','.join([f'[{r[0]},{r[1]}]' for r in all_results])}]\"\n    print(output_str)\n\nclass FMMDiscrepancyCalculator:\n    \"\"\"\n    A class to compute multipole moment transformation discrepancies.\n    \"\"\"\n\n    def __init__(self, p, sources, strengths, center, theta, d_vec):\n        self.p = p\n        self.sources = np.array([complex(x, y) for x, y in sources], dtype=np.complex128)\n        self.strengths = np.array(strengths, dtype=np.float64)\n        self.c = complex(center[0], center[1])\n        self.theta = theta\n        self.d = complex(d_vec[0], d_vec[1])\n        self.orders = np.arange(self.p + 1)\n\n        # property to cache original moments calculation\n        self._original_moments = None\n\n    @property\n    def original_moments(self):\n        if self._original_moments is None:\n            self._original_moments = self._compute_moments(self.sources, self.c)\n        return self._original_moments\n\n    def _compute_moments(self, source_positions, center):\n        \"\"\"Computes moments from source positions relative to a center.\"\"\"\n        moments = np.zeros(self.p + 1, dtype=np.complex128)\n        relative_pos = source_positions - center\n        for n in self.orders:\n            # numpy.power correctly handles z^0 = 1, including 0^0 = 1.\n            moments[n] = np.sum(self.strengths * np.power(relative_pos, n))\n        return moments\n\n    def calculate_rotation_discrepancy(self):\n        \"\"\"\n        Computes the maximum discrepancy for moment rotation.\n        \"\"\"\n        # Predicted moments using M_n' = e^(i*n*theta) * M_n\n        rot_factors = np.exp(1j * self.orders * self.theta)\n        predicted_moments = self.original_moments * rot_factors\n\n        # Recomputed moments from explicitly rotated sources.\n        # The quaternion rotation on the ij-plane is equivalent to multiplication by e^(i*theta).\n        relative_pos = self.sources - self.c\n        rotator = np.exp(1j * self.theta)\n        rotated_sources = self.c + relative_pos * rotator\n        recomputed_moments = self._compute_moments(rotated_sources, self.c)\n        \n        # Calculate maximum absolute difference\n        discrepancy = np.abs(recomputed_moments - predicted_moments)\n        return np.max(discrepancy)\n\n    def calculate_translation_discrepancy(self):\n        \"\"\"\n        Computes the maximum discrepancy for moment translation.\n        \"\"\"\n        # Predicted moments using the binomial M-to-M formula\n        predicted_moments = np.zeros(self.p + 1, dtype=np.complex128)\n        for n in self.orders:\n            for k in range(n + 1):\n                # Use exact=True to avoid float conversion of binomial coefficient\n                binomial_coeff = comb(n, k, exact=True)\n                term = binomial_coeff * ((-self.d) ** (n - k)) * self.original_moments[k]\n                predicted_moments[n] += term\n\n        # Recomputed moments at the new, translated center\n        new_center = self.c + self.d\n        recomputed_moments = self._compute_moments(self.sources, new_center)\n        \n        # Calculate maximum absolute difference\n        discrepancy = np.abs(recomputed_moments - predicted_moments)\n        return np.max(discrepancy)\n        \n    def calculate_all_discrepancies(self):\n        \"\"\"Calculates both discrepancies and returns them as a pair.\"\"\"\n        e_rot = self.calculate_rotation_discrepancy()\n        e_trans = self.calculate_translation_discrepancy()\n        return [e_rot, e_trans]\n\nsolve()\n```"
        },
        {
            "introduction": "The efficiency of the FMM relies on a clear geometric separation of interacting cell groups, which is formalized by the multipole acceptance criterion (MAC). This practice  challenges you to probe the algorithm's robustness by constructing \"pathological\" particle distributions that test the limits of this geometric assumption. By analyzing the size of the resulting interaction lists and the amount of near-field work, you will develop a deeper intuition for how FMM's performance depends critically on the spatial characteristics of the system.",
            "id": "2392072",
            "problem": "You are given the task of constructing particle configurations that are pathological for hierarchical long-range interaction schemes, and of quantifying, for each configuration, how a cell-based grouping test partitions interactions into well-separated versus near-field categories. Consider an axis-aligned cubic computational domain centered at the origin with half-size $H$, containing $N$ point particles at positions $\\{\\mathbf{x}_i\\}_{i=1}^N \\subset \\mathbb{R}^3$. Partition the domain into an octree: recursively subdivide any cube containing more than $C$ particles into $8$ equal child cubes until either the particle count in a cube is at most $C$ or a maximum depth $D$ is reached. A leaf cell is any non-empty cube not subdivided further by this rule. Let $\\mathcal{L}$ denote the set of all such leaf cells with at least one particle.\n\nFor each target leaf cell $T \\in \\mathcal{L}$ with center $\\mathbf{c}_T$ and half-size $h_T$, define its interaction list $\\mathcal{I}(T)$ as a set of source cells $S$ selected by the following geometric rule. For any source cell $S$ with center $\\mathbf{c}_S$ and half-size $h_S$, define the Euclidean distance $r_{ST} = \\|\\mathbf{c}_S - \\mathbf{c}_T\\|_2$ and the disjointness predicate that $S$ and $T$ are disjoint if and only if there exists at least one coordinate axis $a \\in \\{x,y,z\\}$ such that $|\\mathbf{c}_{S,a} - \\mathbf{c}_{T,a}| > h_S + h_T$. Define the multipole-acceptance test with parameter $\\theta \\in (0,1)$ as follows: a source cell $S$ is accepted for $T$ if and only if $S$ and $T$ are disjoint and $h_S / r_{ST} \\le \\theta$. Classify source cells for each target leaf as follows. Starting from the source root cell and a fixed target leaf $T$, if a source cell $S$ satisfies the acceptance test, then include $S$ in $\\mathcal{I}(T)$. If $S$ does not satisfy the acceptance test and $S$ is not a leaf, then classify its children in the same manner. If $S$ does not satisfy the acceptance test and $S$ is a leaf, or if $S$ and $T$ are not disjoint and $S$ is a leaf, then the pair $(S,T)$ is a near-field leaf-pair that must be computed directly. In addition, if $r_{ST} = 0$, then this pair cannot be accepted and must be refined unless $S$ is a leaf, in which case it is a near-field leaf-pair. For each target leaf $T$, this process yields a finite set $\\mathcal{I}(T)$ and a corresponding multiset of near-field leaf-pairs involving $T$.\n\nFor each configuration specified below, build the octree with the given parameters and compute the following four quantities:\n- The maximum interaction-list size across targets, $\\max_{T \\in \\mathcal{L}} |\\mathcal{I}(T)|$.\n- The arithmetic mean interaction-list size across targets, $\\frac{1}{|\\mathcal{L}|} \\sum_{T \\in \\mathcal{L}} |\\mathcal{I}(T)|$, rounded to three digits after the decimal point.\n- The total count of near-field leaf-pairs encountered over all targets, $\\sum_{T \\in \\mathcal{L}} \\#\\{\\text{near-field leaf-pairs involving }T\\}$.\n- The number of target leaves, $|\\mathcal{L}|$.\n\nAngles must be interpreted in radians.\n\nConstruct the following five pathological configurations, each inside the cube of half-size $H = 1.25$, with octree capacity $C = 32$, maximum depth $D = 20$, and multipole-acceptance parameter $\\theta = 0.5$. All distances are unitless. In all cases, take particle radius parameter $R = 1$ unless otherwise stated.\n\nTest Suite (each line is one test case specifying geometry and parameters):\n1. Hollow spherical shell: $N = 4096$ points quasi-uniformly distributed on the sphere of radius $R$. Use the deterministic Fibonacci construction: let $\\varphi = \\frac{1+\\sqrt{5}}{2}$ and $\\alpha = 2\\pi\\left(1 - \\frac{1}{\\varphi}\\right)$. For $k \\in \\{0,1,\\dots,N-1\\}$, define $z_k = 1 - \\frac{2(k+0.5)}{N}$, $r_k = \\sqrt{1 - z_k^2}$, $\\theta_k = \\alpha k$, and set $\\mathbf{x}_k = \\left(R r_k \\cos \\theta_k, R r_k \\sin \\theta_k, R z_k\\right)$.\n2. Great-circle ring: $N = 4096$ points on the circle of radius $R$ in the plane $z=0$. For $k \\in \\{0,1,\\dots,N-1\\}$, set $\\mathbf{x}_k = \\left(R \\cos \\frac{2\\pi k}{N}, R \\sin \\frac{2\\pi k}{N}, 0\\right)$.\n3. Eight-corner clusters: $N = 4096$ points arranged as $8$ identical $8\\times 8 \\times 8$ grids of points, one grid packed inside a small cube of side $0.02$ centered near each corner of the cube of side $2R$, offset inward by $0.05$. Let the inward offset be $\\delta = 0.05$ and the micro-grid span be $\\varepsilon = 0.02$. For each corner sign triple $\\mathbf{s} \\in \\{-1,1\\}^3$, define the cluster center $\\mathbf{c}_{\\mathbf{s}} = \\mathbf{s}\\,(R - \\delta)$. For grid indices $(p,q,r) \\in \\{0,1,\\dots,7\\}^3$, define the local offset $\\mathbf{o}_{pqr} = \\varepsilon \\left(\\frac{p}{7} - \\frac{1}{2}, \\frac{q}{7} - \\frac{1}{2}, \\frac{r}{7} - \\frac{1}{2}\\right)$ and place one particle at $\\mathbf{x} = \\mathbf{c}_{\\mathbf{s}} + \\mathbf{o}_{pqr}$ for each $(\\mathbf{s},p,q,r)$.\n4. Single particle: $N = 1$ particle at the origin, $\\mathbf{x}_1 = (0,0,0)$.\n5. Collinear line: $N = 4096$ particles on the $x$-axis from $-R$ to $R$ inclusive with equal spacing. For $k \\in \\{0,1,\\dots,N-1\\}$, set $\\mathbf{x}_k = \\left(-R + \\frac{2R}{N-1} k, 0, 0\\right)$.\n\nFor all test cases, use the same domain half-size $H$, octree capacity $C$, maximum depth $D$, and multipole parameter $\\theta$ as specified above. Your program must construct the particles, build the octree, perform the classification for each target leaf as defined, and compute the four requested quantities. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of four numbers in the order given. For example, the output should be formatted as a single line like $[\\,[a_1,b_1,c_1,d_1],\\,[a_2,b_2,c_2,d_2],\\,\\dots\\,]$, where $a_i$, $b_i$, $c_i$, and $d_i$ are the four numbers for test case $i$ and $b_i$ is rounded to three digits after the decimal point.",
            "solution": "The problem presented is a well-defined exercise in computational physics, specifically concerning the analysis of a core component of the Fast Multipole Method (FMM). The task requires the implementation of an octree data structure to partition a three-dimensional space containing particles, and subsequently, for each leaf of this tree, to classify all other cells as either far-field (part of an interaction list) or near-field, based on a geometric multipole-acceptance criterion. This analysis must be performed for five specific particle configurations designed to test the algorithm's behavior under pathological conditions. The problem is scientifically sound, mathematically precise, and algorithmically explicit. Therefore, it is deemed a valid problem.\n\nThe solution proceeds by implementing the specified algorithms and applying them to each test case. The main components of the solution are: particle generation, octree construction, and interaction classification.\n\n**1. Particle Generation**\n\nFor each of the five test cases, $N$ particles are generated within a domain defined by a cube of half-size $H = 1.25$ centered at the origin. The coordinates $\\{\\mathbf{x}_i\\}_{i=1}^N$ are determined by the following rules, with radius parameter $R=1$:\n\n- **Case 1: Hollow Spherical Shell ($N=4096$)**\n  Particles are distributed on the surface of a sphere of radius $R=1$ using the Fibonacci lattice construction. For $k \\in \\{0, 1, \\dots, N-1\\}$, with $\\varphi = \\frac{1+\\sqrt{5}}{2}$ and $\\alpha = 2\\pi(1 - 1/\\varphi)$, the coordinates $\\mathbf{x}_k = (x_k, y_k, z_k)$ are:\n  $$ z_k = 1 - \\frac{2(k+0.5)}{N} $$\n  $$ r_k = \\sqrt{1 - z_k^2} $$\n  $$ \\theta_k = \\alpha k $$\n  $$ \\mathbf{x}_k = (R r_k \\cos \\theta_k, R r_k \\sin \\theta_k, R z_k) $$\n\n- **Case 2: Great-circle Ring ($N=4096$)**\n  Particles are uniformly distributed on a circle of radius $R=1$ in the $z=0$ plane. For $k \\in \\{0, 1, \\dots, N-1\\}$:\n  $$ \\mathbf{x}_k = \\left(R \\cos \\frac{2\\pi k}{N}, R \\sin \\frac{2\\pi k}{N}, 0\\right) $$\n\n- **Case 3: Eight-corner Clusters ($N=4096$)**\n  $N/8 = 512$ particles are placed in a small cubic grid near each of the $8$ corners of a cube of side $2R$. Parameters are $\\delta=0.05$ and $\\varepsilon=0.02$. For each corner sign triple $\\mathbf{s} \\in \\{-1,1\\}^3$ and grid indices $(p,q,r) \\in \\{0,1,\\dots,7\\}^3$:\n  $$ \\mathbf{c}_{\\mathbf{s}} = \\mathbf{s}\\,(R - \\delta) $$\n  $$ \\mathbf{o}_{pqr} = \\varepsilon \\left(\\frac{p}{7} - \\frac{1}{2}, \\frac{q}{7} - \\frac{1}{2}, \\frac{r}{7} - \\frac{1}{2}\\right) $$\n  A particle is placed at $\\mathbf{x} = \\mathbf{c}_{\\mathbf{s}} + \\mathbf{o}_{pqr}$.\n\n- **Case 4: Single Particle ($N=1$)**\n  A single particle is placed at the origin:\n  $$ \\mathbf{x}_1 = (0, 0, 0) $$\n\n- **Case 5: Collinear Line ($N=4096$)**\n  Particles are placed with equal spacing on the $x$-axis from $-R$ to $R$. For $k \\in \\{0, 1, \\dots, N-1\\}$:\n  $$ \\mathbf{x}_k = \\left(-R + \\frac{2R}{N-1} k, 0, 0\\right) $$\n\n**2. Octree Construction**\n\nAn octree is constructed to spatially partition the particles. The process starts with a root cell, a cube centered at $(0,0,0)$ with half-size $H=1.25$. This cell contains all $N$ particles. A recursive subdivision procedure is applied: any cell containing more than $C=32$ particles is subdivided into $8$ equal cubic children, unless the cell is already at the maximum depth $D=20$. The particles within the parent cell are then distributed among its children. This process continues until all cells either contain $C$ or fewer particles or are at depth $D$. A non-empty cell that is not subdivided is a leaf cell. The set of all such leaf cells is denoted by $\\mathcal{L}$.\n\n**3. Interaction Classification**\n\nFor each target leaf cell $T \\in \\mathcal{L}$, a traversal of the entire octree (starting from the root as the initial source cell $S$) is performed to build its interaction list $\\mathcal{I}(T)$ and to count near-field leaf-pairs. The classification rule for a source cell $S$ relative to a target leaf $T$ is as follows:\n\n- Let $\\mathbf{c}_S, h_S$ and $\\mathbf{c}_T, h_T$ be the centers and half-sizes of $S$ and $T$, respectively.\n- The cells are disjoint if $|\\mathbf{c}_{S,a} - \\mathbf{c}_{T,a}| > h_S + h_T$ for at least one axis $a \\in \\{x,y,z\\}$.\n- The multipole-acceptance criterion (MAC) is satisfied if $S$ and $T$ are disjoint and the condition $h_S / r_{ST} \\le \\theta$ holds, where $r_{ST} = \\|\\mathbf{c}_S - \\mathbf{c}_T\\|_2$ and $\\theta=0.5$. If $r_{ST}=0$, the test fails.\n\nThe recursive classification proceeds for a given target $T$ and a source cell $S$:\n1. If the MAC is satisfied, $S$ is added to $\\mathcal{I}(T)$, and the traversal of this branch of the source tree terminates.\n2. If the MAC is not satisfied:\n   a. If $S$ is a non-leaf cell, the procedure is recursively applied to each of its children.\n   b. If $S$ is a leaf cell, the pair $(S,T)$ is classified as a near-field leaf-pair.\n\nThis process is repeated for every target leaf $T \\in \\mathcal{L}$.\n\n**4. Computation of Statistics**\n\nAfter classifying interactions for all target leaves, the following four quantities are computed for each test case:\n- The maximum size of any interaction list: $\\max_{T \\in \\mathcal{L}} |\\mathcal{I}(T)|$.\n- The arithmetic mean of interaction list sizes, rounded to three decimal places: $\\frac{1}{|\\mathcal{L}|} \\sum_{T \\in \\mathcal{L}} |\\mathcal{I}(T)|$.\n- The total count of near-field leaf-pairs: $\\sum_{T \\in \\mathcal{L}} \\#\\{S \\in \\mathcal{L} \\mid (S,T) \\text{ is a near-field pair}\\}$.\n- The total number of non-empty leaf cells: $|\\mathcal{L}|$.\n\nThe implementation systematically executes these steps, collecting the four statistics for each of the five particle configurations.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport collections\n\n# from scipy import ... # Scipy is not required for this implementation.\n\nclass Cell:\n    \"\"\"Represents a cell in the octree.\"\"\"\n    def __init__(self, center, half_size, depth, parent=None):\n        self.center = center\n        self.half_size = half_size\n        self.depth = depth\n        self.parent = parent\n        self.children = []\n        self.particle_indices = []\n        self.is_leaf = False\n\ndef generate_particles(config, N, R):\n    \"\"\"Generates particle coordinates for a given configuration.\"\"\"\n    if config == \"sphere\":\n        particles = np.zeros((N, 3))\n        phi = (1.0 + np.sqrt(5.0)) / 2.0\n        alpha = 2.0 * np.pi * (1.0 - 1.0 / phi)\n        for k in range(N):\n            z_k = 1.0 - (2.0 * (k + 0.5)) / N\n            r_k = np.sqrt(1.0 - z_k**2)\n            theta_k = alpha * k\n            particles[k] = [R * r_k * np.cos(theta_k), R * r_k * np.sin(theta_k), R * z_k]\n        return particles\n    elif config == \"ring\":\n        particles = np.zeros((N, 3))\n        for k in range(N):\n            angle = 2.0 * np.pi * k / N\n            particles[k] = [R * np.cos(angle), R * np.sin(angle), 0.0]\n        return particles\n    elif config == \"clusters\":\n        particles = np.zeros((N, 3))\n        delta = 0.05\n        epsilon = 0.02\n        pidx = 0\n        signs = [-1.0, 1.0]\n        for sx in signs:\n            for sy in signs:\n                for sz in signs:\n                    s_vec = np.array([sx, sy, sz])\n                    cluster_center = s_vec * (R - delta)\n                    for p in range(8):\n                        for q in range(8):\n                            for r in range(8):\n                                o_pqr = epsilon * (np.array([p / 7.0, q / 7.0, r / 7.0]) - 0.5)\n                                particles[pidx] = cluster_center + o_pqr\n                                pidx += 1\n        return particles\n    elif config == \"single\":\n        return np.array([[0.0, 0.0, 0.0]])\n    elif config == \"line\":\n        return np.linspace([-R, 0, 0], [R, 0, 0], N)\n    return np.array([])\n\ndef build_octree_recursive(cell, particles, C, D):\n    \"\"\"Recursively builds the octree.\"\"\"\n    if len(cell.particle_indices) <= C or cell.depth >= D:\n        cell.is_leaf = len(cell.particle_indices) > 0\n        return\n\n    child_half_size = cell.half_size / 2.0\n    offsets = np.array([\n        [-1, -1, -1], [ 1, -1, -1], [-1,  1, -1], [ 1,  1, -1],\n        [-1, -1,  1], [ 1, -1,  1], [-1,  1,  1], [ 1,  1,  1]\n    ]) * child_half_size\n\n    child_centers = cell.center + offsets\n    \n    # Partition particles\n    particle_coords = particles[cell.particle_indices]\n    relative_coords = particle_coords - cell.center\n    \n    child_indices_map = collections.defaultdict(list)\n    for i, p_idx in enumerate(cell.particle_indices):\n        pos = relative_coords[i]\n        child_num = ( (1 if pos[0] >= 0 else 0) +\n                      (2 if pos[1] >= 0 else 0) +\n                      (4 if pos[2] >= 0 else 0) )\n        child_indices_map[child_num].append(p_idx)\n\n    for i in range(8):\n        child_center = child_centers[i]\n        child = Cell(child_center, child_half_size, cell.depth + 1, parent=cell)\n        if i in child_indices_map:\n            child.particle_indices = child_indices_map[i]\n            build_octree_recursive(child, particles, C, D)\n        else:\n            # Empty cell, mark as leaf but it won't be collected\n            child.is_leaf = True\n        cell.children.append(child)\n\ndef get_leaf_cells(cell):\n    \"\"\"Collects all non-empty leaf cells from the tree.\"\"\"\n    if not cell:\n        return []\n    if cell.is_leaf:\n        return [cell] if len(cell.particle_indices) > 0 else []\n    \n    leaves = []\n    for child in cell.children:\n        leaves.extend(get_leaf_cells(child))\n    return leaves\n\ndef find_interactions(target_leaf, source_cell, theta, results):\n    \"\"\"Recursively finds interactions for a target leaf.\"\"\"\n    if not source_cell or (source_cell.is_leaf and not source_cell.particle_indices):\n        return\n\n    r_st_vec = source_cell.center - target_leaf.center\n    r_st = np.linalg.norm(r_st_vec)\n\n    # Disjointness check\n    is_disjoint = np.any(np.abs(r_st_vec) > (source_cell.half_size + target_leaf.half_size))\n    \n    # MAC test\n    mac_satisfied = False\n    if is_disjoint:\n        # Avoid division by zero, although r_st > 0 if disjoint\n        if r_st > 0 and (source_cell.half_size / r_st) <= theta:\n            mac_satisfied = True\n\n    if mac_satisfied:\n        results['ilist'].append(source_cell)\n    else:\n        if source_cell.is_leaf:\n            if source_cell != target_leaf:\n                 results['near_field_pairs'] +=1\n            else: # Self interaction S=T fails MAC and is leaf\n                 results['near_field_pairs'] +=1\n        else:\n            for child in source_cell.children:\n                find_interactions(target_leaf, child, theta, results)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        (\"sphere\", 4096, 1.0),\n        (\"ring\", 4096, 1.0),\n        (\"clusters\", 4096, 1.0),\n        (\"single\", 1, 1.0),\n        (\"line\", 4096, 1.0),\n    ]\n\n    H = 1.25\n    C = 32\n    D = 20\n    THETA = 0.5\n\n    all_results = []\n\n    for config, N, R in test_cases:\n        particles = generate_particles(config, N, R)\n        \n        root = Cell(center=np.array([0.0, 0.0, 0.0]), half_size=H, depth=0)\n        root.particle_indices = list(range(N))\n        \n        build_octree_recursive(root, particles, C, D)\n        \n        leaf_cells = get_leaf_cells(root)\n        num_leaves = len(leaf_cells)\n\n        if num_leaves == 0:\n            all_results.append([0, 0.0, 0, 0])\n            continue\n\n        interaction_list_sizes = []\n        total_near_field_pairs = 0\n\n        for target_leaf in leaf_cells:\n            # We must be careful about the problem statement interpretation.\n            # \"total count of near-field leaf-pairs encountered over all targets\"\n            # This is sum over T of |{S in L | (S,T) is near field}|\n            # find_interactions should return the count for a given T.\n            \n            # Re-initialize results for each target leaf\n            current_target_results = {'ilist': [], 'near_field_pairs': 0}\n            find_interactions_wrapper(target_leaf, root, THETA, current_target_results)\n\n            interaction_list_sizes.append(len(current_target_results['ilist']))\n            total_near_field_pairs += current_target_results['near_field_pairs']\n\n        max_ilist_size = max(interaction_list_sizes) if interaction_list_sizes else 0\n        mean_ilist_size = np.mean(interaction_list_sizes) if interaction_list_sizes else 0.0\n\n        all_results.append([\n            max_ilist_size,\n            round(mean_ilist_size, 3),\n            total_near_field_pairs,\n            num_leaves\n        ])\n\n    # Final print statement in the exact required format.\n    # The default str() for lists adds spaces, which seems to be what the example format implies.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef find_interactions_wrapper(target_leaf, root, theta, results):\n    \"\"\"\n    Wrapper for recursive interaction finding for a single target leaf.\n    This starts the traversal from the root for a given target.\n    \"\"\"\n    to_visit = [root]\n    while to_visit:\n        source_cell = to_visit.pop(0)\n\n        if not source_cell or (source_cell.is_leaf and not source_cell.particle_indices):\n            continue\n\n        r_st_vec = source_cell.center - target_leaf.center\n        \n        is_disjoint = np.any(np.abs(r_st_vec) > (source_cell.half_size + target_leaf.half_size))\n        \n        mac_satisfied = False\n        if is_disjoint:\n            r_st = np.linalg.norm(r_st_vec)\n            if r_st > 1e-12 and (source_cell.half_size / r_st) <= theta: # Added tolerance for r_st\n                mac_satisfied = True\n\n        if mac_satisfied:\n            results['ilist'].append(source_cell)\n        else:\n            if source_cell.is_leaf:\n                results['near_field_pairs'] += 1\n            else:\n                for child in source_cell.children:\n                    to_visit.append(child)\n\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "While the FMM dramatically reduces the cost of far-field interactions to $O(N)$, the near-field component still requires direct, pairwise force computations, which can constitute a significant computational bottleneck. This final practice  focuses on optimizing this performance-critical kernel. You will implement and compare a standard implementation with a vectorized approach that mimics Single Instruction, Multiple Data (SIMD) principles, a cornerstone of modern high-performance computing.",
            "id": "2392085",
            "problem": "You are given a system of $N$ point particles in three-dimensional space interacting via a softened inverse-square law kernel (gravitational or electrostatic) with potential pairwise force between particles $i$ and $j$ given by\n$$\n\\mathbf{F}_{ij} \\;=\\; G\\,m_i m_j \\,\\frac{\\mathbf{r}_{j}-\\mathbf{r}_{i}}{\\left(\\lVert \\mathbf{r}_{j}-\\mathbf{r}_{i}\\rVert^2 + \\varepsilon^2\\right)^{3/2}},\n$$\nwhere $G$ is a constant, $m_i$ is the mass of particle $i$, $\\mathbf{r}_i \\in \\mathbb{R}^3$ is its position, and $\\varepsilon>0$ is a softening length to avoid singularities at zero separation. The near-field phase of the Fast Multipole Method (FMM) computes exact pairwise interactions only among particles residing in a local neighborhood of each cell, while far-field interactions are approximated by multipole expansions. In this task, you will implement the near-field computation for a group of cells using Single Instruction, Multiple Data (SIMD) principles by batching operations with vectorized array arithmetic.\n\nStart from the following fundamental base: Newton’s law of universal gravitation states that a particle of mass $m_i$ at position $\\mathbf{r}_i$ experiences a force due to another particle of mass $m_j$ at position $\\mathbf{r}_j$ that is proportional to the product $m_i m_j$ and inversely proportional to the square of the separation distance, directed along the line connecting the particles. In numerical simulations, a standard and well-tested modification introduces a softening parameter $\\varepsilon$ to regularize the kernel at short distances. This yields the softened pairwise force above, which is a widely used and physically realistic model in $N$-body computations.\n\nPartition the unit cube domain $[0,1)^3$ into a uniform Cartesian grid of cubic cells of edge length $h>0$. Each cell is indexed by a triple $(c_x,c_y,c_z)$ with $c_x\\in\\{0,\\dots,n_x-1\\}$, $c_y\\in\\{0,\\dots,n_y-1\\}$, $c_z\\in\\{0,\\dots,n_z-1\\}$, where $n_x=\\lceil 1/h\\rceil$, $n_y=\\lceil 1/h\\rceil$, and $n_z=\\lceil 1/h\\rceil$. The near-field interaction list for a cell consists of itself and its adjacent neighbors in a $3\\times 3\\times 3$ stencil, truncated at domain boundaries. To avoid double counting, enforce the following tie-breaking rule: for a given cell with index $(c_x,c_y,c_z)$, only process neighbor offsets $(\\delta_x,\\delta_y,\\delta_z)$ satisfying $(\\delta_x>0)$ or $(\\delta_x=0 \\wedge \\delta_y>0)$ or $(\\delta_x=0 \\wedge \\delta_y=0 \\wedge \\delta_z\\ge 0)$. For the self-cell case $(\\delta_x,\\delta_y,\\delta_z)=(0,0,0)$, restrict pairwise interactions to index-ordered pairs with $i<j$ so that no particle interacts with itself and each unordered pair within the same cell is processed exactly once. For interactions between two distinct cells, apply Newton’s third law by updating forces on both sides with equal and opposite contributions.\n\nYour task is to implement two functions:\n- A reference near-field computation using straightforward nested loops that strictly adheres to the neighborhood definition and tie-breaking rule described above.\n- A SIMD-like near-field computation that uses vectorized array operations to process sources in blocks of width $w\\in\\mathbb{N}$, mimicking a hardware SIMD width such as Advanced Vector Extensions (AVX). The block width $w$ is a parameter. Your implementation should not compute any far-field terms.\n\nUse double-precision arithmetic. Angles are not involved. No explicit physical unit conversion is required; treat all quantities as dimensionless in the code for this assignment.\n\nTest Suite:\nImplement the following test cases inside your program. All random draws must use the specified seeds to ensure deterministic outputs. For all tests, use $G=1$, and express all comparisons using absolute or relative tolerances as explicit floating-point thresholds inside your code.\n\n- Test $1$ (Equivalence on random system): $N=64$, $h=0.5$, $\\varepsilon=10^{-4}$, $w=4$. Generate positions uniformly in $[0,1)$ with seed $1234$. Generate masses uniformly in $[0.5,1.5)$ with the same seed. Compute total near-field forces with the reference method and with the SIMD-like method, and return a boolean indicating whether the maximum absolute difference across all components is less than $10^{-11}$.\n\n- Test $2$ (Self-force zero): $N=1$, $h=1.0$, $\\varepsilon=10^{-3}$, $w=4$. One particle at $\\mathbf{r}=(0.3,0.4,0.5)$ with mass $m=1.0$. Return a boolean indicating whether the computed force vector is exactly the zero vector within an absolute tolerance of $10^{-15}$.\n\n- Test $3$ (Softening finite limit check): Two particles at $\\mathbf{r}_1=(0.4,0.4,0.4)$ and $\\mathbf{r}_2=(0.4+d,0.4,0.4)$ with $d=10^{-9}$, masses $m_1=2.0$, $m_2=3.0$, $h=1.0$, $\\varepsilon=10^{-3}$, $w=8$. Compute the force magnitude on particle $1$ from the SIMD-like method, and compare it to the analytic softened prediction\n$$\n\\lVert \\mathbf{F}_{12}\\rVert \\;=\\; G\\,m_1 m_2 \\,\\frac{d}{\\left(d^2+\\varepsilon^2\\right)^{3/2}}.\n$$\nReturn a boolean indicating whether the relative error is less than $10^{-12}$.\n\n- Test $4$ (Newton’s third law consistency): $N=30$, positions drawn uniformly in $[0,0.4)$ with seed $2023$, masses uniformly in $[0.5,1.5)$ with the same seed, $h=1.0$, $\\varepsilon=10^{-4}$, $w=4$. Compute the total force on all particles using the SIMD-like method and return a boolean indicating whether the Euclidean norm of the sum of all force vectors is less than $10^{-11}$.\n\n- Test $5$ (Block width invariance): $N=80$, positions uniformly in $[0,1)$ with seed $777$, masses uniformly in $[0.5,1.5)$ with the same seed, $h=0.25$, $\\varepsilon=10^{-4}$. Compute SIMD-like forces with $w=2$, $w=4$, and $w=8$. Return a boolean indicating whether the maximum absolute difference between any pair of these results is less than $10^{-11}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is a boolean corresponding to the tests in the order stated above. No additional text should be printed. The program must be a complete, runnable solution without requiring any user input or external files. You must implement both the reference and SIMD-like near-field functions as specified, and use only vectorized array operations (no explicit per-pair Python loops) in the SIMD-like function for cross-cell interactions, with per-particle loops permitted only for same-cell interactions where enforcing $i<j$ is necessary. The entire computation must occur in double precision and must be deterministic given the random seeds.",
            "solution": "The problem statement submitted for analysis is deemed to be valid. It is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard task in computational physics: the implementation of a direct, near-field force calculation for an $N$-body system, which forms a fundamental component of algorithms such as the Fast Multipole Method (FMM). The problem provides precise mathematical definitions, algorithmic constraints, and a comprehensive suite of verifiable test cases. I will therefore proceed with a complete solution.\n\nThe core of the problem is the computation of the total force $\\mathbf{F}_i = \\sum_{j \\neq i} \\mathbf{F}_{ij}$ on each particle $i$ in a system of $N$ particles, where the pairwise force is given by the softened inverse-square law:\n$$\n\\mathbf{F}_{ij} \\;=\\; G\\,m_i m_j \\,\\frac{\\mathbf{r}_{j}-\\mathbf{r}_{i}}{\\left(\\lVert \\mathbf{r}_{j}-\\mathbf{r}_{i}\\rVert^2 + \\varepsilon^2\\right)^{3/2}}\n$$\nA direct summation over all pairs has a computational cost of $O(N^2)$, which is prohibitive for large $N$. The FMM mitigates this cost by decomposing interactions into near-field and far-field contributions. This problem focuses exclusively on the near-field part, where interactions are computed directly.\n\nTo structure this computation, the domain is partitioned into a grid of cubic cells. The near-field of a particle consists of particles within its own cell and adjacent cells. For a given cell, its interaction list comprises itself and its $26$ neighbors in a $3 \\times 3 \\times 3$ stencil. To ensure each pair of particles is considered exactly once and to uphold Newton's third law ($\\mathbf{F}_{ij} = -\\mathbf{F}_{ji}$), a systematic rule for processing cell pairs is required. The problem specifies a lexicographical tie-breaking rule on the cell offset vector $(\\delta_x, \\delta_y, \\delta_z)$, which guarantees that for any two distinct interacting cells, the calculation is performed only once.\n\nWe will construct two implementations as required: a reference implementation using explicit loops, which serves as a baseline for correctness, and a vectorized implementation that mimics Single Instruction, Multiple Data (SIMD) processing for improved performance on modern computer architectures.\n\nFirst, the particles must be efficiently assigned to their respective cells. Given a particle position $\\mathbf{r}_i = (x_i, y_i, z_i)$ and a cell edge length $h$, the cell index $(c_x, c_y, c_z)$ is found by $c_k = \\lfloor r_{i,k} / h \\rfloor$. A single scalar key can be computed for each cell, for instance, via $k = c_x n_y n_z + c_y n_z + c_z$, which facilitates sorting and grouping of particles by cell.\n\n**Reference Implementation**\n\nThis implementation follows the prescribed logic directly using nested loops.\n$1$. A data structure, such as a dictionary, is created to map each cell index $(c_x, c_y, c_z)$ to a list of particle indices that reside within it.\n$2$. The algorithm iterates through every cell $C_1$ in the grid.\n$3$. For each $C_1$, it iterates through the $14$ valid neighbor offsets $(\\delta_x, \\delta_y, \\delta_z)$ as defined by the tie-breaking rule. This finds the target cell $C_2$.\n$4$. If $C_1$ and $C_2$ are the same cell (offset is $(0,0,0)$), we compute interactions between its resident particles. To avoid self-interaction and double-counting, we iterate over pairs $(i, j)$ such that $i < j$, compute $\\mathbf{F}_{ij}$, and add it to the total force on particle $i$ and $-\\mathbf{F}_{ij}$ to the total force on particle $j$.\n$5$. If $C_1$ and $C_2$ are distinct cells, we iterate over all particles $i \\in C_1$ and all particles $j \\in C_2$. For each pair, we compute $\\mathbf{F}_{ij}$ and update the forces on both particles accordingly.\n\n**Vectorized (SIMD-like) Implementation**\n\nThis approach leverages vectorized array operations, as provided by libraries like NumPy, to process multiple data elements in a single operation.\n$1$. **Data Reorganization**: To enable efficient block processing, the particle data arrays (positions and masses) are sorted based on their cell keys. This physically groups particles from the same cell together in memory. We must also store the reverse mapping to restore the final force array to its original order.\n$2$. **Cell Pair Iteration**: The main loop structure over cell pairs remains similar to the reference implementation. However, instead of lists of indices, we now deal with slices of the sorted arrays that correspond to the particles in a given cell.\n$3$. **Cross-Cell Interaction**: This is where vectorization is most effective. For an interaction between a target cell $C_1$ and a source cell $C_2$, we process particles from $C_2$ in blocks of a specified width $w$. For each block of $w$ source particles, we compute its interaction with all target particles in $C_1$ simultaneously using broadcasting:\n    - Let positions for $N_t$ targets in $C_1$ be $\\mathbf{R}_t$ (shape $(N_t, 3)$) and for a block of $w$ sources in $C_2$ be $\\mathbf{R}_s$ (shape $(w, 3)$).\n    - The displacement vectors are computed as a tensor: $\\Delta\\mathbf{R} = \\mathbf{R}_s[None, :, :] - \\mathbf{R}_t[:, None, :]$, resulting in shape $(N_t, w, 3)$.\n    - The squared distances $\\lVert \\Delta\\mathbf{R} \\rVert^2$ and subsequent force calculations are performed on these tensors.\n    - The total force on each target particle from the source block is found by summing over the source dimension (axis $1$).\n    - By Newton's third law, the force on each source particle in the block is the negative sum over the target dimension (axis $0$). These force contributions are then accumulated into the global force array.\n$4$. **Same-Cell Interaction**: The condition $i < j$ is inherently serial and does not vectorize efficiently with simple array operations. As permitted by the problem statement, this part is implemented with a standard nested loop over the particles within the cell slice, identical to the reference method.\n\nThis vectorized strategy, particularly for cross-cell interactions, significantly reduces the number of explicit Python loops and allows the underlying compiled libraries to exploit hardware-level parallelism, leading to substantial performance gains. The final computed force array for the sorted particles is then reordered back to match the original particle input order. The provided test suite serves to validate the correctness of both implementations.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # --- Helper function for force calculation kernel ---\n    def compute_pairwise_force(pos_i, mass_i, pos_j, mass_j, G, eps):\n        \"\"\"\n        Computes pairwise forces between two sets of particles i and j.\n        This is a vectorized kernel.\n        Returns:\n            F_on_i: Force on particles i from particles j.\n            F_on_j: Force on particles j from particles i.\n        \"\"\"\n        # Broadcasting to compute all pairs of interactions\n        # dr shape: (num_i, num_j, 3)\n        dr = pos_j[np.newaxis, :, :] - pos_i[:, np.newaxis, :]\n        \n        # r_sq shape: (num_i, num_j)\n        r_sq = np.sum(dr**2, axis=2)\n        \n        # inv_r3 shape: (num_i, num_j)\n        inv_r3 = (r_sq + eps**2)**(-1.5)\n        \n        # mass_prod shape: (num_i, num_j)\n        mass_prod = mass_i[:, np.newaxis] * mass_j[np.newaxis, :]\n        \n        # F_matrix shape: (num_i, num_j, 3)\n        # force on i from j\n        F_matrix = G * mass_prod[:, :, np.newaxis] * dr * inv_r3[:, :, np.newaxis]\n        \n        # F_on_i shape: (num_i, 3)\n        F_on_i = np.sum(F_matrix, axis=1)\n        \n        # F_on_j shape: (num_j, 3)\n        F_on_j = -np.sum(F_matrix, axis=0)\n        \n        return F_on_i, F_on_j\n\n    # --- Reference implementation using loops ---\n    def reference_near_field(N, pos, mass, h, G, eps):\n        \"\"\"\n        Reference near-field computation using nested loops.\n        \"\"\"\n        forces = np.zeros((N, 3), dtype=np.float64)\n        \n        if N == 0:\n            return forces\n\n        nx = ny = nz = int(np.ceil(1.0 / h))\n        \n        # Build cell map\n        cell_map = {}\n        for i in range(N):\n            cell_idx = tuple(np.floor(pos[i] / h).astype(int))\n            if cell_idx not in cell_map:\n                cell_map[cell_idx] = []\n            cell_map[cell_idx].append(i)\n\n        # Generate valid neighbor offsets\n        offsets = []\n        for dx in range(-1, 2):\n            for dy in range(-1, 2):\n                for dz in range(-1, 2):\n                    if dx > 0 or (dx == 0 and dy > 0) or (dx == 0 and dy == 0 and dz >= 0):\n                        offsets.append((dx, dy, dz))\n        \n        for cx in range(nx):\n            for cy in range(ny):\n                for cz in range(nz):\n                    c1_idx = (cx, cy, cz)\n                    if c1_idx not in cell_map:\n                        continue\n                    \n                    particles_c1 = cell_map[c1_idx]\n                    \n                    for dx, dy, dz in offsets:\n                        c2_idx = (cx + dx, cy + dy, cz + dz)\n                        \n                        if not (0 <= c2_idx[0] < nx and 0 <= c2_idx[1] < ny and 0 <= c2_idx[2] < nz):\n                            continue\n                        \n                        if c2_idx not in cell_map:\n                            continue\n                        \n                        particles_c2 = cell_map[c2_idx]\n\n                        if c1_idx == c2_idx: # Same-cell interaction\n                            for i_idx, p_i in enumerate(particles_c1):\n                                for p_j in particles_c1[i_idx + 1:]:\n                                    dr = pos[p_j] - pos[p_i]\n                                    r_sq = np.sum(dr**2)\n                                    inv_r3 = (r_sq + eps**2)**(-1.5)\n                                    f_vec = G * mass[p_i] * mass[p_j] * dr * inv_r3\n                                    forces[p_i] += f_vec\n                                    forces[p_j] -= f_vec\n                        else: # Cross-cell interaction\n                            for p_i in particles_c1:\n                                for p_j in particles_c2:\n                                    dr = pos[p_j] - pos[p_i]\n                                    r_sq = np.sum(dr**2)\n                                    inv_r3 = (r_sq + eps**2)**(-1.5)\n                                    f_vec = G * mass[p_i] * mass[p_j] * dr * inv_r3\n                                    forces[p_i] += f_vec\n                                    forces[p_j] -= f_vec\n        return forces\n\n    # --- SIMD-like implementation using vectorization ---\n    def simd_like_near_field(N, pos, mass, h, G, eps, w):\n        \"\"\"\n        SIMD-like near-field computation using vectorized array operations.\n        \"\"\"\n        forces_sorted = np.zeros((N, 3), dtype=np.float64)\n        if N == 0:\n            return forces_sorted\n        \n        nx = ny = nz = int(np.ceil(1.0 / h))\n\n        # Reorganize data by cell\n        cell_indices_per_particle = np.floor(pos / h).astype(int)\n        cell_keys = cell_indices_per_particle[:, 0] * (ny * nz) + cell_indices_per_particle[:, 1] * nz + cell_indices_per_particle[:, 2]\n        \n        sorted_indices = np.argsort(cell_keys)\n        unsorted_indices = np.argsort(sorted_indices)\n\n        pos_sorted = pos[sorted_indices]\n        mass_sorted = mass[sorted_indices]\n        \n        unique_cell_keys, cell_starts = np.unique(cell_keys[sorted_indices], return_index=True)\n        cell_counts = np.diff(np.append(cell_starts, N))\n\n        cell_info = {key: {'start': start, 'count': count} \n                     for key, start, count in zip(unique_cell_keys, cell_starts, cell_counts)}\n\n        # Generate valid neighbor offsets\n        offsets = []\n        for dx in range(-1, 2):\n            for dy in range(-1, 2):\n                for dz in range(-1, 2):\n                    if dx > 0 or (dx == 0 and dy > 0) or (dx == 0 and dy == 0 and dz >= 0):\n                        offsets.append((dx, dy, dz))\n        \n        for cx in range(nx):\n            for cy in range(ny):\n                for cz in range(nz):\n                    c1_key = cx * (ny * nz) + cy * nz + cz\n                    if c1_key not in cell_info:\n                        continue\n                    \n                    c1_meta = cell_info[c1_key]\n                    c1_start, c1_count = c1_meta['start'], c1_meta['count']\n                    c1_end = c1_start + c1_count\n                    \n                    pos_c1 = pos_sorted[c1_start:c1_end]\n                    mass_c1 = mass_sorted[c1_start:c1_end]\n\n                    for dx, dy, dz in offsets:\n                        ncx, ncy, ncz = cx + dx, cy + dy, cz + dz\n                        \n                        if not (0 <= ncx < nx and 0 <= ncy < ny and 0 <= ncz < nz):\n                            continue\n                        \n                        c2_key = ncx * (ny * nz) + ncy * nz + ncz\n                        if c2_key not in cell_info:\n                            continue\n                        \n                        c2_meta = cell_info[c2_key]\n                        c2_start, c2_count = c2_meta['start'], c2_meta['count']\n                        c2_end = c2_start + c2_count\n\n                        pos_c2 = pos_sorted[c2_start:c2_end]\n                        mass_c2 = mass_sorted[c2_start:c2_end]\n\n                        if c1_key == c2_key: # Same-cell interaction (loop-based)\n                            for i in range(c1_count):\n                                for j in range(i + 1, c1_count):\n                                    p_i, p_j = c1_start + i, c1_start + j\n                                    dr = pos_sorted[p_j] - pos_sorted[p_i]\n                                    r_sq = np.sum(dr**2)\n                                    inv_r3 = (r_sq + eps**2)**(-1.5)\n                                    f_vec = G * mass_sorted[p_i] * mass_sorted[p_j] * dr * inv_r3\n                                    forces_sorted[p_i] += f_vec\n                                    forces_sorted[p_j] -= f_vec\n                        else: # Cross-cell interaction (vectorized)\n                            for k in range(0, c2_count, w):\n                                block_end = min(k + w, c2_count)\n                                pos_c2_block = pos_c2[k:block_end]\n                                mass_c2_block = mass_c2[k:block_end]\n\n                                f_on_c1, f_on_c2_block = compute_pairwise_force(\n                                    pos_c1, mass_c1, pos_c2_block, mass_c2_block, G, eps)\n                                \n                                forces_sorted[c1_start:c1_end] += f_on_c1\n                                forces_sorted[c2_start + k : c2_start + block_end] += f_on_c2_block\n\n        return forces_sorted[unsorted_indices]\n\n    # --- Test Suite ---\n    results = []\n    G_val = 1.0\n\n    # Test 1: Equivalence on random system\n    N1, h1, eps1, w1 = 64, 0.5, 1e-4, 4\n    rng1 = np.random.default_rng(1234)\n    pos1 = rng1.uniform(0, 1, size=(N1, 3))\n    mass1 = rng1.uniform(0.5, 1.5, size=N1)\n    f_ref = reference_near_field(N1, pos1, mass1, h1, G_val, eps1)\n    f_simd = simd_like_near_field(N1, pos1, mass1, h1, G_val, eps1, w1)\n    results.append(np.max(np.abs(f_ref - f_simd)) < 1e-11)\n\n    # Test 2: Self-force zero\n    N2, h2, eps2, w2 = 1, 1.0, 1e-3, 4\n    pos2 = np.array([[0.3, 0.4, 0.5]])\n    mass2 = np.array([1.0])\n    f_self = simd_like_near_field(N2, pos2, mass2, h2, G_val, eps2, w2)\n    results.append(np.all(np.abs(f_self) < 1e-15))\n\n    # Test 3: Softening finite limit check\n    h3, eps3, w3 = 1.0, 1e-3, 8\n    d3 = 1e-9\n    pos3 = np.array([[0.4, 0.4, 0.4], [0.4 + d3, 0.4, 0.4]])\n    mass3 = np.array([2.0, 3.0])\n    f_soft = simd_like_near_field(2, pos3, mass3, h3, G_val, eps3, w3)\n    f_num = np.linalg.norm(f_soft[0])\n    f_analytic = G_val * mass3[0] * mass3[1] * d3 / (d3**2 + eps3**2)**1.5\n    results.append(abs(f_num - f_analytic) / f_analytic < 1e-12)\n\n    # Test 4: Newton’s third law consistency\n    N4, h4, eps4, w4 = 30, 1.0, 1e-4, 4\n    rng4 = np.random.default_rng(2023)\n    pos4 = rng4.uniform(0, 0.4, size=(N4, 3))\n    mass4 = rng4.uniform(0.5, 1.5, size=N4)\n    f_newton = simd_like_near_field(N4, pos4, mass4, h4, G_val, eps4, w4)\n    total_force = np.sum(f_newton, axis=0)\n    results.append(np.linalg.norm(total_force) < 1e-11)\n\n    # Test 5: Block width invariance\n    N5, h5, eps5 = 80, 0.25, 1e-4\n    rng5 = np.random.default_rng(777)\n    pos5 = rng5.uniform(0, 1, size=(N5, 3))\n    mass5 = rng5.uniform(0.5, 1.5, size=N5)\n    f_w2 = simd_like_near_field(N5, pos5, mass5, h5, G_val, eps5, w=2)\n    f_w4 = simd_like_near_field(N5, pos5, mass5, h5, G_val, eps5, w=4)\n    f_w8 = simd_like_near_field(N5, pos5, mass5, h5, G_val, eps5, w=8)\n    diff1 = np.max(np.abs(f_w2 - f_w4))\n    diff2 = np.max(np.abs(f_w2 - f_w8))\n    results.append(diff1 < 1e-11 and diff2 < 1e-11)\n    \n    print(f\"[{','.join([str(r).lower() for r in results])}]\")\n\nsolve()\n```"
        }
    ]
}