{
    "hands_on_practices": [
        {
            "introduction": "The heart of Algebraic Multigrid (AMG) lies in its ability to represent error on a fine grid using a smaller set of variables on a coarse grid. This is achieved through an interpolation operator, whose entries (weights) are not arbitrary. This exercise will guide you through the fundamental process of deriving these interpolation weights directly from the system matrix, a cornerstone of classical AMG theory. By working through the calculation for an anisotropic Poisson equation, you will gain a concrete understanding of how the operator's structure informs the interpolation strategy.",
            "id": "22396",
            "problem": "In computational electrostatics, solving the Poisson equation on a grid often leads to a large, sparse system of linear equations, $A\\mathbf{x} = \\mathbf{b}$. Algebraic Multigrid (AMG) methods are powerful iterative solvers for such systems. A key component of AMG is the construction of an interpolation operator, $P$, that relates values on a coarse grid to a fine grid.\n\nThe grid points (nodes) are partitioned into a set of coarse (C) points and fine (F) points. The value of the error, $e_i$, at a fine point $i$ is approximated by a weighted sum of the error values at its neighboring coarse points, $j \\in N_i^C$:\n$$ e_i = \\sum_{j \\in N_i^C} w_{ij} e_j $$\nThe weights $w_{ij}$ form the entries of the interpolation operator $P$.\n\nA standard approach to derive these weights for a fine point $i$ is from its corresponding row in the system matrix $A$. The stencil equation for a smooth error vector $e$ is $(Ae)_i \\approx 0$. By approximating the contribution from neighboring fine points, one arrives at the following formula for the interpolation weights:\n$$ w_{ij} = \\frac{-A_{ij}}{A_{ii} + \\sum_{k \\in N_i^F} A_{ik}} \\quad \\text{for } j \\in N_i^C $$\nwhere $N_i^C$ is the set of coarse neighbors of $i$, and $N_i^F$ is the set of fine neighbors of $i$.\n\nConsider the two-dimensional anisotropic Poisson equation with constants $\\alpha > 0$ and $\\beta > 0$:\n$$ \\alpha \\frac{\\partial^2 \\phi}{\\partial x^2} + \\beta \\frac{\\partial^2 \\phi}{\\partial y^2} = -\\rho(x,y) $$\nThis equation is discretized on a uniform square grid with spacing $h=1$ using a standard 5-point finite difference stencil. This results in a system matrix $A$.\n\nLet a point $P_0$ be a fine point. Its four neighbors are denoted as $P_N$ (North), $P_S$ (South), $P_E$ (East), and $P_W$ (West). The coarse/fine partitioning of the neighbors is as follows:\n-   $P_N$ and $P_E$ are Coarse (C) points.\n-   $P_S$ and $P_W$ are Fine (F) points.\n\nYour task is to calculate the interpolation weight $w_{0N}$, which connects the fine point $P_0$ to its coarse North neighbor $P_N$. Express your answer in terms of $\\alpha$ and $\\beta$.",
            "solution": "1. Interpolation weight formula for fine point $i$ and coarse neighbor $j$:\n   $$w_{ij}=\\frac{-A_{ij}}{A_{ii}+\\sum_{k\\in N_i^F}A_{ik}}\\,. $$\n\n2. Entries of the anisotropic 5‐point stencil:\n   $$A_{ii}=2\\alpha+2\\beta,\\quad\n     A_{i,E}=A_{i,W}=-\\alpha,\\quad\n     A_{i,N}=A_{i,S}=-\\beta.$$\n\n3. For $P_0$, coarse neighbors $N,E$; fine neighbors $S,W$:\n   $$\\sum_{k\\in N_0^F}A_{0k}=A_{0,S}+A_{0,W}=(-\\beta)+(-\\alpha)=-(\\alpha+\\beta).$$\n\n4. Numerator for $w_{0N}$:\n   $$-A_{0,N}=-(-\\beta)=\\beta.$$\n\n5. Denominator:\n   $$A_{00}+\\sum_{F}A_{0k}=2(\\alpha+\\beta)-(\\alpha+\\beta)=\\alpha+\\beta.$$\n\n6. Therefore,\n   $$w_{0N}=\\frac{\\beta}{\\alpha+\\beta}.$$",
            "answer": "$$\\boxed{\\frac{\\beta}{\\alpha+\\beta}}$$"
        },
        {
            "introduction": "The effectiveness of any iterative solver is measured by its convergence factor, which quantifies the rate of error reduction and should ideally be much smaller than one. In this practice, you will analyze a deliberately constructed two-grid method with a deficient interpolation operator to see why it fails to converge effectively. By explicitly computing the two-grid error-propagation matrix and its spectral radius, you will diagnose the failure and develop a deeper appreciation for how proper interpolation is critical for accurately representing coarse-grid corrections.",
            "id": "2372552",
            "problem": "Consider the linear system obtained by the standard second-order finite-difference discretization of the one-dimensional Poisson equation $-u'' = f$ with homogeneous Dirichlet boundary conditions on the unit interval, using $N=3$ interior points. The resulting stiffness matrix is\n$$\nA \\in \\mathbb{R}^{3 \\times 3}, \\quad\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nDefine a two-grid method within the framework of Algebraic Multigrid (AMG). Choose a deliberately poor coarse space consisting of the single coarse variable at the middle node, and define the interpolation operator\n$$\nP \\in \\mathbb{R}^{3 \\times 1}, \\quad P = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nUse restriction by transpose, $R = P^{\\top}$, and the Galerkin coarse operator $A_{c} = R A P \\in \\mathbb{R}^{1 \\times 1}$. Use one post-smoothing step of weighted Jacobi with relaxation weight $\\omega = \\frac{2}{3}$, so that the smoother is\n$$\nS = I - \\omega D^{-1} A, \\quad D = \\operatorname{diag}(A).\n$$\nLet the two-grid error-propagation matrix be\n$$\nE = \\bigl(I - P A_{c}^{-1} R A\\bigr)\\, S.\n$$\nCompute the spectral radius $\\rho(E)$ exactly and provide your final answer as an exact number (no rounding). The answer is unitless and must be a single real number.",
            "solution": "The problem as stated is mathematically well-posed and provides all necessary information for a unique solution. We are asked to compute the spectral radius of a specifically defined matrix. The provided givens are:\nThe fine-grid system matrix $A \\in \\mathbb{R}^{3 \\times 3}$:\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nThe interpolation operator $P \\in \\mathbb{R}^{3 \\times 1}$:\n$$\nP = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe restriction operator is the transpose of the interpolation operator, $R = P^{\\top}$. The coarse-grid operator is defined by the Galerkin projection $A_{c} = R A P$. The smoother is a weighted Jacobi iteration with weight $\\omega = \\frac{2}{3}$, leading to the smoothing operator $S = I - \\omega D^{-1} A$, where $D = \\operatorname{diag}(A)$. The two-grid error-propagation matrix is explicitly defined as $E = \\bigl(I - P A_{c}^{-1} R A\\bigr)\\, S$.\n\nIt must be noted that the problem text specifies \"one post-smoothing step,\" for which the error propagation matrix is conventionally written as $S \\bigl(I - P A_{c}^{-1} R A\\bigr)$. The provided formula $E = \\bigl(I - P A_{c}^{-1} R A\\bigr)S$ corresponds to a pre-smoothing step. However, the definition of $E$ is explicit and unambiguous, so we shall proceed with the calculation as specified. For square matrices, the spectral radii of $XY$ and $YX$ are identical, so this terminological inaccuracy does not affect the final result.\n\nThe calculation proceeds in several steps.\n\nFirst, we compute the smoother operator $S$. The diagonal matrix $D$ is:\n$$\nD = \\operatorname{diag}(A) = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix} = 2I\n$$\nIts inverse is $D^{-1} = \\frac{1}{2}I$.\nWith the relaxation weight $\\omega = \\frac{2}{3}$, the smoother operator $S$ is:\n$$\nS = I - \\omega D^{-1} A = I - \\frac{2}{3} \\left(\\frac{1}{2}I\\right) A = I - \\frac{1}{3} A\n$$\n$$\nS = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1-\\frac{2}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & 1-\\frac{2}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{3} & 1-\\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}\n$$\n\nNext, we construct the coarse-grid correction operator, which we denote as $C = I - P A_{c}^{-1} R A$.\nThe restriction operator $R$ is:\n$$\nR = P^{\\top} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}\n$$\nThe coarse-grid operator $A_c$ is a $1 \\times 1$ matrix:\n$$\nA_{c} = R A P = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = [2]\n$$\nThe inverse is $A_c^{-1} = [\\frac{1}{2}]$.\nNow we compute the term $P A_c^{-1} R A$:\n$$\nP A_c^{-1} R = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} [\\frac{1}{2}] \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nP A_c^{-1} R A = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ -\\frac{1}{2} & 1 & -\\frac{1}{2} \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThe coarse-grid correction operator $C$ is therefore:\n$$\nC = I - P A_c^{-1} R A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 & 0 \\\\ -\\frac{1}{2} & 1 & -\\frac{1}{2} \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n\nNow we compute the two-grid error-propagation matrix $E = C S$:\n$$\nE = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix}\n1 \\cdot \\frac{1}{3} & 1 \\cdot \\frac{1}{3} & 0 \\\\\n\\frac{1}{2} \\cdot \\frac{1}{3} & \\frac{1}{2} \\cdot \\frac{1}{3} + \\frac{1}{2} \\cdot \\frac{1}{3} & \\frac{1}{2} \\cdot \\frac{1}{3} \\\\\n0 & 1 \\cdot \\frac{1}{3} & 1 \\cdot \\frac{1}{3}\n\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{6} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}\n$$\n\nFinally, we compute the spectral radius $\\rho(E)$ by finding the eigenvalues of $E$. The characteristic equation is $\\det(E - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} \\frac{1}{3}-\\lambda & \\frac{1}{3} & 0 \\\\ \\frac{1}{6} & \\frac{1}{3}-\\lambda & \\frac{1}{6} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3}-\\lambda \\end{pmatrix} = 0\n$$\nTo simplify, let us find the eigenvalues $\\mu$ of $3E$, where $\\lambda = \\mu/3$.\n$$\n3E = \\begin{pmatrix} 1 & 1 & 0 \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nThe characteristic equation is $\\det(3E - \\mu I) = 0$:\n$$\n\\det\\begin{pmatrix} 1-\\mu & 1 & 0 \\\\ \\frac{1}{2} & 1-\\mu & \\frac{1}{2} \\\\ 0 & 1 & 1-\\mu \\end{pmatrix} = 0\n$$\n$$\n(1-\\mu) \\left| \\begin{matrix} 1-\\mu & \\frac{1}{2} \\\\ 1 & 1-\\mu \\end{matrix} \\right| - 1 \\left| \\begin{matrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 1-\\mu \\end{matrix} \\right| = 0\n$$\n$$\n(1-\\mu)\\left( (1-\\mu)^2 - \\frac{1}{2} \\right) - \\left( \\frac{1}{2}(1-\\mu) \\right) = 0\n$$\n$$\n(1-\\mu)\\left( (1-\\mu)^2 - \\frac{1}{2} - \\frac{1}{2} \\right) = 0\n$$\n$$\n(1-\\mu)\\left( (1-\\mu)^2 - 1 \\right) = 0\n$$\nThis equation yields three solutions for $\\mu$:\n1. $1-\\mu = 0 \\implies \\mu_1 = 1$.\n2. $(1-\\mu)^2 - 1 = 0 \\implies (1-\\mu)^2 = 1 \\implies 1-\\mu = \\pm 1$.\n   - $1-\\mu = 1 \\implies \\mu_2 = 0$.\n   - $1-\\mu = -1 \\implies \\mu_3 = 2$.\n\nThe eigenvalues of $3E$ are $\\{0, 1, 2\\}$. The eigenvalues of $E$ are $\\lambda_i = \\mu_i/3$, so they are $\\{\\frac{0}{3}, \\frac{1}{3}, \\frac{2}{3}\\} = \\{0, \\frac{1}{3}, \\frac{2}{3}\\}$.\n\nThe spectral radius $\\rho(E)$ is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(E) = \\max\\left(|0|, \\left|\\frac{1}{3}\\right|, \\left|\\frac{2}{3}\\right|\\right) = \\frac{2}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "While theoretical algorithms are defined with exact arithmetic, real-world computations are performed using finite-precision floating-point numbers, a distinction with profound practical consequences. This problem explores a critical aspect of numerical stability: the construction of the coarse-grid operator via the Galerkin product, $A_c = P^{\\top} A P$. You will investigate a hypothetical scenario where small, bounded round-off errors during this matrix multiplication can accumulate and cause the theoretically symmetric positive definite operator to lose this essential property, leading to solver failure. This exercise highlights the tangible link between numerical analysis theory and computational practice.",
            "id": "2372482",
            "problem": "Consider Algebraic Multigrid (AMG) with a V-cycle applied to a symmetric positive definite linear system. The coarse-grid operator is constructed by the Galerkin triple product $A_{c} = R A P$, where $R = P^{\\top}$. Let the fine-grid system matrix be the parametrized family\n$$\nA(\\varepsilon) = v v^{\\top} + \\varepsilon I_{3},\n$$\nwith $v = (\\,1,\\,1,\\,0\\,)^{\\top}$, $\\varepsilon > 0$, and $I_{3}$ the $3 \\times 3$ identity. Let the prolongation be the $3 \\times 2$ matrix\n$$\nP = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{pmatrix},\n$$\nand take the restriction as $R = P^{\\top}$. The code constructs only the upper-triangular entries of $A_{c}$ by the standard three-nested-loop matrix multiplication formula,\n$$\n(A_{c})_{ij} = \\sum_{p=1}^{3} \\sum_{q=1}^{3} R_{i p}\\, A_{p q}\\, P_{q j} \\quad \\text{for } i \\leq j,\n$$\nand then copies $(A_{c})_{ij}$ to $(A_{c})_{ji}$ exactly. Assume that each floating-point multiplication is rounded to nearest and obeys the standard relative error model $\\operatorname{fl}(x y) = (x y)(1+\\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff. Additions by zero and the final symmetric copy are exact. No other rounding errors occur.\n\nDefine a “catastrophic failure of the V-cycle” to mean that the computed coarse-grid operator loses symmetric positive definiteness on the coarse grid, equivalently that its determinant is non-positive.\n\nDetermine, as a function of $u$, the smallest $\\varepsilon > 0$ such that there exist admissible rounding errors (with $|\\delta| \\leq u$ at the multiplications used to build $A_{c}$) for which the computed $A_{c}$ is not positive definite. Provide a closed-form analytic expression in $u$ for this critical $\\varepsilon$ and do not round your answer.",
            "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It represents a tractable problem in numerical analysis concerning the stability of algebraic multigrid methods under floating-point arithmetic. I will now proceed with the solution.\n\nThe fine-grid system matrix is given by the family $A(\\varepsilon)$ for $\\varepsilon > 0$:\n$$\nA(\\varepsilon) = v v^{\\top} + \\varepsilon I_{3}\n$$\nwhere $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $I_3$ is the $3 \\times 3$ identity matrix.\nFirst, we construct the matrix $v v^{\\top}$:\n$$\nv v^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThus, the fine-grid matrix $A(\\varepsilon)$ is:\n$$\nA(\\varepsilon) = \\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\varepsilon \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+\\varepsilon & 1 & 0 \\\\ 1 & 1+\\varepsilon & 0 \\\\ 0 & 0 & \\varepsilon \\end{pmatrix}\n$$\nFor $\\varepsilon > 0$, this matrix is symmetric and positive definite (SPD). Its eigenvalues are $\\varepsilon$, $2+\\varepsilon$, and $0+\\varepsilon=\\varepsilon$. Since all are positive, $A(\\varepsilon)$ is SPD.\n\nThe coarse-grid operator $A_c$ is constructed via the Galerkin product $A_c = R A(\\varepsilon) P$. The prolongation $P$ and restriction $R$ are given by:\n$$\nP = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\quad R = P^{\\top} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nIn exact arithmetic, the coarse-grid operator is:\n$$\nA_c = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1+\\varepsilon & 1 & 0 \\\\ 1 & 1+\\varepsilon & 0 \\\\ 0 & 0 & \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1+\\varepsilon & 1 \\\\ 1 & 1+\\varepsilon \\end{pmatrix}\n$$\nThe determinant of the exact $A_c$ is $\\det(A_c) = (1+\\varepsilon)^2 - 1 = \\varepsilon^2 + 2\\varepsilon$. For $\\varepsilon > 0$, $\\det(A_c) > 0$. The trace is $2(1+\\varepsilon) > 0$. Thus, the exact $A_c$ is also SPD, as expected.\n\nWe now analyze the computation of $A_c$ in floating-point arithmetic. Let $\\tilde{A}_c$ be the computed matrix. The problem states that the upper-triangular entries $(\\tilde{A}_c)_{ij}$ for $i \\leq j$ are computed using the formula:\n$$\n(\\tilde{A}_c)_{ij} = \\sum_{p=1}^{3} \\sum_{q=1}^{3} \\operatorname{fl}(R_{ip} A_{pq} P_{qj})\n$$\nwhere the summation itself is exact, as additions by zero are exact and for this specific problem, there is at most one non-zero term in each sum. The product $R_{ip} A_{pq} P_{qj}$ involves two multiplications. Following the specified error model $\\operatorname{fl}(xy) = xy(1+\\delta)$ with $|\\delta| \\leq u$, the computation of a triple product $xyz$ as $\\operatorname{fl}(\\operatorname{fl}(xy)z)$ results in:\n$$\n\\operatorname{fl}(xyz) = ((xy)(1+\\delta_1)z)(1+\\delta_2) = xyz(1+\\delta_1)(1+\\delta_2)\n$$\nwhere $|\\delta_1|, |\\delta_2| \\leq u$. The error factor $(1+\\delta_1)(1+\\delta_2)$ lies in the interval $[(1-u)^2, (1+u)^2]$.\n\nLet's compute the entries of $\\tilde{A}_c$:\nFor $(\\tilde{A}_c)_{11}$ ($i=1, j=1$): The only non-zero term in the sum is for $p=1, q=1$. The term is $R_{11}A_{11}P_{11} = 1 \\cdot (1+\\varepsilon) \\cdot 1 = 1+\\varepsilon$. Its computed value is:\n$$\n(\\tilde{A}_c)_{11} = (1+\\varepsilon)(1+\\delta_a)(1+\\delta_b)\n$$\nWe can choose $\\delta_a, \\delta_b \\in [-u, u]$ to produce any value in the range $[(1+\\varepsilon)(1-u)^2, (1+\\varepsilon)(1+u)^2]$.\n\nFor $(\\tilde{A}_c)_{22}$ ($i=2, j=2$): The only non-zero term is for $p=2, q=2$. The term is $R_{22}A_{22}P_{22} = 1 \\cdot (1+\\varepsilon) \\cdot 1 = 1+\\varepsilon$. Its computed value is:\n$$\n(\\tilde{A}_c)_{22} = (1+\\varepsilon)(1+\\delta_c)(1+\\delta_d)\n$$\nwith a value in $[(1+\\varepsilon)(1-u)^2, (1+\\varepsilon)(1+u)^2]$.\n\nFor $(\\tilde{A}_c)_{12}$ ($i=1, j=2$): The only non-zero term is for $p=1, q=2$. The term is $R_{11}A_{12}P_{22} = 1 \\cdot 1 \\cdot 1 = 1$. Its computed value is:\n$$\n(\\tilde{A}_c)_{12} = 1 \\cdot (1+\\delta_e)(1+\\delta_f)\n$$\nwith a value in $[(1-u)^2, (1+u)^2]$.\n\nThe lower-triangular entry $(\\tilde{A}_c)_{21}$ is an exact copy of $(\\tilde{A}_c)_{12}$. Thus, the computed matrix $\\tilde{A}_c$ is symmetric:\n$$\n\\tilde{A}_c = \\begin{pmatrix} (1+\\varepsilon)(1+\\delta_{11}) & 1 \\cdot (1+\\delta_{12}) \\\\ 1 \\cdot (1+\\delta_{12}) & (1+\\varepsilon)(1+\\delta_{22}) \\end{pmatrix}\n$$\nwhere each factor $(1+\\delta_{ij})$ represents a term of the form $(1+\\delta_1)(1+\\delta_2)$ and lies in $[(1-u)^2, (1+u)^2]$.\n\nA \"catastrophic failure\" occurs if $\\tilde{A}_c$ is not positive definite. Since $\\tilde{A}_c$ is symmetric, we check its trace and determinant.\nThe trace is $\\operatorname{Tr}(\\tilde{A}_c) = (1+\\varepsilon)((1+\\delta_{11}) + (1+\\delta_{22}))$. Since $\\varepsilon > 0$, $(1+\\varepsilon) > 1$. The error factors $(1+\\delta_{ii}) \\geq (1-u)^2 > 0$ for typical, small unit roundoff $u$. Thus, the trace is positive.\nFailure must therefore arise from a non-positive determinant: $\\det(\\tilde{A}_c) \\leq 0$.\n$$\n\\det(\\tilde{A}_c) = (1+\\varepsilon)^2 (1+\\delta_{11})(1+\\delta_{22}) - (1 \\cdot (1+\\delta_{12}))^2 \\leq 0\n$$\nThis inequality can be rewritten as:\n$$\n(1+\\varepsilon)^2 \\leq \\frac{(1+\\delta_{12})^2}{(1+\\delta_{11})(1+\\delta_{22})}\n$$\nFor a given $\\varepsilon > 0$, failure is possible if there exist admissible rounding errors that satisfy this condition. To find the smallest $\\varepsilon$ for which this is possible, we must find the maximum possible value of the right-hand side. This corresponds to the \"worst-case\" rounding scenario.\n\nThe right-hand side is maximized when the numerator $(1+\\delta_{12})^2$ is maximized and the denominator $(1+\\delta_{11})(1+\\delta_{22})$ is minimized. The rounding error factors $(1+\\delta_{ij})$ are independent and lie in $[(1-u)^2, (1+u)^2]$.\n- Maximize numerator: Choose $(1+\\delta_{12}) = (1+u)^2$.\n- Minimize denominator: Choose $(1+\\delta_{11}) = (1-u)^2$ and $(1+\\delta_{22}) = (1-u)^2$.\n\nSubstituting these worst-case values gives the condition for failure to be possible:\n$$\n(1+\\varepsilon)^2 \\leq \\frac{((1+u)^2)^2}{((1-u)^2)((1-u)^2)} = \\frac{(1+u)^4}{(1-u)^4} = \\left(\\frac{1+u}{1-u}\\right)^4\n$$\nTaking the square root of both sides (all terms are positive):\n$$\n1+\\varepsilon \\leq \\left(\\frac{1+u}{1-u}\\right)^2\n$$\nSolving for $\\varepsilon$:\n$$\n\\varepsilon \\leq \\left(\\frac{1+u}{1-u}\\right)^2 - 1 = \\frac{(1+u)^2 - (1-u)^2}{(1-u)^2} = \\frac{(1+2u+u^2) - (1-2u+u^2)}{(1-u)^2} = \\frac{4u}{(1-u)^2}\n$$\nThis means that a catastrophic failure is possible if and only if $0 < \\varepsilon \\leq \\frac{4u}{(1-u)^2}$. For any $\\varepsilon$ greater than this value, $\\det(\\tilde{A}_c)$ is guaranteed to be positive regardless of the specific rounding errors.\n\nThe problem asks for the \"smallest $\\varepsilon > 0$\" for which failure is possible. The set of such $\\varepsilon$ is the interval $(0, \\frac{4u}{(1-u)^2}]$. This set does not contain a minimum element; its infimum is $0$. However, an answer of $0$ would be independent of $u$ and physically trivial, suggesting it is not the intended answer. The standard interpretation of such a question in a scientific context is to ask for the critical threshold value that defines the boundary of the failure regime. This critical value is the supremum of the set, which is $\\varepsilon_{crit} = \\frac{4u}{(1-u)^2}$. This value represents the largest value of $\\varepsilon$ for which catastrophic failure is a computational possibility. It is the proper, non-trivial function of $u$ that the problem seeks.",
            "answer": "$$\n\\boxed{\\frac{4u}{(1-u)^{2}}}\n$$"
        }
    ]
}