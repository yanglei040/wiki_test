{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Algebraic Multigrid (AMG), we must move from theory to practice. This first exercise focuses on a fundamental building block: the interpolation operator, which is responsible for transferring corrections from the coarse grid back to the fine grid. By deriving an interpolation weight directly from the system matrix and a given coarse/fine splitting, you will gain a concrete understanding of how AMG builds its grid-transfer operators purely from the algebraic information in the matrix $A$ .",
            "id": "22396",
            "problem": "In computational electrostatics, solving the Poisson equation on a grid often leads to a large, sparse system of linear equations, $A\\mathbf{x} = \\mathbf{b}$. Algebraic Multigrid (AMG) methods are powerful iterative solvers for such systems. A key component of AMG is the construction of an interpolation operator, $P$, that relates values on a coarse grid to a fine grid.\n\nThe grid points (nodes) are partitioned into a set of coarse (C) points and fine (F) points. The value of the error, $e_i$, at a fine point $i$ is approximated by a weighted sum of the error values at its neighboring coarse points, $j \\in N_i^C$:\n$$ e_i = \\sum_{j \\in N_i^C} w_{ij} e_j $$\nThe weights $w_{ij}$ form the entries of the interpolation operator $P$.\n\nA standard approach to derive these weights for a fine point $i$ is from its corresponding row in the system matrix $A$. The stencil equation for a smooth error vector $e$ is $(Ae)_i \\approx 0$. By approximating the contribution from neighboring fine points, one arrives at the following formula for the interpolation weights:\n$$ w_{ij} = \\frac{-A_{ij}}{A_{ii} + \\sum_{k \\in N_i^F} A_{ik}} \\quad \\text{for } j \\in N_i^C $$\nwhere $N_i^C$ is the set of coarse neighbors of $i$, and $N_i^F$ is the set of fine neighbors of $i$.\n\nConsider the two-dimensional anisotropic Poisson equation with constants $\\alpha > 0$ and $\\beta > 0$:\n$$ \\alpha \\frac{\\partial^2 \\phi}{\\partial x^2} + \\beta \\frac{\\partial^2 \\phi}{\\partial y^2} = -\\rho(x,y) $$\nThis equation is discretized on a uniform square grid with spacing $h=1$ using a standard 5-point finite difference stencil. This results in a system matrix $A$.\n\nLet a point $P_0$ be a fine point. Its four neighbors are denoted as $P_N$ (North), $P_S$ (South), $P_E$ (East), and $P_W$ (West). The coarse/fine partitioning of the neighbors is as follows:\n-   $P_N$ and $P_E$ are Coarse (C) points.\n-   $P_S$ and $P_W$ are Fine (F) points.\n\nYour task is to calculate the interpolation weight $w_{0N}$, which connects the fine point $P_0$ to its coarse North neighbor $P_N$. Express your answer in terms of $\\alpha$ and $\\beta$.",
            "solution": "1. Interpolation weight formula for fine point $i$ and coarse neighbor $j$:\n   $$w_{ij}=\\frac{-A_{ij}}{A_{ii}+\\sum_{k\\in N_i^F}A_{ik}}\\,. $$\n\n2. Entries of the anisotropic 5-point stencil:\n   $$A_{ii}=2\\alpha+2\\beta,\\quad\n     A_{i,E}=A_{i,W}=-\\alpha,\\quad\n     A_{i,N}=A_{i,S}=-\\beta.$$\n\n3. For $P_0$, coarse neighbors $N,E$; fine neighbors $S,W$:\n   $$\\sum_{k\\in N_0^F}A_{0k}=A_{0,S}+A_{0,W}=(-\\beta)+(-\\alpha)=-(\\alpha+\\beta).$$\n\n4. Numerator for $w_{0N}$:\n   $$-A_{0,N}=-(-\\beta)=\\beta.$$\n\n5. Denominator:\n   $$A_{00}+\\sum_{F}A_{0k}=2(\\alpha+\\beta)-(\\alpha+\\beta)=\\alpha+\\beta.$$\n\n6. Therefore,\n   $$w_{0N}=\\frac{\\beta}{\\alpha+\\beta}.$$",
            "answer": "$$\\boxed{\\frac{\\beta}{\\alpha+\\beta}}$$"
        },
        {
            "introduction": "A key principle of multigrid methods is the complementary nature of the smoother and the coarse-grid correction; the former handles high-frequency error, while the latter must effectively eliminate low-frequency error. This practice problem provides a powerful lesson by demonstrating what happens when this principle is violated . By analyzing a deliberately ineffective interpolation operator, you will see precisely how a poor choice for the coarse-grid representation fails to handle smooth error components, leading to a stalled or slowly converging method.",
            "id": "2372552",
            "problem": "Consider the linear system obtained by the standard second-order finite-difference discretization of the one-dimensional Poisson equation $-u'' = f$ with homogeneous Dirichlet boundary conditions on the unit interval, using $N=3$ interior points. The resulting stiffness matrix is\n$$\nA \\in \\mathbb{R}^{3 \\times 3}, \\quad\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nDefine a two-grid method within the framework of Algebraic Multigrid (AMG). Choose a deliberately poor coarse space consisting of the single coarse variable at the middle node, and define the interpolation operator\n$$\nP \\in \\mathbb{R}^{3 \\times 1}, \\quad P = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nUse restriction by transpose, $R = P^{\\top}$, and the Galerkin coarse operator $A_{c} = R A P \\in \\mathbb{R}^{1 \\times 1}$. Use one post-smoothing step of weighted Jacobi with relaxation weight $\\omega = \\frac{2}{3}$, so that the smoother is\n$$\nS = I - \\omega D^{-1} A, \\quad D = \\operatorname{diag}(A).\n$$\nLet the two-grid error-propagation matrix be\n$$\nE = \\bigl(I - P A_{c}^{-1} R A\\bigr)\\, S.\n$$\nCompute the spectral radius $\\rho(E)$ exactly and provide your final answer as an exact number (no rounding). The answer is unitless and must be a single real number.",
            "solution": "The problem as stated is mathematically well-posed and provides all necessary information for a unique solution. We are asked to compute the spectral radius of a specifically defined matrix. The provided givens are:\nThe fine-grid system matrix $A \\in \\mathbb{R}^{3 \\times 3}$:\n$$\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}\n$$\nThe interpolation operator $P \\in \\mathbb{R}^{3 \\times 1}$:\n$$\nP = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe restriction operator is the transpose of the interpolation operator, $R = P^{\\top}$. The coarse-grid operator is defined by the Galerkin projection $A_{c} = R A P$. The smoother is a weighted Jacobi iteration with weight $\\omega = \\frac{2}{3}$, leading to the smoothing operator $S = I - \\omega D^{-1} A$, where $D = \\operatorname{diag}(A)$. The two-grid error-propagation matrix is explicitly defined as $E = \\bigl(I - P A_{c}^{-1} R A\\bigr)\\, S$.\n\nIt must be noted that the problem text specifies \"one post-smoothing step,\" for which the error propagation matrix is conventionally written as $S \\bigl(I - P A_{c}^{-1} R A\\bigr)$. The provided formula $E = \\bigl(I - P A_{c}^{-1} R A\\bigr)S$ corresponds to a pre-smoothing step. However, the definition of $E$ is explicit and unambiguous, so we shall proceed with the calculation as specified. For square matrices, the spectral radii of $XY$ and $YX$ are identical, so this terminological inaccuracy does not affect the final result.\n\nThe calculation proceeds in several steps.\n\nFirst, we compute the smoother operator $S$. The diagonal matrix $D$ is:\n$$\nD = \\operatorname{diag}(A) = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix} = 2I\n$$\nIts inverse is $D^{-1} = \\frac{1}{2}I$.\nWith the relaxation weight $\\omega = \\frac{2}{3}$, the smoother operator $S$ is:\n$$\nS = I - \\omega D^{-1} A = I - \\frac{2}{3} \\left(\\frac{1}{2}I\\right) A = I - \\frac{1}{3} A\n$$\n$$\nS = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1-\\frac{2}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & 1-\\frac{2}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{3} & 1-\\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}\n$$\n\nNext, we construct the coarse-grid correction operator, which we denote as $C = I - P A_{c}^{-1} R A$.\nThe restriction operator $R$ is:\n$$\nR = P^{\\top} = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix}\n$$\nThe coarse-grid operator $A_c$ is a $1 \\times 1$ matrix:\n$$\nA_{c} = R A P = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = [2]\n$$\nThe inverse is $A_c^{-1} = [\\frac{1}{2}]$.\nNow we compute the term $P A_c^{-1} R A$:\n$$\nP A_c^{-1} R = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} [\\frac{1}{2}] \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\nP A_c^{-1} R A = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ -\\frac{1}{2} & 1 & -\\frac{1}{2} \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nThe coarse-grid correction operator $C$ is therefore:\n$$\nC = I - P A_c^{-1} R A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 0 & 0 & 0 \\\\ -\\frac{1}{2} & 1 & -\\frac{1}{2} \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n\nNow we compute the two-grid error-propagation matrix $E = C S$:\n$$\nE = \\begin{pmatrix} 1 & 0 & 0 \\\\ \\frac{1}{2} & 0 & \\frac{1}{2} \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix}\n1 \\cdot \\frac{1}{3} & 1 \\cdot \\frac{1}{3} & 0 \\\\\n\\frac{1}{2} \\cdot \\frac{1}{3} & \\frac{1}{2} \\cdot \\frac{1}{3} + \\frac{1}{2} \\cdot \\frac{1}{3} & \\frac{1}{2} \\cdot \\frac{1}{3} \\\\\n0 & 1 \\cdot \\frac{1}{3} & 1 \\cdot \\frac{1}{3}\n\\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & 0 \\\\ \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{6} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}\n$$\n\nFinally, we compute the spectral radius $\\rho(E)$ by finding the eigenvalues of $E$. The characteristic equation is $\\det(E - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} \\frac{1}{3}-\\lambda & \\frac{1}{3} & 0 \\\\ \\frac{1}{6} & \\frac{1}{3}-\\lambda & \\frac{1}{6} \\\\ 0 & \\frac{1}{3} & \\frac{1}{3}-\\lambda \\end{pmatrix} = 0\n$$\nTo simplify, let us find the eigenvalues $\\mu$ of $3E$, where $\\lambda = \\mu/3$.\n$$\n3E = \\begin{pmatrix} 1 & 1 & 0 \\\\ \\frac{1}{2} & 1 & \\frac{1}{2} \\\\ 0 & 1 & 1 \\end{pmatrix}\n$$\nThe characteristic equation is $\\det(3E - \\mu I) = 0$:\n$$\n\\det\\begin{pmatrix} 1-\\mu & 1 & 0 \\\\ \\frac{1}{2} & 1-\\mu & \\frac{1}{2} \\\\ 0 & 1 & 1-\\mu \\end{pmatrix} = 0\n$$\n$$\n(1-\\mu) \\left| \\begin{matrix} 1-\\mu & \\frac{1}{2} \\\\ 1 & 1-\\mu \\end{matrix} \\right| - 1 \\left| \\begin{matrix} \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 1-\\mu \\end{matrix} \\right| = 0\n$$\n$$\n(1-\\mu)\\left( (1-\\mu)^2 - \\frac{1}{2} \\right) - \\left( \\frac{1}{2}(1-\\mu) \\right) = 0\n$$\n$$\n(1-\\mu)\\left( (1-\\mu)^2 - \\frac{1}{2} - \\frac{1}{2} \\right) = 0\n$$\n$$\n(1-\\mu)\\left( (1-\\mu)^2 - 1 \\right) = 0\n$$\nThis equation yields three solutions for $\\mu$:\n1. $1-\\mu = 0 \\implies \\mu_1 = 1$.\n2. $(1-\\mu)^2 - 1 = 0 \\implies (1-\\mu)^2 = 1 \\implies 1-\\mu = \\pm 1$.\n   - $1-\\mu = 1 \\implies \\mu_2 = 0$.\n   - $1-\\mu = -1 \\implies \\mu_3 = 2$.\n\nThe eigenvalues of $3E$ are $\\{0, 1, 2\\}$. The eigenvalues of $E$ are $\\lambda_i = \\mu_i/3$, so they are $\\{\\frac{0}{3}, \\frac{1}{3}, \\frac{2}{3}\\} = \\{0, \\frac{1}{3}, \\frac{2}{3}\\}$.\n\nThe spectral radius $\\rho(E)$ is the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(E) = \\max\\left(|0|, \\left|\\frac{1}{3}\\right|, \\left|\\frac{2}{3}\\right|\\right) = \\frac{2}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "Now that we have examined the individual components, it's time to assemble and diagnose a complete multigrid solver. This hands-on coding exercise guides you through implementing a full V-cycle using an intuitive aggregation-based approach to build the multigrid hierarchy . The central task is to create a diagnostic tool that measures the error reduction factor at each level, a crucial skill for debugging and optimizing multigrid performance in practical scientific and engineering applications.",
            "id": "2372556",
            "problem": "Design and implement a complete, runnable program that constructs a simple Algebraic Multigrid (AMG) hierarchy for a symmetric positive definite linear system and performs a single V-cycle while measuring, at each individual level, the error reduction factor. The goal is to create a diagnostic tool to debug a poorly performing multigrid solver by quantifying how much the error is reduced at each level.\n\nStart from the following fundamental base:\n- A linear system has the form $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite (SPD). The exact solution is $x^\\star = A^{-1} b$.\n- The error at an iterate $x$ is $e = x^\\star - x$. The residual is $r = b - A x$.\n- A relaxation method updates an iterate by $x^{k+1} = x^k + M r^k$ for some iteration matrix $M$. For weighted Jacobi relaxation, $M = \\omega D^{-1}$ with $D$ equal to the diagonal of $A$ and $0 < \\omega \\leq 1$.\n- A multigrid hierarchy consists of levels $\\ell = 0, 1, \\dots, L$, with matrices $A_\\ell$, restriction $R_\\ell$, and prolongation $P_\\ell$, related by the Galerkin relation $A_{\\ell+1} = R_\\ell A_\\ell P_\\ell$. The level $\\ell = 0$ is the finest, $\\ell = L$ is the coarsest.\n- A V-cycle on level $\\ell$ with $\\nu_1$ pre-relaxations and $\\nu_2$ post-relaxations proceeds as: pre-relax on $x_\\ell$, form residual $r_\\ell = b_\\ell - A_\\ell x_\\ell$, restrict $b_{\\ell+1} = R_\\ell r_\\ell$, approximately solve $A_{\\ell+1} y_{\\ell+1} = b_{\\ell+1}$ on the coarse level (recursively or exactly at the coarsest), prolong and correct $x_\\ell \\leftarrow x_\\ell + P_\\ell y_{\\ell+1}$, then post-relax.\n\nDefinitions and measurements required:\n- For each level $\\ell$, define the per-level reduction factor as the ratio of the Euclidean norm of the error immediately after completing all work at level $\\ell$ in the V-cycle to the Euclidean norm of the error immediately before starting work at level $\\ell$ in the same V-cycle:\n$$\\rho_\\ell \\equiv \\frac{\\lVert e_\\ell^{\\text{out}} \\rVert_2}{\\lVert e_\\ell^{\\text{in}} \\rVert_2}.$$\n- Here $e_\\ell^{\\text{in}} = x_\\ell^\\star - x_\\ell^{\\text{in}}$ and $e_\\ell^{\\text{out}} = x_\\ell^\\star - x_\\ell^{\\text{out}}$, where $x_\\ell^\\star$ is the exact solution of $A_\\ell x_\\ell^\\star = b_\\ell$ and $x_\\ell^{\\text{in}}$ and $x_\\ell^{\\text{out}}$ are, respectively, the iterate upon entry to the level and the iterate upon exit from the level in the V-cycle. At the coarsest level, take the solve to be exact so that $\\rho_L = 0$.\n\nImplementation details and constraints:\n- Construct $A$ on the finest level as the standard $1$-dimensional Poisson operator with Dirichlet boundary conditions on $n$ interior points, i.e., the tridiagonal matrix with $2$ on the diagonal and $-1$ on the sub- and super-diagonals.\n- Build an Algebraic Multigrid hierarchy using simple aggregation:\n  - Partition consecutive fine variables into aggregates of size $2$ (the last aggregate may have size $1$ if $n$ is odd).\n  - Define the prolongation $P_\\ell$ as piecewise-constant interpolation: each fine variable receives weight $1$ from its aggregateâ€™s coarse variable.\n  - Use $R_\\ell = P_\\ell^\\top$ and the Galerkin product $A_{\\ell+1} = R_\\ell A_\\ell P_\\ell$.\n  - Stop coarsening when $n_\\ell \\leq n_{\\min}$ for a fixed small threshold $n_{\\min}$.\n- Use weighted Jacobi relaxation with relaxation weight $\\omega$ for both pre- and post-relaxations. Each relaxation step updates $x \\leftarrow x + \\omega D^{-1} (b - A x)$, where $D$ is the diagonal of $A$ on that level.\n- Compute $x_\\ell^\\star$ on each level by a direct sparse solve so that the error can be measured exactly in $\\ell^2$ norm. This enforces that $\\rho_\\ell$ is well-defined and numerically reproducible.\n\nTest suite and final output:\n- Implement the diagnostic tool and evaluate the per-level reduction factors for the following three test cases. For each case, use the specified number of unknowns $n$, the number of pre-relaxations $\\nu_1$, the number of post-relaxations $\\nu_2$, the Jacobi relaxation weight $\\omega$, the coarse-size threshold $n_{\\min} = 5$, and the following right-hand sides $b$:\n  - Case A (general performance): $n = 63$, $\\nu_1 = 2$, $\\nu_2 = 2$, $\\omega = \\tfrac{2}{3}$, and $b_i = 1$ for all $i$.\n  - Case B (boundary-sized problem): $n = 3$, $\\nu_1 = 1$, $\\nu_2 = 1$, $\\omega = 0.8$, and $b_i = i + 1$ for $i = 1, \\dots, n$.\n  - Case C (poor smoothing choice): $n = 127$, $\\nu_1 = 1$, $\\nu_2 = 0$, $\\omega = 1$, and $b_i = 1$ for all $i$.\n- For each case, start the V-cycle with the zero vector as the initial guess on the finest level.\n- Your program must output a single line containing a comma-separated list of lists, one list per test case, where each inner list contains the per-level reduction factors $\\rho_\\ell$ for that case from finest level to coarsest level. For example, the output format must be exactly of the form\n  \"[[rA0,rA1,...],[rB0,rB1,...],[rC0,rC1,...]]\"\n  where each $r$ is a floating-point number. No units are involved in this problem.",
            "solution": "The problem is valid. It is scientifically grounded in the established theory of numerical linear algebra and algebraic multigrid methods. The problem statement is well-posed, providing all necessary definitions, constraints, and parameters to construct a unique, deterministic, and verifiable computational solution. The language is objective and precise, free from ambiguity or contradiction.\n\nThe solution is implemented by developing a Python program structured into three primary components: a function for constructing the Algebraic Multigrid (AMG) hierarchy, a recursive function that executes a single V-cycle while measuring per-level error reduction, and a main driver to process the specified test cases.\n\n**1. Algebraic Multigrid Hierarchy Construction**\nThe hierarchy of matrices and grid-transfer operators is constructed starting from the finest level, denoted as level $\\ell=0$.\n\n-   **Finest-Grid Operator ($A_0$)**: For a problem with $n$ interior points, the matrix $A_0$ is an $n \\times n$ symmetric positive definite matrix corresponding to the $1$-dimensional finite difference discretization of the negative Laplacian operator with homogeneous Dirichlet boundary conditions. Its entries are given by:\n    $$\n    (A_0)_{ij} = \\begin{cases} 2 & \\text{if } i = j \\\\ -1 & \\text{if } |i-j| = 1 \\\\ 0 & \\text{otherwise} \\end{cases}\n    $$\n-   **Coarsening Process**: The hierarchy is generated by repeatedly coarsening the grid. The process for creating level $\\ell+1$ from level $\\ell$ is as follows, and it is stopped when the matrix size $n_\\ell$ at level $\\ell$ becomes less than or equal to a specified threshold, $n_{\\min}$.\n    -   **Aggregation and Prolongation ($P_\\ell$)**: A simple aggregation scheme is used, where consecutive pairs of fine-grid variables indexed by $(2j, 2j+1)$ are grouped to form the $j$-th coarse-grid variable. The prolongation operator $P_\\ell$, which maps coarse-grid vectors to fine-grid vectors, is defined as a piecewise-constant interpolation matrix. It is an $n_\\ell \\times n_{\\ell+1}$ matrix, where the number of coarse variables is $n_{\\ell+1} = \\lceil n_\\ell / 2 \\rceil$. An entry $(P_\\ell)_{ik}$ is $1$ if the fine-grid variable $i$ belongs to the aggregate represented by the coarse-grid variable $k$, and $0$ otherwise.\n    -   **Restriction ($R_\\ell$)**: The restriction operator $R_\\ell$, which transfers fine-grid vectors to the coarse grid, is defined as the transpose of the prolongation operator, i.e., $R_\\ell = P_\\ell^\\top$.\n    -   **Galerkin Operator ($A_{\\ell+1}$)**: The coarse-grid operator $A_{\\ell+1}$ is computed using the Galerkin product. This formulation ensures that properties like symmetry and positive definiteness are preserved.\n        $$ A_{\\ell+1} = R_\\ell A_\\ell P_\\ell $$\n    This sequence of operations is repeated to generate matrices $A_1, A_2, \\dots, A_L$ until the size of the matrix at the coarsest level, $L$, satisfies $n_L \\le n_{\\min}$.\n\n**2. V-Cycle Algorithm with Per-Level Diagnostics**\nA single V-cycle is implemented as a recursive function that traverses the hierarchy from a given level $\\ell$ down to the coarsest level $L$ and back up. The core of the diagnostic tool is the measurement of the error reduction factor $\\rho_\\ell$ at each level.\n\n-   **Recursive Function and State**: A function `v_cycle(level, x, b)` is defined, which accepts the current level index $\\ell$, the current iterate $x_\\ell$, and the corresponding right-hand side vector $b_\\ell$.\n\n-   **Measurement of $\\rho_\\ell$**: The reduction factor is defined as $\\rho_\\ell \\equiv \\lVert e_\\ell^{\\text{out}} \\rVert_2 / \\lVert e_\\ell^{\\text{in}} \\rVert_2$. To compute this, the state is measured upon entry to and exit from the function for level $\\ell$:\n    -   Upon entry, the initial iterate is $x_\\ell^{\\text{in}} = x_\\ell$. To compute the initial error $e_\\ell^{\\text{in}} = x_\\ell^\\star - x_\\ell^{\\text{in}}$, the exact solution $x_\\ell^\\star$ of the level-specific linear system $A_\\ell x_\\ell = b_\\ell$ is first computed using a direct solver: $x_\\ell^\\star = A_\\ell^{-1} b_\\ell$. The Euclidean norm of the initial error, $\\lVert e_\\ell^{\\text{in}} \\rVert_2$, is then calculated and stored.\n    -   The V-cycle logic (detailed below) is executed, yielding a final iterate $x_\\ell^{\\text{out}}$.\n    -   The final error $e_\\ell^{\\text{out}} = x_\\ell^\\star - x_\\ell^{\\text{out}}$ and its norm $\\lVert e_\\ell^{\\text{out}} \\rVert_2$ are computed.\n    -   Finally, the reduction factor $\\rho_\\ell$ is calculated.\n\n-   **Recursive V-Cycle Steps**:\n    1.  **Base Case (Coarsest Level)**: If the current level is the coarsest level ($ \\ell = L $), the system $A_L x_L = b_L$ is solved exactly. The output iterate is $x_L^{\\text{out}} = x_L^\\star = A_L^{-1} b_L$. As specified, the reduction factor for this level is taken to be $\\rho_L = 0$.\n    2.  **Pre-relaxation**: For a number of steps $\\nu_1$, the current iterate $x_\\ell$ is smoothed using the weighted Jacobi method with relaxation weight $\\omega$:\n        $$ x_\\ell \\leftarrow x_\\ell + \\omega D_\\ell^{-1} (b_\\ell - A_\\ell x_\\ell) $$\n        Here, $D_\\ell$ is the diagonal of the matrix $A_\\ell$.\n    3.  **Coarse-Grid Correction**:\n        -   The residual of the smoothed iterate is computed: $r_\\ell = b_\\ell - A_\\ell x_\\ell$.\n        -   The residual is restricted to the next coarser level, forming the right-hand side for the coarse-grid problem: $b_{\\ell+1} = R_\\ell r_\\ell$.\n        -   The `v_cycle` function is called recursively to solve the coarse-grid correction equation $A_{\\ell+1} y_{\\ell+1} = b_{\\ell+1}$, starting with a zero initial guess for the correction, $y_{\\ell+1} = 0$. This call returns the computed correction, $y_{\\ell+1}^{\\text{solved}}$.\n    4.  **Prolongation and Correction**: The coarse-grid correction is interpolated back to the fine grid and added to the current iterate: $x_\\ell \\leftarrow x_\\ell + P_\\ell y_{\\ell+1}^{\\text{solved}}$.\n    5.  **Post-relaxation**: $\\nu_2$ steps of weighted Jacobi smoothing are applied to the corrected iterate to smooth out high-frequency errors introduced by the prolongation step.\n\nThe recursive function returns both the final iterate $x_\\ell^{\\text{out}}$ and a list containing the computed $\\rho_\\ell$ prepended to the list of reduction factors received from the coarser levels.\n\n**3. Execution of Test Cases**\nThe main part of the program sets up and runs the three specified test cases. For each case, it defines the parameters ($n, \\nu_1, \\nu_2, \\omega$), constructs the initial matrix $A_0$ and right-hand side vector $b_0$, builds the complete AMG hierarchy based on the coarse-size threshold $n_{\\min}=5$, and initiates the diagnostic V-cycle starting with a zero vector as the initial guess on the finest level. The resulting lists of per-level reduction factors are collected and printed to standard output in the specified string format.",
            "answer": "```python\nimport numpy as np\n\ndef construct_poisson_1d(n):\n    \"\"\"\n    Constructs the n x n matrix for the 1D Poisson problem.\n    \"\"\"\n    if n == 1:\n        return np.array([[2.0]])\n    A = 2.0 * np.eye(n) - 1.0 * np.eye(n, k=1) - 1.0 * np.eye(n, k=-1)\n    return A\n\ndef build_hierarchy(A0, n_min):\n    \"\"\"\n    Builds the AMG hierarchy using simple aggregation.\n    Returns a list of dictionaries, one for each level.\n    \"\"\"\n    hierarchy = []\n    A = A0\n    while A.shape[0] > n_min:\n        n_fine = A.shape[0]\n        n_coarse = (n_fine + 1) // 2\n\n        P = np.zeros((n_fine, n_coarse))\n        for j in range(n_coarse):\n            fine_idx1 = 2 * j\n            fine_idx2 = 2 * j + 1\n            P[fine_idx1, j] = 1.0\n            if fine_idx2 < n_fine:\n                P[fine_idx2, j] = 1.0\n        \n        R = P.T\n        A_coarse = R @ A @ P\n        \n        hierarchy.append({'A': A, 'P': P, 'R': R})\n        A = A_coarse\n    \n    hierarchy.append({'A': A})\n    return hierarchy\n\ndef jacobi_relax(A, x, b, omega, nu):\n    \"\"\"\n    Performs nu steps of weighted Jacobi relaxation.\n    \"\"\"\n    # Using np.diag(A) is safe as A is SPD and small, so diagonal is positive.\n    D_inv = 1.0 / np.diag(A)\n    x_new = x.copy()\n    for _ in range(nu):\n        r = b - A @ x_new\n        x_new += omega * D_inv * r\n    return x_new\n\ndef v_cycle_recursive(level, x, b, hierarchy, nu1, nu2, omega):\n    \"\"\"\n    Performs one recursive V-cycle and measures per-level reduction factors.\n    \"\"\"\n    A = hierarchy[level]['A']\n    \n    try:\n        x_star = np.linalg.solve(A, b)\n    except np.linalg.LinAlgError:\n        # For singular matrices that might appear due to floating point issues,\n        # use pseudo-inverse. A should be SPD, but as a safeguard.\n        x_star = np.linalg.pinv(A) @ b\n\n    norm_e_in = np.linalg.norm(x_star - x)\n\n    # Base case: coarsest level\n    if level == len(hierarchy) - 1:\n        x_out = x_star  # Exact solve on the coarsest level\n        rho_level = 0.0 # Per problem specification\n        return x_out, [rho_level]\n\n    # --- Downward stroke of V ---\n    # 1. Pre-relaxation\n    x_pre_relaxed = jacobi_relax(A, x, b, omega, nu1)\n    \n    # 2. Coarse-grid correction\n    r = b - A @ x_pre_relaxed\n    R = hierarchy[level]['R']\n    b_coarse = R @ r\n    \n    y_coarse = np.zeros(hierarchy[level+1]['A'].shape[0])\n    \n    y_coarse_solved, rhos_coarser = v_cycle_recursive(\n        level + 1, y_coarse, b_coarse, hierarchy, nu1, nu2, omega\n    )\n    \n    # --- Upward stroke of V ---\n    # 3. Prolongation and correction\n    P = hierarchy[level]['P']\n    x_corrected = x_pre_relaxed + P @ y_coarse_solved\n    \n    # 4. Post-relaxation\n    x_out = jacobi_relax(A, x_corrected, b, omega, nu2)\n    \n    # --- Measurement for current level ---\n    norm_e_out = np.linalg.norm(x_star - x_out)\n    \n    if norm_e_in > 1e-15:\n      rho_level = norm_e_out / norm_e_in\n    else:\n      rho_level = 0.0\n\n    all_rhos = [rho_level] + rhos_coarser\n    \n    return x_out, all_rhos\n\ndef solve_case(n, nu1, nu2, omega, b_func, n_min):\n    \"\"\"\n    Sets up and solves one test case, returning the list of reduction factors.\n    \"\"\"\n    A0 = construct_poisson_1d(n)\n    b0 = b_func(n)\n    \n    hierarchy = build_hierarchy(A0, n_min)\n    \n    x0 = np.zeros(n)\n    \n    _, rhos = v_cycle_recursive(0, x0, b0, hierarchy, nu1, nu2, omega)\n    \n    return rhos\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Case A: n=63, nu1=2, nu2=2, omega=2/3, b_i=1\n        (63, 2, 2, 2.0/3.0, lambda n: np.ones(n, dtype=float)),\n        # Case B: n=3, nu1=1, nu2=1, omega=0.8, b_i = i+1 for i=1..n\n        (3, 1, 1, 0.8, lambda n: np.arange(2, n + 2, dtype=float)),\n        # Case C: n=127, nu1=1, nu2=0, omega=1, b_i=1\n        (127, 1, 0, 1.0, lambda n: np.ones(n, dtype=float))\n    ]\n    \n    n_min = 5\n    all_results = []\n\n    for case_params in test_cases:\n        n, nu1, nu2, omega, b_func = case_params\n        rhos = solve_case(n, nu1, nu2, omega, b_func, n_min)\n        all_results.append(rhos)\n\n    # Format the final output string exactly as required.\n    # The str() representation of a list of lists is already very close.\n    # Remove spaces to be perfectly compliant.\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}