{
    "hands_on_practices": [
        {
            "introduction": "Understanding a hybrid algorithm involves not just knowing its rules, but also recognizing its behavior in practice. This exercise is a form of numerical forensics, where you will deduce the strategy of a 'black-box' solver by analyzing the sequence of its root estimates . By comparing the pattern of convergence to the known characteristics of methods like bisection and Newton's method, you can uncover the point at which the algorithm switches from a safe, bracketing strategy to a fast, open one.",
            "id": "2402250",
            "problem": "Consider the nonlinear equation $f(x)=x^3-x-1=0$, which has a unique real root in the interval $[a_0,b_0]=[1,2]$. A black-box solver is run with the initial bracketing interval $[1,2]$, and it reports the following sequence of root estimates $\\{x_k\\}$:\n$x_0=1.5,\\;\\; x_1=1.25,\\;\\; x_2=1.375,\\;\\; x_3=1.3125,\\;\\; x_4=1.32486,\\;\\; x_5=1.32472.$\nNo other internal state is revealed by the solver. Based solely on the observed iterates and the definition of $f(x)$, select the most plausible hybrid strategy and the iteration index at which the method switched its update rule. For this question, the “switching iteration” is the index $k$ at which the first iterate consistent with a non-bracketing step appears.\n\nWhich option best explains the observed sequence?\n\nA. A hybrid that performs bisection for $k=0,1,2,3$, then switches at $k=4$ to Newton’s method applied to $f(x)$, accepting a Newton step only if it lies within the current bracketing interval.\n\nB. A hybrid that performs the classical regula falsi (false-position) method for $k=0,1,2,3$, then switches at $k=4$ to the secant method.\n\nC. A pure secant method from the start, with initial guesses $x_0=1$ and $x_1=2$.\n\nD. Pure bisection with no switching; all $x_k$ are midpoints of the current bracketing interval.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens\n- Nonlinear equation: $f(x) = x^3 - x - 1 = 0$.\n- Initial bracketing interval: $[a_0, b_0] = [1, 2]$.\n- Sequence of root estimates: $\\{x_k\\}$.\n  $x_0 = 1.5$\n  $x_1 = 1.25$\n  $x_2 = 1.375$\n  $x_3 = 1.3125$\n  $x_4 = 1.32486$\n  $x_5 = 1.32472$\n- Definition of \"switching iteration\": The index $k$ at which the first iterate consistent with a non-bracketing step appears.\n- Task: Identify the most plausible hybrid strategy and switching index from the given options.\n\nStep 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem uses standard numerical root-finding algorithms (bisection, Newton's method, regula falsi, secant method) applied to a simple polynomial function. These topics are fundamental to computational physics and numerical analysis. The premise is scientifically sound.\n- **Well-Posed:** The problem provides a sequence of numerical results and asks for the identification of the generating algorithm from a fixed set of choices. This is a well-defined deductive problem.\n- **Objective:** The problem is stated with precise numerical data and standard terminology. It is free of ambiguity and subjective claims.\n- The problem is self-contained, consistent, and scientifically verifiable through calculation. No flaws are identified.\n\nStep 3: Verdict and Action\nThe problem is valid. A solution will be derived.\n\nThe governing equation is $f(x) = x^3 - x - 1 = 0$. The derivative is $f'(x) = 3x^2 - 1$.\nThe initial interval is $[a_0, b_0] = [1, 2]$. We evaluate the function at the endpoints:\n$f(1) = 1^3 - 1 - 1 = -1$.\n$f(2) = 2^3 - 2 - 1 = 5$.\nSince $f(1)  0$ and $f(2)  0$, there is at least one root in the interval $[1, 2]$. As $f'(x) = 3x^2 - 1  0$ for all $x \\in [1, 2]$, the function is monotonically increasing on this interval, so the root is unique.\n\nWe will now analyze the sequence of iterates $\\{x_k\\}$ to determine the algorithm used.\n\nAnalysis of iterates for $k=0, 1, 2, 3$:\n1.  **k=0**: The first iterate is $x_0 = 1.5$. This is the midpoint of the initial interval $[1, 2]$, since $\\frac{1+2}{2} = 1.5$. This is consistent with the bisection method.\n    We find the new interval for a bracketing method. $f(x_0) = f(1.5) = (1.5)^3 - 1.5 - 1 = 3.375 - 2.5 = 0.875$. Since $f(1)  0$ and $f(1.5)  0$, the new interval is $[a_1, b_1] = [1, 1.5]$.\n\n2.  **k=1**: The second iterate is $x_1 = 1.25$. This is the midpoint of the new interval $[1, 1.5]$, since $\\frac{1+1.5}{2} = 1.25$. This is also consistent with the bisection method.\n    We find the next interval. $f(x_1) = f(1.25) = (1.25)^3 - 1.25 - 1 = 1.953125 - 2.25 = -0.296875$. Since $f(1.25)  0$ and $f(1.5)  0$, the new interval is $[a_2, b_2] = [1.25, 1.5]$.\n\n3.  **k=2**: The third iterate is $x_2 = 1.375$. This is the midpoint of the interval $[1.25, 1.5]$, since $\\frac{1.25+1.5}{2} = 1.375$. This is consistent with the bisection method.\n    We find the next interval. $f(x_2) = f(1.375) = (1.375)^3 - 1.375 - 1 \\approx 2.599609 - 2.375 = 0.224609$. Since $f(1.25)  0$ and $f(1.375)  0$, the new interval is $[a_3, b_3] = [1.25, 1.375]$.\n\n4.  **k=3**: The fourth iterate is $x_3 = 1.3125$. This is the midpoint of the interval $[1.25, 1.375]$, since $\\frac{1.25+1.375}{2} = 1.3125$. This is also consistent with the bisection method.\n    We find the next interval. $f(x_3) = f(1.3125) = (1.3125)^3 - 1.3125 - 1 \\approx 2.260986 - 2.3125 = -0.051514$. Since $f(1.3125)  0$ and $f(1.375)  0$, the new interval is $[a_4, b_4] = [1.3125, 1.375]$.\n\nThe iterates $x_0, x_1, x_2, x_3$ are perfectly explained by the bisection method.\n\nNow we evaluate each option.\n\n**D. Pure bisection with no switching; all $x_k$ are midpoints of the current bracketing interval.**\nTo obtain $x_4$ via bisection, we would take the midpoint of the last interval, $[1.3125, 1.375]$.\n$x_4^{\\text{bisect}} = \\frac{1.3125 + 1.375}{2} = 1.34375$.\nThe given iterate is $x_4 = 1.32486$. Since $1.34375 \\neq 1.32486$, the method is not pure bisection.\nVerdict: **Incorrect**.\n\n**C. A pure secant method from the start, with initial guesses $x_0=1$ and $x_1=2$.**\nThe secant method formula for the next iterate $x_{n+1}$ based on $x_n$ and $x_{n-1}$ is $x_{n+1} = x_n - f(x_n) \\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})}$.\nWith initial guesses $p_0=1$ and $p_1=2$, the first computed iterate is:\n$p_2 = p_1 - f(p_1) \\frac{p_1 - p_0}{f(p_1) - f(p_0)} = 2 - f(2) \\frac{2-1}{f(2)-f(1)} = 2 - 5 \\frac{1}{5 - (-1)} = 2 - \\frac{5}{6} = \\frac{7}{6} \\approx 1.16667$.\nThe first iterate in the given sequence is $x_0=1.5$. These do not match.\nVerdict: **Incorrect**.\n\n**B. A hybrid that performs the classical regula falsi (false-position) method for $k=0,1,2,3$, then switches at $k=4$ to the secant method.**\nThe regula falsi method computes the next iterate as the root of the secant line connecting the endpoints of the current bracketing interval $[a_k, b_k]$: $x_{k+1} = \\frac{a_k f(b_k) - b_k f(a_k)}{f(b_k) - f(a_k)}$.\nFor the first step, $[a_0, b_0] = [1, 2]$. $f(1)=-1, f(2)=5$.\n$x_0^{\\text{RF}} = \\frac{1 \\cdot 5 - 2 \\cdot (-1)}{5 - (-1)} = \\frac{5+2}{6} = \\frac{7}{6} \\approx 1.16667$.\nThe first iterate in the given sequence is $x_0=1.5$. These do not match.\nVerdict: **Incorrect**.\n\n**A. A hybrid that performs bisection for $k=0,1,2,3$, then switches at $k=4$ to Newton’s method applied to $f(x)$, accepting a Newton step only if it lies within the current bracketing interval.**\nAs established, the iterates for $k=0,1,2,3$ are perfectly consistent with the bisection method. This means the first part of the statement is correct.\nThe option proposes a switch at $k=4$. This means the iterate $x_4$ is computed using a new rule, Newton's method. The Newton's method update rule is $x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$.\nA common strategy in hybrid methods is to launch the Newton step from the best current estimate, which is $x_3 = 1.3125$.\nLet's compute $x_4$ using one step of Newton's method starting from $x_3$:\n$x_4^{\\text{Newton}} = x_3 - \\frac{f(x_3)}{f'(x_3)} = 1.3125 - \\frac{(1.3125)^3 - 1.3125 - 1}{3(1.3125)^2 - 1}$.\n$f(1.3125) = -0.051513671875$.\n$f'(1.3125) = 3(1.72265625) - 1 = 5.16796875 - 1 = 4.16796875$.\n$x_4^{\\text{Newton}} = 1.3125 - \\frac{-0.051513671875}{4.16796875} \\approx 1.3125 + 0.012359434 \\approx 1.324859434$.\nRounding this result to $5$ decimal places gives $1.32486$, which is an exact match for the given $x_4$.\nAccording to the problem definition, the switch happens at $k=4$, which is the index of the first iterate ($x_4$) that does not follow the initial (bisection) rule. This is consistent.\nThe option also states a condition: the Newton step is accepted only if it lies within the current bracketing interval. The bracketing interval before computing $x_4$ was $[a_4, b_4] = [1.3125, 1.375]$. The computed iterate $x_4 \\approx 1.32486$ is indeed within this interval, so this condition is met.\nNow let's check the next iterate, $x_5$. If the method continues with Newton's method, the next step would be:\n$x_5^{\\text{Newton}} = x_4 - \\frac{f(x_4)}{f'(x_4)}$, starting from $x_4 = 1.32486$.\n$f(1.32486) \\approx (1.32486)^3 - 1.32486 - 1 \\approx 2.325376 - 1.32486 - 1 = 0.000516$.\n$f'(1.32486) \\approx 3(1.32486)^2 - 1 \\approx 3(1.75525) - 1 = 4.26575$.\n$x_5^{\\text{Newton}} \\approx 1.32486 - \\frac{0.000516}{4.26575} \\approx 1.32486 - 0.000121 \\approx 1.324739$.\nThe given value is $x_5 = 1.32472$. Our calculated value is $1.32474$ (when rounded). This is a very small discrepancy, most likely attributable to rounding in the problem's provided sequence. The given $x_5$ is in fact a better approximation to the true root ($r \\approx 1.324717957...$) than the one calculated from the rounded $x_4$.\nGiven the perfect match for the first $5$ iterates ($x_0$ to $x_4$) and the plausible mechanism of switching from the safe bisection method to the fast-converging Newton's method once the root is localized, this option is by far the most plausible explanation. The other options fail at the first step.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The ultimate goal of algorithm design is not just correctness, but efficiency. This practice challenges you to build and compare two hybrid root-finders: one based on the familiar Newton's method and another on the higher-order Halley's method, which you will first derive from a Taylor series expansion . By implementing these methods and evaluating their performance with a weighted cost model that accounts for the price of evaluating $f(x)$, $f'(x)$, and $f''(x)$, you will directly investigate the fundamental trade-off between convergence rate and computational cost per iteration.",
            "id": "2402194",
            "problem": "You are tasked with designing and evaluating a globally convergent hybrid root-finding algorithm in which the open component is a third-order method. The core setting is to find a simple root $x^\\star$ of a smooth nonlinear scalar function $f(x)$ given a guaranteed bracketing interval $[a,b]$ with $f(a)f(b)  0$. The foundation for your design must be based on the following widely accepted principles: (i) the Intermediate Value Theorem, which guarantees the existence of at least one root in $[a,b]$ if $f(a)f(b)  0$, and (ii) Taylor expansions of smooth functions, which justify local high-order open-method steps near the root. The angle unit for trigonometric functions must be radians.\n\nYour tasks are:\n\n- Derive, from Taylor expansions and without using any pre-provided iteration formula, an open-step update that attains third-order local convergence for a simple root under standard smoothness and nondegeneracy conditions. This open step must only use $f(x)$, $f'(x)$, and $f''(x)$ evaluated at the current iterate.\n- Implement a robust hybrid method that:\n  - Maintains a bracketing interval $[a_k,b_k]$ at each iteration $k$ with $f(a_k)f(b_k) \\le 0$.\n  - Proposes an open step from the current point $x_k$ using your third-order update.\n  - Accepts the open step only if it remains inside $(a_k,b_k)$ and strictly decreases the residual magnitude. Otherwise, it must fall back to a bisection step.\n  - Updates the bracketing interval using the sign of the function at the accepted new point.\n  - Terminates when either the absolute residual is below a prescribed tolerance or the bracket width is sufficiently small.\n- For comparison, also implement a baseline hybrid method where the open step is the classical Newton update (second-order local convergence), with the same safeguards and fallbacks as above.\n- Model computational cost as a weighted sum of evaluations, where the cost of one evaluation of $f(x)$ is $c_0$, of $f'(x)$ is $c_1$, and of $f''(x)$ is $c_2$. Count every evaluation performed by your algorithm and compute the total weighted cost for each method.\n\nUse absolute residual tolerance $\\lvert f(x) \\rvert \\le \\varepsilon$ with $\\varepsilon = 10^{-12}$ and a bracket-width tolerance $\\lvert b-a \\rvert \\le \\varepsilon$, and enforce a maximum of $100$ iterations. Angles must be in radians. There are no physical units involved in the final answers.\n\nTest Suite Specification. Implement $f(x)$, $f'(x)$, and $f''(x)$ analytically for each of the following test cases, each of which includes the bracketing interval $[a,b]$ and evaluation costs $(c_0,c_1,c_2)$:\n\n- Case $1$: $f(x) = \\cos(x) - x$, $[a,b] = [0,1]$, $(c_0,c_1,c_2) = (1.0,1.0,1.0)$.\n- Case $2$: $f(x) = \\cos(x) - x$, $[a,b] = [0,1]$, $(c_0,c_1,c_2) = (1.0,1.0,10.0)$.\n- Case $3$: $f(x) = x^3 - 2x - 5$, $[a,b] = [2,3]$, $(c_0,c_1,c_2) = (1.0,1.0,1.0)$.\n- Case $4$: $f(x) = \\mathrm{e}^x - 3$, $[a,b] = [0,2]$, $(c_0,c_1,c_2) = (1.0,1.0,5.0)$.\n- Case $5$: $f(x) = \\tanh(x) - 0.5$, $[a,b] = [0,2]$, $(c_0,c_1,c_2) = (1.0,2.0,4.0)$.\n- Case $6$: $f(x) = x^5 - x - 1$, $[a,b] = [1,2]$, $(c_0,c_1,c_2) = (1.0,1.0,3.0)$.\n\nFor each case, run both hybrid solvers (the third-order one you derived and the Newton-based baseline) from the interval midpoint $x_0 = (a+b)/2$, apply the same acceptance criteria and fallbacks, and compute the total weighted cost of function and derivative evaluations.\n\nFinal Output Requirement. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be a boolean indicating whether, under the specified costs, the hybrid with the third-order open step has a strictly lower total weighted cost than the hybrid with the Newton open step for that case. For example, a valid output might be of the form \"[True,False,True,True,False,True]\".",
            "solution": "The problem presented is a well-defined exercise in numerical analysis, concerning the design and comparative evaluation of hybrid root-finding algorithms. The premises are scientifically sound, the objectives are clear and quantitative, and all necessary data and constraints are provided. The problem is therefore deemed valid and a full solution shall be presented.\n\nThe task is to construct a globally convergent root-finding algorithm by combining a robust, bracketing method (bisection) with a fast, open method of third-order convergence. This hybrid strategy aims to leverage the rapid local convergence of the open method while retaining the guaranteed convergence of the bracketing method.\n\nFirst, the derivation of the third-order open-step update is required, based on Taylor series expansions and using only the function $f(x)$ and its first two derivatives, $f'(x)$ and $f''(x)$. Let $x_k$ be the current approximation to a simple root $x^\\star$, where $f(x^\\star) = 0$. The Taylor series expansion of $f(x)$ around $x_k$ is:\n$$ f(x^\\star) = f(x_k) + (x^\\star - x_k)f'(x_k) + \\frac{(x^\\star-x_k)^2}{2!}f''(x_k) + O\\left((x^\\star-x_k)^3\\right) $$\nSetting $f(x^\\star) = 0$ and letting $h = x_{k+1} - x_k$ be the step to the next iterate, we use $x_{k+1}$ as an improved approximation for $x^\\star$. Thus, we have the approximation $x^\\star - x_k \\approx h$. Substituting this into the truncated series gives a quadratic equation for the step $h$:\n$$ 0 \\approx f(x_k) + h f'(x_k) + \\frac{h^2}{2} f''(x_k) $$\nTo avoid solving a quadratic equation, we can linearize this expression by using a lower-order approximation for one of the $h$ factors in the quadratic term. The first-order Taylor approximation, which forms the basis of Newton's method, provides the estimate $h \\approx -f(x_k)/f'(x_k)$. Substituting this into the equation above yields:\n$$ 0 \\approx f(x_k) + h f'(x_k) + \\frac{h}{2} \\left( -\\frac{f(x_k)}{f'(x_k)} \\right) f''(x_k) $$\nSolving this equation for $h$ gives the update step:\n$$ h \\left( f'(x_k) - \\frac{f(x_k) f''(x_k)}{2 f'(x_k)} \\right) \\approx -f(x_k) $$\n$$ h = -\\frac{f(x_k)}{f'(x_k) - \\frac{f(x_k) f''(x_k)}{2 f'(x_k)}} $$\nThe next iterate is then $x_{k+1} = x_k + h$. This formula is known as Halley's method, and it exhibits cubic (third-order) local convergence for a simple root under standard smoothness conditions.\n\nFor comparison, a baseline hybrid method using the classical second-order Newton's method is required. The Newton step is derived from the first-order Taylor expansion:\n$$ 0 \\approx f(x_k) + (x^\\star - x_k)f'(x_k) $$\nThis directly yields the step $h = x_{k+1} - x_k \\approx -f(x_k)/f'(x_k)$, and the update rule:\n$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\n\nThe hybrid algorithm integrating these open steps is designed as follows. It starts with a bracketing interval $[a_0, b_0]$ where $f(a_0)f(b_0)  0$ and an initial guess $x_0 = (a_0+b_0)/2$. At each iteration $k$, given the current bracket $[a_k, b_k]$ and iterate $x_k$:\n$1$. An open step is proposed from $x_k$ to a new point $x_{open}$, calculated using either Halley's method or Newton's method. This step itself requires evaluations of $f(x_k)$, $f'(x_k)$, and for Halley's method, $f''(x_k)$.\n$2$. The proposed step is validated against two safeguarding conditions:\n   a. Bracket condition: The point $x_{open}$ must lie strictly within the current bracketing interval, i.e., $x_{open} \\in (a_k, b_k)$.\n   b. Progress condition: The absolute value of the function at the new point must be strictly less than at the current point, i.e., $|f(x_{open})|  |f(x_k)|$. This requires an additional evaluation of $f$ at $x_{open}$.\n$3$. If both safeguards are passed, the open step is accepted, and the next iterate is set to $x_{k+1} = x_{open}$.\n$4$. If the open step is rejected (due to failing a safeguard, or due to a numerical issue like division by zero), the algorithm falls back to a bisection step. The next iterate is set to the midpoint of the bracket, $x_{k+1} = (a_k + b_k) / 2$.\n$5$. The bracketing interval is updated. Depending on the sign of $f(x_{k+1})$, the new interval $[a_{k+1}, b_{k+1}]$ becomes either $[a_k, x_{k+1}]$ or $[x_{k+1}, b_k]$ to preserve the property $f(a_{k+1})f(b_{k+1}) \\le 0$.\n$6$. The process stops when either the bracket width $|b_k - a_k|$ or the absolute function residual $|f(x_k)|$ is less than or equal to a tolerance $\\varepsilon = 10^{-12}$, or after a maximum of $100$ iterations.\n\nThe computational cost is evaluated using a weighted sum of the number of function and derivative evaluations. If an algorithm completes with $N_0$ evaluations of $f(x)$, $N_1$ evaluations of $f'(x)$, and $N_2$ evaluations of $f''(x)$, with per-evaluation costs $c_0, c_1, c_2$ respectively, the total cost $C$ is calculated as:\n$$ C = N_0 c_0 + N_1 c_1 + N_2 c_2 $$\nThis cost model enables a fair comparison between the second-order and third-order hybrid methods. The question is whether the potential reduction in the number of iterations for the third-order method is sufficient to offset the additional cost of evaluating the second derivative $f''(x)$ at each open-step attempt.",
            "answer": "```python\nimport numpy as np\n\nclass FuncWithCounter:\n    \"\"\"A wrapper class for a function and its derivatives to count evaluations.\"\"\"\n    def __init__(self, f_def, df_def, d2f_def, costs):\n        self._f = f_def\n        self._df = df_def\n        self._d2f = d2f_def\n        self.costs = costs\n        self.f_evals = 0\n        self.df_evals = 0\n        self.d2f_evals = 0\n\n    def f(self, x):\n        self.f_evals += 1\n        return self._f(x)\n\n    def df(self, x):\n        self.df_evals += 1\n        return self._df(x)\n\n    def d2f(self, x):\n        self.d2f_evals += 1\n        return self._d2f(x)\n\n    def reset(self):\n        \"\"\"Resets all evaluation counters to zero.\"\"\"\n        self.f_evals = 0\n        self.df_evals = 0\n        self.d2f_evals = 0\n\n    def total_cost(self):\n        \"\"\"Computes the total weighted cost of all evaluations.\"\"\"\n        c0, c1, c2 = self.costs\n        return self.f_evals * c0 + self.df_evals * c1 + self.d2f_evals * c2\n\ndef hybrid_solver(f_obj, a_start, b_start, tol, max_iter, open_step_type):\n    \"\"\"\n    Globally convergent hybrid root-finding algorithm.\n\n    Args:\n        f_obj (FuncWithCounter): The function object with evaluation counters.\n        a_start (float): The start of the bracketing interval.\n        b_start (float): The end of the bracketing interval.\n        tol (float): The tolerance for termination.\n        max_iter (int): The maximum number of iterations.\n        open_step_type (str): The type of open step ('newton' or 'halley').\n\n    Returns:\n        float: The total computational cost.\n    \"\"\"\n    a, b = float(a_start), float(b_start)\n    f_obj.reset()\n\n    f_a = f_obj.f(a)\n    f_b = f_obj.f(b)\n\n    if np.sign(f_a) == np.sign(f_b):\n        return float('inf')\n\n    x_cur = (a + b) / 2.0\n    f_cur = f_obj.f(x_cur)\n\n    for _ in range(max_iter):\n        if abs(f_cur) = tol or (b - a) = tol:\n            break\n\n        # Propose an open step from the current point\n        open_step_valid = False\n        try:\n            df_cur = f_obj.df(x_cur)\n            if abs(df_cur)  1e-15: # Avoid division by zero\n                raise ValueError(\"Derivative is too small.\")\n            \n            if open_step_type == 'newton':\n                x_open = x_cur - f_cur / df_cur\n                open_step_valid = True\n            elif open_step_type == 'halley':\n                d2f_cur = f_obj.d2f(x_cur)\n                denom = df_cur - (f_cur * d2f_cur) / (2.0 * df_cur)\n                if abs(denom)  1e-15: # Avoid division by zero\n                    raise ValueError(\"Halley denominator is too small.\")\n                x_open = x_cur - f_cur / denom\n                open_step_valid = True\n        except (ValueError, ZeroDivisionError):\n            open_step_valid = False\n\n        accepted = False\n        if open_step_valid and (a  x_open  b):\n            f_open = f_obj.f(x_open)\n            if abs(f_open)  abs(f_cur):\n                x_next = x_open\n                f_next = f_open\n                accepted = True\n\n        if not accepted:\n            x_next = (a + b) / 2.0\n            if x_next == x_cur: # Interval is at machine precision limit\n                f_next = f_cur\n            else:\n                f_next = f_obj.f(x_next)\n\n        # Update current point and function value\n        x_cur = x_next\n        f_cur = f_next\n        \n        # Update bracket\n        if np.sign(f_cur) == np.sign(f_a):\n            a = x_cur\n            f_a = f_cur\n        else:\n            b = x_cur\n            f_b = f_cur\n\n    return f_obj.total_cost()\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {'func': lambda x: np.cos(x) - x, 'df': lambda x: -np.sin(x) - 1.0, 'd2f': lambda x: -np.cos(x),\n         'interval': (0.0, 1.0), 'costs': (1.0, 1.0, 1.0)},\n        {'func': lambda x: np.cos(x) - x, 'df': lambda x: -np.sin(x) - 1.0, 'd2f': lambda x: -np.cos(x),\n         'interval': (0.0, 1.0), 'costs': (1.0, 1.0, 10.0)},\n        {'func': lambda x: x**3 - 2.0*x - 5.0, 'df': lambda x: 3.0*x**2 - 2.0, 'd2f': lambda x: 6.0*x,\n         'interval': (2.0, 3.0), 'costs': (1.0, 1.0, 1.0)},\n        {'func': lambda x: np.exp(x) - 3.0, 'df': lambda x: np.exp(x), 'd2f': lambda x: np.exp(x),\n         'interval': (0.0, 2.0), 'costs': (1.0, 1.0, 5.0)},\n        {'func': lambda x: np.tanh(x) - 0.5, 'df': lambda x: 1.0 / (np.cosh(x)**2), 'd2f': lambda x: -2.0 * np.tanh(x) / (np.cosh(x)**2),\n         'interval': (0.0, 2.0), 'costs': (1.0, 2.0, 4.0)},\n        {'func': lambda x: x**5 - x - 1.0, 'df': lambda x: 5.0*x**4 - 1.0, 'd2f': lambda x: 20.0*x**3,\n         'interval': (1.0, 2.0), 'costs': (1.0, 1.0, 3.0)}\n    ]\n\n    results = []\n    TOL = 1e-12\n    MAX_ITER = 100\n\n    for case in test_cases:\n        a, b = case['interval']\n        f_obj = FuncWithCounter(case['func'], case['df'], case['d2f'], case['costs'])\n\n        cost_newton = hybrid_solver(f_obj, a, b, TOL, MAX_ITER, 'newton')\n        cost_halley = hybrid_solver(f_obj, a, b, TOL, MAX_ITER, 'halley')\n        \n        results.append(cost_halley  cost_newton)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Most real-world physical systems are described not by a single equation, but by a system of coupled nonlinear equations. This practice guides you through the essential generalization of root-finding techniques to multiple dimensions, where the goal is to solve $\\mathbf{f}(\\mathbf{x}) = \\mathbf{0}$ . You will implement a powerful hybrid strategy that uses the Newton-Raphson method, built upon the Jacobian matrix $\\mathbf{J}(\\mathbf{x})$, for fast local convergence, while employing a gradient-based descent on a merit function to ensure the solver makes reliable progress from any starting point.",
            "id": "2402214",
            "problem": "Construct a deterministic program that, for each nonlinear system of equations specified below, computes an approximate root vector that satisfies a strict residual tolerance. Let the system be written as $ \\mathbf{f}(\\mathbf{x}) = \\mathbf{0} $ with $ \\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n $ and $ \\mathbf{x} \\in \\mathbb{R}^n $. A valid approximate root $ \\widehat{\\mathbf{x}} $ must satisfy $ \\lVert \\mathbf{f}(\\widehat{\\mathbf{x}}) \\rVert_2 \\leq 10^{-10} $ and must be reported as real numbers rounded to eight decimal places. No physical units enter these systems. Angles, if any appear inside trigonometric functions, are in radians. Your program must use the exact mathematical definitions of the functions provided, and it must return a single deterministic output without any user interaction.\n\nTest suite. For each test case, the system $ \\mathbf{f}(\\mathbf{x}) = \\mathbf{0} $ and the initial guess $ \\mathbf{x}_0 $ are provided. For each case, compute one approximate root $ \\widehat{\\mathbf{x}} $ starting from the specified $ \\mathbf{x}_0 $ that meets the residual tolerance. If your program cannot find such an approximate root within a reasonable iteration limit, it must still produce a result; however, the evaluation criterion for correctness will require the residual tolerance to be met.\n\n- Test Case $1$ (two variables):\n  System:\n  $$\n  \\begin{cases}\n  f_1(x,y) = e^{x} + y - 1.5 = 0, \\\\\n  f_2(x,y) = x^2 + y - 0.5 = 0,\n  \\end{cases}\n  $$\n  with initial guess $ \\mathbf{x}_0 = (-0.7, 1.0) $.\n\n- Test Case $2$ (two variables, multiple solutions exist):\n  System:\n  $$\n  \\begin{cases}\n  f_1(x,y) = \\sin(x) - y = 0, \\\\\n  f_2(x,y) = x^2 + y^2 - 0.5 = 0,\n  \\end{cases}\n  $$\n  with initial guess $ \\mathbf{x}_0 = (0.8, 0.6) $.\n\n- Test Case $3$ (three variables, two isolated solutions):\n  System:\n  $$\n  \\begin{cases}\n  f_1(x,y,z) = x + y + z - 1 = 0, \\\\\n  f_2(x,y,z) = x^2 + y^2 + z^2 - 1 = 0, \\\\\n  f_3(x,y,z) = x - y = 0,\n  \\end{cases}\n  $$\n  with initial guess $ \\mathbf{x}_0 = (0.2, 0.2, 0.8) $.\n\n- Test Case $4$ (two variables, small-scale coupling):\n  System:\n  $$\n  \\begin{cases}\n  f_1(x,y) = x\\,y - 10^{-6} = 0, \\\\\n  f_2(x,y) = x - y = 0,\n  \\end{cases}\n  $$\n  with initial guess $ \\mathbf{x}_0 = (1.0, 0.001) $.\n\nValidation. For each test case, your program must return a vector $ \\widehat{\\mathbf{x}} $ such that $ \\lVert \\mathbf{f}(\\widehat{\\mathbf{x}}) \\rVert_2 \\leq 10^{-10} $. The Euclidean norm $ \\lVert \\cdot \\rVert_2 $ is to be used. Each component must be rounded to eight decimal places in the final output.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a sub-list of its components, in the same order as the variables are presented above. For example, the format must be\n`[ [x_1,y_1], [x_2,y_2], [x_3,y_3,z_3], [x_4,y_4] ]`\nwith each $ x_i, y_i, z_i $ rounded to eight decimal places. No other text should be printed.",
            "solution": "We are asked to compute approximate roots of nonlinear systems $ \\mathbf{f}(\\mathbf{x}) = \\mathbf{0} $ in $ \\mathbb{R}^n $ to high accuracy, starting from provided initial guesses. A robust approach that leverages first principles is built on two core ideas: local quadratic convergence of the Newton step derived from the first-order Taylor expansion, and globalization via minimizing a merit function to ensure progress from remote initial guesses.\n\nPrinciples.\n\n1. Root of a system. A root $ \\mathbf{x}^\\star $ satisfies $ \\mathbf{f}(\\mathbf{x}^\\star) = \\mathbf{0} $, where $ \\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n $ is continuously differentiable. The Jacobian matrix is $ \\mathbf{J}(\\mathbf{x}) = \\left[ \\partial f_i / \\partial x_j \\right]_{i,j} $.\n\n2. Newton direction. The Newton step is obtained from the first-order Taylor expansion\n$$\n\\mathbf{f}(\\mathbf{x} + \\mathbf{s}) \\approx \\mathbf{f}(\\mathbf{x}) + \\mathbf{J}(\\mathbf{x}) \\mathbf{s} .\n$$\nSetting the right-hand side to $ \\mathbf{0} $ yields the linear system\n$$\n\\mathbf{J}(\\mathbf{x}) \\mathbf{s}_N = - \\mathbf{f}(\\mathbf{x}) ,\n$$\nwhose solution $ \\mathbf{s}_N $ is the Newton direction. If $ \\mathbf{J}(\\mathbf{x}^\\star) $ is nonsingular and $ \\mathbf{x} $ is sufficiently close to $ \\mathbf{x}^\\star $, the iteration $ \\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}_N $ converges quadratically.\n\n3. Merit function and gradient. To globalize convergence from remote initial guesses, a standard device is the least-squares merit function\n$$\n\\phi(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{f}(\\mathbf{x}) \\rVert_2^2 .\n$$\nBy the chain rule, the gradient of $ \\phi $ is\n$$\n\\nabla \\phi(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})^\\top \\mathbf{f}(\\mathbf{x}) .\n$$\nThe steepest descent direction for $ \\phi $ is $ -\\nabla \\phi(\\mathbf{x}) $.\n\n4. Hybrid globalization. Two complementary properties guide a robust solver:\n- Far from a root, reducing $ \\phi(\\mathbf{x}) $ reliably improves the quality of the current estimate. A few steps of steepest descent on $ \\phi $ can move $ \\mathbf{x} $ into a region where second-order information is more informative.\n- Near a root with a well-conditioned Jacobian, the Newton direction provides rapid (often quadratic) local convergence. A line search along the Newton direction using the Armijo condition ensures sufficient decrease in $ \\phi $ for global stability.\n\nAlgorithmic structure.\n\n- Initialization. Start from the given $ \\mathbf{x}_0 $. Define a maximum number of iterations $ N_{\\max} $, a residual tolerance $ \\varepsilon $ (here $ \\varepsilon = 10^{-10} $ for the residual norm target; we use a tighter internal tolerance for safety), and a gradient-norm-based stopping threshold for merit descent.\n\n- Preconditioning stage. Perform a small, fixed number of steepest descent steps on $ \\phi(\\mathbf{x}) $:\n  - Compute $ \\mathbf{g} = \\nabla \\phi(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})^\\top \\mathbf{f}(\\mathbf{x}) $.\n  - Use a backtracking line search to find $ \\alpha  0 $ such that\n    $$\n    \\phi(\\mathbf{x} - \\alpha \\mathbf{g}) \\le \\phi(\\mathbf{x}) - c_1 \\alpha \\langle \\mathbf{g}, \\mathbf{g} \\rangle ,\n    $$\n    where $ c_1 \\in (0,1) $ is small (e.g., $ c_1 = 10^{-4} $). Update $ \\mathbf{x} \\leftarrow \\mathbf{x} - \\alpha \\mathbf{g} $. This reduces $ \\phi $ and provides a better initial estimate.\n\n- Main loop (damped Newton with fallback). For $ k = 0, 1, \\dots $ until convergence:\n  - Evaluate $ \\mathbf{f}(\\mathbf{x}) $ and $ \\mathbf{J}(\\mathbf{x}) $.\n  - If $ \\lVert \\mathbf{f}(\\mathbf{x}) \\rVert_2 \\le \\varepsilon $, stop.\n  - Compute the Newton direction $ \\mathbf{s}_N $ by solving $ \\mathbf{J}(\\mathbf{x}) \\mathbf{s}_N = -\\mathbf{f}(\\mathbf{x}) $ (using a direct solve when well conditioned, or a least-squares solve if needed).\n  - Compute $ \\mathbf{g} = \\mathbf{J}(\\mathbf{x})^\\top \\mathbf{f}(\\mathbf{x}) $. If $ \\langle \\mathbf{g}, \\mathbf{s}_N \\rangle \\ge 0 $ (i.e., the Newton step is not a descent direction for $ \\phi $), replace it with the steepest descent direction $ \\mathbf{s} = -\\mathbf{g} $; otherwise set $ \\mathbf{s} = \\mathbf{s}_N $.\n  - Perform Armijo backtracking on $ \\phi $ along $ \\mathbf{s} $ to find $ \\alpha  0 $ such that\n    $$\n    \\phi(\\mathbf{x} + \\alpha \\mathbf{s}) \\le \\phi(\\mathbf{x}) + c_1 \\alpha \\langle \\mathbf{g}, \\mathbf{s} \\rangle .\n    $$\n    If backtracking fails for the Newton direction (e.g., step length underflows), retry with the steepest descent direction.\n  - Update $ \\mathbf{x} \\leftarrow \\mathbf{x} + \\alpha \\mathbf{s} $.\n\nThis hybrid strategy respects first principles: it uses the Jacobian linearization to derive the Newton step and the gradient of the squared residual to define a descent direction and a rigorous sufficient-decrease line search. This ensures global convergence from the provided initial guesses while recovering fast local convergence near roots.\n\nJacobian matrices for the test suite.\n\n- Test Case $1$:\n  $$\n  \\mathbf{f}(x,y) =\n  \\begin{bmatrix}\n  e^x + y - 1.5 \\\\\n  x^2 + y - 0.5\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y) =\n  \\begin{bmatrix}\n  e^x  1 \\\\\n  2x  1\n  \\end{bmatrix}.\n  $$\n\n- Test Case $2$:\n  $$\n  \\mathbf{f}(x,y) =\n  \\begin{bmatrix}\n  \\sin x - y \\\\\n  x^2 + y^2 - 0.5\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y) =\n  \\begin{bmatrix}\n  \\cos x  -1 \\\\\n  2x  2y\n  \\end{bmatrix}.\n  $$\n\n- Test Case $3$:\n  $$\n  \\mathbf{f}(x,y,z) =\n  \\begin{bmatrix}\n  x + y + z - 1 \\\\\n  x^2 + y^2 + z^2 - 1 \\\\\n  x - y\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y,z) =\n  \\begin{bmatrix}\n  1  1  1 \\\\\n  2x  2y  2z \\\\\n  1  -1  0\n  \\end{bmatrix}.\n  $$\n\n- Test Case $4$:\n  $$\n  \\mathbf{f}(x,y) =\n  \\begin{bmatrix}\n  x y - 10^{-6} \\\\\n  x - y\n  \\end{bmatrix}, \\quad\n  \\mathbf{J}(x,y) =\n  \\begin{bmatrix}\n  y  x \\\\\n  1  -1\n  \\end{bmatrix}.\n  $$\n\nConvergence and expected results.\n\n- Test Case $1$ has an exact solution $ (x,y) = (0, 0.5) $, which satisfies both equations. The hybrid method converges rapidly to $ (0, 0.5) $ from the given initial guess.\n- Test Case $2$ has multiple solutions defined by the intersection of $ y = \\sin x $ and the circle $ x^2 + y^2 = 0.5 $. From the given initial guess with positive $ y $, a nearby solution has $ x \\approx 0.5109 $ and $ y \\approx \\sin(0.5109) \\approx 0.4890 $. The solver converges to that solution.\n- Test Case $3$ has two isolated solutions satisfying $ x = y $: either $ (x,y,z) = (0,0,1) $ or $ (x,y,z) = (2/3, 2/3, -1/3) $. From the given initial guess near $ (0,0,1) $, the solver converges to $ (0,0,1) $.\n- Test Case $4$ enforces $ x = y $ and $ x y = 10^{-6} $, yielding the exact solution $ (x,y) = (10^{-3}, 10^{-3}) $. The solver converges to this solution despite the small scale and mild ill-conditioning of the Jacobian.\n\nThe final program implements the described hybrid strategy: a brief steepest-descent preconditioning of the merit function $ \\phi(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{f}(\\mathbf{x}) \\rVert_2^2 $, followed by a damped Newton method with Armijo line search and a fallback to the steepest-descent direction if the Newton step fails to be a descent direction or fails the line search. It computes each test case solution to satisfy the required residual tolerance and prints a single line containing the rounded results in the required format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0, c1=1e-4, tau=0.5, min_alpha=1e-12, max_backtracks=50):\n    alpha = alpha0\n    for _ in range(max_backtracks):\n        x_new = x + alpha * d\n        phi_new = phi(x_new)\n        if phi_new = phi_x + c1 * alpha * gTd:\n            return alpha, phi_new\n        alpha *= tau\n        if alpha  min_alpha:\n            break\n    return 0.0, phi_x\n\ndef hybrid_solver(f, J, x0, max_iter=200, tol_res=1e-12, pre_gd_steps=5):\n    x = np.array(x0, dtype=float)\n    # Define merit function and its gradient via J^T f\n    def phi(xv):\n        fv = f(xv)\n        return 0.5 * float(np.dot(fv, fv))\n    def grad_phi(xv):\n        fv = f(xv)\n        Jv = J(xv)\n        return Jv.T @ fv\n\n    # Preconditioning: a few steepest-descent steps on phi\n    for _ in range(pre_gd_steps):\n        fv = f(x)\n        if np.linalg.norm(fv) = tol_res:\n            return x\n        g = grad_phi(x)\n        gTd = -float(np.dot(g, g))\n        if gTd = 0:\n            break\n        phi_x = phi(x)\n        d = -g\n        alpha, _ = armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0)\n        if alpha == 0.0:\n            break\n        x = x + alpha * d\n\n    # Main loop: damped Newton with fallback to steepest descent\n    for _ in range(max_iter):\n        fv = f(x)\n        resn = np.linalg.norm(fv)\n        if resn = tol_res:\n            return x\n        Jx = J(x)\n        # Compute Newton direction: solve J s = -f\n        try:\n            sN = np.linalg.solve(Jx, -fv)\n        except np.linalg.LinAlgError:\n            # Fallback to least squares\n            sN, *_ = np.linalg.lstsq(Jx, -fv, rcond=None)\n        # Gradient of merit phi\n        g = Jx.T @ fv\n        phi_x = 0.5 * float(np.dot(fv, fv))\n        # If Newton direction is not a descent direction for phi, use steepest descent\n        if float(np.dot(g, sN)) = 0:\n            d = -g\n        else:\n            d = sN\n        gTd = float(np.dot(g, d))\n        # Perform Armijo backtracking; if fails with Newton, try steepest descent\n        alpha, phi_new = armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0)\n        if alpha == 0.0:\n            # Fallback to gradient direction if not already using it\n            d = -g\n            gTd = float(np.dot(g, d))\n            alpha, phi_new = armijo_backtracking(phi, x, d, gTd, phi_x, alpha0=1.0)\n            if alpha == 0.0:\n                # If still failing, take a tiny step to avoid stalling\n                alpha = 1e-8\n                phi_new = phi(x + alpha * d)\n        x = x + alpha * d\n        # Optional: check for tiny step and small gradient as a secondary stop\n        if np.linalg.norm(alpha * d) = 1e-14 and resn = tol_res*10:\n            return x\n    return x  # Return best effort\n\n# Define test systems and Jacobians\n\ndef f1(v):\n    x, y = v\n    return np.array([np.exp(x) + y - 1.5,\n                     x*x + y - 0.5], dtype=float)\n\ndef J1(v):\n    x, y = v\n    return np.array([[np.exp(x), 1.0],\n                     [2.0*x,     1.0]], dtype=float)\n\ndef f2(v):\n    x, y = v\n    return np.array([np.sin(x) - y,\n                     x*x + y*y - 0.5], dtype=float)\n\ndef J2(v):\n    x, y = v\n    return np.array([[np.cos(x), -1.0],\n                     [2.0*x,     2.0*y]], dtype=float)\n\ndef f3(v):\n    x, y, z = v\n    return np.array([x + y + z - 1.0,\n                     x*x + y*y + z*z - 1.0,\n                     x - y], dtype=float)\n\ndef J3(v):\n    x, y, z = v\n    return np.array([[1.0, 1.0, 1.0],\n                     [2.0*x, 2.0*y, 2.0*z],\n                     [1.0, -1.0, 0.0]], dtype=float)\n\ndef f4(v):\n    x, y = v\n    return np.array([x*y - 1e-6,\n                     x - y], dtype=float)\n\ndef J4(v):\n    x, y = v\n    return np.array([[y, x],\n                     [1.0, -1.0]], dtype=float)\n\ndef format_vector(vec):\n    return \"[\" + \",\".join(f\"{float(val):.8f}\" for val in vec) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (f1, J1, np.array([-0.7, 1.0])),\n        (f2, J2, np.array([0.8, 0.6])),\n        (f3, J3, np.array([0.2, 0.2, 0.8])),\n        (f4, J4, np.array([1.0, 0.001])),\n    ]\n\n    results = []\n    for f, J, x0 in test_cases:\n        root = hybrid_solver(f, J, x0, max_iter=200, tol_res=1e-12, pre_gd_steps=5)\n        # Round for output only; internal accuracy meets the tolerance\n        results.append(format_vector(root))\n\n    # Final print statement in the exact required format.\n    print(\"[\" + \",\".join(results) + \"]\")\n\nsolve()\n```"
        }
    ]
}