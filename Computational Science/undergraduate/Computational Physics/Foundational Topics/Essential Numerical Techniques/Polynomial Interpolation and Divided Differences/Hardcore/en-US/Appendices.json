{
    "hands_on_practices": [
        {
            "introduction": "The first step toward mastering a numerical method is to build a robust, reusable tool. This practice  guides you through creating a callable function that represents the unique interpolating polynomial for a given set of data points. By using Newton's divided differences for coefficient calculation and a nested multiplication scheme for evaluation, you will construct a computationally efficient interpolant, a core skill that mirrors the design of professional scientific computing libraries.",
            "id": "2428309",
            "problem": "You are given a finite set of distinct sample points $\\{(x_i, y_i)\\}_{i=0}^{n-1}$ with $x_i \\in \\mathbb{R}$ and $y_i \\in \\mathbb{R}$. There exists a unique polynomial $p(x)$ of degree at most $n-1$ such that $p(x_i) = y_i$ for all $i$. Write a complete program that constructs a function from the input data which, when called on any real argument $x$, returns the value of the interpolating polynomial $p(x)$. The constructed function must be a callable that closes over the data-dependent information and is directly evaluable at arbitrary $x \\in \\mathbb{R}$. All $x_i$ are distinct real numbers.\n\nUse the following test suite. For each test case, you must first construct the interpolating polynomial from the given data points and then produce the specified scalar quantity. The program must aggregate the results from all test cases, in order, into a single line as described in the final output format. All floating-point outputs must be rounded to exactly $12$ decimal places.\n\nTest suite:\n(1) Single-point interpolation (degree $0$): data points $[(0.0, 2.75)]$. Evaluate the interpolant at the three inputs $[-1.0, 0.0, 3.0]$ and compute the maximum absolute deviation from the reference values $[2.75, 2.75, 2.75]$. Output this maximum as a single floating-point number.\n(2) Two-point interpolation (degree $1$): data points $[(2.0, 3.0), (-1.0, -3.0)]$. Evaluate the interpolant at the three inputs $[0.5, 0.0, 1.0]$ and compute the maximum absolute deviation from the reference values $[0.0, -1.0, 1.0]$. Output this maximum as a single floating-point number.\n(3) Three-point interpolation (degree $2$): data points $[(-1.0, 4.0), (0.5, 1.75), (3.0, 8.0)]$. Evaluate the interpolant at the three inputs $[-1.0, 1.0, 2.5]$ and compute the maximum absolute deviation from the reference values $[4.0, 2.0, 5.75]$. Output this maximum as a single floating-point number.\n(4) Four-point interpolation (degree $3$): data points $[(-2.0, -3.0), (-0.5, 1.875), (1.0, 0.0), (2.5, 11.625)]$. Evaluate the interpolant at the three inputs $[-1.5, 0.0, 2.0]$ and compute the maximum absolute deviation from the reference values $[0.625, 1.0, 5.0]$. Output this maximum as a single floating-point number.\n(5) Five-point interpolation of a non-polynomial dataset: data points $[(0.3, $e^{0.3}$), (-0.2, $e^{-0.2}$), (0.0, $e^{0.0}$), (0.1, $e^{0.1}$), (-0.1, $e^{-0.1}$)]$. Evaluate the interpolant at $x = 0.05$ and output the interpolated value as a single floating-point number.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $(1)$ through $(5)$, for example, $[r_1,r_2,r_3,r_4,r_5]$. Each $r_k$ must be rounded to exactly $12$ decimal places.\n\nNo external input is required. No physical units are involved. No angles appear. All percentages, if any appeared, would need to be expressed as decimals, but none are present in this problem.",
            "solution": "The problem statement is valid. It presents a well-posed and fundamental task in computational physics and numerical analysis: the construction and evaluation of an interpolating polynomial for a given set of data points. The problem specifies a finite set of $n$ distinct data points $\\{(x_i, y_i)\\}_{i=0}^{n-1}$ where $x_i \\in \\mathbb{R}$ and $y_i \\in \\mathbb{R}$. The existence and uniqueness of an interpolating polynomial $p(x)$ of degree at most $n-1$ such that $p(x_i) = y_i$ for all $i \\in \\{0, 1, \\dots, n-1\\}$ is a classical theorem of mathematics. The condition that all $x_i$ are distinct is crucial and is provided, ensuring the problem is not ill-posed. The task is to create a computational procedure to evaluate this polynomial $p(x)$ for any arbitrary input $x \\in \\mathbb{R}$.\n\nA naive approach would be to represent the polynomial in its canonical form, $p(x) = a_0 + a_1 x + a_2 x^2 + \\dots + a_{n-1} x^{n-1}$. The coefficients $\\{a_k\\}_{k=0}^{n-1}$ could be found by solving the system of linear equations given by $p(x_i) = y_i$ for all $i$:\n$$\n\\begin{pmatrix}\n1 & x_0 & x_0^2 & \\dots & x_0^{n-1} \\\\\n1 & x_1 & x_1^2 & \\dots & x_1^{n-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n-1} & x_{n-1}^2 & \\dots & x_{n-1}^{n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\na_0 \\\\\na_1 \\\\\n\\vdots \\\\\na_{n-1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\vdots \\\\\ny_{n-1}\n\\end{pmatrix}\n$$\nHowever, the matrix in this system, known as the Vandermonde matrix, is notoriously ill-conditioned, making this method numerically unstable and computationally inefficient for even moderately large $n$.\n\nA superior and standard method is to construct the polynomial in Newton's form. This form is both computationally efficient to construct and numerically stable to evaluate. The Newton form of the interpolating polynomial $p(x)$ is given by:\n$$\np(x) = c_0 + c_1(x-x_0) + c_2(x-x_0)(x-x_1) + \\dots + c_{n-1}(x-x_0)\\dots(x-x_{n-2})\n$$\nThis can be expressed compactly as:\n$$\np(x) = \\sum_{k=0}^{n-1} c_k \\prod_{j=0}^{k-1} (x - x_j)\n$$\nThe coefficients $c_k$ are the so-called divided differences, denoted by $c_k = f[x_0, x_1, \\dots, x_k]$. They are computed recursively. The base cases are the zeroth-order divided differences, which are simply the function values themselves:\n$$\nf[x_i] = y_i\n$$\nThe higher-order divided differences are defined by the recurrence relation:\n$$\nf[x_i, x_{i+1}, \\dots, x_{i+j}] = \\frac{f[x_{i+1}, \\dots, x_{i+j}] - f[x_i, \\dots, x_{i+j-1}]}{x_{i+j} - x_i}\n$$\nThe required coefficients $c_k$ for the Newton polynomial are the entries on the top diagonal of the divided difference table: $c_0 = f[x_0]$, $c_1 = f[x_0, x_1]$, $c_2 = f[x_0, x_1, x_2]$, and so on, up to $c_{n-1} = f[x_0, \\dots, x_{n-1}]$.\n\nComputationally, these coefficients can be determined efficiently. One can initialize an array with the values $\\{y_i\\}_{i=0}^{n-1}$ and iteratively compute the divided differences in-place. For each column $j$ from $1$ to $n-1$ in the conceptual divided difference table, one updates the values from the bottom up. After this process, which requires $O(n^2)$ operations, the array will contain the coefficients $\\{c_k\\}_{k=0}^{n-1}$.\n\nOnce the coefficients $\\{c_k\\}$ and the nodes $\\{x_k\\}$ are known, the polynomial $p(x)$ can be evaluated for a given value of $x$ with optimal efficiency using a nested multiplication scheme analogous to Horner's method:\n$$\np(x) = c_0 + (x-x_0) \\bigg( c_1 + (x-x_1) \\Big( c_2 + \\dots + (x-x_{n-2})(c_{n-1}) \\dots \\Big) \\bigg)\n$$\nThis evaluation requires only $O(n)$ arithmetic operations and is numerically stable. The algorithm proceeds as follows:\n$1$. Initialize a result variable with the highest-order coefficient: $v = c_{n-1}$.\n$2$. Iterate downwards from $k = n-2$ to $0$: update the result as $v \\leftarrow c_k + (x - x_k) \\cdot v$.\n$3$. The final value of $v$ is $p(x)$.\n\nThe implementation will therefore consist of two stages. First, a function will be created that takes the data points $\\{(x_i, y_i)\\}$ and computes the divided difference coefficients $\\{c_k\\}$. This function will then return a callable object (a closure) which encapsulates these coefficients and the $x$-nodes $\\{x_i\\}$. This callable object, when invoked with an argument $x$, will perform the efficient nested evaluation described above to return the value of the interpolating polynomial $p(x)$. This design directly addresses the problem's requirement to construct a function from the data. The test suite is then executed using this constructed function.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the polynomial interpolation problem for a given test suite.\n    \"\"\"\n\n    def create_newton_interpolant(points):\n        \"\"\"\n        Creates a callable function that evaluates the interpolating polynomial\n        for the given points using Newton's divided difference form.\n\n        Args:\n            points: A list of tuples, where each tuple is an (x, y) point.\n\n        Returns:\n            A callable function that takes a single argument x and returns the\n            interpolated value p(x).\n        \"\"\"\n        x_nodes = np.array([p[0] for p in points], dtype=np.float64)\n        coeffs = np.array([p[1] for p in points], dtype=np.float64)\n        n = len(points)\n\n        # Compute the divided difference coefficients\n        # The coefficients are calculated in-place in the 'coeffs' array.\n        for j in range(1, n):\n            for i in range(n - 1, j - 1, -1):\n                denominator = x_nodes[i] - x_nodes[i - j]\n                # Avoid division by zero, although problem statement guarantees distinct x_i\n                if denominator == 0.0:\n                    raise ValueError(\"x_nodes must be distinct.\")\n                coeffs[i] = (coeffs[i] - coeffs[i - 1]) / denominator\n        \n        # The coefficients are now coeffs[0], coeffs[1], ..., coeffs[n-1]\n        \n        def evaluate_polynomial(x_val):\n            \"\"\"\n            Evaluates the Newton polynomial at x_val using Horner's method.\n            This function is a closure over x_nodes and coeffs.\n            \"\"\"\n            # Ensure input is float for calculations\n            x_val = np.float64(x_val)\n            \n            # Nested multiplication scheme (Horner's method for Newton form)\n            result = coeffs[n - 1]\n            for i in range(n - 2, -1, -1):\n                result = coeffs[i] + (x_val - x_nodes[i]) * result\n            return result\n            \n        return evaluate_polynomial\n\n    test_cases = [\n        # (1) Single-point interpolation (degree 0)\n        {\n            \"data\": [(0.0, 2.75)],\n            \"eval_points\": [-1.0, 0.0, 3.0],\n            \"ref_values\": [2.75, 2.75, 2.75],\n            \"task\": \"max_abs_deviation\"\n        },\n        # (2) Two-point interpolation (degree 1)\n        {\n            \"data\": [(2.0, 3.0), (-1.0, -3.0)],\n            \"eval_points\": [0.5, 0.0, 1.0],\n            \"ref_values\": [0.0, -1.0, 1.0],\n            \"task\": \"max_abs_deviation\"\n        },\n        # (3) Three-point interpolation (degree 2)\n        {\n            \"data\": [(-1.0, 4.0), (0.5, 1.75), (3.0, 8.0)],\n            \"eval_points\": [-1.0, 1.0, 2.5],\n            \"ref_values\": [4.0, 2.0, 5.75],\n            \"task\": \"max_abs_deviation\"\n        },\n        # (4) Four-point interpolation (degree 3)\n        {\n            \"data\": [(-2.0, -3.0), (-0.5, 1.875), (1.0, 0.0), (2.5, 11.625)],\n            \"eval_points\": [-1.5, 0.0, 2.0],\n            \"ref_values\": [0.625, 1.0, 5.0],\n            \"task\": \"max_abs_deviation\"\n        },\n        # (5) Five-point interpolation of a non-polynomial dataset\n        {\n            \"data\": [(0.3, np.exp(0.3)), (-0.2, np.exp(-0.2)), (0.0, np.exp(0.0)),\n                     (0.1, np.exp(0.1)), (-0.1, np.exp(-0.1))],\n            \"eval_points\": [0.05],\n            \"ref_values\": None, # No reference for this case\n            \"task\": \"evaluate\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        interpolant = create_newton_interpolant(case[\"data\"])\n        \n        if case[\"task\"] == \"max_abs_deviation\":\n            eval_points = np.array(case[\"eval_points\"], dtype=np.float64)\n            ref_values = np.array(case[\"ref_values\"], dtype=np.float64)\n            \n            evaluated_values = np.array([interpolant(x) for x in eval_points])\n            \n            max_deviation = np.max(np.abs(evaluated_values - ref_values))\n            results.append(max_deviation)\n        \n        elif case[\"task\"] == \"evaluate\":\n            eval_point = case[\"eval_points\"][0]\n            interpolated_value = interpolant(eval_point)\n            results.append(interpolated_value)\n\n    # Format the results to exactly 12 decimal places.\n    formatted_results = [\"{:.12f}\".format(r) for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Interpolating polynomials are not just for filling in gaps; they serve as powerful local models for physical phenomena described by discrete data. In this exercise , you will model the temperature dependence of a fluid's Schmidt number from experimental values. This practice moves beyond simple interpolation to explore extrapolation for predicting trends and, critically, demonstrates how to estimate a physical rate of change by computing the derivative of the polynomial model.",
            "id": "2428283",
            "problem": "A fluid is characterized by its Schmidt number, defined as the ratio of momentum diffusivity to mass diffusivity, which is dimensionless. Suppose that experiments at constant pressure provide the following tabulated values of the Schmidt number as a function of temperature:\n$$(T,\\ \\mathrm{Sc}) \\in \\{(283.15\\,\\mathrm{K},\\ 1600.0),\\ (303.15\\,\\mathrm{K},\\ 700.0),\\ (333.15\\,\\mathrm{K},\\ 300.0)\\}.$$\nLet $S(T)$ denote the unknown true dependence of the Schmidt number on temperature $T$ in kelvin. Define $P(T)$ to be the unique polynomial of minimal degree that satisfies $P(T_i)=\\mathrm{Sc}_i$ for each provided data pair $(T_i,\\mathrm{Sc}_i)$. Temperatures must be treated in kelvin and the Schmidt number is dimensionless. The derivative $P'(T)$ is with respect to $T$ and has units of per kelvin.\n\nConstruct $P(T)$ from the tabulated data and evaluate the following test suite:\n- Case A (interpolation at an interior point): compute $P(318.15\\,\\mathrm{K})$.\n- Case B (exact data reproduction at a node): compute $P(303.15\\,\\mathrm{K})$.\n- Case C (extrapolation beyond the highest temperature): compute $P(338.15\\,\\mathrm{K})$.\n- Case D (rate of change at a midrange temperature): compute $P'(308.15\\,\\mathrm{K})$.\n\nAll numeric answers must be expressed as floating-point numbers rounded to six decimal places. Temperatures are in kelvin, the Schmidt number is dimensionless, and the derivative is in per kelvin.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A, Case B, Case C, Case D]. For example, the format must be exactly like $[r_A,r_B,r_C,r_D]$ where each $r_\\cdot$ is a floating-point number with six digits after the decimal point.",
            "solution": "The problem statement is subjected to validation before any solution is attempted.\n\n**Step 1: Extracted Givens**\n- Data points $(T_i, \\mathrm{Sc}_i)$: $\\{(283.15\\,\\mathrm{K}, 1600.0), (303.15\\,\\mathrm{K}, 700.0), (333.15\\,\\mathrm{K}, 300.0)\\}$.\n- Definition of polynomial: $P(T)$ is the unique polynomial of minimal degree that interpolates the data, such that $P(T_i)=\\mathrm{Sc}_i$.\n- Tasks:\n    - Case A: Compute $P(318.15\\,\\mathrm{K})$.\n    - Case B: Compute $P(303.15\\,\\mathrm{K})$.\n    - Case C: Compute $P(338.15\\,\\mathrm{K})$.\n    - Case D: Compute $P'(308.15\\,\\mathrm{K})$.\n- Output specification: Results must be floating-point numbers rounded to six decimal places, formatted as a comma-separated list in square brackets.\n\n**Step 2: Validation of Givens**\n- **Scientific Grounding**: The concept of the Schmidt number and its dependence on temperature is a standard topic in transport phenomena. Modeling such dependence with a polynomial is a common technique in computational science. The provided data values are physically plausible. The problem is scientifically grounded.\n- **Well-Posedness**: For $n=3$ distinct data points, the fundamental theorem of polynomial interpolation guarantees the existence of a unique interpolating polynomial of degree at most $n-1=2$. The temperature values are distinct, so the problem is well-posed.\n- **Objectivity**: The problem is stated using precise, objective mathematical and physical terminology. It is free of ambiguity and subjective claims.\n\n**Step 3: Verdict**\nThe problem is valid. It is a standard exercise in numerical methods, specifically polynomial interpolation. I will proceed with the solution.\n\nThe task is to find a polynomial $P(T)$ of minimal degree that passes through the three given data points $(T_i, y_i)$, where $y_i$ represents the Schmidt number $\\mathrm{Sc}_i$. The points are:\n- $(T_0, y_0) = (283.15, 1600.0)$\n- $(T_1, y_1) = (303.15, 700.0)$\n- $(T_2, y_2) = (333.15, 300.0)$\n\nA robust and systematic method for constructing such a polynomial is Newton's method of divided differences. The polynomial $P(T)$ is represented in Newton's form as:\n$$P(T) = c_0 + c_1(T - T_0) + c_2(T - T_0)(T - T_1)$$\nwhere the coefficients $c_i$ are the divided differences.\n\nFirst, we compute the divided differences.\nThe zeroth-order divided difference is simply the value of the function at a point:\n$$f[T_0] = y_0 = 1600.0$$\n\nThe first-order divided differences are:\n$$f[T_0, T_1] = \\frac{y_1 - y_0}{T_1 - T_0} = \\frac{700.0 - 1600.0}{303.15 - 283.15} = \\frac{-900.0}{20.0} = -45.0$$\n$$f[T_1, T_2] = \\frac{y_2 - y_1}{T_2 - T_1} = \\frac{300.0 - 700.0}{333.15 - 303.15} = \\frac{-400.0}{30.0} = -\\frac{40.0}{3.0}$$\n\nThe second-order divided difference is:\n$$f[T_0, T_1, T_2] = \\frac{f[T_1, T_2] - f[T_0, T_1]}{T_2 - T_0} = \\frac{-\\frac{40.0}{3.0} - (-45.0)}{333.15 - 283.15} = \\frac{45.0 - \\frac{40.0}{3.0}}{50.0} = \\frac{\\frac{135.0 - 40.0}{3.0}}{50.0} = \\frac{95.0}{3.0 \\times 50.0} = \\frac{95.0}{150.0} = \\frac{19.0}{30.0}$$\n\nThe coefficients for Newton's polynomial are the leading diagonal of the divided difference table:\n$c_0 = f[T_0] = 1600.0$\n$c_1 = f[T_0, T_1] = -45.0$\n$c_2 = f[T_0, T_1, T_2] = \\frac{19.0}{30.0}$\n\nThe interpolating polynomial is:\n$$P(T) = 1600.0 - 45.0(T - 283.15) + \\frac{19.0}{30.0}(T - 283.15)(T - 303.15)$$\n\nThe derivative of the polynomial, $P'(T) = \\frac{dP}{dT}$, is found by differentiating the expression term by term:\n$$P'(T) = 0 - 45.0 \\cdot 1 + \\frac{19.0}{30.0} \\left[ 1 \\cdot (T - 303.15) + (T - 283.15) \\cdot 1 \\right]$$\n$$P'(T) = -45.0 + \\frac{19.0}{30.0} (2T - 586.30)$$\n\nNow, we evaluate the required test cases.\n\n**Case A: Compute $P(318.15\\,\\mathrm{K})$**\nWe substitute $T = 318.15$ into the expression for $P(T)$:\n$$P(318.15) = 1600.0 - 45.0(318.15 - 283.15) + \\frac{19.0}{30.0}(318.15 - 283.15)(318.15 - 303.15)$$\n$$P(318.15) = 1600.0 - 45.0(35.0) + \\frac{19.0}{30.0}(35.0)(15.0)$$\n$$P(318.15) = 1600.0 - 1575.0 + \\frac{19.0}{30.0}(525.0) = 25.0 + 19.0 \\times 17.5 = 25.0 + 332.5 = 357.5$$\n\n**Case B: Compute $P(303.15\\,\\mathrm{K})$**\nThis is an interpolation at a node, $T_1$. The result must be the given value $y_1$.\n$$P(303.15) = 1600.0 - 45.0(303.15 - 283.15) + \\frac{19.0}{30.0}(303.15 - 283.15)(303.15 - 303.15)$$\n$$P(303.15) = 1600.0 - 45.0(20.0) + \\frac{19.0}{30.0}(20.0)(0.0) = 1600.0 - 900.0 + 0 = 700.0$$\nThis confirms the correctness of the polynomial construction.\n\n**Case C: Compute $P(338.15\\,\\mathrm{K})$**\nWe substitute $T = 338.15$ for an extrapolation:\n$$P(338.15) = 1600.0 - 45.0(338.15 - 283.15) + \\frac{19.0}{30.0}(338.15 - 283.15)(338.15 - 303.15)$$\n$$P(338.15) = 1600.0 - 45.0(55.0) + \\frac{19.0}{30.0}(55.0)(35.0)$$\n$$P(338.15) = 1600.0 - 2475.0 + \\frac{19.0}{30.0}(1925.0) = -875.0 + \\frac{36575.0}{30.0} = -875.0 + 1219.166666...$$\n$$P(338.15) \\approx 344.166667$$\n\n**Case D: Compute $P'(308.15\\,\\mathrm{K})$**\nWe substitute $T = 308.15$ into the expression for $P'(T)$:\n$$P'(308.15) = -45.0 + \\frac{19.0}{30.0} (2 \\times 308.15 - 586.30)$$\n$$P'(308.15) = -45.0 + \\frac{19.0}{30.0} (616.30 - 586.30) = -45.0 + \\frac{19.0}{30.0} (30.0)$$\n$$P'(308.15) = -45.0 + 19.0 = -26.0$$\n\nThe final results, rounded to six decimal places, are:\n- Case A: $357.500000$\n- Case B: $700.000000$\n- Case C: $344.166667$\n- Case D: $-26.000000$\nThe implementation will use a standard numerical library to perform the interpolation, as this is efficient and minimizes error. The theory presented here justifies the method.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import KroghInterpolator\n\ndef solve():\n    \"\"\"\n    Constructs an interpolating polynomial for Schmidt number vs. temperature data\n    and evaluates it and its derivative at specified points.\n    \"\"\"\n    \n    # Define the data points from the problem statement.\n    # T_data is temperature in Kelvin, Sc_data is the dimensionless Schmidt number.\n    T_data = np.array([283.15, 303.15, 333.15])\n    Sc_data = np.array([1600.0, 700.0, 300.0])\n\n    # Construct the unique polynomial P(T) of minimal degree.\n    # For n=3 points, this is a quadratic polynomial.\n    # KroghInterpolator finds this unique polynomial.\n    P = KroghInterpolator(T_data, Sc_data)\n\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'A': 318.15,  # Interpolation at an interior point\n        'B': 303.15,  # Exact data reproduction at a node\n        'C': 338.15,  # Extrapolation beyond the highest temperature\n        'D': 308.15,  # Rate of change at a midrange temperature\n    }\n\n    # Evaluate the polynomial and its derivative for each case.\n    result_A = P(test_cases['A'])\n    result_B = P(test_cases['B'])\n    result_C = P(test_cases['C'])\n    \n    # The derivative method evaluates the first derivative of the polynomial.\n    result_D = P.derivative(test_cases['D'], der=1)\n\n    # Store results in a list, formatted to six decimal places.\n    results = [\n        f\"{result_A:.6f}\",\n        f\"{result_B:.6f}\",\n        f\"{result_C:.6f}\",\n        f\"{result_D:.6f}\"\n    ]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice reveals a more sophisticated application where the interpolating polynomial is not the end-product, but a powerful intermediate step in a larger analysis. By fitting a local polynomial model to a set of points , we can accurately estimate the curve's derivatives. This exercise shows how these numerical derivatives can be used in a geometric context to determine the parameters of an osculating circle, and importantly, it provides a first look into how measurement noise can impact the stability of such calculations.",
            "id": "2428257",
            "problem": "You are given three distinct sample points $\\left(x_0,y_0\\right)$, $\\left(x_1,y_1\\right)$, and $\\left(x_2,y_2\\right)$ that lie on a smooth planar curve which, in the vicinity of $x_1$, can be represented as a single-valued function $y=y(x)$. Assume the underlying curve is a circle of unknown center $\\left(a,b\\right)$ and unknown radius $R$. Your task is to use polynomial interpolation, constructed by Newton's divided differences, to estimate the circle's center and radius from just these three points, and then to examine how sensitive these estimates are to observation noise in the $y$-coordinates.\n\nFundamental base and goals:\n- Start from the definition of Newton's divided differences and the construction of the second-degree interpolating polynomial through $\\left(x_0,y_0\\right)$, $\\left(x_1,y_1\\right)$, and $\\left(x_2,y_2\\right)$.\n- Using only these foundations and core geometric facts, derive from first principles how to obtain the first and second derivatives of the quadratic interpolant at $x_1$ without resorting to any result not derivable from the definitions.\n- From the geometric relationship between curvature of a plane curve and the osculating circle, derive how the curvature at $x_1$ determines the radius of the osculating circle and the location of its center relative to $\\left(x_1,y_1\\right)$ and the local tangent and normal directions.\n- Implement this method in code and evaluate it on a small test suite that includes noise-free and noisy data, symmetric and asymmetric node placement, and a nearly straight arc (large radius) case. All quantities are treated as dimensionless; no physical units are involved.\n\nDetailed requirements:\n1. Construct the unique quadratic interpolant $p_2(x)$ that passes through the three given points by using Newton's divided differences. Express $p_2(x)$ in the Newton form based on the nodes $x_0$, $x_1$, and $x_2$. Then, by differentiating this polynomial, obtain $p_2'(x)$ and $p_2''(x)$ and hence compute $p_2'(x_1)$ and $p_2''(x_1)$ directly from the divided differences. Do not assume any special formulas beyond what follows from the definitions of divided differences and polynomial differentiation.\n2. From geometric principles, the curvature $\\kappa$ of a planar curve given as $y=y(x)$ relates to its first and second derivatives at a point. Use this relation to connect the curvature at $x_1$ to the radius $R$ of the osculating circle. Additionally, determine the center $\\left(a,b\\right)$ of the osculating circle by moving from $\\left(x_1,y_1\\right)$ along the appropriate unit normal direction by a distance equal to the radius. Use the sign of the local concavity to choose the correct normal direction. In your derivation, do not assume any closed-form shortcut beyond basic differential geometry definitions and vector normalization.\n3. Implement a program that, for each provided test case, does the following:\n   - Generates three points $\\left(x_i,y_i\\right)$ on a circle with given true parameters $\\left(a,b,R\\right)$ by using $y_i=b+s\\sqrt{R^2-\\left(x_i-a\\right)^2}$ for a specified branch sign $s\\in\\{+1,-1\\}$. Use $s=+1$ in all cases below.\n   - Optionally adds independent, zero-mean Gaussian noise with a specified standard deviation $\\sigma$ to each $y_i$, leaving $x_i$ unchanged, to obtain observed values $\\tilde{y}_i$.\n   - Constructs the quadratic Newton interpolant through $\\left(x_i,\\tilde{y}_i\\right)$, evaluates the first and second derivatives at $x_1$, and from these estimates computes the osculating circle center $\\left(\\hat{a},\\hat{b}\\right)$ and radius $\\hat{R}$.\n   - Reports the absolute errors $\\left(|\\hat{a}-a|,|\\hat{b}-b|,|\\hat{R}-R|\\right)$ for the case.\n\nNumerical and formatting requirements:\n- All computations are dimensionless.\n- If noise is used, it is applied as independent samples from a normal distribution with zero mean and the specified standard deviation $\\sigma$ added to each $y_i$. Use the provided random seeds for reproducibility. Angles, if any are implicitly involved in derivations, need not be reported and no angular unit is required.\n- For each test case, report a list of three floating-point numbers corresponding to $\\left(|\\hat{a}-a|,|\\hat{b}-b|,|\\hat{R}-R|\\right)$. Round each reported float to $12$ decimal places before printing.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case's triple enclosed in its own square brackets and no spaces. For example, a valid format is $\\left[\\left[0.0,0.0,0.0\\right],\\left[1.0,2.0,3.0\\right]\\right]$.\n\nTest suite:\n- Case $1$ (noise-free, symmetric nodes, moderate curvature):\n  - True parameters: $\\left(a,b,R\\right)=\\left(0.75,-0.5,1.9\\right)$.\n  - Nodes: $x_0=a-0.5$, $x_1=a$, $x_2=a+0.5$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=0$.\n- Case $2$ (noise-free, asymmetric nodes, moderate curvature):\n  - True parameters: $\\left(a,b,R\\right)=\\left(0.75,-0.5,1.9\\right)$.\n  - Nodes: $x_0=a-0.6$, $x_1=a+0.1$, $x_2=a+0.7$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=0$.\n- Case $3$ (noisy, symmetric nodes, moderate curvature):\n  - True parameters: $\\left(a,b,R\\right)=\\left(0.75,-0.5,1.9\\right)$.\n  - Nodes: $x_0=a-0.5$, $x_1=a$, $x_2=a+0.5$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=10^{-5}$, random seed $12345$.\n- Case $4$ (noise-free, symmetric nodes, nearly straight arc):\n  - True parameters: $\\left(a,b,R\\right)=\\left(10.0,-20.0,1000.0\\right)$.\n  - Nodes: $x_0=a-0.1$, $x_1=a$, $x_2=a+0.1$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=0$.\n- Case $5$ (noisy, symmetric nodes, nearly straight arc):\n  - True parameters: $\\left(a,b,R\\right)=\\left(10.0,-20.0,1000.0\\right)$.\n  - Nodes: $x_0=a-0.1$, $x_1=a$, $x_2=a+0.1$.\n  - Branch: $s=+1$.\n  - Noise: $\\sigma=10^{-8}$, random seed $24680$.\n\nFinal output format:\n- A single line containing a list of the five error triples, in the order of Cases $1$ through $5$, as a single comma-separated list enclosed in square brackets, with no spaces, and each triple rounded to $12$ decimal places. For example: $\\left[\\left[e_{1a},e_{1b},e_{1R}\\right],\\left[e_{2a},e_{2b},e_{2R}\\right],\\left[e_{3a},e_{3b},e_{3R}\\right],\\left[e_{4a},e_{4b},e_{4R}\\right],\\left[e_{5a},e_{5b},e_{5R}\\right]\\right]$ where each $e$ denotes the corresponding absolute error.",
            "solution": "The problem statement presented is valid. It is scientifically grounded in the principles of numerical analysis and differential geometry, is well-posed with a clear objective, and provides all necessary information to proceed with a unique solution. The task is to estimate the parameters of a circle from three sample points by employing a quadratic interpolant constructed via Newton's divided differences and then using the geometric properties of its osculating circle at the central point. This is a standard and verifiable problem in computational physics.\n\nWe shall proceed with the derivation and solution, following the specified requirements meticulously.\n\n**Part 1: The Quadratic Interpolant and its Derivatives**\n\nGiven three distinct points $(x_0, y_0)$, $(x_1, y_1)$, and $(x_2, y_2)$ with distinct abscissas, there exists a unique polynomial of degree at most two that passes through them. We construct this polynomial, $p_2(x)$, using the Newton form.\n\nFirst, we define the necessary divided differences:\nThe zeroth-order divided difference is simply the function value:\n$$f[x_i] = y_i$$\nThe first-order divided difference is the slope of the secant line between two points:\n$$f[x_i, x_j] = \\frac{f[x_j] - f[x_i]}{x_j - x_i}$$\nThe second-order divided difference is the difference of first-order differences:\n$$f[x_i, x_j, x_k] = \\frac{f[x_j, x_k] - f[x_i, x_j]}{x_k - x_i}$$\n\nThe Newton form of the quadratic interpolating polynomial passing through $(x_0, y_0)$, $(x_1, y_1)$, and $(x_2, y_2)$ is given by:\n$$p_2(x) = f[x_0] + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1)$$\nLet us denote the coefficients, which are the divided differences, as $c_0 = f[x_0]$, $c_1 = f[x_0, x_1]$, and $c_2 = f[x_0, x_1, x_2]$. The polynomial is then:\n$$p_2(x) = c_0 + c_1(x - x_0) + c_2(x - x_0)(x - x_1)$$\n\nTo find the first and second derivatives of the interpolating polynomial, we differentiate $p_2(x)$ with respect to $x$.\nThe first derivative, $p_2'(x)$, is:\n$$p_2'(x) = \\frac{d}{dx} \\left[ c_0 + c_1(x - x_0) + c_2(x^2 - (x_0 + x_1)x + x_0x_1) \\right]$$\n$$p_2'(x) = 0 + c_1 + c_2(2x - (x_0 + x_1))$$\n$$p_2'(x) = c_1 + c_2((x - x_0) + (x - x_1))$$\nWe evaluate this at the central point $x_1$:\n$$p_2'(x_1) = c_1 + c_2((x_1 - x_0) + (x_1 - x_1)) = c_1 + c_2(x_1 - x_0)$$\nSubstituting the definitions of the divided differences $c_1$ and $c_2$:\n$$p_2'(x_1) = f[x_0, x_1] + f[x_0, x_1, x_2](x_1 - x_0)$$\n\nThe second derivative, $p_2''(x)$, is found by differentiating $p_2'(x)$:\n$$p_2''(x) = \\frac{d}{dx} \\left[ c_1 + c_2(2x - x_0 - x_1) \\right] = 2c_2$$\nThe second derivative of a quadratic polynomial is constant. Therefore, at $x_1$:\n$$p_2''(x_1) = 2c_2 = 2f[x_0, x_1, x_2]$$\nThese expressions for $p_2'(x_1)$ and $p_2''(x_1)$ are derived directly from the definition of the Newton polynomial and are what we will use for our computation.\n\n**Part 2: Osculating Circle Parameters**\n\nThe osculating circle at a point on a curve is the circle that best approximates the curve at that point. Its radius is the reciprocal of the curvature, and its center lies along the normal line.\n\nFor a curve defined by $y = y(x)$, the curvature $\\kappa$ is given by the formula:\n$$\\kappa(x) = \\frac{|y''(x)|}{(1 + [y'(x)]^2)^{3/2}}$$\nWe approximate the curvature of the underlying curve at $(x_1, y_1)$ by the curvature of the interpolating polynomial $p_2(x)$ at $x=x_1$. Let this be $\\hat{\\kappa}$.\n$$\\hat{\\kappa} = \\frac{|p_2''(x_1)|}{(1 + [p_2'(x_1)]^2)^{3/2}}$$\nThe estimated radius of the osculating circle, $\\hat{R}$, is the reciprocal of this curvature:\n$$\\hat{R} = \\frac{1}{\\hat{\\kappa}} = \\frac{(1 + [p_2'(x_1)]^2)^{3/2}}{|p_2''(x_1)|}$$\n\nThe center of the osculating circle, $(\\hat{a}, \\hat{b})$, is located at a distance $\\hat{R}$ from the point $(x_1, y_1)$ along the unit normal vector. The tangent vector to the curve $y=p_2(x)$ at $x_1$ is proportional to $\\langle 1, p_2'(x_1) \\rangle$. A normal vector is therefore proportional to $\\langle -p_2'(x_1), 1 \\rangle$. The unit normal vector $\\vec{n}$ is:\n$$\\vec{n} = \\frac{\\langle -p_2'(x_1), 1 \\rangle}{\\sqrt{(-p_2'(x_1))^2 + 1^2}} = \\frac{\\langle -p_2'(x_1), 1 \\rangle}{\\sqrt{1 + [p_2'(x_1)]^2}}$$\nThis vector points \"upwards\" (positive $y$-component). The center of curvature lies in this direction if the curve is concave up ($p_2''(x_1) > 0$), and in the opposite direction if the curve is concave down ($p_2''(x_1) < 0$). We can thus express the displacement vector from $(x_1, y_1)$ to $(\\hat{a}, \\hat{b})$ as $\\hat{R} \\cdot \\text{sign}(p_2''(x_1)) \\cdot \\vec{n}$.\n\nThe position vector of the center is:\n$$\\langle \\hat{a}, \\hat{b} \\rangle = \\langle x_1, y_1 \\rangle + \\hat{R} \\cdot \\text{sign}(p_2''(x_1)) \\cdot \\vec{n}$$\nSubstituting the expressions for $\\hat{R}$ and $\\vec{n}$:\n$$\\langle \\hat{a}, \\hat{b} \\rangle = \\langle x_1, y_1 \\rangle + \\frac{(1 + [p_2'(x_1)]^2)^{3/2}}{|p_2''(x_1)|} \\cdot \\text{sign}(p_2''(x_1)) \\cdot \\frac{\\langle -p_2'(x_1), 1 \\rangle}{\\sqrt{1 + [p_2'(x_1)]^2}}$$\nUsing the identity $|z| \\cdot \\text{sign}(z) = z$, let $z = p_2''(x_1)$, we simplify the scalar part:\n$$\\frac{(1 + [p_2'(x_1)]^2)^{3/2}}{p_2''(x_1)} \\cdot \\frac{1}{\\sqrt{1 + [p_2'(x_1)]^2}} = \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)}$$\nThe vector equation for the center becomes:\n$$\\langle \\hat{a}, \\hat{b} \\rangle = \\langle x_1, y_1 \\rangle + \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)} \\langle -p_2'(x_1), 1 \\rangle$$\nFrom this, we extract the coordinates of the estimated center:\n$$\\hat{a} = x_1 - p_2'(x_1) \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)}$$\n$$\\hat{b} = y_1 + \\frac{1 + [p_2'(x_1)]^2}{p_2''(x_1)}$$\nNote that the $y$-coordinate used in the formula for $\\hat{b}$ is the observed (and potentially noisy) value $\\tilde{y}_1$, as this is the point on the interpolating polynomial through which the osculating circle is constructed.\n\nThese derived formulas are implemented to solve for the estimated parameters and their errors for each test case. The sensitivity to noise is particularly apparent in cases with small curvature (large radius), where $p_2''(x_1)$ is close to zero, making the expressions for $\\hat{a}$, $\\hat{b}$, and $\\hat{R}$ numerically unstable.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the estimated circle parameters using a quadratic interpolant\n    and calculates the absolute errors for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: noise-free, symmetric, moderate curvature\n        {'true_params': (0.75, -0.5, 1.9), 'a_offset': -0.5, 'c_offset': 0.5,\n         'noise_sigma': 0.0, 'seed': None},\n        # Case 2: noise-free, asymmetric, moderate curvature\n        {'true_params': (0.75, -0.5, 1.9), 'a_offset': -0.6, 'c_offset': 0.7,\n         'x1_offset': 0.1, 'noise_sigma': 0.0, 'seed': None},\n        # Case 3: noisy, symmetric, moderate curvature\n        {'true_params': (0.75, -0.5, 1.9), 'a_offset': -0.5, 'c_offset': 0.5,\n         'noise_sigma': 1e-5, 'seed': 12345},\n        # Case 4: noise-free, symmetric, large radius\n        {'true_params': (10.0, -20.0, 1000.0), 'a_offset': -0.1, 'c_offset': 0.1,\n         'noise_sigma': 0.0, 'seed': None},\n        # Case 5: noisy, symmetric, large radius\n        {'true_params': (10.0, -20.0, 1000.0), 'a_offset': -0.1, 'c_offset': 0.1,\n         'noise_sigma': 1e-8, 'seed': 24680},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        a_true, b_true, R_true = case['true_params']\n        sigma = case['noise_sigma']\n        seed = case['seed']\n        \n        # Define nodes\n        x0 = a_true + case['a_offset']\n        x1 = a_true + case.get('x1_offset', 0.0) # a_true if symmetric\n        x2 = a_true + case['c_offset']\n        \n        xs = np.array([x0, x1, x2])\n        \n        # Generate true y-values on the upper branch (s=+1)\n        ys_true = b_true + np.sqrt(R_true**2 - (xs - a_true)**2)\n        \n        # Add noise if specified\n        ys_observed = ys_true.copy()\n        if sigma > 0:\n            rng = np.random.default_rng(seed)\n            noise = rng.normal(0, sigma, 3)\n            ys_observed += noise\n        \n        y0_obs, y1_obs, y2_obs = ys_observed\n        \n        # Calculate divided differences\n        # c0 = f[x0]\n        # c1 = f[x0, x1]\n        # c2 = f[x0, x1, x2]\n        c0 = y0_obs\n        \n        # First order differences\n        f_x0_x1 = (y1_obs - y0_obs) / (x1 - x0)\n        f_x1_x2 = (y2_obs - y1_obs) / (x2 - x1)\n        \n        c1 = f_x0_x1\n        \n        # Second order difference\n        c2 = (f_x1_x2 - f_x0_x1) / (x2 - x0)\n        \n        # Calculate derivatives of the interpolant p2(x) at x1\n        p2_prime_x1 = c1 + c2 * (x1 - x0)\n        p2_double_prime_x1 = 2 * c2\n        \n        # Check for near-zero second derivative (collinear points)\n        if abs(p2_double_prime_x1)  1e-15:\n            # For collinear points, radius is infinite, center is at infinity.\n            # This problem's setup avoids this, but it's good practice.\n            # Report large error.\n            errors = [np.inf, np.inf, np.inf]\n            results.append(errors)\n            continue\n            \n        # Estimate parameters of the osculating circle\n        y_prime_sq = p2_prime_x1**2\n        common_term = (1 + y_prime_sq) / p2_double_prime_x1\n        \n        a_hat = x1 - p2_prime_x1 * common_term\n        b_hat = y1_obs + common_term\n        R_hat = np.abs((1 + y_prime_sq)**(3/2) / p2_double_prime_x1)\n\n        # Calculate absolute errors\n        error_a = abs(a_hat - a_true)\n        error_b = abs(b_hat - b_true)\n        error_R = abs(R_hat - R_true)\n        \n        results.append([error_a, error_b, error_R])\n\n    # Format the final output string as per requirements\n    formatted_results = []\n    for res_triple in results:\n        formatted_triple = f\"[{res_triple[0]:.12f},{res_triple[1]:.12f},{res_triple[2]:.12f}]\"\n        formatted_results.append(formatted_triple)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}