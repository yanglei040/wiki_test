{
    "hands_on_practices": [
        {
            "introduction": "To truly understand an adaptive algorithm, there is no substitute for walking through its logic step-by-step. This first exercise invites you to manually trace an adaptive trapezoidal rule, acting as the processor that decides whether to accept an interval's result or refine it further. By working through a simple polynomial, you will gain a concrete understanding of the recursive subdivision and tolerance-checking at the heart of adaptive quadrature .",
            "id": "2153105",
            "problem": "An adaptive quadrature algorithm refines its calculation mesh to meet a desired error tolerance. Consider the following recursive algorithm designed to approximate the definite integral $I = \\int_a^b f(x) \\,dx$.\n\n**Adaptive Integration Algorithm:**\nThe function `AdaptiveIntegrate(f, a, b, ε)` takes a function $f$, an interval $[a, b]$, and a tolerance $\\epsilon$ as input.\n\n1.  Calculate $S_1$, the approximation from the trapezoidal rule using a single interval of width $h_1 = b-a$. The formula is $S_1 = \\frac{h_1}{2}(f(a) + f(b))$.\n2.  Let $c = (a+b)/2$ be the midpoint of the interval. Calculate $S_2$, the approximation from the composite trapezoidal rule using two subintervals $[a, c]$ and $[c, b]$, each of width $h_2 = (b-a)/2$. The formula is $S_2 = \\frac{h_2}{2}(f(a) + f(c)) + \\frac{h_2}{2}(f(c) + f(b))$.\n3.  An estimate for the error in the more accurate approximation $S_2$ is based on the difference between the two approximations. The stopping criterion is $|S_2 - S_1| < 3\\epsilon$.\n4.  If the criterion is met, the procedure for this interval terminates and returns the value of $S_2$.\n5.  If the criterion is not met, the algorithm refines the interval. It recursively calls itself on the two subintervals, halving the tolerance for each. The result for the interval $[a, b]$ is the sum of the results from these two calls:\n    `AdaptiveIntegrate(f, a, c, ε/2) + AdaptiveIntegrate(f, c, b, ε/2)`\n\nYour task is to manually trace this algorithm to find the numerical approximation of the integral\n$$ I = \\int_0^4 x^2 \\,dx $$\nwith an initial tolerance of $\\epsilon = 2$.\n\nCalculate the final numerical approximation for the integral returned by the algorithm. Provide your answer as an exact integer or fraction.",
            "solution": "We apply the given adaptive trapezoidal algorithm to $f(x)=x^{2}$ on $[0,4]$ with initial tolerance $\\epsilon=2$. The trapezoidal approximation on $[a,b]$ with one panel is $S_{1}=\\frac{h_{1}}{2}\\left(f(a)+f(b)\\right)$ where $h_{1}=b-a$, and with two panels is\n$$\nS_{2}=\\frac{h_{2}}{2}\\left(f(a)+f(c)\\right)+\\frac{h_{2}}{2}\\left(f(c)+f(b)\\right),\n$$\nwhere $c=\\frac{a+b}{2}$ and $h_{2}=\\frac{b-a}{2}$. The stopping criterion for an interval is $\\left|S_{2}-S_{1}\\right|<3\\epsilon$. If not met, we recurse on $[a,c]$ and $[c,b]$ with tolerance $\\epsilon/2$ each and sum the returned values.\n\nTop-level interval $[0,4]$ with $\\epsilon=2$:\n- Compute $h_{1}=4$, $S_{1}=\\frac{4}{2}\\left(f(0)+f(4)\\right)=2\\left(0+16\\right)=32$.\n- Midpoint $c=2$, $h_{2}=2$, and\n$$\nS_{2}=\\frac{2}{2}\\left(f(0)+f(2)\\right)+\\frac{2}{2}\\left(f(2)+f(4)\\right)=1\\left(0+4\\right)+1\\left(4+16\\right)=4+20=24.\n$$\n- Error check: $\\left|S_{2}-S_{1}\\right|=\\left|24-32\\right|=8$, while $3\\epsilon=3\\cdot 2=6$. Since $8\\not<6$, we recurse on $[0,2]$ and $[2,4]$ with tolerance $\\epsilon/2=1$ each.\n\nSubinterval $[0,2]$ with $\\epsilon=1$:\n- Compute $h_{1}=2$, $S_{1}=\\frac{2}{2}\\left(f(0)+f(2)\\right)=1\\left(0+4\\right)=4$.\n- Midpoint $c=1$, $h_{2}=1$, and\n$$\nS_{2}=\\frac{1}{2}\\left(f(0)+f(1)\\right)+\\frac{1}{2}\\left(f(1)+f(2)\\right)=\\frac{1}{2}\\left(0+1\\right)+\\frac{1}{2}\\left(1+4\\right)=\\frac{1}{2}+\\frac{5}{2}=3.\n$$\n- Error check: $\\left|S_{2}-S_{1}\\right|=\\left|3-4\\right|=1$, while $3\\epsilon=3\\cdot 1=3$. Since $1<3$, accept $S_{2}=3$ for $[0,2]$.\n\nSubinterval $[2,4]$ with $\\epsilon=1$:\n- Compute $h_{1}=2$, $S_{1}=\\frac{2}{2}\\left(f(2)+f(4)\\right)=1\\left(4+16\\right)=20$.\n- Midpoint $c=3$, $h_{2}=1$, and\n$$\nS_{2}=\\frac{1}{2}\\left(f(2)+f(3)\\right)+\\frac{1}{2}\\left(f(3)+f(4)\\right)=\\frac{1}{2}\\left(4+9\\right)+\\frac{1}{2}\\left(9+16\\right)=\\frac{13}{2}+\\frac{25}{2}=19.\n$$\n- Error check: $\\left|S_{2}-S_{1}\\right|=\\left|19-20\\right|=1$, while $3\\epsilon=3$. Since $1<3$, accept $S_{2}=19$ for $[2,4]$.\n\nFinally, the top-level call returns the sum of the accepted subinterval results:\n$$\n\\text{Result} = 3+19=22.\n$$\nThis is the final numerical approximation returned by the algorithm.",
            "answer": "$$\\boxed{22}$$"
        },
        {
            "introduction": "Adaptive methods are powerful because they automatically concentrate computational effort where it is most needed. This practice explores the direct relationship between a function's local 'complexity'—quantified here by its higher-order derivatives—and the final grid spacing produced by the algorithm . By analyzing a hypothetical function with a controlled jump in its fourth derivative, you will derive how the algorithm intelligently adjusts subinterval widths to maintain a consistent level of accuracy across the entire integration domain.",
            "id": "2153092",
            "problem": "An adaptive quadrature algorithm based on Simpson's rule is used to numerically compute the integral of a function $f(x)$ over an interval $[A, B]$. The algorithm works recursively. For any given subinterval $[a, b]$, it computes two approximations of the integral: a coarse approximation, $S_{coarse}$, using a single Simpson's rule application over $[a, b]$, and a more refined approximation, $S_{fine}$, by summing the results of Simpson's rule applied to the two halves, $[a, (a+b)/2]$ and $[(a+b)/2, b]$.\n\nThe algorithm estimates the error of the refined approximation as $E_{est} = \\frac{1}{15} |S_{fine} - S_{coarse}|$. If this estimated error is less than a local tolerance $\\epsilon_{local}$, the algorithm accepts $S_{fine}$ as the result for the interval $[a, b]$ and terminates for this branch. Otherwise, the algorithm is applied recursively to the two subintervals, with the local tolerance for each new sub-subinterval being half of the current local tolerance. The initial local tolerance for the whole interval $[A, B]$ is a user-defined global tolerance $\\epsilon_{global}$.\n\nIt is known that the true error of Simpson's rule on an interval of width $h$ is approximately $E_{true} \\approx C h^5 f^{(4)}(\\xi)$ for some point $\\xi$ in the interval, where $C$ is a constant and $f^{(4)}$ is the fourth derivative of the function. For a sufficiently smooth function, the error estimate $E_{est}$ is a good approximation of the true error of $S_{fine}$.\n\nConsider a function $f(x)$ defined on the interval $[0, L]$ whose derivatives up to the third order are continuous. Its fourth derivative, $f^{(4)}(x)$, is piecewise constant with a single jump discontinuity at $x=c$, where $0 < c < L$. Specifically,\n$$\nf^{(4)}(x) = \\begin{cases} K_1 & \\text{if } 0 \\le x < c \\\\ K_2 & \\text{if } c < x \\le L \\end{cases}\n$$\nwhere $K_1$ and $K_2$ are given positive constants.\n\nThe adaptive algorithm is run with a very small global tolerance $\\epsilon_{global}$, resulting in a fine partition of $[0, L]$ into many terminal subintervals. In the limit as $\\epsilon_{global} \\to 0$, the widths of these subintervals become very small. Let $h_L$ be the characteristic width of the terminal subintervals in an infinitesimal region just to the left of the discontinuity (i.e., at $x=c^-$) and $h_R$ be the characteristic width of the terminal subintervals in an infinitesimal region just to the right of the discontinuity (i.e., at $x=c^+$).\n\nDetermine the ratio $h_L / h_R$. Express your answer as a closed-form analytic expression in terms of $K_1$ and $K_2$.",
            "solution": "The local truncation error of a single-panel Simpson rule on an interval of width $h$ is $E_{true} \\approx C h^{5} f^{(4)}(\\xi)$, where $C$ is a constant and $\\xi$ lies in the interval. When $f^{(4)}$ is constant on the interval, the adaptive estimator based on $S_{fine}$ and $S_{coarse}$ has the same $h^{5}$ scaling and differs only by a constant factor, so we can write the estimated error on such a subinterval as\n$$\nE_{est} \\approx D\\,K\\,h^{5},\n$$\nwhere $K$ is the value of $f^{(4)}$ on that subinterval and $D>0$ is a constant that depends only on the Simpson error constant and the refinement procedure.\n\nIn the adaptive algorithm, an interval $[a,b]$ at recursion depth $d$ has:\n- width $h = b-a = L\\,2^{-d}$, since each refinement halves the interval length;\n- local tolerance $\\epsilon_{local} = \\epsilon_{global}\\,2^{-d}$, since each refinement halves the tolerance.\n\nAcceptance occurs (asymptotically) when the estimated error meets the local tolerance, i.e.,\n$$\nD\\,K\\,h^{5} = \\epsilon_{local}.\n$$\nSubstituting $h = L\\,2^{-d}$ and $\\epsilon_{local} = \\epsilon_{global}\\,2^{-d}$ gives\n$$\nD\\,K\\,(L\\,2^{-d})^{5} = \\epsilon_{global}\\,2^{-d}\n\\;\\;\\Longrightarrow\\;\\;\nD\\,K\\,L^{5}\\,2^{-5d} = \\epsilon_{global}\\,2^{-d}\n\\;\\;\\Longrightarrow\\;\\;\nD\\,K\\,L^{5}\\,2^{-4d} = \\epsilon_{global}.\n$$\nSolving for $2^{-d}$ yields\n$$\n2^{-d} = \\left(\\frac{\\epsilon_{global}}{D\\,K\\,L^{5}}\\right)^{\\frac{1}{4}}.\n$$\nTherefore, the terminal width on a side where $f^{(4)} \\equiv K$ is\n$$\nh = L\\,2^{-d} = L\\left(\\frac{\\epsilon_{global}}{D\\,K\\,L^{5}}\\right)^{\\frac{1}{4}}\n= \\left(\\frac{\\epsilon_{global}}{D}\\right)^{\\frac{1}{4}} L^{-\\frac{1}{4}} K^{-\\frac{1}{4}}.\n$$\nApplying this just to the left of $c$ with $K=K_{1}$ yields\n$$\nh_{L} = \\left(\\frac{\\epsilon_{global}}{D}\\right)^{\\frac{1}{4}} L^{-\\frac{1}{4}} K_{1}^{-\\frac{1}{4}},\n$$\nand just to the right with $K=K_{2}$ yields\n$$\nh_{R} = \\left(\\frac{\\epsilon_{global}}{D}\\right)^{\\frac{1}{4}} L^{-\\frac{1}{4}} K_{2}^{-\\frac{1}{4}}.\n$$\nTaking the ratio cancels all common factors, giving\n$$\n\\frac{h_{L}}{h_{R}} = \\left(\\frac{K_{2}}{K_{1}}\\right)^{\\frac{1}{4}}.\n$$\nThis is the characteristic ratio in the limit $\\epsilon_{global} \\to 0$.",
            "answer": "$$\\boxed{\\left(\\frac{K_{2}}{K_{1}}\\right)^{\\frac{1}{4}}}$$"
        },
        {
            "introduction": "An algorithm's theoretical elegance can be undermined by the practical limitations of finite-precision computer arithmetic. This final exercise investigates a common failure mode where catastrophic cancellation during the integrand's evaluation can mislead the adaptive error estimator, leading to incorrect results with no warning . Understanding this interaction between algorithm design and floating-point behavior is a critical skill for any practitioner in computational science.",
            "id": "2371904",
            "problem": "In computational physics, adaptive quadrature algorithms estimate and control local truncation error by recursively subdividing intervals until a prescribed tolerance is met. Consider computing the integral\n$$\nI=\\int_{0}^{1}\\frac{e^{x}-1-x}{x^{2}}\\,dx\n$$\nwith an adaptive Simpson-type method implemented entirely in Institute of Electrical and Electronics Engineers (IEEE) 754 binary32 arithmetic. Let the unit roundoff be $u=2^{-24}\\approx 5.96\\times 10^{-8}$. The routine accepts a subinterval when the absolute difference between a coarse and a refined Simpson estimate on that subinterval is less than a user-specified tolerance $\\tau$. The integrand is evaluated naively as $f(x)=\\big(e^{x}-1-x\\big)/x^{2}$ using the standard library exponential.\n\nWhich of the following statements are correct regarding the potential failure of such an adaptive integrator due to catastrophic cancellation at low precision? Select all that apply.\n\nA. There exists a positive threshold $x_{0}$ on the order of $\\sqrt{u}$ such that, for $0<x<x_{0}$, the floating-point evaluation of $e^{x}-1-x$ in binary32 rounds to exactly $0$, so the computed $f(x)$ is $0$ even though $\\lim_{x\\to 0}f(x)=\\tfrac{1}{2}$. On sufficiently fine subintervals near $x=0$, the error estimator can therefore be artificially small, allowing premature acceptance with a large hidden error.\n\nB. Because adaptive refinement samples more points, it necessarily overcomes catastrophic cancellation near $x=0$; therefore, any adaptive method will converge to the correct value in binary32 arithmetic if $\\tau$ is sufficiently small.\n\nC. A robust remedy is to replace the naive formula by a series for small $\\lvert x \\rvert$, for example $f(x)=\\tfrac{1}{2}+\\tfrac{x}{6}+\\tfrac{x^{2}}{24}+\\cdots$ for $\\lvert x \\rvert \\le \\delta$, with $\\delta$ chosen on the order of $\\sqrt{u}$, and to use the original formula elsewhere.\n\nD. Using an adaptive Gauss–Kronrod pair with the same naive evaluation of $f(x)$ eliminates the failure because Gauss nodes avoid the endpoint $x=0$.\n\nE. Computing the numerator with the specialized function $\\operatorname{expm1}(x)$, that is, evaluating $f(x)=\\big(\\operatorname{expm1}(x)-x\\big)/x^{2}$, pushes the onset of cancellation from $x\\sim \\sqrt{u}$ down to $x\\sim u$, greatly enlarging the range of $x$ over which binary32 yields accurate values and thereby mitigating failure of the adaptive method.",
            "solution": "The analysis begins from first principles: the Taylor expansion of the exponential function and a standard floating-point rounding model. The exponential has the series\n$$\ne^{x}=1+x+\\frac{x^{2}}{2}+\\frac{x^{3}}{6}+\\cdots \\, ,\n$$\nso the exact numerator is\n$$\ne^{x}-1-x=\\frac{x^{2}}{2}+\\frac{x^{3}}{6}+\\cdots \\, ,\n$$\nand the exact integrand is\n$$\nf(x)=\\frac{e^{x}-1-x}{x^{2}}=\\frac{1}{2}+\\frac{x}{6}+\\frac{x^{2}}{24}+\\cdots \\, .\n$$\nTherefore, $f(x)$ is smooth on $[0,1]$ with\n$$\n\\lim_{x\\to 0} f(x)=\\frac{1}{2} \\, .\n$$\n\nIn Institute of Electrical and Electronics Engineers (IEEE) 754 binary32 arithmetic with rounding to nearest, each elementary operation incurs a relative error bounded by the unit roundoff $u=2^{-24}$. For small $x$, the term $\\frac{x^{2}}{2}$ can be below the rounding threshold of the much larger terms $1$ and $x$. In that case, the computed value of $e^{x}$ is rounded to $1\\oplus x$, and the subsequent subtractions by $1$ and by $x$ produce exactly $0$ in floating point. A back-of-the-envelope threshold for when $\\frac{x^{2}}{2}$ is lost is obtained by demanding\n$$\n\\frac{x^{2}}{2}\\lesssim u \\quad \\Longrightarrow \\quad x \\lesssim \\sqrt{2u} \\, .\n$$\nWith $u\\approx 5.96\\times 10^{-8}$, this gives $x$ on the order of $\\sqrt{u}\\approx 3\\times 10^{-4}$. Thus there is a nontrivial interval $(0,x_{0})$ with $x_{0}=\\Theta(\\sqrt{u})$ on which the computed numerator is exactly $0$, and hence the computed $f(x)$ equals $0$, despite the true $f(x)\\approx \\tfrac{1}{2}$ there.\n\nThis behavior directly impacts adaptive error estimation. On sufficiently fine subintervals that lie within $(0,x_{0})$, all function evaluations at the quadrature nodes will return the same erroneous value (namely $0$). Both the coarse and the refined Simpson estimates will then be equal, their difference will be $0<\\tau$ for any $\\tau>0$, and the subinterval will be accepted with an estimated local error of $0$, even though the true contribution is of order (subinterval length)$\\times \\tfrac{1}{2}$. Hence premature acceptance with a large hidden error can occur.\n\nRefining the mesh does not fix catastrophic cancellation; indeed, making subintervals smaller moves nodes closer to $x=0$, which exacerbates cancellation and enlarges the region of zeroed evaluations. The fundamental issue is in the evaluation of the integrand, not in the density of samples.\n\nA principled remedy is to avoid subtracting nearly equal numbers. For small $\\lvert x\\rvert$, one can evaluate $f(x)$ by its convergent Taylor series\n$$\nf(x)=\\frac{1}{2}+\\frac{x}{6}+\\frac{x^{2}}{24}+\\cdots \\, ,\n$$\ntruncated to a suitable order. Choosing a switch radius $\\delta=\\Theta(\\sqrt{u})$ ensures that the series is both numerically stable and accurate on $\\lvert x\\rvert\\le \\delta$, while for $\\lvert x\\rvert>\\delta$ the naive formula is sufficiently well conditioned. This prevents the plateau of zeroed values and restores a reliable error estimator.\n\nAlternatively, one can improve the naive formula by computing the small difference $e^{x}-1$ with the specialized function $\\operatorname{expm1}(x)$, designed to return $e^{x}-1$ with high relative accuracy for small $x$. Then the numerator becomes $\\operatorname{expm1}(x)-x$. The rounding error in $\\operatorname{expm1}(x)$ scales with the magnitude of $e^{x}-1\\sim x$, not with $e^{x}\\sim 1$, so cancellation when subtracting $x$ is much less severe. A similar threshold analysis yields the condition for the computed numerator to round to zero as\n$$\n\\frac{x^{2}}{2}\\lesssim u\\,\\lvert x\\rvert \\quad \\Longrightarrow \\quad \\lvert x\\rvert \\lesssim 2u \\, ,\n$$\nwhich pushes the onset of the zero plateau from $x=\\Theta(\\sqrt{u})$ down to $x=\\Theta(u)$, a dramatic improvement in binary32.\n\nFinally, switching the quadrature family without addressing the integrand evaluation does not remove the failure mode. Although Gauss–Kronrod rules do not place nodes exactly at $x=0$, they do place nodes arbitrarily close to $x=0$ when a subinterval near the endpoint is refined, and the same catastrophic cancellation will occur at those small nodes if the naive formula is used.\n\nOption-by-option analysis:\n\nA. Correct. As shown, there is a threshold $x_{0}=\\Theta(\\sqrt{u})$ below which $e^{x}-1-x$ rounds to exactly $0$ in binary32, so the computed $f(x)$ is $0$ although the true limit is $\\tfrac{1}{2}$. This destroys the reliability of the local error estimator on sufficiently fine subintervals near $x=0$.\n\nB. Incorrect. Adaptive refinement does not remedy cancellation; smaller subintervals force evaluations at smaller $x$, where cancellation is worse. The problem is not insufficient sampling but ill-conditioned evaluation.\n\nC. Correct. A piecewise definition that uses a Taylor series for $\\lvert x\\rvert\\le \\delta$ with $\\delta=\\Theta(\\sqrt{u})$ and the naive formula otherwise avoids subtractive cancellation and yields stable, accurate values needed by the adaptive estimator.\n\nD. Incorrect. Changing to Gauss–Kronrod nodes does not fix the evaluation; nodes near $x=0$ still suffer from cancellation, so the same failure can occur.\n\nE. Correct. Using $\\operatorname{expm1}(x)$ to compute $e^{x}-1$ improves the conditioning of the numerator and moves the cancellation threshold from $x=\\Theta(\\sqrt{u})$ to $x=\\Theta(u)$, greatly expanding the usable range in binary32 and mitigating adaptive failure.",
            "answer": "$$\\boxed{ACE}$$"
        }
    ]
}