{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, our first practice involves applying the bisection method manually. This exercise walks you through the core iterative process of narrowing a bracket to find a root. By working with a hypothetical profit function, you will gain a tangible feel for how the algorithm systematically halves the search interval, ensuring the root remains contained within the ever-shrinking bounds .",
            "id": "2157541",
            "problem": "A small manufacturing company wants to determine its break-even production level. The profit, $P$, from producing and selling $x$ thousand units of a product is modeled by the function $P(x) = -1.5x^2 + 140x - 1000$. A root of this function corresponds to a break-even point where profit is zero. The company analysts use the bisection method to find this break-even point. They start with an initial interval $[a_0, b_0] = [5, 10]$. After performing three iterations of the bisection method, they obtain a new, smaller interval $[a_3, b_3]$ that contains the root. What are the values of $a_3$ and $b_3$?\n\nRepresent your answer as a row matrix $\\begin{pmatrix} a_3  b_3 \\end{pmatrix}$.",
            "solution": "We are given the profit function $P(x) = -1.5x^{2} + 140x - 1000$, a continuous polynomial. By the Intermediate Value Theorem, if $P(a)$ and $P(b)$ have opposite signs, then there exists a root in $(a,b)$. The bisection method proceeds by repeatedly halving the interval and selecting the subinterval where the sign change persists. We start with $[a_{0},b_{0}] = [5,10]$.\n\nFirst, compute the signs at the endpoints:\n$$\nP(5) = -\\frac{3}{2}\\cdot 25 + 140\\cdot 5 - 1000 = -\\frac{75}{2} - 300 = -\\frac{675}{2}  0,\n$$\n$$\nP(10) = -\\frac{3}{2}\\cdot 100 + 140\\cdot 10 - 1000 = -150 + 400 = 250  0.\n$$\nThus a root lies in $[5,10]$.\n\nIteration 1: midpoint $m_{1} = \\frac{a_{0}+b_{0}}{2} = \\frac{5+10}{2} = \\frac{15}{2}$.\nEvaluate\n$$\nP\\!\\left(\\frac{15}{2}\\right) = -\\frac{3}{2}\\cdot \\frac{225}{4} + 140\\cdot \\frac{15}{2} - 1000 = -\\frac{675}{8} + 1050 - 1000 = -\\frac{275}{8}  0.\n$$\nSince $P(a_{0})0$ and $P(m_{1})0$ while $P(b_{0})0$, the sign change is in $[m_{1},b_{0}]$. Hence $[a_{1},b_{1}] = \\left[\\frac{15}{2}, 10\\right]$.\n\nIteration 2: midpoint $m_{2} = \\frac{a_{1}+b_{1}}{2} = \\frac{\\frac{15}{2}+10}{2} = \\frac{35}{4}$.\nEvaluate\n$$\nP\\!\\left(\\frac{35}{4}\\right) = -\\frac{3}{2}\\cdot \\frac{1225}{16} + 140\\cdot \\frac{35}{4} - 1000 = -\\frac{3675}{32} + 1225 - 1000 = \\frac{3525}{32}  0.\n$$\nSince $P(a_{1})0$ and $P(m_{2})0$, the sign change is in $[a_{1},m_{2}]$. Hence $[a_{2},b_{2}] = \\left[\\frac{15}{2}, \\frac{35}{4}\\right]$.\n\nIteration 3: midpoint $m_{3} = \\frac{a_{2}+b_{2}}{2} = \\frac{\\frac{15}{2}+\\frac{35}{4}}{2} = \\frac{65}{8}$.\nEvaluate\n$$\nP\\!\\left(\\frac{65}{8}\\right) = -\\frac{3}{2}\\cdot \\frac{4225}{64} + 140\\cdot \\frac{65}{8} - 1000 = -\\frac{12675}{128} + \\frac{2275}{2} - 1000 = \\frac{4925}{128}  0.\n$$\nSince $P(a_{2})0$ and $P(m_{3})0$, the sign change is in $[a_{2},m_{3}]$. Therefore, after three iterations the interval is\n$$\n[a_{3},b_{3}] = \\left[\\frac{15}{2}, \\frac{65}{8}\\right].\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{15}{2}  \\frac{65}{8} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While the bisection method is reliable, is it the most efficient bracketing method? This question leads us to explore alternatives like the regula falsi (or false position) method. In this problem, you will compare the first approximation from both bisection and regula falsi, providing direct insight into how each method's strategy—one based on interval midpoints, the other on linear interpolation—affects convergence on a given function .",
            "id": "2157489",
            "problem": "Consider the task of finding a root for the function $f(x) = x^3 - 5$ within the interval $[1, 2]$. Two common iterative bracketing methods for this task are the bisection method and the regula falsi method (also known as the method of false position).\n\nLet $c_B$ be the first approximation of the root obtained by applying one iteration of the bisection method to the initial interval $[1, 2]$.\nLet $c_{RF}$ be the first approximation of the root obtained by applying one iteration of the regula falsi method to the same initial interval $[1, 2]$.\n\nThe true root of the function is $x_{true} = \\sqrt[3]{5}$. For the purpose of this problem, you may use the value $x_{true} \\approx 1.709976$.\n\nCalculate the absolute difference between the approximation errors of these two methods. That is, compute the value of $| |c_B - x_{true}| - |c_{RF} - x_{true}| |$. Round your final answer to four significant figures.",
            "solution": "We are given $f(x) = x^{3} - 5$ on $[1,2]$, with $f(1) = -4$ and $f(2) = 3$, so the root lies in $[1,2]$.\n\nFor one bisection step, the midpoint is\n$$\nc_{B} = \\frac{1 + 2}{2} = \\frac{3}{2}.\n$$\n\nFor one regula falsi step from $(a,f(a)) = (1,-4)$ and $(b,f(b)) = (2,3)$, the linear interpolation root is\n$$\nc_{RF} = a - f(a)\\,\\frac{b - a}{f(b) - f(a)} = 1 - (-4)\\,\\frac{2 - 1}{3 - (-4)} = 1 + \\frac{4}{7} = \\frac{11}{7}.\n$$\n\nSince $f$ is strictly increasing on $[1,2]$ and $x_{true} \\in [1,2]$, we have $c_{B}  x_{true}$ and $c_{RF}  x_{true}$, hence\n$$\n|c_{B} - x_{true}| = x_{true} - \\frac{3}{2}, \\quad |c_{RF} - x_{true}| = x_{true} - \\frac{11}{7}.\n$$\nTherefore, the absolute difference of errors is\n$$\n\\left||c_{B} - x_{true}| - |c_{RF} - x_{true}|\\right| = \\left|(x_{true} - \\tfrac{3}{2}) - (x_{true} - \\tfrac{11}{7})\\right| = \\left|\\frac{11}{7} - \\frac{3}{2}\\right| = \\left|\\frac{22 - 21}{14}\\right| = \\frac{1}{14}.\n$$\n\nNumerically,\n$$\n\\frac{1}{14} \\approx 0.071428571\\ldots,\n$$\nwhich rounds to four significant figures as $0.07143$.",
            "answer": "$$\\boxed{0.07143}$$"
        },
        {
            "introduction": "Practical numerical solvers are often sophisticated hybrids, combining the strengths of multiple algorithms. This final, advanced practice challenges you to implement such a solver, wedding the robustness of the bisection method with the rapid local convergence of the Newton-Raphson method. This coding exercise demonstrates how to build a powerful and efficient root-finding tool, reflecting the design principles used in professional scientific software libraries .",
            "id": "2377926",
            "problem": "You are to implement a hybrid bracketing root-finding algorithm that combines the guaranteed convergence of interval bisection with the local rapid convergence of the Newton–Raphson method. The algorithm must begin with a strict bracket $\\left[a,b\\right]$ satisfying $f(a)\\,f(b) lt; 0$ for a continuous function $f(x)$, thereby invoking the Intermediate Value Theorem, and must maintain the bracketing invariant until a well-defined switching criterion is met.\n\nFrom first principles, the bracketing phase is based on the Intermediate Value Theorem for continuous $f(x)$ on $\\left[a,b\\right]$, and updates the bracket by repeatedly selecting the subinterval where the sign change persists. The Newton–Raphson phase is based on first-order Taylor linearization of $f(x)$ around a current iterate $x$, which leads to an iteration that approaches a root when $f'(x)$ is nonzero and the initial guess is sufficiently close.\n\nDesign requirements for the hybrid method:\n- Use a bracketing phase that always preserves $f(a)\\,f(b) lt; 0$. Continue this phase until the current bracket width satisfies $(b-a) \\le \\delta$, where $\\delta gt; 0$ is a prescribed threshold.\n- Once $(b-a) \\le \\delta$, switch to an in-bracket Newton–Raphson phase using the midpoint $x=\\frac{a+b}{2}$ as the initial guess. A Newton step $x_{\\text{new}} = x - \\frac{f(x)}{f'(x)}$ may only be accepted if $x_{\\text{new}} \\in (a,b)$ and $f'(x)$ is finite and nonzero. If the acceptance conditions fail, perform a bisection step instead. After producing a candidate $x_{\\text{new}}$, update the bracket $\\left[a,b\\right]$ by selecting the subinterval that retains the sign change, and continue.\n- Termination conditions: stop when either $|f(x)| \\le \\tau_f$ or $(b-a) \\le \\tau_x$, where $\\tau_f gt; 0$ and $\\tau_x gt; 0$ are absolute tolerances.\n- Angles for any trigonometric function must be in radians.\n- Your implementation must be deterministic and must not use randomness.\n\nUse the following absolute tolerances for all test cases: $\\tau_f = 10^{-12}$ and $\\tau_x = 10^{-12}$. Use a maximum of $N_{\\max} = 1000$ total iterations to prevent infinite loops.\n\nTest suite specification. Implement your solver and apply it to the following five cases. For each case, define $f(x)$ and its derivative $f'(x)$ exactly as given. Ensure that the initial endpoints satisfy $f(a)\\,f(b) lt; 0$ as stated.\n\n- Case $1$ (polynomial, single simple root):\n  - $f_1(x) = x^3 - 2x - 5$\n  - $f_1'(x) = 3x^2 - 2$\n  - $\\left[a,b\\right] = \\left[2,3\\right]$, $\\delta = 10^{-3}$\n\n- Case $2$ (transcendental, fixed point type, angles in radians):\n  - $f_2(x) = \\cos(x) - x$\n  - $f_2'(x) = -\\sin(x) - 1$\n  - $\\left[a,b\\right] = \\left[0,1\\right]$, $\\delta = 10^{-8}$\n\n- Case $3$ (odd-multiplicity root, derivative vanishes at the root):\n  - $f_3(x) = (x - 1)^3$\n  - $f_3'(x) = 3(x - 1)^2$\n  - $\\left[a,b\\right] = \\left[0.5,2.0\\right]$, $\\delta = 10^{-4}$\n\n- Case $4$ (immediate switching boundary where initial width equals threshold):\n  - $f_4(x) = e^x - 3$\n  - $f_4'(x) = e^x$\n  - $\\left[a,b\\right] = \\left[1.0,1.1\\right]$, $\\delta = 10^{-1}$\n\n- Case $5$ (root at the midpoint, exact detection during bracketing):\n  - $f_5(x) = x^3$\n  - $f_5'(x) = 3x^2$\n  - $\\left[a,b\\right] = \\left[-10^{-1},10^{-1}\\right]$, $\\delta = 10^{-6}$\n\nAll functions are continuous on their respective intervals, and for Cases $1$, $2$, $3$, and $5$, the endpoints satisfy $f(a)\\,f(b) lt; 0$. For Case $4$, the initial bracket width is exactly equal to $\\delta$.\n\nRequired outputs:\n- For each case, output the computed root approximation as a floating-point number. Express angles in radians where applicable. No physical units are involved.\n- Round each root to exactly $12$ digits after the decimal point using fixed-point formatting.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$ (for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$).",
            "solution": "The problem presented is a well-posed and scientifically grounded task in the field of computational physics. It requires the implementation of a hybrid root-finding algorithm that synergizes the guaranteed convergence of the bisection method with the quadratic convergence of the Newton-Raphson method. The problem statement is complete, consistent, and provides all necessary parameters, initial conditions, functions, and their derivatives for a set of test cases. It is therefore deemed valid.\n\nThe core of the problem is to design an algorithm that locates a root of a continuous function $f(x)$ within an initial bracket $[a, b]$ where the condition $f(a)f(b)  0$ holds. This condition, by the Intermediate Value Theorem, guarantees the existence of at least one root in the interval. The algorithm must operate in two phases, governed by the width of the current bracket $(b-a)$ relative to a specified threshold $\\delta  0$.\n\nLet us formalize the algorithm step-by-step.\n\n1.  **Initialization**:\n    The algorithm begins with an initial interval $[a, b]$, a continuous function $f(x)$, its derivative $f'(x)$, a switching threshold $\\delta$, convergence tolerances $\\tau_f$ and $\\tau_x$, and a maximum number of iterations $N_{\\max}$. We first evaluate the function at the endpoints, $f_a = f(a)$ and $f_b = f(b)$, and confirm the root is bracketed, i.e., $f_a f_b  0$. If $f_a$ or $f_b$ is zero, the root is found immediately.\n\n2.  **Iterative Refinement Loop**:\n    The process iterates, refining the bracket $[a, b]$ until a termination condition is met, for a maximum of $N_{\\max}$ iterations. At the start of each iteration, we check for termination based on the bracket width: if $(b-a) \\le \\tau_x$, the process halts, and the best estimate for the root is the midpoint of the final interval, $\\frac{a+b}{2}$.\n\n3.  **Phase Determination and Point Selection**:\n    The core of the hybrid strategy lies in the selection of the next candidate point, which depends on the current bracket width.\n    -   **Phase 1: Bisection Method ($ (b-a)  \\delta $)**\n        If the bracket is wide, we are in the bracketing phase. The convergence is guaranteed but slow. We use the bisection method, which selects the midpoint of the interval as the next candidate:\n        $$x_{\\text{next}} = \\frac{a+b}{2}$$\n    -   **Phase 2: Hybrid Newton-Raphson/Bisection Method ($ (b-a) \\le \\delta $)**\n        Once the bracket is sufficiently narrow, we switch to a more aggressive strategy to accelerate convergence.\n        a. The initial guess for the Newton-Raphson method is taken as the midpoint of the current interval, $x_k = \\frac{a+b}{2}$.\n        b. We evaluate the derivative $f'(x_k)$. For the Newton-Raphson step to be valid, the derivative must be finite and non-zero.\n        c. If the derivative is valid, we compute the Newton-Raphson update:\n        $$x_{\\text{Newton}} = x_k - \\frac{f(x_k)}{f'(x_k)}$$\n        d. This new point $x_{\\text{Newton}}$ is only accepted if it falls strictly within the current bracket, i.e., $x_{\\text{Newton}} \\in (a, b)$. This safeguard prevents the iteration from jumping outside the region where a root is known to exist.\n        e. If the Newton-Raphson step is accepted, we set $x_{\\text{next}} = x_{\\text{Newton}}$.\n        f. If any of the conditions for the Newton-Raphson step fail (zero or non-finite derivative, or the new point is out of bounds), the algorithm must fall back to a safe bisection step for the current iteration. In this case, we set $x_{\\text{next}} = \\frac{a+b}{2}$.\n\n4.  **Bracket Update and Termination Check**:\n    After determining $x_{\\text{next}}$ (either from bisection or Newton-Raphson), we evaluate $f_{\\text{next}} = f(x_{\\text{next}})$.\n    - We check for termination based on the function value: if $|f_{\\text{next}}| \\le \\tau_f$, the root is considered found, and $x_{\\text{next}}$ is returned.\n    - If the process has not terminated, we update the bracket. We use the sign of $f_{\\text{next}}$ to determine which subinterval, $[a, x_{\\text{next}}]$ or $[x_{\\text{next}}, b]$, contains the root.\n        - If $f(a)f(x_{\\text{next}})  0$, the root lies in the first half, so we set the new bracket to $[a, x_{\\text{next}}]$ by updating $b = x_{\\text{next}}$.\n        - Otherwise, the root must lie in the second half (since $f(a)f(b)$ was initially negative), so we set the new bracket to $[x_{\\text{next}}, b]$ by updating $a = x_{\\text{next}}$.\n    The loop then continues with the new, narrower bracket.\n\n5.  **Finalization**:\n    If the loop completes $N_{\\max}$ iterations without meeting the $\\tau_f$ or $\\tau_x$ tolerance, the algorithm terminates and returns the midpoint of the last computed interval, $\\frac{a+b}{2}$, as the best available approximation of the root.\n\nThis design ensures both robustness and efficiency. The initial bisection phase reliably narrows the search space, and the subsequent switch to the safeguarded Newton-Raphson method provides rapid convergence once the iterate is sufficiently close to the root. The fallback to bisection is a critical safety mechanism that maintains the guarantee of convergence even when Newton's method would fail, such as in the vicinity of roots with zero derivatives (e.g., Case $3$ where $f_3'(1)=0$) or where the step is too large. The implementation will be applied to the five specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef hybrid_root_finder(f, df, a, b, delta, tau_f, tau_x, n_max):\n    \"\"\"\n    Finds a root of a function using a hybrid bisection-Newton-Raphson method.\n\n    Args:\n        f (callable): The function for which to find a root.\n        df (callable): The derivative of the function f.\n        a (float): The lower bound of the initial bracket.\n        b (float): The upper bound of the initial bracket.\n        delta (float): The bracket width threshold to switch to Newton's method.\n        tau_f (float): The absolute tolerance for the function value.\n        tau_x (float): The absolute tolerance for the bracket width.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        float: The approximation of the root.\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n\n    if np.sign(fa) == np.sign(fb):\n        # As per problem specification, initial brackets are valid.\n        # This check is for general robustness.\n        raise ValueError(\"Root not bracketed or multiple roots in bracket.\")\n\n    if abs(fa)  tau_f:\n        return a\n    if abs(fb)  tau_f:\n        return b\n\n    for _ in range(n_max):\n        # Termination condition 1: Bracket width is smaller than tolerance.\n        if (b - a) = tau_x:\n            return (a + b) / 2\n\n        x_next = None\n\n        # Check for switching condition to attempt Newton-Raphson.\n        if (b - a) = delta:\n            # Candidate for Newton's method is the midpoint.\n            x_mid = (a + b) / 2\n            f_mid = f(x_mid)\n\n            # Termination condition 2: Function value at midpoint is small enough.\n            if abs(f_mid)  tau_f:\n                return x_mid\n\n            df_mid = df(x_mid)\n\n            # Check if Newton step is valid and safe.\n            if df_mid != 0 and np.isfinite(df_mid):\n                x_newton = x_mid - f_mid / df_mid\n                # Accept step only if it's within the current bracket.\n                if a  x_newton  b:\n                    x_next = x_newton\n\n        # If Newton step was not attempted or failed, fall back to bisection.\n        if x_next is None:\n            x_next = (a + b) / 2\n\n        f_next = f(x_next)\n\n        # Termination condition 3: Function value at the new point is small enough.\n        if abs(f_next)  tau_f:\n            return x_next\n\n        # Update the bracket to maintain the sign change.\n        if np.sign(fa) * np.sign(f_next)  0:\n            b = x_next\n            fb = f_next\n        else:\n            a = x_next\n            fa = f_next\n    \n    # If max iterations reached, return the midpoint of the final bracket.\n    return (a + b) / 2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define global tolerances and maximum iterations.\n    tau_f = 1e-12\n    tau_x = 1e-12\n    n_max = 1000\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda x: x**3 - 2*x - 5,\n            \"df\": lambda x: 3*x**2 - 2,\n            \"a\": 2.0, \"b\": 3.0, \"delta\": 1e-3\n        },\n        {\n            \"f\": lambda x: np.cos(x) - x,\n            \"df\": lambda x: -np.sin(x) - 1,\n            \"a\": 0.0, \"b\": 1.0, \"delta\": 1e-8\n        },\n        {\n            \"f\": lambda x: (x - 1)**3,\n            \"df\": lambda x: 3*(x - 1)**2,\n            \"a\": 0.5, \"b\": 2.0, \"delta\": 1e-4\n        },\n        {\n            \"f\": lambda x: np.exp(x) - 3,\n            \"df\": lambda x: np.exp(x),\n            \"a\": 1.0, \"b\": 1.1, \"delta\": 1e-1\n        },\n        {\n            \"f\": lambda x: x**3,\n            \"df\": lambda x: 3*x**2,\n            \"a\": -1e-1, \"b\": 1e-1, \"delta\": 1e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        root = hybrid_root_finder(\n            f=case[\"f\"],\n            df=case[\"df\"],\n            a=case[\"a\"],\n            b=case[\"b\"],\n            delta=case[\"delta\"],\n            tau_f=tau_f,\n            tau_x=tau_x,\n            n_max=n_max\n        )\n        # Format result to 12 decimal places as a fixed-point string.\n        results.append(f\"{root:.12f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}