{
    "hands_on_practices": [
        {
            "introduction": "Evaluating functions near their limits can expose hidden numerical pitfalls. This exercise  explores a classic case of **catastrophic cancellation**, where the direct computation of a mathematically well-behaved function fails dramatically in finite-precision arithmetic. By comparing a naive implementation of $f(x) = \\frac{\\tan(x) - \\sin(x)}{x^3}$ with a strategically reformulated, stable version, you will gain firsthand experience in diagnosing and fixing numerical instabilities arising from the subtraction of nearly equal quantities.",
            "id": "2389878",
            "problem": "You are tasked with assessing numerical stability and error propagation when evaluating the function $f(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$ for values of $x$ near $0$ in radians. Your program must implement two different numerical evaluations of $f(x)$ for each test value of $x$, and quantify the error of each evaluation relative to a high-accuracy reference constructed from a truncated series expansion.\n\nDefinitions and requirements:\n\n- Work in radians.\n- Define the following three evaluations for any given nonzero $x$:\n  1. The direct evaluation $f_{\\mathrm{dir}}(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$.\n  2. A numerically stable evaluation obtained by the trigonometric rearrangement\n     $$f_{\\mathrm{stab}}(x) = \\dfrac{2 \\sin(x) \\sin^2\\!\\left(\\dfrac{x}{2}\\right)}{x^3 \\cos(x)},$$\n     which avoids subtractive cancellation by using $1 - \\cos(x) = 2\\sin^2\\!\\left(\\dfrac{x}{2}\\right)$.\n  3. A reference value constructed from the Maclaurin series of $f(x)$ truncated at order $x^6$:\n     $$f_{\\mathrm{ref}}(x) = \\dfrac{1}{2} + \\dfrac{x^2}{8} + \\dfrac{91}{1680} x^4 + \\dfrac{529}{24192} x^6.$$\n     This reference is valid for $|x| \\leq 10^{-1}$ with a remainder that is $\\mathcal{O}(x^8)$.\n\n- For each $x$, compute the absolute relative errors\n  $$E_{\\mathrm{dir}}(x) = \\dfrac{\\left| f_{\\mathrm{dir}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|}, \\quad E_{\\mathrm{stab}}(x) = \\dfrac{\\left| f_{\\mathrm{stab}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|}.$$\n\nTest suite:\n\nEvaluate and report the errors for the following inputs (all in radians): $x \\in \\{10^{-1}, 10^{-4}, 10^{-8}, -10^{-8}, 10^{-12}, 10^{-16}\\}$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each test value of $x$, first $E_{\\mathrm{dir}}(x)$ and then $E_{\\mathrm{stab}}(x)$. For the test values given above in the stated order, the output must therefore be\n$$[E_{\\mathrm{dir}}(10^{-1}), E_{\\mathrm{stab}}(10^{-1}), E_{\\mathrm{dir}}(10^{-4}), E_{\\mathrm{stab}}(10^{-4}), E_{\\mathrm{dir}}(10^{-8}), E_{\\mathrm{stab}}(10^{-8}), E_{\\mathrm{dir}}(-10^{-8}), E_{\\mathrm{stab}}(-10^{-8}), E_{\\mathrm{dir}}(10^{-12}), E_{\\mathrm{stab}}(10^{-12}), E_{\\mathrm{dir}}(10^{-16}), E_{\\mathrm{stab}}(10^{-16})].$$\nAll angles must be interpreted in radians. All reported quantities must be decimal floating-point numbers.",
            "solution": "The problem as stated is valid. It is a well-posed problem in computational physics that is scientifically grounded, internally consistent, and free of ambiguity. It addresses the fundamental numerical concept of catastrophic cancellation and its mitigation.\n\nThe core task is to evaluate the function $f(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$ for values of $x$ near $0$. This evaluation is performed using three distinct methods to demonstrate and quantify numerical error.\n\nThe first method is the direct evaluation, $f_{\\mathrm{dir}}(x) = \\dfrac{\\tan(x) - \\sin(x)}{x^3}$. This formulation is susceptible to catastrophic cancellation. For $x \\approx 0$, we have $\\tan(x) \\approx x$ and $\\sin(x) \\approx x$. In finite-precision floating-point arithmetic, the computation of $\\tan(x) - \\sin(x)$ involves the subtraction of two very close numbers. This operation causes the leading, significant digits to cancel, leaving a result dominated by representation errors. This loss of precision is then amplified by the division by a very small number, $x^3$.\n\nTo understand the behavior of $f(x)$ for small $x$, we examine the Maclaurin series expansions of the numerator's components:\n$$ \\tan(x) = x + \\frac{x^3}{3} + \\frac{2x^5}{15} + \\mathcal{O}(x^7) $$\n$$ \\sin(x) = x - \\frac{x^3}{6} + \\frac{x^5}{120} + \\mathcal{O}(x^7) $$\nSubtracting these series reveals the true behavior of the numerator:\n$$ \\tan(x) - \\sin(x) = \\left( \\frac{1}{3} - (-\\frac{1}{6}) \\right) x^3 + \\left( \\frac{2}{15} - \\frac{1}{120} \\right) x^5 + \\mathcal{O}(x^7) = \\frac{1}{2}x^3 + \\frac{1}{8}x^5 + \\mathcal{O}(x^7) $$\nThus, the function $f(x)$ approaches a finite limit as $x \\to 0$:\n$$ \\lim_{x \\to 0} f(x) = \\lim_{x \\to 0} \\frac{\\frac{1}{2}x^3 + \\frac{1}{8}x^5 + \\mathcal{O}(x^7)}{x^3} = \\frac{1}{2} $$\nThe direct formula attempts to compute a small leading term of order $\\mathcal{O}(x^3)$ by subtracting two much larger terms of order $\\mathcal{O}(x)$, which is the recipe for numerical disaster.\n\nThe second method, $f_{\\mathrm{stab}}(x)$, employs a trigonometric identity to reformulate the expression and avoid this cancellation. The derivation is as follows:\n$$ f(x) = \\frac{\\frac{\\sin(x)}{\\cos(x)} - \\sin(x)}{x^3} = \\frac{\\sin(x) \\left( \\frac{1}{\\cos(x)} - 1 \\right)}{x^3} = \\frac{\\sin(x) (1 - \\cos(x))}{x^3 \\cos(x)} $$\nThe term $1 - \\cos(x)$ is still a problematic subtraction for small $x$. However, it can be replaced using the half-angle identity $1 - \\cos(x) = 2 \\sin^2\\left(\\frac{x}{2}\\right)$. This yields the numerically stable form:\n$$ f_{\\mathrm{stab}}(x) = \\frac{2 \\sin(x) \\sin^2\\left(\\frac{x}{2}\\right)}{x^3 \\cos(x)} $$\nThis expression contains no subtractions of nearly equal quantities and is expected to be numerically stable.\n\nThe third method provides a high-accuracy reference value, $f_{\\mathrm{ref}}(x)$, using a truncated Maclaurin series of $f(x)$ itself. From the expansion of $\\tan(x) - \\sin(x)$, we derive the series for $f(x)$:\n$$ f(x) = \\frac{\\frac{1}{2}x^3 + \\frac{1}{8}x^5 + \\frac{91}{1680}x^7 + \\dots}{x^3} = \\frac{1}{2} + \\frac{1}{8}x^2 + \\frac{91}{1680}x^4 + \\dots $$\nThe problem provides a more accurate truncation:\n$$ f_{\\mathrm{ref}}(x) = \\frac{1}{2} + \\frac{x^2}{8} + \\frac{91}{1680} x^4 + \\frac{529}{24192} x^6 $$\nThis polynomial evaluation is inherently stable for small $x$ because it involves only additions of positive terms (since $x$ is squared) and multiplications. This makes it an ideal benchmark.\n\nThe numerical implementation will consist of three functions corresponding to $f_{\\mathrm{dir}}$, $f_{\\mathrm{stab}}$, and $f_{\\mathrm{ref}}$. We will then iterate through the test suite of $x$ values: $\\{10^{-1}, 10^{-4}, 10^{-8}, -10^{-8}, 10^{-12}, 10^{-16}\\}$. For each $x$, we compute the absolute relative errors for the direct and stable methods against the reference:\n$$ E_{\\mathrm{dir}}(x) = \\frac{\\left| f_{\\mathrm{dir}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|}, \\quad E_{\\mathrm{stab}}(x) = \\frac{\\left| f_{\\mathrm{stab}}(x) - f_{\\mathrm{ref}}(x) \\right|}{\\left| f_{\\mathrm{ref}}(x) \\right|} $$\nSince $f_{\\mathrm{ref}}(x)$ is a sum of a positive constant ($1/2$) and non-negative terms, its value is always greater than or equal to $1/2$, so there is no risk of division by zero in the error calculation. The function $f(x)$ is an even function, meaning $f(x) = f(-x)$, so the results for $x=10^{-8}$ and $x=-10^{-8}$ are expected to be identical.\n\nThe implementation will use Horner's method for the polynomial evaluation of $f_{\\mathrm{ref}}(x)$ to optimize performance and maintain numerical accuracy. The final output will be a list of the computed errors, $E_{\\mathrm{dir}}(x)$ and $E_{\\mathrm{stab}}(x)$, for each test case in the specified order. We anticipate that $E_{\\mathrm{dir}}(x)$ will increase dramatically as $|x| \\to 0$, while $E_{\\mathrm{stab}}(x)$ will remain small, confirming the superior stability of the reformulated expression.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_dir(x: float) -> float:\n    \"\"\"\n    Direct evaluation of f(x) = (tan(x) - sin(x)) / x**3.\n    This form is prone to catastrophic cancellation for x near 0.\n    \"\"\"\n    if x == 0.0:\n        return 0.5  # The limit as x -> 0\n    return (np.tan(x) - np.sin(x)) / (x**3)\n\ndef f_stab(x: float) -> float:\n    \"\"\"\n    Numerically stable evaluation of f(x) using trigonometric rearrangement.\n    f_stab(x) = (2 * sin(x) * sin(x/2)**2) / (x**3 * cos(x)).\n    \"\"\"\n    if x == 0.0:\n        return 0.5  # The limit as x -> 0\n    \n    # Pre-calculate common terms\n    sin_x = np.sin(x)\n    x_half = x / 2.0\n    sin_x_half = np.sin(x_half)\n    cos_x = np.cos(x)\n    x_cubed = x**3\n    \n    # Avoid division by zero if cos(x) is zero, though not for test cases\n    if cos_x == 0.0:\n        return np.inf * np.sign(sin_x)\n        \n    numerator = 2.0 * sin_x * sin_x_half**2\n    denominator = x_cubed * cos_x\n    \n    return numerator / denominator\n\ndef f_ref(x: float) -> float:\n    \"\"\"\n    Reference evaluation of f(x) using its Maclaurin series expansion\n    truncated at the O(x^6) term.\n    f_ref(x) = 1/2 + x^2/8 + (91/1680)x^4 + (529/24192)x^6.\n    \"\"\"\n    if x == 0.0:\n        return 0.5\n        \n    # Use Horner's method for efficient and stable polynomial evaluation\n    y = x**2\n    c0 = 1.0 / 2.0\n    c1 = 1.0 / 8.0\n    c2 = 91.0 / 1680.0\n    c3 = 529.0 / 24192.0\n\n    return c0 + y * (c1 + y * (c2 + y * c3))\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem's requirements.\n    It calculates and reports the relative errors for two numerical methods\n    against a reference value for a given set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        1e-1,\n        1e-4,\n        1e-8,\n        -1e-8,\n        1e-12,\n        1e-16\n    ]\n\n    results = []\n    for x in test_cases:\n        # Calculate the function value using all three methods\n        val_dir = f_dir(x)\n        val_stab = f_stab(x)\n        val_ref = f_ref(x)\n\n        # Calculate the absolute relative errors\n        # Note: abs(val_ref) is guaranteed to be >= 0.5, so no division by zero\n        error_dir = np.abs(val_dir - val_ref) / np.abs(val_ref)\n        error_stab = np.abs(val_stab - val_ref) / np.abs(val_ref)\n        \n        results.append(error_dir)\n        results.append(error_stab)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Numerical instability is not confined to scalar functions; it can also compromise fundamental vector operations crucial to physics simulations. This practice  investigates the numerical stability of the cross product, specifically when dealing with two nearly parallel vectors. You will discover how the standard formula, which involves subtractions, suffers from catastrophic cancellation in this geometric configuration, and you will quantify the resulting loss of accuracy for vectors of varying magnitudes.",
            "id": "2389869",
            "problem": "You are asked to assess, by direct numerical experiment, the numerical stability of computing the cross product of two nearly parallel three-dimensional vectors. The analysis must be based strictly on the mathematical definition of the cross product and the definition of relative error.\n\nFor each ordered pair of vectors $\\mathbf{a}=(a_1,a_2,a_3)$ and $\\mathbf{b}=(b_1,b_2,b_3)$ in the test suite below, do the following:\n\n1. Compute, using standard double-precision floating-point arithmetic, the magnitude $\\lVert \\mathbf{a}\\times\\mathbf{b}\\rVert$ defined by\n$$\n\\mathbf{a}\\times\\mathbf{b}=\\big(a_2 b_3 - a_3 b_2,\\; a_3 b_1 - a_1 b_3,\\; a_1 b_2 - a_2 b_1\\big)\n$$\nand\n$$\n\\lVert \\mathbf{a}\\times\\mathbf{b}\\rVert=\\sqrt{(a_2 b_3 - a_3 b_2)^2+(a_3 b_1 - a_1 b_3)^2+(a_1 b_2 - a_2 b_1)^2}.\n$$\n\n2. Independently, compute a high-precision reference value of the same magnitude using arithmetic with at least $80$ correct decimal digits.\n\n3. For each pair, compute the relative error\n$$\n\\varepsilon=\\frac{\\left|\\lVert \\mathbf{a}\\times\\mathbf{b}\\rVert_{\\text{double}}-\\lVert \\mathbf{a}\\times\\mathbf{b}\\rVert_{\\text{high-precision}}\\right|}{\\lVert \\mathbf{a}\\times\\mathbf{b}\\rVert_{\\text{high-precision}}}.\n$$\n\nAll quantities in this problem are dimensionless, so no physical units apply. Angles, where implicitly present through the geometric interpretation of the cross product, are not to be directly used.\n\nTest suite (each component is given as an exact decimal string):\n\n- Case $1$ (moderate magnitude, nearly parallel):\n  - $\\mathbf{a}_1=(1, 1, 1)$, $\\mathbf{b}_1=(1.000000000001, 0.999999999999, 1)$.\n- Case $2$ (large magnitude, nearly parallel):\n  - $\\mathbf{a}_2=(10000000000000000, 10000000000000000, 10000000000000000)$,\n    $\\mathbf{b}_2=(10000000000010000, 9999999999990000, 10000000000000000)$.\n- Case $3$ (small magnitude, nearly parallel):\n  - $\\mathbf{a}_3=(1\\text{e-}16, 1\\text{e-}16, 1\\text{e-}16)$, $\\mathbf{b}_3=(1.000000000001\\text{e-}16, 0.999999999999\\text{e-}16, 1\\text{e-}16)$.\n- Case $4$ (very small magnitude, nearly parallel; susceptible to underflow in double precision):\n  - $\\mathbf{a}_4=(1\\text{e-}200, 1\\text{e-}200, 1\\text{e-}200)$, $\\mathbf{b}_4=(1.000000000001\\text{e-}200, 0.999999999999\\text{e-}200, 1\\text{e-}200)$.\n- Case $5$ (orthogonal control):\n  - $\\mathbf{a}_5=(1, 0, 0)$, $\\mathbf{b}_5=(0, 1, 0)$.\n\nYour program must compute, for each case $k\\in\\{1,2,3,4,5\\}$, the relative error $\\varepsilon_k$ as defined above, using double-precision floating-point arithmetic for the approximate value and at least $80$ decimal digits for the reference. The final output must be a single line containing the list of the five relative errors in order, rounded to $12$ significant digits, as a comma-separated list enclosed in square brackets, for example, \"$[x_1,x_2,x_3,x_4,x_5]$\".",
            "solution": "The problem requires an assessment of the numerical stability of computing the magnitude of the cross product of two nearly parallel vectors using standard double-precision floating-point arithmetic. The analysis is to be performed by comparing the results against a high-precision reference calculation.\n\nThe fundamental principle being investigated is **catastrophic cancellation**, a major source of error in numerical computations. This phenomenon occurs when two nearly equal numbers are subtracted. In floating-point arithmetic, numbers are stored with a finite number of significant digits. When two numbers that are very close to each other are subtracted, the leading, most significant digits cancel each other out, leaving a result dominated by the trailing, less significant digits which are often contaminated by representation or rounding errors. This effectively amplifies the relative error of the inputs, leading to a result with a large relative error and few, if any, correct significant digits.\n\nThe standard Cartesian formula for the cross product $\\mathbf{c} = \\mathbf{a} \\times \\mathbf{b}$ involves three such subtractions:\n$$c_1 = a_2 b_3 - a_3 b_2$$\n$$c_2 = a_3 b_1 - a_1 b_3$$\n$$c_3 = a_1 b_2 - a_2 b_1$$\nIf vectors $\\mathbf{a}$ and $\\mathbf{b}$ are nearly parallel, then $\\mathbf{b} \\approx k\\mathbf{a}$ for some scalar $k$. Consequently, the terms in each subtraction become nearly equal: $a_i b_j \\approx a_i (k a_j) = k a_i a_j$ and $a_j b_i \\approx a_j (k a_i) = k a_i a_j$. This setup is thus highly susceptible to catastrophic cancellation when using finite-precision arithmetic.\n\nThe solution proceeds by implementing two computational pathways for each test case: a double-precision calculation (`numpy.float64`) and a high-precision reference calculation (using Python's `decimal` module).\n\n1.  **High-Precision Reference Calculation**: To establish a \"ground truth\" value, we use Python's `decimal` module with a precision of $100$ digits, which comfortably exceeds the required $80$ digits. The input vector components, given as strings, are converted to `Decimal` objects to prevent any loss of precision upon initialization. The cross product components and the subsequent magnitude are calculated using `Decimal` arithmetic, yielding $\\|\\mathbf{a} \\times \\mathbf{b}\\|_{\\text{high-precision}}$.\n\n2.  **Double-Precision Calculation**: The same input strings are converted to standard Python `float` objects, which correspond to IEEE $754$ double-precision numbers (`numpy.float64`). The cross product and its magnitude, $\\|\\mathbf{a} \\times \\mathbf{b}\\|_{\\text{double}}$, are computed using these floating-point numbers, simulating a standard numerical environment.\n\n3.  **Relative Error**: The relative error $\\varepsilon$ is then computed as specified:\n    $$ \\varepsilon = \\frac{\\left| \\|\\mathbf{a} \\times \\mathbf{b}\\|_{\\text{double}} - \\|\\mathbf{a} \\times \\mathbf{b}\\|_{\\text{high-precision}} \\right|}{\\|\\mathbf{a} \\times \\mathbf{b}\\|_{\\text{high-precision}}} $$\n    To perform this calculation accurately, the double-precision result is converted to a `Decimal` object before the subtraction, ensuring that the error calculation itself does not introduce further significant error.\n\nWe analyze each case:\n\n-   **Case 1 (Moderate magnitude, nearly parallel)**: The vectors are $\\mathbf{a}_1=(1, 1, 1)$ and $\\mathbf{b}_1=(1+10^{-12}, 1-10^{-12}, 1)$. The component subtractions, such as $a_{1,1}b_{1,2} - a_{1,2}b_{1,1} = 1 \\cdot (1-10^{-12}) - 1 \\cdot (1+10^{-12}) = -2 \\times 10^{-12}$, involve subtracting numbers that differ by approximately $2 \\times 10^{-12}$. In double-precision arithmetic, the representation of $1 \\pm 10^{-12}$ introduces a small initial error. The subtraction amplifies this error's contribution to the final result. The relative error of a component is approximately $\\epsilon_{\\text{mach}} / (10^{-12}) \\approx 10^{-16} / 10^{-12} = 10^{-4}$, leading to the loss of about $4$ decimal digits of precision. The computed relative error for the magnitude is on this order.\n\n-   **Case 2 (Large magnitude, nearly parallel)**: The components are of order $10^{16}$. The intermediate products in the cross product calculation are of order $10^{32}$. The differences between these products, which should be of order $10^{20}$, are lost entirely due to catastrophic cancellation. For example, computing $a_{2,2}b_{2,3} - a_{2,3}b_{2,2}$ involves $(10^{16})^2 - 10^{16}(10^{16}-10^4) = 10^{32} - (10^{32} - 10^{20})$. A double-precision float has about $16$ decimal digits of precision. Thus, $10^{32} - 10^{20}$ evaluates to $10^{32}$, as the second term is too small to be represented. The subtraction yields $0$. All components of the cross product become zero, resulting in a magnitude of $0$ and a relative error of $1$.\n\n-   **Case 3 (Small magnitude, nearly parallel)**: The components are of order $10^{-16}$. The initial floating-point representation is the source of error. A number like $1.000000000001 \\times 10^{-16} = 10^{-16} + 10^{-28}$ is stored. Since double-precision machine epsilon is about $2.2 \\times 10^{-16}$, the term $10^{-28}$ is more than $12$ orders of magnitude smaller than $10^{-16}$ and is lost during the initial conversion to `float64`. The processor effectively sees `1e-16 + 1e-28 == 1e-16`. Thus, vector $\\mathbf{b}_3$ becomes identical to $\\mathbf{a}_3$ in double precision. The cross product of two identical vectors is the zero vector, so the computed magnitude is $0$, and the relative error is $1$.\n\n-   **Case 4 (Very small magnitude, nearly parallel)**: The components are of order $10^{-200}$. The intermediate products in the cross product formula are of order $(10^{-200})^2 = 10^{-400}$. This value is smaller than the smallest representable subnormal double-precision number (which is approx. $4.9 \\times 10^{-324}$). The calculation underflows to $0$. All components of the cross product vector are computed as $0$, yielding a magnitude of $0$ and a relative error of $1$.\n\n-   **Case 5 (Orthogonal control)**: The vectors are $\\mathbf{a}_5=(1, 0, 0)$ and $\\mathbf{b}_5=(0, 1, 0)$. All components are small integers, which are represented exactly in floating-point. The cross product calculation involves only multiplication and subtraction of $0$ and $1$, operations which are performed exactly. The result, $\\|\\mathbf{a}_5 \\times \\mathbf{b}_5\\|_{\\text{double}} = 1$, is identical to the high-precision result, so the relative error is $0$. This case confirms that the algorithm is correct when not afflicted by numerical instability.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the relative error in the magnitude of the cross product\n    for several pairs of 3D vectors.\n    \"\"\"\n\n    # Set the precision for high-precision calculations.\n    # Required is at least 80 digits; 100 provides a safe margin.\n    getcontext().prec = 100\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (('1', '1', '1'), ('1.000000000001', '0.999999999999', '1')),\n        # Case 2\n        (('10000000000000000', '10000000000000000', '10000000000000000'),\n         ('10000000000010000', '9999999999990000', '10000000000000000')),\n        # Case 3\n        (('1e-16', '1e-16', '1e-16'),\n         ('1.000000000001e-16', '0.999999999999e-16', '1e-16')),\n        # Case 4\n        (('1e-200', '1e-200', '1e-200'),\n         ('1.000000000001e-200', '0.999999999999e-200', '1e-200')),\n        # Case 5\n        (('1', '0', '0'), ('0', '1', '0')),\n    ]\n\n    def compute_magnitude_hp(a_str, b_str):\n        a = [Decimal(s) for s in a_str]\n        b = [Decimal(s) for s in b_str]\n        \n        c1 = a[1] * b[2] - a[2] * b[1]\n        c2 = a[2] * b[0] - a[0] * b[2]\n        c3 = a[0] * b[1] - a[1] * b[0]\n        \n        mag_sq = c1**2 + c2**2 + c3**2\n        \n        return mag_sq.sqrt()\n\n    def compute_magnitude_dp(a_str, b_str):\n        a = np.array([float(s) for s in a_str], dtype=np.float64)\n        b = np.array([float(s) for s in b_str], dtype=np.float64)\n        \n        c1 = a[1] * b[2] - a[2] * b[1]\n        c2 = a[2] * b[0] - a[0] * b[2]\n        c3 = a[0] * b[1] - a[1] * b[0]\n        \n        mag_sq = c1**2 + c2**2 + c3**2\n        \n        return np.sqrt(mag_sq)\n\n    results = []\n    for a_vec_str, b_vec_str in test_cases:\n        mag_hp = compute_magnitude_hp(a_vec_str, b_vec_str)\n        mag_dp = compute_magnitude_dp(a_vec_str, b_vec_str)\n        \n        if mag_hp == 0:\n            # If the true value is zero, relative error is 0 if dp is also 0,\n            # otherwise it is infinite. In the given cases, this path is not taken.\n            relative_error = Decimal(0) if mag_dp == 0 else Decimal('inf')\n        else:\n            # Convert dp result to Decimal for high-precision error calculation\n            mag_dp_dec = Decimal(str(mag_dp))\n            absolute_error = abs(mag_dp_dec - mag_hp)\n            relative_error = absolute_error / mag_hp\n        \n        results.append(relative_error)\n\n    def format_to_12_sig_digits(d):\n        \"\"\"Formats a Decimal number to a string with 12 significant digits in scientific notation.\"\"\"\n        if d == 0:\n            return \"0.00000000000e+00\"\n        return f\"{d:.11e}\"\n\n    formatted_results = [format_to_12_sig_digits(res) for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond precision loss from cancellation, another common failure in numerical computing is **intermediate overflow**, where a calculation fails because an intermediate step produces a number larger than the maximum representable value. This exercise  demonstrates this issue using the binomial coefficient $\\binom{n}{k}$. By comparing the naive factorial-based formula with a more robust multiplicative algorithm, you will see how a simple algorithmic reformulation can prevent overflow and deliver accurate results where the direct method would fail completely.",
            "id": "2389940",
            "problem": "You are asked to assess numerical stability and error propagation when computing the binomial coefficient $\\binom{n}{k}$ in double-precision floating-point arithmetic. Consider two algorithms: (i) the factorial-quotient method using the identity $\\binom{n}{k} = \\dfrac{n!}{k!(n-k)!}$ computed entirely in floating-point arithmetic, and (ii) a multiplicative product method that forms the product by iteratively multiplying ratios that avoid large intermediate values. The goal is to quantify and compare the forward relative error of both methods across a designed test suite and to interpret any catastrophic cancellation or loss of significance.\n\nFundamental base for reasoning and implementation:\n- Use the standard model of floating-point arithmetic in base $2$ with rounding to nearest, where each basic arithmetic operation satisfies $\\mathrm{fl}(a \\,\\mathrm{op}\\, b) = (a \\,\\mathrm{op}\\, b)(1+\\delta)$ with $|\\delta| \\leq \\varepsilon_{\\text{machine}}$, and double-precision has $\\varepsilon_{\\text{machine}} \\approx 2^{-53} \\approx 1.11\\times 10^{-16}$.\n- The binomial coefficient is defined for integers $n \\geq 0$ and $0 \\leq k \\leq n$ as $\\binom{n}{k} = \\dfrac{n!}{k!(n-k)!}$ and is exactly an integer in exact arithmetic. You may assume exact integer arithmetic is available in your programming language for computing a reference value.\n\nAlgorithm definitions you must implement:\n- Factorial-quotient method (floating-point): compute $n!$, $k!$, and $(n-k)!$ in floating-point (not using logarithms), then divide to obtain $\\widehat{C}_{\\text{fact}} = \\dfrac{\\mathrm{fl}(n!)}{\\mathrm{fl}(k!)\\,\\mathrm{fl}((n-k)!)}$. If any intermediate factorial overflows to $+\\infty$ or produces Not-a-Number, then the resulting approximation is considered unusable.\n- Multiplicative method (floating-point): compute $\\widehat{C}_{\\text{mult}}$ by setting $k \\leftarrow \\min(k, n-k)$ and iterating for $i = 1,2,\\dots,k$,\n$$\n\\widehat{C}_{\\text{mult}} \\leftarrow \\widehat{C}_{\\text{mult}} \\times \\frac{n - k + i}{i},\n$$\nentirely in floating-point arithmetic.\n\nError metrics to report:\n- For each test case, compute the exact reference $C_{\\text{exact}} = \\binom{n}{k}$ using exact integer arithmetic. Then compute the forward relative error for each method:\n$$\ne_{\\text{fact}} = \\frac{\\left|\\widehat{C}_{\\text{fact}} - C_{\\text{exact}}\\right|}{C_{\\text{exact}}}, \\qquad\ne_{\\text{mult}} = \\frac{\\left|\\widehat{C}_{\\text{mult}} - C_{\\text{exact}}\\right|}{C_{\\text{exact}}}.\n$$\nIf $\\widehat{C}_{\\text{fact}}$ is $+\\infty$ or Not-a-Number, set $e_{\\text{fact}} = +\\infty$. Do the analogous for $\\widehat{C}_{\\text{mult}}$ if it overflows or is Not-a-Number.\n- For each test case, also report a boolean $b$ defined as $b = \\text{True}$ if $e_{\\text{mult}} \\leq e_{\\text{fact}}$, and $b = \\text{False}$ otherwise.\n\nAngle units: not applicable. Physical units: not applicable.\n\nTest suite you must use:\n- Case $1$: $(n,k) = (10,5)$.\n- Case $2$: $(n,k) = (60,30)$.\n- Case $3$: $(n,k) = (100,1)$.\n- Case $4$: $(n,k) = (170,85)$.\n- Case $5$: $(n,k) = (200,2)$.\n- Case $6$: $(n,k) = (300,150)$.\n\nDesign for coverage:\n- The cases cover small symmetric $(k \\approx n/2)$, moderate symmetric, extreme asymmetric $(k \\ll n)$, a boundary near double-precision factorial overflow $(n=170)$, and cases where factorial overflow occurs $(n=200$ and $n=300)$ but the multiplicative method should remain finite.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of length $6$, where each element is a list of three entries $[e_{\\text{fact}}, e_{\\text{mult}}, b]$ in this order, corresponding to the cases $1$ through $6$ above.\n- Each finite floating-point error must be printed in scientific notation with exactly six significant digits (for example, $1.234560\\mathrm{e}{-07}$). If a value is infinite or Not-a-Number, print it as $inf$.\n- The boolean must be printed as either $\\text{True}$ or $\\text{False}$.\n- For example, the overall printed line should look like\n$[[e_{1,\\text{fact}}, e_{1,\\text{mult}}, b_1], [e_{2,\\text{fact}}, e_{2,\\text{mult}}, b_2], \\dots, [e_{6,\\text{fact}}, e_{6,\\text{mult}}, b_6]]$.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of numerical analysis, specifically floating-point arithmetic, and is well-posed, providing a clear and computable task. We are to compare two algorithms for the computation of the binomial coefficient $\\binom{n}{k}$.\n\nThe core of the analysis rests on understanding the limitations of floating-point arithmetic, governed by a finite range and precision. According to the standard model, any basic arithmetic operation `op` is subject to a rounding error: $\\mathrm{fl}(a \\,\\mathrm{op}\\, b) = (a \\,\\mathrm{op}\\, b)(1+\\delta)$, where $|\\delta| \\leq \\varepsilon_{\\text{machine}}$. For the IEEE 754 double-precision standard, the machine epsilon $\\varepsilon_{\\text{machine}} \\approx 2^{-53} \\approx 1.11 \\times 10^{-16}$.\n\nThe first algorithm is the factorial-quotient method, which computes $\\widehat{C}_{\\text{fact}}$ based on the identity $\\binom{n}{k} = \\dfrac{n!}{k!(n-k)!}$. The implementation computes the numerator $\\mathrm{fl}(n!)$ and the denominator $\\mathrm{fl}(k!) \\times \\mathrm{fl}((n-k)!)$ in floating-point arithmetic before the division. The primary vulnerability of this method is **overflow**. The factorial function $n!$ grows super-exponentially. In double-precision arithmetic, the largest representable finite number is approximately $1.8 \\times 10^{308}$. The value of $170!$ is approximately $7.26 \\times 10^{306}$, which is representable, but $171!$ is approximately $1.24 \\times 10^{309}$, which overflows to infinity ($+\\infty$). Consequently, for any $n > 170$, $\\mathrm{fl}(n!)$ will be $+\\infty$. This renders the method unusable, as the computation will result in $+\\infty$ (if the denominator is finite) or Not-a-Number (`NaN`) if the denominator also overflows (e.g., in the indeterminate form $\\infty/\\infty$). This is a catastrophic failure due to the limited range of floating-point numbers, not an issue of precision loss from round-off.\n\nThe second algorithm is the multiplicative method. This method is based on rewriting the binomial coefficient as a product:\n$$ \\binom{n}{k} = \\frac{n \\times (n-1) \\times \\dots \\times (n-k+1)}{k \\times (k-1) \\times \\dots \\times 1} = \\prod_{i=1}^{k} \\frac{n-i+1}{i} $$\nBy using the identity $\\binom{n}{k} = \\binom{n}{n-k}$, we can ensure the number of terms in the product is minimized by choosing the smaller of $k$ and $n-k$. The algorithm is then defined by the iteration $\\widehat{C}_{\\text{mult}} \\leftarrow \\widehat{C}_{\\text{mult}} \\times \\frac{n - k' + i}{i}$ for $i = 1, \\dots, k'$, where $k' = \\min(k, n-k)$. The crucial advantage of this formulation is that it avoids the computation of large intermediate factorials. The terms $\\frac{n-k'+i}{i}$ are of moderate magnitude, and the partial product $\\prod_{j=1}^{i} \\frac{n-k'+j}{j} = \\binom{n-k'+i}{i}$ grows smoothly towards the final value. As long as the final result $\\binom{n}{k}$ itself does not overflow, this method will yield a finite result. This approach is fundamentally more stable as it maintains intermediate values at a manageable scale, thereby avoiding overflow and also reducing the accumulation of round-off errors that are more pronounced when dealing with numbers at the extremes of the floating-point range.\n\nTo quantify the performance, we compute the forward relative error for each method, $e_{\\text{fact}}$ and $e_{\\text{mult}}$, with respect to an exact value $C_{\\text{exact}}$ obtained using arbitrary-precision integer arithmetic. An unusable result from overflow or `NaN` is correctly treated as yielding an infinite error. The test suite is designed to demonstrate these principles: it includes cases where both methods are expected to work well (small $n$), a case at the boundary of overflow for the factorial method ($n=170$), and cases where the factorial method is guaranteed to fail ($n > 170$) while the multiplicative method succeeds. The comparison boolean $b = (e_{\\text{mult}} \\leq e_{\\text{fact}})$ will therefore systematically evaluate the superior stability of the multiplicative approach.",
            "answer": "```python\nimport math\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the numerical error of two algorithms for the binomial coefficient.\n    \"\"\"\n\n    def float_factorial(m: int) -> float:\n        \"\"\"Computes factorial of m as a float, handling overflow to inf.\"\"\"\n        if m < 0:\n            return float('nan')\n        if m == 0:\n            return 1.0\n        res = 1.0\n        for i in range(1, m + 1):\n            res *= float(i)\n        return res\n\n    def compute_fact_method(n: int, k: int) -> float:\n        \"\"\"\n        Computes binomial(n, k) using the factorial-quotient method in floating-point.\n        C = n! / (k! * (n-k)!)\n        \"\"\"\n        if k < 0 or k > n:\n            return float('nan')\n\n        n_fact = float_factorial(n)\n        k_fact = float_factorial(k)\n        nk_fact = float_factorial(n - k)\n\n        # The problem statement implies we perform the floating-point operations\n        # on the (potentially infinite) results of the factorials.\n        denominator = k_fact * nk_fact\n        if denominator == 0.0:\n            return float('inf') # Division by zero\n        \n        return n_fact / denominator\n\n    def compute_mult_method(n: int, k: int) -> float:\n        \"\"\"\n        Computes binomial(n, k) using the stable multiplicative method.\n        C = product_{i=1 to k} (n-k+i)/i\n        \"\"\"\n        if k < 0 or k > n:\n            return float('nan')\n\n        # Use symmetry property binom(n, k) = binom(n, n-k)\n        k_comp = n - k\n        if k_comp < k:\n            k = k_comp\n        \n        if k == 0:\n            return 1.0\n\n        res = 1.0\n        for i in range(1, k + 1):\n            # Formula is derived from C(n,k) = (n/k) * C(n-1, k-1)\n            # which can be iteratively computed as prod_{i=1 to k} (n-i+1)/i\n            # The problem gives an equivalent form: C_mult <- C_mult * (n-k+i)/i\n            term = float(n - k + i) / float(i)\n            res *= term\n        \n        return res\n\n    def format_val(v: float) -> str:\n        \"\"\"Formats a float value according to the problem specification.\"\"\"\n        if np.isinf(v) or np.isnan(v):\n            return 'inf'\n        # The problem example \"1.234560e-07\" has 7 significant digits. The text description\n        # \"six significant digits\" is inconsistent with the example. We follow the example.\n        # Python's \"{:.6e}\" format specifier produces exactly this style.\n        return f\"{v:.6e}\"\n\n    test_cases = [\n        (10, 5),\n        (60, 30),\n        (100, 1),\n        (170, 85),\n        (200, 2),\n        (300, 150),\n    ]\n\n    results_list = []\n    for n, k in test_cases:\n        # Compute exact value using arbitrary-precision integers\n        c_exact = math.comb(n, k)\n        c_exact_float = float(c_exact)\n\n        # Method 1: Factorial-quotient\n        c_fact = compute_fact_method(n, k)\n        if np.isinf(c_fact) or np.isnan(c_fact):\n            e_fact = float('inf')\n        else:\n            e_fact = abs(c_fact - c_exact_float) / c_exact_float if c_exact_float != 0 else 0.0\n\n        # Method 2: Multiplicative\n        c_mult = compute_mult_method(n, k)\n        if np.isinf(c_mult) or np.isnan(c_mult):\n            e_mult = float('inf')\n        else:\n            e_mult = abs(c_mult - c_exact_float) / c_exact_float if c_exact_float != 0 else 0.0\n\n        # Comparison\n        b = e_mult <= e_fact\n        \n        results_list.append(f\"[{format_val(e_fact)},{format_val(e_mult)},{str(b)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_list)}]\")\n\nsolve()\n```"
        }
    ]
}