{
    "hands_on_practices": [
        {
            "introduction": "The quadratic formula is a cornerstone of algebra, familiar to every student of science and engineering. However, its mathematical exactness can be deceiving when implemented in the world of finite-precision computers. This exercise demonstrates how the standard formula can lead to a drastic loss of accuracy due to a phenomenon called \"catastrophic cancellation\" when subtracting two nearly-equal numbers. By working through this calculation and applying a stabilized alternative, you will gain a concrete understanding of a fundamental pitfall in numerical computation and the importance of choosing a stable algorithm. ",
            "id": "2393691",
            "problem": "Consider the quadratic polynomial $p(x) = x^2 - 10^8 x + 1$ with real coefficients. All arithmetic in this problem is to be carried out in a normalized base-$10$ floating-point system with precision $t=8$ significant digits, rounding to nearest with ties to even, and with a sufficiently large exponent range so that no overflow or underflow occurs for the quantities encountered. Each elementary operation (addition, subtraction, multiplication, division, and square root) is performed exactly and then rounded to the nearest representable floating-point number according to the stated rule before being used in any subsequent computation.\n\n(a) Using the standard quadratic formula applied in this floating-point system, compute the two floating-point numbers that would be returned as the roots of $p(x)=0$.\n\n(b) Then, using only identities that hold exactly for monic quadratics (in particular, the fact that the product of the two roots equals $c/a$), compute a numerically stabilized value for the smaller-magnitude root by first computing the larger-magnitude root with the standard quadratic formula in this floating-point system and then using the exact product relation to obtain the smaller-magnitude root in this floating-point system.\n\nReport only the stabilized smaller-magnitude root from part (b) as your final answer, rounded to eight significant figures. Express your final answer in scientific notation of the form $a \\times 10^b$ with $1 \\leq |a| < 10$.",
            "solution": "The problem requires the computation of the roots of a quadratic polynomial within a specified floating-point arithmetic system. Before proceeding to a solution, we must validate the problem statement.\n\nThe givens are:\n1.  The quadratic polynomial is $p(x) = x^2 - 10^8 x + 1$. From this, we identify the coefficients as $a=1$, $b=-10^8$, and $c=1$.\n2.  The arithmetic framework is a normalized base-$10$ floating-point system.\n3.  The precision is $t=8$ significant digits.\n4.  The rounding rule is round to nearest, with ties to even.\n5.  There is no overflow or underflow.\n6.  Each elementary arithmetic operation ($+$, $-$, $*$, $/$, $\\sqrt{\\cdot}$) is individually rounded.\n\nThe problem is scientifically grounded, being a standard exercise in numerical analysis concerning loss of significance. It is well-posed, with all necessary parameters defined. It is objective and its structure is sound. The problem is therefore deemed valid. We proceed to the solution.\n\nThe roots of the quadratic equation $ax^2 + bx + c = 0$ are given by the standard quadratic formula:\n$$x_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$$\nAll computations are performed in the specified floating-point system, which we denote by $\\text{fl}(\\cdot)$.\n\nFirst, we compute the discriminant, $D = b^2 - 4ac$.\nThe coefficients are $a = 1.0000000 \\times 10^0$, $b = -1.0000000 \\times 10^8$, and $c = 1.0000000 \\times 10^0$.\nThe term $b^2$ is $(-10^8)^2 = 10^{16}$. This is represented exactly as $1.0000000 \\times 10^{16}$.\nThe term $4ac$ is $4 \\times 1 \\times 1 = 4$. This is represented exactly as $4.0000000 \\times 10^0$.\n\nWe must now compute the subtraction $\\text{fl}(b^2 - 4ac) = \\text{fl}(10^{16} - 4)$. To perform this operation, the smaller number must be shifted to match the exponent of the larger number.\n$$10^{16} - 4 = (1.0000000 \\times 10^{16}) - (0.0000000000000004 \\times 10^{16})$$\nThe mantissa for $10^{16}$ is $1.0000000$. The system stores $t=8$ significant digits. The subtraction of $4$ affects digits far beyond the precision of the system. The subtraction of the mantissas is $1.0000000 - 0.0000000000000004$, which, when rounded back to $8$ significant digits, remains $1.0000000$. This is a classic example of absorption.\nThus, the computed discriminant is:\n$$\\hat{D} = \\text{fl}(b^2 - 4ac) = 1.0000000 \\times 10^{16}$$\nNext, we compute the square root of $\\hat{D}$.\n$$fl(\\sqrt{\\hat{D}}) = \\text{fl}(\\sqrt{1.0000000 \\times 10^{16}}) = 1.0000000 \\times 10^8$$\nThis is an exact computation. Let us denote this result $\\hat{S} = 1.0000000 \\times 10^8$.\n\nNow, we compute the two roots as specified in part (a).\nThe larger root, $\\hat{x}_1$, is computed using the '$+$' sign in the numerator, as $-b = 10^8$ is positive.\n$$\\hat{x}_1 = \\text{fl}\\left(\\frac{-b + \\hat{S}}{2a}\\right) = \\text{fl}\\left(\\frac{10^8 + 10^8}{2}\\right) = \\text{fl}\\left(\\frac{2 \\times 10^8}{2}\\right) = 1.0000000 \\times 10^8$$\nThis computation is numerically stable, as it involves the addition of two positive numbers of similar magnitude.\n\nThe smaller root, $\\hat{x}_2$, is computed using the '$-$' sign.\n$$\\hat{x}_2 = \\text{fl}\\left(\\frac{-b - \\hat{S}}{2a}\\right) = \\text{fl}\\left(\\frac{10^8 - 10^8}{2}\\right) = \\text{fl}\\left(\\frac{0}{2}\\right) = 0$$\nThis computation suffers from catastrophic cancellation. The subtraction of two nearly identical numbers, $-b$ and $\\hat{S}$, results in a complete loss of significant digits. The computed root $\\hat{x}_2=0$ is grossly inaccurate. The true smaller root is approximately $10^{-8}$.\n\nFor part (b), we are instructed to use a numerically stabilized method. For a monic quadratic equation $x^2 + \\frac{b}{a}x + \\frac{c}{a} = 0$, Vieta's formulas state that the product of the roots is $x_1 x_2 = \\frac{c}{a}$.\nIn our case, $a=1$ and $c=1$, so $x_1 x_2 = 1$.\n\nThe stable procedure is to first compute the larger-magnitude root $\\hat{x}_1$, which we have already found to be accurate:\n$$\\hat{x}_1 = 1.0000000 \\times 10^8$$\nThen, we use the product relation to find the smaller root, $\\hat{x}'_2$:\n$$\\hat{x}'_2 = \\text{fl}\\left(\\frac{c/a}{\\hat{x}_1}\\right)$$\nSubstituting the values:\n$$\\hat{x}'_2 = \\text{fl}\\left(\\frac{1}{1.0000000 \\times 10^8}\\right) = \\text{fl}(1.0000000 \\times 10^{-8})$$\nThe number $1.0000000 \\times 10^{-8}$ is exactly representable in the specified floating-point system. Therefore, the computation yields this value without any rounding error.\nThe stabilized smaller root is:\n$$\\hat{x}'_2 = 1.0000000 \\times 10^{-8}$$\nThis value is required as the final answer, expressed in scientific notation with eight significant figures.",
            "answer": "$$\\boxed{1.0000000 \\times 10^{-8}}$$"
        },
        {
            "introduction": "In practical programming, comparing two numbers for equality is a routine task. When dealing with floating-point numbers, however, a simple `==` check can lead to unexpected and incorrect behavior. This practice uses an intuitive thermostat simulation to illustrate the dangers of direct equality comparison, which arise because many common decimal numbers do not have an exact binary representation. By building a simulation where a \"naive\" controller fails and a \"robust\" one succeeds, you will discover firsthand why tolerance-based comparisons are a crucial technique for writing reliable and accurate programs. ",
            "id": "2395285",
            "problem": "You will write a complete, runnable program that demonstrates, using a simple thermostat simulation, why the naive floating-point comparison pattern `if (a/b == c)` is dangerous. The scenario is a discrete-time thermostat controller that heats a system in steps toward a setpoint and decides when to switch the heater off by comparing the average of recent sensor readings to the setpoint. Your program must simulate two controllers: a naive one that uses exact equality `a/b == c` and a robust one that uses a tolerance to decide when the target is reached.\n\nStart from the following fundamental base:\n\n1. Institute of Electrical and Electronics Engineers (IEEE) 754 floating-point arithmetic represents real numbers in binary and rounds to a nearby representable number after each operation. Many decimal fractions such as $\\,0.1\\,$, $\\,0.2\\,$, and $\\,19.95\\,$ are not exactly representable in binary, so operations like addition and division yield rounding errors.\n2. A discrete-time controller samples at steps $\\,n = 0,1,2,\\dots\\,$ and computes averages over finite windows, which are of the form `a/b` where `a` is a running sum of floating-point readings and `b` is the window size (or the number of available samples up to the window size). This average is then compared to a target setpoint $c$.\n\nControl logic definitions:\n\n- Naive controller: Maintain a list of recent readings and compute the average $\\,\\bar{r} = a/b\\,$ of the last $\\,m\\,$ readings (or of all available readings if fewer than $\\,m\\,$ are available). The heater turns off at the first step $\\,n\\,$ where $\\bar{r}$ `==` $c$ in floating-point.\n- Robust controller: Turn off at the first step $\\,n\\,$ where $\\,\\bar{r} \\ge c - \\tau\\,$ for a small tolerance $\\,\\tau > 0\\,$. You must use $\\,\\tau = 10^{-12}\\,$.\n\nYour task is to implement a program that, for each test case, simulates both controllers up to a maximum number of steps $\\,N_{\\text{max}}\\,$ and returns the number of steps each controller takes to turn the heater off. If a controller does not turn off within $\\,N_{\\text{max}}\\,$ steps, report $\\,{-1}\\,$ for that controller.\n\nAll temperatures are in degrees Celsius; however, the outputs are integers (step counts) and thus unitless. Angles are not involved. No percentages are involved.\n\nTest suite (each case specified as a tuple $\\,\\left(T_0, c, \\Delta, m, N_{\\text{max}}\\right)\\,$):\n\n1. Case A (happy path with exact binary fractions): $\\,\\left(19.0,\\, 20.0,\\, 0.5,\\, 1,\\, 20\\right)\\,$.\n2. Case B (inexact decimal average across three readings): $\\,\\left(20.0,\\, 20.1,\\, 0.1,\\, 3,\\, 20\\right)\\,$.\n3. Case C (target equals a simple average in decimal, likely not exactly representable in binary): $\\,\\left(19.8,\\, 19.95,\\, 0.05,\\, 2,\\, 20\\right)\\,$.\n4. Case D (triple average equals the target in exact decimal arithmetic, stressing rounding): $\\,\\left(17.2,\\, 17.3,\\, 0.1,\\, 3,\\, 20\\right)\\,$.\n5. Case E (boundary condition: already at setpoint): $\\,\\left(21.5,\\, 21.5,\\, 0.1,\\, 4,\\, 10\\right)\\,$.\n\nPrecise requirements for the simulation and outputs:\n\n1. Initialize with the reading $\\,r_0 = T_0\\,$ and check both controllers at step $\\,n=0\\,$.\n2. At each step $\\,n \\to n+1\\,$ while the heater is on, update $\\,T_{n+1} = T_n + \\Delta\\,$, append $\\,r_{n+1} = T_{n+1}\\,$, and recompute the average $\\,\\bar{r}\\,$ over the last $\\,m\\,$ readings (or over all available readings if fewer than $\\,m\\,$ exist). Check the controller conditions in the order new average is computed at that step.\n3. Record the first step $\\,n\\,$ at which each controller turns off. If no turn-off event occurs within $\\,N_{\\text{max}}\\,$ steps, record $\\,{-1}\\,$.\n4. For each test case, output two integers: the step count for the naive controller followed by the step count for the robust controller.\n\nFinal output format: Your program should produce a single line of output containing all results, flattened, as a comma-separated list enclosed in square brackets. For the above five cases, you will therefore output a list of length $\\,10\\,$ in the order\n$\\,\\left[\\text{naive}_A, \\text{robust}_A, \\text{naive}_B, \\text{robust}_B, \\dots, \\text{naive}_E, \\text{robust}_E\\right]\\,$.",
            "solution": "The problem requires an implementation of a discrete-time simulation for a thermostat to demonstrate the unreliability of direct equality comparison with floating-point numbers. This is a classic issue in computational science, rooted in the principles of digital number representation.\n\nThe fundamental principle is the IEEE 754 standard for floating-point arithmetic. Real numbers are stored in a binary format with a finite number of bits. Consequently, decimal fractions that do not have a finite binary representation, such as $0.1$ (which is $0.0001100110011...$ in binary), cannot be stored exactly. Each arithmetic operation may introduce a small rounding error. When these operations are chained, as in a summation for calculating an average, these errors can accumulate. As a result, a calculation that should yield an exact value in pure mathematics, for example $\\sum_{i=1}^{3} (x + (i-1)\\Delta) / 3 = x+\\Delta$, might produce a value in a computer that is infinitesimally different from the true mathematical result. This makes the direct comparison `a/b == c` fragile and likely to fail.\n\nThe simulation will model a system's temperature $T_n$ over discrete time steps $n=0, 1, 2, \\dots$. The system starts at an initial temperature $T_0$. At each step, a heater adds a constant increment $\\Delta$, so the temperature evolves according to the rule $T_{n+1} = T_n + \\Delta$. The sensor reading at step $n$ is simply $r_n = T_n$.\n\nTwo controller strategies are simulated to decide when to turn off the heater:\n\n1.  **Naive Controller**: This controller calculates the average of the most recent readings. Let $S_n = \\{r_i | \\max(0, n-m+1) \\le i \\le n\\}$ be the set of readings in the averaging window of size $m$ at step $n$. The window size is $\\min(n+1, m)$. The average is $\\bar{r}_n = \\frac{1}{|S_n|} \\sum_{r \\in S_n} r$. The controller's logic is to turn off the heater at the first step $n$ where the condition $\\bar{r}_n$ `==` $c$ is met, with $c$ being the target setpoint. This is a direct floating-point comparison.\n\n2.  **Robust Controller**: This controller uses the same average calculation $\\bar{r}_n$. However, its deactivation condition is $\\bar{r}_n \\ge c - \\tau$, where $\\tau$ is a small, positive tolerance, given as $\\tau = 10^{-12}$. This approach acknowledges that the computed average may not be exactly equal to the setpoint due to discretization and rounding. It turns off the heater once the average is \"close enough\" to, or has slightly surpassed, the setpoint.\n\nThe simulation algorithm proceeds as follows for each test case $(T_0, c, \\Delta, m, N_{\\max})$:\n\n1.  Initialize the temperature $T_{\\text{current}} = T_0$ and a list of readings with the first value $r_0 = T_0$.\n2.  Initialize stop steps for both controllers to $-1$.\n3.  **Step $n=0$**: Check the conditions for both controllers using the initial average $\\bar{r}_0 = r_0$. If a condition is met, record the stop step as $0$.\n4.  **Steps $n=1, \\dots, N_{\\max}$**:\n    a. Update the temperature: $T_{\\text{current}} \\leftarrow T_{\\text{current}} + \\Delta$.\n    b. Append the new reading, $r_n = T_{\\text{current}}$, to the list of readings.\n    c. Compute the average $\\bar{r}_n$ over the last $\\min(n+1, m)$ readings.\n    d. For each controller that has not yet stopped, check its condition. If the condition is met, record the current step $n$ as its stop step.\n5.  If any controller's stop step remains $-1$ after iterating up to $N_{\\max}$, it means the condition was never met.\n\nLet us analyze an example, Case D: $(T_0, c, \\Delta, m) = (17.2, 17.3, 0.1, 3)$.\nIn ideal arithmetic:\n-   At $n=0$, $T_0 = 17.2$. Average is $17.2$.\n-   At $n=1$, $T_1 = 17.2 + 0.1 = 17.3$. Average of $\\{17.2, 17.3\\}$ is $17.25$.\n-   At $n=2$, $T_2 = 17.3 + 0.1 = 17.4$. Average of $\\{17.2, 17.3, 17.4\\}$ is $\\frac{17.2+17.3+17.4}{3} = \\frac{51.9}{3} = 17.3$.\nSo, mathematically, the average equals the setpoint $c=17.3$ at step $n=2$.\n\nHowever, in floating-point arithmetic, the value $0.1$ is not exactly representable. The repeated addition of this inexact value leads to an accumulated error. The temperatures $T_1$ and $T_2$ and their subsequent sum will not be the exact mathematical values. The computed average $\\bar{r}_2$ will be a value extremely close to, but not identical to, $17.3$. A typical floating-point result might be $17.300000000000004$.\n\n-   **Naive Controller**: The check $\\bar{r}_2$ `==` `17.3` will evaluate to false. As the temperature continues to rise, the average will further deviate from $17.3$, so the naive controller will never turn off.\n-   **Robust Controller**: The check $\\bar{r}_2 \\ge 17.3 - 10^{-12}$ will evaluate to true, since $17.300000000000004 \\ge 17.299999999999$. The robust controller correctly turns off at step $n=2$.\n\nThis discrepancy illustrates why tolerance-based comparisons are essential for reliable control systems and other applications involving floating-point numbers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the thermostat simulations and print the results.\n    \"\"\"\n    # Test suite defined in the problem statement.\n    # Each case is a tuple: (T_0, c, Delta, m, N_max)\n    test_cases = [\n        (19.0, 20.0, 0.5, 1, 20),      # Case A\n        (20.0, 20.1, 0.1, 3, 20),      # Case B\n        (19.8, 19.95, 0.05, 2, 20),    # Case C\n        (17.2, 17.3, 0.1, 3, 20),      # Case D\n        (21.5, 21.5, 0.1, 4, 10),      # Case E\n    ]\n\n    results = []\n    for case in test_cases:\n        naive_steps, robust_steps = simulate_thermostat(*case)\n        results.extend([naive_steps, robust_steps])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_thermostat(T_0, c, delta, m, N_max):\n    \"\"\"\n    Simulates naive and robust thermostat controllers for a given set of parameters.\n\n    Args:\n        T_0 (float): Initial temperature.\n        c (float): Setpoint temperature.\n        delta (float): Heating increment per step.\n        m (int): Averaging window size.\n        N_max (int): Maximum number of simulation steps.\n\n    Returns:\n        tuple[int, int]: A tuple containing the turn-off step for the naive\n                         and robust controllers, respectively. Returns -1 if\n                         a controller does not turn off within N_max steps.\n    \"\"\"\n    # Use floating-point numbers for all a-priori known physical quantities\n    T_0_f = float(T_0)\n    c_f = float(c)\n    delta_f = float(delta)\n    tau = 1e-12\n\n    # --- Naive Controller Simulation ---\n    naive_stop_step = -1\n    T_current_naive = T_0_f\n    readings_naive = [T_0_f]\n    \n    # Step n=0 check\n    if np.mean([T_0_f]) == c_f:\n        naive_stop_step = 0\n    else:\n        for n in range(1, N_max + 1):\n            T_current_naive += delta_f\n            readings_naive.append(T_current_naive)\n            \n            window_size = min(len(readings_naive), m)\n            readings_to_average = readings_naive[-window_size:]\n            avg_reading = np.mean(readings_to_average)\n\n            if avg_reading == c_f:\n                naive_stop_step = n\n                break\n    \n    # --- Robust Controller Simulation ---\n    robust_stop_step = -1\n    T_current_robust = T_0_f\n    readings_robust = [T_0_f]\n\n    # Step n=0 check\n    if np.mean([T_0_f]) >= c_f - tau:\n        robust_stop_step = 0\n    else:\n        for n in range(1, N_max + 1):\n            T_current_robust += delta_f\n            readings_robust.append(T_current_robust)\n\n            window_size = min(len(readings_robust), m)\n            readings_to_average = readings_robust[-window_size:]\n            avg_reading = np.mean(readings_to_average)\n            \n            if avg_reading >= c_f - tau:\n                robust_stop_step = n\n                break\n\n    return naive_stop_step, robust_stop_step\n\nsolve()\n```"
        },
        {
            "introduction": "Summing a sequence of numbers seems trivial, but when the values span a wide range of magnitudes, a simple running total can produce highly inaccurate results. This issue, known as \"swamping,\" occurs when the addition of a small number to a much larger sum is lost due to rounding. This hands-on exercise challenges you to implement the Kahan summation algorithm, a compensated summation method that cleverly tracks and corrects for the small errors accumulated at each step. Completing this practice will not only highlight the limitations of naive summation but also equip you with a powerful professional tool for achieving high-accuracy results in your computational work. ",
            "id": "2393714",
            "problem": "You are to study the accumulation of rounding error in floating-point addition by contrasting the naive running sum with a compensated summation method. You must implement a complete, runnable program that, for a fixed test suite, computes both the naive floating-point sum and the compensated sum using the Kahan summation algorithm, and compares both against a high-precision reference computed with exact rational arithmetic. Your program must not read any input and must print a single line of output as specified below.\n\nThe fundamental base to be used is the standard rounding-error model for floating-point arithmetic with rounding to nearest: for any two real numbers $a$ and $b$, the computed floating-point addition satisfies $\\text{fl}(a+b) = (a+b)(1+\\delta)$ with $|\\delta|\\le u$, where $u$ is the unit roundoff of the chosen format. You may also use the fact that the naive summation of $n$ terms can accumulate $O(nu)$ rounding error in the worst-case order, whereas compensated summation techniques are designed to reduce the leading-order accumulation by explicitly accounting for the low-order bits lost to rounding.\n\nRequirements:\n- Implement two summation routines operating on lists of real numbers in double precision:\n  - A naive summation that iteratively updates a running sum by $s \\leftarrow s + x_i$ for each term $x_i$.\n  - A Kahan compensated summation that uses a compensation variable to carry forward low-order information lost at each addition.\n- For a high-precision reference, compute the exact sum of each list using exact rational arithmetic (for example, a rational number type that represents each term $x_i$ as a fraction and sums exactly). This reference serves as the ground truth in real arithmetic.\n\nTest suite:\n- Use the following four test cases, each specified as an ordered list. Each list contains large-magnitude terms mixed with small-magnitude terms to expose catastrophic cancellation and loss of significance.\n  1. $[\\,10^{16},\\,1,\\,-10^{16}\\,]$.\n  2. $[\\,10^{16},\\,\\underbrace{1,\\,1,\\,\\dots,\\,1}_{100000\\ \\text{times}},\\,-10^{16}\\,]$. The exact mathematical sum is $100000$.\n  3. $[\\,1,\\,\\underbrace{10^{-16},\\,10^{-16},\\,\\dots,\\,10^{-16}}_{100000\\ \\text{times}},\\,-1\\,]$. The exact mathematical sum is $100000\\cdot 10^{-16}=10^{-11}$.\n  4. $[\\,10^{16},\\,\\underbrace{10^{-6},\\,10^{-6},\\,\\dots,\\,10^{-6}}_{100000\\ \\text{times}},\\,-10^{16}\\,]$. The exact mathematical sum is $100000\\cdot 10^{-6}=10^{-1}$.\n\nComputations and comparisons:\n- For each test case, compute:\n  - The naive floating-point sum $s_{\\text{naive}}\\in\\mathbb{R}$.\n  - The Kahan floating-point sum $s_{\\text{kahan}}\\in\\mathbb{R}$.\n  - The exact reference sum $s_{\\star}\\in\\mathbb{R}$ using exact rational arithmetic.\n- For each test case, compute the absolute errors $e_{\\text{naive}}=\\lvert s_{\\text{naive}}-s_{\\star}\\rvert$ and $e_{\\text{kahan}}=\\lvert s_{\\text{kahan}}-s_{\\star}\\rvert$, and determine a boolean flag $\\text{better}$ that is true if and only if $e_{\\text{kahan}}<e_{\\text{naive}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of four per-test-case records, in the same order as the test suite above. Each record must itself be a list of four elements in the order $[\\,s_{\\text{naive}},\\,s_{\\text{kahan}},\\,s_{\\star},\\,\\text{better}\\,]$, where $s_{\\text{naive}}$, $s_{\\text{kahan}}$, and $s_{\\star}$ are output as floating-point numbers and $\\text{better}$ is a boolean. Aggregate the four records into one list and print it as a single line with comma-separated elements enclosed in square brackets, for example\n  $[\\, [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,], [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,], [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,], [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,] \\,]$.\n\nNotes:\n- No physical units are involved.\n- Angles are not used.\n- Percentages are not used; all quantities are real numbers in $\\mathbb{R}$.\n- The implementation must be self-contained and runnable without user input. Use exact rational arithmetic or a multiple-precision facility available in your languageâ€™s standard library to compute $s_{\\star}$ so that the comparisons of $e_{\\text{naive}}$ and $e_{\\text{kahan}}$ are meaningful.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the established principles of numerical analysis, specifically floating-point arithmetic. It is well-posed, providing a clear definition of the algorithms to be implemented, a complete set of test data, and an unambiguous output format. The problem is objective and free of contradictions or missing information. We may proceed with the solution.\n\nThe fundamental issue addressed is that computer floating-point addition is not associative and is subject to rounding error. For two real numbers $a$ and $b$, their floating-point sum is modeled as $\\text{fl}(a+b) = (a+b)(1+\\delta)$, where the relative error $\\lvert\\delta\\rvert$ is bounded by the unit roundoff $u$. When summing a sequence of numbers, these small errors can accumulate. This analysis will contrast a naive summation approach with a compensated method designed to mitigate this error accumulation.\n\n**1. Naive Summation**\n\nThe most direct method is to iterate through the list of numbers $\\{x_i\\}_{i=1}^n$ and accumulate the sum in a single floating-point variable $s$. The update rule is $s \\leftarrow \\text{fl}(s + x_i)$. The primary weakness of this method is **swamping**. If the magnitude of the running sum $s$ is much larger than the term $x_i$ being added, the contribution of $x_i$ may be partially or completely lost in the rounding process. For instance, in standard double-precision arithmetic, if $s \\approx 10^{16}$ and $x_i = 1$, the operation $\\text{fl}(10^{16} + 1)$ evaluates to $10^{16}$, as the precision is insufficient to represent the result exactly. The term $x_i$ is effectively discarded.\n\n**2. Kahan Compensated Summation**\n\nThe Kahan summation algorithm is a technique that significantly reduces the accumulation of roundoff error. It maintains a second variable, a compensation $c$, which accumulates the errors that occur at each step. For each term $x_i$ in the input sequence, the algorithm performs the following operations:\n1.  Correct the current term: $y \\leftarrow x_i - c$. This step subtracts the accumulated error from the previous additions from the current term.\n2.  Add to the sum: $t \\leftarrow s + y$. This is the standard floating-point addition, where low-order bits of $y$ might be lost if $s$ is large.\n3.  Recover the error: $c \\leftarrow (t - s) - y$. This is the critical step. The term $(t - s)$ represents the part of $y$ that was successfully added to $s$. By subtracting the original (corrected) term $y$, we isolate the negative of the roundoff error from the addition $s+y$. This error is stored in $c$.\n4.  Update the sum: $s \\leftarrow t$.\n\nThis iterative process carries forward the lost \"change\" from each addition and incorporates it into the next, ensuring that the final accumulated error is dramatically smaller than in the naive case. The error bound for Kahan summation is of the order $O(u + N\\epsilon u)$, where $\\epsilon$ is related to machine precision, a vast improvement over the worst-case $O(Nu)$ error for naive summation.\n\n**3. Exact Rational Arithmetic**\n\nTo establish an authoritative ground truth, designated $s_{\\star}$, we must compute the sum without any floating-point error. This is achieved by using exact rational arithmetic. Each input number, which is given as a floating-point value, is first converted to its exact rational representation, i.e., a fraction $p/q$ where $p, q \\in \\mathbb{Z}$. All subsequent additions are performed using the exact rules of fraction arithmetic, e.g., $\\frac{a}{b} + \\frac{c}{d} = \\frac{ad+bc}{bd}$. This procedure is free from the representational and computational errors of floating-point systems, yielding the true mathematical sum of the input values.\n\n**Evaluation Procedure**\n\nFor each of the four specified test cases, we compute the naive sum $s_{\\text{naive}}$, the Kahan sum $s_{\\text{kahan}}$, and the exact reference sum $s_{\\star}$. The test cases are specifically designed to expose the failure modes of naive summation, particularly swamping and catastrophic cancellation, by mixing numbers of vastly different magnitudes. We then compute the absolute errors $e_{\\text{naive}} = \\lvert s_{\\text{naive}} - s_{\\star} \\rvert$ and $e_{\\text{kahan}} = \\lvert s_{\\text{kahan}} - s_{\\star} \\rvert$. The boolean flag $\\text{better}$ is set to $\\text{True}$ if and only if $e_{\\text{kahan}}  e_{\\text{naive}}$, quantitatively demonstrating the superior accuracy of the Kahan algorithm for the given input. The final output is structured as a list of records, with each record containing $[s_{\\text{naive}}, s_{\\text{kahan}}, s_{\\star}, \\text{better}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy\nfrom fractions import Fraction\n\ndef naive_sum(numbers: list[float]) - float:\n    \"\"\"\n    Computes the sum of a list of numbers using a naive iterative approach.\n    \"\"\"\n    s = 0.0\n    for x in numbers:\n        s += x\n    return s\n\ndef kahan_sum(numbers: list[float]) - float:\n    \"\"\"\n    Computes the sum of a list of numbers using the Kahan summation algorithm\n    to reduce the accumulation of floating-point error.\n    \"\"\"\n    s = 0.0  # The running sum.\n    c = 0.0  # The compensation for lost low-order bits.\n    for x in numbers:\n        y = x - c    # c is the error from the previous sum.\n        t = s + y    # s is large, y is small, so low-order digits of y are lost.\n        c = (t - s) - y  # (t - s) recovers the high-order part of y.\n                         # Subtracting y recovers the low part, negated.\n        s = t        # Algebraically, c should be 0. But with rounding, it's not.\n    return s\n\ndef exact_sum(numbers: list[float]) - float:\n    \"\"\"\n    Computes the exact sum of a list of floating-point numbers by\n    converting them to Fractions and using rational arithmetic.\n    \"\"\"\n    s = Fraction(0)\n    for x in numbers:\n        s += Fraction(x)\n    return float(s)\n\ndef solve():\n    \"\"\"\n    Runs the full test suite, comparing naive and Kahan summation against\n    an exact rational arithmetic reference, and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: A simple case of catastrophic cancellation.\n        [1e16, 1.0, -1e16],\n        # Case 2: Summing many small numbers in the presence of a large one.\n        [1e16] + [1.0] * 100000 + [-1e16],\n        # Case 3: Summing many tiny numbers that are smaller than machine epsilon\n        # relative to the initial sum.\n        [1.0] + [1e-16] * 100000 + [-1.0],\n        # Case 4: A similar case to #2, but with smaller additions.\n        [1e16] + [1e-6] * 100000 + [-1e16],\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Compute the sum using all three methods.\n        s_naive = naive_sum(case_data)\n        s_kahan = kahan_sum(case_data)\n        s_star = exact_sum(case_data)\n        \n        # Calculate the absolute errors for both floating-point methods.\n        e_naive = abs(s_naive - s_star)\n        e_kahan = abs(s_kahan - s_star)\n        \n        # Determine if Kahan's method produced a smaller error.\n        better = e_kahan  e_naive\n        \n        # Store the record for this test case.\n        record = [s_naive, s_kahan, s_star, better]\n        results.append(record)\n\n    # Final print statement in the exact required format.\n    # The format template from the prompt is used: print(f\"[{','.join(map(str, results))}]\")\n    # str(list) in Python automatically includes spaces, which matches the example\n    # format diagram '[ [ . , . , . , . ], ... ]'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}