{
    "hands_on_practices": [
        {
            "introduction": "在单细胞分析中，基于图的聚类是鉴定细胞群落的核心方法。然而，不同的图聚类算法基于不同的数学原理，可能产生截然不同的结果。本练习将引导您比较两种主流方法：基于模块度优化的聚类（如Louvain算法）和谱聚类。通过构建一个包含大小和密度不均簇的合成数据集，您将亲手揭示它们在面对真实生物数据中常见挑战时的性能差异，从而理解算法选择的重要性。",
            "id": "2379606",
            "problem": "给定一个通过单细胞RNA测序（scRNA-seq）获得的单细胞转录组实验的抽象表示。每个细胞由一个二维嵌入空间（例如，主成分分析（PCA）嵌入）中的一个点表示。这些细胞属于少数几个生物种群（细胞状态），目标是仅使用这些点的几何结构来恢复这些种群。\n\n形式上，给定一个有限点集 $\\{x_i\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^2$。对于每个数据集，在顶点集 $V=\\{1,\\dots,N\\}$ 上构建一个无向、无权、对称的k-近邻图 $G=(V,E)$，方法如下：对于每对 $i \\neq j$ 的顶点 $(i,j)$，当且仅当 $j$ 是 $i$ 在欧几里得距离下的 $k$ 个最近邻之一，或者 $i$ 是 $j$ 的 $k$ 个最近邻之一时（对称并集规则），边 $\\{i,j\\}$ 存在于 $E$ 中。令 $A$ 为 $G$ 的邻接矩阵，其元素为 $A_{ij} \\in \\{0,1\\}$，度为 $k_i = \\sum_j A_{ij}$，且 $2m = \\sum_{i,j} A_{ij}$。\n\n将顶点集 $V$ 的一个划分 $P$ 的模块度定义为\n$$\nQ(P) \\;=\\; \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\frac{k_i k_j}{2m} \\right) \\mathbf{1}\\{c(i)=c(j)\\},\n$$\n其中 $c(i)$ 表示顶点 $i$ 在划分 $P$ 中的社区标签，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。考虑以下两种 $V$ 的划分：\n\n1. 一个模块度优化的划分 $P_{\\mathrm{mod}}$，通过贪婪的局部节点移动获得。该方法从单例社区开始，通过单顶点重分配来寻求 $Q(P)$ 的局部最大值。具体来说，从所有 $i$ 的 $c(i)=i$ 开始，重复考虑顶点 $i$（以任意顺序）；对于每个顶点，暂时将其从当前社区中移除，并将其插入到能够产生最大非负 $Q(P)$ 增量的社区中，该增量通过为单个顶点计算的精确模块度变化量得出。重复此过程，直到没有单个顶点的移动能增加 $Q(P)$。最终的划分为 $P_{\\mathrm{mod}}$。\n\n2. 一个谱划分 $P_{\\mathrm{spec}}$，将顶点集划分为恰好 $C$ 个组，构建方法如下。令 $D=\\mathrm{diag}(k_1,\\dots,k_N)$，并令对称归一化图拉普拉斯矩阵为\n$$\nL_{\\mathrm{sym}} \\;=\\; I - D^{-1/2} A D^{-1/2}.\n$$\n令 $U \\in \\mathbb{R}^{N \\times C}$ 的列为 $L_{\\mathrm{sym}}$ 的 $C$ 个最小特征值对应的 $C$ 个特征向量。将 $U$ 的行归一化为单位长度。然后，找到一个将这 $N$ 行划分为 $C$ 个簇的划分，该划分最小化每行到其所属簇的质心的平方欧几里得距离之和；这就定义了 $P_{\\mathrm{spec}}$。\n\n对于每个数据集，通过构造提供了已知的真实标签 $y^\\star \\in \\{1,\\dots,C\\}^N$。将一个带有标签 $\\hat{y}\\in\\mathbb{N}^N$ 的预测划分 $P$ 相对于真实标签 $y^\\star$ 的错分类计数定义为在 $\\{1,\\dots,C\\}$ 的所有排列 $\\pi$ 上的最小汉明损失：\n$$\n\\mathrm{err}(P) \\;=\\; \\min_{\\pi} \\sum_{i=1}^N \\mathbf{1}\\{\\pi(\\hat{y}_i) \\neq y^\\star_i\\}.\n$$\n如果预测的组数与 $C$ 不同，则最小化过程在从预测标签集到 $\\{1,\\dots,C\\}$ 的所有单射上进行，任何未被分配的预测标签所对应的样本点都完全计为错分类。\n\n您的任务是使用具有指定参数和固定伪随机种子（以确保可复现性）的多元正态模型，生成三个在 $\\mathbb{R}^2$ 中的合成数据集（解释为单细胞转录组的低维嵌入）。对于每个数据集，使用指定的 $k$ 构建图 $G$，计算 $P_{\\mathrm{mod}}$ 和 $P_{\\mathrm{spec}}$（使用指定的 $C$），并返回一个布尔值，该值指示谱划分的错分类计数是否严格小于模块度优化划分的错分类计数，即 $\\mathrm{err}(P_{\\mathrm{spec}})  \\mathrm{err}(P_{\\mathrm{mod}})$ 是否成立。\n\n数据生成。对于每个数据集，令种群 $a \\in \\{1,2,3\\}$ 贡献 $n_a$ 个点，这些点独立地从均值为 $\\mu_a \\in \\mathbb{R}^2$、对角协方差矩阵为 $\\Sigma_a = \\mathrm{diag}(\\sigma_{a,1}^2,\\sigma_{a,2}^2)$ 的二维正态分布中采样。按 $a=1,2,3$ 的顺序串联这三个种群，并相应地将真实标签 $y^\\star$ 设置为 $1,2,3$。在每个测试用例中，使用由所提供的整数种子初始化的固定伪随机数生成器。\n\n测试套件。使用以下三个数据集，每个数据集的 $C=3$。\n\n- 测试用例1（形状和密度异质性；中等邻域大小）：\n  - 种子 $= 42$。\n  - $(n_1,n_2,n_3) = (450, 380, 25)$。\n  - $\\mu_1 = (0,0)$，$\\Sigma_1 = \\mathrm{diag}(1.2^2, 0.6^2)$。\n  - $\\mu_2 = (6,0)$，$\\Sigma_2 = \\mathrm{diag}(1.0^2, 1.0^2)$。\n  - $\\mu_3 = (1.2,0.1)$，$\\Sigma_3 = \\mathrm{diag}(0.08^2, 0.08^2)$。\n  - $k = 15$。\n\n- 测试用例2（均衡、分离良好的高斯分布）：\n  - 种子 $= 17$。\n  - $(n_1,n_2,n_3) = (200, 200, 200)$。\n  - $\\mu_1 = (-6,0)$，$\\Sigma_1 = \\mathrm{diag}(0.7^2, 0.7^2)$。\n  - $\\mu_2 = (0,0)$，$\\Sigma_2 = \\mathrm{diag}(0.7^2, 0.7^2)$。\n  - $\\mu_3 = (6,0)$，$\\Sigma_3 = \\mathrm{diag}(0.7^2, 0.7^2)$。\n  - $k = 15$。\n\n- 测试用例3（极大邻域大小的边界条件）：\n  - 种子 $= 123$。\n  - $(n_1,n_2,n_3) = (450, 380, 25)$。\n  - $\\mu_1 = (0,0)$，$\\Sigma_1 = \\mathrm{diag}(1.2^2, 0.6^2)$。\n  - $\\mu_2 = (6,0)$，$\\Sigma_2 = \\mathrm{diag}(1.0^2, 1.0^2)$。\n  - $\\mu_3 = (1.2,0.1)$，$\\Sigma_3 = \\mathrm{diag}(0.08^2, 0.08^2)$。\n  - $k = 100$。\n\n最终输出。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”）。对于上述顺序的三个测试用例中的每一个，输出该测试用例的布尔值 $\\mathrm{err}(P_{\\mathrm{spec}})  \\mathrm{err}(P_{\\mathrm{mod}})$。",
            "solution": "该问题是有效的。它提出了一个定义明确的计算任务，植根于计算生物学和网络科学的既定原则。具体来说，它要求在模拟单细胞转录组嵌入的合成数据集上，实现并比较两种标准的基于图的聚类算法——模块度优化和谱聚类。所有参数、方法和评估标准都已足够精确地指定，以允许一个确定性且可复现的解决方案。\n\n对于提供的3个测试用例，解决方案系统地执行以下步骤：\n\n1.  **数据生成**：对于每个测试用例，在 $\\mathbb{R}^2$ 中合成一个包含 $N$ 个点的数据集。总点数 $N$ 是指定种群大小的总和，即 $N = n_1 + n_2 + n_3$。使用由给定种子初始化的伪随机数生成器，为 $C=3$ 个种群中的每一个，从其各自的二元正态分布 $\\mathcal{N}(\\mu_a, \\Sigma_a)$ 中采样点。将得到的点集串联起来，并根据其来源种群分配真实标签 $y^\\star \\in \\{1, 2, 3\\}^N$。在实现中，这些标签被映射到 $\\{0, 1, 2\\}$。\n\n2.  **对称k-近邻图构建**：根据 $N$ 个点构建一个无向、无权图 $G=(V, E)$。首先，计算所有点对之间的欧几里得距离。对于每个点 $i$，识别其 $k$ 个最近邻。如果点 $j$ 是 $i$ 的 $k$ 个最近邻之一，或者 $i$ 是 $j$ 的 $k$ 个最近邻之一，则将边 $\\{i, j\\}$ 添加到边集 $E$ 中。此对称并集规则确保了生成的图是无向的。该图由其邻接矩阵 $A \\in \\{0, 1\\}^{N \\times N}$ 表示。从 $A$ 中，我们为每个顶点 $i$ 计算其度 $k_i = \\sum_j A_{ij}$，以及总度数之和 $2m = \\sum_i k_i$。\n\n3.  **模块度优化 ($P_{\\mathrm{mod}}$)**：该算法寻求图的一个划分，以最大化模块度 $Q$。模块度是一个质量函数，用于衡量社区内部的边密度与具有相同度序列的随机图中期望的边密度的比较。模块度的公式为\n    $$\n    Q(P) = \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\frac{k_i k_j}{2m} \\right) \\mathbf{1}\\{c(i)=c(j)\\}\n    $$\n    其中 $c(i)$ 是顶点 $i$ 的社区。我们实现一个贪婪的迭代算法。初始时，每个顶点都位于其自身的单例社区中。然后，算法重复遍历所有顶点 $i \\in V$。对于每个顶点，它计算将其移动到其每个邻居所在社区所导致的模块度变化 $\\Delta Q$。该顶点被移动到能产生最大正 $\\Delta Q$ 的社区。重复此过程，直到没有单次移动可以增加总模块度，从而得到一个局部最优划分 $P_{\\mathrm{mod}}$。簇的数量由算法自动确定。将顶点 $u$ 从社区 $S$ 移动到社区 $T$ 的模块度变化量可以高效地计算为：\n    $$\n    \\Delta Q = \\left( \\frac{k_{u,T}}{m} - \\frac{k_u \\Sigma_{tot,T}}{2m^2} \\right) - \\left( \\frac{k_{u,S \\setminus \\{u\\}}}{m} - \\frac{k_u (\\Sigma_{tot,S}-k_u)}{2m^2} \\right)\n    $$\n    其中 $k_{u,C}$ 是 $u$ 在社区 $C$ 中的邻居数量，而 $\\Sigma_{tot,C}$ 是社区 $C$ 中所有顶点的度之和。\n\n4.  **谱聚类 ($P_{\\mathrm{spec}}$)**：该方法使用图拉普拉斯矩阵的特征谱来寻找顶点的低维嵌入，该嵌入非常适合聚类。我们使用对称归一化拉普拉斯矩阵，其定义为：\n    $$\n    L_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2}\n    $$\n    其中 $D$ 是顶点度 $k_i$ 的对角矩阵。计算与 $L_{\\mathrm{sym}}$ 的 $C$ 个最小特征值相对应的 $C=3$ 个特征向量。这些特征向量构成一个矩阵 $U \\in \\mathbb{R}^{N \\times C}$ 的列。$U$ 的行在 $C$ 维空间中为每个顶点提供了一个新的表示。将 $U$ 的每一行归一化为单位欧几里得长度后，我们对这 $N$ 个行向量应用k-均值聚类，将它们划分为 $C$ 个组。k-均值的初始质心被确定性地选择以确保可复现性。这样就得到了划分 $P_{\\mathrm{spec}}$。\n\n5.  **误差计算与比较**：对于 $P_{\\mathrm{mod}}$ 和 $P_{\\mathrm{spec}}$ 这两个划分，我们计算它们相对于真实标签 $y^\\star$ 的错分类计数。由于聚类标签是任意的，误差被定义为在预测标签集和真实标签集之间所有可能匹配上的最小汉明距离。这是一个经典的分配问题，我们使用匈牙利算法来解决，该算法可在 `scipy.optimize.linear_sum_assignment` 中获得。我们构建预测标签和真实标签之间的混淆矩阵，并找到能够最大化正确分类样本数的标签映射。误差等于总样本数减去这个最大值。每个测试用例的最终输出是一个布尔值，指示谱划分是否严格更优：$\\mathrm{err}(P_{\\mathrm{spec}})  \\mathrm{err}(P_{\\mathrm{mod}})$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import cdist\nfrom scipy.cluster.vq import kmeans2\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the full pipeline for all test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (shape and density heterogeneity; moderate neighborhood size)\n        {\n            'seed': 42,\n            'n_samples': (450, 380, 25),\n            'means': [(0, 0), (6, 0), (1.2, 0.1)],\n            'covs': [[1.2**2, 0], [0, 0.6**2]],\n            'covs_list': [np.diag([1.2**2, 0.6**2]), np.diag([1.0**2, 1.0**2]), np.diag([0.08**2, 0.08**2])],\n            'k': 15,\n            'C': 3\n        },\n        # Test case 2 (balanced, well-separated Gaussians)\n        {\n            'seed': 17,\n            'n_samples': (200, 200, 200),\n            'means': [(-6, 0), (0, 0), (6, 0)],\n            'covs_list': [np.diag([0.7**2, 0.7**2]), np.diag([0.7**2, 0.7**2]), np.diag([0.7**2, 0.7**2])],\n            'k': 15,\n            'C': 3\n        },\n        # Test case 3 (boundary condition with very large neighborhood size)\n        {\n            'seed': 123,\n            'n_samples': (450, 380, 25),\n            'means': [(0, 0), (6, 0), (1.2, 0.1)],\n            'covs_list': [np.diag([1.2**2, 0.6**2]), np.diag([1.0**2, 1.0**2]), np.diag([0.08**2, 0.08**2])],\n            'k': 100,\n            'C': 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_single_case(case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(n_samples, means, covs_list, seed):\n    \"\"\"Generates synthetic data from a mixture of Gaussians.\"\"\"\n    rng = np.random.default_rng(seed)\n    points = []\n    labels = []\n    for i, (n, mean, cov) in enumerate(zip(n_samples, means, covs_list)):\n        points.append(rng.multivariate_normal(mean, cov, n))\n        labels.extend([i] * n)\n    X = np.vstack(points)\n    y_true = np.array(labels)\n    return X, y_true\n\ndef _build_knn_graph(X, k):\n    \"\"\"Constructs a symmetric k-NN graph.\"\"\"\n    N = X.shape[0]\n    dist_matrix = cdist(X, X, 'euclidean')\n    \n    # Get indices of k-nearest neighbors for each point\n    neighbors = np.argsort(dist_matrix, axis=1)[:, 1:k+1]\n    \n    A = np.zeros((N, N), dtype=int)\n    for i in range(N):\n        A[i, neighbors[i]] = 1\n        \n    # Symmetrize the adjacency matrix\n    A = np.maximum(A, A.T)\n    return A\n\ndef _modularity_clustering(A):\n    \"\"\"Performs modularity-based greedy clustering.\"\"\"\n    N = A.shape[0]\n    degrees = A.sum(axis=1)\n    m = degrees.sum() / 2.0\n    \n    if m == 0:\n        return np.arange(N)\n\n    # Initial state: each node in its own community\n    communities = np.arange(N)\n    comm_degrees = degrees.copy().astype(float)\n    \n    # Store neighbor lists to avoid repeated lookups\n    adj_list = [np.where(row > 0)[0] for row in A]\n\n    while True:\n        moved = False\n        for u in range(N):\n            old_comm = communities[u]\n            k_u = degrees[u]\n            \n            # Gain from removing u from its old community\n            sigma_old = comm_degrees[old_comm]\n            k_u_in_old = np.sum(A[u, adj_list[u]][communities[adj_list[u]] == old_comm])\n            loss_remove = k_u_in_old / m - k_u * (sigma_old - k_u) / (2 * m**2)\n            \n            best_comm = old_comm\n            max_gain = 0.0\n\n            # Consider moving to neighbors' communities\n            neighbor_comms = set(communities[v] for v in adj_list[u])\n            for new_comm in neighbor_comms:\n                if new_comm == old_comm:\n                    continue\n                \n                sigma_new = comm_degrees[new_comm]\n                k_u_in_new = np.sum(A[u, adj_list[u]][communities[adj_list[u]] == new_comm])\n                gain_add = k_u_in_new / m - k_u * sigma_new / (2 * m**2)\n                \n                delta_q = gain_add - loss_remove\n                if delta_q > max_gain:\n                    max_gain = delta_q\n                    best_comm = new_comm\n\n            if best_comm != old_comm and max_gain > 1e-9: # Use tolerance for float comparison\n                # Move u to the best new community\n                comm_degrees[old_comm] -= k_u\n                comm_degrees[best_comm] += k_u\n                communities[u] = best_comm\n                moved = True\n        \n        if not moved:\n            break\n            \n    # Relabel communities to be contiguous integers starting from 0\n    unique_labels, contiguous_labels = np.unique(communities, return_inverse=True)\n    return contiguous_labels\n\ndef _spectral_clustering(A, C):\n    \"\"\"Performs spectral clustering.\"\"\"\n    N = A.shape[0]\n    degrees = A.sum(axis=1).astype(float)\n    \n    # Handle isolated nodes\n    isolated_nodes = degrees == 0\n    if np.all(isolated_nodes):\n        return np.arange(N) # All isolated, spectral clustering undefined\n\n    # Construct symmetric normalized Laplacian\n    inv_sqrt_degrees = np.zeros_like(degrees)\n    np.divide(1.0, np.sqrt(degrees), out=inv_sqrt_degrees, where=degrees > 0)\n    D_inv_sqrt = np.diag(inv_sqrt_degrees)\n    L_sym = np.identity(N) - D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Eigen-decomposition\n    eigvals, eigvecs = eigh(L_sym)\n    \n    # Form U from the first C eigenvectors\n    U = eigvecs[:, :C]\n\n    # Normalize rows of U\n    norms = np.linalg.norm(U, axis=1, keepdims=True)\n    U_norm = np.zeros_like(U)\n    np.divide(U, norms, out=U_norm, where=norms > 0)\n\n    # K-means on the rows of normalized U, with deterministic initialization\n    centroids, labels = kmeans2(U_norm, C, minit='points', missing='raise')\n    \n    return labels\n\ndef _calculate_error(y_true, y_pred):\n    \"\"\"Calculates the misclassification error using the Hungarian algorithm.\"\"\"\n    N = len(y_true)\n    true_labels = np.unique(y_true)\n    pred_labels = np.unique(y_pred)\n    \n    cost_matrix = np.zeros((len(pred_labels), len(true_labels)))\n    for i, pl in enumerate(pred_labels):\n        for j, tl in enumerate(true_labels):\n            cost_matrix[i, j] = np.sum((y_pred == pl)  (y_true == tl))\n            \n    # Use linear_sum_assignment to find the best mapping (max weight matching)\n    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n    \n    num_correct = cost_matrix[row_ind, col_ind].sum()\n    error = N - num_correct\n    return error\n\ndef _solve_single_case(case):\n    \"\"\"Solves a single test case from generation to comparison.\"\"\"\n    X, y_true = _generate_data(case['n_samples'], case['means'], case['covs_list'], case['seed'])\n    N = X.shape[0]\n\n    # Build k-NN graph\n    A = _build_knn_graph(X, case['k'])\n\n    # Get modularity partition\n    y_mod = _modularity_clustering(A)\n\n    # Get spectral partition\n    y_spec = _spectral_clustering(A, case['C'])\n\n    # Calculate errors\n    err_mod = _calculate_error(y_true, y_mod)\n    err_spec = _calculate_error(y_true, y_spec)\n\n    return err_spec  err_mod\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在应用任何聚类算法之前，对原始计数数据进行预处理是至关重要的一步。对数转换，如 $\\ln(X+\\alpha)$，是稳定方差和压缩数据范围的常用技术，但其中伪计数 $\\alpha$ 的选择往往被视为一个次要参数。本练习将通过一个精心设计的思想实验，揭示 $\\alpha$ 的选择对于稀疏（尤其是近乎二元）的单细胞数据所产生的深刻数学影响。通过这个实践，您将学会审视预处理步骤背后的理论假设，理解这些“小”决策如何塑造最终的聚类结果。",
            "id": "2379581",
            "problem": "给定一个用于非负整数计数矩阵的逐元素变换族，该变换族用于模拟单细胞信使核糖核酸（mRNA）转录本计数。对于任意伪计数参数 $\\alpha \\in \\mathbb{R}_{0}$，该变换定义为 $T_{\\alpha}(X) = \\log(X + \\alpha)$，其中 $\\log$ 表示自然对数，$X$ 是一个计数矩阵。对于任意两个经过变换的细胞-基因矩阵 $T_{\\alpha}(C) \\in \\mathbb{R}^{n \\times d}$，将细胞间的成对相异性定义为其对应行之间的欧几里得距离。对于一个固定的正整数 $K$，定义聚类 $\\mathcal{K}_{\\alpha}(C,K)$ 为：对 $T_{\\alpha}(C)$ 的完整成对欧几里得距离矩阵应用采用平均链接法的凝聚式层次聚类，然后切割生成的树状图以恰好产生 $K$ 个簇，从而将 $n$ 个细胞划分为恰好 $K$ 个非空簇的划分。对于相同 $n$ 个项目的任意两个划分 $U$ 和 $V$，定义它们之间的调整兰德指数（ARI）为\n$$\n\\mathrm{ARI}(U,V) \\;=\\; \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} \\;-\\; \\frac{\\left(\\sum_{i} \\binom{a_i}{2}\\right)\\left(\\sum_{j} \\binom{b_j}{2}\\right)}{\\binom{n}{2}}}{\\tfrac{1}{2}\\left(\\sum_{i} \\binom{a_i}{2} + \\sum_{j} \\binom{b_j}{2}\\right) \\;-\\; \\frac{\\left(\\sum_{i} \\binom{a_i}{2}\\right)\\left(\\sum_{j} \\binom{b_j}{2}\\right)}{\\binom{n}{2}}},\n$$\n其中 $n_{ij}$ 是在划分 $U$ 的簇 $i$ 和划分 $V$ 的簇 $j$ 中的项目数，$a_i = \\sum_j n_{ij}$，$b_j = \\sum_i n_{ij}$，且对于任意整数 $m \\ge 0$，$\\binom{m}{2} = \\frac{m(m-1)}{2}$。在任何分母为 $0$ 且分子也为 $0$ 的退化情况下，定义 $\\mathrm{ARI}(U,V)=1$。\n\n通过计算下面每个测试用例中 $\\mathcal{K}_{\\alpha_1}(C,K)$ 和 $\\mathcal{K}_{\\alpha_2}(C,K)$ 之间的调整兰德指数，来研究不同伪计数值对最终聚类的影响，其中 $\\alpha_1 = 1$ 和 $\\alpha_2 = 0.01$。\n\n测试套件规范。对于每个用例，$C$ 是一个 $n \\times d$ 矩阵，其各行按顺序列出；所有条目均为非负整数。每行显示一个行（细胞），形式为一个包含 $d$ 个整数的列表。\n\n- 测试用例 1：$C^{(1)} \\in \\mathbb{N}_0^{8 \\times 6}$，$K=2$，各行按顺序为：\n  $[\\,1,1,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0\\,]$,\n  $[\\,1,1,1,0,0,0\\,]$,\n  $[\\,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,1,1,1\\,]$,\n  $[\\,0,0,0,0,1,1\\,]$.\n- 测试用例 2：$C^{(2)} \\in \\mathbb{N}_0^{6 \\times 5}$，$K=2$，各行按顺序为：\n  $[\\,0,0,0,0,0\\,]$,\n  $[\\,1,0,0,0,0\\,]$,\n  $[\\,1,0,0,0,0\\,]$,\n  $[\\,0,1,0,0,0\\,]$,\n  $[\\,0,1,0,0,0\\,]$,\n  $[\\,0,0,1,0,0\\,]$.\n- 测试用例 3：$C^{(3)} \\in \\mathbb{N}_0^{9 \\times 7}$，$K=3$，各行按顺序为：\n  $[\\,1,1,0,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0,0\\,]$,\n  $[\\,1,1,0,0,0,0,0\\,]$,\n  $[\\,0,0,1,1,0,0,0\\,]$,\n  $[\\,0,0,1,1,0,0,0\\,]$,\n  $[\\,0,0,1,1,0,0,0\\,]$,\n  $[\\,0,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,0,0,1,1\\,]$,\n  $[\\,0,0,0,0,0,1,1\\,]$.\n- 测试用例 4：$C^{(4)} \\in \\mathbb{N}_0^{5 \\times 4}$，$K=2$，各行按顺序为：\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$,\n  $[\\,0,0,0,0\\,]$.\n\n您的程序必须为每个测试用例 $t \\in \\{1,2,3,4\\}$ 计算单个实数 $\\mathrm{ARI}\\!\\left(\\mathcal{K}_{\\alpha_1}\\!\\left(C^{(t)},K\\right), \\mathcal{K}_{\\alpha_2}\\!\\left(C^{(t)},K\\right)\\right)$，并将四个结果汇总到一行输出中。最终输出格式必须是包含四个结果的单行，形式为 Python 风格的列表，其中每个结果都是小数点后恰好保留 $6$ 位的小数，并按测试用例的顺序排列，例如 $[\\,a_1,a_2,a_3,a_4\\,]$，其中每个 $a_t$ 是测试用例 $t$ 的舍入值。不应打印任何其他文本。",
            "solution": "该问题要求研究伪计数参数 $\\alpha$ 对应用于单细胞计数数据的特定聚类流程结果的影响。该流程包括对数变换、成对欧几里得距离的计算以及采用平均链接法的凝聚式层次聚类。使用不同 $\\alpha$ 值获得的聚类之间的比较将通过调整兰德指数（ARI）进行量化。\n\n在进行计算之前，必须对问题进行理论分析。让我们将各个组成部分形式化。\n\n数据为一个计数矩阵 $C \\in \\mathbb{N}_0^{n \\times d}$，其中 $n$ 是细胞数，$d$ 是基因数。变换由 $T_{\\alpha}(C)$ 给出，其中 $C$ 的每个元素 $x_{ij}$ 被替换为 $\\log(x_{ij} + \\alpha)$，对于给定的 $\\alpha  0$。设 $u, v \\in \\mathbb{N}_0^d$ 为两个不同细胞（$C$ 的行）的计数向量。变换后，它们的新表示为 $u' = (\\log(u_1+\\alpha), \\dots, \\log(u_d+\\alpha))$ 和 $v' = (\\log(v_1+\\alpha), \\dots, \\log(v_d+\\alpha))$。\n\n相异性是这些变换后向量之间的欧几里得距离：\n$$\nd_{\\alpha}(u,v) = \\sqrt{\\sum_{j=1}^{d} (\\log(u_j+\\alpha) - \\log(v_j+\\alpha))^2} = \\sqrt{\\sum_{j=1}^{d} \\left(\\log\\left(\\frac{u_j+\\alpha}{v_j+\\alpha}\\right)\\right)^2}\n$$\n\n必须对所有四个测试用例中提供的数据做出一个关键的观察。计数矩阵 $C^{(1)}, C^{(2)}, C^{(3)}, C^{(4)}$ 只包含二进制值，即对于所有 $i,j$，条目 $c_{ij} \\in \\{0, 1\\}$。这极大地简化了分析。\n\n对于任意两个二进制向量 $u, v \\in \\{0, 1\\}^d$，让我们分析距离平方 $d_{\\alpha}(u,v)^2$ 求和中的各项。对于每个基因 $j \\in \\{1, \\dots, d\\}$，对 $(u_j, v_j)$ 有四种可能性：\n1. 如果 $u_j = 0$ 且 $v_j = 0$，对和的贡献是 $\\left(\\log\\left(\\frac{0+\\alpha}{0+\\alpha}\\right)\\right)^2 = (\\log(1))^2 = 0$。\n2. 如果 $u_j = 1$ 且 $v_j = 1$，贡献是 $\\left(\\log\\left(\\frac{1+\\alpha}{1+\\alpha}\\right)\\right)^2 = (\\log(1))^2 = 0$。\n3. 如果 $u_j = 1$ 且 $v_j = 0$，贡献是 $\\left(\\log\\left(\\frac{1+\\alpha}{0+\\alpha}\\right)\\right)^2 = \\left(\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2$。\n4. 如果 $u_j = 0$ 且 $v_j = 1$，贡献是 $\\left(\\log\\left(\\frac{0+\\alpha}{1+\\alpha}\\right)\\right)^2 = \\left(-\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2 = \\left(\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2$。\n\n总距离平方是这些贡献的总和。唯一的非零贡献来自于计数不同的基因。设 $H(u,v)$ 为向量 $u$ 和 $v$ 之间的汉明距离，即 $u_j \\neq v_j$ 的位置 $j$ 的数量。因此，欧几里得距离的平方是：\n$$\nd_{\\alpha}(u,v)^2 = H(u,v) \\cdot \\left(\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right)^2\n$$\n取平方根，距离为：\n$$\nd_{\\alpha}(u,v) = \\sqrt{H(u,v)} \\cdot \\left|\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)\\right|\n$$\n因为 $\\alpha  0$，所以 $1+\\alpha  \\alpha$，因此 $\\frac{1+\\alpha}{\\alpha}  1$，且 $\\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)  0$。我们定义一个缩放因子 $S_{\\alpha} = \\log\\left(\\frac{1+\\alpha}{\\alpha}\\right)$。则距离为 $d_{\\alpha}(u,v) = S_{\\alpha} \\sqrt{H(u,v)}$。\n\n对于给定的 $\\alpha$，整个成对距离矩阵就是汉明距离平方根的矩阵，再乘以一个常数因子 $S_{\\alpha}$。\n\n聚类算法是采用平均链接法的凝聚式层次聚类。在每一步，该算法合并两个最接近的簇。两个簇 $C_A$ 和 $C_B$ 之间的距离定义为其组成点之间所有成对距离的平均值：\n$$\nD_{\\alpha}(C_A, C_B) = \\frac{1}{|C_A||C_B|} \\sum_{u \\in C_A, v \\in C_B} d_{\\alpha}(u,v)\n$$\n代入我们对 $d_{\\alpha}(u,v)$ 的表达式：\n$$\nD_{\\alpha}(C_A, C_B) = \\frac{1}{|C_A||C_B|} \\sum_{u \\in C_A, v \\in C_B} S_{\\alpha} \\sqrt{H(u,v)} = S_{\\alpha} \\left( \\frac{1}{|C_A||C_B|} \\sum_{u \\in C_A, v \\in C_B} \\sqrt{H(u,v)} \\right)\n$$\n括号中的项与 $\\alpha$ 无关。因此，任意两个簇之间的距离也按相同的正常数 $S_{\\alpha}$ 进行缩放。\n\n层次聚类算法通过比较簇间距离来找到最小值。如果我们比较两对簇之间的距离，比如 $(C_A, C_B)$ 和 $(C_X, C_Y)$，我们有：\n$$\nD_{\\alpha}(C_A, C_B)  D_{\\alpha}(C_X, C_Y) \\iff S_{\\alpha} \\cdot (\\text{项}_1)  S_{\\alpha} \\cdot (\\text{项}_2)\n$$\n因为 $S_{\\alpha}  0$，这等价于 $(\\text{项}_1)  (\\text{项}_2)$。因此，在任何给定步骤中合并哪些簇的决策都与 $\\alpha$ 的值无关。\n\n这意味着，只要输入数据是二进制的，合并的顺序以及因此生成的树状图的整个结构对于任何 $\\alpha  0$ 的选择都是相同的。无论 $\\alpha$ 如何，切割树状图以产生恰好 $K$ 个簇都将产生相同的细胞划分。\n\n因此，对于每个测试用例 $t \\in \\{1,2,3,4\\}$，由于输入矩阵 $C^{(t)}$ 是二进制的，所获得的划分是相同的：\n$$\n\\mathcal{K}_{\\alpha_1}(C^{(t)}, K) = \\mathcal{K}_{\\alpha_2}(C^{(t)}, K)\n$$\n其中 $\\alpha_1 = 1$ 和 $\\alpha_2 = 0.01$。\n\n最后一步是计算两个相同划分之间的调整兰德指数。设两个划分为 $U$ 和 $V$，且 $U=V$。在这种情况下，列联表 $n_{ij}$ 变成一个对角矩阵（在适当排序簇之后）。ARI 公式简化后得到值为 $1$，表示完全一致。具体来说，$\\sum_{i,j} \\binom{n_{ij}}{2} = \\sum_i \\binom{a_i}{2}$ 且 $\\sum_i \\binom{a_i}{2} = \\sum_j \\binom{b_j}{2}$。这导致 ARI 公式的分子和分母相等（并且非零，除非划分是平凡的），从而得到 $\\mathrm{ARI}(U,U) = 1$。在所有簇都是单元素簇的平凡情况下，分子和分母都变为 $0$，问题陈述中指定在这种情况下定义 $\\mathrm{ARI}=1$。\n\n结论：对于所有四个给定的测试用例，调整兰德指数的理论结果都恰好是 1。下面的计算解决方案将通过显式执行所有必需的步骤来验证这一推论。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, fcluster\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases as specified.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"C\": np.array([\n                [1, 1, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0],\n                [1, 1, 1, 0, 0, 0],\n                [0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 1, 1, 1],\n                [0, 0, 0, 0, 1, 1]\n            ]),\n            \"K\": 2\n        },\n        {\n            \"C\": np.array([\n                [0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0],\n                [0, 1, 0, 0, 0],\n                [0, 1, 0, 0, 0],\n                [0, 0, 1, 0, 0]\n            ]),\n            \"K\": 2\n        },\n        {\n            \"C\": np.array([\n                [1, 1, 0, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0, 0],\n                [1, 1, 0, 0, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0],\n                [0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 1, 1]\n            ]),\n            \"K\": 3\n        },\n        {\n            \"C\": np.array([\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0]\n            ]),\n            \"K\": 2\n        }\n    ]\n\n    alpha1 = 1.0\n    alpha2 = 0.01\n\n    results = []\n    for case in test_cases:\n        C = case[\"C\"]\n        K = case[\"K\"]\n\n        # Get clustering for alpha1\n        labels1 = get_clustering(C, alpha1, K)\n        \n        # Get clustering for alpha2\n        labels2 = get_clustering(C, alpha2, K)\n        \n        # Compute ARI\n        ari = adjusted_rand_index(labels1, labels2)\n        results.append(ari)\n\n    # Format output as specified\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_clustering(C, alpha, K):\n    \"\"\"\n    Performs the specified clustering pipeline for a given count matrix,\n    pseudo-count, and number of clusters.\n    \"\"\"\n    # 1. Transform data: T_alpha(C) = log(C + alpha)\n    transformed_C = np.log(C + alpha)\n    \n    # 2. Compute pairwise Euclidean distance matrix\n    # pdist computes a condensed distance matrix (upper triangle)\n    dist_matrix = pdist(transformed_C, 'euclidean')\n    \n    # 3. Perform agglomerative hierarchical clustering with average linkage\n    # 'average' linkage corresponds to UPGMA\n    linkage_matrix = linkage(dist_matrix, method='average')\n    \n    # 4. Cut dendrogram to get K clusters\n    # 'maxclust' criterion forms flat clusters from the linkage matrix\n    cluster_labels = fcluster(linkage_matrix, K, criterion='maxclust')\n    \n    return cluster_labels\n\ndef comb2(n):\n    \"\"\"\n    Computes nC2 = n * (n - 1) / 2.\n    Handles arrays element-wise.\n    \"\"\"\n    # Ensure input is an array of integers and handle n  2 case\n    n = np.asarray(n, dtype=np.int64)\n    return n * (n - 1) / 2\n\ndef adjusted_rand_index(labels_true, labels_pred):\n    \"\"\"\n    Computes the Adjusted Rand Index from scratch based on the provided formula.\n    \"\"\"\n    n = len(labels_true)\n    if n  2:\n        return 1.0\n\n    # Create contingency table\n    unique_labels_true = np.unique(labels_true)\n    unique_labels_pred = np.unique(labels_pred)\n    \n    contingency_table = np.zeros((len(unique_labels_true), len(unique_labels_pred)), dtype=int)\n    \n    true_map = {label: i for i, label in enumerate(unique_labels_true)}\n    pred_map = {label: i for i, label in enumerate(unique_labels_pred)}\n\n    for i in range(n):\n        true_idx = true_map[labels_true[i]]\n        pred_idx = pred_map[labels_pred[i]]\n        contingency_table[true_idx, pred_idx] += 1\n    \n    # Sum of n_ij choose 2\n    sum_nij_c2 = np.sum(comb2(contingency_table))\n\n    # Sums of row totals and column totals choose 2\n    a = np.sum(contingency_table, axis=1)\n    b = np.sum(contingency_table, axis=0)\n    \n    sum_a_c2 = np.sum(comb2(a))\n    sum_b_c2 = np.sum(comb2(b))\n\n    # Total number of pairs\n    n_c2 = comb2(n)\n    \n    # Calculate expected index\n    # Handle case where n  2, so n_c2 is 0\n    if n_c2 == 0:\n        expected_index = 0\n    else:\n        expected_index = (sum_a_c2 * sum_b_c2) / n_c2\n\n    # Calculate max index\n    max_index = 0.5 * (sum_a_c2 + sum_b_c2)\n    \n    # Calculate ARI\n    numerator = sum_nij_c2 - expected_index\n    denominator = max_index - expected_index\n    \n    if numerator == 0 and denominator == 0:\n        # Per problem specification for this degenerate case\n        return 1.0\n    elif denominator == 0:\n        # This implies no agreement above chance\n        return 0.0\n    else:\n        return numerator / denominator\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "计算聚类算法产生的初始结果往往无法完美对应真实的生物细胞类型，一个常见的问题是“过度聚类”，即将一个同质的细胞群落错误地分割成多个亚群。因此，一个关键的后续步骤是基于生物学知识对这些簇进行评估和修正。本练习将指导您设计并实现一个严谨的自动化流程，通过统计检验来判断簇之间是否缺乏显著的差异表达基因（标记基因），从而合并过度聚类的细胞群。这个实践将计算几何分组与生物学解释联系起来，是连接计算与生物洞察的关键一步。",
            "id": "2379646",
            "problem": "给定一个基因-细胞计数矩阵和一组用于单细胞核糖核酸测序 (scRNA-seq) 转录组的初始聚类标签。目标是设计并实现一个有原则的程序，通过检验成对的聚类是否缺乏统计上显著的分离标记基因，来自动合并过度聚类的群体。该方法必须基于单细胞转录组学中常用的核心定义和经过充分检验的统计程序。\n\n从以下基础开始：\n- 一个细胞-基因计数矩阵是一个形状为 $n \\times g$ 的非负整数数组，其中 $n$ 是细胞数量，$g$ 是基因数量。将该矩阵表示为 $X \\in \\mathbb{N}_{0}^{n \\times g}$，其条目 $X_{i j}$ 代表细胞 $i$ 和基因 $j$ 的计数值。\n- 文库大小归一化对每个细胞进行重新缩放，使得每个细胞的总计数值具有可比性。一种标准方法是为每个细胞 $i$ 计算总计数值 $s_{i} = \\sum_{j=1}^{g} X_{i j}$，然后将计数值转换为每 $10^{4}$ 个计数值的单位，再进行自然对数稳定化，即定义\n$$\n\\tilde{X}_{i j} = \\log\\left(1 + \\frac{X_{i j}}{s_{i}} \\cdot 10^{4}\\right),\n$$\n对于所有细胞 $i \\in \\{1,\\dots,n\\}$ 和基因 $j \\in \\{1,\\dots,g\\}$，其中 $\\log$ 表示自然对数。\n- 对于两个聚类 $a$ 和 $b$，设 $A$ 是标签为 $a$ 的细胞索引集，$B$ 是标签为 $b$ 的细胞索引集。对每个基因 $j \\in \\{1,\\dots,g\\}$，考虑对样本 $\\{\\tilde{X}_{i j} : i \\in A\\}$ 和 $\\{\\tilde{X}_{i j} : i \\in B\\}$ 应用双边 Mann–Whitney $U$ 检验（也称为 Wilcoxon 秩和检验），得到一个 p 值 $p_{j} \\in [0,1]$。设每个基因的绝对 log-倍数变化为\n$$\nL_{j} = \\left|\\frac{1}{|A|} \\sum_{i \\in A} \\tilde{X}_{i j} - \\frac{1}{|B|} \\sum_{i \\in B} \\tilde{X}_{i j}\\right|.\n$$\n- 使用 Benjamini–Hochberg 假发现率 (FDR) 步升程序来控制对 $g$ 个基因进行多重检验的问题。给定向量 $(p_{1},\\dots,p_{g})$，将其按非降序排序得到 $p_{(1)} \\le \\cdots \\le p_{(g)}$，然后计算\n$$\nq_{(k)} = \\min_{t \\in \\{k,\\dots,g\\}} \\left\\{ \\frac{g}{t} \\, p_{(t)} \\right\\},\n$$\n再进行逆排序以获得校正后的 q 值 $(q_{1},\\dots,q_{g})$。\n- 在固定阈值 $\\alpha \\in (0,1)$、$\\tau  0$ 和 $m_{\\min} \\in \\mathbb{N}$ 下定义一个决策规则：如果同时满足 $q_{j} \\le \\alpha$ 和 $L_{j} \\ge \\tau$ 的基因数量严格小于 $m_{\\min}$，则聚类对 $(a,b)$ 被宣告为不可区分。等价地，如果\n$$\nS(a,b) = \\left| \\left\\{ j \\in \\{1,\\dots,g\\} : q_{j} \\le \\alpha \\;\\wedge\\; L_{j} \\ge \\tau \\right\\} \\right|  m_{\\min},\n$$\n则聚类 $a$ 和 $b$ 将被合并。\n\n算法要求：\n- 构建一个无向图，其节点是当前的聚类标识。当且仅当 $S(a,b)  m_{\\min}$ 时，在不同的聚类 $a$ 和 $b$ 之间添加一条边。用该图的连通分量替换聚类集合（将每个连通分量合并成一个单一的聚类）。迭代此过程，直到某次迭代中没有添加任何边为止（即达到不动点）。最后，在扫描细胞 $i=1$ 到 $n$ 时，按照首次出现的顺序将聚类标签压缩为从 0 开始的连续整数。\n\n对所有测试用例使用以下固定的超参数：\n- 显著性水平 $\\alpha = 0.01$，\n- Log-倍数变化阈值 $\\tau = 0.25$，\n- 最小标记基因数 $m_{\\min} = 2$。\n\n程序输入规范：\n- 将下方的测试套件直接硬编码在程序中。不从标准输入或文件中读取任何输入。\n\n测试套件：\n- 测试用例 1（过度聚类的正常路径）：$g = 5$ 个基因，$n = 12$ 个细胞。矩阵 $X \\in \\mathbb{N}_{0}^{12 \\times 5}$ 按行定义如下：\n  - 第 1 到 4 行：$[50, 100, 80, 30, 20]$，\n  - 第 5 到 7 行：$[50, 100, 80, 30, 20]$，\n  - 第 8 到 12 行：$[50, 100, 80, 300, 200]$。\n  初始标签向量为 $L = [0,0,0,0,1,1,1,2,2,2,2,2]$。标签为 0 和 1 的聚类在组成上是相同的，应该合并；聚类 2 是不同的。\n- 测试用例 2（良好分离的聚类）：$g = 5$，$n = 10$。矩阵 $X \\in \\mathbb{N}_{0}^{10 \\times 5}$ 为：\n  - 第 1 到 5 行：$[5, 5, 5, 5, 30]$，\n  - 第 6 到 10 行：$[30, 5, 5, 5, 5]$。\n  初始标签 $L = [0,0,0,0,0,1,1,1,1,1]$。这两个聚类在至少 2 个基因上存在差异，不应合并。\n- 测试用例 3（包含单个体聚类和有限标记基因的边界情况）：$g = 5$，$n = 5$。矩阵 $X \\in \\mathbb{N}_{0}^{5 \\times 5}$ 为：\n  - 第 1 到 4 行：$[20, 20, 20, 20, 20]$，\n  - 第 5 行：$[20, 20, 50, 20, 20]$。\n  初始标签 $L = [0,0,0,0,1]$。这模拟了一个小的、过度分裂的单个体，仅在 1 个基因上存在差异；由于 $m_{\\min} = 2$，它应该被合并回去。\n\n输出规格：\n- 对于每个测试用例，程序必须输出一个包含两个元素的列表：收敛后的最终聚类数量，以及长度为 $n$ 的最终压缩标签向量。\n- 将所有三个测试用例的结果汇总到一行，形式为用方括号括起来的逗号分隔列表。具体来说，输出必须是表示\n$$\n\\left[ [K_{1}, \\text{labels}_{1}], [K_{2}, \\text{labels}_{2}], [K_{3}, \\text{labels}_{3}] \\right],\n$$\n的单行文本，其中 $K_{t} \\in \\mathbb{N}$ 且 $\\text{labels}_{t}$ 是测试用例 $t \\in \\{1,2,3\\}$ 的一个包含 $n_{t}$ 个整数的列表。列表内的整数打印时不得有空格。例如，一个语法正确的输出可能看起来像 $[[2,[0,0,0]],[2,[0,1]],[1,[0]]]$ （这只是格式示例，不是预期的答案）。",
            "solution": "我们通过将聚类间缺乏分离标记基因的情况形式化为一个在受控的假发现率 (FDR) 下无法被拒绝的原假设，为过度聚类的单细胞核糖核酸测序 (scRNA-seq) 数据集设计了一个有统计学原则的合并程序。推导从转录组学中的核心定义和广泛使用的统计工具开始。\n\n预处理与归一化。每个观测到的计数值 $X_{i j}$ 都受到细胞基因表达程序和诸如文库大小等技术因素的影响。一种广为接受的一阶校正方法是文库大小归一化结合方差稳定变换。对于每个细胞 $i$，计算文库大小 $s_{i} = \\sum_{j=1}^{g} X_{i j}$。然后，连续尺度上的归一化表达为\n$$\n\\tilde{X}_{i j} = \\log\\left(1 + \\frac{X_{i j}}{s_{i}} \\cdot 10^{4}\\right).\n$$\n此过程将所有细胞重新缩放到一个共同的有效深度，即总计数值为 $10^{4}$，并应用自然对数来减弱均值-方差关系，这是单细胞分析中一种经过充分检验的做法。\n\n检验分离标记基因。考虑两个聚类 $a$ 和 $b$，其索引集分别为 $A$ 和 $B$。对于每个基因 $j \\in \\{1,\\dots,g\\}$，我们需要一个检验来检测 $A$ 和 $B$ 之间 $\\tilde{X}_{i j}$ 集中趋势的差异。由于 scRNA-seq 归一化数据通常是非高斯的，并且可能存在秩次相同的情况，我们采用双边 Mann–Whitney $U$ 检验（Wilcoxon 秩和检验），该检验测试\n$$\nH_{0}: \\{\\tilde{X}_{i j}\\}_{i \\in A} \\text{ 和 } \\{\\tilde{X}_{i j}\\}_{i \\in B} \\text{ 的分布相等}\n$$\n与\n$$\nH_{1}: \\text{分布在位置上存在差异}。\n$$\n这为每个基因 $j$ 产生一个 p 值 $p_{j} \\in [0,1]$。\n\n多重检验校正。由于我们对 $g$ 个基因进行检验，我们使用 Benjamini–Hochberg (BH) 步升 FDR 程序来控制假发现的预期比例。给定 p 值 $(p_{1},\\dots,p_{g})$，将其排序得到 $p_{(1)} \\le \\cdots \\le p_{(g)}$。通过以下方式定义 BH 校正值\n$$\nq_{(k)} = \\min_{t \\in \\{k,\\dots,g\\}} \\left\\{ \\frac{g}{t} \\, p_{(t)} \\right\\},\n$$\n然后映射回原始基因顺序以获得 $(q_{1},\\dots,q_{g})$。这会产生 q 值，在标准的独立性或正相关性假设下，该 q 值可以控制发现中的假阳性预期比例。\n\n效应量阈值。为避免合并那些在统计上可检测到但实际上差异可忽略不计的聚类，我们通过绝对 log-倍数变化引入一个效应量约束\n$$\nL_{j} = \\left|\\mu_{A j} - \\mu_{B j}\\right|, \\quad \\text{其中 } \\mu_{A j} = \\frac{1}{|A|} \\sum_{i \\in A} \\tilde{X}_{i j}, \\;\\; \\mu_{B j} = \\frac{1}{|B|} \\sum_{i \\in B} \\tilde{X}_{i j}.\n$$\n一个基因仅当同时满足 $q_{j} \\le \\alpha$ 和 $L_{j} \\ge \\tau$ 时才被视为分离标记基因，其中 $\\alpha$ 和 $\\tau$ 是固定阈值。分离标记基因的数量为\n$$\nS(a,b) = \\left| \\left\\{ j \\in \\{1,\\dots,g\\} : q_{j} \\le \\alpha \\;\\wedge\\; L_{j} \\ge \\tau \\right\\} \\right|.\n$$\n\n合并规则与收敛。我们定义一个无向图，其节点等于当前的聚类标识。对于每对不同的聚类 $(a,b)$，如果 $S(a,b)  m_{\\min}$，则我们在 $a$ 和 $b$ 之间添加一条边，这被解释为这两个聚类不可区分的证据（在 FDR 和效应量阈值下，分离标记基因不足）。一个连通分量内的所有聚类被合并成一个单一的聚类。我们进行迭代：在每一步，重新计算合并后聚类的图，然后再次合并，直到没有新的边出现。这个过程必须在有限步内终止，因为每次合并都会严格减少聚类的数量，而聚类的数量是一个以 1 为下界的非负整数。\n\n在给定测试套件下的正确性推理。我们使用固定的超参数 $\\alpha = 0.01$、$\\tau = 0.25$ 和 $m_{\\min} = 2$。\n\n- 测试用例 1。初始标签为 0 和 1 的聚类在所有 $g = 5$ 个基因上具有相同的组成。归一化后，对于任何基因 $j$，聚类 0 和聚类 1 中 $i$ 的 $\\tilde{X}_{i j}$ 分布是相等的，得到 $p_{j} = 1$，因此 $q_{j} = 1$。对于所有 $j$，$q_{j} \\le \\alpha$ 和 $L_{j} \\ge \\tau$ 都不成立；因此 $S(0,1) = 0  m_{\\min}$，所以它们合并。对于合并后的聚类与聚类 2，基因 $j = 4$ 和 $j = 5$（基因从 1 到 5 索引）的组成差异很大，导致 $L_{j}$ 很大且 $q_{j}$ 很小，对至少 2 个基因都通过了两个阈值。因此 $S(\\text{合并},2) \\ge 2 \\ge m_{\\min}$，所以它们不合并，最终得到 $K_{1} = 2$ 个聚类。\n\n- 测试用例 2。这两个聚类在至少 2 个基因（基因 1 和基因 5）上存在组成差异，这将导致 $S(0,1) \\ge 2$，且这些基因的 $L_{j}$ 很大，$q_{j}$ 很小。因此 $S(0,1) \\ge m_{\\min}$，没有边被添加，最终得到 $K_{2} = 2$ 个聚类。\n\n- 测试用例 3。单个体聚类相对于较大的聚类仅在一个基因（基因 3）上存在差异，所以 $S(0,1) = 1  m_{\\min}$，该对被视为不可区分；它们合并成一个单一的聚类，得到 $K_{3} = 1$。\n\n标签压缩。收敛后，我们按照细胞 $i = 1$ 到 $n$ 的首次出现顺序，将聚类重新标记为从 0 开始的整数。这确保了输出的规范性和可复现性。\n\n算法考量。Mann–Whitney U 检验是针对每个基因计算的，使用双边备择假设和渐近 p 值，这适用于中小型样本量并能处理秩次相同的情况。Benjamini–Hochberg 程序完全按照规定实现。基于图的合并使用并查集或通过标准的不相交集逻辑实现连通分量。如上所述，终止性得到保证。\n\n程序完全按照规定编码三个矩阵 $X$ 和标签向量 $L$，应用归一化，计算每对聚类的统计数据，执行迭代合并，并以指定的格式在单行上输出所需的聚合列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\ndef normalize_log1p_cpm(X, scale=1e4):\n    \"\"\"\n    Library-size normalize counts per cell to counts-per-scale and apply log1p (natural log).\n    X: (n_cells, n_genes) nonnegative counts\n    Returns: (n_cells, n_genes) normalized matrix\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    # Compute library sizes per cell; add small epsilon to avoid divide-by-zero if necessary\n    lib = X.sum(axis=1, keepdims=True)\n    # Assumes lib > 0 in our test suite; if zeros occur, leave as zeros.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        norm = np.where(lib > 0, (X / lib) * scale, 0.0)\n    return np.log1p(norm)\n\ndef bh_fdr(pvals):\n    \"\"\"\n    Benjamini-Hochberg FDR adjustment.\n    pvals: array-like of length m\n    Returns: adjusted q-values array of length m\n    \"\"\"\n    p = np.asarray(pvals, dtype=float)\n    m = p.size\n    if m == 0:\n        return np.array([])\n    order = np.argsort(p)\n    p_sorted = p[order]\n    # Compute adjusted values\n    # q_(k) = min_{t >= k} (m/t * p_(t))\n    denom = np.arange(1, m + 1)\n    q_sorted = (m / denom) * p_sorted\n    # Enforce monotonicity\n    q_sorted = np.minimum.accumulate(q_sorted[::-1])[::-1]\n    # Cap at 1\n    q_sorted = np.minimum(q_sorted, 1.0)\n    # Unsort\n    q = np.empty_like(q_sorted)\n    q[order] = q_sorted\n    return q\n\ndef count_markers_between_clusters(Xnorm, labels, a, b, alpha, tau):\n    \"\"\"\n    For clusters a and b, compute the number of genes that satisfy q = alpha and |logFC| >= tau.\n    Uses two-sided Mann-Whitney U test with asymptotic p-values.\n    \"\"\"\n    labels = np.asarray(labels)\n    idx_a = np.where(labels == a)[0]\n    idx_b = np.where(labels == b)[0]\n    Xa = Xnorm[idx_a, :]\n    Xb = Xnorm[idx_b, :]\n    # Compute per-gene means for effect size\n    mean_a = Xa.mean(axis=0)\n    mean_b = Xb.mean(axis=0)\n    logfc = np.abs(mean_a - mean_b)\n    # Compute per-gene p-values using Mann-Whitney U test\n    g = Xnorm.shape[1]\n    pvals = np.empty(g, dtype=float)\n    # Use asymptotic method to handle ties deterministically\n    for j in range(g):\n        col_a = Xa[:, j]\n        col_b = Xb[:, j]\n        if np.allclose(col_a, col_b):\n             pvals[j] = 1.0\n        elif np.allclose(col_a, col_a[0]) and np.allclose(col_b, col_b[0]) and np.allclose(col_a[0], col_b[0]):\n            pvals[j] = 1.0\n        else:\n            try:\n                # Scipy auto-selects exact vs asymptotic based on size and ties\n                res = mannwhitneyu(col_a, col_b, alternative='two-sided')\n                pvals[j] = res.pvalue if not np.isnan(res.pvalue) else 1.0\n            except ValueError:\n                # This can happen if all values in one group are the same.\n                pvals[j] = 1.0\n    qvals = bh_fdr(pvals)\n    # Count markers satisfying both thresholds\n    markers = np.logical_and(qvals = alpha, logfc >= tau)\n    return int(np.count_nonzero(markers))\n\ndef connected_components_merge(labels, indist_pairs):\n    \"\"\"\n    Merge clusters based on indistinguishable pairs using union-find.\n    labels: array of cluster ids\n    indist_pairs: list of tuples (a,b) that should be merged\n    Returns: new_labels with merged cluster ids (not yet compressed)\n    \"\"\"\n    labels = np.asarray(labels)\n    unique_clusters = np.unique(labels)\n    parent = {int(c): int(c) for c in unique_clusters}\n\n    def find(x):\n        # Path compression\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n\n    def union(x, y):\n        rx, ry = find(x), find(y)\n        if rx != ry:\n            parent[ry] = rx\n\n    # Union all indistinguishable pairs\n    for a, b in indist_pairs:\n        union(int(a), int(b))\n\n    # Map each original cluster to its root\n    root_map = {c: find(int(c)) for c in unique_clusters}\n    # Build new labels by replacing each label with its root\n    new_labels = np.array([root_map[int(c)] for c in labels], dtype=int)\n    return new_labels\n\ndef compress_labels_stable(labels):\n    \"\"\"\n    Compress labels to 0..K-1 in order of first appearance across the array.\n    \"\"\"\n    labels = np.asarray(labels, dtype=int)\n    mapping = {}\n    next_id = 0\n    compressed = np.empty_like(labels)\n    for i, c in enumerate(labels):\n        if int(c) not in mapping:\n            mapping[int(c)] = next_id\n            next_id += 1\n        compressed[i] = mapping[int(c)]\n    return compressed\n\ndef merge_overclustered(X, init_labels, alpha=0.01, tau=0.25, m_min=2):\n    \"\"\"\n    Main procedure:\n    - Normalize X with log1p CPM.\n    - Iteratively merge clusters with insufficient markers until convergence.\n    - Return final compressed labels and number of clusters.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    labels = np.asarray(init_labels, dtype=int)\n    Xnorm = normalize_log1p_cpm(X)\n\n    changed = True\n    while changed:\n        changed = False\n        # Current distinct clusters\n        clusters = np.unique(labels)\n        if len(clusters)  2:\n            break\n        # Compute indistinguishable pairs\n        indist_pairs = []\n        for i in range(len(clusters)):\n            for j in range(i + 1, len(clusters)):\n                a = clusters[i]\n                b = clusters[j]\n                S = count_markers_between_clusters(Xnorm, labels, a, b, alpha, tau)\n                if S  m_min:\n                    indist_pairs.append((int(a), int(b)))\n        if indist_pairs:\n            new_labels = connected_components_merge(labels, indist_pairs)\n            if not np.array_equal(new_labels, labels):\n                labels = new_labels\n                changed = True\n    # Compress to consecutive integers in order of first appearance\n    final_labels = compress_labels_stable(labels)\n    K = int(np.unique(final_labels).size)\n    return K, final_labels.tolist()\n\ndef fmt(obj):\n    \"\"\"\n    Format lists (possibly nested) and integers into a compact string without spaces.\n    \"\"\"\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(fmt(x) for x in obj) + \"]\"\n    elif isinstance(obj, (tuple, np.ndarray)):\n        return \"[\" + \",\".join(fmt(x) for x in list(obj)) + \"]\"\n    else:\n        return str(int(obj)) if isinstance(obj, (np.integer,)) else str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    X1_top = [50, 100, 80, 30, 20]\n    X1_mid = [50, 100, 80, 30, 20]\n    X1_bot = [50, 100, 80, 300, 200]\n    X1 = np.array(\n        [X1_top] * 4 + [X1_mid] * 3 + [X1_bot] * 5,\n        dtype=float\n    )\n    L1 = [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2]\n\n    # Test case 2\n    X2_a = [5, 5, 5, 5, 30]\n    X2_b = [30, 5, 5, 5, 5]\n    X2 = np.array([X2_a] * 5 + [X2_b] * 5, dtype=float)\n    L2 = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n    # Test case 3\n    X3_a = [20, 20, 20, 20, 20]\n    X3_b = [20, 20, 50, 20, 20]\n    X3 = np.array([X3_a] * 4 + [X3_b] * 1, dtype=float)\n    L3 = [0, 0, 0, 0, 1]\n\n    test_cases = [\n        (X1, L1),\n        (X2, L2),\n        (X3, L3),\n    ]\n\n    alpha = 0.01\n    tau = 0.25\n    m_min = 2\n\n    results = []\n    for X, L in test_cases:\n        K, labels = merge_overclustered(X, L, alpha=alpha, tau=tau, m_min=m_min)\n        results.append([K, labels])\n\n    # Final print statement in the exact required format: no spaces inside lists.\n    print(fmt(results))\n\nsolve()\n```"
        }
    ]
}