{
    "hands_on_practices": [
        {
            "introduction": "Clustering single-cell data often begins by representing cell-cell similarities as a graph. However, the choice of algorithm to partition this graph can lead to different biological conclusions. This exercise  pits two titans of graph clustering against each other: modularity optimization, the engine behind popular methods like Louvain, and spectral clustering. By creating a synthetic dataset with known properties, you will discover a classic failure mode of modularity-based methods and see how an alternative approach can yield a more accurate result.",
            "id": "2379606",
            "problem": "You are given an abstract representation of a single-cell transcriptomic experiment obtained by Single-cell RNA sequencing (scRNA-seq). Each cell is represented by a point in a two-dimensional embedding space (for example, a Principal Component Analysis (PCA) embedding). The cells belong to a small number of biological populations (cell states), and the goal is to recover these populations using only the geometry of the points.\n\nFormally, you are given a finite set of points $\\{x_i\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^2$. For each dataset, construct an undirected, unweighted, symmetric $k$-nearest neighbor graph $G=(V,E)$ on the vertex set $V=\\{1,\\dots,N\\}$ as follows: for each pair $(i,j)$ with $i \\neq j$, let there be an edge $\\{i,j\\} \\in E$ if and only if $j$ is among the $k$ nearest neighbors of $i$ in Euclidean distance or $i$ is among the $k$ nearest neighbors of $j$ (symmetric union rule). Let $A$ be the adjacency matrix of $G$, with entries $A_{ij} \\in \\{0,1\\}$, degree $k_i = \\sum_j A_{ij}$, and $2m = \\sum_{i,j} A_{ij}$.\n\nDefine the modularity of a partition $P$ of $V$ by\n$$\nQ(P) \\;=\\; \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\frac{k_i k_j}{2m} \\right) \\mathbf{1}\\{c(i)=c(j)\\},\n$$\nwhere $c(i)$ denotes the community label of vertex $i$ in partition $P$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Consider the following two partitions of $V$:\n\n1. A modularity-optimized partition $P_{\\mathrm{mod}}$ obtained by greedy local node moves that seek a local maximum of $Q(P)$ under single-vertex reassignments starting from singleton communities. Concretely, starting from $c(i)=i$ for all $i$, repeatedly consider vertices $i$ (in any order); for each vertex, temporarily remove it from its current community and insert it into the community that yields the largest nonnegative increase in $Q(P)$ computed by the exact modularity change for adding a single vertex. Repeat until no single-vertex move yields an increase in $Q(P)$. The final partition is $P_{\\mathrm{mod}}$.\n\n2. A spectral partition $P_{\\mathrm{spec}}$ into exactly $C$ groups constructed as follows. Let $D=\\mathrm{diag}(k_1,\\dots,k_N)$ and let the symmetric normalized graph Laplacian be\n$$\nL_{\\mathrm{sym}} \\;=\\; I - D^{-1/2} A D^{-1/2}.\n$$\nLet $U \\in \\mathbb{R}^{N \\times C}$ contain as columns the $C$ eigenvectors corresponding to the $C$ smallest eigenvalues of $L_{\\mathrm{sym}}$. Normalize the rows of $U$ to unit length. Then find a partition of the $N$ rows into $C$ clusters that minimizes the sum of squared Euclidean distances from each row to the centroid of its assigned cluster; this defines $P_{\\mathrm{spec}}$.\n\nFor each dataset, known ground truth labels $y^\\star \\in \\{1,\\dots,C\\}^N$ are provided by construction. Define the misclassification count of a predicted partition $P$ with labels $\\hat{y}\\in\\mathbb{N}^N$ against the ground truth $y^\\star$ as the minimum Hamming loss over all permutations $\\pi$ of $\\{1,\\dots,C\\}$:\n$$\n\\mathrm{err}(P) \\;=\\; \\min_{\\pi} \\sum_{i=1}^N \\mathbf{1}\\{\\pi(\\hat{y}_i) \\neq y^\\star_i\\}.\n$$\nIf the number of predicted groups differs from $C$, the minimization is over all injections from the set of predicted labels to $\\{1,\\dots,C\\}$, with any unassigned predicted labels counted entirely as misclassified.\n\nYour task is to generate three synthetic datasets in $\\mathbb{R}^2$ (interpreted as low-dimensional embeddings of single-cell transcriptomes) using multivariate normal models with specified parameters and a fixed pseudo-random seed for reproducibility. For each dataset, construct $G$ using the specified $k$, compute $P_{\\mathrm{mod}}$ and $P_{\\mathrm{spec}}$ (with the specified $C$), and return a boolean indicating whether the spectral partition strictly reduces the misclassification count compared to the modularity-optimized partition, that is, whether $\\mathrm{err}(P_{\\mathrm{spec}}) < \\mathrm{err}(P_{\\mathrm{mod}})$.\n\nData generation. For each dataset, let population $a \\in \\{1,2,3\\}$ contribute $n_a$ points sampled independently from a two-dimensional normal distribution with mean $\\mu_a \\in \\mathbb{R}^2$ and diagonal covariance $\\Sigma_a = \\mathrm{diag}(\\sigma_{a,1}^2,\\sigma_{a,2}^2)$. Concatenate the three populations in the order $a=1,2,3$, and set the ground truth labels $y^\\star$ accordingly to be $1,2,3$. Use a fixed pseudo-random number generator initialized by the provided integer seed in each test case.\n\nTest suite. Use the following three datasets, each with $C=3$.\n\n- Test case $1$ (shape and density heterogeneity; moderate neighborhood size):\n  - Seed $= 42$.\n  - $(n_1,n_2,n_3) = (450, 380, 25)$.\n  - $\\mu_1 = (0,0)$, $\\Sigma_1 = \\mathrm{diag}(1.2^2, 0.6^2)$.\n  - $\\mu_2 = (6,0)$, $\\Sigma_2 = \\mathrm{diag}(1.0^2, 1.0^2)$.\n  - $\\mu_3 = (1.2,0.1)$, $\\Sigma_3 = \\mathrm{diag}(0.08^2, 0.08^2)$.\n  - $k = 15$.\n\n- Test case $2$ (balanced, well-separated Gaussians):\n  - Seed $= 17$.\n  - $(n_1,n_2,n_3) = (200, 200, 200)$.\n  - $\\mu_1 = (-6,0)$, $\\Sigma_1 = \\mathrm{diag}(0.7^2, 0.7^2)$.\n  - $\\mu_2 = (0,0)$, $\\Sigma_2 = \\mathrm{diag}(0.7^2, 0.7^2)$.\n  - $\\mu_3 = (6,0)$, $\\Sigma_3 = \\mathrm{diag}(0.7^2, 0.7^2)$.\n  - $k = 15$.\n\n- Test case $3$ (boundary condition with very large neighborhood size):\n  - Seed $= 123$.\n  - $(n_1,n_2,n_3) = (450, 380, 25)$.\n  - $\\mu_1 = (0,0)$, $\\Sigma_1 = \\mathrm{diag}(1.2^2, 0.6^2)$.\n  - $\\mu_2 = (6,0)$, $\\Sigma_2 = \\mathrm{diag}(1.0^2, 1.0^2)$.\n  - $\\mu_3 = (1.2,0.1)$, $\\Sigma_3 = \\mathrm{diag}(0.08^2, 0.08^2)$.\n  - $k = 100$.\n\nFinal output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). For each of the three test cases in the order above, output the boolean value of $\\mathrm{err}(P_{\\mathrm{spec}}) < \\mathrm{err}(P_{\\mathrm{mod}})$ for that test case.",
            "solution": "The problem is valid. It presents a well-defined computational task rooted in the established principles of computational biology and network science. Specifically, it asks for the implementation and comparison of two standard graph-based clustering algorithms—modularity optimization and spectral clustering—on synthetic datasets mimicking single-cell transcriptomic embeddings. All parameters, methodologies, and evaluation criteria are specified with sufficient precision to permit a deterministic and reproducible solution.\n\nThe solution proceeds systematically through the following steps for each of the $3$ test cases provided:\n\n1.  **Data Generation**: For each test case, a dataset of $N$ points in $\\mathbb{R}^2$ is synthesized. The total number of points $N$ is the sum of the specified population sizes, $N = n_1 + n_2 + n_3$. A pseudo-random number generator, initialized with the given seed, is used to sample points for each of the $C=3$ populations from their respective bivariate normal distributions, $\\mathcal{N}(\\mu_a, \\Sigma_a)$. The resulting point sets are concatenated, and ground truth labels $y^\\star \\in \\{1, 2, 3\\}^N$ are assigned according to the population of origin. For implementation, these labels are mapped to $\\{0, 1, 2\\}$.\n\n2.  **Symmetric $k$-Nearest Neighbor Graph Construction**: An undirected, unweighted graph $G=(V, E)$ is constructed from the $N$ points. First, the Euclidean distance between all pairs of points is computed. For each point $i$, its $k$ nearest neighbors are identified. An edge $\\{i, j\\}$ is added to the edge set $E$ if point $j$ is among the $k$ nearest neighbors of $i$, or if $i$ is among the $k$ nearest neighbors of $j$. This symmetric union rule ensures the resulting graph is undirected. The graph is represented by its adjacency matrix $A \\in \\{0, 1\\}^{N \\times N}$. From $A$, we compute the degree $k_i = \\sum_j A_{ij}$ for each vertex $i$ and the total edge weight proxy $2m = \\sum_i k_i$.\n\n3.  **Modularity Optimization ($P_{\\mathrm{mod}}$)**: This algorithm seeks a partition of the graph that maximizes the modularity $Q$, a quality function that measures the density of edges within communities compared to what is expected in a random graph with the same degree sequence. The formula for modularity is\n    $$\n    Q(P) = \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\frac{k_i k_j}{2m} \\right) \\mathbf{1}\\{c(i)=c(j)\\}\n    $$\n    where $c(i)$ is the community of vertex $i$. We implement a greedy, iterative algorithm. Initially, each vertex is in its own singleton community. The algorithm then repeatedly passes over all vertices $i \\in V$. For each vertex, it calculates the change in modularity, $\\Delta Q$, that would result from moving it to the community of each of its neighbors. The vertex is moved to the community that yields the largest positive $\\Delta Q$. This process is repeated until no single move can increase the total modularity, resulting in a locally optimal partition $P_{\\mathrm{mod}}$. The number of clusters is determined automatically by the algorithm. The change in modularity for moving vertex $u$ from community $S$ to community $T$ is calculated efficiently as:\n    $$\n    \\Delta Q = \\left( \\frac{k_{u,T}}{m} - \\frac{k_u \\Sigma_{tot,T}}{2m^2} \\right) - \\left( \\frac{k_{u,S \\setminus \\{u\\}}}{m} - \\frac{k_u (\\Sigma_{tot,S}-k_u)}{2m^2} \\right)\n    $$\n    where $k_{u,C}$ is the number of neighbors of $u$ in community $C$, and $\\Sigma_{tot,C}$ is the sum of degrees of all vertices in $C$.\n\n4.  **Spectral Clustering ($P_{\\mathrm{spec}}$)**: This method uses the eigenspectrum of the graph Laplacian to find a low-dimensional embedding of the vertices that is well-suited for clustering. We use the symmetric normalized Laplacian, defined as:\n    $$\n    L_{\\mathrm{sym}} = I - D^{-1/2} A D^{-1/2}\n    $$\n    where $D$ is the diagonal matrix of vertex degrees $k_i$. The $C=3$ eigenvectors corresponding to the $C$ smallest eigenvalues of $L_{\\mathrm{sym}}$ are computed. These eigenvectors form the columns of a matrix $U \\in \\mathbb{R}^{N \\times C}$. The rows of $U$ provide a new representation for each vertex in a $C$-dimensional space. After normalizing each row of $U$ to have unit Euclidean length, we apply k-means clustering to these $N$ row-vectors to partition them into $C$ groups. The initial centroids for k-means are chosen deterministically to ensure reproducibility. This yields the partition $P_{\\mathrm{spec}}$.\n\n5.  **Error Calculation and Comparison**: For both partitions $P_{\\mathrm{mod}}$ and $P_{\\mathrm{spec}}$, we compute the misclassification count against the ground truth labels $y^\\star$. Since the cluster labels are arbitrary, the error is defined as the minimum Hamming distance over all possible matchings between predicted and true label sets. This is a classic assignment problem, which we solve using the Hungarian algorithm, available in `scipy.optimize.linear_sum_assignment`. We construct a confusion matrix between predicted and true labels and find the label mapping that maximizes the number of correctly classified samples. The error is the total number of samples minus this maximum. The final output for each test case is a boolean value indicating if the spectral partition is strictly better: $\\mathrm{err}(P_{\\mathrm{spec}}) < \\mathrm{err}(P_{\\mathrm{mod}})$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\nfrom scipy.spatial.distance import cdist\nfrom scipy.cluster.vq import kmeans2\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the full pipeline for all test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (shape and density heterogeneity; moderate neighborhood size)\n        {\n            'seed': 42,\n            'n_samples': (450, 380, 25),\n            'means': [(0, 0), (6, 0), (1.2, 0.1)],\n            'covs': [[1.2**2, 0], [0, 0.6**2]],\n            'covs_list': [np.diag([1.2**2, 0.6**2]), np.diag([1.0**2, 1.0**2]), np.diag([0.08**2, 0.08**2])],\n            'k': 15,\n            'C': 3\n        },\n        # Test case 2 (balanced, well-separated Gaussians)\n        {\n            'seed': 17,\n            'n_samples': (200, 200, 200),\n            'means': [(-6, 0), (0, 0), (6, 0)],\n            'covs_list': [np.diag([0.7**2, 0.7**2]), np.diag([0.7**2, 0.7**2]), np.diag([0.7**2, 0.7**2])],\n            'k': 15,\n            'C': 3\n        },\n        # Test case 3 (boundary condition with very large neighborhood size)\n        {\n            'seed': 123,\n            'n_samples': (450, 380, 25),\n            'means': [(0, 0), (6, 0), (1.2, 0.1)],\n            'covs_list': [np.diag([1.2**2, 0.6**2]), np.diag([1.0**2, 1.0**2]), np.diag([0.08**2, 0.08**2])],\n            'k': 100,\n            'C': 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_single_case(case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(n_samples, means, covs_list, seed):\n    \"\"\"Generates synthetic data from a mixture of Gaussians.\"\"\"\n    rng = np.random.default_rng(seed)\n    points = []\n    labels = []\n    for i, (n, mean, cov) in enumerate(zip(n_samples, means, covs_list)):\n        points.append(rng.multivariate_normal(mean, cov, n))\n        labels.extend([i] * n)\n    X = np.vstack(points)\n    y_true = np.array(labels)\n    return X, y_true\n\ndef _build_knn_graph(X, k):\n    \"\"\"Constructs a symmetric k-NN graph.\"\"\"\n    N = X.shape[0]\n    dist_matrix = cdist(X, X, 'euclidean')\n    \n    # Get indices of k-nearest neighbors for each point\n    neighbors = np.argsort(dist_matrix, axis=1)[:, 1:k+1]\n    \n    A = np.zeros((N, N), dtype=int)\n    for i in range(N):\n        A[i, neighbors[i]] = 1\n        \n    # Symmetrize the adjacency matrix\n    A = np.maximum(A, A.T)\n    return A\n\ndef _modularity_clustering(A):\n    \"\"\"Performs modularity-based greedy clustering.\"\"\"\n    N = A.shape[0]\n    degrees = A.sum(axis=1)\n    m = degrees.sum() / 2.0\n    \n    if m == 0:\n        return np.arange(N)\n\n    # Initial state: each node in its own community\n    communities = np.arange(N)\n    comm_degrees = degrees.copy().astype(float)\n    \n    # Store neighbor lists to avoid repeated lookups\n    adj_list = [np.where(row > 0)[0] for row in A]\n\n    while True:\n        moved = False\n        for u in range(N):\n            old_comm = communities[u]\n            k_u = degrees[u]\n            \n            # Gain from removing u from its old community\n            sigma_old = comm_degrees[old_comm]\n            k_u_in_old = np.sum(A[u, adj_list[u]][communities[adj_list[u]] == old_comm])\n            loss_remove = k_u_in_old / m - k_u * (sigma_old - k_u) / (2 * m**2)\n            \n            best_comm = old_comm\n            max_gain = 0.0\n\n            # Consider moving to neighbors' communities\n            neighbor_comms = set(communities[v] for v in adj_list[u])\n            for new_comm in neighbor_comms:\n                if new_comm == old_comm:\n                    continue\n                \n                sigma_new = comm_degrees[new_comm]\n                k_u_in_new = np.sum(A[u, adj_list[u]][communities[adj_list[u]] == new_comm])\n                gain_add = k_u_in_new / m - k_u * sigma_new / (2 * m**2)\n                \n                delta_q = gain_add - loss_remove\n                if delta_q > max_gain:\n                    max_gain = delta_q\n                    best_comm = new_comm\n\n            if best_comm != old_comm and max_gain > 1e-9: # Use tolerance for float comparison\n                # Move u to the best new community\n                comm_degrees[old_comm] -= k_u\n                comm_degrees[best_comm] += k_u\n                communities[u] = best_comm\n                moved = True\n        \n        if not moved:\n            break\n            \n    # Relabel communities to be contiguous integers starting from 0\n    unique_labels, contiguous_labels = np.unique(communities, return_inverse=True)\n    return contiguous_labels\n\ndef _spectral_clustering(A, C):\n    \"\"\"Performs spectral clustering.\"\"\"\n    N = A.shape[0]\n    degrees = A.sum(axis=1).astype(float)\n    \n    # Handle isolated nodes\n    isolated_nodes = degrees == 0\n    if np.all(isolated_nodes):\n        return np.arange(N) # All isolated, spectral clustering undefined\n\n    # Construct symmetric normalized Laplacian\n    inv_sqrt_degrees = np.zeros_like(degrees)\n    np.divide(1.0, np.sqrt(degrees), out=inv_sqrt_degrees, where=degrees > 0)\n    D_inv_sqrt = np.diag(inv_sqrt_degrees)\n    L_sym = np.identity(N) - D_inv_sqrt @ A @ D_inv_sqrt\n\n    # Eigen-decomposition\n    eigvals, eigvecs = eigh(L_sym)\n    \n    # Form U from the first C eigenvectors\n    U = eigvecs[:, :C]\n\n    # Normalize rows of U\n    norms = np.linalg.norm(U, axis=1, keepdims=True)\n    U_norm = np.zeros_like(U)\n    np.divide(U, norms, out=U_norm, where=norms > 0)\n\n    # K-means on the rows of normalized U, with deterministic initialization\n    centroids, labels = kmeans2(U_norm, C, minit='points', missing='raise')\n    \n    return labels\n\ndef _calculate_error(y_true, y_pred):\n    \"\"\"Calculates the misclassification error using the Hungarian algorithm.\"\"\"\n    N = len(y_true)\n    true_labels = np.unique(y_true)\n    pred_labels = np.unique(y_pred)\n    \n    cost_matrix = np.zeros((len(pred_labels), len(true_labels)))\n    for i, pl in enumerate(pred_labels):\n        for j, tl in enumerate(true_labels):\n            cost_matrix[i, j] = np.sum((y_pred == pl)  (y_true == tl))\n            \n    # Use linear_sum_assignment to find the best mapping (max weight matching)\n    row_ind, col_ind = linear_sum_assignment(-cost_matrix)\n    \n    num_correct = cost_matrix[row_ind, col_ind].sum()\n    error = N - num_correct\n    return error\n\ndef _solve_single_case(case):\n    \"\"\"Solves a single test case from generation to comparison.\"\"\"\n    X, y_true = _generate_data(case['n_samples'], case['means'], case['covs_list'], case['seed'])\n    N = X.shape[0]\n\n    # Build k-NN graph\n    A = _build_knn_graph(X, case['k'])\n\n    # Get modularity partition\n    y_mod = _modularity_clustering(A)\n\n    # Get spectral partition\n    y_spec = _spectral_clustering(A, case['C'])\n\n    # Calculate errors\n    err_mod = _calculate_error(y_true, y_mod)\n    err_spec = _calculate_error(y_true, y_spec)\n\n    return err_spec  err_mod\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A common challenge in single-cell analysis is \"over-clustering,\" where an algorithm splits a single biological cell type into multiple, statistically similar clusters. This practice  equips you with a powerful strategy to correct this by merging clusters that lack a strong biological distinction. You will implement a workflow that uses differential gene expression analysis and robust statistical testing to decide which clusters are truly distinct and which should be unified, bridging the gap between raw algorithmic output and a biologically meaningful cell atlas.",
            "id": "2379646",
            "problem": "You are given a gene-by-cell count matrix and an initial set of cluster labels for single-cell ribonucleic acid sequencing (scRNA-seq) transcriptomes. The goal is to design and implement a principled procedure that automatically merges over-clustered populations by testing whether pairs of clusters lack statistically significant separating marker genes. The method must be based on core definitions and well-tested statistical procedures commonly used in single-cell transcriptomics.\n\nStart from the following foundations:\n- A cell-by-gene count matrix is a nonnegative integer array of shape $n \\times g$, where $n$ is the number of cells and $g$ is the number of genes. Denote the matrix by $X \\in \\mathbb{N}_{0}^{n \\times g}$, with entries $X_{i j}$ representing the count for cell $i$ and gene $j$.\n- Library-size normalization rescales each cell so that total counts per cell are comparable. A standard approach is to compute for each cell $i$ the total count $s_{i} = \\sum_{j=1}^{g} X_{i j}$, and then transform counts to counts-per-$10^{4}$ followed by natural logarithm stabilization, i.e., define\n$$\n\\tilde{X}_{i j} = \\log\\left(1 + \\frac{X_{i j}}{s_{i}} \\cdot 10^{4}\\right),\n$$\nfor all cells $i \\in \\{1,\\dots,n\\}$ and genes $j \\in \\{1,\\dots,g\\}$, where $\\log$ denotes the natural logarithm.\n- For two clusters $a$ and $b$, let $A$ be the index set of cells with label $a$ and $B$ the index set of cells with label $b$. For each gene $j \\in \\{1,\\dots,g\\}$, consider the two-sided Mann–Whitney $U$ test (also known as the Wilcoxon rank-sum test) applied to the samples $\\{\\tilde{X}_{i j} : i \\in A\\}$ and $\\{\\tilde{X}_{i j} : i \\in B\\}$, yielding a $p$-value $p_{j} \\in [0,1]$. Let the absolute log-fold-change per gene be\n$$\nL_{j} = \\left|\\frac{1}{|A|} \\sum_{i \\in A} \\tilde{X}_{i j} - \\frac{1}{|B|} \\sum_{i \\in B} \\tilde{X}_{i j}\\right|.\n$$\n- Control for multiple testing across the $g$ genes using the Benjamini–Hochberg False Discovery Rate (FDR) step-up procedure. Given the vector $(p_{1},\\dots,p_{g})$, sort it in nondecreasing order to obtain $p_{(1)} \\le \\cdots \\le p_{(g)}$, and compute\n$$\nq_{(k)} = \\min_{t \\in \\{k,\\dots,g\\}} \\left\\{ \\frac{g}{t} \\, p_{(t)} \\right\\},\n$$\nthen unsort to obtain adjusted $q$-values $(q_{1},\\dots,q_{g})$.\n- Define a decision rule at fixed thresholds $\\alpha \\in (0,1)$, $\\tau  0$, and $m_{\\min} \\in \\mathbb{N}$: the pair of clusters $(a,b)$ is declared indistinguishable if the number of genes satisfying both $q_{j} \\le \\alpha$ and $L_{j} \\ge \\tau$ is strictly less than $m_{\\min}$. Equivalently, if\n$$\nS(a,b) = \\left| \\left\\{ j \\in \\{1,\\dots,g\\} : q_{j} \\le \\alpha \\;\\wedge\\; L_{j} \\ge \\tau \\right\\} \\right|  m_{\\min},\n$$\nthen clusters $a$ and $b$ are to be merged.\n\nAlgorithmic requirement:\n- Construct an undirected graph whose nodes are the current cluster identities. Add an edge between distinct clusters $a$ and $b$ if and only if $S(a,b)  m_{\\min}$. Replace the set of clusters by the connected components of this graph (merging each connected component into a single cluster). Iterate this process until no edges are added in an iteration (i.e., a fixed point is reached). At the end, compress cluster labels to consecutive integers starting at $0$ using order of first appearance when scanning cells $i=1$ to $n$.\n\nUse the following fixed hyperparameters for all test cases:\n- Significance level $\\alpha = 0.01$,\n- Log-fold-change threshold $\\tau = 0.25$,\n- Minimum marker count $m_{\\min} = 2$.\n\nInput specification for the program:\n- Hard-code the test suite below directly in the program. No input is to be read from standard input or files.\n\nTest suite:\n- Test case $1$ (happy path with over-clustering): $g = 5$ genes, $n = 12$ cells. The matrix $X \\in \\mathbb{N}_{0}^{12 \\times 5}$ is defined row-wise as\n  - Rows $1$ to $4$: $[50, 100, 80, 30, 20]$,\n  - Rows $5$ to $7$: $[50, 100, 80, 30, 20]$,\n  - Rows $8$ to $12$: $[50, 100, 80, 300, 200]$.\n  The initial label vector is $L = [0,0,0,0,1,1,1,2,2,2,2,2]$. Clusters with labels $0$ and $1$ are compositionally identical and should merge; cluster $2$ is distinct.\n- Test case $2$ (well-separated clusters): $g = 5$, $n = 10$. The matrix $X \\in \\mathbb{N}_{0}^{10 \\times 5}$ is\n  - Rows $1$ to $5$: $[5, 5, 5, 5, 30]$,\n  - Rows $6$ to $10$: $[30, 5, 5, 5, 5]$.\n  Initial labels $L = [0,0,0,0,0,1,1,1,1,1]$. These two clusters differ on at least $2$ genes and should not merge.\n- Test case $3$ (edge case with a singleton cluster and limited markers): $g = 5$, $n = 5$. The matrix $X \\in \\mathbb{N}_{0}^{5 \\times 5}$ is\n  - Rows $1$ to $4$: $[20, 20, 20, 20, 20]$,\n  - Row $5$: $[20, 20, 50, 20, 20]$.\n  Initial labels $L = [0,0,0,0,1]$. This simulates a small over-split singleton differing on only $1$ gene; with $m_{\\min} = 2$, it should merge back.\n\nOutput specification:\n- For each test case, the program must output a list with two elements: the final number of clusters after convergence, and the final compressed label vector of length $n$.\n- Aggregate the results for all three test cases into a single line as a comma-separated list enclosed in square brackets. Concretely, the output must be a single line representing\n$$\n\\left[ [K_{1}, \\text{labels}_{1}], [K_{2}, \\text{labels}_{2}], [K_{3}, \\text{labels}_{3}] \\right],\n$$\nwhere $K_{t} \\in \\mathbb{N}$ and $\\text{labels}_{t}$ is a list of $n_{t}$ integers for test case $t \\in \\{1,2,3\\}$. The integers inside the lists must be printed without spaces. For example, a syntactically correct output could look like $[[2,[0,0,0]],[2,[0,1]],[1,[0]]]$ (this is only an example format, not the expected answer).",
            "solution": "We design a statistically principled merge procedure for over-clustered single-cell ribonucleic acid sequencing (scRNA-seq) datasets by formalizing the lack of separating marker genes between clusters as a null hypothesis that cannot be rejected at a controlled False Discovery Rate (FDR). The derivation begins with core definitions and widely used statistical tools in transcriptomics.\n\nPreprocessing and normalization. Each observed count $X_{i j}$ is influenced by both the cellular gene expression program and technical factors such as library size. A widely accepted first-order correction is library-size normalization combined with a variance-stabilizing transform. For each cell $i$, compute the library size $s_{i} = \\sum_{j=1}^{g} X_{i j}$. The normalized expression on a continuous scale is then\n$$\n\\tilde{X}_{i j} = \\log\\left(1 + \\frac{X_{i j}}{s_{i}} \\cdot 10^{4}\\right).\n$$\nThis procedure rescales all cells to a common effective depth of $10^{4}$ total counts and applies a natural logarithm to dampen the mean-variance relationship, which is a well-tested practice in single-cell analysis.\n\nTesting for separating marker genes. Consider two clusters $a$ and $b$ with index sets $A$ and $B$, respectively. For each gene $j \\in \\{1,\\dots,g\\}$ we require a test to detect a difference in the central tendency of $\\tilde{X}_{i j}$ between $A$ and $B$. Since the scRNA-seq normalized data are typically non-Gaussian and may have ties, we adopt the two-sided Mann–Whitney $U$ test (Wilcoxon rank-sum), which tests\n$$\nH_{0}: \\text{the distributions of } \\{\\tilde{X}_{i j}\\}_{i \\in A} \\text{ and } \\{\\tilde{X}_{i j}\\}_{i \\in B} \\text{ are equal}\n$$\nversus\n$$\nH_{1}: \\text{the distributions differ in location}.\n$$\nThis yields a $p$-value $p_{j} \\in [0,1]$ for each gene $j$.\n\nMultiple-testing correction. Since we test across $g$ genes, we control the expected proportion of false discoveries using the Benjamini–Hochberg (BH) step-up FDR procedure. Given $p$-values $(p_{1},\\dots,p_{g})$, sort them to obtain $p_{(1)} \\le \\cdots \\le p_{(g)}$. Define the BH-adjusted values via\n$$\nq_{(k)} = \\min_{t \\in \\{k,\\dots,g\\}} \\left\\{ \\frac{g}{t} \\, p_{(t)} \\right\\},\n$$\nand then map back to the original gene order to obtain $(q_{1},\\dots,q_{g})$. This produces $q$-values that control the expected proportion of false positives among discoveries under standard independence or positive-dependence assumptions.\n\nEffect size threshold. To avoid merging clusters that have statistically detectable yet practically negligible differences, we include an effect size constraint via the absolute log-fold-change\n$$\nL_{j} = \\left|\\mu_{A j} - \\mu_{B j}\\right|, \\quad \\text{where } \\mu_{A j} = \\frac{1}{|A|} \\sum_{i \\in A} \\tilde{X}_{i j}, \\;\\; \\mu_{B j} = \\frac{1}{|B|} \\sum_{i \\in B} \\tilde{X}_{i j}.\n$$\nA gene is deemed a separating marker only if both $q_{j} \\le \\alpha$ and $L_{j} \\ge \\tau$, for fixed thresholds $\\alpha$ and $\\tau$. The number of separating markers is\n$$\nS(a,b) = \\left| \\left\\{ j \\in \\{1,\\dots,g\\} : q_{j} \\le \\alpha \\;\\wedge\\; L_{j} \\ge \\tau \\right\\} \\right|.\n$$\n\nMerge rule and convergence. We define an undirected graph with nodes equal to current cluster identities. For each pair of distinct clusters $(a,b)$, if $S(a,b)  m_{\\min}$, then we add an edge between $a$ and $b$, interpreting this as evidence that the clusters are indistinguishable (insufficient separating markers at the FDR and effect-size thresholds). All clusters within a connected component are merged into a single cluster. We iterate: at each step, recompute the graph on the merged clusters, and merge again, until no new edges appear. This process must terminate in finitely many steps because each merge strictly reduces the number of clusters, and the number of clusters is a nonnegative integer bounded below by $1$.\n\nCorrectness reasoning under the given test suite. We use fixed hyperparameters $\\alpha = 0.01$, $\\tau = 0.25$, and $m_{\\min} = 2$.\n\n- Test case $1$. Clusters with initial labels $0$ and $1$ have identical compositions across all $g = 5$ genes by construction. After normalization, for any gene $j$, the distributions of $\\tilde{X}_{i j}$ for $i$ in cluster $0$ and cluster $1$ are equal, yielding $p_{j} = 1$, hence $q_{j} = 1$. For all $j$, neither $q_{j} \\le \\alpha$ nor $L_{j} \\ge \\tau$ holds; thus $S(0,1) = 0  m_{\\min}$, so they merge. For the merged cluster versus cluster $2$, the compositions differ strongly for genes $j = 4$ and $j = 5$ (indexing genes from $1$ to $5$), giving large $L_{j}$ and small $q_{j}$ that pass both thresholds for at least $2$ genes. Hence $S(\\text{merged},2) \\ge 2 \\ge m_{\\min}$, so they do not merge, and we end with $K_{1} = 2$ clusters.\n\n- Test case $2$. The two clusters differ compositionally on at least $2$ genes (gene $1$ and gene $5$), which yields $S(0,1) \\ge 2$ with $L_{j}$ large and $q_{j}$ small for those genes. Therefore $S(0,1) \\ge m_{\\min}$, no edge is added, and we end with $K_{2} = 2$ clusters.\n\n- Test case $3$. The singleton cluster has a difference relative to the larger cluster on only one gene (gene $3$), so $S(0,1) = 1  m_{\\min}$, and the pair is deemed indistinguishable; they merge to a single cluster with $K_{3} = 1$.\n\nLabel compression. After convergence, we relabel clusters to integers starting at $0$ in order of first appearance across cells $i = 1$ to $n$. This ensures a canonical, reproducible output.\n\nAlgorithmic considerations. The Mann–Whitney $U$ test is computed per gene with a two-sided alternative and an asymptotic $p$-value, which is appropriate for small to moderate sample sizes and can handle ties. The Benjamini–Hochberg procedure is implemented exactly as specified. The graph-based merging uses union-find or connected components via standard disjoint-set logic. Termination is guaranteed as noted above.\n\nThe program encodes the three matrices $X$ and label vectors $L$ exactly as specified, applies the normalization, computes per-pair statistics, performs iterative merging, and outputs the required aggregate list on a single line in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import mannwhitneyu\n\ndef normalize_log1p_cpm(X, scale=1e4):\n    \"\"\"\n    Library-size normalize counts per cell to counts-per-scale and apply log1p (natural log).\n    X: (n_cells, n_genes) nonnegative counts\n    Returns: (n_cells, n_genes) normalized matrix\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    # Compute library sizes per cell; add small epsilon to avoid divide-by-zero if necessary\n    lib = X.sum(axis=1, keepdims=True)\n    # Assumes lib > 0 in our test suite; if zeros occur, leave as zeros.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        norm = np.where(lib > 0, (X / lib) * scale, 0.0)\n    return np.log1p(norm)\n\ndef bh_fdr(pvals):\n    \"\"\"\n    Benjamini-Hochberg FDR adjustment.\n    pvals: array-like of length m\n    Returns: adjusted q-values array of length m\n    \"\"\"\n    p = np.asarray(pvals, dtype=float)\n    m = p.size\n    order = np.argsort(p)\n    p_sorted = p[order]\n    # Compute adjusted values\n    # q_(k) = min_{t >= k} (m/t * p_(t))\n    denom = np.arange(1, m + 1)\n    q_sorted = (m / denom) * p_sorted\n    # Enforce monotonicity\n    q_sorted = np.minimum.accumulate(q_sorted[::-1])[::-1]\n    # Cap at 1\n    q_sorted = np.minimum(q_sorted, 1.0)\n    # Unsort\n    q = np.empty_like(q_sorted)\n    q[order] = q_sorted\n    return q\n\ndef count_markers_between_clusters(Xnorm, labels, a, b, alpha, tau):\n    \"\"\"\n    For clusters a and b, compute the number of genes that satisfy q = alpha and |logFC| >= tau.\n    Uses two-sided Mann-Whitney U test with asymptotic p-values.\n    \"\"\"\n    labels = np.asarray(labels)\n    idx_a = np.where(labels == a)[0]\n    idx_b = np.where(labels == b)[0]\n    Xa = Xnorm[idx_a, :]\n    Xb = Xnorm[idx_b, :]\n    # Compute per-gene means for effect size\n    mean_a = Xa.mean(axis=0)\n    mean_b = Xb.mean(axis=0)\n    logfc = np.abs(mean_a - mean_b)\n    # Compute per-gene p-values using Mann-Whitney U test\n    g = Xnorm.shape[1]\n    pvals = np.empty(g, dtype=float)\n    # Use asymptotic method to handle ties deterministically\n    for j in range(g):\n        # Handle degenerate case where both groups are constant and equal => p-value = 1.0\n        col_a = Xa[:, j]\n        col_b = Xb[:, j]\n        if np.all(col_a == col_a[0]) and np.all(col_b == col_b[0]) and (col_a[0] == col_b[0]):\n            pvals[j] = 1.0\n        else:\n            try:\n                res = mannwhitneyu(col_a, col_b, alternative='two-sided', method='asymptotic')\n                pvals[j] = res.pvalue\n            except Exception:\n                # Fallback: if test fails for any numerical reason, treat as non-significant\n                pvals[j] = 1.0\n    qvals = bh_fdr(pvals)\n    # Count markers satisfying both thresholds\n    markers = np.logical_and(qvals = alpha, logfc >= tau)\n    return int(np.count_nonzero(markers))\n\ndef connected_components_merge(labels, indist_pairs):\n    \"\"\"\n    Merge clusters based on indistinguishable pairs using union-find.\n    labels: array of cluster ids\n    indist_pairs: list of tuples (a,b) that should be merged\n    Returns: new_labels with merged cluster ids (not yet compressed)\n    \"\"\"\n    labels = np.asarray(labels)\n    unique_clusters = np.unique(labels)\n    parent = {int(c): int(c) for c in unique_clusters}\n\n    def find(x):\n        # Path compression\n        if parent[x] != x:\n            parent[x] = find(parent[x])\n        return parent[x]\n\n    def union(x, y):\n        rx, ry = find(x), find(y)\n        if rx != ry:\n            parent[ry] = rx\n\n    # Union all indistinguishable pairs\n    for a, b in indist_pairs:\n        union(int(a), int(b))\n\n    # Map each original cluster to its root\n    root_map = {c: find(int(c)) for c in unique_clusters}\n    # Build new labels by replacing each label with its root\n    new_labels = np.array([root_map[int(c)] for c in labels], dtype=int)\n    return new_labels\n\ndef compress_labels_stable(labels):\n    \"\"\"\n    Compress labels to 0..K-1 in order of first appearance across the array.\n    \"\"\"\n    labels = np.asarray(labels, dtype=int)\n    mapping = {}\n    next_id = 0\n    compressed = np.empty_like(labels)\n    for i, c in enumerate(labels):\n        if int(c) not in mapping:\n            mapping[int(c)] = next_id\n            next_id += 1\n        compressed[i] = mapping[int(c)]\n    return compressed\n\ndef merge_overclustered(X, init_labels, alpha=0.01, tau=0.25, m_min=2):\n    \"\"\"\n    Main procedure:\n    - Normalize X with log1p CPM.\n    - Iteratively merge clusters with insufficient markers until convergence.\n    - Return final compressed labels and number of clusters.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    labels = np.asarray(init_labels, dtype=int)\n    Xnorm = normalize_log1p_cpm(X)\n\n    changed = True\n    while changed:\n        changed = False\n        # Current distinct clusters\n        clusters = np.unique(labels)\n        # Compute indistinguishable pairs\n        indist_pairs = []\n        for i in range(len(clusters)):\n            for j in range(i + 1, len(clusters)):\n                a = clusters[i]\n                b = clusters[j]\n                S = count_markers_between_clusters(Xnorm, labels, a, b, alpha, tau)\n                if S  m_min:\n                    indist_pairs.append((int(a), int(b)))\n        if indist_pairs:\n            new_labels = connected_components_merge(labels, indist_pairs)\n            if not np.array_equal(new_labels, labels):\n                labels = new_labels\n                changed = True\n    # Compress to consecutive integers in order of first appearance\n    final_labels = compress_labels_stable(labels)\n    K = int(np.unique(final_labels).size)\n    return K, final_labels.tolist()\n\ndef fmt(obj):\n    \"\"\"\n    Format lists (possibly nested) and integers into a compact string without spaces.\n    \"\"\"\n    if isinstance(obj, list):\n        return \"[\" + \",\".join(fmt(x) for x in obj) + \"]\"\n    elif isinstance(obj, (tuple, np.ndarray)):\n        return \"[\" + \",\".join(fmt(x) for x in list(obj)) + \"]\"\n    else:\n        return str(int(obj)) if isinstance(obj, (np.integer,)) else str(obj)\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    X1_top = [50, 100, 80, 30, 20]\n    X1_mid = [50, 100, 80, 30, 20]\n    X1_bot = [50, 100, 80, 300, 200]\n    X1 = np.array(\n        [X1_top] * 4 + [X1_mid] * 3 + [X1_bot] * 5,\n        dtype=float\n    )\n    L1 = [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2]\n\n    # Test case 2\n    X2_a = [5, 5, 5, 5, 30]\n    X2_b = [30, 5, 5, 5, 5]\n    X2 = np.array([X2_a] * 5 + [X2_b] * 5, dtype=float)\n    L2 = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n\n    # Test case 3\n    X3_a = [20, 20, 20, 20, 20]\n    X3_b = [20, 20, 50, 20, 20]\n    X3 = np.array([X3_a] * 4 + [X3_b] * 1, dtype=float)\n    L3 = [0, 0, 0, 0, 1]\n\n    test_cases = [\n        (X1, L1),\n        (X2, L2),\n        (X3, L3),\n    ]\n\n    alpha = 0.01\n    tau = 0.25\n    m_min = 2\n\n    results = []\n    for X, L in test_cases:\n        K, labels = merge_overclustered(X, L, alpha=alpha, tau=tau, m_min=m_min)\n        results.append([K, labels])\n\n    # Final print statement in the exact required format: no spaces inside lists.\n    print(fmt(results))\n\nsolve()\n```"
        },
        {
            "introduction": "Visualizing single-cell data with algorithms like UMAP is crucial for interpretation, but the resulting images are highly dependent on user-chosen parameters. This exercise  isolates a key parameter analogous to UMAP's `min_dist` to explore how it can dramatically alter whether we see cell populations as discrete subtypes or a connected continuum. By modeling cell relationships as a graph and applying a simple distance threshold, you'll gain hands-on intuition for how tuning this single parameter can change the entire biological story.",
            "id": "2379667",
            "problem": "You are given a fully specified, deterministic synthetic single-cell transcriptome dataset designed to test how an abstracted proximity threshold parameter, inspired by Uniform Manifold Approximation and Projection (UMAP), can alter the interpretation of cell subtype separation independently of the number of neighbors used to build a base neighborhood graph. The dataset consists of cells embedded in a gene-expression space of dimension $3$, with coordinates interpreted as normalized expression levels (arbitrary units). There are three labeled subtypes, denoted $A$, $B$, and $C$, and an unlabeled set of transitional cells $T$ that lie between $A$ and $B$.\n\nThe coordinates are specified as follows, all points are in $\\mathbb{R}^3$ and all omitted coordinates are $0$.\n\n- Subtype $A$ (indices listed in the order below): $(-0.5,0,0)$, $(-0.4,0,0)$, $(-0.3,0,0)$, $(-0.2,0,0)$, $(-0.1,0,0)$, $(0.0,0,0)$.\n- Transitional $T$: $(0.5,0,0)$, $(1.0,0,0)$, $(1.5,0,0)$, $(2.0,0,0)$, $(2.5,0,0)$.\n- Subtype $B$: $(3.1,0,0)$, $(3.2,0,0)$, $(3.3,0,0)$, $(3.4,0,0)$, $(3.5,0,0)$, $(3.6,0,0)$.\n- Subtype $C$: $(-3.0,0,0)$, $(-2.9,0,0)$, $(-2.8,0,0)$, $(-2.7,0,0)$, $(-2.6,0,0)$, $(-2.5,0,0)$.\n\nLet the total set of points be indexed from $0$ to $N-1$ in the exact order listed above (first all of $A$, then all of $T$, then all of $B$, then all of $C$), so $N=23$. Let $S_A$ be the index set of subtype $A$, $S_B$ the index set of subtype $B$, and $S_C$ the index set of subtype $C$.\n\nFor any integer $k$ with $1 \\le k \\le N-1$, define the undirected base neighborhood graph $G_k$ on the $N$ points by the following rule:\n- For each node $i$, compute Euclidean distances to all other nodes $j \\ne i$.\n- Determine the $k$ nearest neighbors of $i$ by sorting by increasing distance, breaking ties by increasing node index.\n- Add undirected edges so that $\\{i,j\\}$ is an edge of $G_k$ if $i$ is among the $k$ nearest neighbors of $j$ or $j$ is among the $k$ nearest neighbors of $i$.\n- Each edge $\\{i,j\\}$ in $G_k$ is endowed with its Euclidean length $d_{ij}$.\n\nFor any real parameter $m \\ge 0$ (interpreted as a minimal effective proximity, analogous to UMAP's $min\\_dist$), define the thresholded graph $H_{k,m}$ as the subgraph of $G_k$ that retains only those edges $\\{i,j\\}$ with $d_{ij} \\le m$.\n\nDefine the interpretation function $\\mathcal{I}(S_1,S_2;k,m)$ to be a boolean that is $True$ if and only if there is no path in $H_{k,m}$ from any node in $S_1$ to any node in $S_2$, and $False$ otherwise. In words, $\\mathcal{I}(S_1,S_2;k,m)$ returns $True$ when subtypes $S_1$ and $S_2$ are interpreted as distinct (disconnected) under the proximity threshold $m$ on the base graph of $k$-nearest neighbors, and $False$ when they are interpreted as merged or connected by a sequence of sufficiently short steps.\n\nYour task is to compute $\\mathcal{I}(S_1,S_2;k,m)$ for each parameter combination in the test suite below. All distances are Euclidean with no physical unit. The input is fully specified; there is no randomness.\n\nTest suite of parameter combinations:\n- Case $1$: $(S_1,S_2)=(S_A,S_B)$, $k=5$, $m=0.55$.\n- Case $2$: $(S_1,S_2)=(S_A,S_B)$, $k=5$, $m=0.60$.\n- Case $3$: $(S_1,S_2)=(S_A,S_B)$, $k=15$, $m=0.60$.\n- Case $4$: $(S_1,S_2)=(S_A,S_B)$, $k=15$, $m=0.55$.\n- Case $5$: $(S_1,S_2)=(S_B,S_C)$, $k=5$, $m=0.60$.\n\nYour program must produce a single line of output containing the results for Cases $1$ through $5$ in order, as a comma-separated list of booleans enclosed in square brackets, for example, $\"[True,False,True,True,False]\"$. No additional text may be printed.\n\nThis scenario is designed so that varying the proximity threshold $m$ alters connectivity between $S_A$ and $S_B$ due to the transitional cells $T$, while varying $k$ within the provided range does not change that connectivity outcome, thereby isolating the effect of the parameter $m$ on the biological interpretation of subtype separation.",
            "solution": "The problem requires an analysis of cell subtype connectivity in a synthetic single-cell dataset based on a graph-theoretic model inspired by manifold learning techniques like UMAP. The core of the problem is to determine whether two specified cell subtypes, $S_1$ and $S_2$, are connected by a path in a specially constructed graph, $H_{k,m}$. The result is a boolean value returned by the interpretation function $\\mathcal{I}(S_1, S_2; k, m)$, which is $True$ if the subtypes are disconnected and $False$ otherwise.\n\nFirst, we must formalize the problem by representing the locations of all $N=23$ cells as a set of points in $\\mathbb{R}^3$. The cell coordinates are provided in a specific order: $6$ cells of subtype $A$, $5$ transitional cells $T$, $6$ cells of subtype $B$, and $6$ cells of subtype $C$. We can store these coordinates in a single $N \\times 3$ matrix, which we denote as $P$. The index sets for the subtypes are thus defined as $S_A = \\{0, ..., 5\\}$, $S_B = \\{11, ..., 16\\}$, and $S_C = \\{17, ..., 22\\}$. The transitional cells $T$ occupy indices $\\{6, ..., 10\\}$.\n\nThe analysis proceeds in three main stages for each test case, which is defined by a pair of subtypes $(S_1, S_2)$, a number of neighbors $k$, and a proximity threshold $m$.\n\n1.  **Construction of the Base Neighborhood Graph $G_k$**:\n    The initial step is to model the local structure of the data by constructing a $k$-nearest neighbor ($k$-NN) graph, denoted $G_k$. This graph represents proximity relationships between cells. The foundation for this is the matrix of pairwise Euclidean distances, $D$, where each element $D_{ij}$ is the distance between cell $i$ and cell $j$. Since all given cell coordinates lie on a single axis, the Euclidean distance simplifies to the absolute difference of their first coordinates, i.e., $d_{ij} = |x_i - x_j|$.\n\n    For each cell $i$, we identify its $k$ nearest neighbors. The problem specifies a deterministic tie-breaking rule: if two cells are equidistant from $i$, the one with the smaller index is considered closer. This requires sorting potential neighbors first by distance, then by index.\n\n    The graph $G_k$ is undirected. An edge $\\{i, j\\}$ exists in $G_k$ if cell $j$ is among the $k$-NN of cell $i$, or if cell $i$ is among the $k$-NN of cell $j$. This symmetric definition ensures that relationships are mutual, a common practice in constructing such graphs. Each edge $\\{i,j\\}$ in $G_k$ is weighted by the distance $d_{ij}$.\n\n2.  **Thresholding to Form the Graph $H_{k,m}$**:\n    The parameter $m$ acts as a proximity threshold, analogous to the `min_dist` parameter in UMAP, which controls the minimum distance between points in the low-dimensional embedding and thus influences the visual separation of clusters. Here, we apply this concept directly to the base graph $G_k$ to form a new graph, $H_{k,m}$. This is a subgraph of $G_k$ that retains only those edges $\\{i,j\\}$ whose weight (length) $d_{ij}$ is less than or equal to the threshold $m$. This step effectively filters out connections that are deemed too long to represent a continuous biological process or a coherent cell state, leaving only connections that signify strong proximity.\n\n3.  **Connectivity Analysis of $H_{k,m}$**:\n    The final step is to determine if there is a path in $H_{k,m}$ connecting any cell in subtype $S_1$ to any cell in subtype $S_2$. If such a path exists, the subtypes are considered 'merged' or part of a continuum under the given parameters $(k, m)$, and $\\mathcal{I}$ is $False$. If no such path exists, they are considered distinct, and $\\mathcal{I}$ is $True$.\n\n    This is a standard graph connectivity problem. An efficient and robust algorithm for this task is the Union-Find (or Disjoint Set Union) data structure. We initialize each of the $N$ cells in its own disjoint set. Then, for each edge $\\{u, v\\}$ in the thresholded graph $H_{k,m}$, we merge the sets containing $u$ and $v$ using the `union` operation. After processing all edges, the cells are partitioned into connected components.\n\n    To determine if $S_1$ and $S_2$ are connected, we can obtain the set of root representatives for all cells in $S_1$ and the corresponding set for $S_2$. If the intersection of these two sets of roots is non-empty, it means at least one cell in $S_1$ shares a connected component with at least one cell in $S_2$, implying a path exists. In this case, $\\mathcal{I}$ is $False$. If the sets are disjoint, no such path exists, and $\\mathcal{I}$ is $True$.\n\nThis entire procedure is deterministic and will be applied to each of the five test cases provided. The critical parts of the data are the small distance ($d=0.5$) separating subtype $A$ from the transitional cells $T$, and the slightly larger distance ($d=0.6$) separating $T$ from subtype $B$. The connectivity between $A$ and $B$ will therefore hinge on whether the threshold $m$ is large enough to span these gaps. Connectivity involving subtype $C$ is unlikely, as it is spatially distant from the other groups.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as described. It computes subtype connectivity\n    for five test cases and prints the results in the specified format.\n    \"\"\"\n    \n    # Define cell coordinates as specified in the problem statement.\n    # The order is: 6 points for A, 5 for T, 6 for B, 6 for C.\n    points_data = [\n        # Subtype A\n        (-0.5, 0, 0), (-0.4, 0, 0), (-0.3, 0, 0), (-0.2, 0, 0), (-0.1, 0, 0), (0.0, 0, 0),\n        # Transitional T\n        (0.5, 0, 0), (1.0, 0, 0), (1.5, 0, 0), (2.0, 0, 0), (2.5, 0, 0),\n        # Subtype B\n        (3.1, 0, 0), (3.2, 0, 0), (3.3, 0, 0), (3.4, 0, 0), (3.5, 0, 0), (3.6, 0, 0),\n        # Subtype C\n        (-3.0, 0, 0), (-2.9, 0, 0), (-2.8, 0, 0), (-2.7, 0, 0), (-2.6, 0, 0), (-2.5, 0, 0)\n    ]\n    points = np.array(points_data, dtype=np.float64)\n    N = len(points)\n\n    # Define index sets for the cell subtypes.\n    S_A = set(range(0, 6))\n    S_B = set(range(11, 17))\n    S_C = set(range(17, 23))\n    \n    # Test suite parameters.\n    test_cases = [\n        {'S1': S_A, 'S2': S_B, 'k': 5, 'm': 0.55},  # Case 1\n        {'S1': S_A, 'S2': S_B, 'k': 5, 'm': 0.60},  # Case 2\n        {'S1': S_A, 'S2': S_B, 'k': 15, 'm': 0.60}, # Case 3\n        {'S1': S_A, 'S2': S_B, 'k': 15, 'm': 0.55}, # Case 4\n        {'S1': S_B, 'S2': S_C, 'k': 5, 'm': 0.60},  # Case 5\n    ]\n    \n    # Pre-compute the pairwise Euclidean distance matrix.\n    # Using numpy broadcasting for an efficient calculation.\n    # diff.shape = (N, N, 3), dists.shape = (N, N)\n    diff = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n    dists = np.sqrt(np.sum(diff**2, axis=-1))\n\n    results = []\n    \n    # Process each test case.\n    for case in test_cases:\n        S1, S2, k, m = case['S1'], case['S2'], case['k'], case['m']\n\n        # --- Step 1: Construct the base neighborhood graph G_k ---\n        # Gk_edges will store unique undirected edges as sorted tuples (i, j).\n        Gk_edges = set()\n        for i in range(N):\n            # Get distances and indices for all other points.\n            all_indices = np.arange(N)\n            mask = all_indices != i\n            other_indices = all_indices[mask]\n            other_dists = dists[i, mask]\n            \n            # Sort neighbors by distance, breaking ties by index.\n            # np.lexsort sorts by the last key first, so it's (secondary_key, primary_key).\n            sorted_neighbor_indices = other_indices[np.lexsort((other_indices, other_dists))]\n            \n            # Get the k nearest neighbors.\n            k_neighbors = sorted_neighbor_indices[:k]\n            \n            # Add directed k-NN relations to the set.\n            # The union of these relations will form the undirected graph G_k.\n            for j in k_neighbors:\n                Gk_edges.add(tuple(sorted((i, j))))\n\n        # --- Step 2  3: Threshold H_{k,m} and check connectivity with Union-Find ---\n        parent = list(range(N))\n        \n        # Path compression for find operation\n        def find(i):\n            if parent[i] == i:\n                return i\n            parent[i] = find(parent[i])\n            return parent[i]\n\n        # Union operation\n        def union(i, j):\n            root_i = find(i)\n            root_j = find(j)\n            if root_i != root_j:\n                parent[root_j] = root_i\n\n        # Build connected components for H_{k,m}\n        for i, j in Gk_edges:\n            if dists[i, j] = m:\n                union(i, j)\n\n        # Check if S1 and S2 are connected.\n        # They are connected if any node in S1 is in the same component as any node in S2.\n        # This is true if the sets of their component roots have a non-empty intersection.\n        s1_roots = {find(i) for i in S1}\n        s2_roots = {find(j) for j in S2}\n\n        # The interpretation function I is True if disconnected.\n        is_disconnected = s1_roots.isdisjoint(s2_roots)\n        results.append(is_disconnected)\n        \n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}