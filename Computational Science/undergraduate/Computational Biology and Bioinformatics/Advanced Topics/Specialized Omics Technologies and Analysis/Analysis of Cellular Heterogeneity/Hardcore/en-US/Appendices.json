{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in analyzing single-cell data is ensuring data quality by identifying and removing compromised cells. In practice, cells that are stressed or dying often exhibit a high proportion of mitochondrial gene transcripts, a key quality control (QC) metric. This exercise provides a hands-on method for programmatically identifying such low-quality cell clusters based on precise, quantitative criteria, a foundational skill for any robust single-cell analysis workflow .",
            "id": "2371645",
            "problem": "You are given multiple independent single-cell ribonucleic acid sequencing (scRNA-seq) scenarios. In each scenario, you have a nonnegative integer cell-by-gene count matrix, a specification of which genes are mitochondrial and ribosomal, and a cluster label for each cell. You must identify the single cluster that best represents a group of \"stressed\" or dying cells, based strictly on the proportional expression of mitochondrial and ribosomal genes.\n\nStart from the following fundamental base:\n- In scRNA-seq, each cell has total messenger ribonucleic acid (mRNA) counts. The mitochondrial fraction for a cell is the sum of counts over mitochondrial genes divided by the total counts for that cell. The ribosomal fraction is defined analogously for ribosomal genes. For any cell with zero total counts, define both fractions as zero.\n- A proportion is the ratio of a subset count to a total count. A cluster-wise proportion of stressed cells is the number of stressed cells in the cluster divided by the total number of cells in that cluster.\n\nFormally, let there be $n$ cells and $g$ genes, with a count matrix $X \\in \\mathbb{N}_0^{n \\times g}$. Let $M \\subseteq \\{0,1,\\dots,g-1\\}$ denote the mitochondrial gene indices and $R \\subseteq \\{0,1,\\dots,g-1\\}$ the ribosomal gene indices. For each cell $i \\in \\{0,1,\\dots,n-1\\}$, let the total count be $T_i = \\sum_{j=0}^{g-1} X_{i,j}$. Define the mitochondrial fraction $f^{\\mathrm{mt}}_i$ and ribosomal fraction $f^{\\mathrm{rb}}_i$ by\n$$\nf^{\\mathrm{mt}}_i = \n\\begin{cases}\n\\frac{\\sum_{j \\in M} X_{i,j}}{T_i}, & \\text{if } T_i > 0, \\\\\n0, & \\text{if } T_i = 0,\n\\end{cases}\n\\qquad\nf^{\\mathrm{rb}}_i = \n\\begin{cases}\n\\frac{\\sum_{j \\in R} X_{i,j}}{T_i}, & \\text{if } T_i > 0, \\\\\n0, & \\text{if } T_i = 0.\n\\end{cases}\n$$\nGiven thresholds $\\theta_{\\mathrm{mt}} \\in [0,1]$ and $\\theta_{\\mathrm{rb}} \\in [0,1]$, define a stressed indicator for each cell:\n$$\nS_i =\n\\begin{cases}\n1, & \\text{if } f^{\\mathrm{mt}}_i \\ge \\theta_{\\mathrm{mt}} \\ \\text{or} \\ f^{\\mathrm{rb}}_i \\ge \\theta_{\\mathrm{rb}}, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nLet cluster labels be given by a vector $\\mathbf{c} \\in \\mathbb{Z}^{n}$, where $\\mathbf{c}_i$ is the label of cell $i$. For any cluster label $k$, define the stressed proportion in cluster $k$ as\n$$\np_k = \\frac{\\sum_{i: \\mathbf{c}_i = k} S_i}{\\#\\{i: \\mathbf{c}_i = k\\}},\n$$\nand define a severity score for cluster $k$ as the mean over cells in that cluster of $\\max(f^{\\mathrm{mt}}_i, f^{\\mathrm{rb}}_i)$.\n\nYour task is, for each scenario, to select the unique cluster label $k^\\star$ that maximizes $p_k$. In case of a tie in $p_k$, choose the cluster with the larger severity score. If there is still a tie, choose the smallest cluster label (numerically). All proportions must be treated as decimals in $[0,1]$.\n\nImplement a program that applies this rule to the following test suite. For each test case $t \\in \\{1,2,3,4\\}$, you are given $X^{(t)}$, $M^{(t)}$, $R^{(t)}$, $\\mathbf{c}^{(t)}$, $\\theta_{\\mathrm{mt}}^{(t)}$, and $\\theta_{\\mathrm{rb}}^{(t)}$:\n\n- Test case $1$ (happy path with clear winner):\n  - $X^{(1)} = \\begin{bmatrix}\n  5 & 5 & 30 & 20 & 10 & 10 \\\\\n  2 & 3 & 25 & 25 & 5 & 10 \\\\\n  1 & 1 & 10 & 15 & 20 & 2 \\\\\n  30 & 10 & 5 & 5 & 0 & 0 \\\\\n  1 & 0 & 0 & 0 & 10 & 10 \\\\\n  20 & 0 & 0 & 0 & 0 & 0\n  \\end{bmatrix}$\n  - $M^{(1)} = \\{0,1\\}$, $R^{(1)} = \\{2,3\\}$\n  - $\\mathbf{c}^{(1)} = (0,0,0,1,1,1)$\n  - $\\theta_{\\mathrm{mt}}^{(1)} = 0.2$, $\\theta_{\\mathrm{rb}}^{(1)} = 0.4$\n\n- Test case $2$ (tie on stressed proportion; resolved by severity and then by label if needed):\n  - $X^{(2)} = \\begin{bmatrix}\n  6 & 0 & 4 & 0 \\\\\n  0 & 0 & 5 & 5 \\\\\n  8 & 0 & 2 & 0 \\\\\n  10 & 0 & 0 & 0\n  \\end{bmatrix}$\n  - $M^{(2)} = \\{0,1\\}$, $R^{(2)} = \\{2,3\\}$\n  - $\\mathbf{c}^{(2)} = (0,0,1,1)$\n  - $\\theta_{\\mathrm{mt}}^{(2)} = 0.9$, $\\theta_{\\mathrm{rb}}^{(2)} = 0.9$\n\n- Test case $3$ (zero-total-count cell; must be handled without division by zero):\n  - $X^{(3)} = \\begin{bmatrix}\n  0 & 0 & 0 \\\\\n  1 & 0 & 1 \\\\\n  0 & 2 & 0\n  \\end{bmatrix}$\n  - $M^{(3)} = \\{0\\}$, $R^{(3)} = \\{1\\}$\n  - $\\mathbf{c}^{(3)} = (0,0,1)$\n  - $\\theta_{\\mathrm{mt}}^{(3)} = 0.5$, $\\theta_{\\mathrm{rb}}^{(3)} = 0.5$\n\n- Test case $4$ (all clusters with zero stressed proportion; resolved by label after tie on severity):\n  - $X^{(4)} = \\begin{bmatrix}\n  0 & 0 & 5 & 5 \\\\\n  1 & 0 & 9 & 0 \\\\\n  0 & 1 & 9 & 0 \\\\\n  0 & 0 & 1 & 9\n  \\end{bmatrix}$\n  - $M^{(4)} = \\{0\\}$, $R^{(4)} = \\{1\\}$\n  - $\\mathbf{c}^{(4)} = (0,0,1,1)$\n  - $\\theta_{\\mathrm{mt}}^{(4)} = 0.9$, $\\theta_{\\mathrm{rb}}^{(4)} = 0.9$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_t$ is the selected cluster label $k^\\star$ for test case $t$. The answers are integers. No additional output or whitespace is permitted.",
            "solution": "The problem presented is a well-defined computational task in bioinformatics, specifically in the quality control phase of single-cell ribonucleic acid sequencing (scRNA-seq) data analysis. It is scientifically grounded, mathematically coherent, and all necessary parameters are provided. The problem is therefore deemed valid and a solution will be presented.\n\nThe objective is to identify a single cluster of cells that is most representative of a \"stressed\" or low-quality cellular state. This identification is based on a hierarchical decision rule involving three metrics: the proportion of stressed cells within a cluster, a cluster-level severity score, and the numerical cluster label. The methodology is deterministic and guarantees a unique solution.\n\nThe solution is implemented by first performing calculations at the single-cell level, and then aggregating these results at the cluster level to make a final decision. The process is outlined below.\n\nFirst, for each cell $i \\in \\{0, 1, \\dots, n-1\\}$, we compute several key metrics based on the provided count matrix $X$, the sets of mitochondrial ($M$) and ribosomal ($R$) gene indices.\n\nThe total count for cell $i$, denoted $T_i$, is the sum of its gene counts:\n$$\nT_i = \\sum_{j=0}^{g-1} X_{i,j}\n$$\nThe mitochondrial and ribosomal fractions, $f^{\\mathrm{mt}}_i$ and $f^{\\mathrm{rb}}_i$, are calculated as the ratio of mitochondrial or ribosomal gene counts to the total count. As stipulated, if a cell has a total count $T_i=0$, both fractions are defined to be $0$ to prevent division by zero. This is a crucial step for numerical stability and biological interpretation, as a cell with no detected transcripts provides no information about fractional expression.\n$$\nf^{\\mathrm{mt}}_i = \\begin{cases} \\frac{\\sum_{j \\in M} X_{i,j}}{T_i}, & \\text{if } T_i > 0 \\\\ 0, & \\text{if } T_i = 0 \\end{cases}\n\\qquad\nf^{\\mathrm{rb}}_i = \\begin{cases} \\frac{\\sum_{j \\in R} X_{i,j}}{T_i}, & \\text{if } T_i > 0 \\\\ 0, & \\text{if } T_i = 0 \\end{cases}\n$$\nA cell is defined as \"stressed\" if either its mitochondrial fraction or its ribosomal fraction exceeds a given threshold, $\\theta_{\\mathrm{mt}}$ or $\\theta_{\\mathrm{rb}}$, respectively. We compute a binary stressed indicator, $S_i$, for each cell:\n$$\nS_i = \\begin{cases} 1, & \\text{if } f^{\\mathrm{mt}}_i \\ge \\theta_{\\mathrm{mt}} \\ \\text{or} \\ f^{\\mathrm{rb}}_i \\ge \\theta_{\\mathrm{rb}} \\\\ 0, & \\text{otherwise} \\end{cases}\n$$\nAdditionally, we compute a per-cell severity metric, defined as $\\max(f^{\\mathrm{mt}}_i, f^{\\mathrm{rb}}_i)$, which will be used in the tie-breaking procedure.\n\nSecond, with these per-cell metrics computed, we proceed to cluster-level analysis. For each unique cluster label $k$ present in the vector $\\mathbf{c}$, we calculate two aggregate scores. Let $C_k = \\{i \\mid \\mathbf{c}_i = k\\}$ be the set of indices of cells belonging to cluster $k$.\n\nThe primary metric is the stressed proportion, $p_k$, defined as the fraction of stressed cells in the cluster:\n$$\np_k = \\frac{\\sum_{i \\in C_k} S_i}{|C_k|}\n$$\nThe secondary, tie-breaking metric is the cluster's severity score, defined as the mean of the per-cell severity metrics over all cells in that cluster:\n$$\n\\text{severity}_k = \\frac{\\sum_{i \\in C_k} \\max(f^{\\mathrm{mt}}_i, f^{\\mathrm{rb}}_i)}{|C_k|}\n$$\nFinally, we apply the specified decision rule to select the optimal cluster, $k^\\star$. We iterate through all unique cluster labels and identify the one that maximizes the stressed proportion $p_k$.\nIf a tie occurs where multiple clusters share the same maximal $p_k$, the tie is resolved by selecting the cluster with the highest severity score among the tied candidates.\nIf a further tie persists (i.e., multiple clusters have the same maximal $p_k$ and the same maximal severity score), the final tie-breaker is to select the cluster with the smallest numerical label.\n\nThis hierarchical sorting process can be conceptualized as ordering all clusters based on the tuple $(p_k, \\text{severity}_k, -k)$ in descending lexicographical order. The winning cluster is the one that appears first in this sorted list. The negative sign on $k$ is used to handle the ascending order requirement for the final tie-breaker.\n\nThe algorithm is implemented in Python, leveraging the `numpy` library for efficient vectorized computations of total counts, fractional expressions, and cluster-level aggregations. This approach is not only computationally efficient but also directly mirrors the mathematical formalism described. The solution is applied independently to each test case provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": np.array([\n                [5, 5, 30, 20, 10, 10],\n                [2, 3, 25, 25, 5, 10],\n                [1, 1, 10, 15, 20, 2],\n                [30, 10, 5, 5, 0, 0],\n                [1, 0, 0, 0, 10, 10],\n                [20, 0, 0, 0, 0, 0]\n            ]),\n            \"M\": [0, 1],\n            \"R\": [2, 3],\n            \"c\": np.array([0, 0, 0, 1, 1, 1]),\n            \"theta_mt\": 0.2,\n            \"theta_rb\": 0.4\n        },\n        {\n            \"X\": np.array([\n                [6, 0, 4, 0],\n                [0, 0, 5, 5],\n                [8, 0, 2, 0],\n                [10, 0, 0, 0]\n            ]),\n            \"M\": [0, 1],\n            \"R\": [2, 3],\n            \"c\": np.array([0, 0, 1, 1]),\n            \"theta_mt\": 0.9,\n            \"theta_rb\": 0.9\n        },\n        {\n            \"X\": np.array([\n                [0, 0, 0],\n                [1, 0, 1],\n                [0, 2, 0]\n            ]),\n            \"M\": [0],\n            \"R\": [1],\n            \"c\": np.array([0, 0, 1]),\n            \"theta_mt\": 0.5,\n            \"theta_rb\": 0.5\n        },\n        {\n            \"X\": np.array([\n                [0, 0, 5, 5],\n                [1, 0, 9, 0],\n                [0, 1, 9, 0],\n                [0, 0, 1, 9]\n            ]),\n            \"M\": [0],\n            \"R\": [1],\n            \"c\": np.array([0, 0, 1, 1]),\n            \"theta_mt\": 0.9,\n            \"theta_rb\": 0.9\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        winner = _calculate_best_cluster(\n            case[\"X\"],\n            case[\"M\"],\n            case[\"R\"],\n            case[\"c\"],\n            case[\"theta_mt\"],\n            case[\"theta_rb\"]\n        )\n        results.append(winner)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef _calculate_best_cluster(X, M, R, c, theta_mt, theta_rb):\n    \"\"\"\n    Processes a single scRNA-seq scenario to find the most stressed cluster.\n\n    Args:\n        X (np.ndarray): Cell-by-gene count matrix.\n        M (list): List of mitochondrial gene indices.\n        R (list): List of ribosomal gene indices.\n        c (np.ndarray): Array of cluster labels for each cell.\n        theta_mt (float): Mitochondrial fraction threshold.\n        theta_rb (float): Ribosomal fraction threshold.\n\n    Returns:\n        int: The label of the winning cluster.\n    \"\"\"\n    # Step 1: Per-cell calculations\n    total_counts = X.sum(axis=1)\n    \n    # Use lists M and R to sum counts for specified genes\n    mt_counts = X[:, M].sum(axis=1) if M else np.zeros(X.shape[0])\n    rb_counts = X[:, R].sum(axis=1) if R else np.zeros(X.shape[0])\n\n    # Calculate fractions, handling division by zero as specified\n    f_mt = np.divide(mt_counts, total_counts, out=np.zeros_like(mt_counts, dtype=float), where=total_counts != 0)\n    f_rb = np.divide(rb_counts, total_counts, out=np.zeros_like(rb_counts, dtype=float), where=total_counts != 0)\n\n    # Determine stressed indicator for each cell\n    S = (f_mt >= theta_mt) | (f_rb >= theta_rb)\n\n    # Calculate per-cell severity\n    per_cell_severity = np.maximum(f_mt, f_rb)\n\n    # Step 2: Cluster-level aggregation\n    unique_clusters = np.unique(c)\n    cluster_scores = []\n\n    for k in unique_clusters:\n        mask = (c == k)\n        \n        # Stressed proportion for cluster k\n        p_k = np.mean(S[mask])\n        \n        # Severity score for cluster k\n        severity_k = np.mean(per_cell_severity[mask])\n        \n        cluster_scores.append((p_k, severity_k, k))\n\n    # Step 3: Select the best cluster using the hierarchical rule\n    # 1. Maximize p_k (sort descending)\n    # 2. Maximize severity_k (sort descending)\n    # 3. Minimize k (sort ascending)\n    # The key function implements this logic: (-p_k, -severity_k, k) ensures correct sorting order.\n    cluster_scores.sort(key=lambda x: (-x[0], -x[1], x[2]))\n    \n    # The best cluster is the first one in the sorted list\n    best_cluster_label = cluster_scores[0][2]\n    \n    return best_cluster_label\n\nsolve()\n```"
        },
        {
            "introduction": "Once data quality has been assured, the central task in analyzing cellular heterogeneity is to partition cells into distinct groups or clusters, which often correspond to different cell types or states. This practice explores a powerful graph-based clustering algorithm, the Louvain method, which is widely used for community detection in large networks. You will implement a sophisticated strategy to select the optimal 'resolution' parameter, $\\gamma$, by evaluating the stability of the resulting clusters, ensuring the biological relevance of the identified cell populations .",
            "id": "2371617",
            "problem": "You are given the task of designing and implementing an algorithm to automatically suggest the optimal resolution parameter for Louvain clustering based on the stability of the resulting partitions when analyzing cellular heterogeneity from single-cell similarity graphs. Your implementation must be a complete and runnable program that performs the following tasks for a set of predefined test cases.\n\nFundamental base and definitions:\n- A cell-by-cell similarity graph is represented as an undirected weighted graph with adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij} \\ge 0$, $A_{ij} = A_{ji}$, and $A_{ii} = 0$.\n- The degree of node $i$ is $k_i = \\sum_{j=1}^n A_{ij}$, and $m = \\frac{1}{2} \\sum_{i,j=1}^n A_{ij}$ is the total edge weight.\n- The Louvain objective optimized is the generalized modularity with resolution parameter $\\gamma > 0$:\n$$\nQ(\\gamma) \\;=\\; \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\Big( A_{ij} \\;-\\; \\gamma \\frac{k_i k_j}{2m} \\Big) \\, \\mathbb{1}\\{c_i = c_j\\},\n$$\nwhere $c_i$ is the community label of node $i$, and $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n- Cluster stability is quantified using the Adjusted Rand Index (ARI). For two partitions of $n$ items with contingency table entries $n_{ij}$, row sums $a_i = \\sum_j n_{ij}$, and column sums $b_j = \\sum_i n_{ij}$, the Adjusted Rand Index is\n$$\n\\mathrm{ARI} \\;=\\; \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} \\;-\\; \\frac{\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}}{\\frac{1}{2}\\Big( \\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2} \\Big) \\;-\\; \\frac{\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}}{\\binom{n}{2}}}.\n$$\n- The stability score for a given $\\gamma$ is defined as the mean pairwise Adjusted Rand Index across $R$ independent runs (distinct random seeds) of a Louvain-like local moving procedure.\n\nAlgorithmic requirements:\n- Implement a Louvain-like local moving algorithm that, for a fixed $\\gamma$, starts with each node in its own community and repeatedly attempts to move individual nodes to neighboring communities if doing so increases $Q(\\gamma)$. At each step:\n  - Visit nodes in a random order.\n  - For a node $i$, consider moving it to the community of any neighboring node (and the option to stay). For each candidate, compute the resulting modularity $Q(\\gamma)$ and choose the move that yields the largest increase. Break ties randomly. Apply the best move if and only if it strictly increases $Q(\\gamma)$.\n  - Repeat passes until a full pass yields no moves that increase $Q(\\gamma)$.\n- For each $\\gamma$ in a candidate set, run the local moving algorithm $R$ times with different random seeds. Compute:\n  - The mean pairwise Adjusted Rand Index across the $R$ partitions as the stability score $S(\\gamma)$.\n  - The mean modularity $\\overline{Q}(\\gamma)$ over the $R$ runs.\n  - The mean number of clusters $\\overline{K}(\\gamma)$ over the $R$ runs.\n- Selection rule for the suggested resolution:\n  - Choose $\\gamma^\\star$ that maximizes $S(\\gamma)$.\n  - If multiple $\\gamma$ values share the maximum $S(\\gamma)$ within a tolerance of $10^{-8}$, choose the one with the largest $\\overline{Q}(\\gamma)$.\n  - If still tied, choose the smallest $\\gamma$.\n\nTest suite:\nImplement and evaluate your method on the following three synthetic graphs. Each graph is defined by block structure parameters from which you must construct $A$.\n- Case $1$ (two well-separated groups):\n  - Group sizes: $[5, 5]$.\n  - Within-group edge weight: $1.0$.\n  - Between-group edge weight: $0.05$ (uniformly between any two nodes from different groups).\n  - Candidate $\\gamma$ values: $[0.5, 1.0, 1.5, 2.0]$.\n  - Number of runs per $\\gamma$: $R = 6$.\n- Case $2$ (homogeneous graph):\n  - A complete graph on $n = 9$ nodes with all off-diagonal weights $1.0$ and diagonal entries $0$.\n  - Candidate $\\gamma$ values: $[0.1, 0.5, 1.0, 2.0]$.\n  - Number of runs per $\\gamma$: $R = 6$.\n- Case $3$ (three moderately separated groups):\n  - Group sizes: $[4, 4, 4]$.\n  - Within-group edge weight: $1.0$.\n  - Between-group edge weight: $0.1$ (uniformly between any two nodes from different groups).\n  - Candidate $\\gamma$ values: $[0.5, 1.0, 1.5, 2.5]$.\n  - Number of runs per $\\gamma$: $R = 6$.\n\nConstruction of $A$ for block graphs:\n- For a block model with group sizes $[s_1, s_2, \\dots, s_g]$, set $A_{ij} = 1.0$ if nodes $i$ and $j$ are in the same group and $i \\ne j$, and $A_{ij} = w_{\\text{between}}$ if they are in different groups. Set all diagonals to $0$.\n- For the homogeneous graph, set $A_{ij} = 1.0$ for all $i \\ne j$, and $A_{ii} = 0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the suggested $\\gamma^\\star$ for each of the three cases, in order, as a comma-separated list enclosed in square brackets. For example, the output should look like $[1.0,0.5,1.5]$.\n- Each entry must be a single floating-point number equal to one of the candidate $\\gamma$ values for that case.\n\nAll computations are dimensionless; no physical units are involved. Angles are not present. Percentages, if any derived, must be represented as decimals, but the final outputs here are floating-point numbers as specified.",
            "solution": "The problem statement has been rigorously evaluated and is determined to be **valid**. It is scientifically grounded in established principles of network science and computational biology, is mathematically and algorithmically well-posed, and provides a complete and consistent set of requirements for a solvable computational task. There are no violations of fundamental principles, ambiguous definitions, or missing information that would preclude a unique and verifiable solution.\n\nThe task is to determine the optimal resolution parameter $\\gamma$ for Louvain community detection by assessing the stability of partitions. The approach is to execute a local moving algorithm multiple times for each candidate $\\gamma$, and then select the $\\gamma$ that produces the most consistent (stable) clustering results across runs.\n\nThe methodology is implemented through the following sequence of steps for each test case.\n\n**1. Graph Representation**\n\nFor each test case, an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ is constructed based on the provided block model parameters. Given group sizes, a within-group edge weight $w_{\\text{in}}$, and a between-group edge weight $w_{\\text{out}}$, the matrix elements $A_{ij}$ are defined as:\n$$\nA_{ij} = \\begin{cases}\n    w_{\\text{in}} & \\text{if nodes } i \\text{ and } j \\text{ are in the same group and } i \\neq j \\\\\n    w_{\\text{out}} & \\text{if nodes } i \\text{ and } j \\text{ are in different groups} \\\\\n    0 & \\text{if } i = j\n\\end{cases}\n$$\nThe degree of each node, $k_i = \\sum_{j=1}^n A_{ij}$, and the total edge weight of the graph, $m = \\frac{1}{2}\\sum_{i,j} A_{ij}$, are pre-calculated as they are fundamental to modularity computations.\n\n**2. Louvain-like Local Moving Algorithm**\n\nThe core of the clustering process is a local moving heuristic designed to optimize the generalized modularity function $Q(\\gamma)$:\n$$\nQ(\\gamma) = \\frac{1}{2m} \\sum_{i,j} \\left( A_{ij} - \\gamma \\frac{k_i k_j}{2m} \\right) \\mathbb{1}\\{c_i = c_j\\}\n$$\nDirectly re-calculating $Q(\\gamma)$ for every potential node move is computationally prohibitive. Instead, we compute the change in modularity, $\\Delta Q$, that results from moving a node $i$ from its current community $C_{\\text{old}}$ to a candidate community $C_{\\text{new}}$. This change is given by the efficient formula:\n$$\n\\Delta Q = \\frac{k_{i, \\text{new}} - k_{i, \\text{old}}}{m} - \\frac{\\gamma k_i}{2m^2} \\left( \\Sigma_{\\text{tot}, \\text{new}} - \\Sigma_{\\text{tot}, \\text{old}} + k_i \\right)\n$$\nwhere $k_{i,C}$ is the sum of weights of edges connecting node $i$ to nodes in community $C$, and $\\Sigma_{\\text{tot}, C}$ is the sum of degrees of all nodes in community $C$.\n\nThe algorithm proceeds as follows:\n-   **Initialization**: Each node $i$ is assigned to its own unique community, $c_i = i$.\n-   **Iteration**: The algorithm repeatedly passes through all nodes. In each pass:\n    1.  The nodes are visited in a random order to avoid any bias from a fixed ordering.\n    2.  For each node $i$, we consider moving it to the community of one of its neighbors.\n    3.  The $\\Delta Q$ is calculated for each potential move.\n    4.  If the maximum $\\Delta Q$ is strictly positive, the move is executed. Ties for the maximal gain are broken randomly.\n    5.  Community-level statistics, such as $\\Sigma_{\\text{tot}, C}$, are updated after each move.\n-   **Termination**: The process halts when a full pass over all nodes results in no moves that yield a strict increase in $Q(\\gamma)$, indicating a local optimum has been reached.\n\n**3. Stability Quantification using Adjusted Rand Index (ARI)**\n\nFor each candidate $\\gamma$, the local moving algorithm is executed $R$ times, each with a different random seed, yielding a set of $R$ partitions. The stability of clustering at this resolution is quantified by the mean pairwise Adjusted Rand Index (ARI) over all pairs of these partitions. The ARI is a measure of similarity between two data clusterings, corrected for chance. Given two partitions, a contingency table $n_{ij}$ is formed. The ARI is computed using the formula:\n$$\n\\mathrm{ARI} = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - \\frac{[\\sum_i \\binom{a_i}{2}][\\sum_j \\binom{b_j}{2}]}{\\binom{n}{2}}}{\\frac{1}{2}[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - \\frac{[\\sum_i \\binom{a_i}{2}][\\sum_j \\binom{b_j}{2}]}{\\binom{n}{2}}}\n$$\nwhere $a_i = \\sum_j n_{ij}$ and $b_j = \\sum_i n_{ij}$ are the marginal sums of the contingency table, and $n$ is the total number of nodes. The binomial coefficient $\\binom{k}{2}$ is calculated as $k(k-1)/2$. The stability score for a given $\\gamma$ is then $S(\\gamma) = \\text{mean}(\\text{ARI}_{jk})$ for $1 \\le j < k \\le R$.\n\n**4. Optimal Resolution Selection**\n\nThe optimal resolution parameter, $\\gamma^\\star$, is selected from the set of candidate values based on a deterministic, three-tiered rule:\n1.  Primary Criterion: Select the value(s) of $\\gamma$ that maximize the stability score $S(\\gamma)$. A tolerance of $10^{-8}$ is used to identify ties for the maximum.\n2.  First Tie-Breaker: If multiple $\\gamma$ values are tied, select from this subset the one that maximizes the mean modularity, $\\overline{Q}(\\gamma)$, averaged over the $R$ runs.\n3.  Second Tie-Breaker: If a tie persists, select the smallest $\\gamma$ value among the remaining candidates.\n\nThis procedure is systematically applied to each of the three test cases specified in the problem to derive the final suggested resolutions.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_graph(group_sizes, w_in, w_out):\n    \"\"\"Constructs the adjacency matrix for a block model graph.\"\"\"\n    n_nodes = sum(group_sizes)\n    A = np.zeros((n_nodes, n_nodes))\n    node_to_group = np.zeros(n_nodes, dtype=int)\n    \n    start_idx = 0\n    for i, size in enumerate(group_sizes):\n        end_idx = start_idx + size\n        node_to_group[start_idx:end_idx] = i\n        start_idx = end_idx\n\n    for i in range(n_nodes):\n        for j in range(i + 1, n_nodes):\n            if node_to_group[i] == node_to_group[j]:\n                weight = w_in\n            else:\n                weight = w_out\n            A[i, j] = A[j, i] = weight\n            \n    return A\n\ndef n_choose_2(n):\n    \"\"\"Computes the binomial coefficient C(n, 2).\"\"\"\n    if n < 2:\n        return 0\n    return n * (n - 1) // 2\n\ndef calculate_ari(labels_1, labels_2):\n    \"\"\"Calculates the Adjusted Rand Index between two clusterings.\"\"\"\n    n = len(labels_1)\n    if n <= 1:\n        return 1.0\n\n    # Relabel to contiguous integers starting from 0\n    _, u_labels_1 = np.unique(labels_1, return_inverse=True)\n    _, u_labels_2 = np.unique(labels_2, return_inverse=True)\n    \n    n_labels_1 = np.max(u_labels_1) + 1 if len(u_labels_1) > 0 else 0\n    n_labels_2 = np.max(u_labels_2) + 1 if len(u_labels_2) > 0 else 0\n\n    contingency = np.zeros((n_labels_1, n_labels_2), dtype=np.int64)\n    for i in range(n):\n        contingency[u_labels_1[i], u_labels_2[i]] += 1\n\n    sum_comb_nij = np.sum([n_choose_2(nij) for nij in contingency.flat])\n    \n    sum_comb_a = np.sum([n_choose_2(k) for k in np.sum(contingency, axis=1)])\n    sum_comb_b = np.sum([n_choose_2(k) for k in np.sum(contingency, axis=0)])\n    \n    total_comb = n_choose_2(n)\n    if total_comb == 0:\n        return 1.0\n\n\n    expected_index = (sum_comb_a * sum_comb_b) / total_comb\n    max_index = (sum_comb_a + sum_comb_b) / 2\n    \n    denominator = max_index - expected_index\n    if denominator == 0:\n        # This occurs on trivial clusterings. Conventionally ARI is 0,\n        # unless it is a perfect match (numerator is also 0).\n        return 0.0\n\n    numerator = sum_comb_nij - expected_index\n    return numerator / denominator\n\n\ndef calculate_modularity(A, gamma, communities, two_m):\n    \"\"\"Calculates the modularity of a given partition.\"\"\"\n    if two_m == 0:\n        return 0.0\n    \n    Q = 0.0\n    unique_comms = np.unique(communities)\n    k = A.sum(axis=1)\n\n    for comm_id in unique_comms:\n        nodes_in_comm = np.where(communities == comm_id)[0]\n        subgraph = A[np.ix_(nodes_in_comm, nodes_in_comm)]\n        sigma_in = np.sum(subgraph) / 2.0  # Sum of internal edge weights\n        sigma_tot = np.sum(k[nodes_in_comm])\n        \n        term1 = sigma_in / two_m\n        term2 = (sigma_tot / two_m)**2\n        Q += term1 - gamma * term2\n\n    return Q * 2 # The formula Q = sum(...) represents sum over pairs, we summed over edges, so double it. Wait no.\n    # The formula is 1/2m * sum(...) -> sum (sigma_in/2m - (gamma*sigma_tot^2)/(2m)^2)\n    # The sum of weights is Sum(A_ij) over i,j in C. Sum(subgraph) is this. But A is symmetric.\n    # sum_ij in C A_ij = 2 * sum_edges_in_C. Let's use Blondel's definition.\n    # Q = sum_c [ (sum_in_c / 2m) - (sum_tot_c / 2m)^2 ]\n    # My sum_in sums weights of edges (not pairs). Total sum of weights is m. sum(subgraph) is 2 * sum_in.\n    # So sum_in is sum(subgraph)/2. And total edge weight m is two_m/2.\n    # Q_c = (sum(subgraph)/2 / m) - gamma * (sigma_tot / 2m)^2\n    # Q_c = sum(subgraph)/two_m - gamma * (sigma_tot / two_m)^2\n    # So sum over communities. Let's re-verify.\n    mod = 0.0\n    for comm_id in unique_comms:\n        nodes_in_comm = np.where(communities == comm_id)[0]\n        sigma_tot = np.sum(k[nodes_in_comm])\n        \n        # Sum of weights within community\n        sum_in_comm_weights = 0\n        for i in range(len(nodes_in_comm)):\n            for j in range(i + 1, len(nodes_in_comm)):\n                u, v = nodes_in_comm[i], nodes_in_comm[j]\n                sum_in_comm_weights += A[u,v]\n\n        mod += (sum_in_comm_weights / (two_m / 2.0)) - gamma * (sigma_tot / two_m)**2\n    \n    return mod\n\n\ndef louvain_local_moving(A, gamma, rng):\n    \"\"\"Performs Louvain-like local moving to optimize modularity.\"\"\"\n    n = A.shape[0]\n    k = A.sum(axis=1)\n    two_m = k.sum()\n\n    if two_m == 0:\n        return np.arange(n)\n\n    communities = np.arange(n)\n    sigma_tot = np.copy(k) # Sigma_tot for each community\n    \n    while True:\n        moved = False\n        node_order = rng.permutation(n)\n        \n        for i in node_order:\n            old_comm_id = communities[i]\n            ki = k[i]\n            \n            # Efficiently compute sum of weights to each community\n            k_i_comms = np.bincount(communities, weights=A[i], minlength=n)\n            k_i_in = k_i_comms[old_comm_id]\n\n            neighbors = np.where(A[i] > 0)[0]\n            cand_comm_ids = np.unique(communities[neighbors])\n            \n            best_gain = 0.0\n            best_comm_id = old_comm_id\n            \n            potential_gains = []\n            potential_comms = []\n\n            for cand_comm_id in cand_comm_ids:\n                if cand_comm_id == old_comm_id:\n                    continue\n\n                k_i_to = k_i_comms[cand_comm_id]\n                sigma_tot_old = sigma_tot[old_comm_id]\n                sigma_tot_new = sigma_tot[cand_comm_id]\n\n                gain = (k_i_to - k_i_in) - gamma * ki * (sigma_tot_new - sigma_tot_old + ki) / two_m\n                potential_gains.append(gain)\n                potential_comms.append(cand_comm_id)\n            \n            if potential_gains:\n                max_gain = np.max(potential_gains)\n                if max_gain > 1e-12: # Strict increase, with a small tolerance for floating point\n                    max_indices = np.where(np.abs(potential_gains - max_gain) < 1e-12)[0]\n                    chosen_idx = rng.choice(max_indices)\n                    best_gain = potential_gains[chosen_idx]\n                    best_comm_id = potential_comms[chosen_idx]\n\n            if best_gain > 0:\n                moved = True\n                sigma_tot[old_comm_id] -= ki\n                sigma_tot[best_comm_id] += ki\n                communities[i] = best_comm_id\n        \n        if not moved:\n            break\n            \n    return communities\n\ndef solve():\n    \"\"\"Main solver function.\"\"\"\n    \n    test_cases = [\n        { # Case 1\n            \"group_sizes\": [5, 5], \"w_in\": 1.0, \"w_out\": 0.05,\n            \"gammas\": [0.5, 1.0, 1.5, 2.0], \"R\": 6\n        },\n        { # Case 2\n            \"group_sizes\": [9], \"w_in\": 1.0, \"w_out\": 1.0, # Homogeneous\n            \"gammas\": [0.1, 0.5, 1.0, 2.0], \"R\": 6\n        },\n        { # Case 3\n            \"group_sizes\": [4, 4, 4], \"w_in\": 1.0, \"w_out\": 0.1,\n            \"gammas\": [0.5, 1.0, 1.5, 2.5], \"R\": 6\n        }\n    ]\n    \n    base_seed = 42 # For deterministic \"random\" runs\n    final_gammas = []\n    \n    for case in test_cases:\n        A = construct_graph(case[\"group_sizes\"], case[\"w_in\"], case[\"w_out\"])\n        n = A.shape[0]\n        two_m = A.sum()\n        \n        results_per_gamma = []\n        \n        for gamma in case[\"gammas\"]:\n            partitions = []\n            modularities = []\n            num_clusters = []\n            \n            for r in range(case[\"R\"]):\n                rng = np.random.default_rng(base_seed + r)\n                partition = louvain_local_moving(A, gamma, rng)\n                partitions.append(partition)\n\n                # Re-calculate correct modularity for final partition\n                # The simple DeltaQ gain is scaled by 2m. Let's use a full calculator.\n                k = A.sum(axis=1)\n                mod = 0.0\n                unique_comms = np.unique(partition)\n                for comm_id in unique_comms:\n                     nodes_in_comm = np.where(partition == comm_id)[0]\n                     subgraph_A = A[np.ix_(nodes_in_comm, nodes_in_comm)]\n                     sum_in = np.sum(subgraph_A)\n                     sum_tot = np.sum(k[nodes_in_comm])\n                     mod += sum_in - gamma * sum_tot**2 / two_m\n                \n                mod /= two_m\n                modularities.append(mod)\n\n                num_clusters.append(len(np.unique(partition)))\n\n            aris = []\n            if case[\"R\"] > 1:\n                for i in range(case[\"R\"]):\n                    for j in range(i + 1, case[\"R\"]):\n                        ari = calculate_ari(partitions[i], partitions[j])\n                        aris.append(ari)\n            \n            S_gamma = np.mean(aris) if aris else 1.0\n            Q_bar_gamma = np.mean(modularities)\n            \n            results_per_gamma.append((S_gamma, Q_bar_gamma, gamma))\n            \n        # Selection rule\n        # 1. Maximize S(gamma)\n        max_S = -np.inf\n        for S, Q, g in results_per_gamma:\n            if S > max_S:\n                max_S = S\n                \n        tied_on_S = []\n        for S, Q, g in results_per_gamma:\n            if S >= max_S - 1e-8:\n                tied_on_S.append((S, Q, g))\n\n        # 2. Tie-break with Q_bar\n        max_Q = -np.inf\n        for S, Q, g in tied_on_S:\n            if Q > max_Q:\n                max_Q = Q\n        \n        tied_on_Q = []\n        for S, Q, g in tied_on_S:\n            if np.isclose(Q, max_Q):\n                tied_on_Q.append((S, Q, g))\n\n        # 3. Tie-break with smallest gamma\n        tied_on_Q.sort(key=lambda x: x[2])\n        best_gamma = tied_on_Q[0][2]\n        \n        final_gammas.append(best_gamma)\n\n    print(f\"[{','.join(map(str, final_gammas))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "After identifying cell clusters, the next challenge is to assign them biological meaning, a process known as cell type annotation. While this is often done by looking for the expression of a single 'marker gene,' this approach can be unreliable due to technical noise like gene dropout. This exercise introduces a more robust method by generalizing from a single gene to a 'gene module,' a set of co-expressed genes, and demonstrates how averaging their signals creates a more powerful score for reliably identifying cell types .",
            "id": "2371687",
            "problem": "You are given the task of formalizing and implementing a robust generalization of a single \"marker gene\" to a \"marker gene module\" for identifying a target cell type in single-cell gene expression data. The core idea is to exploit co-expression: a small set of genes that are statistically co-expressed with a seed marker gene yields a more stable signal across heterogeneous cells than any single gene alone.\n\nStart from the following fundamental base:\n- The Central Dogma of Molecular Biology: genes are transcribed into messenger ribonucleic acid (mRNA), which can be quantified to estimate gene expression.\n- A gene expression experiment yields a matrix where rows are genes and columns are cells. Entry $X_{g,c} \\ge 0$ is the measured abundance (e.g., counts) for gene $g$ in cell $c$.\n- Co-expression can be quantified with the Pearson correlation coefficient. Standardization across cells converts raw expression into a dimensionless score comparable across genes.\n\nYour program must implement the following pipeline for each dataset in the test suite.\n\nDefinitions and procedure:\n1. Data model. Let $G = \\{g_1,\\dots,g_p\\}$ be the set of $p$ genes and $C = \\{c_1,\\dots,c_n\\}$ be the set of $n$ cells. You are given a nonnegative matrix $X \\in \\mathbb{R}_{\\ge 0}^{p \\times n}$ and a binary label vector $y \\in \\{0,1\\}^n$ indicating whether each cell $c$ is of the target type ($y_c = 1$) or not ($y_c = 0$). A seed marker gene $g^\\ast \\in G$ and a correlation threshold $\\tau \\in [0,1]$ are also given.\n2. Co-expression and module construction.\n   - For each gene $g \\in G$, compute the Pearson correlation $r(g^\\ast,g)$ between the vectors $X_{g^\\ast,\\cdot}$ and $X_{g,\\cdot}$ across the $n$ cells, using the population formulation: for centered variables with population standard deviation (denominator $n$).\n   - Exclude any gene with zero variance across cells (population standard deviation equal to $0$) from consideration (both in correlation and subsequent scoring).\n   - Define the marker gene module at threshold $\\tau$ as\n     $$ M(\\tau) \\equiv \\{ g \\in G \\,:\\, r(g^\\ast,g) \\ge \\tau \\ \\text{and}\\ r(g^\\ast,g) > 0 \\} \\cup \\{ g^\\ast \\}. $$\n     If any gene in $M(\\tau)$ has zero variance, remove it from $M(\\tau)$. The seed $g^\\ast$ is always included unless its variance is zero; if that happens, include it only for the single-gene baseline and exclude it from the module if needed to avoid division by zero.\n3. Standardization and scoring.\n   - For each gene $g$ with nonzero variance, compute the standardized $z$-score across cells using the population mean and population standard deviation:\n     $$ z_{g,c} \\equiv \\frac{X_{g,c} - \\mu_g}{\\sigma_g}, \\quad \\mu_g \\equiv \\frac{1}{n}\\sum_{c=1}^n X_{g,c}, \\quad \\sigma_g \\equiv \\sqrt{\\frac{1}{n}\\sum_{c=1}^n (X_{g,c} - \\mu_g)^2}. $$\n   - Define the module score for each cell $c$ as the average of standardized expressions over genes in the module:\n     $$ s_c \\equiv \\frac{1}{|M(\\tau)|}\\sum_{g \\in M(\\tau)} z_{g,c}. $$\n   - Define the single-gene score for each cell $c$ using only the seed gene:\n     $$ s_c^{(1)} \\equiv z_{g^\\ast,c}. $$\n4. Classification rule.\n   - Predict the target cell type from the module and from the single gene using the sign of the score:\n     $$ \\widehat{y}^{(M)}_c \\equiv \\mathbb{I}[\\, s_c > 0 \\,], \\qquad \\widehat{y}^{(1)}_c \\equiv \\mathbb{I}[\\, s_c^{(1)} > 0 \\,], $$\n     where $\\mathbb{I}[\\cdot]$ is the indicator function that equals $1$ when its argument is true and $0$ otherwise.\n   - Count the number of correct predictions in each case:\n     $$ N^{(M)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(M)}_c = y_c \\,], \\qquad N^{(1)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(1)}_c = y_c \\,]. $$\n5. Effect size comparison via pooled standardized difference (Cohen’s $d$). Compute the effect size for both the module scores and the single-gene scores. Let $S = \\{ c : y_c = 1 \\}$ and $T = \\{ c : y_c = 0 \\}$ with sizes $n_S$ and $n_T$. For a score vector $u \\in \\mathbb{R}^n$, let\n   $$ \\overline{u}_S \\equiv \\frac{1}{n_S}\\sum_{c \\in S} u_c, \\quad \\overline{u}_T \\equiv \\frac{1}{n_T}\\sum_{c \\in T} u_c, $$\n   $$ s_S^2 \\equiv \\frac{1}{n_S - 1} \\sum_{c \\in S} (u_c - \\overline{u}_S)^2, \\quad s_T^2 \\equiv \\frac{1}{n_T - 1} \\sum_{c \\in T} (u_c - \\overline{u}_T)^2, $$\n   $$ s_p \\equiv \\sqrt{ \\frac{(n_S - 1)s_S^2 + (n_T - 1)s_T^2}{n_S + n_T - 2} }, \\quad d(u) \\equiv \\frac{\\overline{u}_S - \\overline{u}_T}{s_p}. $$\n   Apply this with $u = s$ (module) and $u = s^{(1)}$ (single gene).\n6. Output specification for each dataset. Report the list\n   $$ \\big[\\, |M(\\tau)|,\\ \\mathrm{round}(d(s^{(1)}), 3),\\ \\mathrm{round}(d(s), 3),\\ N^{(M)} - N^{(1)} \\,\\big], $$\n   where $\\mathrm{round}(\\cdot,3)$ rounds to three decimal places.\n\nTest suite. Implement your solution on the following three datasets. The rows are ordered $(g_1,g_2,g_3,g_4,g_5,g_6)$ and the columns are ordered $(c_1,\\dots,c_n)$. The seed is always $g^\\ast = g_1$. The label vector $y$ is provided for each dataset.\n\n- Dataset A (happy path; strong co-expression module):\n  $$ X^{(A)} = \\begin{bmatrix}\n  10 & 11 & 9 & 10 & 1 & 2 & 1 & 1 \\\\\n  9 & 10 & 8 & 9 & 1 & 1 & 2 & 1 \\\\\n  1 & 1 & 1 & 2 & 9 & 8 & 10 & 9 \\\\\n  1 & 2 & 1 & 1 & 8 & 9 & 9 & 8 \\\\\n  2 & 1 & 2 & 2 & 2 & 2 & 1 & 2 \\\\\n  6 & 7 & 5 & 6 & 1 & 1 & 1 & 2\n  \\end{bmatrix}, \\quad y^{(A)} = [\\,1,1,1,1,0,0,0,0\\,], \\quad \\tau^{(A)} = 0.6. $$\n- Dataset B (boundary condition; threshold forces single-gene module):\n  $$ X^{(B)} = X^{(A)}, \\quad y^{(B)} = y^{(A)}, \\quad \\tau^{(B)} = 1.0. $$\n- Dataset C (dropout edge case; module aids recovery):\n  $$ X^{(C)} = \\begin{bmatrix}\n  0 & 0 & 8 & 1 & 1 & 1 \\\\\n  7 & 6 & 7 & 1 & 1 & 1 \\\\\n  1 & 1 & 1 & 6 & 7 & 6 \\\\\n  1 & 1 & 1 & 5 & 6 & 5 \\\\\n  2 & 2 & 2 & 2 & 2 & 2 \\\\\n  5 & 5 & 6 & 1 & 1 & 1\n  \\end{bmatrix}, \\quad y^{(C)} = [\\,1,1,1,0,0,0\\,], \\quad \\tau^{(C)} = 0.3. $$\n\nFinal output format. Your program should produce a single line of output containing the results for Datasets A, B, and C, in this exact format (with no spaces): a comma-separated list of the three per-dataset result lists, enclosed in square brackets, for example\n$$ \\text{\"[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]\"} $$\nwhere each $a_i$ is an integer and each $b_i,c_i$ is a float rounded to three decimal places and $d_i$ is an integer. There must be no other printed text.\n\nNotes:\n- All mathematical entities (symbols, variables, functions, operators, and numbers) above are written in LaTeX, but your implementation must follow their computational definitions.\n- Angles and physical units are not applicable.",
            "solution": "We formalize a robust marker gene module by aggregating standardized, co-expressed genes. The reasoning proceeds from basic definitions in molecular biology and statistics to a concrete algorithm.\n\nFundamental basis and motivation. The Central Dogma of Molecular Biology states that genes are transcribed into messenger ribonucleic acid (mRNA), which can be measured to obtain quantitative gene expression. Single-cell measurements yield an expression matrix $X_{g,c}$ where the variability across cells reflects both biological heterogeneity and technical noise. A single marker gene $g^\\ast$ may fail under dropout or subtle shifts, whereas a small set of co-expressed genes can average out noise and amplify the true biological signal. Statistically, if co-expressed genes share signal but have partially independent noise, then averaging their standardized expressions reduces variance approximately in proportion to the module size by an argument akin to the additivity of variance under independence.\n\nCo-expression via correlation. A standard measure of co-expression is the Pearson correlation, which for two gene vectors across cells quantifies linear concordance. Using population standardization (denominator $n$) for both mean and variance yields the population correlation coefficient. Denote centered vectors by $X^\\circ_{g,c} \\equiv X_{g,c} - \\mu_g$, population standard deviation $\\sigma_g \\equiv \\sqrt{\\frac{1}{n} \\sum_c (X_{g,c} - \\mu_g)^2}$, and correlation\n$$ r(g^\\ast,g) \\equiv \\frac{1}{n}\\sum_{c=1}^n \\frac{(X_{g^\\ast,c} - \\mu_{g^\\ast})(X_{g,c} - \\mu_g)}{\\sigma_{g^\\ast}\\sigma_g}. $$\nIf $\\sigma_g = 0$, correlation is undefined and the gene must be excluded from both correlation and subsequent scoring to avoid division by zero.\n\nModule definition. For a threshold $\\tau \\in [0,1]$, define\n$$ M(\\tau) \\equiv \\{ g \\in G \\,:\\, r(g^\\ast,g) \\ge \\tau \\ \\text{and}\\ r(g^\\ast,g) > 0 \\} \\cup \\{ g^\\ast \\}, $$\nand remove any gene with zero variance if present. The positivity constraint ensures inclusion of co-upregulated genes and excludes anti-markers. The seed $g^\\ast$ is always included unless it has zero variance; this guarantees at least the single-gene baseline is available.\n\nStandardization and scoring. For each retained gene, compute the population $z$-score\n$$ z_{g,c} \\equiv \\frac{X_{g,c} - \\mu_g}{\\sigma_g}. $$\nThe module score is the average standardized expression\n$$ s_c \\equiv \\frac{1}{|M(\\tau)|}\\sum_{g \\in M(\\tau)} z_{g,c}, $$\nand the single-gene baseline is $s_c^{(1)} \\equiv z_{g^\\ast,c}$. Because each $z_{g,\\cdot}$ has population mean $0$ across cells, the average $s_c$ also has population mean $0$, so a natural, nonparametric threshold for classification is the sign test at $0$:\n$$ \\widehat{y}^{(M)}_c \\equiv \\mathbb{I}[\\, s_c > 0 \\,], \\qquad \\widehat{y}^{(1)}_c \\equiv \\mathbb{I}[\\, s_c^{(1)} > 0 \\,]. $$\n\nPerformance quantification. We compute two metrics:\n1. The number of correct predictions\n   $$ N^{(M)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(M)}_c = y_c \\,], \\qquad N^{(1)} \\equiv \\sum_{c=1}^n \\mathbb{I}[\\, \\widehat{y}^{(1)}_c = y_c \\,]. $$\n   The improvement is $N^{(M)} - N^{(1)}$.\n2. The standardized mean difference (Cohen’s $d$), reflecting separation between target and non-target cells. For any score vector $u \\in \\mathbb{R}^n$, let $S = \\{ c : y_c = 1 \\}$, $T = \\{ c : y_c = 0 \\}$, means $\\overline{u}_S$, $\\overline{u}_T$, sample variances $s_S^2$, $s_T^2$ (denominator $n_S - 1$ and $n_T - 1$ respectively), and pooled standard deviation\n   $$ s_p \\equiv \\sqrt{ \\frac{(n_S - 1)s_S^2 + (n_T - 1)s_T^2}{n_S + n_T - 2} }. $$\n   Then\n   $$ d(u) \\equiv \\frac{\\overline{u}_S - \\overline{u}_T}{s_p}. $$\n   We report $d(s^{(1)})$ for the single-gene baseline and $d(s)$ for the module, each rounded to three decimals.\n\nAlgorithmic steps for each dataset:\n- Compute population means $\\mu_g$ and standard deviations $\\sigma_g$ for each gene; discard zero-variance genes for correlation and scoring.\n- Compute population Pearson correlations $r(g^\\ast,g)$ for all genes $g$ with nonzero variance.\n- Construct $M(\\tau)$ by including $g^\\ast$ and all genes with $r(g^\\ast,g) \\ge \\tau$ and strictly positive.\n- Compute $z_{g,c}$ for $g \\in M(\\tau) \\cup \\{g^\\ast\\}$, then compute $s_c$ and $s_c^{(1)}$, followed by $\\widehat{y}^{(M)}_c$, $\\widehat{y}^{(1)}_c$, $N^{(M)}$, $N^{(1)}$, and $d(s)$, $d(s^{(1)})$.\n\nTest-suite expectations:\n- Dataset A with $\\tau = 0.6$ yields a meaningful module containing multiple positively co-expressed genes with $g^\\ast = g_1$, so $|M(\\tau)|$ is greater than $1$, and both $d(s)$ and classification accuracy are strong.\n- Dataset B uses the same data as A but $\\tau = 1.0$, so only $g^\\ast$ meets the threshold (exact correlation of $1$ is not attained by other genes), hence $|M(\\tau)| = 1$, $d(s) = d(s^{(1)})$, and $N^{(M)} - N^{(1)} = 0$.\n- Dataset C includes dropout for $g^\\ast$ in some target cells, while co-expressed partners remain high. With $\\tau = 0.3$, at least two co-expressed genes join the module; averaging $z$-scores recovers target cells, improving both $d(s)$ and $N^{(M)} - N^{(1)}$ relative to the single-gene baseline.\n\nThe program implements these steps exactly and prints a single line with three lists\n$$ \\big[\\, |M(\\tau)|,\\ \\mathrm{round}(d(s^{(1)}), 3),\\ \\mathrm{round}(d(s), 3),\\ N^{(M)} - N^{(1)} \\,\\big] $$\nfor Datasets A, B, and C, respectively, concatenated as a comma-separated list with no spaces as required.",
            "answer": "```python\nimport numpy as np\n\ndef population_mean_std(X, axis):\n    \"\"\"\n    Compute population mean and population standard deviation (ddof=0).\n    Returns (mean, std).\n    \"\"\"\n    mu = np.mean(X, axis=axis)\n    sigma = np.std(X, axis=axis, ddof=0)\n    return mu, sigma\n\ndef population_pearson_corr(x, Y):\n    \"\"\"\n    Compute population Pearson correlation between a 1D vector x (length n)\n    and each row of 2D array Y (m x n). Uses population std (ddof=0).\n    Returns a 1D array of length m with correlations; returns 0.0 for zero-variance rows.\n    \"\"\"\n    # Center x and Y\n    x = np.asarray(x, dtype=float)\n    Y = np.asarray(Y, dtype=float)\n    n = x.shape[0]\n    x_mu = np.mean(x)\n    x_std = np.std(x, ddof=0)\n    # Handle zero variance in x (rare by construction)\n    if x_std == 0:\n        return np.zeros(Y.shape[0], dtype=float)\n    Xc = x - x_mu\n    Y_mu = np.mean(Y, axis=1)\n    Y_std = np.std(Y, axis=1, ddof=0)\n    Yc = Y - Y_mu[:, None]\n    # Compute covariance: mean of product of centered variables\n    cov = (Yc @ Xc) / n  # shape (m,)\n    # Avoid division by zero: where Y_std==0 set corr to 0\n    denom = x_std * Y_std\n    with np.errstate(divide='ignore', invalid='ignore'):\n        r = np.where(denom > 0, cov / denom, 0.0)\n    return r\n\ndef build_module(X, seed_idx, tau):\n    \"\"\"\n    Build marker gene module M(tau) around seed gene index seed_idx with threshold tau.\n    Exclude genes with zero population variance from module and correlation.\n    Only include positively correlated genes with r >= tau.\n    Always include the seed gene unless its variance is zero.\n    Returns a sorted list of unique gene indices in the module.\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    p, n = X.shape\n    # Compute per-gene population std to filter zero-variance genes\n    mu_g = np.mean(X, axis=1)\n    std_g = np.std(X, axis=1, ddof=0)\n    nonzero_var = std_g > 0\n    # Compute correlations for eligible genes\n    r = population_pearson_corr(X[seed_idx, :], X)\n    module = set()\n    # Include genes with positive correlation >= tau and nonzero variance\n    for g in range(p):\n        if nonzero_var[g] and (g == seed_idx or (r[g] > 0 and r[g] >= tau)):\n            module.add(g)\n    # Ensure seed inclusion if it has nonzero variance; if zero, exclude to avoid div-by-zero\n    if nonzero_var[seed_idx]:\n        module.add(seed_idx)\n    else:\n        module.discard(seed_idx)\n    # Remove any zero-variance genes (safety)\n    module = [g for g in module if nonzero_var[g]]\n    module.sort()\n    return module\n\ndef z_scores(X):\n    \"\"\"\n    Compute population z-scores per gene across cells.\n    X is (p x n). Returns Z of same shape.\n    For zero-variance genes, set z-scores to 0 (they will be excluded upstream anyway).\n    \"\"\"\n    mu, sigma = population_mean_std(X, axis=1)\n    # Avoid division by zero: where sigma==0, set to 1 to produce zeros\n    sigma_safe = np.where(sigma == 0, 1.0, sigma)\n    Z = (X - mu[:, None]) / sigma_safe[:, None]\n    return Z, mu, sigma\n\ndef cohen_d(u, y):\n    \"\"\"\n    Compute Cohen's d between groups y==1 and y==0 using pooled sample standard deviation.\n    u: 1D array of scores length n\n    y: 1D array of 0/1 labels length n\n    Returns float d. Assumes both groups have at least 2 samples and nonzero pooled variance.\n    \"\"\"\n    u = np.asarray(u, dtype=float)\n    y = np.asarray(y, dtype=int)\n    S = (y == 1)\n    T = (y == 0)\n    uS = u[S]\n    uT = u[T]\n    nS = uS.size\n    nT = uT.size\n    mS = np.mean(uS) if nS > 0 else 0.0\n    mT = np.mean(uT) if nT > 0 else 0.0\n    # Sample variances\n    sS2 = np.var(uS, ddof=1) if nS > 1 else 0.0\n    sT2 = np.var(uT, ddof=1) if nT > 1 else 0.0\n    # Pooled standard deviation\n    denom_df = (nS + nT - 2)\n    if denom_df <= 0:\n        sp = 0.0\n    else:\n        sp = np.sqrt(((nS - 1) * sS2 + (nT - 1) * sT2) / denom_df)\n    if sp == 0.0:\n        return 0.0\n    return (mS - mT) / sp\n\ndef evaluate_dataset(X, y, seed_idx, tau):\n    \"\"\"\n    For a dataset (X, y), seed gene index, and threshold tau:\n    - Build module\n    - Compute z-scores\n    - Compute module score and single-gene score\n    - Classify with threshold > 0\n    - Count correct predictions\n    - Compute Cohen's d for both scores\n    Returns [module_size, d_single_rounded, d_module_rounded, improvement_int]\n    \"\"\"\n    X = np.asarray(X, dtype=float)\n    y = np.asarray(y, dtype=int)\n    p, n = X.shape\n    module = build_module(X, seed_idx, tau)\n    Z, _, _ = z_scores(X)\n    # Single-gene score\n    s_single = Z[seed_idx, :]\n    # Module score: average across genes in module\n    if len(module) == 0:\n        s_module = s_single.copy()\n    else:\n        s_module = np.mean(Z[module, :], axis=0)\n    # Predictions with threshold > 0\n    yhat_single = (s_single > 0).astype(int)\n    yhat_module = (s_module > 0).astype(int)\n    correct_single = int(np.sum(yhat_single == y))\n    correct_module = int(np.sum(yhat_module == y))\n    # Effect sizes\n    d_single = cohen_d(s_single, y)\n    d_module = cohen_d(s_module, y)\n    # Round to 3 decimals for reporting\n    d_single_r = round(float(d_single), 3)\n    d_module_r = round(float(d_module), 3)\n    improvement = int(correct_module - correct_single)\n    return [len(module), d_single_r, d_module_r, improvement]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Dataset A\n    XA = np.array([\n        [10, 11,  9, 10, 1, 2, 1, 1],\n        [ 9, 10,  8,  9, 1, 1, 2, 1],\n        [ 1,  1,  1,  2, 9, 8,10, 9],\n        [ 1,  2,  1,  1, 8, 9, 9, 8],\n        [ 2,  1,  2,  2, 2, 2, 1, 2],\n        [ 6,  7,  5,  6, 1, 1, 1, 2]\n    ], dtype=float)\n    yA = np.array([1,1,1,1,0,0,0,0], dtype=int)\n    seed_idx = 0  # g1\n    tauA = 0.6\n\n    # Dataset B (same X and y, higher tau)\n    XB = XA.copy()\n    yB = yA.copy()\n    tauB = 1.0\n\n    # Dataset C\n    XC = np.array([\n        [0, 0, 8, 1, 1, 1],\n        [7, 6, 7, 1, 1, 1],\n        [1, 1, 1, 6, 7, 6],\n        [1, 1, 1, 5, 6, 5],\n        [2, 2, 2, 2, 2, 2],\n        [5, 5, 6, 1, 1, 1]\n    ], dtype=float)\n    yC = np.array([1,1,1,0,0,0], dtype=int)\n    tauC = 0.3\n\n    test_cases = [\n        (XA, yA, seed_idx, tauA),\n        (XB, yB, seed_idx, tauB),\n        (XC, yC, seed_idx, tauC),\n    ]\n\n    results = []\n    for X, y, seed, tau in test_cases:\n        res = evaluate_dataset(X, y, seed, tau)\n        results.append(res)\n\n    # Final print statement in the exact required format (no spaces).\n    # Example: [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]]\n    # Convert to string without spaces.\n    def to_str_list(lst):\n        return \"[\" + \",\".join(str(x) for x in lst) + \"]\"\n    output = \"[\" + \",\".join(to_str_list(r) for r in results) + \"]\"\n    print(output)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}