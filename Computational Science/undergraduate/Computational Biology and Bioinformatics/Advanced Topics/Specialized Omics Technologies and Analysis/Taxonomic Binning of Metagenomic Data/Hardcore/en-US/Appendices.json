{
    "hands_on_practices": [
        {
            "introduction": "Metagenomic datasets are often colossal, making the storage and comparison of complete sequence information computationally prohibitive. This exercise explores the concept of data sketching through MinHash, a cornerstone algorithm for comparing massive sets, such as the k-mer profiles of entire metagenomes. By calculating the information loss involved in creating a compressed \"sketch,\" you will gain a quantitative understanding of the trade-off between computational efficiency and descriptive completeness in bioinformatics .",
            "id": "2433915",
            "problem": "A metagenomic taxonomic binning pipeline represents each sample by its Deoxyribonucleic Acid (DNA) $k$-mer profile. Consider a $10 \\times 10^{9}$ base-pair metagenome (length $L = 1.0 \\times 10^{10}$ nucleotides) and $k = 31$. Assume each position contributes a distinct $k$-mer so that the number of distinct $k$-mers is $D = L - k + 1$, and further assume repeats are negligible compared to the total space of possible $k$-mers. The alphabet is $\\{ \\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T} \\}$, so the universe size is $U = 4^{k}$. Define the information content of the exact $k$-mer set as $I_{\\mathrm{full}} = \\log_{2} \\binom{U}{D}$ bits, based on counting the number of possible subsets of size $D$ from a universe of size $U$. A MinHash sketch stores the $s$ smallest hash values of the $k$-mers under a fixed $b$-bit hash function. Treat the information contained in the sketch as $I_{\\mathrm{sketch}} = s \\, b$ bits, and ignore any additional metadata or overhead.\n\nWith $s = 10{,}000$ and $b = 64$, compute the fractional information loss\n$$\n\\Lambda \\equiv 1 - \\frac{I_{\\mathrm{sketch}}}{I_{\\mathrm{full}}}\n$$\nas a unitless decimal number. Round your answer to six significant figures.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. All parameters and definitions required for a unique solution are provided. The assumptions, such as each position contributing a distinct $k$-mer and the specific formula for the information content of the sketch, are explicitly stated, rendering the problem self-contained and solvable. Therefore, I will proceed with the solution.\n\nThe problem requires the computation of the fractional information loss, $\\Lambda$, defined as:\n$$\n\\Lambda = 1 - \\frac{I_{\\mathrm{sketch}}}{I_{\\mathrm{full}}}\n$$\n\nThe givens are:\n- Metagenome length: $L = 1.0 \\times 10^{10}$\n- $k$-mer length: $k = 31$\n- Alphabet size: $4$\n- MinHash sketch size: $s = 10000$\n- Hash function bit-depth: $b = 64$\n\nFirst, we calculate the information content of the MinHash sketch, $I_{\\mathrm{sketch}}$. According to the problem statement, this is given by:\n$$\nI_{\\mathrm{sketch}} = s \\cdot b\n$$\nSubstituting the given values:\n$$\nI_{\\mathrm{sketch}} = 10000 \\times 64 = 640000 = 6.4 \\times 10^5 \\text{ bits}\n$$\n\nNext, we calculate the information content of the exact $k$-mer set, $I_{\\mathrm{full}}$. This is defined as:\n$$\nI_{\\mathrm{full}} = \\log_{2} \\binom{U}{D}\n$$\nThe parameters $U$ and $D$ must be determined first.\nThe universe of all possible $k$-mers, $U$, is given by:\n$$\nU = 4^k = 4^{31} = (2^2)^{31} = 2^{62}\n$$\nThe number of distinct $k$-mers in the metagenome, $D$, is given by the formula:\n$$\nD = L - k + 1 = 1.0 \\times 10^{10} - 31 + 1 = 10^{10} - 30\n$$\nThe binomial coefficient $\\binom{U}{D} = \\binom{2^{62}}{10^{10} - 30}$ is a tremendously large number, so a direct computation is infeasible. We must use an approximation for its logarithm. Since $U \\gg D \\gg 1$, we can use a standard result derived from Stirling's approximation for $\\ln(n!)$ or from information theory (the entropy of a source). The formula for $\\log_2 \\binom{n}{k}$ for $k \\ll n$ is:\n$$\n\\log_2 \\binom{n}{k} \\approx k (\\log_2 n - \\log_2 k) + k \\log_2 e\n$$\nThis can be written as $k \\log_2(n/k) + k \\log_2(e)$. Applying this approximation to $I_{\\mathrm{full}}$ with $n=U$ and $k=D$:\n$$\nI_{\\mathrm{full}} \\approx D (\\log_2 U - \\log_2 D + \\log_2 e)\n$$\nNow, we compute the values of the terms in this expression.\nThe value of $\\log_2 U$ is exact:\n$$\n\\log_2 U = \\log_2 (2^{62}) = 62\n$$\nFor $\\log_2 D$, we have $D = 10^{10} - 30$. Since $30$ is negligible compared to $10^{10}$, we can approximate $D \\approx 10^{10}$ for the purpose of calculating its logarithm.\n$$\n\\log_2 D \\approx \\log_2(10^{10}) = 10 \\log_2(10)\n$$\nWe use the change of base formula, $\\log_2 x = \\frac{\\ln x}{\\ln 2}$.\n$$\n\\log_2 D \\approx 10 \\frac{\\ln 10}{\\ln 2} \\approx 10 \\times \\frac{2.302585}{0.693147} \\approx 10 \\times 3.321928 = 33.21928\n$$\nThe value of $\\log_2 e$ is:\n$$\n\\log_2 e = \\frac{\\ln e}{\\ln 2} = \\frac{1}{\\ln 2} \\approx \\frac{1}{0.693147} \\approx 1.442695\n$$\nNow we substitute these values into the approximation for $I_{\\mathrm{full}}$. We use $D = 10^{10} - 30$:\n$$\nI_{\\mathrm{full}} \\approx (10^{10} - 30) (62 - 33.21928 + 1.442695)\n$$\n$$\nI_{\\mathrm{full}} \\approx (10^{10} - 30) (30.223415)\n$$\nThe term $(1 - 30/10^{10})$ is extremely close to $1$, so its effect on the final calculation is negligible.\n$$\nI_{\\mathrm{full}} \\approx 10^{10} \\times 30.223415 = 3.0223415 \\times 10^{11} \\text{ bits}\n$$\nNow we can compute the ratio $\\frac{I_{\\mathrm{sketch}}}{I_{\\mathrm{full}}}$:\n$$\n\\frac{I_{\\mathrm{sketch}}}{I_{\\mathrm{full}}} \\approx \\frac{6.4 \\times 10^5}{3.0223415 \\times 10^{11}} = \\frac{6.4}{3.0223415} \\times 10^{-6} \\approx 2.117625 \\times 10^{-6}\n$$\nFinally, we compute the fractional information loss $\\Lambda$:\n$$\n\\Lambda = 1 - \\frac{I_{\\mathrm{sketch}}}{I_{\\mathrm{full}}} \\approx 1 - 2.117625 \\times 10^{-6} = 1 - 0.000002117625 = 0.999997882375\n$$\nThe problem asks for the answer to be rounded to six significant figures.\n$$\n\\Lambda \\approx 0.999998\n$$",
            "answer": "$$\n\\boxed{0.999998}\n$$"
        },
        {
            "introduction": "Effective taxonomic binning relies on integrating multiple, often noisy, lines of evidence to infer which DNA fragments belong to the same organism. This practice delves into the probabilistic nature of this task by focusing on the information provided by paired-end read links. You will use a Bayesian framework and Shannon entropy to precisely quantify how observing these links reduces uncertainty, providing a concrete example of how information theory is used to evaluate the strength of biological evidence .",
            "id": "2433913",
            "problem": "In taxonomic binning of metagenomic data, paired-end read information can support linking two contigs if mates from the same DNA fragment map uniquely, one to each contig, with proper orientation and insert size. Consider two contigs and the binary latent hypothesis $S$ (same taxon) versus $D$ (different taxa). Assume a prior probability $P(S) = 0.2$ and $P(D) = 1 - P(S)$. Let the number of properly mapped paired-end links $K$ observed between the two contigs be modeled as Poisson with mean $\\lambda_{S} = 8$ under $S$ and mean $\\lambda_{D} = 0.5$ under $D$. That is, for $k \\in \\{0,1,2,\\dots\\}$,\n$$\nP(K = k \\mid S) = \\exp(-\\lambda_{S}) \\frac{\\lambda_{S}^{k}}{k!}, \\quad P(K = k \\mid D) = \\exp(-\\lambda_{D}) \\frac{\\lambda_{D}^{k}}{k!}.\n$$\nYou observe $K = 5$. Define the information gain from using the paired-end data as the reduction in Shannon entropy (in bits) of the binary variable indicating whether the two contigs are from the same taxon, namely\n$$\n\\Delta I \\equiv H_{\\text{prior}} - H_{\\text{posterior}},\n$$\nwhere for a Bernoulli parameter $p \\in (0,1)$, $H(p) = -\\big[p \\log_{2}(p) + (1-p)\\log_{2}(1-p)\\big]$, $H_{\\text{prior}} = H\\big(P(S)\\big)$, and $H_{\\text{posterior}} = H\\big(P(S \\mid K=5)\\big)$. Compute $\\Delta I$ in bits. Round your answer to four significant figures. Express the final result in bits.",
            "solution": "The problem requires the computation of the information gain, $\\Delta I$, defined as the reduction in Shannon entropy of the state variable $S$ after observing the data $K=5$. The information gain is given by:\n$$\n\\Delta I = H_{\\text{prior}} - H_{\\text{posterior}}\n$$\nwhere $H_{\\text{prior}}$ is the entropy of the prior distribution of $S$, and $H_{\\text{posterior}}$ is the entropy of the posterior distribution of $S$ given the observation. The Shannon entropy for a Bernoulli variable with parameter $p$ is defined in bits as $H(p) = -\\left[p \\log_{2}(p) + (1-p)\\log_{2}(1-p)\\right]$.\n\nFirst, we calculate the prior entropy, $H_{\\text{prior}}$. The prior probability that the two contigs are from the same taxon is given as $P(S) = 0.2$.\nLet $p_{\\text{prior}} = P(S) = 0.2$. Then $1 - p_{\\text{prior}} = P(D) = 0.8$.\nThe prior entropy is:\n$$\nH_{\\text{prior}} = H(p_{\\text{prior}}) = H(0.2) = -\\left[0.2 \\log_{2}(0.2) + 0.8 \\log_{2}(0.8)\\right]\n$$\nUsing the change of base formula $\\log_{2}(x) = \\frac{\\ln(x)}{\\ln(2)}$, we have:\n$$\nH_{\\text{prior}} = -\\frac{1}{\\ln(2)} \\left[0.2 \\ln(0.2) + 0.8 \\ln(0.8)\\right]\n$$\nSubstituting the values $\\ln(0.2) \\approx -1.609438$ and $\\ln(0.8) \\approx -0.223144$, and $\\ln(2) \\approx 0.693147$:\n$$\nH_{\\text{prior}} \\approx -\\frac{1}{0.693147} \\left[0.2(-1.609438) + 0.8(-0.223144)\\right] \\approx -\\frac{-0.500403}{0.693147} \\approx 0.721928 \\text{ bits}\n$$\n\nNext, we must find the posterior probability $P(S \\mid K=5)$ to calculate the posterior entropy, $H_{\\text{posterior}}$. We use Bayes' theorem:\n$$\nP(S \\mid K=5) = \\frac{P(K=5 \\mid S) P(S)}{P(K=5)}\n$$\nThe marginal probability of observing $K=5$, $P(K=5)$, is given by the law of total probability:\n$$\nP(K=5) = P(K=5 \\mid S) P(S) + P(K=5 \\mid D) P(D)\n$$\nThe conditional probabilities, or likelihoods, are given by the Poisson probability mass function.\nFor hypothesis $S$, with $\\lambda_S = 8$:\n$$\nP(K=5 \\mid S) = \\frac{\\lambda_S^5 \\exp(-\\lambda_S)}{5!} = \\frac{8^5 \\exp(-8)}{120}\n$$\nFor hypothesis $D$, with $\\lambda_D = 0.5$:\n$$\nP(K=5 \\mid D) = \\frac{\\lambda_D^5 \\exp(-\\lambda_D)}{5!} = \\frac{(0.5)^5 \\exp(-0.5)}{120}\n$$\nThe posterior probability $p_{\\text{posterior}} = P(S \\mid K=5)$ is therefore:\n$$\np_{\\text{posterior}} = \\frac{P(K=5 \\mid S) P(S)}{P(K=5 \\mid S) P(S) + P(K=5 \\mid D) P(D)}\n$$\nSubstituting the expressions for the likelihoods and the prior probabilities $P(S)=0.2$ and $P(D)=0.8$:\n$$\np_{\\text{posterior}} = \\frac{\\frac{8^5 \\exp(-8)}{120} \\times 0.2}{\\frac{8^5 \\exp(-8)}{120} \\times 0.2 + \\frac{(0.5)^5 \\exp(-0.5)}{120} \\times 0.8}\n$$\nThe term $1/120$ cancels from the numerator and denominator:\n$$\np_{\\text{posterior}} = \\frac{8^5 \\exp(-8) \\times 0.2}{8^5 \\exp(-8) \\times 0.2 + (0.5)^5 \\exp(-0.5) \\times 0.8}\n$$\nLet us compute the terms:\n$8^5 \\exp(-8) = 32768 \\times \\exp(-8) \\approx 32768 \\times 0.00033546 = 10.9936$\n$(0.5)^5 \\exp(-0.5) = 0.03125 \\times \\exp(-0.5) \\approx 0.03125 \\times 0.60653 = 0.018954$\n$$\np_{\\text{posterior}} \\approx \\frac{10.9936 \\times 0.2}{10.9936 \\times 0.2 + 0.018954 \\times 0.8} = \\frac{2.19872}{2.19872 + 0.015163} = \\frac{2.19872}{2.213883} \\approx 0.9931499\n$$\nThis gives the posterior probability of the contigs being from the same taxon. Now we can compute the posterior entropy, $H_{\\text{posterior}}$:\n$$\nH_{\\text{posterior}} = H(p_{\\text{posterior}}) \\approx H(0.99315) = -\\left[0.99315 \\log_{2}(0.99315) + (1-0.99315)\\log_{2}(1-0.99315)\\right]\n$$\nLet $p_{\\text{post}} \\approx 0.99315$ and $1-p_{\\text{post}} \\approx 0.00685$.\n$$\nH_{\\text{posterior}} \\approx -\\frac{1}{\\ln(2)} \\left[0.99315 \\ln(0.99315) + 0.00685 \\ln(0.00685)\\right]\n$$\n$$\nH_{\\text{posterior}} \\approx -\\frac{1}{0.693147} \\left[0.99315(-0.00687) + 0.00685(-4.9836)\\right] \\approx -\\frac{-0.00682 - 0.03414}{0.693147} \\approx \\frac{0.04096}{0.693147} \\approx 0.059101 \\text{ bits}\n$$\nFinally, the information gain $\\Delta I$ is the difference between the prior and posterior entropies:\n$$\n\\Delta I = H_{\\text{prior}} - H_{\\text{posterior}} \\approx 0.721928 - 0.059101 = 0.662827\n$$\nRounding the result to four significant figures gives $0.6628$.",
            "answer": "$$\n\\boxed{0.6628}\n$$"
        },
        {
            "introduction": "State-of-the-art bioinformatics tools often employ a hybrid strategy, combining fast, approximate methods for initial filtering with slower, more accurate algorithms for final validation. This practical coding challenge asks you to build such a two-stage binning pipeline from the ground up. You will first use k-mer based similarity to rapidly identify candidate taxa and then apply the rigorous Smith-Waterman algorithm on protein domains to make a final, high-confidence assignment, mirroring the design of real-world genomic analysis software .",
            "id": "2433908",
            "problem": "You are given a finite set of taxa, each defined by a Deoxyribonucleic Acid (DNA) reference sequence and a protein domain signature sequence. You must implement a two-stage decision function that maps each input DNA read to a single taxon index or to an unassigned value. All steps are to be implemented exactly as defined below, using only the provided parameters and definitions. The alphabet for DNA is $\\{A,C,G,T\\}$, and the protein alphabet is the $20$ standard amino acids with the stop symbol $*$.\n\nStage 1 (fast set-similarity preselection): For a DNA string $s$ and a positive integer $k$, define the $k$-mer set $K(s,k)$ as the set of all length-$k$ substrings of $s$ (overlapping substrings included). For a read $r$ and a reference genome $g_i$, define the Jaccard similarity\n$$\nJ_k(r,g_i) \\coloneqq \\frac{\\lvert K(r,k)\\cap K(g_i,k)\\rvert}{\\lvert K(r,k)\\cup K(g_i,k)\\rvert}.\n$$\nGiven a threshold $\\tau_k$ and a maximum candidate count $m$, define the candidate set $C(r)$ for read $r$ as follows:\n- Compute $J_k(r,g_i)$ for every taxon index $i$.\n- Let $S_{\\ge}(r) \\coloneqq \\{ i : J_k(r,g_i) \\ge \\tau_k \\}$. If $\\lvert S_{\\ge}(r)\\rvert = 0$, then set $C(r)$ to be the set of the $m$ indices with the largest $J_k(r,g_i)$ values, breaking ties by choosing the smaller index first. Otherwise, set $C(r)$ to be the subset of $S_{\\ge}(r)$ containing up to $m$ indices with the largest $J_k(r,g_i)$ values, breaking ties by choosing the smaller index first. If $\\lvert S_{\\ge}(r)\\rvert < m$, then $\\lvert C(r)\\rvert = \\lvert S_{\\ge}(r)\\rvert$.\n\nStage 2 (slow domain-level validation by local alignment): Define the translation $\\mathcal{T}(r)$ of a DNA read $r$ as the amino acid sequence obtained by translating $r$ from its first nucleotide in the forward direction using the standard genetic code with the three-nucleotide codon length, ignoring any partial trailing codon (if the length of $r$ is not divisible by $3$). The stop codons translate to $*$, and $*$ is treated as a literal character. Let $d_i$ be the protein domain signature for taxon $i$.\n\nFor two protein strings $x$ and $y$, define the Smith–Waterman local alignment score $SW(x,y)$ with the following linear scoring:\n- Match score $s_{\\mathrm{match}} = +2$,\n- Mismatch score $s_{\\mathrm{mismatch}} = -1$,\n- Gap penalty $s_{\\mathrm{gap}} = -2$ per gap symbol,\nand the dynamic programming recurrence\n$$\nH_{a,b} \\coloneqq \\max\\Big(0,\\ H_{a-1,b-1} + \\sigma(x_a,y_b),\\ H_{a-1,b} + s_{\\mathrm{gap}},\\ H_{a,b-1} + s_{\\mathrm{gap}}\\Big),\n$$\nwith $H_{0,b} = 0$ and $H_{a,0} = 0$ for all $a,b$, where\n$$\n\\sigma(u,v) \\coloneqq\n\\begin{cases}\ns_{\\mathrm{match}} & \\text{if } u=v,\\\\\ns_{\\mathrm{mismatch}} & \\text{if } u\\ne v.\n\\end{cases}\n$$\nThe local alignment score is $\\max_{a,b} H_{a,b}$.\n\nAssignment rule: For a read $r$, compute $x=\\mathcal{T}(r)$ and for each $i\\in C(r)$ compute $SW(x,d_i)$. Let $i^\\star$ be the index in $C(r)$ with the largest $SW(x,d_i)$; break ties by choosing the smaller index. Given a domain-score threshold $\\tau_d$, if $SW(x,d_{i^\\star}) \\ge \\tau_d$, output $i^\\star$; otherwise output $-1$.\n\nAll indices are zero-based integers. The unassigned value is the integer $-1$.\n\nParameters to use for all computations:\n- $k = 3$,\n- $\\tau_k = 0.2$,\n- $m = 2$,\n- $s_{\\mathrm{match}} = +2$,\n- $s_{\\mathrm{mismatch}} = -1$,\n- $s_{\\mathrm{gap}} = -2$,\n- $\\tau_d = 4$.\n\nTaxa and sequences:\n- Taxon 0: reference genome $g_0$ = `ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG`, protein domain $d_0$ = `MAIVMGRKG`.\n- Taxon 1: reference genome $g_1$ = `ATGGCGGCTGCTGCTGAAAACCCGTTGAATGCCGCCGAA`, protein domain $d_1$ = `MAAAAAK`.\n- Taxon 2: reference genome $g_2$ = `TTGAAACCCGGGTTTAAACCCGGGTTTAAAGGGCCCAA`, protein domain $d_2$ = `MKTLLILAL`.\n\nGenetic code: Use the standard genetic code for translation in the forward frame starting at nucleotide position $1$; ignore any trailing $1$ or $2$ nucleotides that do not form a complete codon.\n\nTest suite (reads):\n- Read $r_1$ = `ATGGCCATTGTAATGGGCCGCTG`,\n- Read $r_2$ = `ATGGCGGCTGCTGCT`,\n- Read $r_3$ = `TTTTTTTTTTTTTTT`,\n- Read $r_4$ = `ATGGCC`.\n\nYour program must process the reads in the order $(r_1,r_2,r_3,r_4)$ and produce a single line of output containing the integer assignments as a comma-separated list enclosed in square brackets (for example, `[0,1,-1,0]`). The only permitted outputs for each read are $-1$, $0$, $1$, or $2$.",
            "solution": "The task specifies a two-stage mapping from each DNA read to either a taxon index or the unassigned value. The solution proceeds by formalizing each stage and applying it deterministically to the provided inputs.\n\nStage 1 formalization: For a fixed $k=3$, the $k$-mer set $K(s,3)$ contains all contiguous substrings of length $3$ from $s$. The Jaccard similarity is $J_3(r,g_i) = \\lvert K(r,3)\\cap K(g_i,3)\\rvert / \\lvert K(r,3)\\cup K(g_i,3)\\rvert$. For each read $r$, we compute $J_3(r,g_i)$ for $i\\in\\{0,1,2\\}$. We define the set $S_{\\ge}(r)=\\{i: J_3(r,g_i)\\ge \\tau_k\\}$ with $\\tau_k = 0.2$. If $\\lvert S_{\\ge}(r)\\rvert = 0$, then $C(r)$ contains the $m=2$ indices with largest $J_3(r,g_i)$, with ties broken by smaller index. Otherwise $C(r)$ contains up to $m$ indices from $S_{\\ge}(r)$ with the largest $J_3(r,g_i)$ values, ties broken by smaller index; if there are fewer than $m$ indices in $S_{\\ge}(r)$, all of them are included.\n\nStage 2 formalization: We translate each DNA read in the forward frame starting at the first nucleotide using the standard genetic code and ignoring any trailing incomplete codon. This yields $x=\\mathcal{T}(r)$. For each candidate $i\\in C(r)$, we compute the Smith–Waterman local alignment score $SW(x,d_i)$ using the recurrence\n$$\nH_{a,b} = \\max\\left(0,\\,H_{a-1,b-1} + \\sigma(x_a,y_b),\\,H_{a-1,b} + s_{\\mathrm{gap}},\\,H_{a,b-1} + s_{\\mathrm{gap}}\\right),\n$$\nwhere $y=d_i$, $s_{\\mathrm{match}}=+2$, $s_{\\mathrm{mismatch}}=-1$, and $s_{\\mathrm{gap}}=-2$. The scoring function $\\sigma(u,v)$ is $+2$ if $u=v$ and $-1$ otherwise. The local score is $\\max_{a,b} H_{a,b}$. We identify $i^\\star\\in C(r)$ that maximizes $SW(x,d_i)$, with ties resolved by selecting the smaller index. With $\\tau_d=4$, if $SW(x,d_{i^\\star})\\ge 4$, we assign $i^\\star$; otherwise we output $-1$.\n\nApplication to the given test suite:\n\n- Read $r_1$ = `ATGGCCATTGTAATGGGCCGCTG`. Translation in the forward frame gives $\\mathcal{T}(r_1)$ = `MAIVMGR` because the codons are `ATG` $\\to$ M, `GCC` $\\to$ A, `ATT` $\\to$ I, `GTA` $\\to$ V, `ATG` $\\to$ M, `GGC` $\\to$ G, `CGC` $\\to$ R, with the trailing `TG` ignored. The $3$-mer set of $r_1$ has substantial overlap with $K(g_0,3)$ due to $r_1$ being a substring of $g_0$, so $J_3(r_1,g_0)$ is large relative to $J_3(r_1,g_1)$ and $J_3(r_1,g_2)$. Thus $0\\in C(r_1)$, and $C(r_1)$ will include at most one other index. The local alignment $SW$(`MAIVMGR`, `MAIVMGRKG`) yields a perfect contiguous match of length $7$, score $7\\times 2=14$. Any alignment against $d_1$=`MAAAAAK` or $d_2$=`MKTLLILAL` is at most a short prefix match; for $d_1$ the best contiguous match is `MA` with score $2\\times 2=4$, and for $d_2$ it is at most `M` with score $2$. Therefore $i^\\star=0$ and $SW\\ge 4$, so the assignment is $0$.\n\n- Read $r_2$ = `ATGGCGGCTGCTGCT`. Translation yields $\\mathcal{T}(r_2)$=`MAAAA` from `ATG` $\\to$ M, `GCG` $\\to$ A, `GCT` $\\to$ A, `GCT` $\\to$ A, `GCT` $\\to$ A. The $3$-mer content overlaps more strongly with $K(g_1,3)$ than with $K(g_0,3)$ or $K(g_2,3)$, so $1\\in C(r_2)$ (with possibly another index due to $m=2$). The alignment $SW$(`MAAAA`, `MAAAAAK`) yields a contiguous match of length $5$, score $10$. Alignments to $d_0$ or $d_2$ score at most $4$ or less due to shorter shared prefixes. Therefore $i^\\star=1$ with score $10 \\ge 4$, so the assignment is $1$.\n\n- Read $r_3$ = `TTTTTTTTTTTTTTT`. Translation produces $\\mathcal{T}(r_3)$=`FFFFF` because `TTT` $\\to$ F repeatedly. The $3$-mer set $K(r_3,3)$={`TTT`} has minimal overlap with $K(g_0,3)$ and $K(g_1,3)$ and some overlap with $K(g_2,3)$ if `TTT` appears, but regardless of $C(r_3)$ composition (bounded by $m=2$), the local alignment scores $SW$(`FFFFF`, $d_i$) for $i\\in C(r_3)$ are $0$ because none of $d_0$=`MAIVMGRKG`, $d_1$=`MAAAAAK`, or $d_2$=`MKTLLILAL` contains `F`, and any alignment accrues mismatches and gaps that are not beneficial under the local alignment recurrence that floors at $0$. Thus the best score is $0 < \\tau_d$, and the assignment is $-1$.\n\n- Read $r_4$ = `ATGGCC`. Translation yields $\\mathcal{T}(r_4)$=`MA`. The $3$-mers of $r_4$ are {`ATG`,`TGG`,`GGC`,`GCC`}, which overlap with both $K(g_0,3)$ and $K(g_1,3)$, so $C(r_4)$ will contain indices $\\{0,1\\}$ after applying the $m=2$ cap and the $\\tau_k$ threshold. The local alignment scores $SW$(`MA`, `MAIVMGRKG`) and $SW$(`MA`, `MAAAAAK`) both attain the value $4$ via matching the prefix `MA`, while $SW$(`MA`, `MKTLLILAL`) is smaller. Hence there is a tie at the maximum score between indices $0$ and $1$, and the tie-break rule selects the smaller index $0$. Since $4 \\ge \\tau_d$, the assignment is $0$.\n\nCollecting these results in order $(r_1,r_2,r_3,r_4)$ yields the final list `[0,1,-1,0]`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Standard genetic code mapping (forward strand, frame starting at position 0)\nGENETIC_CODE = {\n    'TTT': 'F', 'TTC': 'F', 'TTA': 'L', 'TTG': 'L',\n    'TCT': 'S', 'TCC': 'S', 'TCA': 'S', 'TCG': 'S',\n    'TAT': 'Y', 'TAC': 'Y', 'TAA': '*', 'TAG': '*',\n    'TGT': 'C', 'TGC': 'C', 'TGA': '*', 'TGG': 'W',\n    'CTT': 'L', 'CTC': 'L', 'CTA': 'L', 'CTG': 'L',\n    'CCT': 'P', 'CCC': 'P', 'CCA': 'P', 'CCG': 'P',\n    'CAT': 'H', 'CAC': 'H', 'CAA': 'Q', 'CAG': 'Q',\n    'CGT': 'R', 'CGC': 'R', 'CGA': 'R', 'CGG': 'R',\n    'ATT': 'I', 'ATC': 'I', 'ATA': 'I', 'ATG': 'M',\n    'ACT': 'T', 'ACC': 'T', 'ACA': 'T', 'ACG': 'T',\n    'AAT': 'N', 'AAC': 'N', 'AAA': 'K', 'AAG': 'K',\n    'AGT': 'S', 'AGC': 'S', 'AGA': 'R', 'AGG': 'R',\n    'GTT': 'V', 'GTC': 'V', 'GTA': 'V', 'GTG': 'V',\n    'GCT': 'A', 'GCC': 'A', 'GCA': 'A', 'GCG': 'A',\n    'GAT': 'D', 'GAC': 'D', 'GAA': 'E', 'GAG': 'E',\n    'GGT': 'G', 'GGC': 'G', 'GGA': 'G', 'GGG': 'G',\n}\n\ndef kmer_set(seq: str, k: int):\n    return {seq[i:i+k] for i in range(len(seq) - k + 1)} if len(seq) >= k else set()\n\ndef jaccard_kmer(a: str, b: str, k: int) -> float:\n    ka = kmer_set(a, k)\n    kb = kmer_set(b, k)\n    if not ka and not kb:\n        return 0.0\n    inter = ka & kb\n    union = ka | kb\n    return len(inter) / len(union) if union else 0.0\n\ndef translate_frame1(dna: str) -> str:\n    aa = []\n    n = len(dna) // 3\n    for i in range(n):\n        codon = dna[3*i:3*i+3]\n        aa.append(GENETIC_CODE.get(codon, 'X'))  # 'X' for any unexpected codon (should not happen)\n    return ''.join(aa)\n\ndef smith_waterman(x: str, y: str, match: int = 2, mismatch: int = -1, gap: int = -2) -> int:\n    # Local alignment with linear gap penalty\n    n, m = len(x), len(y)\n    # Use a 1D rolling DP to save memory; track max\n    H = [0] * (m + 1)\n    max_score = 0\n    for i in range(1, n + 1):\n        prev_diag = 0\n        for j in range(1, m + 1):\n            temp = H[j]\n            score_sub = match if x[i-1] == y[j-1] else mismatch\n            h_diag = prev_diag + score_sub\n            h_up = H[j] + gap\n            h_left = H[j-1] + gap\n            h_ij = max(0, h_diag, h_up, h_left)\n            H[j] = h_ij\n            prev_diag = temp\n            if h_ij > max_score:\n                max_score = h_ij\n        # reset H[0] to 0 each row (already 0)\n    return max_score\n\ndef select_candidates_by_kmer(read: str, refs: list[str], k: int, tau_k: float, m: int):\n    sims = []\n    for idx, g in enumerate(refs):\n        sims.append((idx, jaccard_kmer(read, g, k)))\n    # Sort by similarity desc, then index asc\n    sims_sorted = sorted(sims, key=lambda t: (-t[1], t[0]))\n    above = [idx for idx, s in sims_sorted if s >= tau_k]\n    if len(above) == 0:\n        return [idx for idx, _ in sims_sorted[:m]]\n    else:\n        return above[:m]\n\ndef assign_read(read: str, refs: list[str], domains: list[str],\n                k: int, tau_k: float, m: int, tau_d: int,\n                match: int = 2, mismatch: int = -1, gap: int = -2) -> int:\n    candidates = select_candidates_by_kmer(read, refs, k, tau_k, m)\n    aa = translate_frame1(read)\n    best_idx = -1\n    best_score = -1_000_000\n    for idx in candidates:\n        score = smith_waterman(aa, domains[idx], match, mismatch, gap)\n        if score > best_score or (score == best_score and idx < best_idx):\n            best_score = score\n            best_idx = idx\n    if best_idx == -1:\n        return -1\n    return best_idx if best_score >= tau_d else -1\n\ndef solve():\n    # Parameters\n    k = 3\n    tau_k = 0.2\n    m = 2\n    match, mismatch, gap = 2, -1, -2\n    tau_d = 4\n\n    # References and domains\n    refs = [\n        \"ATGGCCATTGTAATGGGCCGCTGAAAGGGTGCCCGATAG\",  # g0\n        \"ATGGCGGCTGCTGCTGAAAACCCGTTGAATGCCGCCGAA\",  # g1\n        \"TTGAAACCCGGGTTTAAACCCGGGTTTAAAGGGCCCAA\",   # g2\n    ]\n    domains = [\n        \"MAIVMGRKG\",  # d0\n        \"MAAAAAK\",    # d1\n        \"MKTLLILAL\",  # d2\n    ]\n\n    # Test cases (reads)\n    test_reads = [\n        \"ATGGCCATTGTAATGGGCCGCTG\",   # r1\n        \"ATGGCGGCTGCTGCT\",           # r2\n        \"TTTTTTTTTTTTTTT\",           # r3\n        \"ATGGCC\",                    # r4\n    ]\n\n    results = []\n    for read in test_reads:\n        assign_idx = assign_read(\n            read, refs, domains, k, tau_k, m, tau_d, match, mismatch, gap\n        )\n        results.append(assign_idx)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}