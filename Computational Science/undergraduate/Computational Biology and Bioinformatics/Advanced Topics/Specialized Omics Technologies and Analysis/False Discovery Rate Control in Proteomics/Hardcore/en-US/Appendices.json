{
    "hands_on_practices": [
        {
            "introduction": "In high-throughput proteomics, we simultaneously test for the presence of thousands of proteins, a classic multiple hypothesis testing scenario. Simply applying a traditional significance threshold like $p \\lt 0.05$ for each test can lead to an overwhelming number of false positives. This introductory exercise  demonstrates this issue quantitatively, asking you to calculate the expected number of false discoveries from first principles to solidify your understanding of why controlling the False Discovery Rate (FDR) is essential.",
            "id": "2389430",
            "problem": "A shotgun, bottom-up proteomics database search is performed on a human cell lysate. For each protein entry in the reference database, a hypothesis test is conducted that outputs a calibrated per-protein $p$-value for the null hypothesis that the protein is absent. The reference database contains $20{,}100$ distinct protein entries. In this experiment’s ground truth, exactly $3{,}100$ proteins are truly present in the sample; the remaining entries correspond to proteins that are absent. Assume that the per-protein tests are valid and independent, and that under the null hypothesis the $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$. An analyst naively declares a protein to be identified if its reported $p$-value satisfies $p<0.05$.\n\nUsing only the information above and first principles, compute the expected number of false positive protein identifications under this naive thresholding. Report your answer as a single real number without units. Do not round.",
            "solution": "The problem statement has been critically validated and is deemed to be valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The premises are standard in the field of statistical bioinformatics for analyzing proteomics data. We may now proceed with the solution.\n\nLet $M$ denote the total number of distinct protein entries in the reference database. From the problem statement, we are given $M = 20,100$.\nLet $S$ be the number of proteins that are truly present in the sample. We are given $S = 3,100$.\nThe remaining proteins in the database are truly absent. The null hypothesis, stating that a protein is absent, is true for these proteins. Let $M_0$ be the number of such proteins.\n$$M_0 = M - S = 20,100 - 3,100 = 17,000$$\nA protein identification is declared if its associated $p$-value is less than a given threshold, $\\alpha$. The problem specifies this threshold as $\\alpha = 0.05$.\nA \"false positive\" identification occurs when a protein that is truly absent (i.e., one of the $M_0$ proteins) is incorrectly declared as present. This happens if the $p$-value for a truly absent protein is less than $\\alpha$.\n\nWe are asked to compute the expected number of false positive protein identifications. Let us denote the total number of false positives by the random variable $V$.\nFor each of the $M_0$ truly absent proteins, let us define an indicator random variable, $X_i$, for $i = 1, 2, \\dots, M_0$.\n$$\nX_i = \n\\begin{cases} \n1 & \\text{if protein } i \\text{ is a false positive} \\\\\n0 & \\text{otherwise} \n\\end{cases}\n$$\nThe total number of false positives, $V$, is the sum of these indicator variables over all proteins for which the null hypothesis is true:\n$$V = \\sum_{i=1}^{M_0} X_i$$\nWe seek the expectation of $V$, denoted $E[V]$. By the linearity of expectation, we have:\n$$E[V] = E\\left[\\sum_{i=1}^{M_0} X_i\\right] = \\sum_{i=1}^{M_0} E[X_i]$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, for each $i$:\n$$E[X_i] = P(X_i = 1)$$\nThe event $X_i = 1$ occurs if the $p$-value for the $i$-th null protein, let's call it $p_i$, satisfies $p_i < \\alpha$.\n$$E[X_i] = P(p_i < \\alpha)$$\nThe problem states that for proteins under the null hypothesis, the $p$-values are independent and identically distributed as $\\mathrm{Uniform}(0,1)$.\nFor a random variable $P \\sim \\mathrm{Uniform}(0,1)$, its probability density function (PDF) is $f(p) = 1$ for $p \\in [0, 1]$ and $f(p) = 0$ otherwise. The probability $P(P < \\alpha)$ is given by the integral of the PDF from $0$ to $\\alpha$:\n$$P(P < \\alpha) = \\int_{0}^{\\alpha} f(p) \\,dp = \\int_{0}^{\\alpha} 1 \\,dp = [p]_{0}^{\\alpha} = \\alpha$$\nTherefore, for each of the $M_0$ null proteins, the probability of it being a false positive is $\\alpha$.\n$$E[X_i] = \\alpha = 0.05 \\quad \\text{for all } i=1, \\dots, M_0$$\nSince the expectation $E[X_i]$ is the same for all $M_0$ proteins, we can write:\n$$E[V] = \\sum_{i=1}^{M_0} \\alpha = M_0 \\cdot \\alpha$$\nSubstituting the known values for $M_0$ and $\\alpha$:\n$$E[V] = 17,000 \\times 0.05$$\n$$E[V] = 17,000 \\times \\frac{5}{100} = 170 \\times 5 = 850$$\nThus, the expected number of false positive protein identifications is $850$. This result follows directly from the definition of a $p$-value and the properties of its theoretical distribution under the null hypothesis. The naive thresholding at $\\alpha$ results in an expected $M_0 \\cdot \\alpha$ false discoveries, a fundamental concept in multiple hypothesis testing.",
            "answer": "$$\\boxed{850}$$"
        },
        {
            "introduction": "Once we recognize the need for multiple testing correction, the next step is to apply a formal procedure. The Benjamini-Hochberg (BH) procedure is a cornerstone algorithm for controlling the False Discovery Rate across a large set of hypothesis tests. This hands-on coding challenge  guides you to implement the BH algorithm from scratch, providing a deep, practical understanding of its mechanics and the calculation of the resulting $q$-values.",
            "id": "2389454",
            "problem": "You are given several independent lists of peptide-spectrum match (PSM) $p$-values arising from a proteomics search. Let there be $m$ hypotheses with $p$-values $p_{1},\\dots,p_{m}$, where each $p_{i} \\in [0,1]$. The objective is to control the False Discovery Rate (FDR) using the Benjamini–Hochberg (BH) step-up procedure at a specified target level $\\alpha \\in [0,1]$, and to compute the Benjamini–Hochberg adjusted $p$-values (also called $q$-values). A peptide-spectrum match (PSM) is a pairing between a measured tandem mass spectrum and a candidate peptide sequence, and the False Discovery Rate (FDR) is the expected proportion of false positives among all discoveries.\n\nFormally, let $p_{(1)} \\le \\cdots \\le p_{(m)}$ denote the sorted $p$-values, and let $\\pi(1),\\dots,\\pi(m)$ be the corresponding original indices such that $p_{(i)} = p_{\\pi(i)}$. Define the BH rejection set as follows. Let\n$$\nk^{\\ast} = \\max\\left\\{ i \\in \\{1,\\dots,m\\} \\,:\\, p_{(i)} \\le \\frac{i}{m}\\,\\alpha \\right\\},\n$$\nwith the convention that if the set is empty then $k^{\\ast}=0$. The set of rejected hypotheses is then $\\{\\pi(1),\\dots,\\pi(k^{\\ast})\\}$. Ties in $p$-values must be broken by increasing original index, that is, if $p_{i} = p_{j}$ and $i<j$, then $i$ must be ordered before $j$ when forming $p_{(1)},\\dots,p_{(m)}$.\n\nThe Benjamini–Hochberg adjusted $p$-values ($q$-values) are defined by first computing the sorted adjusted values\n$$\nq_{(i)} = \\min\\left(1, \\min_{k \\ge i} \\frac{m}{k}\\, p_{(k)}\\right), \\quad i=1,\\dots,m,\n$$\nand then mapping back to the original order by setting $q_{\\pi(i)} = q_{(i)}$. By construction, the sequence $q_{(1)} \\le \\cdots \\le q_{(m)}$ is nondecreasing.\n\nFor each input case below, you must compute three outputs:\n- the integer number of rejections $k^{\\ast}$,\n- the list of rejected indices in increasing order of original indices (use $0$-based indices),\n- the list of $q$-values in the original $p$-value order.\n\nRound all floating-point outputs to $6$ decimal places. All values of $\\alpha$ and all $p$-values must be expressed as decimals in $[0,1]$.\n\nTest suite. Your program must run on the following four cases:\n- Case $1$: original order $p$-values $[0.0005, 0.001, 0.002, 0.03, 0.04, 0.07, 0.12, 0.15, 0.20, 0.50]$ with corresponding original indices $[7, 4, 0, 2, 5, 8, 1, 9, 3, 6]$ when sorted ascending by value and breaking ties by increasing original index; original order $p$-values are $[0.002, 0.12, 0.03, 0.20, 0.001, 0.04, 0.50, 0.0005, 0.07, 0.15]$, target level $\\alpha = 0.1$.\n- Case $2$: original order $p$-values $[0.01, 0.01, 0.01, 0.02, 0.02]$, target level $\\alpha = 0.05$.\n- Case $3$: original order $p$-values $[0.6, 0.8, 0.9, 0.7, 0.95, 1.0]$, target level $\\alpha = 0.2$.\n- Case $4$: original order $p$-values $[0.0, 0.2, 0.001, 0.5]$, target level $\\alpha = 0.0$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, the result must be a list with three elements $[k^{\\ast}, L, Q]$, where $k^{\\ast}$ is the integer number of rejections, $L$ is the list of rejected $0$-based indices in increasing order, and $Q$ is the list of $q$-values in original order rounded to $6$ decimal places. For example, the overall output must look like\n$[[k^{\\ast}_{1},L_{1},Q_{1}],[k^{\\ast}_{2},L_{2},Q_{2}],[k^{\\ast}_{3},L_{3},Q_{3}],[k^{\\ast}_{4},L_{4},Q_{4}]]$\nwith no additional text.",
            "solution": "The problem presented is a well-defined computational task from the field of bioinformatics, specifically concerning the statistical analysis of high-throughput proteomics data. The objective is to implement the Benjamini–Hochberg (BH) step-up procedure for controlling the False Discovery Rate (FDR). This involves identifying which of a set of null hypotheses to reject and calculating the corresponding adjusted p-values, known as q-values. The problem is scientifically grounded, mathematically precise, and algorithmically tractable.\n\nThe problem provides a set of $m$ $p$-values, $\\{p_1, \\dots, p_m\\}$, and a target FDR level $\\alpha$. The BH procedure is executed through a series of deterministic steps, which we shall formalize and implement.\n\nFirst, the $m$ $p$-values must be sorted in non-decreasing order. Let the original $p$-values be paired with their $0$-based indices, $(p_i, i)$. The sorting must adhere to the specified tie-breaking rule: if $p_i = p_j$ for $i < j$, the pair $(p_i, i)$ must precede $(p_j, j)$. This stable sorting yields the ordered p-values $p_{(1)} \\le p_{(2)} \\le \\cdots \\le p_{(m)}$, and the corresponding list of original indices $\\pi(1), \\pi(2), \\dots, \\pi(m)$, where $p_{(i)} = p_{\\pi(i)}$.\n\nSecond, we determine the number of null hypotheses to reject, denoted by $k^{\\ast}$. This is found by identifying the largest rank $i$ (from $1$ to $m$) for which the sorted p-value $p_{(i)}$ is less than or equal to its BH-critical value. The critical value for rank $i$ is defined as $\\frac{i}{m} \\alpha$. Formally,\n$$\nk^{\\ast} = \\max\\left\\{ i \\in \\{1,\\dots,m\\} \\,:\\, p_{(i)} \\le \\frac{i}{m}\\,\\alpha \\right\\}\n$$\nIf no such $i$ exists, the set is considered empty and we define $k^{\\ast}=0$. The hypotheses corresponding to the first $k^{\\ast}$ sorted p-values, i.e., those with original indices $\\{\\pi(1), \\dots, \\pi(k^{\\ast})\\}$, are rejected. This set of rejected indices must be presented in increasing order.\n\nThird, we compute the Benjamini–Hochberg adjusted $p$-values, or $q$-values. The $q$-value for a given hypothesis provides the minimum FDR level at which that hypothesis would be rejected. The q-values corresponding to the sorted p-values, denoted $q_{(i)}$, are given by the formula:\n$$\nq_{(i)} = \\min\\left(1, \\min_{k \\ge i} \\frac{m}{k}\\, p_{(k)}\\right), \\quad i=1,\\dots,m\n$$\nThis definition ensures that the sequence of sorted q-values is non-decreasing: $q_{(1)} \\le q_{(2)} \\le \\cdots \\le q_{(m)}$. An efficient method to compute these values is to first calculate the intermediate values $v_k = \\frac{m}{k} p_{(k)}$ for $k=1, \\dots, m$. Then, one can compute the cumulative minimum of this sequence in reverse. That is, $q_{(m)} = \\min(1, v_m)$, and for $i = m-1, \\dots, 1$, we have $q_{(i)} = \\min(1, v_i, q_{(i+1)})$. This enforces the non-decreasing property by propagating the smallest adjusted value from higher ranks to lower ranks.\n\nFinally, the computed $q_{(i)}$ values must be re-ordered to match the original sequence of $p$-values. If $p_{(i)} = p_{\\pi(i)}$ and $q_{(i)}$ is the corresponding sorted q-value, then the final $q$-value for the original hypothesis $\\pi(i)$ is $q_{\\pi(i)} = q_{(i)}$. The final list of $q$-values, $Q = [q_1, q_2, \\dots, q_m]$, must be presented with all floating-point numbers rounded to $6$ decimal places.\n\nThe entire process is a direct application of these definitions and will be implemented for each test case to produce the required outputs: the number of rejections $k^{\\ast}$, the sorted list of rejected indices $L$, and the list of $q$-values $Q$ in original order.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the Benjamini-Hochberg problem for all test cases.\n    \"\"\"\n\n    def compute_bh(p_values, alpha):\n        \"\"\"\n        Performs the Benjamini-Hochberg procedure on a list of p-values.\n\n        Args:\n            p_values (list): A list of floating-point p-values.\n            alpha (float): The target False Discovery Rate (FDR) level.\n\n        Returns:\n            tuple: A tuple containing:\n                - k_star (int): The number of rejected hypotheses.\n                - rejected_indices (list): A sorted list of 0-based indices of rejected hypotheses.\n                - q_values (list): The list of q-values in their original order.\n        \"\"\"\n        m = len(p_values)\n        if m == 0:\n            return 0, [], []\n\n        # Step 1: Sort p-values with tie-breaking\n        # Use np.lexsort for stable sorting: sort by p-value, then by original index\n        original_indices = np.arange(m)\n        p_values_np = np.array(p_values)\n        \n        # argsort_indices provides the indices that would sort the p_values array.\n        # We use a stable sort to respect the original index tie-breaking rule.\n        # A simpler way is to use np.lexsort.\n        # lexsort sorts by the last key first. So we pass original_indices, then p_values.\n        sorted_indices_map = np.lexsort((original_indices, p_values_np))\n\n        p_sorted = p_values_np[sorted_indices_map]\n        original_indices_sorted = original_indices[sorted_indices_map]\n        \n        # Step 2: Find k*\n        k_star = 0\n        for i in range(m - 1, -1, -1):\n            rank = i + 1\n            if p_sorted[i] <= (rank / m) * alpha:\n                k_star = rank\n                break\n        \n        # Step 3: Get rejected indices\n        if k_star > 0:\n            rejected_indices = sorted(original_indices_sorted[:k_star])\n        else:\n            rejected_indices = []\n\n        # Step 4: Compute q-values\n        q_values_sorted = np.zeros(m)\n        if m > 0:\n            # Calculate m/k * p_(k) for all k\n            temp_q = m / (np.arange(m) + 1) * p_sorted\n            \n            # Compute cumulative minimum from the end\n            q_values_sorted[m - 1] = temp_q[m - 1]\n            for i in range(m - 2, -1, -1):\n                q_values_sorted[i] = min(q_values_sorted[i + 1], temp_q[i])\n            \n            # Ensure q-values do not exceed 1\n            q_values_sorted = np.minimum(q_values_sorted, 1.0)\n        \n        # Reorder q-values to original order\n        q_values = np.zeros(m)\n        q_values[original_indices_sorted] = q_values_sorted\n        \n        return k_star, rejected_indices, q_values.tolist()\n\n    test_cases = [\n        ([0.002, 0.12, 0.03, 0.20, 0.001, 0.04, 0.50, 0.0005, 0.07, 0.15], 0.1),\n        ([0.01, 0.01, 0.01, 0.02, 0.02], 0.05),\n        ([0.6, 0.8, 0.9, 0.7, 0.95, 1.0], 0.2),\n        ([0.0, 0.2, 0.001, 0.5], 0.0)\n    ]\n\n    results_str = []\n    for p_values, alpha in test_cases:\n        k_star, L, Q = compute_bh(p_values, alpha)\n        \n        # Format Q to 6 decimal places\n        q_str_list = [f\"{q:.6f}\" for q in Q]\n        q_str = f\"[{','.join(q_str_list)}]\"\n        \n        # Format L to remove spaces\n        l_str = str(L).replace(\" \", \"\")\n        \n        case_result_str = f\"[{k_star},{l_str},{q_str}]\"\n        results_str.append(case_result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the Benjamini-Hochberg procedure provides a general framework, proteomics practice relies heavily on a specialized technique: the target-decoy approach. The statistical validity of this method hinges on the careful construction of a decoy database that mirrors the properties of the real target database. This conceptual exercise  challenges you to critically evaluate a plausible but flawed experimental design, honing your ability to reason about the core assumptions that ensure a reliable estimation of the False Discovery Rate.",
            "id": "2389441",
            "problem": "You are planning a shotgun liquid chromatography–tandem mass spectrometry experiment on a human cell lysate and want to control the False Discovery Rate (FDR) at the peptide-spectrum match (PSM) level. Instead of using conventional reversed or shuffled decoys, a colleague suggests appending the complete Saccharomyces cerevisiae proteome to the human reference proteome and treating yeast as a “pseudo-decoy” to estimate and control FDR via target–decoy competition. Assume tryptic digestion with standard specificity, typical mass tolerances, and that the yeast proteome is roughly $1/3$ the size of the human proteome in terms of the number of proteins. Consider the general behavior of database searches, that incorrect PSMs tend to be distributed across the available peptide search space, and that some tryptic peptides are conserved across distant species. No explicit formulas are provided, and you must reason from first principles (definitions of FDR and properties of decoy-based estimation).\n\nWhich of the following statements best captures a sound experimental design and the severe pitfalls of this “pseudo-decoy” approach? Choose all that apply.\n\nA. Using a distant-species “pseudo-decoy” as the only decoy breaks the equal-chance premise of target–decoy competition because the pseudo-decoy peptide space and composition differ from the human target. If the pseudo-decoy database is substantially smaller than the target, the observed number of pseudo-decoy PSMs among incorrect matches will be reduced roughly in proportion to search-space size, tending to underestimate FDR at the PSM level unless an explicit and accurate search-space scaling is applied.\n\nB. Because some tryptic peptides are conserved or highly similar between human and yeast, a fraction of true human spectra can achieve top scores against yeast entries and be labeled as decoy under competition. This inflates the pseudo-decoy hit count with genuine signals, leading to overly conservative FDR estimates and a loss of sensitivity.\n\nC. A more defensible design is to keep a standard reversed (or shuffled) decoy for the combined target (human) and entrapment (yeast) databases, control PSM-level FDR using conventional target–decoy competition on these synthetic decoys, and then use the fraction of accepted PSMs assigned to yeast targets as an empirical “entrapment” calibration check for the FDR. This provides a way to detect miscalibration caused by database or scoring mismatches.\n\nD. Using yeast as a pseudo-decoy eliminates protein-inference ambiguity because human and yeast share no tryptic peptides of identical mass within typical mass tolerances, so shared-peptide problems no longer arise.\n\nE. If the yeast database is approximately $3$ times smaller than the human database, then estimating FDR as $3$ times the ratio of yeast hits to human hits guarantees an unbiased FDR estimate, because database size alone determines decoy behavior regardless of sequence composition and homologous peptides.",
            "solution": "The problem statement describes a proposed method for false discovery rate (FDR) control in proteomics that substitutes a standard synthetic decoy database (e.g., reversed sequences) with a \"pseudo-decoy\" database derived from the proteome of a distant species, *Saccharomyces cerevisiae* (yeast), for the analysis of a human sample. This proposal must be evaluated based on the fundamental principles of target-decoy competition (TDC).\n\nThe core principle of TDC for FDR estimation is that for any given experimental spectrum that does not originate from a true peptide in the sample (the null case), the probability of it matching a sequence in the target database is equal to the probability of it matching a sequence in the decoy database. To achieve this \"equal-chance\" premise, a standard decoy database is constructed to have the same size, amino acid composition, and precursor mass distribution as the target database. This is typically achieved by reversing or shuffling the sequences of the target proteins. The proposed method violates this fundamental premise in several critical ways.\n\nLet $T$ denote the human target database and $D_{pseudo}$ denote the yeast \"pseudo-decoy\" database. The sample contains only human peptides.\n\n$1$. **Database Size and Composition Mismatch**: The problem states that the yeast proteome is approximately $\\frac{1}{3}$ the size of the human proteome in terms of protein count, which implies a significantly smaller peptide search space, i.e., $|D_{pseudo}| \\approx \\frac{1}{3} |T|$. Because incorrect peptide-spectrum matches (PSMs) are assumed to be distributed somewhat randomly across the available peptide search space, an incorrect spectrum is approximately $3$ times more likely to find a spurious match in the larger human database than in the smaller yeast database. Furthermore, the amino acid composition and resulting tryptic peptide properties (e.g., length, charge, hydrophobicity) of yeast differ from humans. This means the score distribution for incorrect matches against yeast peptides will not be the same as for incorrect matches against human peptides. Both the size and composition differences break the equal-chance premise of TDC.\n\n$2$. **Presence of Conserved Peptides**: Evolutionarily conserved proteins (e.g., histones, actin, metabolic enzymes) share identical or nearly identical tryptic peptide sequences between human and yeast. A true spectrum from a human peptide that is conserved in yeast can match the yeast peptide sequence with a high score. In a competitive search, if the match to the yeast peptide is the top-scoring one, this genuine signal from the human sample will be incorrectly classified as a \"decoy\" hit.\n\nWith these principles in mind, we evaluate each option.\n\n**A. Using a distant-species “pseudo-decoy” as the only decoy breaks the equal-chance premise of target–decoy competition because the pseudo-decoy peptide space and composition differ from the human target. If the pseudo-decoy database is substantially smaller than the target, the observed number of pseudo-decoy PSMs among incorrect matches will be reduced roughly in proportion to search-space size, tending to underestimate FDR at the PSM level unless an explicit and accurate search-space scaling is applied.**\n\nThis statement is a correct and precise description of one of the primary flaws. It correctly identifies that the difference in database size and composition breaks the foundational \"equal-chance\" premise of TDC. With $|D_{pseudo}| < |T|$, the number of null matches found in the pseudo-decoy database, $N_{D_{pseudo}}$, will be proportionally smaller than the number of null matches found in the target database, $N_{T, \\text{FP}}$. Using $N_{D_{pseudo}}$ as an estimator for the number of false positives in the target list, $N_{T, \\text{FP}}$, without correction, will lead to a significant underestimation of the true FDR. For example, a naive estimate might be $\\text{FDR}_{\\text{est}} = N_{D_{pseudo}} / N_T$, while the true FDR is closer to $(|T|/|D_{pseudo}|) \\times (N_{D_{pseudo}} / N_T)$. This underestimation allows an unacceptably high number of false positives to pass the filter.\n**Verdict: Correct.**\n\n**B. Because some tryptic peptides are conserved or highly similar between human and yeast, a fraction of true human spectra can achieve top scores against yeast entries and be labeled as decoy under competition. This inflates the pseudo-decoy hit count with genuine signals, leading to overly conservative FDR estimates and a loss of sensitivity.**\n\nThis statement accurately describes the second major pitfall. True signals from human peptides that are conserved in yeast can be misclassified as decoy hits. This inflates the number of observed pseudo-decoy hits, $N_{D_{pseudo}}$. When this inflated number is used in the numerator of the FDR calculation (e.g., $\\text{FDR} \\propto N_{D_{pseudo}}$), the resulting FDR for any given score threshold will be overestimated. To achieve a target FDR, such as $1\\%$, one would need to apply a much more stringent score threshold than is actually necessary. This discards many true positive PSMs that fall below this artificially high threshold, resulting in a loss of sensitivity (i.e., fewer correct identifications). The estimate is thus overly conservative.\n**Verdict: Correct.**\n\n**C. A more defensible design is to keep a standard reversed (or shuffled) decoy for the combined target (human) and entrapment (yeast) databases, control PSM-level FDR using conventional target–decoy competition on these synthetic decoys, and then use the fraction of accepted PSMs assigned to yeast targets as an empirical “entrapment” calibration check for the FDR. This provides a way to detect miscalibration caused by database or scoring mismatches.**\n\nThis statement describes a valid and powerful quality control methodology known as the \"entrapment database\" method. In this design, one is not using the yeast database as the *decoy*, but as a known-negative part of the *target* database. A proper synthetic decoy database (e.g., reversed) is created for the combined human-plus-yeast target database. This allows for a valid FDR calculation using standard TDC. After filtering the PSMs at a target FDR (e.g., $1\\%$), the accepted hits assigned to yeast peptides can be counted. Since the sample contains no yeast protein, all such hits are by definition false positives. The rate of these \"entrapment hits\" among all accepted hits serves as an empirical measurement of the actual FDR. If the empirical rate is close to the target FDR, it provides confidence in the results. If it is significantly different, it signals a problem with the analysis that requires investigation. This is an excellent and sound experimental design.\n**Verdict: Correct.**\n\n**D. Using yeast as a pseudo-decoy eliminates protein-inference ambiguity because human and yeast share no tryptic peptides of identical mass within typical mass tolerances, so shared-peptide problems no longer arise.**\n\nThis statement is factually incorrect. The central premise, that \"human and yeast share no tryptic peptides of identical mass,\" is false. Many fundamental proteins are highly conserved across eukaryotes, leading to a significant number of identical or isobaric tryptic peptides. Thus, the problem of shared peptides, which complicates the inference of proteins from identified peptides, is not eliminated and may even be exacerbated by adding a large set of homologous proteins to the search space.\n**Verdict: Incorrect.**\n\n**E. If the yeast database is approximately $3$ times smaller than the human database, then estimating FDR as $3$ times the ratio of yeast hits to human hits guarantees an unbiased FDR estimate, because database size alone determines decoy behavior regardless of sequence composition and homologous peptides.**\n\nThis statement proposes a naive correction for the size difference, $\\text{FDR}_{\\text{est}} = 3 \\times (N_{D_{pseudo}} / N_T)$. While correcting for size is a step in the right direction, the statement makes two fatally strong claims. First, it asserts that this correction \"guarantees an unbiased FDR estimate.\" This is false because it ignores the two other critical, confounding factors: ($1$) differences in sequence composition between human and yeast, which affects score distributions of null matches, and ($2$) the inflation of yeast hits by conserved true human peptides (as described in option B), which biases the estimate in the opposite direction. The net result of these competing biases is unknown and certainly not guaranteed to be unbiased. Second, it wrongly claims that \"database size alone determines decoy behavior.\" This is a gross oversimplification that contradicts the principles of TDC.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{ABC}$$"
        }
    ]
}