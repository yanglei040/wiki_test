{
    "hands_on_practices": [
        {
            "introduction": "A successful differential expression analysis hinges on correctly interpreting its two primary outputs for each gene: the effect size, typically measured as the $\\log_2(\\text{Fold Change})$, and its statistical significance, given by the $p$-value. This exercise  tackles a classic scenario that often puzzles newcomers, where a large fold change is paired with a high $p$-value. Understanding how to interpret this apparent contradiction is fundamental to separating meaningful biological signals from experimental noise.",
            "id": "2281817",
            "problem": "A team of computational biologists is conducting a differential gene expression analysis using Ribonucleic Acid (RNA)-sequencing data. Their goal is to identify genes whose expression levels are altered by a new experimental drug in a cancer cell line. They compare gene expression in drug-treated cells to that in untreated control cells. For each gene, their analysis pipeline calculates two key metrics:\n\n1.  **$\\log_2(\\text{Fold Change})$**: This value quantifies the magnitude and direction of the expression change. A positive value indicates upregulation (increased expression), while a negative value indicates downregulation. For example, a $\\log_2(\\text{Fold Change})$ of 2.0 corresponds to a $2^{2} = 4$-fold increase in expression.\n2.  **p-value**: This value assesses the statistical significance of the observed fold change. It represents the probability of observing a fold change at least as extreme as the one measured, assuming that the drug has no actual effect (the null hypothesis). A smaller p-value indicates stronger evidence against the null hypothesis. Conventionally, a p-value less than 0.05 is considered statistically significant.\n\nThe team's analysis of a particular gene, named `REG-17`, yields a $\\log_2(\\text{Fold Change})$ of 4.5 and a p-value of 0.38.\n\nWhich of the following statements provides the most accurate and sound interpretation of this result for the `REG-17` gene?\n\nA. The drug causes a very large and statistically significant upregulation of `REG-17`, making it a high-priority candidate for further investigation.\n\nB. A large upregulation of `REG-17` was observed in the experiment, but due to high variability in the data or an insufficient sample size, we cannot be confident that this change is a real effect of the drug rather than a result of random chance.\n\nC. The drug has no meaningful effect on `REG-17` because the observed change is not statistically significant.\n\nD. The calculation must be incorrect, as a $\\log_2(\\text{Fold Change})$ as large as 4.5 cannot be associated with a p-value as high as 0.38.\n\nE. `REG-17` is a biologically unimportant gene, which is why its expression change did not reach statistical significance.",
            "solution": "To determine the correct interpretation, we must carefully consider the meaning of both the $\\log_2(\\text{Fold Change})$ and the p-value.\n\n1.  **Interpreting the $\\log_2(\\text{Fold Change})$:** The $\\log_2(\\text{Fold Change})$ is a measure of the effect size. A value of 4.5 for `REG-17` means that the average expression of this gene in the drug-treated group was $2^{4.5} \\approx 22.6$ times higher than in the control group. This is a very large magnitude of change from a biological perspective. It indicates that a strong upregulation was indeed *observed* in the collected data.\n\n2.  **Interpreting the p-value:** The p-value assesses the statistical reliability of the observed effect. A p-value of 0.38 means there is a 38% probability of observing a fold change of this magnitude (or greater) purely by random chance, even if the drug had no true effect on the gene's expression (i.e., if the null hypothesis were true). In biomedical research, a conventional threshold for statistical significance is a p-value less than 0.05. Since 0.38 is much larger than 0.05, the result is considered **not statistically significant**. This high p-value suggests that we cannot confidently rule out random variation as the cause of the observed change. The lack of statistical significance often arises from high variability between the replicate samples within a group (e.g., some treated cells responded strongly, others not at all) or an insufficient number of replicates, which reduces the statistical power of the experiment to detect a true effect.\n\n3.  **Synthesizing the two metrics:** We have a seemingly contradictory situation: a very large effect size ($\\log_2(\\text{Fold Change})$ = 4.5) paired with a lack of statistical significance (p-value = 0.38). The correct interpretation synthesizes these two facts. It acknowledges the large change seen in the data but also acknowledges the high uncertainty surrounding this observation. We have observed something dramatic, but we cannot be sure it's a real, repeatable effect of the drug.\n\nNow, let's evaluate the given options based on this understanding:\n\n*   **A. The drug causes a very large and statistically significant upregulation of `REG-17`, making it a high-priority candidate for further investigation.** This is incorrect. While the upregulation is large, it is explicitly **not** statistically significant (p-value = 0.38 > 0.05).\n\n*   **B. A large upregulation of `REG-17` was observed in the experiment, but due to high variability in the data or an insufficient sample size, we cannot be confident that this change is a real effect of the drug rather than a result of random chance.** This is the correct interpretation. It accurately reports the large observed effect (large upregulation) while correctly interpreting the high p-value as a lack of confidence in the result, rightfully pointing to common causes like high variability or low sample size.\n\n*   **C. The drug has no meaningful effect on `REG-17` because the observed change is not statistically significant.** This is an incorrect conclusion. \"Absence of evidence is not evidence of absence.\" The experiment failed to provide *statistically significant evidence* of an effect, but this does not prove there is *no effect*. The large fold change suggests an effect might be present, but the experiment lacked the power to confirm it.\n\n*   **D. The calculation must be incorrect, as a $\\log_2(\\text{Fold Change})$ as large as 4.5 cannot be associated with a p-value as high as 0.38.** This is incorrect. This is a very common scenario in biological experiments, particularly those with a small number of replicates or inherently \"noisy\" biological systems where variance within groups is high.\n\n*   **E. `REG-17` is a biologically unimportant gene, which is why its expression change did not reach statistical significance.** This is an unsupported leap in logic. The statistical significance in a single experiment says nothing about the overall biological importance of a gene. A gene could be critically important, but the drug might not affect it, or the experiment might not have been robust enough to detect the change.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "After conducting an experiment and running the analysis pipeline, one of the most common and sometimes perplexing outcomes is finding zero differentially expressed genes. Far from being a simple failure, this result is a valuable diagnostic clue about your experiment. This practice  challenges you to think like a seasoned bioinformatician, identifying plausible reasons for a null result that range from low statistical power and design flaws to the simple truth of no biological effect.",
            "id": "2385515",
            "problem": "You perform a two-condition RNA-sequencing differential gene expression analysis with $n=3$ biological replicates per group using a standard count-based method that models counts with a negative binomial distribution and controls the False Discovery Rate (FDR) at $q=0.05$. The result reports $0$ significant genes at $q<0.05$. Which of the following are plausible reasons for observing $0$ discoveries in this setting? Select all that apply; exactly 5 options are correct.\n\nA. The sample size per group is small (e.g., $n=3$) and gene-wise dispersion is high, so estimated standard errors are large; after multiple-testing correction at $q=0.05$, no gene passes the threshold.\n\nB. The sequencing depth is low across samples, yielding low counts and high uncertainty for many genes, which reduces statistical power and can lead to $0$ discoveries.\n\nC. The design matrix includes a batch factor that is perfectly confounded with condition (for example, all condition $1$ samples in batch $A$ and all condition $2$ samples in batch $B$), making the condition coefficient non-estimable; the software therefore cannot test the condition effect and returns no significant genes.\n\nD. The condition labels in the sample metadata were accidentally set to the same value for all samples, so the method tested a null contrast and returned no significant genes.\n\nE. A scatterplot of the first two components from Principal Component Analysis (PCA) showed overlap between groups; therefore, there cannot be differential expression, so zero discoveries are expected.\n\nF. The true underlying biology has negligible expression changes between conditions; that is, for all genes the null hypothesis $H_0$ is true, so after FDR control at $q=0.05$ it is plausible to obtain $0$ rejections.\n\nG. The Benjamini–Hochberg FDR procedure at $q=0.05$ always yields at least some discoveries when many genes are tested; therefore, zero significant genes implies a software error rather than a plausible outcome.\n\nH. An excessively aggressive independent filtering step that removed nearly all genes prior to testing cannot reduce the chance of discovery because it lowers the multiple-testing burden, so it cannot explain zero discoveries.\n\nI. Using a two-sided test instead of a one-sided test guarantees zero significant genes when true effects exist in both directions, so the sidedness choice explains the absence of discoveries.",
            "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n-   **Experiment:** Two-condition RNA-sequencing differential gene expression analysis.\n-   **Replication:** $n=3$ biological replicates per group.\n-   **Statistical Model:** A count-based method using the negative binomial distribution.\n-   **Multiple Testing Control:** False Discovery Rate (FDR) is controlled at a threshold $q=0.05$.\n-   **Outcome:** The analysis reports $0$ significant genes.\n-   **Question:** Identify the plausible reasons for observing $0$ discoveries.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes a standard and common scenario in bioinformatics and computational biology. The use of RNA-sequencing, a two-group comparison, a small sample size ($n=3$), modeling counts with a negative binomial distribution (as done by popular tools like DESeq2 and edgeR), and controlling the FDR are all established and scientifically sound practices.\n-   **Well-Posed:** The problem is well-posed. It asks for an explanation of a realistic experimental outcome, which requires understanding the statistical and practical factors that influence the power and validity of a differential expression analysis. A definite set of plausible explanations exists.\n-   **Objective:** The problem is stated using precise, objective, and technical language common to the field of bioinformatics. There are no subjective or ambiguous terms.\n\nThe problem is free of scientific unsoundness, is formalizable, is complete, and is realistically structured.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. A full solution will be derived.\n\nThe problem asks for plausible reasons why a standard RNA-sequencing differential gene expression (DE) analysis with $n=3$ replicates per condition resulted in zero significant gene discoveries at an FDR of $q=0.05$. In such an analysis, for each gene $g$, raw counts are modeled using a negative binomial (NB) distribution, which is characterized by a mean $\\mu_g$ and a dispersion parameter $\\phi_g$. The variance is given by $\\sigma_g^2 = \\mu_g + \\phi_g \\mu_g^2$. A generalized linear model (GLM) is used to test the hypothesis of no difference in expression between the two conditions, yielding a $p$-value for each gene. These $p$-values are then adjusted for multiple testing to control the FDR. Discovering zero genes means that for all tested genes, the adjusted $p$-value (or $q$-value) was greater than or equal to $0.05$. This typically occurs when the raw $p$-values are not sufficiently small, which can be due to a variety of factors related to statistical power, experimental design, or the underlying biology. Each option is evaluated below.\n\nA. The sample size per group is small (e.g., $n=3$) and gene-wise dispersion is high, so estimated standard errors are large; after multiple-testing correction at $q=0.05$, no gene passes the threshold.\nThe power of a statistical test depends on the effect size, the sample size, and the variance of the data. For DE analysis, the test statistic for a gene (e.g., a Wald statistic) is the estimated log$_2$-fold change ($\\hat{\\beta}$) divided by its standard error ($SE(\\hat{\\beta})$). A small sample size, such as $n=3$, provides very limited data to estimate the within-group variance, which is captured by the dispersion parameter $\\phi$ in the NB model. High biological variability among replicates translates to a high gene-wise dispersion. A high dispersion leads to a large variance ($\\sigma^2 = \\mu + \\phi\\mu^2$) and consequently a large standard error for the coefficient estimate. A large $SE(\\hat{\\beta})$ reduces the magnitude of the test statistic, leading to a larger (less significant) $p$-value. When this occurs for most or all genes, even the smallest raw $p$-values may not be small enough to survive the stringent correction for multiple testing across thousands of genes. This scenario of low power due to small $n$ and high variance is a classic and very frequent reason for finding zero DE genes.\n**Verdict: Correct**\n\nB. The sequencing depth is low across samples, yielding low counts and high uncertainty for many genes, which reduces statistical power and can lead to $0$ discoveries.\nSequencing depth refers to the average number of reads sequenced per sample. Low sequencing depth results in low read counts for a majority of genes. For count data, the variance is inherently linked to the mean. At low counts, the uncertainty relative to the measurement is high (e.g., for Poisson counts, the coefficient of variation is $1/\\sqrt{\\text{count}}$). This makes it difficult to obtain precise estimates of the mean expression levels for each group. Consequently, the ability to detect a statistically significant difference between the groups is diminished. Genes must be sampled with sufficient depth to overcome this sampling noise. If most genes have low counts, the statistical power of the experiment is severely compromised, which can easily lead to zero discoveries after FDR correction.\n**Verdict: Correct**\n\nC. The design matrix includes a batch factor that is perfectly confounded with condition (for example, all condition $1$ samples in batch $A$ and all condition $2$ samples in batch $B$), making the condition coefficient non-estimable; the software therefore cannot test the condition effect and returns no significant genes.\nProper experimental design is critical. If an experimental variable (like `condition`) is perfectly correlated with a technical variable (like `batch`), the two factors are confounded. In a GLM framework, this leads to a design matrix that is not of full rank (i.e., it is singular). Consequently, the model cannot simultaneously estimate the effect of batch and the effect of condition because there is no way to mathematically disentangle them. Standard statistical software (like DESeq2) will detect this issue and report that the `condition` coefficient is not estimable. Since the coefficient cannot be estimated, no statistical test can be performed for the effect of `condition`. The software will therefore return no results (or results with `NA` $p$-values) for this contrast, leading to an output of $0$ significant genes. This is a catastrophic design flaw that makes the intended analysis impossible.\n**Verdict: Correct**\n\nD. The condition labels in the sample metadata were accidentally set to the same value for all samples, so the method tested a null contrast and returned no significant genes.\nThis represents a common user error. The statistical software relies on a metadata table to associate samples with their respective conditions. If, by mistake, all samples are assigned to the same condition (e.g., `condition 1`), the software is being asked to compare `condition 1` to itself. This is a null contrast. The true biological difference is, by definition, $0$. The statistical test will correctly find no evidence of a difference. The estimated fold changes will be centered around $0$, and the resulting $p$-values will be high (distributed uniformly between $0$ and $1$). After multiple testing correction, it is extremely unlikely that any gene would be declared significant.\n**Verdict: Correct**\n\nE. A scatterplot of the first two components from Principal Component Analysis (PCA) showed overlap between groups; therefore, there cannot be differential expression, so zero discoveries are expected.\nPCA is an unsupervised method that summarizes the major axes of variance in the data. If samples cluster cleanly by condition, it suggests the condition effect is a large source of variance. If they overlap, it suggests that other factors (e.g., inter-replicate variability, batch effects) are large relative to the condition effect. While this overlap is consistent with a weak biological signal and thus low power (making zero discoveries a likely outcome), the reasoning presented in the option is fallacious. The statement \"therefore, there cannot be differential expression\" is an invalid conclusion. PCA reflects the dominant variance sources across all genes. It is possible for a small subset of genes to be strongly differentially expressed, yet their signal is \"drowned out\" in the overall variance structure captured by the first few PCs. The lack of separation in a PCA plot does not rule out the existence of DE genes. It is a symptom, not a guarantee of a null result. The explanation is logically flawed.\n**Verdict: Incorrect**\n\nF. The true underlying biology has negligible expression changes between conditions; that is, for all genes the null hypothesis $H_0$ is true, so after FDR control at $q=0.05$ it is plausible to obtain $0$ rejections.\nThis is the \"global null\" hypothesis. It is entirely possible that the experimental conditions being compared do not induce any substantial changes in gene expression at the transcriptional level. In this scenario, the null hypothesis ($H_0$: no difference in expression) is true for all, or nearly all, genes. The purpose of statistical testing and FDR control is precisely to avoid making false discoveries in such a situation. When all null hypotheses are true, the resulting raw $p$-values are expected to follow a Uniform($0,1$) distribution. The FDR procedure, such as Benjamini-Hochberg, is designed to be conservative under the global null, and the most probable outcome is rejecting zero hypotheses (i.e., finding zero significant genes). This is not an error but the correct functioning of the statistical method.\n**Verdict: Correct**\n\nG. The Benjamini–Hochberg FDR procedure at $q=0.05$ always yields at least some discoveries when many genes are tested; therefore, zero significant genes implies a software error rather than a plausible outcome.\nThis statement is factually incorrect. The Benjamini-Hochberg (BH) procedure does not guarantee discoveries. It provides a statistical guarantee on the proportion of false discoveries *if any discoveries are made*. The procedure begins by ordering all $m$ raw $p$-values: $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$. It then finds the largest index $k$ such that $p_{(k)} \\le \\frac{k}{m}q$. If even the smallest $p$-value, $p_{(1)}$, does not satisfy this condition (i.e., if $p_{(1)} > q/m$), then no gene will be declared significant. As discussed in (A), (B), and (F), it is very common for all raw $p$-values to be insufficiently small, leading the BH procedure to correctly report zero discoveries.\n**Verdict: Incorrect**\n\nH. An excessively aggressive independent filtering step that removed nearly all genes prior to testing cannot reduce the chance of discovery because it lowers the multiple-testing burden, so it cannot explain zero discoveries.\nIndependent filtering aims to increase statistical power by removing genes that are unlikely to show significant evidence of DE (e.g., those with very low expression counts) before testing. This reduces the total number of tests, $m$, making the multiple-testing correction less stringent. However, the option describes \"excessively aggressive\" filtering. If the filtering criteria are too strict, they may erroneously remove genes that are, in fact, truly differentially expressed. For instance, a filter based on a high mean count could discard a gene that is lowly expressed but has a large and biologically important fold change. If such genes are removed, they are never tested and thus can never be discovered. The premise that filtering \"cannot reduce the chance of discovery\" is false; a poorly designed, aggressive filter can easily be the direct cause of finding zero significant genes.\n**Verdict: Incorrect**\n\nI. Using a two-sided test instead of a one-sided test guarantees zero significant genes when true effects exist in both directions, so the sidedness choice explains the absence of discoveries.\nThis statement is incorrect. A two-sided test (alternative hypothesis $H_A: \\beta \\neq 0$) is the standard and appropriate choice for DE analysis, as it is designed to detect changes in expression regardless of direction (up- or down-regulation). The presence of true effects in both directions is the typical biological scenario. A two-sided test does not \"guarantee\" zero discoveries; it is fully capable of detecting them. While a one-sided test ($H_A: \\beta > 0$ or $H_A: \\beta < 0$) is more powerful for an effect in the pre-specified direction, it will have no power to detect an effect in the opposite direction. The use of a two-sided test is a standard, sound practice and does not explain a total lack of discoveries.\n**Verdict: Incorrect**\n\nSummary of plausible reasons: A, B, C, D, F.",
            "answer": "$$\\boxed{ABCDF}$$"
        },
        {
            "introduction": "As explored in our diagnostic exercise , analyzing thousands of genes requires a correction for multiple hypothesis testing to avoid a flood of false positives. This hands-on coding challenge  demystifies this process by guiding you through the implementation of the Benjamini-Hochberg procedure, the most common method for controlling the False Discovery Rate (FDR). By translating the core mathematical algorithm into functional code, you will gain a deep and practical understanding of this essential statistical tool.",
            "id": "2385494",
            "problem": "You are given collections of raw $p$-values arising from $m$ independent gene-level null hypotheses in a differential gene expression experiment comparing two conditions. Let the set of null hypotheses be $\\{H_1,\\dots,H_m\\}$ with corresponding raw $p$-values $p_1,\\dots,p_m \\in [0,1]$. For a target level $q \\in (0,1)$, the Benjamini–Hochberg (BH) procedure for controlling the False Discovery Rate (FDR) at level $q$ is defined as follows. Write the $p$-values in nondecreasing order as $p_{(1)} \\le \\dots \\le p_{(m)}$, where the subscript denotes order statistics. Define the index\n$$\nk \\;=\\; \\max\\left\\{ i \\in \\{1,\\dots,m\\} \\;:\\; p_{(i)} \\le \\frac{i}{m}\\,q \\right\\},\n$$\nwith the convention that if the set is empty then no hypotheses are rejected. The BH rejection set is then all hypotheses with raw $p$-values not exceeding the threshold $p_{(k)}$, that is,\n$$\n\\mathcal{R} \\;=\\; \\{ j \\in \\{1,\\dots,m\\} \\;:\\; p_j \\le p_{(k)} \\},\n$$\nwhen $k$ exists, and $\\mathcal{R}=\\varnothing$ otherwise. The BH adjusted $p$-values (also called BH $q$-values in some contexts) are defined for each hypothesis by first assigning to the $i$-th order statistic the value\n$$\n\\tilde{p}_{(i)} \\;=\\; \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}\\,p_{(j)} \\right),\n$$\nthen truncating at $1$ by $\\min\\{\\tilde{p}_{(i)},1\\}$, and finally mapping back to the original hypothesis order. To make the rank assignment deterministic in the presence of ties among equal $p$-values, use a stable ordering that breaks ties by increasing original index, that is, if $p_a=p_b$ and $a<b$ then $p_a$ precedes $p_b$ in the order. Index hypotheses by their original positions using $0$-based indices.\n\nTask. For each test case below, given a list of raw $p$-values and a target FDR level $q$, compute:\n- the list of BH adjusted $p$-values in the original hypothesis order, each rounded to $6$ decimal places (expressed as decimals, not as percentages), and\n- the list of rejected hypothesis indices (using $0$-based indexing) in increasing order under the BH procedure at level $q$.\n\nTest suite. Evaluate your implementation on the following four cases, which together cover a typical case, boundary values, ties, and a case with no rejections:\n- Case $1$: $p=(0.001, 0.04, 0.03, 0.2, 0.5, 0.0005, 0.07, 0.9)$, $q=0.1$.\n- Case $2$: $p=(0, 1, 0.5, 0.05, 0.2)$, $q=0.05$.\n- Case $3$: $p=(0.02, 0.02, 0.02, 0.5, 0.8, 0.9)$, $q=0.1$.\n- Case $4$: $p=(0.2, 0.3, 0.4, 0.6, 0.8)$, $q=0.01$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one element per test case. Each element must itself be a two-element list whose first element is the list of rounded BH adjusted $p$-values (in original order) and whose second element is the list of rejected $0$-based indices in increasing order. No other text should be printed. For example, the overall structure must be of the form\n[[adj_case1,rej_case1],[adj_case2,rej_case2],[adj_case3,rej_case3],[adj_case4,rej_case4]]\nwhere adj_case$k$ and rej_case$k$ are lists for case $k$ as specified above.",
            "solution": "The problem statement is assessed to be valid. It presents a clear, well-defined computational task based on the standard and scientifically sound Benjamini-Hochberg procedure for controlling the False Discovery Rate, a fundamental method in computational biology and statistics. The definitions, conditions, and test cases are complete, consistent, and objective, permitting a unique and verifiable solution.\n\nThe task is to implement the Benjamini-Hochberg (BH) procedure. Given a set of $m$ raw $p$-values $\\{p_1, \\dots, p_m\\}$ from multiple hypothesis tests and a target False Discovery Rate (FDR) level $q$, we must compute the BH adjusted $p$-values and identify the set of rejected hypotheses. The procedure is deterministic and will be implemented through a sequence of principled steps derived directly from the provided mathematical definitions.\n\nLet $m$ be the total number of hypotheses. The algorithm proceeds as follows:\n\n1.  **Data Structuring and Sorting**: Each raw $p$-value $p_j$ is associated with its original $0$-based index $j$. The resulting pairs $(p_j, j)$ are then sorted in non-decreasing order of $p_j$. The problem specifies a stable sorting mechanism to handle ties: if $p_a=p_b$ for original indices $a<b$, the pair corresponding to $a$ must precede the pair for $b$. This step yields the ordered $p$-values $p_{(1)} \\le p_{(2)} \\le \\dots \\le p_{(m)}$, along with their original indices. The rank of $p_{(i)}$ is $i$.\n\n2.  **Identification of the Rejection Threshold**: The core of the BH procedure is to find the largest rank $k \\in \\{1,\\dots,m\\}$ for which the ordered $p$-value $p_{(k)}$ satisfies the condition:\n    $$\n    p_{(k)} \\le \\frac{k}{m}q\n    $$\n    This condition compares the $k$-th smallest $p$-value to a threshold that becomes linearly more lenient with increasing rank $k$. If the set of ranks satisfying this inequality is empty, it is concluded that no hypotheses can be rejected at the specified FDR level $q$, and the rejection set $\\mathcal{R}$ is empty. Otherwise, the value $p_{(k)}$ corresponding to the largest such rank $k$ becomes the significance threshold.\n\n3.  **Determination of the Rejection Set**: The set of rejected hypotheses, $\\mathcal{R}$, consists of all hypotheses whose original, unadjusted $p$-value $p_j$ is less than or equal to the threshold $p_{(k)}$ found in the previous step. That is, $\\mathcal{R} = \\{ j \\in \\{1,\\dots,m\\} : p_j \\le p_{(k)} \\}$. The final output requires the $0$-based indices of these hypotheses, sorted in increasing order.\n\n4.  **Computation of Adjusted $p$-values**: The BH adjusted $p$-value for the hypothesis corresponding to the $i$-th ordered raw $p$-value, $p_{(i)}$, is defined as:\n    $$\n    \\tilde{p}_{(i)} = \\min_{j \\in \\{i,\\dots,m\\}} \\left( \\frac{m}{j}p_{(j)} \\right)\n    $$\n    This is then truncated at $1$ if necessary. This definition ensures that the adjusted $p$-values are monotonically non-decreasing with respect to their rank, i.e., $\\tilde{p}_{(1)} \\le \\tilde{p}_{(2)} \\le \\dots \\le \\tilde{p}_{(m)}$. An efficient computational strategy for this step involves first computing the scaled values $\\frac{m}{j}p_{(j)}$ for all ranks $j=1, \\dots, m$. Then, one iterates backwards from rank $m$ down to $1$, calculating the cumulative minimum at each step. Specifically, the adjusted $p$-value for rank $i$ is the minimum of its own scaled value and the adjusted $p$-value of rank $i+1$. The final adjusted $p$-values are then mapped back to their original hypothesis order and rounded to $6$ decimal places as required.\n\nThe implementation will apply this four-step logic to each of the provided test cases to generate the required outputs.",
            "answer": "```python\nimport numpy as np\n\ndef _format_output(data):\n    \"\"\"\n    Custom recursive function to format the final list into a string\n    without spaces and with floats formatted to 6 decimal places.\n    \"\"\"\n    if isinstance(data, list):\n        return f\"[{','.join(_format_output(item) for item in data)}]\"\n    if isinstance(data, float):\n        return f\"{data:.6f}\"\n    return str(data)\n\ndef benjamini_hochberg(p_values: np.ndarray, q: float):\n    \"\"\"\n    Performs the Benjamini-Hochberg procedure for FDR control.\n\n    Args:\n        p_values: A numpy array of raw p-values.\n        q: The target False Discovery Rate level.\n\n    Returns:\n        A tuple containing:\n        - A list of BH adjusted p-values, rounded to 6 decimal places, in original order.\n        - A sorted list of 0-based indices of rejected hypotheses.\n    \"\"\"\n    m = len(p_values)\n    if m == 0:\n        return [], []\n        \n    original_indices = np.arange(m)\n    \n    # Sort p-values while keeping track of original indices.\n    # The 'stable' kind ensures that for equal p-values, the original order is preserved,\n    # satisfying the problem's tie-breaking rule.\n    sorted_order_indices = np.argsort(p_values, kind='stable')\n    sorted_p_values = p_values[sorted_order_indices]\n    \n    # --- Step 1: Find rejection threshold ---\n    ranks = np.arange(1, m + 1)\n    bh_thresholds = (ranks / m) * q\n    \n    significant_mask = sorted_p_values <= bh_thresholds\n    \n    rejected_indices = []\n    if np.any(significant_mask):\n        # Find the largest rank k satisfying the BH condition\n        k = np.max(ranks[significant_mask])\n        \n        # The rejection threshold is the p-value at this rank k\n        rejection_threshold = sorted_p_values[k - 1]\n        \n        # Identify all hypotheses with original p-values <= threshold\n        rejected_mask = p_values <= rejection_threshold\n        rejected_indices = original_indices[rejected_mask].tolist()\n\n    # --- Step 2: Calculate adjusted p-values ---\n    # Calculate raw scaled p-values: (m/i) * p_(i)\n    scaled_p_values = (m / ranks) * sorted_p_values\n    \n    # Enforce monotonicity by taking the cumulative minimum from the end (right-to-left)\n    adj_p_sorted = np.minimum.accumulate(scaled_p_values[::-1])[::-1]\n    \n    # Truncate values at 1.0\n    adj_p_sorted = np.minimum(adj_p_sorted, 1.0)\n    \n    # Unsort the adjusted p-values to match the original p-value order\n    adj_p_original = np.empty(m)\n    adj_p_original[sorted_order_indices] = adj_p_sorted\n    \n    # Round to 6 decimal places for final output\n    adj_p_rounded = [round(p, 6) for p in adj_p_original]\n\n    return adj_p_rounded, rejected_indices\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final result.\n    \"\"\"\n    test_cases = [\n        ((0.001, 0.04, 0.03, 0.2, 0.5, 0.0005, 0.07, 0.9), 0.1),\n        ((0.0, 1.0, 0.5, 0.05, 0.2), 0.05),\n        ((0.02, 0.02, 0.02, 0.5, 0.8, 0.9), 0.1),\n        ((0.2, 0.3, 0.4, 0.6, 0.8), 0.01),\n    ]\n\n    results = []\n    for p_tuple, q_val in test_cases:\n        p_values_np = np.array(p_tuple)\n        adj_p, rej_idx = benjamini_hochberg(p_values_np, q_val)\n        results.append([adj_p, rej_idx])\n    \n    # Print the final result in the specified single-line format\n    print(_format_output(results))\n\nsolve()\n```"
        }
    ]
}