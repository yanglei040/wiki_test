{
    "hands_on_practices": [
        {
            "introduction": "The core goal of a CRISPR screen is to identify genes whose perturbation causes a measurable phenotype. This requires robust statistical methods to distinguish true biological signals from experimental noise in high-throughput sequencing counts. This practice guides you through constructing a complete differential analysis pipeline from first principles, mirroring the logic of widely-used bioinformatics tools. By implementing normalization, dispersion estimation, and generalized linear modeling yourself, you will gain a deep, practical understanding of how raw guide counts are transformed into meaningful biological discoveries .",
            "id": "2371981",
            "problem": "You are asked to implement, from first principles, a complete differential analysis procedure for comparing two pooled Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) perturbation screens, in the spirit of Differential Expression analysis for Sequence count data (DESeq2). The goal is to identify condition-specific essential genes, defined here as those with significantly lower guide RNA abundance in one condition relative to the other after proper normalization and statistical modeling.\n\nStart from the following foundational bases and well-tested facts:\n- Count data from pooled screens are nonnegative integers and exhibit overdispersion relative to the Poisson model. A widely used model is the Negative Binomial (NB) distribution with variance function $V(\\mu) = \\mu + \\alpha \\mu^{2}$, where $\\mu$ is the mean and $\\alpha \\ge 0$ is the dispersion.\n- Generalized Linear Models (GLMs) with a log link are standard for modeling counts across conditions. For sample $j$ with size factor $s_{j}$ and a design matrix $X$ having an intercept and a binary condition indicator $x_{j} \\in \\{0,1\\}$, the model is $\\log \\mu_{gj} = \\log s_{j} + \\beta_{g0} + \\beta_{g1} x_{j}$ for gene $g$.\n- The median-of-ratios method provides robust library size normalization. For each sample $j$, the size factor $s_{j}$ is the median across genes of $k_{gj}/G_{g}$, where $k_{gj}$ is the raw count and $G_{g}$ is the geometric mean of $k_{g\\cdot}$ across samples, computed only over genes with strictly positive counts in all samples.\n- The Wald test using the asymptotic normal approximation with variance from the Fisher Information provides a way to test whether a coefficient equals zero.\n- The Benjamini–Hochberg (BH) procedure controls the False Discovery Rate (FDR).\n\nYou must implement the following steps:\n1) Size-factor normalization via the median-of-ratios method:\n   - For each gene $g$, compute the geometric mean $G_{g} = \\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log k_{gj} \\right)$ only if $k_{gj}  0$ for all $j \\in \\{1,\\dots,m\\}$; otherwise exclude gene $g$ from this step.\n   - For each sample $j$, compute the ratios $r_{gj} = k_{gj} / G_{g}$ over all genes $g$ with valid $G_{g}$ and set $s_{j}$ to be the median of $\\{ r_{gj} \\}$.\n   - Center the size factors so that $\\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log s_{j} \\right) = 1$.\n2) Common dispersion estimation by method-of-moments:\n   - For each gene $g$, compute normalized counts $y_{gj} = k_{gj} / s_{j}$ and their mean $\\bar{y}_{g}$ and sample variance $S^{2}_{g}$ across all $m$ samples.\n   - For genes with $\\bar{y}_{g}  0$, define a per-gene dispersion estimate $\\hat{\\alpha}_{g} = \\max\\left(0,\\; \\frac{S^{2}_{g} - \\bar{y}_{g}}{\\bar{y}_{g}^{2}} \\right)$.\n   - Define a single common dispersion $\\hat{\\alpha}$ as the median of $\\{ \\hat{\\alpha}_{g} \\}$ over genes with $\\bar{y}_{g}  0$.\n3) For each gene $g$, fit a Negative Binomial GLM with a log link using Fisher scoring (Iteratively Reweighted Least Squares) with offset $\\log s_{j}$, common dispersion $\\hat{\\alpha}$, and design matrix with intercept and condition indicator $x_{j} \\in \\{0,1\\}$:\n   - Initialize $\\beta_{g0}$ and $\\beta_{g1}$ (for example, $\\beta_{g1}=0$ and $\\beta_{g0}$ as the log of the mean normalized count).\n   - At each iteration, compute the linear predictor $\\eta_{gj} = \\log s_{j} + \\beta_{g0} + \\beta_{g1} x_{j}$, mean $\\mu_{gj} = \\exp(\\eta_{gj})$, weights $w_{gj} = \\frac{\\mu_{gj}}{1 + \\hat{\\alpha}\\mu_{gj}}$, and working response $z_{gj} = \\eta_{gj} + \\frac{k_{gj} - \\mu_{gj}}{\\mu_{gj}}$.\n   - Update $\\beta_{g} = (\\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{z}$ until convergence or a fixed iteration cap, where $\\mathbf{W}$ is diagonal with entries $w_{gj}$.\n4) After convergence for gene $g$, extract the estimated coefficient $\\hat{\\beta}_{g1}$ and its standard error $\\operatorname{se}(\\hat{\\beta}_{g1})$ from the inverse of $(\\mathbf{X}^{\\top} \\mathbf{W} \\mathbf{X})$. Form the Wald statistic $Z_{g} = \\hat{\\beta}_{g1} / \\operatorname{se}(\\hat{\\beta}_{g1})$ and the two-sided $p$-value using the standard normal distribution.\n5) Apply the Benjamini–Hochberg procedure across genes to obtain adjusted $q$-values.\n6) Define a gene to be “condition-specific essential in condition $1$ relative to condition $0$” if two criteria are both satisfied:\n   - The adjusted $q$-value is less than $\\alpha_{\\mathrm{FDR}}$.\n   - The estimated log$_{2}$ fold-change $\\widehat{\\mathrm{LFC}}_{g} = \\hat{\\beta}_{g1} / \\log 2$ is less than or equal to $- \\tau$, where $\\tau$ is a user-specified nonnegative threshold.\n\nGenes with zero counts in all samples, or with $\\bar{y}_{g} = 0$, should be excluded from testing.\n\nYour program must implement the above procedure and run on the following test suite. For each test case, the input consists of a raw count matrix with genes as rows and samples as columns, and a binary condition assignment vector of length equal to the number of samples. Use $\\alpha_{\\mathrm{FDR}} = 0.1$ and $\\tau = 1.0$, and report zero-based indices of genes called condition-specific essential for condition $1$ relative to condition $0$.\n\nTest suite:\n- Case $1$ (balanced replicates, clear differential depletion):\n  - Counts matrix $K$ with $6$ genes and $6$ samples (first $3$ samples condition $0$, last $3$ samples condition $1$):\n    - Gene $0$: $[100, 90, 110, 120, 80, 100]$\n    - Gene $1$: $[50, 45, 55, 60, 40, 50]$\n    - Gene $2$: $[30, 27, 33, 9, 6, 8]$\n    - Gene $3$: $[20, 18, 22, 24, 16, 20]$\n    - Gene $4$: $[10, 9, 11, 3, 2, 3]$\n    - Gene $5$: $[60, 54, 66, 72, 48, 60]$\n  - Condition vector $x = [0, 0, 0, 1, 1, 1]$.\n- Case $2$ (strong library size imbalance, single differential gene):\n  - Counts matrix $K$ with $5$ genes and $4$ samples (first $2$ samples condition $0$, last $2$ samples condition $1$):\n    - Gene $0$: $[160, 40, 120, 56]$\n    - Gene $1$: $[80, 20, 12, 6]$\n    - Gene $2$: $[40, 10, 30, 14]$\n    - Gene $3$: $[30, 7, 23, 10]$\n    - Gene $4$: $[10, 3, 8, 4]$\n  - Condition vector $x = [0, 0, 1, 1]$.\n- Case $3$ (sparse counts with zeros, one strong differential gene):\n  - Counts matrix $K$ with $6$ genes and $4$ samples (first $2$ samples condition $0$, last $2$ samples condition $1$):\n    - Gene $0$: $[2, 1, 2, 1]$\n    - Gene $1$: $[0, 0, 0, 0]$\n    - Gene $2$: $[3, 2, 3, 2]$\n    - Gene $3$: $[1, 0, 1, 0]$\n    - Gene $4$: $[4, 4, 4, 4]$\n    - Gene $5$: $[6, 5, 1, 1]$\n  - Condition vector $x = [0, 0, 1, 1]$.\n\nRequirements and outputs:\n- Implement the complete pipeline exactly as described.\n- For each case, return the sorted list of zero-based gene indices that satisfy the significance and effect-size criteria.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_{1},result_{2},result_{3}]$), where each $result_{i}$ is a list of integers denoting the indices of the called genes for case $i$. For the provided test suite, the program should output a single line like $[[i_{1},i_{2}], [j_{1}], [k_{1}]]$ with the discovered indices.",
            "solution": "The problem presented is a well-posed and scientifically sound task in computational biology. It requires the implementation of a standard statistical pipeline for identifying differentially abundant features from count data, specifically in the context of CRISPR screens. The methodology is a simplified variant of established algorithms like DESeq2, resting on a firm foundation of generalized linear models and established statistical practices. The problem is valid, and a rigorous solution can be constructed by following the outlined steps.\n\nThe analysis proceeds from first principles, as follows.\n\n**1. Statistical Foundation: The Negative Binomial Model**\nThe raw data from pooled CRISPR screens are counts, $k_{gj}$, representing the number of sequencing reads for guide RNA $g$ in sample $j$. Such data are non-negative integers and invariably exhibit overdispersion, meaning the variance is greater than the mean. The Poisson distribution, for which variance equals the mean, is therefore inadequate. We employ the Negative Binomial (NB) distribution, which adds a dispersion parameter $\\alpha$ to model this extra variance. The variance-mean relationship is given by:\n$$V(\\mu_{g}) = \\mu_{g} + \\alpha \\mu_{g}^{2}$$\nwhere $\\mu_{g}$ is the mean abundance for guide $g$ and $\\alpha \\ge 0$ is the dispersion parameter. For $\\alpha=0$, the NB distribution reduces to the Poisson.\n\n**2. Normalization for Library Size**\nDifferent samples are sequenced to different depths, leading to systematic variations in total read counts (library sizes). To make counts comparable across samples, we must normalize. The problem specifies the median-of-ratios method, which is robust to a large proportion of differentially abundant genes.\n\nFirst, for a subset of genes that have non-zero counts $k_{gj}  0$ across all $m$ samples, we compute a pseudo-reference sample by taking the geometric mean of counts for each gene $g$:\n$$G_{g} = \\left( \\prod_{j=1}^{m} k_{gj} \\right)^{1/m} = \\exp\\left( \\frac{1}{m} \\sum_{j=1}^{m} \\log k_{gj} \\right)$$\nSecond, for each sample $j$, we calculate the ratio of its count $k_{gj}$ to the pseudo-reference $G_{g}$ for all genes in the reference subset. The size factor $s_{j}$ for sample $j$ is the median of these ratios:\n$$s_{j} = \\underset{g}{\\text{median}} \\left\\{ \\frac{k_{gj}}{G_{g}} \\right\\}$$\nFinally, these size factors are centered to have a geometric mean of $1$, ensuring that the normalization does not change the total scale of the count data. This is achieved by dividing each $s_j$ by the geometric mean of all size factors.\n\n**3. Common Dispersion Estimation**\nThe dispersion parameter $\\alpha$ must be estimated from the data. While a per-gene dispersion estimate is possible, it is unstable for low replicate numbers. A common practice is to estimate a single, global dispersion value $\\hat{\\alpha}$ shared across all genes. This is a robust approach that pools information. The method-of-moments is used.\n\nFor each gene $g$, normalized counts are computed as $y_{gj} = k_{gj} / s_{j}$. We calculate the mean $\\bar{y}_{g}$ and sample variance $S^{2}_{g}$ of these normalized counts across all samples. From the NB variance function, we have $S_g^2 \\approx \\bar{y}_g + \\alpha_g \\bar{y}_g^2$. Rearranging gives a per-gene dispersion estimate:\n$$\\hat{\\alpha}_{g} = \\frac{S^{2}_{g} - \\bar{y}_{g}}{\\bar{y}_{g}^{2}}$$\nTo ensure non-negativity, we take $\\max(0, \\hat{\\alpha}_{g})$. The final common dispersion $\\hat{\\alpha}$ is the median of these per-gene estimates, calculated only over genes where $\\bar{y}_{g}  0$. The median provides robustness against outlier genes with extreme variance.\n\n**4. Generalized Linear Model (GLM) Fitting**\nTo model the effect of the experimental condition on gene abundance, we fit a Negative Binomial GLM to the counts for each gene. The model relates the expected count $\\mu_{gj}$ to the experimental variables via a log link function:\n$$\\log(\\mu_{gj}) = \\log(s_{j}) + \\beta_{g0} + \\beta_{g1} x_{j}$$\nHere, $\\log(s_j)$ is an offset term to account for the library size normalization. $\\mathbf{X}$ is the design matrix with a column of ones for the intercept and a column for the binary condition indicator $x_j \\in \\{0, 1\\}$. The coefficients $\\beta_{g0}$ and $\\beta_{g1}$ represent the log-baseline abundance and the log-fold change between conditions, respectively.\n\nWe estimate the coefficients $\\beta_g = [\\beta_{g0}, \\beta_{g1}]^T$ using Iteratively Reweighted Least Squares (IRLS), which is equivalent to Fisher scoring for this GLM. The iterative update is:\n$$\\beta_g^{(t+1)} = (\\mathbf{X}^{\\top} \\mathbf{W}^{(t)} \\mathbf{X})^{-1} \\mathbf{X}^{\\top} \\mathbf{W}^{(t)} \\mathbf{z}^{(t)}$$\nwhere, at iteration $t$:\n- $\\eta_{gj} = \\log(s_j) + \\beta_{g0}^{(t)} + \\beta_{g1}^{(t)} x_{j}$ is the linear predictor.\n- $\\mu_{gj} = \\exp(\\eta_{gj})$ is the estimated mean.\n- $w_{gj} = \\frac{\\mu_{gj}}{1 + \\hat{\\alpha}\\mu_{gj}}$ are the diagonal elements of the weight matrix $\\mathbf{W}$.\n- $z_{gj} = \\eta_{gj} + \\frac{k_{gj} - \\mu_{gj}}{\\mu_{gj}}$ is the working response.\nThe iteration proceeds until the change in $\\beta_g$ is below a tolerance threshold or a maximum number of iterations is reached.\n\n**5. Hypothesis Testing and Multiple Test Correction**\nOur primary interest is whether the condition has a significant effect, i.e., if $\\beta_{g1} \\neq 0$. We use a Wald test for this purpose. The test statistic is:\n$$Z_{g} = \\frac{\\hat{\\beta}_{g1}}{\\operatorname{se}(\\hat{\\beta}_{g1})}$$\nwhere $\\hat{\\beta}_{g1}$ is the coefficient estimate from the converged IRLS, and its standard error $\\operatorname{se}(\\hat{\\beta}_{g1})$ is derived from the asymptotic covariance matrix of the estimator, given by $\\widehat{\\text{Cov}}(\\hat{\\beta}_g) = (\\mathbf{X}^{\\top} \\mathbf{W}_{\\text{final}} \\mathbf{X})^{-1}$. Specifically, $\\operatorname{se}(\\hat{\\beta}_{g1})$ is the square root of the diagonal element of this matrix corresponding to $\\beta_{g1}$. Under the null hypothesis $H_0: \\beta_{g1} = 0$, the statistic $Z_g$ follows a standard normal distribution. This allows for the calculation of a two-sided $p$-value.\n\nSince we perform this test for thousands of genes simultaneously, we face a multiple testing problem. To control the proportion of false discoveries, we adjust the $p$-values using the Benjamini-Hochberg (BH) procedure, which yields $q$-values (FDR-adjusted $p$-values).\n\n**6. Identification of Condition-Specific Essential Genes**\nA gene is identified as \"condition-specific essential in condition $1$ relative to condition $0$\" if it meets two criteria:\n1.  **Statistical Significance**: The adjusted $q$-value must be below a specified threshold, $q_g  \\alpha_{\\mathrm{FDR}}$ (e.g., $0.1$).\n2.  **Effect Size**: The gene must be depleted in condition $1$. We quantify this using the log-base-$2$ fold change, $\\widehat{\\mathrm{LFC}}_{g} = \\hat{\\beta}_{g1} / \\log 2$. The criterion is $\\widehat{\\mathrm{LFC}}_{g} \\le -\\tau$, where $\\tau$ is a non-negative magnitude threshold (e.g., $1.0$, corresponding to at least a halving of abundance).\n\nGenes with zero counts across all samples or for which a model cannot be reliably fit are excluded from the analysis. The final output is the sorted list of indices for genes that satisfy both criteria.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the differential analysis pipeline on all test cases.\n    \"\"\"\n\n    class DifferentialAnalysis:\n        \"\"\"\n        Implements the complete differential analysis pipeline for CRISPR screen count data.\n        \"\"\"\n        \n        def __init__(self, counts, conditions, alpha_fdr, tau):\n            self.raw_counts = np.asarray(counts, dtype=float)\n            self.conditions = np.asarray(conditions, dtype=float)\n            self.alpha_fdr = alpha_fdr\n            self.tau_lfc = tau\n            self.num_genes, self.num_samples = self.raw_counts.shape\n            self.log2 = np.log(2)\n            self.irls_iterations = 25\n            self.irls_tol = 1e-6\n\n        def run(self):\n            \"\"\"\n            Executes the full analysis pipeline.\n            \"\"\"\n            # Step 0: Filter genes that are untestable from the start\n            genes_with_counts = self.raw_counts.sum(axis=1) > 0\n            if not np.any(genes_with_counts):\n                return []\n\n            # Step 1: Size-factor normalization\n            size_factors = self._calculate_size_factors()\n            if size_factors is None: # Happens if no genes for geo mean calc\n                return []\n            \n            normalized_counts = self.raw_counts / size_factors\n            mean_normalized_counts = np.mean(normalized_counts, axis=1)\n            \n            # Further filter genes where mean normalized count is zero\n            testable_genes_mask = genes_with_counts  (mean_normalized_counts > 0)\n            testable_gene_indices = np.where(testable_genes_mask)[0]\n            if len(testable_gene_indices) == 0:\n                return []\n\n            # Step 2: Common dispersion estimation\n            common_dispersion = self._estimate_common_dispersion(size_factors, testable_genes_mask)\n\n            # Step 3  4: Per-gene GLM fit and Wald test\n            design_matrix = np.vstack([np.ones(self.num_samples), self.conditions]).T\n            \n            p_values = []\n            betas = []\n            final_testable_indices = []\n\n            for i in testable_gene_indices:\n                counts_g = self.raw_counts[i, :]\n                try:\n                    beta, cov_matrix = self._fit_nb_glm(counts_g, design_matrix, size_factors, common_dispersion)\n                    \n                    beta1 = beta[1]\n                    se_beta1 = np.sqrt(cov_matrix[1, 1])\n                    \n                    if np.isinf(se_beta1) or se_beta1  1e-8:\n                        p_val = 1.0\n                    else:\n                        wald_stat = beta1 / se_beta1\n                        p_val = 2 * (1 - norm.cdf(np.abs(wald_stat)))\n                    \n                    p_values.append(p_val)\n                    betas.append(beta1)\n                    final_testable_indices.append(i)\n                except (np.linalg.LinAlgError, ValueError):\n                    continue\n\n            # Step 5: Benjamini-Hochberg FDR correction\n            if not p_values:\n                return []\n            q_values = self._benjamini_hochberg(p_values)\n\n            # Step 6: Identify condition-specific essential genes\n            significant_genes = []\n            for gene_idx, beta1, q_val in zip(final_testable_indices, betas, q_values):\n                lfc = beta1 / self.log2\n                if q_val  self.alpha_fdr and lfc = -self.tau_lfc:\n                    significant_genes.append(gene_idx)\n            \n            return sorted(significant_genes)\n\n        def _calculate_size_factors(self):\n            genes_for_geo_mean_mask = np.all(self.raw_counts > 0, axis=1)\n            \n            if not np.any(genes_for_geo_mean_mask):\n                return np.ones(self.num_samples) # Fallback if no gene is viable\n\n            counts_subset = self.raw_counts[genes_for_geo_mean_mask, :]\n            geo_means = np.exp(np.mean(np.log(counts_subset), axis=1))\n            \n            ratios = counts_subset / geo_means[:, np.newaxis]\n            size_factors_uncentered = np.median(ratios, axis=0)\n            \n            if np.any(size_factors_uncentered = 0): # Avoid log(0) or log(-)\n                return size_factors_uncentered / np.mean(size_factors_uncentered)\n\n            geo_mean_sf = np.exp(np.mean(np.log(size_factors_uncentered)))\n            size_factors = size_factors_uncentered / geo_mean_sf\n            return size_factors\n\n        def _estimate_common_dispersion(self, size_factors, valid_genes_mask):\n            counts_subset = self.raw_counts[valid_genes_mask, :]\n            if counts_subset.shape[0] == 0:\n                return 0.0\n\n            normalized_counts = counts_subset / size_factors\n            mean_y = np.mean(normalized_counts, axis=1)\n            var_y = np.var(normalized_counts, axis=1, ddof=1)\n            \n            mean_y_sq = mean_y**2\n            # Add a small epsilon to avoid division by zero\n            mean_y_sq[mean_y_sq == 0] = 1e-8\n\n            dispersions_g = (var_y - mean_y) / mean_y_sq\n            dispersions_g[dispersions_g  0] = 0\n            \n            return np.median(dispersions_g)\n\n        def _fit_nb_glm(self, counts_g, X, s, alpha):\n            mean_norm_count = np.mean(counts_g / s)\n            if mean_norm_count = 0:\n                raise ValueError(\"Mean normalized count is zero or negative.\")\n            \n            beta = np.array([np.log(mean_norm_count), 0.0])\n\n            for _ in range(self.irls_iterations):\n                beta_old = beta.copy()\n                \n                eta = X @ beta + np.log(s)\n                mu = np.exp(eta)\n                \n                weights = mu / (1.0 + alpha * mu)\n                if np.sum(weights)  1e-8:\n                    raise np.linalg.LinAlgError(\"All weights are near zero.\")\n                \n                W = np.diag(weights)\n                z = eta + (counts_g - mu) / mu\n                \n                XT_W_X = X.T @ W @ X\n                XT_W_X_inv = np.linalg.inv(XT_W_X)\n                beta = XT_W_X_inv @ X.T @ W @ z\n                \n                if np.sum(np.abs(beta - beta_old)) / (np.sum(np.abs(beta_old)) + 1e-8)  self.irls_tol:\n                    break\n            \n            # Recalculate final covariance matrix with converged beta\n            eta = X @ beta + np.log(s)\n            mu = np.exp(eta)\n            weights = mu / (1.0 + alpha * mu)\n            W = np.diag(weights)\n            XT_W_X = X.T @ W @ X\n            cov_matrix = np.linalg.inv(XT_W_X)\n            \n            return beta, cov_matrix\n\n        def _benjamini_hochberg(self, p_values):\n            p_values = np.asarray(p_values)\n            num_tests = len(p_values)\n            \n            sorted_indices = np.argsort(p_values)\n            sorted_p_values = p_values[sorted_indices]\n            \n            ranks = np.arange(1, num_tests + 1)\n            q_values_sorted = sorted_p_values * num_tests / ranks\n            \n            q_values_sorted = np.minimum.accumulate(q_values_sorted[::-1])[::-1]\n            \n            q_values = np.empty_like(p_values)\n            q_values[sorted_indices] = q_values_sorted\n            return q_values\n\n    # Test suite from the problem statement\n    test_cases = [\n        # Case 1\n        (\n            [[100, 90, 110, 120, 80, 100], [50, 45, 55, 60, 40, 50], [30, 27, 33, 9, 6, 8],\n             [20, 18, 22, 24, 16, 20], [10, 9, 11, 3, 2, 3], [60, 54, 66, 72, 48, 60]],\n            [0, 0, 0, 1, 1, 1]\n        ),\n        # Case 2\n        (\n            [[160, 40, 120, 56], [80, 20, 12, 6], [40, 10, 30, 14],\n             [30, 7, 23, 10], [10, 3, 8, 4]],\n            [0, 0, 1, 1]\n        ),\n        # Case 3\n        (\n            [[2, 1, 2, 1], [0, 0, 0, 0], [3, 2, 3, 2], [1, 0, 1, 0],\n             [4, 4, 4, 4], [6, 5, 1, 1]],\n            [0, 0, 1, 1]\n        )\n    ]\n    \n    alpha_fdr = 0.1\n    tau = 1.0\n    all_results = []\n\n    for counts, conditions in test_cases:\n        analyzer = DifferentialAnalysis(counts, conditions, alpha_fdr, tau)\n        result = analyzer.run()\n        all_results.append(result)\n\n    # Format the final output exactly as required, handling spaces in list string representation.\n    # The default str() includes spaces, which is consistent with the problem's example format: [[i_1, i_2], [j_1], [k_1]]\n    output_str = ','.join(map(str, all_results))\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After identifying potential \"hits\" from a screen, a critical next step is to ensure they are not artifacts. A primary concern is systematic off-target effects, where a guide RNA affects genes other than its intended target due to sequence similarity. This exercise tackles this quality control challenge by combining sequence analysis with statistical enrichment testing, a foundational skill in bioinformatics. You will implement a procedure to find sequence motifs that are significantly over-represented among guides with unexpectedly similar phenotypes, providing a powerful method for flagging potential off-target-driven results .",
            "id": "2372025",
            "problem": "You are given a toy dataset representing a Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-based perturbation screen, consisting of guide sequences and an associated real-valued phenotype for each guide. The goal is to define and implement a decision procedure that detects whether there is a shared sequence motif among guides that exhibit unexpectedly similar phenotypes. The procedure must be defined purely in mathematical terms as follows.\n\nLet there be $N$ guides indexed by $i \\in \\{1,\\dots,N\\}$. Each guide has:\n- A nucleotide sequence $s_i$ over the alphabet $\\{ \\text{A}, \\text{C}, \\text{G}, \\text{T} \\}$.\n- A real-valued phenotype $y_i \\in \\mathbb{R}$.\n\nGiven a similarity threshold $\\tau  0$ and a neighbor degree threshold $t \\in \\mathbb{Z}_{\\ge 0}$, define the set of unexpectedly similar guides\n$$\nS \\;=\\; \\left\\{ i \\in \\{1,\\dots,N\\} \\;:\\; \\left|\\left\\{ j \\in \\{1,\\dots,N\\}\\setminus\\{i\\} \\;:\\; |y_i - y_j| \\le \\tau \\right\\}\\right| \\;\\ge\\; t \\right\\}.\n$$\nLet $n_S = |S|$. For a motif length $k \\in \\mathbb{Z}_{\\ge 1}$, a $k$-mer motif $m$ is any string of length $k$ over $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$. A guide $i$ is said to contain motif $m$ if $m$ appears as a contiguous substring in $s_i$ (occurrences may overlap). For a motif $m$, define the indicator\n$$\nI_i(m) \\;=\\; \\begin{cases}\n1  \\text{if $m$ is a substring of $s_i$,}\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\nDefine\n$$\nK(m) \\;=\\; \\sum_{i=1}^{N} I_i(m), \\qquad x(m) \\;=\\; \\sum_{i \\in S} I_i(m).\n$$\nInterpret $x(m)$ as the number of guides in $S$ that contain $m$, and $K(m)$ as the number of guides overall that contain $m$. Consider the null model that the $n_S$ guides in $S$ are drawn uniformly without replacement from the $N$ guides. Under this null, $X \\sim \\text{Hypergeometric}(N, K(m), n_S)$ is the number of motif-containing guides in a random subset of size $n_S$. Define the one-sided enrichment $p$-value as\n$$\np(m) \\;=\\; \\mathbb{P}\\left[ X \\ge x(m) \\right] \\;=\\; \\sum_{r = x(m)}^{\\min\\{K(m),\\, n_S\\}} \\frac{\\binom{K(m)}{r} \\binom{N - K(m)}{n_S - r}}{\\binom{N}{n_S}}.\n$$\nLet $M$ be a nonempty finite set of allowed motif lengths. Let $\\mathcal{C}$ be the set of candidate motifs defined by\n$$\n\\mathcal{C} \\;=\\; \\bigcup_{k \\in M} \\left\\{ m \\in \\{\\text{A},\\text{C},\\text{G},\\text{T}\\}^k \\;:\\; K(m) \\ge 1 \\right\\}.\n$$\nSelect a single “best” motif\n$$\nm^\\star \\;\\in\\; \\operatorname*{arg\\,min}_{m \\in \\mathcal{C}} \\; p(m),\n$$\nwith the following deterministic tie-breaking rule: if multiple motifs achieve the same minimal $p(m)$ (up to standard real-number comparison), choose the lexicographically smallest motif with respect to the ordering $\\text{A}  \\text{C}  \\text{G}  \\text{T}$ and standard string lexicographic comparison.\n\nMap any motif $m = m_1 m_2 \\dots m_k$ to a base-$4$ integer rank using the digit map $\\text{A}\\mapsto 0$, $\\text{C}\\mapsto 1$, $\\text{G}\\mapsto 2$, $\\text{T}\\mapsto 3$, as\n$$\n\\mathrm{rank}(m) \\;=\\; \\sum_{\\ell=1}^{k} d(m_\\ell)\\, 4^{k-\\ell},\n$$\nwhere $d(\\text{A})=0$, $d(\\text{C})=1$, $d(\\text{G})=2$, $d(\\text{T})=3$.\n\nGiven a significance level $\\alpha \\in (0,1)$, report, for each test case, the triple\n$$\n\\left[ \\;\\mathrm{rank}(m^\\star),\\; \\text{round}\\!\\left(p(m^\\star),\\, 6\\right),\\; \\mathbf{1}\\{ p(m^\\star)  \\alpha \\} \\; \\right],\n$$\nwhere $\\text{round}(\\cdot,6)$ denotes rounding to $6$ decimal places and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case result represented as the three-element list described above.\n\nTest suite specification. Use the following three test cases, each defined by the tuple $\\left( \\{s_i\\}_{i=1}^{N}, \\{y_i\\}_{i=1}^{N}, \\tau, t, M, \\alpha \\right)$:\n\n- Test case A (happy path):\n  - $N = 8$.\n  - Sequences $\\{s_i\\}$:\n    - $s_1 = \\text{ACGTAAACGT}$,\n    - $s_2 = \\text{TTTTCGTAAG}$,\n    - $s_3 = \\text{GGGGCGTCCC}$,\n    - $s_4 = \\text{AACGTTTTTT}$,\n    - $s_5 = \\text{AAAATTTTGG}$,\n    - $s_6 = \\text{CCCCCAAAAA}$,\n    - $s_7 = \\text{GACGTGAAAA}$,\n    - $s_8 = \\text{TAAAAAACGT}$.\n  - Phenotypes $\\{y_i\\}$ with no physical units: $[\\,1.02,\\;0.98,\\;1.05,\\;1.00,\\;2.00,\\;2.10,\\;0.97,\\;1.01\\,]$.\n  - Thresholds: $\\tau = 0.08$, $t = 3$.\n  - Allowed motif lengths: $M = \\{3,4\\}$.\n  - Significance level: $\\alpha = 0.05$.\n\n- Test case B (tie-breaking):\n  - $N = 4$.\n  - Sequences $\\{s_i\\}$:\n    - $s_1 = \\text{AAACCCGGGT}$,\n    - $s_2 = \\text{AAACCCGGGA}$,\n    - $s_3 = \\text{TTTGGGCCCA}$,\n    - $s_4 = \\text{TTTGGGCCCT}$.\n  - Phenotypes $\\{y_i\\}$ with no physical units: $[\\,0.50,\\;0.49,\\;1.50,\\;1.60\\,]$.\n  - Thresholds: $\\tau = 0.02$, $t = 1$.\n  - Allowed motif lengths: $M = \\{3\\}$.\n  - Significance level: $\\alpha = 0.05$.\n\n- Test case C (edge case with empty $S$):\n  - $N = 4$.\n  - Sequences $\\{s_i\\}$:\n    - $s_1 = \\text{AGTCAGTCAG}$,\n    - $s_2 = \\text{CAGTCAGTCA}$,\n    - $s_3 = \\text{GTCAGTCAGT}$,\n    - $s_4 = \\text{TCAGTCAGTC}$.\n  - Phenotypes $\\{y_i\\}$ with no physical units: $[\\,0.00,\\;1.00,\\;2.00,\\;3.00\\,]$.\n  - Thresholds: $\\tau = 0.05$, $t = 1$.\n  - Allowed motif lengths: $M = \\{4\\}$.\n  - Significance level: $\\alpha = 0.05$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is the list $[\\,\\mathrm{rank}(m^\\star),\\; \\text{round}(p(m^\\star),6),\\; \\mathbf{1}\\{ p(m^\\star)  \\alpha \\}\\,]$ for the corresponding test case, in the order A, B, C.",
            "solution": "The problem requires the implementation of a deterministic procedure to identify a sequence motif that is statistically enriched in a specific subset of guide RNAs from a CRISPR perturbation screen. The problem is computationally well-defined, scientifically grounded in the principles of bioinformatics and statistics, and all terms and procedures are specified with mathematical precision. My validation confirms that the problem is valid and admits a unique solution for each test case. I will now detail the solution process.\n\nThe overall procedure can be broken down into five main steps for each test case:\n1.  Identify the set of \"unexpectedly similar guides,\" denoted by $S$.\n2.  Generate the set of all candidate motifs, $\\mathcal{C}$, based on the provided sequences and allowed motif lengths $M$.\n3.  For each candidate motif $m \\in \\mathcal{C}$, calculate its enrichment $p$-value, $p(m)$, using the hypergeometric distribution.\n4.  Determine the \"best\" motif, $m^\\star$, by finding the one that minimizes $p(m)$, applying a specified lexicographical tie-breaking rule.\n5.  Compute the final output triple: $[\\mathrm{rank}(m^\\star), \\mathrm{round}(p(m^\\star), 6), \\mathbf{1}\\{p(m^\\star)  \\alpha\\}]$.\n\nLet us dissect each step with the required mathematical rigor.\n\nStep 1: Determination of the set $S$\nThe set $S$ consists of all guides $i$ that have at least $t$ \"neighbors,\" where a neighbor $j$ is another guide whose phenotype $y_j$ is \"close\" to $y_i$. Formally, for each guide $i \\in \\{1, \\dots, N\\}$, we compute the size of its neighborhood, $c_i = |\\left\\{ j \\in \\{1,\\dots,N\\}\\setminus\\{i\\} \\;:\\; |y_i - y_j| \\le \\tau \\right\\}\\right|$. The set $S$ is then defined as $S = \\{i : c_i \\ge t\\}$. The size of this set is $n_S = |S|$. This is a direct implementation of the definition provided.\n\nStep 2: Generation of Candidate Motifs $\\mathcal{C}$\nThe set of candidate motifs $\\mathcal{C}$ is the collection of all unique substrings of lengths specified in the set $M$ that appear in at least one of the guide sequences $\\{s_i\\}_{i=1}^N$. To construct this set, we iterate through each allowed length $k \\in M$. For each $k$, we iterate through all guide sequences $s_i$. We then extract all substrings of length $k$ from each $s_i$ and add them to a master set data structure to ensure uniqueness. The resulting set $\\mathcal{C}$ is then sorted lexicographically. This sorting is crucial for a correct and efficient implementation of the tie-breaking rule prescribed for selecting $m^\\star$.\n\nStep 3: Calculation of Enrichment $p$-value\nFor each motif $m \\in \\mathcal{C}$, we must first compute two quantities:\n-   $K(m)$: The total number of guides in the dataset that contain the motif $m$ as a substring. This is computed by iterating through all $N$ sequences and checking for the presence of $m$.\n-   $x(m)$: The number of guides within the set $S$ that contain the motif $m$. This is computed by iterating through the guides indexed by $S$ and checking for the presence of $m$.\n\nThe statistical significance of the enrichment of motif $m$ in the set $S$ is quantified by a $p$-value derived from the hypergeometric distribution. The null hypothesis is that the $n_S$ guides in $S$ are a random sample drawn without replacement from the total population of $N$ guides. The random variable $X$ represents the number of guides containing motif $m$ in such a random sample. $X$ follows a hypergeometric distribution, $X \\sim \\text{Hypergeometric}(N, K(m), n_S)$, where the parameters are:\n-   Population size: $N$ (total number of guides)\n-   Number of successes in population: $K(m)$ (total guides with motif $m$)\n-   Sample size: $n_S$ (size of set $S$)\n\nThe one-sided $p$-value is the probability of observing at least $x(m)$ successes in the sample, given by $p(m) = \\mathbb{P}[X \\ge x(m)]$. This is calculated using the survival function (SF) of the hypergeometric distribution, which is typically defined as $\\text{sf}(q) = \\mathbb{P}[X  q]$. Therefore, we have $p(m) = \\mathbb{P}[X \\ge x(m)] = \\text{sf}(x(m)-1)$. Computationally, this is reliably handled by functions available in scientific libraries like `scipy.stats` in Python.\n\nAn important edge case, as seen in Test Case C, is when the set $S$ is empty, i.e., $n_S=0$. In this scenario, we are drawing a sample of size $0$. The number of observed successes, $x(m)$, must be $0$. The probability of observing at least $0$ successes in a sample of size $0$ is, by definition, $1$. Thus, for any motif $m$, if $n_S=0$, then $p(m)=1$.\n\nStep 4: Selection of the Best Motif $m^\\star$\nThe best motif $m^\\star$ is the one with the minimum $p$-value among all motifs in $\\mathcal{C}$. The problem specifies a strict tie-breaking rule: if multiple motifs yield the same minimal $p$-value, the one that is lexicographically smallest is chosen. By iterating through the lexicographically sorted list of motifs from Step 2, the first motif encountered that achieves the minimum $p$-value is guaranteed to be the correct $m^\\star$. We maintain a variable for the minimum $p$-value found so far, $p_{min}$, and the corresponding motif, $m^\\star$. We update these whenever a motif with a $p$-value strictly less than the current $p_{min}$ is found.\n\nStep 5: Computation and Formatting of the Final Output\nOnce $m^\\star$ and its associated $p$-value, $p(m^\\star)$, are determined, the final result is constructed.\n-   The rank of $m^\\star = m_1 m_2 \\dots m_k$ is calculated as a base-$4$ integer. Using the provided mapping $d(\\text{A})=0, d(\\text{C})=1, d(\\text{G})=2, d(\\text{T})=3$, the rank is given by $\\mathrm{rank}(m^\\star) = \\sum_{\\ell=1}^{k} d(m_\\ell) 4^{k-\\ell}$. This can be implemented efficiently using Horner's method.\n-   The $p$-value $p(m^\\star)$ is rounded to $6$ decimal places.\n-   The significance indicator $\\mathbf{1}\\{ p(m^\\star)  \\alpha \\}$ is computed, which evaluates to $1$ if the condition is true and $0$ otherwise.\n\nThese three values form the final output list for the test case. The results from all test cases are then aggregated into a single list as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import hypergeom\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    test_cases = [\n        # Test case A (happy path)\n        (\n            [ # sequences {s_i}\n                \"ACGTAAACGT\", \"TTTTCGTAAG\", \"GGGGCGTCCC\", \"AACGTTTTTT\",\n                \"AAAATTTTGG\", \"CCCCCAAAAA\", \"GACGTGAAAA\", \"TAAAAAACGT\"\n            ],\n            [1.02, 0.98, 1.05, 1.00, 2.00, 2.10, 0.97, 1.01], # phenotypes {y_i}\n            0.08, # tau\n            3,    # t\n            {3, 4}, # M\n            0.05  # alpha\n        ),\n        # Test case B (tie-breaking)\n        (\n            [ # sequences {s_i}\n                \"AAACCCGGGT\", \"AAACCCGGGA\", \"TTTGGGCCCA\", \"TTTGGGCCCT\"\n            ],\n            [0.50, 0.49, 1.50, 1.60], # phenotypes {y_i}\n            0.02, # tau\n            1,    # t\n            {3},  # M\n            0.05  # alpha\n        ),\n        # Test case C (edge case with empty S)\n        (\n            [ # sequences {s_i}\n                \"AGTCAGTCAG\", \"CAGTCAGTCA\", \"GTCAGTCAGT\", \"TCAGTCAGTC\"\n            ],\n            [0.00, 1.00, 2.00, 3.00], # phenotypes {y_i}\n            0.05, # tau\n            1,    # t\n            {4},  # M\n            0.05  # alpha\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(*case)\n        # Format the result list into a string representation for the final output.\n        # Ensure floating point numbers are represented correctly.\n        formatted_result = f\"[{result[0]}, {result[1]:.6f}, {result[2]}]\" if isinstance(result[1], float) else f\"[{result[0]}, {result[1]}, {result[2]}]\"\n        # round() can return int if result is .0. str() prints 1.0 which is fine. The above is overkill.\n        # The problem states round(), so float output is adequate. Let's use simpler formatting.\n        results.append(str(result).replace(\" \", \"\"))\n\n\n    print(f\"[{','.join(results)}]\")\n\ndef rank_motif(motif: str) - int:\n    \"\"\"\n    Maps a motif to its base-4 integer rank.\n    \"\"\"\n    d_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    rank = 0\n    for char in motif:\n        rank = rank * 4 + d_map[char]\n    return rank\n\ndef process_case(sequences: list[str], phenotypes: list[float], tau: float, t: int, M: set[int], alpha: float):\n    \"\"\"\n    Processes a single test case according to the problem specification.\n    \"\"\"\n    N = len(sequences)\n\n    # Step 1: Determine the set S of unexpectedly similar guides\n    S = set()\n    for i in range(N):\n        neighbor_count = 0\n        for j in range(N):\n            if i == j:\n                continue\n            if abs(phenotypes[i] - phenotypes[j]) = tau:\n                neighbor_count += 1\n        if neighbor_count >= t:\n            S.add(i)\n    nS = len(S)\n\n    # Step 2: Generate the set of candidate motifs C\n    candidate_motifs = set()\n    for k in M:\n        if k = 0:\n            continue\n        for seq in sequences:\n            if len(seq) >= k:\n                for i in range(len(seq) - k + 1):\n                    candidate_motifs.add(seq[i:i+k])\n    \n    # Sort for deterministic tie-breaking\n    sorted_motifs = sorted(list(candidate_motifs))\n\n    if not sorted_motifs:\n        # This case suggests the problem setup is ill-posed (no motifs of specified lengths exist).\n        # A robust implementation might return an error or default.\n        return [None, None, None]\n\n    best_motif = \"\"\n    min_p_value = float('inf')\n\n    # Step 3  4: For each motif, calculate p-value and find the best motif\n    for m in sorted_motifs:\n        K_m = sum(1 for s in sequences if m in s)\n        x_m = sum(1 for i in S if m in sequences[i])\n\n        # Calculate the one-sided enrichment p-value using the hypergeometric survival function.\n        # P[X >= x_m] is computed as sf(x_m-1).\n        # M_pop -> N (total guides)\n        # n_success -> K_m (guides with motif)\n        # N_sample -> nS (guides in S)\n        p_val = hypergeom.sf(x_m - 1, N, K_m, nS)\n\n        if p_val  min_p_value:\n            min_p_value = p_val\n            best_motif = m\n            # Because the motifs are pre-sorted lexicographically, the first time we\n            # find a minimum p-value, we are guaranteed to satisfy the tie-breaking rule.\n\n    # Step 5: Calculate the final result triple\n    m_star_rank = rank_motif(best_motif)\n    p_m_star_rounded = round(min_p_value, 6)\n    is_significant = 1 if min_p_value  alpha else 0\n\n    return [m_star_rank, p_m_star_rounded, is_significant]\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "A robust analysis method must perform reliably not only in simple cases but also under complex biological scenarios. This final practice introduces the power of computational simulation for validating and comparing algorithms. You will create a virtual CRISPR screen with a known ground truth, specifically modeling a challenging case where a single gene produces isoforms with opposing functions. By testing how different hit-calling strategies behave in this controlled environment, you will learn to critically evaluate the assumptions and limitations of bioinformatics tools, a key skill for any computational biologist .",
            "id": "2372047",
            "problem": "You are given the task of constructing a fully specified simulation for the analysis of Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-based pooled perturbation screens to evaluate how two different hit-calling algorithms behave when a single gene has two isoforms with opposing functional effects and is targeted by different guide ribonucleic acids (gRNAs). You must implement a program that follows the mathematical specification below, runs a specified set of test cases, and outputs a single-line list of integers encoding the comparative correctness of the two algorithms on each test case.\n\nConsider a pooled screen with $G$ genes and $K$ guides per gene. There is one special gene, denoted $g^\\star$, that has two isoforms with opposing functions. Each guide is sequenced before selection (time $0$) and after selection (time $1$). For each guide $i$ in gene $j$, the pre-selection count $X_{0,ji}$ and post-selection count $X_{1,ji}$ are independent draws from a Negative Binomial distribution chosen to have specified means and a common dispersion parameter. All random sampling must be performed deterministically by seeding the random number generator with the provided seed of each test case.\n\nDefine the following parameters, which are fixed across all test cases unless stated otherwise:\n\n- Total number of genes $G = 200$.\n- Guides per gene $K = 6$.\n- Special gene index $g^\\star = 0$ (zero-based indexing).\n- Baseline pre-selection mean count per guide $\\mu_0 = 500$.\n- Common Negative Binomial dispersion (size parameter) $r = 50$.\n- One-sided significance level for Algorithm M $\\alpha = 0.05$.\n- Bayes factor threshold for Algorithm B $T = 10$.\n- Alternative-effect mean magnitude parameter for Algorithm B $\\beta = 0.6$.\n- Natural logarithms must be used for all logarithmic operations.\n\nNegative Binomial parameterization and sampling: For a desired mean $\\mu$ and dispersion (size) $r$, let $p = \\frac{r}{r+\\mu}$. Then a draw $Y \\sim \\mathrm{NB}(r,p)$ has mean $\\mu$ and variance $\\mu + \\frac{\\mu^2}{r}$. Use this parameterization for all count draws.\n\nTrue guide-level effects: Let $\\ell_{ji}$ denote the true log fold change for guide $i$ in gene $j$.\n\n- For all genes $j \\ne g^\\star$ and all their guides, set $\\ell_{ji} = 0$.\n- For the special gene $g^\\star$, there are three categories of guides:\n  1. Isoform-A-targeting guides with true effect $+\\mu$ (enrichment).\n  2. Isoform-B-targeting guides with true effect $-\\mu$ (depletion).\n  3. Shared-exon guides with true effect $0$.\n  The fractions of the $K$ guides assigned to these three categories are $p_A$, $p_B$, and $p_S$, respectively, with $p_A + p_B + p_S = 1$. To convert fractions to integers $(K_A, K_B, K_S)$ summing to $K$, first compute $(\\lfloor p_A K \\rfloor, \\lfloor p_B K \\rfloor, \\lfloor p_S K \\rfloor)$ and the remainders $(\\{p_A K\\}, \\{p_B K\\}, \\{p_S K\\})$. Let $R = K - (\\lfloor p_A K \\rfloor + \\lfloor p_B K \\rfloor + \\lfloor p_S K \\rfloor)$. Distribute the remaining $R$ guides by giving $+1$ to the categories in decreasing order of their remainders; break any ties by the fixed order A, then B, then S. Assign the first $K_A$ guides to $+\\mu$, the next $K_B$ guides to $-\\mu$, and the remaining $K_S$ guides to $0$.\n\nCount generation and observed effects: For each guide $i$ in gene $j$, draw\n- $X_{0,ji} \\sim \\mathrm{NB}(r, \\frac{r}{r+\\mu_0})$,\n- $X_{1,ji} \\sim \\mathrm{NB}(r, \\frac{r}{r+\\mu_0 \\exp(\\ell_{ji})})$.\nDefine the observed guide-level log fold change\n$$\nL_{ji} = \\log\\left(\\frac{X_{1,ji} + 1}{X_{0,ji} + 1}\\right).\n$$\n\nDefine a robust scale estimate $\\hat{\\sigma}$ from all guide-level $L_{ji}$ values by the median absolute deviation: Let $m = \\mathrm{median}(\\{L_{ji}\\})$ and $\\mathrm{MAD} = \\mathrm{median}(|L_{ji} - m|)$. Set $\\hat{\\sigma} = 1.4826 \\cdot \\mathrm{MAD}$. If $\\hat{\\sigma} = 0$, replace it by the sample standard deviation of $\\{L_{ji}\\}$; if that is also $0$, replace by a small constant $10^{-8}$.\n\nAlgorithm M (a direction-specific rank-minimum $p$-value with Bonferroni adjustment):\n1. For each guide $L_{ji}$, define the standardized value $Z_{ji} = \\frac{L_{ji}}{\\hat{\\sigma}}$.\n2. Let $\\Phi(\\cdot)$ denote the standard normal cumulative distribution function. Define one-sided $p$-values for depletion and enrichment as $p^-_{ji} = \\Phi(Z_{ji})$ and $p^+_{ji} = 1 - \\Phi(Z_{ji})$, respectively.\n3. For each gene $j$, define Bonferroni-adjusted gene-level values $P^-_j = \\min\\{1, K \\cdot \\min_i p^-_{ji}\\}$ and $P^+_j = \\min\\{1, K \\cdot \\min_i p^+_{ji}\\}$.\n4. Define $P_j = \\min(P^-_j, P^+_j)$ and the predicted direction $\\mathrm{dir}_M(j) = +1$ if $P^+_j  P^-_j$, otherwise $\\mathrm{dir}_M(j) = -1$. In case $P^+_j = P^-_j$, break the tie by setting $\\mathrm{dir}_M(j) = +1$.\n5. Algorithm M calls gene $j$ a hit if $P_j \\le \\alpha$; otherwise, it does not call a hit.\n\nAlgorithm B (a two-sided Bayes factor using a Gaussian likelihood ratio with fixed alternatives):\n1. Let the null distribution for $L_{ji}$ be $\\mathcal{N}(0, \\tau^2)$ with $\\tau = \\hat{\\sigma}$.\n2. Define two alternative distributions: depletion $\\mathcal{N}(-\\beta, \\tau^2)$ and enrichment $\\mathcal{N}(+\\beta, \\tau^2)$.\n3. For gene $j$, compute the log Bayes factors\n$$\n\\log \\mathrm{BF}^-(j) = \\sum_{i=1}^K \\left[ \\log \\phi(L_{ji}; -\\beta, \\tau) - \\log \\phi(L_{ji}; 0, \\tau) \\right],\n$$\n$$\n\\log \\mathrm{BF}^+(j) = \\sum_{i=1}^K \\left[ \\log \\phi(L_{ji}; +\\beta, \\tau) - \\log \\phi(L_{ji}; 0, \\tau) \\right],\n$$\nwhere $\\phi(x; \\mu, \\tau)$ is the Gaussian probability density function with mean $\\mu$ and standard deviation $\\tau$. Let $\\log \\mathrm{BF}_{\\max}(j) = \\max(\\log \\mathrm{BF}^-(j), \\log \\mathrm{BF}^+(j))$. The predicted direction is $\\mathrm{dir}_B(j) = +1$ if $\\log \\mathrm{BF}^+(j)  \\log \\mathrm{BF}^-(j)$, otherwise $\\mathrm{dir}_B(j) = -1$. In case of equality, break the tie by setting $\\mathrm{dir}_B(j) = +1$.\n4. Algorithm B calls gene $j$ a hit if $\\log \\mathrm{BF}_{\\max}(j) \\ge \\log T$; otherwise, it does not call a hit.\n\nGround-truth direction for the special gene $g^\\star$ is determined solely by the design fractions: define $L^\\star = +1$ if $p_A  p_B$, $L^\\star = -1$ if $p_B  p_A$, and $L^\\star = 0$ if $p_A = p_B$. An algorithm’s decision on $g^\\star$ is deemed correct as follows: if $L^\\star \\in \\{+1, -1\\}$, the algorithm is correct if and only if it calls a hit and its predicted direction equals $L^\\star$; if $L^\\star = 0$, the algorithm is correct if and only if it does not call a hit.\n\nFor each test case, compute two correctness indicators $C_M$ and $C_B$ (each either $0$ or $1$) for Algorithms M and B on the special gene $g^\\star$. Map them to a single integer result $R = 2 \\cdot C_M + C_B$, which takes values in $\\{0,1,2,3\\}$, corresponding to neither correct, only Algorithm B correct, only Algorithm M correct, or both correct, respectively.\n\nTest suite: For each case, you are given a tuple $(\\mathrm{seed}, \\mu, p_A, p_B, p_S)$ with $p_A + p_B + p_S = 1$. Use the following four test cases:\n1. $(42, 0.8, 0.8, 0.2, 0.0)$,\n2. $(43, 0.8, 0.25, 0.75, 0.0)$,\n3. $(44, 0.8, 0.5, 0.5, 0.0)$,\n4. $(45, 1.0, 0.1, 0.1, 0.8)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the integer $R$ for the corresponding test case, in the same order as listed above. No other text should be printed. No physical units or angles are involved, and any fractions must be represented as decimals in the input parameters. All computations must be performed as specified and be reproducible given the seeds.",
            "solution": "The problem requires the construction and execution of a detailed simulation for a Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)-based pooled perturbation screen. The objective is to assess the performance of two distinct hit-calling algorithms, designated Algorithm M and Algorithm B, in a specific scenario. This scenario involves a single gene, $g^\\star$, which possesses two isoforms with antagonistic biological effects (enrichment and depletion). The problem is determined to be valid as it is scientifically grounded in the principles of computational biology and bioinformatics, is mathematically and algorithmically well-posed, and provides a complete and unambiguous specification for the simulation.\n\nThe simulation process is structured as follows. We begin by defining the parameters of the screen and the statistical models for data generation. We then describe the implementation of the two analytical algorithms. Finally, we establish the criteria for evaluating the correctness of each algorithm's classification of the special gene $g^\\star$.\n\n**1. Simulation Framework**\n\nThe simulated screen comprises $G = 200$ genes, each targeted by $K = 6$ guide RNAs (gRNAs). A specific gene, with index $g^\\star = 0$, is modeled to have two isoforms with opposing effects. For each guide $i$ in gene $j$, we simulate read counts from both a pre-selection library ($T_0$) and a post-selection library ($T_1$). These counts, denoted $X_{0,ji}$ and $X_{1,ji}$, are generated from a Negative Binomial distribution, $\\mathrm{NB}(r, p)$, which is a standard choice for modeling overdispersed count data from high-throughput sequencing. The distribution is parameterized by its dispersion (size) parameter $r = 50$ and a probability parameter $p$. For a desired mean count $\\mu$, the probability is set to $p = \\frac{r}{r+\\mu}$. The baseline mean count for a guide in the pre-selection library is $\\mu_0 = 500$.\n\nThe true biological effect of a guide is represented by its log fold change, $\\ell_{ji}$. For all guides targeting non-special genes ($j \\ne g^\\star$), the effect is null, i.e., $\\ell_{ji} = 0$. For the special gene $g^\\star$, the guides are partitioned into three categories:\n- $K_A$ guides targeting Isoform-A, with a true effect of $+\\mu$ (enrichment).\n- $K_B$ guides targeting Isoform-B, with a true effect of $-\\mu$ (depletion).\n- $K_S$ guides targeting a shared region, with a true effect of $0$.\n\nThe counts $K_A$, $K_B$, and $K_S$ are determined from specified fractions $(p_A, p_B, p_S)$ such that $p_A + p_B + p_S = 1$. The integer counts are calculated deterministically by first taking the floor of $p \\cdot K$ for each category, and then distributing the remaining $R = K - (\\lfloor p_A K \\rfloor + \\lfloor p_B K \\rfloor + \\lfloor p_S K \\rfloor)$ guides one by one to the categories with the largest fractional parts, breaking ties in the order A, B, S.\n\nWith the true effects $\\ell_{ji}$ defined, the mean of the post-selection counts is $\\mu_0 \\exp(\\ell_{ji})$. Thus, the count generation is as follows:\n$$\nX_{0,ji} \\sim \\mathrm{NB}\\left(r, \\frac{r}{r+\\mu_0}\\right)\n$$\n$$\nX_{1,ji} \\sim \\mathrm{NB}\\left(r, \\frac{r}{r+\\mu_0 \\exp(\\ell_{ji})}\\right)\n$$\nAll random draws are performed deterministically using a per-case seed for the random number generator. From these counts, the observed guide-level log fold change $L_{ji}$ is calculated, incorporating a pseudocount of $1$ to handle zero counts:\n$$\nL_{ji} = \\log\\left(\\frac{X_{1,ji} + 1}{X_{0,ji} + 1}\\right)\n$$\nwhere $\\log$ denotes the natural logarithm.\n\nA robust estimate of the standard deviation of the null log fold changes, $\\hat{\\sigma}$, is computed from the full set of observed $L_{ji}$ values using the median absolute deviation (MAD): $\\hat{\\sigma} = 1.4826 \\cdot \\mathrm{median}(|L_{ji} - \\mathrm{median}(\\{L\\cdot\\})|)$. Specified fallbacks are used if $\\hat{\\sigma}$ is zero.\n\n**2. Hit-Calling Algorithms**\n\nTwo algorithms are applied to the data to call hits.\n\n**Algorithm M (Modified Rank-Minimum P-value):**\nThis algorithm is based on identifying the single most extreme guide effect for each gene and applying a Bonferroni correction.\n1. For each guide, a standardized Z-score is computed: $Z_{ji} = L_{ji} / \\hat{\\sigma}$.\n2. One-sided p-values for depletion and enrichment are calculated using the standard normal cumulative distribution function $\\Phi(\\cdot)$:\n   $$ p^-_{ji} = \\Phi(Z_{ji}) \\quad \\text{and} \\quad p^+_{ji} = 1 - \\Phi(Z_{ji}) $$\n3. Gene-level p-values are obtained by taking the minimum guide-level p-value for each direction and applying a Bonferroni correction for the $K$ guides:\n   $$ P^-_j = \\min\\left(1, K \\cdot \\min_i p^-_{ji}\\right) \\quad \\text{and} \\quad P^+_j = \\min\\left(1, K \\cdot \\min_i p^+_{ji}\\right) $$\n4. The final gene p-value is $P_j = \\min(P^-_j, P^+_j)$. The predicted direction, $\\mathrm{dir}_M(j)$, is $+1$ if $P^+_j  P^-_j$ and $-1$ otherwise (with a tie-break rule of $+1$).\n5. A gene is called a hit if $P_j \\le \\alpha$, where the significance level is $\\alpha = 0.05$.\n\n**Algorithm B (Bayesian Factor-Based):**\nThis algorithm uses a Bayesian framework to compare a null hypothesis against two alternative hypotheses of fixed effect sizes.\n1. The observed log fold changes $L_{ji}$ for a non-hit gene are assumed to follow a null distribution $\\mathcal{N}(0, \\hat{\\sigma}^2)$.\n2. Two alternative distributions are defined: $\\mathcal{N}(-\\beta, \\hat{\\sigma}^2)$ for depletion and $\\mathcal{N}(+\\beta, \\hat{\\sigma}^2)$ for enrichment, with a fixed effect magnitude $\\beta = 0.6$.\n3. For each gene $j$, log Bayes factors are computed to compare the alternatives against the null. The log-likelihood ratio for a single guide $L_{ji}$ under the positive alternative versus the null simplifies to $\\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2}$. Summing over all guides for gene $j$:\n   $$ \\log \\mathrm{BF}^+(j) = \\sum_{i=1}^K \\left( \\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2} \\right) = \\frac{\\beta}{\\hat{\\sigma}^2} \\sum_{i=1}^K L_{ji} - \\frac{K\\beta^2}{2\\hat{\\sigma}^2} $$\n   Similarly, for the negative alternative:\n   $$ \\log \\mathrm{BF}^-(j) = \\sum_{i=1}^K \\left( -\\frac{L_{ji}\\beta}{\\hat{\\sigma}^2} - \\frac{\\beta^2}{2\\hat{\\sigma}^2} \\right) = -\\frac{\\beta}{\\hat{\\sigma}^2} \\sum_{i=1}^K L_{ji} - \\frac{K\\beta^2}{2\\hat{\\sigma}^2} $$\n4. The maximal log Bayes factor is $\\log \\mathrm{BF}_{\\max}(j) = \\max(\\log \\mathrm{BF}^-(j), \\log \\mathrm{BF}^+(j))$. The predicted direction, $\\mathrm{dir}_B(j)$, is $+1$ if $\\log \\mathrm{BF}^+(j)  \\log \\mathrm{BF}^-(j)$ and $-1$ otherwise (with a tie-break rule of $+1$).\n5. A gene is called a hit if the evidence is sufficiently strong, i.e., $\\log \\mathrm{BF}_{\\max}(j) \\ge \\log T$, where the Bayes factor threshold is $T = 10$.\n\n**3. Evaluation and Output**\n\nThe performance of each algorithm is assessed only on the special gene $g^\\star$. The ground-truth direction $L^\\star$ for this gene is defined by the input design: $L^\\star = +1$ if $p_A  p_B$, $L^\\star = -1$ if $p_B  p_A$, and $L^\\star = 0$ if $p_A = p_B$.\n- If $L^\\star \\in \\{+1, -1\\}$, an algorithm is correct if and only if it calls $g^\\star$ as a hit and its predicted direction matches $L^\\star$.\n- If $L^\\star = 0$, an algorithm is correct if and only if it does not call $g^\\star$ as a hit.\n\nFor each test case, we compute binary correctness indicators $C_M$ and $C_B$ for Algorithm M and B, respectively. These are combined into a single integer result $R = 2 \\cdot C_M + C_B$, which uniquely encodes the outcome for both algorithms. The program will execute the simulation for each provided test case and output the list of these integer results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import nbinom, norm\n\ndef solve():\n    \"\"\"\n    Runs the full CRISPR screen simulation and analysis for a suite of test cases.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    G = 200  # Total number of genes\n    K = 6  # Guides per gene\n    g_star_idx = 0  # Special gene index\n    mu_0 = 500.0  # Baseline pre-selection mean count\n    r = 50.0  # Common Negative Binomial dispersion (size)\n    alpha = 0.05  # Significance level for Algorithm M\n    T = 10.0  # Bayes factor threshold for Algorithm B\n    beta = 0.6  # Alternative effect size for Algorithm B\n\n    # --- Test Cases (seed, mu, p_A, p_B, p_S) ---\n    test_cases = [\n        (42, 0.8, 0.8, 0.2, 0.0),\n        (43, 0.8, 0.25, 0.75, 0.0),\n        (44, 0.8, 0.5, 0.5, 0.0),\n        (45, 1.0, 0.1, 0.1, 0.8),\n    ]\n\n    results = []\n\n    for seed, mu_effect, p_A, p_B, p_S in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # --- 1. Determine Guide Counts for Special Gene ---\n        fractions = {'A': p_A, 'B': p_B, 'S': p_S}\n        base_counts = {cat: int(frac * K) for cat, frac in fractions.items()}\n        remainders = {cat: frac * K - base_counts[cat] for cat, frac in fractions.items()}\n        \n        # Sort categories by remainder descending, with tie-break A, B, S\n        sorted_cats = sorted(fractions.keys(), key=lambda c: (-remainders[c], ['A', 'B', 'S'].index(c)))\n        \n        R = K - sum(base_counts.values())\n        final_counts = base_counts.copy()\n        for i in range(R):\n            final_counts[sorted_cats[i]] += 1\n        \n        K_A, K_B, K_S = final_counts['A'], final_counts['B'], final_counts['S']\n\n        # --- 2. Generate True Log Fold Changes (l_ji) ---\n        l_ji = np.zeros((G, K))\n        effects = np.concatenate([\n            np.full(K_A, mu_effect),\n            np.full(K_B, -mu_effect),\n            np.full(K_S, 0.0)\n        ])\n        l_ji[g_star_idx, :] = effects\n\n        # --- 3. Generate Counts and Observed LFCs ---\n        p0 = r / (r + mu_0)\n        X0 = nbinom.rvs(n=r, p=p0, size=(G, K), random_state=rng)\n        \n        mu1 = mu_0 * np.exp(l_ji)\n        p1 = r / (r + mu1)\n        X1 = nbinom.rvs(n=r, p=p1, size=(G, K), random_state=rng)\n\n        L_ji = np.log((X1 + 1) / (X0 + 1))\n\n        # --- 4. Estimate Robust Scale (sigma_hat) ---\n        median_lfc = np.median(L_ji)\n        mad = np.median(np.abs(L_ji - median_lfc))\n        sigma_hat = 1.4826 * mad\n        if sigma_hat == 0.0:\n            sigma_hat = np.std(L_ji)\n            if sigma_hat == 0.0:\n                sigma_hat = 1e-8\n\n        # --- 5. Run Algorithms on Special Gene g* ---\n        L_star = L_ji[g_star_idx, :]\n        \n        # --- Algorithm M ---\n        Z_star = L_star / sigma_hat\n        p_minus_guides = norm.cdf(Z_star)\n        p_plus_guides = 1.0 - p_minus_guides\n        \n        P_minus_star = min(1.0, K * np.min(p_minus_guides))\n        P_plus_star = min(1.0, K * np.min(p_plus_guides))\n        \n        P_star_M = min(P_minus_star, P_plus_star)\n        \n        # Tie-break: dir=+1 if P+ = P-\n        dir_M = 1 if P_plus_star = P_minus_star else -1\n        hit_M = P_star_M = alpha\n\n        # --- Algorithm B ---\n        sum_L_star = np.sum(L_star)\n        log_BF_plus = (beta / sigma_hat**2) * sum_L_star - (K * beta**2) / (2 * sigma_hat**2)\n        log_BF_minus = (-beta / sigma_hat**2) * sum_L_star - (K * beta**2) / (2 * sigma_hat**2)\n        \n        log_BF_max = max(log_BF_plus, log_BF_minus)\n\n        # Tie-break: dir=+1 if BF+ >= BF-\n        dir_B = 1 if log_BF_plus >= log_BF_minus else -1\n        hit_B = log_BF_max >= np.log(T)\n\n        # --- 6. Evaluate Correctness ---\n        if p_A > p_B:\n            L_star_truth = 1\n        elif p_B > p_A:\n            L_star_truth = -1\n        else:\n            L_star_truth = 0\n\n        C_M = 0\n        if L_star_truth in [1, -1]:\n            if hit_M and dir_M == L_star_truth:\n                C_M = 1\n        elif L_star_truth == 0:\n            if not hit_M:\n                C_M = 1\n\n        C_B = 0\n        if L_star_truth in [1, -1]:\n            if hit_B and dir_B == L_star_truth:\n                C_B = 1\n        elif L_star_truth == 0:\n            if not hit_B:\n                C_B = 1\n        \n        # --- 7. Compute Final Result ---\n        R = 2 * C_M + C_B\n        results.append(R)\n\n    # --- Print Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}