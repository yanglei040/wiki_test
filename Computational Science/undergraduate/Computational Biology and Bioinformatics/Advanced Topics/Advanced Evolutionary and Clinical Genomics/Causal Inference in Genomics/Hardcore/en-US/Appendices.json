{
    "hands_on_practices": [
        {
            "introduction": "Observational data in genomics can often be deceptive due to hidden variables that distort the true relationship between a genetic factor and a disease. This exercise provides a hands-on numerical demonstration of Simpson's paradox, a powerful illustration of such confounding. By working through this hypothetical case-control scenario, you will see how a genetic variant can appear protective when data is naively pooled, while actually being a risk factor within distinct population subgroups, reinforcing the critical need to identify and adjust for confounders like population structure .",
            "id": "2377412",
            "problem": "A human case-control study investigates the causal effect of carrying at least one copy of a bi-allelic allele $G$ at a locus $L$ on a binary disease outcome $D \\in \\{0,1\\}$, where $D=1$ denotes a case and $D=0$ denotes a control. An unobserved binary stratifier $S \\in \\{S_1,S_2\\}$ (representing hidden population structure) influences both allele frequencies and disease risk. Data were collected as follows, stratified by $S$ but without $S$ being recorded in the main analysis:\n\n- For stratum $S_1$: among cases there are $40$ carriers and $20$ non-carriers; among controls there are $90$ carriers and $60$ non-carriers.\n- For stratum $S_2$: among cases there are $10$ carriers and $90$ non-carriers; among controls there are $4$ carriers and $46$ non-carriers.\n\nA naive analyst, unaware of $S$, pools individuals across $S_1$ and $S_2$ and models the genetic factor as a binary exposure $X \\in \\{0,1\\}$, where $X=1$ if and only if the individual carries at least one copy of $G$. Ignoring $S$, compute the marginal odds ratio of disease for $X=1$ versus $X=0$ across all individuals combined. Report your answer as a real number rounded to four significant figures.",
            "solution": "The odds ratio compares the odds of disease among exposed versus unexposed. For a $2 \\times 2$ table with counts $a$ (cases with $X=1$), $b$ (cases with $X=0$), $c$ (controls with $X=1$), and $d$ (controls with $X=0$), the odds ratio is\n$$\n\\text{OR} \\;=\\; \\frac{a/b}{c/d} \\;=\\; \\frac{a d}{b c}.\n$$\nThe naive analysis ignores the stratifier $S$ and therefore pools counts across $S_1$ and $S_2$.\n\nFirst, aggregate the counts across strata:\n- Cases with $X=1$: $a \\,=\\, 40 + 10 \\,=\\, 50$.\n- Cases with $X=0$: $b \\,=\\, 20 + 90 \\,=\\, 110$.\n- Controls with $X=1$: $c \\,=\\, 90 + 4 \\,=\\, 94$.\n- Controls with $X=0$: $d \\,=\\, 60 + 46 \\,=\\, 106$.\n\nCompute the naive marginal odds ratio:\n$$\n\\text{OR}_{\\text{naive}} \\;=\\; \\frac{a d}{b c} \\;=\\; \\frac{50 \\times 106}{110 \\times 94} \\;=\\; \\frac{5300}{10340} \\;=\\; \\frac{265}{517}.\n$$\nAs a decimal, this is\n$$\n\\text{OR}_{\\text{naive}} \\;=\\; \\frac{265}{517} \\;\\approx\\; 0.512572533849\\ldots\n$$\nRounded to four significant figures,\n$$\n\\text{OR}_{\\text{naive}} \\;\\approx\\; 0.5126.\n$$\n\nTo verify that this setup demonstrates Simpson’s paradox, consider the within-stratum odds ratios (not required for the final numeric output but confirming the paradoxical reversal). For $S_1$,\n$$\n\\text{OR}_{S_1} \\;=\\; \\frac{(40/20)}{(90/60)} \\;=\\; \\frac{2}{1.5} \\;=\\; \\frac{4}{3} \\;>\\; 1,\n$$\nand for $S_2$,\n$$\n\\text{OR}_{S_2} \\;=\\; \\frac{(10/90)}{(4/46)} \\;=\\; \\frac{1/9}{2/23} \\;=\\; \\frac{23}{18} \\;>\\; 1.\n$$\nThus, within each stratum carriers have higher disease odds, but when strata are pooled the naive marginal odds ratio satisfies $\\text{OR}_{\\text{naive}} = 265/517 < 1$, revealing Simpson’s paradox due to confounding by the hidden stratifier $S$.",
            "answer": "$$\\boxed{0.5126}$$"
        },
        {
            "introduction": "Mendelian Randomization (MR) is a powerful method that uses genetic variants as instrumental variables to infer causality, effectively bypassing many issues of traditional confounding. However, MR relies on strong assumptions, and this practice explores the consequences of violating one: the absence of horizontal pleiotropy. In this coding exercise, you will model a scenario where the chosen instrumental SNP is in linkage disequilibrium (LD) with another variant that directly affects the outcome, and quantify the resulting bias in the causal estimate . This will build your intuition for how instrument validity is paramount for reliable MR results.",
            "id": "2377409",
            "problem": "Consider two biallelic single-nucleotide polymorphisms (SNPs), denoted $G$ and $Z$, each coded additively and standardized to have mean $0$ and variance $1$. Let their correlation be the signed linkage disequilibrium (LD) correlation $r \\in [-1,1]$, and let the LD measure be $r^2 \\in [0,1]$. Suppose there is an exposure $E$ and an outcome $Y$ generated according to linear structural equations:\n$E = \\gamma G + \\xi$ and $Y = \\beta E + \\delta Z + \\varepsilon$,\nwhere $\\gamma, \\beta, \\delta \\in \\mathbb{R}$ are fixed constants, and the noise terms $\\xi$ and $\\varepsilon$ are mean-zero and jointly independent of $(G,Z)$. Assume $Z$ has no causal effect on $E$, and $G$ has no direct effect on $Y$ other than through $E$. Consider a two-sample Mendelian randomization (MR) study, where the large-sample marginal association of $E$ on $G$ is estimated in one sample, and the large-sample marginal association of $Y$ on $G$ is estimated in an independent sample. The MR estimator is defined as the ratio of these two large-sample marginal associations.\n\nYou are given the signed LD correlation indirectly by $r^2$ and a sign parameter $s \\in \\{-1,+1\\}$, where $r = s \\sqrt{r^2}$. For each specified parameter tuple $(\\gamma, \\beta, \\delta, r^2, s)$, compute:\n1) the large-sample MR estimate of the causal effect of $E$ on $Y$ using $G$ as the instrument,\n2) the absolute bias of this estimate relative to the true causal effect $\\beta$,\n3) a boolean indicating whether the MR estimate has a sign opposite to the sign of $\\beta$.\n\nAll computations are dimensionless; no physical units apply. Your program must use the following test suite of parameter tuples $(\\gamma, \\beta, \\delta, r^2, s)$:\n- Case $1$: $(0.2, 0.5, 0.3, 0.25, +1)$.\n- Case $2$: $(0.2, 0.5, 0.3, 0.0, +1)$.\n- Case $3$: $(0.4, 0.5, 0.3, 0.36, -1)$.\n- Case $4$: $(0.05, 0.1, 0.2, 0.81, -1)$.\n- Case $5$: $(-0.3, 0.4, 0.2, 1.0, +1)$.\n\nFor each case, output a three-element list $[\\widehat{\\beta}_{\\mathrm{MR}}, \\mathrm{bias}, \\mathrm{flip}]$, where $\\widehat{\\beta}_{\\mathrm{MR}}$ is the large-sample MR estimate, $\\mathrm{bias}$ is the absolute bias $|\\widehat{\\beta}_{\\mathrm{MR}} - \\beta|$, and $\\mathrm{flip}$ is a boolean equal to $\\mathrm{True}$ if $\\widehat{\\beta}_{\\mathrm{MR}}$ and $\\beta$ have opposite signs and $\\mathrm{False}$ otherwise. Express all floating-point numbers rounded to $6$ decimal places. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a three-element list corresponding to a test case, for example $[[a,b, \\mathrm{False}],[c,d,\\mathrm{True}],\\dots]$ in the given order of cases.",
            "solution": "The problem statement submitted for analysis is deemed valid. It is scientifically grounded within the field of statistical genetics and causal inference, specifically Mendelian randomization (MR). The problem is well-posed, with all necessary parameters and assumptions for a unique solution provided. It is objective and free from ambiguity. We may therefore proceed with a formal solution.\n\nThe problem asks for the computation of a large-sample two-sample MR estimate, its bias, and a sign-change indicator, based on a defined linear structural model. We are given two standardized single-nucleotide polymorphisms (SNPs), $G$ and $Z$, with mean $0$ and variance $1$. Their covariance is thus equal to their correlation, $\\mathrm{Cov}(G, Z) = r = s \\sqrt{r^2}$. The system is described by the equations:\n$$ E = \\gamma G + \\xi $$\n$$ Y = \\beta E + \\delta Z + \\varepsilon $$\nwhere $G$ is the instrumental variable for the effect of the exposure $E$ on the outcome $Y$. The true causal effect of $E$ on $Y$ is $\\beta$. The SNP $Z$ is a source of horizontal pleiotropy, as it affects $Y$ directly ($\\delta \\neq 0$) and is in linkage disequilibrium (LD) with the instrument $G$ ($r \\neq 0$).\n\nThe MR estimator, $\\widehat{\\beta}_{\\mathrm{MR}}$, is defined as the ratio of the large-sample association of the outcome $Y$ on the instrument $G$ to the association of the exposure $E$ on $G$. In large samples, these estimated associations converge to their population analogs. For a standardized instrument $G$ with $\\mathrm{Var}(G) = 1$, the association coefficients are equal to the covariances.\n\nFirst, we derive the SNP-exposure association, which we denote $\\gamma_{G,E}$.\n$$ \\gamma_{G,E} = \\frac{\\mathrm{Cov}(E, G)}{\\mathrm{Var}(G)} = \\mathrm{Cov}(E, G) $$\nSubstituting the model for $E$:\n$$ \\gamma_{G,E} = \\mathrm{Cov}(\\gamma G + \\xi, G) = \\gamma \\mathrm{Cov}(G, G) + \\mathrm{Cov}(\\xi, G) $$\nGiven $\\mathrm{Var}(G) = 1$ and that the noise term $\\xi$ is independent of $G$ ($\\mathrm{Cov}(\\xi, G) = 0$), this simplifies to:\n$$ \\gamma_{G,E} = \\gamma \\cdot 1 + 0 = \\gamma $$\n\nSecond, we derive the SNP-outcome association, $\\Gamma_{G,Y}$.\n$$ \\Gamma_{G,Y} = \\frac{\\mathrm{Cov}(Y, G)}{\\mathrm{Var}(G)} = \\mathrm{Cov}(Y, G) $$\nWe substitute the model for $Y$, and then the model for $E$:\n$$ Y = \\beta E + \\delta Z + \\varepsilon = \\beta (\\gamma G + \\xi) + \\delta Z + \\varepsilon = \\beta\\gamma G + \\beta\\xi + \\delta Z + \\varepsilon $$\nNow, we compute the covariance with $G$:\n$$ \\Gamma_{G,Y} = \\mathrm{Cov}(\\beta\\gamma G + \\beta\\xi + \\delta Z + \\varepsilon, G) $$\nUsing the linearity of covariance and the independence of noise terms ($\\mathrm{Cov}(\\xi, G)=0, \\mathrm{Cov}(\\varepsilon, G)=0$):\n$$ \\Gamma_{G,Y} = \\beta\\gamma \\mathrm{Cov}(G, G) + \\beta \\mathrm{Cov}(\\xi, G) + \\delta \\mathrm{Cov}(Z, G) + \\mathrm{Cov}(\\varepsilon, G) = \\beta\\gamma(1) + \\beta(0) + \\delta r + 0 = \\beta\\gamma + \\delta r $$\n\nThe large-sample MR estimate $\\widehat{\\beta}_{\\mathrm{MR}}$ is the ratio of these two quantities:\n$$ \\widehat{\\beta}_{\\mathrm{MR}} = \\frac{\\Gamma_{G,Y}}{\\gamma_{G,E}} = \\frac{\\beta\\gamma + \\delta r}{\\gamma} = \\beta + \\delta \\frac{r}{\\gamma} $$\nUsing the given relation $r = s\\sqrt{r^2}$, the final expression for the estimate is:\n$$ \\widehat{\\beta}_{\\mathrm{MR}} = \\beta + \\delta \\frac{s\\sqrt{r^2}}{\\gamma} $$\nThis expression is valid provided $\\gamma \\neq 0$, which holds for all test cases.\n\nThe absolute bias is the absolute difference between the estimate and the true effect $\\beta$:\n$$ \\mathrm{bias} = |\\widehat{\\beta}_{\\mathrm{MR}} - \\beta| = \\left| \\left(\\beta + \\delta \\frac{r}{\\gamma}\\right) - \\beta \\right| = \\left| \\delta \\frac{r}{\\gamma} \\right| = \\frac{|\\delta| \\sqrt{r^2}}{|\\gamma|} $$\n\nThe sign flip indicator, $\\mathrm{flip}$, is a boolean that is true if $\\widehat{\\beta}_{\\mathrm{MR}}$ and $\\beta$ have opposite signs. This is equivalent to testing the condition $\\widehat{\\beta}_{\\mathrm{MR}} \\cdot \\beta < 0$, as none of the provided $\\beta$ values are zero.\n\nWe now apply these formulas to each test case.\n\n**Case 1:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.2, 0.5, 0.3, 0.25, +1)$\n$r = +1 \\cdot \\sqrt{0.25} = 0.5$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.5 + 0.3 \\cdot \\frac{0.5}{0.2} = 0.5 + 0.75 = 1.25$.\n$\\mathrm{bias} = |1.25 - 0.5| = 0.75$.\n$\\mathrm{flip}$: $1.25 \\cdot 0.5 > 0$, so $\\mathrm{False}$.\nResult: $[1.250000, 0.750000, \\mathrm{False}]$.\n\n**Case 2:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.2, 0.5, 0.3, 0.0, +1)$\n$r = +1 \\cdot \\sqrt{0.0} = 0.0$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.5 + 0.3 \\cdot \\frac{0.0}{0.2} = 0.5$.\n$\\mathrm{bias} = |0.5 - 0.5| = 0.0$.\n$\\mathrm{flip}$: $0.5 \\cdot 0.5 > 0$, so $\\mathrm{False}$.\nResult: $[0.500000, 0.000000, \\mathrm{False}]$. This is the unbiased case where LD is zero.\n\n**Case 3:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.4, 0.5, 0.3, 0.36, -1)$\n$r = -1 \\cdot \\sqrt{0.36} = -0.6$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.5 + 0.3 \\cdot \\frac{-0.6}{0.4} = 0.5 - 0.45 = 0.05$.\n$\\mathrm{bias} = |0.05 - 0.5| = |-0.45| = 0.45$.\n$\\mathrm{flip}$: $0.05 \\cdot 0.5 > 0$, so $\\mathrm{False}$.\nResult: $[0.050000, 0.450000, \\mathrm{False}]$.\n\n**Case 4:** $(\\gamma, \\beta, \\delta, r^2, s) = (0.05, 0.1, 0.2, 0.81, -1)$\n$r = -1 \\cdot \\sqrt{0.81} = -0.9$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.1 + 0.2 \\cdot \\frac{-0.9}{0.05} = 0.1 - 3.6 = -3.5$.\n$\\mathrm{bias} = |-3.5 - 0.1| = |-3.6| = 3.6$.\n$\\mathrm{flip}$: $-3.5 \\cdot 0.1 < 0$, so $\\mathrm{True}$.\nResult: $[-3.500000, 3.600000, \\mathrm{True}]$. Strong bias due to a weak instrument ($\\gamma=0.05$) leads to a sign flip.\n\n**Case 5:** $(\\gamma, \\beta, \\delta, r^2, s) = (-0.3, 0.4, 0.2, 1.0, +1)$\n$r = +1 \\cdot \\sqrt{1.0} = 1.0$.\n$\\widehat{\\beta}_{\\mathrm{MR}} = 0.4 + 0.2 \\cdot \\frac{1.0}{-0.3} = 0.4 - \\frac{2}{3} \\approx -0.266667$.\n$\\mathrm{bias} = |(-0.266667) - 0.4| = |-0.666667| \\approx 0.666667$.\n$\\mathrm{flip}$: $(-0.266667) \\cdot 0.4 < 0$, so $\\mathrm{True}$.\nResult: $[-0.266667, 0.666667, \\mathrm{True}]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Mendelian randomization estimate, bias, and sign flip\n    for a series of parameter sets.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (gamma, beta, delta, r_squared, s)\n    test_cases = [\n        (0.2, 0.5, 0.3, 0.25, 1),\n        (0.2, 0.5, 0.3, 0.0, 1),\n        (0.4, 0.5, 0.3, 0.36, -1),\n        (0.05, 0.1, 0.2, 0.81, -1),\n        (-0.3, 0.4, 0.2, 1.0, 1),\n    ]\n\n    results = []\n    for case in test_cases:\n        gamma, beta, delta, r_squared, s = case\n\n        # Calculate the signed LD correlation r\n        r = s * np.sqrt(r_squared)\n\n        # Calculate the large-sample MR estimate beta_mr\n        # The formula is derived as beta_mr = beta + delta * r / gamma\n        # This is valid as gamma is non-zero in all test cases.\n        beta_mr = beta + (delta * r) / gamma\n\n        # Calculate the absolute bias of the estimate\n        bias = abs(beta_mr - beta)\n\n        # Determine if the estimate's sign is opposite to the true effect's sign.\n        # This condition is equivalent to beta_mr * beta < 0, assuming beta is not 0.\n        # In all test cases, beta is positive, so this check simplifies to beta_mr < 0.\n        flip = (beta_mr * beta) < 0\n\n        results.append([beta_mr, bias, flip])\n\n    # Format the results into the required string format.\n    # Each result is a list [beta_mr, bias, flip].\n    # Floating point numbers are formatted to 6 decimal places.\n    # Booleans are converted to their string representation ('True' or 'False').\n    formatted_results = []\n    for res in results:\n        # Format: [float_val, float_val, Boolean]\n        # Example: [-3.500000, 3.600000, True]\n        formatted_res_str = f\"[{res[0]:.6f}, {res[1]:.6f}, {str(res[2])}]\"\n        formatted_results.append(formatted_res_str)\n\n    # Final print statement in the exact required format.\n    # Example: [[result1], [result2], ...]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many complex diseases are influenced by a web of interconnected risk factors, such as body mass index and education level, which are themselves correlated. To address this complexity, Multivariable Mendelian Randomization (MVMR) extends the MR framework to simultaneously estimate the direct causal effects of multiple exposures on an outcome. This advanced practice puts you in the role of an analyst working with summary-level genetic data, a common scenario in modern bioinformatics . By implementing the MVMR estimator, you will learn a key technique for disentangling the causal contributions of correlated exposures.",
            "id": "2377474",
            "problem": "You are given a summary-level formulation of a multivariable instrumental variables problem arising in Mendelian Randomization (MR). The goal is to estimate the vector of direct causal effects of two exposures, body mass index (BMI) and years of schooling, on an income outcome using Multivariable Mendelian Randomization (MVMR). Assume you have $L$ genetic variants used as instruments, indexed by $l \\in \\{1,\\dots,L\\}$. For each variant $l$, you are given:\n- The association estimates with the two exposures, collected row-wise into a matrix $A_{x} \\in \\mathbb{R}^{L \\times 2}$.\n- The association estimate with the outcome, collected into a vector $b_{y} \\in \\mathbb{R}^{L}$.\n- The standard errors of the outcome associations, collected into a diagonal matrix $S_{y} = \\mathrm{diag}(s_{1,y},\\dots,s_{L,y}) \\in \\mathbb{R}^{L \\times L}$.\n- A symmetric positive definite instrument correlation matrix $R \\in \\mathbb{R}^{L \\times L}$.\n\nAssume the asymptotic covariance of $b_{y}$ is $\\Sigma_{y} = S_{y} R S_{y}$. Define the MVMR estimator $\\hat{\\beta} \\in \\mathbb{R}^{2}$ as the minimizer of the weighted quadratic form\n$$\nQ(\\beta) = (b_{y} - A_{x} \\beta)^{\\top} \\Sigma_{y}^{-1} (b_{y} - A_{x} \\beta),\n$$\nwhere $\\beta = [\\beta_{\\mathrm{BMI}}, \\beta_{\\mathrm{Schooling}}]^{\\top}$ is the vector of direct causal effects. Compute $\\hat{\\beta}$ for each of the following test cases.\n\nAll arrays below are provided in row-major order, with each number given as a real value. For each case, use exactly the matrices and vectors as given, with $S_{y} = \\mathrm{diag}(s_{1,y},\\dots,s_{L,y})$ and $\\Sigma_{y} = S_{y} R S_{y}$.\n\nTest case 1 (independent instruments, well-conditioned):\n- $L = 5$.\n- $A_{x} = \\begin{bmatrix}\n0.08 & 0.02\\\\\n0.06 & 0.01\\\\\n0.07 & 0.03\\\\\n0.05 & 0.02\\\\\n0.09 & 0.01\n\\end{bmatrix}$.\n- $b_{y} = \\begin{bmatrix} 0.044\\\\ 0.028\\\\ 0.0445\\\\ 0.0295\\\\ 0.041 \\end{bmatrix}$.\n- $s_{y} = \\begin{bmatrix} 0.02\\\\ 0.02\\\\ 0.02\\\\ 0.02\\\\ 0.02 \\end{bmatrix}$, so $S_{y} = \\mathrm{diag}(0.02, 0.02, 0.02, 0.02, 0.02)$.\n- $R = I_{5}$, the $5 \\times 5$ identity matrix.\n\nTest case 2 (correlated instruments, heteroskedastic standard errors):\n- $L = 4$.\n- $A_{x} = \\begin{bmatrix}\n0.05 & 0.04\\\\\n0.045 & 0.035\\\\\n0.06 & 0.05\\\\\n0.055 & 0.045\n\\end{bmatrix}$.\n- $b_{y} = \\begin{bmatrix} 0.04\\\\ 0.0333\\\\ 0.0488\\\\ 0.0429 \\end{bmatrix}$.\n- $s_{y} = \\begin{bmatrix} 0.015\\\\ 0.02\\\\ 0.018\\\\ 0.022 \\end{bmatrix}$, so $S_{y} = \\mathrm{diag}(0.015, 0.02, 0.018, 0.022)$.\n- $R = \\begin{bmatrix}\n1 & 0.25 & 0.0625 & 0.015625\\\\\n0.25 & 1 & 0.25 & 0.0625\\\\\n0.0625 & 0.25 & 1 & 0.25\\\\\n0.015625 & 0.0625 & 0.25 & 1\n\\end{bmatrix}$.\n\nTest case 3 (nearly collinear exposures, weak instruments):\n- $L = 3$.\n- $A_{x} = \\begin{bmatrix}\n0.02 & 0.0204\\\\\n0.018 & 0.01836\\\\\n0.017 & 0.01751\n\\end{bmatrix}$.\n- $b_{y} = \\begin{bmatrix} -0.0002\\\\ 0.00032\\\\ -0.000555 \\end{bmatrix}$.\n- $s_{y} = \\begin{bmatrix} 0.02\\\\ 0.02\\\\ 0.02 \\end{bmatrix}$, so $S_{y} = \\mathrm{diag}(0.02, 0.02, 0.02)$.\n- $R = I_{3}$, the $3 \\times 3$ identity matrix.\n\nYour task: For each test case, compute the two-dimensional estimator $\\hat{\\beta} = [\\hat{\\beta}_{\\mathrm{BMI}}, \\hat{\\beta}_{\\mathrm{Schooling}}]^{\\top}$ that minimizes $Q(\\beta)$ as defined above. Report each component as a real number.\n\nFinal output format: Your program should produce a single line of output containing the results for all test cases as a single flat list in the order $[\\hat{\\beta}_{\\mathrm{BMI}}^{(1)}, \\hat{\\beta}_{\\mathrm{Schooling}}^{(1)}, \\hat{\\beta}_{\\mathrm{BMI}}^{(2)}, \\hat{\\beta}_{\\mathrm{Schooling}}^{(2)}, \\hat{\\beta}_{\\mathrm{BMI}}^{(3)}, \\hat{\\beta}_{\\mathrm{Schooling}}^{(3)}]$, where the superscript denotes the test case index. Each number must be rounded to six decimal places. The list must be enclosed in square brackets with comma separators, for example, $[0.123456,0.654321, \\dots]$.",
            "solution": "The problem statement is scrutinized and found to be valid. It is a well-posed problem in statistical estimation, grounded in the established principles of Multivariable Mendelian Randomization (MVMR). All data and definitions required for a unique solution are provided, and there are no scientific or logical contradictions. I will now proceed with the solution.\n\nThe objective is to find the vector of causal effects $\\hat{\\beta} \\in \\mathbb{R}^{2}$ that minimizes the quadratic form:\n$$\nQ(\\beta) = (b_{y} - A_{x} \\beta)^{\\top} \\Sigma_{y}^{-1} (b_{y} - A_{x} \\beta)\n$$\nwhere $b_{y} \\in \\mathbb{R}^{L}$ is the vector of instrument-outcome associations, $A_{x} \\in \\mathbb{R}^{L \\times 2}$ is the matrix of instrument-exposure associations, and $\\Sigma_{y}^{-1}$ is the inverse of the covariance matrix of $b_{y}$. This is a standard generalized least squares (GLS) problem.\n\nTo find the minimizer $\\hat{\\beta}$, we must compute the gradient of $Q(\\beta)$ with respect to $\\beta$ and set it to the zero vector. First, we expand the quadratic form:\n$$\nQ(\\beta) = b_{y}^{\\top}\\Sigma_{y}^{-1}b_{y} - b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta - \\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y} + \\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta\n$$\nThe terms $b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta$ and $\\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}$ are scalars. Since the transpose of a scalar is itself, and noting that $\\Sigma_{y}^{-1}$ is symmetric, we have $(\\beta^{\\top}A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y})^{\\top} = b_{y}^{\\top}(\\Sigma_{y}^{-1})^{\\top}(A_{x}^{\\top})^{\\top}\\beta = b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta$. Therefore, the two terms are equal. The expression simplifies to:\n$$\nQ(\\beta) = b_{y}^{\\top}\\Sigma_{y}^{-1}b_{y} - 2 b_{y}^{\\top}\\Sigma_{y}^{-1}A_{x}\\beta + \\beta^{\\top}(A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})\\beta\n$$\nThis is a quadratic function of $\\beta$. The gradient with respect to $\\beta$ is obtained by applying standard rules of vector calculus:\n$$\n\\frac{\\partial Q(\\beta)}{\\partial \\beta} = -2 A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y} + 2 (A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})\\beta\n$$\nSetting the gradient to the zero vector, $\\frac{\\partial Q(\\beta)}{\\partial \\beta} = 0$, to find the minimum yields the so-called normal equations for the GLS estimator:\n$$\n(A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})\\hat{\\beta} = A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}\n$$\nProvided that the matrix $A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x}$ is invertible, which requires that the matrix of instrument-exposure associations $A_x$ has full column rank, we can solve for $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = (A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x})^{-1} A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}\n$$\nThe covariance matrix $\\Sigma_{y}$ is given by $\\Sigma_{y} = S_{y} R S_{y}$, where $S_{y}$ is the diagonal matrix of standard errors and $R$ is the instrument correlation matrix. Both $S_y$ and $R$ are specified as invertible, so $\\Sigma_y$ is also invertible.\n\nThis formula provides the analytical solution. The implementation will compute this for each test case. For numerical stability, instead of explicitly computing the inverse of the $2 \\times 2$ matrix $C = A_{x}^{\\top}\\Sigma_{y}^{-1}A_{x}$, we solve the linear system $C\\hat{\\beta} = d$, where $d = A_{x}^{\\top}\\Sigma_{y}^{-1}b_{y}$.\n\nFor Test Case 1 and Test Case 3, the instrument correlation matrix $R$ is the identity matrix $I$, and the standard errors are uniform, i.e., $s_{l,y} = s$ for all $l$. In this special case, $\\Sigma_{y} = \\mathrm{diag}(s, \\dots, s) \\cdot I \\cdot \\mathrm{diag}(s, \\dots, s) = s^2 I$. The inverse is $\\Sigma_{y}^{-1} = (1/s^2)I$. The estimator simplifies:\n$$\n\\hat{\\beta} = (A_{x}^{\\top}(\\frac{1}{s^2}I)A_{x})^{-1} A_{x}^{\\top}(\\frac{1}{s^2}I)b_{y} = (\\frac{1}{s^2}A_{x}^{\\top}A_{x})^{-1} (\\frac{1}{s^2}A_{x}^{\\top}b_{y}) = (s^2(A_{x}^{\\top}A_{x})^{-1}) (\\frac{1}{s^2}A_{x}^{\\top}b_{y}) = (A_{x}^{\\top}A_{x})^{-1}A_{x}^{\\top}b_{y}\n$$\nThis is the ordinary least squares (OLS) estimator. For Test Case 2, the full GLS formula must be used due to the non-diagonal $R$ and heteroskedastic standard errors.\n\nThe computation proceeds as follows for each test case:\n1. Define the matrices $A_x$, $R$ and the vectors $b_y$, $s_y$.\n2. Construct the diagonal matrix $S_y$ from the vector $s_y$.\n3. Compute the covariance matrix $\\Sigma_y = S_y R S_y$.\n4. Compute the weight matrix $W = \\Sigma_y^{-1}$.\n5. Compute the matrix $C = A_x^{\\top} W A_x$.\n6. Compute the vector $d = A_x^{\\top} W b_y$.\n7. Solve the $2 \\times 2$ linear system $C\\hat{\\beta} = d$ to obtain $\\hat{\\beta} = [\\hat{\\beta}_{\\mathrm{BMI}}, \\hat{\\beta}_{\\mathrm{Schooling}}]^{\\top}$.\n8. The results from all test cases are collected and formatted as specified.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Multivariable Mendelian Randomization (MVMR) estimator\n    for three test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"L\": 5,\n            \"Ax\": np.array([\n                [0.08, 0.02],\n                [0.06, 0.01],\n                [0.07, 0.03],\n                [0.05, 0.02],\n                [0.09, 0.01]\n            ]),\n            \"by\": np.array([0.044, 0.028, 0.0445, 0.0295, 0.041]),\n            \"sy\": np.array([0.02, 0.02, 0.02, 0.02, 0.02]),\n            \"R\": np.identity(5)\n        },\n        {\n            \"L\": 4,\n            \"Ax\": np.array([\n                [0.05, 0.04],\n                [0.045, 0.035],\n                [0.06, 0.05],\n                [0.055, 0.045]\n            ]),\n            \"by\": np.array([0.04, 0.0333, 0.0488, 0.0429]),\n            \"sy\": np.array([0.015, 0.02, 0.018, 0.022]),\n            \"R\": np.array([\n                [1.0, 0.25, 0.0625, 0.015625],\n                [0.25, 1.0, 0.25, 0.0625],\n                [0.0625, 0.25, 1.0, 0.25],\n                [0.015625, 0.0625, 0.25, 1.0]\n            ])\n        },\n        {\n            \"L\": 3,\n            \"Ax\": np.array([\n                [0.02, 0.0204],\n                [0.018, 0.01836],\n                [0.017, 0.01751]\n            ]),\n            \"by\": np.array([-0.0002, 0.00032, -0.000555]),\n            \"sy\": np.array([0.02, 0.02, 0.02]),\n            \"R\": np.identity(3)\n        }\n    ]\n\n    all_betas = []\n\n    for case in test_cases:\n        Ax = case[\"Ax\"]\n        by = case[\"by\"]\n        sy = case[\"sy\"]\n        R = case[\"R\"]\n        L = case[\"L\"]\n\n        # 1. Construct the diagonal matrix S_y from the standard error vector s_y.\n        Sy = np.diag(sy)\n\n        # 2. Compute the covariance matrix of the outcome associations, Sigma_y.\n        # Sigma_y = S_y * R * S_y\n        Sigma_y = Sy @ R @ Sy\n        \n        # 3. Compute the weight matrix W, which is the inverse of Sigma_y.\n        # This is the precision matrix.\n        W = np.linalg.inv(Sigma_y)\n\n        # The MVMR estimator is the solution to the generalized least squares problem.\n        # beta_hat = (Ax^T * W * Ax)^-1 * Ax^T * W * by\n        \n        # 4. Compute the matrix C = Ax^T * W * Ax.\n        C = Ax.T @ W @ Ax\n        \n        # 5. Compute the vector d = Ax^T * W * by.\n        d = Ax.T @ W @ by\n        \n        # 6. Solve the linear system C * beta_hat = d for beta_hat.\n        # This is more numerically stable than computing the inverse of C directly.\n        beta_hat = np.linalg.solve(C, d)\n        \n        all_betas.extend(beta_hat)\n\n    # Format the results as specified: a flat list with numbers rounded to 6 decimal places.\n    formatted_results = [f\"{x:.6f}\" for x in all_betas]\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}