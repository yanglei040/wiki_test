{
    "hands_on_practices": [
        {
            "introduction": "Before building and training complex models, it's crucial to understand their mathematical foundations. This first exercise demystifies the Evidence Lower Bound (ELBO), the core objective function of a VAE. By walking through a concrete calculation with a pre-defined model, you will gain a hands-on understanding of how the reconstruction loss and the Kullback-Leibler divergence term combine to guide the model's learning process on a simplified representation of genomic and epigenomic data .",
            "id": "2439756",
            "problem": "You are given a simplified linear-Gaussian formulation of a Variational Autoencoder (VAE), specifically a Variational Autoencoder (VAE) adapted for multi-channel biological sequence data. Each input $x \\in \\mathbb{R}^D$ concatenates a deoxyribonucleic acid (DNA) one-hot sequence and a cytosine methylation pattern across $L$ genomic positions. For $L=2$, the DNA is represented with $4$ channels per position (adenine, cytosine, guanine, thymine), and methylation is represented with $1$ channel per position. Therefore, $D = 5L = 10$.\n\nThe latent variable is $z \\in \\mathbb{R}^d$ with $d=2$. The probabilistic model components are:\n- Prior: $p(z) = \\mathcal{N}(0, I)$.\n- Encoder (approximate posterior): $q(z \\mid x) = \\mathcal{N}(\\mu(x), \\operatorname{diag}(\\sigma^2_z(x)))$, where $\\mu(x) = M x + c$ and $\\log \\sigma^2_z(x) = v(x) = B x + a$.\n- Decoder (likelihood): $p(x \\mid z) = \\mathcal{N}(W z + b, \\operatorname{diag}(\\sigma^2_x))$.\n\nFor this problem, all parameters are fixed and given explicitly:\n- Dimensions: $L=2$, $D=10$, $d=2$.\n- Matrices and vectors:\n  - $M \\in \\mathbb{R}^{2 \\times 10}$:\n    $\n    \\begin{bmatrix}\n    0.1  -0.2  0.0  0.1  0.05  -0.1  0.2  0.0  0.3  -0.3 \\\\\n    -0.05  0.1  0.15  -0.05  0.2  0.0  -0.1  0.1  -0.2  0.2\n    \\end{bmatrix}\n    $\n  - $B \\in \\mathbb{R}^{2 \\times 10}$:\n    $\n    \\begin{bmatrix}\n    0.05  0.0  -0.05  0.1  0.0  0.05  -0.1  0.0  0.02  -0.02 \\\\\n    0.0  0.05  0.1  0.0  -0.05  0.0  0.05  -0.1  -0.02  0.02\n    \\end{bmatrix}\n    $\n  - $c \\in \\mathbb{R}^{2}$: $[0.0,\\; 0.0]$\n  - $a \\in \\mathbb{R}^{2}$: $[-0.5,\\; -0.3]$\n  - $W \\in \\mathbb{R}^{10 \\times 2}$:\n    $\n    \\begin{bmatrix}\n    0.2  -0.1 \\\\\n    -0.1  0.2 \\\\\n    0.15  0.05 \\\\\n    0.0  0.1 \\\\\n    0.05  -0.05 \\\\\n    0.1  0.0 \\\\\n    -0.05  0.15 \\\\\n    0.2  -0.2 \\\\\n    0.3  0.1 \\\\\n    -0.2  0.2\n    \\end{bmatrix}\n    $\n  - $b \\in \\mathbb{R}^{10}$: $[0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0,\\; 0.0]$\n  - $\\sigma_x^2 \\in \\mathbb{R}^{10}$: $[0.25,\\; 0.25,\\; 0.25,\\; 0.25,\\; 0.25,\\; 0.25,\\; 0.25,\\; 0.25,\\; 0.1,\\; 0.1]$\n\nThe inputs $x \\in \\mathbb{R}^{10}$ are structured as the concatenation of a 4-element one-hot DNA vector for position 1, a 4-element one-hot DNA vector for position 2, a 1-element methylation value for position 1, and a 1-element methylation value for position 2. The test suite consists of the following three inputs, each represented as a single vector:\n- Test case $1$: $x^{(1)} = [1,\\; 0,\\; 0,\\; 0,\\; 0,\\; 1,\\; 0,\\; 0,\\; 0,\\; 1]$\n- Test case $2$: $x^{(2)} = [0,\\; 0,\\; 0,\\; 1,\\; 0,\\; 0,\\; 1,\\; 0,\\; 1,\\; 0]$\n- Test case $3$: $x^{(3)} = [0,\\; 0,\\; 1,\\; 0,\\; 1,\\; 0,\\; 0,\\; 0,\\; 0,\\; 0]$\n\nFor a given $x$, the Evidence Lower Bound (ELBO) is defined as\n$\n\\mathcal{L}(x) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big).\n$\nYour task is to compute $\\mathcal{L}(x)$ for each of the three test cases using the model defined above. All constants and definitions must be used exactly as specified.\n\nYour program should produce a single line of output containing the three ELBO values for the test cases in the same order as given, each rounded to $6$ decimal places, formatted as a comma-separated list enclosed in square brackets, for example: $[v_1,v_2,v_3]$.",
            "solution": "The problem requires the computation of the Evidence Lower Bound (ELBO) for a specified linear-Gaussian Variational Autoencoder (VAE) model. The ELBO, $\\mathcal{L}(x)$, for a given input $x$, is defined by the following expression:\n$$\n\\mathcal{L}(x) = \\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] - \\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big)\n$$\nWe shall derive the analytical expressions for both terms on the right-hand side.\n\nThe model is defined with the following components:\n- Prior: $p(z) = \\mathcal{N}(z; 0, I)$, where $z \\in \\mathbb{R}^d$.\n- Approximate posterior (encoder): $q(z \\mid x) = \\mathcal{N}(z; \\mu_z(x), \\Sigma_z(x))$. The mean is $\\mu_z(x) = Mx+c$ and the covariance is diagonal, $\\Sigma_z(x) = \\operatorname{diag}(\\sigma_z^2(x))$, with the log-variances given by the vector $v(x) = \\log \\sigma_z^2(x) = Bx+a$.\n- Likelihood (decoder): $p(x \\mid z) = \\mathcal{N}(x; Wz+b, \\Sigma_x)$. The covariance $\\Sigma_x = \\operatorname{diag}(\\sigma_x^2)$ is fixed.\n\nFirst, we address the Kullback-Leibler (KL) divergence term. The KL-divergence between two multivariate normal distributions, $q(z) = \\mathcal{N}(\\mu_0, \\Sigma_0)$ and $p(z) = \\mathcal{N}(\\mu_1, \\Sigma_1)$, is given by:\n$$\n\\mathrm{KL}(\\mathcal{N}_0 \\,\\|\\, \\mathcal{N}_1) = \\frac{1}{2} \\left[ \\operatorname{tr}(\\Sigma_1^{-1} \\Sigma_0) + (\\mu_1 - \\mu_0)^T \\Sigma_1^{-1} (\\mu_1 - \\mu_0) - d + \\ln\\left(\\frac{\\det \\Sigma_1}{\\det \\Sigma_0}\\right) \\right]\n$$\nIn our case, $q(z \\mid x) = \\mathcal{N}(\\mu_z, \\Sigma_z)$ and $p(z)=\\mathcal{N}(0, I)$. Substituting $\\mu_0 = \\mu_z$, $\\Sigma_0=\\Sigma_z$, $\\mu_1=0$, and $\\Sigma_1=I$ into the formula, we obtain:\n$$\n\\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big) = \\frac{1}{2} \\left[ \\operatorname{tr}(\\Sigma_z) + \\mu_z^T \\mu_z - d - \\ln(\\det \\Sigma_z) \\right]\n$$\nSince $\\Sigma_z$ is a diagonal matrix with diagonal elements $\\sigma_{z,j}^2$ for $j=1, \\dots, d$, its trace is $\\sum_{j=1}^d \\sigma_{z,j}^2$ and its log-determinant is $\\sum_{j=1}^d \\ln(\\sigma_{z,j}^2)$. This simplifies the KL-divergence to:\n$$\n\\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big) = \\frac{1}{2} \\sum_{j=1}^d \\left( \\mu_{z,j}^2 + \\sigma_{z,j}^2 - 1 - \\ln(\\sigma_{z,j}^2) \\right)\n$$\nUsing the definition $v_j = \\ln(\\sigma_{z,j}^2)$ and $\\sigma_{z,j}^2 = \\exp(v_j)$, the expression becomes:\n$$\n\\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big) = -\\frac{1}{2} \\sum_{j=1}^d \\left( 1 + v_j - \\mu_{z,j}^2 - \\exp(v_j) \\right)\n$$\n\nSecond, we address the expected log-likelihood term, $\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)]$. The log-likelihood of the decoder is:\n$$\n\\log p(x \\mid z) = -\\frac{D}{2} \\log(2\\pi) - \\frac{1}{2} \\log(\\det \\Sigma_x) - \\frac{1}{2} (x - (Wz+b))^T \\Sigma_x^{-1} (x - (Wz+b))\n$$\nSince $\\Sigma_x = \\operatorname{diag}(\\sigma_x^2)$ is constant, we can write this as:\n$$\n\\log p(x \\mid z) = C - \\frac{1}{2} \\sum_{i=1}^D \\frac{(x_i - (Wz+b)_i)^2}{\\sigma_{x,i}^2}\n$$\nwhere $C = -\\frac{D}{2} \\log(2\\pi) - \\frac{1}{2} \\sum_{i=1}^D \\log(\\sigma_{x,i}^2)$ is a constant. We must compute the expectation of this quantity with respect to $z \\sim q(z \\mid x)$. The only term dependent on $z$ is the sum.\n$$\n\\mathbb{E}_{q(z \\mid x)} \\left[ \\sum_{i=1}^D \\frac{(x_i - (Wz+b)_i)^2}{\\sigma_{x,i}^2} \\right] = \\sum_{i=1}^D \\frac{1}{\\sigma_{x,i}^2} \\mathbb{E}_{q(z \\mid x)} \\left[ (x_i - b_i - (Wz)_i)^2 \\right]\n$$\nUsing the property $\\mathbb{E}[Y^2] = (\\mathbb{E}[Y])^2 + \\operatorname{Var}(Y)$, and a random variable $y_i = (Wz)_i = w_i^T z$ where $w_i^T$ is the $i$-th row of $W$:\n$\\mathbb{E}[y_i] = w_i^T \\mathbb{E}[z] = w_i^T \\mu_z$\n$\\operatorname{Var}(y_i) = w_i^T \\operatorname{Cov}(z) w_i = w_i^T \\Sigma_z w_i$\nThe expectation of the squared error for the $i$-th component is:\n$$\n\\mathbb{E}_q [(x_i - b_i - y_i)^2] = (x_i - b_i - \\mathbb{E}[y_i])^2 + \\operatorname{Var}(y_i) = (x_i - b_i - w_i^T \\mu_z)^2 + w_i^T \\Sigma_z w_i\n$$\nSumming over all components $i=1, \\dots, D$:\n$$\n\\mathbb{E}_{q(z \\mid x)}[\\log p(x \\mid z)] = C - \\frac{1}{2} \\sum_{i=1}^D \\frac{(x_i - (W\\mu_z+b)_i)^2}{\\sigma_{x,i}^2} - \\frac{1}{2} \\sum_{i=1}^D \\frac{w_i^T \\Sigma_z w_i}{\\sigma_{x,i}^2}\n$$\nSince $\\Sigma_z$ is diagonal, $w_i^T \\Sigma_z w_i = \\sum_{j=1}^d W_{ij}^2 \\sigma_{z,j}^2$. The final expression for the expected log-likelihood is:\n$$\n\\mathbb{E}_{q}[\\log p(x \\mid z)] = -\\frac{D}{2}\\log(2\\pi) - \\frac{1}{2}\\sum_{i=1}^D \\log(\\sigma_{x,i}^2) - \\frac{1}{2}\\sum_{i=1}^D \\frac{(x_i - (W\\mu_z+b)_i)^2}{\\sigma_{x,i}^2} - \\frac{1}{2}\\sum_{i=1}^D \\sum_{j=1}^d \\frac{W_{ij}^2\\sigma_{z,j}^2}{\\sigma_{x,i}^2}\n$$\nThe computational procedure for each test vector $x$ is as follows:\n1.  Calculate the encoder outputs: $\\mu_z = Mx+c$ and $v = Bx+a$. From $v$, compute $\\sigma_z^2 = \\exp(v)$ element-wise.\n2.  Calculate the KL-divergence term, $\\mathrm{KL}$, using the derived formula.\n3.  Calculate the reconstruction term, $\\mathbb{E}_{q}[\\log p(x \\mid z)]$. This involves:\n    a. Computing the mean reconstruction $x_{\\text{rec}} = W\\mu_z+b$.\n    b. Calculating the squared error term: $\\sum_{i=1}^D (x_i - (x_{\\text{rec}})_i)^2 / \\sigma_{x,i}^2$.\n    c. Calculating the propagated variance term: $\\sum_{i,j} W_{ij}^2\\sigma_{z,j}^2 / \\sigma_{x,i}^2$.\n    d. Adding the constant terms involving $\\log(2\\pi)$ and $\\log(\\sigma_{x,i}^2)$.\n4.  The ELBO is then $\\mathcal{L}(x) = \\mathbb{E}_{q}[\\log p(x \\mid z)] - \\mathrm{KL}$.\n\nThis procedure is implemented for each of the three provided test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Evidence Lower Bound (ELBO) for a linear-Gaussian VAE.\n    \"\"\"\n    \n    # --- Model Parameters ---\n    D = 10\n    d = 2\n    \n    M = np.array([\n        [0.1, -0.2, 0.0, 0.1, 0.05, -0.1, 0.2, 0.0, 0.3, -0.3],\n        [-0.05, 0.1, 0.15, -0.05, 0.2, 0.0, -0.1, 0.1, -0.2, 0.2]\n    ])\n    \n    c = np.array([0.0, 0.0])\n    \n    B = np.array([\n        [0.05, 0.0, -0.05, 0.1, 0.0, 0.05, -0.1, 0.0, 0.02, -0.02],\n        [0.0, 0.05, 0.1, 0.0, -0.05, 0.0, 0.05, -0.1, -0.02, 0.02]\n    ])\n    \n    a = np.array([-0.5, -0.3])\n    \n    W = np.array([\n        [0.2, -0.1],\n        [-0.1, 0.2],\n        [0.15, 0.05],\n        [0.0, 0.1],\n        [0.05, -0.05],\n        [0.1, 0.0],\n        [-0.05, 0.15],\n        [0.2, -0.2],\n        [0.3, 0.1],\n        [-0.2, 0.2]\n    ])\n    \n    b = np.array([0.0] * 10)\n    \n    sigma_x_sq = np.array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.1, 0.1])\n    \n    # --- Test Cases ---\n    test_cases = [\n        np.array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1]),\n        np.array([0, 0, 0, 1, 0, 0, 1, 0, 1, 0]),\n        np.array([0, 0, 1, 0, 1, 0, 0, 0, 0, 0])\n    ]\n\n    results = []\n    \n    for x in test_cases:\n        # --- Step 1: Encoder pass ---\n        # Calculate mean and log-variance of the approximate posterior q(z|x)\n        mu_z = M @ x + c\n        v_z = B @ x + a  # This is log(sigma_z^2)\n        sigma_z_sq = np.exp(v_z)\n\n        # --- Step 2: KL Divergence term ---\n        # KL(q(z|x) || p(z)) where p(z) is N(0, I)\n        # KL = 0.5 * sum(mu_z^2 + sigma_z^2 - 1 - log(sigma_z^2))\n        kl_divergence = 0.5 * np.sum(mu_z**2 + sigma_z_sq - 1 - v_z)\n\n        # --- Step 3: Reconstruction Loss term ---\n        # E[log p(x|z)] where z ~ q(z|x)\n        \n        # Constant part of the reconstruction log-likelihood\n        log_p_const = -0.5 * D * np.log(2 * np.pi) - 0.5 * np.sum(np.log(sigma_x_sq))\n        \n        # Mean of reconstruction\n        x_rec_mean = W @ mu_z + b\n        \n        # Term 1: Squared error between input and reconstructed mean\n        recon_err_term_1 = np.sum((x - x_rec_mean)**2 / sigma_x_sq)\n        \n        # Term 2: Propagated variance from the latent space\n        # This can be calculated as sum_j sigma_z_sq[j] * sum_i (W[i,j]^2 / sigma_x_sq[i])\n        W_sq_div_sigma_x_sq = W**2 / sigma_x_sq[:, np.newaxis]\n        recon_err_term_2 = np.sum(sigma_z_sq * np.sum(W_sq_div_sigma_x_sq, axis=0))\n        \n        # Full expected reconstruction log-likelihood\n        expected_log_likelihood = log_p_const - 0.5 * (recon_err_term_1 + recon_err_term_2)\n        \n        # --- Step 4: ELBO calculation ---\n        elbo = expected_log_likelihood - kl_divergence\n        results.append(elbo)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key skill in bioinformatics is tailoring general machine learning models to specific biological contexts. This practice explores how to infuse a VAE with prior biological knowledge by modifying its loss function. You will derive and implement an objective that uses a Protein-Protein Interaction (PPI) network to penalize reconstruction errors more heavily for functionally related genes, demonstrating how to create a more biologically-informed and sensitive model .",
            "id": "2439822",
            "problem": "You are given a setting in which a Variational Autoencoder (VAE) is trained on gene expression profiles, where each sample is a real-valued vector representing the expression levels of $G$ genes. For a single sample, denote the true expression vector by $\\mathbf{x} \\in \\mathbb{R}^G$ and the decoder reconstruction by $\\hat{\\mathbf{x}} \\in \\mathbb{R}^G$. The canonical reconstruction contribution to the Evidence Lower Bound (ELBO) for a Gaussian likelihood with unit variance is proportional to the squared error $\\sum_{i=1}^{G} (x_i - \\hat{x}_i)^2$. You are also given a Protein-Protein Interaction (PPI) network, represented by a symmetric, nonnegative adjacency matrix $\\mathbf{A} \\in \\mathbb{R}_{\\ge 0}^{G \\times G}$ with zero diagonal that encodes interacting gene pairs.\n\nYour task is to modify the reconstruction term to penalize reconstruction errors more heavily for interacting gene pairs. Start from the following pairwise-augmented reconstruction objective for a single sample:\n$$\n\\mathcal{L}_{\\text{mod}}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\mathbf{A}, \\alpha) \\;=\\; \\sum_{i=1}^{G} \\big(x_i - \\hat{x}_i\\big)^2 \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} \\Big(\\big(x_i - \\hat{x}_i\\big)^2 + \\big(x_j - \\hat{x}_j\\big)^2\\Big),\n$$\nwhere $\\alpha \\in \\mathbb{R}_{\\ge 0}$ is a tunable scalar that controls the strength of the pairwise penalty. Derive, from this definition alone and standard properties of sums on graphs, an equivalent expression for $\\mathcal{L}_{\\text{mod}}$ that can be computed in $\\mathcal{O}(G + \\lvert E \\rvert)$ time using only the per-gene squared errors and the node degrees of the PPI graph (where $\\lvert E \\rvert$ is the number of nonzero off-diagonal entries in $\\mathbf{A}$ counted once for each undirected edge). Justify each step of your derivation using algebraic manipulations and graph-theoretic identities.\n\nThen implement a program that computes $\\mathcal{L}_{\\text{mod}}$ for several specified test cases. In each test case, you are given $\\mathbf{x}$, $\\hat{\\mathbf{x}}$, $\\mathbf{A}$, and $\\alpha$. All arrays and matrices are small and explicitly provided below. No stochasticity is involved. There are no physical units to report.\n\nImplementation constraints:\n- Use real-valued arithmetic.\n- Assume $\\mathbf{A}$ is intended to be symmetric with a zero diagonal; if minor asymmetries are present, you may symmetrize via $\\tfrac{1}{2}\\big(\\mathbf{A} + \\mathbf{A}^\\top\\big)$ and then zero the diagonal.\n- Your program must compute a scalar $\\mathcal{L}_{\\text{mod}}$ for each test case.\n\nTest suite (each case is independent):\n- Case $1$ (happy path, mixed connectivity):\n  - $G = 4$,\n  - $\\mathbf{x} = [\\,10.0,\\, 7.5,\\, 3.0,\\, 0.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,9.5,\\, 7.0,\\, 2.5,\\, 0.5\\,]$,\n  - $\\mathbf{A}$ has edges $(0,1)$, $(1,2)$, $(2,3)$ with weight $1.0$ each and zero otherwise; explicitly,\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  0  0 \\\\\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    0  0  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.3$.\n- Case $2$ (boundary, $\\alpha = 0$):\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,1.0,\\, 2.0,\\, 3.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.0,\\, 1.0,\\, 3.0\\,]$,\n  - $\\mathbf{A}$ fully connected without self-loops; explicitly,\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  1 \\\\\n    1  0  1 \\\\\n    1  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.0$.\n- Case $3$ (no edges, $\\mathbf{A}=\\mathbf{0}$):\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,4.0,\\, 0.0,\\, 5.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,2.0,\\, 1.0,\\, 7.0\\,]$,\n  - $\\mathbf{A} = \\mathbf{0}_{3 \\times 3}$,\n  - $\\alpha = 0.8$.\n- Case $4$ (fully connected, stronger amplification):\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,5.0,\\, 5.0,\\, 5.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,4.0,\\, 7.0,\\, 2.0\\,]$,\n  - $\\mathbf{A}$ fully connected without self-loops; explicitly,\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  1 \\\\\n    1  0  1 \\\\\n    1  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.5$.\n- Case $5$ (exact reconstruction):\n  - $G = 2$,\n  - $\\mathbf{x} = [\\,1.2,\\, 3.4\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.2,\\, 3.4\\,]$,\n  - $\\mathbf{A}$ is a single edge; explicitly,\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1 \\\\\n    1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 2.0$.\n- Case $6$ (weighted PPI edges):\n  - $G = 3$,\n  - $\\mathbf{x} = [\\,2.0,\\, 0.0,\\, 1.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.0,\\, 1.0,\\, 0.0\\,]$,\n  - $\\mathbf{A}$ with edge weights $A_{01} = A_{10} = 0.5$, $A_{12} = A_{21} = 0.2$, others zero; explicitly,\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  0.5  0 \\\\\n    0.5  0  0.2 \\\\\n    0  0.2  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 1.25$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each scalar $\\mathcal{L}_{\\text{mod}}$ corresponding to Cases $1$ through $6$ in order, each value rounded to exactly $6$ decimal places (for example, $[1.234000,2.000000]$).",
            "solution": "The problem as stated is scientifically sound, mathematically well-posed, and complete. It presents a standard task in computational biology: modifying a machine learning objective function to incorporate prior biological knowledge—in this case, a protein-protein interaction network—and then deriving an efficient computational method. The premises rest on established principles of variational autoencoders and graph theory. All necessary data for a unique solution are provided. We may therefore proceed with the derivation and implementation.\n\nThe task is to derive an equivalent but more computationally efficient expression for the modified reconstruction loss, $\\mathcal{L}_{\\text{mod}}$. The provided definition is:\n$$\n\\mathcal{L}_{\\text{mod}}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\mathbf{A}, \\alpha) \\;=\\; \\sum_{i=1}^{G} \\big(x_i - \\hat{x}_i\\big)^2 \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} \\Big(\\big(x_i - \\hat{x}_i\\big)^2 + \\big(x_j - \\hat{x}_j\\big)^2\\Big)\n$$\nHere, $G$ is the number of genes, $\\mathbf{x}$ is the true expression vector, $\\hat{\\mathbf{x}}$ is its reconstruction, $\\mathbf{A}$ is the symmetric adjacency matrix of the PPI network with $A_{ii}=0$, and $\\alpha$ is a non-negative scalar weight.\n\nTo simplify the analysis, let us define the per-gene squared error as $e_i = (x_i - \\hat{x}_i)^2$. The expression for $\\mathcal{L}_{\\text{mod}}$ becomes:\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} (e_i + e_j)\n$$\nThe first term, $\\sum_{i=1}^{G} e_i$, is the standard sum of squared errors, computable in $\\mathcal{O}(G)$ time. The second term, which we will denote $S_{_P}$, represents the pairwise penalty. A naive evaluation of $S_{_P}$ involves a double loop over all pairs $(i, j)$, leading to $\\mathcal{O}(G^2)$ complexity. Our goal is to find an expression for this term that can be computed more efficiently.\n$$\nS_{_P} = \\sum_{1 \\le i  j \\le G} A_{ij} (e_i + e_j)\n$$\nWe can split this sum into two parts:\n$$\nS_{_P} = \\sum_{1 \\le i  j \\le G} A_{ij} e_i + \\sum_{1 \\le i  j \\le G} A_{ij} e_j\n$$\nThis form is not immediately helpful. A more productive approach is to rearrange the summation by focusing on the contribution of each individual node's error, $e_k$, to the total sum $S_{_P}$. For any given node $k$, its error term $e_k$ appears in the sum whenever an edge incident to $k$ is considered. The summation $\\sum_{1 \\le i  j \\le G}$ iterates over all unique undirected edges in the graph represented by $\\mathbf{A}$.\n\nFor a specific node $k$, the term $e_k$ is included when:\n1.  $i = k$, for all $j  k$. The contribution to the sum is $\\sum_{j  k} A_{kj} e_k$.\n2.  $j = k$, for all $i  k$. The contribution to the sum is $\\sum_{i  k} A_{ik} e_k$.\n\nThus, the total coefficient multiplying $e_k$ in the expression for $S_{_P}$ is the sum of these two parts:\n$$\n\\text{Coefficient of } e_k = \\sum_{j  k} A_{kj} + \\sum_{i  k} A_{ik}\n$$\nThe problem states that the adjacency matrix $\\mathbf{A}$ is symmetric ($A_{ij} = A_{ji}$) and has a zero diagonal ($A_{ii} = 0$). Using the symmetry property, we can rewrite the second summation: $\\sum_{i  k} A_{ik} = \\sum_{i  k} A_{ki}$.\nThe coefficient for $e_k$ is then:\n$$\n\\sum_{j  k} A_{kj} + \\sum_{i  k} A_{ki} = \\sum_{j \\neq k} A_{kj}\n$$\nThis sum, $\\sum_{j \\neq k} A_{kj}$, is precisely the weighted degree of node $k$, which we denote by $d_k$. Since $A_{kk} = 0$, the weighted degree is simply the sum over the $k$-th row (or column) of the adjacency matrix: $d_k = \\sum_{j=1}^{G} A_{kj}$.\n\nBy summing the contributions for all nodes $k = 1, \\dots, G$, we reconstruct the entire pairwise sum $S_{_P}$:\n$$\nS_{_P} = \\sum_{k=1}^{G} e_k d_k\n$$\nThis derivation is correct because each term $A_{ij}(e_i+e_j)$ in the original sum is perfectly accounted for: $A_{ij}e_i$ is counted in the $e_i d_i$ term, and $A_{ij}e_j$ (which is $A_{ji}e_j$) is counted in the $e_j d_j$ term.\n\nSubstituting this simplified expression for $S_{_P}$ back into the formula for $\\mathcal{L}_{\\text{mod}}$, we obtain:\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i + \\alpha \\sum_{i=1}^{G} e_i d_i\n$$\nWe can factor out $e_i$ to arrive at the final, computationally efficient expression:\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i (1 + \\alpha d_i)\n$$\nLet us analyze the computational complexity of this final form.\n1.  The per-gene squared errors $e_i = (x_i - \\hat{x}_i)^2$ for all $i=1, \\dots, G$ can be computed in $\\mathcal{O}(G)$ time.\n2.  The weighted degrees $d_i = \\sum_{j=1}^{G} A_{ij}$ for all $i=1, \\dots, G$ can be computed. If $\\mathbf{A}$ is a dense matrix, this takes $\\mathcal{O}(G^2)$ time. However, for a sparse graph with $|E|$ edges (where $|E|$ is the number of non-zero entries in the upper triangle), we can compute all degrees in $\\mathcal{O}(G+|E|)$ time by iterating through an adjacency list representation.\n3.  The final summation $\\sum_{i=1}^{G} e_i (1 + \\alpha d_i)$ requires a single pass over the $G$ genes, taking $\\mathcal{O}(G)$ time.\n\nTherefore, the total time complexity using this derived formula is $\\mathcal{O}(G + |E|)$ if degrees are computed from a sparse representation, or $\\mathcal{O}(G^2)$ if computed from a dense matrix. As the problem asks for a formula that *can be computed* in $\\mathcal{O}(G+|E|)$ time, our derived expression satisfies the requirement. This form avoids the explicit $\\mathcal{O}(G^2)$ iteration over pairs of genes, which is the main bottleneck of the original definition for large $G$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the modified reconstruction loss L_mod for a VAE\n    using an efficient, derived formula for several test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            \"x\": np.array([10.0, 7.5, 3.0, 0.0]),\n            \"x_hat\": np.array([9.5, 7.0, 2.5, 0.5]),\n            \"A\": np.array([\n                [0.0, 1.0, 0.0, 0.0],\n                [1.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 1.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.3\n        },\n        # Case 2\n        {\n            \"x\": np.array([1.0, 2.0, 3.0]),\n            \"x_hat\": np.array([1.0, 1.0, 3.0]),\n            \"A\": np.array([\n                [0.0, 1.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [1.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.0\n        },\n        # Case 3\n        {\n            \"x\": np.array([4.0, 0.0, 5.0]),\n            \"x_hat\": np.array([2.0, 1.0, 7.0]),\n            \"A\": np.zeros((3, 3)),\n            \"alpha\": 0.8\n        },\n        # Case 4\n        {\n            \"x\": np.array([5.0, 5.0, 5.0]),\n            \"x_hat\": np.array([4.0, 7.0, 2.0]),\n            \"A\": np.array([\n                [0.0, 1.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [1.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.5\n        },\n        # Case 5\n        {\n            \"x\": np.array([1.2, 3.4]),\n            \"x_hat\": np.array([1.2, 3.4]),\n            \"A\": np.array([\n                [0.0, 1.0],\n                [1.0, 0.0]\n            ]),\n            \"alpha\": 2.0\n        },\n        # Case 6\n        {\n            \"x\": np.array([2.0, 0.0, 1.0]),\n            \"x_hat\": np.array([1.0, 1.0, 0.0]),\n            \"A\": np.array([\n                [0.0, 0.5, 0.0],\n                [0.5, 0.0, 0.2],\n                [0.0, 0.2, 0.0]\n            ]),\n            \"alpha\": 1.25\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x = case[\"x\"]\n        x_hat = case[\"x_hat\"]\n        A = case[\"A\"]\n        alpha = case[\"alpha\"]\n\n        # Per problem specification, ensure A is symmetric with a zero diagonal.\n        # This is a robust preprocessing step.\n        A_sym = 0.5 * (A + A.T)\n        np.fill_diagonal(A_sym, 0)\n\n        # Calculate per-gene squared errors: e_i = (x_i - x_hat_i)^2\n        e = (x - x_hat)**2\n\n        # Calculate weighted node degrees: d_i = sum_j A_ij\n        d = A_sym.sum(axis=1)\n\n        # Compute L_mod using the derived efficient formula:\n        # L_mod = sum_i e_i * (1 + alpha * d_i)\n        l_mod = np.sum(e * (1.0 + alpha * d))\n        \n        results.append(l_mod)\n\n    # Format the output as a comma-separated list with 6 decimal places.\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice integrates all the concepts into a complete, end-to-end application, mirroring a real-world computational biology workflow. You will train a VAE on synthetic single-cell RNA-sequencing data and then perform the crucial step of interpreting its latent space to uncover a hidden biological process—in this case, the cell cycle. This exercise solidifies your understanding of how VAEs can be used not just for data compression, but as powerful tools for scientific discovery .",
            "id": "2439780",
            "problem": "You will implement a complete, runnable program that trains a one-dimensional Variational Autoencoder (VAE) on a synthetic single-cell RNA sequencing (scRNA-seq) dataset representing a dominant cell-cycle–like factor. You will then demonstrate that traversing the single latent dimension corresponds to interpretable cell-cycle variation by analyzing the reconstructed trajectories of two canonical cell-cycle marker genes.\n\nFundamental base. Use only the following foundational definitions and facts to derive and design your solution:\n- The Variational Autoencoder (VAE) is based on maximizing the Evidence Lower Bound (ELBO). For data $x$, latent $z$, prior $p(z)$, likelihood $p_\\theta(x \\mid z)$, and variational posterior $q_\\phi(z \\mid x)$, the ELBO per sample is\n$$\n\\mathcal{L}(\\theta,\\phi;x) \\;=\\; \\mathbb{E}_{q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x \\mid z)\\right] \\;-\\; \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\nwhere $\\mathrm{KL}$ denotes Kullback–Leibler divergence.\n- The reparameterization trick writes $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\,\\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,1)$ to enable gradient-based optimization of $\\mathbb{E}_{q_\\phi}$.\n- For a Gaussian prior $p(z)=\\mathcal{N}(0,1)$ and a Gaussian variational posterior $q_\\phi(z \\mid x)=\\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x))$, the Kullback–Leibler divergence per sample equals\n$$\n\\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) \\;=\\; \\tfrac{1}{2}\\left(\\mu_\\phi(x)^2 + \\sigma_\\phi(x)^2 - \\log \\sigma_\\phi(x)^2 - 1\\right).\n$$\n- For a Gaussian decoder $p_\\theta(x \\mid z) = \\mathcal{N}(f_\\theta(z), \\sigma_x^2 I)$, maximizing $\\mathbb{E}_{q_\\phi}[\\log p_\\theta(x \\mid z)]$ is equivalent (up to an additive constant) to minimizing the expected squared reconstruction error $\\tfrac{1}{2\\sigma_x^2}\\,\\mathbb{E}_{q_\\phi}\\!\\left[\\lVert x - f_\\theta(z)\\rVert_2^2\\right]$.\n\nModel and dataset specification to implement. You must build a one-dimensional VAE with a linear encoder and linear decoder:\n- Prior: $p(z)=\\mathcal{N}(0,1)$.\n- Decoder: $p_\\theta(x\\mid z)=\\mathcal{N}(W z + b,\\; \\sigma_x^2 I)$ with fixed $\\sigma_x^2 = 1$, where $W \\in \\mathbb{R}^{D \\times 1}$ and $b \\in \\mathbb{R}^{D}$ are trainable.\n- Encoder: $q_\\phi(z \\mid x)=\\mathcal{N}(a^\\top x + c,\\; \\exp(\\ell))$ where $a \\in \\mathbb{R}^{D}$, $c \\in \\mathbb{R}$, and $\\ell \\in \\mathbb{R}$ (a scalar log-variance) are trainable.\n- Training objective: minimize the negative ELBO with an optional $\\beta$ weight on the Kullback–Leibler divergence term (the $\\beta$-VAE objective), i.e.,\n$$\n\\mathcal{J}(\\theta,\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\left\\{\\tfrac{1}{2}\\left\\lVert x_i - (W z_i + b)\\right\\rVert_2^2 \\;+\\; \\beta \\cdot \\tfrac{1}{2}\\left(\\mu_i^2 + \\sigma^2 - \\ell - 1\\right)\\right\\},\n$$\nwith $z_i = \\mu_i + \\sigma \\epsilon_i$, $\\mu_i = a^\\top x_i + c$, $\\sigma = \\exp(\\tfrac{1}{2}\\ell)$, and $\\epsilon_i \\sim \\mathcal{N}(0,1)$.\n- Optimization: use stochastic gradient descent or full-batch gradient descent with the reparameterization trick. The parameters to optimize are $W$, $b$, $a$, $c$, and $\\ell$.\n\nSynthetic scRNA-seq data generator to implement. For each test case, generate $N$ cells with $D$ genes following a dominant linear factor that represents a cell-cycle score:\n- Choose $D = 5$. Let gene index $0$ denote a canonical S-phase marker (e.g., proliferating cell nuclear antigen), and gene index $1$ denote a canonical G2/M marker (e.g., cyclin B1). For simplicity we will refer to them as “PCNA” (index $0$) and “CCNB1” (index $1$).\n- Sample latent scores $s_i \\sim \\mathcal{N}(0,1)$ for $i = 1,\\dots,N$.\n- Let the true loading vector be $u \\in \\mathbb{R}^D$ with $u_0 = 1.0$, $u_1 = -0.8$, $u_2 = 0.25$, $u_3 = 0.1$, and $u_4 = -0.05$.\n- Let a baseline vector be $m \\in \\mathbb{R}^D$ with $m_0 = 2.0$, $m_1 = 1.5$, $m_2 = 0.5$, $m_3 = 0.1$, and $m_4 = -0.2$.\n- Observations are generated as\n$$\nx_i \\;=\\; m \\;+\\; u\\, s_i \\;+\\; \\varepsilon_i,\n\\quad\\text{with}\\quad\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\mathrm{noise}^2 I_D),\n$$\nwhere $\\sigma_\\mathrm{noise}$ is specified per test case. This construction ensures that the dominant variation is a single factor that increases “PCNA” and decreases “CCNB1,” capturing a cell-cycle–like axis.\n\nLatent traversal and alignment criterion. After training, define a latent traversal grid $z_j$ for $j=1,\\dots,K$ with $K=41$ equally spaced points from $-2$ to $2$. For each $z_j$, decode $\\hat{x}(z_j) = W z_j + b$. Because the latent axis orientation is arbitrary, enforce a standard orientation by flipping the sign of $W$ (and correspondingly the encoder direction if maintained internally) so that the reconstructed “PCNA” increases with $z$. Define the cell-cycle alignment criterion as:\n- The Spearman rank correlation between $z$ and the reconstructed “PCNA” trajectory is nonnegative with magnitude at least $\\tau$.\n- The Spearman rank correlation between $z$ and the reconstructed “CCNB1” trajectory is nonpositive with magnitude at least $\\tau$.\nUse $\\tau = 0.95$ and compute correlations using the standard Spearman rank correlation definition.\n\nTest suite. Your program must run the following three test cases and return a boolean per case indicating whether the alignment criterion holds:\n- Case A (happy path): $N=300$, $\\sigma_\\mathrm{noise}=0.30$, $\\beta=1.0$, seed $=42$.\n- Case B (moderate noise, under-regularized): $N=120$, $\\sigma_\\mathrm{noise}=0.60$, $\\beta=0.5$, seed $=123$.\n- Case C (small sample, over-regularized): $N=40$, $\\sigma_\\mathrm{noise}=0.80$, $\\beta=4.0$, seed $=2024$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[resultA,resultB,resultC]”), where each result is a boolean literal True or False corresponding to Cases A, B, and C, respectively. No other text should be printed.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n\n**Variational Autoencoder (VAE) Formulation:**\n- Evidence Lower Bound (ELBO): $\\mathcal{L}(\\theta,\\phi;x) \\;=\\; \\mathbb{E}_{q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x \\mid z)\\right] \\;-\\; \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right)$\n- Reparameterization Trick: $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\,\\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,1)$\n- Kullback–Leibler (KL) Divergence for Gaussian posterior and prior: $\\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) \\;=\\; \\tfrac{1}{2}\\left(\\mu_\\phi(x)^2 + \\sigma_\\phi(x)^2 - \\log \\sigma_\\phi(x)^2 - 1\\right)$\n- Gaussian Decoder Reconstruction Term: minimize $\\tfrac{1}{2\\sigma_x^2}\\,\\mathbb{E}_{q_\\phi}\\!\\left[\\lVert x - f_\\theta(z)\\rVert_2^2\\right]$\n\n**Specific Model and Dataset Implementation:**\n- **Model Architecture (1D Linear VAE):**\n    - Prior: $p(z)=\\mathcal{N}(0,1)$.\n    - Decoder: $p_\\theta(x\\mid z)=\\mathcal{N}(W z + b,\\; \\sigma_x^2 I)$ with fixed $\\sigma_x^2 = 1$. Trainable parameters are $W \\in \\mathbb{R}^{D \\times 1}$ and $b \\in \\mathbb{R}^{D}$.\n    - Encoder: $q_\\phi(z \\mid x)=\\mathcal{N}(a^\\top x + c,\\; \\exp(\\ell))$. Trainable parameters are $a \\in \\mathbb{R}^{D}$, $c \\in \\mathbb{R}$, and $\\ell \\in \\mathbb{R}$.\n- **Training Objective ($\\beta$-VAE):**\n    - Minimize: $\\mathcal{J}(\\theta,\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\left\\{\\tfrac{1}{2}\\left\\lVert x_i - (W z_i + b)\\right\\rVert_2^2 \\;+\\; \\beta \\cdot \\tfrac{1}{2}\\left(\\mu_i^2 + \\sigma^2 - \\ell - 1\\right)\\right\\}$\n    - Definitions: $z_i = \\mu_i + \\sigma \\epsilon_i$, $\\mu_i = a^\\top x_i + c$, $\\sigma = \\exp(\\tfrac{1}{2}\\ell)$, $\\epsilon_i \\sim \\mathcal{N}(0,1)$.\n- **Synthetic Data Generator:**\n    - Number of genes: $D = 5$.\n    - Gene identities: Index $0$ is \"PCNA\", index $1$ is \"CCNB1\".\n    - Latent scores: $s_i \\sim \\mathcal{N}(0,1)$ for $i=1,\\dots,N$.\n    - True loading vector: $u = [1.0, -0.8, 0.25, 0.1, -0.05]^\\top$.\n    - Baseline vector: $m = [2.0, 1.5, 0.5, 0.1, -0.2]^\\top$.\n    - Observation model: $x_i \\;=\\; m \\;+\\; u\\, s_i \\;+\\; \\varepsilon_i$, with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\mathrm{noise}^2 I_D)$.\n- **Latent Traversal and Alignment Criterion:**\n    - Traversal grid: $K=41$ equally spaced points $z_j$ from $-2$ to $2$.\n    - Reconstructed trajectory: $\\hat{x}(z_j) = W z_j + b$.\n    - Orientation: Flip sign of $W$ if necessary to ensure reconstructed \"PCNA\" increases with $z$.\n    - Alignment threshold: $\\tau = 0.95$.\n    - Conditions: Spearman rank correlation $\\rho(z, \\text{\"PCNA\"}) \\ge \\tau$ and $\\rho(z, \\text{\"CCNB1\"}) \\le -\\tau$.\n\n**Test Suite:**\n- Case A: $N=300$, $\\sigma_\\mathrm{noise}=0.30$, $\\beta=1.0$, seed $=42$.\n- Case B: $N=120$, $\\sigma_\\mathrm{noise}=0.60$, $\\beta=0.5$, seed $=123$.\n- Case C: $N=40$, $\\sigma_\\mathrm{noise}=0.80$, $\\beta=4.0$, seed $=2024$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded:** The problem is sound. It uses a standard machine learning model (Variational Autoencoder) and a classical statistical model (linear factor analysis) to address a problem in computational biology (identifying latent structure in single-cell data). The synthetic data generation process is a well-defined simulation, which is a common and valid method for testing algorithms.\n- **Well-Posed:** The problem is well-posed. All model components, the objective function, data generation parameters, and evaluation criteria are explicitly and mathematically defined. There are no missing or contradictory specifications.\n- **Objective:** The problem is stated in objective, formal language. All criteria are quantitative and verifiable.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-defined, scientifically grounded computational task. A complete solution will now be constructed.\n\n### Solution\n\nThe task is to construct and train a one-dimensional linear Variational Autoencoder ($\\beta$-VAE) on synthetic single-cell data and to verify that the learned latent dimension correctly captures the underlying biological process, in this case, a cell-cycle-like factor.\n\n**1. Synthetic Data Generation**\nWe first generate a synthetic dataset representing $N$ cells and $D=5$ genes. The expression profile $x_i \\in \\mathbb{R}^D$ for cell $i$ is generated by a linear factor model:\n$$\nx_i = m + u s_i + \\varepsilon_i\n$$\nHere, $s_i \\sim \\mathcal{N}(0,1)$ is a latent score representing the cell's position along the cell-cycle axis. The vector $m \\in \\mathbb{R}^D$ is a baseline mean expression level, and $u \\in \\mathbb{R}^D$ is the loading vector that defines the direction of variation. The term $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_D)$ represents independent biological and technical noise for each gene. The specified loadings, $u_0 = 1.0$ (\"PCNA\") and $u_1 = -0.8$ (\"CCNB1\"), establish an anticorrelation characteristic of the S-phase to G2/M-phase transition. Our VAE must recover this axis of variation from the data $x_i$.\n\n**2. VAE Model and Objective Function**\nWe employ a linear VAE with a one-dimensional latent space $z \\in \\mathbb{R}$.\n- The **encoder**, $q_\\phi(z|x)$, maps a data point $x \\in \\mathbb{R}^D$ to a distribution over the latent space. We use a Gaussian posterior with a linear mapping for the mean:\n$$\nq_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu(x), \\sigma^2)\n\\quad \\text{where} \\quad\n\\mu(x) = a^\\top x + c \\quad \\text{and} \\quad \\sigma^2 = \\exp(\\ell)\n$$\nThe parameters to be learned are the weight vector $a \\in \\mathbb{R}^D$, the bias $c \\in \\mathbb{R}$, and the log-variance $\\ell \\in \\mathbb{R}$.\n\n- The **decoder**, $p_\\theta(x|z)$, maps a point $z$ from the latent space back to a distribution in the data space. We use a linear Gaussian decoder with fixed unit variance:\n$$\np_\\theta(x \\mid z) = \\mathcal{N}(x \\mid W z + b, I_D)\n$$\nThe parameters to be learned are the weight matrix (here, a vector) $W \\in \\mathbb{R}^{D \\times 1}$ and the bias vector $b \\in \\mathbb{R}^D$.\n\nThe training objective is to minimize the negative Evidence Lower Bound (ELBO), weighted by a parameter $\\beta$ for the KL term (the $\\beta$-VAE objective). For a batch of $N$ samples, this is:\n$$\n\\mathcal{J} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\underbrace{ \\frac{1}{2} \\|x_i - \\hat{x}_i\\|^2_2 }_{\\text{Reconstruction Error}} + \\beta \\cdot \\underbrace{ \\frac{1}{2} \\left( \\mu_i^2 + \\sigma^2 - \\log(\\sigma^2) - 1 \\right) }_{\\text{KL Divergence}} \\right)\n$$\nwhere $\\hat{x}_i = W z_i + b$, and $z_i$ is sampled from $q_\\phi(z|x_i)$ using the reparameterization trick: $z_i = \\mu_i + \\sigma \\epsilon_i$, with $\\epsilon_i \\sim \\mathcal{N}(0,1)$. Substituting $\\log(\\sigma^2) = \\ell$, we obtain the precise objective function from the problem statement.\n\n**3. Optimization via Gradient Descent**\nWe minimize $\\mathcal{J}$ with respect to the parameters $\\phi = \\{a, c, \\ell\\}$ and $\\theta = \\{W, b\\}$ using full-batch gradient descent. The gradients are computed for the total loss over the dataset and then averaged. For a single sample $i$, let the residual be $r_i = x_i - (W z_i + b)$. The gradients of the loss $L_i$ are derived using the chain rule.\n\n- **Decoder parameter gradients:**\n$$ \\nabla_W L_i = -r_i z_i^\\top \\quad\\quad \\nabla_b L_i = -r_i $$\n- **Encoder parameter gradients:**\n$$ \\nabla_a L_i = \\left( -W^\\top r_i + \\beta \\mu_i \\right) x_i^\\top $$\n$$ \\nabla_c L_i = -W^\\top r_i + \\beta \\mu_i $$\n$$ \\nabla_\\ell L_i = \\frac{1}{2} \\left[ (-W^\\top r_i) \\sigma \\epsilon_i + \\beta (\\sigma^2 - 1) \\right] $$\nwhere $\\sigma = \\exp(\\ell/2)$. The parameters are updated iteratively: $p \\leftarrow p - \\eta \\nabla_p \\mathcal{J}$, where $\\eta$ is the learning rate.\n\n**4. Latent Space Traversal and Verification**\nAfter training converges, we assess whether the latent variable $z$ has captured the primary axis of variation.\nFirst, we establish a canonical orientation. The direction of the learned latent axis is arbitrary; the model might learn that increasing $z$ corresponds to a decrease in \"PCNA\" expression, which is the reverse of the true generating factor $s$. We enforce a standard orientation by checking the correlation between a latent variable traversal and the reconstructed \"PCNA\" expression. A grid of $K=41$ values $z_j$ is created, spanning from $-2$ to $2$. These are decoded to obtain trajectories $\\hat{x}(z_j) = W z_j + b$. We compute the Spearman rank correlation $\\rho(z, \\hat{x}_0(z))$. If $\\rho  0$, we invert the latent axis by flipping the sign of the decoder weights: $W \\leftarrow -W$. This action ensures that increasing $z$ corresponds to increasing \"PCNA\" expression, aligning our latent space with the biological convention. The encoder parameters would also need to be flipped ($a \\leftarrow -a$, $c \\leftarrow -c$) if we were to continue using it, but for analysis of the decoder, only $W$ matters.\n\nFinally, we test the alignment criterion. With the correctly oriented $W$, we compute the Spearman correlations for the \"PCNA\" (gene index $0$) and \"CCNB1\" (gene index $1$) trajectories. The criterion is met if:\n$$\n\\rho(z, \\hat{x}_0(z)) \\ge 0.95 \\quad \\text{and} \\quad \\rho(z, \\hat{x}_1(z)) \\le -0.95\n$$\nThis confirms that the model has learned the anticorrelated behavior of the two key marker genes along its single latent dimension. The procedure is repeated for all three test cases to yield the final boolean results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Main function to run the VAE experiment for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Happy path\n        {'N': 300, 'sigma_noise': 0.30, 'beta': 1.0, 'seed': 42},\n        # Case B: Moderate noise, under-regularized\n        {'N': 120, 'sigma_noise': 0.60, 'beta': 0.5, 'seed': 123},\n        # Case C: Small sample, over-regularized\n        {'N': 40, 'sigma_noise': 0.80, 'beta': 4.0, 'seed': 2024},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_single_case(**params)\n        results.append(result)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(N, sigma_noise, seed):\n    \"\"\"\n    Generates synthetic scRNA-seq data based on a linear factor model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    D = 5\n    \n    # True generating parameters\n    u = np.array([1.0, -0.8, 0.25, 0.1, -0.05]).reshape(D, 1)\n    m = np.array([2.0, 1.5, 0.5, 0.1, -0.2])\n\n    # Latent scores and noise\n    s = rng.normal(loc=0.0, scale=1.0, size=(N, 1))\n    noise = rng.normal(loc=0.0, scale=sigma_noise, size=(N, D))\n\n    # Generate data\n    X = m + s @ u.T + noise\n    return X\n\ndef train_vae(X, beta, seed, learning_rate=0.01, epochs=1000):\n    \"\"\"\n    Trains a 1D linear Variational Autoencoder.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    N, D = X.shape\n\n    # Initialize VAE parameters\n    # Decoder params\n    W = rng.normal(loc=0.0, scale=0.1, size=(D, 1))\n    b = np.zeros(D)\n    # Encoder params\n    a = rng.normal(loc=0.0, scale=0.1, size=(D, 1))\n    c = 0.0\n    ell = 0.0 # log-variance\n\n    for epoch in range(epochs):\n        # --- Forward pass ---\n        # Encoder\n        mu = X @ a + c  # Shape (N, 1)\n        sigma = np.exp(0.5 * ell)\n        \n        # Reparameterization trick\n        epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, 1))\n        z = mu + sigma * epsilon  # Shape (N, 1)\n        \n        # Decoder\n        X_recon = z @ W.T + b  # Shape (N, D)\n        \n        # --- Loss calculation (for monitoring, not strictly needed for gradients) ---\n        # recon_loss = 0.5 * np.mean(np.sum((X - X_recon)**2, axis=1))\n        # kl_loss = 0.5 * np.mean(mu**2 + sigma**2 - ell - 1)\n        # loss = recon_loss + beta * kl_loss\n        \n        # --- Gradient calculation (full-batch) ---\n        # Residual term\n        res = X - X_recon  # Shape (N, D)\n\n        # Gradients for decoder parameters (W, b)\n        grad_W = - (res.T @ z) / N\n        grad_b = - np.mean(res, axis=0)\n\n        # Gradients for encoder parameters (a, c, ell)\n        # Chain rule term from reconstruction loss back to z\n        d_recon_loss_d_z = - (res @ W) # Shape (N, 1)\n        \n        # Gradient for 'a'\n        d_kl_loss_d_mu = beta * mu\n        d_loss_d_mu = d_recon_loss_d_z + d_kl_loss_d_mu # Shape (N, 1)\n        grad_a = (X.T @ d_loss_d_mu) / N\n        \n        # Gradient for 'c'\n        grad_c = np.mean(d_loss_d_mu)\n\n        # Gradient for 'ell'\n        d_loss_d_z = d_recon_loss_d_z\n        d_z_d_ell = 0.5 * sigma * epsilon\n        d_kl_loss_d_ell = 0.5 * beta * (sigma**2 - 1)\n        grad_ell = np.mean(d_loss_d_z * d_z_d_ell) + d_kl_loss_d_ell\n\n        # --- Parameter update ---\n        W -= learning_rate * grad_W\n        b -= learning_rate * grad_b\n        a -= learning_rate * grad_a\n        c -= learning_rate * grad_c\n        ell -= learning_rate * grad_ell\n\n    return W, b\n\ndef analyze_model(W, b):\n    \"\"\"\n    Performs latent traversal and checks the alignment criterion.\n    \"\"\"\n    tau = 0.95\n    K = 41\n    z_grid = np.linspace(-2.0, 2.0, K)\n\n    # Decode the latent traversal grid\n    X_recon_traj = z_grid.reshape(-1, 1) @ W.T + b.reshape(1, -1)\n\n    # Extract trajectories for \"PCNA\" (gene 0) and \"CCNB1\" (gene 1)\n    pcna_traj = X_recon_traj[:, 0]\n    \n    # Check orientation and flip if necessary\n    rho_pcna, _ = spearmanr(z_grid, pcna_traj)\n\n    if rho_pcna  0:\n        W = -W\n        # Recalculate trajectories with flipped W\n        X_recon_traj = z_grid.reshape(-1, 1) @ W.T + b.reshape(1, -1)\n        pcna_traj = X_recon_traj[:, 0]\n        # Recalculate correlation for the check\n        rho_pcna, _ = spearmanr(z_grid, pcna_traj)\n        \n    ccnb1_traj = X_recon_traj[:, 1]\n    rho_ccnb1, _ = spearmanr(z_grid, ccnb1_traj)\n\n    # Verify the alignment criterion\n    is_aligned = (rho_pcna >= tau) and (rho_ccnb1 = -tau)\n    \n    return is_aligned\n\ndef run_single_case(N, sigma_noise, beta, seed):\n    \"\"\"\n    Executes one full test case from data generation to analysis.\n    \"\"\"\n    # Generate data with the case-specific seed for the entire process.\n    X = generate_data(N=N, sigma_noise=sigma_noise, seed=seed)\n    \n    # Train the VAE. The same seed ensures reproducible parameter initialization.\n    W, b = train_vae(X=X, beta=beta, seed=seed)\n\n    # Analyze the trained model\n    is_aligned = analyze_model(W, b)\n    \n    return is_aligned\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}