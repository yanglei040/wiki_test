{
    "hands_on_practices": [
        {
            "introduction": "这项实践通过指导您从头开始构建一个随机森林分类器，提供了终极的动手体验。通过亲手实现基尼不纯度 ($Gini~impurity$) 用于节点分裂和多数投票 ($majority~voting$) 用于最终预测等核心概念，您将对这一强大算法的内部工作原理有深刻而实际的理解。该练习使用一个假设的化学信息学数据集来对化合物进行分类，展示了其在计算生物学中的直接应用。",
            "id": "2384429",
            "problem": "构建一个完整的、可运行的程序，该程序为一个化学信息学分类任务实现一个基于决策树集成的二元分类器。该分类器必须遵循随机森林（RF）的数学定义，此处解释为对多个轴对齐、二元、有限深度的决策树（Decision Trees (DT)）的预测结果进行多数投票。这些决策树在由结构描述符表示的固定化合物训练集上进行训练。任务是预测一个化合物是否具有荧光性，表示为类别标签 $1$（荧光）和 $0$（非荧光）。\n\n定义和数据：\n- 每个化合物由一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 表示，其中包含 $d=2$ 个特征：\n  - 特征 0：最长共轭体系长度，以交替键的数量计（整数）。\n  - 特征 1：给电子原子数（整数）。\n- 带标签的训练集为 $\\mathcal{D}=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $n=8$ 且标签 $y_i \\in \\{0,1\\}$。训练样本如下：\n  - $\\mathbf{x}_1=(3,0)$，对应 $y_1=0$。\n  - $\\mathbf{x}_2=(4,1)$，对应 $y_2=0$。\n  - $\\mathbf{x}_3=(5,0)$，对应 $y_3=0$。\n  - $\\mathbf{x}_4=(6,1)$，对应 $y_4=1$。\n  - $\\mathbf{x}_5=(7,1)$，对应 $y_5=1$。\n  - $\\mathbf{x}_6=(8,2)$，对应 $y_6=1$。\n  - $\\mathbf{x}_7=(9,1)$，对应 $y_7=1$。\n  - $\\mathbf{x}_8=(10,0)$，对应 $y_8=1$。\n- 需要返回预测结果的查询集是以下三个化合物的列表：\n  - $\\mathbf{q}_1=(5,1)$。\n  - $\\mathbf{q}_2=(6,0)$。\n  - $\\mathbf{q}_3=(8,1)$。\n\n决策树分类器 $h$ 的数学规范：\n- 一个节点 $v$ 关联一个索引集 $S_v \\subseteq \\{1,\\dots,n\\}$，表示到达该节点的训练样本，以及一个深度 $\\mathrm{depth}(v)\\in \\mathbb{N}_0$。根节点的 $\\mathrm{depth}=0$，且 $S_{\\text{root}}=\\{1,\\dots,n\\}$。\n- 对于任何节点 $v$，定义类别计数 $c_k(v)=|\\{i\\in S_v: y_i=k\\}|$（其中 $k\\in\\{0,1\\}$）、大小 $|S_v|$ 以及经验类别概率 $p_k(v)=c_k(v)/|S_v|$。$v$ 处的基尼不纯度为\n  $$G(v)=1-\\sum_{k\\in\\{0,1\\}} p_k(v)^2.$$\n- 节点 $v$ 处的候选轴对齐分裂定义如下。对于每个特征索引 $j\\in\\{0,1\\}$ 和多重集 $\\{x_{i,j} : i\\in S_v\\}$，取其排序后的唯一值列表 $z_1<z_2<\\dots<z_m$。候选阈值为中点\n  $$\\tau_\\ell=\\frac{z_\\ell+z_{\\ell+1}}{2}\\quad \\text{for }\\ell\\in\\{1,\\dots,m-1\\}.$$\n  一个二元分裂由一对 $(j,\\tau)$ 参数化，它将 $S_v$ 划分成\n  $$S_v^{\\text{L}}(j,\\tau)=\\{i\\in S_v: x_{i,j}\\le \\tau\\},\\quad S_v^{\\text{R}}(j,\\tau)=\\{i\\in S_v: x_{i,j}>\\tau\\}.$$\n  一个候选分裂是有效的，当且仅当 $|S_v^{\\text{L}}(j,\\tau)|\\ge n_{\\min}$ 和 $|S_v^{\\text{R}}(j,\\tau)|\\ge n_{\\min}$，其中 $n_{\\min}\\in \\mathbb{N}$ 是最小叶节点大小参数。\n- 对于 $v$ 处的任何有效分裂 $(j,\\tau)$，定义加权分裂后不纯度\n  $$\\Phi(v;j,\\tau)=\\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G\\big(v^{\\text{L}}(j,\\tau)\\big)+\\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G\\big(v^{\\text{R}}(j,\\tau)\\big),$$\n  其中 $v^{\\text{L}}(j,\\tau)$ 和 $v^{\\text{R}}(j,\\tau)$ 表示由该分裂形成的左右子节点。在所有有效的候选分裂中，选择使 $\\Phi(v;j,\\tau)$ 最小的那个。如果 $\\Phi$ 值出现平局，则选择最小的阈值 $\\tau$ 来打破平局；如果仍然平局，则选择最小的特征索引 $j$。\n- 每个内部节点的特征子抽样：设 $m_{\\text{try}}\\in\\{1,2\\}$ 为每次分裂时考虑的特征数。对于一个节点 $v$，如果 $m_{\\text{try}}<2$，则从 $\\{0,1\\}$ 中无放回地均匀随机选择一个大小为 $m_{\\text{try}}$ 的子集 $F_v$；否则如果 $m_{\\text{try}}=2$，则取所有特征。候选分裂限制在 $j\\in F_v$ 中。\n- 停止规则：如果以下任一条件成立，节点 $v$ 将成为叶节点：$G(v)=0$，或 $\\mathrm{depth}(v)\\ge D_{\\max}$（其中 $D_{\\max}\\in \\mathbb{N}_0$ 是最大深度参数），或由于 $n_{\\min}$ 约束或 $|S_v|<2$ 而没有有效的分裂。叶节点 $v$ 预测的类别为\n  $$\\hat{y}(v)=\\arg\\max_{k\\in\\{0,1\\}} c_k(v),$$\n  平局时选择类别 $0$。\n- 一个决策树 $h$ 将任何 $\\mathbf{x}\\in\\mathbb{R}^2$ 映射到 $\\{0,1\\}$ 中的一个类别。它从根节点开始，利用每个内部节点存储的 $(j,\\tau)$，重复应用规则“如果 $x_j\\le \\tau$，则进入左子节点，否则进入右子节点”，直到到达一个叶节点；输出为该叶节点的 $\\hat{y}(v)$。\n\n随机森林聚合：\n- 一个包含 $T\\in\\mathbb{N}$ 棵树的随机森林是函数\n  $$H(\\mathbf{x})=\\mathrm{majority}\\big(h_1(\\mathbf{x}),h_2(\\mathbf{x}),\\dots,h_T(\\mathbf{x})\\big),$$\n  其中平局时选择类别 $0$。每棵树 $h_t$ 都是在训练集 $\\mathcal{D}$ 上训练的，根据每个测试用例的指定，可以采用或不采用自助采样法（bootstrapping）：自助采样法“关闭”时，使用完整的训练集索引 $S_{\\text{root}}=\\{1,\\dots,n\\}$；自助采样法“开启”时，使用从 $\\{1,\\dots,n\\}$ 中有放回地抽取的 $n$ 个样本。为了保证可复现性，每个测试用例都指定一个整数种子 $s\\in\\mathbb{Z}$；当自助采样法开启时，为每个树索引确定性地设置独立的副本种子（例如，通过将 $s$ 与树索引相加）。特征子抽样遵循上述规则。所有其他选择、分裂和决胜规则必须遵循此处陈述的定义以确保确定性。\n\n测试套件：\n- 使用上面列出的训练数据和查询集。在以下三组参数上评估随机森林。每个元组中的参数为 $(T, D_{\\max}, m_{\\text{try}}, n_{\\min}, s, \\text{bootstrap})$：\n  - 情况 A（一般情况）：$(7, 3, 2, 1, 13, \\text{False})$。\n  - 情况 B（边界深度）：$(1, 1, 2, 1, 7, \\text{False})$。\n  - 情况 C（因最小叶节点大小而阻止任何分裂的边缘情况）：$(5, 3, 2, 5, 5, \\text{False})$。\n- 对于每种情况，按顺序计算随机森林对 $(\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3)$ 的预测结果 $H(\\mathbf{q}_1)$、$H(\\mathbf{q}_2)$ 和 $H(\\mathbf{q}_3)$，结果为 $\\{0,1\\}$ 中的整数。\n\n最终输出格式：\n- 你的程序应生成一行输出，其中包含所有三种情况的结果，格式为一个包含三个列表的逗号分隔列表，每个内部列表按 $(\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3)$ 的顺序给出三个预测。确切要求的格式是\n  $$\\texttt{[[a\\_1,a\\_2,a\\_3],[b\\_1,b\\_2,b\\_3],[c\\_1,c\\_2,c\\_3]]}$$\n  其中每个 $a_i$、$b_i$、$c_i$ 都是 $\\{0,1\\}$ 中的整数，整行中无任何空格。",
            "solution": "该问题陈述经过严格验证，被确定为**有效**。它是一个自包含、有科学依据且适定的计算科学问题，为构建随机森林分类器提供了完整的规范。所有必需的数据、数学定义、超参数和决胜规则都已提供，确保了唯一且确定性的解决方案。\n\n该任务是构建一个随机森林（$H$）分类器，它是一个由 $T$ 棵决策树（$h_t$）组成的集成模型。森林的预测是各个树预测结果的多数投票。问题陈述提供了三个具有特定超参数的测试用例。对于所有三个测试用例，一个关键的观察是 `bootstrap` 参数被设置为 `False`，特征子抽样参数 $m_{\\text{try}}$ 被设置为 $2$，即特征总数（$d=2$）。这带来了一个重要的后果：\n- **无自助采样法**：森林中的每棵树 $h_t$ 都是在完全相同的完整训练数据集 $\\mathcal{D}$ 上训练的。\n- **无特征子抽样**：在每棵树的每个节点，所有 $d=2$ 个特征都会被考虑用于分裂。\n由于包括所有决胜规则在内的树构建算法是确定性的，因此在单个随机森林实例中的所有 $T$ 棵树都将是完全相同的。因此，对于任何给定的测试用例，随机森林的预测 $H(\\mathbf{x})$ 简化为使用相应超参数构建的单棵决策树 $h(\\mathbf{x})$ 的预测。参数 $T$（树的数量）对于最终预测结果变得无关紧要。\n\n我们的流程将是为三个测试用例中的每一个，使用指定的参数（$D_{\\max}, n_{\\min}$）构建一棵决策树，然后用它来对三个查询向量进行分类。\n\n决策树是递归构建的，从包含所有训练数据索引 $S_{\\text{root}} = \\{1, 2, \\dots, 8\\}$ 的根节点开始。对于任何具有关联训练索引 $S_v$ 的节点 $v$，我们执行以下步骤：\n\n1.  **检查停止标准**：节点 $v$ 在以下情况下成为叶节点：\n    a. 节点是纯的，即其基尼不纯度 $G(v) = 1 - \\sum_{k \\in \\{0,1\\}} p_k(v)^2$ 为 $0$。\n    b. 节点的深度达到允许的最大深度，$\\mathrm{depth}(v) \\ge D_{\\max}$。\n    c. 从该节点无法进行有效的划分。一个划分是有效的，如果它将大小为 $|S_v|$ 的节点数据划分为两个子节点，其大小为 $|S_v^{\\text{L}}|$ 和 $|S_v^{\\text{R}}|$，且满足 $|S_v^{\\text{L}}| \\ge n_{\\min}$ 和 $|S_v^{\\text{R}}| \\ge n_{\\min}$。\n\n    如果一个节点是叶节点，其预测 $\\hat{y}(v)$ 是 $S_v$ 中样本的多数类别。平局时偏向类别 $0$。\n\n2.  **寻找最佳分裂**：如果没有满足停止标准，我们搜索最优的分裂 $(j^*, \\tau^*)$，以最小化加权基尼不纯度：\n    $$ \\Phi(v; j, \\tau) = \\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G(v^{\\text{L}}(j,\\tau)) + \\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G(v^{\\text{R}}(j,\\tau)) $$\n    搜索在所有特征 $j \\in \\{0, 1\\}$ 和所有有效的候选阈值 $\\tau$ 上进行。阈值是排序后唯一特征值之间的中点。决胜规则是选择具有最小阈值 $\\tau$ 的分裂，然后是最小的特征索引 $j$。\n\n3.  **递归**：节点 $v$ 成为一个存储 $(j^*, \\tau^*)$ 的内部节点。通过将 $S_v$ 划分为 $S_v^{\\text{L}}(j^*,\\tau^*)$ 和 $S_v^{\\text{R}}(j^*,\\tau^*)$ 来创建两个子节点，并且在 $\\mathrm{depth}(v)+1$ 的深度上对它们递归地应用树构建过程。\n\n训练数据包含 $n=8$ 个样本，标签 $y \\in \\{0,1\\}$：\n$\\mathcal{D} = \\{(\\mathbf{x}_1=(3,0), y_1=0), (\\mathbf{x}_2=(4,1), y_2=0), (\\mathbf{x}_3=(5,0), y_3=0), (\\mathbf{x}_4=(6,1), y_4=1), (\\mathbf{x}_5=(7,1), y_5=1), (\\mathbf{x}_6=(8,2), y_6=1), (\\mathbf{x}_7=(9,1), y_7=1), (\\mathbf{x}_8=(10,0), y_8=1)\\}$。\n根节点包含索引 $\\{1, \\dots, 8\\}$，对应标签 $\\{0,0,0,1,1,1,1,1\\}$。类别计数为 $c_0=3, c_1=5$。初始基尼不纯度为 $G(\\text{root}) = 1 - ((3/8)^2 + (5/8)^2) = 30/64 \\approx 0.46875$。\n\n让我们分析每种情况：\n\n**情况 A: $(T=7, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=1)$ 和 情况 B: $(T=1, D_{\\max}=1, m_{\\text{try}}=2, n_{\\min}=1)$**\n\n对于这两种情况， $n_{\\min}=1$，因此任何产生非空子节点的划分都是有效的。我们为根节点寻找最佳分裂。\n- **特征 $j=0$**：值为 $\\{3,4,5,6,7,8,9,10\\}$。一个候选分裂在 $\\tau=5.5$。\n  - 左子节点 ($x_0 \\le 5.5$)：索引 $\\{1,2,3\\}$，标签 $\\{0,0,0\\}$。这个集合是纯的，所以 $G(v^{\\text{L}})=0$。大小为 $3$。\n  - 右子节点 ($x_0 > 5.5$)：索引 $\\{4,5,6,7,8\\}$，标签 $\\{1,1,1,1,1\\}$。这个集合也是纯的， $G(v^{\\text{R}})=0$。大小为 $5$。\n  - 加权不纯度为 $\\Phi(v; j=0, \\tau=5.5) = (3/8) \\cdot 0 + (5/8) \\cdot 0 = 0$。\n这是一个完美的分裂，达到了可能的最小不纯度 $0$。没有其他分裂能做得更好。因此，根节点在特征 $j=0$ 和阈值 $\\tau=5.5$ 上分裂。\n\n- **对于情况 B ($D_{\\max}=1$)**：子节点位于深度 $1$。由于 $\\mathrm{depth} \\ge D_{\\max}$，两个子节点都成为叶节点。\n  - 左叶节点：包含标签为 $\\{0,0,0\\}$ 的样本。多数类别是 $0$。预测 $\\hat{y}=0$。\n  - 右叶节点：包含标签为 $\\{1,1,1,1,1\\}$ 的样本。多数类别是 $1$。预测 $\\hat{y}=1$。\n\n- **对于情况 A ($D_{\\max}=3$)**：子节点位于深度 $1$。我们检查停止标准。两个子节点都是纯节点（$G=0$），所以它们立即成为叶节点，而无需考虑 $D_{\\max}$。\n  - 最终的树结构与情况 B 的相同。\n\n因此，对于情况 A 和情况 B，决策树是：如果 $x_0 \\le 5.5$，预测 $0$；否则，预测 $1$。\n对查询集 $\\{\\mathbf{q}_1=(5,1), \\mathbf{q}_2=(6,0), \\mathbf{q}_3=(8,1)\\}$ 的预测：\n- $\\mathbf{q}_1=(5,1)$: $x_0=5 \\le 5.5 \\implies$ 预测 $0$。\n- $\\mathbf{q}_2=(6,0)$: $x_0=6 > 5.5 \\implies$ 预测 $1$。\n- $\\mathbf{q}_3=(8,1)$: $x_0=8 > 5.5 \\implies$ 预测 $1$。\n两种情况的预测向量都是 $[0,1,1]$。\n\n**情况 C: $(T=5, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=5)$**\n\n这里，最小叶节点大小参数为 $n_{\\min}=5$。在包含 $|S_{\\text{root}}|=8$ 个样本的根节点，我们寻找一个有效的分裂。一个分裂是有效的，当且仅当两个子节点都至少有 $n_{\\min}=5$ 个样本。如果一个分裂产生大小为 $|S^{\\text{L}}|$ 和 $|S^{\\text{R}}|$ 的子节点，我们必须满足 $|S^{\\text{L}}| \\ge 5$ 和 $|S^{\\text{R}}| \\ge 5$。然而，$|S^{\\text{L}}| + |S^{\\text{R}}| = 8$，所以这是不可能的（例如，$5+5=10>8$）。\n因此，根节点没有有效的分裂。根据停止规则，根节点成为一个叶节点。\n- 叶节点预测：根节点的数据具有计数 $c_0=3$ 和 $c_1=5$。多数类别是 $1$。\n情况 C 的树只是一个总是预测 $1$ 的单一叶节点。\n对查询集的预测：\n- $\\mathbf{q}_1=(5,1) \\implies$ 预测 $1$。\n- $\\mathbf{q}_2=(6,0) \\implies$ 预测 $1$。\n- $\\mathbf{q}_3=(8,1) \\implies$ 预测 $1$。\n情况 C 的预测向量是 $[1,1,1]$。\n\n**预测总结：**\n- 情况 A: $[0,1,1]$\n- 情况 B: $[0,1,1]$\n- 情况 C: $[1,1,1]$\n最终的输出字符串由这些结果构成。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, depth):\n        self.depth = depth\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.is_leaf = False\n        self.prediction = None\n\nclass DecisionTree:\n    \"\"\"Implements a single decision tree classifier.\"\"\"\n    def __init__(self, max_depth, min_samples_leaf):\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n    \n    def _gini(self, y):\n        \"\"\"Calculates the Gini impurity for a set of labels.\"\"\"\n        n_samples = len(y)\n        if n_samples == 0:\n            return 0.0\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / n_samples\n        return 1.0 - np.sum(probabilities**2)\n\n    def _find_best_split(self, X, y, indices):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples = len(indices)\n        if n_samples < 2:\n            return None, None\n            \n        # Gini of the current node before splitting\n        parent_gini = self._gini(y[indices])\n        \n        best_gini = parent_gini\n        best_split = None\n        \n        n_features = X.shape[1]\n        \n        # As per problem, m_try=2, so we iterate through all features\n        for feature_index in range(n_features):\n            feature_values = X[indices, feature_index]\n            unique_values = np.unique(feature_values)\n            \n            if len(unique_values) < 2:\n                continue\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            \n            for threshold in thresholds:\n                left_indices = indices[X[indices, feature_index] <= threshold]\n                right_indices = indices[X[indices, feature_index] > threshold]\n                \n                # Check min_samples_leaf constraint\n                if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n                    continue\n\n                gini_left = self._gini(y[left_indices])\n                gini_right = self._gini(y[right_indices])\n                \n                weighted_gini = (len(left_indices) / n_samples) * gini_left + \\\n                                (len(right_indices) / n_samples) * gini_right\n                \n                # Tie-breaking rules from problem: smallest tau, then smallest feature index.\n                # The loop order naturally handles this.\n                if weighted_gini < best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_index, threshold)\n\n        if best_split is None:\n            return None, None # No valid split found\n        \n        return best_split\n\n    def _grow_tree(self, X, y, indices, depth):\n        \"\"\"Recursively grows the decision tree.\"\"\"\n        node = Node(depth)\n        \n        # Determine leaf prediction\n        labels_at_node = y[indices]\n        count_1 = np.sum(labels_at_node == 1)\n        count_0 = len(labels_at_node) - count_1\n        # Tie-breaking: favor class 0\n        node.prediction = 1 if count_1 > count_0 else 0\n\n        # Check stopping criteria\n        is_pure = self._gini(labels_at_node) == 0.0\n        if is_pure or depth >= self.max_depth or len(indices) < 2:\n            node.is_leaf = True\n            return node\n\n        feature_index, threshold = self._find_best_split(X, y, indices)\n        \n        if feature_index is None: # No valid split found\n            node.is_leaf = True\n            return node\n\n        node.feature_index = feature_index\n        node.threshold = threshold\n        \n        left_indices = indices[X[indices, feature_index] <= threshold]\n        right_indices = indices[X[indices, feature_index] > threshold]\n        \n        node.left = self._grow_tree(X, y, left_indices, depth + 1)\n        node.right = self._grow_tree(X, y, right_indices, depth + 1)\n        \n        return node\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree.\"\"\"\n        initial_indices = np.arange(len(y))\n        self.root = self._grow_tree(X, y, initial_indices, 0)\n        \n    def _predict_one(self, x, node):\n        \"\"\"Predicts class for a single sample.\"\"\"\n        current_node = node\n        while not current_node.is_leaf:\n            if x[current_node.feature_index] <= current_node.threshold:\n                current_node = current_node.left\n            else:\n                current_node = current_node.right\n        return current_node.prediction\n\n    def predict(self, X):\n        \"\"\"Predicts classes for a set of samples.\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\nclass RandomForest:\n    \"\"\"Implements a Random Forest classifier.\"\"\"\n    def __init__(self, n_trees, max_depth, min_samples_leaf, bootstrap, seed):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.bootstrap = bootstrap\n        self.seed = seed\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"Trains the Random Forest.\"\"\"\n        self.trees = []\n        n_samples = len(y)\n        \n        # The problem states m_try=2, so no feature subsampling is needed.\n        # It also sets bootstrap=False, so no data bootstrapping is needed.\n        # This means all trees will be identical for a given hyperparameter set.\n        \n        for i in range(self.n_trees):\n            tree = DecisionTree(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n            \n            if self.bootstrap:\n                # Per problem, we'd use seed + i for reproducibility\n                rng = np.random.default_rng(self.seed + i)\n                indices = rng.choice(n_samples, n_samples, replace=True)\n                tree.fit(X[indices], y[indices])\n            else:\n                tree.fit(X, y)\n                \n            self.trees.append(tree)\n            \n    def predict(self, X):\n        \"\"\"Makes predictions with the Random Forest.\"\"\"\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        # tree_preds shape is (n_trees, n_samples)\n        \n        # Majority vote; ties go to class 0\n        n_ones = np.sum(tree_preds == 1, axis=0)\n        n_zeros = self.n_trees - n_ones\n        \n        return np.where(n_ones > n_zeros, 1, 0)\n\ndef solve():\n    # Define training data\n    X_train = np.array([\n        [3, 0], [4, 1], [5, 0], [6, 1], [7, 1], [8, 2], [9, 1], [10, 0]\n    ])\n    y_train = np.array([0, 0, 0, 1, 1, 1, 1, 1])\n\n    # Define query data\n    X_query = np.array([\n        [5, 1], [6, 0], [8, 1]\n    ])\n\n    # Define test cases: (T, D_max, m_try, n_min, s, bootstrap)\n    test_cases = [\n        (7, 3, 2, 1, 13, False),  # Case A\n        (1, 1, 2, 1, 7, False),   # Case B\n        (5, 3, 2, 5, 5, False)   # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T, D_max, m_try, n_min, s, bootstrap = case\n        \n        # As per problem, m_try is always 2. It is not passed to the RF, as the DT handles all features.\n        rf = RandomForest(n_trees=T, max_depth=D_max, min_samples_leaf=n_min, bootstrap=bootstrap, seed=s)\n        rf.fit(X_train, y_train)\n        predictions = rf.predict(X_query)\n        all_results.append(list(predictions))\n\n    # Format the final output string exactly as required\n    inner_parts = []\n    for res_list in all_results:\n        inner_parts.append(f\"[{','.join(map(str, res_list))}]\")\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在分类任务的基础上，本练习通过将随机森林框架应用于回归问题，展示了其多功能性。您将修改核心逻辑，使用平方误差最小化 ($squared-error~minimization$) 进行分裂，并使用均值进行预测，以解决根据序列特征预测 mRNA 半衰期这一关键的生物信息学问题。这项实践将巩固您对目标函数的改变如何将模型应用从分类任务转换为定量预测的理解。",
            "id": "2384472",
            "problem": "您将执行一项来自计算生物学和生物信息学的监督学习任务：根据从序列衍生的特征，预测信使核糖核酸 (mRNA) 分子的半衰期（以分钟为单位）。该模型应为一个由决策树组成的随机森林回归器。您的程序必须在每次分裂时使用平方误差最小化来构建决策树，并通过对树的预测值进行平均，将它们聚合成一个随机森林。每棵决策树必须在每个内部节点上根据单个特征的阈值进行分裂，并通过达到最大深度或最小样本约束来终止。随机森林必须支持标准的超参数：树的数量、最大深度、每次分裂考虑的特征数量、分裂所需的最小样本数，以及启用或禁用自助采样的选项。\n\n您将获得一个固定的转录本训练数据集，每个转录本由从其序列中提取的特征向量表示：\n- 每个转录本 $i$ 的特征定义：\n    - $x_{i,1}$: $5^{\\prime}$ 非翻译区 (UTR) 长度，以核苷酸为单位。\n    - $x_{i,2}$: 编码序列长度，以密码子为单位。\n    - $x_{i,3}$: 编码序列的鸟嘌呤-胞嘧啶 (GC) 比例，为 $[0,1]$ 区间内的实数。\n    - $x_{i,4}$: 在固定参考下被认为是最优的密码子比例（无单位实数，在 $[0,1]$ 区间内）。\n    - $x_{i,5}$: $3^{\\prime}$ UTR 中富含AU的元件基序 $AUUUA$ 的计数（非负整数）。\n    - $x_{i,6}$: $3^{\\prime}$ UTR 中的尿嘧啶 (U) 比例，为 $[0,1]$ 区间内的实数。\n- 响应变量：\n    - $y_i$: 测得的mRNA半衰期，以分钟为单位（正实数）。\n\n包含 $n = 12$ 个转录本的训练集（每行列出 $(x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, x_{i,6}; y_i)$）：\n- $\\mathrm{T}_1$: $(80, 300, 0.55, 0.60, 0, 0.35; 49.85)$\n- $\\mathrm{T}_2$: $(120, 400, 0.40, 0.30, 2, 0.55; 17.85)$\n- $\\mathrm{T}_3$: $(60, 250, 0.65, 0.80, 0, 0.20; 64.90)$\n- $\\mathrm{T}_4$: $(30, 150, 0.35, 0.10, 3, 0.70; 0.60)$\n- $\\mathrm{T}_5$: $(90, 320, 0.50, 0.50, 1, 0.40; 37.00)$\n- $\\mathrm{T}_6$: $(70, 280, 0.45, 0.20, 2, 0.60; 14.50)$\n- $\\mathrm{T}_7$: $(110, 380, 0.70, 0.90, 0, 0.15; 69.15)$\n- $\\mathrm{T}_8$: $(50, 200, 0.30, 0.25, 1, 0.65; 18.15)$\n- $\\mathrm{T}_9$: $(95, 310, 0.60, 0.55, 0, 0.30; 48.60)$\n- $\\mathrm{T}_{10}$: $(85, 290, 0.42, 0.35, 1, 0.45; 28.65)$\n- $\\mathrm{T}_{11}$: $(130, 450, 0.52, 0.62, 0, 0.25; 49.95)$\n- $\\mathrm{T}_{12}$: $(40, 180, 0.38, 0.15, 2, 0.50; 11.70)$\n\n您的程序必须在每个测试用例中使用此固定数据集进行训练。您必须预测其半衰期的查询（留出）转录本具有以下特征\n$$\n\\mathbf{x}^{\\ast} = (75, 270, 0.58, 0.65, 0, 0.28).\n$$\n\n需要假设的基本原理：\n- 用于回归的决策树通过轴对齐分裂递归地划分特征空间，并在叶节点使用该叶节点上响应的样本均值进行预测。\n- 具有响应 $\\{y_i\\}_{i \\in S}$ 的集合 $S$ 的平方误差不纯度为 $I(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$，其中 $\\bar{y}_S$ 是 $S$ 中 $\\{y_i\\}$ 的均值。\n- 将 $S$ 分裂为不相交的 $L$ 和 $R$ 两个子集时，选择能使 $I(L) + I(R)$ 最小化的分裂。\n\n测试套件。对于下面的每个参数集，请在训练集上训练一个随机森林，并输出对 $\\mathbf{x}^{\\ast}$ 的预测值（以分钟为单位），四舍五入到恰好两位小数：\n- 情况A（理想情况，无分裂）：树的数量 $T = 25$，最大深度 $d_{\\max} = 0$，每次分裂考虑的特征数量 $m_{\\mathrm{try}} = 6$，分裂所需的最小样本数 $n_{\\min} = 2$，禁用自助采样。\n- 情况B（单层决策树）：$T = 1$，$d_{\\max} = 1$，$m_{\\mathrm{try}} = 6$，$n_{\\min} = 2$，禁用自助采样。\n- 情况C（小的单层决策树森林）：$T = 25$，$d_{\\max} = 1$，$m_{\\mathrm{try}} = 6$，$n_{\\min} = 2$，禁用自助采样。\n\n角度单位不适用。将预测的半衰期以分钟为单位表示为浮点数。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，顺序为 [情况A, 情况B, 情况C]，例如，\"[12.34,56.78,90.12]\"。",
            "solution": "该问题陈述已根据既定标准进行了严格验证。\n\n### 步骤1：提取已知条件\n- **任务**：使用随机森林回归器预测mRNA半衰期（$y_i$，以分钟为单位）。\n- **特征**：对于每个转录本 $i$，一个包含6个特征的向量：$\\mathbf{x}_i = (x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, x_{i,6})$。\n    - $x_{i,1}$: $5^{\\prime}$ UTR 长度。\n    - $x_{i,2}$: CDS 长度。\n    - $x_{i,3}$: CDS GC 比例。\n    - $x_{i,4}$: 最优密码子比例。\n    - $x_{i,5}$: $AUUUA$ 基序计数。\n    - $x_{i,6}$: $3^{\\prime}$ UTR 尿嘧啶比例。\n- **训练数据**：一个包含 $n=12$ 个转录本及其特征向量和相应半衰期的固定数据集。\n- **查询点**：一个特征向量为 $\\mathbf{x}^{\\ast} = (75, 270, 0.58, 0.65, 0, 0.28)$ 的单一留出转录本。\n- **决策树模型**：\n    - **分裂准则**：平方误差最小化。集合 $S$ 的不纯度为 $I(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$。将 $S$ 分裂为 $L$ 和 $R$ 的选择旨在最小化 $I(L) + I(R)$。\n    - **叶节点预测**：叶节点处响应值的样本均值。\n    - **终止条件**：达到最大深度（$d_{\\max}$），或者节点的样本数小于分裂所需的最小样本数（$n_{\\min}$）。\n- **随机森林模型**：\n    - **聚合**：对单个树的预测值进行平均。\n    - **超参数**：树的数量（$T$）、最大深度（$d_{\\max}$）、每次分裂的特征数（$m_{\\mathrm{try}}$）、分裂所需的最小样本数（$n_{\\min}$）和自助采样。\n- **测试套件**：\n    - 情况A: $T = 25, d_{\\max} = 0, m_{\\mathrm{try}} = 6, n_{\\min} = 2$, 禁用自助采样。\n    - 情况B: $T = 1, d_{\\max} = 1, m_{\\mathrm{try}} = 6, n_{\\min} = 2$, 禁用自助采样。\n    - 情况C: $T = 25, d_{\\max} = 1, m_{\\mathrm{try}} = 6, n_{\\min} = 2$, 禁用自助采样。\n\n### 步骤2：使用提取的已知条件进行验证\n- **科学依据**：该问题具有科学合理性。所选特征是已知的mRNA稳定性调节因子，预测半衰期是计算生物学中的一个标准任务。所选模型“随机森林”是一种成熟且强大的机器学习技术。\n- **良定性**：该问题在数学和算法上是良定的。它提供了一个完整的训练数据集、一个特定的查询点，以及对要构建的模型的精确定义，包括每个测试用例所需的所有超参数。这确保了可以计算出唯一且可验证的解。\n- **客观性**：问题陈述使用了精确、客观的语言，没有歧义或主观论断。\n\n### 步骤3：结论与行动\n该问题被判定为**有效**。它是一个定义明确且基于科学原理的标准计算任务。将提供一个解决方案。\n\n### 基于原理的解决方案\n任务是根据基本原理实现一个随机森林回归器来预测mRNA半衰期。该解决方案包括两个主要部分：一个决策树回归器和聚合多个此类树的随机森林集成。\n\n**决策树回归器**\n回归树是一种层次模型，它将特征空间递归地划分为多个超矩形。树中的每个内部节点对应于基于单个特征和阈值的分裂。每个终端节点（或称叶节点）包含一个预测值。\n\n1.  **递归划分**：从包含所有训练数据的根节点开始构建树。在每个节点，我们寻找最佳分裂，将数据划分到两个子节点中。\n2.  **分裂准则**：对于回归任务，寻找最佳分裂的一个常用准则是最小化平方和误差 (SSE)。给定一个节点上的数据集 $S$，其不纯度是相对于其均值的SSE：\n    $$I(S) = \\sum_{i \\in S} (y_i - \\bar{y}_S)^2$$\n    其中 $\\bar{y}_S = \\frac{1}{|S|} \\sum_{i \\in S} y_i$。一次分裂将数据 $S$ 划分为左子集 $L$ 和右子集 $R$。最优分裂（即最佳特征和阈值）是使子节点组合不纯度最小化的分裂：$\\min(I(L) + I(R))$。\n3.  **终止条件**：在以下任一情况下，递归划分停止，并创建一个叶节点：\n    - 树的当前深度达到超参数 $d_{\\max}$。\n    - 当前节点的样本数量小于超参数 $n_{\\min}$。\n    - 节点是“纯”的，意味着所有目标值 $y_i$ 都相同，因此不纯度 $I(S)$ 为0。\n4.  **预测**：对于一个新的数据点，它会根据每个内部节点的分裂被传递到树的下方。预测值是它到达的叶节点中存储的值，该值是落入该叶节点的训练目标值的均值：$\\bar{y}_{\\text{leaf}}$。\n\n**随机森林回归器**\n随机森林是一种集成方法，通过降低方差和过拟合来改进单个决策树。它构建多棵决策树并聚合它们的预测。\n\n1.  **Bagging (自助聚合)**：森林中的每棵树都在训练数据的随机子样本上进行训练，通常是有放回地抽取（自助法）。问题指明此功能可以被禁用。\n2.  **特征随机性**：在决策树的每次分裂中，只考虑 $m_{\\mathrm{try}}$ 个特征的随机子集来寻找最佳分裂。这可以降低森林中树之间的相关性。\n3.  **预测**：随机森林对新数据点的最终预测是森林中所有单棵树预测值的平均值。\n\n**测试用例分析**\n\n训练数据集包含 $n=12$ 个样本。查询点为 $\\mathbf{x}^{\\ast} = (75, 270, 0.58, 0.65, 0, 0.28)$。所有测试用例的超参数都包括禁用自助采样和 $m_{\\mathrm{try}}=6$。由于总共有6个特征，每次分裂考虑6个特征意味着所有特征都被考虑，从而消除了特征选择的随机性。由于自助采样也被禁用，单个森林内的所有树都将是完全相同的。\n\n**情况A**：$T = 25$，$d_{\\max} = 0$，$m_{\\mathrm{try}} = 6$，$n_{\\min} = 2$，禁用自助采样。\n此处，$d_{\\max}=0$。树的构建在根节点处立即终止。这棵树只是一个单独的叶节点。其预测值是训练集中所有目标值的均值。\n$$ \\bar{y}_{\\text{train}} = \\frac{1}{12} \\sum_{i=1}^{12} y_i = \\frac{410.9}{12} \\approx 34.24167 $$\n25棵树中的每一棵都将是这个单叶节点，每棵树的预测值都约为 $34.24167$。森林的预测是这些相同值的平均值，即约 $34.24167$。四舍五入到两位小数，结果是 $34.24$。\n\n**情况B**：$T = 1$，$d_{\\max} = 1$，$m_{\\mathrm{try}} = 6$，$n_{\\min} = 2$，禁用自助采样。\n构建一棵 $d_{\\max}=1$ 的单决策树。这是一个“决策树桩”。我们必须在所有6个特征和所有可能的阈值上找到最佳的单次分裂。实现会系统地评估每个可能的分裂。发现最优分裂发生在特征索引4（$AUUUA$基序的计数，$x_5$），阈值为 $0.5$。\n- **$x_5 \\le 0.5$（即$x_5=0$）的数据点：** 转录本 $T_1, T_3, T_7, T_9, T_{11}$。\n  - 该组（左叶节点）的平均半衰期为 $\\frac{49.85 + 64.90 + 69.15 + 48.60 + 49.95}{5} = \\frac{282.45}{5} = 56.49$。\n- **$x_5 > 0.5$（即$x_5 \\ge 1$）的数据点：** 转录本 $T_2, T_4, T_5, T_6, T_8, T_{10}, T_{12}$。\n  - 该组（右叶节点）的平均半衰期为 $\\frac{17.85 + 0.60 + 37.00 + 14.50 + 18.15 + 28.65 + 11.70}{7} = \\frac{128.45}{7} \\approx 18.35$。\n查询点 $\\mathbf{x}^{\\ast}$ 的第五个特征值为 $0$。由于 $0 \\le 0.5$，它被分配到左叶节点。预测值为 $56.49$。\n\n**情况C**：$T = 25$，$d_{\\max} = 1$，$m_{\\mathrm{try}} = 6$，$n_{\\min} = 2$，禁用自助采样。\n该情况构建了一个由25个决策树桩组成的森林。如前所述，因为禁用了自助采样并且在每次分裂时考虑所有特征，所以在树的构建过程中没有随机性来源。因此，所有25棵树都将与情况B中构建的单个决策树桩完全相同。每棵树对 $\\mathbf{x}^{\\ast}$ 的预测都是 $56.49$。森林的最终预测是25个相同值的平均值，即 $56.49$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs a Random Forest regressor from scratch based on the problem specification.\n    \"\"\"\n\n    class Node:\n        \"\"\"\n        Represents a node in a decision tree.\n        A node is either a leaf node (with a prediction 'value') or an\n        internal node (with split 'feature_idx', 'threshold', and child nodes).\n        \"\"\"\n        def __init__(self, value=None, feature_idx=None, threshold=None, left=None, right=None):\n            self.value = value\n            self.feature_idx = feature_idx\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n\n    class DecisionTree:\n        \"\"\"\n        Decision Tree regressor implementation.\n        \"\"\"\n        def __init__(self, max_depth=100, min_samples_split=2, m_try=None):\n            self.max_depth = max_depth\n            self.min_samples_split = min_samples_split\n            self.m_try = m_try\n            self.root = None\n            self.n_features_ = 0\n\n        def fit(self, X, y):\n            \"\"\"Builds the decision tree.\"\"\"\n            self.n_features_ = X.shape[1] if X.ndim > 1 else 1\n            if self.m_try is None:\n                self.m_try = self.n_features_\n            self.root = self._build_tree(X, y)\n\n        def _build_tree(self, X, y, depth=0):\n            \"\"\"Recursively builds the tree.\"\"\"\n            n_samples = len(y)\n            \n            # Termination conditions\n            is_leaf = (\n                depth >= self.max_depth or\n                n_samples < self.min_samples_split or\n                len(np.unique(y)) == 1\n            )\n            if is_leaf:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            # In this problem, m_try equals total features, so no random selection.\n            feat_idxs = np.arange(self.n_features_)\n\n            best_split = self._find_best_split(X, y, feat_idxs)\n            \n            if best_split['score'] == np.inf:\n                leaf_value = np.mean(y)\n                return Node(value=leaf_value)\n\n            left_indices = X[:, best_split['feature_idx']] <= best_split['threshold']\n            right_indices = ~left_indices\n            \n            X_left, y_left = X[left_indices], y[left_indices]\n            X_right, y_right = X[right_indices], y[right_indices]\n\n            left_child = self._build_tree(X_left, y_left, depth + 1)\n            right_child = self._build_tree(X_right, y_right, depth + 1)\n            \n            return Node(feature_idx=best_split['feature_idx'], threshold=best_split['threshold'], left=left_child, right=right_child)\n\n        def _squared_error(self, y):\n            \"\"\"Calculates the sum of squared errors for a set of values.\"\"\"\n            if len(y) == 0:\n                return 0.0\n            mean = np.mean(y)\n            return np.sum((y - mean)**2)\n\n        def _find_best_split(self, X, y, feat_idxs):\n            \"\"\"Finds the best split (feature and threshold) for a node.\"\"\"\n            best_split = {'score': np.inf, 'feature_idx': None, 'threshold': None}\n            \n            for feat_idx in feat_idxs:\n                feature_values = X[:, feat_idx]\n                unique_vals = np.unique(feature_values)\n                \n                for i in range(len(unique_vals) - 1):\n                    threshold = (unique_vals[i] + unique_vals[i+1]) / 2.0\n                    \n                    left_indices = feature_values <= threshold\n                    \n                    y_left, y_right = y[left_indices], y[~left_indices]\n                    \n                    if len(y_left) == 0 or len(y_right) == 0:\n                        continue\n                        \n                    current_sse = self._squared_error(y_left) + self._squared_error(y_right)\n                    \n                    if current_sse < best_split['score']:\n                        best_split['score'] = current_sse\n                        best_split['feature_idx'] = feat_idx\n                        best_split['threshold'] = threshold\n            \n            return best_split\n\n        def predict(self, x):\n            \"\"\"Predicts the value for a single sample x.\"\"\"\n            return self._traverse_tree(x, self.root)\n\n        def _traverse_tree(self, x, node):\n            \"\"\"Traverses the tree to find the prediction for a sample x.\"\"\"\n            if node.value is not None:\n                return node.value\n            \n            if x[node.feature_idx] <= node.threshold:\n                return self._traverse_tree(x, node.left)\n            else:\n                return self._traverse_tree(x, node.right)\n\n    class RandomForest:\n        \"\"\"\n        Random Forest regressor implementation.\n        \"\"\"\n        def __init__(self, n_trees=25, max_depth=10, min_samples_split=2, m_try=None, bootstrap=True):\n            self.n_trees = n_trees\n            self.max_depth = max_depth\n            self.min_samples_split = min_samples_split\n            self.m_try = m_try\n            self.bootstrap = bootstrap\n            self.trees = []\n        \n        def fit(self, X, y):\n            \"\"\"Trains the Random Forest model.\"\"\"\n            self.trees = []\n            n_samples = len(y)\n            for _ in range(self.n_trees):\n                tree = DecisionTree(\n                    max_depth=self.max_depth,\n                    min_samples_split=self.min_samples_split,\n                    m_try=self.m_try\n                )\n                \n                if self.bootstrap:\n                    idxs = np.random.choice(n_samples, n_samples, replace=True)\n                    X_sample, y_sample = X[idxs], y[idxs]\n                else:\n                    X_sample, y_sample = X, y\n\n                tree.fit(X_sample, y_sample)\n                self.trees.append(tree)\n\n        def predict(self, x):\n            \"\"\"Makes a prediction for a single sample.\"\"\"\n            tree_predictions = [tree.predict(x) for tree in self.trees]\n            return np.mean(tree_predictions)\n\n    # Define the fixed training dataset from the problem statement.\n    X_train = np.array([\n        [80.0, 300.0, 0.55, 0.60, 0.0, 0.35],\n        [120.0, 400.0, 0.40, 0.30, 2.0, 0.55],\n        [60.0, 250.0, 0.65, 0.80, 0.0, 0.20],\n        [30.0, 150.0, 0.35, 0.10, 3.0, 0.70],\n        [90.0, 320.0, 0.50, 0.50, 1.0, 0.40],\n        [70.0, 280.0, 0.45, 0.20, 2.0, 0.60],\n        [110.0, 380.0, 0.70, 0.90, 0.0, 0.15],\n        [50.0, 200.0, 0.30, 0.25, 1.0, 0.65],\n        [95.0, 310.0, 0.60, 0.55, 0.0, 0.30],\n        [85.0, 290.0, 0.42, 0.35, 1.0, 0.45],\n        [130.0, 450.0, 0.52, 0.62, 0.0, 0.25],\n        [40.0, 180.0, 0.38, 0.15, 2.0, 0.50]\n    ])\n    y_train = np.array([49.85, 17.85, 64.90, 0.60, 37.00, 14.50, 69.15, 18.15, 48.60, 28.65, 49.95, 11.70])\n\n    # Query transcript features\n    x_star = np.array([75.0, 270.0, 0.58, 0.65, 0.0, 0.28])\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'n_trees': 25, 'max_depth': 0, 'm_try': 6, 'min_samples_split': 2, 'bootstrap': False},\n        # Case B\n        {'n_trees': 1, 'max_depth': 1, 'm_try': 6, 'min_samples_split': 2, 'bootstrap': False},\n        # Case C\n        {'n_trees': 25, 'max_depth': 1, 'm_try': 6, 'min_samples_split': 2, 'bootstrap': False},\n    ]\n\n    results = []\n    for params in test_cases:\n        rf = RandomForest(\n            n_trees=params['n_trees'],\n            max_depth=params['max_depth'],\n            min_samples_split=params['min_samples_split'],\n            m_try=params['m_try'],\n            bootstrap=params['bootstrap']\n        )\n        \n        rf.fit(X_train, y_train)\n        prediction = rf.predict(x_star)\n        \n        # Format the result to exactly two decimal places\n        results.append(f\"{prediction:.2f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "这项高级实践将带您从模型实现走向模型分析与解释——这是计算生物学家必备的两项关键技能。您将通过比较一个标准的、无约束的决策树和一个在初始分裂时被强制基于假设的先验知识（例如，一个已知的致病基因）进行分裂的决策树，来研究决策树的结构敏感性。这项练习能让您深刻洞察决策树构建的贪心本质，以及单个约束如何级联式地产生一个截然不同的模型，这是理解模型稳定性的关键概念。",
            "id": "2384430",
            "problem": "给定一个计算生物学中的二元分类任务，其中每个实例对应一个样本，该样本具有多个基因的测量表达水平。特征矩阵表示为 $X \\in \\mathbb{R}^{n \\times d}$，标签表示为 $y \\in \\{0,1\\}^n$。考虑如下定义的一族二元决策树。\n\n一个节点对应于样本索引的一个子集 $S \\subseteq \\{1,\\dots,n\\}$。对于节点 $S$，其基尼不纯度定义为\n$$\nG(S) \\;=\\; 1 \\;-\\; \\sum_{c \\in \\{0,1\\}} \\left(\\frac{| \\{ i \\in S \\,:\\, y_i = c \\} |}{|S|}\\right)^2.\n$$\n对于任意特征索引 $j \\in \\{0,\\dots,d-1\\}$ 和阈值 $t \\in \\mathbb{R}$，将 $S$ 分裂为\n$$\nS_L(j,t) \\;=\\; \\{ i \\in S \\,:\\, X_{i,j} \\le t \\}, \\quad S_R(j,t) \\;=\\; \\{ i \\in S \\,:\\, X_{i,j} > t \\}.\n$$\n在节点 $S$ 处，特征 $j$ 的可选阈值集合，是 $\\{ X_{i,j} \\,:\\, i \\in S \\}$ 中所有不同值按升序排序后，各对相邻值的中点所组成的集合。在节点 $S$ 处，一个分裂 $(j,t)$ 的不纯度下降量为\n$$\n\\Delta(S;j,t) \\;=\\; G(S) \\;-\\; \\frac{|S_L(j,t)|}{|S|} G(S_L(j,t)) \\;-\\; \\frac{|S_R(j,t)|}{|S|} G(S_R(j,t)).\n$$\n在节点 $S$ 处，一个分裂是有效的，前提是其两个子节点都非空，并且都满足最小叶节点大小约束 $|S_L(j,t)| \\ge m_{\\min}$ 和 $|S_R(j,t)| \\ge m_{\\min}$，其中 $m_{\\min} \\in \\mathbb{N}$ 是一个给定参数。在所有有效的分裂中，节点选择能使 $\\Delta(S;j,t)$ 最大化的那个分裂。如果 $\\Delta(S;j,t)$ 出现平局，则选择最小的特征索引 $j$ 来打破平局；如果仍然平局，则选择最小的阈值 $t$。如果没有有效的分割能产生严格为正的不纯度下降量，或者节点深度达到最大深度 $D_{\\max}$，或者 $|S| < 2$，那么该节点成为叶节点。叶节点预测 $S$ 中的多数类，如果票数相等，则预测类别 $0$。\n\n在相同的数据和相同的 $(D_{\\max}, m_{\\min})$ 参数下，定义两棵树：\n- 无约束树（unconstrained tree）：从根节点 $S = \\{1,\\dots,n\\}$ 开始，递归应用上述规则构建。\n- 根节点约束树（constrained-root tree）：其根节点被强制在指定的特征索引 $j^\\star \\in \\{0,\\dots,d-1\\}$ 上进行分裂。仅在根节点处，通过在可选阈值中最大化 $\\Delta(\\{1,\\dots,n\\}; j^\\star, t)$ 来选择 $t$。如果在根节点，特征 $j^\\star$ 没有任何阈值能满足最小叶节点大小约束，则在仅产生非空子节点的可选阈值中，通过最大化 $\\Delta(\\{1,\\dots,n\\}; j^\\star, t)$ 来选择 $t$，此时仅在根节点忽略最小叶节点大小约束。如果不存在可选阈值（即所有的 $X_{i,j^\\star}$ 都相等），则根节点成为叶节点。根节点以下的所有节点都遵守与无约束树相同的规则，使用给定的 $(D_{\\max}, m_{\\min})$。\n\n对于每棵树，将树的深度定义为从根节点到任意叶节点的路径上的最大边数。将训练集误分类率定义为\n$$\n\\mathrm{err} \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ \\hat{y}_i \\ne y_i \\},\n$$\n其中 $\\hat{y}_i$ 是树对样本 $i$ 的预测结果。将一棵树的前序内部结构序列定义为：通过前序遍历得到的对偶列表 $(j,t)$，该遍历记录了每个被访问的内部节点所选择的特征索引 $j$ 和阈值 $t$；叶节点不贡献任何条目。当且仅当两个前序内部结构序列在长度上不同，或在至少一个对应的对偶 $(j,t)$ 上不同（其中阈值作为实数进行比较）时，我们声明根节点约束树相对于无约束树“结构上发生了改变”。\n\n你的任务是编写一个完整的程序，该程序为下方的每个测试用例，严格按照定义构建两棵树，并为每个用例输出一个列表，其中包含：\n- 无约束树的深度（整数），\n- 根节点约束树的深度（整数），\n- 无约束树的训练集误分类率（浮点数，保留六位小数），\n- 根节点约束树的训练集误分类率（浮点数，保留六位小数），\n- 根节点约束树中实际使用的强制根节点阈值（浮点数，保留六位小数；如果根节点约束树在根处未分裂，则此处输出单个数字 $-1.0$），\n- 一个布尔值，指示前序内部结构序列是否改变（如果改变则为 True，否则为 False）。\n\n程序必须将所有测试用例的结果汇总到单行输出中，该输出包含一个用方括号括起来的逗号分隔列表，且无空格。\n\n测试套件。对于每个用例，给定 $(X, y, D_{\\max}, m_{\\min}, j^\\star)$：\n1) 用例 A:\n- $X =$ [\n[$0.1$, $0.3$, $0.2$],\n[$0.2$, $0.1$, $0.5$],\n[$0.4$, $0.6$, $0.7$],\n[$0.6$, $0.2$, $0.4$],\n[$0.7$, $0.8$, $0.9$],\n[$0.8$, $0.1$, $0.3$],\n[$0.9$, $0.5$, $0.2$],\n[$0.55$, $0.4$, $0.6$]\n]\n- $y = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n- $D_{\\max} = 3$\n- $m_{\\min} = 2$\n- $j^\\star = 0$\n\n2) 用例 B:\n- $X =$ [\n[$0.2$, $0.3$, $0.1$],\n[$0.1$, $0.2$, $0.8$],\n[$0.4$, $0.6$, $0.5$],\n[$0.35$, $0.7$, $0.4$],\n[$0.5$, $0.9$, $0.6$],\n[$0.45$, $0.55$, $0.2$],\n[$0.3$, $0.4$, $0.9$],\n[$0.25$, $0.8$, $0.3$],\n[$0.6$, $0.1$, $0.7$],\n[$0.7$, $0.85$, $0.2$]\n]\n- $y = [\\,0,\\,0,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1\\,]$\n- $D_{\\max} = 3$\n- $m_{\\min} = 2$\n- $j^\\star = 2$\n\n3) 用例 C:\n- $X =$ [\n[$0.0$, $0.0$],\n[$0.0$, $1.0$],\n[$1.0$, $0.0$],\n[$1.0$, $1.0$],\n[$0.5$, $0.5$],\n[$0.5$, $0.5$]\n]\n- $y = [\\,0,\\,0,\\,1,\\,1,\\,0,\\,1\\,]$\n- $D_{\\max} = 3$\n- $m_{\\min} = 1$\n- $j^\\star = 1$\n\n所有数值都没有单位。中间计算中可能出现的任何角度都是无关紧要的。最终输出中的浮点数必须按规定保留六位小数。你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[$[a_1,a_2,\\dots],\\,[b_1,b_2,\\dots],\\,[c_1,c_2,\\dots]$]”），且无空格。",
            "solution": "用户要求一个程序来构建和分析用于计算生物学分类任务的两种二元决策树：一种是无约束树，另一种是根节点约束树。该问题是一个定义明确的算法任务，基于机器学习的标准原理，特别是使用基尼不纯度准则的决策树归纳。\n\n### **问题验证**\n\n第一步是验证问题陈述。\n\n**1. 提取给定条件：**\n- **数据：** 特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$，标签 $y \\in \\{0,1\\}^n$。\n- **超参数：** 最大深度 $D_{\\max}$，最小叶节点大小 $m_{\\min}$。\n- **树构建规则：**\n    - 基尼不纯度：$G(S) = 1 - \\sum_{c \\in \\{0,1\\}} (p_c)^2$。\n    - 分裂：节点 $S$ 基于特征 $j$ 和阈值 $t$ 分裂为 $S_L$ 和 $S_R$。\n    - 可选阈值：连续唯一排序特征值的中点。\n    - 不纯度下降量：$\\Delta(S;j,t) = G(S) - \\frac{|S_L|}{|S|} G(S_L) - \\frac{|S_R|}{|S|} G(S_R)$。\n    - 最佳分裂：在所有*有效*分裂中使 $\\Delta$ 最大化的分裂。若子节点大小均不小于 $m_{\\min}$，则分裂有效。\n    - 平局打破规则：最小的特征索引 $j$，然后是最小的阈值 $t$。\n    - 终止（叶节点）条件：\n        1. 没有有效分裂能产生严格为正的不纯度下降量（$\\Delta \\le 0$）。\n        2. 节点深度等于 $D_{\\max}$。\n        3. 节点大小 $|S| < 2$。\n    - 叶节点预测：节点中的多数类；平局时预测类别 0。\n- **根节点约束树：**\n    - 根节点必须在指定的特征 $j^\\star$ 上分裂。\n    - 为特征 $j^\\star$ 选择使 $\\Delta$ 最大化的阈值 $t$。\n    - 如果 $j^\\star$ 上没有分裂能满足 $m_{\\min}$，则放宽约束，只要求子节点非空。\n    - 如果在 $j^\\star$ 上不可能进行分裂（例如，特征值恒定），则根节点成为叶节点。\n- **输出指标：**\n    - 树的深度、训练集误分类率、约束根节点使用的阈值，以及一个布尔值，指示树的内部结构是否不同。结构由内部节点分裂规则 $(j, t)$ 的前序遍历定义。\n\n**2. 验证结论：**\n- **科学依据：** 该问题描述了 CART 算法的一个变体，它是机器学习的基石之一。所有定义（基尼不纯度、信息增益）都是标准的。该问题在科学上是合理的。\n- **适定性：** 所有关于树构建的规则，包括分裂、停止和打破平局，都得到了明确且无歧义的定义。这确保了对于任何给定的输入，都会产生一个唯一的、确定性的树结构。该问题是适定的。\n- **客观性：** 问题以形式化的数学语言陈述，没有主观性或歧义。\n- **完整性和一致性：** 每个测试用例都提供了所有必要的参数（$X$, $y$, $D_{\\max}$, $m_{\\min}$, $j^\\star$）。规则，包括对受约束根节点的特殊条件，是内部一致的，并覆盖了所有场景。\n\n**结论：** 该问题被认为是**有效的**。它是一个严谨且定义明确的算法问题。我现在将着手设计解决方案。\n\n### **解决方案设计**\n\n解决方案的核心是一个构建决策树的递归算法。为了清晰性和模块化，该算法将被封装在 Python 的类结构中。\n\n**1. 数据结构：** 一个 `Node` 类将代表树中的每个节点，存储其深度、是否为叶节点、其预测值（如果是叶节点）、其分裂参数（如果是内部节点），以及对其子节点的引用。\n\n**2. 递归树构建 (`_build_tree`):** 该函数接收一组样本`indices`和当前`depth`作为输入，并返回一个`Node`。\n- **基本情况（叶节点创建）：** 如果满足任何停止标准，递归就会终止并创建一个叶节点：深度达到 $D_{\\max}$、样本数量太少（$<2$），或者节点是纯的（所有样本都属于同一类别，这意味着没有分裂可以产生正的不纯度下降）。如果没有找到具有正下降量的有效分裂，也会创建一个叶节点。叶节点的预测由多数类决定，出现平局时预测类别 0。\n- **递归步骤（内部节点创建）：** 该函数通过遍历所有特征和所有可选阈值来搜索最佳分裂。对于每个潜在的分裂，它会检查其有效性（子节点大小 $\\ge m_{\\min}$），计算基尼不纯度下降量 $\\Delta$，并更新到目前为止找到的最佳分裂。打破平局的规则（最小的特征索引，然后是最小的阈值）通过迭代的顺序隐式处理。如果找到了一个 $\\Delta > 0$ 的分裂，则创建一个新的内部节点，并使用最佳分裂参数。然后对左、右子节点的索引子集递归调用 `_build_tree` 函数，深度加一。\n\n**3. 处理受约束的根节点：** `fit` 方法将接受一个可选的 `j_star` 参数。如果提供了 `j_star`，根节点的分裂逻辑将被修改：\n- **第一遍：** 首先尝试在特征 $j^\\star$ 上找到满足 $m_{\\min}$ 约束的最佳分裂。\n- **第二遍（回退）：** 如果不存在这样的分裂，它会再次在特征 $j^\\star$ 上搜索仅产生非空子节点的最佳分裂，并忽略 $m_{\\min}$。\n- 如果在 $j^\\star$ 上不可能进行分裂（例如，特征值为常数），或者最佳可能分裂的 $\\Delta \\le 0$，则根节点成为一个叶节点。\n\n**4. 输出指标的计算：** 树构建完成后，使用辅助方法来计算所需的指标：\n- **深度：** 通过递归遍历找到所有叶节点中的最大深度。\n- **误分类率：** 树为训练集中的每个样本预测一个类别，错误率是错误预测的比例。\n- **前序序列：** 递归的前序遍历从所有内部节点收集 `(split_feature, split_threshold)` 对。\n\n主函数 `solve` 将为每个测试用例执行这整个过程，构建无约束树和根节点约束树，计算定义的指标，并严格按照规定格式化结果。",
            "answer": "```python\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, depth):\n        self.depth = depth\n        self.is_leaf = False\n        self.prediction = None\n        self.split_feature = None\n        self.split_threshold = None\n        self.left_child = None\n        self.right_child = None\n\nclass DecisionTree:\n    \"\"\"A binary decision tree classifier.\"\"\"\n    def __init__(self, D_max, m_min):\n        self.D_max = D_max\n        self.m_min = m_min\n        self.root = None\n        self.X = None\n        self.y = None\n\n    def _gini(self, indices):\n        \"\"\"Calculates Gini impurity for a given set of sample indices.\"\"\"\n        n_samples = len(indices)\n        if n_samples == 0:\n            return 0.0\n        \n        y_subset = self.y[indices]\n        _, counts = np.unique(y_subset, return_counts=True)\n        proportions = counts / n_samples\n        return 1.0 - np.sum(proportions**2)\n\n    def _find_best_split(self, indices):\n        \"\"\"Finds the best valid split for a node.\"\"\"\n        n_samples = len(indices)\n        if n_samples < 2:\n            return None, -1.0\n        \n        parent_gini = self._gini(indices)\n        best_decrease = -1.0\n        best_split = None\n        n_features = self.X.shape[1]\n        \n        for j in range(n_features):\n            feature_values = self.X[indices, j]\n            unique_values = np.unique(feature_values)\n            \n            if len(unique_values) <= 1:\n                continue\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            \n            for t in thresholds:\n                left_indices = indices[np.where(self.X[indices, j] <= t)[0]]\n                right_indices = indices[np.where(self.X[indices, j] > t)[0]]\n                \n                if len(left_indices) < self.m_min or len(right_indices) < self.m_min:\n                    continue\n                \n                p_left = len(left_indices) / n_samples\n                p_right = len(right_indices) / n_samples\n                decrease = parent_gini - (p_left * self._gini(left_indices) + p_right * self._gini(right_indices))\n                \n                if decrease > best_decrease:\n                    best_decrease = decrease\n                    best_split = (j, t, left_indices, right_indices)\n        \n        return best_split, best_decrease\n\n    def _find_constrained_root_split(self, indices, j_star):\n        \"\"\"Finds the best split for the root node, constrained to a single feature.\"\"\"\n        n_samples = len(indices)\n        parent_gini = self._gini(indices)\n        feature_values = self.X[indices, j_star]\n        unique_values = np.unique(feature_values)\n        \n        if len(unique_values) <= 1:\n            return None, -1.0\n\n        thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n        \n        # Pass 1: Try to find a split respecting m_min\n        best_decrease_valid = -1.0\n        best_split_valid = None\n        for t in thresholds:\n            left_indices = indices[np.where(self.X[indices, j_star] <= t)[0]]\n            right_indices = indices[np.where(self.X[indices, j_star] > t)[0]]\n\n            if len(left_indices) < self.m_min or len(right_indices) < self.m_min: continue\n            \n            p_left = len(left_indices) / n_samples\n            p_right = len(right_indices) / n_samples\n            decrease = parent_gini - (p_left * self._gini(left_indices) + p_right * self._gini(right_indices))\n            \n            if decrease > best_decrease_valid:\n                best_decrease_valid = decrease\n                best_split_valid = (j_star, t, left_indices, right_indices)\n        \n        if best_split_valid is not None:\n            return best_split_valid, best_decrease_valid\n\n        # Pass 2: Relax m_min constraint, require only non-empty children\n        best_decrease_nonempty = -1.0\n        best_split_nonempty = None\n        for t in thresholds:\n            left_indices = indices[np.where(self.X[indices, j_star] <= t)[0]]\n            right_indices = indices[np.where(self.X[indices, j_star] > t)[0]]\n\n            if len(left_indices) == 0 or len(right_indices) == 0: continue\n            \n            p_left = len(left_indices) / n_samples\n            p_right = len(right_indices) / n_samples\n            decrease = parent_gini - (p_left * self._gini(left_indices) + p_right * self._gini(right_indices))\n            \n            if decrease > best_decrease_nonempty:\n                best_decrease_nonempty = decrease\n                best_split_nonempty = (j_star, t, left_indices, right_indices)\n\n        if best_split_nonempty is not None:\n             return best_split_nonempty, best_decrease_nonempty\n        \n        return None, -1.0\n\n    def _build_tree(self, indices, depth, is_constrained_root=False, j_star=None):\n        \"\"\"Recursively builds the tree.\"\"\"\n        node = Node(depth)\n        n_samples = len(indices)\n        y_subset = self.y[indices]\n        \n        # Leaf conditions\n        is_leaf = False\n        if depth >= self.D_max or n_samples < 2 or len(np.unique(y_subset)) == 1:\n            is_leaf = True\n        \n        if not is_leaf:\n            best_split, best_decrease = None, -1.0\n            if is_constrained_root:\n                best_split, best_decrease = self._find_constrained_root_split(indices, j_star)\n            else:\n                best_split, best_decrease = self._find_best_split(indices)\n            \n            if best_decrease is None or best_decrease <= 0:\n                is_leaf = True\n            else:\n                node.split_feature, node.split_threshold, left_indices, right_indices = best_split\n                node.left_child = self._build_tree(left_indices, depth + 1)\n                node.right_child = self._build_tree(right_indices, depth + 1)\n        \n        if is_leaf:\n            node.is_leaf = True\n            classes, counts = np.unique(y_subset, return_counts=True)\n            if len(classes) == 0:\n                node.prediction = 0\n            elif len(classes) == 1:\n                node.prediction = classes[0]\n            else: # Tie-breaking for prediction\n                if counts[0] == counts[1]:\n                    node.prediction = classes[np.where(classes == 0)[0][0]]\n                else:\n                    node.prediction = classes[np.argmax(counts)]\n        \n        return node\n    \n    def fit(self, X, y, j_star=None):\n        self.X = X\n        self.y = y\n        n_samples = X.shape[0]\n        initial_indices = np.arange(n_samples, dtype=int)\n        \n        self.root = self._build_tree(initial_indices, 0, is_constrained_root=(j_star is not None), j_star=j_star)\n\n    def get_depth(self):\n        if self.root is None: return 0\n        return self._get_depth_recursive(self.root)\n\n    def _get_depth_recursive(self, node):\n        if node.is_leaf: return node.depth\n        return max(self._get_depth_recursive(node.left_child), self._get_depth_recursive(node.right_child))\n        \n    def _predict_single(self, x, node):\n        if node.is_leaf: return node.prediction\n        if x[node.split_feature] <= node.split_threshold:\n            return self._predict_single(x, node.left_child)\n        return self._predict_single(x, node.right_child)\n            \n    def predict(self, X):\n        return np.array([self._predict_single(x, self.root) for x in X])\n\n    def calculate_error(self, X, y):\n        if self.root is None or len(y) == 0: return 0.0\n        y_pred = self.predict(X)\n        return np.sum(y_pred != y) / len(y)\n\n    def get_preorder_sequence(self):\n        sequence = []\n        if self.root is not None:\n            self._get_preorder_recursive(self.root, sequence)\n        return sequence\n\n    def _get_preorder_recursive(self, node, sequence):\n        if not node.is_leaf:\n            sequence.append((node.split_feature, node.split_threshold))\n            self._get_preorder_recursive(node.left_child, sequence)\n            self._get_preorder_recursive(node.right_child, sequence)\n\ndef solve():\n    test_cases = [\n        (np.array([[0.1, 0.3, 0.2], [0.2, 0.1, 0.5], [0.4, 0.6, 0.7], [0.6, 0.2, 0.4], [0.7, 0.8, 0.9], [0.8, 0.1, 0.3], [0.9, 0.5, 0.2], [0.55, 0.4, 0.6]]),\n         np.array([0, 0, 0, 1, 1, 1, 1, 1]), 3, 2, 0),\n        (np.array([[0.2, 0.3, 0.1], [0.1, 0.2, 0.8], [0.4, 0.6, 0.5], [0.35, 0.7, 0.4], [0.5, 0.9, 0.6], [0.45, 0.55, 0.2], [0.3, 0.4, 0.9], [0.25, 0.8, 0.3], [0.6, 0.1, 0.7], [0.7, 0.85, 0.2]]),\n         np.array([0, 0, 1, 1, 1, 1, 0, 1, 0, 1]), 3, 2, 2),\n        (np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0], [0.5, 0.5], [0.5, 0.5]]),\n         np.array([0, 0, 1, 1, 0, 1]), 3, 1, 1)\n    ]\n    \n    all_results_str = []\n    \n    for X, y, D_max, m_min, j_star in test_cases:\n        # Unconstrained tree\n        tree_unc = DecisionTree(D_max, m_min)\n        tree_unc.fit(X, y)\n        d_unc = tree_unc.get_depth()\n        err_unc = tree_unc.calculate_error(X, y)\n        seq_unc = tree_unc.get_preorder_sequence()\n\n        # Constrained-root tree\n        tree_con = DecisionTree(D_max, m_min)\n        tree_con.fit(X, y, j_star=j_star)\n        d_con = tree_con.get_depth()\n        err_con = tree_con.calculate_error(X, y)\n        seq_con = tree_con.get_preorder_sequence()\n        \n        # Constrained root threshold\n        if tree_con.root.is_leaf:\n            root_t_con = -1.0\n        else:\n            root_t_con = tree_con.root.split_threshold\n        \n        # Structural change\n        struct_change = (seq_unc != seq_con)\n        \n        case_results = [\n            d_unc,\n            d_con,\n            f\"{err_unc:.6f}\",\n            f\"{err_con:.6f}\",\n            f\"{root_t_con:.6f}\",\n            struct_change\n        ]\n        \n        all_results_str.append(f\"[{','.join(map(str, case_results))}]\")\n\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        }
    ]
}