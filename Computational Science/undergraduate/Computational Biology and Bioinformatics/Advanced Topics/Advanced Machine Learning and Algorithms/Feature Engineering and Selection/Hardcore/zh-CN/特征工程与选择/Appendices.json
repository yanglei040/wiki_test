{
    "hands_on_practices": [
        {
            "introduction": "从生物序列中工程化特征的最基本方法之一是分析其组成成分。密码子使用偏好，即生物体倾向于使用某些同义密码子而非其他密码子，是一种强有力的生物信号。本练习 () 将提供动手实践，通过实现一个特征来量化基因与其宿主生物体之间密码子使用的差异，从而让你掌握这一信号的量化方法。这在病毒基因组学和水平基因转移检测等领域是一项常用技术。",
            "id": "2389797",
            "problem": "给定一个偏差特征的形式化定义，该特征量化了一个基因的密码子使用与其宿主生物平均密码子使用的差异程度。设密码子字母表为脱氧核糖核酸 (DNA) 字母表 $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 上所有 $64$ 种可能的三联体集合，并设终止密码子集合为 $C_{\\mathrm{stop}}=\\{\\text{TAA}, \\text{TAG}, \\text{TGA}\\}$。定义有义密码子集合 $C$ 为所有DNA三联体的集合除去终止密码子集合 $C_{\\mathrm{stop}}$，该集合包含 $61$ 个密码子。对于任何长度为 $3$ 的倍数的DNA序列，其密码子化是指从左到右分解为不重叠的三联体 $(c_1,c_2,\\dots,c_n)$，其中 $n$ 是密码子的数量。\n\n对于任何密码子化的序列 $S$（测试套件中不含终止密码子），其在 $C$ 上的经验密码子使用分布 $p_S$ 定义为\n$$\np_S(c) = \\frac{\\#\\{i \\in \\{1,\\dots,n\\}: c_i = c\\}}{n} \\quad \\text{对于每个 } c \\in C,\n$$\n其中 $n$ 是 $S$ 中密码子的总数，$\\#\\{\\cdot\\}$ 表示计数。对于一个宿主编码序列的多重集 $H=\\{H_1,H_2,\\dots,H_m\\}$，通过汇总所有宿主序列的计数并按密码子总数进行归一化，定义宿主的合并平均密码子使用分布 $q_H$ 如下：\n$$\nq_H(c) = \\frac{\\sum_{j=1}^{m} \\#\\{i: (H_j)_i = c\\}}{\\sum_{j=1}^{m} n_j} \\quad \\text{对于每个 } c \\in C,\n$$\n其中 $n_j$ 是 $H_j$ 中的密码子数量。要为一个基因序列 $G$ 计算的偏差特征是其密码子使用与宿主合并使用之间的 $\\ell_1$ 距离，\n$$\nD(G,H) = \\sum_{c \\in C} \\left| p_G(c) - q_H(c) \\right|.\n$$\n\n测试套件中的所有序列都是长度可被 $3$ 整除的大写DNA字符串，且均不包含终止密码子。密码子化必须使用从位置 $1$ 开始的阅读框，采用不重叠的三联体。使用上文定义的有义密码子集合 $C$。您的程序必须完全按照定义计算 $D(G,H)$，并且输出结果必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 宿主编码集 $H$ 由以下三个序列组成：\n  - $H_1 = \\text{ATGGCTGCTGGTGGCATGGCCGCTGGT}$，\n  - $H_2 = \\text{GCTGGTGCTGGCATGGCTGCCGGT}$，\n  - $H_3 = \\text{ATGGCTGGTGCTGCTGGC}$。\n- 待评估的四个候选基因序列 $G$（针对同一宿主 $H$）：\n  1. $G_1 = \\text{ATGGCTGCTGGTGGCATGGCCGCTGGTGCTGGTGCTGGCATGGCTGCCGGTATGGCTGGTGCTGCTGGC}$，\n  2. $G_2 = \\text{TTTTTCTTATTGAAAAAGAACAATCAACAC}$，\n  3. $G_3 = \\text{ATGGCTGGTGCTAAGGCTTACGGCGCCTTT}$，\n  4. $G_4 = \\text{ATG}$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，包含四个结果 $[D(G_1,H),D(G_2,H),D(G_3,H),D(G_4,H)]$，形式为逗号分隔的十进制数列表，四舍五入到 $6$ 位小数，并用方括号括起来，不含空格。例如，格式必须与 $[\\dots]$ 完全一致。\n- 不涉及任何物理单位或角度单位。所有值均为无量纲实数。\n\n覆盖说明：\n- 测试套件包括一个通用案例、一个精确匹配案例、一个不相交支持集边界案例以及一个最小长度基因案例。答案必须是四舍五入到 $6$ 位小数的十进制数。",
            "solution": "解决此问题的核心方法是分步计算偏差特征 $D(G,H)$。该方法包括三个主要步骤：\n1.  建立有义密码子的规范表示。\n2.  计算宿主的合并平均密码子使用分布，记为 $q_H$。\n3.  对每个候选基因序列 $G$，计算其经验密码子使用分布 $p_G$，然后计算偏差特征 $D(G,H) = \\sum_{c \\in C} |p_G(c) - q_H(c)|$。\n\n首先，我们定义有义密码子集合 $C$。这是所有 $4^3=64$ 种可能的DNA三联体集合，排除了三个标准的终止密码子：$\\mathrm{TAA}$、$\\mathrm{TAG}$ 和 $\\mathrm{TGA}$。这样剩下 $|C|=61$ 个有义密码子。为便于计算，我们建立一个包含这 $61$ 个密码子的固定有序列表，以及一个从每个密码子字符串到唯一索引的映射。这使我们能够将密码子使用分布表示为 $\\mathbb{R}^{61}$ 中的向量。\n\n其次，我们计算宿主生物的分布 $q_H$。问题将其定义为合并平均使用。计算方法是首先将所有宿主序列 $\\{H_1, H_2, \\dots, H_m\\}$ 连接成一个大的序列。然后，我们统计这个合并序列中 $61$ 个有义密码子中每个密码子的出现次数。通过将这些计数除以合并序列中的密码子总数进行归一化，得到分布 $q_H$。\n给定的宿主集是 $H = \\{H_1, H_2, H_3\\}$，其中：\n$H_1 = \\text{ATGGCTGCTGGTGGCATGGCCGCTGGT}$ (长度 $27$， $n_1 = 9$ 个密码子)\n$H_2 = \\text{GCTGGTGCTGGCATGGCTGCCGGT}$ (长度 $24$， $n_2 = 8$ 个密码子)\n$H_3 = \\text{ATGGCTGGTGCTGCTGGC}$ (长度 $18$， $n_3 = 6$ 个密码子)\n\n宿主池中的密码子总数为 $N_H = n_1 + n_2 + n_3 = 9 + 8 + 6 = 23$。\n合并的密码子计数通过对每个序列的计数求和来确定：\n- 计数(ATG) = $4$\n- 计数(GCT) = $9$\n- 计数(GGT) = $5$\n- 计数(GGC) = $3$\n- 计数(GCC) = $2$\n所有其他密码子的计数为 $0$。\n宿主分布 $q_H$ 是一个向量，其中每个密码子 $c$ 的分量为 $q_H(c) = \\text{计数}(c) / N_H$。例如，$q_H(\\text{ATG}) = 4/23$。\n\n第三，对于每个候选基因 $G_k$ ($k=1,2,3,4$)，我们计算其经验密码子使用分布 $p_{G_k}$。这包括对基因序列进行密码子化，统计每个有义密码子的出现次数，并按基因的密码子总数 $n_{G_k}$ 进行归一化。\n\n最后，偏差特征 $D(G_k, H)$ 是两个概率分布向量 $p_{G_k}$ 和 $q_H$ 之间的 $\\ell_1$ 距离。\n$D(G_k, H) = \\| p_{G_k} - q_H \\|_1 = \\sum_{i=1}^{61} |(p_{G_k})_i - (q_H)_i|$。\n\n让我们将此方法应用于四个测试案例。\n\n**案例 1: $G_1$**\n$G_1$ 是 $H_1, H_2, H_3$ 的串联。因此，其密码子组成与合并的宿主组成相同。密码子数量为 $n_{G_1} = 23$。因此，分布 $p_{G_1}$ 与 $q_H$ 完全相同。\n$D(G_1, H) = \\sum_{c \\in C} |q_H(c) - q_H(c)| = 0$。\n\n**案例 2: $G_2 = \\text{TTTTTCTTATTGAAAAAGAACAATCAACAC}$**\n该基因有 $n_{G_2} = 10$ 个密码子：{TTT, TTC, TTA, TTG, AAA, AAG, AAC, AAT, CAA, CAC}，每个出现一次。因此，对于这 $10$ 个密码子中的每一个 $c$，$p_{G_2}(c) = 1/10$。$G_2$ 中的密码子集合与宿主池 $H$ 中存在的密码子集合不相交。当两个概率分布具有不相交的支持集时，它们的 $\\ell_1$ 距离达到最大值。\n$D(G_2, H) = \\sum_{c \\in \\text{codons}(G_2)} |p_{G_2}(c) - 0| + \\sum_{c \\in \\text{codons}(H)} |0 - q_H(c)| = \\sum p_{G_2}(c) + \\sum q_H(c) = 1 + 1 = 2$。\n\n**案例 3: $G_3 = \\text{ATGGCTGGTGCTAAGGCTTACGGCGCCTTT}$**\n该基因有 $n_{G_3} = 10$ 个密码子。密码子计数为：ATG(1), GCT(3), GGT(1), AAG(1), TAC(1), GGC(1), GCC(1), TTT(1)。通过将这些计数除以 $10$ 来找到分布 $p_{G_3}$。然后我们计算 $\\sum_{c \\in C} |p_{G_3}(c) - q_H(c)|$。\n非零项来自存在于 $G_3$ 或 $H$ 中的密码子。\n- $|p_{G_3}(\\text{ATG}) - q_H(\\text{ATG})| = |1/10 - 4/23| = |23/230 - 40/230| = 17/230$\n- $|p_{G_3}(\\text{GCT}) - q_H(\\text{GCT})| = |3/10 - 9/23| = |69/230 - 90/230| = 21/230$\n- $|p_{G_3}(\\text{GGT}) - q_H(\\text{GGT})| = |1/10 - 5/23| = |23/230 - 50/230| = 27/230$\n- $|p_{G_3}(\\text{GGC}) - q_H(\\text{GGC})| = |1/10 - 3/23| = |23/230 - 30/230| = 7/230$\n- $|p_{G_3}(\\text{GCC}) - q_H(\\text{GCC})| = |1/10 - 2/23| = |23/230 - 20/230| = 3/230$\n仅存在于 $G_3$ 中的密码子项：\n- $|p_{G_3}(\\text{AAG}) - 0| = 1/10 = 23/230$\n- $|p_{G_3}(\\text{TAC}) - 0| = 1/10 = 23/230$\n- $|p_{G_3}(\\text{TTT}) - 0| = 1/10 = 23/230$\n总和为 $(17+21+27+7+3+23+23+23)/230 = 144/230 = 72/115 \\approx 0.626087$。\n\n**案例 4: $G_4 = \\text{ATG}$**\n该基因有 $n_{G_4} = 1$ 个密码子，即 ATG。其分布为 $p_{G_4}(\\text{ATG}) = 1$，对所有其他密码子 $p_{G_4}(c) = 0$。\n距离为 $D(G_4, H) = |p_{G_4}(\\text{ATG}) - q_H(\\text{ATG})| + \\sum_{c \\neq \\text{ATG}} |0 - q_H(c)|$。\n这等于 $|1 - 4/23| + (\\sum_{c \\in C} q_H(c) - q_H(\\text{ATG})) = 19/23 + (1 - 4/23) = 19/23 + 19/23 = 38/23 \\approx 1.652174$。\n\n实现将以编程方式对所有 $61$ 个密码子执行这些计算，以确保正确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the codon usage deviation feature for a set of genes against a host.\n    \"\"\"\n    # Define test cases as per the problem statement.\n    host_sequences = [\n        \"ATGGCTGCTGGTGGCATGGCCGCTGGT\",\n        \"GCTGGTGCTGGCATGGCTGCCGGT\",\n        \"ATGGCTGGTGCTGCTGGC\",\n    ]\n    gene_sequences = [\n        \"ATGGCTGCTGGTGGCATGGCCGCTGGTGCTGGTGCTGGCATGGCTGCCGGTATGGCTGGTGCTGCTGGC\",\n        \"TTTTTCTTATTGAAAAAGAACAATCAACAC\",\n        \"ATGGCTGGTGCTAAGGCTTACGGCGCCTTT\",\n        \"ATG\",\n    ]\n\n    # Step 1: Establish a canonical representation of sense codons.\n    bases = ['A', 'C', 'G', 'T']\n    all_codons = [b1 + b2 + b3 for b1 in bases for b2 in bases for b3 in bases]\n    stop_codons = {\"TAA\", \"TAG\", \"TGA\"}\n    sense_codons = sorted([c for c in all_codons if c not in stop_codons])\n    codon_to_idx = {codon: i for i, codon in enumerate(sense_codons)}\n    num_sense_codons = len(sense_codons)\n\n    def get_codon_distribution(sequence: str) -> np.ndarray:\n        \"\"\"\n        Calculates the empirical codon usage distribution for a given DNA sequence.\n        \n        Args:\n            sequence: A DNA sequence string with length divisible by 3.\n\n        Returns:\n            A numpy array representing the probability distribution over sense codons.\n        \"\"\"\n        counts = np.zeros(num_sense_codons, dtype=np.float64)\n        num_codons = len(sequence) // 3\n\n        if num_codons == 0:\n            return counts\n\n        for i in range(0, len(sequence), 3):\n            codon = sequence[i:i+3]\n            if codon in codon_to_idx:\n                counts[codon_to_idx[codon]] += 1\n        \n        return counts / num_codons\n\n    # Step 2: Compute the host's pooled average codon usage distribution q_H.\n    pooled_host_sequence = \"\".join(host_sequences)\n    q_H = get_codon_distribution(pooled_host_sequence)\n\n    results = []\n    # Step 3: For each gene, compute its distribution p_G and the deviation D(G, H).\n    for gene_seq in gene_sequences:\n        # Calculate p_G for the current gene.\n        p_G = get_codon_distribution(gene_seq)\n        \n        # Calculate the L1 distance (deviation feature).\n        # D(G,H) = sum |p_G(c) - q_H(c)|\n        deviation = np.sum(np.abs(p_G - q_H))\n        \n        results.append(deviation)\n\n    # Format the final output as a comma-separated list of decimals rounded to 6 places.\n    # The format \"{:.6f}\" handles rounding and ensures 6 decimal places are shown.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "特征工程不仅限于对原始数据进行直接计算。我们还可以将统计模型的参数定义为特征，这些模型能够捕捉数据内部的关系。本练习 () 探讨了上位性（epistasis），即基因间的相互作用，这是一个超越简单累加效应的关键遗传学概念。你将通过拟合一个逻辑回归模型并提取其交互作用项的系数，来学习如何工程化一个代表上位性相互作用强度的特征。",
            "id": "2389799",
            "problem": "您正在构建一个二分类器，根据脱氧核糖核酸 (DNA) 序列来预测一个基因组片段是否为增强子。每个样本都是一个固定长度为 $L$ 的 DNA 序列，其字母表为 $\\Sigma=\\{A,C,G,T\\}$。考虑两个为同一个线性分类器提供输入的特征管道。\n\n管道 $\\mathrm{O}$ (独热编码)：将每个位置 $i\\in\\{1,\\dots,L\\}$ 映射到一个 $\\{0,1\\}^{4}$ 中的独热向量，以表示该位置的核苷酸，然后沿各位置拼接以获得一个特征向量 $\\phi_{\\mathrm{OH}}(s)\\in\\{0,1\\}^{4L}$。线性模型为参数 $w\\in\\mathbb{R}^{4L}$ 和 $b\\in\\mathbb{R}$ 计算一个分数 $f_{\\mathrm{O}}(s)=w^{\\top}\\phi_{\\mathrm{OH}}(s)+b$。\n\n管道 $\\mathrm{E}$ (学习的 $k$-mer 嵌入)：将 $s$ 以步长为 1 分词为所有重叠的 $k$-mer，即 $m_t=s_{t:t+k-1}$，其中 $t\\in\\{1,\\dots,T\\}$ 且 $T=L-k+1$。使用一个从大型未标记基因组语料库中学习到的预训练嵌入 $e:\\Sigma^{k}\\to\\mathbb{R}^{d}$ (例如，通过 $k$-mer 共现进行类似 dna2vec 的训练)。通过均值聚合成一个固定长度的向量 $x(s)=\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)\\in\\mathbb{R}^{d}$。线性模型为参数 $u\\in\\mathbb{R}^{d}$ 和 $c\\in\\mathbb{R}$ 计算一个分数 $f_{\\mathrm{E}}(s)=u^{\\top}x(s)+c$。\n\n假设 $k\\ge 2$, $d\\ll 4L$，并且两个管道使用包含 $N$ 个标记序列的相同训练集和相同的线性学习器。回答以下关于这些管道的表征和统计属性的多项选择题。选择所有正确的选项。\n\nA. 因为 $f_{\\mathrm{E}}(s)=\\frac{1}{T}\\sum_{t=1}^{T}u^{\\top}e(m_t)+c$ 仅取决于 $s$ 中的 $k$-mer 多重集，而与其位置无关，所以带有线性分类器的管道 $\\mathrm{E}$ 可以实现一个位置无关的信息性 $k$-mer 内容检测器（一个 $k$-mer 袋模型）。相比之下，带有线性分类器的管道 $\\mathrm{O}$ 在没有显式交互特征的情况下无法检测 $k$-mer 模式，因为 $f_{\\mathrm{O}}(s)$ 是独立的、按位置计的核苷酸贡献之和。\n\nB. 如果 $d\\ll 4L$ 且 $N$ 有限，管道 $\\mathrm{E}$ 通过将自由线性参数的数量从 $4L$ 减少到 $d$，通常比管道 $\\mathrm{O}$ 提供方差更低的学习，从而在其他条件相当的情况下提高样本效率。\n\nC. 在测试时将任何未见过的 $k$-mer 映射到 $\\mathbb{R}^{d}$ 中的零向量，可以确保平均嵌入空间中序列之间的两两距离在常数缩放范围内保持不变，因此这种词汇表外 (out-of-vocabulary) 处理不会扭曲序列间的相对相似性。\n\nD. 因为嵌入 $k$-mer 的均值是通过一个固定变换得到的、关于按位置计的独热编码核苷酸的线性函数，所以作用于 $x(s)$ 的线性分类器可以复现任何作用于 $\\phi_{\\mathrm{OH}}(s)$ 的特定位置线性评分函数 $f_{\\mathrm{O}}(s)$。因此，对于线性模型，管道 $\\mathrm{E}$ 在表征能力上严格优于管道 $\\mathrm{O}$。\n\nE. 不同 $k$-mer 的数量以 $4^{k}$ 的速度增长，但嵌入将它们映射到 $\\mathbb{R}^{d}$ 中，其中 $d\\ll 4^{k}$。这种压缩可以作为一种正则化形式，将具有相似上下文的罕见 $k$-mer 关联到相近的向量中，这在训练集中许多 $k$-mer 不常见时可能会改善泛化能力。",
            "solution": "此问题要求我们为给定的二元表型数据和两个基因型特征拟合一个逻辑回归模型，并提取交互项的系数 $\\beta_3$ 作为工程化特征。该模型在遗传学中常用于量化上位性效应。\n\n问题的核心是为指定的逻辑回归模型找到参数 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2, \\beta_3]^T$ 的最大似然估计（MLE）。模型中，$y_i=1$ 的概率由 sigmoid 函数 $\\sigma(\\eta_i)$ 给出：\n$$ p_i = \\mathbb{P}(y_i=1 \\mid \\mathbf{x}_i, \\boldsymbol{\\beta}) = \\sigma(\\eta_i) = \\frac{1}{1 + e^{-\\eta_i}} $$\n其中线性预测器 $\\eta_i = \\mathbf{x}_i^T \\boldsymbol{\\beta}$，设计向量 $\\mathbf{x}_i = [1, S_{1,i}, S_{2,i}, S_{1,i} \\cdot S_{2,i}]^T$。\n\n最大化对数似然函数 $\\ell(\\boldsymbol{\\beta})$ 等价于最小化负对数似然函数。对于逻辑回归，该函数是凸的，保证了在数据非完全可分的情况下存在唯一的最小值。\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i (\\mathbf{x}_i^T \\boldsymbol{\\beta}) - \\log(1 + e^{\\mathbf{x}_i^T \\boldsymbol{\\beta}}) \\right] $$\n由于没有闭式解，我们需要使用数值优化方法。一个标准高效的算法是诸如BFGS之类的拟牛顿法，可以通过 `scipy.optimize.minimize` 实现。为了提高效率和准确性，我们将同时提供目标函数（负对数似然）及其梯度（雅可比矩阵）：\n$$ \\nabla_{\\boldsymbol{\\beta}} (-\\ell(\\boldsymbol{\\beta})) = \\sum_{i=1}^n (p_i - y_i) \\mathbf{x}_i = \\mathbf{X}^T (\\mathbf{p} - \\mathbf{y}) $$\n其中 $\\mathbf{X}$ 是设计矩阵，$\\mathbf{y}$ 是结果向量，$\\mathbf{p}$ 是概率向量。\n\n我们将按以下方式处理每个案例：\n\n**案例 A**:\n数据集包含 $n=20$ 个独立样本。我们直接根据给定数据构建 $20 \\times 4$ 的设计矩阵 $\\mathbf{X}$ 和 $20 \\times 1$ 的响应向量 $\\mathbf{y}$。$\\mathbf{X}$ 的四列分别对应截距项($1$)、$S_1$、$S_2$ 以及交互项 $S_1 \\cdot S_2$。然后，我们使用初始猜测值 $\\boldsymbol{\\beta} = \\mathbf{0}$ 进行数值优化，找到MLE向量 $\\hat{\\boldsymbol{\\beta}}$。所需的特征即为该向量的第四个分量 $\\hat{\\beta}_3$。\n\n**案例 B**:\n数据集包含 $n=40$ 个样本，并按 $(S_1, S_2)$ 的组合进行了分组。这种结构允许我们使用更高效的对数似然公式。令 $j$ 为四个唯一组的索引，$N_j$ 为第 $j$ 组的个体总数，$k_j$ 为该组中表型为阳性（$y=1$）的个体数。对数似然函数可以写为：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{j=1}^4 \\left[ k_j (\\mathbf{x}_j^T \\boldsymbol{\\beta}) - N_j \\log(1 + e^{\\mathbf{x}_j^T \\boldsymbol{\\beta}}) \\right] $$\n其梯度也相应地进行聚合。我们构建 $4 \\times 4$ 的唯一设计向量矩阵以及对应的 $N_j$ 和 $k_j$ 向量，然后像案例 A 一样进行优化，并提取 $\\hat{\\beta}_3$。\n\n**案例 C**:\n数据集中所有个体的 $S_{2,i}$ 值均为 $0$。因此，交互项 $S_{1,i} \\cdot S_{2,i}$ 也恒为 $0$。这导致设计矩阵中对应于系数 $\\beta_3$ 的列是一个零向量。此时，$\\beta_3$ 的值无法被唯一确定，因为任何 $\\beta_3$ 值都会产生相同的似然。问题陈述为此情况提供了明确的约定：将工程化的交互特征定义为 $0$。因此，本案例的解根据定义为 $\\hat{\\beta}_3 = 0$，无需计算。\n\n实现将使用 `scipy.optimize.minimize` 和 'BFGS' 方法，并利用 `scipy.special` 模块中的函数（如 `logsumexp` 和 `expit`）来保证数值稳定性。",
            "answer": "```python\nimport numpy as np\nimport scipy.optimize\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression problem for three test cases and computes the\n    engineered interaction feature (MLE of beta_3) for each.\n    \"\"\"\n\n    # --- Case A ---\n    data_A = [\n        (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 1),\n        (1, 0, 0), (1, 0, 0), (1, 0, 1), (1, 0, 0), (1, 0, 1),\n        (0, 1, 0), (0, 1, 1), (0, 1, 0), (0, 1, 0), (0, 1, 1),\n        (1, 1, 1), (1, 1, 1), (1, 1, 0), (1, 1, 1), (1, 1, 1),\n    ]\n    S1_A = np.array([d[0] for d in data_A])\n    S2_A = np.array([d[1] for d in data_A])\n    y_A = np.array([d[2] for d in data_A])\n    X_A = np.c_[np.ones(len(S1_A)), S1_A, S2_A, S1_A * S2_A]\n    \n    beta_A = _fit_logistic_regression(X_A, y_A)\n    beta3_A = beta_A[3]\n\n    # --- Case B ---\n    # The data can be aggregated for efficiency\n    # (S1, S2): {N: total count, k: count with y=1}\n    data_B_grouped = {\n        (0, 0): {'N': 10, 'k': 3},\n        (1, 0): {'N': 10, 'k': 5},\n        (0, 1): {'N': 10, 'k': 5},\n        (1, 1): {'N': 10, 'k': 7},\n    }\n    X_unique_B = []\n    N_B, k_B = [], []\n    for (s1, s2), counts in data_B_grouped.items():\n        X_unique_B.append([1, s1, s2, s1 * s2])\n        N_B.append(counts['N'])\n        k_B.append(counts['k'])\n    \n    X_B_agg = np.array(X_unique_B)\n    N_B_agg = np.array(N_B)\n    k_B_agg = np.array(k_B)\n\n    beta_B = _fit_logistic_regression(X_B_agg, k_B_agg, N=N_B_agg, grouped=True)\n    beta3_B = beta_B[3]\n\n    # --- Case C ---\n    # For this case, S2 is always 0. The interaction term S1*S2 is always 0.\n    # The problem statement defines the engineered feature to be 0 in this scenario.\n    beta3_C = 0.0\n\n    results = [beta3_A, beta3_B, beta3_C]\n    \n    # Format the output as specified\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\n\ndef _fit_logistic_regression(X, y, N=None, grouped=False):\n    \"\"\"\n    Fits a logistic regression model and returns the estimated coefficients.\n    Can handle both individual and grouped data.\n    \"\"\"\n    from scipy.special import expit\n\n    num_features = X.shape[1]\n    beta_initial = np.zeros(num_features)\n\n    if grouped:\n        # For grouped data\n        def objective(beta, X_grp, k_grp, N_grp):\n            eta = X_grp @ beta\n            # Numerically stable calculation of log(1 + exp(eta))\n            log_1_plus_exp_eta = np.logaddexp(0, eta)\n            neg_log_likelihood = -np.sum(k_grp * eta - N_grp * log_1_plus_exp_eta)\n            return neg_log_likelihood\n\n        def jacobian(beta, X_grp, k_grp, N_grp):\n            eta = X_grp @ beta\n            p = expit(eta)\n            grad = X_grp.T @ (N_grp * p - k_grp)\n            return grad\n        \n        args = (X, y, N)\n\n    else:\n        # For individual data\n        def objective(beta, X_ind, y_ind):\n            eta = X_ind @ beta\n            log_1_plus_exp_eta = np.logaddexp(0, eta)\n            neg_log_likelihood = -np.sum(y_ind * eta - log_1_plus_exp_eta)\n            return neg_log_likelihood\n\n        def jacobian(beta, X_ind, y_ind):\n            eta = X_ind @ beta\n            p = expit(eta)\n            grad = X_ind.T @ (p - y_ind)\n            return grad\n        \n        args = (X, y)\n\n    result = scipy.optimize.minimize(\n        fun=objective,\n        x0=beta_initial,\n        args=args,\n        method='BFGS',\n        jac=jacobian\n    )\n\n    if not result.success:\n        raise RuntimeError(f\"Optimization failed: {result.message}\")\n\n    return result.x\n\nsolve()\n\n```"
        },
        {
            "introduction": "如何为机器学习模型表示复杂的生物序列是一个关键挑战，而表示方法的选择是一项核心的特征工程决策。本练习 () 对比了两种强大但概念上截然不同的方法：传统的稀疏独热编码（one-hot encoding）和从大规模数据中学习到的现代密集特征表示（嵌入，embeddings）。理解这些方法在表示能力、统计效率和位置信息处理等方面的权衡，对于构建有效的基因组学预测模型至关重要。",
            "id": "2389823",
            "problem": "您正在构建一个二分类器，根据脱氧核糖核酸 (DNA) 序列来预测一个基因组片段是否为增强子。每个样本都是一个固定长度为 $L$ 的 DNA 序列，其字母表为 $\\Sigma=\\{A,C,G,T\\}$。考虑两个为同一个线性分类器提供输入的特征管道。\n\n管道 $\\mathrm{O}$ (独热编码)：将每个位置 $i\\in\\{1,\\dots,L\\}$ 映射到一个 $\\{0,1\\}^{4}$ 中的独热向量，以表示该位置的核苷酸，然后沿各位置拼接以获得一个特征向量 $\\phi_{\\mathrm{OH}}(s)\\in\\{0,1\\}^{4L}$。线性模型为参数 $w\\in\\mathbb{R}^{4L}$ 和 $b\\in\\mathbb{R}$ 计算一个分数 $f_{\\mathrm{O}}(s)=w^{\\top}\\phi_{\\mathrm{OH}}(s)+b$。\n\n管道 $\\mathrm{E}$ (学习的 $k$-mer 嵌入)：将 $s$ 以步长为 1 分词为所有重叠的 $k$-mer，即 $m_t=s_{t:t+k-1}$，其中 $t\\in\\{1,\\dots,T\\}$ 且 $T=L-k+1$。使用一个从大型未标记基因组语料库中学习到的预训练嵌入 $e:\\Sigma^{k}\\to\\mathbb{R}^{d}$ (例如，通过 $k$-mer 共现进行类似 dna2vec 的训练)。通过均值聚合成一个固定长度的向量 $x(s)=\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)\\in\\mathbb{R}^{d}$。线性模型为参数 $u\\in\\mathbb{R}^{d}$ 和 $c\\in\\mathbb{R}$ 计算一个分数 $f_{\\mathrm{E}}(s)=u^{\\top}x(s)+c$。\n\n假设 $k\\ge 2$, $d\\ll 4L$，并且两个管道使用包含 $N$ 个标记序列的相同训练集和相同的线性学习器。回答以下关于这些管道的表征和统计属性的多项选择题。选择所有正确的选项。\n\nA. 因为 $f_{\\mathrm{E}}(s)=\\frac{1}{T}\\sum_{t=1}^{T}u^{\\top}e(m_t)+c$ 仅取决于 $s$ 中的 $k$-mer 多重集，而与其位置无关，所以带有线性分类器的管道 $\\mathrm{E}$ 可以实现一个位置无关的信息性 $k$-mer 内容检测器（一个 $k$-mer 袋模型）。相比之下，带有线性分类器的管道 $\\mathrm{O}$ 在没有显式交互特征的情况下无法检测 $k$-mer 模式，因为 $f_{\\mathrm{O}}(s)$ 是独立的、按位置计的核苷酸贡献之和。\n\nB. 如果 $d\\ll 4L$ 且 $N$ 有限，管道 $\\mathrm{E}$ 通过将自由线性参数的数量从 $4L$ 减少到 $d$，通常比管道 $\\mathrm{O}$ 提供方差更低的学习，从而在其他条件相当的情况下提高样本效率。\n\nC. 在测试时将任何未见过的 $k$-mer 映射到 $\\mathbb{R}^{d}$ 中的零向量，可以确保平均嵌入空间中序列之间的两两距离在常数缩放范围内保持不变，因此这种词汇表外 (out-of-vocabulary) 处理不会扭曲序列间的相对相似性。\n\nD. 因为嵌入 $k$-mer 的均值是通过一个固定变换得到的、关于按位置计的独热编码核苷酸的线性函数，所以作用于 $x(s)$ 的线性分类器可以复现任何作用于 $\\phi_{\\mathrm{OH}}(s)$ 的特定位置线性评分函数 $f_{\\mathrm{O}}(s)$。因此，对于线性模型，管道 $\\mathrm{E}$ 在表征能力上严格优于管道 $\\mathrm{O}$。\n\nE. 不同 $k$-mer 的数量以 $4^{k}$ 的速度增长，但嵌入将它们映射到 $\\mathbb{R}^{d}$ 中，其中 $d\\ll 4^{k}$。这种压缩可以作为一种正则化形式，将具有相似上下文的罕见 $k$-mer 关联到相近的向量中，这在训练集中许多 $k$-mer 不常见时可能会改善泛化能力。",
            "solution": "问题陈述提出了一个有效且定义明确的问题，比较了计算生物学中用于线性分类器的两种标准特征工程管道。管道 O（独热编码）和管道 E（预训练的 $k$-mer 嵌入）的定义是精确的、有科学依据的，并且允许对其各自的属性进行严格分析。我们将逐一评估每个选项。\n\n管道 O 的评分函数由 $f_{\\mathrm{O}}(s)=w^{\\top}\\phi_{\\mathrm{OH}}(s)+b$ 给出。特征向量 $\\phi_{\\mathrm{OH}}(s) \\in \\{0,1\\}^{4L}$ 是 $L$ 个独热向量的拼接，每个向量对应序列 $s$ 中的一个位置。设 $\\phi_i(s_i) \\in \\{0,1\\}^4$ 为位置 $i$ 处核苷酸 $s_i$ 的独热向量。权重向量 $w \\in \\mathbb{R}^{4L}$ 可以被划分为 $L$ 个向量 $w_i \\in \\mathbb{R}^4$，每个向量对应一个位置。那么分数可以写成：\n$$f_{\\mathrm{O}}(s) = \\sum_{i=1}^{L} w_i^{\\top}\\phi_i(s_i) + b$$\n该模型在位置上是线性和可加的。它学习了对每个核苷酸的位置特异性偏好，但无法捕捉不同位置核苷酸之间的任何依赖关系或相互作用。这等价于一个位置权重矩阵 (PWM) 模型。\n\n管道 E 的评分函数是 $f_{\\mathrm{E}}(s)=u^{\\top}x(s)+c$，其中 $x(s)=\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)$ 是序列中所有重叠 $k$-mer $m_t$ 的预训练嵌入向量的均值。$T = L-k+1$ 是此类 $k$-mer 的数量。根据点积的线性性质，分数为：\n$$f_{\\mathrm{E}}(s) = u^{\\top}\\left(\\frac{1}{T}\\sum_{t=1}^{T}e(m_t)\\right)+c = \\frac{1}{T}\\sum_{t=1}^{T} u^{\\top}e(m_t) + c$$\n项 $u^{\\top}e(m_t)$ 可以被看作是 $k$-mer $m_t$ 的一个分数。函数 $f_{\\mathrm{E}}(s)$ 计算序列中所有 $k$-mer 的平均分数。平均操作丢弃了关于 $k$-mer 的所有位置信息。表征 $x(s)$ 仅取决于 $s$ 中存在的 $k$-mer 多重集，而不取决于它们的顺序或位置。\n\n有了这些公式，我们来分析每个选项。\n\n**A. 因为 $f_{\\mathrm{E}}(s)=\\frac{1}{T}\\sum_{t=1}^{T}u^{\\top}e(m_t)+c$ 仅取决于 $s$ 中的 $k$-mer 多重集，而与其位置无关，所以带有线性分类器的管道 $\\mathrm{E}$ 可以实现一个位置无关的信息性 $k$-mer 内容检测器（一个 $k$-mer 袋模型）。相比之下，带有线性分类器的管道 $\\mathrm{O}$ 在没有显式交互特征的情况下无法检测 $k$-mer 模式，因为 $f_{\\mathrm{O}}(s)$ 是独立的、按位置计的核苷酸贡献之和。**\n这个陈述是正确的。第一部分，关于管道 E，直接源于 $x(s)$ 被公式化为 $k$-mer 嵌入的平均值。根据定义，这种聚合方法对 $k$-mer 的位置不敏感，使其成为一个“$k$-mer 袋”模型。第二部分，关于管道 O，也是正确的。分数 $f_{\\mathrm{O}}(s)$ 是指示特定位置上特定核苷酸存在的特征的线性组合。这样的线性模型无法表示逻辑与条件（例如，位置 $i$ 上的核苷酸 $N_1$ 与位置 $j$ 上的核苷酸 $N_2$ 同时存在），而定义长度 $k \\ge 2$ 的模式需要这种条件。要检测一个特定的 $k$-mer，需要一个特征，该特征是相应独热特征的乘积，这是一个在管道 O 的线性模型中不可用的非线性交互项。\n**结论：正确。**\n\n**B. 如果 $d\\ll 4L$ 且 $N$ 有限，管道 $\\mathrm{E}$ 通过将自由线性参数的数量从 $4L$ 减少到 $d$，通常比管道 $\\mathrm{O}$ 提供方差更低的学习，从而在其他条件相当的情况下提高样本效率。**\n这个陈述是正确的。对于管道 O，最终分类层中的可学习参数数量是 $w$ 的维度加上偏置，即 $4L+1$。对于管道 E，由于嵌入 $e$ 是预训练且固定的，可学习参数的数量是 $u$ 的维度加上偏置，即 $d+1$。问题假设 $d \\ll 4L$。在统计学习中，一个在固定大小为 $N$ 的数据集上训练的、参数数量显著较少（复杂度较低）的模型，不太容易过拟合训练数据。这导致所学假设的方差更低。较低的方差通常会提高对未见数据的泛化能力，并意味着模型可以从更少的样本中更有效地学习（提高样本效率）。\n**结论：正确。**\n\n**C. 在测试时将任何未见过的 $k$-mer 映射到 $\\mathbb{R}^{d}$ 中的零向量，可以确保平均嵌入空间中序列之间的两两距离在常数缩放范围内保持不变，因此这种词汇表外 (out-of-vocabulary) 处理不会扭曲序列间的相对相似性。**\n这个陈述是错误的。所提出的词汇表外 (OOV) 策略包括将未见过的 $k$-mer 的嵌入替换为零向量 $\\vec{0} \\in \\mathbb{R}^d$。这会严重扭曲嵌入空间的几何结构。考虑三个分别由单个 $k$-mer $m_A, m_B, m_C$ 组成的序列 $s_A, s_B, s_C$。假设 $m_A, m_B$ 在词汇表中，而 $m_C$ 是一个 OOV $k$-mer。它们的表征分别为 $x(s_A)=e(m_A)$，$x(s_B)=e(m_B)$ 和 $x(s_C)=\\vec{0}$。原始的相对相似性可能意味着 $m_C$ 与 $m_A$ 非常相似但与 $m_B$ 不相似，即距离 $\\|e(m_A) - e(m_C)\\|$ 应该很小，而 $\\|e(m_B) - e(m_C)\\|$ 应该很大。经过 OOV 处理后，新的距离变为 $\\|e(m_A) - \\vec{0}\\| = \\|e(m_A)\\|$ 和 $\\|e(m_B) - \\vec{0}\\| = \\|e(m_B)\\|$。这些值与真实（但未知）的相似性没有保证的关系。这种映射使 OOV $k$-mer 仅仅基于其他 $k$-mer 的向量范数而显得与它们同等相关或不相关，这是一种任意的扭曲。因此，相对相似性没有被保留。\n**结论：错误。**\n\n**D. 因为嵌入 $k$-mer 的均值是通过一个固定变换得到的、关于按位置计的独热编码核苷酸的线性函数，所以作用于 $x(s)$ 的线性分类器可以复现任何作用于 $\\phi_{\\mathrm{OH}}(s)$ 的特定位置线性评分函数 $f_{\\mathrm{O}}(s)$。因此，对于线性模型，管道 $\\mathrm{E}$ 在表征能力上严格优于管道 $\\mathrm{O}$。**\n这个陈述是错误的。它建立在一个错误的前提之上，并得出了一个错误的结论。首先，前提“嵌入 $k$-mer 的均值是按位置计的独热编码核苷酸的线性函数”是错误的。函数 $e:\\Sigma^k \\to \\mathbb{R}^d$ 是一个查找表，它是从核苷酸序列空间出发的一个高度非线性的映射。一个特定的 $k$-mer 是由 $k$ 个相对位置上的 $k$ 个特定核苷酸的组合来识别的。代表这种组合的特征不能表示为单个核苷酸特征的线性组合。其次，结论是错误的。管道 O 实现的是位置敏感的函数，而管道 E（按其构建方式）实现的是位置不敏感的函数。一个位置不敏感的模型通常不能表示一个位置敏感的模型。例如，假设 $s_1$ 是一个在开头有特定基序的序列，而 $s_2$ 是同一个序列但在末尾有该基序。管道 O 中的增强子模型可以轻易区分它们，给出 $f_{\\mathrm{O}}(s_1) \\neq f_{\\mathrm{O}}(s_2)$。而在管道 E 中，如果 $s_1$ 和 $s_2$ 包含完全相同的 $k$-mer 多重集，那么 $x(s_1) = x(s_2)$，因此 $f_{\\mathrm{E}}(s_1) = f_{\\mathrm{E}}(s_2)$。所以，管道 E 无法复现管道 O 假设空间中的任意函数。两个管道都没有严格优于另一个；它们捕获了不同类型的信息。\n**结论：错误。**\n\n**E. 不同 $k$-mer 的数量以 $4^{k}$ 的速度增长，但嵌入将它们映射到 $\\mathbb{R}^{d}$ 中，其中 $d\\ll 4^{k}$。这种压缩可以作为一种正则化形式，将具有相似上下文的罕见 $k$-mer 关联到相近的向量中，这在训练集中许多 $k$-mer 不常见时可能会改善泛化能力。**\n这个陈述是正确的。可能的 $k$-mer 数量 $4^k$ 是巨大的。嵌入将这个高维离散空间投影到一个低维连续空间 $\\mathbb{R}^d$ 中，其中 $d \\ll 4^k$。这种压缩不是随机的，而是结构化的。像 dna2vec 这样的方法学习到的嵌入，会使得出现在相似基因组上下文中的 $k$-mer 被映射到 $\\mathbb{R}^d$ 中的相近向量。这种参数的“捆绑”——即一个 $k$-mer 的表示受到其他 $k$-mer 表示的约束——是一种正则化形式。它允许模型进行泛化。例如，如果一个罕见的 $k$-mer 在标记的训练集中不常出现，但在大型未标记的预训练语料库中，它与一个常见的功能性 $k$-mer 出现在相似的上下文中，那么它们的嵌入就会很接近。然后模型可以将关于常见 $k$-mer 功能作用的已学知识迁移到罕见的 $k$-mer 上，从而提高其对包含该罕见 $k$-mer 的序列的预测性能。这是使用预训练嵌入的一个主要好处。\n**结论：正确。**",
            "answer": "$$\\boxed{ABE}$$"
        }
    ]
}