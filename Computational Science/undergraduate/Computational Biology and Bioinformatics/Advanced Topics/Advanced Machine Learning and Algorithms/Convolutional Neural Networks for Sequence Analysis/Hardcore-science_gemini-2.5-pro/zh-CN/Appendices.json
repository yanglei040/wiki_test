{
    "hands_on_practices": [
        {
            "introduction": "要真正理解卷积神经网络，最好的方法就是亲手构建一个。本练习将引导你完成一个简单CNN从输入到输出的完整计算过程，即“前向传播”。通过实现包括独热编码、卷积、ReLU激活、全局最大池化和最终逻辑回归层的每一个步骤，你将对信息如何在网络中流动并最终形成预测有一个具体而清晰的认识。",
            "id": "2382334",
            "problem": "考虑一个用于脱氧核糖核酸（DNA）序列的二元分类模型，该模型首先使用一个带修正线性单元激活和全局最大池化的单层一维卷积层计算潜在特征向量，然后对该潜在向量应用逻辑回归。设输入为字母表 $\\{A,C,G,T\\}$ 上长度为 $L$ 的 DNA 序列，该序列使用固定的通道顺序 $(A,C,G,T)$ 进行独热编码，转换为矩阵 $X \\in \\{0,1\\}^{L \\times 4}$，因此对于每个位置 $i \\in \\{0,\\dots,L-1\\}$，行 $X_{i,:}$ 是对应于位置 $i$ 处碱基的独热向量。\n\n卷积层有 $K=2$ 个滤波器，每个滤波器的核宽度为 $F=3$，采用有效卷积（无填充）且步幅为 $1$。将滤波器权重表示为 $W \\in \\mathbb{R}^{K \\times F \\times 4}$，偏置表示为 $b \\in \\mathbb{R}^{K}$。对于每个位置 $i \\in \\{0,\\dots,L-F\\}$ 和滤波器索引 $k \\in \\{0,1\\}$，其预激活值为\n$$\nH_{i,k} \\;=\\; \\sum_{j=0}^{F-1} \\sum_{c=0}^{3} W_{k,j,c}\\, X_{i+j,c} \\;+\\; b_k.\n$$\n激活函数为修正线性单元 $R_{i,k} = \\max(0, H_{i,k})$。全局最大池化产生潜在向量 $z \\in \\mathbb{R}^{K}$，其分量为\n$$\nz_k \\;=\\; \\max_{0 \\le i \\le L-F} R_{i,k}.\n$$\n逻辑回归计算正类的概率为\n$$\n\\hat{y} \\;=\\; \\sigma\\!\\left(\\sum_{k=0}^{K-1} u_k z_k + a \\right), \\quad \\text{其中} \\quad \\sigma(t) = \\frac{1}{1+e^{-t}},\n$$\n其中权重为 $u \\in \\mathbb{R}^{K}$，偏置为 $a \\in \\mathbb{R}$。\n\n在所有测试用例中，使用以下固定参数：\n- 通过显式权重和偏置定义的滤波器（通道顺序为 $(A,C,G,T)$）：\n  - 对于滤波器 $k=0$（匹配偏移量为 $0$ 的 $A$、偏移量为 $1$ 的 $C$、偏移量为 $2$ 的 $G$ 的模式）：\n    - $W_{0,0,:} = [\\,1,\\,-1,\\,-1,\\,-1\\,]$,\n    - $W_{0,1,:} = [\\,-1,\\,1,\\,-1,\\,-1\\,]$,\n    - $W_{0,2,:} = [\\,-1,\\,-1,\\,1,\\,-1\\,]$,\n    - $b_0 = -1$.\n  - 对于滤波器 $k=1$（匹配偏移量为 $0,1,2$ 的 $T$ 的模式）：\n    - $W_{1,0,:} = [\\,-1,\\,-1,\\,-1,\\,1\\,]$,\n    - $W_{1,1,:} = [\\,-1,\\,-1,\\,-1,\\,1\\,]$,\n    - $W_{1,2,:} = [\\,-1,\\,-1,\\,-1,\\,1\\,]$,\n    - $b_1 = -1$.\n- 逻辑回归参数：\n  - $u = [\\,1.0,\\,-0.5\\,]$,\n  - $a = -1.0$.\n\n测试套件中的所有序列均满足 $L \\ge F$。对于每个序列，计算独热编码 $X$，然后计算 $H$、$R$、$z$，最后按上述定义计算 $\\hat{y}$。\n\n测试套件：\n- 用例 1：序列 $S_1 = \\text{\"ACGAC\"}$。\n- 用例 2：序列 $S_2 = \\text{\"TTT\"}$。\n- 用例 3：序列 $S_3 = \\text{\"ACGTTT\"}$。\n- 用例 4：序列 $S_4 = \\text{\"GCA\"}$。\n\n您的程序必须输出一行，其中包含四个用例的结果，即四个概率 $[\\hat{y}_1,\\hat{y}_2,\\hat{y}_3,\\hat{y}_4]$ 的逗号分隔列表，每个概率四舍五入到恰好 $6$ 位小数，不含空格，并用方括号括起来。例如，输出的形式应为 $[\\text{float},\\text{float},\\text{float},\\text{float}]$，如 $[0.123456,0.234567,0.345678,0.456789]$。",
            "solution": "问题陈述科学合理、定义明确且完整。它描述了一个用于序列分类的卷积神经网络的标准（尽管简化了）前向传播过程。所有必要的参数和测试用例都已提供，没有歧义或矛盾。我们将继续进行计算。\n\n任务是针对四个给定的 DNA 序列，计算一个小型卷积神经网络的输出概率 $\\hat{y}$。该网络架构由一个一维卷积层、一个修正线性单元（ReLU）激活函数、一个全局最大池化层和一个逻辑回归输出层组成。我们将对每个序列严格遵循定义的数学运算。\n\n首先，我们定义固定的模型参数：\n- 字母表为 $\\{\\text{A, C, G, T}\\}$。独热编码的通道顺序是 $(\\text{A, C, G, T})$。\n- 卷积层：$K=2$ 个滤波器，核宽度 $F=3$，步幅 $1$，无填充。\n- 滤波器 $k=0$ 设计用于检测基序 \"ACG\"。其权重 $W_0$ 和偏置 $b_0$ 如下所示：\n$$W_0 = \\begin{pmatrix} [\\,1,-1,-1,-1\\,] \\\\ [-1,\\,1,-1,-1\\,] \\\\ [-1,-1,\\,1,-1\\,] \\end{pmatrix}, \\quad b_0 = -1$$\n一个完美的 \"ACG\" 匹配产生的预激活值为 $(1 \\times 1 + 1 \\times 1 + 1 \\times 1) + b_0 = 3 - 1 = 2$。\n- 滤波器 $k=1$ 设计用于检测 \"TTT\"。其权重 $W_1$ 和偏置 $b_1$ 为：\n$$W_1 = \\begin{pmatrix} [-1,-1,-1,\\,1\\,] \\\\ [-1,-1,-1,\\,1\\,] \\\\ [-1,-1,-1,\\,1\\,] \\end{pmatrix}, \\quad b_1 = -1$$\n一个完美的 \"TTT\" 匹配产生的预激活值为 $(1 \\times 1 + 1 \\times 1 + 1 \\times 1) + b_1 = 3 - 1 = 2$。\n- 逻辑回归参数为权重 $u = [1.0, -0.5]$ 和偏置 $a = -1.0$。\n\n每个序列的计算按五个步骤进行：\n1.  **独热编码**：将长度为 $L$ 的输入序列 $S$ 转换为一个二元矩阵 $X \\in \\{0, 1\\}^{L \\times 4}$。\n2.  **卷积**：计算预激活矩阵 $H \\in \\mathbb{R}^{(L-F+1) \\times K}$。对于每个滤波器 $k$ 和核的每个可能位置 $i$，$H_{i,k} = \\sum_{j=0}^{F-1} \\sum_{c=0}^{3} W_{k,j,c} X_{i+j,c} + b_k$。\n3.  **ReLU 激活**：非线性激活矩阵为 $R_{i,k} = \\max(0, H_{i,k})$。\n4.  **全局最大池化**：通过取每个滤波器在所有位置上的最大激活值来形成潜在特征向量 $z \\in \\mathbb{R}^K$：$z_k = \\max_i R_{i,k}$。\n5.  **逻辑回归**：最终概率计算为 $\\hat{y} = \\sigma(u \\cdot z + a)$，其中 $\\sigma(t) = 1/(1+e^{-t})$ 是 sigmoid 函数。\n\n我们现在将此过程应用于每个测试用例。\n\n**用例 1：序列 $S_1 = \\text{\"ACGAC\"}$**\n- 长度 $L=5$。宽度为 $F=3$ 的窗口数量为 $5-3+1=3$。这些窗口是：\"ACG\"、\"CGA\"、\"GAC\"。\n- 对于滤波器 $k=0$（ACG 检测器）：\n  - 窗口 \"ACG\" ($i=0$)：完美匹配。$H_{0,0} = 2$。\n  - 窗口 \"CGA\" ($i=1$)：不匹配。$H_{1,0} = (-1-1-1) - 1 = -4$。\n  - 窗口 \"GAC\" ($i=2$)：不匹配。$H_{2,0} = (-1-1+1) - 1 = -2$。\n  - 预激活值 $H_{:,0} = [2, -4, -2]$。经过 ReLU 后，$R_{:,0} = [2, 0, 0]$。\n  - $z_0 = \\max(2, 0, 0) = 2$。\n- 对于滤波器 $k=1$（TTT 检测器）：所有窗口均不匹配，导致预激活值 $H_{0,1}=-4$、$H_{1,1}=-4$、$H_{2,1}=-4$。经过 ReLU 后，所有值都为 $0$。\n  - $z_1 = 0$。\n- 潜在向量 $z = [2, 0]$。\n- 逻辑回归输入：$t_1 = u \\cdot z + a = (1.0 \\times 2) + (-0.5 \\times 0) - 1.0 = 1.0$。\n- 最终概率：$\\hat{y}_1 = \\sigma(1.0) = \\frac{1}{1+e^{-1}} \\approx 0.73105858$。\n\n**用例 2：序列 $S_2 = \\text{\"TTT\"}$**\n- 长度 $L=3$。窗口数量为 $3-3+1=1$。窗口是 \"TTT\"。\n- 对于滤波器 $k=0$（ACG 检测器）：窗口 \"TTT\" 是不匹配项。$H_{0,0} = (-1-1-1)-1=-4$。ReLU 给出 $0$。\n  - $z_0 = 0$。\n- 对于滤波器 $k=1$（TTT 检测器）：窗口 \"TTT\" 是一个完美匹配。$H_{0,1} = 2$。ReLU 给出 $2$。\n  - $z_1 = 2$。\n- 潜在向量 $z = [0, 2]$。\n- 逻辑回归输入：$t_2 = u \\cdot z + a = (1.0 \\times 0) + (-0.5 \\times 2) - 1.0 = -2.0$。\n- 最终概率：$\\hat{y}_2 = \\sigma(-2.0) = \\frac{1}{1+e^{2}} \\approx 0.11920292$。\n\n**用例 3：序列 $S_3 = \\text{\"ACGTTT\"}$**\n- 长度 $L=6$。窗口数量为 $6-3+1=4$。这些窗口是：\"ACG\"、\"CGT\"、\"GTT\"、\"TTT\"。\n- 对于滤波器 $k=0$（ACG 检测器）：\n  - 窗口 \"ACG\" ($i=0$) 是一个完美匹配（$H_{0,0}=2$）。其他窗口不匹配，产生负的预激活值。\n  - 经过 ReLU 后，激活向量为 $[2, 0, 0, 0]$。\n  - $z_0 = \\max(2, 0, 0, 0) = 2$。\n- 对于滤波器 $k=1$（TTT 检测器）：\n  - 窗口 \"TTT\" ($i=3$) 是一个完美匹配（$H_{3,1}=2$）。其他窗口产生 $H_{0,1}=-4, H_{1,1}=-2, H_{2,1}=0$。\n  - 经过 ReLU 后，激活向量为 $[0, 0, 0, 2]$。\n  - $z_1 = \\max(0, 0, 0, 2) = 2$。\n- 潜在向量 $z = [2, 2]$。\n- 逻辑回归输入：$t_3 = u \\cdot z + a = (1.0 \\times 2) + (-0.5 \\times 2) - 1.0 = 2 - 1 - 1 = 0$。\n- 最终概率：$\\hat{y}_3 = \\sigma(0) = \\frac{1}{1+e^{0}} = 0.5$。\n\n**用例 4：序列 $S_4 = \\text{\"GCA\"}$**\n- 长度 $L=3$。窗口数量为 $3-3+1=1$。窗口是 \"GCA\"。\n- 对于滤波器 $k=0$（ACG 检测器）：窗口 \"GCA\" 是不匹配项。$H_{0,0} = (-1+1-1)-1=-2$。ReLU 给出 $0$。\n  - $z_0 = 0$。\n- 对于滤波器 $k=1$（TTT 检测器）：窗口 \"GCA\" 是不匹配项。$H_{0,1} = (-1-1-1)-1=-4$。ReLU 给出 $0$。\n  - $z_1 = 0$。\n- 潜在向量 $z = [0, 0]$。\n- 逻辑回归输入：$t_4 = u \\cdot z + a = (1.0 \\times 0) + (-0.5 \\times 0) - 1.0 = -1.0$。\n- 最终概率：$\\hat{y}_4 = \\sigma(-1.0) = \\frac{1}{1+e^{1}} \\approx 0.26894142$。\n\n最终结果，四舍五入到 $6$ 位小数，如下：\n- $\\hat{y}_1 \\approx 0.731059$\n- $\\hat{y}_2 \\approx 0.119203$\n- $\\hat{y}_3 = 0.500000$\n- $\\hat{y}_4 \\approx 0.268941$",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the output of a simple CNN for DNA sequence classification.\n    The solution rigorously follows the forward pass defined in the problem statement.\n    \"\"\"\n    \n    # Define fixed model parameters as specified in the problem statement.\n    # Channel order is (A, C, G, T)\n    W = np.array([\n        [  # Filter k=0 for \"ACG\"\n            [1, -1, -1, -1],\n            [-1, 1, -1, -1],\n            [-1, -1, 1, -1]\n        ],\n        [  # Filter k=1 for \"TTT\"\n            [-1, -1, -1, 1],\n            [-1, -1, -1, 1],\n            [-1, -1, -1, 1]\n        ]\n    ], dtype=np.float64) # Shape: K x F x 4 = 2 x 3 x 4\n    \n    b = np.array([-1.0, -1.0], dtype=np.float64) # Shape: K = 2\n    \n    u = np.array([1.0, -0.5], dtype=np.float64) # Shape: K = 2\n    a = -1.0\n    \n    F = 3 # Kernel width\n    \n    # Mapping from DNA base to channel index\n    base_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    \n    test_cases = [\n        \"ACGAC\",    # Case 1\n        \"TTT\",      # Case 2\n        \"ACGTTT\",   # Case 3\n        \"GCA\"       # Case 4\n    ]\n    \n    results = []\n    \n    for seq in test_cases:\n        L = len(seq)\n        \n        # Step 1: One-hot encode the sequence\n        X = np.zeros((L, 4), dtype=np.float64)\n        for i, base in enumerate(seq):\n            X[i, base_to_idx[base]] = 1.0\n            \n        # The number of positions for the convolutional kernel\n        num_windows = L - F + 1\n        \n        # Step 2: Convolution to get pre-activations H\n        K = W.shape[0]\n        H = np.zeros((num_windows, K), dtype=np.float64)\n        \n        for i in range(num_windows):\n            window = X[i : i + F, :] # Shape F x 4\n            for k in range(K):\n                # Element-wise product and sum, plus bias\n                pre_activation = np.sum(W[k] * window) + b[k]\n                H[i, k] = pre_activation\n        \n        # Step 3: ReLU Activation\n        R = np.maximum(0, H)\n        \n        # Step 4: Global Max Pooling\n        # If there are no windows (L  F), max would fail. Problem states L >= F.\n        # axis=0 takes the max over all window positions for each filter.\n        z = np.max(R, axis=0)\n        \n        # Step 5: Logistic Regression\n        logit = np.dot(u, z) + a\n        y_hat = 1.0 / (1.0 + np.exp(-logit))\n        \n        results.append(y_hat)\n        \n    # Format the final output string exactly as required.\n    # Each probability is rounded to exactly 6 decimal places.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "在掌握了CNN的前向传播机制后，我们可以将其作为一个强大的分析工具来探究生物学问题。本练习将模拟一个常见的计算生物学任务：评估单核苷酸多态性（SNP）对模型预测的影响。通过系统地改变输入序列中的一个碱基并计算模型输出的变化，你将执行一次“计算机中的”灵敏度分析，这有助于我们理解序列中的哪些位置对模型的决策至关重要。",
            "id": "2382374",
            "problem": "给定一个用于脱氧核糖核酸（DNA）序列分析的小型确定性卷积神经网络（CNN），以及一个通过改变 one-hot 编码输入中的一个位置来模拟单核苷酸多态性（SNP）的协议。您的任务是实现精确的数学流程，并为所提供的一组序列和突变测试套件，计算每个 SNP 引起的模型预测值的带符号变化。您的程序必须以严格指定的格式，将最终结果生成为单行输出。\n\n输入域是基于字母表 $\\{A,C,G,T\\}$ 的 DNA 序列集合。每个长度为 $L$ 的序列通过 one-hot 编码表示为一个矩阵 $X \\in \\mathbb{R}^{L \\times 4}$，其映射关系为 $A \\mapsto (1,0,0,0)$，$C \\mapsto (0,1,0,0)$，$G \\mapsto (0,0,1,0)$，$T \\mapsto (0,0,0,1)$。单核苷酸多态性通过将序列中某个确切位置 $p$（基于 0 的索引）的原始碱基更改为不同的目标碱基 $B'$ 来模拟，然后重新编码以获得突变后的矩阵 $X' \\in \\mathbb{R}^{L \\times 4}$。\n\n卷积层使用 $F$ 个滤波器进行一维互相关（卷积神经网络中使用的常规操作），步幅为 $1$，并采用“valid”边界（无填充）。设 $K$ 为卷积核宽度，$C=4$ 为输入通道数。对于权重为 $W^{(f)} \\in \\mathbb{R}^{K \\times C}$、偏置为 $b^{(f)} \\in \\mathbb{R}$ 的滤波器 $f \\in \\{0,1,\\dots,F-1\\}$，以及输出位置 $i \\in \\{0,1,\\dots,L-K\\}$，其激活前的值为\n$$\nz^{(f)}[i] \\;=\\; \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\, X[i+k,c] \\;+\\; b^{(f)}.\n$$\n非线性函数是修正线性单元（ReLU），定义为\n$$\n\\mathrm{ReLU}(u) \\;=\\; \\max(0,u).\n$$\n由此得到激活值\n$$\na^{(f)}[i] \\;=\\; \\mathrm{ReLU}\\!\\left(z^{(f)}[i]\\right).\n$$\n然后对每个滤波器独立应用全局最大池化，\n$$\nm^{(f)} \\;=\\; \\max_{i} \\, a^{(f)}[i] \\, ,\n$$\n产生一个向量 $m \\in \\mathbb{R}^{F}$。最后，一个全连接线性读出层产生标量预测值\n$$\ny \\;=\\; \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\, m^{(f)} \\;+\\; b^{\\mathrm{fc}} \\, .\n$$\n\n您将获得一个具有 $F=2$ 个滤波器和卷积核宽度 $K=3$ 的特定网络，其权重和偏置如下。所有省略的条目均为零。\n- 滤波器 $f=0$（检测模式 $A\\!C\\!G$）：\n  $$\n  W^{(0)} \\;=\\; \\begin{bmatrix}\n  1  0  0  0 \\\\\n  0  1  0  0 \\\\\n  0  0  1  0\n  \\end{bmatrix}, \\quad b^{(0)} \\;=\\; -2.5 \\, .\n  $$\n- 滤波器 $f=1$（检测模式 $T\\!T\\!T$）：\n  $$\n  W^{(1)} \\;=\\; \\begin{bmatrix}\n  0  0  0  1 \\\\\n  0  0  0  1 \\\\\n  0  0  0  1\n  \\end{bmatrix}, \\quad b^{(1)} \\;=\\; -2.5 \\, .\n  $$\n- 全连接读出层：\n  $$\n  w^{\\mathrm{fc}} \\;=\\; \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}, \\quad b^{\\mathrm{fc}} \\;=\\; 0.1 \\, .\n  $$\n\n对于每个测试用例，计算预测值的带符号变化量\n$$\n\\Delta \\;=\\; y_{\\mathrm{mut}} \\;-\\; y_{\\mathrm{ref}} \\, ,\n$$\n其中 $y_{\\mathrm{ref}}$ 是原始序列的预测值，$y_{\\mathrm{mut}}$ 是应用 SNP 后的预测值。位置使用基于 0 的索引。序列仅包含字符 $A$、$C$、$G$、$T$。每个 SNP 都将一个碱基更改为另一个不同的碱基。\n\n需要实现和评估的测试套件：\n- 用例 1：序列 $S = \\text{\"AACGTTGA\"}$，位置 $p=2$，新碱基 $B'=\\text{\"T\"}$。\n- 用例 2：序列 $S = \\text{\"TTTACGTT\"}$，位置 $p=0$，新碱基 $B'=\\text{\"G\"}$。\n- 用例 3：序列 $S = \\text{\"ACG\"}$，位置 $p=1$，新碱基 $B'=\\text{\"G\"}$。\n- 用例 4：序列 $S = \\text{\"ACGACG\"}$，位置 $p=1$，新碱基 $B'=\\text{\"T\"}$。\n\n您的程序必须：\n- 完全按照上述说明实现 one-hot 编码、卷积互相关、修正线性单元、全局最大池化和全连接读出层。\n- 对每个测试用例，计算所定义的 $\\Delta$。\n- 将每个 $\\Delta$ 四舍五入到 6 位小数。\n\n最终输出格式：\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序排列，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。每个值必须使用标准四舍五入规则保留 6 位小数（例如，$0.5$ 变为 $\\text{\"0.500000\"}$）。不应打印任何其他文本。",
            "solution": "该问题陈述已经过验证，并被认定为有效。它在科学上基于计算生物学和深度学习的原理，提出了一个具有所有必要参数的良定数学任务，并以客观、无歧义的语言表述。所描述的卷积神经网络的所有组成部分——one-hot 编码、卷积、ReLU 激活、最大池化和全连接层——都是该领域的标准元素。计算单核苷酸多态性（SNP）对模型输出影响的任务是一种标准的计算机（*in silico*）灵敏度分析技术。因此，我们着手提供一个完整的解决方案。\n\n该问题要求计算确定性神经网络输出的变化量 $\\Delta = y_{\\mathrm{mut}} - y_{\\mathrm{ref}}$，该变化由输入 DNA 序列中的单点突变引起。为实现这一目标，需要对参考序列（$S_{\\mathrm{ref}}$）和突变序列（$S_{\\mathrm{mut}}$）分别执行网络的前向传播，以获得它们各自的标量预测值 $y_{\\mathrm{ref}}$ 和 $y_{\\mathrm{mut}}$。\n\n计算流程的结构如下：\n\n**1. 输入编码**\n长度为 $L$ 的 DNA 序列通过 one-hot 编码被转换为一个数值矩阵 $X \\in \\mathbb{R}^{L \\times 4}$。每个核苷酸被映射到一个唯一的 4 维二进制向量，从而建立一个规范基：$A \\mapsto [1, 0, 0, 0]$，$C \\mapsto [0, 1, 0, 0]$，$G \\mapsto [0, 0, 1, 0]$，以及 $T \\mapsto [0, 0, 0, 1]$。\n\n**2. 卷积层**\n该层用于检测序列中的局部模式。它由 $F=2$ 个滤波器组成，每个滤波器的卷积核宽度为 $K=3$，权重为 $W^{(f)} \\in \\mathbb{R}^{3 \\times 4}$。操作是一维互相关，步幅为 $1$，无填充（“valid” 卷积）。对于每个滤波器 $f$ 和输入序列中每个可能的起始位置 $i$（其中 $i \\in \\{0, 1, \\dots, L-K\\}$），计算一个激活前的值 $z^{(f)}[i]$。该值是滤波器权重 $W^{(f)}$ 与输入矩阵 $X[i:i+K, :]$ 对应 $K \\times 4$ 切片的逐元素乘积之和，再加上一个特定于该滤波器的偏置 $b^{(f)}$。公式如下：\n$$\nz^{(f)}[i] = \\left( \\sum_{k=0}^{K-1} \\sum_{c=0}^{C-1} W^{(f)}[k,c] \\cdot X[i+k,c] \\right) + b^{(f)}\n$$\n所提供的滤波器权重旨在检测模体“ACG”（$f=0$）和“TTT”（$f=1$）。当序列窗口与滤波器权重中编码的模体完全匹配时，点积之和为 $3$，加上偏置 $b^{(f)} = -2.5$ 后，得到的激活前的值为 $0.5$。\n\n**3. 激活函数**\n激活前的值通过一个非线性激活函数——修正线性单元（ReLU），其定义为 $\\mathrm{ReLU}(u) = \\max(0, u)$。该函数引入了非线性，使模型能够学习更复杂的模式。它有效地将任何负的激活前的值置为零，这意味着只有当输入模体提供足够强的匹配以克服负偏置时，滤波器才会“激活”。激活值由下式给出：\n$$\na^{(f)}[i] = \\mathrm{ReLU}(z^{(f)}[i])\n$$\n\n**4. 全局最大池化层**\n对于每个滤波器的激活图 $a^{(f)}$，通过取所有位置 $i$ 的最大值来提取单个特征值 $m^{(f)}$。这被称为全局最大池化。\n$$\nm^{(f)} = \\max_{i} a^{(f)}[i]\n$$\n此操作使模型的预测对检测到的模体的位置不敏感，并将表示的维度降低到一个固定大小的向量 $m \\in \\mathbb{R}^{F}$，而与输入序列的长度 $L$ 无关。如果没有位置产生正激活值，则最大值为 $0$。\n\n**5. 全连接读出层**\n最终的预测是一个标量值 $y$，通过池化后的特征值 $m^{(f)}$ 的线性组合计算得出，该组合由全连接层的权重 $w^{\\mathrm{fc}}$ 加权，并加上一个最终的偏置项 $b^{\\mathrm{fc}}$。\n$$\ny = \\sum_{f=0}^{F-1} w^{\\mathrm{fc}}[f] \\cdot m^{(f)} + b^{\\mathrm{fc}}\n$$\n\n**计算示例：用例 1**\n让我们逐步完成用例 1 的计算：$S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$，位置 $p=2$，新碱基 $B'=\\text{\"T\"}$。突变序列为 $S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$。\n\n**参考序列预测值 ($y_{\\mathrm{ref}}$):**\n- 序列：$S_{\\mathrm{ref}} = \\text{\"AACGTTGA\"}$ ($L=8$)。\n- 卷积窗口 ($K=3$)：\"AAC\"、\"ACG\"、\"CGT\"、\"GTT\"、\"TTG\"、\"TGA\"。\n- **滤波器 0 (\"ACG\"):** 位于 $i=1$ 的窗口 \"ACG\" 产生完全匹配。点积为 $3$。$z^{(0)}[1] = 3 - 2.5 = 0.5$。所有其他窗口的点积都小于 $2.5$，因此它们的激活前的值为负。激活图为 $a^{(0)}_{\\mathrm{ref}} = [0, 0.5, 0, 0, 0, 0]$。\n- **滤波器 1 (\"TTT\"):** 没有窗口能与 \"TTT\" 完全匹配。最高的点积来自 \"GTT\" 和 \"TTG\"（得分为 $2$），产生的激活前的值为 $2 - 2.5 = -0.5$。因此，该滤波器的所有激活值均为 $0$。激活图为 $a^{(1)}_{\\mathrm{ref}} = [0, 0, 0, 0, 0, 0]$。\n- **最大池化:** $m^{(0)}_{\\mathrm{ref}} = \\max(a^{(0)}_{\\mathrm{ref}}) = 0.5$。$m^{(1)}_{\\mathrm{ref}} = \\max(a^{(1)}_{\\mathrm{ref}}) = 0$。池化后的特征向量为 $m_{\\mathrm{ref}} = [0.5, 0]$。\n- **读出层:** $y_{\\mathrm{ref}} = (1.0 \\cdot 0.5) + (-0.5 \\cdot 0) + 0.1 = 0.5 + 0 + 0.1 = 0.6$。\n\n**突变序列预测值 ($y_{\\mathrm{mut}}$):**\n- 序列：$S_{\\mathrm{mut}} = \\text{\"AATGTTGA\"}$。位置 $p=2$ 的突变将 \"C\" 变为 \"T\"。\n- 原始的 \"ACG\" 模体被破坏。受影响的新窗口是 \"AAT\"（$i=0$）、\"ATG\"（$i=1$）和 \"TGT\"（$i=2$）。\n- **滤波器 0 (\"ACG\"):** 现在任何窗口产生的最高点积是 $2$（来自 \"ATG\"），得到激活前的值为 $2 - 2.5 = -0.5$。所有激活值均为 $0$。$a^{(0)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$。\n- **滤波器 1 (\"TTT\"):** 该突变未创建 \"TTT\" 模体。最高的点积为 $2$（来自 \"TGT\"、\"GTT\"、\"TTG\"），产生的激活前的值为 $-0.5$。所有激活值均为 $0$。$a^{(1)}_{\\mathrm{mut}} = [0, 0, 0, 0, 0, 0]$。\n- **最大池化:** $m^{(0)}_{\\mathrm{mut}} = 0$。$m^{(1)}_{\\mathrm{mut}} = 0$。池化后的特征向量为 $m_{\\mathrm{mut}} = [0, 0]$。\n- **读出层:** $y_{\\mathrm{mut}} = (1.0 \\cdot 0) + (-0.5 \\cdot 0) + 0.1 = 0.1$。\n\n**用例 1 的最终计算:**\n预测值的变化为 $\\Delta_1 = y_{\\mathrm{mut}} - y_{\\mathrm{ref}} = 0.1 - 0.6 = -0.5$。\n\n此精确过程将应用于所有测试用例，结果四舍五入至 6 位小数。所提供的程序实现了这一逻辑。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the signed change in a CNN's prediction due to single nucleotide polymorphisms.\n    \"\"\"\n    # Define the fixed network parameters\n    params = {\n        'W0': np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=float),\n        'b0': -2.5,\n        'W1': np.array([[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]], dtype=float),\n        'b1': -2.5,\n        'w_fc': np.array([1.0, -0.5], dtype=float),\n        'b_fc': 0.1\n    }\n    \n    # One-hot encoding mapping\n    one_hot_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    num_channels = 4\n    kernel_width = 3\n\n    def get_prediction(sequence: str) -> float:\n        \"\"\"\n        Performs a full forward pass of the CNN for a given DNA sequence.\n        \"\"\"\n        seq_len = len(sequence)\n        \n        # Handle sequences shorter than the kernel\n        if seq_len  kernel_width:\n             # No convolution is possible, so all activations are zero by default\n             m = np.array([0.0, 0.0])\n             y = np.sum(m * params['w_fc']) + params['b_fc']\n             return y\n        \n        # 1. One-hot encode the sequence\n        X = np.zeros((seq_len, num_channels), dtype=float)\n        for i, base in enumerate(sequence):\n            if base in one_hot_map:\n                X[i, one_hot_map[base]] = 1.0\n\n        # Define filters and biases\n        filters = [(params['W0'], params['b0']), (params['W1'], params['b1'])]\n        pooled_features = []\n\n        for W_f, b_f in filters:\n            # 2. Convolutional Layer (pre-activations)\n            conv_len = seq_len - kernel_width + 1\n            pre_activations = np.zeros(conv_len, dtype=float)\n            for i in range(conv_len):\n                window = X[i : i + kernel_width, :]\n                pre_activations[i] = np.sum(window * W_f) + b_f\n            \n            # 3. ReLU Activation\n            activations = np.maximum(0, pre_activations)\n            \n            # 4. Global Max Pooling\n            # np.max on an empty array raises error. If activations is empty, this\n            # means conv_len was 0. In this case max is 0.\n            if activations.size > 0:\n                max_activation = np.max(activations)\n            else:\n                max_activation = 0.0\n            \n            pooled_features.append(max_activation)\n        \n        m = np.array(pooled_features)\n        \n        # 5. Fully Connected Readout\n        y = np.sum(m * params['w_fc']) + params['b_fc']\n        \n        return y\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"AACGTTGA\", 2, \"T\"),\n        (\"TTTACGTT\", 0, \"G\"),\n        (\"ACG\", 1, \"G\"),\n        (\"ACGACG\", 1, \"T\"),\n    ]\n\n    results = []\n    for seq_ref, p, new_base in test_cases:\n        # Get prediction for reference sequence\n        y_ref = get_prediction(seq_ref)\n        \n        # Create mutated sequence\n        seq_list = list(seq_ref)\n        seq_list[p] = new_base\n        seq_mut = \"\".join(seq_list)\n        \n        # Get prediction for mutated sequence\n        y_mut = get_prediction(seq_mut)\n        \n        # Compute the change and round\n        delta = y_mut - y_ref\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在前面的练习中，网络的权重都是预先给定的。但在实际应用中，这些权重是如何从数据中学习得到的呢？这个高级练习将带你深入CNN的核心——训练过程。你将从零开始实现梯度下降算法，并探索$L_1$和$L_2$正则化对学习过程的影响，从而理解它们如何帮助模型学习到稀疏且有意义的生物序列基序（motif），同时防止过拟合。",
            "id": "2382359",
            "problem": "您需要编写一个完整的、可运行的程序，研究正则化惩罚项的选择如何影响使用卷积神经网络（CNN）进行脱氧核糖核酸（DNA）序列分析时，单个一维卷积核所学习基序（motif）的稀疏性。该模型被形式化为具有逻辑斯谛链接（logistic link）和平均池化的线性模型。您必须从第一性原理推导出算法，并仅使用线性代数和向量微积分来实现它。本问题陈述中出现的所有符号、变量、函数、运算符和数字都必须使用 LaTeX 数学模式表示。\n\n考虑字母表 $\\{A,C,G,T\\}$ 上的序列，其在 $\\mathbb{R}^{4}$ 中进行独热编码（one-hot encoding）。设序列长度为 $S$，核（基序）长度为 $L$。对于一个独热编码的序列 $X \\in \\{0,1\\}^{S \\times 4}$，定义其在位置 $t$ 的一维有效卷积响应为\n$$\ns_t = \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b} \\, X_{t+i-1,\\,b},\n$$\n其中 $W \\in \\mathbb{R}^{L \\times 4}$ 是核权重。假设跨位置进行平均池化以形成一个标量预激活值\n$$\nz = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} s_t + b,\n$$\n其中 $b \\in \\mathbb{R}$ 是一个截距。设对于二元标签 $y \\in \\{0,1\\}$ 的预测概率由逻辑斯谛函数给出\n$$\np = \\sigma(z) = \\frac{1}{1+\\exp(-z)}.\n$$\n给定一个数据集 $\\{(X^{(n)},y^{(n)})\\}_{n=1}^{N}$，定义平均逻辑斯谛损失为\n$$\n\\mathcal{L}(W,b) = \\frac{1}{N} \\sum_{n=1}^{N} \\left[ -y^{(n)} \\log p^{(n)} - (1-y^{(n)}) \\log(1-p^{(n)}) \\right],\n$$\n其中 $p^{(n)} = \\sigma(z^{(n)})$ 如上计算。仅考虑对 $W$ 的两种正则化器：$L_1$ 惩罚项\n$$\nR_{1}(W) = \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\left| W_{i,b} \\right|\n$$\n和 $L_2$ 惩罚项\n$$\nR_{2}(W) = \\frac{1}{2} \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b}^{2}.\n$$\n对于正则化强度 $\\lambda \\ge 0$，要最小化的经验风险是\n$$\nF(W,b) = \\mathcal{L}(W,b) + \\lambda R(W),\n$$\n其中 $R(W)$ 是 $R_{1}(W)$ 或 $R_{2}(W)$。当用 $X$ 的所有滑动窗口的平均值表示时，平均池化确保了模型对于 $W$ 和 $b$ 是线性的，因此无正则化部分是凸的。\n\n您的任务是：\n- 从第一性原理出发，使用链式法则推导出 $\\mathcal{L}(W,b)$ 相对于 $W$ 和 $b$ 的梯度，并根据池化特征的数据矩阵，为梯度指定一个由利普希茨常数界（Lipschitz constant bound）证明其合理性的步长选择。然后，您必须实现两种训练程序：\n  - 对于 $L_2$ 正则化，使用梯度下降法对 $W$ 和 $b$ 进行 $T$ 步更新，步长为 $\\alpha$。\n  - 对于 $L_1$ 正则化，使用近端梯度下降法（proximal gradient descent）：对 $W$ 的无正则化部分执行一个梯度步，然后应用阈值为 $\\alpha \\lambda$ 的软阈值近端算子（soft-thresholding proximal operator）；通过无惩罚的梯度下降法更新 $b$。\n- 按如下方式生成合成数据。固定整数 $S$ 和 $L$，并通过为每个位置 $i \\in \\{1,\\dots,L\\}$ 选择一个偏好的碱基 $b_i \\in \\{1,2,3,4\\}$（对应于 $\\{A,C,G,T\\}$）来构建一个真实基序。对于正序列，生成一个长度为 $S$ 的背景序列，其碱基是独立同分布的均匀分布，然后选择一个起始索引 $t^{\\star} \\in \\{1, \\dots, S-L+1\\}$，并用偏好的碱基覆盖相应偏移处的长度为 $L$ 的片段。对于负序列，生成一个长度为 $S$ 的背景序列，不覆盖任何片段。将序列编码为 $\\{0,1\\}^{S \\times 4}$ 中的独热矩阵。正序列的标签为 $y=1$，负序列的标签为 $y=0$。对于每个序列，计算所有长度为 $L$ 的独热编码片段的平均值：\n$$\n\\Phi(X) = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} X_{t:t+L,:} \\in \\mathbb{R}^{L \\times 4},\n$$\n并将其展平为 $\\mathbb{R}^{4L}$ 中的一个向量，作为线性模型的特征向量。这种特征构造与上述平均池化卷积模型完全等价。\n- 对每个指定的测试用例训练模型，直到 $T$ 步。设 $\\tau0$ 为一个小阈值。定义所学核的稀疏度分数为\n$$\n\\mathrm{spar}(W) = \\frac{1}{4L} \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\mathbf{1}\\left\\{ |W_{i,b}|  \\tau \\right\\}.\n$$\n为每个测试用例报告此值，作为一个在 $[0,1]$ 区间内的实数。\n\n实现要求：\n- 您必须仅使用基本的线性代数以及所要求的逻辑斯谛链接和近端算子来实现训练；不要使用任何机器学习库。\n- 对于无正则化的逻辑斯谛损失，使用从特征矩阵的谱范数推导出的利普希茨常数上界来选择步长 $\\alpha$。对于 $L_2$ 正则化，在您的步长界中适当考虑其带来的额外平滑性。\n- 使用 $T=1000$ 个梯度步，$\\tau=10^{-3}$，并且不要对截距 $b$ 进行惩罚。\n- 所有随机选择必须可以从一个指定的整数种子复现。\n\n测试套件：\n- 在所有情况下，使用 $S=40$，$L=8$，以及包含 $N/2$ 个正样本和 $N/2$ 个负样本的平衡类别。\n- 用例 $\\mathbf{1}$ (正常路径, $L_1$)：正则化 $L_1$，$\\lambda = 0.05$，$N=600$，种子 $=7$，有信号（正样本包含基序，负样本不包含）。\n- 用例 $\\mathbf{2}$ (正常路径控制, $L_2$)：正则化 $L_2$，$\\lambda = 0.05$，$N=600$，种子 $=7$，有信号。\n- 用例 $\\mathbf{3}$ (边缘情况, 无信号)：正则化 $L_1$，$\\lambda = 0.3$，$N=600$，种子 $=21$，无信号（两个类别都作为背景生成；标签为前 $N/2$ 个序列分配 $y=1$，为其余 $N/2$ 个序列分配 $y=0$）。\n- 用例 $\\mathbf{4}$ (边界条件, 正则化消失)：正则化 $L_1$，$\\lambda = 10^{-6}$，$N=600$，种子 $=11$，有信号。\n\n最终输出规范：\n- 您的程序必须生成单行输出，其中包含用例 $\\mathbf{1}$ 到 $\\mathbf{4}$ 的稀疏度分数，按此顺序排列，形式为方括号括起来的逗号分隔列表。每个值必须四舍五入到小数点后三位。例如，输出格式必须与以下完全一样\n$$\n[\\text{v}_1,\\text{v}_2,\\text{v}_3,\\text{v}_4]\n$$\n其中每个 $\\text{v}_k$ 是一个小数点后有三位的小数，并且不打印任何额外文本。",
            "solution": "该问题要求实现一个数值实验，以研究 $L_1$ 与 $L_2$ 正则化在一个简化的用于 DNA 序列分析的一维卷积神经网络模型中引发稀疏性的效果。该模型等价于对所有子序列片段进行平均池化得到的特征进行逻辑斯谛回归。解决方案将从第一性原理推导得出。\n\n首先，我们将模型形式化为线性逻辑斯谛回归。一个序列 $X \\in \\{0,1\\}^{S \\times 4}$ 被独热编码。核是 $W \\in \\mathbb{R}^{L \\times 4}$。卷积响应后跟平均池化，产生一个预激活值 $z$：\n$$\nz = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} \\left( \\sum_{i=1}^{L} \\sum_{b=1}^{4} W_{i,b} X_{t+i-1,b} \\right) + b\n$$\n根据求和的线性性质，这可以表示为一个内积。设 $w = \\text{vec}(W) \\in \\mathbb{R}^{4L}$ 为核权重的展平向量。设 $\\phi(X) \\in \\mathbb{R}^{4L}$ 为与序列矩阵 $X$ 的所有长度为 $L$ 的独热编码片段的平均值相对应的展平向量：\n$$\n\\Phi(X) = \\frac{1}{S-L+1} \\sum_{t=1}^{S-L+1} X_{[t:t+L-1],:} \\in \\mathbb{R}^{L \\times 4}\n$$\n且 $\\phi(X) = \\text{vec}(\\Phi(X))$。这里，$X_{[t:t+L-1],:}$ 表示 $X$ 从行 $t$ 到行 $t+L-1$ 的子矩阵。预激活值可以写成一个标准的线性模型：\n$$\nz = w^T \\phi(X) + b\n$$\n给定一个数据集 $\\{(\\phi^{(n)}, y^{(n)})\\}_{n=1}^N$，其中 $\\phi^{(n)} = \\phi(X^{(n)})$，预测模型为 $p^{(n)} = \\sigma(z^{(n)}) = \\sigma(w^T\\phi^{(n)} + b)$。目标是最小化正则化的逻辑斯谛损失：\n$$\nF(w,b) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{L}^{(n)}(w,b) + \\lambda R(w)\n$$\n其中 $\\mathcal{L}^{(n)} = -y^{(n)} \\log p^{(n)} - (1-y^{(n)}) \\log(1-p^{(n)})$ 且 $R(w)$ 是 $w$ 的 $L_1$ 或 $L_2$ 范数。\n\n为了实现优化算法，我们需要无正则化损失 $\\mathcal{L}(w,b) = \\frac{1}{N} \\sum_n \\mathcal{L}^{(n)}$ 的梯度。使用链式法则，单个样本 $n$ 的损失相对于预激活值 $z^{(n)}$ 的梯度是：\n$$\n\\frac{\\partial \\mathcal{L}^{(n)}}{\\partial z^{(n)}} = \\frac{\\partial \\mathcal{L}^{(n)}}{\\partial p^{(n)}} \\frac{\\partial p^{(n)}}{\\partial z^{(n)}} = \\left( -\\frac{y^{(n)}}{p^{(n)}} + \\frac{1-y^{(n)}}{1-p^{(n)}} \\right) \\cdot (p^{(n)}(1-p^{(n)})) = p^{(n)} - y^{(n)}\n$$\n$z^{(n)}$ 相对于参数的梯度是 $\\nabla_w z^{(n)} = \\phi^{(n)}$ 和 $\\partial z^{(n)}/\\partial b = 1$。平均损失 $\\mathcal{L}$ 的完整梯度是：\n$$\n\\nabla_w \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} (p^{(n)} - y^{(n)}) \\phi^{(n)}\n$$\n$$\n\\nabla_b \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} (p^{(n)} - y^{(n)})\n$$\n用矩阵表示法，设 $\\Phi$ 是 $N \\times (4L)$ 的设计矩阵，其行是 $(\\phi^{(n)})^T$，$y$ 是标签向量，$p$ 是预测向量。梯度为 $\\nabla_w \\mathcal{L} = \\frac{1}{N} \\Phi^T (p-y)$ 和 $\\nabla_b \\mathcal{L} = \\frac{1}{N} \\mathbf{1}^T (p-y)$。\n\n对于基于梯度的优化，必须仔细选择步长 $\\alpha$。一个稳定的 $\\alpha$ 选择与被优化函数梯度的利普希茨常数成反比。对于逻辑斯谛损失 $\\mathcal{L}(w,b)$，我们找到其海森矩阵（Hessian）的一个上界。设 $\\theta = [w^T, b]^T$ 和 $\\tilde{\\phi}^{(n)}=[(\\phi^{(n)})^T, 1]^T$。海森矩阵是：\n$$\n\\nabla^2_{\\theta} \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{\\partial (p^{(n)}-y^{(n)})}{\\partial \\theta^T} \\tilde{\\phi}^{(n)} = \\frac{1}{N} \\sum_{n=1}^{N} p^{(n)}(1-p^{(n)}) \\tilde{\\phi}^{(n)} (\\tilde{\\phi}^{(n)})^T\n$$\n由于 $p(1-p) \\le 1/4$，我们有 $\\nabla^2_{\\theta} \\mathcal{L} \\preceq \\frac{1}{4N} \\sum_{n=1}^{N} \\tilde{\\phi}^{(n)} (\\tilde{\\phi}^{(n)})^T = \\frac{1}{4N} \\tilde{\\Phi}^T \\tilde{\\Phi}$，其中 $\\tilde{\\Phi}$ 是 $N \\times (4L+1)$ 的增广设计矩阵。$\\nabla \\mathcal{L}$ 的利普希茨常数 $L_{\\nabla \\mathcal{L}}$ 受该矩阵最大特征值的限制：$L_{\\nabla \\mathcal{L}} \\le \\frac{1}{4N} \\lambda_{\\text{max}}(\\tilde{\\Phi}^T \\tilde{\\Phi}) = \\frac{\\sigma_{\\text{max}}(\\tilde{\\Phi})^2}{4N}$。我们将基于这个上界来设置步长，$L_{bound} = \\frac{\\sigma_{\\text{max}}(\\tilde{\\Phi})^2}{4N}$。\n\n对于 $L_2$ 正则化，目标函数 $F(w,b) = \\mathcal{L}(w,b) + \\frac{\\lambda}{2} \\|w\\|_2^2$ 是平滑的。梯度是 $\\nabla_w F = \\nabla_w \\mathcal{L} + \\lambda w$ 和 $\\nabla_b F = \\nabla_b \\mathcal{L}$。该目标函数的海森矩阵是 $\\nabla^2_\\theta F = \\nabla^2_\\theta \\mathcal{L} + \\text{diag}(\\lambda, \\dots, \\lambda, 0)$。梯度 $\\nabla F$ 是利普希茨连续的，其常数 $L_{\\nabla F} \\le L_{bound} + \\lambda$。梯度下降的更新步骤是：\n$$\nw_{k+1} = w_k - \\alpha (\\nabla_w \\mathcal{L}(w_k, b_k) + \\lambda w_k)\n$$\n$$\nb_{k+1} = b_k - \\alpha \\nabla_b \\mathcal{L}(w_k, b_k)\n$$\n一个安全的步长是 $\\alpha = 1/(L_{bound} + \\lambda)$。\n\n对于 $L_1$ 正则化，目标函数 $F(w,b) = \\mathcal{L}(w,b) + \\lambda \\|w\\|_1$ 是非平滑的。我们使用近端梯度下降法，它将平滑部分（$\\mathcal{L}$）上的梯度步与非平滑部分（$\\lambda \\|w\\|_1$）的近端步相结合。更新步骤是：\n$$\nw_{k+1} = \\text{prox}_{\\alpha\\lambda, \\|\\cdot\\|_1} ( w_k - \\alpha \\nabla_w \\mathcal{L}(w_k, b_k) )\n$$\n$$\nb_{k+1} = b_k - \\alpha \\nabla_b \\mathcal{L}(w_k, b_k)\n$$\n步长 $\\alpha$ 仅根据平滑部分选择，因此我们设置 $\\alpha = 1/L_{bound}$。$L_1$ 范数的近端算子是软阈值函数，逐元素应用：\n$$\n\\text{prox}_{\\gamma, \\|\\cdot\\|_1}(v)_i = \\text{sign}(v_i) \\max(|v_i| - \\gamma, 0)\n$$\n在我们的例子中，阈值是 $\\gamma = \\alpha\\lambda$。\n\n最终权重核 $W$（从 $w$ 重塑而来）的稀疏度计算为绝对值小于一个小阈值 $\\tau  0$ 的权重所占的比例：\n$$\n\\mathrm{spar}(W) = \\frac{1}{4L} \\sum_{i=1}^{L} \\sum_{b=1}^{4} \\mathbf{1}\\left\\{ |W_{i,b}|  \\tau \\right\\}.\n$$\n实现将遵循这些推导来执行所需的数值实验。",
            "answer": "```python\nimport numpy as np\n\ndef one_hot_encode(seq, alphabet_size=4):\n    \"\"\"Converts a sequence of integers into a one-hot encoded matrix.\"\"\"\n    return np.eye(alphabet_size)[seq]\n\ndef generate_data(N, S, L, seed, signal=True):\n    \"\"\"\n    Generates synthetic DNA sequence data.\n    \n    Args:\n        N (int): Total number of sequences.\n        S (int): Sequence length.\n        L (int): Motif length.\n        seed (int): Random seed for reproducibility.\n        signal (bool): If True, positive samples contain a motif. If False,\n                       all samples are random background.\n\n    Returns:\n        tuple: (sequences, labels)\n            sequences (np.ndarray): N x S x 4 one-hot encoded sequences.\n            labels (np.ndarray): N x 1 binary labels.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    n_pos = N // 2\n    n_neg = N - n_pos\n    \n    sequences_int = []\n    labels = np.array([1] * n_pos + [0] * n_neg)\n\n    # Ground-truth motif\n    motif_bases = rng.choice(4, size=L)\n\n    # Positive sequences\n    for _ in range(n_pos):\n        seq = rng.choice(4, size=S)\n        if signal:\n            start_pos = rng.integers(0, S - L + 1)\n            seq[start_pos : start_pos + L] = motif_bases\n        sequences_int.append(seq)\n        \n    # Negative sequences\n    for _ in range(n_neg):\n        seq = rng.choice(4, size=S)\n        sequences_int.append(seq)\n\n    sequences_one_hot = np.array([one_hot_encode(s) for s in sequences_int])\n    return sequences_one_hot, labels\n\ndef create_feature_matrix(sequences, S, L):\n    \"\"\"\n    Computes the average patch features for a set of sequences.\n    \n    Args:\n        sequences (np.ndarray): N x S x 4 one-hot encoded sequences.\n        S (int): Sequence length.\n        L (int): Motif length.\n\n    Returns:\n        np.ndarray: N x (4*L) feature matrix.\n    \"\"\"\n    N = sequences.shape[0]\n    num_windows = S - L + 1\n    phi_matrix = np.zeros((N, L, 4))\n\n    for n in range(N):\n        avg_patch = np.zeros((L, 4))\n        for t in range(num_windows):\n            avg_patch += sequences[n, t:t+L, :]\n        phi_matrix[n, :, :] = avg_patch / num_windows\n    \n    return phi_matrix.reshape(N, 4 * L)\n\ndef train_model(phi_matrix, y, N, L, reg_type, lambda_val, T, tau):\n    \"\"\"\n    Trains the logistic regression model and computes sparsity.\n    \n    Args:\n        phi_matrix (np.ndarray): N x (4*L) feature matrix.\n        y (np.ndarray): N-element label vector.\n        N (int): Number of samples.\n        L (int): Kernel length.\n        reg_type (str): 'L1' or 'L2'.\n        lambda_val (float): Regularization strength.\n        T (int): Number of training steps.\n        tau (float): Threshold for sparsity calculation.\n\n    Returns:\n        float: Sparsity fraction of the learned kernel weights.\n    \"\"\"\n    n_features = 4 * L\n    \n    # Augment features for intercept and Lipschitz constant calculation\n    tilde_phi = np.c_[phi_matrix, np.ones(N)]\n    \n    # Calculate Lipschitz constant bound for the gradient of the logistic loss\n    # L_bound = norm(tilde_Phi.T @ tilde_Phi) / (4*N) = sigma_max(tilde_Phi)^2 / (4*N)\n    # Using power iteration to find the largest eigenvalue of tilde_phi.T @ tilde_phi\n    # which is sigma_max^2. This is more stable than SVD for large matrices.\n    A = tilde_phi.T @ tilde_phi\n    v = np.random.rand(A.shape[1])\n    for _ in range(10): # Power iteration\n        v = A @ v\n        v = v / np.linalg.norm(v)\n    lambda_max = v.T @ A @ v\n    \n    lipschitz_L = lambda_max / (4 * N)\n    \n    # Initialize weights and bias\n    w = np.zeros(n_features)\n    b = 0.0\n\n    if reg_type == 'L1':\n        alpha = 1.0 / lipschitz_L\n    elif reg_type == 'L2':\n        alpha = 1.0 / (lipschitz_L + lambda_val)\n    else:\n        raise ValueError(\"Invalid regularization type\")\n\n    for _ in range(T):\n        z = phi_matrix @ w + b\n        p = 1.0 / (1.0 + np.exp(-z))\n        \n        error = p - y\n        \n        grad_w = (1.0 / N) * phi_matrix.T @ error\n        grad_b = (1.0 / N) * np.sum(error)\n        \n        if reg_type == 'L1':\n            w_temp = w - alpha * grad_w\n            threshold = alpha * lambda_val\n            w = np.sign(w_temp) * np.maximum(np.abs(w_temp) - threshold, 0)\n            b = b - alpha * grad_b\n        elif reg_type == 'L2':\n            w = w - alpha * (grad_w + lambda_val * w)\n            b = b - alpha * grad_b\n            \n    sparsity = np.sum(np.abs(w)  tau) / n_features\n    return sparsity\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Global parameters\n    S = 40\n    L = 8\n    T = 1000\n    tau = 1e-3\n\n    # Test cases from the problem statement\n    test_cases = [\n        {'case': 1, 'reg_type': 'L1', 'lambda_val': 0.05, 'N': 600, 'seed': 7,  'signal': True},\n        {'case': 2, 'reg_type': 'L2', 'lambda_val': 0.05, 'N': 600, 'seed': 7, 'signal': True},\n        {'case': 3, 'reg_type': 'L1', 'lambda_val': 0.3,  'N': 600, 'seed': 21, 'signal': False},\n        {'case': 4, 'reg_type': 'L1', 'lambda_val': 1e-6, 'N': 600, 'seed': 11, 'signal': True},\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Generate data\n        sequences, labels = generate_data(\n            N=case['N'], S=S, L=L, seed=case['seed'], signal=case['signal']\n        )\n        \n        # 2. Create feature matrix\n        phi_matrix = create_feature_matrix(sequences, S, L)\n        \n        # 3. Train model and get sparsity\n        sparsity = train_model(\n            phi_matrix, labels, case['N'], L,\n            case['reg_type'], case['lambda_val'], T, tau\n        )\n        \n        results.append(sparsity)\n\n    # Format and print the final output as specified.\n    print(f\"[{','.join(f'{v:.3f}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}