{
    "hands_on_practices": [
        {
            "introduction": "第一个练习将让你直接上手比较监督学习和无监督学习。你将解决一个经典的生物信息学问题——区分蛋白质编码与非编码DNA序列——并使用两种不同的方法：一种是利用已知标签进行学习的支持向量机，另一种是试图在数据中寻找内在结构的聚类算法。这个练习旨在阐明，标签的可用性和使用方式如何从根本上区分这两种学习范式并影响其性能。",
            "id": "2432827",
            "problem": "给定一个有限字母表 $\\mathcal{A} = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 和一个特征映射的定义，该映射将长度为 $L$ 的脱氧核糖核酸 (DNA) 序列 $s$ 映射到一个归一化的 $k$-mer 频率向量 $\\phi_k(s) \\in \\mathbb{R}^{4^k}$。对于给定的正整数 $k$，设 $\\mathcal{M}_k$ 是字母表 $\\mathcal{A}$ 上所有长度为 $k$ 的字符串的集合，按字典序列举。对于每个 $m \\in \\mathcal{M}_k$，分量 $\\left[\\phi_k(s)\\right]_m$ 定义为在 $s$ 中使用步长为 $1$ 的滑动窗口出现 $m$ 的次数，除以 $s$ 中 $k$-mer 的总数，即 $L - k + 1$（假设 $L \\ge k$ 且没有出现模糊字符）。形式上，\n$$\n\\left[\\phi_k(s)\\right]_m = \\frac{1}{L - k + 1} \\sum_{i=1}^{L-k+1} \\mathbf{1}\\left\\{ s_i s_{i+1} \\cdots s_{i+k-1} = m \\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数，索引是基于 $1$ 的。\n\n一个二元标签 $y \\in \\{-1, +1\\}$ 表示一个 DNA 序列是蛋白质编码 ($+1$) 还是非编码 ($-1$)。考虑一个由权重向量 $w \\in \\mathbb{R}^{4^k}$ 和偏置 $b \\in \\mathbb{R}$ 定义的监督线性分类器，它通过得分 $f(s) = w^\\top \\phi_k(s) + b$ 和预测规则 $\\hat{y}(s) = \\mathrm{sign}(f(s))$ 作用于特征 $\\phi_k(s)$，其中如果 $z \\ge 0$ 则 $\\mathrm{sign}(z) = +1$，否则为 $-1$。模型参数 $(w,b)$ 通过最小化带有平方合页损失的正则化经验风险得到：\n$$\nJ(w,b) = \\frac{1}{2} \\lVert w \\rVert_2^2 + C \\sum_{i=1}^{N} \\left( \\max\\left(0, 1 - y_i (w^\\top x_i + b) \\right) \\right)^2,\n$$\n其中 $C > 0$ 是给定的正则化参数，$\\{(x_i, y_i)\\}_{i=1}^N$ 是训练样本，其中 $x_i = \\phi_k(s_i)$ 且 $y_i \\in \\{-1, +1\\}$，$\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。\n\n对于无监督基线，考虑将一组给定的特征向量 $\\{x_j\\}_{j=1}^M$（其中 $x_j = \\phi_k(s_j)$ 对应 $M$ 个序列）划分为恰好两个非空簇 $\\mathcal{C}_0$ 和 $\\mathcal{C}_1$，以最小化簇内平方和：\n$$\nW(\\mathcal{C}_0, \\mathcal{C}_1) = \\sum_{j \\in \\mathcal{C}_0} \\lVert x_j - \\mu_0 \\rVert_2^2 + \\sum_{j \\in \\mathcal{C}_1} \\lVert x_j - \\mu_1 \\rVert_2^2,\n$$\n其中 $\\mu_0$ 和 $\\mu_1$ 分别是它们各自簇的均值。在获得最小化划分后，通过将两个簇映射到 $\\{-1, +1\\}$ 的方式，为每个项目定义一个基于簇的预测 $\\tilde{y}_j \\in \\{-1, +1\\}$，该映射方式能够最大化与相同项目上真实标签的匹配分数；将该最大分数作为无监督准确率（以小数表示）报告。\n\n提供给您一个带标签的训练集 $\\mathcal{D}_{\\mathrm{train}}$ 和一个带标签的测试集 $\\mathcal{D}_{\\mathrm{test}}$ 的 DNA 序列。所有序列仅由字母表 $\\mathcal{A}$ 中的符号组成。\n\n训练序列（括号中显示标签 $y$，其中 $+1$ 表示编码，$-1$ 表示非编码）：\n- $s_1 = \\texttt{ATGGCGGCCGCGGGCGCCGCGGGCGACGGCTGA}$ $(+1)$\n- $s_2 = \\texttt{ATGCGCGCGCGGGCCGCGGCTGCGGCGTAG}$ $(+1)$\n- $s_3 = \\texttt{ATGGGCGACGGCGGCGACGGCGGCGACTAA}$ $(+1)$\n- $s_4 = \\texttt{ATGGCCGCTGCGGCTGGCGCTGCGGCTTGA}$ $(+1)$\n- $s_5 = \\texttt{ATGGCGGCGGCGGCGGCGGCGGCGGCGGCGTAA}$ $(+1)$\n- $s_6 = \\texttt{ATGGGCGCCGCGGGCGCCGCGGGCGCCTGA}$ $(+1)$\n- $s_7 = \\texttt{TATATAAATAATATATATTTATATAATAATA}$ $(-1)$\n- $s_8 = \\texttt{AAATATATATTTAAATATATATATATAAAA}$ $(-1)$\n- $s_9 = \\texttt{TTTATATATAAATATAATATATTTATAAAT}$ $(-1)$\n- $s_{10} = \\texttt{AATAATAATATATTTATAAATAATATATAT}$ $(-1)$\n- $s_{11} = \\texttt{ATATATAAATATATAATATATAAATATATA}$ $(-1)$\n- $s_{12} = \\texttt{TATATATAAATAAATATATATATAAATATA}$ $(-1)$\n\n测试序列（带标签）：\n- $t_1 = \\texttt{ATGGCGGGCGGGCGACGGCTAA}$ $(+1)$\n- $t_2 = \\texttt{ATGGCCGCGGCTGGCGCTGCGTAG}$ $(+1)$\n- $t_3 = \\texttt{ATGGCGGCGGCGGCGGCGTGA}$ $(+1)$\n- $t_4 = \\texttt{AATATATATATAAATATATATAAATAATA}$ $(-1)$\n- $t_5 = \\texttt{TATATTTATAAATATATATAAATATTTAT}$ $(-1)$\n- $t_6 = \\texttt{AAATAATATATATATAAATAATATATATA}$ $(-1)$\n\n您的任务如下。\n\n1. 监督分类。对于每个指定的配对 $(k, C)$，为所有 $s \\in \\mathcal{D}_{\\mathrm{train}}$ 计算特征向量 $\\phi_k(s)$，通过最小化 $J(w,b)$ 来学习 $(w,b)$，然后使用 $\\hat{y}(s) = \\mathrm{sign}(w^\\top \\phi_k(s) + b)$ 计算在 $\\mathcal{D}_{\\mathrm{test}}$ 上的测试准确率（以小数表示）。\n2. 无监督聚类基线。对于每个指定的 $k$，为所有 $s \\in \\mathcal{D}_{\\mathrm{test}}$ 计算 $\\phi_k(s)$，并找到一个划分为恰好两个非空簇的方案，以最小化 $W(\\mathcal{C}_0,\\mathcal{C}_1)$。将簇映射到标签 $\\{-1,+1\\}$ 以最大化与 $\\mathcal{D}_{\\mathrm{test}}$ 上真实标签的一致性，并报告该最大一致性作为准确率（以小数表示）。\n\n测试套件。按此确切顺序运行以下五个案例：\n- 案例 1：监督学习，其中 $k = 3, C = 1$。\n- 案例 2：监督学习，其中 $k = 2, C = 1$。\n- 案例 3：监督学习，其中 $k = 3, C = 0.01$。\n- 案例 4：无监督学习，其中 $k = 3$，仅在测试集上。\n- 案例 5：无监督学习，其中 $k = 2$，仅在测试集上。\n\n答案规格和输出格式。\n- 对于每个案例，答案是一个等于 $\\mathcal{D}_{\\mathrm{test}}$ 上准确率的实数，以小数表示，四舍五入到小数点后恰好三位。\n- 您的程序应生成单行输出，其中包含五个结果，形式为用方括号括起来的逗号分隔列表，顺序与上述案例一致。例如，一个包含五个占位符值的输出应类似于 $[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4,\\alpha_5]$，其中每个 $\\alpha_i$ 是一个四舍五入到小数点后三位的实数。",
            "solution": "所提出的问题要求实现和评估两种不同的机器学习范式——监督分类和无监督聚类——用于区分蛋白质编码和非编码的脱氧核糖核酸 (DNA) 序列。该问题具有科学依据，定义明确，形式规范。它为完整的计算解决方案提供了所有必要的数据和定义。因此，这是一个有效的问题。解决方案首先构建特征表示，然后为两种范式实现指定的算法。\n\n两种方法的基本步骤都是将符号化的 DNA 序列转换为适合算法处理的定量数值格式。这是通过特征映射 $\\phi_k(s)$ 实现的，该映射将一个长度为 $L$ 的序列 $s$ 投射到 $\\mathbb{R}^{4^k}$ 中的一个实值向量。该向量的每个维度对应于字母表 $\\mathcal{A} = \\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$ 上 $4^k$ 个可能的长度为 $k$ 的字符串（称为 $k$-mer）之一。每个分量的值是相应 $k$-mer 在序列中的归一化频率。具体来说，对于一个 $k$-mer $m$，其频率是在大小为 $k$、步长为 $1$ 的滑动窗口内的计数，除以窗口总数，即 $L-k+1$。这建立了一个向量空间模型，其中每个序列都是一个点，学习算法可以利用这些点之间的几何关系。所有 $k$-mer 的集合 $\\mathcal{M}_k$ 按字典序排列，这定义了一个从 $k$-mer 到特征向量中索引的一致映射。这可以通过将字符 A, C, G, T 视为四进制数系统中的数字（例如，$A=0, C=1, G=2, T=3$）来实现。\n\n首先，我们处理监督分类任务。给定一个包含 $N$ 个序列及其已知标签 $y_i \\in \\{-1, +1\\}$ 的训练数据集 $\\mathcal{D}_{\\mathrm{train}}$。目标是学习一个线性决策函数 $f(s) = w^\\top \\phi_k(s) + b$，该函数可以预测新序列的标签。参数，即权重向量 $w \\in \\mathbb{R}^{4^k}$ 和标量偏置 $b \\in \\mathbb{R}$，通过最小化一个正则化经验风险函数来确定。指定的目标函数是：\n$$\nJ(w,b) = \\frac{1}{2} \\lVert w \\rVert_2^2 + C \\sum_{i=1}^{N} \\left( \\max\\left(0, 1 - y_i (w^\\top x_i + b) \\right) \\right)^2\n$$\n这里，$x_i = \\phi_k(s_i)$ 是第 $i$ 个训练序列的特征向量。第一项 $\\frac{1}{2} \\lVert w \\rVert_2^2$ 是一个 $\\ell_2$-正则化项，它惩罚大的权重以防止过拟合。第二项是训练集上平方合页损失的总和，它惩罚错误分类和裕量不足的正确分类。参数 $C > 0$ 控制正则化与拟合训练数据之间的权衡。这个目标函数 $J(w,b)$ 是凸函数且处处可微，确保存在唯一的全局最小值。我们可以使用基于梯度的优化算法，如 L-BFGS-B（带边界的有限内存 Broyden–Fletcher–Goldfarb–Shanno 算法）来找到这个最小值。为此，我们计算 $J(w,b)$ 关于 $w$ 和 $b$ 的偏导数。令 $\\mathcal{S}$ 为满足 $y_i(w^\\top x_i + b) < 1$ 的索引集合。梯度为：\n$$\n\\nabla_w J = w - 2C \\sum_{i \\in \\mathcal{S}} y_i (1 - y_i(w^\\top x_i + b)) x_i\n$$\n$$\n\\nabla_b J = -2C \\sum_{i \\in \\mathcal{S}} y_i (1 - y_i(w^\\top x_i + b))\n$$\n从一个初始猜测（例如，$w=0, b=0$）开始，优化器迭代更新参数以找到最优的 $(w^*, b^*)$。训练完成后，通过计算其在测试集 $\\mathcal{D}_{\\mathrm{test}}$ 上的准确率来评估模型的性能：即预测标签 $\\hat{y}(s) = \\mathrm{sign}(w^{*\\top} \\phi_k(s) + b^*)$ 与真实标签匹配的测试序列所占的比例。\n\n其次，我们处理无监督聚类基线。这种方法在学习阶段不使用标签。任务是将从测试集 $\\mathcal{D}_{\\mathrm{test}}$ 派生的特征向量 $\\{x_j\\}_{j=1}^M$ 划分为恰好两个非空簇 $\\mathcal{C}_0$ 和 $\\mathcal{C}_1$。该划分必须最小化簇内平方和 (WCSS)，这是 k-means 聚类的标准目标：\n$$\nW(\\mathcal{C}_0, \\mathcal{C}_1) = \\sum_{j \\in \\mathcal{C}_0} \\lVert x_j - \\mu_0 \\rVert_2^2 + \\sum_{j \\in \\mathcal{C}_1} \\lVert x_j - \\mu_1 \\rVert_2^2\n$$\n其中 $\\mu_0$ 和 $\\mu_1$ 是簇的质心（均值）。虽然对于一般的 k-means 找到最优划分是 NP-难问题，但测试集中的数据点数量很少（$M=6$）。这允许通过枚举所有可能的非平凡划分来获得精确解。将一个包含 $M$ 个项目的集合划分为两个非空子集的方法数由第二类斯特林数 $S(M, 2) = 2^{M-1} - 1$ 给出。对于 $M=6$，这是 $2^5 - 1 = 31$ 个唯一划分。我们可以遍历这些划分中的每一个，计算相应的 WCSS，并确定产生全局最小值的划分。在找到最优簇 $\\mathcal{C}_0^*$ 和 $\\mathcal{C}_1^*$ 后，我们通过将它们与真实标签进行比较来评估它们的质量。由于簇的身份（$0$ 和 $1$）是任意的，我们必须测试两种可能的到真实标签 $\\{-1, +1\\}$ 的映射：（$\\mathcal{C}_0^* \\to -1, \\mathcal{C}_1^* \\to +1$）和（$\\mathcal{C}_0^* \\to +1, \\mathcal{C}_1^* \\to -1$）。无监督准确率定义为这两种映射中实现的最大准确率。\n\n最终的步骤是为指定的参数 ($k, C$) 执行这两个算法框架，并报告所得的测试准确率，四舍五入到三位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the complete problem, including both supervised and unsupervised tasks.\n    \"\"\"\n    # Define the problem data\n    train_seqs = [\n        \"ATGGCGGCCGCGGGCGCCGCGGGCGACGGCTGA\", \"ATGCGCGCGCGGGCCGCGGCTGCGGCGTAG\",\n        \"ATGGGCGACGGCGGCGACGGCGGCGACTAA\", \"ATGGCCGCTGCGGCTGGCGCTGCGGCTTGA\",\n        \"ATGGCGGCGGCGGCGGCGGCGGCGGCGGCGTAA\", \"ATGGGCGCCGCGGGCGCCGCGGGCGCCTGA\",\n        \"TATATAAATAATATATATTTATATAATAATA\", \"AAATATATATTTAAATATATATATATAAAA\",\n        \"TTTATATATAAATATAATATATTTATAAAT\", \"AATAATAATATATTTATAAATAATATATAT\",\n        \"ATATATAAATATATAATATATAAATATATA\", \"TATATATAAATAAATATATATATAAATATA\"\n    ]\n    train_labels = np.array([1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1])\n\n    test_seqs = [\n        \"ATGGCGGGCGGGCGACGGCTAA\", \"ATGGCCGCGGCTGGCGCTGCGTAG\",\n        \"ATGGCGGCGGCGGCGGCGTGA\", \"AATATATATATAAATATATATAAATAATA\",\n        \"TATATTTATAAATATATATAAATATTTAT\", \"AAATAATATATATATAAATAATATATATA\"\n    ]\n    test_labels = np.array([1, 1, 1, -1, -1, -1])\n\n    alphabet_map = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n\n    def get_kmer_index(kmer):\n        \"\"\"Calculates the lexicographical index of a k-mer.\"\"\"\n        index = 0\n        for char in kmer:\n            index = index * 4 + alphabet_map[char]\n        return index\n\n    def phi_k(s, k):\n        \"\"\"Computes the k-mer frequency vector phi_k(s).\"\"\"\n        L = len(s)\n        dim = 4**k\n        if L  k:\n            return np.zeros(dim)\n        \n        counts = np.zeros(dim)\n        num_kmers = L - k + 1\n        \n        for i in range(num_kmers):\n            kmer = s[i:i+k]\n            idx = get_kmer_index(kmer)\n            counts[idx] += 1\n            \n        return counts / num_kmers\n\n    def objective_function(theta, X, y, C):\n        \"\"\"Computes J(w, b) and its gradient for L2-SVM.\"\"\"\n        N, D = X.shape\n        w = theta[:-1]\n        b = theta[-1]\n        \n        margins = y * (X.dot(w) + b)\n        loss_terms = 1 - margins\n        violations_mask = loss_terms > 0\n        \n        squared_hinge_loss = np.sum(loss_terms[violations_mask]**2)\n        objective_value = 0.5 * np.dot(w, w) + C * squared_hinge_loss\n        \n        grad = np.zeros_like(theta)\n        grad[:-1] = w\n        \n        if np.any(violations_mask):\n            common_grad_factor = -2 * C * loss_terms[violations_mask] * y[violations_mask]\n            grad[:-1] += np.dot(common_grad_factor, X[violations_mask, :])\n            grad[-1] += np.sum(common_grad_factor)\n        \n        return objective_value, grad\n\n    def solve_supervised(k, C):\n        \"\"\"Trains the SVM and computes test accuracy.\"\"\"\n        D = 4**k\n        X_train = np.array([phi_k(s, k) for s in train_seqs])\n        \n        initial_theta = np.zeros(D + 1)\n        res = minimize(\n            objective_function,\n            initial_theta,\n            args=(X_train, train_labels, C),\n            jac=True,\n            method='L-BFGS-B'\n        )\n        \n        w_opt, b_opt = res.x[:-1], res.x[-1]\n        \n        X_test = np.array([phi_k(s, k) for s in test_seqs])\n        \n        scores = X_test.dot(w_opt) + b_opt\n        predictions = np.sign(scores)\n        predictions[predictions == 0] = 1 # Per problem: sign(z>=0) = +1\n        \n        accuracy = np.mean(predictions == test_labels)\n        return accuracy\n\n    def solve_unsupervised(k):\n        \"\"\"Performs clustering and computes best-match accuracy.\"\"\"\n        M = len(test_seqs)\n        X_test = np.array([phi_k(s, k) for s in test_seqs])\n        \n        min_wcss = np.inf\n        best_partition = None\n        \n        # Enumerate all non-trivial partitions of M items into 2 clusters.\n        # Fix the first item in cluster 0 and iterate through assignments for the rest.\n        # This gives 2^(M-1) possibilities. The case where all items are in cluster 0\n        # (i=0) is excluded to ensure non-empty clusters.\n        num_items_to_partition = M - 1\n        for i in range(1, 2**num_items_to_partition):\n            c0_indices = [0]\n            c1_indices = []\n            \n            for j in range(num_items_to_partition):\n                if (i >> j)  1:\n                    c1_indices.append(j + 1)\n                else:\n                    c0_indices.append(j + 1)\n\n            C0 = X_test[c0_indices, :]\n            C1 = X_test[c1_indices, :]\n            \n            wcss = np.sum((C0 - np.mean(C0, axis=0))**2) + \\\n                   np.sum((C1 - np.mean(C1, axis=0))**2)\n\n            if wcss  min_wcss:\n                min_wcss = wcss\n                best_partition = (c0_indices, c1_indices)\n\n        c0_indices, c1_indices = best_partition\n        \n        # Mapping 1: C0 -> -1, C1 -> +1\n        pred1 = np.ones(M)\n        pred1[c0_indices] = -1\n        acc1 = np.mean(pred1 == test_labels)\n        \n        # Mapping 2: C0 -> +1, C1 -> -1\n        pred2 = np.ones(M)\n        pred2[c1_indices] = -1\n        acc2 = np.mean(pred2 == test_labels)\n        \n        return max(acc1, acc2)\n\n    # Execute all five test cases\n    results = [\n        solve_supervised(k=3, C=1.0),\n        solve_supervised(k=2, C=1.0),\n        solve_supervised(k=3, C=0.01),\n        solve_unsupervised(k=3),\n        solve_unsupervised(k=2)\n    ]\n    \n    # Format and print the final output\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在全局聚类的基础上，本练习将引入一种更强大的无监督技术——双聚类（biclustering），用于发现基因表达数据中的局部模式。你的任务是识别出在部分实验条件下表现出一致行为的基因子集，这是功能基因组学中的一个常见目标。通过实现均方残差（MSR）准则，你将在没有任何监督信号的情况下，获得发现隐藏且有意义的结构模式的宝贵实践经验。",
            "id": "2432840",
            "problem": "给定代表基因表达水平的小型实值矩阵，其中每行对应一个基因，每列对应一个实验条件。设矩阵表示为 $X \\in \\mathbb{R}^{G \\times C}$，其元素为 $x_{ij}$，其中行（基因）索引 $i \\in \\{0,1,\\ldots,G-1\\}$，列（条件）索引 $j \\in \\{0,1,\\ldots,C-1\\}$。一个双聚类（bicluster）是一个索引集对 $(I,J)$，其中 $I \\subset \\{0,1,\\ldots,G-1\\}$ 且 $J \\subset \\{0,1,\\ldots,C-1\\}$，并且满足 $|I| \\ge 2$ 和 $|J| \\ge 2$。\n\n对于任意双聚类 $(I,J)$，其均方残差（Mean Squared Residue, MSR）定义如下。设由 $I$ 和 $J$ 导出的子矩阵为 $\\{x_{ij} : i \\in I, j \\in J\\}$。定义行均值\n$$\n\\bar{x}_{iJ} = \\frac{1}{|J|} \\sum_{j \\in J} x_{ij} \\quad \\text{对于每个 } i \\in I,\n$$\n列均值\n$$\n\\bar{x}_{Ij} = \\frac{1}{|I|} \\sum_{i \\in I} x_{ij} \\quad \\text{对于每个 } j \\in J,\n$$\n以及总均值\n$$\n\\bar{x}_{IJ} = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} x_{ij}.\n$$\n在 $(i,j)$ 处的残差为\n$$\nr_{ij} = x_{ij} - \\bar{x}_{iJ} - \\bar{x}_{Ij} + \\bar{x}_{IJ},\n$$\n而 $(I,J)$ 的均方残差（MSR）为\n$$\n\\mathrm{MSR}(I,J) = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} r_{ij}^2.\n$$\n\n对于一个跨条件的二元标签向量 $y \\in \\{0,1\\}^C$，定义条件子集 $J$ 与 $y$ 的标签一致性分数如下：\n$$\nA(J,y) = \\frac{1}{C} \\max\\left\\{ \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 1) \\big), \\; \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 0) \\big) \\right\\},\n$$\n其中 $\\mathbf{1}(\\cdot)$ 表示指示函数。\n\n对于下方的每个测试用例，您将获得：\n- 一个矩阵 $X$，\n- 一个长度为 $C$ 的二元标签向量 $y$，\n- 一个候选双聚类的有限集 $\\mathcal{S}$，其中每个候选者是一个索引列表对 $(I,J)$，其元素严格递增且满足 $|I| \\ge 2$ 和 $|J| \\ge 2$。\n\n您在每个测试用例中的任务是，从 $\\mathcal{S}$ 中选择使 $\\mathrm{MSR}(I,J)$ 最小化的双聚类 $(I^\\star,J^\\star)$。如果 $\\mathrm{MSR}$ 值出现平局，则选择字典序最小的对，方法是首先比较 $I$，如有必要，再比较 $J$，使用严格递增整数列表上的标准字典序。\n\n对于每个测试用例中选定的 $(I^\\star,J^\\star)$，计算：\n- 基因数 $|I^\\star|$（一个整数），\n- 条件数 $|J^\\star|$（一个整数），\n- 最小 $\\mathrm{MSR}(I^\\star,J^\\star)$，作为一个四舍五入到小数点后六位的实数，\n- 一致性分数 $A(J^\\star,y)$，作为一个四舍五入到小数点后六位的实数。\n\n您的程序应生成单行输出，其中包含每个测试用例的这四个值，按顺序连接成一个用方括号括起来的逗号分隔列表，不含空格。换句话说，如果有 $T$ 个测试用例，并且对于测试用例 $t$，四个输出是 $(g_t, c_t, m_t, a_t)$，则单行输出必须是\n$$\n[ g_1, c_1, m_1, a_1, g_2, c_2, m_2, a_2, \\ldots, g_T, c_T, m_T, a_T ].\n$$\n所有实数必须精确到小数点后六位进行四舍五入。\n\n测试套件：\n\n- 测试用例 1：\n  - 由以下行给出的 $X$（$G=4, C=5$）：\n    - 行 0：$4,6,3,8,5$\n    - 行 1：$9,5,5,7,6$\n    - 行 2：$10,7,7,9,8$\n    - 行 3：$2,1,4,6,3$\n  - $y$：$0,0,1,1,0$\n  - $\\mathcal{S}$：\n    - $(I,J) = ([1,2],[2,3])$\n    - $(I,J) = ([0,1],[0,1])$\n    - $(I,J) = ([2,3],[1,4])$\n\n- 测试用例 2：\n  - 由以下行给出的 $X$（$G=3, C=3$）：\n    - 行 0：$1,2,3$\n    - 行 1：$2,3,4$\n    - 行 2：$3,4,5$\n  - $y$：$1,1,0$\n  - $\\mathcal{S}$：\n    - $(I,J) = ([0,1],[0,1])$\n    - $(I,J) = ([1,2],[1,2])$\n    - $(I,J) = ([0,2],[0,2])$\n\n- 测试用例 3：\n  - 由以下行给出的 $X$（$G=4, C=4$）：\n    - 行 0：$8,9,13,9$\n    - 行 1：$10,11,14,11$\n    - 行 2：$9,9,12,11$\n    - 行 3：$12,13,17,13$\n  - $y$：$1,1,0,1$\n  - $\\mathcal{S}$：\n    - $(I,J) = ([1,3],[0,2])$\n    - $(I,J) = ([0,2],[1,3])$\n    - $(I,J) = ([0,1],[2,3])$\n    - $(I,J) = ([2,3],[0,1])$\n\n所有索引都是从零开始的。不涉及物理单位。未使用角度。如上所述，将最终的实值结果表示为四舍五入到小数点后六位的小数。输出必须是单行，且严格遵循指定格式。",
            "solution": "用户提供了一个生物信息学领域的计算问题，该问题需要验证，如果有效，则需要提供解决方案。\n\n### 第 1 步：提取已知信息\n\n问题提供了以下定义和数据：\n\n- 一个基因表达矩阵 $X \\in \\mathbb{R}^{G \\times C}$，其中 $G$ 是基因数量，$C$ 是实验条件数量。\n- 一个双聚类（bicluster）是索引集对 $(I,J)$，其中 $I \\subset \\{0,1,\\ldots,G-1\\}$，$J \\subset \\{0,1,\\ldots,C-1\\}$，并且 $|I| \\ge 2$ 和 $|J| \\ge 2$。\n- 由 $(I,J)$ 导出的子矩阵是 $\\{x_{ij} : i \\in I, j \\in J\\}$。\n- 一个双聚类的行均值：$\\bar{x}_{iJ} = \\frac{1}{|J|} \\sum_{j \\in J} x_{ij}$，对于每个 $i \\in I$。\n- 一个双聚类的列均值：$\\bar{x}_{Ij} = \\frac{1}{|I|} \\sum_{i \\in I} x_{ij}$，对于每个 $j \\in J$。\n- 一个双聚类的总均值：$\\bar{x}_{IJ} = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} x_{ij}$。\n- 在 $(i,j)$ 处对一个双聚类的残差：$r_{ij} = x_{ij} - \\bar{x}_{iJ} - \\bar{x}_{Ij} + \\bar{x}_{IJ}$。\n- 一个双聚类 $(I,J)$ 的均方残差（Mean Squared Residue, MSR）：$\\mathrm{MSR}(I,J) = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} r_{ij}^2$。\n- 一个条件的二元标签向量：$y \\in \\{0,1\\}^C$。\n- 条件子集 $J$ 与 $y$ 的标签一致性分数：\n$$\nA(J,y) = \\frac{1}{C} \\max\\left\\{ \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 1) \\big), \\; \\sum_{j=0}^{C-1} \\mathbf{1}\\big( \\mathbf{1}(j \\in J) = \\mathbf{1}(y_j = 0) \\big) \\right\\}\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数。\n- 任务是，对于每个测试用例，从给定的候选有限集 $\\mathcal{S}$ 中选择使 $\\mathrm{MSR}(I,J)$ 最小化的双聚类 $(I^\\star,J^\\star)$。\n- 平局打破规则：如果 MSR 值相等，则选择字典序最小的对 $(I,J)$，方法是先比较 $I$ 再比较 $J$。索引列表 $I$ 和 $J$ 是严格递增的。\n- 对于选定的双聚类 $(I^\\star, J^\\star)$，必须计算四个值：$|I^\\star|$、$|J^\\star|$、$\\mathrm{MSR}(I^\\star,J^\\star)$ 和 $A(J^\\star,y)$。\n- 提供了三个具体的测试用例，每个用例都有一个矩阵 $X$、一个向量 $y$ 和一组候选双聚类 $\\mathcal{S}$。\n- 输出格式被指定为单行字符串：一个包含每个测试用例的四个计算值的逗号分隔列表，这些值按顺序连接，并用方括号括起来。实数必须四舍五入到小数点后六位。\n\n### 第 2 步：使用提取的已知信息进行验证\n\n对问题陈述进行验证。\n\n- **科学依据**：该问题基于基因表达数据的双聚类分析。均方残差（MSR）是加性类型双聚类的一种标准一致性度量，源自双向方差分析（ANOVA）模型。标签一致性分数是一种定义明确的度量，用于将一个特征（双聚类中的条件集）与外部类别标签进行比较。这些是生物信息学和计算统计学中的既定概念。该问题具有科学合理性。\n- **适定性**：任务要求从一个有限的候选集 $\\mathcal{S}$ 中，根据最小化一个定义明确的函数 MSR 来选择一个最优双聚类。提供了一个清晰无歧义的平局打破规则（字典序），这保证了解的存在性和唯一性。后续的计算是确定性的。该问题是适定的。\n- **客观性**：所有定义都以精确的数学术语给出。数据是数值的，评估标准是客观的。该问题不含主观或基于意见的陈述。\n- **完整性与一致性**：每个测试用例都提供了执行所需计算的所有必要信息（$X$、$y$、$\\mathcal{S}$）。不存在缺失的定义或矛盾的约束。\n- **可行性**：数据矩阵很小，包含标准的实数。计算在计算上是可行的。不存在物理上不可能的条件。\n- **结构性**：问题结构逻辑清晰。术语在使用前已定义。它不包含循环推理或歧义。\n\n### 第 3 步：结论与行动\n\n该问题在所有标准上均为**有效**。将提供一个合理的解决方案。\n\n### 解决方案\n\n任务是为每个测试用例从给定的候选集 $\\mathcal{S}$ 中找到最优的双聚类 $(I^\\star, J^\\star)$。优化标准是最小化均方残差（MSR），并采用字典序的平局打破规则。对于选定的双聚类，我们必须计算其维度、MSR 和一个标签一致性分数。\n\n每个测试用例的步骤如下：\n\n1.  初始化一个变量来存储迄今为止找到的最佳双聚类，例如 $(I_{best}, J_{best})$，及其 MSR 值 $MSR_{min}$。将 $MSR_{min}$ 初始化为一个大于任何可能 MSR 的值（例如，无穷大）。\n\n2.  遍历提供的候选集 $\\mathcal{S}$ 中的每个候选双聚类 $(I,J)$。\n\n3.  对于每个候选者 $(I,J)$，计算其 MSR。这需要几个步骤：\n    a.  从完整数据矩阵 $X$ 中提取子矩阵 $X_{I,J}$。该子矩阵的维度为 $|I| \\times |J|$。\n    b.  计算 $X_{I,J}$ 的每一行的均值，记为 $\\bar{x}_{iJ}$，其中 $i \\in I$。\n    c.  计算 $X_{I,J}$ 的每一列的均值，记为 $\\bar{x}_{Ij}$，其中 $j \\in J$。\n    d.  计算 $X_{I,J}$ 中所有元素的总均值，记为 $\\bar{x}_{IJ}$。\n    e.  对于子矩阵中的每个元素 $x_{ij}$（其中 $i \\in I, j \\in J$），计算残差 $r_{ij} = x_{ij} - \\bar{x}_{iJ} - \\bar{x}_{Ij} + \\bar{x}_{IJ}$。\n    f.  将 MSR 计算为平方残差的均值：$\\mathrm{MSR}(I,J) = \\frac{1}{|I|\\,|J|} \\sum_{i \\in I} \\sum_{j \\in J} r_{ij}^2$。\n\n4.  将计算出的 $\\mathrm{MSR}(I,J)$ 与当前最小值 $MSR_{min}$ 进行比较。\n    a.  如果 $\\mathrm{MSR}(I,J)  MSR_{min}$，则更新 $MSR_{min} = \\mathrm{MSR}(I,J)$ 并设置 $(I_{best}, J_{best}) = (I,J)$。\n    b.  如果 $\\mathrm{MSR}(I,J) = MSR_{min}$，应用平局打破规则。将当前候选者 $(I,J)$ 与 $(I_{best}, J_{best})$ 进行字典序比较。如果 $I$ 的字典序小于 $I'$，或者如果 $I=I'$ 且 $J$ 的字典序小于 $J'$，则对 $(I,J)$ 小于 $(I',J')$。如果 $(I,J)$ 更小，则更新 $(I_{best}, J_{best}) = (I,J)$。\n\n5.  遍历完 $\\mathcal{S}$ 中的所有候选者后，最终的 $(I_{best}, J_{best})$ 就是最优双聚类 $(I^\\star, J^\\star)$。\n\n6.  一旦确定了 $(I^\\star, J^\\star)$，计算四个所需的输出值：\n    a.  基因数：$|I^\\star|$。\n    b.  条件数：$|J^\\star|$。\n    c.  最小 MSR：$\\mathrm{MSR}(I^\\star, J^\\star)$。此值必须四舍五入到小数点后六位。\n    d.  标签一致性分数 $A(J^\\star, y)$。计算方法如下：\n        i.  设 $C$ 为原始矩阵 $X$ 的总列数。\n        ii. 令 $v_{J^\\star}$ 为一个长度为 $C$ 的二元向量，其中如果 $j \\in J^\\star$，则第 $j$ 个元素为 1，否则为 0。\n        iii. 计算直接一致性计数：$S_{direct} = \\sum_{j=0}^{C-1} \\mathbf{1}( (v_{J^\\star})_j = y_j )$。这是双聚类的条件成员关系与 $y$ 中标签匹配的位置数量。\n        iv. 计算反向一致性计数：$S_{inverted} = \\sum_{j=0}^{C-1} \\mathbf{1}( (v_{J^\\star})_j = (1-y_j) )$。这是双聚类的条件成员关系与反转标签匹配的位置数量。\n        v. 分数为 $A(J^\\star, y) = \\frac{1}{C} \\max(S_{direct}, S_{inverted})$。此值必须四舍五入到小数点后六位。\n\n7.  为当前测试用例收集这四个值。对所有测试用例重复整个过程。最后，将所有收集到的值格式化为指定的单个逗号分隔字符串。\n\n此过程在最终答案中提供的 Python 代码中实现。使用 `numpy` 库有助于高效地进行子矩阵提取和均值计算的数组操作。通过比较 $(\\mathrm{MSR}, I, J)$ 形式的元组（其中 $I$ 和 $J$ 本身是索引的元组）可以自然地处理平局打破逻辑。",
            "answer": "```python\nimport numpy as np\n\ndef calculate_msr(X, I, J):\n    \"\"\"\n    Calculates the Mean Squared Residue (MSR) for a given bicluster.\n    \n    Args:\n        X (np.ndarray): The full data matrix.\n        I (list or tuple): A list of row indices for the bicluster.\n        J (list or tuple): A list of column indices for the bicluster.\n        \n    Returns:\n        float: The MSR value.\n    \"\"\"\n    # Defensive check against empty indices\n    if not I or not J:\n        return float('inf')\n    \n    # Extract submatrix using np.ix_ for advanced indexing\n    submatrix = X[np.ix_(I, J)]\n    \n    num_rows, num_cols = submatrix.shape\n    if num_rows == 0 or num_cols == 0:\n        return float('inf')\n\n    # Calculate means\n    row_means = submatrix.mean(axis=1, keepdims=True)\n    col_means = submatrix.mean(axis=0, keepdims=True)\n    overall_mean = submatrix.mean()\n\n    # Calculate residues using broadcasting\n    residues = submatrix - row_means - col_means + overall_mean\n\n    # Calculate MSR\n    msr = np.mean(residues**2)\n    return msr\n\ndef calculate_agreement_score(J, y, C):\n    \"\"\"\n    Calculates the label-agreement score.\n    \n    Args:\n        J (list or tuple): The list of column indices for the selected bicluster.\n        y (np.ndarray): The binary label vector.\n        C (int): The total number of conditions (columns in X).\n    \n    Returns:\n        float: The label-agreement score.\n    \"\"\"\n    # Create a binary vector for the condition set J\n    v_J = np.zeros(C, dtype=int)\n    if J: # Ensure J is not empty\n        v_J[J] = 1\n\n    # Calculate direct and inverted agreement counts\n    direct_agreement = np.sum(v_J == y)\n    inverted_agreement = np.sum(v_J == (1 - y))\n\n    # The score is the max of the two agreements, normalized by C\n    score = max(direct_agreement, inverted_agreement) / C\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": np.array([\n                [4, 6, 3, 8, 5],\n                [9, 5, 5, 7, 6],\n                [10, 7, 7, 9, 8],\n                [2, 1, 4, 6, 3]\n            ], dtype=float),\n            \"y\": np.array([0, 0, 1, 1, 0], dtype=int),\n            \"S\": [\n                ([1, 2], [2, 3]),\n                ([0, 1], [0, 1]),\n                ([2, 3], [1, 4])\n            ]\n        },\n        {\n            \"X\": np.array([\n                [1, 2, 3],\n                [2, 3, 4],\n                [3, 4, 5]\n            ], dtype=float),\n            \"y\": np.array([1, 1, 0], dtype=int),\n            \"S\": [\n                ([0, 1], [0, 1]),\n                ([1, 2], [1, 2]),\n                ([0, 2], [0, 2])\n            ]\n        },\n        {\n            \"X\": np.array([\n                [8, 9, 13, 9],\n                [10, 11, 14, 11],\n                [9, 9, 12, 11],\n                [12, 13, 17, 13]\n            ], dtype=float),\n            \"y\": np.array([1, 1, 0, 1], dtype=int),\n            \"S\": [\n                ([1, 3], [0, 2]),\n                ([0, 2], [1, 3]),\n                ([0, 1], [2, 3]),\n                ([2, 3], [0, 1])\n            ]\n        }\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        X, y, S = case[\"X\"], case[\"y\"], case[\"S\"]\n        _, C = X.shape\n\n        best_bicluster_info = (float('inf'), (), ())\n\n        for I, J in S:\n            msr = calculate_msr(X, I, J)\n            # Use tuple comparison for MSR and lexicographical tie-breaking\n            # The structure for comparison is (MSR, I_tuple, J_tuple)\n            current_bicluster_info = (msr, tuple(I), tuple(J))\n\n            if current_bicluster_info  best_bicluster_info:\n                best_bicluster_info = current_bicluster_info\n\n        min_msr, I_star_tuple, J_star_tuple = best_bicluster_info\n        I_star = list(I_star_tuple)\n        J_star = list(J_star_tuple)\n\n        # Compute the four required values for the best bicluster\n        gene_count = len(I_star)\n        condition_count = len(J_star)\n        agreement_score = calculate_agreement_score(J_star, y, C)\n\n        # Format floating-point numbers to six decimal places\n        msr_str = f\"{min_msr:.6f}\"\n        agreement_str = f\"{agreement_score:.6f}\"\n\n        final_results.extend([gene_count, condition_count, msr_str, agreement_str])\n\n    # Print the final output in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后一个练习展示了在典型的生物信息学工作流程中，监督学习与无监督学习之间强大的协同作用。你将首先使用一种无监督方法——主成分分析（PCA），来学习高维基因表达数据的紧凑且有意义的表示。然后，你将把这些学习到的新特征输入一个监督模型中，用以预测临床结果，从而阐明无监督特征提取如何能够显著提升预测模型的性能和可解释性。",
            "id": "2432878",
            "problem": "您需要编写一个完整、可运行的程序，该程序对多个测试用例的模拟基因表达数据执行一个两阶段学习流程，该流程结合了无监督表示学习阶段和有监督预测阶段。目标和数据生成过程在下面进行了数学规定，您的程序必须精确地实现这些定义。\n\n数据生成过程。对于每个测试用例，按如下方式生成一个训练集和一个独立的测试集。设训练患者数量为 $n_{\\mathrm{train}}$，测试患者数量为 $n_{\\mathrm{test}}$，基因数量为 $p$，真实潜在维度为 $r^\\star$。设学习器使用的无监督潜在维度为 $k$，其中 $k$ 是一个在 $0 \\le k \\le \\min\\{p,n_{\\mathrm{train}}-1\\}$ 范围内的整数。为每个测试用例固定一个基础随机种子 $s$ 以确保可复现性。\n\n- 使用基础种子 $s$，抽取一个基因载荷矩阵 $W \\in \\mathbb{R}^{p \\times r^\\star}$，其元素为独立的标准正态分布。\n- 使用相同的基础种子 $s$ 并继续生成值序列，抽取一个生存系数向量 $b \\in \\mathbb{R}^{r^\\star}$，其元素为独立的标准正态分布。\n- 使用相同的基础种子 $s$ 并继续生成值序列，抽取训练潜在因子 $Z_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times r^\\star}$，其元素为独立的标准正态分布。\n- 使用相同的基础种子 $s$ 并继续生成值序列，抽取训练基因表达噪声 $E^{(X)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$，其元素独立分布为 $\\mathcal{N}(0,\\sigma_X^2)$。\n- 使用相同的基础种子 $s$ 并继续生成值序列，抽取训练生存噪声 $e^{(y)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$，其元素独立分布为 $\\mathcal{N}(0,\\sigma_y^2)$。\n- 将训练基因表达定义为 $X_{\\mathrm{train}} = Z_{\\mathrm{train}} W^\\top + E^{(X)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times p}$。\n- 将训练生存时间定义为 $y_{\\mathrm{train}} = Z_{\\mathrm{train}} b + e^{(y)}_{\\mathrm{train}} \\in \\mathbb{R}^{n_{\\mathrm{train}}}$。\n\n对于测试集，使用与上述相同的 $W$ 和 $b$，但使用独立的潜在因子和噪声：\n- 使用测试种子 $s+1$ 分别抽取 $Z_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times r^\\star}$（其元素为独立的标准正态分布）、$E^{(X)}_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}} \\times p}$（其元素独立分布为 $\\mathcal{N}(0,\\sigma_X^2)$）和 $e^{(y)}_{\\mathrm{test}} \\in \\mathbb{R}^{n_{\\mathrm{test}}}$（其元素独立分布为 $\\mathcal{N}(0,\\sigma_y^2)$）。\n- 将测试基因表达和生存定义为 $X_{\\mathrm{test}} = Z_{\\mathrm{test}} W^\\top + E^{(X)}_{\\mathrm{test}}$ 和 $y_{\\mathrm{test}} = Z_{\\mathrm{test}} b + e^{(y)}_{\\mathrm{test}}$。\n\n两阶段学习流程。您的程序必须对训练数据执行以下两个优化问题，然后在测试数据上进行评估。\n\n1. 无监督表示学习。通过求解以下问题来学习一个线性编码器 $E: \\mathbb{R}^p \\to \\mathbb{R}^k$ 和一个线性解码器 $D: \\mathbb{R}^k \\to \\mathbb{R}^p$，以及一个中心化向量 $\\mu \\in \\mathbb{R}^p$：\n$$\n\\min_{E,D,\\mu} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left\\| x_i - D\\!\\left(E\\!\\left(x_i - \\mu\\right)\\right) \\right\\|_2^2,\n$$\n其中 $x_i \\in \\mathbb{R}^p$ 表示 $X_{\\mathrm{train}}$ 的第 $i$ 行。学习到的编码器 $E$ 必须是线性的，学习到的解码器 $D$ 也必须是线性的。使用学习到的编码器和中心化向量为每个训练样本 $i \\in \\{1,\\dots,n_{\\mathrm{train}}\\}$ 生成潜在表示 $z_i^{(\\mathrm{lat})} = E(x_i - \\mu) \\in \\mathbb{R}^k$，并为每个测试样本 $j \\in \\{1,\\dots,n_{\\mathrm{test}}\\}$ 使用从训练数据中估计出的相同 $E$ 和 $\\mu$ 来生成 $z_j^{(\\mathrm{lat,test})} = E(x^{(\\mathrm{test})}_j - \\mu)$。\n\n2. 有监督预测。通过求解以下问题来学习一个形式为 $g(z) = c^\\top z + d$ 的仿射预测器 $g: \\mathbb{R}^k \\to \\mathbb{R}$：\n$$\n\\min_{c \\in \\mathbb{R}^k, \\ d \\in \\mathbb{R}} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( y_i - \\left(c^\\top z_i^{(\\mathrm{lat})} + d\\right) \\right)^2,\n$$\n其中 $y_i$ 表示 $y_{\\mathrm{train}}$ 的第 $i$ 个条目。使用学习到的 $(c,d)$，为每个测试样本 $j$ 计算预测值 $\\hat{y}_j = c^\\top z_j^{(\\mathrm{lat,test})} + d$，并评估测试均方误差\n$$\n\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=1}^{n_{\\mathrm{test}}} \\left( \\hat{y}_j - y^{(\\mathrm{test})}_j \\right)^2.\n$$\n\n测试套件。您的程序必须实现上述过程，并使用所描述的精确生成和训练程序，为以下每个参数设置输出测试均方误差。每个测试用例是一个元组 $(s, n_{\\mathrm{train}}, n_{\\mathrm{test}}, p, r^\\star, k, \\sigma_X, \\sigma_y)$：\n\n- 测试用例 1：(7, 120, 80, 60, 5, 5, 0.3, 0.5).\n- 测试用例 2：(13, 150, 150, 80, 6, 6, 2.0, 2.0).\n- 测试用例 3：(21, 50, 50, 200, 3, 3, 0.5, 0.5).\n- 测试用例 4：(1, 100, 100, 70, 8, 2, 0.4, 0.4).\n- 测试用例 5：(99, 100, 100, 50, 4, 0, 0.5, 0.5).\n\n答案规格和输出格式。您的程序必须生成单行输出，其中包含按上述顺序列出的测试用例的测试均方误差列表。每个值必须四舍五入到恰好 $6$ 位小数。输出必须是方括号括起来的逗号分隔列表，例如 $[\\mathrm{mse}_1,\\mathrm{mse}_2,\\dots]$，不得包含多余的空格或文本。",
            "solution": "问题陈述是有效的。它具有科学依据、是适定的、客观的，并为计算实验提供了完整且一致的规范。该任务涉及将一个标准的两阶段机器学习流程应用于模拟数据，这是生物信息学研究中评估方法的常见做法。数据生成过程是一个线性因子模型，学习算法是主成分分析和普通最小二乘回归，这两种算法都是基础且易于理解的。所有测试用例的参数都在有效范围内。因此，我们可以着手解决问题。\n\n问题要求实现一个两阶段学习流程。第一阶段是无监督的，学习基因表达数据的低维表示。第二阶段是有监督的，使用这个学习到的表示来预测临床结果（生存时间）。\n\n**阶段 1：无监督表示学习**\n\n第一个优化问题是：\n$$\n\\min_{E,D,\\mu} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left\\| x_i - D\\!\\left(E\\!\\left(x_i - \\mu\\right)\\right) \\right\\|_2^2\n$$\n其中 $E: \\mathbb{R}^p \\to \\mathbb{R}^k$ 和 $D: \\mathbb{R}^k \\to \\mathbb{R}^p$ 是线性变换，$\\mu \\in \\mathbb{R}^p$ 是一个中心化向量。这个目标函数旨在最小化线性自编码器的重建误差。这个问题的解由主成分分析 (PCA) 给出。\n\n首先，最优中心化向量 $\\mu$ 是训练数据向量 $x_i$ 的经验均值：\n$$\n\\mu = \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} x_i = \\frac{1}{n_{\\mathrm{train}}} X_{\\mathrm{train}}^\\top \\mathbf{1}_{n_{\\mathrm{train}}}\n$$\n令 $\\bar{X}_{\\mathrm{train}} = X_{\\mathrm{train}} - \\mathbf{1}_{n_{\\mathrm{train}}} \\mu^\\top$ 为中心化的训练数据矩阵。优化问题简化为寻找 $\\bar{X}_{\\mathrm{train}}$ 的最佳秩-$k$ 近似。根据 Eckart-Young-Mirsky 定理，这可以通过对 $\\bar{X}_{\\mathrm{train}}$ 进行奇异值分解 (SVD) 来实现。\n\n设中心化数据矩阵的 SVD 为 $\\bar{X}_{\\mathrm{train}} = U S V^\\top$，其中 $V \\in \\mathbb{R}^{p \\times p}$ 的列是主成分（样本协方差矩阵的特征向量）。令 $V_k \\in \\mathbb{R}^{p \\times k}$ 是一个矩阵，其列是前 $k$ 个主成分，对应于 $k$ 个最大的奇异值。\n\n线性编码器 $E$ 将中心化数据投影到由这 $k$ 个成分定义的基上。其矩阵表示为 $V_k^\\top$。训练样本的学习到的潜在表示为：\n$$\nZ^{(\\mathrm{lat})} = \\bar{X}_{\\mathrm{train}} V_k \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times k}\n$$\n对于测试数据 $X_{\\mathrm{test}}$，潜在表示是通过首先使用训练集中的均值 $\\mu$ 对数据进行中心化，然后应用相同的投影来计算的：\n$$\nZ^{(\\mathrm{lat,test})} = (X_{\\mathrm{test}} - \\mathbf{1}_{n_{\\mathrm{test}}} \\mu^\\top) V_k\n$$\n\n**阶段 2：有监督预测**\n\n第二阶段涉及通过在训练数据上求解以下普通最小二乘 (OLS) 问题来学习一个仿射预测器 $g(z) = c^\\top z + d$：\n$$\n\\min_{c \\in \\mathbb{R}^k, \\ d \\in \\mathbb{R}} \\ \\frac{1}{n_{\\mathrm{train}}} \\sum_{i=1}^{n_{\\mathrm{train}}} \\left( y_i - \\left(c^\\top z_i^{(\\mathrm{lat})} + d\\right) \\right)^2\n$$\n为解决此问题，我们可以用一列 1 来增广潜在表示矩阵 $Z^{(\\mathrm{lat})}$，以考虑截距项 $d$。令 $\\tilde{Z}^{(\\mathrm{lat})} = [Z^{(\\mathrm{lat})} \\ \\mathbf{1}_{n_{\\mathrm{train}}}] \\in \\mathbb{R}^{n_{\\mathrm{train}} \\times (k+1)}$ 和 $\\tilde{c} = [c^\\top, d]^\\top \\in \\mathbb{R}^{k+1}$。目标变为最小化 $\\|\\mathbf{y}_{\\mathrm{train}} - \\tilde{Z}^{(\\mathrm{lat})} \\tilde{c}\\|_2^2$。\n\n通过求解正规方程可以找到解，得到：\n$$\n\\tilde{c}_{\\mathrm{opt}} = (\\tilde{Z}^{(\\mathrm{lat})\\top} \\tilde{Z}^{(\\mathrm{lat})})^\\dagger \\tilde{Z}^{(\\mathrm{lat})\\top} \\mathbf{y}_{\\mathrm{train}}\n$$\n其中 $\\dagger$ 表示 Moore-Penrose 伪逆，即使矩阵不可逆，它也能提供一个稳定的解。\n\n当 $k=0$ 时会出现一个特殊情况。此时，潜在空间是平凡的（零维），因此 $Z^{(\\mathrm{lat})}$ 是一个空矩阵。模型简化为 $g(z)=d$。优化问题变为 $\\min_d \\sum_i (y_i - d)^2$，其解是训练标签的均值：$d = \\bar{y}_{\\mathrm{train}} = \\frac{1}{n_{\\mathrm{train}}} \\sum_i y_i$。\n\n**评估**\n\n最后，使用学习到的系数 $\\tilde{c}_{\\mathrm{opt}} = [c_{\\mathrm{opt}}^\\top, d_{\\mathrm{opt}}]^\\top$ 和测试潜在表示 $Z^{(\\mathrm{lat,test})}$ 来生成测试集的预测。对于 $k  0$，我们形成一个增广的测试矩阵 $\\tilde{Z}^{(\\mathrm{lat,test})} = [Z^{(\\mathrm{lat,test})} \\ \\mathbf{1}_{n_{\\mathrm{test}}}]$，预测值为 $\\hat{\\mathbf{y}}_{\\mathrm{test}} = \\tilde{Z}^{(\\mathrm{lat,test})} \\tilde{c}_{\\mathrm{opt}}$。对于 $k=0$，每个测试样本的预测值就是常数值 $\\hat{y}_j = d_{\\mathrm{opt}} = \\bar{y}_{\\mathrm{train}}$。\n\n性能使用测试均方误差 (MSE) 进行评估：\n$$\n\\mathrm{MSE}_{\\mathrm{test}} = \\frac{1}{n_{\\mathrm{test}}} \\sum_{j=1}^{n_{\\mathrm{test}}} \\left( \\hat{y}_j - y^{(\\mathrm{test})}_j \\right)^2\n$$\n以下程序为每个指定的测试用例实现了这整个过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_pipeline(s, n_train, n_test, p, r_star, k, sigma_X, sigma_y):\n    \"\"\"\n    Implements the full data generation and two-stage learning pipeline for a single test case.\n    \n    Args:\n        s (int): Base random seed.\n        n_train (int): Number of training patients.\n        n_test (int): Number of test patients.\n        p (int): Number of genes.\n        r_star (int): True latent dimension.\n        k (int): Unsupervised latent dimension for the learner.\n        sigma_X (float): Standard deviation of gene expression noise.\n        sigma_y (float): Standard deviation of survival noise.\n        \n    Returns:\n        float: The test mean squared error.\n    \"\"\"\n    \n    # --- Data Generation ---\n    # Training data generation\n    rng_train = np.random.default_rng(s)\n    W = rng_train.standard_normal(size=(p, r_star))\n    b = rng_train.standard_normal(size=r_star)\n    Z_train = rng_train.standard_normal(size=(n_train, r_star))\n    E_X_train = rng_train.normal(scale=sigma_X, size=(n_train, p))\n    e_y_train = rng_train.normal(scale=sigma_y, size=n_train)\n    \n    X_train = Z_train @ W.T + E_X_train\n    y_train = Z_train @ b + e_y_train\n    \n    # Test data generation\n    rng_test = np.random.default_rng(s + 1)\n    Z_test = rng_test.standard_normal(size=(n_test, r_star))\n    E_X_test = rng_test.normal(scale=sigma_X, size=(n_test, p))\n    e_y_test = rng_test.normal(scale=sigma_y, size=n_test)\n    \n    X_test = Z_test @ W.T + E_X_test\n    y_test = Z_test @ b + e_y_test\n\n    # --- Two-Stage Learning Pipeline ---\n\n    # Stage 1: Unsupervised Representation Learning (PCA)\n    mu = np.mean(X_train, axis=0)\n    X_train_centered = X_train - mu\n    \n    if k > 0:\n        # SVD on centered training data\n        # full_matrices=False is more efficient\n        _, _, Vt = np.linalg.svd(X_train_centered, full_matrices=False)\n        \n        # Encoder is based on the top k principal components (right singular vectors)\n        # Vt is V.T, so we take the first k rows and transpose\n        Vk = Vt[:k, :].T  # Shape: (p, k)\n        \n        # Get latent representations for training and test sets\n        Z_lat_train = X_train_centered @ Vk\n        X_test_centered = X_test - mu\n        Z_lat_test = X_test_centered @ Vk\n    \n    # Stage 2: Supervised Prediction (OLS)\n    if k == 0:\n        # If k=0, the model is y = d. The best predictor is the mean of y_train.\n        d = np.mean(y_train)\n        y_hat_test = np.full(n_test, d)\n    else:\n        # Augment latent features with a column of ones for the intercept\n        Z_train_aug = np.c_[Z_lat_train, np.ones(n_train)]\n        \n        # Solve for coefficients (c and d) using least squares\n        # coeffs will contain [c_1, ..., c_k, d]\n        coeffs, _, _, _ = np.linalg.lstsq(Z_train_aug, y_train, rcond=None)\n        \n        # Predict on the test set\n        Z_test_aug = np.c_[Z_lat_test, np.ones(n_test)]\n        y_hat_test = Z_test_aug @ coeffs\n\n    # --- Evaluation ---\n    mse_test = np.mean((y_hat_test - y_test)**2)\n    return mse_test\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test cases: (s, n_train, n_test, p, r_star, k, sigma_X, sigma_y)\n    test_cases = [\n        (7, 120, 80, 60, 5, 5, 0.3, 0.5),\n        (13, 150, 150, 80, 6, 6, 2.0, 2.0),\n        (21, 50, 50, 200, 3, 3, 0.5, 0.5),\n        (1, 100, 100, 70, 8, 2, 0.4, 0.4),\n        (99, 100, 100, 50, 4, 0, 0.5, 0.5),\n    ]\n\n    results = []\n    for params in test_cases:\n        mse = solve_pipeline(*params)\n        results.append(mse)\n\n    # Format the output as specified: a comma-separated list in brackets,\n    # with each value rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}