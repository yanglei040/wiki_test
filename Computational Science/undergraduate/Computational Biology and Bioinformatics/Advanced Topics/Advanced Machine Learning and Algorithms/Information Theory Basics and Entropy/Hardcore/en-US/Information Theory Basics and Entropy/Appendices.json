{
    "hands_on_practices": [
        {
            "introduction": "Information theory provides a powerful framework for quantifying uncertainty. We begin with the most fundamental question: how much information is needed to resolve uncertainty completely? This practice exercise uses a simplified, hypothetical scenario from genomics—pinpointing the location of a TATA box motif within a DNA sequence—to introduce the concept of self-information and establish the direct relationship between the number of possibilities and the bits required to specify a single outcome .",
            "id": "2399714",
            "problem": "In a promoter annotation task within computational genomics, consider a contiguous segment of Deoxyribonucleic Acid (DNA) of length $10{,}000$ base pairs (bp). The segment is known to contain exactly one TATA box motif, and for this purpose the TATA box is taken to span $6$ bp in length (the canonical core $\\text{TATAAA}$). Define the \"location\" of the TATA box as the index of its first base measured from the $5'$ end of this segment on the given strand. Assume that all admissible start indices that keep the entire $6$ bp motif within the $10{,}000$ bp segment are a priori equally likely, and that there is no additional side information.\n\nDetermine the minimum amount of information required to uniquely specify the TATA box location under these assumptions. Round your answer to four significant figures and express the result in bits.",
            "solution": "Let $L$ denote the length of the Deoxyribonucleic Acid (DNA) segment in base pairs, and let $m$ denote the length of the TATA box motif in base pairs. A start index is admissible if the entire motif lies within the segment. Therefore, the number of admissible start positions is\n$$\nN \\;=\\; L - m + 1 \\, .\n$$\nWith the assumptions given, all $N$ locations are a priori equally likely. The probability of any specific location is therefore\n$$\np \\;=\\; \\frac{1}{N} \\, .\n$$\nBy the fundamental definition in Shannon information theory, the self-information (in bits) of an outcome with probability $p$ is\n$$\nI \\;=\\; - \\log_{2}(p) \\, .\n$$\nSubstituting $p = 1/N$ gives\n$$\nI \\;=\\; \\log_{2}(N) \\;=\\; \\log_{2}\\!\\big(L - m + 1\\big) \\, .\n$$\nWith $L = 10{,}000$ and $m = 6$, we have\n$$\nN \\;=\\; 10{,}000 - 6 + 1 \\;=\\; 9{,}995 \\, ,\n$$\nso the information required is\n$$\nI \\;=\\; \\log_{2}(9{,}995) \\, .\n$$\nEvaluating this logarithm,\n$$\nI \\;\\approx\\; 13.286990852\\ldots \\, \\text{bits} \\, .\n$$\nRounded to four significant figures (as required), the information is\n$$\n13.29 \\, \\text{bits} \\, .\n$$",
            "answer": "$$\\boxed{13.29}$$"
        },
        {
            "introduction": "Building upon the idea of information in a single event, we now consider the average information content of a system with multiple outcomes, each having a different likelihood. This exercise introduces Shannon entropy, a cornerstone of information theory, which measures the expected \"surprise\" or uncertainty of a random variable. By calculating the entropy of a hypothetical distribution of cell types in the human body, you will learn how to quantify the complexity of a biological system where some components are common and others are rare .",
            "id": "2399729",
            "problem": "A single-cell RNA sequencing (scRNA-seq) atlas of a healthy adult human aggregates cell counts across all organs and assigns each cell to one of eight broad cell-type categories. Treat the type of a randomly sampled cell from the whole-body atlas as a discrete random variable. For this exercise, suppose the atlas estimates the following fractions for the eight categories (each value is a probability and the list sums to $1$):\n- Erythrocyte (red blood cell): $0.80$\n- Neutrophil: $0.05$\n- Lymphocyte: $0.04$\n- Endothelial cell: $0.03$\n- Epithelial cell: $0.03$\n- Fibroblast: $0.025$\n- Hepatocyte: $0.015$\n- Neuron: $0.01$\n\nUsing only the foundational definitions of self-information and the entropy of a discrete random variable from information theory, derive the expected number of binary digits needed to specify the cell’s type when one cell is drawn at random from this atlas. Express your final answer in bits, and round to four significant figures.",
            "solution": "The problem avers that it is solvable using only the foundational definitions of self-information and the entropy of a discrete random variable from information theory. I will validate this claim.\nThe problem requires the calculation of the expected number of binary digits needed to specify the type of a randomly sampled cell. In the framework of information theory, this quantity is precisely the Shannon entropy of the probability distribution of cell types, with the entropy measured in units of bits.\n\nLet $X$ be a discrete random variable representing the cell type. The set of possible outcomes for $X$ is $\\mathcal{X} = \\{x_1, x_2, \\dots, x_8\\}$, where each $x_i$ corresponds to one of the eight specified cell types. The problem provides the probability mass function, $P(X=x_i) = p_i$, for each state $x_i$. The given probabilities are:\n- $p_1 = P(x_1=\\text{Erythrocyte}) = 0.80$\n- $p_2 = P(x_2=\\text{Neutrophil}) = 0.05$\n- $p_3 = P(x_3=\\text{Lymphocyte}) = 0.04$\n- $p_4 = P(x_4=\\text{Endothelial cell}) = 0.03$\n- $p_5 = P(x_5=\\text{Epithelial cell}) = 0.03$\n- $p_6 = P(x_6=\\text{Fibroblast}) = 0.025$\n- $p_7 = P(x_7=\\text{Hepatocyte}) = 0.015$\n- $p_8 = P(x_8=\\text{Neuron}) = 0.01$\n\nIt is a necessary condition that these probabilities sum to unity: $\\sum_{i=1}^{8} p_i = 0.80 + 0.05 + 0.04 + 0.03 + 0.03 + 0.025 + 0.015 + 0.01 = 1.00$. This condition is satisfied.\n\nThe foundational definition of self-information, or \"surprisal,\" quantifies the information content of a single, specific outcome. For an outcome $x_i$ with probability $p_i$, the self-information $I(x_i)$ is defined as:\n$$I(x_i) = -\\log_b(p_i)$$\nThe base of the logarithm, $b$, determines the unit of information. The problem requests the result in \"binary digits,\" which corresponds to the unit \"bits.\" This requires the use of a base-$2$ logarithm, so $b=2$. Thus, the self-information of outcome $x_i$ is:\n$$I(x_i) = -\\log_2(p_i)$$\n\nThe entropy of the discrete random variable $X$, denoted by $H(X)$, is defined as the expected value of the self-information, computed over all possible outcomes. This represents the average information content of the distribution, which is equivalent to the average number of bits required for an optimal encoding of the outcomes of the random variable. The definition is:\n$$H(X) = E[I(X)] = \\sum_{i \\in \\mathcal{X}} P(X=x_i) I(x_i)$$\nSubstituting the specific definitions for $P(X=x_i)$ and $I(x_i)$ yields the formula for Shannon entropy:\n$$H(X) = \\sum_{i=1}^{8} p_i (-\\log_2(p_i)) = -\\sum_{i=1}^{8} p_i \\log_2(p_i)$$\nThis is precisely the quantity we must compute.\n\nWe substitute the given numerical probabilities into the entropy formula:\n$$H(X) = - \\left[ 0.80 \\log_2(0.80) + 0.05 \\log_2(0.05) + 0.04 \\log_2(0.04) + 2 \\times (0.03 \\log_2(0.03)) + 0.025 \\log_2(0.025) + 0.015 \\log_2(0.015) + 0.01 \\log_2(0.01) \\right]$$\nWe calculate the value of each term $p_i \\log_2(p_i)$:\n- $0.80 \\log_2(0.80) \\approx 0.80 \\times (-0.321928) \\approx -0.257542$\n- $0.05 \\log_2(0.05) \\approx 0.05 \\times (-4.321928) \\approx -0.216096$\n- $0.04 \\log_2(0.04) \\approx 0.04 \\times (-4.643856) \\approx -0.185754$\n- $0.03 \\log_2(0.03) \\approx 0.03 \\times (-5.058894) \\approx -0.151767$\n- $0.025 \\log_2(0.025) \\approx 0.025 \\times (-5.321928) \\approx -0.133048$\n- $0.015 \\log_2(0.015) \\approx 0.015 \\times (-6.058894) \\approx -0.090883$\n- $0.01 \\log_2(0.01) \\approx 0.01 \\times (-6.643856) \\approx -0.066439$\n\nNow, we sum these values and apply the negative sign from the formula:\n$$H(X) \\approx - [ -0.257542 - 0.216096 - 0.185754 + 2 \\times (-0.151767) - 0.133048 - 0.090883 - 0.066439 ]$$\n$$H(X) \\approx - [ -0.257542 - 0.216096 - 0.185754 - 0.303534 - 0.133048 - 0.090883 - 0.066439 ]$$\n$$H(X) \\approx - [ -1.253296 ]$$\n$$H(X) \\approx 1.253296 \\text{ bits}$$\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $1$, $2$, $5$, and $3$. The fifth digit is $2$, so we round down.\n$$H(X) \\approx 1.253 \\text{ bits}$$\nThis value represents the theoretical lower bound on the average number of bits required to encode the cell type of a randomly drawn cell from this atlas.",
            "answer": "$$\n\\boxed{1.253}\n$$"
        },
        {
            "introduction": "After learning to measure the information within a single system, a natural next step is to compare two different systems or the same system at two different times. This practice introduces the Kullback-Leibler (KL) divergence, a measure of how one probability distribution differs from a second, reference distribution. Using a highly relevant scenario from microbiome research—analyzing compositional changes after antibiotic treatment—you will learn how to quantify the magnitude of perturbation in a complex biological community .",
            "id": "2399737",
            "problem": "A patient's gut microbiome composition is profiled at two time points: before and after an antibiotic treatment. Each profile is summarized by counts of sequencing reads assigned to the same set of taxa, producing two nonnegative integer vectors that can be viewed as empirical distributions over taxa. Let the pre-treatment count vector be $\\mathbf{x} \\in \\mathbb{N}_0^m$ and the post-treatment count vector be $\\mathbf{y} \\in \\mathbb{N}_0^m$, where $m$ is the number of taxa and the $i$-th entry is the count for the $i$-th taxon. For a given additive smoothing parameter $\\lambda \\geq 0$, define the smoothed relative abundance distributions $\\mathbf{p}$ and $\\mathbf{q}$ by\n$$\np_i \\;=\\; \\frac{x_i + \\lambda}{\\sum_{j=1}^{m} (x_j + \\lambda)} \\quad \\text{and} \\quad q_i \\;=\\; \\frac{y_i + \\lambda}{\\sum_{j=1}^{m} (y_j + \\lambda)} \\quad \\text{for } i=1,\\dots,m.\n$$\nUsing the natural logarithm, the Kullback–Leibler (KL) divergence from $\\mathbf{p}$ to $\\mathbf{q}$ in nats is defined by\n$$\nD_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) \\;=\\; \\sum_{i=1}^{m} p_i \\,\\ln\\!\\left(\\frac{p_i}{q_i}\\right).\n$$\nAssume that the vectors $\\mathbf{x}$ and $\\mathbf{y}$ list the same taxa in the same order. For each test case below, compute $D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q})$ using the specified $\\lambda$ and express each result as a real number rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$: $m = 2$, $\\mathbf{x} = (2,2)$, $\\mathbf{y} = (2,2)$, $\\lambda = 0.5$.\n- Case $2$: $m = 2$, $\\mathbf{x} = (3,1)$, $\\mathbf{y} = (2,2)$, $\\lambda = 0$.\n- Case $3$: $m = 2$, $\\mathbf{x} = (1,0)$, $\\mathbf{y} = (0,1)$, $\\lambda = 0.5$.\n- Case $4$: $m = 2$, $\\mathbf{x} = (1,1)$, $\\mathbf{y} = (0,2)$, $\\lambda = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases (for example, $[r_1,r_2,r_3,r_4]$), with each $r_k$ rounded to $6$ decimal places and no spaces.",
            "solution": "The posed problem requires the computation of the Kullback-Leibler (KL) divergence between two smoothed probability distributions derived from microbiome count data. Before proceeding to the solution, a rigorous validation of the problem statement is necessary.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n- Pre-treatment count vector: $\\mathbf{x} \\in \\mathbb{N}_0^m$.\n- Post-treatment count vector: $\\mathbf{y} \\in \\mathbb{N}_0^m$.\n- Number of taxa: $m$.\n- Additive smoothing parameter: $\\lambda \\geq 0$.\n- Smoothed relative abundance distribution $\\mathbf{p}$: $p_i = \\frac{x_i + \\lambda}{\\sum_{j=1}^{m} (x_j + \\lambda)}$.\n- Smoothed relative abundance distribution $\\mathbf{q}$: $q_i = \\frac{y_i + \\lambda}{\\sum_{j=1}^{m} (y_j + \\lambda)}$.\n- Kullback-Leibler divergence from $\\mathbf{p}$ to $\\mathbf{q}$ using the natural logarithm: $D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = \\sum_{i=1}^{m} p_i \\,\\ln\\!\\left(\\frac{p_i}{q_i}\\right)$.\n- Test suite:\n    - Case $1$: $m = 2$, $\\mathbf{x} = (2,2)$, $\\mathbf{y} = (2,2)$, $\\lambda = 0.5$.\n    - Case $2$: $m = 2$, $\\mathbf{x} = (3,1)$, $\\mathbf{y} = (2,2)$, $\\lambda = 0$.\n    - Case $3$: $m = 2$, $\\mathbf{x} = (1,0)$, $\\mathbf{y} = (0,1)$, $\\lambda = 0.5$.\n    - Case $4$: $m = 2$, $\\mathbf{x} = (1,1)$, $\\mathbf{y} = (0,2)$, $\\lambda = 0.5$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria:\n- **Scientifically Grounded**: The problem is based on standard, well-established concepts in computational biology and information theory. The use of count vectors to represent microbiome composition, the application of additive (Laplace) smoothing to handle zero counts, and the calculation of KL divergence to measure the difference between probability distributions are all routine procedures in the field. The formulas are correct and standard.\n- **Well-Posed**: The problem is well-posed. For each test case, all necessary inputs ($\\mathbf{x}$, $\\mathbf{y}$, $\\lambda$) are provided. The functions for calculating $\\mathbf{p}$, $\\mathbf{q}$, and $D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q})$ are explicitly defined. The KL divergence is well-defined for the given cases. When $\\lambda > 0$ (Cases $1$, $3$, $4$), all components of $\\mathbf{p}$ and $\\mathbf{q}$ are strictly positive, so the term $\\ln(p_i/q_i)$ is always defined. For Case $2$, where $\\lambda = 0$, the count vector $\\mathbf{y}=(2,2)$ has no zero entries, which ensures that all components of the distribution $\\mathbf{q}$ are strictly positive, again preventing any division by zero or logarithm of zero. The problem specification is therefore mathematically sound.\n- **Objective**: The problem is stated using precise, objective mathematical language. There are no subjective or ambiguous terms.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or infeasibility.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived.\n\nThe computational procedure for each test case is as follows:\n$1$. Given a count vector $\\mathbf{x}$, a count vector $\\mathbf{y}$, and a smoothing parameter $\\lambda$.\n$2$. Compute the smoothed count vectors $\\mathbf{x}' = \\mathbf{x} + \\lambda$ and $\\mathbf{y}' = \\mathbf{y} + \\lambda$.\n$3$. Compute the total smoothed counts $S_x = \\sum_j x'_j$ and $S_y = \\sum_j y'_j$.\n$4$. Normalize to obtain the probability distributions $\\mathbf{p} = \\mathbf{x}' / S_x$ and $\\mathbf{q} = \\mathbf{y}' / S_y$.\n$5$. Compute the KL divergence $D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = \\sum_{i=1}^{m} p_i \\ln(p_i/q_i)$.\n$6$. Round the final numerical result to $6$ decimal places.\n\nLet us demonstrate the calculation for Case $3$: $m = 2$, $\\mathbf{x} = (1,0)$, $\\mathbf{y} = (0,1)$, $\\lambda = 0.5$.\n- Smoothed count vector for $\\mathbf{x}$: $(1+0.5, 0+0.5) = (1.5, 0.5)$.\n- Total smoothed count $S_x = 1.5 + 0.5 = 2.0$.\n- Smoothed distribution $\\mathbf{p}$: $(1.5/2.0, 0.5/2.0) = (0.75, 0.25)$.\n- Smoothed count vector for $\\mathbf{y}$: $(0+0.5, 1+0.5) = (0.5, 1.5)$.\n- Total smoothed count $S_y = 0.5 + 1.5 = 2.0$.\n- Smoothed distribution $\\mathbf{q}$: $(0.5/2.0, 1.5/2.0) = (0.25, 0.75)$.\n- The KL divergence is:\n$$ D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = p_1 \\ln\\left(\\frac{p_1}{q_1}\\right) + p_2 \\ln\\left(\\frac{p_2}{q_2}\\right) $$\n$$ D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = 0.75 \\ln\\left(\\frac{0.75}{0.25}\\right) + 0.25 \\ln\\left(\\frac{0.25}{0.75}\\right) $$\n$$ D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = 0.75 \\ln(3) + 0.25 \\ln(1/3) $$\n$$ D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = 0.75 \\ln(3) - 0.25 \\ln(3) = 0.5 \\ln(3) $$\nUsing the value $\\ln(3) \\approx 1.09861228867$, we get:\n$$ D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) \\approx 0.5 \\times 1.09861228867 \\approx 0.54930614433 $$\nRounded to $6$ decimal places, the result is $0.549306$.\n\nThe same procedure is applied to all test cases.\n- **Case 1**: $\\mathbf{x}=(2,2)$, $\\mathbf{y}=(2,2)$, $\\lambda=0.5$. Here, $\\mathbf{p} = \\mathbf{q} = (0.5, 0.5)$. Thus, $D_{\\mathrm{KL}}(\\mathbf{p}\\,\\|\\,\\mathbf{q}) = \\sum p_i \\ln(1) = 0$. Result: $0.000000$.\n- **Case 2**: $\\mathbf{x}=(3,1)$, $\\mathbf{y}=(2,2)$, $\\lambda=0$. Here, $\\mathbf{p}=(0.75, 0.25)$ and $\\mathbf{q}=(0.5, 0.5)$. $D_{\\mathrm{KL}} = 0.75 \\ln(0.75/0.5) + 0.25 \\ln(0.25/0.5) \\approx 0.130812$.\n- **Case 4**: $\\mathbf{x}=(1,1)$, $\\mathbf{y}=(0,2)$, $\\lambda=0.5$. Here, $\\mathbf{p}=(0.5, 0.5)$ and $\\mathbf{q}=(0.5/3, 2.5/3) \\approx (0.1667, 0.8333)$. $D_{\\mathrm{KL}} = 0.5 \\ln(0.5 / (0.5/3)) + 0.5 \\ln(0.5 / (2.5/3)) \\approx 0.293893$.\n\nThe implementation will use numerical libraries for precision and efficiency.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Kullback-Leibler divergence for given microbiome count vectors.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (x_counts, y_counts, lambda_val)\n        (np.array([2, 2]), np.array([2, 2]), 0.5),\n        (np.array([3, 1]), np.array([2, 2]), 0.0),\n        (np.array([1, 0]), np.array([0, 1]), 0.5),\n        (np.array([1, 1]), np.array([0, 2]), 0.5),\n    ]\n\n    results = []\n    for x_counts, y_counts, lambda_val in test_cases:\n        # Step 1: Apply additive smoothing to the count vectors.\n        # x_i + lambda\n        smoothed_x = x_counts + lambda_val\n        # y_i + lambda\n        smoothed_y = y_counts + lambda_val\n\n        # Step 2: Compute the total sum for normalization.\n        # sum_j(x_j + lambda)\n        total_x = np.sum(smoothed_x)\n        # sum_j(y_j + lambda)\n        total_y = np.sum(smoothed_y)\n        \n        # Step 3: Compute the smoothed relative abundance distributions p and q.\n        # p_i = (x_i + lambda) / sum_j(x_j + lambda)\n        p = smoothed_x / total_x\n        # q_i = (y_i + lambda) / sum_j(y_j + lambda)\n        q = smoothed_y / total_y\n\n        # Step 4: Compute the KL divergence D_KL(p || q).\n        # The term p_i * log(p_i/q_i) is 0 if p_i is 0.\n        # The sum is over non-zero elements of p.\n        # Given the problem constraints (lambda >= 0), and specifically\n        # for these test cases, p_i and q_i are always > 0,\n        # so we do not need to handle p_i=0 cases explicitly.\n        kl_divergence = np.sum(p * np.log(p / q))\n        \n        # Step 5: Round the result to 6 decimal places.\n        rounded_result = round(kl_divergence, 6)\n        results.append(f\"{rounded_result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}