{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的巩固方式莫过于亲自动手实践。本练习将引导你手动完成期望最大化（EM）算法的两个完整迭代步骤。通过解决一个经典的生物信息学问题——基序（motif）发现，你将具体地体会EM算法如何通过E步和M步的交替迭代，从不完整的数据中逐步优化模型参数。",
            "id": "2388823",
            "problem": "观测到一个长度为 $4$ 的双链 DNA 序列：位置 $1$ 到 $4$ 的碱基依次为 A、C、G、T。一个概率性模体发现模型规定如下。\n\n- 一个长度为 $L=2$ 的模体在该序列中仅出现一次，且只有两个候选的模体放置位置：位置 $1$–$2$ 或位置 $3$–$4$。设潜变量 $Z \\in \\{1,2\\}$ 表示模体的起始位置，其中 $Z=1$ 表示位置 $1$–$2$ 是模体，$Z=2$ 表示位置 $3$–$4$ 是模体。\n- 非模体位置的背景是独立同分布的，其在脱氧核糖核酸（DNA）字母表 $\\{ \\text{A}, \\text{C}, \\text{G}, \\text{T} \\}$ 上的固定分布为 $b_{\\text{A}}=b_{\\text{C}}=b_{\\text{G}}=b_{\\text{T}}=\\frac{1}{4}$。\n- 模体由一个大小为 $2 \\times 4$ 的位置权重矩阵（PWM）$\\theta$ 建模，其中 PWM 的每一行都是在 $\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$ 上的一个分类分布。初始时（$t=0$），PWM 为\n  - 模体位置 $1$：$[\\theta_{1,\\text{A}},\\theta_{1,\\text{C}},\\theta_{1,\\text{G}},\\theta_{1,\\text{T}}] = \\left[\\frac{3}{5},\\frac{1}{5},\\frac{1}{10},\\frac{1}{10}\\right]$，\n  - 模体位置 $2$：$[\\theta_{2,\\text{A}},\\theta_{2,\\text{C}},\\theta_{2,\\text{G}},\\theta_{2,\\text{T}}] = \\left[\\frac{1}{10},\\frac{1}{10},\\frac{3}{5},\\frac{1}{5}\\right]$。\n- 两种模体放置位置的先验混合权重初始为 $\\pi^{(0)}_1=\\pi^{(0)}_2=\\frac{1}{2}$。\n- 背景分布 $b$ 是固定的（不更新）。不使用伪计数或正则化。\n\n将该模型视为一个有限混合模型，其完整数据似然性按照通常的方式定义，即模体位置的独立分类发射来自 PWM，非模体位置的独立分类发射来自背景。手动执行期望最大化（EM）算法的两次完整迭代，即从上述初始参数开始，执行一次 E 步和一次 M 步，重复两次。\n\n完成第二个 E 步后，模体起始于位置 $1$–$2$ 的后验概率是多少，即 $\\Pr(Z=1 \\mid \\text{A C G T}; \\text{第一个 M 步之后的参数})$？将您的答案表示为保留四位有效数字的小数。",
            "solution": "该问题定义明确，具有科学依据，并包含了执行所需计算的所有必要组成部分。这是期望最大化（EM）算法在生物信息学中模体发现问题上的一个标准应用。因此，该问题被视为有效，并将按其陈述进行求解。\n\n期望最大化（EM）算法是一种迭代方法，用于在依赖于未观测到的潜变量的统计模型中，寻找参数的最大似然估计。每次迭代包括两个步骤：期望（E）步和最大化（M）步。\n\n设观测数据为 DNA 序列 $X = (X_1, X_2, X_3, X_4) = (\\text{A}, \\text{C}, \\text{G}, \\text{T})$。潜变量 $Z \\in \\{1, 2\\}$ 表示模体的起始位置。模型的参数为 $\\Theta = (\\pi, \\theta)$，其中 $\\pi = (\\pi_1, \\pi_2)$ 是混合权重，$\\theta$ 是长度为 $L=2$ 的模体的位置权重矩阵（PWM）。\n\n对于单个观测值 $X$ 和潜变量 $Z$ 的特定值 $z$，完整数据似然为 $\\Pr(X, Z=z; \\Theta) = \\Pr(X \\mid Z=z; \\theta) \\Pr(Z=z; \\pi) = \\pi_z \\Pr(X \\mid Z=z; \\theta)$。\n\nEM 算法的步骤如下：\n1.  **E 步**：计算在给定数据 $X$ 和当前参数 $\\Theta^{(t)}$ 的条件下，每个混合成分 $z$ 的后验概率，或称为“责任”（responsibility）。\n    $$ \\gamma_z^{(t)} = \\Pr(Z=z \\mid X; \\Theta^{(t)}) = \\frac{\\pi_z^{(t)} \\Pr(X \\mid Z=z; \\theta^{(t)})}{\\sum_{z'} \\pi_{z'}^{(t)} \\Pr(X \\mid Z=z'; \\theta^{(t)})} $$\n2.  **M 步**：通过最大化完整数据对数似然的期望值来更新参数为 $\\Theta^{(t+1)}$，其中使用在 E 步中计算的责任。对于这个问题：\n    $$ \\pi_z^{(t+1)} = \\gamma_z^{(t)} $$\n    $$ \\theta_{j,k}^{(t+1)} = \\frac{\\sum_{z} \\gamma_z^{(t)} I(X_{s_z + j - 1} = k)}{\\sum_{z} \\gamma_z^{(t)}} $$\n    其中 $I(\\cdot)$ 是指示函数，$s_z$ 是成分 $z$ 的起始位置（$s_1=1, s_2=3$），$j \\in \\{1, 2\\}$ 是模体内的位置，$k \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$。\n\n我们从 $t=0$ 时的初始参数开始：\n- 序列：$X = (\\text{A}, \\text{C}, \\text{G}, \\text{T})$\n- 混合权重：$\\pi^{(0)}_1 = \\frac{1}{2}, \\pi^{(0)}_2 = \\frac{1}{2}$\n- 背景分布：对于 $k \\in \\{\\text{A}, \\text{C}, \\text{G}, \\text{T}\\}$，$b_k = \\frac{1}{4}$\n- PWM $\\theta^{(0)}$：\n  - $\\theta^{(0)}_{1,\\cdot} = [\\theta_{1,\\text{A}},\\theta_{1,\\text{C}},\\theta_{1,\\text{G}},\\theta_{1,\\text{T}}] = \\left[\\frac{3}{5},\\frac{1}{5},\\frac{1}{10},\\frac{1}{10}\\right]$\n  - $\\theta^{(0)}_{2,\\cdot} = [\\theta_{2,\\text{A}},\\theta_{2,\\text{C}},\\theta_{2,\\text{G}},\\theta_{2,\\text{T}}] = \\left[\\frac{1}{10},\\frac{1}{10},\\frac{3}{5},\\frac{1}{5}\\right]$\n\n**第 1 次迭代：E 步**\n\n首先，我们计算每种可能的模体放置位置下数据的似然。\n- 如果 $Z=1$，模体在位置 $1-2$（`AC`），背景在位置 $3-4$（`GT`）。似然为：\n  $$ \\Pr(X \\mid Z=1; \\theta^{(0)}) = (\\theta^{(0)}_{1,\\text{A}} \\cdot \\theta^{(0)}_{2,\\text{C}}) \\cdot (b_{\\text{G}} \\cdot b_{\\text{T}}) = \\left(\\frac{3}{5} \\cdot \\frac{1}{10}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{3}{50} \\cdot \\frac{1}{16} = \\frac{3}{800} $$\n- 如果 $Z=2$，模体在位置 $3-4$（`GT`），背景在位置 $1-2$（`AC`）。似然为：\n  $$ \\Pr(X \\mid Z=2; \\theta^{(0)}) = (\\theta^{(0)}_{1,\\text{G}} \\cdot \\theta^{(0)}_{2,\\text{T}}) \\cdot (b_{\\text{A}} \\cdot b_{\\text{C}}) = \\left(\\frac{1}{10} \\cdot \\frac{1}{5}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{1}{50} \\cdot \\frac{1}{16} = \\frac{1}{800} $$\n\n接下来，我们计算联合概率和边际似然：\n- $\\Pr(X, Z=1; \\Theta^{(0)}) = \\pi^{(0)}_1 \\Pr(X \\mid Z=1; \\theta^{(0)}) = \\frac{1}{2} \\cdot \\frac{3}{800} = \\frac{3}{1600}$\n- $\\Pr(X, Z=2; \\Theta^{(0)}) = \\pi^{(0)}_2 \\Pr(X \\mid Z=2; \\theta^{(0)}) = \\frac{1}{2} \\cdot \\frac{1}{800} = \\frac{1}{1600}$\n- $\\Pr(X; \\Theta^{(0)}) = \\frac{3}{1600} + \\frac{1}{1600} = \\frac{4}{1600} = \\frac{1}{400}$\n\n现在，我们计算责任 $\\gamma_z^{(0)}$：\n- $\\gamma_1^{(0)} = \\Pr(Z=1 \\mid X; \\Theta^{(0)}) = \\frac{3/1600}{4/1600} = \\frac{3}{4}$\n- $\\gamma_2^{(0)} = \\Pr(Z=2 \\mid X; \\Theta^{(0)}) = \\frac{1/1600}{4/1600} = \\frac{1}{4}$\n\n**第 1 次迭代：M 步**\n\n我们使用责任 $\\gamma_1^{(0)}$ 和 $\\gamma_2^{(0)}$ 更新参数。\n- 更新混合权重：\n  $$ \\pi_1^{(1)} = \\gamma_1^{(0)} = \\frac{3}{4} $$\n  $$ \\pi_2^{(1)} = \\gamma_2^{(0)} = \\frac{1}{4} $$\n- 更新 PWM $\\theta^{(1)}$：\n  对于模体位置 $j=1$：\n  - $\\theta_{1,\\text{A}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_1=\\text{A}) + \\gamma_2^{(0)} \\cdot I(X_3=\\text{A}) = \\frac{3}{4} \\cdot 1 + \\frac{1}{4} \\cdot 0 = \\frac{3}{4}$\n  - $\\theta_{1,\\text{C}}^{(1)} = 0$\n  - $\\theta_{1,\\text{G}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_1=\\text{G}) + \\gamma_2^{(0)} \\cdot I(X_3=\\text{G}) = \\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 1 = \\frac{1}{4}$\n  - $\\theta_{1,\\text{T}}^{(1)} = 0$\n\n  对于模体位置 $j=2$：\n  - $\\theta_{2,\\text{A}}^{(1)} = 0$\n  - $\\theta_{2,\\text{C}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_2=\\text{C}) + \\gamma_2^{(0)} \\cdot I(X_4=\\text{C}) = \\frac{3}{4} \\cdot 1 + \\frac{1}{4} \\cdot 0 = \\frac{3}{4}$\n  - $\\theta_{2,\\text{G}}^{(1)} = 0$\n  - $\\theta_{2,\\text{T}}^{(1)} = \\gamma_1^{(0)} \\cdot I(X_2=\\text{T}) + \\gamma_2^{(0)} \\cdot I(X_4=\\text{T}) = \\frac{3}{4} \\cdot 0 + \\frac{1}{4} \\cdot 1 = \\frac{1}{4}$\n\n一次完整迭代后的参数是 $\\Theta^{(1)} = (\\pi^{(1)}, \\theta^{(1)})$。\n\n**第 2 次迭代：E 步**\n\n问题要求的是后验概率 $\\Pr(Z=1 \\mid X; \\Theta^{(1)})$，这是在第二次 E 步中计算的责任 $\\gamma_1^{(1)}$。我们使用更新后的参数 $\\pi^{(1)}$ 和 $\\theta^{(1)}$。\n\n首先，我们用 $\\theta^{(1)}$ 重新计算似然：\n- 如果 $Z=1$：\n  $$ \\Pr(X \\mid Z=1; \\theta^{(1)}) = (\\theta^{(1)}_{1,\\text{A}} \\cdot \\theta^{(1)}_{2,\\text{C}}) \\cdot (b_{\\text{G}} \\cdot b_{\\text{T}}) = \\left(\\frac{3}{4} \\cdot \\frac{3}{4}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{9}{16} \\cdot \\frac{1}{16} = \\frac{9}{256} $$\n- 如果 $Z=2$：\n  $$ \\Pr(X \\mid Z=2; \\theta^{(1)}) = (\\theta^{(1)}_{1,\\text{G}} \\cdot \\theta^{(1)}_{2,\\text{T}}) \\cdot (b_{\\text{A}} \\cdot b_{\\text{C}}) = \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) \\cdot \\left(\\frac{1}{4} \\cdot \\frac{1}{4}\\right) = \\frac{1}{16} \\cdot \\frac{1}{16} = \\frac{1}{256} $$\n\n接下来，我们使用 $\\pi^{(1)}$ 计算联合概率：\n- $\\Pr(X, Z=1; \\Theta^{(1)}) = \\pi^{(1)}_1 \\Pr(X \\mid Z=1; \\theta^{(1)}) = \\frac{3}{4} \\cdot \\frac{9}{256} = \\frac{27}{1024}$\n- $\\Pr(X, Z=2; \\Theta^{(1)}) = \\pi^{(1)}_2 \\Pr(X \\mid Z=2; \\theta^{(1)}) = \\frac{1}{4} \\cdot \\frac{1}{256} = \\frac{1}{1024}$\n\n新的边际似然是：\n- $\\Pr(X; \\Theta^{(1)}) = \\frac{27}{1024} + \\frac{1}{1024} = \\frac{28}{1024}$\n\n最后，新的责任 $\\gamma_z^{(1)}$ 是：\n- $\\gamma_1^{(1)} = \\Pr(Z=1 \\mid X; \\Theta^{(1)}) = \\frac{27/1024}{28/1024} = \\frac{27}{28}$\n- $\\gamma_2^{(1)} = \\Pr(Z=2 \\mid X; \\Theta^{(1)}) = \\frac{1/1024}{28/1024} = \\frac{1}{28}$\n\n模体起始于位置 $1–2$ 的后验概率是 $\\gamma_1^{(1)} = \\frac{27}{28}$。将其转换为小数并保留四位有效数字：\n$$ \\frac{27}{28} \\approx 0.9642857... \\approx 0.9643 $$",
            "answer": "$$\\boxed{0.9643}$$"
        },
        {
            "introduction": "掌握了手动计算的流程后，下一步是将其转化为代码，以解决更真实规模的问题。这个练习要求你为一个高斯混合模型（GMM）实现EM算法，用于聚类基因表达数据。更重要的是，它揭示了EM算法的一个关键特性：对初始值的敏感性。你将通过实验看到，不同的初始参数如何引导算法收敛到不同的局部最优解，其中一些可能并不符合生物学上的真实情况。",
            "id": "2388760",
            "problem": "给定一个一维基因表达情景，其中每个观测值都是一个表示对数转换后表达水平的实数值。考虑一个具有正好 $2$ 个高斯分量和已知公共方差 $\\sigma^2$ 的有限混合模型。设参数向量为 $\\theta = (\\pi_1,\\pi_2,\\mu_1,\\mu_2)$，其中 $\\pi_k \\in (0,1)$，$\\pi_1+\\pi_2=1$，且对于 $k \\in \\{1,2\\}$，$\\mu_k \\in \\mathbb{R}$。对于任意 $\\theta$，针对 $n$ 个独立观测值 $x_1,\\dots,x_n$ 的观测数据似然为\n$$\n\\mathcal{L}(\\theta; x_{1:n}) \\;=\\; \\prod_{i=1}^n \\left[ \\pi_1 \\,\\phi(x_i;\\mu_1,\\sigma^2) + \\pi_2 \\,\\phi(x_i;\\mu_2,\\sigma^2) \\right],\n$$\n其中 $\\phi(x;\\mu,\\sigma^2)$ 表示均值为 $\\mu$、方差为 $\\sigma^2$ 的高斯密度。对数似然为 $\\ell(\\theta; x_{1:n}) = \\log \\mathcal{L}(\\theta; x_{1:n})$。\n\n对于此模型，如果存在一个参数向量 $\\theta^\\star$，其混合权重严格介于 $0$ 和 $1$ 之间，且均值不同，使得得到的观测数据对数似然的最大值对应于分离两个已知的生物样本组，则该数据集被称为“生物学上正确”。具体来说，设 $y_i \\in \\{0,1\\}$ 表示 $x_i$ 的生物学分组标签，其中 $y_i=0$ 代表 $A$ 组， $y_i=1$ 代表 $B$ 组。对于任意参数 $\\theta$，定义后验分量成员概率\n$$\n\\gamma_{ik}(\\theta) \\;=\\; \\frac{\\pi_k\\,\\phi(x_i;\\mu_k,\\sigma^2)}{\\pi_1\\,\\phi(x_i;\\mu_1,\\sigma^2)+\\pi_2\\,\\phi(x_i;\\mu_2,\\sigma^2)}, \\quad k\\in\\{1,2\\},\n$$\n和硬分配 $\\hat{z}_i(\\theta) = \\arg\\max_{k\\in\\{1,2\\}} \\gamma_{ik}(\\theta)$。由于混合标签是不可识别的，通过对 $\\{0,1\\}$ 进行排列来定义标签对齐的预测类别 $\\hat{y}_i(\\theta)$，该排列在比较 $\\hat{z}_i(\\theta)$ 和 $y_i$ 时能最小化错分计数。错分率为\n$$\n\\mathrm{Err}(\\theta) \\;=\\; \\frac{1}{n}\\,\\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i(\\theta) \\neq y_i \\right\\}.\n$$\n定义少数类召回率为\n$$\n\\mathrm{Rec}_{\\text{minor}}(\\theta) \\;=\\; \\frac{\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\}\\,\\mathbf{1}\\{\\hat{y}_i(\\theta)=1\\}}{\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\}},\n$$\n约定如果分母为 $0$，则召回率定义为 $1$。\n\n您的任务是，对于下方的每个测试用例，从所提供的初始参数向量开始，使用已知的方差 $\\sigma^2$，为指定的数据集计算一个观测数据对数似然的驻点参数向量 $\\hat{\\theta}$。近似驻点必须通过连续迭代之间观测数据对数似然的绝对变化小于 $\\varepsilon = 10^{-8}$ 的条件来认证。然后，为每个测试用例报告序对 $[\\mathrm{Err}(\\hat{\\theta}), \\mathrm{Rec}_{\\text{minor}}(\\hat{\\theta})]$。\n\n所有数据集均通过以下可复现规则生成。对于给定的整数 $n_A \\ge 1$ 和 $n_B \\ge 1$，实数均值 $m_A,m_B$，方差 $\\sigma^2>0$ 以及种子 $s$，首先将随机数生成器设置为种子 $s$。然后抽取 $n_A$ 个独立样本 $x_i \\sim \\mathcal{N}(m_A,\\sigma^2)$，其标签为 $y_i=0$ ($A$ 组)，以及 $n_B$ 个独立样本 $x_i \\sim \\mathcal{N}(m_B,\\sigma^2)$，其标签为 $y_i=1$ ($B$ 组)。将这 $n=n_A+n_B$ 个样本按生成顺序连接成单个数据集 $\\{(x_i,y_i)\\}_{i=1}^n$。\n\n使用公共已知方差 $\\sigma^2 = 1$。测试套件包含三个用例：\n\n- 测试用例 1 (分离良好，平衡初始化):\n  - 数据: $n_A=200$, $n_B=5$, $m_A=0$, $m_B=6$, 种子 $s=31415$。\n  - 初始参数: $\\theta^{(0)} = (\\pi_1^{(0)},\\pi_2^{(0)},\\mu_1^{(0)},\\mu_2^{(0)}) = (0.5,\\,0.5,\\,-0.5,\\,6.5)$。\n\n- 测试用例 2 (数据相同，不平衡初始化，预计会达到一个生物学上不正确的局部最大值):\n  - 数据: 与测试用例 1 相同。\n  - 初始参数: $\\theta^{(0)} = (0.99,\\,0.01,\\,0.2,\\,-0.2)$。\n\n- 测试用例 3 (边界少数类，极端不平衡):\n  - 数据: $n_A=100$, $n_B=1$, $m_A=0$, $m_B=6$, 种子 $s=27182$。\n  - 初始参数: $\\theta^{(0)} = (0.999,\\,0.001,\\,0.1,\\,-0.1)$。\n\n您的程序必须为每个测试用例计算一个满足上述近似驻点标准（$\\varepsilon = 10^{-8}$）的参数向量 $\\hat{\\theta}$，然后计算并返回两个标量指标 $[\\mathrm{Err}(\\hat{\\theta}), \\mathrm{Rec}_{\\text{minor}}(\\hat{\\theta})]$ 作为实数。\n\n最终输出格式：您的程序应生成单行输出，包含一个逗号分隔的列表，其中包含三个项目，每个项目本身是一个由方括号括起来的、逗号分隔的实数对，对应一个测试用例。例如，一个语法上有效的输出看起来像\n\"[a1,b1],[a2,b2],[a3,b3]\"\n但您必须以完全相同的单行格式输出三个测试用例的实际计算值，外部没有多余的括号，也没有额外的文本。",
            "solution": "在尝试解决方案之前，问题陈述经过了严格的验证。\n\n**第 1 步：提取给定条件**\n- **模型**：一个具有 $K=2$ 个高斯分量的一维有限混合模型。公共方差 $\\sigma^2$ 是已知的。\n- **参数向量**：$\\theta = (\\pi_1, \\pi_2, \\mu_1, \\mu_2)$，其中 $\\pi_k \\in (0,1)$ 是混合权重，满足 $\\pi_1 + \\pi_2 = 1$，$\\mu_k \\in \\mathbb{R}$ 是分量均值。\n- **已知方差**：$\\sigma^2 = 1$。\n- **观测数据似然**：对于观测值 $x_{1:n} = \\{x_1, \\dots, x_n\\}$，$\\mathcal{L}(\\theta; x_{1:n}) = \\prod_{i=1}^n [ \\pi_1 \\phi(x_i;\\mu_1,\\sigma^2) + \\pi_2 \\phi(x_i;\\mu_2,\\sigma^2) ]$，其中 $\\phi(x;\\mu,\\sigma^2)$ 是高斯概率密度函数。\n- **对数似然**：$\\ell(\\theta; x_{1:n}) = \\log \\mathcal{L}(\\theta; x_{1:n})$。\n- **后验成员概率（责任）**：$\\gamma_{ik}(\\theta) = \\frac{\\pi_k\\,\\phi(x_i;\\mu_k,\\sigma^2)}{\\sum_{j=1}^2 \\pi_j\\,\\phi(x_i;\\mu_j,\\sigma^2)}$。\n- **硬分配**：$\\hat{z}_i(\\theta) = \\arg\\max_{k\\in\\{1,2\\}} \\gamma_{ik}(\\theta)$。\n- **指标**：错分率 $\\mathrm{Err}(\\theta)$ 和少数类召回率 $\\mathrm{Rec}_{\\text{minor}}(\\theta)$，在将预测标签 $\\hat{z}_i$ 与真实标签 $y_i \\in \\{0,1\\}$ 对齐以最小化错分后计算得出。\n- **任务**：对于每个测试用例，使用期望最大化 (EM) 算法，从给定的 $\\theta^{(0)}$ 开始，找到对数似然的一个驻点 $\\hat{\\theta}$。\n- **收敛准则**：当连续迭代之间的对数似然绝对变化小于 $\\varepsilon = 10^{-8}$ 时，算法终止。\n- **数据生成**：数据集从两个正态分布 $\\mathcal{N}(m_A, \\sigma^2)$（$A$ 组，$y_i=0$）和 $\\mathcal{N}(m_B, \\sigma^2)$（$B$ 组，$y_i=1$）生成，具有指定的样本大小 $n_A, n_B$，均值 $m_A, m_B$，方差 $\\sigma^2=1$ 和随机种子 $s$。\n- **测试用例**：定义了三个具体用例，包括其数据生成参数和初始参数向量 $\\theta^{(0)}$。\n\n**第 2 步：使用提取的给定条件进行验证**\n- **科学依据**：该问题基于高斯混合模型 (GMM) 和期望最大化 (EM) 算法，这些是统计学和机器学习中的基本且广泛应用的概念，尤其在计算生物学的聚类任务中。其科学基础是无可挑剔的。\n- **良定性**：目标是找到似然函数的一个驻点。EM 算法是完成此任务的一种标准、可证明收敛（收敛到局部最大值或鞍点）的方法。该问题定义明确，目标清晰可达。\n- **客观性**：问题以精确的数学术语陈述。所有量都有明确的定义。数据生成通过可复现的程序指定。没有主观或基于观点的内容。\n- **完整性**：提供了所有必要信息：模型、要使用的算法、初始条件、收敛准则、数据生成协议和评估指标。问题是自包含的。\n- **一致性与可行性**：所提供的信息没有矛盾之处。数据生成的参数和初始化在数值上是合理的，不会立即引发任何物理或计算上的不可行性问题。\n\n**第 3 步：结论与行动**\n该问题被确定为**有效**。这是一个良定的、科学上合理的计算统计学问题，与指定主题直接相关。将制定一个正式的解决方案。\n\n该问题要求找到高斯混合模型观测数据对数似然的一个驻点。完成此任务的标准且适当的方法是期望最大化 (EM) 算法。EM 算法是一种迭代过程，用于在具有潜变量的统计模型中寻找参数的最大似然估计。\n\n在我们的情景中，潜变量是每个观测值的成分分配。设 $z_i \\in \\{1, 2\\}$ 是一个潜变量，指示两个高斯分量中的哪一个生成了观测值 $x_i$。假设 $(x_i, z_i)$ 对是已知的，则完全数据似然为：\n$$\n\\mathcal{L}_c(\\theta; x_{1:n}, z_{1:n}) = \\prod_{i=1}^n \\left[ \\pi_{z_i} \\phi(x_i; \\mu_{z_i}, \\sigma^2) \\right]\n$$\n相应的完全数据对数似然是：\n$$\n\\ell_c(\\theta; x_{1:n}, z_{1:n}) = \\sum_{i=1}^n \\left[ \\log \\pi_{z_i} + \\log \\phi(x_i; \\mu_{z_i}, \\sigma^2) \\right]\n$$\nEM 算法在两个步骤之间交替进行：期望 (E) 步和最大化 (M) 步。设 $\\theta^{(t)} = (\\pi_1^{(t)}, \\pi_2^{(t)}, \\mu_1^{(t)}, \\mu_2^{(t)})$ 是第 $t$ 次迭代时的参数估计。\n\n**E 步 (期望):**\n在这一步中，我们计算完全数据对数似然关于潜变量 $z_i$ 在给定观测数据 $x_{1:n}$ 和当前参数估计 $\\theta^{(t)}$ 下的条件分布的期望。这等同于计算成分成员的后验概率，这些概率被称为责任。对于每个观测值 $i$ 和成分 $k \\in \\{1, 2\\}$，责任 $\\gamma_{ik}^{(t)}$ 为：\n$$\n\\gamma_{ik}^{(t)} \\equiv \\mathrm{P}(z_i = k | x_i, \\theta^{(t)}) = \\frac{\\pi_k^{(t)} \\phi(x_i; \\mu_k^{(t)}, \\sigma^2)}{\\sum_{j=1}^2 \\pi_j^{(t)} \\phi(x_i; \\mu_j^{(t)}, \\sigma^2)}\n$$\n这个量代表了观测值 $i$ 对成分 $k$ 的“软分配”。\n\n**M 步 (最大化):**\n在这一步中，我们找到使 E 步中计算的期望完全数据对数似然（Q 函数）最大化的参数 $\\theta^{(t+1)}$。该最大化过程产生以下更新规则。\n混合权重 $\\pi_k$ 的更新是每个成分在所有数据点上的平均责任：\n$$\n\\pi_k^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n \\gamma_{ik}^{(t)}\n$$\n均值 $\\mu_k$ 的更新是数据点的加权平均，其中权重是责任：\n$$\n\\mu_k^{(t+1)} = \\frac{\\sum_{i=1}^n \\gamma_{ik}^{(t)} x_i}{\\sum_{i=1}^n \\gamma_{ik}^{(t)}}\n$$\n方差 $\\sigma^2$ 是已知的，并固定为 $\\sigma^2=1$，因此不进行更新。\n\n**迭代过程与收敛：**\n算法从一个初始参数猜测 $\\theta^{(0)}$ 开始。重复 E 步和 M 步，生成一系列参数估计 $\\theta^{(1)}, \\theta^{(2)}, \\dots$。EM 算法保证观测数据的对数似然在每次迭代中都是非递减的，即 $\\ell(\\theta^{(t+1)}; x_{1:n}) \\ge \\ell(\\theta^{(t)}; x_{1:n})$。我们监控观测数据对数似然 $\\ell(\\theta; x_{1:n}) = \\sum_{i=1}^n \\log(\\sum_{k=1}^2 \\pi_k \\phi(x_i; \\mu_k, \\sigma^2))$ 的变化，当其绝对变化小于指定的容差 $\\varepsilon = 10^{-8}$ 时终止。最终的参数记为 $\\hat{\\theta}$。\n\n**评估指标：**\n一旦算法收敛到 $\\hat{\\theta}$，我们就评估其性能。\n首先，我们计算最终的责任 $\\gamma_{ik}(\\hat{\\theta})$，并为每个数据点 $i$ 执行硬分配，将其分配给责任最高的成分：$\\hat{z}_i = \\arg\\max_{k \\in \\{1,2\\}} \\gamma_{ik}(\\hat{\\theta})$。在实现中，我们将 $\\{1,2\\}$ 映射到 $\\{0,1\\}$。\n其次，由于混合成分的不可识别性（标签切换），我们必须将预测的簇标签 $\\hat{z}_i$ 与真实的生物学标签 $y_i$ 对齐。我们考虑两种可能的映射：恒等映射 $(\\hat{z}_i \\to \\hat{z}_i)$ 和切换映射 $(\\hat{z}_i \\to 1-\\hat{z}_i)$。我们选择能最小化总错分数的映射，从而得到对齐的预测 $\\hat{y}_i$。\n然后，错分率为 $\\mathrm{Err}(\\hat{\\theta}) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{\\hat{y}_i \\neq y_i\\}$。\n对于 $y_i=1$ 的类，少数类召回率计算为 $\\mathrm{Rec}_{\\text{minor}}(\\hat{\\theta}) = (\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\} \\mathbf{1}\\{\\hat{y}_i=1\\}) / (\\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\})$。\n\n以下实现将为三个测试用例中的每一个执行此完整过程。",
            "answer": "```python\nimport numpy as np\n\ndef generate_data(n_A, n_B, m_A, m_B, sigma2, seed):\n    \"\"\"\n    Generates a dataset based on the problem specification.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    sigma = np.sqrt(sigma2)\n    \n    group_A_samples = rng.normal(loc=m_A, scale=sigma, size=n_A)\n    group_B_samples = rng.normal(loc=m_B, scale=sigma, size=n_B)\n    \n    x = np.concatenate([group_A_samples, group_B_samples])\n    \n    labels_A = np.zeros(n_A, dtype=int)\n    labels_B = np.ones(n_B, dtype=int)\n    y = np.concatenate([labels_A, labels_B])\n    \n    return x, y\n\ndef log_norm_pdf(x, mu, sigma2):\n    \"\"\"\n    Computes the log of the Gaussian PDF, vectorized for x.\n    \"\"\"\n    const = -0.5 * np.log(2 * np.pi * sigma2)\n    return const - (x - mu)**2 / (2 * sigma2)\n\ndef run_em(x, theta0, sigma2, epsilon):\n    \"\"\"\n    Runs the EM algorithm for a 2-component GMM with fixed variance.\n    \"\"\"\n    pi1, pi2, mu1, mu2 = theta0\n    n = len(x)\n    \n    # Initial log-likelihood\n    log_pi1, log_pi2 = np.log(pi1), np.log(pi2)\n    log_pdf1 = log_norm_pdf(x, mu1, sigma2)\n    log_pdf2 = log_norm_pdf(x, mu2, sigma2)\n    \n    log_term1 = log_pi1 + log_pdf1\n    log_term2 = log_pi2 + log_pdf2\n    \n    log_likelihood_per_point = np.logaddexp(log_term1, log_term2)\n    current_log_likelihood = np.sum(log_likelihood_per_point)\n\n    while True:\n        old_log_likelihood = current_log_likelihood\n        \n        # E-step: Compute responsibilities\n        log_gamma1 = log_term1 - log_likelihood_per_point\n        gamma1 = np.exp(log_gamma1)\n        gamma2 = 1.0 - gamma1\n        \n        # M-step: Update parameters\n        N1 = np.sum(gamma1)\n        N2 = np.sum(gamma2)\n        \n        # Guard against empty clusters, though not expected in these cases\n        if N1  1e-9 or N2  1e-9:\n            break\n\n        pi1 = N1 / n\n        pi2 = 1.0 - pi1\n        \n        mu1 = np.sum(gamma1 * x) / N1\n        mu2 = np.sum(gamma2 * x) / N2\n        \n        # Compute new log-likelihood for convergence check\n        log_pi1 = np.log(pi1) if pi1 > 0 else -np.inf\n        log_pi2 = np.log(pi2) if pi2 > 0 else -np.inf\n        log_pdf1 = log_norm_pdf(x, mu1, sigma2)\n        log_pdf2 = log_norm_pdf(x, mu2, sigma2)\n\n        log_term1 = log_pi1 + log_pdf1\n        log_term2 = log_pi2 + log_pdf2\n\n        log_likelihood_per_point = np.logaddexp(log_term1, log_term2)\n        current_log_likelihood = np.sum(log_likelihood_per_point)\n        \n        if abs(current_log_likelihood - old_log_likelihood)  epsilon:\n            break\n            \n    return pi1, pi2, mu1, mu2\n\ndef calculate_metrics(x, y, theta_hat, sigma2):\n    \"\"\"\n    Calculates misclassification error and minority-class recall.\n    \"\"\"\n    pi1, pi2, mu1, mu2 = theta_hat\n    n = len(x)\n    \n    # Calculate final responsibilities\n    pdf1 = np.exp(log_norm_pdf(x, mu1, sigma2))\n    pdf2 = np.exp(log_norm_pdf(x, mu2, sigma2))\n\n    term1 = pi1 * pdf1\n    term2 = pi2 * pdf2\n    denominator = term1 + term2\n    \n    # Handle case where a point is far from both means, denominator is ~0\n    # In such a case, responsibilities are ill-defined. Assign to closer component.\n    gamma1 = np.divide(term1, denominator, out=np.zeros_like(term1), where=denominator!=0)\n    \n    # Let cluster label 0 correspond to component 1, and label 1 to component 2\n    z_hat = (gamma1 = 0.5).astype(int) \n    \n    # Label alignment to minimize misclassification\n    misclassifications1 = np.sum(z_hat != y)\n    misclassifications2 = np.sum((1 - z_hat) != y)\n    \n    err = min(misclassifications1, misclassifications2) / n\n    \n    if misclassifications1 = misclassifications2:\n        y_hat = z_hat\n    else:\n        y_hat = 1 - z_hat\n        \n    # Minority-class recall\n    is_minority = (y == 1)\n    n_minority = np.sum(is_minority)\n    \n    if n_minority == 0:\n        recall = 1.0\n    else:\n        true_positives = np.sum(y_hat[is_minority] == 1)\n        recall = true_positives / n_minority\n        \n    return [err, recall]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"data_params\": {\"n_A\": 200, \"n_B\": 5, \"m_A\": 0, \"m_B\": 6, \"seed\": 31415},\n            \"init_params\": (0.5, 0.5, -0.5, 6.5)\n        },\n        {\n            \"data_params\": {\"n_A\": 200, \"n_B\": 5, \"m_A\": 0, \"m_B\": 6, \"seed\": 31415},\n            \"init_params\": (0.99, 0.01, 0.2, -0.2)\n        },\n        {\n            \"data_params\": {\"n_A\": 100, \"n_B\": 1, \"m_A\": 0, \"m_B\": 6, \"seed\": 27182},\n            \"init_params\": (0.999, 0.001, 0.1, -0.1)\n        }\n    ]\n    \n    common_sigma2 = 1.0\n    epsilon = 1e-8\n    \n    results = []\n    \n    for case in test_cases:\n        x, y = generate_data(**case[\"data_params\"], sigma2=common_sigma2)\n        \n        theta_hat = run_em(x, case[\"init_params\"], common_sigma2, epsilon)\n        \n        metrics = calculate_metrics(x, y, theta_hat, common_sigma2)\n        \n        results.append(f\"[{metrics[0]},{metrics[1]}]\")\n        \n    print(','.join(results))\n\nsolve()\n```"
        },
        {
            "introduction": "EM算法的强大之处在于其通用性，它不仅能处理混合模型中的潜在类别，还能有效应对其他类型的缺失数据。本练习将EM算法应用于生存分析领域，一个在临床研究中至关重要的问题。你将学习如何推导并实现一个EM算法，来处理带有右删失（right-censored）的生存数据，从而估计生存率参数。",
            "id": "2388747",
            "problem": "考虑独立的患者生存时间，其模型如下。每位患者的生存时间是一个随机变量 $T_i$，独立地从一个指数分布中抽取。该指数分布具有恒定的风险率 $\\lambda$（单位为天分之一），其概率密度函数为 $f(t \\mid \\lambda) = \\lambda e^{-\\lambda t}$（对于 $t \\ge 0$），生存函数为 $S(t \\mid \\lambda) = e^{-\\lambda t}$。右删失是无信息且独立的：对于某些患者，研究在事件发生前结束，因此我们只观察到一个删失时间 $c_i$ 并知道 $T_i \\ge c_i$。对于每位患者，我们观察到一个时间 $x_i$ 和一个事件指示符 $\\delta_i$，其中 $x_i = \\min(T_i, c_i)$，如果事件被观察到，则 $\\delta_i = 1$；如果观察被右删失，则 $\\delta_i = 0$。所有时间单位均为天，$\\lambda$ 必须以天分之一表示。\n\n您的任务是设计并实现期望最大化（Expectation–Maximization, EM）算法，该算法将删失患者的未观察到失效时间视为潜变量，并估计指数分布的率参数 $\\lambda$。请仅从上述基本定义、独立性假设和条件期望的定义出发。不要调用直接给出EM算法更新步骤的现成公式。\n\n需要推导和实现的EM过程规范：\n- 期望最大化（EM）算法应在以下两个步骤之间交替进行：在给定观测数据和当前参数的情况下，计算完整数据对数似然的条件期望；以及对该期望关于 $\\lambda$ 进行最大化。\n- 从一个严格为正的初始值 $\\lambda^{(0)}$（单位为天分之一）开始。\n- 迭代直至 $\\lambda$ 的绝对变化量小于一个容差 $\\varepsilon$，或达到最大迭代次数 $K_{\\max}$。\n- 返回最终估计值 $\\widehat{\\lambda}$，单位为天分之一。\n\n测试集。您的程序必须为以下三个测试用例运行EM算法。对于每个用例，$x$ 是观察到的时间列表（单位为天），$\\delta$ 是事件指示符列表，$\\lambda^{(0)}$ 是初始值（单位为天分之一），$\\varepsilon$ 是容差，$K_{\\max}$ 是最大迭代次数。\n\n- 案例 $1$（混合删失，一般情况）：\n  - $x^{(1)} = [\\,5,\\,12,\\,9,\\,3,\\,15,\\,7,\\,20,\\,4,\\,11,\\,8\\,]$\n  - $\\delta^{(1)} = [\\,1,\\,0,\\,1,\\,1,\\,0,\\,1,\\,0,\\,1,\\,1,\\,0\\,]$\n  - $\\lambda^{(0)}_{(1)} = 0.1$, $\\varepsilon_{(1)} = 10^{-12}$, $K_{\\max,(1)} = 100000$\n\n- 案例 $2$（无删失，边界条件）：\n  - $x^{(2)} = [\\,1,\\,2,\\,3,\\,4,\\,5\\,]$\n  - $\\delta^{(2)} = [\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n  - $\\lambda^{(0)}_{(2)} = 0.25$, $\\varepsilon_{(2)} = 10^{-12}$, $K_{\\max,(2)} = 100000$\n\n- 案例 $3$（重度删失，极端情况）：\n  - $x^{(3)} = [\\,10,\\,10,\\,10,\\,2,\\,10,\\,10\\,]$\n  - $\\delta^{(3)} = [\\,0,\\,0,\\,0,\\,1,\\,0,\\,0\\,]$\n  - $\\lambda^{(0)}_{(3)} = 0.05$, $\\varepsilon_{(3)} = 10^{-12}$, $K_{\\max,(3)} = 100000$\n\n最终输出格式。您的程序应生成单行输出，其中包含三个最终估计值，以十进制浮点数形式表示（单位为天分之一），每个值都精确到小数点后六位，并聚合成一个用方括号括起来的逗号分隔列表。例如，输出必须类似于 $[a,b,c]$，其中 $a$、$b$ 和 $c$ 分别是案例1、2和3的三个四舍五入后的估计值（无多余空格、单位或其他附加文本）。",
            "solution": "所述问题具有科学依据、自成体系且提法得当。它描述了期望最大化（EM）算法在带有右删失数据的生存模型中进行参数估计的标准应用，这是生物统计学中的一项常见任务。该问题是有效的，我将着手解决它。\n\n目标是推导并实现EM算法，以找到指数分布率参数 $\\lambda$ 的最大似然估计（MLE），其中一些生存时间观测值是右删失的。该算法必须从基本原理推导，将删失观测值的未知真实生存时间视为潜变量。\n\n设完整数据为真实、独立同分布的生存时间集合 $\\mathbf{T} = \\{T_1, T_2, \\ldots, T_n\\}$，其中每个 $T_i \\sim \\text{Exponential}(\\lambda)$。概率密度函数（PDF）为 $f(t | \\lambda) = \\lambda e^{-\\lambda t}$（对于 $t \\ge 0$）。完整数据对数似然函数 $\\ell_c(\\lambda; \\mathbf{T})$ 由下式给出：\n$$ \\ell_c(\\lambda; \\mathbf{T}) = \\log \\left( \\prod_{i=1}^{n} f(T_i | \\lambda) \\right) = \\log \\left( \\prod_{i=1}^{n} \\lambda e^{-\\lambda T_i} \\right) = \\sum_{i=1}^{n} (\\log \\lambda - \\lambda T_i) = n \\log \\lambda - \\lambda \\sum_{i=1}^{n} T_i $$\n观测数据由数据对 $\\mathbf{Y}_{\\text{obs}} = \\{(x_i, \\delta_i)\\}_{i=1}^n$ 组成，其中 $x_i$ 是观测时间，$\\delta_i$ 是事件指示符。如果 $\\delta_i=1$，则事件被观测到，真实生存时间 $T_i$ 已知为 $x_i$。如果 $\\delta_i=0$，则观测值被删失，真实生存时间 $T_i$ 仅已知大于 $x_i$。因此，未观测到的（潜）变量是所有被删失对象的精确生存时间 $T_i$。\n\nEM算法是一个迭代过程，包括两个步骤：期望（E）步骤和最大化（M）步骤。\n\n**E-步：期望**\n在第 $(k+1)$ 次迭代中，E-步计算完整数据对数似然函数在给定观测数据 $\\mathbf{Y}_{\\text{obs}}$ 和当前参数估计 $\\lambda^{(k)}$ 下的期望。这就是 $Q$ 函数。\n$$ Q(\\lambda | \\lambda^{(k)}) = E_{\\mathbf{T} | \\mathbf{Y}_{\\text{obs}}, \\lambda^{(k)}}[\\ell_c(\\lambda; \\mathbf{T})] $$\n代入 $\\ell_c(\\lambda; \\mathbf{T})$ 的表达式：\n$$ Q(\\lambda | \\lambda^{(k)}) = E_{\\mathbf{T} | \\mathbf{Y}_{\\text{obs}}, \\lambda^{(k)}} \\left[ n \\log \\lambda - \\lambda \\sum_{i=1}^{n} T_i \\right] $$\n根据期望的线性性质，这变为：\n$$ Q(\\lambda | \\lambda^{(k)}) = n \\log \\lambda - \\lambda \\, E \\left[ \\sum_{i=1}^{n} T_i \\mid \\mathbf{Y}_{\\text{obs}}, \\lambda^{(k)} \\right] = n \\log \\lambda - \\lambda \\sum_{i=1}^{n} E[T_i | (x_i, \\delta_i), \\lambda^{(k)}] $$\n我们必须根据每个观测的状态 $\\delta_i$ 来计算其条件期望 $E[T_i | (x_i, \\delta_i), \\lambda^{(k)}]$：\n\n1.  对于未删失的观测（$\\delta_i=1$），真实生存时间已知：$T_i = x_i$。因此，期望就是观测值：\n    $$ E[T_i | (x_i, \\delta_i=1), \\lambda^{(k)}] = x_i $$\n\n2.  对于删失的观测（$\\delta_i=0$），我们只知道 $T_i  x_i$。条件期望为 $E[T_i | T_i  x_i, \\lambda^{(k)}]$。我们从条件期望的定义推导出这个值。给定 $T_i  x_i$ 时 $T_i$ 的条件PDF为：\n    $$ f(t | T_i  x_i, \\lambda^{(k)}) = \\frac{f(t | \\lambda^{(k)})}{P(T_i  x_i | \\lambda^{(k)})} = \\frac{\\lambda^{(k)} e^{-\\lambda^{(k)} t}}{e^{-\\lambda^{(k)} x_i}} = \\lambda^{(k)} e^{-\\lambda^{(k)}(t - x_i)} \\quad \\text{for } t  x_i $$\n    那么条件期望是：\n    $$ E[T_i | T_i  x_i, \\lambda^{(k)}] = \\int_{x_i}^{\\infty} t \\cdot \\lambda^{(k)} e^{-\\lambda^{(k)}(t - x_i)} \\, dt $$\n    使用换元法 $u = t - x_i$（因此 $t = u + x_i$ 且 $dt = du$）：\n    $$ = \\int_{0}^{\\infty} (u + x_i) \\lambda^{(k)} e^{-\\lambda^{(k)} u} \\, du = \\int_{0}^{\\infty} u \\lambda^{(k)} e^{-\\lambda^{(k)} u} \\, du + x_i \\int_{0}^{\\infty} \\lambda^{(k)} e^{-\\lambda^{(k)} u} \\, du $$\n    第一个积分是速率为 $\\lambda^{(k)}$ 的指数随机变量的期望，即 $1/\\lambda^{(k)}$。第二个积分是 $x_i$ 乘以一个PDF在其支撑集上的积分，结果为 $1$。这证实了从无记忆性得到的结果。\n    $$ E[T_i | T_i  x_i, \\lambda^{(k)}] = \\frac{1}{\\lambda^{(k)}} + x_i $$\n\n结合这两种情况，期望总和变为：\n$$ \\sum_{i=1}^{n} E[T_i | (x_i, \\delta_i), \\lambda^{(k)}] = \\sum_{i: \\delta_i=1} x_i + \\sum_{i: \\delta_i=0} \\left(x_i + \\frac{1}{\\lambda^{(k)}}\\right) $$\n令 $n_c = \\sum_{i=1}^n (1-\\delta_i)$ 为删失观测的数量。表达式简化为：\n$$ \\sum_{i=1}^{n} E[T_i | \\dots] = \\sum_{i=1}^{n} x_i + \\frac{n_c}{\\lambda^{(k)}} $$\n$Q$ 函数因此为：\n$$ Q(\\lambda | \\lambda^{(k)}) = n \\log \\lambda - \\lambda \\left( \\sum_{i=1}^{n} x_i + \\frac{n_c}{\\lambda^{(k)}} \\right) $$\n\n**M-步：最大化**\nM-步相对于 $\\lambda$ 最大化函数 $Q(\\lambda | \\lambda^{(k)})$，以找到更新后的参数估计 $\\lambda^{(k+1)}$。我们将 $Q$ 对 $\\lambda$ 求导，并令导数等于零：\n$$ \\frac{\\partial Q(\\lambda | \\lambda^{(k)})}{\\partial \\lambda} = \\frac{n}{\\lambda} - \\left( \\sum_{i=1}^{n} x_i + \\frac{n_c}{\\lambda^{(k)}} \\right) = 0 $$\n求解 $\\lambda$ 可得出 $\\lambda^{(k+1)}$ 的更新规则：\n$$ \\frac{n}{\\lambda^{(k+1)}} = \\sum_{i=1}^{n} x_i + \\frac{n_c}{\\lambda^{(k)}} \\implies \\lambda^{(k+1)} = \\frac{n}{\\sum_{i=1}^{n} x_i + \\frac{n_c}{\\lambda^{(k)}}} $$\n\n**迭代算法**\nEM算法按以下步骤进行：\n1.  使用一个初始值 $\\lambda^{(0)}  0$ 进行初始化。\n2.  对于 $k = 0, 1, 2, \\ldots$：使用推导出的更新规则计算下一个估计值：\n    $$ \\lambda^{(k+1)} = \\frac{n}{\\sum_{i=1}^{n} x_i + \\frac{\\sum_{i=1}^n (1-\\delta_i)}{\\lambda^{(k)}}} $$\n3.  当绝对差 $|\\lambda^{(k+1)} - \\lambda^{(k)}|$ 小于指定的容差 $\\varepsilon$ 或达到最大迭代次数 $K_{\\max}$ 时终止。最终值即为MLE估计 $\\widehat{\\lambda}$。\n可以验证，此迭代的不动点 $\\lambda^*$ 满足 $\\lambda^* = n / (\\sum x_i + n_c/\\lambda^*)$，可简化为 $\\lambda^* = (\\sum \\delta_i) / (\\sum x_i)$，这正是该问题的正确闭式MLE解。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef em_survival_exponential(x: list[float], \n                            delta: list[int], \n                            lambda0: float, \n                            epsilon: float, \n                            k_max: int) - float:\n    \"\"\"\n    Estimates the rate parameter lambda of an exponential distribution with\n    right-censored data using the Expectation-Maximization (EM) algorithm.\n\n    Args:\n        x: A list of observed times (event or censoring times) in days.\n        delta: A list of event indicators (1 for event, 0 for censored).\n        lambda0: The initial guess for the rate parameter lambda (in reciprocal days).\n        epsilon: The tolerance for convergence.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The final estimate of the rate parameter lambda.\n    \"\"\"\n    x_np = np.array(x, dtype=float)\n    delta_np = np.array(delta, dtype=int)\n    \n    n = len(x_np)\n    if n == 0:\n        return np.nan\n\n    # Pre-compute fixed quantities\n    sum_x = np.sum(x_np)\n    # Number of censored observations\n    n_c = n - np.sum(delta_np)\n    \n    lambda_current = lambda0\n    \n    for _ in range(k_max):\n        # A check to prevent division by zero if lambda becomes non-positive,\n        # which can happen with pathological inputs, though not for these test cases.\n        if lambda_current = 0:\n            # A strictly positive rate is required.\n            # Returning NaN indicates failure to converge to a valid estimate.\n            return np.nan\n        \n        # M-step: update lambda based on the Q-function maximization\n        # which involves the expected sum of true survival times from the E-step.\n        # Expected sum of T_i = sum(x_i) + n_c * (1 / lambda_current)\n        lambda_next = n / (sum_x + n_c / lambda_current)\n        \n        # Check for convergence\n        if np.abs(lambda_next - lambda_current)  epsilon:\n            return lambda_next\n        \n        lambda_current = lambda_next\n        \n    return lambda_current\n\ndef solve():\n    \"\"\"\n    Runs the EM algorithm on the specified test cases and prints the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"x\": [5.0, 12.0, 9.0, 3.0, 15.0, 7.0, 20.0, 4.0, 11.0, 8.0],\n            \"delta\": [1, 0, 1, 1, 0, 1, 0, 1, 1, 0],\n            \"lambda0\": 0.1,\n            \"epsilon\": 1e-12,\n            \"k_max\": 100000\n        },\n        {\n            \"x\": [1.0, 2.0, 3.0, 4.0, 5.0],\n            \"delta\": [1, 1, 1, 1, 1],\n            \"lambda0\": 0.25,\n            \"epsilon\": 1e-12,\n            \"k_max\": 100000\n        },\n        {\n            \"x\": [10.0, 10.0, 10.0, 2.0, 10.0, 10.0],\n            \"delta\": [0, 0, 0, 1, 0, 0],\n            \"lambda0\": 0.05,\n            \"epsilon\": 1e-12,\n            \"k_max\": 100000\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        lambda_hat = em_survival_exponential(\n            case[\"x\"], case[\"delta\"], case[\"lambda0\"], case[\"epsilon\"], case[\"k_max\"]\n        )\n        results.append(f\"{lambda_hat:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}