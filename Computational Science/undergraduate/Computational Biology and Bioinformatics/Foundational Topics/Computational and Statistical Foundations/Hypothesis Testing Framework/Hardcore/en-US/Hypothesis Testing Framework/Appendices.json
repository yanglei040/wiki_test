{
    "hands_on_practices": [
        {
            "introduction": "Many questions in biology boil down to analyzing counts in different categories. For instance, is a particular allele associated with a disease, or as in this problem, is antibiotic resistance independent of the bacterial species? The Pearson chi-squared test provides a robust framework for answering such questions by comparing the observed counts to what we would expect if no association existed. This exercise  will guide you through the fundamental mechanics of this widely used test.",
            "id": "2398945",
            "problem": "A hospital microbiology lab is monitoring antibiotic resistance across bacterial species isolated from bloodstream infections. For a single antibiotic of interest, isolates are categorized by bacterial species and by phenotypic outcome of susceptibility testing (resistant or sensitive). Over one month, the following counts are recorded:\n- Escherichia coli: $40$ resistant, $60$ sensitive.\n- Staphylococcus aureus: $25$ resistant, $75$ sensitive.\n- Pseudomonas aeruginosa: $35$ resistant, $15$ sensitive.\n\nTreat these data as a single random sample of size $N$ from a joint distribution over the categorical variables “species” and “resistance status.” Under the null hypothesis that resistance status is independent of species, determine the Pearson chi-squared test statistic for independence using the $3 \\times 2$ contingency table implied by the data.\n\nProvide only the value of the test statistic. Round your answer to four significant figures.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. It is a standard application of statistical hypothesis testing in computational biology. We shall proceed with the solution.\n\nThe problem requires the calculation of the Pearson chi-squared test statistic, denoted as $\\chi^2$, for a test of independence on a contingency table. The data concern antibiotic resistance across three bacterial species.\n\nFirst, we must organize the provided counts into a contingency table of observed frequencies, denoted as $O_{ij}$. Let the rows $i$ represent the bacterial species and the columns $j$ represent the resistance status (Resistant, Sensitive).\n\nThe observed frequencies are:\n\\begin{itemize}\n    \\item \\textit{Escherichia coli}: $O_{11} = 40$ resistant, $O_{12} = 60$ sensitive.\n    \\item \\textit{Staphylococcus aureus}: $O_{21} = 25$ resistant, $O_{22} = 75$ sensitive.\n    \\item \\textit{Pseudomonas aeruginosa}: $O_{31} = 35$ resistant, $O_{32} = 15$ sensitive.\n\\end{itemize}\n\nWe construct the $3 \\times 2$ contingency table and calculate the row totals ($R_i$), column totals ($C_j$), and the grand total ($N$).\n\nThe table of observed frequencies $O_{ij}$ is:\n$$\n\\begin{array}{c|cc|c}\n\\text{Species} & \\text{Resistant} & \\text{Sensitive} & \\text{Row Total } (R_i) \\\\\n\\hline\n\\text{E. coli} & 40 & 60 & 100 \\\\\n\\text{S. aureus} & 25 & 75 & 100 \\\\\n\\text{P. aeruginosa} & 35 & 15 & 50 \\\\\n\\hline\n\\text{Column Total } (C_j) & 100 & 150 & 250\n\\end{array}\n$$\nThe row totals are $R_1 = 40 + 60 = 100$, $R_2 = 25 + 75 = 100$, and $R_3 = 35 + 15 = 50$.\nThe column totals are $C_1 = 40 + 25 + 35 = 100$ and $C_2 = 60 + 75 + 15 = 150$.\nThe grand total is $N = 100 + 100 + 50 = 250$.\n\nThe null hypothesis, $H_0$, states that resistance status is independent of bacterial species. Under this assumption, the expected frequency for each cell, $E_{ij}$, is calculated using the formula:\n$$ E_{ij} = \\frac{R_i \\times C_j}{N} $$\n\nWe now calculate the expected frequencies for each cell:\n\\begin{itemize}\n    \\item $E_{11} = \\frac{R_1 \\times C_1}{N} = \\frac{100 \\times 100}{250} = \\frac{10000}{250} = 40$\n    \\item $E_{12} = \\frac{R_1 \\times C_2}{N} = \\frac{100 \\times 150}{250} = \\frac{15000}{250} = 60$\n    \\item $E_{21} = \\frac{R_2 \\times C_1}{N} = \\frac{100 \\times 100}{250} = \\frac{10000}{250} = 40$\n    \\item $E_{22} = \\frac{R_2 \\times C_2}{N} = \\frac{100 \\times 150}{250} = \\frac{15000}{250} = 60$\n    \\item $E_{31} = \\frac{R_3 \\times C_1}{N} = \\frac{50 \\times 100}{250} = \\frac{5000}{250} = 20$\n    \\item $E_{32} = \\frac{R_3 \\times C_2}{N} = \\frac{50 \\times 150}{250} = \\frac{7500}{250} = 30$\n\\end{itemize}\n\nThe Pearson chi-squared test statistic is calculated as the sum of the squared differences between observed and expected frequencies, divided by the expected frequencies, over all cells in the table:\n$$ \\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}} $$\nwhere $r=3$ is the number of rows and $c=2$ is the number of columns.\n\nWe compute the contribution of each cell to the $\\chi^2$ statistic:\n\\begin{itemize}\n    \\item Cell $(1,1)$: $\\frac{(40 - 40)^2}{40} = \\frac{0}{40} = 0$\n    \\item Cell $(1,2)$: $\\frac{(60 - 60)^2}{60} = \\frac{0}{60} = 0$\n    \\item Cell $(2,1)$: $\\frac{(25 - 40)^2}{40} = \\frac{(-15)^2}{40} = \\frac{225}{40} = 5.625$\n    \\item Cell $(2,2)$: $\\frac{(75 - 60)^2}{60} = \\frac{(15)^2}{60} = \\frac{225}{60} = 3.75$\n    \\item Cell $(3,1)$: $\\frac{(35 - 20)^2}{20} = \\frac{(15)^2}{20} = \\frac{225}{20} = 11.25$\n    \\item Cell $(3,2)$: $\\frac{(15 - 30)^2}{30} = \\frac{(-15)^2}{30} = \\frac{225}{30} = 7.5$\n\\end{itemize}\n\nThe total $\\chi^2$ statistic is the sum of these values:\n$$ \\chi^2 = 0 + 0 + 5.625 + 3.75 + 11.25 + 7.5 = 28.125 $$\n\nThe problem requires the answer to be rounded to four significant figures. The value $28.125$ rounded to four significant figures is $28.13$.",
            "answer": "$$\n\\boxed{28.13}\n$$"
        },
        {
            "introduction": "When analyzing numerical data like gene expression levels from RNA-seq, a common task is to compare the central tendency between two conditions. While the $t$-test is a popular choice, its reliance on the data's mean and standard deviation makes it highly sensitive to outliers—a frequent issue in high-throughput biological experiments. This practice  demonstrates this critical concept by contrasting a parametric test with the robust, rank-based Wilcoxon test, highlighting the importance of choosing a statistical tool that matches your data's characteristics.",
            "id": "2398972",
            "problem": "Construct a program that, for a fixed gene measured across two biological conditions in Ribonucleic Acid sequencing (RNA-seq), computes the two-sided p-values of both the two-sample Welch $t$-test and the Wilcoxon rank-sum test (also known as the Mann–Whitney $U$ test) and aggregates the results across a specified test suite. Assume each condition produces independent replicate counts for the gene, and treat the provided counts as the observed values. The null hypothesis for both tests is that the two conditions have equal central tendency, stated as $H_0$: the two conditions are sampled from populations with equal means (Welch $t$-test) or equal continuous distributions (Wilcoxon rank-sum test). No transformations are to be applied to the counts. All tests must be two-sided.\n\nYour program must process the following test suite, where each item specifies the counts for condition $A$ and condition $B$:\n\n- Test case $1$ (baseline, no outlier, comparable distributions):\n  - Condition $A$: $\\left(43, 50, 39, 61, 55, 47\\right)$\n  - Condition $B$: $\\left(45, 52, 41, 58, 53, 49\\right)$\n\n- Test case $2$ (single extreme high outlier in condition $B$):\n  - Condition $A$: $\\left(43, 50, 39, 61, 55, 47\\right)$\n  - Condition $B$: $\\left(45, 52, 41, 58, 53, 1000\\right)$\n\n- Test case $3$ (small sample size with a single extreme high outlier in condition $B$):\n  - Condition $A$: $\\left(20, 22, 25\\right)$\n  - Condition $B$: $\\left(19, 21, 400\\right)$\n\n- Test case $4$ (location shift with a single extreme low outlier in condition $B$):\n  - Condition $A$: $\\left(15, 16, 18, 20, 19, 17\\right)$\n  - Condition $B$: $\\left(28, 30, 27, 29, 31, 0\\right)$\n\nFor each test case $i \\in \\{1,2,3,4\\}$, compute:\n- $p_{t,i}$: the two-sided p-value from the two-sample Welch $t$-test comparing condition $A$ and condition $B$.\n- $p_{w,i}$: the two-sided p-value from the Wilcoxon rank-sum test comparing condition $A$ and condition $B$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $8$ floating-point numbers ordered as $\\left[p_{t,1}, p_{w,1}, p_{t,2}, p_{w,2}, p_{t,3}, p_{w,3}, p_{t,4}, p_{w,4}\\right]$, where each number is rounded to $6$ decimal places. No additional text should be printed. All numbers are dimensionless and must be expressed as decimal fractions (for example, $0.012345$), not as percentages.",
            "solution": "The problem requires the computation of p-values from two distinct statistical hypothesis tests—the Welch two-sample $t$-test and the Wilcoxon rank-sum test—applied to RNA-sequencing count data from two conditions. The objective is to compare the performance of a parametric test against a non-parametric test, particularly in the presence of outliers, which are common in biological datasets. Before proceeding to the computational solution, we must establish the theoretical foundation for each test.\n\n**1. Welch's Two-Sample $t$-test**\n\nThe Welch's $t$-test is a statistical tool for comparing the means of two independent samples, denoted here as condition $A$ and condition $B$. It is an adaptation of the Student's $t$-test and is considered more reliable when the two samples have unequal variances, a scenario known as the Behrens-Fisher problem. The null hypothesis, $H_0$, posits that the population means are equal, i.e., $H_0: \\mu_A = \\mu_B$.\n\nThe test statistic $t$ is calculated as the difference between the sample means, scaled by the standard error of the difference:\n$$\nt = \\frac{\\bar{x}_A - \\bar{x}_B}{\\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}}}\n$$\nwhere:\n- $\\bar{x}_A$ and $\\bar{x}_B$ are the sample means for conditions $A$ and $B$, respectively.\n- $s_A^2$ and $s_B^2$ are the unbiased sample variances.\n- $n_A$ and $n_B$ are the sample sizes.\n\nUnlike the Student's $t$-test, the degrees of freedom, $\\nu$, for the Welch's test are not simply $n_A + n_B - 2$. Instead, they are approximated using the Welch-Satterthwaite equation:\n$$\n\\nu \\approx \\frac{\\left( \\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B} \\right)^2}{\\frac{(s_A^2/n_A)^2}{n_A - 1} + \\frac{(s_B^2/n_B)^2}{n_B - 1}}\n$$\nThe p-value is then determined from the Student's $t$-distribution with $\\nu$ degrees of freedom. For a two-sided test, the p-value is $2 \\cdot P(T_\\nu > |t|)$, where $T_\\nu$ is a random variable following the $t$-distribution.\n\nA critical assumption of the $t$-test is that the data in both samples are approximately normally distributed. The test's performance degrades in the presence of strong deviations from normality, such as skewness or heavy tails. Crucially, the sample mean $(\\bar{x})$ and variance $(s^2)$ are highly sensitive to extreme values (outliers), which can disproportionately influence the $t$-statistic and lead to erroneous conclusions.\n\n**2. Wilcoxon Rank-Sum (Mann-Whitney U) Test**\n\nThe Wilcoxon rank-sum test is a non-parametric alternative to the $t$-test. It does not assume a specific distribution for the data, making it more robust. The null hypothesis for this test is that for randomly selected values $X$ and $Y$ from the two populations, the probability of $X$ being greater than $Y$ is equal to the probability of $Y$ being greater than $X$. More generally, it tests if the two samples are drawn from populations with the same distribution.\n\nThe procedure is as follows:\n1. Combine all $n_A + n_B$ observations into a single ranked list. If ties exist, assign the average of the ranks that would have been assigned.\n2. Calculate the sum of the ranks for one of the samples, for instance, sample $A$, denoted as $R_A$.\n3. The test statistic $U$ is calculated based on this rank sum. For sample $A$, the statistic $U_A$ is:\n   $$\n   U_A = R_A - \\frac{n_A(n_A + 1)}{2}\n   $$\n   The test statistic $U$ is typically taken as $U = \\min(U_A, U_B)$, where $U_B$ is calculated similarly for sample $B$.\n4. The p-value is determined from the known distribution of the $U$ statistic under the null hypothesis. For small sample sizes, an exact distribution is used. For larger samples, a normal approximation with a continuity correction is employed.\n\nBecause the Wilcoxon test operates on ranks rather than the original data values, its statistic is not affected by the magnitude of outliers, only by their ordinal position. An extreme value is simply treated as the highest (or lowest) rank, and its specific numerical value does not further influence the test statistic. This property confers substantial robustness against outliers.\n\n**3. Application and Interpretation of Test Cases**\n\nThe provided test cases are designed to highlight the differing sensitivities of these two tests.\n- **Test case 1**: The distributions are similar and lack outliers. Both tests are expected to yield high p-values, indicating no significant difference.\n- **Test cases 2 and 3**: An extreme high outlier is introduced into condition $B$. The $t$-test's p-value is expected to be substantially affected because the outlier will inflate both the mean and variance of condition $B$. In contrast, the Wilcoxon test should be relatively unaffected, providing a more stable assessment of the central tendency shift.\n- **Test case 4**: An extreme low outlier is introduced into condition $B$, whose other members have shifted to higher values compared to condition $A$. This outlier pulls the mean of condition $B$ down, potentially masking the true location shift from the $t$-test. The rank-sum test, however, is better equipped to detect the consistent rank difference between the bulk of the two samples.\n\nThe computational implementation will use the `scipy.stats` library, specifically `ttest_ind` with the parameter `equal_var=False` to perform the Welch's $t$-test, and `mannwhitneyu` with `alternative='two-sided'` for the Wilcoxon rank-sum test. The results will be systematically computed for each test case and formatted as required.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import ttest_ind, mannwhitneyu\n\ndef solve():\n    \"\"\"\n    Computes and prints p-values for Welch's t-test and Wilcoxon rank-sum test\n    for a suite of RNA-seq count data test cases.\n    \"\"\"\n\n    # Define the test suite as specified in the problem statement.\n    test_cases = [\n        # Test case 1: baseline, no outlier, comparable distributions\n        (np.array([43, 50, 39, 61, 55, 47]), np.array([45, 52, 41, 58, 53, 49])),\n        # Test case 2: single extreme high outlier in condition B\n        (np.array([43, 50, 39, 61, 55, 47]), np.array([45, 52, 41, 58, 53, 1000])),\n        # Test case 3: small sample size with a single extreme high outlier in condition B\n        (np.array([20, 22, 25]), np.array([19, 21, 400])),\n        # Test case 4: location shift with a single extreme low outlier in condition B\n        (np.array([15, 16, 18, 20, 19, 17]), np.array([28, 30, 27, 29, 31, 0])),\n    ]\n\n    results = []\n    for cond_a, cond_b in test_cases:\n        # Perform Welch's two-sample t-test.\n        # The `equal_var=False` argument specifies that we should perform Welch's t-test,\n        # which does not assume equal population variance.\n        # The test is two-sided by default.\n        t_stat, p_t = ttest_ind(cond_a, cond_b, equal_var=False)\n        results.append(p_t)\n\n        # Perform the Wilcoxon rank-sum test (Mann-Whitney U test).\n        # We explicitly specify a two-sided test.\n        # The 'auto' method for p-value calculation is used by default,\n        # which chooses between an exact test and a normal approximation\n        # based on sample size and presence of ties.\n        u_stat, p_w = mannwhitneyu(cond_a, cond_b, alternative='two-sided')\n        results.append(p_w)\n\n    # Format the results as a comma-separated list of floating-point numbers\n    # rounded to 6 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Averages can be deceiving, and a major challenge in data analysis is the presence of confounding variables that can obscure or even reverse true relationships. This phenomenon, known as Simpson's Paradox, occurs when a trend appears in aggregated data but disappears or reverses when the data is broken down into subgroups. This exercise  provides a striking, hands-on example, teaching you to be vigilant for hidden stratifications and to appreciate the power of disaggregated analysis to reveal the correct conclusion.",
            "id": "2398958",
            "problem": "You are given a binary-outcome clinical comparison framed for computational biology and bioinformatics: for each test case, two groups exist (a new treatment and a control), and the cohort is stratified by biological sex (male and female). In each stratum and arm you are provided the total number of individuals and the number who experienced an adverse event. Let $N_{s,g}$ denote the total number in sex $s \\in \\{\\text{male}, \\text{female}\\}$ and group $g \\in \\{\\text{treatment}, \\text{control}\\}$, and let $X_{s,g}$ denote the corresponding number of adverse events. Define the event probabilities $p_{s,g}$ by $p_{s,g} = X_{s,g} / N_{s,g}$, and the overall arm totals $N_{\\text{treatment}} = \\sum_{s} N_{s,\\text{treatment}}$, $N_{\\text{control}} = \\sum_{s} N_{s,\\text{control}}$, with overall events $X_{\\text{treatment}} = \\sum_{s} X_{s,\\text{treatment}}$ and $X_{\\text{control}} = \\sum_{s} X_{s,\\text{control}}$, and overall probabilities $p_{\\text{treatment}} = X_{\\text{treatment}} / N_{\\text{treatment}}$ and $p_{\\text{control}} = X_{\\text{control}} / N_{\\text{control}}$.\n\nFor each test case, you must determine whether the dataset exhibits Simpson’s paradox with statistical significance at a two-sided significance level $\\alpha = 0.05$, defined here as:\n- Within the male stratum, the treatment is significantly harmful: $p_{\\text{male},\\text{treatment}} > p_{\\text{male},\\text{control}}$ and the difference is statistically significant at level $\\alpha$ under a valid hypothesis test of equality of adverse-event probabilities between treatment and control.\n- Within the female stratum, the treatment is significantly harmful: $p_{\\text{female},\\text{treatment}} > p_{\\text{female},\\text{control}}$ and the difference is statistically significant at level $\\alpha$ under a valid hypothesis test of equality of adverse-event probabilities between treatment and control.\n- Overall (ignoring sex), the treatment is significantly beneficial: $p_{\\text{treatment}} < p_{\\text{control}}$ and the difference is statistically significant at level $\\alpha$ under a valid hypothesis test of equality of adverse-event probabilities between treatment and control.\n\nIn each hypothesis test, the null hypothesis is that the adverse-event probability is equal in the treatment and control arms, and the decision rule is to reject the null hypothesis if and only if the two-sided $p$-value is less than or equal to $\\alpha$. You must return a boolean value for each test case indicating whether all three bullets above hold simultaneously.\n\nTest suite. For each test case, the input parameters are eight integers $(N_{\\text{male},\\text{treatment}}, X_{\\text{male},\\text{treatment}}, N_{\\text{male},\\text{control}}, X_{\\text{male},\\text{control}}, N_{\\text{female},\\text{treatment}}, X_{\\text{female},\\text{treatment}}, N_{\\text{female},\\text{control}}, X_{\\text{female},\\text{control}})$:\n- Case A (intended to satisfy Simpson’s paradox with strong statistical significance): $(1000, 180, 20000, 1800, 20000, 400, 5000, 50)$.\n- Case B (directional reversal present but not significant overall): $(200, 40, 1000, 100, 2000, 80, 1000, 20)$.\n- Case C (no paradox; treatment harmful within strata and overall): $(1000, 150, 1000, 100, 1000, 40, 1000, 20)$.\n\nProgram requirements. Your program must, for each test case, construct the $2\\times 2$ tables for male, female, and overall; perform valid hypothesis tests at two-sided level $\\alpha = 0.05$ to assess equality of adverse-event probabilities between treatment and control; verify the required direction of effect in each comparison; and output a boolean indicating whether Simpson’s paradox with statistical significance is present. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,False]\"), where each entry is either the literal token True or False. No other output is permitted.",
            "solution": "The problem statement is subjected to validation.\n\nStep 1: Extract Givens.\n- Cohort stratification: Two strata by biological sex, $s \\in \\{\\text{male}, \\text{female}\\}$.\n- Experimental arms: Two groups, $g \\in \\{\\text{treatment}, \\text{control}\\}$.\n- Raw data: For each stratum $s$ and group $g$, we are given the total number of individuals, $N_{s,g}$, and the number of individuals experiencing an adverse event, $X_{s,g}$.\n- Derived quantities:\n  - Stratum-specific event probabilities: $p_{s,g} = X_{s,g} / N_{s,g}$.\n  - Overall total individuals: $N_{\\text{treatment}} = \\sum_{s} N_{s,\\text{treatment}}$, $N_{\\text{control}} = \\sum_{s} N_{s,\\text{control}}$.\n  - Overall total events: $X_{\\text{treatment}} = \\sum_{s} X_{s,\\text{treatment}}$, $X_{\\text{control}} = \\sum_{s} X_{s,\\text{control}}$.\n  - Overall event probabilities: $p_{\\text{treatment}} = X_{\\text{treatment}} / N_{\\text{treatment}}$, $p_{\\text{control}} = X_{\\text{control}} / N_{\\text{control}}$.\n- Significance level: $\\alpha = 0.05$ for two-sided tests.\n- Hypothesis Test: The null hypothesis is the equality of adverse-event probabilities between treatment and control arms. The null is rejected if the two-sided $p$-value is less than or equal to $\\alpha$.\n- Definition of Simpson's paradox with statistical significance: The simultaneous satisfaction of three conditions:\n  1. Male stratum: Treatment is significantly harmful, i.e., $p_{\\text{male},\\text{treatment}} > p_{\\text{male},\\text{control}}$ and the difference is statistically significant at level $\\alpha$.\n  2. Female stratum: Treatment is significantly harmful, i.e., $p_{\\text{female},\\text{treatment}} > p_{\\text{female},\\text{control}}$ and the difference is statistically significant at level $\\alpha$.\n  3. Overall population: Treatment is significantly beneficial, i.e., $p_{\\text{treatment}} < p_{\\text{control}}$ and the difference is statistically significant at level $\\alpha$.\n- Test Cases (Input: $(N_{m,t}, X_{m,t}, N_{m,c}, X_{m,c}, N_{f,t}, X_{f,t}, N_{f,c}, X_{f,c})$):\n  - Case A: $(1000, 180, 20000, 1800, 20000, 400, 5000, 50)$.\n  - Case B: $(200, 40, 1000, 100, 2000, 80, 1000, 20)$.\n  - Case C: $(1000, 150, 1000, 100, 1000, 40, 1000, 20)$.\n- Output format: A single line with a list of boolean results, e.g., `[True,False,False]`.\n\nStep 2: Validate Using Extracted Givens.\n- The problem is **scientifically grounded**. It addresses Simpson's paradox, a well-documented statistical phenomenon, in the context of a clinical trial analysis, which is a standard application in biostatistics and computational biology.\n- The problem is **well-posed**. The conditions for the paradox are stated with mathematical precision. All necessary data and parameters, including the significance level $\\alpha$, are provided. The task of determining a boolean outcome for each case is unambiguous.\n- The problem is **objective**. It is formulated using precise, quantitative language, free of subjective or opinion-based assertions. The term \"valid hypothesis test\" grants the choice of a standard statistical procedure, which is an acceptable specification in a scientific context. The counts provided are integers where $X \\le N$, which is physically consistent.\n\nStep 3: Verdict and Action.\nThe problem is deemed **valid**. It is self-contained, scientifically sound, and well-posed. A rigorous solution can be constructed.\n\nThe task is to verify, for each test case, whether a specific set of three conditions defining a statistically significant Simpson's paradox is met. This requires performing three separate hypothesis tests: one for the male stratum, one for the female stratum, and one for the aggregated (overall) data. Each test assesses the null hypothesis of equal adverse-event probabilities between the treatment and control groups ($H_0: p_{\\text{treatment}} = p_{\\text{control}}$) against a two-sided alternative ($H_1: p_{\\text{treatment}} \\neq p_{\\text{control}}$).\n\nA scientifically appropriate and \"valid hypothesis test\" for comparing two proportions derived from independent samples is Pearson's chi-squared test of independence on a $2 \\times 2$ contingency table. This test is equivalent to the two-sided two-proportion $z$-test for large samples, which are present here. For any comparison between a treatment group with $X_t$ events out of $N_t$ individuals and a control group with $X_c$ events out of $N_c$ individuals, the data can be arranged in the following contingency table:\n$$\n\\begin{array}{c|cc|c}\n & \\text{Adverse Event} & \\text{No Event} & \\text{Total} \\\\ \\hline\n\\text{Treatment} & X_t & N_t - X_t & N_t \\\\\n\\text{Control} & X_c & N_c - X_c & N_c\n\\end{array}\n$$\nThe chi-squared test statistic is calculated from this table, and its corresponding $p$-value is compared against the significance level $\\alpha = 0.05$. The null hypothesis is rejected if $p \\le \\alpha$.\n\nThe procedure for each test case is as follows:\n1.  Verify the condition for the male stratum.\n    - First, check the directionality: Is the sample proportion of adverse events in the treatment group greater than in the control group? That is, is $p_{\\text{male},\\text{treatment}} > p_{\\text{male},\\text{control}}$?\n    - If the direction is correct, construct the $2 \\times 2$ table for males and perform the chi-squared test. Check if the resulting $p$-value is $\\le 0.05$.\n    - The condition for males is met only if both the direction and statistical significance criteria are fulfilled.\n\n2.  Verify the condition for the female stratum.\n    - Check the directionality: Is $p_{\\text{female},\\text{treatment}} > p_{\\text{female},\\text{control}}$?\n    - If so, construct the $2 \\times 2$ table for females, perform the chi-squared test, and check if the $p$-value is $\\le 0.05$.\n    - The condition for females is met only if both criteria are fulfilled.\n\n3.  Verify the condition for the overall population.\n    - First, aggregate the data: $N_t = N_{m,t} + N_{f,t}$, $X_t = X_{m,t} + X_{f,t}$, $N_c = N_{m,c} + N_{f,c}$, and $X_c = X_{m,c} + X_{f,c}$.\n    - Check the directionality: Is the overall proportion of adverse events in the treatment group less than in the control group? That is, is $p_{\\text{treatment}} < p_{\\text{control}}$?\n    - If the direction is correct, construct the $2 \\times 2$ table for the aggregated data, perform the chi-squared test, and check if the $p$-value is $\\le 0.05$.\n    - The overall condition is met only if both criteria are fulfilled.\n\nFinally, Simpson's paradox, as defined, is present if and only if all three of the above conditions—for males, females, and the overall population—are simultaneously true. A boolean result is returned for each test case based on this final determination.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\ndef solve():\n    \"\"\"\n    Solves the Simpson's paradox problem for the given test cases.\n    \"\"\"\n\n    # Significance level\n    alpha = 0.05\n\n    # Test cases: (N_mt, X_mt, N_mc, X_mc, N_ft, X_ft, N_fc, X_fc)\n    # m = male, f = female, t = treatment, c = control\n    test_cases = [\n        # Case A: Intended to satisfy Simpson’s paradox with strong statistical significance\n        (1000, 180, 20000, 1800, 20000, 400, 5000, 50),\n        # Case B: Directional reversal present but not significant overall\n        (200, 40, 1000, 100, 2000, 80, 1000, 20),\n        # Case C: No paradox; treatment harmful within strata and overall\n        (1000, 150, 1000, 100, 1000, 40, 1000, 20),\n    ]\n\n    def check_hypothesis(X_t, N_t, X_c, N_c, direction, alpha_level):\n        \"\"\"\n        Performs a hypothesis test for two proportions and checks directionality.\n\n        Args:\n            X_t (int): Number of events in treatment group.\n            N_t (int): Total individuals in treatment group.\n            X_c (int): Number of events in control group.\n            N_c (int): Total individuals in control group.\n            direction (str): 'greater' or 'less', specifying the required\n                             relationship p_t vs p_c.\n            alpha_level (float): The significance level.\n\n        Returns:\n            bool: True if direction and significance conditions are met.\n        \"\"\"\n        # Avoid division by zero if a group has no subjects.\n        if N_t == 0 or N_c == 0:\n            return False\n\n        p_t = X_t / N_t\n        p_c = X_c / N_c\n\n        # 1. Check directionality\n        if direction == 'greater':\n            if not p_t > p_c:\n                return False\n        elif direction == 'less':\n            if not p_t < p_c:\n                return False\n        else:\n            # Invalid direction specification\n            return False\n            \n        # 2. Check statistical significance\n        # Construct the 2x2 contingency table\n        # Table: [[events_t, no_events_t], [events_c, no_events_c]]\n        contingency_table = np.array([\n            [X_t, N_t - X_t],\n            [X_c, N_c - X_c]\n        ])\n\n        # Perform Pearson's chi-squared test.\n        # The p-value is for a two-sided test, as required.\n        # correction=True is the default (Yates' correction for continuity).\n        try:\n            _, p_value, _, _ = chi2_contingency(contingency_table)\n        except ValueError:\n            # This can happen if a row/column sum is zero, which is handled\n            # by the N_t/N_c check, but as a safeguard.\n            return False\n\n        return p_value <= alpha_level\n\n    results = []\n    for case in test_cases:\n        N_mt, X_mt, N_mc, X_mc, N_ft, X_ft, N_fc, X_fc = case\n\n        # Condition 1: Male stratum - Treatment significantly harmful\n        male_cond_met = check_hypothesis(X_mt, N_mt, X_mc, N_mc, 'greater', alpha)\n\n        # Condition 2: Female stratum - Treatment significantly harmful\n        female_cond_met = check_hypothesis(X_ft, N_ft, X_fc, N_fc, 'greater', alpha)\n\n        # Condition 3: Overall - Treatment significantly beneficial\n        # Aggregate data\n        N_t_overall = N_mt + N_ft\n        X_t_overall = X_mt + X_ft\n        N_c_overall = N_mc + N_fc\n        X_c_overall = X_mc + X_fc\n        \n        overall_cond_met = check_hypothesis(X_t_overall, N_t_overall, X_c_overall, N_c_overall, 'less', alpha)\n        \n        # Simpson's Paradox holds if all three conditions are met\n        is_paradox = male_cond_met and female_cond_met and overall_cond_met\n        results.append(is_paradox)\n\n    # Format the output as a string list of booleans literal\n    # e.g., \"[True,False,False]\"\n    output_str = f\"[{','.join(str(r) for r in results)}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}