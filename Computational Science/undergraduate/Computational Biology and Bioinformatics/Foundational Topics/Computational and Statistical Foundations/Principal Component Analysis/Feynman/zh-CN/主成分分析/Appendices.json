{
    "hands_on_practices": [
        {
            "introduction": "要真正掌握主成分分析 (PCA)，我们必须超越理论，亲自动手实践。第一个练习将引导你手动完成计算主成分的整个过程，揭开软件包的“黑箱”。通过在一个特意设计的简单数据集上进行操作，你将实践 PCA 的核心步骤：数据中心化、构建协方差矩阵以及求解其特征向量。这项基础练习旨在让你对 PCA 背后的数学原理建立起直观且扎实的理解。",
            "id": "2416060",
            "problem": "一项基因表达实验测量了基因 $G_1$、$G_2$ 和 $G_3$ 在样本 $S_1$、$S_2$、$S_3$ 和 $S_4$ 上的经对数转换的表达值。数据矩阵 $X \\in \\mathbb{R}^{4 \\times 3}$ 以样本为行，基因为列：\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 2 & 3 \\\\\n2 & 3 & 4 \\\\\n3 & 4 & 5 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n$$\n将样本视为独立观测值，基因视为变量。使用主成分分析 (PCA)，通过以下步骤计算基因空间中的第一个主成分载荷向量：\n- 对 $X$ 进行列中心化，\n- 构建基因间的样本协方差矩阵，其中 $n=4$ 个样本，分母为 $n-1$，以及\n- 取该协方差矩阵对应于最大特征值的单位范数特征向量。\n\n请以 $(G_1, G_2, G_3)$ 顺序排列的 $1 \\times 3$ 行矩阵形式报告该载荷向量，并选择符号以使其第一个非零项为正。无需四舍五入。",
            "solution": "我们需要计算基因空间中的第一个主成分载荷向量，方法是对基因间的样本协方差矩阵进行特征分解。令样本数量为 $n=4$，基因数量为 $p=3$。数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中行代表样本，列代表基因。\n\n步骤 $1$：列中心化。计算 $X$ 的列均值：\n$$\n\\bar{x}_{\\cdot 1} \\;=\\; \\frac{1+2+3+4}{4} \\;=\\; 2.5,\\quad\n\\bar{x}_{\\cdot 2} \\;=\\; \\frac{2+3+4+5}{4} \\;=\\; 3.5,\\quad\n\\bar{x}_{\\cdot 3} \\;=\\; \\frac{3+4+5+6}{4} \\;=\\; 4.5.\n$$\n从每列中减去这些均值，以获得中心化矩阵 $Z$：\n$$\nZ \\;=\\; X - \\mathbf{1}\\bar{x}^{\\top}\n\\;=\\;\n\\begin{pmatrix}\n1-2.5 & 2-3.5 & 3-4.5 \\\\\n2-2.5 & 3-3.5 & 4-4.5 \\\\\n3-2.5 & 4-3.5 & 5-4.5 \\\\\n4-2.5 & 5-3.5 & 6-4.5\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n-1.5 & -1.5 & -1.5 \\\\\n-0.5 & -0.5 & -0.5 \\\\\n\\phantom{-}0.5 & \\phantom{-}0.5 & \\phantom{-}0.5 \\\\\n\\phantom{-}1.5 & \\phantom{-}1.5 & \\phantom{-}1.5\n\\end{pmatrix}.\n$$\n\n步骤 $2$：基因间的样本协方差矩阵。使用分母 $n-1=3$，基因的样本协方差为\n$$\nS \\;=\\; \\frac{1}{n-1}\\, Z^{\\top} Z \\;=\\; \\frac{1}{3}\\, Z^{\\top} Z.\n$$\n观察到 $Z$ 的每一行都是 $(1,\\,1,\\,1)$ 的标量倍数，因此所有三个中心化的基因列都是相同的。计算 $Z^{\\top}Z$ 时，注意到对于任意两列 $j$ 和 $k$，其 $(j,k)$ 位置的元素等于 $\\sum_{i=1}^{n} Z_{ij} Z_{ik}$。由于所有三列都相同，$Z^{\\top}Z$ 的每个元素都等于单个中心化列的平方和：\n$$\n\\sum_{i=1}^{4} Z_{i1}^{2} \\;=\\; (-1.5)^{2} + (-0.5)^{2} + (0.5)^{2} + (1.5)^{2} \\;=\\; 2.25 + 0.25 + 0.25 + 2.25 \\;=\\; 5.\n$$\n因此，\n$$\nZ^{\\top} Z \\;=\\; 5 \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}\n\\quad\\text{和}\\quad\nS \\;=\\; \\frac{1}{3} Z^{\\top}Z \\;=\\; \\frac{5}{3}\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1\n\\end{pmatrix}.\n$$\n\n步骤 $3$：特征分解。令 $J \\in \\mathbb{R}^{3 \\times 3}$ 表示全1矩阵，即对所有 $j,k$ 都有 $J_{jk}=1$。根据基本原理可知，$J$ 的秩为 $1$，其特征值为 $3$ 和 $0$ (重数为 $2$)。对应于特征值 $3$ 的一个单位范数特征向量与 $(1,\\,1,\\,1)^{\\top}$ 成比例，具体为\n$$\nv \\;=\\; \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\n由于 $S = \\frac{5}{3} J$， $S$ 的特征值为 $\\lambda_{1} = \\frac{5}{3} \\cdot 3 = 5$ 以及 $\\lambda_{2} = 0$, $\\lambda_{3} = 0$，其特征向量与 $J$ 相同。因此，基因空间中的第一个主成分载荷向量是与 $\\lambda_{1}=5$ 相关联的单位范数特征向量，即上面给出的 $v$。选择符号以使第一个非零项为正，得到\n$$\nv \\;=\\; \\left( \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}},\\; \\frac{1}{\\sqrt{3}} \\right).\n$$\n\n因此，按 $(G_1, G_2, G_3)$ 顺序排列并写成 $1 \\times 3$ 行矩阵的形式，第一个主成分载荷向量是\n$$\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{3}}\\end{pmatrix}}$$"
        },
        {
            "introduction": "理论上，主成分是数据变异最大的方向，但在实践中，这些成分的稳定性可能受到个别数据点的显著影响。这个编程练习将探讨一个重要概念：影响点 (influential points) 和 PCA 结果的稳健性。你将构建一个场景，其中移除单个样本会导致前两个主成分 ($PC_1$ 和 $PC_2$) 的顺序完全交换，从而直观地展示 PCA 对异常值的敏感性。这项实践强调了在解释 PCA 结果时，评估其稳定性的重要性。",
            "id": "2416110",
            "problem": "给定一个主成分分析 (PCA) 应用于患者基因表达矩阵的定义。令 $X \\in \\mathbb{R}^{n \\times m}$ 表示一个数据矩阵，其中包含 $n$ 个患者样本（行）和 $m$ 个基因（列）。通过从 $X$ 的每一列中减去样本均值，定义列中心化矩阵 $\\tilde{X}$。样本协方差矩阵为\n$$\nC \\;=\\; \\frac{1}{n-1}\\,\\tilde{X}^\\top \\tilde{X} \\;\\in\\; \\mathbb{R}^{m \\times m}.\n$$\n令 $C$ 的前两个特征对为 $(\\lambda_1, v_1)$ 和 $(\\lambda_2, v_2)$，其中 $\\lambda_1 \\ge \\lambda_2 \\ge 0$ 且 $v_1, v_2 \\in \\mathbb{R}^m$ 为单位范数向量。对于给定的索引 $r \\in \\{0,1,\\dots,n-1\\}$，令 $X^{(-r)}$ 表示从 $X$ 中移除第 $r$ 行后得到的矩阵，并类似地定义其协方差矩阵 $C^{(-r)}$，其前两个特征对为 $(\\lambda_1', v_1')$ 和 $(\\lambda_2', v_2')$。\n\n固定两个阈值：一个特征值绑定容差 $\\epsilon > 0$ 和一个方向对齐阈值 $\\tau \\in (0,1)$。当且仅当以下所有条件都成立时，移除单个患者样本的行为被定义为“完全交换了 $PC_1$ 和 $PC_2$ 的顺序”：\n- 在任一分析中，主特征值均未绑定：$|\\lambda_1 - \\lambda_2| > \\epsilon$ 且 $|\\lambda_1' - \\lambda_2'| > \\epsilon$。\n- 主方向交换（符号除外）：$|v_1^\\top v_2'| \\ge \\tau$ 且 $|v_2^\\top v_1'| \\ge \\tau$。\n\n使用 $\\epsilon = 1\\times 10^{-9}$ 和 $\\tau = 0.99$。\n\n构建一个程序，对于下方的每个测试用例，计算移除指定样本是否会根据上述定义产生完全交换，并输出一个与测试用例相对应的布尔值列表。\n\n测试套件。每个用例包含一个 $n$ 个样本乘以 $m$ 个基因的矩阵 $X^{(i)}$，以及一个使用 0-基索引的移除索引 $r^{(i)}$。所有值都是实数，表示以任意一致单位计的基因表达量。\n\n- 用例 1（设计的交换，有 $m=2$ 个基因，$n=5$ 个患者）：选择参数 $a=1.0$，$k=1.6$，$p=3.0$。令\n$$\nX^{(1)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n3.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(1)} = 4.\n$$\n\n- 用例 2（无交换；较小杠杆，相同结构）：$a=1.0$，$k=1.6$，$p=1.0$。\n$$\nX^{(2)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n1.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(2)} = 4.\n$$\n\n- 用例 3（边界绑定；移除前主特征值相等）：$a=1.0$，$k=1.6$ 且 $p = \\sqrt{\\frac{5}{2}\\,\\big(k^2 - a^2\\big)}$。\n$$\nX^{(3)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n\\sqrt{\\tfrac{5}{2}\\,(1.6^2 - 1.0^2)} & 0.0\n\\end{bmatrix},\n\\quad r^{(3)} = 4.\n$$\n\n- 用例 4（移除一个非高杠杆点样本；预期无交换）：重用用例 1 的矩阵并移除第一行。\n$$\nX^{(4)} \\;=\\;\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n-1.0 & 0.0 \\\\\n0.0 & 1.6 \\\\\n0.0 & -1.6 \\\\\n3.0 & 0.0\n\\end{bmatrix},\n\\quad r^{(4)} = 0.\n$$\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，例如 $[b_1,b_2,b_3,b_4]$，其中每个 $b_i$ 是用例 $i$ 的布尔结果。",
            "solution": "该问题的目标是实现一个算法，对每个提供的测试用例，验证从数据矩阵中移除特定样本（行）是否会导致前两个主成分（$PC_1$ 和 $PC_2$）的“完全交换”。单个测试用例 $(X, r)$ 的流程如下。\n\n1.  **分析完整数据矩阵 $X$**：\n    令给定的矩阵为 $X$，有 $n$ 行和 $m$ 列。\n    a. 计算列均值：$\\mu = \\frac{1}{n} \\sum_{i=1}^n x_{i,:}$，其中 $x_{i,:}$ 是 $X$ 的第 $i$ 行。\n    b. 中心化数据：$\\tilde{X} = X - \\mathbf{1}_{n \\times 1} \\mu^\\top$，其中 $\\mathbf{1}_{n \\times 1}$ 是一个全为 1 的列向量。\n    c. 计算样本协方差矩阵：$C = \\frac{1}{n-1} \\tilde{X}^\\top \\tilde{X}$。\n    d. 对对称矩阵 $C$ 进行特征分解。这将产生一组实数特征值和对应的正交特征向量基。\n    e. 识别出两个最大的特征值 $\\lambda_1 \\ge \\lambda_2$ 及其对应的单位范数特征向量 $v_1$ 和 $v_2$。\n\n2.  **分析留一法数据矩阵 $X^{(-r)}$**：\n    令 $X^{(-r)}$ 为从 $X$ 中移除第 $r$ 行得到的矩阵。该矩阵有 $n' = n-1$ 行和 $m$ 列。\n    a. 计算新的列均值：$\\mu' = \\frac{1}{n'} \\sum_{i \\ne r} x_{i,:}$。\n    b. 中心化简化后的数据：$\\tilde{X}^{(-r)} = X^{(-r)} - \\mathbf{1}_{(n-1) \\times 1} (\\mu')^\\top$。\n    c. 计算新的样本协方差矩阵：$C^{(-r)} = \\frac{1}{n'-1} (\\tilde{X}^{(-r)})^\\top \\tilde{X}^{(-r)}$。\n    d. 对 $C^{(-r)}$ 进行特征分解，找到其两个最大的特征值 $\\lambda_1' \\ge \\lambda_2'$ 及对应的单位范数特征向量 $v_1'$ 和 $v_2'$。\n\n3.  **评估“完全交换”条件**：\n    使用指定的阈值 $\\epsilon = 1 \\times 10^{-9}$ 和 $\\tau = 0.99$，评估四个条件：\n    a. 条件 1（原始数据中无绑定）：测试是否 $|\\lambda_1 - \\lambda_2| > \\epsilon$。\n    b. 条件 2（简化后数据中无绑定）：测试是否 $|\\lambda_1' - \\lambda_2'| > \\epsilon$。\n    c. 条件 3（$PC_1$ 和 $PC_2'$ 的对齐）：测试是否 $|v_1^\\top v_2'| \\ge \\tau$。点积 $v_1^\\top v_2'$ 衡量两个向量方向之间夹角的余弦值。绝对值考虑了特征向量的任意符号。\n    d. 条件 4（$PC_2$ 和 $PC_1'$ 的对齐）：测试是否 $|v_2^\\top v_1'| \\ge \\tau$。\n\n    当且仅当所有四个条件都为真时，发生完全交换。该过程将应用于所提供的四个测试用例中的每一个。数值计算将依赖于标准库进行矩阵运算和特征分解，以确保准确性并遵循既定算法。具体来说，对于像协方差矩阵这样的实对称矩阵，存在专门的算法（例如，雅可比方法或基于 QR 的算法），这些算法在数值上既稳定又高效。使用 `numpy.linalg.eigh` 是合适的，因为它专为厄米矩阵（或实对称矩阵）设计，并保证返回实数特征值和一套完整的标准正交特征向量，按特征值大小排序。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing PCA stability for each test case.\n    \"\"\"\n\n    def check_swap(X, r_idx, epsilon, tau):\n        \"\"\"\n        Checks if removing a sample causes a complete swap of PC1 and PC2.\n\n        Args:\n            X (np.ndarray): The data matrix (n_samples, n_features).\n            r_idx (int): The 0-based index of the row to remove.\n            epsilon (float): The eigenvalue tie-tolerance.\n            tau (float): The direction-alignment threshold.\n\n        Returns:\n            bool: True if a complete swap occurs, False otherwise.\n        \"\"\"\n\n        # --- PCA on the full dataset ---\n        n_samples, n_features = X.shape\n        if n_samples <= 1:\n            return False # Covariance is not well-defined.\n\n        # Center the data\n        X_centered = X - X.mean(axis=0)\n        # Compute the sample covariance matrix\n        cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n        \n        # Eigendecomposition of the symmetric covariance matrix\n        # np.linalg.eigh returns eigenvalues in ascending order\n        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n        \n        # Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        # Extract top two eigenpairs\n        lambda1, lambda2 = eigenvalues[0], eigenvalues[1]\n        v1, v2 = eigenvectors[:, 0], eigenvectors[:, 1]\n        \n        # --- PCA on the leave-one-out dataset ---\n        X_loo = np.delete(X, r_idx, axis=0)\n        n_samples_loo, _ = X_loo.shape\n        \n        if n_samples_loo <= 1:\n            return False\n\n        # Center the leave-one-out data\n        X_loo_centered = X_loo - X_loo.mean(axis=0)\n        # Compute the new sample covariance matrix\n        cov_matrix_loo = (X_loo_centered.T @ X_loo_centered) / (n_samples_loo - 1)\n        \n        # Eigendecomposition\n        eigenvalues_loo, eigenvectors_loo = np.linalg.eigh(cov_matrix_loo)\n        \n        # Sort in descending order\n        sorted_indices_loo = np.argsort(eigenvalues_loo)[::-1]\n        eigenvalues_loo = eigenvalues_loo[sorted_indices_loo]\n        eigenvectors_loo = eigenvectors_loo[:, sorted_indices_loo]\n        \n        # Extract top two eigenpairs\n        lambda1_p, lambda2_p = eigenvalues_loo[0], eigenvalues_loo[1]\n        v1_p, v2_p = eigenvectors_loo[:, 0], eigenvectors_loo[:, 1]\n\n        # --- Evaluate the four conditions for a complete swap ---\n        \n        # Condition 1: Eigenvalues are not tied in the original analysis\n        cond1 = abs(lambda1 - lambda2) > epsilon\n        \n        # Condition 2: Eigenvalues are not tied in the leave-one-out analysis\n        cond2 = abs(lambda1_p - lambda2_p) > epsilon\n        \n        # Condition 3: Original PC1 aligns with new PC2\n        cond3 = abs(np.dot(v1, v2_p)) >= tau\n        \n        # Condition 4: Original PC2 aligns with new PC1\n        cond4 = abs(np.dot(v2, v1_p)) >= tau\n        \n        return cond1 and cond2 and cond3 and cond4\n\n    # Define constants from the problem statement\n    epsilon = 1e-9\n    tau = 0.99\n\n    # Define the test cases from the problem statement\n    X1 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [3.0, 0.0]\n    ])\n    r1 = 4\n\n    X2 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [1.0, 0.0]\n    ])\n    r2 = 4\n\n    # Calculate p for Case 3\n    a = 1.0\n    k = 1.6\n    p_val = np.sqrt((5.0 / 2.0) * (k**2 - a**2))\n    X3 = np.array([\n        [1.0, 0.0],\n        [-1.0, 0.0],\n        [0.0, 1.6],\n        [0.0, -1.6],\n        [p_val, 0.0]\n    ])\n    r3 = 4\n\n    X4 = X1.copy() # Reuse matrix from Case 1\n    r4 = 0\n    \n    test_cases = [\n        (X1, r1),\n        (X2, r2),\n        (X3, r3),\n        (X4, r4),\n    ]\n\n    results = []\n    for X, r_idx in test_cases:\n        is_swap = check_swap(X, r_idx, epsilon, tau)\n        # Python's bool `True` stringifies to 'True', which is standard.\n        results.append(str(is_swap))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "标准 PCA 在寻找数据中的线性关系时非常有效，但当数据结构本质上是非线性时，它就会遇到困难。当细胞群或其他生物数据形成非线性可分的模式（例如同心圆）时，我们该怎么办？这个高级编程练习介绍了核主成分分析 (Kernel PCA)，这是一种强大的非线性扩展。你将学习如何利用“核技巧”，将数据映射到更高维的特征空间，从而在这个新空间中找到能够分离复杂模式的主成分。",
            "id": "2416090",
            "problem": "给定一个场景，其中有两个细胞群，每个细胞通过 $2$ 个定量特征（例如，两个基因表达测量值）来度量。每个细胞群由确定性地排列在 $\\mathbb{R}^2$ 中一个圆上的点组成，形成一个在原始空间中线性不可分的构型。设第一个细胞群（细胞群 $A$）位于半径为 $r_A$ 的圆上，第二个细胞群（细胞群 $B$）位于半径为 $r_B$ 的圆上，其中 $r_A \\ne r_B$。细胞群 $A$ 的第 $i$ 个点定义为\n$$\n\\mathbf{x}_i^{(A)} = \\begin{bmatrix} r_A \\cos\\left(\\frac{2\\pi i}{n_A}\\right) \\\\ r_A \\sin\\left(\\frac{2\\pi i}{n_A}\\right) \\end{bmatrix}, \\quad i \\in \\{0,1,\\dots,n_A-1\\},\n$$\n细胞群 $B$ 的第 $j$ 个点定义为\n$$\n\\mathbf{x}_j^{(B)} = \\begin{bmatrix} r_B \\cos\\left(\\frac{2\\pi j}{n_B}\\right) \\\\ r_B \\sin\\left(\\frac{2\\pi j}{n_B}\\right) \\end{bmatrix}, \\quad j \\in \\{0,1,\\dots,n_B-1\\},\n$$\n其中所有角度均以弧度为单位。\n\n通过连接细胞群 $A$ 和 $B$ 的点来定义总体数据集 $\\{\\mathbf{x}_k\\}_{k=1}^N$，其中 $N = n_A + n_B$。考虑在由多项式核诱导的再生核希尔伯特空间中对此数据集进行核主成分分析（Kernel PCA）\n$$\n\\kappa(\\mathbf{x},\\mathbf{y}) = \\left(\\alpha \\, \\mathbf{x}^\\top \\mathbf{y} + \\beta\\right)^\\delta,\n$$\n其中 $\\alpha \\ge 0$, $\\beta \\ge 0$, 且整数次数 $\\delta \\in \\mathbb{N}$。令 $K \\in \\mathbb{R}^{N \\times N}$ 为核矩阵，其元素为 $K_{ij} = \\kappa(\\mathbf{x}_i,\\mathbf{x}_j)$。令 $H = I_N - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ 表示中心化矩阵，令 $K_c = H K H$ 为中心化核矩阵。令 $K_c$ 的特征值分解为 $K_c = V \\Lambda V^\\top$，其中 $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_N)$ 且 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_N$，$V$ 的列是标准正交特征向量。对于选定的目标维度 $m \\in \\{1,2\\}$，构建嵌入 $Z \\in \\mathbb{R}^{N \\times m}$，其元素为\n$$\nZ_{i\\ell} = \\sqrt{\\lambda_\\ell} \\, V_{i\\ell}, \\quad \\text{对于 } \\ell \\in \\{1,\\dots,m\\} \\text{ 且 } \\lambda_\\ell > 0。\n$$\n\n$\\mathbb{R}^m$中的线性可分性定义如下：嵌入集 $Z_A$ 和 $Z_B$（分别为$Z$中对应于细胞群$A$和$B$的行）是线性可分的，当且仅当存在$\\mathbf{w} \\in \\mathbb{R}^m$和$b \\in \\mathbb{R}$，使得对于所有$\\mathbf{z} \\in Z_A$都有$\\mathbf{w}^\\top \\mathbf{z} + b > 0$，且对于所有$\\mathbf{z} \\in Z_B$都有$\\mathbf{w}^\\top \\mathbf{z} + b < 0$。对每个测试用例，用一个布尔值回答在指定核函数下，两个细胞群在维度为$m$的嵌入空间中是否线性可分。\n\n测试套件：\n对于下面的每个元组 $(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m)$，使用上述定义构建数据集并计算维度为$m$的核主成分分析嵌入，然后确定$Z_A$和$Z_B$是否线性可分。\n\n- 测试 $1$：$(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,2.0,\\,1.0,\\,1.0,\\,2,\\,2\\,)$。\n- 测试 $2$：$(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,2.0,\\,1.0,\\,0.0,\\,1,\\,2\\,)$。\n- 测试 $3$：$(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,1.4,\\,0.2,\\,1.0,\\,2,\\,2\\,)$。\n- 测试 $4$：$(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,24,\\,24,\\,1.0,\\,2.0,\\,1.0,\\,1.0,\\,2,\\,1\\,)$。\n- 测试 $5$：$(n_A, n_B, r_A, r_B, \\alpha, \\beta, \\delta, m) = (\\,8,\\,8,\\,1.0,\\,2.0,\\,1.0,\\,1.0,\\,3,\\,2\\,)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中按顺序包含测试1到5的结果，形式为方括号内由逗号分隔的布尔值列表（例如，[True,False,True,False,True]），不含空格。",
            "solution": "该问题提出了一个典型的生物信息学场景，即细胞群在其原始特征空间（如基因表达水平）中不是线性可分的。所提供的数据集由两个点群 $A$ 和 $B$ 组成，它们分别分布在 $\\mathbb{R}^2$ 中半径为 $r_A$ 和 $r_B$ 的两个同心圆上。这种构型在其原始的二维空间中显然不是线性可分的。\n\n问题的核心是应用核主成分分析（Kernel PCA），通过一个非线性核函数将数据投影到一个更高维的特征空间，并期望在该空间中细胞群变得线性可分。指定的核是多项式核：\n$$\n\\kappa(\\mathbf{x},\\mathbf{y}) = \\left(\\alpha \\, \\mathbf{x}^\\top \\mathbf{y} + \\beta\\right)^\\delta\n$$\n这个核函数计算两个向量在高维特征空间中的点积，而无需显式地生成该空间的特征向量。对于半径为 $r$ 的圆上的一个数据点 $\\mathbf{x}$，其范数的平方为 $\\|\\mathbf{x}\\|^2 = \\mathbf{x}^\\top \\mathbf{x} = r^2$。对单个点求值的核函数为 $\\kappa(\\mathbf{x},\\mathbf{x}) = (\\alpha \\|\\mathbf{x}\\|^2 + \\beta)^\\delta = (\\alpha r^2 + \\beta)^\\delta$。由于半径 $r_A$ 和 $r_B$ 不同，这个值在每个群内部是恒定的，但在两个群之间是不同的。这意味着在特征空间中，这两个群被映射到可以根据范数区分的流形上。这种与范数相关的特征差异正是PCA设计用来捕获的方差来源。因此，预计至少有一个主成分会与这个分离特征强对齐，从而使得在PCA导出的嵌入中，这两个群是线性可分的。\n\n为每个测试用例执行的算法如下：\n\n1.  **数据生成**：构建总数据集 $X \\in \\mathbb{R}^{N \\times 2}$，其中 $N = n_A + n_B$。前 $n_A$ 行对应细胞群 $A$，随后的 $n_B$ 行对应细胞群 $B$，点的坐标根据给定的公式计算。\n\n2.  **核矩阵构建**：计算 $N \\times N$ 的格拉姆矩阵 $K$，其中每个元素 $K_{ij}$ 是第 $i$ 个点和第 $j$ 个点的核函数求值结果，$K_{ij} = \\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$。\n\n3.  **核矩阵中心化**：对核矩阵进行中心化，以对应于在特征空间中对中心化数据进行PCA。中心化核矩阵 $K_c$ 计算为 $K_c = H K H$，其中 $H = I_N - \\frac{1}{N}\\mathbf{1}\\mathbf{1}^\\top$ 是中心化矩阵。\n\n4.  **特征分解**：求解对称矩阵 $K_c$ 的特征值问题：$K_c V = V \\Lambda$。这将产生 $N$ 个实特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_N$（存储在对角矩阵 $\\Lambda$ 中），以及一个列为相应标准正交特征向量的矩阵 $V$。\n\n5.  **数据嵌入**：使用前 $m$ 个主成分构建嵌入 $Z \\in \\mathbb{R}^{N \\times m}$。根据问题的定义，第 $i$ 个数据点在嵌入空间中的坐标由以下公式给出：\n    $$\n    Z_{i\\ell} = \\sqrt{\\lambda_\\ell} V_{i\\ell}, \\quad \\text{对于 } \\ell \\in \\{1, \\dots, m\\}\n    $$\n    这将对与 $m$ 个最大的正特征值对应的分量执行。\n\n6.  **线性可分性测试**：最后一步是确定两组嵌入点 $Z_A$（$Z$的前 $n_A$ 行）和 $Z_B$（$Z$的后 $n_B$ 行）在 $\\mathbb{R}^m$ 中是否线性可分。两个有限点集是线性可分的，当且仅当存在一个超平面将它们分开。这可以被表述为一个线性规划可行性问题。如果存在一个解，则细胞群是线性可分的；否则，它们不是。\n\n一个例外是线性核（$\\delta=1, \\beta=0$）的情况，此时核PCA等同于对原始数据进行标准PCA。由于原始数据形成两个已中心化的同心圆，标准PCA将找不到分离方向，因为所有方向上的方差都相等（对于连续圆）。因此，变换后的数据仍然是非线性可分的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Solves the Kernel PCA linear separability problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (n_A, n_B, r_A, r_B, alpha, beta, delta, m)\n        (24, 24, 1.0, 2.0, 1.0, 1.0, 2, 2),\n        (24, 24, 1.0, 2.0, 1.0, 0.0, 1, 2),\n        (24, 24, 1.0, 1.4, 0.2, 1.0, 2, 2),\n        (24, 24, 1.0, 2.0, 1.0, 1.0, 2, 1),\n        (8, 8, 1.0, 2.0, 1.0, 1.0, 3, 2),\n    ]\n\n    results = []\n    for case in test_cases:\n        n_a, n_b, r_a, r_b, alpha, beta, delta, m = case\n        N = n_a + n_b\n\n        # Step 1: Data Generation\n        t_a = np.linspace(0, 2 * np.pi, n_a, endpoint=False)\n        pop_a = r_a * np.c_[np.cos(t_a), np.sin(t_a)]\n\n        t_b = np.linspace(0, 2 * np.pi, n_b, endpoint=False)\n        pop_b = r_b * np.c_[np.cos(t_b), np.sin(t_b)]\n\n        X = np.vstack([pop_a, pop_b])\n\n        # Step 2: Kernel Matrix Construction\n        # K_ij = (alpha * x_i^T x_j + beta)^delta\n        K = (alpha * (X @ X.T) + beta) ** delta\n\n        # Step 3: Kernel Matrix Centering\n        # K_c = H K H, where H = I - 1/N * 1_N\n        one_n = np.ones((N, N)) / N\n        K_c = K - one_n @ K - K @ one_n + one_n @ K @ one_n\n\n        # Step 4: Eigendecomposition\n        # Use eigh for symmetric matrices. It returns eigenvalues in ascending order.\n        eigenvalues, eigenvectors = np.linalg.eigh(K_c)\n\n        # Sort eigenvalues and eigenvectors in descending order\n        sorted_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sorted_indices]\n        eigenvectors = eigenvectors[:, sorted_indices]\n        \n        # Step 5: Data Embedding\n        # Select top m components with positive eigenvalues\n        valid_indices = np.where(eigenvalues > 1e-9)[0]\n        if len(valid_indices) < m:\n            # If not enough positive eigenvalues, separability might be ambiguous\n            # or impossible. It is safe to assume not separable.\n            results.append(False)\n            continue\n            \n        top_m_indices = valid_indices[:m]\n        lambdas_m = eigenvalues[top_m_indices]\n        V_m = eigenvectors[:, top_m_indices]\n\n        # Z_il = V_il / sqrt(lambda_l)\n        Z = V_m / np.sqrt(lambdas_m)\n        \n        # Get the embedding for each population\n        Z_a = Z[:n_a, :]\n        Z_b = Z[n_a:, :]\n\n        # Step 6: Linear Separability Test using Linear Programming\n        # We want to find w, b such that:\n        # w^T z + b >= 1 for z in Z_a\n        # w^T z + b <= -1 for z in Z_b\n        #\n        # In linprog format (A_ub @ x <= b_ub):\n        # -w^T z - b <= -1 for z in Z_a\n        #  w^T z + b <= -1 for z in Z_b\n        # where x = [w_1, ..., w_m, b]\n        \n        # The number of variables for linprog is m (for w) + 1 (for b)\n        num_vars = m + 1\n        \n        # The objective function is irrelevant; we only care about feasibility.\n        c = np.zeros(num_vars)\n        \n        # Construct the inequality constraints matrix A_ub and vector b_ub\n        A_ub = np.zeros((N, num_vars))\n        \n        # Constraints for population A\n        A_ub[:n_a, :m] = -Z_a\n        A_ub[:n_a, m] = -1\n        \n        # Constraints for population B\n        A_ub[n_a:, :m] = Z_b\n        A_ub[n_a:, m] = 1\n        \n        b_ub = np.full(N, -1.0)\n\n        # The variables w and b are unbounded.\n        bounds = (None, None)\n        \n        # Solve the linear program\n        # The 'highs' method is robust and recommended for new code.\n        res = linprog(c, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method='highs')\n        \n        # res.success is True if a feasible solution was found.\n        results.append(res.success)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}