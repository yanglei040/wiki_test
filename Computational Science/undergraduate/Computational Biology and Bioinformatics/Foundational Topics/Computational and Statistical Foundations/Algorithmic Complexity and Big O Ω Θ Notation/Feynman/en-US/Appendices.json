{
    "hands_on_practices": [
        {
            "introduction": "In computational biology, simulating the physical movement of molecules is a cornerstone for understanding their function. This first practice invites you to analyze the computational heart of a molecular dynamics simulation. By examining a simplified 'leapfrog' integration algorithm, you'll learn to count the fundamental operations and determine how the runtime scales with the number of particles, $N$. This exercise is fundamental for understanding why brute-force simulations of large systems are so computationally demanding and provides a concrete example of an algorithm with quadratic time complexity but linear space complexity .",
            "id": "2372962",
            "problem": "A molecular dynamics code advances a system of $N$ identical particles of mass $m$ in three spatial dimensions using a leapfrog integrator with pairwise central forces. One full time step of size $\\Delta t$ is implemented as follows, where bold symbols denote vectors in $\\mathbb{R}^{3}$ and indices run over particles:\n\n- Half-kick: For $i = 1,\\dots,N$, update velocities by\n$$\\mathbf{v}_{i} \\leftarrow \\mathbf{v}_{i} + \\frac{\\Delta t}{2}\\,\\mathbf{a}_{i}.$$\n\n- Drift: For $i = 1,\\dots,N$, update positions by\n$$\\mathbf{x}_{i} \\leftarrow \\mathbf{x}_{i} + \\Delta t\\,\\mathbf{v}_{i}.$$\n\n- Recompute accelerations:\n  - Reset: For $i = 1,\\dots,N$, set\n  $$\\mathbf{a}_{i} \\leftarrow \\mathbf{0}.$$\n  - Pairwise accumulation: For $i = 1,\\dots,N-1$, and for $j = i+1,\\dots,N$, compute the relative displacement\n  $$\\mathbf{r}_{ij} = \\mathbf{x}_{i} - \\mathbf{x}_{j},$$\n  evaluate a central pairwise force of the form\n  $$\\mathbf{f}_{ij} = \\varphi\\!\\left(|\\mathbf{r}_{ij}|\\right)\\,\\mathbf{r}_{ij},$$\n  where $\\varphi$ is a fixed function independent of $N$, and accumulate\n  $$\\mathbf{a}_{i} \\leftarrow \\mathbf{a}_{i} + \\frac{\\mathbf{f}_{ij}}{m}, \\qquad \\mathbf{a}_{j} \\leftarrow \\mathbf{a}_{j} - \\frac{\\mathbf{f}_{ij}}{m}.$$\n\n- Half-kick: For $i = 1,\\dots,N$, update velocities by\n$$\\mathbf{v}_{i} \\leftarrow \\mathbf{v}_{i} + \\frac{\\Delta t}{2}\\,\\mathbf{a}_{i}.$$\n\nAssumptions:\n- The spatial dimension $d=3$ is fixed and does not scale with $N$.\n- Evaluating $\\varphi\\!\\left(|\\mathbf{r}_{ij}|\\right)$, forming $\\mathbf{f}_{ij}$, and applying the two acceleration updates for a single pair $(i,j)$ takes a constant number of arithmetic operations independent of $N$, and uses a constant amount of additional memory.\n- There are no neighbor lists, cutoffs, domain decompositions, or other data structures that alter the asymptotic pair count.\n- All arithmetic operations run in constant time, and loop overheads are linear in the number of iterations.\n- The integrator stores the arrays $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$, $\\{\\mathbf{v}_{i}\\}_{i=1}^{N}$, and $\\{\\mathbf{a}_{i}\\}_{i=1}^{N}$; do not count any other data beyond constant-size parameters and loop indices.\n\nUsing the definition of algorithmic time complexity and space complexity, determine, as functions of $N$ and in Big-O notation, the total time complexity per single time step and the total space complexity of the integrator state described above. Provide your final answer as a row matrix with two entries, the first being the time complexity and the second being the space complexity, each expressed as a single Big-O expression.",
            "solution": "The problem statement is first validated for correctness and solvability.\n\n**Step 1: Extraction of Givens**\n- System: $N$ identical particles of mass $m$ in $3$ spatial dimensions.\n- Algorithm: A single time step of a leapfrog integrator of size $\\Delta t$.\n  - Step 1: Half-kick velocity update: $\\mathbf{v}_{i} \\leftarrow \\mathbf{v}_{i} + \\frac{\\Delta t}{2}\\,\\mathbf{a}_{i}$ for $i = 1,\\dots,N$.\n  - Step 2: Drift position update: $\\mathbf{x}_{i} \\leftarrow \\mathbf{x}_{i} + \\Delta t\\,\\mathbf{v}_{i}$ for $i = 1,\\dots,N$.\n  - Step 3: Recompute accelerations $\\mathbf{a}_i$. This involves resetting all $\\mathbf{a}_i$ to $\\mathbf{0}$ and then performing a pairwise force accumulation over all unique pairs $(i,j)$ with $i<j$. The force is $\\mathbf{f}_{ij} = \\varphi(|\\mathbf{r}_{ij}|)\\,\\mathbf{r}_{ij}$, and updates are $\\mathbf{a}_{i} \\leftarrow \\mathbf{a}_{i} + \\frac{\\mathbf{f}_{ij}}{m}$ and $\\mathbf{a}_{j} \\leftarrow \\mathbf{a}_{j} - \\frac{\\mathbf{f}_{ij}}{m}$.\n  - Step 4: Second half-kick velocity update, identical to Step $1$.\n- Assumptions:\n  - Spatial dimension $d=3$ is a fixed constant.\n  - The computational work for a single pair force calculation and accumulation is $O(1)$ (constant time).\n  - No algorithmic optimizations like neighbor lists are used; all pairs are checked.\n  - Arithmetic operations are $O(1)$; loop overheads are linear in the number of iterations.\n- Data Storage for Space Complexity: The arrays for position $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$, velocity $\\{\\mathbf{v}_{i}\\}_{i=1}^{N}$, and acceleration $\\{\\mathbf{a}_{i}\\}_{i=1}^{N}$.\n\n**Step 2: Validation**\nThe problem describes a standard brute-force N-body simulation using the Velocity Verlet algorithm, a common variant of the leapfrog integrator. The scenario is scientifically grounded in classical mechanics and computational physics. The problem is well-posed, providing a clear algorithm and sufficient assumptions to perform an unambiguous complexity analysis. All terms are defined, and the premises are consistent. There are no elements of pseudoscience, subjectivity, or logical contradiction.\n\n**Step 3: Verdict**\nThe problem is deemed valid. A solution will be constructed.\n\n**Analysis of Time Complexity**\n\nThe total time complexity for one full time step is the sum of the complexities of its constituent parts. We analyze each part as a function of the number of particles, $N$.\n\n1.  **First Half-kick**: This step involves a loop that iterates from $i=1$ to $N$. Inside the loop, the velocity vector $\\mathbf{v}_{i}$ is updated. Since the spatial dimension is fixed at $3$, this vector operation involves a constant number of floating-point operations (one scalar-vector multiplication and one vector addition). Therefore, the work inside the loop is $O(1)$. The total time complexity for this step is the number of iterations multiplied by the work per iteration, which is $N \\times O(1) = O(N)$.\n\n2.  **Drift**: This step is structurally identical to the first. A loop runs from $i=1$ to $N$, and inside, a position vector $\\mathbf{x}_{i}$ is updated. This is also a constant-time operation for a fixed-dimension vector. The total time complexity is thus $N \\times O(1) = O(N)$.\n\n3.  **Recompute Accelerations**: This part consists of two sub-steps.\n    -   **Reset**: The accelerations for all $N$ particles are set to the zero vector, $\\mathbf{a}_{i} \\leftarrow \\mathbf{0}$. This is done in a loop from $i=1$ to $N$. The operation is constant time for each particle. The complexity of this sub-step is $O(N)$.\n    -   **Pairwise Accumulation**: This is the computationally dominant part. The algorithm specifies a double loop: `for i = 1,...,N-1` and `for j = i+1,...,N`. This structure iterates over all unique pairs of particles $(i,j)$. The total number of such pairs is given by the sum:\n    $$ \\sum_{i=1}^{N-1} \\sum_{j=i+1}^{N} 1 = \\sum_{i=1}^{N-1} (N - i) = (N-1) + (N-2) + \\dots + 1 = \\frac{(N-1)N}{2} $$\n    The number of pairs is $\\frac{1}{2}N^2 - \\frac{1}{2}N$. For each pair, the algorithm computes the relative displacement $\\mathbf{r}_{ij}$, evaluates the force $\\mathbf{f}_{ij}$, and updates two acceleration vectors, $\\mathbf{a}_i$ and $\\mathbf{a}_j$. According to the problem's assumptions, all these operations for a single pair take a constant amount of time, $O(1)$. Therefore, the total time complexity for this sub-step is the number of pairs multiplied by the constant work per pair: $(\\frac{1}{2}N^2 - \\frac{1}{2}N) \\times O(1) = O(N^2)$.\n    The total complexity for recomputing accelerations is the sum of its sub-steps: $O(N) + O(N^2) = O(N^2)$.\n\n4.  **Second Half-kick**: This step is identical to the first half-kick, involving a loop over $N$ particles with $O(1)$ work inside. Its complexity is $O(N)$.\n\n**Total Time Complexity**: The total time complexity per time step, $T(N)$, is the sum of the complexities of all four steps:\n$$ T(N) = O(N) + O(N) + O(N^2) + O(N) $$\nIn Big-O notation, we retain only the highest-order term. Thus, the total time complexity is $O(N^2)$.\n\n**Analysis of Space Complexity**\n\nThe space complexity is determined by the amount of memory required to store the state of the system, as specified in the problem.\n\n-   **Position data**: The array $\\{\\mathbf{x}_{i}\\}_{i=1}^{N}$ stores the positions of $N$ particles. Each position $\\mathbf{x}_i$ is a vector in $\\mathbb{R}^3$, requiring storage for $3$ numbers (e.g., doubles). The memory required for one particle's position is constant. Thus, the total memory for all positions is proportional to $N$, which is $O(N)$.\n\n-   **Velocity data**: Similarly, the array $\\{\\mathbf{v}_{i}\\}_{i=1}^{N}$ stores the velocities of $N$ particles. Each velocity $\\mathbf{v}_i$ is also a $3$-dimensional vector, so the total storage required is $O(N)$.\n\n-   **Acceleration data**: The array $\\{\\mathbf{a}_{i}\\}_{i=1}^{N}$ stores the accelerations of $N$ particles. Each acceleration $\\mathbf{a}_i$ is a $3$-dimensional vector, so the storage required is also $O(N)$.\n\n**Total Space Complexity**: The total space complexity, $S(N)$, is the sum of the memory requirements for these three arrays. The problem specifies to not count other data like constant-size parameters.\n$$ S(N) = O(N) + O(N) + O(N) = O(N) $$\nThus, the total space complexity of the integrator state is $O(N)$.\n\nIn summary, the time complexity is dominated by the pairwise force calculation, scaling quadratically with the number of particles, while the space complexity scales linearly as we must store state information for each particle.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nO(N^2) & O(N)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond simulation, a major part of bioinformatics involves analyzing experimental or computational data. In this exercise, we will analyze the efficiency of a common data analysis task: fitting a power-law model to a dataset. This practice will guide you through a multi-step procedure involving data transformation and linear regression, asking you to determine its overall time complexity . You will discover how breaking down a problem into sequential parts is a key strategy for complexity analysis and see how even sophisticated statistical methods can sometimes be surprisingly efficient.",
            "id": "2372946",
            "problem": "In computational physics, scaling laws are frequently analyzed by fitting a power-law model of the form $y = C x^{\\alpha}$ to experimental or simulation data. A common approach is to perform a linear regression on the logarithms: define $z_i = \\ln(x_i)$ and $w_i = \\ln(y_i)$ for $i = 1, 2, \\dots, N$, and fit the linear model $w_i \\approx \\beta_0 + \\beta_1 z_i$ using ordinary least squares. Assume the following realistic computational model: each floating-point addition, subtraction, multiplication, division, and each evaluation of the natural logarithm $\\,\\ln(\\cdot)\\,$ and the exponential $\\,\\exp(\\cdot)\\,$ has constant-time cost independent of $N$; input data are already in memory; and the solver for the linear regression is a standard numerically stable method that either computes sufficient statistics and solves a $2 \\times 2$ system or performs a Householder-QR factorization of an $N \\times 2$ design matrix, with $2$ being independent of $N$. Using only fundamental definitions and without invoking any prepackaged complexity results, derive the asymptotic time complexity, as a function of $N$, for performing this power-law fit (including the logarithmic transformation and the linear regression solve). Express your final answer as a single Big O expression in terms of $N$. No numerical rounding is required, and no physical units are needed in the final expression.",
            "solution": "The problem statement is critically validated and is deemed to be valid. It is a well-posed problem in numerical analysis and computational physics, is scientifically grounded, and is expressed with objective and precise language. All necessary information for a rigorous derivation is provided.\n\nThe objective is to derive the asymptotic time complexity, as a function of the number of data points $N$, for fitting a power-law model $y = C x^{\\alpha}$ to a dataset $\\{(x_i, y_i)\\}_{i=1}^N$. The specified procedure involves two sequential stages: a logarithmic data transformation followed by a linear regression solved using ordinary least squares (OLS). We will analyze the computational cost of each stage individually and then combine them to find the total complexity.\n\n**Stage 1: Logarithmic Data Transformation**\n\nThe power-law relationship $y = C x^{\\alpha}$ is linearized by taking the natural logarithm of both sides, yielding a linear equation:\n$$ \\ln(y) = \\ln(C x^{\\alpha}) = \\ln(C) + \\alpha \\ln(x) $$\nThis transformation is applied to each of the $N$ data points. For each point $(x_i, y_i)$, we compute a new point $(z_i, w_i)$ defined as:\n$$ z_i = \\ln(x_i) $$\n$$ w_i = \\ln(y_i) $$\nThe resulting linear model to be fitted is $w_i \\approx \\beta_0 + \\beta_1 z_i$, where the parameters correspond to $\\beta_0 = \\ln(C)$ and $\\beta_1 = \\alpha$.\n\nAccording to the problem's computational model, the evaluation of the natural logarithm function, $\\ln(\\cdot)$, has a constant time cost. Let this cost be $c_{log}$. For each of the $N$ data points, we perform two such evaluations. Therefore, the total time required for the transformation stage, $T_{transform}$, is:\n$$ T_{transform}(N) = \\sum_{i=1}^{N} (c_{log} + c_{log}) = N \\cdot (2 c_{log}) $$\nIn terms of asymptotic complexity, this is linear in $N$.\n$$ T_{transform}(N) = O(N) $$\n\n**Stage 2: Ordinary Least Squares Linear Regression**\n\nAfter the transformation, we must determine the parameters $\\beta_0$ and $\\beta_1$ for the linear model $w_i \\approx \\beta_0 + \\beta_1 z_i$. OLS achieves this by minimizing the sum of squared residuals, $S(\\beta_0, \\beta_1)$:\n$$ S(\\beta_0, \\beta_1) = \\sum_{i=1}^{N} (w_i - (\\beta_0 + \\beta_1 z_i))^2 $$\nThe problem states that a standard numerically stable method is used, giving two examples: solving the normal equations via sufficient statistics or using Householder-QR factorization. We analyze both to confirm their complexity.\n\n**Method A: Solving the Normal Equations**\nThe minimization of $S$ leads to a $2 \\times 2$ system of linear equations known as the normal equations:\n$$ \\begin{pmatrix} N & \\sum_{i=1}^{N} z_i \\\\ \\sum_{i=1}^{N} z_i & \\sum_{i=1}^{N} z_i^2 \\end{pmatrix} \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix} = \\begin{pmatrix} \\sum_{i=1}^{N} w_i \\\\ \\sum_{i=1}^{N} z_i w_i \\end{pmatrix} $$\nTo set up this system, we must first compute the four required sums (sufficient statistics). Each sum is computed over the $N$ transformed data points.\n- $\\sum_{i=1}^{N} z_i$: Requires $N-1$ additions, which is $O(N)$.\n- $\\sum_{i=1}^{N} w_i$: Requires $N-1$ additions, which is $O(N)$.\n- $\\sum_{i=1}^{N} z_i^2$: Requires $N$ multiplications and $N-1$ additions, which is $O(N)$.\n- $\\sum_{i=1}^{N} z_i w_i$: Requires $N$ multiplications and $N-1$ additions, which is $O(N)$.\n\nThese sums can all be computed in a single pass through the data, requiring a constant number of arithmetic operations per data point. Thus, the total cost to compute the coefficients of the matrix and the right-hand side vector is $O(N)$.\nOnce constructed, we must solve this $2 \\times 2$ linear system. The size of the system is constant and independent of $N$. Solving a fixed-size system requires a constant number of operations, giving a complexity of $O(1)$.\nThe total complexity for this method is the sum of the costs of these two steps: $T_{reg-A}(N) = O(N) + O(1) = O(N)$.\n\n**Method B: Householder-QR Factorization**\nAlternatively, the problem can be expressed in matrix form as minimizing $\\|A\\beta - W\\|_2^2$, where $W$ is the $N \\times 1$ vector of $w_i$ values, $\\beta = (\\beta_0, \\beta_1)^T$, and $A$ is the $N \\times 2$ design matrix:\n$$ A = \\begin{pmatrix} 1 & z_1 \\\\ 1 & z_2 \\\\ \\vdots & \\vdots \\\\ 1 & z_N \\end{pmatrix} $$\nThe solution using QR factorization involves decomposing $A = QR$, where $Q$ is an $N \\times N$ orthogonal matrix and $R$ is an $N \\times 2$ upper trapezoidal matrix. For an $N \\times 2$ matrix, a Householder-based QR factorization requires a sequence of $2$ Householder transformations.\n1. The first Householder reflection is constructed to zero out the $N-1$ elements below the diagonal in the first column. This involves vector norms and subtractions on vectors of length $N$, costing $O(N)$. Applying this reflection to the second column costs an additional $O(N)$.\n2. The second reflection operates on an $(N-1) \\times 1$ sub-problem, costing $O(N-1) = O(N)$.\nThe total cost to obtain the QR factorization is $O(N) + O(N) = O(N)$.\nNext, the problem is transformed to solving $R\\beta = Q^T W$. The product $Q^T W$ is computed by applying the two reflections to $W$, which costs $O(N) + O(N-1) = O(N)$. Finally, we solve the resulting $2 \\times 2$ upper triangular system for $\\beta$ via back substitution, which takes $O(1)$ time.\nThe total complexity for this method is again the sum of the step costs: $T_{reg-B}(N) = O(N) + O(N) + O(1) = O(N)$.\n\n**Total Complexity**\n\nBoth standard methods for the linear regression stage have a time complexity of $O(N)$. The entire algorithm is a sequence of the transformation and regression stages. The total time complexity, $T_{total}(N)$, is the sum of their complexities:\n$$ T_{total}(N) = T_{transform}(N) + T_{regression}(N) = O(N) + O(N) $$\nBased on the fundamental rules of Big O notation, this simplifies to:\n$$ T_{total}(N) = O(N) $$\nThe total time to perform the power-law fit grows linearly with the number of data points.",
            "answer": "$$ \\boxed{O(N)} $$"
        },
        {
            "introduction": "An algorithm's performance is not just a property of its code; it is deeply intertwined with the structure of the input data. In our final practice, you will engage in some 'adversarial thinking' by exploring what makes a suffix-tree-based read mapper slow. Your task is to identify which types of genomic sequences would trigger the algorithm's worst-case performance, leading to a massive number of reported matches . This exercise will sharpen your intuition about the critical link between data characteristics, such as repetitiveness, and algorithmic efficiency, a crucial insight for anyone working with real-world genomic data.",
            "id": "2370310",
            "problem": "A suffix-tree-based exact read mapper is built on a reference Deoxyribonucleic Acid (DNA) string $T$ of length $N$ over the alphabet $\\Sigma=\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$. For a query read (pattern) $P$ of length $L$, the mapper follows the standard suffix tree traversal to find all exact occurrences of $P$ in $T$ and reports every starting position where $P$ occurs. Assume $1 \\le L \\le N/2$. You are allowed to choose the reference genome sequence $T$ (of length $N$) adversarially as a function of $N$.\n\nWhich of the following families of genomes $T_N$ force the per-query running time of any suffix-tree-based exact matcher that reports all occurrences to be in $\\Theta(L+N)$, in the sense that for every $L$ with $1 \\le L \\le N/2$ there exists some read $P$ of length $L$ whose query on $T_N$ requires $\\Theta(L+N)$ time? Select all that apply.\n\nA. $T_N$ is the homopolymer string consisting of $\\text{A}$ repeated $N$ times.\n\nB. $T_N$ is a De Bruijn sequence of order $k=\\lfloor \\log_{4} N \\rfloor$ over $\\Sigma$ (so that every length-$k$ string over $\\Sigma$ appears exactly once as a substring).\n\nC. $T_N$ is drawn uniformly at random with characters Independent and Identically Distributed (i.i.d.) over $\\Sigma$.\n\nD. $T_N$ is the period-$2$ tandem repeat $T_N = (\\text{AC})^{\\lfloor N/2 \\rfloor}$ (truncated to length $N$ if necessary).\n\nE. $T_N$ is the period-$4$ tandem repeat $T_N = (\\text{ACGT})^{\\lfloor N/4 \\rfloor}$ (truncated to length $N$ if necessary).",
            "solution": "The user has presented a problem concerning the worst-case time complexity of suffix-tree-based exact read mapping. A full validation of the problem statement is required before proceeding to a solution.\n\n### Step 1: Extract Givens\n\n*   **Algorithm**: A suffix-tree-based exact read mapper.\n*   **Reference String**: $T$, a Deoxyribonucleic Acid (DNA) string of length $N$.\n*   **Alphabet**: $\\Sigma=\\{\\text{A},\\text{C},\\text{G},\\text{T}\\}$.\n*   **Query Read**: $P$, a pattern of length $L$.\n*   **Task**: Find all exact occurrences of $P$ in $T$ and report every starting position.\n*   **Constraint on $L$**: $1 \\le L \\le N/2$.\n*   **Adversarial Condition**: The reference genome $T$ (as a function of $N$, denoted $T_N$) can be chosen adversarially.\n*   **Question**: Identify which families of genomes $T_N$ force the per-query running time of *any* such mapper to be in $\\Theta(L+N)$. The forcing condition is specified as: for every $L$ in the range $1 \\le L \\le N/2$, there must exist some read $P$ of length $L$ for which the query time on $T_N$ is $\\Theta(L+N)$.\n\n### Step 2: Validate Using Extracted Givens\n\n*   **Scientific Grounding**: The problem is grounded in the well-established fields of algorithm design, stringology, and bioinformatics. Suffix trees are a standard data structure for string matching, and their complexity analysis is a staple of computer science. The concepts of DNA sequences, alphabets, and read mapping are central to bioinformatics. The problem is scientifically and mathematically sound.\n*   **Well-Posedness**: The problem is well-posed. It asks for specific input classes ($T_N$) that elicit the known worst-case performance of a standard algorithm. The performance metric ($\\Theta(L+N)$) and the conditions under which it must hold are precisely defined. A unique set of correct options exists based on standard algorithmic analysis.\n*   **Objectivity**: The problem statement is expressed in objective, technical language. There are no subjective or ambiguous terms.\n*   **Conclusion**: The problem statement is self-contained, consistent, and scientifically valid. It does not violate any of the criteria for an invalid problem.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. A full solution will be derived.\n\n### Derivation of Solution\n\nThe time complexity of finding all occurrences of a pattern $P$ of length $L$ in a reference $T$ of length $N$ using a pre-built suffix tree for $T$ has two components:\n1.  **Matching Phase**: Traversing the suffix tree from the root to match the characters of $P$. This takes time proportional to the length of the pattern, which is $O(L)$.\n2.  **Reporting Phase**: If the pattern $P$ is found, the traversal ends at a specific node, say $u$, in the tree. Every leaf in the subtree rooted at $u$ corresponds to a starting position of an occurrence of $P$ in $T$. Let $z$ be the total number of occurrences of $P$. To report all these occurrences, one must visit all $z$ leaves in the subtree of $u$. This can be done in $O(z)$ time via a simple traversal (e.g., Depth First Search) of the subtree.\n\nThe total time complexity for a query is therefore $O(L+z)$. The problem asks for cases where this complexity becomes $\\Theta(L+N)$. This requires the number of occurrences, $z$, to be on the order of $N$, i.e., $z = \\Omega(N)$. Since $z$ can be at most $N-L+1$, $z$ is also $O(N)$. Thus, the condition simplifies to finding genomes $T_N$ such that for any $L$ in the given range, there exists a pattern $P$ of length $L$ for which $z = \\Theta(N)$.\n\nWe now analyze each option.\n\n**A. $T_N$ is the homopolymer string consisting of $\\text{A}$ repeated $N$ times.**\n\nLet $T_N = \\text{A}^N$. We must show that for any given $L$ where $1 \\le L \\le N/2$, there exists a pattern $P$ of length $L$ that occurs $\\Theta(N)$ times. Let us choose the pattern $P = \\text{A}^L$. This pattern consists of $L$ consecutive 'A' characters. This pattern occurs in $T_N$ starting at positions $1, 2, \\dots, N-L+1$. The total number of occurrences is $z = N - L + 1$. Since we are given that $L \\le N/2$, we have $z = N - L + 1 \\ge N - N/2 + 1 = N/2 + 1$. Therefore, $z = \\Theta(N)$. The query time is $\\Theta(L+z) = \\Theta(L+N)$. This holds for our choice of $P$ for any valid $L$. Thus, this option satisfies the adversarial condition.\n\n**Verdict: Correct.**\n\n**B. $T_N$ is a De Bruijn sequence of order $k=\\lfloor \\log_{4} N \\rfloor$ over $\\Sigma$.**\n\nA De Bruijn sequence $T_N$ of order $k$ over an alphabet of size $|\\Sigma|=4$ has length $N=4^k$. By definition, every possible string of length $k$ (a $k$-mer) over $\\Sigma$ appears as a substring of $T_N$ exactly once. The problem requires that *for every* $L$ in the range $1 \\le L \\le N/2$, there is a pattern $P$ with $\\Theta(N)$ occurrences. Let us test this for $L=k=\\lfloor \\log_4 N \\rfloor$. For $N \\ge 4$, $k = \\lfloor \\log_4 N \\rfloor \\le \\log_4 N \\le N/2$, so this is a valid choice of $L$. For any pattern $P$ of length $L=k$, the number of occurrences in $T_N$ is exactly $z=1$. The query time is $\\Theta(L+z) = \\Theta(k+1) = \\Theta(\\log_4 N)$. This is not $\\Theta(L+N) = \\Theta(k+N) = \\Theta(N)$. Since the condition fails for $L=k$, a De Bruijn sequence does not force the worst-case running time for all specified values of $L$.\n\n**Verdict: Incorrect.**\n\n**C. $T_N$ is drawn uniformly at random with characters Independent and Identically Distributed (i.i.d.) over $\\Sigma$.**\n\nFor a random string $T_N$, we can analyze the expected number of occurrences of a pattern $P$. For a fixed pattern $P$ of length $L$, the probability of it appearing at any given position is $(1/4)^L$. The expected number of occurrences is $E[z] = (N-L+1)(1/4)^L$. The adversarial condition requires that for *every* $L \\in [1, N/2]$, we can find *some* $P$ of length $L$ that occurs $\\Theta(N)$ times. Let us choose a large $L$, for example $L = \\lceil \\log_4 N \\rceil + 1$. This $L$ is in the range $[1, N/2]$ for sufficiently large $N$. For this $L$, the expected number of occurrences of any single pattern $P$ is $E[z] \\approx N \\cdot (1/4)^{\\log_4 N + 1} = N \\cdot (1/N \\cdot 1/4) = 1/4$. By linearity of expectation, the expected total number of occurrences over all $4^L$ possible patterns is also small. With very high probability, for large $L$, no substring of length $L$ repeats many times, and certainly not $\\Theta(N)$ times. A random string is characterized by low repetitiveness, which is the opposite of what is needed to trigger the worst-case time complexity. The condition fails.\n\n**Verdict: Incorrect.**\n\n**D. $T_N$ is the period-$2$ tandem repeat $T_N = (\\text{AC})^{\\lfloor N/2 \\rfloor}$ (truncated to length $N$ if necessary).**\n\nLet $T_N = \\text{ACACAC}\\dots$. We must show that for any $L$ with $1 \\le L \\le N/2$, there exists a pattern $P$ of length $L$ with $\\Theta(N)$ occurrences. For any given $L$, let us choose $P$ to be the prefix of $T_N$ of length $L$, i.e., $P = T_N[1..L]$. This pattern is an alternating sequence of 'A's and 'C's. This pattern will reappear in $T_N$ starting at every other position, specifically at positions $1, 3, 5, \\dots$, provided the starting character matches. The number of occurrences $z$ will be approximately $(N-L)/2 \\approx N/2$. Since $L \\le N/2$, $z \\ge (N-N/2)/2 = N/4$. Thus, $z=\\Theta(N)$. The query time is $\\Theta(L+z) = \\Theta(L+N)$. This holds for any valid $L$.\n\n**Verdict: Correct.**\n\n**E. $T_N$ is the period-$4$ tandem repeat $T_N = (\\text{ACGT})^{\\lfloor N/4 \\rfloor}$ (truncated to length $N$ if necessary).**\n\nThe logic is identical to that for option D. Let $T_N = \\text{ACGTACGT}\\dots$. For any given $L$ with $1 \\le L \\le N/2$, we choose the pattern $P$ to be the prefix of $T_N$ of length $L$, i.e., $P = T_N[1..L]$. This pattern is a substring of the repeating unit 'ACGT'. This pattern will reappear in $T_N$ every $4$ positions, starting at positions $1, 5, 9, \\dots$. The number of occurrences $z$ will be approximately $(N-L)/4 \\approx N/4$. Since $L \\le N/2$, $z \\ge (N-N/2)/4 = N/8$. Thus, $z=\\Theta(N)$. The query time is $\\Theta(L+z) = \\Theta(L+N)$. This holds for any valid $L$.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ADE}$$"
        }
    ]
}