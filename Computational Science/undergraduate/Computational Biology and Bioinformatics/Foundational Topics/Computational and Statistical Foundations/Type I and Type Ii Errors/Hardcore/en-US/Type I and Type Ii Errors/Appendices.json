{
    "hands_on_practices": [
        {
            "introduction": "A diagnostic test's performance metrics, such as sensitivity and specificity, can be misleading when viewed in isolation. This exercise demonstrates how the prevalence of a condition in a population dramatically impacts a test's real-world utility, particularly its Positive Predictive Value (PPV). By working through this scenario , you will see how a test with excellent specificity can still generate a surprisingly high number of false positives when screening for a rare disease.",
            "id": "2438715",
            "problem": "A genomics lab is evaluating a binary variant-detection assay to screen a large biobank for a rare Mendelian disease. The population prevalence is $p = 10^{-4}$. The assay has sensitivity $0.99$ and specificity $0.999$. The lab plans to screen $10^{6}$ unrelated individuals.\n\nWhich of the following statements about the expected outcomes among those tested is correct?\n\nA. With these parameters, the expected number of false positives exceeds the expected number of true positives, and the positive predictive value is approximately $9\\%$.\n\nB. With these parameters, the expected number of false positives is less than $10$, and the positive predictive value exceeds $90\\%$.\n\nC. If specificity were increased from $0.999$ to $0.9999$ while holding prevalence and sensitivity fixed, the positive predictive value would increase only slightly, from about $9\\%$ to about $10\\%$.\n\nD. If sensitivity were decreased from $0.99$ to $0.90$ while holding prevalence and specificity fixed, the expected number of false positives would decrease by about $10\\%$.\n\nE. For any prevalence, the fraction of positive test results that are false positives equals the Type I error rate.",
            "solution": "Let $D+$ denote the state of having the disease and $D-$ denote the state of not having the disease. Let $T+$ and $T-$ be the outcomes of a positive and negative test, respectively. The problem provides the following parameters: a total of $N = 10^{6}$ individuals are tested. The prevalence of the disease is $p = P(D+) = 10^{-4}$. The probability of an individual not having the disease is $P(D-) = 1 - p = 1 - 10^{-4} = 0.9999$. The assay's sensitivity is the true positive rate, $P(T+|D+) = 0.99$. The assay's specificity is the true negative rate, $P(T-|D-) = 0.999$.\n\nFrom these definitions, we can derive other key probabilities. The false positive rate (Type I error, $\\alpha$) is $\\alpha = P(T+|D-) = 1 - \\text{Specificity} = 1 - 0.999 = 0.001$. The false negative rate (Type II error, $\\beta$) is $\\beta = P(T-|D+) = 1 - \\text{Sensitivity} = 1 - 0.99 = 0.01$.\n\nWe now calculate the expected number of individuals in each category for a population of $N=10^6$.\n\nThe expected number of individuals with the disease is:\n$$E[D+] = N \\times p = 10^{6} \\times 10^{-4} = 100$$\nThe expected number of individuals without the disease is:\n$$E[D-] = N \\times (1-p) = 10^{6} \\times 0.9999 = 999,900$$\n\nUsing these values, we can find the expected counts for the four possible outcomes of testing:\nExpected number of True Positives ($TP$):\n$$E[TP] = E[D+] \\times P(T+|D+) = 100 \\times 0.99 = 99$$\nExpected number of False Negatives ($FN$):\n$$E[FN] = E[D+] \\times P(T-|D+) = 100 \\times 0.01 = 1$$\nExpected number of False Positives ($FP$):\n$$E[FP] = E[D-] \\times P(T+|D-) = 999,900 \\times 0.001 = 999.9$$\nExpected number of True Negatives ($TN$):\n$$E[TN] = E[D-] \\times P(T-|D-) = 999,900 \\times 0.999 = 998,900.1$$\n\nThe total number of expected positive tests is $E[T+] = E[TP] + E[FP] = 99 + 999.9 = 1098.9$.\nThe Positive Predictive Value ($PPV$) is the probability that an individual with a positive test result actually has the disease, $P(D+|T+)$. It can be calculated from the expected numbers:\n$$PPV = \\frac{E[TP]}{E[TP] + E[FP]} = \\frac{99}{99 + 999.9} = \\frac{99}{1098.9} \\approx 0.090089...$$\nThis is approximately $9.01\\%$.\n\nNow we evaluate each statement.\n\n**A. With these parameters, the expected number of false positives exceeds the expected number of true positives, and the positive predictive value is approximately $9\\%$.**\nThe first part of the statement claims $E[FP]  E[TP]$. Our calculation shows $E[FP] = 999.9$ and $E[TP] = 99$. The inequality $999.9  99$ is true. The second part claims the $PPV$ is approximately $9\\%$. Our calculation shows $PPV \\approx 9.01\\%$, which is indeed approximately $9\\%$. Both parts of the statement are correct.\nVerdict: **Correct**.\n\n**B. With these parameters, the expected number of false positives is less than $10$, and the positive predictive value exceeds $90\\%$.**\nThe first part claims $E[FP]  10$. Our calculation gives $E[FP] = 999.9$. The statement $999.9  10$ is false. The second part claims $PPV  90\\%$. Our calculation gives $PPV \\approx 9.01\\%$. The statement $9.01\\%  90\\%$ is false.\nVerdict: **Incorrect**.\n\n**C. If specificity were increased from $0.999$ to $0.9999$ while holding prevalence and sensitivity fixed, the positive predictive value would increase only slightly, from about $9\\%$ to about $10\\%$.**\nLet us recalculate the necessary values with the new specificity of $0.9999$. The new false positive rate is $\\alpha' = 1 - 0.9999 = 0.0001$. The expected number of true positives, $E[TP]$, remains $99$. The new expected number of false positives is:\n$$E[FP]' = E[D-] \\times \\alpha' = 999,900 \\times 0.0001 = 99.99$$\nThe new positive predictive value would be:\n$$PPV' = \\frac{E[TP]}{E[TP] + E[FP]'} = \\frac{99}{99 + 99.99} = \\frac{99}{198.99} \\approx 0.4975$$\nThe new $PPV$ is approximately $49.8\\%$. The statement claims a slight increase from about $9\\%$ to about $10\\%$. An increase from $9\\%$ to nearly $50\\%$ is a substantial, not slight, increase. The claim is quantitatively incorrect.\nVerdict: **Incorrect**.\n\n**D. If sensitivity were decreased from $0.99$ to $0.90$ while holding prevalence and specificity fixed, the expected number of false positives would decrease by about $10\\%$.**\nThe expected number of false positives is calculated as $E[FP] = N \\times (1-p) \\times (1 - \\text{Specificity})$. This quantity is functionally independent of the sensitivity. Therefore, changing the sensitivity has no effect on the expected number of false positives. The premise of the statement is fundamentally flawed.\nVerdict: **Incorrect**.\n\n**E. For any prevalence, the fraction of positive test results that are false positives equals the Type I error rate.**\nThe fraction of positive test results that are false positives is given by $\\frac{E[FP]}{E[TP]+E[FP]} = 1 - PPV$. In terms of probabilities, this is $P(D-|T+)$. The Type I error rate, $\\alpha$, is the false positive rate, $P(T+|D-)$. The statement thus claims that $P(D-|T+) = P(T+|D-)$ for any prevalence. This is a common logical error confusing two different conditional probabilities. Using Bayes' theorem:\n$$P(D-|T+) = \\frac{P(T+|D-)P(D-)}{P(T+)}$$\nThe equality holds only if $\\frac{P(D-)}{P(T+)} = 1$, which is not generally true. For the given parameters in this problem, we calculated $1 - PPV \\approx 1 - 0.0901 = 0.9099$. The Type I error rate is $\\alpha = 0.001$. Clearly, $0.9099 \\neq 0.001$. The statement is false.\nVerdict: **Incorrect**.\n\nBased on this rigorous analysis, only statement A is correct.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Beyond simply counting errors, a robust analysis must consider the different consequences of each error type. This practice  introduces the critical concept of asymmetric costs, where a false positive and a false negative carry vastly different penalties. You will learn to apply a decision-theoretic framework to determine which diagnostic threshold minimizes total expected harm, a crucial skill for optimizing strategies in clinical medicine and public health.",
            "id": "2438745",
            "problem": "A national newborn screening program uses tandem mass spectrometry to flag a rare but treatable inborn error of metabolism. The disease prevalence is $p = 1/5000$. A calibrated risk score is thresholded to produce a binary screen result. Consider two operating points for the same classifier:\n\n- Threshold $T_1$ (more sensitive): sensitivity $= 0.99$, specificity $= 0.95$.\n- Threshold $T_2$ (more specific): sensitivity $= 0.90$, specificity $= 0.995$.\n\nAssume decisions are evaluated by expected harm per $N = 100{,}000$ screened newborns using the following loss model: each false positive (Type I error) incurs $c_{\\mathrm{FP}} = 1$ harm unit due to parental anxiety and follow-up procedures; each false negative (Type II error) incurs $c_{\\mathrm{FN}} = 5000$ harm units due to missed early treatment and subsequent morbidity. All true positives and true negatives are assigned zero harm.\n\nWhich statement is correct under this model?\n\nA. Under $T_1$, expected harm from false positives exceeds that from false negatives; therefore $T_2$ minimizes expected harm.\n\nB. Under $T_1$, expected harm from false negatives dominates; thus $T_2$ should be used to reduce missed cases.\n\nC. Expected harm is lower under $T_1$ despite many false positives, because each missed case is very costly; for decision-making in this model, a Type II error (missing a treatable disorder) is worse than a Type I error.\n\nD. Both thresholds yield the same expected harm, so the choice between them is indifferent under this model.",
            "solution": "The objective is to select the threshold, $T_1$ or $T_2$, that minimizes the total expected harm for a cohort of $N = 100,000$ newborns.\n\nFirst, we define the key metrics. Let $D$ denote the state of having the disease and $H$ denote the healthy state. Let $+$ and $-$ denote a positive and negative test result, respectively.\n- Prevalence: $\\Pr(D) = p = 1/5000 = 0.0002$.\n- $\\Pr(H) = 1 - p = 1 - 0.0002 = 0.9998$.\n- Sensitivity (True Positive Rate, TPR): $\\text{TPR} = \\Pr(+|D)$.\n- Specificity (True Negative Rate, TNR): $\\text{TNR} = \\Pr(-|H)$.\n- False Positive Rate (Type I Error Rate): $\\text{FPR} = \\Pr(+|H) = 1 - \\text{TNR}$.\n- False Negative Rate (Type II Error Rate): $\\text{FNR} = \\Pr(-|D) = 1 - \\text{TPR}$.\n\nThe total number of diseased individuals in the cohort is $N_D = N \\times p = 100,000 \\times (1/5000) = 20$.\nThe total number of healthy individuals is $N_H = N \\times (1-p) = 100,000 \\times 0.9998 = 99,980$.\n\nThe total expected harm, $E[H]$, is the sum of harm from false positives and false negatives, as harm from true positives and true negatives is zero.\n$E[H] = (\\text{Number of False Positives} \\times c_{\\mathrm{FP}}) + (\\text{Number of False Negatives} \\times c_{\\mathrm{FN}})$\n$E[H] = (N_H \\times \\text{FPR} \\times c_{\\mathrm{FP}}) + (N_D \\times \\text{FNR} \\times c_{\\mathrm{FN}})$\n\nWe now calculate the total expected harm for each threshold.\n\n**Analysis for Threshold $T_1$:**\n- Sensitivity$_1 = 0.99$, so $\\text{FNR}_1 = 1 - 0.99 = 0.01$.\n- Specificity$_1 = 0.95$, so $\\text{FPR}_1 = 1 - 0.95 = 0.05$.\n\n- Expected number of false positives: $N_{\\mathrm{FP,1}} = N_H \\times \\text{FPR}_1 = 99,980 \\times 0.05 = 4999$.\n- Expected harm from false positives: $H_{\\mathrm{FP,1}} = N_{\\mathrm{FP,1}} \\times c_{\\mathrm{FP}} = 4999 \\times 1 = 4999$.\n\n- Expected number of false negatives: $N_{\\mathrm{FN,1}} = N_D \\times \\text{FNR}_1 = 20 \\times 0.01 = 0.2$.\n- Expected harm from false negatives: $H_{\\mathrm{FN,1}} = N_{\\mathrm{FN,1}} \\times c_{\\mathrm{FN}} = 0.2 \\times 5000 = 1000$.\n\n- Total expected harm for $T_1$: $E[H_1] = H_{\\mathrm{FP,1}} + H_{\\mathrm{FN,1}} = 4999 + 1000 = 5999$.\n\n**Analysis for Threshold $T_2$:**\n- Sensitivity$_2 = 0.90$, so $\\text{FNR}_2 = 1 - 0.90 = 0.10$.\n- Specificity$_2 = 0.995$, so $\\text{FPR}_2 = 1 - 0.995 = 0.005$.\n\n- Expected number of false positives: $N_{\\mathrm{FP,2}} = N_H \\times \\text{FPR}_2 = 99,980 \\times 0.005 = 499.9$.\n- Expected harm from false positives: $H_{\\mathrm{FP,2}} = N_{\\mathrm{FP,2}} \\times c_{\\mathrm{FP}} = 499.9 \\times 1 = 499.9$.\n\n- Expected number of false negatives: $N_{\\mathrm{FN,2}} = N_D \\times \\text{FNR}_2 = 20 \\times 0.10 = 2$.\n- Expected harm from false negatives: $H_{\\mathrm{FN,2}} = N_{\\mathrm{FN,2}} \\times c_{\\mathrm{FN}} = 2 \\times 5000 = 10,000$.\n\n- Total expected harm for $T_2$: $E[H_2] = H_{\\mathrm{FP,2}} + H_{\\mathrm{FN,2}} = 499.9 + 10,000 = 10,499.9$.\n\n**Comparison:**\n- Total harm under $T_1$: $E[H_1] = 5999$.\n- Total harm under $T_2$: $E[H_2] = 10,499.9$.\n\nSince $E[H_1]  E[H_2]$, the threshold $T_1$ minimizes the total expected harm.\n\n### Option-by-Option Analysis\n\n**A. Under $T_1$, expected harm from false positives exceeds that from false negatives; therefore $T_2$ minimizes expected harm.**\nThe first clause is correct: under $T_1$, harm from false positives ($4999$) is greater than harm from false negatives ($1000$). However, the conclusion \"therefore $T_2$ minimizes expected harm\" is incorrect. Our calculation shows that $T_1$ is the superior choice ($5999  10,499.9$). The reasoning presented is a non sequitur; one cannot conclude which threshold is optimal by only comparing the components of harm for one of the thresholds.\n**Verdict: Incorrect.**\n\n**B. Under $T_1$, expected harm from false negatives dominates; thus $T_2$ should be used to reduce missed cases.**\nThe premise \"Under $T_1$, expected harm from false negatives dominates\" is factually incorrect. We calculated harm from false positives as $4999$ and harm from false negatives as $1000$. The harm from false positives is dominant under $T_1$.\n**Verdict: Incorrect.**\n\n**C. Expected harm is lower under $T_1$ despite many false positives, because each missed case is very costly; for decision-making in this model, a Type II error (missing a treatable disorder) is worse than a Type I error.**\nThis statement is composed of several correct claims.\n- \"Expected harm is lower under $T_1$\": This is correct, as $E[H_1] = 5999$ is lower than $E[H_2] = 10,499.9$.\n- \"despite many false positives\": This is also correct. $T_1$ generates $4999$ expected false positives, a much larger number than the $499.9$ from $T_2$.\n- \"because each missed case is very costly\": This provides the correct causal reason. The cost of a false negative ($c_{\\mathrm{FN}} = 5000$) is vastly higher than the cost of a false positive ($c_{\\mathrm{FP}} = 1$). This high cost amplifies the harm of even a small number of missed cases, making the higher sensitivity of $T_1$ (which reduces false negatives) more important than its lower specificity (which increases false positives).\n- \"for decision-making in this model, a Type II error (missing a treatable disorder) is worse than a Type I error\": This is a direct consequence of the loss model where $c_{\\mathrm{FN}} = 5000  c_{\\mathrm{FP}} = 1$. It is a fundamental premise driving the result.\nAll parts of this statement are logically and numerically sound.\n**Verdict: Correct.**\n\n**D. Both thresholds yield the same expected harm, so the choice between them is indifferent under this model.**\nThis statement is factually incorrect. Our calculations show $E[H_1] = 5999$ and $E[H_2] = 10,499.9$, which are not equal.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "The previous practices focused on analyzing errors from established tests; this exercise shifts our focus to proactive experimental design. Controlling the risk of a Type II error (a false negative) is a primary goal when planning research. By performing a power calculation for an RNA-seq experiment , you will learn the fundamental skill of determining the sample size needed to confidently detect a specific biological effect, ensuring your study is neither wastefully large nor insufficiently powered.",
            "id": "2438773",
            "problem": "A laboratory plans a differential expression analysis between two conditions using Ribonucleic Acid sequencing (RNA-seq). After normalization, the analysis will be performed on the base-$2$ logarithm of counts per gene. For a particular gene of interest, pilot data suggest that within each condition the log$_{2}$ expression values are approximately independent and normally distributed with a common variance of $\\sigma^{2} = 0.09$. The study aims to detect a subtle $1.2$-fold change in mean expression between the two conditions. On the log$_{2}$ scale, let the mean difference be $\\delta = \\log_{2}(1.2)$. The hypothesis test will be two-sided with a Type I error rate $\\alpha = 0.05$, and the design must achieve a power of $1-\\beta = 0.90$ for this effect size. Assume equal sample sizes $n$ per condition and that the variance estimate is accurate for planning purposes.\n\nCompute the minimal integer value of $n$ (samples per condition) required to achieve the specified power under these assumptions. Report the smallest integer $n$ that satisfies the requirement. No rounding by significant figures is needed because $n$ must be an integer.",
            "solution": "The problem is to determine the sample size $n$ for a two-sample test comparing the means of two normal distributions with a known common variance. Let $\\mu_1$ and $\\mu_2$ be the true mean log-expression values for the two conditions. The hypotheses are:\n- Null hypothesis $H_0: \\mu_1 = \\mu_2$.\n- Alternative hypothesis $H_1: \\mu_1 \\neq \\mu_2$.\n\nThe problem specifies that the difference to be detected is $\\delta = \\mu_1 - \\mu_2 = \\log_2(1.2)$. We perform a two-sided test. Let $\\bar{X}_1$ and $\\bar{X}_2$ be the sample means from the two conditions, each with $n$ samples. Since the population variance $\\sigma^2$ is assumed to be known, the appropriate test statistic under $H_0$ is the Z-statistic:\n$$ Z = \\frac{(\\bar{X}_1 - \\bar{X}_2) - 0}{\\sqrt{\\frac{\\sigma^2}{n} + \\frac{\\sigma^2}{n}}} = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sigma \\sqrt{\\frac{2}{n}}} $$\nUnder $H_0$, $Z$ follows a standard normal distribution, $N(0,1)$. For a two-sided test with a Type I error rate of $\\alpha$, we reject $H_0$ if $|Z|  z_{1-\\alpha/2}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution.\n\nPower, $1-\\beta$, is the probability of correctly rejecting $H_0$ when the alternative hypothesis $H_1$ is true, i.e., when the true mean difference is $\\delta$. Under $H_1$, the statistic $(\\bar{X}_1 - \\bar{X}_2)$ is normally distributed with mean $\\delta$ and variance $\\frac{2\\sigma^2}{n}$. Therefore, the standardized test statistic $Z$ is not standard normal. Instead, under $H_1$:\n$$ E[Z] = \\frac{E[\\bar{X}_1 - \\bar{X}_2]}{\\sigma \\sqrt{\\frac{2}{n}}} = \\frac{\\delta}{\\sigma \\sqrt{\\frac{2}{n}}} $$\nThe power is the probability that $Z$ falls into the rejection region:\n$$ 1-\\beta = P\\left(Z  z_{1-\\alpha/2} \\mid H_1\\right) + P\\left(Z  -z_{1-\\alpha/2} \\mid H_1\\right) $$\nTo evaluate this, we standardize $Z$ under $H_1$:\n$$ \\frac{Z - \\frac{\\delta}{\\sigma \\sqrt{2/n}}}{1} \\sim N(0,1) $$\nAssuming $\\delta  0$ (which is true since $\\log_2(1.2)  0$), the power is dominated by the first term. The second term, $P(Z  -z_{1-\\alpha/2})$, is negligible. Thus, we approximate the power as:\n$$ 1-\\beta \\approx P\\left(Z  z_{1-\\alpha/2} \\mid H_1\\right) $$\nLet $Z' \\sim N(0,1)$.\n$$ 1-\\beta = P\\left( Z' + \\frac{\\delta}{\\sigma\\sqrt{2/n}}  z_{1-\\alpha/2} \\right) = P\\left( Z'  z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}} \\right) $$\nFor a standard normal variable $Z'$, $P(Z'k) = 1-\\Phi(k)$, where $\\Phi$ is the cumulative distribution function. Also, $1-\\Phi(k) = \\Phi(-k)$. So, we require the probability to be $1-\\beta$, which means the argument of $\\Phi$ must be $z_{1-\\beta}$.\n$$ - \\left( z_{1-\\alpha/2} - \\frac{\\delta}{\\sigma\\sqrt{2/n}} \\right) \\ge z_{1-\\beta} $$\nThis simplifies to the standard power equation for a two-sided Z-test:\n$$ \\frac{|\\delta|}{\\sigma\\sqrt{2/n}} \\ge z_{1-\\alpha/2} + z_{1-\\beta} $$\nWe solve for $n$:\n$$ \\sqrt{n} \\ge \\frac{\\sqrt{2}\\sigma(z_{1-\\alpha/2} + z_{1-\\beta})}{|\\delta|} $$\n$$ n \\ge \\frac{2\\sigma^2(z_{1-\\alpha/2} + z_{1-\\beta})^2}{\\delta^2} $$\nNow we substitute the values provided in the problem.\n- $\\sigma^2 = 0.09$.\n- $\\delta = \\log_2(1.2) = \\frac{\\ln(1.2)}{\\ln(2)}$.\n- $\\alpha = 0.05 \\implies \\alpha/2 = 0.025$. The critical value is $z_{1-0.025} = z_{0.975} \\approx 1.96$.\n- $1-\\beta = 0.90 \\implies \\beta = 0.10$. The corresponding one-sided quantile is $z_{1-0.10} = z_{0.90} \\approx 1.2816$.\n\nThe sum of the quantiles is $z_{1-\\alpha/2} + z_{1-\\beta} \\approx 1.96 + 1.2816 = 3.2416$.\nThe effect size on the log scale is $\\delta = \\log_2(1.2) \\approx 0.26303$.\n\nSubstituting these into the inequality for $n$:\n$$ n \\ge \\frac{2 \\times 0.09 \\times (1.96 + 1.2816)^2}{(\\log_2(1.2))^2} $$\n$$ n \\ge \\frac{0.18 \\times (3.2416)^2}{(0.26303)^2} $$\n$$ n \\ge \\frac{0.18 \\times 10.5080}{(0.069185)} $$\n$$ n \\ge \\frac{1.89144}{0.069185} $$\n$$ n \\ge 27.339... $$\nSince the number of samples $n$ must be an integer, we must take the smallest integer that satisfies this condition. This is the ceiling of the calculated value.\n$$ n = \\lceil 27.339... \\rceil = 28 $$\nTherefore, a minimum of $28$ samples per condition is required to achieve the desired power of $0.90$.",
            "answer": "$$\\boxed{28}$$"
        }
    ]
}