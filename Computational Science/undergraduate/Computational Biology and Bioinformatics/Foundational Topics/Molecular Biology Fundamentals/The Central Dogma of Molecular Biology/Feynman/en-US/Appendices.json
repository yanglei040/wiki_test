{
    "hands_on_practices": [
        {
            "introduction": "This first exercise is a fundamental practice in molecular biology: translating a gene's DNA sequence into its corresponding protein. Starting with a hypothetical DNA template strand, you will walk through the two key steps of the central dogma—transcription into messenger RNA ($mRNA$) and its subsequent translation into a polypeptide chain. This core skill is foundational for understanding how genetic information is expressed and for analyzing gene function. ",
            "id": "1471646",
            "problem": "In a computational model simulating prokaryotic gene expression, a researcher is analyzing a short, hypothetical gene. The template strand of the Deoxyribonucleic Acid (DNA) for this gene is found to have the following sequence, read from the 3' end to the 5' end:\n\n`3'-TAC GGA GGT CAT TTC AGT ATT-5'`\n\nAssuming transcription starts at the first nucleotide and translation follows the standard genetic code, determine the correct amino acid sequence of the polypeptide synthesized from this gene. The process terminates when a stop codon is encountered in the messenger Ribonucleic Acid (mRNA) sequence.\n\nUse the provided standard codon table below, where the codons are written in the 5' to 3' direction.\n\n**Codon Table:**\n*   **Alanine (Ala):** GCU, GCC, GCA, GCG\n*   **Arginine (Arg):** CGU, CGC, CGA, CGG, AGA, AGG\n*   **Asparagine (Asn):** AAU, AAC\n*   **Aspartic Acid (Asp):** GAU, GAC\n*   **Cysteine (Cys):** UGU, UGC\n*   **Glutamic Acid (Glu):** GAA, GAG\n*   **Glutamine (Gln):** CAA, CAG\n*   **Glycine (Gly):** GGU, GGC, GGA, GGG\n*   **Histidine (His):** CAU, CAC\n*   **Isoleucine (Ile):** AUU, AUC, AUA\n*   **Leucine (Leu):** UUA, UUG, CUU, CUC, CUA, CUG\n*   **Lysine (Lys):** AAA, AAG\n*   **Methionine (Met):** AUG\n*   **Phenylalanine (Phe):** UUU, UUC\n*   **Proline (Pro):** CCU, CCC, CCA, CCG\n*   **Serine (Ser):** UCU, UCC, UCA, UCG, AGU, AGC\n*   **Threonine (Thr):** ACU, ACC, ACA, ACG\n*   **Tryptophan (Trp):** UGG\n*   **Tyrosine (Tyr):** UAU, UAC\n*   **Valine (Val):** GUU, GUC, GUA, GUG\n*   **Stop Codons:** UAA, UAG, UGA\n\nWhich of the following represents the correct amino acid sequence? The three-letter abbreviations are used for the amino acids.\n\nA. Met-Pro-Pro-Val-Lys-Ser\n\nB. Met-Gly-Pro-Val-Lys-Ser\n\nC. Cys-Leu-Gln\n\nD. Tyr-Gly-Gly-His-Phe-Ser-Ile\n\nE. Asn-Ser-Lys-Val-Pro-Pro-Met",
            "solution": "Transcription uses the DNA template strand, which is read by RNA polymerase in the 3' to 5' direction to synthesize mRNA in the 5' to 3' direction. The mRNA is complementary to the template, with base-pairing rules $T \\leftrightarrow A$, $C \\leftrightarrow G$, and in RNA $A$ pairs with $U$ instead of $T$.\n\nGiven the template strand (3' to 5'):\n3'-TAC GGA GGT CAT TTC AGT ATT-5'\n\nForm the mRNA (5' to 3') by complementarity, starting at the first nucleotide:\n- Template TAC gives mRNA AUG\n- Template GGA gives mRNA CCU\n- Template GGT gives mRNA CCA\n- Template CAT gives mRNA GUA\n- Template TTC gives mRNA AAG\n- Template AGT gives mRNA UCA\n- Template ATT gives mRNA UAA\n\nThus the mRNA is:\n5'-AUG CCU CCA GUA AAG UCA UAA-3'\n\nTranslation proceeds from the 5' end at the start codon AUG (Met) and continues codon by codon until a stop codon is reached:\n- AUG → Methionine (Met)\n- CCU → Proline (Pro)\n- CCA → Proline (Pro)\n- GUA → Valine (Val)\n- AAG → Lysine (Lys)\n- UCA → Serine (Ser)\n- UAA → Stop (terminates translation)\n\nTherefore, the polypeptide sequence is:\nMet-Pro-Pro-Val-Lys-Ser\n\nThis corresponds to option A.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The genetic code is read in discrete, non-overlapping blocks of three nucleotides, establishing a \"reading frame\" that is crucial for protein synthesis. This thought experiment explores the profound consequences of a frameshift mutation, where the insertion of nucleotides disrupts this frame and scrambles the genetic message downstream. Analyzing the impact of such mutations is essential for bioinformatics and for understanding the molecular basis of many genetic disorders. ",
            "id": "2141980",
            "problem": "Consider a gene within a eukaryotic organism that codes for a specific enzyme. The coding sequence of this gene, after being transcribed into messenger Ribonucleic Acid (mRNA), is read by ribosomes in sequential, non-overlapping units of three nucleotides called codons. This \"reading frame\" begins at a specific start codon and determines the amino acid sequence of the resulting protein.\n\nImagine a mutational event where two adjacent nucleotide bases are inserted into the Deoxyribonucleic Acid (DNA) sequence that corresponds to a position very early in the gene's coding region, just after the start codon. Assuming this mutation does not prevent the gene from being transcribed into mRNA, what is the most probable effect on the amino acid sequence of the protein synthesized from this mutated gene?\n\nA. A single amino acid in the protein sequence will be incorrect, but the rest of the protein sequence will be normal.\n\nB. The resulting protein will be identical to the original protein, but two amino acids longer.\n\nC. The amino acid sequence will be completely altered from the point of insertion onwards, and the protein is likely to be terminated prematurely.\n\nD. The mutation will be silent, causing no change to the final amino acid sequence of the protein.\n\nE. The protein will have two extra amino acids inserted at the site of the mutation, but the reading frame will be restored for all subsequent codons.",
            "solution": "The genetic code is read in non-overlapping triplets (codons) beginning at a start codon, which establishes a reading frame. Each codon specifies one amino acid, and translation proceeds codon by codon.\n\nLet the original coding sequence be read in triplets as:\n$$[b_{1}b_{2}b_{3}]\\,[b_{4}b_{5}b_{6}]\\,[b_{7}b_{8}b_{9}]\\,\\dots$$\nwhere each bracketed group is a codon. An insertion of two adjacent bases immediately after the start codon increases the nucleotide count by two at that position. The reading frame is defined modulo three, so the effect of an insertion of length $k$ on the reading frame is determined by $k \\bmod 3$. If $k \\equiv 0 \\pmod{3}$, the frame is preserved; if $k \\not\\equiv 0 \\pmod{3}$, all downstream codons are regrouped, causing a frameshift.\n\nHere, $k=2$, so $2 \\not\\equiv 0 \\pmod{3}$, and the reading frame shifts by two nucleotides from the point of insertion onward. Consequently, the identity of every downstream codon changes, altering the encoded amino acids starting at the insertion site. Because the new reading frame is effectively random with respect to the original, it has a high probability of encountering a stop codon (UAA, UAG, or UGA) relatively soon. This is especially likely since the insertion occurs early in the coding region, making premature termination likely and producing a truncated, nonfunctional protein.\n\nEvaluation of options based on these principles:\n- A is incorrect because a frameshift from a two-base insertion does not typically affect only a single amino acid; it alters all downstream codons.\n- B is incorrect because adding two nucleotides does not preserve the frame or simply add two amino acids; restoration of the original sequence length without frameshift would require insertion of a multiple of three nucleotides.\n- C is correct: a two-base insertion causes a frameshift, altering the amino acid sequence from the insertion onward and likely introducing a premature stop codon.\n- D is incorrect because a silent outcome is highly unlikely for a two-base insertion; silent changes typically involve single-nucleotide substitutions that do not alter the encoded amino acid.\n- E is incorrect because inserting two nucleotides does not restore the frame after two extra amino acids; restoration requires an insertion or deletion whose length is a multiple of three.\n\nTherefore, the most probable effect is a frameshift that alters the sequence from the point of insertion with likely premature termination.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Moving from manual analysis to computational modeling, this final exercise applies our understanding of gene structure to a classic bioinformatics task. We can leverage the distinct statistical properties of exons and introns—a direct consequence of their roles in the central dogma—to automatically identify them within a genome. This advanced practice guides you in formalizing a Hidden Markov Model ($HMM$), a powerful probabilistic tool, to perform this exact task of computational gene annotation. ",
            "id": "2434915",
            "problem": "You are asked to formalize and implement a probabilistic sequence model to classify genomic positions into exon, intron, or intergenic states using a Hidden Markov Model (HMM) informed by $k$-mer frequencies. The biological foundation is the central dogma of molecular biology, where genomic DNA is transcribed and processed, creating characteristic composition patterns in exons, introns, and intergenic regions. You must design the model and algorithm from first principles, adhering to probabilistic definitions and using only the following foundational bases: the Markov property for latent states, independence of emissions conditioned on the current state, and the observation that exons, introns, and intergenic regions have different $k$-mer composition biases.\n\nDefine a Hidden Markov Model with latent states $\\mathcal{S} = \\{\\mathrm{E}, \\mathrm{I}, \\mathrm{G}\\}$ corresponding to exon, intron, and intergenic. The observable alphabet is the set $\\mathcal{K}$ of all DNA $k$-mers over $\\{\\mathrm{A}, \\mathrm{C}, \\mathrm{G}, \\mathrm{T}\\}$, so $\\lvert \\mathcal{K} \\rvert = 4^{k}$. Let the initial distribution be $\\boldsymbol{\\pi} = (\\pi_{s})_{s \\in \\mathcal{S}}$, the state transition matrix be $\\mathbf{A} = (a_{s \\to s^{\\prime}})_{s,s^{\\prime} \\in \\mathcal{S}}$, and the emission matrix be $\\mathbf{B} = (b_{s}(o))_{s \\in \\mathcal{S}, o \\in \\mathcal{K}}$, where $b_{s}(o) = \\mathbb{P}(O_{t} = o \\mid S_{t} = s)$. Assume the standard HMM generative assumptions: $S_{t}$ is first-order Markov with $\\mathbb{P}(S_{t} \\mid S_{1:t-1}) = \\mathbb{P}(S_{t} \\mid S_{t-1})$, and $O_{t}$ is conditionally independent of other variables given $S_{t}$.\n\nTraining data are provided as follows.\n- Emission training sequences: for each state $s \\in \\{\\mathrm{E},\\mathrm{I},\\mathrm{G}\\}$, you are given labeled DNA sequences that are known to be generated from $s$. For a chosen $k$, extract overlapping $k$-mers from each sequence and accumulate counts $c_{s}(o)$ for $o \\in \\mathcal{K}$. Use Laplace smoothing with hyperparameter $\\alpha > 0$ to construct emission probabilities:\n$$\nb_{s}(o) = \\frac{c_{s}(o) + \\alpha}{\\sum_{o^{\\prime} \\in \\mathcal{K}} c_{s}(o^{\\prime}) + \\alpha \\lvert \\mathcal{K} \\rvert}.\n$$\n- Transition training state paths: you are given labeled state paths over $\\{\\mathrm{E},\\mathrm{I},\\mathrm{G}\\}$ that represent example traversals through genomic regions. Let $n_{s \\to s^{\\prime}}$ denote the count of transitions from $s$ to $s^{\\prime}$ across all provided paths. Estimate transition probabilities with Laplace smoothing parameter $\\beta > 0$:\n$$\na_{s \\to s^{\\prime}} = \\frac{n_{s \\to s^{\\prime}} + \\beta}{\\sum_{u \\in \\mathcal{S}} n_{s \\to u} + \\beta \\lvert \\mathcal{S} \\rvert}.\n$$\nAlso estimate the initial distribution with Laplace smoothing parameter $\\gamma > 0$ from the starting states of the training paths. Let $m_{s}$ be the number of paths that start in $s$, and let $M$ be the number of paths. Then\n$$\n\\pi_{s} = \\frac{m_{s} + \\gamma}{M + \\gamma \\lvert \\mathcal{S} \\rvert}.\n$$\nFor each test case below, take $\\beta = \\gamma = \\alpha$.\n\nInference requirement:\n- Given a test DNA sequence and a fixed $k$, transform the sequence into a length-$T$ observation sequence $O_{1:T}$ by taking overlapping $k$-mers starting at each position, so $T = L - k + 1$ for a DNA string of length $L$.\n- Decode the most likely state path $\\hat{S}_{1:T}$ using the Viterbi algorithm that maximizes\n$$\n\\hat{S}_{1:T} = \\arg\\max_{s_{1:T} \\in \\mathcal{S}^{T}} \\left[ \\log \\pi_{s_{1}} + \\sum_{t=2}^{T} \\log a_{s_{t-1} \\to s_{t}} + \\sum_{t=1}^{T} \\log b_{s_{t}}(O_{t}) \\right].\n$$\n\nLabeling convention for test evaluation:\n- Each test sequence is provided as a concatenation of labeled segments $(\\text{state}, \\text{DNA segment})$. For a given $k$, the ground-truth label for observation index $t \\in \\{1,\\dots,T\\}$ is taken to be the state of the segment in which the $k$-mer starting at position $t$ begins. That is, if the starting nucleotide index (zero-based) of the $k$-mer lies in the range of a segment labeled $s$, then the ground-truth state at index $t$ is $s$, regardless of whether the $k$-mer crosses a segment boundary.\n- Compute the accuracy for a test case as the fraction in $[0,1]$ of indices $t \\in \\{1,\\dots,T\\}$ where $\\hat{S}_{t}$ equals the ground-truth state.\n\nYour program must implement the above model estimation and decoding and then evaluate the accuracy for multiple test cases using the following fixed training data and test suite. All sequences contain only $\\{\\mathrm{A},\\mathrm{C},\\mathrm{G},\\mathrm{T}\\}$.\n\nCommon training data (used by every test case):\n- Emission training sequences by state:\n  - Exon ($\\mathrm{E}$): \n    - \"GCGCGCGCGCGCGCGCATGCGCGC\"\n    - \"CGCGCCGCGCGCGGCGCGC\"\n  - Intron ($\\mathrm{I}$):\n    - \"ATATATATATATATATATATTTATA\"\n    - \"AATAATATATAAATATATA\"\n  - Intergenic ($\\mathrm{G}$):\n    - \"AGCTAGCTAGCTAGCTAGCTAGCT\"\n    - \"GATCGATCGATCGATC\"\n- Transition training state paths:\n  - \"GGGGGIIIIIEEEEEEEGGGGG\"\n  - \"GGGIIIIIGGGEEE\"\n  - \"EEEEEEIIIIGGGG\"\n\nTest suite:\n- Test case $1$ (happy path): $k = 2$, $\\alpha = 0.5$. Test sequence is the concatenation of:\n  - $(\\mathrm{G}, \\text{\"AGCTAGCTAGC\"})$\n  - $(\\mathrm{I}, \\text{\"ATATATATATAT\"})$\n  - $(\\mathrm{E}, \\text{\"GCGCGCGCGC\"})$\n- Test case $2$ (boundary length): $k = 2$, $\\alpha = 0.5$. Test sequence is the single segment:\n  - $(\\mathrm{I}, \\text{\"AT\"})$\n- Test case $3$ (ambiguous composition, stronger smoothing): $k = 2$, $\\alpha = 1.0$. Test sequence is the concatenation of:\n  - $(\\mathrm{G}, \\text{\"AGCTAGC\"})$\n  - $(\\mathrm{E}, \\text{\"GCGCGC\"})$\n  - $(\\mathrm{I}, \\text{\"ATATAT\"})$\n- Test case $4$ (larger alphabet, $k$-mers of length $3$): $k = 3$, $\\alpha = 0.5$. Test sequence is the concatenation of:\n  - $(\\mathrm{G}, \\text{\"AGCTAGCT\"})$\n  - $(\\mathrm{I}, \\text{\"ATATATAT\"})$\n  - $(\\mathrm{E}, \\text{\"GCGCGCGC\"})$\n\nComputational and numerical requirements:\n- Use only floating-point arithmetic in base-$e$ logarithms when implementing the Viterbi algorithm to maintain numerical stability. No angle units are involved.\n- Round each accuracy to exactly $4$ decimal places. Express each accuracy as a decimal in $[0,1]$ without a percent sign.\n- The final program output must be a single line containing a Python-style list of the accuracy values for the test suite, in order from test case $1$ to test case $4$, formatted as a comma-separated list enclosed in square brackets. For example, the output must look like \"[$x_1,x_2,x_3,x_4$]\" where each $x_i$ has exactly $4$ digits after the decimal point.",
            "solution": "The problem statement poses a valid and well-defined task in computational biology. It requires the construction and application of a first-order Hidden Markov Model (HMM) to classify genomic regions. All necessary data and definitions for model training and inference are provided, and the problem is scientifically grounded in the established use of HMMs for sequence annotation. The premises are internally consistent and formalizable. We proceed with the derivation and implementation.\n\nThe HMM is defined by the tuple $(\\mathcal{S}, \\mathcal{K}, \\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B})$, where:\n- $\\mathcal{S} = \\{\\mathrm{E}, \\mathrm{I}, \\mathrm{G}\\}$ is the set of $3$ latent states (Exon, Intron, Intergenic).\n- $\\mathcal{K}$ is the observation alphabet of all possible DNA $k$-mers, with size $\\lvert \\mathcal{K} \\rvert = 4^{k}$.\n- $\\boldsymbol{\\pi}$ is the initial state probability vector.\n- $\\mathbf{A}$ is the state transition probability matrix.\n- $\\mathbf{B}$ is the emission probability matrix.\n\nThe solution is structured into three parts: model parameter estimation, sequence decoding via the Viterbi algorithm, and performance evaluation.\n\n**1. Model Parameter Estimation**\n\nThe HMM parameters $(\\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B})$ are estimated from the provided training data using Maximum Likelihood Estimation with Laplace smoothing to prevent zero probabilities. All probabilities are handled in the logarithmic domain to ensure numerical stability.\n\n**Initial State Probabilities ($\\boldsymbol{\\pi}$)**:\nThe initial probability $\\pi_s$ for each state $s \\in \\mathcal{S}$ is estimated from the $M$ provided state paths. Let $m_s$ be the count of paths starting with state $s$, and $\\gamma$ be the smoothing hyperparameter. The smoothed probability is:\n$$\n\\pi_s = \\frac{m_s + \\gamma}{M + \\gamma \\lvert\\mathcal{S}\\rvert}\n$$\nThe corresponding log-probability is $\\log \\pi_s = \\log(m_s + \\gamma) - \\log(M + \\gamma \\lvert\\mathcal{S}\\rvert)$.\n\n**State Transition Probabilities ($\\mathbf{A}$)**:\nThe transition probability $a_{s \\to s'}$ from state $s$ to $s'$ is estimated by counting such transitions, $n_{s \\to s'}$, in the training paths. With smoothing parameter $\\beta$, the probability is:\n$$\na_{s \\to s'} = \\frac{n_{s \\to s'} + \\beta}{\\sum_{u \\in \\mathcal{S}} n_{s \\to u} + \\beta \\lvert\\mathcal{S}\\rvert}\n$$\nThe log-probability is $\\log a_{s \\to s'} = \\log(n_{s \\to s'} + \\beta) - \\log(\\sum_{u \\in \\mathcal{S}} n_{s \\to u} + \\beta \\lvert\\mathcal{S}\\rvert)$.\n\n**Emission Probabilities ($\\mathbf{B}$)**:\nThe emission probability $b_s(o)$ of observing $k$-mer $o \\in \\mathcal{K}$ from state $s \\in \\mathcal{S}$ is estimated from labeled training sequences. First, for each state $s$, we count the occurrences $c_s(o)$ of each $k$-mer $o$ by sliding a window of length $k$ over the corresponding training sequences. With smoothing parameter $\\alpha$, the probability is:\n$$\nb_s(o) = \\frac{c_s(o) + \\alpha}{\\sum_{o' \\in \\mathcal{K}} c_s(o') + \\alpha \\lvert\\mathcal{K}\\rvert}\n$$\nThe log-probability is $\\log b_s(o) = \\log(c_s(o) + \\alpha) - \\log(\\sum_{o' \\in \\mathcal{K}} c_s(o') + \\alpha \\lvert\\mathcal{K}\\rvert)$.\n\nFor all test cases, the problem specifies $\\alpha = \\beta = \\gamma$.\n\n**2. State Path Decoding via Viterbi Algorithm**\n\nGiven a test DNA sequence of length $L$, it is converted into an observation sequence $O_{1:T} = (O_1, O_2, \\dots, O_T)$ of $T = L - k + 1$ overlapping $k$-mers. The Viterbi algorithm is a dynamic programming approach to find the most likely sequence of states $\\hat{S}_{1:T}$ that generated $O_{1:T}$. It works by maximizing the joint log-probability:\n$$\n\\log \\mathbb{P}(O_{1:T}, S_{1:T}) = \\log \\pi_{S_1} + \\sum_{t=2}^{T} \\log a_{S_{t-1} \\to S_t} + \\sum_{t=1}^{T} \\log b_{S_t}(O_t)\n$$\nWe define two tables: $\\delta_t(s)$, which stores the maximum log-probability of any path ending in state $s$ at time $t$, and $\\psi_t(s)$, which stores the previous state that leads to this maximum probability.\n\n- **Initialization ($t=1$)**: For each state $s \\in \\mathcal{S}$:\n$$\n\\delta_1(s) = \\log \\pi_s + \\log b_s(O_1)\n$$\n$$\n\\psi_1(s) = 0 \\quad (\\text{or any convention for the start})\n$$\n\n- **Recursion ($t=2, \\dots, T$)**: For each state $s' \\in \\mathcal{S}$:\n$$\n\\delta_t(s') = \\left( \\max_{s \\in \\mathcal{S}} \\left\\{ \\delta_{t-1}(s) + \\log a_{s \\to s'} \\right\\} \\right) + \\log b_{s'}(O_t)\n$$\n$$\n\\psi_t(s') = \\arg\\max_{s \\in \\mathcal{S}} \\left\\{ \\delta_{t-1}(s) + \\log a_{s \\to s'} \\right\\}\n$$\n\n- **Termination**: The probability of the most likely path is $\\max_{s \\in \\mathcal{S}} \\delta_T(s)$. The final state of the path is:\n$$\n\\hat{S}_T = \\arg\\max_{s \\in \\mathcal{S}} \\delta_T(s)\n$$\n\n- **Path Backtracking**: The most likely path is reconstructed by starting from $\\hat{S}_T$ and tracing back using the $\\psi$ table:\n$$\n\\hat{S}_{t-1} = \\psi_t(\\hat{S}_t) \\quad \\text{for } t = T, T-1, \\dots, 2\n$$\n\n**3. Evaluation**\n\nThe performance of the decoder is quantified by its accuracy. For each test case, a ground-truth state path is constructed. The test sequence is formed by concatenating labeled DNA segments. The ground-truth state for observation $O_t$ (the $k$-mer starting at nucleotide index $t-1$ of the concatenated sequence) is the label of the segment containing this start position.\n\nThe accuracy is then computed as the fraction of correctly predicted states:\n$$\n\\text{Accuracy} = \\frac{1}{T} \\sum_{t=1}^{T} \\mathbb{I}(\\hat{S}_t = S^{\\text{truth}}_t)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. The final result for each test case is this accuracy value, rounded to $4$ decimal places.\n\nThe implementation will follow these principles precisely. For each test case, we first determine the parameters $k$ and $\\alpha$, train the HMM if its parameters have not been computed already, prepare the test observation sequence and corresponding ground-truth path, execute the Viterbi algorithm to obtain the predicted path, and finally calculate the accuracy.",
            "answer": "```python\nimport numpy as np\nfrom itertools import product\nfrom collections import Counter\n\n# Define states and bases for consistency.\nSTATES = ['E', 'I', 'G']\nSTATE_MAP = {s: i for i, s in enumerate(STATES)}\nN_STATES = len(STATES)\nBASES = ['A', 'C', 'G', 'T']\n\ndef generate_kmers(seq, k):\n    \"\"\"Generates a list of overlapping k-mers from a sequence.\"\"\"\n    if len(seq) < k:\n        return []\n    return [seq[i:i+k] for i in range(len(seq) - k + 1)]\n\ndef train_hmm(emission_data, transition_paths, k, alpha, beta, gamma):\n    \"\"\"\n    Trains HMM parameters (pi, A, B) from training data and returns them in log-space.\n    \"\"\"\n    # 1. K-mer alphabet generation\n    all_kmers = [''.join(p) for p in product(BASES, repeat=k)]\n    kmer_map = {kmer: i for i, kmer in enumerate(all_kmers)}\n    n_kmers = len(all_kmers)\n\n    # 2. Emission probabilities (B) estimation\n    emission_counts = np.zeros((N_STATES, n_kmers))\n    total_state_kmers = np.zeros(N_STATES)\n    for state_str, seqs in emission_data.items():\n        state_idx = STATE_MAP[state_str]\n        for seq in seqs:\n            kmers = generate_kmers(seq, k)\n            total_state_kmers[state_idx] += len(kmers)\n            counts = Counter(kmers)\n            for kmer, count in counts.items():\n                if kmer in kmer_map:\n                    emission_counts[state_idx, kmer_map[kmer]] += count\n    \n    # Laplace smoothing for B and conversion to log-space\n    log_B = np.log(emission_counts + alpha) - np.log(total_state_kmers.reshape(-1, 1) + alpha * n_kmers)\n\n    # 3. Initial probabilities (pi) estimation\n    initial_counts = np.zeros(N_STATES)\n    num_paths = len(transition_paths)\n    for path in transition_paths:\n        initial_counts[STATE_MAP[path[0]]] += 1\n\n    # Laplace smoothing for pi and conversion to log-space\n    log_pi = np.log(initial_counts + gamma) - np.log(num_paths + gamma * N_STATES)\n\n    # 4. Transition probabilities (A) estimation\n    transition_counts = np.zeros((N_STATES, N_STATES))\n    total_state_transitions = np.zeros(N_STATES)\n    for path in transition_paths:\n        for i in range(len(path) - 1):\n            s1_idx, s2_idx = STATE_MAP[path[i]], STATE_MAP[path[i+1]]\n            transition_counts[s1_idx, s2_idx] += 1\n            total_state_transitions[s1_idx] += 1\n\n    # Laplace smoothing for A and conversion to log-space\n    log_A = np.log(transition_counts + beta) - np.log(total_state_transitions.reshape(-1, 1) + beta * N_STATES)\n\n    return log_pi, log_A, log_B, kmer_map\n\ndef viterbi_decode(obs_seq, log_pi, log_A, log_B, kmer_map):\n    \"\"\"\n    Decodes the most likely state path for an observation sequence using the Viterbi algorithm.\n    \"\"\"\n    T = len(obs_seq)\n    \n    delta = np.zeros((T, N_STATES))\n    psi = np.zeros((T, N_STATES), dtype=int)\n\n    # Initialization step\n    first_obs_idx = kmer_map[obs_seq[0]]\n    delta[0, :] = log_pi + log_B[:, first_obs_idx]\n\n    # Recursion step\n    for t in range(1, T):\n        obs_idx = kmer_map[obs_seq[t]]\n        for j in range(N_STATES):  # current state\n            # Broadcasted calculation of max previous path probability\n            probs = delta[t-1, :] + log_A[:, j]\n            psi[t, j] = np.argmax(probs)\n            delta[t, j] = np.max(probs) + log_B[j, obs_idx]\n\n    # Termination and backtracking\n    path = np.zeros(T, dtype=int)\n    path[T-1] = np.argmax(delta[T-1, :])\n    for t in range(T-2, -1, -1):\n        path[t] = psi[t+1, path[t+1]]\n\n    # Convert state indices back to state labels\n    str_path = [STATES[i] for i in path]\n    return str_path\n\ndef solve():\n    # Common training data defined in the problem\n    emission_training_seqs = {\n        \"E\": [\"GCGCGCGCGCGCGCGCATGCGCGC\", \"CGCGCCGCGCGCGGCGCGC\"],\n        \"I\": [\"ATATATATATATATATATATTTATA\", \"AATAATATATAAATATATA\"],\n        \"G\": [\"AGCTAGCTAGCTAGCTAGCTAGCT\", \"GATCGATCGATCGATC\"]\n    }\n    transition_training_paths = [\n        \"GGGGGIIIIIEEEEEEEGGGGG\",\n        \"GGGIIIIIGGGEEE\",\n        \"EEEEEEIIIIGGGG\"\n    ]\n\n    # Test suite defined in the problem\n    test_cases = [\n        {'k': 2, 'alpha': 0.5, 'test_seq_parts': [(\"G\", \"AGCTAGCTAGC\"), (\"I\", \"ATATATATATAT\"), (\"E\", \"GCGCGCGCGC\")]},\n        {'k': 2, 'alpha': 0.5, 'test_seq_parts': [(\"I\", \"AT\")]},\n        {'k': 2, 'alpha': 1.0, 'test_seq_parts': [(\"G\", \"AGCTAGC\"), (\"E\", \"GCGCGC\"), (\"I\", \"ATATAT\")]},\n        {'k': 3, 'alpha': 0.5, 'test_seq_parts': [(\"G\", \"AGCTAGCT\"), (\"I\", \"ATATATAT\"), (\"E\", \"GCGCGCGC\")]}\n    ]\n\n    results = []\n    trained_models = {}\n\n    for case in test_cases:\n        k = case['k']\n        alpha = case['alpha']\n        beta = gamma = alpha\n        \n        # Memoize model training to avoid redundant computations\n        model_key = (k, alpha)\n        if model_key not in trained_models:\n            trained_models[model_key] = train_hmm(\n                emission_training_seqs, transition_training_paths, k, alpha, beta, gamma\n            )\n        log_pi, log_A, log_B, kmer_map = trained_models[model_key]\n\n        # Prepare test observation sequence\n        test_dna_seq = \"\".join([part[1] for part in case['test_seq_parts']])\n        obs_seq = generate_kmers(test_dna_seq, k)\n\n        # Generate ground-truth path\n        ground_truth_map = {}\n        current_pos = 0\n        for state, segment in case['test_seq_parts']:\n            for i in range(len(segment)):\n                ground_truth_map[current_pos + i] = state\n            current_pos += len(segment)\n        \n        num_obs = len(test_dna_seq) - k + 1 if len(test_dna_seq) >= k else 0\n        ground_truth = [ground_truth_map[i] for i in range(num_obs)]\n        \n        # Handle cases with no observations\n        if not obs_seq:\n            accuracy = 1.0 if not ground_truth else 0.0\n        else:\n            # Decode using Viterbi\n            predicted_path = viterbi_decode(obs_seq, log_pi, log_A, log_B, kmer_map)\n\n            # Calculate accuracy\n            correct_predictions = sum(p == g for p, g in zip(predicted_path, ground_truth))\n            accuracy = correct_predictions / len(ground_truth) if ground_truth else 1.0\n        \n        results.append(f\"{accuracy:.4f}\")\n\n    # Print a single line with the list of accuracies\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}