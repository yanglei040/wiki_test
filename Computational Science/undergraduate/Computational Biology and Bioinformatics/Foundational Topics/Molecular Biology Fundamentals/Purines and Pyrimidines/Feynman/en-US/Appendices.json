{
    "hands_on_practices": [
        {
            "introduction": "Understanding the elegant structure of the DNA double helix begins with its fundamental pairing rules. This practice problem  challenges you to apply Chargaff's rules, a cornerstone of molecular biology that dictates the stoichiometric relationship between purine and pyrimidine bases. Mastering this calculation is a crucial first step in analyzing genomic composition and understanding the physical basis of genetic information.",
            "id": "1516170",
            "problem": "A molecular biologist is analyzing the genome of a newly discovered extremophilic bacterium. The genome is found to consist of a single, linear molecule of double-stranded Deoxyribonucleic acid (DNA). Through base composition analysis, it is determined that cytosine bases account for 22.0% of the total nitrogenous bases in the molecule. Assuming standard Watson-Crick base pairing governs the structure of this DNA, calculate the expected proportion of adenine bases.\n\nExpress your answer as a decimal, rounded to three significant figures.",
            "solution": "Let the fractions of the four bases in the double-stranded DNA be denoted by $p_{A}$, $p_{T}$, $p_{G}$, and $p_{C}$. Standard Watson-Crick pairing implies Chargaff’s rules: $p_{A} = p_{T}$ and $p_{G} = p_{C}$. The fractions must sum to unity:\n$$\np_{A} + p_{T} + p_{G} + p_{C} = 1.\n$$\nGiven $p_{C} = 0.220$, we have $p_{G} = p_{C} = 0.220$. Therefore\n$$\np_{A} + p_{T} = 1 - (p_{G} + p_{C}) = 1 - 2p_{C}.\n$$\nUsing $p_{A} = p_{T}$ gives\n$$\n2p_{A} = 1 - 2p_{C} \\quad \\Rightarrow \\quad p_{A} = \\frac{1 - 2p_{C}}{2}.\n$$\nSubstituting $p_{C} = 0.220$,\n$$\np_{A} = \\frac{1 - 0.440}{2} = \\frac{0.560}{2} = 0.280.\n$$\nRounded to three significant figures, the expected proportion of adenine is $0.280$.",
            "answer": "$$\\boxed{0.280}$$"
        },
        {
            "introduction": "While we often think of DNA base pairing as fixed, the bases themselves are dynamic molecules that can temporarily shift into alternative chemical forms called tautomers. This exercise  explores how a rare tautomeric shift in guanine can disrupt normal Watson-Crick pairing, leading to a point mutation during replication. By tracing the outcome through multiple cell divisions, you will gain a deeper insight into the chemical origins of spontaneous mutation and the importance of replication fidelity.",
            "id": "1516208",
            "problem": "In the study of spontaneous mutations in Deoxyribonucleic Acid (DNA), tautomeric shifts of nitrogenous bases are a key mechanism. Normally, in a DNA double helix, adenine (A) pairs with thymine (T) via two hydrogen bonds, and guanine (G) pairs with cytosine (C) via three hydrogen bonds. Guanine and thymine typically exist in a stable keto form. However, guanine can undergo a transient tautomeric shift to a rare enol form. In its common keto form, the oxygen at position 6 (O6) of guanine acts as a hydrogen bond acceptor and the nitrogen at position 1 (N1) acts as a hydrogen bond donor. In its rare enol form, a proton shifts from N1 to O6, causing O6 to become a hydrogen bond donor and N1 to become a hydrogen bond acceptor. This altered pattern allows the enol form of guanine to form a stable base pair with keto-thymine.\n\nConsider a segment of a DNA molecule with the sequence:\n5'-ATGC-3'\n3'-TACG-5'\n\nDuring a single, specific event of DNA replication, the guanine (G) base on the top strand (at the third position) momentarily shifts to its enol tautomer just as it is being replicated. It then immediately reverts to its stable keto form after the new complementary base has been incorporated. Assume no DNA repair mechanisms are activated. After this event, the cell and its descendants undergo a second, complete round of DNA replication. One of the four resulting granddaughter DNA molecules will carry a fixed point mutation.\n\nWhat will be the base pair at this third position in the stably mutated granddaughter DNA molecule?\n\nA. G-C\n\nB. A-T\n\nC. T-A\n\nD. C-G\n\nE. G-T",
            "solution": "Original duplex:\nTop: 5'-ATGC-3'\nBottom: 3'-TACG-5'\nAt position 3, the normal pair is G (top) opposite C (bottom).\n\nDuring the first replication, the top strand serves as template. The guanine at position 3 transiently shifts to the enol tautomer, which pairs with thymine. DNA polymerase incorporates T opposite this enol G. Immediately after incorporation, G reverts to the keto form, leaving a G-T mismatch in one daughter duplex. The two daughter duplexes after this first replication are:\n1) Mismatched daughter: Top (old) 5'-ATGC-3' paired with Bottom (new) 3'-TATG-5' (G opposite T at position 3).\n2) Correct daughter: Top (new) 5'-ATGC-3' paired with Bottom (old) 3'-TACG-5' (G opposite C at position 3).\n\nIn the second replication, the mismatched daughter yields two outcomes because each strand templates normal base pairing:\n- Using the top G (keto) as template gives a new C opposite it, restoring G-C in that granddaughter.\n- Using the bottom T as template gives a new A opposite it, fixing an A-T base pair in that granddaughter.\n\nThus, among the four granddaughters, one carries the fixed mutation at position 3 as an A-T base pair.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "The deluge of genomic data requires computational tools to identify meaningful features, such as distinguishing protein-coding exons from non-coding introns. This hands-on coding challenge  bridges molecular biology and machine learning, tasking you to exploit a subtle statistical signal—a period-3 pattern in the purine-pyrimidine sequence—that arises from the triplet nature of the genetic code. By building a classifier from scratch, you will experience firsthand how abstract sequence properties can be leveraged for practical gene prediction.",
            "id": "2423535",
            "problem": "You are given short deoxyribonucleic acid (DNA) segments and must build a program that learns to distinguish protein-coding exons from introns using only features derived from their local purine and pyrimidine composition. A purine is either adenine or guanine, and a pyrimidine is either cytosine or thymine. The training set contains labeled sequences. The test set contains unlabeled sequences. Your program must compute features from the purine/pyrimidine landscape, fit a principled linear classifier on the training set, and output predictions for the test set in the prescribed format.\n\nFundamental base and assumptions to use:\n- Coding theory: the genetic code is read in codons of length $3$, and protein-coding exons reflect codon structure. Introns do not encode amino acids and lack codon-aligned constraints.\n- Definition: map each nucleotide to a binary indicator $x_i$ for position $i$ in a sequence of length $L$ by $x_i = 1$ if the base is a purine (adenine or guanine) and $x_i = 0$ if it is a pyrimidine (cytosine or thymine).\n- Definition: from the binary sequence $(x_1,\\dots,x_L)$, define the following features:\n  1. Purine fraction $f_1 = \\frac{1}{L}\\sum_{i=1}^L x_i$.\n  2. Standard deviation $f_2 = \\sqrt{\\frac{1}{L}\\sum_{i=1}^L (x_i - \\bar{x})^2}$, where $\\bar{x} = \\frac{1}{L}\\sum_{i=1}^L x_i$.\n  3. Lag-$3$ autocorrelation coefficient $f_3 = \\rho_3$, defined by\n     $$\\rho_3 = \\begin{cases}\n     \\dfrac{\\sum_{i=1}^{L-3} (x_i - \\bar{x})(x_{i+3} - \\bar{x})}{(L-3)\\, f_2^2}, & \\text{if } f_2^2 > 0,\\\\\n     0, & \\text{if } f_2^2 = 0.\n     \\end{cases}$$\n  4. Run-length contrast $f_4 = \\mathbb{E}[L_R] - \\mathbb{E}[L_Y]$, where $\\mathbb{E}[L_R]$ is the mean length of contiguous runs of purines ($x_i = 1$) and $\\mathbb{E}[L_Y]$ is the mean length of contiguous runs of pyrimidines ($x_i = 0$). If a sequence contains no runs of a given type, define the corresponding mean as $0$ by convention.\n\nClassifier design requirement:\n- Train a linear classifier grounded in the following probabilistic modeling principle: assume the feature vectors are drawn from class-conditional multivariate normal distributions with a shared covariance matrix and equal class priors; learn a linear decision rule that separates the two classes. The implementation must not rely on any external data or libraries beyond those allowed. The program must be fully deterministic.\n\nInput to be hard-coded in the program:\n- Training set: eight labeled sequences, with label $1$ for exon and $0$ for intron. Each sequence is specified as an exact repetition of a motif:\n  - Exon sequences (label $1$):\n    1. Repeat the motif \"AAT\" exactly $30$ times.\n    2. Repeat the motif \"GAA\" exactly $30$ times.\n    3. Repeat the motif \"CCG\" exactly $30$ times.\n    4. Repeat the motif \"ATG\" exactly $30$ times.\n  - Intron sequences (label $0$):\n    1. Repeat the motif \"ACGT\" exactly $24$ times.\n    2. Repeat the motif \"TCGA\" exactly $24$ times.\n    3. Repeat the motif \"TTTCCCTTTCCC\" exactly $5$ times.\n    4. Repeat the motif \"AGCT\" exactly $24$ times.\n\n- Test suite: six unlabeled sequences to classify, constructed by repeating the indicated motif the specified number of times:\n  1. Repeat \"AAT\" exactly $20$ times.\n  2. Repeat \"ACGT\" exactly $15$ times.\n  3. Repeat \"TTTCCCTTTCCC\" exactly $3$ times.\n  4. Repeat \"ATG\" exactly $20$ times.\n  5. A sequence of $60$ adenines (\"A\").\n  6. Repeat \"AGCT\" exactly $15$ times.\n\nAlgorithmic and implementation requirements:\n- Map each sequence to $(x_1,\\dots,x_L)$ using the purine/pyrimidine indicator definition.\n- Compute the feature vector $(f_1,f_2,f_3,f_4)$ for every sequence.\n- Fit the linear classifier on the training feature vectors using the shared-covariance Gaussian assumption with equal priors, and use it to predict labels for the test feature vectors. All computations must be performed using finite-dimensional linear algebra. If any covariance matrix is singular or ill-conditioned, you must use a Moore–Penrose pseudoinverse.\n- All intermediate quantities must be computed in floating-point arithmetic. No physical units or angles are involved.\n- Edge cases to handle explicitly:\n  - If $f_2^2 = 0$, set $\\rho_3 = 0$.\n  - If there are no runs of purines or no runs of pyrimidines, set the corresponding mean run length to $0$ when computing $f_4$.\n\nFinal output format:\n- Your program should produce a single line of output containing the predicted labels for the six test sequences as a comma-separated list of integers enclosed in square brackets, for example, \"[1,0,1,1,0,0]\". The order must match the order listed above.\n\nAnswer specification:\n- The outputs are integers in $\\{0,1\\}$.\n\nYour program must be complete and runnable as-is. It must not read any external input. It must use only the specified libraries.",
            "solution": "The problem presented is a binary classification task in computational biology: to distinguish deoxyribonucleic acid (DNA) sequences corresponding to exons from those corresponding to introns. This task must be accomplished by constructing a linear classifier based only on features derived from the local purine and pyrimidine composition of the sequences. The problem is well-defined, scientifically grounded, and provides all necessary data and constraints for a unique, verifiable solution.\n\nThe solution proceeds in three principal stages. First, we formalize the feature extraction process, converting each DNA sequence into a low-dimensional numerical vector. Second, we derive the linear classifier from first principles, as specified by the problem statement. Third, we outline the complete algorithmic procedure for training the classifier and predicting the labels of the test sequences.\n\n**1. Feature Engineering from Purine-Pyrimidine Content**\n\nThe core hypothesis is that the statistical properties of purine and pyrimidine distributions differ between exons and introns due to the presence of codon structure in coding regions. We begin by transforming a given DNA sequence of length $L$ into a binary numerical sequence.\n\nLet a DNA sequence be represented by $S = (s_1, s_2, \\dots, s_L)$, where $s_i \\in \\{\\text{'A'}, \\text{'C'}, \\text{'G'}, \\text{'T'}\\}$. We define a binary indicator variable $x_i$ for each position $i$ from $1$ to $L$. The mapping is based on the chemical classification of nucleotides:\n- Adenine ('A') and Guanine ('G') are purines.\n- Cytosine ('C') and Thymine ('T') are pyrimidines.\n\nThe binary sequence $\\mathbf{x} = (x_1, x_2, \\dots, x_L)$ is defined as:\n$$\nx_i = \\begin{cases} 1 & \\text{if } s_i \\text{ is a purine (A or G)} \\\\ 0 & \\text{if } s_i \\text{ is a pyrimidine (C or T)} \\end{cases}\n$$\nFrom this binary sequence, we compute a $4$-dimensional feature vector $\\mathbf{f} = (f_1, f_2, f_3, f_4)$.\n\n**Feature $f_1$: Purine Fraction**\nThis feature measures the overall compositional bias towards purines. It is the mean of the binary sequence.\n$$ f_1 = \\bar{x} = \\frac{1}{L} \\sum_{i=1}^L x_i $$\n\n**Feature $f_2$: Standard Deviation**\nThis feature measures the local variation in purine/pyrimidine content. It is the standard deviation of the binary sequence. A sequence with uniform composition (all purines or all pyrimidines) will have $f_2 = 0$.\n$$ f_2 = \\sqrt{\\frac{1}{L} \\sum_{i=1}^L (x_i - \\bar{x})^2} $$\nThe variance is simply $f_2^2 = \\bar{x}(1-\\bar{x})$ for a sequence of Bernoulli variables, but the direct formula is used for computation.\n\n**Feature $f_3$: Lag-$3$ Autocorrelation Coefficient**\nThis is the most critical feature for detecting coding potential. The genetic code is read in codons, which are triplets of nucleotides. This imposes a periodicity of $3$ on the statistical properties of exon sequences, which is absent in introns. The lag-$3$ autocorrelation, $\\rho_3$, is designed to capture this signal.\n$$\nf_3 = \\rho_3 = \\begin{cases}\n     \\dfrac{\\sum_{i=1}^{L-3} (x_i - \\bar{x})(x_{i+3} - \\bar{x})}{(L-3)\\, f_2^2}, & \\text{if } f_2^2 > 0,\\\\\n     0, & \\text{if } f_2^2 = 0.\n     \\end{cases}\n$$\nFor a sequence with a strong periodic signal of period $3$, we expect $x_i \\approx x_{i+3}$, leading to a high positive value for $\\rho_3$. For a random sequence, we expect $\\rho_3 \\approx 0$.\n\n**Feature $f_4$: Run-Length Contrast**\nThis feature quantifies the difference in the lengths of contiguous segments of purines versus pyrimidines. Let $\\mathbb{E}[L_R]$ be the mean length of purine runs (where $x_i=1$) and $\\mathbb{E}[L_Y]$ be the mean length of pyrimidine runs (where $x_i=0$).\n$$ f_4 = \\mathbb{E}[L_R] - \\mathbb{E}[L_Y] $$\nBy convention, if a sequence contains no runs of a certain type (e.g., an all-purine sequence has no pyrimidine runs), the corresponding mean length is defined as $0$.\n\n**2. Classifier Design: Linear Discriminant Analysis**\n\nThe problem requires training a linear classifier based on a specific generative model. We assume the feature vectors for each class $k \\in \\{0, 1\\}$ (where $1$ is exon, $0$ is intron) are drawn from multivariate normal distributions, $p(\\mathbf{f}|C_k) = \\mathcal{N}(\\mathbf{f} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$. The problem specifies two critical constraints: the covariance matrices are shared ($\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_0 = \\boldsymbol{\\Sigma}$) and the class priors are equal ($P(C_1) = P(C_0) = 0.5$). This model is known as Linear Discriminant Analysis (LDA).\n\nFirst, we estimate the model parameters from the training set, which consists of $N$ feature vectors $\\{\\mathbf{f}_i\\}_{i=1}^N$ and their corresponding labels $\\{y_i\\}_{i=1}^N$. Let $N_1$ be the number of exon samples and $N_0$ be the number of intron samples.\n\nThe class means, $\\boldsymbol{\\mu}_1$ and $\\boldsymbol{\\mu}_0$, are estimated by the sample means of the feature vectors within each class:\n$$ \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{i: y_i=k} \\mathbf{f}_i \\quad \\text{for } k \\in \\{0, 1\\} $$\n\nThe shared covariance matrix, $\\boldsymbol{\\Sigma}$, is estimated by the pooled covariance of the two classes. This is an unbiased estimate:\n$$ \\boldsymbol{\\Sigma} = \\frac{1}{N_0 + N_1 - 2} \\left[ \\sum_{i: y_i=0} (\\mathbf{f}_i - \\boldsymbol{\\mu}_0)(\\mathbf{f}_i - \\boldsymbol{\\mu}_0)^T + \\sum_{i: y_i=1} (\\mathbf{f}_i - \\boldsymbol{\\mu}_1)(\\mathbf{f}_i - \\boldsymbol{\\mu}_1)^T \\right] $$\n\nThe classification of a new feature vector $\\mathbf{f}$ is based on the posterior probability ratio, derived from Bayes' theorem. We classify to class $1$ if $P(C_1|\\mathbf{f}) > P(C_0|\\mathbf{f})$, which is equivalent to comparing their log-ratio to $0$.\n$$ \\ln\\frac{P(C_1|\\mathbf{f})}{P(C_0|\\mathbf{f})} = \\ln\\frac{p(\\mathbf{f}|C_1)P(C_1)}{p(\\mathbf{f}|C_0)P(C_0)} > 0 $$\nGiven equal priors, this simplifies to $\\ln p(\\mathbf{f}|C_1) - \\ln p(\\mathbf{f}|C_0) > 0$. Substituting the multivariate normal probability density function and simplifying, the quadratic terms in $\\mathbf{f}$ cancel, yielding a linear decision rule.\n$$ (\\mathbf{f} - \\boldsymbol{\\mu}_0)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{f} - \\boldsymbol{\\mu}_0) - (\\mathbf{f} - \\boldsymbol{\\mu}_1)^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{f} - \\boldsymbol{\\mu}_1) < 0 $$\nExpanding and rearranging terms leads to the decision function:\n$$ \\mathbf{w}^T \\mathbf{f} - c > 0 $$\nwhere the weight vector $\\mathbf{w}$ and the threshold $c$ are defined as:\n$$ \\mathbf{w} = \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0) $$\n$$ c = \\frac{1}{2} (\\boldsymbol{\\mu}_1^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0^T \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_0) = \\mathbf{w}^T \\left( \\frac{\\boldsymbol{\\mu}_0 + \\boldsymbol{\\mu}_1}{2} \\right) $$\nAs per the instruction, if the matrix $\\boldsymbol{\\Sigma}$ is singular or ill-conditioned, its Moore-Penrose pseudoinverse, $\\boldsymbol{\\Sigma}^{\\dagger}$, is used in place of $\\boldsymbol{\\Sigma}^{-1}$.\nThe final prediction $\\hat{y}$ for a test vector $\\mathbf{f}_{\\text{test}}$ is:\n$$ \\hat{y} = \\begin{cases} 1 & \\text{if } \\mathbf{w}^T \\mathbf{f}_{\\text{test}} > c \\\\ 0 & \\text{otherwise} \\end{cases} $$\n\n**3. Algorithmic Procedure**\n\nThe implementation follows a direct translation of the principles above.\n\n1.  **Data Representation**: The training and test sequences are generated by repeating the specified motifs.\n2.  **Feature Computation**:\n    -   A function is created to map a DNA sequence string to its binary purine/pyrimidine representation.\n    -   A second function computes the $4$-dimensional feature vector $(f_1, f_2, f_3, f_4)$ for any given binary sequence. This function must correctly implement the specified formulas and handle the edge cases for $f_3$ (when $f_2=0$) and $f_4$ (when runs of a certain type are absent).\n3.  **Training Phase**:\n    -   Compute the feature vectors for all $N_1=4$ exon training sequences and all $N_0=4$ intron training sequences.\n    -   Calculate the class mean vectors $\\boldsymbol{\\mu}_0$ and $\\boldsymbol{\\mu}_1$.\n    -   Calculate the pooled covariance matrix $\\boldsymbol{\\Sigma}$.\n    -   Compute the pseudoinverse $\\boldsymbol{\\Sigma}^{\\dagger}$ using numerical linear algebra routines.\n    -   Determine the classifier weight vector $\\mathbf{w}$ and the decision threshold $c$.\n4.  **Prediction Phase**:\n    -   For each of the $6$ test sequences, compute its feature vector $\\mathbf{f}_{\\text{test}}$.\n    -   For each $\\mathbf{f}_{\\text{test}}$, calculate the discriminant score $S = \\mathbf{w}^T \\mathbf{f}_{\\text{test}}$.\n    -   Compare the score $S$ to the threshold $c$ to obtain the predicted label, $1$ or $0$.\n5.  **Output**: Collect the predicted labels for the test sequences in the specified order and format them as a single output string.\n\nThis rigorous, step-by-step procedure ensures the solution is both correct according to the problem's constraints and grounded in established statistical principles.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DNA sequence classification problem.\n    The solution involves feature extraction, training a Linear Discriminant Analysis (LDA)\n    classifier, and predicting labels for a test set.\n    \"\"\"\n\n    # --- 1. Data Definition ---\n    # Training set\n    train_exons_motifs = [(\"AAT\", 30), (\"GAA\", 30), (\"CCG\", 30), (\"ATG\", 30)]\n    train_introns_motifs = [(\"ACGT\", 24), (\"TCGA\", 24), (\"TTTCCCTTTCCC\", 5), (\"AGCT\", 24)]\n    \n    train_exons = [m * r for m, r in train_exons_motifs]\n    train_introns = [m * r for m, r in train_introns_motifs]\n    \n    # Test set\n    test_motifs = [\n        (\"AAT\", 20),\n        (\"ACGT\", 15),\n        (\"TTTCCCTTTCCC\", 3),\n        (\"ATG\", 20),\n        (\"A\", 60),\n        (\"AGCT\", 15),\n    ]\n    test_sequences = [m * r for m, r in test_motifs]\n\n    # --- 2. Feature Extraction ---\n    \n    def dna_to_binary(seq):\n        \"\"\"Maps a DNA sequence to a binary purine(1)/pyrimidine(0) sequence.\"\"\"\n        purines = {'A', 'G'}\n        return np.array([1 if base in purines else 0 for base in seq], dtype=np.float64)\n\n    def compute_features(seq_str):\n        \"\"\"Computes the 4-dimensional feature vector for a DNA sequence.\"\"\"\n        L = len(seq_str)\n        if L == 0:\n            return np.zeros(4)\n\n        x = dna_to_binary(seq_str)\n        \n        # f1: Purine fraction\n        f1 = np.mean(x)\n\n        # f2: Standard deviation\n        # use np.var to avoid floating point issues from (x_i - mean(x))^2\n        var_x = np.var(x)\n        f2 = np.sqrt(var_x)\n\n        # f3: Lag-3 autocorrelation\n        if var_x > 1e-12:  # Check for f2^2 > 0 with tolerance\n            x_minus_mean = x - f1\n            numerator = np.sum(x_minus_mean[:-3] * x_minus_mean[3:])\n            denominator = (L - 3) * var_x\n            f3 = numerator / denominator if denominator != 0 else 0.0\n        else:\n            f3 = 0.0\n\n        # f4: Run-length contrast\n        if L > 0:\n            runs_R = []\n            runs_Y = []\n            current_run_type = x[0]\n            current_run_length = 1\n            for i in range(1, L):\n                if x[i] == current_run_type:\n                    current_run_length += 1\n                else:\n                    if current_run_type == 1:\n                        runs_R.append(current_run_length)\n                    else:\n                        runs_Y.append(current_run_length)\n                    current_run_type = x[i]\n                    current_run_length = 1\n            # Add the last run\n            if current_run_type == 1:\n                runs_R.append(current_run_length)\n            else:\n                runs_Y.append(current_run_length)\n\n            mean_R = np.mean(runs_R) if runs_R else 0.0\n            mean_Y = np.mean(runs_Y) if runs_Y else 0.0\n            f4 = mean_R - mean_Y\n        else:\n            f4 = 0.0\n            \n        return np.array([f1, f2, f3, f4])\n\n    # --- 3. Training the LDA Classifier ---\n\n    # Compute feature vectors for training data\n    features_exons = np.array([compute_features(s) for s in train_exons])\n    features_introns = np.array([compute_features(s) for s in train_introns])\n    \n    # Estimate parameters\n    mu1 = np.mean(features_exons, axis=0) # Exon mean\n    mu0 = np.mean(features_introns, axis=0) # Intron mean\n\n    N1 = len(features_exons)\n    N0 = len(features_introns)\n    \n    # Pooled covariance matrix\n    S1 = np.zeros((4, 4))\n    for f in features_exons:\n        diff = (f - mu1).reshape(4, 1)\n        S1 += diff @ diff.T\n\n    S0 = np.zeros((4, 4))\n    for f in features_introns:\n        diff = (f - mu0).reshape(4, 1)\n        S0 += diff @ diff.T\n        \n    # Unbiased estimate of shared covariance\n    pooled_sigma = (S1 + S0) / (N1 + N0 - 2)\n    \n    # Use Moore-Penrose pseudoinverse for stability\n    sigma_inv = np.linalg.pinv(pooled_sigma)\n\n    # Calculate LDA weights and threshold\n    w = sigma_inv @ (mu1 - mu0)\n    c = w @ (mu1 + mu0) / 2.0\n\n    # --- 4. Prediction on Test Set ---\n    \n    predictions = []\n    for seq in test_sequences:\n        f_test = compute_features(seq)\n        score = w @ f_test\n        prediction = 1 if score > c else 0\n        predictions.append(prediction)\n        \n    # --- 5. Output ---\n    \n    print(f\"[{','.join(map(str, predictions))}]\")\n\nsolve()\n```"
        }
    ]
}