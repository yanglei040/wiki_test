{
    "hands_on_practices": [
        {
            "introduction": "In modern bioinformatics, reproducibility is paramount. A critical component of a stable database architecture is a clear system for tracking changes to its records. This exercise  challenges you to adapt Semantic Versioning (SemVer), a standard from software engineering, to the world of gene annotations. Your task is to think like a database architect and decide what kinds of data changes are minor fixes versus major, \"breaking\" changes that could invalidate downstream scientific analyses.",
            "id": "2373018",
            "problem": "You are designing versioning for gene annotation records in a curated secondary biological database that builds upon a primary archive. A primary database stores submitted raw sequences and minimally processed feature annotations, while a secondary database consolidates, cross-references, and refines annotations across sources, aiming to provide stable identifiers and consistent models. The curation team wants to adopt Semantic Versioning (SemVer), where a record’s version is written as $M.m.p$ and the labels MAJOR, MINOR, and PATCH must be mapped to specific classes of changes in the gene annotation. Consider that downstream users depend on stable gene identifiers and coding sequences for pipelines such as variant effect prediction, protein functional analysis, and cross-database mapping.\n\nWhich of the following policies most appropriately maps MAJOR, MINOR, and PATCH to changes in a gene annotation record in a secondary database, in a way that adheres to the SemVer principle that MAJOR corresponds to backward-incompatible changes, MINOR to backward-compatible additions, and PATCH to backward-compatible fixes?\n\nA. MAJOR when the gene identifier changes, the gene is split or merged, gene boundaries shift in a way that alters the coding sequence, the reference assembly or coordinate system remapping invalidates prior coordinates, or the gene type changes in a way that alters interpretability; MINOR when new transcript isoforms or cross-references are added without changing existing transcript identifiers or coding sequences; PATCH when only metadata are corrected, such as description text, synonyms, publication links, or evidence codes, without changing any sequences, coordinates, or identifiers.\n\nB. MAJOR when functional description text or preferred gene symbol changes; MINOR when coding sequence or protein sequence changes but the gene identifier remains the same; PATCH when new transcript isoforms are added or removed.\n\nC. MAJOR only when the database schema or file format for the entire repository changes; MINOR when any feature coordinate changes, including small shifts in untranslated regions, but coding sequence remains identical; PATCH when metadata are corrected and when new cross-references or ontology annotations are added.\n\nD. MAJOR when new cross-references are added to external resources; MINOR when gene boundaries move or transcript models are retired; PATCH when the reference coding sequence is corrected to fix an erroneous amino acid, since it is a small change to the protein but keeps the identifier stable.",
            "solution": "The problem statement requires the formulation of a versioning policy for gene annotation records within a curated secondary biological database, using the principles of Semantic Versioning (SemVer). The core task is to map the version increments—MAJOR, MINOR, and PATCH—to specific types of changes in a gene annotation record.\n\nFirst, let us formalize the principles of Semantic Versioning, or SemVer, for a version identifier of the form $M.m.p$:\n1.  **MAJOR version ($M$)**: Incremented for backward-incompatible changes. A user of a previous version cannot necessarily switch to the new version without modifications to their dependent code or analysis pipeline. The \"API\" has been broken.\n2.  **MINOR version ($m$)**: Incremented for adding new functionality in a backward-compatible manner. Existing functionality remains, and users can safely upgrade.\n3.  **PATCH version ($p$)**: Incremented for making backward-compatible bug fixes. These are corrections that do not add new features or break existing ones.\n\nThe problem states that this system is to be applied to a gene annotation record in a secondary database. Downstream users rely on stable gene identifiers and coding sequences (CDS). Therefore, the \"API\" of a gene annotation record is constituted by its stable identifier, its genomic coordinates, its defined features (exons, CDS), and the sequences derived from these features (e.g., transcript and protein sequences). A \"backward-incompatible\" change is one that would break or invalidate analyses that depend on these core data elements.\n\nBased on these first principles, we can proceed to validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **System**: Versioning for gene annotation records.\n-   **Database Type**: Curated secondary biological database, building upon a primary archive.\n-   **Database Function**: Consolidates, cross-references, refines annotations; provides stable identifiers and consistent models.\n-   **Versioning Scheme**: Semantic Versioning (SemVer), format $M.m.p$.\n-   **SemVer Definitions**:\n    -   MAJOR: Backward-incompatible changes.\n    -   MINOR: Backward-compatible additions.\n    -   PATCH: Backward-compatible fixes.\n-   **User Dependencies**: Stable gene identifiers and coding sequences.\n-   **User Applications**: Variant effect prediction, protein functional analysis, cross-database mapping.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and logically sound.\n-   **Scientifically Grounded**: The distinction between primary (e.g., GenBank/ENA) and secondary curated databases (e.g., RefSeq, Ensembl/GENCODE) is a fundamental concept in bioinformatics. The challenges of versioning annotations and maintaining stability for downstream users are real and critical issues in the field. Semantic Versioning is a widely accepted standard in software engineering, and its application to data versioning is a logical extension.\n-   **Well-Posed**: The problem is well-defined. It provides the versioning framework (SemVer), the data context (gene annotations), and the criteria for what constitutes a breaking change (dependencies of downstream users). The task is to find the most appropriate mapping among the given options, which is a solvable and unambiguous question.\n-   **Objective**: The question is objective, based on the formal definitions of SemVer and the concrete requirements of bioinformatics pipelines.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed problem in computational biology that requires a rigorous application of established principles. We will now proceed with the solution by evaluating each option.\n\n### Option-by-Option Analysis\n\n**Option A:**\n-   **MAJOR**: `when the gene identifier changes, the gene is split or merged, gene boundaries shift in a way that alters the coding sequence, the reference assembly or coordinate system remapping invalidates prior coordinates, or the gene type changes in a way that alters interpretability`\n    -   A change in the gene identifier is the most definitive backward-incompatible change, as it breaks any direct lookup.\n    -   Splitting or merging genes fundamentally alters the gene model, invalidating all prior assumptions about that locus.\n    -   A change to the coding sequence (CDS) is also a critical breaking change. Pipelines for variant effect prediction or protein analysis depend on the exact reference CDS; altering it makes previous analyses invalid.\n    -   Remapping to a new assembly or coordinate system makes all previous coordinates obsolete.\n    -   Changing a gene type (e.g., from `protein_coding` to `pseudogene`) is a major re-interpretation that breaks assumptions for any analysis focused on protein function.\n    -   This mapping correctly identifies changes that are backward-incompatible for the specified downstream applications. This aligns with the purpose of a MAJOR version increment.\n-   **MINOR**: `when new transcript isoforms or cross-references are added without changing existing transcript identifiers or coding sequences`\n    -   Adding new isoforms or new cross-references enriches the record with new information without invalidating or altering the existing, stable data. A user interested in a pre-existing transcript can still find it unchanged. This is the definition of a backward-compatible addition of functionality. This aligns with the MINOR version increment.\n-   **PATCH**: `when only metadata are corrected, such as description text, synonyms, publication links, or evidence codes, without changing any sequences, coordinates, or identifiers`\n    -   Correcting typos in a description, updating a URL, or refining an evidence code are all fixes that improve the quality of the record but do not affect the core structural or sequence data upon which computational pipelines depend. These are backward-compatible fixes. This aligns with the PATCH version increment.\n\n**Verdict for A**: This option correctly and precisely maps the principles of SemVer to the context of gene annotation versioning. **Correct**.\n\n**Option B:**\n-   **MAJOR**: `when functional description text or preferred gene symbol changes`\n    -   Changing a text description is a minor correction, not a MAJOR breaking change. A gene symbol change can be disruptive but is often handled via synonym lists; it is not as severe as a CDS change, especially if a stable primary ID is maintained. This is a poor mapping for a MAJOR change.\n-   **MINOR**: `when coding sequence or protein sequence changes but the gene identifier remains the same`\n    -   This is a catastrophic misinterpretation of backward compatibility. A change to the coding sequence is a fundamental, breaking change that invalidates most downstream analyses. To label this as a MINOR, backward-compatible change is scientifically dangerous and defeats the purpose of versioning.\n-   **PATCH**: `new transcript isoforms are added or removed`\n    -   Adding a new isoform is a feature addition (MINOR). *Removing* an isoform is a breaking change (MAJOR). Classifying these actions, especially removal, as a mere PATCH is incorrect.\n\n**Verdict for B**: This policy is fundamentally flawed and misrepresents the impact of critical data changes. **Incorrect**.\n\n**Option C:**\n-   **MAJOR**: `only when the database schema or file format for the entire repository changes`\n    -   This confuses the versioning of an individual data record with the versioning of the entire database system. A record can undergo a breaking change (e.g., CDS update) independently of the global database schema. This policy is far too restrictive and fails to capture record-level breaking changes.\n-   **MINOR**: `when any feature coordinate changes, including small shifts in untranslated regions, but coding sequence remains identical`\n    -   This is a plausible definition for a MINOR or PATCH change, but the policy as a whole is flawed.\n-   **PATCH**: `when metadata are corrected and when new cross-references or ontology annotations are added`\n    -   This conflates two distinct types of changes. Metadata correction is a PATCH. Adding new features like cross-references or annotations is a MINOR change. Lumping them together is imprecise.\n\n**Verdict for C**: This policy misapplies the scope of versioning and conflates PATCH and MINOR changes. **Incorrect**.\n\n**Option D:**\n-   **MAJOR**: `when new cross-references are added to external resources`\n    -   Adding a cross-reference is a non-breaking feature addition. This should be a MINOR change. Assigning it to MAJOR is a complete misunderstanding of SemVer.\n-   **MINOR**: `when gene boundaries move or transcript models are retired`\n    -   Retiring a transcript model is a backward-incompatible change, as any analysis relying on it will fail. This must be a MAJOR change. Moving gene boundaries can also be a MAJOR change if it alters the CDS.\n-   **PATCH**: `when the reference coding sequence is corrected to fix an erroneous amino acid, since it is a small change to the protein but keeps the identifier stable`\n    -   As with option B, this is a dangerous and incorrect classification. A correction to the CDS that changes the resulting protein sequence is a backward-incompatible change, regardless of how \"small\" it seems. It is a bug fix, but a breaking one, thus necessitating a MAJOR version increment.\n\n**Verdict for D**: This policy demonstrates a fundamental lack of understanding of SemVer principles, incorrectly classifying the severity of every type of change. **Incorrect**.\n\nIn conclusion, only option A presents a logically consistent, scientifically sound, and pragmatically useful application of Semantic Versioning to gene annotation records, correctly prioritizing the stability of identifiers and sequences that are critical for downstream bioinformatics research.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Moving from individual records to a holistic view, how can we assess the overall quality of an entire database? This thought experiment  asks you to construct a composite \"energy function\" for a major resource like the Protein Data Bank (PDB), where lower energy corresponds to higher quality. You will learn to quantify and weigh different architectural flaws—such as redundancy, inconsistent cross-references, and missing metadata—to produce a single, comprehensive quality score.",
            "id": "2373027",
            "problem": "A primary database aggregates experimentally determined biological macromolecule structures, while secondary databases classify and annotate structures derived from primary archives. Consider the Protein Data Bank (PDB) as a primary database, cross-referenced to secondary classification resources such as the Structural Classification of Proteins—extended (SCOPe) and Class, Architecture, Topology, Homologous superfamily (CATH). Model the entire PDB as a single system of $N$ entries and define a dimensionless \"energy function\" $E_{PDB}$, where lower values correspond to higher data quality and consistency, as a weighted sum of four penalties that reflect architectural properties of primary and secondary databases:\n- Redundancy penalty $R$: Let $U$ be the number of distinct mappings from PDB entries to UniProt (Universal Protein Resource) accessions. Define $R = \\dfrac{N - U}{N}$.\n- Cross-reference inconsistency penalty $C$: Let $X$ be the number of PDB entries that have both SCOPe and CATH annotations but whose top-level class assignments disagree. Define $C = \\dfrac{X}{N}$.\n- Missing metadata penalty $M$: Consider four required metadata fields per entry: experimental method, organism taxonomy, UniProt cross-reference, and deposition date. Let $T$ be the total count of missing field instances across all entries. Define $M = \\dfrac{T}{4N}$.\n- Structural resolution penalty $S$: For an X-ray crystallography entry with resolution $r_i$ (in Angstroms), define the per-entry penalty $s_i = \\max\\!\\left(0, \\dfrac{r_i - r_0}{r_0}\\right)$ with threshold $r_0$. For non–X-ray entries, use a constant per-entry penalty $s_O$. Define $S = \\dfrac{1}{N}\\sum_{i=1}^{N} s_i$. You are given aggregated counts sufficient to compute $S$: among X-ray entries, $G$ have $r_i \\le r_0$ and $B$ have $r_i > r_0$, and the mean resolution among the latter is $\\bar{r}_B$. Non–X-ray entries are $N_O$ in number, each with penalty $s_O$.\n\nLet the overall energy function be\n$$\nE_{PDB} \\;=\\; w_r R \\;+\\; w_c C \\;+\\; w_m M \\;+\\; w_s S,\n$$\nwith nonnegative weights $w_r, w_c, w_m, w_s$.\n\nUsing the following dataset-level parameters, compute $E_{PDB}$:\n- Total entries $N = 200000$, distinct UniProt mappings $U = 150000$.\n- Entries with SCOPe and CATH disagreement $X = 5000$.\n- Total missing metadata field instances $T = 60000$ across the four required fields.\n- Resolution threshold $r_0 = 2.5$; X-ray counts $G = 120000$ with $r_i \\le r_0$ and $B = 40000$ with $r_i > r_0$; among the latter, mean resolution $\\bar{r}_B = 3.1$.\n- Non–X-ray entries $N_O = 40000$ with constant penalty $s_O = 0.15$.\n- Weights $w_r = 0.4$, $w_c = 0.2$, $w_m = 0.2$, $w_s = 0.2$.\n\nExpress the final energy as a single dimensionless decimal number. Round your answer to four significant figures.",
            "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded in the principles of bioinformatics and database architecture, quantitatively well-posed with sufficient and consistent data, and objectively formulated. There are no violations of scientific principles, logical inconsistencies, or ambiguities that would prevent a rigorous, unique solution. The model, though a simplification, is a plausible framework for assessing database quality. We will therefore proceed with the calculation.\n\nThe problem requires the computation of a dimensionless \"energy function\" $E_{PDB}$ for the Protein Data Bank (PDB), defined as a weighted sum of four penalty terms:\n$$\nE_{PDB} = w_r R + w_c C + w_m M + w_s S\n$$\nThe weights are given as $w_r = 0.4$, $w_c = 0.2$, $w_m = 0.2$, and $w_s = 0.2$. Their sum is $0.4 + 0.2 + 0.2 + 0.2 = 1.0$, which is consistent. We must calculate each penalty term, $R$, $C$, $M$, and $S$, using the provided data.\n\n1.  **Redundancy Penalty, $R$**\n    This penalty is defined as $R = \\dfrac{N - U}{N}$, where $N$ is the total number of entries and $U$ is the number of distinct UniProt mappings.\n    The given data are $N = 200000$ and $U = 150000$.\n    Substituting these values:\n    $$\n    R = \\frac{200000 - 150000}{200000} = \\frac{50000}{200000} = \\frac{1}{4} = 0.25\n    $$\n\n2.  **Cross-reference Inconsistency Penalty, $C$**\n    This penalty is defined as $C = \\dfrac{X}{N}$, where $X$ is the number of entries with disagreeing SCOPe and CATH classifications.\n    The given data are $X = 5000$ and $N = 200000$.\n    Substituting these values:\n    $$\n    C = \\frac{5000}{200000} = \\frac{5}{200} = \\frac{1}{40} = 0.025\n    $$\n\n3.  **Missing Metadata Penalty, $M$**\n    This penalty is defined as $M = \\dfrac{T}{4N}$, where $T$ is the total count of missing instances across $4$ required metadata fields for all $N$ entries.\n    The given data are $T = 60000$ and $N = 200000$.\n    Substituting these values:\n    $$\n    M = \\frac{60000}{4 \\times 200000} = \\frac{60000}{800000} = \\frac{6}{80} = \\frac{3}{40} = 0.075\n    $$\n\n4.  **Structural Resolution Penalty, $S$**\n    This penalty is an average of per-entry penalties, $S = \\dfrac{1}{N}\\sum_{i=1}^{N} s_i$. The total population of $N = 200000$ entries is partitioned into three groups for this calculation:\n    -   Group $1$: $G = 120000$ X-ray entries with resolution $r_i \\le r_0$, where $r_0 = 2.5$ Angstroms. For these entries, the penalty is $s_i = \\max(0, \\frac{r_i - r_0}{r_0}) = 0$, as $\\frac{r_i - r_0}{r_0} \\le 0$. The total penalty from this group is $0$.\n    -   Group $2$: $B = 40000$ X-ray entries with resolution $r_i > r_0$. The penalty is $s_i = \\frac{r_i - r_0}{r_0}$. The total penalty for this group is $\\sum_{j=1}^{B} s_j$. Given the mean resolution for this group is $\\bar{r}_B = 3.1$, we can write the sum as:\n        $$\n        \\sum_{j=1}^{B} \\frac{r_j - r_0}{r_0} = \\frac{1}{r_0} \\left( \\left( \\sum_{j=1}^{B} r_j \\right) - B r_0 \\right) = \\frac{1}{r_0} (B \\bar{r}_B - B r_0) = \\frac{B(\\bar{r}_B - r_0)}{r_0}\n        $$\n        Substituting the values $B = 40000$, $\\bar{r}_B = 3.1$, and $r_0 = 2.5$:\n        $$\n        \\text{Sum for Group 2} = \\frac{40000(3.1 - 2.5)}{2.5} = \\frac{40000 \\times 0.6}{2.5} = \\frac{24000}{2.5} = 9600\n        $$\n    -   Group $3$: $N_O = 40000$ non–X-ray entries, each assigned a constant penalty $s_O = 0.15$. The total penalty for this group is:\n        $$\n        N_O s_O = 40000 \\times 0.15 = 6000\n        $$\n    The total sum of penalties across all $N$ entries is the sum of contributions from these three groups: $\\sum_{i=1}^{N} s_i = 0 + 9600 + 6000 = 15600$.\n    The average penalty $S$ is therefore:\n    $$\n    S = \\frac{1}{N} \\sum_{i=1}^{N} s_i = \\frac{15600}{200000} = \\frac{156}{2000} = \\frac{78}{1000} = 0.078\n    $$\n    The number of entries sums correctly: $G + B + N_O = 120000 + 40000 + 40000 = 200000 = N$.\n\n5.  **Final Calculation of $E_{PDB}$**\n    Now we substitute all calculated penalty values and the given weights into the energy function expression:\n    $$\n    E_{PDB} = w_r R + w_c C + w_m M + w_s S\n    $$\n    $$\n    E_{PDB} = (0.4)(0.25) + (0.2)(0.025) + (0.2)(0.075) + (0.2)(0.078)\n    $$\n    $$\n    E_{PDB} = 0.1000 + 0.0050 + 0.0150 + 0.0156\n    $$\n    $$\n    E_{PDB} = 0.1356\n    $$\nThe problem requires the answer to be rounded to four significant figures. The calculated value $0.1356$ already has exactly four significant figures. Thus, no further rounding is necessary.",
            "answer": "$$\n\\boxed{0.1356}\n$$"
        },
        {
            "introduction": "Scientific knowledge is not static, and neither are the databases that store it. This practice  shifts our focus to the temporal dynamics of data, exploring how quickly information is updated or revised. By modeling annotation updates as a stochastic process, you will define and calculate the \"annotation half-life\" of a database entry, providing a powerful metric to understand the currency and stability of biological data over time.",
            "id": "2373028",
            "problem": "In the architecture of biological databases, a primary database stores archival records derived directly from experiments, whereas a secondary database integrates, derives, or predicts features by aggregating and analyzing content from primary sources. Consider a single entry that contains two categories of annotations: a fraction $f_{\\mathrm{pri}}$ from the primary layer and a fraction $f_{\\mathrm{sec}} = 1 - f_{\\mathrm{pri}}$ from the secondary layer. Model each annotation’s first update or revision time as an independent arrival in a homogeneous Poisson process with constant hazard rate. Specifically, each primary-layer annotation updates with rate $\\lambda_{\\mathrm{pri}}$ (in $\\mathrm{y}^{-1}$) and each secondary-layer annotation updates with rate $\\lambda_{\\mathrm{sec}}$ (in $\\mathrm{y}^{-1}$). Assume the entry is sufficiently large that the expected fraction of updated annotations equals the observed fraction at any time $t$.\n\nDefine the annotation half-life $T_{1/2}$ of the entry as the smallest $t \\ge 0$ such that the expected proportion of its initial annotations that have been updated or revised by time $t$ equals $0.5$.\n\nGiven $f_{\\mathrm{pri}} = 0.6$, $\\lambda_{\\mathrm{pri}} = 1.1\\ \\mathrm{y}^{-1}$, and $\\lambda_{\\mathrm{sec}} = 0.35\\ \\mathrm{y}^{-1}$, compute $T_{1/2}$. Round your answer to $4$ significant figures. Express the final time in years.",
            "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- A primary database stores archival records from experiments.\n- A secondary database integrates, derives, or predicts features from primary sources.\n- An entry contains primary and secondary annotations.\n- Fraction of primary annotations: $f_{\\mathrm{pri}}$.\n- Fraction of secondary annotations: $f_{\\mathrm{sec}} = 1 - f_{\\mathrm{pri}}$.\n- The first update or revision time for each annotation is modeled as an independent arrival in a homogeneous Poisson process.\n- Constant hazard rate for primary-layer annotation updates: $\\lambda_{\\mathrm{pri}}$ (in $\\mathrm{y}^{-1}$).\n- Constant hazard rate for secondary-layer annotation updates: $\\lambda_{\\mathrm{sec}}$ (in $\\mathrm{y}^{-1}$).\n- Assumption: The entry is sufficiently large so that the expected fraction of updated annotations equals the observed fraction at any time $t$.\n- Definition of annotation half-life $T_{1/2}$: The smallest time $t \\ge 0$ such that the expected proportion of its initial annotations that have been updated or revised by time $t$ equals $0.5$.\n- Given values: $f_{\\mathrm{pri}} = 0.6$, $\\lambda_{\\mathrm{pri}} = 1.1\\ \\mathrm{y}^{-1}$, and $\\lambda_{\\mathrm{sec}} = 0.35\\ \\mathrm{y}^{-1}$.\n- Required output: Compute $T_{1/2}$ rounded to $4$ significant figures, in units of years.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded. The model uses standard concepts from survival analysis and stochastic processes (Poisson process, exponential distribution) which are commonly applied in fields like bioinformatics to model events over time. The distinction between primary and secondary databases is a fundamental concept in biological data management. The problem is well-posed, providing all necessary definitions, constants, and a clear objective. The language is objective and precise. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be provided.\n\nThe time to the first event in a homogeneous Poisson process with a constant rate $\\lambda$ is an exponentially distributed random variable. Let $T$ be the time until the first update of a single annotation. The probability that this annotation has not been updated by time $t$ is given by the survival function $S(t) = P(T > t) = \\exp(-\\lambda t)$. Consequently, the probability that the annotation has been updated by time $t$ is given by the cumulative distribution function (CDF), $F(t) = P(T \\le t) = 1 - S(t) = 1 - \\exp(-\\lambda t)$.\n\nWe apply this model to the two categories of annotations present in the database entry.\nFor a single primary-layer annotation, the probability that it has been updated by time $t$ is:\n$$P_{\\mathrm{pri, upd}}(t) = 1 - \\exp(-\\lambda_{\\mathrm{pri}} t)$$\nFor a single secondary-layer annotation, the probability that it has been updated by time $t$ is:\n$$P_{\\mathrm{sec, upd}}(t) = 1 - \\exp(-\\lambda_{\\mathrm{sec}} t)$$\n\nThe problem states that the database entry is large enough to equate the expected fraction of updated annotations with the observed fraction. This allows us to calculate the overall expected proportion of updated annotations, denoted $U(t)$, as the weighted average of the update probabilities for the two categories. The weights are their respective fractions, $f_{\\mathrm{pri}}$ and $f_{\\mathrm{sec}}$.\n$$U(t) = f_{\\mathrm{pri}} P_{\\mathrm{pri, upd}}(t) + f_{\\mathrm{sec}} P_{\\mathrm{sec, upd}}(t)$$\nSubstituting the expressions for the probabilities, we get:\n$$U(t) = f_{\\mathrm{pri}} (1 - \\exp(-\\lambda_{\\mathrm{pri}} t)) + f_{\\mathrm{sec}} (1 - \\exp(-\\lambda_{\\mathrm{sec}} t))$$\n\nThe annotation half-life, $T_{1/2}$, is defined as the time at which the expected proportion of updated annotations equals $0.5$. We set $U(T_{1/2}) = 0.5$ and solve for $T_{1/2}$:\n$$0.5 = f_{\\mathrm{pri}} (1 - \\exp(-\\lambda_{\\mathrm{pri}} T_{1/2})) + f_{\\mathrm{sec}} (1 - \\exp(-\\lambda_{\\mathrm{sec}} T_{1/2}))$$\n\nWe are given the values $f_{\\mathrm{pri}} = 0.6$, $\\lambda_{\\mathrm{pri}} = 1.1\\ \\mathrm{y}^{-1}$, and $\\lambda_{\\mathrm{sec}} = 0.35\\ \\mathrm{y}^{-1}$. The fraction of secondary annotations is $f_{\\mathrm{sec}} = 1 - f_{\\mathrm{pri}} = 1 - 0.6 = 0.4$. Substituting these values into the equation:\n$$0.5 = 0.6 (1 - \\exp(-1.1 T_{1/2})) + 0.4 (1 - \\exp(-0.35 T_{1/2}))$$\nLet us simplify this expression.\n$$0.5 = 0.6 - 0.6 \\exp(-1.1 T_{1/2}) + 0.4 - 0.4 \\exp(-0.35 T_{1/2})$$\n$$0.5 = (0.6 + 0.4) - (0.6 \\exp(-1.1 T_{1/2}) + 0.4 \\exp(-0.35 T_{1/2}))$$\n$$0.5 = 1 - (0.6 \\exp(-1.1 T_{1/2}) + 0.4 \\exp(-0.35 T_{1/2}))$$\nRearranging the terms, we arrive at the equation to be solved for $T_{1/2}$:\n$$0.6 \\exp(-1.1 T_{1/2}) + 0.4 \\exp(-0.35 T_{1/2}) = 0.5$$\n\nThis is a transcendental equation and does not possess a straightforward analytical solution for $T_{1/2}$. We must employ a numerical method to find the root. Let us define a function $g(t) = 0.6 \\exp(-1.1 t) + 0.4 \\exp(-0.35 t) - 0.5$. We seek the value of $t > 0$ for which $g(t) = 0$.\n\nThe function $g(t)$ is continuous and monotonically decreasing for $t \\ge 0$, since its derivative $g'(t) = -0.66 \\exp(-1.1 t) - 0.14 \\exp(-0.35 t)$ is always negative. Furthermore, $g(0) = 0.6 + 0.4 - 0.5 = 0.5$ and $\\lim_{t \\to \\infty} g(t) = -0.5$. By the Intermediate Value Theorem, a unique positive root for $g(t) = 0$ must exist.\n\nUsing a numerical solver (such as the Newton-Raphson method or bisection method), we find the root of the equation.\nThe solution is found to be approximately $T_{1/2} \\approx 0.94363$ years.\nThe problem requires the answer to be rounded to $4$ significant figures.\n$$T_{1/2} \\approx 0.9436\\ \\mathrm{years}$$\nThis is the annotation half-life of the entry under the given model and parameters.",
            "answer": "$$\\boxed{0.9436}$$"
        }
    ]
}