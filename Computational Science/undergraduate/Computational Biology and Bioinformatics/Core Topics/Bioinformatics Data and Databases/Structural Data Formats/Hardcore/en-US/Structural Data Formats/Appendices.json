{
    "hands_on_practices": [
        {
            "introduction": "Bioinformatics often begins with the challenge of parsing complex, semi-structured data into a format suitable for analysis. This first practice  immerses you in this foundational task by requiring you to \"flatten\" a GenBank flatfile, a cornerstone format for sequence data. By writing a parser that interprets feature locations, including complex `join()` and `complement()` constructs, you will develop the critical ability to transform raw biological data into a structured table, making it ready for computational inquiry.",
            "id": "2431179",
            "problem": "Write a complete program that reads an in-memory multi-entry National Center for Biotechnology Information (NCBI) GenBank flatfile and produces a logical flattening into a Tab-Separated Values (TSV) representation, where each row corresponds to a single biological feature. Instead of writing files, for each input in a provided test suite, compute a set of summary integers derived deterministically from the TSV rows and aggregate them into a single-line output as specified below.\n\nFormal specification of records and features:\n- A record begins at a line whose first token is the literal string \"LOCUS\" and ends at a line that is exactly \"//\".\n- The accession identifier for a record is the first non-space token that appears on the line whose first token is \"ACCESSION\". Use that token unchanged.\n- The features block of a record begins at the line whose first token is \"FEATURES\" and ends strictly before the line whose first token is \"ORIGIN\" or the record terminator line \"//\", whichever appears first.\n- Feature entries appear within the features block. A feature entry consists of:\n  - A feature key (for example, \"source\", \"gene\", \"CDS\", \"mRNA\", \"misc_feature\", \"repeat_region\") followed by a location expression. The first non-space token on a line not starting with \"/\" is interpreted as the feature key, and the remainder of that line, together with any immediately following lines of the features block that are not starting with \"/\" and are visually indented more deeply than the key line, together form the location expression for that feature. Qualifiers (lines beginning with \"/\") and their continuations are present but must be ignored for the purpose of computing the requested summary integers in this task. Lines within the features block visually indented more deeply than the key line that start with \"/\" are qualifiers and do not contribute to the location. Qualifier continuations are lines within the features block that are visually indented more deeply than the key line and do not start with \"/\".\n  - For this problem, treat all feature keys as valid and include all features in the TSV if and only if their location expression yields at least one on-sequence interval under the rules below.\n\nLocation expression interpretation rules:\n- The location expression is to be parsed over the alphabet consisting of letters, digits, parentheses, commas, colons, dots, carets, less-than and greater-than symbols. All whitespace in the location expression is insignificant and is to be ignored during interpretation.\n- The base coordinate system is one-based and inclusive. If a location denotes a single base position, it contributes an interval whose start and end are that base. If a location denotes a closed range with two base positions separated by two dots, it contributes an interval whose inclusive endpoints are those two bases.\n- The only location constructs to support are:\n  - The symbol \"complement(\" applied to a balanced parenthetic subexpression.\n  - The symbols \"join(\" or \"order(\" applied to a comma-separated list of balanced subexpressions.\n  - Single-base coordinates and closed ranges with the potential presence of the characters \"&lt;\" or \"&gt;\" immediately preceding a base coordinate; these characters indicate fuzziness and must be ignored when extracting the numeric value.\n  - Remote intervals indicated by a prefix of the form \"X:\" immediately before a single base or closed range, where \"X\" is any non-empty sequence of non-colon characters. Remote intervals must be completely ignored and treated as contributing no on-sequence interval.\n- The overall strand to report is determined exclusively by whether the outermost syntactic form of the entire location expression is a single \"complement(\" wrapper whose matching closing parenthesis is the final character of the expression. If and only if that condition holds, report the strand as the single character \"-\"; otherwise report the strand as the single character \"+\".\n- For purposes of this problem, do not consider or attempt to detect mixed-orientation substructures; the only required strand rule is the one stated above.\n- When a location expression contains \"join(\" or \"order(\", compute the feature’s reported start as the minimum numeric start among all non-remote intervals contained anywhere within the expression (recursively), and compute the feature’s reported end as the maximum numeric end among all such intervals. If after ignoring all remote intervals there are no remaining on-sequence intervals, the feature must be omitted from the TSV.\n- Let the reported inclusive length of a TSV row be defined by the formula $L = E - S + 1$, where $S$ is the reported start and $E$ is the reported end of that row.\n\nTSV logical schema to be used for this problem:\n- Each TSV row comprises the following fields in order: accession, feature key, start, end, strand. The accession is the record accession string as defined above. The start and end are decimal integers according to the interpretation rules above. The strand is the single character \"+\" or \"-\".\n\nRequired computed outputs for each input in the test suite:\n- For each input, after constructing the TSV rows by applying the rules above across all records in that input, compute and return the list $[T, C, \\Sigma, M]$, where:\n  - $T$ is the total number of TSV rows constructed (an integer).\n  - $C$ is the number of TSV rows whose feature key string is exactly \"CDS\" (an integer).\n  - $\\Sigma$ is the sum over all TSV rows of the inclusive lengths $L$ (an integer).\n  - $M$ is the number of TSV rows whose strand is \"-\" (an integer).\n\nTest suite:\n- Your program must compute results for the following three inputs. Each input is a complete multi-entry GenBank flatfile content in plain text. The inputs are specified here by their records and features, and your program must embed corresponding strings that satisfy the described structure.\n\n  Input $1$ (two records):\n  - Record with accession \"REC1\" and the following features and locations:\n    - source: $1..100$\n    - gene: $10..20$\n    - CDS: $complement(10..20)$\n    - mRNA: $join(10..12,15..20)$\n  - Record with accession \"REC2\" and the following features and locations:\n    - source: $1..200$\n    - gene: $join(50..60,80..90)$\n    - CDS: $join(50..60,80..90)$\n\n  Input $2$ (one record):\n  - Record with accession \"REC3\" and the following features and locations:\n    - source: $1..1000$\n    - gene: $&lt;5..&gt;10$ (treat as $5..10$ for numeric purposes)\n    - misc_feature: $100$\n    - CDS: $join(200..210,RECX:300..310)$ (ignore the remote interval $RECX:300..310$)\n    - repeat_region: $complement(500)$\n    - misc_feature: $join(700..705,710..715,720..725)$\n\n  Input $3$ (one record):\n  - Record with accession \"REC4\" and the following features and locations:\n    - source: $1..300$\n    - gene: $complement(join(50..60,70..80))$\n    - CDS: $complement(join(50..60,70..80))$\n    - misc_feature: $join(RECZ:10..20,RECY:30..40)$ (remote-only; omit entirely)\n    - gene: $250..260$\n\nFinal output format:\n- Your program must produce a single line of output containing a list of three lists, one per input, in input order. For each input, output the list $[T,C,\\Sigma,M]$ defined above. The single line must be exactly a comma-separated list enclosed in square brackets, with each inner list also enclosed in square brackets and with no spaces, for example: \"[[1,2,3,4],[5,6,7,8],[9,10,11,12]]\".",
            "solution": "The problem proposed is a well-defined task in computational biology, specifically in the domain of bioinformatics data parsing. It requires the implementation of a parser for a simplified, yet representative, subset of the GenBank flatfile format. The objective is to extract structured information from semi-structured text data, a common and fundamental challenge in the field.\n\nThe problem statement is validated as follows:\n\n**Step 1: Extract Givens**\n\n*   **Record Structure**: A record begins with a line starting with `LOCUS` and ends with a line containing exactly `//`.\n*   **Accession ID**: The first non-space token on the `ACCESSION` line.\n*   **Features Block**: Begins at the `FEATURES` line and ends just before `ORIGIN` or `//`.\n*   **Feature Entry**: Consists of a feature key and a location expression. The key is the first non-space token on a non-qualifier line. The location is the rest of that line, plus any immediately following, more deeply indented lines that do not begin with `/`.\n*   **Qualifiers**: Lines starting with `/` and their continuations are to be ignored.\n*   **Location Expression Parsing**:\n    *   Whitespace is insignificant.\n    *   Coordinates are $1$-based and inclusive. A single number $N$ represents the interval $[N, N]$.\n    *   Supported constructs: `complement(...)`, `join(...)`, `order(...)`, single numbers, ranges `N..M`, fuzzy markers `<` and `>` (to be ignored), and remote intervals `X:...` (to be ignored).\n*   **Strand Rule**: The strand is `-` if and only if the *entire* location string is wrapped in `complement(...)`. Otherwise, it is `+`.\n*   **Start/End for `join`/`order`**: For features with `join` or `order`, the start is the minimum of all start coordinates from non-remote sub-intervals, and the end is the maximum of all end coordinates from the same.\n*   **Omission Rule**: Features with no on-sequence intervals (after ignoring remote ones) are excluded from the output.\n*   **TSV Schema**: `(accession, feature_key, start, end, strand)`.\n*   **Length Definition**: The length $L$ of a feature is $E - S + 1$, where $S$ is the start and $E$ is the end.\n*   **Summary Outputs**: For each input file, a list $[T, C, \\Sigma, M]$ must be computed, where:\n    *   $T$: Total count of TSV rows.\n    *   $C$: Count of rows where the feature key is `CDS`.\n    *   $\\Sigma$: Sum of all lengths $L$.\n    *   $M$: Count of rows where the strand is `-`.\n*   **Test Suite**: Three specific multi-record/multi-feature inputs are provided.\n*   **Final Output Format**: A single line representing a list of lists, e.g., `[[T1,C1,S1,M1],[T2,C2,S2,M2],...]`.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n\n*   **Scientifically Grounded**: The problem is an accurate, if simplified, representation of parsing GenBank files, a standard task in bioinformatics. The concepts of accessions, features, location strings, and strands are fundamental to molecular biology and genomics. It is scientifically sound.\n*   **Well-Posed**: The problem is well-posed. The rules for parsing, though simplified, are explicit and unambiguous. For instance, the strand determination is reduced to a simple syntactic check, and the handling of `join`/`order` is precisely defined as taking the minimum start and maximum end. This ensures a unique, deterministic solution can be derived.\n*   **Objective**: The problem is formulated with objective, technical language, free from subjectivity or ambiguity.\n\nThe problem does not exhibit any invalidating flaws. It is not based on false premises, is formalizable, is self-contained, requires feasible computations, and is well-structured.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. A solution will be constructed.\n\nThe overall algorithm will process each input string by first segmenting it into individual GenBank records. For each record, it will extract the accession number and then isolate the features block. A dedicated parser for the features block will iterate through its lines, identifying each feature key and its complete, potentially multi-line, location string, correctly handling indentation as per the rules.\n\nThe core of the solution is a recursive parser for the location string. This parser will be designed to handle the specified constructs: `complement()`, `join()`, and `order()`.\n\n1.  A function `_get_intervals` will recursively descend through the location string structure. It will decompose `join` and `order` constructs by splitting their arguments at top-level commas (i.e., commas not enclosed in parentheses). It will strip `complement` wrappers.\n2.  The recursion terminates at base cases: simple coordinates ($N$) or ranges ($N..M$). Remote intervals (containing `:`) are identified and yield no coordinates. Fuzzy markers (`<`, `>`) are removed before numeric conversion.\n3.  This recursive function will return a flat list of all elementary, non-remote `(start, end)` coordinate pairs found within the location string.\n4.  A higher-level function, `parse_location`, will orchestrate this process. First, it determines the strand based on the top-level structure of the raw location string. Then, it calls `_get_intervals`. If the returned list of intervals is empty, the feature is discarded. Otherwise, it calculates the feature's final start as the minimum of all start coordinates and the final end as the maximum of all end coordinates.\n5.  After processing all features from all records in an input, the summary statistics $T$, $C$, $\\Sigma$, and $M$ are calculated from the collected set of valid TSV rows. This process is repeated for each input in the test suite, and the results are aggregated into the specified final format.\n\nLet us trace an example: `complement(join(50..60,70..80))`.\n1.  `parse_location` is called with this string. It sees the `complement(...)` wrapper and sets strand to `-`.\n2.  It then calls `_get_intervals` on the inner part, `join(50..60,70..80)`.\n3.  `_get_intervals` sees `join(...)` and calls itself on the comma-separated parts: `50..60` and `70..80`.\n4.  For `50..60`, the base case is reached, returning `[(50, 60)]`.\n5.  For `70..80`, the base case is reached, returning `[(70, 80)]`.\n6.  The `join` level combines these into `[(50, 60), (70, 80)]`.\n7.  `parse_location` receives this list. It is not empty. It computes start = $\\min(50, 70) = 50$ and end = $\\max(60, 80) = 80$.\n8.  The final parsed feature data is `(start=50, end=80, strand='-')`. The length is $80 - 50 + 1 = 31$.\n\nThis structured, recursive approach correctly implements the specified logic for all cases. The implementation will be encapsulated within a Python program adhering to the specified constraints.",
            "answer": "```python\nimport re\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print the final result.\n    \"\"\"\n    # Construct GenBank flatfile strings for the test suite.\n    # The indentation is crucial for correct parsing based on problem rules.\n    input1 = \"\"\"LOCUS       REC1   100 bp    DNA     linear   SYN 01-JAN-2023\nACCESSION   REC1\nFEATURES             Location/Qualifiers\n     source          1..100\n     gene            10..20\n     CDS             complement(10..20)\n     mRNA            join(10..12,15..20)\n//\nLOCUS       REC2   200 bp    DNA     linear   SYN 01-JAN-2023\nACCESSION   REC2\nFEATURES             Location/Qualifiers\n     source          1..200\n     gene            join(50..60,80..90)\n     CDS             join(50..60,80..90)\n//\n\"\"\"\n\n    input2 = \"\"\"LOCUS       REC3   1000 bp   DNA     linear   SYN 01-JAN-2023\nACCESSION   REC3\nFEATURES             Location/Qualifiers\n     source          1..1000\n     gene            <5..>10\n     misc_feature    100\n     CDS             join(200..210,RECX:300..310)\n     repeat_region   complement(500)\n     misc_feature    join(700..705,710..715,720..725)\n//\n\"\"\"\n\n    input3 = \"\"\"LOCUS       REC4   300 bp    DNA     linear   SYN 01-JAN-2023\nACCESSION   REC4\nFEATURES             Location/Qualifiers\n     source          1..300\n     gene            complement(join(50..60,70..80))\n     CDS             complement(join(50..60,70..80))\n     misc_feature    join(RECZ:10..20,RECY:30..40)\n     gene            250..260\n//\n\"\"\"\n    test_cases = [input1, input2, input3]\n    all_results = [process_gbk_content(case) for case in test_cases]\n    \n    # Format the final output string as specified.\n    result_str = \",\".join(map(str, all_results))\n    print(f\"[{result_str}]\".replace(\" \", \"\"))\n\ndef _split_on_top_level_comma(text):\n    \"\"\"\n    Splits a string by commas that are not inside parentheses.\n    Example: \"join(a,b),c\" -> [\"join(a,b)\", \"c\"]\n    \"\"\"\n    parts = []\n    balance = 0\n    start_index = 0\n    for i, char in enumerate(text):\n        if char == '(':\n            balance += 1\n        elif char == ')':\n            balance -= 1\n        elif char == ',' and balance == 0:\n            parts.append(text[start_index:i])\n            start_index = i + 1\n    parts.append(text[start_index:])\n    return parts\n\ndef _get_intervals_recursive(loc_str):\n    \"\"\"\n    Recursively parses a location string to find all elementary, non-remote intervals.\n    Returns a list of (start, end) tuples.\n    \"\"\"\n    # Normalize by removing all whitespace\n    loc_str = \"\".join(loc_str.split())\n\n    # Handle complement(), join(), order() recursively\n    if (m := re.fullmatch(r\"complement\\((.*)\\)\", loc_str)):\n        return _get_intervals_recursive(m.group(1))\n    \n    if (m := re.fullmatch(r\"(?:join|order)\\((.*)\\)\", loc_str)):\n        sub_locs_str = m.group(1)\n        sub_locs = _split_on_top_level_comma(sub_locs_str)\n        intervals = []\n        for sub in sub_locs:\n            intervals.extend(_get_intervals_recursive(sub))\n        return intervals\n\n    # Base case: handle simple coordinates, ranges, and remote intervals\n    if ':' in loc_str:  # Ignore remote intervals\n        return []\n\n    # Ignore fuzziness markers\n    loc_str = loc_str.replace('<', '').replace('>', '')\n\n    # Find all numbers in the simple location string\n    nums = [int(n) for n in re.findall(r\"\\d+\", loc_str)]\n\n    if len(nums) == 1:\n        return [(nums[0], nums[0])]\n    elif len(nums) == 2:\n        return [(min(nums), max(nums))]\n    \n    return []\n\ndef parse_location_string(raw_location_string):\n    \"\"\"\n    Parses a complete location string to determine start, end, and strand.\n    Returns a tuple (start, end, strand) or None if the feature has no on-sequence parts.\n    \"\"\"\n    clean_loc = raw_location_string.strip()\n    \n    # Determine strand based on the outermost structure\n    strand = '-' if clean_loc.startswith('complement(') and clean_loc.endswith(')') else '+'\n    \n    intervals = _get_intervals_recursive(raw_location_string)\n\n    if not intervals:\n        return None\n\n    starts = [i[0] for i in intervals]\n    ends = [i[1] for i in intervals]\n    \n    min_start = min(starts)\n    max_end = max(ends)\n    \n    return min_start, max_end, strand\n\ndef parse_features_block(feature_lines):\n    \"\"\"\n    Parses the lines of a FEATURES block into a list of key-location dictionaries.\n    \"\"\"\n    features = []\n    i = 0\n    while i < len(feature_lines):\n        line = feature_lines[i]\n        stripped_line = line.lstrip()\n        \n        # Skip empty lines and qualifier lines\n        if not stripped_line or stripped_line.startswith('/'):\n            i += 1\n            continue\n\n        # Found a new feature line\n        key_line_indent = len(line) - len(stripped_line)\n        tokens = stripped_line.split(None, 1)\n        key = tokens[0]\n        location = tokens[1] if len(tokens) > 1 else \"\"\n\n        # Greedily consume subsequent continuation lines for the location\n        j = i + 1\n        while j < len(feature_lines):\n            next_line = feature_lines[j]\n            next_stripped = next_line.lstrip()\n            next_indent = len(next_line) - len(next_stripped)\n            \n            # A location continuation must be more indented and not a qualifier\n            if next_indent > key_line_indent and next_stripped and not next_stripped.startswith('/'):\n                location += next_stripped # Whitespace is ignored in location string later\n                j += 1\n            else:\n                break\n        \n        features.append({'key': key, 'location': location})\n        i = j  # Continue parsing from the line after the consumed block\n    return features\n\n\ndef process_gbk_content(gbk_content):\n    \"\"\"\n    Processes a complete GenBank file content string and computes summary statistics.\n    Returns the list [T, C, Sigma, M].\n    \"\"\"\n    tsv_rows = []\n    \n    # Split content into records\n    # The trailing `\\n` in `//\\n` is important for splitting multi-record files.\n    records_text = filter(None, gbk_content.split('//\\n'))\n\n    for record_text in records_text:\n        lines = record_text.splitlines()\n        accession = \"\"\n        in_features = False\n        feature_lines = []\n\n        for line in lines:\n            if line.startswith('ACCESSION'):\n                accession = line.split()[1]\n            elif line.startswith('FEATURES'):\n                in_features = True\n            elif line.startswith('ORIGIN') or line.startswith('//'):\n                in_features = False\n            elif in_features:\n                feature_lines.append(line)\n        \n        parsed_features = parse_features_block(feature_lines)\n\n        for feature in parsed_features:\n            key = feature['key']\n            location_str = feature['location']\n            \n            parsed_loc = parse_location_string(location_str)\n            if parsed_loc:\n                start, end, strand = parsed_loc\n                length = end - start + 1\n                tsv_rows.append({\n                    'key': key,\n                    'strand': strand,\n                    'length': length\n                })\n\n    T = len(tsv_rows)\n    C = sum(1 for row in tsv_rows if row['key'] == 'CDS')\n    Sigma = sum(row['length'] for row in tsv_rows)\n    M = sum(1 for row in tsv_rows if row['strand'] == '-')\n    \n    return [T, C, Sigma, M]\n\n# The script is designed to be executed directly.\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A correctly formatted data file is the bedrock of reproducible science, and structural data is no exception. This exercise  moves beyond simple data extraction to focus on semantic validation within the Protein Data Bank (PDB) format. You will implement a validator to detect anomalous `TER` records—chain termination markers that fail to separate distinct protein chains—a subtle but critical formatting error that can corrupt structural analysis. This practice sharpens your attention to the logical integrity of data files, ensuring the reliability of your scientific inputs.",
            "id": "2431238",
            "problem": "You are given the task of validating the structural integrity of Protein Data Bank (PDB) formatted records with respect to chain termination markers. The Protein Data Bank (PDB) flatfile is a fixed-column format in which records such as ATOM and HETATM specify atoms that belong to residues within a polymer chain, and the TER record denotes the end of a chain. The fundamental and well-tested facts you must use are: (i) the PDB format is fixed-width, and the chain identifier is stored at column $22$ (one character, using $1$-based indexing), (ii) the TER record designates a chain terminus such that the nearest ATOM or HETATM record preceding the TER belongs to the terminated chain, and the nearest ATOM or HETATM record following the TER must belong to a different chain if the TER is correctly separating chains, and (iii) the ATOM and HETATM records carry the chain identifier at column $22$ just like TER.\n\nA formatting anomaly of interest occurs when a TER record does not separate different chain identifiers. Formally, let the chain identifier of the nearest preceding ATOM or HETATM record be $c_{\\text{prev}}$ and that of the nearest succeeding ATOM or HETATM record be $c_{\\text{next}}$. A TER record is anomalous if and only if both $c_{\\text{prev}}$ and $c_{\\text{next}}$ exist and $c_{\\text{prev}} = c_{\\text{next}}$. If either side is missing (for example, a TER at the beginning of the file or at the end of the file), then there is no anomaly by this criterion.\n\nYour program must implement the following from first principles using these core definitions:\n\n- Parse each line and classify it as ATOM, HETATM, TER, or other. Only ATOM and HETATM contribute chain identifiers for defining $c_{\\text{prev}}$ and $c_{\\text{next}}$. Ignore other record types for this purpose.\n- Extract the chain identifier of ATOM, HETATM, and TER records from column $22$ (using $1$-based indexing). If a line is shorter than $22$ characters, treat its chain identifier as a single space character.\n- For each TER line, find the nearest preceding and nearest succeeding ATOM or HETATM lines. If both exist and their chain identifiers are equal, then the TER line is anomalous and you must report its $1$-based line index within that specific PDB text.\n\nDesign your algorithm so that it can operate on any modern programming language by relying only on the logical procedure described above and fixed-column extraction at column $22$.\n\nTest suite and required output:\n\nImplement and use the following five PDB test inputs. Each “record” listed here is a single line in the PDB text. The test inputs are defined abstractly as sequences of record types with their chain identifier in parentheses to ensure universal applicability; your implementation should construct concrete PDB-like lines that conform to the fixed-column requirement for chain identifiers. The required sequences are:\n\n- Test $1$ (happy path): ATOM(A), ATOM(A), TER, ATOM(B). This TER separates chain $A$ from chain $B$ and should produce no anomalies.\n- Test $2$ (single anomaly): ATOM(A), TER, ATOM(A). The TER does not separate different chains and is anomalous.\n- Test $3$ (mixed with HETATM and end-terminal TER): ATOM(A), ATOM(A), TER, HETATM(B), TER, ATOM(B), TER. Only the TER between HETATM(B) and ATOM(B) is anomalous since it does not separate different chains; the trailing TER at the end has no succeeding ATOM/HETATM and is not anomalous.\n- Test $4$ (blank chain identifier): ATOM(␠), TER, ATOM(␠), where ␠ denotes a single space. The TER does not separate different chains and is anomalous.\n- Test $5$ (consecutive TERs and edge positions): TER, ATOM(A), TER, TER, ATOM(B), TER, ATOM(B). Only the TER immediately preceding the final ATOM(B) is anomalous; TER at the start is not anomalous, and TERs that separate A from B are correct.\n\nFor each test input, your program must return the list of $1$-based line indices of anomalous TER records within that input. Aggregate the results for all five tests into a single line of output in the exact format:\n\n- A single line containing a list of five lists, one per test, with no spaces anywhere. For example, a valid output format looks like \"[[],[2],[5],[2],[6]]\" where each inner list contains integers in increasing order, and an empty list denotes no anomalies.\n\nNo physical units or angles are involved. All indices are $1$-based and must be integers.\n\nYour final deliverable must be a complete, runnable program that constructs concrete PDB-like lines for these abstract test definitions, applies the detection algorithm, and prints the aggregate results as a single line in the exact format described above. The program must not read from or write to any files or require any input.",
            "solution": "The problem as stated constitutes a valid and well-posed exercise in computational bioinformatics. It is scientifically grounded in the established, rigorous specification of the Protein Data Bank (PDB) file format, an objective standard in structural biology. The problem is self-contained, providing all necessary definitions and constraints, most notably a precise, formal definition of an anomalous $\\text{TER}$ record. The givens—that is, the fixed-column nature of the PDB format, the function of $\\text{ATOM}$, $\\text{HETATM}$, and $\\text{TER}$ records, and the location of the chain identifier at column $22$—are factually correct. The task is to implement a validation algorithm based on these first principles, which is a standard procedure in data-driven scientific fields. The problem formulation is unambiguous and free of scientific or logical contradictions, making it suitable for a rigorous algorithmic solution.\n\nThe objective is to identify anomalous $\\text{TER}$ records within a given PDB-formatted text. An anomaly is defined specifically: a $\\text{TER}$ record is anomalous if and only if the nearest preceding $\\text{ATOM}$ or $\\text{HETATM}$ record and the nearest succeeding $\\text{ATOM}$ or $\\text{HETATM}$ record both exist and share the same chain identifier. Let $c_{\\text{prev}}$ be the chain identifier of the nearest preceding such record and $c_{\\text{next}}$ be that of the nearest succeeding record. The condition for an anomaly at a given $\\text{TER}$ line is that both $c_{\\text{prev}}$ and $c_{\\text{next}}$ are defined and $c_{\\text{prev}} = c_{\\text{next}}$.\n\nA robust algorithm to solve this problem is designed in two primary stages, adhering to the principle of separation of concerns: data collation followed by rule-based validation.\n\n**Stage 1: Data Collation and Indexing**\n\nThe initial step involves a single pass through the input text to extract and index all records relevant to the validation logic. The input is a sequence of lines, and we process them one by one, keeping track of the $1$-based line index.\n\n1.  A list is compiled to store the contextual data provided by $\\text{ATOM}$ and $\\text{HETATM}$ records. For each such record encountered, a tuple containing its $1$-based line index and its chain identifier (extracted from column $22$) is stored. Let us call this the `atom_records` list.\n2.  A second list is compiled to store the $1$-based line indices of all $\\text{TER}$ records. These are the candidates for validation. Let us call this the `ter_indices` list.\n\nThis preprocessing stage effectively transforms the raw text into structured data, primed for the logical analysis in the next stage. The `atom_records` list will be naturally sorted by line index, a property that is critical for efficient searching.\n\n**Stage 2: Validation of TER Records**\n\nWith the structured data from Stage $1$, each $\\text{TER}$ record is validated against the defined anomaly condition. We iterate through each `ter_index` in the `ter_indices` list. For each `ter_index`, we must determine $c_{\\text{prev}}$ and $c_{\\text{next}}$.\n\n1.  **Finding $c_{\\text{prev}}$**: We search the `atom_records` list for the record with the maximum line index that is strictly less than the current `ter_index`. If no such record exists (e.g., if the $\\text{TER}$ record is the first record in the file, or is preceded only by other $\\text{TER}$ records), then $c_{\\text{prev}}$ is considered undefined.\n\n2.  **Finding $c_{\\text{next}}$**: We search the `atom_records` list for the record with the minimum line index that is strictly greater than the current `ter_index`. If no such record exists (e.g., the $\\text{TER}$ record is at the end of the file), then $c_{\\text{next}}$ is considered undefined.\n\n3.  **Applying the Anomaly Rule**: According to the problem definition, an anomaly exists if and only if both searches were successful (i.e., both $c_{\\text{prev}}$ and $c_{\\text{next}}$ are defined) and their chain identifiers are identical. If this condition is met, the `ter_index` is recorded as anomalous.\n\nThis procedure is repeated for all `ter_indices`. The final output for a given test case is the sorted list of all anomalous `ter_index` values.\n\nFor an efficient implementation of the search for preceding and succeeding records, the sorted nature of the `atom_records` list is exploited. Instead of a linear scan for each $\\text{TER}$ record, a binary search algorithm can be employed. The Python library `numpy`, specifically the function `np.searchsorted`, provides a highly optimized implementation of binary search. Given a `ter_index`, `np.searchsorted` can find the correct position for that index within the sorted array of `atom_records` line indices in $O(\\log N)$ time, where $N$ is the number of $\\text{ATOM}$/$\\text{HETATM}$ records. This allows for the immediate identification of the indices for the preceding and succeeding records, making the overall algorithm highly efficient.\n\nThis entire two-stage process is encapsulated into a function that is applied to each of the five specified test cases. The results are then aggregated and formatted into the required single-line string output.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the validation of PDB TER records for a suite of test cases.\n    It constructs the tests, processes each one, and prints the final aggregated result\n    in the specified compact format.\n    \"\"\"\n\n    def generate_pdb_line(rec_type, chain_id=' '):\n        \"\"\"\n        Generates a minimal, 22-character PDB-like line with the record type\n        and a chain identifier at column 22 (1-based index).\n\n        Args:\n            rec_type (str): The record type (e.g., 'ATOM', 'TER').\n            chain_id (str): The one-character chain identifier.\n\n        Returns:\n            str: A formatted PDB-like line.\n        \"\"\"\n        # The chain identifier is at column 22, which is index 21 (0-based).\n        # The format string ensures the record type is left-aligned in a 6-character field,\n        # followed by 15 spaces of padding, and then the chain ID.\n        return f\"{rec_type:<6}{' ' * 15}{chain_id}\"\n\n    def find_anomalous_ters(abstract_records):\n        \"\"\"\n        Analyzes a sequence of abstract PDB records to find anomalous TER records.\n\n        An anomalous TER is one where the nearest preceding and succeeding ATOM/HETATM\n        records exist and have the same chain identifier.\n\n        Args:\n            abstract_records (list of tuple): A list where each tuple is (record_type, chain_id).\n\n        Returns:\n            list of int: A sorted list of 1-based line indices of anomalous TER records.\n        \"\"\"\n        \n        # Stage 1: Data Collation and Indexing\n        atom_hetatm_records = []\n        ter_indices = []\n        \n        for i, (rec_type, chain_id) in enumerate(abstract_records, 1):\n            if rec_type in ['ATOM', 'HETATM']:\n                # The problem statement states to use a space for chain ID if the line is too short.\n                # Here we ensure the chain_id from the test definition is handled.\n                current_chain = chain_id if chain_id is not None else ' '\n                atom_hetatm_records.append({'index': i, 'chain': current_chain})\n            elif rec_type == 'TER':\n                ter_indices.append(i)\n\n        if not atom_hetatm_records or not ter_indices:\n            return []\n\n        # Convert to NumPy arrays for efficient searching\n        atom_indices = np.array([rec['index'] for rec in atom_hetatm_records])\n        atom_chains = np.array([rec['chain'] for rec in atom_hetatm_records])\n        \n        anomalies = []\n        \n        # Stage 2: Validation of TER Records\n        for ter_index in ter_indices:\n            # Find the insertion point for ter_index in the sorted atom_indices array.\n            # 'left' side gives the index of the first element >= ter_index.\n            # The record before this point is the nearest preceding one.\n            preceding_pos = np.searchsorted(atom_indices, ter_index, side='left')\n            \n            # 'right' side gives the index of the first element > ter_index.\n            # This is the nearest succeeding record.\n            succeeding_pos = np.searchsorted(atom_indices, ter_index, side='right')\n            \n            # Check edge cases as per the problem definition: an anomaly requires\n            # BOTH preceding and succeeding records to exist.\n            if preceding_pos == 0:  # No preceding ATOM/HETATM found.\n                continue\n            if succeeding_pos == len(atom_indices):  # No succeeding ATOM/HETATM found.\n                continue\n\n            # Get the chain identifiers for the preceding and succeeding records.\n            # The index into atom_chains for the preceding record is preceding_pos - 1.\n            prev_chain = atom_chains[preceding_pos - 1]\n            # The index for the succeeding record is simply succeeding_pos.\n            next_chain = atom_chains[succeeding_pos]\n            \n            # Apply the anomaly rule.\n            if prev_chain == next_chain:\n                anomalies.append(ter_index)\n        \n        return sorted(anomalies)\n\n    # Define the abstract test suites from the problem description.\n    # A None for chain_id indicates a TER record where chain ID is not specified.\n    test_cases = [\n        # Test 1: ATOM(A), ATOM(A), TER, ATOM(B)\n        [('ATOM', 'A'), ('ATOM', 'A'), ('TER', None), ('ATOM', 'B')],\n        # Test 2: ATOM(A), TER, ATOM(A)\n        [('ATOM', 'A'), ('TER', None), ('ATOM', 'A')],\n        # Test 3: ATOM(A), ATOM(A), TER, HETATM(B), TER, ATOM(B), TER\n        [('ATOM', 'A'), ('ATOM', 'A'), ('TER', None), ('HETATM', 'B'), ('TER', None), ('ATOM', 'B'), ('TER', None)],\n        # Test 4: ATOM( ), TER, ATOM( )\n        [('ATOM', ' '), ('TER', None), ('ATOM', ' ')],\n        # Test 5: TER, ATOM(A), TER, TER, ATOM(B), TER, ATOM(B)\n        [('TER', None), ('ATOM', 'A'), ('TER', None), ('TER', None), ('ATOM', 'B'), ('TER', None), ('ATOM', 'B')]\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = find_anomalous_ters(case)\n        all_results.append(result)\n\n    # Format the final output string exactly as specified: a list of lists with no spaces.\n    # The default string representation of a list includes spaces, which must be removed.\n    output_str = f\"[{','.join(map(str, all_results))}]\".replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate goal of mastering data formats is to answer meaningful scientific questions. This final practice  guides you through this transition from data parsing to structural analysis, focusing on the significant role of water in protein architecture. By parsing a PDB file to extract atomic coordinates and applying a geometric criterion based on a distance cutoff, $r$, you will identify water molecules that form bridges between different protein chains. This exercise exemplifies how a firm grasp of data structures enables the discovery of biophysically important interactions directly from raw structural data.",
            "id": "2431199",
            "problem": "You are given textual fragments that conform to the Protein Data Bank (PDB) flatfile format, each consisting of lines that begin with either \"ATOM\" or \"HETATM\". Each line encodes one atom. Interpret the fields of each atom line as follows, in order when splitting by whitespace: record name, atom serial number, atom name, residue name, chain identifier, residue sequence number, Cartesian coordinates $x$, $y$, $z$ in Angstroms, occupancy, temperature factor, and element symbol. Treat only lines whose record name equals the literal string \"ATOM\" as protein atoms. Treat water molecules as residues with record name equal to the literal string \"HETATM\" and residue name equal to the literal string \"HOH\". For each water residue, consider only the atom whose atom name equals the literal string \"O\" as the position of that water.\n\nDefine the Euclidean distance between two atoms with coordinates $(x_1,y_1,z_1)$ and $(x_2,y_2,z_2)$ in Angstroms as\n$$\nd\\big((x_1,y_1,z_1),(x_2,y_2,z_2)\\big)=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}.\n$$\nLet the nonnegative real number $r$ (in Angstroms) denote a distance cutoff. A water residue acts as a bridge between two different protein chains if and only if there exist two distinct protein chain identifiers $c_1$ and $c_2$ with $c_1\\neq c_2$, and there exists at least one protein atom in chain $c_1$ with distance $\\leq r$ to the water oxygen atom, and at least one protein atom in chain $c_2$ with distance $\\leq r$ to the water oxygen atom. For each test case below, you must identify all such bridging water residues and report their residue sequence numbers as integers, sorted in ascending order. Distances must be computed in Angstroms and the cutoff $r$ must be applied with the inclusive condition $\\leq r$.\n\nYour program must process the following test suite, where each test case is a pair consisting of a PDB text block and a cutoff $r$ in Angstroms:\n\n- Test case 1:\n  - PDB:\n    ATOM      1  CA  ALA A   1       0.000   0.000   0.000  1.00 20.00           C\n    ATOM      2  CA  ALA B   1       6.000   0.000   0.000  1.00 20.00           C\n    HETATM  101  O   HOH W 101       3.000   0.000   0.000  1.00 10.00           O\n    HETATM  102  O   HOH W 102       0.000   3.100   0.000  1.00 10.00           O\n    HETATM  103  O   HOH W 103      20.000  20.000  20.000  1.00 10.00           O\n  - $r = 3.2$\n- Test case 2:\n  - PDB:\n    ATOM      1  CA  GLY A   1       0.000   0.000   0.000  1.00 20.00           C\n    ATOM      2  CA  GLY B   1       7.000   0.000   0.000  1.00 20.00           C\n    HETATM  201  O   HOH W 201       3.500   0.000   0.000  1.00 10.00           O\n    HETATM  202  O   HOH W 202       3.500   3.500   0.000  1.00 10.00           O\n  - $r = 3.5$\n- Test case 3:\n  - PDB:\n    ATOM      1  CA  LYS A   1       0.000   0.000   0.000  1.00 20.00           C\n    ATOM      2  CA  LYS B   1       0.000   6.000   0.000  1.00 20.00           C\n    ATOM      3  CA  LYS C   1      10.000   0.000   0.000  1.00 20.00           C\n    HETATM  301  O   HOH W 301       0.000   3.000   0.000  1.00 10.00           O\n    HETATM  302  O   HOH W 302       5.000   0.000   0.000  1.00 10.00           O\n    HETATM  303  O   HOH W 303      12.500   0.000   0.000  1.00 10.00           O\n  - $r = 5.0$\n- Test case 4:\n  - PDB:\n    ATOM      1  CA  SER A   1       0.000   0.000   0.000  1.00 20.00           C\n    ATOM      2  CA  SER B   1       8.000   0.000   0.000  1.00 20.00           C\n    HETATM  401  O   HOH W 401       2.500   0.000   0.000  1.00 10.00           O\n    HETATM  402  O   HOH W 402       4.000   0.000   0.000  1.00 10.00           O\n    HETATM  900  FE  HEM L   1       2.500   0.000   0.000  1.00 15.00           FE\n  - $r = 4.0$\n\nFor each test case, the required output is a list of integers representing the residue sequence numbers of all water residues that are bridges according to the definition above, sorted in ascending order. Your program should produce a single line of output containing the results for all four test cases as a comma-separated list of these lists, enclosed in square brackets, with no spaces. For example, a valid output format would be \"[[a,b],[c],[d,e,f],[]]\" where $a$, $b$, $c$, $d$, $e$, and $f$ are integers.",
            "solution": "The problem will first be subjected to a rigorous validation process.\n\nStep 1: Extract Givens\n\n- **Input Data**: A set of textual fragments conforming to the Protein Data Bank (PDB) flatfile format.\n- **Line Types**: Each line begins with \"ATOM\" or \"HETATM\".\n- **PDB Line Fields (whitespace-separated)**:\n    1. Record name\n    2. Atom serial number\n    3. Atom name\n    4. Residue name\n    5. Chain identifier\n    6. Residue sequence number\n    7. Cartesian coordinate $x$ (in Angstroms)\n    8. Cartesian coordinate $y$ (in Angstroms)\n    9. Cartesian coordinate $z$ (in Angstroms)\n    10. Occupancy\n    11. Temperature factor\n    12. Element symbol\n- **Protein Atom Definition**: A line whose record name is the literal string \"ATOM\".\n- **Water Molecule Definition**: A residue with record name \"HETATM\" and residue name \"HOH\".\n- **Water Position Definition**: For a water residue, its position is defined by the coordinates of the atom whose atom name is \"O\".\n- **Distance Metric**: The Euclidean distance between two atoms at $(x_1, y_1, z_1)$ and $(x_2, y_2, z_2)$ is $d = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 + (z_1-z_2)^2}$.\n- **Distance Cutoff**: A non-negative real number $r$ in Angstroms.\n- **Bridging Water Definition**: A water residue acts as a bridge if its oxygen atom is at a distance less than or equal to $r$ ($\\leq r$) from at least one protein atom in a chain $c_1$ and also from at least one protein atom in a different chain $c_2$, where $c_1 \\neq c_2$.\n- **Required Output**: For each test case, a list of integer residue sequence numbers of all bridging water residues, sorted in ascending order. The final output for all test cases is a comma-separated list of these lists, within a single pair of square brackets.\n- **Test Suite**: Four specific test cases are provided, each with a PDB text block and a corresponding cutoff value $r$.\n\nStep 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is based on fundamental concepts in structural biology and bioinformatics. The PDB format is a standard for representing three-dimensional structures of biological macromolecules. The concept of water-mediated bridges between protein chains is a well-established principle in protein-protein interactions and protein stability analysis. The Euclidean distance is the standard metric for spatial calculations in this context. The problem is scientifically valid and realistic.\n- **Well-Posed**: The problem is clearly defined. All terms such as \"protein atom\", \"water molecule\", \"bridging water\", and \"distance\" are given precise, unambiguous definitions. The input data is provided, and the expected output format is specified. For any given input, the set of rules leads to a single, deterministic outcome. A unique solution exists and can be computed.\n- **Objective**: The problem statement is free of subjective language, opinions, or speculation. All criteria are based on objective, quantifiable measures (record names, distances, counts).\n- **Completeness and Consistency**: The problem is self-contained. All necessary information to solve it is provided within the statement, including the data for all test cases and the exact definitions to be used. There are no contradictions in the provided rules. For example, the criteria for \"ATOM\" and \"HETATM\" records are mutually exclusive for the purposes of categorization into protein vs. water.\n- **Feasibility and Realism**: The coordinate values and distances are physically plausible for atomic structures. The computational task is straightforward and does not require unrealistic resources.\n- **Structure**: The problem is well-structured and does not contain circular reasoning or tautologies.\n\nStep 3: Verdict and Action\n\nThe problem is **valid**. It is scientifically grounded, well-posed, objective, and self-contained. Therefore, a solution will be developed.\n\nThe solution requires implementing an algorithm that processes each test case according to the given rules. The algorithm can be structured as follows:\n\n1.  **Data Parsing and Segregation**: For each test case, parse the provided multi-line PDB string. Iterate through each line, split it by whitespace to extract the relevant fields. Based on the record name and residue name, categorize each relevant atom.\n    -   If the record name is \"ATOM\", store its chain identifier (field 5) and its coordinates (fields 7-9) as a protein atom.\n    -   If the record name is \"HETATM\", the residue name is \"HOH\", and the atom name is \"O\", store its residue sequence number (field 6) and its coordinates (fields 7-9) as a water molecule.\n    -   All other lines (e.g., other `HETATM` types like the `FE` in a `HEM` group) are ignored as per the problem definition.\n\n2.  **Bridging Logic**: For each identified water molecule, determine if it functions as a bridge.\n    -   Initialize an empty set, `contacted_chains`, to store the unique identifiers of protein chains that are in close contact with the current water molecule.\n    -   Iterate through all aformentioned protein atoms.\n    -   For each protein atom, calculate the squared Euclidean distance to the current water molecule's oxygen atom. The squared distance is given by $d^2 = (x_w - x_p)^2 + (y_w - y_p)^2 + (z_w - z_p)^2$, where $(x_w, y_w, z_w)$ are the water coordinates and $(x_p, y_p, z_p)$ are the protein atom coordinates.\n    -   Comparing $d^2$ with $r^2$ is computationally more efficient than calculating the square root. If $d^2 \\leq r^2$, the protein atom is within the cutoff distance.\n    -   If a protein atom is within the cutoff, add its chain identifier to the `contacted_chains` set. The use of a set automatically handles uniqueness.\n    -   After checking all protein atoms against the current water molecule, inspect the size of the `contacted_chains` set.\n    -   If the number of elements in `contacted_chains` is greater than or equal to $2$, the water molecule bridges at least two different chains and is therefore a \"bridging water\". Add its residue sequence number to a list of results for the current test case.\n\n3.  **Finalization**: After iterating through all water molecules in a test case, sort the collected list of bridging water residue sequence numbers in ascending order. Repeat this entire process for all test cases in the suite.\n\n4.  **Output Formatting**: Aggregate the sorted lists of residue numbers from all test cases into a final structure as specified: a single string representing a list of lists.\n\nThe use of `numpy` arrays is highly advisable for storing coordinates, as it enables vectorized and efficient computation of distances, which is superior to performing calculations in a simple Python loop, especially for larger datasets. For a given water molecule with coordinate vector $\\vec{v}_w$, and a set of $N$ protein atoms with coordinate matrix $\\mathbf{P}$ (of shape $N \\times 3$), the squared distances can be computed in a single operation by broadcasting $\\vec{v}_w$ across all rows of $\\mathbf{P}$ and summing the squared differences along the coordinate axis.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the bridging water identification problem for a suite of test cases.\n    \"\"\"\n    test_suite = [\n        {\n            \"pdb_text\": \"\"\"\nATOM      1  CA  ALA A   1       0.000   0.000   0.000  1.00 20.00           C\nATOM      2  CA  ALA B   1       6.000   0.000   0.000  1.00 20.00           C\nHETATM  101  O   HOH W 101       3.000   0.000   0.000  1.00 10.00           O\nHETATM  102  O   HOH W 102       0.000   3.100   0.000  1.00 10.00           O\nHETATM  103  O   HOH W 103      20.000  20.000  20.000  1.00 10.00           O\n\"\"\",\n            \"r\": 3.2\n        },\n        {\n            \"pdb_text\": \"\"\"\nATOM      1  CA  GLY A   1       0.000   0.000   0.000  1.00 20.00           C\nATOM      2  CA  GLY B   1       7.000   0.000   0.000  1.00 20.00           C\nHETATM  201  O   HOH W 201       3.500   0.000   0.000  1.00 10.00           O\nHETATM  202  O   HOH W 202       3.500   3.500   0.000  1.00 10.00           O\n\"\"\",\n            \"r\": 3.5\n        },\n        {\n            \"pdb_text\": \"\"\"\nATOM      1  CA  LYS A   1       0.000   0.000   0.000  1.00 20.00           C\nATOM      2  CA  LYS B   1       0.000   6.000   0.000  1.00 20.00           C\nATOM      3  CA  LYS C   1      10.000   0.000   0.000  1.00 20.00           C\nHETATM  301  O   HOH W 301       0.000   3.000   0.000  1.00 10.00           O\nHETATM  302  O   HOH W 302       5.000   0.000   0.000  1.00 10.00           O\nHETATM  303  O   HOH W 303      12.500   0.000   0.000  1.00 10.00           O\n\"\"\",\n            \"r\": 5.0\n        },\n        {\n            \"pdb_text\": \"\"\"\nATOM      1  CA  SER A   1       0.000   0.000   0.000  1.00 20.00           C\nATOM      2  CA  SER B   1       8.000   0.000   0.000  1.00 20.00           C\nHETATM  401  O   HOH W 401       2.500   0.000   0.000  1.00 10.00           O\nHETATM  402  O   HOH W 402       4.000   0.000   0.000  1.00 10.00           O\nHETATM  900  FE  HEM L   1       2.500   0.000   0.000  1.00 15.00           FE\n\"\"\",\n            \"r\": 4.0\n        }\n    ]\n\n    all_results = []\n\n    for case in test_suite:\n        pdb_text = case[\"pdb_text\"]\n        r = case[\"r\"]\n        r_squared = r**2\n\n        protein_atoms = [] # List of tuples: (chain_id, np.array([x, y, z]))\n        water_molecules = [] # List of tuples: (res_seq_num, np.array([x, y, z]))\n\n        lines = pdb_text.strip().split('\\n')\n        for line in lines:\n            fields = line.split()\n            record_name = fields[0]\n            \n            if record_name == \"ATOM\":\n                chain_id = fields[4]\n                coords = np.array([float(fields[6]), float(fields[7]), float(fields[8])])\n                protein_atoms.append((chain_id, coords))\n            elif record_name == \"HETATM\":\n                atom_name = fields[2]\n                res_name = fields[3]\n                if res_name == \"HOH\" and atom_name == \"O\":\n                    res_seq_num = int(fields[5])\n                    coords = np.array([float(fields[6]), float(fields[7]), float(fields[8])])\n                    water_molecules.append((res_seq_num, coords))\n\n        if not protein_atoms:\n            all_results.append([])\n            continue\n\n        protein_chain_ids = np.array([p[0] for p in protein_atoms])\n        protein_coords = np.array([p[1] for p in protein_atoms])\n\n        bridging_waters = []\n        for water_res_seq, water_coord in water_molecules:\n            # Calculate squared Euclidean distances from this water to all protein atoms\n            squared_distances = np.sum((protein_coords - water_coord)**2, axis=1)\n            \n            # Find which protein atoms are within the cutoff\n            contact_mask = squared_distances <= r_squared\n            \n            # Get the unique chain IDs of the contacted protein atoms\n            contacted_chains = set(protein_chain_ids[contact_mask])\n            \n            # A water is a bridge if it contacts 2 or more distinct chains\n            if len(contacted_chains) >= 2:\n                bridging_waters.append(water_res_seq)\n        \n        bridging_waters.sort()\n        all_results.append(bridging_waters)\n\n    # Format the final output string\n    result_str = \",\".join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}