{
    "hands_on_practices": [
        {
            "introduction": "贝叶斯推断的核心在于将先验信念与数据提供的证据相结合，从而更新我们对假设的认识。本练习  将指导您完成一个基础但至关重要的计算任务：从给定的似然值和先验概率出发，计算拓扑结构的后验概率。通过这个实践，您不仅能深入理解贝叶斯定理的直接应用，还能体会到先验概率在最终推断中扮演的关键角色，并掌握处理对数似然时保证数值稳定性的实用技巧。",
            "id": "2375013",
            "problem": "给定一个包含$4$个分类单元的离散无根树拓扑结构集合，以及它们对应的对数似然值和一个在这些拓扑结构上的先验概率质量函数。您的任务是通过有原则的推导和计算，展示一个强先验如何导致最大似然（ML）树具有非常低的后验概率。\n\n使用的起点和定义：\n- 从专门用于有限假设空间的贝叶斯定理以及最大似然（ML）和最大后验（MAP）的标准定义开始。将每个拓扑结构视为一个离散假设。\n- 令 $T_i$ 表示第 $i$ 个拓扑结构，令 $D$ 表示观测到的序列数据，令 $L_i$ 表示在固定的、时间可逆且位点独立的替换模型下的对数似然 $\\log p(D \\mid T_i)$，并令 $\\pi_i$ 表示拓扑结构 $T_i$ 上的先验概率 $p(T_i)$。假设 $L_i$ 已经通过标准算法（例如，剪枝算法）在固定模型下计算得出，并且 $\\sum_i \\pi_i = 1$，其中每个 $\\pi_i \\in (0,1)$。\n\n您必须做到：\n1. 从第一性原理出发，推导出后验概率 $p(T_i \\mid D)$ 关于 $\\{L_i\\}$ 和 $\\{\\pi_i\\}$ 的表达式，然后将其特化为ML拓扑结构 $T_m$ 的后验概率，其中 $m = \\arg\\max_i L_i$。\n2. 为了数值稳定性，论证当 $L_i$ 的量级很大且为负数时，如何计算归一化常数以避免下溢。您的实现必须在对数空间中操作，并且对于所提供的值必须是稳定的。\n3. 实现一个程序，对下面的每个测试用例，计算赋予ML拓扑结构 $T_m$ 的后验概率，并将其作为四舍五入到六位小数的浮点数返回。\n\n测试套件（每个用例列出 $\\{L_1,L_2,L_3\\}$ 和 $\\{\\pi_1,\\pi_2,\\pi_3\\}$）：\n- 用例1（先验强烈支持非ML拓扑结构；数据强度中等）：\n  - $\\{L_i\\} = \\{-100.0, -101.0, -102.0\\}$\n  - $\\{\\pi_i\\} = \\{0.05, 0.90, 0.05\\}$\n- 用例2（均匀先验；数据与用例1相同）：\n  - $\\{L_i\\} = \\{-100.0, -101.0, -102.0\\}$\n  - $\\{\\pi_i\\} = \\{\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\}$\n- 用例3（数据极弱；极强的先验反对ML拓扑结构）：\n  - $\\{L_i\\} = \\{-100.0, -100.001, -100.002\\}$\n  - $\\{\\pi_i\\} = \\{0.01, 0.98, 0.01\\}$\n- 用例4（极强的数据压倒强先验）：\n  - $\\{L_i\\} = \\{-90.0, -100.0, -110.0\\}$\n  - $\\{\\pi_i\\} = \\{0.05, 0.90, 0.05\\}$\n\n输出规格：\n- 对于每个用例，计算ML拓扑结构的后验概率 $p(T_m \\mid D)$，结果为四舍五入到六位小数的浮点数。\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[0.123456,0.234567,0.345678,0.456789]$）。",
            "solution": "所述问题是有效的。它有科学依据，表述清晰，并包含了推导唯一、有意义解所需的所有信息。它代表了贝叶斯推断在计算系统发育学领域的标准应用。我们将着手进行推导和求解。\n\n问题的核心在于将贝叶斯定理应用于一组离散的假设，在当前背景下，这些假设是给定分类单元的可能的无根树拓扑结构。对于$4$个分类单元，存在$3$种这样的唯一拓扑结构。我们将这些拓扑结构表示为集合 $\\{T_1, T_2, T_3\\}$。给定观测数据 $D$，它通常由对齐的分子序列组成。\n\n**1. 后验概率的推导**\n\n我们从贝叶斯定理开始，对于一个特定的假设 $T_i$ 和数据 $D$，其表述如下：\n$$ p(T_i \\mid D) = \\frac{p(D \\mid T_i) p(T_i)}{p(D)} $$\n这里，各项分别为：\n- $p(T_i \\mid D)$ 是给定数据 $D$ 时拓扑结构 $T_i$ 的后验概率。这是我们希望计算的量。\n- $p(D \\mid T_i)$ 是在拓扑结构 $T_i$ 为真的情况下，观测到数据 $D$ 的似然。\n- $p(T_i)$ 是拓扑结构 $T_i$ 的先验概率，反映了我们在观测数据之前对该假设的信念。\n- $p(D)$ 是数据的边际似然，或称为证据。它作为一个归一化常数。\n\n边际似然 $p(D)$ 是通过根据全概率定律对所有可能的假设求和来计算的：\n$$ p(D) = \\sum_{j=1}^{3} p(D, T_j) = \\sum_{j=1}^{3} p(D \\mid T_j) p(T_j) $$\n将此代回贝叶斯定理，我们得到拓扑结构 $T_i$ 后验概率的完整表达式：\n$$ p(T_i \\mid D) = \\frac{p(D \\mid T_i) p(T_i)}{\\sum_{j=1}^{3} p(D \\mid T_j) p(T_j)} $$\n问题给出了对数似然 $L_i = \\log p(D \\mid T_i)$ 和先验概率 $\\pi_i = p(T_i)$。为了使用这些值，我们必须通过指数化将对数似然转换回似然：$p(D \\mid T_i) = \\exp(L_i)$。将这些代入我们的公式中，得到：\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i) \\pi_i}{\\sum_{j=1}^{3} \\exp(L_j) \\pi_j} $$\n这是任何拓扑结构 $T_i$ 的后验概率的通用表达式。\n\n最大似然（ML）拓扑结构，我们记为 $T_m$，是使似然函数最大化的那个拓扑结构。由于对数是严格单调函数，最大化似然等同于最大化对数似然。因此，ML拓扑结构的索引 $m$ 由下式给出：\n$$ m = \\arg\\max_i L_i $$\n这个特定的ML拓扑结构 $T_m$ 的后验概率则为：\n$$ p(T_m \\mid D) = \\frac{\\exp(L_m) \\pi_m}{\\sum_{j=1}^{3} \\exp(L_j) \\pi_j} $$\n\n**2. 数值稳定的计算**\n\n所提供的对数似然值 $L_i$ 是大的负数（例如，$-100.0$）。直接计算 $\\exp(L_i)$ 将导致一个非常接近零的值，以至于标准浮点运算会将其舍入为 $0.0$，这种现象称为算术下溢。这将使分子和分母中的所有项都变为零，导致一个不确定的 $0/0$ 形式。\n\n为避免这种情况，我们采用一种标准的数值稳定技术，通常称为“log-sum-exp”技巧。我们确定最大的对数似然，根据定义即为 $L_m$：$L_m = \\max_j\\{L_j\\}$。然后，我们从后验概率表达式的分子和分母中提出公因子 $\\exp(L_m)$。这不会改变分数的值。\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i) \\pi_i}{\\sum_{j} \\exp(L_j) \\pi_j} = \\frac{\\exp(L_m) \\cdot \\exp(L_i - L_m) \\pi_i}{\\exp(L_m) \\cdot \\sum_{j} \\exp(L_j - L_m) \\pi_j} $$\n消去 $\\exp(L_m)$ 项，我们得到一个数值稳定的公式：\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i - L_m) \\pi_i}{\\sum_{j=1}^{3} \\exp(L_j - L_m) \\pi_j} $$\n在此表达式中，每一项的指数是 $L_j - L_m$。由于 $L_m$ 是最大值，这个差值总是小于或等于零。对于ML拓扑结构本身，其指数为 $L_m - L_m = 0$，得到 $\\exp(0) = 1$。所有其他项 $\\exp(L_j - L_m)$（其中 $j \\neq m$）的值将在 $0$ 和 $1$ 之间，从而防止下溢。\n\n具体对于ML拓扑结构 $T_m$ 的后验概率，分子项变为 $\\exp(L_m - L_m) \\pi_m = \\exp(0) \\pi_m = \\pi_m$。我们所求量的最终稳定公式是：\n$$ p(T_m \\mid D) = \\frac{\\pi_m}{\\sum_{j=1}^{3} \\exp(L_j - L_m) \\pi_j} $$\n这个表达式构成了我们计算的基础。\n\n**3. 实现与最终计算**\n\n我们现在将实现一个程序，将此公式应用于给定的四个测试用例。目标是为每个用例计算 $p(T_m \\mid D)$。该程序将首先确定最大对数似然 $L_m$ 及其对应的索引 $m$。然后，它将通过对所有三个拓扑结构求和项 $\\exp(L_j - L_m) \\pi_j$ 来计算分母（归一化常数）。最后，它将使用上面推导出的稳定公式计算ML拓扑结构的后验概率。结果将展示来自数据的似然信号和先验信念之间的相互作用如何决定最终的后验概率，以及一个强先验如何导致ML拓扑结构在贝叶斯分析后被认为是不太可能的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogenetics problem for the given test cases.\n    It calculates the posterior probability of the Maximum Likelihood (ML) topology.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple of (log_likelihoods, priors).\n    test_cases = [\n        # Case 1: Prior strongly favors a non-ML topology; moderate data\n        (np.array([-100.0, -101.0, -102.0]), np.array([0.05, 0.90, 0.05])),\n        # Case 2: Uniform prior; same data as Case 1\n        (np.array([-100.0, -101.0, -102.0]), np.array([1/3, 1/3, 1/3])),\n        # Case 3: Extremely weak data; very strong prior against the ML topology\n        (np.array([-100.0, -100.001, -100.002]), np.array([0.01, 0.98, 0.01])),\n        # Case 4: Very strong data overriding a strong prior\n        (np.array([-90.0, -100.0, -110.0]), np.array([0.05, 0.90, 0.05])),\n    ]\n\n    results = []\n    for log_likelihoods, priors in test_cases:\n        result = calculate_ml_posterior(log_likelihoods, priors)\n        results.append(result)\n\n    # Format the final output as specified.\n    # The map function converts each rounded float to a string for joining.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_ml_posterior(log_likelihoods: np.ndarray, priors: np.ndarray) -> float:\n    \"\"\"\n    Calculates the posterior probability of the Maximum Likelihood (ML) topology\n    using a numerically stable method.\n\n    Args:\n        log_likelihoods: A numpy array of log-likelihoods for each topology.\n        priors: A numpy array of prior probabilities for each topology.\n\n    Returns:\n        The posterior probability of the ML topology as a float.\n    \"\"\"\n    # 1. Identify the Maximum Likelihood (ML) topology and its properties.\n    # The ML topology has the maximum log-likelihood.\n    # np.argmax returns the index of the maximum value.\n    ml_index = np.argmax(log_likelihoods)\n    \n    # Get the maximum log-likelihood value. This is L_m.\n    L_m = log_likelihoods[ml_index]\n    \n    # Get the prior probability for the ML topology. This is pi_m.\n    pi_m = priors[ml_index]\n\n    # 2. Compute the normalization constant (denominator) in a numerically stable way.\n    # The formula for the normalization constant C is:\n    # C = sum_j(exp(L_j - L_m) * pi_j)\n    \n    # Calculate the log-likelihood differences (L_j - L_m).\n    # This prevents underflow when exponentiating.\n    log_likelihood_diffs = log_likelihoods - L_m\n    \n    # Exponentiate the differences and multiply by the priors.\n    terms = np.exp(log_likelihood_diffs) * priors\n    \n    # Sum the terms to get the normalization constant.\n    normalization_constant = np.sum(terms)\n\n    # 3. Calculate the posterior probability of the ML topology.\n    # The stable formula for p(T_m | D) is:\n    # p(T_m | D) = (exp(L_m - L_m) * pi_m) / C = pi_m / C\n    # The numerator of the ML topology's posterior after stabilization is simply its prior.\n    numerator_ml = np.exp(L_m - L_m) * pi_m # which simplifies to pi_m\n    \n    posterior_ml = numerator_ml / normalization_constant\n    \n    return posterior_ml\n    \nsolve()\n```"
        },
        {
            "introduction": "在许多贝叶斯分析中，我们通常直接使用软件计算出的似然值。但这些数值究竟是如何得到的？本练习  将带您深入了解贝叶斯系统发育学引擎的内部工作原理。我们将通过一个简化的三物种树模型，手动推导和计算其边际似然。这个过程需要您对未知的枝长进行积分，并对未观察到的根节点状态进行求和，从而让您深刻理解替换模型、枝长先验和参数边缘化是如何共同构成模型证据的核心。",
            "id": "2375048",
            "problem": "考虑一个有根的 $3$-分类单元树，其叶节点 $A$、$B$ 和 $C$ 通过三条长度未知的枝 $\\ell_A$、$\\ell_B$ 和 $\\ell_C$ 直接连接到一个根节点。在叶节点上观察到一个单一的二元性状，其状态为 $x_A=0$、$x_B=0$ 和 $x_C=1$。未观察到的根节点状态 $r \\in \\{0,1\\}$ 具有平稳先验 $\\pi(0)=\\pi(1)=\\frac{1}{2}$。沿着长度为 $\\ell \\ge 0$ 的枝，演化遵循对称双态替换模型，其转移概率定义为\n$$\np_{\\text{same}}(\\ell) \\;=\\; \\frac{1}{2} + \\frac{1}{2}\\exp(-2\\ell), \n\\qquad\np_{\\text{diff}}(\\ell) \\;=\\; \\frac{1}{2} - \\frac{1}{2}\\exp(-2\\ell),\n$$\n其中 $p_{\\text{same}}(\\ell)$ 是枝末端的状态与其起始状态相同的概率，而 $p_{\\text{diff}}(\\ell)$ 是它们不同的概率。枝长先验独立，服从 $\\ell_A \\sim \\text{Exponential}(\\beta)$、$\\ell_B \\sim \\text{Exponential}(\\beta)$ 和 $\\ell_C \\sim \\text{Exponential}(\\beta)$，其中 $\\beta>0$ 是已知的，其密度函数为 $p(\\ell \\mid \\beta)=\\beta \\exp(-\\beta \\ell)$（当 $\\ell \\ge 0$ 时）。\n\n计算在该模型下观测数据的边际似然，\n$$\np(x_A=0, x_B=0, x_C=1 \\mid \\beta),\n$$\n通过对未知的枝长 $\\ell_A$、$\\ell_B$、$\\ell_C$ 进行积分，并对未观察到的根节点状态 $r$ 进行求和得到。请将您的最终答案表示为关于 $\\beta$ 的单个封闭形式解析表达式。请勿对您的答案进行四舍五入。",
            "solution": "该问题在计算生物学领域内提法明确且具有科学依据。所有必要信息均已提供，目标是进行一次清晰的数学计算。我们着手解决它。\n\n目标是计算给定超参数 $\\beta$ 时，观测数据 $D = \\{x_A=0, x_B=0, x_C=1\\}$ 的边际似然。这通过对未知的枝长 $\\boldsymbol{\\ell} = (\\ell_A, \\ell_B, \\ell_C)$ 进行积分，并对未观察到的根节点状态 $r \\in \\{0, 1\\}$ 进行求和得到。边际似然由以下公式给出：\n$$\np(D \\mid \\beta) = \\int_0^\\infty \\int_0^\\infty \\int_0^\\infty p(D \\mid \\ell_A, \\ell_B, \\ell_C) \\, p(\\ell_A, \\ell_B, \\ell_C \\mid \\beta) \\, d\\ell_A \\, d\\ell_B \\, d\\ell_C\n$$\n枝长的先验分布由独立的指数分布给出：\n$$\np(\\ell_A, \\ell_B, \\ell_C \\mid \\beta) = p(\\ell_A \\mid \\beta) p(\\ell_B \\mid \\beta) p(\\ell_C \\mid \\beta) = \\beta^3 \\exp(-\\beta(\\ell_A+\\ell_B+\\ell_C))\n$$\n给定枝长的数据似然 $p(D \\mid \\boldsymbol{\\ell})$ 通过对根节点状态 $r$ 进行边缘化得到：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\sum_{r \\in \\{0, 1\\}} p(D \\mid r, \\boldsymbol{\\ell}) \\, p(r)\n$$\n给定根节点状态 $r$ 和枝长，叶节点上的性状状态是条件独立的。因此，以 $r$ 为条件的似然为：\n$$\np(D \\mid r, \\boldsymbol{\\ell}) = p(x_A \\mid r, \\ell_A) \\, p(x_B \\mid r, \\ell_B) \\, p(x_C \\mid r, \\ell_C)\n$$\n我们考虑两种可能的根节点状态，使用转移概率 $p_{\\text{same}}(\\ell) = \\frac{1}{2} + \\frac{1}{2}\\exp(-2\\ell)$ 和 $p_{\\text{diff}}(\\ell) = \\frac{1}{2} - \\frac{1}{2}\\exp(-2\\ell)$。\n\n情况1：根节点状态 $r=0$。\n数据为 $x_A=0$、$x_B=0$、$x_C=1$。\n- 对于叶节点 $A$，状态与根节点相同 ($0 \\to 0$)。概率为 $p_{\\text{same}}(\\ell_A)$。\n- 对于叶节点 $B$，状态与根节点相同 ($0 \\to 0$)。概率为 $p_{\\text{same}}(\\ell_B)$。\n- 对于叶节点 $C$，状态与根节点不同 ($0 \\to 1$)。概率为 $p_{\\text{diff}}(\\ell_C)$。\n此情况下的联合概率为 $p(D \\mid r=0, \\boldsymbol{\\ell}) = p_{\\text{same}}(\\ell_A)p_{\\text{same}}(\\ell_B)p_{\\text{diff}}(\\ell_C)$。\n\n情况2：根节点状态 $r=1$。\n- 对于叶节点 $A$，状态与根节点不同 ($1 \\to 0$)。概率为 $p_{\\text{diff}}(\\ell_A)$。\n- 对于叶节点 $B$，状态与根节点不同 ($1 \\to 0$)。概率为 $p_{\\text{diff}}(\\ell_B)$。\n- 对于叶节点 $C$，状态与根节点相同 ($1 \\to 1$)。概率为 $p_{\\text{same}}(\\ell_C)$。\n此情况下的联合概率为 $p(D \\mid r=1, \\boldsymbol{\\ell}) = p_{\\text{diff}}(\\ell_A)p_{\\text{diff}}(\\ell_B)p_{\\text{same}}(\\ell_C)$。\n\n总似然 $p(D \\mid \\boldsymbol{\\ell})$ 是这些可能性的加权和，权重为根节点状态的先验概率 $p(r=0) = p(r=1) = \\frac{1}{2}$：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} p_{\\text{same}}(\\ell_A)p_{\\text{same}}(\\ell_B)p_{\\text{diff}}(\\ell_C) + \\frac{1}{2} p_{\\text{diff}}(\\ell_A)p_{\\text{diff}}(\\ell_B)p_{\\text{same}}(\\ell_C)\n$$\n代入转移概率的定义：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} \\left[ \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_A}\\right) \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_B}\\right) \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_C}\\right) + \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_A}\\right) \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_B}\\right) \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_C}\\right) \\right]\n$$\n从每一项中提出因子 $\\frac{1}{2}$：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{8}\\right) \\left[ (1+e^{-2\\ell_A})(1+e^{-2\\ell_B})(1-e^{-2\\ell_C}) + (1-e^{-2\\ell_A})(1-e^{-2\\ell_B})(1+e^{-2\\ell_C}) \\right]\n$$\n设 $u_i = \\exp(-2\\ell_i)$。方括号中的表达式为 $(1+u_A)(1+u_B)(1-u_C) + (1-u_A)(1-u_B)(1+u_C)$。\n展开第一项：$(1+u_A+u_B+u_Au_B)(1-u_C) = 1+u_A+u_B-u_C+u_Au_B-u_Au_C-u_Bu_C-u_Au_Bu_C$。\n展开第二项：$(1-u_A-u_B+u_Au_B)(1+u_C) = 1-u_A-u_B+u_C+u_Au_B-u_Au_C-u_Bu_C+u_Au_Bu_C$。\n将这两个展开式相加得到 $2+2u_Au_B-2u_Au_C-2u_Bu_C$。\n代回到似然表达式中：\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{16} \\left( 2 + 2e^{-2\\ell_A}e^{-2\\ell_B} - 2e^{-2\\ell_A}e^{-2\\ell_C} - 2e^{-2\\ell_B}e^{-2\\ell_C} \\right)\n$$\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{8} \\left( 1 + e^{-2(\\ell_A+\\ell_B)} - e^{-2(\\ell_A+\\ell_C)} - e^{-2(\\ell_B+\\ell_C)} \\right)\n$$\n现在，我们通过对 $p(D \\mid \\boldsymbol{\\ell})$ 与先验 $p(\\boldsymbol{\\ell} \\mid \\beta)$ 进行积分来计算边际似然：\n$$\np(D \\mid \\beta) = \\int_0^\\infty \\int_0^\\infty \\int_0^\\infty \\frac{1}{8} \\left( 1 + e^{-2(\\ell_A+\\ell_B)} - e^{-2(\\ell_A+\\ell_C)} - e^{-2(\\ell_B+\\ell_C)} \\right) \\beta^3 e^{-\\beta(\\ell_A+\\ell_B+\\ell_C)} \\,d\\ell_A \\,d\\ell_B \\,d\\ell_C\n$$\n该积分可分为四项。我们使用通用的定积分：\n$$\nI(\\alpha) = \\int_0^\\infty e^{-\\alpha\\ell} \\beta e^{-\\beta\\ell} \\,d\\ell = \\beta \\int_0^\\infty e^{-(\\alpha+\\beta)\\ell} \\,d\\ell = \\beta \\left[ \\frac{-1}{\\alpha+\\beta}e^{-(\\alpha+\\beta)\\ell} \\right]_0^\\infty = \\frac{\\beta}{\\alpha+\\beta}\n$$\n对于没有额外指数项的枝，其积分对应 $\\alpha=0$，所以 $I(0) = \\frac{\\beta}{\\beta} = 1$。对于带有 $e^{-2\\ell}$ 项的枝，其积分对应 $\\alpha=2$，所以 $I(2)=\\frac{\\beta}{\\beta+2}$。\n总的边际似然是：\n$$\np(D \\mid \\beta) = \\frac{1}{8} \\left[ \\iiint_V 1 \\cdot p(\\boldsymbol{\\ell} \\mid \\beta) dV + \\iiint_V e^{-2(\\ell_A+\\ell_B)} p(\\boldsymbol{\\ell} \\mid \\beta) dV - \\iiint_V e^{-2(\\ell_A+\\ell_C)} p(\\boldsymbol{\\ell} \\mid \\beta) dV - \\iiint_V e^{-2(\\ell_B+\\ell_C)} p(\\boldsymbol{\\ell} \\mid \\beta) dV \\right]\n$$\n其中 $dV = d\\ell_A d\\ell_B d\\ell_C$ 且 $p(\\boldsymbol{\\ell} \\mid \\beta) = p(\\ell_A|\\beta)p(\\ell_B|\\beta)p(\\ell_C|\\beta)$。\n每个三重积分是三个一维积分的乘积：\n1. 第一项：$\\frac{1}{8} [I(0) \\cdot I(0) \\cdot I(0)] = \\frac{1}{8} [1 \\cdot 1 \\cdot 1] = \\frac{1}{8}$。\n2. 第二项：$\\frac{1}{8} [I(2) \\cdot I(2) \\cdot I(0)] = \\frac{1}{8} \\left[\\frac{\\beta}{\\beta+2} \\cdot \\frac{\\beta}{\\beta+2} \\cdot 1\\right] = \\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$。\n3. 第三项：$-\\frac{1}{8} [I(2) \\cdot I(0) \\cdot I(2)] = -\\frac{1}{8} \\left[\\frac{\\beta}{\\beta+2} \\cdot 1 \\cdot \\frac{\\beta}{\\beta+2}\\right] = -\\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$。\n4. 第四项：$-\\frac{1}{8} [I(0) \\cdot I(2) \\cdot I(2)] = -\\frac{1}{8} \\left[1 \\cdot \\frac{\\beta}{\\beta+2} \\cdot \\frac{\\beta}{\\beta+2}\\right] = -\\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$。\n\n将这些项相加：\n$$\np(D \\mid \\beta) = \\frac{1}{8} + \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 - \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 - \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 = \\frac{1}{8} \\left[1 - \\left(\\frac{\\beta}{\\beta+2}\\right)^2\\right]\n$$\n我们简化这个表达式：\n$$\np(D \\mid \\beta) = \\frac{1}{8} \\left[ \\frac{(\\beta+2)^2 - \\beta^2}{(\\beta+2)^2} \\right] = \\frac{1}{8} \\left[ \\frac{\\beta^2+4\\beta+4 - \\beta^2}{(\\beta+2)^2} \\right] = \\frac{1}{8} \\left[ \\frac{4\\beta+4}{(\\beta+2)^2} \\right]\n$$\n$$\np(D \\mid \\beta) = \\frac{4(\\beta+1)}{8(\\beta+2)^2} = \\frac{\\beta+1}{2(\\beta+2)^2}\n$$\n这就是边际似然的最终封闭形式表达式。",
            "answer": "$$\\boxed{\\frac{\\beta+1}{2(\\beta+2)^{2}}}$$"
        },
        {
            "introduction": "我们已经了解了如何计算后验概率以及似然值是如何形成的。现在，让我们将这些知识应用于一个完整（尽管是模拟的）研究周期中。本练习  通过要求您模拟数据并对其进行推断，从而将理论与实践联系起来。您将实现一个完整的流程：首先从一个已知的真实拓扑结构生成DNA序列，然后使用贝叶斯方法计算不同拓扑结构的后验概率。通过观察随着序列长度增加，真实拓扑的后验概率如何趋近于$1$，您将亲眼见证贝叶斯推断的一个基本性质：统计一致性。",
            "id": "2375068",
            "problem": "给定一个包含四个标记为 $A$、$B$、$C$ 和 $D$ 的分类单元的二叉无根系统发育树。真实的无根拓扑结构为 $T_{\\text{true}} = ((A,B),(C,D))$。所有枝长均以在核苷酸上使用Jukes–Cantor模型的连续时间马尔可夫链（CTMC）下每位点的预期替换数来衡量。设叶端枝长为 $l_A = 0.2$、$l_B = 0.2$、$l_C = 0.2$、$l_D = 0.2$，唯一的内部枝长为 $l_{\\text{int}} = 0.1$。考虑一个有根表示，通过将根置于内部边的中点形成，使得从根到两个内部节点的长度各为 $l_{\\text{int}}/2$。\n\n替换过程为Jukes–Cantor模型，速率为每单位枝长 $\\mu = 1$。状态空间为 $\\{A,C,G,T\\}$。沿长度为 $t$ 的分支从状态 $i$ 转换到状态 $j$ 的转移概率为\n$$\nP_{ij}(t) =\n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{- \\frac{4}{3} t}  \\text{if } i = j, \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{- \\frac{4}{3} t}  \\text{if } i \\ne j,\n\\end{cases}\n$$\n且平稳分布为均匀分布，对所有 $i$ 都有 $\\pi_i = \\frac{1}{4}$。\n\n数据生成：对每个位点 $k \\in \\{1,\\dots,L\\}$，从平稳分布中抽取根状态 $R_k$。将状态沿树向下传播，各位点之间独立进行，子状态根据其父状态使用沿每个长度为 $t$ 的分支的 $P(t)$ 进行采样。观测数据为分类单元 $A$、$B$、$C$ 和 $D$ 在 $k = 1,\\dots,L$ 时的叶节点状态 $(X_{A,k}, X_{B,k}, X_{C,k}, X_{D,k})$。\n\n推断：考虑这四个分类单元上的三个无根二叉拓扑结构集合，\n$$\n\\mathcal{T} = \\{T_1=((A,B),(C,D)),\\; T_2=((A,C),(B,D)),\\; T_3=((A,D),(B,C))\\}.\n$$\n为这三个拓扑结构分配一个均匀先验，使得对每个 $i \\in \\{1,2,3\\}$，都有 $\\Pr(T_i)=\\frac{1}{3}$。对于任何拓扑结构 $T \\in \\mathcal{T}$，其在给定长度为 $L$ 的序列比对下的似然定义为，在Jukes–Cantor模型下，使用相同的枝长 $l_A, l_B, l_C, l_D, l_{\\text{int}}$ 并且根置于内部边中点（因此与根相邻的两条边的长度各为 $l_{\\text{int}}/2$）时，各位点似然的乘积。对于具有观测叶节点 $(x_A, x_B, x_C, x_D)$ 的单个位点，其位点似然是根节点的平稳权重与沿所有分支到叶节点的转移概率的乘积，对所有内部节点和根节点状态求和的结果。$\\mathcal{T}$ 上的后验概率与先验概率乘以似然成正比，并且必须进行归一化，以使三个拓扑结构上的概率之和为1。\n\n任务：在 $T_{\\text{true}}$ 下模拟序列数据，并为以下每个测试用例计算分配给 $T_{\\text{true}}$ 的后验概率。为保证可复现性，在模拟每个数据集之前，使用该测试用例指定的整数种子 $s$ 初始化伪随机数生成器。测试套件是数对 $(L,s)$ 的集合：\n$$\n\\{(1,7),\\; (25,11),\\; (100,13),\\; (400,17),\\; (1200,19)\\}.\n$$\n\n要求：\n- 在模拟和似然计算中均使用上述指定的Jukes–Cantor模型。\n- 对所有拓扑结构使用相同的枝长 $l_A = l_B = l_C = l_D = 0.2$ 和 $l_{\\text{int}} = 0.1$，并将根置于内部边的中点。\n- 假设各位点是独立同分布的。\n- 后验概率必须根据似然和三个拓扑结构上的均匀先验精确计算。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，其中包含按上述顺序列出的每个测试用例分配给 $T_{\\text{true}}$ 的后验概率，每个概率值四舍五入到六位小数。例如，输出格式必须为\n$$\n[\\text{p}_1,\\text{p}_2,\\text{p}_3,\\text{p}_4,\\text{p}_5],\n$$\n其中 $\\text{p}_i$ 是第 $i$ 个测试用例中 $T_{\\text{true}}$ 的后验概率，四舍五入到六位小数。本问题无需报告物理单位。不涉及角度。所有最终数值答案必须为小数。",
            "solution": "所提供的问题陈述已经过严格验证，被认为是科学上合理、形式完备且客观的。它提出了计算系统发育学中的一个标准任务：在给定序列数据的情况下，使用贝叶斯推断来评估一组相互竞争的树拓扑结构的后验概率。该问题指定了一个完整的数据生成模型（在固定的真实拓扑结构上使用Jukes-Cantor模型）和一个精确的推断框架（使用拓扑结构的均匀先验和固定枝长的贝叶斯模型选择）。所有参数都已明确提供。因此，我们将着手提供一个规范解法。\n\n问题的核心是计算真实拓扑结构 $T_{\\text{true}} = ((A,B),(C,D))$ 的后验概率，该概率是在给定从该拓扑结构模拟的DNA序列比对数据的情况下得出的。该后验概率使用贝叶斯定理计算。四个分类单元（$A, B, C, D$）的可能无根拓扑结构集合为 $\\mathcal{T} = \\{T_1, T_2, T_3\\}$，其中 $T_1 = ((A,B),(C,D))$，$T_2 = ((A,C),(B,D))$，以及 $T_3 = ((A,D),(B,C))$。\n\n在给定观测到的序列比对数据 $D$ 的情况下，拓扑结构 $T_i \\in \\mathcal{T}$ 的后验概率由下式给出：\n$$\n\\Pr(T_i | D) = \\frac{\\Pr(D | T_i) \\Pr(T_i)}{\\sum_{j=1}^{3} \\Pr(D | T_j) \\Pr(T_j)}\n$$\n该问题指定了拓扑结构上的均匀先验，$\\Pr(T_i) = 1/3$，对于 $i \\in \\{1, 2, 3\\}$。这将后验概率简化为与似然 $\\Pr(D|T_i)$ 成正比，我们将其表示为 $L(T_i|D)$:\n$$\n\\Pr(T_i | D) = \\frac{L(T_i | D)}{\\sum_{j=1}^{3} L(T_j | D)}\n$$\n比对中的位点被假定为独立同分布（i.i.d.）。因此，长度为 $L$ 的比对 $D$ 的总似然是每个位点 $D_k$ 的似然的乘积：\n$$\nL(T_i | D) = \\prod_{k=1}^{L} L(T_i | D_k)\n$$\n其中 $D_k = (x_{A,k}, x_{B,k}, x_{C,k}, x_{D,k})$ 表示位点 $k$ 处四个分类单元上观测到的核苷酸。\n\n替换过程遵循Jukes-Cantor模型。沿长度为 $t$ 的分支从状态 $i$ 转换到状态 $j$ 的概率由矩阵 $P(t)$ 给出，其元素为：\n$$\nP_{ij}(t) =\n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{- \\frac{4}{3} t}  \\text{if } i = j \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{- \\frac{4}{3} t}  \\text{if } i \\ne j\n\\end{cases}\n$$\n该模型假设核苷酸的平稳分布是均匀的，$\\pi = (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$。对于模拟和推断，都需要一个有根树。这是通过将根置于长度为 $l_{\\text{int}} = 0.1$ 的唯一内部边的中点来形成的。这导致从根发出的两条内部枝长为 $l_{\\text{int}}/2 = 0.05$。通向叶节点的叶端枝长均为 $0.2$。\n\n位点似然 $L(T_i | D_k)$ 使用Felsenstein的剪枝算法计算。该算法有效地对树中内部节点的所有可能状态进行求和。对于给定的拓扑结构 $T_i$ 和位点 $k$，该算法计算每个节点的部分似然，从叶节点向根节点移动。\n设 $L_u(s)$ 为节点 $u$ 处状态 $s$ 的部分似然。\n1.  **叶节点：** 在叶节点 $u \\in \\{A, B, C, D\\}$，部分似然向量根据观测数据进行初始化。例如，对于叶节点 $A$，如果观测状态为 $x_{A,k}$，则 $L_A(s) = \\delta_{s, x_{A,k}}$，其中 $\\delta$ 是克罗内克δ函数。这可以表示为一个独热向量。\n2.  **内部节点：** 对于一个内部节点 $u$，其子节点为 $v$ 和 $w$，通过长度为 $t_v$ 和 $t_w$ 的分支连接，其部分似然为：\n    $$\n    L_u(s) = \\left( \\sum_{s_v} P_{s,s_v}(t_v) L_v(s_v) \\right) \\left( \\sum_{s_w} P_{s,s_w}(t_w) L_w(s_w) \\right)\n    $$\n    在矩阵向量表示法中，如果 $L_v$ 和 $L_w$ 是子节点的部分似然列向量，则上式变为：\n    $$\n    L_u = (P(t_v)^T L_v) \\odot (P(t_w)^T L_w)\n    $$\n    其中 `T` 表示转置，$\\odot$ 表示逐元素乘积。\n3.  **根节点：** 这个过程一直持续到根节点 $R$。然后，该位点的总似然是根节点处的部分似然与平稳分布 $\\pi$ 加权后的和：\n    $$\n    L(T_i | D_k) = \\sum_{s} \\pi_s L_R(s) = \\pi^T L_R\n    $$\n拓扑结构 $T_i$ 决定了哪些叶节点是姐妹群。例如，在 $T_1 = ((A,B),(C,D))$ 下，内部节点分别是 $(A,B)$ 和 $(C,D)$ 的父节点。在 $T_2 = ((A,C),(B,D))$ 下，它们是 $(A,C)$ 和 $(B,D)$ 的父节点。所有拓扑结构的枝长都是相同的：叶端枝长为 $0.2$，从根向下的两条分支长度为 $0.05$。\n\n总体步骤如下：\n对于每个测试用例 $(L, s)$:\n1.  使用种子 $s$ 初始化伪随机数生成器。\n2.  模拟一个长度为 $L$ 的序列比对。对于每个位点，从 $\\pi$ 中抽取一个根状态，并使用转移概率 $P(t)$ 将状态沿真实的有根树 $T_1 = ((A,B),(C,D))$ 向下传播。\n3.  计算三种拓扑结构中每一种的总对数似然，$\\log L(T_i|D) = \\sum_{k=1}^{L} \\log L(T_i | D_k)$。对于较大的 $L$，使用对数对于避免数值下溢至关重要。\n4.  根据对数似然计算后验概率。为保持数值稳定性，我们使用log-sum-exp技巧。设 $\\ell_i = \\log L(T_i|D)$ 和 $\\ell_{\\max} = \\max(\\ell_1, \\ell_2, \\ell_3)$。真实拓扑结构 $T_1$ 的后验概率为：\n    $$\n    \\Pr(T_1 | D) = \\frac{e^{\\ell_1}}{e^{\\ell_1} + e^{\\ell_2} + e^{\\ell_3}} = \\frac{e^{\\ell_1 - \\ell_{\\max}}}{e^{\\ell_1 - \\ell_{\\max}} + e^{\\ell_2 - \\ell_{\\max}} + e^{\\ell_3 - \\ell_{\\max}}}\n    $$\n最终实现的程序对每个指定的测试用例执行此过程，并报告 $T_1$ 的后验概率，四舍五入到六位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogenetics problem by simulating sequence data and\n    computing the posterior probability of the true tree topology.\n    \"\"\"\n    # Define problem constants\n    N_STATES = 4\n    STATES = np.arange(N_STATES)  # 0:A, 1:C, 2:G, 3:T\n    STATIONARY_PI = np.full(N_STATES, 1.0 / N_STATES)\n\n    # Branch lengths\n    PENDANT_T = 0.2\n    INTERNAL_T = 0.1 / 2.0\n\n    # Define the three unrooted topologies for 4 taxa {A, B, C, D}\n    # These are mapped to indices {0, 1, 2, 3}\n    # T1 = ((A,B),(C,D)), T2 = ((A,C),(B,D)), T3 = ((A,D),(B,C))\n    TOPOLOGIES = [\n        (((0, 1), (2, 3))),  # T1, the true topology\n        (((0, 2), (1, 3))),  # T2\n        (((0, 3), (1, 2))),  # T3\n    ]\n\n    def get_jc_p_matrix(t):\n        \"\"\"\n        Calculates the Jukes-Cantor transition probability matrix for a branch of length t.\n        \"\"\"\n        p_ii = 0.25 + 0.75 * np.exp(-4.0 / 3.0 * t)\n        p_ij = 0.25 - 0.25 * np.exp(-4.0 / 3.0 * t)\n        P = np.full((N_STATES, N_STATES), p_ij)\n        np.fill_diagonal(P, p_ii)\n        return P\n\n    # Pre-calculate transition matrices for the given branch lengths\n    P_PENDANT = get_jc_p_matrix(PENDANT_T)\n    P_INTERNAL = get_jc_p_matrix(INTERNAL_T)\n\n    def simulate_site():\n        \"\"\"\n        Simulates a single site down the true tree topology T1=((A,B),(C,D)).\n        \"\"\"\n        # 1. Sample root state from the stationary distribution\n        root_state = np.random.choice(STATES, p=STATIONARY_PI)\n\n        # 2. Propagate states to internal nodes N1 (ancestor of A,B) and N2 (ancestor of C,D)\n        n1_state = np.random.choice(STATES, p=P_INTERNAL[root_state, :])\n        n2_state = np.random.choice(STATES, p=P_INTERNAL[root_state, :])\n\n        # 3. Propagate states to leaf nodes A, B, C, D\n        a_state = np.random.choice(STATES, p=P_PENDANT[n1_state, :])\n        b_state = np.random.choice(STATES, p=P_PENDANT[n1_state, :])\n        c_state = np.random.choice(STATES, p=P_PENDANT[n2_state, :])\n        d_state = np.random.choice(STATES, p=P_PENDANT[n2_state, :])\n\n        return [a_state, b_state, c_state, d_state]\n\n    def calculate_site_likelihood(site_data, topology):\n        \"\"\"\n        Calculates the likelihood of a single site for a given topology using \n        Felsenstein's pruning algorithm.\n        \"\"\"\n        # Initialize partial likelihoods at the leaves (one-hot vectors)\n        leaf_likelihoods = np.zeros((4, N_STATES))\n        leaf_likelihoods[0, site_data[0]] = 1.0  # Taxon A\n        leaf_likelihoods[1, site_data[1]] = 1.0  # Taxon B\n        leaf_likelihoods[2, site_data[2]] = 1.0  # Taxon C\n        leaf_likelihoods[3, site_data[3]] = 1.0  # Taxon D\n\n        # Get leaf pairings for the two internal nodes from the topology definition\n        (p1_left_idx, p1_right_idx), (p2_left_idx, p2_right_idx) = topology\n\n        # Calculate partial likelihoods for internal node 1\n        L_n1_from_left = P_PENDANT.T @ leaf_likelihoods[p1_left_idx]\n        L_n1_from_right = P_PENDANT.T @ leaf_likelihoods[p1_right_idx]\n        L_n1 = L_n1_from_left * L_n1_from_right\n\n        # Calculate partial likelihoods for internal node 2\n        L_n2_from_left = P_PENDANT.T @ leaf_likelihoods[p2_left_idx]\n        L_n2_from_right = P_PENDANT.T @ leaf_likelihoods[p2_right_idx]\n        L_n2 = L_n2_from_left * L_n2_from_right\n\n        # Propagate likelihoods to the root\n        L_root_from_n1 = P_INTERNAL.T @ L_n1\n        L_root_from_n2 = P_INTERNAL.T @ L_n2\n        L_root = L_root_from_n1 * L_root_from_n2\n\n        # Final site likelihood is the dot product with the stationary distribution\n        site_likelihood = STATIONARY_PI @ L_root\n        return site_likelihood\n\n    test_cases = [\n        (1, 7),\n        (25, 11),\n        (100, 13),\n        (400, 17),\n        (1200, 19),\n    ]\n\n    results = []\n    for L, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Simulate sequence alignment of length L\n        alignment = [simulate_site() for _ in range(L)]\n\n        # 2. Calculate log-likelihoods for each of the three topologies\n        log_likelihoods = np.zeros(len(TOPOLOGIES))\n        for i, topo in enumerate(TOPOLOGIES):\n            total_log_lik = 0.0\n            for site in alignment:\n                site_lik = calculate_site_likelihood(site, topo)\n                if site_lik > 0:\n                    total_log_lik += np.log(site_lik)\n                else:\n                    total_log_lik = -np.inf # Should not happen in practice\n            log_likelihoods[i] = total_log_lik\n\n        # 3. Compute posterior probabilities from log-likelihoods\n        # Use log-sum-exp trick for numerical stability\n        max_log_lik = np.max(log_likelihoods)\n        shifted_log_liks = log_likelihoods - max_log_lik\n        likelihoods = np.exp(shifted_log_liks)\n        \n        # With a uniform prior, posterior is proportional to likelihood\n        sum_likelihoods = np.sum(likelihoods)\n        posteriors = likelihoods / sum_likelihoods\n\n        # The posterior for T_true is the first one in the list\n        posterior_T1 = posteriors[0]\n        results.append(posterior_T1)\n\n    # Format the results to exactly six decimal places and print\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}