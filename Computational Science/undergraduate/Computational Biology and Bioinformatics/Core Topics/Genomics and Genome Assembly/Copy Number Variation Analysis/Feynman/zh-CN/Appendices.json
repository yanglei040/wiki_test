{
    "hands_on_practices": [
        {
            "introduction": "双末端测序不仅能提供DNA片段的序列，还包含了关于基因组结构的关键几何信息。串联重复等结构变异会扭曲这种几何关系，从而在测序数据中产生可检测的信号。本练习将引导您推导这种信号的一种数学形式——跨越重复连接点的读段（reads）其插入片段大小（insert size）分布的变化，这是许多结构变异检测算法所使用的基本原理。",
            "id": "2382724",
            "problem": "对一个二倍体人类基因组进行双末端测序实验。文库的制备方式使得每个DNA片段两端之间的真实物理模板长度（表示为 $T$）是一个连续随机变量，服从均值为 $I_s$、方差为 $\\sigma^2$ 的正态分布，即 $T \\sim \\mathcal{N}(I_s,\\sigma^2)$。与 $I_s$ 相比，读长可以忽略不计，因此报告的插入片段大小可以等同于读对在基因组上最外侧比对位置之间的跨度。样本中存在一个长度为 $L_d$ 的串联重复导致的拷贝数变异（CNV）事件，但在参考基因组中不存在。使用一个标准比对软件将读段比对到参考基因组上，该软件报告的插入片段大小 $D$ 是指读对在参考基因组上最外侧比对碱基之间的基因组距离。\n\n我们只关注那些物理模板跨越样本中串联重复连接点（即片段跨越了重复片段的第一个拷贝和第二个拷贝之间的连接处）的读对。假设对于这类读对，与单拷贝参考基因组的比对会“折叠”掉重复的片段，因此报告的插入片段大小等于样本的物理模板长度减去重复片段的长度，即 $D = T - L_d$。为简化问题，忽略软剪切、比对错误和方向异常。假设 $I_s \\gg 0$ 且 $\\sigma > 0$。\n\n请推导这些跨连接点读对的报告插入片段大小 $D$ 的概率密度函数，并用 $I_s$、$ \\sigma$ 和 $L_d$ 以闭合形式表示。请提供 $f_D(d)$ 的单一解析表达式作为最终答案。无需四舍五入。",
            "solution": "首先，将对问题进行严格验证，以确定其科学和逻辑上的完整性。\n\n### 步骤1：提取已知条件\n\n问题陈述中提供了以下信息：\n- 真实物理模板长度 $T$ 是一个连续随机变量。\n- $T$ 的分布是均值为 $I_s$、方差为 $\\sigma^2$ 的正态分布，记为 $T \\sim \\mathcal{N}(I_s, \\sigma^2)$。\n- 样本基因组中存在一个长度为 $L_d$ 的串联重复。\n- 对于跨越重复连接点的读对，报告的插入片段大小 $D$ 与真实模板长度 $T$ 的关系由方程 $D = T - L_d$ 给出。\n- 参数 $I_s$ 和 $\\sigma$ 满足条件 $I_s \\gg 0$ 和 $\\sigma > 0$。\n- 目标是推导 $D$ 的概率密度函数（PDF），记为 $f_D(d)$。\n\n### 步骤2：使用已知条件进行验证\n\n根据所需标准对问题进行评估：\n- **科学依据**：该问题描述了一个简化但标准的模型，该模型在生物信息学中用于从双末端测序数据中检测结构变异，特别是串联重复。对于跨连接点的读段，其报告的插入片段大小会因重复片段的长度而减小（$D = T - L_d$）这一假设是此类分析的基础概念。用正态分布对文库片段大小进行建模也是一种常见且合理的近似。因此，该问题牢固地植根于计算生物学的原理。\n- **适定性**：该问题要求推导一个随机变量 $D$ 的概率分布，而 $D$ 是另一个已知分布的随机变量 $T$ 的简单线性变换。这是概率论中一个标准的、定义明确的问题，存在唯一且稳定的解。\n- **客观性**：问题陈述使用了精确、明确的技术语言，不含任何主观或非科学的主张。\n\n基于此分析，问题陈述没有科学上的不健全、模糊不清或矛盾之处。它是在指定领域内一个可形式化且相关的问题。\n\n### 步骤3：结论与行动\n\n问题陈述被判定为**有效**。现在开始推导解决方案。\n\n问题要求我们求出报告插入片段大小 $D$ 的概率密度函数（PDF）。我们已知真实物理模板长度 $T$ 是一个随机变量，服从均值为 $I_s$、方差为 $\\sigma^2$ 的正态分布。因此，$T$ 的概率密度函数 $f_T(t)$ 为：\n$$\nf_T(t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t - I_s)^2}{2\\sigma^2}\\right)\n$$\n问题指出，对于所考虑的特定读对子集，报告的插入片段大小 $D$ 是真实长度 $T$ 的一个线性变换：\n$$\nD = T - L_d\n$$\n这里，$L_d$ 是一个常数，代表重复的长度。\n\n正态分布的一个基本性质是它对线性变换是封闭的。如果一个随机变量 $X$ 服从正态分布，即 $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$，那么任何定义为 $Y = aX + b$（其中 $a$ 和 $b$ 是常数）的新随机变量 $Y$ 也服从正态分布。其均值和方差由以下公式给出：\n$$\nE[Y] = E[aX + b] = aE[X] + b = a\\mu_X + b\n$$\n$$\n\\text{Var}(Y) = \\text{Var}(aX + b) = a^2\\text{Var}(X) = a^2\\sigma_X^2\n$$\n我们将此原理应用于我们的随机变量 $D = T - L_d$。在这种情况下，$T$ 对应于 $X$，$D$ 对应于 $Y$，$a=1$，$b = -L_d$。\n\n$D$ 的均值，我们记为 $\\mu_D$，是：\n$$\n\\mu_D = E[D] = E[T - L_d] = E[T] - L_d\n$$\n由于 $E[T] = I_s$，我们有：\n$$\n\\mu_D = I_s - L_d\n$$\n$D$ 的方差，我们记为 $\\sigma_D^2$，是：\n$$\n\\sigma_D^2 = \\text{Var}(D) = \\text{Var}(T - L_d) = 1^2 \\cdot \\text{Var}(T)\n$$\n由于 $\\text{Var}(T) = \\sigma^2$，我们有：\n$$\n\\sigma_D^2 = \\sigma^2\n$$\n因此，跨连接点读对的报告插入片段大小 $D$ 也服从正态分布，具体为 $D \\sim \\mathcal{N}(I_s - L_d, \\sigma^2)$。\n\n一个均值为 $\\mu_D$、方差为 $\\sigma_D^2$ 的正态分布变量 $d$ 的概率密度函数由以下通用形式给出：\n$$\nf_D(d) = \\frac{1}{\\sqrt{2\\pi\\sigma_D^2}} \\exp\\left(-\\frac{(d - \\mu_D)^2}{2\\sigma_D^2}\\right)\n$$\n将推导出的均值 $\\mu_D = I_s - L_d$ 和方差 $\\sigma_D^2 = \\sigma^2$ 代入此方程，我们得到所需的 $D$ 的概率密度函数：\n$$\nf_D(d) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(d - (I_s - L_d))^2}{2\\sigma^2}\\right)\n$$\n该表达式可以简化为：\n$$\nf_D(d) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(d - I_s + L_d)^2}{2\\sigma^2}\\right)\n$$\n这就是指定读对群体的报告插入片段大小 $D$ 的概率密度函数的闭合形式解析表达式。",
            "answer": "$$\n\\boxed{f_D(d) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(d - I_s + L_d)^2}{2\\sigma^2}\\right)}\n$$"
        },
        {
            "introduction": "读深度（Read-depth）是检测拷贝数变异最直接和广泛使用的信号。然而，读深度信号很容易受到各种技术偏差的影响，例如局部 $GC$ 含量或比对唯一性问题，这些偏差可能模仿真实的CNV信号。因此，一个关键的挑战是如何区分一个真实的、信号微弱的CNV（如嵌合性缺失）和一个在所有样本中都表现为系统性低覆盖度的区域。本练习将带您建立一个严谨的统计假设检验框架来解决这个问题，这是任何稳健的CNV分析流程中的一项核心技能。",
            "id": "2382685",
            "problem": "您面临一个基于读数深度的拷贝数变异 (CNV) 分析的形式化决策问题。其目标是区分单个样本中的真实低振幅嵌合性缺失与一个仅在整个对照组中表现出系统性低测序覆盖度的基因组区域。\n\n假设在一个划分为 $n$ 个不重叠区间的固定基因组区域内，读数计数符合以下生成模型。设有 $m$ 个对照样本和一个测试（病例）样本。对于区间 $i \\in \\{1,\\dots,n\\}$：\n- 潜含的、区间特异性的基线率为 $\\lambda_i > 0$。\n- 病例样本具有已知的文库大小（暴露度）$L_s > 0$。\n- 对于 $j \\in \\{1,\\dots,m\\}$，对照样本 $j$ 具有已知的文库大小（暴露度）$L_{cj} > 0$。\n- 观测到的读数计数为：病例样本在区间 $i$ 的读数是 $S_i \\in \\mathbb{Z}_{\\ge 0}$，对照样本 $j$ 在区间 $i$ 的读数是 $C_{ij} \\in \\mathbb{Z}_{\\ge 0}$。\n\n假设在均值条件下为独立的泊松抽样。考虑两个假设：\n- $H_0$（共享低覆盖度）：对于每个区间 $i$，病例和对照样本共享相同的、按其暴露度缩放的基线率，\n$$\nS_i \\sim \\mathrm{Pois}(L_s \\lambda_i), \\quad C_{ij} \\sim \\mathrm{Pois}(L_{cj} \\lambda_i).\n$$\n- $H_1$（仅病例样本存在嵌合性缺失）：病例特异性率按因子 $(1-\\rho)$ 进行乘性降低，其中嵌合比例参数 $\\rho \\in [0,1)$ 未知，而对照样本则与 $H_0$ 下的情况相同，\n$$\nS_i \\sim \\mathrm{Pois}\\big(L_s (1-\\rho)\\lambda_i\\big), \\quad C_{ij} \\sim \\mathrm{Pois}(L_{cj} \\lambda_i).\n$$\n\n对于一个给定的区域，定义病例总计数 $S_{\\mathrm{tot}} = \\sum_{i=1}^n S_i$，对照总计数 $C_{\\mathrm{tot}} = \\sum_{i=1}^n \\sum_{j=1}^m C_{ij}$，以及总计数 $T_{\\mathrm{tot}} = S_{\\mathrm{tot}} + C_{\\mathrm{tot}}$。定义 $A=L_s$ 和 $B=\\sum_{j=1}^m L_{cj}$。\n\n您的程序必须针对每个区域，在最大似然意义上判断 $H_1$ 是否比 $H_0$ 提供了更好的解释。形式上，令 $\\ell_0^\\star$ 表示 $H_0$ 下的最大化对数似然，$\\ell_1^\\star$ 表示 $H_1$ 下的最大化对数似然（两者均在 $\\{\\lambda_i\\}_{i=1}^n$ 上进行最大化，而对于 $H_1$，还需在 $\\rho \\in [0,1)$ 上进行最大化）。对一个区域的决策必须是：\n- 如果 $\\ell_1^\\star > \\ell_0^\\star$，则输出布尔值 True。\n- 否则，输出布尔值 False。\n\n本问题中的输入数据是固定的（无用户输入）。对于以下所有测试用例，$n=5$, $m=2$, $L_s = 100$, $L_{c1} = 100$, $L_{c2} = 100$，所以 $A=100$, $B=200$。对于每个测试用例，程序应使用所提供的病例样本和两个对照样本的每个区间的计数。\n\n测试套件（每个用例是一个独立的区域；所有计数都是整数，按区间 $i=1,\\dots,5$ 的顺序列出）：\n- 用例 1（推定的嵌合性缺失）：\n  - 病例计数 $S = [60,55,65,60,60]$。\n  - 对照 1 计数 $C_{\\cdot 1} = [110,105,100,95,90]$。\n  - 对照 2 计数 $C_{\\cdot 2} = [110,105,100,95,90]$。\n- 用例 2（在病例和对照中均一致的系统性低覆盖区域）：\n  - 病例计数 $S = [40,40,40,40,40]$。\n  - 对照 1 计数 $C_{\\cdot 1} = [40,40,40,40,40]$。\n  - 对照 2 计数 $C_{\\cdot 2} = [40,40,40,40,40]$。\n- 用例 3（计数消失的极端边缘情况）：\n  - 病例计数 $S = [0,0,0,0,0]$。\n  - 对照 1 计数 $C_{\\cdot 1} = [0,0,0,0,0]$。\n  - 对照 2 计数 $C_{\\cdot 2} = [0,0,0,0,0]$。\n- 用例 4（细微的低振幅类嵌合性偏差）：\n  - 病例计数 $S = [55,50,45,55,45]$。\n  - 对照 1 计数 $C_{\\cdot 1} = [60,50,55,90,50]$。\n  - 对照 2 计数 $C_{\\cdot 2} = [50,45,40,60,10]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、针对 4 个用例的逗号分隔的布尔结果列表，不含空格。例如，输出形式可能为“[True,False,True,False]”。\n\n本问题不涉及物理单位或角度单位。所有必需的量均为无量纲的计数或缩放因子。唯一允许的输出是如上定义的布尔值，并将 4 个测试用例的结果汇总为指定的单行格式。",
            "solution": "所述问题是一个基于计数数据的最大似然估计原理的、良置的统计决策问题。该问题在科学上是合理的，形式化定义清晰，且不存在矛盾。目标是确定包含嵌合性缺失的模型 ($H_1$) 是否比关于共享系统性覆盖变异的更简单的零模型 ($H_0$) 更好地拟合观测到的读数计数数据。决策准则是判断在 $H_1$ 下的最大化对数似然（记为 $\\ell_1^\\star$）是否严格大于在 $H_0$ 下的最大化对数似然（记为 $\\ell_0^\\star$）。\n\n分析过程首先推导作为嵌合参数 $\\rho$ 的函数的剖面似然，然后建立一个简单条件，用于判断备择假设 $H_1$ 何时更优。\n\n在通用模型下，单个区间 $i$ 的观测计数的联合概率质量函数由独立泊松分布的乘积给出：\n$$\nP(S_i, \\{C_{ij}\\}_{j=1}^m | \\lambda_i, \\rho) = P(S_i | \\lambda_i, \\rho) \\prod_{j=1}^m P(C_{ij} | \\lambda_i)\n$$\n所有 $n$ 个区间的相应对数似然函数（忽略常数项，即计数的阶乘）为：\n$$\n\\ell(\\{\\lambda_i\\}_{i=1}^n, \\rho) = \\sum_{i=1}^n \\left[ S_i \\log\\big(L_s (1-\\rho)\\lambda_i\\big) - L_s (1-\\rho)\\lambda_i + \\sum_{j=1}^m \\left( C_{ij} \\log(L_{cj} \\lambda_i) - L_{cj} \\lambda_i \\right) \\right]\n$$\n通过分离涉及参数 $\\lambda_i$ 和 $\\rho$ 的项，可将此表达式重排为：\n$$\n\\ell(\\{\\lambda_i\\}, \\rho) = \\sum_{i=1}^n \\left[ (S_i + \\sum_{j=1}^m C_{ij}) \\log \\lambda_i - (L_s(1-\\rho) + \\sum_{j=1}^m L_{cj}) \\lambda_i \\right] + (\\sum_{i=1}^n S_i) \\log(1-\\rho) + K\n$$\n其中 $K$ 合并了所有相对于 $\\lambda_i$ 和 $\\rho$ 为常数的项（即仅涉及计数和文库大小的项）。令 $T_i = S_i + \\sum_{j=1}^m C_{ij}$ 为区间 $i$ 的总计数，$S_{\\mathrm{tot}} = \\sum_i S_i$, $A = L_s$, 且 $B = \\sum_j L_{cj}$。对数似然函数变为：\n$$\n\\ell(\\{\\lambda_i\\}, \\rho) = \\sum_{i=1}^n \\left[ T_i \\log \\lambda_i - (A(1-\\rho) + B) \\lambda_i \\right] + S_{\\mathrm{tot}} \\log(1-\\rho) + K\n$$\n为了找到最大化的对数似然，我们首先对固定的 $\\rho$ 求出每个讨厌参数 $\\lambda_i$ 的最大似然估计 (MLE)。参数 $\\lambda_i$ 在各区间之间是解耦的，因此我们可以对每个 $\\lambda_i$ 进行独立优化。对 $\\lambda_i$ 求导并令其为零，可得：\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda_i} = \\frac{T_i}{\\lambda_i} - (A(1-\\rho) + B) = 0\n$$\n这给出了 $\\lambda_i$ 的 MLE，是 $\\rho$ 的函数：\n$$\n\\hat{\\lambda}_i(\\rho) = \\frac{T_i}{A(1-\\rho) + B}\n$$\n将 $\\hat{\\lambda}_i(\\rho)$ 代回对数似然表达式，得到剖面似然函数，它仅是 $\\rho$ 的函数：\n$$\n\\ell(\\rho) = \\sum_{i=1}^n \\left[ T_i \\log\\left(\\frac{T_i}{A(1-\\rho) + B}\\right) - T_i \\right] + S_{\\mathrm{tot}} \\log(1-\\rho) + K\n$$\n令 $T_{\\mathrm{tot}} = \\sum_i T_i$。我们可以将依赖于 $\\rho$ 的项与不依赖于 $\\rho$ 的项分开：\n$$\n\\ell(\\rho) = \\left[ \\sum_i (T_i \\log T_i - T_i) + K \\right] - T_{\\mathrm{tot}} \\log(A(1-\\rho) + B) + S_{\\mathrm{tot}} \\log(1-\\rho)\n$$\n令方括号中的项为 $K'$，它是一个相对于 $\\rho$ 的常数。对数似然中与 $\\rho$ 的优化相关的部分是：\n$$\nf(\\rho) = S_{\\mathrm{tot}} \\log(1-\\rho) - T_{\\mathrm{tot}} \\log(A(1-\\rho) + B)\n$$\n$H_0$ 下最大化对数似然的值可通过设置 $\\rho=0$ 找到：\n$$\n\\ell_0^\\star = \\ell(0) = K' + f(0) = K' - T_{\\mathrm{tot}} \\log(A+B)\n$$\n$H_1$ 下的最大化对数似然是 $\\ell(\\rho)$ 在区间 $\\rho \\in [0, 1)$ 上的上确界：\n$$\n\\ell_1^\\star = \\sup_{\\rho \\in [0, 1)} \\ell(\\rho) = K' + \\sup_{\\rho \\in [0, 1)} f(\\rho)\n$$\n决策准则为 $\\ell_1^\\star > \\ell_0^\\star$，这等价于 $\\sup_{\\rho \\in [0, 1)} f(\\rho) > f(0)$。\n\n为确定此条件何时成立，我们分析函数 $f(\\rho)$ 在边界点 $\\rho=0$ 处的行为。该函数的二阶导数为：\n$$\n\\frac{d^2 f}{d\\rho^2} = -\\frac{S_{\\mathrm{tot}}}{(1-\\rho)^2} - \\frac{A^2 T_{\\mathrm{tot}}}{(A(1-\\rho)+B)^2}\n$$\n因为 $S_{\\mathrm{tot}} \\ge 0$，$T_{\\mathrm{tot}} \\ge 0$，且 $A>0$，所以该二阶导数始终为非正。因此，$f(\\rho)$ 是一个凹函数。对于在 $[0,1)$ 上的凹函数，其上确界严格大于其在 $\\rho=0$ 处的值，当且仅当该函数在 $\\rho=0$ 处是严格递增的。这由一阶导数在 $\\rho=0$ 处的符号决定。\n$f(\\rho)$ 的一阶导数为：\n$$\n\\frac{df}{d\\rho} = \\frac{-S_{\\mathrm{tot}}}{1-\\rho} + \\frac{A \\cdot T_{\\mathrm{tot}}}{A(1-\\rho) + B}\n$$\n在 $\\rho=0$ 处求值：\n$$\n\\left. \\frac{df}{d\\rho} \\right|_{\\rho=0} = -S_{\\mathrm{tot}} + \\frac{A \\cdot T_{\\mathrm{tot}}}{A+B} = \\frac{-S_{\\mathrm{tot}}(A+B) + A(S_{\\mathrm{tot}} + C_{\\mathrm{tot}})}{A+B} = \\frac{A C_{\\mathrm{tot}} - B S_{\\mathrm{tot}}}{A+B}\n$$\n其中 $C_{\\mathrm{tot}} = T_{\\mathrm{tot}} - S_{\\mathrm{tot}} = \\sum_i \\sum_j C_{ij}$。\n条件 $\\ell_1^\\star > \\ell_0^\\star$ 等价于 $\\left. \\frac{df}{d\\rho} \\right|_{\\rho=0} > 0$。考虑到 $A+B > 0$，这可以简化为以下简洁的决策规则：\n$$\nA C_{\\mathrm{tot}} > B S_{\\mathrm{tot}}\n$$\n这个不等式可以直观地解释。将其重排为 $\\frac{S_{\\mathrm{tot}}}{A} < \\frac{C_{\\mathrm{tot}}}{B}$，这表明，如果病例样本中的标准化总计数严格小于对照组中的标准化总计数，那么缺失假设 ($H_1$) 更优。\n\n现在将此分析应用于所提供的测试用例，参数为 $A = L_s = 100$ 和 $B = L_{c1} + L_{c2} = 100 + 100 = 200$。决策规则为 $100 \\cdot C_{\\mathrm{tot}} > 200 \\cdot S_{\\mathrm{tot}}$，可简化为 $C_{\\mathrm{tot}} > 2 \\cdot S_{\\mathrm{tot}}$。\n\n- **用例 1**：\n  - $S = [60,55,65,60,60] \\implies S_{\\mathrm{tot}} = 300$。\n  - $C_{\\cdot 1} = [110,105,100,95,90] \\implies \\sum_i C_{i1} = 500$。\n  - $C_{\\cdot 2} = [110,105,100,95,90] \\implies \\sum_i C_{i2} = 500$。\n  - $C_{\\mathrm{tot}} = 500 + 500 = 1000$。\n  - 测试：$1000 > 2 \\cdot 300 \\implies 1000 > 600$。该不等式成立。结果：`True`。\n\n- **用例 2**：\n  - $S = [40,40,40,40,40] \\implies S_{\\mathrm{tot}} = 200$。\n  - $C_{\\cdot 1} = [40,40,40,40,40] \\implies \\sum_i C_{i1} = 200$。\n  - $C_{\\cdot 2} = [40,40,40,40,40] \\implies \\sum_i C_{i2} = 200$。\n  - $C_{\\mathrm{tot}} = 200 + 200 = 400$。\n  - 测试：$400 > 2 \\cdot 200 \\implies 400 > 400$。该不等式不成立。结果：`False`。\n\n- **用例 3**：\n  - $S = [0,0,0,0,0] \\implies S_{\\mathrm{tot}} = 0$。\n  - $C_{\\cdot 1} = [0,0,0,0,0] \\implies \\sum_i C_{i1} = 0$。\n  - $C_{\\cdot 2} = [0,0,0,0,0] \\implies \\sum_i C_{i2} = 0$。\n  - $C_{\\mathrm{tot}} = 0 + 0 = 0$。\n  - 测试：$0 > 2 \\cdot 0 \\implies 0 > 0$。该不等式不成立。结果：`False`。\n\n- **用例 4**：\n  - $S = [55,50,45,55,45] \\implies S_{\\mathrm{tot}} = 250$。\n  - $C_{\\cdot 1} = [60,50,55,90,50] \\implies \\sum_i C_{i1} = 305$。\n  - $C_{\\cdot 2} = [50,45,40,60,10] \\implies \\sum_i C_{i2} = 205$。\n  - $C_{\\mathrm{tot}} = 305 + 205 = 510$。\n  - 测试：$510 > 2 \\cdot 250 \\implies 510 > 500$。该不等式成立。结果：`True`。\n\n四个用例的汇总结果是 `[True, False, False, True]`。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CNV analysis decision problem for a fixed set of test cases.\n\n    The problem requires comparing two hypotheses, H0 (shared low coverage) and\n    H1 (mosaic deletion in a case sample), based on maximized log-likelihoods.\n    The decision rule simplifies to comparing the normalized total read counts\n    between the case and control cohorts.\n    \"\"\"\n    \n    # Define problem-wide parameters\n    L_s = 100.0  # Case sample library size (exposure)\n    L_c1 = 100.0 # Control 1 library size\n    L_c2 = 100.0 # Control 2 library size\n    \n    # Total exposures for case (A) and controls (B)\n    A = L_s\n    B = L_c1 + L_c2\n    \n    # Define the test cases from the problem statement.\n    # Each case is a dictionary containing count vectors for the case (S),\n    # control 1 (C1), and control 2 (C2).\n    test_cases = [\n        # Case 1: Putative mosaic deletion\n        {\n            \"S\": np.array([60, 55, 65, 60, 60]),\n            \"C1\": np.array([110, 105, 100, 95, 90]),\n            \"C2\": np.array([110, 105, 100, 95, 90])\n        },\n        # Case 2: Systematically low region\n        {\n            \"S\": np.array([40, 40, 40, 40, 40]),\n            \"C1\": np.array([40, 40, 40, 40, 40]),\n            \"C2\": np.array([40, 40, 40, 40, 40])\n        },\n        # Case 3: Extreme edge with vanishing counts\n        {\n            \"S\": np.array([0, 0, 0, 0, 0]),\n            \"C1\": np.array([0, 0, 0, 0, 0]),\n            \"C2\": np.array([0, 0, 0, 0, 0])\n        },\n        # Case 4: Subtle low-amplitude mosaic-like deviation\n        {\n            \"S\": np.array([55, 50, 45, 55, 45]),\n            \"C1\": np.array([60, 50, 55, 90, 50]),\n            \"C2\": np.array([50, 45, 40, 60, 10])\n        }\n    ]\n\n    results = []\n    \n    # As derived, the decision rule l1* > l0* simplifies to:\n    # A * C_tot > B * S_tot\n    # where S_tot and C_tot are total counts for the case and controls, respectively.\n    \n    for case in test_cases:\n        # Calculate total counts for the case sample.\n        S_tot = np.sum(case[\"S\"])\n        \n        # Calculate total counts for the control cohort.\n        C_tot = np.sum(case[\"C1\"]) + np.sum(case[\"C2\"])\n        \n        # Apply the decision rule.\n        # The hypothesis H1 (deletion) is favored if the inequality holds.\n        decision = A * C_tot > B * S_tot\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\n# Execute the solver.\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在实际分析中，仅依赖单一类型的信号来进行CNV检测可能会导致很高的假阳性率。最可靠的CNV鉴定来自于整合多种独立的证据，例如读深度、异常配对读段（discordant paired-end reads）和分裂读段（split reads）。这个综合性练习将演示如何构建一个贝叶斯模型来正式地结合这些不同的数据类型，从而计算每种拷贝数状态的后验概率，并做出最终的数据驱动决策。",
            "id": "2382738",
            "problem": "给定一个数学模型，该模型结合了来自三个独立的测序衍生信号的证据，用于拷贝数变异 (CNV) 分析：这些信号包括读取深度、不一致双末端比对计数和分裂读取计数。目标是计算离散拷贝数状态的后验分布，并报告几个测试用例的最大后验 (MAP) 状态及其后验概率。\n\n数学设定：\n- 令拷贝数 (CN) 状态为一个离散随机变量 $CN \\in \\{0,1,2,3,4\\}$。\n- 令一个基因组区域的观测数据为三元组 $(D,PE,SR)$，其中 $D$ 是标准化的读取深度比率（无单位），$PE$ 是不一致双末端比对的计数，而 $SR$ 是分裂读取的计数。\n- 假设在给定 $CN$ 的条件下，$D$、$PE$ 和 $SR$ 是条件独立的。\n- $CN$ 的先验分布由下式指定\n$$\nP(CN=0)=0.03,\\quad P(CN=1)=0.07,\\quad P(CN=2)=0.80,\\quad P(CN=3)=0.07,\\quad P(CN=4)=0.03.\n$$\n- 似然模型为：\n  1. 读取深度：$D \\mid CN=k \\sim \\mathcal{N}\\!\\left(\\mu_k,\\sigma^2\\right)$，其中 $\\mu_k = \\frac{k}{2}$ 且 $\\sigma^2 = 0.04$。\n  2. 不一致双末端比对：$PE \\mid CN=k \\sim \\mathrm{Pois}\\!\\left(\\lambda_{PE}(k)\\right)$，其中 $\\lambda_{PE}(k) = \\beta_{PE} + \\alpha_{PE}\\,\\lvert k-2 \\rvert$，且 $\\alpha_{PE}=3.0$，$\\beta_{PE}=0.1$。\n  3. 分裂读取：$SR \\mid CN=k \\sim \\mathrm{Pois}\\!\\left(\\lambda_{SR}(k)\\right)$，其中 $\\lambda_{SR}(k) = \\beta_{SR} + \\alpha_{SR}\\,\\lvert k-2 \\rvert$，且 $\\alpha_{SR}=2.0$，$\\beta_{SR}=0.05$。\n\n贝叶斯目标：\n- 对于每个观测到的三元组 $(D,PE,SR)$，计算后验\n$$\nP(CN=k \\mid D,PE,SR) \\propto P(D \\mid CN=k)\\;P(PE \\mid CN=k)\\;P(SR \\mid CN=k)\\;P(CN=k),\n$$\n对 $k \\in \\{0,1,2,3,4\\}$ 进行归一化，以获得一个总和为 $1$ 的有效概率分布。\n- 对于每个案例，报告 MAP 估计\n$$\n\\hat{k} = \\arg\\max_{k \\in \\{0,1,2,3,4\\}} P(CN=k \\mid D,PE,SR),\n$$\n以及相应的后验概率 $P(CN=\\hat{k} \\mid D,PE,SR)$。\n\n测试用例集：\n- 使用以下 $5$ 个观测三元组 $(D,PE,SR)$：\n  1. $(0.52, 4, 3)$\n  2. $(1.47, 3, 2)$\n  3. $(1.02, 0, 0)$\n  4. $(0.08, 7, 5)$\n  5. $(0.78, 1, 1)$\n\n答案规格：\n- 对于每个测试用例，输出整数 $\\hat{k}$ 和浮点数 $P(CN=\\hat{k} \\mid D,PE,SR)$，四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含按测试用例顺序连接的结果，形式为方括号括起来的逗号分隔列表。具体来说，输出格式必须是\n$$\n[\\hat{k}_1, p_1, \\hat{k}_2, p_2, \\hat{k}_3, p_3, \\hat{k}_4, p_4, \\hat{k}_5, p_5],\n$$\n其中 $p_i$ 表示四舍五入到 $6$ 位小数的 $P(CN=\\hat{k}_i \\mid D_i,PE_i,SR_i)$。不应打印额外的文本或行。",
            "solution": "该问题要求在给定三个独立的基因组数据来源（标准化读取深度 $D$、不一致双末端比对计数 $PE$ 和分裂读取计数 $SR$）的情况下，计算离散拷贝数状态 $CN$ 的后验概率分布。目标是为一组给定的观测值确定最大后验 (MAP) 状态及其相关的后验概率。\n\n该分析框架基于贝叶斯定理。拷贝数状态 $CN$ 是一个离散随机变量，可在集合 $k \\in \\{0, 1, 2, 3, 4\\}$ 中取值。对于每个潜在状态 $k$，我们旨在计算后验概率 $P(CN=k \\mid D, PE, SR)$。根据贝叶斯定理，其公式为：\n$$ P(CN=k \\mid D, PE, SR) = \\frac{P(D, PE, SR \\mid CN=k) P(CN=k)}{P(D, PE, SR)} $$\n项 $P(CN=k)$ 是观测到拷贝数状态 $k$ 的先验概率。问题中提供了这些先验概率：$P(CN=0)=0.03$，$P(CN=1)=0.07$，$P(CN=2)=0.80$，$P(CN=3)=0.07$ 和 $P(CN=4)=0.03$。\n\n项 $P(D, PE, SR \\mid CN=k)$ 是在给定状态 $k$ 时观测到数据的似然。问题指出，数据源 $D$、$PE$ 和 $SR$ 在给定 $CN$ 的条件下是条件独立的。这使我们能够对似然进行分解：\n$$ P(D, PE, SR \\mid CN=k) = P(D \\mid CN=k) \\times P(PE \\mid CN=k) \\times P(SR \\mid CN=k) $$\n这些单独的似然项中的每一项都由一个特定的统计模型定义：\n\n1.  读取深度似然, $P(D \\mid CN=k)$：标准化的读取深度 $D$ 由高斯（正态）分布建模，$D \\mid CN=k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$。其概率密度函数为：\n    $$ P(D \\mid CN=k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(D - \\mu_k)^2}{2\\sigma^2}\\right) $$\n    参数给定为 $\\mu_k = \\frac{k}{2}$，方差 $\\sigma^2 = 0.04$。\n\n2.  不一致双末端比对似然, $P(PE \\mid CN=k)$：不一致双末端比对的计数 $PE$ 由泊松分布建模，$PE \\mid CN=k \\sim \\mathrm{Pois}(\\lambda_{PE}(k))$。其概率质量函数为：\n    $$ P(PE \\mid CN=k) = \\frac{\\lambda_{PE}(k)^{PE} e^{-\\lambda_{PE}(k)}}{PE!} $$\n    率参数 $\\lambda_{PE}(k)$ 是拷贝数状态 $k$ 的函数，定义为 $\\lambda_{PE}(k) = \\beta_{PE} + \\alpha_{PE}\\,\\lvert k-2 \\rvert$，其中常数 $\\alpha_{PE} = 3.0$ 和 $\\beta_{PE} = 0.1$。\n\n3.  分裂读取似然, $P(SR \\mid CN=k)$：类似地，分裂读取计数 $SR$ 由泊松分布建模，$SR \\mid CN=k \\sim \\mathrm{Pois}(\\lambda_{SR}(k))$。其概率质量函数为：\n    $$ P(SR \\mid CN=k) = \\frac{\\lambda_{SR}(k)^{SR} e^{-\\lambda_{SR}(k)}}{SR!} $$\n    率参数为 $\\lambda_{SR}(k) = \\beta_{SR} + \\alpha_{SR}\\,\\lvert k-2 \\rvert$，其中常数 $\\alpha_{SR} = 2.0$ 和 $\\beta_{SR} = 0.05$。\n\n贝叶斯定理中的分母 $P(D, PE, SR)$ 是边缘似然或“证据”。它作为一个归一化常数，确保所有可能的 $k$ 状态的后验概率之和为 $1$。它通过对所有状态的未归一化后验概率求和来计算：\n$$ P(D, PE, SR) = \\sum_{j=0}^{4} P(D, PE, SR \\mid CN=j) P(CN=j) $$\n因此，对于每个状态 $k$，完整的后验概率是：\n$$ P(CN=k \\mid D, PE, SR) = \\frac{P(D \\mid CN=k) P(PE \\mid CN=k) P(SR \\mid CN=k) P(CN=k)}{\\sum_{j=0}^{4} P(D \\mid CN=j) P(PE \\mid CN=j) P(SR \\mid CN=j) P(CN=j)} $$\n对于 $5$ 个测试用例中的每一个，算法流程如下：\n1.  对于每个拷贝数状态 $k \\in \\{0, 1, 2, 3, 4\\}$，计算未归一化的后验概率，即先验概率与在给定数据 $(D, PE, SR)$ 下评估的三个似然的乘积。\n2.  将所有 $k$ 的这些未归一化后验值相加，以计算归一化常数（证据）。\n3.  将每个未归一化的后验值除以证据，以获得每个状态 $k$ 的归一化后验概率。\n4.  确定具有最大后验概率的状态 $\\hat{k}$。这就是 MAP 估计：$\\hat{k} = \\arg\\max_{k} P(CN=k \\mid D, PE, SR)$。\n5.  报告整数 MAP 状态 $\\hat{k}$ 及其对应的后验概率，四舍五入到 $6$ 位小数。对所有 $5$ 个提供的测试用例重复此过程。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, poisson\n\ndef solve():\n    \"\"\"\n    Computes the MAP estimate and posterior probability for CNV states.\n    \"\"\"\n    # Define the parameter space and models as specified in the problem.\n    cn_states = np.array([0, 1, 2, 3, 4])\n    priors = np.array([0.03, 0.07, 0.80, 0.07, 0.03])\n\n    # 1. Read depth model: D | CN=k ~ N(mu_k, sigma^2)\n    rd_sigma_sq = 0.04\n    rd_sigma = np.sqrt(rd_sigma_sq)\n    rd_mu_k = cn_states / 2.0\n\n    # 2. Discordant paired-end model: PE | CN=k ~ Pois(lambda_pe(k))\n    pe_alpha = 3.0\n    pe_beta = 0.1\n    pe_lambda_k = pe_beta + pe_alpha * np.abs(cn_states - 2)\n\n    # 3. Split-read model: SR | CN=k ~ Pois(lambda_sr(k))\n    sr_alpha = 2.0\n    sr_beta = 0.05\n    sr_lambda_k = sr_beta + sr_alpha * np.abs(cn_states - 2)\n\n    # Test suite of observed data triples (D, PE, SR)\n    test_cases = [\n        (0.52, 4, 3),\n        (1.47, 3, 2),\n        (1.02, 0, 0),\n        (0.08, 7, 5),\n        (0.78, 1, 1),\n    ]\n\n    results = []\n    for D, PE, SR in test_cases:\n        # Array to hold the unnormalized posterior for each CN state k\n        unnormalized_posteriors = np.zeros_like(cn_states, dtype=float)\n\n        for i, k in enumerate(cn_states):\n            # Calculate the likelihood for each data type\n            likelihood_D = norm.pdf(D, loc=rd_mu_k[i], scale=rd_sigma)\n            likelihood_PE = poisson.pmf(PE, mu=pe_lambda_k[i])\n            likelihood_SR = poisson.pmf(SR, mu=sr_lambda_k[i])\n\n            # Combine likelihoods and prior based on conditional independence\n            # This is the unnormalized posterior (proportional to joint probability)\n            unnormalized_posteriors[i] = likelihood_D * likelihood_PE * likelihood_SR * priors[i]\n\n        # Normalize to get the posterior distribution\n        evidence = np.sum(unnormalized_posteriors)\n        \n        # Handle the case where all posteriors are zero (e.g., underflow),\n        # though unlikely with these parameters.\n        if evidence > 0:\n            posterior_probs = unnormalized_posteriors / evidence\n        else:\n            # If evidence is zero, probabilities cannot be determined,\n            # though this indicates an issue. A uniform distribution is a fallback.\n            posterior_probs = np.full_like(cn_states, 1.0 / len(cn_states), dtype=float)\n\n        # Find the MAP estimate (state with the highest posterior probability)\n        map_k_index = np.argmax(posterior_probs)\n        map_k = cn_states[map_k_index]\n        map_prob = posterior_probs[map_k_index]\n\n        # Append results to the list\n        results.append(int(map_k))\n        results.append(f\"{map_prob:.6f}\")\n\n    # Format the final output string as per the specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}