{
    "hands_on_practices": [
        {
            "introduction": "The stored-program concept is not merely an abstract principle; it has direct, physical consequences for system performance. Before any program can run, its instructions must be loaded into memory as data, a process we know as booting. This exercise  grounds the concept in reality by modeling the boot loading process, allowing you to calculate the time required based on fundamental hardware parameters like memory bus width and clock frequency. By working through this, you will gain a concrete understanding of how system start-up time is fundamentally limited by memory bandwidth.",
            "id": "3682329",
            "problem": "A stored-program system implements the idea that instructions are data residing in memory, and the central processing unit (CPU) fetches them from memory to execute. Consider a minimal boot loader for such a system that copies an instruction image from nonvolatile memory (NVM) into random-access memory (RAM) over a synchronous bus before the CPU begins fetching instructions from RAM. The bus has width $W=128$ bits, the CPU clock frequency is $f=800 \\times 10^{6}$ cycles per second, and the loader issues contiguous burst reads of $B=16$ bus words per burst. Each burst incurs a fixed setup latency of $L=12$ cycles, after which one bus word arrives per cycle until the burst completes. The instruction image size is $N=64\\,\\text{MiB}$, where $1\\,\\text{MiB}=2^{20}\\,\\text{bytes}$. Assume the loader issues the minimal number of bursts to cover $N$ bytes; if the final burst would be partial, it is still issued and incurs the same setup latency.\n\nStarting only from core definitions (for example, bytes per bus word are determined by $W$, cycles per second are determined by $f$, and time equals cycles divided by $f$), derive the total boot time as a function of $N$, $W$, $B$, $L$, and $f$, and then evaluate it numerically for the given parameters. Express the final boot time in seconds and round your answer to four significant figures. In your derivation, make clear how the stored-program concept makes start-up time depend on memory bandwidth and transfer granularity without invoking any pre-provided shortcut formulas.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of computer architecture, is well-posed with sufficient and consistent information, and is objective. The scenario described is a standard boot-loading process, and the parameters provided are realistic.\n\nThe solution begins by establishing the connection between the stored-program concept and the physical task of loading an instruction image. The stored-program concept is the cornerstone of modern computer architecture, stating that instructions, just like data, are stored in memory. For a central processing unit (CPU) to execute a program, the program's instructions must first reside in a type of memory from which the CPU can fetch them, typically random-access memory (RAM). The problem describes the boot process where an initial instruction image is copied from a non-volatile memory (NVM) source to RAM. This transfer is a mandatory prerequisite for execution, and its duration—the boot time—is therefore a critical performance metric. This time is directly dependent on the physical characteristics of the memory subsystem, such as the data path width and transfer protocol, which we will now model from first principles.\n\nFirst, we must determine the total amount of data to be transferred, expressed in a base unit of bits. The instruction image size is given as $N = 64\\,\\text{MiB}$. Using the provided definition $1\\,\\text{MiB} = 2^{20}\\,\\text{bytes}$ and the core definition of a byte as $8$ bits, we find the total image size in bits, $N_{bits}$:\n$$N_{bits} = N \\times (2^{20}\\ \\text{bytes/MiB}) \\times (8\\ \\text{bits/byte}) = 64 \\times 2^{20} \\times 8\\ \\text{bits}$$\n\nThe data is transferred over a synchronous bus with a width of $W = 128$ bits. Each transfer on the bus moves one \"bus word\" of $W$ bits. The total number of bus words, $T_{words}$, that must be transferred to copy the entire image is the total size in bits divided by the bus width in bits:\n$$T_{words} = \\frac{N_{bits}}{W} = \\frac{N \\times 2^{20} \\times 8}{W}$$\nSubstituting the given values, $N=64$ and $W=128$:\n$$T_{words} = \\frac{64 \\times 2^{20} \\times 8}{128} = \\frac{512 \\times 2^{20}}{128} = 4 \\times 2^{20} = 4 \\times 1048576 = 4194304\\ \\text{words}$$\n\nThe loader issues contiguous burst reads of $B = 16$ bus words per burst. To find the total number of bursts required, $N_{bursts}$, we divide the total number of words, $T_{words}$, by the number of words per burst, $B$. Since the problem specifies that even a partial final burst is issued as a full operation (incurring the same setup latency), we must take the ceiling of this division:\n$$N_{bursts} = \\left\\lceil \\frac{T_{words}}{B} \\right\\rceil$$\nSubstituting the calculated value for $T_{words}$ and the given value $B=16$:\n$$N_{bursts} = \\left\\lceil \\frac{4194304}{16} \\right\\rceil = \\lceil 262144 \\rceil = 262144\\ \\text{bursts}$$\nIn this specific case, the total number of words is an integer multiple of the burst size, so there are no partial bursts. However, the general formula must include the ceiling function.\n\nNext, we calculate the total time in clock cycles. The total time is the sum of two components: the time spent on data transfer and the cumulative time spent on setup latencies for all bursts.\nEach burst, regardless of being full or partial, incurs a setup latency of $L = 12$ cycles. With $N_{bursts}$ bursts in total, the total time spent on setup is:\n$$C_{latency} = N_{bursts} \\times L$$\nAfter the initial latency for a burst, one bus word arrives per cycle. Therefore, the total time spent transferring the actual data is equal to the total number of words to be transferred, $T_{words}$.\n$$C_{transfer} = T_{words}$$\nThe total time in clock cycles, $C_{total}$, is the sum of these two components:\n$$C_{total} = C_{latency} + C_{transfer} = (N_{bursts} \\times L) + T_{words}$$\n\nSubstituting the numerical values we have calculated: $N_{bursts} = 262144$, $L=12$, and $T_{words} = 4194304$:\n$$C_{total} = (262144 \\times 12) + 4194304 = 3145728 + 4194304 = 7340032\\ \\text{cycles}$$\n\nTo find the total boot time in seconds, $T_{boot}$, we divide the total number of cycles, $C_{total}$, by the CPU clock frequency, $f = 800 \\times 10^{6}$ cycles per second.\n$$T_{boot} = \\frac{C_{total}}{f}$$\nSubstituting the values:\n$$T_{boot} = \\frac{7340032}{800 \\times 10^{6}} = \\frac{7340032}{8 \\times 10^{8}} = 0.00917504\\ \\text{seconds}$$\n\nThe problem requires the answer to be rounded to four significant figures.\n$$T_{boot} \\approx 0.009175\\ \\text{seconds}$$\nThis can be written in scientific notation as $9.175 \\times 10^{-3}$ seconds.\n\nFinally, we express the total boot time as a function of the given parameters $N$, $W$, $B$, $L$, and $f$.\n$$T_{boot}(N, W, B, L, f) = \\frac{C_{total}}{f} = \\frac{1}{f} \\left( L \\cdot N_{bursts} + T_{words} \\right)$$\nSubstituting the expressions for $N_{bursts}$ and $T_{words}$:\n$$T_{words} = \\frac{8 \\cdot N \\cdot 2^{20}}{W}$$\n$$N_{bursts} = \\left\\lceil \\frac{T_{words}}{B} \\right\\rceil = \\left\\lceil \\frac{8 \\cdot N \\cdot 2^{20}}{W \\cdot B} \\right\\rceil$$\nThus, the general symbolic expression for the boot time is:\n$$T_{boot} = \\frac{1}{f} \\left( L \\cdot \\left\\lceil \\frac{8 \\cdot N \\cdot 2^{20}}{W \\cdot B} \\right\\rceil + \\frac{8 \\cdot N \\cdot 2^{20}}{W} \\right)$$\nThis formula explicitly demonstrates how the boot time, a direct consequence of implementing the stored-program concept, is determined by the image size ($N$), memory bus bandwidth (related to $W$ and $f$), and transfer granularity/overhead (related to $B$ and $L$).",
            "answer": "$$\\boxed{9.175 \\times 10^{-3}}$$"
        },
        {
            "introduction": "If a program's instructions are simply data stored in memory, can we load that program anywhere we want without breaking it? This question leads to the powerful technique of Position-Independent Code (PIC), which is essential for modern operating systems, shared libraries, and dynamic loading. This practice  explores how a processor's architecture, specifically through PC-relative addressing, makes this flexibility possible. You will analyze how target addresses are calculated relative to the program counter to ensure that code can be relocated in memory without needing any modification.",
            "id": "3682297",
            "problem": "A reduced instruction set computing processor implements the stored-program concept: both instructions and data share a single linear byte-addressed memory, and the processor uses a Program Counter (PC) to fetch the next instruction. Consider Position-Independent Code (PIC), in which control-flow targets are expressed relative to the Program Counter (PC) rather than as absolute memory addresses.\n\nStart from the following base facts and definitions only: the stored-program concept, the definition of the Program Counter (PC) as the register holding the address of the instruction to be fetched next, the notion of a translation (relocation) by a constant address increment $\\Delta$ applied uniformly to all code and data addresses, and the existence of a signed displacement field embedded in an instruction that is expressed in units of one instruction. Do not assume any pre-derived formula for a PC-relative target.\n\nYou are given a specific in-order pipeline and encoding model:\n- The pipeline has $n=7$ stages numbered from $1$ to $7$, where stage $1$ is instruction fetch and stage $4$ is the arithmetic/logic execute stage where control-flow effective addresses are formed.\n- The instruction width is $w=4$ bytes.\n- The Program Counter (PC) is incremented by $w$ at stage $1$ (fetch) on each cycle. When an instruction is in stage $4$ (execute), the value that stage $4$ observes as the PC corresponds to the fetch unit having advanced by exactly $\\kappa$ whole instructions ahead of the address of the instruction in execute. In this microarchitecture, the execute stage is stage $4$, so $\\kappa=3$.\n- A control-flow instruction contains a signed displacement $\\delta$ encoded in units of one instruction, meaning that the displacement is measured in multiples of $w$ bytes.\n\nPart A (conceptual): Using only the base facts stated above, argue why forming a control-flow target from the Program Counter (PC) and an embedded signed displacement yields translation invariance under relocation by any constant $\\Delta$. That is, if all code is shifted by $\\Delta$ in memory, explain why the computed target address for a given instruction shifts by exactly the same $\\Delta$ without modifying the instruction encoding, as required by Position-Independent Code (PIC).\n\nPart B (calculation): Consider a control-flow instruction after relocation whose own address is $A = L + o$, where the load base is $L = 0x400000$ and the instruction’s offset within its code segment is $o = 0x160$. The signed displacement field is $\\delta=-5$. Under the pipeline semantics above, determine the exact byte address of the control-flow target as observed by the execute stage, expressed as a base-$10$ integer. No rounding is required; give the exact integer value. Do not include any unit symbols in your final numeric answer.",
            "solution": "The problem as stated is valid. It is scientifically grounded in the principles of computer organization and architecture, specifically the stored-program concept, pipelined execution, and position-independent code (PIC). The problem is well-posed, providing all necessary definitions, constraints, and data for a unique solution. The language is objective and precise.\n\nThis solution is divided into two parts, corresponding to the two parts of the problem statement.\n\n### Part A: Conceptual Argument for Translation Invariance\n\nThe core principle of Position-Independent Code (PIC) is that the memory addresses for control-flow targets and data accesses are computed relative to the current execution location, rather than being hard-coded as absolute addresses. This allows a block of code to be loaded and executed from any location in memory without modification. We will demonstrate how a PC-relative addressing scheme achieves this property of translation invariance.\n\nLet $A_{instr}$ be the absolute memory address of a given control-flow instruction.\nAccording to the problem's pipeline model, when this instruction is in the execute stage (stage $i=4$), the Program Counter (PC) has been updated multiple times. The PC always holds the address of the *next instruction to be fetched* (at stage $1$). Given that the instruction at $A_{instr}$ is in stage $4$, the pipeline has advanced by $4-1=3$ cycles since it was fetched. The problem defines this lookahead as $\\kappa = 3$ instructions.\nThe instruction width is given as $w$ bytes. Therefore, the value of the PC observed by the execute stage, denoted as $PC_{exec}$, is the address of the instruction that is $\\kappa$ instructions ahead of the one currently executing.\n$$PC_{exec} = A_{instr} + \\kappa \\cdot w$$\n\nThe control-flow instruction contains an embedded signed displacement, $\\delta$, which is expressed in units of one instruction. To compute the byte offset, this displacement must be multiplied by the instruction width, $w$. The target address, $A_{target}$, is then computed by adding this byte offset to the PC value observed at the execute stage.\n$$A_{target} = PC_{exec} + \\delta \\cdot w$$\nSubstituting the expression for $PC_{exec}$, we get:\n$$A_{target} = (A_{instr} + \\kappa \\cdot w) + \\delta \\cdot w$$\n\nNow, let us consider the effect of a memory relocation. Suppose the entire block of code and data is shifted (translated) in memory by a constant address increment $\\Delta$. Each original address $X$ is mapped to a new address $X' = X + \\Delta$.\n\nThe new address of our control-flow instruction becomes:\n$$A'_{instr} = A_{instr} + \\Delta$$\n\nThe instruction itself is not modified; its binary encoding, including the displacement field $\\delta$, remains unchanged. When this relocated instruction is in the execute stage, the processor computes the target address using the same logic as before, but based on the new, relocated addresses. The PC value observed at the execute stage, $PC'_{exec}$, will be relative to the new instruction address $A'_{instr}$:\n$$PC'_{exec} = A'_{instr} + \\kappa \\cdot w = (A_{instr} + \\Delta) + \\kappa \\cdot w$$\n\nThe new target address, $A'_{target}$, is computed using this new PC value and the unchanged displacement:\n$$A'_{target} = PC'_{exec} + \\delta \\cdot w = ((A_{instr} + \\Delta) + \\kappa \\cdot w) + \\delta \\cdot w$$\nBy rearranging the terms, we can isolate the relocation offset $\\Delta$:\n$$A'_{target} = (A_{instr} + \\kappa \\cdot w + \\delta \\cdot w) + \\Delta$$\nThe expression in the parenthesis is precisely the original target address, $A_{target}$. Therefore, we have:\n$$A'_{target} = A_{target} + \\Delta$$\n\nThis result demonstrates that a relocation of the code by a constant $\\Delta$ results in the computed target address being relocated by the exact same constant $\\Delta$. This is achieved without any modification to the instruction's encoded displacement $\\delta$. The difference between the target address and the instruction's own address, $A_{target} - A_{instr} = (\\kappa + \\delta)w$, is a constant determined by the instruction encoding and the pipeline architecture, not the absolute location of the instruction. This is the essence of translation invariance and the functionality of Position-Independent Code.\n\n### Part B: Calculation of the Control-Flow Target Address\n\nWe will use the formula derived in Part A to calculate the exact byte address of the control-flow target. The formula is:\n$$A_{target} = A_{instr} + (\\kappa + \\delta) \\cdot w$$\n\nFirst, we must determine the absolute address of the control-flow instruction, $A_{instr}$. The problem states it is located at an address $A = L + o$, where the load base is $L = 0x400000$ and the offset is $o = 0x160$. We must convert these hexadecimal values to base-$10$.\n\nThe load base $L$ is:\n$$L = 0x400000 = 4 \\times 16^{5} = 4 \\times 1048576 = 4194304$$\n\nThe offset $o$ is:\n$$o = 0x160 = 1 \\times 16^{2} + 6 \\times 16^{1} + 0 \\times 16^{0} = 256 + 96 + 0 = 352$$\n\nThe absolute address of the instruction, $A_{instr}$, is the sum of these two values:\n$$A_{instr} = L + o = 4194304 + 352 = 4194656$$\n\nNext, we assemble the remaining parameters provided in the problem statement:\n- Pipeline lookahead instructions: $\\kappa = 3$.\n- Signed displacement in instructions: $\\delta = -5$.\n- Instruction width in bytes: $w = 4$.\n\nNow, we substitute these values into the formula for the target address $A_{target}$:\n$$A_{target} = 4194656 + (3 + (-5)) \\cdot 4$$\n$$A_{target} = 4194656 + (-2) \\cdot 4$$\n$$A_{target} = 4194656 - 8$$\n$$A_{target} = 4194648$$\n\nThus, the exact byte address of the control-flow target is $4194648$.",
            "answer": "$$\\boxed{4194648}$$"
        },
        {
            "introduction": "What happens when we push the \"instructions as data\" duality to its modern limit in a multi-core environment? This scenario, common in Just-In-Time (JIT) compilers, involves one processor core generating machine code as data and a second core executing it. This practice  delves into the complex synchronization required to ensure this process works correctly. You will explore the critical roles of memory barriers and cache management primitives, revealing how a foundational computing concept creates profound challenges and solutions in contemporary parallel systems.",
            "id": "3682322",
            "problem": "Consider a symmetric multiprocessor system with $2$ cores, Core $0$ and Core $1$, within a single Central Processing Unit (CPU). The system implements cache coherence for the shared physical memory but uses separate Instruction Cache (I-cache) and Data Cache (D-cache) per core without automatic instruction-data coherence. That is, writes performed through a D-cache are not guaranteed to invalidate or update any I-cache without explicit action. The Translation Lookaside Buffer (TLB) mappings remain unchanged during this scenario. Core $0$ generates machine code for a function $F$ by writing $N$ bytes into a shared, page-aligned region $[P, P+N)$, and then signals availability by writing a ready-flag value to address $Q$. Core $1$ later branches to address $P$ to execute $F$.\n\nAvailable primitives and their semantics:\n- $\\mathrm{flushD}(R)$: Ensures all previous stores to region $R$ performed by the calling core have been written back from the D-cache to the coherence point and are visible to other cores’ memory hierarchy.\n- $\\mathrm{mb\\_full}()$: A full, system-wide memory barrier. It orders all prior memory operations before all subsequent memory operations across cores, establishing a happens-before relation when combined with inter-core synchronization via a flag.\n- $\\mathrm{invalidateI}(R)$: Invalidates I-cache lines covering region $R$ in the calling core, so subsequent instruction fetches will refetch from the coherence point.\n- $\\mathrm{isync}()$: An instruction synchronization barrier that ensures any subsequent instruction fetch and decode observes the effects of prior I-cache invalidations and memory-ordering operations.\n\nAssume that:\n- Prior to Core $1$ executing $F$, Core $1$ may have stale I-cache lines for $[P, P+N)$.\n- Writes by Core $0$ initially reside in Core $0$’s D-cache and may not be globally visible until explicitly flushed.\n- The ready-flag at $Q$ is polled by Core $1$ and used to coordinate when to proceed.\n\nUnder these assumptions and the stored-program concept, where instructions are data stored in memory and fetched as byte sequences, choose the single sequence of operations that guarantees Core $1$ will execute the newly generated function $F$ correctly and deterministically on first attempt.\n\nOptions:\n\nA. Core $0$: write $F$ into $[P,P+N)$; $\\mathrm{flushD}([P,P+N))$; $\\mathrm{mb\\_full}()$; store ready-flag value $1$ to $Q$. Core $1$: spin until load from $Q$ returns $1$; $\\mathrm{invalidateI}([P,P+N))$; $\\mathrm{isync}()$; branch to $P$.\n\nB. Core $0$: write $F$ into $[P,P+N)$; store ready-flag value $1$ to $Q$. Core $1$: spin until load from $Q$ returns $1$; $\\mathrm{mb\\_full}()$; branch to $P$.\n\nC. Core $0$: $\\mathrm{invalidateI}([P,P+N))$; write $F$ into $[P,P+N)$; $\\mathrm{mb\\_full}()$; store ready-flag value $1$ to $Q$. Core $1$: spin until load from $Q$ returns $1$; branch to $P$.\n\nD. Core $0$: write $F$ into $[P,P+N)$; $\\mathrm{flushD}([P,P+N))$; store ready-flag value $1$ to $Q$. Core $1$: spin until load from $Q$ returns $1$; $\\mathrm{isync}()$; branch to $P$.\n\nSelect the option that, by correct ordering of $\\mathrm{flush}$ and $\\mathrm{barrier}$ primitives, guarantees correctness for Core $1$ executing the code generated by Core $0$.",
            "solution": "The problem statement describes a classic scenario in multi-core systems programming involving dynamic code generation, often encountered in Just-In-Time (JIT) compilation. The core challenge stems from the separation of Instruction Caches (I-caches) and Data Caches (D-caches) without automatic hardware-managed coherence between them. This requires an explicit sequence of software operations to ensure that code written as data by one core is correctly executed as instructions by another.\n\nThe problem is determined to be valid as it is scientifically grounded in the principles of computer organization and architecture, well-posed, and objective. It accurately models the complexities of modern processors regarding cache coherence and memory ordering.\n\nTo ensure Core $1$ correctly executes the function $F$ generated by Core $0$, a precise sequence of events must be enforced. Let us derive this sequence from first principles.\n\n**Core $0$ (The Writer/Producer):**\n1.  **Code Generation:** Core $0$ writes the machine code for function $F$ into the memory region $[P, P+N)$. These write operations modify lines in Core $0$'s private D-cache. At this point, the new code is not visible to Core $1$ or even to the main memory system.\n2.  **Data Visibility:** The newly written code must be propagated from Core $0$'s private D-cache to the point of coherence (e.g., a shared last-level cache or main memory), from where Core $1$ can fetch it. The primitive $\\mathrm{flushD}([P, P+N))$ accomplishes this, ensuring the data is \"written back.\"\n3.  **Memory Ordering:** Modern processors may reorder memory operations for performance. It is critical that the writes of the function's code (made visible by the flush) are observed globally *before* the ready-flag at address $Q$ is updated. If Core $1$ sees the flag before the code is available, it will attempt to execute stale or garbage instructions. A full memory barrier, $\\mathrm{mb\\_full}()$, is required to enforce this ordering. It guarantees that all prior memory operations (including the effects of the $\\mathrm{flushD}$) complete and become globally visible before any subsequent memory operations (the store to $Q$) are performed.\n4.  **Synchronization Signal:** After ensuring the code is visible and ordered, Core $0$ writes to the ready-flag at address $Q$. This store acts as a signal to Core $1$.\n\nTherefore, the correct, deterministic sequence for Core $0$ is:\n- write $F$ into $[P,P+N)$\n- $\\mathrm{flushD}([P,P+N))$\n- $\\mathrm{mb\\_full}()$\n- store ready-flag value $1$ to $Q$\n\n**Core $1$ (The Executor/Consumer):**\n1.  **Synchronization Wait:** Core $1$ spins, repeatedly loading the value from address $Q$ until it reads the ready value ($1$). The system's base cache coherence for data ensures that the store by Core $0$ will eventually be observed by Core $1$.\n2.  **Instruction Cache Coherence:** Upon reading the ready-flag, Core $1$ knows that the correct machine code for $F$ is available at the point of coherence. However, its own I-cache may hold stale instructions for the address range $[P, P+N)$. The problem states there is no automatic I-D coherence. Therefore, Core $1$ must explicitly invalidate these stale I-cache entries. The primitive $\\mathrm{invalidateI}([P,P+N))$ performs this action on the calling core's I-cache.\n3.  **Instruction Pipeline Synchronization:** Invalidating I-cache lines is not sufficient on its own. The processor's instruction fetch pipeline might have already fetched and decoded instructions from the stale I-cache *before* the invalidation completed. An instruction synchronization barrier, $\\mathrm{isync}()$, is required to flush the pipeline and ensure that any instructions executed after the barrier are fetched anew from the memory hierarchy (which will now miss in the I-cache and fetch the correct code).\n4.  **Execution:** With its I-cache and instruction pipeline properly synchronized, Core $1$ can now safely branch to address $P$ and begin executing the correct, newly-generated function $F$.\n\nTherefore, the correct, deterministic sequence for Core $1$ is:\n- spin until load from $Q$ returns $1$\n- $\\mathrm{invalidateI}([P,P+N))$\n- $\\mathrm{isync}()$\n- branch to $P$\n\nCombining these two sequences gives the complete, correct protocol. Let us evaluate the options based on this derivation.\n\n**Option A:**\n- Core $0$: write $F$ into $[P,P+N)$; $\\mathrm{flushD}([P,P+N))$; $\\mathrm{mb\\_full}()$; store ready-flag value $1$ to $Q$.\n- Core $1$: spin until load from $Q$ returns $1$; $\\mathrm{invalidateI}([P,P+N))$; $\\mathrm{isync}()$; branch to $P$.\nThis sequence perfectly matches the one derived from first principles. Core $0$ correctly ensures data visibility and ordering. Core $1$ correctly ensures instruction cache coherence and pipeline synchronization.\n**Verdict: Correct**\n\n**Option B:**\n- Core $0$: write $F$ into $[P,P+N)$; store ready-flag value $1$ to $Q$.\nThis is flawed. It omits $\\mathrm{flushD}()$, so the code may never leave Core $0$'s D-cache. It also omits $\\mathrm{mb\\_full}()$, allowing the flag-store to be reordered before the code-writes are visible, creating a race condition.\n- Core $1$: spin until load from $Q$ returns $1$; $\\mathrm{mb\\_full}()$; branch to $P$.\nThis is flawed. It omits the crucial $\\mathrm{invalidateI}()$ and $\\mathrm{isync}()$ steps, meaning Core $1$ would likely execute stale instructions from its I-cache.\n**Verdict: Incorrect**\n\n**Option C:**\n- Core $0$: $\\mathrm{invalidateI}([P,P+N))$; write $F$ into $[P,P+N)$; $\\mathrm{mb\\_full}()$; store ready-flag value $1$ to $Q$.\nThis is flawed. The $\\mathrm{invalidateI}()$ on Core $0$ is irrelevant, as Core $0$ is not executing the code. The critical $\\mathrm{flushD}()$ operation is missing, so the code written by Core $0$ is not guaranteed to be visible to Core $1$.\n- Core $1$: spin until load from $Q$ returns $1$; branch to $P$.\nThis is flawed. It omits both $\\mathrm{invalidateI}()$ and $\\mathrm{isync}()$, failing to address the stale I-cache problem on Core $1$.\n**Verdict: Incorrect**\n\n**Option D:**\n- Core $0$: write $F$ into $[P,P+N)$; $\\mathrm{flushD}([P,P+N))$; store ready-flag value $1$ to $Q$.\nThis is flawed. It omits the $\\mathrm{mb\\_full}()$ between the data flush and the flag store. On a weakly-ordered architecture, this allows the flag to become visible to Core $1$ before the flushed code data is guaranteed to be visible, leading to a race condition.\n- Core $1$: spin until load from $Q$ returns $1$; $\\mathrm{isync}()$; branch to $P$.\nThis is flawed. It omits the $\\mathrm{invalidateI}()$. The $\\mathrm{isync}()$ primitive synchronizes the pipeline with respect to prior cache maintenance operations; without a preceding $\\mathrm{invalidateI}()$, it has no relevant effect on I-cache contents. Core $1$ would still be at risk of using stale cached instructions.\n**Verdict: Incorrect**\n\nBased on a rigorous analysis, only option A provides the complete and correctly ordered set of primitives required to guarantee deterministic and correct execution under the specified architectural constraints.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}