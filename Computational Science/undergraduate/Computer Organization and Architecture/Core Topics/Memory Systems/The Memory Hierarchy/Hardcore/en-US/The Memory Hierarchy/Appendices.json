{
    "hands_on_practices": [
        {
            "introduction": "Understanding how memory addresses map to cache locations is a cornerstone of performance analysis. In direct-mapped caches, this mapping is rigid, which can sometimes lead to \"pathological\" scenarios where different, frequently-used data blocks repeatedly evict each other, nullifying the cache's benefits. This exercise  guides you through a classic example of such a conflict, asking you to analyze why it occurs and then to quantify the immense performance gain achieved by a simple, deliberate change in memory layout.",
            "id": "3684789",
            "problem": "A byte-addressable system uses a direct-mapped cache with total capacity $C$ bytes and block size $B$ bytes, where $C$ is an integer multiple of $B$. Let the number of cache lines be $L = \\frac{C}{B}$. Two arrays, $A$ and $D$, each occupy exactly $C$ bytes and are laid out in main memory as follows: the base address of $A$ is $A_{\\mathrm{base}} = 0$, and the base address of $D$ is $D_{\\mathrm{base}} = C$. Each array stores one byte per element, so the $i$-th element of $A$ is at address $A_{\\mathrm{base}} + i$, and the $i$-th element of $D$ is at address $D_{\\mathrm{base}} + i$. Consider the alternating access pattern that, for all $i \\in \\{0, 1, \\dots, C-1\\}$, performs a read of $A[i]$ followed by a read of $D[i]$. Assume the cache is initially empty.\n\nUsing only the fundamental definition of direct-mapped indexing—namely, that the cache line index for a byte address $x$ is given by $\\left\\lfloor \\frac{x}{B} \\right\\rfloor \\pmod L$—and the notion of compulsory misses, first reason about the total number of cache misses incurred by the above alternating access pattern with the given layout. Next, propose a remapping that eliminates the pathological conflicts by changing only the base address of $D$ to $D_{\\mathrm{base}}' = C + B$ (i.e., padding $B$ bytes between $A$ and $D$), and re-analyze the total number of misses under the same alternating access pattern.\n\nWhat is the multiplicative reduction factor in the total miss count due to this remapping, expressed as a single closed-form expression in terms of $B$ only? Provide your final answer as a single symbolic expression. No rounding is required.",
            "solution": "The problem is deemed valid as it is scientifically grounded in the principles of computer architecture, is well-posed with all necessary information provided, and is stated objectively.\n\nLet us denote the cache capacity by $C$ bytes, the block size by $B$ bytes, and the number of cache lines by $L = \\frac{C}{B}$. The system is byte-addressable and the cache is direct-mapped. The cache line index for a given byte address $x$ is given by the formula $I(x) = \\left\\lfloor \\frac{x}{B} \\right\\rfloor \\pmod L$.\n\nFirst, we will analyze the total number of cache misses for the initial memory layout, and then for the remapped layout.\n\n**Case 1: Initial Memory Layout**\n\nIn the initial configuration, the base address of array $A$ is $A_{\\mathrm{base}} = 0$ and the base address of array $D$ is $D_{\\mathrm{base}} = C$. Each array has a size of $C$ bytes. The access pattern is an alternating read of corresponding elements from $A$ and $D$: for all $i \\in \\{0, 1, \\dots, C-1\\}$, the system reads $A[i]$ followed by a read of $D[i]$.\n\nThe address of the $i$-th element of $A$, denoted $A[i]$, is $x_A(i) = A_{\\mathrm{base}} + i = i$.\nThe cache line index for this access is:\n$$I_A(i) = \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\pmod L$$\n\nThe address of the $i$-th element of $D$, denoted $D[i]$, is $x_D(i) = D_{\\mathrm{base}} + i = C + i$.\nSince $C=LB$, the cache line index for this access is:\n$$I_D(i) = \\left\\lfloor \\frac{C+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor \\frac{LB+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor L + \\frac{i}{B} \\right\\rfloor \\pmod L$$\nUsing the property of the floor function, $\\lfloor n+y \\rfloor = n + \\lfloor y \\rfloor$ for integer $n$:\n$$I_D(i) = \\left( L + \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\right) \\pmod L$$\nSince $L \\pmod L = 0$, this simplifies to:\n$$I_D(i) = \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\pmod L$$\n\nWe observe that $I_A(i) = I_D(i)$ for all $i$. This means that for any given $i$, the access to $A[i]$ and the access to $D[i]$ map to the exact same cache line.\n\nNow, consider the alternating access sequence (`read $A[i]`, read $D[i]$).\nLet's trace the state of a cache line. Suppose the access to $A[i]$ occurs. If the block containing $A[i]$ is not in the cache, it is fetched, resulting in a miss. Immediately after, the access to $D[i]$ occurs. Since $D[i]$ maps to the same cache line, and it belongs to a different memory block, its block must be fetched. This action evicts the block for $A[i]$ that was just loaded. This is a conflict miss.\nFor the next iteration, $i+1$, the access to $A[i+1]$ occurs. If $A[i+1]$ is in the same block as $A[i]$, its block is no longer in the cache because it was evicted by the access to $D[i]$. Thus, this access to $A[i+1]$ is also a miss.\nThis \"ping-pong\" effect continues for the entire loop. The access to an element in $A$ loads a block, which is immediately evicted by the access to the corresponding element in $D$, which is then evicted by the next access to $A$, and so on.\n\nConsequently, every single memory access in this sequence results in a cache miss. The loop runs for $i$ from $0$ to $C-1$, and for each $i$, there are two accesses ($A[i]$ and $D[i]$).\nThe total number of accesses is $2 \\times C$.\nThe total number of misses in this initial configuration, $M_1$, is therefore:\n$$M_1 = 2C$$\n\n**Case 2: Remapped Memory Layout**\n\nIn the remapped configuration, the base address of $A$ remains $A_{\\mathrm{base}} = 0$, but the base address of $D$ is changed to $D_{\\mathrm{base}}' = C + B$.\n\nThe cache line index for an access to $A[i]$ is unchanged:\n$$I_A(i) = \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\pmod L$$\n\nThe address of the $i$-th element of $D$, $D[i]$, is now $x_D'(i) = D_{\\mathrm{base}}' + i = C + B + i$.\nThe cache line index for this access is:\n$$I_D'(i) = \\left\\lfloor \\frac{C+B+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor \\frac{LB+B+i}{B} \\right\\rfloor \\pmod L = \\left\\lfloor L + 1 + \\frac{i}{B} \\right\\rfloor \\pmod L$$\n$$I_D'(i) = \\left( L + 1 + \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\right) \\pmod L = \\left( 1 + \\left\\lfloor \\frac{i}{B} \\right\\rfloor \\right) \\pmod L$$\n\nWe now have $I_D'(i) = (I_A(i) + 1) \\pmod L$. The accesses to corresponding elements $A[i]$ and $D[i]$ now map to different (adjacent, wrapping around at $L-1$) cache lines. This remapping eliminates the pathological conflict observed in the first case.\n\nLet's analyze the miss pattern with this new layout. We can group the accesses by memory blocks. The loop variable $i$ ranges from $0$ to $C-1$. The block index within the arrays can be defined as $k = \\lfloor i/B \\rfloor$, which ranges from $k=0$ to $k = \\lfloor (C-1)/B \\rfloor = L-1$.\n\nFor each block index $k \\in \\{0, 1, \\dots, L-1\\}$, the loop iterates through $i$ from $kB$ to $(k+1)B-1$.\n- All accesses to $A[i]$ in this range fall into the $k$-th block of $A$. This block maps to cache line $I_A = k \\pmod L = k$.\n- All accesses to $D[i]$ in this range fall into the $k$-th block of $D$ (in terms of array structure, not memory). This block maps to cache line $I_D' = (k+1) \\pmod L$.\n\nConsider the accesses for a given block index $k$:\n- The first access to the $k$-th block of $A$ (e.g., at $i=kB$ for $A[kB]$) will cause a miss, as this block has not been accessed before (or was evicted). This could be a compulsory miss (if the line is empty or holds a block we won't need again) or a conflict miss. This miss brings the $k$-th block of $A$ into cache line $k$.\n- The subsequent $B-1$ accesses to this block of $A$ (i.e., $A[kB+1], \\dots, A[(k+1)B-1]$) will all be hits, as the block is now in the cache.\n- Similarly, the first access to the $k$-th block of $D$ (e.g., at $i=kB$ for $D[kB]$) will cause a miss, bringing it into cache line $(k+1) \\pmod L$.\n- The subsequent $B-1$ accesses to this block of $D$ will all be hits.\n\nFor each block index $k$ from $0$ to $L-1$, there is exactly one miss for the corresponding block of $A$ and one miss for the corresponding block of $D$. This gives $2$ misses for each of the $L$ block-iterations.\nThe total number of misses in the remapped configuration, $M_2$, is:\n$$M_2 = 2 \\times L$$\nSince $L=C/B$, we can write this as $M_2 = \\frac{2C}{B}$.\n\n**Multiplicative Reduction Factor**\n\nThe problem asks for the multiplicative reduction factor in the total miss count. This is the ratio of the original number of misses to the new number of misses.\n$$\\text{Reduction Factor} = \\frac{M_1}{M_2}$$\nSubstituting the expressions for $M_1$ and $M_2$:\n$$\\text{Reduction Factor} = \\frac{2C}{2L} = \\frac{2C}{2C/B} = B$$\nThe multiplicative reduction factor is $B$. This expression depends only on $B$ as required.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Beyond data placement, a cache's effectiveness is also governed by its policies for handling different types of memory access, particularly writes. This practice  delves into the critical distinction between `write-allocate` and `no-write-allocate` policies, especially for workloads with minimal data reuse, such as streaming data. By calculating the \"wasted bandwidth\" under each policy, you will develop a quantitative understanding of how these choices impact memory traffic and why selecting the right policy for a given access pattern is crucial for system efficiency.",
            "id": "3684724",
            "problem": "Consider a single-level cache in a memory hierarchy with cache line size $L = 64$ bytes. The processor executes a store stream of $N = 1.5 \\times 10^{8}$ stores, each storing a word of size $w = 8$ bytes, to a memory region much larger than the cache. The store addresses are separated by a stride $s = 128$ bytes between consecutive stores, and the stream does not revisit any line once written. Assume the following two policies for handling a store miss:\n\n1. Write-allocate with write-back: on a store miss, the cache performs a Read For Ownership (RFO), reading the entire cache line from memory into the cache before updating the word, and later writes back the line upon eviction.\n2. No-write-allocate with write-around and write-combining: on a store miss, the cache does not allocate the line and writes the word directly to memory without reading the line.\n\nDefine the wasted bandwidth due to line fills as the total number of bytes transferred from memory to the cache that are never subsequently read by the processor and occur solely because of line fills triggered by store misses. Using only the scenario above, derive from first principles the total wasted bandwidth due to line fills under both policies and compute the difference (write-allocate minus no-write-allocate). Express the final difference in gigabytes, where $1$ gigabyte is defined as $10^{9}$ bytes. Round your answer to four significant figures.",
            "solution": "The problem asks for the difference in \"wasted bandwidth\" between two cache write policies for a specific store stream. \"Wasted bandwidth\" is defined as the total bytes transferred from memory to the cache for store miss line fills that are never subsequently read by the processor.\n\n**Analysis of the Access Pattern**\n\nThe store stream has a stride of $s = 128$ bytes, while the cache line size is $L = 64$ bytes. Since the stride is larger than the line size ($s > L$), each consecutive store is guaranteed to access a different cache line. The problem also states that the memory region is much larger than the cache and no line is revisited, which confirms that every store instruction will cause a compulsory cache miss.\n\n**Policy 1: Write-allocate with write-back**\n\nUnder this policy, every store miss triggers a **Read For Ownership (RFO)**. This means that for each of the $N$ store misses, the hardware reads the entire cache line of size $L$ from memory into the cache before modifying the specified word.\n\nAccording to the problem's definition, we must count the bytes transferred from memory to the cache that are not subsequently read. Since the processor is only executing a store stream, none of the data within the $L$ bytes fetched during the RFO is ever read by the processor. Therefore, the entire line fill constitutes wasted bandwidth.\n\nThe total wasted bandwidth for this policy, $B_{W,1}$, is the number of stores multiplied by the line size:\n$$B_{W,1} = N \\times L$$\n\n**Policy 2: No-write-allocate with write-around**\n\nUnder this policy, a store miss does not cause a line to be allocated in the cache. The store operation bypasses the cache and writes the data directly to memory.\n\nConsequently, there are no line fills triggered by store misses. No data is transferred from memory *to the cache* for these operations.\n\nThe total wasted bandwidth for this policy is therefore zero:\n$$B_{W,2} = 0$$\n\n**Calculating the Difference**\n\nThe difference in wasted bandwidth is $\\Delta B_W = B_{W,1} - B_{W,2}$.\n$$\\Delta B_W = (N \\times L) - 0 = N \\times L$$\n\nWe substitute the given values:\n-   Number of stores, $N = 1.5 \\times 10^{8}$\n-   Cache line size, $L = 64$ bytes\n\n$$\\Delta B_W = (1.5 \\times 10^{8}) \\times 64 \\,\\text{bytes} = 96 \\times 10^{8} \\,\\text{bytes} = 9.6 \\times 10^{9} \\,\\text{bytes}$$\n\nThe problem asks for the result in gigabytes, where $1\\,\\text{GB} = 10^9\\,\\text{bytes}$.\n$$\\Delta B_W = \\frac{9.6 \\times 10^9 \\,\\text{bytes}}{10^9 \\,\\text{bytes/GB}} = 9.6 \\,\\text{GB}$$\n\nFinally, rounding to four significant figures, we get $9.600$.",
            "answer": "$$\n\\boxed{9.600}\n$$"
        },
        {
            "introduction": "The ultimate goal of understanding the memory hierarchy is to design software that actively leverages it. This exercise  introduces one of the most fundamental techniques in high-performance computing: blocking, or tiling. Using the canonical example of matrix multiplication, you will determine the optimal block size to ensure that the core computation's working set fits entirely within the cache. This practice bridges the gap between hardware architecture and algorithm design, demonstrating how to restructure code to maximize data reuse and achieve significant speedups.",
            "id": "3684821",
            "problem": "A matrix multiplication kernel computes $\\mathbf{C} \\leftarrow \\mathbf{C} + \\mathbf{A}\\mathbf{B}$ for large square matrices using a blocked algorithm. In the blocked version, the kernel repeatedly multiplies two tiles of size $b \\times b$ from $\\mathbf{A}$ and $\\mathbf{B}$ and accumulates into a $b \\times b$ tile of $\\mathbf{C}$, keeping all three tiles resident in the Level-1 (L1) data cache to maximize reuse. Assume the following:\n\n- The L1 data cache has capacity $C = 192 \\,\\text{KiB}$.\n- Each element is a double-precision floating-point number of size $s = 8 \\,\\text{bytes}$.\n- The cache uses write-back and write-allocate, and the tag and metadata overhead can be neglected relative to $C$ for this calculation.\n- You may assume an ideal fully associative capacity model such that the three $b \\times b$ tiles must collectively occupy no more than the available capacity $C$ to be simultaneously resident.\n\nStarting from core definitions of the memory hierarchy (capacity measured in bytes and data footprints computed as element count times element size), determine the largest theoretical block size $b$ such that the total footprint of the three $b \\times b$ tiles does not exceed $C$. Express your final $b$ as a real number and round your answer to four significant figures. Do not include any units in your final number.",
            "solution": "The core principle of this problem is that the total memory footprint of the data that must be simultaneously resident in the cache cannot exceed the cache's capacity. The problem states that three tiles—one from matrix $\\mathbf{A}$, one from $\\mathbf{B}$, and one from $\\mathbf{C}$—must reside in the L1 data cache.\n\nLet $b$ be the dimension of the square block, or tile.\nEach tile has $b \\times b = b^2$ elements.\nEach element is a double-precision floating-point number, with a size $s = 8 \\,\\text{bytes}$.\n\nThe memory footprint of a single $b \\times b$ tile, denoted $F_{\\text{tile}}$, is the number of elements multiplied by the size of each element:\n$$F_{\\text{tile}} = b^2 \\cdot s$$\n\nThe blocked algorithm requires three such tiles to be held in the cache at once. Therefore, the total memory footprint, $F_{\\text{total}}$, required by the algorithm's inner loop is:\n$$F_{\\text{total}} = 3 \\cdot F_{\\text{tile}} = 3 \\cdot b^2 \\cdot s$$\n\nThis total footprint must not exceed the L1 data cache capacity, $C$. This gives us the governing inequality:\n$$3 \\cdot b^2 \\cdot s \\le C$$\n\nTo find the largest theoretical block size $b$, we solve for the case where the footprint exactly matches the capacity:\n$$3 \\cdot b^2 \\cdot s = C$$\n\nWe can now solve for $b$:\n$$b^2 = \\frac{C}{3s}$$\n$$b = \\sqrt{\\frac{C}{3s}}$$\n\nThe given values are $C = 192 \\,\\text{KiB}$ and $s = 8 \\,\\text{bytes}$. First, we must express the cache capacity $C$ in units of bytes to be consistent with the element size $s$. In computer science, a kilobyte (KiB) is $2^{10}$ bytes.\n$$C = 192 \\,\\text{KiB} = 192 \\times 2^{10} \\,\\text{bytes} = 192 \\times 1024 \\,\\text{bytes} = 196608 \\,\\text{bytes}$$\n\nNow, we substitute the numerical values for $C$ and $s$ into the equation for $b$:\n$$b = \\sqrt{\\frac{196608}{3 \\times 8}}$$\n$$b = \\sqrt{\\frac{196608}{24}}$$\n$$b = \\sqrt{8192}$$\n\nTo find the numerical value, we compute the square root:\n$$b = \\sqrt{8192} \\approx 90.509667...$$\n\nThe problem requires a real number rounded to four significant figures.\n$$b \\approx 90.51$$\nWhile in practice $b$ must be an integer, the problem asks for a real number representation.",
            "answer": "$$\n\\boxed{90.51}\n$$"
        }
    ]
}