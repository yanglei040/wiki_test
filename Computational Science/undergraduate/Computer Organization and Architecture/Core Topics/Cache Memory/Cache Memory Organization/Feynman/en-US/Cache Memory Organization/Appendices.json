{
    "hands_on_practices": [
        {
            "introduction": "A cache is designed to speed up memory access, but under specific conditions, its performance can collapse. This first exercise guides you through diagnosing this collapse by constructing a memory access pattern that induces maximum \"thrashing\" in a direct-mapped cache. By working through this problem, you will solidify your understanding of address mapping and see firsthand how conflict misses can dramatically degrade the Average Memory Access Time (AMAT) .",
            "id": "3625110",
            "problem": "A byte-addressed machine uses a single-level, direct-mapped cache with associativity $A=1$, number of sets $S=64$, and block size $B=64$ bytes. The cache begins empty. Consider read-only accesses of aligned blocks. You will analyze a deliberately adversarial access pattern with $2$-cycle periodicity that maximally thrashes this cache.\n\nTask:\n1) Construct two distinct block-aligned addresses $x$ and $y$ such that repeatedly alternating accesses $x,y,x,y,\\dots$ produce maximal thrashing in this direct-mapped cache. Justify your choice by explicitly computing the set index and tag for each of $x$ and $y$ using the standard decomposition of $\\lfloor \\text{addr}/B \\rfloor$ into a set index in $\\{0,\\dots,S-1\\}$ and a tag, and by showing that the two blocks map to the same set but have different tags.\n\n2) Starting from the empty cache, argue from first principles what the steady-state miss rate of the alternating trace $x,y,x,y,\\dots$ must be.\n\n3) Given a cache hit time of $t_{h}=0.85$ nanoseconds and a miss penalty of $t_{m}=73.6$ nanoseconds (defined as the additional latency beyond the hit time when a miss occurs), compute the Average Memory Access Time (AMAT) for the alternating trace in steady state. Express your final AMAT in nanoseconds and round your answer to four significant figures.\n\nYour final submitted answer must be the single numerical value for the AMAT as specified above. All intermediate reasoning, including the mapping justification for your chosen $x$ and $y$, should be shown in your working.",
            "solution": "The problem is first validated to ensure it is self-contained, scientifically grounded, and well-posed.\n\n**Step 1: Extract Givens**\n- Machine addressing: Byte-addressed\n- Cache level: Single-level\n- Cache mapping: Direct-mapped (associativity $A=1$)\n- Number of cache sets: $S=64$\n- Block size: $B=64$ bytes\n- Initial cache state: Empty\n- Access characteristics: Read-only, aligned blocks\n- Access trace: Repeatedly alternating accesses $x, y, x, y, \\dots$\n- Objective: Maximal thrashing\n- Cache hit time: $t_h=0.85$ nanoseconds\n- Cache miss penalty: $t_m=73.6$ nanoseconds (defined as additional latency beyond hit time)\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with fundamental principles of computer architecture, specifically cache memory organization and performance evaluation. The parameters given ($S=64$, $B=64$ bytes, $A=1$) are standard and realistic for a simple cache model. The terminology used, such as \"byte-addressed\", \"direct-mapped\", \"block size\", \"miss penalty\", and \"AMAT\", is precise and standard. The problem is well-posed; it asks for the construction of an example demonstrating a specific behavior (thrashing) and the calculation of a resulting performance metric (AMAT), for which all necessary data is provided. The definition of \"miss penalty\" as an *additional* time is explicitly stated, removing ambiguity. The problem is objective, complete, and contains no contradictions or scientifically unsound premises.\n\n**Verdict:** The problem is valid.\n\n**Solution**\n\nThe analysis proceeds in three parts as requested by the problem statement.\n\n**Part 1: Construction of Addresses for Maximal Thrashing**\n\nA memory address is partitioned into three fields: tag, set index, and block offset. We first determine the number of bits for each field.\nThe machine is byte-addressed.\nThe block size is $B=64$ bytes. The number of bits for the block offset is $\\log_2(B) = \\log_2(64) = 6$ bits.\nThe number of sets is $S=64$. The number of bits for the set index is $\\log_2(S) = \\log_2(64) = 6$ bits.\nThe remaining high-order bits of the address constitute the tag.\n\nAn address `addr` is mapped to a cache set using the formula for the set index:\n$$ \\text{Set Index} = \\left\\lfloor \\frac{\\text{addr}}{B} \\right\\rfloor \\pmod S $$\nThe tag is computed as:\n$$ \\text{Tag} = \\left\\lfloor \\frac{\\left\\lfloor \\frac{\\text{addr}}{B} \\right\\rfloor}{S} \\right\\rfloor = \\left\\lfloor \\frac{\\text{addr}}{B \\times S} \\right\\rfloor $$\nThe term $\\lfloor \\text{addr}/B \\rfloor$ is the block address.\n\nMaximal thrashing for a direct-mapped cache under an alternating access pattern $x,y,x,y,\\dots$ occurs when the memory blocks corresponding to addresses $x$ and $y$ map to the same cache set but are distinct blocks (i.e., have different tags). This ensures that each access evicts the block needed for the subsequent access.\n\nThe problem states that accesses are to aligned blocks. An address `addr` is block-aligned if it is a multiple of the block size $B$. This means $\\text{addr} \\pmod B = 0$.\n\nWe need to find two distinct block-aligned addresses, $x$ and $y$, such that:\n1. $\\text{SetIndex}(x) = \\text{SetIndex}(y)$\n2. $\\text{Tag}(x) \\neq \\text{Tag}(y)$\n\nLet's choose an arbitrary set index, for instance, set $1$.\nWe require $\\lfloor x/B \\rfloor \\pmod S = 1$ and $\\lfloor y/B \\rfloor \\pmod S = 1$.\n\nLet's construct address $x$. We choose the simplest block address that maps to set $1$, which is the block address $1$.\nLet $\\lfloor x/B \\rfloor = 1$. Since $x$ must be block-aligned, we have $x = 1 \\times B = 1 \\times 64 = 64$.\nFor $x=64$:\n- Block address: $\\lfloor 64/64 \\rfloor = 1$.\n- Set Index: $1 \\pmod{64} = 1$.\n- Tag: $\\lfloor 1/64 \\rfloor = 0$.\n\nNow, let's construct address $y$. We need another block address that also maps to set $1$ but has a different tag. This means the block address must be of the form $k \\cdot S + 1$ for some integer $k$, where the tag $\\lfloor (k \\cdot S + 1)/S \\rfloor = k$ is different from the tag of $x$, which is $0$.\nLet's choose the simplest non-zero integer for $k$, which is $k=1$.\nThe new block address is $1 \\cdot S + 1 = 1 \\cdot 64 + 1 = 65$.\nLet $\\lfloor y/B \\rfloor = 65$. Since $y$ must be block-aligned, we have $y = 65 \\times B = 65 \\times 64 = 4160$.\nFor $y=4160$:\n- Block address: $\\lfloor 4160/64 \\rfloor = 65$.\n- Set Index: $65 \\pmod{64} = 1$.\n- Tag: $\\lfloor 65/64 \\rfloor = 1$.\n\nThe chosen addresses are $x=64$ and $y=4160$. Both are block-aligned. They both map to set index $1$. Their tags, $0$ and $1$, are different. Thus, they satisfy all conditions for causing maximal thrashing.\n\n**Part 2: Steady-State Miss Rate**\n\nWe analyze the trace $x,y,x,y,\\dots$ starting from an empty cache.\n1. Access $x$: The cache is empty. This is a compulsory miss. The block containing address $x$ is fetched from memory and placed into set $1$. The tag stored in set $1$ is $\\text{Tag}(x)$.\n2. Access $y$: The processor requests address $y$. The hardware checks set $1$. The tag stored in set $1$ is $\\text{Tag}(x)$, which does not match $\\text{Tag}(y)$. This is a conflict miss. The block for $x$ is evicted, and the block for $y$ is fetched and placed into set $1$. The tag is updated to $\\text{Tag}(y)$.\n3. Access $x$: The processor requests address $x$. The hardware checks set $1$. The tag stored is now $\\text{Tag}(y)$, which does not match $\\text{Tag}(x)$. This is another conflict miss. The block for $y$ is evicted and replaced by the block for $x$.\n4. Access $y$: This access will again be a conflict miss, evicting the block for $x$.\n\nThe steady state is reached after the first access. In this state, every subsequent access results in a conflict miss. The block needed for the current access is never the one present in the cache because it was evicted by the immediately preceding access.\nTherefore, in the steady state, every access is a miss.\nThe miss rate is the ratio of misses to total accesses. In steady state, this is $1/1$.\n$$ \\text{Miss Rate} = 1 $$\n\n**Part 3: Average Memory Access Time (AMAT)**\n\nThe Average Memory Access Time (AMAT) is the weighted average of the time for a hit and the time for a miss. The general formula is:\n$$ \\text{AMAT} = (\\text{Hit Rate} \\times \\text{Time}_\\text{hit}) + (\\text{Miss Rate} \\times \\text{Time}_\\text{miss}) $$\nThe problem defines the miss penalty ($t_m$) as the *additional* time required on a miss, beyond the hit time ($t_h$).\nSo, $\\text{Time}_\\text{hit} = t_h$.\nAnd $\\text{Time}_\\text{miss} = t_h + t_m$.\n\nSubstituting these into the AMAT formula:\n$$ \\text{AMAT} = ((1 - \\text{Miss Rate}) \\times t_h) + (\\text{Miss Rate} \\times (t_h + t_m)) $$\nSimplifying this expression:\n$$ \\text{AMAT} = t_h - (\\text{Miss Rate} \\times t_h) + (\\text{Miss Rate} \\times t_h) + (\\text{Miss Rate} \\times t_m) $$\n$$ \\text{AMAT} = t_h + (\\text{Miss Rate} \\times t_m) $$\n\nWe are given the following values:\n- Hit time $t_h = 0.85$ ns.\n- Miss penalty $t_m = 73.6$ ns.\n- From Part 2, the steady-state Miss Rate is $1$.\n\nSubstituting these values into the derived formula:\n$$ \\text{AMAT} = 0.85 + (1 \\times 73.6) $$\n$$ \\text{AMAT} = 0.85 + 73.6 $$\n$$ \\text{AMAT} = 74.45 \\text{ ns} $$\n\nThe problem asks for the answer to be rounded to four significant figures. The calculated value $74.45$ already has exactly four significant figures.",
            "answer": "$$\\boxed{74.45}$$"
        },
        {
            "introduction": "Having diagnosed the problem of conflict misses, a computer architect's next step is to engineer a solution. This practice introduces the \"victim cache,\" a clever hardware enhancement designed to mitigate the very thrashing you explored previously. You will trace how this small, secondary cache works to salvage recently evicted blocks, transforming costly main memory accesses into much faster cache-to-cache swaps and dramatically improving the AMAT .",
            "id": "3624576",
            "problem": "A two-level on-chip memory system is organized with a first-level data cache of capacity $4 \\times 2^{10}$ bytes, block size $2^{6}$ bytes, and associativity $E=2$ (two-way set associative). The cache uses Least Recently Used (LRU) replacement within each set. Let the number of sets be $S$, and let the set index be the bits immediately above the block offset bits. A small Victim Cache (VC) of size $2$ blocks sits between the first-level cache and main memory; it is fully associative and uses swap-on-hit: on a first-level miss that hits in the VC, the VC block and the to-be-evicted first-level block are swapped without accessing main memory.\n\nConsider three word addresses\n$a_0=0x00000000$, $a_1=0x00000800$, and $a_2=0x00001000$. Define a trace $\\mathcal{T}$ of length $N=300$ that repeats the cyclic pattern $(a_0,a_1,a_2)$ exactly $100$ times, starting from a cold state (both the first-level cache and the victim cache are initially empty). All loads are aligned within their blocks and use the specified block size.\n\nUsing only fundamental definitions of set-associative mapping, conflict misses, and temporal locality, do the following:\n1. Determine $S$ and show that $a_0$, $a_1$, and $a_2$ map to the same set in the first-level cache.\n2. Explain why this trace produces worst-case self-conflict for $E=2$ in the first-level cache under LRU.\n3. Explain the temporal pattern under which a victim cache of size $2$ cures the self-conflict in terms of swaps and re-reference intervals.\n4. Given the following access-time model per memory operation:\n   - First-level cache hit time $t_1=1$ cycles,\n   - First-level miss that hits in the victim cache costs $t_v=6$ cycles in total,\n   - First-level miss that misses in the victim cache and goes to main memory costs $t_m=100$ cycles in total,\ncompute the Average Memory Access Time (AMAT) in cycles per access for the configuration with the victim cache of size $2$ on the trace $\\mathcal{T}$.\n\nRound your final AMAT to four significant figures and express it in cycles per access (do not include any unit symbol in the final boxed answer).",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All necessary parameters for cache analysis are provided, and the scenario is a standard exercise in computer architecture.\n\nThe solution is presented in four parts as requested by the problem statement.\n\nFirst, we determine the number of sets, $S$, in the first-level cache and verify the mapping of the given addresses.\nThe cache capacity is $C_1 = 4 \\times 2^{10} = 2^{12}$ bytes.\nThe block size is $B = 2^6$ bytes.\nThe associativity is $E = 2$.\nThe number of sets $S$ is given by the formula $S = \\frac{C_1}{B \\times E}$.\nSubstituting the given values:\n$$S = \\frac{2^{12} \\text{ bytes}}{2^6 \\text{ bytes/block} \\times 2 \\text{ blocks/set}} = \\frac{2^{12}}{2^7} = 2^5 = 32$$\nAn address is partitioned into a tag, a set index, and a block offset. The number of bits for each part is determined as follows:\nThe number of block offset bits, $b$, is $b = \\log_2(B) = \\log_2(2^6) = 6$. These are the least significant bits of the address (bits $0$ to $5$).\nThe number of set index bits, $s$, is $s = \\log_2(S) = \\log_2(32) = 5$. The problem states that these bits are immediately above the block offset bits, so they are bits $6$ to $10$.\nThe remaining higher-order bits form the tag.\nNow we examine the addresses $a_0$, $a_1$, and $a_2$. We need to inspect bits $6$ through $10$ to find their set index.\nFor $a_0 = 0x00000000$:\nThe binary representation is $...0000 \\, 00000 \\, 000000$. The bits from $6$ to $10$ (the set index) are all $0$. Thus, $a_0$ maps to set $0$.\nFor $a_1 = 0x00000800$:\n$0x800 = 2048 = 2^{11}$. The binary representation is $...0001000 \\, 00000 \\, 000000$. The bit at position $11$ is $1$, but bits $6$ through $10$ are all $0$. Thus, $a_1$ also maps to set $0$.\nFor $a_2 = 0x00001000$:\n$0x1000 = 4096 = 2^{12}$. The binary representation is $...0010000 \\, 00000 \\, 000000$. The bit at position $12$ is $1$, but bits $6$ through $10$ are all $0$. Thus, $a_2$ also maps to set $0$.\nAll three addresses, $a_0$, $a_1$, and $a_2$, map to the same set index ($0$), but have different tags, making them a conflict group for that set.\n\nSecond, we explain why the trace $\\mathcal{T}$ produces worst-case self-conflict for the first-level cache alone.\nThe set has an associativity of $E=2$, meaning it can only hold two distinct blocks at any time. The trace cyclically accesses three addresses, $(a_0, a_1, a_2)$, that all map to this same set. The replacement policy is LRU (Least Recently Used).\nLet $B_0, B_1, B_2$ be the cache blocks corresponding to addresses $a_0, a_1, a_2$.\n1. Access $a_0$: Miss. $B_0$ is loaded into the set. Cache state for set $0$: $\\{B_0\\}$.\n2. Access $a_1$: Miss. $B_1$ is loaded. Cache state: $\\{B_0, B_1\\}$. The LRU order is $(B_1, B_0)$, with $B_1$ being most recently used.\n3. Access $a_2$: Miss. The set is full. According to LRU, the least recently used block, $B_0$, is evicted. $B_2$ is loaded. Cache state: $\\{B_1, B_2\\}$. LRU order: $(B_2, B_1)$.\n4. Access $a_0$: Miss. The block $B_0$ was just evicted in the previous step. The LRU block is now $B_1$, so it is evicted. $B_0$ is loaded. Cache state: $\\{B_0, B_2\\}$. LRU order: $(B_0, B_2)$.\nThis sequence of events, where an access requests a block that was just evicted by a previous access in the conflict group, is known as thrashing. After the initial compulsory misses, every subsequent access results in a conflict miss. The hit rate is $0\\%$. This is the worst-case scenario as the cache provides no benefit for temporal locality within this access pattern. The number of conflicting addresses ($3$) exceeds the associativity of the set ($2$).\n\nThird, we explain how the victim cache (VC) of size $2$ cures this conflict.\nThe VC is a small, fully associative cache that holds blocks recently evicted from the L1 cache. On an L1 miss, the VC is checked. If the block is in the VC (a \"VC hit\"), it is swapped with the block being evicted from the L1 cache. This is much faster than a main memory access.\nLet's trace the access pattern with the L1 cache and the VC. Both are initially empty.\n1. Access $a_0$: L1 miss, VC miss. Fetch $B_0$ from main memory. L1 Set $0$: $\\{B_0\\}$. VC: $\\{\\}$.\n2. Access $a_1$: L1 miss, VC miss. Fetch $B_1$ from main memory. L1 Set $0$: $\\{B_0, B_1\\}$. VC: $\\{\\}$.\n3. Access $a_2$: L1 miss, VC miss. L1 Set $0$ must evict its LRU block, $B_0$. $B_0$ is moved to the VC. $B_2$ is fetched from main memory and placed in L1. L1 Set $0$: $\\{B_1, B_2\\}$. VC: $\\{B_0\\}$.\nAt this point, the working set of three blocks $\\{B_0, B_1, B_2\\}$ is contained within the L1+VC system.\n4. Access $a_0$: L1 miss. However, $B_0$ is present in the VC (VC hit). The swap-on-hit policy is triggered. The LRU block in L1 Set $0$ is $B_1$. $B_0$ from the VC is swapped with $B_1$ from L1. L1 Set $0$: $\\{B_0, B_2\\}$. VC: $\\{B_1\\}$. No main memory access is needed.\n5. Access $a_1$: L1 miss. $B_1$ is in the VC (VC hit). LRU block in L1 is $B_2$. Swap $B_1$ and $B_2$. L1 Set $0$: $\\{B_0, B_1\\}$. VC: $\\{B_2\\}$.\n6. Access $a_2$: L1 miss. $B_2$ is in the VC (VC hit). LRU block in L1 is $B_0$. Swap $B_2$ and $B_0$. L1 Set $0$: $\\{B_1, B_2\\}$. VC: $\\{B_0\\}$.\nThis pattern repeats for the rest of the trace. After the initial three compulsory misses, every subsequent access results in an L1 miss but a VC hit. The effective associativity for this conflict group becomes the L1 set associativity plus the VC size, i.e., $2+2=4$. Since the size of the conflict group ($3$) is less than the effective associativity ($4$), all three blocks can be retained within the L1-VC system, eliminating conflict misses to main memory. The temporal pattern is that the re-reference interval for any block is $3$ accesses. The L1 cache alone (associativity $2$) cannot handle this, but the L1+VC system (effective associativity $4$) can.\n\nFourth, we compute the Average Memory Access Time (AMAT).\nThe total number of accesses in the trace $\\mathcal{T}$ is $N = 300$.\nThe access costs are given as:\nL1 hit time: $t_1 = 1$ cycle.\nL1 miss, VC hit time: $t_v = 6$ cycles.\nL1 miss, VC miss (main memory) time: $t_m = 100$ cycles.\n\nBased on the trace analysis from the third part:\n- The first three accesses ($a_0, a_1, a_2$) are L1 misses and VC misses (compulsory misses), as both caches start cold.\n- The number of main memory accesses is $N_{mem} = 3$. The cost for each is $t_m$.\n- The subsequent $N - 3 = 300 - 3 = 297$ accesses are all L1 misses followed by VC hits.\n- The number of VC hits is $N_{vc\\_hit} = 297$. The cost for each is $t_v$.\n- There are no L1 hits in this trace. The number of L1 hits is $N_{hit} = 0$.\n\nThe total time for all accesses is the sum of the costs of each access type:\n$$ \\text{Total Time} = (N_{hit} \\times t_1) + (N_{vc\\_hit} \\times t_v) + (N_{mem} \\times t_m) $$\n$$ \\text{Total Time} = (0 \\times 1) + (297 \\times 6) + (3 \\times 100) $$\n$$ \\text{Total Time} = 0 + 1782 + 300 = 2082 \\text{ cycles} $$\nThe Average Memory Access Time (AMAT) is the total time divided by the total number of accesses:\n$$ \\text{AMAT} = \\frac{\\text{Total Time}}{N} = \\frac{2082}{300} $$\n$$ \\text{AMAT} = 6.94 \\text{ cycles/access} $$\nThe problem asks for the answer to be rounded to four significant figures.\n$$ \\text{AMAT} = 6.940 \\text{ cycles/access} $$",
            "answer": "$$\\boxed{6.940}$$"
        },
        {
            "introduction": "While increasing associativity reduces conflict misses, it introduces a new challenge: which block to evict on a miss? While the Least Recently Used (LRU) policy is theoretically optimal, its hardware implementation can be complex and slow for many-way associative caches. This final practice delves into a common, practical approximation called binary-tree pseudo-LRU (PLRU), challenging you to find a scenario where its behavior diverges from true LRU, thereby illustrating the crucial engineering trade-offs between performance and design complexity .",
            "id": "3624666",
            "problem": "Consider a single cache set in an $E=8$ way set-associative cache that uses the binary-tree pseudo Least Recently Used (PLRU) replacement algorithm. The eight ways are indexed by leaves $\\{0,1,2,3,4,5,6,7\\}$ of a complete binary tree of depth $3$. Each internal node stores one replacement bit that indicates which of its two child subtrees is considered less recently used. Let the internal nodes be labeled $b_{0},b_{1},b_{2},b_{3},b_{4},b_{5},b_{6}$ as follows: $b_{0}$ is the root; $b_{1}$ is the left child of $b_{0}$ covering leaves $\\{0,1,2,3\\}$; $b_{2}$ is the right child of $b_{0}$ covering leaves $\\{4,5,6,7\\}$; $b_{3}$ covers $\\{0,1\\}$; $b_{4}$ covers $\\{2,3\\}$; $b_{5}$ covers $\\{4,5\\}$; $b_{6}$ covers $\\{6,7\\}$. A bit value $0$ means the left subtree (or left leaf) is marked as less recently used; a bit value $1$ means the right subtree (or right leaf) is marked as less recently used. On each cache hit to leaf $i$, every bit along the path from the root to leaf $i$ is updated to point to the sibling subtree (or sibling leaf) of the path just taken, thereby marking the accessed leaf as most recently used relative to its sibling. On replacement, the victim leaf is selected by starting at $b_{0}$ and descending to the child indicated by each bit (always taking the subtree currently marked less recently used) until a leaf is reached; the replacement bits along the selected path are then flipped to point to the opposite child.\n\nAssume the set is initially filled by a sequence of compulsory misses that install distinct blocks into ways $\\{0,1,2,3,4,5,6,7\\}$ in that order, with each installation behaving like a cache hit for PLRU state updates. After this initial fill, perform the following access trace of cache hits within the set: access way $0$, then $1$, then $2$, then $3$, then $4$, then $5$ (in that order). Immediately after this trace, a new block $X$ maps to the same set and causes a miss, requiring one victim to be evicted.\n\nUsing only the core definitions of true Least Recently Used (LRU) and the binary-tree PLRU update and victim-selection rules given above, determine the index (from $0$ to $7$) of the way that the binary-tree PLRU will evict for block $X$. In your reasoning, explain how the internal bit states evolve during the trace and why the PLRU choice diverges from true LRU in this scenario. Express your final answer as a single integer index with no units.",
            "solution": "The problem requires us to determine the victim way selected by a binary-tree pseudo-Least Recently Used (PLRU) replacement algorithm in an $E=8$-way set-associative cache. We must first establish the state of the PLRU bits after an initial fill sequence and a subsequent access trace, and then apply the victim selection rule. We are also asked to compare this outcome to that of a true Least Recently Used (LRU) policy.\n\nFirst, let us formalize the structure of the binary tree and the PLRU state. The cache set has $E=8$ ways, indexed $\\{0, 1, 2, 3, 4, 5, 6, 7\\}$. The PLRU logic uses a complete binary tree of depth $3$ with $7$ internal nodes, each storing a single bit. The state of the PLRU is represented by a vector of these bits: $S = (b_0, b_1, b_2, b_3, b_4, b_5, b_6)$.\nThe tree structure is as follows:\n- $b_0$ is the root.\n- Its left child is $b_1$ (covering leaves $\\{0,1,2,3\\}$) and its right child is $b_2$ (covering leaves $\\{4,5,6,7\\}$).\n- $b_1$'s children are $b_3$ (covering $\\{0,1\\}$) and $b_4$ (covering $\\{2,3\\}$).\n- $b_2$'s children are $b_5$ (covering $\\{4,5\\}$) and $b_6$ (covering $\\{6,7\\}$).\n- The leaf children of $b_3, b_4, b_5, b_6$ are ways $\\{0,1\\}, \\{2,3\\}, \\{4,5\\}, \\{6,7\\}$, respectively.\n\nA bit value of $0$ indicates the left child/subtree is less recently used, and a value of $1$ indicates the right child/subtree is less recently used.\n\nThe path from the root to each leaf consists of a sequence of left (L) or right (R) branches:\n- Way 0: L, L, L (path through $b_0, b_1, b_3$)\n- Way 1: L, L, R (path through $b_0, b_1, b_3$)\n- Way 2: L, R, L (path through $b_0, b_1, b_4$)\n- Way 3: L, R, R (path through $b_0, b_1, b_4$)\n- Way 4: R, L, L (path through $b_0, b_2, b_5$)\n- Way 5: R, L, R (path through $b_0, b_2, b_5$)\n- Way 6: R, R, L (path through $b_0, b_2, b_6$)\n- Way 7: R, R, R (path through $b_0, b_2, b_6$)\n\nOn a cache hit to a way, all bits along the path from the root to that way are updated to point to the sibling of the path taken. For example, if the path to an accessed way goes through the left child of a node, that node's bit is set to $1$ (pointing to the right child as now being less recently used).\n\nWe begin by determining the state of the bits after the initial fill sequence $\\{0,1,2,3,4,5,6,7\\}$. We assume an initial state where all bits are $0$, so $S_{initial} = (0,0,0,0,0,0,0)$. We trace the effect of each access, which behaves as a cache hit.\n\n1.  **Initial fill sequence $\\{0,1,2,3,4,5,6,7\\}$**:\n    - Let the state be $S = (b_0, b_1, b_2, b_3, b_4, b_5, b_6)$. Starting from $S = (0,0,0,0,0,0,0)$.\n    - Access 0 (L,L,L): bits $b_0, b_1, b_3$ are on the path. They are updated to point to their right siblings. $b_0 \\to 1, b_1 \\to 1, b_3 \\to 1$. $S = (1,1,0,1,0,0,0)$.\n    - Access 1 (L,L,R): bits $b_0, b_1, b_3$ are on the path. $b_0 \\to 1, b_1 \\to 1$. $b_3$ is updated to point left. $b_3 \\to 0$. $S = (1,1,0,0,0,0,0)$.\n    - Access 2 (L,R,L): bits $b_0, b_1, b_4$ are on the path. $b_0 \\to 1$. $b_1$ points left. $b_1 \\to 0$. $b_4$ points right. $b_4 \\to 1$. $S = (1,0,0,0,1,0,0)$.\n    - Access 3 (L,R,R): bits $b_0, b_1, b_4$ are on the path. $b_0 \\to 1, b_1 \\to 0, b_4 \\to 0$. $S = (1,0,0,0,0,0,0)$.\n    - Access 4 (R,L,L): bits $b_0, b_2, b_5$ are on the path. $b_0 \\to 0, b_2 \\to 1, b_5 \\to 1$. $S = (0,0,1,0,0,1,0)$.\n    - Access 5 (R,L,R): bits $b_0, b_2, b_5$ are on the path. $b_0 \\to 0, b_2 \\to 1, b_5 \\to 0$. $S = (0,0,1,0,0,0,0)$.\n    - Access 6 (R,R,L): bits $b_0, b_2, b_6$ are on the path. $b_0 \\to 0, b_2 \\to 0, b_6 \\to 1$. $S = (0,0,0,0,0,0,1)$.\n    - Access 7 (R,R,R): bits $b_0, b_2, b_6$ are on the path. $b_0 \\to 0, b_2 \\to 0, b_6 \\to 0$. $S = (0,0,0,0,0,0,0)$.\n    The state after the initial fill sequence, which we denote $S_{fill}$, is $(0,0,0,0,0,0,0)$.\n\n2.  **Access trace of hits $\\{0,1,2,3,4,5\\}$**:\n    - Starting from $S_{fill} = (0,0,0,0,0,0,0)$.\n    - Access 0 (L,L,L): $b_0 \\to 1, b_1 \\to 1, b_3 \\to 1$. $S = (1,1,0,1,0,0,0)$.\n    - Access 1 (L,L,R): $b_0 \\to 1, b_1 \\to 1, b_3 \\to 0$. $S = (1,1,0,0,0,0,0)$.\n    - Access 2 (L,R,L): $b_0 \\to 1, b_1 \\to 0, b_4 \\to 1$. $S = (1,0,0,0,1,0,0)$.\n    - Access 3 (L,R,R): $b_0 \\to 1, b_1 \\to 0, b_4 \\to 0$. $S = (1,0,0,0,0,0,0)$.\n    - Access 4 (R,L,L): $b_0 \\to 0, b_2 \\to 1, b_5 \\to 1$. $S = (0,0,1,0,0,1,0)$.\n    - Access 5 (R,L,R): $b_0 \\to 0, b_2 \\to 1, b_5 \\to 0$. $S = (0,0,1,0,0,0,0)$.\n\n    The final state of the PLRU bits before the miss is $S_{final} = (0,0,1,0,0,0,0)$.\n\n3.  **Victim Selection for block X**:\n    On a miss, the victim is found by traversing the tree from the root, following the direction indicated by each bit ($0$ for left, $1$ for right) to find the less recently used path.\n    - Start at root $b_0$. Its value is $b_0 = 0$. Follow the left path to the subtree managed by $b_1$.\n    - At node $b_1$, its value is $b_1 = 0$. Follow the left path to the subtree managed by $b_3$.\n    - At node $b_3$, its value is $b_3 = 0$. Follow the left path to the leaf.\n    The left leaf of $b_3$ is way $0$.\n    Thus, the PLRU algorithm will evict the block in way $0$.\n\n4.  **Comparison with True LRU and Explanation of Divergence**:\n    To find the true LRU victim, we track the recency of all ways. An access to a way makes it the Most Recently Used (MRU). The Least Recently Used (LRU) is the way that has not been accessed for the longest time.\n    - After the initial fill sequence $\\{0,1,2,3,4,5,6,7\\}$, the recency order from MRU to LRU is $\\{7,6,5,4,3,2,1,0\\}$. The true LRU way is $0$.\n    - Now we apply the hit trace $\\{0,1,2,3,4,5\\}$. We can represent the state as an ordered list (stack), with MRU at the head.\n    - Initial stack: $[7,6,5,4,3,2,1,0]$\n    - Access 0: $[0,7,6,5,4,3,2,1]$\n    - Access 1: $[1,0,7,6,5,4,3,2]$\n    - Access 2: $[2,1,0,7,6,5,4,3]$\n    - Access 3: $[3,2,1,0,7,6,5,4]$\n    - Access 4: $[4,3,2,1,0,7,6]$\n    - Access 5: $[5,4,3,2,1,0,7,6]$\n    After the trace, the least recently used way is at the end of the list, which is way $6$. So, a true LRU policy would evict way $6$.\n\n    The PLRU victim is way $0$, while the true LRU victim is way $6$. The policies diverge. The reason for this divergence lies in the nature of the binary-tree PLRU algorithm. It does not maintain a total ordering of recency across all ways. Instead, an access updates a small number of bits that encode partial orderings within subtrees.\n    - The sequence of accesses $\\{0,1,2,3,4,5\\}$ makes ways $6$ and $7$ the two least recently used ways in a global sense.\n    - However, the last accesses, $4$ and $5$, were to the right subtree of the root (which contains ways $\\{4,5,6,7\\}$). This sets the root bit $b_0$ to $0$, indicating that the entire left subtree (ways $\\{0,1,2,3\\}$) is now considered the \"less recently used half\".\n    - Consequently, the victim selection process is immediately constrained to search within the left subtree. It will never consider evicting any way from the right subtree, including the true LRU way $6$, because that entire subtree has been marked as more recently used.\n    - The PLRU algorithm then proceeds to find the \"most LRU\" element within the \"LRU half\" $\\{0,1,2,3\\}$. Based on the prior access pattern $\\{0,1,2,3\\}$, it correctly identifies way $0$ as the LRU block within this subgroup. This phenomenon, where a single access can \"protect\" a truly old block residing in the same subtree from eviction, is a well-known limitation of this PLRU implementation.\n\nThe index of the way that PLRU will evict is $0$.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}