{
    "hands_on_practices": [
        {
            "introduction": "The Average Memory Access Time ($AMAT$) is the cornerstone metric for evaluating memory system performance. Cache designers constantly face a fundamental trade-off: a smaller, simpler cache might offer a faster hit time, but a larger, more complex one often achieves a lower miss rate. This exercise  challenges you to quantify this trade-off by calculating the 'break-even' point, allowing you to determine precisely how much a cache's miss rate must improve to justify a slower hit time.",
            "id": "3626050",
            "problem": "A processor designer is evaluating two first-level (L$1$) cache options, labeled Cache A and Cache B. Each cache serves the same instruction stream and data stream and uses the same lower-level memory hierarchy, so both experience the same miss penalty. For Cache A, the cache hit time is $t_{hA}$ and the miss rate is $MR_{A}$. For Cache B, the cache hit time is $t_{hB}$ and the miss rate is $MR_{B}$. The common miss penalty (the additional latency beyond the cache hit time to service a miss from the next level) is $t_{m}$. All time quantities are positive, and miss rates are within $[0,1]$.\n\nAssume that on any single access there are two mutually exclusive outcomes: a hit with probability $1 - MR_{X}$ or a miss with probability $MR_{X}$ for cache $X \\in \\{\\text{A}, \\text{B}\\}$. If an access hits in cache $X$, the access time equals the cache hit time $t_{hX}$. If an access misses in cache $X$, the access time equals the cache hit time plus the miss penalty, that is $t_{hX} + t_{m}$. Use the law of total expectation to express the Average Memory Access Time (AMAT) for each cache in terms of these quantities, then determine the break-even miss rate for Cache B, $MR_{B}$, such that the AMAT of Cache A equals the AMAT of Cache B.\n\nProvide your final answer as a single closed-form symbolic expression for $MR_{B}$ in terms of $t_{hA}$, $t_{hB}$, $MR_{A}$, and $t_{m}$. No numerical evaluation or rounding is required. Do not include units in your final expression.",
            "solution": "The problem requires the determination of a break-even miss rate for a cache, based on the principle of equating the Average Memory Access Time (AMAT) of two different cache configurations. The validation process confirms that the problem statement is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution.\n\nFirst, we must derive the expression for the Average Memory Access Time (AMAT) for a generic cache, which we will denote as cache $X$. The problem provides a probabilistic model for memory access. For any single access, there are two mutually exclusive outcomes: a cache hit or a cache miss.\n\nLet $MR_X$ be the miss rate for cache $X$. The probability of a miss is $P(\\text{miss}) = MR_X$.\nConsequently, the probability of a hit is $P(\\text{hit}) = 1 - MR_X$.\n\nThe problem specifies the time taken for each outcome:\n-   If the access results in a hit, the access time is the cache hit time, $t_{hX}$.\n-   If the access results in a miss, the access time is the sum of the cache hit time and the miss penalty, $t_{hX} + t_{m}$.\n\nThe problem directs us to use the law of total expectation to find the AMAT. The AMAT is the expected value of the memory access time. Let $T_X$ be the random variable for the access time for cache $X$.\nThe AMAT for cache $X$, denoted $AMAT_X$, is given by:\n$$AMAT_X = E[T_X] = (\\text{Time on hit}) \\times P(\\text{hit}) + (\\text{Time on miss}) \\times P(\\text{miss})$$\nSubstituting the given quantities:\n$$AMAT_X = t_{hX} \\cdot (1 - MR_X) + (t_{hX} + t_m) \\cdot MR_X$$\n\nWe can simplify this expression algebraically:\n$$AMAT_X = t_{hX} - t_{hX} \\cdot MR_X + t_{hX} \\cdot MR_X + t_m \\cdot MR_X$$\nThe terms $- t_{hX} \\cdot MR_X$ and $+ t_{hX} \\cdot MR_X$ cancel each other out, yielding the standard formula for AMAT:\n$$AMAT_X = t_{hX} + MR_X \\cdot t_m$$\n\nNow, we apply this general formula to the two specific caches, Cache A and Cache B.\n\nFor Cache A, the parameters are the hit time $t_{hA}$ and the miss rate $MR_A$. The AMAT is:\n$$AMAT_A = t_{hA} + MR_A \\cdot t_m$$\n\nFor Cache B, the parameters are the hit time $t_{hB}$ and the miss rate $MR_B$. The AMAT is:\n$$AMAT_B = t_{hB} + MR_B \\cdot t_m$$\nThe miss penalty, $t_m$, is common to both configurations as stated in the problem.\n\nThe problem asks for the break-even miss rate for Cache B, $MR_B$, which is the value of $MR_B$ that makes the AMAT of both caches equal. Therefore, we set $AMAT_A = AMAT_B$:\n$$t_{hA} + MR_A \\cdot t_m = t_{hB} + MR_B \\cdot t_m$$\n\nOur goal is to solve this equation for $MR_B$. We begin by isolating the term containing $MR_B$:\n$$MR_B \\cdot t_m = t_{hA} - t_{hB} + MR_A \\cdot t_m$$\n\nThe problem states that all time quantities are positive, which implies $t_m > 0$. We can therefore divide both sides of the equation by $t_m$ without ambiguity:\n$$MR_B = \\frac{t_{hA} - t_{hB} + MR_A \\cdot t_m}{t_m}$$\n\nThis expression can be separated into two terms for clarity:\n$$MR_B = \\frac{t_{hA} - t_{hB}}{t_m} + \\frac{MR_A \\cdot t_m}{t_m}$$\n$$MR_B = MR_A + \\frac{t_{hA} - t_{hB}}{t_m}$$\n\nThis is the final, closed-form symbolic expression for the break-even miss rate of Cache B, $MR_B$, in terms of the given quantities $t_{hA}$, $t_{hB}$, $MR_A$, and $t_m$.",
            "answer": "$$\\boxed{MR_{A} + \\frac{t_{hA} - t_{hB}}{t_{m}}}$$"
        },
        {
            "introduction": "While the $AMAT$ formula provides a high-level view, a deep understanding of cache performance requires exploring the root causes of misses. This practice  delves into the pathological behavior known as 'cache thrashing,' where a program's access pattern conflicts with the cache's replacement policy, leading to near-zero hit rates. By designing a sequence of memory accesses that consistently evicts data just before it is needed, you will gain a concrete, mechanical understanding of how conflict misses arise in a set-associative cache.",
            "id": "3626035",
            "problem": "A processor uses a Level-1 (L1) data cache that is a $2$-way set-associative cache with total capacity $C$ bytes and block size $B$ bytes. The replacement policy is Least Recently Used (LRU), and the cache is initially empty. Consider three distinct memory blocks with addresses $X$, $Y$, and $Z$ that map to the same set index in this cache. Each access is a read. The processor repeatedly issues a deterministic infinite sequence of accesses over $\\{X,Y,Z\\}$ that alternates among these three blocks in order to maximize conflict misses under LRU.\n\nUsing only core definitions of cache hit, cache miss, hit rate, and Average Memory Access Time (AMAT), do the following:\n\n- Design an explicit infinite access sequence over $\\{X,Y,Z\\}$ that alternates and maximizes conflict misses for this $2$-way set-associative LRU cache.\n- From first principles, derive the steady-state hit rate $H$ for this access sequence, where $H$ is the long-run fraction of accesses that are hits after any transient warm-up effects.\n- Given a hit latency of $t_h = 3$ cycles and a miss penalty of $t_m = 120$ cycles, compute the steady-state Average Memory Access Time (AMAT) in cycles.\n\nExpress $H$ as a decimal or fraction, and express AMAT in cycles. No rounding is required. The final answer must be the pair $\\{H,\\text{AMAT}\\}$.",
            "solution": "The problem is evaluated as scientifically grounded, well-posed, and objective. All necessary information is provided to determine the access sequence, hit rate, and Average Memory Access Time (AMAT). The parameters and concepts are standard in computer architecture. The problem is valid.\n\nThe solution is developed in three parts as requested: designing the access sequence, deriving the steady-state hit rate, and computing the AMAT.\n\n**Part 1: Design of the Infinite Access Sequence**\n\nThe cache is $k=2$-way set-associative, meaning each set can hold exactly $2$ distinct memory blocks. The replacement policy is Least Recently Used (LRU), which dictates that when a new block must be brought into a full set, the block that has not been accessed for the longest time is evicted.\n\nWe are given $3$ distinct memory blocks, with addresses $X$, $Y$, and $Z$, that all map to the same cache set. The task is to design a deterministic, alternating, infinite access sequence over $\\{X, Y, Z\\}$ that maximizes conflict misses. A conflict miss occurs when a block is evicted from a set to make room for another block that maps to the same set, and the evicted block is accessed again later.\n\nTo maximize misses, we should access the blocks in a way that ensures the requested block is never in the cache. This phenomenon is known as thrashing. Consider the cyclic access sequence: $S = (X, Y, Z, X, Y, Z, \\dots)$. Let's trace the state of the single relevant cache set, which is initially empty. The tuple will denote the blocks in the set, with the rightmost element being the Most Recently Used (MRU) and the leftmost being the Least Recently Used (LRU).\n\n1.  Access $X$: **Miss** (compulsory). The cache is empty. $X$ is loaded.\n    - Set State: $\\{X\\}$\n    - LRU Status: $X$ is MRU.\n\n2.  Access $Y$: **Miss** (compulsory). The set is not full. $Y$ is loaded.\n    - Set State: $\\{X, Y\\}$\n    - LRU Status: $Y$ is MRU, $X$ is LRU.\n\n3.  Access $Z$: **Miss** (conflict). The set is full with $\\{X, Y\\}$. According to LRU, the least recently used block, $X$, is evicted. $Z$ is loaded.\n    - Set State: $\\{Y, Z\\}$\n    - LRU Status: $Z$ is MRU, $Y$ is LRU.\n\n4.  Access $X$: **Miss** (conflict). The set is full with $\\{Y, Z\\}$. The LRU block is $Y$. $Y$ is evicted and $X$ is loaded.\n    - Set State: $\\{Z, X\\}$\n    - LRU Status: $X$ is MRU, $Z$ is LRU.\n\n5.  Access $Y$: **Miss** (conflict). The set is full with $\\{Z, X\\}$. The LRU block is $Z$. $Z$ is evicted and $Y$ is loaded.\n    - Set State: $\\{X, Y\\}$\n    - LRU Status: $Y$ is MRU, $X$ is LRU.\n\n6.  Access $Z$: **Miss** (conflict). The set is full with $\\{X, Y\\}$. The LRU block is $X$. $X$ is evicted and $Z$ is loaded.\n    - Set State: $\\{Y, Z\\}$\n    - LRU Status: $Z$ is MRU, $Y$ is LRU.\n\nThe pattern of cache states $\\{Y, Z\\} \\to \\{Z, X\\} \\to \\{X, Y\\}$ repeats. For any access in this steady-state cycle (from access $3$ onwards), the block being requested is precisely the one that was evicted two steps prior. For example, $Z$ evicts $X$ at step $3$, then $X$ is requested at step $4$. $X$ evicts $Y$ at step $4$, then $Y$ is requested at step $5$. $Y$ evicts $Z$ at step $5$, then $Z$ would be requested at step $6$ if the sequence was $X,Y,Z,X,Y,Z...$. The provided trace confirms this.\n\nThis sequence results in a miss for every access in the steady state. A hit rate of $0$ is the minimum possible, which means the miss rate is maximized. Therefore, the explicit infinite access sequence that maximizes conflict misses is the repeating sequence $(X, Y, Z)^\\infty$, which is $X, Y, Z, X, Y, Z, \\dots$. Any cyclic permutation, such as $(Y, Z, X)^\\infty$, would also satisfy the condition.\n\n**Part 2: Derivation of the Steady-State Hit Rate ($H$)**\n\nThe steady-state hit rate, $H$, is the fraction of accesses that are hits in the long run, after any initial transient effects (like compulsory misses) have occurred. As established above, for the sequence $S = (X, Y, Z, X, Y, Z, \\dots)$, every access from the third one onwards is a conflict miss.\n\nThe hit rate is defined as:\n$$ H = \\lim_{N \\to \\infty} \\frac{\\text{Number of Hits in the first } N \\text{ accesses}}{\\text{Total number of accesses } (N)} $$\nIn our sequence, the number of hits is always $0$ for any $N \\geq 1$. Let $N_h(N)$ be the number of hits in the first $N$ accesses.\n$$ N_h(N) = 0 \\quad \\forall N \\in \\mathbb{Z}^+ $$\nTherefore, the hit rate is:\n$$ H = \\lim_{N \\to \\infty} \\frac{0}{N} = 0 $$\nThe steady-state hit rate $H$ is $0$.\n\n**Part 3: Computation of the Average Memory Access Time (AMAT)**\n\nThe Average Memory Access Time (AMAT) is the weighted average of the time for a hit and the time for a miss.\nThe core definition is:\n$$ \\text{AMAT} = (\\text{Hit Rate} \\times \\text{Time per Hit}) + (\\text{Miss Rate} \\times \\text{Time per Miss}) $$\nLet $H$ be the hit rate and $M$ be the miss rate. By definition, $M = 1 - H$.\nLet $t_h$ be the hit latency (time per hit) and $t_{miss}$ be the total time required to service a miss.\nA memory access first involves checking the cache, which takes time $t_h$. If it is a hit, the access is complete. If it is a miss, an additional time, the miss penalty ($t_m$), is incurred to fetch the data from the next level of memory.\nThus:\n- Time per Hit $= t_h$\n- Time per Miss $= t_{miss} = t_h + t_m$\n\nSubstituting these into the AMAT formula:\n$$ \\text{AMAT} = H \\cdot t_h + (1 - H) \\cdot (t_h + t_m) $$\nThis expression can be simplified:\n$$ \\text{AMAT} = H \\cdot t_h + t_h + t_m - H \\cdot t_h - H \\cdot t_m $$\n$$ \\text{AMAT} = t_h + (1 - H) \\cdot t_m $$\nThis is the standard formula for AMAT.\n\nWe are given the following values:\n- Hit latency, $t_h = 3$ cycles\n- Miss penalty, $t_m = 120$ cycles\n\nFrom Part 2, we derived the steady-state hit rate:\n- Hit rate, $H = 0$\n\nNow, we substitute these values into the AMAT formula:\n$$ \\text{AMAT} = 3 \\text{ cycles} + (1 - 0) \\times 120 \\text{ cycles} $$\n$$ \\text{AMAT} = 3 + 1 \\times 120 $$\n$$ \\text{AMAT} = 3 + 120 $$\n$$ \\text{AMAT} = 123 \\text{ cycles} $$\n\nThe steady-state hit rate $H$ is $0$, and the steady-state Average Memory Access Time is $123$ cycles. The final answer is the pair $\\{H, \\text{AMAT}\\}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 123 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Identifying a performance bottleneck like cache thrashing is only the first step; the next is to evaluate potential solutions. This exercise  models a realistic scenario where two data arrays cause severe conflict misses in a direct-mapped cache, and then asks you to analyze the impact of an architectural upgrade to a skewed-associative design. By calculating the average memory access time for both configurations and computing the resulting speedup, you will learn to quantitatively justify hardware design choices that mitigate pathological cache behavior.",
            "id": "3626006",
            "problem": "A uniprocessor system has a Level-1 (L1) data cache that is physically indexed, physically tagged, and initially direct-mapped. The L1 cache has total capacity $C = 64\\,\\mathrm{KiB}$ and block size $B = 64\\,\\mathrm{B}$. Thus the number of cache lines is $C/B$, and the number of indexable sets equals $C/B$ because the associativity is initially one. Consider two arrays, $\\mathcal{A}$ and $\\mathcal{B}$, each of size exactly $C$, stored in contiguous memory and aligned to $C$-byte boundaries. The arrays are accessed with a spatial stride equal to the cache block size so that each access touches a new block within an array. Specifically, define the pass through the arrays as the infinite repetition of the following block-wise alternating sequence:\n- For $i = 0, 1, 2, \\dots, (C/B) - 1$: access $\\mathcal{A}[i \\cdot B]$, then access $\\mathcal{B}[i \\cdot B]$.\nThe base virtual addresses of $\\mathcal{A}$ and $\\mathcal{B}$ are set so that for every block index $i$, the block $\\mathcal{A}[i \\cdot B]$ and $\\mathcal{B}[i \\cdot B]$ map to the same L1 cache index but have different tags. For example, take $\\mathrm{base}(\\mathcal{B}) = \\mathrm{base}(\\mathcal{A}) + C$. Assume a perfect translation lookaside buffer and no virtual aliasing effects.\nThe Level-2 (L2) cache is large enough to hold both arrays simultaneously in steady state and is highly associative, with block size equal to $B$. After an initial warm-up, the L2 cache hits on every L1 miss. The L1 hit time is $1.0\\,\\mathrm{ns}$, the L2 hit time (additional latency beyond the L1 hit time when servicing an L1 miss that hits in L2) is $8.0\\,\\mathrm{ns}$, and main memory is never accessed in steady state for this experiment.\nTasks:\n1) Using only core definitions of cache hit, miss, miss rate, and the notion of time per access as an expectation over hits and misses, design the above access pattern to induce worst-case conflict behavior in the direct-mapped L1 cache and justify, in the steady state (ignore the compulsory first touches), the long-run L1 miss rate for this pattern.\n2) Using your miss-rate conclusion from part (1) and the given timing parameters, derive symbolically and then compute the long-run average memory access time per access in steady state for the initial direct-mapped L1 design.\nNow suppose the L1 is redesigned to a $2$-way skewed-associative cache of the same total capacity $C$ and block size $B$, using two independent index functions (one per way) that distribute blocks uniformly across sets. Assume that under this redesign and the same access pattern, the steady-state L1 miss rate achieves the ideal lower bound imposed by the working-set to cache-size ratio, namely exactly one half of the accesses miss in the L1 in steady state. The L1 hit time increases by exactly $10\\%$ due to the added way and hashing logic, while all other timing parameters are unchanged.\n3) Using the same first-principles reasoning as in part (2), derive and compute the steady-state average memory access time for the skewed-associative L1.\nFinally, report as your single numerical result the speedup $S$, defined as the ratio of the initial average access time to the redesigned average access time. Round your final answer for $S$ to four significant figures. Express $S$ as a pure number with no units.",
            "solution": "The problem is first validated against the specified criteria.\n\n**Step 1: Extract Givens**\n- L1 cache initial configuration: physically indexed, physically tagged, direct-mapped (associativity $k=1$).\n- L1 cache capacity: $C = 64\\,\\mathrm{KiB}$.\n- L1 cache block size: $B = 64\\,\\mathrm{B}$.\n- Number of L1 sets (direct-mapped): $N = C/B$.\n- Arrays: $\\mathcal{A}$ and $\\mathcal{B}$, each of size $C$.\n- Array memory layout: Contiguous, aligned to $C$-byte boundaries.\n- Access pattern: Infinite repetition of the sequence: For $i = 0, 1, \\dots, (C/B) - 1$, access $\\mathcal{A}[i \\cdot B]$, then access $\\mathcal{B}[i \\cdot B]$.\n- Mapping constraint: For every block index $i$, $\\mathcal{A}[i \\cdot B]$ and $\\mathcal{B}[i \\cdot B]$ map to the same L1 cache index but have different tags.\n- System assumptions: Perfect Translation Lookaside Buffer (TLB), no virtual aliasing.\n- L2 cache: Large enough to hold both arrays, highly associative, block size $B$.\n- Steady-state L2 behavior: Every L1 miss is an L2 hit. Main memory is not accessed.\n- Timing for initial design:\n  - L1 hit time: $T_{H1} = 1.0\\,\\mathrm{ns}$.\n  - L2 hit time (additional latency for an L1 miss): $T_{H2} = 8.0\\,\\mathrm{ns}$.\n- L1 cache redesigned configuration: $2$-way skewed-associative, same $C$ and $B$.\n- Skewed-associative cache properties: two independent, uniform index functions.\n- Steady-state L1 miss rate for redesigned cache: $m_{1, \\text{skewed}} = 1/2$.\n- Timing for redesigned design:\n  - New L1 hit time: $T'_{H1} = T_{H1} \\times (1 + 0.10) = 1.1 \\times T_{H1}$.\n  - Other timings ($T_{H2}$) are unchanged.\n- Final requested result: Speedup $S = \\frac{\\text{AMAT}_{\\text{initial}}}{\\text{AMAT}_{\\text{redesigned}}}$, rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is based on fundamental principles of computer architecture, specifically cache memory organization, performance metrics like miss rate and Average Memory Access Time (AMAT), and the concept of conflict misses. The scenario described is a standard textbook example used to illustrate the pathological case of cache conflicts. All parameters are physically realistic for a modern processor.\n- **Well-Posed**: The problem is clearly defined, and all necessary data for the calculations are provided. The definitions of timing parameters, such as the L2 hit time being an \"additional latency,\" are precise and map directly to the miss penalty term in the AMAT formula.\n- **Objective**: The problem statement is free of subjective language and uses precise technical terminology.\n- **Consistency**: The givens are self-consistent. The condition that $\\mathcal{A}[i \\cdot B]$ and $\\mathcal{B}[i \\cdot B]$ map to the same L1 index is a direct consequence of the specified memory layout (e.g., $\\mathrm{base}(\\mathcal{B}) = \\mathrm{base}(\\mathcal{A}) + C$) for a direct-mapped cache with $C/B$ sets where $C$ is a power of two, which is standard.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically sound, well-posed, objective, and internally consistent. A complete solution can be derived from the provided information.\n\n**Solution Derivation**\n\nThe problem asks for an analysis of cache performance under two different L1 cache designs. We will address each part of the problem in sequence.\n\n**Part 1: L1 Miss Rate for the Direct-Mapped Cache**\n\nThe L1 cache is direct-mapped, meaning each memory block can map to only one specific cache line (or set). The number of sets is $N = C/B = (64 \\times 1024\\,\\mathrm{B}) / (64\\,\\mathrm{B}) = 1024$. The access pattern is an infinite loop over the blocks of arrays $\\mathcal{A}$ and $\\mathcal{B}$. For each block index $i$, the block from $\\mathcal{A}$ is accessed, followed by the block from $\\mathcal{B}$. A critical piece of information is that for any given $i$, the memory block containing $\\mathcal{A}[i \\cdot B]$ and the memory block containing $\\mathcal{B}[i \\cdot B]$ map to the same L1 cache set. They have different tags, confirming they are distinct memory blocks.\n\nLet's analyze the sequence of events for a single cache set, say set $j$, which corresponds to some block index $i$. Let the block from $\\mathcal{A}$ be denoted $A_i$ and the block from $\\mathcal{B}$ be denoted $B_i$.\nThe access sequence pertaining to this set is $\\dots, A_i, B_i, A_i, B_i, \\dots$. We are interested in the steady state, so we ignore the initial cold misses.\n\n1.  Assume at some point in the steady-state execution, we access block $A_i$. Just before this access, the cache set $j$ must contain block $B_i$, which was loaded during the previous iteration of the loop (when $\\mathcal{B}[i \\cdot B]$ was accessed). Since set $j$ holds $B_i$, the access to $A_i$ results in a **miss**. Because the cache is direct-mapped, block $B_i$ is evicted and block $A_i$ is loaded into set $j$.\n\n2.  Immediately following the access to $A_i$, the program accesses block $B_i$. The cache set $j$ now contains block $A_i$. The access to $B_i$ is therefore also a **miss**. Block $A_i$ is evicted, and block $B_i$ is loaded into set $j$.\n\nThis cycle of mutual eviction repeats for every pair of accesses $(\\mathcal{A}[i \\cdot B], \\mathcal{B}[i \\cdot B])$ and for every block index $i$. Every single memory access in the sequence results in a conflict miss. This access pattern is perfectly designed to thrash the direct-mapped cache, creating the worst-case scenario.\n\nTherefore, the steady-state L1 miss rate, $m_{1, \\text{direct}}$, is the ratio of misses to total accesses. Since every access is a miss, the miss rate is $1$.\n$$m_{1, \\text{direct}} = 1.0$$\n\n**Part 2: Average Memory Access Time (AMAT) for the Direct-Mapped Cache**\n\nThe Average Memory Access Time (AMAT) is the expected time for a memory access. It is calculated using the L1 hit time ($T_{H1}$), the L1 miss rate ($m_1$), and the L1 miss penalty ($M_{P1}$). The general formula is:\n$$AMAT = T_{H1} + m_1 \\times M_{P1}$$\nThe problem states that any L1 miss is serviced by the L2 cache, which always hits in steady state. The additional latency for this service is $T_{H2}$. This additional latency is precisely the L1 miss penalty, $M_{P1} = T_{H2}$.\n\nFor the initial direct-mapped design, let's denote the AMAT as $AMAT_{\\text{direct}}$. We have:\n- L1 hit time: $T_{H1} = 1.0\\,\\mathrm{ns}$\n- L1 miss rate: $m_{1, \\text{direct}} = 1.0$ (from Part 1)\n- L1 miss penalty: $M_{P1} = T_{H2} = 8.0\\,\\mathrm{ns}$\n\nSubstituting these values into the AMAT formula:\n$$AMAT_{\\text{direct}} = T_{H1} + m_{1, \\text{direct}} \\times T_{H2}$$\n$$AMAT_{\\text{direct}} = 1.0\\,\\mathrm{ns} + (1.0) \\times (8.0\\,\\mathrm{ns}) = 1.0\\,\\mathrm{ns} + 8.0\\,\\mathrm{ns} = 9.0\\,\\mathrm{ns}$$\n\n**Part 3: AMAT for the Skewed-Associative Cache**\n\nThe L1 cache is now redesigned to be $2$-way skewed-associative with the same capacity $C$ and block size $B$. This redesign affects the L1 hit time and the L1 miss rate.\n\nThe new parameters are:\n- New L1 hit time: $T'_{H1} = T_{H1} \\times (1 + 0.10) = 1.0\\,\\mathrm{ns} \\times 1.1 = 1.1\\,\\mathrm{ns}$.\n- New L1 miss rate: $m_{1, \\text{skewed}} = 1/2 = 0.5$, as given in the problem statement.\n- The L1 miss penalty is unchanged, as the L2 cache system is not modified: $M'_{P1} = T_{H2} = 8.0\\,\\mathrm{ns}$.\n\nWe can calculate the new AMAT, denoted $AMAT_{\\text{skewed}}$, using the same formula:\n$$AMAT_{\\text{skewed}} = T'_{H1} + m_{1, \\text{skewed}} \\times T_{H2}$$\nSubstituting the new values:\n$$AMAT_{\\text{skewed}} = 1.1\\,\\mathrm{ns} + (0.5) \\times (8.0\\,\\mathrm{ns}) = 1.1\\,\\mathrm{ns} + 4.0\\,\\mathrm{ns} = 5.1\\,\\mathrm{ns}$$\n\n**Final Calculation: Speedup S**\n\nThe speedup $S$ is defined as the ratio of the initial (slower) average access time to the redesigned (faster) average access time.\n$$S = \\frac{AMAT_{\\text{direct}}}{AMAT_{\\text{skewed}}}$$\nUsing the computed values:\n$$S = \\frac{9.0\\,\\mathrm{ns}}{5.1\\,\\mathrm{ns}} = \\frac{9.0}{5.1} = \\frac{90}{51} = \\frac{30}{17}$$\nTo provide the final numerical answer, we compute the value of this fraction and round it to four significant figures.\n$$S = \\frac{30}{17} \\approx 1.76470588\\dots$$\nRounding to four significant figures gives $1.765$.",
            "answer": "$$\\boxed{1.765}$$"
        }
    ]
}