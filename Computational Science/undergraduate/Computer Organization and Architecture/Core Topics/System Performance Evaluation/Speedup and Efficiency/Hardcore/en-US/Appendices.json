{
    "hands_on_practices": [
        {
            "introduction": "Optimizing code for modern processors often involves complex trade-offs. This practice explores one such scenario: loop unrolling. While this compiler technique can reduce the overhead of branch instructions, it also increases the static code size, which can put pressure on the instruction cache. By calculating the total execution time before and after unrolling, you will gain a quantitative understanding of how competing factors—dynamic instruction count and cache performance—determine the final speedup. ",
            "id": "3679725",
            "problem": "A program executes a hot loop with $N$ iterations on a single-issue, in-order core at fixed clock frequency. Each original loop iteration performs $L$ loads, $A$ Arithmetic Logic Unit (ALU) operations, and $B$ branches. The Cycles Per Instruction (CPI) of each instruction type is assumed constant and given by $\\mathrm{CPI}_{\\mathrm{load}}$, $\\mathrm{CPI}_{\\mathrm{ALU}}$, and $\\mathrm{CPI}_{\\mathrm{branch}}$. The instruction cache (I-cache) contributes additional stall cycles per instruction, modeled as $r \\cdot p$, where $r$ is the I-cache miss rate in misses per instruction and $p$ is the miss penalty in cycles per miss. Assume the data-side behavior and all other microarchitectural features remain unchanged between versions. Use the foundational definition\n$$\n\\mathrm{CPI}_{\\text{total}} \\;=\\; \\sum_{i \\in \\{\\text{load}, \\text{ALU}, \\text{branch}\\}} f_i \\cdot \\mathrm{CPI}_i \\;+\\; r \\cdot p,\n$$\nwhere $f_i$ is the dynamic fraction of instruction type $i$ in the executed mix, and the execution time at fixed frequency is proportional to $\\text{IC} \\cdot \\mathrm{CPI}_{\\text{total}}$, with $\\text{IC}$ the dynamic instruction count. Speedup $S$ is defined as $S \\;=\\; T_{\\text{original}} / T_{\\text{unrolled}}$.\n\nConsider the following concrete scenario:\n- Original loop body per iteration: $L = 3$, $A = 4$, $B = 1$ (so $8$ instructions per iteration).\n- Instruction-type CPIs: $\\mathrm{CPI}_{\\mathrm{load}} = 1.2$, $\\mathrm{CPI}_{\\mathrm{ALU}} = 1.0$, $\\mathrm{CPI}_{\\mathrm{branch}} = 3.0$.\n- Original I-cache miss rate and penalty: $r_{\\text{orig}} = 0.01$, $p = 10$.\n- The compiler unrolls the loop by factor $U = 4$, replicating the body and reducing the dynamic branch count by a factor of $1/U$ while leaving $L$ and $A$ per original iteration unchanged. Assume $N$ is divisible by $U$ and ignore any remainder handling.\n- Due to increased code size, the unrolled version’s I-cache miss rate rises to $r_{\\text{unroll}} = 0.03$; $p$ is unchanged.\n\nUnder these assumptions and using only the definitions above, which option is the closest value to the speedup $S$ achieved by the unrolled version relative to the original?\n\nA. $S \\approx 1.08$\n\nB. $S \\approx 0.95$\n\nC. $S \\approx 1.00$\n\nD. $S \\approx 1.15$",
            "solution": "The derivation must proceed from the foundational definitions. First, at fixed clock frequency, execution time $T$ is proportional to $\\text{IC} \\cdot \\mathrm{CPI}_{\\text{total}}$. Therefore, speedup $S$ can be written as\n$$\nS \\;=\\; \\frac{T_{\\text{original}}}{T_{\\text{unrolled}}} \\;=\\; \\frac{\\text{IC}_{\\text{orig}} \\cdot \\mathrm{CPI}_{\\text{orig}}}{\\text{IC}_{\\text{unroll}} \\cdot \\mathrm{CPI}_{\\text{unroll}}}.\n$$\nWe compute the dynamic instruction mix fractions $f_i$ and the total CPI for each version, then the ratio of $\\text{IC} \\cdot \\mathrm{CPI}$.\n\nOriginal version:\n- Per iteration counts: loads $= L = 3$, ALU $= A = 4$, branches $= B = 1$, total $= 3 + 4 + 1 = 8$ instructions.\n- Fractions: $f_{\\text{load,orig}} = 3/8 = 0.375$, $f_{\\text{ALU,orig}} = 4/8 = 0.5$, $f_{\\text{branch,orig}} = 1/8 = 0.125$.\n- I-cache stall per instruction: $r_{\\text{orig}} \\cdot p = 0.01 \\cdot 10 = 0.1$.\n- Total CPI:\n$$\n\\mathrm{CPI}_{\\text{orig}} \\;=\\; 0.375 \\cdot 1.2 \\;+\\; 0.5 \\cdot 1.0 \\;+\\; 0.125 \\cdot 3.0 \\;+\\; 0.1 \\;=\\; 0.45 \\;+\\; 0.5 \\;+\\; 0.375 \\;+\\; 0.1 \\;=\\; 1.425.\n$$\n- Dynamic instruction count per original iteration is $8$. Over $N$ iterations, $\\text{IC}_{\\text{orig}} = 8N$.\n\nUnrolled version ($U = 4$):\n- Loads and ALU operations per original iteration remain the same in total: loads $= 3$, ALU $= 4$.\n- Branches per original iteration reduce by factor $1/U$: branches $= 1/4 = 0.25$.\n- Total instructions per original iteration after unrolling: $3 + 4 + 0.25 = 7.25$, so $\\text{IC}_{\\text{unroll}} = 7.25N$.\n- Fractions: $f_{\\text{load,unroll}} = 3/7.25 \\approx 0.413793$, $f_{\\text{ALU,unroll}} = 4/7.25 \\approx 0.551724$, $f_{\\text{branch,unroll}} = 0.25/7.25 \\approx 0.0344828$.\n- I-cache stall per instruction: $r_{\\text{unroll}} \\cdot p = 0.03 \\cdot 10 = 0.3$.\n- Total CPI:\n$$\n\\mathrm{CPI}_{\\text{unroll}} \\;=\\; 0.413793 \\cdot 1.2 \\;+\\; 0.551724 \\cdot 1.0 \\;+\\; 0.0344828 \\cdot 3.0 \\;+\\; 0.3.\n$$\nCompute the terms:\n$$\n0.413793 \\cdot 1.2 \\approx 0.4965516,\\quad 0.551724 \\cdot 1.0 = 0.551724,\\quad 0.0344828 \\cdot 3.0 \\approx 0.1034484.\n$$\nSum:\n$$\n\\mathrm{CPI}_{\\text{unroll}} \\;\\approx\\; 0.4965516 \\;+\\; 0.551724 \\;+\\; 0.1034484 \\;+\\; 0.3 \\;=\\; 1.451724.\n$$\n\nNow compute the speedup:\n$$\nS \\;=\\; \\frac{\\text{IC}_{\\text{orig}} \\cdot \\mathrm{CPI}_{\\text{orig}}}{\\text{IC}_{\\text{unroll}} \\cdot \\mathrm{CPI}_{\\text{unroll}}} \\;=\\; \\frac{8N \\cdot 1.425}{7.25N \\cdot 1.451724} \\;=\\; \\frac{8 \\cdot 1.425}{7.25 \\cdot 1.451724}.\n$$\nCompute numerator: $8 \\cdot 1.425 = 11.4$.\nCompute denominator: $7.25 \\cdot 1.451724 \\approx 10.525$.\nTherefore,\n$$\nS \\;\\approx\\; \\frac{11.4}{10.525} \\;\\approx\\; 1.083.\n$$\nThis is closest to $1.08$ among the options.\n\nConceptual discussion from first principles: Loop unrolling reduces dynamic branch instructions, which lowers the dynamic instruction count and shifts the mix toward loads and ALU operations. This reduces the branch contribution to $\\mathrm{CPI}_{\\text{total}}$ and $\\text{IC}$, but larger code can increase the I-cache miss rate, raising the stall term $r \\cdot p$ and potentially increasing $\\mathrm{CPI}_{\\text{total}}$. In this model, unrolling improves speed (i.e., $S  1$) precisely when the product $\\text{IC}_{\\text{unroll}} \\cdot \\mathrm{CPI}_{\\text{unroll}}$ is less than $\\text{IC}_{\\text{orig}} \\cdot \\mathrm{CPI}_{\\text{orig}}$, which here holds because while $\\mathrm{CPI}$ slightly increases (from $1.425$ to $1.451724$), the dynamic instruction count decreases more (from $8N$ to $7.25N$), yielding a net reduction in total cycles.\n\nOption-by-option analysis:\n- A. $S \\approx 1.08$. As computed, $S \\approx 1.083$, so this is Correct.\n- B. $S \\approx 0.95$. This claims a slowdown ($S  1$), which contradicts the computed $S \\approx 1.083$. Incorrect.\n- C. $S \\approx 1.00$. This claims no change, but the computed speedup is greater than $1$. Incorrect.\n- D. $S \\approx 1.15$. This overestimates the benefit; our computed value is substantially lower. Incorrect.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The performance of the memory hierarchy is critical to overall system speed, and the choice of cache write policy is a fundamental design decision. This exercise contrasts the write-through and write-back policies in the context of a multicore processor, where the shared bus is a precious resource. You will analyze the bus traffic generated by each policy during different workload phases, providing insight into how write strategies impact memory bandwidth consumption and inter-core coherence. ",
            "id": "3679713",
            "problem": "Consider a shared-memory multiprocessor with $N = 8$ identical cores. Each core has a private Level-1 (L1) data cache. Two alternative cache write policies are considered: write-through (WT) and write-back (WB). The cache coherence protocol is Modified-Shared-Invalid (MSI). The cache-line size is $L = 64$ bytes, and the machine word size is $w = 8$ bytes. The interconnect is a single snooping bus with peak data bandwidth $R = 25$ gigabytes per second (GB/s). The bus service time is dominated by data and coherence traffic; computation, queuing, and other latencies are negligible. A coherence invalidation broadcast (including directory or snoop metadata carried on the bus) consumes $b_{\\mathrm{inv}} = 16$ bytes on the bus. Read misses fetch an entire cache line, costing $L$ bytes on the bus. Under write-through, each write transmits the written word to memory, costing $w$ bytes per write. Under write-back, dirty lines are written back only once on eviction, costing $L$ bytes per dirty line. Assume write-allocate on a write miss for both policies.\n\nA parallel workload executes two phases sequentially:\n\n- Phase $\\mathcal{R}$ (read-mostly): Each core performs $2.0 \\times 10^{6}$ word reads to a shared array, scanning sequentially so that each core touches $n_{\\mathrm{R}} = \\frac{2.0 \\times 10^{6}}{8} = 250{,}000$ distinct cache lines. For modeling, assume that for each core, each of its $n_{\\mathrm{R}}$ lines is brought from memory exactly once (one read miss per line per core), and that the coherence overhead due to shared reads is negligible compared to data movement.\n\n- Phase $\\mathcal{W}$ (write-heavy): Each core writes to $n_{\\mathrm{W}} = 125{,}000$ lines drawn from the same array. For each such line, a core performs $n_{\\mathrm{per\\_line}} = 16$ distinct word writes within the line. At the start of Phase $\\mathcal{W}$, assume that each line to be written by a core is not resident in that core and must be fetched once before the first write, and that, on average, there is exactly one other sharer for each such line, so that obtaining exclusive permission for the first write generates a single invalidation broadcast per line. Under write-back, assume each line written by a core becomes dirty and is evicted once at the end of the phase (one write-back per such line). No cache-to-cache transfers occur; all misses are served by main memory.\n\nThe total execution time of each phase is taken to be the total bus transfer time, which equals the total bytes transmitted on the bus divided by $R$. Let the overall execution time of the workload be the sum of the two phase times. Using the traffic model above, compute the overall speedup of write-back relative to write-through across the entire workload, defined as the ratio of the total execution time under write-through to the total execution time under write-back. Round your final answer to four significant figures. The answer is dimensionless.",
            "solution": "The objective is to compute the speedup, $S$, of the write-back (WB) policy relative to the write-through (WT) policy. The speedup is defined as the ratio of the total execution times:\n$$S = \\frac{T_{WT}}{T_{WB}}$$\nThe problem states that the execution time for any phase is determined solely by the bus traffic it generates. The total execution time $T$ is the total bytes transferred on the bus, $B$, divided by the bus bandwidth, $R$.\n$$T = \\frac{B}{R}$$\nTherefore, the speedup can be expressed as a ratio of the total bus traffic generated by each policy:\n$$S = \\frac{B_{WT} / R}{B_{WB} / R} = \\frac{B_{WT}}{B_{WB}}$$\nThe total traffic is the sum of the traffic from Phase $\\mathcal{R}$ and Phase $\\mathcal{W}$: $B = B_{\\mathcal{R}} + B_{\\mathcal{W}}$. We will now calculate the total bus traffic for each policy.\n\n**1. Bus Traffic for Phase $\\mathcal{R}$**\n\nIn Phase $\\mathcal{R}$, each of the $N$ cores incurs a read miss for each of the $n_{\\mathrm{R}}$ distinct cache lines it accesses. A read miss fetches one full cache line of size $L$ from memory. The read behavior and resulting traffic are independent of the write policy. Thus, the traffic for Phase $\\mathcal{R}$ is the same for both WT and WB.\n$$B_{\\mathcal{R}, WT} = B_{\\mathcal{R}, WB} = N \\times n_{\\mathrm{R}} \\times L$$\nSubstituting the given values:\n$N = 8$\n$n_{\\mathrm{R}} = 250{,}000 = 2.5 \\times 10^5$\n$L = 64$ bytes\n$$B_{\\mathcal{R}} = 8 \\times (2.5 \\times 10^5) \\times 64 = 20 \\times 10^5 \\times 64 = 1280 \\times 10^5 = 1.28 \\times 10^8 \\text{ bytes}$$\n\n**2. Bus Traffic for Phase $\\mathcal{W}$**\n\nIn Phase $\\mathcal{W}$, we analyze the traffic components for each of the $N$ cores accessing its $n_{\\mathrm{W}}$ lines. For each line, the following events generate bus traffic:\n-   **Write Miss:** The line is not resident initially, so the first write to it causes a write miss. With a write-allocate policy, this triggers a \"read for ownership\" (RFO) request, which fetches the entire line from memory. This generates $L$ bytes of traffic.\n-   **Invalidation:** To gain exclusive ownership for writing, the core must invalidate other cached copies. The problem states there is one other sharer, so one invalidation broadcast is sent per line, generating $b_{\\mathrm{inv}}$ bytes of traffic.\n-   **Write Data Transfer:** The method of transferring the written data to memory depends on the policy.\n\n**2a. Traffic for Phase $\\mathcal{W}$ with Write-Through (WT)**\n\nUnder WT, every write operation is propagated to main memory. Each core performs $n_{\\mathrm{per\\_line}}$ word writes to each of its $n_{\\mathrm{W}}$ lines. Each word write places $w$ bytes on the bus.\nThe total traffic for Phase $\\mathcal{W}$ with WT is the sum of traffic from RFOs, invalidations, and all individual writes from all cores.\n$$B_{\\mathcal{W}, WT} = N \\times n_{\\mathrm{W}} \\times (L + b_{\\mathrm{inv}} + n_{\\mathrm{per\\_line}} \\times w)$$\nSubstituting the given values:\n$N = 8$\n$n_{\\mathrm{W}} = 125{,}000 = 1.25 \\times 10^5$\n$L = 64$ bytes\n$b_{\\mathrm{inv}} = 16$ bytes\n$n_{\\mathrm{per\\_line}} = 16$\n$w = 8$ bytes\n$$B_{\\mathcal{W}, WT} = 8 \\times (1.25 \\times 10^5) \\times (64 + 16 + 16 \\times 8)$$\n$$B_{\\mathcal{W}, WT} = 10^6 \\times (80 + 128) = 10^6 \\times 208 = 2.08 \\times 10^8 \\text{ bytes}$$\n\n**2b. Traffic for Phase $\\mathcal{W}$ with Write-Back (WB)**\n\nUnder WB, writes are made only to the local cache. The bus traffic occurs only when a modified (\"dirty\") line is evicted and written back to memory. The problem states that each of the $n_{\\mathrm{W}}$ lines written by a core becomes dirty and is evicted once, generating a single write-back of the entire line.\nThe total traffic for Phase $\\mathcal{W}$ with WB is the sum of traffic from RFOs, invalidations, and the final write-backs from all cores.\n$$B_{\\mathcal{W}, WB} = N \\times n_{\\mathrm{W}} \\times (L + b_{\\mathrm{inv}} + L) = N \\times n_{\\mathrm{W}} \\times (2L + b_{\\mathrm{inv}})$$\nSubstituting the values:\n$$B_{\\mathcal{W}, WB} = 8 \\times (1.25 \\times 10^5) \\times (2 \\times 64 + 16)$$\n$$B_{\\mathcal{W}, WB} = 10^6 \\times (128 + 16) = 10^6 \\times 144 = 1.44 \\times 10^8 \\text{ bytes}$$\n\n**3. Total Traffic and Speedup Calculation**\n\nNow we can compute the total traffic for each policy by summing the traffic from both phases.\n\nTotal traffic for WT:\n$$B_{WT} = B_{\\mathcal{R}} + B_{\\mathcal{W}, WT} = 1.28 \\times 10^8 + 2.08 \\times 10^8 = 3.36 \\times 10^8 \\text{ bytes}$$\n\nTotal traffic for WB:\n$$B_{WB} = B_{\\mathcal{R}} + B_{\\mathcal{W}, WB} = 1.28 \\times 10^8 + 1.44 \\times 10^8 = 2.72 \\times 10^8 \\text{ bytes}$$\n\nFinally, we compute the speedup $S$:\n$$S = \\frac{B_{WT}}{B_{WB}} = \\frac{3.36 \\times 10^8}{2.72 \\times 10^8} = \\frac{3.36}{2.72}$$\nTo simplify the fraction:\n$$S = \\frac{336}{272} = \\frac{168}{136} = \\frac{84}{68} = \\frac{21}{17}$$\nPerforming the division and rounding to four significant figures:\n$$S = \\frac{21}{17} \\approx 1.235294...$$\n$$S \\approx 1.235$$\nThe overall speedup of write-back relative to write-through is approximately $1.235$.",
            "answer": "$$\\boxed{1.235}$$"
        },
        {
            "introduction": "To achieve high memory bandwidth, modern DRAM systems are organized into multiple banks that can operate in parallel. However, this parallelism can only be exploited if memory requests are distributed evenly across the banks. This practice investigates how memory bank conflicts arise from strided memory access patterns, leading to significant throughput degradation. You will derive the performance impact of these conflicts and discover how a simple software modification—array padding—can restore ideal interleaved memory performance. ",
            "id": "3679711",
            "problem": "A system implements interleaved main memory with $N_b$ identical single-ported banks. Consecutive logical words are striped across banks by mapping the bank index of the $k$-th requested word as $b_k = (k \\cdot s) \\bmod N_b$, where $s$ is the stride in words. The memory controller can issue at most one request per bank per cycle. Assume large, steady-state sequences of requests (i.e., $k$ ranges over many consecutive integers so that transient effects from startup are negligible). The goal is to understand how stride-induced bank conflicts impact speedup and efficiency and to determine padding that removes conflicts.\n\nUsing only the following fundamental base:\n- Definitions of speedup and efficiency: $S = \\frac{T_{\\text{serial}}}{T_{\\text{parallel}}}$ and $E = \\frac{S}{P}$ for $P$ parallel resources.\n- The fact that the mapping $b_k = (k \\cdot s) \\bmod N_b$ is periodic in $k$ with a period that divides $N_b$.\n- Each bank can service at most one request per cycle.\n\nPerform the following:\n1. By reasoning from the periodic structure of $b_k$, determine the number of distinct banks visited by the stride access in steady state and derive the conflict factor $c$, defined as the average number of requests per bank within any window of $N_b$ consecutive requests. Express $c$ in terms of $N_b$ and $s$ without assuming any shortcut formulas.\n2. Using the definitions of speedup and efficiency, deduce the throughput degradation factor (the ratio of actual throughput to the ideal throughput with no conflicts) in terms of the conflict factor $c$ and $N_b$. Explain why this factor directly characterizes the loss in speedup relative to the ideal interleaving.\n3. For $N_b = 16$ and $s = 12$, compute the numerical values of the conflict factor $c$, the throughput degradation factor (express it as a simplified fraction), and the minimal positive padding $p$ such that the new stride $s' = s + p$ is coprime to $N_b$ (i.e., $s'$ shares no common divisors greater than $1$ with $N_b$), thereby enabling maximal speedup. No rounding is required.\n\nReport your final numerical results for part $3$ in the order: conflict factor $c$, throughput degradation factor, minimal padding $p$.",
            "solution": "The solution proceeds in three parts as requested.\n\n### Part 1: Derivation of the Conflict Factor $c$\n\nThe mapping from the $k$-th word request to a bank index $b_k$ is given by the linear congruential relation $b_k = (k \\cdot s) \\bmod N_b$. The set of bank indices visited in steady state is the set of all values generated by this expression as $k$ ranges over the integers. This is the set of all multiples of the stride $s$ modulo $N_b$.\n\nFrom elementary number theory, the set of distinct values generated by $(k \\cdot a) \\bmod n$ for $k \\in \\{0, 1, 2, \\dots\\}$ is $\\{0 \\cdot g, 1 \\cdot g, 2 \\cdot g, \\dots, (\\frac{n}{g}-1) \\cdot g\\}$, where $g = \\gcd(a, n)$.\nApplying this to our problem, with $a=s$ and $n=N_b$, the set of distinct bank indices visited is $\\{0, g, 2g, \\dots, (\\frac{N_b}{g}-1)g\\}$, where $g = \\gcd(s, N_b)$.\nThe number of distinct banks visited, $N_{distinct}$, is therefore the number of elements in this set, which is:\n$$ N_{distinct} = \\frac{N_b}{\\gcd(s, N_b)} $$\nThe problem defines the conflict factor $c$ as the average number of requests per bank within any window of $N_b$ consecutive requests. This definition is interpreted to mean the number of requests that are mapped to any single *active* bank within such a window.\n\nConsider a window of $N_b$ consecutive requests, for example, for $k = 0, 1, \\dots, N_b-1$. A total of $N_b$ requests are made. These $N_b$ requests are distributed among the $N_{distinct}$ active banks. Due to the periodic nature of the mapping, each of these $N_{distinct}$ banks is accessed an equal number of times over a sufficiently large window representative of steady-state behavior. A window of size $N_b$ contains exactly $\\gcd(s, N_b)$ full periods of the access pattern, where the period length is $N_b / \\gcd(s, N_b)$.\n\nThus, the total of $N_b$ requests are distributed uniformly over the $N_{distinct}$ active banks. The number of requests per active bank is:\n$$ c = \\frac{\\text{Total Requests}}{\\text{Number of Active Banks}} = \\frac{N_b}{N_{distinct}} = \\frac{N_b}{N_b / \\gcd(s, N_b)} $$\nSimplifying this expression gives the conflict factor $c$:\n$$ c = \\gcd(s, N_b) $$\nThis factor represents the degree of collision; a value of $c=1$ indicates no conflicts (all banks are visited before any is repeated), while a higher value indicates that requests are concentrated on fewer banks, leading to more frequent collisions.\n\n### Part 2: Derivation of the Throughput Degradation Factor\n\nSpeedup, $S$, is defined as the ratio of serial execution time to parallel execution time, $S = T_{\\text{serial}}/T_{\\text{parallel}}$. In an ideal parallel system with $P$ resources, the speedup is ideally $P$. For our interleaved memory system, the number of parallel resources is the number of memory banks, so $P = N_b$. The ideal speedup is $S_{ideal} = N_b$, which occurs when all banks can be utilized in parallel without conflict.\n\nThroughput is the number of requests serviced per unit time and is proportional to the speedup. The ideal throughput, $\\Theta_{ideal}$, corresponds to the ideal speedup $S_{ideal}$. The actual throughput, $\\Theta_{actual}$, corresponds to the actual speedup, $S_{actual}$. Therefore, the throughput degradation factor is the ratio of actual to ideal throughput, which is equal to the ratio of actual to ideal speedup:\n$$ \\text{Degradation Factor} = \\frac{\\Theta_{actual}}{\\Theta_{ideal}} = \\frac{S_{actual}}{S_{ideal}} $$\nThe speedup of a parallel system is fundamentally limited by the number of resources that can be effectively utilized. In the ideal case (e.g., when $s$ is coprime to $N_b$), all $N_b$ banks are utilized, so the effective parallelism is $N_b$.\nIn the general case with potential conflicts, only $N_{distinct} = N_b / \\gcd(s, N_b)$ banks are ever accessed. The remaining $N_b - N_{distinct}$ banks remain idle. The maximum achievable speedup is therefore limited by the number of active banks. Assuming these active banks can be utilized efficiently, the actual speedup is $S_{actual} = N_{distinct}$.\n\nSubstituting the expressions for ideal and actual speedup:\n$$ S_{ideal} = N_b $$\n$$ S_{actual} = N_{distinct} = \\frac{N_b}{\\gcd(s, N_b)} = \\frac{N_b}{c} $$\nThe throughput degradation factor is then:\n$$ \\text{Degradation Factor} = \\frac{S_{actual}}{S_{ideal}} = \\frac{N_b / c}{N_b} = \\frac{1}{c} $$\nThis factor directly characterizes the loss in speedup because it represents the fraction of the ideal parallel capability that is actually realized. A factor of $1/c$ means the system performs as if it had only $1/c$ of its total banks.\n\n### Part 3: Numerical Computations\n\nWe are given $N_b = 16$ and a stride of $s = 12$.\n\n**Conflict Factor $c$**:\nUsing the formula derived in Part 1:\n$$ c = \\gcd(s, N_b) = \\gcd(12, 16) $$\nTo find the greatest common divisor, we can use the prime factorization of each number:\n$12 = 2^2 \\cdot 3$\n$16 = 2^4$\nThe greatest common divisor is $2^2 = 4$.\n$$ c = 4 $$\n\n**Throughput Degradation Factor**:\nUsing the formula from Part 2, the degradation factor is $1/c$:\n$$ \\text{Degradation Factor} = \\frac{1}{c} = \\frac{1}{4} $$\n\n**Minimal Positive Padding $p$**:\nWe need to find the minimal positive integer $p$ such that the new stride $s' = s + p$ is coprime to $N_b$. This means $\\gcd(s', N_b) = 1$.\nSubstituting the given values:\n$$ s' = 12 + p $$\n$$ N_b = 16 $$\nWe require $\\gcd(12+p, 16) = 1$.\nFor $\\gcd(12+p, 16)$ to be $1$, the number $12+p$ must not share any prime factors with $16$. The only prime factor of $16=2^4$ is $2$. Therefore, $12+p$ must be an odd number.\nSince $12$ is an even number, for $12+p$ to be odd, $p$ must be an odd number.\nThe problem asks for the *minimal positive* padding $p$. The sequence of positive odd integers for $p$ starts with $1, 3, 5, \\dots$. The minimal positive value for $p$ is therefore $1$.\nLet's verify for $p=1$:\n$$ s' = 12 + 1 = 13 $$\n$$ \\gcd(13, 16) = 1 $$\nThis is correct, as $13$ is a prime number and not a factor of $16$. Thus, the minimal positive padding is $p=1$.\n\nThe final numerical results for part $3$ are: conflict factor $c=4$, throughput degradation factor $1/4$, and minimal padding $p=1$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 4  \\frac{1}{4}  1 \\end{pmatrix} } $$"
        }
    ]
}