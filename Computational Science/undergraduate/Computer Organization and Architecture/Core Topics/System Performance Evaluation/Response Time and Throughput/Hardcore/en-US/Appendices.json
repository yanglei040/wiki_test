{
    "hands_on_practices": [
        {
            "introduction": "A long chain of logic, like cascaded adders, creates a significant delay that forces a low clock frequency. By inserting registers to pipeline this path, we can dramatically increase the clock speed and thus the throughput. In this exercise , you will transform a combinational adder chain into a sequential pipeline and calculate its maximum throughput, learning to account for the real-world timing characteristics of registers and clock skew that determine the ultimate performance.",
            "id": "3628087",
            "problem": "A datapath in a synchronous accumulator unit sums four $64$-bit operands by cascading three $64$-bit adder blocks, implemented using Complementary Metal-Oxide-Semiconductor (CMOS) logic. In its original form, the design is purely combinational between an input register and an output register: the three adders are chained with two inter-adder interconnect segments, and there are no registers between the adders. You are tasked with refactoring this long combinational path into a sequential pipeline by inserting intermediate registers so that each adder resides in its own pipeline stage. The device uses Positive-Edge-Triggered D Flip-Flops (DFFs) with specified timing characteristics.\n\nAssume the following measured worst-case delays and register parameters under the target process and voltage:\n- Each $64$-bit adder has a worst-case propagation delay of $1.80 \\, \\text{ns}$.\n- Each inter-adder interconnect segment contributes an additional worst-case propagation delay of $0.20 \\, \\text{ns}$.\n- Each DFF has a clock-to-$Q$ delay of $0.08 \\, \\text{ns}$ and a setup time of $0.12 \\, \\text{ns}$.\n- The worst-case clock skew along the path from the launching register to the capturing register is $0.05 \\, \\text{ns}$.\n- Hold time constraints are satisfied by existing buffering and may be ignored.\n\nYou insert two intermediate registers between the three adder blocks so that the refactored pipeline has three stages, each stage containing one adder and the interconnect immediately following it. Assume that the interconnect delay per stage remains $0.20 \\, \\text{ns}$ after refactoring and that the register parameters and clock skew apply per stage.\n\nStarting from first principles, use the definitions of combinational versus sequential logic and the timing constraints of synchronous pipelines to determine the latency and throughput of the refactored pipeline. Report:\n- The pipeline latency in cycles (an integer number of cycles).\n- The maximum steady-state throughput in operations per second, expressed in scientific notation.\n\nRound the throughput to four significant figures. Express throughput in operations per second. No angle units are involved in this problem.",
            "solution": "The problem requires the calculation of the latency and throughput of a synchronous digital pipeline. The analysis begins with a precise definition of these two performance metrics and the fundamental timing constraints of synchronous circuits.\n\nA synchronous pipeline is a sequential circuit composed of multiple stages separated by registers (in this case, D Flip-Flops or DFFs). State transitions are synchronized by a global clock signal.\n\n**1. Pipeline Latency**\n\nLatency is the total time required for a single operation to propagate through the entire pipeline, from input to output. In a pipeline with $N$ stages, an operation requires one clock cycle to be processed by each stage and have its result latched by the subsequent register. Therefore, the operation's final result is available at the output of the last stage after $N$ complete clock cycles.\n\nThe problem states that the original combinational path of three adders is refactored into a sequential pipeline by inserting intermediate registers, such that \"each adder resides in its own pipeline stage\". This creates a three-stage pipeline.\n\nLet $N$ be the number of pipeline stages.\n$$N = 3$$\nThe latency, $L$, measured in clock cycles, is therefore equal to the number of stages.\n$$L = N = 3 \\, \\text{cycles}$$\n\n**2. Pipeline Throughput**\n\nThroughput is the rate at which the pipeline can complete operations in steady state. Once the pipeline is full (i.e., each stage is processing a different operation), a new result is produced at the output on every rising edge of the clock. The throughput, $\\Theta$, is therefore the reciprocal of the minimum possible clock period, $T_{\\text{min}}$.\n\n$$\\Theta = \\frac{1}{T_{\\text{min}}}$$\n\nThe minimum clock period is determined by the timing constraints of the slowest (longest delay) stage in the pipeline. For a synchronous path between a launching register and a capturing register, the clock period, $T_{\\text{clk}}$, must be long enough to allow for the data to propagate from the output of the launching register, through the combinational logic, and be stable at the input of the capturing register for at least the setup time before the next clock edge arrives. This relationship, known as the setup time constraint, is given by the inequality:\n\n$$T_{\\text{clk}} \\ge t_{\\text{clk-q}} + t_{\\text{comb}} + t_{\\text{setup}} + t_{\\text{skew}}$$\n\nwhere:\n- $t_{\\text{clk-q}}$ is the clock-to-Q delay of the launching register.\n- $t_{\\text{comb}}$ is the worst-case propagation delay of the combinational logic within a single pipeline stage.\n- $t_{\\text{setup}}$ is the setup time requirement of the capturing register.\n- $t_{\\text{skew}}$ is the worst-case clock skew between the launching and capturing registers.\n\nTo find the maximum throughput, we must calculate the minimum clock period, $T_{\\text{min}}$, which is the smallest value of $T_{\\text{clk}}$ that satisfies the inequality.\n\n$$T_{\\text{min}} = t_{\\text{clk-q}} + t_{\\text{comb}} + t_{\\text{setup}} + t_{\\text{skew}}$$\n\nFirst, we determine the combinational delay per stage, $t_{\\text{comb}}$. The problem states that the refactored pipeline has stages \"each containing one adder and the interconnect immediately following it\" and that the \"interconnect delay per stage remains $0.20 \\, \\text{ns}$\". This establishes a uniform combinational delay for each stage, which is the sum of the adder delay and the interconnect delay.\n\nThe given values are:\n- Adder propagation delay, $t_{\\text{adder}} = 1.80 \\, \\text{ns}$.\n- Interconnect propagation delay, $t_{\\text{interconnect}} = 0.20 \\, \\text{ns}$.\n\nSo, the combinational delay per stage is:\n$$t_{\\text{comb}} = t_{\\text{adder}} + t_{\\text{interconnect}} = 1.80 \\, \\text{ns} + 0.20 \\, \\text{ns} = 2.00 \\, \\text{ns}$$\n\nNext, we substitute all the given timing parameters into the equation for $T_{\\text{min}}$:\n- DFF clock-to-Q delay, $t_{\\text{clk-q}} = 0.08 \\, \\text{ns}$.\n- DFF setup time, $t_{\\text{setup}} = 0.12 \\, \\text{ns}$.\n- Worst-case clock skew, $t_{\\text{skew}} = 0.05 \\, \\text{ns}$.\n\n$$T_{\\text{min}} = 0.08 \\, \\text{ns} + 2.00 \\, \\text{ns} + 0.12 \\, \\text{ns} + 0.05 \\, \\text{ns}$$\n$$T_{\\text{min}} = 2.25 \\, \\text{ns}$$\n\nThis is the minimum clock period at which the pipeline can operate reliably. The maximum steady-state throughput is the reciprocal of this period.\n\n$$\\Theta = \\frac{1}{T_{\\text{min}}} = \\frac{1}{2.25 \\, \\text{ns}} = \\frac{1}{2.25 \\times 10^{-9} \\, \\text{s}}$$\n$$\\Theta = \\frac{4}{9} \\times 10^9 \\, \\frac{\\text{operations}}{\\text{s}} \\approx 0.44444... \\times 10^9 \\, \\frac{\\text{operations}}{\\text{s}}$$\nExpressing this in scientific notation and rounding to four significant figures as required:\n$$\\Theta \\approx 4.444 \\times 10^8 \\, \\frac{\\text{operations}}{\\text{s}}$$\n\nThe two required quantities are the latency in cycles and the maximum throughput in operations per second.\n- Latency: $3$ cycles.\n- Throughput: $4.444 \\times 10^8$ operations/s.",
            "answer": "$$\\boxed{\\begin{pmatrix} 3 & 4.444 \\times 10^{8} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Pipelining boosts throughput, but how deep should a pipeline be? A deeper pipeline can support a higher clock frequency, but it also increases the penalty for events like branch mispredictions and makes the response time for a single, isolated task longer. This practice  puts you in the role of a CPU architect facing a classic trade-off, requiring you to determine the conditions under which a deep, fast-clocked pipeline is better for batch workloads (throughput) but worse for single requests (response time).",
            "id": "3673527",
            "problem": "A microarchitectural team is comparing two in-order Central Processing Unit (CPU) designs for a latency-sensitive service that occasionally operates in batch mode. Design A uses a $20$-stage pipeline clocked at frequency $f_1 = 3.3 \\text{ GHz}$ with a branch misprediction penalty of $P_1 = 20$ cycles. Design B uses a $12$-stage pipeline clocked at frequency $f_2 = 3.0 \\text{ GHz}$ with a branch misprediction penalty of $P_2 = 10$ cycles. For both designs, assume a base of one cycle per instruction (no cache misses or other stalls), perfect forwarding, and that a branch misprediction incurs an additional stall of $P_i$ cycles for design $i \\in \\{1,2\\}$.\n\nConsider a workload in which a single request consists of $L = 100$ sequential instructions containing exactly one conditional branch that is executed once per request. The branch predictor mispredicts this branch with probability $p \\in [0,1]$, independently per request. A large batch consists of $T \\gg 1$ independent requests executed back-to-back; pipeline fill and drain across requests are negligible at batch scale.\n\nUse only the following foundational definitions:\n- Throughput is the number of completed requests per unit time, equal to CPU frequency divided by expected cycles per request in steady state for a large batch.\n- Single-request response time is the completion time for one isolated request, equal to expected cycles for that request divided by CPU frequency. For an isolated request on a $D_i$-stage pipeline, include the pipeline fill cost of $D_i - 1$ cycles.\n\nLet $D_1 = 20$ and $D_2 = 12$ denote the pipeline depths for designs A and B, respectively. Derive from first principles the conditions under which design A achieves strictly higher batch throughput than design B while also yielding a strictly larger single-request response time than design B, as a function of $p$. Then, determine the smallest misprediction probability $p^{\\star} \\in [0,1]$ for which both conditions hold simultaneously. Express your final answer for $p^{\\star}$ as an exact fraction in simplest terms. Do not include units with your final answer.",
            "solution": "The problem asks for the smallest branch misprediction probability $p^{\\star}$ such that Design A has strictly higher batch throughput than Design B, while also having a strictly larger single-request response time than Design B. We will derive expressions for these two performance metrics for each design, establish the two inequalities, and solve for the probability $p$.\n\nLet the subscript $i \\in \\{1, 2\\}$ refer to Design A and Design B, respectively. The given parameters are:\n- Pipeline depths: $D_1 = 20$, $D_2 = 12$\n- Clock frequencies: $f_1 = 3.3 \\text{ GHz}$, $f_2 = 3.0 \\text{ GHz}$\n- Branch misprediction penalties: $P_1 = 20$ cycles, $P_2 = 10$ cycles\n- Instructions per request: $L = 100$\n\nFirst, we determine the expected number of cycles to execute the instructions of a single request. A request consists of $L$ instructions, which would take $L$ cycles with a base CPI of $1$. There is one branch per request, which is mispredicted with probability $p$. A misprediction adds $P_i$ stall cycles. The expected number of cycles for instruction execution for design $i$ is therefore:\n$$ C_{exec, i} = L + p \\cdot P_i + (1 - p) \\cdot 0 = L + p P_i $$\n\nNext, we model the two required performance metrics based on the provided definitions.\n\n**1. Batch Throughput**\nThroughput ($Th_i$) is defined as the CPU frequency divided by the expected cycles per request in a large batch. The problem states that for a large batch, pipeline fill and drain effects are negligible. Thus, the cycles per request are simply the execution cycles, $C_{exec, i}$.\n$$ Th_i = \\frac{f_i}{C_{exec, i}} = \\frac{f_i}{L + p P_i} $$\nThe first condition is that Design A has strictly higher throughput than Design B:\n$$ Th_1 > Th_2 \\implies \\frac{f_1}{L + p P_1} > \\frac{f_2}{L + p P_2} $$\nSince $L$, $P_i$, and $f_i$ are all positive, and $p \\ge 0$, the denominators are always positive. We can cross-multiply without changing the inequality's direction:\n$$ f_1 (L + p P_2) > f_2 (L + p P_1) $$\n$$ f_1 L + p f_1 P_2 > f_2 L + p f_2 P_1 $$\n$$ p (f_1 P_2 - f_2 P_1) > L (f_2 - f_1) $$\nLet's substitute the given values: $f_1 = 3.3$, $f_2 = 3.0$, $P_1 = 20$, $P_2 = 10$, and $L = 100$.\nThe coefficient of $p$ is $f_1 P_2 - f_2 P_1 = (3.3)(10) - (3.0)(20) = 33 - 60 = -27$.\nThe right-hand side is $L(f_2 - f_1) = 100(3.0 - 3.3) = 100(-0.3) = -30$.\nThe inequality becomes:\n$$ -27p > -30 $$\nDividing by $-27$ and reversing the inequality sign gives:\n$$ p  \\frac{-30}{-27} \\implies p  \\frac{10}{9} $$\nSince the misprediction probability $p$ is defined to be in the interval $[0, 1]$, the condition $p  \\frac{10}{9}$ is always satisfied. Thus, Design A has a higher throughput than Design B for all valid values of $p$.\n\n**2. Single-Request Response Time**\nSingle-request response time ($RT_i$) is the completion time for one isolated request. This includes the pipeline fill cost of $D_i - 1$ cycles. The total expected cycles for a single request, $C_{single, i}$, are:\n$$ C_{single, i} = C_{exec, i} + (D_i - 1) = L + p P_i + D_i - 1 $$\nThe response time for design $i$ is then:\n$$ RT_i = \\frac{C_{single, i}}{f_i} = \\frac{L + p P_i + D_i - 1}{f_i} $$\nThe second condition is that Design A has a strictly larger response time than Design B:\n$$ RT_1 > RT_2 \\implies \\frac{L + p P_1 + D_1 - 1}{f_1} > \\frac{L + p P_2 + D_2 - 1}{f_2} $$\nSince frequencies are positive, we can cross-multiply:\n$$ f_2 (L + p P_1 + D_1 - 1) > f_1 (L + p P_2 + D_2 - 1) $$\nExpanding and grouping terms by $p$:\n$$ p f_2 P_1 - p f_1 P_2 > L(f_1 - f_2) + f_1(D_2 - 1) - f_2(D_1 - 1) $$\n$$ p (f_2 P_1 - f_1 P_2) > L(f_1 - f_2) + f_1(D_2 - 1) - f_2(D_1 - 1) $$\nLet's substitute the given values: $D_1 = 20, D_2 = 12$.\nThe coefficient of $p$ is $f_2 P_1 - f_1 P_2 = (3.0)(20) - (3.3)(10) = 60 - 33 = 27$.\nThe right-hand side (RHS) is:\n$$ \\text{RHS} = 100(3.3 - 3.0) + 3.3(12 - 1) - 3.0(20 - 1) $$\n$$ \\text{RHS} = 100(0.3) + 3.3(11) - 3.0(19) $$\n$$ \\text{RHS} = 30 + 36.3 - 57 $$\n$$ \\text{RHS} = 66.3 - 57 = 9.3 $$\nThe inequality becomes:\n$$ 27p > 9.3 $$\nSince the coefficient of $p$ is positive, the inequality direction is preserved upon division:\n$$ p > \\frac{9.3}{27} $$\nTo express this as an exact fraction, we write $9.3$ as $\\frac{93}{10}$:\n$$ p > \\frac{93/10}{27} = \\frac{93}{270} $$\nBoth the numerator and the denominator are divisible by $3$:\n$$ p > \\frac{31}{90} $$\nThe number $31$ is prime, so this fraction is in simplest terms.\n\n**Finding $p^{\\star}$**\nWe need to find the smallest probability $p^{\\star} \\in [0, 1]$ for which both conditions hold simultaneously.\nCondition 1: $p  \\frac{10}{9}$\nCondition 2: $p > \\frac{31}{90}$\nThe intersection of these conditions with the domain $p \\in [0, 1]$ defines the set of solutions for $p$:\n$$ p \\in \\left(\\frac{31}{90}, 1\\right] $$\nThe problem asks for the smallest probability $p^{\\star}$ for which both conditions hold. Since the inequalities are strict, there is no minimum value of $p$ in the solution set. The question is therefore interpreted as asking for the infimum (greatest lower bound) of this set. This boundary value is where the second condition transitions from false to true.\nThis value is the lower bound of the interval.\n$$ p^{\\star} = \\frac{31}{90} $$\nFor any value of $p$ strictly greater than $p^{\\star}$ (and less than or equal to $1$), both conditions will be met.",
            "answer": "$$\\boxed{\\frac{31}{90}}$$"
        },
        {
            "introduction": "Modern processor performance often hinges not on the core's clock speed, but on its ability to hide the long latency of memory accesses. This is achieved through parallelism, not just pipelining. This exercise  uses two contrasting microbenchmarks—one with serial dependencies and one with independent operations—to demonstrate the profound impact of Memory Level Parallelism ($MLP$). You will see how an out-of-order core can achieve high throughput despite high latency, and why program structure is key to unlocking this potential.",
            "id": "3673535",
            "problem": "A single-socket superscalar out-of-order core is used to study how data dependence affects response time and throughput of memory loads. The following machine traits are relevant and may be assumed constant during the experiment:\n- The core can track up to $M=16$ outstanding last-level-cache misses to dynamic random-access memory (DRAM) without stalling the front end.\n- The load-issue width is $W_{\\mathrm{ld}}=2$ loads per cycle.\n- Each DRAM access has a service time (latency) of approximately $D=200$ cycles from the point the miss is sent to the memory controller until the data returns to the core.\n- The clock frequency is $f=3\\,\\mathrm{GHz}$. The memory system bandwidth is sufficiently high that, at the predicted levels of concurrency below, it is not the bottleneck.\n- Each load transfers $8\\,\\mathrm{B}$.\n\nTwo microbenchmarks are designed to quantify response time and to demonstrate when and why throughput benefits from increasing Memory Level Parallelism (MLP), defined as the number of independent memory operations overlapped in time.\n\n- Dependent benchmark (pointer chasing): Construct a randomly permuted singly linked list over a data region much larger than the last-level cache. The loop repeatedly loads the next pointer from the current node and advances the pointer for $N$ steps, with $N$ large. Because each address is obtained only after the previous load completes, there is a single dependence chain.\n- Independent benchmark (many independent loads): Prepare $K$ base addresses into $K$ separate arrays whose elements are spaced so that accesses miss in the last-level cache and map to distinct banks. In each loop iteration, issue $K$ loads to elements that are independent of one another and defer using their values until after all $K$ loads have been issued. Choose $K$ so as to attempt to maximize MLP without exceeding hardware limits.\n\nUse the fundamental definitions that response time is the elapsed time per operation and throughput is the number of operations completed per unit time. Assume all accesses in both benchmarks miss the private caches and are served by DRAM with latency $D$ as above, and that reorder buffering is not otherwise limiting.\n\nWhich of the following statements about the expected measurements and their interpretation are correct?\n\nA. In the dependent benchmark, the average cycles per load tends to approximately $D$ for large $N$, aside from a small loop overhead.\n\nB. In the independent benchmark with $K \\ge M$, the steady-state load throughput tends to approximately $M/D$ loads per cycle, so the average cycles per load measured as total cycles divided by total loads tends to approximately $D/M$.\n\nC. Increasing $K$ beyond $M$ further reduces the measured cycles per load proportionally because more independent loads increase MLP without bound.\n\nD. If the dependent benchmark is modified to insert software prefetch instructions that attempt to prefetch $P=8$ nodes ahead, then with perfect prefetch the measured cycles per load should approach $D/M$, even though the pointer chain is conceptually dependent.\n\nE. If $K=8M$ while $W_{\\mathrm{ld}}=2$, the independent benchmark’s throughput is limited by the load-issue width and equals $W_{\\mathrm{ld}}$ loads per cycle, so the cycles per load is $1/W_{\\mathrm{ld}}=0.5$ cycles.\n\nF. In the independent benchmark under saturated MLP, the end-to-end response time of an individual load request (from issue to completion) remains approximately $D$ cycles, even though the measured cycles per load based on throughput is approximately $D/M$.\n\nG. To observe near-maximum MLP in the independent benchmark, the loop should be structured so that address generation and memory operations are independent and the consumption of each load’s value is deferred until after at least $M$ loads have been issued; otherwise, early dependencies will reduce the number of requests that can overlap.\n\nSelect all that apply.",
            "solution": "The core of this problem lies in understanding the relationship between latency, throughput, and concurrency (MLP). The key formula, derived from Little's Law, connects these quantities:\n$$\n\\text{Throughput} = \\frac{\\text{MLP}}{D}\n$$\nwhere throughput is in loads per cycle, MLP is the number of concurrent outstanding loads, and $D$ is the load latency in cycles. The average cycles per load (CPL) is the reciprocal of throughput:\n$$\n\\text{CPL} = \\frac{1}{\\text{Throughput}} = \\frac{D}{\\text{MLP}}\n$$\nThe hardware has two key limits:\n1.  The number of outstanding misses it can track: $\\text{MLP} \\le M = 16$.\n2.  The number of loads it can issue per cycle: $\\text{Throughput}_{\\text{issue}} \\le W_{\\mathrm{ld}} = 2$ loads/cycle.\n\nThe overall system throughput will be the minimum of the rate at which loads can be issued and the rate at which they can be completed by the memory system.\n\n**A.** In the dependent pointer-chasing benchmark, the address of load $i+1$ is obtained from the data returned by load $i$. This is a true data dependency (`p = p->next`). An out-of-order execution engine cannot break this dependency. Consequently, the loads must be processed serially. The processor issues a load, waits approximately $D=200$ cycles for the data to return, computes the next address (a small overhead), and then issues the next load. Only one memory access can be \"in-flight\" at any time for this dependency chain. Thus, the effective MLP is $1$. The time elapsed per load is dominated by the memory latency, $D$. The cycles per load (CPL) is:\n$$\n\\text{CPL} = \\frac{D}{\\text{MLP}} = \\frac{D}{1} = D = 200 \\text{ cycles}\n$$\nThe statement is consistent with this analysis.\n\n**B.** In the independent benchmark, the core has a supply of $K$ independent loads. The hardware can track a maximum of $M=16$ outstanding misses. If the number of available independent loads $K$ is greater than or equal to $M$, the core can saturate its memory-tracking resources. The sustained MLP will be limited by the hardware, so $\\text{MLP} = M = 16$. The throughput of the memory system is then:\n$$\n\\text{Throughput} = \\frac{\\text{MLP}}{D} = \\frac{M}{D} = \\frac{16}{200} = 0.08 \\text{ loads/cycle}\n$$\nThe core's issue width is $W_{\\mathrm{ld}}=2$ loads/cycle. Since $0.08  2$, the bottleneck is the memory system's ability to service concurrent requests, not the core's issue rate. The average cycles per load is the reciprocal of the throughput:\n$$\n\\text{CPL} = \\frac{1}{\\text{Throughput}} = \\frac{D}{M} = \\frac{200}{16} = 12.5 \\text{ cycles/load}\n$$\nThe statement's claims match this derivation.\n\n**C.** The problem explicitly states that the core can track up to $M=16$ outstanding misses. This is a hard hardware limit. Once the number of in-flight misses reaches $M$, the load issue mechanism must stall until one of the existing misses completes, freeing up a tracking slot. Therefore, the MLP cannot exceed $M$, regardless of how large $K$ (the number of available independent loads) becomes. MLP is capped: $\\text{MLP}_{\\text{max}} = M$. Since $\\text{CPL} = D/\\text{MLP}$, the minimum CPL is $D/M$. Increasing $K$ beyond $M$ provides no further benefit to throughput or CPL. The claim that MLP increases \"without bound\" is false.\n\n**D.** This statement is flawed on multiple levels. In a linked list where each node's address is stored in the previous node, it is impossible for software to know the address of node $i+P$ without first accessing nodes $i, i+1, \\dots, i+P-1$. A software prefetch instruction requires an address, which is the very information that is serially dependent. Therefore, software prefetching cannot create parallelism in this scenario. Even if one were to assume the existence of a special hardware pointer-chasing prefetcher that could \"look ahead\" by $P=8$ nodes, it would create an MLP of at most $P=8$. The resulting CPL would be $D/P = D/8$. The statement claims the CPL should approach $D/M = D/16$. This is numerically inconsistent with the given prefetch distance $P=8$.\n\n**E.** With $K=8$ available independent loads and a hardware capacity of $M=16$, the maximum achievable MLP is limited by the software, i.e., $\\text{MLP} = K = 8$. The memory system can sustain a completion rate (throughput) of:\n$$\n\\text{Throughput}_{\\text{memory}} = \\frac{\\text{MLP}}{D} = \\frac{K}{D} = \\frac{8}{200} = 0.04 \\text{ loads/cycle}\n$$\nThe overall throughput is the minimum of the issue rate and the memory completion rate:\n$$\n\\text{Throughput} = \\min(W_{\\mathrm{ld}}, \\frac{K}{D}) = \\min(2, 0.04) = 0.04 \\text{ loads/cycle}\n$$\nThe bottleneck is clearly the memory system, not the load-issue width. The statement that throughput is limited by and equal to $W_{\\mathrm{ld}}$ is false. Consequently, the claim that $\\text{CPL} = 1/W_{\\mathrm{ld}} = 0.5$ is also false. The actual CPL would be $D/K = 200/8 = 25$ cycles.\n\n**F.** This statement correctly distinguishes between two key performance metrics. The response time (latency) for a single memory load is the time from its dispatch to the memory controller until its data returns to the core, which the problem defines as $D=200$ cycles. This value does not change just because other loads are being processed in parallel. Cycles Per Load (CPL), however, is an amortized measure of throughput, calculated as (Total Cycles) / (Total Loads). As derived in B, under saturated MLP, the system completes $M$ loads every $D$ cycles on average in steady state. So, the CPL is $D/M = 12.5$ cycles. The ability to have a low amortized CPL while individual operations have high latency is the entire point of parallelism. The statement accurately describes this fundamental concept.\n\n**G.** This statement describes the correct methodology for writing a microbenchmark to measure maximum MLP. To allow the out-of-order engine to find and issue many loads in parallel, the instruction stream must contain a large window of independent operations. If the result of a load is consumed immediately by a subsequent instruction (e.g., `load r1, [A]; add r2, r1, #1`), it creates a dependency. If this dependency is on a long-latency load, it will cause the dependent instruction and all subsequent instructions to wait in the reorder buffer (ROB). This can fill the ROB and prevent the frontend from fetching and dispatching new instructions, including other independent loads that are further down the instruction stream. This \"ROB clog\" effectively stifles the core's ability to find and exploit parallelism. By issuing a large number of loads ($M$ or more) first and only then consuming their results, the benchmark ensures that the dependency chain does not limit the number of loads that can be in flight. This allows the hardware to reach its natural MLP limit, which is $M$.",
            "answer": "$$\\boxed{ABFG}$$"
        }
    ]
}