{
    "hands_on_practices": [
        {
            "introduction": "The most fundamental method for increasing a processor's throughput is pipelining, which involves breaking a long computational task into a series of smaller, sequential stages. This practice provides a concrete example of this transformation, starting with a purely combinational circuit—a chain of adders—that represents a bottleneck for the system's clock speed. By inserting registers to create a pipeline, you will apply the core timing principles of synchronous digital circuits to calculate the new, higher throughput and understand the associated cost in latency. ",
            "id": "3628087",
            "problem": "A datapath in a synchronous accumulator unit sums four $64$-bit operands by cascading three $64$-bit adder blocks, implemented using Complementary Metal-Oxide-Semiconductor (CMOS) logic. In its original form, the design is purely combinational between an input register and an output register: the three adders are chained with two inter-adder interconnect segments, and there are no registers between the adders. You are tasked with refactoring this long combinational path into a sequential pipeline by inserting intermediate registers so that each adder resides in its own pipeline stage. The device uses Positive-Edge-Triggered D Flip-Flops (DFFs) with specified timing characteristics.\n\nAssume the following measured worst-case delays and register parameters under the target process and voltage:\n- Each $64$-bit adder has a worst-case propagation delay of $1.80 \\, \\text{ns}$.\n- Each inter-adder interconnect segment contributes an additional worst-case propagation delay of $0.20 \\, \\text{ns}$.\n- Each DFF has a clock-to-$Q$ delay of $0.08 \\, \\text{ns}$ and a setup time of $0.12 \\, \\text{ns}$.\n- The worst-case clock skew along the path from the launching register to the capturing register is $0.05 \\, \\text{ns}$.\n- Hold time constraints are satisfied by existing buffering and may be ignored.\n\nYou insert two intermediate registers between the three adder blocks so that the refactored pipeline has three stages, each stage containing one adder and the interconnect immediately following it. Assume that the interconnect delay per stage remains $0.20 \\, \\text{ns}$ after refactoring and that the register parameters and clock skew apply per stage.\n\nStarting from first principles, use the definitions of combinational versus sequential logic and the timing constraints of synchronous pipelines to determine the latency and throughput of the refactored pipeline. Report:\n- The pipeline latency in cycles (an integer number of cycles).\n- The maximum steady-state throughput in operations per second, expressed in scientific notation.\n\nRound the throughput to four significant figures. Express throughput in operations per second. No angle units are involved in this problem.",
            "solution": "The problem requires the calculation of the latency and throughput of a synchronous digital pipeline. The analysis begins with a precise definition of these two performance metrics and the fundamental timing constraints of synchronous circuits.\n\nA synchronous pipeline is a sequential circuit composed of multiple stages separated by registers (in this case, D Flip-Flops or DFFs). State transitions are synchronized by a global clock signal.\n\n**1. Pipeline Latency**\n\nLatency is the total time required for a single operation to propagate through the entire pipeline, from input to output. In a pipeline with $N$ stages, an operation requires one clock cycle to be processed by each stage and have its result latched by the subsequent register. Therefore, the operation's final result is available at the output of the last stage after $N$ complete clock cycles.\n\nThe problem states that the original combinational path of three adders is refactored into a sequential pipeline by inserting intermediate registers, such that \"each adder resides in its own pipeline stage\". This creates a three-stage pipeline.\n\nLet $N$ be the number of pipeline stages.\n$$N = 3$$\nThe latency, $L$, measured in clock cycles, is therefore equal to the number of stages.\n$$L = N = 3 \\, \\text{cycles}$$\n\n**2. Pipeline Throughput**\n\nThroughput is the rate at which the pipeline can complete operations in steady state. Once the pipeline is full (i.e., each stage is processing a different operation), a new result is produced at the output on every rising edge of the clock. The throughput, $\\Theta$, is therefore the reciprocal of the minimum possible clock period, $T_{\\text{min}}$.\n\n$$\\Theta = \\frac{1}{T_{\\text{min}}}$$\n\nThe minimum clock period is determined by the timing constraints of the slowest (longest delay) stage in the pipeline. For a synchronous path between a launching register and a capturing register, the clock period, $T_{\\text{clk}}$, must be long enough to allow for the data to propagate from the output of the launching register, through the combinational logic, and be stable at the input of the capturing register for at least the setup time before the next clock edge arrives. This relationship, known as the setup time constraint, is given by the inequality:\n\n$$T_{\\text{clk}} \\ge t_{\\text{clk-q}} + t_{\\text{comb}} + t_{\\text{setup}} + t_{\\text{skew}}$$\n\nwhere:\n- $t_{\\text{clk-q}}$ is the clock-to-Q delay of the launching register.\n- $t_{\\text{comb}}$ is the worst-case propagation delay of the combinational logic within a single pipeline stage.\n- $t_{\\text{setup}}$ is the setup time requirement of the capturing register.\n- $t_{\\text{skew}}$ is the worst-case clock skew between the launching and capturing registers.\n\nTo find the maximum throughput, we must calculate the minimum clock period, $T_{\\text{min}}$, which is the smallest value of $T_{\\text{clk}}$ that satisfies the inequality.\n\n$$T_{\\text{min}} = t_{\\text{clk-q}} + t_{\\text{comb}} + t_{\\text{setup}} + t_{\\text{skew}}$$\n\nFirst, we determine the combinational delay per stage, $t_{\\text{comb}}$. The problem states that the refactored pipeline has stages \"each containing one adder and the interconnect immediately following it\" and that the \"interconnect delay per stage remains $0.20 \\, \\text{ns}$\". This establishes a uniform combinational delay for each stage, which is the sum of the adder delay and the interconnect delay.\n\nThe given values are:\n- Adder propagation delay, $t_{\\text{adder}} = 1.80 \\, \\text{ns}$.\n- Interconnect propagation delay, $t_{\\text{interconnect}} = 0.20 \\, \\text{ns}$.\n\nSo, the combinational delay per stage is:\n$$t_{\\text{comb}} = t_{\\text{adder}} + t_{\\text{interconnect}} = 1.80 \\, \\text{ns} + 0.20 \\, \\text{ns} = 2.00 \\, \\text{ns}$$\n\nNext, we substitute all the given timing parameters into the equation for $T_{\\text{min}}$:\n- DFF clock-to-Q delay, $t_{\\text{clk-q}} = 0.08 \\, \\text{ns}$.\n- DFF setup time, $t_{\\text{setup}} = 0.12 \\, \\text{ns}$.\n- Worst-case clock skew, $t_{\\text{skew}} = 0.05 \\, \\text{ns}$.\n\n$$T_{\\text{min}} = 0.08 \\, \\text{ns} + 2.00 \\, \\text{ns} + 0.12 \\, \\text{ns} + 0.05 \\, \\text{ns}$$\n$$T_{\\text{min}} = 2.25 \\, \\text{ns}$$\n\nThis is the minimum clock period at which the pipeline can operate reliably. The maximum steady-state throughput is the reciprocal of this period.\n\n$$\\Theta = \\frac{1}{T_{\\text{min}}} = \\frac{1}{2.25 \\, \\text{ns}} = \\frac{1}{2.25 \\times 10^{-9} \\, \\text{s}}$$\n$$\\Theta = \\frac{4}{9} \\times 10^9 \\, \\frac{\\text{operations}}{\\text{s}} \\approx 0.44444... \\times 10^9 \\, \\frac{\\text{operations}}{\\text{s}}$$\nExpressing this in scientific notation and rounding to four significant figures as required:\n$$\\Theta \\approx 4.444 \\times 10^8 \\, \\frac{\\text{operations}}{\\text{s}}$$\n\nThe two required quantities are the latency in cycles and the maximum throughput in operations per second.\n- Latency: $3$ cycles.\n- Throughput: $4.444 \\times 10^8$ operations/s.",
            "answer": "$$\\boxed{\\begin{pmatrix} 3 & 4.444 \\times 10^{8} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Architectural design is a game of trade-offs, and nowhere is this clearer than in choosing a pipeline's depth. A deeper pipeline may allow for a higher clock frequency, boosting peak throughput, but it also increases the latency for a single operation and magnifies the penalty from events like branch mispredictions. This exercise places you in the role of a microarchitect, tasked with comparing two CPU designs with different pipeline depths. You will model both batch throughput and single-request response time to determine quantitatively when one design's higher clock speed overcomes its longer pipeline penalties. ",
            "id": "3673527",
            "problem": "A microarchitectural team is comparing two in-order Central Processing Unit (CPU) designs for a latency-sensitive service that occasionally operates in batch mode. Design A uses a $20$-stage pipeline clocked at frequency $f_1 = 3.3 \\text{ GHz}$ with a branch misprediction penalty of $P_1 = 20$ cycles. Design B uses a $12$-stage pipeline clocked at frequency $f_2 = 3.0 \\text{ GHz}$ with a branch misprediction penalty of $P_2 = 10$ cycles. For both designs, assume a base of one cycle per instruction (no cache misses or other stalls), perfect forwarding, and that a branch misprediction incurs an additional stall of $P_i$ cycles for design $i \\in \\{1,2\\}$.\n\nConsider a workload in which a single request consists of $L = 100$ sequential instructions containing exactly one conditional branch that is executed once per request. The branch predictor mispredicts this branch with probability $p \\in [0,1]$, independently per request. A large batch consists of $T \\gg 1$ independent requests executed back-to-back; pipeline fill and drain across requests are negligible at batch scale.\n\nUse only the following foundational definitions:\n- Throughput is the number of completed requests per unit time, equal to CPU frequency divided by expected cycles per request in steady state for a large batch.\n- Single-request response time is the completion time for one isolated request, equal to expected cycles for that request divided by CPU frequency. For an isolated request on a $D_i$-stage pipeline, include the pipeline fill cost of $D_i - 1$ cycles.\n\nLet $D_1 = 20$ and $D_2 = 12$ denote the pipeline depths for designs A and B, respectively. Derive from first principles the conditions under which design A achieves strictly higher batch throughput than design B while also yielding a strictly larger single-request response time than design B, as a function of $p$. Then, determine the smallest misprediction probability $p^{\\star} \\in [0,1]$ for which both conditions hold simultaneously. Express your final answer for $p^{\\star}$ as an exact fraction in simplest terms. Do not include units with your final answer.",
            "solution": "The problem asks for the smallest branch misprediction probability $p^{\\star}$ such that Design A has strictly higher batch throughput than Design B, while also having a strictly larger single-request response time than Design B. We will derive expressions for these two performance metrics for each design, establish the two inequalities, and solve for the probability $p$.\n\nLet the subscript $i \\in \\{1, 2\\}$ refer to Design A and Design B, respectively. The given parameters are:\n- Pipeline depths: $D_1 = 20$, $D_2 = 12$\n- Clock frequencies: $f_1 = 3.3 \\text{ GHz}$, $f_2 = 3.0 \\text{ GHz}$\n- Branch misprediction penalties: $P_1 = 20$ cycles, $P_2 = 10$ cycles\n- Instructions per request: $L = 100$\n\nFirst, we determine the expected number of cycles to execute the instructions of a single request. A request consists of $L$ instructions, which would take $L$ cycles with a base CPI of $1$. There is one branch per request, which is mispredicted with probability $p$. A misprediction adds $P_i$ stall cycles. The expected number of cycles for instruction execution for design $i$ is therefore:\n$$ C_{exec, i} = L + p \\cdot P_i + (1 - p) \\cdot 0 = L + p P_i $$\n\nNext, we model the two required performance metrics based on the provided definitions.\n\n**1. Batch Throughput**\nThroughput ($Th_i$) is defined as the CPU frequency divided by the expected cycles per request in a large batch. The problem states that for a large batch, pipeline fill and drain effects are negligible. Thus, the cycles per request are simply the execution cycles, $C_{exec, i}$.\n$$ Th_i = \\frac{f_i}{C_{exec, i}} = \\frac{f_i}{L + p P_i} $$\nThe first condition is that Design A has strictly higher throughput than Design B:\n$$ Th_1 > Th_2 \\implies \\frac{f_1}{L + p P_1} > \\frac{f_2}{L + p P_2} $$\nSince $L$, $P_i$, and $f_i$ are all positive, and $p \\ge 0$, the denominators are always positive. We can cross-multiply without changing the inequality's direction:\n$$ f_1 (L + p P_2) > f_2 (L + p P_1) $$\n$$ f_1 L + p f_1 P_2 > f_2 L + p f_2 P_1 $$\n$$ p (f_1 P_2 - f_2 P_1) > L (f_2 - f_1) $$\nLet's substitute the given values: $f_1 = 3.3$, $f_2 = 3.0$, $P_1 = 20$, $P_2 = 10$, and $L = 100$.\nThe coefficient of $p$ is $f_1 P_2 - f_2 P_1 = (3.3)(10) - (3.0)(20) = 33 - 60 = -27$.\nThe right-hand side is $L(f_2 - f_1) = 100(3.0 - 3.3) = 100(-0.3) = -30$.\nThe inequality becomes:\n$$ -27p > -30 $$\nDividing by $-27$ and reversing the inequality sign gives:\n$$ p < \\frac{-30}{-27} \\implies p < \\frac{10}{9} $$\nSince the misprediction probability $p$ is defined to be in the interval $[0, 1]$, the condition $p < \\frac{10}{9}$ is always satisfied. Thus, Design A has a higher throughput than Design B for all valid values of $p$.\n\n**2. Single-Request Response Time**\nSingle-request response time ($RT_i$) is the completion time for one isolated request. This includes the pipeline fill cost of $D_i - 1$ cycles. The total expected cycles for a single request, $C_{single, i}$, are:\n$$ C_{single, i} = C_{exec, i} + (D_i - 1) = L + p P_i + D_i - 1 $$\nThe response time for design $i$ is then:\n$$ RT_i = \\frac{C_{single, i}}{f_i} = \\frac{L + p P_i + D_i - 1}{f_i} $$\nThe second condition is that Design A has a strictly larger response time than Design B:\n$$ RT_1 > RT_2 \\implies \\frac{L + p P_1 + D_1 - 1}{f_1} > \\frac{L + p P_2 + D_2 - 1}{f_2} $$\nSince frequencies are positive, we can cross-multiply:\n$$ f_2 (L + p P_1 + D_1 - 1) > f_1 (L + p P_2 + D_2 - 1) $$\nExpanding and grouping terms by $p$:\n$$ p f_2 P_1 - p f_1 P_2 > L(f_1 - f_2) + f_1(D_2 - 1) - f_2(D_1 - 1) $$\n$$ p (f_2 P_1 - f_1 P_2) > L(f_1 - f_2) + f_1(D_2 - 1) - f_2(D_1 - 1) $$\nLet's substitute the given values: $D_1 = 20, D_2 = 12$.\nThe coefficient of $p$ is $f_2 P_1 - f_1 P_2 = (3.0)(20) - (3.3)(10) = 60 - 33 = 27$.\nThe right-hand side (RHS) is:\n$$ \\text{RHS} = 100(3.3 - 3.0) + 3.3(12 - 1) - 3.0(20 - 1) $$\n$$ \\text{RHS} = 100(0.3) + 3.3(11) - 3.0(19) $$\n$$ \\text{RHS} = 30 + 36.3 - 57 $$\n$$ \\text{RHS} = 66.3 - 57 = 9.3 $$\nThe inequality becomes:\n$$ 27p > 9.3 $$\nSince the coefficient of $p$ is positive, the inequality direction is preserved upon division:\n$$ p > \\frac{9.3}{27} $$\nTo express this as an exact fraction, we write $9.3$ as $\\frac{93}{10}$:\n$$ p > \\frac{93/10}{27} = \\frac{93}{270} $$\nBoth the numerator and the denominator are divisible by $3$:\n$$ p > \\frac{31}{90} $$\nThe number $31$ is prime, so this fraction is in simplest terms.\n\n**Finding $p^{\\star}$**\nWe need to find the smallest probability $p^{\\star} \\in [0, 1]$ for which both conditions hold simultaneously.\nCondition 1: $p < \\frac{10}{9}$\nCondition 2: $p > \\frac{31}{90}$\nThe intersection of these conditions with the domain $p \\in [0, 1]$ defines the set of solutions for $p$:\n$$ p \\in \\left(\\frac{31}{90}, 1\\right] $$\nThe problem asks for the smallest probability $p^{\\star}$ for which both conditions hold. This value is the lower bound of the interval.\n$$ p^{\\star} = \\frac{31}{90} $$\nFor any value of $p$ strictly greater than $p^{\\star}$ (and less than or equal to $1$), both conditions will be met.",
            "answer": "$$\\boxed{\\frac{31}{90}}$$"
        },
        {
            "introduction": "Improving the performance of a modern processor requires identifying and alleviating the primary bottlenecks. This practice introduces the CPI (Cycles Per Instruction) stack, a powerful model for attributing performance loss to distinct components like memory stalls or instruction fetch issues. By analyzing a processor's performance profile, you will apply a concept analogous to Amdahl's Law to predict the overall system speedup from a targeted optimization—in this case, a new prefetcher that reduces memory-related stalls. This exercise demonstrates how architects quantify the impact of improvements and justify their design decisions. ",
            "id": "3673593",
            "problem": "A performance analyst is evaluating a superscalar out-of-order processor with clock frequency $f = 3.2 \\text{ GHz}$. The analyst’s goal is to separate memory stall cycles from front-end stalls using a Cycle Per Instruction (CPI) stack decomposition and then quantify how reducing one component changes both throughput and single-thread response time.\n\nTo make the decomposition valid, the analyst uses hardware performance counters that attribute non-overlapping stall cycles to disjoint buckets. The experiment is designed as follows: the analyst conducts paired runs in which microarchitectural perturbations selectively neutralize one bottleneck at a time while keeping the instruction mix constant. Specifically, one run constrains the instruction supply path to expose front-end stalls (for example, by disabling the branch target buffer and instruction predecode optimizations) while keeping data set sizes small enough to avoid data cache misses; a complementary run constructs a pointer-chasing kernel to expose back-end memory stalls (for example, by using large working sets to exceed last-level cache capacity) while maintaining a simple instruction stream to avoid front-end pressure. From these controlled runs, and from counters on the target workload that attribute stall cycles to “Front-End Bound” and “Back-End Memory Bound,” the analyst estimates per-instruction contributions that sum to the total CPI. The measured per-instruction CPI components on the target workload are:\n- Base execution (useful work and contention-free retire): $B = 0.6$,\n- Front-end stalls (instruction supply and branch-related): $F = 0.5$,\n- Memory stalls (data cache and main memory related): $M = 1.1$.\n\nAssume these components are mutually exclusive and additive so that the baseline total CPI is $C_{\\text{old}} = B + F + M$. Consider a chip with $N = 8$ identical cores, each running an independent instance of the same application, with no shared resource interference and ample memory bandwidth. The instructions per cycle (IPC) satisfies $\\text{IPC} = \\frac{1}{\\text{CPI}}$, single-thread response time for executing $I$ instructions satisfies $T = \\frac{I \\cdot \\text{CPI}}{f}$, and aggregate chip throughput in instructions per second satisfies $\\Theta = N \\cdot \\text{IPC} \\cdot f$ under these assumptions.\n\nSuppose a new prefetcher reduces the memory-stall component by $40\\%$ without changing $B$ or $F$ and without changing the dynamic instruction count. Let $C_{\\text{new}}$ be the new CPI after applying the prefetcher. Define the common speedup factor $S$ that simultaneously applies to aggregate throughput and to single-thread response time under these assumptions as $S = \\frac{\\Theta_{\\text{new}}}{\\Theta_{\\text{old}}} = \\frac{T_{\\text{old}}}{T_{\\text{new}}}$.\n\nCompute $S$. Express your final result as a unitless scalar rounded to four significant figures.",
            "solution": "The objective is to compute the speedup factor, $S$, resulting from a microarchitectural improvement that reduces memory-related stalls. The speedup is defined as the ratio of old performance to new performance, which for response time is $\\frac{T_{\\text{old}}}{T_{\\text{new}}}$ and for throughput is $\\frac{\\Theta_{\\text{new}}}{\\Theta_{\\text{old}}}$.\n\nFirst, we compute the baseline total CPI, denoted as $C_{\\text{old}}$, by summing its given components: the base execution component $B$, the front-end stall component $F$, and the memory stall component $M$.\nThe given values are $B = 0.6$, $F = 0.5$, and $M = 1.1$.\nThe total baseline CPI is therefore:\n$$C_{\\text{old}} = B + F + M = 0.6 + 0.5 + 1.1 = 2.2$$\n\nNext, we determine the new CPI, $C_{\\text{new}}$, after the proposed improvement. The problem states that a new prefetcher reduces the memory-stall component, $M$, by $40\\%$. This means the new memory-stall component, $M_{\\text{new}}$, is $100\\% - 40\\% = 60\\%$ of the original component.\n$$M_{\\text{new}} = M \\cdot (1 - 0.40) = 1.1 \\cdot 0.60 = 0.66$$\n\nThe problem specifies that the other components, $B$ and $F$, as well as the dynamic instruction count $I$, remain unchanged. The new total CPI, $C_{\\text{new}}$, is the sum of the unchanged components and the new memory component:\n$$C_{\\text{new}} = B + F + M_{\\text{new}} = 0.6 + 0.5 + 0.66 = 1.76$$\n\nNow, we express the speedup factor $S$ in terms of CPI. The problem defines $S$ using both single-thread response time, $T$, and aggregate chip throughput, $\\Theta$. Let us verify their equivalence.\n\nFor single-thread response time, $T = \\frac{I \\cdot \\text{CPI}}{f}$, where $I$ is the instruction count and $f$ is the clock frequency. The speedup is the ratio of old response time to new response time:\n$$S = \\frac{T_{\\text{old}}}{T_{\\text{new}}} = \\frac{\\frac{I \\cdot C_{\\text{old}}}{f}}{\\frac{I \\cdot C_{\\text{new}}}{f}} = \\frac{C_{\\text{old}}}{C_{\\text{new}}}$$\nThis is because $I$ and $f$ are constant for both scenarios.\n\nFor aggregate throughput, $\\Theta = N \\cdot \\text{IPC} \\cdot f = N \\cdot \\frac{1}{\\text{CPI}} \\cdot f$, where $N$ is the number of cores. The speedup is the ratio of new throughput to old throughput:\n$$S = \\frac{\\Theta_{\\text{new}}}{\\Theta_{\\text{old}}} = \\frac{N \\cdot \\frac{1}{C_{\\text{new}}} \\cdot f}{N \\cdot \\frac{1}{C_{\\text{old}}} \\cdot f} = \\frac{\\frac{1}{C_{\\text{new}}}}{\\frac{1}{C_{\\text{old}}}} = \\frac{C_{\\text{old}}}{C_{\\text{new}}}$$\nBoth definitions yield the same expression for speedup, $S = \\frac{C_{\\text{old}}}{C_{\\text{new}}}$. This confirms the internal consistency of the problem's formulation. Notably, the speedup calculation is independent of the number of cores $N$, the clock frequency $f$, and the instruction count $I$.\n\nFinally, we substitute the calculated CPI values into the expression for $S$:\n$$S = \\frac{C_{\\text{old}}}{C_{\\text{new}}} = \\frac{2.2}{1.76}$$\nTo simplify the fraction, we can write:\n$$S = \\frac{2.20}{1.76} = \\frac{220}{176}$$\nDividing the numerator and denominator by common factors:\n$$S = \\frac{110}{88} = \\frac{55}{44} = \\frac{5}{4} = 1.25$$\n\nThe problem requires the final answer to be rounded to four significant figures. The exact value $1.25$ can be written as $1.250$ to satisfy this requirement.",
            "answer": "$$\\boxed{1.250}$$"
        }
    ]
}