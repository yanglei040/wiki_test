{
    "hands_on_practices": [
        {
            "introduction": "The choice of instruction length is a fundamental design decision with a direct impact on code density. This exercise explores how variable-length encodings, much like Morse code, can compress programs by assigning shorter codes to more frequent instructions. By applying the principles of Huffman coding, you will construct an optimal prefix-free code from a given frequency distribution and quantify the resulting improvement in code size, providing a tangible measure of the benefits of this approach .",
            "id": "3650084",
            "problem": "Consider an Instruction Set Architecture (ISA) with $N=12$ opcodes executed on a Central Processing Unit (CPU). In the spirit of the variable-length Morse code, assume the ISA permits variable-length binary encodings provided they are prefix-free (no codeword is a prefix of another). You are given an opcode frequency table measured over a window of $64$ dynamic instructions. The relative frequencies per $64$ instructions are: $O_{1}:16$, $O_{2}:12$, $O_{3}:8$, $O_{4}:6$, $O_{5}:6$, $O_{6}:4$, $O_{7}:4$, $O_{8}:2$, $O_{9}:2$, $O_{10}:1$, $O_{11}:1$, $O_{12}:2$. \n\nStarting from first principles of uniquely decodable binary encodings and prefix-free codes, construct an optimal binary prefix code (Huffman-like) for these opcodes. From this construction, determine the average number of bits per opcode, denoted $L_{\\text{var}}$, for the variable-length encoding, and compare it against the fixed-length baseline. The fixed-length baseline uses the minimum number of bits $w$ required to assign unique fixed-width opcodes to all $N$ distinct operations.\n\nDefine the code density improvement as the fractional reduction $\\delta$ in average bits per opcode achieved by the variable-length encoding relative to the fixed-length baseline. Compute $\\delta$ and report its value as a decimal fraction. Round your final answer to four significant figures.",
            "solution": "The user wants to analyze the efficiency of variable-length instruction encoding compared to a fixed-length scheme for a given set of opcodes and their execution frequencies. This involves constructing an optimal prefix-free code, calculating the average number of bits per opcode for both schemes, and determining the fractional improvement.\n\n### Step 1: Fixed-Length Encoding Baseline\n\nFor a fixed-length encoding scheme, each of the $N=12$ distinct opcodes must be assigned a unique binary codeword of the same length, $w$. The minimum value of $w$ must satisfy the condition $2^w \\ge N$.\n\nWe have $N=12$.\nFor $w=3$, $2^3 = 8 < 12$, which is insufficient.\nFor $w=4$, $2^4 = 16 \\ge 12$, which is sufficient.\nTherefore, the minimum number of bits required for a fixed-length encoding is $w=4$. The average number of bits per opcode for this baseline scheme, denoted $L_{\\text{fixed}}$, is simply the fixed width of each opcode.\n$$L_{\\text{fixed}} = w = 4$$\n\n### Step 2: Optimal Variable-Length Encoding (Huffman Code)\n\nTo construct an optimal prefix-free code, we employ Huffman's algorithm. The algorithm builds a binary tree from the bottom up by iteratively merging the two nodes with the lowest frequencies (or probabilities) until a single root node remains. The length of the codeword for each opcode is its depth in the resulting tree. We will use the given frequencies directly.\n\nThe opcodes and their frequencies $f_i$ are:\n- $O_1: f_1=16$\n- $O_2: f_2=12$\n- $O_3: f_3=8$\n- $O_4, O_5: f_4=f_5=6$\n- $O_6, O_7: f_6=f_7=4$\n- $O_8, O_9, O_{12}: f_8=f_9=f_{12}=2$\n- $O_{10}, O_{11}: f_{10}=f_{11}=1$\n\nLet's build the Huffman tree. We maintain a list of nodes (leaves or subtrees) sorted by frequency.\n1.  Initial list of leaf nodes (frequency, opcode): `[(1, O10), (1, O11), (2, O8), (2, O9), (2, O12), (4, O6), (4, O7), (6, O4), (6, O5), (8, O3), (12, O2), (16, O1)]`\n2.  Merge `(1, O10)` and `(1, O11)` to form a new node `N1` with frequency $1+1=2$.\n3.  Merge `(2, O8)` and `(2, O9)` to form a new node `N2` with frequency $2+2=4$.\n4.  Merge `(2, O12)` and `(2, N1)` to form a new node `N3` with frequency $2+2=4$.\n5.  Merge `(4, O6)` and `(4, O7)` to form a new node `N4` with frequency $4+4=8$.\n6.  Merge `(4, N2)` and `(4, N3)` to form a new node `N5` with frequency $4+4=8$.\n7.  Merge `(6, O4)` and `(6, O5)` to form a new node `N6` with frequency $6+6=12$.\n8.  Merge `(8, O3)` and `(8, N4)` to form a new node `N7` with frequency $8+8=16$.\n9.  Merge `(8, N5)` and `(12, O2)` to form a new node `N8` with frequency $8+12=20$.\n10. Merge `(12, N6)` and `(16, O1)` to form a new node `N9` with frequency $12+16=28$.\n11. Merge `(16, N7)` and `(20, N8)` to form a new node `N10` with frequency $16+20=36$.\n12. Merge `(28, N9)` and `(36, N10)` to form the root node with frequency $28+36=64$.\n\nThe depth of each leaf node in the final tree gives the length $l_i$ of its codeword. Tracing back from the root:\n- Depth 1 nodes: `N9`, `N10`.\n- Depth 2 nodes: `N6`, `O1` (from `N9`); `N7`, `N8` (from `N10`). The length for $O_1$ is $l_1=2$.\n- Depth 3 nodes: `O4`, `O5` (from `N6`); `O3`, `N4` (from `N7`); `N5`, `O2` (from `N8`). The lengths are $l_4=3$, $l_5=3$, $l_3=3$, $l_2=3$.\n- Depth 4 nodes: `O6`, `O7` (from `N4`); `N2`, `N3` (from `N5`). The lengths are $l_6=4$, $l_7=4$.\n- Depth 5 nodes: `O8`, `O9` (from `N2`); `O12`, `N1` (from `N3`). The lengths are $l_8=5$, $l_9=5$, $l_{12}=5$.\n- Depth 6 nodes: `O10`, `O11` (from `N1`). The lengths are $l_{10}=6$, $l_{11}=6$.\n\nThe resulting codeword lengths $l_i$ for each opcode are:\n- $O_1 (f_1=16) \\rightarrow l_1=2$ bits\n- $O_2 (f_2=12) \\rightarrow l_2=3$ bits\n- $O_3 (f_3=8) \\rightarrow l_3=3$ bits\n- $O_4 (f_4=6) \\rightarrow l_4=3$ bits\n- $O_5 (f_5=6) \\rightarrow l_5=3$ bits\n- $O_6 (f_6=4) \\rightarrow l_6=4$ bits\n- $O_7 (f_7=4) \\rightarrow l_7=4$ bits\n- $O_8 (f_8=2) \\rightarrow l_8=5$ bits\n- $O_9 (f_9=2) \\rightarrow l_9=5$ bits\n- $O_{12} (f_{12}=2) \\rightarrow l_{12}=5$ bits\n- $O_{10} (f_{10}=1) \\rightarrow l_{10}=6$ bits\n- $O_{11} (f_{11}=1) \\rightarrow l_{11}=6$ bits\n\n### Step 3: Average Bits for Variable-Length Encoding\n\nThe average number of bits per opcode, $L_{\\text{var}}$, is the weighted average of the codeword lengths, where the weights are the probabilities (or relative frequencies) of the opcodes.\n$$L_{\\text{var}} = \\sum_{i=1}^{N} p_i l_i = \\frac{1}{\\sum f_i} \\sum_{i=1}^{N} f_i l_i$$\nThe total number of instructions is $\\sum f_i = 64$.\nThe total number of bits for the $64$ instructions is $\\sum f_i l_i$:\n$$ \\sum_{i=1}^{12} f_i l_i = (16 \\times 2) + (12 \\times 3) + (8 \\times 3) + (6 \\times 3) + (6 \\times 3) + (4 \\times 4) + (4 \\times 4) + (2 \\times 5) + (2 \\times 5) + (2 \\times 5) + (1 \\times 6) + (1 \\times 6) $$\n$$ \\sum f_i l_i = 32 + 36 + 24 + 18 + 18 + 16 + 16 + 10 + 10 + 10 + 6 + 6 = 202 \\text{ bits} $$\nThe average number of bits per opcode is:\n$$ L_{\\text{var}} = \\frac{202}{64} = \\frac{101}{32} = 3.15625 $$\n\n### Step 4: Code Density Improvement\n\nThe code density improvement $\\delta$ is the fractional reduction in average bits per opcode relative to the fixed-length baseline.\n$$ \\delta = \\frac{L_{\\text{fixed}} - L_{\\text{var}}}{L_{\\text{fixed}}} $$\nSubstituting the calculated values:\n$$ \\delta = \\frac{4 - 3.15625}{4} = \\frac{0.84375}{4} = 0.2109375 $$\n\n### Step 5: Final Answer\n\nThe problem requires the value of $\\delta$ as a decimal fraction rounded to four significant figures.\n$$ \\delta = 0.2109375 $$\nThe first four significant figures are $2$, $1$, $0$, and $9$. The fifth significant figure is $3$, which is less than $5$, so we round down.\n$$ \\delta \\approx 0.2109 $$",
            "answer": "$$\\boxed{0.2109}$$"
        },
        {
            "introduction": "While variable-length instructions can improve code density, they introduce challenges for the processor's front-end, which must fetch bytes and decode them into a stream of instructions. This practice models the performance trade-off between the fetch unit's byte-per-cycle bandwidth and the decoder's instructions-per-cycle width. You will analyze how the average instruction length can create a \"fetch bottleneck\" and limit the processor's overall throughput, measured in Instructions Per Cycle (IPC) .",
            "id": "3650071",
            "problem": "A processor’s pipeline front end consists of an instruction fetch engine and a decode stage. The fetch engine can supply up to $F$ bytes per cycle, and the decode stage can accept up to $D$ instructions per cycle. Assume a steady-state regime with a deep instruction queue, perfect instruction cache, no control hazards (perfect branch prediction), and that bytes fetched in one cycle can be carried over to decode in subsequent cycles (no loss at fetch block boundaries). Let the front end be the sole throughput limiter, so the back end can retire whatever the front end supplies.\n\nConsider two instruction set architecture styles:\n\n- Fixed-length: every instruction is exactly $4$ bytes long.\n- Variable-length: instructions have lengths between $1$ and $15$ bytes. The instruction length $L$ is an independent and identically distributed random variable with the following probability mass function $\\Pr(L=\\ell)$ for $\\ell \\in \\{1,2,\\dots,15\\}$:\n  - $\\Pr(L=1)=0.05$\n  - $\\Pr(L=2)=0.10$\n  - $\\Pr(L=3)=0.15$\n  - $\\Pr(L=4)=0.20$\n  - $\\Pr(L=5)=0.15$\n  - $\\Pr(L=6)=0.10$\n  - $\\Pr(L=7)=0.08$\n  - $\\Pr(L=8)=0.06$\n  - $\\Pr(L=9)=0.04$\n  - $\\Pr(L=10)=0.03$\n  - $\\Pr(L=11)=0.024$\n  - $\\Pr(L=12)=0.008$\n  - $\\Pr(L=13)=0.004$\n  - $\\Pr(L=14)=0.003$\n  - $\\Pr(L=15)=0.001$\n\nUse first-principles reasoning grounded in definitions of throughput and Instructions Per Cycle (IPC) to estimate the steady-state IPC supplied by the front end under each ISA style. Take $F=16$ and $D=4$. Express your final answer as a row vector $\\left(\\text{IPC}_{\\text{fixed}},\\ \\text{IPC}_{\\text{variable}}\\right)$, rounded to four significant figures. No physical units are required.",
            "solution": "The problem requires an estimation of the steady-state Instructions Per Cycle (IPC) for a processor's front end under two different Instruction Set Architectures (ISAs): fixed-length and variable-length. The analysis will be based on the provided pipeline parameters and instruction length characteristics.\n\nThe overall throughput of the front end, measured in Instructions Per Cycle (IPC), is limited by the bottleneck between the instruction fetch stage and the decode stage. We must calculate the maximum sustainable IPC for each stage and the overall IPC will be the minimum of these two values.\n\nThe throughput of the decode stage, $IPC_{\\text{decode}}$, is given directly as its capacity to accept instructions per cycle.\n$$IPC_{\\text{decode}} = D$$\n\nThe throughput of the instruction fetch stage, $IPC_{\\text{fetch}}$, depends on the rate at which bytes are fetched ($F$) and the average size of an instruction. In a steady state, the number of instructions fetched per cycle is the total number of bytes fetched per cycle divided by the expected instruction length in bytes, $E[L]$.\n$$IPC_{\\text{fetch}} = \\frac{F}{E[L]}$$\n\nThe overall front-end throughput is the minimum of the throughputs of its constituent stages.\n$$IPC = \\min(IPC_{\\text{decode}}, IPC_{\\text{fetch}}) = \\min\\left(D, \\frac{F}{E[L]}\\right)$$\nWe are given $F=16$ bytes/cycle and $D=4$ instructions/cycle.\n\n#### Case 1: Fixed-Length ISA\n\nFor the fixed-length ISA, every instruction is exactly $4$ bytes long. Therefore, the instruction length is a deterministic value, not a random variable. The expected length is simply this value.\n$$E[L_{\\text{fixed}}] = 4 \\text{ bytes}$$\nNow, we can calculate the IPC for the fixed-length case, $IPC_{\\text{fixed}}$.\n$$IPC_{\\text{fixed}} = \\min\\left(D, \\frac{F}{E[L_{\\text{fixed}}]}\\right) = \\min\\left(4, \\frac{16}{4}\\right)$$\n$$IPC_{\\text{fixed}} = \\min(4, 4) = 4$$\nFor the fixed-length ISA, the front end is perfectly balanced, achieving a throughput of $4$ IPC.\n\n#### Case 2: Variable-Length ISA\n\nFor the variable-length ISA, we must first compute the expected instruction length, $E[L_{\\text{variable}}]$, using the provided probability mass function $\\Pr(L=\\ell)$. The expected value is calculated as the sum of each possible length multiplied by its probability.\n$$E[L_{\\text{variable}}] = \\sum_{\\ell=1}^{15} \\ell \\cdot \\Pr(L=\\ell)$$\nSubstituting the given values:\n$$\n\\begin{align*}\nE[L_{\\text{variable}}] = & (1)(0.05) + (2)(0.10) + (3)(0.15) + (4)(0.20) + (5)(0.15) + (6)(0.10) \\\\\n& + (7)(0.08) + (8)(0.06) + (9)(0.04) + (10)(0.03) + (11)(0.024) \\\\\n& + (12)(0.008) + (13)(0.004) + (14)(0.003) + (15)(0.001)\n\\end{align*}\n$$\n$$\n\\begin{align*}\nE[L_{\\text{variable}}] = & \\, 0.05 + 0.20 + 0.45 + 0.80 + 0.75 + 0.60 + 0.56 + 0.48 + 0.36 + 0.30 \\\\\n& + 0.264 + 0.096 + 0.052 + 0.042 + 0.015\n\\end{align*}\n$$\nSumming these products gives the expected length:\n$$E[L_{\\text{variable}}] = 5.019 \\text{ bytes}$$\nNow, we calculate the IPC for the variable-length case, $IPC_{\\text{variable}}$.\n$$IPC_{\\text{variable}} = \\min\\left(D, \\frac{F}{E[L_{\\text{variable}}]}\\right) = \\min\\left(4, \\frac{16}{5.019}\\right)$$\nThe fetch-limited throughput is:\n$$\\frac{16}{5.019} \\approx 3.187886033$$\nComparing this with the decode-limited throughput:\n$$IPC_{\\text{variable}} = \\min(4, 3.187886033) = 3.187886033$$\nIn this case, the instruction fetch stage is the bottleneck due to the larger average instruction size.\n\n#### Final Answer Formulation\n\nThe problem asks for the two IPC values, $(\\text{IPC}_{\\text{fixed}}, \\text{IPC}_{\\text{variable}})$, rounded to four significant figures.\n- $IPC_{\\text{fixed}} = 4$. To four significant figures, this is $4.000$.\n- $IPC_{\\text{variable}} \\approx 3.187886$. To four significant figures, this is $3.188$.\n\nThe final result is the row vector $(4.000, 3.188)$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.000 & 3.188\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The impact of instruction length extends beyond simple code size and front-end throughput to the memory hierarchy and energy consumption. This exercise demonstrates how the superior code density of a 16-bit encoding (like ARM Thumb) compared to a 32-bit one can allow a program's working set to fit into the instruction cache, drastically reducing miss rates. You will calculate the resulting effect on cache performance (MPKI) and front-end energy, revealing a powerful system-level advantage of compact instruction encodings .",
            "id": "3650038",
            "problem": "A program’s hot region is executed repeatedly as a tight, sequentially streaming loop. Consider two encodings of the same program: the Advanced RISC Machines (ARM) $32$-bit fixed-length encoding (ARM32) with instruction length $s_{32} = 4\\,\\mathrm{B}$ and the ARM Thumb $16$-bit fixed-length encoding (Thumb) with instruction length $s_{16} = 2\\,\\mathrm{B}$. The instruction cache (I-cache) is fully associative with least-recently-used replacement, total capacity $C = 4\\,\\mathrm{KiB}$, and cache line size $L = 64\\,\\mathrm{B}$. The hot region’s static footprint when encoded in ARM32 is $F_{32} = 6\\,\\mathrm{KiB}$ and when encoded in Thumb is $F_{16} = 3\\,\\mathrm{KiB}$. Assume steady-state execution over many passes of the region so that compulsory cold misses are amortized and capacity behavior dominates. Misses per thousand instructions (MPKI) is defined as total I-cache misses divided by total executed instructions, multiplied by $1000$.\n\nAssume the following front-end dynamic energy per committed instruction model:\n- Each instruction fetch that hits in the I-cache costs $E_{\\mathrm{hit}} = 2\\,\\mathrm{pJ}$.\n- Each I-cache miss incurs an additional $E_{\\mathrm{miss}} = 200\\,\\mathrm{pJ}$ once per miss event.\n- Instruction fetch and decode energy scales linearly with instruction length as $k \\cdot s$, with $k = 0.5\\,\\mathrm{pJ/byte}$ and instruction length $s$ in bytes.\n\nIgnore all other energy components (e.g., branch prediction, instruction translation lookaside buffer).\n\nUsing only the definitions of code density, cache capacity, cache line, the meaning of fully associative least-recently-used behavior under sequential streaming, and the definition of MPKI, derive the steady-state MPKI for each encoding and then the average front-end dynamic energy per committed instruction for each encoding. Finally, compute the ratio\n$$R \\equiv \\frac{E_{\\mathrm{front\\text{-}end/instr}}(\\text{ARM32})}{E_{\\mathrm{front\\text{-}end/instr}}(\\text{Thumb})}.$$\nReport $R$ as a unitless number. Round your answer to four significant figures.",
            "solution": "The solution requires calculating the Misses Per Thousand Instructions (MPKI) and then the average front-end dynamic energy per instruction for both the ARM32 and Thumb encodings.\n\nFirst, we must analyze the instruction cache (I-cache) behavior for each encoding. The I-cache has a capacity of $C = 4\\,\\mathrm{KiB}$ and a line size of $L = 64\\,\\mathrm{B}$. The cache is fully associative with a Least-Recently-Used (LRU) replacement policy.\n\nFor the ARM32 encoding, the static code footprint is $F_{32} = 6\\,\\mathrm{KiB}$. Since $F_{32} > C$ ($6\\,\\mathrm{KiB} > 4\\,\\mathrm{KiB}$), the entire hot loop does not fit into the cache. The problem specifies a \"tight, sequentially streaming loop\" and \"steady-state execution\". In this scenario, as the program counter streams through the code, new cache lines are fetched. Because the footprint is larger than the cache, the LRU policy ensures that by the time the loop wraps around to its beginning, the initial cache lines have been evicted. Consequently, every fetch to a new cache line results in a capacity miss.\n\nThe number of misses per pass through the loop is the total number of cache lines the footprint occupies, which is $\\frac{F_{32}}{L}$. The number of instructions in one pass is the footprint size divided by the instruction size, $\\frac{F_{32}}{s_{32}}$, where $s_{32} = 4\\,\\mathrm{B}$.\n\nThe MPKI is defined as the number of misses per $1000$ instructions. For the ARM32 case:\n$$\n\\mathrm{MPKI}_{32} = \\frac{\\text{Number of misses per pass}}{\\text{Number of instructions per pass}} \\times 1000 = \\frac{F_{32} / L}{F_{32} / s_{32}} \\times 1000 = \\frac{s_{32}}{L} \\times 1000\n$$\nSubstituting the given values:\n$$\n\\mathrm{MPKI}_{32} = \\frac{4\\,\\mathrm{B}}{64\\,\\mathrm{B}} \\times 1000 = \\frac{1}{16} \\times 1000 = 62.5\n$$\n\nFor the ARM Thumb encoding, the static footprint is $F_{16} = 3\\,\\mathrm{KiB}$. Since $F_{16} < C$ ($3\\,\\mathrm{KiB} < 4\\,\\mathrm{KiB}$), the entire hot loop fits within the I-cache. The problem states to assume steady-state execution where compulsory (cold) misses are amortized. This implies that after an initial warm-up phase (the first pass), the entire code of the loop resides in the cache. For all subsequent passes, every instruction fetch will be a hit. Thus, in steady state, the number of misses is zero.\n$$\n\\mathrm{MPKI}_{16} = 0\n$$\n\nNext, we establish the model for the average front-end dynamic energy per committed instruction, $E_{\\mathrm{front\\text{-}end/instr}}$. The problem specifies three energy components. We form an additive model where a base energy is incurred for every instruction, and a penalty is added for miss events, amortized over all instructions.\nThe base energy per instruction consists of the fetch/decode cost, $k \\cdot s$, and the cache hit access cost, $E_{\\mathrm{hit}}$. The additional miss penalty, $E_{\\mathrm{miss}}$, is incurred per miss event. The number of miss events per instruction is $\\frac{\\mathrm{MPKI}}{1000}$.\nThe total average energy per instruction is therefore:\n$$\nE_{\\mathrm{front\\text{-}end/instr}} = (k \\cdot s + E_{\\mathrm{hit}}) + \\frac{\\mathrm{MPKI}}{1000} \\cdot E_{\\mathrm{miss}}\n$$\n\nWe now calculate this energy for each encoding using the given constants: $k = 0.5\\,\\mathrm{pJ/B}$, $E_{\\mathrm{hit}} = 2\\,\\mathrm{pJ}$, and $E_{\\mathrm{miss}} = 200\\,\\mathrm{pJ}$.\n\nFor the ARM32 encoding ($s_{32} = 4\\,\\mathrm{B}$, $\\mathrm{MPKI}_{32} = 62.5$):\n$$\nE_{\\mathrm{front\\text{-}end/instr}}(\\text{ARM32}) = (0.5\\,\\mathrm{pJ/B} \\cdot 4\\,\\mathrm{B} + 2\\,\\mathrm{pJ}) + \\frac{62.5}{1000} \\cdot 200\\,\\mathrm{pJ}\n$$\n$$\nE_{\\mathrm{front\\text{-}end/instr}}(\\text{ARM32}) = (2\\,\\mathrm{pJ} + 2\\,\\mathrm{pJ}) + 0.0625 \\cdot 200\\,\\mathrm{pJ}\n$$\n$$\nE_{\\mathrm{front\\text{-}end/instr}}(\\text{ARM32}) = 4\\,\\mathrm{pJ} + 12.5\\,\\mathrm{pJ} = 16.5\\,\\mathrm{pJ}\n$$\n\nFor the Thumb encoding ($s_{16} = 2\\,\\mathrm{B}$, $\\mathrm{MPKI}_{16} = 0$):\n$$\nE_{\\mathrm{front\\text{-}end/instr}}(\\text{Thumb}) = (0.5\\,\\mathrm{pJ/B} \\cdot 2\\,\\mathrm{B} + 2\\,\\mathrm{pJ}) + \\frac{0}{1000} \\cdot 200\\,\\mathrm{pJ}\n$$\n$$\nE_{\\mathrm{front\\text{-}end/instr}}(\\text{Thumb}) = (1\\,\\mathrm{pJ} + 2\\,\\mathrm{pJ}) + 0\\,\\mathrm{pJ}\n$$\n$$\nE_{\\mathrm{front\\text{-}end/instr}}(\\text{Thumb}) = 3.0\\,\\mathrm{pJ}\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{E_{\\mathrm{front\\text{-}end/instr}}(\\text{ARM32})}{E_{\\mathrm{front\\text{-}end/instr}}(\\text{Thumb})} = \\frac{16.5\\,\\mathrm{pJ}}{3.0\\,\\mathrm{pJ}} = 5.5\n$$\nThe problem requires the answer to be reported to four significant figures.\n$$\nR = 5.500\n$$",
            "answer": "$$\\boxed{5.500}$$"
        }
    ]
}