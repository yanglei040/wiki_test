{
    "hands_on_practices": [
        {
            "introduction": "In modern accelerators, moving data from off-chip memory is often far more expensive in terms of energy and time than performing the computation itself. This exercise explores tiling, a fundamental technique to maximize data reuse within fast on-chip memory and minimize costly external data transfers. By analyzing the ubiquitous matrix multiplication operation, you will derive the optimal tile size that balances on-chip storage constraints with off-chip bandwidth, a core skill for any hardware architect .",
            "id": "3636754",
            "problem": "Consider a Domain-Specific Architecture (DSA) designed for dense matrix multiplication on a large square problem of dimension $N$, computing $C = A B$ where $A$, $B$, and $C$ are $N \\times N$ matrices of $8$-bit integers. The accelerator has an on-chip static random-access memory (SRAM) scratchpad of capacity $S$ bytes and communicates with off-chip dynamic random-access memory (DRAM). The compute kernel uses a single-level, square tiling scheme with tile side length $T$, such that tiles of $A$, $B$, and $C$ of shape $T \\times T$ are brought into the scratchpad and reused while a $T \\times T$ output tile of $C$ is accumulated. Assume the following conditions:\n\n- Each element is $1$ byte, so the tile footprints are $T^{2}$ bytes for each of $A$, $B$, and $C$.\n- While producing one $T \\times T$ output tile of $C$, the algorithm streams across the $k$-dimension in tiles, loading one $T \\times T$ tile from $A$ and one $T \\times T$ tile from $B$ per $k$-step, and keeps the corresponding $T \\times T$ tile of $C$ resident in SRAM until its accumulation is complete.\n- The output tile $C$ is initialized on-chip (no initial read from off-chip) and is written back to off-chip memory once per tile after accumulation.\n- Ignore control overhead, boundary tiles for non-divisible $N$, and double-buffering; treat $T$ as a positive integer that must satisfy the SRAM capacity constraint.\n\nStarting from the definition of matrix multiplication and from first principles of data reuse under single-level blocking, derive the off-chip data movement in bytes, $B(T)$, as a function of $T$, and show the monotonicity of $B(T)$ over all feasible $T$. Using the SRAM capacity constraint and your monotonicity argument, determine the optimal integer tile side length $T$ that minimizes the total off-chip bytes. Express your final answer as a single closed-form analytic expression in terms of $S$. No rounding is required, and your answer should be dimensionless.",
            "solution": "The problem requires the derivation of the optimal integer tile side length, $T$, that minimizes off-chip data movement for a tiled matrix multiplication algorithm on a Domain-Specific Architecture (DSA). The derivation must proceed from first principles.\n\nFirst, let us establish the total data movement, $B(T)$, as a function of the tile side length $T$. The computation is the multiplication of two dense $N \\times N$ matrices, $C = AB$. The elements are $8$-bit integers, which means each element occupies $1$ byte.\n\nThe algorithm uses a single-level square tiling scheme. The $N \\times N$ matrices are partitioned into tiles of size $T \\times T$. The total number of tiles in any of the matrices $A$, $B$, or $C$ is $\\left(\\frac{N}{T}\\right) \\times \\left(\\frac{N}{T}\\right) = \\left(\\frac{N}{T}\\right)^2$, assuming $N$ is perfectly divisible by $T$ as per the problem statement.\n\nThe problem specifies an \"output-stationary\" dataflow. This means that a single $T \\times T$ tile of the output matrix $C$ is held resident in the on-chip SRAM scratchpad while it is being accumulated. Let us analyze the data movement required to compute one such output tile.\n\nThe computation of a single $T \\times T$ tile of $C$, say $C_{ii,jj}$, is given by the sum over the tile index $kk$:\n$$C_{ii,jj} = \\sum_{kk=0}^{(N/T)-1} A_{ii,kk} B_{kk,jj}$$\nwhere $A_{ii,kk}$ and $B_{kk,jj}$ are the corresponding $T \\times T$ input tiles.\n\nAccording to the problem description:\n$1.$ The $T \\times T$ output tile of $C$ is initialized on-chip. After its full accumulation over all $N/T$ values of $kk$, it is written back to off-chip DRAM once. Since each element is $1$ byte, the size of a tile is $T^2$ bytes. This contributes $T^2$ bytes to the total write traffic for this one output tile.\n$2.$ For each step in the accumulation (i.e., for each value of $kk$ from $0$ to $(N/T)-1$), one $T \\times T$ tile of $A$ and one $T \\times T$ tile of $B$ are loaded from DRAM into the SRAM.\nThe total number of such steps to compute one $C$ tile is $N/T$.\nThe volume of data read from DRAM for one $C$ tile is the number of steps multiplied by the data loaded per step:\n$$ \\text{Read traffic per C tile} = \\frac{N}{T} \\times (\\text{size of A tile} + \\text{size of B tile}) = \\frac{N}{T} \\times (T^2 + T^2) = \\frac{N}{T} \\times 2T^2 = 2NT \\text{ bytes} $$\nThe total off-chip data movement (reads + writes) to compute one $T \\times T$ tile of $C$ is therefore $2NT + T^2$ bytes.\n\nThe entire matrix $C$ consists of $(N/T)^2$ such tiles. The total data movement, $B(T)$, is the movement per tile multiplied by the total number of tiles:\n$$ B(T) = \\left(\\frac{N}{T}\\right)^2 \\times (2NT + T^2) $$\n$$ B(T) = \\frac{N^2}{T^2} (2NT) + \\frac{N^2}{T^2} (T^2) = \\frac{2N^3}{T} + N^2 $$\n\nNext, we must consider the constraint imposed by the on-chip SRAM capacity, $S$. The algorithm requires that at any point during the accumulation of a $C$ tile, the scratchpad must hold the $T \\times T$ accumulator for $C$, one $T \\times T$ tile of $A$, and one $T \\times T$ tile of $B$. The total required capacity is the sum of the sizes of these three tiles.\n$$ \\text{SRAM requirement} = \\text{size}(A_{\\text{tile}}) + \\text{size}(B_{\\text{tile}}) + \\text{size}(C_{\\text{tile}}) $$\n$$ \\text{SRAM requirement} = T^2 + T^2 + T^2 = 3T^2 \\text{ bytes} $$\nThis required capacity must not exceed the available SRAM size $S$:\n$$ 3T^2 \\le S $$\n\nOur goal is to find the integer $T$ that minimizes $B(T) = \\frac{2N^3}{T} + N^2$ subject to the constraint $3T^2 \\le S$ and $T$ being a positive integer.\n\nTo determine how $B(T)$ changes with $T$, we analyze its monotonicity. We can treat $T$ as a continuous positive variable for this purpose. The derivative of $B(T)$ with respect to $T$ is:\n$$ \\frac{dB}{dT} = \\frac{d}{dT} \\left( \\frac{2N^3}{T} + N^2 \\right) = \\frac{d}{dT} (2N^3 T^{-1}) + \\frac{d}{dT} (N^2) $$\n$$ \\frac{dB}{dT} = -1 \\cdot 2N^3 T^{-2} + 0 = -\\frac{2N^3}{T^2} $$\nSince $N$ is a problem dimension, $N > 0$, and $T$ is the tile side length, $T > 0$. Thus, $N^3 > 0$ and $T^2 > 0$. Consequently, the derivative $\\frac{dB}{dT}$ is strictly negative for all feasible $T$. This proves that $B(T)$ is a strictly monotonically decreasing function of $T$.\n\nTo minimize a monotonically decreasing function, one must select the largest possible value for its argument. The feasible range for $T$ is defined by the SRAM capacity constraint, $3T^2 \\le S$.\n$$ T^2 \\le \\frac{S}{3} $$\n$$ T \\le \\sqrt{\\frac{S}{3}} $$\nSince $T$ must be an integer, the optimal value for $T$ is the largest integer that satisfies this inequality. This is the floor of the expression on the right-hand side.\n$$ T_{\\text{optimal}} = \\left\\lfloor \\sqrt{\\frac{S}{3}} \\right\\rfloor $$\nThis expression gives the optimal integer tile side length $T$ as a closed-form analytic expression in terms of the SRAM capacity $S$.",
            "answer": "$$\\boxed{\\left\\lfloor \\sqrt{\\frac{S}{3}} \\right\\rfloor}$$"
        },
        {
            "introduction": "Once data is efficiently brought on-chip, the next challenge is to ensure the specialized processing elements (PEs) are used to their full potential. This practice investigates the concept of compute utilization, particularly how it is affected when a problem's dimensions do not perfectly match the fixed size of a hardware accelerator like a systolic array. You will quantify the efficiency loss due to these \"edge effects\" and understand the inherent trade-offs in mapping algorithms to physical hardware .",
            "id": "3636753",
            "problem": "A Domain-Specific Architecture (DSA) is specialized to accelerate a narrowly defined class of computations by shaping its datapath and memory hierarchy to the computation’s structure. Consider a Systolic Array (SA) of fixed spatial dimensions $m \\times n$ accelerating General Matrix–Matrix Multiply (GEMM), where $C = A \\cdot B$ with $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{k \\times c}$. The SA consists of $m \\times n$ identical Processing Elements (PEs). In the standard dataflow, each PE accumulates one element of $C$ across $k$ multiply–accumulate steps, and the array executes a sequence of spatial tiles until all $r \\times c$ outputs of $C$ are produced. Assume $r$ and $c$ are not multiples of $m$ and $n$ respectively.\n\nDefine the time-averaged PE utilization as the ratio of useful multiply–accumulate operations performed by all PEs to the total multiply–accumulate capacity consumed by scheduling the SA over the entire GEMM. Assume the following, consistent with widely used SA GEMM mappings:\n- Each spatial tile schedules the entire $m \\times n$ array for $k$ accumulation cycles, regardless of whether some PEs have no valid output to produce in a partial edge tile.\n- Control and reconfiguration overheads outside the $k$ accumulation cycles are negligible compared to $k$ and can be ignored.\n- Data transfers are scheduled so that the $k$ accumulation cycles per tile are not shortened or extended by pipeline fill and drain effects across tiles.\n\nStarting from these definitions and the basic structure of GEMM, derive the utilization for the case $r$ and $c$ are not multiples of $m$ and $n$. Propose a padding or partitioning approach that maximizes PE utilization under the fixed $m \\times n$ SA, and express the maximal achievable utilization as a single closed-form analytic expression in terms of $r$, $c$, $m$, and $n$. Your final answer must be a single expression. Do not provide intermediate formulas in the final answer. No rounding is required, and no units are to be reported in the final answer.",
            "solution": "The time-averaged PE utilization, $U$, is defined as:\n$$U = \\frac{\\text{Total Useful MAC Operations}}{\\text{Total MAC Capacity Consumed}}$$\nWe will derive expressions for the numerator and the denominator separately.\n\n**Numerator: Total Useful MAC Operations**\nThe computation is the matrix product $C = A \\cdot B$, where $A$ is an $r \\times k$ matrix and $B$ is a $k \\times c$ matrix. The resulting matrix $C$ has dimensions $r \\times c$. Each element $C_{ij}$ of the matrix $C$ is the inner product of the $i$-th row of $A$ and the $j$-th column of $B$. This requires $k$ multiply-accumulate (MAC) operations.\nTherefore, the total number of useful MAC operations required to compute the entire matrix $C$ is:\n$$\\text{Total Useful MACs} = (r \\times c) \\times k = rck$$\n\n**Denominator: Total MAC Capacity Consumed**\nThe SA has dimensions $m \\times n$ and processes the $r \\times c$ output matrix by breaking it into tiles of size $m \\times n$.\nTo cover the $r$ rows of matrix $C$ with tiles of height $m$, the number of tiles required along the row dimension is $\\lceil \\frac{r}{m} \\rceil$.\nSimilarly, to cover the $c$ columns of matrix $C$ with tiles of width $n$, the number of tiles required along the column dimension is $\\lceil \\frac{c}{n} \\rceil$.\nThe total number of spatial tiles, $N_{\\text{tiles}}$, required to cover the entire $r \\times c$ output space is the product of these two quantities:\n$$N_{\\text{tiles}} = \\left\\lceil \\frac{r}{m} \\right\\rceil \\times \\left\\lceil \\frac{c}{n} \\right\\rceil$$\nAccording to the problem's explicit assumption, each of these tiles occupies the entire $m \\times n$ array of PEs for a duration of $k$ MAC cycles.\nThe MAC capacity of the SA for a single tile's execution is the number of PEs multiplied by the number of cycles:\n$$\\text{Capacity per Tile} = (m \\times n) \\times k = mnk$$\nThe total MAC capacity consumed for the entire GEMM operation is the capacity per tile multiplied by the total number of tiles:\n$$\\text{Total MAC Capacity Consumed} = N_{\\text{tiles}} \\times (\\text{Capacity per Tile}) = \\left( \\left\\lceil \\frac{r}{m} \\right\\rceil \\left\\lceil \\frac{c}{n} \\right\\rceil \\right) (mnk)$$\n\n**Utilization Expression and Maximization**\nSubstituting the expressions for the numerator and denominator into the utilization formula:\n$$U = \\frac{rck}{mnk \\left\\lceil \\frac{r}{m} \\right\\rceil \\left\\lceil \\frac{c}{n} \\right\\rceil}$$\nThe term $k$, representing the accumulation depth, cancels out:\n$$U = \\frac{rc}{mn \\left\\lceil \\frac{r}{m} \\right\\rceil \\left\\lceil \\frac{c}{n} \\right\\rceil}$$\nThe problem asks for a padding or partitioning approach to maximize this utilization. The numerator, $r \\times c$, is the fixed amount of useful work. To maximize $U$, the denominator must be minimized. This is equivalent to minimizing the total number of tiles, $N_{\\text{tiles}} = \\lceil \\frac{r}{m} \\rceil \\lceil \\frac{c}{n} \\rceil$.\n\nThe minimal number of $m \\times n$ tiles required to cover an $r \\times c$ area is precisely $\\lceil \\frac{r}{m} \\rceil \\times \\lceil \\frac{c}{n} \\rceil$. Any \"partitioning\" scheme must still cover all $r \\times c$ outputs, and any \"padding\" of the problem to new dimensions $r' \\ge r$ and $c' \\ge c$ cannot decrease the number of tiles.\n\nTherefore, the standard tiling approach already uses the minimum possible number of tiles and thereby minimizes the total capacity consumed. The expression derived for $U$ represents the maximal achievable utilization under the stated constraints.\n\nThe final closed-form analytic expression for the maximal achievable utilization is:\n$$U_{\\text{max}} = \\frac{rc}{mn \\left\\lceil \\frac{r}{m} \\right\\rceil \\left\\lceil \\frac{c}{n} \\right\\rceil}$$",
            "answer": "$$\\boxed{\\frac{rc}{mn \\left\\lceil \\frac{r}{m} \\right\\rceil \\left\\lceil \\frac{c}{n} \\right\\rceil}}$$"
        },
        {
            "introduction": "The ultimate goal of many DSAs is to achieve high throughput by creating a balanced pipeline where computation and data transfer overlap seamlessly. This problem models a realistic stencil processing pipeline, using double-buffering to hide the latency of Direct Memory Access (DMA) from off-chip memory. You will determine the minimal tile dimensions required to ensure the compute core is never stalled waiting for data, a critical analysis for designing high-performance streaming systems .",
            "id": "3636696",
            "problem": "A Domain-Specific Architecture (DSA) with software-managed scratchpad memory processes a two-dimensional grid using a stencil that requires a halo of width $h$ around each compute tile. The DSA has two independent Direct Memory Access (DMA) channels (one for input, one for output), each with sustained bandwidth $b$ and a per-transfer setup latency $t_0$. The scratchpad is organized as a double buffer so that, while one buffer is used for computation, the other buffer is used for DMA transfers of the next tile’s input data and the previous tile’s output data. The computation for each grid point update is throughput-limited to $r$ updates per second. Each grid element is a word of size $s$ bytes. The stencil requires the tile input region to include the compute region plus the halo on all four sides.\n\nConsider streaming tiles of fixed width $W$ elements across a large grid of dimensions $N_x \\times N_y$ (with $N_x$ and $N_y$ large enough that boundary effects can be ignored for the purpose of steady-state scheduling). Each tile processes $W \\times H$ compute elements, and must read $(W + 2h) \\times (H + 2h)$ input elements (to cover halos) and write $W \\times H$ output elements. Assume the input and output transfers for a tile are each performed as a single DMA transaction per tile per channel.\n\nParameters:\n- Grid dimensions: $N_x = 8192$, $N_y = 8192$.\n- Halo width: $h = 2$.\n- Element size: $s = 8$ bytes.\n- Tile width: $W = 512$.\n- DMA bandwidth per channel: $b = 25 \\times 10^{9}$ bytes per second.\n- DMA setup latency per transfer: $t_0 = 2 \\times 10^{-6}$ seconds.\n- Compute throughput: $r = 2 \\times 10^{9}$ element updates per second.\n\nTask:\n1. Using first principles, derive the inequality that guarantees full overlap of DMA transfers with computation under double buffering, and solve for the minimal tile height $H_{\\min}$ that satisfies this inequality. Model the input transfer time for one tile as $T_{\\text{in}} = t_0 + \\frac{s (W + 2h)(H + 2h)}{b}$ and the output transfer time as $T_{\\text{out}} = t_0 + \\frac{s W H}{b}$. Model the compute time as $T_{\\text{comp}} = \\frac{W H}{r}$. Full overlap requires $T_{\\text{comp}} \\ge \\max\\{T_{\\text{in}}, T_{\\text{out}}\\}$.\n2. For $H = H_{\\min}$, quantify the required scratchpad capacity per buffer to hold both the input region (including halo) and the output region simultaneously during compute, i.e., $S_{\\text{buffer}} = s \\left[ (W + 2h)(H + 2h) + W H \\right]$ bytes. Also report the total scratchpad capacity for the double buffer, $S_{\\text{total}} = 2 S_{\\text{buffer}}$.\n3. Express $H_{\\min}$ as a dimensionless integer and the scratchpad capacities in kilobytes, where one kilobyte is defined as $1024$ bytes. Round all numerical results to four significant figures.\n\nYour final answer must be a single row matrix containing $H_{\\min}$, $S_{\\text{buffer}}$ in kilobytes, and $S_{\\text{total}}$ in kilobytes, in that order.",
            "solution": "The core task is to find the minimal tile height $H_{\\min}$ that allows computation to fully overlap with DMA transfers. The architecture uses a double buffer, meaning that while the processor computes on data in one buffer, the DMA engines can transfer data for the next tile into and out of the other buffer. For full overlap, the computation time for a tile, $T_{\\text{comp}}$, must be greater than or equal to the time required for data transfers. With independent channels, the total transfer time is determined by the slower of the two transfers. The condition for full overlap is thus given as:\n$$\nT_{\\text{comp}} \\ge \\max\\{T_{\\text{in}}, T_{\\text{out}}\\}\n$$\nThe expressions for the times are provided:\n- Compute time: $T_{\\text{comp}} = \\frac{W H}{r}$\n- Input transfer time: $T_{\\text{in}} = t_0 + \\frac{s (W + 2h)(H + 2h)}{b}$\n- Output transfer time: $T_{\\text{out}} = t_0 + \\frac{s W H}{b}$\n\nFirst, we determine which of $T_{\\text{in}}$ or $T_{\\text{out}}$ is larger. The input region is $(W + 2h) \\times (H + 2h)$ elements, while the output region is $W \\times H$ elements. Since $W, H, h$ are all positive, the input data volume is strictly greater than the output data volume. Consequently, $T_{\\text{in}} > T_{\\text{out}}$. The condition for full overlap simplifies to:\n$$\nT_{\\text{comp}} \\ge T_{\\text{in}}\n$$\nSubstituting the expressions for $T_{\\text{comp}}$ and $T_{\\text{in}}$:\n$$\n\\frac{W H}{r} \\ge t_0 + \\frac{s (W + 2h)(H + 2h)}{b}\n$$\nWe need to solve this inequality for $H$. Expanding and grouping terms involving $H$:\n$$\n\\frac{W H}{r} \\ge t_0 + \\frac{sWH}{b} + \\frac{2shH}{b} + \\frac{s(2hW + 4h^2)}{b}\n$$\nCollecting all terms with $H$ on one side:\n$$\nH \\left( \\frac{W}{r} - \\frac{sW}{b} - \\frac{2sh}{b} \\right) \\ge t_0 + \\frac{s}{b}(2hW + 4h^2)\n$$\nWe substitute the given numerical values:\n- $W = 512$\n- $h = 2$\n- $s = 8 \\, \\text{bytes}$\n- $b = 25 \\times 10^9 \\, \\text{bytes/s}$\n- $t_0 = 2 \\times 10^{-6} \\, \\text{s}$\n- $r = 2 \\times 10^9 \\, \\text{updates/s}$\n\nThe coefficient of $H$ is:\n$$\nC_1 = \\frac{512}{2 \\times 10^9} - \\frac{8 \\cdot 512}{25 \\times 10^9} - \\frac{2 \\cdot 8 \\cdot 2}{25 \\times 10^9} = (256 - 163.84 - 1.28) \\times 10^{-9} = 90.88 \\times 10^{-9} \\, \\text{s}\n$$\nSince $C_1$ is positive, we can divide by it without changing the inequality's direction. The right-hand side is:\n$$\nC_2 = 2 \\times 10^{-6} + \\frac{8}{25 \\times 10^9}(2 \\cdot 2 \\cdot 512 + 4 \\cdot 2^2) \\\\\nC_2 = 2 \\times 10^{-6} + (0.32 \\times 10^{-9})(2048 + 16) = 2 \\times 10^{-6} + 660.48 \\times 10^{-9} = 2660.48 \\times 10^{-9} \\, \\text{s}\n$$\nNow we find the lower bound for $H$:\n$$\nH \\ge \\frac{2660.48 \\times 10^{-9}}{90.88 \\times 10^{-9}} \\approx 29.2768\n$$\nSince the tile height $H$ must be an integer, the minimal value for $H$ is the smallest integer greater than or equal to this bound:\n$$\nH_{\\min} = \\lceil 29.2768 \\rceil = 30\n$$\nNext, we calculate the required scratchpad memory capacity per buffer, $S_{\\text{buffer}}$, and the total capacity for the double buffer, $S_{\\text{total}}$, using $H = H_{\\min} = 30$.\n$$\nS_{\\text{buffer}} = s \\left[ (W + 2h)(H_{\\min} + 2h) + W H_{\\min} \\right]\n$$\nSubstituting the values:\n$$\nW + 2h = 512 + 4 = 516 \\\\\nH_{\\min} + 2h = 30 + 4 = 34\n$$\n$$\nS_{\\text{buffer}} = 8 \\, \\text{bytes} \\times [ (516 \\times 34) + (512 \\times 30) ] = 8 \\times (17544 + 15360) = 8 \\times 32904 = 263232 \\, \\text{bytes}\n$$\nThe total scratchpad capacity is for two such buffers:\n$$\nS_{\\text{total}} = 2 \\times S_{\\text{buffer}} = 2 \\times 263232 = 526464 \\, \\text{bytes}\n$$\nFinally, we convert the capacities to kilobytes ($1 \\, \\text{KB} = 1024 \\, \\text{bytes}$) and round to four significant figures.\n$$\nS_{\\text{buffer}} = \\frac{263232}{1024} \\, \\text{KB} = 257.0625 \\, \\text{KB} \\approx 257.1 \\, \\text{KB}\n$$\n$$\nS_{\\text{total}} = \\frac{526464}{1024} \\, \\text{KB} = 514.125 \\, \\text{KB} \\approx 514.1 \\, \\text{KB}\n$$\nThe three requested values are $H_{\\min}=30$, $S_{\\text{buffer}} \\approx 257.1 \\, \\text{KB}$, and $S_{\\text{total}} \\approx 514.1 \\, \\text{KB}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n30  257.1  514.1\n\\end{pmatrix}\n}\n$$"
        }
    ]
}