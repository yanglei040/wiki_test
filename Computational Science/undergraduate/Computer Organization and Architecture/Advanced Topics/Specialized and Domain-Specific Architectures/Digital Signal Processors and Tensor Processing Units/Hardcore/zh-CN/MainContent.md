## 引言
在当今计算领域，通用处理器已无法满足日益增长的特定应用对性能和[能效](@entry_id:272127)的极致追求，这催生了[专用处理器架构](@entry_id:755152)的蓬勃发展。其中，[数字信号处理](@entry_id:263660)器（DSP）和张量处理单元（TPU）是两个最具代表性的例子，它们分别主宰着实时信号处理和大规模人工智能计算的领域。然而，许多开发者和学生常常将它们混为一谈，或对其底层架构的深刻差异缺乏系统性认知。本文旨在填补这一知识鸿沟，通过系统性的比较和分析，揭示这两种处理器在设计哲学、工作原理和应用场景上的本质区别。

在接下来的内容中，读者将踏上一段从理论到实践的探索之旅。我们将首先在“原理与机制”一章中，深入剖析 DSP 与 TPU 在计算[范式](@entry_id:161181)、[数据表示](@entry_id:636977)、内存组织和[控制流](@entry_id:273851)方面的核心差异，并用量化分析揭示其性能特征。随后，在“应用与跨学科连接”一章中，我们将通过 FIR 滤波、[快速傅里叶变换 (FFT)](@entry_id:146372) 和[卷积神经网络](@entry_id:178973) (CNN) 等真实案例，展示这些架构原理如何在实际应用中发挥作用，并探讨算法-硬件协同设计的重要性。最后，“动手实践”部分将提供一系列精心设计的问题，帮助读者巩固所学知识，将理论应用于解决具体工程问题。通过这三章的学习，您将建立起对 DSP 和 TPU 的全面而深刻的理解，并能够根据应用需求做出更明智的架构选择。

## 原理与机制

本章在前一章介绍数字信号处理器（DSP）和张量处理单元（TPU）背景的基础上，深入探讨了支撑这两种[处理器架构](@entry_id:753770)的核心工作原理与底层机制。我们将系统性地剖析它们在计算[范式](@entry_id:161181)、[数据表示](@entry_id:636977)、内存组织和控制流管理等方面的根本差异。通过一系列精确的分析和计算示例，我们将揭示这些架构选择如何塑造了它们各自的性能、效率和适用领域。

### 核心计算[范式](@entry_id:161181)：从[指令级并行](@entry_id:750671)到空间并行

[处理器性能](@entry_id:177608)的提升根植于并行计算的有效利用。然而，DSP 和 TPU 在如何发现并利用并行性上采取了截然不同的哲学。DSP 精于挖掘指令流中的**[指令级并行](@entry_id:750671)（Instruction-Level Parallelism, ILP）**，而 TPU 则致力于 harnessing **大规[模空间](@entry_id:159780)并行（Spatial Parallelism）**。

#### DSP 的理念：[指令级并行](@entry_id:750671)的深化

传统的 DSP 架构，如**[超长指令字](@entry_id:756491)（Very Long Instruction Word, VLIW）**处理器，其设计核心在于在一个[时钟周期](@entry_id:165839)内执行多个独立的操作。一个 VLIW 指令包可以同时编码多个操作，例如，一次加载、一次存储和数次算术运算，这些操作被分派到处理器内部多个并行的功能单元。这是一种时间上的并行，即在同一时刻“做更多的事情”。

另一种常见的 DSP 并行策略是**单指令多数据（Single Instruction Multiple Data, SIMD）**。一个 SIMD 指令作用于一个向量数据，在多个数据通道上并行执行相同的操作。例如，一个具有 8 通道 SIMD 的 MAC（乘法累加）单元，在一个时钟周期内可以完成 8 次独立的乘法累加运算。

让我们考虑一个[点积](@entry_id:149019)计算的例子，$y = \sum_{i=1}^{K} x_{i} w_{i}$。一个 VLIW DSP 可以被编程为在每个[时钟周期](@entry_id:165839)内发起 $p$ 次并行的 MAC 操作。例如，如果一个 DSP 能够实现 $p=5$ 的平均[指令级并行](@entry_id:750671)度，那么它每个周期可以处理 5 对 $(x_i, w_i)$。当所有 $K$ 次乘法完成后，这 $p$ 个并行的部分[累加器](@entry_id:175215)中的结果需要通过一个额外的归约步骤（reduction step）合并成最终的标量结果。这个归约步骤本身也需要少量额外的时钟周期，例如，合并 $p$ 个结果需要 $\lceil \log_{2}(p) \rceil$ 个周期。这种方式的性能扩展，是通过在一个核心内集成更复杂、功能更强大的[指令流水线](@entry_id:750685)和执行单元来实现的 。

当单个核心的性能达到极限时，DSP 系统可以通过集成多个这样的复杂核心来进一步提升总吞吐量。在一个固定的功率预算下，设计者需要权衡核心数量和单个核心的性能。例如，在一个 50W 的功率预算下，扣除 5W 的平台固定开销后，剩余的 45W 可以容纳 90 个动态[功耗](@entry_id:264815)为 0.5W 的 DSP 核心。如果每个核心拥有 8 通道的 SIMD MAC 单元，运行在 1.0 GHz，并且考虑到控制开销等因素导致 90% 的持续利用率，那么整个 DSP 集群的持续[吞吐量](@entry_id:271802)可以计算为：

$T_{\mathrm{DSP}} = N_{d} \times w_{d} \times f_{d} \times u_{d} = 90 \times 8 \times (1.0 \times 10^9) \times 0.90 = 6.48 \times 10^{11} \text{ MAC/s} = 0.648 \text{ TMAC/s}$

其中 $N_d$ 是核心数，$w_d$ 是 SIMD 宽度，$f_d$ 是[时钟频率](@entry_id:747385)，$u_d$ 是利用率 。这种扩展方式体现了 DSP 通过复制多个强大的、具备[指令级并行](@entry_id:750671)的核心来提升整体性能的策略。

#### TPU 的理念：空间并行的极致利用

与 DSP 相反，TPU 的核心是一个由大量简单**处理单元（Processing Elements, PE）**构成的二维**[脉动阵列](@entry_id:755785)（Systolic Array）**。并行性不是来自于复杂的单条指令，而是来自于数据在物理空间上的排布和流动。

再次以[点积](@entry_id:149019)计算为例，它可以被映射到由 $B$ 个 PE 组成的一维脉动链上。输入数据 $x_i$ 和权重 $w_i$ 以流水线的方式从一端泵入阵列。在每个[时钟周期](@entry_id:165839)，每个 PE 从其邻居接收[部分和](@entry_id:162077)（partial sum），乘以本地的权重，并加上输入的激活值，然后将更新后的[部分和](@entry_id:162077)传递给下一个 PE。数据就像心跳的脉搏一样，同步地流过整个阵列，每一“波”数据都在空间上与 PE 阵列中的权重进行计算。最终的[点积](@entry_id:149019)结果在 $B$ 个周期后从阵列的另一端流出。这种架构将计算任务在空间上展开，实现了高度的并行 。

对于矩阵乘法等更复杂的运算，TPU 使用二维[脉动阵列](@entry_id:755785)。一个 $N \times N$ 的[脉动阵列](@entry_id:755785)在理想情况下每个时钟周期可以执行 $N^2$ 次 MAC 操作。例如，一个 $128 \times 128$ 的[脉动阵列](@entry_id:755785)，包含 $128^2 = 16384$ 个 PE。即使其时钟频率可能低于高端 DSP（例如 700 MHz），其巨大的空间并行度也能带来惊人的峰值算力。假设这样一个阵列的持续利用率为 85%，其持续[吞吐量](@entry_id:271802)为：

$T_{\mathrm{TPU}} = N^2 \times f_{t} \times u_{t} = 128^2 \times (0.7 \times 10^9) \times 0.85 \approx 9.75 \times 10^{12} \text{ MAC/s} = 9.75 \text{ TMAC/s}$

这个结果远超之前例子中的多核 DSP 集群。这鲜明地展示了 TPU 的设计哲学：通过集成成千上万个简单的、在空间上协同工作的 PE，以实现对特定计算模式（如矩阵运算）的超高[吞吐量](@entry_id:271802) 。

### [数据表示](@entry_id:636977)与[数值精度](@entry_id:173145)

DSP 和 TPU 不仅在计算架构上存在差异，它们对数据的表示方式也反映了其不同的应用目标，这直接影响了计算的精度、动态范围和错误特性。

#### DSP 的选择：高精度定点数

DSP 广泛应用于[音频处理](@entry_id:273289)、通信和控制系统等领域，这些领域对信号的保真度和计算的确定性要求极高。因此，DSP 传统上偏爱**定点（fixed-point）**算术。一种常见的格式是 $Qm.n$，表示一个[有符号数](@entry_id:165424)，包含 1 个[符号位](@entry_id:176301)、$m$ 个整数位和 $n$ 个小数位。

定点数的优势在于其硬件实现简单、功耗低，且没有浮点数的不确定性。其[数值范围](@entry_id:752817)和精度是预先固定的。对于一个 $1+m+n=16$ 比特的 $Q m.n$ 数，其可表示的范围为 $[-2^m, 2^m - 2^{-n}]$，最小量化步长（LSB weight）为 $2^{-n}$。一个有趣且重要的特性是，对于固定总位数（例如 16 位，即 $m+n=15$），其**理想动态范围（Dynamic Range）**——定义为最大可表示正值与最小量化步长之比——实际上与 $m$ 和 $n$ 的具体划分无关。该比值近似为 $(2^m - 2^{-n}) / 2^{-n} = 2^{m+n} - 1 = 2^{15} - 1$。因此，无论是 $Q1.14$（高精度、低范围）还是 $Q4.11$（低精度、高范围），其理想动态范围都由总的数值位数 $m+n=15$ 决定，约为 $20 \log_{10}(2^{15}) \approx 90.3$ dB。设计者选择 $m$ 和 $n$ 的分割点，是在为一个具体的算法在“[数值范围](@entry_id:752817)”和“表示精度”之间做权衡，而不是改变其固有的动态范围 。

在[误差分析](@entry_id:142477)方面，定点 MAC 链的误差来源主要有两个：输入数据的初始[量化误差](@entry_id:196306)和计算结束后的最终[舍入误差](@entry_id:162651)。在一个精心设计的 DSP 中，乘法结果（例如 $Q15 \times Q15 \to Q30$）和累加器（例如 40 位或更宽）拥有足够的“保护位”（guard bits），可以确保在长序列的 MAC 操作中，中间结果的累加是精确无损的。对于一个长度为 $K=256$ 的[点积](@entry_id:149019)，输入为 $|a_i|, |b_i| \le 1$ 的 $Q15$ 数据，其最坏情况下的[绝对误差](@entry_id:139354)主要由 $K$ 次乘法引入的误差和一次最终舍入误差构成，其上界约为 $K \cdot (2 \cdot 2^{-16}) + 2^{-16} = 256 \cdot 2^{-15} + 2^{-16} \approx 0.0078$。这表明 DSP 能够以非常高的精度完成计算 。

#### TPU 的选择：适应性强的低精度[浮点数](@entry_id:173316)

与 DSP 不同，TPU 主要服务于深度学习，这类应用的特点是模型规模巨大，且对[数值误差](@entry_id:635587)有一定的容忍度。为了在功耗和面积限制下实现最高的算力密度，TPU 采用了低精度**浮点（floating-point）**格式，其中最具代表性的是 **[bfloat16](@entry_id:746775)（Brain Floating Point）**。[bfloat16](@entry_id:746775) 拥有 1 个[符号位](@entry_id:176301)、8 个指数位和 7 个小数位。它的巧妙之处在于，其指数位数与标准的 32 位[浮点数](@entry_id:173316)（FP32）完全相同，这意味着它拥有与 FP32 几乎一样的动态范围，但小数位数却大大减少。这种设计牺牲了精度，换取了巨大的存储和带宽节省，以及更小的计算单元。

使用浮点数需要管理其宽广但有限的动态范围。例如，一个 [bfloat16](@entry_id:746775) 数的最大[正规数](@entry_id:141052)指数为 $127$。如果一个张量的最大值为 $|x|_{\max} = 2^{20}$，为了防止在计算中[溢出](@entry_id:172355)并留出安全余量（例如，约束缩放后的最大值不超过 $2^{126}$），我们需要对输入数据进行预缩放（pre-scaling）。这可以通过乘以一个缩放因子 $s = 2^k$ 来实现。约束 $2^{20} \cdot 2^k \le 2^{126}$ 意味着 $20+k \le 126$，因此最大可接受的整数 $k$ 为 $106$。这种对数据进行缩放以适应硬件[数值范围](@entry_id:752817)的操作在 ML 加速器中是常规步骤 。值得注意的是，乘以 $2$ 的幂次只会改变浮点数的指数部分，而不会改变其[尾数](@entry_id:176652)位，因此不会引入额外的舍入误差。

TPU 的误差特性与 DSP 截然不同。在[点积](@entry_id:149019)计算中，TPU 通常将 [bfloat16](@entry_id:746775) 输入的乘积累加到一个更高精度的[累加器](@entry_id:175215)中，例如 FP32。尽管 FP32 的累加过程本身引入的误差很小（其[尾数](@entry_id:176652)有 24 位），但总误差的绝大部分来源于初始步骤——将 FP32 或更高精度的“真实”权重和激活值量化为 [bfloat16](@entry_id:746775)。[bfloat16](@entry_id:746775) 的[单位舍入误差](@entry_id:756332)（unit roundoff）为 $u_{bf16} = 2^{-8}$，远大于 $Q15$ 的量化步长。对于长度为 $K=256$ 的[点积](@entry_id:149019)，其最坏情况下的[绝对误差](@entry_id:139354)[上界](@entry_id:274738)主要由 $K$ 次乘法中每个输入的 [bfloat16](@entry_id:746775) 量化误差决定，约为 $K \cdot (2 \cdot u_{bf16}) = 256 \cdot 2 \cdot 2^{-8} = 2$。这个[误差界](@entry_id:139888)限比 DSP 的定点方案高出几个[数量级](@entry_id:264888)。这揭示了一个关键的架构权衡：TPU 通过接受较低的[数值精度](@entry_id:173145)，换取了巨大的[吞吐量](@entry_id:271802)和处理大规模模型的能力，这对于具有统计鲁棒性的[神经网](@entry_id:276355)络训练和推理是完全可接受的 。

### [内存架构](@entry_id:751845)与数据编排

处理器的计算能力必须有相应的数据供给能力来支撑。DSP 和 TPU 在内存系统的设计上再次展现了它们不同的架构哲学，其核心都在于如何高效地将数据送入计算核心。

#### DSP 内存系统：克服冯·诺依曼瓶颈

DSP 的程序通常包含密集的算术运算和频繁的数据访问，这使得内存带宽成为一个关键瓶颈。为了缓解[冯·诺依曼架构](@entry_id:756577)中指令和数据争夺同一总线的**冯·诺依曼瓶颈（von Neumann bottleneck）**，许多高性能 DSP 采用了**[哈佛架构](@entry_id:750194)（Harvard architecture）**，即拥有独立的指令内存和数据内存，以及各自的访问总线。

让我们通过一个简单的向量乘法内核 `c[i] = a[i] * b[i]` 来量化这种差异。假设每次迭代需要取 1 条 4 字节的指令，读取 2 个 2 字节的操作数，并[写回](@entry_id:756770) 1 个 2 字节的结果。总的数据流量为 $4+2=6$ 字节，指令流量为 4 字节。

*   在一个[共享总线](@entry_id:177993)带宽为 8 字节/周期的**冯·诺依曼**系统中，每次迭代总共需要 $4+6=10$ 字节的带宽。由于总线每个周期只能提供 8 字节，完成这 10 字节的传输至少需要 2 个周期。因此，处理器的[吞吐量](@entry_id:271802)被限制在每个[时钟周期](@entry_id:165839) $1/2$ 次 MAC 操作。
*   在一个拥有 4 字节/周期指令总线和 8 字节/周期[数据总线](@entry_id:167432)的**哈佛**系统中，指令和数据访问可以并行进行。指令获取（4 字节）和数据访问（6 字节）各自都可以在一个周期内完成。因此，处理器可以达到每个时钟周期 1 次 MAC 操作的峰值吞吐量，性能是冯·诺依曼系统的两倍 。

除了分离总线，DSP 还采用多种技术来隐藏来自较慢的主存（如 DRAM）的延迟。对于像 FIR 滤波器这样的流式应用，$y[n] = \sum_{k=0}^{N-1} h[k]x[n-k]$，DSP 通常使用片上**[循环缓冲区](@entry_id:634047)（circular buffer）**来存储输入样本 $x$ 的一个窗口。这样，计算每个新输出 $y[n]$ 时，只需从 DRAM 中加载一个新样本 $x[n]$，而其余 $N-1$ 个样本可从高速的片上 RAM 中重复使用。通过**预取（prefetching）**，DSP 可以在计算当前输出 $y[n]$ 的同时，发起对下一个所需样本 $x[n+1]$ 的加载请求。如果计算一个输出需要 $N$ 个周期，而 D[RAM](@entry_id:173159) 延迟为 $L$ 个周期，那么流水线的[稳态](@entry_id:182458)[吞吐量](@entry_id:271802)就由两者中的较慢者决定，即每个输出需要 $\max(N, L)$ 个周期。只有当 $L > N$ 时，流水线才会因等待数据而停顿  。

为了实现这种计算与数据传输的重叠，DSP 广泛使用**直接内存访问（Direct Memory Access, DMA）**引擎和**双缓冲（double buffering）**，也称作乒乓缓冲（ping-pong buffering）。当核心处理器正在处理一个缓冲区（例如，Buffer A）中的数据时，DMA 引擎可以独立地、并行地将下一帧数据从主存填充到另一个缓冲区（Buffer B）中。为了确保流水线不间断，DMA 传输下一帧数据的时间 $T_{DMA}$ 必须小于或等于核心处理当前帧数据的时间 $T_{compute}$。设计 DMA 传输策略时需要仔细考虑其开销，包括每次传输的固定设置成本。例如，将一个 4096 字节的数据帧作为一个连续的大块进行传输，其总时间（设置+传输）可能远小于将其拆分成 32 个 128 字节的小块进行传输的时间，因为后者会累积 32 次设置开销，可能导致 $T_{DMA} > T_{compute}$，从而造成性能瓶颈 。

#### TPU 内存系统：为[脉动阵列](@entry_id:755785)“喂食”

TPU 的内存[系统设计](@entry_id:755777)目标非常明确：以极高的带宽为[脉动阵列](@entry_id:755785)持续不断地“喂食”数据。TPU 通常采用一个多级的[内存层次结构](@entry_id:163622)，包括巨大的片外 D[RAM](@entry_id:173159) 和相对较小但带宽极高的片上**暂存器内存（scratchpad memory）**，例如 TPU 中的统一缓冲器（Unified Buffer）。与 DSP 的通用缓存不同，暂存器内存由软件（编译器）显式管理，这使得数据流的调度可以被精确地优化。

TPU 的内存组织方式也不同于[哈佛架构](@entry_id:750194)。它不是分离指令和数据，而是为不同类型的数据流（如激活值、权重）提供专用的片上缓冲区和数据通路。这种设计是为了匹配[脉动阵列](@entry_id:755785)特定且高度规则的数据消费模式 。

TPU 性能的关键在于**数据重用（data reuse）**和**[运算强度](@entry_id:752956)（operational intensity）**，即每次从 D[RAM](@entry_id:173159) 读取一个字节的数据后，能在片上完成多少次计算。通过将大型矩阵运算**分块（tiling）**，TPU 可以将一小块数据（tile）加载到高速的暂存器中，然后在该[数据块](@entry_id:748187)上执行大量的计算，从而摊薄 D[RAM](@entry_id:173159) 的访问开销。系统的性能可能受限于计算峰值，也可能受限于内存带宽，这取决于算法的[运算强度](@entry_id:752956)。例如，在一个 $N \times N$ 的[脉动阵列](@entry_id:755785)上计算[矩阵乘法](@entry_id:156035)，理想情况下，每次从 DRAM 读取的数据（激活值、权重）可以在片上被重复使用 $N$ 次。其带宽限制下的性能可以通过 Roofline 模型来分析，即 $T_{bandwidth} = B \times I$，其中 $B$ 是 DRAM 带宽，$I$ 是[运算强度](@entry_id:752956)（MACs/byte）。如果 $T_{bandwidth}$ 低于[脉动阵列](@entry_id:755785)的计算峰值 $T_{peak}$，系统就是**带宽受限的（memory-bound）** 。

与 DSP 类似，TPU 也使用 DMA 和双缓冲来隐藏 D[RAM](@entry_id:173159) 延迟，但其规模要大得多。TPU 在 tile 级别上进行操作。当[脉动阵列](@entry_id:755785)正在处理当前的一组[数据块](@entry_id:748187)（例如，矩阵 $A$ 的一个 panel $A_{128 \times K_s}$ 和矩阵 $B$ 的一个 panel $B_{K_s \times 128}$）时，DMA 会预取下一组 panels 到双缓冲区的另一半。为了完全隐藏内存传输的延迟，计算一个 tile 的时间 $T_{compute}$ 必须大于或等于从 D[RAM](@entry_id:173159) 获取下一个 tile 所需的时间 $T_{memory}$。即必须满足：

$\frac{T_a T_b T_k}{S^2} \ge \lambda + \frac{T_a T_k + T_k T_b}{b}$

其中 $T_a, T_b, T_k$ 是 tile 的维度，$S^2$ 是阵列的计算能力，$\lambda$ 是 DMA 启动延迟，$b$ 是 DRAM 带宽。如果这个条件不满足，TPU 的性能就会受到内存的制约  。

### [控制流](@entry_id:273851)与流水线管理

最后，我们考察 DSP 和 TPU 如何处理程序的[控制流](@entry_id:273851)以及管理其内部的计算流水线。

#### DSP 中的控制：动态分支与[数据冒险](@entry_id:748203)

作为可编程处理器，DSP 必须能高效执行包含 `if-else`、`for` 循环等复杂[控制流](@entry_id:273851)的程序。在深度流水线的处理器中，**分支指令（branch instruction）**是一个主要的性能障碍。现代 DSP 使用**[动态分支预测](@entry_id:748724)（dynamic branch prediction）**来猜测分支的结果，并投机地执行后续指令。如果预测正确，流水线继续全速运行。如果预测错误，流水线必须被清空（flush）并从正确的分支路径重新填充，这会引入显著的**分支预测错误惩罚（misprediction penalty）**。

我们可以量化这种性能影响。假设一个处理器的理想 [CPI](@entry_id:748135)（Cycles Per Instruction）为 1，指令中有 $f_b = 0.25$ 是分支指令，分支预测的错误率为 $r_m = 0.15$，每次预测错误的惩罚为 $L = 8$ 个周期。那么，平均每条指令带来的[停顿](@entry_id:186882)周期数为 $f_b \times r_m \times L = 0.25 \times 0.15 \times 8 = 0.3$。因此，有效的 [CPI](@entry_id:748135) 变为 $CPI_{eff} = 1 + 0.3 = 1.3$。这意味着仅分支预测错误一项就导致了 30% 的性能下降 。

除了[控制冒险](@entry_id:168933)，DSP 流水线还必须处理**[数据冒险](@entry_id:748203)（data hazards）**，特别是**写后读（Read-After-Write, RAW）**冒险。这发生在一条指令需要读取的寄存器恰好是前一条指令将要写入的目标。如果处理器没有**转发网络（forwarding network）**，后一条指令必须被**互锁（interlock）**机制[停顿](@entry_id:186882)，直到前一条指令完成[写回](@entry_id:756770)阶段。例如，在一个 MAC 指令[写回](@entry_id:756770)延迟为 3 个周期的流水线中，执行一串背靠背的、依赖于同一累加器的 MAC 指令，下一条 MAC 必须等待 2 个周期的停顿，才能在前一条 MAC [写回](@entry_id:756770)其结果后安全地发出。这导致有效吞吐率下降到峰值的三分之一，即每 3 个周期才能完成一个 MAC 操作 。

#### TPU 中的控制：[静态调度](@entry_id:755377)与[数据流](@entry_id:748201)

TPU 通过将动态的、指令驱动的控制流转变为静态的、数据驱动的**数据流（dataflow）**执行模型，从根本上规避了上述问题。对于一个给定的计算任务（如一个[神经网](@entry_id:276355)络层），TPU 的编译器会预先生成一个静态的**调度（schedule）**。这个调度精确地规定了在每个[时钟周期](@entry_id:165839)，数据的哪一部分应该从哪里移动到哪里，以及 PE 应该执行什么操作。这个调度信息被加载到片上控制内存中，并像一个节拍器一样确定性地驱动整个[脉动阵列](@entry_id:755785)。

这种模型完全消除了[动态分支预测](@entry_id:748724)及其相关的惩罚。TPU 中没有“if-else”指令流；条件逻辑被转换成数据层面的操作，例如使用掩码或选择性计算。虽然[静态调度](@entry_id:755377)本身可能有一些微小的开销，例如更新循环计数器或切换 tile 配置可能需要几个周期的全局[停顿](@entry_id:186882)，但这种开销是固定的、可预测的，并且在长达数万个周期的计算任务中被摊薄到几乎可以忽略不计。例如，一个运行 16384 个周期的任务，如果每 1024 周期停顿 2 周期，其总停顿开销仅为 $16 \times 2 = 32$ 周期，开销比例仅为 $32 / 16384 \approx 0.195\%$，远低于 DSP 中[动态分支预测](@entry_id:748724)错误的代价 。

TPU 中的“冒险”或性能损失体现在更高的层次上。它不是指令间的冲突，而是计算任务与硬件资源的不[完美匹配](@entry_id:273916)导致的效率损失。主要有两种：

1.  **空间效率低下**：当计算的矩阵或 tile 的尺寸小于[脉动阵列](@entry_id:755785)的物理尺寸时，阵列的一部分 PE 会处于闲置状态。例如，在一个 $128 \times 128$ 的阵列上处理一个 $96 \times 96$ 的 tile，只有 $(96/128)^2 = 56.25\%$ 的 PE 在工作 。
2.  **时间效率低下**：[脉动阵列](@entry_id:755785)的流水线特性意味着在每个 tile 计算的开始阶段（填充流水线）和结束阶段（排空流水线），阵列都不是满负荷工作的。这种“流水线气泡”的开销在 tile 的计算时间中所占的比例会影响时间效率。例如，处理一个深度为 $K=128$ 的 tile 需要 $K+T-1 = 128+128-1 = 255$ 个周期，其时间效率约为 $K/(K+T-1) = 128/255 \approx 50.2\%$ 。

因此，实现 TPU 的高性能不仅需要[硬件设计](@entry_id:170759)，还需要**算法-硬件协同设计（algorithm-hardware co-design）**。开发者或编译器必须明智地[选择算法](@entry_id:637237)（例如 FFT 的 radix 选择）和 tiling策略，以最大化硬件的利用率。例如，对于 DSP，在处理远大于缓存的数据集时，选择一个总计算阶段更少的算法（如 Radix-4 FFT vs. Radix-2 FFT）可以显著减少因数据反复流经内存层次而产生的缓存[强制性未命中](@entry_id:747599)（compulsory misses）次数。对于 TPU，选择与阵列尺寸匹配的 tile 尺寸是最大化空间利用率的关键 。