{
    "hands_on_practices": [
        {
            "introduction": "To begin our practical exploration, we first compare how Digital Signal Processors (DSPs) and Tensor Processing Units (TPUs) handle one of the most fundamental resources: memory. This exercise requires you to calculate and contrast the memory footprint for an identical signal processing task implemented on both architectures . By accounting for differences in data organization—per-channel circular buffers on the DSP versus a single contiguous tensor on the TPU—and architecture-specific memory padding rules, you will gain a concrete understanding of how design philosophy impacts resource efficiency at a low level.",
            "id": "3634514",
            "problem": "A laboratory is comparing two implementations of the same one-dimensional finite impulse response convolution, one on a Digital Signal Processor (DSP) and one on a Tensor Processing Unit (TPU). The task is a real-time, multi-channel finite impulse response filtering of a discrete-time signal, using overlap-save with frame size $N_{f}$ and filter length $L$, applied independently to $C$ channels. Each channel processes $N_{f}$ output samples per frame with a shared filter of length $L$.\n\nAssume the following well-tested facts and core definitions:\n- In overlap-save for a finite impulse response filter, each channel requires access to $N_{f} + L - 1$ recent input samples to produce $N_{f}$ consecutive valid outputs.\n- A memory footprint equals the number of elements multiplied by the bytes per element, with any allocator-imposed padding rounding the allocation up to its required alignment granularity.\n- Brain floating point $16$-bit ($bfloat16$) and fixed-point $Q1.15$ both use $16$-bit storage per sample, i.e., $2$ bytes per element.\n\nUse the following scenario-specific details:\n- DSP side (fixed-point $Q1.15$):\n  1) Each of the $C$ channels stores its input history in a separate circular buffer of length $N_{f}+L-1$ samples.\n  2) Each of the $C$ channels stores its output frame in a separate buffer of length $N_{f}$ samples.\n  3) The filter coefficients are shared across channels and stored once as $L$ samples.\n  4) All three buffer types above are allocated independently and padded by the allocator to the smallest number of bytes that is a multiple of $32$ not less than their unpadded size.\n- TPU side ($bfloat16$):\n  1) The input activation tensor packs all channels contiguously and contains $C\\,(N_{f}+L-1)$ samples.\n  2) The output activation tensor packs all channels contiguously and contains $C\\,N_{f}$ samples.\n  3) The kernel tensor contains the shared $L$ tap values.\n  4) The TPU allocator rounds the total byte size of each of these three tensors up to the smallest multiple of $256$ bytes for the input and output tensors, and to the smallest multiple of $128$ bytes for the kernel tensor.\n\nAssume there is no additional temporary workspace beyond the buffers and tensors described above, and that each element is stored in $2$ bytes on both systems. Let $C = 96$, $N_{f} = 1023$, and $L = 257$.\n\nDefine the signed memory footprint difference $\\Delta$ (in bytes) as\n$$\n\\Delta = \\text{DSP total bytes} - \\text{TPU total bytes}.\n$$\n\nCompute $\\Delta$ exactly. Express your final answer as a single signed integer number of bytes. No rounding is required.",
            "solution": "The problem requires the exact computation of the signed memory footprint difference, $\\Delta$, between a Digital Signal Processor (DSP) implementation and a Tensor Processing Unit (TPU) implementation of a multi-channel finite impulse response (FIR) filter. The difference is defined as $\\Delta = M_{DSP} - M_{TPU}$, where $M_{DSP}$ and $M_{TPU}$ are the total memory footprints in bytes for the respective systems.\n\nFirst, we establish the given parameters:\n- Number of channels, $C = 96$.\n- Frame size, $N_{f} = 1023$ samples.\n- Filter length, $L = 257$ samples.\n- Bytes per sample, $S_{elem} = 2$.\n\nThe calculation of memory footprint for both systems must account for padding. We define an operator $P(s, a)$ that represents the padding of a raw size $s$ (in bytes) up to the next multiple of an alignment granularity $a$ (in bytes). This can be expressed using the ceiling function:\n$$\nP(s, a) = a \\cdot \\left\\lceil \\frac{s}{a} \\right\\rceil\n$$\n\nWe will now calculate the total memory footprint for each system.\n\n**1. DSP Memory Footprint ($M_{DSP}$)**\n\nThe DSP system uses three types of buffers, each allocated and padded independently.\n\n- **Input Buffers**: There are $C$ separate circular buffers for input history. Each buffer stores $N_{f} + L - 1$ samples.\n  - The number of samples per input buffer is $N_{f} + L - 1 = 1023 + 257 - 1 = 1279$.\n  - The unpadded size of one input buffer in bytes is $s_{DSP,in} = (N_{f} + L - 1) \\cdot S_{elem} = 1279 \\cdot 2 = 2558$.\n  - The DSP allocator pads each buffer to a multiple of $32$ bytes. The padded size of one input buffer is $S_{DSP,in} = P(s_{DSP,in}, 32) = 32 \\cdot \\left\\lceil \\frac{2558}{32} \\right\\rceil = 32 \\cdot \\lceil 79.9375 \\rceil = 32 \\cdot 80 = 2560$ bytes.\n  - The total memory for all $C$ input buffers is $M_{DSP,in} = C \\cdot S_{DSP,in} = 96 \\cdot 2560 = 245760$ bytes.\n\n- **Output Buffers**: There are $C$ separate buffers for the output frames. Each buffer stores $N_{f}$ samples.\n  - The number of samples per output buffer is $N_{f} = 1023$.\n  - The unpadded size of one output buffer in bytes is $s_{DSP,out} = N_{f} \\cdot S_{elem} = 1023 \\cdot 2 = 2046$.\n  - The padded size of one output buffer is $S_{DSP,out} = P(s_{DSP,out}, 32) = 32 \\cdot \\left\\lceil \\frac{2046}{32} \\right\\rceil = 32 \\cdot \\lceil 63.9375 \\rceil = 32 \\cdot 64 = 2048$ bytes.\n  - The total memory for all $C$ output buffers is $M_{DSP,out} = C \\cdot S_{DSP,out} = 96 \\cdot 2048 = 196608$ bytes.\n\n- **Filter Buffer**: There is one shared buffer for the filter coefficients, storing $L$ samples.\n  - The number of samples is $L = 257$.\n  - The unpadded size of the filter buffer in bytes is $s_{DSP,filt} = L \\cdot S_{elem} = 257 \\cdot 2 = 514$.\n  - The padded size of the filter buffer is $M_{DSP,filt} = P(s_{DSP,filt}, 32) = 32 \\cdot \\left\\lceil \\frac{514}{32} \\right\\rceil = 32 \\cdot \\lceil 16.0625 \\rceil = 32 \\cdot 17 = 544$ bytes.\n\nThe total memory footprint for the DSP is the sum of these components:\n$$\nM_{DSP} = M_{DSP,in} + M_{DSP,out} + M_{DSP,filt} = 245760 + 196608 + 544 = 442912 \\text{ bytes}\n$$\n\n**2. TPU Memory Footprint ($M_{TPU}$)**\n\nThe TPU system uses three tensors, each with its own padding rule.\n\n- **Input Activation Tensor**: This single tensor packs all channels, containing $C \\cdot (N_{f} + L - 1)$ samples.\n  - The total number of samples is $C \\cdot (N_{f} + L - 1) = 96 \\cdot 1279 = 122784$.\n  - The unpadded size of the input tensor in bytes is $s_{TPU,in} = C \\cdot (N_{f} + L - 1) \\cdot S_{elem} = 122784 \\cdot 2 = 245568$.\n  - The TPU allocator pads this tensor to a multiple of $256$ bytes. The padded size is $M_{TPU,in} = P(s_{TPU,in}, 256) = 256 \\cdot \\left\\lceil \\frac{245568}{256} \\right\\rceil = 256 \\cdot \\lceil 959.25 \\rceil = 256 \\cdot 960 = 245760$ bytes.\n\n- **Output Activation Tensor**: This single tensor packs all channels, containing $C \\cdot N_{f}$ samples.\n  - The total number of samples is $C \\cdot N_{f} = 96 \\cdot 1023 = 98208$.\n  - The unpadded size of the output tensor in bytes is $s_{TPU,out} = C \\cdot N_{f} \\cdot S_{elem} = 98208 \\cdot 2 = 196416$.\n  - This tensor is also padded to a multiple of $256$ bytes. The padded size is $M_{TPU,out} = P(s_{TPU,out}, 256) = 256 \\cdot \\left\\lceil \\frac{196416}{256} \\right\\rceil = 256 \\cdot \\lceil 767.25 \\rceil = 256 \\cdot 768 = 196608$ bytes.\n\n- **Kernel Tensor**: This tensor stores the shared filter coefficients, comprising $L$ samples.\n  - The number of samples is $L = 257$.\n  - The unpadded size of the kernel tensor in bytes is $s_{TPU,kern} = L \\cdot S_{elem} = 257 \\cdot 2 = 514$.\n  - The TPU allocator pads the kernel tensor to a multiple of $128$ bytes. The padded size is $M_{TPU,kern} = P(s_{TPU,kern}, 128) = 128 \\cdot \\left\\lceil \\frac{514}{128} \\right\\rceil = 128 \\cdot \\lceil 4.015625 \\rceil = 128 \\cdot 5 = 640$ bytes.\n\nThe total memory footprint for the TPU is the sum of these components:\n$$\nM_{TPU} = M_{TPU,in} + M_{TPU,out} + M_{TPU,kern} = 245760 + 196608 + 640 = 443008 \\text{ bytes}\n$$\n\n**3. Memory Footprint Difference ($\\Delta$)**\n\nFinally, we compute the signed memory footprint difference $\\Delta$.\n$$\n\\Delta = M_{DSP} - M_{TPU} = 442912 - 443008 = -96 \\text{ bytes}\n$$\nThe result is a negative value, indicating that the total memory footprint of the DSP implementation is smaller than that of the TPU implementation by $96$ bytes under these specific conditions. This difference arises purely from the distinct memory allocation and padding strategies of the two architectures.",
            "answer": "$$\n\\boxed{-96}\n$$"
        },
        {
            "introduction": "Moving from static memory allocation to dynamic performance, this practice investigates the critical role of memory bandwidth. You will analyze the data movement patterns for a DSP performing a convolution and a TPU executing a matrix multiplication, the fundamental operations for each architecture . By calculating the required DRAM traffic and comparing it to the available bandwidth, you will learn to identify potential performance bottlenecks and appreciate how data reuse strategies, such as on-chip coefficient storage or output tiling, are essential for balancing computation with memory access.",
            "id": "3634524",
            "problem": "A Digital Signal Processor (DSP) will compute a one-dimensional finite impulse response (FIR) convolution of a long input stream. The DSP uses a circular buffer of length $L$ for inputs and preloads the $L$ coefficients into on-chip static random-access memory (SRAM) at initialization. The input samples are eight-bit ($8$-bit) or sixteen-bit ($16$-bit) wide depending on mode; for this problem assume the following steady-state mode: input samples are sixteen-bit ($16$-bit), coefficients are sixteen-bit ($16$-bit), and outputs are thirty-two-bit ($32$-bit). The circular buffer is maintained by reading the single newest input sample from Dynamic Random-Access Memory (DRAM) per output sample and evicting the oldest sample, while all coefficients are reused without further DRAM access during steady-state. Each output sample is written back to DRAM. The DSP must sustain a target sample rate of $f_{\\text{DSP}} = 1.5 \\times 10^{9}$ samples per second. The available DRAM bandwidth for the DSP is $B_{\\text{DSP}} = 6.0 \\times 10^{9}$ bytes per second.\n\nA Tensor Processing Unit (TPU) performs General Matrix Multiply (GEMM) $C = A B$ with $A \\in \\mathbb{R}^{M \\times K}$, $B \\in \\mathbb{R}^{K \\times N}$, and $C \\in \\mathbb{R}^{M \\times N}$. The TPU uses weight-stationary systolic processing with on-chip accumulators and tiling. Consider one output tile of size $T_{m} \\times T_{n}$ with $T_{m} = 128$, $T_{n} = 128$, and the full inner dimension $K = 1024$ partitioned into $K$-blocks of size $T_{k} = 256$. For each $K$-block, the TPU streams one activation sub-tile $A_{\\text{tile}} \\in \\mathbb{R}^{T_{m} \\times T_{k}}$ and one weight sub-tile $B_{\\text{tile}} \\in \\mathbb{R}^{T_{k} \\times T_{n}}$ from DRAM into on-chip SRAM; the partial sums for $C_{\\text{tile}} \\in \\mathbb{R}^{T_{m} \\times T_{n}}$ are held on-chip until the full $K$ dimension is processed, at which point the completed $C_{\\text{tile}}$ is written once to DRAM. Assume eight-bit ($8$-bit) elements for $A$ and $B$ and sixteen-bit ($16$-bit) elements for the final stored $C$. The TPU’s systolic array sustains $P = 4.096 \\times 10^{12}$ multiply-accumulate (MAC) operations per second. The available DRAM bandwidth for the TPU is $B_{\\text{TPU}} = 3.0 \\times 10^{11}$ bytes per second. For this problem, interpret gigabytes per second (GB/s) as $10^{9}$ bytes per second.\n\nUsing only fundamental capacity and reuse arguments grounded in the sliding-window model for convolution and the triple-nested-loop interpretation of matrix multiplication, do the following:\n\n1. Derive the steady-state DRAM traffic per DSP output sample, denoted $T_{\\text{DSP}}$ in bytes, explicitly accounting for one new input sample fetched per output and one output sample stored per output. Coefficient reuse should not incur steady-state DRAM traffic.\n2. Derive the DRAM traffic per TPU output tile, denoted $T_{\\text{TPU}}$ in bytes, counting all activation and weight sub-tiles streamed across the $K$-blocks and the one final write of the $C$ tile.\n3. Identify the bandwidth bottlenecks by computing the ratios $r_{\\text{DSP}}$ and $r_{\\text{TPU}}$, where $r_{\\text{DSP}}$ is the required DRAM bandwidth to sustain $f_{\\text{DSP}}$ divided by $B_{\\text{DSP}}$, and $r_{\\text{TPU}}$ is the required DRAM bandwidth to sustain the compute-limited tile rate divided by $B_{\\text{TPU}}$.\n\nReport your final answer as a single row matrix $\\begin{pmatrix} T_{\\text{DSP}} & T_{\\text{TPU}} & r_{\\text{DSP}} & r_{\\text{TPU}} \\end{pmatrix}$. Express $T_{\\text{DSP}}$ and $T_{\\text{TPU}}$ in bytes, and express $r_{\\text{DSP}}$ and $r_{\\text{TPU}}$ as decimals. Round $r_{\\text{DSP}}$ and $r_{\\text{TPU}}$ to four significant figures. Do not include units inside the final boxed answer.",
            "solution": "The solution requires calculating four quantities: the steady-state DRAM traffic per output sample for the DSP, $T_{\\text{DSP}}$; the DRAM traffic per output tile for the TPU, $T_{\\text{TPU}}$; the ratio of required to available DRAM bandwidth for the DSP, $r_{\\text{DSP}}$; and the corresponding ratio for the TPU, $r_{\\text{TPU}}$.\n\n**1. Derivation of DSP DRAM Traffic ($T_{\\text{DSP}}$)**\n\nThe problem specifies that in steady-state operation, the DSP fetches one new input sample from DRAM and writes one completed output sample to DRAM for each output generated. The coefficients for the Finite Impulse Response (FIR) filter are pre-loaded and held in on-chip SRAM, thus not contributing to the steady-state DRAM traffic.\n\nThe size of one input sample, $S_{\\text{in}}$, is given as sixteen-bit ($16$-bit). The size in bytes is:\n$$S_{\\text{in}} = \\frac{16 \\text{ bits}}{8 \\text{ bits/byte}} = 2 \\text{ bytes}$$\nThe size of one output sample, $S_{\\text{out}}$, is given as thirty-two-bit ($32$-bit). The size in bytes is:\n$$S_{\\text{out}} = \\frac{32 \\text{ bits}}{8 \\text{ bits/byte}} = 4 \\text{ bytes}$$\nThe total DRAM traffic per output sample, $T_{\\text{DSP}}$, is the sum of the input read traffic and the output write traffic.\n$$T_{\\text{DSP}} = S_{\\text{in}} + S_{\\text{out}} = 2 \\text{ bytes} + 4 \\text{ bytes} = 6 \\text{ bytes}$$\n\n**2. Derivation of TPU DRAM Traffic ($T_{\\text{TPU}}$)**\n\nThe TPU calculates a tile of the output matrix $C$ of size $T_{m} \\times T_{n}$ where $T_{m} = 128$ and $T_{n} = 128$. This is done by processing the inner dimension $K=1024$ in blocks of size $T_{k}=256$. For each block of the inner dimension, the TPU streams an activation sub-tile and a weight sub-tile from DRAM. The completed output tile is written to DRAM once.\n\nThe number of blocks of the inner dimension, $N_k$, is:\n$$N_k = \\frac{K}{T_k} = \\frac{1024}{256} = 4$$\nFor each of these $N_k$ blocks, the following data is read from DRAM:\n- An activation sub-tile $A_{\\text{sub}}$ of size $T_{m} \\times T_{k} = 128 \\times 256$, with eight-bit ($8$-bit) elements ($1$ byte/element). The size is $S_{A_{\\text{sub}}} = 128 \\times 256 \\times 1 = 32768$ bytes.\n- A weight sub-tile $B_{\\text{sub}}$ of size $T_{k} \\times T_{n} = 256 \\times 128$, with eight-bit ($8$-bit) elements ($1$ byte/element). The size is $S_{B_{\\text{sub}}} = 256 \\times 128 \\times 1 = 32768$ bytes.\n\nThe total read traffic, $T_{\\text{read}}$, is the sum of traffic for all sub-tiles over all $N_k$ blocks:\n$$T_{\\text{read}} = N_k \\times (S_{A_{\\text{sub}}} + S_{B_{\\text{sub}}}) = 4 \\times (32768 \\text{ bytes} + 32768 \\text{ bytes}) = 4 \\times 65536 \\text{ bytes} = 262144 \\text{ bytes}$$\nThe completed output tile $C_{\\text{tile}}$ is written to DRAM once. Its dimensions are $T_{m} \\times T_{n} = 128 \\times 128$, and its elements are sixteen-bit ($16$-bit) ($2$ bytes/element). The write traffic, $T_{\\text{write}}$, is:\n$$T_{\\text{write}} = T_{m} \\times T_{n} \\times 2 \\text{ bytes} = 128 \\times 128 \\times 2 = 16384 \\times 2 = 32768 \\text{ bytes}$$\nThe total traffic per output tile, $T_{\\text{TPU}}$, is the sum of read and write traffic.\n$$T_{\\text{TPU}} = T_{\\text{read}} + T_{\\text{write}} = 262144 \\text{ bytes} + 32768 \\text{ bytes} = 294912 \\text{ bytes}$$\n\n**3. Derivation of DSP Bandwidth Bottleneck Ratio ($r_{\\text{DSP}}$)**\n\nThis ratio is the required DRAM bandwidth ($B_{\\text{req, DSP}}$) divided by the available DRAM bandwidth ($B_{\\text{DSP}}$). The required bandwidth is the product of the traffic per sample ($T_{\\text{DSP}}$) and the target sample rate ($f_{\\text{DSP}}$).\n$$B_{\\text{req, DSP}} = T_{\\text{DSP}} \\times f_{\\text{DSP}} = (6 \\text{ bytes/sample}) \\times (1.5 \\times 10^{9} \\text{ samples/s}) = 9.0 \\times 10^{9} \\text{ bytes/s}$$\nThe available bandwidth is given as $B_{\\text{DSP}} = 6.0 \\times 10^{9}$ bytes/s. The ratio $r_{\\text{DSP}}$ is:\n$$r_{\\text{DSP}} = \\frac{B_{\\text{req, DSP}}}{B_{\\text{DSP}}} = \\frac{9.0 \\times 10^{9} \\text{ bytes/s}}{6.0 \\times 10^{9} \\text{ bytes/s}} = 1.5$$\nExpressed to four significant figures, this value is $1.500$.\n\n**4. Derivation of TPU Bandwidth Bottleneck Ratio ($r_{\\text{TPU}}$)**\n\nThis ratio is the required DRAM bandwidth to sustain the compute-limited tile rate ($B_{\\text{req, TPU}}$) divided by the available DRAM bandwidth ($B_{\\text{TPU}}$).\n\nFirst, we find the number of multiply-accumulate (MAC) operations per output tile, $N_{\\text{ops}}$:\n$$N_{\\text{ops}} = T_{m} \\times T_{n} \\times K = 128 \\times 128 \\times 1024 = 2^7 \\times 2^7 \\times 2^{10} = 2^{24} = 16777216 \\text{ MACs}$$\nThe compute-limited tile rate, $f_{\\text{tile}}$, is the peak performance $P = 4.096 \\times 10^{12}$ MACs/s divided by $N_{\\text{ops}}$:\n$$f_{\\text{tile}} = \\frac{P}{N_{\\text{ops}}} = \\frac{4.096 \\times 10^{12} \\text{ MACs/s}}{16777216 \\text{ MACs/tile}} = 244140.625 \\text{ tiles/s}$$\nThe required bandwidth is the product of the traffic per tile ($T_{\\text{TPU}}$) and this rate:\n$$B_{\\text{req, TPU}} = T_{\\text{TPU}} \\times f_{\\text{tile}} = 294912 \\text{ bytes/tile} \\times 244140.625 \\text{ tiles/s}$$\nTo maintain precision, we use the exact representations: $T_{\\text{TPU}} = 9 \\times 2^{15}$ bytes and $f_{\\text{tile}} = \\frac{4.096 \\times 10^{12}}{2^{24}} = \\frac{2^{12} \\times 10^9}{2^{24}} = \\frac{10^9}{2^{12}}$ tiles/s.\n$$B_{\\text{req, TPU}} = (9 \\times 2^{15}) \\times \\left(\\frac{10^9}{2^{12}}\\right) = 9 \\times 2^{3} \\times 10^9 = 72 \\times 10^9 \\text{ bytes/s}$$\nThe available TPU bandwidth is given as $B_{\\text{TPU}} = 3.0 \\times 10^{11}$ bytes/s. The ratio $r_{\\text{TPU}}$ is:\n$$r_{\\text{TPU}} = \\frac{B_{\\text{req, TPU}}}{B_{\\text{TPU}}} = \\frac{72 \\times 10^9 \\text{ bytes/s}}{3.0 \\times 10^{11} \\text{ bytes/s}} = \\frac{72}{300} = 0.24$$\nExpressed to four significant figures, this value is $0.2400$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 6 & 294912 & 1.500 & 0.2400 \\end{pmatrix}}$$"
        },
        {
            "introduction": "Finally, we examine the subtle but profound effects of finite-precision arithmetic on computational quality. This problem models the impact of quantization—the process of converting continuous or high-precision numbers into a limited set of discrete values—on both a DSP and a TPU . You will use a standard noise model to connect the quantization step size to a classic signal processing metric, the Signal-to-Noise Ratio ($SNR$), and a modern machine learning metric, classification accuracy. This exercise builds a crucial conceptual bridge, demonstrating how the same underlying principles of quantization noise can be used to analyze and predict performance degradation in seemingly disparate application domains.",
            "id": "3634533",
            "problem": "A Digital Signal Processor (DSP) implements a length-$M$ finite-impulse-response (FIR) filter with coefficients $\\{h_k\\}_{k=0}^{M-1}$. The input $x[n]$ is zero-mean, wide-sense stationary, white with variance $\\sigma_x^2 = 1$. The unquantized coefficient energy is $\\sum_{k=0}^{M-1} h_k^2 = E_h$. Each coefficient is uniformly quantized with step size $\\Delta_{\\mathrm{DSP}}$, producing coefficient errors $\\epsilon_k$ that are modeled as independent, zero-mean, uniformly distributed on $[-\\Delta_{\\mathrm{DSP}}/2,\\ \\Delta_{\\mathrm{DSP}}/2]$, independent of $x[n]$ and of each other. Under these assumptions, define the signal-to-noise ratio (SNR) due to coefficient quantization as the ratio of the output power using the ideal coefficients to the output error power induced solely by coefficient quantization.\n\nSeparately, consider a single fully connected layer executed on a Tensor Processing Unit (TPU) with fan-in $n$, weights $\\{w_i\\}_{i=1}^{n}$, and input activations $\\{a_i\\}_{i=1}^{n}$. Assume $a_i$ are independent, zero-mean with variance $\\sigma_a^2$, and $w_i$ are independent, zero-mean with variance $\\sigma_w^2 = 1/n$, so that the pre-activation signal variance is $n \\sigma_w^2 \\sigma_a^2 = \\sigma_a^2$. Suppose the TPU quantizes weights uniformly with step size $\\Delta_{\\mathrm{TPU}}$, producing independent, zero-mean weight errors $\\varepsilon_i$ uniformly distributed on $[-\\Delta_{\\mathrm{TPU}}/2,\\ \\Delta_{\\mathrm{TPU}}/2]$, independent of $a_i$ and of each other. In the small-noise regime, model the per-layer test-accuracy drop $\\delta_{\\mathrm{acc}}$ as being proportional to the layer’s noise-to-signal ratio at pre-activation: $\\delta_{\\mathrm{acc}} = \\kappa \\times \\mathrm{NSR}$, where $\\kappa$ is a given dimensionless sensitivity constant.\n\nFor the following parameters:\n- $M = 64$, $E_h = 2.0$, $\\Delta_{\\mathrm{DSP}} = 2^{-12}$,\n- $n = 1024$, $\\sigma_a^2 = 0.5$, $\\kappa = 0.25$,\n\nuse only the additive white quantization noise model for uniform quantizers (variance of a uniform random variable on $[-\\Delta/2,\\ \\Delta/2]$ equals $\\Delta^2/12$) and linear time-invariant filtering with white inputs to:\n1) derive the DSP’s SNR due to coefficient quantization, and interpret its inverse as the DSP’s relative output noise power $r$,\n2) determine the value of $\\Delta_{\\mathrm{TPU}}$ that makes the TPU’s per-layer accuracy drop $\\delta_{\\mathrm{acc}}$ equal to $r$.\n\nReport only the final value of $\\Delta_{\\mathrm{TPU}}$ as a pure number, rounded to four significant figures.",
            "solution": "The problem requires finding the quantization step size $\\Delta_{\\mathrm{TPU}}$ for a Tensor Processing Unit (TPU) such that its predicted accuracy drop $\\delta_{\\mathrm{acc}}$ equals the relative output noise power $r$ of a Digital Signal Processor (DSP). The solution proceeds in two parts: first, we analyze the DSP to find $r$, and second, we analyze the TPU to find an expression for $\\delta_{\\mathrm{acc}}$, which we then equate to $r$ to solve for $\\Delta_{\\mathrm{TPU}}$.\n\n**Part 1: DSP Relative Output Noise Power**\n\nThe output of the ideal finite-impulse-response (FIR) filter is given by the convolution $y[n] = \\sum_{k=0}^{M-1} h_k x[n-k]$. The signal power at the output, $P_{y_h}$, is the variance of $y[n]$. Since the input $x[n]$ is zero-mean, the output $y[n]$ is also zero-mean.\n$$P_{y_h} = E[y[n]^2] = E\\left[\\left(\\sum_{k=0}^{M-1} h_k x[n-k]\\right)^2\\right] = \\sum_{k=0}^{M-1} \\sum_{j=0}^{M-1} h_k h_j E[x[n-k]x[n-j]]$$\nThe input signal $x[n]$ is white and wide-sense stationary, so its autocorrelation is $R_x[m] = E[x[l]x[l-m]] = \\sigma_x^2 \\delta[m]$, where $\\delta[m]$ is the Kronecker delta function. Given $\\sigma_x^2 = 1$.\n$$E[x[n-k]x[n-j]] = R_x[k-j] = \\sigma_x^2 \\delta[k-j] = 1 \\cdot \\delta[k-j]$$\nSubstituting this into the power expression, the double summation collapses to a single sum:\n$$P_{y_h} = \\sum_{k=0}^{M-1} h_k^2 \\sigma_x^2 = \\sigma_x^2 \\sum_{k=0}^{M-1} h_k^2 = 1 \\cdot E_h = E_h$$\nwhere $E_h = \\sum_{k=0}^{M-1} h_k^2$ is the given coefficient energy.\n\nNext, we find the output error power $P_e$ due to coefficient quantization. The quantized coefficients are $\\hat{h}_k = h_k + \\epsilon_k$, where $\\epsilon_k$ is the quantization error. The output error signal is $e[n] = \\sum_{k=0}^{M-1} \\epsilon_k x[n-k]$. The error power is its variance:\n$$P_e = E[e[n]^2] = E\\left[\\left(\\sum_{k=0}^{M-1} \\epsilon_k x[n-k]\\right)^2\\right] = \\sum_{k=0}^{M-1} \\sum_{j=0}^{M-1} E[\\epsilon_k \\epsilon_j x[n-k]x[n-j]]$$\nThe coefficient errors $\\epsilon_k$ are independent of the input $x[n]$ and of each other. They are also zero-mean. Thus, $E[\\epsilon_k \\epsilon_j] = E[\\epsilon_k^2]\\delta[k-j] = \\sigma_\\epsilon^2 \\delta[k-j]$.\n$$P_e = \\sum_{k=0}^{M-1} \\sum_{j=0}^{M-1} E[\\epsilon_k \\epsilon_j] E[x[n-k]x[n-j]] = \\sum_{k=0}^{M-1} \\sum_{j=0}^{M-1} (\\sigma_\\epsilon^2 \\delta[k-j]) (\\sigma_x^2 \\delta[k-j])$$\nThis simplifies to:\n$$P_e = \\sum_{k=0}^{M-1} \\sigma_\\epsilon^2 \\sigma_x^2 = M \\sigma_\\epsilon^2 \\sigma_x^2$$\nThe quantization error $\\epsilon_k$ is uniformly distributed on $[-\\Delta_{\\mathrm{DSP}}/2, \\Delta_{\\mathrm{DSP}}/2]$. The variance of this distribution is $\\sigma_\\epsilon^2 = \\Delta_{\\mathrm{DSP}}^2 / 12$. Substituting $\\sigma_x^2=1$:\n$$P_e = M \\cdot 1 \\cdot \\frac{\\Delta_{\\mathrm{DSP}}^2}{12} = \\frac{M \\Delta_{\\mathrm{DSP}}^2}{12}$$\nThe signal-to-noise ratio is $\\mathrm{SNR}_{\\mathrm{DSP}} = P_{y_h} / P_e$. The relative output noise power $r$ is its inverse:\n$$r = \\frac{1}{\\mathrm{SNR}_{\\mathrm{DSP}}} = \\frac{P_e}{P_{y_h}} = \\frac{M \\Delta_{\\mathrm{DSP}}^2 / 12}{E_h} = \\frac{M \\Delta_{\\mathrm{DSP}}^2}{12 E_h}$$\n\n**Part 2: TPU Accuracy Drop and Final Solution**\n\nFor the TPU, the pre-activation of a neuron is $z = \\sum_{i=1}^{n} w_i a_i$. The problem states the pre-activation signal variance (power) is $P_S = \\sigma_a^2$.\n\nThe quantization of weights $w_i$ introduces errors $\\varepsilon_i$, so the quantized weights are $\\hat{w}_i = w_i + \\varepsilon_i$. The noise at the pre-activation is $z_{\\mathrm{noise}} = \\sum_{i=1}^{n} \\varepsilon_i a_i$. The power of this noise, $P_N$, is:\n$$P_N = E[z_{\\mathrm{noise}}^2] = E\\left[\\left(\\sum_{i=1}^{n} \\varepsilon_i a_i\\right)^2\\right] = \\sum_{i=1}^{n} \\sum_{j=1}^{n} E[\\varepsilon_i \\varepsilon_j] E[a_i a_j]$$\nThe weight errors $\\varepsilon_i$ are independent and zero-mean, so $E[\\varepsilon_i \\varepsilon_j] = \\sigma_\\varepsilon^2 \\delta[i-j]$. The input activations $a_i$ are independent and zero-mean, so $E[a_i a_j] = \\sigma_a^2 \\delta[i-j]$.\n$$P_N = \\sum_{i=1}^{n} \\sum_{j=1}^{n} (\\sigma_\\varepsilon^2 \\delta[i-j]) (\\sigma_a^2 \\delta[i-j]) = \\sum_{i=1}^{n} \\sigma_\\varepsilon^2 \\sigma_a^2 = n \\sigma_\\varepsilon^2 \\sigma_a^2$$\nThe weight error $\\varepsilon_i$ is uniform on $[-\\Delta_{\\mathrm{TPU}}/2, \\Delta_{\\mathrm{TPU}}/2]$, so its variance is $\\sigma_\\varepsilon^2 = \\Delta_{\\mathrm{TPU}}^2 / 12$.\n$$P_N = n \\sigma_a^2 \\frac{\\Delta_{\\mathrm{TPU}}^2}{12}$$\nThe noise-to-signal ratio at pre-activation is $\\mathrm{NSR}_{\\mathrm{TPU}} = P_N / P_S$.\n$$\\mathrm{NSR}_{\\mathrm{TPU}} = \\frac{n \\sigma_a^2 \\Delta_{\\mathrm{TPU}}^2 / 12}{\\sigma_a^2} = \\frac{n \\Delta_{\\mathrm{TPU}}^2}{12}$$\nThe accuracy drop is modeled as $\\delta_{\\mathrm{acc}} = \\kappa \\times \\mathrm{NSR}_{\\mathrm{TPU}}$.\n$$\\delta_{\\mathrm{acc}} = \\kappa \\frac{n \\Delta_{\\mathrm{TPU}}^2}{12}$$\nWe are asked to find $\\Delta_{\\mathrm{TPU}}$ such that $\\delta_{\\mathrm{acc}} = r$.\n$$\\kappa \\frac{n \\Delta_{\\mathrm{TPU}}^2}{12} = \\frac{M \\Delta_{\\mathrm{DSP}}^2}{12 E_h}$$\nSolving for $\\Delta_{\\mathrm{TPU}}^2$:\n$$\\Delta_{\\mathrm{TPU}}^2 = \\frac{M \\Delta_{\\mathrm{DSP}}^2}{\\kappa n E_h}$$\nTaking the positive square root for the step size:\n$$\\Delta_{\\mathrm{TPU}} = \\sqrt{\\frac{M \\Delta_{\\mathrm{DSP}}^2}{\\kappa n E_h}} = \\Delta_{\\mathrm{DSP}} \\sqrt{\\frac{M}{\\kappa n E_h}}$$\nNow, we substitute the given numerical values: $M = 64$, $E_h = 2.0$, $\\Delta_{\\mathrm{DSP}} = 2^{-12}$, $n = 1024$, and $\\kappa = 0.25$.\n$$\\Delta_{\\mathrm{TPU}} = 2^{-12} \\sqrt{\\frac{64}{0.25 \\cdot 1024 \\cdot 2.0}}$$\nThe term inside the square root is:\n$$\\frac{64}{0.25 \\cdot 1024 \\cdot 2.0} = \\frac{64}{256 \\cdot 2.0} = \\frac{64}{512} = \\frac{2^6}{2^9} = 2^{-3} = \\frac{1}{8}$$\nSo, we have:\n$$\\Delta_{\\mathrm{TPU}} = 2^{-12} \\sqrt{\\frac{1}{8}} = 2^{-12} \\frac{1}{\\sqrt{2^3}} = 2^{-12} \\cdot 2^{-3/2} = 2^{-12-1.5} = 2^{-13.5}$$\nTo express this as a number, we calculate the value:\n$$\\Delta_{\\mathrm{TPU}} = 2^{-13.5} \\approx 8.631608 \\times 10^{-5}$$\nRounding to four significant figures as required:\n$$\\Delta_{\\mathrm{TPU}} \\approx 8.632 \\times 10^{-5}$$",
            "answer": "$$\\boxed{8.632 \\times 10^{-5}}$$"
        }
    ]
}