## 引言
如今，从智能手机到自动驾驶汽车，几乎所有先进的电子设备都由一颗强大的“心脏”驱动——片上系统（System-on-Chip, SoC）。SoC将处理器、内存、加速器和各种外设集成在单一硅片上，是现代计算技术发展的基石。

然而，将一个完整的系统微缩到方寸之间，也带来了前所未有的设计挑战。设计师必须在性能、功耗、成本和安全等多重相互制约的目标之间取得精妙平衡。如何高效地连接数十亿晶体管，确保数据正确、快速地流动，同时控制芯片的“体温”和能耗？这正是SoC设计所要解决的核心问题。

本文将系统性地引导读者深入SoC设计的世界。在“原理与机制”一章中，我们将剖析构成SoC的底层技术，从片上通信网络到[内存一致性](@entry_id:635231)协议，再到动态[功耗管理](@entry_id:753652)和[硬件安全](@entry_id:169931)基础。接着，在“应用与跨学科连接”一章中，我们将展示这些原理如何应用于解决实际问题，例如设计高效的硬件/软件接口、构建实时系统和实现[可信执行环境](@entry_id:756203)。最后，“动手实践”部分将提供一系列练习，帮助读者巩固所学知识并将其应用于具体的设计分析中。

## 原理与机制

本章将深入探讨现代片上系统（SoC）设计的核心原理与关键机制。在“绪论”章节所建立的基础上，我们将系统性地剖析构成复杂SoC的各个子系统，并研究它们之间相互作用所遵循的基本法则。我们将从片上通信网络开始，逐步深入[内存层次结构](@entry_id:163622)、[异构计算](@entry_id:750240)、动态系统管理和[硬件安全](@entry_id:169931)等领域。每一部分都将围绕SoC设计者面临的根本性权衡展开——例如性能与成本、功耗与效率、[可扩展性](@entry_id:636611)与复杂性之间的权衡。

### 片上通信网络

SoC的本质是一个集成了众多处理核心、内存模块和外设的计算系统。将这些分离的组件连接成一个协同工作的整体，是片上通信网络（或称互连）的首要任务。互连结构的选择直接决定了系统的性能上限、面积成本和功耗预算。

#### 基础互连拓扑：总线与[交叉](@entry_id:147634)开关

最简单的互连方式是**[共享总线](@entry_id:177993)（Shared Bus）**。在这种结构中，所有的**主设备（Masters）**（如[CPU核心](@entry_id:748005)、DMA控制器）和**从设备（Slaves）**（如[内存控制器](@entry_id:167560)、外设）共享一组公共的电线。总线结构简单，面积开销小。然而，它的根本局限在于其共享性：在任何时刻，只有一个主设备能通过总[线与](@entry_id:177118)一个从设备通信。当多个主设备试图同时访问时，**仲裁器（Arbiter）**必须介入，串行化这些请求。这使得[共享总线](@entry_id:177993)成为一个性能瓶颈，其总带宽被限制为单次传输的带宽。

与总线相对的是**全连接[交叉](@entry_id:147634)开关（Fully Connected Crossbar）**。[交叉](@entry_id:147634)开关为每个从设备提供一个专用的输入端口，并通过一个开关矩阵网络，允许每个主设备独立地连接到任何一个从设备。其核心硬件是在每个从设备的输入端设置一个 $N:1$ 的[多路复用器](@entry_id:172320)（MUX），其中 $N$ 是主设备的数量。这种结构允许同时发生多达 $\min(N, M)$ 次的非冲突传输（其中 $M$ 是从设备数量），只要这些传输的目标从设备各不相同。

这两种拓扑之间的权衡是典型的面积与性能的折中。假设我们考虑一个拥有 $N=4$ 个主设备和 $M=3$ 个从设备的系统，我们可以从第一性原理分析其成本和性能。在面积方面，[交叉](@entry_id:147634)开关需要在每个从设备的每个数据位上实现一个 $4:1$ 的多路复用器。如果一个 $N:1$ [多路复用器](@entry_id:172320)由 $N-1$ 个 $2:1$ [多路复用器](@entry_id:172320)构成，则[交叉](@entry_id:147634)开关数据通路的面积复杂度为 $O(N \cdot M)$。相比之下，若忽略仲裁逻辑，[共享总线](@entry_id:177993)的面积开销可以视为常数 $O(1)$。在性能方面，交叉开关的优势在特定流量模式下才会显现。当多个主设备频繁访问不同的从设备时，[交叉](@entry_id:147634)开关的并行能力能提供显著高于总线的[吞吐量](@entry_id:271802)。然而，如果出现**热点（Hotspot）**流量，即所有请求都集中于同一个从设备，交叉开关的性能会退化到与总线相当，因为对该从设备的访问仍然需要被其端口的仲裁器串行化。在系统负载极低时，由于请求本身很稀疏，两种结构也表现出相似的性能。

#### 可扩展互连：[片上网络](@entry_id:752421)（NoC）

随着SoC集成的核心数量增长到数十甚至数百个，总线和交叉开关都面临着严峻的[可扩展性](@entry_id:636611)挑战。总线成为性能瓶颈，而交叉开关的面积和功耗则以 $O(N \cdot M)$ 的速度增长，变得不切实际。**[片上网络](@entry_id:752421)（Network-on-Chip, NoC）**[范式](@entry_id:161181)应运而生，它借鉴了[大规模并行计算](@entry_id:268183)机和广域网中的[网络理论](@entry_id:150028)，将通信视为片上的数据包交换。

一个典型的NoC由**路由器（Routers）**和连接它们的**链路（Links）**组成。组件（核心、内存等）通过网络接口连接到路由器。数据被封装成**数据包（Packets）**，由路由器根据其目标地址进行转发。

NoC的设计涉及几个关键方面：
- **拓扑（Topology）**：定义了路由器和链路的物理布局。**二维网格（2D Mesh）**是最常见的拓扑之一，因其结构规整、易于布线而备受青睐。
- **路由算法（Routing Algorithm）**：决定了数据包从源到目的地的路径。**维度顺序路由（Dimension-Ordered Routing）**，如XY路由，是一种简单且无死锁的确定性路由算法。在XY路由中，数据包首先在X维度上行进，直到到达目标列，然后再在Y维度上行进。
- **流控（Flow Control）**：管理网络资源（如缓冲区和链路）的分配。**虫洞流控（Wormhole Flow Control）**将数据包分割成更小的单元——**流片（flits）**，并以流水线方式在路由器之间传输，从而减少了对大型数据包缓冲区的需求，有效降低了延迟。

NoC的性能分析是评估其设计的核心。一个关键指标是**零负载延迟（Zero-load Latency）**，它指在网络空载时，一个数据包从源端注入到被目的端接收所经历的时间。该延迟通常由两部分组成：**序列化延迟（Serialization Latency）**，即将整个数据包注入网络所需的时间（$T_{ser} = \text{包大小} / \text{链路带宽}$）；以及**[网络延迟](@entry_id:752433)（Network Latency）**，即数据包头部流片在网络中跨越多跳转发所需的时间，它等于**跳数（Hop Count）**乘以每跳的延迟（路由器处理延迟+链路传播延迟）。在二维网格和XY路由下，最小跳数等于源和目的地之间的**[曼哈顿距离](@entry_id:141126)（Manhattan Distance）**，即 $|x_1 - x_2| + |y_1 - y_2|$。

通过对特定流量模式进行数学分析，我们可以精确量化NoC的性能。例如，在一个 $N \times N$ 的网格NoC中，若传感器数据从所有边缘节点均匀随机地发送到一个位于中心的[数字信号处理](@entry_id:263660)器（DSP），我们可以通过计算所有边缘节点到中心的[曼哈顿距离](@entry_id:141126)的总和，再除以边缘节点的总数，来推导出平均跳数。这样的分析表明，对于特定的通信模式，平均跳数和延迟是SoC尺寸 $N$ 的函数，这为架构师在早期设计阶段评估系统性能提供了有力的数学工具。

### [内存层次结构](@entry_id:163622)与一致性

数据不仅要在片上组件间流动，还必须被高效地存储和访问。SoC的内存系统是一个多层次的结构，通常包括每个核心私有的高速缓存（L1 Cache）、多个核心共享的末级缓存（LLC），以及片外的主存（DRAM）。

#### 包容性与排除性缓存层次

在[多级缓存](@entry_id:752248)设计中，一个关键的策略是决定不同层级之间的数据关系。主要有两种策略：**包容性（Inclusive）**和**排除性（Exclusive）**。

一个**包容性**层次结构规定，任何存在于低级别缓存（如L1）中的数据块，必须同时存在于高级别缓存（如L2）中。换言之，L2是所有L1缓存内容的超集。这种策略的主要优点是简化了[缓存一致性](@entry_id:747053)管理。当外部探针（例如来自其他核心或I/O设备的请求）需要检查一个数据块是否存在于某个核心的L1缓存中时，只需检查L2的标签即可。如果L2中不存在该[数据块](@entry_id:748187)，那么它必然也不在任何L1中，从而避免了向所有L1广播探针请求。然而，包容性的代价是**[数据冗余](@entry_id:187031)**。L2中必须分配空间来复制所有L1中的内容，这减少了L2可用于存储独特数据的[有效容量](@entry_id:748806)。例如，在一个双核集群中，每个核心拥有64KB的L1缓存，共享一个256KB的L2缓存，那么L2中将有 $2 \times 64\,\text{kB} = 128\,\text{kB}$ 的空间被用于复制L1的内容，占其总容量的50%。此外，当L2需要驱逐一个同时存在于L1中的数据块时，它必须向该L1发送一个**反向失效（Back-invalidation）**消息，以维持包容性。

相比之下，一个**排除性**层次结构规定，一个数据块不能同时存在于L1和L2中。L2在这种模式下充当L1的**牺牲缓存（Victim Cache）**。当一个[数据块](@entry_id:748187)从L1被驱逐时，它会被放入L2。这种策略的优点是最大化了片上缓存的总[有效容量](@entry_id:748806)，避免了任何[数据冗余](@entry_id:187031)。然而，它也使得一致性管理变得复杂。外部探针命中L2并不能保证该数据块不在L1中，可能需要更复杂的查找机制。同时，L1的驱逐（即使是干净块）也会引发到L2的数据传输流量。

#### [基于目录的缓存一致性](@entry_id:748455)

当SoC中的核心数量增加时，传统的基于**总线嗅探（Snooping）**的[缓存一致性协议](@entry_id:747051)变得不再适用，因为广播探针会消耗大量的总线带宽。**基于目录（Directory-based）**的协议提供了一种可扩展的替代方案。

在这种方案中，系统维护一个中心化的**目录（Directory）**结构，通常位于[内存控制器](@entry_id:167560)旁边。该目录为物理内存中的每一个[数据块](@entry_id:748187)（Cache Block）都设有一个条目。每个条目记录着该数据块的当前**一致性状态**（例如，在MSI协议中，是**已修改 Modified**、**共享 Shared** 还是**无效 Invalid**），以及一个**共享者向量（Sharer Vector）**。共享者向量是一个[位向量](@entry_id:746852)，其中每一位对应一个核心，用于指示哪个核心的缓存中可能存有该数据块的副本。

当一个核心需要对某个[数据块](@entry_id:748187)进行读或写操作时，它会向目录发送请求。目录根据该块的当前[状态和](@entry_id:193625)共享者信息，发送点对点的消息给相关的核心，以获取最新数据或使它们的副本失效，而无需向所有核心广播。

目录协议的[可扩展性](@entry_id:636611)并非没有代价。其主要开销在于存储目录本身所需的额外SRAM。我们可以精确计算这个开销。例如，在一个拥有8GiB主存、64字节缓存块和4个核心的系统中，物理内存被划分为 $2^{27}$ 个块。因此，目录需要 $2^{27}$ 个条目。对于每个条目，我们需要存储：
1. **状态**：对于MSI（3个状态），至少需要 $\lceil \log_2(3) \rceil = 2$ 位来编码。
2. **共享者向量**：对于4个核心，需要4位。
因此，每个目录条目需要 $2+4=6$ 位。总的目录存储开销为 $2^{27} \times 6$ 位。通过与一个仅存储存在信息的**嗅探过滤器（Snoop Filter）**（每个条目只需4位）进行比较，我们可以量化目录存储完整状态信息的额外开销。这个例子说明了目录协议的[可扩展性](@entry_id:636611)是以显著的存储开销为代价的。

### 集成异构与异步组件

现代SoC通常是异构的，集成了不同类型、不同供应商、运行在不同时钟频率下的IP核。这给系统集成带来了独特的挑战，尤其是在通信、同步和中断管理方面。

#### 异构系统中的处理器间通信（IPC）

在集成了不同类型处理器（例如，一个参与[缓存一致性](@entry_id:747053)域的应用处理器AP和一个拥有私有非一致性缓存的[数字信号处理](@entry_id:263660)器DSP）的SoC中，它们之间的通信必须小心处理。一种常见的机制是通过共享内存（如S[RAM](@entry_id:173159)）中的**[环形缓冲区](@entry_id:634142)（Ring Buffer）**来实现**邮箱（Mailbox）**通信。

然而，由于核心的内存行为不同，确保数据正确、有序地传递需要遵循严格的协议。假设AP（一致性）向DSP（非一致性）发送数据：
1.  **数据可见性**：当AP将数据写入其私有缓存后，这个更新对DSP是不可见的。AP必须执行一次**缓存清刷（Cache Clean/Flush）**操作，将包含新数据的缓存行强制[写回](@entry_id:756770)到共享S[RAM](@entry_id:173159)中。反之，当DSP要读取数据时，它必须先执行一次**缓存失效（Cache Invalidate）**操作，以确保它会从S[RAM](@entry_id:173159)中读取最新的数据，而不是其私有缓存中可能存在的过时副本。
2.  **操作排序**：生产者（例如AP）必须确保数据本身（如消息描述符）和用于指示数据有效的[元数据](@entry_id:275500)（如更新后的队列尾指针）在对消费者可见时保持正确的顺序。由于现代处理器可能[乱序执行](@entry_id:753020)内存操作，因此在更新尾指针后、触发通知信号（如门铃中断）前，必须插入一个**[内存屏障](@entry_id:751859)（Memory Barrier）**。此屏障确保所有先前的数据写入操作都已全局可见。
3.  **性能考量**：为避免**[伪共享](@entry_id:634370)（False Sharing）**——即两个核心频繁写入位于同一缓存行但逻辑上无关的数据，导致缓存行在核心间不必要地来回摆动——生产者和消费者更新的[数据结构](@entry_id:262134)（如队列的头指针和尾指针）应该被对齐并填充到不同的缓存行中。此外，为了降低中断开销，门铃中断应仅在队列从“空”变为“非空”时触发一次，而不是每次推入新数据时都触发。

#### 管理异步时钟边界

当SoC中的两个模块运行在独立的、异步的时钟域时（例如，一个写时钟域为200MHz，读时钟域为150MHz），它们之间传递的任何信号都必须经过**[时钟域交叉](@entry_id:173614)（Clock Domain Crossing, CDC）**处理。直接用目标时钟对源时钟域的信号进行采样是极其危险的。

CDC设计面临两个主要问题：
1.  **[亚稳态](@entry_id:167515)（Metastability）**：如果输入信号在采样[触发器](@entry_id:174305)的采样时钟边沿附近发生变化，违反了其[建立时间](@entry_id:167213)或保持时间要求，[触发器](@entry_id:174305)可能进入一个不确定的中间状态，既非0也非1。虽然这种状态最终会衰减到稳定状态，但所需时间是无界的。标准解决方案是使用一个**[两级触发器同步器](@entry_id:166595)（2-flip-flop synchronizer）**，它为第一级[触发器](@entry_id:174305)提供一个完整的时钟周期来解决亚稳态，从而将出现未解决[亚稳态](@entry_id:167515)的概率降低到可接受的极低水平。
2.  **数据偏移（Data Skew）**：当[跨时钟域](@entry_id:173614)传递一个多位总线（如FIFO的地址指针）时，由于布线延迟的微小差异，总线上的各个位不会在完全相同的时刻到达目标域的采样[触发器](@entry_id:174305)。如果此时源端的指针值正在改变，且该改变涉及多个位的翻转（例如二[进制](@entry_id:634389)从`0111`变为`1000`），目标域可能会采样到一个混合了新旧位值的、完全错误的中间值（如`1111`）。

为了解决数据偏移问题，业界标准做法是使用**格雷码（Gray Code）**来编码指针。格雷码的一个关键特性是，任意两个连续的码字之间只有一位不同（[汉明距离](@entry_id:157657)为1）。当使用[格雷码](@entry_id:166435)指针时，每次指针递增，只有一个位会翻转。然后，将这个多位的[格雷码](@entry_id:166435)总线上的每一位都通过一个独立的[两级触发器同步器](@entry_id:166595)进行同步。这样，即使正在翻转的那一位在同步过程中被解析为旧值或新值，所有其他位都保持不变。因此，在目标域中，同步后的[格雷码](@entry_id:166435)值要么是递增前的值，要么是递增后的值，绝不会是一个远离真实值的伪随机值。在数据被安全地同步到目标域之后，再将其从[格雷码](@entry_id:166435)转换回[二进制码](@entry_id:266597)进行后续的逻辑处理。

#### 中断传递架构

SoC中的大量外设需要向处理器核心发送中断请求。中断传递架构的设计直接影响系统的响应能力，特别是**最坏情况下的[中断延迟](@entry_id:750776)**——从外设发出中断信号到核心开始执行相应[中断服务程序](@entry_id:750778)（ISR）第一条指令的时间。

主要有两种架构：**集中式中断控制器（Centralized Interrupt Controller, CIC）**和**[分布](@entry_id:182848)式本地中断控制器（Local Interrupt Controllers, LICs）**。CIC将所有 $K$ 个外设的中断线连接到一个单一的控制器，该控制器对所有中断进行仲裁，然后分派给目标核心。而LIC架构则将外设分区，每个核心拥有一个专用的LIC，只负责处理一部分（如 $K/M$ 个，M为核心数）中断。

通过分析一个最坏情况的场景，即所有 $K$ 个外设同时触发中断，我们可以看到这两种架构的显著差异。在CIC设计中，所有 $K$ 个中断请求进入一个队列，由单个仲裁流水线串行处理。最后一个被处理的中断将经历漫长的等待时间，其总延迟主要由仲裁时间（$K \times \text{每中断处理周期}$）决定。而在LIC设计中，$K$ 个中断被并行地分发到 $M$ 个LIC中，每个LIC只需处理 $K/M$ 个中断。由于这 $M$ 个仲裁过程是并行发生的，最坏情况下的延迟取决于处理时间最长的那个LIC，其仲裁时间仅为 $(K/M) \times \text{每中断处理周期}$。因此，[分布](@entry_id:182848)式架构通过并行化仲裁过程，极大地降低了最坏情况下的[中断延迟](@entry_id:750776)，展示了并行化是克服串行瓶颈的有效手段。

### 系统级动态管理

为了在严苛的[功耗](@entry_id:264815)和散热限制下实现高性能，现代SoC广泛采用动态管理技术，在运行时根据工作负载和环境条件调整自身行为。

#### 使用DVFS进行[功耗管理](@entry_id:753652)

**动态电压频率调节（Dynamic Voltage and Frequency Scaling, DVFS）**是SoC中最核心的[功耗管理](@entry_id:753652)技术之一。其原理基于[CMOS](@entry_id:178661)电路的两个基本物理特性：
- **动态[功耗](@entry_id:264815)**：$P_{dyn} \propto C V^2 f$，其中 $C$ 是[开关电容](@entry_id:197049)， $V$ 是供电电压， $f$ 是时钟频率。功耗与电压的平方成正比，与频率成正比。
- **电路延迟**：时钟频率受限于电路的最长路径延迟，而延迟与供电电压密切相关。一个一阶模型是 $f \propto (V - V_t) / V$，其中 $V_t$ 是晶体管的阈值电压。降低电压会增加延迟，从而限制了最高可运行频率。

DVFS利用这些关系，在低负载时降低电压和频率以节省功耗，在高负载时提升电压和频率以提高性能。更有趣的是，DVFS揭示了[并行计算](@entry_id:139241)在能效方面的一个深刻优势。考虑一个性能目标，例如达到单个高性能核心吞吐量的2倍。一种方法是将单个核心的电压和频率推至极限，但这会导致功耗急剧上升（因为 $P \propto V^2 f$，而 $f$ 的提升又需要更高的 $V$）。另一种方法是激活多个（例如3个）核心，但让它们都运行在比单核基准点更低的电压和频率上。因为[功耗](@entry_id:264815)对电压的依赖性（二次方）远强于性能（频率）对电压的依赖性（近似线性），所以多个低[功耗](@entry_id:264815)核心的总功耗可能远低于一个高功耗核心，同时它们的总吞吐量可以达到甚至超过目标。通过求解[功耗](@entry_id:264815)和性能的[约束方程](@entry_id:138140)，我们可以精确地确定满足给定性能目标和功耗上限所需的最少核心数量，这为所谓的“[暗硅](@entry_id:748171)（Dark Silicon）”时代的设计提供了理论指导。

#### 动态热管理（DTM）

功耗最终会以热量的形式耗散。SoC中过高的温度会降低器件可靠性、加速[老化](@entry_id:198459)，甚至导致逻辑错误。**动态[热管理](@entry_id:146042)（Dynamic Thermal Management, DTM）**是一系列用于在运行时控制芯片温度的技术。

我们可以使用一个简化的**集总RC热模型**来理解芯片的热行为。在这个模型中，一个芯片模块（如CPU或GPU）被视为一个单一的热节点，它具有**热容 $C_{th}$**（存储热能的能力，单位J/K）和**热阻 $R_{th}$**（将热量传递到环境的阻力，单位K/W）。当施加功率 $P$ 时，温度 $T(t)$ 的变化由[一阶微分方程](@entry_id:173139)描述：$C_{th} \frac{dT}{dt} = P - \frac{T - T_{amb}}{R_{th}}$，其中 $T_{amb}$ 是环境温度。该系统的行为类似于一个[RC电路](@entry_id:275926)，其温度会以**[热时间常数](@entry_id:151841) $\tau_{th} = R_{th} C_{th}$** 指数趋近于[稳态温度](@entry_id:136775) $T_{ss} = T_{amb} + P \cdot R_{th}$。

当一个高功率任务导致某个模块（如CPU）的温度接近预设的危险阈值 $T_{max}$ 时，DTM策略必须介入。一种有效的策略是**任务迁移（Task Migration）**：将该计算密集型任务从发热的CPU迁移到一个相对凉爽的模块，如GPU。然而，任务迁移本身需要时间（迁移延迟 $L$）。在这段延迟期间，CPU可能仍在以高功率运行，导致其温度继续上升，超过 $T_{max}$，形成**[温度过冲](@entry_id:195464)（Overshoot）**。通过求解热模型方程，我们可以精确计算出到达阈值所需的时间，以及在迁移延迟期间[温度过冲](@entry_id:195464)的大小。分析表明，过冲的大小与迁移延迟 $L$ 和[热时间常数](@entry_id:151841) $\tau_{th}$ 的比值有关。如果一个模块的 $\tau_{th}$ 很大（即[热容](@entry_id:137594)大或热阻大），它的温度变化会很缓慢，即使在迁移延迟期间，温度上升也有限，从而使得任务迁移成为一种有效的[热管理](@entry_id:146042)策略。

### [硬件安全](@entry_id:169931)基础

随着SoC在安全关键领域的应用日益广泛，硬件本身的设计也成为安全体系中不可或缺的一环。一个重要的威胁来自于**[微架构](@entry_id:751960)[侧信道攻击](@entry_id:275985)（Microarchitectural Side-channel Attacks）**。

#### [微架构](@entry_id:751960)[侧信道](@entry_id:754810)

[侧信道攻击](@entry_id:275985)的核心思想是，一个非特权攻击者进程可以通过观察系统的物理特性（如功耗、[电磁辐射](@entry_id:152916)或**执行时间**）来推断一个受害者进程正在处理的敏感数据。在SoC中，**时序[侧信道](@entry_id:754810)（Timing Side-channels）**尤其普遍。其根源在于**共享[微架构](@entry_id:751960)资源**的争用。

当一个受信任的租户 $T$ 和一个不受信任的租户 $U$ 在不同的核心上运行时，它们仍然共享许多片上资源。例如：
- **末级缓存（LLC）**：$T$ 的内存访问模式会影响LLC中的内容，可能驱逐 $U$ 的缓存行。$U$ 通过测量自己访问内存的延迟（命中或缺失），可以推断出 $T$ 的访问模式。
- **DRAM控制器**：$T$ 和 $U$ 的内存请求在DRAM控制器的请求队列和DRAM bank中发生争用，影响彼此的请求服务时间。
- **[片上网络](@entry_id:752421)（NoC）**：$T$ 和 $U$ 的数据包在NoC的链路和路由器上争用带宽和缓冲区，导致[网络延迟](@entry_id:752433)随对方的通信强度而变化。
- **DMA引擎**：如果DMA引擎的描述符队列是共享的，$T$ 提交的大量DMA请求会增加 $U$ 提交请求的排队延迟。

所有这些争用都会导致 $U$ 可观察到的执行时间 $T_{obs}$ 与 $T$ 的秘密相关活动 $S$ 之间产生[统计依赖性](@entry_id:267552)，即互信息 $I(S; T_{obs}) > 0$，从而构成一个[信息泄露](@entry_id:155485)渠道。需要注意的是，核心私有的资源，如L1缓存，不构成跨核[侧信道](@entry_id:754810)的媒介。

#### 通过分区和随机化进行缓解

应对[侧信道攻击](@entry_id:275985)的基本策略是削弱或消除共享资源上的[信息泄露](@entry_id:155485)。主要有两种方法：**分区（Partitioning）**和**随机化（Randomization）**。

- **分区**：将共享资源在逻辑上或物理上划分为多个独立的、隔离的区域，每个区域专用于一个安全域（如一个租户）。
  - **空间分区**：例如，在LLC中进行**路分配（Way Partitioning）**，将8路缓存中的一部分路固定分配给租户 $T$，另一部分分配给 $U$。这样，$T$ 的活动就无法驱逐 $U$ 在其专属路中的数据。同样，对DRAM bank进行**物理地址着色（Physical Address Coloring）**，使得不同租户访问不相交的bank集合，可以消除bank级冲突。
  - **时间分区**：例如，在NoC上实施**时分多址（Time Division Multiple Access, TDMA）**，为每个核心分配固定的传输时间片。这使得一个核心的[网络延迟](@entry_id:752433)不再受另一个核心瞬时流量的影响。

- **随机化**：在资源访问路径中引入随机性，以混淆争用信号，增加攻击者提取有用信息的难度。例如，对于共享的DMA请求队列，可以先用每域私有队列进[行空间](@entry_id:148831)分区，然后在选择服务哪个域时，采用**随机化[轮询](@entry_id:754431)（Randomized Round-Robin）**调度器。这使得一个域的服务时间不再确定性地依赖于另一个域的请求突发性。

理想情况下，严格的分区可以完全消除特定资源上的信道，而随机化则旨在将信道容量降低到噪声水平以下。这些缓解措施是当前安全导向的计算机体系结构研究的核心内容。