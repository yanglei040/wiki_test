{
    "hands_on_practices": [
        {
            "introduction": "Vector processors achieve high performance by operating on multiple data elements simultaneously. However, this power can be bottlenecked if the data isn't supplied efficiently from memory. This first practice explores a fundamental aspect of memory performance: data alignment. You will analyze a hypothetical cost model to quantify how loading data that crosses cache line boundaries can introduce performance penalties, a critical consideration for writing high-performance vectorized code. ",
            "id": "3687644",
            "problem": "A Single Instruction Multiple Data (SIMD) vector processor has a Level 1 data cache line size of $L=64$ bytes and a single vector load unit that can retire up to $P=16$ bytes of aligned data per cycle. Consider streaming loads from a large array of single-precision floating-point values, where each element is $4$ bytes. Assume the array is resident in the Level 1 cache, so cache hit latency does not dominate, and the steady-state throughput model applies. A vector register holds $W$ floats, so the per-load byte size is $S=4W$ bytes. An aligned vector load is defined as one whose starting address and extent fit entirely within a single cache line. A cross-boundary vector load is defined as one whose extent straddles two distinct cache lines, causing the hardware to split the load into two parts. Use the following cost model, which is representative of split-load handling in modern vector architectures:\n- For an aligned vector load of size $S$ bytes, the load unit issues $\\lceil S/P \\rceil$ aligned micro-loads of $P$ bytes each, for a steady-state cost of $\\lceil S/P \\rceil$ cycles per vector load.\n- For a cross-boundary vector load of size $S$ bytes, the split causes an additional fixed overhead of $\\delta=1$ cycle per vector load (to account for address-split and merge), in addition to the $\\lceil S/P \\rceil$ cycles of data movement.\n\nStarting from the definitions above and the given cost model, derive the cycles per element for aligned and cross-boundary vector loads, and then quantify the alignment penalty by the ratio\n$$\\rho(W)=\\frac{\\text{cycles per element (cross-boundary)}}{\\text{cycles per element (aligned)}}$$\nfor $W \\in \\{4,8,16\\}$. Express your final answer as the three values of $\\rho(W)$ in a single row matrix, ordered by increasing $W$. No rounding is required.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of computer architecture, specifically vector processor performance modeling. All terms are clearly defined, the premises are consistent, and sufficient information is provided to derive a unique solution.\n\nThe objective is to derive the alignment penalty ratio, $\\rho(W)$, for a SIMD vector processor under a specific cost model for memory loads. The ratio is defined as:\n$$\n\\rho(W) = \\frac{\\text{cycles per element (cross-boundary)}}{\\text{cycles per element (aligned)}}\n$$\nThe problem provides the following parameters:\n- Cache line size, $L = 64$ bytes.\n- Vector load unit throughput, $P = 16$ bytes per cycle.\n- Size of a single-precision float element, $4$ bytes.\n- Number of float elements in a vector register, $W$.\n- The total byte size of a vector load, $S = 4W$ bytes.\n- Fixed overhead for a cross-boundary load, $\\delta = 1$ cycle.\n\nThe costs per vector load are given by the model:\n- For an aligned vector load, the cost is $C_{align} = \\lceil S/P \\rceil$ cycles.\n- For a cross-boundary vector load, the cost is $C_{cross} = \\lceil S/P \\rceil + \\delta$ cycles.\n\nFirst, we express these costs as a function of the vector width $W$. Substituting $S = 4W$, $P = 16$, and $\\delta = 1$ into the cost models:\n$$\nC_{align}(W) = \\left\\lceil \\frac{4W}{16} \\right\\rceil = \\left\\lceil \\frac{W}{4} \\right\\rceil \\quad \\text{cycles per vector load}\n$$\n$$\nC_{cross}(W) = \\left\\lceil \\frac{4W}{16} \\right\\rceil + 1 = \\left\\lceil \\frac{W}{4} \\right\\rceil + 1 \\quad \\text{cycles per vector load}\n$$\nThe problem asks for a ratio of cycles per *element*. Since each vector load operation processes $W$ elements, we can find the cycles per element ($CPE$) by dividing the cycles per vector load by $W$.\n\nThe cycles per element for an aligned load is:\n$$\nCPE_{align}(W) = \\frac{C_{align}(W)}{W} = \\frac{\\left\\lceil \\frac{W}{4} \\right\\rceil}{W}\n$$\nThe cycles per element for a cross-boundary load is:\n$$\nCPE_{cross}(W) = \\frac{C_{cross}(W)}{W} = \\frac{\\left\\lceil \\frac{W}{4} \\right\\rceil + 1}{W}\n$$\nNow, we can formulate the alignment penalty ratio, $\\rho(W)$:\n$$\n\\rho(W) = \\frac{CPE_{cross}(W)}{CPE_{align}(W)} = \\frac{\\frac{\\left\\lceil \\frac{W}{4} \\right\\rceil + 1}{W}}{\\frac{\\left\\lceil \\frac{W}{4} \\right\\rceil}{W}}\n$$\nThe term $W$ in the denominator of both the numerator and the denominator cancels out, simplifying the expression for $\\rho(W)$ to:\n$$\n\\rho(W) = \\frac{\\left\\lceil \\frac{W}{4} \\right\\rceil + 1}{\\left\\lceil \\frac{W}{4} \\right\\rceil} = 1 + \\frac{1}{\\left\\lceil \\frac{W}{4} \\right\\rceil}\n$$\nWe are asked to evaluate this ratio for $W \\in \\{4, 8, 16\\}$.\n\nCase 1: $W = 4$\nThe number of micro-loads is $\\lceil 4/4 \\rceil = \\lceil 1 \\rceil = 1$.\nThe penalty ratio is:\n$$\n\\rho(4) = 1 + \\frac{1}{\\lceil 4/4 \\rceil} = 1 + \\frac{1}{1} = 2\n$$\n\nCase 2: $W = 8$\nThe number of micro-loads is $\\lceil 8/4 \\rceil = \\lceil 2 \\rceil = 2$.\nThe penalty ratio is:\n$$\n\\rho(8) = 1 + \\frac{1}{\\lceil 8/4 \\rceil} = 1 + \\frac{1}{2} = \\frac{3}{2}\n$$\n\nCase 3: $W = 16$\nThe number of micro-loads is $\\lceil 16/4 \\rceil = \\lceil 4 \\rceil = 4$.\nThe penalty ratio is:\n$$\n\\rho(16) = 1 + \\frac{1}{\\lceil 16/4 \\rceil} = 1 + \\frac{1}{4} = \\frac{5}{4}\n$$\n\nThe final answer requires these three values, $\\rho(4)$, $\\rho(8)$, and $\\rho(16)$, ordered by increasing $W$, in a single row matrix. The values are $2$, $\\frac{3}{2}$, and $\\frac{5}{4}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 2  \\frac{3}{2}  \\frac{5}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "With data loaded into vector registers, the next step is to perform computations in parallel. A common and powerful parallel pattern is the 'reduction,' where an array of values is reduced to a single result, such as a sum or maximum. This exercise challenges you to design and analyze two different strategies for a vectorized reduction on a hypothetical SIMD processor, revealing the trade-offs between intra-lane and inter-lane data movement. ",
            "id": "3687551",
            "problem": "Consider a Single Instruction Multiple Data (SIMD) vector processor with $L$ lanes, where each lane holds $W$ single-precision floating-point elements in its vector register. You are asked to design a reduction of $N$ floats to a single scalar using two alternative vectorized strategies that exploit horizontal adds and lane-crossing shuffles. Assume all data are resident in registers and memory effects are negligible. The two strategies are as follows.\n\nStrategy A (tree-based reduction within lanes first): Within each lane, perform a binary-tree reduction using horizontal add (HADD) operations that halve the number of elements per lane at each step until a single value per lane remains. Then, reduce across lanes using a binary-tree that pairs lanes using lane-crossing shuffle (SHFL) followed by vector add (VADD) at each level until a single scalar is produced for the vector register. Finally, accumulate across all vector-register results produced while streaming through the $N$ elements.\n\nStrategy B (lane-crossing shuffles first): First, reduce across lanes using a binary-tree that pairs lanes with SHFL followed by VADD at each level, while maintaining $W$ elements per lane. After a single lane remains, perform the within-lane HADD binary-tree to reduce the remaining $W$ elements to a single scalar. Finally, accumulate across all vector-register results produced while streaming through the $N$ elements.\n\nAssume the following latencies:\n- Horizontal add (HADD) latency: $h = 3$ cycles per stage.\n- Lane-crossing shuffle (SHFL) latency: $s = 5$ cycles per stage.\n- Vector add (VADD) latency: $a = 4$ cycles per stage.\n\nAssume $L = 8$ lanes, $W = 16$ elements per lane, and $N = 2^{20}$ floats. Assume there is sufficient functional-unit capacity to overlap the scalar accumulation of each vector-register result with the vector reduction of the subsequent register, so that the overall critical-path latency is the sum of the vector-reduction latency per register across all registers plus one final scalar add latency to incorporate the last registerâ€™s result. Express the final answer in cycles. No rounding is required.\n\nFrom first principles and core definitions of binary-tree reductions, compute the total latency (in cycles) of the faster strategy among Strategy A and Strategy B.",
            "solution": "The problem is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- **Vector Processor Parameters:**\n  - Number of lanes, $L = 8$.\n  - Elements per lane, $W = 16$.\n  - Total number of single-precision floats to reduce, $N = 2^{20}$.\n- **Operation Latencies:**\n  - Horizontal add (HADD) latency, $h = 3$ cycles/stage.\n  - Lane-crossing shuffle (SHFL) latency, $s = 5$ cycles/stage.\n  - Vector add (VADD) latency, $a = 4$ cycles/stage.\n- **Reduction Strategies:**\n  - **Strategy A:** Within-lane HADD reduction tree, followed by across-lane SHFL+VADD reduction tree.\n  - **Strategy B:** Across-lane SHFL+VADD reduction tree, followed by within-lane HADD reduction tree.\n- **Latency Calculation Model:**\n  - Data is resident in registers; memory effects are negligible.\n  - Scalar accumulation of a result is overlapped with the vector reduction of the next chunk of data.\n  - The total latency is the sum of vector-reduction latencies for all registers plus one final scalar add latency.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on standard concepts in computer architecture, specifically SIMD vector processors and parallel reduction algorithms. The operations (HADD, SHFL, VADD) are representative of modern vector instruction set architectures. The model is scientifically sound.\n- **Well-Posed:** All necessary parameters ($L$, $W$, $N$) and latencies ($h$, $s$, $a$) are provided. The strategies and the overall latency calculation model are defined, leading to a unique and meaningful solution.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased technical language.\n- **Completeness and Consistency:** The total number of elements $N = 2^{20}$ is perfectly divisible by the number of elements in a vector register, $L \\times W = 8 \\times 16 = 128 = 2^7$. The number of registers to process is $2^{20} / 2^7 = 2^{13}$, an integer. The problem is internally consistent and complete.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe task is to find the total latency of the faster of two vectorized reduction strategies. We begin from first principles of parallel reduction.\n\nA binary-tree reduction on $k$ elements requires a number of stages equal to $\\lceil \\log_2(k) \\rceil$. The total latency for a reduction phase is the number of stages multiplied by the latency per stage.\n\nFirst, we determine the number of elements processed per vector register and the total number of vector registers that need to be processed.\nThe total number of elements held in one vector register is $V = L \\times W$.\nGiven $L=8$ and $W=16$, we have:\n$$V = 8 \\times 16 = 128 = 2^7 \\text{ elements}$$\nThe total number of elements to reduce is $N = 2^{20}$. The data must be processed in chunks, where each chunk fills one vector register. The number of chunks, $N_{chunks}$, is:\n$$N_{chunks} = \\frac{N}{V} = \\frac{2^{20}}{2^7} = 2^{13} = 8192$$\n\nNext, we analyze the latency to reduce a single vector register's worth of data down to a single scalar value for each strategy. Let this be $T_{reg}$.\n\n**Strategy A: Within-lane reduction first**\n\n1.  **Within-lane reduction:** For each of the $L$ lanes, we reduce $W$ elements to a single element using a binary tree of horizontal adds (HADD).\n    -   Number of stages: $\\lceil \\log_2(W) \\rceil = \\lceil \\log_2(16) \\rceil = \\lceil 4 \\rceil = 4$.\n    -   Latency of this phase: $(\\lceil \\log_2(W) \\rceil) \\times h = 4 \\times 3 = 12$ cycles.\n\n2.  **Across-lane reduction:** After the first phase, we have $L$ partial sums, one in each lane. These are reduced to a single scalar. This requires a binary tree of lane-crossing shuffles (SHFL) to align data, followed by vector adds (VADD).\n    -   Number of stages: $\\lceil \\log_2(L) \\rceil = \\lceil \\log_2(8) \\rceil = \\lceil 3 \\rceil = 3$.\n    -   Latency per stage: $s + a = 5 + 4 = 9$ cycles.\n    -   Latency of this phase: $(\\lceil \\log_2(L) \\rceil) \\times (s + a) = 3 \\times 9 = 27$ cycles.\n\nThe total latency per vector register for Strategy A, $T_A$, is the sum of the latencies of these two sequential phases:\n$$T_A = ((\\lceil \\log_2(W) \\rceil) \\times h) + ((\\lceil \\log_2(L) \\rceil) \\times (s + a)) = 12 + 27 = 39 \\text{ cycles}$$\n\n**Strategy B: Across-lane reduction first**\n\n1.  **Across-lane reduction:** We first reduce across the $L$ lanes. In each stage, lanes are paired using SHFL, and their $W$ corresponding elements are added using VADD. This process continues until all results are accumulated into a single lane.\n    -   Number of stages: $\\lceil \\log_2(L) \\rceil = \\lceil \\log_2(8) \\rceil = 3$.\n    -   Latency per stage: $s + a = 5 + 4 = 9$ cycles.\n    -   Latency of this phase: $(\\lceil \\log_2(L) \\rceil) \\times (s + a) = 3 \\times 9 = 27$ cycles.\n\n2.  **Within-lane reduction:** After the first phase, one lane contains $W$ partial sums. These are reduced to a single scalar using a binary tree of HADD operations.\n    -   Number of stages: $\\lceil \\log_2(W) \\rceil = \\lceil \\log_2(16) \\rceil = 4$.\n    -   Latency of this phase: $(\\lceil \\log_2(W) \\rceil) \\times h = 4 \\times 3 = 12$ cycles.\n\nThe total latency per vector register for Strategy B, $T_B$, is the sum of the latencies of these two sequential phases:\n$$T_B = ((\\lceil \\log_2(L) \\rceil) \\times (s + a)) + ((\\lceil \\log_2(W) \\rceil) \\times h) = 27 + 12 = 39 \\text{ cycles}$$\n\n**Comparison of Strategies**\nThe analysis shows that $T_A = T_B = 39$ cycles. The order of operations does not affect the total latency for reducing a single vector register, as the same set of operations must be performed in both cases. Therefore, both strategies are equally fast. We define the latency per register as $T_{reg} = 39$ cycles.\n\n**Total Latency Calculation**\nThe problem states that the scalar accumulation of one result is overlapped with the vector reduction of the next. This describes a pipelined execution model. The stages of the pipeline are the vector reduction (latency $T_{reg}$) and the scalar accumulation (let its latency be $T_{sadd}$). From the provided latencies, it is reasonable to assume the latency of a single scalar floating-point add is the same as the latency of the vector add functional unit, so $T_{sadd} = a = 4$ cycles.\n\nThe total time for a pipeline of $N_{chunks}$ items is the time to process the first item through all stages, plus $(N_{chunks}-1)$ times the pipeline's cycle time. The cycle time is determined by the longest stage. Here, $T_{reg} = 39$ cycles and $T_{sadd} = 4$ cycles, so the vector reduction stage is the bottleneck.\nThe total latency, $T_{total}$, is:\n$$T_{total} = (T_{reg} + T_{sadd}) + (N_{chunks} - 1) \\times \\max(T_{reg}, T_{sadd})$$\n$$T_{total} = (T_{reg} + T_{sadd}) + (N_{chunks} - 1) \\times T_{reg}$$\n$$T_{total} = T_{reg} + T_{sadd} + N_{chunks} \\times T_{reg} - T_{reg} = N_{chunks} \\times T_{reg} + T_{sadd}$$\nThis derived formula is consistent with the problem's literal instruction that \"the overall critical-path latency is the sum of the vector-reduction latency per register across all registers plus one final scalar add latency\".\n\nSubstituting the computed values:\n- $N_{chunks} = 8192$\n- $T_{reg} = 39$ cycles\n- $T_{sadd} = 4$ cycles\n\n$$T_{total} = (8192 \\times 39) + 4$$\n$$T_{total} = (8192 \\times (40 - 1)) + 4 = (8192 \\times 40 - 8192) + 4$$\n$$T_{total} = (327680 - 8192) + 4$$\n$$T_{total} = 319488 + 4 = 319492 \\text{ cycles}$$\n\nThe latency of the faster strategy is $319492$ cycles, as both strategies have identical performance.",
            "answer": "$$\\boxed{319492}$$"
        },
        {
            "introduction": "Real-world algorithms are rarely simple, straight-line code; they often contain conditional logic. In a vector architecture, traditional branching can disrupt the parallel flow of execution and incur significant performance penalties from branch mispredictions. This practice delves into a core trade-off by comparing branching with predication, a technique that uses masks to conditionally execute operations without breaking the control flow. By analyzing a performance model, you will derive the critical threshold at which predication becomes the superior strategy. ",
            "id": "3687643",
            "problem": "A Single Instruction, Multiple Data (SIMD) vector processor executes a data-parallel kernel of the form: for each element, evaluate a condition and, if it is true, perform an arithmetic update. Two implementation strategies are possible:\n\n- Branching: for each element, the processor evaluates the condition and executes a branch. The branch is predicted as not-taken by a static predictor; if the condition is actually true (with probability $p \\in [0,1]$ independently across elements), the branch is mispredicted and incurs a penalty of $m$ cycles. When the branch is taken, the arithmetic update is executed.\n- Predication: the processor eliminates the branch and executes the arithmetic update under a predicate mask. A mask setup overhead amortizes to $c_p$ cycles per element. Masked-off lanes still consume execution resources.\n\nAssume the following per-element cycle costs:\n- Condition evaluation: $c_c$ cycles.\n- Branch overhead (independent of outcome): $c_b$ cycles.\n- Arithmetic update when executed: $c_o$ cycles.\n- Misprediction penalty when the branch outcome is true: $m$ cycles.\n- Predication setup amortized per element: $c_p$ cycles.\n\nAssume $c_o + m > 0$ and $0  c_p + c_o - c_b  c_o + m$ so that the threshold derived below lies strictly in $(0,1)$. Using the linearity of expectation and basic probability, derive an analytic expression for the critical branch probability $p^{\\star}$ such that for all $p \\geq p^{\\star}$, predication has a strictly lower expected per-element cycle cost than branching. Provide your final answer as a single simplified expression for $p^{\\star}$ in terms of $c_b$, $c_c$, $c_o$, $c_p$, and $m$. Express $p^{\\star}$ as a dimensionless probability (no units). Do not round your answer.",
            "solution": "The objective is to find the critical branch probability, denoted as $p^{\\star}$, at which predication becomes a more efficient strategy than branching. This critical point is the threshold where, for any probability $p \\geq p^{\\star}$, the expected per-element cycle cost of predication is strictly lower than that of branching. This threshold is found by determining the probability at which the expected costs of the two strategies are equal.\n\nFirst, we must formulate the expected per-element cycle cost for each strategy. Let $p$ be the probability that the condition is true for any given element.\n\n1.  **Expected Cost of the Branching Strategy ($E[C_{branch}]$)**\n\nThe total cost per element depends on the outcome of the condition.\n- The cost to evaluate the condition, $c_c$, is always incurred.\n- The overhead of the branch instruction itself, $c_b$, is also always incurred.\n\nThere are two cases for the conditional part of the execution:\n-   **Case 1: The condition is false.** This occurs with probability $1-p$. The branch is not taken, and the static \"not-taken\" prediction is correct. No misprediction penalty is paid, and the arithmetic update is skipped. The cost for this case is $c_c + c_b$.\n-   **Case 2: The condition is true.** This occurs with probability $p$. The branch must be taken, which contradicts the static \"not-taken\" prediction. This results in a branch misprediction, incurring a penalty of $m$ cycles. The arithmetic update is also executed, costing $c_o$ cycles. The cost for this case is $c_c + c_b + m + c_o$.\n\nUsing the linearity of expectation, the expected cost per element for the branching strategy is the sum of the costs of these outcomes weighted by their respective probabilities:\n$$E[C_{branch}] = (c_c + c_b)(1-p) + (c_c + c_b + m + c_o)p$$\nExpanding and simplifying the expression:\n$$E[C_{branch}] = c_c + c_b - p(c_c + c_b) + p(c_c + c_b) + p(m + c_o)$$\n$$E[C_{branch}] = c_c + c_b + p(m + c_o)$$\n\n2.  **Expected Cost of the Predication Strategy ($E[C_{pred}]$)**\n\nIn the predication strategy, the control flow is data-independent; there is no branch. The same sequence of operations is performed for every element, and a predicate mask determines which results are committed.\n- The cost to evaluate the condition to generate the mask is $c_c$.\n- The amortized overhead for setting up the predicate mask is $c_p$.\n- The arithmetic update is executed for all elements in the vector, as masked-off lanes still consume execution resources. This costs $c_o$.\n\nSince these operations are performed for every element regardless of the condition's outcome, the cost is deterministic.\n$$C_{pred} = c_c + c_p + c_o$$\nThe expected cost is therefore equal to this deterministic cost:\n$$E[C_{pred}] = c_c + c_p + c_o$$\n\n3.  **Determining the Critical Probability ($p^{\\star}$)**\n\nThe problem asks for the critical probability $p^{\\star}$ such that for $p \\geq p^{\\star}$, predication has a strictly lower cost. The boundary of this region is the point where the two strategies have equal performance. We find $p^{\\star}$ by setting the expected costs equal to each other:\n$$E[C_{pred}] = E[C_{branch}]$$\n$$c_c + c_p + c_o = c_c + c_b + p^{\\star}(m + c_o)$$\nThe term $c_c$ is a common cost to both strategies and cancels out:\n$$c_p + c_o = c_b + p^{\\star}(m + c_o)$$\nNow, we solve for $p^{\\star}$ by isolating it on one side of the equation:\n$$c_p + c_o - c_b = p^{\\star}(m + c_o)$$\nThe problem statement provides the assumption that $c_o + m > 0$. This ensures the term $(m+c_o)$ is strictly positive, allowing us to divide by it without changing any inequalities and without risk of division by zero.\n$$p^{\\star} = \\frac{c_p + c_o - c_b}{m + c_o}$$\nFor any probability $p > p^{\\star}$, the term $p(m+c_o)$ will be greater than $p^{\\star}(m+c_o)$, which implies $E[C_{branch}] > E[C_{pred}]$, satisfying the condition for predication being the superior strategy. The value $p^{\\star}$ is the break-even point. The additional constraints $0  c_p + c_o - c_b  c_o + m$ ensure that $0  p^{\\star}  1$, making it a meaningful probability threshold within the valid range.",
            "answer": "$$\\boxed{\\frac{c_p + c_o - c_b}{m + c_o}}$$"
        }
    ]
}