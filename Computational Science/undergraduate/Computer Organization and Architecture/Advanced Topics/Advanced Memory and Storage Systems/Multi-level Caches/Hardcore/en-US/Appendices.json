{
    "hands_on_practices": [
        {
            "introduction": "The Average Memory Access Time ($AMAT$) is a cornerstone metric for evaluating memory hierarchy performance. This practice bridges the gap between the theoretical $AMAT$ formula and practical performance tuning by exploring the model's sensitivity to its parameters. By using calculus to determine how changes in miss rates at each cache level impact overall performance, we can identify which levels offer the greatest potential for optimization, a skill essential for both hardware designers and performance engineers .",
            "id": "3660682",
            "problem": "A three-level cache hierarchy consists of Level-1 ($L1$), Level-2 ($L2$), and Level-3 ($L3$) caches, followed by main memory. The Average Memory Access Time (AMAT) is defined as the expected latency per memory reference. Let $m_1$, $m_2$, and $m_3$ denote the miss rates at $L1$, $L2$, and $L3$, respectively, where each $m_i$ is the conditional miss probability given that the access has reached level $i$. Let $t_1$, $t_2$, and $t_3$ denote the service time (latency) to access $L1$, $L2$, and $L3$, respectively, and let $t_M$ denote the main memory access latency. Assume that a memory reference incurs $t_1$ to probe $L1$; if $L1$ misses, it incurs $t_2$ to probe $L2$; if $L2$ misses, it incurs $t_3$ to probe $L3$; if $L3$ misses, it incurs $t_M$ to access main memory. Assume no overlap and that latencies add along the miss path.\n\nStarting only from the law of total expectation and the definitions of conditional probability and expected cost per event, do the following:\n\n1. Express the AMAT as an expectation over the mutually exclusive events of hitting or missing at each level, in terms of $t_1$, $t_2$, $t_3$, $t_M$ and the conditional miss rates $m_1$, $m_2$, $m_3$. Then, symbolically derive the sensitivity of AMAT to each $m_i$ by computing the partial derivatives $\\frac{\\partial\\, AMAT}{\\partial m_1}$, $\\frac{\\partial\\, AMAT}{\\partial m_2}$, and $\\frac{\\partial\\, AMAT}{\\partial m_3}$.\n\n2. Suppose you have Hardware Performance Counters (HPC) that report total cycles $C$, total memory references $N$, and per-level misses $M_1$, $M_2$, $M_3$. Describe an empirically grounded procedure to estimate each partial derivative $\\frac{\\partial\\, AMAT}{\\partial m_i}$ near an operating point by designing controlled perturbation experiments that adjust $m_i$ while keeping other $m_j$ approximately constant. Your design must explain how to isolate changes in $m_1$, $m_2$, and $m_3$ using workload working sets relative to $L1$, $L2$, and $L3$ capacities, and how to use HPC-derived estimates $\\hat{AMAT} = \\frac{C}{N}$ and $\\hat{m}_i$ from counters to approximate each derivative via small finite differences.\n\n3. For a system with $t_1 = 1$ cycles, $t_2 = 12$ cycles, $t_3 = 35$ cycles, $t_M = 220$ cycles, $m_1 = 0.07$, $m_2 = 0.22$, and $m_3 = 0.40$, compute the numerical value of $\\frac{\\partial\\, AMAT}{\\partial m_2}$ at this operating point. Express the final answer in cycles and round your answer to $4$ significant figures.",
            "solution": "The problem statement is scientifically grounded, well-posed, and all necessary parameters and definitions are provided. It is consistent with established principles of computer architecture and performance analysis. Therefore, the problem is deemed valid and a full solution follows.\n\nThe solution is presented in three parts as requested.\n\n### Part 1: AMAT Derivation and Sensitivity Analysis\n\nThe Average Memory Access Time (AMAT) is the expectation of the total time, $T$, required to service a memory reference. We can derive an expression for AMAT using the law of total expectation, considering the costs incurred at each level of the memory hierarchy. The model specifies a sequential, non-overlapped access pattern.\n\nEvery memory reference must first access the Level-$1$ ($L1$) cache. This takes a time of $t_1$.\nA fraction of these references, given by the conditional $L1$ miss rate $m_1$, will miss in $L1$ and must then proceed to access the Level-$2$ ($L2$) cache. The additional time incurred for an $L2$ access is $t_2$.\nA fraction of the references that reach $L2$, given by the conditional $L2$ miss rate $m_2$, will also miss in $L2$. The absolute fraction of total references that miss in both $L1$ and $L2$ is the product of the probabilities, $m_1 m_2$. These references proceed to access the Level-$3$ ($L3$) cache, incurring an additional time of $t_3$.\nFinally, a fraction of references reaching $L3$, given by the conditional $L3$ miss rate $m_3$, will miss. The absolute fraction of total references that miss in all three cache levels is $m_1 m_2 m_3$. These references must access main memory, incurring a final additional penalty of $t_M$.\n\nSumming these costs, weighted by the probability of incurring them, gives the AMAT equation:\n$$AMAT = t_1 + m_1 t_2 + (m_1 m_2) t_3 + (m_1 m_2 m_3) t_M$$\n\nThis expression represents the expected cost, starting with the base cost to access $L1$ and adding the expected penalties for misses at each successive level.\n\nTo find the sensitivity of AMAT to each conditional miss rate $m_i$, we compute the partial derivatives of the AMAT expression with respect to $m_1$, $m_2$, and $m_3$.\n\n**Sensitivity to $m_1$:**\nWe treat $m_2$ and $m_3$ as constants and differentiate with respect to $m_1$:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_1} = \\frac{\\partial}{\\partial m_1} (t_1 + m_1 t_2 + m_1 m_2 t_3 + m_1 m_2 m_3 t_M) $$\n$$ \\frac{\\partial\\, AMAT}{\\partial m_1} = 0 + t_2 + m_2 t_3 + m_2 m_3 t_M $$\n$$ \\frac{\\partial\\, AMAT}{\\partial m_1} = t_2 + m_2(t_3 + m_3 t_M) $$\nThis derivative represents the change in AMAT for a small change in the $L1$ miss rate. It is the full miss penalty for an $L1$ miss, which includes the time to access $L2$ ($t_2$) plus the expected miss penalty of the lower levels of the hierarchy.\n\n**Sensitivity to $m_2$:**\nWe treat $m_1$ and $m_3$ as constants and differentiate with respect to $m_2$:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} = \\frac{\\partial}{\\partial m_2} (t_1 + m_1 t_2 + m_1 m_2 t_3 + m_1 m_2 m_3 t_M) $$\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} = 0 + 0 + m_1 t_3 + m_1 m_3 t_M $$\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} = m_1 (t_3 + m_3 t_M) $$\nThis derivative is scaled by $m_1$ because any change in $m_2$ only affects the fraction of accesses that have already missed in $L1$. The term $(t_3 + m_3 t_M)$ is the miss penalty for an $L2$ miss.\n\n**Sensitivity to $m_3$:**\nWe treat $m_1$ and $m_2$ as constants and differentiate with respect to $m_3$:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_3} = \\frac{\\partial}{\\partial m_3} (t_1 + m_1 t_2 + m_1 m_2 t_3 + m_1 m_2 m_3 t_M) $$\n$$ \\frac{\\partial\\, AMAT}{\\partial m_3} = 0 + 0 + 0 + m_1 m_2 t_M $$\n$$ \\frac{\\partial\\, AMAT}{\\partial m_3} = m_1 m_2 t_M $$\nThis derivative is scaled by the factor $m_1 m_2$ because a change in $m_3$ is only relevant for accesses that have already missed in both $L1$ and $L2$. The term $t_M$ is the miss penalty for an $L3$ miss.\n\n### Part 2: Empirical Estimation Procedure\n\nTo empirically estimate the partial derivatives $\\frac{\\partial\\, AMAT}{\\partial m_i}$, we can design controlled experiments that perturb each $m_i$ individually and measure the resulting change in AMAT. This procedure uses Hardware Performance Counters (HPCs) and the finite difference method.\n\nFirst, we establish the relationship between the model parameters and the observables from HPCs:\n-   Total memory references: $N$\n-   Total execution cycles for these references: $C$\n-   $L1$ misses: $M_1$\n-   $L2$ misses: $M_2$\n-   $L3$ misses: $M_3$\n\nFrom these counters, we can estimate the parameters of our model near an operating point:\n-   Estimated AMAT: $\\hat{AMAT} = \\frac{C}{N}$\n-   Estimated conditional $L1$ miss rate: $\\hat{m}_1 = \\frac{M_1}{N}$\n-   Estimated conditional $L2$ miss rate: $\\hat{m}_2 = \\frac{M_2}{M_1}$ (for $M_1 > 0$)\n-   Estimated conditional $L3$ miss rate: $\\hat{m}_3 = \\frac{M_3}{M_2}$ (for $M_2 > 0$)\n\nThe experimental procedure to estimate $\\frac{\\partial\\, AMAT}{\\partial m_i}$ is as follows:\n\n**General Procedure:**\n$1$. **Isolate the Target Variable:** To estimate the sensitivity with respect to a specific miss rate, $m_i$, we must devise a method to change $m_i$ while keeping the other miss rates, $m_j$ (where $j \\neq i$), as constant as possible. This is achieved by carefully controlling the working set size (WSS) of a test workload relative to the cache capacities $S_1  S_2  S_3$.\n\n$2$. **Design Perturbation Workloads:** For each target $m_i$, two workloads are created: a baseline workload ($W_a$) and a perturbed workload ($W_b$).\n    -   To perturb $m_1$: Design $W_a$ with WSS $ S_1$ and $W_b$ with WSS $> S_1$ but WSS $\\ll S_2$.\n    -   To perturb $m_2$: Design $W_a$ with WSS $> S_1$ but WSS $ S_2$, and $W_b$ with WSS $> S_2$ but WSS $\\ll S_3$.\n    -   To perturb $m_3$: Design $W_a$ with WSS $> S_2$ but WSS $ S_3$, and $W_b$ with WSS $> S_3$.\n\n$3$. **Data Collection:** Execute both workloads ($W_a$ and $W_b$) on the target system and collect the HPC data ($C, N, M_1, M_2, M_3$) for each run. Let the collected data for the baseline run be ($C_a, N_a, M_{1a}, M_{2a}, M_{3a}$) and for the perturbed run be ($C_b, N_b, M_{1b}, M_{2b}, M_{3b}$).\n\n$4$. **Calculate Empirical Parameters:** For each run $k \\in \\{a, b\\}$, calculate the estimated parameters $\\hat{AMAT}_k$, $\\hat{m}_{1,k}$, $\\hat{m}_{2,k}$, and $\\hat{m}_{3,k}$ using the formulas defined above.\n\n$5$. **Approximate the Derivative:** Use the finite difference approximation for the partial derivative. For example, to estimate the sensitivity to $m_2$:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} \\approx \\frac{\\Delta \\hat{AMAT}}{\\Delta \\hat{m}_2} = \\frac{\\hat{AMAT}_b - \\hat{AMAT}_a}{\\hat{m}_{2,b} - \\hat{m}_{2,a}} $$\n\n$6$. **Validation:** A crucial step is to verify the quality of the experiment. The changes in the non-target miss rates, $|\\hat{m}_{j,b} - \\hat{m}_{j,a}|$ for $j \\neq i$, should be significantly smaller than the change in the target miss rate, $|\\hat{m}_{i,b} - \\hat{m}_{i,a}|$. If this condition is not met, the experimental setup (e.g., workload WSS) must be refined to better isolate the variable of interest.\n\nThis systematic procedure allows for an empirically grounded estimation of how sensitive the system's performance (AMAT) is to the efficiency of each level of the cache hierarchy.\n\n### Part 3: Numerical Calculation\n\nWe are asked to compute the numerical value of $\\frac{\\partial\\, AMAT}{\\partial m_2}$ at a specific operating point. The formula derived in Part 1 is:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} = m_1 (t_3 + m_3 t_M) $$\n\nThe given values are:\n-   $t_1 = 1$ cycle\n-   $t_2 = 12$ cycles\n-   $t_3 = 35$ cycles\n-   $t_M = 220$ cycles\n-   $m_1 = 0.07$\n-   $m_2 = 0.22$\n-   $m_3 = 0.40$\n\nSubstituting the relevant values into the expression for the partial derivative:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} = 0.07 \\times (35 + 0.40 \\times 220) $$\nFirst, we calculate the term inside the parentheses, which represents the $L2$ miss penalty:\n$$ 35 + 0.40 \\times 220 = 35 + 88 = 123 $$\nNow, we multiply by the $L1$ miss rate:\n$$ \\frac{\\partial\\, AMAT}{\\partial m_2} = 0.07 \\times 123 = 8.61 $$\nThe result is $8.61$ cycles. The problem requires the answer to be rounded to $4$ significant figures. Therefore, the value is $8.610$. This value signifies that for a small increase in the $L2$ conditional miss rate of, for instance, $0.01$ (i.e., $1\\%$), the AMAT would increase by approximately $0.01 \\times 8.610 = 0.0861$ cycles.",
            "answer": "$$\n\\boxed{8.610}\n$$"
        },
        {
            "introduction": "While the $AMAT$ formula provides an expected value, real-world performance can be dominated by worst-case scenarios. This practice examines a critical failure mode in cache design known as thrashing, which occurs when multiple memory streams contend for the same cache sets. By analyzing a microbenchmark where instruction and data accesses conflict within a unified L1 cache, you will quantify how insufficient associativity can lead to a catastrophic increase in miss rates and pipeline stalls, providing a concrete lesson in the importance of workload-aware cache organization .",
            "id": "3660665",
            "problem": "A single-threaded microbenchmark executes on a processor with a unified Level 1 cache (L1) and a multi-level memory hierarchy. The unified Level 1 cache has capacity $64 \\,\\text{KiB}$, associativity $4$ (four-way set associative), and cache line size $64 \\,\\text{B}$. The Level 2 cache (L2) is unified, has capacity $512 \\,\\text{KiB}$, associativity $8$, and line size $64 \\,\\text{B}$. The Level 3 cache (L3) is unified, has capacity $8 \\,\\text{MiB}$, associativity $16$, and line size $64 \\,\\text{B}$. The unified Level 1 cache is physically indexed and physically tagged with the set index formed from address bits $b_{6..13}$.\n\nThe microbenchmark places a large code section and its read-only data array contiguously in memory and aligned on $64 \\,\\text{B}$. The code section size is $96 \\,\\text{KiB}$, and the data array size is $48 \\,\\text{KiB}$. In each iteration of the microbenchmark:\n- The instruction stream sequentially executes through the entire $96 \\,\\text{KiB}$ code section once, fetching instructions at a constant rate with spatial locality that touches each $64 \\,\\text{B}$ line exactly once.\n- The data access pattern sequentially loads one $8 \\,\\text{B}$ element per $64 \\,\\text{B}$ line across the entire $48 \\,\\text{KiB}$ data array once, producing one demand load per data line.\n\nAssume the following:\n- The placement of the code and data is such that their addresses uniformly span the same set indices in the unified Level 1 cache, i.e., both streams distribute uniformly across all Level 1 sets and contend for the same sets because they are contiguous and aligned, and the index bits $b_{6..13}$ select $256$ sets for the Level 1 cache.\n- Replacement is true Least Recently Used (LRU).\n- The Level 2 cache can hold the entire working set from both streams without eviction during an iteration, and the Level 2 hit latency for refilling a Level 1 line is $12$ cycles. Ignore Level 3 and main memory for this scenario.\n- Misses from the unified Level 1 cache do not overlap; i.e., each Level 1 miss contributes additively to pipeline stall cycles, and the processor cannot hide these penalties.\n\nStarting from definitions of miss rate and associativity constraints, determine the steady-state Level 1 miss rates for instruction fetches and data loads, denoted $m_{I1}$ and $m_{D1}$, respectively. Then compute the total pipeline stall cycles per iteration due to Level 1 misses from both streams, assuming each Level 1 miss is serviced by the Level 2 hit latency of $12$ cycles. Express the final stall count in cycles per iteration, and round this stall count to four significant figures. The final answer must be a single row matrix containing $m_{I1}$, $m_{D1}$, and the rounded stall cycles per iteration, in that order.",
            "solution": "The user wants me to solve a problem about cache performance.\n\n### Step 1: Problem Validation\n\nI will first validate the problem statement.\n\n#### Extracted Givens:\n- **Processor:** Single-threaded.\n- **L1 Cache (unified):**\n  - Capacity: $C_1 = 64 \\,\\text{KiB}$\n  - Associativity: $A_1 = 4$ (four-way set associative)\n  - Line Size: $L = 64 \\,\\text{B}$\n  - Indexing/Tagging: Physically indexed, physically tagged.\n  - Set Index: Address bits $b_{6..13}$.\n- **L2 Cache (unified):**\n  - Capacity: $C_2 = 512 \\,\\text{KiB}$\n  - Associativity: $A_2 = 8$\n  - Line Size: $L = 64 \\,\\text{B}$\n- **L3 Cache (unified):**\n  - Capacity: $C_3 = 8 \\,\\text{MiB}$\n  - Associativity: $A_3 = 16$\n  - Line Size: $L = 64 \\,\\text{B}$\n- **Microbenchmark Workload:**\n  - Code Section Size: $W_I = 96 \\,\\text{KiB}$\n  - Data Array Size: $W_D = 48 \\,\\text{KiB}$\n  - Placement: Code and data are contiguous and aligned on a $64 \\,\\text{B}$ boundary.\n- **Access Patterns (per iteration):**\n  - **Instructions:** Sequential fetch through the entire $96 \\,\\text{KiB}$ code section, touching each $64 \\,\\text{B}$ line exactly once.\n  - **Data:** Sequential load of one $8 \\,\\text{B}$ element from each $64 \\,\\text{B}$ line across the entire $48 \\,\\text{KiB}$ data array, resulting in one demand load per data line.\n- **Assumptions:**\n  1. Code and data addresses uniformly span the same L1 set indices, causing contention. The index bits $b_{6..13}$ correspond to $256$ sets.\n  2. Replacement Policy: True Least Recently Used (LRU).\n  3. L2 cache is large enough to hold the entire working set ($96 \\,\\text{KiB} + 48 \\,\\text{KiB} = 144 \\,\\text{KiB}  512 \\,\\text{KiB}$).\n  4. L1 miss penalty (serviced by L2 hit): $T_{penalty} = 12$ cycles.\n  5. L1 misses are non-overlapping and their penalties are additive.\n- **Required Outputs:**\n  1. Steady-state L1 instruction miss rate, $m_{I1}$.\n  2. Steady-state L1 data miss rate, $m_{D1}$.\n  3. Total pipeline stall cycles per iteration, rounded to four significant figures.\n\n#### Validation Using Extracted Givens:\n1.  **Scientific or Factual Unsoundness:** The problem is scientifically sound. The cache parameters are realistic. Let's verify the L1 cache structure. The offset bits are for a $64 \\,\\text{B}$ line, so $2^6 = 64$, which means address bits $b_{0..5}$ are the byte offset. The index is given by bits $b_{6..13}$, which is $13 - 6 + 1 = 8$ bits. The number of sets is $S_1 = 2^8 = 256$. The total L1 capacity is $C_1 = S_1 \\times A_1 \\times L = 256 \\times 4 \\times 64 \\,\\text{B} = 1024 \\times 64 \\,\\text{B} = 65536 \\,\\text{B} = 64 \\,\\text{KiB}$. This is consistent with the given capacity. All premises are based on standard principles of computer organization and architecture.\n2.  **Non-Formalizable or Irrelevant:** The problem is formal and directly relevant to the topic of multi-level caches.\n3.  **Incomplete or Contradictory Setup:** The problem is well-defined and self-contained. All necessary data (sizes, latencies, policies, access patterns) are provided. There are no contradictions.\n4.  **Unrealistic or Infeasible:** The parameters and scenario are plausible for a microbenchmark designed to test cache contention.\n5.  **Ill-Posed or Poorly Structured:** The problem is well-posed, with clear assumptions that lead to a unique, derivable solution.\n6.  **Pseudo-Profound, Trivial, or Tautological:** The problem is not trivial; it requires a careful analysis of cache contention under an LRU policy.\n7.  **Outside Scientific Verifiability:** The results can be verified by calculation or simulation.\n\n#### Verdict:\nThe problem is valid. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe problem requires us to determine the L1 miss rates for instruction and data streams and the total resulting pipeline stalls. The core of the problem lies in analyzing the contention between the instruction and data streams in the unified L1 cache.\n\nFirst, let's analyze the L1 cache structure and the memory access patterns relative to it.\nThe L1 cache has $S_1 = 256$ sets and an associativity of $A_1 = 4$. This means each set can hold $4$ unique cache lines. The replacement policy is LRU.\n\nThe total working set consists of a $W_I = 96 \\,\\text{KiB}$ code section and a $W_D = 48 \\,\\text{KiB}$ data array. The cache line size is $L = 64 \\,\\text{B}$.\nThe number of cache lines for the instruction stream is:\n$$N_I = \\frac{W_I}{L} = \\frac{96 \\times 2^{10} \\,\\text{B}}{64 \\,\\text{B}} = \\frac{98304 \\,\\text{B}}{64 \\,\\text{B}} = 1536 \\text{ lines}$$\nThe number of cache lines for the data stream is:\n$$N_D = \\frac{W_D}{L} = \\frac{48 \\times 2^{10} \\,\\text{B}}{64 \\,\\text{B}} = \\frac{49152 \\,\\text{B}}{64 \\,\\text{B}} = 768 \\text{ lines}$$\n\nThe problem states that both streams are distributed uniformly across all $S_1 = 256$ L1 sets. We can therefore analyze the behavior of a single representative set.\nNumber of instruction lines mapping to each set:\n$$N_{I,set} = \\frac{N_I}{S_1} = \\frac{1536}{256} = 6$$\nNumber of data lines mapping to each set:\n$$N_{D,set} = \\frac{N_D}{S_1} = \\frac{768}{256} = 3$$\n\nThe total number of unique memory lines that map to any given set is:\n$$N_{total,set} = N_{I,set} + N_{D,set} = 6 + 3 = 9$$\n\nThe L1 cache is $A_1 = 4$-way set associative. However, for each set, there are $9$ unique lines from the combined working set that must be accessed in each iteration. Since the number of competing lines ($9$) is greater than the set associativity ($4$), conflict misses are inevitable.\n\nThe access pattern within an iteration is sequential for both streams, and each line is accessed exactly once. Let's consider the sequence of accesses to a representative set. The combined stream consists of accesses to $6$ unique instruction lines and $3$ unique data lines. Since no line is accessed more than once within an iteration, the sequence of accesses for a given set is a permutation of $9$ unique line addresses.\n\nUnder the LRU replacement policy, when a miss occurs in a full set, the line that was least recently used is evicted. Let's trace the state of a set.\n1. The first access to a line in the set is a compulsory miss. The line is loaded.\n2. The second access to a different line is also a compulsory miss. It is loaded.\n3. This continues for the first $4$ unique line accesses. The set is now full. Let the contents be $\\{L_1, L_2, L_3, L_4\\}$.\n4. The fifth unique line access, for line $L_5$, will be a miss. Since the set is full, the LRU policy will evict the least recently used line (which is $L_1$). The set now contains $\\{L_2, L_3, L_4, L_5\\}$.\n5. This eviction process continues for every subsequent unique access. The $k$-th unique access (for $k > 4$) will evict the $(k-4)$-th line that was brought in.\n\nSince the microbenchmark accesses $9$ unique lines per set in each iteration, and the associativity is only $4$, every access will find that the required line is not in the cache. The first $4$ accesses are compulsory misses that fill the set. Every subsequent access from the $5^{th}$ to the $9^{th}$ is a conflict miss, as it will have displaced a line that was brought in earlier. Because no line is re-referenced within the same iteration, there is no opportunity for a hit. At the end of the iteration, the set will contain the last $4$ lines that were accessed. At the start of the next iteration, the program will once again begin accessing lines starting from the first one, which was evicted long ago. Therefore, in the steady state, every single access to a unique line will result in a miss.\n\nThe problem asks for the miss rate, defined as the ratio of misses to accesses. The \"accesses\" in this context are the demand requests for each cache line.\nFor the instruction stream, there is one access for each of the $N_I = 1536$ lines. All of these will miss.\nNumber of instruction misses = $1536$.\nNumber of instruction accesses = $1536$.\nThe L1 instruction miss rate is:\n$$m_{I1} = \\frac{\\text{Number of I-misses}}{\\text{Number of I-accesses}} = \\frac{1536}{1536} = 1$$\n\nFor the data stream, there is one access for each of the $N_D = 768$ lines. All of these will miss.\nNumber of data misses = $768$.\nNumber of data accesses = $768$.\nThe L1 data miss rate is:\n$$m_{D1} = \\frac{\\text{Number of D-misses}}{\\text{Number of D-accesses}} = \\frac{768}{768} = 1$$\n\nNow, we compute the total pipeline stall cycles per iteration.\nTotal number of L1 misses per iteration is the sum of instruction and data misses:\n$$\\text{Total Misses} = N_I + N_D = 1536 + 768 = 2304$$\nEach miss incurs a penalty of $T_{penalty} = 12$ cycles, and these penalties are additive.\nTotal stall cycles per iteration are:\n$$\\text{Stall Cycles} = (\\text{Total Misses}) \\times T_{penalty} = 2304 \\times 12$$\n$$\\text{Stall Cycles} = 27648$$\nThe problem requires this value to be rounded to four significant figures. The number $27648$ has five significant figures. To round it to four, we examine the fifth digit, which is $8$. Since $8 \\ge 5$, we round up the fourth digit ($4$) to $5$.\n$$\\text{Rounded Stall Cycles} = 27650$$\n\nThe final answer consists of the three computed values: $m_{I1}$, $m_{D1}$, and the rounded stall cycles.\n$m_{I1} = 1$\n$m_{D1} = 1$\nRounded Stall Cycles = $27650$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  27650\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Cache performance is not a static quantity, but a dynamic process that evolves over time, especially in modern multi-tasking operating systems. This final practice moves from steady-state analysis to modeling the transient behavior of caches during the \"warm-up\" period that follows an OS context switch. By quantifying the performance penalty incurred as a new process populates the cache, you will learn how to connect low-level hardware behavior to system-level scheduling policies and understand the trade-offs in amortizing these warm-up costs .",
            "id": "3660694",
            "problem": "A single core executes a time-sharing workload under an Operating System (OS) with context switches between independent processes. The core has a two-level private cache hierarchy in front of a large inclusive Last-Level Cache (LLC): a Level-1 (L1) cache and a Level-2 (L2) cache. For the purpose of timing, treat the LLC/memory service beyond L2 as a single stage with a fixed additional latency. The following properties hold for a representative process when it is scheduled to run after being descheduled long enough that its private caches contain no useful lines for its own working set.\n\n- L1 hit latency is $t_1 = 1\\,\\text{ns}$.\n- On an L1 miss, the additional service time from L2 (if L2 hits) is $t_2 = 4\\,\\text{ns}$.\n- On an L2 miss, the additional service time from the LLC/memory is $t_M = 60\\,\\text{ns}$.\n- In steady state for this process (after warm-up), the L1 and L2 miss rates are $m_1^{\\star} = 0.04$ and $m_2^{\\star} = 0.15$, respectively.\n- Immediately after a context switch into this process, the effective miss rates start at $m_1(0) = 1$ and $m_2(0) = 1$ and then warm toward $m_1^{\\star}$ and $m_2^{\\star}$.\n- Assume an Independent Reference Model (IRM) with Poisson novelty for the stream of distinct cache-line touches at each level. Under this assumption, the fraction of not-yet-touched lines at level $i \\in \\{1,2\\}$ decays exponentially with rate $\\alpha_i$. For this workload, the effective novelty rates are $\\alpha_1 = 5 \\times 10^{6}\\,\\text{s}^{-1}$ and $\\alpha_2 = 1 \\times 10^{6}\\,\\text{s}^{-1}$.\n\nUsing only the core definitions of cache hit, cache miss, miss rate, and the notion that Poisson arrivals imply exponential decay of the fraction of not-yet-seen items, carry out the following.\n\n1) Define a physically justified functional form for $m_1(t)$ and $m_2(t)$ that is consistent with the given assumptions, and from it determine the smallest warm-up time $\\tau$ such that both $|m_1(t) - m_1^{\\star}| \\le 0.05\\, m_1^{\\star}$ and $|m_2(t) - m_2^{\\star}| \\le 0.05\\, m_2^{\\star}$ hold for all $t \\ge \\tau$.\n\n2) Let Average Memory Access Time (AMAT) at time $t$ be the expected access latency constructed from $t_1$, $t_2$, $t_M$, and the instantaneous miss rates $m_1(t)$ and $m_2(t)$. The OS uses fixed-length time slices of length $T$. To amortize warm-up cost, it requires that the time-average AMAT over the first slice after a context switch exceeds the steady-state AMAT by no more than $0.05$ of the steady-state AMAT. Compute the minimal $T$ that satisfies this requirement.\n\nProvide your final answers as two numbers: the warm-up time $\\tau$ and the minimal time-slice length $T$ that achieves the amortization requirement. Round your answers to three significant figures and express both times in $\\mu\\text{s}$.",
            "solution": "The problem is valid as it is scientifically grounded in standard cache performance models, well-posed with sufficient and consistent data, and objectively stated.\n\n### Part 1: Warm-up time $\\tau$\n\nThe problem states that after a context switch, the private caches (L1, L2) are cold. The miss rates start at their maximum possible value and decay towards a steady-state value. The decay process is governed by an Independent Reference Model (IRM) with Poisson novelty, where the fraction of not-yet-touched cache lines at level $i$ decays exponentially with a rate $\\alpha_i$.\n\nLet's define a functional form for the time-dependent miss rate, $m_i(t)$, for cache level $i \\in \\{1, 2\\}$. The total miss rate at any time is the sum of misses on \"new\" (never-before-seen) lines and misses on \"old\" (previously seen) lines, weighted by their respective probabilities.\n$m_i(t) = P(\\text{miss} | \\text{new line}) P(\\text{new line}) + P(\\text{miss} | \\text{old line}) P(\\text{old line})$\n\nAn access to a new line is always a miss (a compulsory miss), so $P(\\text{miss} | \\text{new line}) = 1$. The problem states that the fraction of not-yet-touched lines, which corresponds to $P(\\text{new line})$, decays exponentially:\n$P(\\text{new line}) = \\exp(-\\alpha_i t)$\nConsequently, $P(\\text{old line}) = 1 - P(\\text{new line}) = 1 - \\exp(-\\alpha_i t)$.\n\nFor old lines, the cache has been populated with the process's working set. The miss rate in this scenario is the steady-state miss rate, $m_i^{\\star}$, which accounts for capacity and conflict misses.\n$P(\\text{miss} | \\text{old line}) = m_i^{\\star}$\n\nCombining these gives the functional form for the miss rate:\n$m_i(t) = 1 \\cdot \\exp(-\\alpha_i t) + m_i^{\\star} \\cdot (1 - \\exp(-\\alpha_i t))$\n$m_i(t) = \\exp(-\\alpha_i t) + m_i^{\\star} - m_i^{\\star} \\exp(-\\alpha_i t)$\n$m_i(t) = m_i^{\\star} + (1 - m_i^{\\star}) \\exp(-\\alpha_i t)$\n\nWe can verify this model with the given initial conditions. At $t=0$, $m_i(0) = m_i^{\\star} + (1-m_i^{\\star})\\exp(0) = m_i^{\\star} + 1 - m_i^{\\star} = 1$, which matches the given $m_1(0)=1$ and $m_2(0)=1$. As $t \\to \\infty$, $m_i(t) \\to m_i^{\\star}$, which is the correct steady-state behavior.\n\nThe warm-up time $\\tau$ is the smallest time such that for all $t \\ge \\tau$, the miss rates are within $5\\%$ of their steady-state values.\nFor L1: $|m_1(\\tau) - m_1^{\\star}| \\le 0.05 \\, m_1^{\\star}$\nFor L2: $|m_2(\\tau) - m_2^{\\star}| \\le 0.05 \\, m_2^{\\star}$\n\nSince $m_i(t) \\ge m_i^{\\star}$ for $t \\ge 0$, we can drop the absolute value.\nFor L1, let $\\tau_1$ be the time:\n$m_1(\\tau_1) - m_1^{\\star} = (1 - m_1^{\\star}) \\exp(-\\alpha_1 \\tau_1) \\le 0.05 \\, m_1^{\\star}$\n$\\exp(-\\alpha_1 \\tau_1) \\le \\frac{0.05 \\, m_1^{\\star}}{1 - m_1^{\\star}}$\n$-\\alpha_1 \\tau_1 \\le \\ln\\left(\\frac{0.05 \\, m_1^{\\star}}{1 - m_1^{\\star}}\\right)$\n$\\tau_1 \\ge \\frac{1}{\\alpha_1} \\ln\\left(\\frac{1 - m_1^{\\star}}{0.05 \\, m_1^{\\star}}\\right)$\n\nSimilarly for L2, let $\\tau_2$ be the time:\n$\\tau_2 \\ge \\frac{1}{\\alpha_2} \\ln\\left(\\frac{1 - m_2^{\\star}}{0.05 \\, m_2^{\\star}}\\right)$\n\nThe overall warm-up time $\\tau$ must satisfy both conditions, so $\\tau = \\max(\\tau_1, \\tau_2)$.\n\nUsing the given values:\n$m_1^{\\star} = 0.04$, $\\alpha_1 = 5 \\times 10^{6}\\,\\text{s}^{-1}$\n$m_2^{\\star} = 0.15$, $\\alpha_2 = 1 \\times 10^{6}\\,\\text{s}^{-1}$\n\n$\\tau_1 = \\frac{1}{5 \\times 10^6} \\ln\\left(\\frac{1 - 0.04}{0.05 \\times 0.04}\\right) = \\frac{1}{5 \\times 10^6} \\ln\\left(\\frac{0.96}{0.002}\\right) = \\frac{\\ln(480)}{5 \\times 10^6} \\approx \\frac{6.173785}{5 \\times 10^6} \\approx 1.23476 \\times 10^{-6}\\,\\text{s}$\n\n$\\tau_2 = \\frac{1}{1 \\times 10^6} \\ln\\left(\\frac{1 - 0.15}{0.05 \\times 0.15}\\right) = \\frac{1}{1 \\times 10^6} \\ln\\left(\\frac{0.85}{0.0075}\\right) = \\frac{\\ln(113.333...)}{1 \\times 10^6} \\approx \\frac{4.73029}{1 \\times 10^6} \\approx 4.73029 \\times 10^{-6}\\,\\text{s}$\n\n$\\tau = \\max(1.23476 \\times 10^{-6}\\,\\text{s}, 4.73029 \\times 10^{-6}\\,\\text{s}) = 4.73029 \\times 10^{-6}\\,\\text{s}$.\nExpressing in microseconds ($\\mu\\text{s}$) and rounding to three significant figures, $\\tau = 4.73\\,\\mu\\text{s}$.\n\n### Part 2: Minimal Time-Slice Length $T$\n\nThe Average Memory Access Time (AMAT) is the expected latency for a memory access. It can be formulated based on the cache hierarchy latencies and miss rates. With hit time $t_1$ and miss rates $m_1(t)$ and $m_2(t)$, we can write AMAT as:\n$\\text{AMAT}(t) = (\\text{L1 Hit Time}) + m_1(t) \\times (\\text{L1 Miss Penalty})$\nThe L1 miss penalty is the additional time to service the miss from lower levels.\n$\\text{L1 Miss Penalty} = (\\text{L2 Service Time}) + m_2(t) \\times (\\text{L2 Miss Penalty})$\nThe problem gives the *additional* service times: $t_2$ for an L2 hit and $t_M$ for an L2 miss (serviced by LLC/memory). This leads to the following expression:\n$\\text{AMAT}(t) = t_1 + m_1(t) (t_2 + m_2(t) t_M) = t_1 + m_1(t) t_2 + m_1(t) m_2(t) t_M$\n\nThe steady-state AMAT, $\\text{AMAT}^{\\star}$, is found by using the steady-state miss rates $m_1^{\\star}$ and $m_2^{\\star}$:\n$\\text{AMAT}^{\\star} = t_1 + m_1^{\\star} t_2 + m_1^{\\star} m_2^{\\star} t_M$\nUsing the given values $t_1=1\\,\\text{ns}$, $t_2=4\\,\\text{ns}$, $t_M=60\\,\\text{ns}$, $m_1^{\\star}=0.04$, $m_2^{\\star}=0.15$:\n$\\text{AMAT}^{\\star} = 1 + (0.04)(4) + (0.04)(0.15)(60) = 1 + 0.16 + (0.006)(60) = 1 + 0.16 + 0.36 = 1.52\\,\\text{ns}$\n\nThe condition is that the time-average AMAT over the interval $[0, T]$ must not exceed the steady-state AMAT by more than $5\\%$:\n$\\frac{1}{T} \\int_0^T \\text{AMAT}(t) \\,dt \\le 1.05 \\, \\text{AMAT}^{\\star}$\n$\\frac{1}{T} \\int_0^T \\text{AMAT}(t) \\,dt \\le \\text{AMAT}^{\\star} + 0.05 \\, \\text{AMAT}^{\\star}$\nSubtracting $\\text{AMAT}^{\\star} = \\frac{1}{T} \\int_0^T \\text{AMAT}^{\\star} \\,dt$ from both sides:\n$\\frac{1}{T} \\int_0^T (\\text{AMAT}(t) - \\text{AMAT}^{\\star}) \\,dt \\le 0.05 \\, \\text{AMAT}^{\\star}$\n\nLet's find the expression for the transient overhead, $\\Delta\\text{AMAT}(t) = \\text{AMAT}(t) - \\text{AMAT}^{\\star}$:\n$\\Delta\\text{AMAT}(t) = t_2(m_1(t) - m_1^{\\star}) + t_M(m_1(t)m_2(t) - m_1^{\\star}m_2^{\\star})$\nUsing $m_i(t) = m_i^{\\star} + (1 - m_i^{\\star}) e^{-\\alpha_i t}$, we find:\n$m_1(t) - m_1^{\\star} = (1 - m_1^{\\star}) e^{-\\alpha_1 t}$\n$m_1(t)m_2(t) - m_1^{\\star}m_2^{\\star} = m_1^{\\star}(1 - m_2^{\\star})e^{-\\alpha_2 t} + m_2^{\\star}(1-m_1^{\\star})e^{-\\alpha_1 t} + (1-m_1^{\\star})(1-m_2^{\\star})e^{-(\\alpha_1+\\alpha_2)t}$\nCombining these gives:\n$\\Delta\\text{AMAT}(t) = e^{-\\alpha_1 t} (1-m_1^{\\star})(t_2 + m_2^{\\star}t_M) + e^{-\\alpha_2 t} m_1^{\\star}(1-m_2^{\\star})t_M + e^{-(\\alpha_1+\\alpha_2)t} (1-m_1^{\\star})(1-m_2^{\\star})t_M$\n\nLet $C_1 = (1-m_1^{\\star})(t_2 + m_2^{\\star}t_M)$, $C_2 = m_1^{\\star}(1-m_2^{\\star})t_M$, and $C_{12} = (1-m_1^{\\star})(1-m_2^{\\star})t_M$.\nThe condition to find the minimal $T$ becomes an equality:\n$\\frac{1}{T} \\int_0^T (C_1 e^{-\\alpha_1 t} + C_2 e^{-\\alpha_2 t} + C_{12} e^{-(\\alpha_1+\\alpha_2)t}) \\,dt = 0.05 \\, \\text{AMAT}^{\\star}$\nIntegrating term by term: $\\int_0^T e^{-at} dt = \\frac{1-e^{-aT}}{a}$.\n$\\frac{1}{T} \\left[ C_1 \\frac{1-e^{-\\alpha_1 T}}{\\alpha_1} + C_2 \\frac{1-e^{-\\alpha_2 T}}{\\alpha_2} + C_{12} \\frac{1-e^{-(\\alpha_1+\\alpha_2)T}}{\\alpha_1+\\alpha_2} \\right] = 0.05 \\, \\text{AMAT}^{\\star}$\n\nThe time constants are $1/\\alpha_1 = 200\\,\\text{ns}$ and $1/\\alpha_2 = 1000\\,\\text{ns}$. The warm-up time $\\tau \\approx 4730\\,\\text{ns}$ is significantly larger than these. We can expect $T$ to be even larger, such that for all relevant $\\alpha$, $\\alpha T \\gg 1$. This implies $e^{-\\alpha T} \\approx 0$. The equation simplifies to:\n$\\frac{1}{T} \\left[ \\frac{C_1}{\\alpha_1} + \\frac{C_2}{\\alpha_2} + \\frac{C_{12}}{\\alpha_1+\\alpha_2} \\right] \\approx 0.05 \\, \\text{AMAT}^{\\star}$\n$T \\approx \\frac{1}{0.05 \\, \\text{AMAT}^{\\star}} \\left( \\frac{C_1}{\\alpha_1} + \\frac{C_2}{\\alpha_2} + \\frac{C_{12}}{\\alpha_1+\\alpha_2} \\right)$\n\nFirst, convert rates to $\\text{ns}^{-1}$:\n$\\alpha_1 = 5 \\times 10^6\\,\\text{s}^{-1} = 5 \\times 10^{-3}\\,\\text{ns}^{-1}$\n$\\alpha_2 = 1 \\times 10^6\\,\\text{s}^{-1} = 1 \\times 10^{-3}\\,\\text{ns}^{-1}$\n$\\alpha_1 + \\alpha_2 = 6 \\times 10^{-3}\\,\\text{ns}^{-1}$\n\nNext, calculate the coefficients $C_i$:\n$C_1 = (1-0.04)(4 + 0.15 \\times 60) = 0.96 \\times (4+9) = 0.96 \\times 13 = 12.48\\,\\text{ns}$\n$C_2 = 0.04(1-0.15) \\times 60 = 0.04 \\times 0.85 \\times 60 = 2.04\\,\\text{ns}$\n$C_{12} = (1-0.04)(1-0.15) \\times 60 = 0.96 \\times 0.85 \\times 60 = 48.96\\,\\text{ns}$\n\nNow calculate the terms in the parenthesis:\n$\\frac{C_1}{\\alpha_1} = \\frac{12.48\\,\\text{ns}}{5 \\times 10^{-3}\\,\\text{ns}^{-1}} = 2496\\,\\text{ns}^2$\n$\\frac{C_2}{\\alpha_2} = \\frac{2.04\\,\\text{ns}}{1 \\times 10^{-3}\\,\\text{ns}^{-1}} = 2040\\,\\text{ns}^2$\n$\\frac{C_{12}}{\\alpha_1+\\alpha_2} = \\frac{48.96\\,\\text{ns}}{6 \\times 10^{-3}\\,\\text{ns}^{-1}} = 8160\\,\\text{ns}^2$\n\nThe sum is $2496 + 2040 + 8160 = 12696\\,\\text{ns}^2$.\nThe right side of the condition is $0.05 \\, \\text{AMAT}^{\\star} = 0.05 \\times 1.52\\,\\text{ns} = 0.076\\,\\text{ns}$.\n\nNow solve for $T$:\n$T \\approx \\frac{12696\\,\\text{ns}^2}{0.076\\,\\text{ns}} = 167052.63...\\,\\text{ns}$\nThis value of $T$ is very large compared to the time constants, confirming the validity of the approximation $e^{-\\alpha T} \\approx 0$.\nExpressing in microseconds and rounding to three significant figures, $T = 167\\,\\mu\\text{s}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.73  167\n\\end{pmatrix}\n}\n$$"
        }
    ]
}