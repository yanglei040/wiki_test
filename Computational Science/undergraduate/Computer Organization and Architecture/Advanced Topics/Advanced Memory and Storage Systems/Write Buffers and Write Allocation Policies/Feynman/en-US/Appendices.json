{
    "hands_on_practices": [
        {
            "introduction": "How a processor handles a write that misses in the cache has profound implications for performance and energy. This first exercise directly compares two common strategies: the write-allocate policy, which fetches a cache line before modifying it, and the non-temporal store policy, which bypasses the cache entirely. By modeling the memory transactions for a large sequential write workload, you will quantify the significant energy savings offered by avoiding unnecessary memory reads, a key optimization for certain algorithms .",
            "id": "3688510",
            "problem": "A single-core Central Processing Unit (CPU) executes a store-heavy loop that writes an array of size $N$ bytes sequentially with aligned $16$-byte stores. The memory hierarchy has a line size of $L$ bytes in the first-level cache and uses a write-back data cache. The loop’s working set is much larger than the cache, so each cache line is touched once and then evicted without reuse. Assume sequential, contiguous writes that fully cover each cache line they touch. The energy model for main memory traffic is linear, given by\n$$E=\\alpha N_{\\text{reads}}+\\beta N_{\\text{writes}},$$\nwhere $N_{\\text{reads}}$ and $N_{\\text{writes}}$ are the counts of full-line memory transactions and $\\alpha$ and $\\beta$ are the dynamic energy costs per memory transaction for a read and a write, respectively. Reads are counted only when the memory system issues a read-for-ownership (RFO) to bring a line into the cache.\n\nConsider two store policies:\n- Write-allocate: on a store miss, the cache allocates the line and performs a Read-For-Ownership (RFO). The line becomes dirty and is written back once, as there is no reuse.\n- Non-temporal stores: stores bypass the cache and are merged in a write-combining buffer so that sequential stores to a line generate a single full-line write to Dynamic Random-Access Memory (DRAM) without any RFO.\n\nLet $N=128$ MiB and $L=64$ bytes. Let $\\alpha=13.7$ nJ per $64$-byte read and $\\beta=17.9$ nJ per $64$-byte write. Starting from core definitions of cache line allocation and memory transaction counting, derive the energy per byte under each policy from $E=\\alpha N_{\\text{reads}}+\\beta N_{\\text{writes}}$, and then compute the fractional energy reduction per byte achieved by non-temporal stores relative to write-allocate, defined as\n$$\\frac{E_{\\text{WA, per-byte}}-E_{\\text{NT, per-byte}}}{E_{\\text{WA, per-byte}}}.$$\nRound your final answer to four significant figures. This fractional reduction is dimensionless; report it as a decimal number.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of computer architecture, specifically memory hierarchy and cache policies. It is well-posed, providing all necessary definitions, variables, and constants ($N$, $L$, $\\alpha$, $\\beta$, and descriptions of write-allocate vs. non-temporal store policies) to derive a unique, meaningful solution. The problem is objective and uses standard terminology.\n\nThe objective is to compute the fractional energy reduction per byte achieved by using non-temporal (NT) stores relative to a write-allocate (WA) policy. This is defined as:\n$$\n\\text{Fractional Reduction} = \\frac{E_{\\text{WA, per-byte}} - E_{\\text{NT, per-byte}}}{E_{\\text{WA, per-byte}}}\n$$\nThe energy per byte for a given policy is the total energy divided by the total number of bytes written, $N$. Let $E_{\\text{WA}}$ and $E_{\\text{NT}}$ be the total energy for the write-allocate and non-temporal policies, respectively. Then $E_{\\text{WA, per-byte}} = \\frac{E_{\\text{WA}}}{N}$ and $E_{\\text{NT, per-byte}} = \\frac{E_{\\text{NT}}}{N}$. Substituting these into the formula gives:\n$$\n\\text{Fractional Reduction} = \\frac{\\frac{E_{\\text{WA}}}{N} - \\frac{E_{\\text{NT}}}{N}}{\\frac{E_{\\text{WA}}}{N}} = \\frac{E_{\\text{WA}} - E_{\\text{NT}}}{E_{\\text{WA}}} = 1 - \\frac{E_{\\text{NT}}}{E_{\\text{WA}}}\n$$\nWe must first determine the total energy for each policy using the given model, $E = \\alpha N_{\\text{reads}} + \\beta N_{\\text{writes}}$. The problem involves sequentially writing an array of size $N$ bytes, with a cache line size of $L$ bytes. The total number of cache lines covered by the array is:\n$$\nN_{\\text{lines}} = \\frac{N}{L}\n$$\nThe problem states that the writes are sequential and fully cover each cache line they touch.\n\nFirst, we analyze the write-allocate (WA) policy.\nThe problem states that on a store miss, the cache allocates the line and performs a Read-For-Ownership (RFO). Since the working set is much larger than the cache and access is sequential without reuse, every access to a new cache line will result in a cache miss. An RFO is a memory read operation to fetch the line from main memory before it can be written to. Therefore, for each of the $N_{\\text{lines}}$ lines, one read transaction is issued.\n$$\nN_{\\text{reads, WA}} = N_{\\text{lines}} = \\frac{N}{L}\n$$\nAfter the RFO, the CPU writes to the line in the cache, marking it as dirty. Since there is no reuse, this dirty line is eventually evicted to make space for a subsequent line. Evicting a dirty line from a write-back cache forces a write transaction to main memory. Thus, each of the $N_{\\text{lines}}$ lines is written back to memory once.\n$$\nN_{\\text{writes, WA}} = N_{\\text{lines}} = \\frac{N}{L}\n$$\nThe total energy for the write-allocate policy is:\n$$\nE_{\\text{WA}} = \\alpha N_{\\text{reads, WA}} + \\beta N_{\\text{writes, WA}} = \\alpha \\frac{N}{L} + \\beta \\frac{N}{L} = (\\alpha + \\beta) \\frac{N}{L}\n$$\n\nNext, we analyze the non-temporal (NT) stores policy.\nThe problem states that non-temporal stores bypass the cache and do not perform any RFOs. This means there are no memory reads associated with the write operations.\n$$\nN_{\\text{reads, NT}} = 0\n$$\nThe stores are merged in a write-combining buffer, and sequential stores to a line generate a single full-line write to memory. Since the entire array of size $N$ is written, the total number of full-line write transactions is equal to the number of lines in the array.\n$$\nN_{\\text{writes, NT}} = N_{\\text{lines}} = \\frac{N}{L}\n$$\nThe total energy for the non-temporal policy is:\n$$\nE_{\\text{NT}} = \\alpha N_{\\text{reads, NT}} + \\beta N_{\\text{writes, NT}} = \\alpha(0) + \\beta \\frac{N}{L} = \\beta \\frac{N}{L}\n$$\n\nNow, we can compute the fractional energy reduction.\n$$\n\\text{Fractional Reduction} = 1 - \\frac{E_{\\text{NT}}}{E_{\\text{WA}}} = 1 - \\frac{\\beta \\frac{N}{L}}{(\\alpha + \\beta) \\frac{N}{L}} = 1 - \\frac{\\beta}{\\alpha + \\beta}\n$$\nThis expression can be simplified:\n$$\n\\text{Fractional Reduction} = \\frac{(\\alpha + \\beta) - \\beta}{\\alpha + \\beta} = \\frac{\\alpha}{\\alpha + \\beta}\n$$\nThe reduction is solely dependent on the relative energy costs of a read and a write transaction, and is independent of the total data size $N$ and the line size $L$.\n\nWe are given the numerical values:\n$\\alpha = 13.7$ nJ per read\n$\\beta = 17.9$ nJ per write\n\nSubstituting these values into the expression for the fractional reduction:\n$$\n\\text{Fractional Reduction} = \\frac{13.7}{13.7 + 17.9} = \\frac{13.7}{31.6}\n$$\nPerforming the division:\n$$\n\\frac{13.7}{31.6} \\approx 0.433544303...\n$$\nThe problem requires the final answer to be rounded to four significant figures. The first four significant figures are $4$, $3$, $3$, and $5$. The fifth digit is $4$, so we round down.\n$$\n\\text{Fractional Reduction} \\approx 0.4335\n$$\nThis is the final dimensionless decimal number representing the fractional energy reduction.",
            "answer": "$$\\boxed{0.4335}$$"
        },
        {
            "introduction": "While the write-allocate policy is common, it can create severe performance bottlenecks under the wrong conditions. This practice challenges you to analyze a pathological scenario where interspersed store operations systematically evict useful data from a direct-mapped cache, causing a \"throughput collapse.\" By deriving the conditions that trigger this cache thrashing, you will gain a deeper appreciation for the delicate interplay between software access patterns and hardware cache policies .",
            "id": "3688576",
            "problem": "Consider a central processing unit with a Level 1 (L1) data cache that is direct-mapped with capacity of $C$ lines. The cache uses a write-allocate policy for stores, specifically Read-For-Ownership (RFO): on a store miss, the cache controller fetches the target line and allocates it in L1, potentially evicting the line currently resident at the same index. The replacement behavior of a direct-mapped cache is that a single incoming line to an index evicts the single line already occupying that index. The processor employs a write buffer to hold write-back traffic; assume the write buffer is sufficiently deep to avoid causing stalls on its own and drains at a rate that does not limit the steady-state throughput under the conditions described.\n\nA loop executes a pathological memory access pattern constructed to thrash the L1 under write-allocate while making the read phase depend entirely on lines previously resident in L1. The loop consists of two phases repeated indefinitely:\n\n- A read phase that touches exactly $C$ distinct lines (forming a hot working set) whose indices cover all $C$ cache indices. These lines must remain in L1 to achieve read hits on the next read phase.\n- An interference store phase that performs stores to lines that are not currently resident in L1, with addresses chosen so that each store miss maps to an index used by the hot read set. Under write-allocate, each store miss allocates a line into L1 and evicts the line currently at its index. The interference store phase is interleaved with other memory operations so that across the entire interval between consecutive touches to the same hot read line, the fraction of memory operations that are stores is $f_s$, where $0 < f_s \\leq 1$.\n\nDefine the reuse distance $R$ as the total number of memory operations (reads plus stores) between consecutive touches to a given hot read line. The system is said to reach the throughput collapse point when, upon entering a subsequent read phase, all $C$ read accesses to the hot working set miss in L1 because the corresponding lines were evicted during the prior interference store phase, making the throughput limited by miss latencies rather than by L1 hit latencies.\n\nStarting from the definitions of a direct-mapped cache, write-allocate and Read-For-Ownership semantics, and reuse distance, and assuming adversarial address selection in the interference store phase, derive the minimal reuse distance at which throughput collapses, denoted $R^{\\star}(C,f_s)$, as a function of $C$ and $f_s$.\n\nProvide your final answer as a single closed-form analytic expression for $R^{\\star}(C,f_s)$ in terms of $C$ and $f_s$. No numerical approximation is required.",
            "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- The Level 1 (L1) data cache is direct-mapped with a capacity of $C$ lines.\n- The cache policy is write-allocate, specifically Read-For-Ownership (RFO), meaning a store miss allocates the line in the cache.\n- The replacement policy for the direct-mapped cache is that an incoming line to an index evicts the line currently at that index.\n- A hot working set of $C$ distinct lines is accessed in a read phase, with these lines mapping to all $C$ distinct cache indices.\n- An interference store phase performs stores to non-resident lines, with addresses chosen adversarially such that each store miss maps to an index used by the hot read set.\n- The fraction of memory operations that are stores is $f_s$, where $0 < f_s \\leq 1$.\n- The reuse distance, $R$, is the total number of memory operations (reads and stores) between two consecutive accesses to the same hot read line.\n- Throughput collapse is defined as the point where all $C$ read accesses to the hot working set result in misses.\n- The objective is to find the minimal reuse distance, $R^{\\star}(C,f_s)$, at which throughput collapse occurs.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes a canonical scenario for analyzing cache thrashing, a fundamental topic in computer organization and architecture. All terms like \"direct-mapped\", \"write-allocate\", \"reuse distance\", and \"Read-For-Ownership\" are standard and well-defined. The setup is self-contained and free of contradictions. The use of an \"adversarial\" access pattern is a common technique in performance analysis to establish worst-case bounds. The problem asks for a specific, derivable quantity based on the provided model. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A formal derivation of the solution will be provided.\n\n### Solution Derivation\nThe core of the problem is to determine the conditions under which the entire hot working set is evicted from the L1 cache.\n\nLet the hot working set consist of $C$ lines, denoted $\\{L_0, L_1, \\dots, L_{C-1}\\}$. Since the cache is direct-mapped with $C$ lines (indices $0, 1, \\dots, C-1$) and the hot set covers all indices, we can assume without loss of generality that line $L_i$ maps to index $i$ of the cache.\n\nThe problem defines \"throughput collapse\" as the state where, upon starting a read phase, all $C$ accesses to the hot working set result in misses. This means that every line $L_i$ from the hot set must have been evicted from the cache since its last access.\n\nEviction of a line $L_i$ at index $i$ occurs when another memory operation allocates a line at the same index $i$. According to the problem statement, this is caused by an \"interference store\" that misses in the cache. The write-allocate policy ensures that such a store miss will fetch the new line into the cache, thereby evicting the resident line, which is one of our hot lines $L_i$.\n\nThe analysis centers on the reuse distance, $R$. Consider a single hot line, say $L_j$. It is accessed at some time $t$. Its next access occurs after $R$ total memory operations. For this next access to be a miss, $L_j$ must be evicted during this interval of $R$ operations. For throughput collapse to occur, this must be true for all $C$ lines of the hot set.\n\nThe total number of memory operations within the reuse interval is $R$. The fraction of these operations that are stores is given as $f_s$. Therefore, the total number of stores, $N_s$, that occur within this interval is:\n$$N_s = f_s \\times R$$\nThese are interference stores to non-resident lines. Each such store causes a miss and, due to the write-allocate policy, an allocation and a corresponding eviction.\n\nThe problem states that the interference stores have \"adversarial address selection\". This is a critical condition. It means the addresses for the $N_s$ stores are chosen in the most harmful way possible with respect to the hot working set. To evict all $C$ hot lines $\\{L_0, L_1, \\dots, L_{C-1}\\}$, the interference stores must target all $C$ distinct cache indices $\\{0, 1, \\dots, C-1\\}$. An adversarial strategy will ensure this is achieved with the minimum number of stores. Instead of randomly selecting store addresses (which would require, on average, more than $C$ stores to hit all $C$ indices, as per the coupon collector's problem), an adversary can deterministically perform one store to a line that maps to index $0$, one to a line that maps to index $1$, and so on, until all $C$ indices have been targeted.\n\nTherefore, the minimum number of interference stores required to guarantee the eviction of all $C$ hot lines is exactly $C$.\n\nFor throughput collapse to occur, the number of stores within the reuse interval, $N_s$, must be at least equal to this minimum requirement.\n$$N_s \\ge C$$\n\nSubstituting the expression for $N_s$ in terms of $R$ and $f_s$, we get the condition for collapse:\n$$f_s \\times R \\ge C$$\n\nThe problem asks for the *minimal* reuse distance, denoted $R^{\\star}$, at which collapse occurs. This corresponds to the threshold where the number of stores is just sufficient to evict all $C$ lines. We therefore set the inequality to an equality:\n$$f_s \\times R^{\\star} = C$$\n\nSolving for $R^{\\star}$ as a function of $C$ and $f_s$ yields the final expression:\n$$R^{\\star}(C, f_s) = \\frac{C}{f_s}$$\nThis expression represents the minimal number of total memory operations between consecutive accesses to a hot line that guarantees the entire hot set of size $C$ is evicted, given an adversarial store fraction of $f_s$.",
            "answer": "$$\\boxed{\\frac{C}{f_s}}$$"
        },
        {
            "introduction": "Write buffers are critical for decoupling CPU execution from slow main memory writes, but they are finite resources that must be sized correctly. This exercise applies fundamental principles of flow conservation to a real-time system, where a task generates a burst of writes that must be drained before a deadline. Your task is to determine the minimum write buffer capacity needed to prevent overflow, connecting abstract queueing concepts to a concrete hardware design constraint in a performance-critical environment .",
            "id": "3688545",
            "problem": "A processor employs a Level-1 data cache with line size $L$ bytes and a write-back policy with write-allocate on write misses. The cache is connected to a write buffer that forwards full cache lines to main memory. A periodic real-time task of period $P$ finishes each instance by producing, in the worst case, $W$ distinct dirty cache lines that must be drained to memory. The memory system guarantees an exclusive drain window of duration $t_{\\text{slack}}$ at the end of each period during which the write buffer can transmit data to memory at a sustained drain bandwidth $B_{w}$ (in bytes per second). Outside this window, the write buffer does not drain.\n\nAssume the platform’s flush engine schedules the $W$ dirty lines for emission over the entire drain window and that the write buffer performs perfect write merging, so each cache line occupies exactly one buffer entry throughout its residence. Under these conditions, the write buffer’s occupancy evolves according to conservation of flow: instantaneous occupancy equals cumulative arrivals minus cumulative service.\n\nStarting from this conservation principle and the basic definitions above, derive the minimal integer buffer capacity $K$ (in entries) that ensures the write buffer does not overflow during the drain window, and compute $K$ for the parameters:\n- $L = 64$ bytes,\n- $W = 160$ lines,\n- $B_{w} = 1.024 \\times 10^{9}$ bytes per second,\n- $t_{\\text{slack}} = 7.8125 \\times 10^{-6}$ seconds,\n- $P = 1.0 \\times 10^{-3}$ seconds.\n\nExpress your final answer as an integer number of entries. If an intermediate expression is not an integer number of lines, use the mathematical ceiling to obtain the minimal integer $K$. No rounding by significant figures is required for the final answer.",
            "solution": "The user wants to find the minimal integer write buffer capacity, $K$, required to prevent overflow under a specific set of conditions. The problem will be solved by modeling the write buffer as a queue and applying the principle of flow conservation to determine its maximum occupancy.\n\n### Step 1: Problem Validation\n\nFirst, the problem statement is validated against the specified criteria.\n\n**1.1. Extracted Givens:**\n- Cache line size: $L$ bytes\n- Number of dirty lines produced per task instance: $W$\n- Real-time task period: $P$\n- Exclusive drain window duration: $t_{\\text{slack}}$\n- Sustained write buffer drain bandwidth: $B_{w}$ bytes/second\n- Write buffer behavior: Drains only during $t_{\\text{slack}}$; perfect write merging (1 line = 1 entry).\n- Operating principle: Instantaneous occupancy equals cumulative arrivals minus cumulative service.\n- Arrival scheduling: The $W$ dirty lines are scheduled for emission into the write buffer uniformly over the entire drain window of duration $t_{\\text{slack}}$.\n- Goal: Derive the minimal integer buffer capacity $K$ (in entries).\n- Numerical parameters:\n  - $L = 64$ bytes\n  - $W = 160$ lines\n  - $B_{w} = 1.024 \\times 10^{9}$ bytes/second\n  - $t_{\\text{slack}} = 7.8125 \\times 10^{-6}$ seconds\n  - $P = 1.0 \\times 10^{-3}$ seconds\n\n**1.2. Validation:**\n- **Scientific Grounding**: The problem is well-grounded in computer organization and architecture, specifically concerning cache and memory subsystem performance analysis. The concepts used (write buffer, bandwidth, write-back cache, real-time deadlines) are standard.\n- **Well-Posedness**: The problem is well-posed. It provides sufficient information and a clear objective (calculating $K$). The relationship between arrivals and service is defined, allowing for the derivation of a unique solution. The parameter $P$ provides context for the real-time system but is not directly needed for the calculation of buffer occupancy within the $t_{\\text{slack}}$ window, which does not render the problem invalid.\n- **Objectivity**: The language is technical and unambiguous.\n\n**1.3. Verdict:**\nThe problem is valid. It is a well-defined, scientifically sound problem in performance analysis of a computer's memory subsystem.\n\n### Step 2: Derivation of the Buffer Capacity Formula\n\nLet $t$ be the time elapsed since the beginning of the drain window, such that $t \\in [0, t_{\\text{slack}}]$. The occupancy of the write buffer at time $t$, denoted by $N(t)$, can be determined from the conservation of flow principle. We assume the buffer is empty at $t=0$.\n\n$$N(t) = (\\text{Cumulative Arrivals by time } t) - (\\text{Cumulative Service by time } t)$$\n\n**2.1. Arrival Process:**\nThe $W$ dirty cache lines are scheduled for emission into the buffer over the duration $t_{\\text{slack}}$. This implies a constant arrival rate, $R_{\\text{in}}$, into the buffer.\n$$R_{\\text{in}} = \\frac{W}{t_{\\text{slack}}}$$\nThe units of $R_{\\text{in}}$ are lines per second. The cumulative number of arrivals at time $t$ is:\n$$A(t) = R_{\\text{in}} \\cdot t = \\frac{W}{t_{\\text{slack}}} t$$\n\n**2.2. Service Process:**\nThe write buffer is drained at a sustained bandwidth of $B_{w}$ bytes per second. Since each line has a size of $L$ bytes, the service rate in lines per second, $R_{\\text{out}}$, is:\n$$R_{\\text{out}} = \\frac{B_w}{L}$$\nThe cumulative number of lines serviced by time $t$ is:\n$$S(t) = R_{\\text{out}} \\cdot t = \\frac{B_w}{L} t$$\n\n**2.3. Buffer Occupancy:**\nThe number of lines in the buffer at time $t$ is the difference between cumulative arrivals and cumulative service:\n$$N(t) = A(t) - S(t) = \\left(\\frac{W}{t_{\\text{slack}}} - \\frac{B_w}{L}\\right) t$$\nThe minimal buffer capacity $K$ must be large enough to hold the maximum occupancy, $N_{\\max}$, that occurs during the drain window.\n$$K \\ge N_{\\max} = \\max_{t \\in [0, t_{\\text{slack}}]} N(t)$$\nSince $N(t)$ is a linear function of $t$, its maximum over the interval $[0, t_{\\text{slack}}]$ will occur at one of the endpoints.\n- If the arrival rate is greater than the service rate ($R_{\\text{in}} > R_{\\text{out}}$), the term multiplying $t$ is positive. Thus, $N(t)$ is an increasing function, and its maximum occurs at $t = t_{\\text{slack}}$.\n$$N_{\\max} = \\left(\\frac{W}{t_{\\text{slack}}} - \\frac{B_w}{L}\\right) t_{\\text{slack}} = W - \\frac{B_w t_{\\text{slack}}}{L}$$\n- If the arrival rate is less than or equal to the service rate ($R_{\\text{in}} \\le R_{\\text{out}}$), the term multiplying $t$ is non-positive. Thus, $N(t)$ is non-increasing, and its maximum is $N(0)=0$. In this case, the buffer drains as fast as or faster than it fills, so a minimal capacity to hold a transient line is sufficient. The formula yields a non-positive result, which implies a theoretical maximum backlog of $0$.\n\nThe required buffer capacity $K$ must be an integer. Therefore, we must take the ceiling of the maximum occupancy.\n$$K = \\left\\lceil \\max\\left(0, W - \\frac{B_w t_{\\text{slack}}}{L}\\right) \\right\\rceil$$\n\n### Step 3: Calculation with Given Parameters\n\nNow, we substitute the provided numerical values into the derived expression.\n- $L = 64$ bytes\n- $W = 160$ lines\n- $B_{w} = 1.024 \\times 10^{9}$ bytes/s\n- $t_{\\text{slack}} = 7.8125 \\times 10^{-6}$ s\n\nFirst, we compute the total number of lines that can be serviced during the drain window, which is the term $\\frac{B_w t_{\\text{slack}}}{L}$.\n$$ \\frac{B_w t_{\\text{slack}}}{L} = \\frac{(1.024 \\times 10^{9} \\text{ bytes/s}) \\times (7.8125 \\times 10^{-6} \\text{ s})}{64 \\text{ bytes/line}} $$\nTo simplify the calculation, we can express the values as powers of $2$ and fractions:\n- $1.024 = 1024 \\times 10^{-3} = 2^{10} \\times 10^{-3}$\n- $64 = 2^6$\n- $7.8125 = 7 + \\frac{13}{16} = \\frac{112+13}{16} = \\frac{125}{16} = \\frac{125}{2^4}$\n\nSubstituting these into the expression:\n$$ \\frac{B_w t_{\\text{slack}}}{L} = \\frac{(2^{10} \\times 10^{-3} \\times 10^{9}) \\times (\\frac{125}{2^4} \\times 10^{-6})}{2^6} \\text{ lines} $$\n$$ = \\frac{2^{10} \\times 10^{6} \\times 125 \\times 10^{-6}}{2^{10}} \\text{ lines} $$\n$$ = 125 \\text{ lines} $$\nThis means the memory system can drain $125$ lines during the specified window.\nThe maximum occupancy is then:\n$$N_{\\max} = W - 125 = 160 - 125 = 35 \\text{ lines}$$\nSince $N_{\\max} = 35$ is a positive integer, the minimal integer capacity $K$ required is:\n$$K = \\lceil 35 \\rceil = 35$$\nThe minimal write buffer capacity required to ensure no overflow is $35$ entries.",
            "answer": "$$\\boxed{35}$$"
        }
    ]
}