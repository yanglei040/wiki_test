{
    "hands_on_practices": [
        {
            "introduction": "Software often uses general-purpose atomic instructions like Compare-and-Swap (CAS) to build synchronization mechanisms, such as counters. This exercise challenges you to model and contrast this software approach with a dedicated hardware atomic, Fetch-and-Add (FAA). By analyzing a high-contention scenario, you will quantify the dramatic performance difference and understand why the cost of failed atomic attempts is a critical bottleneck for scalability .",
            "id": "3621231",
            "problem": "A multicore processor with $N$ identical cores increments a shared hot-spot counter stored in a single cache line. Two designs are considered:\n\n- A loop using Compare-and-Swap (CAS), where each core repeatedly performs a read of the counter, computes the incremented value locally, and issues a CAS attempting to swap the old value with the incremented value. There is no backoff; all $N$ cores run tight loops and continuously contend on the same cache line.\n- A hardware Fetch-and-Add (FAA) instruction, where each core issues an atomic add on the counter.\n\nAssume a coherent memory system that serializes all atomic attempts (whether they eventually succeed or fail) to the cache line. Each atomic attempt that reaches the head of the coherence/interconnect queue requires exclusive ownership of the line and occupies the serialization point for exactly $L$ cycles, after which the line is released. In the saturated steady state, there is always at least one pending atomic request in the queue. Assume symmetric contenders, negligible local compute time compared to $L$, and that coherence arbitration is fair.\n\nStarting only from the definitions of atomicity (one-at-a-time visibility at a serialization point) and the above serialization and symmetry assumptions, derive the steady-state throughput of completed counter increments (in completed increments per cycle) for:\n\n- The CAS-based counter under the described tight-loop contention.\n- The FAA-based design under the same memory system.\n\nThen, define the throughput ratio $R$ as the FAA throughput divided by the CAS throughput and express $R$ in closed form in terms of $N$. Provide only the final expression for $R$ as your answer. No rounding is required.",
            "solution": "The problem asks for the ratio of throughputs for two different methods of incrementing a shared counter on a multicore processor with $N$ cores: one using a hardware Fetch-and-Add (FAA) instruction, and the other using a software loop with a Compare-and-Swap (CAS) instruction. Throughput is defined as the number of completed increments per cycle in the steady state.\n\nThe system model specifies that all atomic attempts (successful or not) are serialized. Each attempt that is serviced by the memory controller occupies the serialization point for a duration of $L$ cycles. The system is in a saturated steady state, meaning there is always a queue of pending requests, and arbitration among the $N$ symmetric cores is fair.\n\nLet's derive the throughput for each case.\n\n**1. Fetch-and-Add (FAA) Throughput**\n\nThe Fetch-and-Add (FAA) instruction is a single, atomic operation that reads the value of a memory location, adds a value to it, and writes the result back. By its definition, an FAA operation, when executed, is always successful in performing the increment.\n\nAccording to the problem statement, the memory system serializes atomic attempts, and each attempt processed takes $L$ cycles. In the FAA-based design, each atomic attempt is an FAA instruction. Since every FAA instruction that gets serialized completes a successful increment, the system performs exactly one increment for every $L$-cycle time slot.\n\nThe rate of completed increments, or throughput, is the number of increments divided by the number of cycles.\n$$\n\\text{Throughput}_{\\text{FAA}} = \\frac{1 \\text{ increment}}{L \\text{ cycles}} = \\frac{1}{L}\n$$\nThis throughput is independent of the number of cores $N$, as the bottleneck is the single serialization point in the memory system, which can only service one request at a time. The saturation assumption guarantees that this bottleneck is always busy.\n\n**2. Compare-and-Swap (CAS) Throughput**\n\nThe CAS-based approach involves a loop where each core first reads the current value of the counter, locally computes the new value, and then uses a CAS instruction to attempt to atomically update the counter. A `CAS(address, expected_value, new_value)` operation succeeds only if the value at `address` is equal to `expected_value`.\n\nConsider the system in a steady state where the counter holds a value $v$. Due to the \"tight loop\" and \"continuous contention\" specification, all $N$ cores will read the value $v$ and prepare to execute `CAS(address, v, v+1)`. All $N$ cores will then have a pending CAS request in the interconnect queue.\n\nThe fair arbiter will select one of these $N$ requests to be serviced. Let's say the request from core $C_1$ is chosen. Since the value in memory is still $v$, this CAS will succeed. This successful atomic attempt takes $L$ cycles, and the counter's value is updated to $v+1$.\n\nNow, consider the remaining $N-1$ cores. Their pending CAS attempts are all based on the old, stale `expected_value` of $v$. As the fair arbiter services each of these $N-1$ requests one by one, each CAS will fail because the value in memory is now $v+1$, which does not match the expected value $v$. According to the problem statement, each of these failed atomic attempts also occupies the serialization point for $L$ cycles.\n\nTherefore, for one single successful increment to occur, the system must process one successful CAS attempt followed by $N-1$ failed CAS attempts. The total number of serialized attempts required for one successful increment is $1 + (N-1) = N$.\n\nThe total time taken to complete one increment is the time for these $N$ serialized attempts:\n$$\n\\text{Total time per increment} = N \\text{ attempts} \\times L \\frac{\\text{cycles}}{\\text{attempt}} = NL \\text{ cycles}\n$$\nThe throughput for the CAS-based design is thus:\n$$\n\\text{Throughput}_{\\text{CAS}} = \\frac{1 \\text{ increment}}{NL \\text{ cycles}} = \\frac{1}{NL}\n$$\nThis result can also be understood probabilistically. At any point where multiple cores contend with the same `expected_value`, any given core's attempt has a $\\frac{1}{N}$ chance of being the one that succeeds. The expected number of trials needed for one success in a geometric distribution with success probability $p$ is $\\frac{1}{p}$. Here, the \"trials\" are the serialized $L$-cycle slots, and the effective probability of success in any given slot is $\\frac{1}{N}$ (one successful outcome distributed over $N$ contending attempts). Thus, the expected number of slots (and thus attempts) per increment is $N$, leading to an expected time of $NL$ cycles.\n\n**3. Throughput Ratio R**\n\nThe problem defines the throughput ratio $R$ as the FAA throughput divided by the CAS throughput.\n$$\nR = \\frac{\\text{Throughput}_{\\text{FAA}}}{\\text{Throughput}_{\\text{CAS}}}\n$$\nSubstituting the expressions derived above:\n$$\nR = \\frac{\\frac{1}{L}}{\\frac{1}{NL}}\n$$\nSimplifying the expression gives:\n$$\nR = \\frac{1}{L} \\times \\frac{NL}{1} = N\n$$\nThe throughput ratio $R$ is equal to the number of cores, $N$. This signifies that for this model of high contention without backoff, the hardware FAA primitive is $N$ times more efficient at utilizing the memory serialization point for this specific task than the software-based CAS loop.",
            "answer": "$$\n\\boxed{N}\n$$"
        },
        {
            "introduction": "On modern processors, ensuring an update is \"atomic\" is only half the battle; we must also control the visibility of memory operations across threads. This practice dives into the famous double-checked locking pattern, a classic case study where seemingly correct atomic logic fails spectacularly on weakly-ordered memory systems. You will learn to diagnose these failures and apply the correct memory ordering semantics, using release-acquire patterns or memory fences, to guarantee safe data publication between threads .",
            "id": "3621227",
            "problem": "A concurrent program uses a double-checked initialization pattern to publish a heap-allocated singleton object. There is one shared atomic pointer $P$ to a structure $S$ with two integer fields $a$ and $b$. The pointer $P$ is initially $\\text{null}$. The goal is to ensure that once any thread observes $P \\neq \\text{null}$, it will then observe $r \\rightarrow a = 1$ and $r \\rightarrow b = 2$ for $r = P$.\n\nThreads execute the following high-level logic:\n- Fast-path check in readers: read $P$ with memory order $mo_R$. If the result is not $\\text{null}$, read the fields $a$ and $b$ from the object that $P$ points to.\n- Slow path (initialization) in exactly one thread when $P$ is $\\text{null}$:\n  1. Allocate a new object $t$ of type $S$.\n  2. Write $t \\rightarrow a \\leftarrow 1$ and $t \\rightarrow b \\leftarrow 2$ using ordinary (non-atomic) stores in program order.\n  3. Publish $t$ into $P$ using an atomic primitive with memory order $mo_W$.\n\nAssume a weakly ordered multiprocessor (for example, a system whose hardware may reorder independent memory operations in the absence of ordering constraints). Also assume the standard release/acquire axioms for atomic operations: a store with release semantics can establish a synchronizes-with relation with a load with acquire semantics that reads the value written by that store, thereby creating a happens-before edge ordering prior writes before subsequent reads in the observing thread. Operations with relaxed memory order impose atomicity on the location but do not by themselves order other memory operations across threads.\n\nThe following mechanisms may be used in step $3$ and in the reader:\n- Compare-And-Swap (CAS): an atomic read-modify-write that, on success, writes the new value and returns success; on failure, returns the observed old value. Its success and failure memory orders can be chosen.\n- Load-Linked/Store-Conditional (LL/SC): an atomic pair that loads a value (LL) and conditionally stores a new value (SC) if no intervening conflicting write occurred.\n- Test-and-Set (TAS): an atomic primitive that writes a value (e.g., sets a lock) and returns the old value.\n\nConsider the following five claims about the correctness of the double-checked initialization and about possible outcomes on weak memory when $mo$ is relaxed. Select all correct claims.\n\nA. Using Compare-And-Swap (CAS) to publish $t$ into $P$ with a successful CAS that has release semantics for $mo_W$, and having readers perform the fast-path check by loading $P$ with acquire semantics for $mo_R$, guarantees that any reader observing $P \\neq \\text{null}$ will subsequently observe $r \\rightarrow a = 1$ and $r \\rightarrow b = 2$.\n\nB. If both the publishing operation in step $3$ and the reader’s fast-path load use relaxed memory order (that is, $mo_W = \\text{relaxed}$ and $mo_R = \\text{relaxed}$), then on a weakly ordered architecture it is possible for a reader to observe $P \\neq \\text{null}$ while still reading $r \\rightarrow a = 0$ and $r \\rightarrow b = 0$.\n\nC. Replacing CAS by a Load-Linked/Store-Conditional (LL/SC) pair to publish $t$ into $P$, without any explicit fences and with readers still using relaxed loads of $P$, is sufficient to guarantee that once $P \\neq \\text{null}$ is observed, subsequent reads will see $r \\rightarrow a = 1$ and $r \\rightarrow b = 2$.\n\nD. Protecting the initialization with a spinlock implemented using Test-and-Set (TAS) that provides mutual exclusion but uses no acquire semantics on lock and no release semantics on unlock, while readers still use a relaxed load of $P$ on the fast path, is sufficient to prevent any reader from observing a partially initialized object after $P$ becomes non-$\\text{null}$.\n\nE. Using explicit fences instead of release/acquire atomics as follows is sufficient: the initializing thread executes a release fence immediately before a relaxed store that publishes $t$ into $P$, and any reader that finds $P \\neq \\text{null}$ executes an acquire fence immediately after the relaxed load of $P$ and before reading $r \\rightarrow a$ and $r \\rightarrow b$.",
            "solution": "The user wants a critical analysis of a concurrent programming problem concerning the double-checked initialization pattern on weakly-ordered multiprocessors.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A concurrent program uses a double-checked initialization pattern for a singleton.\n- There is a shared atomic pointer `$P$` to a structure `$S$`.\n- Structure `$S$` has two integer fields, `$a$` and `$b$`.\n- The initial value of `$P$` is `$\\text{null}$`.\n- The correctness goal is: if any thread observes `$P \\neq \\text{null}$`, it must then observe `$r \\rightarrow a = 1$` and `$r \\rightarrow b = 2$`, where `$r = P$`.\n- Reader fast-path logic: Read `$P$` with memory order `$mo_R$`. If not `$\\text{null}$`, read fields `$a$` and `$b$`.\n- Initializer slow-path logic (executed by one thread):\n  1. Allocate a new object `$t$` of type `$S$`.\n  2. Write `$t \\rightarrow a \\leftarrow 1$` and `$t \\rightarrow b \\leftarrow 2$` using ordinary (non-atomic) stores in program order.\n  3. Publish `$t$` into `$P$` using an atomic primitive with memory order `$mo_W$`.\n- System assumptions:\n  - Weakly ordered multiprocessor (hardware may reorder independent memory operations).\n  - Standard release/acquire axioms: A `release` store `synchronizes-with` an `acquire` load that reads its value, creating a `happens-before` relationship.\n  - `relaxed` memory order provides atomicity but no cross-thread ordering of other memory operations.\n- Available primitives: Compare-And-Swap (`$\\text{CAS}$`), Load-Linked/Store-Conditional (`$\\text{LL/SC}$`), Test-and-Set (`$\\text{TAS}$`).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly grounded in the principles of computer architecture and concurrent programming. It deals with standard, well-defined concepts like memory consistency models, atomic operations (`$\\text{CAS}$`, `$\\text{LL/SC}$`), memory ordering semantics (`release`, `acquire`, `relaxed`), and the `happens-before` relation. These are central topics in systems-level computer science.\n- **Well-Posed:** The problem is well-posed. It presents a clear scenario (double-checked locking), a precise goal (safe publication of an object), and a set of explicit assumptions (weak memory model, semantics of atomic operations). The task is to evaluate five specific claims against this formal background, which admits a unique, correct analysis for each claim.\n- **Objective:** The problem statement is objective and uses precise, standard terminology. There is no ambiguity in terms like \"release semantics\", \"acquire semantics\", or \"weakly ordered\".\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a standard and non-trivial problem in concurrent programming that is scientifically sound, well-posed, and objective. The solution will proceed by analyzing each claim based on the principles of memory consistency models.\n\n### Solution Derivation\n\nThe core challenge is to prevent data races on the fields `$a$` and `$b$`. On a weakly-ordered architecture, without explicit ordering constraints, the writes that initialize `$a$` and `$b$` are not guaranteed to be visible to another thread at the same time as the write that publishes the pointer `$P$`. A reader thread might see the non-`$\\text{null}$` value of `$P$` but read the uninitialized (default, e.g., `$0$`) values for `$a$` and `$b$`.\n\nTo guarantee correctness, a `happens-before` relationship must be established between the initialization of the object's fields and the subsequent reads of those fields by another thread. The standard mechanism for this is a `release` operation in the writer thread and an `acquire` operation in the reader thread on the same atomic variable (`$P$`).\n\nA store to `$P$` with `release` semantics ensures that all memory writes that happen-before it in the program order (i.e., the writes to `$a$` and `$b$`) are made visible to other threads. A load from `$P$` with `acquire` semantics ensures that all memory reads that happen-after it in program order (i.e., the reads of `$a$` and `$b$`) will see the effects of writes that were made visible by the corresponding `release` operation. The pairing of `release` and `acquire` `synchronizes-with`, establishing the necessary `happens-before` edge.\n\nNow, we evaluate each claim.\n\n**A. Using Compare-And-Swap (CAS) to publish $t$ into $P$ with a successful CAS that has release semantics for $mo_W$, and having readers perform the fast-path check by loading $P$ with acquire semantics for $mo_R$, guarantees that any reader observing $P \\neq \\text{null}$ will subsequently observe $r \\rightarrow a = 1$ and $r \\rightarrow b = 2$.**\n\n- **Analysis:** This describes the canonical correct implementation of the double-checked locking pattern using modern atomic operations.\n  - The initializing thread performs the writes `$t \\rightarrow a \\leftarrow 1$` and `$t \\rightarrow b \\leftarrow 2$`. These writes happen-before the `release` `$\\text{CAS}$` operation on `$P$`.\n  - A reader thread performs an `acquire` load on `$P$`.\n  - If the reader's load reads the value written by the writer's `$\\text{CAS}$`, the `release` `$\\text{CAS}$` `synchronizes-with` the `acquire` load.\n  - This `synchronizes-with` relationship establishes a `happens-before` edge. By transitivity, the writes to `$a$` and `$b$` happen-before the reads of `$a$` and `$b$` in the reader thread.\n  - Therefore, any reader that sees the non-`$\\text{null}$` pointer via an `acquire` load is guaranteed to see the correctly initialized values of `$a$` and `$b$`.\n- **Verdict:** **Correct**.\n\n**B. If both the publishing operation in step $3$ and the reader’s fast-path load use relaxed memory order (that is, $mo_W = \\text{relaxed}$ and $mo_R = \\text{relaxed}$), then on a weakly ordered architecture it is possible for a reader to observe $P \\neq \\text{null}$ while still reading $r \\rightarrow a = 0$ and $r \\rightarrow b = 0$.**\n\n- **Analysis:** `Relaxed` memory ordering only guarantees the atomicity of the operation on `$P$` itself. It provides no ordering constraints with respect to other memory operations.\n  - In the writer, the CPU or compiler is free to reorder the `relaxed` atomic store to `$P$` to occur before the non-atomic stores to `$t \\rightarrow a$` and `$t \\rightarrow b$`.\n  - Even if the writes are not reordered in the writer thread, their propagation to the memory system and other caches is not synchronized. A reader thread could observe the updated value of `$P$` (because that write has reached its cache) while the writes to `$a$` and `$b$` have not yet propagated and are not visible. The reader would then load a valid pointer but read uninitialized or stale data from the object. This is the classic failure mode of naive double-checked locking.\n- **Verdict:** **Correct**. The claim accurately describes a possible and well-known failure scenario.\n\n**C. Replacing CAS by a Load-Linked/Store-Conditional (LL/SC) pair to publish $t$ into $P$, without any explicit fences and with readers still using relaxed loads of $P$, is sufficient to guarantee that once $P \\neq \\text{null}$ is observed, subsequent reads will see $r \\rightarrow a = 1$ and $r \\rightarrow b = 2$.**\n\n- **Analysis:** A successful `$\\text{Store-Conditional}$` (`$\\text{SC}$`) operation often has implicit `release` semantics on many architectures. However, for the synchronization to be complete, the reader must use `acquire` semantics. The claim states that readers use `relaxed` loads. A `release` store (even an implicit one via `$\\text{SC}$`) does not `synchronize-with` a `relaxed` load. There is no `happens-before` edge created between the writer's initialization of the fields and the reader's access to them. The situation is therefore no better than using a `release` store with a `relaxed` load, which is incorrect and suffers from the same potential data races as in option B.\n- **Verdict:** **Incorrect**.\n\n**D. Protecting the initialization with a spinlock implemented using Test-and-Set (TAS) that provides mutual exclusion but uses no acquire semantics on lock and no release semantics on unlock, while readers still use a relaxed load of $P$ on the fast path, is sufficient to prevent any reader from observing a partially initialized object after $P$ becomes non-$\\text{null}$.**\n\n- **Analysis:** A lock guarantees mutual exclusion for the critical section, ensuring only one thread can perform the initialization. However, the ordering guarantees of a lock only apply between threads that acquire and release the same lock. The readers on the fast path do not interact with the lock at all. They perform a `relaxed` load on `$P$`. Since the lock's `unlock` operation is specified to have no `release` semantics, and the write to `$P$` is not specified to have `release` semantics either, there is no operation in the writer thread that can `synchronize-with` the reader. The reader's `relaxed` load cannot establish an `acquire` barrier. Consequently, there is no `happens-before` relationship, and readers can still observe a partially initialized object.\n- **Verdict:** **Incorrect**.\n\n**E. Using explicit fences instead of release/acquire atomics as follows is sufficient: the initializing thread executes a release fence immediately before a relaxed store that publishes $t$ into $P$, and any reader that finds $P \\neq \\text{null}$ executes an acquire fence immediately after the relaxed load of $P$ and before reading $r \\rightarrow a$ and $r \\rightarrow b$.**\n\n- **Analysis:** This describes an alternative method for achieving the required ordering using memory fences.\n  - **Writer:** The sequence is: (`writes to $a, b$`); `release_fence()`; `relaxed_store(P, t)`. The `release_fence` prevents any prior memory accesses (the writes to `$a$` and `$b$`) from being reordered past any subsequent store operations (the store to `$P$`).\n  - **Reader:** The sequence is: `r = relaxed_load(P)`; `acquire_fence()`; (`reads of $r \\rightarrow a, r \\rightarrow b$`). The `acquire_fence` prevents any subsequent memory accesses (the reads of `$a$` and `$b$`) from being reordered before any prior load operations (the load of `$P$`).\n  - The combination of a `release fence` followed by a store in one thread and a load followed by an `acquire fence` in another thread creates a `synchronizes-with` relationship, provided the load reads the value from the store. This establishes the necessary `happens-before` edge. The effect is equivalent to using a `release` store and an `acquire` load.\n- **Verdict:** **Correct**.",
            "answer": "A, B, E"
        },
        {
            "introduction": "As we venture into designing complex lock-free data structures, we encounter more subtle and dangerous pitfalls that go beyond basic memory ordering. This practice introduces the infamous ABA problem, a concurrency bug that can silently corrupt data even when using atomic instructions like CAS correctly. You will dissect how this anomaly occurs in a lock-free stack and derive a robust, quantitative solution using tagged pointers to ensure the integrity of your data structure .",
            "id": "3621915",
            "problem": "A lock-free Last-In-First-Out (LIFO) free list is implemented as a singly linked stack whose head is updated using the atomic compare-and-swap (CAS) instruction. Compare-and-swap (CAS) takes three arguments, an address, an expected value, and a new value; it atomically compares the current contents at the address with the expected value and, only if they are bitwise equal, writes the new value, returning whether it succeeded. In the basic design, the head is a single-word pointer to the top node. Consider two threads, each performing either a push or pop, with all updates linearized at the successful CAS.\n\nPart A. Illustrate how the classic ABA anomaly can occur in this free list without any tag or versioning, by specifying a plausible interleaving where one thread reads the head pointer as a node labeled $A$, another thread pops $A$, performs some sequence of pushes and pops, and eventually restores the head pointer to $A$, causing a stale CAS by the first thread to succeed even though the logical stack has changed.\n\nTo mitigate ABA, the head is augmented to be a pair $\\left(p, t\\right)$ packed into a single machine word, where $p$ is the pointer and $t$ is a $b$-bit tag. On every successful head update, the implementation increments $t$ by $1$ modulo $2^{b}$ and writes the new pair using a single CAS on the packed word. A CAS by any thread compares both $p$ and $t$ to the stale pair it previously read.\n\nPart B. Suppose that, for any thread between the moment it reads $\\left(p, t\\right)$ and the moment it executes its CAS, there can be at most $n$ successful head updates by all threads combined. Derive, from first principles, the minimal tag width $b$ as a function of $n$ that guarantees that no stale pair $\\left(p, t\\right)$ can match the current head pair again within that window, even if the pointer $p$ is reused, so that a stale CAS cannot succeed due to tag wraparound. Express your final answer as a single closed-form analytic expression in terms of $n$. Do not assume any particular data word size, and treat $n$ as an arbitrary positive integer. The final answer must be an expression without units.",
            "solution": "We begin from the operational definition of compare-and-swap (CAS): CAS on an address atomically compares the current value with an expected value and writes a new value if and only if they are bitwise equal. In a lock-free free list implemented as a stack, both push and pop operations update the head pointer using CAS. Linearizability requires that the logical effect of an operation occurs at the moment its CAS succeeds.\n\nPart A requires demonstrating the ABA anomaly. Consider the head initially pointing to node labeled $A$. A typical interleaving is:\n- Thread $T_{1}$ reads the head pointer and observes $A$. It computes a new head based on this observation but is delayed before performing its CAS.\n- Thread $T_{2}$ pops $A$ from the stack. The head now points to some other node, say $B$, or becomes $\\text{null}$ if the stack was of size $1$.\n- Thread $T_{2}$ proceeds to push and pop additional nodes in some sequence. In particular, it may push $A$ back onto the top of the stack, restoring the head pointer to $A$.\n- Thread $T_{1}$ resumes and performs its CAS, which compares only the pointer value. Because the head pointer is again $A$, the CAS sees the expected pointer and succeeds, even though the logical contents of the stack between $T_{1}$’s read and CAS changed arbitrarily. This is the ABA anomaly: the observed value went from $A$ to some $X$ and back to $A$, masking intervening changes.\n\nPart B augments the head with a version tag to distinguish different incarnations of the same pointer value. The head is the pair $\\left(p, t\\right)$, where $t$ is a $b$-bit counter that is incremented by $1$ modulo $2^{b}$ on each successful head update. A CAS by a thread now compares both $p$ and $t$ to its previously read pair. The goal is to choose $b$ so that, within any window in which a thread may attempt a CAS after reading $\\left(p, t\\right)$, there has not been sufficient wraparound to return the tag to the same value for the same pointer, even if the pointer $p$ is reused.\n\nWe formalize the requirement: let a thread read the head as $\\left(p_{0}, t_{0}\\right)$ at time $\\tau$. Between $\\tau$ and the thread’s CAS at time $\\tau'$, suppose there are $k$ successful head updates by any threads, where $k$ satisfies $1 \\leq k \\leq n$. Each successful update increments the tag by $1$ modulo $2^{b}$. Therefore, at time $\\tau'$, the current tag value associated with the head is\n$$\nt' \\equiv t_{0} + k \\pmod{2^{b}}.\n$$\nTo prevent a stale CAS from succeeding due to tag wraparound, we must ensure that no $k$ in the range $1 \\leq k \\leq n$ yields $t' = t_{0}$ modulo $2^{b}$. That is, we require\n$$\n\\forall k \\in \\{1, 2, \\dots, n\\}, \\quad k \\not\\equiv 0 \\pmod{2^{b}}.\n$$\nEquivalently, we require that the modulus $2^{b}$ exceeds the maximum possible number of increments in the window, so that no multiple of $2^{b}$ lies in $\\{1, \\dots, n\\}$. This condition is\n$$\n2^{b} > n.\n$$\nThe minimal integer $b$ satisfying this strict inequality is obtained by taking the base-$2$ logarithm and using the ceiling function over $n+1$:\n$$\nb = \\left\\lceil \\log_{2}\\!\\left(n+1\\right) \\right\\rceil.\n$$\nTo verify minimality, suppose $b' = \\left\\lceil \\log_{2}\\!\\left(n+1\\right) \\right\\rceil - 1$. Then $2^{b'} \\leq n$, so there exists some $k = 2^{b'}$ with $1 \\leq k \\leq n$ and $k \\equiv 0 \\pmod{2^{b'}}$, implying possible wraparound to the same tag. Therefore any smaller $b$ fails to guarantee non-reuse within the window. Thus the derived expression is both necessary and sufficient.\n\nThis derivation relies only on the definition of CAS atomicity, the increment-by-one update rule for the tag on each successful head change, and basic modular arithmetic. It ensures that, even if the pointer $p$ reappears (for example, due to memory reuse), the tag component will differ within any window of at most $n$ updates, preventing a stale CAS from succeeding on an identical $\\left(p, t\\right)$ pair.",
            "answer": "$$\\boxed{\\left\\lceil \\log_{2}\\!\\left(n+1\\right) \\right\\rceil}$$"
        }
    ]
}