{
    "hands_on_practices": [
        {
            "introduction": "Many concurrent programs use a simple \"flag\" to signal that some data is ready. However, without explicit instructions, a processor might reorder memory operations, allowing a reader to see the flag as \"ready\" before the data has actually been written. This exercise () challenges you to solve this classic problem by correctly placing the minimal memory fences required to guarantee that the data is fully visible once the ready flag is observed. Mastering this fundamental release-acquire synchronization pattern is essential for writing safe and efficient concurrent code.",
            "id": "3656212",
            "problem": "Consider the following message-passing pattern between two threads running on different Central Processing Units (CPU):\n\n- Shared state: an array $B[0 \\ldots N-1]$ of bytes and a flag $R$; initially, the contents of $B$ are unspecified and $R = 0$.\n- Writer thread $T_w$ performs, in program order: for each index $i$ with $0 \\le i \\le N-1$, write $0$ to $B[i]$, and then write $1$ to $R$.\n- Reader thread $T_r$ performs: spin until reading $R = 1$, then immediately read all elements $B[0 \\ldots N-1]$ and rely on each read being equal to $0$.\n\nAssume a weakly ordered multiprocessor with cache coherence per location but without implicit global ordering of independent locations, where the following are true:\n\n- Program order within a thread does not imply visibility order to other threads for different locations unless constrained by synchronization.\n- A release fence prohibits reordering of preceding memory accesses after the fence; an acquire fence prohibits reordering of following memory accesses before the fence.\n- A write to one location can become visible to other CPUs before earlier writes to other locations unless prevented by ordering.\n- The correctness criterion is: once $T_r$ observes $R = 1$, all writes setting $B[i] = 0$ by $T_w$ must be visible to $T_r$ for all $i$ with $0 \\le i \\le N-1$.\n\nYour task is to ensure the reader never observes $R = 1$ while any element of $B$ still holds a pre-zero value. Choose the minimal placement and strength of fences that, across weakly ordered but coherent architectures, establishes the necessary ordering to guarantee the correctness criterion. “Minimal” means the smallest total ordering strength and count that suffices on such architectures to enforce that the writer’s zeroing of $B$ happens-before the reader’s subsequent reads of $B$ after seeing $R = 1$.\n\nWhich option correctly specifies the minimal fences?\n\nA. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.\n\nB. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: no fence is needed; read $B$ immediately after observing $R = 1$.\n\nC. In $T_w$: no fence is needed; write $R \\leftarrow 1$ after zeroing $B$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.\n\nD. Place a full sequentially consistent fence (both acquire and release effects) before and after the critical operations in both $T_w$ and $T_r$, i.e., $2$ full fences per thread surrounding the zeroing and the flag operations.\n\nE. Introduce only a data dependency in $T_r$ by computing a single index $j$ from the read of $R$ and then reading $B[j]$ first; no fences are needed anywhere, and the remaining reads of $B$ follow in ordinary program order.",
            "solution": "This solution proceeds in three parts. First, the problem is validated for correctness and solvability. Second, a solution is derived from first principles. Third, each provided option is analyzed against the derived solution.\n\n### Part 1: Problem Validation\n\n*   **Step 1: Extract Givens**\n    *   Threads: Writer $T_w$, Reader $T_r$, on different CPUs.\n    *   Shared state: array $B[0 \\ldots N-1]$ of bytes, flag $R$.\n    *   Initial state: $B$ contents unspecified, $R = 0$.\n    *   Writer $T_w$ behavior:\n        1.  `for i from 0 to N-1`: `B[i] = 0`\n        2.  `R = 1`\n    *   Reader $T_r$ behavior:\n        1.  `spin until R == 1`\n        2.  Read all $B[0 \\ldots N-1]$\n    *   Architecture assumption: Weakly ordered multiprocessor with cache coherence per location, but no implicit global ordering of independent locations.\n    *   Memory model properties:\n        *   Program order within a thread does not imply visibility order to other threads for different locations unless constrained by synchronization.\n        *   A write to one location can become visible to other CPUs before earlier writes to other locations.\n    *   Fence definitions:\n        *   Release fence: Prohibits reordering of preceding memory accesses *after* the fence.\n        *   Acquire fence: Prohibits reordering of following memory accesses *before* the fence.\n    *   Correctness Criterion: Once $T_r$ observes $R=1$, all writes setting $B[i]=0$ by $T_w$ must be visible to $T_r$.\n    *   Task: Find the minimal placement and strength of fences to guarantee the correctness criterion. \"Minimal\" means smallest total ordering strength and count.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   The problem is scientifically grounded, well-posed, and objective. It represents a canonical producer-consumer synchronization problem used to teach memory consistency models. The terms and concepts (weak ordering, cache coherence, release/acquire fences) are standard in computer architecture. The problem provides all necessary information and is free from contradictions or flaws.\n\n*   **Step 3: Verdict and Action**\n    *   **Verdict:** The problem is **VALID**.\n    *   **Action:** Proceed to the solution phase.\n\n### Part 2: Solution Derivation\n\n1.  **Identify the Hazard:** The core problem is memory reordering on a weakly ordered system. The write to the flag $R$ (i.e., $R \\leftarrow 1$) by $T_w$ could be made visible to other CPUs *before* the writes that zero out the array $B$ are made visible. This is because the writes are to different memory locations.\n\n2.  **How the Hazard Manifests:** $T_r$ could see $R = 1$, exit its spin-loop, and immediately start reading the $B$ array. If the writes to $B$ have not yet become visible to $T_r$'s CPU, $T_r$ will read the old, unspecified values from $B$, violating the correctness criterion.\n\n3.  **Formulate the Required Ordering:** We need to establish a \"happens-before\" relationship. Specifically, all the writes to the array $B$ by $T_w$ must happen-before the write to $R$ by $T_w$. And the read of $R$ by $T_r$ (that sees the value $1$) must happen-before all the reads of the array $B$ by $T_r$. The synchronization on $R$ must then link these two chains of events.\n\n4.  **Applying Synchronization Primitives (Fences):**\n    *   **Writer's Side ($T_w$):** To prevent the write $R \\leftarrow 1$ from being reordered *before* the writes to $B$, a **release fence** is needed. Placed between the writes to $B$ and the write to $R$, it ensures that by the time $R=1$ is visible to any other CPU, all the preceding writes to $B$ are also visible.\n    *   **Reader's Side ($T_r$):** To prevent the reads of $B$ from being reordered *before* the read of $R$ confirms the value is $1$, an **acquire fence** is needed. Placed after reading $R=1$ and before reading $B$, it ensures that the reads of $B$ will only happen after the read of $R$ has completed.\n\n5.  **The Release-Acquire Pairing:** The release fence in $T_w$ pairs with the acquire fence in $T_r$. When $T_r$ reads $R = 1$ (a value written after a release fence), and then executes an acquire fence, it is guaranteed to see all the memory writes from $T_w$ that happened *before* the release fence. This establishes the necessary \"happens-before\" relationship and is the standard, minimal mechanism for this pattern.\n\n### Part 3: Option-by-Option Analysis\n\n*   **A. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.**\n    *   This corresponds exactly to the derived solution. The release fence in $T_w$ ensures its prior writes are published before the flag is set. The acquire fence in $T_r$ ensures it does not read the data until after it has observed the flag. This pairing is minimal and correct.\n    *   Verdict: **Correct**.\n\n*   **B. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: no fence is needed; read $B$ immediately after observing $R = 1$.**\n    *   This is insufficient. Without an acquire fence, the reader's CPU is free to reorder its reads of $B$ to occur before the read of $R$ confirms the value is $1$ (e.g., due to speculative execution).\n    *   Verdict: **Incorrect**.\n\n*   **C. In $T_w$: no fence is needed; write $R \\leftarrow 1$ after zeroing $B$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.**\n    *   This is also insufficient. Without a release fence, the writer's CPU is free to make the write to $R$ visible to other CPUs *before* making the writes to $B$ visible. The reader would see the flag but read stale data.\n    *   Verdict: **Incorrect**.\n\n*   **D. Place a full sequentially consistent fence (both acquire and release effects) before and after the critical operations in both $T_w$ and $T_r$, i.e., $2$ full fences per thread surrounding the zeroing and the flag operations.**\n    *   This would work but is not **minimal**. A full fence is stronger than necessary. For this one-way data publication, only release semantics are needed on the writer and acquire semantics on the reader.\n    *   Verdict: **Incorrect**.\n\n*   **E. Introduce only a data dependency in $T_r$ by computing a single index $j$ from the read of $R$ and then reading $B[j]$ first; no fences are needed anywhere, and the remaining reads of $B$ follow in ordinary program order.**\n    *   This relies on \"address dependency ordering,\" which is not a universal guarantee across all weakly ordered architectures. More importantly, it would only order the read of $B[j]$, not the reads of any other elements in $B$. This is not a general or complete solution.\n    *   Verdict: **Incorrect**.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "A `release` operation on its own is only half of the story; it is a promise that prior writes are ready, but a promise that must be \"accepted\" by an `acquire` operation to establish a synchronization point. This practice explores a common pitfall where a programmer correctly uses a `release` store but the reader uses a `relaxed` load, breaking the synchronization chain. By analyzing this scenario (), you will understand why the pairing of release and acquire semantics is non-negotiable for establishing a \"happens-before\" guarantee and preventing data races.",
            "id": "3656251",
            "problem": "Consider two threads, a writer thread and a reader thread, running on a weakly ordered Instruction Set Architecture (ISA). There are two shared variables: a plain integer $data$ initialized to $0$, and an atomic flag $ready$ initialized to $0$. The writer thread executes in program order: first $data \\leftarrow 42$, then an atomic store to $ready$ with release semantics setting $ready \\leftarrow 1$. The reader thread spins on $ready$ using relaxed loads until it observes $ready = 1$, and then reads $data$ into a local register $r$. Concretely:\n- Writer thread: $data \\leftarrow 42$; `atomic_store_release(ready, 1)`.\n- Reader thread: while (`atomic_load_relaxed(ready)` $==$ $0$) { } ; $r \\leftarrow data$.\n\nAssume that cache coherence provides a single total order per location but does not, by itself, order distinct locations, and that the language and hardware allow both compiler and hardware reordering subject to the atomic semantics selected. Under these assumptions, reason from the core definitions of program order, cache coherence, release/acquire synchronization, and happens-before, and answer the following:\n\nWhich option best characterizes whether $r$ can be observed as $0$ after the reader sees $ready = 1$, and identifies a correct minimal fix that relies on appropriate memory barriers or fences?\n\nA. No. Observing $ready = 1$ guarantees that $data$ is up-to-date due to cache coherence; no fence is needed.\n\nB. Yes. A store with release semantics by the writer only orders its prior writes with respect to a matching acquire load by the reader; a relaxed load does not synchronize, so $r$ may be $0$ on weak memory models. A minimal fix is to make the spin load an acquire load or to insert an acquire fence between reading $ready$ and reading $data$.\n\nC. Yes. But the fix is to declare $data$ as an atomic and read it with relaxed semantics; no ordering on $ready$ is needed.\n\nD. No. A release store on $ready$ forbids any following reader from reordering its subsequent loads, so the pattern is safe as written.\n\nE. Yes. But the minimal fix is to make the writer use a sequentially consistent store on $ready$; no change is needed on the reader.",
            "solution": "The user-provided problem statement shall first be validated for its scientific and logical integrity.\n\n### Step 1: Extract Givens\n- **System**: Two threads, a writer and a reader, on a weakly ordered Instruction Set Architecture (ISA).\n- **Shared Variables**:\n    - plain integer $data$, initialized to $0$.\n    - atomic flag $ready$, initialized to $0$.\n- **Writer Thread Logic**:\n    1.  $data \\leftarrow 42$\n    2.  `atomic_store_release(ready, 1)`\n- **Reader Thread Logic**:\n    1.  `while (atomic_load_relaxed(ready) == 0) { }`\n    2.  $r \\leftarrow data$ (where $r$ is a local register).\n- **Assumptions**:\n    1. Cache coherence provides a single total order per memory location.\n    2. Cache coherence does not order operations on distinct memory locations.\n    3. Both compiler and hardware reordering are permitted, constrained only by the specified atomic semantics.\n- **Question**: Can the register $r$ be observed to hold the value $0$ after the reader thread observes $ready = 1$, and what is the correct minimal fix?\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a classic producer-consumer or flag synchronization pattern in concurrent programming. The concepts used—weakly ordered architectures, cache coherence, program order, happens-before semantics, and atomic operations with different memory orderings (`release`, `relaxed`)—are fundamental and well-defined topics in computer organization and architecture, and in the memory models of languages like C++ and Rust. The problem statement is scientifically sound, well-posed, and objective.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be derived.\n\n### Derivation of the Solution\n\nThe core of this problem lies in the \"happens-before\" relationship, which defines the formal memory-ordering guarantees between operations in different threads.\n\n1.  **Program Order and Release Semantics (Writer)**: The writer thread executes $W_1: data \\leftarrow 42$ followed by $W_2: \\text{`atomic_store_release(ready, 1)`}$. The `release` semantic of operation $W_2$ guarantees that all prior memory writes in the same thread (in this case, $W_1$) are made visible to other threads that synchronize with this operation. It acts as a barrier, preventing reordering of $W_1$ *after* $W_2$.\n\n2.  **Relaxed Semantics and Reordering (Reader)**: The reader thread executes $R_1$: a loop of `atomic_load_relaxed(ready)` until it reads $1$, followed by $R_2: r \\leftarrow data$. A `relaxed` memory ordering only guarantees atomicity but provides **no synchronization or ordering guarantees**. It does not \"synchronize-with\" a `release` store.\n\n3.  **The Synchronization Failure**: For the write to $data$ to be guaranteed to be visible before the read from $data$, a \"happens-before\" relationship must be established between $W_1$ and $R_2$. This is achieved if the `release` store ($W_2$) \"synchronizes-with\" an `acquire` load. Since the reader performs a `relaxed` load ($R_1$), no such synchronization occurs.\n\n4.  **Consequences on a Weakly Ordered ISA**: Without synchronization, the writer's store to `ready` may become visible to the reader's core before the store to $data$. The reader could see `ready = 1`, exit its loop, and its subsequent read of $data$ might be served from its local cache, which still holds the stale value $0$. Therefore, it is entirely possible for the reader thread to observe `ready = 1` and subsequently read $0$ into $r$. The code has a data race.\n\n5.  **The Minimal Fix**: To correct this, a \"synchronizes-with\" relationship must be established. The `release` store by the writer must be paired with an `acquire` operation by the reader. There are two standard minimal ways to achieve this:\n    a.  Modify the reader's load: Change `atomic_load_relaxed(ready)` to `atomic_load_acquire(ready)`. The `acquire` load synchronizes with the `release` store, establishing the happens-before relationship.\n    b.  Insert an `acquire` fence: Keep the `relaxed` load but insert an `acquire` fence after the loop and before reading $data$: `atomic_thread_fence(memory_order_acquire);`.\n\nBoth fixes are considered minimal.\n\n### Option-by-Option Analysis\n\n**A. No. Observing $ready = 1$ guarantees that $data$ is up-to-date due to cache coherence; no fence is needed.**\n- **Analysis**: Incorrect. Cache coherence provides consistency for individual memory locations but does not enforce ordering between different locations ($data$ and $ready$).\n- **Verdict**: Incorrect.\n\n**B. Yes. A store with release semantics by the writer only orders its prior writes with respect to a matching acquire load by the reader; a relaxed load does not synchronize, so $r$ may be $0$ on weak memory models. A minimal fix is to make the spin load an acquire load or to insert an acquire fence between reading $ready$ and reading $data$.**\n- **Analysis**: Correct. This accurately identifies that $r$ can be $0$, explains that a `release` store requires a matching `acquire` operation, and correctly identifies the two standard minimal fixes.\n- **Verdict**: Correct.\n\n**C. Yes. But the fix is to declare $data$ as an atomic and read it with relaxed semantics; no ordering on $ready$ is needed.**\n- **Analysis**: Incorrect. Merely making $data$ atomic and using a `relaxed` load does not solve the ordering problem. The lack of synchronization via the $ready$ flag remains.\n- **Verdict**: Incorrect.\n\n**D. No. A release store on $ready$ forbids any following reader from reordering its subsequent loads, so the pattern is safe as written.**\n- **Analysis**: Incorrect. A `release` store by one thread does not impose ordering constraints on a reader thread that is not performing a corresponding `acquire` operation.\n- **Verdict**: Incorrect.\n\n**E. Yes. But the minimal fix is to make the writer use a sequentially consistent store on $ready$; no change is needed on the reader.**\n- **Analysis**: Incorrect. While using a `seq_cst` store would work, it is stronger than necessary and thus not the *minimal* fix. The minimal fix is to complete the existing `release` with a matching `acquire` on the reader's side.\n- **Verdict**: Incorrect.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Memory fences are powerful tools for ordering operations, but they are not a silver bullet. A common misconception is that a fence can fuse multiple memory accesses into a single, indivisible transaction. This practice () challenges that notion by exploring \"load tearing,\" where a large piece of data is read in smaller, non-atomic chunks. You will discover that even with perfect release-acquire synchronization, atomicity is a separate concern that must be addressed to prevent reading corrupted data when other threads are concurrently writing.",
            "id": "3656190",
            "problem": "Consider a shared record with two fields stored in coherent main memory accessed by multiple Central Processing Units (CPUs) under an Instruction Set Architecture (ISA) that provides the following widely accepted guarantees for normal, cacheable memory: (i) cache coherence, (ii) single-copy atomicity for naturally aligned accesses of sizes $1$, $2$, $4$, and $8$ bytes, and (iii) no architectural guarantee that two separate loads, even if back-to-back, behave as one indivisible transaction. The record has a flag field $flag$ and a $64$-bit data field $data$, with $data$ naturally aligned on an $8$-byte boundary. One producer thread $T_1$ signals the availability of $data$ by storing to $flag$ using a store with release semantics, and one consumer thread $T_2$ waits by repeatedly loading $flag$ using a load with acquire semantics until it observes the value stored by $T_1$. After the acquire, $T_2$ immediately reads $data$ using two $32$-bit loads, first the low half $data_{lo}$ and then the high half $data_{hi}$, reconstructing a $64$-bit value. Assume that single-copy atomicity applies per access, not across mixed-size pairs, and that acquire and release enforce inter-thread ordering (happens-before) but do not change the atomicity granularity of individual loads or stores.\n\nYou must reason from the following base facts: (a) cache coherence imposes a total order per memory location, (b) single-copy atomicity means an aligned $n$-byte access is observed as an indivisible read or write of those $n$ bytes, and (c) a load with acquire semantics prevents subsequent program-ordered memory operations from being reordered before it and, when it reads a value from a store with release semantics, all program-ordered writes that occurred before that release in the producing thread happen-before the acquire and become visible to the acquiring thread.\n\nNow analyze the interplay between potential load tearing and the acquire on $flag$ in the following cases.\n\nCase setup for $T_1$ (producer) before its store-release to $flag$:\n- Case $1$: $T_1$ writes $data$ using two $32$-bit stores in program order to $data_{lo}$ then $data_{hi}$, then performs a store-release to $flag$.\n- Case $2$: $T_1$ writes $data$ using one naturally aligned $64$-bit store, then performs a store-release to $flag$.\n\nOn the consumer $T_2$ side, after a load-acquire of $flag$ reads the value from $T_1$, $T_2$ performs two $32$-bit loads from $data_{lo}$ and $data_{hi}$ to reconstruct the $64$-bit value. Additionally, consider a third thread $T_3$ that may, in some scenarios, perform writes to $data$ after $T_1$ has set $flag$.\n\nWhich statements are correct?\n\nA. The load-acquire on $flag$ upgrades $T_2$’s two $32$-bit loads of $data$ into an effective atomic $64$-bit read, so tearing cannot occur in any of the cases.\n\nB. In Case $1$ with exactly one producer $T_1$ and one consumer $T_2$, and no other writers to $data$, once $T_2$’s load-acquire of $flag$ reads the store-release from $T_1$, $T_2$ cannot observe a torn value of $data$ when it reads via two $32$-bit loads.\n\nC. If a third thread $T_3$ writes to $data$ after $flag$ has been set by $T_1$, then even though $T_2$ performs a load-acquire of $flag$, $T_2$’s two $32$-bit loads of $data$ can observe a torn value formed from two different $T_3$ or $T_1$ versions.\n\nD. Replacing $T_2$’s two $32$-bit loads with a single naturally aligned $64$-bit atomic load of $data$ eliminates tearing without any additional fences.\n\nE. Inserting a full memory fence between $T_2$’s two $32$-bit loads prevents tearing even if $T_3$ writes to $data$ after $flag$ has been set.\n\nSelect all that apply.",
            "solution": "We begin from the specified base facts and the architectural guarantees. Let $T_1$ denote the producer, $T_2$ the consumer, and $T_3$ a potential concurrent writer. Let $data$ be a $64$-bit naturally aligned location whose low and high halves are denoted $data_{lo}$ and $data_{hi}$, each $32$ bits. The ISA guarantees single-copy atomicity for aligned $n$-byte accesses for $n \\in \\{1, 2, 4, 8\\}$. Crucially, acquire and release enforce ordering and visibility but do not alter the atomicity granularity of individual memory accesses. Two separate loads at $T_2$ remain two distinct, individually atomic operations; they are not fused into a single larger atomic transaction by the preceding acquire.\n\nWe now evaluate each option.\n\n**Option A:** The claim is that the load-acquire on $flag$ effectively transforms two $32$-bit loads into an atomic $64$-bit read. This contradicts the base fact that acquire semantics enforce ordering but do not change the atomicity of operations. The two $32$-bit loads are still separate accesses. Acquire ensures visibility of $T_1$’s prior writes, but it does not create a transactional group that is indivisible with respect to other threads. Therefore, the statement is **Incorrect**.\n\n**Option B:** In Case 1, $T_1$ writes to $data_{lo}$ then $data_{hi}$ before its release-store. When $T_2$’s load-acquire reads this value, the happens-before relation ensures that both of $T_1$’s $32$-bit stores are visible to $T_2$. Because there are no other writers to $data$, there is no other state for the data to be in. $T_2$'s two subsequent loads will read the two halves written by $T_1$. There is no possibility of reading one half from an old version and one half from the new version because no other versions are being written concurrently. Thus, no tearing can occur. Therefore, the statement is **Correct**.\n\n**Option C:** Here, $T_3$ writes to $data$ after $T_1$ has set $flag$. The acquire by $T_2$ only creates a happens-before relationship with $T_1$’s release; it does not synchronize with the unsynchronized writes from $T_3$. Consequently, while $T_2$ performs its two $32$-bit loads, $T_3$ could write to $data_{lo}$ or $data_{hi}$ in the interval between those two loads. Because the loads are not a single atomic transaction, $T_2$ can observe a low half from one version (e.g., from $T_1$) and a high half from another version (e.g., from $T_3$), creating a torn value. Therefore, the statement is **Correct**.\n\n**Option D:** Replacing the two $32$-bit loads with one naturally aligned $64$-bit load leverages the ISA guarantee of single-copy atomicity for aligned $8$-byte accesses. An aligned $64$-bit load reads the entire $64$-bit value as one indivisible operation. By definition, this prevents tearing, as it's impossible to observe a composite value from different points in time. No additional fences are needed to achieve tear-freedom for this specific read, as atomicity is an intrinsic property of the access itself. Therefore, the statement is **Correct**.\n\n**Option E:** Inserting a full memory fence between the two $32$-bit loads of $T_2$ does not make the pair atomic. A fence constrains the ordering of operations within $T_2$, but it cannot prevent another thread like $T_3$ from writing to $data$ in the interval between the two loads. A fence does not create mutual exclusion. Thus, tearing remains possible. Therefore, the statement is **Incorrect**.\n\nSummary of correctness:\n- Option A: Incorrect.\n- Option B: Correct.\n- Option C: Correct.\n- Option D: Correct.\n- Option E: Incorrect.",
            "answer": "$$\\boxed{BCD}$$"
        }
    ]
}