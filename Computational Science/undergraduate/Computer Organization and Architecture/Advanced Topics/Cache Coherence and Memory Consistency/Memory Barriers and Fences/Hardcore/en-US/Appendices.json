{
    "hands_on_practices": [
        {
            "introduction": "Understanding memory barriers begins with a classic scenario: a producer thread preparing data and then signaling a consumer thread that the data is ready. This exercise  explores how, on modern weakly-ordered processors, the signal can be observed before the data is actually visible, leading to incorrect behavior. Your task is to apply the minimal set of memory fences to enforce the correct \"happens-before\" ordering, illustrating the fundamental release-acquire synchronization pattern.",
            "id": "3656212",
            "problem": "Consider the following message-passing pattern between two threads running on different Central Processing Units (CPU):\n\n- Shared state: an array $B[0 \\ldots N-1]$ of bytes and a flag $R$; initially, the contents of $B$ are unspecified and $R = 0$.\n- Writer thread $T_w$ performs, in program order: for each index $i$ with $0 \\le i \\le N-1$, write $0$ to $B[i]$, and then write $1$ to $R$.\n- Reader thread $T_r$ performs: spin until reading $R = 1$, then immediately read all elements $B[0 \\ldots N-1]$ and rely on each read being equal to $0$.\n\nAssume a weakly ordered multiprocessor with cache coherence per location but without implicit global ordering of independent locations, where the following are true:\n\n- Program order within a thread does not imply visibility order to other threads for different locations unless constrained by synchronization.\n- A release fence prohibits reordering of preceding memory accesses after the fence; an acquire fence prohibits reordering of following memory accesses before the fence.\n- A write to one location can become visible to other CPUs before earlier writes to other locations unless prevented by ordering.\n- The correctness criterion is: once $T_r$ observes $R = 1$, all writes setting $B[i] = 0$ by $T_w$ must be visible to $T_r$ for all $i$ with $0 \\le i \\le N-1$.\n\nYour task is to ensure the reader never observes $R = 1$ while any element of $B$ still holds a pre-zero value. Choose the minimal placement and strength of fences that, across weakly ordered but coherent architectures, establishes the necessary ordering to guarantee the correctness criterion. “Minimal” means the smallest total ordering strength and count that suffices on such architectures to enforce that the writer’s zeroing of $B$ happens-before the reader’s subsequent reads of $B$ after seeing $R = 1$.\n\nWhich option correctly specifies the minimal fences?\n\nA. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.\n\nB. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: no fence is needed; read $B$ immediately after observing $R = 1$.\n\nC. In $T_w$: no fence is needed; write $R \\leftarrow 1$ after zeroing $B$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.\n\nD. Place a full sequentially consistent fence (both acquire and release effects) before and after the critical operations in both $T_w$ and $T_r$, i.e., $2$ full fences per thread surrounding the zeroing and the flag operations.\n\nE. Introduce only a data dependency in $T_r$ by computing a single index $j$ from the read of $R$ and then reading $B[j]$ first; no fences are needed anywhere, and the remaining reads of $B$ follow in ordinary program order.",
            "solution": "### Problem Validation\nThe problem statement is scientifically valid. It describes a classic producer-consumer scenario on a weakly-ordered multiprocessor, which is the canonical use case for illustrating the necessity of memory fences. The givens—thread behaviors, initial state, memory model properties, fence definitions, and correctness criterion—are complete, consistent, and well-defined. The question asks for a minimal solution, which is a standard and meaningful constraint in performance-sensitive concurrent programming.\n\n### Principle-Based Derivation\nThe fundamental hazard in a weakly-ordered memory system is that memory operations to different locations can be reordered, not just by the compiler, but by the hardware itself.\n\n1.  **Writer's Hazard ($T_w$):** The writer thread $T_w$ first zeroes the buffer $B$ and then sets the flag $R \\leftarrow 1$. On a weakly-ordered machine, the processor is permitted to make the write to the flag $R$ globally visible *before* the writes to the buffer $B$ are visible.\n2.  **Reader's Hazard ($T_r$):** The reader thread $T_r$ spins on the flag $R$. Upon seeing $R=1$, it proceeds to read the buffer $B$. A weakly-ordered processor might speculatively execute the reads of $B$ *before* the read of $R$ has been confirmed to be 1, leading to reads of stale data.\n\nTo prevent this, a \"happens-before\" relationship must be established between the writer's writes to $B$ and the reader's reads of $B$, synchronized via the flag $R$. This is achieved with a release-acquire pair:\n\n-   **Writer's Side ($T_w$):** To ensure the writes to $B$ are visible before the write to $R$ is, the writer must use a **release fence**. This fence is placed after the writes to $B$ but before the write to $R$. The release semantics prevent prior memory operations (the writes to $B$) from being reordered past the fence. Thus, any thread that sees the subsequent write to $R$ will also see the preceding writes to $B$.\n\n-   **Reader's Side ($T_r$):** To ensure the reads of $B$ are not executed before the flag $R$ is confirmed to be 1, the reader must use an **acquire fence**. This fence is placed after reading $R=1$ but before reading from $B$. The acquire semantics prevent subsequent memory operations (the reads of $B$) from being reordered before the fence.\n\nThis release-acquire pairing is the standard, minimal mechanism to ensure correct one-way data publication on weakly-ordered systems.\n\n### Option-by-Option Analysis\n\n**A. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.**\n-   This is the canonical solution. The release fence in $T_w$ ensures its writes to $B$ are published before the flag is set. The acquire fence in $T_r$ ensures it observes the published data from $B$ after it has observed the flag. This correctly establishes the necessary happens-before relationship and is the minimal required synchronization.\n-   **Verdict: Correct.**\n\n**B. In $T_w$: place a release fence between the zeroing of $B$ and the write $R \\leftarrow 1$. In $T_r$: no fence is needed; read $B$ immediately after observing $R = 1$.**\n-   This is incorrect. Without an acquire fence, the reader's processor could speculatively reorder its reads of $B$ to occur before its read of $R$ completes, thus reading stale data.\n-   **Verdict: Incorrect.**\n\n**C. In $T_w$: no fence is needed; write $R \\leftarrow 1$ after zeroing $B$. In $T_r$: place an acquire fence immediately after reading $R = 1$ and before any reads of $B$.**\n-   This is incorrect. Without a release fence, the writer's processor could make the write to $R$ visible before the writes to $B$. The reader would correctly wait for the flag and use an acquire fence, but would still read stale data from $B$ because those writes have not yet been made globally visible by the writer.\n-   **Verdict: Incorrect.**\n\n**D. Place a full sequentially consistent fence (both acquire and release effects) before and after the critical operations in both $T_w$ and $T_r$, i.e., $2$ full fences per thread surrounding the zeroing and the flag operations.**\n-   This solution would work, as full fences are very strong. However, it is not minimal. It provides much stronger ordering guarantees than necessary (e.g., the writer does not need acquire semantics, and the reader does not need release semantics).\n-   **Verdict: Incorrect.**\n\n**E. Introduce only a data dependency in $T_r$ by computing a single index $j$ from the read of $R$ and then reading $B[j]$ first; no fences are needed anywhere, and the remaining reads of $B$ follow in ordinary program order.**\n-   This relies on address dependency ordering, a guarantee that is not provided by all weakly-ordered architectures. More importantly, even on architectures where it exists, it typically only orders the dependent read ($B[j]$), not the other independent reads of the $B$ array. It is not a general or complete solution.\n-   **Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "Synchronization is a cooperative effort between threads, and using the correct memory ordering on one side is not enough. This practice  presents a common bug where a producer correctly uses a `release` store, but the consumer uses a `relaxed` load, failing to establish the necessary synchronization. By analyzing this broken pattern, you will gain a deeper understanding of why a `release` operation must be paired with a corresponding `acquire` operation to guarantee memory visibility.",
            "id": "3656251",
            "problem": "Consider two threads, a writer thread and a reader thread, running on a weakly ordered Instruction Set Architecture (ISA). There are two shared variables: a plain integer $data$ initialized to $0$, and an atomic flag $ready$ initialized to $0$. The writer thread executes in program order: first $data \\leftarrow 42$, then an atomic store to $ready$ with release semantics setting $ready \\leftarrow 1$. The reader thread spins on $ready$ using relaxed loads until it observes $ready = 1$, and then reads $data$ into a local register $r$. Concretely:\n- Writer thread: $data \\leftarrow 42$; `atomic_store_release(ready, 1)`.\n- Reader thread: `while (atomic_load_relaxed(ready) == 0) { }` ; $r \\leftarrow data$.\n\nAssume that cache coherence provides a single total order per location but does not, by itself, order distinct locations, and that the language and hardware allow both compiler and hardware reordering subject to the atomic semantics selected. Under these assumptions, reason from the core definitions of program order, cache coherence, release/acquire synchronization, and happens-before, and answer the following:\n\nWhich option best characterizes whether $r$ can be observed as $0$ after the reader sees $ready = 1$, and identifies a correct minimal fix that relies on appropriate memory barriers or fences?\n\nA. No. Observing $ready = 1$ guarantees that $data$ is up-to-date due to cache coherence; no fence is needed.\n\nB. Yes. A store with release semantics by the writer only orders its prior writes with respect to a matching acquire load by the reader; a relaxed load does not synchronize, so $r$ may be $0$ on weak memory models. A minimal fix is to make the spin load an acquire load or to insert an acquire fence between reading $ready$ and reading $data$.\n\nC. Yes. But the fix is to declare $data$ as an atomic and read it with relaxed semantics; no ordering on $ready$ is needed.\n\nD. No. A release store on $ready$ forbids any following reader from reordering its subsequent loads, so the pattern is safe as written.\n\nE. Yes. But the minimal fix is to make the writer use a sequentially consistent store on $ready$; no change is needed on the reader.",
            "solution": "The user-provided problem statement shall first be validated for its scientific and logical integrity.\n\n### Step 1: Extract Givens\n- **System**: Two threads, a writer and a reader, on a weakly ordered Instruction Set Architecture (ISA).\n- **Shared Variables**:\n    - plain integer $data$, initialized to $0$.\n    - atomic flag $ready$, initialized to $0$.\n- **Writer Thread Logic**:\n    1. $data \\leftarrow 42$\n    2. `atomic_store_release(ready, 1)`\n- **Reader Thread Logic**:\n    1. `while (atomic_load_relaxed(ready) == 0) { }`\n    2. $r \\leftarrow data$ (where $r$ is a local register).\n- **Assumptions**:\n    1. Cache coherence provides a single total order per memory location.\n    2. Cache coherence does not order operations on distinct memory locations.\n    3. Both compiler and hardware reordering are permitted, constrained only by the specified atomic semantics.\n- **Question**: Can the register $r$ be observed to hold the value $0$ after the reader thread observes $ready = 1$, and what is the correct minimal fix?\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement describes a classic producer-consumer or flag synchronization pattern in concurrent programming. The concepts used—weakly ordered architectures, cache coherence, program order, happens-before semantics, and atomic operations with different memory orderings (`release`, `relaxed`)—are fundamental and well-defined topics in computer organization and architecture, and in the memory models of languages like C++ and Rust.\n\n- **Scientifically Grounded**: The problem is firmly based on established principles of memory consistency models. The scenario is a standard textbook example used to illustrate the need for proper memory synchronization. It is scientifically sound.\n- **Well-Posed**: The problem is clearly defined with all necessary initial conditions, thread behaviors, and underlying architectural assumptions. The question it asks is specific and has a determinable answer based on the provided framework.\n- **Objective**: The language is precise, technical, and free of ambiguity or subjective claims.\n\nThe problem statement exhibits no flaws. It is a valid, well-structured problem in computer architecture.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be derived.\n\n### Derivation of the Solution\n\nThe core of this problem lies in the \"happens-before\" relationship, which defines the formal memory-ordering guarantees between operations in different threads.\n\n1.  **Program Order and Release Semantics (Writer)**: The writer thread executes its operations in program order.\n    - `W1`: $data \\leftarrow 42$\n    - `W2`: `atomic_store_release(ready, 1)`\n    The `release` semantic of operation `W2` guarantees that all prior memory writes in the same thread (in this case, `W1`) are made visible to other threads that synchronize with this operation. A `release` operation acts as a barrier, preventing reordering of `W1` *after* `W2`. Therefore, from the writer's perspective, the memory system is instructed to make the new value of $data$ available to other cores no later than the new value of $ready$.\n\n2.  **Relaxed Semantics and Reordering (Reader)**: The reader thread executes:\n    - `R1`: A loop of `atomic_load_relaxed(ready)` until it reads $1$.\n    - `R2`: $r \\leftarrow data$\n    The load of $ready$ uses `relaxed` memory ordering. `relaxed` atomics only guarantee atomicity (the load cannot see a \"torn\" value), but they provide **no synchronization or ordering guarantees**. Specifically, a `relaxed` load does not \"synchronize-with\" a `release` store.\n\n3.  **The Synchronization Failure**: For the write to $data$ to be guaranteed to be visible before the read from $data$, a \"happens-before\" relationship must be established between `W1` and `R2`. In the release-acquire model, this is achieved if the `release` store (`W2`) \"synchronizes-with\" an `acquire` load. Since the reader performs a `relaxed` load (`R1`), no such synchronization occurs.\n\n4.  **Consequences on a Weakly Ordered ISA**: Without a synchronization point, the system is free to reorder operations.\n    - The write to $data$ and the write to $ready$ are on two different memory locations. Cache coherence guarantees a total order for all writes to $ready$ and a separate total order for all writes to $data$, but it does not order the visibility of $data$ relative to $ready$.\n    - It is possible for the writer's store to $ready$ to become visible to the reader's core before the store to $data$ becomes visible. The reader's core could see $ready = 1$ and exit its loop, but its subsequent read of $data$ might be served from its local cache, which still holds the stale value $0$.\n    - Alternatively, the reader's processor itself might reorder the operations, speculatively executing the load from $data$ (`R2`) before the loop (`R1`) has definitively terminated. Subject to data dependencies, a weakly-ordered processor is allowed such reordering in the absence of an appropriate memory fence/barrier.\n\nTherefore, it is entirely possible for the reader thread to observe $ready = 1$ and subsequently read the initial value $0$ from $data$ into $r$. The code has a data race.\n\n5.  **The Minimal Fix**: To correct this race condition, a \"synchronizes-with\" relationship must be established between the writer's store to $ready$ and the reader's subsequent load from $data$. The `release` store by the writer must be paired with an `acquire` operation by the reader. There are two standard ways to achieve this:\n    a.  Modify the reader's load of the flag: Change `atomic_load_relaxed(ready)` to `atomic_load_acquire(ready)`. This is the most direct fix. The `acquire` load synchronizes with the `release` store, establishing a happens-before relationship. The `acquire` semantic acts as a barrier, ensuring that the prior write to $data$ by the writer is visible before any subsequent memory operations (like the read of $data$) in the reader thread are executed.\n    b.  Insert an `acquire` fence: Keep the `relaxed` load in the loop, but insert an `acquire` fence after the loop and before reading $data$.\n        `while (atomic_load_relaxed(ready) == 0) { }`\n        `atomic_thread_fence(memory_order_acquire);`\n        $r \\leftarrow data$;\n        The `acquire` fence synchronizes with the prior `release` store, providing the same guarantee as the `acquire` load.\n\nBoth fixes are considered minimal.\n\n### Option-by-Option Analysis\n\n**A. No. Observing $ready = 1$ guarantees that $data$ is up-to-date due to cache coherence; no fence is needed.**\n- **Analysis**: This statement is fundamentally incorrect. As stated in the problem's assumptions and the derivation above, cache coherence provides consistency for individual memory locations but does not enforce ordering between different locations ($data$ and $ready$). This misunderstands the distinction between coherence and consistency.\n- **Verdict**: Incorrect.\n\n**B. Yes. A store with release semantics by the writer only orders its prior writes with respect to a matching acquire load by the reader; a relaxed load does not synchronize, so $r$ may be $0$ on weak memory models. A minimal fix is to make the spin load an acquire load or to insert an acquire fence between reading $ready$ and reading $data$.**\n- **Analysis**: This statement correctly identifies that $r$ can be $0$. It accurately explains why: a `release` store requires a matching `acquire` operation to synchronize, and a `relaxed` load does not provide this. It then correctly identifies the two standard minimal fixes: changing the load to `acquire` or inserting an `acquire` fence. This analysis is perfectly aligned with the principles of modern memory models.\n- **Verdict**: Correct.\n\n**C. Yes. But the fix is to declare $data$ as an atomic and read it with relaxed semantics; no ordering on $ready$ is needed.**\n- **Analysis**: This is incorrect. Merely making $data$ atomic and using a `relaxed` load on it does not solve the ordering problem. The core issue is the lack of synchronization between the threads, which is the purpose of the $ready$ flag. Without ordering on $ready$, the read of $data$ can still be reordered or read a stale value relative to the read of $ready$. This proposed fix fails to establish the necessary happens-before relationship.\n- **Verdict**: Incorrect.\n\n**D. No. A release store on $ready$ forbids any following reader from reordering its subsequent loads, so the pattern is safe as written.**\n- **Analysis**: This statement is incorrect. A `release` store by one thread does not impose ordering constraints on an unrelated reader thread that is not performing a corresponding `acquire` operation. The ordering guarantees are conditional upon the reader's participation in the synchronization protocol.\n- **Verdict**: Incorrect.\n\n**E. Yes. But the minimal fix is to make the writer use a sequentially consistent store on $ready$; no change is needed on the reader.**\n- **Analysis**: Using a sequentially consistent store (`atomic_store_seq_cst`) on the writer would indeed fix the data race, as a `seq_cst` operation synchronizes with any atomic operation in another thread that reads the written value. However, the problem asks for a *minimal* fix. The `release-acquire` ordering is specifically designed for this type of one-way synchronization and is less restrictive (and potentially more performant) than sequential consistency. Upgrading the writer from `release` to `seq_cst` is a stronger-than-necessary change. The minimal fix is to complete the existing `release` operation with a matching `acquire` on the reader's side, as described in option B.\n- **Verdict**: Incorrect.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "While memory fences are essential for correctness in concurrent programming, they are not without cost. This exercise  shifts our focus from logical correctness to performance implications by modeling a processor's store buffer. You will derive the performance penalty, in terms of stall cycles, incurred when a full memory fence forces the processor to wait for all pending writes to become globally visible, providing a concrete understanding of why programmers prefer weaker, more targeted fences when possible.",
            "id": "3656213",
            "problem": "A processor core implements a store buffer with capacity $k$ entries to decouple store issue from attaining global visibility in a coherent memory system. A full memory fence requires that all stores issued before the fence become globally visible before any instruction after the fence can proceed. In this model, global visibility is achieved when a store has been committed to the last-level cache or main memory, and its coherence update has been acknowledged, which takes $L$ cycles from the instant the store begins its commit. Assume the following well-tested facts and definitions:\n\n- The store buffer is a first-in-first-out queue of pending stores, with capacity $k$.\n- A full memory fence must block the core until the store buffer is empty of all stores that were issued before the fence, ensuring no memory operation after the fence can be perceived as occurring before any store preceding the fence.\n- The memory system commits stores from the buffer in program order with no overlap, i.e., at most one store can be in the commit stage at a time, and each store’s commit takes $L$ cycles to reach global visibility. This models a strictly serialized write-commit path under strong ordering constraints.\n\nImmediately before the fence executes, suppose the store buffer is full with $k$ stores, all issued before the fence. Derive, from first principles of ordering and latency, the maximum number of stall cycles the core experiences due to the fence, as a function of $k$ and $L$. Express your final answer as a closed-form analytic expression in cycles. No rounding is required.",
            "solution": "The problem asks for the maximum number of stall cycles a processor core experiences due to a full memory fence, given a specific model of a store buffer and memory subsystem. The process begins with a critical validation of the problem statement.\n\n### Step 1: Extract Givens\n- Store buffer capacity: $k$ entries.\n- Store buffer discipline: First-in-first-out (FIFO) queue.\n- Full memory fence semantics: The core is blocked until all stores issued before the fence become globally visible.\n- Global visibility latency: A store becomes globally visible $L$ cycles after its commit begins.\n- Commit serialization: The memory system commits stores from the buffer sequentially, in program order. There is no overlap between commit operations; at most one store can be in the commit stage at a time.\n- Initial condition: At the moment the fence is executed, the store buffer is full with $k$ stores, all of which were issued before the fence.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is analyzed for validity.\n- **Scientifically Grounded**: The concepts presented—store buffers, memory fences (barriers), commit latency, and global visibility—are fundamental and standard topics in computer organization and architecture. The model, while simplified, is a valid abstraction for analyzing processor performance and memory ordering.\n- **Well-Posed**: The problem is well-posed. It provides a clear initial state (a full store buffer with $k$ stores), a defined process (serialized commits with latency $L$), and a clear termination condition for the stall (global visibility of all $k$ stores). This structure allows for the derivation of a unique, deterministic answer. The use of \"maximum\" stall cycles is consistent with this deterministic model, as the derived duration is the single possible outcome under the given constraints.\n- **Objective**: The problem is stated using precise, objective, and standard terminology from the field of computer architecture. It is free from subjective or ambiguous language.\n- **Conclusion**: The problem does not violate any of the invalidity criteria. It is scientifically sound, well-posed, and objective. It is a formalizable problem within its specified domain.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be derived from first principles.\n\n### Derivation of Stall Cycles\n\nLet $T=0$ represent the instant the full memory fence instruction begins execution. At this point, the processor core stalls. The stall will persist until all $k$ stores that are in the store buffer have achieved global visibility.\n\nThe store buffer is a FIFO queue containing $k$ stores. Let us denote these stores in program order as $S_1, S_2, \\dots, S_k$. $S_1$ is the oldest store (at the head of the queue) and $S_k$ is the most recent store (at the tail of the queue).\n\nThe problem states that the memory system commits these stores from the buffer in program order. This means $S_1$ will be committed first, followed by $S_2$, and so on, up to $S_k$.\n\nThe crucial constraint is that the write-commit path is \"strictly serialized\" with \"no overlap\". This means that the commit process for one store must fully complete before the commit process for the next store can begin. The problem defines that a store's commit process, from its initiation to achieving global visibility, takes $L$ cycles. This implies that the serialized commit resource is occupied for a duration of $L$ cycles for each store.\n\nWe can now construct a timeline for the commit and global visibility of each store, starting from the stall at $T=0$.\n\n1.  **Store $S_1$**: At $T=0$, the memory system begins committing the first store, $S_1$.\n    - The commit process starts at $T_{start,1} = 0$.\n    - This process takes $L$ cycles.\n    - $S_1$ becomes globally visible at time $T_{visible,1} = T_{start,1} + L = 0 + L = L$.\n\n2.  **Store $S_2$**: Due to the strict serialization and no-overlap rule, the commit of $S_2$ cannot start until the commit process for $S_1$ has concluded. The resource becomes free at $T=L$.\n    - The commit process for $S_2$ starts at $T_{start,2} = L$.\n    - This process also takes $L$ cycles.\n    - $S_2$ becomes globally visible at time $T_{visible,2} = T_{start,2} + L = L + L = 2L$.\n\n3.  **Store $S_3$**: The commit of $S_3$ begins after $S_2$'s process completes.\n    - The commit process for $S_3$ starts at $T_{start,3} = 2L$.\n    - It becomes globally visible at time $T_{visible,3} = T_{start,3} + L = 2L + L = 3L$.\n\nBy induction, we can establish a general formula for the $i$-th store, $S_i$, where $i$ is an integer from $1$ to $k$.\n\n-   **Store $S_i$**: The commit process for store $S_i$ begins after the $(i-1)$ preceding stores have completed their commit processes. Each of these takes $L$ cycles.\n    - The commit process for $S_i$ starts at time $T_{start,i} = (i-1)L$.\n    - It becomes globally visible $L$ cycles later, at time $T_{visible,i} = T_{start,i} + L = (i-1)L + L = iL$.\n\nThe fence instruction stalls the core until all stores that were in the buffer at $T=0$ are globally visible. This condition is met only when the very last store, $S_k$, has become globally visible.\n\nThe time at which $S_k$ becomes globally visible is found by setting $i=k$ in our general formula:\n$$\nT_{visible,k} = kL\n$$\n\nSince the stall begins at $T=0$ and ends at $T=kL$, the total duration of the stall is $kL$ cycles. This is the maximum (and only) number of stall cycles under the specified model.\n\nTherefore, the maximum number of stall cycles the core experiences due to the fence is the product of the number of stores in the buffer, $k$, and the latency to global visibility for each store, $L$.",
            "answer": "$$\n\\boxed{kL}\n$$"
        }
    ]
}