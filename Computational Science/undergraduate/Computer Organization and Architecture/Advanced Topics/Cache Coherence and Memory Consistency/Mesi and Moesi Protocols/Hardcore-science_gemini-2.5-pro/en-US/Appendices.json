{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp how cache coherence protocols work, it's essential to trace the state of a cache line through a sequence of operations. This practice problem guides you through a specific timeline of memory reads and writes, allowing you to observe the state transitions under both MESI and MOESI protocols firsthand . By meticulously tracking the states and calculating completion times based on a fixed-latency model, you will uncover the precise mechanism through which the MOESI protocol can offer a performance advantage.",
            "id": "3658496",
            "problem": "A shared-bus multiprocessor with $4$ private, write-back, write-allocate caches implements a snooping directory-less protocol. Consider a single cache line $X$ that is not present in any cache at time $t=0$ and is clean in main memory. The system obeys the Single-Writer, Multiple-Reader (SWMR) invariant: at any instant, at most one cache may hold a writable copy, while any number of caches may hold read-only copies. Two coherence protocols are considered: Modified, Exclusive, Shared, Invalid (MESI) and Modified, Owned, Exclusive, Shared, Invalid (MOESI). All cores issue coherence requests on a single broadcast address bus. The following fixed-latency response model holds for all requests, with latencies measured from the cycle when the request’s address is broadcast:\n- Main memory data response latency is $L_{\\mathrm{mem}}=20$ cycles.\n- Cache-to-cache data response latency from a cache that supplies data (only caches in state Modified under MESI, and in states Modified or Owned under MOESI) is $L_{\\mathrm{cc}}=8$ cycles.\n- Invalidation acknowledgments from each sharer arrive $L_{\\mathrm{inv}}=4$ cycles after the address broadcast. A requester may proceed with a write only after all required invalidation acknowledgments have been received and line data has arrived; the completion time is the maximum of these two arrival times.\n- If both a cache and main memory could supply data for the same request, the first valid data response to appear on the data bus “wins” and all later data responses are aborted. Caches in state Shared or Exclusive do not supply data; only main memory supplies clean data in those cases. In MESI, on a read that hits a Modified line in some other cache, the Modified cache supplies data and downgrades to Shared and the memory is updated (treated as clean) by the transaction completion. In MOESI, on a read that hits a Modified line, the supplier downgrades to Owned and memory remains stale.\n\nThe following trace of requests occurs at the given timestamps (in cycles), where “BusRd” denotes a read miss that requests a read-only copy, and “BusRdX” denotes a read-for-ownership (write miss) that requests a writable copy:\n- At time $t=0$, core $C_{0}$ issues BusRd for $X$.\n- At time $t=25$, core $C_{1}$ issues BusRd for $X$.\n- At time $t=50$, core $C_{0}$ issues a write to $X$ (this issues BusUpgr if it holds $X$ in Shared).\n- At time $t=60$, core $C_{2}$ issues BusRd for $X$.\n- At time $t=80$, core $C_{3}$ issues BusRdX for $X$.\n\nAssume arbitration grants the address bus immediately at each timestamp shown (i.e., the address broadcast occurs exactly at the stated time), and that data transfers and invalidation acknowledgments take place asynchronously according to the fixed latencies above. There are no structural hazards on the data bus beyond the winner-takes-all rule for concurrent potential data suppliers for the same request.\n\nTasks:\n- For each request, order the snoop responses and determine which agent (a cache or main memory) supplies the data, consistent with the given latencies and the SWMR invariant. Determine the coherence state transitions of $X$ in the involved caches at the completion of each request under both Modified, Exclusive, Shared, Invalid (MESI) and Modified, Owned, Exclusive, Shared, Invalid (MOESI).\n- Let $T_{\\mathrm{MESI}}$ be the completion time (in cycles) at which core $C_{3}$’s BusRdX for $X$ becomes able to perform the write under MESI, and let $T_{\\mathrm{MOESI}}$ be the corresponding completion time under MOESI. Compute the difference $\\Delta T = T_{\\mathrm{MESI}} - T_{\\mathrm{MOESI}}$.\n\nExpress your final answer for $\\Delta T$ as an integer number of cycles. No rounding is required.",
            "solution": "We will trace the state of cache line $X$ in the four caches ($C_0, C_1, C_2, C_3$) and main memory for both MESI and MOESI protocols. The initial state at $t=0$ for all caches is Invalid (I).\n\n**MESI Protocol Analysis**\n\n1.  **$t=0$: $C_0$ issues BusRd for $X$.**\n    - $C_0$ has a read miss. It broadcasts a BusRd request.\n    - No other cache holds $X$. Main memory must supply the data.\n    - Data arrives at $t = 0 + L_{\\mathrm{mem}} = 20$.\n    - At $t=20$, $C_0$ receives the data. Since it is the only cache with a copy, its state becomes Exclusive (E).\n    - States at $t=20$: $C_0(\\text{E}), C_1(\\text{I}), C_2(\\text{I}), C_3(\\text{I})$, Mem(clean).\n\n2.  **$t=25$: $C_1$ issues BusRd for $X$.**\n    - $C_1$ has a read miss and broadcasts BusRd.\n    - $C_0$ snoops the request and sees it has the line in state E. As per the rules, a cache in state E does not supply data. Main memory supplies the clean data.\n    - Data arrives at $C_1$ at $t = 25 + L_{\\mathrm{mem}} = 45$.\n    - The presence of a sharer forces $C_0$ to downgrade its state.\n    - At $t=45$, the transaction completes. $C_1$ transitions to Shared (S), and $C_0$ transitions from E to S.\n    - States at $t=45$: $C_0(\\text{S}), C_1(\\text{S}), C_2(\\text{I}), C_3(\\text{I})$, Mem(clean).\n\n3.  **$t=50$: $C_0$ issues a write to $X$.**\n    - $C_0$ holds the line in S, requiring an upgrade to gain write permission. It broadcasts a BusUpgr request (equivalent to a BusRdX without data request).\n    - $C_1$ snoops the request and must invalidate its copy. It sends an invalidation acknowledgment (ack).\n    - The ack from $C_1$ arrives at $C_0$ at $t = 50 + L_{\\mathrm{inv}} = 54$.\n    - $C_0$ already has the data. It can perform the write after receiving all acks. The write completes at $t=54$.\n    - Upon completion, $C_0$ transitions to Modified (M). $C_1$ transitions to I.\n    - States at $t=54$: $C_0(\\text{M}), C_1(\\text{I}), C_2(\\text{I}), C_3(\\text{I})$, Mem(stale).\n\n4.  **$t=60$: $C_2$ issues BusRd for $X$.**\n    - $C_2$ has a read miss and broadcasts BusRd.\n    - $C_0$ snoops the request and sees it holds the line in M. According to the rules, $C_0$ must supply the data. This is a cache-to-cache transfer.\n    - Data arrives at $C_2$ at $t = 60 + L_{\\mathrm{cc}} = 68$.\n    - The MESI rule for this case states the supplier ($C_0$) downgrades to S and memory is updated. The transaction completes at $t=68$.\n    - $C_2$ transitions to S. $C_0$ transitions from M to S.\n    - States at $t=68$: $C_0(\\text{S}), C_1(\\text{I}), C_2(\\text{S}), C_3(\\text{I})$, Mem(clean).\n\n5.  **$t=80$: $C_3$ issues BusRdX for $X$.**\n    - $C_3$ has a write miss and broadcasts BusRdX.\n    - $C_0$ and $C_2$ both hold the line in S. They must invalidate their copies and send acks.\n    - Acks from $C_0$ and $C_2$ arrive at $t = 80 + L_{\\mathrm{inv}} = 84$.\n    - Since the existing copies in $C_0$ and $C_2$ are clean (S state), main memory supplies the data to $C_3$.\n    - Data arrives at $C_3$ at $t = 80 + L_{\\mathrm{mem}} = 100$.\n    - $C_3$ can perform its write after both data and all acks have arrived. Completion time is $\\max(100, 84) = 100$.\n    - Thus, $T_{\\mathrm{MESI}} = 100$ cycles.\n\n**MOESI Protocol Analysis**\n\nThe first three events are identical to the MESI protocol.\n\n1.  **$t=0$: $C_0$ issues BusRd for $X$.**\n    - Completion at $t=20$. States: $C_0(\\text{E}), C_1(\\text{I}), C_2(\\text{I}), C_3(\\text{I})$, Mem(clean).\n2.  **$t=25$: $C_1$ issues BusRd for $X$.**\n    - Completion at $t=45$. States: $C_0(\\text{S}), C_1(\\text{S}), C_2(\\text{I}), C_3(\\text{I})$, Mem(clean).\n3.  **$t=50$: $C_0$ issues a write to $X$.**\n    - Completion at $t=54$. States: $C_0(\\text{M}), C_1(\\text{I}), C_2(\\text{I}), C_3(\\text{I})$, Mem(stale).\n\n4.  **$t=60$: $C_2$ issues BusRd for $X$.**\n    - $C_2$ has a read miss and broadcasts BusRd.\n    - $C_0$ snoops the request and holds the line in M. It must supply the data.\n    - Data arrives at $C_2$ at $t = 60 + L_{\\mathrm{cc}} = 68$.\n    - Here, the MOESI protocol diverges. The rule states: \"the supplier downgrades to Owned and memory remains stale.\"\n    - At transaction completion ($t=68$), $C_0$ transitions from M to Owned (O), and $C_2$ transitions to S. Memory is not updated.\n    - States at $t=68$: $C_0(\\text{O}), C_1(\\text{I}), C_2(\\text{S}), C_3(\\text{I})$, Mem(stale).\n\n5.  **$t=80$: $C_3$ issues BusRdX for $X$.**\n    - $C_3$ has a write miss and broadcasts BusRdX.\n    - $C_0$ (in state O) and $C_2$ (in state S) are sharers. They must invalidate and send acks.\n    - Acks arrive at $t = 80 + L_{\\mathrm{inv}} = 84$.\n    - The rule states that a cache in state O must supply data. $C_0$ is the owner and holds the most recent copy. Main memory is stale.\n    - $C_0$ supplies the data to $C_3$. This is a cache-to-cache transfer.\n    - Data arrives at $C_3$ at $t = 80 + L_{\\mathrm{cc}} = 88$.\n    - $C_3$ can perform its write after both data and acks have arrived. Completion time is $\\max(88, 84) = 88$.\n    - Thus, $T_{\\mathrm{MOESI}} = 88$ cycles.\n\n**Final Calculation**\n\nThe problem asks for the difference $\\Delta T = T_{\\mathrm{MESI}} - T_{\\mathrm{MOESI}}$.\n- $T_{\\mathrm{MESI}} = 100$ cycles.\n- $T_{\\mathrm{MOESI}} = 88$ cycles.\n$$\n\\Delta T = T_{\\mathrm{MESI}} - T_{\\mathrm{MOESI}} = 100 - 88 = 12\n$$\nThe difference in completion time arises because in the final step, the data for $C_3$'s request is supplied by slow main memory ($L_{\\mathrm{mem}}=20$) in the MESI case, whereas it is supplied by a fast cache-to-cache transfer ($L_{\\mathrm{cc}}=8$) from the Owned cache in the MOESI case.",
            "answer": "$$\\boxed{12}$$"
        },
        {
            "introduction": "While tracing individual operations is fundamental, a key goal in computer architecture is to predict system-level performance. This exercise elevates our analysis from a specific scenario to a probabilistic model, asking you to quantify the average latency savings of MOESI over MESI . By considering a large number of random memory requests, you will derive the expected performance gain, a crucial skill for evaluating and comparing architectural designs.",
            "id": "3658472",
            "problem": "Consider a symmetric multiprocessor with $N=8$ cores using a snoop-based cache coherence protocol. Two coherence protocols are available: Modified, Exclusive, Shared, Invalid (MESI) and Modified, Owned, Exclusive, Shared, Invalid (MOESI). In the Modified, Exclusive, Shared, Invalid (MESI) protocol, when a core performs a read to a cache line that is currently in the Modified state in another core, the modified core must first write back the line to memory before the requester can obtain the data, so the read is effectively served by main memory. In the Modified, Owned, Exclusive, Shared, Invalid (MOESI) protocol, the Owned state permits the cache that holds the modified line to directly supply the data to the requester without an immediate writeback to memory.\n\nAssume the following latencies:\n- Cache-to-cache transfer latency $t_{cc} = 20\\,\\text{ns}$.\n- Main memory (dynamic random-access memory) latency $t_{\\text{dram}} = 80\\,\\text{ns}$.\n\nSuppose $k=1000$ read requests are issued by cores, and each request targets a cache line that is currently in the Modified state in exactly one of the $N$ caches. Model the selection of the requesting core and the owning core (the one holding the Modified line) as independent and uniformly random over the $N$ cores. Under these assumptions, derive from first principles the expected total latency savings achieved by the Modified, Owned, Exclusive, Shared, Invalid (MOESI) protocol relative to the Modified, Exclusive, Shared, Invalid (MESI) protocol due to owner-served transfers, and then evaluate the expression numerically using the parameter values above. Express the final latency savings in nanoseconds and round your answer to four significant figures.",
            "solution": "The objective is to find the expected total latency savings of the MOESI protocol relative to the MESI protocol for $k$ read requests. The total savings will be the product of the number of requests and the expected saving per request. Let us first analyze the latency for a single read request.\n\nLet $C_r$ be the core issuing the read request and $C_o$ be the core that owns the cache line in the Modified state. Both $C_r$ and $C_o$ are drawn independently and uniformly from the set of $N$ cores. There are two distinct cases for any single request:\n1.  The requesting core is the same as the owning core, i.e., $C_r = C_o$.\n2.  The requesting core is different from the owning core, i.e., $C_r \\neq C_o$.\n\nWe first determine the probabilities of these two events. Since the selections are independent and uniform over $N$ possibilities, there are $N \\times N = N^2$ equally likely pairs of $(C_r, C_o)$.\nThe number of pairs where $C_r = C_o$ is $N$ (i.e., $(1,1), (2,2), \\dots, (N,N)$). The probability of this event is:\n$$ P(C_r = C_o) = \\frac{N}{N^2} = \\frac{1}{N} $$\nThe probability of the complementary event, $C_r \\neq C_o$, is:\n$$ P(C_r \\neq C_o) = 1 - P(C_r = C_o) = 1 - \\frac{1}{N} = \\frac{N-1}{N} $$\n\nNow, let's analyze the latency for each protocol based on these cases.\nIn the case where $C_r = C_o$, the core is reading a cache line that it already possesses in a modifiable state. This is a local cache hit. The latency for a local hit, let's call it $t_{\\text{hit}}$, is independent of the coherence protocol (MESI or MOESI) as no inter-core communication is needed.\n\nIn the case where $C_r \\neq C_o$, the read request is a coherence miss. The latency depends on the protocol:\n-   **MESI Protocol:** According to the problem description, the owning core $C_o$ must first write the modified data back to main memory. Then, the requesting core $C_r$ reads the data from main memory. The latency of this operation is dominated by the access to main memory, which is $t_{\\text{dram}}$.\n-   **MOESI Protocol:** The protocol allows the owning core $C_o$ to directly forward the data to the requesting core $C_r$ via a cache-to-cache transfer. The latency for this operation is $t_{cc}$.\n\nLet $L_{MESI}$ and $L_{MOESI}$ be the random variables for the latency of a single request under each protocol. Using the law of total expectation, we can write the expected latencies:\n$$ E[L_{MESI}] = P(C_r = C_o) \\cdot t_{\\text{hit}} + P(C_r \\neq C_o) \\cdot t_{\\text{dram}} $$\n$$ E[L_{MOESI}] = P(C_r = C_o) \\cdot t_{\\text{hit}} + P(C_r \\neq C_o) \\cdot t_{cc} $$\n\nSubstituting the probabilities:\n$$ E[L_{MESI}] = \\frac{1}{N} t_{\\text{hit}} + \\frac{N-1}{N} t_{\\text{dram}} $$\n$$ E[L_{MOESI}] = \\frac{1}{N} t_{\\text{hit}} + \\frac{N-1}{N} t_{cc} $$\n\nThe expected latency saving for a single request, $\\Delta L$, is the difference between the expected latencies:\n$$ \\Delta L = E[L_{MESI}] - E[L_{MOESI}] $$\n$$ \\Delta L = \\left(\\frac{1}{N} t_{\\text{hit}} + \\frac{N-1}{N} t_{\\text{dram}}\\right) - \\left(\\frac{1}{N} t_{\\text{hit}} + \\frac{N-1}{N} t_{cc}\\right) $$\nThe term involving $t_{\\text{hit}}$ cancels out, which is expected as the savings only occur in the $C_r \\neq C_o$ case.\n$$ \\Delta L = \\frac{N-1}{N} t_{\\text{dram}} - \\frac{N-1}{N} t_{cc} $$\n$$ \\Delta L = \\frac{N-1}{N} (t_{\\text{dram}} - t_{cc}) $$\n\nThis expression represents the expected latency saving per request. The total expected latency saving for $k$ independent requests, $\\Delta L_{\\text{total}}$, is $k$ times the saving per request:\n$$ \\Delta L_{\\text{total}} = k \\cdot \\Delta L = k \\frac{N-1}{N} (t_{\\text{dram}} - t_{cc}) $$\n\nNow we substitute the given numerical values: $k = 1000$, $N=8$, $t_{\\text{dram}} = 80\\,\\text{ns}$, and $t_{cc} = 20\\,\\text{ns}$.\n$$ \\Delta L_{\\text{total}} = 1000 \\cdot \\frac{8-1}{8} (80 - 20) $$\n$$ \\Delta L_{\\text{total}} = 1000 \\cdot \\frac{7}{8} (60) $$\n$$ \\Delta L_{\\text{total}} = 1000 \\cdot 0.875 \\cdot 60 $$\n$$ \\Delta L_{\\text{total}} = 875 \\cdot 60 $$\n$$ \\Delta L_{\\text{total}} = 52500 $$\nThe units of the result are nanoseconds, as the input latencies were in nanoseconds. The problem requires the answer to be rounded to four significant figures. In scientific notation, this is $5.250 \\times 10^4$.",
            "answer": "$$\n\\boxed{5.250 \\times 10^{4}}\n$$"
        },
        {
            "introduction": "The performance of a multicore system depends not only on the hardware's coherence protocol but also on how software organizes data in memory. This exercise explores \"false sharing,\" a common and costly performance pitfall where unrelated data items reside on the same cache line, causing excessive invalidation traffic . By quantifying the number of invalidations before and after a simple data layout transformation, you will understand the critical link between software design and hardware efficiency.",
            "id": "3658524",
            "problem": "A symmetric multiprocessor has $4$ identical cores, each with a private level-$1$ data cache that is write-back, write-allocate, and enforces coherence by a snooping protocol that is either Modified, Exclusive, Shared, Invalid (MESI) or Modified, Owned, Exclusive, Shared, Invalid (MOESI). The cache line size is $64$ bytes. All caches are initially empty.\n\nConsider a data structure consisting of $4$ unrelated $8$-byte counters laid out contiguously in memory at addresses $A_0 + 8k$ for $k \\in \\{0,1,2,3\\}$, where $A_0$ is aligned to a $64$-byte cache line boundary. Thus, all $4$ counters occupy the same cache line. Core $k$ repeatedly executes a tight loop that increments its own counter at address $A_0 + 8k$ exactly $W$ times, where $W = 10^{6}$. There are no other memory accesses to these addresses. The execution is scheduled in strict round-robin order across cores, one store per core per time slot, so that ownership of the shared cache line alternates between cores on every store. Define an invalidation event as a coherence-induced transition of a cache line in some core from a valid state ($M$, $E$, $S$, or $O$) to the invalid state ($I$) because of another core’s request.\n\nUsing only the standard semantics of MESI and MOESI states, write-allocate behavior, and the fact that two addresses map to the same cache line if and only if they fall within the same $64$-byte block, answer the following:\n\n- Derive, under MESI, the total number of invalidation events caused by the $4W$ stores to the $4$ counters in the given layout and schedule.\n- Argue whether this total changes under MOESI for this specific write–write pattern and justify your conclusion from first principles.\n- Propose a transformation that segregates these $4$ unrelated counters across cache lines (for example, padding so that each counter starts at the beginning of a distinct $64$-byte line with $64$-byte alignment), and derive the total number of invalidation events under the same schedule and $W$ after this transformation.\n\nLet $D$ denote the difference between the total number of invalidation events before the transformation and the total number after the transformation. Compute $D$ as a single integer. Express your final answer as an integer with no units. Do not round.",
            "solution": "The problem asks for an analysis of cache coherence-induced invalidations in a symmetric multiprocessor system under two different data layouts. The core of the problem is to understand the state transitions of a cache line under MESI and MOESI protocols in a \"false sharing\" scenario versus a scenario with proper data alignment.\n\nFirst, we establish the fundamental states and transitions relevant to the problem. The MESI protocol defines four states for a cache line:\n- **Modified ($M$)**: The line is present only in the current cache, is dirty (modified), and its value in main memory is stale.\n- **Exclusive ($E$)**: The line is present only in the current cache, is clean, and matches the value in main memory.\n- **Shared ($S$)**: The line is present in this cache and at least one other cache, is clean, and matches main memory.\n- **Invalid ($I$)**: The line is not present in the cache.\n\nThe MOESI protocol adds a fifth state:\n- **Owned ($O$)**: The line is present in this cache and may be present in other caches in the $S$ state. The line is dirty, and this cache is responsible for writing it back to memory. Main memory's copy is stale. The $O$ state allows sharing of dirty data without a write-back to memory.\n\nAn invalidation event is defined as a transition from any valid state ($M$, $E$, $S$, or $O$) to the $I$ state, triggered by another core's bus request. A write miss by a core causes it to issue a read-with-intent-to-modify (RWITM) bus request, often denoted as `BusRdX`. When a cache holding a line in state $M$ or $O$ snoops a `BusRdX` from another core for that same line, it must relinquish ownership and invalidate its copy.\n\nThe system has $4$ cores, and each performs $W = 10^6$ increments. The total number of stores is $4W$. The schedule is a strict round-robin of stores: $C_0, C_1, C_2, C_3, C_0, C_1, \\ldots$.\n\n**Part 1: Analysis Before Transformation (False Sharing)**\n\nIn this scenario, all $4$ counters, each of size $8$ bytes, reside in a single $64$-byte cache line, as the base address $A_0$ is aligned and $4 \\times 8 = 32 < 64$. This is a classic false sharing situation, where logically independent data items cause coherence traffic because they physically share a cache line.\n\n**Derivation of Invalidations under MESI:**\nLet's trace the state of the shared cache line across the $4$ cores, $C_0, C_1, C_2, C_3$. Initially, the line is $I$ in all caches.\n\n1.  **Store 1 (Core $C_0$):** $C_0$ has a write miss. It issues a `BusRdX` for the line. Since all other caches have the line in state $I$, main memory provides the data. $C_0$'s cache loads the line and sets its state to $M$. No other core holds a valid copy.\n    - State: $C_0:M, C_1:I, C_2:I, C_3:I$.\n    - Invalidation Events: $0$.\n\n2.  **Store 2 (Core $C_1$):** $C_1$ has a write miss and issues a `BusRdX`. $C_0$ snoops this request and sees it has the line in state $M$. To maintain coherence, $C_0$ must provide the data to $C_1$ (or let it go to memory first, while writing back its version) and then invalidate its own copy. $C_0$'s line state transitions from $M \\to I$. $C_1$ loads the line and sets its state to $M$.\n    - State: $C_0:I, C_1:M, C_2:I, C_3:I$.\n    - Invalidation Events: $1$ (in $C_0$).\n\n3.  **Store 3 (Core $C_2$):** $C_2$ has a write miss and issues a `BusRdX`. $C_1$ snoops this request, has the line in state $M$, provides the data, and invalidates its copy ($M \\to I$). $C_2$ sets its line state to $M$.\n    - State: $C_0:I, C_1:I, C_2:M, C_3:I$.\n    - Invalidation Events: $1$ (in $C_1$).\n\nThis pattern repeats. For every store from the second store onwards, the core that performed the previous store holds the line exclusively in the $M$ state. The current core's `BusRdX` request forces the previous owner to invalidate its copy.\n\nThe total number of stores is $4W$. The first store causes $0$ invalidations. Each of the subsequent $(4W-1)$ stores causes exactly one invalidation.\nTherefore, the total number of invalidation events before the transformation, $N_{\\text{before}}$, is:\n$$N_{\\text{before}} = 0 + (4W-1) \\times 1 = 4W-1$$\n\n**Justification for MOESI:**\nWe must argue whether this total changes under the MOESI protocol. The MOESI protocol introduces the $O$ state to optimize read-sharing of dirty lines. A transition from $M$ to $O$ happens when a core holding a line in state $M$ snoops a read request (`BusRd`). The owner supplies the data to the requester (which enters state $S$) and transitions its own state to $O$.\n\nHowever, the workload described in this problem consists *exclusively* of writes (increments are read-modify-write operations, which culminate in a store and, from a coherence perspective, are treated as writes that require exclusive ownership). A write miss always generates a `BusRdX` request. When a core in state $M$ (or $O$) snoops a `BusRdX`, it must invalidate its copy. The $O$ state is never entered because there are no simple read requests from other cores that would trigger the $M \\to O$ transition. The sequence of events is therefore identical to the MESI protocol. The cache line is always in state $M$ in exactly one cache and $I$ in all others. The total number of invalidations remains $4W-1$ under MOESI for this specific write-only, migratory access pattern.\n\n**Part 2: Analysis After Transformation**\n\nThe proposed transformation involves padding the data structure so that each of the $4$ counters resides on its own separate, dedicated $64$-byte cache line. Let's denote these lines as $L_0, L_1, L_2, L_3$. Core $C_k$ exclusively accesses line $L_k$.\n\nLet's analyze the accesses for a single core, say $C_0$ accessing line $L_0$.\n1.  **First Store by $C_0$:** $C_0$ has a write miss on line $L_0$. It issues a `BusRdX` for $L_0$. Since this is a distinct line, no other core has a copy. The line is fetched from memory, and $C_0$'s cache sets the state of $L_0$ to $M$.\n    - Invalidation Events: $0$.\n\n2.  **Subsequent $W-1$ Stores by $C_0$:** For all subsequent increments, $C_0$ finds line $L_0$ in its cache in state $M$. These are write hits. The writes are performed locally without generating any bus traffic.\n\nCrucially, no other core ($C_1, C_2, C_3$) ever accesses line $L_0$. An invalidation of $L_0$ in $C_0$'s cache would only occur if another core issued a `BusRd` or `BusRdX` for $L_0$. As this never happens, the line $L_0$ remains in state $M$ in $C_0$'s cache for the duration of its $W$ writes after the initial miss.\n\nThe same logic applies independently to cores $C_1, C_2,$ and $C_3$ and their respective dedicated lines $L_1, L_2,$ and $L_3$. Each core loads its line once, and it remains in state $M$ in that core's cache. No coherence traffic is generated for these lines after the initial compulsory miss for each.\n\nConsequently, there are zero coherence-induced invalidation events. The total number of invalidations after the transformation, $N_{\\text{after}}$, is:\n$$N_{\\text{after}} = 0$$\nThis result is independent of whether the protocol is MESI or MOESI.\n\n**Part 3: Final Calculation**\n\nThe problem asks for the difference, $D$, between the total number of invalidations before and after the transformation.\nWe are given $W = 10^6$.\n\n$$D = N_{\\text{before}} - N_{\\text{after}}$$\n$$D = (4W - 1) - 0$$\n$$D = 4 \\times 10^6 - 1$$\n$$D = 4,000,000 - 1$$\n$$D = 3,999,999$$\n\nThis substantial difference highlights the severe performance penalty of false sharing and the effectiveness of data padding as a mitigation strategy.",
            "answer": "$$\\boxed{3999999}$$"
        }
    ]
}