{
    "hands_on_practices": [
        {
            "introduction": "Modern processors boost performance using speculative execution, which heavily relies on branch predictors to guess the direction of program flow. This very mechanism, however, can become a security flaw. In this exercise , we explore how the state of a shared branch predictor can create a side-channel, allowing one security domain to infer information about another. Your task is to analyze a fundamental mitigation strategy—isolating the predictor's history tables—and to quantify its direct hardware cost by deriving the storage overhead as a function of the number of predictor entries $H$ and security domains $D$.",
            "id": "3645422",
            "problem": "A modern out-of-order processor uses speculation guided by branch prediction. In the baseline design, the predictor maintains a single Branch History Table (BHT), defined as an array indexed by low-order bits of the program counter, where each entry is a saturating counter used to predict branch direction. Cross-domain interactions can create microarchitectural side channels, where predictor state learned in one security domain influences prediction behavior in another, potentially violating the noninterference principle in information flow security. One widely used mitigation in hardware security is to isolate predictor state across domains so that the predictor behavior does not carry information from one domain to another.\n\nConsider the following isolation mechanism: the processor maintains per-domain BHTs, one BHT for each security domain, and switches the active BHT based on a domain identifier when the operating system changes security domains. Assume the following:\n- The baseline predictor uses a single BHT with $H$ entries.\n- Each BHT entry stores a $2$-bit saturating counter (a well-tested standard in branch prediction).\n- In the isolated design, there are $D$ security domains, each with its own BHT of $H$ entries, with the same $2$-bit saturating counters per entry.\n- The processor also maintains a domain identifier register that encodes the current domain with enough bits to represent all $D$ domains.\n- Ignore all non-memory overheads (for example, combinational selection logic, wires, and control flip-flops other than the domain identifier register). Count only stored bits in memory arrays and the domain identifier register as storage.\n\nStarting from the definitions above and the security objective of noninterference, derive and express the total storage overhead of the isolated design relative to the baseline design as a single closed-form analytic expression in terms of $H$ and $D$. Express your final answer in bits. Do not perform any numerical substitutions. Do not round; return the exact symbolic expression.",
            "solution": "The problem statement is coherent and valid. It is scientifically grounded in the principles of computer architecture and information flow security, specifically addressing side-channel attacks in branch predictors. The problem is well-posed, providing all necessary definitions and variables to derive a unique analytical solution. It is objective and free from ambiguity or contradiction.\n\nThe objective is to derive an expression for the total storage overhead of an isolated branch predictor design relative to a baseline design. The overhead is defined as the additional storage, measured in bits, required by the isolated design.\n\nLet $S_{baseline}$ be the total storage of the baseline design in bits.\nLet $S_{isolated}$ be the total storage of the isolated design in bits.\nThe storage overhead, $S_{overhead}$, is then defined as:\n$$S_{overhead} = S_{isolated} - S_{baseline}$$\n\nFirst, we calculate the storage for the baseline design, $S_{baseline}$. The baseline design consists of a single Branch History Table (BHT).\n- The BHT has $H$ entries.\n- Each entry is a $2$-bit saturating counter.\nThe total storage for the baseline BHT is the product of the number of entries and the size of each entry in bits.\n$$S_{baseline} = H \\times 2$$\n\nNext, we calculate the storage for the isolated design, $S_{isolated}$. The problem specifies that this design includes two components whose storage must be counted: the per-domain BHTs and a domain identifier register.\n1.  **Storage for the BHTs:**\n    - There are $D$ security domains.\n    - For each domain, there is a dedicated BHT with $H$ entries.\n    - Each entry in every BHT is a $2$-bit saturating counter.\n    The total storage for all $D$ BHTs is the product of the number of domains, the number of entries per BHT, and the size of each entry.\n    $$S_{BHTs\\_isolated} = D \\times H \\times 2 = 2DH$$\n\n2.  **Storage for the domain identifier register:**\n    - This register must encode the current security domain, of which there are $D$.\n    - To represent $D$ distinct states, the minimum number of bits required, denoted by $k$, must satisfy the inequality $2^k \\ge D$.\n    - Solving for $k$ gives $k \\ge \\log_2(D)$. Since the number of bits must be an integer, we take the ceiling of this value.\n    - The storage for the domain identifier register is therefore:\n    $$S_{register} = \\lceil \\log_2(D) \\rceil$$\n\nThe total storage for the isolated design, $S_{isolated}$, is the sum of the storage for the BHTs and the domain identifier register.\n$$S_{isolated} = S_{BHTs\\_isolated} + S_{register} = 2DH + \\lceil \\log_2(D) \\rceil$$\n\nFinally, we compute the storage overhead, $S_{overhead}$, by subtracting the baseline storage from the isolated design storage.\n$$S_{overhead} = S_{isolated} - S_{baseline}$$\n$$S_{overhead} = \\left( 2DH + \\lceil \\log_2(D) \\rceil \\right) - (2H)$$\nRearranging the terms and factoring out the common factor $2H$ yields the final closed-form expression for the overhead:\n$$S_{overhead} = 2DH - 2H + \\lceil \\log_2(D) \\rceil$$\n$$S_{overhead} = 2H(D-1) + \\lceil \\log_2(D) \\rceil$$\nThis expression represents the total additional storage in bits required by the isolated design, in terms of the number of BHT entries $H$ and the number of security domains $D$.",
            "answer": "$$\\boxed{2H(D-1) + \\lceil \\log_{2}(D) \\rceil}$$"
        },
        {
            "introduction": "The memory management system is another critical area where performance optimizations can clash with security. This practice  focuses on the Translation Lookaside Buffer (TLB) and the security risks posed by performance-enhancing huge pages when shared across domains. Acting as a system designer, you will implement a security policy to prevent such leakage by selectively disabling huge pages, and your challenge is to calculate the real-world performance penalty of this decision. This involves modeling the application's memory access behavior and computing the expected increase in DTLB misses per time slice.",
            "id": "3645354",
            "problem": "A system designer is hardening a multiprogrammed, $64$-bit system against cross-domain leakage via shared Translation Lookaside Buffer (TLB) entries when using huge pages. Consider a single out-of-order core that time-slices two security domains, Domain A and Domain B, with Domain A running in $1$ millisecond quanta. For isolation, the operating system enforces the following secure policy: disable $1$ gigabyte pages, allow $2$ megabyte pages only for per-domain private regions, map any region visible in more than one domain using $4$ kilobyte pages, and flush all TLB structures on every domain switch because Process-Context Identifiers (PCID) are not used.\n\nAssume the following hardware configuration and workload properties for Domain A while it is running:\n- Data Translation Lookaside Buffer (DTLB) is partitioned by page size and is fully associative in each partition with perfect least recently used replacement. It has $E_{4\\mathrm{K}} = 256$ entries for $4$ kilobyte pages, $E_{2\\mathrm{M}} = 64$ entries for $2$ megabyte pages, and $E_{1\\mathrm{G}} = 8$ entries for $1$ gigabyte pages. Under the secure policy, $1$ gigabyte pages are disabled.\n- Domain A’s data footprint during its time slice consists of a private hot region of size $S_{2\\mathrm{M}} = 256$ megabytes, eligible for $2$ megabyte pages under the secure policy, and a shared read-only hot region of size $S_{4\\mathrm{K}} = 4$ megabytes, which must be mapped with $4$ kilobyte pages. Accesses are uniformly random within each region. The fraction of data references targeting the private region is $p = 0.8$, and the fraction targeting the shared region is $1 - p$.\n- The core runs at frequency $f = 3.0$ gigahertz, retires $r = 2.5$ instructions per cycle on average, and performs $\\alpha = 0.4$ data memory references per retired instruction. Each data memory reference requires a DTLB translation.\n\nAdopt the secure policy as stated, and evaluate the effect of avoiding shared huge pages on TLB behavior by computing the expected number of DTLB misses incurred by Domain A per $1$ millisecond slice. Assume a TLB flush occurs at the start of each slice. State any modeling assumptions you use that are consistent with well-tested models in the literature, and base your derivation on first principles such as working-set cardinality, page counts, and simple probability for uniformly random references.\n\nExpress the final answer in misses per millisecond, and round your answer to $4$ significant figures.",
            "solution": "The problem as stated is valid, self-contained, and consistent with established principles of computer architecture and performance analysis. We can proceed to compute the expected number of Data Translation Lookaside Buffer (DTLB) misses for Domain A during its time slice.\n\nThe total number of expected DTLB misses is the sum of misses from accesses to the private region and misses from accesses to the shared region. The calculation proceeds as follows:\n\nFirst, we determine the total number of data memory references made by Domain A during its time slice. Each data memory reference requires a DTLB lookup.\nLet $T$ be the time slice duration, $f$ be the core frequency, $r$ be the average instructions retired per cycle (IPC), and $\\alpha$ be the average number of data memory references per instruction.\nThe given values are:\nTime slice, $T = 1 \\text{ ms} = 1 \\times 10^{-3} \\text{ s}$\nFrequency, $f = 3.0 \\text{ GHz} = 3.0 \\times 10^9 \\text{ cycles/s}$\nIPC, $r = 2.5 \\text{ instructions/cycle}$\nData references per instruction, $\\alpha = 0.4 \\text{ references/instruction}$\n\nThe total number of cycles in the time slice is:\n$$C = T \\times f = (1 \\times 10^{-3} \\text{ s}) \\times (3.0 \\times 10^9 \\text{ cycles/s}) = 3.0 \\times 10^6 \\text{ cycles}$$\nThe total number of instructions retired is:\n$$I = C \\times r = (3.0 \\times 10^6 \\text{ cycles}) \\times (2.5 \\text{ instructions/cycle}) = 7.5 \\times 10^6 \\text{ instructions}$$\nThe total number of data memory references, which is the total number of DTLB accesses, is:\n$$N_{ref} = I \\times \\alpha = (7.5 \\times 10^6 \\text{ instructions}) \\times (0.4 \\text{ references/instruction}) = 3.0 \\times 10^6 \\text{ references}$$\n\nNext, we analyze the DTLB misses for the private and shared regions separately. The DTLB is partitioned by page size, so accesses to different regions do not interfere with each other's TLB entries.\n\nAnalysis for the private region (using $2$ MB pages):\nThe fraction of references to the private region is $p = 0.8$. The number of references to this region is:\n$$N_{ref, 2\\mathrm{M}} = N_{ref} \\times p = (3.0 \\times 10^6) \\times 0.8 = 2.4 \\times 10^6$$\nThe size of the private region is $S_{2\\mathrm{M}} = 256 \\text{ MB}$. Under the secure policy, this region is mapped using $2 \\text{ MB}$ pages, so $P_{2\\mathrm{M}} = 2 \\text{ MB}$. The number of unique pages in this region's working set is:\n$$W_{2\\mathrm{M}} = \\frac{S_{2\\mathrm{M}}}{P_{2\\mathrm{M}}} = \\frac{256 \\text{ MB}}{2 \\text{ MB}} = 128 \\text{ pages}$$\nThe DTLB provides $E_{2\\mathrm{M}} = 64$ entries for $2 \\text{ MB}$ pages. Since the working set size ($W_{2\\mathrm{M}} = 128$) is greater than the DTLB capacity ($E_{2\\mathrm{M}} = 64$), capacity misses are inevitable after the initial compulsory misses.\n\nWe adopt a standard model for a fully associative cache with LRU replacement and a uniform random access pattern. For a large number of accesses, the steady-state miss rate, $m$, for a working set of $W$ pages and a cache capacity of $E$ pages (where $W > E$) is the fraction of the working set that does not fit in the cache.\n$$m = \\frac{W - E}{W} = 1 - \\frac{E}{W}$$\nThis model is appropriate here given the accesses are uniformly random and the number of references is much larger than the working set size, justifying the use of a steady-state approximation that accounts for both compulsory and capacity misses over the long run.\n\nThe miss rate for the private region is:\n$$m_{2\\mathrm{M}} = \\frac{W_{2\\mathrm{M}} - E_{2\\mathrm{M}}}{W_{2\\mathrm{M}}} = \\frac{128 - 64}{128} = \\frac{64}{128} = 0.5$$\nThe expected number of misses for the private region is:\n$$N_{miss, 2\\mathrm{M}} = N_{ref, 2\\mathrm{M}} \\times m_{2\\mathrm{M}} = (2.4 \\times 10^6) \\times 0.5 = 1,200,000$$\n\nAnalysis for the shared region (using $4$ KB pages):\nThe fraction of references to the shared region is $1 - p = 0.2$. The number of references is:\n$$N_{ref, 4\\mathrm{K}} = N_{ref} \\times (1 - p) = (3.0 \\times 10^6) \\times 0.2 = 600,000$$\nThe size of the shared region is $S_{4\\mathrm{K}} = 4 \\text{ MB}$. Under the secure policy, this region is mapped using $4 \\text{ KB}$ pages, so $P_{4\\mathrm{K}} = 4 \\text{ KB}$. The number of unique pages in this region's working set is:\n$$W_{4\\mathrm{K}} = \\frac{S_{4\\mathrm{K}}}{P_{4\\mathrm{K}}} = \\frac{4 \\text{ MB}}{4 \\text{ KB}} = \\frac{4 \\times 1024 \\text{ KB}}{4 \\text{ KB}} = 1024 \\text{ pages}$$\nThe DTLB provides $E_{4\\mathrm{K}} = 256$ entries for $4 \\text{ KB}$ pages. Here again, the working set size ($W_{4\\mathrm{K}} = 1024$) exceeds the DTLB capacity ($E_{4\\mathrm{K}} = 256$).\nUsing the same miss rate model, the miss rate for the shared region is:\n$$m_{4\\mathrm{K}} = \\frac{W_{4\\mathrm{K}} - E_{4\\mathrm{K}}}{W_{4\\mathrm{K}}} = \\frac{1024 - 256}{1024} = \\frac{768}{1024} = 0.75$$\nThe expected number of misses for the shared region is:\n$$N_{miss, 4\\mathrm{K}} = N_{ref, 4\\mathrm{K}} \\times m_{4\\mathrm{K}} = 600,000 \\times 0.75 = 450,000$$\n\nFinally, the total expected number of DTLB misses per $1 \\text{ ms}$ slice is the sum of the misses from both regions:\n$$N_{miss} = N_{miss, 2\\mathrm{M}} + N_{miss, 4\\mathrm{K}} = 1,200,000 + 450,000 = 1,650,000$$\nThe problem requires the answer to be rounded to $4$ significant figures. The result is $1,650,000$, which can be written in scientific notation as $1.650 \\times 10^6$.",
            "answer": "$$\\boxed{1.650 \\times 10^{6}}$$"
        },
        {
            "introduction": "Ultimately, even if all data is encrypted, the mere pattern of memory locations a program accesses can reveal secret information to an adversary monitoring the system bus. This final practice  introduces you to a powerful countermeasure known as Oblivious RAM (ORAM), which aims to make all memory access patterns indistinguishable. You will analyze a simplified hardware scheme that enforces a fixed sequence of reads and writes for every logical memory operation. Your goal is to derive the fundamental cost of this obliviousness by calculating the extra memory bus traffic, $\\tau$, this design imposes compared to a standard, non-oblivious system.",
            "id": "3645349",
            "problem": "A processor implements hardware support for Oblivious Random Access Memory (ORAM)-like oblivious memory access to mitigate memory-bus side channels in a single-socket system. The threat model assumes an adversary who can observe the memory-bus address stream and the sequence of reads and writes but not the values transferred. To ensure that each logical memory access is indistinguishable from any other, the controller enforces a fixed external memory access pattern per logical access.\n\nThe memory system transfers data in fixed-size cache lines of size $L$ bytes. A specialized on-chip staging buffer of capacity $S$ lines holds recently used and decoy lines and is not directly observable on the memory bus. For each logical memory access to one line, the controller performs the following external-memory sequence:\n\n- Stage a contiguous window of $W$ lines from main memory into the staging buffer by issuing $W$ full-line reads, where the window location is a deterministic function of the logical time index and is independent of the secret address. The window always contains the target line.\n- After processing in the staging buffer, write back the same window by issuing $W$ full-line writes, even if no data changed in some lines, so that the read/write pattern is invariant.\n- Evict $E$ lines from the staging buffer to main memory on every logical access to keep the staging buffer occupancy bounded. To make read/write counts per access independent of data values and the access type, each eviction consists of a full-line read (to standardize the operation) followed by a full-line write, using decoy addresses chosen by a pseudo-random function seeded per epoch and independent of the secret address.\n\nAssume that the baseline (non-oblivious) system transfers $L$ bytes for each logical read and transfers an additional $L$ bytes for a logical write, with the fraction of logical accesses that are writes equal to $\\alpha$, where $0 \\leq \\alpha \\leq 1$. All transfers are full-line and there is no compression.\n\nUsing only these assumptions and the requirement that the observable trace per access must be of fixed length and composition to achieve obliviousness, derive a closed-form analytic expression for the extra external memory traffic per logical access, $\\tau$, measured in bytes per access, introduced by the ORAM-like staging-buffer design relative to the baseline. Express your answer as a single analytic expression in terms of $L$, $W$, $E$, and $\\alpha$. Do not include units in your final boxed answer. If you choose to simplify, ensure algebraic steps are valid; rounding is not required.",
            "solution": "The adversary observes the memory bus, including addresses and whether each operation is a read or write. To achieve obliviousness, the observable sequence per logical access must be invariant with respect to the secret address and data; that is, each access must induce the same number of reads and writes to externally visible memory locations, independent of the access type and contents. This motivates issuing a fixed set of memory operations per logical access.\n\nWe first quantify the expected baseline traffic per logical access, $T_{\\text{base}}$. A logical read transfers $L$ bytes. The problem states that a logical write transfers an \"additional $L$ bytes,\" implying a total traffic of $2L$ per write (e.g., for a read-for-ownership and subsequent write). Given a write fraction of $\\alpha$, the expected baseline traffic is the weighted average of read and write traffic:\n$$\nT_{\\text{base}} = (1-\\alpha)L + \\alpha(2L) = L(1+\\alpha).\n$$\n\nNow consider the oblivious scheme. Per logical access, the controller performs:\n- A window stage consisting of $W$ full-line reads and $W$ full-line writes. This contributes $W$ reads and $W$ writes, totaling $2W$ line transfers.\n- Evictions consisting of $E$ full-line reads and $E$ full-line writes. This contributes $E$ reads and $E$ writes, totaling $2E$ line transfers.\n\nBecause each read or write transfers a full line of $L$ bytes and the operations are performed regardless of the access type, the total external memory traffic per logical access under the oblivious scheme is\n$$\nT_{\\text{obl}} = L \\cdot (W+W+E+E) = 2L(W+E).\n$$\n\nThe extra external memory traffic per logical access, $\\tau$, introduced by the oblivious scheme relative to the baseline is the difference:\n$$\n\\tau = T_{\\text{obl}} - T_{\\text{base}}.\n$$\nSubstituting the expressions derived above,\n$$\n\\tau = 2L(W+E) - L(1+\\alpha).\n$$\n\nFactoring out $L$,\n$$\n\\tau = L\\left(2(W+E)-(1+\\alpha)\\right).\n$$\n\nThis expression is a closed-form analytic function of $L$, $W$, $E$, and $\\alpha$. It quantifies the overhead of enforcing a fixed access pattern (a total of $W+E$ reads and $W+E$ writes per logical access) to achieve obliviousness. The difference between this fixed traffic and the average baseline traffic quantifies the overhead of enforcing this oblivious access pattern.",
            "answer": "$$\\boxed{L\\left(2(W+E)-(1+\\alpha)\\right)}$$"
        }
    ]
}