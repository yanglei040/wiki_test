{
    "hands_on_practices": [
        {
            "introduction": "The power of a Very Long Instruction Word (VLIW) architecture comes from the compiler's ability to pack multiple independent operations into a single instruction bundle. However, this packing is constrained by the hardware's available functional units and their specific operational rules. This first exercise provides a foundational look at these resource constraints, including typed issue slots and the multi-cycle occupancy of complex units like multipliers . By contrasting a correctly formed schedule with a naive one, you will gain a concrete understanding of the structural hazards that a VLIW compiler must avoid to ensure correct and efficient execution.",
            "id": "3681276",
            "problem": "Consider a statically scheduled Very Long Instruction Word (VLIW) processor with one bundle issued per cycle. The bundle has four typed slots: two integer arithmetic logic unit slots $S_{\\text{ALU},0}$ and $S_{\\text{ALU},1}$, one multiply slot $S_{\\text{MUL}}$, and one memory slot $S_{\\text{MEM}}$. Each instruction must be placed in a bundle and targets exactly one slot that matches its type. Assume the following widely accepted base facts and constraints:\n- Static multiple issue schedules must respect true data dependencies: if an instruction with latency $L$ produces a value used by its consumer, and it is issued at cycle $c$, then its consumer may be issued no earlier than cycle $c + L$.\n- Functional unit occupancy constraints must be respected: when an instruction is issued into a slot $s$ with occupancy $O_s$, that slot cannot issue another instruction for $O_s$ consecutive cycles counted from the cycle of issue. This includes multi-cycle operations whose occupancy extends over multiple cycles.\n- Integer addition has latency $L_{\\text{ADD}} = 1$ and occupies its ALU slot for $O_{\\text{ALU}} = 1$ cycle.\n- Multiply has latency $L_{\\text{MUL}} = 3$ and occupies $S_{\\text{MUL}}$ for $O_{\\text{MUL}} = L_{\\text{MUL}}$ cycles.\n- Load has latency $L_{\\text{LD}} = 2$ and occupies $S_{\\text{MEM}}$ for $O_{\\text{MEM}} = 1$ cycle.\n- Store has latency $L_{\\text{ST}} = 1$ and occupies $S_{\\text{MEM}}$ for $O_{\\text{MEM}} = 1$ cycle.\n\nThe program to be scheduled is the following sequence (useful registers are distinct unless a dependency is stated):\n- $I_{1}$: $\\text{LD } r_{1} \\leftarrow [r_{A}]$.\n- $I_{2}$: $\\text{LD } r_{2} \\leftarrow [r_{B}]$.\n- $I_{3}$: $\\text{MUL } r_{3} \\leftarrow r_{1} \\times r_{2}$, depends on $I_{1}$ and $I_{2}$.\n- $I_{4}$: $\\text{ADD } r_{4} \\leftarrow r_{3} + r_{C}$, depends on $I_{3}$.\n- $I_{5}$: $\\text{MUL } r_{5} \\leftarrow r_{4} \\times r_{D}$, depends on $I_{4}$.\n- $I_{6}$: $\\text{ADD } r_{6} \\leftarrow r_{5} + r_{E}$, depends on $I_{5}$.\n- $I_{7}$: $\\text{MUL } r_{7} \\leftarrow r_{6} \\times r_{F}$, depends on $I_{6}$.\n- $I_{8}$: $\\text{ADD } r_{8} \\leftarrow r_{7} + r_{G}$, depends on $I_{7}$.\n- $I_{9}$: $\\text{ST } [r_{H}] \\leftarrow r_{8}$, depends on $I_{8}$.\n- $I_{10}$: $\\text{ADD } r_{10} \\leftarrow r_{11} + r_{12}$, no dependencies.\n- $I_{11}$: $\\text{ADD } r_{13} \\leftarrow r_{14} + r_{15}$, no dependencies.\n- $I_{12}$: $\\text{ADD } r_{16} \\leftarrow r_{17} + r_{18}$, no dependencies.\n- $I_{13}$: $\\text{ADD } r_{19} \\leftarrow r_{20} + r_{21}$, no dependencies.\n\nTask:\n- First, design a conflict-free static schedule that respects both data-dependence latencies and slot occupancy constraints described above. You may assume one bundle is issued per cycle, and you may place independent $\\text{ADD}$ instructions opportunistically to fill available $S_{\\text{ALU},0}$ and $S_{\\text{ALU},1}$ slots without violating any constraints.\n- Second, consider a naive bundling policy that ignores all latencies and occupancy constraints and simply takes the next up to $4$ instructions in program order each cycle, mapping them by type into the matching slots ($\\text{LD}/\\text{ST} \\rightarrow S_{\\text{MEM}}$, $\\text{MUL} \\rightarrow S_{\\text{MUL}}$, $\\text{ADD} \\rightarrow S_{\\text{ALU},0}$ or $S_{\\text{ALU},1}$). Define a bundle conflict to occur whenever, in a given cycle, this naive policy attempts to assign an instruction to a slot that is unavailable in that cycle due to a prior instruction’s occupancy of that same slot (including simultaneous over-subscription of $S_{\\text{MEM}}$ or $S_{\\text{ALU},k}$ within the same cycle or multi-cycle occupancy of $S_{\\text{MUL}}$ across cycles). Count the total number of bundle conflicts that would be produced by this naive policy for the entire program sequence.\n\nProvide only the total number of bundle conflicts as your final answer. No rounding is needed.",
            "solution": "The problem asks for the total number of bundle conflicts produced by a naive bundling policy for a given sequence of instructions on a VLIW processor. A bundle conflict is defined as an attempt by the naive policy to assign an instruction to a slot that is unavailable, either due to a concurrent assignment within the same cycle (over-subscription) or due to the multi-cycle occupancy of a slot from a previous cycle.\n\nThe VLIW processor has four slots per bundle: two integer ALU slots ($S_{\\text{ALU},0}$, $S_{\\text{ALU},1}$), one multiply slot ($S_{\\text{MUL}}$), and one memory slot ($S_{\\text{MEM}}$). The naive policy forms bundles by taking up to four instructions in program order per cycle and mapping them to the appropriately typed slots. For ALU instructions, the policy will fill $S_{\\text{ALU},0}$ first, then $S_{\\text{ALU},1}$.\n\nWe will analyze the scheduling process cycle by cycle, tracking the occupancy of each slot and counting the conflicts as they occur. Let $F_s$ be the first cycle in which slot $s$ is free. Initially, at cycle $C=1$, all slots are available, so $F_s = 1$ for all slots $s$.\n\nThe instruction sequence is:\n- $I_1$: LD (MEM)\n- $I_2$: LD (MEM)\n- $I_3$: MUL (MUL)\n- $I_4$: ADD (ALU)\n- $I_5$: MUL (MUL)\n- $I_6$: ADD (ALU)\n- $I_7$: MUL (MUL)\n- $I_8$: ADD (ALU)\n- $I_9$: ST (MEM)\n- $I_{10}$: ADD (ALU)\n- $I_{11}$: ADD (ALU)\n- $I_{12}$: ADD (ALU)\n- $I_{13}$: ADD (ALU)\n\n**Cycle $1$**\n\nThe first bundle consists of instructions $I_1, I_2, I_3, I_4$.\n- Initial slot availability at $C=1$:\n  - $F_{\\text{ALU},0} = 1$\n  - $F_{\\text{ALU},1} = 1$\n  - $F_{\\text{MUL}} = 1$\n  - $F_{\\text{MEM}} = 1$\n\n- Naive assignment attempts:\n  - $I_1$ (LD) targets $S_{\\text{MEM}}$. The slot is available ($F_{\\text{MEM}} \\le 1$).\n  - $I_2$ (LD) targets $S_{\\text{MEM}}$. The slot is unavailable due to the simultaneous assignment of $I_1$. This is a resource over-subscription. This constitutes **$1$ conflict**.\n  - $I_3$ (MUL) targets $S_{\\text{MUL}}$. The slot is available ($F_{\\text{MUL}} \\le 1$).\n  - $I_4$ (ADD) targets $S_{\\text{ALU},0}$. The slot is available ($F_{\\text{ALU},0} \\le 1$).\n\n- Total conflicts in Cycle $1$: $1$.\n\n- Update slot occupancy for the next cycle, assuming instructions that did not conflict are issued.\n  - $I_1$ issues in $S_{\\text{MEM}}$ ($O_{\\text{MEM}}=1$). New $F_{\\text{MEM}} = 1 + 1 = 2$.\n  - $I_3$ issues in $S_{\\text{MUL}}$ ($O_{\\text{MUL}}=3$). New $F_{\\text{MUL}} = 1 + 3 = 4$.\n  - $I_4$ issues in $S_{\\text{ALU},0}$ ($O_{\\text{ALU}}=1$). New $F_{\\text{ALU},0} = 1 + 1 = 2$.\n  - $S_{\\text{ALU},1}$ remains unused. $F_{\\text{ALU},1} = 1$.\n\n**Cycle $2$**\n\nThe next bundle consists of instructions $I_5, I_6, I_7, I_8$.\n- Slot availability at $C=2$:\n  - $F_{\\text{ALU},0} = 2$ (Available)\n  - $F_{\\text{ALU},1} = 1$ (Available)\n  - $F_{\\text{MUL}} = 4$ (Unavailable, busy by $I_3$)\n  - $F_{\\text{MEM}} = 2$ (Available)\n\n- Naive assignment attempts:\n  - $I_5$ (MUL) targets $S_{\\text{MUL}}$. The slot is unavailable because $F_{\\text{MUL}} = 4 > 2$. This constitutes **$1$ conflict**.\n  - $I_6$ (ADD) targets $S_{\\text{ALU},0}$. The slot is available ($F_{\\text{ALU},0} \\le 2$).\n  - $I_7$ (MUL) targets $S_{\\text{MUL}}$. The slot is unavailable because $F_{\\text{MUL}} = 4 > 2$. This is a distinct attempt to use an unavailable slot. This constitutes another **$1$ conflict**.\n  - $I_8$ (ADD) targets $S_{\\text{ALU},1}$ (second ADD in the bundle). The slot is available ($F_{\\text{ALU},1} \\le 2$).\n\n- Total conflicts in Cycle $2$: $2$.\n\n- Update slot occupancy:\n  - $I_6$ issues in $S_{\\text{ALU},0}$ ($O_{\\text{ALU}}=1$). New $F_{\\text{ALU},0} = 2 + 1 = 3$.\n  - $I_8$ issues in $S_{\\text{ALU},1}$ ($O_{\\text{ALU}}=1$). New $F_{\\text{ALU},1} = 2 + 1 = 3$.\n  - $S_{\\text{MUL}}$ remains busy from $I_3$. $F_{\\text{MUL}}$ is still $4$.\n\n**Cycle $3$**\n\nThe next bundle consists of instructions $I_9, I_{10}, I_{11}, I_{12}$.\n- Slot availability at $C=3$:\n  - $F_{\\text{ALU},0} = 3$ (Available)\n  - $F_{\\text{ALU},1} = 3$ (Available)\n  - $F_{\\text{MUL}} = 4$ (Unavailable)\n  - $F_{\\text{MEM}} = 2$ (Available)\n\n- Naive assignment attempts:\n  - $I_9$ (ST) targets $S_{\\text{MEM}}$. The slot is available ($F_{\\text{MEM}} \\le 3$).\n  - $I_{10}$ (ADD) targets $S_{\\text{ALU},0}$. The slot is available ($F_{\\text{ALU},0} \\le 3$).\n  - $I_{11}$ (ADD) targets $S_{\\text{ALU},1}$. The slot is available ($F_{\\text{ALU},1} \\le 3$).\n  - $I_{12}$ (ADD) is the third ALU-type instruction. The naive policy attempts to map it to an ALU slot. However, $S_{\\text{ALU},0}$ and $S_{\\text{ALU},1}$ are already targeted by $I_{10}$ and $I_{11}$ in this same cycle. This is an over-subscription of ALU slots. This constitutes **$1$ conflict**.\n\n- Total conflicts in Cycle $3$: $1$.\n\n- Update slot occupancy:\n  - $I_9$ issues in $S_{\\text{MEM}}$ ($O_{\\text{MEM}}=1$). New $F_{\\text{MEM}} = 3 + 1 = 4$.\n  - $I_{10}$ issues in $S_{\\text{ALU},0}$ ($O_{\\text{ALU}}=1$). New $F_{\\text{ALU},0} = 3 + 1 = 4$.\n  - $I_{11}$ issues in $S_{\\text{ALU},1}$ ($O_{\\text{ALU}}=1$). New $F_{\\text{ALU},1} = 3 + 1 = 4$.\n  - $S_{\\text{MUL}}$ remains busy. $F_{\\text{MUL}}$ is still $4$.\n\n**Cycle $4$**\n\nThe final bundle consists of the last instruction, $I_{13}$.\n- Slot availability at $C=4$:\n  - $F_{\\text{ALU},0} = 4$ (Available)\n  - $F_{\\text{ALU},1} = 4$ (Available)\n  - $F_{\\text{MUL}} = 4$ (Available)\n  - $F_{\\text{MEM}} = 4$ (Available)\n\n- Naive assignment attempts:\n  - $I_{13}$ (ADD) targets $S_{\\text{ALU},0}$. The slot is available ($F_{\\text{ALU},0} \\le 4$). No conflict.\n\n- Total conflicts in Cycle $4$: $0$.\n\n**Total Conflicts**\n\nSumming the conflicts from each cycle:\nTotal Conflicts = (Conflicts in Cycle $1$) + (Conflicts in Cycle $2$) + (Conflicts in Cycle $3$) + (Conflicts in Cycle $4$)\nTotal Conflicts = $1 + 2 + 1 + 0 = 4$.\n\nThe naive bundling policy would result in a total of $4$ bundle conflicts.",
            "answer": "$$\\boxed{4}$$"
        },
        {
            "introduction": "Extracting performance from loops is the primary goal of compilers for VLIW processors, and software pipelining is the principal technique for achieving this. This practice guides you through a complete loop optimization process, starting with the calculation of the minimum initiation interval ($II$), which dictates the loop's throughput . You will then apply loop-invariant code motion, a classic optimization, and reschedule the loop to achieve a better $II$, directly observing the performance improvement and analyzing the critical trade-off between increased parallelism and the resulting demand on the processor's register file.",
            "id": "3681186",
            "problem": "A statically scheduled Very Long Instruction Word (VLIW) processor has fully pipelined functional units with the following per-cycle availability and latencies: one memory load/store unit per cycle (latency to produce a register result is $3$ cycles), one integer multiplier per cycle (latency $2$ cycles), and two integer adders per cycle (latency $1$ cycle). Consider the loop below, where all arrays are non-aliasing, memory is single-ported only through the single load/store unit, and all operations are independent except as shown by data dependences. The program variable $acc$ is initialized before the loop and accumulates across iterations. The array element $K[0]$ is read in every iteration, even though it is loop-invariant.\n\nFor iteration index $i \\in \\{0,1,\\dots,N-1\\}$, the loop body is:\n- $t_A \\leftarrow \\mathrm{load}\\;A[i]$\n- $t_B \\leftarrow \\mathrm{load}\\;B[i]$\n- $k \\leftarrow \\mathrm{load}\\;K[0]$\n- $s_1 \\leftarrow t_A + k$\n- $p \\leftarrow s_1 \\times t_B$\n- $acc \\leftarrow p + acc$\n\nAssume there are no other instructions in the loop, and ignore prologue/epilogue effects; analyze the steady-state modulo-scheduled kernel only. You are to perform two transformations: first, perform loop-invariant code motion to hoist the load of $K[0]$ out of the loop, keeping its value in a non-rotating scalar register thereafter; second, reschedule the loop using modulo scheduling to minimize the initiation interval (II) subject to the given resource and dependence constraints. Treat the per-iteration temporaries and the loop-carried accumulator as allocated from a rotating register file of size $R$, and treat the invariant $k$ after hoisting as a non-rotating scalar that does not consume rotating registers.\n\nStarting from core definitions of resource-constrained and recurrence-constrained initiation intervals, and the notion of value lifetimes within a modulo schedule, do the following:\n- Determine the minimal achievable initiation interval before the transformation and after hoisting and rescheduling.\n- For each of the two schedules, construct a feasible modulo schedule (slot assignments modulo the II with integer stage offsets) that respects latencies and resources, and from it determine the per-value lifetime in cycles from producer issue to last consumer issue within the steady-state kernel.\n- Using the lifetimes and the chosen II, determine the number of simultaneously live instances required for each temporary value in the rotating register file; the total rotating-register demand is the sum over values of the ceiling of the lifetime divided by the II.\n- Report the reduction in the initiation interval due to hoisting and rescheduling, and the number of unused rotating registers in the final (after-transformation) kernel as a function of $R$.\n\nExpress your final answer as a row matrix with two entries: the initiation-interval reduction and the number of unused rotating registers in the final kernel, in terms of $R$. No rounding is required.",
            "solution": "We begin from two core, widely accepted definitions used in modulo scheduling for Very Long Instruction Word architectures:\n\n1. The resource-constrained initiation interval, denoted $II_{\\mathrm{res}}$, is bounded below by the maximum, over each functional unit class, of the per-iteration demand divided by the per-cycle capacity, rounded up to the next integer.\n\n2. The recurrence-constrained initiation interval, denoted $II_{\\mathrm{rec}}$, is bounded below by the maximum, over all loop-carried dependence cycles, of the total latency along the cycle divided by the distance (in iterations), rounded up to the next integer.\n\nThe achievable initiation interval is thus bounded below by $II \\ge \\max\\{II_{\\mathrm{res}}, II_{\\mathrm{rec}}\\}$, and for a fully pipelined issue with integer capacities, we target $II$ equal to that bound by constructing a feasible modulo schedule.\n\nStep 1: Baseline per-iteration resource demands and $II$ before hoisting. In the given loop body per iteration, we have:\n- Memory load/store unit: $3$ loads ($t_A$, $t_B$, $k$), capacity $1$ per cycle, so the memory contribution to $II_{\\mathrm{res}}$ is $\\lceil 3/1 \\rceil = 3$.\n- Integer multiplier: $1$ multiply ($p \\leftarrow s_1 \\times t_B$), capacity $1$ per cycle, contribution $\\lceil 1/1 \\rceil = 1$.\n- Integer adders: $2$ adds ($s_1 \\leftarrow t_A + k$ and $acc \\leftarrow p + acc$), capacity $2$ per cycle, contribution $\\lceil 2/2 \\rceil = 1$.\n\nThus $II_{\\mathrm{res,\\,before}} = \\max\\{3, 1, 1\\} = 3$.\n\nFor loop-carried recurrences, the only explicit recurrence is the accumulator: $acc$ flows from iteration $i-1$ into the addition in iteration $i$. The recurrence cycle comprises the addition that produces $acc$ with latency $1$ cycle and a distance of $1$ iteration, hence $II_{\\mathrm{rec}} = \\lceil 1/1 \\rceil = 1$. There are no memory-carried dependences by assumption of non-aliasing and per-iteration independent array elements.\n\nTherefore, before hoisting, $II_{\\mathrm{before}} = \\max\\{II_{\\mathrm{res}}, II_{\\mathrm{rec}}\\} = \\max\\{3, 1\\} = 3$.\n\nStep 2: Feasible modulo schedule before hoisting and steady-state lifetimes. We choose a modulo schedule with slots modulo $II=3$ labeled $0,1,2$, and assign stage offsets (multiples of $II$) to satisfy latencies and resource constraints. Let the issue times for iteration $i$ be absolute time $T = \\mathrm{slot} + 3 \\times \\mathrm{stage} + 3 i$. One feasible assignment is:\n- $t_A \\leftarrow \\mathrm{load}\\;A[i]$: slot $0$, stage $0$; $T = 0 + 0 + 3 i$.\n- $t_B \\leftarrow \\mathrm{load}\\;B[i]$: slot $1$, stage $0$; $T = 1 + 0 + 3 i$.\n- $k \\leftarrow \\mathrm{load}\\;K[0]$: slot $2$, stage $0$; $T = 2 + 0 + 3 i$.\n- $s_1 \\leftarrow t_A + k$: slot $2$, stage $1$; $T = 2 + 3 + 3 i = 5 + 3 i$. The operands are ready no earlier than $3 + 3 i$ (from $t_A$ with latency $3$) and $5 + 3 i$ (from $k$), so issuing at $5 + 3 i$ is valid. Two adders per cycle allow two adds in slot $2$.\n- $p \\leftarrow s_1 \\times t_B$: slot $0$, stage $2$; $T = 0 + 6 + 3 i = 6 + 3 i$, respecting that $s_1$ is issued at $5 + 3 i$ with latency $1$ (ready at $6 + 3 i$) and $t_B$ is ready at $1 + 3 + 3 i = 4 + 3 i$.\n- $acc \\leftarrow p + acc$: slot $2$, stage $2$; $T = 2 + 6 + 3 i = 8 + 3 i$, respecting that $p$ is issued at $6 + 3 i$ with latency $2$ (ready at $8 + 3 i$). The recurrence to the next iteration’s $acc$ consumes this value at $8 + 3 (i+1) = 11 + 3 i$, which is at least one cycle after production considering the $1$-cycle latency and $3$-cycle separation between iterations in the kernel.\n\nThis assignment satisfies per-slot capacities: slot $0$ has one load and one multiply, slot $1$ has one load, slot $2$ has two adds and one load, which is permissible because there are two adders and one memory unit, and the multiply is not in slot $2$.\n\nFrom these issue times, compute per-value lifetimes as the time from producer issue to consumer issue in the steady-state kernel:\n- $t_A$: produced at $0 + 3 i$, used by $s_1$ at $5 + 3 i$; lifetime $\\ell_{t_A} = (5 + 3 i) - (0 + 3 i) = 5$ cycles.\n- $t_B$: produced at $1 + 3 i$, used by multiply at $6 + 3 i$; lifetime $\\ell_{t_B} = 5$ cycles.\n- $k$ (before hoisting): produced at $2 + 3 i$, used by $s_1$ at $5 + 3 i$; lifetime $\\ell_{k} = 3$ cycles.\n- $s_1$: produced at $5 + 3 i$, used by multiply at $6 + 3 i$; lifetime $\\ell_{s_1} = 1$ cycle.\n- $p$: produced at $6 + 3 i$, used by $acc$ at $8 + 3 i$; lifetime $\\ell_{p} = 2$ cycles.\n- $acc$ recurrence instance: produced at $8 + 3 (i-1)$, used at $8 + 3 i$; lifetime $\\ell_{acc} = 3$ cycles.\n\nThe number of simultaneously live instances for each value in the rotating register file equals the number of overlapping instances across iterations, which is given by $\\left\\lceil \\frac{\\ell}{II} \\right\\rceil$ for lifetime $\\ell$ under the modulo schedule. With $II=3$:\n- $t_A$: $\\left\\lceil \\frac{5}{3} \\right\\rceil = 2$.\n- $t_B$: $\\left\\lceil \\frac{5}{3} \\right\\rceil = 2$.\n- $k$: $\\left\\lceil \\frac{3}{3} \\right\\rceil = 1$.\n- $s_1$: $\\left\\lceil \\frac{1}{3} \\right\\rceil = 1$.\n- $p$: $\\left\\lceil \\frac{2}{3} \\right\\rceil = 1$.\n- $acc$: $\\left\\lceil \\frac{3}{3} \\right\\rceil = 1$.\n\nThus the total rotating-register demand before hoisting is $2 + 2 + 1 + 1 + 1 + 1 = 8$ registers.\n\nStep 3: After hoisting $k$ and rescheduling, recompute $II$ and lifetimes. Hoisting removes the invariant load from the loop. Per-iteration demands become:\n- Memory load/store unit: $2$ loads ($t_A$, $t_B$) per iteration, capacity $1$, contributing $\\lceil 2/1 \\rceil = 2$.\n- Integer multiplier: $1$ per iteration, contributing $\\lceil 1/1 \\rceil = 1$.\n- Integer adders: $2$ per iteration, contributing $\\lceil 2/2 \\rceil = 1$.\n\nTherefore $II_{\\mathrm{res,\\,after}} = \\max\\{2, 1, 1\\} = 2$. The recurrence on $acc$ is unchanged, $II_{\\mathrm{rec}} = 1$. Hence the minimal initiation interval after hoisting is $II_{\\mathrm{after}} = \\max\\{2, 1\\} = 2$.\n\nConstruct a feasible modulo schedule with slots modulo $2$ labeled $0,1$, and absolute issue time $T = \\mathrm{slot} + 2 \\times \\mathrm{stage} + 2 i$ for iteration $i$:\n- $t_A \\leftarrow \\mathrm{load}\\;A[i]$: slot $0$, stage $0$; $T = 0 + 0 + 2 i = 0 + 2 i$.\n- $t_B \\leftarrow \\mathrm{load}\\;B[i]$: slot $1$, stage $0$; $T = 1 + 0 + 2 i = 1 + 2 i$.\n- $s_1 \\leftarrow t_A + k$: slot $1$, stage $1$; $T = 1 + 2 + 2 i = 3 + 2 i$, respecting that $t_A$ is issued at $0 + 2 i$ with latency $3$ (ready at $3 + 2 i$), and $k$ is available as a non-rotating scalar with no in-kernel producing latency.\n- $p \\leftarrow s_1 \\times t_B$: slot $0$, stage $2$; $T = 0 + 4 + 2 i = 4 + 2 i$, respecting that $s_1$ is issued at $3 + 2 i$ with latency $1$ (ready at $4 + 2 i$) and $t_B$ is ready at $1 + 3 + 2 i = 4 + 2 i$.\n- $acc \\leftarrow p + acc$: slot $0$, stage $3$; $T = 0 + 6 + 2 i = 6 + 2 i$, respecting that $p$ is issued at $4 + 2 i$ with latency $2$ (ready at $6 + 2 i$). The recurrence to iteration $i+1$ consumes at $6 + 2 (i+1) = 8 + 2 i$, safely after production considering the $1$-cycle latency and $2$-cycle separation per iteration.\n\nPer-slot capacities are respected: slot $0$ has one load, one multiply, and one add; slot $1$ has one load and one add; the capacities match the one memory unit, one multiplier, and two adders per cycle.\n\nCompute lifetimes after hoisting from producer issue to consumer issue:\n- $t_A$: produced at $0 + 2 i$, used by $s_1$ at $3 + 2 i$; lifetime $\\ell'_{t_A} = 3$ cycles.\n- $t_B$: produced at $1 + 2 i$, used by multiply at $4 + 2 i$; lifetime $\\ell'_{t_B} = 3$ cycles.\n- $s_1$: produced at $3 + 2 i$, used by multiply at $4 + 2 i$; lifetime $\\ell'_{s_1} = 1$ cycle.\n- $p$: produced at $4 + 2 i$, used by $acc$ at $6 + 2 i$; lifetime $\\ell'_{p} = 2$ cycles.\n- $acc$ recurrence instance: produced at $6 + 2 (i-1) = 4 + 2 i$, used at $6 + 2 i$; lifetime $\\ell'_{acc} = 2$ cycles.\n\nThe invariant $k$ no longer consumes a rotating register because it is hoisted and stored in a non-rotating scalar register.\n\nWith $II=2$, the number of simultaneously live instances per value equals $\\left\\lceil \\frac{\\ell'}{2} \\right\\rceil$:\n- $t_A$: $\\left\\lceil \\frac{3}{2} \\right\\rceil = 2$.\n- $t_B$: $\\left\\lceil \\frac{3}{2} \\right\\rceil = 2$.\n- $s_1$: $\\left\\lceil \\frac{1}{2} \\right\\rceil = 1$.\n- $p$: $\\left\\lceil \\frac{2}{2} \\right\\rceil = 1$.\n- $acc$: $\\left\\lceil \\frac{2}{2} \\right\\rceil = 1$.\n\nThus the total rotating-register demand after hoisting is $2 + 2 + 1 + 1 + 1 = 7$ registers.\n\nStep 4: Requested quantities. The reduction in initiation interval equals $II_{\\mathrm{before}} - II_{\\mathrm{after}} = 3 - 2 = 1$. For a rotating register file of size $R$, the number of unused rotating registers in the final kernel equals $R - 7$.\n\nTherefore, the requested pair is the row matrix with entries $1$ and $R - 7$.",
            "answer": "$$\\boxed{\\begin{pmatrix}1 & R-7\\end{pmatrix}}$$"
        },
        {
            "introduction": "Static scheduling faces a significant challenge when confronted with dynamic uncertainties, such as whether two memory addresses will alias at runtime. This advanced exercise places you in the role of a compiler designer who must make a static scheduling decision despite this uncertainty . By using probabilistic analysis and the concept of expected execution time, you will determine a critical threshold for the likelihood of aliasing. This threshold informs the decision to either schedule two loads in parallel speculatively, risking a penalty, or to serialize them conservatively, demonstrating how compilers can make mathematically sound choices in the face of incomplete information.",
            "id": "3681231",
            "problem": "A Very Long Instruction Word (VLIW) processor can issue up to $2$ memory operations in the same instruction packet. Consider two loads, $L_1$ and $L_2$, whose effective addresses may alias with probability $a \\in [0,1]$. If $L_1$ and $L_2$ are placed in the same packet and they alias, the microarchitecture detects the same-address conflict and imposes a deterministic replay penalty of $r = 6$ cycles on the packet’s completion time. This penalty does not occur if the loads are not issued in the same cycle. Load latencies are independent random variables with the following distributions:\n- $L_1$ has latency $\\ell_1 = 3$ cycles with probability $2/3$, and $\\ell_1 = 11$ cycles with probability $1/3$.\n- $L_2$ has latency $\\ell_2 = 5$ cycles with probability $1/2$, and $\\ell_2 = 9$ cycles with probability $1/2$.\n\nAssume the following completion-time model based on core definitions of pipeline timing:\n- If two operations are issued in the same cycle, the packet completion time is $\\max(\\ell_1,\\ell_2)$, plus the replay penalty $r$ if and only if aliasing occurs.\n- If $L_1$ is issued in cycle $0$ and $L_2$ is issued in cycle $1$ (a one-cycle fence or “stop” between them), the completion time is $\\max(\\ell_1,\\,1+\\ell_2)$.\n- If $L_2$ is issued in cycle $0$ and $L_1$ is issued in cycle $1$, the completion time is $\\max(\\ell_2,\\,1+\\ell_1)$.\n\nYou must choose a static schedule among three options: parallel issue (same packet), stagger with $L_1$ first, or stagger with $L_2$ first. Using only the fundamental definition of expected value and the stated timing model, derive the expected completion cycles for each schedule as a function of $a$ (for the parallel case) or as constants (for the staggered cases). Then, by comparing these expectations, determine the critical alias probability $a^{\\star}$ at which the optimal static choice is indifferent between parallel issue and the best one-cycle stagger order.\n\nExpress the final answer for $a^{\\star}$ as an exact fraction. No rounding is required.",
            "solution": "To find the critical alias probability $a^{\\star}$, we must compare the expected completion time of the parallel schedule with that of the best staggered schedule. We will calculate the expected time for each of the three options.\n\n**1. Expected Completion Time for Parallel Issue ($E[T_{\\text{par}}]$)**\nThe completion time for the parallel schedule is $T_{\\text{par}} = \\max(\\ell_1, \\ell_2) + (\\text{penalty term})$. The penalty term is $r=6$ with probability $a$ (aliasing) and $0$ with probability $1-a$. By the law of total expectation, the expected completion time is:\n$E[T_{\\text{par}}] = E[\\max(\\ell_1, \\ell_2)] + a \\cdot r$\n\nFirst, we calculate $E[\\max(\\ell_1, \\ell_2)]$ by considering all four independent outcomes for the pair $(\\ell_1, \\ell_2)$:\n- $P(\\ell_1=3, \\ell_2=5) = \\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{1}{3}$. In this case, $\\max(3, 5) = 5$.\n- $P(\\ell_1=3, \\ell_2=9) = \\frac{2}{3} \\cdot \\frac{1}{2} = \\frac{1}{3}$. In this case, $\\max(3, 9) = 9$.\n- $P(\\ell_1=11, \\ell_2=5) = \\frac{1}{3} \\cdot \\frac{1}{2} = \\frac{1}{6}$. In this case, $\\max(11, 5) = 11$.\n- $P(\\ell_1=11, \\ell_2=9) = \\frac{1}{3} \\cdot \\frac{1}{2} = \\frac{1}{6}$. In this case, $\\max(11, 9) = 11$.\n\nThe expected value of the maximum is the sum of each maximum weighted by its probability:\n$E[\\max(\\ell_1, \\ell_2)] = (5 \\cdot \\frac{1}{3}) + (9 \\cdot \\frac{1}{3}) + (11 \\cdot \\frac{1}{6}) + (11 \\cdot \\frac{1}{6}) = \\frac{5}{3} + \\frac{9}{3} + \\frac{11}{6} + \\frac{11}{6} = \\frac{14}{3} + \\frac{22}{6} = \\frac{28}{6} + \\frac{22}{6} = \\frac{50}{6} = \\frac{25}{3}$.\n\nTherefore, the expected time for the parallel schedule is $E[T_{\\text{par}}] = \\frac{25}{3} + 6a$.\n\n**2. Expected Completion Time for Staggered Schedules**\nFor the staggered schedules, there is no aliasing penalty.\n\n**Staggered, $L_1$ first ($E[T_{\\text{stag},1}]$):** The completion time is $T_{\\text{stag},1} = \\max(\\ell_1, 1+\\ell_2)$.\n- $P(\\ell_1=3, 1+\\ell_2=6) = \\frac{1}{3}$. $\\max(3, 6) = 6$.\n- $P(\\ell_1=3, 1+\\ell_2=10) = \\frac{1}{3}$. $\\max(3, 10) = 10$.\n- $P(\\ell_1=11, 1+\\ell_2=6) = \\frac{1}{6}$. $\\max(11, 6) = 11$.\n- $P(\\ell_1=11, 1+\\ell_2=10) = \\frac{1}{6}$. $\\max(11, 10) = 11$.\n$E[T_{\\text{stag},1}] = (6 \\cdot \\frac{1}{3}) + (10 \\cdot \\frac{1}{3}) + (11 \\cdot \\frac{1}{6}) + (11 \\cdot \\frac{1}{6}) = \\frac{16}{3} + \\frac{22}{6} = \\frac{32}{6} + \\frac{22}{6} = \\frac{54}{6} = 9$.\n\n**Staggered, $L_2$ first ($E[T_{\\text{stag},2}]$):** The completion time is $T_{\\text{stag},2} = \\max(\\ell_2, 1+\\ell_1)$.\n- $P(\\ell_2=5, 1+\\ell_1=4) = \\frac{1}{3}$. $\\max(5, 4) = 5$.\n- $P(\\ell_2=5, 1+\\ell_1=12) = \\frac{1}{6}$. $\\max(5, 12) = 12$.\n- $P(\\ell_2=9, 1+\\ell_1=4) = \\frac{1}{3}$. $\\max(9, 4) = 9$.\n- $P(\\ell_2=9, 1+\\ell_1=12) = \\frac{1}{6}$. $\\max(9, 12) = 12$.\n$E[T_{\\text{stag},2}] = (5 \\cdot \\frac{1}{3}) + (12 \\cdot \\frac{1}{6}) + (9 \\cdot \\frac{1}{3}) + (12 \\cdot \\frac{1}{6}) = \\frac{14}{3} + \\frac{24}{6} = \\frac{28}{6} + \\frac{24}{6} = \\frac{52}{6} = \\frac{26}{3}$.\n\n**3. Determine the Critical Probability $a^{\\star}$**\nFirst, we find the best (fastest) staggered schedule by comparing their expected completion times:\n$E[T_{\\text{stag},1}] = 9 = \\frac{27}{3}$\n$E[T_{\\text{stag},2}] = \\frac{26}{3}$\nSince $\\frac{26}{3}  \\frac{27}{3}$, the optimal staggered schedule is to issue $L_2$ first. The expected time for the best conservative schedule is $E[T_{\\text{best}}] = \\frac{26}{3}$.\n\nThe critical probability $a^{\\star}$ is the value of $a$ where the parallel schedule's expected time equals the best staggered schedule's expected time:\n$E[T_{\\text{par}}] = E[T_{\\text{best}}]$\n$\\frac{25}{3} + 6a^{\\star} = \\frac{26}{3}$\n$6a^{\\star} = \\frac{26}{3} - \\frac{25}{3}$\n$6a^{\\star} = \\frac{1}{3}$\n$a^{\\star} = \\frac{1}{18}$\n\nFor an alias probability below $\\frac{1}{18}$, the parallel schedule is faster on average. Above this value, staggering the loads with $L_2$ first is the better static choice.",
            "answer": "$$\\boxed{\\frac{1}{18}}$$"
        }
    ]
}