{
    "hands_on_practices": [
        {
            "introduction": "The performance of a modern superscalar processor is fundamentally a contest between two factors: the inherent sequential nature of a task, defined by its longest chain of data dependencies, and the processor's parallel execution capability, defined by its issue width. This exercise challenges you to analyze a block of code with a known dependency structure and determine the achievable Instructions Per Cycle ($IPC$) . By calculating the bounds imposed by both the critical path and the issue width, you will gain a core understanding of how these two limits interact to define the practical boundaries of instruction-level parallelism.",
            "id": "3651332",
            "problem": "A program basic block contains arithmetic instructions only and is executed on two microarchitectures built from the same pipeline design: a scalar core that can issue at most $1$ instruction per cycle, and a superscalar core that can issue at most $w$ instructions per cycle with $w=4$. Both cores are out-of-order with perfect register renaming, have an unbounded instruction window, and have enough identical integer arithmetic functional units such that the only per-cycle resource limit is the issue width. All instructions are single-cycle integer arithmetic operations with unit latency and no multi-cycle effects. Ignore all effects of cache misses, memory disambiguation, pipeline bubbles unrelated to true data dependence, and branch misprediction. Assume that only true data dependences constrain reordering. Consider the following data dependences inside the basic block:\n- There is a single true-dependence chain of length $10$ consisting of the following $10$ instructions (all integer adds), where each instruction depends on the immediately preceding one in the list:\n  $I_{1}$ defines a temporary, $I_{2}$ uses the result of $I_{1}$, $I_{3}$ uses the result of $I_{2}$, and so on up to $I_{10}$ using the result of $I_{9}$.\n- In addition, there are $17$ other arithmetic instructions that are mutually independent of each other and independent of the $10$-instruction chain described above.\n\nLet the total number of instructions in the block be $N$, and let the length (in cycles, under unit latency) of the longest true-dependence chain be $L$. The Instructions Per Cycle (IPC) is defined as the total number of retired instructions divided by the total number of cycles for the blockâ€™s execution under an optimal out-of-order schedule respecting the issue-width limit and true dependences.\n\nCompute the expected steady-state IPC achieved by the $w=4$ superscalar core when executing this basic block in isolation, under the assumptions above. Express your answer as an exact value; no rounding is required.",
            "solution": "The problem statement has been evaluated and is deemed valid. It is a well-posed, self-contained problem in computer architecture, grounded in established principles of instruction-level parallelism and superscalar processor performance analysis. All necessary parameters and simplifying assumptions are provided.\n\nThe objective is to compute the Instructions Per Cycle (IPC) for a given basic block on a superscalar, out-of-order processor. The IPC is defined as the total number of instructions executed divided by the total number of cycles required for execution.\n\nLet $N$ be the total number of instructions in the basic block.\nLet $L$ be the length of the longest true-dependence chain, measured in cycles.\nLet $w$ be the maximum issue width of the superscalar core.\n\nFrom the problem statement, we are given:\nThere is a single true-dependence chain of $10$ instructions.\nThere are $17$ other instructions that are independent of the chain and of each other.\nThe issue width is $w=4$.\nAll instructions have a unit latency of $1$ cycle.\n\nFirst, we determine the total number of instructions, $N$. This is the sum of the instructions in the dependence chain and the independent instructions.\n$$N = 10 + 17 = 27$$\n\nNext, we identify the length of the critical path, $L$. The problem states there is a chain of $10$ instructions, where each instruction $I_{k+1}$ depends on the result of the preceding instruction $I_k$. Since each instruction has a latency of $1$ cycle, the execution of this chain is sequential. Let's assume $I_1$ is issued and executes in cycle $1$. Its result is available at the start of cycle $2$. Thus, $I_2$ can be issued and executed in cycle $2$. This continues down the chain, with instruction $I_k$ executing in cycle $k$. The final instruction of the chain, $I_{10}$, can only execute in cycle $10$. It will complete at the end of cycle $10$. Therefore, the length of the critical dependence path, which dictates the minimum possible execution time, is $10$ cycles. The problem defines this value as $L$.\n$$L = 10 \\text{ cycles}$$\n\nThe performance of an out-of-order superscalar processor is constrained by two primary factors:\n$1$. Data dependencies, which define the critical path length ($L$). The total execution time, $T$, cannot be less than $L$.\n$$T \\ge L$$\n$2$. Resource limitations, primarily the instruction issue width ($w$). The processor can issue at most $w$ instructions per cycle. To execute $N$ instructions, a minimum number of cycles is required, even with infinite parallelism.\n$$w \\times T \\ge N \\implies T \\ge \\frac{N}{w}$$\nSince the number of cycles $T$ must be an integer, this resource-based lower bound is $T \\ge \\lceil \\frac{N}{w} \\rceil$.\n\nCombining these two bounds, the total execution time $T$ must be at least the maximum of the two lower bounds:\n$$T \\ge \\max\\left(L, \\left\\lceil \\frac{N}{w} \\right\\rceil\\right)$$\nLet's substitute the values for our problem:\n$$T \\ge \\max\\left(10, \\left\\lceil \\frac{27}{4} \\right\\rceil\\right)$$\n$$T \\ge \\max(10, \\lceil 6.75 \\rceil)$$\n$$T \\ge \\max(10, 7)$$\n$$T \\ge 10 \\text{ cycles}$$\n\nThis gives a theoretical lower bound for the execution time. We now need to verify if this execution time of $T=10$ cycles is achievable. An optimal scheduler in an out-of-order machine with an unbounded instruction window will try to fill the issue slots in each cycle with independent instructions.\n\nThe critical path requires one instruction to be executed in each of the $10$ cycles. This occupies one issue slot per cycle. The issue width is $w=4$, so in each cycle where a critical path instruction is being executed, there are $w-1 = 4-1 = 3$ additional issue slots available for independent instructions.\n\nOver the course of the $L=10$ cycles determined by the critical path, the total number of available slots for independent instructions is:\n$$ \\text{Available slots for independent work} = (w-1) \\times L = (4-1) \\times 10 = 30 $$\nThe number of independent instructions to be executed is $17$.\n\nSince the number of independent instructions ($17$) is less than the number of available slots to execute them in parallel with the critical path ($30$), all independent instructions can be issued and executed without extending the total execution time beyond the $10$ cycles required by the critical path.\n\nLet's illustrate with an explicit schedule:\n-   Cycle $1$: Issue $I_1$ (critical) and $3$ independent instructions.\n-   Cycle $2$: Issue $I_2$ (critical) and $3$ independent instructions.\n-   Cycle $3$: Issue $I_3$ (critical) and $3$ independent instructions.\n-   Cycle $4$: Issue $I_4$ (critical) and $3$ independent instructions.\n-   Cycle $5$: Issue $I_5$ (critical) and $3$ independent instructions. (Total $15$ independent instructions issued)\n-   Cycle $6$: Issue $I_6$ (critical) and the remaining $2$ independent instructions. (All $17$ independent instructions are now issued)\n-   Cycle $7$: Issue $I_7$ (critical).\n-   Cycle $8$: Issue $I_8$ (critical).\n-   Cycle $9$: Issue $I_9$ (critical).\n-   Cycle $10$: Issue $I_{10}$ (critical).\n\nAt the end of cycle $10$, all $27$ instructions have been issued. The last instruction to complete is $I_{10}$, which was issued in cycle $10$ and finishes at the end of cycle $10$ due to its unit latency. All other instructions complete no later than this.\nThus, the total execution time is indeed dominated by the critical path length.\n$$T = L = 10 \\text{ cycles}$$\n\nFinally, we can compute the IPC.\n$$ \\text{IPC} = \\frac{\\text{Total Instructions}}{\\text{Total Cycles}} = \\frac{N}{T} $$\n$$ \\text{IPC} = \\frac{27}{10} = 2.7 $$\nThe expected steady-state IPC for this basic block is $2.7$.",
            "answer": "$$\n\\boxed{2.7}\n$$"
        },
        {
            "introduction": "While issue width provides a high-level view of a processor's parallelism, performance in reality is often dictated by a more granular constraint: the availability of specific functional units. A program heavy on memory operations will stress the Load/Store Unit, while a calculation-heavy program will saturate the ALUs. This practice asks you to act as a performance analyst, determining the maximum sustainable $ILP$ for different instruction mixes on a processor with a fixed set of functional units . This exercise will teach you how to use bottleneck analysis to predict performance and understand the critical importance of resource balance in computer architecture.",
            "id": "3651306",
            "problem": "An out-of-order superscalar processor is designed with the following execution resources and assumptions. The processor has two integer Arithmetic Logic Units (ALUs) and one Load/Store Unit (LSU). All functional units are fully pipelined and can initiate one new operation per cycle per unit. The instruction window is sufficiently large to expose all available parallelism, register renaming is perfect, memory disambiguation is perfect, and branch prediction is perfect; control and data dependencies beyond structural resource contention can be ignored. The commit bandwidth is not a bottleneck. The issue width in any cycle is limited only by the number of operations that can be initiated by the available functional units. The memory system is idealized such that one memory operation per cycle can be initiated by the LSU in steady state. Instruction-Level Parallelism (ILP) is defined as the steady-state average number of instructions completed per cycle, equivalently the steady-state throughput in instructions per cycle (IPC).\n\nConsider two independent instruction streams whose dynamic mixes consist only of integer ALU operations and memory operations (loads or stores), with no other instruction types:\n\n- Integer-heavy mix: a fraction $p_{\\text{ALU}}^{(I)} = \\frac{4}{5}$ are integer ALU operations and a fraction $p_{\\text{MEM}}^{(I)} = \\frac{1}{5}$ are memory operations.\n- Memory-heavy mix: a fraction $p_{\\text{ALU}}^{(M)} = \\frac{2}{5}$ are integer ALU operations and a fraction $p_{\\text{MEM}}^{(M)} = \\frac{3}{5}$ are memory operations.\n\nUsing only conservation-of-flow reasoning and the capacity constraints implied by the two ALUs and one LSU, determine the maximum sustainable Instruction-Level Parallelism for each mix and then compute the ratio of the integer-heavy ILP to the memory-heavy ILP. Provide your final answer as an exact real number (do not round). No units are required in the final answer.",
            "solution": "The problem requires the calculation of the a ratio between the maximum sustainable Instruction-Level Parallelism ($ILP$) for two different instruction mixes on an idealized superscalar processor. The $ILP$ is defined as the steady-state average number of instructions completed per cycle ($IPC$). The performance of the processor is limited by the capacity of its functional units. The problem directs the use of \"conservation-of-flow reasoning,\" which means the steady-state throughput is determined by the resource that saturates first, i.e., the bottleneck.\n\nLet $N_{ALU}$ be the number of integer Arithmetic Logic Units ($ALU$s) and $N_{LSU}$ be the number of Load/Store Units ($LSU$s). According to the problem statement, the processor is equipped with:\n$$ N_{ALU} = 2 $$\n$$ N_{LSU} = 1 $$\n\nEach functional unit is fully pipelined and can initiate one new operation per cycle. Therefore, the maximum rate of $ALU$ operations is $2$ per cycle, and the maximum rate of memory operations is $1$ per cycle.\n\nFor any given dynamic instruction stream, let $p_{ALU}$ be the fraction of $ALU$ operations and $p_{MEM}$ be the fraction of memory operations. If the processor sustains an overall throughput of $I$ instructions per cycle, then the rate at which $ALU$ instructions are executed is $I \\times p_{ALU}$, and the rate at which memory instructions are executed is $I \\times p_{MEM}$.\n\nFor a steady state to be possible, the demand for each type of resource must not exceed its capacity. This leads to two independent constraints on the maximum possible value of $I$:\n\n1.  The $ALU$ constraint: The rate of $ALU$ instruction execution cannot exceed the total $ALU$ capacity.\n    $$ I \\times p_{ALU} \\le N_{ALU} \\implies I \\le \\frac{N_{ALU}}{p_{ALU}} $$\n2.  The $LSU$ constraint: The rate of memory instruction execution cannot exceed the total $LSU$ capacity.\n    $$ I \\times p_{MEM} \\le N_{LSU} \\implies I \\le \\frac{N_{LSU}}{p_{MEM}} $$\n\nThe maximum sustainable $ILP$, which we will denote as $ILP_{max}$, is the highest value of $I$ that simultaneously satisfies both constraints. This value is determined by the more restrictive of the two upper bounds, corresponding to the bottleneck resource.\n$$ ILP_{max} = \\min\\left(\\frac{N_{ALU}}{p_{ALU}}, \\frac{N_{LSU}}{p_{MEM}}\\right) $$\n\nWe will now apply this framework to the two instruction mixes provided.\n\n**Integer-Heavy Mix (I)**\nFor this mix, the fractions are $p_{\\text{ALU}}^{(I)} = \\frac{4}{5}$ and $p_{\\text{MEM}}^{(I)} = \\frac{1}{5}$.\nThe upper bound on $ILP$ imposed by the $ALU$ resources is:\n$$ I_{ALU-bound}^{(I)} = \\frac{N_{ALU}}{p_{\\text{ALU}}^{(I)}} = \\frac{2}{4/5} = \\frac{2 \\times 5}{4} = \\frac{10}{4} = 2.5 $$\nThe upper bound on $ILP$ imposed by the $LSU$ resource is:\n$$ I_{LSU-bound}^{(I)} = \\frac{N_{LSU}}{p_{\\text{MEM}}^{(I)}} = \\frac{1}{1/5} = 5 $$\nThe maximum sustainable $ILP$ for the integer-heavy mix, $ILP_I$, is the minimum of these two bounds:\n$$ ILP_I = \\min(2.5, 5) = 2.5 $$\nIn this scenario, the two $ALU$s are the bottleneck.\n\n**Memory-Heavy Mix (M)**\nFor this mix, the fractions are $p_{\\text{ALU}}^{(M)} = \\frac{2}{5}$ and $p_{\\text{MEM}}^{(M)} = \\frac{3}{5}$.\nThe upper bound on $ILP$ imposed by the $ALU$ resources is:\n$$ I_{ALU-bound}^{(M)} = \\frac{N_{ALU}}{p_{\\text{ALU}}^{(M)}} = \\frac{2}{2/5} = \\frac{2 \\times 5}{2} = 5 $$\nThe upper bound on $ILP$ imposed by the $LSU$ resource is:\n$$ I_{LSU-bound}^{(M)} = \\frac{N_{LSU}}{p_{\\text{MEM}}^{(M)}} = \\frac{1}{3/5} = \\frac{5}{3} $$\nThe maximum sustainable $ILP$ for the memory-heavy mix, $ILP_M$, is the minimum of these two bounds:\n$$ ILP_M = \\min\\left(5, \\frac{5}{3}\\right) = \\frac{5}{3} $$\nIn this scenario, the single $LSU$ is the bottleneck.\n\n**Ratio Calculation**\nFinally, we compute the ratio of the integer-heavy $ILP$ to the memory-heavy $ILP$:\n$$ \\text{Ratio} = \\frac{ILP_I}{ILP_M} $$\nSubstituting the calculated values:\n$$ \\text{Ratio} = \\frac{2.5}{5/3} = \\frac{5/2}{5/3} = \\frac{5}{2} \\times \\frac{3}{5} = \\frac{15}{10} = \\frac{3}{2} $$\nExpressing this as an exact real number gives $1.5$.",
            "answer": "$$\\boxed{1.5}$$"
        },
        {
            "introduction": "Beyond data and resource constraints, control flow presents one of the most significant hurdles to achieving high $ILP$, primarily due to the high cost of recovering from a mispredicted branch. Architects and compilers have developed techniques to mitigate this, sometimes by avoiding branches altogether. This problem places you in the role of a compiler designer, evaluating the trade-off between a traditional branching implementation and an alternative using conditional moves . By calculating the expected performance of each approach, you will learn to quantify the impact of control hazards and appreciate the nuanced decisions required to optimize code for modern processors.",
            "id": "3654336",
            "problem": "A loop body contains a conditional computation of a temporary value followed by a dependent accumulation. At the beginning of each iteration, operands $A$, $B$, $C$, $D$, $K$, and a condition register $r_0$ are all ready. The computation is logically:\n- If $r_0 < 0$, compute $t \\leftarrow A + B$; else compute $t \\leftarrow C + D$.\n- Then compute $u \\leftarrow t + K$.\n\nConsider two implementations on the same out-of-order superscalar processor:\n\nImplementation 1 (branching):\n- The conditional is implemented as a predicted conditional branch. The machine fetches and executes the predicted path speculatively and recovers on a misprediction.\n- On a correct prediction, there is no additional delay beyond data dependences and issue width.\n- On a misprediction, there is a front-end recovery penalty of $B$ cycles; assume all wrong-path work is fully accounted for by this penalty.\n\nImplementation 2 (conditional move):\n- The conditional is implemented using a compare that sets condition codes and a conditional move instruction $\\operatorname{cmov}$ that selects between the two candidate sums without branching.\n- Both candidate sums are executed unconditionally, and $\\operatorname{cmov}$ selects the correct one.\n\nProcessor model and assumptions (apply to both implementations):\n- The issue width is $w = 2$ instructions per cycle, with perfect out-of-order scheduling limited only by true data dependences and the width $w$.\n- Integer add, compare, and conditional move each have latency $1$ cycle and initiate at a rate of up to $w$ per cycle subject to independence.\n- There are no cache misses, no structural hazards beyond the issue width, and an effectively unbounded instruction window and rename capacity.\n- The branch predictor accuracy is $p = 0.78$ (probability of correct prediction), so the misprediction probability is $m = 1 - p$.\n- The branch misprediction recovery penalty is $B = 11$ cycles.\n- Ignore any loop-control branch overhead not shown above, as it is identical for both implementations.\n\nStarting from first principles about data-dependence-limited scheduling on a machine of finite width and expected-value reasoning for mispredictions, determine the speedup of the conditional-move implementation relative to the branching implementation, defined as\n$$S \\equiv \\frac{\\text{average cycles per iteration of branching}}{\\text{cycles per iteration of conditional move}}.$$\nExpress your final answer as a pure number (no units). Round your answer to four significant figures.",
            "solution": "To find the speedup, we must first calculate the execution time in cycles for each of the two implementations and then find their ratio.\n\n**Analysis of Implementation 2 (Conditional Move)**\n\nThis implementation avoids branches by computing both potential results and then selecting the correct one with a `cmov` instruction. The operations are:\n1.  `add_1`: $t_1 \\leftarrow A + B$\n2.  `add_2`: $t_2 \\leftarrow C + D$\n3.  `cmp`: sets condition codes $cc$ based on $r_0  0$\n4.  `cmov`: $t \\leftarrow \\operatorname{cmov}(t_1, t_2, cc)$\n5.  `add_3`: $u \\leftarrow t + K$\n\nThe data dependencies are: `add_1`, `add_2`, and `cmp` are independent. `cmov` depends on the results of all three. `add_3` depends on the result of `cmov`. We can schedule these five 1-cycle instructions on a processor with an issue width of $w=2$:\n\n- **Cycle 1**: Issue `add_1` and `add_2`. Their results ($t_1, t_2$) are ready at the start of Cycle 2.\n- **Cycle 2**: Issue `cmp`. Its result ($cc$) is ready at the start of Cycle 3. Only one instruction is ready, so one issue slot is unused.\n- **Cycle 3**: Issue `cmov`. Its inputs ($t_1, t_2, cc$) are now available. Its result ($t$) is ready at the start of Cycle 4.\n- **Cycle 4**: Issue `add_3`. Its input ($t$) is now available. The final result ($u$) is computed.\n\nThe computation completes in 4 cycles. Therefore, the number of cycles per iteration for the conditional-move implementation is:\n$$T_{\\text{cmov}} = 4 \\text{ cycles}$$\n\n**Analysis of Implementation 1 (Branching)**\n\nThe execution time for this implementation is probabilistic. We must calculate the expected (average) number of cycles per iteration using the branch prediction accuracy $p = 0.78$ and misprediction probability $m = 1-p = 0.22$.\n$$T_{\\text{branch}} = p \\times T_{\\text{correct}} + m \\times T_{\\text{mispredict}}$$\n\n*   **Case 1: Correctly Predicted Branch ($T_{\\text{correct}}$)**\n    When the branch is predicted correctly, the processor executes only the instructions on the correct path. The critical path is one addition followed by a dependent addition (`add_t` $\\rightarrow$ `add_u`). This is a chain of two 1-cycle instructions, so the execution takes 2 cycles. The issue width $w=2$ is not a bottleneck.\n    $$T_{\\text{correct}} = 2 \\text{ cycles}$$\n\n*   **Case 2: Mispredicted Branch ($T_{\\text{mispredict}}$)**\n    When a misprediction occurs, the total time is the sum of the correct-path execution time and the misprediction penalty $B=11$.\n    $$T_{\\text{mispredict}} = T_{\\text{correct}} + B = 2 + 11 = 13 \\text{ cycles}$$\n\n*   **Average Cycles for Branching Implementation**\n    We can now compute the average number of cycles:\n    $$T_{\\text{branch}} = (0.78 \\times 2) + (0.22 \\times 13)$$\n    $$T_{\\text{branch}} = 1.56 + 2.86 = 4.42 \\text{ cycles}$$\n\n**Calculation of Speedup ($S$)**\n\nThe speedup $S$ is the ratio of the average time for the branching implementation to the time for the conditional-move implementation.\n$$S = \\frac{T_{\\text{branch}}}{T_{\\text{cmov}}} = \\frac{4.42}{4} = 1.105$$\nThe answer, rounded to four significant figures as requested, is 1.105.",
            "answer": "$$\\boxed{1.105}$$"
        }
    ]
}