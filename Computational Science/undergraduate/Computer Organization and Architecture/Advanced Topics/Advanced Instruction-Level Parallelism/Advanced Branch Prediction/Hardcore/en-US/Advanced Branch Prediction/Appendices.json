{
    "hands_on_practices": [
        {
            "introduction": "To truly understand advanced branch predictors, we must first analyze their most fundamental component: the 2-bit saturating counter. While its update rule seems simple, its long-term behavior for a biased branch can be precisely quantified. This exercise  invites you to model the counter as a Markov chain, a powerful tool for analyzing systems that transition between states probabilistically. By calculating the steady-state probabilities, you will gain a deep, quantitative understanding of how a predictor's accuracy is directly tied to the statistical properties of the branch it is trying to predict.",
            "id": "3619723",
            "problem": "Consider a Two-Level Adaptive Branch Predictor whose Pattern History Table (PHT) entries are standard $2$-bit saturating counters indexed by a Global History Register (GHR). Focus on a single PHT entry associated with one static branch whose outcomes are independent and identically distributed (i.i.d.), with taken probability $P(T)=0.6$ and not-taken probability $P(N)=0.4$. The $2$-bit saturating counter has four states representing the integer values $0,1,2,3$. On a taken outcome, the counter increments by $1$ with saturation at $3$; on a not-taken outcome, the counter decrements by $1$ with saturation at $0$. The predictor emits a not-taken prediction when the counter is in states $0$ or $1$, and a taken prediction when in states $2$ or $3$.\n\nModel the counter evolution as a discrete-time Markov chain with states $s_{0}, s_{1}, s_{2}, s_{3}$ corresponding to counter values $0,1,2,3$, respectively. Using only fundamental definitions from probability and Markov chains, and the standard $2$-bit counter update rule described above:\n\n- Derive the steady-state probability distribution $\\pi = (\\pi_{0}, \\pi_{1}, \\pi_{2}, \\pi_{3})$ of the counter states.\n- Compute the expected long-run misprediction rate, defined as the steady-state expected fraction of prediction-outcome mismatches, given the predictor’s mapping of states to predictions and the branch’s i.i.d. outcomes.\n\nExpress $P(T)$ as $p$ and $P(N)$ as $q$, then set $p=0.6$ and $q=0.4$ at the end. Provide your final answer as a single row vector containing the four steady-state probabilities $(\\pi_{0}, \\pi_{1}, \\pi_{2}, \\pi_{3})$ followed by the misprediction rate, using exact values. Do not round. No units are required.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of Markov chains, well-posed with a unique solution, objective, and contains all necessary information. We proceed with the solution.\n\nThe evolution of the $2$-bit saturating counter can be modeled as a discrete-time Markov chain. The state space is $S = \\{s_0, s_1, s_2, s_3\\}$, where $s_i$ corresponds to the counter having the integer value $i$. The branch outcomes are independent and identically distributed, with a probability of being taken $P(T) = p = 0.6$ and not-taken $P(N) = q = 0.4$.\n\nThe state transitions are as follows:\n- From state $s_0$ (value $0$): On a 'Taken' outcome (probability $p$), the counter increments to $1$ (state $s_1$). On a 'Not-Taken' outcome (probability $q$), it remains at $0$ (state $s_0$) due to saturation.\n- From state $s_1$ (value $1$): On a 'Taken' outcome (probability $p$), the counter increments to $2$ (state $s_2$). On a 'Not-Taken' outcome (probability $q$), it decrements to $0$ (state $s_0$).\n- From state $s_2$ (value $2$): On a 'Taken' outcome (probability $p$), the counter increments to $3$ (state $s_3$). On a 'Not-Taken' outcome (probability $q$), it decrements to $1$ (state $s_1$).\n- From state $s_3$ (value $3$): On a 'Taken' outcome (probability $p$), the counter remains at $3$ (state $s_3$) due to saturation. On a 'Not-Taken' outcome (probability $q$), it decrements to $2$ (state $s_2$).\n\nThis system is a birth-death process. At steady state, the probability flow between any two adjacent states must be balanced. Let $\\pi = (\\pi_0, \\pi_1, \\pi_2, \\pi_3)$ be the steady-state probability distribution. The detailed balance equations for the net flow across cuts between adjacent states are:\n\\begin{align*} \\label{eq:1}\n\\pi_0 p &= \\pi_1 q & (\\text{Flow } s_0 \\leftrightarrow s_1) \\\\\n\\pi_1 p &= \\pi_2 q & (\\text{Flow } s_1 \\leftrightarrow s_2) \\\\\n\\pi_2 p &= \\pi_3 q & (\\text{Flow } s_2 \\leftrightarrow s_3)\n\\end{align*}\nFrom these equations, we can express $\\pi_1$, $\\pi_2$, and $\\pi_3$ in terms of $\\pi_0$. Let $\\rho = p/q$.\n\\begin{align*}\n\\pi_1 &= \\pi_0 \\frac{p}{q} = \\pi_0 \\rho \\\\\n\\pi_2 &= \\pi_1 \\frac{p}{q} = (\\pi_0 \\rho) \\rho = \\pi_0 \\rho^2 \\\\\n\\pi_3 &= \\pi_2 \\frac{p}{q} = (\\pi_0 \\rho^2) \\rho = \\pi_0 \\rho^3\n\\end{align*}\nThe sum of the steady-state probabilities must be $1$:\n$$ \\pi_0 + \\pi_1 + \\pi_2 + \\pi_3 = 1 $$\nSubstituting the expressions in terms of $\\pi_0$:\n$$ \\pi_0 + \\pi_0 \\rho + \\pi_0 \\rho^2 + \\pi_0 \\rho^3 = 1 $$\n$$ \\pi_0 (1 + \\rho + \\rho^2 + \\rho^3) = 1 $$\nThis is a finite geometric series, so we can write:\n$$ \\pi_0 \\left( \\frac{1 - \\rho^4}{1 - \\rho} \\right) = 1 $$\n$$ \\pi_0 = \\frac{1 - \\rho}{1 - \\rho^4} = \\frac{1}{1 + \\rho + \\rho^2 + \\rho^3} $$\nGiven $p=0.6$ and $q=0.4$, we have $\\rho = \\frac{0.6}{0.4} = \\frac{3}{2}$.\nNow we find the denominator:\n$$ 1 + \\rho + \\rho^2 + \\rho^3 = 1 + \\frac{3}{2} + \\left(\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^3 = 1 + \\frac{3}{2} + \\frac{9}{4} + \\frac{27}{8} $$\n$$ = \\frac{8}{8} + \\frac{12}{8} + \\frac{18}{8} + \\frac{27}{8} = \\frac{8+12+18+27}{8} = \\frac{65}{8} $$\nSo, $\\pi_0 = \\frac{1}{65/8} = \\frac{8}{65}$.\nNow we can find the other probabilities:\n$$ \\pi_1 = \\pi_0 \\rho = \\frac{8}{65} \\times \\frac{3}{2} = \\frac{12}{65} $$\n$$ \\pi_2 = \\pi_1 \\rho = \\frac{12}{65} \\times \\frac{3}{2} = \\frac{18}{65} $$\n$$ \\pi_3 = \\pi_2 \\rho = \\frac{18}{65} \\times \\frac{3}{2} = \\frac{27}{65} $$\nThe steady-state probability distribution is $\\pi = (\\frac{8}{65}, \\frac{12}{65}, \\frac{18}{65}, \\frac{27}{65})$.\n\nNext, we compute the long-run misprediction rate, $M$. The predictor's rule is:\n- Predict 'Not-Taken' if the counter is in state $s_0$ or $s_1$.\n- Predict 'Taken' if the counter is in state $s_2$ or $s_3$.\n\nA misprediction occurs under two conditions:\n1. The predictor predicts 'Not-Taken' (is in state $s_0$ or $s_1$) and the branch outcome is 'Taken' (probability $p$).\n2. The predictor predicts 'Taken' (is in state $s_2$ or $s_3$) and the branch outcome is 'Not-Taken' (probability $q$).\n\nThe total misprediction rate is the sum of the probabilities of these disjoint events:\n$$ M = P(\\text{state} \\in \\{s_0, s_1\\}) \\times p + P(\\text{state} \\in \\{s_2, s_3\\}) \\times q $$\n$$ M = (\\pi_0 + \\pi_1) p + (\\pi_2 + \\pi_3) q $$\nUsing the calculated steady-state probabilities:\n$$ \\pi_0 + \\pi_1 = \\frac{8}{65} + \\frac{12}{65} = \\frac{20}{65} $$\n$$ \\pi_2 + \\pi_3 = \\frac{18}{65} + \\frac{27}{65} = \\frac{45}{65} $$\nNow, substitute these sums and the values of $p$ and $q$ into the expression for $M$:\n$$ M = \\left(\\frac{20}{65}\\right) p + \\left(\\frac{45}{65}\\right) q = \\left(\\frac{20}{65}\\right) \\frac{3}{5} + \\left(\\frac{45}{65}\\right) \\frac{2}{5} $$\n$$ M = \\frac{60}{325} + \\frac{90}{325} = \\frac{150}{325} $$\nSimplifying the fraction:\n$$ M = \\frac{150 \\div 25}{325 \\div 25} = \\frac{6}{13} $$\nThe long-run misprediction rate is $\\frac{6}{13}$.\n\nThe final answer requires a row vector containing the four steady-state probabilities followed by the misprediction rate.\nThe values are $(\\pi_0, \\pi_1, \\pi_2, \\pi_3, M) = (\\frac{8}{65}, \\frac{12}{65}, \\frac{18}{65}, \\frac{27}{65}, \\frac{6}{13})$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{8}{65} & \\frac{12}{65} & \\frac{18}{65} & \\frac{27}{65} & \\frac{6}{13} \\end{pmatrix} } $$"
        },
        {
            "introduction": "A predictor's accuracy is often limited by a phenomenon known as aliasing, where different execution paths destructively interfere by sharing the same prediction resources. This exercise  presents a carefully crafted scenario where two branches, despite having identical histories, have opposite outcomes, creating a worst-case scenario for a shared predictor table. By analyzing the resulting mispredictions, you will develop a concrete intuition for path interference and explore the fundamental trade-off between history length and predictor table size, governed by the pigeonhole principle.",
            "id": "3619712",
            "problem": "A two-level local branch predictor consists of a Local History Table (LHT) that stores, for each static branch, the last $m$ outcomes as an $m$-bit local history, and a Pattern History Table (PHT) of $S$ entries, each entry being a two-bit saturating counter. The PHT is indexed solely by the $m$-bit local history value (no branch address bits are used), meaning the PHT is shared across all branches and all identical $m$-bit histories map to the same PHT entry. The saturating counter update rule is the standard two-bit scheme: states $\\{00,01,10,11\\}$ predict $\\{\\text{NT},\\text{NT},\\text{T},\\text{T}\\}$ respectively, increment on a taken outcome, and decrement on a not-taken outcome, with saturation at the extremes. Assume all PHT counters are initialized to $01$.\n\nConsider two static conditional branches, $B_1$ and $B_2$, that are executed in a loop in the following epochal trace. At the start of each epoch, both $B_1$ and $B_2$ have their local histories equal to the $m$-bit zero vector $H = 0^{m}$ in the LHT. The epoch proceeds as:\n- Execute $B_1$ once, with actual outcome taken.\n- Execute $B_2$ once, with actual outcome not-taken.\n- Then, to restore the identical local-history condition for the next epoch, execute $B_1$ for $m$ consecutive not-taken outcomes, and execute $B_2$ for $m$ consecutive not-taken outcomes, so that both local histories are again $H = 0^{m}$ just before the next epoch.\n\nThus, at the moment each branch accesses the PHT for the first time in an epoch, both have identical last $m$ outcomes $H$, yet their immediate futures diverge (one taken, one not-taken). The PHT entry indexed by $H$ is therefore shared by $B_1$ and $B_2$ and receives alternating updates with taken and not-taken outcomes.\n\nStarting from fundamental definitions of two-level local prediction and the two-bit saturating counter dynamics, analyze the per-access prediction behavior at the shared PHT entry across many epochs and explain why this constitutes path interference. Argue from first principles how increasing $m$ with a fixed-size PHT can worsen mispredictions due to the pigeonhole principle when $2^{m} > S$ (more distinct histories than counters), making collisions inevitable and preventing disambiguation of divergent futures. Then, under direct indexing of the PHT by the $m$-bit local history (no hashing, no folding), determine the largest integer $m$ for which there exists a one-to-one mapping from all possible $m$-bit histories to PHT entries, thereby eliminating aliasing in principle.\n\nCompute this bound for a PHT of size $S = 2{,}048$, and report the single integer value $m_{\\max}$ that satisfies the above condition. No rounding is required.",
            "solution": "The problem is determined to be valid as it is scientifically grounded in the principles of computer architecture, specifically branch prediction, and is well-posed with all necessary information provided for a unique solution.\n\nThe problem describes a two-level local branch predictor. This predictor uses a Local History Table (LHT) to store an $m$-bit history vector for each static branch and a single, shared Pattern History Table (PHT) of size $S$. The PHT is indexed by the $m$-bit local history. Each entry in the PHT is a $2$-bit saturating counter, which determines the prediction and is updated based on the actual branch outcome. The states of the counter $\\{00, 01, 10, 11\\}$ correspond to predictions of Not-Taken (NT), NT, Taken (T), and T, respectively. The counter increments for a T outcome and decrements for an NT outcome, saturating at $11$ and $00$. All counters are initialized to the state $01$ (Weakly Not-Taken).\n\nWe first analyze the prediction behavior for the shared PHT entry. The problem sets up an epochal execution trace involving two branches, $B_1$ and $B_2$. At the start of each epoch, both branches have an identical local history of all zeros, $H = 0^m$. They both will therefore access the same PHT entry, which we denote as $PHT[0^m]$. Let the state of this counter at the beginning of an epoch be $C$. Initially, all counters are $01$, so at the start of the first epoch, $C = 01$.\n\nThe epoch unfolds as follows:\n$1$. $B_1$ is executed. Its history is $H = 0^m$. The predictor accesses $PHT[0^m]$, which is in state $01$. The prediction is NT. The actual outcome is T. This is a **misprediction**. The counter is updated by incrementing: $01 \\rightarrow 10$.\n$2$. $B_2$ is executed. Its history is also $H = 0^m$. The predictor accesses the same entry, $PHT[0^m]$, which is now in state $10$. The prediction is T. The actual outcome is NT. This is also a **misprediction**. The counter is updated by decrementing: $10 \\rightarrow 01$.\n\nAt the end of these two critical steps, the counter $PHT[0^m]$ has returned to its initial state of $01$. The subsequent restoration phase, where both branches execute $m$ consecutive NT outcomes, serves to reset their local histories in the LHT back to $0^m$ for the next epoch. During this restoration, the histories of $B_1$ and $B_2$ will change with each NT outcome. These new histories will likely map to different PHT entries, so the state of $PHT[0^m]$ is unaffected and remains $01$ for the start of the next epoch.\n\nTherefore, this cycle repeats in every epoch. $B_1$ always encounters the counter in state $01$, predicts NT, but is actually T, causing a misprediction and updating the counter to $10$. Subsequently, $B_2$ always encounters the counter in state $10$, predicts T, but is actually NT, causing another misprediction and resetting the counter to $01$. Both branches consistently mispredict due to their interaction at the shared PHT entry. This phenomenon is known as **path interference** or **aliasing**. Two distinct execution paths (one leading to branch $B_1$ being taken, another to $B_2$ being not-taken), which share a common history pattern leading up to the branch, are mapped to the same hardware prediction resource. Because their outcomes diverge, they \"fight\" for control of the predictor's state, leading to consistently poor prediction accuracy for both. Here, $B_1$ trains the predictor to expect a T outcome for the history $0^m$, while $B_2$ trains it to expect an NT outcome. The alternating execution prevents the predictor from learning either pattern correctly.\n\nNext, we consider the effect of increasing the history length $m$ while the PHT size $S$ remains fixed. The purpose of a longer history $m$ is to capture more complex patterns and provide better context for prediction, thereby disambiguating branch behaviors that might seem identical with a shorter history. The number of unique possible $m$-bit histories is $2^m$. These $2^m$ distinct histories must be mapped to the $S$ entries of the PHT.\n\nIf $2^m \\le S$, it is possible, in principle, to assign a unique PHT entry to each distinct history. This would eliminate aliasing between different history patterns. However, if we increase $m$ such that $2^m > S$, the **pigeonhole principle** dictates that multiple distinct history patterns must map to the same PHT entry. The number of histories grows exponentially with $m$, while the number of predictor entries $S$ is fixed. For a large $m$, the number of histories colliding at a single PHT entry becomes very large. This exacerbates the problem of path interference. For instance, a history $H_1$, which is strongly correlated with a 'Taken' outcome, might be mapped to the same PHT entry as a history $H_2$, which is strongly correlated with a 'Not-Taken' outcome. Their updates will destructively interfere, degrading prediction accuracy. Thus, while a larger $m$ theoretically provides more information for disambiguation, its benefit is negated and even reversed if the PHT is not large enough to support it, as the increased rate of aliasing pollutes the predictions. The predictor's performance can worsen significantly.\n\nFinally, we are asked to find the largest integer $m$, denoted $m_{\\max}$, for which a one-to-one mapping from all possible $m$-bit histories to PHT entries is possible. This condition eliminates aliasing between distinct history patterns. A one-to-one mapping requires that the number of distinct items to be mapped (the $2^m$ possible histories) does not exceed the number of available slots (the $S$ PHT entries). This can be expressed by the inequality:\n$$2^m \\le S$$\nGiven the PHT size $S = 2{,}048$. We must find the largest integer $m_{\\max}$ that satisfies:\n$$2^{m_{\\max}} \\le 2048$$\nTo solve for $m_{\\max}$, we can take the base-$2$ logarithm of both sides:\n$$\\log_2(2^{m_{\\max}}) \\le \\log_2(2048)$$\n$$m_{\\max} \\le \\log_2(2048)$$\nWe can compute $\\log_2(2048)$ by recognizing the powers of $2$:\n$$2^{10} = 1024$$\n$$2^{11} = 2 \\times 2^{10} = 2 \\times 1024 = 2048$$\nTherefore, $\\log_2(2048) = 11$. The inequality becomes:\n$$m_{\\max} \\le 11$$\nSince $m$ must be an integer, the largest integer value that satisfies this condition is $11$.\nThus, $m_{\\max} = 11$. For any history length $m > 11$, aliasing between different history patterns becomes unavoidable in a PHT of size $2{,}048$ with direct indexing.",
            "answer": "$$\\boxed{11}$$"
        },
        {
            "introduction": "Modern processors often employ tournament predictors, which pit multiple prediction strategies against each other and dynamically choose the winner for a given branch. The key to this design is the chooser mechanism, which must adapt to changes in branch behavior over time. This practice  models the dynamic adaptation of a tournament chooser as a branch's behavior shifts between two distinct 'seasons.' You will calculate the time it takes for the chooser to switch its preference, providing a quantitative measure of the predictor's learning rate and its built-in hysteresis.",
            "id": "3619732",
            "problem": "A superscalar core uses a tournament predictor that combines a global-history Exclusive-OR (XOR) index predictor (gshare) and a two-level local predictor. The tournament chooser employs a $c$-bit saturating counter per static branch, updated only when the two component predictors disagree: it increments by $+1$ if gshare is correct and the local predictor is incorrect, decrements by $-1$ if the local predictor is correct and gshare is incorrect, and does not change otherwise. The chooser selects gshare if the counter value is at least $2^{c-1}$ and selects the local predictor otherwise. The counter saturates at $0$ and $2^{c}-1$.\n\nConsider a single static branch with outcomes forming a trace that is piecewise periodic in time, with periodicity $p$. The workload alternates between two seasons:\n\nSeason $1$: The outcome of the branch is a deterministic function of the last $p$ global outcomes and of the Program Counter (PC). Assume the global-history XOR index predictor has a Global History Register (GHR) of length $p$, a Pattern History Table (PHT) large enough to avoid aliasing on this branch’s indices, and converges to perfect prediction after a warm-up shorter than one period $p$. Assume the local predictor uses per-branch local history of length $p-1$ and converges to mispredict exactly one outcome per period $p$ after its own warm-up. Under these conditions, assume disagreements occur exactly once per period $p$, and on each disagreement gshare is correct and the local predictor is incorrect. Let the season last sufficiently long that the chooser counter for this branch saturates to $2^{c}-1$ by the end of Season $1$.\n\nSeason $2$: The outcome of the branch shifts to a different deterministic function that depends on the per-branch local history of length $p-1$, and the local predictor converges to perfect prediction after a warm-up shorter than one period $p$. Assume that due to interference from other control-flow active in Season $2$, gshare mispredicts exactly one outcome per period $p$ for this branch and that all disagreements occur exactly once per period $p$, with the local predictor correct and gshare incorrect on each disagreement.\n\nStarting at the beginning of Season $2$, the tournament chooser counter for this branch is at $2^{c}-1$. Under the stated update rule and the Season $2$ disagreement behavior, derive a closed-form expression for the expected number of chooser updates, denoted $u$, required for the chooser to flip selection from gshare to the local predictor for this branch. Express your final answer for $u$ as a closed-form analytic expression in terms of $c$ only. No rounding is required. Do not include units.",
            "solution": "The problem requires the derivation of the number of chooser updates needed for a tournament predictor to switch its preference from a gshare predictor to a local predictor.\n\nLet $C$ denote the value of the $c$-bit saturating counter used by the tournament chooser. The value of $C$ can range from $0$ to $2^c - 1$.\nThe selection rule of the chooser is defined as follows:\n- The gshare predictor is selected if the counter value $C \\ge 2^{c-1}$.\n- The local predictor is selected if the counter value $C < 2^{c-1}$.\n\nThe scenario begins at the start of Season $2$. From the description of Season $1$, the chooser counter for the branch in question has saturated at its maximum value. Let $C_0$ be the initial counter value at the beginning of Season $2$.\n$$C_0 = 2^c - 1$$\nAt this initial state, the chooser selects the gshare predictor, because for any integer $c \\ge 1$ (a minimal $1$-bit counter), the inequality $2^c - 1 \\ge 2^{c-1}$ holds. For $c=1$, we have $1 \\ge 1$. For $c > 1$, $2^c - 2^{c-1} = 2^{c-1}$, so $2^c = 2^{c-1} + 2^{c-1}$. The inequality becomes $2^{c-1} + 2^{c-1} - 1 \\ge 2^{c-1}$, which simplifies to $2^{c-1} - 1 \\ge 0$, a condition that is true for $c \\ge 1$.\n\nDuring Season $2$, the problem states that disagreements between the component predictors occur exactly once per period $p$. In every instance of disagreement, the local predictor is correct and the gshare predictor is incorrect. According to the specified update rule, \"the chooser ... decrements by $-1$ if the local predictor is correct and gshare is incorrect\". This means that upon each update event in Season $2$, the counter value $C$ is decremented by $1$.\n\nLet $u$ be the number of chooser updates that have occurred since the start of Season $2$. Each update corresponds to a single disagreement event. The evolution of the counter value, $C_u$, after $u$ updates is given by the deterministic recurrence relation:\n$$C_u = C_{u-1} - 1$$\nwith the initial condition $C_0 = 2^c - 1$.\nUnrolling this recurrence, we obtain a closed-form expression for $C_u$:\n$$C_u = C_0 - u = (2^c - 1) - u$$\nThis expression is valid as long as the counter has not hit its saturation point at $0$, which occurs for $u \\ge 2^c - 1$.\n\nThe chooser is said to \"flip\" its selection from gshare to the local predictor when the counter value $C$ drops below the threshold $2^{c-1}$ for the first time. We are looking for the smallest integer number of updates, $u$, that satisfies this condition.\n$$C_u < 2^{c-1}$$\nSubstituting the expression for $C_u$:\n$$(2^c - 1) - u < 2^{c-1}$$\nWe must solve this inequality for $u$:\n$$u > (2^c - 1) - 2^{c-1}$$\nBy rewriting $2^c$ as $2 \\times 2^{c-1}$, we can simplify the right-hand side of the inequality:\n$$u > (2 \\times 2^{c-1}) - 2^{c-1} - 1$$\n$$u > (2-1) \\times 2^{c-1} - 1$$\n$$u > 2^{c-1} - 1$$\nSince $u$ must be an integer representing the count of updates, the smallest integer value for $u$ that satisfies this strict inequality is one greater than the lower bound:\n$$u = (2^{c-1} - 1) + 1 = 2^{c-1}$$\n\nTo confirm this result, we can inspect the state of the counter just before and at the time of the flip.\nLet's consider the number of updates $u = 2^{c-1}$.\nAt the $(u-1)$-th update, i.e., after $2^{c-1}-1$ updates, the counter value is:\n$$C_{u-1} = (2^c - 1) - (2^{c-1} - 1) = 2^c - 2^{c-1} = 2 \\times 2^{c-1} - 2^{c-1} = 2^{c-1}$$\nAt this point, $C_{u-1} = 2^{c-1}$, which satisfies the condition $C \\ge 2^{c-1}$. Thus, the chooser still selects the gshare predictor.\n\nAt the $u$-th update, i.e., after $2^{c-1}$ updates, the counter value becomes:\n$$C_u = C_{u-1} - 1 = 2^{c-1} - 1$$\nNow, the counter value is $C_u < 2^{c-1}$. This triggers the flip, and the chooser selects the local predictor for the first time.\n\nTherefore, the exact number of updates required for the chooser to flip is $2^{c-1}$. The problem statement asks for the \"expected number\" of updates. Since the system's evolution as described is entirely deterministic, the number of updates is a fixed quantity. In this case, the expected value is identical to this deterministic value. The final expression depends only on $c$, as requested.",
            "answer": "$$\\boxed{2^{c-1}}$$"
        }
    ]
}