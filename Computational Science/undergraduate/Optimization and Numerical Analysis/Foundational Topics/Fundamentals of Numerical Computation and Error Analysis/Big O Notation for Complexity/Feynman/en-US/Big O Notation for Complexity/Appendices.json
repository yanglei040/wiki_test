{
    "hands_on_practices": [
        {
            "introduction": "We begin with a practical problem from numerical linear algebra. Verifying if a vector is an eigenvector of a matrix is a fundamental task in scientific computing, and its efficiency is crucial for large-scale applications. This exercise  will guide you through analyzing a straightforward, iterative algorithm by breaking it down into its constituent parts and identifying the computational bottleneck, a core skill in performance analysis.",
            "id": "2156952",
            "problem": "In the development of a numerical linear algebra library, a function is needed to verify the results of an eigenvector algorithm. This verification function takes as input an $n \\times n$ square matrix $A$ with real-valued entries, and a non-zero $n$-dimensional column vector $x$, also with real-valued entries. The function must determine if $x$ is an eigenvector of $A$. The analysis of the algorithm's performance is crucial for large-scale computations.\n\nYour task is to determine the worst-case time complexity of the most efficient algorithm to perform this verification. The time complexity should be expressed using Big O notation as a function of the matrix dimension $n$. Assume that basic arithmetic operations (addition, subtraction, multiplication, division) and comparisons take constant time, $O(1)$.\n\nWhat is the tightest Big O time complexity for this verification procedure?\n\nA. $O(n)$\n\nB. $O(n \\log n)$\n\nC. $O(n^2)$\n\nD. $O(n^3)$\n\nE. $O(n!)$",
            "solution": "We must verify whether the given nonzero vector $x \\in \\mathbb{R}^{n}$ is an eigenvector of the given matrix $A \\in \\mathbb{R}^{n \\times n}$. By definition, $x$ is an eigenvector of $A$ if and only if there exists a scalar $\\lambda \\in \\mathbb{R}$ such that\n$$\nA x = \\lambda x,\n$$\nwith $x \\neq 0$ given.\n\nAn efficient verification algorithm proceeds as follows.\n\n1) Compute the matrix-vector product $y = A x$. For each $i \\in \\{1,\\dots,n\\}$,\n$$\ny_{i} = \\sum_{j=1}^{n} a_{ij} x_{j}.\n$$\nThis requires $n$ multiplications and $n-1$ additions per row, thus a total of $\\Theta(n^{2})$ arithmetic operations. Under the assumption that each arithmetic operation is $O(1)$, this step costs $O(n^{2})$.\n\n2) Determine a candidate eigenvalue $\\lambda$. Find an index $k$ with $x_{k} \\neq 0$ (which must exist since $x \\neq 0$). In the worst case this search costs $O(n)$. Define\n$$\n\\lambda = \\frac{y_{k}}{x_{k}},\n$$\nwhich takes $O(1)$ time once $k$ is found.\n\n3) Verify the equality $y = \\lambda x$ componentwise. For each $j \\in \\{1,\\dots,n\\}$:\n- If $x_{j} \\neq 0$, check whether $y_{j} = \\lambda x_{j}$.\n- If $x_{j} = 0$, check whether $y_{j} = 0$.\nThis step uses $O(n)$ comparisons and at most $O(n)$ multiplications.\n\nTherefore, the total running time of this algorithm is\n$$\nO(n^{2}) + O(n) + O(n) = O(n^{2}),\n$$\ndominated by the matrix-vector multiplication.\n\nTo see that no algorithm can asymptotically do better in the worst case, consider a lower bound argument. Suppose $x$ has no zero entries. Any algorithm that inspects fewer than all $n^{2}$ entries of $A$ leaves at least one entry $a_{pq}$ unexamined. Construct two matrices $A$ and $A'$ that agree on all examined entries and differ only in $a_{pq}$ by some $\\delta \\neq 0$. Then\n$$\n(A' x)_{p} = (A x)_{p} + \\delta x_{q},\n$$\nwhich changes the $p$-th component by a nonzero amount since $x_{q} \\neq 0$. By choosing $\\delta$ appropriately, one of $A$ or $A'$ can satisfy $A x = \\lambda x$ for some $\\lambda$, while the other does not. An algorithm that did not inspect $a_{pq}$ cannot distinguish these cases, so any correct algorithm must, in the worst case, inspect $\\Omega(n^{2})$ entries, implying a time lower bound of $\\Omega(n^{2})$.\n\nCombining the $O(n^{2})$ upper bound with the $\\Omega(n^{2})$ lower bound gives a tight $\\Theta(n^{2})$ bound. In Big O notation, the tightest choice among the options is $O(n^{2})$, which corresponds to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Many of the most powerful algorithms leverage recursionâ€”a strategy where a problem is solved by breaking it into smaller, identical subproblems. This practice  introduces a classic \"divide-and-conquer\" scenario, modeling algorithms like the famous Merge Sort. By analyzing its recurrence relation, you will see how this elegant structure leads to the highly efficient $O(n \\log n)$ complexity class, which is a hallmark of many optimal sorting and searching algorithms.",
            "id": "2156959",
            "problem": "A software engineer is designing a new algorithm, called 'LogSynthesizer', to process and consolidate server log files. The algorithm operates on a log file containing $n$ entries.\n\nThe LogSynthesizer algorithm is designed as follows:\n1. If the log file has only one entry (i.e., $n=1$), the algorithm performs a constant number of operations and terminates.\n2. If the log file has more than one entry, the algorithm performs the following three steps:\n    a. It divides the log file into two equal halves, each containing $n/2$ entries.\n    b. It recursively calls itself on each of these two halves independently.\n    c. After the two recursive calls return, it merges their processed results. This merging step requires a single pass through all the original $n$ entries to ensure consistency, and this pass takes a total time directly proportional to $n$.\n\nLet $T(n)$ be the function representing the total number of operations required by the LogSynthesizer algorithm for a log file of size $n$. Based on the description, determine the tightest asymptotic upper bound (Big O notation) for the time complexity of this algorithm.\n\nSelect the correct option from the following choices:\nA. $O(\\log n)$\n\nB. $O(n)$\n\nC. $O(n \\log n)$\n\nD. $O(n^2)$\n\nE. $O(2^n)$",
            "solution": "Let $T(n)$ denote the total operations. From the algorithm description:\n- Base case: $T(1)=\\Theta(1)$.\n- For $n>1$, the algorithm splits into two subproblems of size $n/2$, solves them recursively, and merges in linear time. Thus there exists a constant $c>0$ such that\n$$\nT(n)=2\\,T\\!\\left(\\frac{n}{2}\\right)+c\\,n.\n$$\n\nUsing a recursion tree argument, assume for simplicity that $n=2^{h}$ so that the tree has height $h=\\log_{2}(n)$. At level $i$ ($0 \\leq i \\leq h-1$), there are $2^{i}$ subproblems of size $n/2^{i}$. The total merging cost at level $i$ is\n$$\n2^{i}\\cdot c\\left(\\frac{n}{2^{i}}\\right)=c\\,n.\n$$\nSince there are $h=\\log_{2}(n)$ such levels before the leaves, the total non-leaf cost is\n$$\nc\\,n\\,\\log_{2}(n).\n$$\nAt the leaves, there are $2^{h}=n$ subproblems of size $1$, each costing $\\Theta(1)$, so the total leaf cost is $\\Theta(n)$.\n\nTherefore,\n$$\nT(n)=\\Theta\\!\\left(n\\log_{2}(n)\\right)+\\Theta(n)=\\Theta\\!\\left(n\\log_{2}(n)\\right).\n$$\nEquivalently, $T(n)=\\Theta\\!\\left(n\\log(n)\\right)$ up to a constant factor in the logarithm base. Hence the tightest asymptotic upper bound is $O(n\\log n)$, which corresponds to option C.\n\nBy the Master Theorem, with $a=2$, $b=2$, and $f(n)=c\\,n$, we have $n^{\\log_{b}a}=n$, and $f(n)=\\Theta\\!\\left(n^{\\log_{b}a}\\right)$, placing the recurrence in Case 2 and yielding the same result $T(n)=\\Theta\\!\\left(n\\log n\\right)$.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Building on the divide-and-conquer principle, this final practice  explores a more subtle and powerful optimization. Based on the Karatsuba algorithm for fast multiplication, it shows how a clever algebraic trick can reduce the number of recursive calls needed, leading to a surprising complexity that is substantially better than the naive $O(n^2)$ approach. This problem highlights how deep algorithmic insights can yield non-intuitive yet significant performance gains, pushing the boundaries of computational efficiency.",
            "id": "2156902",
            "problem": "A computer scientist is tasked with optimizing the multiplication of very large integers in a high-precision arithmetic library. The standard algorithm taught in elementary school has a time complexity of $O(n^2)$ for multiplying two $n$-digit numbers. To improve this, a divide-and-conquer approach is implemented.\n\nThis new method works as follows: to multiply two $n$-digit integers, the algorithm breaks each integer into two halves, each with approximately $n/2$ digits. Through an ingenious series of algebraic manipulations, the product of the original $n$-digit numbers can be computed by performing only three multiplications of numbers with $n/2$ digits. In addition to these recursive multiplications, the algorithm requires a fixed number of additions and subtractions on integers that have a number of digits proportional to $n$. The time complexity of adding or subtracting two $k$-digit numbers is linear in $k$, i.e., $O(k)$.\n\nLet $T(n)$ represent the total time complexity for multiplying two $n$-digit numbers using this new algorithm. Assuming $n$ is a power of 2 for simplicity, the recurrence relation governing this process can be accurately modeled as:\n$$T(n) = 3 T\\left(\\frac{n}{2}\\right) + f(n)$$\nwhere $f(n)$ represents the cost of the additions and subtractions, and is known to be in $\\Theta(n)$.\n\nWhich of the following expressions best describes the asymptotic time complexity of this fast multiplication algorithm?\n\nA. $O(n \\log_2 n)$\n\nB. $O(n^2)$\n\nC. $O(n^{\\log_2 3})$\n\nD. $O(n^{\\log_3 2})$\n\nE. $O(n^3)$",
            "solution": "We model the algorithm by the recurrence\n$$T(n)=3\\,T\\!\\left(\\frac{n}{2}\\right)+f(n),$$\nwith $f(n)\\in \\Theta(n)$.\n\nApply the Master Theorem with parameters $a=3$, $b=2$, and $f(n)\\in \\Theta\\!\\left(n^{d}\\right)$ where $d=1$. Compute the critical exponent:\n$$n^{\\log_{2}(a)}=n^{\\log_{2}(3)}.$$\nWe compare $f(n)$ to $n^{\\log_{2}(3)}$. Since $\\log_{2}(3)>1$, define $\\epsilon=\\log_{2}(3)-1>0$. Then\n$$f(n)\\in \\Theta(n)=O\\!\\left(n^{\\log_{2}(3)-\\epsilon}\\right).$$\nThis is Master Theorem case 1, so\n$$T(n)\\in \\Theta\\!\\left(n^{\\log_{2}(3)}\\right).$$\nAmong the options, this corresponds to $O\\!\\left(n^{\\log_{2}(3)}\\right)$, which is choice C.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}