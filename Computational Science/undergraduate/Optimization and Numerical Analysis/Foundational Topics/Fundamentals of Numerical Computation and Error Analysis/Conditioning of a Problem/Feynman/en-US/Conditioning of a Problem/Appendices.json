{
    "hands_on_practices": [
        {
            "introduction": "Understanding the sensitivity of a calculation to its inputs is a cornerstone of numerical analysis. This first practice explores this concept in a familiar context: computing the arithmetic mean. By calculating the relative condition number, you will quantify how much an error in a single data point can affect the final average, a crucial skill when dealing with real-world data that may contain outliers or measurement uncertainties .",
            "id": "2161810",
            "problem": "In numerical analysis, the conditioning of a problem refers to the sensitivity of its solution to small relative perturbations in the input data. Consider the problem of computing the arithmetic mean of a set of $n$ positive data points, $\\{x_1, x_2, \\dots, x_n\\}$. The mean is given by the function $M(x_1, \\dots, x_n) = \\frac{1}{n}\\sum_{i=1}^n x_i$.\n\nThe relative condition number, $\\kappa_k$, of this computation with respect to a specific data point $x_k$, measures how a relative error in $x_k$ is amplified in the computed mean. For a differentiable function $f$ of multiple variables $x_1, \\dots, x_n$, this condition number is defined as:\n$$ \\kappa_k = \\left| \\frac{\\partial f}{\\partial x_k} \\frac{x_k}{f} \\right| $$\nSuppose a physicist has collected the following five measurements for a certain quantity: $\\{12.5, 15.2, 11.8, 13.5, 90.0\\}$. The physicist is concerned that the last measurement might have a larger uncertainty and wants to quantify the sensitivity of the arithmetic mean to this specific value.\n\nCalculate the relative condition number of the arithmetic mean computation with respect to the fifth data point, $x_5 = 90.0$. Round your final answer to three significant figures.",
            "solution": "The arithmetic mean of $n$ positive data points is the function $M(x_{1},\\dots,x_{n})=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$. For a differentiable function $f$, the relative condition number with respect to $x_{k}$ is defined as\n$$\n\\kappa_{k}=\\left|\\frac{\\partial f}{\\partial x_{k}}\\frac{x_{k}}{f}\\right|.\n$$\nFor $f=M$, the partial derivative with respect to $x_{k}$ is\n$$\n\\frac{\\partial M}{\\partial x_{k}}=\\frac{1}{n}.\n$$\nTherefore,\n$$\n\\kappa_{k}=\\left|\\frac{1}{n}\\cdot\\frac{x_{k}}{M}\\right|=\\left|\\frac{x_{k}}{nM}\\right|.\n$$\nSince $nM=\\sum_{i=1}^{n}x_{i}$, this simplifies to\n$$\n\\kappa_{k}=\\left|\\frac{x_{k}}{\\sum_{i=1}^{n}x_{i}}\\right|.\n$$\nFor the given data $\\{12.5,15.2,11.8,13.5,90.0\\}$ with $n=5$, the sum is\n$$\n\\sum_{i=1}^{5}x_{i}=12.5+15.2+11.8+13.5+90.0=143.0,\n$$\nand the mean is\n$$\nM=\\frac{143.0}{5}=28.6.\n$$\nFor $x_{5}=90.0$,\n$$\n\\kappa_{5}=\\left|\\frac{1}{5}\\cdot\\frac{90.0}{28.6}\\right|=\\frac{90}{5\\cdot 28.6}=\\frac{90}{143}.\n$$\nNumerically,\n$$\n\\frac{90}{143}\\approx 0.629370\\ldots\n$$\nRounding to three significant figures gives $0.629$.",
            "answer": "$$\\boxed{0.629}$$"
        },
        {
            "introduction": "Not all formulas are created equal, especially in the world of finite-precision computing. This exercise delves into a classic pitfall known as catastrophic cancellation by examining a common formula for calculating sample variance. You will derive how this seemingly benign calculation can lead to a massive loss of accuracy when the data's standard deviation is small compared to its mean, a powerful lesson in choosing numerically stable algorithms .",
            "id": "2161813",
            "problem": "A quality control engineer is responsible for verifying the consistency of a manufacturing process for high-precision components. The engineer collects a sample of $N$ components and measures their masses, denoted by $x_1, x_2, \\ldots, x_N$. The sample mean of these masses is calculated as $\\bar{x}$, and the sample standard deviation is $S$. The process is known to be very stable, resulting in measurements where the standard deviation is significantly smaller than the mean (i.e., $S \\ll \\bar{x}$).\n\nThe engineer's analysis software computes the sample variance, $S^2$, using what is known as the \"computational formula.\" This method involves two steps. First, it calculates two intermediate quantities: $T_1 = \\sum_{i=1}^N x_i^2$ and $T_2 = N\\bar{x}^2$. Second, it uses these values to find the variance: $S^2 = \\frac{T_1 - T_2}{N-1}$.\n\nDue to the finite precision of computer arithmetic, the values stored by the software are not perfectly exact. Let the computed values be $\\hat{T}_1$ and $\\hat{T}_2$, and assume they have small relative errors $\\delta_1$ and $\\delta_2$ with respect to the true values, such that $\\hat{T}_1 = T_1(1+\\delta_1)$ and $\\hat{T}_2 = T_2(1+\\delta_2)$. The magnitudes of these initial errors are bounded by a small positive value $\\epsilon$, which represents the machine precision: $|\\delta_1| \\leq \\epsilon$ and $|\\delta_2| \\leq \\epsilon$.\n\nThe engineer is concerned that the subtraction of two large and nearly equal numbers, $T_1$ and $T_2$, might lead to a significant loss of precision. To investigate this, determine an upper bound for the magnitude of the relative error in the computed difference $D = T_1 - T_2$. The computed difference is $\\hat{D} = \\hat{T}_1 - \\hat{T}_2$. Your task is to derive this upper bound for the magnitude of the relative error, $\\left|\\frac{\\hat{D} - D}{D}\\right|$.\n\nExpress your final answer as a single closed-form analytic expression in terms of the number of samples $N$, the sample mean $\\bar{x}$, the sample standard deviation $S$, and the error bound $\\epsilon$.",
            "solution": "We begin with the exact quantities defined by\n$$\nT_{1}=\\sum_{i=1}^{N}x_{i}^{2},\\qquad T_{2}=N\\bar{x}^{2},\\qquad D=T_{1}-T_{2}.\n$$\nThe computed values satisfy\n$$\n\\hat{T}_{1}=T_{1}(1+\\delta_{1}),\\qquad \\hat{T}_{2}=T_{2}(1+\\delta_{2}),\\qquad |\\delta_{1}|\\leq \\epsilon,\\quad |\\delta_{2}|\\leq \\epsilon,\n$$\nand the computed difference is\n$$\n\\hat{D}=\\hat{T}_{1}-\\hat{T}_{2}=T_{1}(1+\\delta_{1})-T_{2}(1+\\delta_{2})=D+\\delta_{1}T_{1}-\\delta_{2}T_{2}.\n$$\nHence the relative error in the difference is\n$$\n\\frac{\\hat{D}-D}{D}=\\frac{\\delta_{1}T_{1}-\\delta_{2}T_{2}}{D}.\n$$\nTaking absolute values and using the triangle inequality together with the bounds on $\\delta_{1}$ and $\\delta_{2}$ gives\n$$\n\\left|\\frac{\\hat{D}-D}{D}\\right|\\leq \\frac{|\\delta_{1}|T_{1}+|\\delta_{2}|T_{2}}{|D|}\\leq \\epsilon\\,\\frac{T_{1}+T_{2}}{D}.\n$$\nTo express this in terms of $N$, $\\bar{x}$, and $S$, we use the standard identities for the sample mean and sample variance:\n$$\nS^{2}=\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2},\\qquad \\sum_{i=1}^{N}x_{i}^{2}=\\sum_{i=1}^{N}(x_{i}-\\bar{x})^{2}+N\\bar{x}^{2}.\n$$\nTherefore,\n$$\nT_{1}=(N-1)S^{2}+N\\bar{x}^{2},\\qquad T_{2}=N\\bar{x}^{2},\\qquad D=T_{1}-T_{2}=(N-1)S^{2}.\n$$\nSubstituting into the bound yields\n$$\n\\left|\\frac{\\hat{D}-D}{D}\\right|\\leq \\epsilon\\,\\frac{(N-1)S^{2}+2N\\bar{x}^{2}}{(N-1)S^{2}}=\\epsilon\\left(1+\\frac{2N\\bar{x}^{2}}{(N-1)S^{2}}\\right).\n$$\nThis bound is finite provided $S>0$; when $S=0$, $D=0$ and the relative error is undefined, which corresponds to the degenerate zero-variance case.",
            "answer": "$$\\boxed{\\epsilon\\left(1+\\frac{2N\\bar{x}^{2}}{(N-1)S^{2}}\\right)}$$"
        },
        {
            "introduction": "We now move from analyzing single formulas to the behavior of entire systems of linear equations, a fundamental task in computational engineering. This final practice is a hands-on computational experiment where you will confront the infamous Hilbert matrix, a textbook example of severe ill-conditioning. By writing a program to solve $A x = b$ and analyzing the results, you will empirically witness the dramatic link between a high condition number, the explosion of solution error, and the deceptive nature of a small residual .",
            "id": "2428600",
            "problem": "Write a complete program that empirically demonstrates the effect of problem conditioning on the accuracy of solving linear systems in standard floating-point arithmetic. Consider the linear system $A x = b$ where $A$ is the $n \\times n$ Hilbert matrix with entries $A_{i j} = \\dfrac{1}{i + j - 1}$ for $1 \\le i,j \\le n$. For each test case, define the exact solution vector $x_{\\text{true}} \\in \\mathbb{R}^n$ by $x_{\\text{true}} = \\mathbf{1}$ (all ones), and construct the right-hand side $b = A x_{\\text{true}}$. Then compute the numerical solution $\\hat{x}$ using standard floating-point arithmetic conforming to Institute of Electrical and Electronics Engineers (IEEE) 754 double precision (binary64) semantics. Using $\\hat{x}$, compute the following quantities:\n- The $2$-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\,\\|A^{-1}\\|_2$.\n- The relative forward error in the $2$-norm, $\\dfrac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2}$.\n- The scaled residual (a normalized backward error proxy), $\\dfrac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2}$.\n- The estimated number of correct decimal digits in the solution, defined as $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, where $\\varepsilon$ denotes the smallest positive normalized IEEE 754 double-precision number.\n\nTest Suite:\nEvaluate the above for the following problem sizes $n$:\n- $n = 3$,\n- $n = 6$,\n- $n = 10$,\n- $n = 12$.\n\nAll vector and matrix norms are the spectral norm (that is, the matrix $2$-norm and the vector $2$-norm). Angles are not involved. No physical units are involved.\n\nYour program must produce a single line of output containing all test results as a comma-separated list enclosed in square brackets, where each test result is itself a list of the form $[n,\\;\\kappa_2(A),\\;\\text{relative forward error},\\;\\text{scaled residual},\\;\\text{estimated digits}]$. The final output must therefore be a single line representing a list of lists, with no additional text. For example, the structure must be similar to $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$ but with the actual computed numbers in place of the placeholders.",
            "solution": "The problem statement presented is valid. It is a well-posed, scientifically grounded exercise in numerical linear algebra, designed to demonstrate the fundamental concepts of problem conditioning, forward error, and backward error. The problem is self-contained, with all necessary definitions and data provided. It does not violate any scientific principles, logic, or contain any ambiguities. We shall proceed with the solution.\n\nThe problem requires an empirical investigation into the effects of ill-conditioning on the solution of a linear system of equations, $A x = b$. The matrix $A$ is chosen to be the $n \\times n$ Hilbert matrix, a classic example of a severely ill-conditioned matrix. Its entries are given by $A_{i j} = \\frac{1}{i + j - 1}$ for $i, j$ from $1$ to $n$.\n\nThe core of numerical analysis is not only to compute a solution but also to understand its accuracy. The accuracy of the computed solution, which we denote $\\hat{x}$, is affected by two main factors: the stability of the algorithm used and the intrinsic sensitivity of the problem itself. This sensitivity is quantified by the condition number.\n\nFor a linear system $A x = b$, the $2$-norm condition number of the matrix $A$ is defined as:\n$$ \\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 $$\nwhere $\\| \\cdot \\|_2$ is the spectral norm (the largest singular value). A large condition number, $\\kappa_2(A) \\gg 1$, signifies an ill-conditioned problem, where small relative perturbations in the input data ($A$ or $b$) can lead to large relative changes in the solution $x$.\n\nWhen we solve $A x = b$ using floating-point arithmetic, round-off errors are inevitably introduced. A backward stable algorithm, such as the LU decomposition employed by standard solvers, produces a computed solution $\\hat{x}$ that is the exact solution to a slightly perturbed problem:\n$$ (A + \\delta A) \\hat{x} = b + \\delta b $$\nThe \"smallness\" of these perturbations $\\delta A$ and $\\delta b$ is a measure of the algorithm's backward stability. A key result in numerical analysis establishes the following bound on the relative forward error:\n$$ \\frac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2} \\le \\kappa_2(A) \\left( \\frac{\\|\\delta A\\|_2}{\\|A\\|_2} + \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} \\right) $$\nThe right-hand side parenthetical term is the relative backward error. For a backward stable algorithm operating with machine precision $\\varepsilon_{\\text{mach}}$, the backward error is typically of order $\\mathcal{O}(\\varepsilon_{\\text{mach}})$. Machine precision for IEEE 754 double-precision is approximately $2.22 \\times 10^{-16}$. Therefore, we expect the relative forward error to be bounded by approximately $\\kappa_2(A) \\cdot \\varepsilon_{\\text{mach}}$. This demonstrates that a large condition number amplifies the unavoidable round-off errors, potentially destroying the accuracy of the solution.\n\nThe residual vector is defined as $r = b - A \\hat{x}$. The norm of the residual, $\\|r\\|_2$, is related to the backward error. The problem asks for a specific scaled residual:\n$$ \\frac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2} $$\nThis quantity serves as a normalized proxy for the backward error. Due to the backward stability of the solver, we expect this value to remain small, on the order of $\\varepsilon_{\\text{mach}}$, even as the condition number grows and the forward error explodes. This is a crucial distinction: a small residual does not guarantee a small forward error.\n\nFinally, we estimate the number of correct decimal digits in the solution. This is directly related to the relative forward error. If the relative error is $10^{-k}$, the solution is accurate to roughly $k$ decimal digits. The given formula, $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, formalizes this. The term $\\varepsilon$ is the smallest positive normalized double-precision number, approximately $2.225 \\times 10^{-308}$, which prevents a logarithm of zero and handles cases where the error is smaller than representable precision allows.\n\nWe will now perform these calculations for the specified test suite of matrix sizes $n \\in \\{3, 6, 10, 12\\}$. For each $n$:\n1.  Construct the $n \\times n$ Hilbert matrix $A$.\n2.  Define the true solution $x_{\\text{true}}$ as a vector of $n$ ones.\n3.  Compute the right-hand side $b = A x_{\\text{true}}$.\n4.  Solve for the numerical solution $\\hat{x}$ using a standard linear solver.\n5.  Compute the four specified quantities: $\\kappa_2(A)$, relative forward error, scaled residual, and estimated digits.\n\nThe results will empirically validate the theory. The condition number of the Hilbert matrix grows extremely rapidly with $n$. For small $n$ (e.g., $n=3$), $\\kappa_2(A)$ is moderate, and we expect a reasonably accurate solution. As $n$ increases to $10$ and $12$, $\\kappa_2(A)$ will become enormous ($> 10^{13}$), leading to a relative forward error of order $1$ or greater, signifying a complete loss of accuracy. Throughout this process, the scaled residual should remain small, demonstrating the backward stability of the algorithm in contrast to the poor forward accuracy for an ill-conditioned problem.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the effect of problem conditioning on the accuracy\n    of solving linear systems Ax = b using the Hilbert matrix.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [3, 6, 10, 12]\n\n    results = []\n    \n    # Epsilon as defined in the problem: the smallest positive normalized\n    # IEEE 754 double-precision number.\n    smallest_norm_val = np.finfo(np.float64).smallest_normal\n\n    for n in test_cases:\n        # Step 1: Construct the n x n Hilbert matrix A.\n        # The problem statement indices are 1-based, but `scipy.linalg.hilbert` is 0-based,\n        # which results in the same matrix A_ij = 1 / ((i+1) + (j+1) - 1) for 0 = i, j  n.\n        A = hilbert(n)\n\n        # Step 2: Define the exact solution vector x_true (all ones).\n        x_true = np.ones(n)\n\n        # Step 3: Construct the right-hand side b = A * x_true.\n        # This ensures that b is consistent with A and x_true.\n        b = A @ x_true\n\n        # Step 4: Compute the numerical solution x_hat using a standard solver.\n        # numpy.linalg.solve uses LAPACK routines which are backward stable and\n        # operate in IEEE 754 double precision.\n        x_hat = np.linalg.solve(A, b)\n\n        # --- Calculate the required quantities ---\n\n        # The 2-norm condition number kappa_2(A).\n        kappa_2_A = np.linalg.cond(A, 2)\n\n        # The relative forward error in the 2-norm.\n        norm_x_true = np.linalg.norm(x_true, 2)\n        if norm_x_true == 0:\n            # Avoid division by zero, though not possible for x_true = 1.\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2)\n        else:\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2) / norm_x_true\n        \n        # The scaled residual (a normalized backward error proxy).\n        # residual = b - A @ x_hat\n        # norm_residual = ||b - A*x_hat||_2\n        # scaled_residual = norm_residual / (||A||_2 * ||x_hat||_2 + ||b||_2)\n        norm_A = np.linalg.norm(A, 2)\n        norm_x_hat = np.linalg.norm(x_hat, 2)\n        norm_b = np.linalg.norm(b, 2)\n        norm_residual = np.linalg.norm(b - A @ x_hat, 2)\n        \n        denominator = norm_A * norm_x_hat + norm_b\n        if denominator == 0:\n            # Handle potential division by zero.\n            scaled_res = norm_residual\n        else:\n            scaled_res = norm_residual / denominator\n\n        # The estimated number of correct decimal digits.\n        # est_digits = max(0, -log10(max(relative forward error, epsilon)))\n        log_val = max(rel_fwd_err, smallest_norm_val)\n        est_digits = max(0.0, -np.log10(log_val))\n        \n        # Store results for this test case.\n        results.append([n, kappa_2_A, rel_fwd_err, scaled_res, est_digits])\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a list of lists.\n    # Example: [[3, 5.2e+02, ...], [6, 1.5e+07, ...], ...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}