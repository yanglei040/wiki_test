{
    "hands_on_practices": [
        {
            "introduction": "The power of convex optimization lies in its efficiency, but not all problems are inherently convex. A key skill is learning to recognize when a non-convex problem can be transformed into a convex one through a clever change of variables. This exercise, based on a classic engineering design challenge, demonstrates how such a transformation can make an intractable problem solvable and is a crucial step in problem modeling. ",
            "id": "2164032",
            "problem": "The design of a standard cylindrical can to hold a specific quantity of a product is a classic optimization task. An engineer is tasked with designing a can with a fixed internal volume $V > 0$ that minimizes the total amount of material used, which is proportional to the can's total surface area. Let the radius of the can be $r$ and its height be $h$. The total surface area is given by the function $A(r, h) = 2\\pi r^2 + 2\\pi rh$, and the fixed volume is described by the equation $V = \\pi r^2 h$.\n\nIn the field of convex optimization, a problem is typically formulated as minimizing a convex function over a convex set. A set is convex if the line segment between any two points in the set is also in the set. An optimization problem is convex if its objective function is convex, its inequality constraints are defined by convex functions, and its equality constraints are affine. An affine function is a linear function plus a constant, i.e., of the form $f(\\mathbf{x}) = \\mathbf{a}^T \\mathbf{x} + b$.\n\nConsider two different mathematical formulations of this can design problem:\n\n1.  **Direct Formulation:** The optimization variables are the can's dimensions, $\\mathbf{x} = (r, h)$. The domain is restricted to physically meaningful values, i.e., $r > 0$ and $h > 0$. The objective is to minimize $A(r, h)$ subject to the constraint $\\pi r^2 h = V$.\n\n2.  **Transformed Formulation:** A change of variables is introduced: $y_1 = \\ln r$ and $y_2 = \\ln h$. The optimization variables are now $\\mathbf{y} = (y_1, y_2)$, which can take any real values.\n\nBy analyzing both formulations according to the definitions of a convex optimization problem, identify which of the following statements is correct.\n\nA. The direct formulation is a convex optimization problem, but the transformed formulation is not.\n\nB. The transformed formulation is a convex optimization problem, but the direct formulation is not.\n\nC. Both the direct and transformed formulations are convex optimization problems.\n\nD. Neither the direct nor the transformed formulation is a convex optimization problem.",
            "solution": "We analyze each formulation against the definition of a convex optimization problem: minimize a convex objective over a convex feasible set, with all equality constraints affine.\n\nDirect formulation:\n- Variables: $(r,h)$ with $r>0$, $h>0$.\n- Objective: $A(r,h)=2\\pi r^{2}+2\\pi r h$.\n- Constraint: $\\pi r^{2}h=V$.\n\nFirst, check convexity of the objective. Compute the Hessian with respect to $(r,h)$:\n$$\n\\nabla A(r,h)=\\begin{pmatrix}4\\pi r+2\\pi h \\\\ 2\\pi r\\end{pmatrix},\\quad\n\\nabla^{2}A(r,h)=\\begin{pmatrix}4\\pi & 2\\pi \\\\ 2\\pi & 0\\end{pmatrix}.\n$$\nThe determinant is $4\\pi\\cdot 0-(2\\pi)^{2}=-4\\pi^{2}<0$, so the Hessian is indefinite. Therefore $A$ is not convex on its domain. Moreover, the equality constraint $\\pi r^{2}h=V$ is not affine, which already violates the convex optimization requirements. The feasible set\n$$\n\\{(r,h): r>0,\\ h>0,\\ \\pi r^{2}h=V\\}\n$$\nis not convex: for instance, take $(r_{1},h_{1})=(a,\\,V/(\\pi a^{2}))$ and $(r_{2},h_{2})=(2a,\\,V/(\\pi (2a)^{2}))$ with $a>0$. Their midpoint\n$$\n\\left(\\tfrac{r_{1}+r_{2}}{2},\\,\\tfrac{h_{1}+h_{2}}{2}\\right)=\\left(\\tfrac{3a}{2},\\,\\tfrac{1}{2}\\left(\\tfrac{V}{\\pi a^{2}}+\\tfrac{V}{4\\pi a^{2}}\\right)\\right)\n$$\nsatisfies\n$$\n\\pi\\left(\\tfrac{3a}{2}\\right)^{2}\\left(\\tfrac{1}{2}\\left(\\tfrac{V}{\\pi a^{2}}+\\tfrac{V}{4\\pi a^{2}}\\right)\\right)=\\tfrac{45}{32}V\\neq V,\n$$\nso the segment between feasible points is not feasible. Hence the direct formulation is not a convex optimization problem.\n\nTransformed formulation:\n- Variables: $(y_{1},y_{2})$ with $y_{1}=\\ln r$, $y_{2}=\\ln h$, so $r=\\exp(y_{1})$, $h=\\exp(y_{2})$ and $y\\in\\mathbb{R}^{2}$.\n- Objective:\n$$\nA(y)=2\\pi\\exp(2y_{1})+2\\pi\\exp(y_{1}+y_{2}).\n$$\nEach term is of the form $c\\,\\exp(a^{T}y)$ with $c>0$, which is convex. More explicitly, the Hessians are\n$$\n\\nabla^{2}\\big(2\\pi\\exp(2y_{1})\\big)=2\\pi\\exp(2y_{1})\\begin{pmatrix}4&0\\\\0&0\\end{pmatrix}\\succeq 0,\\quad\n\\nabla^{2}\\big(2\\pi\\exp(y_{1}+y_{2})\\big)=2\\pi\\exp(y_{1}+y_{2})\\begin{pmatrix}1&1\\\\1&1\\end{pmatrix}\\succeq 0,\n$$\nso $A(y)$ is convex as a sum of convex functions.\n\n- Constraint: from $\\pi r^{2}h=V$ with $V>0$ and $\\pi>0$, take logarithms to obtain\n$$\n2y_{1}+y_{2}=\\ln(V)-\\ln(\\pi),\n$$\nwhich is an affine equality. The feasible set is an affine set, hence convex, and the domain $y\\in\\mathbb{R}^{2}$ is convex.\n\nTherefore, the transformed formulation is a convex optimization problem, while the direct formulation is not.\n\nThe correct choice is B.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "For constrained optimization problems, how can we be certain that a candidate point is the true minimum? The Karush-Kuhn-Tucker (KKT) conditions provide a rigorous and versatile framework for verifying optimality by examining the interplay between the objective function and the active constraints. This practice allows you to apply these fundamental conditions to a location optimization problem, building your skills in one of the most important theoretical tools in optimization. ",
            "id": "2163970",
            "problem": "A logistics company is designing the location of a new central warehouse. The planned location is at coordinates $(x, y)$. The primary operational cost is modeled as a function of the distances to two key suppliers. A simplified cost function, $C(x,y)$, is the sum of the squared Euclidean distances from the warehouse to the suppliers, who are located at $(4, 1)$ and $(2, 5)$.\n\nFurthermore, due to zoning regulations, the warehouse must be located within a region defined by the inequality $x + 2y \\le 4$.\n\nThe full optimization problem can be stated as:\nMinimize $C(x,y) = (x-4)^2 + (y-1)^2 + (x-2)^2 + (y-5)^2$\nSubject to $x + 2y \\le 4$.\n\nA junior analyst has proposed the point $(x_c, y_c) = (2, 1)$ as a potential optimal location. Your task is to verify if this point satisfies the Karush-Kuhn-Tucker (KKT) optimality conditions. Based on your analysis, select the correct statement from the options below.\n\nA. The point $(2, 1)$ does not satisfy the KKT conditions.\n\nB. The point $(2, 1)$ satisfies the KKT conditions, and the constraint is inactive ($\\lambda=0$).\n\nC. The point $(2, 1)$ satisfies the KKT conditions with a Lagrange multiplier $\\lambda = 2$.\n\nD. The point $(2, 1)$ satisfies the KKT conditions with a Lagrange multiplier $\\lambda = 4$.\n\nE. The point $(2, 1)$ satisfies the KKT conditions with a Lagrange multiplier $\\lambda = 12$.",
            "solution": "To determine if the point $(x_c, y_c) = (2, 1)$ satisfies the Karush-Kuhn-Tucker (KKT) conditions for the given constrained optimization problem, we must check four conditions: primal feasibility, dual feasibility, complementary slackness, and stationarity.\n\nFirst, let's define the problem in the standard form for applying KKT conditions.\nThe objective function is $C(x,y) = (x-4)^2 + (y-1)^2 + (x-2)^2 + (y-5)^2$.\nThe inequality constraint is $g(x,y) = x + 2y - 4 \\le 0$.\n\nThe KKT conditions for a minimization problem of this form are:\n1.  **Primal Feasibility**: $g(x,y) \\le 0$\n2.  **Dual Feasibility**: $\\lambda \\ge 0$ (where $\\lambda$ is the Lagrange multiplier associated with the constraint $g(x,y)$)\n3.  **Complementary Slackness**: $\\lambda \\cdot g(x,y) = 0$\n4.  **Stationarity**: $\\nabla C(x,y) + \\lambda \\nabla g(x,y) = \\vec{0}$\n\nLet's test the point $(x_c, y_c) = (2, 1)$ against these conditions.\n\n**Step 1: Check Primal Feasibility**\nWe substitute the point $(2, 1)$ into the constraint function $g(x,y)$:\n$g(2, 1) = (2) + 2(1) - 4 = 2 + 2 - 4 = 0$.\nThe condition is $g(2,1) \\le 0$, and since $0 \\le 0$, the point $(2, 1)$ is in the feasible region. It lies on the boundary of the feasible set.\n\n**Step 2: Check Stationarity**\nThis condition requires us to find a $\\lambda$ that satisfies $\\nabla C(2,1) + \\lambda \\nabla g(2,1) = \\vec{0}$.\nFirst, we compute the gradients of $C(x,y)$ and $g(x,y)$.\n\nThe objective function can be simplified by expansion:\n$C(x,y) = (x^2 - 8x + 16) + (y^2 - 2y + 1) + (x^2 - 4x + 4) + (y^2 - 10y + 25)$\n$C(x,y) = 2x^2 - 12x + 2y^2 - 12y + 46$\n\nThe gradient of $C(x,y)$ is:\n$\\nabla C(x,y) = \\left( \\frac{\\partial C}{\\partial x}, \\frac{\\partial C}{\\partial y} \\right) = (4x - 12, 4y - 12)$.\nAt the point $(2, 1)$, the gradient is:\n$\\nabla C(2, 1) = (4(2) - 12, 4(1) - 12) = (8 - 12, 4 - 12) = (-4, -8)$.\n\nThe gradient of the constraint function $g(x,y) = x + 2y - 4$ is:\n$\\nabla g(x,y) = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right) = (1, 2)$.\n\nNow we set up the stationarity equation:\n$$\\nabla C(2, 1) + \\lambda \\nabla g(2, 1) = \\vec{0}$$\n$$(-4, -8) + \\lambda (1, 2) = (0, 0)$$\nThis gives us a system of two linear equations for $\\lambda$:\n1.  $-4 + \\lambda(1) = 0 \\implies \\lambda = 4$\n2.  $-8 + \\lambda(2) = 0 \\implies 2\\lambda = 8 \\implies \\lambda = 4$\n\nBoth equations yield a consistent value, $\\lambda = 4$. So, the stationarity condition is satisfied if we choose $\\lambda = 4$.\n\n**Step 3: Check Dual Feasibility**\nThe value we found for the Lagrange multiplier is $\\lambda = 4$. We must check if it is non-negative.\nSince $4 \\ge 0$, the dual feasibility condition is satisfied.\n\n**Step 4: Check Complementary Slackness**\nThis condition requires $\\lambda \\cdot g(2,1) = 0$.\nFrom Step 1, we found $g(2,1) = 0$. Using $\\lambda=4$ from Step 2:\n$4 \\cdot 0 = 0$.\nThe complementary slackness condition is satisfied. This is expected, as the point is on the boundary of the constraint (the constraint is \"active\"), which generally implies a non-zero multiplier. Option B states that the constraint is inactive ($\\lambda=0$), which is incorrect.\n\n**Conclusion**\nThe point $(2, 1)$ satisfies all four KKT conditions with a unique Lagrange multiplier $\\lambda = 4$. Therefore, the point $(2,1)$ is a KKT point.\nComparing our result with the given options:\nA. The point $(2, 1)$ does not satisfy the KKT conditions. (Incorrect)\nB. The point $(2, 1)$ satisfies the KKT conditions, and the constraint is inactive ($\\lambda=0$). (Incorrect, $\\lambda=4$)\nC. The point $(2, 1)$ satisfies the KKT conditions with a Lagrange multiplier $\\lambda = 2$. (Incorrect, $\\lambda=4$)\nD. The point $(2, 1)$ satisfies the KKT conditions with a Lagrange multiplier $\\lambda = 4$. (Correct)\nE. The point $(2, 1)$ satisfies the KKT conditions with a Lagrange multiplier $\\lambda = 12$. (Incorrect, $\\lambda=4$)\n\nThe correct statement is D.",
            "answer": "$$\\boxed{D}$$"
        },
        {
            "introduction": "Many optimization algorithms find a solution iteratively, and Newton's method is one of the most powerful for its fast convergence. The core of the method is the calculation of a \"step\" or direction, which uses the function's local curvature to determine the best way to move toward the minimum. This exercise focuses on computing the Newton step, $\\Delta \\mathbf{x}_{\\text{nt}}$, giving you direct experience with the mechanics of this fundamental algorithm. ",
            "id": "2163993",
            "problem": "In the field of numerical optimization, Newton's method is an iterative algorithm used to find the minimum of a function. For an unconstrained minimization problem of a twice-differentiable function $f(\\mathbf{x})$ where $\\mathbf{x} \\in \\mathbb{R}^n$, each iteration updates the current point $\\mathbf{x}$ by a quantity called the Newton step, $\\Delta \\mathbf{x}_{\\text{nt}}$. This step is designed to find the minimum of the second-order Taylor approximation of $f$ at the current point $\\mathbf{x}$.\n\nThe Newton step is calculated by solving the linear system $\\nabla^2 f(\\mathbf{x}) \\Delta \\mathbf{x}_{\\text{nt}} = -\\nabla f(\\mathbf{x})$, which leads to the formula $\\Delta \\mathbf{x}_{\\text{nt}} = -(\\nabla^2 f(\\mathbf{x}))^{-1} \\nabla f(\\mathbf{x})$, assuming the Hessian matrix $\\nabla^2 f(\\mathbf{x})$ is invertible.\n\nConsider the function $f: \\mathbb{R}^2 \\to \\mathbb{R}$ defined by:\n$$f(x_1, x_2) = \\exp(x_1) + \\exp(x_2) + x_1^2 + x_1 x_2 + x_2^2 - 3x_1 - 4x_2$$\nYour task is to calculate the Newton step $$\\Delta \\mathbf{x}_{\\text{nt}} = \\begin{pmatrix} \\Delta x_1 \\\\ \\Delta x_2 \\end{pmatrix}$$ for this function at the point $\\mathbf{x} = (0, 0)$. Your final answer should be the two components, $\\Delta x_1$ and $\\Delta x_2$.",
            "solution": "For Newton's method, the step at a current point $\\mathbf{x}$ is defined by solving the linear system\n$$\n\\nabla^{2} f(\\mathbf{x})\\,\\Delta \\mathbf{x}_{\\text{nt}} = -\\nabla f(\\mathbf{x}),\n$$\nwhich is equivalent to\n$$\n\\Delta \\mathbf{x}_{\\text{nt}} = -\\left(\\nabla^{2} f(\\mathbf{x})\\right)^{-1}\\nabla f(\\mathbf{x}).\n$$\nGiven $f(x_{1},x_{2})=\\exp(x_{1})+\\exp(x_{2})+x_{1}^{2}+x_{1}x_{2}+x_{2}^{2}-3x_{1}-4x_{2}$, compute the gradient and Hessian.\n\nThe gradient is\n$$\n\\nabla f(x_{1},x_{2})=\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_{1}}\\\\[4pt]\n\\frac{\\partial f}{\\partial x_{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\exp(x_{1})+2x_{1}+x_{2}-3\\\\[4pt]\n\\exp(x_{2})+x_{1}+2x_{2}-4\n\\end{pmatrix}.\n$$\nEvaluating at $(x_{1},x_{2})=(0,0)$ gives\n$$\n\\nabla f(0,0)=\n\\begin{pmatrix}\n\\exp(0)+0+0-3\\\\[4pt]\n\\exp(0)+0+0-4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1-3\\\\[4pt]\n1-4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-2\\\\[4pt]\n-3\n\\end{pmatrix}.\n$$\n\nThe Hessian is\n$$\n\\nabla^{2} f(x_{1},x_{2})=\n\\begin{pmatrix}\n\\frac{\\partial^{2} f}{\\partial x_{1}^{2}} & \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}}\\\\[4pt]\n\\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{1}} & \\frac{\\partial^{2} f}{\\partial x_{2}^{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\exp(x_{1})+2 & 1\\\\[4pt]\n1 & \\exp(x_{2})+2\n\\end{pmatrix}.\n$$\nEvaluating at $(0,0)$ yields\n$$\n\\nabla^{2} f(0,0)=\n\\begin{pmatrix}\n\\exp(0)+2 & 1\\\\[4pt]\n1 & \\exp(0)+2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 & 1\\\\[4pt]\n1 & 3\n\\end{pmatrix}.\n$$\n\nThe Newton step $\\Delta \\mathbf{x}_{\\text{nt}}$ at $(0,0)$ solves\n$$\n\\begin{pmatrix}\n3 & 1\\\\\n1 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta x_{1}\\\\\n\\Delta x_{2}\n\\end{pmatrix}\n=\n-\\nabla f(0,0)\n=\n\\begin{pmatrix}\n2\\\\\n3\n\\end{pmatrix}.\n$$\nThis system is\n$$\n3\\Delta x_{1}+\\Delta x_{2}=2,\\qquad \\Delta x_{1}+3\\Delta x_{2}=3.\n$$\nFrom the first equation, $\\Delta x_{2}=2-3\\Delta x_{1}$. Substituting into the second,\n$\\Delta x_{1}+3(2-3\\Delta x_{1})=3 \\;\\Rightarrow\\; \\Delta x_{1}+6-9\\Delta x_{1}=3 \\;\\Rightarrow\\; -8\\Delta x_{1}=-3 \\;\\Rightarrow\\; \\Delta x_{1}=\\frac{3}{8}$.\nThen\n$\\Delta x_{2}=2-3\\cdot\\frac{3}{8}=\\frac{16}{8}-\\frac{9}{8}=\\frac{7}{8}$.\nThus,\n$$\n\\Delta \\mathbf{x}_{\\text{nt}}=\n\\begin{pmatrix}\n\\frac{3}{8}\\\\[2pt]\n\\frac{7}{8}\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{3}{8} \\\\ \\frac{7}{8} \\end{pmatrix}}$$"
        }
    ]
}