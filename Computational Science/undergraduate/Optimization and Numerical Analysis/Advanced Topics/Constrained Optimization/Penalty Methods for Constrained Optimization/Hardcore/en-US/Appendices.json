{
    "hands_on_practices": [
        {
            "introduction": "To master penalty methods, we begin with the fundamental procedure: transforming a constrained optimization problem into an unconstrained one. This first exercise provides a direct, hands-on application of the quadratic penalty method for a problem with a linear equality constraint. By analytically deriving the minimizer of the penalized function, you will build the core algebraic skills and see precisely how the solution depends on the penalty parameter $\\mu$ .",
            "id": "2193291",
            "problem": "Consider a constrained optimization problem where the objective is to minimize the function $f(x_1, x_2) = (x_1 - a)^2 + (x_2 - b)^2$ subject to the linear constraint $h(x_1, x_2) = x_1 + x_2 - c = 0$. Here, $a, b,$ and $c$ are real-valued constants.\n\nThis type of problem can be approached using the quadratic penalty method. The method involves creating an unconstrained objective function, called the penalty function, defined as $P(x_1, x_2; \\mu) = f(x_1, x_2) + \\frac{\\mu}{2}[h(x_1, x_2)]^2$, where $\\mu  0$ is a large positive constant known as the penalty parameter. The solution to the original constrained problem can be approximated by finding the minimizer of $P(x_1, x_2; \\mu)$. As $\\mu$ approaches infinity, this minimizer converges to the true solution of the constrained problem.\n\nDetermine the minimizer $(x_1^*(\\mu), x_2^*(\\mu))$ of the unconstrained penalty function $P(x_1, x_2; \\mu)$. Express your answer as a vector of two components, where each component is an analytical expression in terms of the parameters $a, b, c,$ and $\\mu$.",
            "solution": "We are given the penalty function\n$$\nP(x_{1},x_{2};\\mu)= (x_{1}-a)^{2}+(x_{2}-b)^{2}+\\frac{\\mu}{2}\\left(x_{1}+x_{2}-c\\right)^{2},\n$$\nwith $\\mu0$. To find its unconstrained minimizer, set the gradient to zero. The partial derivatives are\n$$\n\\frac{\\partial P}{\\partial x_{1}}=2(x_{1}-a)+\\mu(x_{1}+x_{2}-c),\n$$\n$$\n\\frac{\\partial P}{\\partial x_{2}}=2(x_{2}-b)+\\mu(x_{1}+x_{2}-c).\n$$\nSetting these equal to zero gives the linear system\n$$\n2(x_{1}-a)+\\mu(x_{1}+x_{2}-c)=0,\\qquad 2(x_{2}-b)+\\mu(x_{1}+x_{2}-c)=0.\n$$\nEquivalently,\n$$\n(2+\\mu)x_{1}+\\mu x_{2}=2a+\\mu c,\\qquad \\mu x_{1}+(2+\\mu)x_{2}=2b+\\mu c.\n$$\nIn matrix form,\n$$\n\\begin{pmatrix}\n2+\\mu  \\mu\\\\\n\\mu  2+\\mu\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1}\\\\\nx_{2}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2a+\\mu c\\\\\n2b+\\mu c\n\\end{pmatrix}.\n$$\nThe determinant of the coefficient matrix is $(2+\\mu)^{2}-\\mu^{2}=4(1+\\mu)$, which is positive for $\\mu0$, so the solution is unique. Using Cramerâ€™s rule,\n$$\nx_{1}=\\frac{(2a+\\mu c)(2+\\mu)-\\mu(2b+\\mu c)}{4(1+\\mu)}=\\frac{2a+\\mu(a-b+c)}{2(1+\\mu)},\n$$\n$$\nx_{2}=\\frac{(2+\\mu)(2b+\\mu c)-\\mu(2a+\\mu c)}{4(1+\\mu)}=\\frac{2b+\\mu(b-a+c)}{2(1+\\mu)}.\n$$\nAs $\\mu\\to\\infty$, these converge to the exact constrained solution $(\\frac{a-b+c}{2},\\frac{b-a+c}{2})$, consistent with the projection onto the line $x_{1}+x_{2}=c$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{2 a+\\mu\\left(a-b+c\\right)}{2\\left(1+\\mu\\right)}  \\frac{2 b+\\mu\\left(b-a+c\\right)}{2\\left(1+\\mu\\right)}\\end{pmatrix}}$$"
        },
        {
            "introduction": "Once you are comfortable with the mechanics, it's crucial to build a deeper intuition for how penalty methods behave. This next problem presents a special case: what happens when the unconstrained minimum of the objective function already satisfies the given constraint? By working through this scenario , you will discover how the penalty term operates and see that the method correctly identifies the solution, independent of the penalty parameter's value.",
            "id": "2193301",
            "problem": "Consider the application of the quadratic penalty method to an optimization problem. The goal is to find the minimizer $(x_{1,\\mu}, x_{2,\\mu})$ of a penalized objective function $P(x_1, x_2; \\mu)$ for a generic penalty parameter $\\mu  0$. The function is defined as:\n\n$$P(x_1, x_2; \\mu) = (x_1 - 1)^2 + (x_2 - 2)^2 + \\frac{1}{2\\mu} (x_1 + x_2 - 3)^2$$\n\nwhere $x_1$ and $x_2$ are real variables.\n\nFind the coordinates of the point $(x_{1,\\mu}, x_{2,\\mu})$ that minimizes this function. Your answer should be a point with constant coordinates that holds for any choice of $\\mu  0$. Express your final answer as a $1 \\times 2$ row matrix.",
            "solution": "We minimize the quadratic function $P(x_{1},x_{2};\\mu)=(x_{1}-1)^{2}+(x_{2}-2)^{2}+\\frac{1}{2\\mu}(x_{1}+x_{2}-3)^{2}$ for $\\mu0$. Since $P$ is a strictly convex quadratic in $(x_{1},x_{2})$ (its Hessian is the symmetric matrix\n$$\nH=\\begin{pmatrix}2+\\frac{1}{\\mu}  \\frac{1}{\\mu} \\\\ \\frac{1}{\\mu}  2+\\frac{1}{\\mu}\\end{pmatrix},\n$$\nwhich is strictly diagonally dominant and hence positive definite for $\\mu0$), the unique minimizer is characterized by the first-order optimality conditions $\\nabla P=0$.\n\nCompute the partial derivatives:\n$$\n\\frac{\\partial P}{\\partial x_{1}}=2(x_{1}-1)+\\frac{1}{\\mu}(x_{1}+x_{2}-3),\n$$\n$$\n\\frac{\\partial P}{\\partial x_{2}}=2(x_{2}-2)+\\frac{1}{\\mu}(x_{1}+x_{2}-3).\n$$\nSetting these to zero gives the linear system\n$$\n2(x_{1}-1)+\\frac{1}{\\mu}(x_{1}+x_{2}-3)=0,\\qquad 2(x_{2}-2)+\\frac{1}{\\mu}(x_{1}+x_{2}-3)=0.\n$$\nEquivalently,\n$$\n\\left(2+\\frac{1}{\\mu}\\right)x_{1}+\\frac{1}{\\mu}x_{2}=2+\\frac{3}{\\mu},\\qquad \\frac{1}{\\mu}x_{1}+\\left(2+\\frac{1}{\\mu}\\right)x_{2}=4+\\frac{3}{\\mu}.\n$$\nSubtracting the first equation from the second yields\n$$\n-2x_{1}+2x_{2}=2 \\;\\;\\Rightarrow\\;\\; x_{2}=x_{1}+1.\n$$\nSubstituting into the first equation,\n$$\n\\left(2+\\frac{1}{\\mu}\\right)x_{1}+\\frac{1}{\\mu}(x_{1}+1)=2+\\frac{3}{\\mu}\n\\;\\;\\Rightarrow\\;\\;\n\\left(2+\\frac{2}{\\mu}\\right)x_{1}+\\frac{1}{\\mu}=2+\\frac{3}{\\mu},\n$$\nso\n$$\n\\left(2+\\frac{2}{\\mu}\\right)x_{1}=2+\\frac{2}{\\mu}\n\\;\\;\\Rightarrow\\;\\;\nx_{1}=1.\n$$\nThen $x_{2}=x_{1}+1=2$. This solution is independent of $\\mu0$ and thus holds for any penalty parameter.",
            "answer": "$$\\boxed{\\begin{pmatrix}1  2\\end{pmatrix}}$$"
        },
        {
            "introduction": "Many real-world optimization problems involve inequality constraints, which are handled with a slightly more nuanced penalty formulation. This final practice introduces the standard penalty for inequalities, which uses a $\\max(0, g(x))$ term to ensure the penalty is only active when the constraint is violated. This exercise  not only gives you practice with this important case but also highlights a common pitfall, sharpening your analytical precision by contrasting the correct method with an erroneous one.",
            "id": "2193329",
            "problem": "Consider the one-dimensional constrained optimization problem of minimizing the objective function $f(x) = (x-c)^2$ with respect to the variable $x \\in \\mathbb{R}$. This minimization is subject to the single inequality constraint $x \\le b$. The parameters $c$ and $b$ are fixed real constants that satisfy the condition $c  b$.\n\nThis problem is to be analyzed using the quadratic penalty method, where the penalty parameter is denoted by $\\mu  0$. The penalized objective function for a generic problem of minimizing $f(x)$ subject to an inequality constraint $g(x) \\le 0$ is given by:\n$$P(x; \\mu) = f(x) + \\frac{1}{2\\mu} \\left( \\max(0, g(x)) \\right)^2$$\n\nFirst, let $x_I^*(\\mu)$ be the value of $x$ that minimizes the correctly formulated penalized objective function for the given inequality-constrained problem.\n\nNext, consider a common conceptual error where the analyst mistakenly treats the inequality constraint $x \\le b$ as an equality constraint, $x=b$. The penalized objective for a generic problem of minimizing $f(x)$ subject to an equality constraint $h(x) = 0$ is given by:\n$$P(x; \\mu) = f(x) + \\frac{1}{2\\mu} \\left( h(x) \\right)^2$$\nLet $x_E^*(\\mu)$ be the value of $x$ that minimizes the penalized objective function under this incorrect equality formulation.\n\nYour task is to determine the limiting value of the discrepancy between these two approaches as the penalty becomes infinitely stringent. Specifically, calculate the value of the limit $\\lim_{\\mu \\to 0^+} \\left( x_E^*(\\mu) - x_I^*(\\mu) \\right)$. Your final answer should be an expression in terms of the constants $b$ and $c$.",
            "solution": "We are given $f(x)=(x-c)^{2}$ with the inequality constraint $x\\le b$, where $cb$. For the quadratic penalty for an inequality $g(x)\\le 0$, we set $g(x)=x-b$, so the correctly penalized objective is\n$$\nP_{I}(x;\\mu)= (x-c)^{2}+\\frac{1}{2\\mu}\\left(\\max(0,x-b)\\right)^{2}.\n$$\nFor $x\\le b$, the penalty term is zero and $P_{I}(x;\\mu)=(x-c)^{2}$, which is minimized at $x=c$. Since $cb$, this minimizer is feasible and yields $P_{I}(c;\\mu)=0$. For $xb$, we would have\n$$\nP_{I}(x;\\mu)=(x-c)^{2}+\\frac{1}{2\\mu}(x-b)^{2}.\n$$\nThe stationary condition in this region is obtained by differentiating and setting to zero:\n$$\n2(x-c)+\\frac{1}{\\mu}(x-b)=0 \\;\\;\\Longrightarrow\\;\\; x=\\frac{b+2\\mu c}{1+2\\mu}.\n$$\nBecause $cb$, we have $b+2\\mu cb+2\\mu b=(1+2\\mu)b$, hence $\\frac{b+2\\mu c}{1+2\\mu}b$, so no stationary point lies in $xb$. Therefore the global minimizer of $P_{I}$ is $x_{I}^{*}(\\mu)=c$ for all $\\mu0$.\n\nUnder the incorrect equality-penalty formulation with $h(x)=x-b=0$, the penalized objective is\n$$\nP_{E}(x;\\mu)=(x-c)^{2}+\\frac{1}{2\\mu}(x-b)^{2}.\n$$\nThis is strictly convex. The first-order optimality condition is\n$$\n\\frac{d}{dx}P_{E}(x;\\mu)=2(x-c)+\\frac{1}{\\mu}(x-b)=0,\n$$\nwhich yields\n$$\nx_{E}^{*}(\\mu)=\\frac{2c+\\frac{b}{\\mu}}{2+\\frac{1}{\\mu}}=\\frac{b+2\\mu c}{1+2\\mu}.\n$$\n\nThe discrepancy is\n$$\nx_{E}^{*}(\\mu)-x_{I}^{*}(\\mu)=\\frac{b+2\\mu c}{1+2\\mu}-c=\\frac{b+2\\mu c-c(1+2\\mu)}{1+2\\mu}=\\frac{b-c}{1+2\\mu}.\n$$\nTaking the limit as $\\mu\\to 0^{+}$ gives\n$$\n\\lim_{\\mu\\to 0^{+}}\\left(x_{E}^{*}(\\mu)-x_{I}^{*}(\\mu)\\right)=b-c.\n$$",
            "answer": "$$\\boxed{b-c}$$"
        }
    ]
}