{
    "hands_on_practices": [
        {
            "introduction": "The power of the Alternating Direction Method of Multipliers (ADMM) stems from its ability to decompose a large, difficult optimization problem into a sequence of smaller, more manageable subproblems. This first practice explores the most common type of subproblem, where one of the objective functions is quadratic. By working through this exercise , you will derive a closed-form solution for an update step, a fundamental skill required for implementing ADMM in many practical scenarios involving smooth objective functions.",
            "id": "2153727",
            "problem": "The Alternating Direction Method of Multipliers (ADMM) is an algorithm that solves optimization problems of the form:\n$$ \\min_{x, z} f(x) + g(z) $$\n$$ \\text{subject to } Ax + Bz = b $$\nwhere variables are $x \\in \\mathbb{R}^n$ and $z \\in \\mathbb{R}^m$, and the problem data are given by matrices $A \\in \\mathbb{R}^{p \\times n}$, $B \\in \\mathbb{R}^{p \\times m}$, a vector $b \\in \\mathbb{R}^p$, and convex functions $f: \\mathbb{R}^n \\to \\mathbb{R}$ and $g: \\mathbb{R}^m \\to \\mathbb{R}$.\n\nThe algorithm is based on the augmented Lagrangian:\n$$ L_\\rho(x, z, y) = f(x) + g(z) + y^T(Ax + Bz - b) + \\frac{\\rho}{2}\\|Ax + Bz - b\\|_2^2 $$\nwhere $y \\in \\mathbb{R}^p$ is the dual variable (or Lagrange multiplier) and $\\rho  0$ is a penalty parameter. At each iteration $k$, ADMM performs the following updates sequentially:\n1.  $x^{k+1} := \\arg\\min_x L_\\rho(x, z^k, y^k)$\n2.  $z^{k+1} := \\arg\\min_z L_\\rho(x^{k+1}, z, y^k)$\n3.  $y^{k+1} := y^k + \\rho(Ax^{k+1} + Bz^{k+1} - b)$\n\nConsider a specific instance of this problem where the function $f(x)$ is defined as a quadratic function:\n$$ f(x) = \\frac{1}{2}\\|x - c\\|_2^2 $$\nfor a given constant vector $c \\in \\mathbb{R}^n$.\n\nDerive a closed-form analytical expression for the $x$-update step, $x^{k+1}$. Your expression should be in terms of the problem data $A, B, b, c$, the penalty parameter $\\rho$, and the values from the previous iteration, $z^k$ and $y^k$. In your derivation, let $I$ denote the $n \\times n$ identity matrix and assume the matrix $(I + \\rho A^T A)$ is invertible.",
            "solution": "We derive the $x$-update by minimizing the augmented Lagrangian with respect to $x$ while holding $z^{k}$ and $y^{k}$ fixed. The $x$-subproblem is\n$$\nx^{k+1} := \\arg\\min_{x}\\left\\{\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{T}(Ax + B z^{k} - b) + \\frac{\\rho}{2}\\|Ax + B z^{k} - b\\|_{2}^{2}\\right\\}.\n$$\nDefine $d := B z^{k} - b$, so the objective becomes\n$$\n\\frac{1}{2}\\|x-c\\|_{2}^{2} + (y^{k})^{T}(A x + d) + \\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}.\n$$\nTerms independent of $x$ do not affect the minimizer, so we focus on the $x$-dependent part. Taking the gradient with respect to $x$ and setting it to zero gives\n$$\n\\nabla_{x}\\left(\\frac{1}{2}\\|x-c\\|_{2}^{2}\\right) + \\nabla_{x}\\left((y^{k})^{T}A x\\right) + \\nabla_{x}\\left(\\frac{\\rho}{2}\\|A x + d\\|_{2}^{2}\\right) = 0,\n$$\nwhich simplifies to\n$$\n(x - c) + A^{T} y^{k} + \\rho A^{T}(A x + d) = 0.\n$$\nCollecting the terms in $x$ yields\n$$\n\\left(I + \\rho A^{T}A\\right) x = c - A^{T} y^{k} - \\rho A^{T} d.\n$$\nSubstituting $d = B z^{k} - b$, we have\n$$\n\\left(I + \\rho A^{T}A\\right) x = c - A^{T} y^{k} - \\rho A^{T}(B z^{k} - b).\n$$\nUnder the assumption that $\\left(I + \\rho A^{T}A\\right)$ is invertible, the unique minimizer is\n$$\nx^{k+1} = \\left(I + \\rho A^{T}A\\right)^{-1}\\left(c - A^{T} y^{k} - \\rho A^{T}(B z^{k} - b)\\right).\n$$",
            "answer": "$$\\boxed{\\left(I+\\rho A^{T}A\\right)^{-1}\\left(c-A^{T}y^{k}-\\rho A^{T}\\left(Bz^{k}-b\\right)\\right)}$$"
        },
        {
            "introduction": "Beyond smooth objectives, many optimization problems involve \"hard\" constraints, such as requiring a solution to lie within a specific set. This next practice demonstrates one of ADMM's most elegant features: its ability to handle such constraints through the use of indicator functions. As you will see in this problem , the seemingly complex minimization step for the constrained variable is transformed into a simple geometric projection, providing a powerful and intuitive tool for enforcing feasibility in your solutions.",
            "id": "2153782",
            "problem": "Let's consider an optimization problem of the form:\n$$\n\\begin{aligned}\n \\underset{x, z}{\\text{minimize}}\n  f(x) + g(z) \\\\\n \\text{subject to}\n  x - z = 0\n\\end{aligned}\n$$\nwhere $f$ and $g$ are convex functions. This problem can be solved using the Alternating Direction Method of Multipliers (ADMM). The scaled-form augmented Lagrangian for this problem is given by:\n$$L_{\\rho}(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2} \\|x - z + u\\|_2^2$$\nwhere $\\rho  0$ is a penalty parameter and $u$ is the scaled dual variable. The ADMM algorithm proceeds via the following iterative updates:\n1. $x^{k+1} := \\arg\\min_{x} \\left( f(x) + \\frac{\\rho}{2} \\|x - z^k + u^k\\|_2^2 \\right)$\n2. $z^{k+1} := \\arg\\min_{z} \\left( g(z) + \\frac{\\rho}{2} \\|x^{k+1} - z + u^k\\|_2^2 \\right)$\n3. $u^{k+1} := u^k + x^{k+1} - z^{k+1}$\n\nSuppose the function $g(z)$ is the indicator function of a non-empty closed convex set $\\mathcal{C}$. The indicator function, $I_{\\mathcal{C}}(z)$, is defined as:\n$$\nI_{\\mathcal{C}}(z) = \\begin{cases} 0  \\text{if } z \\in \\mathcal{C} \\\\ \\infty  \\text{if } z \\notin \\mathcal{C} \\end{cases}\n$$\nDetermine the closed-form expression for the $z$-update, $z^{k+1}$. Your answer should be expressed in terms of the projection operator $\\Pi_{\\mathcal{C}}$, the variable $x^{k+1}$, and the scaled dual variable $u^k$. The projection of a vector $v$ onto the set $\\mathcal{C}$ is defined as $\\Pi_{\\mathcal{C}}(v) = \\arg\\min_{w \\in \\mathcal{C}} \\|w - v\\|_2^2$.",
            "solution": "We start from the $z$-update in the scaled ADMM iterations:\n$$\nz^{k+1} := \\arg\\min_{z} \\left( g(z) + \\frac{\\rho}{2} \\|x^{k+1} - z + u^{k}\\|_{2}^{2} \\right).\n$$\nWith $g(z)=I_{\\mathcal{C}}(z)$, the objective is infinite whenever $z \\notin \\mathcal{C}$ and equals the quadratic term when $z \\in \\mathcal{C}$. Therefore, the minimization is equivalent to\n$$\nz^{k+1} = \\arg\\min_{z \\in \\mathcal{C}} \\frac{\\rho}{2} \\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nSince $\\rho0$ is a positive scalar, multiplying the objective by a positive constant does not change the minimizer, so we can drop the factor $\\rho/2$ without affecting the argmin:\n$$\nz^{k+1} = \\arg\\min_{z \\in \\mathcal{C}} \\|x^{k+1} - z + u^{k}\\|_{2}^{2}.\n$$\nUsing the symmetry of the Euclidean norm, we rewrite the argument as\n$$\n\\|x^{k+1} - z + u^{k}\\|_{2}^{2} = \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nThus,\n$$\nz^{k+1} = \\arg\\min_{z \\in \\mathcal{C}} \\|z - (x^{k+1} + u^{k})\\|_{2}^{2}.\n$$\nBy the definition of the projection operator $\\Pi_{\\mathcal{C}}(v) = \\arg\\min_{w \\in \\mathcal{C}} \\|w - v\\|_{2}^{2}$, we identify $v = x^{k+1} + u^{k}$ and obtain the closed-form update\n$$\nz^{k+1} = \\Pi_{\\mathcal{C}}(x^{k+1} + u^{k}).\n$$\nThis is the projection of $x^{k+1} + u^{k}$ onto the set $\\mathcal{C}$.",
            "answer": "$$\\boxed{\\Pi_{\\mathcal{C}}(x^{k+1}+u^{k})}$$"
        },
        {
            "introduction": "Now that we have explored the core mechanics for both smooth objectives and hard constraints, let's apply ADMM to a challenging problem from modern machine learning: sparse linear regression. This problem involves finding a predictive model that uses as few features as possible, a task complicated by a non-convex cardinality constraint. This exercise  showcases how ADMM's variable splitting can be used as a powerful heuristic, separating the smooth least-squares data-fitting term from the difficult sparsity constraint, paving the way for a practical algorithm.",
            "id": "2153785",
            "problem": "In sparse linear regression, the goal is to find a model that explains a set of observations using a minimal number of explanatory variables. One formulation for this problem seeks to minimize the sum of squared errors subject to a cardinality constraint on the model's coefficient vector. This problem is computationally difficult due to the non-convex nature of the cardinality constraint.\n\nConsider the following optimization problem:\n$$\n\\text{minimize} \\quad \\frac{1}{2}\\|y - Ax\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_0 \\le K\n$$\nHere, $y \\in \\mathbb{R}^m$ is the vector of observations, $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix, and $x \\in \\mathbb{R}^n$ is the coefficient vector we wish to find. The term $\\|x\\|_0$ denotes the $\\ell_0$-norm, which counts the number of non-zero elements in $x$, and $K$ is a positive integer representing the maximum desired number of non-zero coefficients.\n\nTo develop a heuristic algorithm for this problem, we can use the Alternating Direction Method of Multipliers (ADMM). The problem is first reformulated by introducing an auxiliary variable $z \\in \\mathbb{R}^n$ and an equality constraint:\n$$\n\\text{minimize} \\quad f(x) + g(z) \\quad \\text{subject to} \\quad x - z = 0\n$$\nwhere $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$ and $g(z)$ is the indicator function for the non-convex set $C = \\{z \\in \\mathbb{R}^n \\mid \\|z\\|_0 \\le K\\}$.\n\nThe scaled-form augmented Lagrangian for this problem is given by:\n$$\nL_\\rho(x, z, u) = f(x) + g(z) + \\frac{\\rho}{2}\\|x - z + u\\|_2^2\n$$\nwhere $u$ is the scaled dual variable and $\\rho  0$ is a penalty parameter. The ADMM algorithm proceeds via the following iterations at step $k$:\n1.  $x^{k+1} := \\arg\\min_x L_\\rho(x, z^k, u^k)$\n2.  $z^{k+1} := \\arg\\min_z L_\\rho(x^{k+1}, z, u^k)$\n3.  $u^{k+1} := u^k + x^{k+1} - z^{k+1}$\n\nAssuming that the matrix $A^TA + \\rho I$ is invertible, derive the closed-form expression for the $x$-update step, $x^{k+1}$, in terms of $A, y, \\rho, z^k,$ and $u^k$.",
            "solution": "We consider the $x$-update at iteration $k$, which minimizes $L_{\\rho}(x,z^{k},u^{k})$ with respect to $x$. Since $g(z^{k})$ does not depend on $x$, the $x$-subproblem is\n$$\n\\min_{x}\\ \\frac{1}{2}\\|y-Ax\\|_{2}^{2}+\\frac{\\rho}{2}\\|x-z^{k}+u^{k}\\|_{2}^{2}.\n$$\nDefine the objective\n$$\n\\phi(x)=\\frac{1}{2}(y-Ax)^{T}(y-Ax)+\\frac{\\rho}{2}(x-z^{k}+u^{k})^{T}(x-z^{k}+u^{k}).\n$$\nCompute the gradient with respect to $x$:\n$$\n\\nabla\\phi(x)=-A^{T}(y-Ax)+\\rho(x-z^{k}+u^{k})= -A^{T}y+A^{T}Ax+\\rho x-\\rho z^{k}+\\rho u^{k}.\n$$\nSet the gradient to zero for optimality:\n$$\n-A^{T}y+A^{T}Ax+\\rho x-\\rho z^{k}+\\rho u^{k}=0.\n$$\nRearrange to collect terms in $x$:\n$$\n(A^{T}A+\\rho I)x=A^{T}y+\\rho(z^{k}-u^{k}).\n$$\nBy the assumption that $A^{T}A+\\rho I$ is invertible, the unique minimizer is\n$$\nx^{k+1}=(A^{T}A+\\rho I)^{-1}\\left(A^{T}y+\\rho(z^{k}-u^{k})\\right).\n$$",
            "answer": "$$\\boxed{(A^{T}A+\\rho I)^{-1}\\left(A^{T}y+\\rho\\left(z^{k}-u^{k}\\right)\\right)}$$"
        }
    ]
}