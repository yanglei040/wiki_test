{
    "hands_on_practices": [
        {
            "introduction": "The Levenberg-Marquardt algorithm is fundamentally designed to minimize the sum of squared errors between a model and a set of data points. The first step in any iteration is to quantify these errors, known as residuals, for the current set of model parameters. This practice  provides a concrete exercise in calculating the residual vector $\\mathbf{r}$, a fundamental quantity representing the discrepancy between the model and the data that the algorithm directly seeks to reduce.",
            "id": "2217008",
            "problem": "A common task in optimization is fitting a geometric model to a set of data points. Consider the problem of fitting a circle to a set of points in a 2D plane. The circle is defined by a parameter vector $\\mathbf{p} = [x_c, y_c, R]^T$, where $(x_c, y_c)$ is the center of the circle and $R$ is its radius.\n\nFor a set of $n$ data points $(x_i, y_i)$, the goal of a nonlinear least squares fit is to find the parameter vector $\\mathbf{p}$ that minimizes the sum of squared residuals. The residual for the $i$-th data point, $r_i(\\mathbf{p})$, is defined as the difference between the point's distance to the proposed center $(x_c, y_c)$ and the proposed radius $R$.\n\nYou are given three data points representing measurements in a Cartesian coordinate system where all coordinates are in meters:\n$P_1 = (1.0, 7.0)$\n$P_2 = (6.0, 2.0)$\n$P_3 = (9.0, 8.0)$\n\nAn iterative optimization algorithm, such as the Levenberg-Marquardt algorithm, begins with an initial guess for the parameters. The initial guess for the circle's parameters is given as $\\mathbf{p}_0 = [x_{c,0}, y_{c,0}, R_0]^T = [5.0, 5.0, 4.0]^T$.\n\nCalculate the residual vector $\\mathbf{r}(\\mathbf{p}_0) = [r_1(\\mathbf{p}_0), r_2(\\mathbf{p}_0), r_3(\\mathbf{p}_0)]^T$ for this initial guess. Express each component of the resulting vector in meters, rounded to four significant figures. Present your final answer as a single row matrix.",
            "solution": "For a circle with parameters $\\mathbf{p} = [x_{c}, y_{c}, R]^{T}$ and a data point $(x_{i}, y_{i})$, the residual is defined as the difference between the Euclidean distance from the point to the center and the radius:\n$$\nr_{i}(\\mathbf{p}) = \\sqrt{(x_{i} - x_{c})^{2} + (y_{i} - y_{c})^{2}} - R.\n$$\nWith the initial guess $\\mathbf{p}_{0} = [5.0, 5.0, 4.0]^{T}$, we compute each residual.\n\nFor $P_{1} = (1.0, 7.0)$:\n$$\nd_{1} = \\sqrt{(1.0 - 5.0)^{2} + (7.0 - 5.0)^{2}} = \\sqrt{(-4.0)^{2} + (2.0)^{2}} = \\sqrt{16 + 4} = \\sqrt{20},\n$$\n$$\nr_{1}(\\mathbf{p}_{0}) = \\sqrt{20} - 4.0 \\approx 0.4721 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{2} = (6.0, 2.0)$:\n$$\nd_{2} = \\sqrt{(6.0 - 5.0)^{2} + (2.0 - 5.0)^{2}} = \\sqrt{(1.0)^{2} + (-3.0)^{2}} = \\sqrt{1 + 9} = \\sqrt{10},\n$$\n$$\nr_{2}(\\mathbf{p}_{0}) = \\sqrt{10} - 4.0 \\approx -0.8377 \\text{ (to four significant figures)}.\n$$\n\nFor $P_{3} = (9.0, 8.0)$:\n$$\nd_{3} = \\sqrt{(9.0 - 5.0)^{2} + (8.0 - 5.0)^{2}} = \\sqrt{(4.0)^{2} + (3.0)^{2}} = \\sqrt{16 + 9} = \\sqrt{25} = 5.0,\n$$\n$$\nr_{3}(\\mathbf{p}_{0}) = 5.0 - 4.0 = 1.000 \\text{ (to four significant figures)}.\n$$\n\nThus the residual vector, as a row matrix, is:\n$$\n\\begin{pmatrix}\n0.4721  -0.8377  1.000\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix} 0.4721  -0.8377  1.000 \\end{pmatrix}}$$"
        },
        {
            "introduction": "After quantifying the errors via the residual vector, the algorithm needs to know how to adjust the parameters to reduce those errors. This sensitivity information is captured in the Jacobian matrix, $\\mathbf{J}$, whose elements are the partial derivatives of each residual with respect to each parameter. This exercise  walks you through the construction of this crucial matrix, which forms the basis of the local linear model that the Levenberg-Marquardt algorithm uses to determine its search direction.",
            "id": "2217052",
            "problem": "In the field of nonlinear optimization, algorithms like the Levenberg-Marquardt algorithm are used to fit a model function to a set of data points by minimizing the sum of the squares of the residuals. A key component in this process is the Jacobian matrix of the residual vector.\n\nConsider a model used to describe the concentration of a substance over time, given by the two-parameter rational function:\n$$f(t; a, b) = \\frac{a}{1 + bt}$$\nwhere $t$ is the independent variable (e.g., time), and $\\mathbf{p} = [a, b]^T$ is the vector of parameters to be determined.\n\nSuppose we have collected the following three data points $(t_i, y_i)$:\n- $(t_1, y_1) = (1.0, 2.5)$\n- $(t_2, y_2) = (2.0, 1.8)$\n- $(t_3, y_3) = (4.0, 1.1)$\n\nThe residuals are defined as $r_i(\\mathbf{p}) = y_i - f(t_i; \\mathbf{p})$. The Jacobian matrix $\\mathbf{J}$ for this fitting problem is an $m \\times n$ matrix, where $m$ is the number of data points and $n$ is the number of parameters. Its elements are defined by $J_{ij} = \\frac{\\partial r_i(\\mathbf{p})}{\\partial p_j}$.\n\nCalculate the numerical value of the Jacobian matrix $\\mathbf{J}$ evaluated at the initial parameter guess $\\mathbf{p}_0 = [a_0, b_0]^T = [3.0, 0.5]^T$. All given numerical values are dimensionless.\n\nExpress your final answer as a $3 \\times 2$ matrix, with each element rounded to three significant figures.",
            "solution": "The residuals are $r_i(\\mathbf{p}) = y_i - f(t_i; \\mathbf{p})$ with the model function $f(t; a, b) = \\dfrac{a}{1 + bt}$. The Jacobian matrix is defined by $J_{ij} = \\dfrac{\\partial r_i(\\mathbf{p})}{\\partial p_{j}} = -\\dfrac{\\partial f(t_i; \\mathbf{p})}{\\partial p_j}$ for the parameter vector $\\mathbf{p} = [a, b]^{T}$. Thus, each row of $\\mathbf{J}$ corresponds to a data point $t_{i}$, and the two columns correspond to the derivatives with respect to $a$ and $b$.\n\nFirst, compute the partial derivatives of $f$ symbolically. Write $f(t; a, b) = a(1 + bt)^{-1}$. Then, using the power rule and chain rule:\n$$\n\\frac{\\partial f}{\\partial a} = (1 + bt)^{-1} = \\frac{1}{1 + bt},\n$$\n$$\n\\frac{\\partial f}{\\partial b} = a \\cdot \\frac{\\partial}{\\partial b}(1 + bt)^{-1} = a \\cdot \\left[-(1 + bt)^{-2} \\cdot t\\right] = -\\frac{a t}{(1 + bt)^{2}}.\n$$\n\nThe derivatives of the residuals are then $\\frac{\\partial r_i}{\\partial a} = -\\frac{1}{1 + bt_i}$ and $\\frac{\\partial r_i}{\\partial b} = \\frac{at_i}{(1 + bt_i)^2}$. Therefore, for each data point $t_{i}$, the Jacobian row is:\n$$\n\\left[-\\frac{1}{1 + b t_{i}},\\ \\frac{a t_{i}}{(1 + b t_{i})^{2}}\\right].\n$$\n\nEvaluate at the initial guess $\\mathbf{p}_{0} = [a_{0}, b_{0}]^{T} = [3.0, 0.5]^{T}$ and the given $t$ values.\n\nFor $t_{1} = 1$:\n$$\n1 + b_{0} t_{1} = 1 + 0.5 \\cdot 1 = 1.5,\n$$\n$$\n\\frac{\\partial r_1}{\\partial a} = -\\frac{1}{1.5} = -\\frac{2}{3},\\quad \\frac{\\partial r_1}{\\partial b} = \\frac{3 \\cdot 1}{(1.5)^{2}} = \\frac{3}{2.25} = \\frac{4}{3}.\n$$\n\nFor $t_{2} = 2$:\n$$\n1 + b_{0} t_{2} = 1 + 0.5 \\cdot 2 = 2,\n$$\n$$\n\\frac{\\partial r_2}{\\partial a} = -\\frac{1}{2},\\quad \\frac{\\partial r_2}{\\partial b} = \\frac{3 \\cdot 2}{2^{2}} = \\frac{6}{4} = \\frac{3}{2}.\n$$\n\nFor $t_{3} = 4$:\n$$\n1 + b_{0} t_{3} = 1 + 0.5 \\cdot 4 = 3,\n$$\n$$\n\\frac{\\partial r_3}{\\partial a} = -\\frac{1}{3},\\quad \\frac{\\partial r_3}{\\partial b} = \\frac{3 \\cdot 4}{3^{2}} = \\frac{12}{9} = \\frac{4}{3}.\n$$\n\nAssemble the Jacobian and round each element to three significant figures:\n$$\n\\mathbf{J}(\\mathbf{p}_{0}) \\approx\n\\begin{pmatrix}\n-0.667  1.33 \\\\\n-0.500  1.50 \\\\\n-0.333  1.33\n\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}-0.667  1.33 \\\\ -0.500  1.50 \\\\ -0.333  1.33\\end{pmatrix}}$$"
        },
        {
            "introduction": "With an understanding of the residual vector $\\mathbf{r}$ and the Jacobian matrix $\\mathbf{J}$, we can analyze the core of the algorithm: the update step calculation. This advanced exercise  delves into the mathematical structure of the Levenberg-Marquardt step by examining the simplified but highly insightful case of a linear least-squares problem. By deriving the relationship between the LM step and the exact step to the solution, you will see precisely how the damping parameter $\\lambda$ intelligently guides the optimization process.",
            "id": "2216994",
            "problem": "Consider the general problem of minimizing a sum of squared functions, $S(\\mathbf{x}) = \\frac{1}{2} \\sum_{i=1}^{m} [r_i(\\mathbf{x})]^2 = \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x})\\|_2^2$, where $\\mathbf{x} \\in \\mathbb{R}^n$ is a vector of parameters and $\\mathbf{r}(\\mathbf{x}) \\in \\mathbb{R}^m$ is a vector of residual functions.\n\nThe Levenberg-Marquardt (LM) algorithm is an iterative method for solving such problems. Starting from an initial guess $\\mathbf{x}_k$, the next iterate $\\mathbf{x}_{k+1}$ is found by $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{p}_k$, where the update step $\\mathbf{p}_k$ is the solution to the linear system:\n$$(J_k^T J_k + \\lambda I) \\mathbf{p}_k = -J_k^T \\mathbf{r}_k$$\nHere, $J_k$ is the Jacobian matrix of the residual vector $\\mathbf{r}(\\mathbf{x})$ evaluated at $\\mathbf{x}_k$, $\\mathbf{r}_k = \\mathbf{r}(\\mathbf{x}_k)$, $I$ is the identity matrix, and $\\lambda > 0$ is a damping parameter.\n\nNow, let's analyze a special case where the optimization problem is a linear least-squares problem. The residual vector is given by $\\mathbf{r}(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$, where $A$ is a real $m \\times n$ matrix with full column rank ($n \\le m$), and $\\mathbf{b}$ is a vector in $\\mathbb{R}^m$. This problem has a unique minimizer, which we will denote by $\\mathbf{x}^*$.\n\nLet $\\mathbf{x}_0$ be an arbitrary starting point for the algorithm. The exact step required to reach the solution in a single iteration is $\\mathbf{p}_{exact} = \\mathbf{x}^* - \\mathbf{x}_0$. Let $\\mathbf{p}_{LM}$ be the first update step calculated by the LM algorithm starting from $\\mathbf{x}_0$ with a given damping parameter $\\lambda$.\n\nYour task is to derive a relationship between the LM step and the exact step. Express the vector $\\mathbf{p}_{LM}$ in terms of the matrix $A$, the damping parameter $\\lambda$, and the vector $\\mathbf{p}_{exact}$.",
            "solution": "We consider the linear least-squares problem with residual $\\mathbf{r}(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$, where $A \\in \\mathbb{R}^{m \\times n}$ has full column rank, so $A^{T}A$ is symmetric positive definite and invertible. The unique minimizer $\\mathbf{x}^{*}$ satisfies the normal equations:\n$$\nA^{T}(A\\mathbf{x}^{*} - \\mathbf{b}) = 0 \\quad \\Longleftrightarrow \\quad A^{T}A\\,\\mathbf{x}^{*} = A^{T}\\mathbf{b}.\n$$\nStarting from $\\mathbf{x}_{0}$, the Levenberg-Marquardt step $\\mathbf{p}_{LM}$ solves\n$$\n(A^{T}A + \\lambda I)\\,\\mathbf{p}_{LM} = -A^{T}\\mathbf{r}_{0} = -A^{T}(A\\mathbf{x}_{0} - \\mathbf{b}) = -A^{T}A\\,\\mathbf{x}_{0} + A^{T}\\mathbf{b}.\n$$\nUsing the normal equations $A^{T}\\mathbf{b} = A^{T}A\\,\\mathbf{x}^{*}$, the right-hand side becomes\n$$\n-A^{T}A\\,\\mathbf{x}_{0} + A^{T}\\mathbf{b} = A^{T}A\\,\\mathbf{x}^{*} - A^{T}A\\,\\mathbf{x}_{0} = A^{T}A\\,(\\mathbf{x}^{*} - \\mathbf{x}_{0}) = A^{T}A\\,\\mathbf{p}_{exact}.\n$$\nTherefore, the LM step satisfies\n$$\n(A^{T}A + \\lambda I)\\,\\mathbf{p}_{LM} = A^{T}A\\,\\mathbf{p}_{exact},\n$$\nand, since $A^{T}A + \\lambda I$ is invertible for $\\lambda > 0$, we obtain\n$$\n\\mathbf{p}_{LM} = (A^{T}A + \\lambda I)^{-1}A^{T}A\\,\\mathbf{p}_{exact}.\n$$\nThis expresses $\\mathbf{p}_{LM}$ in terms of $A$, $\\lambda$, and $\\mathbf{p}_{exact}$, as required.",
            "answer": "$$\\boxed{(A^{T}A+\\lambda I)^{-1}A^{T}A\\,\\mathbf{p}_{exact}}$$"
        }
    ]
}