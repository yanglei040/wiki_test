{
    "hands_on_practices": [
        {
            "introduction": "We begin our hands-on journey with the most fundamental concept in regularization. This exercise provides a geometric intuition for Tikhonov regularization by framing it as a search for the \"smallest\" solution—the one with the minimum Euclidean norm—that satisfies a given constraint. By solving this problem , you will see how adding a simple penalty on solution size transforms an ill-posed problem with infinite solutions into a well-posed one with a unique, stable answer.",
            "id": "2197177",
            "problem": "A simplified model for a resource allocation system involves two control variables, $x_1$ and $x_2$. To achieve a target output, these variables must satisfy the linear constraint equation $3x_1 + 2x_2 = 10$. The system is designed to operate with minimal energy expenditure. The cost of operation, denoted by $L$, is directly proportional to the sum of the squares of the control variables, which means minimizing $L = x_1^2 + x_2^2$.\n\nFrom the infinite number of pairs $(x_1, x_2)$ that satisfy the constraint, determine the unique pair that also minimizes the cost function $L$. Present your answer as a pair of values $(x_1, x_2)$.",
            "solution": "The problem requires us to find the values of $x_1$ and $x_2$ that minimize the objective function $L(x_1, x_2) = x_1^2 + x_2^2$ subject to the linear constraint $3x_1 + 2x_2 = 10$. This is a constrained optimization problem.\n\nFirst, we can use the constraint equation to express one variable in terms of the other. Let's solve the constraint for $x_2$:\n$$3x_1 + 2x_2 = 10$$\n$$2x_2 = 10 - 3x_1$$\n$$x_2 = 5 - \\frac{3}{2}x_1$$\n\nNow, we can substitute this expression for $x_2$ into the objective function $L$. This transforms $L$ into a function of a single variable, $x_1$.\n$$L(x_1) = x_1^2 + \\left(5 - \\frac{3}{2}x_1\\right)^2$$\n\nTo find the value of $x_1$ that minimizes $L$, we first expand the expression for $L(x_1)$:\n$$L(x_1) = x_1^2 + \\left(5^2 - 2(5)\\left(\\frac{3}{2}x_1\\right) + \\left(\\frac{3}{2}x_1\\right)^2\\right)$$\n$$L(x_1) = x_1^2 + 25 - 15x_1 + \\frac{9}{4}x_1^2$$\n\nCombine the terms involving $x_1^2$:\n$$L(x_1) = \\left(1 + \\frac{9}{4}\\right)x_1^2 - 15x_1 + 25$$\n$$L(x_1) = \\frac{13}{4}x_1^2 - 15x_1 + 25$$\n\nThis is a quadratic function of $x_1$ that opens upwards (since the coefficient $\\frac{13}{4}$ is positive), so it has a unique minimum. To find the minimum, we take the derivative of $L(x_1)$ with respect to $x_1$ and set it to zero.\n$$\\frac{dL}{dx_1} = \\frac{d}{dx_1}\\left(\\frac{13}{4}x_1^2 - 15x_1 + 25\\right)$$\n$$\\frac{dL}{dx_1} = 2 \\cdot \\frac{13}{4}x_1 - 15$$\n$$\\frac{dL}{dx_1} = \\frac{13}{2}x_1 - 15$$\n\nSet the derivative to zero to find the critical point:\n$$\\frac{13}{2}x_1 - 15 = 0$$\n$$\\frac{13}{2}x_1 = 15$$\n$$x_1 = 15 \\cdot \\frac{2}{13}$$\n$$x_1 = \\frac{30}{13}$$\n\nNow that we have the value for $x_1$, we can substitute it back into the expression for $x_2$ derived from the constraint:\n$$x_2 = 5 - \\frac{3}{2}x_1$$\n$$x_2 = 5 - \\frac{3}{2}\\left(\\frac{30}{13}\\right)$$\n$$x_2 = 5 - \\frac{90}{26}$$\n$$x_2 = 5 - \\frac{45}{13}$$\n\nTo subtract the fraction, we find a common denominator:\n$$x_2 = \\frac{5 \\cdot 13}{13} - \\frac{45}{13}$$\n$$x_2 = \\frac{65 - 45}{13}$$\n$$x_2 = \\frac{20}{13}$$\n\nThus, the pair of control variables that satisfies the constraint and minimizes the cost function is $(x_1, x_2) = \\left(\\frac{30}{13}, \\frac{20}{13}\\right)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{30}{13}  \\frac{20}{13} \\end{pmatrix}}$$"
        },
        {
            "introduction": "While Tikhonov regularization is powerful, it often produces \"dense\" solutions where all components are non-zero. This practice introduces an alternative, LASSO ($L_1$) regularization, which is famous for promoting \"sparse\" solutions where many components are exactly zero. By directly comparing the outcomes of Tikhonov and LASSO for the same underdetermined system , you will gain a clear understanding of their distinct behaviors and the trade-offs involved in choosing a regularization strategy.",
            "id": "2197169",
            "problem": "Consider an underdetermined linear system $Ax = b$, where the solution vector is $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$, the matrix is $A = \\begin{pmatrix} 2  1 \\end{pmatrix}$, and the right-hand side is the scalar $b=4$. This system, representing the single equation $2x_1 + x_2 = 4$, has an infinite number of solutions. To select a unique and desirable solution, we can employ regularization techniques.\n\nThis problem explores two common regularization methods:\n\n1.  **Tikhonov Regularization (L2 norm):** The Tikhonov-regularized solution, denoted by $x_T$, is the vector $x$ that minimizes the objective function $F(x) = \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_2^2$. Here, $\\|v\\|_2 = \\sqrt{\\sum_i v_i^2}$ is the standard Euclidean norm (or L2 norm) of a vector $v$.\n\n2.  **LASSO Regularization (L1 norm):** The LASSO solution, denoted by $x_L$, is the vector $x$ that minimizes the objective function $G(x) = \\|Ax-b\\|_2^2 + \\lambda \\|x\\|_1$. Here, $\\|v\\|_1 = \\sum_i |v_i|$ is the taxicab norm (or L1 norm) of a vector $v$. The acronym LASSO stands for Least Absolute Shrinkage and Selection Operator.\n\nUsing a regularization parameter of $\\lambda = 4$ for both methods, calculate the ratio of the L1 norms of the two resulting solution vectors, $\\frac{\\|x_T\\|_1}{\\|x_L\\|_1}$.\n\nExpress your answer as an exact fraction in simplest form.",
            "solution": "We have $A=\\begin{pmatrix}2  1\\end{pmatrix}$, $b=4$, and $x=\\begin{pmatrix}x_{1} \\\\ x_{2}\\end{pmatrix}$.\n\nFor Tikhonov regularization with parameter $\\lambda=4$, we minimize $F(x)=\\|Ax-b\\|_{2}^{2}+4\\|x\\|_{2}^{2}$. The first-order optimality condition for ridge regression is\n$$(A^{T}A+\\lambda I)x=A^{T}b.$$\nCompute\n$$A^{T}A=\\begin{pmatrix}4  2 \\\\ 2  1\\end{pmatrix},\\quad \\lambda I=\\begin{pmatrix}4  0 \\\\ 0  4\\end{pmatrix},\\quad A^{T}b=\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}.$$\nHence\n$$\\begin{pmatrix}8  2 \\\\ 2  5\\end{pmatrix}x=\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}.$$\nSolving,\n$$\\det=8\\cdot 5-2\\cdot 2=36,\\quad x_{T}=\\frac{1}{36}\\begin{pmatrix}5  -2 \\\\ -2  8\\end{pmatrix}\\begin{pmatrix}8 \\\\ 4\\end{pmatrix}=\\frac{1}{36}\\begin{pmatrix}32 \\\\ 16\\end{pmatrix}=\\begin{pmatrix}\\frac{8}{9} \\\\ \\frac{4}{9}\\end{pmatrix}.$$\nThus\n$$\\|x_{T}\\|_{1}=\\left|\\frac{8}{9}\\right|+\\left|\\frac{4}{9}\\right|=\\frac{12}{9}=\\frac{4}{3}.$$\n\nFor LASSO with $\\lambda=4$, we minimize $G(x)=\\|Ax-b\\|_{2}^{2}+4\\|x\\|_{1}$. Let $r=Ax-b=2x_{1}+x_{2}-4$. The subgradient optimality condition is\n$$2A^{T}(Ax-b)+\\lambda z=0,\\quad z\\in\\partial\\|x\\|_{1}.$$\nSince $A^{T}(Ax-b)=\\begin{pmatrix}2r \\\\ r\\end{pmatrix}$, we obtain\n$$\\begin{pmatrix}4r \\\\ 2r\\end{pmatrix}+4\\begin{pmatrix}z_{1} \\\\ z_{2}\\end{pmatrix}=0\\quad\\Rightarrow\\quad z_{1}=-r,\\; z_{2}=-\\frac{r}{2}.$$\nFeasibility requires $z_{i}\\in\\partial|x_{i}|$: if $x_{i}\\neq 0$, then $z_{i}=\\operatorname{sign}(x_{i})\\in\\{\\pm 1\\}$; if $x_{i}=0$, then $z_{i}\\in[-1,1]$.\n\n- If both $x_{1}\\neq 0$ and $x_{2}\\neq 0$, then $z_{1},z_{2}\\in\\{\\pm 1\\}$, which would require simultaneously $-r\\in\\{\\pm 1\\}$ and $-r/2\\in\\{\\pm 1\\}$; there is no $r$ satisfying both equalities, so this is impossible.\n\n- If $x_{1}=0$ and $x_{2}\\neq 0$, then $z_{1}=-r\\in[-1,1]$ implies $r\\in[-1,1]$, but $z_{2}=-r/2=\\operatorname{sign}(x_{2})\\in\\{\\pm 1\\}$ forces $r\\in\\{-2,2\\}$, a contradiction.\n\n- If $x_{2}=0$ and $x_{1}\\neq 0$, then $z_{1}=-r=\\operatorname{sign}(x_{1})\\in\\{\\pm 1\\}$ implies $r\\in\\{-1,1\\}$, and $z_{2}=-r/2\\in[-1,1]$ holds automatically for both values. Check consistency with $r=2x_{1}-4$:\n  - If $r=-1$, then $2x_{1}-4=-1$ gives $x_{1}=\\frac{3}{2}$, which matches $\\operatorname{sign}(x_{1})=+1$ so $z_{1}=-r=1$.\n  - If $r=1$, then $2x_{1}-4=1$ gives $x_{1}=\\frac{5}{2}$, which would require $z_{1}=-1$, contradicting $\\operatorname{sign}(x_{1})=+1$.\n\nTherefore the LASSO minimizer is\n$$x_{L}=\\begin{pmatrix}\\frac{3}{2} \\\\ 0\\end{pmatrix},\\quad \\|x_{L}\\|_{1}=\\frac{3}{2}.$$\n\nFinally, the requested ratio is\n$$\\frac{\\|x_{T}\\|_{1}}{\\|x_{L}\\|_{1}}=\\frac{\\frac{4}{3}}{\\frac{3}{2}}=\\frac{4}{3}\\cdot\\frac{2}{3}=\\frac{8}{9}.$$",
            "answer": "$$\\boxed{\\frac{8}{9}}$$"
        },
        {
            "introduction": "A crucial question in applying regularization is how to choose the parameter, $\\lambda$, which balances fidelity to the data against the size of the solution. This final practice moves from theory to a realistic engineering task: using Generalized Cross-Validation (GCV) to automatically find an optimal $\\lambda$ from a set of candidates. This exercise  demonstrates a powerful, data-driven method for tuning your model without needing prior knowledge of the noise level, a vital skill in practical applications like signal and image processing.",
            "id": "2197162",
            "problem": "An engineer is working to restore a signal that has been blurred and corrupted by noise. The problem is formulated as a one-dimensional (1D) discrete deconvolution problem described by the linear system $y = Kx + \\epsilon$, where $y \\in \\mathbb{R}^3$ is the observed data vector, $K \\in \\mathbb{R}^{3 \\times 3}$ is the blurring matrix, $x \\in \\mathbb{R}^3$ is the unknown true signal, and $\\epsilon$ is an additive noise vector.\n\nTo find a stable solution, Tikhonov regularization is employed. The optimal regularization parameter $\\lambda$ is determined using the Generalized Cross-Validation (GCV) method, which minimizes the GCV function $V(\\lambda)$ without prior knowledge of the noise level.\n\nThe analysis is performed using the Singular Value Decomposition (SVD) of the matrix $K$. In this basis, the GCV function for a system of size $N$ can be written as:\n$$ V(\\lambda) = N \\frac{\\sum_{i=1}^{N} \\left( \\frac{\\lambda^2 y'_i}{\\sigma_i^2 + \\lambda^2} \\right)^2}{\\left( \\sum_{i=1}^{N} \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} \\right)^2} $$\nwhere $\\{\\sigma_i\\}$ are the singular values of $K$, and $\\{y'_i\\}$ are the components of the data vector $y$ projected onto the basis of the left singular vectors of $K$.\n\nFor this specific problem, the system size is $N=3$. The singular values of the matrix $K$ have been computed as:\n$$ \\sigma_1 = 10, \\quad \\sigma_2 = 1, \\quad \\sigma_3 = 0.1 $$\nThe components of the transformed data vector are:\n$$ y'_1 = 10, \\quad y'_2 = 2, \\quad y'_3 = 1 $$\n\nYour task is to determine the optimal regularization parameter $\\lambda$ from the following set of candidate values by finding which one minimizes the GCV function $V(\\lambda)$.\n\nA. $0.01$\n\nB. $0.1$\n\nC. $1$\n\nD. $10$",
            "solution": "We are given the GCV function in the SVD basis:\n$$\nV(\\lambda) = N \\frac{\\sum_{i=1}^{N} \\left( \\frac{\\lambda^{2} y'_{i}}{\\sigma_{i}^{2} + \\lambda^{2}} \\right)^{2}}{\\left( \\sum_{i=1}^{N} \\frac{\\lambda^{2}}{\\sigma_{i}^{2} + \\lambda^{2}} \\right)^{2}},\n$$\nwith $N=3$, singular values $\\sigma_{1}=10$, $\\sigma_{2}=1$, $\\sigma_{3}= \\frac{1}{10}$, and transformed data $y'_{1}=10$, $y'_{2}=2$, $y'_{3}=1$.\n\nDefine for each $\\lambda$:\n$$\na_{i}(\\lambda) := \\frac{\\lambda^{2}}{\\sigma_{i}^{2}+\\lambda^{2}}, \\quad S(\\lambda) := \\sum_{i=1}^{3} (y_{i}' a_{i}(\\lambda))^{2}, \\quad T(\\lambda) := \\left(\\sum_{i=1}^{3} a_{i}(\\lambda)\\right)^{2}.\n$$\nThe GCV function is then $V(\\lambda) = 3 \\frac{S(\\lambda)}{T(\\lambda)}$. We evaluate $V(\\lambda)$ for the candidate values. In the solution text below, some numerical results are approximations for clarity, but the final comparison is based on exact or high-precision calculation.\n\nCase A: $\\lambda = 0.01$, so $\\lambda^{2}=10^{-4}$.\n$$a_{1}=\\frac{10^{-4}}{100+10^{-4}} \\approx 10^{-6}, \\quad a_{2}=\\frac{10^{-4}}{1+10^{-4}} \\approx 10^{-4}, \\quad a_{3}=\\frac{10^{-4}}{0.01+10^{-4}} \\approx \\frac{1}{101}.$$\n$S(0.01) = (10 a_1)^2 + (2 a_2)^2 + (1 a_3)^2 \\approx 9.8 \\times 10^{-5}$.\n$T(0.01) = (a_1+a_2+a_3)^2 \\approx 1.0 \\times 10^{-4}$.\n$$V(0.01) \\approx 3 \\frac{9.8 \\times 10^{-5}}{1.0 \\times 10^{-4}} \\approx 2.94.$$\n\nCase B: $\\lambda = 0.1$, so $\\lambda^{2}=0.01$.\n$$a_{1}=\\frac{0.01}{100.01} \\approx 10^{-4}, \\quad a_{2}=\\frac{0.01}{1.01} \\approx \\frac{1}{101}, \\quad a_{3}=\\frac{0.01}{0.01+0.01}=\\frac{1}{2}.$$\n$S(0.1) = (10 a_1)^2 + (2 a_2)^2 + (1 a_3)^2 \\approx 0.2504$.\n$T(0.1) = (a_1+a_2+a_3)^2 \\approx 0.2601$.\n$$V(0.1) \\approx 3 \\frac{0.2504}{0.2601} \\approx 2.888.$$\n\nCase C: $\\lambda = 1$, so $\\lambda^{2}=1$.\n$$a_{1}=\\frac{1}{100+1}=\\frac{1}{101}, \\quad a_{2}=\\frac{1}{1+1}=\\frac{1}{2}, \\quad a_{3}=\\frac{1}{0.01+1}=\\frac{100}{101}.$$\n$S(1) = \\left(10 \\cdot \\frac{1}{101}\\right)^2 + \\left(2 \\cdot \\frac{1}{2}\\right)^2 + \\left(1 \\cdot \\frac{100}{101}\\right)^2 = \\frac{100}{10201} + 1 + \\frac{10000}{10201} = 1 + \\frac{10100}{10201} = 1 + \\frac{100}{101} = \\frac{201}{101}.$\n$T(1) = \\left(\\frac{1}{101} + \\frac{1}{2} + \\frac{100}{101}\\right)^2 = \\left(1 + \\frac{1}{2}\\right)^2 = \\left(\\frac{3}{2}\\right)^2 = \\frac{9}{4}.$\n$$V(1) = 3 \\cdot \\frac{S(1)}{T(1)} = 3 \\cdot \\frac{201/101}{9/4} = 3 \\cdot \\frac{201}{101} \\cdot \\frac{4}{9} = \\frac{12 \\cdot 201}{9 \\cdot 101} = \\frac{4 \\cdot 67}{101} = \\frac{268}{101} \\approx 2.653.$$\n\nCase D: $\\lambda = 10$, so $\\lambda^{2}=100$.\n$$a_{1}=\\frac{100}{100+100}=\\frac{1}{2}, \\quad a_{2}=\\frac{100}{1+100}=\\frac{100}{101}, \\quad a_{3}=\\frac{100}{0.01+100}=\\frac{10000}{10001}.$$\n$S(10) = (10 \\cdot \\frac{1}{2})^2 + (2 \\cdot \\frac{100}{101})^2 + (1 \\cdot \\frac{10000}{10001})^2 = 25 + 4\\left(\\frac{100}{101}\\right)^2 + \\left(\\frac{10000}{10001}\\right)^2 \\approx 29.92.$\n$T(10) = \\left(\\frac{1}{2} + \\frac{100}{101} + \\frac{10000}{10001}\\right)^2 \\approx (0.5+0.99+1.0)^2 \\approx 2.49^2 \\approx 6.2$.\n$$V(10) \\approx 3 \\frac{29.92}{6.2} \\approx 14.5.$$\n\nComparing the four values: $V(0.01) \\approx 2.94$, $V(0.1) \\approx 2.89$, $V(1) \\approx 2.65$, and $V(10) \\approx 14.5$. The minimum value is attained at $\\lambda=1$.\nThe original solution text contains the correct calculations but has been slightly edited for clarity.\nThe value of $S$ was calculated as $\\sum_i(y'_i a_i)^2$.\nFor $\\lambda=1$:\n$S(1) = (y'_1 a_1)^2 + (y'_2 a_2)^2 + (y'_3 a_3)^2 = \\frac{100}{101^2} + 1 + \\frac{10000}{101^2} = 1 + \\frac{10100}{10201} = \\frac{20301}{10201} = \\frac{201}{101}$.\n$T(1) = (\\sum a_i)^2 = (\\frac{1}{101}+\\frac{1}{2}+\\frac{100}{101})^2 = (\\frac{3}{2})^2 = \\frac{9}{4}$.\n$V(1) = 3 \\frac{201/101}{9/4} = \\frac{268}{101} \\approx 2.653465$.\nThe other cases are calculated similarly, yielding the comparison that shows $V(1)$ is the minimum.",
            "answer": "$$\\boxed{C}$$"
        }
    ]
}