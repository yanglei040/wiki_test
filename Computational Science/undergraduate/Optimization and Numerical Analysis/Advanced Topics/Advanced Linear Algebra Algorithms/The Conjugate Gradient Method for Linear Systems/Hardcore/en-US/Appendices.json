{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand an algorithm is to perform it by hand. This first practice will guide you through one complete iteration of the Conjugate Gradient method for a simple system. By calculating the initial residual, the optimal step size, and the first update to the solution, you will build a solid foundation in the core mechanics of the CG algorithm. ",
            "id": "2211037",
            "problem": "Consider the problem of solving the linear system of equations $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a symmetric positive-definite matrix. The Conjugate Gradient (CG) method is an iterative algorithm well-suited for such problems.\n\nYou are given the specific system defined by the matrix $A$ and the vector $b$:\n$$\nA = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nStarting with an initial guess of $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, perform one complete iteration of the Conjugate Gradient algorithm to find the next approximation, $x_1$.\n\nYour final answer should be the vector $x_1$, presented as a row matrix of its components.",
            "solution": "The Conjugate Gradient (CG) method is an iterative algorithm for solving linear systems $A\\mathbf{x}=\\mathbf{b}$ where $A$ is a symmetric positive-definite matrix. We are asked to find the first approximation $x_1$ starting from $x_0 = \\mathbf{0}$.\n\nThe steps for the first iteration (from $k=0$ to $k=1$) of the CG algorithm are as follows:\n\n1.  **Initialize the residual:** The initial residual $r_0$ is defined as $r_0 = b - Ax_0$.\n2.  **Initialize the search direction:** The first search direction $p_0$ is set equal to the initial residual, $p_0 = r_0$.\n3.  **Compute the step size:** The optimal step size $\\alpha_0$ to move along the search direction $p_0$ is given by $\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0}$.\n4.  **Update the solution:** The new approximation $x_1$ is found by taking a step from $x_0$ along the direction $p_0$: $x_1 = x_0 + \\alpha_0 p_0$.\n\nLet's apply these steps to the given problem.\n\n**Step 1: Initialize the residual $r_0$**\nWe are given $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe residual is $r_0 = b - Ax_0$.\n$$\nAx_0 = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nTherefore,\n$$\nr_0 = b - \\mathbf{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\n\n**Step 2: Initialize the search direction $p_0$**\nThe initial search direction is set to the initial residual:\n$$\np_0 = r_0 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\n\n**Step 3: Compute the step size $\\alpha_0$**\nThe formula for the step size is $\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0}$.\nFirst, we compute the numerator, $r_0^T r_0$:\n$$\nr_0^T r_0 = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = (1)(1) + (2)(2) + (3)(3) = 1 + 4 + 9 = 14\n$$\nNext, we compute the denominator, $p_0^T A p_0$. Since $p_0 = r_0$, we need to calculate $p_0^T A p_0$. Let's first compute the vector $A p_0$:\n$$\nA p_0 = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (2)(1) \\\\ (4)(2) \\\\ (5)(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 8 \\\\ 15 \\end{pmatrix}\n$$\nNow we can compute the dot product $p_0^T (A p_0)$:\n$$\np_0^T A p_0 = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 8 \\\\ 15 \\end{pmatrix} = (1)(2) + (2)(8) + (3)(15) = 2 + 16 + 45 = 63\n$$\nFinally, we calculate $\\alpha_0$:\n$$\n\\alpha_0 = \\frac{14}{63} = \\frac{2 \\times 7}{9 \\times 7} = \\frac{2}{9}\n$$\n\n**Step 4: Update the solution to find $x_1$**\nThe updated solution is $x_1 = x_0 + \\alpha_0 p_0$.\n$$\nx_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{2}{9} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (2/9) \\times 1 \\\\ (2/9) \\times 2 \\\\ (2/9) \\times 3 \\end{pmatrix} = \\begin{pmatrix} 2/9 \\\\ 4/9 \\\\ 6/9 \\end{pmatrix} = \\begin{pmatrix} 2/9 \\\\ 4/9 \\\\ 2/3 \\end{pmatrix}\n$$\nThus, the first approximation after one iteration is $x_1 = \\begin{pmatrix} 2/9 \\\\ 4/9 \\\\ 2/3 \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{9} & \\frac{4}{9} & \\frac{2}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Now that we have walked through the mechanics, let's explore a special case to build intuition about the method's efficiency. This thought experiment considers the ideal scenario where the system matrix $A$ is the identity matrix. Solving this problem will demonstrate that the Conjugate Gradient method is \"optimal\" in a powerful sense, converging to the exact solution in a single step. ",
            "id": "2211022",
            "problem": "The Conjugate Gradient (CG) method is an iterative algorithm for numerically solving systems of linear equations of the form $A\\mathbf{x} = \\mathbf{b}$, where $A$ is an $n \\times n$ symmetric positive-definite matrix, $\\mathbf{x}$ is the unknown vector, and $\\mathbf{b}$ is a known vector.\n\nThe algorithm proceeds as follows, starting with an initial guess $\\mathbf{x}_0$:\n1.  Initialize the residual $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$.\n2.  Initialize the search direction $\\mathbf{p}_0 = \\mathbf{r}_0$.\n3.  For $k = 0, 1, 2, \\dots$ until the solution converges:\n    a. Compute the step size: $\\alpha_k = \\frac{\\mathbf{r}_k^T \\mathbf{r}_k}{\\mathbf{p}_k^T A \\mathbf{p}_k}$\n    b. Update the solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$\n    c. Update the residual: $\\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A \\mathbf{p}_k$\n    d. Compute the improvement factor for the search direction: $\\beta_k = \\frac{\\mathbf{r}_{k+1}^T \\mathbf{r}_{k+1}}{\\mathbf{r}_k^T \\mathbf{r}_k}$\n    e. Update the search direction: $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_k \\mathbf{p}_k$\n\nConsider a special case of the linear system $A\\mathbf{x} = \\mathbf{b}$ where $A$ is the $n \\times n$ identity matrix, $I_n$. The right-hand side vector is given by $\\mathbf{b} = \\mathbf{u}$, and the initial guess for the algorithm is $\\mathbf{x}_0 = \\mathbf{v}$. The vectors $\\mathbf{u}$ and $\\mathbf{v}$ are distinct vectors in $\\mathbb{R}^n$.\n\nYour task is to determine the state of the solution vector after one full iteration of the CG algorithm. Find the vector $\\mathbf{x}_1$. Express your answer in terms of $\\mathbf{u}$ and $\\mathbf{v}$.",
            "solution": "We apply the conjugate gradient algorithm to the system $A\\mathbf{x}=\\mathbf{b}$ with $A=I_{n}$, $\\mathbf{b}=\\mathbf{u}$, and initial guess $\\mathbf{x}_{0}=\\mathbf{v}$. The initial residual is\n$$\n\\mathbf{r}_{0}=\\mathbf{b}-A\\mathbf{x}_{0}=\\mathbf{u}-I_{n}\\mathbf{v}=\\mathbf{u}-\\mathbf{v}.\n$$\nThe initial search direction is\n$$\n\\mathbf{p}_{0}=\\mathbf{r}_{0}=\\mathbf{u}-\\mathbf{v}.\n$$\nThe step size is\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}\\mathbf{p}_{0}},\n$$\nand since $\\mathbf{p}_{0}=\\mathbf{r}_{0}$, it follows that\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}=1.\n$$\nTherefore, the updated solution after one iteration is\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}+\\alpha_{0}\\mathbf{p}_{0}=\\mathbf{v}+1\\cdot(\\mathbf{u}-\\mathbf{v})=\\mathbf{u}.\n$$\nFor completeness, the residual would become\n$$\n\\mathbf{r}_{1}=\\mathbf{r}_{0}-\\alpha_{0}A\\mathbf{p}_{0}=(\\mathbf{u}-\\mathbf{v})-1\\cdot(\\mathbf{u}-\\mathbf{v})=\\mathbf{0},\n$$\nso the method terminates in one step. The required vector is $\\mathbf{x}_{1}=\\mathbf{u}$.",
            "answer": "$$\\boxed{\\mathbf{u}}$$"
        },
        {
            "introduction": "The convergence of the Conjugate Gradient method holds some subtleties that are important to appreciate. While the method guarantees a reduction in the error's $A$-norm at each step, the same is not true for the more familiar Euclidean norm of the residual. This exercise provides a concrete, albeit hypothetical, example where the residual norm temporarily increases, highlighting a key and often misunderstood aspect of CG's behavior. ",
            "id": "2211015",
            "problem": "The Conjugate Gradient (CG) method is an iterative algorithm for numerically solving linear systems of equations $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ is symmetric and positive-definite. While the $A$-norm of the residual, $\\|\\mathbf{r}_k\\|_A = \\sqrt{\\mathbf{r}_k^T A \\mathbf{r}_k}$, is guaranteed to decrease monotonically with each iteration $k$, the more standard Euclidean norm, $\\|\\mathbf{r}_k\\|_2$, is not.\n\nConsider a linear system $A\\mathbf{x} = \\mathbf{b}$ defined by the $3 \\times 3$ symmetric positive-definite matrix\n$$\nA = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 50 & 0 \\\\ 0 & 0 & 100 \\end{pmatrix}\n$$\nand a right-hand side vector $\\mathbf{b} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm is initiated with a starting guess $\\mathbf{x}_0 = \\begin{pmatrix} -2 \\\\ 0 \\\\ -0.01 \\end{pmatrix}$. The first iteration ($k=0$) proceeds as follows:\n1. Compute the initial residual: $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$.\n2. Set the initial search direction: $\\mathbf{p}_0 = \\mathbf{r}_0$.\n3. Compute the step size: $\\alpha_0 = \\frac{\\mathbf{r}_0^T \\mathbf{r}_0}{\\mathbf{p}_0^T A \\mathbf{p}_0}$.\n4. Update the residual: $\\mathbf{r}_1 = \\mathbf{r}_0 - \\alpha_0 A \\mathbf{p}_0$.\n\nCalculate the ratio of the Euclidean norms of the residual vectors after the first step, $\\|\\mathbf{r}_1\\|_2 / \\|\\mathbf{r}_0\\|_2$. Round your final answer to four significant figures.",
            "solution": "We are given $A=\\operatorname{diag}(1,50,100)$, $\\mathbf{b}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$, and $\\mathbf{x}_{0}=\\begin{pmatrix}-2\\\\0\\\\-0.01\\end{pmatrix}$. The initial residual is defined by $\\mathbf{r}_{0}=\\mathbf{b}-A\\mathbf{x}_{0}$. Compute $A\\mathbf{x}_{0}=\\begin{pmatrix}1\\cdot(-2)\\\\50\\cdot 0\\\\100\\cdot(-0.01)\\end{pmatrix}=\\begin{pmatrix}-2\\\\0\\\\-1\\end{pmatrix}$, hence\n$$\n\\mathbf{r}_{0}=\\begin{pmatrix}2\\\\0\\\\1\\end{pmatrix}.\n$$\nThe initial search direction is $\\mathbf{p}_{0}=\\mathbf{r}_{0}$. The step size is\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}}=\\frac{2^{2}+0^{2}+1^{2}}{1\\cdot 2^{2}+50\\cdot 0^{2}+100\\cdot 1^{2}}=\\frac{5}{104}.\n$$\nWe update the residual via $\\mathbf{r}_{1}=\\mathbf{r}_{0}-\\alpha_{0}A\\mathbf{p}_{0}$. First compute $A\\mathbf{p}_{0}=\\begin{pmatrix}2\\\\0\\\\100\\end{pmatrix}$, so\n$$\n\\mathbf{r}_{1}=\\begin{pmatrix}2\\\\0\\\\1\\end{pmatrix}-\\frac{5}{104}\\begin{pmatrix}2\\\\0\\\\100\\end{pmatrix}=\\begin{pmatrix}2-\\frac{10}{104}\\\\0\\\\1-\\frac{500}{104}\\end{pmatrix}=\\begin{pmatrix}\\frac{99}{52}\\\\0\\\\-\\frac{99}{26}\\end{pmatrix}=\\frac{99}{52}\\begin{pmatrix}1\\\\0\\\\-2\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\|\\mathbf{r}_{0}\\|_{2}=\\sqrt{2^{2}+0^{2}+1^{2}}=\\sqrt{5},\\qquad \\|\\mathbf{r}_{1}\\|_{2}=\\frac{99}{52}\\sqrt{1^{2}+0^{2}+(-2)^{2}}=\\frac{99}{52}\\sqrt{5}.\n$$\nThe ratio is\n$$\n\\frac{\\|\\mathbf{r}_{1}\\|_{2}}{\\|\\mathbf{r}_{0}\\|_{2}}=\\frac{99}{52}.\n$$\nNumerically, $\\frac{99}{52}=1.903846\\ldots$, which rounded to four significant figures is $1.904$.",
            "answer": "$$\\boxed{1.904}$$"
        }
    ]
}