{
    "hands_on_practices": [
        {
            "introduction": "The Conjugate Gradient method iteratively refines a solution by taking steps in carefully chosen directions. The length of each step, $\\alpha_k$, is chosen to be optimal in a specific sense. This first practice focuses on the crucial calculation of the very first step size, $\\alpha_0$, which minimizes the error along the initial search direction and forms the foundation of the entire algorithm. ",
            "id": "2210975",
            "problem": "Consider the linear system of equations $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a symmetric positive-definite matrix and $\\mathbf{b}$ is a vector. This system is to be solved using the Conjugate Gradient (CG) method. The specific matrix and vector are given by:\n$$ A = \\begin{pmatrix} 5 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nThe iterative process starts from the initial guess $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. In the first iteration of the CG algorithm, the solution is updated according to $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0 \\mathbf{p}_0$, where $\\mathbf{p}_0$ is the initial search direction and $\\alpha_0$ is the optimal step size that minimizes the A-norm of the error along the search direction.\n\nYour task is to compute the value of this first step size, $\\alpha_0$. Express your answer as an exact fraction.",
            "solution": "For a symmetric positive-definite matrix $A$, the Conjugate Gradient method starts with $\\mathbf{x}_{0}$, residual $\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$, and initial search direction $\\mathbf{p}_{0} = \\mathbf{r}_{0}$. The step size $\\alpha_{0}$ along $\\mathbf{p}_{0}$ that minimizes the $A$-norm of the error is obtained by minimizing the quadratic functional $f(\\mathbf{x}) = \\tfrac{1}{2} \\mathbf{x}^{T} A \\mathbf{x} - \\mathbf{b}^{T} \\mathbf{x}$ along $\\mathbf{x}(\\alpha) = \\mathbf{x}_{0} + \\alpha \\mathbf{p}_{0}$. Setting the derivative to zero gives\n$$\n\\mathbf{p}_{0}^{T}\\left(A(\\mathbf{x}_{0} + \\alpha \\mathbf{p}_{0}) - \\mathbf{b}\\right) = \\mathbf{p}_{0}^{T}(A \\mathbf{x}_{0} - \\mathbf{b}) + \\alpha\\, \\mathbf{p}_{0}^{T} A \\mathbf{p}_{0} = -\\mathbf{p}_{0}^{T} \\mathbf{r}_{0} + \\alpha\\, \\mathbf{p}_{0}^{T} A \\mathbf{p}_{0} = 0.\n$$\nhence\n$$\n\\alpha_{0} = \\frac{\\mathbf{p}_{0}^{T} \\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T} A \\mathbf{p}_{0}}.\n$$\nWith the CG initialization $\\mathbf{p}_{0} = \\mathbf{r}_{0}$, this reduces to\n$$\n\\alpha_{0} = \\frac{\\mathbf{r}_{0}^{T} \\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T} A \\mathbf{p}_{0}}.\n$$\nGiven $\\mathbf{x}_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the initial residual is\n$$\n\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0} = \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\nso $\\mathbf{p}_{0} = \\mathbf{r}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Compute the numerator:\n$$\n\\mathbf{r}_{0}^{T} \\mathbf{r}_{0} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 2.\n$$\nCompute the denominator by first evaluating $A \\mathbf{p}_{0}$:\n$$\nA \\mathbf{p}_{0} = \\begin{pmatrix} 5 & 1 \\\\ 1 & 2 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 3 \\end{pmatrix},\n$$\nthen\n$$\n\\mathbf{p}_{0}^{T} A \\mathbf{p}_{0} = \\begin{pmatrix} 1 & 1 \\end{pmatrix}\\begin{pmatrix} 6 \\\\ 3 \\end{pmatrix} = 9.\n$$\nTherefore,\n$$\n\\alpha_{0} = \\frac{2}{9}.\n$$",
            "answer": "$$\\boxed{\\frac{2}{9}}$$"
        },
        {
            "introduction": "Having mastered the calculation of the step size, we can now execute a full iteration of the Conjugate Gradient algorithm. This exercise guides you through all the necessary computations—from finding the initial residual to updating the solution vector—for a simple system. By working through this process, you will solidify your understanding of the algorithm's complete workflow in a single step. ",
            "id": "2211037",
            "problem": "Consider the problem of solving the linear system of equations $A\\mathbf{x} = \\mathbf{b}$, where $A$ is a symmetric positive-definite matrix. The Conjugate Gradient (CG) method is an iterative algorithm well-suited for such problems.\n\nYou are given the specific system defined by the matrix $A$ and the vector $\\mathbf{b}$:\n$$\nA = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}, \\quad \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\nStarting with an initial guess of $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, perform one complete iteration of the Conjugate Gradient algorithm to find the next approximation, $\\mathbf{x}_1$.\n\nYour final answer should be the vector $\\mathbf{x}_1$, presented as a row matrix of its components.",
            "solution": "The Conjugate Gradient (CG) method is an iterative algorithm for solving linear systems $A\\mathbf{x}=\\mathbf{b}$ where $A$ is a symmetric positive-definite matrix. We are asked to find the first approximation $\\mathbf{x}_1$ starting from $\\mathbf{x}_0 = \\mathbf{0}$.\n\nThe steps for the first iteration (from $k=0$ to $k=1$) of the CG algorithm are as follows:\n\n1.  **Initialize the residual:** The initial residual $\\mathbf{r}_0$ is defined as $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$.\n2.  **Initialize the search direction:** The first search direction $\\mathbf{p}_0$ is set equal to the initial residual, $\\mathbf{p}_0 = \\mathbf{r}_0$.\n3.  **Compute the step size:** The optimal step size $\\alpha_0$ to move along the search direction $\\mathbf{p}_0$ is given by $\\alpha_0 = \\frac{\\mathbf{r}_0^T \\mathbf{r}_0}{\\mathbf{p}_0^T A \\mathbf{p}_0}$.\n4.  **Update the solution:** The new approximation $\\mathbf{x}_1$ is found by taking a step from $\\mathbf{x}_0$ along the direction $\\mathbf{p}_0$: $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0 \\mathbf{p}_0$.\n\nLet's apply these steps to the given problem.\n\n**Step 1: Initialize the residual $\\mathbf{r}_0$**\nWe are given $\\mathbf{x}_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe residual is $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$.\n$$\nA\\mathbf{x}_0 = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nTherefore,\n$$\n\\mathbf{r}_0 = \\mathbf{b} - \\mathbf{0} = \\mathbf{b} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\n\n**Step 2: Initialize the search direction $\\mathbf{p}_0$**\nThe initial search direction is set to the initial residual:\n$$\n\\mathbf{p}_0 = \\mathbf{r}_0 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n$$\n\n**Step 3: Compute the step size $\\alpha_0$**\nThe formula for the step size is $\\alpha_0 = \\frac{\\mathbf{r}_0^T \\mathbf{r}_0}{\\mathbf{p}_0^T A \\mathbf{p}_0}$.\nFirst, we compute the numerator, $\\mathbf{r}_0^T \\mathbf{r}_0$:\n$$\n\\mathbf{r}_0^T \\mathbf{r}_0 = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = (1)(1) + (2)(2) + (3)(3) = 1 + 4 + 9 = 14\n$$\nNext, we compute the denominator, $\\mathbf{p}_0^T A \\mathbf{p}_0$. Since $\\mathbf{p}_0 = \\mathbf{r}_0$, we need to calculate $\\mathbf{p}_0^T A \\mathbf{p}_0$. Let's first compute the vector $A \\mathbf{p}_0$:\n$$\nA \\mathbf{p}_0 = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (2)(1) \\\\ (4)(2) \\\\ (5)(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 8 \\\\ 15 \\end{pmatrix}\n$$\nNow we can compute the dot product $\\mathbf{p}_0^T (A \\mathbf{p}_0)$:\n$$\n\\mathbf{p}_0^T A \\mathbf{p}_0 = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 8 \\\\ 15 \\end{pmatrix} = (1)(2) + (2)(8) + (3)(15) = 2 + 16 + 45 = 63\n$$\nFinally, we calculate $\\alpha_0$:\n$$\n\\alpha_0 = \\frac{14}{63} = \\frac{2 \\times 7}{9 \\times 7} = \\frac{2}{9}\n$$\n\n**Step 4: Update the solution to find $\\mathbf{x}_1$**\nThe updated solution is $\\mathbf{x}_1 = \\mathbf{x}_0 + \\alpha_0 \\mathbf{p}_0$.\n$$\n\\mathbf{x}_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{2}{9} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (2/9) \\times 1 \\\\ (2/9) \\times 2 \\\\ (2/9) \\times 3 \\end{pmatrix} = \\begin{pmatrix} 2/9 \\\\ 4/9 \\\\ 6/9 \\end{pmatrix} = \\begin{pmatrix} 2/9 \\\\ 4/9 \\\\ 2/3 \\end{pmatrix}\n$$\nThus, the first approximation after one iteration is $\\mathbf{x}_1 = \\begin{pmatrix} 2/9 \\\\ 4/9 \\\\ 2/3 \\end{pmatrix}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{2}{9} & \\frac{4}{9} & \\frac{2}{3} \\end{pmatrix}}$$"
        },
        {
            "introduction": "A key feature of the Conjugate Gradient method is the guaranteed monotonic decrease of the error in the 'energy' or $A$-norm. However, it is a common misconception that the standard Euclidean norm of the residual vector, $\\|\\mathbf{r}_k\\|_2$, must also decrease at every step. This practice presents a carefully constructed thought experiment that demonstrates how $\\|\\mathbf{r}_k\\|_2$ can temporarily increase, offering a deeper insight into the true nature of convergence in the CG method. ",
            "id": "2211015",
            "problem": "The Conjugate Gradient (CG) method is an iterative algorithm for numerically solving linear systems of equations $A\\mathbf{x} = \\mathbf{b}$, where the matrix $A$ is symmetric and positive-definite. While the $A$-norm of the residual, $\\|\\mathbf{r}_k\\|_A = \\sqrt{\\mathbf{r}_k^T A \\mathbf{r}_k}$, is guaranteed to decrease monotonically with each iteration $k$, the more standard Euclidean norm, $\\|\\mathbf{r}_k\\|_2$, is not.\n\nConsider a linear system $A\\mathbf{x} = \\mathbf{b}$ defined by the $3 \\times 3$ symmetric positive-definite matrix\n$$\nA = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 50 & 0 \\\\ 0 & 0 & 100 \\end{pmatrix}\n$$\nand a right-hand side vector $\\mathbf{b} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm is initiated with a starting guess $\\mathbf{x}_0 = \\begin{pmatrix} -2 \\\\ 0 \\\\ -0.01 \\end{pmatrix}$. The first iteration ($k=0$) proceeds as follows:\n1. Compute the initial residual: $\\mathbf{r}_0 = \\mathbf{b} - A\\mathbf{x}_0$.\n2. Set the initial search direction: $\\mathbf{p}_0 = \\mathbf{r}_0$.\n3. Compute the step size: $\\alpha_0 = \\frac{\\mathbf{r}_0^T \\mathbf{r}_0}{\\mathbf{p}_0^T A \\mathbf{p}_0}$.\n4. Update the residual: $\\mathbf{r}_1 = \\mathbf{r}_0 - \\alpha_0 A \\mathbf{p}_0$.\n\nCalculate the ratio of the Euclidean norms of the residual vectors after the first step, $\\|\\mathbf{r}_1\\|_2 / \\|\\mathbf{r}_0\\|_2$. Round your final answer to four significant figures.",
            "solution": "We are given $A=\\operatorname{diag}(1,50,100)$, $\\mathbf{b}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$, and $\\mathbf{x}_{0}=\\begin{pmatrix}-2\\\\0\\\\-0.01\\end{pmatrix}$. The initial residual is defined by $\\mathbf{r}_{0}=\\mathbf{b}-A\\mathbf{x}_{0}$. Compute $A\\mathbf{x}_{0}=\\begin{pmatrix}1\\cdot(-2)\\\\50\\cdot 0\\\\100\\cdot(-0.01)\\end{pmatrix}=\\begin{pmatrix}-2\\\\0\\\\-1\\end{pmatrix}$, hence\n$$\n\\mathbf{r}_{0}=\\begin{pmatrix}2\\\\0\\\\1\\end{pmatrix}.\n$$\nThe initial search direction is $\\mathbf{p}_{0}=\\mathbf{r}_{0}$. The step size is\n$$\n\\alpha_{0}=\\frac{\\mathbf{r}_{0}^{T}\\mathbf{r}_{0}}{\\mathbf{p}_{0}^{T}A\\mathbf{p}_{0}}=\\frac{2^{2}+0^{2}+1^{2}}{1\\cdot 2^{2}+50\\cdot 0^{2}+100\\cdot 1^{2}}=\\frac{5}{104}.\n$$\nWe update the residual via $\\mathbf{r}_{1}=\\mathbf{r}_{0}-\\alpha_{0}A\\mathbf{p}_{0}$. First compute $A\\mathbf{p}_{0}=\\begin{pmatrix}2\\\\0\\\\100\\end{pmatrix}$, so\n$$\n\\mathbf{r}_{1}=\\begin{pmatrix}2\\\\0\\\\1\\end{pmatrix}-\\frac{5}{104}\\begin{pmatrix}2\\\\0\\\\100\\end{pmatrix}=\\begin{pmatrix}2-\\frac{10}{104}\\\\0\\\\1-\\frac{500}{104}\\end{pmatrix}=\\begin{pmatrix}\\frac{99}{52}\\\\0\\\\-\\frac{99}{26}\\end{pmatrix}=\\frac{99}{52}\\begin{pmatrix}1\\\\0\\\\-2\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\|\\mathbf{r}_{0}\\|_{2}=\\sqrt{2^{2}+0^{2}+1^{2}}=\\sqrt{5},\\qquad \\|\\mathbf{r}_{1}\\|_{2}=\\frac{99}{52}\\sqrt{1^{2}+0^{2}+(-2)^{2}}=\\frac{99}{52}\\sqrt{5}.\n$$\nThe ratio is\n$$\n\\frac{\\|\\mathbf{r}_{1}\\|_{2}}{\\|\\mathbf{r}_{0}\\|_{2}}=\\frac{99}{52}.\n$$\nNumerically, $\\frac{99}{52}=1.903846\\ldots$, which rounded to four significant figures is $1.904$.",
            "answer": "$$\\boxed{1.904}$$"
        }
    ]
}